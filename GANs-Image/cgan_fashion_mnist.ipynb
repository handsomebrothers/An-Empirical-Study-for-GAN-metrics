{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 2,175,249\n",
      "Trainable params: 2,174,353\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 25088)             2533888   \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 128)       295040    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 56, 56, 64)        73792     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 4,086,785\n",
      "Trainable params: 4,084,993\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:1 [D loss: 1.234374, acc.: 17.97%] [G loss: 0.947227]\n",
      "epoch:0 step:2 [D loss: 0.336171, acc.: 80.47%] [G loss: 2.773298]\n",
      "epoch:0 step:3 [D loss: 0.102709, acc.: 98.44%] [G loss: 3.755343]\n",
      "epoch:0 step:4 [D loss: 0.194898, acc.: 91.41%] [G loss: 3.572041]\n",
      "epoch:0 step:5 [D loss: 0.213105, acc.: 92.97%] [G loss: 3.598420]\n",
      "epoch:0 step:6 [D loss: 0.050351, acc.: 100.00%] [G loss: 3.506066]\n",
      "epoch:0 step:7 [D loss: 0.082331, acc.: 99.22%] [G loss: 3.414533]\n",
      "epoch:0 step:8 [D loss: 0.278439, acc.: 88.28%] [G loss: 4.919701]\n",
      "epoch:0 step:9 [D loss: 0.315177, acc.: 86.72%] [G loss: 6.049656]\n",
      "epoch:0 step:10 [D loss: 0.255245, acc.: 90.62%] [G loss: 5.389141]\n",
      "epoch:0 step:11 [D loss: 0.176346, acc.: 92.97%] [G loss: 6.052828]\n",
      "epoch:0 step:12 [D loss: 0.454458, acc.: 81.25%] [G loss: 6.526584]\n",
      "epoch:0 step:13 [D loss: 0.675105, acc.: 72.66%] [G loss: 5.619236]\n",
      "epoch:0 step:14 [D loss: 0.654224, acc.: 71.09%] [G loss: 5.514531]\n",
      "epoch:0 step:15 [D loss: 0.489142, acc.: 80.47%] [G loss: 6.094590]\n",
      "epoch:0 step:16 [D loss: 0.665027, acc.: 71.88%] [G loss: 7.965801]\n",
      "epoch:0 step:17 [D loss: 0.464033, acc.: 85.16%] [G loss: 4.368988]\n",
      "epoch:0 step:18 [D loss: 0.562535, acc.: 74.22%] [G loss: 5.595560]\n",
      "epoch:0 step:19 [D loss: 0.242306, acc.: 92.19%] [G loss: 7.570657]\n",
      "epoch:0 step:20 [D loss: 0.402073, acc.: 78.91%] [G loss: 5.354303]\n",
      "epoch:0 step:21 [D loss: 0.198225, acc.: 92.19%] [G loss: 5.475820]\n",
      "epoch:0 step:22 [D loss: 0.161964, acc.: 95.31%] [G loss: 6.238033]\n",
      "epoch:0 step:23 [D loss: 0.158294, acc.: 92.19%] [G loss: 7.389889]\n",
      "epoch:0 step:24 [D loss: 0.287154, acc.: 91.41%] [G loss: 6.871184]\n",
      "epoch:0 step:25 [D loss: 0.293577, acc.: 95.31%] [G loss: 6.108750]\n",
      "epoch:0 step:26 [D loss: 0.093242, acc.: 96.88%] [G loss: 4.295348]\n",
      "epoch:0 step:27 [D loss: 0.109478, acc.: 95.31%] [G loss: 4.848004]\n",
      "epoch:0 step:28 [D loss: 0.084374, acc.: 97.66%] [G loss: 5.364033]\n",
      "epoch:0 step:29 [D loss: 0.187414, acc.: 94.53%] [G loss: 4.969492]\n",
      "epoch:0 step:30 [D loss: 0.029781, acc.: 99.22%] [G loss: 5.867616]\n",
      "epoch:0 step:31 [D loss: 0.055650, acc.: 97.66%] [G loss: 5.640779]\n",
      "epoch:0 step:32 [D loss: 0.063906, acc.: 96.88%] [G loss: 5.728462]\n",
      "epoch:0 step:33 [D loss: 0.046194, acc.: 98.44%] [G loss: 5.879836]\n",
      "epoch:0 step:34 [D loss: 0.085580, acc.: 96.88%] [G loss: 4.838698]\n",
      "epoch:0 step:35 [D loss: 0.025255, acc.: 99.22%] [G loss: 4.752522]\n",
      "epoch:0 step:36 [D loss: 0.012627, acc.: 100.00%] [G loss: 5.320568]\n",
      "epoch:0 step:37 [D loss: 0.050080, acc.: 98.44%] [G loss: 4.607298]\n",
      "epoch:0 step:38 [D loss: 0.124943, acc.: 96.09%] [G loss: 4.712695]\n",
      "epoch:0 step:39 [D loss: 0.015050, acc.: 100.00%] [G loss: 4.688322]\n",
      "epoch:0 step:40 [D loss: 0.011690, acc.: 100.00%] [G loss: 4.775753]\n",
      "epoch:0 step:41 [D loss: 0.015785, acc.: 100.00%] [G loss: 5.179729]\n",
      "epoch:0 step:42 [D loss: 0.007810, acc.: 100.00%] [G loss: 5.261094]\n",
      "epoch:0 step:43 [D loss: 0.058343, acc.: 99.22%] [G loss: 5.206914]\n",
      "epoch:0 step:44 [D loss: 0.108674, acc.: 95.31%] [G loss: 7.058865]\n",
      "epoch:0 step:45 [D loss: 0.064292, acc.: 97.66%] [G loss: 6.739100]\n",
      "epoch:0 step:46 [D loss: 0.155449, acc.: 94.53%] [G loss: 7.640370]\n",
      "epoch:0 step:47 [D loss: 0.132354, acc.: 96.09%] [G loss: 7.384336]\n",
      "epoch:0 step:48 [D loss: 0.089678, acc.: 97.66%] [G loss: 6.551170]\n",
      "epoch:0 step:49 [D loss: 0.368684, acc.: 82.81%] [G loss: 8.105282]\n",
      "epoch:0 step:50 [D loss: 0.426192, acc.: 86.72%] [G loss: 8.018814]\n",
      "epoch:0 step:51 [D loss: 0.095224, acc.: 97.66%] [G loss: 7.785383]\n",
      "epoch:0 step:52 [D loss: 0.985703, acc.: 82.81%] [G loss: 7.934394]\n",
      "epoch:0 step:53 [D loss: 0.124362, acc.: 95.31%] [G loss: 7.375333]\n",
      "epoch:0 step:54 [D loss: 0.231187, acc.: 93.75%] [G loss: 7.964134]\n",
      "epoch:0 step:55 [D loss: 0.012037, acc.: 100.00%] [G loss: 6.919527]\n",
      "epoch:0 step:56 [D loss: 0.046823, acc.: 98.44%] [G loss: 5.810316]\n",
      "epoch:0 step:57 [D loss: 0.012682, acc.: 100.00%] [G loss: 6.603873]\n",
      "epoch:0 step:58 [D loss: 0.043156, acc.: 98.44%] [G loss: 5.482911]\n",
      "epoch:0 step:59 [D loss: 0.037633, acc.: 98.44%] [G loss: 5.425990]\n",
      "epoch:0 step:60 [D loss: 0.094579, acc.: 99.22%] [G loss: 5.550285]\n",
      "epoch:0 step:61 [D loss: 0.130637, acc.: 98.44%] [G loss: 6.748500]\n",
      "epoch:0 step:62 [D loss: 0.098871, acc.: 98.44%] [G loss: 6.943616]\n",
      "epoch:0 step:63 [D loss: 0.067376, acc.: 98.44%] [G loss: 6.578793]\n",
      "epoch:0 step:64 [D loss: 0.081906, acc.: 98.44%] [G loss: 6.692902]\n",
      "epoch:0 step:65 [D loss: 0.042947, acc.: 99.22%] [G loss: 5.724113]\n",
      "epoch:0 step:66 [D loss: 0.238148, acc.: 90.62%] [G loss: 6.226733]\n",
      "epoch:0 step:67 [D loss: 0.811952, acc.: 85.94%] [G loss: 5.783718]\n",
      "epoch:0 step:68 [D loss: 0.449268, acc.: 94.53%] [G loss: 7.596354]\n",
      "epoch:0 step:69 [D loss: 0.080632, acc.: 96.88%] [G loss: 6.545725]\n",
      "epoch:0 step:70 [D loss: 0.121771, acc.: 95.31%] [G loss: 6.480397]\n",
      "epoch:0 step:71 [D loss: 0.237207, acc.: 91.41%] [G loss: 9.003144]\n",
      "epoch:0 step:72 [D loss: 0.080002, acc.: 96.09%] [G loss: 9.539766]\n",
      "epoch:0 step:73 [D loss: 0.059300, acc.: 98.44%] [G loss: 7.207026]\n",
      "epoch:0 step:74 [D loss: 0.145777, acc.: 96.09%] [G loss: 6.758797]\n",
      "epoch:0 step:75 [D loss: 0.021601, acc.: 100.00%] [G loss: 6.035595]\n",
      "epoch:0 step:76 [D loss: 0.046425, acc.: 97.66%] [G loss: 6.739629]\n",
      "epoch:0 step:77 [D loss: 0.005755, acc.: 100.00%] [G loss: 5.684358]\n",
      "epoch:0 step:78 [D loss: 0.015562, acc.: 99.22%] [G loss: 6.831217]\n",
      "epoch:0 step:79 [D loss: 0.012992, acc.: 100.00%] [G loss: 5.691925]\n",
      "epoch:0 step:80 [D loss: 0.009939, acc.: 100.00%] [G loss: 5.410967]\n",
      "epoch:0 step:81 [D loss: 0.013680, acc.: 100.00%] [G loss: 5.665917]\n",
      "epoch:0 step:82 [D loss: 0.022032, acc.: 100.00%] [G loss: 6.571879]\n",
      "epoch:0 step:83 [D loss: 0.021353, acc.: 100.00%] [G loss: 6.995258]\n",
      "epoch:0 step:84 [D loss: 0.044139, acc.: 99.22%] [G loss: 6.174472]\n",
      "epoch:0 step:85 [D loss: 0.071718, acc.: 98.44%] [G loss: 6.699319]\n",
      "epoch:0 step:86 [D loss: 0.077557, acc.: 97.66%] [G loss: 5.722202]\n",
      "epoch:0 step:87 [D loss: 0.137783, acc.: 94.53%] [G loss: 7.489712]\n",
      "epoch:0 step:88 [D loss: 0.275021, acc.: 94.53%] [G loss: 6.830056]\n",
      "epoch:0 step:89 [D loss: 0.021271, acc.: 100.00%] [G loss: 7.852416]\n",
      "epoch:0 step:90 [D loss: 0.006051, acc.: 100.00%] [G loss: 8.271986]\n",
      "epoch:0 step:91 [D loss: 0.024929, acc.: 100.00%] [G loss: 6.035153]\n",
      "epoch:0 step:92 [D loss: 0.007470, acc.: 100.00%] [G loss: 8.116488]\n",
      "epoch:0 step:93 [D loss: 0.013704, acc.: 100.00%] [G loss: 7.339359]\n",
      "epoch:0 step:94 [D loss: 0.030564, acc.: 99.22%] [G loss: 7.611437]\n",
      "epoch:0 step:95 [D loss: 0.003592, acc.: 100.00%] [G loss: 6.586726]\n",
      "epoch:0 step:96 [D loss: 0.006022, acc.: 100.00%] [G loss: 6.167094]\n",
      "epoch:0 step:97 [D loss: 0.005496, acc.: 100.00%] [G loss: 4.757225]\n",
      "epoch:0 step:98 [D loss: 0.008197, acc.: 100.00%] [G loss: 4.561570]\n",
      "epoch:0 step:99 [D loss: 0.043373, acc.: 99.22%] [G loss: 7.296913]\n",
      "epoch:0 step:100 [D loss: 1.335122, acc.: 86.72%] [G loss: 6.028513]\n",
      "epoch:0 step:101 [D loss: 0.931194, acc.: 78.12%] [G loss: 9.141047]\n",
      "epoch:0 step:102 [D loss: 0.803296, acc.: 90.62%] [G loss: 10.399811]\n",
      "epoch:0 step:103 [D loss: 0.369542, acc.: 94.53%] [G loss: 10.754179]\n",
      "epoch:0 step:104 [D loss: 0.139179, acc.: 96.88%] [G loss: 7.397807]\n",
      "epoch:0 step:105 [D loss: 0.251204, acc.: 92.97%] [G loss: 8.454834]\n",
      "epoch:0 step:106 [D loss: 0.216552, acc.: 91.41%] [G loss: 8.762825]\n",
      "epoch:0 step:107 [D loss: 0.436201, acc.: 91.41%] [G loss: 7.569741]\n",
      "epoch:0 step:108 [D loss: 0.293660, acc.: 93.75%] [G loss: 8.235265]\n",
      "epoch:0 step:109 [D loss: 0.058828, acc.: 100.00%] [G loss: 8.416138]\n",
      "epoch:0 step:110 [D loss: 0.019468, acc.: 100.00%] [G loss: 7.175632]\n",
      "epoch:0 step:111 [D loss: 0.053071, acc.: 99.22%] [G loss: 7.819118]\n",
      "epoch:0 step:112 [D loss: 0.017989, acc.: 99.22%] [G loss: 9.214328]\n",
      "epoch:0 step:113 [D loss: 0.033942, acc.: 100.00%] [G loss: 6.671639]\n",
      "epoch:0 step:114 [D loss: 0.170421, acc.: 91.41%] [G loss: 7.477335]\n",
      "epoch:0 step:115 [D loss: 0.449401, acc.: 88.28%] [G loss: 7.937405]\n",
      "epoch:0 step:116 [D loss: 0.081334, acc.: 97.66%] [G loss: 6.459823]\n",
      "epoch:0 step:117 [D loss: 0.042565, acc.: 100.00%] [G loss: 8.026384]\n",
      "epoch:0 step:118 [D loss: 0.140478, acc.: 93.75%] [G loss: 5.564723]\n",
      "epoch:0 step:119 [D loss: 0.046203, acc.: 97.66%] [G loss: 8.615524]\n",
      "epoch:0 step:120 [D loss: 0.024742, acc.: 98.44%] [G loss: 7.143743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:121 [D loss: 0.062799, acc.: 99.22%] [G loss: 8.141939]\n",
      "epoch:0 step:122 [D loss: 0.012368, acc.: 100.00%] [G loss: 6.363040]\n",
      "epoch:0 step:123 [D loss: 0.052279, acc.: 98.44%] [G loss: 5.787856]\n",
      "epoch:0 step:124 [D loss: 0.051564, acc.: 99.22%] [G loss: 6.192499]\n",
      "epoch:0 step:125 [D loss: 0.316352, acc.: 93.75%] [G loss: 8.801474]\n",
      "epoch:0 step:126 [D loss: 1.003181, acc.: 79.69%] [G loss: 4.562503]\n",
      "epoch:0 step:127 [D loss: 1.569089, acc.: 83.59%] [G loss: 8.332919]\n",
      "epoch:0 step:128 [D loss: 0.379794, acc.: 89.84%] [G loss: 7.376862]\n",
      "epoch:0 step:129 [D loss: 0.534898, acc.: 87.50%] [G loss: 7.084747]\n",
      "epoch:0 step:130 [D loss: 0.589072, acc.: 92.19%] [G loss: 9.820095]\n",
      "epoch:0 step:131 [D loss: 0.334026, acc.: 94.53%] [G loss: 6.171810]\n",
      "epoch:0 step:132 [D loss: 0.545914, acc.: 86.72%] [G loss: 7.869609]\n",
      "epoch:0 step:133 [D loss: 0.063916, acc.: 97.66%] [G loss: 9.714008]\n",
      "epoch:0 step:134 [D loss: 0.223990, acc.: 96.09%] [G loss: 10.179804]\n",
      "epoch:0 step:135 [D loss: 0.186846, acc.: 96.09%] [G loss: 6.270172]\n",
      "epoch:0 step:136 [D loss: 0.305915, acc.: 92.19%] [G loss: 7.728949]\n",
      "epoch:0 step:137 [D loss: 0.190918, acc.: 89.06%] [G loss: 7.732388]\n",
      "epoch:0 step:138 [D loss: 0.231062, acc.: 92.19%] [G loss: 8.838645]\n",
      "epoch:0 step:139 [D loss: 0.769117, acc.: 85.16%] [G loss: 6.995375]\n",
      "epoch:0 step:140 [D loss: 1.020373, acc.: 84.38%] [G loss: 9.936756]\n",
      "epoch:0 step:141 [D loss: 0.720466, acc.: 84.38%] [G loss: 9.323877]\n",
      "epoch:0 step:142 [D loss: 0.075795, acc.: 96.88%] [G loss: 8.437983]\n",
      "epoch:0 step:143 [D loss: 0.514934, acc.: 83.59%] [G loss: 5.938004]\n",
      "epoch:0 step:144 [D loss: 0.754591, acc.: 89.06%] [G loss: 9.658101]\n",
      "epoch:0 step:145 [D loss: 0.048377, acc.: 97.66%] [G loss: 7.970512]\n",
      "epoch:0 step:146 [D loss: 0.058445, acc.: 99.22%] [G loss: 7.297904]\n",
      "epoch:0 step:147 [D loss: 0.011918, acc.: 100.00%] [G loss: 4.299282]\n",
      "epoch:0 step:148 [D loss: 0.018674, acc.: 100.00%] [G loss: 5.397935]\n",
      "epoch:0 step:149 [D loss: 0.049181, acc.: 98.44%] [G loss: 5.396243]\n",
      "epoch:0 step:150 [D loss: 0.027326, acc.: 99.22%] [G loss: 5.152089]\n",
      "epoch:0 step:151 [D loss: 0.071674, acc.: 96.88%] [G loss: 7.631478]\n",
      "epoch:0 step:152 [D loss: 0.007328, acc.: 100.00%] [G loss: 6.688816]\n",
      "epoch:0 step:153 [D loss: 0.227622, acc.: 95.31%] [G loss: 4.575451]\n",
      "epoch:0 step:154 [D loss: 0.388896, acc.: 84.38%] [G loss: 8.914212]\n",
      "epoch:0 step:155 [D loss: 0.355528, acc.: 90.62%] [G loss: 9.859106]\n",
      "epoch:0 step:156 [D loss: 0.336808, acc.: 89.06%] [G loss: 6.295681]\n",
      "epoch:0 step:157 [D loss: 0.209930, acc.: 92.97%] [G loss: 5.533542]\n",
      "epoch:0 step:158 [D loss: 0.274119, acc.: 82.03%] [G loss: 9.418912]\n",
      "epoch:0 step:159 [D loss: 0.093667, acc.: 95.31%] [G loss: 6.967283]\n",
      "epoch:0 step:160 [D loss: 0.532056, acc.: 85.94%] [G loss: 7.059680]\n",
      "epoch:0 step:161 [D loss: 0.777578, acc.: 71.88%] [G loss: 4.954158]\n",
      "epoch:0 step:162 [D loss: 0.697368, acc.: 68.75%] [G loss: 11.805831]\n",
      "epoch:0 step:163 [D loss: 0.622790, acc.: 84.38%] [G loss: 8.180393]\n",
      "epoch:0 step:164 [D loss: 0.625755, acc.: 87.50%] [G loss: 6.203476]\n",
      "epoch:0 step:165 [D loss: 0.409749, acc.: 85.16%] [G loss: 9.379548]\n",
      "epoch:0 step:166 [D loss: 0.353876, acc.: 85.16%] [G loss: 10.468177]\n",
      "epoch:0 step:167 [D loss: 0.170771, acc.: 89.84%] [G loss: 9.337039]\n",
      "epoch:0 step:168 [D loss: 0.430255, acc.: 83.59%] [G loss: 6.653905]\n",
      "epoch:0 step:169 [D loss: 0.180827, acc.: 88.28%] [G loss: 7.508889]\n",
      "epoch:0 step:170 [D loss: 0.208682, acc.: 92.97%] [G loss: 6.917199]\n",
      "epoch:0 step:171 [D loss: 0.308148, acc.: 86.72%] [G loss: 6.721356]\n",
      "epoch:0 step:172 [D loss: 0.265092, acc.: 87.50%] [G loss: 6.114760]\n",
      "epoch:0 step:173 [D loss: 0.137605, acc.: 92.97%] [G loss: 8.101757]\n",
      "epoch:0 step:174 [D loss: 0.328636, acc.: 92.97%] [G loss: 5.146390]\n",
      "epoch:0 step:175 [D loss: 0.179990, acc.: 92.97%] [G loss: 5.054908]\n",
      "epoch:0 step:176 [D loss: 0.012247, acc.: 100.00%] [G loss: 5.442555]\n",
      "epoch:0 step:177 [D loss: 0.317498, acc.: 92.97%] [G loss: 8.481071]\n",
      "epoch:0 step:178 [D loss: 0.239106, acc.: 92.19%] [G loss: 8.411592]\n",
      "epoch:0 step:179 [D loss: 0.246255, acc.: 92.19%] [G loss: 7.579080]\n",
      "epoch:0 step:180 [D loss: 0.015094, acc.: 100.00%] [G loss: 6.790299]\n",
      "epoch:0 step:181 [D loss: 0.400277, acc.: 80.47%] [G loss: 7.047594]\n",
      "epoch:0 step:182 [D loss: 0.082583, acc.: 95.31%] [G loss: 8.838649]\n",
      "epoch:0 step:183 [D loss: 0.413132, acc.: 82.81%] [G loss: 8.292542]\n",
      "epoch:0 step:184 [D loss: 0.349312, acc.: 85.16%] [G loss: 5.429749]\n",
      "epoch:0 step:185 [D loss: 0.949013, acc.: 70.31%] [G loss: 10.878496]\n",
      "epoch:0 step:186 [D loss: 0.837829, acc.: 70.31%] [G loss: 7.704735]\n",
      "epoch:0 step:187 [D loss: 0.207179, acc.: 93.75%] [G loss: 8.389754]\n",
      "epoch:0 step:188 [D loss: 0.328222, acc.: 83.59%] [G loss: 6.860044]\n",
      "epoch:0 step:189 [D loss: 0.087555, acc.: 97.66%] [G loss: 5.185382]\n",
      "epoch:0 step:190 [D loss: 0.169028, acc.: 92.97%] [G loss: 6.111138]\n",
      "epoch:0 step:191 [D loss: 0.520551, acc.: 92.19%] [G loss: 5.127873]\n",
      "epoch:0 step:192 [D loss: 0.130384, acc.: 93.75%] [G loss: 6.352588]\n",
      "epoch:0 step:193 [D loss: 0.220424, acc.: 93.75%] [G loss: 7.476083]\n",
      "epoch:0 step:194 [D loss: 0.655246, acc.: 70.31%] [G loss: 7.440629]\n",
      "epoch:0 step:195 [D loss: 0.099103, acc.: 96.88%] [G loss: 8.486946]\n",
      "epoch:0 step:196 [D loss: 0.237645, acc.: 89.84%] [G loss: 6.553655]\n",
      "epoch:0 step:197 [D loss: 0.243141, acc.: 92.19%] [G loss: 7.699327]\n",
      "epoch:0 step:198 [D loss: 0.881219, acc.: 56.25%] [G loss: 7.387882]\n",
      "epoch:0 step:199 [D loss: 0.446840, acc.: 87.50%] [G loss: 7.514899]\n",
      "epoch:0 step:200 [D loss: 0.420484, acc.: 82.81%] [G loss: 9.899870]\n",
      "epoch:0 step:201 [D loss: 0.285581, acc.: 89.06%] [G loss: 8.703635]\n",
      "epoch:0 step:202 [D loss: 0.184093, acc.: 96.09%] [G loss: 3.628939]\n",
      "epoch:0 step:203 [D loss: 0.064080, acc.: 99.22%] [G loss: 3.670690]\n",
      "epoch:0 step:204 [D loss: 0.274289, acc.: 92.19%] [G loss: 5.619258]\n",
      "epoch:0 step:205 [D loss: 0.108688, acc.: 96.88%] [G loss: 6.123141]\n",
      "epoch:0 step:206 [D loss: 0.309837, acc.: 85.94%] [G loss: 5.935866]\n",
      "epoch:0 step:207 [D loss: 0.124757, acc.: 94.53%] [G loss: 6.442292]\n",
      "epoch:0 step:208 [D loss: 0.397236, acc.: 75.78%] [G loss: 10.594952]\n",
      "epoch:0 step:209 [D loss: 1.545005, acc.: 53.12%] [G loss: 4.707094]\n",
      "epoch:0 step:210 [D loss: 1.282129, acc.: 69.53%] [G loss: 9.798775]\n",
      "epoch:0 step:211 [D loss: 0.549405, acc.: 84.38%] [G loss: 7.804634]\n",
      "epoch:0 step:212 [D loss: 1.036873, acc.: 75.00%] [G loss: 5.861762]\n",
      "epoch:0 step:213 [D loss: 0.536644, acc.: 75.00%] [G loss: 10.721887]\n",
      "epoch:0 step:214 [D loss: 0.339994, acc.: 85.16%] [G loss: 7.062968]\n",
      "epoch:0 step:215 [D loss: 0.234228, acc.: 92.19%] [G loss: 2.492051]\n",
      "epoch:0 step:216 [D loss: 0.941651, acc.: 62.50%] [G loss: 9.707732]\n",
      "epoch:0 step:217 [D loss: 1.481338, acc.: 63.28%] [G loss: 7.575165]\n",
      "epoch:0 step:218 [D loss: 0.359669, acc.: 80.47%] [G loss: 2.584942]\n",
      "epoch:0 step:219 [D loss: 0.567260, acc.: 73.44%] [G loss: 5.810179]\n",
      "epoch:0 step:220 [D loss: 0.156622, acc.: 95.31%] [G loss: 6.918557]\n",
      "epoch:0 step:221 [D loss: 0.510848, acc.: 75.78%] [G loss: 3.507152]\n",
      "epoch:0 step:222 [D loss: 0.866265, acc.: 60.94%] [G loss: 8.616732]\n",
      "epoch:0 step:223 [D loss: 0.308123, acc.: 83.59%] [G loss: 9.993790]\n",
      "epoch:0 step:224 [D loss: 0.730389, acc.: 73.44%] [G loss: 8.459024]\n",
      "epoch:0 step:225 [D loss: 0.218500, acc.: 90.62%] [G loss: 5.904202]\n",
      "epoch:0 step:226 [D loss: 0.137513, acc.: 95.31%] [G loss: 2.629571]\n",
      "epoch:0 step:227 [D loss: 0.639269, acc.: 78.91%] [G loss: 7.382628]\n",
      "epoch:0 step:228 [D loss: 0.391472, acc.: 89.06%] [G loss: 9.038322]\n",
      "epoch:0 step:229 [D loss: 0.142389, acc.: 93.75%] [G loss: 7.794145]\n",
      "epoch:0 step:230 [D loss: 0.089786, acc.: 98.44%] [G loss: 5.887972]\n",
      "epoch:0 step:231 [D loss: 0.015210, acc.: 100.00%] [G loss: 4.848500]\n",
      "epoch:0 step:232 [D loss: 0.072208, acc.: 98.44%] [G loss: 5.330842]\n",
      "epoch:0 step:233 [D loss: 0.121285, acc.: 95.31%] [G loss: 8.643550]\n",
      "epoch:0 step:234 [D loss: 0.201684, acc.: 95.31%] [G loss: 5.063386]\n",
      "epoch:0 step:235 [D loss: 0.791138, acc.: 75.00%] [G loss: 9.449413]\n",
      "epoch:0 step:236 [D loss: 0.276905, acc.: 88.28%] [G loss: 7.087455]\n",
      "epoch:0 step:237 [D loss: 0.601767, acc.: 88.28%] [G loss: 2.016820]\n",
      "epoch:0 step:238 [D loss: 1.259681, acc.: 52.34%] [G loss: 10.137295]\n",
      "epoch:0 step:239 [D loss: 1.449406, acc.: 58.59%] [G loss: 8.310740]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:240 [D loss: 0.428307, acc.: 80.47%] [G loss: 5.015247]\n",
      "epoch:0 step:241 [D loss: 0.426103, acc.: 88.28%] [G loss: 3.368999]\n",
      "epoch:0 step:242 [D loss: 0.132158, acc.: 95.31%] [G loss: 6.517032]\n",
      "epoch:0 step:243 [D loss: 0.369937, acc.: 86.72%] [G loss: 7.510191]\n",
      "epoch:0 step:244 [D loss: 0.273810, acc.: 89.84%] [G loss: 4.918363]\n",
      "epoch:0 step:245 [D loss: 0.453527, acc.: 89.84%] [G loss: 6.511223]\n",
      "epoch:0 step:246 [D loss: 0.172521, acc.: 95.31%] [G loss: 4.701315]\n",
      "epoch:0 step:247 [D loss: 1.675063, acc.: 38.28%] [G loss: 6.171109]\n",
      "epoch:0 step:248 [D loss: 0.043815, acc.: 97.66%] [G loss: 7.996343]\n",
      "epoch:0 step:249 [D loss: 0.775684, acc.: 69.53%] [G loss: 3.204581]\n",
      "epoch:0 step:250 [D loss: 0.149400, acc.: 96.88%] [G loss: 2.721369]\n",
      "epoch:0 step:251 [D loss: 0.164235, acc.: 91.41%] [G loss: 2.295335]\n",
      "epoch:0 step:252 [D loss: 1.459327, acc.: 52.34%] [G loss: 7.574945]\n",
      "epoch:0 step:253 [D loss: 0.370284, acc.: 83.59%] [G loss: 10.219322]\n",
      "epoch:0 step:254 [D loss: 0.924099, acc.: 58.59%] [G loss: 2.625714]\n",
      "epoch:0 step:255 [D loss: 0.289187, acc.: 83.59%] [G loss: 6.315452]\n",
      "epoch:0 step:256 [D loss: 0.682247, acc.: 70.31%] [G loss: 7.284876]\n",
      "epoch:0 step:257 [D loss: 0.333646, acc.: 89.84%] [G loss: 4.606589]\n",
      "epoch:0 step:258 [D loss: 0.408041, acc.: 82.03%] [G loss: 5.041446]\n",
      "epoch:0 step:259 [D loss: 1.100132, acc.: 44.53%] [G loss: 6.489377]\n",
      "epoch:0 step:260 [D loss: 0.885640, acc.: 60.16%] [G loss: 6.052817]\n",
      "epoch:0 step:261 [D loss: 1.076790, acc.: 66.41%] [G loss: 6.355306]\n",
      "epoch:0 step:262 [D loss: 0.455282, acc.: 83.59%] [G loss: 4.545116]\n",
      "epoch:0 step:263 [D loss: 0.546745, acc.: 77.34%] [G loss: 6.473516]\n",
      "epoch:0 step:264 [D loss: 0.878682, acc.: 66.41%] [G loss: 3.830613]\n",
      "epoch:0 step:265 [D loss: 0.533678, acc.: 76.56%] [G loss: 5.746652]\n",
      "epoch:0 step:266 [D loss: 0.649271, acc.: 63.28%] [G loss: 5.118695]\n",
      "epoch:0 step:267 [D loss: 0.543214, acc.: 79.69%] [G loss: 5.507606]\n",
      "epoch:0 step:268 [D loss: 0.617247, acc.: 71.09%] [G loss: 4.478098]\n",
      "epoch:0 step:269 [D loss: 0.640356, acc.: 71.88%] [G loss: 7.114861]\n",
      "epoch:0 step:270 [D loss: 0.918145, acc.: 60.16%] [G loss: 5.929007]\n",
      "epoch:0 step:271 [D loss: 0.510859, acc.: 72.66%] [G loss: 3.458444]\n",
      "epoch:0 step:272 [D loss: 1.538307, acc.: 40.62%] [G loss: 4.185548]\n",
      "epoch:0 step:273 [D loss: 1.003552, acc.: 55.47%] [G loss: 3.340667]\n",
      "epoch:0 step:274 [D loss: 0.995455, acc.: 59.38%] [G loss: 4.601977]\n",
      "epoch:0 step:275 [D loss: 0.921991, acc.: 56.25%] [G loss: 4.366526]\n",
      "epoch:0 step:276 [D loss: 0.702233, acc.: 65.62%] [G loss: 2.903220]\n",
      "epoch:0 step:277 [D loss: 0.830487, acc.: 63.28%] [G loss: 2.535543]\n",
      "epoch:0 step:278 [D loss: 1.010845, acc.: 51.56%] [G loss: 6.484776]\n",
      "epoch:0 step:279 [D loss: 1.135413, acc.: 64.06%] [G loss: 4.848897]\n",
      "epoch:0 step:280 [D loss: 0.722513, acc.: 67.19%] [G loss: 4.283266]\n",
      "epoch:0 step:281 [D loss: 0.830636, acc.: 60.16%] [G loss: 4.586999]\n",
      "epoch:0 step:282 [D loss: 1.774333, acc.: 23.44%] [G loss: 3.566453]\n",
      "epoch:0 step:283 [D loss: 0.827695, acc.: 64.06%] [G loss: 3.765914]\n",
      "epoch:0 step:284 [D loss: 0.155436, acc.: 95.31%] [G loss: 2.788490]\n",
      "epoch:0 step:285 [D loss: 1.000440, acc.: 47.66%] [G loss: 4.138558]\n",
      "epoch:0 step:286 [D loss: 0.616271, acc.: 68.75%] [G loss: 2.939232]\n",
      "epoch:0 step:287 [D loss: 0.554278, acc.: 74.22%] [G loss: 4.743359]\n",
      "epoch:0 step:288 [D loss: 1.064234, acc.: 46.88%] [G loss: 3.809783]\n",
      "epoch:0 step:289 [D loss: 0.672251, acc.: 60.94%] [G loss: 3.952453]\n",
      "epoch:0 step:290 [D loss: 0.994768, acc.: 51.56%] [G loss: 3.614895]\n",
      "epoch:0 step:291 [D loss: 0.902773, acc.: 54.69%] [G loss: 4.542850]\n",
      "epoch:0 step:292 [D loss: 0.570070, acc.: 74.22%] [G loss: 4.474044]\n",
      "epoch:0 step:293 [D loss: 0.702871, acc.: 67.19%] [G loss: 2.984197]\n",
      "epoch:0 step:294 [D loss: 0.495064, acc.: 82.81%] [G loss: 3.272576]\n",
      "epoch:0 step:295 [D loss: 0.510767, acc.: 70.31%] [G loss: 4.886116]\n",
      "epoch:0 step:296 [D loss: 0.594475, acc.: 75.78%] [G loss: 2.466853]\n",
      "epoch:0 step:297 [D loss: 0.577848, acc.: 72.66%] [G loss: 2.643701]\n",
      "epoch:0 step:298 [D loss: 0.391847, acc.: 76.56%] [G loss: 5.245008]\n",
      "epoch:0 step:299 [D loss: 1.071212, acc.: 47.66%] [G loss: 4.671835]\n",
      "epoch:0 step:300 [D loss: 0.549844, acc.: 75.00%] [G loss: 3.138734]\n",
      "epoch:0 step:301 [D loss: 1.306387, acc.: 35.16%] [G loss: 2.722342]\n",
      "epoch:0 step:302 [D loss: 0.682008, acc.: 62.50%] [G loss: 3.346138]\n",
      "epoch:0 step:303 [D loss: 0.830980, acc.: 53.12%] [G loss: 3.490081]\n",
      "epoch:0 step:304 [D loss: 1.170318, acc.: 46.09%] [G loss: 5.666327]\n",
      "epoch:0 step:305 [D loss: 0.624842, acc.: 75.78%] [G loss: 5.735523]\n",
      "epoch:0 step:306 [D loss: 1.047289, acc.: 53.12%] [G loss: 2.856968]\n",
      "epoch:0 step:307 [D loss: 0.598090, acc.: 71.09%] [G loss: 2.358365]\n",
      "epoch:0 step:308 [D loss: 0.631884, acc.: 68.75%] [G loss: 2.780654]\n",
      "epoch:0 step:309 [D loss: 0.389528, acc.: 85.94%] [G loss: 3.505908]\n",
      "epoch:0 step:310 [D loss: 0.441172, acc.: 81.25%] [G loss: 2.470249]\n",
      "epoch:0 step:311 [D loss: 0.820471, acc.: 61.72%] [G loss: 2.364272]\n",
      "epoch:0 step:312 [D loss: 0.577637, acc.: 67.97%] [G loss: 3.197058]\n",
      "epoch:0 step:313 [D loss: 1.207398, acc.: 40.62%] [G loss: 3.742596]\n",
      "epoch:0 step:314 [D loss: 0.486908, acc.: 80.47%] [G loss: 2.653643]\n",
      "epoch:0 step:315 [D loss: 0.834729, acc.: 55.47%] [G loss: 2.664242]\n",
      "epoch:0 step:316 [D loss: 1.414653, acc.: 24.22%] [G loss: 3.100637]\n",
      "epoch:0 step:317 [D loss: 1.104541, acc.: 44.53%] [G loss: 3.417756]\n",
      "epoch:0 step:318 [D loss: 1.052434, acc.: 46.09%] [G loss: 2.188236]\n",
      "epoch:0 step:319 [D loss: 0.416120, acc.: 81.25%] [G loss: 2.178585]\n",
      "epoch:0 step:320 [D loss: 0.847850, acc.: 50.00%] [G loss: 2.026158]\n",
      "epoch:0 step:321 [D loss: 0.752443, acc.: 64.06%] [G loss: 3.343826]\n",
      "epoch:0 step:322 [D loss: 1.033857, acc.: 52.34%] [G loss: 2.833744]\n",
      "epoch:0 step:323 [D loss: 1.394741, acc.: 29.69%] [G loss: 2.443218]\n",
      "epoch:0 step:324 [D loss: 0.863141, acc.: 58.59%] [G loss: 1.942235]\n",
      "epoch:0 step:325 [D loss: 0.616966, acc.: 63.28%] [G loss: 2.339947]\n",
      "epoch:0 step:326 [D loss: 1.044828, acc.: 47.66%] [G loss: 1.191043]\n",
      "epoch:0 step:327 [D loss: 0.472317, acc.: 73.44%] [G loss: 3.100592]\n",
      "epoch:0 step:328 [D loss: 0.586600, acc.: 74.22%] [G loss: 1.828142]\n",
      "epoch:0 step:329 [D loss: 0.707373, acc.: 55.47%] [G loss: 3.072226]\n",
      "epoch:0 step:330 [D loss: 0.801681, acc.: 50.00%] [G loss: 3.499578]\n",
      "epoch:0 step:331 [D loss: 0.731222, acc.: 57.81%] [G loss: 2.335600]\n",
      "epoch:0 step:332 [D loss: 0.446688, acc.: 82.81%] [G loss: 2.070085]\n",
      "epoch:0 step:333 [D loss: 0.901654, acc.: 41.41%] [G loss: 2.709672]\n",
      "epoch:0 step:334 [D loss: 0.473259, acc.: 79.69%] [G loss: 1.877483]\n",
      "epoch:0 step:335 [D loss: 0.995565, acc.: 40.62%] [G loss: 1.669480]\n",
      "epoch:0 step:336 [D loss: 0.533380, acc.: 70.31%] [G loss: 1.838705]\n",
      "epoch:0 step:337 [D loss: 0.623284, acc.: 59.38%] [G loss: 2.564655]\n",
      "epoch:0 step:338 [D loss: 0.766699, acc.: 60.16%] [G loss: 2.389063]\n",
      "epoch:0 step:339 [D loss: 0.898892, acc.: 50.00%] [G loss: 2.064870]\n",
      "epoch:0 step:340 [D loss: 0.516437, acc.: 75.00%] [G loss: 1.786243]\n",
      "epoch:0 step:341 [D loss: 0.441414, acc.: 85.94%] [G loss: 1.286769]\n",
      "epoch:0 step:342 [D loss: 0.654133, acc.: 73.44%] [G loss: 1.917135]\n",
      "epoch:0 step:343 [D loss: 0.434895, acc.: 77.34%] [G loss: 2.803642]\n",
      "epoch:0 step:344 [D loss: 0.541070, acc.: 71.09%] [G loss: 2.440735]\n",
      "epoch:0 step:345 [D loss: 0.774459, acc.: 54.69%] [G loss: 2.860900]\n",
      "epoch:0 step:346 [D loss: 0.806000, acc.: 57.81%] [G loss: 1.595644]\n",
      "epoch:0 step:347 [D loss: 0.890998, acc.: 57.03%] [G loss: 2.072989]\n",
      "epoch:0 step:348 [D loss: 0.472631, acc.: 75.78%] [G loss: 2.414096]\n",
      "epoch:0 step:349 [D loss: 1.073180, acc.: 45.31%] [G loss: 2.126414]\n",
      "epoch:0 step:350 [D loss: 0.975823, acc.: 50.78%] [G loss: 1.140769]\n",
      "epoch:0 step:351 [D loss: 0.585747, acc.: 64.84%] [G loss: 2.565318]\n",
      "epoch:0 step:352 [D loss: 0.669903, acc.: 60.16%] [G loss: 2.573507]\n",
      "epoch:0 step:353 [D loss: 0.464466, acc.: 76.56%] [G loss: 3.193998]\n",
      "epoch:0 step:354 [D loss: 0.572257, acc.: 69.53%] [G loss: 3.528063]\n",
      "epoch:0 step:355 [D loss: 0.278877, acc.: 87.50%] [G loss: 3.527642]\n",
      "epoch:0 step:356 [D loss: 0.501983, acc.: 76.56%] [G loss: 3.068071]\n",
      "epoch:0 step:357 [D loss: 0.881538, acc.: 60.16%] [G loss: 2.157201]\n",
      "epoch:0 step:358 [D loss: 0.972436, acc.: 47.66%] [G loss: 2.693162]\n",
      "epoch:0 step:359 [D loss: 0.980875, acc.: 49.22%] [G loss: 1.030481]\n",
      "epoch:0 step:360 [D loss: 0.770385, acc.: 53.91%] [G loss: 1.968125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:361 [D loss: 0.831313, acc.: 53.12%] [G loss: 1.498783]\n",
      "epoch:0 step:362 [D loss: 0.875708, acc.: 46.88%] [G loss: 1.269283]\n",
      "epoch:0 step:363 [D loss: 0.327830, acc.: 93.75%] [G loss: 2.672110]\n",
      "epoch:0 step:364 [D loss: 0.564133, acc.: 67.19%] [G loss: 2.683984]\n",
      "epoch:0 step:365 [D loss: 0.662445, acc.: 60.16%] [G loss: 2.314220]\n",
      "epoch:0 step:366 [D loss: 0.420270, acc.: 78.12%] [G loss: 2.822919]\n",
      "epoch:0 step:367 [D loss: 0.715627, acc.: 59.38%] [G loss: 1.821158]\n",
      "epoch:0 step:368 [D loss: 0.666598, acc.: 60.16%] [G loss: 1.979919]\n",
      "epoch:0 step:369 [D loss: 0.768904, acc.: 53.91%] [G loss: 1.686277]\n",
      "epoch:0 step:370 [D loss: 0.583540, acc.: 65.62%] [G loss: 1.920111]\n",
      "epoch:0 step:371 [D loss: 0.524395, acc.: 74.22%] [G loss: 2.707217]\n",
      "epoch:0 step:372 [D loss: 0.716402, acc.: 64.06%] [G loss: 2.083354]\n",
      "epoch:0 step:373 [D loss: 0.511481, acc.: 74.22%] [G loss: 1.725709]\n",
      "epoch:0 step:374 [D loss: 0.443177, acc.: 80.47%] [G loss: 2.376539]\n",
      "epoch:0 step:375 [D loss: 0.831953, acc.: 48.44%] [G loss: 1.558667]\n",
      "epoch:0 step:376 [D loss: 1.019403, acc.: 36.72%] [G loss: 2.676878]\n",
      "epoch:0 step:377 [D loss: 0.604832, acc.: 61.72%] [G loss: 2.770291]\n",
      "epoch:0 step:378 [D loss: 0.699054, acc.: 61.72%] [G loss: 2.361227]\n",
      "epoch:0 step:379 [D loss: 0.801108, acc.: 52.34%] [G loss: 1.819364]\n",
      "epoch:0 step:380 [D loss: 0.692330, acc.: 62.50%] [G loss: 1.678613]\n",
      "epoch:0 step:381 [D loss: 0.359479, acc.: 80.47%] [G loss: 1.976768]\n",
      "epoch:0 step:382 [D loss: 0.545807, acc.: 70.31%] [G loss: 2.867344]\n",
      "epoch:0 step:383 [D loss: 0.294246, acc.: 93.75%] [G loss: 2.291018]\n",
      "epoch:0 step:384 [D loss: 0.555775, acc.: 74.22%] [G loss: 2.979284]\n",
      "epoch:0 step:385 [D loss: 0.577952, acc.: 67.97%] [G loss: 1.649465]\n",
      "epoch:0 step:386 [D loss: 0.622517, acc.: 63.28%] [G loss: 1.658966]\n",
      "epoch:0 step:387 [D loss: 0.279195, acc.: 92.97%] [G loss: 1.691134]\n",
      "epoch:0 step:388 [D loss: 0.295540, acc.: 90.62%] [G loss: 1.935695]\n",
      "epoch:0 step:389 [D loss: 0.418596, acc.: 74.22%] [G loss: 2.701636]\n",
      "epoch:0 step:390 [D loss: 0.229213, acc.: 90.62%] [G loss: 3.070261]\n",
      "epoch:0 step:391 [D loss: 0.514990, acc.: 76.56%] [G loss: 1.659204]\n",
      "epoch:0 step:392 [D loss: 0.200299, acc.: 96.09%] [G loss: 2.264502]\n",
      "epoch:0 step:393 [D loss: 0.418937, acc.: 82.03%] [G loss: 2.507896]\n",
      "epoch:0 step:394 [D loss: 0.671410, acc.: 64.06%] [G loss: 2.889244]\n",
      "epoch:0 step:395 [D loss: 0.416061, acc.: 82.03%] [G loss: 3.051349]\n",
      "epoch:0 step:396 [D loss: 0.957077, acc.: 42.19%] [G loss: 2.909777]\n",
      "epoch:0 step:397 [D loss: 1.321291, acc.: 39.84%] [G loss: 1.852256]\n",
      "epoch:0 step:398 [D loss: 0.921916, acc.: 50.78%] [G loss: 1.433509]\n",
      "epoch:0 step:399 [D loss: 0.687331, acc.: 57.03%] [G loss: 1.659773]\n",
      "epoch:0 step:400 [D loss: 0.650492, acc.: 66.41%] [G loss: 3.051472]\n",
      "epoch:0 step:401 [D loss: 0.852597, acc.: 67.19%] [G loss: 1.405169]\n",
      "epoch:0 step:402 [D loss: 0.285507, acc.: 89.06%] [G loss: 2.132225]\n",
      "epoch:0 step:403 [D loss: 0.762488, acc.: 60.94%] [G loss: 1.321094]\n",
      "epoch:0 step:404 [D loss: 0.681701, acc.: 63.28%] [G loss: 1.298889]\n",
      "epoch:0 step:405 [D loss: 0.472668, acc.: 79.69%] [G loss: 2.302792]\n",
      "epoch:0 step:406 [D loss: 0.394137, acc.: 86.72%] [G loss: 1.832552]\n",
      "epoch:0 step:407 [D loss: 0.612296, acc.: 64.84%] [G loss: 1.814693]\n",
      "epoch:0 step:408 [D loss: 0.675309, acc.: 62.50%] [G loss: 3.084011]\n",
      "epoch:0 step:409 [D loss: 0.627956, acc.: 67.97%] [G loss: 2.046806]\n",
      "epoch:0 step:410 [D loss: 0.920106, acc.: 48.44%] [G loss: 1.256517]\n",
      "epoch:0 step:411 [D loss: 0.701898, acc.: 58.59%] [G loss: 2.631104]\n",
      "epoch:0 step:412 [D loss: 0.398502, acc.: 82.81%] [G loss: 2.639739]\n",
      "epoch:0 step:413 [D loss: 1.131127, acc.: 32.03%] [G loss: 2.281097]\n",
      "epoch:0 step:414 [D loss: 0.484986, acc.: 79.69%] [G loss: 3.218071]\n",
      "epoch:0 step:415 [D loss: 0.805071, acc.: 52.34%] [G loss: 2.401468]\n",
      "epoch:0 step:416 [D loss: 0.588862, acc.: 66.41%] [G loss: 2.352214]\n",
      "epoch:0 step:417 [D loss: 0.915739, acc.: 48.44%] [G loss: 1.895053]\n",
      "epoch:0 step:418 [D loss: 0.974166, acc.: 41.41%] [G loss: 1.911337]\n",
      "epoch:0 step:419 [D loss: 0.985029, acc.: 42.97%] [G loss: 1.480376]\n",
      "epoch:0 step:420 [D loss: 0.672265, acc.: 60.16%] [G loss: 1.503123]\n",
      "epoch:0 step:421 [D loss: 0.742823, acc.: 50.78%] [G loss: 1.644349]\n",
      "epoch:0 step:422 [D loss: 0.497650, acc.: 77.34%] [G loss: 2.788146]\n",
      "epoch:0 step:423 [D loss: 0.620080, acc.: 64.06%] [G loss: 2.422114]\n",
      "epoch:0 step:424 [D loss: 0.744171, acc.: 56.25%] [G loss: 1.913022]\n",
      "epoch:0 step:425 [D loss: 0.876564, acc.: 52.34%] [G loss: 2.055124]\n",
      "epoch:0 step:426 [D loss: 0.637284, acc.: 68.75%] [G loss: 1.921969]\n",
      "epoch:0 step:427 [D loss: 0.437601, acc.: 82.03%] [G loss: 1.831480]\n",
      "epoch:0 step:428 [D loss: 0.843778, acc.: 49.22%] [G loss: 1.986823]\n",
      "epoch:0 step:429 [D loss: 0.634134, acc.: 63.28%] [G loss: 1.874762]\n",
      "epoch:0 step:430 [D loss: 0.564081, acc.: 69.53%] [G loss: 1.606566]\n",
      "epoch:0 step:431 [D loss: 0.857985, acc.: 38.28%] [G loss: 2.530550]\n",
      "epoch:0 step:432 [D loss: 0.605619, acc.: 64.84%] [G loss: 2.236469]\n",
      "epoch:0 step:433 [D loss: 0.580935, acc.: 75.78%] [G loss: 2.035670]\n",
      "epoch:0 step:434 [D loss: 0.761423, acc.: 56.25%] [G loss: 1.871506]\n",
      "epoch:0 step:435 [D loss: 0.543811, acc.: 73.44%] [G loss: 1.699716]\n",
      "epoch:0 step:436 [D loss: 0.648520, acc.: 63.28%] [G loss: 0.817586]\n",
      "epoch:0 step:437 [D loss: 0.820912, acc.: 52.34%] [G loss: 1.085791]\n",
      "epoch:0 step:438 [D loss: 0.544588, acc.: 68.75%] [G loss: 1.959173]\n",
      "epoch:0 step:439 [D loss: 0.460122, acc.: 76.56%] [G loss: 2.584367]\n",
      "epoch:0 step:440 [D loss: 0.479600, acc.: 82.03%] [G loss: 1.646503]\n",
      "epoch:0 step:441 [D loss: 0.523077, acc.: 75.78%] [G loss: 1.029590]\n",
      "epoch:0 step:442 [D loss: 0.697634, acc.: 62.50%] [G loss: 2.110605]\n",
      "epoch:0 step:443 [D loss: 0.377232, acc.: 80.47%] [G loss: 2.800347]\n",
      "epoch:0 step:444 [D loss: 0.564239, acc.: 69.53%] [G loss: 2.162105]\n",
      "epoch:0 step:445 [D loss: 0.478485, acc.: 79.69%] [G loss: 1.725532]\n",
      "epoch:0 step:446 [D loss: 0.588563, acc.: 70.31%] [G loss: 1.569133]\n",
      "epoch:0 step:447 [D loss: 0.738791, acc.: 64.84%] [G loss: 2.324250]\n",
      "epoch:0 step:448 [D loss: 0.414611, acc.: 82.03%] [G loss: 2.332015]\n",
      "epoch:0 step:449 [D loss: 0.393353, acc.: 84.38%] [G loss: 1.591662]\n",
      "epoch:0 step:450 [D loss: 1.032411, acc.: 41.41%] [G loss: 2.189278]\n",
      "epoch:0 step:451 [D loss: 0.358339, acc.: 84.38%] [G loss: 1.886077]\n",
      "epoch:0 step:452 [D loss: 0.752443, acc.: 53.12%] [G loss: 1.853790]\n",
      "epoch:0 step:453 [D loss: 0.679815, acc.: 69.53%] [G loss: 1.178469]\n",
      "epoch:0 step:454 [D loss: 0.562577, acc.: 68.75%] [G loss: 1.705816]\n",
      "epoch:0 step:455 [D loss: 0.558417, acc.: 71.88%] [G loss: 1.603856]\n",
      "epoch:0 step:456 [D loss: 0.532579, acc.: 73.44%] [G loss: 1.783968]\n",
      "epoch:0 step:457 [D loss: 0.623206, acc.: 67.97%] [G loss: 2.394169]\n",
      "epoch:0 step:458 [D loss: 0.704031, acc.: 57.03%] [G loss: 1.482969]\n",
      "epoch:0 step:459 [D loss: 0.533290, acc.: 74.22%] [G loss: 1.676625]\n",
      "epoch:0 step:460 [D loss: 0.556150, acc.: 71.88%] [G loss: 1.251450]\n",
      "epoch:0 step:461 [D loss: 0.908001, acc.: 39.06%] [G loss: 1.466516]\n",
      "epoch:0 step:462 [D loss: 0.529206, acc.: 66.41%] [G loss: 1.360513]\n",
      "epoch:0 step:463 [D loss: 1.179365, acc.: 20.31%] [G loss: 2.051910]\n",
      "epoch:0 step:464 [D loss: 0.631864, acc.: 65.62%] [G loss: 1.853107]\n",
      "epoch:0 step:465 [D loss: 0.612260, acc.: 65.62%] [G loss: 2.101836]\n",
      "epoch:0 step:466 [D loss: 0.695983, acc.: 60.94%] [G loss: 1.527057]\n",
      "epoch:0 step:467 [D loss: 0.476547, acc.: 75.00%] [G loss: 1.233961]\n",
      "epoch:0 step:468 [D loss: 0.496061, acc.: 81.25%] [G loss: 1.950547]\n",
      "epoch:0 step:469 [D loss: 0.374893, acc.: 82.81%] [G loss: 1.948052]\n",
      "epoch:0 step:470 [D loss: 0.366634, acc.: 89.06%] [G loss: 1.933746]\n",
      "epoch:0 step:471 [D loss: 0.482425, acc.: 80.47%] [G loss: 1.924283]\n",
      "epoch:0 step:472 [D loss: 0.543341, acc.: 73.44%] [G loss: 1.666941]\n",
      "epoch:0 step:473 [D loss: 1.075952, acc.: 38.28%] [G loss: 1.678476]\n",
      "epoch:0 step:474 [D loss: 0.534640, acc.: 78.12%] [G loss: 1.809461]\n",
      "epoch:0 step:475 [D loss: 0.618133, acc.: 71.09%] [G loss: 1.877752]\n",
      "epoch:0 step:476 [D loss: 0.651837, acc.: 55.47%] [G loss: 1.723482]\n",
      "epoch:0 step:477 [D loss: 0.671026, acc.: 60.94%] [G loss: 2.934314]\n",
      "epoch:0 step:478 [D loss: 0.512475, acc.: 75.78%] [G loss: 1.627017]\n",
      "epoch:0 step:479 [D loss: 0.985852, acc.: 47.66%] [G loss: 1.875983]\n",
      "epoch:0 step:480 [D loss: 0.504780, acc.: 74.22%] [G loss: 1.542954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:481 [D loss: 0.995507, acc.: 48.44%] [G loss: 2.109355]\n",
      "epoch:0 step:482 [D loss: 0.692626, acc.: 58.59%] [G loss: 2.019876]\n",
      "epoch:0 step:483 [D loss: 0.911083, acc.: 50.00%] [G loss: 1.630201]\n",
      "epoch:0 step:484 [D loss: 0.804689, acc.: 60.16%] [G loss: 1.714569]\n",
      "epoch:0 step:485 [D loss: 0.880788, acc.: 50.00%] [G loss: 1.727852]\n",
      "epoch:0 step:486 [D loss: 0.662722, acc.: 63.28%] [G loss: 1.244815]\n",
      "epoch:0 step:487 [D loss: 0.720250, acc.: 60.16%] [G loss: 1.057686]\n",
      "epoch:0 step:488 [D loss: 0.616549, acc.: 60.16%] [G loss: 1.508006]\n",
      "epoch:0 step:489 [D loss: 0.682884, acc.: 60.16%] [G loss: 1.503073]\n",
      "epoch:0 step:490 [D loss: 0.862372, acc.: 51.56%] [G loss: 2.251597]\n",
      "epoch:0 step:491 [D loss: 0.546809, acc.: 74.22%] [G loss: 1.257450]\n",
      "epoch:0 step:492 [D loss: 0.884354, acc.: 41.41%] [G loss: 1.871850]\n",
      "epoch:0 step:493 [D loss: 0.699176, acc.: 63.28%] [G loss: 0.963470]\n",
      "epoch:0 step:494 [D loss: 0.615618, acc.: 65.62%] [G loss: 1.215912]\n",
      "epoch:0 step:495 [D loss: 0.799320, acc.: 53.12%] [G loss: 1.505268]\n",
      "epoch:0 step:496 [D loss: 0.561444, acc.: 76.56%] [G loss: 2.188128]\n",
      "epoch:0 step:497 [D loss: 0.649369, acc.: 64.06%] [G loss: 1.570538]\n",
      "epoch:0 step:498 [D loss: 0.777481, acc.: 46.09%] [G loss: 1.659509]\n",
      "epoch:0 step:499 [D loss: 0.613221, acc.: 62.50%] [G loss: 1.839522]\n",
      "epoch:0 step:500 [D loss: 0.943386, acc.: 32.81%] [G loss: 1.519873]\n",
      "epoch:0 step:501 [D loss: 0.731099, acc.: 53.12%] [G loss: 2.399190]\n",
      "epoch:0 step:502 [D loss: 0.684951, acc.: 64.06%] [G loss: 1.000149]\n",
      "epoch:0 step:503 [D loss: 0.612308, acc.: 64.84%] [G loss: 2.048832]\n",
      "epoch:0 step:504 [D loss: 0.675345, acc.: 57.81%] [G loss: 1.384081]\n",
      "epoch:0 step:505 [D loss: 0.609369, acc.: 66.41%] [G loss: 1.495545]\n",
      "epoch:0 step:506 [D loss: 0.732838, acc.: 58.59%] [G loss: 1.292029]\n",
      "epoch:0 step:507 [D loss: 0.886778, acc.: 42.19%] [G loss: 1.936752]\n",
      "epoch:0 step:508 [D loss: 0.599974, acc.: 64.84%] [G loss: 1.803324]\n",
      "epoch:0 step:509 [D loss: 0.962249, acc.: 36.72%] [G loss: 1.625150]\n",
      "epoch:0 step:510 [D loss: 0.676996, acc.: 55.47%] [G loss: 1.828371]\n",
      "epoch:0 step:511 [D loss: 0.623285, acc.: 60.94%] [G loss: 1.898737]\n",
      "epoch:0 step:512 [D loss: 0.687216, acc.: 57.81%] [G loss: 1.742567]\n",
      "epoch:0 step:513 [D loss: 0.604069, acc.: 64.84%] [G loss: 1.036313]\n",
      "epoch:0 step:514 [D loss: 0.692875, acc.: 59.38%] [G loss: 1.536109]\n",
      "epoch:0 step:515 [D loss: 0.773292, acc.: 55.47%] [G loss: 1.239383]\n",
      "epoch:0 step:516 [D loss: 0.549308, acc.: 69.53%] [G loss: 1.653370]\n",
      "epoch:0 step:517 [D loss: 0.677820, acc.: 60.16%] [G loss: 1.582295]\n",
      "epoch:0 step:518 [D loss: 0.624691, acc.: 61.72%] [G loss: 1.604397]\n",
      "epoch:0 step:519 [D loss: 0.673944, acc.: 61.72%] [G loss: 1.913325]\n",
      "epoch:0 step:520 [D loss: 0.589808, acc.: 68.75%] [G loss: 1.649170]\n",
      "epoch:0 step:521 [D loss: 0.696634, acc.: 60.16%] [G loss: 1.193829]\n",
      "epoch:0 step:522 [D loss: 0.812495, acc.: 48.44%] [G loss: 1.237106]\n",
      "epoch:0 step:523 [D loss: 0.829189, acc.: 46.09%] [G loss: 1.644214]\n",
      "epoch:0 step:524 [D loss: 0.667606, acc.: 61.72%] [G loss: 1.333695]\n",
      "epoch:0 step:525 [D loss: 0.736759, acc.: 55.47%] [G loss: 1.477001]\n",
      "epoch:0 step:526 [D loss: 0.766025, acc.: 51.56%] [G loss: 1.598045]\n",
      "epoch:0 step:527 [D loss: 0.657472, acc.: 60.16%] [G loss: 1.516948]\n",
      "epoch:0 step:528 [D loss: 0.590556, acc.: 67.19%] [G loss: 1.185970]\n",
      "epoch:0 step:529 [D loss: 0.706810, acc.: 55.47%] [G loss: 1.164049]\n",
      "epoch:0 step:530 [D loss: 0.627695, acc.: 65.62%] [G loss: 1.487101]\n",
      "epoch:0 step:531 [D loss: 0.666506, acc.: 58.59%] [G loss: 1.504902]\n",
      "epoch:0 step:532 [D loss: 0.714492, acc.: 58.59%] [G loss: 1.330863]\n",
      "epoch:0 step:533 [D loss: 0.962039, acc.: 35.94%] [G loss: 1.294554]\n",
      "epoch:0 step:534 [D loss: 0.609259, acc.: 61.72%] [G loss: 1.403706]\n",
      "epoch:0 step:535 [D loss: 0.516515, acc.: 73.44%] [G loss: 1.415852]\n",
      "epoch:0 step:536 [D loss: 0.636328, acc.: 64.06%] [G loss: 1.154592]\n",
      "epoch:0 step:537 [D loss: 0.682174, acc.: 56.25%] [G loss: 1.400899]\n",
      "epoch:0 step:538 [D loss: 0.634233, acc.: 62.50%] [G loss: 1.162318]\n",
      "epoch:0 step:539 [D loss: 0.761385, acc.: 46.88%] [G loss: 1.063329]\n",
      "epoch:0 step:540 [D loss: 0.569414, acc.: 66.41%] [G loss: 1.443601]\n",
      "epoch:0 step:541 [D loss: 0.738693, acc.: 56.25%] [G loss: 1.791762]\n",
      "epoch:0 step:542 [D loss: 0.708556, acc.: 54.69%] [G loss: 1.382436]\n",
      "epoch:0 step:543 [D loss: 0.654493, acc.: 68.75%] [G loss: 1.564069]\n",
      "epoch:0 step:544 [D loss: 0.680295, acc.: 55.47%] [G loss: 1.547997]\n",
      "epoch:0 step:545 [D loss: 0.801374, acc.: 46.88%] [G loss: 1.746824]\n",
      "epoch:0 step:546 [D loss: 0.719379, acc.: 60.16%] [G loss: 1.399316]\n",
      "epoch:0 step:547 [D loss: 0.919602, acc.: 35.16%] [G loss: 1.091481]\n",
      "epoch:0 step:548 [D loss: 0.664220, acc.: 58.59%] [G loss: 1.321823]\n",
      "epoch:0 step:549 [D loss: 0.761071, acc.: 53.91%] [G loss: 1.501602]\n",
      "epoch:0 step:550 [D loss: 0.605903, acc.: 64.06%] [G loss: 2.044802]\n",
      "epoch:0 step:551 [D loss: 0.780718, acc.: 46.09%] [G loss: 1.526625]\n",
      "epoch:0 step:552 [D loss: 0.703478, acc.: 59.38%] [G loss: 1.229099]\n",
      "epoch:0 step:553 [D loss: 0.786331, acc.: 56.25%] [G loss: 1.189563]\n",
      "epoch:0 step:554 [D loss: 0.564600, acc.: 73.44%] [G loss: 1.308787]\n",
      "epoch:0 step:555 [D loss: 0.575450, acc.: 69.53%] [G loss: 1.433374]\n",
      "epoch:0 step:556 [D loss: 0.614788, acc.: 66.41%] [G loss: 1.568964]\n",
      "epoch:0 step:557 [D loss: 0.623948, acc.: 60.94%] [G loss: 1.645136]\n",
      "epoch:0 step:558 [D loss: 0.656434, acc.: 65.62%] [G loss: 1.342058]\n",
      "epoch:0 step:559 [D loss: 0.759863, acc.: 48.44%] [G loss: 1.371658]\n",
      "epoch:0 step:560 [D loss: 0.515417, acc.: 77.34%] [G loss: 1.131930]\n",
      "epoch:0 step:561 [D loss: 0.590225, acc.: 71.09%] [G loss: 1.622330]\n",
      "epoch:0 step:562 [D loss: 0.651188, acc.: 58.59%] [G loss: 1.020357]\n",
      "epoch:0 step:563 [D loss: 0.741076, acc.: 53.91%] [G loss: 1.242520]\n",
      "epoch:0 step:564 [D loss: 0.612347, acc.: 68.75%] [G loss: 1.320650]\n",
      "epoch:0 step:565 [D loss: 0.646189, acc.: 58.59%] [G loss: 1.423054]\n",
      "epoch:0 step:566 [D loss: 0.574556, acc.: 77.34%] [G loss: 1.641447]\n",
      "epoch:0 step:567 [D loss: 0.773012, acc.: 51.56%] [G loss: 1.415163]\n",
      "epoch:0 step:568 [D loss: 0.650283, acc.: 60.94%] [G loss: 1.397502]\n",
      "epoch:0 step:569 [D loss: 0.668504, acc.: 56.25%] [G loss: 1.345946]\n",
      "epoch:0 step:570 [D loss: 0.592933, acc.: 67.97%] [G loss: 1.436912]\n",
      "epoch:0 step:571 [D loss: 0.606698, acc.: 67.19%] [G loss: 1.293636]\n",
      "epoch:0 step:572 [D loss: 0.588501, acc.: 65.62%] [G loss: 1.302854]\n",
      "epoch:0 step:573 [D loss: 0.746541, acc.: 51.56%] [G loss: 1.599861]\n",
      "epoch:0 step:574 [D loss: 0.626686, acc.: 66.41%] [G loss: 1.421887]\n",
      "epoch:0 step:575 [D loss: 0.789081, acc.: 50.00%] [G loss: 1.483203]\n",
      "epoch:0 step:576 [D loss: 0.833648, acc.: 48.44%] [G loss: 1.259197]\n",
      "epoch:0 step:577 [D loss: 0.584951, acc.: 63.28%] [G loss: 1.873352]\n",
      "epoch:0 step:578 [D loss: 0.619500, acc.: 70.31%] [G loss: 1.342189]\n",
      "epoch:0 step:579 [D loss: 0.666044, acc.: 60.16%] [G loss: 1.696618]\n",
      "epoch:0 step:580 [D loss: 0.742265, acc.: 60.16%] [G loss: 2.011936]\n",
      "epoch:0 step:581 [D loss: 0.680273, acc.: 55.47%] [G loss: 2.134560]\n",
      "epoch:0 step:582 [D loss: 0.552981, acc.: 74.22%] [G loss: 2.212949]\n",
      "epoch:0 step:583 [D loss: 0.565259, acc.: 67.97%] [G loss: 1.864939]\n",
      "epoch:0 step:584 [D loss: 0.564922, acc.: 62.50%] [G loss: 1.527579]\n",
      "epoch:0 step:585 [D loss: 0.604658, acc.: 67.97%] [G loss: 1.408983]\n",
      "epoch:0 step:586 [D loss: 0.595697, acc.: 64.06%] [G loss: 1.469907]\n",
      "epoch:0 step:587 [D loss: 0.596608, acc.: 64.06%] [G loss: 1.240412]\n",
      "epoch:0 step:588 [D loss: 0.760156, acc.: 47.66%] [G loss: 1.714293]\n",
      "epoch:0 step:589 [D loss: 0.686534, acc.: 57.03%] [G loss: 1.611215]\n",
      "epoch:0 step:590 [D loss: 0.569342, acc.: 64.84%] [G loss: 1.590572]\n",
      "epoch:0 step:591 [D loss: 0.565498, acc.: 67.19%] [G loss: 1.374315]\n",
      "epoch:0 step:592 [D loss: 0.614609, acc.: 67.97%] [G loss: 1.275760]\n",
      "epoch:0 step:593 [D loss: 0.813613, acc.: 46.09%] [G loss: 1.139307]\n",
      "epoch:0 step:594 [D loss: 0.607578, acc.: 68.75%] [G loss: 1.425287]\n",
      "epoch:0 step:595 [D loss: 0.629742, acc.: 61.72%] [G loss: 1.501122]\n",
      "epoch:0 step:596 [D loss: 0.632447, acc.: 61.72%] [G loss: 1.270141]\n",
      "epoch:0 step:597 [D loss: 0.720372, acc.: 57.81%] [G loss: 1.527272]\n",
      "epoch:0 step:598 [D loss: 0.787672, acc.: 47.66%] [G loss: 1.619280]\n",
      "epoch:0 step:599 [D loss: 0.773994, acc.: 46.09%] [G loss: 1.378183]\n",
      "epoch:0 step:600 [D loss: 0.836321, acc.: 46.09%] [G loss: 1.654304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:601 [D loss: 0.747997, acc.: 56.25%] [G loss: 1.361375]\n",
      "epoch:0 step:602 [D loss: 0.555727, acc.: 74.22%] [G loss: 1.343136]\n",
      "epoch:0 step:603 [D loss: 0.666259, acc.: 60.16%] [G loss: 1.425321]\n",
      "epoch:0 step:604 [D loss: 0.798611, acc.: 47.66%] [G loss: 1.465864]\n",
      "epoch:0 step:605 [D loss: 0.599784, acc.: 67.97%] [G loss: 1.246786]\n",
      "epoch:0 step:606 [D loss: 0.752110, acc.: 54.69%] [G loss: 1.381580]\n",
      "epoch:0 step:607 [D loss: 0.709197, acc.: 59.38%] [G loss: 1.808570]\n",
      "epoch:0 step:608 [D loss: 0.629063, acc.: 67.97%] [G loss: 1.265401]\n",
      "epoch:0 step:609 [D loss: 0.526959, acc.: 77.34%] [G loss: 1.362928]\n",
      "epoch:0 step:610 [D loss: 0.622639, acc.: 64.06%] [G loss: 1.337953]\n",
      "epoch:0 step:611 [D loss: 0.717499, acc.: 56.25%] [G loss: 1.107350]\n",
      "epoch:0 step:612 [D loss: 0.676439, acc.: 60.16%] [G loss: 1.321589]\n",
      "epoch:0 step:613 [D loss: 0.617148, acc.: 66.41%] [G loss: 1.580989]\n",
      "epoch:0 step:614 [D loss: 0.560768, acc.: 73.44%] [G loss: 1.836673]\n",
      "epoch:0 step:615 [D loss: 0.632933, acc.: 64.84%] [G loss: 1.558875]\n",
      "epoch:0 step:616 [D loss: 0.659454, acc.: 62.50%] [G loss: 1.500933]\n",
      "epoch:0 step:617 [D loss: 0.710066, acc.: 56.25%] [G loss: 1.302297]\n",
      "epoch:0 step:618 [D loss: 0.506761, acc.: 76.56%] [G loss: 1.889713]\n",
      "epoch:0 step:619 [D loss: 0.805544, acc.: 48.44%] [G loss: 1.551255]\n",
      "epoch:0 step:620 [D loss: 0.630362, acc.: 60.16%] [G loss: 1.770507]\n",
      "epoch:0 step:621 [D loss: 0.661954, acc.: 58.59%] [G loss: 1.880003]\n",
      "epoch:0 step:622 [D loss: 0.615433, acc.: 64.06%] [G loss: 1.508140]\n",
      "epoch:0 step:623 [D loss: 0.736371, acc.: 49.22%] [G loss: 1.344037]\n",
      "epoch:0 step:624 [D loss: 0.698666, acc.: 57.81%] [G loss: 1.526562]\n",
      "epoch:0 step:625 [D loss: 0.635283, acc.: 63.28%] [G loss: 1.454694]\n",
      "epoch:0 step:626 [D loss: 0.736910, acc.: 53.91%] [G loss: 1.328902]\n",
      "epoch:0 step:627 [D loss: 0.624874, acc.: 65.62%] [G loss: 1.356933]\n",
      "epoch:0 step:628 [D loss: 0.744487, acc.: 54.69%] [G loss: 1.208199]\n",
      "epoch:0 step:629 [D loss: 0.748935, acc.: 49.22%] [G loss: 1.248896]\n",
      "epoch:0 step:630 [D loss: 0.682997, acc.: 53.91%] [G loss: 1.477851]\n",
      "epoch:0 step:631 [D loss: 0.729491, acc.: 59.38%] [G loss: 1.243073]\n",
      "epoch:0 step:632 [D loss: 0.580051, acc.: 67.97%] [G loss: 1.191147]\n",
      "epoch:0 step:633 [D loss: 0.687785, acc.: 53.91%] [G loss: 1.323709]\n",
      "epoch:0 step:634 [D loss: 0.535815, acc.: 75.00%] [G loss: 1.538002]\n",
      "epoch:0 step:635 [D loss: 0.571087, acc.: 70.31%] [G loss: 1.512777]\n",
      "epoch:0 step:636 [D loss: 0.748328, acc.: 50.78%] [G loss: 1.433362]\n",
      "epoch:0 step:637 [D loss: 0.598401, acc.: 73.44%] [G loss: 1.427426]\n",
      "epoch:0 step:638 [D loss: 0.778300, acc.: 50.00%] [G loss: 1.374852]\n",
      "epoch:0 step:639 [D loss: 0.680170, acc.: 64.06%] [G loss: 1.212904]\n",
      "epoch:0 step:640 [D loss: 0.633470, acc.: 64.84%] [G loss: 1.528521]\n",
      "epoch:0 step:641 [D loss: 0.569827, acc.: 66.41%] [G loss: 1.316434]\n",
      "epoch:0 step:642 [D loss: 0.701703, acc.: 53.91%] [G loss: 1.232956]\n",
      "epoch:0 step:643 [D loss: 0.691281, acc.: 57.03%] [G loss: 1.109597]\n",
      "epoch:0 step:644 [D loss: 0.701512, acc.: 54.69%] [G loss: 1.264140]\n",
      "epoch:0 step:645 [D loss: 0.604025, acc.: 68.75%] [G loss: 1.515312]\n",
      "epoch:0 step:646 [D loss: 0.842523, acc.: 46.09%] [G loss: 1.641852]\n",
      "epoch:0 step:647 [D loss: 0.687598, acc.: 60.16%] [G loss: 1.649776]\n",
      "epoch:0 step:648 [D loss: 0.614105, acc.: 67.19%] [G loss: 1.501804]\n",
      "epoch:0 step:649 [D loss: 0.757747, acc.: 49.22%] [G loss: 1.242735]\n",
      "epoch:0 step:650 [D loss: 0.652225, acc.: 67.19%] [G loss: 1.031877]\n",
      "epoch:0 step:651 [D loss: 0.627305, acc.: 64.06%] [G loss: 1.202999]\n",
      "epoch:0 step:652 [D loss: 0.677769, acc.: 64.84%] [G loss: 1.165478]\n",
      "epoch:0 step:653 [D loss: 0.605473, acc.: 63.28%] [G loss: 1.195787]\n",
      "epoch:0 step:654 [D loss: 0.696351, acc.: 55.47%] [G loss: 1.220021]\n",
      "epoch:0 step:655 [D loss: 0.693130, acc.: 51.56%] [G loss: 1.020693]\n",
      "epoch:0 step:656 [D loss: 0.607501, acc.: 61.72%] [G loss: 1.159474]\n",
      "epoch:0 step:657 [D loss: 0.668137, acc.: 59.38%] [G loss: 1.216422]\n",
      "epoch:0 step:658 [D loss: 0.724894, acc.: 53.12%] [G loss: 1.318008]\n",
      "epoch:0 step:659 [D loss: 0.673556, acc.: 61.72%] [G loss: 1.091842]\n",
      "epoch:0 step:660 [D loss: 0.742541, acc.: 50.00%] [G loss: 1.223167]\n",
      "epoch:0 step:661 [D loss: 0.651947, acc.: 60.94%] [G loss: 1.623698]\n",
      "epoch:0 step:662 [D loss: 0.686049, acc.: 61.72%] [G loss: 1.490603]\n",
      "epoch:0 step:663 [D loss: 0.638904, acc.: 61.72%] [G loss: 1.426550]\n",
      "epoch:0 step:664 [D loss: 0.642944, acc.: 67.97%] [G loss: 1.384812]\n",
      "epoch:0 step:665 [D loss: 0.791573, acc.: 50.78%] [G loss: 1.475965]\n",
      "epoch:0 step:666 [D loss: 0.679279, acc.: 59.38%] [G loss: 1.232484]\n",
      "epoch:0 step:667 [D loss: 0.707478, acc.: 58.59%] [G loss: 1.174936]\n",
      "epoch:0 step:668 [D loss: 0.795511, acc.: 48.44%] [G loss: 1.547542]\n",
      "epoch:0 step:669 [D loss: 0.735705, acc.: 56.25%] [G loss: 1.202505]\n",
      "epoch:0 step:670 [D loss: 0.772738, acc.: 49.22%] [G loss: 1.207883]\n",
      "epoch:0 step:671 [D loss: 0.657063, acc.: 64.06%] [G loss: 0.964817]\n",
      "epoch:0 step:672 [D loss: 0.707639, acc.: 57.81%] [G loss: 1.220893]\n",
      "epoch:0 step:673 [D loss: 0.744235, acc.: 54.69%] [G loss: 1.125547]\n",
      "epoch:0 step:674 [D loss: 0.691618, acc.: 57.81%] [G loss: 1.153306]\n",
      "epoch:0 step:675 [D loss: 0.883785, acc.: 38.28%] [G loss: 1.147229]\n",
      "epoch:0 step:676 [D loss: 0.762185, acc.: 50.78%] [G loss: 1.079200]\n",
      "epoch:0 step:677 [D loss: 0.666337, acc.: 58.59%] [G loss: 1.194512]\n",
      "epoch:0 step:678 [D loss: 0.679966, acc.: 57.03%] [G loss: 1.039131]\n",
      "epoch:0 step:679 [D loss: 0.642040, acc.: 59.38%] [G loss: 1.228716]\n",
      "epoch:0 step:680 [D loss: 0.723215, acc.: 49.22%] [G loss: 1.016769]\n",
      "epoch:0 step:681 [D loss: 0.728430, acc.: 49.22%] [G loss: 1.088074]\n",
      "epoch:0 step:682 [D loss: 0.778968, acc.: 44.53%] [G loss: 1.159268]\n",
      "epoch:0 step:683 [D loss: 0.655679, acc.: 57.81%] [G loss: 1.144083]\n",
      "epoch:0 step:684 [D loss: 0.659633, acc.: 60.16%] [G loss: 1.299581]\n",
      "epoch:0 step:685 [D loss: 0.703455, acc.: 57.81%] [G loss: 1.152875]\n",
      "epoch:0 step:686 [D loss: 0.711511, acc.: 54.69%] [G loss: 1.239139]\n",
      "epoch:0 step:687 [D loss: 0.683492, acc.: 60.94%] [G loss: 1.098907]\n",
      "epoch:0 step:688 [D loss: 0.664887, acc.: 59.38%] [G loss: 1.075443]\n",
      "epoch:0 step:689 [D loss: 0.685286, acc.: 55.47%] [G loss: 1.193683]\n",
      "epoch:0 step:690 [D loss: 0.607151, acc.: 67.19%] [G loss: 1.199043]\n",
      "epoch:0 step:691 [D loss: 0.664585, acc.: 58.59%] [G loss: 1.307723]\n",
      "epoch:0 step:692 [D loss: 0.660258, acc.: 63.28%] [G loss: 1.396911]\n",
      "epoch:0 step:693 [D loss: 0.736194, acc.: 60.16%] [G loss: 1.199244]\n",
      "epoch:0 step:694 [D loss: 0.681243, acc.: 56.25%] [G loss: 1.165025]\n",
      "epoch:0 step:695 [D loss: 0.664653, acc.: 57.81%] [G loss: 1.069943]\n",
      "epoch:0 step:696 [D loss: 0.692254, acc.: 54.69%] [G loss: 1.115685]\n",
      "epoch:0 step:697 [D loss: 0.622736, acc.: 64.06%] [G loss: 1.375785]\n",
      "epoch:0 step:698 [D loss: 0.784291, acc.: 43.75%] [G loss: 1.146166]\n",
      "epoch:0 step:699 [D loss: 0.689109, acc.: 52.34%] [G loss: 1.083734]\n",
      "epoch:0 step:700 [D loss: 0.680110, acc.: 60.16%] [G loss: 1.230477]\n",
      "epoch:0 step:701 [D loss: 0.608241, acc.: 63.28%] [G loss: 1.432298]\n",
      "epoch:0 step:702 [D loss: 0.731241, acc.: 56.25%] [G loss: 1.291433]\n",
      "epoch:0 step:703 [D loss: 0.706346, acc.: 56.25%] [G loss: 1.223677]\n",
      "epoch:0 step:704 [D loss: 0.648711, acc.: 59.38%] [G loss: 1.253220]\n",
      "epoch:0 step:705 [D loss: 0.737210, acc.: 52.34%] [G loss: 1.061074]\n",
      "epoch:0 step:706 [D loss: 0.682695, acc.: 53.91%] [G loss: 1.270308]\n",
      "epoch:0 step:707 [D loss: 0.699447, acc.: 64.06%] [G loss: 1.226978]\n",
      "epoch:0 step:708 [D loss: 0.629116, acc.: 63.28%] [G loss: 1.300542]\n",
      "epoch:0 step:709 [D loss: 0.633093, acc.: 62.50%] [G loss: 1.331861]\n",
      "epoch:0 step:710 [D loss: 0.638597, acc.: 59.38%] [G loss: 1.284293]\n",
      "epoch:0 step:711 [D loss: 0.690062, acc.: 57.03%] [G loss: 1.214794]\n",
      "epoch:0 step:712 [D loss: 0.697408, acc.: 56.25%] [G loss: 1.233621]\n",
      "epoch:0 step:713 [D loss: 0.637061, acc.: 57.03%] [G loss: 1.374235]\n",
      "epoch:0 step:714 [D loss: 0.754308, acc.: 49.22%] [G loss: 1.550621]\n",
      "epoch:0 step:715 [D loss: 0.681718, acc.: 57.03%] [G loss: 1.169553]\n",
      "epoch:0 step:716 [D loss: 0.772461, acc.: 48.44%] [G loss: 1.065952]\n",
      "epoch:0 step:717 [D loss: 0.594395, acc.: 71.88%] [G loss: 1.209820]\n",
      "epoch:0 step:718 [D loss: 0.644663, acc.: 64.06%] [G loss: 1.512623]\n",
      "epoch:0 step:719 [D loss: 0.723553, acc.: 52.34%] [G loss: 1.288131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:720 [D loss: 0.586594, acc.: 70.31%] [G loss: 1.637085]\n",
      "epoch:0 step:721 [D loss: 0.648684, acc.: 61.72%] [G loss: 1.627885]\n",
      "epoch:0 step:722 [D loss: 0.683113, acc.: 58.59%] [G loss: 1.227320]\n",
      "epoch:0 step:723 [D loss: 0.678113, acc.: 56.25%] [G loss: 1.099153]\n",
      "epoch:0 step:724 [D loss: 0.719051, acc.: 52.34%] [G loss: 1.171120]\n",
      "epoch:0 step:725 [D loss: 0.648330, acc.: 60.16%] [G loss: 1.364503]\n",
      "epoch:0 step:726 [D loss: 0.818725, acc.: 46.09%] [G loss: 1.175782]\n",
      "epoch:0 step:727 [D loss: 0.638529, acc.: 59.38%] [G loss: 1.316591]\n",
      "epoch:0 step:728 [D loss: 0.664934, acc.: 54.69%] [G loss: 1.342849]\n",
      "epoch:0 step:729 [D loss: 0.682721, acc.: 62.50%] [G loss: 1.376214]\n",
      "epoch:0 step:730 [D loss: 0.703257, acc.: 57.81%] [G loss: 0.998218]\n",
      "epoch:0 step:731 [D loss: 0.832316, acc.: 56.25%] [G loss: 1.656700]\n",
      "epoch:0 step:732 [D loss: 0.712314, acc.: 59.38%] [G loss: 1.399998]\n",
      "epoch:0 step:733 [D loss: 0.699987, acc.: 58.59%] [G loss: 1.389420]\n",
      "epoch:0 step:734 [D loss: 0.658828, acc.: 60.94%] [G loss: 1.341091]\n",
      "epoch:0 step:735 [D loss: 0.685794, acc.: 59.38%] [G loss: 1.162114]\n",
      "epoch:0 step:736 [D loss: 0.744907, acc.: 49.22%] [G loss: 1.418598]\n",
      "epoch:0 step:737 [D loss: 0.665960, acc.: 60.16%] [G loss: 1.200671]\n",
      "epoch:0 step:738 [D loss: 0.724897, acc.: 57.03%] [G loss: 1.184191]\n",
      "epoch:0 step:739 [D loss: 0.612706, acc.: 69.53%] [G loss: 1.128283]\n",
      "epoch:0 step:740 [D loss: 0.708495, acc.: 51.56%] [G loss: 1.102179]\n",
      "epoch:0 step:741 [D loss: 0.622634, acc.: 68.75%] [G loss: 1.020603]\n",
      "epoch:0 step:742 [D loss: 0.627840, acc.: 61.72%] [G loss: 1.132756]\n",
      "epoch:0 step:743 [D loss: 0.668817, acc.: 64.06%] [G loss: 1.054910]\n",
      "epoch:0 step:744 [D loss: 0.731356, acc.: 49.22%] [G loss: 1.162923]\n",
      "epoch:0 step:745 [D loss: 0.651577, acc.: 59.38%] [G loss: 1.067596]\n",
      "epoch:0 step:746 [D loss: 0.625153, acc.: 69.53%] [G loss: 1.138988]\n",
      "epoch:0 step:747 [D loss: 0.654735, acc.: 58.59%] [G loss: 1.084286]\n",
      "epoch:0 step:748 [D loss: 0.588531, acc.: 67.19%] [G loss: 1.199106]\n",
      "epoch:0 step:749 [D loss: 0.614082, acc.: 67.97%] [G loss: 1.099888]\n",
      "epoch:0 step:750 [D loss: 0.735707, acc.: 53.12%] [G loss: 1.108566]\n",
      "epoch:0 step:751 [D loss: 0.687059, acc.: 53.91%] [G loss: 1.133078]\n",
      "epoch:0 step:752 [D loss: 0.672658, acc.: 57.03%] [G loss: 1.093630]\n",
      "epoch:0 step:753 [D loss: 0.594983, acc.: 70.31%] [G loss: 1.099849]\n",
      "epoch:0 step:754 [D loss: 0.628404, acc.: 64.06%] [G loss: 1.121165]\n",
      "epoch:0 step:755 [D loss: 0.620342, acc.: 60.94%] [G loss: 1.156942]\n",
      "epoch:0 step:756 [D loss: 0.575168, acc.: 68.75%] [G loss: 1.158147]\n",
      "epoch:0 step:757 [D loss: 0.699597, acc.: 59.38%] [G loss: 1.017273]\n",
      "epoch:0 step:758 [D loss: 0.748360, acc.: 52.34%] [G loss: 1.086100]\n",
      "epoch:0 step:759 [D loss: 0.650334, acc.: 60.94%] [G loss: 1.109014]\n",
      "epoch:0 step:760 [D loss: 0.588993, acc.: 67.19%] [G loss: 1.270087]\n",
      "epoch:0 step:761 [D loss: 0.744107, acc.: 46.09%] [G loss: 1.134475]\n",
      "epoch:0 step:762 [D loss: 0.710097, acc.: 56.25%] [G loss: 1.052706]\n",
      "epoch:0 step:763 [D loss: 0.686276, acc.: 59.38%] [G loss: 0.992117]\n",
      "epoch:0 step:764 [D loss: 0.642066, acc.: 68.75%] [G loss: 1.161748]\n",
      "epoch:0 step:765 [D loss: 0.694032, acc.: 55.47%] [G loss: 1.254866]\n",
      "epoch:0 step:766 [D loss: 0.675876, acc.: 56.25%] [G loss: 1.095382]\n",
      "epoch:0 step:767 [D loss: 0.734132, acc.: 50.00%] [G loss: 1.044766]\n",
      "epoch:0 step:768 [D loss: 0.715185, acc.: 51.56%] [G loss: 1.238003]\n",
      "epoch:0 step:769 [D loss: 0.696366, acc.: 57.81%] [G loss: 1.073244]\n",
      "epoch:0 step:770 [D loss: 0.670084, acc.: 58.59%] [G loss: 1.135077]\n",
      "epoch:0 step:771 [D loss: 0.695323, acc.: 54.69%] [G loss: 1.173404]\n",
      "epoch:0 step:772 [D loss: 0.677048, acc.: 53.91%] [G loss: 1.268963]\n",
      "epoch:0 step:773 [D loss: 0.683167, acc.: 52.34%] [G loss: 1.220563]\n",
      "epoch:0 step:774 [D loss: 0.616159, acc.: 67.19%] [G loss: 1.133182]\n",
      "epoch:0 step:775 [D loss: 0.718732, acc.: 57.81%] [G loss: 1.272537]\n",
      "epoch:0 step:776 [D loss: 0.619374, acc.: 68.75%] [G loss: 1.213892]\n",
      "epoch:0 step:777 [D loss: 0.634887, acc.: 60.94%] [G loss: 1.149493]\n",
      "epoch:0 step:778 [D loss: 0.673745, acc.: 54.69%] [G loss: 1.308362]\n",
      "epoch:0 step:779 [D loss: 0.630698, acc.: 64.06%] [G loss: 1.238774]\n",
      "epoch:0 step:780 [D loss: 0.674788, acc.: 57.03%] [G loss: 1.100829]\n",
      "epoch:0 step:781 [D loss: 0.647267, acc.: 62.50%] [G loss: 1.442536]\n",
      "epoch:0 step:782 [D loss: 0.651840, acc.: 58.59%] [G loss: 1.461711]\n",
      "epoch:0 step:783 [D loss: 0.688114, acc.: 57.81%] [G loss: 1.078759]\n",
      "epoch:0 step:784 [D loss: 0.617009, acc.: 67.97%] [G loss: 1.224122]\n",
      "epoch:0 step:785 [D loss: 0.653035, acc.: 61.72%] [G loss: 1.222785]\n",
      "epoch:0 step:786 [D loss: 0.675313, acc.: 60.94%] [G loss: 1.314054]\n",
      "epoch:0 step:787 [D loss: 0.644150, acc.: 63.28%] [G loss: 1.362372]\n",
      "epoch:0 step:788 [D loss: 0.692812, acc.: 64.06%] [G loss: 1.064666]\n",
      "epoch:0 step:789 [D loss: 0.689374, acc.: 55.47%] [G loss: 1.255247]\n",
      "epoch:0 step:790 [D loss: 0.636452, acc.: 64.84%] [G loss: 1.433278]\n",
      "epoch:0 step:791 [D loss: 0.569124, acc.: 75.00%] [G loss: 1.245397]\n",
      "epoch:0 step:792 [D loss: 0.659794, acc.: 56.25%] [G loss: 1.359809]\n",
      "epoch:0 step:793 [D loss: 0.727993, acc.: 57.03%] [G loss: 1.200321]\n",
      "epoch:0 step:794 [D loss: 0.670105, acc.: 60.16%] [G loss: 1.280141]\n",
      "epoch:0 step:795 [D loss: 0.748582, acc.: 55.47%] [G loss: 1.051793]\n",
      "epoch:0 step:796 [D loss: 0.724268, acc.: 57.03%] [G loss: 1.228845]\n",
      "epoch:0 step:797 [D loss: 0.706902, acc.: 55.47%] [G loss: 1.175704]\n",
      "epoch:0 step:798 [D loss: 0.701905, acc.: 53.91%] [G loss: 1.113687]\n",
      "epoch:0 step:799 [D loss: 0.601293, acc.: 71.09%] [G loss: 1.341736]\n",
      "epoch:0 step:800 [D loss: 0.606663, acc.: 64.84%] [G loss: 1.301780]\n",
      "epoch:0 step:801 [D loss: 0.634498, acc.: 64.06%] [G loss: 1.277603]\n",
      "epoch:0 step:802 [D loss: 0.678586, acc.: 61.72%] [G loss: 1.055536]\n",
      "epoch:0 step:803 [D loss: 0.615766, acc.: 67.19%] [G loss: 1.163562]\n",
      "epoch:0 step:804 [D loss: 0.664108, acc.: 60.16%] [G loss: 1.133766]\n",
      "epoch:0 step:805 [D loss: 0.700625, acc.: 50.00%] [G loss: 1.010073]\n",
      "epoch:0 step:806 [D loss: 0.656038, acc.: 64.06%] [G loss: 1.081458]\n",
      "epoch:0 step:807 [D loss: 0.689397, acc.: 54.69%] [G loss: 1.320822]\n",
      "epoch:0 step:808 [D loss: 0.641937, acc.: 65.62%] [G loss: 1.213233]\n",
      "epoch:0 step:809 [D loss: 0.627535, acc.: 66.41%] [G loss: 1.340883]\n",
      "epoch:0 step:810 [D loss: 0.653866, acc.: 60.94%] [G loss: 1.202468]\n",
      "epoch:0 step:811 [D loss: 0.631527, acc.: 63.28%] [G loss: 1.087636]\n",
      "epoch:0 step:812 [D loss: 0.757800, acc.: 54.69%] [G loss: 1.098396]\n",
      "epoch:0 step:813 [D loss: 0.826693, acc.: 40.62%] [G loss: 1.024969]\n",
      "epoch:0 step:814 [D loss: 0.675404, acc.: 58.59%] [G loss: 1.109091]\n",
      "epoch:0 step:815 [D loss: 0.660542, acc.: 67.97%] [G loss: 1.329974]\n",
      "epoch:0 step:816 [D loss: 0.615565, acc.: 67.19%] [G loss: 1.214643]\n",
      "epoch:0 step:817 [D loss: 0.720239, acc.: 60.16%] [G loss: 1.156453]\n",
      "epoch:0 step:818 [D loss: 0.759813, acc.: 46.88%] [G loss: 1.290762]\n",
      "epoch:0 step:819 [D loss: 0.691324, acc.: 60.94%] [G loss: 1.036698]\n",
      "epoch:0 step:820 [D loss: 0.643241, acc.: 60.94%] [G loss: 1.057565]\n",
      "epoch:0 step:821 [D loss: 0.594151, acc.: 62.50%] [G loss: 0.947792]\n",
      "epoch:0 step:822 [D loss: 0.712110, acc.: 46.09%] [G loss: 1.105768]\n",
      "epoch:0 step:823 [D loss: 0.753035, acc.: 53.12%] [G loss: 1.240430]\n",
      "epoch:0 step:824 [D loss: 0.666695, acc.: 61.72%] [G loss: 1.191584]\n",
      "epoch:0 step:825 [D loss: 0.660989, acc.: 62.50%] [G loss: 0.971046]\n",
      "epoch:0 step:826 [D loss: 0.684053, acc.: 57.81%] [G loss: 0.987845]\n",
      "epoch:0 step:827 [D loss: 0.639494, acc.: 61.72%] [G loss: 1.080471]\n",
      "epoch:0 step:828 [D loss: 0.664234, acc.: 60.16%] [G loss: 1.242529]\n",
      "epoch:0 step:829 [D loss: 0.677395, acc.: 53.12%] [G loss: 1.178973]\n",
      "epoch:0 step:830 [D loss: 0.677623, acc.: 57.03%] [G loss: 1.155611]\n",
      "epoch:0 step:831 [D loss: 0.664683, acc.: 60.94%] [G loss: 1.203185]\n",
      "epoch:0 step:832 [D loss: 0.631744, acc.: 67.19%] [G loss: 1.231555]\n",
      "epoch:0 step:833 [D loss: 0.625351, acc.: 65.62%] [G loss: 1.264901]\n",
      "epoch:0 step:834 [D loss: 0.663157, acc.: 58.59%] [G loss: 1.140038]\n",
      "epoch:0 step:835 [D loss: 0.609567, acc.: 70.31%] [G loss: 1.259910]\n",
      "epoch:0 step:836 [D loss: 0.701696, acc.: 53.12%] [G loss: 1.127067]\n",
      "epoch:0 step:837 [D loss: 0.721434, acc.: 50.78%] [G loss: 1.290449]\n",
      "epoch:0 step:838 [D loss: 0.645610, acc.: 61.72%] [G loss: 1.166900]\n",
      "epoch:0 step:839 [D loss: 0.659137, acc.: 62.50%] [G loss: 1.136771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:840 [D loss: 0.736943, acc.: 54.69%] [G loss: 1.088385]\n",
      "epoch:0 step:841 [D loss: 0.675868, acc.: 58.59%] [G loss: 0.929306]\n",
      "epoch:0 step:842 [D loss: 0.636153, acc.: 64.84%] [G loss: 1.114430]\n",
      "epoch:0 step:843 [D loss: 0.736996, acc.: 50.78%] [G loss: 1.103734]\n",
      "epoch:0 step:844 [D loss: 0.725455, acc.: 51.56%] [G loss: 1.157454]\n",
      "epoch:0 step:845 [D loss: 0.778037, acc.: 48.44%] [G loss: 1.128671]\n",
      "epoch:0 step:846 [D loss: 0.718397, acc.: 56.25%] [G loss: 1.093529]\n",
      "epoch:0 step:847 [D loss: 0.681310, acc.: 57.03%] [G loss: 1.118019]\n",
      "epoch:0 step:848 [D loss: 0.713444, acc.: 53.91%] [G loss: 1.206907]\n",
      "epoch:0 step:849 [D loss: 0.703994, acc.: 57.03%] [G loss: 1.080873]\n",
      "epoch:0 step:850 [D loss: 0.677326, acc.: 62.50%] [G loss: 1.062769]\n",
      "epoch:0 step:851 [D loss: 0.737121, acc.: 53.91%] [G loss: 1.318262]\n",
      "epoch:0 step:852 [D loss: 0.614955, acc.: 66.41%] [G loss: 1.031988]\n",
      "epoch:0 step:853 [D loss: 0.671093, acc.: 60.16%] [G loss: 1.092648]\n",
      "epoch:0 step:854 [D loss: 0.611247, acc.: 65.62%] [G loss: 1.075487]\n",
      "epoch:0 step:855 [D loss: 0.737647, acc.: 47.66%] [G loss: 1.228325]\n",
      "epoch:0 step:856 [D loss: 0.524601, acc.: 75.78%] [G loss: 1.325221]\n",
      "epoch:0 step:857 [D loss: 0.695898, acc.: 60.94%] [G loss: 1.102418]\n",
      "epoch:0 step:858 [D loss: 0.741656, acc.: 55.47%] [G loss: 1.108608]\n",
      "epoch:0 step:859 [D loss: 0.694295, acc.: 56.25%] [G loss: 1.023705]\n",
      "epoch:0 step:860 [D loss: 0.665842, acc.: 57.03%] [G loss: 1.185328]\n",
      "epoch:0 step:861 [D loss: 0.690809, acc.: 55.47%] [G loss: 1.129267]\n",
      "epoch:0 step:862 [D loss: 0.710602, acc.: 48.44%] [G loss: 1.080429]\n",
      "epoch:0 step:863 [D loss: 0.667120, acc.: 54.69%] [G loss: 1.208604]\n",
      "epoch:0 step:864 [D loss: 0.648873, acc.: 61.72%] [G loss: 1.108647]\n",
      "epoch:0 step:865 [D loss: 0.643365, acc.: 61.72%] [G loss: 1.188678]\n",
      "epoch:0 step:866 [D loss: 0.658598, acc.: 64.06%] [G loss: 1.016205]\n",
      "epoch:0 step:867 [D loss: 0.672485, acc.: 59.38%] [G loss: 0.944516]\n",
      "epoch:0 step:868 [D loss: 0.652502, acc.: 57.81%] [G loss: 1.163893]\n",
      "epoch:0 step:869 [D loss: 0.635313, acc.: 65.62%] [G loss: 1.250592]\n",
      "epoch:0 step:870 [D loss: 0.754620, acc.: 50.78%] [G loss: 1.106555]\n",
      "epoch:0 step:871 [D loss: 0.674202, acc.: 60.16%] [G loss: 1.193106]\n",
      "epoch:0 step:872 [D loss: 0.687986, acc.: 53.12%] [G loss: 1.060267]\n",
      "epoch:0 step:873 [D loss: 0.712183, acc.: 53.91%] [G loss: 1.128912]\n",
      "epoch:0 step:874 [D loss: 0.692629, acc.: 53.12%] [G loss: 1.146421]\n",
      "epoch:0 step:875 [D loss: 0.643865, acc.: 61.72%] [G loss: 1.144260]\n",
      "epoch:0 step:876 [D loss: 0.571613, acc.: 71.09%] [G loss: 1.109914]\n",
      "epoch:0 step:877 [D loss: 0.664509, acc.: 61.72%] [G loss: 1.208314]\n",
      "epoch:0 step:878 [D loss: 0.679483, acc.: 61.72%] [G loss: 1.042714]\n",
      "epoch:0 step:879 [D loss: 0.672689, acc.: 56.25%] [G loss: 1.106246]\n",
      "epoch:0 step:880 [D loss: 0.653785, acc.: 57.81%] [G loss: 1.086994]\n",
      "epoch:0 step:881 [D loss: 0.646971, acc.: 64.84%] [G loss: 1.050454]\n",
      "epoch:0 step:882 [D loss: 0.631186, acc.: 61.72%] [G loss: 1.120134]\n",
      "epoch:0 step:883 [D loss: 0.606594, acc.: 69.53%] [G loss: 1.244061]\n",
      "epoch:0 step:884 [D loss: 0.595801, acc.: 68.75%] [G loss: 1.300582]\n",
      "epoch:0 step:885 [D loss: 0.663802, acc.: 59.38%] [G loss: 1.213361]\n",
      "epoch:0 step:886 [D loss: 0.548947, acc.: 68.75%] [G loss: 1.206311]\n",
      "epoch:0 step:887 [D loss: 0.616836, acc.: 67.19%] [G loss: 1.068974]\n",
      "epoch:0 step:888 [D loss: 0.611368, acc.: 67.19%] [G loss: 1.089522]\n",
      "epoch:0 step:889 [D loss: 0.663140, acc.: 58.59%] [G loss: 1.066051]\n",
      "epoch:0 step:890 [D loss: 0.606860, acc.: 67.19%] [G loss: 1.266843]\n",
      "epoch:0 step:891 [D loss: 0.801383, acc.: 49.22%] [G loss: 1.224497]\n",
      "epoch:0 step:892 [D loss: 0.650694, acc.: 60.94%] [G loss: 1.191302]\n",
      "epoch:0 step:893 [D loss: 0.680148, acc.: 57.81%] [G loss: 1.197088]\n",
      "epoch:0 step:894 [D loss: 0.748923, acc.: 56.25%] [G loss: 1.103008]\n",
      "epoch:0 step:895 [D loss: 0.740710, acc.: 52.34%] [G loss: 1.178562]\n",
      "epoch:0 step:896 [D loss: 0.745944, acc.: 54.69%] [G loss: 1.309767]\n",
      "epoch:0 step:897 [D loss: 0.677416, acc.: 61.72%] [G loss: 0.981695]\n",
      "epoch:0 step:898 [D loss: 0.632560, acc.: 66.41%] [G loss: 1.081226]\n",
      "epoch:0 step:899 [D loss: 0.617606, acc.: 59.38%] [G loss: 1.309548]\n",
      "epoch:0 step:900 [D loss: 0.681691, acc.: 57.81%] [G loss: 1.295112]\n",
      "epoch:0 step:901 [D loss: 0.708683, acc.: 55.47%] [G loss: 1.182124]\n",
      "epoch:0 step:902 [D loss: 0.618845, acc.: 64.84%] [G loss: 1.238587]\n",
      "epoch:0 step:903 [D loss: 0.593296, acc.: 66.41%] [G loss: 1.168145]\n",
      "epoch:0 step:904 [D loss: 0.608609, acc.: 69.53%] [G loss: 1.340903]\n",
      "epoch:0 step:905 [D loss: 0.751355, acc.: 49.22%] [G loss: 1.053866]\n",
      "epoch:0 step:906 [D loss: 0.697823, acc.: 53.91%] [G loss: 1.249678]\n",
      "epoch:0 step:907 [D loss: 0.680091, acc.: 58.59%] [G loss: 1.210133]\n",
      "epoch:0 step:908 [D loss: 0.571238, acc.: 72.66%] [G loss: 1.231513]\n",
      "epoch:0 step:909 [D loss: 0.623873, acc.: 67.19%] [G loss: 1.259393]\n",
      "epoch:0 step:910 [D loss: 0.704921, acc.: 56.25%] [G loss: 1.209901]\n",
      "epoch:0 step:911 [D loss: 0.629994, acc.: 60.16%] [G loss: 1.080568]\n",
      "epoch:0 step:912 [D loss: 0.729323, acc.: 53.91%] [G loss: 0.923811]\n",
      "epoch:0 step:913 [D loss: 0.614645, acc.: 68.75%] [G loss: 1.075144]\n",
      "epoch:0 step:914 [D loss: 0.616790, acc.: 65.62%] [G loss: 1.327255]\n",
      "epoch:0 step:915 [D loss: 0.631089, acc.: 61.72%] [G loss: 0.980878]\n",
      "epoch:0 step:916 [D loss: 0.802721, acc.: 50.78%] [G loss: 1.346881]\n",
      "epoch:0 step:917 [D loss: 0.573318, acc.: 71.09%] [G loss: 1.431460]\n",
      "epoch:0 step:918 [D loss: 0.645238, acc.: 66.41%] [G loss: 1.154607]\n",
      "epoch:0 step:919 [D loss: 0.756780, acc.: 53.91%] [G loss: 1.119869]\n",
      "epoch:0 step:920 [D loss: 0.692545, acc.: 57.03%] [G loss: 0.938154]\n",
      "epoch:0 step:921 [D loss: 0.748684, acc.: 49.22%] [G loss: 1.102289]\n",
      "epoch:0 step:922 [D loss: 0.721138, acc.: 49.22%] [G loss: 1.045981]\n",
      "epoch:0 step:923 [D loss: 0.598816, acc.: 70.31%] [G loss: 1.113396]\n",
      "epoch:0 step:924 [D loss: 0.735597, acc.: 52.34%] [G loss: 1.197186]\n",
      "epoch:0 step:925 [D loss: 0.698166, acc.: 51.56%] [G loss: 1.097678]\n",
      "epoch:0 step:926 [D loss: 0.701522, acc.: 55.47%] [G loss: 1.058161]\n",
      "epoch:0 step:927 [D loss: 0.638679, acc.: 63.28%] [G loss: 1.174429]\n",
      "epoch:0 step:928 [D loss: 0.597272, acc.: 67.97%] [G loss: 1.134473]\n",
      "epoch:0 step:929 [D loss: 0.657656, acc.: 60.16%] [G loss: 1.132425]\n",
      "epoch:0 step:930 [D loss: 0.682198, acc.: 59.38%] [G loss: 1.005499]\n",
      "epoch:0 step:931 [D loss: 0.709705, acc.: 57.81%] [G loss: 1.043042]\n",
      "epoch:0 step:932 [D loss: 0.701269, acc.: 51.56%] [G loss: 1.046274]\n",
      "epoch:0 step:933 [D loss: 0.698977, acc.: 53.12%] [G loss: 0.858825]\n",
      "epoch:0 step:934 [D loss: 0.655628, acc.: 57.03%] [G loss: 0.959540]\n",
      "epoch:0 step:935 [D loss: 0.664786, acc.: 59.38%] [G loss: 1.055422]\n",
      "epoch:0 step:936 [D loss: 0.706397, acc.: 55.47%] [G loss: 1.020955]\n",
      "epoch:0 step:937 [D loss: 0.693087, acc.: 60.16%] [G loss: 1.032496]\n",
      "epoch:1 step:938 [D loss: 0.692321, acc.: 62.50%] [G loss: 1.081316]\n",
      "epoch:1 step:939 [D loss: 0.687537, acc.: 55.47%] [G loss: 1.435018]\n",
      "epoch:1 step:940 [D loss: 0.704124, acc.: 55.47%] [G loss: 1.132922]\n",
      "epoch:1 step:941 [D loss: 0.667921, acc.: 62.50%] [G loss: 1.096561]\n",
      "epoch:1 step:942 [D loss: 0.647286, acc.: 57.81%] [G loss: 1.154932]\n",
      "epoch:1 step:943 [D loss: 0.641762, acc.: 60.94%] [G loss: 1.103791]\n",
      "epoch:1 step:944 [D loss: 0.720892, acc.: 59.38%] [G loss: 1.009739]\n",
      "epoch:1 step:945 [D loss: 0.699750, acc.: 53.12%] [G loss: 1.037677]\n",
      "epoch:1 step:946 [D loss: 0.677343, acc.: 61.72%] [G loss: 1.100000]\n",
      "epoch:1 step:947 [D loss: 0.553153, acc.: 69.53%] [G loss: 1.142410]\n",
      "epoch:1 step:948 [D loss: 0.688664, acc.: 56.25%] [G loss: 1.104757]\n",
      "epoch:1 step:949 [D loss: 0.595681, acc.: 70.31%] [G loss: 1.063769]\n",
      "epoch:1 step:950 [D loss: 0.685509, acc.: 56.25%] [G loss: 1.031835]\n",
      "epoch:1 step:951 [D loss: 0.608228, acc.: 67.19%] [G loss: 1.056747]\n",
      "epoch:1 step:952 [D loss: 0.666049, acc.: 54.69%] [G loss: 1.081897]\n",
      "epoch:1 step:953 [D loss: 0.640051, acc.: 65.62%] [G loss: 0.968002]\n",
      "epoch:1 step:954 [D loss: 0.558837, acc.: 65.62%] [G loss: 1.315114]\n",
      "epoch:1 step:955 [D loss: 0.739196, acc.: 46.88%] [G loss: 1.258485]\n",
      "epoch:1 step:956 [D loss: 0.658988, acc.: 62.50%] [G loss: 1.081939]\n",
      "epoch:1 step:957 [D loss: 0.600488, acc.: 64.06%] [G loss: 1.171919]\n",
      "epoch:1 step:958 [D loss: 0.687032, acc.: 57.81%] [G loss: 1.254118]\n",
      "epoch:1 step:959 [D loss: 0.669831, acc.: 56.25%] [G loss: 1.007847]\n",
      "epoch:1 step:960 [D loss: 0.649377, acc.: 61.72%] [G loss: 1.244194]\n",
      "epoch:1 step:961 [D loss: 0.656832, acc.: 57.81%] [G loss: 1.261246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:962 [D loss: 0.655016, acc.: 61.72%] [G loss: 1.154327]\n",
      "epoch:1 step:963 [D loss: 0.733414, acc.: 55.47%] [G loss: 1.034903]\n",
      "epoch:1 step:964 [D loss: 0.710154, acc.: 54.69%] [G loss: 1.108798]\n",
      "epoch:1 step:965 [D loss: 0.682737, acc.: 57.03%] [G loss: 1.171890]\n",
      "epoch:1 step:966 [D loss: 0.745940, acc.: 53.12%] [G loss: 1.189316]\n",
      "epoch:1 step:967 [D loss: 0.637756, acc.: 60.16%] [G loss: 1.216977]\n",
      "epoch:1 step:968 [D loss: 0.751035, acc.: 46.09%] [G loss: 1.088394]\n",
      "epoch:1 step:969 [D loss: 0.630469, acc.: 64.06%] [G loss: 1.193901]\n",
      "epoch:1 step:970 [D loss: 0.648580, acc.: 57.81%] [G loss: 1.052871]\n",
      "epoch:1 step:971 [D loss: 0.636444, acc.: 61.72%] [G loss: 1.054366]\n",
      "epoch:1 step:972 [D loss: 0.726685, acc.: 51.56%] [G loss: 1.083174]\n",
      "epoch:1 step:973 [D loss: 0.667706, acc.: 57.03%] [G loss: 1.123743]\n",
      "epoch:1 step:974 [D loss: 0.671274, acc.: 62.50%] [G loss: 1.395199]\n",
      "epoch:1 step:975 [D loss: 0.680941, acc.: 60.94%] [G loss: 1.024839]\n",
      "epoch:1 step:976 [D loss: 0.741632, acc.: 46.09%] [G loss: 1.004655]\n",
      "epoch:1 step:977 [D loss: 0.683477, acc.: 53.12%] [G loss: 1.279195]\n",
      "epoch:1 step:978 [D loss: 0.614583, acc.: 64.84%] [G loss: 1.142515]\n",
      "epoch:1 step:979 [D loss: 0.716643, acc.: 51.56%] [G loss: 1.129087]\n",
      "epoch:1 step:980 [D loss: 0.568043, acc.: 73.44%] [G loss: 1.203168]\n",
      "epoch:1 step:981 [D loss: 0.631598, acc.: 60.16%] [G loss: 1.246754]\n",
      "epoch:1 step:982 [D loss: 0.735199, acc.: 42.19%] [G loss: 0.978246]\n",
      "epoch:1 step:983 [D loss: 0.675665, acc.: 59.38%] [G loss: 0.930678]\n",
      "epoch:1 step:984 [D loss: 0.679640, acc.: 56.25%] [G loss: 0.937037]\n",
      "epoch:1 step:985 [D loss: 0.639884, acc.: 58.59%] [G loss: 1.121945]\n",
      "epoch:1 step:986 [D loss: 0.632682, acc.: 62.50%] [G loss: 1.154433]\n",
      "epoch:1 step:987 [D loss: 0.675455, acc.: 53.91%] [G loss: 1.013793]\n",
      "epoch:1 step:988 [D loss: 0.624376, acc.: 60.94%] [G loss: 1.037258]\n",
      "epoch:1 step:989 [D loss: 0.673569, acc.: 57.81%] [G loss: 0.915994]\n",
      "epoch:1 step:990 [D loss: 0.596329, acc.: 64.84%] [G loss: 1.063574]\n",
      "epoch:1 step:991 [D loss: 0.641054, acc.: 62.50%] [G loss: 1.083206]\n",
      "epoch:1 step:992 [D loss: 0.687969, acc.: 58.59%] [G loss: 1.206125]\n",
      "epoch:1 step:993 [D loss: 0.645940, acc.: 65.62%] [G loss: 1.041847]\n",
      "epoch:1 step:994 [D loss: 0.632195, acc.: 61.72%] [G loss: 1.035939]\n",
      "epoch:1 step:995 [D loss: 0.605956, acc.: 67.97%] [G loss: 0.986941]\n",
      "epoch:1 step:996 [D loss: 0.586939, acc.: 66.41%] [G loss: 1.045031]\n",
      "epoch:1 step:997 [D loss: 0.698890, acc.: 54.69%] [G loss: 0.982410]\n",
      "epoch:1 step:998 [D loss: 0.682892, acc.: 61.72%] [G loss: 1.111687]\n",
      "epoch:1 step:999 [D loss: 0.624206, acc.: 60.94%] [G loss: 1.264118]\n",
      "epoch:1 step:1000 [D loss: 0.665394, acc.: 62.50%] [G loss: 1.071550]\n",
      "epoch:1 step:1001 [D loss: 0.789019, acc.: 50.00%] [G loss: 1.152711]\n",
      "epoch:1 step:1002 [D loss: 0.746785, acc.: 50.78%] [G loss: 1.186745]\n",
      "epoch:1 step:1003 [D loss: 0.650943, acc.: 66.41%] [G loss: 1.100734]\n",
      "epoch:1 step:1004 [D loss: 0.681312, acc.: 57.81%] [G loss: 1.146984]\n",
      "epoch:1 step:1005 [D loss: 0.661701, acc.: 56.25%] [G loss: 1.052332]\n",
      "epoch:1 step:1006 [D loss: 0.585910, acc.: 67.97%] [G loss: 1.168797]\n",
      "epoch:1 step:1007 [D loss: 0.633234, acc.: 65.62%] [G loss: 1.032935]\n",
      "epoch:1 step:1008 [D loss: 0.676016, acc.: 57.81%] [G loss: 1.222508]\n",
      "epoch:1 step:1009 [D loss: 0.611991, acc.: 65.62%] [G loss: 1.304094]\n",
      "epoch:1 step:1010 [D loss: 0.681869, acc.: 60.94%] [G loss: 1.019620]\n",
      "epoch:1 step:1011 [D loss: 0.684848, acc.: 59.38%] [G loss: 1.085753]\n",
      "epoch:1 step:1012 [D loss: 0.704663, acc.: 51.56%] [G loss: 1.079273]\n",
      "epoch:1 step:1013 [D loss: 0.642330, acc.: 64.84%] [G loss: 1.103986]\n",
      "epoch:1 step:1014 [D loss: 0.623856, acc.: 60.94%] [G loss: 1.063571]\n",
      "epoch:1 step:1015 [D loss: 0.683677, acc.: 56.25%] [G loss: 0.933803]\n",
      "epoch:1 step:1016 [D loss: 0.606419, acc.: 67.97%] [G loss: 0.971290]\n",
      "epoch:1 step:1017 [D loss: 0.703268, acc.: 52.34%] [G loss: 1.188619]\n",
      "epoch:1 step:1018 [D loss: 0.766068, acc.: 50.78%] [G loss: 1.019247]\n",
      "epoch:1 step:1019 [D loss: 0.702713, acc.: 44.53%] [G loss: 0.989948]\n",
      "epoch:1 step:1020 [D loss: 0.607503, acc.: 71.09%] [G loss: 1.072583]\n",
      "epoch:1 step:1021 [D loss: 0.719858, acc.: 54.69%] [G loss: 0.962174]\n",
      "epoch:1 step:1022 [D loss: 0.625069, acc.: 60.94%] [G loss: 1.290833]\n",
      "epoch:1 step:1023 [D loss: 0.726673, acc.: 55.47%] [G loss: 1.130266]\n",
      "epoch:1 step:1024 [D loss: 0.614257, acc.: 64.84%] [G loss: 0.998865]\n",
      "epoch:1 step:1025 [D loss: 0.663032, acc.: 61.72%] [G loss: 1.055924]\n",
      "epoch:1 step:1026 [D loss: 0.625630, acc.: 64.06%] [G loss: 1.207051]\n",
      "epoch:1 step:1027 [D loss: 0.665776, acc.: 62.50%] [G loss: 1.266392]\n",
      "epoch:1 step:1028 [D loss: 0.689002, acc.: 52.34%] [G loss: 1.224805]\n",
      "epoch:1 step:1029 [D loss: 0.593426, acc.: 68.75%] [G loss: 1.093644]\n",
      "epoch:1 step:1030 [D loss: 0.700377, acc.: 60.16%] [G loss: 1.035630]\n",
      "epoch:1 step:1031 [D loss: 0.712273, acc.: 50.78%] [G loss: 0.968544]\n",
      "epoch:1 step:1032 [D loss: 0.690855, acc.: 51.56%] [G loss: 1.199765]\n",
      "epoch:1 step:1033 [D loss: 0.675595, acc.: 58.59%] [G loss: 1.035587]\n",
      "epoch:1 step:1034 [D loss: 0.683094, acc.: 54.69%] [G loss: 1.190467]\n",
      "epoch:1 step:1035 [D loss: 0.624840, acc.: 71.09%] [G loss: 1.104182]\n",
      "epoch:1 step:1036 [D loss: 0.710378, acc.: 60.16%] [G loss: 1.199341]\n",
      "epoch:1 step:1037 [D loss: 0.582255, acc.: 72.66%] [G loss: 1.180955]\n",
      "epoch:1 step:1038 [D loss: 0.631313, acc.: 64.84%] [G loss: 0.971298]\n",
      "epoch:1 step:1039 [D loss: 0.776224, acc.: 50.00%] [G loss: 1.140284]\n",
      "epoch:1 step:1040 [D loss: 0.596329, acc.: 68.75%] [G loss: 1.117324]\n",
      "epoch:1 step:1041 [D loss: 0.682032, acc.: 57.03%] [G loss: 1.133767]\n",
      "epoch:1 step:1042 [D loss: 0.609504, acc.: 65.62%] [G loss: 1.030797]\n",
      "epoch:1 step:1043 [D loss: 0.741962, acc.: 51.56%] [G loss: 1.123010]\n",
      "epoch:1 step:1044 [D loss: 0.614070, acc.: 71.09%] [G loss: 1.137255]\n",
      "epoch:1 step:1045 [D loss: 0.677321, acc.: 58.59%] [G loss: 0.967657]\n",
      "epoch:1 step:1046 [D loss: 0.684759, acc.: 55.47%] [G loss: 1.013108]\n",
      "epoch:1 step:1047 [D loss: 0.714453, acc.: 52.34%] [G loss: 1.112892]\n",
      "epoch:1 step:1048 [D loss: 0.699404, acc.: 56.25%] [G loss: 1.186258]\n",
      "epoch:1 step:1049 [D loss: 0.690302, acc.: 52.34%] [G loss: 1.070147]\n",
      "epoch:1 step:1050 [D loss: 0.730262, acc.: 46.09%] [G loss: 0.981420]\n",
      "epoch:1 step:1051 [D loss: 0.659928, acc.: 62.50%] [G loss: 0.964858]\n",
      "epoch:1 step:1052 [D loss: 0.688412, acc.: 54.69%] [G loss: 1.118143]\n",
      "epoch:1 step:1053 [D loss: 0.732710, acc.: 49.22%] [G loss: 1.019784]\n",
      "epoch:1 step:1054 [D loss: 0.718864, acc.: 56.25%] [G loss: 0.958843]\n",
      "epoch:1 step:1055 [D loss: 0.704349, acc.: 54.69%] [G loss: 1.020288]\n",
      "epoch:1 step:1056 [D loss: 0.693484, acc.: 56.25%] [G loss: 1.047425]\n",
      "epoch:1 step:1057 [D loss: 0.656545, acc.: 64.06%] [G loss: 1.058407]\n",
      "epoch:1 step:1058 [D loss: 0.647661, acc.: 57.81%] [G loss: 0.943006]\n",
      "epoch:1 step:1059 [D loss: 0.691322, acc.: 62.50%] [G loss: 1.098992]\n",
      "epoch:1 step:1060 [D loss: 0.704201, acc.: 52.34%] [G loss: 1.002369]\n",
      "epoch:1 step:1061 [D loss: 0.627217, acc.: 63.28%] [G loss: 1.093481]\n",
      "epoch:1 step:1062 [D loss: 0.634547, acc.: 61.72%] [G loss: 1.050157]\n",
      "epoch:1 step:1063 [D loss: 0.778221, acc.: 49.22%] [G loss: 1.034053]\n",
      "epoch:1 step:1064 [D loss: 0.714241, acc.: 52.34%] [G loss: 1.029978]\n",
      "epoch:1 step:1065 [D loss: 0.664073, acc.: 62.50%] [G loss: 1.074949]\n",
      "epoch:1 step:1066 [D loss: 0.747336, acc.: 43.75%] [G loss: 1.014162]\n",
      "epoch:1 step:1067 [D loss: 0.759402, acc.: 42.19%] [G loss: 1.130269]\n",
      "epoch:1 step:1068 [D loss: 0.662330, acc.: 61.72%] [G loss: 0.955313]\n",
      "epoch:1 step:1069 [D loss: 0.727755, acc.: 53.12%] [G loss: 1.008310]\n",
      "epoch:1 step:1070 [D loss: 0.740553, acc.: 51.56%] [G loss: 1.177373]\n",
      "epoch:1 step:1071 [D loss: 0.640217, acc.: 64.84%] [G loss: 1.031235]\n",
      "epoch:1 step:1072 [D loss: 0.739909, acc.: 45.31%] [G loss: 1.112580]\n",
      "epoch:1 step:1073 [D loss: 0.712789, acc.: 57.03%] [G loss: 1.044664]\n",
      "epoch:1 step:1074 [D loss: 0.642864, acc.: 65.62%] [G loss: 1.025363]\n",
      "epoch:1 step:1075 [D loss: 0.730882, acc.: 54.69%] [G loss: 0.926785]\n",
      "epoch:1 step:1076 [D loss: 0.702853, acc.: 57.81%] [G loss: 0.935401]\n",
      "epoch:1 step:1077 [D loss: 0.571626, acc.: 65.62%] [G loss: 1.085144]\n",
      "epoch:1 step:1078 [D loss: 0.648912, acc.: 64.06%] [G loss: 1.006063]\n",
      "epoch:1 step:1079 [D loss: 0.636644, acc.: 63.28%] [G loss: 1.134486]\n",
      "epoch:1 step:1080 [D loss: 0.713492, acc.: 53.91%] [G loss: 1.148892]\n",
      "epoch:1 step:1081 [D loss: 0.646694, acc.: 62.50%] [G loss: 1.073048]\n",
      "epoch:1 step:1082 [D loss: 0.674011, acc.: 57.03%] [G loss: 1.028625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1083 [D loss: 0.663739, acc.: 60.16%] [G loss: 1.052934]\n",
      "epoch:1 step:1084 [D loss: 0.673552, acc.: 53.91%] [G loss: 1.036386]\n",
      "epoch:1 step:1085 [D loss: 0.701970, acc.: 52.34%] [G loss: 1.110222]\n",
      "epoch:1 step:1086 [D loss: 0.668313, acc.: 64.84%] [G loss: 1.046288]\n",
      "epoch:1 step:1087 [D loss: 0.672949, acc.: 58.59%] [G loss: 1.143915]\n",
      "epoch:1 step:1088 [D loss: 0.676869, acc.: 58.59%] [G loss: 1.153493]\n",
      "epoch:1 step:1089 [D loss: 0.647062, acc.: 64.84%] [G loss: 1.086791]\n",
      "epoch:1 step:1090 [D loss: 0.657375, acc.: 65.62%] [G loss: 1.012437]\n",
      "epoch:1 step:1091 [D loss: 0.739925, acc.: 49.22%] [G loss: 0.851281]\n",
      "epoch:1 step:1092 [D loss: 0.759244, acc.: 49.22%] [G loss: 1.030439]\n",
      "epoch:1 step:1093 [D loss: 0.665642, acc.: 60.16%] [G loss: 0.988890]\n",
      "epoch:1 step:1094 [D loss: 0.644438, acc.: 63.28%] [G loss: 1.036323]\n",
      "epoch:1 step:1095 [D loss: 0.569911, acc.: 73.44%] [G loss: 1.175649]\n",
      "epoch:1 step:1096 [D loss: 0.652058, acc.: 61.72%] [G loss: 1.072190]\n",
      "epoch:1 step:1097 [D loss: 0.642196, acc.: 63.28%] [G loss: 1.065773]\n",
      "epoch:1 step:1098 [D loss: 0.604567, acc.: 67.97%] [G loss: 1.136644]\n",
      "epoch:1 step:1099 [D loss: 0.664856, acc.: 56.25%] [G loss: 1.446025]\n",
      "epoch:1 step:1100 [D loss: 0.768577, acc.: 50.78%] [G loss: 1.056579]\n",
      "epoch:1 step:1101 [D loss: 0.631732, acc.: 61.72%] [G loss: 1.122083]\n",
      "epoch:1 step:1102 [D loss: 0.755060, acc.: 44.53%] [G loss: 0.963384]\n",
      "epoch:1 step:1103 [D loss: 0.715645, acc.: 53.91%] [G loss: 0.957323]\n",
      "epoch:1 step:1104 [D loss: 0.631106, acc.: 64.84%] [G loss: 1.202790]\n",
      "epoch:1 step:1105 [D loss: 0.640626, acc.: 56.25%] [G loss: 1.301493]\n",
      "epoch:1 step:1106 [D loss: 0.632349, acc.: 61.72%] [G loss: 1.403454]\n",
      "epoch:1 step:1107 [D loss: 0.643229, acc.: 62.50%] [G loss: 1.008394]\n",
      "epoch:1 step:1108 [D loss: 0.738242, acc.: 55.47%] [G loss: 0.981912]\n",
      "epoch:1 step:1109 [D loss: 0.678232, acc.: 59.38%] [G loss: 1.030788]\n",
      "epoch:1 step:1110 [D loss: 0.607634, acc.: 64.84%] [G loss: 1.047288]\n",
      "epoch:1 step:1111 [D loss: 0.600958, acc.: 70.31%] [G loss: 1.261369]\n",
      "epoch:1 step:1112 [D loss: 0.707515, acc.: 53.91%] [G loss: 1.082457]\n",
      "epoch:1 step:1113 [D loss: 0.704939, acc.: 51.56%] [G loss: 0.941935]\n",
      "epoch:1 step:1114 [D loss: 0.637919, acc.: 64.84%] [G loss: 1.027469]\n",
      "epoch:1 step:1115 [D loss: 0.624877, acc.: 63.28%] [G loss: 1.043860]\n",
      "epoch:1 step:1116 [D loss: 0.713705, acc.: 53.91%] [G loss: 1.009169]\n",
      "epoch:1 step:1117 [D loss: 0.631630, acc.: 57.81%] [G loss: 1.021071]\n",
      "epoch:1 step:1118 [D loss: 0.681293, acc.: 51.56%] [G loss: 1.008168]\n",
      "epoch:1 step:1119 [D loss: 0.689162, acc.: 60.94%] [G loss: 0.969186]\n",
      "epoch:1 step:1120 [D loss: 0.682951, acc.: 60.16%] [G loss: 1.037920]\n",
      "epoch:1 step:1121 [D loss: 0.680354, acc.: 50.00%] [G loss: 0.996934]\n",
      "epoch:1 step:1122 [D loss: 0.715680, acc.: 51.56%] [G loss: 1.039850]\n",
      "epoch:1 step:1123 [D loss: 0.701230, acc.: 52.34%] [G loss: 1.051983]\n",
      "epoch:1 step:1124 [D loss: 0.621437, acc.: 62.50%] [G loss: 0.995726]\n",
      "epoch:1 step:1125 [D loss: 0.666379, acc.: 53.91%] [G loss: 1.022798]\n",
      "epoch:1 step:1126 [D loss: 0.677694, acc.: 57.81%] [G loss: 0.965374]\n",
      "epoch:1 step:1127 [D loss: 0.707564, acc.: 53.12%] [G loss: 1.121496]\n",
      "epoch:1 step:1128 [D loss: 0.675305, acc.: 58.59%] [G loss: 1.090784]\n",
      "epoch:1 step:1129 [D loss: 0.704660, acc.: 51.56%] [G loss: 0.991906]\n",
      "epoch:1 step:1130 [D loss: 0.599014, acc.: 68.75%] [G loss: 1.115419]\n",
      "epoch:1 step:1131 [D loss: 0.689750, acc.: 58.59%] [G loss: 0.999578]\n",
      "epoch:1 step:1132 [D loss: 0.686820, acc.: 55.47%] [G loss: 1.049221]\n",
      "epoch:1 step:1133 [D loss: 0.670706, acc.: 53.12%] [G loss: 1.015802]\n",
      "epoch:1 step:1134 [D loss: 0.656663, acc.: 66.41%] [G loss: 1.077512]\n",
      "epoch:1 step:1135 [D loss: 0.646740, acc.: 61.72%] [G loss: 1.037021]\n",
      "epoch:1 step:1136 [D loss: 0.644601, acc.: 61.72%] [G loss: 0.974557]\n",
      "epoch:1 step:1137 [D loss: 0.637920, acc.: 61.72%] [G loss: 1.058139]\n",
      "epoch:1 step:1138 [D loss: 0.718711, acc.: 56.25%] [G loss: 0.843159]\n",
      "epoch:1 step:1139 [D loss: 0.731734, acc.: 49.22%] [G loss: 0.981753]\n",
      "epoch:1 step:1140 [D loss: 0.653033, acc.: 56.25%] [G loss: 0.910859]\n",
      "epoch:1 step:1141 [D loss: 0.713926, acc.: 49.22%] [G loss: 0.983242]\n",
      "epoch:1 step:1142 [D loss: 0.695529, acc.: 53.91%] [G loss: 0.964743]\n",
      "epoch:1 step:1143 [D loss: 0.647337, acc.: 57.03%] [G loss: 1.135399]\n",
      "epoch:1 step:1144 [D loss: 0.608815, acc.: 65.62%] [G loss: 1.041833]\n",
      "epoch:1 step:1145 [D loss: 0.683068, acc.: 55.47%] [G loss: 0.983635]\n",
      "epoch:1 step:1146 [D loss: 0.664281, acc.: 58.59%] [G loss: 1.124940]\n",
      "epoch:1 step:1147 [D loss: 0.650687, acc.: 58.59%] [G loss: 1.013720]\n",
      "epoch:1 step:1148 [D loss: 0.704535, acc.: 57.03%] [G loss: 1.107314]\n",
      "epoch:1 step:1149 [D loss: 0.759040, acc.: 48.44%] [G loss: 0.890039]\n",
      "epoch:1 step:1150 [D loss: 0.717569, acc.: 52.34%] [G loss: 1.120192]\n",
      "epoch:1 step:1151 [D loss: 0.765519, acc.: 42.19%] [G loss: 1.046340]\n",
      "epoch:1 step:1152 [D loss: 0.683831, acc.: 58.59%] [G loss: 1.141032]\n",
      "epoch:1 step:1153 [D loss: 0.722791, acc.: 50.78%] [G loss: 1.069952]\n",
      "epoch:1 step:1154 [D loss: 0.613810, acc.: 65.62%] [G loss: 1.025774]\n",
      "epoch:1 step:1155 [D loss: 0.681847, acc.: 55.47%] [G loss: 1.142158]\n",
      "epoch:1 step:1156 [D loss: 0.622058, acc.: 64.84%] [G loss: 1.031365]\n",
      "epoch:1 step:1157 [D loss: 0.701264, acc.: 54.69%] [G loss: 0.878135]\n",
      "epoch:1 step:1158 [D loss: 0.610107, acc.: 65.62%] [G loss: 1.007358]\n",
      "epoch:1 step:1159 [D loss: 0.722563, acc.: 52.34%] [G loss: 0.908887]\n",
      "epoch:1 step:1160 [D loss: 0.681498, acc.: 57.81%] [G loss: 1.062121]\n",
      "epoch:1 step:1161 [D loss: 0.735949, acc.: 54.69%] [G loss: 1.016385]\n",
      "epoch:1 step:1162 [D loss: 0.739114, acc.: 46.88%] [G loss: 0.966810]\n",
      "epoch:1 step:1163 [D loss: 0.639447, acc.: 64.84%] [G loss: 1.149705]\n",
      "epoch:1 step:1164 [D loss: 0.689864, acc.: 54.69%] [G loss: 0.999044]\n",
      "epoch:1 step:1165 [D loss: 0.626158, acc.: 65.62%] [G loss: 0.942040]\n",
      "epoch:1 step:1166 [D loss: 0.763943, acc.: 44.53%] [G loss: 1.002753]\n",
      "epoch:1 step:1167 [D loss: 0.727880, acc.: 49.22%] [G loss: 0.885245]\n",
      "epoch:1 step:1168 [D loss: 0.655946, acc.: 57.81%] [G loss: 0.985890]\n",
      "epoch:1 step:1169 [D loss: 0.674238, acc.: 67.97%] [G loss: 1.064110]\n",
      "epoch:1 step:1170 [D loss: 0.708055, acc.: 55.47%] [G loss: 1.016582]\n",
      "epoch:1 step:1171 [D loss: 0.687695, acc.: 47.66%] [G loss: 0.931483]\n",
      "epoch:1 step:1172 [D loss: 0.714537, acc.: 53.91%] [G loss: 0.940765]\n",
      "epoch:1 step:1173 [D loss: 0.730817, acc.: 47.66%] [G loss: 0.982322]\n",
      "epoch:1 step:1174 [D loss: 0.754729, acc.: 46.88%] [G loss: 0.965907]\n",
      "epoch:1 step:1175 [D loss: 0.692629, acc.: 59.38%] [G loss: 0.910085]\n",
      "epoch:1 step:1176 [D loss: 0.700918, acc.: 57.03%] [G loss: 0.977733]\n",
      "epoch:1 step:1177 [D loss: 0.703947, acc.: 58.59%] [G loss: 1.055372]\n",
      "epoch:1 step:1178 [D loss: 0.642571, acc.: 61.72%] [G loss: 1.050293]\n",
      "epoch:1 step:1179 [D loss: 0.640233, acc.: 59.38%] [G loss: 1.080601]\n",
      "epoch:1 step:1180 [D loss: 0.695494, acc.: 57.81%] [G loss: 0.944472]\n",
      "epoch:1 step:1181 [D loss: 0.703211, acc.: 56.25%] [G loss: 0.957283]\n",
      "epoch:1 step:1182 [D loss: 0.704505, acc.: 53.12%] [G loss: 0.934630]\n",
      "epoch:1 step:1183 [D loss: 0.726126, acc.: 50.78%] [G loss: 0.823175]\n",
      "epoch:1 step:1184 [D loss: 0.625789, acc.: 64.84%] [G loss: 0.979633]\n",
      "epoch:1 step:1185 [D loss: 0.733067, acc.: 52.34%] [G loss: 1.069634]\n",
      "epoch:1 step:1186 [D loss: 0.798761, acc.: 36.72%] [G loss: 0.904253]\n",
      "epoch:1 step:1187 [D loss: 0.669226, acc.: 57.81%] [G loss: 0.994525]\n",
      "epoch:1 step:1188 [D loss: 0.693266, acc.: 53.12%] [G loss: 0.984771]\n",
      "epoch:1 step:1189 [D loss: 0.712524, acc.: 55.47%] [G loss: 0.930442]\n",
      "epoch:1 step:1190 [D loss: 0.674977, acc.: 56.25%] [G loss: 1.138343]\n",
      "epoch:1 step:1191 [D loss: 0.713871, acc.: 52.34%] [G loss: 0.981688]\n",
      "epoch:1 step:1192 [D loss: 0.699086, acc.: 54.69%] [G loss: 1.091317]\n",
      "epoch:1 step:1193 [D loss: 0.666866, acc.: 60.94%] [G loss: 0.901516]\n",
      "epoch:1 step:1194 [D loss: 0.671550, acc.: 60.16%] [G loss: 0.952826]\n",
      "epoch:1 step:1195 [D loss: 0.665172, acc.: 50.78%] [G loss: 0.928404]\n",
      "epoch:1 step:1196 [D loss: 0.682475, acc.: 59.38%] [G loss: 0.924150]\n",
      "epoch:1 step:1197 [D loss: 0.703598, acc.: 53.12%] [G loss: 1.011048]\n",
      "epoch:1 step:1198 [D loss: 0.706972, acc.: 55.47%] [G loss: 1.002960]\n",
      "epoch:1 step:1199 [D loss: 0.690420, acc.: 56.25%] [G loss: 0.977274]\n",
      "epoch:1 step:1200 [D loss: 0.666494, acc.: 53.91%] [G loss: 1.011185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1201 [D loss: 0.652073, acc.: 57.03%] [G loss: 1.030870]\n",
      "epoch:1 step:1202 [D loss: 0.652986, acc.: 64.84%] [G loss: 0.944291]\n",
      "epoch:1 step:1203 [D loss: 0.645504, acc.: 64.84%] [G loss: 0.891313]\n",
      "epoch:1 step:1204 [D loss: 0.649341, acc.: 59.38%] [G loss: 1.014347]\n",
      "epoch:1 step:1205 [D loss: 0.648277, acc.: 60.16%] [G loss: 1.080266]\n",
      "epoch:1 step:1206 [D loss: 0.667423, acc.: 61.72%] [G loss: 0.912796]\n",
      "epoch:1 step:1207 [D loss: 0.633414, acc.: 65.62%] [G loss: 1.002409]\n",
      "epoch:1 step:1208 [D loss: 0.617721, acc.: 64.84%] [G loss: 0.966458]\n",
      "epoch:1 step:1209 [D loss: 0.747986, acc.: 48.44%] [G loss: 0.851291]\n",
      "epoch:1 step:1210 [D loss: 0.672411, acc.: 60.94%] [G loss: 1.001168]\n",
      "epoch:1 step:1211 [D loss: 0.713105, acc.: 51.56%] [G loss: 1.006834]\n",
      "epoch:1 step:1212 [D loss: 0.719218, acc.: 50.00%] [G loss: 0.958520]\n",
      "epoch:1 step:1213 [D loss: 0.677769, acc.: 58.59%] [G loss: 1.022089]\n",
      "epoch:1 step:1214 [D loss: 0.722191, acc.: 50.78%] [G loss: 0.946971]\n",
      "epoch:1 step:1215 [D loss: 0.719733, acc.: 52.34%] [G loss: 0.926538]\n",
      "epoch:1 step:1216 [D loss: 0.712813, acc.: 56.25%] [G loss: 0.907810]\n",
      "epoch:1 step:1217 [D loss: 0.685052, acc.: 57.81%] [G loss: 0.952066]\n",
      "epoch:1 step:1218 [D loss: 0.738827, acc.: 42.19%] [G loss: 0.973474]\n",
      "epoch:1 step:1219 [D loss: 0.616112, acc.: 67.19%] [G loss: 0.965455]\n",
      "epoch:1 step:1220 [D loss: 0.625391, acc.: 59.38%] [G loss: 0.947830]\n",
      "epoch:1 step:1221 [D loss: 0.698902, acc.: 49.22%] [G loss: 1.030513]\n",
      "epoch:1 step:1222 [D loss: 0.608357, acc.: 65.62%] [G loss: 1.129439]\n",
      "epoch:1 step:1223 [D loss: 0.730359, acc.: 48.44%] [G loss: 0.915754]\n",
      "epoch:1 step:1224 [D loss: 0.727866, acc.: 50.00%] [G loss: 0.920750]\n",
      "epoch:1 step:1225 [D loss: 0.712968, acc.: 50.78%] [G loss: 0.926762]\n",
      "epoch:1 step:1226 [D loss: 0.744474, acc.: 43.75%] [G loss: 0.881386]\n",
      "epoch:1 step:1227 [D loss: 0.663782, acc.: 56.25%] [G loss: 0.782160]\n",
      "epoch:1 step:1228 [D loss: 0.655267, acc.: 60.16%] [G loss: 0.976071]\n",
      "epoch:1 step:1229 [D loss: 0.707889, acc.: 55.47%] [G loss: 0.988921]\n",
      "epoch:1 step:1230 [D loss: 0.642611, acc.: 63.28%] [G loss: 1.080917]\n",
      "epoch:1 step:1231 [D loss: 0.677030, acc.: 53.12%] [G loss: 0.931863]\n",
      "epoch:1 step:1232 [D loss: 0.676349, acc.: 56.25%] [G loss: 1.022729]\n",
      "epoch:1 step:1233 [D loss: 0.657332, acc.: 55.47%] [G loss: 1.087987]\n",
      "epoch:1 step:1234 [D loss: 0.666688, acc.: 58.59%] [G loss: 1.057311]\n",
      "epoch:1 step:1235 [D loss: 0.682075, acc.: 57.03%] [G loss: 0.945988]\n",
      "epoch:1 step:1236 [D loss: 0.669783, acc.: 64.06%] [G loss: 0.891871]\n",
      "epoch:1 step:1237 [D loss: 0.608406, acc.: 67.97%] [G loss: 0.850238]\n",
      "epoch:1 step:1238 [D loss: 0.744299, acc.: 48.44%] [G loss: 0.908575]\n",
      "epoch:1 step:1239 [D loss: 0.719671, acc.: 57.03%] [G loss: 0.970134]\n",
      "epoch:1 step:1240 [D loss: 0.764718, acc.: 40.62%] [G loss: 1.078130]\n",
      "epoch:1 step:1241 [D loss: 0.703411, acc.: 57.03%] [G loss: 1.129714]\n",
      "epoch:1 step:1242 [D loss: 0.706252, acc.: 50.78%] [G loss: 0.966452]\n",
      "epoch:1 step:1243 [D loss: 0.705164, acc.: 55.47%] [G loss: 0.919559]\n",
      "epoch:1 step:1244 [D loss: 0.684481, acc.: 52.34%] [G loss: 0.925968]\n",
      "epoch:1 step:1245 [D loss: 0.643170, acc.: 57.03%] [G loss: 1.044857]\n",
      "epoch:1 step:1246 [D loss: 0.668446, acc.: 57.03%] [G loss: 1.016178]\n",
      "epoch:1 step:1247 [D loss: 0.659542, acc.: 60.16%] [G loss: 0.984852]\n",
      "epoch:1 step:1248 [D loss: 0.686853, acc.: 57.03%] [G loss: 0.936748]\n",
      "epoch:1 step:1249 [D loss: 0.674491, acc.: 56.25%] [G loss: 0.950889]\n",
      "epoch:1 step:1250 [D loss: 0.655034, acc.: 57.81%] [G loss: 0.925538]\n",
      "epoch:1 step:1251 [D loss: 0.690357, acc.: 58.59%] [G loss: 1.029575]\n",
      "epoch:1 step:1252 [D loss: 0.656823, acc.: 64.84%] [G loss: 0.925101]\n",
      "epoch:1 step:1253 [D loss: 0.782966, acc.: 45.31%] [G loss: 0.927539]\n",
      "epoch:1 step:1254 [D loss: 0.743490, acc.: 53.12%] [G loss: 0.965084]\n",
      "epoch:1 step:1255 [D loss: 0.694911, acc.: 50.78%] [G loss: 0.923669]\n",
      "epoch:1 step:1256 [D loss: 0.641447, acc.: 64.06%] [G loss: 1.056254]\n",
      "epoch:1 step:1257 [D loss: 0.683310, acc.: 53.91%] [G loss: 0.892148]\n",
      "epoch:1 step:1258 [D loss: 0.694096, acc.: 49.22%] [G loss: 0.858299]\n",
      "epoch:1 step:1259 [D loss: 0.718722, acc.: 46.88%] [G loss: 0.965779]\n",
      "epoch:1 step:1260 [D loss: 0.733013, acc.: 49.22%] [G loss: 0.938169]\n",
      "epoch:1 step:1261 [D loss: 0.628788, acc.: 68.75%] [G loss: 0.965885]\n",
      "epoch:1 step:1262 [D loss: 0.680894, acc.: 51.56%] [G loss: 0.973564]\n",
      "epoch:1 step:1263 [D loss: 0.727881, acc.: 46.09%] [G loss: 0.867772]\n",
      "epoch:1 step:1264 [D loss: 0.667809, acc.: 56.25%] [G loss: 0.901515]\n",
      "epoch:1 step:1265 [D loss: 0.665744, acc.: 54.69%] [G loss: 0.919847]\n",
      "epoch:1 step:1266 [D loss: 0.701783, acc.: 50.00%] [G loss: 0.969305]\n",
      "epoch:1 step:1267 [D loss: 0.711469, acc.: 48.44%] [G loss: 0.919198]\n",
      "epoch:1 step:1268 [D loss: 0.703362, acc.: 53.12%] [G loss: 0.887949]\n",
      "epoch:1 step:1269 [D loss: 0.657719, acc.: 64.84%] [G loss: 1.015817]\n",
      "epoch:1 step:1270 [D loss: 0.683962, acc.: 55.47%] [G loss: 0.965620]\n",
      "epoch:1 step:1271 [D loss: 0.699808, acc.: 48.44%] [G loss: 0.820736]\n",
      "epoch:1 step:1272 [D loss: 0.684945, acc.: 58.59%] [G loss: 1.015797]\n",
      "epoch:1 step:1273 [D loss: 0.601218, acc.: 68.75%] [G loss: 1.014021]\n",
      "epoch:1 step:1274 [D loss: 0.720928, acc.: 52.34%] [G loss: 0.914755]\n",
      "epoch:1 step:1275 [D loss: 0.696028, acc.: 53.12%] [G loss: 0.930785]\n",
      "epoch:1 step:1276 [D loss: 0.736595, acc.: 50.00%] [G loss: 0.907503]\n",
      "epoch:1 step:1277 [D loss: 0.696908, acc.: 56.25%] [G loss: 0.908227]\n",
      "epoch:1 step:1278 [D loss: 0.670218, acc.: 57.81%] [G loss: 0.845668]\n",
      "epoch:1 step:1279 [D loss: 0.769754, acc.: 46.09%] [G loss: 0.978305]\n",
      "epoch:1 step:1280 [D loss: 0.638663, acc.: 59.38%] [G loss: 1.054798]\n",
      "epoch:1 step:1281 [D loss: 0.693808, acc.: 50.78%] [G loss: 1.066450]\n",
      "epoch:1 step:1282 [D loss: 0.679659, acc.: 57.03%] [G loss: 0.862447]\n",
      "epoch:1 step:1283 [D loss: 0.621889, acc.: 68.75%] [G loss: 0.958597]\n",
      "epoch:1 step:1284 [D loss: 0.670793, acc.: 62.50%] [G loss: 0.953673]\n",
      "epoch:1 step:1285 [D loss: 0.714024, acc.: 54.69%] [G loss: 0.877299]\n",
      "epoch:1 step:1286 [D loss: 0.693347, acc.: 53.12%] [G loss: 0.853313]\n",
      "epoch:1 step:1287 [D loss: 0.736290, acc.: 51.56%] [G loss: 0.960190]\n",
      "epoch:1 step:1288 [D loss: 0.719860, acc.: 51.56%] [G loss: 0.962763]\n",
      "epoch:1 step:1289 [D loss: 0.721776, acc.: 48.44%] [G loss: 0.972413]\n",
      "epoch:1 step:1290 [D loss: 0.674078, acc.: 61.72%] [G loss: 1.030567]\n",
      "epoch:1 step:1291 [D loss: 0.722453, acc.: 55.47%] [G loss: 1.014350]\n",
      "epoch:1 step:1292 [D loss: 0.706651, acc.: 55.47%] [G loss: 1.067899]\n",
      "epoch:1 step:1293 [D loss: 0.718259, acc.: 48.44%] [G loss: 0.982187]\n",
      "epoch:1 step:1294 [D loss: 0.700052, acc.: 53.91%] [G loss: 1.031805]\n",
      "epoch:1 step:1295 [D loss: 0.652485, acc.: 64.06%] [G loss: 1.041523]\n",
      "epoch:1 step:1296 [D loss: 0.711316, acc.: 47.66%] [G loss: 0.906095]\n",
      "epoch:1 step:1297 [D loss: 0.690982, acc.: 57.81%] [G loss: 0.990099]\n",
      "epoch:1 step:1298 [D loss: 0.663463, acc.: 55.47%] [G loss: 0.908142]\n",
      "epoch:1 step:1299 [D loss: 0.796146, acc.: 46.09%] [G loss: 0.827843]\n",
      "epoch:1 step:1300 [D loss: 0.641176, acc.: 65.62%] [G loss: 0.918680]\n",
      "epoch:1 step:1301 [D loss: 0.676851, acc.: 60.94%] [G loss: 0.924832]\n",
      "epoch:1 step:1302 [D loss: 0.645116, acc.: 64.84%] [G loss: 0.987576]\n",
      "epoch:1 step:1303 [D loss: 0.697419, acc.: 60.16%] [G loss: 0.872607]\n",
      "epoch:1 step:1304 [D loss: 0.677704, acc.: 54.69%] [G loss: 0.970569]\n",
      "epoch:1 step:1305 [D loss: 0.708138, acc.: 52.34%] [G loss: 0.917569]\n",
      "epoch:1 step:1306 [D loss: 0.663340, acc.: 57.03%] [G loss: 0.945136]\n",
      "epoch:1 step:1307 [D loss: 0.744968, acc.: 48.44%] [G loss: 0.933268]\n",
      "epoch:1 step:1308 [D loss: 0.635386, acc.: 60.94%] [G loss: 0.980291]\n",
      "epoch:1 step:1309 [D loss: 0.647392, acc.: 60.16%] [G loss: 0.967502]\n",
      "epoch:1 step:1310 [D loss: 0.727506, acc.: 42.97%] [G loss: 0.878386]\n",
      "epoch:1 step:1311 [D loss: 0.749890, acc.: 50.78%] [G loss: 0.924944]\n",
      "epoch:1 step:1312 [D loss: 0.673313, acc.: 62.50%] [G loss: 0.844725]\n",
      "epoch:1 step:1313 [D loss: 0.734807, acc.: 50.00%] [G loss: 0.932797]\n",
      "epoch:1 step:1314 [D loss: 0.703386, acc.: 48.44%] [G loss: 0.912719]\n",
      "epoch:1 step:1315 [D loss: 0.661517, acc.: 65.62%] [G loss: 0.954354]\n",
      "epoch:1 step:1316 [D loss: 0.693878, acc.: 50.00%] [G loss: 0.887485]\n",
      "epoch:1 step:1317 [D loss: 0.693395, acc.: 53.12%] [G loss: 0.980639]\n",
      "epoch:1 step:1318 [D loss: 0.662427, acc.: 60.16%] [G loss: 0.964940]\n",
      "epoch:1 step:1319 [D loss: 0.651851, acc.: 64.84%] [G loss: 0.919396]\n",
      "epoch:1 step:1320 [D loss: 0.669087, acc.: 57.81%] [G loss: 0.972989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1321 [D loss: 0.664841, acc.: 58.59%] [G loss: 0.860030]\n",
      "epoch:1 step:1322 [D loss: 0.688497, acc.: 59.38%] [G loss: 0.862892]\n",
      "epoch:1 step:1323 [D loss: 0.714786, acc.: 52.34%] [G loss: 0.911503]\n",
      "epoch:1 step:1324 [D loss: 0.711433, acc.: 50.78%] [G loss: 0.897975]\n",
      "epoch:1 step:1325 [D loss: 0.763333, acc.: 51.56%] [G loss: 0.888656]\n",
      "epoch:1 step:1326 [D loss: 0.729028, acc.: 43.75%] [G loss: 0.874981]\n",
      "epoch:1 step:1327 [D loss: 0.706780, acc.: 50.00%] [G loss: 0.864998]\n",
      "epoch:1 step:1328 [D loss: 0.685525, acc.: 53.12%] [G loss: 0.930470]\n",
      "epoch:1 step:1329 [D loss: 0.705326, acc.: 53.91%] [G loss: 0.922602]\n",
      "epoch:1 step:1330 [D loss: 0.648675, acc.: 62.50%] [G loss: 0.894714]\n",
      "epoch:1 step:1331 [D loss: 0.677698, acc.: 55.47%] [G loss: 0.933465]\n",
      "epoch:1 step:1332 [D loss: 0.662672, acc.: 60.94%] [G loss: 0.902153]\n",
      "epoch:1 step:1333 [D loss: 0.698644, acc.: 53.91%] [G loss: 0.865694]\n",
      "epoch:1 step:1334 [D loss: 0.653984, acc.: 60.94%] [G loss: 0.938937]\n",
      "epoch:1 step:1335 [D loss: 0.695702, acc.: 57.03%] [G loss: 0.911852]\n",
      "epoch:1 step:1336 [D loss: 0.651374, acc.: 62.50%] [G loss: 0.897082]\n",
      "epoch:1 step:1337 [D loss: 0.679708, acc.: 57.81%] [G loss: 0.903104]\n",
      "epoch:1 step:1338 [D loss: 0.663566, acc.: 53.91%] [G loss: 1.010118]\n",
      "epoch:1 step:1339 [D loss: 0.687445, acc.: 53.12%] [G loss: 0.872118]\n",
      "epoch:1 step:1340 [D loss: 0.712827, acc.: 46.09%] [G loss: 0.839041]\n",
      "epoch:1 step:1341 [D loss: 0.684756, acc.: 53.12%] [G loss: 0.894043]\n",
      "epoch:1 step:1342 [D loss: 0.672619, acc.: 57.03%] [G loss: 0.829081]\n",
      "epoch:1 step:1343 [D loss: 0.645954, acc.: 60.16%] [G loss: 0.836573]\n",
      "epoch:1 step:1344 [D loss: 0.628990, acc.: 67.97%] [G loss: 0.901883]\n",
      "epoch:1 step:1345 [D loss: 0.666161, acc.: 57.81%] [G loss: 0.906175]\n",
      "epoch:1 step:1346 [D loss: 0.759836, acc.: 43.75%] [G loss: 0.931655]\n",
      "epoch:1 step:1347 [D loss: 0.721434, acc.: 49.22%] [G loss: 1.029005]\n",
      "epoch:1 step:1348 [D loss: 0.707700, acc.: 50.78%] [G loss: 0.841712]\n",
      "epoch:1 step:1349 [D loss: 0.755899, acc.: 43.75%] [G loss: 0.957012]\n",
      "epoch:1 step:1350 [D loss: 0.753163, acc.: 51.56%] [G loss: 1.033590]\n",
      "epoch:1 step:1351 [D loss: 0.671881, acc.: 55.47%] [G loss: 0.878667]\n",
      "epoch:1 step:1352 [D loss: 0.636095, acc.: 65.62%] [G loss: 0.965292]\n",
      "epoch:1 step:1353 [D loss: 0.698808, acc.: 50.78%] [G loss: 0.869207]\n",
      "epoch:1 step:1354 [D loss: 0.666188, acc.: 59.38%] [G loss: 0.925138]\n",
      "epoch:1 step:1355 [D loss: 0.685245, acc.: 61.72%] [G loss: 0.933236]\n",
      "epoch:1 step:1356 [D loss: 0.712824, acc.: 49.22%] [G loss: 1.031433]\n",
      "epoch:1 step:1357 [D loss: 0.685304, acc.: 57.03%] [G loss: 0.998410]\n",
      "epoch:1 step:1358 [D loss: 0.717481, acc.: 53.12%] [G loss: 0.890394]\n",
      "epoch:1 step:1359 [D loss: 0.681184, acc.: 58.59%] [G loss: 0.920546]\n",
      "epoch:1 step:1360 [D loss: 0.712627, acc.: 54.69%] [G loss: 1.025816]\n",
      "epoch:1 step:1361 [D loss: 0.665374, acc.: 61.72%] [G loss: 0.972674]\n",
      "epoch:1 step:1362 [D loss: 0.715319, acc.: 57.81%] [G loss: 1.008583]\n",
      "epoch:1 step:1363 [D loss: 0.710333, acc.: 54.69%] [G loss: 0.929300]\n",
      "epoch:1 step:1364 [D loss: 0.713518, acc.: 52.34%] [G loss: 0.924327]\n",
      "epoch:1 step:1365 [D loss: 0.706984, acc.: 50.00%] [G loss: 0.912149]\n",
      "epoch:1 step:1366 [D loss: 0.698522, acc.: 57.03%] [G loss: 0.995552]\n",
      "epoch:1 step:1367 [D loss: 0.731195, acc.: 51.56%] [G loss: 0.858680]\n",
      "epoch:1 step:1368 [D loss: 0.701101, acc.: 53.91%] [G loss: 0.905933]\n",
      "epoch:1 step:1369 [D loss: 0.686705, acc.: 56.25%] [G loss: 0.829262]\n",
      "epoch:1 step:1370 [D loss: 0.664633, acc.: 60.94%] [G loss: 0.911913]\n",
      "epoch:1 step:1371 [D loss: 0.734430, acc.: 50.78%] [G loss: 0.914012]\n",
      "epoch:1 step:1372 [D loss: 0.660280, acc.: 66.41%] [G loss: 0.891457]\n",
      "epoch:1 step:1373 [D loss: 0.687952, acc.: 56.25%] [G loss: 0.853102]\n",
      "epoch:1 step:1374 [D loss: 0.679765, acc.: 49.22%] [G loss: 0.995906]\n",
      "epoch:1 step:1375 [D loss: 0.704689, acc.: 52.34%] [G loss: 0.928780]\n",
      "epoch:1 step:1376 [D loss: 0.671321, acc.: 57.81%] [G loss: 0.930534]\n",
      "epoch:1 step:1377 [D loss: 0.712205, acc.: 50.00%] [G loss: 0.865836]\n",
      "epoch:1 step:1378 [D loss: 0.700267, acc.: 48.44%] [G loss: 0.923577]\n",
      "epoch:1 step:1379 [D loss: 0.686573, acc.: 55.47%] [G loss: 0.985341]\n",
      "epoch:1 step:1380 [D loss: 0.665662, acc.: 60.16%] [G loss: 0.922412]\n",
      "epoch:1 step:1381 [D loss: 0.660929, acc.: 64.84%] [G loss: 0.954329]\n",
      "epoch:1 step:1382 [D loss: 0.666669, acc.: 60.16%] [G loss: 0.873392]\n",
      "epoch:1 step:1383 [D loss: 0.674690, acc.: 61.72%] [G loss: 0.872570]\n",
      "epoch:1 step:1384 [D loss: 0.672673, acc.: 53.91%] [G loss: 0.811728]\n",
      "epoch:1 step:1385 [D loss: 0.719338, acc.: 50.00%] [G loss: 0.930640]\n",
      "epoch:1 step:1386 [D loss: 0.705577, acc.: 53.12%] [G loss: 1.003738]\n",
      "epoch:1 step:1387 [D loss: 0.711726, acc.: 54.69%] [G loss: 0.921457]\n",
      "epoch:1 step:1388 [D loss: 0.651612, acc.: 57.81%] [G loss: 0.979034]\n",
      "epoch:1 step:1389 [D loss: 0.654096, acc.: 62.50%] [G loss: 0.957223]\n",
      "epoch:1 step:1390 [D loss: 0.629228, acc.: 63.28%] [G loss: 0.897539]\n",
      "epoch:1 step:1391 [D loss: 0.649577, acc.: 57.81%] [G loss: 0.861859]\n",
      "epoch:1 step:1392 [D loss: 0.685547, acc.: 53.91%] [G loss: 0.917187]\n",
      "epoch:1 step:1393 [D loss: 0.700509, acc.: 57.03%] [G loss: 1.010743]\n",
      "epoch:1 step:1394 [D loss: 0.745349, acc.: 42.97%] [G loss: 0.969633]\n",
      "epoch:1 step:1395 [D loss: 0.710073, acc.: 54.69%] [G loss: 0.853396]\n",
      "epoch:1 step:1396 [D loss: 0.650538, acc.: 60.16%] [G loss: 0.876418]\n",
      "epoch:1 step:1397 [D loss: 0.686208, acc.: 53.91%] [G loss: 0.878346]\n",
      "epoch:1 step:1398 [D loss: 0.692303, acc.: 57.81%] [G loss: 0.913843]\n",
      "epoch:1 step:1399 [D loss: 0.716631, acc.: 48.44%] [G loss: 0.871309]\n",
      "epoch:1 step:1400 [D loss: 0.658252, acc.: 64.84%] [G loss: 0.900604]\n",
      "epoch:1 step:1401 [D loss: 0.654069, acc.: 57.81%] [G loss: 0.869109]\n",
      "epoch:1 step:1402 [D loss: 0.675052, acc.: 59.38%] [G loss: 0.936384]\n",
      "epoch:1 step:1403 [D loss: 0.729664, acc.: 49.22%] [G loss: 0.872800]\n",
      "epoch:1 step:1404 [D loss: 0.705254, acc.: 54.69%] [G loss: 0.812745]\n",
      "epoch:1 step:1405 [D loss: 0.681694, acc.: 57.03%] [G loss: 0.855809]\n",
      "epoch:1 step:1406 [D loss: 0.709025, acc.: 51.56%] [G loss: 0.975115]\n",
      "epoch:1 step:1407 [D loss: 0.647841, acc.: 59.38%] [G loss: 1.026090]\n",
      "epoch:1 step:1408 [D loss: 0.626621, acc.: 64.06%] [G loss: 0.930880]\n",
      "epoch:1 step:1409 [D loss: 0.700365, acc.: 53.12%] [G loss: 1.025025]\n",
      "epoch:1 step:1410 [D loss: 0.675679, acc.: 64.06%] [G loss: 0.927475]\n",
      "epoch:1 step:1411 [D loss: 0.642651, acc.: 66.41%] [G loss: 0.863311]\n",
      "epoch:1 step:1412 [D loss: 0.671220, acc.: 59.38%] [G loss: 0.831978]\n",
      "epoch:1 step:1413 [D loss: 0.642466, acc.: 60.94%] [G loss: 0.846752]\n",
      "epoch:1 step:1414 [D loss: 0.660452, acc.: 60.16%] [G loss: 0.850093]\n",
      "epoch:1 step:1415 [D loss: 0.712267, acc.: 52.34%] [G loss: 0.840515]\n",
      "epoch:1 step:1416 [D loss: 0.693985, acc.: 55.47%] [G loss: 0.928921]\n",
      "epoch:1 step:1417 [D loss: 0.671375, acc.: 56.25%] [G loss: 0.864029]\n",
      "epoch:1 step:1418 [D loss: 0.693161, acc.: 50.78%] [G loss: 0.951574]\n",
      "epoch:1 step:1419 [D loss: 0.641675, acc.: 61.72%] [G loss: 1.062906]\n",
      "epoch:1 step:1420 [D loss: 0.700825, acc.: 57.81%] [G loss: 0.895286]\n",
      "epoch:1 step:1421 [D loss: 0.752035, acc.: 49.22%] [G loss: 0.886479]\n",
      "epoch:1 step:1422 [D loss: 0.648788, acc.: 61.72%] [G loss: 1.045676]\n",
      "epoch:1 step:1423 [D loss: 0.661526, acc.: 62.50%] [G loss: 0.821241]\n",
      "epoch:1 step:1424 [D loss: 0.709394, acc.: 51.56%] [G loss: 0.856852]\n",
      "epoch:1 step:1425 [D loss: 0.672419, acc.: 55.47%] [G loss: 0.922959]\n",
      "epoch:1 step:1426 [D loss: 0.686408, acc.: 56.25%] [G loss: 0.920729]\n",
      "epoch:1 step:1427 [D loss: 0.680888, acc.: 55.47%] [G loss: 0.860704]\n",
      "epoch:1 step:1428 [D loss: 0.685397, acc.: 58.59%] [G loss: 0.954818]\n",
      "epoch:1 step:1429 [D loss: 0.701808, acc.: 58.59%] [G loss: 1.030811]\n",
      "epoch:1 step:1430 [D loss: 0.694574, acc.: 52.34%] [G loss: 0.971953]\n",
      "epoch:1 step:1431 [D loss: 0.656023, acc.: 63.28%] [G loss: 0.934396]\n",
      "epoch:1 step:1432 [D loss: 0.706190, acc.: 50.78%] [G loss: 0.933175]\n",
      "epoch:1 step:1433 [D loss: 0.641829, acc.: 58.59%] [G loss: 0.951962]\n",
      "epoch:1 step:1434 [D loss: 0.703261, acc.: 50.78%] [G loss: 0.950809]\n",
      "epoch:1 step:1435 [D loss: 0.719104, acc.: 51.56%] [G loss: 0.964011]\n",
      "epoch:1 step:1436 [D loss: 0.706585, acc.: 56.25%] [G loss: 0.924099]\n",
      "epoch:1 step:1437 [D loss: 0.719222, acc.: 42.97%] [G loss: 0.931818]\n",
      "epoch:1 step:1438 [D loss: 0.669985, acc.: 56.25%] [G loss: 0.889280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1439 [D loss: 0.628455, acc.: 67.19%] [G loss: 0.904490]\n",
      "epoch:1 step:1440 [D loss: 0.698994, acc.: 56.25%] [G loss: 0.917670]\n",
      "epoch:1 step:1441 [D loss: 0.685590, acc.: 56.25%] [G loss: 0.975485]\n",
      "epoch:1 step:1442 [D loss: 0.666590, acc.: 56.25%] [G loss: 0.895642]\n",
      "epoch:1 step:1443 [D loss: 0.709217, acc.: 52.34%] [G loss: 0.851814]\n",
      "epoch:1 step:1444 [D loss: 0.721992, acc.: 46.88%] [G loss: 0.800169]\n",
      "epoch:1 step:1445 [D loss: 0.642277, acc.: 57.03%] [G loss: 1.006235]\n",
      "epoch:1 step:1446 [D loss: 0.697052, acc.: 58.59%] [G loss: 0.932086]\n",
      "epoch:1 step:1447 [D loss: 0.664812, acc.: 61.72%] [G loss: 0.891151]\n",
      "epoch:1 step:1448 [D loss: 0.660829, acc.: 57.81%] [G loss: 0.911651]\n",
      "epoch:1 step:1449 [D loss: 0.694157, acc.: 51.56%] [G loss: 0.896979]\n",
      "epoch:1 step:1450 [D loss: 0.680593, acc.: 58.59%] [G loss: 0.987724]\n",
      "epoch:1 step:1451 [D loss: 0.650834, acc.: 67.19%] [G loss: 0.928035]\n",
      "epoch:1 step:1452 [D loss: 0.639187, acc.: 67.97%] [G loss: 0.890696]\n",
      "epoch:1 step:1453 [D loss: 0.701285, acc.: 59.38%] [G loss: 0.976516]\n",
      "epoch:1 step:1454 [D loss: 0.685458, acc.: 50.78%] [G loss: 0.960875]\n",
      "epoch:1 step:1455 [D loss: 0.687037, acc.: 52.34%] [G loss: 0.961873]\n",
      "epoch:1 step:1456 [D loss: 0.683686, acc.: 60.94%] [G loss: 0.972135]\n",
      "epoch:1 step:1457 [D loss: 0.690516, acc.: 55.47%] [G loss: 0.834817]\n",
      "epoch:1 step:1458 [D loss: 0.688326, acc.: 56.25%] [G loss: 0.917572]\n",
      "epoch:1 step:1459 [D loss: 0.719266, acc.: 51.56%] [G loss: 0.856503]\n",
      "epoch:1 step:1460 [D loss: 0.617710, acc.: 68.75%] [G loss: 0.945726]\n",
      "epoch:1 step:1461 [D loss: 0.665954, acc.: 50.78%] [G loss: 0.863688]\n",
      "epoch:1 step:1462 [D loss: 0.714801, acc.: 55.47%] [G loss: 1.002823]\n",
      "epoch:1 step:1463 [D loss: 0.703854, acc.: 53.12%] [G loss: 0.958370]\n",
      "epoch:1 step:1464 [D loss: 0.647045, acc.: 65.62%] [G loss: 0.958557]\n",
      "epoch:1 step:1465 [D loss: 0.659928, acc.: 63.28%] [G loss: 0.973977]\n",
      "epoch:1 step:1466 [D loss: 0.671832, acc.: 54.69%] [G loss: 0.992257]\n",
      "epoch:1 step:1467 [D loss: 0.661519, acc.: 57.03%] [G loss: 1.061109]\n",
      "epoch:1 step:1468 [D loss: 0.677605, acc.: 60.16%] [G loss: 0.808875]\n",
      "epoch:1 step:1469 [D loss: 0.679432, acc.: 54.69%] [G loss: 0.860128]\n",
      "epoch:1 step:1470 [D loss: 0.708679, acc.: 54.69%] [G loss: 1.044066]\n",
      "epoch:1 step:1471 [D loss: 0.646124, acc.: 62.50%] [G loss: 0.942574]\n",
      "epoch:1 step:1472 [D loss: 0.672450, acc.: 53.91%] [G loss: 0.830378]\n",
      "epoch:1 step:1473 [D loss: 0.715007, acc.: 50.78%] [G loss: 0.885937]\n",
      "epoch:1 step:1474 [D loss: 0.700664, acc.: 53.12%] [G loss: 0.963773]\n",
      "epoch:1 step:1475 [D loss: 0.734341, acc.: 55.47%] [G loss: 0.890403]\n",
      "epoch:1 step:1476 [D loss: 0.722610, acc.: 50.78%] [G loss: 0.903675]\n",
      "epoch:1 step:1477 [D loss: 0.634259, acc.: 61.72%] [G loss: 0.980637]\n",
      "epoch:1 step:1478 [D loss: 0.649706, acc.: 54.69%] [G loss: 0.921697]\n",
      "epoch:1 step:1479 [D loss: 0.725641, acc.: 45.31%] [G loss: 0.864487]\n",
      "epoch:1 step:1480 [D loss: 0.657466, acc.: 59.38%] [G loss: 0.968833]\n",
      "epoch:1 step:1481 [D loss: 0.694228, acc.: 53.91%] [G loss: 0.905314]\n",
      "epoch:1 step:1482 [D loss: 0.630218, acc.: 62.50%] [G loss: 0.868591]\n",
      "epoch:1 step:1483 [D loss: 0.683419, acc.: 54.69%] [G loss: 0.957024]\n",
      "epoch:1 step:1484 [D loss: 0.716052, acc.: 53.91%] [G loss: 0.987902]\n",
      "epoch:1 step:1485 [D loss: 0.719579, acc.: 48.44%] [G loss: 0.957090]\n",
      "epoch:1 step:1486 [D loss: 0.726086, acc.: 45.31%] [G loss: 0.838816]\n",
      "epoch:1 step:1487 [D loss: 0.685766, acc.: 55.47%] [G loss: 0.853362]\n",
      "epoch:1 step:1488 [D loss: 0.689957, acc.: 54.69%] [G loss: 0.988000]\n",
      "epoch:1 step:1489 [D loss: 0.670848, acc.: 64.06%] [G loss: 0.972279]\n",
      "epoch:1 step:1490 [D loss: 0.691221, acc.: 55.47%] [G loss: 0.806739]\n",
      "epoch:1 step:1491 [D loss: 0.686265, acc.: 56.25%] [G loss: 0.796563]\n",
      "epoch:1 step:1492 [D loss: 0.650726, acc.: 64.84%] [G loss: 0.792688]\n",
      "epoch:1 step:1493 [D loss: 0.699115, acc.: 53.12%] [G loss: 0.982610]\n",
      "epoch:1 step:1494 [D loss: 0.657599, acc.: 60.16%] [G loss: 0.925273]\n",
      "epoch:1 step:1495 [D loss: 0.644972, acc.: 59.38%] [G loss: 0.939972]\n",
      "epoch:1 step:1496 [D loss: 0.709471, acc.: 51.56%] [G loss: 0.973865]\n",
      "epoch:1 step:1497 [D loss: 0.709475, acc.: 49.22%] [G loss: 1.032065]\n",
      "epoch:1 step:1498 [D loss: 0.668491, acc.: 60.16%] [G loss: 0.887482]\n",
      "epoch:1 step:1499 [D loss: 0.681248, acc.: 51.56%] [G loss: 0.862499]\n",
      "epoch:1 step:1500 [D loss: 0.689110, acc.: 53.12%] [G loss: 0.968661]\n",
      "epoch:1 step:1501 [D loss: 0.711090, acc.: 49.22%] [G loss: 1.036715]\n",
      "epoch:1 step:1502 [D loss: 0.678100, acc.: 57.03%] [G loss: 0.789743]\n",
      "epoch:1 step:1503 [D loss: 0.680738, acc.: 57.81%] [G loss: 0.958800]\n",
      "epoch:1 step:1504 [D loss: 0.680283, acc.: 56.25%] [G loss: 0.894833]\n",
      "epoch:1 step:1505 [D loss: 0.695221, acc.: 55.47%] [G loss: 0.959465]\n",
      "epoch:1 step:1506 [D loss: 0.658634, acc.: 60.94%] [G loss: 0.930629]\n",
      "epoch:1 step:1507 [D loss: 0.713200, acc.: 52.34%] [G loss: 0.873480]\n",
      "epoch:1 step:1508 [D loss: 0.665335, acc.: 54.69%] [G loss: 0.947149]\n",
      "epoch:1 step:1509 [D loss: 0.666312, acc.: 57.03%] [G loss: 0.928870]\n",
      "epoch:1 step:1510 [D loss: 0.765108, acc.: 45.31%] [G loss: 1.036189]\n",
      "epoch:1 step:1511 [D loss: 0.645738, acc.: 61.72%] [G loss: 0.992627]\n",
      "epoch:1 step:1512 [D loss: 0.691142, acc.: 55.47%] [G loss: 0.921347]\n",
      "epoch:1 step:1513 [D loss: 0.676168, acc.: 57.03%] [G loss: 0.938950]\n",
      "epoch:1 step:1514 [D loss: 0.658624, acc.: 65.62%] [G loss: 0.950327]\n",
      "epoch:1 step:1515 [D loss: 0.701272, acc.: 53.91%] [G loss: 0.866815]\n",
      "epoch:1 step:1516 [D loss: 0.655783, acc.: 57.81%] [G loss: 0.941386]\n",
      "epoch:1 step:1517 [D loss: 0.720659, acc.: 53.91%] [G loss: 0.830503]\n",
      "epoch:1 step:1518 [D loss: 0.739103, acc.: 49.22%] [G loss: 0.902796]\n",
      "epoch:1 step:1519 [D loss: 0.667455, acc.: 61.72%] [G loss: 0.900143]\n",
      "epoch:1 step:1520 [D loss: 0.703891, acc.: 53.91%] [G loss: 0.873699]\n",
      "epoch:1 step:1521 [D loss: 0.700910, acc.: 54.69%] [G loss: 0.848816]\n",
      "epoch:1 step:1522 [D loss: 0.683158, acc.: 52.34%] [G loss: 0.961077]\n",
      "epoch:1 step:1523 [D loss: 0.720625, acc.: 53.91%] [G loss: 0.923270]\n",
      "epoch:1 step:1524 [D loss: 0.674524, acc.: 56.25%] [G loss: 0.835457]\n",
      "epoch:1 step:1525 [D loss: 0.702576, acc.: 53.12%] [G loss: 1.021469]\n",
      "epoch:1 step:1526 [D loss: 0.649749, acc.: 64.06%] [G loss: 0.956698]\n",
      "epoch:1 step:1527 [D loss: 0.668785, acc.: 60.94%] [G loss: 0.862311]\n",
      "epoch:1 step:1528 [D loss: 0.604968, acc.: 67.97%] [G loss: 1.005309]\n",
      "epoch:1 step:1529 [D loss: 0.632915, acc.: 65.62%] [G loss: 0.966069]\n",
      "epoch:1 step:1530 [D loss: 0.723732, acc.: 49.22%] [G loss: 0.964831]\n",
      "epoch:1 step:1531 [D loss: 0.665014, acc.: 57.81%] [G loss: 0.952788]\n",
      "epoch:1 step:1532 [D loss: 0.732347, acc.: 50.78%] [G loss: 0.968712]\n",
      "epoch:1 step:1533 [D loss: 0.675560, acc.: 57.81%] [G loss: 0.940436]\n",
      "epoch:1 step:1534 [D loss: 0.676542, acc.: 53.91%] [G loss: 0.936982]\n",
      "epoch:1 step:1535 [D loss: 0.695146, acc.: 55.47%] [G loss: 1.047735]\n",
      "epoch:1 step:1536 [D loss: 0.677758, acc.: 57.81%] [G loss: 0.980716]\n",
      "epoch:1 step:1537 [D loss: 0.693223, acc.: 61.72%] [G loss: 0.915912]\n",
      "epoch:1 step:1538 [D loss: 0.748985, acc.: 47.66%] [G loss: 0.893695]\n",
      "epoch:1 step:1539 [D loss: 0.661169, acc.: 60.16%] [G loss: 0.943814]\n",
      "epoch:1 step:1540 [D loss: 0.684244, acc.: 56.25%] [G loss: 0.845222]\n",
      "epoch:1 step:1541 [D loss: 0.741299, acc.: 46.88%] [G loss: 0.888288]\n",
      "epoch:1 step:1542 [D loss: 0.689919, acc.: 57.03%] [G loss: 0.953875]\n",
      "epoch:1 step:1543 [D loss: 0.719046, acc.: 50.78%] [G loss: 0.924668]\n",
      "epoch:1 step:1544 [D loss: 0.626400, acc.: 66.41%] [G loss: 1.003387]\n",
      "epoch:1 step:1545 [D loss: 0.672408, acc.: 58.59%] [G loss: 0.961389]\n",
      "epoch:1 step:1546 [D loss: 0.695113, acc.: 58.59%] [G loss: 0.987500]\n",
      "epoch:1 step:1547 [D loss: 0.651420, acc.: 62.50%] [G loss: 0.956537]\n",
      "epoch:1 step:1548 [D loss: 0.680214, acc.: 60.94%] [G loss: 0.911169]\n",
      "epoch:1 step:1549 [D loss: 0.721034, acc.: 54.69%] [G loss: 1.017799]\n",
      "epoch:1 step:1550 [D loss: 0.686246, acc.: 59.38%] [G loss: 0.959169]\n",
      "epoch:1 step:1551 [D loss: 0.669789, acc.: 59.38%] [G loss: 0.891764]\n",
      "epoch:1 step:1552 [D loss: 0.656695, acc.: 60.94%] [G loss: 0.849245]\n",
      "epoch:1 step:1553 [D loss: 0.745289, acc.: 50.78%] [G loss: 0.934336]\n",
      "epoch:1 step:1554 [D loss: 0.698036, acc.: 46.88%] [G loss: 0.879861]\n",
      "epoch:1 step:1555 [D loss: 0.636831, acc.: 63.28%] [G loss: 0.800756]\n",
      "epoch:1 step:1556 [D loss: 0.710637, acc.: 54.69%] [G loss: 0.985027]\n",
      "epoch:1 step:1557 [D loss: 0.745121, acc.: 50.78%] [G loss: 0.897479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1558 [D loss: 0.673265, acc.: 55.47%] [G loss: 0.896913]\n",
      "epoch:1 step:1559 [D loss: 0.668094, acc.: 55.47%] [G loss: 0.825989]\n",
      "epoch:1 step:1560 [D loss: 0.696531, acc.: 53.12%] [G loss: 0.807581]\n",
      "epoch:1 step:1561 [D loss: 0.667034, acc.: 60.94%] [G loss: 0.907754]\n",
      "epoch:1 step:1562 [D loss: 0.705878, acc.: 51.56%] [G loss: 0.882707]\n",
      "epoch:1 step:1563 [D loss: 0.725897, acc.: 49.22%] [G loss: 0.931920]\n",
      "epoch:1 step:1564 [D loss: 0.702772, acc.: 51.56%] [G loss: 0.952597]\n",
      "epoch:1 step:1565 [D loss: 0.756424, acc.: 44.53%] [G loss: 0.879435]\n",
      "epoch:1 step:1566 [D loss: 0.697877, acc.: 50.78%] [G loss: 0.860688]\n",
      "epoch:1 step:1567 [D loss: 0.686740, acc.: 54.69%] [G loss: 0.986353]\n",
      "epoch:1 step:1568 [D loss: 0.673938, acc.: 60.16%] [G loss: 0.922017]\n",
      "epoch:1 step:1569 [D loss: 0.644276, acc.: 62.50%] [G loss: 0.885207]\n",
      "epoch:1 step:1570 [D loss: 0.665365, acc.: 57.03%] [G loss: 0.870359]\n",
      "epoch:1 step:1571 [D loss: 0.689602, acc.: 58.59%] [G loss: 0.916643]\n",
      "epoch:1 step:1572 [D loss: 0.690961, acc.: 48.44%] [G loss: 0.856092]\n",
      "epoch:1 step:1573 [D loss: 0.709193, acc.: 56.25%] [G loss: 0.756911]\n",
      "epoch:1 step:1574 [D loss: 0.675238, acc.: 60.94%] [G loss: 0.916654]\n",
      "epoch:1 step:1575 [D loss: 0.675535, acc.: 53.91%] [G loss: 0.968853]\n",
      "epoch:1 step:1576 [D loss: 0.689918, acc.: 52.34%] [G loss: 0.987695]\n",
      "epoch:1 step:1577 [D loss: 0.686959, acc.: 50.00%] [G loss: 0.842220]\n",
      "epoch:1 step:1578 [D loss: 0.694266, acc.: 50.00%] [G loss: 0.880134]\n",
      "epoch:1 step:1579 [D loss: 0.686692, acc.: 53.91%] [G loss: 0.934287]\n",
      "epoch:1 step:1580 [D loss: 0.701192, acc.: 49.22%] [G loss: 0.898983]\n",
      "epoch:1 step:1581 [D loss: 0.674653, acc.: 57.81%] [G loss: 0.860694]\n",
      "epoch:1 step:1582 [D loss: 0.668604, acc.: 61.72%] [G loss: 0.858460]\n",
      "epoch:1 step:1583 [D loss: 0.815357, acc.: 39.06%] [G loss: 0.903257]\n",
      "epoch:1 step:1584 [D loss: 0.677393, acc.: 53.12%] [G loss: 0.915749]\n",
      "epoch:1 step:1585 [D loss: 0.661976, acc.: 56.25%] [G loss: 0.978131]\n",
      "epoch:1 step:1586 [D loss: 0.674677, acc.: 54.69%] [G loss: 1.018575]\n",
      "epoch:1 step:1587 [D loss: 0.688064, acc.: 51.56%] [G loss: 0.924204]\n",
      "epoch:1 step:1588 [D loss: 0.656534, acc.: 58.59%] [G loss: 0.897149]\n",
      "epoch:1 step:1589 [D loss: 0.683753, acc.: 51.56%] [G loss: 0.904175]\n",
      "epoch:1 step:1590 [D loss: 0.675627, acc.: 59.38%] [G loss: 0.803142]\n",
      "epoch:1 step:1591 [D loss: 0.665825, acc.: 59.38%] [G loss: 0.868575]\n",
      "epoch:1 step:1592 [D loss: 0.716756, acc.: 48.44%] [G loss: 0.797966]\n",
      "epoch:1 step:1593 [D loss: 0.676913, acc.: 60.94%] [G loss: 0.877398]\n",
      "epoch:1 step:1594 [D loss: 0.665988, acc.: 57.81%] [G loss: 0.887761]\n",
      "epoch:1 step:1595 [D loss: 0.651111, acc.: 60.16%] [G loss: 0.926216]\n",
      "epoch:1 step:1596 [D loss: 0.674677, acc.: 62.50%] [G loss: 0.951931]\n",
      "epoch:1 step:1597 [D loss: 0.660954, acc.: 64.06%] [G loss: 0.985147]\n",
      "epoch:1 step:1598 [D loss: 0.726691, acc.: 48.44%] [G loss: 0.807441]\n",
      "epoch:1 step:1599 [D loss: 0.676860, acc.: 50.78%] [G loss: 0.851952]\n",
      "epoch:1 step:1600 [D loss: 0.700573, acc.: 52.34%] [G loss: 0.859750]\n",
      "epoch:1 step:1601 [D loss: 0.666582, acc.: 60.16%] [G loss: 0.900882]\n",
      "epoch:1 step:1602 [D loss: 0.682064, acc.: 59.38%] [G loss: 0.913794]\n",
      "epoch:1 step:1603 [D loss: 0.662412, acc.: 56.25%] [G loss: 0.946174]\n",
      "epoch:1 step:1604 [D loss: 0.711342, acc.: 50.00%] [G loss: 0.823256]\n",
      "epoch:1 step:1605 [D loss: 0.721644, acc.: 43.75%] [G loss: 0.850024]\n",
      "epoch:1 step:1606 [D loss: 0.718556, acc.: 51.56%] [G loss: 0.951287]\n",
      "epoch:1 step:1607 [D loss: 0.693837, acc.: 55.47%] [G loss: 0.852652]\n",
      "epoch:1 step:1608 [D loss: 0.643759, acc.: 59.38%] [G loss: 0.882730]\n",
      "epoch:1 step:1609 [D loss: 0.704778, acc.: 51.56%] [G loss: 0.831215]\n",
      "epoch:1 step:1610 [D loss: 0.712704, acc.: 49.22%] [G loss: 0.928885]\n",
      "epoch:1 step:1611 [D loss: 0.662877, acc.: 60.16%] [G loss: 0.847578]\n",
      "epoch:1 step:1612 [D loss: 0.708414, acc.: 50.00%] [G loss: 0.862462]\n",
      "epoch:1 step:1613 [D loss: 0.678389, acc.: 54.69%] [G loss: 0.953644]\n",
      "epoch:1 step:1614 [D loss: 0.675905, acc.: 58.59%] [G loss: 0.815331]\n",
      "epoch:1 step:1615 [D loss: 0.690263, acc.: 55.47%] [G loss: 0.822012]\n",
      "epoch:1 step:1616 [D loss: 0.666293, acc.: 62.50%] [G loss: 0.853878]\n",
      "epoch:1 step:1617 [D loss: 0.683457, acc.: 60.16%] [G loss: 0.820672]\n",
      "epoch:1 step:1618 [D loss: 0.725531, acc.: 45.31%] [G loss: 0.895253]\n",
      "epoch:1 step:1619 [D loss: 0.677336, acc.: 58.59%] [G loss: 0.906250]\n",
      "epoch:1 step:1620 [D loss: 0.726417, acc.: 48.44%] [G loss: 0.972738]\n",
      "epoch:1 step:1621 [D loss: 0.676665, acc.: 50.00%] [G loss: 0.921811]\n",
      "epoch:1 step:1622 [D loss: 0.715820, acc.: 51.56%] [G loss: 0.866836]\n",
      "epoch:1 step:1623 [D loss: 0.676178, acc.: 54.69%] [G loss: 1.012090]\n",
      "epoch:1 step:1624 [D loss: 0.729164, acc.: 44.53%] [G loss: 0.880523]\n",
      "epoch:1 step:1625 [D loss: 0.641551, acc.: 60.16%] [G loss: 0.896749]\n",
      "epoch:1 step:1626 [D loss: 0.693616, acc.: 53.91%] [G loss: 0.874819]\n",
      "epoch:1 step:1627 [D loss: 0.684296, acc.: 57.03%] [G loss: 0.929587]\n",
      "epoch:1 step:1628 [D loss: 0.720605, acc.: 48.44%] [G loss: 0.898571]\n",
      "epoch:1 step:1629 [D loss: 0.685726, acc.: 58.59%] [G loss: 0.886253]\n",
      "epoch:1 step:1630 [D loss: 0.687754, acc.: 57.03%] [G loss: 0.964847]\n",
      "epoch:1 step:1631 [D loss: 0.682476, acc.: 60.94%] [G loss: 0.839023]\n",
      "epoch:1 step:1632 [D loss: 0.684723, acc.: 54.69%] [G loss: 0.874900]\n",
      "epoch:1 step:1633 [D loss: 0.750722, acc.: 44.53%] [G loss: 0.852918]\n",
      "epoch:1 step:1634 [D loss: 0.676853, acc.: 55.47%] [G loss: 0.890085]\n",
      "epoch:1 step:1635 [D loss: 0.708866, acc.: 46.88%] [G loss: 0.909330]\n",
      "epoch:1 step:1636 [D loss: 0.691563, acc.: 50.00%] [G loss: 0.892198]\n",
      "epoch:1 step:1637 [D loss: 0.701462, acc.: 54.69%] [G loss: 0.912195]\n",
      "epoch:1 step:1638 [D loss: 0.653939, acc.: 61.72%] [G loss: 1.019533]\n",
      "epoch:1 step:1639 [D loss: 0.724914, acc.: 50.78%] [G loss: 0.875485]\n",
      "epoch:1 step:1640 [D loss: 0.733676, acc.: 50.78%] [G loss: 0.808645]\n",
      "epoch:1 step:1641 [D loss: 0.706677, acc.: 50.00%] [G loss: 0.845979]\n",
      "epoch:1 step:1642 [D loss: 0.738005, acc.: 46.88%] [G loss: 0.779094]\n",
      "epoch:1 step:1643 [D loss: 0.668159, acc.: 64.06%] [G loss: 0.924758]\n",
      "epoch:1 step:1644 [D loss: 0.662834, acc.: 59.38%] [G loss: 0.899665]\n",
      "epoch:1 step:1645 [D loss: 0.674162, acc.: 56.25%] [G loss: 0.838327]\n",
      "epoch:1 step:1646 [D loss: 0.684378, acc.: 50.00%] [G loss: 0.970409]\n",
      "epoch:1 step:1647 [D loss: 0.652236, acc.: 64.84%] [G loss: 0.940525]\n",
      "epoch:1 step:1648 [D loss: 0.645594, acc.: 62.50%] [G loss: 0.801022]\n",
      "epoch:1 step:1649 [D loss: 0.642847, acc.: 65.62%] [G loss: 0.883781]\n",
      "epoch:1 step:1650 [D loss: 0.669958, acc.: 55.47%] [G loss: 0.926950]\n",
      "epoch:1 step:1651 [D loss: 0.674608, acc.: 54.69%] [G loss: 0.931046]\n",
      "epoch:1 step:1652 [D loss: 0.706747, acc.: 54.69%] [G loss: 0.839617]\n",
      "epoch:1 step:1653 [D loss: 0.703540, acc.: 55.47%] [G loss: 0.926348]\n",
      "epoch:1 step:1654 [D loss: 0.709486, acc.: 53.91%] [G loss: 0.840989]\n",
      "epoch:1 step:1655 [D loss: 0.645840, acc.: 60.94%] [G loss: 0.942865]\n",
      "epoch:1 step:1656 [D loss: 0.654117, acc.: 57.03%] [G loss: 0.940074]\n",
      "epoch:1 step:1657 [D loss: 0.673727, acc.: 55.47%] [G loss: 0.908388]\n",
      "epoch:1 step:1658 [D loss: 0.688717, acc.: 54.69%] [G loss: 0.845765]\n",
      "epoch:1 step:1659 [D loss: 0.684040, acc.: 59.38%] [G loss: 0.834052]\n",
      "epoch:1 step:1660 [D loss: 0.664523, acc.: 56.25%] [G loss: 0.858667]\n",
      "epoch:1 step:1661 [D loss: 0.691552, acc.: 54.69%] [G loss: 0.928581]\n",
      "epoch:1 step:1662 [D loss: 0.647802, acc.: 56.25%] [G loss: 0.868948]\n",
      "epoch:1 step:1663 [D loss: 0.698026, acc.: 54.69%] [G loss: 0.903765]\n",
      "epoch:1 step:1664 [D loss: 0.737127, acc.: 46.09%] [G loss: 0.838271]\n",
      "epoch:1 step:1665 [D loss: 0.684107, acc.: 52.34%] [G loss: 0.814863]\n",
      "epoch:1 step:1666 [D loss: 0.685262, acc.: 58.59%] [G loss: 0.932487]\n",
      "epoch:1 step:1667 [D loss: 0.667681, acc.: 58.59%] [G loss: 0.870967]\n",
      "epoch:1 step:1668 [D loss: 0.754777, acc.: 45.31%] [G loss: 1.081771]\n",
      "epoch:1 step:1669 [D loss: 0.692475, acc.: 55.47%] [G loss: 1.011456]\n",
      "epoch:1 step:1670 [D loss: 0.683196, acc.: 57.03%] [G loss: 0.961665]\n",
      "epoch:1 step:1671 [D loss: 0.723652, acc.: 59.38%] [G loss: 0.899655]\n",
      "epoch:1 step:1672 [D loss: 0.628442, acc.: 66.41%] [G loss: 0.989734]\n",
      "epoch:1 step:1673 [D loss: 0.648616, acc.: 64.84%] [G loss: 0.861241]\n",
      "epoch:1 step:1674 [D loss: 0.675062, acc.: 52.34%] [G loss: 0.861006]\n",
      "epoch:1 step:1675 [D loss: 0.756063, acc.: 42.97%] [G loss: 0.803176]\n",
      "epoch:1 step:1676 [D loss: 0.650683, acc.: 60.16%] [G loss: 0.945775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1677 [D loss: 0.684729, acc.: 54.69%] [G loss: 0.913399]\n",
      "epoch:1 step:1678 [D loss: 0.664985, acc.: 55.47%] [G loss: 0.921472]\n",
      "epoch:1 step:1679 [D loss: 0.714255, acc.: 50.78%] [G loss: 0.862955]\n",
      "epoch:1 step:1680 [D loss: 0.716634, acc.: 55.47%] [G loss: 1.012359]\n",
      "epoch:1 step:1681 [D loss: 0.669263, acc.: 63.28%] [G loss: 0.935853]\n",
      "epoch:1 step:1682 [D loss: 0.687186, acc.: 57.03%] [G loss: 0.998845]\n",
      "epoch:1 step:1683 [D loss: 0.637993, acc.: 63.28%] [G loss: 0.921949]\n",
      "epoch:1 step:1684 [D loss: 0.694648, acc.: 52.34%] [G loss: 0.879219]\n",
      "epoch:1 step:1685 [D loss: 0.682411, acc.: 54.69%] [G loss: 0.922427]\n",
      "epoch:1 step:1686 [D loss: 0.675125, acc.: 54.69%] [G loss: 0.912444]\n",
      "epoch:1 step:1687 [D loss: 0.676450, acc.: 54.69%] [G loss: 0.838972]\n",
      "epoch:1 step:1688 [D loss: 0.676358, acc.: 54.69%] [G loss: 0.928511]\n",
      "epoch:1 step:1689 [D loss: 0.652838, acc.: 67.19%] [G loss: 0.822658]\n",
      "epoch:1 step:1690 [D loss: 0.648850, acc.: 62.50%] [G loss: 0.884454]\n",
      "epoch:1 step:1691 [D loss: 0.722160, acc.: 51.56%] [G loss: 0.885595]\n",
      "epoch:1 step:1692 [D loss: 0.684348, acc.: 55.47%] [G loss: 0.844299]\n",
      "epoch:1 step:1693 [D loss: 0.681976, acc.: 54.69%] [G loss: 0.878759]\n",
      "epoch:1 step:1694 [D loss: 0.727822, acc.: 54.69%] [G loss: 0.903101]\n",
      "epoch:1 step:1695 [D loss: 0.688296, acc.: 57.81%] [G loss: 0.965120]\n",
      "epoch:1 step:1696 [D loss: 0.685992, acc.: 55.47%] [G loss: 0.926536]\n",
      "epoch:1 step:1697 [D loss: 0.685066, acc.: 52.34%] [G loss: 0.935230]\n",
      "epoch:1 step:1698 [D loss: 0.716455, acc.: 54.69%] [G loss: 0.934826]\n",
      "epoch:1 step:1699 [D loss: 0.713200, acc.: 50.78%] [G loss: 0.813203]\n",
      "epoch:1 step:1700 [D loss: 0.669368, acc.: 57.81%] [G loss: 0.861482]\n",
      "epoch:1 step:1701 [D loss: 0.648844, acc.: 61.72%] [G loss: 0.878814]\n",
      "epoch:1 step:1702 [D loss: 0.676562, acc.: 57.81%] [G loss: 0.843946]\n",
      "epoch:1 step:1703 [D loss: 0.682773, acc.: 57.81%] [G loss: 0.841113]\n",
      "epoch:1 step:1704 [D loss: 0.679096, acc.: 47.66%] [G loss: 0.838102]\n",
      "epoch:1 step:1705 [D loss: 0.676639, acc.: 54.69%] [G loss: 0.880615]\n",
      "epoch:1 step:1706 [D loss: 0.681162, acc.: 60.16%] [G loss: 0.851881]\n",
      "epoch:1 step:1707 [D loss: 0.685473, acc.: 50.78%] [G loss: 0.908833]\n",
      "epoch:1 step:1708 [D loss: 0.624172, acc.: 64.06%] [G loss: 0.884910]\n",
      "epoch:1 step:1709 [D loss: 0.696297, acc.: 49.22%] [G loss: 0.822763]\n",
      "epoch:1 step:1710 [D loss: 0.645015, acc.: 64.84%] [G loss: 0.869145]\n",
      "epoch:1 step:1711 [D loss: 0.695979, acc.: 55.47%] [G loss: 0.818307]\n",
      "epoch:1 step:1712 [D loss: 0.660742, acc.: 57.81%] [G loss: 0.945421]\n",
      "epoch:1 step:1713 [D loss: 0.689256, acc.: 55.47%] [G loss: 0.946799]\n",
      "epoch:1 step:1714 [D loss: 0.728336, acc.: 47.66%] [G loss: 0.856117]\n",
      "epoch:1 step:1715 [D loss: 0.732754, acc.: 53.12%] [G loss: 0.935419]\n",
      "epoch:1 step:1716 [D loss: 0.714458, acc.: 49.22%] [G loss: 0.877958]\n",
      "epoch:1 step:1717 [D loss: 0.671092, acc.: 56.25%] [G loss: 0.918707]\n",
      "epoch:1 step:1718 [D loss: 0.734815, acc.: 42.19%] [G loss: 0.877426]\n",
      "epoch:1 step:1719 [D loss: 0.694478, acc.: 61.72%] [G loss: 0.854378]\n",
      "epoch:1 step:1720 [D loss: 0.661269, acc.: 56.25%] [G loss: 0.863327]\n",
      "epoch:1 step:1721 [D loss: 0.699262, acc.: 53.91%] [G loss: 0.851132]\n",
      "epoch:1 step:1722 [D loss: 0.680558, acc.: 59.38%] [G loss: 0.860660]\n",
      "epoch:1 step:1723 [D loss: 0.678658, acc.: 53.12%] [G loss: 0.921438]\n",
      "epoch:1 step:1724 [D loss: 0.670592, acc.: 56.25%] [G loss: 0.843727]\n",
      "epoch:1 step:1725 [D loss: 0.673730, acc.: 60.94%] [G loss: 0.770887]\n",
      "epoch:1 step:1726 [D loss: 0.694412, acc.: 54.69%] [G loss: 0.693596]\n",
      "epoch:1 step:1727 [D loss: 0.697360, acc.: 50.78%] [G loss: 0.818651]\n",
      "epoch:1 step:1728 [D loss: 0.654132, acc.: 60.94%] [G loss: 0.969141]\n",
      "epoch:1 step:1729 [D loss: 0.676809, acc.: 55.47%] [G loss: 0.836095]\n",
      "epoch:1 step:1730 [D loss: 0.656519, acc.: 57.81%] [G loss: 0.879712]\n",
      "epoch:1 step:1731 [D loss: 0.706999, acc.: 56.25%] [G loss: 1.012324]\n",
      "epoch:1 step:1732 [D loss: 0.657434, acc.: 61.72%] [G loss: 0.980687]\n",
      "epoch:1 step:1733 [D loss: 0.700982, acc.: 52.34%] [G loss: 0.831521]\n",
      "epoch:1 step:1734 [D loss: 0.693820, acc.: 56.25%] [G loss: 0.897174]\n",
      "epoch:1 step:1735 [D loss: 0.702476, acc.: 54.69%] [G loss: 0.882854]\n",
      "epoch:1 step:1736 [D loss: 0.683448, acc.: 54.69%] [G loss: 0.875356]\n",
      "epoch:1 step:1737 [D loss: 0.681248, acc.: 60.16%] [G loss: 0.895411]\n",
      "epoch:1 step:1738 [D loss: 0.665865, acc.: 54.69%] [G loss: 0.906843]\n",
      "epoch:1 step:1739 [D loss: 0.671439, acc.: 61.72%] [G loss: 0.872495]\n",
      "epoch:1 step:1740 [D loss: 0.667715, acc.: 59.38%] [G loss: 0.876461]\n",
      "epoch:1 step:1741 [D loss: 0.718884, acc.: 46.09%] [G loss: 0.899974]\n",
      "epoch:1 step:1742 [D loss: 0.735466, acc.: 37.50%] [G loss: 0.845643]\n",
      "epoch:1 step:1743 [D loss: 0.658475, acc.: 59.38%] [G loss: 0.889801]\n",
      "epoch:1 step:1744 [D loss: 0.694292, acc.: 57.03%] [G loss: 0.897860]\n",
      "epoch:1 step:1745 [D loss: 0.700179, acc.: 53.91%] [G loss: 0.910522]\n",
      "epoch:1 step:1746 [D loss: 0.653257, acc.: 64.84%] [G loss: 0.934251]\n",
      "epoch:1 step:1747 [D loss: 0.725146, acc.: 50.00%] [G loss: 0.975188]\n",
      "epoch:1 step:1748 [D loss: 0.685923, acc.: 50.00%] [G loss: 0.872832]\n",
      "epoch:1 step:1749 [D loss: 0.713161, acc.: 52.34%] [G loss: 0.882073]\n",
      "epoch:1 step:1750 [D loss: 0.729724, acc.: 53.12%] [G loss: 0.822503]\n",
      "epoch:1 step:1751 [D loss: 0.695872, acc.: 52.34%] [G loss: 0.929115]\n",
      "epoch:1 step:1752 [D loss: 0.664305, acc.: 62.50%] [G loss: 0.940717]\n",
      "epoch:1 step:1753 [D loss: 0.672335, acc.: 59.38%] [G loss: 0.806144]\n",
      "epoch:1 step:1754 [D loss: 0.736496, acc.: 49.22%] [G loss: 0.862059]\n",
      "epoch:1 step:1755 [D loss: 0.766406, acc.: 42.97%] [G loss: 0.951290]\n",
      "epoch:1 step:1756 [D loss: 0.660087, acc.: 64.84%] [G loss: 0.816005]\n",
      "epoch:1 step:1757 [D loss: 0.703754, acc.: 50.00%] [G loss: 0.840786]\n",
      "epoch:1 step:1758 [D loss: 0.672088, acc.: 54.69%] [G loss: 0.821305]\n",
      "epoch:1 step:1759 [D loss: 0.692441, acc.: 54.69%] [G loss: 0.851842]\n",
      "epoch:1 step:1760 [D loss: 0.664151, acc.: 55.47%] [G loss: 0.829998]\n",
      "epoch:1 step:1761 [D loss: 0.664108, acc.: 57.81%] [G loss: 0.992961]\n",
      "epoch:1 step:1762 [D loss: 0.713459, acc.: 49.22%] [G loss: 0.840213]\n",
      "epoch:1 step:1763 [D loss: 0.663916, acc.: 60.94%] [G loss: 0.813310]\n",
      "epoch:1 step:1764 [D loss: 0.661279, acc.: 58.59%] [G loss: 0.881252]\n",
      "epoch:1 step:1765 [D loss: 0.725818, acc.: 50.00%] [G loss: 0.884593]\n",
      "epoch:1 step:1766 [D loss: 0.670781, acc.: 57.81%] [G loss: 0.879091]\n",
      "epoch:1 step:1767 [D loss: 0.659909, acc.: 59.38%] [G loss: 0.892220]\n",
      "epoch:1 step:1768 [D loss: 0.667802, acc.: 60.16%] [G loss: 0.916683]\n",
      "epoch:1 step:1769 [D loss: 0.728364, acc.: 49.22%] [G loss: 0.845716]\n",
      "epoch:1 step:1770 [D loss: 0.650459, acc.: 67.97%] [G loss: 0.838266]\n",
      "epoch:1 step:1771 [D loss: 0.695943, acc.: 57.81%] [G loss: 0.983112]\n",
      "epoch:1 step:1772 [D loss: 0.688063, acc.: 53.91%] [G loss: 0.893493]\n",
      "epoch:1 step:1773 [D loss: 0.674855, acc.: 57.03%] [G loss: 0.922014]\n",
      "epoch:1 step:1774 [D loss: 0.700114, acc.: 50.00%] [G loss: 0.904581]\n",
      "epoch:1 step:1775 [D loss: 0.684731, acc.: 55.47%] [G loss: 0.932298]\n",
      "epoch:1 step:1776 [D loss: 0.706012, acc.: 47.66%] [G loss: 0.811748]\n",
      "epoch:1 step:1777 [D loss: 0.741647, acc.: 49.22%] [G loss: 0.797481]\n",
      "epoch:1 step:1778 [D loss: 0.658921, acc.: 62.50%] [G loss: 0.864649]\n",
      "epoch:1 step:1779 [D loss: 0.671825, acc.: 57.03%] [G loss: 0.869887]\n",
      "epoch:1 step:1780 [D loss: 0.703197, acc.: 53.91%] [G loss: 0.865248]\n",
      "epoch:1 step:1781 [D loss: 0.686603, acc.: 51.56%] [G loss: 0.803298]\n",
      "epoch:1 step:1782 [D loss: 0.705747, acc.: 47.66%] [G loss: 0.840928]\n",
      "epoch:1 step:1783 [D loss: 0.684625, acc.: 59.38%] [G loss: 0.855687]\n",
      "epoch:1 step:1784 [D loss: 0.619252, acc.: 67.19%] [G loss: 0.876201]\n",
      "epoch:1 step:1785 [D loss: 0.678532, acc.: 58.59%] [G loss: 0.861413]\n",
      "epoch:1 step:1786 [D loss: 0.677422, acc.: 61.72%] [G loss: 0.827136]\n",
      "epoch:1 step:1787 [D loss: 0.651271, acc.: 56.25%] [G loss: 0.810521]\n",
      "epoch:1 step:1788 [D loss: 0.668706, acc.: 53.91%] [G loss: 0.895763]\n",
      "epoch:1 step:1789 [D loss: 0.614613, acc.: 72.66%] [G loss: 0.911931]\n",
      "epoch:1 step:1790 [D loss: 0.680605, acc.: 56.25%] [G loss: 0.959495]\n",
      "epoch:1 step:1791 [D loss: 0.663518, acc.: 59.38%] [G loss: 0.875825]\n",
      "epoch:1 step:1792 [D loss: 0.690521, acc.: 55.47%] [G loss: 0.847060]\n",
      "epoch:1 step:1793 [D loss: 0.682012, acc.: 56.25%] [G loss: 0.836285]\n",
      "epoch:1 step:1794 [D loss: 0.718237, acc.: 41.41%] [G loss: 0.818865]\n",
      "epoch:1 step:1795 [D loss: 0.684657, acc.: 57.81%] [G loss: 0.823437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1796 [D loss: 0.689367, acc.: 55.47%] [G loss: 0.795019]\n",
      "epoch:1 step:1797 [D loss: 0.697384, acc.: 54.69%] [G loss: 0.848644]\n",
      "epoch:1 step:1798 [D loss: 0.669041, acc.: 55.47%] [G loss: 0.902077]\n",
      "epoch:1 step:1799 [D loss: 0.658243, acc.: 59.38%] [G loss: 0.877485]\n",
      "epoch:1 step:1800 [D loss: 0.689378, acc.: 57.03%] [G loss: 0.964085]\n",
      "epoch:1 step:1801 [D loss: 0.739674, acc.: 40.62%] [G loss: 0.762743]\n",
      "epoch:1 step:1802 [D loss: 0.672467, acc.: 60.16%] [G loss: 0.915782]\n",
      "epoch:1 step:1803 [D loss: 0.674232, acc.: 49.22%] [G loss: 0.871526]\n",
      "epoch:1 step:1804 [D loss: 0.693398, acc.: 56.25%] [G loss: 0.862602]\n",
      "epoch:1 step:1805 [D loss: 0.715113, acc.: 50.78%] [G loss: 0.832971]\n",
      "epoch:1 step:1806 [D loss: 0.663201, acc.: 56.25%] [G loss: 0.810570]\n",
      "epoch:1 step:1807 [D loss: 0.664628, acc.: 64.06%] [G loss: 0.889200]\n",
      "epoch:1 step:1808 [D loss: 0.689304, acc.: 50.78%] [G loss: 0.821708]\n",
      "epoch:1 step:1809 [D loss: 0.759865, acc.: 42.97%] [G loss: 0.860910]\n",
      "epoch:1 step:1810 [D loss: 0.653689, acc.: 58.59%] [G loss: 0.865985]\n",
      "epoch:1 step:1811 [D loss: 0.682927, acc.: 53.12%] [G loss: 0.889207]\n",
      "epoch:1 step:1812 [D loss: 0.747023, acc.: 42.97%] [G loss: 0.857152]\n",
      "epoch:1 step:1813 [D loss: 0.706418, acc.: 52.34%] [G loss: 0.833948]\n",
      "epoch:1 step:1814 [D loss: 0.692389, acc.: 53.91%] [G loss: 0.879645]\n",
      "epoch:1 step:1815 [D loss: 0.668969, acc.: 59.38%] [G loss: 0.866042]\n",
      "epoch:1 step:1816 [D loss: 0.665100, acc.: 57.03%] [G loss: 0.862700]\n",
      "epoch:1 step:1817 [D loss: 0.635420, acc.: 63.28%] [G loss: 0.874849]\n",
      "epoch:1 step:1818 [D loss: 0.645533, acc.: 61.72%] [G loss: 0.802099]\n",
      "epoch:1 step:1819 [D loss: 0.703615, acc.: 53.91%] [G loss: 0.884700]\n",
      "epoch:1 step:1820 [D loss: 0.727037, acc.: 50.78%] [G loss: 0.902562]\n",
      "epoch:1 step:1821 [D loss: 0.656627, acc.: 58.59%] [G loss: 0.951669]\n",
      "epoch:1 step:1822 [D loss: 0.684897, acc.: 54.69%] [G loss: 0.987263]\n",
      "epoch:1 step:1823 [D loss: 0.657604, acc.: 63.28%] [G loss: 0.976383]\n",
      "epoch:1 step:1824 [D loss: 0.607249, acc.: 70.31%] [G loss: 0.956536]\n",
      "epoch:1 step:1825 [D loss: 0.644904, acc.: 64.84%] [G loss: 1.042722]\n",
      "epoch:1 step:1826 [D loss: 0.703050, acc.: 58.59%] [G loss: 0.941464]\n",
      "epoch:1 step:1827 [D loss: 0.691964, acc.: 51.56%] [G loss: 0.858004]\n",
      "epoch:1 step:1828 [D loss: 0.697963, acc.: 49.22%] [G loss: 0.911409]\n",
      "epoch:1 step:1829 [D loss: 0.753612, acc.: 43.75%] [G loss: 0.838414]\n",
      "epoch:1 step:1830 [D loss: 0.731702, acc.: 49.22%] [G loss: 0.862247]\n",
      "epoch:1 step:1831 [D loss: 0.734773, acc.: 51.56%] [G loss: 0.910164]\n",
      "epoch:1 step:1832 [D loss: 0.707721, acc.: 50.78%] [G loss: 0.898926]\n",
      "epoch:1 step:1833 [D loss: 0.680134, acc.: 50.78%] [G loss: 0.898232]\n",
      "epoch:1 step:1834 [D loss: 0.680763, acc.: 57.81%] [G loss: 0.863010]\n",
      "epoch:1 step:1835 [D loss: 0.654588, acc.: 57.03%] [G loss: 0.801486]\n",
      "epoch:1 step:1836 [D loss: 0.652458, acc.: 57.03%] [G loss: 0.849833]\n",
      "epoch:1 step:1837 [D loss: 0.670364, acc.: 53.12%] [G loss: 0.802312]\n",
      "epoch:1 step:1838 [D loss: 0.704697, acc.: 52.34%] [G loss: 0.835674]\n",
      "epoch:1 step:1839 [D loss: 0.706798, acc.: 51.56%] [G loss: 0.803076]\n",
      "epoch:1 step:1840 [D loss: 0.661933, acc.: 60.16%] [G loss: 0.910524]\n",
      "epoch:1 step:1841 [D loss: 0.686560, acc.: 58.59%] [G loss: 0.905622]\n",
      "epoch:1 step:1842 [D loss: 0.691178, acc.: 54.69%] [G loss: 0.869405]\n",
      "epoch:1 step:1843 [D loss: 0.735162, acc.: 40.62%] [G loss: 0.825840]\n",
      "epoch:1 step:1844 [D loss: 0.685037, acc.: 53.91%] [G loss: 0.899140]\n",
      "epoch:1 step:1845 [D loss: 0.673895, acc.: 59.38%] [G loss: 0.755771]\n",
      "epoch:1 step:1846 [D loss: 0.695865, acc.: 53.12%] [G loss: 0.877931]\n",
      "epoch:1 step:1847 [D loss: 0.702639, acc.: 53.12%] [G loss: 0.869940]\n",
      "epoch:1 step:1848 [D loss: 0.715168, acc.: 44.53%] [G loss: 0.901281]\n",
      "epoch:1 step:1849 [D loss: 0.674239, acc.: 52.34%] [G loss: 0.820999]\n",
      "epoch:1 step:1850 [D loss: 0.651211, acc.: 63.28%] [G loss: 0.822250]\n",
      "epoch:1 step:1851 [D loss: 0.682874, acc.: 53.12%] [G loss: 0.863914]\n",
      "epoch:1 step:1852 [D loss: 0.666429, acc.: 57.81%] [G loss: 0.881552]\n",
      "epoch:1 step:1853 [D loss: 0.710586, acc.: 48.44%] [G loss: 0.845999]\n",
      "epoch:1 step:1854 [D loss: 0.640952, acc.: 60.16%] [G loss: 0.833585]\n",
      "epoch:1 step:1855 [D loss: 0.671351, acc.: 59.38%] [G loss: 0.804196]\n",
      "epoch:1 step:1856 [D loss: 0.710743, acc.: 41.41%] [G loss: 0.862216]\n",
      "epoch:1 step:1857 [D loss: 0.704672, acc.: 52.34%] [G loss: 0.847526]\n",
      "epoch:1 step:1858 [D loss: 0.688166, acc.: 50.78%] [G loss: 0.833830]\n",
      "epoch:1 step:1859 [D loss: 0.696325, acc.: 52.34%] [G loss: 0.916058]\n",
      "epoch:1 step:1860 [D loss: 0.661912, acc.: 58.59%] [G loss: 0.818273]\n",
      "epoch:1 step:1861 [D loss: 0.697392, acc.: 51.56%] [G loss: 0.828618]\n",
      "epoch:1 step:1862 [D loss: 0.693157, acc.: 56.25%] [G loss: 0.911447]\n",
      "epoch:1 step:1863 [D loss: 0.701423, acc.: 55.47%] [G loss: 0.861693]\n",
      "epoch:1 step:1864 [D loss: 0.677178, acc.: 55.47%] [G loss: 0.878355]\n",
      "epoch:1 step:1865 [D loss: 0.669924, acc.: 58.59%] [G loss: 0.888594]\n",
      "epoch:1 step:1866 [D loss: 0.716953, acc.: 50.00%] [G loss: 0.873682]\n",
      "epoch:1 step:1867 [D loss: 0.660643, acc.: 59.38%] [G loss: 0.924496]\n",
      "epoch:1 step:1868 [D loss: 0.717296, acc.: 51.56%] [G loss: 0.893000]\n",
      "epoch:1 step:1869 [D loss: 0.691220, acc.: 50.78%] [G loss: 0.884734]\n",
      "epoch:1 step:1870 [D loss: 0.704720, acc.: 50.78%] [G loss: 0.889722]\n",
      "epoch:1 step:1871 [D loss: 0.674139, acc.: 55.47%] [G loss: 0.934062]\n",
      "epoch:1 step:1872 [D loss: 0.706004, acc.: 47.66%] [G loss: 0.848696]\n",
      "epoch:1 step:1873 [D loss: 0.680379, acc.: 60.16%] [G loss: 0.866014]\n",
      "epoch:1 step:1874 [D loss: 0.730294, acc.: 50.78%] [G loss: 0.867546]\n",
      "epoch:2 step:1875 [D loss: 0.699181, acc.: 54.69%] [G loss: 0.805700]\n",
      "epoch:2 step:1876 [D loss: 0.673915, acc.: 55.47%] [G loss: 0.922198]\n",
      "epoch:2 step:1877 [D loss: 0.673365, acc.: 57.03%] [G loss: 0.953963]\n",
      "epoch:2 step:1878 [D loss: 0.675506, acc.: 54.69%] [G loss: 0.861085]\n",
      "epoch:2 step:1879 [D loss: 0.692480, acc.: 54.69%] [G loss: 0.854244]\n",
      "epoch:2 step:1880 [D loss: 0.680123, acc.: 58.59%] [G loss: 0.903679]\n",
      "epoch:2 step:1881 [D loss: 0.663504, acc.: 55.47%] [G loss: 0.911150]\n",
      "epoch:2 step:1882 [D loss: 0.744617, acc.: 46.88%] [G loss: 0.874259]\n",
      "epoch:2 step:1883 [D loss: 0.684144, acc.: 53.12%] [G loss: 0.829516]\n",
      "epoch:2 step:1884 [D loss: 0.674144, acc.: 53.91%] [G loss: 0.930682]\n",
      "epoch:2 step:1885 [D loss: 0.695156, acc.: 55.47%] [G loss: 0.852422]\n",
      "epoch:2 step:1886 [D loss: 0.685118, acc.: 54.69%] [G loss: 0.789795]\n",
      "epoch:2 step:1887 [D loss: 0.689601, acc.: 55.47%] [G loss: 0.854910]\n",
      "epoch:2 step:1888 [D loss: 0.693240, acc.: 51.56%] [G loss: 0.868236]\n",
      "epoch:2 step:1889 [D loss: 0.683952, acc.: 58.59%] [G loss: 0.829135]\n",
      "epoch:2 step:1890 [D loss: 0.704602, acc.: 52.34%] [G loss: 0.820729]\n",
      "epoch:2 step:1891 [D loss: 0.656743, acc.: 60.16%] [G loss: 0.930730]\n",
      "epoch:2 step:1892 [D loss: 0.673177, acc.: 57.03%] [G loss: 0.865636]\n",
      "epoch:2 step:1893 [D loss: 0.730882, acc.: 45.31%] [G loss: 0.779528]\n",
      "epoch:2 step:1894 [D loss: 0.655930, acc.: 57.81%] [G loss: 0.829154]\n",
      "epoch:2 step:1895 [D loss: 0.673078, acc.: 57.03%] [G loss: 0.803462]\n",
      "epoch:2 step:1896 [D loss: 0.759698, acc.: 42.97%] [G loss: 0.838328]\n",
      "epoch:2 step:1897 [D loss: 0.687204, acc.: 50.78%] [G loss: 0.891126]\n",
      "epoch:2 step:1898 [D loss: 0.668793, acc.: 57.81%] [G loss: 0.972401]\n",
      "epoch:2 step:1899 [D loss: 0.685902, acc.: 55.47%] [G loss: 0.848486]\n",
      "epoch:2 step:1900 [D loss: 0.710008, acc.: 58.59%] [G loss: 0.815289]\n",
      "epoch:2 step:1901 [D loss: 0.681222, acc.: 53.91%] [G loss: 0.887489]\n",
      "epoch:2 step:1902 [D loss: 0.673883, acc.: 58.59%] [G loss: 0.878751]\n",
      "epoch:2 step:1903 [D loss: 0.671283, acc.: 57.81%] [G loss: 0.899731]\n",
      "epoch:2 step:1904 [D loss: 0.703405, acc.: 50.00%] [G loss: 0.781196]\n",
      "epoch:2 step:1905 [D loss: 0.696571, acc.: 57.81%] [G loss: 0.832309]\n",
      "epoch:2 step:1906 [D loss: 0.654882, acc.: 66.41%] [G loss: 0.804934]\n",
      "epoch:2 step:1907 [D loss: 0.664877, acc.: 60.16%] [G loss: 0.835257]\n",
      "epoch:2 step:1908 [D loss: 0.673200, acc.: 57.03%] [G loss: 0.918933]\n",
      "epoch:2 step:1909 [D loss: 0.672207, acc.: 56.25%] [G loss: 0.881125]\n",
      "epoch:2 step:1910 [D loss: 0.718245, acc.: 54.69%] [G loss: 0.839077]\n",
      "epoch:2 step:1911 [D loss: 0.700161, acc.: 52.34%] [G loss: 0.841568]\n",
      "epoch:2 step:1912 [D loss: 0.673303, acc.: 58.59%] [G loss: 0.878529]\n",
      "epoch:2 step:1913 [D loss: 0.716590, acc.: 46.09%] [G loss: 0.808887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1914 [D loss: 0.679968, acc.: 50.00%] [G loss: 0.916248]\n",
      "epoch:2 step:1915 [D loss: 0.726730, acc.: 46.88%] [G loss: 0.776054]\n",
      "epoch:2 step:1916 [D loss: 0.665823, acc.: 60.94%] [G loss: 0.830223]\n",
      "epoch:2 step:1917 [D loss: 0.685600, acc.: 53.91%] [G loss: 0.934939]\n",
      "epoch:2 step:1918 [D loss: 0.727873, acc.: 51.56%] [G loss: 0.903079]\n",
      "epoch:2 step:1919 [D loss: 0.684926, acc.: 50.00%] [G loss: 0.833718]\n",
      "epoch:2 step:1920 [D loss: 0.681153, acc.: 55.47%] [G loss: 0.881711]\n",
      "epoch:2 step:1921 [D loss: 0.712949, acc.: 50.00%] [G loss: 0.858120]\n",
      "epoch:2 step:1922 [D loss: 0.700081, acc.: 50.00%] [G loss: 0.811023]\n",
      "epoch:2 step:1923 [D loss: 0.668414, acc.: 60.16%] [G loss: 0.874052]\n",
      "epoch:2 step:1924 [D loss: 0.648142, acc.: 64.84%] [G loss: 0.900412]\n",
      "epoch:2 step:1925 [D loss: 0.680090, acc.: 52.34%] [G loss: 0.862042]\n",
      "epoch:2 step:1926 [D loss: 0.687878, acc.: 54.69%] [G loss: 0.900880]\n",
      "epoch:2 step:1927 [D loss: 0.677406, acc.: 55.47%] [G loss: 0.897442]\n",
      "epoch:2 step:1928 [D loss: 0.639462, acc.: 62.50%] [G loss: 0.866083]\n",
      "epoch:2 step:1929 [D loss: 0.707036, acc.: 53.12%] [G loss: 0.856814]\n",
      "epoch:2 step:1930 [D loss: 0.672980, acc.: 57.03%] [G loss: 0.884630]\n",
      "epoch:2 step:1931 [D loss: 0.686065, acc.: 54.69%] [G loss: 0.780786]\n",
      "epoch:2 step:1932 [D loss: 0.699780, acc.: 54.69%] [G loss: 0.887808]\n",
      "epoch:2 step:1933 [D loss: 0.624065, acc.: 68.75%] [G loss: 0.847400]\n",
      "epoch:2 step:1934 [D loss: 0.693947, acc.: 54.69%] [G loss: 0.927887]\n",
      "epoch:2 step:1935 [D loss: 0.667623, acc.: 60.94%] [G loss: 0.888039]\n",
      "epoch:2 step:1936 [D loss: 0.630365, acc.: 64.06%] [G loss: 0.938019]\n",
      "epoch:2 step:1937 [D loss: 0.655242, acc.: 64.06%] [G loss: 0.946119]\n",
      "epoch:2 step:1938 [D loss: 0.678224, acc.: 55.47%] [G loss: 0.846427]\n",
      "epoch:2 step:1939 [D loss: 0.711558, acc.: 46.09%] [G loss: 0.872608]\n",
      "epoch:2 step:1940 [D loss: 0.671810, acc.: 60.16%] [G loss: 0.915031]\n",
      "epoch:2 step:1941 [D loss: 0.667997, acc.: 63.28%] [G loss: 0.954050]\n",
      "epoch:2 step:1942 [D loss: 0.633877, acc.: 63.28%] [G loss: 0.919325]\n",
      "epoch:2 step:1943 [D loss: 0.630120, acc.: 65.62%] [G loss: 0.828256]\n",
      "epoch:2 step:1944 [D loss: 0.643787, acc.: 61.72%] [G loss: 0.897233]\n",
      "epoch:2 step:1945 [D loss: 0.714863, acc.: 53.12%] [G loss: 0.891644]\n",
      "epoch:2 step:1946 [D loss: 0.685178, acc.: 57.03%] [G loss: 0.872285]\n",
      "epoch:2 step:1947 [D loss: 0.723260, acc.: 51.56%] [G loss: 0.894389]\n",
      "epoch:2 step:1948 [D loss: 0.718558, acc.: 55.47%] [G loss: 0.909953]\n",
      "epoch:2 step:1949 [D loss: 0.694570, acc.: 51.56%] [G loss: 0.788951]\n",
      "epoch:2 step:1950 [D loss: 0.704241, acc.: 50.00%] [G loss: 0.846377]\n",
      "epoch:2 step:1951 [D loss: 0.698362, acc.: 56.25%] [G loss: 0.848329]\n",
      "epoch:2 step:1952 [D loss: 0.718039, acc.: 48.44%] [G loss: 0.914513]\n",
      "epoch:2 step:1953 [D loss: 0.731331, acc.: 50.00%] [G loss: 0.907977]\n",
      "epoch:2 step:1954 [D loss: 0.757648, acc.: 41.41%] [G loss: 0.887569]\n",
      "epoch:2 step:1955 [D loss: 0.722114, acc.: 50.78%] [G loss: 0.816091]\n",
      "epoch:2 step:1956 [D loss: 0.706422, acc.: 57.03%] [G loss: 0.781048]\n",
      "epoch:2 step:1957 [D loss: 0.683442, acc.: 55.47%] [G loss: 0.907745]\n",
      "epoch:2 step:1958 [D loss: 0.721336, acc.: 43.75%] [G loss: 0.822462]\n",
      "epoch:2 step:1959 [D loss: 0.661094, acc.: 59.38%] [G loss: 0.891702]\n",
      "epoch:2 step:1960 [D loss: 0.710959, acc.: 46.88%] [G loss: 0.928968]\n",
      "epoch:2 step:1961 [D loss: 0.656163, acc.: 66.41%] [G loss: 0.932886]\n",
      "epoch:2 step:1962 [D loss: 0.671633, acc.: 54.69%] [G loss: 0.809761]\n",
      "epoch:2 step:1963 [D loss: 0.670705, acc.: 60.16%] [G loss: 0.775345]\n",
      "epoch:2 step:1964 [D loss: 0.695257, acc.: 53.12%] [G loss: 0.852690]\n",
      "epoch:2 step:1965 [D loss: 0.711738, acc.: 49.22%] [G loss: 0.857918]\n",
      "epoch:2 step:1966 [D loss: 0.697349, acc.: 56.25%] [G loss: 0.874464]\n",
      "epoch:2 step:1967 [D loss: 0.681441, acc.: 50.78%] [G loss: 0.869768]\n",
      "epoch:2 step:1968 [D loss: 0.675636, acc.: 57.81%] [G loss: 0.840553]\n",
      "epoch:2 step:1969 [D loss: 0.701647, acc.: 50.00%] [G loss: 0.825027]\n",
      "epoch:2 step:1970 [D loss: 0.713919, acc.: 49.22%] [G loss: 0.818036]\n",
      "epoch:2 step:1971 [D loss: 0.669040, acc.: 51.56%] [G loss: 0.849166]\n",
      "epoch:2 step:1972 [D loss: 0.667642, acc.: 63.28%] [G loss: 0.814467]\n",
      "epoch:2 step:1973 [D loss: 0.655774, acc.: 58.59%] [G loss: 0.894468]\n",
      "epoch:2 step:1974 [D loss: 0.653080, acc.: 61.72%] [G loss: 0.851123]\n",
      "epoch:2 step:1975 [D loss: 0.704514, acc.: 48.44%] [G loss: 0.810072]\n",
      "epoch:2 step:1976 [D loss: 0.678869, acc.: 60.16%] [G loss: 0.839535]\n",
      "epoch:2 step:1977 [D loss: 0.637892, acc.: 67.97%] [G loss: 0.810511]\n",
      "epoch:2 step:1978 [D loss: 0.684255, acc.: 56.25%] [G loss: 0.766924]\n",
      "epoch:2 step:1979 [D loss: 0.637203, acc.: 61.72%] [G loss: 0.817996]\n",
      "epoch:2 step:1980 [D loss: 0.680684, acc.: 54.69%] [G loss: 0.857041]\n",
      "epoch:2 step:1981 [D loss: 0.711852, acc.: 45.31%] [G loss: 0.821372]\n",
      "epoch:2 step:1982 [D loss: 0.724181, acc.: 54.69%] [G loss: 0.844752]\n",
      "epoch:2 step:1983 [D loss: 0.704442, acc.: 50.00%] [G loss: 0.857301]\n",
      "epoch:2 step:1984 [D loss: 0.700722, acc.: 52.34%] [G loss: 0.828747]\n",
      "epoch:2 step:1985 [D loss: 0.706384, acc.: 45.31%] [G loss: 0.790255]\n",
      "epoch:2 step:1986 [D loss: 0.708476, acc.: 52.34%] [G loss: 0.863250]\n",
      "epoch:2 step:1987 [D loss: 0.705917, acc.: 55.47%] [G loss: 0.913442]\n",
      "epoch:2 step:1988 [D loss: 0.713359, acc.: 53.12%] [G loss: 0.804047]\n",
      "epoch:2 step:1989 [D loss: 0.731838, acc.: 51.56%] [G loss: 0.857382]\n",
      "epoch:2 step:1990 [D loss: 0.690702, acc.: 55.47%] [G loss: 0.872892]\n",
      "epoch:2 step:1991 [D loss: 0.654333, acc.: 62.50%] [G loss: 0.858048]\n",
      "epoch:2 step:1992 [D loss: 0.665141, acc.: 58.59%] [G loss: 0.836493]\n",
      "epoch:2 step:1993 [D loss: 0.643258, acc.: 62.50%] [G loss: 0.830857]\n",
      "epoch:2 step:1994 [D loss: 0.672701, acc.: 57.81%] [G loss: 0.823411]\n",
      "epoch:2 step:1995 [D loss: 0.637579, acc.: 67.19%] [G loss: 0.932339]\n",
      "epoch:2 step:1996 [D loss: 0.705328, acc.: 53.12%] [G loss: 0.864712]\n",
      "epoch:2 step:1997 [D loss: 0.664153, acc.: 59.38%] [G loss: 0.801544]\n",
      "epoch:2 step:1998 [D loss: 0.704171, acc.: 53.91%] [G loss: 0.800046]\n",
      "epoch:2 step:1999 [D loss: 0.708232, acc.: 48.44%] [G loss: 0.836666]\n",
      "epoch:2 step:2000 [D loss: 0.712141, acc.: 47.66%] [G loss: 0.891957]\n",
      "epoch:2 step:2001 [D loss: 0.680977, acc.: 53.91%] [G loss: 1.005883]\n",
      "epoch:2 step:2002 [D loss: 0.722367, acc.: 53.91%] [G loss: 0.899690]\n",
      "epoch:2 step:2003 [D loss: 0.651491, acc.: 60.16%] [G loss: 0.887922]\n",
      "epoch:2 step:2004 [D loss: 0.681043, acc.: 51.56%] [G loss: 0.895365]\n",
      "epoch:2 step:2005 [D loss: 0.664290, acc.: 57.81%] [G loss: 0.862944]\n",
      "epoch:2 step:2006 [D loss: 0.686950, acc.: 51.56%] [G loss: 0.847201]\n",
      "epoch:2 step:2007 [D loss: 0.699405, acc.: 52.34%] [G loss: 0.817184]\n",
      "epoch:2 step:2008 [D loss: 0.688684, acc.: 56.25%] [G loss: 0.757442]\n",
      "epoch:2 step:2009 [D loss: 0.707605, acc.: 54.69%] [G loss: 0.876095]\n",
      "epoch:2 step:2010 [D loss: 0.705238, acc.: 51.56%] [G loss: 0.879165]\n",
      "epoch:2 step:2011 [D loss: 0.670825, acc.: 57.81%] [G loss: 0.810814]\n",
      "epoch:2 step:2012 [D loss: 0.723022, acc.: 47.66%] [G loss: 0.790335]\n",
      "epoch:2 step:2013 [D loss: 0.698970, acc.: 51.56%] [G loss: 0.749427]\n",
      "epoch:2 step:2014 [D loss: 0.649344, acc.: 64.84%] [G loss: 0.867193]\n",
      "epoch:2 step:2015 [D loss: 0.695188, acc.: 53.12%] [G loss: 0.852529]\n",
      "epoch:2 step:2016 [D loss: 0.646193, acc.: 70.31%] [G loss: 0.782682]\n",
      "epoch:2 step:2017 [D loss: 0.677220, acc.: 55.47%] [G loss: 0.824059]\n",
      "epoch:2 step:2018 [D loss: 0.672385, acc.: 53.91%] [G loss: 0.856679]\n",
      "epoch:2 step:2019 [D loss: 0.651215, acc.: 57.81%] [G loss: 0.814385]\n",
      "epoch:2 step:2020 [D loss: 0.698632, acc.: 55.47%] [G loss: 0.828287]\n",
      "epoch:2 step:2021 [D loss: 0.660911, acc.: 60.16%] [G loss: 0.952530]\n",
      "epoch:2 step:2022 [D loss: 0.653074, acc.: 63.28%] [G loss: 0.884512]\n",
      "epoch:2 step:2023 [D loss: 0.692695, acc.: 60.16%] [G loss: 0.852984]\n",
      "epoch:2 step:2024 [D loss: 0.647080, acc.: 63.28%] [G loss: 0.836434]\n",
      "epoch:2 step:2025 [D loss: 0.693905, acc.: 54.69%] [G loss: 0.804003]\n",
      "epoch:2 step:2026 [D loss: 0.668179, acc.: 58.59%] [G loss: 0.913954]\n",
      "epoch:2 step:2027 [D loss: 0.684340, acc.: 57.81%] [G loss: 0.832930]\n",
      "epoch:2 step:2028 [D loss: 0.701457, acc.: 49.22%] [G loss: 0.856463]\n",
      "epoch:2 step:2029 [D loss: 0.654045, acc.: 58.59%] [G loss: 0.876055]\n",
      "epoch:2 step:2030 [D loss: 0.658661, acc.: 63.28%] [G loss: 0.922927]\n",
      "epoch:2 step:2031 [D loss: 0.672805, acc.: 56.25%] [G loss: 0.886861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2032 [D loss: 0.660551, acc.: 62.50%] [G loss: 0.916399]\n",
      "epoch:2 step:2033 [D loss: 0.705225, acc.: 53.12%] [G loss: 0.869010]\n",
      "epoch:2 step:2034 [D loss: 0.681493, acc.: 60.94%] [G loss: 0.838719]\n",
      "epoch:2 step:2035 [D loss: 0.648717, acc.: 64.06%] [G loss: 0.893438]\n",
      "epoch:2 step:2036 [D loss: 0.699976, acc.: 52.34%] [G loss: 0.847702]\n",
      "epoch:2 step:2037 [D loss: 0.681597, acc.: 53.12%] [G loss: 0.876120]\n",
      "epoch:2 step:2038 [D loss: 0.698791, acc.: 55.47%] [G loss: 0.893420]\n",
      "epoch:2 step:2039 [D loss: 0.679117, acc.: 53.91%] [G loss: 0.914094]\n",
      "epoch:2 step:2040 [D loss: 0.683205, acc.: 55.47%] [G loss: 0.821810]\n",
      "epoch:2 step:2041 [D loss: 0.658723, acc.: 64.06%] [G loss: 0.802960]\n",
      "epoch:2 step:2042 [D loss: 0.702091, acc.: 53.91%] [G loss: 0.812736]\n",
      "epoch:2 step:2043 [D loss: 0.674348, acc.: 57.03%] [G loss: 1.007293]\n",
      "epoch:2 step:2044 [D loss: 0.659307, acc.: 57.03%] [G loss: 0.898970]\n",
      "epoch:2 step:2045 [D loss: 0.715976, acc.: 53.91%] [G loss: 0.856812]\n",
      "epoch:2 step:2046 [D loss: 0.716980, acc.: 52.34%] [G loss: 0.782601]\n",
      "epoch:2 step:2047 [D loss: 0.668337, acc.: 54.69%] [G loss: 0.798554]\n",
      "epoch:2 step:2048 [D loss: 0.653602, acc.: 59.38%] [G loss: 0.809346]\n",
      "epoch:2 step:2049 [D loss: 0.720899, acc.: 52.34%] [G loss: 0.869485]\n",
      "epoch:2 step:2050 [D loss: 0.690857, acc.: 60.16%] [G loss: 0.787085]\n",
      "epoch:2 step:2051 [D loss: 0.650062, acc.: 61.72%] [G loss: 0.873930]\n",
      "epoch:2 step:2052 [D loss: 0.671840, acc.: 60.16%] [G loss: 0.915181]\n",
      "epoch:2 step:2053 [D loss: 0.710293, acc.: 52.34%] [G loss: 0.948712]\n",
      "epoch:2 step:2054 [D loss: 0.673983, acc.: 60.16%] [G loss: 0.896221]\n",
      "epoch:2 step:2055 [D loss: 0.684130, acc.: 50.78%] [G loss: 0.929179]\n",
      "epoch:2 step:2056 [D loss: 0.674436, acc.: 53.91%] [G loss: 0.798444]\n",
      "epoch:2 step:2057 [D loss: 0.643524, acc.: 62.50%] [G loss: 0.902301]\n",
      "epoch:2 step:2058 [D loss: 0.715062, acc.: 55.47%] [G loss: 0.906413]\n",
      "epoch:2 step:2059 [D loss: 0.705748, acc.: 46.09%] [G loss: 0.937244]\n",
      "epoch:2 step:2060 [D loss: 0.711158, acc.: 50.78%] [G loss: 0.907347]\n",
      "epoch:2 step:2061 [D loss: 0.680612, acc.: 56.25%] [G loss: 0.811122]\n",
      "epoch:2 step:2062 [D loss: 0.698413, acc.: 58.59%] [G loss: 0.853343]\n",
      "epoch:2 step:2063 [D loss: 0.705492, acc.: 45.31%] [G loss: 0.886783]\n",
      "epoch:2 step:2064 [D loss: 0.683538, acc.: 54.69%] [G loss: 0.849650]\n",
      "epoch:2 step:2065 [D loss: 0.678232, acc.: 63.28%] [G loss: 0.865886]\n",
      "epoch:2 step:2066 [D loss: 0.769245, acc.: 50.00%] [G loss: 0.823503]\n",
      "epoch:2 step:2067 [D loss: 0.694646, acc.: 53.91%] [G loss: 0.881257]\n",
      "epoch:2 step:2068 [D loss: 0.700768, acc.: 50.78%] [G loss: 0.802341]\n",
      "epoch:2 step:2069 [D loss: 0.702256, acc.: 53.91%] [G loss: 0.878662]\n",
      "epoch:2 step:2070 [D loss: 0.699976, acc.: 51.56%] [G loss: 0.852076]\n",
      "epoch:2 step:2071 [D loss: 0.666986, acc.: 60.94%] [G loss: 0.855731]\n",
      "epoch:2 step:2072 [D loss: 0.690800, acc.: 54.69%] [G loss: 0.869885]\n",
      "epoch:2 step:2073 [D loss: 0.680355, acc.: 60.16%] [G loss: 0.888279]\n",
      "epoch:2 step:2074 [D loss: 0.674759, acc.: 52.34%] [G loss: 0.902208]\n",
      "epoch:2 step:2075 [D loss: 0.677326, acc.: 58.59%] [G loss: 0.879790]\n",
      "epoch:2 step:2076 [D loss: 0.671475, acc.: 60.16%] [G loss: 0.801336]\n",
      "epoch:2 step:2077 [D loss: 0.676258, acc.: 57.03%] [G loss: 0.954515]\n",
      "epoch:2 step:2078 [D loss: 0.681143, acc.: 60.94%] [G loss: 0.844136]\n",
      "epoch:2 step:2079 [D loss: 0.693330, acc.: 53.12%] [G loss: 0.819011]\n",
      "epoch:2 step:2080 [D loss: 0.695754, acc.: 53.91%] [G loss: 0.849608]\n",
      "epoch:2 step:2081 [D loss: 0.678761, acc.: 53.91%] [G loss: 0.939244]\n",
      "epoch:2 step:2082 [D loss: 0.656175, acc.: 63.28%] [G loss: 0.923748]\n",
      "epoch:2 step:2083 [D loss: 0.709442, acc.: 51.56%] [G loss: 0.850119]\n",
      "epoch:2 step:2084 [D loss: 0.677456, acc.: 53.12%] [G loss: 0.832217]\n",
      "epoch:2 step:2085 [D loss: 0.687023, acc.: 56.25%] [G loss: 0.812834]\n",
      "epoch:2 step:2086 [D loss: 0.699252, acc.: 54.69%] [G loss: 0.804428]\n",
      "epoch:2 step:2087 [D loss: 0.662742, acc.: 58.59%] [G loss: 0.874271]\n",
      "epoch:2 step:2088 [D loss: 0.669586, acc.: 56.25%] [G loss: 0.789256]\n",
      "epoch:2 step:2089 [D loss: 0.740692, acc.: 44.53%] [G loss: 0.896505]\n",
      "epoch:2 step:2090 [D loss: 0.683741, acc.: 56.25%] [G loss: 0.770825]\n",
      "epoch:2 step:2091 [D loss: 0.697004, acc.: 56.25%] [G loss: 0.761330]\n",
      "epoch:2 step:2092 [D loss: 0.706298, acc.: 52.34%] [G loss: 0.860920]\n",
      "epoch:2 step:2093 [D loss: 0.684716, acc.: 50.00%] [G loss: 0.910825]\n",
      "epoch:2 step:2094 [D loss: 0.722574, acc.: 52.34%] [G loss: 0.852908]\n",
      "epoch:2 step:2095 [D loss: 0.661671, acc.: 60.94%] [G loss: 0.909962]\n",
      "epoch:2 step:2096 [D loss: 0.673024, acc.: 58.59%] [G loss: 0.816714]\n",
      "epoch:2 step:2097 [D loss: 0.705849, acc.: 47.66%] [G loss: 0.969846]\n",
      "epoch:2 step:2098 [D loss: 0.650445, acc.: 55.47%] [G loss: 0.848786]\n",
      "epoch:2 step:2099 [D loss: 0.669903, acc.: 59.38%] [G loss: 0.868983]\n",
      "epoch:2 step:2100 [D loss: 0.683400, acc.: 57.81%] [G loss: 0.889142]\n",
      "epoch:2 step:2101 [D loss: 0.701858, acc.: 51.56%] [G loss: 0.804907]\n",
      "epoch:2 step:2102 [D loss: 0.722914, acc.: 55.47%] [G loss: 0.830362]\n",
      "epoch:2 step:2103 [D loss: 0.710792, acc.: 45.31%] [G loss: 0.891329]\n",
      "epoch:2 step:2104 [D loss: 0.692282, acc.: 49.22%] [G loss: 0.873083]\n",
      "epoch:2 step:2105 [D loss: 0.713015, acc.: 46.88%] [G loss: 0.904307]\n",
      "epoch:2 step:2106 [D loss: 0.710494, acc.: 52.34%] [G loss: 0.841958]\n",
      "epoch:2 step:2107 [D loss: 0.676325, acc.: 52.34%] [G loss: 0.841573]\n",
      "epoch:2 step:2108 [D loss: 0.687169, acc.: 53.91%] [G loss: 0.873336]\n",
      "epoch:2 step:2109 [D loss: 0.728544, acc.: 47.66%] [G loss: 0.781263]\n",
      "epoch:2 step:2110 [D loss: 0.679537, acc.: 57.81%] [G loss: 0.878664]\n",
      "epoch:2 step:2111 [D loss: 0.682744, acc.: 58.59%] [G loss: 0.885194]\n",
      "epoch:2 step:2112 [D loss: 0.672635, acc.: 58.59%] [G loss: 0.832888]\n",
      "epoch:2 step:2113 [D loss: 0.699686, acc.: 53.91%] [G loss: 0.823244]\n",
      "epoch:2 step:2114 [D loss: 0.668898, acc.: 54.69%] [G loss: 0.853073]\n",
      "epoch:2 step:2115 [D loss: 0.645321, acc.: 62.50%] [G loss: 0.881194]\n",
      "epoch:2 step:2116 [D loss: 0.660967, acc.: 61.72%] [G loss: 0.888771]\n",
      "epoch:2 step:2117 [D loss: 0.685887, acc.: 60.94%] [G loss: 0.946536]\n",
      "epoch:2 step:2118 [D loss: 0.655135, acc.: 60.16%] [G loss: 0.896073]\n",
      "epoch:2 step:2119 [D loss: 0.706048, acc.: 50.78%] [G loss: 0.851754]\n",
      "epoch:2 step:2120 [D loss: 0.647259, acc.: 63.28%] [G loss: 0.787448]\n",
      "epoch:2 step:2121 [D loss: 0.668049, acc.: 61.72%] [G loss: 0.771260]\n",
      "epoch:2 step:2122 [D loss: 0.699100, acc.: 50.00%] [G loss: 0.794945]\n",
      "epoch:2 step:2123 [D loss: 0.652801, acc.: 58.59%] [G loss: 0.866520]\n",
      "epoch:2 step:2124 [D loss: 0.707114, acc.: 57.03%] [G loss: 0.850738]\n",
      "epoch:2 step:2125 [D loss: 0.664203, acc.: 60.16%] [G loss: 0.861642]\n",
      "epoch:2 step:2126 [D loss: 0.697191, acc.: 57.81%] [G loss: 0.876076]\n",
      "epoch:2 step:2127 [D loss: 0.692775, acc.: 55.47%] [G loss: 0.886228]\n",
      "epoch:2 step:2128 [D loss: 0.694082, acc.: 58.59%] [G loss: 0.921324]\n",
      "epoch:2 step:2129 [D loss: 0.632194, acc.: 64.84%] [G loss: 0.893658]\n",
      "epoch:2 step:2130 [D loss: 0.687644, acc.: 55.47%] [G loss: 0.913231]\n",
      "epoch:2 step:2131 [D loss: 0.641492, acc.: 59.38%] [G loss: 1.021125]\n",
      "epoch:2 step:2132 [D loss: 0.665378, acc.: 52.34%] [G loss: 0.989943]\n",
      "epoch:2 step:2133 [D loss: 0.718296, acc.: 54.69%] [G loss: 0.947091]\n",
      "epoch:2 step:2134 [D loss: 0.649468, acc.: 57.03%] [G loss: 0.926080]\n",
      "epoch:2 step:2135 [D loss: 0.681375, acc.: 57.03%] [G loss: 0.971047]\n",
      "epoch:2 step:2136 [D loss: 0.704009, acc.: 54.69%] [G loss: 0.931197]\n",
      "epoch:2 step:2137 [D loss: 0.666004, acc.: 64.84%] [G loss: 0.923814]\n",
      "epoch:2 step:2138 [D loss: 0.714900, acc.: 46.09%] [G loss: 0.905132]\n",
      "epoch:2 step:2139 [D loss: 0.695869, acc.: 51.56%] [G loss: 0.897850]\n",
      "epoch:2 step:2140 [D loss: 0.661134, acc.: 60.16%] [G loss: 0.971489]\n",
      "epoch:2 step:2141 [D loss: 0.676501, acc.: 57.03%] [G loss: 0.945127]\n",
      "epoch:2 step:2142 [D loss: 0.682848, acc.: 57.81%] [G loss: 0.785848]\n",
      "epoch:2 step:2143 [D loss: 0.671098, acc.: 56.25%] [G loss: 0.759045]\n",
      "epoch:2 step:2144 [D loss: 0.671962, acc.: 60.16%] [G loss: 0.798422]\n",
      "epoch:2 step:2145 [D loss: 0.650285, acc.: 57.81%] [G loss: 1.011419]\n",
      "epoch:2 step:2146 [D loss: 0.672554, acc.: 57.81%] [G loss: 0.948664]\n",
      "epoch:2 step:2147 [D loss: 0.670563, acc.: 55.47%] [G loss: 0.874351]\n",
      "epoch:2 step:2148 [D loss: 0.703315, acc.: 53.12%] [G loss: 0.969187]\n",
      "epoch:2 step:2149 [D loss: 0.695997, acc.: 47.66%] [G loss: 0.924533]\n",
      "epoch:2 step:2150 [D loss: 0.695183, acc.: 55.47%] [G loss: 0.763744]\n",
      "epoch:2 step:2151 [D loss: 0.726627, acc.: 41.41%] [G loss: 0.865407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2152 [D loss: 0.667089, acc.: 60.16%] [G loss: 0.858491]\n",
      "epoch:2 step:2153 [D loss: 0.683760, acc.: 53.91%] [G loss: 0.825506]\n",
      "epoch:2 step:2154 [D loss: 0.703496, acc.: 53.91%] [G loss: 0.769767]\n",
      "epoch:2 step:2155 [D loss: 0.725368, acc.: 45.31%] [G loss: 0.866929]\n",
      "epoch:2 step:2156 [D loss: 0.660840, acc.: 60.16%] [G loss: 0.906151]\n",
      "epoch:2 step:2157 [D loss: 0.668869, acc.: 57.81%] [G loss: 0.887276]\n",
      "epoch:2 step:2158 [D loss: 0.656381, acc.: 65.62%] [G loss: 0.892287]\n",
      "epoch:2 step:2159 [D loss: 0.640533, acc.: 64.84%] [G loss: 0.793831]\n",
      "epoch:2 step:2160 [D loss: 0.722550, acc.: 49.22%] [G loss: 0.921481]\n",
      "epoch:2 step:2161 [D loss: 0.690192, acc.: 55.47%] [G loss: 0.868227]\n",
      "epoch:2 step:2162 [D loss: 0.712339, acc.: 50.78%] [G loss: 0.845325]\n",
      "epoch:2 step:2163 [D loss: 0.714417, acc.: 50.78%] [G loss: 0.869904]\n",
      "epoch:2 step:2164 [D loss: 0.687704, acc.: 53.12%] [G loss: 0.793664]\n",
      "epoch:2 step:2165 [D loss: 0.671037, acc.: 53.91%] [G loss: 0.800217]\n",
      "epoch:2 step:2166 [D loss: 0.687121, acc.: 52.34%] [G loss: 0.865357]\n",
      "epoch:2 step:2167 [D loss: 0.661162, acc.: 60.94%] [G loss: 0.791617]\n",
      "epoch:2 step:2168 [D loss: 0.671699, acc.: 61.72%] [G loss: 0.848120]\n",
      "epoch:2 step:2169 [D loss: 0.695589, acc.: 60.94%] [G loss: 0.813033]\n",
      "epoch:2 step:2170 [D loss: 0.645407, acc.: 57.81%] [G loss: 0.879635]\n",
      "epoch:2 step:2171 [D loss: 0.645953, acc.: 62.50%] [G loss: 0.822543]\n",
      "epoch:2 step:2172 [D loss: 0.700585, acc.: 51.56%] [G loss: 0.890286]\n",
      "epoch:2 step:2173 [D loss: 0.703335, acc.: 55.47%] [G loss: 0.838390]\n",
      "epoch:2 step:2174 [D loss: 0.689273, acc.: 62.50%] [G loss: 0.828307]\n",
      "epoch:2 step:2175 [D loss: 0.739943, acc.: 51.56%] [G loss: 0.804830]\n",
      "epoch:2 step:2176 [D loss: 0.681370, acc.: 54.69%] [G loss: 0.838130]\n",
      "epoch:2 step:2177 [D loss: 0.719195, acc.: 44.53%] [G loss: 0.908379]\n",
      "epoch:2 step:2178 [D loss: 0.709710, acc.: 51.56%] [G loss: 0.899519]\n",
      "epoch:2 step:2179 [D loss: 0.685307, acc.: 50.78%] [G loss: 0.937990]\n",
      "epoch:2 step:2180 [D loss: 0.695970, acc.: 51.56%] [G loss: 0.848018]\n",
      "epoch:2 step:2181 [D loss: 0.713964, acc.: 56.25%] [G loss: 0.876267]\n",
      "epoch:2 step:2182 [D loss: 0.708289, acc.: 50.00%] [G loss: 0.839461]\n",
      "epoch:2 step:2183 [D loss: 0.651485, acc.: 60.16%] [G loss: 0.908978]\n",
      "epoch:2 step:2184 [D loss: 0.675266, acc.: 60.16%] [G loss: 0.912570]\n",
      "epoch:2 step:2185 [D loss: 0.679541, acc.: 60.16%] [G loss: 0.892013]\n",
      "epoch:2 step:2186 [D loss: 0.671054, acc.: 60.16%] [G loss: 0.828186]\n",
      "epoch:2 step:2187 [D loss: 0.676531, acc.: 61.72%] [G loss: 0.861196]\n",
      "epoch:2 step:2188 [D loss: 0.663424, acc.: 59.38%] [G loss: 0.881618]\n",
      "epoch:2 step:2189 [D loss: 0.633707, acc.: 64.84%] [G loss: 0.887071]\n",
      "epoch:2 step:2190 [D loss: 0.688598, acc.: 51.56%] [G loss: 0.855751]\n",
      "epoch:2 step:2191 [D loss: 0.687220, acc.: 52.34%] [G loss: 0.852897]\n",
      "epoch:2 step:2192 [D loss: 0.648803, acc.: 60.16%] [G loss: 0.829830]\n",
      "epoch:2 step:2193 [D loss: 0.675417, acc.: 53.12%] [G loss: 0.819590]\n",
      "epoch:2 step:2194 [D loss: 0.676720, acc.: 58.59%] [G loss: 0.859985]\n",
      "epoch:2 step:2195 [D loss: 0.657169, acc.: 61.72%] [G loss: 0.816241]\n",
      "epoch:2 step:2196 [D loss: 0.685504, acc.: 55.47%] [G loss: 0.867712]\n",
      "epoch:2 step:2197 [D loss: 0.707271, acc.: 50.78%] [G loss: 0.758765]\n",
      "epoch:2 step:2198 [D loss: 0.693505, acc.: 52.34%] [G loss: 0.754221]\n",
      "epoch:2 step:2199 [D loss: 0.677414, acc.: 57.81%] [G loss: 0.814874]\n",
      "epoch:2 step:2200 [D loss: 0.709458, acc.: 53.91%] [G loss: 0.835878]\n",
      "epoch:2 step:2201 [D loss: 0.678611, acc.: 63.28%] [G loss: 0.883326]\n",
      "epoch:2 step:2202 [D loss: 0.661386, acc.: 57.81%] [G loss: 0.826346]\n",
      "epoch:2 step:2203 [D loss: 0.676785, acc.: 57.03%] [G loss: 0.880839]\n",
      "epoch:2 step:2204 [D loss: 0.653720, acc.: 67.19%] [G loss: 0.908603]\n",
      "epoch:2 step:2205 [D loss: 0.642425, acc.: 66.41%] [G loss: 0.947842]\n",
      "epoch:2 step:2206 [D loss: 0.653379, acc.: 64.84%] [G loss: 0.875929]\n",
      "epoch:2 step:2207 [D loss: 0.699528, acc.: 53.12%] [G loss: 0.805595]\n",
      "epoch:2 step:2208 [D loss: 0.693783, acc.: 51.56%] [G loss: 0.826471]\n",
      "epoch:2 step:2209 [D loss: 0.693768, acc.: 50.78%] [G loss: 0.898094]\n",
      "epoch:2 step:2210 [D loss: 0.693385, acc.: 49.22%] [G loss: 0.808720]\n",
      "epoch:2 step:2211 [D loss: 0.716908, acc.: 48.44%] [G loss: 0.842258]\n",
      "epoch:2 step:2212 [D loss: 0.728570, acc.: 47.66%] [G loss: 0.924673]\n",
      "epoch:2 step:2213 [D loss: 0.639741, acc.: 60.16%] [G loss: 0.935766]\n",
      "epoch:2 step:2214 [D loss: 0.664223, acc.: 57.81%] [G loss: 0.921925]\n",
      "epoch:2 step:2215 [D loss: 0.629363, acc.: 67.19%] [G loss: 0.852924]\n",
      "epoch:2 step:2216 [D loss: 0.678418, acc.: 61.72%] [G loss: 0.862781]\n",
      "epoch:2 step:2217 [D loss: 0.664992, acc.: 56.25%] [G loss: 0.861006]\n",
      "epoch:2 step:2218 [D loss: 0.678079, acc.: 53.12%] [G loss: 0.867500]\n",
      "epoch:2 step:2219 [D loss: 0.673405, acc.: 59.38%] [G loss: 0.907923]\n",
      "epoch:2 step:2220 [D loss: 0.694039, acc.: 48.44%] [G loss: 0.865730]\n",
      "epoch:2 step:2221 [D loss: 0.696296, acc.: 49.22%] [G loss: 0.914292]\n",
      "epoch:2 step:2222 [D loss: 0.701158, acc.: 51.56%] [G loss: 0.886331]\n",
      "epoch:2 step:2223 [D loss: 0.685810, acc.: 52.34%] [G loss: 0.856818]\n",
      "epoch:2 step:2224 [D loss: 0.717693, acc.: 56.25%] [G loss: 0.766925]\n",
      "epoch:2 step:2225 [D loss: 0.706316, acc.: 49.22%] [G loss: 0.885959]\n",
      "epoch:2 step:2226 [D loss: 0.708694, acc.: 48.44%] [G loss: 0.783631]\n",
      "epoch:2 step:2227 [D loss: 0.712915, acc.: 58.59%] [G loss: 0.802453]\n",
      "epoch:2 step:2228 [D loss: 0.702131, acc.: 52.34%] [G loss: 0.966367]\n",
      "epoch:2 step:2229 [D loss: 0.663890, acc.: 57.81%] [G loss: 0.881498]\n",
      "epoch:2 step:2230 [D loss: 0.730903, acc.: 50.00%] [G loss: 0.924416]\n",
      "epoch:2 step:2231 [D loss: 0.719063, acc.: 50.00%] [G loss: 0.885536]\n",
      "epoch:2 step:2232 [D loss: 0.706189, acc.: 48.44%] [G loss: 0.877479]\n",
      "epoch:2 step:2233 [D loss: 0.691154, acc.: 58.59%] [G loss: 0.894404]\n",
      "epoch:2 step:2234 [D loss: 0.679454, acc.: 53.12%] [G loss: 0.843502]\n",
      "epoch:2 step:2235 [D loss: 0.657176, acc.: 56.25%] [G loss: 0.888159]\n",
      "epoch:2 step:2236 [D loss: 0.702735, acc.: 54.69%] [G loss: 0.844307]\n",
      "epoch:2 step:2237 [D loss: 0.656649, acc.: 54.69%] [G loss: 0.934922]\n",
      "epoch:2 step:2238 [D loss: 0.727367, acc.: 49.22%] [G loss: 0.873419]\n",
      "epoch:2 step:2239 [D loss: 0.716958, acc.: 52.34%] [G loss: 0.831348]\n",
      "epoch:2 step:2240 [D loss: 0.686363, acc.: 57.81%] [G loss: 0.877697]\n",
      "epoch:2 step:2241 [D loss: 0.688594, acc.: 50.78%] [G loss: 0.907617]\n",
      "epoch:2 step:2242 [D loss: 0.682305, acc.: 53.91%] [G loss: 0.875704]\n",
      "epoch:2 step:2243 [D loss: 0.687417, acc.: 51.56%] [G loss: 0.792313]\n",
      "epoch:2 step:2244 [D loss: 0.680063, acc.: 53.91%] [G loss: 0.850659]\n",
      "epoch:2 step:2245 [D loss: 0.679207, acc.: 58.59%] [G loss: 0.883660]\n",
      "epoch:2 step:2246 [D loss: 0.685836, acc.: 50.78%] [G loss: 0.857883]\n",
      "epoch:2 step:2247 [D loss: 0.709780, acc.: 49.22%] [G loss: 0.827370]\n",
      "epoch:2 step:2248 [D loss: 0.655455, acc.: 60.94%] [G loss: 0.869512]\n",
      "epoch:2 step:2249 [D loss: 0.694683, acc.: 60.94%] [G loss: 0.845161]\n",
      "epoch:2 step:2250 [D loss: 0.698145, acc.: 46.09%] [G loss: 0.750742]\n",
      "epoch:2 step:2251 [D loss: 0.689201, acc.: 52.34%] [G loss: 0.845740]\n",
      "epoch:2 step:2252 [D loss: 0.663527, acc.: 59.38%] [G loss: 0.789238]\n",
      "epoch:2 step:2253 [D loss: 0.699783, acc.: 57.81%] [G loss: 0.814698]\n",
      "epoch:2 step:2254 [D loss: 0.704091, acc.: 49.22%] [G loss: 0.804373]\n",
      "epoch:2 step:2255 [D loss: 0.660887, acc.: 60.94%] [G loss: 0.883035]\n",
      "epoch:2 step:2256 [D loss: 0.715059, acc.: 48.44%] [G loss: 0.841219]\n",
      "epoch:2 step:2257 [D loss: 0.711991, acc.: 48.44%] [G loss: 0.819880]\n",
      "epoch:2 step:2258 [D loss: 0.684585, acc.: 52.34%] [G loss: 0.846716]\n",
      "epoch:2 step:2259 [D loss: 0.702272, acc.: 57.81%] [G loss: 0.811332]\n",
      "epoch:2 step:2260 [D loss: 0.668766, acc.: 59.38%] [G loss: 0.809269]\n",
      "epoch:2 step:2261 [D loss: 0.660842, acc.: 56.25%] [G loss: 0.837460]\n",
      "epoch:2 step:2262 [D loss: 0.698558, acc.: 53.91%] [G loss: 0.790048]\n",
      "epoch:2 step:2263 [D loss: 0.714069, acc.: 44.53%] [G loss: 0.799046]\n",
      "epoch:2 step:2264 [D loss: 0.690273, acc.: 48.44%] [G loss: 0.815959]\n",
      "epoch:2 step:2265 [D loss: 0.671232, acc.: 55.47%] [G loss: 0.770692]\n",
      "epoch:2 step:2266 [D loss: 0.711190, acc.: 49.22%] [G loss: 0.746580]\n",
      "epoch:2 step:2267 [D loss: 0.696311, acc.: 51.56%] [G loss: 0.795862]\n",
      "epoch:2 step:2268 [D loss: 0.704281, acc.: 52.34%] [G loss: 0.750651]\n",
      "epoch:2 step:2269 [D loss: 0.709392, acc.: 46.88%] [G loss: 0.717719]\n",
      "epoch:2 step:2270 [D loss: 0.720197, acc.: 44.53%] [G loss: 0.798735]\n",
      "epoch:2 step:2271 [D loss: 0.689393, acc.: 46.09%] [G loss: 0.798148]\n",
      "epoch:2 step:2272 [D loss: 0.662303, acc.: 58.59%] [G loss: 0.850026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2273 [D loss: 0.657122, acc.: 61.72%] [G loss: 0.867592]\n",
      "epoch:2 step:2274 [D loss: 0.674363, acc.: 62.50%] [G loss: 0.791677]\n",
      "epoch:2 step:2275 [D loss: 0.634757, acc.: 62.50%] [G loss: 0.844951]\n",
      "epoch:2 step:2276 [D loss: 0.661666, acc.: 60.16%] [G loss: 0.879782]\n",
      "epoch:2 step:2277 [D loss: 0.685863, acc.: 52.34%] [G loss: 0.743055]\n",
      "epoch:2 step:2278 [D loss: 0.696589, acc.: 50.00%] [G loss: 0.848564]\n",
      "epoch:2 step:2279 [D loss: 0.693740, acc.: 55.47%] [G loss: 0.888043]\n",
      "epoch:2 step:2280 [D loss: 0.701590, acc.: 52.34%] [G loss: 0.861653]\n",
      "epoch:2 step:2281 [D loss: 0.677871, acc.: 59.38%] [G loss: 0.870770]\n",
      "epoch:2 step:2282 [D loss: 0.682936, acc.: 54.69%] [G loss: 0.825333]\n",
      "epoch:2 step:2283 [D loss: 0.702781, acc.: 57.03%] [G loss: 0.825410]\n",
      "epoch:2 step:2284 [D loss: 0.677918, acc.: 59.38%] [G loss: 0.807255]\n",
      "epoch:2 step:2285 [D loss: 0.681309, acc.: 56.25%] [G loss: 0.802883]\n",
      "epoch:2 step:2286 [D loss: 0.709287, acc.: 52.34%] [G loss: 0.886107]\n",
      "epoch:2 step:2287 [D loss: 0.753752, acc.: 53.12%] [G loss: 0.886566]\n",
      "epoch:2 step:2288 [D loss: 0.690456, acc.: 54.69%] [G loss: 0.830640]\n",
      "epoch:2 step:2289 [D loss: 0.685025, acc.: 53.12%] [G loss: 0.911103]\n",
      "epoch:2 step:2290 [D loss: 0.700835, acc.: 51.56%] [G loss: 0.881857]\n",
      "epoch:2 step:2291 [D loss: 0.665864, acc.: 60.94%] [G loss: 0.810324]\n",
      "epoch:2 step:2292 [D loss: 0.694794, acc.: 47.66%] [G loss: 0.843874]\n",
      "epoch:2 step:2293 [D loss: 0.691934, acc.: 53.12%] [G loss: 0.812572]\n",
      "epoch:2 step:2294 [D loss: 0.672184, acc.: 57.81%] [G loss: 0.845255]\n",
      "epoch:2 step:2295 [D loss: 0.702465, acc.: 49.22%] [G loss: 0.804843]\n",
      "epoch:2 step:2296 [D loss: 0.675713, acc.: 58.59%] [G loss: 0.767530]\n",
      "epoch:2 step:2297 [D loss: 0.690723, acc.: 53.12%] [G loss: 0.841573]\n",
      "epoch:2 step:2298 [D loss: 0.670981, acc.: 54.69%] [G loss: 0.835749]\n",
      "epoch:2 step:2299 [D loss: 0.690513, acc.: 53.12%] [G loss: 0.838408]\n",
      "epoch:2 step:2300 [D loss: 0.671261, acc.: 58.59%] [G loss: 0.791402]\n",
      "epoch:2 step:2301 [D loss: 0.679159, acc.: 51.56%] [G loss: 0.755875]\n",
      "epoch:2 step:2302 [D loss: 0.679416, acc.: 55.47%] [G loss: 0.900994]\n",
      "epoch:2 step:2303 [D loss: 0.679685, acc.: 53.91%] [G loss: 0.805586]\n",
      "epoch:2 step:2304 [D loss: 0.682587, acc.: 58.59%] [G loss: 0.811311]\n",
      "epoch:2 step:2305 [D loss: 0.691044, acc.: 51.56%] [G loss: 0.816232]\n",
      "epoch:2 step:2306 [D loss: 0.676548, acc.: 64.06%] [G loss: 0.862267]\n",
      "epoch:2 step:2307 [D loss: 0.704991, acc.: 52.34%] [G loss: 0.946853]\n",
      "epoch:2 step:2308 [D loss: 0.691167, acc.: 60.94%] [G loss: 0.889810]\n",
      "epoch:2 step:2309 [D loss: 0.690199, acc.: 53.91%] [G loss: 0.834466]\n",
      "epoch:2 step:2310 [D loss: 0.647827, acc.: 63.28%] [G loss: 0.846591]\n",
      "epoch:2 step:2311 [D loss: 0.675042, acc.: 57.03%] [G loss: 0.819548]\n",
      "epoch:2 step:2312 [D loss: 0.650987, acc.: 57.81%] [G loss: 0.894785]\n",
      "epoch:2 step:2313 [D loss: 0.651191, acc.: 59.38%] [G loss: 0.869584]\n",
      "epoch:2 step:2314 [D loss: 0.695084, acc.: 56.25%] [G loss: 0.826403]\n",
      "epoch:2 step:2315 [D loss: 0.655491, acc.: 58.59%] [G loss: 0.809427]\n",
      "epoch:2 step:2316 [D loss: 0.680667, acc.: 64.06%] [G loss: 0.905329]\n",
      "epoch:2 step:2317 [D loss: 0.690527, acc.: 55.47%] [G loss: 0.807041]\n",
      "epoch:2 step:2318 [D loss: 0.627111, acc.: 62.50%] [G loss: 0.920286]\n",
      "epoch:2 step:2319 [D loss: 0.648542, acc.: 60.94%] [G loss: 0.929959]\n",
      "epoch:2 step:2320 [D loss: 0.709403, acc.: 54.69%] [G loss: 0.863806]\n",
      "epoch:2 step:2321 [D loss: 0.673707, acc.: 56.25%] [G loss: 0.975904]\n",
      "epoch:2 step:2322 [D loss: 0.718722, acc.: 56.25%] [G loss: 0.886905]\n",
      "epoch:2 step:2323 [D loss: 0.666015, acc.: 64.06%] [G loss: 0.797617]\n",
      "epoch:2 step:2324 [D loss: 0.683317, acc.: 57.03%] [G loss: 0.898617]\n",
      "epoch:2 step:2325 [D loss: 0.665891, acc.: 53.91%] [G loss: 0.864109]\n",
      "epoch:2 step:2326 [D loss: 0.705801, acc.: 53.12%] [G loss: 0.887023]\n",
      "epoch:2 step:2327 [D loss: 0.661005, acc.: 57.81%] [G loss: 0.882717]\n",
      "epoch:2 step:2328 [D loss: 0.662011, acc.: 53.91%] [G loss: 0.913293]\n",
      "epoch:2 step:2329 [D loss: 0.664487, acc.: 58.59%] [G loss: 0.807436]\n",
      "epoch:2 step:2330 [D loss: 0.676162, acc.: 61.72%] [G loss: 0.859798]\n",
      "epoch:2 step:2331 [D loss: 0.722835, acc.: 44.53%] [G loss: 0.817834]\n",
      "epoch:2 step:2332 [D loss: 0.673365, acc.: 57.81%] [G loss: 0.853626]\n",
      "epoch:2 step:2333 [D loss: 0.655870, acc.: 60.16%] [G loss: 0.804119]\n",
      "epoch:2 step:2334 [D loss: 0.716393, acc.: 52.34%] [G loss: 0.842967]\n",
      "epoch:2 step:2335 [D loss: 0.670491, acc.: 57.03%] [G loss: 0.859433]\n",
      "epoch:2 step:2336 [D loss: 0.660802, acc.: 60.94%] [G loss: 0.793434]\n",
      "epoch:2 step:2337 [D loss: 0.667220, acc.: 61.72%] [G loss: 0.841839]\n",
      "epoch:2 step:2338 [D loss: 0.643008, acc.: 60.94%] [G loss: 0.918759]\n",
      "epoch:2 step:2339 [D loss: 0.722966, acc.: 48.44%] [G loss: 0.842072]\n",
      "epoch:2 step:2340 [D loss: 0.687601, acc.: 58.59%] [G loss: 0.835431]\n",
      "epoch:2 step:2341 [D loss: 0.664215, acc.: 60.16%] [G loss: 0.843615]\n",
      "epoch:2 step:2342 [D loss: 0.646621, acc.: 64.84%] [G loss: 0.915648]\n",
      "epoch:2 step:2343 [D loss: 0.641447, acc.: 60.94%] [G loss: 0.826961]\n",
      "epoch:2 step:2344 [D loss: 0.699699, acc.: 59.38%] [G loss: 0.864995]\n",
      "epoch:2 step:2345 [D loss: 0.655918, acc.: 65.62%] [G loss: 0.839741]\n",
      "epoch:2 step:2346 [D loss: 0.686087, acc.: 58.59%] [G loss: 0.859216]\n",
      "epoch:2 step:2347 [D loss: 0.676414, acc.: 53.12%] [G loss: 0.832549]\n",
      "epoch:2 step:2348 [D loss: 0.671946, acc.: 57.81%] [G loss: 0.834633]\n",
      "epoch:2 step:2349 [D loss: 0.685656, acc.: 54.69%] [G loss: 0.870083]\n",
      "epoch:2 step:2350 [D loss: 0.748599, acc.: 50.00%] [G loss: 0.851584]\n",
      "epoch:2 step:2351 [D loss: 0.716714, acc.: 49.22%] [G loss: 0.803946]\n",
      "epoch:2 step:2352 [D loss: 0.720264, acc.: 48.44%] [G loss: 0.865025]\n",
      "epoch:2 step:2353 [D loss: 0.694093, acc.: 55.47%] [G loss: 0.861292]\n",
      "epoch:2 step:2354 [D loss: 0.678424, acc.: 54.69%] [G loss: 0.801749]\n",
      "epoch:2 step:2355 [D loss: 0.715831, acc.: 44.53%] [G loss: 0.883044]\n",
      "epoch:2 step:2356 [D loss: 0.670703, acc.: 55.47%] [G loss: 0.844072]\n",
      "epoch:2 step:2357 [D loss: 0.706654, acc.: 52.34%] [G loss: 0.855436]\n",
      "epoch:2 step:2358 [D loss: 0.701022, acc.: 54.69%] [G loss: 0.889322]\n",
      "epoch:2 step:2359 [D loss: 0.650393, acc.: 61.72%] [G loss: 0.840420]\n",
      "epoch:2 step:2360 [D loss: 0.686913, acc.: 54.69%] [G loss: 0.890924]\n",
      "epoch:2 step:2361 [D loss: 0.689879, acc.: 50.78%] [G loss: 0.815773]\n",
      "epoch:2 step:2362 [D loss: 0.699472, acc.: 58.59%] [G loss: 0.860763]\n",
      "epoch:2 step:2363 [D loss: 0.667314, acc.: 59.38%] [G loss: 0.860691]\n",
      "epoch:2 step:2364 [D loss: 0.676403, acc.: 51.56%] [G loss: 0.949155]\n",
      "epoch:2 step:2365 [D loss: 0.715992, acc.: 50.78%] [G loss: 0.849108]\n",
      "epoch:2 step:2366 [D loss: 0.667950, acc.: 57.81%] [G loss: 0.791723]\n",
      "epoch:2 step:2367 [D loss: 0.768825, acc.: 47.66%] [G loss: 0.851046]\n",
      "epoch:2 step:2368 [D loss: 0.660038, acc.: 59.38%] [G loss: 0.841138]\n",
      "epoch:2 step:2369 [D loss: 0.709634, acc.: 51.56%] [G loss: 0.806132]\n",
      "epoch:2 step:2370 [D loss: 0.658063, acc.: 62.50%] [G loss: 0.894029]\n",
      "epoch:2 step:2371 [D loss: 0.693975, acc.: 53.12%] [G loss: 0.808088]\n",
      "epoch:2 step:2372 [D loss: 0.696945, acc.: 55.47%] [G loss: 0.827129]\n",
      "epoch:2 step:2373 [D loss: 0.696669, acc.: 58.59%] [G loss: 0.810712]\n",
      "epoch:2 step:2374 [D loss: 0.694483, acc.: 52.34%] [G loss: 0.782453]\n",
      "epoch:2 step:2375 [D loss: 0.678108, acc.: 55.47%] [G loss: 0.897786]\n",
      "epoch:2 step:2376 [D loss: 0.688615, acc.: 60.16%] [G loss: 0.816123]\n",
      "epoch:2 step:2377 [D loss: 0.690722, acc.: 54.69%] [G loss: 0.808342]\n",
      "epoch:2 step:2378 [D loss: 0.696469, acc.: 50.00%] [G loss: 0.807998]\n",
      "epoch:2 step:2379 [D loss: 0.676866, acc.: 58.59%] [G loss: 0.845003]\n",
      "epoch:2 step:2380 [D loss: 0.683001, acc.: 55.47%] [G loss: 0.832101]\n",
      "epoch:2 step:2381 [D loss: 0.691808, acc.: 55.47%] [G loss: 0.823840]\n",
      "epoch:2 step:2382 [D loss: 0.693448, acc.: 51.56%] [G loss: 0.824671]\n",
      "epoch:2 step:2383 [D loss: 0.690635, acc.: 53.12%] [G loss: 0.801020]\n",
      "epoch:2 step:2384 [D loss: 0.674339, acc.: 53.12%] [G loss: 0.839075]\n",
      "epoch:2 step:2385 [D loss: 0.681749, acc.: 57.03%] [G loss: 0.776090]\n",
      "epoch:2 step:2386 [D loss: 0.696787, acc.: 49.22%] [G loss: 0.744998]\n",
      "epoch:2 step:2387 [D loss: 0.662667, acc.: 60.94%] [G loss: 0.856831]\n",
      "epoch:2 step:2388 [D loss: 0.697742, acc.: 50.78%] [G loss: 0.877607]\n",
      "epoch:2 step:2389 [D loss: 0.659649, acc.: 60.94%] [G loss: 0.872948]\n",
      "epoch:2 step:2390 [D loss: 0.693391, acc.: 58.59%] [G loss: 0.824145]\n",
      "epoch:2 step:2391 [D loss: 0.668213, acc.: 61.72%] [G loss: 0.844573]\n",
      "epoch:2 step:2392 [D loss: 0.688357, acc.: 53.12%] [G loss: 0.868653]\n",
      "epoch:2 step:2393 [D loss: 0.654646, acc.: 62.50%] [G loss: 0.866556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2394 [D loss: 0.670181, acc.: 55.47%] [G loss: 0.795599]\n",
      "epoch:2 step:2395 [D loss: 0.684011, acc.: 54.69%] [G loss: 0.903582]\n",
      "epoch:2 step:2396 [D loss: 0.709679, acc.: 45.31%] [G loss: 0.858173]\n",
      "epoch:2 step:2397 [D loss: 0.653906, acc.: 59.38%] [G loss: 0.800412]\n",
      "epoch:2 step:2398 [D loss: 0.662964, acc.: 57.03%] [G loss: 0.772452]\n",
      "epoch:2 step:2399 [D loss: 0.723021, acc.: 52.34%] [G loss: 0.824554]\n",
      "epoch:2 step:2400 [D loss: 0.709461, acc.: 53.91%] [G loss: 0.869214]\n",
      "epoch:2 step:2401 [D loss: 0.702083, acc.: 48.44%] [G loss: 0.814722]\n",
      "epoch:2 step:2402 [D loss: 0.698981, acc.: 57.03%] [G loss: 0.840260]\n",
      "epoch:2 step:2403 [D loss: 0.666199, acc.: 58.59%] [G loss: 0.867667]\n",
      "epoch:2 step:2404 [D loss: 0.683800, acc.: 57.03%] [G loss: 0.786879]\n",
      "epoch:2 step:2405 [D loss: 0.695970, acc.: 56.25%] [G loss: 0.808179]\n",
      "epoch:2 step:2406 [D loss: 0.709978, acc.: 51.56%] [G loss: 0.877832]\n",
      "epoch:2 step:2407 [D loss: 0.717315, acc.: 50.78%] [G loss: 0.799376]\n",
      "epoch:2 step:2408 [D loss: 0.685099, acc.: 56.25%] [G loss: 0.808985]\n",
      "epoch:2 step:2409 [D loss: 0.685370, acc.: 56.25%] [G loss: 0.829613]\n",
      "epoch:2 step:2410 [D loss: 0.657167, acc.: 61.72%] [G loss: 0.827982]\n",
      "epoch:2 step:2411 [D loss: 0.681534, acc.: 57.81%] [G loss: 0.832243]\n",
      "epoch:2 step:2412 [D loss: 0.685794, acc.: 45.31%] [G loss: 0.845439]\n",
      "epoch:2 step:2413 [D loss: 0.711905, acc.: 49.22%] [G loss: 0.764844]\n",
      "epoch:2 step:2414 [D loss: 0.670205, acc.: 55.47%] [G loss: 0.821665]\n",
      "epoch:2 step:2415 [D loss: 0.669352, acc.: 59.38%] [G loss: 0.844256]\n",
      "epoch:2 step:2416 [D loss: 0.684427, acc.: 53.12%] [G loss: 0.809642]\n",
      "epoch:2 step:2417 [D loss: 0.683106, acc.: 57.81%] [G loss: 0.875174]\n",
      "epoch:2 step:2418 [D loss: 0.680873, acc.: 56.25%] [G loss: 0.877845]\n",
      "epoch:2 step:2419 [D loss: 0.641936, acc.: 62.50%] [G loss: 0.837706]\n",
      "epoch:2 step:2420 [D loss: 0.660898, acc.: 62.50%] [G loss: 0.945974]\n",
      "epoch:2 step:2421 [D loss: 0.716426, acc.: 54.69%] [G loss: 0.913948]\n",
      "epoch:2 step:2422 [D loss: 0.705600, acc.: 50.78%] [G loss: 0.851788]\n",
      "epoch:2 step:2423 [D loss: 0.732825, acc.: 43.75%] [G loss: 0.807520]\n",
      "epoch:2 step:2424 [D loss: 0.698743, acc.: 53.12%] [G loss: 0.824051]\n",
      "epoch:2 step:2425 [D loss: 0.686333, acc.: 58.59%] [G loss: 0.842640]\n",
      "epoch:2 step:2426 [D loss: 0.705698, acc.: 53.12%] [G loss: 0.949500]\n",
      "epoch:2 step:2427 [D loss: 0.691310, acc.: 55.47%] [G loss: 0.853821]\n",
      "epoch:2 step:2428 [D loss: 0.644601, acc.: 64.84%] [G loss: 0.872427]\n",
      "epoch:2 step:2429 [D loss: 0.619343, acc.: 70.31%] [G loss: 0.903207]\n",
      "epoch:2 step:2430 [D loss: 0.676843, acc.: 60.16%] [G loss: 0.869045]\n",
      "epoch:2 step:2431 [D loss: 0.711693, acc.: 49.22%] [G loss: 0.817880]\n",
      "epoch:2 step:2432 [D loss: 0.732138, acc.: 49.22%] [G loss: 0.817017]\n",
      "epoch:2 step:2433 [D loss: 0.692750, acc.: 50.78%] [G loss: 0.803551]\n",
      "epoch:2 step:2434 [D loss: 0.679734, acc.: 58.59%] [G loss: 0.818064]\n",
      "epoch:2 step:2435 [D loss: 0.709259, acc.: 53.91%] [G loss: 0.814304]\n",
      "epoch:2 step:2436 [D loss: 0.689303, acc.: 55.47%] [G loss: 0.794130]\n",
      "epoch:2 step:2437 [D loss: 0.694863, acc.: 46.88%] [G loss: 0.841709]\n",
      "epoch:2 step:2438 [D loss: 0.715850, acc.: 52.34%] [G loss: 0.824863]\n",
      "epoch:2 step:2439 [D loss: 0.688162, acc.: 49.22%] [G loss: 0.889462]\n",
      "epoch:2 step:2440 [D loss: 0.690827, acc.: 50.78%] [G loss: 0.818206]\n",
      "epoch:2 step:2441 [D loss: 0.654807, acc.: 60.16%] [G loss: 0.810046]\n",
      "epoch:2 step:2442 [D loss: 0.661300, acc.: 57.81%] [G loss: 0.835073]\n",
      "epoch:2 step:2443 [D loss: 0.649071, acc.: 60.94%] [G loss: 0.783883]\n",
      "epoch:2 step:2444 [D loss: 0.724816, acc.: 50.00%] [G loss: 0.807409]\n",
      "epoch:2 step:2445 [D loss: 0.693485, acc.: 58.59%] [G loss: 0.806551]\n",
      "epoch:2 step:2446 [D loss: 0.698735, acc.: 50.78%] [G loss: 0.831820]\n",
      "epoch:2 step:2447 [D loss: 0.718550, acc.: 43.75%] [G loss: 0.832084]\n",
      "epoch:2 step:2448 [D loss: 0.700926, acc.: 59.38%] [G loss: 0.842026]\n",
      "epoch:2 step:2449 [D loss: 0.705855, acc.: 55.47%] [G loss: 0.914609]\n",
      "epoch:2 step:2450 [D loss: 0.662414, acc.: 59.38%] [G loss: 0.841970]\n",
      "epoch:2 step:2451 [D loss: 0.674508, acc.: 53.91%] [G loss: 0.898267]\n",
      "epoch:2 step:2452 [D loss: 0.669160, acc.: 58.59%] [G loss: 0.868149]\n",
      "epoch:2 step:2453 [D loss: 0.662939, acc.: 58.59%] [G loss: 0.856125]\n",
      "epoch:2 step:2454 [D loss: 0.692052, acc.: 53.91%] [G loss: 0.889482]\n",
      "epoch:2 step:2455 [D loss: 0.713287, acc.: 48.44%] [G loss: 0.803018]\n",
      "epoch:2 step:2456 [D loss: 0.689601, acc.: 51.56%] [G loss: 0.816187]\n",
      "epoch:2 step:2457 [D loss: 0.704919, acc.: 49.22%] [G loss: 0.840672]\n",
      "epoch:2 step:2458 [D loss: 0.692966, acc.: 54.69%] [G loss: 0.839559]\n",
      "epoch:2 step:2459 [D loss: 0.665347, acc.: 56.25%] [G loss: 0.809183]\n",
      "epoch:2 step:2460 [D loss: 0.687403, acc.: 52.34%] [G loss: 0.802611]\n",
      "epoch:2 step:2461 [D loss: 0.661562, acc.: 61.72%] [G loss: 0.859759]\n",
      "epoch:2 step:2462 [D loss: 0.664593, acc.: 62.50%] [G loss: 0.901297]\n",
      "epoch:2 step:2463 [D loss: 0.669069, acc.: 57.03%] [G loss: 0.845812]\n",
      "epoch:2 step:2464 [D loss: 0.671538, acc.: 62.50%] [G loss: 0.848700]\n",
      "epoch:2 step:2465 [D loss: 0.701439, acc.: 48.44%] [G loss: 0.835408]\n",
      "epoch:2 step:2466 [D loss: 0.641234, acc.: 62.50%] [G loss: 0.788962]\n",
      "epoch:2 step:2467 [D loss: 0.702209, acc.: 50.78%] [G loss: 0.931195]\n",
      "epoch:2 step:2468 [D loss: 0.669900, acc.: 60.16%] [G loss: 0.902436]\n",
      "epoch:2 step:2469 [D loss: 0.696119, acc.: 55.47%] [G loss: 0.851108]\n",
      "epoch:2 step:2470 [D loss: 0.634106, acc.: 68.75%] [G loss: 0.853306]\n",
      "epoch:2 step:2471 [D loss: 0.669596, acc.: 56.25%] [G loss: 0.856090]\n",
      "epoch:2 step:2472 [D loss: 0.699342, acc.: 53.91%] [G loss: 0.891170]\n",
      "epoch:2 step:2473 [D loss: 0.695359, acc.: 46.09%] [G loss: 0.861109]\n",
      "epoch:2 step:2474 [D loss: 0.656723, acc.: 57.81%] [G loss: 0.951089]\n",
      "epoch:2 step:2475 [D loss: 0.743862, acc.: 45.31%] [G loss: 0.859812]\n",
      "epoch:2 step:2476 [D loss: 0.636516, acc.: 67.19%] [G loss: 0.840548]\n",
      "epoch:2 step:2477 [D loss: 0.640847, acc.: 61.72%] [G loss: 0.882893]\n",
      "epoch:2 step:2478 [D loss: 0.699995, acc.: 53.12%] [G loss: 0.854049]\n",
      "epoch:2 step:2479 [D loss: 0.654996, acc.: 62.50%] [G loss: 0.871615]\n",
      "epoch:2 step:2480 [D loss: 0.633785, acc.: 64.84%] [G loss: 0.825056]\n",
      "epoch:2 step:2481 [D loss: 0.648805, acc.: 61.72%] [G loss: 0.790216]\n",
      "epoch:2 step:2482 [D loss: 0.699017, acc.: 53.91%] [G loss: 0.878247]\n",
      "epoch:2 step:2483 [D loss: 0.670802, acc.: 61.72%] [G loss: 0.937054]\n",
      "epoch:2 step:2484 [D loss: 0.679073, acc.: 51.56%] [G loss: 0.886705]\n",
      "epoch:2 step:2485 [D loss: 0.676865, acc.: 58.59%] [G loss: 0.824980]\n",
      "epoch:2 step:2486 [D loss: 0.713258, acc.: 52.34%] [G loss: 0.857836]\n",
      "epoch:2 step:2487 [D loss: 0.736674, acc.: 46.09%] [G loss: 0.791115]\n",
      "epoch:2 step:2488 [D loss: 0.703514, acc.: 52.34%] [G loss: 0.820791]\n",
      "epoch:2 step:2489 [D loss: 0.676192, acc.: 53.91%] [G loss: 0.838878]\n",
      "epoch:2 step:2490 [D loss: 0.666664, acc.: 60.94%] [G loss: 0.879976]\n",
      "epoch:2 step:2491 [D loss: 0.685729, acc.: 54.69%] [G loss: 0.869297]\n",
      "epoch:2 step:2492 [D loss: 0.641766, acc.: 67.19%] [G loss: 0.864209]\n",
      "epoch:2 step:2493 [D loss: 0.708103, acc.: 45.31%] [G loss: 0.929026]\n",
      "epoch:2 step:2494 [D loss: 0.679552, acc.: 49.22%] [G loss: 0.828611]\n",
      "epoch:2 step:2495 [D loss: 0.671720, acc.: 52.34%] [G loss: 0.830767]\n",
      "epoch:2 step:2496 [D loss: 0.664856, acc.: 57.81%] [G loss: 0.887269]\n",
      "epoch:2 step:2497 [D loss: 0.669251, acc.: 60.16%] [G loss: 0.823727]\n",
      "epoch:2 step:2498 [D loss: 0.670764, acc.: 52.34%] [G loss: 0.876172]\n",
      "epoch:2 step:2499 [D loss: 0.691968, acc.: 51.56%] [G loss: 0.826366]\n",
      "epoch:2 step:2500 [D loss: 0.724931, acc.: 49.22%] [G loss: 0.792615]\n",
      "epoch:2 step:2501 [D loss: 0.742234, acc.: 46.88%] [G loss: 0.804830]\n",
      "epoch:2 step:2502 [D loss: 0.714521, acc.: 52.34%] [G loss: 0.799989]\n",
      "epoch:2 step:2503 [D loss: 0.707089, acc.: 46.09%] [G loss: 0.907376]\n",
      "epoch:2 step:2504 [D loss: 0.678001, acc.: 57.81%] [G loss: 0.921459]\n",
      "epoch:2 step:2505 [D loss: 0.698662, acc.: 50.78%] [G loss: 0.800802]\n",
      "epoch:2 step:2506 [D loss: 0.695530, acc.: 50.78%] [G loss: 0.869727]\n",
      "epoch:2 step:2507 [D loss: 0.715677, acc.: 49.22%] [G loss: 0.884290]\n",
      "epoch:2 step:2508 [D loss: 0.690648, acc.: 54.69%] [G loss: 0.817564]\n",
      "epoch:2 step:2509 [D loss: 0.663745, acc.: 57.81%] [G loss: 0.842777]\n",
      "epoch:2 step:2510 [D loss: 0.668933, acc.: 58.59%] [G loss: 0.861625]\n",
      "epoch:2 step:2511 [D loss: 0.680495, acc.: 55.47%] [G loss: 0.780021]\n",
      "epoch:2 step:2512 [D loss: 0.650899, acc.: 56.25%] [G loss: 0.857343]\n",
      "epoch:2 step:2513 [D loss: 0.718913, acc.: 48.44%] [G loss: 0.796535]\n",
      "epoch:2 step:2514 [D loss: 0.711009, acc.: 49.22%] [G loss: 0.741403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2515 [D loss: 0.737469, acc.: 44.53%] [G loss: 0.730402]\n",
      "epoch:2 step:2516 [D loss: 0.679543, acc.: 55.47%] [G loss: 0.839452]\n",
      "epoch:2 step:2517 [D loss: 0.699927, acc.: 49.22%] [G loss: 0.887142]\n",
      "epoch:2 step:2518 [D loss: 0.701834, acc.: 53.91%] [G loss: 0.819828]\n",
      "epoch:2 step:2519 [D loss: 0.692215, acc.: 59.38%] [G loss: 0.797021]\n",
      "epoch:2 step:2520 [D loss: 0.836847, acc.: 41.41%] [G loss: 0.845567]\n",
      "epoch:2 step:2521 [D loss: 0.668999, acc.: 59.38%] [G loss: 0.860130]\n",
      "epoch:2 step:2522 [D loss: 0.675996, acc.: 50.00%] [G loss: 0.851665]\n",
      "epoch:2 step:2523 [D loss: 0.684779, acc.: 55.47%] [G loss: 0.836347]\n",
      "epoch:2 step:2524 [D loss: 0.682827, acc.: 61.72%] [G loss: 0.847474]\n",
      "epoch:2 step:2525 [D loss: 0.664986, acc.: 60.16%] [G loss: 0.849925]\n",
      "epoch:2 step:2526 [D loss: 0.665772, acc.: 57.03%] [G loss: 0.872267]\n",
      "epoch:2 step:2527 [D loss: 0.725909, acc.: 53.91%] [G loss: 0.890793]\n",
      "epoch:2 step:2528 [D loss: 0.678952, acc.: 49.22%] [G loss: 0.760624]\n",
      "epoch:2 step:2529 [D loss: 0.684514, acc.: 51.56%] [G loss: 0.810116]\n",
      "epoch:2 step:2530 [D loss: 0.647681, acc.: 65.62%] [G loss: 0.844641]\n",
      "epoch:2 step:2531 [D loss: 0.637316, acc.: 70.31%] [G loss: 0.804929]\n",
      "epoch:2 step:2532 [D loss: 0.704225, acc.: 54.69%] [G loss: 0.766381]\n",
      "epoch:2 step:2533 [D loss: 0.675593, acc.: 59.38%] [G loss: 0.816007]\n",
      "epoch:2 step:2534 [D loss: 0.672550, acc.: 62.50%] [G loss: 0.776920]\n",
      "epoch:2 step:2535 [D loss: 0.723004, acc.: 42.19%] [G loss: 0.781701]\n",
      "epoch:2 step:2536 [D loss: 0.713081, acc.: 46.09%] [G loss: 0.823056]\n",
      "epoch:2 step:2537 [D loss: 0.660309, acc.: 60.16%] [G loss: 0.852530]\n",
      "epoch:2 step:2538 [D loss: 0.674407, acc.: 60.16%] [G loss: 0.811777]\n",
      "epoch:2 step:2539 [D loss: 0.657157, acc.: 64.06%] [G loss: 0.824256]\n",
      "epoch:2 step:2540 [D loss: 0.678706, acc.: 53.12%] [G loss: 0.830448]\n",
      "epoch:2 step:2541 [D loss: 0.710734, acc.: 53.12%] [G loss: 0.816303]\n",
      "epoch:2 step:2542 [D loss: 0.680273, acc.: 60.16%] [G loss: 0.831123]\n",
      "epoch:2 step:2543 [D loss: 0.672505, acc.: 59.38%] [G loss: 0.830352]\n",
      "epoch:2 step:2544 [D loss: 0.721974, acc.: 53.12%] [G loss: 0.859363]\n",
      "epoch:2 step:2545 [D loss: 0.679307, acc.: 53.91%] [G loss: 0.813596]\n",
      "epoch:2 step:2546 [D loss: 0.691774, acc.: 50.00%] [G loss: 0.827487]\n",
      "epoch:2 step:2547 [D loss: 0.705359, acc.: 43.75%] [G loss: 0.828616]\n",
      "epoch:2 step:2548 [D loss: 0.697506, acc.: 46.09%] [G loss: 0.761128]\n",
      "epoch:2 step:2549 [D loss: 0.733901, acc.: 47.66%] [G loss: 0.800444]\n",
      "epoch:2 step:2550 [D loss: 0.766327, acc.: 35.94%] [G loss: 0.797550]\n",
      "epoch:2 step:2551 [D loss: 0.699338, acc.: 54.69%] [G loss: 0.868117]\n",
      "epoch:2 step:2552 [D loss: 0.694939, acc.: 53.91%] [G loss: 0.856242]\n",
      "epoch:2 step:2553 [D loss: 0.697438, acc.: 51.56%] [G loss: 0.796349]\n",
      "epoch:2 step:2554 [D loss: 0.682745, acc.: 54.69%] [G loss: 0.824965]\n",
      "epoch:2 step:2555 [D loss: 0.705453, acc.: 52.34%] [G loss: 0.809765]\n",
      "epoch:2 step:2556 [D loss: 0.698744, acc.: 49.22%] [G loss: 0.790829]\n",
      "epoch:2 step:2557 [D loss: 0.720695, acc.: 42.19%] [G loss: 0.813795]\n",
      "epoch:2 step:2558 [D loss: 0.690053, acc.: 50.00%] [G loss: 0.865773]\n",
      "epoch:2 step:2559 [D loss: 0.701771, acc.: 53.91%] [G loss: 0.920598]\n",
      "epoch:2 step:2560 [D loss: 0.651446, acc.: 62.50%] [G loss: 0.817533]\n",
      "epoch:2 step:2561 [D loss: 0.705587, acc.: 53.12%] [G loss: 0.788896]\n",
      "epoch:2 step:2562 [D loss: 0.694697, acc.: 50.78%] [G loss: 0.788828]\n",
      "epoch:2 step:2563 [D loss: 0.689911, acc.: 46.88%] [G loss: 0.896674]\n",
      "epoch:2 step:2564 [D loss: 0.691347, acc.: 53.12%] [G loss: 0.786515]\n",
      "epoch:2 step:2565 [D loss: 0.697776, acc.: 50.00%] [G loss: 0.854208]\n",
      "epoch:2 step:2566 [D loss: 0.692389, acc.: 59.38%] [G loss: 0.791977]\n",
      "epoch:2 step:2567 [D loss: 0.695712, acc.: 57.03%] [G loss: 0.823712]\n",
      "epoch:2 step:2568 [D loss: 0.704896, acc.: 52.34%] [G loss: 0.792946]\n",
      "epoch:2 step:2569 [D loss: 0.664724, acc.: 57.81%] [G loss: 0.776225]\n",
      "epoch:2 step:2570 [D loss: 0.674172, acc.: 53.91%] [G loss: 0.785970]\n",
      "epoch:2 step:2571 [D loss: 0.675554, acc.: 59.38%] [G loss: 0.766238]\n",
      "epoch:2 step:2572 [D loss: 0.697145, acc.: 52.34%] [G loss: 0.799432]\n",
      "epoch:2 step:2573 [D loss: 0.725860, acc.: 45.31%] [G loss: 0.845849]\n",
      "epoch:2 step:2574 [D loss: 0.682524, acc.: 50.78%] [G loss: 0.884296]\n",
      "epoch:2 step:2575 [D loss: 0.664550, acc.: 57.03%] [G loss: 0.910992]\n",
      "epoch:2 step:2576 [D loss: 0.692101, acc.: 54.69%] [G loss: 0.872225]\n",
      "epoch:2 step:2577 [D loss: 0.705686, acc.: 53.12%] [G loss: 0.871389]\n",
      "epoch:2 step:2578 [D loss: 0.653700, acc.: 65.62%] [G loss: 0.854540]\n",
      "epoch:2 step:2579 [D loss: 0.716906, acc.: 49.22%] [G loss: 0.833816]\n",
      "epoch:2 step:2580 [D loss: 0.734778, acc.: 47.66%] [G loss: 0.792330]\n",
      "epoch:2 step:2581 [D loss: 0.675482, acc.: 53.12%] [G loss: 0.781224]\n",
      "epoch:2 step:2582 [D loss: 0.668697, acc.: 60.16%] [G loss: 0.859429]\n",
      "epoch:2 step:2583 [D loss: 0.687002, acc.: 50.00%] [G loss: 0.864763]\n",
      "epoch:2 step:2584 [D loss: 0.692273, acc.: 53.91%] [G loss: 0.838566]\n",
      "epoch:2 step:2585 [D loss: 0.682119, acc.: 63.28%] [G loss: 0.809974]\n",
      "epoch:2 step:2586 [D loss: 0.679484, acc.: 61.72%] [G loss: 0.829826]\n",
      "epoch:2 step:2587 [D loss: 0.680330, acc.: 58.59%] [G loss: 0.810830]\n",
      "epoch:2 step:2588 [D loss: 0.670077, acc.: 53.91%] [G loss: 0.855604]\n",
      "epoch:2 step:2589 [D loss: 0.696621, acc.: 50.78%] [G loss: 0.766453]\n",
      "epoch:2 step:2590 [D loss: 0.692708, acc.: 55.47%] [G loss: 0.811602]\n",
      "epoch:2 step:2591 [D loss: 0.686384, acc.: 57.03%] [G loss: 0.816668]\n",
      "epoch:2 step:2592 [D loss: 0.690702, acc.: 50.00%] [G loss: 0.818045]\n",
      "epoch:2 step:2593 [D loss: 0.711891, acc.: 45.31%] [G loss: 0.850717]\n",
      "epoch:2 step:2594 [D loss: 0.698566, acc.: 55.47%] [G loss: 0.927444]\n",
      "epoch:2 step:2595 [D loss: 0.681227, acc.: 57.03%] [G loss: 0.865331]\n",
      "epoch:2 step:2596 [D loss: 0.678406, acc.: 58.59%] [G loss: 0.810163]\n",
      "epoch:2 step:2597 [D loss: 0.639629, acc.: 57.81%] [G loss: 0.857691]\n",
      "epoch:2 step:2598 [D loss: 0.683987, acc.: 51.56%] [G loss: 0.869475]\n",
      "epoch:2 step:2599 [D loss: 0.665095, acc.: 59.38%] [G loss: 0.842072]\n",
      "epoch:2 step:2600 [D loss: 0.689041, acc.: 50.78%] [G loss: 0.927073]\n",
      "epoch:2 step:2601 [D loss: 0.681913, acc.: 57.81%] [G loss: 0.758586]\n",
      "epoch:2 step:2602 [D loss: 0.725545, acc.: 45.31%] [G loss: 0.831664]\n",
      "epoch:2 step:2603 [D loss: 0.695282, acc.: 55.47%] [G loss: 0.877221]\n",
      "epoch:2 step:2604 [D loss: 0.667300, acc.: 55.47%] [G loss: 0.822778]\n",
      "epoch:2 step:2605 [D loss: 0.700483, acc.: 56.25%] [G loss: 0.821476]\n",
      "epoch:2 step:2606 [D loss: 0.660848, acc.: 56.25%] [G loss: 0.863181]\n",
      "epoch:2 step:2607 [D loss: 0.663239, acc.: 61.72%] [G loss: 0.936410]\n",
      "epoch:2 step:2608 [D loss: 0.645409, acc.: 63.28%] [G loss: 0.855971]\n",
      "epoch:2 step:2609 [D loss: 0.718908, acc.: 53.91%] [G loss: 0.831795]\n",
      "epoch:2 step:2610 [D loss: 0.673075, acc.: 57.03%] [G loss: 0.821588]\n",
      "epoch:2 step:2611 [D loss: 0.687908, acc.: 58.59%] [G loss: 0.836760]\n",
      "epoch:2 step:2612 [D loss: 0.710469, acc.: 47.66%] [G loss: 0.846084]\n",
      "epoch:2 step:2613 [D loss: 0.660448, acc.: 59.38%] [G loss: 0.838747]\n",
      "epoch:2 step:2614 [D loss: 0.692268, acc.: 56.25%] [G loss: 0.806368]\n",
      "epoch:2 step:2615 [D loss: 0.668067, acc.: 52.34%] [G loss: 0.818431]\n",
      "epoch:2 step:2616 [D loss: 0.661787, acc.: 53.91%] [G loss: 0.846412]\n",
      "epoch:2 step:2617 [D loss: 0.699753, acc.: 41.41%] [G loss: 0.885083]\n",
      "epoch:2 step:2618 [D loss: 0.689567, acc.: 53.12%] [G loss: 0.861376]\n",
      "epoch:2 step:2619 [D loss: 0.679177, acc.: 53.12%] [G loss: 0.872888]\n",
      "epoch:2 step:2620 [D loss: 0.691729, acc.: 52.34%] [G loss: 0.851664]\n",
      "epoch:2 step:2621 [D loss: 0.675179, acc.: 53.91%] [G loss: 0.784146]\n",
      "epoch:2 step:2622 [D loss: 0.664466, acc.: 63.28%] [G loss: 0.866387]\n",
      "epoch:2 step:2623 [D loss: 0.680938, acc.: 53.12%] [G loss: 0.803581]\n",
      "epoch:2 step:2624 [D loss: 0.675615, acc.: 51.56%] [G loss: 0.788380]\n",
      "epoch:2 step:2625 [D loss: 0.685844, acc.: 54.69%] [G loss: 0.778163]\n",
      "epoch:2 step:2626 [D loss: 0.670829, acc.: 57.81%] [G loss: 0.868740]\n",
      "epoch:2 step:2627 [D loss: 0.670842, acc.: 59.38%] [G loss: 0.832945]\n",
      "epoch:2 step:2628 [D loss: 0.684987, acc.: 58.59%] [G loss: 0.797536]\n",
      "epoch:2 step:2629 [D loss: 0.701433, acc.: 50.78%] [G loss: 0.799283]\n",
      "epoch:2 step:2630 [D loss: 0.729250, acc.: 50.00%] [G loss: 0.758380]\n",
      "epoch:2 step:2631 [D loss: 0.703212, acc.: 50.00%] [G loss: 0.810788]\n",
      "epoch:2 step:2632 [D loss: 0.727713, acc.: 48.44%] [G loss: 0.822210]\n",
      "epoch:2 step:2633 [D loss: 0.678791, acc.: 53.12%] [G loss: 0.798767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2634 [D loss: 0.666020, acc.: 58.59%] [G loss: 0.882419]\n",
      "epoch:2 step:2635 [D loss: 0.677379, acc.: 57.03%] [G loss: 0.898896]\n",
      "epoch:2 step:2636 [D loss: 0.710716, acc.: 45.31%] [G loss: 1.068630]\n",
      "epoch:2 step:2637 [D loss: 0.690533, acc.: 57.03%] [G loss: 0.832101]\n",
      "epoch:2 step:2638 [D loss: 0.671273, acc.: 53.12%] [G loss: 0.792067]\n",
      "epoch:2 step:2639 [D loss: 0.670321, acc.: 57.81%] [G loss: 0.845201]\n",
      "epoch:2 step:2640 [D loss: 0.656875, acc.: 60.16%] [G loss: 0.860052]\n",
      "epoch:2 step:2641 [D loss: 0.708137, acc.: 46.09%] [G loss: 0.813427]\n",
      "epoch:2 step:2642 [D loss: 0.674028, acc.: 61.72%] [G loss: 0.861348]\n",
      "epoch:2 step:2643 [D loss: 0.698049, acc.: 56.25%] [G loss: 0.832082]\n",
      "epoch:2 step:2644 [D loss: 0.679470, acc.: 57.81%] [G loss: 0.810455]\n",
      "epoch:2 step:2645 [D loss: 0.666209, acc.: 60.16%] [G loss: 0.840188]\n",
      "epoch:2 step:2646 [D loss: 0.713125, acc.: 42.97%] [G loss: 0.808950]\n",
      "epoch:2 step:2647 [D loss: 0.679881, acc.: 58.59%] [G loss: 0.822411]\n",
      "epoch:2 step:2648 [D loss: 0.647073, acc.: 60.94%] [G loss: 0.822081]\n",
      "epoch:2 step:2649 [D loss: 0.680667, acc.: 50.00%] [G loss: 0.871470]\n",
      "epoch:2 step:2650 [D loss: 0.657563, acc.: 57.81%] [G loss: 0.854911]\n",
      "epoch:2 step:2651 [D loss: 0.715393, acc.: 48.44%] [G loss: 0.815193]\n",
      "epoch:2 step:2652 [D loss: 0.668350, acc.: 63.28%] [G loss: 0.794279]\n",
      "epoch:2 step:2653 [D loss: 0.696240, acc.: 51.56%] [G loss: 0.823459]\n",
      "epoch:2 step:2654 [D loss: 0.667991, acc.: 58.59%] [G loss: 0.821591]\n",
      "epoch:2 step:2655 [D loss: 0.723598, acc.: 46.09%] [G loss: 0.806304]\n",
      "epoch:2 step:2656 [D loss: 0.689003, acc.: 53.12%] [G loss: 0.807085]\n",
      "epoch:2 step:2657 [D loss: 0.697126, acc.: 50.78%] [G loss: 0.752053]\n",
      "epoch:2 step:2658 [D loss: 0.700797, acc.: 50.78%] [G loss: 0.826148]\n",
      "epoch:2 step:2659 [D loss: 0.689342, acc.: 58.59%] [G loss: 0.831749]\n",
      "epoch:2 step:2660 [D loss: 0.701460, acc.: 50.00%] [G loss: 0.835554]\n",
      "epoch:2 step:2661 [D loss: 0.662985, acc.: 60.16%] [G loss: 0.814020]\n",
      "epoch:2 step:2662 [D loss: 0.669727, acc.: 54.69%] [G loss: 0.880654]\n",
      "epoch:2 step:2663 [D loss: 0.652136, acc.: 58.59%] [G loss: 0.913281]\n",
      "epoch:2 step:2664 [D loss: 0.656840, acc.: 56.25%] [G loss: 0.814987]\n",
      "epoch:2 step:2665 [D loss: 0.690631, acc.: 59.38%] [G loss: 0.850641]\n",
      "epoch:2 step:2666 [D loss: 0.680228, acc.: 49.22%] [G loss: 0.802480]\n",
      "epoch:2 step:2667 [D loss: 0.683750, acc.: 53.12%] [G loss: 0.803645]\n",
      "epoch:2 step:2668 [D loss: 0.742795, acc.: 44.53%] [G loss: 0.814253]\n",
      "epoch:2 step:2669 [D loss: 0.712757, acc.: 50.78%] [G loss: 0.853561]\n",
      "epoch:2 step:2670 [D loss: 0.685622, acc.: 57.03%] [G loss: 0.833032]\n",
      "epoch:2 step:2671 [D loss: 0.702337, acc.: 46.88%] [G loss: 0.837113]\n",
      "epoch:2 step:2672 [D loss: 0.686407, acc.: 53.91%] [G loss: 0.775212]\n",
      "epoch:2 step:2673 [D loss: 0.620978, acc.: 66.41%] [G loss: 0.783419]\n",
      "epoch:2 step:2674 [D loss: 0.630672, acc.: 61.72%] [G loss: 0.955736]\n",
      "epoch:2 step:2675 [D loss: 0.625936, acc.: 64.84%] [G loss: 0.857763]\n",
      "epoch:2 step:2676 [D loss: 0.681640, acc.: 51.56%] [G loss: 0.890818]\n",
      "epoch:2 step:2677 [D loss: 0.660251, acc.: 59.38%] [G loss: 0.833032]\n",
      "epoch:2 step:2678 [D loss: 0.693666, acc.: 50.00%] [G loss: 0.837060]\n",
      "epoch:2 step:2679 [D loss: 0.698611, acc.: 51.56%] [G loss: 0.798702]\n",
      "epoch:2 step:2680 [D loss: 0.687115, acc.: 57.81%] [G loss: 0.868286]\n",
      "epoch:2 step:2681 [D loss: 0.702324, acc.: 53.91%] [G loss: 0.831722]\n",
      "epoch:2 step:2682 [D loss: 0.641919, acc.: 62.50%] [G loss: 0.777952]\n",
      "epoch:2 step:2683 [D loss: 0.661091, acc.: 60.94%] [G loss: 0.905072]\n",
      "epoch:2 step:2684 [D loss: 0.700985, acc.: 52.34%] [G loss: 0.863517]\n",
      "epoch:2 step:2685 [D loss: 0.673521, acc.: 57.81%] [G loss: 0.862907]\n",
      "epoch:2 step:2686 [D loss: 0.693865, acc.: 53.91%] [G loss: 0.847346]\n",
      "epoch:2 step:2687 [D loss: 0.707843, acc.: 57.03%] [G loss: 0.949037]\n",
      "epoch:2 step:2688 [D loss: 0.663505, acc.: 63.28%] [G loss: 0.820337]\n",
      "epoch:2 step:2689 [D loss: 0.698329, acc.: 51.56%] [G loss: 0.806587]\n",
      "epoch:2 step:2690 [D loss: 0.698120, acc.: 57.81%] [G loss: 0.839062]\n",
      "epoch:2 step:2691 [D loss: 0.738698, acc.: 43.75%] [G loss: 0.827915]\n",
      "epoch:2 step:2692 [D loss: 0.722797, acc.: 48.44%] [G loss: 0.822518]\n",
      "epoch:2 step:2693 [D loss: 0.657569, acc.: 61.72%] [G loss: 0.805794]\n",
      "epoch:2 step:2694 [D loss: 0.663560, acc.: 58.59%] [G loss: 0.794249]\n",
      "epoch:2 step:2695 [D loss: 0.687985, acc.: 59.38%] [G loss: 0.839347]\n",
      "epoch:2 step:2696 [D loss: 0.647314, acc.: 60.16%] [G loss: 0.864100]\n",
      "epoch:2 step:2697 [D loss: 0.658441, acc.: 57.03%] [G loss: 0.855641]\n",
      "epoch:2 step:2698 [D loss: 0.668051, acc.: 53.91%] [G loss: 0.854176]\n",
      "epoch:2 step:2699 [D loss: 0.693479, acc.: 56.25%] [G loss: 0.767609]\n",
      "epoch:2 step:2700 [D loss: 0.682784, acc.: 51.56%] [G loss: 0.756474]\n",
      "epoch:2 step:2701 [D loss: 0.660425, acc.: 61.72%] [G loss: 0.838524]\n",
      "epoch:2 step:2702 [D loss: 0.712389, acc.: 52.34%] [G loss: 0.772472]\n",
      "epoch:2 step:2703 [D loss: 0.686249, acc.: 53.91%] [G loss: 0.795126]\n",
      "epoch:2 step:2704 [D loss: 0.646859, acc.: 55.47%] [G loss: 0.887982]\n",
      "epoch:2 step:2705 [D loss: 0.668374, acc.: 53.12%] [G loss: 0.828735]\n",
      "epoch:2 step:2706 [D loss: 0.782858, acc.: 42.19%] [G loss: 0.790076]\n",
      "epoch:2 step:2707 [D loss: 0.714519, acc.: 52.34%] [G loss: 0.866399]\n",
      "epoch:2 step:2708 [D loss: 0.700007, acc.: 51.56%] [G loss: 0.812522]\n",
      "epoch:2 step:2709 [D loss: 0.689377, acc.: 53.91%] [G loss: 0.843298]\n",
      "epoch:2 step:2710 [D loss: 0.700011, acc.: 53.12%] [G loss: 0.820560]\n",
      "epoch:2 step:2711 [D loss: 0.703125, acc.: 46.88%] [G loss: 0.787085]\n",
      "epoch:2 step:2712 [D loss: 0.676762, acc.: 61.72%] [G loss: 0.772666]\n",
      "epoch:2 step:2713 [D loss: 0.689924, acc.: 56.25%] [G loss: 0.803131]\n",
      "epoch:2 step:2714 [D loss: 0.693796, acc.: 53.12%] [G loss: 0.794086]\n",
      "epoch:2 step:2715 [D loss: 0.637764, acc.: 66.41%] [G loss: 0.813006]\n",
      "epoch:2 step:2716 [D loss: 0.670757, acc.: 61.72%] [G loss: 0.854883]\n",
      "epoch:2 step:2717 [D loss: 0.733723, acc.: 40.62%] [G loss: 0.809503]\n",
      "epoch:2 step:2718 [D loss: 0.723594, acc.: 45.31%] [G loss: 0.799347]\n",
      "epoch:2 step:2719 [D loss: 0.701972, acc.: 43.75%] [G loss: 0.857133]\n",
      "epoch:2 step:2720 [D loss: 0.680332, acc.: 61.72%] [G loss: 0.834468]\n",
      "epoch:2 step:2721 [D loss: 0.682674, acc.: 56.25%] [G loss: 0.846495]\n",
      "epoch:2 step:2722 [D loss: 0.667469, acc.: 63.28%] [G loss: 0.854845]\n",
      "epoch:2 step:2723 [D loss: 0.674907, acc.: 53.91%] [G loss: 0.820729]\n",
      "epoch:2 step:2724 [D loss: 0.669052, acc.: 52.34%] [G loss: 0.813924]\n",
      "epoch:2 step:2725 [D loss: 0.692056, acc.: 60.16%] [G loss: 0.807855]\n",
      "epoch:2 step:2726 [D loss: 0.660436, acc.: 54.69%] [G loss: 0.798541]\n",
      "epoch:2 step:2727 [D loss: 0.690004, acc.: 51.56%] [G loss: 0.827857]\n",
      "epoch:2 step:2728 [D loss: 0.709783, acc.: 47.66%] [G loss: 0.849312]\n",
      "epoch:2 step:2729 [D loss: 0.710544, acc.: 46.88%] [G loss: 0.774674]\n",
      "epoch:2 step:2730 [D loss: 0.623233, acc.: 67.97%] [G loss: 0.839726]\n",
      "epoch:2 step:2731 [D loss: 0.670422, acc.: 59.38%] [G loss: 0.763954]\n",
      "epoch:2 step:2732 [D loss: 0.703215, acc.: 46.88%] [G loss: 0.807881]\n",
      "epoch:2 step:2733 [D loss: 0.707519, acc.: 51.56%] [G loss: 0.825231]\n",
      "epoch:2 step:2734 [D loss: 0.650646, acc.: 61.72%] [G loss: 0.770178]\n",
      "epoch:2 step:2735 [D loss: 0.655031, acc.: 62.50%] [G loss: 0.851284]\n",
      "epoch:2 step:2736 [D loss: 0.720725, acc.: 42.19%] [G loss: 0.899047]\n",
      "epoch:2 step:2737 [D loss: 0.702166, acc.: 56.25%] [G loss: 0.813040]\n",
      "epoch:2 step:2738 [D loss: 0.685450, acc.: 53.91%] [G loss: 0.827899]\n",
      "epoch:2 step:2739 [D loss: 0.688760, acc.: 51.56%] [G loss: 0.888885]\n",
      "epoch:2 step:2740 [D loss: 0.689962, acc.: 58.59%] [G loss: 0.815297]\n",
      "epoch:2 step:2741 [D loss: 0.685953, acc.: 51.56%] [G loss: 0.857569]\n",
      "epoch:2 step:2742 [D loss: 0.689081, acc.: 50.00%] [G loss: 0.835124]\n",
      "epoch:2 step:2743 [D loss: 0.712905, acc.: 50.00%] [G loss: 0.805829]\n",
      "epoch:2 step:2744 [D loss: 0.692043, acc.: 51.56%] [G loss: 0.836401]\n",
      "epoch:2 step:2745 [D loss: 0.688070, acc.: 53.91%] [G loss: 0.791512]\n",
      "epoch:2 step:2746 [D loss: 0.715335, acc.: 42.97%] [G loss: 0.841253]\n",
      "epoch:2 step:2747 [D loss: 0.689345, acc.: 52.34%] [G loss: 0.819107]\n",
      "epoch:2 step:2748 [D loss: 0.689369, acc.: 55.47%] [G loss: 0.848591]\n",
      "epoch:2 step:2749 [D loss: 0.683705, acc.: 57.81%] [G loss: 0.779824]\n",
      "epoch:2 step:2750 [D loss: 0.720284, acc.: 48.44%] [G loss: 0.761931]\n",
      "epoch:2 step:2751 [D loss: 0.675588, acc.: 56.25%] [G loss: 0.849544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2752 [D loss: 0.682349, acc.: 54.69%] [G loss: 0.798564]\n",
      "epoch:2 step:2753 [D loss: 0.673577, acc.: 57.81%] [G loss: 0.833759]\n",
      "epoch:2 step:2754 [D loss: 0.690454, acc.: 55.47%] [G loss: 0.801544]\n",
      "epoch:2 step:2755 [D loss: 0.645426, acc.: 65.62%] [G loss: 0.777708]\n",
      "epoch:2 step:2756 [D loss: 0.663109, acc.: 60.16%] [G loss: 0.894849]\n",
      "epoch:2 step:2757 [D loss: 0.682482, acc.: 52.34%] [G loss: 0.875064]\n",
      "epoch:2 step:2758 [D loss: 0.640820, acc.: 65.62%] [G loss: 0.859529]\n",
      "epoch:2 step:2759 [D loss: 0.716040, acc.: 50.78%] [G loss: 0.832691]\n",
      "epoch:2 step:2760 [D loss: 0.640749, acc.: 63.28%] [G loss: 0.897626]\n",
      "epoch:2 step:2761 [D loss: 0.683981, acc.: 57.81%] [G loss: 0.891238]\n",
      "epoch:2 step:2762 [D loss: 0.686790, acc.: 54.69%] [G loss: 0.823421]\n",
      "epoch:2 step:2763 [D loss: 0.734416, acc.: 41.41%] [G loss: 0.782737]\n",
      "epoch:2 step:2764 [D loss: 0.681708, acc.: 52.34%] [G loss: 0.838874]\n",
      "epoch:2 step:2765 [D loss: 0.702225, acc.: 53.91%] [G loss: 0.782859]\n",
      "epoch:2 step:2766 [D loss: 0.703738, acc.: 46.88%] [G loss: 0.853444]\n",
      "epoch:2 step:2767 [D loss: 0.690093, acc.: 47.66%] [G loss: 0.788162]\n",
      "epoch:2 step:2768 [D loss: 0.723686, acc.: 52.34%] [G loss: 0.849247]\n",
      "epoch:2 step:2769 [D loss: 0.683112, acc.: 56.25%] [G loss: 0.773852]\n",
      "epoch:2 step:2770 [D loss: 0.668803, acc.: 61.72%] [G loss: 1.004516]\n",
      "epoch:2 step:2771 [D loss: 0.669850, acc.: 52.34%] [G loss: 0.797014]\n",
      "epoch:2 step:2772 [D loss: 0.711844, acc.: 47.66%] [G loss: 0.849149]\n",
      "epoch:2 step:2773 [D loss: 0.667709, acc.: 53.12%] [G loss: 0.859571]\n",
      "epoch:2 step:2774 [D loss: 0.673805, acc.: 57.03%] [G loss: 0.904839]\n",
      "epoch:2 step:2775 [D loss: 0.696997, acc.: 54.69%] [G loss: 0.888927]\n",
      "epoch:2 step:2776 [D loss: 0.685512, acc.: 54.69%] [G loss: 0.850989]\n",
      "epoch:2 step:2777 [D loss: 0.662884, acc.: 57.81%] [G loss: 0.845467]\n",
      "epoch:2 step:2778 [D loss: 0.679227, acc.: 54.69%] [G loss: 0.862395]\n",
      "epoch:2 step:2779 [D loss: 0.701082, acc.: 50.78%] [G loss: 0.805404]\n",
      "epoch:2 step:2780 [D loss: 0.719354, acc.: 51.56%] [G loss: 0.772013]\n",
      "epoch:2 step:2781 [D loss: 0.684882, acc.: 56.25%] [G loss: 0.771471]\n",
      "epoch:2 step:2782 [D loss: 0.679201, acc.: 59.38%] [G loss: 0.790834]\n",
      "epoch:2 step:2783 [D loss: 0.672151, acc.: 62.50%] [G loss: 0.772567]\n",
      "epoch:2 step:2784 [D loss: 0.682467, acc.: 55.47%] [G loss: 0.772178]\n",
      "epoch:2 step:2785 [D loss: 0.685095, acc.: 57.03%] [G loss: 0.817093]\n",
      "epoch:2 step:2786 [D loss: 0.707485, acc.: 45.31%] [G loss: 0.921161]\n",
      "epoch:2 step:2787 [D loss: 0.672815, acc.: 53.91%] [G loss: 0.826524]\n",
      "epoch:2 step:2788 [D loss: 0.703971, acc.: 56.25%] [G loss: 0.881159]\n",
      "epoch:2 step:2789 [D loss: 0.666627, acc.: 57.81%] [G loss: 0.875808]\n",
      "epoch:2 step:2790 [D loss: 0.665948, acc.: 60.94%] [G loss: 0.816642]\n",
      "epoch:2 step:2791 [D loss: 0.664299, acc.: 57.03%] [G loss: 0.845764]\n",
      "epoch:2 step:2792 [D loss: 0.658578, acc.: 60.94%] [G loss: 0.860483]\n",
      "epoch:2 step:2793 [D loss: 0.733917, acc.: 39.06%] [G loss: 0.789688]\n",
      "epoch:2 step:2794 [D loss: 0.684186, acc.: 55.47%] [G loss: 0.810891]\n",
      "epoch:2 step:2795 [D loss: 0.743218, acc.: 44.53%] [G loss: 0.716386]\n",
      "epoch:2 step:2796 [D loss: 0.695560, acc.: 53.91%] [G loss: 0.798512]\n",
      "epoch:2 step:2797 [D loss: 0.710265, acc.: 50.78%] [G loss: 0.830851]\n",
      "epoch:2 step:2798 [D loss: 0.680496, acc.: 56.25%] [G loss: 0.852420]\n",
      "epoch:2 step:2799 [D loss: 0.679578, acc.: 58.59%] [G loss: 0.841071]\n",
      "epoch:2 step:2800 [D loss: 0.672706, acc.: 54.69%] [G loss: 0.847627]\n",
      "epoch:2 step:2801 [D loss: 0.696454, acc.: 57.03%] [G loss: 0.801668]\n",
      "epoch:2 step:2802 [D loss: 0.648772, acc.: 62.50%] [G loss: 0.936088]\n",
      "epoch:2 step:2803 [D loss: 0.687158, acc.: 57.81%] [G loss: 0.853084]\n",
      "epoch:2 step:2804 [D loss: 0.662202, acc.: 57.81%] [G loss: 0.831179]\n",
      "epoch:2 step:2805 [D loss: 0.722202, acc.: 52.34%] [G loss: 0.813935]\n",
      "epoch:2 step:2806 [D loss: 0.720728, acc.: 46.88%] [G loss: 0.795973]\n",
      "epoch:2 step:2807 [D loss: 0.724616, acc.: 46.09%] [G loss: 0.785029]\n",
      "epoch:2 step:2808 [D loss: 0.659490, acc.: 58.59%] [G loss: 0.845707]\n",
      "epoch:2 step:2809 [D loss: 0.678737, acc.: 57.81%] [G loss: 0.838405]\n",
      "epoch:2 step:2810 [D loss: 0.623066, acc.: 65.62%] [G loss: 0.945588]\n",
      "epoch:2 step:2811 [D loss: 0.690204, acc.: 57.81%] [G loss: 0.883508]\n",
      "epoch:3 step:2812 [D loss: 0.676435, acc.: 57.03%] [G loss: 0.840953]\n",
      "epoch:3 step:2813 [D loss: 0.690634, acc.: 55.47%] [G loss: 0.899232]\n",
      "epoch:3 step:2814 [D loss: 0.683824, acc.: 55.47%] [G loss: 0.862428]\n",
      "epoch:3 step:2815 [D loss: 0.690142, acc.: 56.25%] [G loss: 0.888206]\n",
      "epoch:3 step:2816 [D loss: 0.668361, acc.: 57.81%] [G loss: 0.831871]\n",
      "epoch:3 step:2817 [D loss: 0.658770, acc.: 57.81%] [G loss: 0.873783]\n",
      "epoch:3 step:2818 [D loss: 0.728956, acc.: 48.44%] [G loss: 0.789937]\n",
      "epoch:3 step:2819 [D loss: 0.698215, acc.: 49.22%] [G loss: 0.798191]\n",
      "epoch:3 step:2820 [D loss: 0.682197, acc.: 55.47%] [G loss: 0.850484]\n",
      "epoch:3 step:2821 [D loss: 0.694780, acc.: 54.69%] [G loss: 0.794527]\n",
      "epoch:3 step:2822 [D loss: 0.675847, acc.: 55.47%] [G loss: 0.817146]\n",
      "epoch:3 step:2823 [D loss: 0.699666, acc.: 57.03%] [G loss: 0.859164]\n",
      "epoch:3 step:2824 [D loss: 0.669561, acc.: 54.69%] [G loss: 0.787408]\n",
      "epoch:3 step:2825 [D loss: 0.671063, acc.: 59.38%] [G loss: 0.783480]\n",
      "epoch:3 step:2826 [D loss: 0.685767, acc.: 60.94%] [G loss: 0.866393]\n",
      "epoch:3 step:2827 [D loss: 0.688069, acc.: 50.78%] [G loss: 0.837345]\n",
      "epoch:3 step:2828 [D loss: 0.659635, acc.: 62.50%] [G loss: 0.838024]\n",
      "epoch:3 step:2829 [D loss: 0.677831, acc.: 60.16%] [G loss: 0.828197]\n",
      "epoch:3 step:2830 [D loss: 0.696268, acc.: 53.91%] [G loss: 0.742182]\n",
      "epoch:3 step:2831 [D loss: 0.672144, acc.: 60.16%] [G loss: 0.806814]\n",
      "epoch:3 step:2832 [D loss: 0.669106, acc.: 60.16%] [G loss: 0.918185]\n",
      "epoch:3 step:2833 [D loss: 0.726377, acc.: 48.44%] [G loss: 0.803148]\n",
      "epoch:3 step:2834 [D loss: 0.675678, acc.: 56.25%] [G loss: 0.825692]\n",
      "epoch:3 step:2835 [D loss: 0.680587, acc.: 57.03%] [G loss: 0.831625]\n",
      "epoch:3 step:2836 [D loss: 0.697874, acc.: 51.56%] [G loss: 0.849115]\n",
      "epoch:3 step:2837 [D loss: 0.688990, acc.: 56.25%] [G loss: 0.859275]\n",
      "epoch:3 step:2838 [D loss: 0.703452, acc.: 50.78%] [G loss: 0.762927]\n",
      "epoch:3 step:2839 [D loss: 0.672653, acc.: 57.81%] [G loss: 0.852652]\n",
      "epoch:3 step:2840 [D loss: 0.690568, acc.: 52.34%] [G loss: 0.862574]\n",
      "epoch:3 step:2841 [D loss: 0.658585, acc.: 60.94%] [G loss: 0.840509]\n",
      "epoch:3 step:2842 [D loss: 0.691265, acc.: 54.69%] [G loss: 0.832764]\n",
      "epoch:3 step:2843 [D loss: 0.649424, acc.: 64.06%] [G loss: 0.770816]\n",
      "epoch:3 step:2844 [D loss: 0.674123, acc.: 59.38%] [G loss: 0.800204]\n",
      "epoch:3 step:2845 [D loss: 0.701963, acc.: 48.44%] [G loss: 0.818528]\n",
      "epoch:3 step:2846 [D loss: 0.684725, acc.: 53.12%] [G loss: 0.839957]\n",
      "epoch:3 step:2847 [D loss: 0.709792, acc.: 48.44%] [G loss: 0.824644]\n",
      "epoch:3 step:2848 [D loss: 0.711749, acc.: 52.34%] [G loss: 0.784568]\n",
      "epoch:3 step:2849 [D loss: 0.700209, acc.: 53.91%] [G loss: 0.806938]\n",
      "epoch:3 step:2850 [D loss: 0.692958, acc.: 50.00%] [G loss: 0.821469]\n",
      "epoch:3 step:2851 [D loss: 0.678804, acc.: 51.56%] [G loss: 0.784002]\n",
      "epoch:3 step:2852 [D loss: 0.687727, acc.: 55.47%] [G loss: 0.820994]\n",
      "epoch:3 step:2853 [D loss: 0.698661, acc.: 52.34%] [G loss: 0.811763]\n",
      "epoch:3 step:2854 [D loss: 0.669738, acc.: 57.03%] [G loss: 0.851035]\n",
      "epoch:3 step:2855 [D loss: 0.692797, acc.: 57.03%] [G loss: 0.865831]\n",
      "epoch:3 step:2856 [D loss: 0.738656, acc.: 44.53%] [G loss: 0.877213]\n",
      "epoch:3 step:2857 [D loss: 0.653583, acc.: 63.28%] [G loss: 0.789076]\n",
      "epoch:3 step:2858 [D loss: 0.709761, acc.: 48.44%] [G loss: 0.740042]\n",
      "epoch:3 step:2859 [D loss: 0.670482, acc.: 53.91%] [G loss: 0.865561]\n",
      "epoch:3 step:2860 [D loss: 0.675006, acc.: 58.59%] [G loss: 0.790319]\n",
      "epoch:3 step:2861 [D loss: 0.666569, acc.: 60.16%] [G loss: 0.814298]\n",
      "epoch:3 step:2862 [D loss: 0.680041, acc.: 52.34%] [G loss: 0.848186]\n",
      "epoch:3 step:2863 [D loss: 0.664103, acc.: 60.16%] [G loss: 0.849647]\n",
      "epoch:3 step:2864 [D loss: 0.626754, acc.: 63.28%] [G loss: 0.862762]\n",
      "epoch:3 step:2865 [D loss: 0.709376, acc.: 48.44%] [G loss: 0.837205]\n",
      "epoch:3 step:2866 [D loss: 0.676044, acc.: 57.03%] [G loss: 0.787037]\n",
      "epoch:3 step:2867 [D loss: 0.694134, acc.: 59.38%] [G loss: 0.809119]\n",
      "epoch:3 step:2868 [D loss: 0.688313, acc.: 54.69%] [G loss: 0.755296]\n",
      "epoch:3 step:2869 [D loss: 0.730751, acc.: 45.31%] [G loss: 0.763910]\n",
      "epoch:3 step:2870 [D loss: 0.678326, acc.: 58.59%] [G loss: 0.796745]\n",
      "epoch:3 step:2871 [D loss: 0.667990, acc.: 51.56%] [G loss: 0.831152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2872 [D loss: 0.665646, acc.: 60.16%] [G loss: 0.885001]\n",
      "epoch:3 step:2873 [D loss: 0.697544, acc.: 55.47%] [G loss: 0.886624]\n",
      "epoch:3 step:2874 [D loss: 0.687330, acc.: 54.69%] [G loss: 0.854627]\n",
      "epoch:3 step:2875 [D loss: 0.693654, acc.: 52.34%] [G loss: 0.871720]\n",
      "epoch:3 step:2876 [D loss: 0.716619, acc.: 46.88%] [G loss: 0.882461]\n",
      "epoch:3 step:2877 [D loss: 0.665798, acc.: 53.12%] [G loss: 0.875750]\n",
      "epoch:3 step:2878 [D loss: 0.671410, acc.: 57.81%] [G loss: 0.868070]\n",
      "epoch:3 step:2879 [D loss: 0.730427, acc.: 49.22%] [G loss: 0.779144]\n",
      "epoch:3 step:2880 [D loss: 0.648187, acc.: 60.16%] [G loss: 0.824720]\n",
      "epoch:3 step:2881 [D loss: 0.702399, acc.: 55.47%] [G loss: 0.844736]\n",
      "epoch:3 step:2882 [D loss: 0.693800, acc.: 53.12%] [G loss: 0.853372]\n",
      "epoch:3 step:2883 [D loss: 0.660449, acc.: 63.28%] [G loss: 0.752960]\n",
      "epoch:3 step:2884 [D loss: 0.695457, acc.: 48.44%] [G loss: 0.799442]\n",
      "epoch:3 step:2885 [D loss: 0.681089, acc.: 57.81%] [G loss: 0.794793]\n",
      "epoch:3 step:2886 [D loss: 0.721428, acc.: 46.88%] [G loss: 0.758771]\n",
      "epoch:3 step:2887 [D loss: 0.707033, acc.: 47.66%] [G loss: 0.841520]\n",
      "epoch:3 step:2888 [D loss: 0.670902, acc.: 57.03%] [G loss: 0.812485]\n",
      "epoch:3 step:2889 [D loss: 0.687606, acc.: 55.47%] [G loss: 0.862160]\n",
      "epoch:3 step:2890 [D loss: 0.680165, acc.: 52.34%] [G loss: 0.866554]\n",
      "epoch:3 step:2891 [D loss: 0.712774, acc.: 49.22%] [G loss: 0.809098]\n",
      "epoch:3 step:2892 [D loss: 0.666643, acc.: 58.59%] [G loss: 0.840528]\n",
      "epoch:3 step:2893 [D loss: 0.648861, acc.: 60.94%] [G loss: 0.833263]\n",
      "epoch:3 step:2894 [D loss: 0.665087, acc.: 63.28%] [G loss: 0.823130]\n",
      "epoch:3 step:2895 [D loss: 0.688259, acc.: 52.34%] [G loss: 0.883537]\n",
      "epoch:3 step:2896 [D loss: 0.681330, acc.: 55.47%] [G loss: 0.876175]\n",
      "epoch:3 step:2897 [D loss: 0.670581, acc.: 55.47%] [G loss: 0.883257]\n",
      "epoch:3 step:2898 [D loss: 0.652807, acc.: 59.38%] [G loss: 0.884704]\n",
      "epoch:3 step:2899 [D loss: 0.721422, acc.: 48.44%] [G loss: 0.867946]\n",
      "epoch:3 step:2900 [D loss: 0.711961, acc.: 53.91%] [G loss: 0.849047]\n",
      "epoch:3 step:2901 [D loss: 0.673829, acc.: 57.03%] [G loss: 0.823479]\n",
      "epoch:3 step:2902 [D loss: 0.815881, acc.: 36.72%] [G loss: 0.812744]\n",
      "epoch:3 step:2903 [D loss: 0.702360, acc.: 49.22%] [G loss: 0.710345]\n",
      "epoch:3 step:2904 [D loss: 0.694092, acc.: 54.69%] [G loss: 0.790129]\n",
      "epoch:3 step:2905 [D loss: 0.720335, acc.: 50.78%] [G loss: 0.830784]\n",
      "epoch:3 step:2906 [D loss: 0.715279, acc.: 46.88%] [G loss: 0.769367]\n",
      "epoch:3 step:2907 [D loss: 0.695595, acc.: 54.69%] [G loss: 0.759443]\n",
      "epoch:3 step:2908 [D loss: 0.663086, acc.: 57.81%] [G loss: 0.785114]\n",
      "epoch:3 step:2909 [D loss: 0.698468, acc.: 57.81%] [G loss: 0.813781]\n",
      "epoch:3 step:2910 [D loss: 0.720376, acc.: 46.88%] [G loss: 0.872544]\n",
      "epoch:3 step:2911 [D loss: 0.679856, acc.: 55.47%] [G loss: 0.833308]\n",
      "epoch:3 step:2912 [D loss: 0.662284, acc.: 57.03%] [G loss: 0.825310]\n",
      "epoch:3 step:2913 [D loss: 0.645518, acc.: 60.94%] [G loss: 0.827891]\n",
      "epoch:3 step:2914 [D loss: 0.699114, acc.: 62.50%] [G loss: 0.842577]\n",
      "epoch:3 step:2915 [D loss: 0.678340, acc.: 57.81%] [G loss: 0.765425]\n",
      "epoch:3 step:2916 [D loss: 0.670856, acc.: 57.03%] [G loss: 0.816840]\n",
      "epoch:3 step:2917 [D loss: 0.680433, acc.: 60.16%] [G loss: 0.847765]\n",
      "epoch:3 step:2918 [D loss: 0.678831, acc.: 55.47%] [G loss: 0.849899]\n",
      "epoch:3 step:2919 [D loss: 0.677800, acc.: 60.94%] [G loss: 0.851669]\n",
      "epoch:3 step:2920 [D loss: 0.705190, acc.: 50.00%] [G loss: 0.946734]\n",
      "epoch:3 step:2921 [D loss: 0.656281, acc.: 57.81%] [G loss: 0.863125]\n",
      "epoch:3 step:2922 [D loss: 0.686389, acc.: 50.00%] [G loss: 0.841609]\n",
      "epoch:3 step:2923 [D loss: 0.694165, acc.: 53.12%] [G loss: 0.811744]\n",
      "epoch:3 step:2924 [D loss: 0.684312, acc.: 57.03%] [G loss: 0.847726]\n",
      "epoch:3 step:2925 [D loss: 0.639056, acc.: 67.19%] [G loss: 0.792664]\n",
      "epoch:3 step:2926 [D loss: 0.721267, acc.: 54.69%] [G loss: 0.820016]\n",
      "epoch:3 step:2927 [D loss: 0.722399, acc.: 46.88%] [G loss: 0.820517]\n",
      "epoch:3 step:2928 [D loss: 0.699884, acc.: 54.69%] [G loss: 0.898800]\n",
      "epoch:3 step:2929 [D loss: 0.674960, acc.: 57.03%] [G loss: 0.801999]\n",
      "epoch:3 step:2930 [D loss: 0.689141, acc.: 51.56%] [G loss: 0.803670]\n",
      "epoch:3 step:2931 [D loss: 0.652034, acc.: 56.25%] [G loss: 0.842496]\n",
      "epoch:3 step:2932 [D loss: 0.665017, acc.: 60.16%] [G loss: 0.828238]\n",
      "epoch:3 step:2933 [D loss: 0.694889, acc.: 57.81%] [G loss: 0.845769]\n",
      "epoch:3 step:2934 [D loss: 0.683090, acc.: 56.25%] [G loss: 0.814606]\n",
      "epoch:3 step:2935 [D loss: 0.692530, acc.: 50.78%] [G loss: 0.840854]\n",
      "epoch:3 step:2936 [D loss: 0.657705, acc.: 60.94%] [G loss: 0.762521]\n",
      "epoch:3 step:2937 [D loss: 0.686814, acc.: 48.44%] [G loss: 0.817868]\n",
      "epoch:3 step:2938 [D loss: 0.706482, acc.: 50.78%] [G loss: 0.820796]\n",
      "epoch:3 step:2939 [D loss: 0.711219, acc.: 47.66%] [G loss: 0.767235]\n",
      "epoch:3 step:2940 [D loss: 0.694669, acc.: 55.47%] [G loss: 0.775886]\n",
      "epoch:3 step:2941 [D loss: 0.685582, acc.: 53.12%] [G loss: 0.801258]\n",
      "epoch:3 step:2942 [D loss: 0.687075, acc.: 59.38%] [G loss: 0.838976]\n",
      "epoch:3 step:2943 [D loss: 0.668105, acc.: 57.81%] [G loss: 0.852174]\n",
      "epoch:3 step:2944 [D loss: 0.678972, acc.: 56.25%] [G loss: 0.885190]\n",
      "epoch:3 step:2945 [D loss: 0.646152, acc.: 58.59%] [G loss: 0.838855]\n",
      "epoch:3 step:2946 [D loss: 0.667324, acc.: 58.59%] [G loss: 0.826933]\n",
      "epoch:3 step:2947 [D loss: 0.703087, acc.: 53.91%] [G loss: 0.786208]\n",
      "epoch:3 step:2948 [D loss: 0.687554, acc.: 56.25%] [G loss: 0.764215]\n",
      "epoch:3 step:2949 [D loss: 0.677710, acc.: 56.25%] [G loss: 0.750070]\n",
      "epoch:3 step:2950 [D loss: 0.740274, acc.: 43.75%] [G loss: 0.758435]\n",
      "epoch:3 step:2951 [D loss: 0.640198, acc.: 61.72%] [G loss: 0.786986]\n",
      "epoch:3 step:2952 [D loss: 0.700564, acc.: 50.00%] [G loss: 0.807536]\n",
      "epoch:3 step:2953 [D loss: 0.685077, acc.: 58.59%] [G loss: 0.752855]\n",
      "epoch:3 step:2954 [D loss: 0.687059, acc.: 58.59%] [G loss: 0.800080]\n",
      "epoch:3 step:2955 [D loss: 0.654714, acc.: 65.62%] [G loss: 0.785623]\n",
      "epoch:3 step:2956 [D loss: 0.678585, acc.: 59.38%] [G loss: 0.812306]\n",
      "epoch:3 step:2957 [D loss: 0.681043, acc.: 49.22%] [G loss: 0.806002]\n",
      "epoch:3 step:2958 [D loss: 0.683974, acc.: 53.12%] [G loss: 0.822027]\n",
      "epoch:3 step:2959 [D loss: 0.669445, acc.: 57.03%] [G loss: 0.811855]\n",
      "epoch:3 step:2960 [D loss: 0.639439, acc.: 62.50%] [G loss: 0.839729]\n",
      "epoch:3 step:2961 [D loss: 0.638506, acc.: 61.72%] [G loss: 0.852924]\n",
      "epoch:3 step:2962 [D loss: 0.725765, acc.: 50.78%] [G loss: 0.848057]\n",
      "epoch:3 step:2963 [D loss: 0.682807, acc.: 52.34%] [G loss: 0.808749]\n",
      "epoch:3 step:2964 [D loss: 0.714462, acc.: 50.00%] [G loss: 0.800720]\n",
      "epoch:3 step:2965 [D loss: 0.703263, acc.: 50.78%] [G loss: 0.834964]\n",
      "epoch:3 step:2966 [D loss: 0.699301, acc.: 60.94%] [G loss: 0.868952]\n",
      "epoch:3 step:2967 [D loss: 0.683487, acc.: 60.94%] [G loss: 0.821564]\n",
      "epoch:3 step:2968 [D loss: 0.676856, acc.: 60.16%] [G loss: 0.802521]\n",
      "epoch:3 step:2969 [D loss: 0.687449, acc.: 51.56%] [G loss: 0.793159]\n",
      "epoch:3 step:2970 [D loss: 0.700356, acc.: 46.88%] [G loss: 0.775950]\n",
      "epoch:3 step:2971 [D loss: 0.720260, acc.: 50.78%] [G loss: 0.762443]\n",
      "epoch:3 step:2972 [D loss: 0.691993, acc.: 48.44%] [G loss: 0.934216]\n",
      "epoch:3 step:2973 [D loss: 0.695842, acc.: 48.44%] [G loss: 0.798965]\n",
      "epoch:3 step:2974 [D loss: 0.731388, acc.: 45.31%] [G loss: 0.803690]\n",
      "epoch:3 step:2975 [D loss: 0.672608, acc.: 58.59%] [G loss: 0.829616]\n",
      "epoch:3 step:2976 [D loss: 0.669797, acc.: 59.38%] [G loss: 0.811390]\n",
      "epoch:3 step:2977 [D loss: 0.640927, acc.: 63.28%] [G loss: 0.857556]\n",
      "epoch:3 step:2978 [D loss: 0.664339, acc.: 60.16%] [G loss: 0.799134]\n",
      "epoch:3 step:2979 [D loss: 0.661279, acc.: 57.03%] [G loss: 0.827601]\n",
      "epoch:3 step:2980 [D loss: 0.675143, acc.: 59.38%] [G loss: 0.843965]\n",
      "epoch:3 step:2981 [D loss: 0.657179, acc.: 60.16%] [G loss: 0.824709]\n",
      "epoch:3 step:2982 [D loss: 0.698297, acc.: 51.56%] [G loss: 0.765032]\n",
      "epoch:3 step:2983 [D loss: 0.662614, acc.: 59.38%] [G loss: 0.791940]\n",
      "epoch:3 step:2984 [D loss: 0.712663, acc.: 55.47%] [G loss: 0.810002]\n",
      "epoch:3 step:2985 [D loss: 0.694726, acc.: 53.91%] [G loss: 0.844838]\n",
      "epoch:3 step:2986 [D loss: 0.675972, acc.: 57.81%] [G loss: 0.850320]\n",
      "epoch:3 step:2987 [D loss: 0.677869, acc.: 60.94%] [G loss: 0.814255]\n",
      "epoch:3 step:2988 [D loss: 0.634810, acc.: 63.28%] [G loss: 0.937589]\n",
      "epoch:3 step:2989 [D loss: 0.696051, acc.: 51.56%] [G loss: 0.938391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2990 [D loss: 0.715918, acc.: 46.88%] [G loss: 0.839184]\n",
      "epoch:3 step:2991 [D loss: 0.676034, acc.: 58.59%] [G loss: 0.842544]\n",
      "epoch:3 step:2992 [D loss: 0.713423, acc.: 51.56%] [G loss: 0.899206]\n",
      "epoch:3 step:2993 [D loss: 0.672322, acc.: 57.03%] [G loss: 0.888409]\n",
      "epoch:3 step:2994 [D loss: 0.680325, acc.: 58.59%] [G loss: 0.815783]\n",
      "epoch:3 step:2995 [D loss: 0.697611, acc.: 53.91%] [G loss: 0.812089]\n",
      "epoch:3 step:2996 [D loss: 0.680176, acc.: 51.56%] [G loss: 0.842814]\n",
      "epoch:3 step:2997 [D loss: 0.677594, acc.: 54.69%] [G loss: 0.778719]\n",
      "epoch:3 step:2998 [D loss: 0.675973, acc.: 53.91%] [G loss: 0.827068]\n",
      "epoch:3 step:2999 [D loss: 0.665167, acc.: 54.69%] [G loss: 0.860155]\n",
      "epoch:3 step:3000 [D loss: 0.759372, acc.: 44.53%] [G loss: 0.820228]\n",
      "epoch:3 step:3001 [D loss: 0.659360, acc.: 64.06%] [G loss: 0.840430]\n",
      "epoch:3 step:3002 [D loss: 0.699590, acc.: 57.03%] [G loss: 0.845314]\n",
      "epoch:3 step:3003 [D loss: 0.647102, acc.: 61.72%] [G loss: 0.900962]\n",
      "epoch:3 step:3004 [D loss: 0.664660, acc.: 60.94%] [G loss: 0.783088]\n",
      "epoch:3 step:3005 [D loss: 0.688307, acc.: 54.69%] [G loss: 0.854776]\n",
      "epoch:3 step:3006 [D loss: 0.675996, acc.: 55.47%] [G loss: 0.840872]\n",
      "epoch:3 step:3007 [D loss: 0.691054, acc.: 49.22%] [G loss: 0.832875]\n",
      "epoch:3 step:3008 [D loss: 0.681689, acc.: 58.59%] [G loss: 0.920923]\n",
      "epoch:3 step:3009 [D loss: 0.703124, acc.: 53.12%] [G loss: 0.842752]\n",
      "epoch:3 step:3010 [D loss: 0.685709, acc.: 55.47%] [G loss: 0.832291]\n",
      "epoch:3 step:3011 [D loss: 0.705206, acc.: 50.78%] [G loss: 0.746058]\n",
      "epoch:3 step:3012 [D loss: 0.698287, acc.: 47.66%] [G loss: 0.796264]\n",
      "epoch:3 step:3013 [D loss: 0.721670, acc.: 48.44%] [G loss: 0.802606]\n",
      "epoch:3 step:3014 [D loss: 0.671186, acc.: 61.72%] [G loss: 0.777145]\n",
      "epoch:3 step:3015 [D loss: 0.691968, acc.: 53.91%] [G loss: 0.815652]\n",
      "epoch:3 step:3016 [D loss: 0.739862, acc.: 49.22%] [G loss: 0.808930]\n",
      "epoch:3 step:3017 [D loss: 0.685406, acc.: 52.34%] [G loss: 0.865487]\n",
      "epoch:3 step:3018 [D loss: 0.659473, acc.: 57.03%] [G loss: 0.876308]\n",
      "epoch:3 step:3019 [D loss: 0.643045, acc.: 61.72%] [G loss: 0.875691]\n",
      "epoch:3 step:3020 [D loss: 0.667016, acc.: 63.28%] [G loss: 0.846187]\n",
      "epoch:3 step:3021 [D loss: 0.671424, acc.: 55.47%] [G loss: 0.771098]\n",
      "epoch:3 step:3022 [D loss: 0.707214, acc.: 50.00%] [G loss: 0.844640]\n",
      "epoch:3 step:3023 [D loss: 0.696865, acc.: 54.69%] [G loss: 0.813697]\n",
      "epoch:3 step:3024 [D loss: 0.697419, acc.: 50.00%] [G loss: 0.737334]\n",
      "epoch:3 step:3025 [D loss: 0.715597, acc.: 48.44%] [G loss: 0.857245]\n",
      "epoch:3 step:3026 [D loss: 0.717599, acc.: 47.66%] [G loss: 0.795662]\n",
      "epoch:3 step:3027 [D loss: 0.695086, acc.: 53.12%] [G loss: 0.788976]\n",
      "epoch:3 step:3028 [D loss: 0.718597, acc.: 47.66%] [G loss: 0.767284]\n",
      "epoch:3 step:3029 [D loss: 0.700439, acc.: 50.78%] [G loss: 0.794432]\n",
      "epoch:3 step:3030 [D loss: 0.650141, acc.: 60.94%] [G loss: 0.857015]\n",
      "epoch:3 step:3031 [D loss: 0.669591, acc.: 59.38%] [G loss: 0.864428]\n",
      "epoch:3 step:3032 [D loss: 0.679251, acc.: 56.25%] [G loss: 0.827786]\n",
      "epoch:3 step:3033 [D loss: 0.685240, acc.: 57.03%] [G loss: 0.821245]\n",
      "epoch:3 step:3034 [D loss: 0.731792, acc.: 51.56%] [G loss: 0.850580]\n",
      "epoch:3 step:3035 [D loss: 0.688274, acc.: 53.91%] [G loss: 0.748151]\n",
      "epoch:3 step:3036 [D loss: 0.687080, acc.: 57.03%] [G loss: 0.796566]\n",
      "epoch:3 step:3037 [D loss: 0.708981, acc.: 50.00%] [G loss: 0.805152]\n",
      "epoch:3 step:3038 [D loss: 0.703383, acc.: 51.56%] [G loss: 0.816983]\n",
      "epoch:3 step:3039 [D loss: 0.701016, acc.: 54.69%] [G loss: 0.797573]\n",
      "epoch:3 step:3040 [D loss: 0.709429, acc.: 48.44%] [G loss: 0.787433]\n",
      "epoch:3 step:3041 [D loss: 0.695312, acc.: 49.22%] [G loss: 0.755521]\n",
      "epoch:3 step:3042 [D loss: 0.686178, acc.: 60.16%] [G loss: 0.836029]\n",
      "epoch:3 step:3043 [D loss: 0.670976, acc.: 60.94%] [G loss: 0.803143]\n",
      "epoch:3 step:3044 [D loss: 0.672163, acc.: 54.69%] [G loss: 0.802509]\n",
      "epoch:3 step:3045 [D loss: 0.695456, acc.: 51.56%] [G loss: 0.771024]\n",
      "epoch:3 step:3046 [D loss: 0.690663, acc.: 56.25%] [G loss: 0.787220]\n",
      "epoch:3 step:3047 [D loss: 0.691133, acc.: 53.12%] [G loss: 0.774604]\n",
      "epoch:3 step:3048 [D loss: 0.729810, acc.: 50.78%] [G loss: 0.757755]\n",
      "epoch:3 step:3049 [D loss: 0.696165, acc.: 54.69%] [G loss: 0.800210]\n",
      "epoch:3 step:3050 [D loss: 0.693157, acc.: 46.88%] [G loss: 0.887482]\n",
      "epoch:3 step:3051 [D loss: 0.657760, acc.: 59.38%] [G loss: 0.892019]\n",
      "epoch:3 step:3052 [D loss: 0.677286, acc.: 55.47%] [G loss: 0.849284]\n",
      "epoch:3 step:3053 [D loss: 0.635433, acc.: 58.59%] [G loss: 0.836467]\n",
      "epoch:3 step:3054 [D loss: 0.632442, acc.: 58.59%] [G loss: 0.893780]\n",
      "epoch:3 step:3055 [D loss: 0.711909, acc.: 47.66%] [G loss: 0.851808]\n",
      "epoch:3 step:3056 [D loss: 0.714536, acc.: 50.00%] [G loss: 0.788739]\n",
      "epoch:3 step:3057 [D loss: 0.669670, acc.: 57.81%] [G loss: 0.853338]\n",
      "epoch:3 step:3058 [D loss: 0.714752, acc.: 57.03%] [G loss: 0.778422]\n",
      "epoch:3 step:3059 [D loss: 0.678708, acc.: 53.91%] [G loss: 0.779820]\n",
      "epoch:3 step:3060 [D loss: 0.699774, acc.: 49.22%] [G loss: 0.826698]\n",
      "epoch:3 step:3061 [D loss: 0.666384, acc.: 59.38%] [G loss: 0.877071]\n",
      "epoch:3 step:3062 [D loss: 0.694442, acc.: 51.56%] [G loss: 0.820695]\n",
      "epoch:3 step:3063 [D loss: 0.668893, acc.: 57.03%] [G loss: 0.822143]\n",
      "epoch:3 step:3064 [D loss: 0.686726, acc.: 52.34%] [G loss: 0.854265]\n",
      "epoch:3 step:3065 [D loss: 0.689521, acc.: 55.47%] [G loss: 0.809013]\n",
      "epoch:3 step:3066 [D loss: 0.679911, acc.: 54.69%] [G loss: 0.816204]\n",
      "epoch:3 step:3067 [D loss: 0.696291, acc.: 50.78%] [G loss: 0.817790]\n",
      "epoch:3 step:3068 [D loss: 0.662532, acc.: 57.81%] [G loss: 0.819432]\n",
      "epoch:3 step:3069 [D loss: 0.675313, acc.: 50.78%] [G loss: 0.813478]\n",
      "epoch:3 step:3070 [D loss: 0.683850, acc.: 50.00%] [G loss: 0.768902]\n",
      "epoch:3 step:3071 [D loss: 0.685649, acc.: 53.91%] [G loss: 0.803873]\n",
      "epoch:3 step:3072 [D loss: 0.698486, acc.: 48.44%] [G loss: 0.820486]\n",
      "epoch:3 step:3073 [D loss: 0.686735, acc.: 55.47%] [G loss: 0.838328]\n",
      "epoch:3 step:3074 [D loss: 0.652134, acc.: 69.53%] [G loss: 0.887693]\n",
      "epoch:3 step:3075 [D loss: 0.660488, acc.: 65.62%] [G loss: 0.838557]\n",
      "epoch:3 step:3076 [D loss: 0.671777, acc.: 54.69%] [G loss: 0.912489]\n",
      "epoch:3 step:3077 [D loss: 0.703174, acc.: 59.38%] [G loss: 0.826223]\n",
      "epoch:3 step:3078 [D loss: 0.687658, acc.: 55.47%] [G loss: 0.818786]\n",
      "epoch:3 step:3079 [D loss: 0.688410, acc.: 54.69%] [G loss: 0.811934]\n",
      "epoch:3 step:3080 [D loss: 0.702209, acc.: 55.47%] [G loss: 0.869543]\n",
      "epoch:3 step:3081 [D loss: 0.675320, acc.: 55.47%] [G loss: 0.819231]\n",
      "epoch:3 step:3082 [D loss: 0.659291, acc.: 59.38%] [G loss: 0.898550]\n",
      "epoch:3 step:3083 [D loss: 0.692415, acc.: 55.47%] [G loss: 0.797231]\n",
      "epoch:3 step:3084 [D loss: 0.668813, acc.: 59.38%] [G loss: 0.848972]\n",
      "epoch:3 step:3085 [D loss: 0.678829, acc.: 57.03%] [G loss: 0.786002]\n",
      "epoch:3 step:3086 [D loss: 0.667240, acc.: 59.38%] [G loss: 0.854066]\n",
      "epoch:3 step:3087 [D loss: 0.704790, acc.: 50.78%] [G loss: 0.768628]\n",
      "epoch:3 step:3088 [D loss: 0.702885, acc.: 52.34%] [G loss: 0.749819]\n",
      "epoch:3 step:3089 [D loss: 0.701575, acc.: 55.47%] [G loss: 0.780631]\n",
      "epoch:3 step:3090 [D loss: 0.666044, acc.: 57.81%] [G loss: 0.815617]\n",
      "epoch:3 step:3091 [D loss: 0.670804, acc.: 59.38%] [G loss: 0.816946]\n",
      "epoch:3 step:3092 [D loss: 0.731672, acc.: 38.28%] [G loss: 0.763577]\n",
      "epoch:3 step:3093 [D loss: 0.657312, acc.: 53.91%] [G loss: 0.782547]\n",
      "epoch:3 step:3094 [D loss: 0.626777, acc.: 65.62%] [G loss: 0.792177]\n",
      "epoch:3 step:3095 [D loss: 0.696827, acc.: 57.03%] [G loss: 0.825647]\n",
      "epoch:3 step:3096 [D loss: 0.660590, acc.: 52.34%] [G loss: 0.803954]\n",
      "epoch:3 step:3097 [D loss: 0.686603, acc.: 57.03%] [G loss: 0.852214]\n",
      "epoch:3 step:3098 [D loss: 0.664797, acc.: 53.91%] [G loss: 0.838201]\n",
      "epoch:3 step:3099 [D loss: 0.731836, acc.: 39.84%] [G loss: 0.882881]\n",
      "epoch:3 step:3100 [D loss: 0.717985, acc.: 48.44%] [G loss: 0.817915]\n",
      "epoch:3 step:3101 [D loss: 0.662800, acc.: 60.94%] [G loss: 0.829022]\n",
      "epoch:3 step:3102 [D loss: 0.686397, acc.: 57.03%] [G loss: 0.808162]\n",
      "epoch:3 step:3103 [D loss: 0.661715, acc.: 60.16%] [G loss: 0.753767]\n",
      "epoch:3 step:3104 [D loss: 0.669904, acc.: 58.59%] [G loss: 0.771683]\n",
      "epoch:3 step:3105 [D loss: 0.647030, acc.: 59.38%] [G loss: 0.801186]\n",
      "epoch:3 step:3106 [D loss: 0.694426, acc.: 53.12%] [G loss: 0.800680]\n",
      "epoch:3 step:3107 [D loss: 0.671009, acc.: 53.91%] [G loss: 0.848797]\n",
      "epoch:3 step:3108 [D loss: 0.614243, acc.: 70.31%] [G loss: 0.894171]\n",
      "epoch:3 step:3109 [D loss: 0.700204, acc.: 42.97%] [G loss: 0.826620]\n",
      "epoch:3 step:3110 [D loss: 0.660344, acc.: 62.50%] [G loss: 0.799780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3111 [D loss: 0.650420, acc.: 62.50%] [G loss: 0.845971]\n",
      "epoch:3 step:3112 [D loss: 0.692751, acc.: 51.56%] [G loss: 0.869556]\n",
      "epoch:3 step:3113 [D loss: 0.717927, acc.: 46.09%] [G loss: 0.838997]\n",
      "epoch:3 step:3114 [D loss: 0.690904, acc.: 54.69%] [G loss: 0.892371]\n",
      "epoch:3 step:3115 [D loss: 0.758008, acc.: 37.50%] [G loss: 0.906963]\n",
      "epoch:3 step:3116 [D loss: 0.697652, acc.: 50.00%] [G loss: 0.834169]\n",
      "epoch:3 step:3117 [D loss: 0.666716, acc.: 51.56%] [G loss: 0.874691]\n",
      "epoch:3 step:3118 [D loss: 0.673312, acc.: 57.81%] [G loss: 0.827395]\n",
      "epoch:3 step:3119 [D loss: 0.665043, acc.: 61.72%] [G loss: 0.842908]\n",
      "epoch:3 step:3120 [D loss: 0.701789, acc.: 53.91%] [G loss: 0.809320]\n",
      "epoch:3 step:3121 [D loss: 0.671127, acc.: 57.03%] [G loss: 0.822892]\n",
      "epoch:3 step:3122 [D loss: 0.692191, acc.: 46.88%] [G loss: 0.776932]\n",
      "epoch:3 step:3123 [D loss: 0.697604, acc.: 50.00%] [G loss: 0.832327]\n",
      "epoch:3 step:3124 [D loss: 0.685700, acc.: 63.28%] [G loss: 0.813647]\n",
      "epoch:3 step:3125 [D loss: 0.658638, acc.: 60.16%] [G loss: 0.902676]\n",
      "epoch:3 step:3126 [D loss: 0.658268, acc.: 57.81%] [G loss: 0.859665]\n",
      "epoch:3 step:3127 [D loss: 0.655792, acc.: 60.16%] [G loss: 0.928475]\n",
      "epoch:3 step:3128 [D loss: 0.652790, acc.: 59.38%] [G loss: 0.790812]\n",
      "epoch:3 step:3129 [D loss: 0.646772, acc.: 64.06%] [G loss: 0.840720]\n",
      "epoch:3 step:3130 [D loss: 0.649675, acc.: 58.59%] [G loss: 0.824484]\n",
      "epoch:3 step:3131 [D loss: 0.647445, acc.: 66.41%] [G loss: 0.883603]\n",
      "epoch:3 step:3132 [D loss: 0.704556, acc.: 44.53%] [G loss: 0.836253]\n",
      "epoch:3 step:3133 [D loss: 0.682842, acc.: 53.91%] [G loss: 0.829609]\n",
      "epoch:3 step:3134 [D loss: 0.703439, acc.: 52.34%] [G loss: 0.865905]\n",
      "epoch:3 step:3135 [D loss: 0.719887, acc.: 46.09%] [G loss: 0.903599]\n",
      "epoch:3 step:3136 [D loss: 0.721030, acc.: 46.88%] [G loss: 0.856758]\n",
      "epoch:3 step:3137 [D loss: 0.707813, acc.: 46.09%] [G loss: 0.792755]\n",
      "epoch:3 step:3138 [D loss: 0.667014, acc.: 58.59%] [G loss: 0.768298]\n",
      "epoch:3 step:3139 [D loss: 0.675622, acc.: 53.91%] [G loss: 0.908329]\n",
      "epoch:3 step:3140 [D loss: 0.673099, acc.: 52.34%] [G loss: 0.874301]\n",
      "epoch:3 step:3141 [D loss: 0.724824, acc.: 47.66%] [G loss: 0.777610]\n",
      "epoch:3 step:3142 [D loss: 0.710306, acc.: 46.88%] [G loss: 0.785878]\n",
      "epoch:3 step:3143 [D loss: 0.692311, acc.: 53.12%] [G loss: 0.759378]\n",
      "epoch:3 step:3144 [D loss: 0.691645, acc.: 56.25%] [G loss: 0.803700]\n",
      "epoch:3 step:3145 [D loss: 0.661512, acc.: 58.59%] [G loss: 0.763435]\n",
      "epoch:3 step:3146 [D loss: 0.666665, acc.: 56.25%] [G loss: 0.779502]\n",
      "epoch:3 step:3147 [D loss: 0.668638, acc.: 60.94%] [G loss: 0.770294]\n",
      "epoch:3 step:3148 [D loss: 0.684827, acc.: 53.91%] [G loss: 0.826560]\n",
      "epoch:3 step:3149 [D loss: 0.707308, acc.: 50.78%] [G loss: 0.806961]\n",
      "epoch:3 step:3150 [D loss: 0.703897, acc.: 48.44%] [G loss: 0.747161]\n",
      "epoch:3 step:3151 [D loss: 0.709019, acc.: 52.34%] [G loss: 0.725047]\n",
      "epoch:3 step:3152 [D loss: 0.658032, acc.: 62.50%] [G loss: 0.848416]\n",
      "epoch:3 step:3153 [D loss: 0.674918, acc.: 57.03%] [G loss: 0.784497]\n",
      "epoch:3 step:3154 [D loss: 0.703808, acc.: 50.78%] [G loss: 0.790108]\n",
      "epoch:3 step:3155 [D loss: 0.686894, acc.: 56.25%] [G loss: 0.804831]\n",
      "epoch:3 step:3156 [D loss: 0.698688, acc.: 53.91%] [G loss: 0.837622]\n",
      "epoch:3 step:3157 [D loss: 0.681794, acc.: 57.81%] [G loss: 0.787506]\n",
      "epoch:3 step:3158 [D loss: 0.719691, acc.: 51.56%] [G loss: 0.843017]\n",
      "epoch:3 step:3159 [D loss: 0.681025, acc.: 59.38%] [G loss: 0.806367]\n",
      "epoch:3 step:3160 [D loss: 0.661139, acc.: 55.47%] [G loss: 0.765468]\n",
      "epoch:3 step:3161 [D loss: 0.688358, acc.: 63.28%] [G loss: 0.768477]\n",
      "epoch:3 step:3162 [D loss: 0.698151, acc.: 50.78%] [G loss: 0.766204]\n",
      "epoch:3 step:3163 [D loss: 0.693694, acc.: 50.00%] [G loss: 0.833590]\n",
      "epoch:3 step:3164 [D loss: 0.711195, acc.: 50.78%] [G loss: 0.778153]\n",
      "epoch:3 step:3165 [D loss: 0.711054, acc.: 56.25%] [G loss: 0.805450]\n",
      "epoch:3 step:3166 [D loss: 0.673011, acc.: 56.25%] [G loss: 0.864856]\n",
      "epoch:3 step:3167 [D loss: 0.720830, acc.: 46.09%] [G loss: 0.809425]\n",
      "epoch:3 step:3168 [D loss: 0.676942, acc.: 58.59%] [G loss: 0.813461]\n",
      "epoch:3 step:3169 [D loss: 0.731159, acc.: 46.88%] [G loss: 0.875465]\n",
      "epoch:3 step:3170 [D loss: 0.716914, acc.: 53.12%] [G loss: 0.799698]\n",
      "epoch:3 step:3171 [D loss: 0.694654, acc.: 57.03%] [G loss: 0.823775]\n",
      "epoch:3 step:3172 [D loss: 0.719553, acc.: 39.84%] [G loss: 0.783862]\n",
      "epoch:3 step:3173 [D loss: 0.679793, acc.: 53.91%] [G loss: 0.820958]\n",
      "epoch:3 step:3174 [D loss: 0.652052, acc.: 60.16%] [G loss: 0.912320]\n",
      "epoch:3 step:3175 [D loss: 0.694442, acc.: 49.22%] [G loss: 0.779660]\n",
      "epoch:3 step:3176 [D loss: 0.666914, acc.: 55.47%] [G loss: 0.796830]\n",
      "epoch:3 step:3177 [D loss: 0.661234, acc.: 58.59%] [G loss: 0.834286]\n",
      "epoch:3 step:3178 [D loss: 0.693082, acc.: 51.56%] [G loss: 0.801740]\n",
      "epoch:3 step:3179 [D loss: 0.681665, acc.: 54.69%] [G loss: 0.785000]\n",
      "epoch:3 step:3180 [D loss: 0.686163, acc.: 56.25%] [G loss: 0.810817]\n",
      "epoch:3 step:3181 [D loss: 0.662073, acc.: 57.81%] [G loss: 0.807461]\n",
      "epoch:3 step:3182 [D loss: 0.711312, acc.: 48.44%] [G loss: 0.788114]\n",
      "epoch:3 step:3183 [D loss: 0.694283, acc.: 52.34%] [G loss: 0.765355]\n",
      "epoch:3 step:3184 [D loss: 0.686327, acc.: 58.59%] [G loss: 0.810936]\n",
      "epoch:3 step:3185 [D loss: 0.678216, acc.: 57.03%] [G loss: 0.822008]\n",
      "epoch:3 step:3186 [D loss: 0.673861, acc.: 55.47%] [G loss: 0.884228]\n",
      "epoch:3 step:3187 [D loss: 0.709697, acc.: 48.44%] [G loss: 0.780633]\n",
      "epoch:3 step:3188 [D loss: 0.708155, acc.: 47.66%] [G loss: 0.802635]\n",
      "epoch:3 step:3189 [D loss: 0.641414, acc.: 66.41%] [G loss: 0.841878]\n",
      "epoch:3 step:3190 [D loss: 0.695456, acc.: 51.56%] [G loss: 0.849578]\n",
      "epoch:3 step:3191 [D loss: 0.694339, acc.: 49.22%] [G loss: 0.809459]\n",
      "epoch:3 step:3192 [D loss: 0.675503, acc.: 51.56%] [G loss: 0.831275]\n",
      "epoch:3 step:3193 [D loss: 0.710462, acc.: 49.22%] [G loss: 0.826218]\n",
      "epoch:3 step:3194 [D loss: 0.702919, acc.: 51.56%] [G loss: 0.787856]\n",
      "epoch:3 step:3195 [D loss: 0.679205, acc.: 57.81%] [G loss: 0.751536]\n",
      "epoch:3 step:3196 [D loss: 0.690168, acc.: 55.47%] [G loss: 0.762728]\n",
      "epoch:3 step:3197 [D loss: 0.722788, acc.: 53.12%] [G loss: 0.765420]\n",
      "epoch:3 step:3198 [D loss: 0.683060, acc.: 60.94%] [G loss: 0.831220]\n",
      "epoch:3 step:3199 [D loss: 0.669232, acc.: 54.69%] [G loss: 0.823930]\n",
      "epoch:3 step:3200 [D loss: 0.675445, acc.: 55.47%] [G loss: 0.879319]\n",
      "epoch:3 step:3201 [D loss: 0.720601, acc.: 51.56%] [G loss: 0.803366]\n",
      "epoch:3 step:3202 [D loss: 0.703208, acc.: 53.12%] [G loss: 0.843920]\n",
      "epoch:3 step:3203 [D loss: 0.712966, acc.: 52.34%] [G loss: 0.840684]\n",
      "epoch:3 step:3204 [D loss: 0.693369, acc.: 52.34%] [G loss: 0.791649]\n",
      "epoch:3 step:3205 [D loss: 0.658429, acc.: 57.03%] [G loss: 0.783416]\n",
      "epoch:3 step:3206 [D loss: 0.695767, acc.: 50.00%] [G loss: 0.785946]\n",
      "epoch:3 step:3207 [D loss: 0.680357, acc.: 50.00%] [G loss: 0.847368]\n",
      "epoch:3 step:3208 [D loss: 0.688816, acc.: 53.91%] [G loss: 0.790354]\n",
      "epoch:3 step:3209 [D loss: 0.659149, acc.: 57.81%] [G loss: 0.789274]\n",
      "epoch:3 step:3210 [D loss: 0.690430, acc.: 54.69%] [G loss: 0.802049]\n",
      "epoch:3 step:3211 [D loss: 0.681341, acc.: 54.69%] [G loss: 0.743233]\n",
      "epoch:3 step:3212 [D loss: 0.678303, acc.: 57.03%] [G loss: 0.777951]\n",
      "epoch:3 step:3213 [D loss: 0.646244, acc.: 63.28%] [G loss: 0.781935]\n",
      "epoch:3 step:3214 [D loss: 0.688247, acc.: 51.56%] [G loss: 0.815772]\n",
      "epoch:3 step:3215 [D loss: 0.703684, acc.: 46.09%] [G loss: 0.789314]\n",
      "epoch:3 step:3216 [D loss: 0.672589, acc.: 52.34%] [G loss: 0.807609]\n",
      "epoch:3 step:3217 [D loss: 0.667518, acc.: 57.03%] [G loss: 0.739742]\n",
      "epoch:3 step:3218 [D loss: 0.661888, acc.: 58.59%] [G loss: 0.769289]\n",
      "epoch:3 step:3219 [D loss: 0.686285, acc.: 53.12%] [G loss: 0.823243]\n",
      "epoch:3 step:3220 [D loss: 0.704429, acc.: 52.34%] [G loss: 0.811260]\n",
      "epoch:3 step:3221 [D loss: 0.665421, acc.: 56.25%] [G loss: 0.801884]\n",
      "epoch:3 step:3222 [D loss: 0.696009, acc.: 50.78%] [G loss: 0.820165]\n",
      "epoch:3 step:3223 [D loss: 0.676063, acc.: 57.81%] [G loss: 0.868290]\n",
      "epoch:3 step:3224 [D loss: 0.679877, acc.: 55.47%] [G loss: 0.852673]\n",
      "epoch:3 step:3225 [D loss: 0.670509, acc.: 53.91%] [G loss: 0.846646]\n",
      "epoch:3 step:3226 [D loss: 0.677662, acc.: 56.25%] [G loss: 0.850283]\n",
      "epoch:3 step:3227 [D loss: 0.695921, acc.: 52.34%] [G loss: 0.810728]\n",
      "epoch:3 step:3228 [D loss: 0.683534, acc.: 62.50%] [G loss: 0.838450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3229 [D loss: 0.672810, acc.: 57.81%] [G loss: 0.847687]\n",
      "epoch:3 step:3230 [D loss: 0.718595, acc.: 50.00%] [G loss: 0.796535]\n",
      "epoch:3 step:3231 [D loss: 0.687829, acc.: 53.12%] [G loss: 0.831086]\n",
      "epoch:3 step:3232 [D loss: 0.687905, acc.: 48.44%] [G loss: 0.779192]\n",
      "epoch:3 step:3233 [D loss: 0.689008, acc.: 48.44%] [G loss: 0.809084]\n",
      "epoch:3 step:3234 [D loss: 0.707567, acc.: 48.44%] [G loss: 0.821221]\n",
      "epoch:3 step:3235 [D loss: 0.714588, acc.: 55.47%] [G loss: 0.822078]\n",
      "epoch:3 step:3236 [D loss: 0.714570, acc.: 46.88%] [G loss: 0.809378]\n",
      "epoch:3 step:3237 [D loss: 0.674949, acc.: 55.47%] [G loss: 0.815350]\n",
      "epoch:3 step:3238 [D loss: 0.679908, acc.: 57.81%] [G loss: 0.842114]\n",
      "epoch:3 step:3239 [D loss: 0.686343, acc.: 58.59%] [G loss: 1.015297]\n",
      "epoch:3 step:3240 [D loss: 0.661089, acc.: 64.84%] [G loss: 0.858014]\n",
      "epoch:3 step:3241 [D loss: 0.663023, acc.: 60.16%] [G loss: 0.797319]\n",
      "epoch:3 step:3242 [D loss: 0.711347, acc.: 58.59%] [G loss: 0.731531]\n",
      "epoch:3 step:3243 [D loss: 0.699887, acc.: 53.91%] [G loss: 0.786815]\n",
      "epoch:3 step:3244 [D loss: 0.737384, acc.: 45.31%] [G loss: 0.778129]\n",
      "epoch:3 step:3245 [D loss: 0.693376, acc.: 51.56%] [G loss: 0.800389]\n",
      "epoch:3 step:3246 [D loss: 0.685772, acc.: 57.03%] [G loss: 0.822168]\n",
      "epoch:3 step:3247 [D loss: 0.673505, acc.: 55.47%] [G loss: 0.824202]\n",
      "epoch:3 step:3248 [D loss: 0.680908, acc.: 53.12%] [G loss: 0.886344]\n",
      "epoch:3 step:3249 [D loss: 0.667122, acc.: 57.03%] [G loss: 0.894560]\n",
      "epoch:3 step:3250 [D loss: 0.650025, acc.: 64.84%] [G loss: 0.877439]\n",
      "epoch:3 step:3251 [D loss: 0.677093, acc.: 61.72%] [G loss: 0.856472]\n",
      "epoch:3 step:3252 [D loss: 0.654712, acc.: 60.94%] [G loss: 0.865416]\n",
      "epoch:3 step:3253 [D loss: 0.660065, acc.: 59.38%] [G loss: 0.852551]\n",
      "epoch:3 step:3254 [D loss: 0.668723, acc.: 58.59%] [G loss: 0.826382]\n",
      "epoch:3 step:3255 [D loss: 0.652897, acc.: 56.25%] [G loss: 0.840406]\n",
      "epoch:3 step:3256 [D loss: 0.671527, acc.: 54.69%] [G loss: 0.734773]\n",
      "epoch:3 step:3257 [D loss: 0.676200, acc.: 54.69%] [G loss: 0.816801]\n",
      "epoch:3 step:3258 [D loss: 0.674315, acc.: 60.94%] [G loss: 0.748656]\n",
      "epoch:3 step:3259 [D loss: 0.737568, acc.: 49.22%] [G loss: 0.776721]\n",
      "epoch:3 step:3260 [D loss: 0.689974, acc.: 59.38%] [G loss: 0.848570]\n",
      "epoch:3 step:3261 [D loss: 0.678605, acc.: 59.38%] [G loss: 0.875523]\n",
      "epoch:3 step:3262 [D loss: 0.657188, acc.: 63.28%] [G loss: 0.923557]\n",
      "epoch:3 step:3263 [D loss: 0.654073, acc.: 67.19%] [G loss: 0.957300]\n",
      "epoch:3 step:3264 [D loss: 0.639073, acc.: 62.50%] [G loss: 0.988216]\n",
      "epoch:3 step:3265 [D loss: 0.659886, acc.: 58.59%] [G loss: 0.832927]\n",
      "epoch:3 step:3266 [D loss: 0.682991, acc.: 54.69%] [G loss: 0.903093]\n",
      "epoch:3 step:3267 [D loss: 0.717051, acc.: 48.44%] [G loss: 0.804338]\n",
      "epoch:3 step:3268 [D loss: 0.686700, acc.: 53.12%] [G loss: 0.791516]\n",
      "epoch:3 step:3269 [D loss: 0.699463, acc.: 51.56%] [G loss: 0.831064]\n",
      "epoch:3 step:3270 [D loss: 0.666469, acc.: 56.25%] [G loss: 0.878692]\n",
      "epoch:3 step:3271 [D loss: 0.712820, acc.: 50.00%] [G loss: 0.712506]\n",
      "epoch:3 step:3272 [D loss: 0.703929, acc.: 59.38%] [G loss: 0.865279]\n",
      "epoch:3 step:3273 [D loss: 0.643528, acc.: 63.28%] [G loss: 0.871895]\n",
      "epoch:3 step:3274 [D loss: 0.701083, acc.: 53.91%] [G loss: 0.816597]\n",
      "epoch:3 step:3275 [D loss: 0.691492, acc.: 47.66%] [G loss: 0.777679]\n",
      "epoch:3 step:3276 [D loss: 0.656998, acc.: 64.06%] [G loss: 0.862965]\n",
      "epoch:3 step:3277 [D loss: 0.684023, acc.: 63.28%] [G loss: 0.830552]\n",
      "epoch:3 step:3278 [D loss: 0.707649, acc.: 48.44%] [G loss: 0.884269]\n",
      "epoch:3 step:3279 [D loss: 0.658071, acc.: 59.38%] [G loss: 0.838471]\n",
      "epoch:3 step:3280 [D loss: 0.724999, acc.: 50.78%] [G loss: 0.780967]\n",
      "epoch:3 step:3281 [D loss: 0.705496, acc.: 51.56%] [G loss: 0.844885]\n",
      "epoch:3 step:3282 [D loss: 0.660961, acc.: 56.25%] [G loss: 0.753655]\n",
      "epoch:3 step:3283 [D loss: 0.681478, acc.: 49.22%] [G loss: 0.826881]\n",
      "epoch:3 step:3284 [D loss: 0.697513, acc.: 47.66%] [G loss: 0.826580]\n",
      "epoch:3 step:3285 [D loss: 0.645815, acc.: 61.72%] [G loss: 0.862184]\n",
      "epoch:3 step:3286 [D loss: 0.685304, acc.: 57.81%] [G loss: 0.817256]\n",
      "epoch:3 step:3287 [D loss: 0.694173, acc.: 58.59%] [G loss: 0.761795]\n",
      "epoch:3 step:3288 [D loss: 0.697489, acc.: 46.88%] [G loss: 0.850641]\n",
      "epoch:3 step:3289 [D loss: 0.679916, acc.: 57.81%] [G loss: 0.818256]\n",
      "epoch:3 step:3290 [D loss: 0.699878, acc.: 51.56%] [G loss: 0.883031]\n",
      "epoch:3 step:3291 [D loss: 0.700897, acc.: 51.56%] [G loss: 0.901058]\n",
      "epoch:3 step:3292 [D loss: 0.701994, acc.: 47.66%] [G loss: 0.975984]\n",
      "epoch:3 step:3293 [D loss: 0.696274, acc.: 49.22%] [G loss: 0.842039]\n",
      "epoch:3 step:3294 [D loss: 0.726799, acc.: 49.22%] [G loss: 0.898317]\n",
      "epoch:3 step:3295 [D loss: 0.715131, acc.: 50.78%] [G loss: 0.878343]\n",
      "epoch:3 step:3296 [D loss: 0.714618, acc.: 53.91%] [G loss: 0.879973]\n",
      "epoch:3 step:3297 [D loss: 0.652085, acc.: 65.62%] [G loss: 0.930122]\n",
      "epoch:3 step:3298 [D loss: 0.660455, acc.: 64.06%] [G loss: 0.879521]\n",
      "epoch:3 step:3299 [D loss: 0.652436, acc.: 60.94%] [G loss: 0.815703]\n",
      "epoch:3 step:3300 [D loss: 0.686367, acc.: 52.34%] [G loss: 0.828027]\n",
      "epoch:3 step:3301 [D loss: 0.663637, acc.: 55.47%] [G loss: 0.811433]\n",
      "epoch:3 step:3302 [D loss: 0.711934, acc.: 53.12%] [G loss: 0.788898]\n",
      "epoch:3 step:3303 [D loss: 0.677589, acc.: 59.38%] [G loss: 0.772368]\n",
      "epoch:3 step:3304 [D loss: 0.705940, acc.: 50.00%] [G loss: 0.804325]\n",
      "epoch:3 step:3305 [D loss: 0.686970, acc.: 55.47%] [G loss: 0.807566]\n",
      "epoch:3 step:3306 [D loss: 0.680226, acc.: 57.03%] [G loss: 0.833608]\n",
      "epoch:3 step:3307 [D loss: 0.682730, acc.: 50.78%] [G loss: 0.802265]\n",
      "epoch:3 step:3308 [D loss: 0.682034, acc.: 54.69%] [G loss: 0.787100]\n",
      "epoch:3 step:3309 [D loss: 0.689962, acc.: 50.00%] [G loss: 0.800438]\n",
      "epoch:3 step:3310 [D loss: 0.670586, acc.: 58.59%] [G loss: 0.764149]\n",
      "epoch:3 step:3311 [D loss: 0.706286, acc.: 48.44%] [G loss: 0.824599]\n",
      "epoch:3 step:3312 [D loss: 0.684682, acc.: 49.22%] [G loss: 0.864431]\n",
      "epoch:3 step:3313 [D loss: 0.690081, acc.: 56.25%] [G loss: 0.856854]\n",
      "epoch:3 step:3314 [D loss: 0.687071, acc.: 56.25%] [G loss: 0.812094]\n",
      "epoch:3 step:3315 [D loss: 0.669369, acc.: 55.47%] [G loss: 0.811702]\n",
      "epoch:3 step:3316 [D loss: 0.674008, acc.: 57.03%] [G loss: 0.826122]\n",
      "epoch:3 step:3317 [D loss: 0.669153, acc.: 62.50%] [G loss: 0.803250]\n",
      "epoch:3 step:3318 [D loss: 0.692946, acc.: 51.56%] [G loss: 0.803674]\n",
      "epoch:3 step:3319 [D loss: 0.672478, acc.: 55.47%] [G loss: 0.788153]\n",
      "epoch:3 step:3320 [D loss: 0.676677, acc.: 50.78%] [G loss: 0.818826]\n",
      "epoch:3 step:3321 [D loss: 0.683832, acc.: 50.00%] [G loss: 0.745974]\n",
      "epoch:3 step:3322 [D loss: 0.683273, acc.: 55.47%] [G loss: 0.760426]\n",
      "epoch:3 step:3323 [D loss: 0.681647, acc.: 52.34%] [G loss: 0.867726]\n",
      "epoch:3 step:3324 [D loss: 0.666941, acc.: 62.50%] [G loss: 0.787993]\n",
      "epoch:3 step:3325 [D loss: 0.729079, acc.: 49.22%] [G loss: 0.832750]\n",
      "epoch:3 step:3326 [D loss: 0.691693, acc.: 49.22%] [G loss: 0.808725]\n",
      "epoch:3 step:3327 [D loss: 0.690050, acc.: 61.72%] [G loss: 0.813079]\n",
      "epoch:3 step:3328 [D loss: 0.662254, acc.: 60.16%] [G loss: 0.876774]\n",
      "epoch:3 step:3329 [D loss: 0.700687, acc.: 49.22%] [G loss: 0.810469]\n",
      "epoch:3 step:3330 [D loss: 0.662775, acc.: 57.81%] [G loss: 0.787677]\n",
      "epoch:3 step:3331 [D loss: 0.635835, acc.: 61.72%] [G loss: 0.866040]\n",
      "epoch:3 step:3332 [D loss: 0.690025, acc.: 49.22%] [G loss: 0.793976]\n",
      "epoch:3 step:3333 [D loss: 0.689232, acc.: 58.59%] [G loss: 0.793875]\n",
      "epoch:3 step:3334 [D loss: 0.650579, acc.: 62.50%] [G loss: 0.804384]\n",
      "epoch:3 step:3335 [D loss: 0.681273, acc.: 58.59%] [G loss: 0.757503]\n",
      "epoch:3 step:3336 [D loss: 0.732106, acc.: 48.44%] [G loss: 0.784330]\n",
      "epoch:3 step:3337 [D loss: 0.693054, acc.: 47.66%] [G loss: 0.802926]\n",
      "epoch:3 step:3338 [D loss: 0.710448, acc.: 46.88%] [G loss: 0.811543]\n",
      "epoch:3 step:3339 [D loss: 0.707411, acc.: 53.12%] [G loss: 0.839891]\n",
      "epoch:3 step:3340 [D loss: 0.656202, acc.: 57.03%] [G loss: 0.886024]\n",
      "epoch:3 step:3341 [D loss: 0.683027, acc.: 57.03%] [G loss: 0.829730]\n",
      "epoch:3 step:3342 [D loss: 0.722004, acc.: 46.88%] [G loss: 0.872542]\n",
      "epoch:3 step:3343 [D loss: 0.686800, acc.: 52.34%] [G loss: 0.786901]\n",
      "epoch:3 step:3344 [D loss: 0.724301, acc.: 42.97%] [G loss: 0.829649]\n",
      "epoch:3 step:3345 [D loss: 0.674261, acc.: 54.69%] [G loss: 0.764289]\n",
      "epoch:3 step:3346 [D loss: 0.670870, acc.: 59.38%] [G loss: 0.753785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3347 [D loss: 0.684213, acc.: 55.47%] [G loss: 0.850778]\n",
      "epoch:3 step:3348 [D loss: 0.669692, acc.: 57.81%] [G loss: 0.846288]\n",
      "epoch:3 step:3349 [D loss: 0.699275, acc.: 52.34%] [G loss: 0.907074]\n",
      "epoch:3 step:3350 [D loss: 0.724368, acc.: 50.00%] [G loss: 0.847489]\n",
      "epoch:3 step:3351 [D loss: 0.653016, acc.: 61.72%] [G loss: 0.834563]\n",
      "epoch:3 step:3352 [D loss: 0.668986, acc.: 56.25%] [G loss: 0.819222]\n",
      "epoch:3 step:3353 [D loss: 0.696001, acc.: 53.12%] [G loss: 0.832979]\n",
      "epoch:3 step:3354 [D loss: 0.677569, acc.: 56.25%] [G loss: 0.860314]\n",
      "epoch:3 step:3355 [D loss: 0.721898, acc.: 50.00%] [G loss: 0.830577]\n",
      "epoch:3 step:3356 [D loss: 0.655191, acc.: 60.16%] [G loss: 0.804579]\n",
      "epoch:3 step:3357 [D loss: 0.703393, acc.: 49.22%] [G loss: 0.800095]\n",
      "epoch:3 step:3358 [D loss: 0.702147, acc.: 51.56%] [G loss: 0.808595]\n",
      "epoch:3 step:3359 [D loss: 0.705137, acc.: 50.78%] [G loss: 0.827535]\n",
      "epoch:3 step:3360 [D loss: 0.724992, acc.: 48.44%] [G loss: 0.824080]\n",
      "epoch:3 step:3361 [D loss: 0.693215, acc.: 50.00%] [G loss: 0.805485]\n",
      "epoch:3 step:3362 [D loss: 0.676790, acc.: 57.81%] [G loss: 0.779780]\n",
      "epoch:3 step:3363 [D loss: 0.700632, acc.: 53.12%] [G loss: 0.787400]\n",
      "epoch:3 step:3364 [D loss: 0.713014, acc.: 46.09%] [G loss: 0.788130]\n",
      "epoch:3 step:3365 [D loss: 0.685125, acc.: 55.47%] [G loss: 0.788535]\n",
      "epoch:3 step:3366 [D loss: 0.629990, acc.: 60.94%] [G loss: 0.861971]\n",
      "epoch:3 step:3367 [D loss: 0.681418, acc.: 57.81%] [G loss: 0.863546]\n",
      "epoch:3 step:3368 [D loss: 0.686765, acc.: 54.69%] [G loss: 0.811777]\n",
      "epoch:3 step:3369 [D loss: 0.710487, acc.: 46.09%] [G loss: 0.737937]\n",
      "epoch:3 step:3370 [D loss: 0.719963, acc.: 43.75%] [G loss: 0.944788]\n",
      "epoch:3 step:3371 [D loss: 0.691286, acc.: 56.25%] [G loss: 0.756619]\n",
      "epoch:3 step:3372 [D loss: 0.695854, acc.: 50.00%] [G loss: 0.821631]\n",
      "epoch:3 step:3373 [D loss: 0.716095, acc.: 43.75%] [G loss: 0.781285]\n",
      "epoch:3 step:3374 [D loss: 0.708032, acc.: 49.22%] [G loss: 0.770768]\n",
      "epoch:3 step:3375 [D loss: 0.693248, acc.: 60.94%] [G loss: 0.857190]\n",
      "epoch:3 step:3376 [D loss: 0.697523, acc.: 50.00%] [G loss: 0.842985]\n",
      "epoch:3 step:3377 [D loss: 0.694543, acc.: 49.22%] [G loss: 0.789125]\n",
      "epoch:3 step:3378 [D loss: 0.717433, acc.: 53.12%] [G loss: 0.805567]\n",
      "epoch:3 step:3379 [D loss: 0.710899, acc.: 45.31%] [G loss: 0.804150]\n",
      "epoch:3 step:3380 [D loss: 0.666030, acc.: 58.59%] [G loss: 0.840371]\n",
      "epoch:3 step:3381 [D loss: 0.698581, acc.: 51.56%] [G loss: 0.798902]\n",
      "epoch:3 step:3382 [D loss: 0.664157, acc.: 60.16%] [G loss: 0.854126]\n",
      "epoch:3 step:3383 [D loss: 0.668126, acc.: 58.59%] [G loss: 0.838665]\n",
      "epoch:3 step:3384 [D loss: 0.662177, acc.: 55.47%] [G loss: 0.868500]\n",
      "epoch:3 step:3385 [D loss: 0.700147, acc.: 52.34%] [G loss: 0.831935]\n",
      "epoch:3 step:3386 [D loss: 0.710778, acc.: 50.00%] [G loss: 0.819079]\n",
      "epoch:3 step:3387 [D loss: 0.685326, acc.: 52.34%] [G loss: 0.829077]\n",
      "epoch:3 step:3388 [D loss: 0.673394, acc.: 56.25%] [G loss: 0.795407]\n",
      "epoch:3 step:3389 [D loss: 0.684830, acc.: 55.47%] [G loss: 0.858189]\n",
      "epoch:3 step:3390 [D loss: 0.667949, acc.: 58.59%] [G loss: 0.909720]\n",
      "epoch:3 step:3391 [D loss: 0.690984, acc.: 51.56%] [G loss: 0.904206]\n",
      "epoch:3 step:3392 [D loss: 0.687238, acc.: 54.69%] [G loss: 0.823506]\n",
      "epoch:3 step:3393 [D loss: 0.685479, acc.: 54.69%] [G loss: 0.882878]\n",
      "epoch:3 step:3394 [D loss: 0.694541, acc.: 51.56%] [G loss: 0.892445]\n",
      "epoch:3 step:3395 [D loss: 0.669325, acc.: 59.38%] [G loss: 0.859784]\n",
      "epoch:3 step:3396 [D loss: 0.687929, acc.: 50.00%] [G loss: 0.804959]\n",
      "epoch:3 step:3397 [D loss: 0.702783, acc.: 46.09%] [G loss: 0.752541]\n",
      "epoch:3 step:3398 [D loss: 0.661244, acc.: 58.59%] [G loss: 0.771494]\n",
      "epoch:3 step:3399 [D loss: 0.699144, acc.: 54.69%] [G loss: 0.837149]\n",
      "epoch:3 step:3400 [D loss: 0.694408, acc.: 52.34%] [G loss: 0.780724]\n",
      "epoch:3 step:3401 [D loss: 0.701253, acc.: 54.69%] [G loss: 0.783774]\n",
      "epoch:3 step:3402 [D loss: 0.671211, acc.: 58.59%] [G loss: 0.795630]\n",
      "epoch:3 step:3403 [D loss: 0.640630, acc.: 71.09%] [G loss: 0.804536]\n",
      "epoch:3 step:3404 [D loss: 0.702725, acc.: 50.78%] [G loss: 0.831484]\n",
      "epoch:3 step:3405 [D loss: 0.689320, acc.: 57.03%] [G loss: 0.796778]\n",
      "epoch:3 step:3406 [D loss: 0.685446, acc.: 53.91%] [G loss: 0.813761]\n",
      "epoch:3 step:3407 [D loss: 0.652988, acc.: 65.62%] [G loss: 0.809826]\n",
      "epoch:3 step:3408 [D loss: 0.693846, acc.: 52.34%] [G loss: 0.819091]\n",
      "epoch:3 step:3409 [D loss: 0.665259, acc.: 57.81%] [G loss: 0.893629]\n",
      "epoch:3 step:3410 [D loss: 0.710968, acc.: 48.44%] [G loss: 0.792318]\n",
      "epoch:3 step:3411 [D loss: 0.735310, acc.: 47.66%] [G loss: 0.864893]\n",
      "epoch:3 step:3412 [D loss: 0.689557, acc.: 53.12%] [G loss: 0.915052]\n",
      "epoch:3 step:3413 [D loss: 0.661699, acc.: 64.06%] [G loss: 0.884028]\n",
      "epoch:3 step:3414 [D loss: 0.719642, acc.: 49.22%] [G loss: 0.820210]\n",
      "epoch:3 step:3415 [D loss: 0.709005, acc.: 53.12%] [G loss: 0.795310]\n",
      "epoch:3 step:3416 [D loss: 0.693068, acc.: 50.00%] [G loss: 0.804747]\n",
      "epoch:3 step:3417 [D loss: 0.699016, acc.: 50.00%] [G loss: 0.866915]\n",
      "epoch:3 step:3418 [D loss: 0.687667, acc.: 57.03%] [G loss: 0.836248]\n",
      "epoch:3 step:3419 [D loss: 0.671776, acc.: 61.72%] [G loss: 0.824636]\n",
      "epoch:3 step:3420 [D loss: 0.679317, acc.: 54.69%] [G loss: 0.781371]\n",
      "epoch:3 step:3421 [D loss: 0.693246, acc.: 55.47%] [G loss: 0.817669]\n",
      "epoch:3 step:3422 [D loss: 0.678223, acc.: 56.25%] [G loss: 0.772518]\n",
      "epoch:3 step:3423 [D loss: 0.699758, acc.: 56.25%] [G loss: 0.774841]\n",
      "epoch:3 step:3424 [D loss: 0.684384, acc.: 55.47%] [G loss: 0.766698]\n",
      "epoch:3 step:3425 [D loss: 0.659281, acc.: 62.50%] [G loss: 0.781361]\n",
      "epoch:3 step:3426 [D loss: 0.714453, acc.: 53.12%] [G loss: 0.801001]\n",
      "epoch:3 step:3427 [D loss: 0.679650, acc.: 51.56%] [G loss: 0.803050]\n",
      "epoch:3 step:3428 [D loss: 0.662046, acc.: 53.91%] [G loss: 0.831444]\n",
      "epoch:3 step:3429 [D loss: 0.656501, acc.: 58.59%] [G loss: 0.807906]\n",
      "epoch:3 step:3430 [D loss: 0.680367, acc.: 52.34%] [G loss: 0.861650]\n",
      "epoch:3 step:3431 [D loss: 0.649762, acc.: 55.47%] [G loss: 0.849310]\n",
      "epoch:3 step:3432 [D loss: 0.661366, acc.: 60.94%] [G loss: 0.804293]\n",
      "epoch:3 step:3433 [D loss: 0.695499, acc.: 48.44%] [G loss: 0.840889]\n",
      "epoch:3 step:3434 [D loss: 0.734669, acc.: 55.47%] [G loss: 0.816200]\n",
      "epoch:3 step:3435 [D loss: 0.657385, acc.: 55.47%] [G loss: 0.883264]\n",
      "epoch:3 step:3436 [D loss: 0.712144, acc.: 53.12%] [G loss: 0.754064]\n",
      "epoch:3 step:3437 [D loss: 0.710282, acc.: 42.97%] [G loss: 0.796101]\n",
      "epoch:3 step:3438 [D loss: 0.711478, acc.: 51.56%] [G loss: 0.782140]\n",
      "epoch:3 step:3439 [D loss: 0.691184, acc.: 52.34%] [G loss: 0.788103]\n",
      "epoch:3 step:3440 [D loss: 0.695596, acc.: 53.91%] [G loss: 0.852277]\n",
      "epoch:3 step:3441 [D loss: 0.712395, acc.: 49.22%] [G loss: 0.812590]\n",
      "epoch:3 step:3442 [D loss: 0.694005, acc.: 56.25%] [G loss: 0.833626]\n",
      "epoch:3 step:3443 [D loss: 0.690288, acc.: 55.47%] [G loss: 0.777247]\n",
      "epoch:3 step:3444 [D loss: 0.699790, acc.: 53.91%] [G loss: 0.765270]\n",
      "epoch:3 step:3445 [D loss: 0.706306, acc.: 54.69%] [G loss: 0.837596]\n",
      "epoch:3 step:3446 [D loss: 0.702753, acc.: 49.22%] [G loss: 0.843318]\n",
      "epoch:3 step:3447 [D loss: 0.698276, acc.: 52.34%] [G loss: 0.857543]\n",
      "epoch:3 step:3448 [D loss: 0.676468, acc.: 50.78%] [G loss: 0.851086]\n",
      "epoch:3 step:3449 [D loss: 0.665869, acc.: 60.16%] [G loss: 0.843125]\n",
      "epoch:3 step:3450 [D loss: 0.707172, acc.: 46.88%] [G loss: 0.854455]\n",
      "epoch:3 step:3451 [D loss: 0.664313, acc.: 57.81%] [G loss: 0.847972]\n",
      "epoch:3 step:3452 [D loss: 0.699550, acc.: 46.88%] [G loss: 0.751790]\n",
      "epoch:3 step:3453 [D loss: 0.688689, acc.: 51.56%] [G loss: 0.789488]\n",
      "epoch:3 step:3454 [D loss: 0.687751, acc.: 48.44%] [G loss: 0.756655]\n",
      "epoch:3 step:3455 [D loss: 0.705019, acc.: 50.00%] [G loss: 0.819858]\n",
      "epoch:3 step:3456 [D loss: 0.713444, acc.: 46.09%] [G loss: 0.771200]\n",
      "epoch:3 step:3457 [D loss: 0.814123, acc.: 37.50%] [G loss: 0.817292]\n",
      "epoch:3 step:3458 [D loss: 0.662756, acc.: 60.16%] [G loss: 0.856023]\n",
      "epoch:3 step:3459 [D loss: 0.712429, acc.: 43.75%] [G loss: 0.856756]\n",
      "epoch:3 step:3460 [D loss: 0.704515, acc.: 45.31%] [G loss: 0.804711]\n",
      "epoch:3 step:3461 [D loss: 0.688194, acc.: 53.91%] [G loss: 0.841418]\n",
      "epoch:3 step:3462 [D loss: 0.667359, acc.: 60.94%] [G loss: 0.840493]\n",
      "epoch:3 step:3463 [D loss: 0.692554, acc.: 54.69%] [G loss: 0.776018]\n",
      "epoch:3 step:3464 [D loss: 0.684573, acc.: 55.47%] [G loss: 0.798476]\n",
      "epoch:3 step:3465 [D loss: 0.707950, acc.: 42.97%] [G loss: 0.841964]\n",
      "epoch:3 step:3466 [D loss: 0.700125, acc.: 51.56%] [G loss: 0.792103]\n",
      "epoch:3 step:3467 [D loss: 0.669149, acc.: 56.25%] [G loss: 0.691207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3468 [D loss: 0.671510, acc.: 57.81%] [G loss: 0.781441]\n",
      "epoch:3 step:3469 [D loss: 0.698996, acc.: 57.03%] [G loss: 0.779440]\n",
      "epoch:3 step:3470 [D loss: 0.677154, acc.: 54.69%] [G loss: 0.763615]\n",
      "epoch:3 step:3471 [D loss: 0.665605, acc.: 59.38%] [G loss: 0.762994]\n",
      "epoch:3 step:3472 [D loss: 0.696578, acc.: 47.66%] [G loss: 0.738460]\n",
      "epoch:3 step:3473 [D loss: 0.697040, acc.: 49.22%] [G loss: 0.759404]\n",
      "epoch:3 step:3474 [D loss: 0.684000, acc.: 59.38%] [G loss: 0.733749]\n",
      "epoch:3 step:3475 [D loss: 0.691072, acc.: 51.56%] [G loss: 0.764313]\n",
      "epoch:3 step:3476 [D loss: 0.697470, acc.: 56.25%] [G loss: 0.812000]\n",
      "epoch:3 step:3477 [D loss: 0.691707, acc.: 46.88%] [G loss: 0.831068]\n",
      "epoch:3 step:3478 [D loss: 0.711439, acc.: 49.22%] [G loss: 0.852975]\n",
      "epoch:3 step:3479 [D loss: 0.672408, acc.: 59.38%] [G loss: 0.798221]\n",
      "epoch:3 step:3480 [D loss: 0.698692, acc.: 46.88%] [G loss: 0.790487]\n",
      "epoch:3 step:3481 [D loss: 0.649960, acc.: 53.91%] [G loss: 0.835016]\n",
      "epoch:3 step:3482 [D loss: 0.683681, acc.: 53.12%] [G loss: 0.829125]\n",
      "epoch:3 step:3483 [D loss: 0.682389, acc.: 54.69%] [G loss: 0.818695]\n",
      "epoch:3 step:3484 [D loss: 0.719267, acc.: 46.09%] [G loss: 0.758915]\n",
      "epoch:3 step:3485 [D loss: 0.681523, acc.: 54.69%] [G loss: 0.762228]\n",
      "epoch:3 step:3486 [D loss: 0.703249, acc.: 51.56%] [G loss: 0.800925]\n",
      "epoch:3 step:3487 [D loss: 0.703142, acc.: 51.56%] [G loss: 0.839184]\n",
      "epoch:3 step:3488 [D loss: 0.683929, acc.: 54.69%] [G loss: 0.812540]\n",
      "epoch:3 step:3489 [D loss: 0.702026, acc.: 54.69%] [G loss: 0.818936]\n",
      "epoch:3 step:3490 [D loss: 0.675260, acc.: 54.69%] [G loss: 0.792244]\n",
      "epoch:3 step:3491 [D loss: 0.694916, acc.: 50.78%] [G loss: 0.769314]\n",
      "epoch:3 step:3492 [D loss: 0.666342, acc.: 57.81%] [G loss: 0.782293]\n",
      "epoch:3 step:3493 [D loss: 0.660495, acc.: 60.16%] [G loss: 0.764037]\n",
      "epoch:3 step:3494 [D loss: 0.695137, acc.: 47.66%] [G loss: 0.870466]\n",
      "epoch:3 step:3495 [D loss: 0.691839, acc.: 50.78%] [G loss: 0.809654]\n",
      "epoch:3 step:3496 [D loss: 0.685746, acc.: 63.28%] [G loss: 0.741008]\n",
      "epoch:3 step:3497 [D loss: 0.688441, acc.: 54.69%] [G loss: 0.830665]\n",
      "epoch:3 step:3498 [D loss: 0.724541, acc.: 46.09%] [G loss: 0.776248]\n",
      "epoch:3 step:3499 [D loss: 0.701480, acc.: 50.78%] [G loss: 0.810316]\n",
      "epoch:3 step:3500 [D loss: 0.695995, acc.: 50.78%] [G loss: 0.766736]\n",
      "epoch:3 step:3501 [D loss: 0.710133, acc.: 48.44%] [G loss: 0.763714]\n",
      "epoch:3 step:3502 [D loss: 0.691478, acc.: 45.31%] [G loss: 0.810055]\n",
      "epoch:3 step:3503 [D loss: 0.677561, acc.: 53.12%] [G loss: 0.823136]\n",
      "epoch:3 step:3504 [D loss: 0.711118, acc.: 46.09%] [G loss: 0.842531]\n",
      "epoch:3 step:3505 [D loss: 0.700902, acc.: 55.47%] [G loss: 0.780992]\n",
      "epoch:3 step:3506 [D loss: 0.690128, acc.: 57.03%] [G loss: 0.774140]\n",
      "epoch:3 step:3507 [D loss: 0.696721, acc.: 51.56%] [G loss: 0.795192]\n",
      "epoch:3 step:3508 [D loss: 0.688655, acc.: 50.00%] [G loss: 0.810626]\n",
      "epoch:3 step:3509 [D loss: 0.689218, acc.: 53.12%] [G loss: 0.734932]\n",
      "epoch:3 step:3510 [D loss: 0.668834, acc.: 56.25%] [G loss: 0.825992]\n",
      "epoch:3 step:3511 [D loss: 0.676059, acc.: 61.72%] [G loss: 0.715224]\n",
      "epoch:3 step:3512 [D loss: 0.648752, acc.: 68.75%] [G loss: 0.831954]\n",
      "epoch:3 step:3513 [D loss: 0.662389, acc.: 62.50%] [G loss: 0.786305]\n",
      "epoch:3 step:3514 [D loss: 0.699271, acc.: 51.56%] [G loss: 0.822000]\n",
      "epoch:3 step:3515 [D loss: 0.688807, acc.: 50.00%] [G loss: 0.812577]\n",
      "epoch:3 step:3516 [D loss: 0.725413, acc.: 45.31%] [G loss: 0.806977]\n",
      "epoch:3 step:3517 [D loss: 0.723230, acc.: 44.53%] [G loss: 0.859001]\n",
      "epoch:3 step:3518 [D loss: 0.675317, acc.: 56.25%] [G loss: 0.857165]\n",
      "epoch:3 step:3519 [D loss: 0.666588, acc.: 60.94%] [G loss: 0.839340]\n",
      "epoch:3 step:3520 [D loss: 0.700752, acc.: 49.22%] [G loss: 0.748479]\n",
      "epoch:3 step:3521 [D loss: 0.686087, acc.: 59.38%] [G loss: 0.847090]\n",
      "epoch:3 step:3522 [D loss: 0.690681, acc.: 53.12%] [G loss: 0.808718]\n",
      "epoch:3 step:3523 [D loss: 0.706498, acc.: 52.34%] [G loss: 0.821207]\n",
      "epoch:3 step:3524 [D loss: 0.667506, acc.: 62.50%] [G loss: 0.822149]\n",
      "epoch:3 step:3525 [D loss: 0.676507, acc.: 55.47%] [G loss: 0.861503]\n",
      "epoch:3 step:3526 [D loss: 0.700893, acc.: 56.25%] [G loss: 0.770356]\n",
      "epoch:3 step:3527 [D loss: 0.735709, acc.: 50.00%] [G loss: 0.798396]\n",
      "epoch:3 step:3528 [D loss: 0.700702, acc.: 48.44%] [G loss: 0.787311]\n",
      "epoch:3 step:3529 [D loss: 0.677827, acc.: 57.03%] [G loss: 0.742081]\n",
      "epoch:3 step:3530 [D loss: 0.652432, acc.: 60.16%] [G loss: 0.833610]\n",
      "epoch:3 step:3531 [D loss: 0.693432, acc.: 53.91%] [G loss: 0.892315]\n",
      "epoch:3 step:3532 [D loss: 0.685539, acc.: 53.12%] [G loss: 0.876000]\n",
      "epoch:3 step:3533 [D loss: 0.692930, acc.: 50.78%] [G loss: 0.811143]\n",
      "epoch:3 step:3534 [D loss: 0.690309, acc.: 50.78%] [G loss: 0.819219]\n",
      "epoch:3 step:3535 [D loss: 0.674363, acc.: 58.59%] [G loss: 0.798031]\n",
      "epoch:3 step:3536 [D loss: 0.662907, acc.: 61.72%] [G loss: 0.783452]\n",
      "epoch:3 step:3537 [D loss: 0.708270, acc.: 44.53%] [G loss: 0.774432]\n",
      "epoch:3 step:3538 [D loss: 0.664823, acc.: 60.94%] [G loss: 0.755930]\n",
      "epoch:3 step:3539 [D loss: 0.645650, acc.: 63.28%] [G loss: 0.812789]\n",
      "epoch:3 step:3540 [D loss: 0.674142, acc.: 53.91%] [G loss: 0.793188]\n",
      "epoch:3 step:3541 [D loss: 0.685955, acc.: 50.78%] [G loss: 0.761224]\n",
      "epoch:3 step:3542 [D loss: 0.698161, acc.: 55.47%] [G loss: 0.787461]\n",
      "epoch:3 step:3543 [D loss: 0.683883, acc.: 53.91%] [G loss: 0.865476]\n",
      "epoch:3 step:3544 [D loss: 0.667800, acc.: 53.12%] [G loss: 0.785257]\n",
      "epoch:3 step:3545 [D loss: 0.676115, acc.: 60.16%] [G loss: 0.828942]\n",
      "epoch:3 step:3546 [D loss: 0.689970, acc.: 55.47%] [G loss: 0.825430]\n",
      "epoch:3 step:3547 [D loss: 0.645766, acc.: 61.72%] [G loss: 0.858223]\n",
      "epoch:3 step:3548 [D loss: 0.649814, acc.: 63.28%] [G loss: 0.833375]\n",
      "epoch:3 step:3549 [D loss: 0.691939, acc.: 53.12%] [G loss: 0.923182]\n",
      "epoch:3 step:3550 [D loss: 0.670453, acc.: 55.47%] [G loss: 0.793341]\n",
      "epoch:3 step:3551 [D loss: 0.663508, acc.: 59.38%] [G loss: 0.859317]\n",
      "epoch:3 step:3552 [D loss: 0.654007, acc.: 55.47%] [G loss: 0.867334]\n",
      "epoch:3 step:3553 [D loss: 0.658811, acc.: 65.62%] [G loss: 0.775703]\n",
      "epoch:3 step:3554 [D loss: 0.735951, acc.: 48.44%] [G loss: 0.754892]\n",
      "epoch:3 step:3555 [D loss: 0.648509, acc.: 57.03%] [G loss: 0.841132]\n",
      "epoch:3 step:3556 [D loss: 0.692455, acc.: 52.34%] [G loss: 0.804290]\n",
      "epoch:3 step:3557 [D loss: 0.709731, acc.: 51.56%] [G loss: 0.775614]\n",
      "epoch:3 step:3558 [D loss: 0.690849, acc.: 46.09%] [G loss: 0.772534]\n",
      "epoch:3 step:3559 [D loss: 0.680271, acc.: 59.38%] [G loss: 0.843180]\n",
      "epoch:3 step:3560 [D loss: 0.700635, acc.: 53.12%] [G loss: 0.848025]\n",
      "epoch:3 step:3561 [D loss: 0.669059, acc.: 56.25%] [G loss: 0.840008]\n",
      "epoch:3 step:3562 [D loss: 0.695679, acc.: 50.78%] [G loss: 0.898839]\n",
      "epoch:3 step:3563 [D loss: 0.669645, acc.: 55.47%] [G loss: 0.784843]\n",
      "epoch:3 step:3564 [D loss: 0.699694, acc.: 49.22%] [G loss: 0.897928]\n",
      "epoch:3 step:3565 [D loss: 0.670142, acc.: 57.81%] [G loss: 0.817594]\n",
      "epoch:3 step:3566 [D loss: 0.675379, acc.: 54.69%] [G loss: 0.854899]\n",
      "epoch:3 step:3567 [D loss: 0.674374, acc.: 53.91%] [G loss: 0.812301]\n",
      "epoch:3 step:3568 [D loss: 0.707262, acc.: 50.78%] [G loss: 0.825363]\n",
      "epoch:3 step:3569 [D loss: 0.699652, acc.: 49.22%] [G loss: 0.793098]\n",
      "epoch:3 step:3570 [D loss: 0.681685, acc.: 53.12%] [G loss: 0.832418]\n",
      "epoch:3 step:3571 [D loss: 0.650766, acc.: 60.16%] [G loss: 0.834125]\n",
      "epoch:3 step:3572 [D loss: 0.680286, acc.: 53.91%] [G loss: 0.805986]\n",
      "epoch:3 step:3573 [D loss: 0.697831, acc.: 57.81%] [G loss: 0.818682]\n",
      "epoch:3 step:3574 [D loss: 0.697550, acc.: 57.81%] [G loss: 0.805516]\n",
      "epoch:3 step:3575 [D loss: 0.648085, acc.: 64.84%] [G loss: 0.742125]\n",
      "epoch:3 step:3576 [D loss: 0.672243, acc.: 56.25%] [G loss: 0.796152]\n",
      "epoch:3 step:3577 [D loss: 0.684750, acc.: 50.78%] [G loss: 0.792337]\n",
      "epoch:3 step:3578 [D loss: 0.684132, acc.: 53.12%] [G loss: 0.771007]\n",
      "epoch:3 step:3579 [D loss: 0.664324, acc.: 55.47%] [G loss: 0.792686]\n",
      "epoch:3 step:3580 [D loss: 0.685333, acc.: 51.56%] [G loss: 0.876557]\n",
      "epoch:3 step:3581 [D loss: 0.718163, acc.: 53.12%] [G loss: 0.799483]\n",
      "epoch:3 step:3582 [D loss: 0.707020, acc.: 55.47%] [G loss: 0.856723]\n",
      "epoch:3 step:3583 [D loss: 0.672307, acc.: 60.16%] [G loss: 0.772301]\n",
      "epoch:3 step:3584 [D loss: 0.694227, acc.: 58.59%] [G loss: 0.798391]\n",
      "epoch:3 step:3585 [D loss: 0.653469, acc.: 62.50%] [G loss: 0.817158]\n",
      "epoch:3 step:3586 [D loss: 0.679326, acc.: 54.69%] [G loss: 0.825529]\n",
      "epoch:3 step:3587 [D loss: 0.709399, acc.: 52.34%] [G loss: 0.803968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3588 [D loss: 0.720712, acc.: 53.12%] [G loss: 0.806049]\n",
      "epoch:3 step:3589 [D loss: 0.702015, acc.: 50.00%] [G loss: 0.818139]\n",
      "epoch:3 step:3590 [D loss: 0.701611, acc.: 50.78%] [G loss: 0.815559]\n",
      "epoch:3 step:3591 [D loss: 0.670781, acc.: 52.34%] [G loss: 0.787983]\n",
      "epoch:3 step:3592 [D loss: 0.690001, acc.: 55.47%] [G loss: 0.852423]\n",
      "epoch:3 step:3593 [D loss: 0.662870, acc.: 62.50%] [G loss: 0.830486]\n",
      "epoch:3 step:3594 [D loss: 0.661728, acc.: 59.38%] [G loss: 0.843970]\n",
      "epoch:3 step:3595 [D loss: 0.673998, acc.: 59.38%] [G loss: 0.820161]\n",
      "epoch:3 step:3596 [D loss: 0.680259, acc.: 57.81%] [G loss: 0.790734]\n",
      "epoch:3 step:3597 [D loss: 0.689265, acc.: 48.44%] [G loss: 0.843430]\n",
      "epoch:3 step:3598 [D loss: 0.658758, acc.: 60.16%] [G loss: 0.811181]\n",
      "epoch:3 step:3599 [D loss: 0.683970, acc.: 57.03%] [G loss: 0.781427]\n",
      "epoch:3 step:3600 [D loss: 0.707523, acc.: 51.56%] [G loss: 0.776690]\n",
      "epoch:3 step:3601 [D loss: 0.741526, acc.: 50.00%] [G loss: 0.924556]\n",
      "epoch:3 step:3602 [D loss: 0.674109, acc.: 53.12%] [G loss: 0.807267]\n",
      "epoch:3 step:3603 [D loss: 0.694438, acc.: 50.78%] [G loss: 0.755476]\n",
      "epoch:3 step:3604 [D loss: 0.680802, acc.: 56.25%] [G loss: 0.761014]\n",
      "epoch:3 step:3605 [D loss: 0.678756, acc.: 54.69%] [G loss: 0.813140]\n",
      "epoch:3 step:3606 [D loss: 0.700902, acc.: 50.78%] [G loss: 0.785850]\n",
      "epoch:3 step:3607 [D loss: 0.688041, acc.: 55.47%] [G loss: 0.816077]\n",
      "epoch:3 step:3608 [D loss: 0.659857, acc.: 59.38%] [G loss: 0.800447]\n",
      "epoch:3 step:3609 [D loss: 0.681491, acc.: 53.91%] [G loss: 0.854484]\n",
      "epoch:3 step:3610 [D loss: 0.676223, acc.: 55.47%] [G loss: 0.839153]\n",
      "epoch:3 step:3611 [D loss: 0.652503, acc.: 60.16%] [G loss: 0.862346]\n",
      "epoch:3 step:3612 [D loss: 0.666964, acc.: 55.47%] [G loss: 0.857163]\n",
      "epoch:3 step:3613 [D loss: 0.681242, acc.: 51.56%] [G loss: 0.859861]\n",
      "epoch:3 step:3614 [D loss: 0.657855, acc.: 60.94%] [G loss: 0.803774]\n",
      "epoch:3 step:3615 [D loss: 0.665057, acc.: 59.38%] [G loss: 0.845934]\n",
      "epoch:3 step:3616 [D loss: 0.678317, acc.: 57.81%] [G loss: 0.885966]\n",
      "epoch:3 step:3617 [D loss: 0.675204, acc.: 53.12%] [G loss: 0.806607]\n",
      "epoch:3 step:3618 [D loss: 0.696654, acc.: 50.00%] [G loss: 0.826346]\n",
      "epoch:3 step:3619 [D loss: 0.670190, acc.: 56.25%] [G loss: 0.799383]\n",
      "epoch:3 step:3620 [D loss: 0.652183, acc.: 61.72%] [G loss: 0.793617]\n",
      "epoch:3 step:3621 [D loss: 0.668380, acc.: 57.03%] [G loss: 0.852020]\n",
      "epoch:3 step:3622 [D loss: 0.675805, acc.: 54.69%] [G loss: 0.829732]\n",
      "epoch:3 step:3623 [D loss: 0.689480, acc.: 51.56%] [G loss: 0.820466]\n",
      "epoch:3 step:3624 [D loss: 0.715568, acc.: 54.69%] [G loss: 0.765685]\n",
      "epoch:3 step:3625 [D loss: 0.668683, acc.: 51.56%] [G loss: 0.773867]\n",
      "epoch:3 step:3626 [D loss: 0.687783, acc.: 49.22%] [G loss: 0.763748]\n",
      "epoch:3 step:3627 [D loss: 0.671678, acc.: 57.03%] [G loss: 0.723837]\n",
      "epoch:3 step:3628 [D loss: 0.755164, acc.: 39.84%] [G loss: 0.812506]\n",
      "epoch:3 step:3629 [D loss: 0.698235, acc.: 56.25%] [G loss: 0.789651]\n",
      "epoch:3 step:3630 [D loss: 0.736972, acc.: 47.66%] [G loss: 0.763254]\n",
      "epoch:3 step:3631 [D loss: 0.669841, acc.: 53.12%] [G loss: 0.902438]\n",
      "epoch:3 step:3632 [D loss: 0.697726, acc.: 55.47%] [G loss: 0.909685]\n",
      "epoch:3 step:3633 [D loss: 0.686282, acc.: 52.34%] [G loss: 0.918581]\n",
      "epoch:3 step:3634 [D loss: 0.646323, acc.: 59.38%] [G loss: 0.930270]\n",
      "epoch:3 step:3635 [D loss: 0.697720, acc.: 49.22%] [G loss: 0.895087]\n",
      "epoch:3 step:3636 [D loss: 0.695384, acc.: 51.56%] [G loss: 0.796539]\n",
      "epoch:3 step:3637 [D loss: 0.676585, acc.: 54.69%] [G loss: 0.865399]\n",
      "epoch:3 step:3638 [D loss: 0.651166, acc.: 64.84%] [G loss: 0.833822]\n",
      "epoch:3 step:3639 [D loss: 0.720741, acc.: 47.66%] [G loss: 0.736964]\n",
      "epoch:3 step:3640 [D loss: 0.697459, acc.: 51.56%] [G loss: 0.812107]\n",
      "epoch:3 step:3641 [D loss: 0.703466, acc.: 54.69%] [G loss: 0.830348]\n",
      "epoch:3 step:3642 [D loss: 0.661993, acc.: 56.25%] [G loss: 0.813235]\n",
      "epoch:3 step:3643 [D loss: 0.701264, acc.: 54.69%] [G loss: 0.840207]\n",
      "epoch:3 step:3644 [D loss: 0.692994, acc.: 57.03%] [G loss: 0.834889]\n",
      "epoch:3 step:3645 [D loss: 0.666530, acc.: 61.72%] [G loss: 0.820498]\n",
      "epoch:3 step:3646 [D loss: 0.703055, acc.: 46.88%] [G loss: 0.810908]\n",
      "epoch:3 step:3647 [D loss: 0.672580, acc.: 55.47%] [G loss: 0.803985]\n",
      "epoch:3 step:3648 [D loss: 0.697169, acc.: 47.66%] [G loss: 0.781646]\n",
      "epoch:3 step:3649 [D loss: 0.670766, acc.: 52.34%] [G loss: 0.847554]\n",
      "epoch:3 step:3650 [D loss: 0.707112, acc.: 48.44%] [G loss: 0.839645]\n",
      "epoch:3 step:3651 [D loss: 0.703036, acc.: 53.12%] [G loss: 0.752082]\n",
      "epoch:3 step:3652 [D loss: 0.686553, acc.: 53.12%] [G loss: 0.793393]\n",
      "epoch:3 step:3653 [D loss: 0.701634, acc.: 44.53%] [G loss: 0.751720]\n",
      "epoch:3 step:3654 [D loss: 0.723149, acc.: 42.97%] [G loss: 0.795173]\n",
      "epoch:3 step:3655 [D loss: 0.721197, acc.: 41.41%] [G loss: 0.736973]\n",
      "epoch:3 step:3656 [D loss: 0.674726, acc.: 49.22%] [G loss: 0.776131]\n",
      "epoch:3 step:3657 [D loss: 0.692026, acc.: 54.69%] [G loss: 0.764420]\n",
      "epoch:3 step:3658 [D loss: 0.665236, acc.: 60.16%] [G loss: 0.812765]\n",
      "epoch:3 step:3659 [D loss: 0.664550, acc.: 53.12%] [G loss: 0.793503]\n",
      "epoch:3 step:3660 [D loss: 0.691269, acc.: 51.56%] [G loss: 0.816261]\n",
      "epoch:3 step:3661 [D loss: 0.665962, acc.: 60.94%] [G loss: 0.832934]\n",
      "epoch:3 step:3662 [D loss: 0.694354, acc.: 53.12%] [G loss: 0.738533]\n",
      "epoch:3 step:3663 [D loss: 0.704126, acc.: 50.78%] [G loss: 0.796286]\n",
      "epoch:3 step:3664 [D loss: 0.690860, acc.: 49.22%] [G loss: 0.771412]\n",
      "epoch:3 step:3665 [D loss: 0.704432, acc.: 55.47%] [G loss: 0.811599]\n",
      "epoch:3 step:3666 [D loss: 0.711918, acc.: 47.66%] [G loss: 0.794314]\n",
      "epoch:3 step:3667 [D loss: 0.673023, acc.: 57.03%] [G loss: 0.816203]\n",
      "epoch:3 step:3668 [D loss: 0.702340, acc.: 50.78%] [G loss: 0.776719]\n",
      "epoch:3 step:3669 [D loss: 0.695556, acc.: 54.69%] [G loss: 0.774335]\n",
      "epoch:3 step:3670 [D loss: 0.679446, acc.: 58.59%] [G loss: 0.816506]\n",
      "epoch:3 step:3671 [D loss: 0.701306, acc.: 45.31%] [G loss: 0.794833]\n",
      "epoch:3 step:3672 [D loss: 0.669121, acc.: 64.84%] [G loss: 0.831625]\n",
      "epoch:3 step:3673 [D loss: 0.718929, acc.: 45.31%] [G loss: 0.758166]\n",
      "epoch:3 step:3674 [D loss: 0.681027, acc.: 57.03%] [G loss: 0.862593]\n",
      "epoch:3 step:3675 [D loss: 0.697529, acc.: 56.25%] [G loss: 0.806170]\n",
      "epoch:3 step:3676 [D loss: 0.664743, acc.: 60.16%] [G loss: 0.859190]\n",
      "epoch:3 step:3677 [D loss: 0.667370, acc.: 58.59%] [G loss: 0.847271]\n",
      "epoch:3 step:3678 [D loss: 0.680528, acc.: 59.38%] [G loss: 0.883665]\n",
      "epoch:3 step:3679 [D loss: 0.660706, acc.: 60.94%] [G loss: 0.862605]\n",
      "epoch:3 step:3680 [D loss: 0.713991, acc.: 45.31%] [G loss: 0.878022]\n",
      "epoch:3 step:3681 [D loss: 0.680370, acc.: 55.47%] [G loss: 0.810947]\n",
      "epoch:3 step:3682 [D loss: 0.686661, acc.: 57.81%] [G loss: 0.836516]\n",
      "epoch:3 step:3683 [D loss: 0.663870, acc.: 54.69%] [G loss: 0.797793]\n",
      "epoch:3 step:3684 [D loss: 0.690812, acc.: 50.78%] [G loss: 0.709980]\n",
      "epoch:3 step:3685 [D loss: 0.711089, acc.: 49.22%] [G loss: 0.730145]\n",
      "epoch:3 step:3686 [D loss: 0.703832, acc.: 52.34%] [G loss: 0.729363]\n",
      "epoch:3 step:3687 [D loss: 0.695839, acc.: 52.34%] [G loss: 0.786335]\n",
      "epoch:3 step:3688 [D loss: 0.702287, acc.: 51.56%] [G loss: 0.839041]\n",
      "epoch:3 step:3689 [D loss: 0.706357, acc.: 46.88%] [G loss: 0.812165]\n",
      "epoch:3 step:3690 [D loss: 0.640845, acc.: 59.38%] [G loss: 0.779045]\n",
      "epoch:3 step:3691 [D loss: 0.669926, acc.: 57.81%] [G loss: 0.835422]\n",
      "epoch:3 step:3692 [D loss: 0.657407, acc.: 60.16%] [G loss: 0.838670]\n",
      "epoch:3 step:3693 [D loss: 0.661555, acc.: 58.59%] [G loss: 0.776620]\n",
      "epoch:3 step:3694 [D loss: 0.683810, acc.: 50.00%] [G loss: 0.790980]\n",
      "epoch:3 step:3695 [D loss: 0.629390, acc.: 67.97%] [G loss: 0.855632]\n",
      "epoch:3 step:3696 [D loss: 0.690365, acc.: 50.78%] [G loss: 0.803410]\n",
      "epoch:3 step:3697 [D loss: 0.686337, acc.: 53.91%] [G loss: 0.794936]\n",
      "epoch:3 step:3698 [D loss: 0.677390, acc.: 56.25%] [G loss: 0.827742]\n",
      "epoch:3 step:3699 [D loss: 0.672365, acc.: 55.47%] [G loss: 0.806067]\n",
      "epoch:3 step:3700 [D loss: 0.667379, acc.: 60.16%] [G loss: 0.842648]\n",
      "epoch:3 step:3701 [D loss: 0.700747, acc.: 48.44%] [G loss: 0.802828]\n",
      "epoch:3 step:3702 [D loss: 0.711447, acc.: 49.22%] [G loss: 0.819603]\n",
      "epoch:3 step:3703 [D loss: 0.678640, acc.: 57.03%] [G loss: 0.831213]\n",
      "epoch:3 step:3704 [D loss: 0.702418, acc.: 48.44%] [G loss: 0.935108]\n",
      "epoch:3 step:3705 [D loss: 0.733791, acc.: 43.75%] [G loss: 0.811304]\n",
      "epoch:3 step:3706 [D loss: 0.708242, acc.: 55.47%] [G loss: 0.876884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3707 [D loss: 0.721859, acc.: 47.66%] [G loss: 0.790647]\n",
      "epoch:3 step:3708 [D loss: 0.696081, acc.: 49.22%] [G loss: 0.750269]\n",
      "epoch:3 step:3709 [D loss: 0.661065, acc.: 57.81%] [G loss: 0.798676]\n",
      "epoch:3 step:3710 [D loss: 0.669965, acc.: 55.47%] [G loss: 0.793717]\n",
      "epoch:3 step:3711 [D loss: 0.684617, acc.: 53.91%] [G loss: 0.770660]\n",
      "epoch:3 step:3712 [D loss: 0.680672, acc.: 53.91%] [G loss: 0.762661]\n",
      "epoch:3 step:3713 [D loss: 0.668056, acc.: 60.16%] [G loss: 0.890366]\n",
      "epoch:3 step:3714 [D loss: 0.706545, acc.: 43.75%] [G loss: 0.753378]\n",
      "epoch:3 step:3715 [D loss: 0.710045, acc.: 47.66%] [G loss: 0.751204]\n",
      "epoch:3 step:3716 [D loss: 0.698299, acc.: 52.34%] [G loss: 0.816933]\n",
      "epoch:3 step:3717 [D loss: 0.690571, acc.: 50.00%] [G loss: 0.806759]\n",
      "epoch:3 step:3718 [D loss: 0.667542, acc.: 54.69%] [G loss: 0.742166]\n",
      "epoch:3 step:3719 [D loss: 0.681275, acc.: 59.38%] [G loss: 0.821882]\n",
      "epoch:3 step:3720 [D loss: 0.664837, acc.: 59.38%] [G loss: 0.838179]\n",
      "epoch:3 step:3721 [D loss: 0.702842, acc.: 58.59%] [G loss: 0.799600]\n",
      "epoch:3 step:3722 [D loss: 0.689839, acc.: 51.56%] [G loss: 0.829472]\n",
      "epoch:3 step:3723 [D loss: 0.703003, acc.: 50.78%] [G loss: 0.856486]\n",
      "epoch:3 step:3724 [D loss: 0.657088, acc.: 67.19%] [G loss: 0.757636]\n",
      "epoch:3 step:3725 [D loss: 0.694412, acc.: 50.78%] [G loss: 0.833916]\n",
      "epoch:3 step:3726 [D loss: 0.705014, acc.: 49.22%] [G loss: 0.785627]\n",
      "epoch:3 step:3727 [D loss: 0.682008, acc.: 53.91%] [G loss: 0.886305]\n",
      "epoch:3 step:3728 [D loss: 0.671321, acc.: 60.94%] [G loss: 0.805462]\n",
      "epoch:3 step:3729 [D loss: 0.686885, acc.: 53.12%] [G loss: 0.846712]\n",
      "epoch:3 step:3730 [D loss: 0.721239, acc.: 43.75%] [G loss: 0.833318]\n",
      "epoch:3 step:3731 [D loss: 0.676073, acc.: 58.59%] [G loss: 0.808962]\n",
      "epoch:3 step:3732 [D loss: 0.745484, acc.: 46.09%] [G loss: 0.744080]\n",
      "epoch:3 step:3733 [D loss: 0.732976, acc.: 54.69%] [G loss: 0.799580]\n",
      "epoch:3 step:3734 [D loss: 0.697801, acc.: 56.25%] [G loss: 0.815826]\n",
      "epoch:3 step:3735 [D loss: 0.687280, acc.: 54.69%] [G loss: 0.850746]\n",
      "epoch:3 step:3736 [D loss: 0.679333, acc.: 58.59%] [G loss: 0.910818]\n",
      "epoch:3 step:3737 [D loss: 0.676532, acc.: 58.59%] [G loss: 0.866424]\n",
      "epoch:3 step:3738 [D loss: 0.697307, acc.: 53.12%] [G loss: 0.743977]\n",
      "epoch:3 step:3739 [D loss: 0.660363, acc.: 64.06%] [G loss: 0.816328]\n",
      "epoch:3 step:3740 [D loss: 0.686864, acc.: 53.91%] [G loss: 0.881651]\n",
      "epoch:3 step:3741 [D loss: 0.677267, acc.: 55.47%] [G loss: 0.887682]\n",
      "epoch:3 step:3742 [D loss: 0.710712, acc.: 50.00%] [G loss: 0.822037]\n",
      "epoch:3 step:3743 [D loss: 0.688743, acc.: 59.38%] [G loss: 0.879341]\n",
      "epoch:3 step:3744 [D loss: 0.721704, acc.: 52.34%] [G loss: 0.792195]\n",
      "epoch:3 step:3745 [D loss: 0.677919, acc.: 58.59%] [G loss: 0.736435]\n",
      "epoch:3 step:3746 [D loss: 0.706850, acc.: 44.53%] [G loss: 0.784269]\n",
      "epoch:3 step:3747 [D loss: 0.686628, acc.: 52.34%] [G loss: 0.795128]\n",
      "epoch:3 step:3748 [D loss: 0.698459, acc.: 50.00%] [G loss: 0.796949]\n",
      "epoch:4 step:3749 [D loss: 0.684858, acc.: 56.25%] [G loss: 0.833174]\n",
      "epoch:4 step:3750 [D loss: 0.678764, acc.: 60.16%] [G loss: 0.813174]\n",
      "epoch:4 step:3751 [D loss: 0.702910, acc.: 50.00%] [G loss: 0.865979]\n",
      "epoch:4 step:3752 [D loss: 0.687029, acc.: 54.69%] [G loss: 0.824986]\n",
      "epoch:4 step:3753 [D loss: 0.664692, acc.: 59.38%] [G loss: 0.802439]\n",
      "epoch:4 step:3754 [D loss: 0.650014, acc.: 63.28%] [G loss: 0.843856]\n",
      "epoch:4 step:3755 [D loss: 0.692892, acc.: 52.34%] [G loss: 0.837728]\n",
      "epoch:4 step:3756 [D loss: 0.672161, acc.: 57.03%] [G loss: 0.847202]\n",
      "epoch:4 step:3757 [D loss: 0.712046, acc.: 51.56%] [G loss: 0.835658]\n",
      "epoch:4 step:3758 [D loss: 0.672131, acc.: 60.94%] [G loss: 0.805860]\n",
      "epoch:4 step:3759 [D loss: 0.655674, acc.: 61.72%] [G loss: 0.779668]\n",
      "epoch:4 step:3760 [D loss: 0.694385, acc.: 53.12%] [G loss: 0.786677]\n",
      "epoch:4 step:3761 [D loss: 0.708942, acc.: 53.91%] [G loss: 0.769181]\n",
      "epoch:4 step:3762 [D loss: 0.685337, acc.: 51.56%] [G loss: 0.752987]\n",
      "epoch:4 step:3763 [D loss: 0.658806, acc.: 64.84%] [G loss: 0.847624]\n",
      "epoch:4 step:3764 [D loss: 0.685054, acc.: 52.34%] [G loss: 0.833769]\n",
      "epoch:4 step:3765 [D loss: 0.655557, acc.: 62.50%] [G loss: 0.866343]\n",
      "epoch:4 step:3766 [D loss: 0.679970, acc.: 57.81%] [G loss: 0.790635]\n",
      "epoch:4 step:3767 [D loss: 0.675874, acc.: 59.38%] [G loss: 0.837980]\n",
      "epoch:4 step:3768 [D loss: 0.667127, acc.: 60.94%] [G loss: 0.789728]\n",
      "epoch:4 step:3769 [D loss: 0.665780, acc.: 60.16%] [G loss: 0.799552]\n",
      "epoch:4 step:3770 [D loss: 0.683975, acc.: 53.91%] [G loss: 0.910516]\n",
      "epoch:4 step:3771 [D loss: 0.672594, acc.: 53.91%] [G loss: 0.852899]\n",
      "epoch:4 step:3772 [D loss: 0.670417, acc.: 60.16%] [G loss: 0.909544]\n",
      "epoch:4 step:3773 [D loss: 0.687011, acc.: 58.59%] [G loss: 0.902918]\n",
      "epoch:4 step:3774 [D loss: 0.665526, acc.: 58.59%] [G loss: 0.824046]\n",
      "epoch:4 step:3775 [D loss: 0.713096, acc.: 54.69%] [G loss: 0.828117]\n",
      "epoch:4 step:3776 [D loss: 0.718006, acc.: 48.44%] [G loss: 0.799841]\n",
      "epoch:4 step:3777 [D loss: 0.664283, acc.: 60.94%] [G loss: 0.825650]\n",
      "epoch:4 step:3778 [D loss: 0.656771, acc.: 59.38%] [G loss: 0.793344]\n",
      "epoch:4 step:3779 [D loss: 0.690197, acc.: 50.00%] [G loss: 0.854534]\n",
      "epoch:4 step:3780 [D loss: 0.693751, acc.: 56.25%] [G loss: 0.837005]\n",
      "epoch:4 step:3781 [D loss: 0.637840, acc.: 69.53%] [G loss: 0.828594]\n",
      "epoch:4 step:3782 [D loss: 0.687362, acc.: 50.00%] [G loss: 0.845628]\n",
      "epoch:4 step:3783 [D loss: 0.721486, acc.: 46.09%] [G loss: 0.811943]\n",
      "epoch:4 step:3784 [D loss: 0.676708, acc.: 55.47%] [G loss: 0.813968]\n",
      "epoch:4 step:3785 [D loss: 0.667038, acc.: 57.81%] [G loss: 0.863065]\n",
      "epoch:4 step:3786 [D loss: 0.693011, acc.: 53.12%] [G loss: 0.915916]\n",
      "epoch:4 step:3787 [D loss: 0.677721, acc.: 55.47%] [G loss: 0.897506]\n",
      "epoch:4 step:3788 [D loss: 0.670967, acc.: 58.59%] [G loss: 0.893424]\n",
      "epoch:4 step:3789 [D loss: 0.640085, acc.: 61.72%] [G loss: 0.836195]\n",
      "epoch:4 step:3790 [D loss: 0.674716, acc.: 60.94%] [G loss: 0.828745]\n",
      "epoch:4 step:3791 [D loss: 0.658208, acc.: 58.59%] [G loss: 0.897348]\n",
      "epoch:4 step:3792 [D loss: 0.635115, acc.: 67.19%] [G loss: 0.834879]\n",
      "epoch:4 step:3793 [D loss: 0.718526, acc.: 48.44%] [G loss: 0.795302]\n",
      "epoch:4 step:3794 [D loss: 0.696180, acc.: 56.25%] [G loss: 0.825845]\n",
      "epoch:4 step:3795 [D loss: 0.719217, acc.: 46.88%] [G loss: 0.768013]\n",
      "epoch:4 step:3796 [D loss: 0.684546, acc.: 53.91%] [G loss: 0.749436]\n",
      "epoch:4 step:3797 [D loss: 0.700607, acc.: 49.22%] [G loss: 0.754638]\n",
      "epoch:4 step:3798 [D loss: 0.680771, acc.: 50.00%] [G loss: 0.810600]\n",
      "epoch:4 step:3799 [D loss: 0.671502, acc.: 55.47%] [G loss: 0.832290]\n",
      "epoch:4 step:3800 [D loss: 0.651609, acc.: 67.97%] [G loss: 0.835603]\n",
      "epoch:4 step:3801 [D loss: 0.629875, acc.: 64.84%] [G loss: 0.785559]\n",
      "epoch:4 step:3802 [D loss: 0.700748, acc.: 52.34%] [G loss: 0.871892]\n",
      "epoch:4 step:3803 [D loss: 0.686927, acc.: 49.22%] [G loss: 0.855981]\n",
      "epoch:4 step:3804 [D loss: 0.684990, acc.: 58.59%] [G loss: 0.779993]\n",
      "epoch:4 step:3805 [D loss: 0.691221, acc.: 48.44%] [G loss: 0.822521]\n",
      "epoch:4 step:3806 [D loss: 0.673046, acc.: 65.62%] [G loss: 0.759340]\n",
      "epoch:4 step:3807 [D loss: 0.687983, acc.: 53.12%] [G loss: 0.857550]\n",
      "epoch:4 step:3808 [D loss: 0.683716, acc.: 55.47%] [G loss: 0.824178]\n",
      "epoch:4 step:3809 [D loss: 0.662395, acc.: 61.72%] [G loss: 0.926525]\n",
      "epoch:4 step:3810 [D loss: 0.703308, acc.: 49.22%] [G loss: 0.892373]\n",
      "epoch:4 step:3811 [D loss: 0.682001, acc.: 53.12%] [G loss: 0.804856]\n",
      "epoch:4 step:3812 [D loss: 0.707230, acc.: 52.34%] [G loss: 0.815453]\n",
      "epoch:4 step:3813 [D loss: 0.674915, acc.: 58.59%] [G loss: 0.817999]\n",
      "epoch:4 step:3814 [D loss: 0.676304, acc.: 57.81%] [G loss: 0.811400]\n",
      "epoch:4 step:3815 [D loss: 0.667524, acc.: 55.47%] [G loss: 0.838423]\n",
      "epoch:4 step:3816 [D loss: 0.686796, acc.: 53.91%] [G loss: 0.791375]\n",
      "epoch:4 step:3817 [D loss: 0.669240, acc.: 53.12%] [G loss: 0.854530]\n",
      "epoch:4 step:3818 [D loss: 0.676260, acc.: 53.12%] [G loss: 0.829523]\n",
      "epoch:4 step:3819 [D loss: 0.698848, acc.: 53.12%] [G loss: 0.846788]\n",
      "epoch:4 step:3820 [D loss: 0.640615, acc.: 63.28%] [G loss: 0.859496]\n",
      "epoch:4 step:3821 [D loss: 0.662713, acc.: 57.81%] [G loss: 0.849444]\n",
      "epoch:4 step:3822 [D loss: 0.691313, acc.: 48.44%] [G loss: 0.808623]\n",
      "epoch:4 step:3823 [D loss: 0.686603, acc.: 59.38%] [G loss: 0.760751]\n",
      "epoch:4 step:3824 [D loss: 0.673021, acc.: 53.91%] [G loss: 0.794374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3825 [D loss: 0.676100, acc.: 60.16%] [G loss: 0.826473]\n",
      "epoch:4 step:3826 [D loss: 0.682091, acc.: 55.47%] [G loss: 0.809356]\n",
      "epoch:4 step:3827 [D loss: 0.655653, acc.: 59.38%] [G loss: 0.880580]\n",
      "epoch:4 step:3828 [D loss: 0.697986, acc.: 59.38%] [G loss: 0.859769]\n",
      "epoch:4 step:3829 [D loss: 0.714146, acc.: 53.91%] [G loss: 0.964924]\n",
      "epoch:4 step:3830 [D loss: 0.689796, acc.: 53.91%] [G loss: 0.771906]\n",
      "epoch:4 step:3831 [D loss: 0.677520, acc.: 60.94%] [G loss: 0.829345]\n",
      "epoch:4 step:3832 [D loss: 0.705097, acc.: 53.12%] [G loss: 0.821069]\n",
      "epoch:4 step:3833 [D loss: 0.659954, acc.: 64.84%] [G loss: 0.824886]\n",
      "epoch:4 step:3834 [D loss: 0.660709, acc.: 58.59%] [G loss: 0.766124]\n",
      "epoch:4 step:3835 [D loss: 0.706311, acc.: 50.00%] [G loss: 0.792138]\n",
      "epoch:4 step:3836 [D loss: 0.700956, acc.: 57.81%] [G loss: 0.866984]\n",
      "epoch:4 step:3837 [D loss: 0.698697, acc.: 51.56%] [G loss: 0.893051]\n",
      "epoch:4 step:3838 [D loss: 0.679850, acc.: 56.25%] [G loss: 0.831843]\n",
      "epoch:4 step:3839 [D loss: 0.654597, acc.: 59.38%] [G loss: 0.848685]\n",
      "epoch:4 step:3840 [D loss: 0.690164, acc.: 53.91%] [G loss: 0.782539]\n",
      "epoch:4 step:3841 [D loss: 0.670411, acc.: 61.72%] [G loss: 0.815686]\n",
      "epoch:4 step:3842 [D loss: 0.716829, acc.: 52.34%] [G loss: 0.840916]\n",
      "epoch:4 step:3843 [D loss: 0.730851, acc.: 46.88%] [G loss: 0.813274]\n",
      "epoch:4 step:3844 [D loss: 0.730984, acc.: 46.09%] [G loss: 0.784382]\n",
      "epoch:4 step:3845 [D loss: 0.684544, acc.: 53.12%] [G loss: 0.782942]\n",
      "epoch:4 step:3846 [D loss: 0.715418, acc.: 52.34%] [G loss: 0.833778]\n",
      "epoch:4 step:3847 [D loss: 0.688862, acc.: 53.12%] [G loss: 0.912888]\n",
      "epoch:4 step:3848 [D loss: 0.675637, acc.: 54.69%] [G loss: 0.799649]\n",
      "epoch:4 step:3849 [D loss: 0.680422, acc.: 52.34%] [G loss: 0.830450]\n",
      "epoch:4 step:3850 [D loss: 0.700484, acc.: 53.12%] [G loss: 0.824680]\n",
      "epoch:4 step:3851 [D loss: 0.680505, acc.: 53.12%] [G loss: 0.854244]\n",
      "epoch:4 step:3852 [D loss: 0.696519, acc.: 53.91%] [G loss: 0.767335]\n",
      "epoch:4 step:3853 [D loss: 0.669150, acc.: 53.91%] [G loss: 0.814280]\n",
      "epoch:4 step:3854 [D loss: 0.691577, acc.: 56.25%] [G loss: 0.869739]\n",
      "epoch:4 step:3855 [D loss: 0.674076, acc.: 60.16%] [G loss: 0.951407]\n",
      "epoch:4 step:3856 [D loss: 0.723133, acc.: 50.78%] [G loss: 0.886103]\n",
      "epoch:4 step:3857 [D loss: 0.694373, acc.: 53.12%] [G loss: 0.829945]\n",
      "epoch:4 step:3858 [D loss: 0.680975, acc.: 55.47%] [G loss: 0.837859]\n",
      "epoch:4 step:3859 [D loss: 0.673989, acc.: 53.12%] [G loss: 0.809466]\n",
      "epoch:4 step:3860 [D loss: 0.663985, acc.: 52.34%] [G loss: 0.875456]\n",
      "epoch:4 step:3861 [D loss: 0.669955, acc.: 60.94%] [G loss: 0.818756]\n",
      "epoch:4 step:3862 [D loss: 0.673741, acc.: 57.03%] [G loss: 0.866277]\n",
      "epoch:4 step:3863 [D loss: 0.751804, acc.: 50.00%] [G loss: 0.771837]\n",
      "epoch:4 step:3864 [D loss: 0.694153, acc.: 54.69%] [G loss: 0.895376]\n",
      "epoch:4 step:3865 [D loss: 0.669864, acc.: 59.38%] [G loss: 0.802034]\n",
      "epoch:4 step:3866 [D loss: 0.678622, acc.: 60.94%] [G loss: 0.872439]\n",
      "epoch:4 step:3867 [D loss: 0.652349, acc.: 57.81%] [G loss: 0.818686]\n",
      "epoch:4 step:3868 [D loss: 0.692497, acc.: 58.59%] [G loss: 0.772266]\n",
      "epoch:4 step:3869 [D loss: 0.688206, acc.: 48.44%] [G loss: 0.808585]\n",
      "epoch:4 step:3870 [D loss: 0.678047, acc.: 56.25%] [G loss: 0.801688]\n",
      "epoch:4 step:3871 [D loss: 0.684884, acc.: 50.78%] [G loss: 0.787974]\n",
      "epoch:4 step:3872 [D loss: 0.697405, acc.: 51.56%] [G loss: 0.814248]\n",
      "epoch:4 step:3873 [D loss: 0.701089, acc.: 53.91%] [G loss: 0.769656]\n",
      "epoch:4 step:3874 [D loss: 0.740637, acc.: 39.06%] [G loss: 0.822951]\n",
      "epoch:4 step:3875 [D loss: 0.702476, acc.: 50.78%] [G loss: 0.886350]\n",
      "epoch:4 step:3876 [D loss: 0.712600, acc.: 51.56%] [G loss: 0.794107]\n",
      "epoch:4 step:3877 [D loss: 0.665877, acc.: 60.16%] [G loss: 0.812919]\n",
      "epoch:4 step:3878 [D loss: 0.667846, acc.: 61.72%] [G loss: 0.885167]\n",
      "epoch:4 step:3879 [D loss: 0.653240, acc.: 63.28%] [G loss: 0.877681]\n",
      "epoch:4 step:3880 [D loss: 0.666795, acc.: 57.81%] [G loss: 0.790788]\n",
      "epoch:4 step:3881 [D loss: 0.708201, acc.: 50.00%] [G loss: 0.793330]\n",
      "epoch:4 step:3882 [D loss: 0.698663, acc.: 46.88%] [G loss: 0.719545]\n",
      "epoch:4 step:3883 [D loss: 0.699782, acc.: 49.22%] [G loss: 0.737725]\n",
      "epoch:4 step:3884 [D loss: 0.717511, acc.: 46.88%] [G loss: 0.799765]\n",
      "epoch:4 step:3885 [D loss: 0.682354, acc.: 53.12%] [G loss: 0.800759]\n",
      "epoch:4 step:3886 [D loss: 0.684475, acc.: 55.47%] [G loss: 0.744097]\n",
      "epoch:4 step:3887 [D loss: 0.724083, acc.: 50.00%] [G loss: 0.773314]\n",
      "epoch:4 step:3888 [D loss: 0.657159, acc.: 64.84%] [G loss: 0.797335]\n",
      "epoch:4 step:3889 [D loss: 0.700741, acc.: 55.47%] [G loss: 0.813046]\n",
      "epoch:4 step:3890 [D loss: 0.694055, acc.: 49.22%] [G loss: 0.798825]\n",
      "epoch:4 step:3891 [D loss: 0.665957, acc.: 60.16%] [G loss: 0.825517]\n",
      "epoch:4 step:3892 [D loss: 0.646906, acc.: 64.84%] [G loss: 0.861150]\n",
      "epoch:4 step:3893 [D loss: 0.660538, acc.: 57.81%] [G loss: 0.793524]\n",
      "epoch:4 step:3894 [D loss: 0.697420, acc.: 46.88%] [G loss: 0.872207]\n",
      "epoch:4 step:3895 [D loss: 0.670107, acc.: 53.12%] [G loss: 0.880744]\n",
      "epoch:4 step:3896 [D loss: 0.655992, acc.: 63.28%] [G loss: 0.819754]\n",
      "epoch:4 step:3897 [D loss: 0.685573, acc.: 56.25%] [G loss: 0.878058]\n",
      "epoch:4 step:3898 [D loss: 0.677569, acc.: 54.69%] [G loss: 0.852277]\n",
      "epoch:4 step:3899 [D loss: 0.703077, acc.: 51.56%] [G loss: 0.788355]\n",
      "epoch:4 step:3900 [D loss: 0.671854, acc.: 57.81%] [G loss: 0.834598]\n",
      "epoch:4 step:3901 [D loss: 0.675002, acc.: 50.78%] [G loss: 0.779094]\n",
      "epoch:4 step:3902 [D loss: 0.689227, acc.: 46.09%] [G loss: 0.869719]\n",
      "epoch:4 step:3903 [D loss: 0.689939, acc.: 50.78%] [G loss: 0.818224]\n",
      "epoch:4 step:3904 [D loss: 0.665006, acc.: 60.94%] [G loss: 0.875983]\n",
      "epoch:4 step:3905 [D loss: 0.684063, acc.: 53.12%] [G loss: 0.875390]\n",
      "epoch:4 step:3906 [D loss: 0.686994, acc.: 48.44%] [G loss: 0.794991]\n",
      "epoch:4 step:3907 [D loss: 0.707641, acc.: 46.09%] [G loss: 0.805119]\n",
      "epoch:4 step:3908 [D loss: 0.684805, acc.: 57.03%] [G loss: 0.796895]\n",
      "epoch:4 step:3909 [D loss: 0.687897, acc.: 56.25%] [G loss: 0.804075]\n",
      "epoch:4 step:3910 [D loss: 0.705842, acc.: 50.78%] [G loss: 0.813920]\n",
      "epoch:4 step:3911 [D loss: 0.683994, acc.: 55.47%] [G loss: 0.777882]\n",
      "epoch:4 step:3912 [D loss: 0.720991, acc.: 54.69%] [G loss: 0.782402]\n",
      "epoch:4 step:3913 [D loss: 0.662426, acc.: 63.28%] [G loss: 0.769857]\n",
      "epoch:4 step:3914 [D loss: 0.659074, acc.: 57.81%] [G loss: 0.817309]\n",
      "epoch:4 step:3915 [D loss: 0.631805, acc.: 67.97%] [G loss: 0.784614]\n",
      "epoch:4 step:3916 [D loss: 0.714206, acc.: 50.00%] [G loss: 0.753089]\n",
      "epoch:4 step:3917 [D loss: 0.679593, acc.: 52.34%] [G loss: 0.778361]\n",
      "epoch:4 step:3918 [D loss: 0.656740, acc.: 55.47%] [G loss: 0.829289]\n",
      "epoch:4 step:3919 [D loss: 0.732216, acc.: 42.97%] [G loss: 0.863076]\n",
      "epoch:4 step:3920 [D loss: 0.713573, acc.: 46.88%] [G loss: 0.822151]\n",
      "epoch:4 step:3921 [D loss: 0.686079, acc.: 46.09%] [G loss: 0.755806]\n",
      "epoch:4 step:3922 [D loss: 0.645377, acc.: 61.72%] [G loss: 0.829427]\n",
      "epoch:4 step:3923 [D loss: 0.696948, acc.: 53.12%] [G loss: 0.763400]\n",
      "epoch:4 step:3924 [D loss: 0.708677, acc.: 42.19%] [G loss: 0.770360]\n",
      "epoch:4 step:3925 [D loss: 0.655129, acc.: 54.69%] [G loss: 0.778689]\n",
      "epoch:4 step:3926 [D loss: 0.673288, acc.: 57.03%] [G loss: 0.836635]\n",
      "epoch:4 step:3927 [D loss: 0.657603, acc.: 57.81%] [G loss: 0.897567]\n",
      "epoch:4 step:3928 [D loss: 0.656284, acc.: 60.94%] [G loss: 0.857102]\n",
      "epoch:4 step:3929 [D loss: 0.698439, acc.: 50.00%] [G loss: 0.960722]\n",
      "epoch:4 step:3930 [D loss: 0.645081, acc.: 69.53%] [G loss: 0.861889]\n",
      "epoch:4 step:3931 [D loss: 0.674875, acc.: 56.25%] [G loss: 0.863660]\n",
      "epoch:4 step:3932 [D loss: 0.670820, acc.: 60.94%] [G loss: 0.851444]\n",
      "epoch:4 step:3933 [D loss: 0.719833, acc.: 53.12%] [G loss: 0.855560]\n",
      "epoch:4 step:3934 [D loss: 0.695853, acc.: 50.78%] [G loss: 0.804655]\n",
      "epoch:4 step:3935 [D loss: 0.694913, acc.: 52.34%] [G loss: 0.827718]\n",
      "epoch:4 step:3936 [D loss: 0.724963, acc.: 44.53%] [G loss: 0.837436]\n",
      "epoch:4 step:3937 [D loss: 0.689590, acc.: 56.25%] [G loss: 0.828279]\n",
      "epoch:4 step:3938 [D loss: 0.690776, acc.: 45.31%] [G loss: 0.886870]\n",
      "epoch:4 step:3939 [D loss: 0.696938, acc.: 58.59%] [G loss: 0.839764]\n",
      "epoch:4 step:3940 [D loss: 0.699069, acc.: 50.00%] [G loss: 0.754968]\n",
      "epoch:4 step:3941 [D loss: 0.672669, acc.: 55.47%] [G loss: 0.791816]\n",
      "epoch:4 step:3942 [D loss: 0.687717, acc.: 49.22%] [G loss: 0.839984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3943 [D loss: 0.674032, acc.: 60.16%] [G loss: 0.822171]\n",
      "epoch:4 step:3944 [D loss: 0.680573, acc.: 51.56%] [G loss: 0.848620]\n",
      "epoch:4 step:3945 [D loss: 0.659425, acc.: 56.25%] [G loss: 0.783914]\n",
      "epoch:4 step:3946 [D loss: 0.708731, acc.: 52.34%] [G loss: 0.770048]\n",
      "epoch:4 step:3947 [D loss: 0.657767, acc.: 59.38%] [G loss: 0.787789]\n",
      "epoch:4 step:3948 [D loss: 0.686123, acc.: 53.12%] [G loss: 0.730958]\n",
      "epoch:4 step:3949 [D loss: 0.700300, acc.: 47.66%] [G loss: 0.818811]\n",
      "epoch:4 step:3950 [D loss: 0.681938, acc.: 59.38%] [G loss: 0.790112]\n",
      "epoch:4 step:3951 [D loss: 0.655169, acc.: 57.81%] [G loss: 0.819237]\n",
      "epoch:4 step:3952 [D loss: 0.675256, acc.: 59.38%] [G loss: 0.797285]\n",
      "epoch:4 step:3953 [D loss: 0.683521, acc.: 53.91%] [G loss: 0.831833]\n",
      "epoch:4 step:3954 [D loss: 0.671571, acc.: 56.25%] [G loss: 0.797431]\n",
      "epoch:4 step:3955 [D loss: 0.668697, acc.: 58.59%] [G loss: 0.830221]\n",
      "epoch:4 step:3956 [D loss: 0.693181, acc.: 59.38%] [G loss: 0.909346]\n",
      "epoch:4 step:3957 [D loss: 0.666637, acc.: 55.47%] [G loss: 0.853640]\n",
      "epoch:4 step:3958 [D loss: 0.673176, acc.: 58.59%] [G loss: 0.831088]\n",
      "epoch:4 step:3959 [D loss: 0.664076, acc.: 65.62%] [G loss: 0.877165]\n",
      "epoch:4 step:3960 [D loss: 0.700534, acc.: 47.66%] [G loss: 0.902366]\n",
      "epoch:4 step:3961 [D loss: 0.714401, acc.: 53.91%] [G loss: 0.864965]\n",
      "epoch:4 step:3962 [D loss: 0.713051, acc.: 50.00%] [G loss: 0.828601]\n",
      "epoch:4 step:3963 [D loss: 0.678089, acc.: 58.59%] [G loss: 0.831071]\n",
      "epoch:4 step:3964 [D loss: 0.676361, acc.: 51.56%] [G loss: 0.790166]\n",
      "epoch:4 step:3965 [D loss: 0.727788, acc.: 50.78%] [G loss: 0.859762]\n",
      "epoch:4 step:3966 [D loss: 0.699776, acc.: 52.34%] [G loss: 0.799312]\n",
      "epoch:4 step:3967 [D loss: 0.658781, acc.: 59.38%] [G loss: 0.790567]\n",
      "epoch:4 step:3968 [D loss: 0.707921, acc.: 50.00%] [G loss: 0.800679]\n",
      "epoch:4 step:3969 [D loss: 0.688196, acc.: 53.12%] [G loss: 0.781071]\n",
      "epoch:4 step:3970 [D loss: 0.666558, acc.: 58.59%] [G loss: 0.791446]\n",
      "epoch:4 step:3971 [D loss: 0.736291, acc.: 48.44%] [G loss: 0.786702]\n",
      "epoch:4 step:3972 [D loss: 0.687831, acc.: 57.03%] [G loss: 0.792579]\n",
      "epoch:4 step:3973 [D loss: 0.689700, acc.: 57.81%] [G loss: 0.848244]\n",
      "epoch:4 step:3974 [D loss: 0.706438, acc.: 54.69%] [G loss: 0.797118]\n",
      "epoch:4 step:3975 [D loss: 0.696405, acc.: 52.34%] [G loss: 0.803919]\n",
      "epoch:4 step:3976 [D loss: 0.699652, acc.: 50.78%] [G loss: 0.803085]\n",
      "epoch:4 step:3977 [D loss: 0.716392, acc.: 49.22%] [G loss: 0.802946]\n",
      "epoch:4 step:3978 [D loss: 0.674398, acc.: 53.91%] [G loss: 0.770563]\n",
      "epoch:4 step:3979 [D loss: 0.730310, acc.: 42.19%] [G loss: 0.721268]\n",
      "epoch:4 step:3980 [D loss: 0.695665, acc.: 53.12%] [G loss: 0.755075]\n",
      "epoch:4 step:3981 [D loss: 0.682033, acc.: 54.69%] [G loss: 0.809108]\n",
      "epoch:4 step:3982 [D loss: 0.676962, acc.: 53.91%] [G loss: 0.813765]\n",
      "epoch:4 step:3983 [D loss: 0.689318, acc.: 56.25%] [G loss: 0.817801]\n",
      "epoch:4 step:3984 [D loss: 0.679384, acc.: 60.16%] [G loss: 0.812228]\n",
      "epoch:4 step:3985 [D loss: 0.657193, acc.: 58.59%] [G loss: 0.777518]\n",
      "epoch:4 step:3986 [D loss: 0.661899, acc.: 56.25%] [G loss: 0.806469]\n",
      "epoch:4 step:3987 [D loss: 0.700067, acc.: 46.09%] [G loss: 0.775008]\n",
      "epoch:4 step:3988 [D loss: 0.674328, acc.: 55.47%] [G loss: 0.762318]\n",
      "epoch:4 step:3989 [D loss: 0.696840, acc.: 51.56%] [G loss: 0.828777]\n",
      "epoch:4 step:3990 [D loss: 0.673684, acc.: 60.94%] [G loss: 0.781860]\n",
      "epoch:4 step:3991 [D loss: 0.663034, acc.: 56.25%] [G loss: 0.814544]\n",
      "epoch:4 step:3992 [D loss: 0.670404, acc.: 64.84%] [G loss: 0.769605]\n",
      "epoch:4 step:3993 [D loss: 0.705923, acc.: 47.66%] [G loss: 0.786999]\n",
      "epoch:4 step:3994 [D loss: 0.691054, acc.: 53.91%] [G loss: 0.795892]\n",
      "epoch:4 step:3995 [D loss: 0.652721, acc.: 60.94%] [G loss: 0.794417]\n",
      "epoch:4 step:3996 [D loss: 0.683935, acc.: 58.59%] [G loss: 0.796931]\n",
      "epoch:4 step:3997 [D loss: 0.664834, acc.: 57.81%] [G loss: 0.864107]\n",
      "epoch:4 step:3998 [D loss: 0.670023, acc.: 56.25%] [G loss: 0.787674]\n",
      "epoch:4 step:3999 [D loss: 0.677370, acc.: 56.25%] [G loss: 0.861684]\n",
      "epoch:4 step:4000 [D loss: 0.645164, acc.: 64.06%] [G loss: 0.839247]\n",
      "epoch:4 step:4001 [D loss: 0.684067, acc.: 56.25%] [G loss: 0.820374]\n",
      "epoch:4 step:4002 [D loss: 0.692921, acc.: 52.34%] [G loss: 0.778980]\n",
      "epoch:4 step:4003 [D loss: 0.708426, acc.: 51.56%] [G loss: 0.810089]\n",
      "epoch:4 step:4004 [D loss: 0.680017, acc.: 53.91%] [G loss: 0.796651]\n",
      "epoch:4 step:4005 [D loss: 0.679013, acc.: 50.00%] [G loss: 0.825063]\n",
      "epoch:4 step:4006 [D loss: 0.700350, acc.: 49.22%] [G loss: 0.848192]\n",
      "epoch:4 step:4007 [D loss: 0.688318, acc.: 58.59%] [G loss: 0.818337]\n",
      "epoch:4 step:4008 [D loss: 0.639464, acc.: 62.50%] [G loss: 0.896170]\n",
      "epoch:4 step:4009 [D loss: 0.697913, acc.: 52.34%] [G loss: 0.824747]\n",
      "epoch:4 step:4010 [D loss: 0.742013, acc.: 40.62%] [G loss: 0.831104]\n",
      "epoch:4 step:4011 [D loss: 0.669209, acc.: 58.59%] [G loss: 0.827720]\n",
      "epoch:4 step:4012 [D loss: 0.700591, acc.: 50.78%] [G loss: 0.820960]\n",
      "epoch:4 step:4013 [D loss: 0.678405, acc.: 59.38%] [G loss: 0.835610]\n",
      "epoch:4 step:4014 [D loss: 0.627979, acc.: 66.41%] [G loss: 0.898930]\n",
      "epoch:4 step:4015 [D loss: 0.674894, acc.: 55.47%] [G loss: 0.819891]\n",
      "epoch:4 step:4016 [D loss: 0.665781, acc.: 58.59%] [G loss: 0.794945]\n",
      "epoch:4 step:4017 [D loss: 0.658227, acc.: 60.94%] [G loss: 0.869617]\n",
      "epoch:4 step:4018 [D loss: 0.670369, acc.: 52.34%] [G loss: 0.792840]\n",
      "epoch:4 step:4019 [D loss: 0.666789, acc.: 58.59%] [G loss: 0.803309]\n",
      "epoch:4 step:4020 [D loss: 0.667604, acc.: 57.03%] [G loss: 0.813688]\n",
      "epoch:4 step:4021 [D loss: 0.656836, acc.: 60.94%] [G loss: 0.804191]\n",
      "epoch:4 step:4022 [D loss: 0.653005, acc.: 61.72%] [G loss: 0.895832]\n",
      "epoch:4 step:4023 [D loss: 0.686536, acc.: 55.47%] [G loss: 0.805933]\n",
      "epoch:4 step:4024 [D loss: 0.707201, acc.: 44.53%] [G loss: 0.800100]\n",
      "epoch:4 step:4025 [D loss: 0.714507, acc.: 51.56%] [G loss: 0.779854]\n",
      "epoch:4 step:4026 [D loss: 0.700670, acc.: 50.78%] [G loss: 0.743924]\n",
      "epoch:4 step:4027 [D loss: 0.700533, acc.: 47.66%] [G loss: 0.796703]\n",
      "epoch:4 step:4028 [D loss: 0.669373, acc.: 55.47%] [G loss: 0.829831]\n",
      "epoch:4 step:4029 [D loss: 0.752606, acc.: 44.53%] [G loss: 0.832144]\n",
      "epoch:4 step:4030 [D loss: 0.676501, acc.: 56.25%] [G loss: 0.902431]\n",
      "epoch:4 step:4031 [D loss: 0.651782, acc.: 62.50%] [G loss: 0.811154]\n",
      "epoch:4 step:4032 [D loss: 0.674052, acc.: 55.47%] [G loss: 0.900859]\n",
      "epoch:4 step:4033 [D loss: 0.624273, acc.: 67.19%] [G loss: 0.853398]\n",
      "epoch:4 step:4034 [D loss: 0.715262, acc.: 51.56%] [G loss: 0.836476]\n",
      "epoch:4 step:4035 [D loss: 0.667367, acc.: 58.59%] [G loss: 0.799067]\n",
      "epoch:4 step:4036 [D loss: 0.691794, acc.: 55.47%] [G loss: 0.760688]\n",
      "epoch:4 step:4037 [D loss: 0.797429, acc.: 38.28%] [G loss: 0.793941]\n",
      "epoch:4 step:4038 [D loss: 0.678156, acc.: 58.59%] [G loss: 0.855941]\n",
      "epoch:4 step:4039 [D loss: 0.697437, acc.: 55.47%] [G loss: 0.826219]\n",
      "epoch:4 step:4040 [D loss: 0.701068, acc.: 46.09%] [G loss: 0.818810]\n",
      "epoch:4 step:4041 [D loss: 0.663877, acc.: 59.38%] [G loss: 0.816259]\n",
      "epoch:4 step:4042 [D loss: 0.680750, acc.: 53.12%] [G loss: 0.859776]\n",
      "epoch:4 step:4043 [D loss: 0.694031, acc.: 57.81%] [G loss: 0.812146]\n",
      "epoch:4 step:4044 [D loss: 0.665110, acc.: 48.44%] [G loss: 0.797105]\n",
      "epoch:4 step:4045 [D loss: 0.665368, acc.: 57.03%] [G loss: 0.932384]\n",
      "epoch:4 step:4046 [D loss: 0.684655, acc.: 59.38%] [G loss: 0.844727]\n",
      "epoch:4 step:4047 [D loss: 0.678763, acc.: 53.12%] [G loss: 0.918785]\n",
      "epoch:4 step:4048 [D loss: 0.670516, acc.: 60.16%] [G loss: 0.831490]\n",
      "epoch:4 step:4049 [D loss: 0.708869, acc.: 52.34%] [G loss: 0.774752]\n",
      "epoch:4 step:4050 [D loss: 0.685241, acc.: 53.12%] [G loss: 0.800107]\n",
      "epoch:4 step:4051 [D loss: 0.722012, acc.: 45.31%] [G loss: 0.764486]\n",
      "epoch:4 step:4052 [D loss: 0.746283, acc.: 41.41%] [G loss: 0.751493]\n",
      "epoch:4 step:4053 [D loss: 0.687486, acc.: 56.25%] [G loss: 0.782698]\n",
      "epoch:4 step:4054 [D loss: 0.680361, acc.: 50.00%] [G loss: 0.840340]\n",
      "epoch:4 step:4055 [D loss: 0.686764, acc.: 54.69%] [G loss: 0.820285]\n",
      "epoch:4 step:4056 [D loss: 0.692850, acc.: 54.69%] [G loss: 0.865684]\n",
      "epoch:4 step:4057 [D loss: 0.709717, acc.: 47.66%] [G loss: 0.822174]\n",
      "epoch:4 step:4058 [D loss: 0.679709, acc.: 57.81%] [G loss: 0.883150]\n",
      "epoch:4 step:4059 [D loss: 0.667957, acc.: 58.59%] [G loss: 0.859987]\n",
      "epoch:4 step:4060 [D loss: 0.671955, acc.: 55.47%] [G loss: 0.757190]\n",
      "epoch:4 step:4061 [D loss: 0.686719, acc.: 57.81%] [G loss: 0.792707]\n",
      "epoch:4 step:4062 [D loss: 0.662554, acc.: 54.69%] [G loss: 0.772899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4063 [D loss: 0.650242, acc.: 58.59%] [G loss: 0.814302]\n",
      "epoch:4 step:4064 [D loss: 0.668185, acc.: 53.91%] [G loss: 0.816220]\n",
      "epoch:4 step:4065 [D loss: 0.693214, acc.: 46.88%] [G loss: 0.863690]\n",
      "epoch:4 step:4066 [D loss: 0.699827, acc.: 50.00%] [G loss: 0.854946]\n",
      "epoch:4 step:4067 [D loss: 0.671002, acc.: 56.25%] [G loss: 0.827307]\n",
      "epoch:4 step:4068 [D loss: 0.673006, acc.: 57.81%] [G loss: 0.792626]\n",
      "epoch:4 step:4069 [D loss: 0.676102, acc.: 53.12%] [G loss: 0.799462]\n",
      "epoch:4 step:4070 [D loss: 0.673890, acc.: 54.69%] [G loss: 0.821181]\n",
      "epoch:4 step:4071 [D loss: 0.655711, acc.: 61.72%] [G loss: 0.789823]\n",
      "epoch:4 step:4072 [D loss: 0.689563, acc.: 50.78%] [G loss: 0.788934]\n",
      "epoch:4 step:4073 [D loss: 0.709736, acc.: 50.00%] [G loss: 0.810177]\n",
      "epoch:4 step:4074 [D loss: 0.737767, acc.: 39.84%] [G loss: 0.861573]\n",
      "epoch:4 step:4075 [D loss: 0.684677, acc.: 53.12%] [G loss: 0.743940]\n",
      "epoch:4 step:4076 [D loss: 0.682356, acc.: 58.59%] [G loss: 0.809388]\n",
      "epoch:4 step:4077 [D loss: 0.648867, acc.: 60.16%] [G loss: 0.875080]\n",
      "epoch:4 step:4078 [D loss: 0.683766, acc.: 52.34%] [G loss: 0.838884]\n",
      "epoch:4 step:4079 [D loss: 0.676017, acc.: 61.72%] [G loss: 0.777847]\n",
      "epoch:4 step:4080 [D loss: 0.676357, acc.: 57.81%] [G loss: 0.816617]\n",
      "epoch:4 step:4081 [D loss: 0.731502, acc.: 53.91%] [G loss: 0.830968]\n",
      "epoch:4 step:4082 [D loss: 0.669024, acc.: 54.69%] [G loss: 0.831340]\n",
      "epoch:4 step:4083 [D loss: 0.683155, acc.: 50.00%] [G loss: 0.851744]\n",
      "epoch:4 step:4084 [D loss: 0.646087, acc.: 63.28%] [G loss: 0.827218]\n",
      "epoch:4 step:4085 [D loss: 0.663157, acc.: 52.34%] [G loss: 0.920609]\n",
      "epoch:4 step:4086 [D loss: 0.722100, acc.: 45.31%] [G loss: 0.803686]\n",
      "epoch:4 step:4087 [D loss: 0.650802, acc.: 62.50%] [G loss: 0.837125]\n",
      "epoch:4 step:4088 [D loss: 0.686208, acc.: 51.56%] [G loss: 0.812077]\n",
      "epoch:4 step:4089 [D loss: 0.671630, acc.: 58.59%] [G loss: 0.825976]\n",
      "epoch:4 step:4090 [D loss: 0.686435, acc.: 60.94%] [G loss: 0.844065]\n",
      "epoch:4 step:4091 [D loss: 0.668144, acc.: 60.16%] [G loss: 0.849861]\n",
      "epoch:4 step:4092 [D loss: 0.674130, acc.: 53.12%] [G loss: 0.832961]\n",
      "epoch:4 step:4093 [D loss: 0.696536, acc.: 50.00%] [G loss: 0.827244]\n",
      "epoch:4 step:4094 [D loss: 0.689670, acc.: 58.59%] [G loss: 0.839222]\n",
      "epoch:4 step:4095 [D loss: 0.701000, acc.: 48.44%] [G loss: 0.837867]\n",
      "epoch:4 step:4096 [D loss: 0.694441, acc.: 53.12%] [G loss: 0.729703]\n",
      "epoch:4 step:4097 [D loss: 0.689519, acc.: 50.00%] [G loss: 0.780675]\n",
      "epoch:4 step:4098 [D loss: 0.712728, acc.: 48.44%] [G loss: 0.720170]\n",
      "epoch:4 step:4099 [D loss: 0.690677, acc.: 57.03%] [G loss: 0.850416]\n",
      "epoch:4 step:4100 [D loss: 0.694505, acc.: 51.56%] [G loss: 0.788542]\n",
      "epoch:4 step:4101 [D loss: 0.678816, acc.: 55.47%] [G loss: 0.877010]\n",
      "epoch:4 step:4102 [D loss: 0.693699, acc.: 52.34%] [G loss: 0.800992]\n",
      "epoch:4 step:4103 [D loss: 0.671742, acc.: 55.47%] [G loss: 0.838970]\n",
      "epoch:4 step:4104 [D loss: 0.706294, acc.: 47.66%] [G loss: 0.863008]\n",
      "epoch:4 step:4105 [D loss: 0.665825, acc.: 63.28%] [G loss: 0.816864]\n",
      "epoch:4 step:4106 [D loss: 0.668116, acc.: 59.38%] [G loss: 0.784850]\n",
      "epoch:4 step:4107 [D loss: 0.706713, acc.: 49.22%] [G loss: 0.811612]\n",
      "epoch:4 step:4108 [D loss: 0.652200, acc.: 60.94%] [G loss: 0.812093]\n",
      "epoch:4 step:4109 [D loss: 0.692936, acc.: 53.12%] [G loss: 0.864943]\n",
      "epoch:4 step:4110 [D loss: 0.739252, acc.: 55.47%] [G loss: 0.850000]\n",
      "epoch:4 step:4111 [D loss: 0.658330, acc.: 55.47%] [G loss: 0.928656]\n",
      "epoch:4 step:4112 [D loss: 0.673774, acc.: 53.91%] [G loss: 0.919836]\n",
      "epoch:4 step:4113 [D loss: 0.710967, acc.: 49.22%] [G loss: 0.823826]\n",
      "epoch:4 step:4114 [D loss: 0.643827, acc.: 62.50%] [G loss: 0.875947]\n",
      "epoch:4 step:4115 [D loss: 0.661561, acc.: 59.38%] [G loss: 0.862619]\n",
      "epoch:4 step:4116 [D loss: 0.669967, acc.: 57.03%] [G loss: 0.831815]\n",
      "epoch:4 step:4117 [D loss: 0.683209, acc.: 52.34%] [G loss: 0.852877]\n",
      "epoch:4 step:4118 [D loss: 0.730338, acc.: 46.88%] [G loss: 0.808260]\n",
      "epoch:4 step:4119 [D loss: 0.692819, acc.: 52.34%] [G loss: 0.790107]\n",
      "epoch:4 step:4120 [D loss: 0.692987, acc.: 53.91%] [G loss: 0.807363]\n",
      "epoch:4 step:4121 [D loss: 0.668423, acc.: 54.69%] [G loss: 0.874634]\n",
      "epoch:4 step:4122 [D loss: 0.658349, acc.: 56.25%] [G loss: 0.792222]\n",
      "epoch:4 step:4123 [D loss: 0.698443, acc.: 51.56%] [G loss: 0.789005]\n",
      "epoch:4 step:4124 [D loss: 0.739781, acc.: 50.00%] [G loss: 0.797745]\n",
      "epoch:4 step:4125 [D loss: 0.702279, acc.: 51.56%] [G loss: 0.798122]\n",
      "epoch:4 step:4126 [D loss: 0.674616, acc.: 57.81%] [G loss: 0.807546]\n",
      "epoch:4 step:4127 [D loss: 0.676167, acc.: 56.25%] [G loss: 0.861870]\n",
      "epoch:4 step:4128 [D loss: 0.671126, acc.: 50.78%] [G loss: 0.884267]\n",
      "epoch:4 step:4129 [D loss: 0.701189, acc.: 49.22%] [G loss: 0.802618]\n",
      "epoch:4 step:4130 [D loss: 0.688799, acc.: 51.56%] [G loss: 0.789955]\n",
      "epoch:4 step:4131 [D loss: 0.676310, acc.: 50.78%] [G loss: 0.778416]\n",
      "epoch:4 step:4132 [D loss: 0.715610, acc.: 39.06%] [G loss: 0.767298]\n",
      "epoch:4 step:4133 [D loss: 0.720350, acc.: 46.88%] [G loss: 0.801510]\n",
      "epoch:4 step:4134 [D loss: 0.691583, acc.: 53.12%] [G loss: 0.771134]\n",
      "epoch:4 step:4135 [D loss: 0.694700, acc.: 53.91%] [G loss: 0.832764]\n",
      "epoch:4 step:4136 [D loss: 0.692644, acc.: 52.34%] [G loss: 0.798892]\n",
      "epoch:4 step:4137 [D loss: 0.667569, acc.: 60.16%] [G loss: 0.821468]\n",
      "epoch:4 step:4138 [D loss: 0.724364, acc.: 39.84%] [G loss: 0.801331]\n",
      "epoch:4 step:4139 [D loss: 0.662113, acc.: 57.03%] [G loss: 0.772930]\n",
      "epoch:4 step:4140 [D loss: 0.699917, acc.: 49.22%] [G loss: 0.822964]\n",
      "epoch:4 step:4141 [D loss: 0.680450, acc.: 59.38%] [G loss: 0.835775]\n",
      "epoch:4 step:4142 [D loss: 0.678385, acc.: 60.16%] [G loss: 0.808868]\n",
      "epoch:4 step:4143 [D loss: 0.680574, acc.: 57.03%] [G loss: 0.786993]\n",
      "epoch:4 step:4144 [D loss: 0.686313, acc.: 51.56%] [G loss: 0.771817]\n",
      "epoch:4 step:4145 [D loss: 0.696697, acc.: 46.09%] [G loss: 0.739726]\n",
      "epoch:4 step:4146 [D loss: 0.685674, acc.: 56.25%] [G loss: 0.756942]\n",
      "epoch:4 step:4147 [D loss: 0.708756, acc.: 44.53%] [G loss: 0.783483]\n",
      "epoch:4 step:4148 [D loss: 0.658346, acc.: 61.72%] [G loss: 0.737680]\n",
      "epoch:4 step:4149 [D loss: 0.660988, acc.: 56.25%] [G loss: 0.765736]\n",
      "epoch:4 step:4150 [D loss: 0.679133, acc.: 57.81%] [G loss: 0.820616]\n",
      "epoch:4 step:4151 [D loss: 0.665632, acc.: 56.25%] [G loss: 0.802733]\n",
      "epoch:4 step:4152 [D loss: 0.692480, acc.: 54.69%] [G loss: 0.770406]\n",
      "epoch:4 step:4153 [D loss: 0.715654, acc.: 50.00%] [G loss: 0.772916]\n",
      "epoch:4 step:4154 [D loss: 0.703494, acc.: 53.12%] [G loss: 0.781904]\n",
      "epoch:4 step:4155 [D loss: 0.681174, acc.: 55.47%] [G loss: 0.821197]\n",
      "epoch:4 step:4156 [D loss: 0.699964, acc.: 52.34%] [G loss: 0.915983]\n",
      "epoch:4 step:4157 [D loss: 0.665509, acc.: 61.72%] [G loss: 0.854513]\n",
      "epoch:4 step:4158 [D loss: 0.696257, acc.: 56.25%] [G loss: 0.836239]\n",
      "epoch:4 step:4159 [D loss: 0.677538, acc.: 55.47%] [G loss: 0.794996]\n",
      "epoch:4 step:4160 [D loss: 0.679978, acc.: 52.34%] [G loss: 0.828818]\n",
      "epoch:4 step:4161 [D loss: 0.689576, acc.: 55.47%] [G loss: 0.846607]\n",
      "epoch:4 step:4162 [D loss: 0.692145, acc.: 53.12%] [G loss: 0.813404]\n",
      "epoch:4 step:4163 [D loss: 0.667854, acc.: 57.03%] [G loss: 0.853373]\n",
      "epoch:4 step:4164 [D loss: 0.696286, acc.: 55.47%] [G loss: 0.794893]\n",
      "epoch:4 step:4165 [D loss: 0.677620, acc.: 62.50%] [G loss: 0.822606]\n",
      "epoch:4 step:4166 [D loss: 0.640870, acc.: 64.06%] [G loss: 0.810539]\n",
      "epoch:4 step:4167 [D loss: 0.710225, acc.: 52.34%] [G loss: 0.788047]\n",
      "epoch:4 step:4168 [D loss: 0.697528, acc.: 47.66%] [G loss: 0.789059]\n",
      "epoch:4 step:4169 [D loss: 0.685675, acc.: 55.47%] [G loss: 0.745534]\n",
      "epoch:4 step:4170 [D loss: 0.696281, acc.: 50.78%] [G loss: 0.797818]\n",
      "epoch:4 step:4171 [D loss: 0.708119, acc.: 55.47%] [G loss: 0.764032]\n",
      "epoch:4 step:4172 [D loss: 0.675251, acc.: 51.56%] [G loss: 0.739698]\n",
      "epoch:4 step:4173 [D loss: 0.694005, acc.: 56.25%] [G loss: 0.774459]\n",
      "epoch:4 step:4174 [D loss: 0.671809, acc.: 57.03%] [G loss: 0.795598]\n",
      "epoch:4 step:4175 [D loss: 0.700049, acc.: 51.56%] [G loss: 0.800476]\n",
      "epoch:4 step:4176 [D loss: 0.684474, acc.: 52.34%] [G loss: 0.789899]\n",
      "epoch:4 step:4177 [D loss: 0.674712, acc.: 56.25%] [G loss: 0.798171]\n",
      "epoch:4 step:4178 [D loss: 0.682138, acc.: 56.25%] [G loss: 0.893022]\n",
      "epoch:4 step:4179 [D loss: 0.662484, acc.: 57.81%] [G loss: 0.834943]\n",
      "epoch:4 step:4180 [D loss: 0.723138, acc.: 50.00%] [G loss: 0.851774]\n",
      "epoch:4 step:4181 [D loss: 0.672627, acc.: 57.03%] [G loss: 0.807880]\n",
      "epoch:4 step:4182 [D loss: 0.693451, acc.: 53.91%] [G loss: 0.859611]\n",
      "epoch:4 step:4183 [D loss: 0.695096, acc.: 51.56%] [G loss: 0.927767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4184 [D loss: 0.669104, acc.: 55.47%] [G loss: 0.845583]\n",
      "epoch:4 step:4185 [D loss: 0.666267, acc.: 53.91%] [G loss: 0.800984]\n",
      "epoch:4 step:4186 [D loss: 0.675970, acc.: 52.34%] [G loss: 0.797452]\n",
      "epoch:4 step:4187 [D loss: 0.654300, acc.: 64.06%] [G loss: 0.778768]\n",
      "epoch:4 step:4188 [D loss: 0.695390, acc.: 52.34%] [G loss: 0.787810]\n",
      "epoch:4 step:4189 [D loss: 0.692551, acc.: 50.78%] [G loss: 0.816046]\n",
      "epoch:4 step:4190 [D loss: 0.648622, acc.: 60.94%] [G loss: 0.768655]\n",
      "epoch:4 step:4191 [D loss: 0.679833, acc.: 60.94%] [G loss: 0.835627]\n",
      "epoch:4 step:4192 [D loss: 0.695272, acc.: 54.69%] [G loss: 0.781902]\n",
      "epoch:4 step:4193 [D loss: 0.743517, acc.: 42.19%] [G loss: 0.777639]\n",
      "epoch:4 step:4194 [D loss: 0.696668, acc.: 56.25%] [G loss: 0.802488]\n",
      "epoch:4 step:4195 [D loss: 0.683501, acc.: 55.47%] [G loss: 0.841332]\n",
      "epoch:4 step:4196 [D loss: 0.692826, acc.: 50.00%] [G loss: 0.808737]\n",
      "epoch:4 step:4197 [D loss: 0.680147, acc.: 55.47%] [G loss: 0.881397]\n",
      "epoch:4 step:4198 [D loss: 0.688033, acc.: 53.12%] [G loss: 0.849704]\n",
      "epoch:4 step:4199 [D loss: 0.657060, acc.: 54.69%] [G loss: 0.894236]\n",
      "epoch:4 step:4200 [D loss: 0.675241, acc.: 53.12%] [G loss: 0.893164]\n",
      "epoch:4 step:4201 [D loss: 0.674687, acc.: 64.84%] [G loss: 0.783013]\n",
      "epoch:4 step:4202 [D loss: 0.717176, acc.: 47.66%] [G loss: 0.771046]\n",
      "epoch:4 step:4203 [D loss: 0.694810, acc.: 54.69%] [G loss: 0.766542]\n",
      "epoch:4 step:4204 [D loss: 0.736925, acc.: 46.88%] [G loss: 0.828700]\n",
      "epoch:4 step:4205 [D loss: 0.694574, acc.: 50.00%] [G loss: 0.814375]\n",
      "epoch:4 step:4206 [D loss: 0.740530, acc.: 45.31%] [G loss: 0.766284]\n",
      "epoch:4 step:4207 [D loss: 0.718529, acc.: 48.44%] [G loss: 0.742471]\n",
      "epoch:4 step:4208 [D loss: 0.671151, acc.: 57.81%] [G loss: 0.841089]\n",
      "epoch:4 step:4209 [D loss: 0.661767, acc.: 54.69%] [G loss: 0.785640]\n",
      "epoch:4 step:4210 [D loss: 0.693277, acc.: 49.22%] [G loss: 0.777153]\n",
      "epoch:4 step:4211 [D loss: 0.680356, acc.: 54.69%] [G loss: 0.765353]\n",
      "epoch:4 step:4212 [D loss: 0.669181, acc.: 53.12%] [G loss: 0.770652]\n",
      "epoch:4 step:4213 [D loss: 0.656911, acc.: 57.81%] [G loss: 0.854417]\n",
      "epoch:4 step:4214 [D loss: 0.665750, acc.: 57.81%] [G loss: 0.813362]\n",
      "epoch:4 step:4215 [D loss: 0.688413, acc.: 53.12%] [G loss: 0.809813]\n",
      "epoch:4 step:4216 [D loss: 0.692559, acc.: 54.69%] [G loss: 0.725799]\n",
      "epoch:4 step:4217 [D loss: 0.660385, acc.: 60.94%] [G loss: 0.778805]\n",
      "epoch:4 step:4218 [D loss: 0.685369, acc.: 59.38%] [G loss: 0.789540]\n",
      "epoch:4 step:4219 [D loss: 0.677406, acc.: 57.03%] [G loss: 0.799441]\n",
      "epoch:4 step:4220 [D loss: 0.685943, acc.: 57.03%] [G loss: 0.798172]\n",
      "epoch:4 step:4221 [D loss: 0.665117, acc.: 56.25%] [G loss: 0.765336]\n",
      "epoch:4 step:4222 [D loss: 0.674101, acc.: 59.38%] [G loss: 0.759572]\n",
      "epoch:4 step:4223 [D loss: 0.715392, acc.: 49.22%] [G loss: 0.758172]\n",
      "epoch:4 step:4224 [D loss: 0.693151, acc.: 56.25%] [G loss: 0.830761]\n",
      "epoch:4 step:4225 [D loss: 0.686373, acc.: 54.69%] [G loss: 0.825937]\n",
      "epoch:4 step:4226 [D loss: 0.674273, acc.: 53.91%] [G loss: 0.877129]\n",
      "epoch:4 step:4227 [D loss: 0.696465, acc.: 59.38%] [G loss: 0.814415]\n",
      "epoch:4 step:4228 [D loss: 0.693064, acc.: 53.12%] [G loss: 0.788062]\n",
      "epoch:4 step:4229 [D loss: 0.709485, acc.: 46.09%] [G loss: 0.791782]\n",
      "epoch:4 step:4230 [D loss: 0.709304, acc.: 44.53%] [G loss: 0.763004]\n",
      "epoch:4 step:4231 [D loss: 0.710316, acc.: 47.66%] [G loss: 0.799581]\n",
      "epoch:4 step:4232 [D loss: 0.678569, acc.: 57.81%] [G loss: 0.822929]\n",
      "epoch:4 step:4233 [D loss: 0.683550, acc.: 57.03%] [G loss: 0.804643]\n",
      "epoch:4 step:4234 [D loss: 0.664298, acc.: 58.59%] [G loss: 0.823174]\n",
      "epoch:4 step:4235 [D loss: 0.663823, acc.: 59.38%] [G loss: 0.809856]\n",
      "epoch:4 step:4236 [D loss: 0.695190, acc.: 56.25%] [G loss: 0.790681]\n",
      "epoch:4 step:4237 [D loss: 0.706998, acc.: 44.53%] [G loss: 0.778885]\n",
      "epoch:4 step:4238 [D loss: 0.667934, acc.: 57.03%] [G loss: 0.813946]\n",
      "epoch:4 step:4239 [D loss: 0.693706, acc.: 50.78%] [G loss: 0.777040]\n",
      "epoch:4 step:4240 [D loss: 0.682371, acc.: 58.59%] [G loss: 0.796606]\n",
      "epoch:4 step:4241 [D loss: 0.699761, acc.: 50.00%] [G loss: 0.758222]\n",
      "epoch:4 step:4242 [D loss: 0.699053, acc.: 52.34%] [G loss: 0.752063]\n",
      "epoch:4 step:4243 [D loss: 0.691379, acc.: 45.31%] [G loss: 0.795436]\n",
      "epoch:4 step:4244 [D loss: 0.687505, acc.: 56.25%] [G loss: 0.806457]\n",
      "epoch:4 step:4245 [D loss: 0.665466, acc.: 56.25%] [G loss: 0.793283]\n",
      "epoch:4 step:4246 [D loss: 0.708786, acc.: 49.22%] [G loss: 0.818384]\n",
      "epoch:4 step:4247 [D loss: 0.689017, acc.: 57.81%] [G loss: 0.779329]\n",
      "epoch:4 step:4248 [D loss: 0.734214, acc.: 54.69%] [G loss: 0.791837]\n",
      "epoch:4 step:4249 [D loss: 0.707102, acc.: 48.44%] [G loss: 0.816067]\n",
      "epoch:4 step:4250 [D loss: 0.648341, acc.: 64.06%] [G loss: 0.790054]\n",
      "epoch:4 step:4251 [D loss: 0.685935, acc.: 53.91%] [G loss: 0.794896]\n",
      "epoch:4 step:4252 [D loss: 0.690994, acc.: 51.56%] [G loss: 0.774198]\n",
      "epoch:4 step:4253 [D loss: 0.692730, acc.: 50.78%] [G loss: 0.807317]\n",
      "epoch:4 step:4254 [D loss: 0.683475, acc.: 52.34%] [G loss: 0.717779]\n",
      "epoch:4 step:4255 [D loss: 0.714465, acc.: 39.84%] [G loss: 0.795016]\n",
      "epoch:4 step:4256 [D loss: 0.701472, acc.: 50.78%] [G loss: 0.832808]\n",
      "epoch:4 step:4257 [D loss: 0.690643, acc.: 53.91%] [G loss: 0.789432]\n",
      "epoch:4 step:4258 [D loss: 0.689147, acc.: 47.66%] [G loss: 0.739305]\n",
      "epoch:4 step:4259 [D loss: 0.669602, acc.: 53.12%] [G loss: 0.835097]\n",
      "epoch:4 step:4260 [D loss: 0.672685, acc.: 53.91%] [G loss: 0.808858]\n",
      "epoch:4 step:4261 [D loss: 0.688692, acc.: 56.25%] [G loss: 0.787311]\n",
      "epoch:4 step:4262 [D loss: 0.696436, acc.: 52.34%] [G loss: 0.780244]\n",
      "epoch:4 step:4263 [D loss: 0.687766, acc.: 52.34%] [G loss: 0.776705]\n",
      "epoch:4 step:4264 [D loss: 0.689501, acc.: 59.38%] [G loss: 0.735485]\n",
      "epoch:4 step:4265 [D loss: 0.693066, acc.: 57.03%] [G loss: 0.778808]\n",
      "epoch:4 step:4266 [D loss: 0.670411, acc.: 54.69%] [G loss: 0.769303]\n",
      "epoch:4 step:4267 [D loss: 0.666797, acc.: 54.69%] [G loss: 0.780018]\n",
      "epoch:4 step:4268 [D loss: 0.669947, acc.: 58.59%] [G loss: 0.777307]\n",
      "epoch:4 step:4269 [D loss: 0.688142, acc.: 53.12%] [G loss: 0.822614]\n",
      "epoch:4 step:4270 [D loss: 0.709551, acc.: 50.78%] [G loss: 0.815512]\n",
      "epoch:4 step:4271 [D loss: 0.666476, acc.: 60.16%] [G loss: 0.796537]\n",
      "epoch:4 step:4272 [D loss: 0.689279, acc.: 50.78%] [G loss: 0.802268]\n",
      "epoch:4 step:4273 [D loss: 0.693418, acc.: 53.12%] [G loss: 0.789202]\n",
      "epoch:4 step:4274 [D loss: 0.723743, acc.: 49.22%] [G loss: 0.826020]\n",
      "epoch:4 step:4275 [D loss: 0.675864, acc.: 54.69%] [G loss: 0.834252]\n",
      "epoch:4 step:4276 [D loss: 0.692923, acc.: 48.44%] [G loss: 0.798145]\n",
      "epoch:4 step:4277 [D loss: 0.697721, acc.: 51.56%] [G loss: 0.772619]\n",
      "epoch:4 step:4278 [D loss: 0.701155, acc.: 50.00%] [G loss: 0.795905]\n",
      "epoch:4 step:4279 [D loss: 0.708620, acc.: 47.66%] [G loss: 0.736689]\n",
      "epoch:4 step:4280 [D loss: 0.679286, acc.: 55.47%] [G loss: 0.785954]\n",
      "epoch:4 step:4281 [D loss: 0.674134, acc.: 59.38%] [G loss: 0.757090]\n",
      "epoch:4 step:4282 [D loss: 0.650804, acc.: 61.72%] [G loss: 0.705625]\n",
      "epoch:4 step:4283 [D loss: 0.682293, acc.: 56.25%] [G loss: 0.754019]\n",
      "epoch:4 step:4284 [D loss: 0.694181, acc.: 54.69%] [G loss: 0.783616]\n",
      "epoch:4 step:4285 [D loss: 0.686767, acc.: 52.34%] [G loss: 0.761103]\n",
      "epoch:4 step:4286 [D loss: 0.686570, acc.: 54.69%] [G loss: 0.728040]\n",
      "epoch:4 step:4287 [D loss: 0.728125, acc.: 46.88%] [G loss: 0.765008]\n",
      "epoch:4 step:4288 [D loss: 0.694350, acc.: 54.69%] [G loss: 0.742919]\n",
      "epoch:4 step:4289 [D loss: 0.698665, acc.: 53.12%] [G loss: 0.724576]\n",
      "epoch:4 step:4290 [D loss: 0.703457, acc.: 52.34%] [G loss: 0.775809]\n",
      "epoch:4 step:4291 [D loss: 0.715088, acc.: 48.44%] [G loss: 0.761141]\n",
      "epoch:4 step:4292 [D loss: 0.670618, acc.: 59.38%] [G loss: 0.806933]\n",
      "epoch:4 step:4293 [D loss: 0.681619, acc.: 56.25%] [G loss: 0.787347]\n",
      "epoch:4 step:4294 [D loss: 0.686664, acc.: 58.59%] [G loss: 0.818997]\n",
      "epoch:4 step:4295 [D loss: 0.666617, acc.: 56.25%] [G loss: 0.954448]\n",
      "epoch:4 step:4296 [D loss: 0.691589, acc.: 53.12%] [G loss: 0.790063]\n",
      "epoch:4 step:4297 [D loss: 0.708642, acc.: 52.34%] [G loss: 0.739484]\n",
      "epoch:4 step:4298 [D loss: 0.707091, acc.: 50.00%] [G loss: 0.790720]\n",
      "epoch:4 step:4299 [D loss: 0.679147, acc.: 57.81%] [G loss: 0.771059]\n",
      "epoch:4 step:4300 [D loss: 0.665251, acc.: 60.16%] [G loss: 0.783549]\n",
      "epoch:4 step:4301 [D loss: 0.700094, acc.: 58.59%] [G loss: 0.801294]\n",
      "epoch:4 step:4302 [D loss: 0.690541, acc.: 50.78%] [G loss: 0.854161]\n",
      "epoch:4 step:4303 [D loss: 0.680344, acc.: 54.69%] [G loss: 0.771809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4304 [D loss: 0.685669, acc.: 50.00%] [G loss: 0.773477]\n",
      "epoch:4 step:4305 [D loss: 0.692215, acc.: 50.00%] [G loss: 0.778592]\n",
      "epoch:4 step:4306 [D loss: 0.714749, acc.: 46.88%] [G loss: 0.766197]\n",
      "epoch:4 step:4307 [D loss: 0.679387, acc.: 53.91%] [G loss: 0.755041]\n",
      "epoch:4 step:4308 [D loss: 0.708501, acc.: 41.41%] [G loss: 0.775544]\n",
      "epoch:4 step:4309 [D loss: 0.712698, acc.: 44.53%] [G loss: 0.744427]\n",
      "epoch:4 step:4310 [D loss: 0.678917, acc.: 58.59%] [G loss: 0.736827]\n",
      "epoch:4 step:4311 [D loss: 0.705067, acc.: 46.88%] [G loss: 0.735216]\n",
      "epoch:4 step:4312 [D loss: 0.681092, acc.: 51.56%] [G loss: 0.751058]\n",
      "epoch:4 step:4313 [D loss: 0.702607, acc.: 57.81%] [G loss: 0.741786]\n",
      "epoch:4 step:4314 [D loss: 0.661869, acc.: 53.12%] [G loss: 0.807457]\n",
      "epoch:4 step:4315 [D loss: 0.670740, acc.: 60.16%] [G loss: 0.786376]\n",
      "epoch:4 step:4316 [D loss: 0.701592, acc.: 53.12%] [G loss: 0.801209]\n",
      "epoch:4 step:4317 [D loss: 0.686993, acc.: 57.03%] [G loss: 0.801618]\n",
      "epoch:4 step:4318 [D loss: 0.690638, acc.: 50.00%] [G loss: 0.811252]\n",
      "epoch:4 step:4319 [D loss: 0.690214, acc.: 46.09%] [G loss: 0.793170]\n",
      "epoch:4 step:4320 [D loss: 0.685645, acc.: 57.03%] [G loss: 0.844029]\n",
      "epoch:4 step:4321 [D loss: 0.682702, acc.: 57.81%] [G loss: 0.839451]\n",
      "epoch:4 step:4322 [D loss: 0.708834, acc.: 47.66%] [G loss: 0.800880]\n",
      "epoch:4 step:4323 [D loss: 0.704541, acc.: 51.56%] [G loss: 0.760876]\n",
      "epoch:4 step:4324 [D loss: 0.696416, acc.: 51.56%] [G loss: 0.757358]\n",
      "epoch:4 step:4325 [D loss: 0.690983, acc.: 50.00%] [G loss: 0.778718]\n",
      "epoch:4 step:4326 [D loss: 0.701162, acc.: 52.34%] [G loss: 0.818739]\n",
      "epoch:4 step:4327 [D loss: 0.677983, acc.: 53.12%] [G loss: 0.794315]\n",
      "epoch:4 step:4328 [D loss: 0.711110, acc.: 49.22%] [G loss: 0.829782]\n",
      "epoch:4 step:4329 [D loss: 0.682592, acc.: 54.69%] [G loss: 0.781981]\n",
      "epoch:4 step:4330 [D loss: 0.667814, acc.: 59.38%] [G loss: 0.790477]\n",
      "epoch:4 step:4331 [D loss: 0.662217, acc.: 59.38%] [G loss: 0.802708]\n",
      "epoch:4 step:4332 [D loss: 0.670196, acc.: 60.16%] [G loss: 0.783015]\n",
      "epoch:4 step:4333 [D loss: 0.668980, acc.: 58.59%] [G loss: 0.808734]\n",
      "epoch:4 step:4334 [D loss: 0.687485, acc.: 53.91%] [G loss: 0.859783]\n",
      "epoch:4 step:4335 [D loss: 0.699959, acc.: 46.09%] [G loss: 0.797152]\n",
      "epoch:4 step:4336 [D loss: 0.717098, acc.: 51.56%] [G loss: 0.772709]\n",
      "epoch:4 step:4337 [D loss: 0.698983, acc.: 50.00%] [G loss: 0.832897]\n",
      "epoch:4 step:4338 [D loss: 0.707101, acc.: 53.12%] [G loss: 0.843004]\n",
      "epoch:4 step:4339 [D loss: 0.683175, acc.: 54.69%] [G loss: 0.758422]\n",
      "epoch:4 step:4340 [D loss: 0.670563, acc.: 54.69%] [G loss: 0.750692]\n",
      "epoch:4 step:4341 [D loss: 0.705873, acc.: 56.25%] [G loss: 0.787199]\n",
      "epoch:4 step:4342 [D loss: 0.658145, acc.: 65.62%] [G loss: 0.770683]\n",
      "epoch:4 step:4343 [D loss: 0.684255, acc.: 56.25%] [G loss: 0.780676]\n",
      "epoch:4 step:4344 [D loss: 0.652964, acc.: 61.72%] [G loss: 0.810848]\n",
      "epoch:4 step:4345 [D loss: 0.667939, acc.: 60.16%] [G loss: 0.819699]\n",
      "epoch:4 step:4346 [D loss: 0.676282, acc.: 51.56%] [G loss: 0.799641]\n",
      "epoch:4 step:4347 [D loss: 0.701082, acc.: 47.66%] [G loss: 0.755828]\n",
      "epoch:4 step:4348 [D loss: 0.684400, acc.: 55.47%] [G loss: 0.776241]\n",
      "epoch:4 step:4349 [D loss: 0.669626, acc.: 60.16%] [G loss: 0.776970]\n",
      "epoch:4 step:4350 [D loss: 0.694558, acc.: 57.81%] [G loss: 0.809597]\n",
      "epoch:4 step:4351 [D loss: 0.665356, acc.: 54.69%] [G loss: 0.884420]\n",
      "epoch:4 step:4352 [D loss: 0.679324, acc.: 54.69%] [G loss: 0.790443]\n",
      "epoch:4 step:4353 [D loss: 0.685617, acc.: 53.12%] [G loss: 0.793713]\n",
      "epoch:4 step:4354 [D loss: 0.719550, acc.: 50.00%] [G loss: 0.823727]\n",
      "epoch:4 step:4355 [D loss: 0.691670, acc.: 53.91%] [G loss: 0.836526]\n",
      "epoch:4 step:4356 [D loss: 0.681860, acc.: 51.56%] [G loss: 0.872282]\n",
      "epoch:4 step:4357 [D loss: 0.689417, acc.: 49.22%] [G loss: 0.763539]\n",
      "epoch:4 step:4358 [D loss: 0.668256, acc.: 57.81%] [G loss: 0.871566]\n",
      "epoch:4 step:4359 [D loss: 0.698367, acc.: 49.22%] [G loss: 0.852410]\n",
      "epoch:4 step:4360 [D loss: 0.674710, acc.: 57.81%] [G loss: 0.793215]\n",
      "epoch:4 step:4361 [D loss: 0.675485, acc.: 60.16%] [G loss: 0.816994]\n",
      "epoch:4 step:4362 [D loss: 0.677326, acc.: 54.69%] [G loss: 0.802423]\n",
      "epoch:4 step:4363 [D loss: 0.687046, acc.: 56.25%] [G loss: 0.815700]\n",
      "epoch:4 step:4364 [D loss: 0.683315, acc.: 55.47%] [G loss: 0.840912]\n",
      "epoch:4 step:4365 [D loss: 0.696519, acc.: 50.78%] [G loss: 0.808875]\n",
      "epoch:4 step:4366 [D loss: 0.681294, acc.: 55.47%] [G loss: 0.833065]\n",
      "epoch:4 step:4367 [D loss: 0.712072, acc.: 49.22%] [G loss: 0.787297]\n",
      "epoch:4 step:4368 [D loss: 0.671395, acc.: 57.03%] [G loss: 0.827768]\n",
      "epoch:4 step:4369 [D loss: 0.698876, acc.: 51.56%] [G loss: 0.836803]\n",
      "epoch:4 step:4370 [D loss: 0.678251, acc.: 58.59%] [G loss: 0.767428]\n",
      "epoch:4 step:4371 [D loss: 0.681401, acc.: 57.03%] [G loss: 0.752528]\n",
      "epoch:4 step:4372 [D loss: 0.657811, acc.: 60.16%] [G loss: 0.769819]\n",
      "epoch:4 step:4373 [D loss: 0.702847, acc.: 57.81%] [G loss: 0.796255]\n",
      "epoch:4 step:4374 [D loss: 0.687104, acc.: 54.69%] [G loss: 0.871677]\n",
      "epoch:4 step:4375 [D loss: 0.704324, acc.: 53.12%] [G loss: 0.850959]\n",
      "epoch:4 step:4376 [D loss: 0.694006, acc.: 50.78%] [G loss: 0.810052]\n",
      "epoch:4 step:4377 [D loss: 0.691518, acc.: 53.91%] [G loss: 0.795464]\n",
      "epoch:4 step:4378 [D loss: 0.672072, acc.: 50.00%] [G loss: 0.785450]\n",
      "epoch:4 step:4379 [D loss: 0.679757, acc.: 56.25%] [G loss: 0.832290]\n",
      "epoch:4 step:4380 [D loss: 0.684543, acc.: 54.69%] [G loss: 0.798891]\n",
      "epoch:4 step:4381 [D loss: 0.674320, acc.: 52.34%] [G loss: 0.804727]\n",
      "epoch:4 step:4382 [D loss: 0.687648, acc.: 57.03%] [G loss: 0.776359]\n",
      "epoch:4 step:4383 [D loss: 0.665048, acc.: 64.06%] [G loss: 0.746924]\n",
      "epoch:4 step:4384 [D loss: 0.699819, acc.: 50.78%] [G loss: 0.808160]\n",
      "epoch:4 step:4385 [D loss: 0.643622, acc.: 57.03%] [G loss: 0.846011]\n",
      "epoch:4 step:4386 [D loss: 0.652640, acc.: 58.59%] [G loss: 0.972113]\n",
      "epoch:4 step:4387 [D loss: 0.704881, acc.: 52.34%] [G loss: 0.908764]\n",
      "epoch:4 step:4388 [D loss: 0.646605, acc.: 60.94%] [G loss: 0.839801]\n",
      "epoch:4 step:4389 [D loss: 0.695287, acc.: 52.34%] [G loss: 0.771722]\n",
      "epoch:4 step:4390 [D loss: 0.720959, acc.: 48.44%] [G loss: 0.790469]\n",
      "epoch:4 step:4391 [D loss: 0.711144, acc.: 47.66%] [G loss: 0.788974]\n",
      "epoch:4 step:4392 [D loss: 0.686342, acc.: 50.78%] [G loss: 0.795435]\n",
      "epoch:4 step:4393 [D loss: 0.662062, acc.: 55.47%] [G loss: 0.804597]\n",
      "epoch:4 step:4394 [D loss: 0.858657, acc.: 38.28%] [G loss: 0.834214]\n",
      "epoch:4 step:4395 [D loss: 0.661747, acc.: 56.25%] [G loss: 0.841209]\n",
      "epoch:4 step:4396 [D loss: 0.655089, acc.: 60.16%] [G loss: 0.972035]\n",
      "epoch:4 step:4397 [D loss: 0.689427, acc.: 53.12%] [G loss: 0.917677]\n",
      "epoch:4 step:4398 [D loss: 0.678602, acc.: 57.03%] [G loss: 0.837076]\n",
      "epoch:4 step:4399 [D loss: 0.680232, acc.: 57.81%] [G loss: 0.897643]\n",
      "epoch:4 step:4400 [D loss: 0.708492, acc.: 41.41%] [G loss: 0.811765]\n",
      "epoch:4 step:4401 [D loss: 0.667050, acc.: 57.03%] [G loss: 0.821478]\n",
      "epoch:4 step:4402 [D loss: 0.681188, acc.: 50.78%] [G loss: 0.915085]\n",
      "epoch:4 step:4403 [D loss: 0.683592, acc.: 50.78%] [G loss: 0.813508]\n",
      "epoch:4 step:4404 [D loss: 0.688961, acc.: 52.34%] [G loss: 0.775613]\n",
      "epoch:4 step:4405 [D loss: 0.666515, acc.: 64.84%] [G loss: 0.743242]\n",
      "epoch:4 step:4406 [D loss: 0.718753, acc.: 50.78%] [G loss: 0.738130]\n",
      "epoch:4 step:4407 [D loss: 0.726732, acc.: 50.00%] [G loss: 0.766624]\n",
      "epoch:4 step:4408 [D loss: 0.672586, acc.: 55.47%] [G loss: 0.833382]\n",
      "epoch:4 step:4409 [D loss: 0.681069, acc.: 65.62%] [G loss: 0.759531]\n",
      "epoch:4 step:4410 [D loss: 0.704547, acc.: 44.53%] [G loss: 0.850477]\n",
      "epoch:4 step:4411 [D loss: 0.688730, acc.: 56.25%] [G loss: 0.819227]\n",
      "epoch:4 step:4412 [D loss: 0.702227, acc.: 53.12%] [G loss: 0.800258]\n",
      "epoch:4 step:4413 [D loss: 0.660793, acc.: 63.28%] [G loss: 0.825457]\n",
      "epoch:4 step:4414 [D loss: 0.678446, acc.: 55.47%] [G loss: 0.783163]\n",
      "epoch:4 step:4415 [D loss: 0.659379, acc.: 60.94%] [G loss: 0.719534]\n",
      "epoch:4 step:4416 [D loss: 0.686872, acc.: 54.69%] [G loss: 0.836510]\n",
      "epoch:4 step:4417 [D loss: 0.694925, acc.: 51.56%] [G loss: 0.783687]\n",
      "epoch:4 step:4418 [D loss: 0.701325, acc.: 53.12%] [G loss: 0.821270]\n",
      "epoch:4 step:4419 [D loss: 0.708908, acc.: 46.88%] [G loss: 0.773501]\n",
      "epoch:4 step:4420 [D loss: 0.674201, acc.: 57.81%] [G loss: 0.825024]\n",
      "epoch:4 step:4421 [D loss: 0.689788, acc.: 52.34%] [G loss: 0.801031]\n",
      "epoch:4 step:4422 [D loss: 0.668037, acc.: 61.72%] [G loss: 0.793234]\n",
      "epoch:4 step:4423 [D loss: 0.683309, acc.: 50.78%] [G loss: 0.762007]\n",
      "epoch:4 step:4424 [D loss: 0.716146, acc.: 43.75%] [G loss: 0.788711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4425 [D loss: 0.652920, acc.: 64.06%] [G loss: 0.790684]\n",
      "epoch:4 step:4426 [D loss: 0.702995, acc.: 46.09%] [G loss: 0.825736]\n",
      "epoch:4 step:4427 [D loss: 0.670681, acc.: 57.81%] [G loss: 0.824738]\n",
      "epoch:4 step:4428 [D loss: 0.693011, acc.: 49.22%] [G loss: 0.805378]\n",
      "epoch:4 step:4429 [D loss: 0.650501, acc.: 62.50%] [G loss: 0.785990]\n",
      "epoch:4 step:4430 [D loss: 0.661391, acc.: 60.94%] [G loss: 0.802349]\n",
      "epoch:4 step:4431 [D loss: 0.705324, acc.: 49.22%] [G loss: 0.748901]\n",
      "epoch:4 step:4432 [D loss: 0.673263, acc.: 48.44%] [G loss: 0.755746]\n",
      "epoch:4 step:4433 [D loss: 0.673929, acc.: 54.69%] [G loss: 0.801781]\n",
      "epoch:4 step:4434 [D loss: 0.696889, acc.: 52.34%] [G loss: 0.828023]\n",
      "epoch:4 step:4435 [D loss: 0.716571, acc.: 49.22%] [G loss: 0.778332]\n",
      "epoch:4 step:4436 [D loss: 0.681680, acc.: 58.59%] [G loss: 0.798545]\n",
      "epoch:4 step:4437 [D loss: 0.678544, acc.: 54.69%] [G loss: 0.817834]\n",
      "epoch:4 step:4438 [D loss: 0.665269, acc.: 61.72%] [G loss: 0.810291]\n",
      "epoch:4 step:4439 [D loss: 0.696140, acc.: 56.25%] [G loss: 0.780107]\n",
      "epoch:4 step:4440 [D loss: 0.721117, acc.: 42.97%] [G loss: 0.752855]\n",
      "epoch:4 step:4441 [D loss: 0.703719, acc.: 47.66%] [G loss: 0.780927]\n",
      "epoch:4 step:4442 [D loss: 0.708630, acc.: 49.22%] [G loss: 0.741793]\n",
      "epoch:4 step:4443 [D loss: 0.686510, acc.: 64.06%] [G loss: 0.772890]\n",
      "epoch:4 step:4444 [D loss: 0.661159, acc.: 57.81%] [G loss: 0.837987]\n",
      "epoch:4 step:4445 [D loss: 0.725051, acc.: 45.31%] [G loss: 0.819785]\n",
      "epoch:4 step:4446 [D loss: 0.713760, acc.: 39.84%] [G loss: 0.824978]\n",
      "epoch:4 step:4447 [D loss: 0.731512, acc.: 38.28%] [G loss: 0.733705]\n",
      "epoch:4 step:4448 [D loss: 0.685908, acc.: 60.16%] [G loss: 0.773517]\n",
      "epoch:4 step:4449 [D loss: 0.669938, acc.: 60.16%] [G loss: 0.809034]\n",
      "epoch:4 step:4450 [D loss: 0.650288, acc.: 64.06%] [G loss: 0.823353]\n",
      "epoch:4 step:4451 [D loss: 0.693520, acc.: 53.12%] [G loss: 0.760299]\n",
      "epoch:4 step:4452 [D loss: 0.691361, acc.: 51.56%] [G loss: 0.748769]\n",
      "epoch:4 step:4453 [D loss: 0.688923, acc.: 55.47%] [G loss: 0.820676]\n",
      "epoch:4 step:4454 [D loss: 0.688822, acc.: 54.69%] [G loss: 0.833582]\n",
      "epoch:4 step:4455 [D loss: 0.675264, acc.: 53.12%] [G loss: 0.827304]\n",
      "epoch:4 step:4456 [D loss: 0.659683, acc.: 56.25%] [G loss: 0.878111]\n",
      "epoch:4 step:4457 [D loss: 0.681364, acc.: 56.25%] [G loss: 0.854413]\n",
      "epoch:4 step:4458 [D loss: 0.668531, acc.: 57.81%] [G loss: 0.862965]\n",
      "epoch:4 step:4459 [D loss: 0.669608, acc.: 53.91%] [G loss: 0.851889]\n",
      "epoch:4 step:4460 [D loss: 0.658837, acc.: 64.06%] [G loss: 0.868174]\n",
      "epoch:4 step:4461 [D loss: 0.695287, acc.: 53.12%] [G loss: 0.843425]\n",
      "epoch:4 step:4462 [D loss: 0.667261, acc.: 60.16%] [G loss: 0.822038]\n",
      "epoch:4 step:4463 [D loss: 0.713244, acc.: 47.66%] [G loss: 0.837223]\n",
      "epoch:4 step:4464 [D loss: 0.714551, acc.: 49.22%] [G loss: 0.800945]\n",
      "epoch:4 step:4465 [D loss: 0.727748, acc.: 44.53%] [G loss: 0.771287]\n",
      "epoch:4 step:4466 [D loss: 0.677383, acc.: 58.59%] [G loss: 0.795160]\n",
      "epoch:4 step:4467 [D loss: 0.709597, acc.: 49.22%] [G loss: 0.817803]\n",
      "epoch:4 step:4468 [D loss: 0.711504, acc.: 46.88%] [G loss: 0.852827]\n",
      "epoch:4 step:4469 [D loss: 0.686627, acc.: 55.47%] [G loss: 0.768681]\n",
      "epoch:4 step:4470 [D loss: 0.691062, acc.: 55.47%] [G loss: 0.784738]\n",
      "epoch:4 step:4471 [D loss: 0.662092, acc.: 61.72%] [G loss: 0.829414]\n",
      "epoch:4 step:4472 [D loss: 0.683147, acc.: 52.34%] [G loss: 0.749686]\n",
      "epoch:4 step:4473 [D loss: 0.661153, acc.: 53.91%] [G loss: 0.765261]\n",
      "epoch:4 step:4474 [D loss: 0.697195, acc.: 51.56%] [G loss: 0.802810]\n",
      "epoch:4 step:4475 [D loss: 0.696598, acc.: 51.56%] [G loss: 0.790616]\n",
      "epoch:4 step:4476 [D loss: 0.686726, acc.: 54.69%] [G loss: 0.785206]\n",
      "epoch:4 step:4477 [D loss: 0.701433, acc.: 48.44%] [G loss: 0.746184]\n",
      "epoch:4 step:4478 [D loss: 0.673376, acc.: 57.81%] [G loss: 0.813038]\n",
      "epoch:4 step:4479 [D loss: 0.718301, acc.: 54.69%] [G loss: 0.829998]\n",
      "epoch:4 step:4480 [D loss: 0.647456, acc.: 63.28%] [G loss: 0.800146]\n",
      "epoch:4 step:4481 [D loss: 0.699289, acc.: 50.00%] [G loss: 0.827166]\n",
      "epoch:4 step:4482 [D loss: 0.649243, acc.: 64.84%] [G loss: 0.820840]\n",
      "epoch:4 step:4483 [D loss: 0.653582, acc.: 60.16%] [G loss: 0.825786]\n",
      "epoch:4 step:4484 [D loss: 0.671256, acc.: 57.81%] [G loss: 0.809072]\n",
      "epoch:4 step:4485 [D loss: 0.667644, acc.: 57.81%] [G loss: 0.788776]\n",
      "epoch:4 step:4486 [D loss: 0.685110, acc.: 49.22%] [G loss: 0.784258]\n",
      "epoch:4 step:4487 [D loss: 0.705949, acc.: 54.69%] [G loss: 0.790055]\n",
      "epoch:4 step:4488 [D loss: 0.707558, acc.: 50.78%] [G loss: 0.815183]\n",
      "epoch:4 step:4489 [D loss: 0.670183, acc.: 57.81%] [G loss: 0.833349]\n",
      "epoch:4 step:4490 [D loss: 0.723725, acc.: 53.12%] [G loss: 0.903503]\n",
      "epoch:4 step:4491 [D loss: 0.688781, acc.: 53.12%] [G loss: 0.905262]\n",
      "epoch:4 step:4492 [D loss: 0.659963, acc.: 61.72%] [G loss: 0.834308]\n",
      "epoch:4 step:4493 [D loss: 0.695490, acc.: 48.44%] [G loss: 0.801001]\n",
      "epoch:4 step:4494 [D loss: 0.693391, acc.: 48.44%] [G loss: 0.781334]\n",
      "epoch:4 step:4495 [D loss: 0.691150, acc.: 51.56%] [G loss: 0.715839]\n",
      "epoch:4 step:4496 [D loss: 0.685802, acc.: 50.00%] [G loss: 0.759829]\n",
      "epoch:4 step:4497 [D loss: 0.679872, acc.: 53.91%] [G loss: 0.739674]\n",
      "epoch:4 step:4498 [D loss: 0.684375, acc.: 58.59%] [G loss: 0.754988]\n",
      "epoch:4 step:4499 [D loss: 0.674923, acc.: 60.94%] [G loss: 0.736769]\n",
      "epoch:4 step:4500 [D loss: 0.681776, acc.: 58.59%] [G loss: 0.810983]\n",
      "epoch:4 step:4501 [D loss: 0.684132, acc.: 57.81%] [G loss: 0.824570]\n",
      "epoch:4 step:4502 [D loss: 0.672317, acc.: 60.94%] [G loss: 0.845277]\n",
      "epoch:4 step:4503 [D loss: 0.711763, acc.: 41.41%] [G loss: 0.793228]\n",
      "epoch:4 step:4504 [D loss: 0.706520, acc.: 42.19%] [G loss: 0.769151]\n",
      "epoch:4 step:4505 [D loss: 0.682237, acc.: 50.78%] [G loss: 0.781341]\n",
      "epoch:4 step:4506 [D loss: 0.706888, acc.: 46.88%] [G loss: 0.748724]\n",
      "epoch:4 step:4507 [D loss: 0.702579, acc.: 47.66%] [G loss: 0.754022]\n",
      "epoch:4 step:4508 [D loss: 0.684289, acc.: 53.91%] [G loss: 0.743686]\n",
      "epoch:4 step:4509 [D loss: 0.707394, acc.: 46.09%] [G loss: 0.780539]\n",
      "epoch:4 step:4510 [D loss: 0.712584, acc.: 48.44%] [G loss: 0.805968]\n",
      "epoch:4 step:4511 [D loss: 0.684820, acc.: 51.56%] [G loss: 0.794712]\n",
      "epoch:4 step:4512 [D loss: 0.642944, acc.: 65.62%] [G loss: 0.802341]\n",
      "epoch:4 step:4513 [D loss: 0.667332, acc.: 61.72%] [G loss: 0.865791]\n",
      "epoch:4 step:4514 [D loss: 0.676015, acc.: 54.69%] [G loss: 0.811602]\n",
      "epoch:4 step:4515 [D loss: 0.693179, acc.: 51.56%] [G loss: 0.804717]\n",
      "epoch:4 step:4516 [D loss: 0.685557, acc.: 57.03%] [G loss: 0.812318]\n",
      "epoch:4 step:4517 [D loss: 0.665976, acc.: 59.38%] [G loss: 0.786293]\n",
      "epoch:4 step:4518 [D loss: 0.708138, acc.: 49.22%] [G loss: 0.782517]\n",
      "epoch:4 step:4519 [D loss: 0.645644, acc.: 62.50%] [G loss: 0.757471]\n",
      "epoch:4 step:4520 [D loss: 0.683487, acc.: 52.34%] [G loss: 0.781062]\n",
      "epoch:4 step:4521 [D loss: 0.665988, acc.: 61.72%] [G loss: 0.762048]\n",
      "epoch:4 step:4522 [D loss: 0.700961, acc.: 47.66%] [G loss: 0.767858]\n",
      "epoch:4 step:4523 [D loss: 0.674319, acc.: 57.03%] [G loss: 0.809122]\n",
      "epoch:4 step:4524 [D loss: 0.683183, acc.: 56.25%] [G loss: 0.738394]\n",
      "epoch:4 step:4525 [D loss: 0.698992, acc.: 50.78%] [G loss: 0.789878]\n",
      "epoch:4 step:4526 [D loss: 0.706334, acc.: 46.88%] [G loss: 0.739560]\n",
      "epoch:4 step:4527 [D loss: 0.691859, acc.: 57.03%] [G loss: 0.763002]\n",
      "epoch:4 step:4528 [D loss: 0.689782, acc.: 52.34%] [G loss: 0.830526]\n",
      "epoch:4 step:4529 [D loss: 0.662402, acc.: 58.59%] [G loss: 0.815414]\n",
      "epoch:4 step:4530 [D loss: 0.686655, acc.: 55.47%] [G loss: 0.742486]\n",
      "epoch:4 step:4531 [D loss: 0.699261, acc.: 49.22%] [G loss: 0.781746]\n",
      "epoch:4 step:4532 [D loss: 0.673571, acc.: 61.72%] [G loss: 0.765913]\n",
      "epoch:4 step:4533 [D loss: 0.697770, acc.: 50.00%] [G loss: 0.775964]\n",
      "epoch:4 step:4534 [D loss: 0.682842, acc.: 63.28%] [G loss: 0.773264]\n",
      "epoch:4 step:4535 [D loss: 0.685684, acc.: 51.56%] [G loss: 0.814519]\n",
      "epoch:4 step:4536 [D loss: 0.673885, acc.: 58.59%] [G loss: 0.808008]\n",
      "epoch:4 step:4537 [D loss: 0.679323, acc.: 50.00%] [G loss: 0.828750]\n",
      "epoch:4 step:4538 [D loss: 0.682194, acc.: 53.91%] [G loss: 0.815314]\n",
      "epoch:4 step:4539 [D loss: 0.672097, acc.: 51.56%] [G loss: 0.843462]\n",
      "epoch:4 step:4540 [D loss: 0.645775, acc.: 60.16%] [G loss: 0.815401]\n",
      "epoch:4 step:4541 [D loss: 0.702535, acc.: 57.81%] [G loss: 0.801296]\n",
      "epoch:4 step:4542 [D loss: 0.700312, acc.: 51.56%] [G loss: 0.788073]\n",
      "epoch:4 step:4543 [D loss: 0.695380, acc.: 57.81%] [G loss: 0.859069]\n",
      "epoch:4 step:4544 [D loss: 0.722805, acc.: 49.22%] [G loss: 0.822974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4545 [D loss: 0.722289, acc.: 50.78%] [G loss: 0.822183]\n",
      "epoch:4 step:4546 [D loss: 0.731591, acc.: 39.06%] [G loss: 0.752961]\n",
      "epoch:4 step:4547 [D loss: 0.721190, acc.: 46.88%] [G loss: 0.795613]\n",
      "epoch:4 step:4548 [D loss: 0.676050, acc.: 58.59%] [G loss: 0.820224]\n",
      "epoch:4 step:4549 [D loss: 0.664946, acc.: 58.59%] [G loss: 0.793177]\n",
      "epoch:4 step:4550 [D loss: 0.691024, acc.: 51.56%] [G loss: 0.820347]\n",
      "epoch:4 step:4551 [D loss: 0.674863, acc.: 60.16%] [G loss: 0.775748]\n",
      "epoch:4 step:4552 [D loss: 0.669456, acc.: 58.59%] [G loss: 0.784070]\n",
      "epoch:4 step:4553 [D loss: 0.679638, acc.: 57.81%] [G loss: 0.767689]\n",
      "epoch:4 step:4554 [D loss: 0.701303, acc.: 53.91%] [G loss: 0.765627]\n",
      "epoch:4 step:4555 [D loss: 0.709351, acc.: 50.78%] [G loss: 0.783244]\n",
      "epoch:4 step:4556 [D loss: 0.675352, acc.: 55.47%] [G loss: 0.785756]\n",
      "epoch:4 step:4557 [D loss: 0.656432, acc.: 64.06%] [G loss: 0.738780]\n",
      "epoch:4 step:4558 [D loss: 0.698545, acc.: 50.00%] [G loss: 0.804201]\n",
      "epoch:4 step:4559 [D loss: 0.705599, acc.: 48.44%] [G loss: 0.778565]\n",
      "epoch:4 step:4560 [D loss: 0.703987, acc.: 47.66%] [G loss: 0.731714]\n",
      "epoch:4 step:4561 [D loss: 0.714050, acc.: 46.88%] [G loss: 0.766290]\n",
      "epoch:4 step:4562 [D loss: 0.679443, acc.: 57.81%] [G loss: 0.799814]\n",
      "epoch:4 step:4563 [D loss: 0.687218, acc.: 57.81%] [G loss: 0.802603]\n",
      "epoch:4 step:4564 [D loss: 0.658684, acc.: 59.38%] [G loss: 0.855430]\n",
      "epoch:4 step:4565 [D loss: 0.682721, acc.: 50.78%] [G loss: 0.872400]\n",
      "epoch:4 step:4566 [D loss: 0.654676, acc.: 59.38%] [G loss: 0.927327]\n",
      "epoch:4 step:4567 [D loss: 0.682937, acc.: 56.25%] [G loss: 0.863726]\n",
      "epoch:4 step:4568 [D loss: 0.685819, acc.: 57.03%] [G loss: 0.850780]\n",
      "epoch:4 step:4569 [D loss: 0.669973, acc.: 57.03%] [G loss: 0.828167]\n",
      "epoch:4 step:4570 [D loss: 0.663986, acc.: 58.59%] [G loss: 0.868143]\n",
      "epoch:4 step:4571 [D loss: 0.664820, acc.: 64.84%] [G loss: 0.814245]\n",
      "epoch:4 step:4572 [D loss: 0.690667, acc.: 53.91%] [G loss: 0.857173]\n",
      "epoch:4 step:4573 [D loss: 0.672463, acc.: 60.16%] [G loss: 0.818995]\n",
      "epoch:4 step:4574 [D loss: 0.685155, acc.: 57.03%] [G loss: 0.787009]\n",
      "epoch:4 step:4575 [D loss: 0.687135, acc.: 53.12%] [G loss: 0.799006]\n",
      "epoch:4 step:4576 [D loss: 0.711096, acc.: 46.09%] [G loss: 0.781371]\n",
      "epoch:4 step:4577 [D loss: 0.682203, acc.: 59.38%] [G loss: 0.839165]\n",
      "epoch:4 step:4578 [D loss: 0.703402, acc.: 44.53%] [G loss: 0.809404]\n",
      "epoch:4 step:4579 [D loss: 0.712210, acc.: 47.66%] [G loss: 0.828594]\n",
      "epoch:4 step:4580 [D loss: 0.717794, acc.: 46.09%] [G loss: 0.805697]\n",
      "epoch:4 step:4581 [D loss: 0.688141, acc.: 55.47%] [G loss: 0.760821]\n",
      "epoch:4 step:4582 [D loss: 0.694320, acc.: 52.34%] [G loss: 0.792660]\n",
      "epoch:4 step:4583 [D loss: 0.696429, acc.: 49.22%] [G loss: 0.763231]\n",
      "epoch:4 step:4584 [D loss: 0.677122, acc.: 60.94%] [G loss: 0.760493]\n",
      "epoch:4 step:4585 [D loss: 0.713145, acc.: 40.62%] [G loss: 0.776430]\n",
      "epoch:4 step:4586 [D loss: 0.706558, acc.: 48.44%] [G loss: 0.760751]\n",
      "epoch:4 step:4587 [D loss: 0.662289, acc.: 64.06%] [G loss: 0.820124]\n",
      "epoch:4 step:4588 [D loss: 0.662342, acc.: 57.03%] [G loss: 0.802558]\n",
      "epoch:4 step:4589 [D loss: 0.705079, acc.: 51.56%] [G loss: 0.792348]\n",
      "epoch:4 step:4590 [D loss: 0.691021, acc.: 56.25%] [G loss: 0.812341]\n",
      "epoch:4 step:4591 [D loss: 0.700707, acc.: 47.66%] [G loss: 0.803737]\n",
      "epoch:4 step:4592 [D loss: 0.693571, acc.: 47.66%] [G loss: 0.805560]\n",
      "epoch:4 step:4593 [D loss: 0.686549, acc.: 54.69%] [G loss: 0.811185]\n",
      "epoch:4 step:4594 [D loss: 0.648692, acc.: 64.84%] [G loss: 0.802295]\n",
      "epoch:4 step:4595 [D loss: 0.653835, acc.: 59.38%] [G loss: 0.876727]\n",
      "epoch:4 step:4596 [D loss: 0.678680, acc.: 57.81%] [G loss: 0.798152]\n",
      "epoch:4 step:4597 [D loss: 0.696894, acc.: 53.91%] [G loss: 0.804915]\n",
      "epoch:4 step:4598 [D loss: 0.684323, acc.: 58.59%] [G loss: 0.819275]\n",
      "epoch:4 step:4599 [D loss: 0.688532, acc.: 54.69%] [G loss: 0.785211]\n",
      "epoch:4 step:4600 [D loss: 0.661524, acc.: 60.94%] [G loss: 0.808997]\n",
      "epoch:4 step:4601 [D loss: 0.690714, acc.: 51.56%] [G loss: 0.842628]\n",
      "epoch:4 step:4602 [D loss: 0.705464, acc.: 50.00%] [G loss: 0.843956]\n",
      "epoch:4 step:4603 [D loss: 0.708223, acc.: 45.31%] [G loss: 0.839985]\n",
      "epoch:4 step:4604 [D loss: 0.674207, acc.: 50.78%] [G loss: 0.846631]\n",
      "epoch:4 step:4605 [D loss: 0.680188, acc.: 60.16%] [G loss: 0.779972]\n",
      "epoch:4 step:4606 [D loss: 0.701006, acc.: 50.00%] [G loss: 0.796204]\n",
      "epoch:4 step:4607 [D loss: 0.668743, acc.: 60.94%] [G loss: 0.797448]\n",
      "epoch:4 step:4608 [D loss: 0.709671, acc.: 46.09%] [G loss: 0.789511]\n",
      "epoch:4 step:4609 [D loss: 0.656591, acc.: 61.72%] [G loss: 0.768353]\n",
      "epoch:4 step:4610 [D loss: 0.699508, acc.: 47.66%] [G loss: 0.830775]\n",
      "epoch:4 step:4611 [D loss: 0.686837, acc.: 50.00%] [G loss: 0.790484]\n",
      "epoch:4 step:4612 [D loss: 0.674451, acc.: 51.56%] [G loss: 0.867331]\n",
      "epoch:4 step:4613 [D loss: 0.672754, acc.: 55.47%] [G loss: 0.857317]\n",
      "epoch:4 step:4614 [D loss: 0.651698, acc.: 62.50%] [G loss: 0.827304]\n",
      "epoch:4 step:4615 [D loss: 0.679999, acc.: 57.81%] [G loss: 0.773841]\n",
      "epoch:4 step:4616 [D loss: 0.651019, acc.: 64.84%] [G loss: 0.836783]\n",
      "epoch:4 step:4617 [D loss: 0.675042, acc.: 53.12%] [G loss: 0.818973]\n",
      "epoch:4 step:4618 [D loss: 0.651769, acc.: 60.16%] [G loss: 0.838559]\n",
      "epoch:4 step:4619 [D loss: 0.668754, acc.: 60.16%] [G loss: 0.784879]\n",
      "epoch:4 step:4620 [D loss: 0.706633, acc.: 44.53%] [G loss: 0.775161]\n",
      "epoch:4 step:4621 [D loss: 0.669737, acc.: 61.72%] [G loss: 0.726148]\n",
      "epoch:4 step:4622 [D loss: 0.685086, acc.: 53.91%] [G loss: 0.747231]\n",
      "epoch:4 step:4623 [D loss: 0.726006, acc.: 43.75%] [G loss: 0.700833]\n",
      "epoch:4 step:4624 [D loss: 0.707414, acc.: 50.78%] [G loss: 0.751935]\n",
      "epoch:4 step:4625 [D loss: 0.702177, acc.: 50.00%] [G loss: 0.796465]\n",
      "epoch:4 step:4626 [D loss: 0.703633, acc.: 46.09%] [G loss: 0.801207]\n",
      "epoch:4 step:4627 [D loss: 0.630625, acc.: 71.88%] [G loss: 0.791572]\n",
      "epoch:4 step:4628 [D loss: 0.677868, acc.: 59.38%] [G loss: 0.750125]\n",
      "epoch:4 step:4629 [D loss: 0.644372, acc.: 66.41%] [G loss: 0.857603]\n",
      "epoch:4 step:4630 [D loss: 0.669906, acc.: 57.03%] [G loss: 0.799363]\n",
      "epoch:4 step:4631 [D loss: 0.674116, acc.: 52.34%] [G loss: 0.841647]\n",
      "epoch:4 step:4632 [D loss: 0.647508, acc.: 55.47%] [G loss: 0.784118]\n",
      "epoch:4 step:4633 [D loss: 0.723812, acc.: 46.09%] [G loss: 0.743044]\n",
      "epoch:4 step:4634 [D loss: 0.677643, acc.: 60.16%] [G loss: 0.802650]\n",
      "epoch:4 step:4635 [D loss: 0.724437, acc.: 53.91%] [G loss: 0.853086]\n",
      "epoch:4 step:4636 [D loss: 0.696197, acc.: 57.03%] [G loss: 0.825096]\n",
      "epoch:4 step:4637 [D loss: 0.693915, acc.: 54.69%] [G loss: 0.793403]\n",
      "epoch:4 step:4638 [D loss: 0.666306, acc.: 57.03%] [G loss: 0.765775]\n",
      "epoch:4 step:4639 [D loss: 0.699714, acc.: 52.34%] [G loss: 0.765772]\n",
      "epoch:4 step:4640 [D loss: 0.689104, acc.: 56.25%] [G loss: 0.767214]\n",
      "epoch:4 step:4641 [D loss: 0.687003, acc.: 53.91%] [G loss: 0.807107]\n",
      "epoch:4 step:4642 [D loss: 0.696021, acc.: 50.78%] [G loss: 0.779053]\n",
      "epoch:4 step:4643 [D loss: 0.658965, acc.: 61.72%] [G loss: 0.906145]\n",
      "epoch:4 step:4644 [D loss: 0.687549, acc.: 53.91%] [G loss: 0.827068]\n",
      "epoch:4 step:4645 [D loss: 0.678193, acc.: 53.12%] [G loss: 0.748387]\n",
      "epoch:4 step:4646 [D loss: 0.650697, acc.: 60.16%] [G loss: 0.755093]\n",
      "epoch:4 step:4647 [D loss: 0.716131, acc.: 52.34%] [G loss: 0.814535]\n",
      "epoch:4 step:4648 [D loss: 0.650220, acc.: 57.81%] [G loss: 0.895578]\n",
      "epoch:4 step:4649 [D loss: 0.706935, acc.: 47.66%] [G loss: 0.824237]\n",
      "epoch:4 step:4650 [D loss: 0.685326, acc.: 54.69%] [G loss: 0.873347]\n",
      "epoch:4 step:4651 [D loss: 0.707911, acc.: 46.09%] [G loss: 0.824651]\n",
      "epoch:4 step:4652 [D loss: 0.685073, acc.: 56.25%] [G loss: 0.806336]\n",
      "epoch:4 step:4653 [D loss: 0.714042, acc.: 50.00%] [G loss: 0.769187]\n",
      "epoch:4 step:4654 [D loss: 0.669863, acc.: 53.91%] [G loss: 0.809854]\n",
      "epoch:4 step:4655 [D loss: 0.721809, acc.: 50.78%] [G loss: 0.701565]\n",
      "epoch:4 step:4656 [D loss: 0.705724, acc.: 56.25%] [G loss: 0.790341]\n",
      "epoch:4 step:4657 [D loss: 0.693855, acc.: 49.22%] [G loss: 0.831492]\n",
      "epoch:4 step:4658 [D loss: 0.707098, acc.: 48.44%] [G loss: 0.802818]\n",
      "epoch:4 step:4659 [D loss: 0.712490, acc.: 56.25%] [G loss: 0.779533]\n",
      "epoch:4 step:4660 [D loss: 0.696236, acc.: 49.22%] [G loss: 0.768311]\n",
      "epoch:4 step:4661 [D loss: 0.676748, acc.: 63.28%] [G loss: 0.825170]\n",
      "epoch:4 step:4662 [D loss: 0.665253, acc.: 57.03%] [G loss: 0.794627]\n",
      "epoch:4 step:4663 [D loss: 0.676626, acc.: 53.12%] [G loss: 0.888087]\n",
      "epoch:4 step:4664 [D loss: 0.657874, acc.: 59.38%] [G loss: 0.810016]\n",
      "epoch:4 step:4665 [D loss: 0.681799, acc.: 57.03%] [G loss: 0.790285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4666 [D loss: 0.683347, acc.: 53.91%] [G loss: 0.771500]\n",
      "epoch:4 step:4667 [D loss: 0.751065, acc.: 35.16%] [G loss: 0.783551]\n",
      "epoch:4 step:4668 [D loss: 0.711914, acc.: 50.00%] [G loss: 0.813543]\n",
      "epoch:4 step:4669 [D loss: 0.707919, acc.: 50.00%] [G loss: 0.765784]\n",
      "epoch:4 step:4670 [D loss: 0.689643, acc.: 56.25%] [G loss: 0.810134]\n",
      "epoch:4 step:4671 [D loss: 0.678333, acc.: 52.34%] [G loss: 0.846747]\n",
      "epoch:4 step:4672 [D loss: 0.669275, acc.: 53.91%] [G loss: 0.829811]\n",
      "epoch:4 step:4673 [D loss: 0.683197, acc.: 58.59%] [G loss: 0.844223]\n",
      "epoch:4 step:4674 [D loss: 0.679910, acc.: 58.59%] [G loss: 0.809193]\n",
      "epoch:4 step:4675 [D loss: 0.702122, acc.: 53.91%] [G loss: 0.809420]\n",
      "epoch:4 step:4676 [D loss: 0.676422, acc.: 59.38%] [G loss: 0.779156]\n",
      "epoch:4 step:4677 [D loss: 0.679197, acc.: 53.12%] [G loss: 0.815740]\n",
      "epoch:4 step:4678 [D loss: 0.688470, acc.: 46.88%] [G loss: 0.766019]\n",
      "epoch:4 step:4679 [D loss: 0.682194, acc.: 53.91%] [G loss: 0.808848]\n",
      "epoch:4 step:4680 [D loss: 0.676053, acc.: 53.91%] [G loss: 0.823394]\n",
      "epoch:4 step:4681 [D loss: 0.693350, acc.: 53.91%] [G loss: 0.843706]\n",
      "epoch:4 step:4682 [D loss: 0.670058, acc.: 51.56%] [G loss: 0.839524]\n",
      "epoch:4 step:4683 [D loss: 0.681501, acc.: 63.28%] [G loss: 0.807792]\n",
      "epoch:4 step:4684 [D loss: 0.685881, acc.: 48.44%] [G loss: 0.838121]\n",
      "epoch:4 step:4685 [D loss: 0.726100, acc.: 45.31%] [G loss: 0.771395]\n",
      "epoch:5 step:4686 [D loss: 0.694264, acc.: 57.81%] [G loss: 0.714312]\n",
      "epoch:5 step:4687 [D loss: 0.713372, acc.: 53.12%] [G loss: 0.721489]\n",
      "epoch:5 step:4688 [D loss: 0.690934, acc.: 46.88%] [G loss: 0.714298]\n",
      "epoch:5 step:4689 [D loss: 0.701125, acc.: 51.56%] [G loss: 0.773235]\n",
      "epoch:5 step:4690 [D loss: 0.677892, acc.: 56.25%] [G loss: 0.777809]\n",
      "epoch:5 step:4691 [D loss: 0.662835, acc.: 55.47%] [G loss: 0.826847]\n",
      "epoch:5 step:4692 [D loss: 0.653875, acc.: 60.16%] [G loss: 0.832457]\n",
      "epoch:5 step:4693 [D loss: 0.678349, acc.: 57.03%] [G loss: 0.818708]\n",
      "epoch:5 step:4694 [D loss: 0.704738, acc.: 48.44%] [G loss: 0.842924]\n",
      "epoch:5 step:4695 [D loss: 0.681794, acc.: 56.25%] [G loss: 0.831141]\n",
      "epoch:5 step:4696 [D loss: 0.686033, acc.: 57.81%] [G loss: 0.769701]\n",
      "epoch:5 step:4697 [D loss: 0.682155, acc.: 50.78%] [G loss: 0.771676]\n",
      "epoch:5 step:4698 [D loss: 0.721988, acc.: 42.19%] [G loss: 0.773307]\n",
      "epoch:5 step:4699 [D loss: 0.691581, acc.: 54.69%] [G loss: 0.792246]\n",
      "epoch:5 step:4700 [D loss: 0.690337, acc.: 50.78%] [G loss: 0.761101]\n",
      "epoch:5 step:4701 [D loss: 0.680393, acc.: 53.91%] [G loss: 0.822927]\n",
      "epoch:5 step:4702 [D loss: 0.657851, acc.: 58.59%] [G loss: 0.823483]\n",
      "epoch:5 step:4703 [D loss: 0.688720, acc.: 51.56%] [G loss: 0.731041]\n",
      "epoch:5 step:4704 [D loss: 0.700692, acc.: 53.91%] [G loss: 0.725626]\n",
      "epoch:5 step:4705 [D loss: 0.665287, acc.: 60.94%] [G loss: 0.746340]\n",
      "epoch:5 step:4706 [D loss: 0.693201, acc.: 57.81%] [G loss: 0.802050]\n",
      "epoch:5 step:4707 [D loss: 0.676443, acc.: 58.59%] [G loss: 0.813702]\n",
      "epoch:5 step:4708 [D loss: 0.667311, acc.: 61.72%] [G loss: 0.783618]\n",
      "epoch:5 step:4709 [D loss: 0.654785, acc.: 60.16%] [G loss: 0.815537]\n",
      "epoch:5 step:4710 [D loss: 0.709406, acc.: 50.00%] [G loss: 0.797664]\n",
      "epoch:5 step:4711 [D loss: 0.701804, acc.: 51.56%] [G loss: 0.800332]\n",
      "epoch:5 step:4712 [D loss: 0.709318, acc.: 50.00%] [G loss: 0.793114]\n",
      "epoch:5 step:4713 [D loss: 0.671751, acc.: 60.94%] [G loss: 0.827998]\n",
      "epoch:5 step:4714 [D loss: 0.695852, acc.: 54.69%] [G loss: 0.874447]\n",
      "epoch:5 step:4715 [D loss: 0.668961, acc.: 52.34%] [G loss: 0.809607]\n",
      "epoch:5 step:4716 [D loss: 0.671683, acc.: 59.38%] [G loss: 0.872834]\n",
      "epoch:5 step:4717 [D loss: 0.676844, acc.: 52.34%] [G loss: 0.824134]\n",
      "epoch:5 step:4718 [D loss: 0.667415, acc.: 56.25%] [G loss: 0.846948]\n",
      "epoch:5 step:4719 [D loss: 0.687220, acc.: 51.56%] [G loss: 0.855337]\n",
      "epoch:5 step:4720 [D loss: 0.691890, acc.: 55.47%] [G loss: 0.773210]\n",
      "epoch:5 step:4721 [D loss: 0.685718, acc.: 50.00%] [G loss: 0.792157]\n",
      "epoch:5 step:4722 [D loss: 0.661133, acc.: 57.03%] [G loss: 0.802381]\n",
      "epoch:5 step:4723 [D loss: 0.665597, acc.: 56.25%] [G loss: 0.847964]\n",
      "epoch:5 step:4724 [D loss: 0.718357, acc.: 42.19%] [G loss: 0.766948]\n",
      "epoch:5 step:4725 [D loss: 0.680433, acc.: 53.12%] [G loss: 0.917683]\n",
      "epoch:5 step:4726 [D loss: 0.685472, acc.: 60.16%] [G loss: 0.854259]\n",
      "epoch:5 step:4727 [D loss: 0.679376, acc.: 51.56%] [G loss: 0.813493]\n",
      "epoch:5 step:4728 [D loss: 0.653580, acc.: 62.50%] [G loss: 0.806824]\n",
      "epoch:5 step:4729 [D loss: 0.692102, acc.: 50.78%] [G loss: 0.804065]\n",
      "epoch:5 step:4730 [D loss: 0.692214, acc.: 50.00%] [G loss: 0.766389]\n",
      "epoch:5 step:4731 [D loss: 0.651905, acc.: 57.81%] [G loss: 0.804402]\n",
      "epoch:5 step:4732 [D loss: 0.682324, acc.: 56.25%] [G loss: 0.757624]\n",
      "epoch:5 step:4733 [D loss: 0.703048, acc.: 46.09%] [G loss: 0.742438]\n",
      "epoch:5 step:4734 [D loss: 0.688744, acc.: 55.47%] [G loss: 0.871495]\n",
      "epoch:5 step:4735 [D loss: 0.677512, acc.: 53.91%] [G loss: 0.792777]\n",
      "epoch:5 step:4736 [D loss: 0.669689, acc.: 57.03%] [G loss: 0.782869]\n",
      "epoch:5 step:4737 [D loss: 0.654653, acc.: 63.28%] [G loss: 0.833338]\n",
      "epoch:5 step:4738 [D loss: 0.670770, acc.: 57.81%] [G loss: 0.833746]\n",
      "epoch:5 step:4739 [D loss: 0.695892, acc.: 53.91%] [G loss: 0.777187]\n",
      "epoch:5 step:4740 [D loss: 0.721004, acc.: 47.66%] [G loss: 0.853595]\n",
      "epoch:5 step:4741 [D loss: 0.699349, acc.: 52.34%] [G loss: 0.815957]\n",
      "epoch:5 step:4742 [D loss: 0.697404, acc.: 47.66%] [G loss: 0.818339]\n",
      "epoch:5 step:4743 [D loss: 0.680754, acc.: 54.69%] [G loss: 0.816062]\n",
      "epoch:5 step:4744 [D loss: 0.651748, acc.: 64.84%] [G loss: 0.830559]\n",
      "epoch:5 step:4745 [D loss: 0.698872, acc.: 50.00%] [G loss: 0.849988]\n",
      "epoch:5 step:4746 [D loss: 0.674323, acc.: 61.72%] [G loss: 0.829331]\n",
      "epoch:5 step:4747 [D loss: 0.670528, acc.: 55.47%] [G loss: 0.878758]\n",
      "epoch:5 step:4748 [D loss: 0.667657, acc.: 63.28%] [G loss: 0.861584]\n",
      "epoch:5 step:4749 [D loss: 0.667027, acc.: 55.47%] [G loss: 0.853466]\n",
      "epoch:5 step:4750 [D loss: 0.726016, acc.: 47.66%] [G loss: 0.808359]\n",
      "epoch:5 step:4751 [D loss: 0.679018, acc.: 54.69%] [G loss: 0.858527]\n",
      "epoch:5 step:4752 [D loss: 0.707032, acc.: 50.00%] [G loss: 0.761523]\n",
      "epoch:5 step:4753 [D loss: 0.711804, acc.: 47.66%] [G loss: 0.814754]\n",
      "epoch:5 step:4754 [D loss: 0.661376, acc.: 57.03%] [G loss: 0.762081]\n",
      "epoch:5 step:4755 [D loss: 0.674383, acc.: 58.59%] [G loss: 0.838914]\n",
      "epoch:5 step:4756 [D loss: 0.699905, acc.: 50.78%] [G loss: 0.823071]\n",
      "epoch:5 step:4757 [D loss: 0.698609, acc.: 52.34%] [G loss: 0.843916]\n",
      "epoch:5 step:4758 [D loss: 0.682142, acc.: 54.69%] [G loss: 0.905731]\n",
      "epoch:5 step:4759 [D loss: 0.707402, acc.: 49.22%] [G loss: 0.853711]\n",
      "epoch:5 step:4760 [D loss: 0.697023, acc.: 52.34%] [G loss: 0.772152]\n",
      "epoch:5 step:4761 [D loss: 0.684750, acc.: 48.44%] [G loss: 0.866558]\n",
      "epoch:5 step:4762 [D loss: 0.706048, acc.: 48.44%] [G loss: 0.763472]\n",
      "epoch:5 step:4763 [D loss: 0.680495, acc.: 56.25%] [G loss: 0.798789]\n",
      "epoch:5 step:4764 [D loss: 0.704484, acc.: 50.00%] [G loss: 0.765177]\n",
      "epoch:5 step:4765 [D loss: 0.699163, acc.: 53.12%] [G loss: 0.797960]\n",
      "epoch:5 step:4766 [D loss: 0.690929, acc.: 53.12%] [G loss: 0.795527]\n",
      "epoch:5 step:4767 [D loss: 0.700884, acc.: 50.78%] [G loss: 0.808362]\n",
      "epoch:5 step:4768 [D loss: 0.672477, acc.: 57.03%] [G loss: 0.793710]\n",
      "epoch:5 step:4769 [D loss: 0.686397, acc.: 50.78%] [G loss: 0.754098]\n",
      "epoch:5 step:4770 [D loss: 0.697595, acc.: 62.50%] [G loss: 0.816999]\n",
      "epoch:5 step:4771 [D loss: 0.680468, acc.: 51.56%] [G loss: 0.760169]\n",
      "epoch:5 step:4772 [D loss: 0.678371, acc.: 56.25%] [G loss: 0.793619]\n",
      "epoch:5 step:4773 [D loss: 0.696913, acc.: 50.00%] [G loss: 0.816008]\n",
      "epoch:5 step:4774 [D loss: 0.652788, acc.: 61.72%] [G loss: 0.801898]\n",
      "epoch:5 step:4775 [D loss: 0.656444, acc.: 59.38%] [G loss: 0.798827]\n",
      "epoch:5 step:4776 [D loss: 0.677993, acc.: 58.59%] [G loss: 0.899127]\n",
      "epoch:5 step:4777 [D loss: 0.672072, acc.: 57.03%] [G loss: 0.891490]\n",
      "epoch:5 step:4778 [D loss: 0.679862, acc.: 55.47%] [G loss: 0.907147]\n",
      "epoch:5 step:4779 [D loss: 0.664224, acc.: 57.03%] [G loss: 0.855189]\n",
      "epoch:5 step:4780 [D loss: 0.702454, acc.: 47.66%] [G loss: 0.911547]\n",
      "epoch:5 step:4781 [D loss: 0.653149, acc.: 64.06%] [G loss: 0.792747]\n",
      "epoch:5 step:4782 [D loss: 0.669400, acc.: 53.12%] [G loss: 0.801137]\n",
      "epoch:5 step:4783 [D loss: 0.688793, acc.: 53.91%] [G loss: 0.858243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4784 [D loss: 0.697791, acc.: 45.31%] [G loss: 0.810848]\n",
      "epoch:5 step:4785 [D loss: 0.681477, acc.: 50.00%] [G loss: 0.785316]\n",
      "epoch:5 step:4786 [D loss: 0.649401, acc.: 63.28%] [G loss: 0.853557]\n",
      "epoch:5 step:4787 [D loss: 0.653145, acc.: 57.03%] [G loss: 0.811877]\n",
      "epoch:5 step:4788 [D loss: 0.685011, acc.: 57.03%] [G loss: 0.834397]\n",
      "epoch:5 step:4789 [D loss: 0.671525, acc.: 60.94%] [G loss: 0.859453]\n",
      "epoch:5 step:4790 [D loss: 0.654275, acc.: 64.84%] [G loss: 0.806493]\n",
      "epoch:5 step:4791 [D loss: 0.679749, acc.: 55.47%] [G loss: 0.739036]\n",
      "epoch:5 step:4792 [D loss: 0.671520, acc.: 64.06%] [G loss: 0.787020]\n",
      "epoch:5 step:4793 [D loss: 0.700907, acc.: 49.22%] [G loss: 0.743278]\n",
      "epoch:5 step:4794 [D loss: 0.702820, acc.: 48.44%] [G loss: 0.731656]\n",
      "epoch:5 step:4795 [D loss: 0.683776, acc.: 60.94%] [G loss: 0.796138]\n",
      "epoch:5 step:4796 [D loss: 0.680329, acc.: 53.91%] [G loss: 0.846009]\n",
      "epoch:5 step:4797 [D loss: 0.681524, acc.: 53.91%] [G loss: 0.953025]\n",
      "epoch:5 step:4798 [D loss: 0.688869, acc.: 56.25%] [G loss: 0.901267]\n",
      "epoch:5 step:4799 [D loss: 0.690368, acc.: 56.25%] [G loss: 0.874145]\n",
      "epoch:5 step:4800 [D loss: 0.727735, acc.: 47.66%] [G loss: 0.829452]\n",
      "epoch:5 step:4801 [D loss: 0.672675, acc.: 58.59%] [G loss: 0.801773]\n",
      "epoch:5 step:4802 [D loss: 0.698901, acc.: 50.00%] [G loss: 0.763439]\n",
      "epoch:5 step:4803 [D loss: 0.714858, acc.: 51.56%] [G loss: 0.765923]\n",
      "epoch:5 step:4804 [D loss: 0.684675, acc.: 53.91%] [G loss: 0.794396]\n",
      "epoch:5 step:4805 [D loss: 0.681154, acc.: 53.91%] [G loss: 0.821782]\n",
      "epoch:5 step:4806 [D loss: 0.667195, acc.: 55.47%] [G loss: 0.816321]\n",
      "epoch:5 step:4807 [D loss: 0.678944, acc.: 56.25%] [G loss: 0.816244]\n",
      "epoch:5 step:4808 [D loss: 0.667544, acc.: 62.50%] [G loss: 0.791067]\n",
      "epoch:5 step:4809 [D loss: 0.665598, acc.: 57.81%] [G loss: 0.812514]\n",
      "epoch:5 step:4810 [D loss: 0.666514, acc.: 62.50%] [G loss: 0.840649]\n",
      "epoch:5 step:4811 [D loss: 0.686756, acc.: 49.22%] [G loss: 0.842510]\n",
      "epoch:5 step:4812 [D loss: 0.700022, acc.: 47.66%] [G loss: 0.818177]\n",
      "epoch:5 step:4813 [D loss: 0.687918, acc.: 50.00%] [G loss: 0.780544]\n",
      "epoch:5 step:4814 [D loss: 0.718625, acc.: 50.00%] [G loss: 0.814526]\n",
      "epoch:5 step:4815 [D loss: 0.696070, acc.: 50.00%] [G loss: 0.815199]\n",
      "epoch:5 step:4816 [D loss: 0.691668, acc.: 56.25%] [G loss: 0.827630]\n",
      "epoch:5 step:4817 [D loss: 0.686264, acc.: 50.78%] [G loss: 0.870097]\n",
      "epoch:5 step:4818 [D loss: 0.736749, acc.: 42.97%] [G loss: 0.820922]\n",
      "epoch:5 step:4819 [D loss: 0.705451, acc.: 50.00%] [G loss: 0.850709]\n",
      "epoch:5 step:4820 [D loss: 0.663126, acc.: 56.25%] [G loss: 0.818789]\n",
      "epoch:5 step:4821 [D loss: 0.671980, acc.: 52.34%] [G loss: 0.827215]\n",
      "epoch:5 step:4822 [D loss: 0.695435, acc.: 53.91%] [G loss: 0.843868]\n",
      "epoch:5 step:4823 [D loss: 0.713474, acc.: 42.97%] [G loss: 0.814454]\n",
      "epoch:5 step:4824 [D loss: 0.726865, acc.: 47.66%] [G loss: 0.816105]\n",
      "epoch:5 step:4825 [D loss: 0.661672, acc.: 60.16%] [G loss: 0.788144]\n",
      "epoch:5 step:4826 [D loss: 0.661731, acc.: 61.72%] [G loss: 0.778888]\n",
      "epoch:5 step:4827 [D loss: 0.673618, acc.: 56.25%] [G loss: 0.801493]\n",
      "epoch:5 step:4828 [D loss: 0.670610, acc.: 63.28%] [G loss: 0.887942]\n",
      "epoch:5 step:4829 [D loss: 0.672824, acc.: 56.25%] [G loss: 0.854980]\n",
      "epoch:5 step:4830 [D loss: 0.693953, acc.: 51.56%] [G loss: 0.829586]\n",
      "epoch:5 step:4831 [D loss: 0.672401, acc.: 53.12%] [G loss: 0.804668]\n",
      "epoch:5 step:4832 [D loss: 0.721963, acc.: 50.78%] [G loss: 0.768836]\n",
      "epoch:5 step:4833 [D loss: 0.666569, acc.: 60.94%] [G loss: 0.785982]\n",
      "epoch:5 step:4834 [D loss: 0.647228, acc.: 66.41%] [G loss: 0.790141]\n",
      "epoch:5 step:4835 [D loss: 0.675730, acc.: 57.81%] [G loss: 0.771565]\n",
      "epoch:5 step:4836 [D loss: 0.667356, acc.: 57.03%] [G loss: 0.801116]\n",
      "epoch:5 step:4837 [D loss: 0.668906, acc.: 58.59%] [G loss: 0.741491]\n",
      "epoch:5 step:4838 [D loss: 0.659201, acc.: 58.59%] [G loss: 0.744128]\n",
      "epoch:5 step:4839 [D loss: 0.702254, acc.: 51.56%] [G loss: 0.788257]\n",
      "epoch:5 step:4840 [D loss: 0.732608, acc.: 45.31%] [G loss: 0.809205]\n",
      "epoch:5 step:4841 [D loss: 0.698460, acc.: 55.47%] [G loss: 0.791924]\n",
      "epoch:5 step:4842 [D loss: 0.654585, acc.: 60.94%] [G loss: 0.811647]\n",
      "epoch:5 step:4843 [D loss: 0.644979, acc.: 59.38%] [G loss: 0.879485]\n",
      "epoch:5 step:4844 [D loss: 0.684858, acc.: 54.69%] [G loss: 0.804585]\n",
      "epoch:5 step:4845 [D loss: 0.673003, acc.: 56.25%] [G loss: 0.795205]\n",
      "epoch:5 step:4846 [D loss: 0.702986, acc.: 55.47%] [G loss: 0.853780]\n",
      "epoch:5 step:4847 [D loss: 0.697412, acc.: 46.88%] [G loss: 0.768902]\n",
      "epoch:5 step:4848 [D loss: 0.694694, acc.: 47.66%] [G loss: 0.848031]\n",
      "epoch:5 step:4849 [D loss: 0.657682, acc.: 61.72%] [G loss: 0.788513]\n",
      "epoch:5 step:4850 [D loss: 0.698376, acc.: 49.22%] [G loss: 0.853060]\n",
      "epoch:5 step:4851 [D loss: 0.677629, acc.: 58.59%] [G loss: 0.802241]\n",
      "epoch:5 step:4852 [D loss: 0.663257, acc.: 57.81%] [G loss: 0.762081]\n",
      "epoch:5 step:4853 [D loss: 0.676529, acc.: 55.47%] [G loss: 0.863893]\n",
      "epoch:5 step:4854 [D loss: 0.675824, acc.: 52.34%] [G loss: 0.822270]\n",
      "epoch:5 step:4855 [D loss: 0.682863, acc.: 55.47%] [G loss: 0.759898]\n",
      "epoch:5 step:4856 [D loss: 0.700974, acc.: 46.09%] [G loss: 0.803461]\n",
      "epoch:5 step:4857 [D loss: 0.696437, acc.: 51.56%] [G loss: 0.733141]\n",
      "epoch:5 step:4858 [D loss: 0.689779, acc.: 53.12%] [G loss: 0.752033]\n",
      "epoch:5 step:4859 [D loss: 0.684390, acc.: 54.69%] [G loss: 0.784314]\n",
      "epoch:5 step:4860 [D loss: 0.721742, acc.: 43.75%] [G loss: 0.748531]\n",
      "epoch:5 step:4861 [D loss: 0.705262, acc.: 51.56%] [G loss: 0.800922]\n",
      "epoch:5 step:4862 [D loss: 0.666114, acc.: 57.81%] [G loss: 0.820116]\n",
      "epoch:5 step:4863 [D loss: 0.693961, acc.: 60.16%] [G loss: 0.902695]\n",
      "epoch:5 step:4864 [D loss: 0.680644, acc.: 50.00%] [G loss: 0.821729]\n",
      "epoch:5 step:4865 [D loss: 0.689664, acc.: 54.69%] [G loss: 0.835265]\n",
      "epoch:5 step:4866 [D loss: 0.677770, acc.: 56.25%] [G loss: 0.827615]\n",
      "epoch:5 step:4867 [D loss: 0.682197, acc.: 52.34%] [G loss: 0.812791]\n",
      "epoch:5 step:4868 [D loss: 0.704967, acc.: 53.12%] [G loss: 0.774553]\n",
      "epoch:5 step:4869 [D loss: 0.667334, acc.: 58.59%] [G loss: 0.781927]\n",
      "epoch:5 step:4870 [D loss: 0.714251, acc.: 46.88%] [G loss: 0.780465]\n",
      "epoch:5 step:4871 [D loss: 0.698902, acc.: 48.44%] [G loss: 0.715533]\n",
      "epoch:5 step:4872 [D loss: 0.701388, acc.: 53.12%] [G loss: 0.769892]\n",
      "epoch:5 step:4873 [D loss: 0.701511, acc.: 55.47%] [G loss: 0.810307]\n",
      "epoch:5 step:4874 [D loss: 0.685418, acc.: 55.47%] [G loss: 0.810033]\n",
      "epoch:5 step:4875 [D loss: 0.676735, acc.: 58.59%] [G loss: 0.808455]\n",
      "epoch:5 step:4876 [D loss: 0.696083, acc.: 47.66%] [G loss: 0.759097]\n",
      "epoch:5 step:4877 [D loss: 0.682518, acc.: 56.25%] [G loss: 0.815684]\n",
      "epoch:5 step:4878 [D loss: 0.676841, acc.: 59.38%] [G loss: 0.832743]\n",
      "epoch:5 step:4879 [D loss: 0.717125, acc.: 48.44%] [G loss: 0.784554]\n",
      "epoch:5 step:4880 [D loss: 0.675088, acc.: 52.34%] [G loss: 0.848800]\n",
      "epoch:5 step:4881 [D loss: 0.694953, acc.: 54.69%] [G loss: 0.832139]\n",
      "epoch:5 step:4882 [D loss: 0.690773, acc.: 50.00%] [G loss: 0.836846]\n",
      "epoch:5 step:4883 [D loss: 0.689792, acc.: 51.56%] [G loss: 0.794192]\n",
      "epoch:5 step:4884 [D loss: 0.682951, acc.: 54.69%] [G loss: 0.822893]\n",
      "epoch:5 step:4885 [D loss: 0.707850, acc.: 48.44%] [G loss: 0.806596]\n",
      "epoch:5 step:4886 [D loss: 0.685650, acc.: 55.47%] [G loss: 0.787187]\n",
      "epoch:5 step:4887 [D loss: 0.687916, acc.: 54.69%] [G loss: 0.779478]\n",
      "epoch:5 step:4888 [D loss: 0.693998, acc.: 53.12%] [G loss: 0.768553]\n",
      "epoch:5 step:4889 [D loss: 0.677406, acc.: 57.03%] [G loss: 0.825299]\n",
      "epoch:5 step:4890 [D loss: 0.674190, acc.: 52.34%] [G loss: 0.802873]\n",
      "epoch:5 step:4891 [D loss: 0.659546, acc.: 55.47%] [G loss: 0.797998]\n",
      "epoch:5 step:4892 [D loss: 0.640294, acc.: 61.72%] [G loss: 0.834127]\n",
      "epoch:5 step:4893 [D loss: 0.698211, acc.: 57.03%] [G loss: 0.837147]\n",
      "epoch:5 step:4894 [D loss: 0.698582, acc.: 50.00%] [G loss: 0.825054]\n",
      "epoch:5 step:4895 [D loss: 0.643335, acc.: 67.19%] [G loss: 0.846682]\n",
      "epoch:5 step:4896 [D loss: 0.688414, acc.: 56.25%] [G loss: 0.808487]\n",
      "epoch:5 step:4897 [D loss: 0.705777, acc.: 49.22%] [G loss: 0.792727]\n",
      "epoch:5 step:4898 [D loss: 0.720576, acc.: 47.66%] [G loss: 0.773863]\n",
      "epoch:5 step:4899 [D loss: 0.731679, acc.: 39.84%] [G loss: 0.766632]\n",
      "epoch:5 step:4900 [D loss: 0.710374, acc.: 53.91%] [G loss: 0.771559]\n",
      "epoch:5 step:4901 [D loss: 0.668173, acc.: 50.78%] [G loss: 0.794166]\n",
      "epoch:5 step:4902 [D loss: 0.693903, acc.: 53.91%] [G loss: 0.802496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4903 [D loss: 0.701748, acc.: 39.06%] [G loss: 0.759909]\n",
      "epoch:5 step:4904 [D loss: 0.669289, acc.: 57.81%] [G loss: 0.841808]\n",
      "epoch:5 step:4905 [D loss: 0.689039, acc.: 53.12%] [G loss: 0.793815]\n",
      "epoch:5 step:4906 [D loss: 0.665612, acc.: 60.16%] [G loss: 0.799198]\n",
      "epoch:5 step:4907 [D loss: 0.690881, acc.: 50.00%] [G loss: 0.767519]\n",
      "epoch:5 step:4908 [D loss: 0.699124, acc.: 55.47%] [G loss: 0.759665]\n",
      "epoch:5 step:4909 [D loss: 0.669365, acc.: 55.47%] [G loss: 0.860996]\n",
      "epoch:5 step:4910 [D loss: 0.692832, acc.: 49.22%] [G loss: 0.788071]\n",
      "epoch:5 step:4911 [D loss: 0.694172, acc.: 50.78%] [G loss: 0.748082]\n",
      "epoch:5 step:4912 [D loss: 0.711474, acc.: 53.12%] [G loss: 0.789973]\n",
      "epoch:5 step:4913 [D loss: 0.700452, acc.: 50.00%] [G loss: 0.784852]\n",
      "epoch:5 step:4914 [D loss: 0.680726, acc.: 57.03%] [G loss: 0.789599]\n",
      "epoch:5 step:4915 [D loss: 0.707025, acc.: 49.22%] [G loss: 0.741795]\n",
      "epoch:5 step:4916 [D loss: 0.690722, acc.: 51.56%] [G loss: 0.725988]\n",
      "epoch:5 step:4917 [D loss: 0.706895, acc.: 46.09%] [G loss: 0.771688]\n",
      "epoch:5 step:4918 [D loss: 0.715121, acc.: 47.66%] [G loss: 0.731128]\n",
      "epoch:5 step:4919 [D loss: 0.717724, acc.: 41.41%] [G loss: 0.713215]\n",
      "epoch:5 step:4920 [D loss: 0.707646, acc.: 46.88%] [G loss: 0.750895]\n",
      "epoch:5 step:4921 [D loss: 0.714927, acc.: 48.44%] [G loss: 0.805312]\n",
      "epoch:5 step:4922 [D loss: 0.692849, acc.: 53.12%] [G loss: 0.787396]\n",
      "epoch:5 step:4923 [D loss: 0.666313, acc.: 60.16%] [G loss: 0.761554]\n",
      "epoch:5 step:4924 [D loss: 0.681660, acc.: 55.47%] [G loss: 0.765191]\n",
      "epoch:5 step:4925 [D loss: 0.667542, acc.: 60.94%] [G loss: 0.833070]\n",
      "epoch:5 step:4926 [D loss: 0.670699, acc.: 58.59%] [G loss: 0.762424]\n",
      "epoch:5 step:4927 [D loss: 0.690931, acc.: 50.00%] [G loss: 0.815654]\n",
      "epoch:5 step:4928 [D loss: 0.704991, acc.: 49.22%] [G loss: 0.789993]\n",
      "epoch:5 step:4929 [D loss: 0.687166, acc.: 53.12%] [G loss: 0.784018]\n",
      "epoch:5 step:4930 [D loss: 0.696235, acc.: 53.12%] [G loss: 0.799186]\n",
      "epoch:5 step:4931 [D loss: 0.697311, acc.: 47.66%] [G loss: 0.770854]\n",
      "epoch:5 step:4932 [D loss: 0.663839, acc.: 55.47%] [G loss: 0.785955]\n",
      "epoch:5 step:4933 [D loss: 0.697582, acc.: 50.78%] [G loss: 0.798765]\n",
      "epoch:5 step:4934 [D loss: 0.689705, acc.: 46.09%] [G loss: 0.801808]\n",
      "epoch:5 step:4935 [D loss: 0.678009, acc.: 56.25%] [G loss: 0.800497]\n",
      "epoch:5 step:4936 [D loss: 0.678287, acc.: 64.84%] [G loss: 0.758395]\n",
      "epoch:5 step:4937 [D loss: 0.632947, acc.: 66.41%] [G loss: 0.819941]\n",
      "epoch:5 step:4938 [D loss: 0.704352, acc.: 45.31%] [G loss: 0.798384]\n",
      "epoch:5 step:4939 [D loss: 0.678866, acc.: 56.25%] [G loss: 0.742007]\n",
      "epoch:5 step:4940 [D loss: 0.671591, acc.: 60.94%] [G loss: 0.824547]\n",
      "epoch:5 step:4941 [D loss: 0.686196, acc.: 57.03%] [G loss: 0.807678]\n",
      "epoch:5 step:4942 [D loss: 0.681968, acc.: 54.69%] [G loss: 0.853092]\n",
      "epoch:5 step:4943 [D loss: 0.661997, acc.: 53.91%] [G loss: 0.844544]\n",
      "epoch:5 step:4944 [D loss: 0.683022, acc.: 52.34%] [G loss: 0.792369]\n",
      "epoch:5 step:4945 [D loss: 0.689488, acc.: 53.12%] [G loss: 0.843804]\n",
      "epoch:5 step:4946 [D loss: 0.673407, acc.: 57.03%] [G loss: 0.826304]\n",
      "epoch:5 step:4947 [D loss: 0.661192, acc.: 57.81%] [G loss: 0.829098]\n",
      "epoch:5 step:4948 [D loss: 0.695333, acc.: 53.12%] [G loss: 0.801512]\n",
      "epoch:5 step:4949 [D loss: 0.726068, acc.: 51.56%] [G loss: 0.754058]\n",
      "epoch:5 step:4950 [D loss: 0.664615, acc.: 63.28%] [G loss: 0.816573]\n",
      "epoch:5 step:4951 [D loss: 0.653257, acc.: 64.06%] [G loss: 0.799482]\n",
      "epoch:5 step:4952 [D loss: 0.701041, acc.: 52.34%] [G loss: 0.796149]\n",
      "epoch:5 step:4953 [D loss: 0.681381, acc.: 54.69%] [G loss: 0.810806]\n",
      "epoch:5 step:4954 [D loss: 0.685932, acc.: 55.47%] [G loss: 0.767601]\n",
      "epoch:5 step:4955 [D loss: 0.646052, acc.: 62.50%] [G loss: 0.835829]\n",
      "epoch:5 step:4956 [D loss: 0.684763, acc.: 54.69%] [G loss: 0.846209]\n",
      "epoch:5 step:4957 [D loss: 0.664950, acc.: 61.72%] [G loss: 0.830631]\n",
      "epoch:5 step:4958 [D loss: 0.677174, acc.: 55.47%] [G loss: 0.767702]\n",
      "epoch:5 step:4959 [D loss: 0.662466, acc.: 60.94%] [G loss: 0.823265]\n",
      "epoch:5 step:4960 [D loss: 0.688332, acc.: 49.22%] [G loss: 0.785607]\n",
      "epoch:5 step:4961 [D loss: 0.687300, acc.: 53.91%] [G loss: 0.796223]\n",
      "epoch:5 step:4962 [D loss: 0.712136, acc.: 50.78%] [G loss: 0.790610]\n",
      "epoch:5 step:4963 [D loss: 0.653668, acc.: 54.69%] [G loss: 0.998308]\n",
      "epoch:5 step:4964 [D loss: 0.683926, acc.: 51.56%] [G loss: 0.857210]\n",
      "epoch:5 step:4965 [D loss: 0.697440, acc.: 57.03%] [G loss: 0.812625]\n",
      "epoch:5 step:4966 [D loss: 0.718678, acc.: 45.31%] [G loss: 0.805493]\n",
      "epoch:5 step:4967 [D loss: 0.674449, acc.: 54.69%] [G loss: 0.812014]\n",
      "epoch:5 step:4968 [D loss: 0.643343, acc.: 61.72%] [G loss: 0.862732]\n",
      "epoch:5 step:4969 [D loss: 0.666688, acc.: 61.72%] [G loss: 0.787504]\n",
      "epoch:5 step:4970 [D loss: 0.676619, acc.: 54.69%] [G loss: 0.742204]\n",
      "epoch:5 step:4971 [D loss: 0.690886, acc.: 50.00%] [G loss: 0.818950]\n",
      "epoch:5 step:4972 [D loss: 0.672656, acc.: 53.91%] [G loss: 0.859746]\n",
      "epoch:5 step:4973 [D loss: 0.718655, acc.: 50.00%] [G loss: 0.795641]\n",
      "epoch:5 step:4974 [D loss: 0.724076, acc.: 50.78%] [G loss: 0.807499]\n",
      "epoch:5 step:4975 [D loss: 0.652730, acc.: 66.41%] [G loss: 0.768685]\n",
      "epoch:5 step:4976 [D loss: 0.642629, acc.: 67.97%] [G loss: 0.776370]\n",
      "epoch:5 step:4977 [D loss: 0.701429, acc.: 45.31%] [G loss: 0.752530]\n",
      "epoch:5 step:4978 [D loss: 0.674386, acc.: 51.56%] [G loss: 0.786202]\n",
      "epoch:5 step:4979 [D loss: 0.648465, acc.: 62.50%] [G loss: 0.785228]\n",
      "epoch:5 step:4980 [D loss: 0.700898, acc.: 51.56%] [G loss: 0.830507]\n",
      "epoch:5 step:4981 [D loss: 0.666338, acc.: 53.91%] [G loss: 0.858176]\n",
      "epoch:5 step:4982 [D loss: 0.649782, acc.: 65.62%] [G loss: 0.869869]\n",
      "epoch:5 step:4983 [D loss: 0.669680, acc.: 57.03%] [G loss: 0.886991]\n",
      "epoch:5 step:4984 [D loss: 0.681208, acc.: 58.59%] [G loss: 0.869522]\n",
      "epoch:5 step:4985 [D loss: 0.675790, acc.: 61.72%] [G loss: 0.884243]\n",
      "epoch:5 step:4986 [D loss: 0.718121, acc.: 53.12%] [G loss: 0.891624]\n",
      "epoch:5 step:4987 [D loss: 0.664914, acc.: 53.91%] [G loss: 0.886401]\n",
      "epoch:5 step:4988 [D loss: 0.648385, acc.: 60.16%] [G loss: 0.902638]\n",
      "epoch:5 step:4989 [D loss: 0.725942, acc.: 48.44%] [G loss: 0.731655]\n",
      "epoch:5 step:4990 [D loss: 0.692436, acc.: 46.09%] [G loss: 0.808639]\n",
      "epoch:5 step:4991 [D loss: 0.663955, acc.: 54.69%] [G loss: 0.856288]\n",
      "epoch:5 step:4992 [D loss: 0.686899, acc.: 51.56%] [G loss: 0.776619]\n",
      "epoch:5 step:4993 [D loss: 0.710801, acc.: 48.44%] [G loss: 0.804791]\n",
      "epoch:5 step:4994 [D loss: 0.651168, acc.: 61.72%] [G loss: 0.746414]\n",
      "epoch:5 step:4995 [D loss: 0.750056, acc.: 47.66%] [G loss: 0.747774]\n",
      "epoch:5 step:4996 [D loss: 0.679222, acc.: 59.38%] [G loss: 0.744092]\n",
      "epoch:5 step:4997 [D loss: 0.667942, acc.: 57.81%] [G loss: 0.749494]\n",
      "epoch:5 step:4998 [D loss: 0.662799, acc.: 61.72%] [G loss: 0.768104]\n",
      "epoch:5 step:4999 [D loss: 0.710741, acc.: 56.25%] [G loss: 0.790112]\n",
      "epoch:5 step:5000 [D loss: 0.654758, acc.: 67.97%] [G loss: 0.842524]\n",
      "epoch:5 step:5001 [D loss: 0.657312, acc.: 53.12%] [G loss: 0.819967]\n",
      "epoch:5 step:5002 [D loss: 0.650349, acc.: 60.94%] [G loss: 0.793039]\n",
      "epoch:5 step:5003 [D loss: 0.686350, acc.: 50.78%] [G loss: 0.815074]\n",
      "epoch:5 step:5004 [D loss: 0.672946, acc.: 62.50%] [G loss: 0.758005]\n",
      "epoch:5 step:5005 [D loss: 0.654268, acc.: 64.84%] [G loss: 0.827542]\n",
      "epoch:5 step:5006 [D loss: 0.743083, acc.: 42.97%] [G loss: 0.828095]\n",
      "epoch:5 step:5007 [D loss: 0.670326, acc.: 60.94%] [G loss: 0.822737]\n",
      "epoch:5 step:5008 [D loss: 0.708873, acc.: 52.34%] [G loss: 0.817543]\n",
      "epoch:5 step:5009 [D loss: 0.674215, acc.: 53.12%] [G loss: 0.828449]\n",
      "epoch:5 step:5010 [D loss: 0.672025, acc.: 59.38%] [G loss: 0.822220]\n",
      "epoch:5 step:5011 [D loss: 0.669371, acc.: 58.59%] [G loss: 0.848998]\n",
      "epoch:5 step:5012 [D loss: 0.622481, acc.: 67.97%] [G loss: 0.924475]\n",
      "epoch:5 step:5013 [D loss: 0.650727, acc.: 63.28%] [G loss: 0.862157]\n",
      "epoch:5 step:5014 [D loss: 0.666837, acc.: 57.03%] [G loss: 0.876906]\n",
      "epoch:5 step:5015 [D loss: 0.684735, acc.: 56.25%] [G loss: 0.821164]\n",
      "epoch:5 step:5016 [D loss: 0.689067, acc.: 52.34%] [G loss: 0.836778]\n",
      "epoch:5 step:5017 [D loss: 0.741269, acc.: 46.09%] [G loss: 0.809717]\n",
      "epoch:5 step:5018 [D loss: 0.682746, acc.: 55.47%] [G loss: 0.907916]\n",
      "epoch:5 step:5019 [D loss: 0.661447, acc.: 59.38%] [G loss: 0.790899]\n",
      "epoch:5 step:5020 [D loss: 0.703898, acc.: 51.56%] [G loss: 0.784294]\n",
      "epoch:5 step:5021 [D loss: 0.710262, acc.: 52.34%] [G loss: 0.835869]\n",
      "epoch:5 step:5022 [D loss: 0.664736, acc.: 60.94%] [G loss: 0.851150]\n",
      "epoch:5 step:5023 [D loss: 0.668144, acc.: 59.38%] [G loss: 0.790304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5024 [D loss: 0.704310, acc.: 46.88%] [G loss: 0.806894]\n",
      "epoch:5 step:5025 [D loss: 0.682231, acc.: 52.34%] [G loss: 0.808447]\n",
      "epoch:5 step:5026 [D loss: 0.701929, acc.: 53.12%] [G loss: 0.819032]\n",
      "epoch:5 step:5027 [D loss: 0.703597, acc.: 50.78%] [G loss: 0.776374]\n",
      "epoch:5 step:5028 [D loss: 0.717304, acc.: 52.34%] [G loss: 0.818288]\n",
      "epoch:5 step:5029 [D loss: 0.707249, acc.: 55.47%] [G loss: 0.786926]\n",
      "epoch:5 step:5030 [D loss: 0.697042, acc.: 52.34%] [G loss: 0.807659]\n",
      "epoch:5 step:5031 [D loss: 0.662725, acc.: 56.25%] [G loss: 0.808940]\n",
      "epoch:5 step:5032 [D loss: 0.695083, acc.: 47.66%] [G loss: 0.871331]\n",
      "epoch:5 step:5033 [D loss: 0.684235, acc.: 57.81%] [G loss: 0.807152]\n",
      "epoch:5 step:5034 [D loss: 0.720198, acc.: 55.47%] [G loss: 0.833800]\n",
      "epoch:5 step:5035 [D loss: 0.663279, acc.: 61.72%] [G loss: 0.842996]\n",
      "epoch:5 step:5036 [D loss: 0.697849, acc.: 56.25%] [G loss: 0.835481]\n",
      "epoch:5 step:5037 [D loss: 0.688359, acc.: 52.34%] [G loss: 0.768296]\n",
      "epoch:5 step:5038 [D loss: 0.732884, acc.: 48.44%] [G loss: 0.766288]\n",
      "epoch:5 step:5039 [D loss: 0.740066, acc.: 44.53%] [G loss: 0.799098]\n",
      "epoch:5 step:5040 [D loss: 0.685150, acc.: 54.69%] [G loss: 0.770641]\n",
      "epoch:5 step:5041 [D loss: 0.691686, acc.: 51.56%] [G loss: 0.819852]\n",
      "epoch:5 step:5042 [D loss: 0.667525, acc.: 58.59%] [G loss: 0.848335]\n",
      "epoch:5 step:5043 [D loss: 0.686915, acc.: 53.12%] [G loss: 0.777883]\n",
      "epoch:5 step:5044 [D loss: 0.715034, acc.: 50.78%] [G loss: 0.777261]\n",
      "epoch:5 step:5045 [D loss: 0.687366, acc.: 52.34%] [G loss: 0.776714]\n",
      "epoch:5 step:5046 [D loss: 0.686716, acc.: 53.91%] [G loss: 0.803107]\n",
      "epoch:5 step:5047 [D loss: 0.741550, acc.: 41.41%] [G loss: 0.786645]\n",
      "epoch:5 step:5048 [D loss: 0.669729, acc.: 55.47%] [G loss: 0.809757]\n",
      "epoch:5 step:5049 [D loss: 0.691923, acc.: 48.44%] [G loss: 0.771206]\n",
      "epoch:5 step:5050 [D loss: 0.639469, acc.: 66.41%] [G loss: 0.785767]\n",
      "epoch:5 step:5051 [D loss: 0.645752, acc.: 64.06%] [G loss: 0.895720]\n",
      "epoch:5 step:5052 [D loss: 0.691283, acc.: 58.59%] [G loss: 0.735179]\n",
      "epoch:5 step:5053 [D loss: 0.690168, acc.: 52.34%] [G loss: 0.754153]\n",
      "epoch:5 step:5054 [D loss: 0.677142, acc.: 55.47%] [G loss: 0.802901]\n",
      "epoch:5 step:5055 [D loss: 0.697218, acc.: 52.34%] [G loss: 0.766514]\n",
      "epoch:5 step:5056 [D loss: 0.706401, acc.: 53.12%] [G loss: 0.765495]\n",
      "epoch:5 step:5057 [D loss: 0.674252, acc.: 64.06%] [G loss: 0.821616]\n",
      "epoch:5 step:5058 [D loss: 0.664081, acc.: 52.34%] [G loss: 0.811985]\n",
      "epoch:5 step:5059 [D loss: 0.700600, acc.: 53.91%] [G loss: 0.758282]\n",
      "epoch:5 step:5060 [D loss: 0.695188, acc.: 50.00%] [G loss: 0.755722]\n",
      "epoch:5 step:5061 [D loss: 0.692634, acc.: 50.78%] [G loss: 0.753880]\n",
      "epoch:5 step:5062 [D loss: 0.695594, acc.: 51.56%] [G loss: 0.803005]\n",
      "epoch:5 step:5063 [D loss: 0.682140, acc.: 56.25%] [G loss: 0.748580]\n",
      "epoch:5 step:5064 [D loss: 0.675717, acc.: 60.94%] [G loss: 0.774787]\n",
      "epoch:5 step:5065 [D loss: 0.683817, acc.: 55.47%] [G loss: 0.754750]\n",
      "epoch:5 step:5066 [D loss: 0.687422, acc.: 50.00%] [G loss: 0.755301]\n",
      "epoch:5 step:5067 [D loss: 0.689121, acc.: 49.22%] [G loss: 0.827056]\n",
      "epoch:5 step:5068 [D loss: 0.669137, acc.: 58.59%] [G loss: 0.787771]\n",
      "epoch:5 step:5069 [D loss: 0.703069, acc.: 49.22%] [G loss: 0.795845]\n",
      "epoch:5 step:5070 [D loss: 0.697409, acc.: 51.56%] [G loss: 0.759090]\n",
      "epoch:5 step:5071 [D loss: 0.704281, acc.: 50.78%] [G loss: 0.783035]\n",
      "epoch:5 step:5072 [D loss: 0.656892, acc.: 64.84%] [G loss: 0.784967]\n",
      "epoch:5 step:5073 [D loss: 0.688219, acc.: 50.78%] [G loss: 0.787324]\n",
      "epoch:5 step:5074 [D loss: 0.698218, acc.: 55.47%] [G loss: 0.822927]\n",
      "epoch:5 step:5075 [D loss: 0.696579, acc.: 50.78%] [G loss: 0.849652]\n",
      "epoch:5 step:5076 [D loss: 0.696233, acc.: 46.88%] [G loss: 0.805593]\n",
      "epoch:5 step:5077 [D loss: 0.698836, acc.: 50.78%] [G loss: 0.815215]\n",
      "epoch:5 step:5078 [D loss: 0.707593, acc.: 51.56%] [G loss: 0.823667]\n",
      "epoch:5 step:5079 [D loss: 0.677369, acc.: 54.69%] [G loss: 0.789776]\n",
      "epoch:5 step:5080 [D loss: 0.706824, acc.: 49.22%] [G loss: 0.820160]\n",
      "epoch:5 step:5081 [D loss: 0.697187, acc.: 47.66%] [G loss: 0.778759]\n",
      "epoch:5 step:5082 [D loss: 0.688311, acc.: 53.12%] [G loss: 0.753874]\n",
      "epoch:5 step:5083 [D loss: 0.656938, acc.: 64.84%] [G loss: 0.778296]\n",
      "epoch:5 step:5084 [D loss: 0.682801, acc.: 59.38%] [G loss: 0.789507]\n",
      "epoch:5 step:5085 [D loss: 0.685104, acc.: 54.69%] [G loss: 0.807001]\n",
      "epoch:5 step:5086 [D loss: 0.691393, acc.: 54.69%] [G loss: 0.744216]\n",
      "epoch:5 step:5087 [D loss: 0.656985, acc.: 55.47%] [G loss: 0.740523]\n",
      "epoch:5 step:5088 [D loss: 0.695808, acc.: 51.56%] [G loss: 0.812207]\n",
      "epoch:5 step:5089 [D loss: 0.706068, acc.: 44.53%] [G loss: 0.820895]\n",
      "epoch:5 step:5090 [D loss: 0.689525, acc.: 53.91%] [G loss: 0.777466]\n",
      "epoch:5 step:5091 [D loss: 0.666226, acc.: 58.59%] [G loss: 0.777398]\n",
      "epoch:5 step:5092 [D loss: 0.667331, acc.: 56.25%] [G loss: 0.828185]\n",
      "epoch:5 step:5093 [D loss: 0.681711, acc.: 61.72%] [G loss: 0.827148]\n",
      "epoch:5 step:5094 [D loss: 0.642344, acc.: 67.97%] [G loss: 0.813567]\n",
      "epoch:5 step:5095 [D loss: 0.655360, acc.: 64.06%] [G loss: 0.838665]\n",
      "epoch:5 step:5096 [D loss: 0.666517, acc.: 53.91%] [G loss: 0.811905]\n",
      "epoch:5 step:5097 [D loss: 0.706457, acc.: 49.22%] [G loss: 0.768895]\n",
      "epoch:5 step:5098 [D loss: 0.674414, acc.: 57.03%] [G loss: 0.804450]\n",
      "epoch:5 step:5099 [D loss: 0.669387, acc.: 55.47%] [G loss: 0.874415]\n",
      "epoch:5 step:5100 [D loss: 0.706185, acc.: 50.00%] [G loss: 0.885071]\n",
      "epoch:5 step:5101 [D loss: 0.687396, acc.: 53.91%] [G loss: 0.817774]\n",
      "epoch:5 step:5102 [D loss: 0.709720, acc.: 47.66%] [G loss: 0.827119]\n",
      "epoch:5 step:5103 [D loss: 0.683288, acc.: 53.91%] [G loss: 0.793326]\n",
      "epoch:5 step:5104 [D loss: 0.708386, acc.: 46.09%] [G loss: 0.813969]\n",
      "epoch:5 step:5105 [D loss: 0.703539, acc.: 50.78%] [G loss: 0.853922]\n",
      "epoch:5 step:5106 [D loss: 0.686475, acc.: 50.78%] [G loss: 0.851307]\n",
      "epoch:5 step:5107 [D loss: 0.684201, acc.: 54.69%] [G loss: 0.788059]\n",
      "epoch:5 step:5108 [D loss: 0.690428, acc.: 58.59%] [G loss: 0.769488]\n",
      "epoch:5 step:5109 [D loss: 0.672308, acc.: 60.16%] [G loss: 0.833865]\n",
      "epoch:5 step:5110 [D loss: 0.683972, acc.: 56.25%] [G loss: 0.790497]\n",
      "epoch:5 step:5111 [D loss: 0.689504, acc.: 57.03%] [G loss: 0.822748]\n",
      "epoch:5 step:5112 [D loss: 0.692632, acc.: 53.91%] [G loss: 0.823018]\n",
      "epoch:5 step:5113 [D loss: 0.695115, acc.: 49.22%] [G loss: 0.807154]\n",
      "epoch:5 step:5114 [D loss: 0.693604, acc.: 52.34%] [G loss: 0.736990]\n",
      "epoch:5 step:5115 [D loss: 0.689136, acc.: 58.59%] [G loss: 0.775370]\n",
      "epoch:5 step:5116 [D loss: 0.684641, acc.: 56.25%] [G loss: 0.816706]\n",
      "epoch:5 step:5117 [D loss: 0.723634, acc.: 50.00%] [G loss: 0.784464]\n",
      "epoch:5 step:5118 [D loss: 0.721253, acc.: 46.09%] [G loss: 0.751297]\n",
      "epoch:5 step:5119 [D loss: 0.658872, acc.: 64.06%] [G loss: 0.823787]\n",
      "epoch:5 step:5120 [D loss: 0.691701, acc.: 53.91%] [G loss: 0.739380]\n",
      "epoch:5 step:5121 [D loss: 0.683926, acc.: 50.78%] [G loss: 0.760385]\n",
      "epoch:5 step:5122 [D loss: 0.674506, acc.: 56.25%] [G loss: 0.765507]\n",
      "epoch:5 step:5123 [D loss: 0.660958, acc.: 55.47%] [G loss: 0.783909]\n",
      "epoch:5 step:5124 [D loss: 0.671847, acc.: 55.47%] [G loss: 0.773382]\n",
      "epoch:5 step:5125 [D loss: 0.684496, acc.: 57.81%] [G loss: 0.806686]\n",
      "epoch:5 step:5126 [D loss: 0.686367, acc.: 53.91%] [G loss: 0.777308]\n",
      "epoch:5 step:5127 [D loss: 0.701260, acc.: 44.53%] [G loss: 0.750470]\n",
      "epoch:5 step:5128 [D loss: 0.679782, acc.: 60.94%] [G loss: 0.757273]\n",
      "epoch:5 step:5129 [D loss: 0.667550, acc.: 55.47%] [G loss: 0.832067]\n",
      "epoch:5 step:5130 [D loss: 0.694590, acc.: 51.56%] [G loss: 0.808930]\n",
      "epoch:5 step:5131 [D loss: 0.667575, acc.: 56.25%] [G loss: 0.811288]\n",
      "epoch:5 step:5132 [D loss: 0.687406, acc.: 54.69%] [G loss: 0.780752]\n",
      "epoch:5 step:5133 [D loss: 0.691386, acc.: 51.56%] [G loss: 0.818487]\n",
      "epoch:5 step:5134 [D loss: 0.687949, acc.: 55.47%] [G loss: 0.796376]\n",
      "epoch:5 step:5135 [D loss: 0.712084, acc.: 54.69%] [G loss: 0.776515]\n",
      "epoch:5 step:5136 [D loss: 0.676590, acc.: 54.69%] [G loss: 0.762829]\n",
      "epoch:5 step:5137 [D loss: 0.723268, acc.: 42.19%] [G loss: 0.761288]\n",
      "epoch:5 step:5138 [D loss: 0.687819, acc.: 51.56%] [G loss: 0.785433]\n",
      "epoch:5 step:5139 [D loss: 0.685198, acc.: 55.47%] [G loss: 0.836012]\n",
      "epoch:5 step:5140 [D loss: 0.687598, acc.: 52.34%] [G loss: 0.830488]\n",
      "epoch:5 step:5141 [D loss: 0.684873, acc.: 54.69%] [G loss: 0.798048]\n",
      "epoch:5 step:5142 [D loss: 0.681002, acc.: 53.91%] [G loss: 0.775556]\n",
      "epoch:5 step:5143 [D loss: 0.693818, acc.: 53.12%] [G loss: 0.783328]\n",
      "epoch:5 step:5144 [D loss: 0.670629, acc.: 60.16%] [G loss: 0.762348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5145 [D loss: 0.676994, acc.: 57.03%] [G loss: 0.768203]\n",
      "epoch:5 step:5146 [D loss: 0.720789, acc.: 44.53%] [G loss: 0.775351]\n",
      "epoch:5 step:5147 [D loss: 0.722588, acc.: 41.41%] [G loss: 0.754820]\n",
      "epoch:5 step:5148 [D loss: 0.684325, acc.: 57.03%] [G loss: 0.766147]\n",
      "epoch:5 step:5149 [D loss: 0.672234, acc.: 55.47%] [G loss: 0.773293]\n",
      "epoch:5 step:5150 [D loss: 0.662069, acc.: 58.59%] [G loss: 0.818393]\n",
      "epoch:5 step:5151 [D loss: 0.671983, acc.: 52.34%] [G loss: 0.790860]\n",
      "epoch:5 step:5152 [D loss: 0.683129, acc.: 50.78%] [G loss: 0.801908]\n",
      "epoch:5 step:5153 [D loss: 0.642617, acc.: 61.72%] [G loss: 0.791091]\n",
      "epoch:5 step:5154 [D loss: 0.663436, acc.: 57.03%] [G loss: 0.741605]\n",
      "epoch:5 step:5155 [D loss: 0.675524, acc.: 57.03%] [G loss: 0.788149]\n",
      "epoch:5 step:5156 [D loss: 0.687170, acc.: 54.69%] [G loss: 0.801931]\n",
      "epoch:5 step:5157 [D loss: 0.696398, acc.: 53.12%] [G loss: 0.747822]\n",
      "epoch:5 step:5158 [D loss: 0.689607, acc.: 51.56%] [G loss: 0.766957]\n",
      "epoch:5 step:5159 [D loss: 0.690805, acc.: 53.12%] [G loss: 0.773789]\n",
      "epoch:5 step:5160 [D loss: 0.716399, acc.: 44.53%] [G loss: 0.728409]\n",
      "epoch:5 step:5161 [D loss: 0.668360, acc.: 55.47%] [G loss: 0.808018]\n",
      "epoch:5 step:5162 [D loss: 0.689540, acc.: 53.91%] [G loss: 0.806579]\n",
      "epoch:5 step:5163 [D loss: 0.667956, acc.: 50.78%] [G loss: 0.769924]\n",
      "epoch:5 step:5164 [D loss: 0.687936, acc.: 49.22%] [G loss: 0.790926]\n",
      "epoch:5 step:5165 [D loss: 0.708593, acc.: 51.56%] [G loss: 0.939874]\n",
      "epoch:5 step:5166 [D loss: 0.678644, acc.: 48.44%] [G loss: 0.794572]\n",
      "epoch:5 step:5167 [D loss: 0.680270, acc.: 52.34%] [G loss: 0.799711]\n",
      "epoch:5 step:5168 [D loss: 0.683834, acc.: 53.12%] [G loss: 0.861907]\n",
      "epoch:5 step:5169 [D loss: 0.681418, acc.: 53.91%] [G loss: 0.837263]\n",
      "epoch:5 step:5170 [D loss: 0.685436, acc.: 58.59%] [G loss: 0.775821]\n",
      "epoch:5 step:5171 [D loss: 0.689741, acc.: 54.69%] [G loss: 0.774036]\n",
      "epoch:5 step:5172 [D loss: 0.710399, acc.: 50.78%] [G loss: 0.835362]\n",
      "epoch:5 step:5173 [D loss: 0.679609, acc.: 57.81%] [G loss: 0.915793]\n",
      "epoch:5 step:5174 [D loss: 0.675641, acc.: 53.12%] [G loss: 0.971497]\n",
      "epoch:5 step:5175 [D loss: 0.667627, acc.: 60.16%] [G loss: 0.828909]\n",
      "epoch:5 step:5176 [D loss: 0.677638, acc.: 52.34%] [G loss: 0.761845]\n",
      "epoch:5 step:5177 [D loss: 0.677632, acc.: 53.12%] [G loss: 0.745222]\n",
      "epoch:5 step:5178 [D loss: 0.695246, acc.: 57.03%] [G loss: 0.768760]\n",
      "epoch:5 step:5179 [D loss: 0.733047, acc.: 50.00%] [G loss: 0.773579]\n",
      "epoch:5 step:5180 [D loss: 0.681125, acc.: 53.91%] [G loss: 0.763589]\n",
      "epoch:5 step:5181 [D loss: 0.684211, acc.: 46.88%] [G loss: 0.798140]\n",
      "epoch:5 step:5182 [D loss: 0.665616, acc.: 64.84%] [G loss: 0.770198]\n",
      "epoch:5 step:5183 [D loss: 0.725192, acc.: 47.66%] [G loss: 0.770484]\n",
      "epoch:5 step:5184 [D loss: 0.684514, acc.: 57.03%] [G loss: 0.784093]\n",
      "epoch:5 step:5185 [D loss: 0.697969, acc.: 52.34%] [G loss: 0.792917]\n",
      "epoch:5 step:5186 [D loss: 0.689778, acc.: 53.12%] [G loss: 0.795019]\n",
      "epoch:5 step:5187 [D loss: 0.676298, acc.: 61.72%] [G loss: 0.831021]\n",
      "epoch:5 step:5188 [D loss: 0.659334, acc.: 64.84%] [G loss: 0.816192]\n",
      "epoch:5 step:5189 [D loss: 0.676119, acc.: 59.38%] [G loss: 0.770411]\n",
      "epoch:5 step:5190 [D loss: 0.679883, acc.: 53.12%] [G loss: 0.769254]\n",
      "epoch:5 step:5191 [D loss: 0.698952, acc.: 52.34%] [G loss: 0.811075]\n",
      "epoch:5 step:5192 [D loss: 0.714613, acc.: 40.62%] [G loss: 0.773622]\n",
      "epoch:5 step:5193 [D loss: 0.687269, acc.: 50.78%] [G loss: 0.728433]\n",
      "epoch:5 step:5194 [D loss: 0.665158, acc.: 51.56%] [G loss: 0.760474]\n",
      "epoch:5 step:5195 [D loss: 0.674136, acc.: 50.00%] [G loss: 0.800825]\n",
      "epoch:5 step:5196 [D loss: 0.688951, acc.: 49.22%] [G loss: 0.796843]\n",
      "epoch:5 step:5197 [D loss: 0.678293, acc.: 57.03%] [G loss: 0.752462]\n",
      "epoch:5 step:5198 [D loss: 0.668308, acc.: 53.91%] [G loss: 0.765866]\n",
      "epoch:5 step:5199 [D loss: 0.705645, acc.: 49.22%] [G loss: 0.747929]\n",
      "epoch:5 step:5200 [D loss: 0.687104, acc.: 60.94%] [G loss: 0.792190]\n",
      "epoch:5 step:5201 [D loss: 0.699915, acc.: 57.03%] [G loss: 0.784317]\n",
      "epoch:5 step:5202 [D loss: 0.678577, acc.: 57.03%] [G loss: 0.842709]\n",
      "epoch:5 step:5203 [D loss: 0.703877, acc.: 52.34%] [G loss: 0.800586]\n",
      "epoch:5 step:5204 [D loss: 0.683653, acc.: 55.47%] [G loss: 0.771854]\n",
      "epoch:5 step:5205 [D loss: 0.696826, acc.: 49.22%] [G loss: 0.773355]\n",
      "epoch:5 step:5206 [D loss: 0.680252, acc.: 53.91%] [G loss: 0.788801]\n",
      "epoch:5 step:5207 [D loss: 0.711264, acc.: 47.66%] [G loss: 0.753491]\n",
      "epoch:5 step:5208 [D loss: 0.685649, acc.: 55.47%] [G loss: 0.835177]\n",
      "epoch:5 step:5209 [D loss: 0.682731, acc.: 57.03%] [G loss: 0.791635]\n",
      "epoch:5 step:5210 [D loss: 0.704269, acc.: 57.81%] [G loss: 0.809429]\n",
      "epoch:5 step:5211 [D loss: 0.652182, acc.: 64.06%] [G loss: 0.809617]\n",
      "epoch:5 step:5212 [D loss: 0.692410, acc.: 55.47%] [G loss: 0.773128]\n",
      "epoch:5 step:5213 [D loss: 0.699943, acc.: 54.69%] [G loss: 0.768909]\n",
      "epoch:5 step:5214 [D loss: 0.672587, acc.: 57.03%] [G loss: 0.837078]\n",
      "epoch:5 step:5215 [D loss: 0.697138, acc.: 50.78%] [G loss: 0.832691]\n",
      "epoch:5 step:5216 [D loss: 0.672247, acc.: 60.94%] [G loss: 0.824680]\n",
      "epoch:5 step:5217 [D loss: 0.661948, acc.: 51.56%] [G loss: 0.828029]\n",
      "epoch:5 step:5218 [D loss: 0.668066, acc.: 53.12%] [G loss: 0.805501]\n",
      "epoch:5 step:5219 [D loss: 0.692337, acc.: 47.66%] [G loss: 0.854837]\n",
      "epoch:5 step:5220 [D loss: 0.688965, acc.: 59.38%] [G loss: 0.780540]\n",
      "epoch:5 step:5221 [D loss: 0.767021, acc.: 47.66%] [G loss: 0.757924]\n",
      "epoch:5 step:5222 [D loss: 0.689372, acc.: 50.00%] [G loss: 0.754321]\n",
      "epoch:5 step:5223 [D loss: 0.730435, acc.: 44.53%] [G loss: 0.718115]\n",
      "epoch:5 step:5224 [D loss: 0.716208, acc.: 49.22%] [G loss: 0.750657]\n",
      "epoch:5 step:5225 [D loss: 0.676462, acc.: 60.94%] [G loss: 0.760880]\n",
      "epoch:5 step:5226 [D loss: 0.704335, acc.: 53.91%] [G loss: 0.764210]\n",
      "epoch:5 step:5227 [D loss: 0.680036, acc.: 61.72%] [G loss: 0.790230]\n",
      "epoch:5 step:5228 [D loss: 0.654996, acc.: 60.94%] [G loss: 0.775314]\n",
      "epoch:5 step:5229 [D loss: 0.672109, acc.: 56.25%] [G loss: 0.794785]\n",
      "epoch:5 step:5230 [D loss: 0.646031, acc.: 67.19%] [G loss: 0.887895]\n",
      "epoch:5 step:5231 [D loss: 0.671693, acc.: 60.16%] [G loss: 0.793346]\n",
      "epoch:5 step:5232 [D loss: 0.686563, acc.: 51.56%] [G loss: 0.751976]\n",
      "epoch:5 step:5233 [D loss: 0.711453, acc.: 44.53%] [G loss: 0.748355]\n",
      "epoch:5 step:5234 [D loss: 0.717098, acc.: 39.84%] [G loss: 0.777836]\n",
      "epoch:5 step:5235 [D loss: 0.694607, acc.: 52.34%] [G loss: 0.769565]\n",
      "epoch:5 step:5236 [D loss: 0.683745, acc.: 52.34%] [G loss: 0.775324]\n",
      "epoch:5 step:5237 [D loss: 0.669751, acc.: 58.59%] [G loss: 0.754566]\n",
      "epoch:5 step:5238 [D loss: 0.708770, acc.: 50.78%] [G loss: 0.789801]\n",
      "epoch:5 step:5239 [D loss: 0.702085, acc.: 46.88%] [G loss: 0.755629]\n",
      "epoch:5 step:5240 [D loss: 0.660621, acc.: 60.16%] [G loss: 0.778636]\n",
      "epoch:5 step:5241 [D loss: 0.672085, acc.: 60.16%] [G loss: 0.741265]\n",
      "epoch:5 step:5242 [D loss: 0.693153, acc.: 53.12%] [G loss: 0.801442]\n",
      "epoch:5 step:5243 [D loss: 0.680558, acc.: 54.69%] [G loss: 0.745650]\n",
      "epoch:5 step:5244 [D loss: 0.679965, acc.: 52.34%] [G loss: 0.775653]\n",
      "epoch:5 step:5245 [D loss: 0.679797, acc.: 51.56%] [G loss: 0.729430]\n",
      "epoch:5 step:5246 [D loss: 0.682261, acc.: 57.81%] [G loss: 0.764280]\n",
      "epoch:5 step:5247 [D loss: 0.679970, acc.: 57.81%] [G loss: 0.780253]\n",
      "epoch:5 step:5248 [D loss: 0.691418, acc.: 51.56%] [G loss: 0.816431]\n",
      "epoch:5 step:5249 [D loss: 0.725237, acc.: 48.44%] [G loss: 0.829637]\n",
      "epoch:5 step:5250 [D loss: 0.714983, acc.: 48.44%] [G loss: 0.814809]\n",
      "epoch:5 step:5251 [D loss: 0.724804, acc.: 47.66%] [G loss: 0.788133]\n",
      "epoch:5 step:5252 [D loss: 0.717634, acc.: 48.44%] [G loss: 0.806237]\n",
      "epoch:5 step:5253 [D loss: 0.704221, acc.: 49.22%] [G loss: 0.765789]\n",
      "epoch:5 step:5254 [D loss: 0.665856, acc.: 57.81%] [G loss: 0.790731]\n",
      "epoch:5 step:5255 [D loss: 0.698517, acc.: 49.22%] [G loss: 0.809214]\n",
      "epoch:5 step:5256 [D loss: 0.664700, acc.: 61.72%] [G loss: 0.774822]\n",
      "epoch:5 step:5257 [D loss: 0.700266, acc.: 51.56%] [G loss: 0.799921]\n",
      "epoch:5 step:5258 [D loss: 0.675662, acc.: 59.38%] [G loss: 0.775563]\n",
      "epoch:5 step:5259 [D loss: 0.698019, acc.: 49.22%] [G loss: 0.799839]\n",
      "epoch:5 step:5260 [D loss: 0.712772, acc.: 48.44%] [G loss: 0.773244]\n",
      "epoch:5 step:5261 [D loss: 0.692558, acc.: 56.25%] [G loss: 0.782672]\n",
      "epoch:5 step:5262 [D loss: 0.693983, acc.: 51.56%] [G loss: 0.807560]\n",
      "epoch:5 step:5263 [D loss: 0.666271, acc.: 63.28%] [G loss: 0.810454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5264 [D loss: 0.679361, acc.: 60.94%] [G loss: 0.770594]\n",
      "epoch:5 step:5265 [D loss: 0.663548, acc.: 61.72%] [G loss: 0.770473]\n",
      "epoch:5 step:5266 [D loss: 0.669736, acc.: 61.72%] [G loss: 0.757977]\n",
      "epoch:5 step:5267 [D loss: 0.699673, acc.: 51.56%] [G loss: 0.735915]\n",
      "epoch:5 step:5268 [D loss: 0.713448, acc.: 42.97%] [G loss: 0.732423]\n",
      "epoch:5 step:5269 [D loss: 0.693619, acc.: 51.56%] [G loss: 0.766528]\n",
      "epoch:5 step:5270 [D loss: 0.692965, acc.: 53.12%] [G loss: 0.686312]\n",
      "epoch:5 step:5271 [D loss: 0.698778, acc.: 54.69%] [G loss: 0.737020]\n",
      "epoch:5 step:5272 [D loss: 0.660337, acc.: 63.28%] [G loss: 0.736295]\n",
      "epoch:5 step:5273 [D loss: 0.694677, acc.: 47.66%] [G loss: 0.748065]\n",
      "epoch:5 step:5274 [D loss: 0.690729, acc.: 48.44%] [G loss: 0.794886]\n",
      "epoch:5 step:5275 [D loss: 0.686584, acc.: 53.91%] [G loss: 0.738845]\n",
      "epoch:5 step:5276 [D loss: 0.676322, acc.: 51.56%] [G loss: 0.790785]\n",
      "epoch:5 step:5277 [D loss: 0.643930, acc.: 65.62%] [G loss: 0.823437]\n",
      "epoch:5 step:5278 [D loss: 0.694091, acc.: 53.91%] [G loss: 0.806091]\n",
      "epoch:5 step:5279 [D loss: 0.691706, acc.: 51.56%] [G loss: 0.802449]\n",
      "epoch:5 step:5280 [D loss: 0.695574, acc.: 58.59%] [G loss: 0.864642]\n",
      "epoch:5 step:5281 [D loss: 0.651854, acc.: 63.28%] [G loss: 0.900720]\n",
      "epoch:5 step:5282 [D loss: 0.670099, acc.: 60.94%] [G loss: 0.823668]\n",
      "epoch:5 step:5283 [D loss: 0.696829, acc.: 47.66%] [G loss: 0.853565]\n",
      "epoch:5 step:5284 [D loss: 0.681826, acc.: 53.91%] [G loss: 0.885170]\n",
      "epoch:5 step:5285 [D loss: 0.727203, acc.: 42.97%] [G loss: 0.856875]\n",
      "epoch:5 step:5286 [D loss: 0.659018, acc.: 57.03%] [G loss: 0.812894]\n",
      "epoch:5 step:5287 [D loss: 0.681782, acc.: 54.69%] [G loss: 0.835262]\n",
      "epoch:5 step:5288 [D loss: 0.669902, acc.: 55.47%] [G loss: 0.826775]\n",
      "epoch:5 step:5289 [D loss: 0.695590, acc.: 53.12%] [G loss: 0.793420]\n",
      "epoch:5 step:5290 [D loss: 0.668889, acc.: 60.16%] [G loss: 0.778581]\n",
      "epoch:5 step:5291 [D loss: 0.710084, acc.: 46.88%] [G loss: 0.800577]\n",
      "epoch:5 step:5292 [D loss: 0.674458, acc.: 50.78%] [G loss: 0.719882]\n",
      "epoch:5 step:5293 [D loss: 0.708554, acc.: 46.88%] [G loss: 0.786788]\n",
      "epoch:5 step:5294 [D loss: 0.677187, acc.: 53.12%] [G loss: 0.763434]\n",
      "epoch:5 step:5295 [D loss: 0.657853, acc.: 54.69%] [G loss: 0.825951]\n",
      "epoch:5 step:5296 [D loss: 0.687660, acc.: 58.59%] [G loss: 0.782411]\n",
      "epoch:5 step:5297 [D loss: 0.720020, acc.: 46.88%] [G loss: 0.773222]\n",
      "epoch:5 step:5298 [D loss: 0.677524, acc.: 51.56%] [G loss: 0.810432]\n",
      "epoch:5 step:5299 [D loss: 0.689428, acc.: 56.25%] [G loss: 0.779711]\n",
      "epoch:5 step:5300 [D loss: 0.658370, acc.: 67.19%] [G loss: 0.818506]\n",
      "epoch:5 step:5301 [D loss: 0.671167, acc.: 59.38%] [G loss: 0.830691]\n",
      "epoch:5 step:5302 [D loss: 0.709889, acc.: 42.19%] [G loss: 0.784286]\n",
      "epoch:5 step:5303 [D loss: 0.666148, acc.: 57.03%] [G loss: 0.875575]\n",
      "epoch:5 step:5304 [D loss: 0.692641, acc.: 52.34%] [G loss: 0.805856]\n",
      "epoch:5 step:5305 [D loss: 0.669793, acc.: 53.12%] [G loss: 0.762868]\n",
      "epoch:5 step:5306 [D loss: 0.674919, acc.: 54.69%] [G loss: 0.747776]\n",
      "epoch:5 step:5307 [D loss: 0.718213, acc.: 42.97%] [G loss: 0.764678]\n",
      "epoch:5 step:5308 [D loss: 0.730767, acc.: 48.44%] [G loss: 0.775704]\n",
      "epoch:5 step:5309 [D loss: 0.646444, acc.: 56.25%] [G loss: 0.753772]\n",
      "epoch:5 step:5310 [D loss: 0.717387, acc.: 49.22%] [G loss: 0.791923]\n",
      "epoch:5 step:5311 [D loss: 0.690941, acc.: 47.66%] [G loss: 0.796908]\n",
      "epoch:5 step:5312 [D loss: 0.662323, acc.: 54.69%] [G loss: 0.754212]\n",
      "epoch:5 step:5313 [D loss: 0.655919, acc.: 65.62%] [G loss: 0.862419]\n",
      "epoch:5 step:5314 [D loss: 0.699115, acc.: 47.66%] [G loss: 0.776805]\n",
      "epoch:5 step:5315 [D loss: 0.685427, acc.: 51.56%] [G loss: 0.774021]\n",
      "epoch:5 step:5316 [D loss: 0.708629, acc.: 46.88%] [G loss: 0.743373]\n",
      "epoch:5 step:5317 [D loss: 0.681741, acc.: 56.25%] [G loss: 0.752281]\n",
      "epoch:5 step:5318 [D loss: 0.719560, acc.: 48.44%] [G loss: 0.821437]\n",
      "epoch:5 step:5319 [D loss: 0.711075, acc.: 46.09%] [G loss: 0.783649]\n",
      "epoch:5 step:5320 [D loss: 0.682740, acc.: 52.34%] [G loss: 0.910102]\n",
      "epoch:5 step:5321 [D loss: 0.658744, acc.: 63.28%] [G loss: 0.791078]\n",
      "epoch:5 step:5322 [D loss: 0.669626, acc.: 52.34%] [G loss: 0.839167]\n",
      "epoch:5 step:5323 [D loss: 0.676103, acc.: 53.12%] [G loss: 0.804838]\n",
      "epoch:5 step:5324 [D loss: 0.713280, acc.: 44.53%] [G loss: 0.821333]\n",
      "epoch:5 step:5325 [D loss: 0.666049, acc.: 59.38%] [G loss: 0.835380]\n",
      "epoch:5 step:5326 [D loss: 0.661712, acc.: 59.38%] [G loss: 0.806338]\n",
      "epoch:5 step:5327 [D loss: 0.703672, acc.: 51.56%] [G loss: 0.849188]\n",
      "epoch:5 step:5328 [D loss: 0.679003, acc.: 53.91%] [G loss: 0.780324]\n",
      "epoch:5 step:5329 [D loss: 0.692381, acc.: 56.25%] [G loss: 0.823995]\n",
      "epoch:5 step:5330 [D loss: 0.688261, acc.: 53.12%] [G loss: 0.752699]\n",
      "epoch:5 step:5331 [D loss: 0.805521, acc.: 46.09%] [G loss: 0.786406]\n",
      "epoch:5 step:5332 [D loss: 0.670521, acc.: 57.03%] [G loss: 0.818403]\n",
      "epoch:5 step:5333 [D loss: 0.684838, acc.: 53.91%] [G loss: 0.969960]\n",
      "epoch:5 step:5334 [D loss: 0.701306, acc.: 47.66%] [G loss: 0.806375]\n",
      "epoch:5 step:5335 [D loss: 0.649594, acc.: 59.38%] [G loss: 0.857384]\n",
      "epoch:5 step:5336 [D loss: 0.694772, acc.: 52.34%] [G loss: 0.832017]\n",
      "epoch:5 step:5337 [D loss: 0.712050, acc.: 49.22%] [G loss: 0.793923]\n",
      "epoch:5 step:5338 [D loss: 0.676966, acc.: 60.16%] [G loss: 0.782267]\n",
      "epoch:5 step:5339 [D loss: 0.700736, acc.: 46.88%] [G loss: 0.784438]\n",
      "epoch:5 step:5340 [D loss: 0.697612, acc.: 57.03%] [G loss: 0.764791]\n",
      "epoch:5 step:5341 [D loss: 0.681058, acc.: 60.94%] [G loss: 0.786153]\n",
      "epoch:5 step:5342 [D loss: 0.641576, acc.: 65.62%] [G loss: 0.763941]\n",
      "epoch:5 step:5343 [D loss: 0.693692, acc.: 48.44%] [G loss: 0.780078]\n",
      "epoch:5 step:5344 [D loss: 0.663414, acc.: 60.94%] [G loss: 0.740828]\n",
      "epoch:5 step:5345 [D loss: 0.660802, acc.: 57.81%] [G loss: 0.715994]\n",
      "epoch:5 step:5346 [D loss: 0.690660, acc.: 56.25%] [G loss: 0.720569]\n",
      "epoch:5 step:5347 [D loss: 0.723416, acc.: 43.75%] [G loss: 0.715527]\n",
      "epoch:5 step:5348 [D loss: 0.718202, acc.: 43.75%] [G loss: 0.736499]\n",
      "epoch:5 step:5349 [D loss: 0.670005, acc.: 55.47%] [G loss: 0.771811]\n",
      "epoch:5 step:5350 [D loss: 0.710631, acc.: 50.00%] [G loss: 0.791532]\n",
      "epoch:5 step:5351 [D loss: 0.712688, acc.: 49.22%] [G loss: 0.778782]\n",
      "epoch:5 step:5352 [D loss: 0.671988, acc.: 55.47%] [G loss: 0.776132]\n",
      "epoch:5 step:5353 [D loss: 0.682365, acc.: 56.25%] [G loss: 0.721259]\n",
      "epoch:5 step:5354 [D loss: 0.695472, acc.: 52.34%] [G loss: 0.821666]\n",
      "epoch:5 step:5355 [D loss: 0.657324, acc.: 56.25%] [G loss: 0.809040]\n",
      "epoch:5 step:5356 [D loss: 0.691443, acc.: 54.69%] [G loss: 0.798359]\n",
      "epoch:5 step:5357 [D loss: 0.704497, acc.: 46.88%] [G loss: 0.825645]\n",
      "epoch:5 step:5358 [D loss: 0.684646, acc.: 53.91%] [G loss: 0.869951]\n",
      "epoch:5 step:5359 [D loss: 0.663100, acc.: 60.94%] [G loss: 0.791520]\n",
      "epoch:5 step:5360 [D loss: 0.717471, acc.: 50.00%] [G loss: 0.815295]\n",
      "epoch:5 step:5361 [D loss: 0.671933, acc.: 60.94%] [G loss: 0.765589]\n",
      "epoch:5 step:5362 [D loss: 0.676385, acc.: 57.81%] [G loss: 0.820412]\n",
      "epoch:5 step:5363 [D loss: 0.668763, acc.: 57.81%] [G loss: 0.826333]\n",
      "epoch:5 step:5364 [D loss: 0.649294, acc.: 56.25%] [G loss: 0.792136]\n",
      "epoch:5 step:5365 [D loss: 0.691031, acc.: 52.34%] [G loss: 0.728114]\n",
      "epoch:5 step:5366 [D loss: 0.707287, acc.: 51.56%] [G loss: 0.715262]\n",
      "epoch:5 step:5367 [D loss: 0.684740, acc.: 48.44%] [G loss: 0.738859]\n",
      "epoch:5 step:5368 [D loss: 0.707912, acc.: 47.66%] [G loss: 0.742370]\n",
      "epoch:5 step:5369 [D loss: 0.740012, acc.: 39.84%] [G loss: 0.773742]\n",
      "epoch:5 step:5370 [D loss: 0.703036, acc.: 45.31%] [G loss: 0.736717]\n",
      "epoch:5 step:5371 [D loss: 0.690574, acc.: 52.34%] [G loss: 0.757210]\n",
      "epoch:5 step:5372 [D loss: 0.712456, acc.: 47.66%] [G loss: 0.728452]\n",
      "epoch:5 step:5373 [D loss: 0.690019, acc.: 57.03%] [G loss: 0.758371]\n",
      "epoch:5 step:5374 [D loss: 0.679718, acc.: 60.16%] [G loss: 0.766768]\n",
      "epoch:5 step:5375 [D loss: 0.709459, acc.: 44.53%] [G loss: 0.743432]\n",
      "epoch:5 step:5376 [D loss: 0.691297, acc.: 57.03%] [G loss: 0.754866]\n",
      "epoch:5 step:5377 [D loss: 0.681482, acc.: 56.25%] [G loss: 0.701877]\n",
      "epoch:5 step:5378 [D loss: 0.701009, acc.: 44.53%] [G loss: 0.735682]\n",
      "epoch:5 step:5379 [D loss: 0.704817, acc.: 51.56%] [G loss: 0.764500]\n",
      "epoch:5 step:5380 [D loss: 0.675404, acc.: 58.59%] [G loss: 0.727943]\n",
      "epoch:5 step:5381 [D loss: 0.674407, acc.: 59.38%] [G loss: 0.791242]\n",
      "epoch:5 step:5382 [D loss: 0.698964, acc.: 53.12%] [G loss: 0.775324]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5383 [D loss: 0.724629, acc.: 45.31%] [G loss: 0.763393]\n",
      "epoch:5 step:5384 [D loss: 0.695191, acc.: 50.00%] [G loss: 0.792506]\n",
      "epoch:5 step:5385 [D loss: 0.697062, acc.: 55.47%] [G loss: 0.770830]\n",
      "epoch:5 step:5386 [D loss: 0.650349, acc.: 61.72%] [G loss: 0.812606]\n",
      "epoch:5 step:5387 [D loss: 0.704777, acc.: 46.09%] [G loss: 0.829787]\n",
      "epoch:5 step:5388 [D loss: 0.679943, acc.: 46.88%] [G loss: 0.828095]\n",
      "epoch:5 step:5389 [D loss: 0.683357, acc.: 56.25%] [G loss: 0.746811]\n",
      "epoch:5 step:5390 [D loss: 0.721699, acc.: 48.44%] [G loss: 0.780589]\n",
      "epoch:5 step:5391 [D loss: 0.681665, acc.: 51.56%] [G loss: 0.742035]\n",
      "epoch:5 step:5392 [D loss: 0.675136, acc.: 53.91%] [G loss: 0.780997]\n",
      "epoch:5 step:5393 [D loss: 0.669364, acc.: 50.78%] [G loss: 0.762140]\n",
      "epoch:5 step:5394 [D loss: 0.711900, acc.: 47.66%] [G loss: 0.725217]\n",
      "epoch:5 step:5395 [D loss: 0.672311, acc.: 54.69%] [G loss: 0.764401]\n",
      "epoch:5 step:5396 [D loss: 0.676758, acc.: 57.81%] [G loss: 0.790633]\n",
      "epoch:5 step:5397 [D loss: 0.697321, acc.: 49.22%] [G loss: 0.776791]\n",
      "epoch:5 step:5398 [D loss: 0.691748, acc.: 50.78%] [G loss: 0.777448]\n",
      "epoch:5 step:5399 [D loss: 0.681695, acc.: 53.91%] [G loss: 0.846084]\n",
      "epoch:5 step:5400 [D loss: 0.680855, acc.: 56.25%] [G loss: 0.808123]\n",
      "epoch:5 step:5401 [D loss: 0.723499, acc.: 49.22%] [G loss: 0.771645]\n",
      "epoch:5 step:5402 [D loss: 0.695915, acc.: 45.31%] [G loss: 0.760958]\n",
      "epoch:5 step:5403 [D loss: 0.677930, acc.: 55.47%] [G loss: 0.790856]\n",
      "epoch:5 step:5404 [D loss: 0.720252, acc.: 42.19%] [G loss: 0.786724]\n",
      "epoch:5 step:5405 [D loss: 0.681971, acc.: 53.12%] [G loss: 0.751120]\n",
      "epoch:5 step:5406 [D loss: 0.674034, acc.: 53.12%] [G loss: 0.774238]\n",
      "epoch:5 step:5407 [D loss: 0.680249, acc.: 53.12%] [G loss: 0.803234]\n",
      "epoch:5 step:5408 [D loss: 0.669477, acc.: 57.81%] [G loss: 0.807178]\n",
      "epoch:5 step:5409 [D loss: 0.691914, acc.: 52.34%] [G loss: 0.777044]\n",
      "epoch:5 step:5410 [D loss: 0.689139, acc.: 54.69%] [G loss: 0.760496]\n",
      "epoch:5 step:5411 [D loss: 0.714227, acc.: 42.97%] [G loss: 0.750324]\n",
      "epoch:5 step:5412 [D loss: 0.687850, acc.: 57.81%] [G loss: 0.761035]\n",
      "epoch:5 step:5413 [D loss: 0.673008, acc.: 60.94%] [G loss: 0.760322]\n",
      "epoch:5 step:5414 [D loss: 0.678623, acc.: 56.25%] [G loss: 0.752255]\n",
      "epoch:5 step:5415 [D loss: 0.682350, acc.: 56.25%] [G loss: 0.713171]\n",
      "epoch:5 step:5416 [D loss: 0.745976, acc.: 49.22%] [G loss: 0.745198]\n",
      "epoch:5 step:5417 [D loss: 0.652489, acc.: 60.16%] [G loss: 0.826490]\n",
      "epoch:5 step:5418 [D loss: 0.685798, acc.: 51.56%] [G loss: 0.826455]\n",
      "epoch:5 step:5419 [D loss: 0.676906, acc.: 57.81%] [G loss: 0.749452]\n",
      "epoch:5 step:5420 [D loss: 0.666717, acc.: 57.81%] [G loss: 0.779493]\n",
      "epoch:5 step:5421 [D loss: 0.668020, acc.: 55.47%] [G loss: 0.763555]\n",
      "epoch:5 step:5422 [D loss: 0.679110, acc.: 55.47%] [G loss: 0.787864]\n",
      "epoch:5 step:5423 [D loss: 0.712051, acc.: 51.56%] [G loss: 0.777819]\n",
      "epoch:5 step:5424 [D loss: 0.729460, acc.: 44.53%] [G loss: 0.796649]\n",
      "epoch:5 step:5425 [D loss: 0.679307, acc.: 55.47%] [G loss: 0.796053]\n",
      "epoch:5 step:5426 [D loss: 0.670208, acc.: 55.47%] [G loss: 0.792719]\n",
      "epoch:5 step:5427 [D loss: 0.667168, acc.: 58.59%] [G loss: 0.873547]\n",
      "epoch:5 step:5428 [D loss: 0.681783, acc.: 56.25%] [G loss: 0.878796]\n",
      "epoch:5 step:5429 [D loss: 0.702916, acc.: 47.66%] [G loss: 0.822469]\n",
      "epoch:5 step:5430 [D loss: 0.670209, acc.: 60.16%] [G loss: 0.814890]\n",
      "epoch:5 step:5431 [D loss: 0.697533, acc.: 50.00%] [G loss: 0.803824]\n",
      "epoch:5 step:5432 [D loss: 0.695369, acc.: 51.56%] [G loss: 0.794580]\n",
      "epoch:5 step:5433 [D loss: 0.665759, acc.: 61.72%] [G loss: 0.760203]\n",
      "epoch:5 step:5434 [D loss: 0.663434, acc.: 63.28%] [G loss: 0.809468]\n",
      "epoch:5 step:5435 [D loss: 0.694317, acc.: 49.22%] [G loss: 0.729211]\n",
      "epoch:5 step:5436 [D loss: 0.670831, acc.: 63.28%] [G loss: 0.773815]\n",
      "epoch:5 step:5437 [D loss: 0.684256, acc.: 60.16%] [G loss: 0.819280]\n",
      "epoch:5 step:5438 [D loss: 0.661184, acc.: 64.84%] [G loss: 0.756792]\n",
      "epoch:5 step:5439 [D loss: 0.680803, acc.: 56.25%] [G loss: 0.818934]\n",
      "epoch:5 step:5440 [D loss: 0.685284, acc.: 57.81%] [G loss: 0.808709]\n",
      "epoch:5 step:5441 [D loss: 0.661690, acc.: 53.12%] [G loss: 0.797449]\n",
      "epoch:5 step:5442 [D loss: 0.692782, acc.: 57.03%] [G loss: 0.771280]\n",
      "epoch:5 step:5443 [D loss: 0.703504, acc.: 56.25%] [G loss: 0.789640]\n",
      "epoch:5 step:5444 [D loss: 0.678264, acc.: 59.38%] [G loss: 0.755399]\n",
      "epoch:5 step:5445 [D loss: 0.655319, acc.: 57.81%] [G loss: 0.713696]\n",
      "epoch:5 step:5446 [D loss: 0.716644, acc.: 47.66%] [G loss: 0.739110]\n",
      "epoch:5 step:5447 [D loss: 0.708345, acc.: 52.34%] [G loss: 0.718516]\n",
      "epoch:5 step:5448 [D loss: 0.705881, acc.: 52.34%] [G loss: 0.690522]\n",
      "epoch:5 step:5449 [D loss: 0.665250, acc.: 59.38%] [G loss: 0.760840]\n",
      "epoch:5 step:5450 [D loss: 0.696075, acc.: 51.56%] [G loss: 0.762875]\n",
      "epoch:5 step:5451 [D loss: 0.676996, acc.: 57.03%] [G loss: 0.872992]\n",
      "epoch:5 step:5452 [D loss: 0.695383, acc.: 53.12%] [G loss: 0.745424]\n",
      "epoch:5 step:5453 [D loss: 0.726944, acc.: 47.66%] [G loss: 0.795868]\n",
      "epoch:5 step:5454 [D loss: 0.672520, acc.: 60.94%] [G loss: 0.802310]\n",
      "epoch:5 step:5455 [D loss: 0.669386, acc.: 57.81%] [G loss: 0.876079]\n",
      "epoch:5 step:5456 [D loss: 0.664247, acc.: 58.59%] [G loss: 0.805287]\n",
      "epoch:5 step:5457 [D loss: 0.674024, acc.: 60.94%] [G loss: 0.845716]\n",
      "epoch:5 step:5458 [D loss: 0.672668, acc.: 57.81%] [G loss: 0.801744]\n",
      "epoch:5 step:5459 [D loss: 0.694348, acc.: 53.12%] [G loss: 0.793133]\n",
      "epoch:5 step:5460 [D loss: 0.682518, acc.: 51.56%] [G loss: 0.802358]\n",
      "epoch:5 step:5461 [D loss: 0.660624, acc.: 60.94%] [G loss: 0.771474]\n",
      "epoch:5 step:5462 [D loss: 0.691482, acc.: 60.94%] [G loss: 0.722346]\n",
      "epoch:5 step:5463 [D loss: 0.675181, acc.: 56.25%] [G loss: 0.764791]\n",
      "epoch:5 step:5464 [D loss: 0.721857, acc.: 39.06%] [G loss: 0.756262]\n",
      "epoch:5 step:5465 [D loss: 0.692632, acc.: 53.12%] [G loss: 0.819618]\n",
      "epoch:5 step:5466 [D loss: 0.656091, acc.: 60.16%] [G loss: 0.771606]\n",
      "epoch:5 step:5467 [D loss: 0.670412, acc.: 53.12%] [G loss: 0.739557]\n",
      "epoch:5 step:5468 [D loss: 0.716587, acc.: 43.75%] [G loss: 0.799069]\n",
      "epoch:5 step:5469 [D loss: 0.670204, acc.: 54.69%] [G loss: 0.812467]\n",
      "epoch:5 step:5470 [D loss: 0.697424, acc.: 54.69%] [G loss: 0.807559]\n",
      "epoch:5 step:5471 [D loss: 0.733681, acc.: 46.09%] [G loss: 0.890532]\n",
      "epoch:5 step:5472 [D loss: 0.698402, acc.: 56.25%] [G loss: 0.836426]\n",
      "epoch:5 step:5473 [D loss: 0.663686, acc.: 62.50%] [G loss: 0.849141]\n",
      "epoch:5 step:5474 [D loss: 0.688700, acc.: 52.34%] [G loss: 0.805778]\n",
      "epoch:5 step:5475 [D loss: 0.693156, acc.: 56.25%] [G loss: 0.799166]\n",
      "epoch:5 step:5476 [D loss: 0.638852, acc.: 60.16%] [G loss: 0.841190]\n",
      "epoch:5 step:5477 [D loss: 0.712097, acc.: 45.31%] [G loss: 0.781862]\n",
      "epoch:5 step:5478 [D loss: 0.737107, acc.: 41.41%] [G loss: 0.847172]\n",
      "epoch:5 step:5479 [D loss: 0.728129, acc.: 46.88%] [G loss: 0.834942]\n",
      "epoch:5 step:5480 [D loss: 0.679554, acc.: 55.47%] [G loss: 0.863545]\n",
      "epoch:5 step:5481 [D loss: 0.665060, acc.: 61.72%] [G loss: 0.823747]\n",
      "epoch:5 step:5482 [D loss: 0.660487, acc.: 55.47%] [G loss: 0.810484]\n",
      "epoch:5 step:5483 [D loss: 0.679414, acc.: 53.12%] [G loss: 0.818702]\n",
      "epoch:5 step:5484 [D loss: 0.691371, acc.: 54.69%] [G loss: 0.799389]\n",
      "epoch:5 step:5485 [D loss: 0.650839, acc.: 58.59%] [G loss: 0.801849]\n",
      "epoch:5 step:5486 [D loss: 0.692551, acc.: 57.81%] [G loss: 0.757498]\n",
      "epoch:5 step:5487 [D loss: 0.738297, acc.: 44.53%] [G loss: 0.758158]\n",
      "epoch:5 step:5488 [D loss: 0.651984, acc.: 60.16%] [G loss: 0.774173]\n",
      "epoch:5 step:5489 [D loss: 0.651918, acc.: 58.59%] [G loss: 0.810154]\n",
      "epoch:5 step:5490 [D loss: 0.685407, acc.: 51.56%] [G loss: 0.789396]\n",
      "epoch:5 step:5491 [D loss: 0.657474, acc.: 64.06%] [G loss: 0.898712]\n",
      "epoch:5 step:5492 [D loss: 0.671243, acc.: 56.25%] [G loss: 0.900997]\n",
      "epoch:5 step:5493 [D loss: 0.688910, acc.: 56.25%] [G loss: 0.784512]\n",
      "epoch:5 step:5494 [D loss: 0.642428, acc.: 61.72%] [G loss: 0.784539]\n",
      "epoch:5 step:5495 [D loss: 0.693081, acc.: 55.47%] [G loss: 0.802558]\n",
      "epoch:5 step:5496 [D loss: 0.695527, acc.: 53.91%] [G loss: 0.839508]\n",
      "epoch:5 step:5497 [D loss: 0.724978, acc.: 51.56%] [G loss: 0.774210]\n",
      "epoch:5 step:5498 [D loss: 0.714607, acc.: 49.22%] [G loss: 0.726440]\n",
      "epoch:5 step:5499 [D loss: 0.764161, acc.: 44.53%] [G loss: 0.771438]\n",
      "epoch:5 step:5500 [D loss: 0.699252, acc.: 46.88%] [G loss: 0.797993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5501 [D loss: 0.688739, acc.: 57.03%] [G loss: 0.760792]\n",
      "epoch:5 step:5502 [D loss: 0.713241, acc.: 50.78%] [G loss: 0.755167]\n",
      "epoch:5 step:5503 [D loss: 0.695367, acc.: 50.78%] [G loss: 0.791637]\n",
      "epoch:5 step:5504 [D loss: 0.660643, acc.: 62.50%] [G loss: 0.880246]\n",
      "epoch:5 step:5505 [D loss: 0.653906, acc.: 59.38%] [G loss: 0.916253]\n",
      "epoch:5 step:5506 [D loss: 0.666411, acc.: 61.72%] [G loss: 0.866097]\n",
      "epoch:5 step:5507 [D loss: 0.655089, acc.: 60.16%] [G loss: 0.927294]\n",
      "epoch:5 step:5508 [D loss: 0.664290, acc.: 57.81%] [G loss: 0.823438]\n",
      "epoch:5 step:5509 [D loss: 0.710424, acc.: 48.44%] [G loss: 0.866280]\n",
      "epoch:5 step:5510 [D loss: 0.695136, acc.: 56.25%] [G loss: 0.716578]\n",
      "epoch:5 step:5511 [D loss: 0.700288, acc.: 54.69%] [G loss: 0.709284]\n",
      "epoch:5 step:5512 [D loss: 0.717234, acc.: 48.44%] [G loss: 0.745172]\n",
      "epoch:5 step:5513 [D loss: 0.717973, acc.: 48.44%] [G loss: 0.770792]\n",
      "epoch:5 step:5514 [D loss: 0.688744, acc.: 59.38%] [G loss: 0.756777]\n",
      "epoch:5 step:5515 [D loss: 0.671164, acc.: 62.50%] [G loss: 0.760988]\n",
      "epoch:5 step:5516 [D loss: 0.689931, acc.: 53.91%] [G loss: 0.697347]\n",
      "epoch:5 step:5517 [D loss: 0.714657, acc.: 50.00%] [G loss: 0.825664]\n",
      "epoch:5 step:5518 [D loss: 0.685800, acc.: 50.00%] [G loss: 0.784272]\n",
      "epoch:5 step:5519 [D loss: 0.670240, acc.: 60.94%] [G loss: 0.812739]\n",
      "epoch:5 step:5520 [D loss: 0.677328, acc.: 59.38%] [G loss: 0.788394]\n",
      "epoch:5 step:5521 [D loss: 0.682703, acc.: 53.91%] [G loss: 0.855076]\n",
      "epoch:5 step:5522 [D loss: 0.683489, acc.: 56.25%] [G loss: 0.824926]\n",
      "epoch:5 step:5523 [D loss: 0.699618, acc.: 55.47%] [G loss: 0.799071]\n",
      "epoch:5 step:5524 [D loss: 0.701478, acc.: 50.00%] [G loss: 0.785118]\n",
      "epoch:5 step:5525 [D loss: 0.709313, acc.: 48.44%] [G loss: 0.772030]\n",
      "epoch:5 step:5526 [D loss: 0.666102, acc.: 60.16%] [G loss: 0.717975]\n",
      "epoch:5 step:5527 [D loss: 0.683217, acc.: 53.91%] [G loss: 0.772513]\n",
      "epoch:5 step:5528 [D loss: 0.744748, acc.: 42.19%] [G loss: 0.776477]\n",
      "epoch:5 step:5529 [D loss: 0.720022, acc.: 46.88%] [G loss: 0.831612]\n",
      "epoch:5 step:5530 [D loss: 0.710810, acc.: 50.78%] [G loss: 0.794091]\n",
      "epoch:5 step:5531 [D loss: 0.670532, acc.: 61.72%] [G loss: 0.863668]\n",
      "epoch:5 step:5532 [D loss: 0.696116, acc.: 53.12%] [G loss: 0.812596]\n",
      "epoch:5 step:5533 [D loss: 0.683683, acc.: 57.03%] [G loss: 0.845948]\n",
      "epoch:5 step:5534 [D loss: 0.680702, acc.: 53.91%] [G loss: 0.814244]\n",
      "epoch:5 step:5535 [D loss: 0.692698, acc.: 51.56%] [G loss: 0.767570]\n",
      "epoch:5 step:5536 [D loss: 0.715636, acc.: 46.09%] [G loss: 0.798520]\n",
      "epoch:5 step:5537 [D loss: 0.675942, acc.: 57.81%] [G loss: 0.757641]\n",
      "epoch:5 step:5538 [D loss: 0.705036, acc.: 51.56%] [G loss: 0.802764]\n",
      "epoch:5 step:5539 [D loss: 0.692125, acc.: 52.34%] [G loss: 0.777689]\n",
      "epoch:5 step:5540 [D loss: 0.698858, acc.: 46.09%] [G loss: 0.745512]\n",
      "epoch:5 step:5541 [D loss: 0.671085, acc.: 53.91%] [G loss: 0.827701]\n",
      "epoch:5 step:5542 [D loss: 0.686982, acc.: 61.72%] [G loss: 0.814656]\n",
      "epoch:5 step:5543 [D loss: 0.678595, acc.: 52.34%] [G loss: 0.809929]\n",
      "epoch:5 step:5544 [D loss: 0.690918, acc.: 50.78%] [G loss: 0.827540]\n",
      "epoch:5 step:5545 [D loss: 0.691943, acc.: 53.12%] [G loss: 0.757338]\n",
      "epoch:5 step:5546 [D loss: 0.663268, acc.: 60.16%] [G loss: 0.800627]\n",
      "epoch:5 step:5547 [D loss: 0.693795, acc.: 57.81%] [G loss: 0.752178]\n",
      "epoch:5 step:5548 [D loss: 0.700702, acc.: 48.44%] [G loss: 0.721859]\n",
      "epoch:5 step:5549 [D loss: 0.671892, acc.: 55.47%] [G loss: 0.755645]\n",
      "epoch:5 step:5550 [D loss: 0.683848, acc.: 53.91%] [G loss: 0.721410]\n",
      "epoch:5 step:5551 [D loss: 0.639662, acc.: 64.84%] [G loss: 0.772086]\n",
      "epoch:5 step:5552 [D loss: 0.680439, acc.: 50.78%] [G loss: 0.770355]\n",
      "epoch:5 step:5553 [D loss: 0.694643, acc.: 57.03%] [G loss: 0.776937]\n",
      "epoch:5 step:5554 [D loss: 0.723402, acc.: 45.31%] [G loss: 0.770813]\n",
      "epoch:5 step:5555 [D loss: 0.690588, acc.: 55.47%] [G loss: 0.763549]\n",
      "epoch:5 step:5556 [D loss: 0.703508, acc.: 46.09%] [G loss: 0.780776]\n",
      "epoch:5 step:5557 [D loss: 0.678607, acc.: 53.12%] [G loss: 0.824060]\n",
      "epoch:5 step:5558 [D loss: 0.681132, acc.: 55.47%] [G loss: 0.793960]\n",
      "epoch:5 step:5559 [D loss: 0.657139, acc.: 64.06%] [G loss: 0.844907]\n",
      "epoch:5 step:5560 [D loss: 0.692359, acc.: 51.56%] [G loss: 0.761423]\n",
      "epoch:5 step:5561 [D loss: 0.719277, acc.: 43.75%] [G loss: 0.793354]\n",
      "epoch:5 step:5562 [D loss: 0.720219, acc.: 42.97%] [G loss: 0.807799]\n",
      "epoch:5 step:5563 [D loss: 0.715820, acc.: 43.75%] [G loss: 0.746752]\n",
      "epoch:5 step:5564 [D loss: 0.682138, acc.: 53.91%] [G loss: 0.755838]\n",
      "epoch:5 step:5565 [D loss: 0.681468, acc.: 55.47%] [G loss: 0.760320]\n",
      "epoch:5 step:5566 [D loss: 0.668481, acc.: 53.91%] [G loss: 0.739941]\n",
      "epoch:5 step:5567 [D loss: 0.701065, acc.: 50.00%] [G loss: 0.770276]\n",
      "epoch:5 step:5568 [D loss: 0.690455, acc.: 48.44%] [G loss: 0.756470]\n",
      "epoch:5 step:5569 [D loss: 0.694446, acc.: 50.00%] [G loss: 0.768248]\n",
      "epoch:5 step:5570 [D loss: 0.695663, acc.: 45.31%] [G loss: 0.804724]\n",
      "epoch:5 step:5571 [D loss: 0.674933, acc.: 55.47%] [G loss: 0.782913]\n",
      "epoch:5 step:5572 [D loss: 0.672952, acc.: 59.38%] [G loss: 0.805752]\n",
      "epoch:5 step:5573 [D loss: 0.659390, acc.: 59.38%] [G loss: 0.792806]\n",
      "epoch:5 step:5574 [D loss: 0.682104, acc.: 56.25%] [G loss: 0.793986]\n",
      "epoch:5 step:5575 [D loss: 0.698404, acc.: 52.34%] [G loss: 0.722654]\n",
      "epoch:5 step:5576 [D loss: 0.694447, acc.: 57.03%] [G loss: 0.776814]\n",
      "epoch:5 step:5577 [D loss: 0.691437, acc.: 48.44%] [G loss: 0.796286]\n",
      "epoch:5 step:5578 [D loss: 0.702656, acc.: 46.88%] [G loss: 0.792847]\n",
      "epoch:5 step:5579 [D loss: 0.679789, acc.: 53.91%] [G loss: 0.753863]\n",
      "epoch:5 step:5580 [D loss: 0.680038, acc.: 55.47%] [G loss: 0.755993]\n",
      "epoch:5 step:5581 [D loss: 0.700435, acc.: 50.00%] [G loss: 0.780806]\n",
      "epoch:5 step:5582 [D loss: 0.650256, acc.: 62.50%] [G loss: 0.797514]\n",
      "epoch:5 step:5583 [D loss: 0.703340, acc.: 54.69%] [G loss: 0.787804]\n",
      "epoch:5 step:5584 [D loss: 0.654532, acc.: 63.28%] [G loss: 0.801087]\n",
      "epoch:5 step:5585 [D loss: 0.686988, acc.: 53.91%] [G loss: 0.770546]\n",
      "epoch:5 step:5586 [D loss: 0.701246, acc.: 57.03%] [G loss: 0.825520]\n",
      "epoch:5 step:5587 [D loss: 0.677713, acc.: 53.91%] [G loss: 0.790090]\n",
      "epoch:5 step:5588 [D loss: 0.692113, acc.: 50.78%] [G loss: 0.753556]\n",
      "epoch:5 step:5589 [D loss: 0.703548, acc.: 45.31%] [G loss: 0.751058]\n",
      "epoch:5 step:5590 [D loss: 0.680510, acc.: 54.69%] [G loss: 0.772823]\n",
      "epoch:5 step:5591 [D loss: 0.675363, acc.: 56.25%] [G loss: 0.700668]\n",
      "epoch:5 step:5592 [D loss: 0.723309, acc.: 45.31%] [G loss: 0.780475]\n",
      "epoch:5 step:5593 [D loss: 0.690655, acc.: 50.78%] [G loss: 0.801293]\n",
      "epoch:5 step:5594 [D loss: 0.690278, acc.: 53.12%] [G loss: 0.809516]\n",
      "epoch:5 step:5595 [D loss: 0.720452, acc.: 44.53%] [G loss: 0.747858]\n",
      "epoch:5 step:5596 [D loss: 0.661392, acc.: 57.81%] [G loss: 0.768637]\n",
      "epoch:5 step:5597 [D loss: 0.706885, acc.: 50.00%] [G loss: 0.737285]\n",
      "epoch:5 step:5598 [D loss: 0.669173, acc.: 61.72%] [G loss: 0.772654]\n",
      "epoch:5 step:5599 [D loss: 0.696159, acc.: 53.91%] [G loss: 0.757225]\n",
      "epoch:5 step:5600 [D loss: 0.682132, acc.: 55.47%] [G loss: 0.754917]\n",
      "epoch:5 step:5601 [D loss: 0.689528, acc.: 54.69%] [G loss: 0.808888]\n",
      "epoch:5 step:5602 [D loss: 0.699028, acc.: 53.12%] [G loss: 0.842616]\n",
      "epoch:5 step:5603 [D loss: 0.680510, acc.: 54.69%] [G loss: 0.758806]\n",
      "epoch:5 step:5604 [D loss: 0.694031, acc.: 49.22%] [G loss: 0.765879]\n",
      "epoch:5 step:5605 [D loss: 0.684629, acc.: 50.00%] [G loss: 0.774912]\n",
      "epoch:5 step:5606 [D loss: 0.697827, acc.: 53.91%] [G loss: 0.734208]\n",
      "epoch:5 step:5607 [D loss: 0.688223, acc.: 53.12%] [G loss: 0.758443]\n",
      "epoch:5 step:5608 [D loss: 0.665326, acc.: 61.72%] [G loss: 0.757215]\n",
      "epoch:5 step:5609 [D loss: 0.715456, acc.: 46.88%] [G loss: 0.796600]\n",
      "epoch:5 step:5610 [D loss: 0.674430, acc.: 59.38%] [G loss: 0.830281]\n",
      "epoch:5 step:5611 [D loss: 0.679806, acc.: 58.59%] [G loss: 0.797206]\n",
      "epoch:5 step:5612 [D loss: 0.679280, acc.: 55.47%] [G loss: 0.799128]\n",
      "epoch:5 step:5613 [D loss: 0.664350, acc.: 57.03%] [G loss: 0.784522]\n",
      "epoch:5 step:5614 [D loss: 0.687780, acc.: 50.78%] [G loss: 0.796517]\n",
      "epoch:5 step:5615 [D loss: 0.664761, acc.: 60.16%] [G loss: 0.867526]\n",
      "epoch:5 step:5616 [D loss: 0.693225, acc.: 50.78%] [G loss: 0.779357]\n",
      "epoch:5 step:5617 [D loss: 0.724468, acc.: 49.22%] [G loss: 0.767574]\n",
      "epoch:5 step:5618 [D loss: 0.699258, acc.: 50.00%] [G loss: 0.746438]\n",
      "epoch:5 step:5619 [D loss: 0.674923, acc.: 57.03%] [G loss: 0.776887]\n",
      "epoch:5 step:5620 [D loss: 0.710881, acc.: 47.66%] [G loss: 0.753834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5621 [D loss: 0.655424, acc.: 59.38%] [G loss: 0.741091]\n",
      "epoch:5 step:5622 [D loss: 0.708013, acc.: 47.66%] [G loss: 0.795675]\n",
      "epoch:6 step:5623 [D loss: 0.684321, acc.: 50.00%] [G loss: 0.788520]\n",
      "epoch:6 step:5624 [D loss: 0.691943, acc.: 48.44%] [G loss: 0.763926]\n",
      "epoch:6 step:5625 [D loss: 0.662445, acc.: 60.94%] [G loss: 0.786807]\n",
      "epoch:6 step:5626 [D loss: 0.716005, acc.: 50.00%] [G loss: 0.794468]\n",
      "epoch:6 step:5627 [D loss: 0.675318, acc.: 55.47%] [G loss: 0.839704]\n",
      "epoch:6 step:5628 [D loss: 0.622596, acc.: 69.53%] [G loss: 0.818461]\n",
      "epoch:6 step:5629 [D loss: 0.664541, acc.: 53.91%] [G loss: 0.748303]\n",
      "epoch:6 step:5630 [D loss: 0.682087, acc.: 52.34%] [G loss: 0.767159]\n",
      "epoch:6 step:5631 [D loss: 0.715830, acc.: 50.78%] [G loss: 0.748271]\n",
      "epoch:6 step:5632 [D loss: 0.678549, acc.: 55.47%] [G loss: 0.817435]\n",
      "epoch:6 step:5633 [D loss: 0.699272, acc.: 53.12%] [G loss: 0.749358]\n",
      "epoch:6 step:5634 [D loss: 0.695153, acc.: 53.91%] [G loss: 0.770797]\n",
      "epoch:6 step:5635 [D loss: 0.718273, acc.: 47.66%] [G loss: 0.767059]\n",
      "epoch:6 step:5636 [D loss: 0.686434, acc.: 53.12%] [G loss: 0.733956]\n",
      "epoch:6 step:5637 [D loss: 0.683383, acc.: 57.81%] [G loss: 0.743971]\n",
      "epoch:6 step:5638 [D loss: 0.712200, acc.: 52.34%] [G loss: 0.780395]\n",
      "epoch:6 step:5639 [D loss: 0.681068, acc.: 53.91%] [G loss: 0.749957]\n",
      "epoch:6 step:5640 [D loss: 0.686125, acc.: 50.78%] [G loss: 0.781279]\n",
      "epoch:6 step:5641 [D loss: 0.687951, acc.: 55.47%] [G loss: 0.790924]\n",
      "epoch:6 step:5642 [D loss: 0.673076, acc.: 59.38%] [G loss: 0.763808]\n",
      "epoch:6 step:5643 [D loss: 0.730461, acc.: 47.66%] [G loss: 0.823431]\n",
      "epoch:6 step:5644 [D loss: 0.683413, acc.: 57.03%] [G loss: 0.773877]\n",
      "epoch:6 step:5645 [D loss: 0.676372, acc.: 57.81%] [G loss: 0.788953]\n",
      "epoch:6 step:5646 [D loss: 0.674329, acc.: 55.47%] [G loss: 0.817911]\n",
      "epoch:6 step:5647 [D loss: 0.700270, acc.: 46.88%] [G loss: 0.789458]\n",
      "epoch:6 step:5648 [D loss: 0.714590, acc.: 49.22%] [G loss: 0.808033]\n",
      "epoch:6 step:5649 [D loss: 0.678861, acc.: 60.94%] [G loss: 0.764755]\n",
      "epoch:6 step:5650 [D loss: 0.676122, acc.: 56.25%] [G loss: 0.791064]\n",
      "epoch:6 step:5651 [D loss: 0.673700, acc.: 59.38%] [G loss: 0.825091]\n",
      "epoch:6 step:5652 [D loss: 0.689874, acc.: 50.78%] [G loss: 0.788024]\n",
      "epoch:6 step:5653 [D loss: 0.705300, acc.: 50.78%] [G loss: 0.784457]\n",
      "epoch:6 step:5654 [D loss: 0.696312, acc.: 48.44%] [G loss: 0.762643]\n",
      "epoch:6 step:5655 [D loss: 0.673962, acc.: 58.59%] [G loss: 0.782956]\n",
      "epoch:6 step:5656 [D loss: 0.697621, acc.: 47.66%] [G loss: 0.987832]\n",
      "epoch:6 step:5657 [D loss: 0.697114, acc.: 57.03%] [G loss: 0.844880]\n",
      "epoch:6 step:5658 [D loss: 0.700632, acc.: 46.88%] [G loss: 0.757778]\n",
      "epoch:6 step:5659 [D loss: 0.697223, acc.: 54.69%] [G loss: 0.763545]\n",
      "epoch:6 step:5660 [D loss: 0.716948, acc.: 50.78%] [G loss: 0.748886]\n",
      "epoch:6 step:5661 [D loss: 0.697451, acc.: 49.22%] [G loss: 0.794012]\n",
      "epoch:6 step:5662 [D loss: 0.677213, acc.: 57.81%] [G loss: 0.756579]\n",
      "epoch:6 step:5663 [D loss: 0.653789, acc.: 63.28%] [G loss: 0.797980]\n",
      "epoch:6 step:5664 [D loss: 0.648265, acc.: 59.38%] [G loss: 0.819808]\n",
      "epoch:6 step:5665 [D loss: 0.660221, acc.: 57.81%] [G loss: 0.795241]\n",
      "epoch:6 step:5666 [D loss: 0.656371, acc.: 64.06%] [G loss: 0.762112]\n",
      "epoch:6 step:5667 [D loss: 0.693860, acc.: 49.22%] [G loss: 0.799297]\n",
      "epoch:6 step:5668 [D loss: 0.653630, acc.: 57.81%] [G loss: 0.751768]\n",
      "epoch:6 step:5669 [D loss: 0.685705, acc.: 52.34%] [G loss: 0.740284]\n",
      "epoch:6 step:5670 [D loss: 0.659157, acc.: 60.94%] [G loss: 0.760501]\n",
      "epoch:6 step:5671 [D loss: 0.669707, acc.: 57.03%] [G loss: 0.782171]\n",
      "epoch:6 step:5672 [D loss: 0.652134, acc.: 67.97%] [G loss: 0.734563]\n",
      "epoch:6 step:5673 [D loss: 0.677302, acc.: 57.81%] [G loss: 0.838834]\n",
      "epoch:6 step:5674 [D loss: 0.685465, acc.: 51.56%] [G loss: 0.698302]\n",
      "epoch:6 step:5675 [D loss: 0.664419, acc.: 61.72%] [G loss: 0.818794]\n",
      "epoch:6 step:5676 [D loss: 0.702022, acc.: 48.44%] [G loss: 0.849325]\n",
      "epoch:6 step:5677 [D loss: 0.704161, acc.: 47.66%] [G loss: 0.831249]\n",
      "epoch:6 step:5678 [D loss: 0.695285, acc.: 49.22%] [G loss: 0.786045]\n",
      "epoch:6 step:5679 [D loss: 0.715821, acc.: 47.66%] [G loss: 0.799193]\n",
      "epoch:6 step:5680 [D loss: 0.712255, acc.: 44.53%] [G loss: 0.823632]\n",
      "epoch:6 step:5681 [D loss: 0.688430, acc.: 56.25%] [G loss: 1.056099]\n",
      "epoch:6 step:5682 [D loss: 0.681029, acc.: 53.12%] [G loss: 0.812839]\n",
      "epoch:6 step:5683 [D loss: 0.669580, acc.: 55.47%] [G loss: 0.780351]\n",
      "epoch:6 step:5684 [D loss: 0.696254, acc.: 55.47%] [G loss: 0.873209]\n",
      "epoch:6 step:5685 [D loss: 0.690821, acc.: 46.88%] [G loss: 0.872413]\n",
      "epoch:6 step:5686 [D loss: 0.685702, acc.: 49.22%] [G loss: 0.849556]\n",
      "epoch:6 step:5687 [D loss: 0.680637, acc.: 55.47%] [G loss: 0.825188]\n",
      "epoch:6 step:5688 [D loss: 0.687674, acc.: 55.47%] [G loss: 0.840886]\n",
      "epoch:6 step:5689 [D loss: 0.666458, acc.: 60.16%] [G loss: 0.822277]\n",
      "epoch:6 step:5690 [D loss: 0.691189, acc.: 51.56%] [G loss: 0.837156]\n",
      "epoch:6 step:5691 [D loss: 0.658757, acc.: 63.28%] [G loss: 0.802074]\n",
      "epoch:6 step:5692 [D loss: 0.698919, acc.: 51.56%] [G loss: 0.812233]\n",
      "epoch:6 step:5693 [D loss: 0.697826, acc.: 50.78%] [G loss: 0.810735]\n",
      "epoch:6 step:5694 [D loss: 0.703949, acc.: 50.00%] [G loss: 0.780953]\n",
      "epoch:6 step:5695 [D loss: 0.712839, acc.: 50.00%] [G loss: 0.772585]\n",
      "epoch:6 step:5696 [D loss: 0.715435, acc.: 46.09%] [G loss: 0.804666]\n",
      "epoch:6 step:5697 [D loss: 0.679367, acc.: 54.69%] [G loss: 0.783140]\n",
      "epoch:6 step:5698 [D loss: 0.710271, acc.: 47.66%] [G loss: 0.742286]\n",
      "epoch:6 step:5699 [D loss: 0.670503, acc.: 56.25%] [G loss: 0.785360]\n",
      "epoch:6 step:5700 [D loss: 0.680934, acc.: 56.25%] [G loss: 0.741325]\n",
      "epoch:6 step:5701 [D loss: 0.678669, acc.: 54.69%] [G loss: 0.734179]\n",
      "epoch:6 step:5702 [D loss: 0.682160, acc.: 50.00%] [G loss: 0.740863]\n",
      "epoch:6 step:5703 [D loss: 0.703982, acc.: 49.22%] [G loss: 0.740589]\n",
      "epoch:6 step:5704 [D loss: 0.717112, acc.: 45.31%] [G loss: 0.812442]\n",
      "epoch:6 step:5705 [D loss: 0.669254, acc.: 63.28%] [G loss: 0.774048]\n",
      "epoch:6 step:5706 [D loss: 0.715915, acc.: 48.44%] [G loss: 0.794837]\n",
      "epoch:6 step:5707 [D loss: 0.648896, acc.: 62.50%] [G loss: 0.833952]\n",
      "epoch:6 step:5708 [D loss: 0.688825, acc.: 53.12%] [G loss: 0.800497]\n",
      "epoch:6 step:5709 [D loss: 0.679134, acc.: 58.59%] [G loss: 0.770268]\n",
      "epoch:6 step:5710 [D loss: 0.688946, acc.: 51.56%] [G loss: 0.860298]\n",
      "epoch:6 step:5711 [D loss: 0.665048, acc.: 58.59%] [G loss: 0.832561]\n",
      "epoch:6 step:5712 [D loss: 0.649579, acc.: 59.38%] [G loss: 0.877039]\n",
      "epoch:6 step:5713 [D loss: 0.673932, acc.: 48.44%] [G loss: 0.814810]\n",
      "epoch:6 step:5714 [D loss: 0.686848, acc.: 57.03%] [G loss: 0.796522]\n",
      "epoch:6 step:5715 [D loss: 0.673969, acc.: 54.69%] [G loss: 0.791602]\n",
      "epoch:6 step:5716 [D loss: 0.716098, acc.: 50.00%] [G loss: 0.762367]\n",
      "epoch:6 step:5717 [D loss: 0.702697, acc.: 45.31%] [G loss: 0.772452]\n",
      "epoch:6 step:5718 [D loss: 0.697575, acc.: 44.53%] [G loss: 0.729244]\n",
      "epoch:6 step:5719 [D loss: 0.689704, acc.: 53.91%] [G loss: 0.723103]\n",
      "epoch:6 step:5720 [D loss: 0.687284, acc.: 52.34%] [G loss: 0.722115]\n",
      "epoch:6 step:5721 [D loss: 0.691631, acc.: 48.44%] [G loss: 0.804289]\n",
      "epoch:6 step:5722 [D loss: 0.671686, acc.: 54.69%] [G loss: 0.725577]\n",
      "epoch:6 step:5723 [D loss: 0.693411, acc.: 54.69%] [G loss: 0.720451]\n",
      "epoch:6 step:5724 [D loss: 0.678521, acc.: 49.22%] [G loss: 0.741086]\n",
      "epoch:6 step:5725 [D loss: 0.677647, acc.: 53.12%] [G loss: 0.766035]\n",
      "epoch:6 step:5726 [D loss: 0.701268, acc.: 52.34%] [G loss: 0.746928]\n",
      "epoch:6 step:5727 [D loss: 0.665189, acc.: 60.16%] [G loss: 0.763776]\n",
      "epoch:6 step:5728 [D loss: 0.674596, acc.: 52.34%] [G loss: 0.792915]\n",
      "epoch:6 step:5729 [D loss: 0.674187, acc.: 60.16%] [G loss: 0.748717]\n",
      "epoch:6 step:5730 [D loss: 0.705845, acc.: 53.91%] [G loss: 0.749179]\n",
      "epoch:6 step:5731 [D loss: 0.701938, acc.: 48.44%] [G loss: 0.736181]\n",
      "epoch:6 step:5732 [D loss: 0.685092, acc.: 56.25%] [G loss: 0.767946]\n",
      "epoch:6 step:5733 [D loss: 0.705601, acc.: 53.12%] [G loss: 0.782108]\n",
      "epoch:6 step:5734 [D loss: 0.709662, acc.: 40.62%] [G loss: 0.787671]\n",
      "epoch:6 step:5735 [D loss: 0.700535, acc.: 54.69%] [G loss: 0.768615]\n",
      "epoch:6 step:5736 [D loss: 0.661703, acc.: 59.38%] [G loss: 0.796916]\n",
      "epoch:6 step:5737 [D loss: 0.697881, acc.: 48.44%] [G loss: 0.814461]\n",
      "epoch:6 step:5738 [D loss: 0.687901, acc.: 56.25%] [G loss: 0.803973]\n",
      "epoch:6 step:5739 [D loss: 0.691589, acc.: 54.69%] [G loss: 0.791835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5740 [D loss: 0.691388, acc.: 56.25%] [G loss: 0.772126]\n",
      "epoch:6 step:5741 [D loss: 0.679561, acc.: 57.03%] [G loss: 0.744066]\n",
      "epoch:6 step:5742 [D loss: 0.663276, acc.: 59.38%] [G loss: 0.771743]\n",
      "epoch:6 step:5743 [D loss: 0.676213, acc.: 59.38%] [G loss: 0.807491]\n",
      "epoch:6 step:5744 [D loss: 0.699251, acc.: 53.91%] [G loss: 0.805307]\n",
      "epoch:6 step:5745 [D loss: 0.710018, acc.: 40.62%] [G loss: 0.840324]\n",
      "epoch:6 step:5746 [D loss: 0.680272, acc.: 53.91%] [G loss: 0.783811]\n",
      "epoch:6 step:5747 [D loss: 0.676082, acc.: 57.81%] [G loss: 0.779506]\n",
      "epoch:6 step:5748 [D loss: 0.673901, acc.: 52.34%] [G loss: 0.809003]\n",
      "epoch:6 step:5749 [D loss: 0.675041, acc.: 51.56%] [G loss: 0.824466]\n",
      "epoch:6 step:5750 [D loss: 0.697222, acc.: 53.12%] [G loss: 0.787643]\n",
      "epoch:6 step:5751 [D loss: 0.638345, acc.: 65.62%] [G loss: 0.800527]\n",
      "epoch:6 step:5752 [D loss: 0.704844, acc.: 51.56%] [G loss: 0.801856]\n",
      "epoch:6 step:5753 [D loss: 0.684969, acc.: 57.81%] [G loss: 0.709262]\n",
      "epoch:6 step:5754 [D loss: 0.726000, acc.: 43.75%] [G loss: 0.716550]\n",
      "epoch:6 step:5755 [D loss: 0.708412, acc.: 46.09%] [G loss: 0.757437]\n",
      "epoch:6 step:5756 [D loss: 0.701171, acc.: 50.00%] [G loss: 0.795937]\n",
      "epoch:6 step:5757 [D loss: 0.691682, acc.: 50.78%] [G loss: 0.755583]\n",
      "epoch:6 step:5758 [D loss: 0.679827, acc.: 50.78%] [G loss: 0.747237]\n",
      "epoch:6 step:5759 [D loss: 0.708122, acc.: 49.22%] [G loss: 0.724746]\n",
      "epoch:6 step:5760 [D loss: 0.700304, acc.: 50.78%] [G loss: 0.781028]\n",
      "epoch:6 step:5761 [D loss: 0.688161, acc.: 58.59%] [G loss: 0.839887]\n",
      "epoch:6 step:5762 [D loss: 0.679386, acc.: 56.25%] [G loss: 0.820184]\n",
      "epoch:6 step:5763 [D loss: 0.705402, acc.: 55.47%] [G loss: 0.829302]\n",
      "epoch:6 step:5764 [D loss: 0.698187, acc.: 49.22%] [G loss: 0.754933]\n",
      "epoch:6 step:5765 [D loss: 0.683678, acc.: 59.38%] [G loss: 0.778948]\n",
      "epoch:6 step:5766 [D loss: 0.671919, acc.: 57.03%] [G loss: 0.740989]\n",
      "epoch:6 step:5767 [D loss: 0.684613, acc.: 51.56%] [G loss: 0.803959]\n",
      "epoch:6 step:5768 [D loss: 0.683455, acc.: 54.69%] [G loss: 0.735117]\n",
      "epoch:6 step:5769 [D loss: 0.683527, acc.: 57.03%] [G loss: 0.769589]\n",
      "epoch:6 step:5770 [D loss: 0.676306, acc.: 54.69%] [G loss: 0.765536]\n",
      "epoch:6 step:5771 [D loss: 0.696080, acc.: 50.78%] [G loss: 0.761197]\n",
      "epoch:6 step:5772 [D loss: 0.677315, acc.: 48.44%] [G loss: 0.758765]\n",
      "epoch:6 step:5773 [D loss: 0.678818, acc.: 55.47%] [G loss: 0.757120]\n",
      "epoch:6 step:5774 [D loss: 0.676289, acc.: 59.38%] [G loss: 0.826929]\n",
      "epoch:6 step:5775 [D loss: 0.699952, acc.: 55.47%] [G loss: 0.771512]\n",
      "epoch:6 step:5776 [D loss: 0.771283, acc.: 38.28%] [G loss: 0.758511]\n",
      "epoch:6 step:5777 [D loss: 0.652580, acc.: 65.62%] [G loss: 0.907036]\n",
      "epoch:6 step:5778 [D loss: 0.681221, acc.: 58.59%] [G loss: 0.836855]\n",
      "epoch:6 step:5779 [D loss: 0.689702, acc.: 58.59%] [G loss: 0.782348]\n",
      "epoch:6 step:5780 [D loss: 0.670699, acc.: 63.28%] [G loss: 0.807684]\n",
      "epoch:6 step:5781 [D loss: 0.689758, acc.: 53.12%] [G loss: 0.773753]\n",
      "epoch:6 step:5782 [D loss: 0.716890, acc.: 46.09%] [G loss: 0.757173]\n",
      "epoch:6 step:5783 [D loss: 0.708029, acc.: 49.22%] [G loss: 0.738448]\n",
      "epoch:6 step:5784 [D loss: 0.687748, acc.: 50.78%] [G loss: 0.783952]\n",
      "epoch:6 step:5785 [D loss: 0.703928, acc.: 46.88%] [G loss: 0.755437]\n",
      "epoch:6 step:5786 [D loss: 0.684729, acc.: 53.12%] [G loss: 0.823709]\n",
      "epoch:6 step:5787 [D loss: 0.679167, acc.: 53.12%] [G loss: 0.815018]\n",
      "epoch:6 step:5788 [D loss: 0.688679, acc.: 51.56%] [G loss: 0.769081]\n",
      "epoch:6 step:5789 [D loss: 0.662411, acc.: 55.47%] [G loss: 0.778384]\n",
      "epoch:6 step:5790 [D loss: 0.695799, acc.: 51.56%] [G loss: 0.796061]\n",
      "epoch:6 step:5791 [D loss: 0.680945, acc.: 57.03%] [G loss: 0.783476]\n",
      "epoch:6 step:5792 [D loss: 0.684769, acc.: 55.47%] [G loss: 0.796213]\n",
      "epoch:6 step:5793 [D loss: 0.729208, acc.: 38.28%] [G loss: 0.760055]\n",
      "epoch:6 step:5794 [D loss: 0.705360, acc.: 53.12%] [G loss: 0.743478]\n",
      "epoch:6 step:5795 [D loss: 0.672807, acc.: 56.25%] [G loss: 0.756593]\n",
      "epoch:6 step:5796 [D loss: 0.666529, acc.: 60.94%] [G loss: 0.731616]\n",
      "epoch:6 step:5797 [D loss: 0.699331, acc.: 46.09%] [G loss: 0.730542]\n",
      "epoch:6 step:5798 [D loss: 0.688455, acc.: 55.47%] [G loss: 0.698641]\n",
      "epoch:6 step:5799 [D loss: 0.681173, acc.: 58.59%] [G loss: 0.742473]\n",
      "epoch:6 step:5800 [D loss: 0.681055, acc.: 53.12%] [G loss: 0.730017]\n",
      "epoch:6 step:5801 [D loss: 0.681026, acc.: 56.25%] [G loss: 0.752957]\n",
      "epoch:6 step:5802 [D loss: 0.663950, acc.: 54.69%] [G loss: 0.753017]\n",
      "epoch:6 step:5803 [D loss: 0.707431, acc.: 50.78%] [G loss: 0.763470]\n",
      "epoch:6 step:5804 [D loss: 0.675846, acc.: 58.59%] [G loss: 0.774154]\n",
      "epoch:6 step:5805 [D loss: 0.706070, acc.: 50.00%] [G loss: 0.728789]\n",
      "epoch:6 step:5806 [D loss: 0.690017, acc.: 50.00%] [G loss: 0.765005]\n",
      "epoch:6 step:5807 [D loss: 0.721884, acc.: 46.09%] [G loss: 0.782665]\n",
      "epoch:6 step:5808 [D loss: 0.690022, acc.: 49.22%] [G loss: 0.760567]\n",
      "epoch:6 step:5809 [D loss: 0.702959, acc.: 51.56%] [G loss: 0.748680]\n",
      "epoch:6 step:5810 [D loss: 0.657911, acc.: 56.25%] [G loss: 0.812772]\n",
      "epoch:6 step:5811 [D loss: 0.690154, acc.: 50.00%] [G loss: 0.807898]\n",
      "epoch:6 step:5812 [D loss: 0.702633, acc.: 53.12%] [G loss: 0.877895]\n",
      "epoch:6 step:5813 [D loss: 0.719338, acc.: 51.56%] [G loss: 0.815499]\n",
      "epoch:6 step:5814 [D loss: 0.673283, acc.: 60.16%] [G loss: 0.803067]\n",
      "epoch:6 step:5815 [D loss: 0.709933, acc.: 52.34%] [G loss: 0.788521]\n",
      "epoch:6 step:5816 [D loss: 0.709799, acc.: 52.34%] [G loss: 0.834510]\n",
      "epoch:6 step:5817 [D loss: 0.665318, acc.: 60.94%] [G loss: 0.783120]\n",
      "epoch:6 step:5818 [D loss: 0.679609, acc.: 51.56%] [G loss: 0.753705]\n",
      "epoch:6 step:5819 [D loss: 0.679083, acc.: 58.59%] [G loss: 0.769092]\n",
      "epoch:6 step:5820 [D loss: 0.697415, acc.: 46.09%] [G loss: 0.785497]\n",
      "epoch:6 step:5821 [D loss: 0.671766, acc.: 57.81%] [G loss: 0.759243]\n",
      "epoch:6 step:5822 [D loss: 0.671839, acc.: 57.81%] [G loss: 0.732388]\n",
      "epoch:6 step:5823 [D loss: 0.692373, acc.: 53.12%] [G loss: 0.739485]\n",
      "epoch:6 step:5824 [D loss: 0.691992, acc.: 48.44%] [G loss: 0.829059]\n",
      "epoch:6 step:5825 [D loss: 0.680996, acc.: 52.34%] [G loss: 0.809535]\n",
      "epoch:6 step:5826 [D loss: 0.683377, acc.: 55.47%] [G loss: 0.739248]\n",
      "epoch:6 step:5827 [D loss: 0.716922, acc.: 46.09%] [G loss: 0.740358]\n",
      "epoch:6 step:5828 [D loss: 0.686323, acc.: 57.81%] [G loss: 0.792398]\n",
      "epoch:6 step:5829 [D loss: 0.671111, acc.: 62.50%] [G loss: 0.768903]\n",
      "epoch:6 step:5830 [D loss: 0.655739, acc.: 67.97%] [G loss: 0.766473]\n",
      "epoch:6 step:5831 [D loss: 0.694660, acc.: 50.78%] [G loss: 0.810904]\n",
      "epoch:6 step:5832 [D loss: 0.656076, acc.: 57.81%] [G loss: 0.798799]\n",
      "epoch:6 step:5833 [D loss: 0.684597, acc.: 50.78%] [G loss: 0.787020]\n",
      "epoch:6 step:5834 [D loss: 0.676618, acc.: 60.94%] [G loss: 0.759350]\n",
      "epoch:6 step:5835 [D loss: 0.714700, acc.: 40.62%] [G loss: 0.770035]\n",
      "epoch:6 step:5836 [D loss: 0.708301, acc.: 48.44%] [G loss: 0.772471]\n",
      "epoch:6 step:5837 [D loss: 0.687096, acc.: 57.03%] [G loss: 0.784804]\n",
      "epoch:6 step:5838 [D loss: 0.664901, acc.: 62.50%] [G loss: 0.786090]\n",
      "epoch:6 step:5839 [D loss: 0.710676, acc.: 46.88%] [G loss: 0.782590]\n",
      "epoch:6 step:5840 [D loss: 0.708442, acc.: 49.22%] [G loss: 0.761753]\n",
      "epoch:6 step:5841 [D loss: 0.657635, acc.: 54.69%] [G loss: 0.759498]\n",
      "epoch:6 step:5842 [D loss: 0.684220, acc.: 56.25%] [G loss: 0.770520]\n",
      "epoch:6 step:5843 [D loss: 0.725672, acc.: 42.97%] [G loss: 0.807191]\n",
      "epoch:6 step:5844 [D loss: 0.688866, acc.: 51.56%] [G loss: 0.754198]\n",
      "epoch:6 step:5845 [D loss: 0.697223, acc.: 53.91%] [G loss: 0.782348]\n",
      "epoch:6 step:5846 [D loss: 0.691667, acc.: 56.25%] [G loss: 0.755937]\n",
      "epoch:6 step:5847 [D loss: 0.674234, acc.: 53.12%] [G loss: 0.731595]\n",
      "epoch:6 step:5848 [D loss: 0.701296, acc.: 50.00%] [G loss: 0.710683]\n",
      "epoch:6 step:5849 [D loss: 0.677062, acc.: 61.72%] [G loss: 0.800226]\n",
      "epoch:6 step:5850 [D loss: 0.685209, acc.: 55.47%] [G loss: 0.732536]\n",
      "epoch:6 step:5851 [D loss: 0.670215, acc.: 59.38%] [G loss: 0.761109]\n",
      "epoch:6 step:5852 [D loss: 0.697779, acc.: 53.12%] [G loss: 0.725797]\n",
      "epoch:6 step:5853 [D loss: 0.698367, acc.: 55.47%] [G loss: 0.755050]\n",
      "epoch:6 step:5854 [D loss: 0.675332, acc.: 54.69%] [G loss: 0.743856]\n",
      "epoch:6 step:5855 [D loss: 0.685026, acc.: 50.00%] [G loss: 0.772966]\n",
      "epoch:6 step:5856 [D loss: 0.674804, acc.: 55.47%] [G loss: 0.770089]\n",
      "epoch:6 step:5857 [D loss: 0.698542, acc.: 48.44%] [G loss: 0.796835]\n",
      "epoch:6 step:5858 [D loss: 0.683963, acc.: 59.38%] [G loss: 0.803683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5859 [D loss: 0.694127, acc.: 53.12%] [G loss: 0.781869]\n",
      "epoch:6 step:5860 [D loss: 0.696286, acc.: 53.12%] [G loss: 0.819952]\n",
      "epoch:6 step:5861 [D loss: 0.700542, acc.: 48.44%] [G loss: 0.829827]\n",
      "epoch:6 step:5862 [D loss: 0.713581, acc.: 54.69%] [G loss: 0.790193]\n",
      "epoch:6 step:5863 [D loss: 0.710132, acc.: 50.00%] [G loss: 0.739994]\n",
      "epoch:6 step:5864 [D loss: 0.666635, acc.: 52.34%] [G loss: 0.728400]\n",
      "epoch:6 step:5865 [D loss: 0.691596, acc.: 51.56%] [G loss: 0.747098]\n",
      "epoch:6 step:5866 [D loss: 0.685606, acc.: 50.78%] [G loss: 0.777102]\n",
      "epoch:6 step:5867 [D loss: 0.717205, acc.: 42.97%] [G loss: 0.776977]\n",
      "epoch:6 step:5868 [D loss: 0.691795, acc.: 53.91%] [G loss: 0.828118]\n",
      "epoch:6 step:5869 [D loss: 0.670705, acc.: 60.94%] [G loss: 0.797056]\n",
      "epoch:6 step:5870 [D loss: 0.685806, acc.: 57.03%] [G loss: 0.826500]\n",
      "epoch:6 step:5871 [D loss: 0.695837, acc.: 51.56%] [G loss: 0.797686]\n",
      "epoch:6 step:5872 [D loss: 0.654460, acc.: 61.72%] [G loss: 0.791052]\n",
      "epoch:6 step:5873 [D loss: 0.656345, acc.: 60.94%] [G loss: 0.817838]\n",
      "epoch:6 step:5874 [D loss: 0.662642, acc.: 66.41%] [G loss: 0.790040]\n",
      "epoch:6 step:5875 [D loss: 0.676625, acc.: 50.78%] [G loss: 0.783531]\n",
      "epoch:6 step:5876 [D loss: 0.702739, acc.: 48.44%] [G loss: 0.799796]\n",
      "epoch:6 step:5877 [D loss: 0.664084, acc.: 61.72%] [G loss: 0.791214]\n",
      "epoch:6 step:5878 [D loss: 0.715136, acc.: 50.78%] [G loss: 0.777021]\n",
      "epoch:6 step:5879 [D loss: 0.670901, acc.: 57.81%] [G loss: 0.801601]\n",
      "epoch:6 step:5880 [D loss: 0.649067, acc.: 67.97%] [G loss: 0.783628]\n",
      "epoch:6 step:5881 [D loss: 0.665967, acc.: 63.28%] [G loss: 0.785645]\n",
      "epoch:6 step:5882 [D loss: 0.669690, acc.: 59.38%] [G loss: 0.752828]\n",
      "epoch:6 step:5883 [D loss: 0.676858, acc.: 60.16%] [G loss: 0.789323]\n",
      "epoch:6 step:5884 [D loss: 0.664970, acc.: 66.41%] [G loss: 0.805925]\n",
      "epoch:6 step:5885 [D loss: 0.658362, acc.: 67.97%] [G loss: 0.799344]\n",
      "epoch:6 step:5886 [D loss: 0.650277, acc.: 58.59%] [G loss: 0.841258]\n",
      "epoch:6 step:5887 [D loss: 0.657822, acc.: 61.72%] [G loss: 0.851834]\n",
      "epoch:6 step:5888 [D loss: 0.636365, acc.: 67.19%] [G loss: 0.836897]\n",
      "epoch:6 step:5889 [D loss: 0.675611, acc.: 58.59%] [G loss: 0.857500]\n",
      "epoch:6 step:5890 [D loss: 0.665927, acc.: 58.59%] [G loss: 0.866278]\n",
      "epoch:6 step:5891 [D loss: 0.679901, acc.: 51.56%] [G loss: 0.795199]\n",
      "epoch:6 step:5892 [D loss: 0.653792, acc.: 67.19%] [G loss: 0.906448]\n",
      "epoch:6 step:5893 [D loss: 0.654274, acc.: 60.94%] [G loss: 0.888104]\n",
      "epoch:6 step:5894 [D loss: 0.702358, acc.: 56.25%] [G loss: 0.834418]\n",
      "epoch:6 step:5895 [D loss: 0.703020, acc.: 53.12%] [G loss: 0.868054]\n",
      "epoch:6 step:5896 [D loss: 0.656563, acc.: 60.94%] [G loss: 0.882512]\n",
      "epoch:6 step:5897 [D loss: 0.691307, acc.: 52.34%] [G loss: 0.845793]\n",
      "epoch:6 step:5898 [D loss: 0.703907, acc.: 51.56%] [G loss: 0.868823]\n",
      "epoch:6 step:5899 [D loss: 0.719667, acc.: 43.75%] [G loss: 0.762913]\n",
      "epoch:6 step:5900 [D loss: 0.705604, acc.: 49.22%] [G loss: 0.916517]\n",
      "epoch:6 step:5901 [D loss: 0.690604, acc.: 46.09%] [G loss: 0.781724]\n",
      "epoch:6 step:5902 [D loss: 0.670467, acc.: 51.56%] [G loss: 0.911303]\n",
      "epoch:6 step:5903 [D loss: 0.696136, acc.: 46.88%] [G loss: 0.848041]\n",
      "epoch:6 step:5904 [D loss: 0.663059, acc.: 62.50%] [G loss: 0.756636]\n",
      "epoch:6 step:5905 [D loss: 0.629911, acc.: 68.75%] [G loss: 0.830486]\n",
      "epoch:6 step:5906 [D loss: 0.653018, acc.: 63.28%] [G loss: 0.798549]\n",
      "epoch:6 step:5907 [D loss: 0.639596, acc.: 64.06%] [G loss: 0.798970]\n",
      "epoch:6 step:5908 [D loss: 0.682761, acc.: 57.03%] [G loss: 0.818522]\n",
      "epoch:6 step:5909 [D loss: 0.680591, acc.: 56.25%] [G loss: 0.936456]\n",
      "epoch:6 step:5910 [D loss: 0.708840, acc.: 49.22%] [G loss: 0.846447]\n",
      "epoch:6 step:5911 [D loss: 0.698610, acc.: 54.69%] [G loss: 0.804874]\n",
      "epoch:6 step:5912 [D loss: 0.655658, acc.: 56.25%] [G loss: 0.721089]\n",
      "epoch:6 step:5913 [D loss: 0.724990, acc.: 49.22%] [G loss: 0.829624]\n",
      "epoch:6 step:5914 [D loss: 0.685122, acc.: 53.91%] [G loss: 0.818238]\n",
      "epoch:6 step:5915 [D loss: 0.680117, acc.: 57.03%] [G loss: 0.803679]\n",
      "epoch:6 step:5916 [D loss: 0.676160, acc.: 55.47%] [G loss: 0.803003]\n",
      "epoch:6 step:5917 [D loss: 0.690481, acc.: 52.34%] [G loss: 0.803435]\n",
      "epoch:6 step:5918 [D loss: 0.640818, acc.: 60.94%] [G loss: 0.812534]\n",
      "epoch:6 step:5919 [D loss: 0.644938, acc.: 64.84%] [G loss: 0.858385]\n",
      "epoch:6 step:5920 [D loss: 0.665032, acc.: 53.12%] [G loss: 0.857628]\n",
      "epoch:6 step:5921 [D loss: 0.691983, acc.: 50.78%] [G loss: 0.816918]\n",
      "epoch:6 step:5922 [D loss: 0.706569, acc.: 55.47%] [G loss: 0.780258]\n",
      "epoch:6 step:5923 [D loss: 0.679386, acc.: 60.16%] [G loss: 0.870827]\n",
      "epoch:6 step:5924 [D loss: 0.675967, acc.: 58.59%] [G loss: 0.815737]\n",
      "epoch:6 step:5925 [D loss: 0.686649, acc.: 53.12%] [G loss: 0.794507]\n",
      "epoch:6 step:5926 [D loss: 0.709350, acc.: 48.44%] [G loss: 0.820561]\n",
      "epoch:6 step:5927 [D loss: 0.672892, acc.: 53.91%] [G loss: 0.786396]\n",
      "epoch:6 step:5928 [D loss: 0.700518, acc.: 46.88%] [G loss: 0.731231]\n",
      "epoch:6 step:5929 [D loss: 0.714920, acc.: 48.44%] [G loss: 0.787338]\n",
      "epoch:6 step:5930 [D loss: 0.726948, acc.: 38.28%] [G loss: 0.785279]\n",
      "epoch:6 step:5931 [D loss: 0.671883, acc.: 51.56%] [G loss: 0.864188]\n",
      "epoch:6 step:5932 [D loss: 0.714287, acc.: 44.53%] [G loss: 0.839253]\n",
      "epoch:6 step:5933 [D loss: 0.691783, acc.: 51.56%] [G loss: 0.808143]\n",
      "epoch:6 step:5934 [D loss: 0.691085, acc.: 57.03%] [G loss: 0.805889]\n",
      "epoch:6 step:5935 [D loss: 0.668718, acc.: 60.16%] [G loss: 0.804407]\n",
      "epoch:6 step:5936 [D loss: 0.665357, acc.: 59.38%] [G loss: 0.759672]\n",
      "epoch:6 step:5937 [D loss: 0.646828, acc.: 62.50%] [G loss: 0.796920]\n",
      "epoch:6 step:5938 [D loss: 0.681071, acc.: 53.91%] [G loss: 0.815714]\n",
      "epoch:6 step:5939 [D loss: 0.651612, acc.: 60.16%] [G loss: 0.824555]\n",
      "epoch:6 step:5940 [D loss: 0.672659, acc.: 59.38%] [G loss: 0.717854]\n",
      "epoch:6 step:5941 [D loss: 0.662013, acc.: 56.25%] [G loss: 0.824964]\n",
      "epoch:6 step:5942 [D loss: 0.717734, acc.: 47.66%] [G loss: 0.757185]\n",
      "epoch:6 step:5943 [D loss: 0.717001, acc.: 44.53%] [G loss: 0.867025]\n",
      "epoch:6 step:5944 [D loss: 0.662620, acc.: 60.16%] [G loss: 0.761534]\n",
      "epoch:6 step:5945 [D loss: 0.669773, acc.: 57.03%] [G loss: 0.874178]\n",
      "epoch:6 step:5946 [D loss: 0.677146, acc.: 54.69%] [G loss: 0.789651]\n",
      "epoch:6 step:5947 [D loss: 0.683886, acc.: 53.91%] [G loss: 0.767934]\n",
      "epoch:6 step:5948 [D loss: 0.725015, acc.: 49.22%] [G loss: 0.749013]\n",
      "epoch:6 step:5949 [D loss: 0.689085, acc.: 59.38%] [G loss: 0.793864]\n",
      "epoch:6 step:5950 [D loss: 0.649344, acc.: 60.94%] [G loss: 0.817265]\n",
      "epoch:6 step:5951 [D loss: 0.677716, acc.: 57.81%] [G loss: 0.931172]\n",
      "epoch:6 step:5952 [D loss: 0.668022, acc.: 51.56%] [G loss: 0.863259]\n",
      "epoch:6 step:5953 [D loss: 0.666884, acc.: 59.38%] [G loss: 0.849555]\n",
      "epoch:6 step:5954 [D loss: 0.708119, acc.: 54.69%] [G loss: 0.857979]\n",
      "epoch:6 step:5955 [D loss: 0.709068, acc.: 55.47%] [G loss: 0.784496]\n",
      "epoch:6 step:5956 [D loss: 0.683959, acc.: 54.69%] [G loss: 0.782992]\n",
      "epoch:6 step:5957 [D loss: 0.667180, acc.: 63.28%] [G loss: 0.796798]\n",
      "epoch:6 step:5958 [D loss: 0.685389, acc.: 53.91%] [G loss: 0.808349]\n",
      "epoch:6 step:5959 [D loss: 0.676082, acc.: 55.47%] [G loss: 0.816923]\n",
      "epoch:6 step:5960 [D loss: 0.702303, acc.: 47.66%] [G loss: 0.766981]\n",
      "epoch:6 step:5961 [D loss: 0.687806, acc.: 53.12%] [G loss: 0.757738]\n",
      "epoch:6 step:5962 [D loss: 0.694452, acc.: 53.12%] [G loss: 0.728121]\n",
      "epoch:6 step:5963 [D loss: 0.668168, acc.: 64.84%] [G loss: 0.825804]\n",
      "epoch:6 step:5964 [D loss: 0.700403, acc.: 48.44%] [G loss: 0.766675]\n",
      "epoch:6 step:5965 [D loss: 0.656558, acc.: 59.38%] [G loss: 0.795095]\n",
      "epoch:6 step:5966 [D loss: 0.708900, acc.: 53.12%] [G loss: 0.806159]\n",
      "epoch:6 step:5967 [D loss: 0.686781, acc.: 54.69%] [G loss: 0.794978]\n",
      "epoch:6 step:5968 [D loss: 0.684249, acc.: 55.47%] [G loss: 0.849297]\n",
      "epoch:6 step:5969 [D loss: 0.709798, acc.: 49.22%] [G loss: 0.884770]\n",
      "epoch:6 step:5970 [D loss: 0.711792, acc.: 43.75%] [G loss: 0.848073]\n",
      "epoch:6 step:5971 [D loss: 0.677202, acc.: 53.12%] [G loss: 0.807984]\n",
      "epoch:6 step:5972 [D loss: 0.695276, acc.: 50.78%] [G loss: 0.767380]\n",
      "epoch:6 step:5973 [D loss: 0.696647, acc.: 46.88%] [G loss: 0.769674]\n",
      "epoch:6 step:5974 [D loss: 0.684797, acc.: 55.47%] [G loss: 0.801642]\n",
      "epoch:6 step:5975 [D loss: 0.687234, acc.: 52.34%] [G loss: 0.795606]\n",
      "epoch:6 step:5976 [D loss: 0.687218, acc.: 58.59%] [G loss: 0.823355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5977 [D loss: 0.675413, acc.: 56.25%] [G loss: 0.793496]\n",
      "epoch:6 step:5978 [D loss: 0.724410, acc.: 39.84%] [G loss: 0.762901]\n",
      "epoch:6 step:5979 [D loss: 0.676981, acc.: 47.66%] [G loss: 0.819088]\n",
      "epoch:6 step:5980 [D loss: 0.662042, acc.: 72.66%] [G loss: 0.730543]\n",
      "epoch:6 step:5981 [D loss: 0.692726, acc.: 49.22%] [G loss: 0.817039]\n",
      "epoch:6 step:5982 [D loss: 0.686964, acc.: 60.16%] [G loss: 0.750917]\n",
      "epoch:6 step:5983 [D loss: 0.687161, acc.: 51.56%] [G loss: 0.753994]\n",
      "epoch:6 step:5984 [D loss: 0.677702, acc.: 64.06%] [G loss: 0.844147]\n",
      "epoch:6 step:5985 [D loss: 0.643098, acc.: 64.06%] [G loss: 0.852729]\n",
      "epoch:6 step:5986 [D loss: 0.665835, acc.: 54.69%] [G loss: 0.820825]\n",
      "epoch:6 step:5987 [D loss: 0.685094, acc.: 62.50%] [G loss: 0.814038]\n",
      "epoch:6 step:5988 [D loss: 0.679705, acc.: 60.94%] [G loss: 0.765063]\n",
      "epoch:6 step:5989 [D loss: 0.698084, acc.: 53.91%] [G loss: 0.849517]\n",
      "epoch:6 step:5990 [D loss: 0.674490, acc.: 60.94%] [G loss: 0.850557]\n",
      "epoch:6 step:5991 [D loss: 0.667582, acc.: 60.16%] [G loss: 0.882068]\n",
      "epoch:6 step:5992 [D loss: 0.675345, acc.: 60.94%] [G loss: 0.790905]\n",
      "epoch:6 step:5993 [D loss: 0.701137, acc.: 50.00%] [G loss: 0.815932]\n",
      "epoch:6 step:5994 [D loss: 0.651566, acc.: 60.94%] [G loss: 0.816193]\n",
      "epoch:6 step:5995 [D loss: 0.694771, acc.: 46.09%] [G loss: 0.827897]\n",
      "epoch:6 step:5996 [D loss: 0.730350, acc.: 45.31%] [G loss: 0.820227]\n",
      "epoch:6 step:5997 [D loss: 0.668325, acc.: 60.94%] [G loss: 0.788693]\n",
      "epoch:6 step:5998 [D loss: 0.701523, acc.: 50.78%] [G loss: 0.873198]\n",
      "epoch:6 step:5999 [D loss: 0.716403, acc.: 50.78%] [G loss: 0.789980]\n",
      "epoch:6 step:6000 [D loss: 0.681575, acc.: 61.72%] [G loss: 0.820991]\n",
      "epoch:6 step:6001 [D loss: 0.645763, acc.: 66.41%] [G loss: 0.838027]\n",
      "epoch:6 step:6002 [D loss: 0.713983, acc.: 47.66%] [G loss: 0.834392]\n",
      "epoch:6 step:6003 [D loss: 0.656547, acc.: 62.50%] [G loss: 0.778226]\n",
      "epoch:6 step:6004 [D loss: 0.690505, acc.: 50.00%] [G loss: 0.788644]\n",
      "epoch:6 step:6005 [D loss: 0.701033, acc.: 44.53%] [G loss: 0.743996]\n",
      "epoch:6 step:6006 [D loss: 0.694480, acc.: 52.34%] [G loss: 0.742071]\n",
      "epoch:6 step:6007 [D loss: 0.682002, acc.: 54.69%] [G loss: 0.792324]\n",
      "epoch:6 step:6008 [D loss: 0.649745, acc.: 64.06%] [G loss: 0.815313]\n",
      "epoch:6 step:6009 [D loss: 0.697275, acc.: 53.91%] [G loss: 0.746582]\n",
      "epoch:6 step:6010 [D loss: 0.664363, acc.: 57.03%] [G loss: 0.787801]\n",
      "epoch:6 step:6011 [D loss: 0.734928, acc.: 41.41%] [G loss: 0.811447]\n",
      "epoch:6 step:6012 [D loss: 0.705744, acc.: 50.78%] [G loss: 0.781537]\n",
      "epoch:6 step:6013 [D loss: 0.672103, acc.: 60.16%] [G loss: 0.863988]\n",
      "epoch:6 step:6014 [D loss: 0.687055, acc.: 54.69%] [G loss: 0.761723]\n",
      "epoch:6 step:6015 [D loss: 0.709602, acc.: 45.31%] [G loss: 0.784024]\n",
      "epoch:6 step:6016 [D loss: 0.692815, acc.: 51.56%] [G loss: 0.821999]\n",
      "epoch:6 step:6017 [D loss: 0.663355, acc.: 59.38%] [G loss: 0.767495]\n",
      "epoch:6 step:6018 [D loss: 0.685291, acc.: 53.12%] [G loss: 0.790845]\n",
      "epoch:6 step:6019 [D loss: 0.663122, acc.: 56.25%] [G loss: 0.793083]\n",
      "epoch:6 step:6020 [D loss: 0.708453, acc.: 50.00%] [G loss: 0.831384]\n",
      "epoch:6 step:6021 [D loss: 0.669749, acc.: 55.47%] [G loss: 0.796116]\n",
      "epoch:6 step:6022 [D loss: 0.655202, acc.: 56.25%] [G loss: 0.752122]\n",
      "epoch:6 step:6023 [D loss: 0.675805, acc.: 49.22%] [G loss: 0.768055]\n",
      "epoch:6 step:6024 [D loss: 0.672445, acc.: 51.56%] [G loss: 0.759372]\n",
      "epoch:6 step:6025 [D loss: 0.679617, acc.: 54.69%] [G loss: 0.753243]\n",
      "epoch:6 step:6026 [D loss: 0.687835, acc.: 56.25%] [G loss: 0.816279]\n",
      "epoch:6 step:6027 [D loss: 0.650441, acc.: 63.28%] [G loss: 0.771566]\n",
      "epoch:6 step:6028 [D loss: 0.688369, acc.: 56.25%] [G loss: 0.722512]\n",
      "epoch:6 step:6029 [D loss: 0.632599, acc.: 64.84%] [G loss: 0.814252]\n",
      "epoch:6 step:6030 [D loss: 0.702191, acc.: 48.44%] [G loss: 0.774833]\n",
      "epoch:6 step:6031 [D loss: 0.665660, acc.: 55.47%] [G loss: 0.768648]\n",
      "epoch:6 step:6032 [D loss: 0.695233, acc.: 49.22%] [G loss: 0.830943]\n",
      "epoch:6 step:6033 [D loss: 0.686328, acc.: 46.88%] [G loss: 0.769949]\n",
      "epoch:6 step:6034 [D loss: 0.711604, acc.: 51.56%] [G loss: 0.751364]\n",
      "epoch:6 step:6035 [D loss: 0.759522, acc.: 53.12%] [G loss: 0.820406]\n",
      "epoch:6 step:6036 [D loss: 0.662685, acc.: 54.69%] [G loss: 0.930002]\n",
      "epoch:6 step:6037 [D loss: 0.655632, acc.: 60.94%] [G loss: 0.902759]\n",
      "epoch:6 step:6038 [D loss: 0.697695, acc.: 54.69%] [G loss: 0.907032]\n",
      "epoch:6 step:6039 [D loss: 0.655753, acc.: 64.06%] [G loss: 0.777923]\n",
      "epoch:6 step:6040 [D loss: 0.672856, acc.: 60.16%] [G loss: 0.820066]\n",
      "epoch:6 step:6041 [D loss: 0.724496, acc.: 47.66%] [G loss: 0.775994]\n",
      "epoch:6 step:6042 [D loss: 0.706690, acc.: 51.56%] [G loss: 0.812918]\n",
      "epoch:6 step:6043 [D loss: 0.708541, acc.: 53.12%] [G loss: 0.782911]\n",
      "epoch:6 step:6044 [D loss: 0.724086, acc.: 47.66%] [G loss: 0.776781]\n",
      "epoch:6 step:6045 [D loss: 0.691343, acc.: 55.47%] [G loss: 0.809054]\n",
      "epoch:6 step:6046 [D loss: 0.661712, acc.: 60.16%] [G loss: 0.789479]\n",
      "epoch:6 step:6047 [D loss: 0.681647, acc.: 52.34%] [G loss: 0.778953]\n",
      "epoch:6 step:6048 [D loss: 0.672720, acc.: 59.38%] [G loss: 0.777337]\n",
      "epoch:6 step:6049 [D loss: 0.671580, acc.: 60.16%] [G loss: 0.833204]\n",
      "epoch:6 step:6050 [D loss: 0.683401, acc.: 59.38%] [G loss: 0.758018]\n",
      "epoch:6 step:6051 [D loss: 0.682641, acc.: 60.94%] [G loss: 0.822909]\n",
      "epoch:6 step:6052 [D loss: 0.692743, acc.: 58.59%] [G loss: 0.740205]\n",
      "epoch:6 step:6053 [D loss: 0.654563, acc.: 57.03%] [G loss: 0.841338]\n",
      "epoch:6 step:6054 [D loss: 0.695996, acc.: 54.69%] [G loss: 0.808312]\n",
      "epoch:6 step:6055 [D loss: 0.693111, acc.: 51.56%] [G loss: 0.784951]\n",
      "epoch:6 step:6056 [D loss: 0.636543, acc.: 62.50%] [G loss: 0.823553]\n",
      "epoch:6 step:6057 [D loss: 0.660265, acc.: 59.38%] [G loss: 0.742533]\n",
      "epoch:6 step:6058 [D loss: 0.661274, acc.: 58.59%] [G loss: 0.796087]\n",
      "epoch:6 step:6059 [D loss: 0.672415, acc.: 59.38%] [G loss: 0.846598]\n",
      "epoch:6 step:6060 [D loss: 0.670442, acc.: 59.38%] [G loss: 0.805535]\n",
      "epoch:6 step:6061 [D loss: 0.655535, acc.: 62.50%] [G loss: 0.847517]\n",
      "epoch:6 step:6062 [D loss: 0.717493, acc.: 46.88%] [G loss: 0.768265]\n",
      "epoch:6 step:6063 [D loss: 0.674111, acc.: 60.94%] [G loss: 0.770225]\n",
      "epoch:6 step:6064 [D loss: 0.678279, acc.: 58.59%] [G loss: 0.794393]\n",
      "epoch:6 step:6065 [D loss: 0.728591, acc.: 44.53%] [G loss: 0.719936]\n",
      "epoch:6 step:6066 [D loss: 0.663251, acc.: 63.28%] [G loss: 0.764352]\n",
      "epoch:6 step:6067 [D loss: 0.684096, acc.: 53.12%] [G loss: 0.750457]\n",
      "epoch:6 step:6068 [D loss: 0.726402, acc.: 49.22%] [G loss: 0.724286]\n",
      "epoch:6 step:6069 [D loss: 0.668545, acc.: 54.69%] [G loss: 0.802679]\n",
      "epoch:6 step:6070 [D loss: 0.678288, acc.: 58.59%] [G loss: 0.806890]\n",
      "epoch:6 step:6071 [D loss: 0.653535, acc.: 64.84%] [G loss: 0.805944]\n",
      "epoch:6 step:6072 [D loss: 0.680949, acc.: 60.16%] [G loss: 0.786851]\n",
      "epoch:6 step:6073 [D loss: 0.645066, acc.: 58.59%] [G loss: 0.790018]\n",
      "epoch:6 step:6074 [D loss: 0.654684, acc.: 59.38%] [G loss: 0.781329]\n",
      "epoch:6 step:6075 [D loss: 0.674496, acc.: 57.81%] [G loss: 0.821301]\n",
      "epoch:6 step:6076 [D loss: 0.731394, acc.: 44.53%] [G loss: 0.853430]\n",
      "epoch:6 step:6077 [D loss: 0.708453, acc.: 48.44%] [G loss: 0.832310]\n",
      "epoch:6 step:6078 [D loss: 0.691242, acc.: 53.91%] [G loss: 0.784226]\n",
      "epoch:6 step:6079 [D loss: 0.700692, acc.: 46.09%] [G loss: 0.825247]\n",
      "epoch:6 step:6080 [D loss: 0.700841, acc.: 50.78%] [G loss: 0.779946]\n",
      "epoch:6 step:6081 [D loss: 0.704213, acc.: 47.66%] [G loss: 0.778411]\n",
      "epoch:6 step:6082 [D loss: 0.675426, acc.: 54.69%] [G loss: 0.815587]\n",
      "epoch:6 step:6083 [D loss: 0.706400, acc.: 49.22%] [G loss: 0.871516]\n",
      "epoch:6 step:6084 [D loss: 0.662808, acc.: 55.47%] [G loss: 0.806519]\n",
      "epoch:6 step:6085 [D loss: 0.697423, acc.: 48.44%] [G loss: 0.799040]\n",
      "epoch:6 step:6086 [D loss: 0.684004, acc.: 51.56%] [G loss: 0.826562]\n",
      "epoch:6 step:6087 [D loss: 0.677573, acc.: 55.47%] [G loss: 0.798348]\n",
      "epoch:6 step:6088 [D loss: 0.666874, acc.: 58.59%] [G loss: 0.810522]\n",
      "epoch:6 step:6089 [D loss: 0.667014, acc.: 60.16%] [G loss: 0.796795]\n",
      "epoch:6 step:6090 [D loss: 0.684973, acc.: 53.12%] [G loss: 0.780465]\n",
      "epoch:6 step:6091 [D loss: 0.697535, acc.: 52.34%] [G loss: 0.774198]\n",
      "epoch:6 step:6092 [D loss: 0.698161, acc.: 50.00%] [G loss: 0.788798]\n",
      "epoch:6 step:6093 [D loss: 0.667337, acc.: 53.12%] [G loss: 0.830765]\n",
      "epoch:6 step:6094 [D loss: 0.701148, acc.: 55.47%] [G loss: 0.779027]\n",
      "epoch:6 step:6095 [D loss: 0.705337, acc.: 51.56%] [G loss: 0.846903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6096 [D loss: 0.702594, acc.: 46.09%] [G loss: 0.806407]\n",
      "epoch:6 step:6097 [D loss: 0.672354, acc.: 60.94%] [G loss: 0.854633]\n",
      "epoch:6 step:6098 [D loss: 0.690172, acc.: 52.34%] [G loss: 0.855869]\n",
      "epoch:6 step:6099 [D loss: 0.690013, acc.: 51.56%] [G loss: 0.845084]\n",
      "epoch:6 step:6100 [D loss: 0.687697, acc.: 53.91%] [G loss: 0.819683]\n",
      "epoch:6 step:6101 [D loss: 0.688648, acc.: 53.12%] [G loss: 0.794825]\n",
      "epoch:6 step:6102 [D loss: 0.684387, acc.: 57.81%] [G loss: 0.762191]\n",
      "epoch:6 step:6103 [D loss: 0.678733, acc.: 60.94%] [G loss: 0.785402]\n",
      "epoch:6 step:6104 [D loss: 0.656347, acc.: 64.84%] [G loss: 0.797738]\n",
      "epoch:6 step:6105 [D loss: 0.681718, acc.: 60.16%] [G loss: 0.786914]\n",
      "epoch:6 step:6106 [D loss: 0.662764, acc.: 52.34%] [G loss: 0.865404]\n",
      "epoch:6 step:6107 [D loss: 0.683719, acc.: 57.03%] [G loss: 0.775426]\n",
      "epoch:6 step:6108 [D loss: 0.667197, acc.: 53.91%] [G loss: 0.810670]\n",
      "epoch:6 step:6109 [D loss: 0.682876, acc.: 53.12%] [G loss: 0.800887]\n",
      "epoch:6 step:6110 [D loss: 0.673505, acc.: 60.16%] [G loss: 0.790875]\n",
      "epoch:6 step:6111 [D loss: 0.676139, acc.: 54.69%] [G loss: 0.775047]\n",
      "epoch:6 step:6112 [D loss: 0.657692, acc.: 60.16%] [G loss: 0.814294]\n",
      "epoch:6 step:6113 [D loss: 0.706594, acc.: 49.22%] [G loss: 0.814970]\n",
      "epoch:6 step:6114 [D loss: 0.684298, acc.: 57.81%] [G loss: 0.782977]\n",
      "epoch:6 step:6115 [D loss: 0.661844, acc.: 58.59%] [G loss: 0.837634]\n",
      "epoch:6 step:6116 [D loss: 0.691757, acc.: 57.03%] [G loss: 0.795586]\n",
      "epoch:6 step:6117 [D loss: 0.691640, acc.: 51.56%] [G loss: 0.786426]\n",
      "epoch:6 step:6118 [D loss: 0.654905, acc.: 60.94%] [G loss: 0.748384]\n",
      "epoch:6 step:6119 [D loss: 0.689898, acc.: 53.12%] [G loss: 0.768019]\n",
      "epoch:6 step:6120 [D loss: 0.677491, acc.: 48.44%] [G loss: 0.740900]\n",
      "epoch:6 step:6121 [D loss: 0.671750, acc.: 53.91%] [G loss: 0.855115]\n",
      "epoch:6 step:6122 [D loss: 0.706146, acc.: 52.34%] [G loss: 0.780444]\n",
      "epoch:6 step:6123 [D loss: 0.663335, acc.: 59.38%] [G loss: 0.757303]\n",
      "epoch:6 step:6124 [D loss: 0.680493, acc.: 54.69%] [G loss: 0.786495]\n",
      "epoch:6 step:6125 [D loss: 0.707943, acc.: 43.75%] [G loss: 0.811460]\n",
      "epoch:6 step:6126 [D loss: 0.713882, acc.: 50.78%] [G loss: 0.797226]\n",
      "epoch:6 step:6127 [D loss: 0.672358, acc.: 50.78%] [G loss: 0.771716]\n",
      "epoch:6 step:6128 [D loss: 0.708838, acc.: 48.44%] [G loss: 0.782568]\n",
      "epoch:6 step:6129 [D loss: 0.691206, acc.: 48.44%] [G loss: 0.825426]\n",
      "epoch:6 step:6130 [D loss: 0.693110, acc.: 51.56%] [G loss: 0.834296]\n",
      "epoch:6 step:6131 [D loss: 0.694272, acc.: 52.34%] [G loss: 0.843389]\n",
      "epoch:6 step:6132 [D loss: 0.668531, acc.: 60.16%] [G loss: 0.823463]\n",
      "epoch:6 step:6133 [D loss: 0.669454, acc.: 56.25%] [G loss: 0.815674]\n",
      "epoch:6 step:6134 [D loss: 0.696434, acc.: 53.91%] [G loss: 0.768215]\n",
      "epoch:6 step:6135 [D loss: 0.695503, acc.: 55.47%] [G loss: 0.858593]\n",
      "epoch:6 step:6136 [D loss: 0.646703, acc.: 60.94%] [G loss: 0.860880]\n",
      "epoch:6 step:6137 [D loss: 0.670817, acc.: 59.38%] [G loss: 0.910525]\n",
      "epoch:6 step:6138 [D loss: 0.665609, acc.: 57.81%] [G loss: 0.887798]\n",
      "epoch:6 step:6139 [D loss: 0.691960, acc.: 49.22%] [G loss: 0.902554]\n",
      "epoch:6 step:6140 [D loss: 0.709441, acc.: 46.88%] [G loss: 0.782250]\n",
      "epoch:6 step:6141 [D loss: 0.701639, acc.: 49.22%] [G loss: 0.763077]\n",
      "epoch:6 step:6142 [D loss: 0.673866, acc.: 60.94%] [G loss: 0.759888]\n",
      "epoch:6 step:6143 [D loss: 0.660485, acc.: 57.03%] [G loss: 0.816253]\n",
      "epoch:6 step:6144 [D loss: 0.731556, acc.: 42.97%] [G loss: 0.782854]\n",
      "epoch:6 step:6145 [D loss: 0.634296, acc.: 61.72%] [G loss: 0.822089]\n",
      "epoch:6 step:6146 [D loss: 0.709392, acc.: 45.31%] [G loss: 0.754448]\n",
      "epoch:6 step:6147 [D loss: 0.700531, acc.: 48.44%] [G loss: 0.799572]\n",
      "epoch:6 step:6148 [D loss: 0.677937, acc.: 60.94%] [G loss: 0.834912]\n",
      "epoch:6 step:6149 [D loss: 0.717795, acc.: 44.53%] [G loss: 0.818296]\n",
      "epoch:6 step:6150 [D loss: 0.677458, acc.: 54.69%] [G loss: 0.802448]\n",
      "epoch:6 step:6151 [D loss: 0.668529, acc.: 55.47%] [G loss: 0.788438]\n",
      "epoch:6 step:6152 [D loss: 0.680581, acc.: 56.25%] [G loss: 0.762998]\n",
      "epoch:6 step:6153 [D loss: 0.696961, acc.: 50.00%] [G loss: 0.755581]\n",
      "epoch:6 step:6154 [D loss: 0.659667, acc.: 60.94%] [G loss: 0.759495]\n",
      "epoch:6 step:6155 [D loss: 0.694388, acc.: 51.56%] [G loss: 0.784604]\n",
      "epoch:6 step:6156 [D loss: 0.686124, acc.: 53.91%] [G loss: 0.741805]\n",
      "epoch:6 step:6157 [D loss: 0.654518, acc.: 63.28%] [G loss: 0.753497]\n",
      "epoch:6 step:6158 [D loss: 0.743571, acc.: 45.31%] [G loss: 0.788901]\n",
      "epoch:6 step:6159 [D loss: 0.665575, acc.: 54.69%] [G loss: 0.760696]\n",
      "epoch:6 step:6160 [D loss: 0.674330, acc.: 55.47%] [G loss: 0.794412]\n",
      "epoch:6 step:6161 [D loss: 0.734089, acc.: 45.31%] [G loss: 0.828904]\n",
      "epoch:6 step:6162 [D loss: 0.688439, acc.: 53.12%] [G loss: 0.728149]\n",
      "epoch:6 step:6163 [D loss: 0.684822, acc.: 58.59%] [G loss: 0.728239]\n",
      "epoch:6 step:6164 [D loss: 0.695665, acc.: 54.69%] [G loss: 0.726926]\n",
      "epoch:6 step:6165 [D loss: 0.678880, acc.: 56.25%] [G loss: 0.783128]\n",
      "epoch:6 step:6166 [D loss: 0.663809, acc.: 59.38%] [G loss: 0.857459]\n",
      "epoch:6 step:6167 [D loss: 0.653666, acc.: 64.06%] [G loss: 0.734589]\n",
      "epoch:6 step:6168 [D loss: 0.682334, acc.: 55.47%] [G loss: 0.792407]\n",
      "epoch:6 step:6169 [D loss: 0.708008, acc.: 45.31%] [G loss: 0.831092]\n",
      "epoch:6 step:6170 [D loss: 0.698666, acc.: 49.22%] [G loss: 0.799001]\n",
      "epoch:6 step:6171 [D loss: 0.692701, acc.: 52.34%] [G loss: 0.767678]\n",
      "epoch:6 step:6172 [D loss: 0.676057, acc.: 53.91%] [G loss: 0.796385]\n",
      "epoch:6 step:6173 [D loss: 0.673665, acc.: 61.72%] [G loss: 0.792913]\n",
      "epoch:6 step:6174 [D loss: 0.695148, acc.: 57.81%] [G loss: 0.760221]\n",
      "epoch:6 step:6175 [D loss: 0.692549, acc.: 51.56%] [G loss: 0.745800]\n",
      "epoch:6 step:6176 [D loss: 0.679144, acc.: 53.91%] [G loss: 0.804938]\n",
      "epoch:6 step:6177 [D loss: 0.668366, acc.: 55.47%] [G loss: 0.759839]\n",
      "epoch:6 step:6178 [D loss: 0.687339, acc.: 51.56%] [G loss: 0.825272]\n",
      "epoch:6 step:6179 [D loss: 0.674433, acc.: 53.91%] [G loss: 0.757245]\n",
      "epoch:6 step:6180 [D loss: 0.664887, acc.: 57.03%] [G loss: 0.704236]\n",
      "epoch:6 step:6181 [D loss: 0.654211, acc.: 65.62%] [G loss: 0.820506]\n",
      "epoch:6 step:6182 [D loss: 0.663656, acc.: 60.94%] [G loss: 0.806497]\n",
      "epoch:6 step:6183 [D loss: 0.691142, acc.: 58.59%] [G loss: 0.770724]\n",
      "epoch:6 step:6184 [D loss: 0.689080, acc.: 55.47%] [G loss: 0.720146]\n",
      "epoch:6 step:6185 [D loss: 0.708352, acc.: 49.22%] [G loss: 0.752772]\n",
      "epoch:6 step:6186 [D loss: 0.735262, acc.: 51.56%] [G loss: 0.768009]\n",
      "epoch:6 step:6187 [D loss: 0.690661, acc.: 53.12%] [G loss: 0.754919]\n",
      "epoch:6 step:6188 [D loss: 0.684246, acc.: 55.47%] [G loss: 0.742611]\n",
      "epoch:6 step:6189 [D loss: 0.711454, acc.: 50.00%] [G loss: 0.785742]\n",
      "epoch:6 step:6190 [D loss: 0.687546, acc.: 51.56%] [G loss: 0.723674]\n",
      "epoch:6 step:6191 [D loss: 0.677139, acc.: 56.25%] [G loss: 0.738894]\n",
      "epoch:6 step:6192 [D loss: 0.704258, acc.: 46.88%] [G loss: 0.786140]\n",
      "epoch:6 step:6193 [D loss: 0.694812, acc.: 55.47%] [G loss: 0.803360]\n",
      "epoch:6 step:6194 [D loss: 0.680245, acc.: 55.47%] [G loss: 0.791923]\n",
      "epoch:6 step:6195 [D loss: 0.671508, acc.: 58.59%] [G loss: 0.855729]\n",
      "epoch:6 step:6196 [D loss: 0.713846, acc.: 49.22%] [G loss: 0.818359]\n",
      "epoch:6 step:6197 [D loss: 0.676404, acc.: 57.03%] [G loss: 0.829906]\n",
      "epoch:6 step:6198 [D loss: 0.692400, acc.: 54.69%] [G loss: 0.775028]\n",
      "epoch:6 step:6199 [D loss: 0.668559, acc.: 65.62%] [G loss: 0.793689]\n",
      "epoch:6 step:6200 [D loss: 0.691536, acc.: 60.16%] [G loss: 0.776504]\n",
      "epoch:6 step:6201 [D loss: 0.678581, acc.: 56.25%] [G loss: 0.781652]\n",
      "epoch:6 step:6202 [D loss: 0.727554, acc.: 46.88%] [G loss: 0.771024]\n",
      "epoch:6 step:6203 [D loss: 0.677802, acc.: 58.59%] [G loss: 0.790231]\n",
      "epoch:6 step:6204 [D loss: 0.686734, acc.: 50.00%] [G loss: 0.800471]\n",
      "epoch:6 step:6205 [D loss: 0.707142, acc.: 47.66%] [G loss: 0.799666]\n",
      "epoch:6 step:6206 [D loss: 0.674032, acc.: 60.16%] [G loss: 0.788458]\n",
      "epoch:6 step:6207 [D loss: 0.681578, acc.: 57.03%] [G loss: 0.803648]\n",
      "epoch:6 step:6208 [D loss: 0.687231, acc.: 46.88%] [G loss: 0.838711]\n",
      "epoch:6 step:6209 [D loss: 0.682039, acc.: 58.59%] [G loss: 0.780445]\n",
      "epoch:6 step:6210 [D loss: 0.716011, acc.: 53.91%] [G loss: 0.767200]\n",
      "epoch:6 step:6211 [D loss: 0.691075, acc.: 50.78%] [G loss: 0.810249]\n",
      "epoch:6 step:6212 [D loss: 0.696259, acc.: 54.69%] [G loss: 0.836937]\n",
      "epoch:6 step:6213 [D loss: 0.695866, acc.: 43.75%] [G loss: 0.809403]\n",
      "epoch:6 step:6214 [D loss: 0.678795, acc.: 57.03%] [G loss: 0.767492]\n",
      "epoch:6 step:6215 [D loss: 0.664821, acc.: 57.03%] [G loss: 0.798918]\n",
      "epoch:6 step:6216 [D loss: 0.684941, acc.: 52.34%] [G loss: 0.786138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6217 [D loss: 0.687179, acc.: 55.47%] [G loss: 0.755726]\n",
      "epoch:6 step:6218 [D loss: 0.674619, acc.: 56.25%] [G loss: 0.806054]\n",
      "epoch:6 step:6219 [D loss: 0.680783, acc.: 53.12%] [G loss: 0.752009]\n",
      "epoch:6 step:6220 [D loss: 0.668223, acc.: 59.38%] [G loss: 0.772869]\n",
      "epoch:6 step:6221 [D loss: 0.656436, acc.: 61.72%] [G loss: 0.772956]\n",
      "epoch:6 step:6222 [D loss: 0.670018, acc.: 59.38%] [G loss: 0.776135]\n",
      "epoch:6 step:6223 [D loss: 0.699503, acc.: 53.12%] [G loss: 0.794626]\n",
      "epoch:6 step:6224 [D loss: 0.675346, acc.: 57.81%] [G loss: 0.832440]\n",
      "epoch:6 step:6225 [D loss: 0.696291, acc.: 46.88%] [G loss: 0.794745]\n",
      "epoch:6 step:6226 [D loss: 0.663669, acc.: 64.06%] [G loss: 0.735582]\n",
      "epoch:6 step:6227 [D loss: 0.668103, acc.: 60.16%] [G loss: 0.822906]\n",
      "epoch:6 step:6228 [D loss: 0.693753, acc.: 50.00%] [G loss: 0.799062]\n",
      "epoch:6 step:6229 [D loss: 0.703712, acc.: 51.56%] [G loss: 0.782146]\n",
      "epoch:6 step:6230 [D loss: 0.679186, acc.: 47.66%] [G loss: 0.807712]\n",
      "epoch:6 step:6231 [D loss: 0.716383, acc.: 46.09%] [G loss: 0.795134]\n",
      "epoch:6 step:6232 [D loss: 0.688707, acc.: 52.34%] [G loss: 0.827909]\n",
      "epoch:6 step:6233 [D loss: 0.705414, acc.: 48.44%] [G loss: 0.764614]\n",
      "epoch:6 step:6234 [D loss: 0.717224, acc.: 39.84%] [G loss: 0.781951]\n",
      "epoch:6 step:6235 [D loss: 0.696262, acc.: 49.22%] [G loss: 0.802072]\n",
      "epoch:6 step:6236 [D loss: 0.683695, acc.: 49.22%] [G loss: 0.819748]\n",
      "epoch:6 step:6237 [D loss: 0.670863, acc.: 54.69%] [G loss: 0.838592]\n",
      "epoch:6 step:6238 [D loss: 0.700517, acc.: 51.56%] [G loss: 0.859428]\n",
      "epoch:6 step:6239 [D loss: 0.663814, acc.: 60.94%] [G loss: 0.863798]\n",
      "epoch:6 step:6240 [D loss: 0.643592, acc.: 60.94%] [G loss: 0.798800]\n",
      "epoch:6 step:6241 [D loss: 0.648999, acc.: 58.59%] [G loss: 0.832330]\n",
      "epoch:6 step:6242 [D loss: 0.686000, acc.: 51.56%] [G loss: 0.845868]\n",
      "epoch:6 step:6243 [D loss: 0.696677, acc.: 49.22%] [G loss: 0.738003]\n",
      "epoch:6 step:6244 [D loss: 0.696491, acc.: 49.22%] [G loss: 0.854701]\n",
      "epoch:6 step:6245 [D loss: 0.686759, acc.: 50.00%] [G loss: 0.815549]\n",
      "epoch:6 step:6246 [D loss: 0.658998, acc.: 58.59%] [G loss: 0.806409]\n",
      "epoch:6 step:6247 [D loss: 0.700406, acc.: 46.88%] [G loss: 0.782601]\n",
      "epoch:6 step:6248 [D loss: 0.685115, acc.: 57.03%] [G loss: 0.780186]\n",
      "epoch:6 step:6249 [D loss: 0.654141, acc.: 60.16%] [G loss: 0.796217]\n",
      "epoch:6 step:6250 [D loss: 0.671046, acc.: 51.56%] [G loss: 0.788698]\n",
      "epoch:6 step:6251 [D loss: 0.688649, acc.: 48.44%] [G loss: 0.791478]\n",
      "epoch:6 step:6252 [D loss: 0.681822, acc.: 58.59%] [G loss: 0.795120]\n",
      "epoch:6 step:6253 [D loss: 0.684466, acc.: 57.81%] [G loss: 0.877220]\n",
      "epoch:6 step:6254 [D loss: 0.686222, acc.: 57.81%] [G loss: 0.822544]\n",
      "epoch:6 step:6255 [D loss: 0.740230, acc.: 47.66%] [G loss: 0.775727]\n",
      "epoch:6 step:6256 [D loss: 0.711703, acc.: 48.44%] [G loss: 0.783122]\n",
      "epoch:6 step:6257 [D loss: 0.689389, acc.: 51.56%] [G loss: 0.773550]\n",
      "epoch:6 step:6258 [D loss: 0.683865, acc.: 57.81%] [G loss: 0.784914]\n",
      "epoch:6 step:6259 [D loss: 0.671670, acc.: 54.69%] [G loss: 0.794946]\n",
      "epoch:6 step:6260 [D loss: 0.656745, acc.: 57.81%] [G loss: 0.916609]\n",
      "epoch:6 step:6261 [D loss: 0.692215, acc.: 53.91%] [G loss: 0.919549]\n",
      "epoch:6 step:6262 [D loss: 0.701496, acc.: 54.69%] [G loss: 0.816974]\n",
      "epoch:6 step:6263 [D loss: 0.689600, acc.: 56.25%] [G loss: 0.799584]\n",
      "epoch:6 step:6264 [D loss: 0.698879, acc.: 51.56%] [G loss: 0.758195]\n",
      "epoch:6 step:6265 [D loss: 0.673009, acc.: 61.72%] [G loss: 0.755515]\n",
      "epoch:6 step:6266 [D loss: 0.669903, acc.: 61.72%] [G loss: 0.741985]\n",
      "epoch:6 step:6267 [D loss: 0.682339, acc.: 53.12%] [G loss: 0.767910]\n",
      "epoch:6 step:6268 [D loss: 0.931302, acc.: 38.28%] [G loss: 0.848629]\n",
      "epoch:6 step:6269 [D loss: 0.650286, acc.: 60.16%] [G loss: 0.933111]\n",
      "epoch:6 step:6270 [D loss: 0.676047, acc.: 57.03%] [G loss: 0.829386]\n",
      "epoch:6 step:6271 [D loss: 0.682237, acc.: 53.91%] [G loss: 0.817693]\n",
      "epoch:6 step:6272 [D loss: 0.687478, acc.: 51.56%] [G loss: 0.816936]\n",
      "epoch:6 step:6273 [D loss: 0.666569, acc.: 60.94%] [G loss: 0.858081]\n",
      "epoch:6 step:6274 [D loss: 0.672549, acc.: 63.28%] [G loss: 0.790458]\n",
      "epoch:6 step:6275 [D loss: 0.700278, acc.: 50.78%] [G loss: 0.770542]\n",
      "epoch:6 step:6276 [D loss: 0.711312, acc.: 52.34%] [G loss: 0.808806]\n",
      "epoch:6 step:6277 [D loss: 0.681907, acc.: 55.47%] [G loss: 0.803403]\n",
      "epoch:6 step:6278 [D loss: 0.663911, acc.: 62.50%] [G loss: 0.784870]\n",
      "epoch:6 step:6279 [D loss: 0.664675, acc.: 64.06%] [G loss: 0.751719]\n",
      "epoch:6 step:6280 [D loss: 0.684545, acc.: 50.00%] [G loss: 0.735083]\n",
      "epoch:6 step:6281 [D loss: 0.658976, acc.: 58.59%] [G loss: 0.759750]\n",
      "epoch:6 step:6282 [D loss: 0.646898, acc.: 66.41%] [G loss: 0.738057]\n",
      "epoch:6 step:6283 [D loss: 0.693688, acc.: 50.78%] [G loss: 0.811976]\n",
      "epoch:6 step:6284 [D loss: 0.673598, acc.: 64.06%] [G loss: 0.757563]\n",
      "epoch:6 step:6285 [D loss: 0.683592, acc.: 57.81%] [G loss: 0.767402]\n",
      "epoch:6 step:6286 [D loss: 0.732218, acc.: 40.62%] [G loss: 0.788312]\n",
      "epoch:6 step:6287 [D loss: 0.693830, acc.: 57.81%] [G loss: 0.810396]\n",
      "epoch:6 step:6288 [D loss: 0.686338, acc.: 50.00%] [G loss: 0.834712]\n",
      "epoch:6 step:6289 [D loss: 0.654216, acc.: 64.06%] [G loss: 0.809714]\n",
      "epoch:6 step:6290 [D loss: 0.681886, acc.: 53.12%] [G loss: 0.836914]\n",
      "epoch:6 step:6291 [D loss: 0.691614, acc.: 46.09%] [G loss: 0.783383]\n",
      "epoch:6 step:6292 [D loss: 0.685212, acc.: 55.47%] [G loss: 0.783959]\n",
      "epoch:6 step:6293 [D loss: 0.676771, acc.: 48.44%] [G loss: 0.903883]\n",
      "epoch:6 step:6294 [D loss: 0.681962, acc.: 51.56%] [G loss: 0.815739]\n",
      "epoch:6 step:6295 [D loss: 0.686890, acc.: 54.69%] [G loss: 0.773003]\n",
      "epoch:6 step:6296 [D loss: 0.666032, acc.: 57.03%] [G loss: 0.796351]\n",
      "epoch:6 step:6297 [D loss: 0.685174, acc.: 51.56%] [G loss: 0.783869]\n",
      "epoch:6 step:6298 [D loss: 0.695931, acc.: 57.81%] [G loss: 0.808742]\n",
      "epoch:6 step:6299 [D loss: 0.696081, acc.: 63.28%] [G loss: 0.803927]\n",
      "epoch:6 step:6300 [D loss: 0.681796, acc.: 55.47%] [G loss: 0.812993]\n",
      "epoch:6 step:6301 [D loss: 0.696935, acc.: 52.34%] [G loss: 0.819151]\n",
      "epoch:6 step:6302 [D loss: 0.696906, acc.: 50.00%] [G loss: 0.834291]\n",
      "epoch:6 step:6303 [D loss: 0.698014, acc.: 50.78%] [G loss: 0.748948]\n",
      "epoch:6 step:6304 [D loss: 0.694427, acc.: 46.09%] [G loss: 0.788956]\n",
      "epoch:6 step:6305 [D loss: 0.689376, acc.: 50.78%] [G loss: 0.811499]\n",
      "epoch:6 step:6306 [D loss: 0.667881, acc.: 57.03%] [G loss: 0.812681]\n",
      "epoch:6 step:6307 [D loss: 0.703517, acc.: 44.53%] [G loss: 0.768436]\n",
      "epoch:6 step:6308 [D loss: 0.664092, acc.: 54.69%] [G loss: 0.802288]\n",
      "epoch:6 step:6309 [D loss: 0.694199, acc.: 53.12%] [G loss: 0.845188]\n",
      "epoch:6 step:6310 [D loss: 0.700264, acc.: 56.25%] [G loss: 0.966596]\n",
      "epoch:6 step:6311 [D loss: 0.708447, acc.: 50.00%] [G loss: 0.797917]\n",
      "epoch:6 step:6312 [D loss: 0.667713, acc.: 60.16%] [G loss: 0.850713]\n",
      "epoch:6 step:6313 [D loss: 0.690627, acc.: 50.78%] [G loss: 0.857974]\n",
      "epoch:6 step:6314 [D loss: 0.713214, acc.: 45.31%] [G loss: 0.800016]\n",
      "epoch:6 step:6315 [D loss: 0.692567, acc.: 53.12%] [G loss: 0.805233]\n",
      "epoch:6 step:6316 [D loss: 0.705091, acc.: 43.75%] [G loss: 0.826193]\n",
      "epoch:6 step:6317 [D loss: 0.666161, acc.: 60.94%] [G loss: 0.813895]\n",
      "epoch:6 step:6318 [D loss: 0.682327, acc.: 54.69%] [G loss: 0.765944]\n",
      "epoch:6 step:6319 [D loss: 0.661850, acc.: 66.41%] [G loss: 0.768949]\n",
      "epoch:6 step:6320 [D loss: 0.704531, acc.: 49.22%] [G loss: 0.755216]\n",
      "epoch:6 step:6321 [D loss: 0.683754, acc.: 54.69%] [G loss: 0.784313]\n",
      "epoch:6 step:6322 [D loss: 0.664083, acc.: 64.06%] [G loss: 0.733316]\n",
      "epoch:6 step:6323 [D loss: 0.678868, acc.: 58.59%] [G loss: 0.767698]\n",
      "epoch:6 step:6324 [D loss: 0.692303, acc.: 52.34%] [G loss: 0.715292]\n",
      "epoch:6 step:6325 [D loss: 0.712953, acc.: 46.88%] [G loss: 0.768905]\n",
      "epoch:6 step:6326 [D loss: 0.687458, acc.: 52.34%] [G loss: 0.768749]\n",
      "epoch:6 step:6327 [D loss: 0.708014, acc.: 46.09%] [G loss: 0.739165]\n",
      "epoch:6 step:6328 [D loss: 0.684553, acc.: 51.56%] [G loss: 0.783966]\n",
      "epoch:6 step:6329 [D loss: 0.668562, acc.: 57.03%] [G loss: 0.785491]\n",
      "epoch:6 step:6330 [D loss: 0.682978, acc.: 59.38%] [G loss: 0.761600]\n",
      "epoch:6 step:6331 [D loss: 0.724010, acc.: 50.00%] [G loss: 0.814480]\n",
      "epoch:6 step:6332 [D loss: 0.644606, acc.: 57.81%] [G loss: 0.849480]\n",
      "epoch:6 step:6333 [D loss: 0.667609, acc.: 60.16%] [G loss: 0.855954]\n",
      "epoch:6 step:6334 [D loss: 0.645869, acc.: 58.59%] [G loss: 0.875951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6335 [D loss: 0.686905, acc.: 54.69%] [G loss: 0.789704]\n",
      "epoch:6 step:6336 [D loss: 0.677455, acc.: 56.25%] [G loss: 0.781813]\n",
      "epoch:6 step:6337 [D loss: 0.700011, acc.: 56.25%] [G loss: 0.747224]\n",
      "epoch:6 step:6338 [D loss: 0.705167, acc.: 50.78%] [G loss: 0.767824]\n",
      "epoch:6 step:6339 [D loss: 0.671727, acc.: 61.72%] [G loss: 0.752313]\n",
      "epoch:6 step:6340 [D loss: 0.661454, acc.: 57.03%] [G loss: 0.802686]\n",
      "epoch:6 step:6341 [D loss: 0.691388, acc.: 49.22%] [G loss: 0.797800]\n",
      "epoch:6 step:6342 [D loss: 0.670301, acc.: 61.72%] [G loss: 0.785100]\n",
      "epoch:6 step:6343 [D loss: 0.667695, acc.: 62.50%] [G loss: 0.760320]\n",
      "epoch:6 step:6344 [D loss: 0.703524, acc.: 51.56%] [G loss: 0.772375]\n",
      "epoch:6 step:6345 [D loss: 0.681771, acc.: 56.25%] [G loss: 0.811513]\n",
      "epoch:6 step:6346 [D loss: 0.666514, acc.: 52.34%] [G loss: 0.815121]\n",
      "epoch:6 step:6347 [D loss: 0.669190, acc.: 56.25%] [G loss: 0.796086]\n",
      "epoch:6 step:6348 [D loss: 0.714640, acc.: 50.78%] [G loss: 0.812227]\n",
      "epoch:6 step:6349 [D loss: 0.651685, acc.: 66.41%] [G loss: 0.817712]\n",
      "epoch:6 step:6350 [D loss: 0.675509, acc.: 59.38%] [G loss: 0.776876]\n",
      "epoch:6 step:6351 [D loss: 0.669634, acc.: 58.59%] [G loss: 0.813022]\n",
      "epoch:6 step:6352 [D loss: 0.707380, acc.: 50.78%] [G loss: 0.778374]\n",
      "epoch:6 step:6353 [D loss: 0.703494, acc.: 51.56%] [G loss: 0.831236]\n",
      "epoch:6 step:6354 [D loss: 0.664590, acc.: 57.03%] [G loss: 0.825678]\n",
      "epoch:6 step:6355 [D loss: 0.665851, acc.: 61.72%] [G loss: 0.819790]\n",
      "epoch:6 step:6356 [D loss: 0.660002, acc.: 61.72%] [G loss: 0.826480]\n",
      "epoch:6 step:6357 [D loss: 0.666767, acc.: 60.94%] [G loss: 0.792747]\n",
      "epoch:6 step:6358 [D loss: 0.675680, acc.: 58.59%] [G loss: 0.826958]\n",
      "epoch:6 step:6359 [D loss: 0.667463, acc.: 57.03%] [G loss: 0.864529]\n",
      "epoch:6 step:6360 [D loss: 0.696730, acc.: 52.34%] [G loss: 0.822975]\n",
      "epoch:6 step:6361 [D loss: 0.702727, acc.: 50.78%] [G loss: 0.758032]\n",
      "epoch:6 step:6362 [D loss: 0.729595, acc.: 42.19%] [G loss: 0.749063]\n",
      "epoch:6 step:6363 [D loss: 0.709002, acc.: 48.44%] [G loss: 0.778498]\n",
      "epoch:6 step:6364 [D loss: 0.709424, acc.: 54.69%] [G loss: 0.797171]\n",
      "epoch:6 step:6365 [D loss: 0.729209, acc.: 51.56%] [G loss: 0.764028]\n",
      "epoch:6 step:6366 [D loss: 0.674523, acc.: 53.12%] [G loss: 0.815120]\n",
      "epoch:6 step:6367 [D loss: 0.678880, acc.: 55.47%] [G loss: 0.834830]\n",
      "epoch:6 step:6368 [D loss: 0.702280, acc.: 50.78%] [G loss: 0.729719]\n",
      "epoch:6 step:6369 [D loss: 0.675199, acc.: 52.34%] [G loss: 0.760581]\n",
      "epoch:6 step:6370 [D loss: 0.687720, acc.: 54.69%] [G loss: 0.761506]\n",
      "epoch:6 step:6371 [D loss: 0.699675, acc.: 60.16%] [G loss: 0.774035]\n",
      "epoch:6 step:6372 [D loss: 0.676278, acc.: 57.03%] [G loss: 0.775912]\n",
      "epoch:6 step:6373 [D loss: 0.668229, acc.: 57.81%] [G loss: 0.790362]\n",
      "epoch:6 step:6374 [D loss: 0.692459, acc.: 53.91%] [G loss: 0.782144]\n",
      "epoch:6 step:6375 [D loss: 0.708956, acc.: 57.03%] [G loss: 0.784590]\n",
      "epoch:6 step:6376 [D loss: 0.666627, acc.: 66.41%] [G loss: 0.836451]\n",
      "epoch:6 step:6377 [D loss: 0.697678, acc.: 51.56%] [G loss: 0.791744]\n",
      "epoch:6 step:6378 [D loss: 0.660221, acc.: 60.16%] [G loss: 0.876466]\n",
      "epoch:6 step:6379 [D loss: 0.711796, acc.: 45.31%] [G loss: 0.789166]\n",
      "epoch:6 step:6380 [D loss: 0.668141, acc.: 57.81%] [G loss: 0.763719]\n",
      "epoch:6 step:6381 [D loss: 0.664669, acc.: 53.91%] [G loss: 0.780005]\n",
      "epoch:6 step:6382 [D loss: 0.663586, acc.: 64.06%] [G loss: 0.792981]\n",
      "epoch:6 step:6383 [D loss: 0.705992, acc.: 52.34%] [G loss: 0.817814]\n",
      "epoch:6 step:6384 [D loss: 0.696095, acc.: 57.81%] [G loss: 0.824903]\n",
      "epoch:6 step:6385 [D loss: 0.700223, acc.: 45.31%] [G loss: 0.796055]\n",
      "epoch:6 step:6386 [D loss: 0.684748, acc.: 59.38%] [G loss: 0.788229]\n",
      "epoch:6 step:6387 [D loss: 0.668508, acc.: 64.84%] [G loss: 0.736751]\n",
      "epoch:6 step:6388 [D loss: 0.695506, acc.: 49.22%] [G loss: 0.776525]\n",
      "epoch:6 step:6389 [D loss: 0.694604, acc.: 50.78%] [G loss: 0.769527]\n",
      "epoch:6 step:6390 [D loss: 0.679016, acc.: 47.66%] [G loss: 0.762795]\n",
      "epoch:6 step:6391 [D loss: 0.664220, acc.: 59.38%] [G loss: 0.770968]\n",
      "epoch:6 step:6392 [D loss: 0.668999, acc.: 54.69%] [G loss: 0.852160]\n",
      "epoch:6 step:6393 [D loss: 0.641529, acc.: 59.38%] [G loss: 0.793730]\n",
      "epoch:6 step:6394 [D loss: 0.701973, acc.: 53.12%] [G loss: 0.809905]\n",
      "epoch:6 step:6395 [D loss: 0.671828, acc.: 57.81%] [G loss: 0.795751]\n",
      "epoch:6 step:6396 [D loss: 0.666524, acc.: 53.91%] [G loss: 0.805976]\n",
      "epoch:6 step:6397 [D loss: 0.694841, acc.: 55.47%] [G loss: 0.766921]\n",
      "epoch:6 step:6398 [D loss: 0.687931, acc.: 53.12%] [G loss: 0.782708]\n",
      "epoch:6 step:6399 [D loss: 0.682280, acc.: 53.12%] [G loss: 0.835848]\n",
      "epoch:6 step:6400 [D loss: 0.684455, acc.: 51.56%] [G loss: 0.796154]\n",
      "epoch:6 step:6401 [D loss: 0.719259, acc.: 46.09%] [G loss: 0.778769]\n",
      "epoch:6 step:6402 [D loss: 0.651778, acc.: 61.72%] [G loss: 0.861504]\n",
      "epoch:6 step:6403 [D loss: 0.715755, acc.: 45.31%] [G loss: 0.818611]\n",
      "epoch:6 step:6404 [D loss: 0.670550, acc.: 61.72%] [G loss: 0.854443]\n",
      "epoch:6 step:6405 [D loss: 0.715975, acc.: 48.44%] [G loss: 0.779210]\n",
      "epoch:6 step:6406 [D loss: 0.678911, acc.: 60.94%] [G loss: 0.780400]\n",
      "epoch:6 step:6407 [D loss: 0.719658, acc.: 52.34%] [G loss: 0.769916]\n",
      "epoch:6 step:6408 [D loss: 0.679636, acc.: 54.69%] [G loss: 0.766604]\n",
      "epoch:6 step:6409 [D loss: 0.683559, acc.: 57.03%] [G loss: 0.740450]\n",
      "epoch:6 step:6410 [D loss: 0.681990, acc.: 59.38%] [G loss: 0.754217]\n",
      "epoch:6 step:6411 [D loss: 0.682185, acc.: 58.59%] [G loss: 0.723589]\n",
      "epoch:6 step:6412 [D loss: 0.707001, acc.: 47.66%] [G loss: 0.778575]\n",
      "epoch:6 step:6413 [D loss: 0.656810, acc.: 55.47%] [G loss: 0.774677]\n",
      "epoch:6 step:6414 [D loss: 0.678798, acc.: 56.25%] [G loss: 0.782813]\n",
      "epoch:6 step:6415 [D loss: 0.711212, acc.: 52.34%] [G loss: 0.787686]\n",
      "epoch:6 step:6416 [D loss: 0.672753, acc.: 60.94%] [G loss: 0.747959]\n",
      "epoch:6 step:6417 [D loss: 0.682167, acc.: 57.03%] [G loss: 0.805897]\n",
      "epoch:6 step:6418 [D loss: 0.681980, acc.: 50.78%] [G loss: 0.818370]\n",
      "epoch:6 step:6419 [D loss: 0.722747, acc.: 54.69%] [G loss: 0.753759]\n",
      "epoch:6 step:6420 [D loss: 0.675417, acc.: 55.47%] [G loss: 0.766707]\n",
      "epoch:6 step:6421 [D loss: 0.682054, acc.: 56.25%] [G loss: 0.775066]\n",
      "epoch:6 step:6422 [D loss: 0.690638, acc.: 52.34%] [G loss: 0.752952]\n",
      "epoch:6 step:6423 [D loss: 0.695288, acc.: 52.34%] [G loss: 0.767563]\n",
      "epoch:6 step:6424 [D loss: 0.709021, acc.: 49.22%] [G loss: 0.812875]\n",
      "epoch:6 step:6425 [D loss: 0.658051, acc.: 64.06%] [G loss: 0.792518]\n",
      "epoch:6 step:6426 [D loss: 0.678493, acc.: 53.91%] [G loss: 0.823737]\n",
      "epoch:6 step:6427 [D loss: 0.685171, acc.: 58.59%] [G loss: 0.765868]\n",
      "epoch:6 step:6428 [D loss: 0.715522, acc.: 47.66%] [G loss: 0.783581]\n",
      "epoch:6 step:6429 [D loss: 0.674887, acc.: 60.16%] [G loss: 0.742586]\n",
      "epoch:6 step:6430 [D loss: 0.691159, acc.: 47.66%] [G loss: 0.774430]\n",
      "epoch:6 step:6431 [D loss: 0.653436, acc.: 60.16%] [G loss: 0.854818]\n",
      "epoch:6 step:6432 [D loss: 0.701331, acc.: 56.25%] [G loss: 0.788048]\n",
      "epoch:6 step:6433 [D loss: 0.666909, acc.: 54.69%] [G loss: 0.799192]\n",
      "epoch:6 step:6434 [D loss: 0.712077, acc.: 50.78%] [G loss: 0.797961]\n",
      "epoch:6 step:6435 [D loss: 0.718093, acc.: 43.75%] [G loss: 0.756380]\n",
      "epoch:6 step:6436 [D loss: 0.668897, acc.: 57.81%] [G loss: 0.774168]\n",
      "epoch:6 step:6437 [D loss: 0.686657, acc.: 50.78%] [G loss: 0.799709]\n",
      "epoch:6 step:6438 [D loss: 0.664289, acc.: 60.16%] [G loss: 0.766076]\n",
      "epoch:6 step:6439 [D loss: 0.687525, acc.: 57.03%] [G loss: 0.791469]\n",
      "epoch:6 step:6440 [D loss: 0.662310, acc.: 59.38%] [G loss: 0.784881]\n",
      "epoch:6 step:6441 [D loss: 0.681923, acc.: 52.34%] [G loss: 0.828854]\n",
      "epoch:6 step:6442 [D loss: 0.696264, acc.: 54.69%] [G loss: 0.765273]\n",
      "epoch:6 step:6443 [D loss: 0.661072, acc.: 60.16%] [G loss: 0.736480]\n",
      "epoch:6 step:6444 [D loss: 0.659757, acc.: 59.38%] [G loss: 0.801903]\n",
      "epoch:6 step:6445 [D loss: 0.654008, acc.: 67.19%] [G loss: 0.763128]\n",
      "epoch:6 step:6446 [D loss: 0.703553, acc.: 45.31%] [G loss: 0.775116]\n",
      "epoch:6 step:6447 [D loss: 0.680565, acc.: 52.34%] [G loss: 0.799688]\n",
      "epoch:6 step:6448 [D loss: 0.687409, acc.: 55.47%] [G loss: 0.802812]\n",
      "epoch:6 step:6449 [D loss: 0.710804, acc.: 48.44%] [G loss: 0.726461]\n",
      "epoch:6 step:6450 [D loss: 0.703575, acc.: 48.44%] [G loss: 0.751287]\n",
      "epoch:6 step:6451 [D loss: 0.677171, acc.: 57.81%] [G loss: 0.800846]\n",
      "epoch:6 step:6452 [D loss: 0.700201, acc.: 47.66%] [G loss: 0.752798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6453 [D loss: 0.687551, acc.: 50.78%] [G loss: 0.805210]\n",
      "epoch:6 step:6454 [D loss: 0.706453, acc.: 49.22%] [G loss: 0.807077]\n",
      "epoch:6 step:6455 [D loss: 0.686490, acc.: 60.16%] [G loss: 0.792035]\n",
      "epoch:6 step:6456 [D loss: 0.681940, acc.: 54.69%] [G loss: 0.804022]\n",
      "epoch:6 step:6457 [D loss: 0.698994, acc.: 50.78%] [G loss: 0.830504]\n",
      "epoch:6 step:6458 [D loss: 0.702360, acc.: 53.12%] [G loss: 0.756937]\n",
      "epoch:6 step:6459 [D loss: 0.697416, acc.: 48.44%] [G loss: 0.742112]\n",
      "epoch:6 step:6460 [D loss: 0.707849, acc.: 48.44%] [G loss: 0.761444]\n",
      "epoch:6 step:6461 [D loss: 0.683316, acc.: 64.06%] [G loss: 0.745553]\n",
      "epoch:6 step:6462 [D loss: 0.684381, acc.: 51.56%] [G loss: 0.773069]\n",
      "epoch:6 step:6463 [D loss: 0.703257, acc.: 48.44%] [G loss: 0.752778]\n",
      "epoch:6 step:6464 [D loss: 0.685178, acc.: 53.12%] [G loss: 0.786534]\n",
      "epoch:6 step:6465 [D loss: 0.692220, acc.: 54.69%] [G loss: 0.803078]\n",
      "epoch:6 step:6466 [D loss: 0.707915, acc.: 49.22%] [G loss: 0.770958]\n",
      "epoch:6 step:6467 [D loss: 0.697966, acc.: 47.66%] [G loss: 0.752504]\n",
      "epoch:6 step:6468 [D loss: 0.651781, acc.: 61.72%] [G loss: 0.839644]\n",
      "epoch:6 step:6469 [D loss: 0.657005, acc.: 65.62%] [G loss: 0.796583]\n",
      "epoch:6 step:6470 [D loss: 0.652482, acc.: 57.03%] [G loss: 0.876691]\n",
      "epoch:6 step:6471 [D loss: 0.690849, acc.: 51.56%] [G loss: 0.797305]\n",
      "epoch:6 step:6472 [D loss: 0.672945, acc.: 56.25%] [G loss: 0.731629]\n",
      "epoch:6 step:6473 [D loss: 0.677977, acc.: 60.16%] [G loss: 0.781819]\n",
      "epoch:6 step:6474 [D loss: 0.676916, acc.: 57.03%] [G loss: 0.749427]\n",
      "epoch:6 step:6475 [D loss: 0.698522, acc.: 49.22%] [G loss: 0.796251]\n",
      "epoch:6 step:6476 [D loss: 0.692877, acc.: 55.47%] [G loss: 0.758218]\n",
      "epoch:6 step:6477 [D loss: 0.683142, acc.: 55.47%] [G loss: 0.751228]\n",
      "epoch:6 step:6478 [D loss: 0.653654, acc.: 61.72%] [G loss: 0.747701]\n",
      "epoch:6 step:6479 [D loss: 0.665917, acc.: 56.25%] [G loss: 0.786846]\n",
      "epoch:6 step:6480 [D loss: 0.729986, acc.: 41.41%] [G loss: 0.762138]\n",
      "epoch:6 step:6481 [D loss: 0.677020, acc.: 60.16%] [G loss: 0.787943]\n",
      "epoch:6 step:6482 [D loss: 0.700717, acc.: 47.66%] [G loss: 0.808399]\n",
      "epoch:6 step:6483 [D loss: 0.678465, acc.: 50.78%] [G loss: 0.853794]\n",
      "epoch:6 step:6484 [D loss: 0.689127, acc.: 53.12%] [G loss: 0.842141]\n",
      "epoch:6 step:6485 [D loss: 0.680549, acc.: 57.03%] [G loss: 0.784305]\n",
      "epoch:6 step:6486 [D loss: 0.680459, acc.: 53.91%] [G loss: 0.762132]\n",
      "epoch:6 step:6487 [D loss: 0.666118, acc.: 59.38%] [G loss: 0.797170]\n",
      "epoch:6 step:6488 [D loss: 0.660259, acc.: 57.03%] [G loss: 0.769911]\n",
      "epoch:6 step:6489 [D loss: 0.697286, acc.: 53.12%] [G loss: 0.802938]\n",
      "epoch:6 step:6490 [D loss: 0.669173, acc.: 59.38%] [G loss: 0.735544]\n",
      "epoch:6 step:6491 [D loss: 0.720655, acc.: 46.09%] [G loss: 0.772925]\n",
      "epoch:6 step:6492 [D loss: 0.690835, acc.: 56.25%] [G loss: 0.755797]\n",
      "epoch:6 step:6493 [D loss: 0.694515, acc.: 47.66%] [G loss: 0.738976]\n",
      "epoch:6 step:6494 [D loss: 0.690612, acc.: 50.78%] [G loss: 0.732289]\n",
      "epoch:6 step:6495 [D loss: 0.688314, acc.: 57.03%] [G loss: 0.766489]\n",
      "epoch:6 step:6496 [D loss: 0.686094, acc.: 59.38%] [G loss: 0.729476]\n",
      "epoch:6 step:6497 [D loss: 0.692188, acc.: 50.00%] [G loss: 0.781283]\n",
      "epoch:6 step:6498 [D loss: 0.699853, acc.: 53.12%] [G loss: 0.738179]\n",
      "epoch:6 step:6499 [D loss: 0.685592, acc.: 57.03%] [G loss: 0.753632]\n",
      "epoch:6 step:6500 [D loss: 0.693202, acc.: 50.00%] [G loss: 0.757107]\n",
      "epoch:6 step:6501 [D loss: 0.653131, acc.: 62.50%] [G loss: 0.769240]\n",
      "epoch:6 step:6502 [D loss: 0.677307, acc.: 57.03%] [G loss: 0.794273]\n",
      "epoch:6 step:6503 [D loss: 0.684154, acc.: 53.91%] [G loss: 0.746100]\n",
      "epoch:6 step:6504 [D loss: 0.669580, acc.: 59.38%] [G loss: 0.782719]\n",
      "epoch:6 step:6505 [D loss: 0.697001, acc.: 46.88%] [G loss: 0.797387]\n",
      "epoch:6 step:6506 [D loss: 0.659863, acc.: 58.59%] [G loss: 0.822157]\n",
      "epoch:6 step:6507 [D loss: 0.696623, acc.: 53.91%] [G loss: 0.857670]\n",
      "epoch:6 step:6508 [D loss: 0.687444, acc.: 50.78%] [G loss: 0.906799]\n",
      "epoch:6 step:6509 [D loss: 0.657104, acc.: 58.59%] [G loss: 0.835427]\n",
      "epoch:6 step:6510 [D loss: 0.674594, acc.: 59.38%] [G loss: 0.777830]\n",
      "epoch:6 step:6511 [D loss: 0.677568, acc.: 54.69%] [G loss: 0.779169]\n",
      "epoch:6 step:6512 [D loss: 0.685045, acc.: 55.47%] [G loss: 0.787891]\n",
      "epoch:6 step:6513 [D loss: 0.679405, acc.: 53.12%] [G loss: 0.801807]\n",
      "epoch:6 step:6514 [D loss: 0.702951, acc.: 54.69%] [G loss: 0.853640]\n",
      "epoch:6 step:6515 [D loss: 0.722721, acc.: 50.00%] [G loss: 0.759457]\n",
      "epoch:6 step:6516 [D loss: 0.656722, acc.: 65.62%] [G loss: 0.775602]\n",
      "epoch:6 step:6517 [D loss: 0.656959, acc.: 63.28%] [G loss: 0.768343]\n",
      "epoch:6 step:6518 [D loss: 0.670943, acc.: 60.94%] [G loss: 0.809639]\n",
      "epoch:6 step:6519 [D loss: 0.675181, acc.: 55.47%] [G loss: 0.750561]\n",
      "epoch:6 step:6520 [D loss: 0.650760, acc.: 58.59%] [G loss: 0.796110]\n",
      "epoch:6 step:6521 [D loss: 0.653547, acc.: 57.81%] [G loss: 0.818730]\n",
      "epoch:6 step:6522 [D loss: 0.704370, acc.: 48.44%] [G loss: 0.741215]\n",
      "epoch:6 step:6523 [D loss: 0.699961, acc.: 45.31%] [G loss: 0.776608]\n",
      "epoch:6 step:6524 [D loss: 0.670984, acc.: 56.25%] [G loss: 0.798634]\n",
      "epoch:6 step:6525 [D loss: 0.683971, acc.: 57.81%] [G loss: 0.791188]\n",
      "epoch:6 step:6526 [D loss: 0.701533, acc.: 49.22%] [G loss: 0.778682]\n",
      "epoch:6 step:6527 [D loss: 0.721167, acc.: 44.53%] [G loss: 0.805506]\n",
      "epoch:6 step:6528 [D loss: 0.678927, acc.: 53.91%] [G loss: 0.751149]\n",
      "epoch:6 step:6529 [D loss: 0.698959, acc.: 45.31%] [G loss: 0.753144]\n",
      "epoch:6 step:6530 [D loss: 0.672646, acc.: 53.12%] [G loss: 0.768836]\n",
      "epoch:6 step:6531 [D loss: 0.688228, acc.: 58.59%] [G loss: 0.728043]\n",
      "epoch:6 step:6532 [D loss: 0.660873, acc.: 57.03%] [G loss: 0.774826]\n",
      "epoch:6 step:6533 [D loss: 0.674304, acc.: 47.66%] [G loss: 0.769720]\n",
      "epoch:6 step:6534 [D loss: 0.685583, acc.: 53.91%] [G loss: 0.773640]\n",
      "epoch:6 step:6535 [D loss: 0.636999, acc.: 67.97%] [G loss: 0.799848]\n",
      "epoch:6 step:6536 [D loss: 0.680940, acc.: 47.66%] [G loss: 0.821191]\n",
      "epoch:6 step:6537 [D loss: 0.672228, acc.: 53.12%] [G loss: 0.838417]\n",
      "epoch:6 step:6538 [D loss: 0.664346, acc.: 56.25%] [G loss: 0.806391]\n",
      "epoch:6 step:6539 [D loss: 0.670996, acc.: 55.47%] [G loss: 0.877351]\n",
      "epoch:6 step:6540 [D loss: 0.646299, acc.: 65.62%] [G loss: 0.840589]\n",
      "epoch:6 step:6541 [D loss: 0.689055, acc.: 50.78%] [G loss: 0.831262]\n",
      "epoch:6 step:6542 [D loss: 0.701544, acc.: 55.47%] [G loss: 0.824218]\n",
      "epoch:6 step:6543 [D loss: 0.700159, acc.: 53.12%] [G loss: 0.738080]\n",
      "epoch:6 step:6544 [D loss: 0.686286, acc.: 53.91%] [G loss: 0.773109]\n",
      "epoch:6 step:6545 [D loss: 0.683762, acc.: 61.72%] [G loss: 0.756504]\n",
      "epoch:6 step:6546 [D loss: 0.716909, acc.: 47.66%] [G loss: 0.804820]\n",
      "epoch:6 step:6547 [D loss: 0.710700, acc.: 44.53%] [G loss: 0.849881]\n",
      "epoch:6 step:6548 [D loss: 0.679574, acc.: 52.34%] [G loss: 0.732237]\n",
      "epoch:6 step:6549 [D loss: 0.701246, acc.: 55.47%] [G loss: 0.753059]\n",
      "epoch:6 step:6550 [D loss: 0.656809, acc.: 59.38%] [G loss: 0.762684]\n",
      "epoch:6 step:6551 [D loss: 0.713877, acc.: 43.75%] [G loss: 0.804128]\n",
      "epoch:6 step:6552 [D loss: 0.678956, acc.: 53.12%] [G loss: 0.809304]\n",
      "epoch:6 step:6553 [D loss: 0.681675, acc.: 55.47%] [G loss: 0.842094]\n",
      "epoch:6 step:6554 [D loss: 0.675095, acc.: 57.81%] [G loss: 0.810146]\n",
      "epoch:6 step:6555 [D loss: 0.660651, acc.: 58.59%] [G loss: 0.760611]\n",
      "epoch:6 step:6556 [D loss: 0.661288, acc.: 61.72%] [G loss: 0.839753]\n",
      "epoch:6 step:6557 [D loss: 0.658843, acc.: 64.06%] [G loss: 0.833676]\n",
      "epoch:6 step:6558 [D loss: 0.666371, acc.: 59.38%] [G loss: 0.810927]\n",
      "epoch:6 step:6559 [D loss: 0.714256, acc.: 56.25%] [G loss: 0.786027]\n",
      "epoch:7 step:6560 [D loss: 0.687400, acc.: 59.38%] [G loss: 0.758764]\n",
      "epoch:7 step:6561 [D loss: 0.716164, acc.: 42.19%] [G loss: 0.783603]\n",
      "epoch:7 step:6562 [D loss: 0.689631, acc.: 55.47%] [G loss: 0.797535]\n",
      "epoch:7 step:6563 [D loss: 0.635480, acc.: 66.41%] [G loss: 0.867271]\n",
      "epoch:7 step:6564 [D loss: 0.656374, acc.: 55.47%] [G loss: 0.860656]\n",
      "epoch:7 step:6565 [D loss: 0.649274, acc.: 61.72%] [G loss: 0.825325]\n",
      "epoch:7 step:6566 [D loss: 0.710811, acc.: 49.22%] [G loss: 0.841106]\n",
      "epoch:7 step:6567 [D loss: 0.703058, acc.: 50.00%] [G loss: 0.857111]\n",
      "epoch:7 step:6568 [D loss: 0.721933, acc.: 42.97%] [G loss: 0.874794]\n",
      "epoch:7 step:6569 [D loss: 0.676904, acc.: 51.56%] [G loss: 0.781781]\n",
      "epoch:7 step:6570 [D loss: 0.691431, acc.: 53.91%] [G loss: 0.811350]\n",
      "epoch:7 step:6571 [D loss: 0.683158, acc.: 53.12%] [G loss: 0.774837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6572 [D loss: 0.699426, acc.: 53.12%] [G loss: 0.799324]\n",
      "epoch:7 step:6573 [D loss: 0.706870, acc.: 48.44%] [G loss: 0.772537]\n",
      "epoch:7 step:6574 [D loss: 0.688171, acc.: 54.69%] [G loss: 0.784379]\n",
      "epoch:7 step:6575 [D loss: 0.718902, acc.: 42.19%] [G loss: 0.775275]\n",
      "epoch:7 step:6576 [D loss: 0.665084, acc.: 58.59%] [G loss: 0.773269]\n",
      "epoch:7 step:6577 [D loss: 0.694198, acc.: 50.78%] [G loss: 0.796033]\n",
      "epoch:7 step:6578 [D loss: 0.692629, acc.: 55.47%] [G loss: 0.784327]\n",
      "epoch:7 step:6579 [D loss: 0.681163, acc.: 54.69%] [G loss: 0.777164]\n",
      "epoch:7 step:6580 [D loss: 0.677043, acc.: 52.34%] [G loss: 0.760576]\n",
      "epoch:7 step:6581 [D loss: 0.711990, acc.: 47.66%] [G loss: 0.798958]\n",
      "epoch:7 step:6582 [D loss: 0.691386, acc.: 52.34%] [G loss: 0.805164]\n",
      "epoch:7 step:6583 [D loss: 0.653216, acc.: 61.72%] [G loss: 0.757720]\n",
      "epoch:7 step:6584 [D loss: 0.691203, acc.: 48.44%] [G loss: 0.902523]\n",
      "epoch:7 step:6585 [D loss: 0.683461, acc.: 57.81%] [G loss: 0.780792]\n",
      "epoch:7 step:6586 [D loss: 0.707657, acc.: 49.22%] [G loss: 0.777465]\n",
      "epoch:7 step:6587 [D loss: 0.701041, acc.: 55.47%] [G loss: 0.752458]\n",
      "epoch:7 step:6588 [D loss: 0.716220, acc.: 50.00%] [G loss: 0.691835]\n",
      "epoch:7 step:6589 [D loss: 0.693652, acc.: 52.34%] [G loss: 0.805079]\n",
      "epoch:7 step:6590 [D loss: 0.693567, acc.: 48.44%] [G loss: 0.797406]\n",
      "epoch:7 step:6591 [D loss: 0.710546, acc.: 46.88%] [G loss: 0.765283]\n",
      "epoch:7 step:6592 [D loss: 0.678391, acc.: 53.91%] [G loss: 0.781288]\n",
      "epoch:7 step:6593 [D loss: 0.656705, acc.: 63.28%] [G loss: 0.840820]\n",
      "epoch:7 step:6594 [D loss: 0.661784, acc.: 65.62%] [G loss: 0.822986]\n",
      "epoch:7 step:6595 [D loss: 0.684079, acc.: 47.66%] [G loss: 0.809663]\n",
      "epoch:7 step:6596 [D loss: 0.677196, acc.: 57.03%] [G loss: 0.800272]\n",
      "epoch:7 step:6597 [D loss: 0.717608, acc.: 49.22%] [G loss: 0.804610]\n",
      "epoch:7 step:6598 [D loss: 0.670864, acc.: 59.38%] [G loss: 0.852862]\n",
      "epoch:7 step:6599 [D loss: 0.687547, acc.: 43.75%] [G loss: 0.751536]\n",
      "epoch:7 step:6600 [D loss: 0.668097, acc.: 53.12%] [G loss: 0.827071]\n",
      "epoch:7 step:6601 [D loss: 0.692877, acc.: 46.09%] [G loss: 0.806262]\n",
      "epoch:7 step:6602 [D loss: 0.674962, acc.: 58.59%] [G loss: 0.768834]\n",
      "epoch:7 step:6603 [D loss: 0.697969, acc.: 50.78%] [G loss: 0.787106]\n",
      "epoch:7 step:6604 [D loss: 0.695609, acc.: 52.34%] [G loss: 0.769511]\n",
      "epoch:7 step:6605 [D loss: 0.677083, acc.: 54.69%] [G loss: 0.712495]\n",
      "epoch:7 step:6606 [D loss: 0.712026, acc.: 49.22%] [G loss: 0.734113]\n",
      "epoch:7 step:6607 [D loss: 0.686040, acc.: 49.22%] [G loss: 0.812873]\n",
      "epoch:7 step:6608 [D loss: 0.692167, acc.: 47.66%] [G loss: 0.811003]\n",
      "epoch:7 step:6609 [D loss: 0.676867, acc.: 58.59%] [G loss: 0.781934]\n",
      "epoch:7 step:6610 [D loss: 0.660453, acc.: 57.81%] [G loss: 0.748435]\n",
      "epoch:7 step:6611 [D loss: 0.698474, acc.: 52.34%] [G loss: 0.742584]\n",
      "epoch:7 step:6612 [D loss: 0.706320, acc.: 48.44%] [G loss: 0.773100]\n",
      "epoch:7 step:6613 [D loss: 0.698194, acc.: 53.12%] [G loss: 0.768582]\n",
      "epoch:7 step:6614 [D loss: 0.701634, acc.: 43.75%] [G loss: 0.796594]\n",
      "epoch:7 step:6615 [D loss: 0.681554, acc.: 53.91%] [G loss: 0.837003]\n",
      "epoch:7 step:6616 [D loss: 0.678267, acc.: 53.91%] [G loss: 0.857711]\n",
      "epoch:7 step:6617 [D loss: 0.679729, acc.: 53.91%] [G loss: 0.748766]\n",
      "epoch:7 step:6618 [D loss: 0.684986, acc.: 50.00%] [G loss: 0.810251]\n",
      "epoch:7 step:6619 [D loss: 0.684080, acc.: 50.78%] [G loss: 0.732077]\n",
      "epoch:7 step:6620 [D loss: 0.719967, acc.: 49.22%] [G loss: 0.805199]\n",
      "epoch:7 step:6621 [D loss: 0.729807, acc.: 46.88%] [G loss: 0.845569]\n",
      "epoch:7 step:6622 [D loss: 0.672580, acc.: 55.47%] [G loss: 0.797712]\n",
      "epoch:7 step:6623 [D loss: 0.674659, acc.: 52.34%] [G loss: 0.960452]\n",
      "epoch:7 step:6624 [D loss: 0.682634, acc.: 53.91%] [G loss: 0.862300]\n",
      "epoch:7 step:6625 [D loss: 0.684923, acc.: 55.47%] [G loss: 0.801017]\n",
      "epoch:7 step:6626 [D loss: 0.657498, acc.: 62.50%] [G loss: 0.795683]\n",
      "epoch:7 step:6627 [D loss: 0.698088, acc.: 51.56%] [G loss: 0.843808]\n",
      "epoch:7 step:6628 [D loss: 0.641625, acc.: 63.28%] [G loss: 0.849323]\n",
      "epoch:7 step:6629 [D loss: 0.729420, acc.: 52.34%] [G loss: 0.804252]\n",
      "epoch:7 step:6630 [D loss: 0.698075, acc.: 46.88%] [G loss: 0.784656]\n",
      "epoch:7 step:6631 [D loss: 0.700025, acc.: 44.53%] [G loss: 0.790160]\n",
      "epoch:7 step:6632 [D loss: 0.726709, acc.: 45.31%] [G loss: 0.787255]\n",
      "epoch:7 step:6633 [D loss: 0.707415, acc.: 50.78%] [G loss: 0.734656]\n",
      "epoch:7 step:6634 [D loss: 0.688792, acc.: 53.12%] [G loss: 0.768122]\n",
      "epoch:7 step:6635 [D loss: 0.683293, acc.: 56.25%] [G loss: 0.777489]\n",
      "epoch:7 step:6636 [D loss: 0.684082, acc.: 58.59%] [G loss: 0.752122]\n",
      "epoch:7 step:6637 [D loss: 0.685225, acc.: 53.91%] [G loss: 0.767476]\n",
      "epoch:7 step:6638 [D loss: 0.699633, acc.: 57.81%] [G loss: 0.759894]\n",
      "epoch:7 step:6639 [D loss: 0.701851, acc.: 48.44%] [G loss: 0.792854]\n",
      "epoch:7 step:6640 [D loss: 0.694615, acc.: 51.56%] [G loss: 0.718826]\n",
      "epoch:7 step:6641 [D loss: 0.690338, acc.: 57.03%] [G loss: 0.747136]\n",
      "epoch:7 step:6642 [D loss: 0.688551, acc.: 54.69%] [G loss: 0.761015]\n",
      "epoch:7 step:6643 [D loss: 0.710337, acc.: 49.22%] [G loss: 0.780313]\n",
      "epoch:7 step:6644 [D loss: 0.663861, acc.: 60.16%] [G loss: 0.792543]\n",
      "epoch:7 step:6645 [D loss: 0.685691, acc.: 52.34%] [G loss: 0.768240]\n",
      "epoch:7 step:6646 [D loss: 0.669639, acc.: 60.94%] [G loss: 0.748076]\n",
      "epoch:7 step:6647 [D loss: 0.715576, acc.: 42.19%] [G loss: 0.766961]\n",
      "epoch:7 step:6648 [D loss: 0.695514, acc.: 47.66%] [G loss: 0.739250]\n",
      "epoch:7 step:6649 [D loss: 0.670384, acc.: 54.69%] [G loss: 0.774040]\n",
      "epoch:7 step:6650 [D loss: 0.697333, acc.: 57.03%] [G loss: 0.809669]\n",
      "epoch:7 step:6651 [D loss: 0.675239, acc.: 57.81%] [G loss: 0.806532]\n",
      "epoch:7 step:6652 [D loss: 0.648987, acc.: 64.06%] [G loss: 0.791974]\n",
      "epoch:7 step:6653 [D loss: 0.667006, acc.: 59.38%] [G loss: 0.835112]\n",
      "epoch:7 step:6654 [D loss: 0.681898, acc.: 52.34%] [G loss: 0.866180]\n",
      "epoch:7 step:6655 [D loss: 0.696248, acc.: 54.69%] [G loss: 0.813107]\n",
      "epoch:7 step:6656 [D loss: 0.669607, acc.: 60.16%] [G loss: 0.751535]\n",
      "epoch:7 step:6657 [D loss: 0.694126, acc.: 57.81%] [G loss: 0.763210]\n",
      "epoch:7 step:6658 [D loss: 0.678302, acc.: 60.16%] [G loss: 0.749929]\n",
      "epoch:7 step:6659 [D loss: 0.699125, acc.: 60.94%] [G loss: 0.720154]\n",
      "epoch:7 step:6660 [D loss: 0.709298, acc.: 53.91%] [G loss: 0.727875]\n",
      "epoch:7 step:6661 [D loss: 0.682734, acc.: 57.03%] [G loss: 0.801358]\n",
      "epoch:7 step:6662 [D loss: 0.661484, acc.: 58.59%] [G loss: 0.804044]\n",
      "epoch:7 step:6663 [D loss: 0.691893, acc.: 54.69%] [G loss: 0.823930]\n",
      "epoch:7 step:6664 [D loss: 0.661529, acc.: 57.03%] [G loss: 0.786702]\n",
      "epoch:7 step:6665 [D loss: 0.663110, acc.: 57.81%] [G loss: 0.761079]\n",
      "epoch:7 step:6666 [D loss: 0.700763, acc.: 50.00%] [G loss: 0.793846]\n",
      "epoch:7 step:6667 [D loss: 0.701333, acc.: 51.56%] [G loss: 0.701845]\n",
      "epoch:7 step:6668 [D loss: 0.711751, acc.: 50.78%] [G loss: 0.754916]\n",
      "epoch:7 step:6669 [D loss: 0.679036, acc.: 54.69%] [G loss: 0.800756]\n",
      "epoch:7 step:6670 [D loss: 0.719597, acc.: 43.75%] [G loss: 0.742866]\n",
      "epoch:7 step:6671 [D loss: 0.666604, acc.: 55.47%] [G loss: 0.786588]\n",
      "epoch:7 step:6672 [D loss: 0.702930, acc.: 48.44%] [G loss: 0.770827]\n",
      "epoch:7 step:6673 [D loss: 0.672522, acc.: 64.06%] [G loss: 0.786687]\n",
      "epoch:7 step:6674 [D loss: 0.704566, acc.: 56.25%] [G loss: 0.771703]\n",
      "epoch:7 step:6675 [D loss: 0.713706, acc.: 48.44%] [G loss: 0.743359]\n",
      "epoch:7 step:6676 [D loss: 0.700978, acc.: 57.03%] [G loss: 0.809178]\n",
      "epoch:7 step:6677 [D loss: 0.678377, acc.: 59.38%] [G loss: 0.782959]\n",
      "epoch:7 step:6678 [D loss: 0.650279, acc.: 64.84%] [G loss: 0.737014]\n",
      "epoch:7 step:6679 [D loss: 0.666275, acc.: 62.50%] [G loss: 0.803296]\n",
      "epoch:7 step:6680 [D loss: 0.675268, acc.: 57.03%] [G loss: 0.791749]\n",
      "epoch:7 step:6681 [D loss: 0.691462, acc.: 49.22%] [G loss: 0.806107]\n",
      "epoch:7 step:6682 [D loss: 0.681530, acc.: 56.25%] [G loss: 0.767546]\n",
      "epoch:7 step:6683 [D loss: 0.711648, acc.: 49.22%] [G loss: 0.759283]\n",
      "epoch:7 step:6684 [D loss: 0.662844, acc.: 54.69%] [G loss: 0.781094]\n",
      "epoch:7 step:6685 [D loss: 0.727652, acc.: 38.28%] [G loss: 0.800607]\n",
      "epoch:7 step:6686 [D loss: 0.708619, acc.: 46.88%] [G loss: 0.787025]\n",
      "epoch:7 step:6687 [D loss: 0.726645, acc.: 39.84%] [G loss: 0.749920]\n",
      "epoch:7 step:6688 [D loss: 0.685492, acc.: 54.69%] [G loss: 0.748681]\n",
      "epoch:7 step:6689 [D loss: 0.689782, acc.: 50.00%] [G loss: 0.799355]\n",
      "epoch:7 step:6690 [D loss: 0.662791, acc.: 62.50%] [G loss: 0.788106]\n",
      "epoch:7 step:6691 [D loss: 0.708489, acc.: 48.44%] [G loss: 0.791756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6692 [D loss: 0.698866, acc.: 48.44%] [G loss: 0.758370]\n",
      "epoch:7 step:6693 [D loss: 0.692480, acc.: 55.47%] [G loss: 0.804203]\n",
      "epoch:7 step:6694 [D loss: 0.679195, acc.: 51.56%] [G loss: 0.790534]\n",
      "epoch:7 step:6695 [D loss: 0.706280, acc.: 47.66%] [G loss: 0.734338]\n",
      "epoch:7 step:6696 [D loss: 0.690469, acc.: 49.22%] [G loss: 0.748659]\n",
      "epoch:7 step:6697 [D loss: 0.671179, acc.: 57.81%] [G loss: 0.765803]\n",
      "epoch:7 step:6698 [D loss: 0.704011, acc.: 51.56%] [G loss: 0.758711]\n",
      "epoch:7 step:6699 [D loss: 0.677914, acc.: 53.91%] [G loss: 0.805906]\n",
      "epoch:7 step:6700 [D loss: 0.672925, acc.: 53.12%] [G loss: 0.755512]\n",
      "epoch:7 step:6701 [D loss: 0.674044, acc.: 53.91%] [G loss: 0.759895]\n",
      "epoch:7 step:6702 [D loss: 0.700961, acc.: 50.00%] [G loss: 0.786819]\n",
      "epoch:7 step:6703 [D loss: 0.691265, acc.: 53.12%] [G loss: 0.803594]\n",
      "epoch:7 step:6704 [D loss: 0.708949, acc.: 52.34%] [G loss: 0.768270]\n",
      "epoch:7 step:6705 [D loss: 0.688624, acc.: 50.78%] [G loss: 0.733908]\n",
      "epoch:7 step:6706 [D loss: 0.693707, acc.: 53.91%] [G loss: 0.694089]\n",
      "epoch:7 step:6707 [D loss: 0.680242, acc.: 56.25%] [G loss: 0.795715]\n",
      "epoch:7 step:6708 [D loss: 0.695794, acc.: 51.56%] [G loss: 0.726337]\n",
      "epoch:7 step:6709 [D loss: 0.661982, acc.: 59.38%] [G loss: 0.769732]\n",
      "epoch:7 step:6710 [D loss: 0.710722, acc.: 48.44%] [G loss: 0.789855]\n",
      "epoch:7 step:6711 [D loss: 0.707198, acc.: 44.53%] [G loss: 0.758450]\n",
      "epoch:7 step:6712 [D loss: 0.685567, acc.: 57.81%] [G loss: 0.808150]\n",
      "epoch:7 step:6713 [D loss: 0.693563, acc.: 52.34%] [G loss: 0.761960]\n",
      "epoch:7 step:6714 [D loss: 0.673062, acc.: 57.81%] [G loss: 0.826152]\n",
      "epoch:7 step:6715 [D loss: 0.646337, acc.: 67.19%] [G loss: 0.806112]\n",
      "epoch:7 step:6716 [D loss: 0.658581, acc.: 58.59%] [G loss: 0.843529]\n",
      "epoch:7 step:6717 [D loss: 0.667861, acc.: 52.34%] [G loss: 0.822421]\n",
      "epoch:7 step:6718 [D loss: 0.703466, acc.: 50.78%] [G loss: 0.757548]\n",
      "epoch:7 step:6719 [D loss: 0.699221, acc.: 54.69%] [G loss: 0.812538]\n",
      "epoch:7 step:6720 [D loss: 0.704434, acc.: 52.34%] [G loss: 0.815607]\n",
      "epoch:7 step:6721 [D loss: 0.688022, acc.: 48.44%] [G loss: 0.736162]\n",
      "epoch:7 step:6722 [D loss: 0.698018, acc.: 50.78%] [G loss: 0.762611]\n",
      "epoch:7 step:6723 [D loss: 0.685307, acc.: 51.56%] [G loss: 0.742764]\n",
      "epoch:7 step:6724 [D loss: 0.680535, acc.: 51.56%] [G loss: 0.751319]\n",
      "epoch:7 step:6725 [D loss: 0.675806, acc.: 55.47%] [G loss: 0.713402]\n",
      "epoch:7 step:6726 [D loss: 0.680734, acc.: 57.03%] [G loss: 0.790794]\n",
      "epoch:7 step:6727 [D loss: 0.703274, acc.: 42.97%] [G loss: 0.733673]\n",
      "epoch:7 step:6728 [D loss: 0.668946, acc.: 57.81%] [G loss: 0.763596]\n",
      "epoch:7 step:6729 [D loss: 0.682901, acc.: 57.03%] [G loss: 0.797641]\n",
      "epoch:7 step:6730 [D loss: 0.715844, acc.: 50.78%] [G loss: 0.745237]\n",
      "epoch:7 step:6731 [D loss: 0.685580, acc.: 53.91%] [G loss: 0.819427]\n",
      "epoch:7 step:6732 [D loss: 0.686212, acc.: 47.66%] [G loss: 0.742120]\n",
      "epoch:7 step:6733 [D loss: 0.695532, acc.: 55.47%] [G loss: 0.792211]\n",
      "epoch:7 step:6734 [D loss: 0.682459, acc.: 59.38%] [G loss: 0.736230]\n",
      "epoch:7 step:6735 [D loss: 0.680125, acc.: 56.25%] [G loss: 0.798602]\n",
      "epoch:7 step:6736 [D loss: 0.658284, acc.: 63.28%] [G loss: 0.761426]\n",
      "epoch:7 step:6737 [D loss: 0.701502, acc.: 55.47%] [G loss: 0.772149]\n",
      "epoch:7 step:6738 [D loss: 0.665836, acc.: 54.69%] [G loss: 0.815042]\n",
      "epoch:7 step:6739 [D loss: 0.674106, acc.: 55.47%] [G loss: 0.754487]\n",
      "epoch:7 step:6740 [D loss: 0.695025, acc.: 49.22%] [G loss: 0.795388]\n",
      "epoch:7 step:6741 [D loss: 0.690843, acc.: 50.00%] [G loss: 0.834070]\n",
      "epoch:7 step:6742 [D loss: 0.690990, acc.: 57.03%] [G loss: 0.779981]\n",
      "epoch:7 step:6743 [D loss: 0.694815, acc.: 50.00%] [G loss: 0.797527]\n",
      "epoch:7 step:6744 [D loss: 0.711116, acc.: 46.09%] [G loss: 0.802762]\n",
      "epoch:7 step:6745 [D loss: 0.694265, acc.: 53.12%] [G loss: 0.747623]\n",
      "epoch:7 step:6746 [D loss: 0.705519, acc.: 48.44%] [G loss: 0.806750]\n",
      "epoch:7 step:6747 [D loss: 0.720955, acc.: 49.22%] [G loss: 0.754675]\n",
      "epoch:7 step:6748 [D loss: 0.710314, acc.: 50.00%] [G loss: 0.753762]\n",
      "epoch:7 step:6749 [D loss: 0.682046, acc.: 52.34%] [G loss: 0.772144]\n",
      "epoch:7 step:6750 [D loss: 0.689488, acc.: 54.69%] [G loss: 0.785461]\n",
      "epoch:7 step:6751 [D loss: 0.692474, acc.: 50.78%] [G loss: 0.724773]\n",
      "epoch:7 step:6752 [D loss: 0.703843, acc.: 47.66%] [G loss: 0.740537]\n",
      "epoch:7 step:6753 [D loss: 0.676562, acc.: 55.47%] [G loss: 0.803728]\n",
      "epoch:7 step:6754 [D loss: 0.664911, acc.: 57.03%] [G loss: 0.741572]\n",
      "epoch:7 step:6755 [D loss: 0.711163, acc.: 46.88%] [G loss: 0.752321]\n",
      "epoch:7 step:6756 [D loss: 0.732649, acc.: 41.41%] [G loss: 0.772179]\n",
      "epoch:7 step:6757 [D loss: 0.693019, acc.: 55.47%] [G loss: 0.759666]\n",
      "epoch:7 step:6758 [D loss: 0.667970, acc.: 60.16%] [G loss: 0.739290]\n",
      "epoch:7 step:6759 [D loss: 0.668741, acc.: 64.84%] [G loss: 0.761904]\n",
      "epoch:7 step:6760 [D loss: 0.688247, acc.: 53.12%] [G loss: 0.780254]\n",
      "epoch:7 step:6761 [D loss: 0.679435, acc.: 60.94%] [G loss: 0.781106]\n",
      "epoch:7 step:6762 [D loss: 0.683697, acc.: 55.47%] [G loss: 0.760695]\n",
      "epoch:7 step:6763 [D loss: 0.685624, acc.: 53.12%] [G loss: 0.788199]\n",
      "epoch:7 step:6764 [D loss: 0.684824, acc.: 56.25%] [G loss: 0.827664]\n",
      "epoch:7 step:6765 [D loss: 0.686871, acc.: 51.56%] [G loss: 0.765126]\n",
      "epoch:7 step:6766 [D loss: 0.683169, acc.: 53.91%] [G loss: 0.760740]\n",
      "epoch:7 step:6767 [D loss: 0.679628, acc.: 57.03%] [G loss: 0.771841]\n",
      "epoch:7 step:6768 [D loss: 0.696188, acc.: 53.91%] [G loss: 0.798213]\n",
      "epoch:7 step:6769 [D loss: 0.709795, acc.: 52.34%] [G loss: 0.795776]\n",
      "epoch:7 step:6770 [D loss: 0.659905, acc.: 60.94%] [G loss: 0.801668]\n",
      "epoch:7 step:6771 [D loss: 0.676478, acc.: 56.25%] [G loss: 0.815898]\n",
      "epoch:7 step:6772 [D loss: 0.685686, acc.: 50.78%] [G loss: 0.818457]\n",
      "epoch:7 step:6773 [D loss: 0.715765, acc.: 43.75%] [G loss: 0.827513]\n",
      "epoch:7 step:6774 [D loss: 0.701453, acc.: 49.22%] [G loss: 0.833394]\n",
      "epoch:7 step:6775 [D loss: 0.688111, acc.: 53.12%] [G loss: 0.752902]\n",
      "epoch:7 step:6776 [D loss: 0.705710, acc.: 50.00%] [G loss: 0.730143]\n",
      "epoch:7 step:6777 [D loss: 0.718114, acc.: 42.97%] [G loss: 0.776840]\n",
      "epoch:7 step:6778 [D loss: 0.693489, acc.: 49.22%] [G loss: 0.737601]\n",
      "epoch:7 step:6779 [D loss: 0.697480, acc.: 50.00%] [G loss: 0.740033]\n",
      "epoch:7 step:6780 [D loss: 0.683503, acc.: 57.03%] [G loss: 0.735881]\n",
      "epoch:7 step:6781 [D loss: 0.693890, acc.: 47.66%] [G loss: 0.788424]\n",
      "epoch:7 step:6782 [D loss: 0.706495, acc.: 47.66%] [G loss: 0.753776]\n",
      "epoch:7 step:6783 [D loss: 0.689986, acc.: 57.81%] [G loss: 0.740637]\n",
      "epoch:7 step:6784 [D loss: 0.693374, acc.: 52.34%] [G loss: 0.758836]\n",
      "epoch:7 step:6785 [D loss: 0.687420, acc.: 54.69%] [G loss: 0.777434]\n",
      "epoch:7 step:6786 [D loss: 0.661839, acc.: 60.94%] [G loss: 0.775420]\n",
      "epoch:7 step:6787 [D loss: 0.671658, acc.: 57.81%] [G loss: 0.789196]\n",
      "epoch:7 step:6788 [D loss: 0.693548, acc.: 54.69%] [G loss: 0.771986]\n",
      "epoch:7 step:6789 [D loss: 0.676336, acc.: 54.69%] [G loss: 0.764224]\n",
      "epoch:7 step:6790 [D loss: 0.690165, acc.: 53.12%] [G loss: 0.751575]\n",
      "epoch:7 step:6791 [D loss: 0.701512, acc.: 46.88%] [G loss: 0.765038]\n",
      "epoch:7 step:6792 [D loss: 0.687314, acc.: 51.56%] [G loss: 0.765947]\n",
      "epoch:7 step:6793 [D loss: 0.698394, acc.: 57.03%] [G loss: 0.753886]\n",
      "epoch:7 step:6794 [D loss: 0.710589, acc.: 50.00%] [G loss: 0.747357]\n",
      "epoch:7 step:6795 [D loss: 0.692918, acc.: 57.03%] [G loss: 0.690507]\n",
      "epoch:7 step:6796 [D loss: 0.695549, acc.: 50.78%] [G loss: 0.709394]\n",
      "epoch:7 step:6797 [D loss: 0.695853, acc.: 53.12%] [G loss: 0.754563]\n",
      "epoch:7 step:6798 [D loss: 0.692589, acc.: 51.56%] [G loss: 0.707498]\n",
      "epoch:7 step:6799 [D loss: 0.673886, acc.: 57.81%] [G loss: 0.733007]\n",
      "epoch:7 step:6800 [D loss: 0.686830, acc.: 53.91%] [G loss: 0.757000]\n",
      "epoch:7 step:6801 [D loss: 0.679537, acc.: 53.91%] [G loss: 0.723083]\n",
      "epoch:7 step:6802 [D loss: 0.661577, acc.: 59.38%] [G loss: 0.751784]\n",
      "epoch:7 step:6803 [D loss: 0.686889, acc.: 54.69%] [G loss: 0.759512]\n",
      "epoch:7 step:6804 [D loss: 0.679687, acc.: 50.00%] [G loss: 0.714134]\n",
      "epoch:7 step:6805 [D loss: 0.702917, acc.: 48.44%] [G loss: 0.753529]\n",
      "epoch:7 step:6806 [D loss: 0.664261, acc.: 58.59%] [G loss: 0.749288]\n",
      "epoch:7 step:6807 [D loss: 0.691921, acc.: 50.78%] [G loss: 0.787970]\n",
      "epoch:7 step:6808 [D loss: 0.695680, acc.: 55.47%] [G loss: 0.777350]\n",
      "epoch:7 step:6809 [D loss: 0.688295, acc.: 52.34%] [G loss: 0.747938]\n",
      "epoch:7 step:6810 [D loss: 0.667189, acc.: 62.50%] [G loss: 0.772001]\n",
      "epoch:7 step:6811 [D loss: 0.679225, acc.: 57.81%] [G loss: 0.764934]\n",
      "epoch:7 step:6812 [D loss: 0.677772, acc.: 50.78%] [G loss: 0.798357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6813 [D loss: 0.703031, acc.: 45.31%] [G loss: 0.749393]\n",
      "epoch:7 step:6814 [D loss: 0.673028, acc.: 58.59%] [G loss: 0.759042]\n",
      "epoch:7 step:6815 [D loss: 0.714020, acc.: 48.44%] [G loss: 0.759667]\n",
      "epoch:7 step:6816 [D loss: 0.684743, acc.: 57.03%] [G loss: 0.741627]\n",
      "epoch:7 step:6817 [D loss: 0.681161, acc.: 57.03%] [G loss: 0.793409]\n",
      "epoch:7 step:6818 [D loss: 0.677920, acc.: 57.03%] [G loss: 0.789721]\n",
      "epoch:7 step:6819 [D loss: 0.683901, acc.: 49.22%] [G loss: 0.784593]\n",
      "epoch:7 step:6820 [D loss: 0.679585, acc.: 45.31%] [G loss: 0.706638]\n",
      "epoch:7 step:6821 [D loss: 0.716276, acc.: 45.31%] [G loss: 0.724571]\n",
      "epoch:7 step:6822 [D loss: 0.687935, acc.: 48.44%] [G loss: 0.775177]\n",
      "epoch:7 step:6823 [D loss: 0.669049, acc.: 57.03%] [G loss: 0.734650]\n",
      "epoch:7 step:6824 [D loss: 0.709745, acc.: 51.56%] [G loss: 0.723463]\n",
      "epoch:7 step:6825 [D loss: 0.655510, acc.: 57.81%] [G loss: 0.763979]\n",
      "epoch:7 step:6826 [D loss: 0.663472, acc.: 55.47%] [G loss: 0.852679]\n",
      "epoch:7 step:6827 [D loss: 0.714547, acc.: 49.22%] [G loss: 0.772694]\n",
      "epoch:7 step:6828 [D loss: 0.690232, acc.: 53.91%] [G loss: 0.761275]\n",
      "epoch:7 step:6829 [D loss: 0.678269, acc.: 53.91%] [G loss: 0.786289]\n",
      "epoch:7 step:6830 [D loss: 0.661343, acc.: 59.38%] [G loss: 0.795957]\n",
      "epoch:7 step:6831 [D loss: 0.675009, acc.: 54.69%] [G loss: 0.780918]\n",
      "epoch:7 step:6832 [D loss: 0.673033, acc.: 51.56%] [G loss: 0.823385]\n",
      "epoch:7 step:6833 [D loss: 0.671161, acc.: 57.03%] [G loss: 0.853675]\n",
      "epoch:7 step:6834 [D loss: 0.683287, acc.: 50.78%] [G loss: 0.733558]\n",
      "epoch:7 step:6835 [D loss: 0.704290, acc.: 53.91%] [G loss: 0.725092]\n",
      "epoch:7 step:6836 [D loss: 0.705097, acc.: 43.75%] [G loss: 0.745830]\n",
      "epoch:7 step:6837 [D loss: 0.692121, acc.: 48.44%] [G loss: 0.758307]\n",
      "epoch:7 step:6838 [D loss: 0.675259, acc.: 57.81%] [G loss: 0.826446]\n",
      "epoch:7 step:6839 [D loss: 0.676656, acc.: 58.59%] [G loss: 0.769627]\n",
      "epoch:7 step:6840 [D loss: 0.697345, acc.: 50.78%] [G loss: 0.754260]\n",
      "epoch:7 step:6841 [D loss: 0.663782, acc.: 60.16%] [G loss: 0.808669]\n",
      "epoch:7 step:6842 [D loss: 0.652217, acc.: 57.03%] [G loss: 0.777264]\n",
      "epoch:7 step:6843 [D loss: 0.683929, acc.: 51.56%] [G loss: 0.788155]\n",
      "epoch:7 step:6844 [D loss: 0.665819, acc.: 64.84%] [G loss: 0.737578]\n",
      "epoch:7 step:6845 [D loss: 0.714953, acc.: 41.41%] [G loss: 0.806151]\n",
      "epoch:7 step:6846 [D loss: 0.680147, acc.: 54.69%] [G loss: 0.780877]\n",
      "epoch:7 step:6847 [D loss: 0.705148, acc.: 42.97%] [G loss: 0.792327]\n",
      "epoch:7 step:6848 [D loss: 0.700171, acc.: 46.88%] [G loss: 0.781823]\n",
      "epoch:7 step:6849 [D loss: 0.663883, acc.: 60.94%] [G loss: 0.767366]\n",
      "epoch:7 step:6850 [D loss: 0.716568, acc.: 50.00%] [G loss: 0.800781]\n",
      "epoch:7 step:6851 [D loss: 0.722338, acc.: 39.84%] [G loss: 0.737375]\n",
      "epoch:7 step:6852 [D loss: 0.695652, acc.: 50.00%] [G loss: 0.761980]\n",
      "epoch:7 step:6853 [D loss: 0.688187, acc.: 57.81%] [G loss: 0.746371]\n",
      "epoch:7 step:6854 [D loss: 0.701273, acc.: 49.22%] [G loss: 0.774636]\n",
      "epoch:7 step:6855 [D loss: 0.694256, acc.: 53.12%] [G loss: 0.778238]\n",
      "epoch:7 step:6856 [D loss: 0.639835, acc.: 67.19%] [G loss: 0.829292]\n",
      "epoch:7 step:6857 [D loss: 0.689516, acc.: 52.34%] [G loss: 0.788773]\n",
      "epoch:7 step:6858 [D loss: 0.678599, acc.: 48.44%] [G loss: 0.803312]\n",
      "epoch:7 step:6859 [D loss: 0.664743, acc.: 56.25%] [G loss: 0.805208]\n",
      "epoch:7 step:6860 [D loss: 0.687195, acc.: 60.16%] [G loss: 0.769752]\n",
      "epoch:7 step:6861 [D loss: 0.663715, acc.: 55.47%] [G loss: 0.794689]\n",
      "epoch:7 step:6862 [D loss: 0.704741, acc.: 51.56%] [G loss: 0.759751]\n",
      "epoch:7 step:6863 [D loss: 0.710949, acc.: 51.56%] [G loss: 0.744638]\n",
      "epoch:7 step:6864 [D loss: 0.674860, acc.: 54.69%] [G loss: 0.779820]\n",
      "epoch:7 step:6865 [D loss: 0.690748, acc.: 51.56%] [G loss: 0.763335]\n",
      "epoch:7 step:6866 [D loss: 0.687214, acc.: 53.91%] [G loss: 0.790539]\n",
      "epoch:7 step:6867 [D loss: 0.677832, acc.: 56.25%] [G loss: 0.774987]\n",
      "epoch:7 step:6868 [D loss: 0.679876, acc.: 47.66%] [G loss: 0.823511]\n",
      "epoch:7 step:6869 [D loss: 0.689615, acc.: 53.91%] [G loss: 0.762108]\n",
      "epoch:7 step:6870 [D loss: 0.675152, acc.: 55.47%] [G loss: 0.798934]\n",
      "epoch:7 step:6871 [D loss: 0.679918, acc.: 57.03%] [G loss: 0.745101]\n",
      "epoch:7 step:6872 [D loss: 0.708451, acc.: 49.22%] [G loss: 0.795007]\n",
      "epoch:7 step:6873 [D loss: 0.663189, acc.: 58.59%] [G loss: 0.710885]\n",
      "epoch:7 step:6874 [D loss: 0.645438, acc.: 63.28%] [G loss: 0.761634]\n",
      "epoch:7 step:6875 [D loss: 0.698453, acc.: 49.22%] [G loss: 0.752483]\n",
      "epoch:7 step:6876 [D loss: 0.708060, acc.: 50.78%] [G loss: 0.759146]\n",
      "epoch:7 step:6877 [D loss: 0.689280, acc.: 52.34%] [G loss: 0.764620]\n",
      "epoch:7 step:6878 [D loss: 0.681946, acc.: 57.03%] [G loss: 0.761783]\n",
      "epoch:7 step:6879 [D loss: 0.709305, acc.: 56.25%] [G loss: 0.783786]\n",
      "epoch:7 step:6880 [D loss: 0.696575, acc.: 52.34%] [G loss: 0.858681]\n",
      "epoch:7 step:6881 [D loss: 0.680259, acc.: 61.72%] [G loss: 0.759058]\n",
      "epoch:7 step:6882 [D loss: 0.673356, acc.: 59.38%] [G loss: 0.808458]\n",
      "epoch:7 step:6883 [D loss: 0.685522, acc.: 57.81%] [G loss: 0.739852]\n",
      "epoch:7 step:6884 [D loss: 0.708880, acc.: 45.31%] [G loss: 0.747313]\n",
      "epoch:7 step:6885 [D loss: 0.698549, acc.: 50.00%] [G loss: 0.721707]\n",
      "epoch:7 step:6886 [D loss: 0.662956, acc.: 63.28%] [G loss: 0.814029]\n",
      "epoch:7 step:6887 [D loss: 0.692609, acc.: 53.12%] [G loss: 0.748677]\n",
      "epoch:7 step:6888 [D loss: 0.697794, acc.: 47.66%] [G loss: 0.768083]\n",
      "epoch:7 step:6889 [D loss: 0.686781, acc.: 50.78%] [G loss: 0.745874]\n",
      "epoch:7 step:6890 [D loss: 0.693315, acc.: 56.25%] [G loss: 0.775101]\n",
      "epoch:7 step:6891 [D loss: 0.676100, acc.: 54.69%] [G loss: 0.736381]\n",
      "epoch:7 step:6892 [D loss: 0.669595, acc.: 58.59%] [G loss: 0.779871]\n",
      "epoch:7 step:6893 [D loss: 0.682019, acc.: 55.47%] [G loss: 0.784470]\n",
      "epoch:7 step:6894 [D loss: 0.671839, acc.: 56.25%] [G loss: 0.791768]\n",
      "epoch:7 step:6895 [D loss: 0.730692, acc.: 54.69%] [G loss: 0.791001]\n",
      "epoch:7 step:6896 [D loss: 0.668421, acc.: 53.91%] [G loss: 0.789114]\n",
      "epoch:7 step:6897 [D loss: 0.706877, acc.: 45.31%] [G loss: 0.796419]\n",
      "epoch:7 step:6898 [D loss: 0.704665, acc.: 53.12%] [G loss: 0.780223]\n",
      "epoch:7 step:6899 [D loss: 0.704452, acc.: 50.00%] [G loss: 0.796399]\n",
      "epoch:7 step:6900 [D loss: 0.690539, acc.: 63.28%] [G loss: 0.796458]\n",
      "epoch:7 step:6901 [D loss: 0.677435, acc.: 54.69%] [G loss: 0.771439]\n",
      "epoch:7 step:6902 [D loss: 0.682807, acc.: 50.78%] [G loss: 0.843130]\n",
      "epoch:7 step:6903 [D loss: 0.731700, acc.: 46.09%] [G loss: 0.790676]\n",
      "epoch:7 step:6904 [D loss: 0.671854, acc.: 53.91%] [G loss: 0.731531]\n",
      "epoch:7 step:6905 [D loss: 0.690573, acc.: 52.34%] [G loss: 0.793991]\n",
      "epoch:7 step:6906 [D loss: 0.688235, acc.: 46.88%] [G loss: 0.857948]\n",
      "epoch:7 step:6907 [D loss: 0.672930, acc.: 54.69%] [G loss: 0.764782]\n",
      "epoch:7 step:6908 [D loss: 0.681150, acc.: 56.25%] [G loss: 0.763844]\n",
      "epoch:7 step:6909 [D loss: 0.696608, acc.: 48.44%] [G loss: 0.783385]\n",
      "epoch:7 step:6910 [D loss: 0.708529, acc.: 57.81%] [G loss: 0.814438]\n",
      "epoch:7 step:6911 [D loss: 0.699444, acc.: 49.22%] [G loss: 0.725344]\n",
      "epoch:7 step:6912 [D loss: 0.704589, acc.: 53.12%] [G loss: 0.769527]\n",
      "epoch:7 step:6913 [D loss: 0.682795, acc.: 57.81%] [G loss: 0.824428]\n",
      "epoch:7 step:6914 [D loss: 0.664863, acc.: 57.03%] [G loss: 0.814269]\n",
      "epoch:7 step:6915 [D loss: 0.694290, acc.: 45.31%] [G loss: 0.799812]\n",
      "epoch:7 step:6916 [D loss: 0.653697, acc.: 58.59%] [G loss: 0.729354]\n",
      "epoch:7 step:6917 [D loss: 0.697996, acc.: 58.59%] [G loss: 0.762445]\n",
      "epoch:7 step:6918 [D loss: 0.681684, acc.: 54.69%] [G loss: 0.799440]\n",
      "epoch:7 step:6919 [D loss: 0.685378, acc.: 51.56%] [G loss: 0.758894]\n",
      "epoch:7 step:6920 [D loss: 0.696583, acc.: 46.88%] [G loss: 0.749685]\n",
      "epoch:7 step:6921 [D loss: 0.705773, acc.: 53.91%] [G loss: 0.781432]\n",
      "epoch:7 step:6922 [D loss: 0.655766, acc.: 64.84%] [G loss: 0.790574]\n",
      "epoch:7 step:6923 [D loss: 0.661275, acc.: 56.25%] [G loss: 0.802496]\n",
      "epoch:7 step:6924 [D loss: 0.664508, acc.: 68.75%] [G loss: 0.790806]\n",
      "epoch:7 step:6925 [D loss: 0.666529, acc.: 59.38%] [G loss: 0.793414]\n",
      "epoch:7 step:6926 [D loss: 0.673363, acc.: 58.59%] [G loss: 0.818077]\n",
      "epoch:7 step:6927 [D loss: 0.687391, acc.: 53.91%] [G loss: 0.766630]\n",
      "epoch:7 step:6928 [D loss: 0.675354, acc.: 63.28%] [G loss: 0.776802]\n",
      "epoch:7 step:6929 [D loss: 0.676843, acc.: 57.03%] [G loss: 0.756449]\n",
      "epoch:7 step:6930 [D loss: 0.690439, acc.: 53.91%] [G loss: 0.764654]\n",
      "epoch:7 step:6931 [D loss: 0.680369, acc.: 60.16%] [G loss: 0.726082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6932 [D loss: 0.669008, acc.: 58.59%] [G loss: 0.757539]\n",
      "epoch:7 step:6933 [D loss: 0.724413, acc.: 46.09%] [G loss: 0.786492]\n",
      "epoch:7 step:6934 [D loss: 0.686974, acc.: 57.03%] [G loss: 0.792178]\n",
      "epoch:7 step:6935 [D loss: 0.668031, acc.: 60.94%] [G loss: 0.786526]\n",
      "epoch:7 step:6936 [D loss: 0.721573, acc.: 47.66%] [G loss: 0.788381]\n",
      "epoch:7 step:6937 [D loss: 0.665947, acc.: 56.25%] [G loss: 0.759306]\n",
      "epoch:7 step:6938 [D loss: 0.705236, acc.: 52.34%] [G loss: 0.826446]\n",
      "epoch:7 step:6939 [D loss: 0.701319, acc.: 49.22%] [G loss: 0.780609]\n",
      "epoch:7 step:6940 [D loss: 0.664163, acc.: 53.12%] [G loss: 0.771656]\n",
      "epoch:7 step:6941 [D loss: 0.672197, acc.: 61.72%] [G loss: 0.787485]\n",
      "epoch:7 step:6942 [D loss: 0.700570, acc.: 54.69%] [G loss: 0.780166]\n",
      "epoch:7 step:6943 [D loss: 0.703182, acc.: 49.22%] [G loss: 0.735371]\n",
      "epoch:7 step:6944 [D loss: 0.711902, acc.: 54.69%] [G loss: 0.762110]\n",
      "epoch:7 step:6945 [D loss: 0.694497, acc.: 49.22%] [G loss: 0.749774]\n",
      "epoch:7 step:6946 [D loss: 0.694598, acc.: 53.12%] [G loss: 0.793095]\n",
      "epoch:7 step:6947 [D loss: 0.664796, acc.: 62.50%] [G loss: 0.803489]\n",
      "epoch:7 step:6948 [D loss: 0.712804, acc.: 48.44%] [G loss: 0.736096]\n",
      "epoch:7 step:6949 [D loss: 0.726966, acc.: 46.09%] [G loss: 0.743029]\n",
      "epoch:7 step:6950 [D loss: 0.712490, acc.: 50.78%] [G loss: 0.755194]\n",
      "epoch:7 step:6951 [D loss: 0.676723, acc.: 54.69%] [G loss: 0.799973]\n",
      "epoch:7 step:6952 [D loss: 0.701228, acc.: 53.91%] [G loss: 0.767718]\n",
      "epoch:7 step:6953 [D loss: 0.681211, acc.: 59.38%] [G loss: 0.808492]\n",
      "epoch:7 step:6954 [D loss: 0.690589, acc.: 51.56%] [G loss: 0.823764]\n",
      "epoch:7 step:6955 [D loss: 0.685797, acc.: 54.69%] [G loss: 0.764901]\n",
      "epoch:7 step:6956 [D loss: 0.689923, acc.: 56.25%] [G loss: 0.792140]\n",
      "epoch:7 step:6957 [D loss: 0.674023, acc.: 55.47%] [G loss: 0.775945]\n",
      "epoch:7 step:6958 [D loss: 0.695004, acc.: 50.00%] [G loss: 0.811583]\n",
      "epoch:7 step:6959 [D loss: 0.669138, acc.: 48.44%] [G loss: 0.776076]\n",
      "epoch:7 step:6960 [D loss: 0.667815, acc.: 59.38%] [G loss: 0.789752]\n",
      "epoch:7 step:6961 [D loss: 0.700565, acc.: 52.34%] [G loss: 0.794548]\n",
      "epoch:7 step:6962 [D loss: 0.676000, acc.: 56.25%] [G loss: 0.774721]\n",
      "epoch:7 step:6963 [D loss: 0.677355, acc.: 52.34%] [G loss: 0.758919]\n",
      "epoch:7 step:6964 [D loss: 0.670062, acc.: 58.59%] [G loss: 0.750984]\n",
      "epoch:7 step:6965 [D loss: 0.654269, acc.: 65.62%] [G loss: 0.733409]\n",
      "epoch:7 step:6966 [D loss: 0.663941, acc.: 61.72%] [G loss: 0.757317]\n",
      "epoch:7 step:6967 [D loss: 0.684338, acc.: 51.56%] [G loss: 0.766870]\n",
      "epoch:7 step:6968 [D loss: 0.686218, acc.: 56.25%] [G loss: 0.760998]\n",
      "epoch:7 step:6969 [D loss: 0.689868, acc.: 51.56%] [G loss: 0.791083]\n",
      "epoch:7 step:6970 [D loss: 0.683486, acc.: 52.34%] [G loss: 0.800691]\n",
      "epoch:7 step:6971 [D loss: 0.688840, acc.: 46.09%] [G loss: 0.807767]\n",
      "epoch:7 step:6972 [D loss: 0.689429, acc.: 53.91%] [G loss: 0.805316]\n",
      "epoch:7 step:6973 [D loss: 0.648257, acc.: 60.16%] [G loss: 0.856032]\n",
      "epoch:7 step:6974 [D loss: 0.663118, acc.: 53.12%] [G loss: 0.809292]\n",
      "epoch:7 step:6975 [D loss: 0.684032, acc.: 49.22%] [G loss: 0.763638]\n",
      "epoch:7 step:6976 [D loss: 0.691648, acc.: 50.78%] [G loss: 0.765783]\n",
      "epoch:7 step:6977 [D loss: 0.654701, acc.: 59.38%] [G loss: 0.786827]\n",
      "epoch:7 step:6978 [D loss: 0.702905, acc.: 44.53%] [G loss: 0.769567]\n",
      "epoch:7 step:6979 [D loss: 0.675660, acc.: 60.16%] [G loss: 0.762812]\n",
      "epoch:7 step:6980 [D loss: 0.679516, acc.: 58.59%] [G loss: 0.741830]\n",
      "epoch:7 step:6981 [D loss: 0.714774, acc.: 51.56%] [G loss: 0.789377]\n",
      "epoch:7 step:6982 [D loss: 0.690618, acc.: 59.38%] [G loss: 0.759563]\n",
      "epoch:7 step:6983 [D loss: 0.718146, acc.: 48.44%] [G loss: 0.772383]\n",
      "epoch:7 step:6984 [D loss: 0.669404, acc.: 61.72%] [G loss: 0.751893]\n",
      "epoch:7 step:6985 [D loss: 0.669248, acc.: 53.12%] [G loss: 0.792049]\n",
      "epoch:7 step:6986 [D loss: 0.682535, acc.: 57.81%] [G loss: 0.819137]\n",
      "epoch:7 step:6987 [D loss: 0.685662, acc.: 52.34%] [G loss: 0.803241]\n",
      "epoch:7 step:6988 [D loss: 0.681116, acc.: 53.91%] [G loss: 0.849271]\n",
      "epoch:7 step:6989 [D loss: 0.665266, acc.: 56.25%] [G loss: 0.852948]\n",
      "epoch:7 step:6990 [D loss: 0.675403, acc.: 60.16%] [G loss: 0.820820]\n",
      "epoch:7 step:6991 [D loss: 0.700939, acc.: 54.69%] [G loss: 0.808621]\n",
      "epoch:7 step:6992 [D loss: 0.675845, acc.: 60.16%] [G loss: 0.782804]\n",
      "epoch:7 step:6993 [D loss: 0.647499, acc.: 58.59%] [G loss: 0.772522]\n",
      "epoch:7 step:6994 [D loss: 0.673663, acc.: 58.59%] [G loss: 0.843634]\n",
      "epoch:7 step:6995 [D loss: 0.685515, acc.: 57.03%] [G loss: 0.820533]\n",
      "epoch:7 step:6996 [D loss: 0.710974, acc.: 47.66%] [G loss: 0.819580]\n",
      "epoch:7 step:6997 [D loss: 0.666942, acc.: 60.16%] [G loss: 0.878689]\n",
      "epoch:7 step:6998 [D loss: 0.685642, acc.: 52.34%] [G loss: 0.885658]\n",
      "epoch:7 step:6999 [D loss: 0.679894, acc.: 50.78%] [G loss: 0.859579]\n",
      "epoch:7 step:7000 [D loss: 0.658619, acc.: 60.16%] [G loss: 0.778555]\n",
      "epoch:7 step:7001 [D loss: 0.674985, acc.: 53.12%] [G loss: 0.785143]\n",
      "epoch:7 step:7002 [D loss: 0.706672, acc.: 49.22%] [G loss: 0.773063]\n",
      "epoch:7 step:7003 [D loss: 0.658444, acc.: 61.72%] [G loss: 0.800734]\n",
      "epoch:7 step:7004 [D loss: 0.697158, acc.: 50.00%] [G loss: 0.796651]\n",
      "epoch:7 step:7005 [D loss: 0.687417, acc.: 57.81%] [G loss: 0.761517]\n",
      "epoch:7 step:7006 [D loss: 0.668728, acc.: 57.03%] [G loss: 0.749837]\n",
      "epoch:7 step:7007 [D loss: 0.713453, acc.: 48.44%] [G loss: 0.789481]\n",
      "epoch:7 step:7008 [D loss: 0.731740, acc.: 46.09%] [G loss: 0.765466]\n",
      "epoch:7 step:7009 [D loss: 0.684603, acc.: 56.25%] [G loss: 0.782388]\n",
      "epoch:7 step:7010 [D loss: 0.658958, acc.: 57.81%] [G loss: 0.784976]\n",
      "epoch:7 step:7011 [D loss: 0.667005, acc.: 61.72%] [G loss: 0.775967]\n",
      "epoch:7 step:7012 [D loss: 0.680433, acc.: 57.03%] [G loss: 0.758241]\n",
      "epoch:7 step:7013 [D loss: 0.682885, acc.: 55.47%] [G loss: 0.770620]\n",
      "epoch:7 step:7014 [D loss: 0.688312, acc.: 49.22%] [G loss: 0.745864]\n",
      "epoch:7 step:7015 [D loss: 0.667071, acc.: 58.59%] [G loss: 0.811784]\n",
      "epoch:7 step:7016 [D loss: 0.660087, acc.: 60.16%] [G loss: 0.762154]\n",
      "epoch:7 step:7017 [D loss: 0.650803, acc.: 61.72%] [G loss: 0.762779]\n",
      "epoch:7 step:7018 [D loss: 0.700314, acc.: 50.78%] [G loss: 0.836576]\n",
      "epoch:7 step:7019 [D loss: 0.634499, acc.: 65.62%] [G loss: 0.808561]\n",
      "epoch:7 step:7020 [D loss: 0.640668, acc.: 60.94%] [G loss: 0.840360]\n",
      "epoch:7 step:7021 [D loss: 0.654072, acc.: 62.50%] [G loss: 0.851948]\n",
      "epoch:7 step:7022 [D loss: 0.652833, acc.: 58.59%] [G loss: 0.838958]\n",
      "epoch:7 step:7023 [D loss: 0.696893, acc.: 47.66%] [G loss: 0.846655]\n",
      "epoch:7 step:7024 [D loss: 0.673261, acc.: 56.25%] [G loss: 0.844188]\n",
      "epoch:7 step:7025 [D loss: 0.683826, acc.: 53.12%] [G loss: 0.806129]\n",
      "epoch:7 step:7026 [D loss: 0.676938, acc.: 58.59%] [G loss: 0.891930]\n",
      "epoch:7 step:7027 [D loss: 0.610157, acc.: 67.97%] [G loss: 0.817367]\n",
      "epoch:7 step:7028 [D loss: 0.672869, acc.: 57.81%] [G loss: 0.890265]\n",
      "epoch:7 step:7029 [D loss: 0.702969, acc.: 53.12%] [G loss: 0.880904]\n",
      "epoch:7 step:7030 [D loss: 0.679944, acc.: 55.47%] [G loss: 0.820053]\n",
      "epoch:7 step:7031 [D loss: 0.715437, acc.: 48.44%] [G loss: 0.885039]\n",
      "epoch:7 step:7032 [D loss: 0.678041, acc.: 53.91%] [G loss: 0.833268]\n",
      "epoch:7 step:7033 [D loss: 0.695861, acc.: 54.69%] [G loss: 0.779196]\n",
      "epoch:7 step:7034 [D loss: 0.681309, acc.: 58.59%] [G loss: 0.825114]\n",
      "epoch:7 step:7035 [D loss: 0.661136, acc.: 60.16%] [G loss: 0.820293]\n",
      "epoch:7 step:7036 [D loss: 0.717055, acc.: 39.84%] [G loss: 0.833310]\n",
      "epoch:7 step:7037 [D loss: 0.692678, acc.: 55.47%] [G loss: 0.833010]\n",
      "epoch:7 step:7038 [D loss: 0.723229, acc.: 50.00%] [G loss: 0.865557]\n",
      "epoch:7 step:7039 [D loss: 0.694130, acc.: 52.34%] [G loss: 0.821673]\n",
      "epoch:7 step:7040 [D loss: 0.666996, acc.: 61.72%] [G loss: 0.865754]\n",
      "epoch:7 step:7041 [D loss: 0.669425, acc.: 63.28%] [G loss: 0.844009]\n",
      "epoch:7 step:7042 [D loss: 0.704139, acc.: 56.25%] [G loss: 0.813555]\n",
      "epoch:7 step:7043 [D loss: 0.675687, acc.: 53.91%] [G loss: 0.796374]\n",
      "epoch:7 step:7044 [D loss: 0.697008, acc.: 51.56%] [G loss: 0.808231]\n",
      "epoch:7 step:7045 [D loss: 0.642402, acc.: 58.59%] [G loss: 0.765641]\n",
      "epoch:7 step:7046 [D loss: 0.695758, acc.: 50.00%] [G loss: 0.733750]\n",
      "epoch:7 step:7047 [D loss: 0.668530, acc.: 54.69%] [G loss: 0.817222]\n",
      "epoch:7 step:7048 [D loss: 0.660193, acc.: 57.81%] [G loss: 0.820280]\n",
      "epoch:7 step:7049 [D loss: 0.676475, acc.: 60.16%] [G loss: 0.801688]\n",
      "epoch:7 step:7050 [D loss: 0.719152, acc.: 46.09%] [G loss: 0.799415]\n",
      "epoch:7 step:7051 [D loss: 0.673778, acc.: 55.47%] [G loss: 0.776166]\n",
      "epoch:7 step:7052 [D loss: 0.696530, acc.: 52.34%] [G loss: 0.758278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7053 [D loss: 0.682920, acc.: 61.72%] [G loss: 0.782322]\n",
      "epoch:7 step:7054 [D loss: 0.676224, acc.: 55.47%] [G loss: 0.813133]\n",
      "epoch:7 step:7055 [D loss: 0.656817, acc.: 64.06%] [G loss: 0.750279]\n",
      "epoch:7 step:7056 [D loss: 0.700505, acc.: 50.00%] [G loss: 0.764082]\n",
      "epoch:7 step:7057 [D loss: 0.687873, acc.: 52.34%] [G loss: 0.846708]\n",
      "epoch:7 step:7058 [D loss: 0.670582, acc.: 61.72%] [G loss: 0.810137]\n",
      "epoch:7 step:7059 [D loss: 0.706482, acc.: 46.09%] [G loss: 0.819370]\n",
      "epoch:7 step:7060 [D loss: 0.686262, acc.: 55.47%] [G loss: 0.782172]\n",
      "epoch:7 step:7061 [D loss: 0.690104, acc.: 52.34%] [G loss: 0.781354]\n",
      "epoch:7 step:7062 [D loss: 0.742670, acc.: 36.72%] [G loss: 0.807628]\n",
      "epoch:7 step:7063 [D loss: 0.707144, acc.: 43.75%] [G loss: 0.779177]\n",
      "epoch:7 step:7064 [D loss: 0.665725, acc.: 54.69%] [G loss: 0.761265]\n",
      "epoch:7 step:7065 [D loss: 0.697233, acc.: 45.31%] [G loss: 0.760423]\n",
      "epoch:7 step:7066 [D loss: 0.686786, acc.: 50.78%] [G loss: 0.852329]\n",
      "epoch:7 step:7067 [D loss: 0.658716, acc.: 59.38%] [G loss: 0.781808]\n",
      "epoch:7 step:7068 [D loss: 0.682737, acc.: 56.25%] [G loss: 0.764498]\n",
      "epoch:7 step:7069 [D loss: 0.656427, acc.: 55.47%] [G loss: 0.778926]\n",
      "epoch:7 step:7070 [D loss: 0.656692, acc.: 57.81%] [G loss: 0.811386]\n",
      "epoch:7 step:7071 [D loss: 0.684846, acc.: 50.00%] [G loss: 0.782591]\n",
      "epoch:7 step:7072 [D loss: 0.678498, acc.: 60.94%] [G loss: 0.773042]\n",
      "epoch:7 step:7073 [D loss: 0.646315, acc.: 69.53%] [G loss: 0.757278]\n",
      "epoch:7 step:7074 [D loss: 0.677156, acc.: 56.25%] [G loss: 0.836441]\n",
      "epoch:7 step:7075 [D loss: 0.663263, acc.: 59.38%] [G loss: 0.766259]\n",
      "epoch:7 step:7076 [D loss: 0.642425, acc.: 64.84%] [G loss: 0.802877]\n",
      "epoch:7 step:7077 [D loss: 0.697617, acc.: 51.56%] [G loss: 0.809481]\n",
      "epoch:7 step:7078 [D loss: 0.673549, acc.: 53.91%] [G loss: 0.824395]\n",
      "epoch:7 step:7079 [D loss: 0.692668, acc.: 52.34%] [G loss: 0.798676]\n",
      "epoch:7 step:7080 [D loss: 0.672095, acc.: 55.47%] [G loss: 0.780569]\n",
      "epoch:7 step:7081 [D loss: 0.725147, acc.: 51.56%] [G loss: 0.771344]\n",
      "epoch:7 step:7082 [D loss: 0.647936, acc.: 68.75%] [G loss: 0.835985]\n",
      "epoch:7 step:7083 [D loss: 0.691317, acc.: 62.50%] [G loss: 0.750703]\n",
      "epoch:7 step:7084 [D loss: 0.709127, acc.: 44.53%] [G loss: 0.749210]\n",
      "epoch:7 step:7085 [D loss: 0.691505, acc.: 59.38%] [G loss: 0.839473]\n",
      "epoch:7 step:7086 [D loss: 0.687349, acc.: 53.91%] [G loss: 0.809133]\n",
      "epoch:7 step:7087 [D loss: 0.667848, acc.: 64.06%] [G loss: 0.793994]\n",
      "epoch:7 step:7088 [D loss: 0.691828, acc.: 53.91%] [G loss: 0.842082]\n",
      "epoch:7 step:7089 [D loss: 0.678092, acc.: 57.03%] [G loss: 0.792004]\n",
      "epoch:7 step:7090 [D loss: 0.691149, acc.: 55.47%] [G loss: 0.756065]\n",
      "epoch:7 step:7091 [D loss: 0.691466, acc.: 51.56%] [G loss: 0.817087]\n",
      "epoch:7 step:7092 [D loss: 0.696810, acc.: 50.00%] [G loss: 0.843095]\n",
      "epoch:7 step:7093 [D loss: 0.677036, acc.: 50.78%] [G loss: 0.741592]\n",
      "epoch:7 step:7094 [D loss: 0.641420, acc.: 67.19%] [G loss: 0.833677]\n",
      "epoch:7 step:7095 [D loss: 0.704066, acc.: 53.91%] [G loss: 0.842000]\n",
      "epoch:7 step:7096 [D loss: 0.712115, acc.: 47.66%] [G loss: 0.819871]\n",
      "epoch:7 step:7097 [D loss: 0.712085, acc.: 50.00%] [G loss: 0.828767]\n",
      "epoch:7 step:7098 [D loss: 0.717477, acc.: 48.44%] [G loss: 0.744385]\n",
      "epoch:7 step:7099 [D loss: 0.666580, acc.: 57.81%] [G loss: 0.780177]\n",
      "epoch:7 step:7100 [D loss: 0.699668, acc.: 41.41%] [G loss: 0.750391]\n",
      "epoch:7 step:7101 [D loss: 0.671507, acc.: 52.34%] [G loss: 0.773132]\n",
      "epoch:7 step:7102 [D loss: 0.666458, acc.: 61.72%] [G loss: 0.833134]\n",
      "epoch:7 step:7103 [D loss: 0.646531, acc.: 60.94%] [G loss: 0.782949]\n",
      "epoch:7 step:7104 [D loss: 0.682738, acc.: 57.03%] [G loss: 0.805459]\n",
      "epoch:7 step:7105 [D loss: 0.680794, acc.: 51.56%] [G loss: 0.860092]\n",
      "epoch:7 step:7106 [D loss: 0.721894, acc.: 47.66%] [G loss: 0.771725]\n",
      "epoch:7 step:7107 [D loss: 0.673103, acc.: 54.69%] [G loss: 0.762651]\n",
      "epoch:7 step:7108 [D loss: 0.733582, acc.: 49.22%] [G loss: 0.785148]\n",
      "epoch:7 step:7109 [D loss: 0.684720, acc.: 57.03%] [G loss: 0.793137]\n",
      "epoch:7 step:7110 [D loss: 0.700876, acc.: 49.22%] [G loss: 0.770674]\n",
      "epoch:7 step:7111 [D loss: 0.696382, acc.: 47.66%] [G loss: 0.821274]\n",
      "epoch:7 step:7112 [D loss: 0.744531, acc.: 47.66%] [G loss: 0.791180]\n",
      "epoch:7 step:7113 [D loss: 0.688671, acc.: 50.00%] [G loss: 0.766729]\n",
      "epoch:7 step:7114 [D loss: 0.667759, acc.: 56.25%] [G loss: 0.758606]\n",
      "epoch:7 step:7115 [D loss: 0.703267, acc.: 49.22%] [G loss: 0.783657]\n",
      "epoch:7 step:7116 [D loss: 0.707817, acc.: 51.56%] [G loss: 0.790922]\n",
      "epoch:7 step:7117 [D loss: 0.648359, acc.: 67.97%] [G loss: 0.790717]\n",
      "epoch:7 step:7118 [D loss: 0.691635, acc.: 53.12%] [G loss: 0.761628]\n",
      "epoch:7 step:7119 [D loss: 0.683839, acc.: 52.34%] [G loss: 0.773150]\n",
      "epoch:7 step:7120 [D loss: 0.694767, acc.: 48.44%] [G loss: 0.829648]\n",
      "epoch:7 step:7121 [D loss: 0.698397, acc.: 43.75%] [G loss: 0.743107]\n",
      "epoch:7 step:7122 [D loss: 0.661191, acc.: 58.59%] [G loss: 0.805231]\n",
      "epoch:7 step:7123 [D loss: 0.679811, acc.: 58.59%] [G loss: 0.795360]\n",
      "epoch:7 step:7124 [D loss: 0.684865, acc.: 60.94%] [G loss: 0.756801]\n",
      "epoch:7 step:7125 [D loss: 0.666897, acc.: 60.94%] [G loss: 0.765082]\n",
      "epoch:7 step:7126 [D loss: 0.680057, acc.: 53.91%] [G loss: 0.823687]\n",
      "epoch:7 step:7127 [D loss: 0.689964, acc.: 49.22%] [G loss: 0.753292]\n",
      "epoch:7 step:7128 [D loss: 0.663865, acc.: 54.69%] [G loss: 0.744718]\n",
      "epoch:7 step:7129 [D loss: 0.725035, acc.: 43.75%] [G loss: 0.803314]\n",
      "epoch:7 step:7130 [D loss: 0.703436, acc.: 47.66%] [G loss: 0.782724]\n",
      "epoch:7 step:7131 [D loss: 0.666703, acc.: 61.72%] [G loss: 0.846604]\n",
      "epoch:7 step:7132 [D loss: 0.695629, acc.: 46.09%] [G loss: 0.862628]\n",
      "epoch:7 step:7133 [D loss: 0.672847, acc.: 53.91%] [G loss: 0.829301]\n",
      "epoch:7 step:7134 [D loss: 0.713516, acc.: 46.88%] [G loss: 0.806295]\n",
      "epoch:7 step:7135 [D loss: 0.676185, acc.: 57.03%] [G loss: 0.784482]\n",
      "epoch:7 step:7136 [D loss: 0.693014, acc.: 58.59%] [G loss: 0.766592]\n",
      "epoch:7 step:7137 [D loss: 0.681766, acc.: 54.69%] [G loss: 0.834860]\n",
      "epoch:7 step:7138 [D loss: 0.672899, acc.: 55.47%] [G loss: 0.805804]\n",
      "epoch:7 step:7139 [D loss: 0.702665, acc.: 46.88%] [G loss: 0.808000]\n",
      "epoch:7 step:7140 [D loss: 0.691276, acc.: 56.25%] [G loss: 0.818596]\n",
      "epoch:7 step:7141 [D loss: 0.664186, acc.: 64.06%] [G loss: 0.784685]\n",
      "epoch:7 step:7142 [D loss: 0.671535, acc.: 57.81%] [G loss: 0.839681]\n",
      "epoch:7 step:7143 [D loss: 0.679279, acc.: 54.69%] [G loss: 0.795244]\n",
      "epoch:7 step:7144 [D loss: 0.666369, acc.: 62.50%] [G loss: 0.778659]\n",
      "epoch:7 step:7145 [D loss: 0.703880, acc.: 43.75%] [G loss: 0.865313]\n",
      "epoch:7 step:7146 [D loss: 0.679201, acc.: 53.12%] [G loss: 0.829148]\n",
      "epoch:7 step:7147 [D loss: 0.652795, acc.: 56.25%] [G loss: 0.764066]\n",
      "epoch:7 step:7148 [D loss: 0.678783, acc.: 53.12%] [G loss: 0.777543]\n",
      "epoch:7 step:7149 [D loss: 0.738179, acc.: 40.62%] [G loss: 0.789709]\n",
      "epoch:7 step:7150 [D loss: 0.668997, acc.: 53.91%] [G loss: 0.809951]\n",
      "epoch:7 step:7151 [D loss: 0.659390, acc.: 58.59%] [G loss: 0.788042]\n",
      "epoch:7 step:7152 [D loss: 0.667919, acc.: 54.69%] [G loss: 0.800744]\n",
      "epoch:7 step:7153 [D loss: 0.664888, acc.: 60.94%] [G loss: 0.789995]\n",
      "epoch:7 step:7154 [D loss: 0.668038, acc.: 58.59%] [G loss: 0.786145]\n",
      "epoch:7 step:7155 [D loss: 0.634781, acc.: 65.62%] [G loss: 0.836484]\n",
      "epoch:7 step:7156 [D loss: 0.679549, acc.: 54.69%] [G loss: 0.855920]\n",
      "epoch:7 step:7157 [D loss: 0.661870, acc.: 58.59%] [G loss: 0.887456]\n",
      "epoch:7 step:7158 [D loss: 0.652241, acc.: 60.16%] [G loss: 0.870240]\n",
      "epoch:7 step:7159 [D loss: 0.702884, acc.: 47.66%] [G loss: 0.805250]\n",
      "epoch:7 step:7160 [D loss: 0.672910, acc.: 57.03%] [G loss: 0.848943]\n",
      "epoch:7 step:7161 [D loss: 0.657366, acc.: 61.72%] [G loss: 0.795939]\n",
      "epoch:7 step:7162 [D loss: 0.691306, acc.: 50.78%] [G loss: 0.755966]\n",
      "epoch:7 step:7163 [D loss: 0.683658, acc.: 54.69%] [G loss: 0.858460]\n",
      "epoch:7 step:7164 [D loss: 0.656910, acc.: 61.72%] [G loss: 0.866737]\n",
      "epoch:7 step:7165 [D loss: 0.675962, acc.: 58.59%] [G loss: 0.873780]\n",
      "epoch:7 step:7166 [D loss: 0.670534, acc.: 55.47%] [G loss: 0.789390]\n",
      "epoch:7 step:7167 [D loss: 0.724389, acc.: 58.59%] [G loss: 0.753394]\n",
      "epoch:7 step:7168 [D loss: 0.686577, acc.: 49.22%] [G loss: 0.832035]\n",
      "epoch:7 step:7169 [D loss: 0.708908, acc.: 46.88%] [G loss: 0.803707]\n",
      "epoch:7 step:7170 [D loss: 0.711012, acc.: 47.66%] [G loss: 0.764168]\n",
      "epoch:7 step:7171 [D loss: 0.663935, acc.: 53.91%] [G loss: 0.776594]\n",
      "epoch:7 step:7172 [D loss: 0.699802, acc.: 46.09%] [G loss: 0.754307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7173 [D loss: 0.688243, acc.: 48.44%] [G loss: 0.805708]\n",
      "epoch:7 step:7174 [D loss: 0.662831, acc.: 63.28%] [G loss: 0.834417]\n",
      "epoch:7 step:7175 [D loss: 0.695589, acc.: 53.12%] [G loss: 0.798483]\n",
      "epoch:7 step:7176 [D loss: 0.670670, acc.: 60.94%] [G loss: 0.826984]\n",
      "epoch:7 step:7177 [D loss: 0.681494, acc.: 61.72%] [G loss: 0.787150]\n",
      "epoch:7 step:7178 [D loss: 0.630761, acc.: 64.84%] [G loss: 0.827981]\n",
      "epoch:7 step:7179 [D loss: 0.688875, acc.: 56.25%] [G loss: 0.772461]\n",
      "epoch:7 step:7180 [D loss: 0.673301, acc.: 64.06%] [G loss: 0.744750]\n",
      "epoch:7 step:7181 [D loss: 0.715478, acc.: 44.53%] [G loss: 0.732647]\n",
      "epoch:7 step:7182 [D loss: 0.684004, acc.: 61.72%] [G loss: 0.796970]\n",
      "epoch:7 step:7183 [D loss: 0.655705, acc.: 66.41%] [G loss: 0.783626]\n",
      "epoch:7 step:7184 [D loss: 0.685329, acc.: 51.56%] [G loss: 0.858497]\n",
      "epoch:7 step:7185 [D loss: 0.708035, acc.: 51.56%] [G loss: 0.750200]\n",
      "epoch:7 step:7186 [D loss: 0.696503, acc.: 51.56%] [G loss: 0.770101]\n",
      "epoch:7 step:7187 [D loss: 0.667367, acc.: 56.25%] [G loss: 0.763200]\n",
      "epoch:7 step:7188 [D loss: 0.682136, acc.: 51.56%] [G loss: 0.804877]\n",
      "epoch:7 step:7189 [D loss: 0.687107, acc.: 50.78%] [G loss: 0.769166]\n",
      "epoch:7 step:7190 [D loss: 0.681649, acc.: 52.34%] [G loss: 0.838273]\n",
      "epoch:7 step:7191 [D loss: 0.726142, acc.: 40.62%] [G loss: 0.745646]\n",
      "epoch:7 step:7192 [D loss: 0.668290, acc.: 59.38%] [G loss: 0.745468]\n",
      "epoch:7 step:7193 [D loss: 0.708203, acc.: 54.69%] [G loss: 0.820069]\n",
      "epoch:7 step:7194 [D loss: 0.670243, acc.: 56.25%] [G loss: 0.838298]\n",
      "epoch:7 step:7195 [D loss: 0.710644, acc.: 47.66%] [G loss: 0.812969]\n",
      "epoch:7 step:7196 [D loss: 0.696286, acc.: 52.34%] [G loss: 0.820575]\n",
      "epoch:7 step:7197 [D loss: 0.644365, acc.: 60.16%] [G loss: 0.817703]\n",
      "epoch:7 step:7198 [D loss: 0.693285, acc.: 46.88%] [G loss: 0.864145]\n",
      "epoch:7 step:7199 [D loss: 0.678714, acc.: 60.94%] [G loss: 0.796339]\n",
      "epoch:7 step:7200 [D loss: 0.678253, acc.: 55.47%] [G loss: 0.787462]\n",
      "epoch:7 step:7201 [D loss: 0.683462, acc.: 53.91%] [G loss: 0.761099]\n",
      "epoch:7 step:7202 [D loss: 0.692197, acc.: 54.69%] [G loss: 0.765527]\n",
      "epoch:7 step:7203 [D loss: 0.714963, acc.: 44.53%] [G loss: 0.763924]\n",
      "epoch:7 step:7204 [D loss: 0.666809, acc.: 57.81%] [G loss: 0.758357]\n",
      "epoch:7 step:7205 [D loss: 0.900052, acc.: 48.44%] [G loss: 0.807243]\n",
      "epoch:7 step:7206 [D loss: 0.642660, acc.: 63.28%] [G loss: 0.829666]\n",
      "epoch:7 step:7207 [D loss: 0.671001, acc.: 53.91%] [G loss: 0.917549]\n",
      "epoch:7 step:7208 [D loss: 0.662541, acc.: 60.94%] [G loss: 0.890692]\n",
      "epoch:7 step:7209 [D loss: 0.710151, acc.: 50.00%] [G loss: 0.807506]\n",
      "epoch:7 step:7210 [D loss: 0.678063, acc.: 59.38%] [G loss: 0.755690]\n",
      "epoch:7 step:7211 [D loss: 0.672711, acc.: 60.16%] [G loss: 0.815823]\n",
      "epoch:7 step:7212 [D loss: 0.674960, acc.: 53.91%] [G loss: 0.820107]\n",
      "epoch:7 step:7213 [D loss: 0.689749, acc.: 51.56%] [G loss: 0.781661]\n",
      "epoch:7 step:7214 [D loss: 0.678025, acc.: 56.25%] [G loss: 0.762548]\n",
      "epoch:7 step:7215 [D loss: 0.708335, acc.: 48.44%] [G loss: 0.790990]\n",
      "epoch:7 step:7216 [D loss: 0.704510, acc.: 48.44%] [G loss: 0.734760]\n",
      "epoch:7 step:7217 [D loss: 0.725106, acc.: 38.28%] [G loss: 0.752276]\n",
      "epoch:7 step:7218 [D loss: 0.675735, acc.: 53.12%] [G loss: 0.790328]\n",
      "epoch:7 step:7219 [D loss: 0.685826, acc.: 56.25%] [G loss: 0.813683]\n",
      "epoch:7 step:7220 [D loss: 0.678189, acc.: 52.34%] [G loss: 0.759652]\n",
      "epoch:7 step:7221 [D loss: 0.679045, acc.: 53.91%] [G loss: 0.721387]\n",
      "epoch:7 step:7222 [D loss: 0.687735, acc.: 50.78%] [G loss: 0.737033]\n",
      "epoch:7 step:7223 [D loss: 0.709308, acc.: 47.66%] [G loss: 0.749755]\n",
      "epoch:7 step:7224 [D loss: 0.655460, acc.: 58.59%] [G loss: 0.751479]\n",
      "epoch:7 step:7225 [D loss: 0.694289, acc.: 50.78%] [G loss: 0.716267]\n",
      "epoch:7 step:7226 [D loss: 0.676477, acc.: 50.78%] [G loss: 0.768716]\n",
      "epoch:7 step:7227 [D loss: 0.687567, acc.: 53.12%] [G loss: 0.720654]\n",
      "epoch:7 step:7228 [D loss: 0.682267, acc.: 57.03%] [G loss: 0.826300]\n",
      "epoch:7 step:7229 [D loss: 0.707983, acc.: 56.25%] [G loss: 0.822029]\n",
      "epoch:7 step:7230 [D loss: 0.682039, acc.: 55.47%] [G loss: 0.776959]\n",
      "epoch:7 step:7231 [D loss: 0.665671, acc.: 60.94%] [G loss: 0.832946]\n",
      "epoch:7 step:7232 [D loss: 0.679510, acc.: 51.56%] [G loss: 0.845881]\n",
      "epoch:7 step:7233 [D loss: 0.660398, acc.: 60.16%] [G loss: 0.779925]\n",
      "epoch:7 step:7234 [D loss: 0.737550, acc.: 53.91%] [G loss: 0.800186]\n",
      "epoch:7 step:7235 [D loss: 0.670638, acc.: 56.25%] [G loss: 0.844399]\n",
      "epoch:7 step:7236 [D loss: 0.663057, acc.: 60.16%] [G loss: 0.849676]\n",
      "epoch:7 step:7237 [D loss: 0.694450, acc.: 53.12%] [G loss: 0.826663]\n",
      "epoch:7 step:7238 [D loss: 0.650148, acc.: 59.38%] [G loss: 0.896602]\n",
      "epoch:7 step:7239 [D loss: 0.701137, acc.: 49.22%] [G loss: 0.799697]\n",
      "epoch:7 step:7240 [D loss: 0.665083, acc.: 57.03%] [G loss: 0.788893]\n",
      "epoch:7 step:7241 [D loss: 0.737330, acc.: 32.03%] [G loss: 0.769873]\n",
      "epoch:7 step:7242 [D loss: 0.702955, acc.: 48.44%] [G loss: 0.790992]\n",
      "epoch:7 step:7243 [D loss: 0.689820, acc.: 42.19%] [G loss: 0.767192]\n",
      "epoch:7 step:7244 [D loss: 0.711388, acc.: 42.97%] [G loss: 0.783744]\n",
      "epoch:7 step:7245 [D loss: 0.714018, acc.: 44.53%] [G loss: 0.744641]\n",
      "epoch:7 step:7246 [D loss: 0.707030, acc.: 44.53%] [G loss: 0.712460]\n",
      "epoch:7 step:7247 [D loss: 0.689511, acc.: 57.81%] [G loss: 0.720377]\n",
      "epoch:7 step:7248 [D loss: 0.698479, acc.: 55.47%] [G loss: 0.760198]\n",
      "epoch:7 step:7249 [D loss: 0.679169, acc.: 56.25%] [G loss: 0.799120]\n",
      "epoch:7 step:7250 [D loss: 0.673981, acc.: 57.81%] [G loss: 0.731935]\n",
      "epoch:7 step:7251 [D loss: 0.681708, acc.: 57.03%] [G loss: 0.785745]\n",
      "epoch:7 step:7252 [D loss: 0.717493, acc.: 42.97%] [G loss: 0.758911]\n",
      "epoch:7 step:7253 [D loss: 0.702847, acc.: 45.31%] [G loss: 0.742622]\n",
      "epoch:7 step:7254 [D loss: 0.686802, acc.: 60.94%] [G loss: 0.743710]\n",
      "epoch:7 step:7255 [D loss: 0.708144, acc.: 53.91%] [G loss: 0.781590]\n",
      "epoch:7 step:7256 [D loss: 0.674957, acc.: 59.38%] [G loss: 0.771933]\n",
      "epoch:7 step:7257 [D loss: 0.701573, acc.: 43.75%] [G loss: 0.748841]\n",
      "epoch:7 step:7258 [D loss: 0.695978, acc.: 57.81%] [G loss: 0.732744]\n",
      "epoch:7 step:7259 [D loss: 0.729928, acc.: 46.09%] [G loss: 0.722831]\n",
      "epoch:7 step:7260 [D loss: 0.676996, acc.: 54.69%] [G loss: 0.812587]\n",
      "epoch:7 step:7261 [D loss: 0.705774, acc.: 51.56%] [G loss: 0.767103]\n",
      "epoch:7 step:7262 [D loss: 0.698184, acc.: 49.22%] [G loss: 0.797760]\n",
      "epoch:7 step:7263 [D loss: 0.706496, acc.: 49.22%] [G loss: 0.812909]\n",
      "epoch:7 step:7264 [D loss: 0.704800, acc.: 50.78%] [G loss: 0.789742]\n",
      "epoch:7 step:7265 [D loss: 0.641295, acc.: 57.03%] [G loss: 0.778400]\n",
      "epoch:7 step:7266 [D loss: 0.685897, acc.: 57.81%] [G loss: 0.765579]\n",
      "epoch:7 step:7267 [D loss: 0.660471, acc.: 63.28%] [G loss: 0.774497]\n",
      "epoch:7 step:7268 [D loss: 0.688670, acc.: 52.34%] [G loss: 0.781850]\n",
      "epoch:7 step:7269 [D loss: 0.658726, acc.: 62.50%] [G loss: 0.808030]\n",
      "epoch:7 step:7270 [D loss: 0.690668, acc.: 57.03%] [G loss: 0.816695]\n",
      "epoch:7 step:7271 [D loss: 0.679119, acc.: 58.59%] [G loss: 0.805403]\n",
      "epoch:7 step:7272 [D loss: 0.618064, acc.: 60.16%] [G loss: 0.751879]\n",
      "epoch:7 step:7273 [D loss: 0.688781, acc.: 54.69%] [G loss: 0.784989]\n",
      "epoch:7 step:7274 [D loss: 0.698522, acc.: 52.34%] [G loss: 0.786643]\n",
      "epoch:7 step:7275 [D loss: 0.683243, acc.: 53.91%] [G loss: 0.760335]\n",
      "epoch:7 step:7276 [D loss: 0.681807, acc.: 54.69%] [G loss: 0.754563]\n",
      "epoch:7 step:7277 [D loss: 0.665674, acc.: 59.38%] [G loss: 0.811064]\n",
      "epoch:7 step:7278 [D loss: 0.681549, acc.: 53.12%] [G loss: 0.784686]\n",
      "epoch:7 step:7279 [D loss: 0.679446, acc.: 53.12%] [G loss: 0.775955]\n",
      "epoch:7 step:7280 [D loss: 0.684383, acc.: 55.47%] [G loss: 0.729393]\n",
      "epoch:7 step:7281 [D loss: 0.675712, acc.: 60.94%] [G loss: 0.736100]\n",
      "epoch:7 step:7282 [D loss: 0.689644, acc.: 56.25%] [G loss: 0.769164]\n",
      "epoch:7 step:7283 [D loss: 0.695755, acc.: 50.78%] [G loss: 0.781184]\n",
      "epoch:7 step:7284 [D loss: 0.704704, acc.: 49.22%] [G loss: 0.761651]\n",
      "epoch:7 step:7285 [D loss: 0.695258, acc.: 48.44%] [G loss: 0.800490]\n",
      "epoch:7 step:7286 [D loss: 0.687256, acc.: 49.22%] [G loss: 0.780394]\n",
      "epoch:7 step:7287 [D loss: 0.648437, acc.: 60.16%] [G loss: 0.833753]\n",
      "epoch:7 step:7288 [D loss: 0.675053, acc.: 60.94%] [G loss: 0.752882]\n",
      "epoch:7 step:7289 [D loss: 0.674827, acc.: 58.59%] [G loss: 0.811994]\n",
      "epoch:7 step:7290 [D loss: 0.695562, acc.: 51.56%] [G loss: 0.827034]\n",
      "epoch:7 step:7291 [D loss: 0.670588, acc.: 54.69%] [G loss: 0.793337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7292 [D loss: 0.650196, acc.: 63.28%] [G loss: 0.826241]\n",
      "epoch:7 step:7293 [D loss: 0.656886, acc.: 61.72%] [G loss: 0.789329]\n",
      "epoch:7 step:7294 [D loss: 0.652868, acc.: 66.41%] [G loss: 0.766839]\n",
      "epoch:7 step:7295 [D loss: 0.642381, acc.: 60.94%] [G loss: 0.818968]\n",
      "epoch:7 step:7296 [D loss: 0.661984, acc.: 52.34%] [G loss: 0.812952]\n",
      "epoch:7 step:7297 [D loss: 0.673346, acc.: 53.12%] [G loss: 0.769077]\n",
      "epoch:7 step:7298 [D loss: 0.743802, acc.: 46.88%] [G loss: 0.775575]\n",
      "epoch:7 step:7299 [D loss: 0.675757, acc.: 53.12%] [G loss: 0.894909]\n",
      "epoch:7 step:7300 [D loss: 0.639961, acc.: 64.06%] [G loss: 0.900469]\n",
      "epoch:7 step:7301 [D loss: 0.660555, acc.: 56.25%] [G loss: 0.842561]\n",
      "epoch:7 step:7302 [D loss: 0.695761, acc.: 52.34%] [G loss: 0.898503]\n",
      "epoch:7 step:7303 [D loss: 0.707035, acc.: 50.78%] [G loss: 0.816012]\n",
      "epoch:7 step:7304 [D loss: 0.652917, acc.: 58.59%] [G loss: 0.810060]\n",
      "epoch:7 step:7305 [D loss: 0.702536, acc.: 46.88%] [G loss: 0.838396]\n",
      "epoch:7 step:7306 [D loss: 0.702208, acc.: 55.47%] [G loss: 0.732177]\n",
      "epoch:7 step:7307 [D loss: 0.680016, acc.: 52.34%] [G loss: 0.764482]\n",
      "epoch:7 step:7308 [D loss: 0.688119, acc.: 55.47%] [G loss: 0.796861]\n",
      "epoch:7 step:7309 [D loss: 0.654745, acc.: 63.28%] [G loss: 0.836662]\n",
      "epoch:7 step:7310 [D loss: 0.710315, acc.: 53.12%] [G loss: 0.777305]\n",
      "epoch:7 step:7311 [D loss: 0.703651, acc.: 46.09%] [G loss: 0.840476]\n",
      "epoch:7 step:7312 [D loss: 0.650103, acc.: 67.19%] [G loss: 0.796202]\n",
      "epoch:7 step:7313 [D loss: 0.682647, acc.: 53.91%] [G loss: 0.819864]\n",
      "epoch:7 step:7314 [D loss: 0.695009, acc.: 49.22%] [G loss: 0.793939]\n",
      "epoch:7 step:7315 [D loss: 0.647838, acc.: 63.28%] [G loss: 0.786077]\n",
      "epoch:7 step:7316 [D loss: 0.734311, acc.: 45.31%] [G loss: 0.749772]\n",
      "epoch:7 step:7317 [D loss: 0.722950, acc.: 45.31%] [G loss: 0.813397]\n",
      "epoch:7 step:7318 [D loss: 0.677067, acc.: 59.38%] [G loss: 0.790343]\n",
      "epoch:7 step:7319 [D loss: 0.644878, acc.: 61.72%] [G loss: 0.811917]\n",
      "epoch:7 step:7320 [D loss: 0.671635, acc.: 59.38%] [G loss: 0.792434]\n",
      "epoch:7 step:7321 [D loss: 0.675274, acc.: 55.47%] [G loss: 0.773799]\n",
      "epoch:7 step:7322 [D loss: 0.661953, acc.: 62.50%] [G loss: 0.803333]\n",
      "epoch:7 step:7323 [D loss: 0.619774, acc.: 65.62%] [G loss: 0.838475]\n",
      "epoch:7 step:7324 [D loss: 0.678264, acc.: 54.69%] [G loss: 0.808293]\n",
      "epoch:7 step:7325 [D loss: 0.666051, acc.: 58.59%] [G loss: 0.961774]\n",
      "epoch:7 step:7326 [D loss: 0.716096, acc.: 50.78%] [G loss: 0.707940]\n",
      "epoch:7 step:7327 [D loss: 0.682009, acc.: 54.69%] [G loss: 0.788780]\n",
      "epoch:7 step:7328 [D loss: 0.663039, acc.: 57.81%] [G loss: 0.792145]\n",
      "epoch:7 step:7329 [D loss: 0.681861, acc.: 58.59%] [G loss: 0.799741]\n",
      "epoch:7 step:7330 [D loss: 0.665362, acc.: 61.72%] [G loss: 0.767749]\n",
      "epoch:7 step:7331 [D loss: 0.720966, acc.: 42.97%] [G loss: 0.850258]\n",
      "epoch:7 step:7332 [D loss: 0.686381, acc.: 53.12%] [G loss: 0.798387]\n",
      "epoch:7 step:7333 [D loss: 0.658402, acc.: 57.81%] [G loss: 0.771033]\n",
      "epoch:7 step:7334 [D loss: 0.713938, acc.: 49.22%] [G loss: 0.817336]\n",
      "epoch:7 step:7335 [D loss: 0.693867, acc.: 53.12%] [G loss: 0.835895]\n",
      "epoch:7 step:7336 [D loss: 0.673468, acc.: 58.59%] [G loss: 0.804183]\n",
      "epoch:7 step:7337 [D loss: 0.677771, acc.: 56.25%] [G loss: 0.811921]\n",
      "epoch:7 step:7338 [D loss: 0.715642, acc.: 51.56%] [G loss: 0.821125]\n",
      "epoch:7 step:7339 [D loss: 0.694925, acc.: 50.78%] [G loss: 0.815654]\n",
      "epoch:7 step:7340 [D loss: 0.683925, acc.: 54.69%] [G loss: 0.814410]\n",
      "epoch:7 step:7341 [D loss: 0.672792, acc.: 55.47%] [G loss: 0.790763]\n",
      "epoch:7 step:7342 [D loss: 0.713722, acc.: 40.62%] [G loss: 0.847788]\n",
      "epoch:7 step:7343 [D loss: 0.658048, acc.: 64.84%] [G loss: 0.809023]\n",
      "epoch:7 step:7344 [D loss: 0.718176, acc.: 47.66%] [G loss: 0.792123]\n",
      "epoch:7 step:7345 [D loss: 0.664565, acc.: 56.25%] [G loss: 0.833995]\n",
      "epoch:7 step:7346 [D loss: 0.715806, acc.: 54.69%] [G loss: 0.803689]\n",
      "epoch:7 step:7347 [D loss: 0.671541, acc.: 57.81%] [G loss: 0.828562]\n",
      "epoch:7 step:7348 [D loss: 0.645704, acc.: 64.06%] [G loss: 0.778510]\n",
      "epoch:7 step:7349 [D loss: 0.718946, acc.: 46.09%] [G loss: 0.809772]\n",
      "epoch:7 step:7350 [D loss: 0.660334, acc.: 59.38%] [G loss: 0.821283]\n",
      "epoch:7 step:7351 [D loss: 0.698375, acc.: 50.78%] [G loss: 0.763757]\n",
      "epoch:7 step:7352 [D loss: 0.746808, acc.: 49.22%] [G loss: 0.784454]\n",
      "epoch:7 step:7353 [D loss: 0.701105, acc.: 50.78%] [G loss: 0.808645]\n",
      "epoch:7 step:7354 [D loss: 0.658029, acc.: 65.62%] [G loss: 0.784244]\n",
      "epoch:7 step:7355 [D loss: 0.688733, acc.: 56.25%] [G loss: 0.918008]\n",
      "epoch:7 step:7356 [D loss: 0.673463, acc.: 57.81%] [G loss: 0.800099]\n",
      "epoch:7 step:7357 [D loss: 0.657079, acc.: 68.75%] [G loss: 0.830663]\n",
      "epoch:7 step:7358 [D loss: 0.674223, acc.: 60.16%] [G loss: 0.805526]\n",
      "epoch:7 step:7359 [D loss: 0.656046, acc.: 62.50%] [G loss: 0.828239]\n",
      "epoch:7 step:7360 [D loss: 0.626441, acc.: 68.75%] [G loss: 0.815846]\n",
      "epoch:7 step:7361 [D loss: 0.665940, acc.: 59.38%] [G loss: 0.797173]\n",
      "epoch:7 step:7362 [D loss: 0.637453, acc.: 61.72%] [G loss: 0.818158]\n",
      "epoch:7 step:7363 [D loss: 0.619291, acc.: 71.09%] [G loss: 0.758649]\n",
      "epoch:7 step:7364 [D loss: 0.684241, acc.: 57.03%] [G loss: 0.798080]\n",
      "epoch:7 step:7365 [D loss: 0.716773, acc.: 49.22%] [G loss: 0.767234]\n",
      "epoch:7 step:7366 [D loss: 0.695464, acc.: 49.22%] [G loss: 0.789891]\n",
      "epoch:7 step:7367 [D loss: 0.704269, acc.: 50.78%] [G loss: 0.787538]\n",
      "epoch:7 step:7368 [D loss: 0.634825, acc.: 64.84%] [G loss: 0.770630]\n",
      "epoch:7 step:7369 [D loss: 0.704919, acc.: 52.34%] [G loss: 0.735010]\n",
      "epoch:7 step:7370 [D loss: 0.706919, acc.: 51.56%] [G loss: 0.801942]\n",
      "epoch:7 step:7371 [D loss: 0.717777, acc.: 44.53%] [G loss: 0.743723]\n",
      "epoch:7 step:7372 [D loss: 0.687793, acc.: 47.66%] [G loss: 0.817765]\n",
      "epoch:7 step:7373 [D loss: 0.670983, acc.: 59.38%] [G loss: 0.814024]\n",
      "epoch:7 step:7374 [D loss: 0.673934, acc.: 57.03%] [G loss: 0.769498]\n",
      "epoch:7 step:7375 [D loss: 0.710732, acc.: 46.09%] [G loss: 0.790219]\n",
      "epoch:7 step:7376 [D loss: 0.734296, acc.: 43.75%] [G loss: 0.745667]\n",
      "epoch:7 step:7377 [D loss: 0.734135, acc.: 44.53%] [G loss: 0.768520]\n",
      "epoch:7 step:7378 [D loss: 0.683124, acc.: 55.47%] [G loss: 0.860479]\n",
      "epoch:7 step:7379 [D loss: 0.664020, acc.: 58.59%] [G loss: 0.797460]\n",
      "epoch:7 step:7380 [D loss: 0.672731, acc.: 62.50%] [G loss: 0.813371]\n",
      "epoch:7 step:7381 [D loss: 0.694262, acc.: 51.56%] [G loss: 0.834739]\n",
      "epoch:7 step:7382 [D loss: 0.673708, acc.: 56.25%] [G loss: 0.816161]\n",
      "epoch:7 step:7383 [D loss: 0.684915, acc.: 60.94%] [G loss: 0.799824]\n",
      "epoch:7 step:7384 [D loss: 0.671999, acc.: 57.03%] [G loss: 0.853854]\n",
      "epoch:7 step:7385 [D loss: 0.705999, acc.: 52.34%] [G loss: 0.783109]\n",
      "epoch:7 step:7386 [D loss: 0.668895, acc.: 57.03%] [G loss: 0.903730]\n",
      "epoch:7 step:7387 [D loss: 0.687978, acc.: 53.91%] [G loss: 0.847468]\n",
      "epoch:7 step:7388 [D loss: 0.687879, acc.: 53.91%] [G loss: 0.803471]\n",
      "epoch:7 step:7389 [D loss: 0.693413, acc.: 39.84%] [G loss: 0.792931]\n",
      "epoch:7 step:7390 [D loss: 0.683508, acc.: 59.38%] [G loss: 0.768668]\n",
      "epoch:7 step:7391 [D loss: 0.699708, acc.: 53.12%] [G loss: 0.857069]\n",
      "epoch:7 step:7392 [D loss: 0.679574, acc.: 50.78%] [G loss: 0.841633]\n",
      "epoch:7 step:7393 [D loss: 0.661580, acc.: 57.03%] [G loss: 0.823132]\n",
      "epoch:7 step:7394 [D loss: 0.678614, acc.: 51.56%] [G loss: 0.792788]\n",
      "epoch:7 step:7395 [D loss: 0.692122, acc.: 51.56%] [G loss: 0.816508]\n",
      "epoch:7 step:7396 [D loss: 0.665219, acc.: 59.38%] [G loss: 0.845034]\n",
      "epoch:7 step:7397 [D loss: 0.688559, acc.: 46.09%] [G loss: 0.821038]\n",
      "epoch:7 step:7398 [D loss: 0.651972, acc.: 63.28%] [G loss: 0.838646]\n",
      "epoch:7 step:7399 [D loss: 0.672916, acc.: 53.12%] [G loss: 0.828980]\n",
      "epoch:7 step:7400 [D loss: 0.690398, acc.: 50.00%] [G loss: 0.872067]\n",
      "epoch:7 step:7401 [D loss: 0.691465, acc.: 57.81%] [G loss: 0.750356]\n",
      "epoch:7 step:7402 [D loss: 0.712804, acc.: 46.09%] [G loss: 0.806754]\n",
      "epoch:7 step:7403 [D loss: 0.674601, acc.: 53.12%] [G loss: 0.832135]\n",
      "epoch:7 step:7404 [D loss: 0.719979, acc.: 41.41%] [G loss: 0.794887]\n",
      "epoch:7 step:7405 [D loss: 0.708102, acc.: 48.44%] [G loss: 0.766156]\n",
      "epoch:7 step:7406 [D loss: 0.691962, acc.: 51.56%] [G loss: 0.745044]\n",
      "epoch:7 step:7407 [D loss: 0.675210, acc.: 55.47%] [G loss: 0.756758]\n",
      "epoch:7 step:7408 [D loss: 0.661595, acc.: 60.94%] [G loss: 0.727769]\n",
      "epoch:7 step:7409 [D loss: 0.709315, acc.: 53.12%] [G loss: 0.705258]\n",
      "epoch:7 step:7410 [D loss: 0.706270, acc.: 53.91%] [G loss: 0.726431]\n",
      "epoch:7 step:7411 [D loss: 0.685128, acc.: 61.72%] [G loss: 0.742808]\n",
      "epoch:7 step:7412 [D loss: 0.697779, acc.: 48.44%] [G loss: 0.748936]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7413 [D loss: 0.654458, acc.: 64.84%] [G loss: 0.810533]\n",
      "epoch:7 step:7414 [D loss: 0.709976, acc.: 46.88%] [G loss: 0.815625]\n",
      "epoch:7 step:7415 [D loss: 0.673546, acc.: 53.12%] [G loss: 0.788608]\n",
      "epoch:7 step:7416 [D loss: 0.688297, acc.: 54.69%] [G loss: 0.783015]\n",
      "epoch:7 step:7417 [D loss: 0.692722, acc.: 50.78%] [G loss: 0.778559]\n",
      "epoch:7 step:7418 [D loss: 0.702528, acc.: 48.44%] [G loss: 0.751336]\n",
      "epoch:7 step:7419 [D loss: 0.707450, acc.: 45.31%] [G loss: 1.067880]\n",
      "epoch:7 step:7420 [D loss: 0.684007, acc.: 60.94%] [G loss: 0.814624]\n",
      "epoch:7 step:7421 [D loss: 0.706188, acc.: 48.44%] [G loss: 0.812655]\n",
      "epoch:7 step:7422 [D loss: 0.676366, acc.: 54.69%] [G loss: 0.828738]\n",
      "epoch:7 step:7423 [D loss: 0.671800, acc.: 55.47%] [G loss: 0.901350]\n",
      "epoch:7 step:7424 [D loss: 0.684581, acc.: 60.94%] [G loss: 0.740338]\n",
      "epoch:7 step:7425 [D loss: 0.685932, acc.: 54.69%] [G loss: 0.806981]\n",
      "epoch:7 step:7426 [D loss: 0.704940, acc.: 56.25%] [G loss: 0.815998]\n",
      "epoch:7 step:7427 [D loss: 0.648257, acc.: 59.38%] [G loss: 0.754616]\n",
      "epoch:7 step:7428 [D loss: 0.709643, acc.: 53.12%] [G loss: 0.765288]\n",
      "epoch:7 step:7429 [D loss: 0.681058, acc.: 60.16%] [G loss: 0.779059]\n",
      "epoch:7 step:7430 [D loss: 0.691363, acc.: 57.03%] [G loss: 0.779635]\n",
      "epoch:7 step:7431 [D loss: 0.668334, acc.: 56.25%] [G loss: 0.770168]\n",
      "epoch:7 step:7432 [D loss: 0.664108, acc.: 55.47%] [G loss: 0.810562]\n",
      "epoch:7 step:7433 [D loss: 0.682667, acc.: 56.25%] [G loss: 0.815972]\n",
      "epoch:7 step:7434 [D loss: 0.701875, acc.: 50.78%] [G loss: 0.756256]\n",
      "epoch:7 step:7435 [D loss: 0.658180, acc.: 57.81%] [G loss: 0.782980]\n",
      "epoch:7 step:7436 [D loss: 0.665990, acc.: 59.38%] [G loss: 0.816017]\n",
      "epoch:7 step:7437 [D loss: 0.726424, acc.: 47.66%] [G loss: 0.758624]\n",
      "epoch:7 step:7438 [D loss: 0.663550, acc.: 58.59%] [G loss: 0.781320]\n",
      "epoch:7 step:7439 [D loss: 0.689435, acc.: 57.03%] [G loss: 0.707015]\n",
      "epoch:7 step:7440 [D loss: 0.685886, acc.: 54.69%] [G loss: 0.755522]\n",
      "epoch:7 step:7441 [D loss: 0.666366, acc.: 60.94%] [G loss: 0.847384]\n",
      "epoch:7 step:7442 [D loss: 0.662929, acc.: 59.38%] [G loss: 0.759384]\n",
      "epoch:7 step:7443 [D loss: 0.655396, acc.: 58.59%] [G loss: 0.840486]\n",
      "epoch:7 step:7444 [D loss: 0.716896, acc.: 47.66%] [G loss: 0.769773]\n",
      "epoch:7 step:7445 [D loss: 0.707366, acc.: 50.78%] [G loss: 0.755005]\n",
      "epoch:7 step:7446 [D loss: 0.656325, acc.: 64.84%] [G loss: 0.806124]\n",
      "epoch:7 step:7447 [D loss: 0.652213, acc.: 67.97%] [G loss: 0.789599]\n",
      "epoch:7 step:7448 [D loss: 0.692385, acc.: 57.03%] [G loss: 0.823187]\n",
      "epoch:7 step:7449 [D loss: 0.680025, acc.: 53.91%] [G loss: 0.785956]\n",
      "epoch:7 step:7450 [D loss: 0.709324, acc.: 46.88%] [G loss: 0.786893]\n",
      "epoch:7 step:7451 [D loss: 0.696434, acc.: 50.78%] [G loss: 0.844092]\n",
      "epoch:7 step:7452 [D loss: 0.707050, acc.: 53.12%] [G loss: 0.792174]\n",
      "epoch:7 step:7453 [D loss: 0.693266, acc.: 53.12%] [G loss: 0.856863]\n",
      "epoch:7 step:7454 [D loss: 0.674253, acc.: 61.72%] [G loss: 0.810213]\n",
      "epoch:7 step:7455 [D loss: 0.686619, acc.: 60.94%] [G loss: 0.811543]\n",
      "epoch:7 step:7456 [D loss: 0.709456, acc.: 41.41%] [G loss: 0.766619]\n",
      "epoch:7 step:7457 [D loss: 0.663168, acc.: 58.59%] [G loss: 0.767550]\n",
      "epoch:7 step:7458 [D loss: 0.689179, acc.: 55.47%] [G loss: 0.836326]\n",
      "epoch:7 step:7459 [D loss: 0.698334, acc.: 51.56%] [G loss: 0.804611]\n",
      "epoch:7 step:7460 [D loss: 0.683446, acc.: 53.12%] [G loss: 0.766287]\n",
      "epoch:7 step:7461 [D loss: 0.676123, acc.: 60.94%] [G loss: 0.799863]\n",
      "epoch:7 step:7462 [D loss: 0.712286, acc.: 53.12%] [G loss: 0.789757]\n",
      "epoch:7 step:7463 [D loss: 0.691162, acc.: 53.91%] [G loss: 0.797817]\n",
      "epoch:7 step:7464 [D loss: 0.696041, acc.: 53.12%] [G loss: 0.790188]\n",
      "epoch:7 step:7465 [D loss: 0.677493, acc.: 56.25%] [G loss: 0.750133]\n",
      "epoch:7 step:7466 [D loss: 0.692763, acc.: 56.25%] [G loss: 0.763098]\n",
      "epoch:7 step:7467 [D loss: 0.711091, acc.: 42.19%] [G loss: 0.744996]\n",
      "epoch:7 step:7468 [D loss: 0.681968, acc.: 50.00%] [G loss: 0.763146]\n",
      "epoch:7 step:7469 [D loss: 0.697835, acc.: 48.44%] [G loss: 0.797650]\n",
      "epoch:7 step:7470 [D loss: 0.673742, acc.: 56.25%] [G loss: 0.737954]\n",
      "epoch:7 step:7471 [D loss: 0.692008, acc.: 47.66%] [G loss: 0.801798]\n",
      "epoch:7 step:7472 [D loss: 0.660536, acc.: 57.03%] [G loss: 0.762215]\n",
      "epoch:7 step:7473 [D loss: 0.686058, acc.: 52.34%] [G loss: 0.824224]\n",
      "epoch:7 step:7474 [D loss: 0.692537, acc.: 52.34%] [G loss: 0.789763]\n",
      "epoch:7 step:7475 [D loss: 0.660996, acc.: 62.50%] [G loss: 0.833172]\n",
      "epoch:7 step:7476 [D loss: 0.688017, acc.: 50.78%] [G loss: 0.810302]\n",
      "epoch:7 step:7477 [D loss: 0.663263, acc.: 59.38%] [G loss: 0.835283]\n",
      "epoch:7 step:7478 [D loss: 0.684041, acc.: 59.38%] [G loss: 0.902286]\n",
      "epoch:7 step:7479 [D loss: 0.680170, acc.: 58.59%] [G loss: 0.793656]\n",
      "epoch:7 step:7480 [D loss: 0.687579, acc.: 57.81%] [G loss: 0.805732]\n",
      "epoch:7 step:7481 [D loss: 0.680654, acc.: 51.56%] [G loss: 0.849135]\n",
      "epoch:7 step:7482 [D loss: 0.682838, acc.: 57.81%] [G loss: 0.765829]\n",
      "epoch:7 step:7483 [D loss: 0.643611, acc.: 64.06%] [G loss: 0.732412]\n",
      "epoch:7 step:7484 [D loss: 0.636069, acc.: 62.50%] [G loss: 0.771720]\n",
      "epoch:7 step:7485 [D loss: 0.678448, acc.: 59.38%] [G loss: 0.751072]\n",
      "epoch:7 step:7486 [D loss: 0.671170, acc.: 51.56%] [G loss: 0.724017]\n",
      "epoch:7 step:7487 [D loss: 0.663758, acc.: 60.94%] [G loss: 0.761629]\n",
      "epoch:7 step:7488 [D loss: 0.664780, acc.: 56.25%] [G loss: 0.790956]\n",
      "epoch:7 step:7489 [D loss: 0.697546, acc.: 53.91%] [G loss: 0.781584]\n",
      "epoch:7 step:7490 [D loss: 0.700122, acc.: 57.03%] [G loss: 0.829958]\n",
      "epoch:7 step:7491 [D loss: 0.687312, acc.: 57.81%] [G loss: 0.840641]\n",
      "epoch:7 step:7492 [D loss: 0.650754, acc.: 58.59%] [G loss: 0.775818]\n",
      "epoch:7 step:7493 [D loss: 0.673447, acc.: 53.12%] [G loss: 0.789788]\n",
      "epoch:7 step:7494 [D loss: 0.688945, acc.: 55.47%] [G loss: 0.818012]\n",
      "epoch:7 step:7495 [D loss: 0.670851, acc.: 58.59%] [G loss: 0.779983]\n",
      "epoch:7 step:7496 [D loss: 0.762028, acc.: 39.84%] [G loss: 0.718772]\n",
      "epoch:8 step:7497 [D loss: 0.732457, acc.: 50.00%] [G loss: 0.771022]\n",
      "epoch:8 step:7498 [D loss: 0.700195, acc.: 45.31%] [G loss: 0.782351]\n",
      "epoch:8 step:7499 [D loss: 0.699383, acc.: 44.53%] [G loss: 0.731446]\n",
      "epoch:8 step:7500 [D loss: 0.720994, acc.: 48.44%] [G loss: 0.815453]\n",
      "epoch:8 step:7501 [D loss: 0.676722, acc.: 50.78%] [G loss: 0.784062]\n",
      "epoch:8 step:7502 [D loss: 0.676162, acc.: 62.50%] [G loss: 0.847158]\n",
      "epoch:8 step:7503 [D loss: 0.651059, acc.: 63.28%] [G loss: 0.813471]\n",
      "epoch:8 step:7504 [D loss: 0.693731, acc.: 51.56%] [G loss: 0.796396]\n",
      "epoch:8 step:7505 [D loss: 0.691588, acc.: 54.69%] [G loss: 0.734217]\n",
      "epoch:8 step:7506 [D loss: 0.668026, acc.: 58.59%] [G loss: 0.731450]\n",
      "epoch:8 step:7507 [D loss: 0.708921, acc.: 46.88%] [G loss: 0.737076]\n",
      "epoch:8 step:7508 [D loss: 0.691031, acc.: 57.03%] [G loss: 0.777126]\n",
      "epoch:8 step:7509 [D loss: 0.713458, acc.: 46.09%] [G loss: 0.798092]\n",
      "epoch:8 step:7510 [D loss: 0.692100, acc.: 49.22%] [G loss: 0.783051]\n",
      "epoch:8 step:7511 [D loss: 0.716157, acc.: 50.78%] [G loss: 0.760931]\n",
      "epoch:8 step:7512 [D loss: 0.695117, acc.: 46.88%] [G loss: 0.771705]\n",
      "epoch:8 step:7513 [D loss: 0.691874, acc.: 50.00%] [G loss: 0.784571]\n",
      "epoch:8 step:7514 [D loss: 0.693302, acc.: 50.78%] [G loss: 0.766136]\n",
      "epoch:8 step:7515 [D loss: 0.718763, acc.: 45.31%] [G loss: 0.798005]\n",
      "epoch:8 step:7516 [D loss: 0.660523, acc.: 71.09%] [G loss: 0.733090]\n",
      "epoch:8 step:7517 [D loss: 0.687754, acc.: 57.03%] [G loss: 0.773179]\n",
      "epoch:8 step:7518 [D loss: 0.699498, acc.: 53.12%] [G loss: 0.770554]\n",
      "epoch:8 step:7519 [D loss: 0.664217, acc.: 60.16%] [G loss: 0.778549]\n",
      "epoch:8 step:7520 [D loss: 0.653174, acc.: 62.50%] [G loss: 0.846526]\n",
      "epoch:8 step:7521 [D loss: 0.673353, acc.: 54.69%] [G loss: 0.766294]\n",
      "epoch:8 step:7522 [D loss: 0.687977, acc.: 58.59%] [G loss: 0.789345]\n",
      "epoch:8 step:7523 [D loss: 0.676718, acc.: 59.38%] [G loss: 0.740995]\n",
      "epoch:8 step:7524 [D loss: 0.717421, acc.: 53.12%] [G loss: 0.761098]\n",
      "epoch:8 step:7525 [D loss: 0.681854, acc.: 61.72%] [G loss: 0.824865]\n",
      "epoch:8 step:7526 [D loss: 0.664638, acc.: 60.16%] [G loss: 0.830696]\n",
      "epoch:8 step:7527 [D loss: 0.684300, acc.: 52.34%] [G loss: 0.810845]\n",
      "epoch:8 step:7528 [D loss: 0.722286, acc.: 44.53%] [G loss: 0.796392]\n",
      "epoch:8 step:7529 [D loss: 0.682397, acc.: 53.91%] [G loss: 0.826147]\n",
      "epoch:8 step:7530 [D loss: 0.687161, acc.: 50.78%] [G loss: 0.777485]\n",
      "epoch:8 step:7531 [D loss: 0.678971, acc.: 52.34%] [G loss: 0.770268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7532 [D loss: 0.683098, acc.: 59.38%] [G loss: 0.799903]\n",
      "epoch:8 step:7533 [D loss: 0.697126, acc.: 50.00%] [G loss: 0.777055]\n",
      "epoch:8 step:7534 [D loss: 0.682750, acc.: 51.56%] [G loss: 0.789880]\n",
      "epoch:8 step:7535 [D loss: 0.669285, acc.: 56.25%] [G loss: 0.805944]\n",
      "epoch:8 step:7536 [D loss: 0.674506, acc.: 60.16%] [G loss: 0.766265]\n",
      "epoch:8 step:7537 [D loss: 0.664531, acc.: 57.03%] [G loss: 0.789229]\n",
      "epoch:8 step:7538 [D loss: 0.675945, acc.: 57.03%] [G loss: 0.779830]\n",
      "epoch:8 step:7539 [D loss: 0.668661, acc.: 53.91%] [G loss: 0.778029]\n",
      "epoch:8 step:7540 [D loss: 0.705678, acc.: 51.56%] [G loss: 0.752341]\n",
      "epoch:8 step:7541 [D loss: 0.702737, acc.: 53.12%] [G loss: 0.772321]\n",
      "epoch:8 step:7542 [D loss: 0.683370, acc.: 50.00%] [G loss: 0.758261]\n",
      "epoch:8 step:7543 [D loss: 0.704753, acc.: 50.00%] [G loss: 0.754071]\n",
      "epoch:8 step:7544 [D loss: 0.694013, acc.: 53.91%] [G loss: 0.732573]\n",
      "epoch:8 step:7545 [D loss: 0.650855, acc.: 67.97%] [G loss: 0.789297]\n",
      "epoch:8 step:7546 [D loss: 0.678992, acc.: 53.91%] [G loss: 0.769087]\n",
      "epoch:8 step:7547 [D loss: 0.662653, acc.: 57.81%] [G loss: 0.758370]\n",
      "epoch:8 step:7548 [D loss: 0.674251, acc.: 56.25%] [G loss: 0.776475]\n",
      "epoch:8 step:7549 [D loss: 0.655941, acc.: 54.69%] [G loss: 0.770133]\n",
      "epoch:8 step:7550 [D loss: 0.719553, acc.: 44.53%] [G loss: 0.800666]\n",
      "epoch:8 step:7551 [D loss: 0.708020, acc.: 55.47%] [G loss: 0.781930]\n",
      "epoch:8 step:7552 [D loss: 0.665527, acc.: 55.47%] [G loss: 0.768872]\n",
      "epoch:8 step:7553 [D loss: 0.672374, acc.: 57.81%] [G loss: 0.752982]\n",
      "epoch:8 step:7554 [D loss: 0.702116, acc.: 54.69%] [G loss: 0.796115]\n",
      "epoch:8 step:7555 [D loss: 0.663038, acc.: 60.16%] [G loss: 0.820460]\n",
      "epoch:8 step:7556 [D loss: 0.697113, acc.: 57.03%] [G loss: 0.831116]\n",
      "epoch:8 step:7557 [D loss: 0.669153, acc.: 61.72%] [G loss: 0.827059]\n",
      "epoch:8 step:7558 [D loss: 0.611006, acc.: 71.88%] [G loss: 0.825284]\n",
      "epoch:8 step:7559 [D loss: 0.693708, acc.: 53.91%] [G loss: 1.004312]\n",
      "epoch:8 step:7560 [D loss: 0.730494, acc.: 46.09%] [G loss: 0.801482]\n",
      "epoch:8 step:7561 [D loss: 0.708255, acc.: 53.12%] [G loss: 0.752290]\n",
      "epoch:8 step:7562 [D loss: 0.698690, acc.: 56.25%] [G loss: 0.837845]\n",
      "epoch:8 step:7563 [D loss: 0.691783, acc.: 55.47%] [G loss: 0.837121]\n",
      "epoch:8 step:7564 [D loss: 0.674568, acc.: 56.25%] [G loss: 0.798684]\n",
      "epoch:8 step:7565 [D loss: 0.649818, acc.: 61.72%] [G loss: 0.806764]\n",
      "epoch:8 step:7566 [D loss: 0.698279, acc.: 50.78%] [G loss: 0.826533]\n",
      "epoch:8 step:7567 [D loss: 0.702749, acc.: 53.91%] [G loss: 0.785196]\n",
      "epoch:8 step:7568 [D loss: 0.694001, acc.: 51.56%] [G loss: 0.729660]\n",
      "epoch:8 step:7569 [D loss: 0.703154, acc.: 47.66%] [G loss: 0.726308]\n",
      "epoch:8 step:7570 [D loss: 0.694904, acc.: 47.66%] [G loss: 0.761062]\n",
      "epoch:8 step:7571 [D loss: 0.687204, acc.: 52.34%] [G loss: 0.759847]\n",
      "epoch:8 step:7572 [D loss: 0.662735, acc.: 64.06%] [G loss: 0.804626]\n",
      "epoch:8 step:7573 [D loss: 0.671459, acc.: 61.72%] [G loss: 0.806284]\n",
      "epoch:8 step:7574 [D loss: 0.697988, acc.: 54.69%] [G loss: 0.785472]\n",
      "epoch:8 step:7575 [D loss: 0.695722, acc.: 56.25%] [G loss: 0.776300]\n",
      "epoch:8 step:7576 [D loss: 0.681893, acc.: 55.47%] [G loss: 0.810065]\n",
      "epoch:8 step:7577 [D loss: 0.663038, acc.: 60.16%] [G loss: 0.796756]\n",
      "epoch:8 step:7578 [D loss: 0.674837, acc.: 63.28%] [G loss: 0.781265]\n",
      "epoch:8 step:7579 [D loss: 0.671200, acc.: 54.69%] [G loss: 0.762764]\n",
      "epoch:8 step:7580 [D loss: 0.697670, acc.: 49.22%] [G loss: 0.787805]\n",
      "epoch:8 step:7581 [D loss: 0.685697, acc.: 48.44%] [G loss: 0.741078]\n",
      "epoch:8 step:7582 [D loss: 0.703719, acc.: 51.56%] [G loss: 0.755154]\n",
      "epoch:8 step:7583 [D loss: 0.675200, acc.: 57.81%] [G loss: 0.736927]\n",
      "epoch:8 step:7584 [D loss: 0.703698, acc.: 46.88%] [G loss: 0.760923]\n",
      "epoch:8 step:7585 [D loss: 0.717200, acc.: 46.09%] [G loss: 0.788875]\n",
      "epoch:8 step:7586 [D loss: 0.686584, acc.: 51.56%] [G loss: 0.786915]\n",
      "epoch:8 step:7587 [D loss: 0.696905, acc.: 53.12%] [G loss: 0.791640]\n",
      "epoch:8 step:7588 [D loss: 0.689385, acc.: 51.56%] [G loss: 0.816856]\n",
      "epoch:8 step:7589 [D loss: 0.630943, acc.: 64.06%] [G loss: 0.740217]\n",
      "epoch:8 step:7590 [D loss: 0.670230, acc.: 55.47%] [G loss: 0.764316]\n",
      "epoch:8 step:7591 [D loss: 0.723783, acc.: 40.62%] [G loss: 0.749345]\n",
      "epoch:8 step:7592 [D loss: 0.671195, acc.: 54.69%] [G loss: 0.832556]\n",
      "epoch:8 step:7593 [D loss: 0.677270, acc.: 52.34%] [G loss: 0.706429]\n",
      "epoch:8 step:7594 [D loss: 0.716238, acc.: 51.56%] [G loss: 0.768103]\n",
      "epoch:8 step:7595 [D loss: 0.679979, acc.: 54.69%] [G loss: 0.782271]\n",
      "epoch:8 step:7596 [D loss: 0.696623, acc.: 53.12%] [G loss: 0.768877]\n",
      "epoch:8 step:7597 [D loss: 0.679300, acc.: 53.12%] [G loss: 0.799701]\n",
      "epoch:8 step:7598 [D loss: 0.686545, acc.: 54.69%] [G loss: 0.796556]\n",
      "epoch:8 step:7599 [D loss: 0.673472, acc.: 51.56%] [G loss: 0.802767]\n",
      "epoch:8 step:7600 [D loss: 0.698727, acc.: 46.88%] [G loss: 0.764025]\n",
      "epoch:8 step:7601 [D loss: 0.696870, acc.: 53.91%] [G loss: 0.790558]\n",
      "epoch:8 step:7602 [D loss: 0.696339, acc.: 53.91%] [G loss: 0.753695]\n",
      "epoch:8 step:7603 [D loss: 0.682752, acc.: 56.25%] [G loss: 0.785730]\n",
      "epoch:8 step:7604 [D loss: 0.682999, acc.: 55.47%] [G loss: 0.711127]\n",
      "epoch:8 step:7605 [D loss: 0.680861, acc.: 53.91%] [G loss: 0.743173]\n",
      "epoch:8 step:7606 [D loss: 0.691623, acc.: 50.78%] [G loss: 0.734388]\n",
      "epoch:8 step:7607 [D loss: 0.677013, acc.: 53.91%] [G loss: 0.763935]\n",
      "epoch:8 step:7608 [D loss: 0.708228, acc.: 45.31%] [G loss: 0.722449]\n",
      "epoch:8 step:7609 [D loss: 0.720410, acc.: 52.34%] [G loss: 0.736772]\n",
      "epoch:8 step:7610 [D loss: 0.679135, acc.: 60.16%] [G loss: 0.750783]\n",
      "epoch:8 step:7611 [D loss: 0.710974, acc.: 47.66%] [G loss: 0.762379]\n",
      "epoch:8 step:7612 [D loss: 0.696329, acc.: 52.34%] [G loss: 0.766585]\n",
      "epoch:8 step:7613 [D loss: 0.666861, acc.: 55.47%] [G loss: 0.761443]\n",
      "epoch:8 step:7614 [D loss: 0.664212, acc.: 55.47%] [G loss: 0.791936]\n",
      "epoch:8 step:7615 [D loss: 0.663581, acc.: 64.84%] [G loss: 0.780484]\n",
      "epoch:8 step:7616 [D loss: 0.650761, acc.: 58.59%] [G loss: 0.754832]\n",
      "epoch:8 step:7617 [D loss: 0.646025, acc.: 61.72%] [G loss: 0.736521]\n",
      "epoch:8 step:7618 [D loss: 0.678867, acc.: 59.38%] [G loss: 0.763261]\n",
      "epoch:8 step:7619 [D loss: 0.698391, acc.: 55.47%] [G loss: 0.724665]\n",
      "epoch:8 step:7620 [D loss: 0.693186, acc.: 50.00%] [G loss: 0.737784]\n",
      "epoch:8 step:7621 [D loss: 0.704224, acc.: 53.91%] [G loss: 0.776363]\n",
      "epoch:8 step:7622 [D loss: 0.730712, acc.: 34.38%] [G loss: 0.811523]\n",
      "epoch:8 step:7623 [D loss: 0.695498, acc.: 50.78%] [G loss: 0.783372]\n",
      "epoch:8 step:7624 [D loss: 0.721092, acc.: 50.78%] [G loss: 0.796766]\n",
      "epoch:8 step:7625 [D loss: 0.669641, acc.: 64.84%] [G loss: 0.824570]\n",
      "epoch:8 step:7626 [D loss: 0.691881, acc.: 50.78%] [G loss: 0.822671]\n",
      "epoch:8 step:7627 [D loss: 0.693555, acc.: 55.47%] [G loss: 0.774851]\n",
      "epoch:8 step:7628 [D loss: 0.690773, acc.: 52.34%] [G loss: 0.768749]\n",
      "epoch:8 step:7629 [D loss: 0.693927, acc.: 46.88%] [G loss: 0.793408]\n",
      "epoch:8 step:7630 [D loss: 0.691063, acc.: 50.00%] [G loss: 0.771635]\n",
      "epoch:8 step:7631 [D loss: 0.699632, acc.: 48.44%] [G loss: 0.777635]\n",
      "epoch:8 step:7632 [D loss: 0.720565, acc.: 48.44%] [G loss: 0.828681]\n",
      "epoch:8 step:7633 [D loss: 0.675294, acc.: 57.03%] [G loss: 0.744389]\n",
      "epoch:8 step:7634 [D loss: 0.674148, acc.: 57.81%] [G loss: 0.731616]\n",
      "epoch:8 step:7635 [D loss: 0.678267, acc.: 59.38%] [G loss: 0.822672]\n",
      "epoch:8 step:7636 [D loss: 0.677824, acc.: 57.81%] [G loss: 0.771533]\n",
      "epoch:8 step:7637 [D loss: 0.711009, acc.: 48.44%] [G loss: 0.730399]\n",
      "epoch:8 step:7638 [D loss: 0.696285, acc.: 48.44%] [G loss: 0.742032]\n",
      "epoch:8 step:7639 [D loss: 0.701764, acc.: 45.31%] [G loss: 0.719529]\n",
      "epoch:8 step:7640 [D loss: 0.679905, acc.: 59.38%] [G loss: 0.759868]\n",
      "epoch:8 step:7641 [D loss: 0.694632, acc.: 50.78%] [G loss: 0.742603]\n",
      "epoch:8 step:7642 [D loss: 0.683871, acc.: 53.12%] [G loss: 0.752401]\n",
      "epoch:8 step:7643 [D loss: 0.692546, acc.: 48.44%] [G loss: 0.743633]\n",
      "epoch:8 step:7644 [D loss: 0.642693, acc.: 65.62%] [G loss: 0.775210]\n",
      "epoch:8 step:7645 [D loss: 0.654584, acc.: 62.50%] [G loss: 0.774450]\n",
      "epoch:8 step:7646 [D loss: 0.684126, acc.: 50.78%] [G loss: 0.799941]\n",
      "epoch:8 step:7647 [D loss: 0.711074, acc.: 45.31%] [G loss: 0.806654]\n",
      "epoch:8 step:7648 [D loss: 0.685943, acc.: 60.16%] [G loss: 0.753354]\n",
      "epoch:8 step:7649 [D loss: 0.712083, acc.: 50.00%] [G loss: 0.825323]\n",
      "epoch:8 step:7650 [D loss: 0.687413, acc.: 52.34%] [G loss: 0.787685]\n",
      "epoch:8 step:7651 [D loss: 0.669292, acc.: 53.12%] [G loss: 0.761740]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7652 [D loss: 0.690907, acc.: 54.69%] [G loss: 0.787113]\n",
      "epoch:8 step:7653 [D loss: 0.676747, acc.: 59.38%] [G loss: 0.757869]\n",
      "epoch:8 step:7654 [D loss: 0.686305, acc.: 50.78%] [G loss: 0.787531]\n",
      "epoch:8 step:7655 [D loss: 0.716238, acc.: 48.44%] [G loss: 0.774733]\n",
      "epoch:8 step:7656 [D loss: 0.699048, acc.: 53.91%] [G loss: 0.762332]\n",
      "epoch:8 step:7657 [D loss: 0.684242, acc.: 52.34%] [G loss: 0.791270]\n",
      "epoch:8 step:7658 [D loss: 0.683604, acc.: 50.78%] [G loss: 0.825069]\n",
      "epoch:8 step:7659 [D loss: 0.684150, acc.: 51.56%] [G loss: 0.802521]\n",
      "epoch:8 step:7660 [D loss: 0.695802, acc.: 50.00%] [G loss: 0.773635]\n",
      "epoch:8 step:7661 [D loss: 0.688617, acc.: 64.06%] [G loss: 0.765379]\n",
      "epoch:8 step:7662 [D loss: 0.668351, acc.: 58.59%] [G loss: 0.751619]\n",
      "epoch:8 step:7663 [D loss: 0.676299, acc.: 57.81%] [G loss: 0.757909]\n",
      "epoch:8 step:7664 [D loss: 0.686271, acc.: 53.91%] [G loss: 0.809786]\n",
      "epoch:8 step:7665 [D loss: 0.672266, acc.: 56.25%] [G loss: 0.820105]\n",
      "epoch:8 step:7666 [D loss: 0.675639, acc.: 60.94%] [G loss: 0.798366]\n",
      "epoch:8 step:7667 [D loss: 0.710404, acc.: 47.66%] [G loss: 0.761881]\n",
      "epoch:8 step:7668 [D loss: 0.698261, acc.: 55.47%] [G loss: 0.758738]\n",
      "epoch:8 step:7669 [D loss: 0.683927, acc.: 55.47%] [G loss: 0.789031]\n",
      "epoch:8 step:7670 [D loss: 0.687289, acc.: 56.25%] [G loss: 0.797904]\n",
      "epoch:8 step:7671 [D loss: 0.676248, acc.: 57.03%] [G loss: 0.779439]\n",
      "epoch:8 step:7672 [D loss: 0.715793, acc.: 49.22%] [G loss: 0.795559]\n",
      "epoch:8 step:7673 [D loss: 0.662661, acc.: 57.81%] [G loss: 0.769664]\n",
      "epoch:8 step:7674 [D loss: 0.696138, acc.: 57.03%] [G loss: 0.794018]\n",
      "epoch:8 step:7675 [D loss: 0.683520, acc.: 59.38%] [G loss: 0.789812]\n",
      "epoch:8 step:7676 [D loss: 0.663643, acc.: 59.38%] [G loss: 0.892623]\n",
      "epoch:8 step:7677 [D loss: 0.698310, acc.: 53.91%] [G loss: 0.816336]\n",
      "epoch:8 step:7678 [D loss: 0.678327, acc.: 57.03%] [G loss: 0.799100]\n",
      "epoch:8 step:7679 [D loss: 0.683321, acc.: 53.12%] [G loss: 0.790851]\n",
      "epoch:8 step:7680 [D loss: 0.690682, acc.: 57.03%] [G loss: 0.751640]\n",
      "epoch:8 step:7681 [D loss: 0.704506, acc.: 50.78%] [G loss: 0.758195]\n",
      "epoch:8 step:7682 [D loss: 0.692769, acc.: 53.91%] [G loss: 0.746651]\n",
      "epoch:8 step:7683 [D loss: 0.699697, acc.: 51.56%] [G loss: 0.751735]\n",
      "epoch:8 step:7684 [D loss: 0.681434, acc.: 57.03%] [G loss: 0.826995]\n",
      "epoch:8 step:7685 [D loss: 0.680481, acc.: 53.91%] [G loss: 0.691462]\n",
      "epoch:8 step:7686 [D loss: 0.703989, acc.: 42.97%] [G loss: 0.756606]\n",
      "epoch:8 step:7687 [D loss: 0.699999, acc.: 47.66%] [G loss: 0.785265]\n",
      "epoch:8 step:7688 [D loss: 0.673401, acc.: 60.16%] [G loss: 0.757557]\n",
      "epoch:8 step:7689 [D loss: 0.670492, acc.: 58.59%] [G loss: 0.762107]\n",
      "epoch:8 step:7690 [D loss: 0.694058, acc.: 51.56%] [G loss: 0.762136]\n",
      "epoch:8 step:7691 [D loss: 0.687861, acc.: 52.34%] [G loss: 0.757881]\n",
      "epoch:8 step:7692 [D loss: 0.672840, acc.: 53.91%] [G loss: 0.786750]\n",
      "epoch:8 step:7693 [D loss: 0.673176, acc.: 57.03%] [G loss: 0.794097]\n",
      "epoch:8 step:7694 [D loss: 0.682983, acc.: 57.81%] [G loss: 0.787711]\n",
      "epoch:8 step:7695 [D loss: 0.690486, acc.: 52.34%] [G loss: 0.775736]\n",
      "epoch:8 step:7696 [D loss: 0.689704, acc.: 56.25%] [G loss: 0.733464]\n",
      "epoch:8 step:7697 [D loss: 0.686670, acc.: 58.59%] [G loss: 0.829623]\n",
      "epoch:8 step:7698 [D loss: 0.684016, acc.: 54.69%] [G loss: 0.825582]\n",
      "epoch:8 step:7699 [D loss: 0.628543, acc.: 70.31%] [G loss: 0.846728]\n",
      "epoch:8 step:7700 [D loss: 0.639629, acc.: 67.19%] [G loss: 0.819144]\n",
      "epoch:8 step:7701 [D loss: 0.648285, acc.: 61.72%] [G loss: 0.786354]\n",
      "epoch:8 step:7702 [D loss: 0.697192, acc.: 56.25%] [G loss: 0.767742]\n",
      "epoch:8 step:7703 [D loss: 0.673428, acc.: 63.28%] [G loss: 0.783846]\n",
      "epoch:8 step:7704 [D loss: 0.646494, acc.: 64.06%] [G loss: 0.749384]\n",
      "epoch:8 step:7705 [D loss: 0.697689, acc.: 47.66%] [G loss: 0.768407]\n",
      "epoch:8 step:7706 [D loss: 0.696982, acc.: 56.25%] [G loss: 0.771950]\n",
      "epoch:8 step:7707 [D loss: 0.658595, acc.: 60.16%] [G loss: 0.739162]\n",
      "epoch:8 step:7708 [D loss: 0.692342, acc.: 54.69%] [G loss: 0.723031]\n",
      "epoch:8 step:7709 [D loss: 0.727634, acc.: 46.09%] [G loss: 0.745118]\n",
      "epoch:8 step:7710 [D loss: 0.706756, acc.: 44.53%] [G loss: 0.788059]\n",
      "epoch:8 step:7711 [D loss: 0.680402, acc.: 57.81%] [G loss: 0.842787]\n",
      "epoch:8 step:7712 [D loss: 0.682936, acc.: 53.91%] [G loss: 0.856151]\n",
      "epoch:8 step:7713 [D loss: 0.671825, acc.: 59.38%] [G loss: 0.816903]\n",
      "epoch:8 step:7714 [D loss: 0.690375, acc.: 53.91%] [G loss: 0.757323]\n",
      "epoch:8 step:7715 [D loss: 0.654562, acc.: 64.84%] [G loss: 0.769150]\n",
      "epoch:8 step:7716 [D loss: 0.678094, acc.: 59.38%] [G loss: 0.831048]\n",
      "epoch:8 step:7717 [D loss: 0.693761, acc.: 53.12%] [G loss: 0.760976]\n",
      "epoch:8 step:7718 [D loss: 0.669495, acc.: 53.12%] [G loss: 0.820245]\n",
      "epoch:8 step:7719 [D loss: 0.705474, acc.: 47.66%] [G loss: 0.770813]\n",
      "epoch:8 step:7720 [D loss: 0.667948, acc.: 57.81%] [G loss: 0.749885]\n",
      "epoch:8 step:7721 [D loss: 0.726246, acc.: 50.00%] [G loss: 0.781207]\n",
      "epoch:8 step:7722 [D loss: 0.688687, acc.: 51.56%] [G loss: 0.757808]\n",
      "epoch:8 step:7723 [D loss: 0.683768, acc.: 50.78%] [G loss: 0.823440]\n",
      "epoch:8 step:7724 [D loss: 0.715648, acc.: 50.78%] [G loss: 0.817166]\n",
      "epoch:8 step:7725 [D loss: 0.674984, acc.: 56.25%] [G loss: 0.805873]\n",
      "epoch:8 step:7726 [D loss: 0.683419, acc.: 56.25%] [G loss: 0.768320]\n",
      "epoch:8 step:7727 [D loss: 0.693245, acc.: 53.12%] [G loss: 0.772597]\n",
      "epoch:8 step:7728 [D loss: 0.698970, acc.: 52.34%] [G loss: 0.715825]\n",
      "epoch:8 step:7729 [D loss: 0.687103, acc.: 53.91%] [G loss: 0.772419]\n",
      "epoch:8 step:7730 [D loss: 0.701075, acc.: 55.47%] [G loss: 0.785847]\n",
      "epoch:8 step:7731 [D loss: 0.707173, acc.: 47.66%] [G loss: 0.718882]\n",
      "epoch:8 step:7732 [D loss: 0.671425, acc.: 57.03%] [G loss: 0.787544]\n",
      "epoch:8 step:7733 [D loss: 0.682257, acc.: 58.59%] [G loss: 0.723704]\n",
      "epoch:8 step:7734 [D loss: 0.704097, acc.: 46.88%] [G loss: 0.761912]\n",
      "epoch:8 step:7735 [D loss: 0.675083, acc.: 55.47%] [G loss: 0.717776]\n",
      "epoch:8 step:7736 [D loss: 0.692535, acc.: 49.22%] [G loss: 0.778360]\n",
      "epoch:8 step:7737 [D loss: 0.659750, acc.: 61.72%] [G loss: 0.724303]\n",
      "epoch:8 step:7738 [D loss: 0.645860, acc.: 64.84%] [G loss: 0.705785]\n",
      "epoch:8 step:7739 [D loss: 0.657811, acc.: 58.59%] [G loss: 0.807151]\n",
      "epoch:8 step:7740 [D loss: 0.685053, acc.: 57.03%] [G loss: 0.775680]\n",
      "epoch:8 step:7741 [D loss: 0.680838, acc.: 58.59%] [G loss: 0.798351]\n",
      "epoch:8 step:7742 [D loss: 0.683404, acc.: 51.56%] [G loss: 0.735733]\n",
      "epoch:8 step:7743 [D loss: 0.676154, acc.: 57.03%] [G loss: 0.799333]\n",
      "epoch:8 step:7744 [D loss: 0.723049, acc.: 43.75%] [G loss: 0.746179]\n",
      "epoch:8 step:7745 [D loss: 0.684205, acc.: 62.50%] [G loss: 0.800496]\n",
      "epoch:8 step:7746 [D loss: 0.703551, acc.: 50.78%] [G loss: 0.863971]\n",
      "epoch:8 step:7747 [D loss: 0.666581, acc.: 58.59%] [G loss: 0.863857]\n",
      "epoch:8 step:7748 [D loss: 0.720235, acc.: 50.78%] [G loss: 0.871892]\n",
      "epoch:8 step:7749 [D loss: 0.670827, acc.: 54.69%] [G loss: 0.887917]\n",
      "epoch:8 step:7750 [D loss: 0.714906, acc.: 52.34%] [G loss: 1.043329]\n",
      "epoch:8 step:7751 [D loss: 0.690048, acc.: 53.91%] [G loss: 0.831192]\n",
      "epoch:8 step:7752 [D loss: 0.717285, acc.: 48.44%] [G loss: 0.851208]\n",
      "epoch:8 step:7753 [D loss: 0.682449, acc.: 59.38%] [G loss: 0.777921]\n",
      "epoch:8 step:7754 [D loss: 0.696868, acc.: 47.66%] [G loss: 0.761385]\n",
      "epoch:8 step:7755 [D loss: 0.664272, acc.: 61.72%] [G loss: 0.773845]\n",
      "epoch:8 step:7756 [D loss: 0.678884, acc.: 51.56%] [G loss: 0.818035]\n",
      "epoch:8 step:7757 [D loss: 0.709631, acc.: 49.22%] [G loss: 0.761961]\n",
      "epoch:8 step:7758 [D loss: 0.703758, acc.: 46.09%] [G loss: 0.761681]\n",
      "epoch:8 step:7759 [D loss: 0.686234, acc.: 51.56%] [G loss: 0.773909]\n",
      "epoch:8 step:7760 [D loss: 0.659336, acc.: 60.94%] [G loss: 0.808021]\n",
      "epoch:8 step:7761 [D loss: 0.663132, acc.: 60.16%] [G loss: 0.870874]\n",
      "epoch:8 step:7762 [D loss: 0.670268, acc.: 57.81%] [G loss: 0.722907]\n",
      "epoch:8 step:7763 [D loss: 0.674213, acc.: 58.59%] [G loss: 0.788171]\n",
      "epoch:8 step:7764 [D loss: 0.701937, acc.: 53.12%] [G loss: 0.758631]\n",
      "epoch:8 step:7765 [D loss: 0.686656, acc.: 60.16%] [G loss: 0.799333]\n",
      "epoch:8 step:7766 [D loss: 0.641710, acc.: 65.62%] [G loss: 0.833491]\n",
      "epoch:8 step:7767 [D loss: 0.679818, acc.: 58.59%] [G loss: 0.790304]\n",
      "epoch:8 step:7768 [D loss: 0.719790, acc.: 42.97%] [G loss: 0.756610]\n",
      "epoch:8 step:7769 [D loss: 0.658677, acc.: 60.94%] [G loss: 0.856362]\n",
      "epoch:8 step:7770 [D loss: 0.665964, acc.: 60.16%] [G loss: 0.836941]\n",
      "epoch:8 step:7771 [D loss: 0.674214, acc.: 56.25%] [G loss: 0.803449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7772 [D loss: 0.689453, acc.: 50.78%] [G loss: 0.824711]\n",
      "epoch:8 step:7773 [D loss: 0.695801, acc.: 52.34%] [G loss: 0.780899]\n",
      "epoch:8 step:7774 [D loss: 0.682725, acc.: 55.47%] [G loss: 0.767276]\n",
      "epoch:8 step:7775 [D loss: 0.718953, acc.: 47.66%] [G loss: 0.758520]\n",
      "epoch:8 step:7776 [D loss: 0.716881, acc.: 52.34%] [G loss: 0.785255]\n",
      "epoch:8 step:7777 [D loss: 0.702477, acc.: 56.25%] [G loss: 0.810225]\n",
      "epoch:8 step:7778 [D loss: 0.658165, acc.: 57.03%] [G loss: 0.840555]\n",
      "epoch:8 step:7779 [D loss: 0.675418, acc.: 61.72%] [G loss: 0.767012]\n",
      "epoch:8 step:7780 [D loss: 0.702922, acc.: 52.34%] [G loss: 0.850369]\n",
      "epoch:8 step:7781 [D loss: 0.675254, acc.: 53.91%] [G loss: 0.829392]\n",
      "epoch:8 step:7782 [D loss: 0.680954, acc.: 51.56%] [G loss: 0.777324]\n",
      "epoch:8 step:7783 [D loss: 0.697962, acc.: 47.66%] [G loss: 0.776800]\n",
      "epoch:8 step:7784 [D loss: 0.677363, acc.: 57.03%] [G loss: 0.830887]\n",
      "epoch:8 step:7785 [D loss: 0.706570, acc.: 45.31%] [G loss: 0.760348]\n",
      "epoch:8 step:7786 [D loss: 0.668692, acc.: 54.69%] [G loss: 0.760164]\n",
      "epoch:8 step:7787 [D loss: 0.665006, acc.: 57.81%] [G loss: 0.741646]\n",
      "epoch:8 step:7788 [D loss: 0.695924, acc.: 40.62%] [G loss: 0.785365]\n",
      "epoch:8 step:7789 [D loss: 0.665899, acc.: 57.03%] [G loss: 0.736361]\n",
      "epoch:8 step:7790 [D loss: 0.662880, acc.: 58.59%] [G loss: 0.754291]\n",
      "epoch:8 step:7791 [D loss: 0.674125, acc.: 57.03%] [G loss: 0.827615]\n",
      "epoch:8 step:7792 [D loss: 0.685280, acc.: 45.31%] [G loss: 0.741028]\n",
      "epoch:8 step:7793 [D loss: 0.654511, acc.: 56.25%] [G loss: 0.747619]\n",
      "epoch:8 step:7794 [D loss: 0.679391, acc.: 50.00%] [G loss: 0.826385]\n",
      "epoch:8 step:7795 [D loss: 0.679324, acc.: 57.03%] [G loss: 0.763952]\n",
      "epoch:8 step:7796 [D loss: 0.696526, acc.: 52.34%] [G loss: 0.805731]\n",
      "epoch:8 step:7797 [D loss: 0.705034, acc.: 50.78%] [G loss: 0.710058]\n",
      "epoch:8 step:7798 [D loss: 0.713448, acc.: 42.97%] [G loss: 0.717429]\n",
      "epoch:8 step:7799 [D loss: 0.688108, acc.: 47.66%] [G loss: 0.765793]\n",
      "epoch:8 step:7800 [D loss: 0.709461, acc.: 50.78%] [G loss: 0.750757]\n",
      "epoch:8 step:7801 [D loss: 0.689079, acc.: 50.00%] [G loss: 0.774862]\n",
      "epoch:8 step:7802 [D loss: 0.682223, acc.: 53.91%] [G loss: 0.784202]\n",
      "epoch:8 step:7803 [D loss: 0.659119, acc.: 57.03%] [G loss: 0.780516]\n",
      "epoch:8 step:7804 [D loss: 0.705670, acc.: 53.91%] [G loss: 0.765470]\n",
      "epoch:8 step:7805 [D loss: 0.680966, acc.: 58.59%] [G loss: 0.748220]\n",
      "epoch:8 step:7806 [D loss: 0.664814, acc.: 57.81%] [G loss: 0.807588]\n",
      "epoch:8 step:7807 [D loss: 0.700466, acc.: 57.03%] [G loss: 0.747948]\n",
      "epoch:8 step:7808 [D loss: 0.675822, acc.: 58.59%] [G loss: 0.777715]\n",
      "epoch:8 step:7809 [D loss: 0.673890, acc.: 57.03%] [G loss: 0.822598]\n",
      "epoch:8 step:7810 [D loss: 0.690197, acc.: 53.12%] [G loss: 0.787698]\n",
      "epoch:8 step:7811 [D loss: 0.641026, acc.: 61.72%] [G loss: 0.784906]\n",
      "epoch:8 step:7812 [D loss: 0.677728, acc.: 50.00%] [G loss: 0.784750]\n",
      "epoch:8 step:7813 [D loss: 0.688221, acc.: 52.34%] [G loss: 0.731581]\n",
      "epoch:8 step:7814 [D loss: 0.686798, acc.: 53.12%] [G loss: 0.712485]\n",
      "epoch:8 step:7815 [D loss: 0.687016, acc.: 53.12%] [G loss: 0.769579]\n",
      "epoch:8 step:7816 [D loss: 0.694307, acc.: 60.16%] [G loss: 0.755692]\n",
      "epoch:8 step:7817 [D loss: 0.703700, acc.: 46.88%] [G loss: 0.730204]\n",
      "epoch:8 step:7818 [D loss: 0.696225, acc.: 47.66%] [G loss: 0.734982]\n",
      "epoch:8 step:7819 [D loss: 0.687373, acc.: 56.25%] [G loss: 0.777088]\n",
      "epoch:8 step:7820 [D loss: 0.679007, acc.: 58.59%] [G loss: 0.779487]\n",
      "epoch:8 step:7821 [D loss: 0.682519, acc.: 54.69%] [G loss: 0.832707]\n",
      "epoch:8 step:7822 [D loss: 0.744217, acc.: 40.62%] [G loss: 0.821475]\n",
      "epoch:8 step:7823 [D loss: 0.680868, acc.: 60.16%] [G loss: 0.860905]\n",
      "epoch:8 step:7824 [D loss: 0.710605, acc.: 53.12%] [G loss: 0.811432]\n",
      "epoch:8 step:7825 [D loss: 0.660403, acc.: 65.62%] [G loss: 0.818858]\n",
      "epoch:8 step:7826 [D loss: 0.706481, acc.: 47.66%] [G loss: 0.791748]\n",
      "epoch:8 step:7827 [D loss: 0.681060, acc.: 54.69%] [G loss: 0.749040]\n",
      "epoch:8 step:7828 [D loss: 0.704376, acc.: 46.88%] [G loss: 0.799996]\n",
      "epoch:8 step:7829 [D loss: 0.689039, acc.: 58.59%] [G loss: 0.823437]\n",
      "epoch:8 step:7830 [D loss: 0.733973, acc.: 41.41%] [G loss: 0.786503]\n",
      "epoch:8 step:7831 [D loss: 0.672175, acc.: 63.28%] [G loss: 0.730735]\n",
      "epoch:8 step:7832 [D loss: 0.675580, acc.: 54.69%] [G loss: 0.799287]\n",
      "epoch:8 step:7833 [D loss: 0.663624, acc.: 57.81%] [G loss: 0.790269]\n",
      "epoch:8 step:7834 [D loss: 0.696505, acc.: 50.00%] [G loss: 0.759079]\n",
      "epoch:8 step:7835 [D loss: 0.693030, acc.: 46.09%] [G loss: 0.745426]\n",
      "epoch:8 step:7836 [D loss: 0.693810, acc.: 44.53%] [G loss: 0.727666]\n",
      "epoch:8 step:7837 [D loss: 0.699321, acc.: 50.78%] [G loss: 0.795750]\n",
      "epoch:8 step:7838 [D loss: 0.674640, acc.: 53.91%] [G loss: 0.776022]\n",
      "epoch:8 step:7839 [D loss: 0.698525, acc.: 49.22%] [G loss: 0.794375]\n",
      "epoch:8 step:7840 [D loss: 0.662392, acc.: 57.81%] [G loss: 0.813108]\n",
      "epoch:8 step:7841 [D loss: 0.683149, acc.: 60.16%] [G loss: 0.740550]\n",
      "epoch:8 step:7842 [D loss: 0.661889, acc.: 57.81%] [G loss: 0.782309]\n",
      "epoch:8 step:7843 [D loss: 0.693494, acc.: 53.91%] [G loss: 0.788477]\n",
      "epoch:8 step:7844 [D loss: 0.684884, acc.: 53.91%] [G loss: 0.782576]\n",
      "epoch:8 step:7845 [D loss: 0.707896, acc.: 51.56%] [G loss: 0.823684]\n",
      "epoch:8 step:7846 [D loss: 0.661572, acc.: 56.25%] [G loss: 0.786939]\n",
      "epoch:8 step:7847 [D loss: 0.697727, acc.: 50.00%] [G loss: 0.759982]\n",
      "epoch:8 step:7848 [D loss: 0.705392, acc.: 53.12%] [G loss: 0.757084]\n",
      "epoch:8 step:7849 [D loss: 0.652810, acc.: 61.72%] [G loss: 0.789109]\n",
      "epoch:8 step:7850 [D loss: 0.694060, acc.: 53.91%] [G loss: 0.792347]\n",
      "epoch:8 step:7851 [D loss: 0.700160, acc.: 49.22%] [G loss: 0.744199]\n",
      "epoch:8 step:7852 [D loss: 0.716024, acc.: 42.19%] [G loss: 0.777687]\n",
      "epoch:8 step:7853 [D loss: 0.665815, acc.: 57.81%] [G loss: 0.822422]\n",
      "epoch:8 step:7854 [D loss: 0.694372, acc.: 54.69%] [G loss: 0.789452]\n",
      "epoch:8 step:7855 [D loss: 0.682998, acc.: 54.69%] [G loss: 0.740635]\n",
      "epoch:8 step:7856 [D loss: 0.685303, acc.: 49.22%] [G loss: 0.736434]\n",
      "epoch:8 step:7857 [D loss: 0.665215, acc.: 60.94%] [G loss: 0.753903]\n",
      "epoch:8 step:7858 [D loss: 0.682839, acc.: 60.16%] [G loss: 0.751705]\n",
      "epoch:8 step:7859 [D loss: 0.644249, acc.: 61.72%] [G loss: 0.765044]\n",
      "epoch:8 step:7860 [D loss: 0.688311, acc.: 51.56%] [G loss: 0.787774]\n",
      "epoch:8 step:7861 [D loss: 0.662223, acc.: 60.94%] [G loss: 0.724614]\n",
      "epoch:8 step:7862 [D loss: 0.688257, acc.: 56.25%] [G loss: 0.792049]\n",
      "epoch:8 step:7863 [D loss: 0.677650, acc.: 57.81%] [G loss: 0.846187]\n",
      "epoch:8 step:7864 [D loss: 0.696427, acc.: 53.91%] [G loss: 0.837633]\n",
      "epoch:8 step:7865 [D loss: 0.682767, acc.: 57.81%] [G loss: 0.816839]\n",
      "epoch:8 step:7866 [D loss: 0.662005, acc.: 59.38%] [G loss: 0.800165]\n",
      "epoch:8 step:7867 [D loss: 0.659724, acc.: 60.16%] [G loss: 0.771421]\n",
      "epoch:8 step:7868 [D loss: 0.689540, acc.: 51.56%] [G loss: 0.770806]\n",
      "epoch:8 step:7869 [D loss: 0.655092, acc.: 62.50%] [G loss: 0.796224]\n",
      "epoch:8 step:7870 [D loss: 0.701054, acc.: 45.31%] [G loss: 0.737758]\n",
      "epoch:8 step:7871 [D loss: 0.678716, acc.: 51.56%] [G loss: 0.762621]\n",
      "epoch:8 step:7872 [D loss: 0.659416, acc.: 70.31%] [G loss: 0.785717]\n",
      "epoch:8 step:7873 [D loss: 0.717889, acc.: 44.53%] [G loss: 0.849720]\n",
      "epoch:8 step:7874 [D loss: 0.661567, acc.: 60.16%] [G loss: 0.768636]\n",
      "epoch:8 step:7875 [D loss: 0.664891, acc.: 61.72%] [G loss: 0.797718]\n",
      "epoch:8 step:7876 [D loss: 0.742702, acc.: 38.28%] [G loss: 0.849127]\n",
      "epoch:8 step:7877 [D loss: 0.662872, acc.: 53.12%] [G loss: 0.781127]\n",
      "epoch:8 step:7878 [D loss: 0.694028, acc.: 50.78%] [G loss: 0.840156]\n",
      "epoch:8 step:7879 [D loss: 0.642268, acc.: 67.19%] [G loss: 0.808827]\n",
      "epoch:8 step:7880 [D loss: 0.689261, acc.: 51.56%] [G loss: 0.864126]\n",
      "epoch:8 step:7881 [D loss: 0.720150, acc.: 42.19%] [G loss: 0.810788]\n",
      "epoch:8 step:7882 [D loss: 0.675995, acc.: 52.34%] [G loss: 0.807409]\n",
      "epoch:8 step:7883 [D loss: 0.660756, acc.: 62.50%] [G loss: 0.776565]\n",
      "epoch:8 step:7884 [D loss: 0.669731, acc.: 61.72%] [G loss: 0.848555]\n",
      "epoch:8 step:7885 [D loss: 0.690023, acc.: 53.91%] [G loss: 0.783424]\n",
      "epoch:8 step:7886 [D loss: 0.704540, acc.: 52.34%] [G loss: 0.782529]\n",
      "epoch:8 step:7887 [D loss: 0.676755, acc.: 52.34%] [G loss: 0.764049]\n",
      "epoch:8 step:7888 [D loss: 0.685148, acc.: 55.47%] [G loss: 0.787833]\n",
      "epoch:8 step:7889 [D loss: 0.733963, acc.: 48.44%] [G loss: 0.815852]\n",
      "epoch:8 step:7890 [D loss: 0.700485, acc.: 45.31%] [G loss: 0.778849]\n",
      "epoch:8 step:7891 [D loss: 0.673583, acc.: 63.28%] [G loss: 0.789234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7892 [D loss: 0.723557, acc.: 45.31%] [G loss: 0.781399]\n",
      "epoch:8 step:7893 [D loss: 0.686161, acc.: 51.56%] [G loss: 0.770160]\n",
      "epoch:8 step:7894 [D loss: 0.675171, acc.: 56.25%] [G loss: 0.782942]\n",
      "epoch:8 step:7895 [D loss: 0.686863, acc.: 53.12%] [G loss: 0.782562]\n",
      "epoch:8 step:7896 [D loss: 0.686638, acc.: 52.34%] [G loss: 0.832094]\n",
      "epoch:8 step:7897 [D loss: 0.700494, acc.: 46.09%] [G loss: 0.789624]\n",
      "epoch:8 step:7898 [D loss: 0.655564, acc.: 57.81%] [G loss: 0.759458]\n",
      "epoch:8 step:7899 [D loss: 0.686810, acc.: 54.69%] [G loss: 0.765709]\n",
      "epoch:8 step:7900 [D loss: 0.672801, acc.: 53.91%] [G loss: 0.809749]\n",
      "epoch:8 step:7901 [D loss: 0.676275, acc.: 62.50%] [G loss: 0.755196]\n",
      "epoch:8 step:7902 [D loss: 0.651096, acc.: 56.25%] [G loss: 0.755688]\n",
      "epoch:8 step:7903 [D loss: 0.671491, acc.: 57.81%] [G loss: 0.787529]\n",
      "epoch:8 step:7904 [D loss: 0.651094, acc.: 64.06%] [G loss: 0.794918]\n",
      "epoch:8 step:7905 [D loss: 0.653924, acc.: 56.25%] [G loss: 0.752432]\n",
      "epoch:8 step:7906 [D loss: 0.690194, acc.: 51.56%] [G loss: 0.765757]\n",
      "epoch:8 step:7907 [D loss: 0.706513, acc.: 53.12%] [G loss: 0.818871]\n",
      "epoch:8 step:7908 [D loss: 0.684434, acc.: 51.56%] [G loss: 0.785326]\n",
      "epoch:8 step:7909 [D loss: 0.687025, acc.: 53.91%] [G loss: 0.754041]\n",
      "epoch:8 step:7910 [D loss: 0.675798, acc.: 64.06%] [G loss: 0.754795]\n",
      "epoch:8 step:7911 [D loss: 0.624410, acc.: 70.31%] [G loss: 0.861667]\n",
      "epoch:8 step:7912 [D loss: 0.653675, acc.: 64.06%] [G loss: 0.806325]\n",
      "epoch:8 step:7913 [D loss: 0.674573, acc.: 56.25%] [G loss: 0.751143]\n",
      "epoch:8 step:7914 [D loss: 0.648083, acc.: 63.28%] [G loss: 0.820771]\n",
      "epoch:8 step:7915 [D loss: 0.672829, acc.: 57.03%] [G loss: 0.804138]\n",
      "epoch:8 step:7916 [D loss: 0.682224, acc.: 57.81%] [G loss: 0.734536]\n",
      "epoch:8 step:7917 [D loss: 0.678809, acc.: 58.59%] [G loss: 0.720587]\n",
      "epoch:8 step:7918 [D loss: 0.729407, acc.: 48.44%] [G loss: 0.763485]\n",
      "epoch:8 step:7919 [D loss: 0.681424, acc.: 54.69%] [G loss: 0.797364]\n",
      "epoch:8 step:7920 [D loss: 0.702494, acc.: 49.22%] [G loss: 0.731933]\n",
      "epoch:8 step:7921 [D loss: 0.680449, acc.: 69.53%] [G loss: 0.732223]\n",
      "epoch:8 step:7922 [D loss: 0.707516, acc.: 55.47%] [G loss: 0.817684]\n",
      "epoch:8 step:7923 [D loss: 0.673422, acc.: 57.81%] [G loss: 0.745505]\n",
      "epoch:8 step:7924 [D loss: 0.682156, acc.: 53.12%] [G loss: 0.799491]\n",
      "epoch:8 step:7925 [D loss: 0.708868, acc.: 53.91%] [G loss: 0.818625]\n",
      "epoch:8 step:7926 [D loss: 0.718117, acc.: 50.78%] [G loss: 0.816694]\n",
      "epoch:8 step:7927 [D loss: 0.683564, acc.: 50.78%] [G loss: 0.808490]\n",
      "epoch:8 step:7928 [D loss: 0.683146, acc.: 57.03%] [G loss: 0.760307]\n",
      "epoch:8 step:7929 [D loss: 0.708802, acc.: 50.78%] [G loss: 0.785588]\n",
      "epoch:8 step:7930 [D loss: 0.695504, acc.: 52.34%] [G loss: 0.743917]\n",
      "epoch:8 step:7931 [D loss: 0.695151, acc.: 51.56%] [G loss: 0.743821]\n",
      "epoch:8 step:7932 [D loss: 0.653394, acc.: 54.69%] [G loss: 0.811870]\n",
      "epoch:8 step:7933 [D loss: 0.656319, acc.: 51.56%] [G loss: 0.839420]\n",
      "epoch:8 step:7934 [D loss: 0.684113, acc.: 58.59%] [G loss: 0.816610]\n",
      "epoch:8 step:7935 [D loss: 0.698915, acc.: 54.69%] [G loss: 0.779440]\n",
      "epoch:8 step:7936 [D loss: 0.670717, acc.: 58.59%] [G loss: 0.781237]\n",
      "epoch:8 step:7937 [D loss: 0.700349, acc.: 46.09%] [G loss: 0.868299]\n",
      "epoch:8 step:7938 [D loss: 0.670926, acc.: 56.25%] [G loss: 0.813355]\n",
      "epoch:8 step:7939 [D loss: 0.726041, acc.: 45.31%] [G loss: 0.834225]\n",
      "epoch:8 step:7940 [D loss: 0.726849, acc.: 47.66%] [G loss: 0.788432]\n",
      "epoch:8 step:7941 [D loss: 0.700224, acc.: 50.00%] [G loss: 0.791096]\n",
      "epoch:8 step:7942 [D loss: 0.686163, acc.: 49.22%] [G loss: 0.835855]\n",
      "epoch:8 step:7943 [D loss: 0.677440, acc.: 59.38%] [G loss: 0.756532]\n",
      "epoch:8 step:7944 [D loss: 0.711669, acc.: 42.97%] [G loss: 0.784635]\n",
      "epoch:8 step:7945 [D loss: 0.686378, acc.: 53.12%] [G loss: 0.768232]\n",
      "epoch:8 step:7946 [D loss: 0.675826, acc.: 57.81%] [G loss: 0.808343]\n",
      "epoch:8 step:7947 [D loss: 0.681894, acc.: 54.69%] [G loss: 0.756474]\n",
      "epoch:8 step:7948 [D loss: 0.668579, acc.: 53.91%] [G loss: 0.775511]\n",
      "epoch:8 step:7949 [D loss: 0.681161, acc.: 51.56%] [G loss: 0.774521]\n",
      "epoch:8 step:7950 [D loss: 0.684974, acc.: 51.56%] [G loss: 0.801411]\n",
      "epoch:8 step:7951 [D loss: 0.662073, acc.: 60.16%] [G loss: 0.813560]\n",
      "epoch:8 step:7952 [D loss: 0.659429, acc.: 54.69%] [G loss: 0.813676]\n",
      "epoch:8 step:7953 [D loss: 0.704132, acc.: 48.44%] [G loss: 0.854908]\n",
      "epoch:8 step:7954 [D loss: 0.679144, acc.: 53.91%] [G loss: 0.828578]\n",
      "epoch:8 step:7955 [D loss: 0.669651, acc.: 53.91%] [G loss: 0.819322]\n",
      "epoch:8 step:7956 [D loss: 0.702517, acc.: 50.78%] [G loss: 0.824352]\n",
      "epoch:8 step:7957 [D loss: 0.674333, acc.: 54.69%] [G loss: 0.794456]\n",
      "epoch:8 step:7958 [D loss: 0.691363, acc.: 53.12%] [G loss: 0.760385]\n",
      "epoch:8 step:7959 [D loss: 0.696152, acc.: 50.00%] [G loss: 0.816968]\n",
      "epoch:8 step:7960 [D loss: 0.667605, acc.: 45.31%] [G loss: 0.791669]\n",
      "epoch:8 step:7961 [D loss: 0.675103, acc.: 57.03%] [G loss: 0.808851]\n",
      "epoch:8 step:7962 [D loss: 0.667123, acc.: 60.94%] [G loss: 0.781064]\n",
      "epoch:8 step:7963 [D loss: 0.676533, acc.: 54.69%] [G loss: 0.817633]\n",
      "epoch:8 step:7964 [D loss: 0.655229, acc.: 59.38%] [G loss: 0.764579]\n",
      "epoch:8 step:7965 [D loss: 0.666621, acc.: 64.06%] [G loss: 0.797546]\n",
      "epoch:8 step:7966 [D loss: 0.705031, acc.: 52.34%] [G loss: 0.770016]\n",
      "epoch:8 step:7967 [D loss: 0.640715, acc.: 66.41%] [G loss: 0.766055]\n",
      "epoch:8 step:7968 [D loss: 0.663924, acc.: 61.72%] [G loss: 0.754308]\n",
      "epoch:8 step:7969 [D loss: 0.674661, acc.: 48.44%] [G loss: 0.783162]\n",
      "epoch:8 step:7970 [D loss: 0.692327, acc.: 49.22%] [G loss: 0.778566]\n",
      "epoch:8 step:7971 [D loss: 0.692734, acc.: 50.78%] [G loss: 0.735830]\n",
      "epoch:8 step:7972 [D loss: 0.657243, acc.: 60.16%] [G loss: 0.783929]\n",
      "epoch:8 step:7973 [D loss: 0.680735, acc.: 57.03%] [G loss: 0.763777]\n",
      "epoch:8 step:7974 [D loss: 0.667055, acc.: 60.94%] [G loss: 0.786642]\n",
      "epoch:8 step:7975 [D loss: 0.736483, acc.: 50.00%] [G loss: 0.762727]\n",
      "epoch:8 step:7976 [D loss: 0.698151, acc.: 50.78%] [G loss: 0.747032]\n",
      "epoch:8 step:7977 [D loss: 0.691602, acc.: 57.81%] [G loss: 0.851166]\n",
      "epoch:8 step:7978 [D loss: 0.667400, acc.: 57.81%] [G loss: 0.845684]\n",
      "epoch:8 step:7979 [D loss: 0.712102, acc.: 51.56%] [G loss: 0.909261]\n",
      "epoch:8 step:7980 [D loss: 0.684672, acc.: 53.12%] [G loss: 0.874937]\n",
      "epoch:8 step:7981 [D loss: 0.690117, acc.: 56.25%] [G loss: 0.810932]\n",
      "epoch:8 step:7982 [D loss: 0.691506, acc.: 52.34%] [G loss: 0.753869]\n",
      "epoch:8 step:7983 [D loss: 0.707906, acc.: 49.22%] [G loss: 0.809932]\n",
      "epoch:8 step:7984 [D loss: 0.668256, acc.: 57.03%] [G loss: 0.796344]\n",
      "epoch:8 step:7985 [D loss: 0.710305, acc.: 46.09%] [G loss: 0.803802]\n",
      "epoch:8 step:7986 [D loss: 0.672220, acc.: 57.81%] [G loss: 0.805585]\n",
      "epoch:8 step:7987 [D loss: 0.692400, acc.: 53.12%] [G loss: 0.800559]\n",
      "epoch:8 step:7988 [D loss: 0.676921, acc.: 54.69%] [G loss: 0.793267]\n",
      "epoch:8 step:7989 [D loss: 0.702158, acc.: 50.78%] [G loss: 0.766734]\n",
      "epoch:8 step:7990 [D loss: 0.672365, acc.: 54.69%] [G loss: 0.766729]\n",
      "epoch:8 step:7991 [D loss: 0.683715, acc.: 57.81%] [G loss: 0.777076]\n",
      "epoch:8 step:7992 [D loss: 0.662420, acc.: 63.28%] [G loss: 0.775173]\n",
      "epoch:8 step:7993 [D loss: 0.674850, acc.: 60.94%] [G loss: 0.796570]\n",
      "epoch:8 step:7994 [D loss: 0.698614, acc.: 50.78%] [G loss: 0.786782]\n",
      "epoch:8 step:7995 [D loss: 0.693825, acc.: 53.12%] [G loss: 0.744566]\n",
      "epoch:8 step:7996 [D loss: 0.718956, acc.: 44.53%] [G loss: 0.749635]\n",
      "epoch:8 step:7997 [D loss: 0.678690, acc.: 52.34%] [G loss: 1.133260]\n",
      "epoch:8 step:7998 [D loss: 0.690867, acc.: 50.78%] [G loss: 0.754554]\n",
      "epoch:8 step:7999 [D loss: 0.698024, acc.: 53.91%] [G loss: 0.787019]\n",
      "epoch:8 step:8000 [D loss: 0.706768, acc.: 44.53%] [G loss: 0.737728]\n",
      "epoch:8 step:8001 [D loss: 0.690132, acc.: 50.00%] [G loss: 0.754103]\n",
      "epoch:8 step:8002 [D loss: 0.700377, acc.: 49.22%] [G loss: 0.761849]\n",
      "epoch:8 step:8003 [D loss: 0.687707, acc.: 52.34%] [G loss: 0.722591]\n",
      "epoch:8 step:8004 [D loss: 0.687358, acc.: 55.47%] [G loss: 0.763158]\n",
      "epoch:8 step:8005 [D loss: 0.671924, acc.: 64.84%] [G loss: 0.759279]\n",
      "epoch:8 step:8006 [D loss: 0.662136, acc.: 62.50%] [G loss: 0.746481]\n",
      "epoch:8 step:8007 [D loss: 0.684045, acc.: 58.59%] [G loss: 0.752301]\n",
      "epoch:8 step:8008 [D loss: 0.701888, acc.: 53.12%] [G loss: 0.802512]\n",
      "epoch:8 step:8009 [D loss: 0.673471, acc.: 60.16%] [G loss: 0.794649]\n",
      "epoch:8 step:8010 [D loss: 0.683475, acc.: 49.22%] [G loss: 0.736757]\n",
      "epoch:8 step:8011 [D loss: 0.666093, acc.: 63.28%] [G loss: 0.783647]\n",
      "epoch:8 step:8012 [D loss: 0.671345, acc.: 60.16%] [G loss: 0.769780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8013 [D loss: 0.692128, acc.: 53.12%] [G loss: 0.740090]\n",
      "epoch:8 step:8014 [D loss: 0.691163, acc.: 49.22%] [G loss: 0.785853]\n",
      "epoch:8 step:8015 [D loss: 0.687716, acc.: 55.47%] [G loss: 0.872815]\n",
      "epoch:8 step:8016 [D loss: 0.639741, acc.: 62.50%] [G loss: 0.830520]\n",
      "epoch:8 step:8017 [D loss: 0.682510, acc.: 53.91%] [G loss: 0.849577]\n",
      "epoch:8 step:8018 [D loss: 0.716905, acc.: 41.41%] [G loss: 0.804646]\n",
      "epoch:8 step:8019 [D loss: 0.633907, acc.: 65.62%] [G loss: 0.806832]\n",
      "epoch:8 step:8020 [D loss: 0.684207, acc.: 52.34%] [G loss: 0.761621]\n",
      "epoch:8 step:8021 [D loss: 0.686045, acc.: 53.91%] [G loss: 0.786942]\n",
      "epoch:8 step:8022 [D loss: 0.708451, acc.: 50.78%] [G loss: 0.751895]\n",
      "epoch:8 step:8023 [D loss: 0.684663, acc.: 53.12%] [G loss: 0.819046]\n",
      "epoch:8 step:8024 [D loss: 0.678908, acc.: 59.38%] [G loss: 0.801804]\n",
      "epoch:8 step:8025 [D loss: 0.682258, acc.: 50.78%] [G loss: 0.765547]\n",
      "epoch:8 step:8026 [D loss: 0.692966, acc.: 55.47%] [G loss: 0.807093]\n",
      "epoch:8 step:8027 [D loss: 0.664944, acc.: 61.72%] [G loss: 0.749099]\n",
      "epoch:8 step:8028 [D loss: 0.675921, acc.: 60.16%] [G loss: 0.779300]\n",
      "epoch:8 step:8029 [D loss: 0.634966, acc.: 64.84%] [G loss: 0.771512]\n",
      "epoch:8 step:8030 [D loss: 0.681230, acc.: 50.78%] [G loss: 0.731198]\n",
      "epoch:8 step:8031 [D loss: 0.681429, acc.: 57.03%] [G loss: 0.803548]\n",
      "epoch:8 step:8032 [D loss: 0.694310, acc.: 53.91%] [G loss: 0.838026]\n",
      "epoch:8 step:8033 [D loss: 0.695367, acc.: 50.78%] [G loss: 0.778894]\n",
      "epoch:8 step:8034 [D loss: 0.696755, acc.: 56.25%] [G loss: 0.783775]\n",
      "epoch:8 step:8035 [D loss: 0.693984, acc.: 51.56%] [G loss: 0.814050]\n",
      "epoch:8 step:8036 [D loss: 0.694011, acc.: 50.00%] [G loss: 0.866683]\n",
      "epoch:8 step:8037 [D loss: 0.690104, acc.: 55.47%] [G loss: 0.775774]\n",
      "epoch:8 step:8038 [D loss: 0.711701, acc.: 43.75%] [G loss: 0.750805]\n",
      "epoch:8 step:8039 [D loss: 0.687015, acc.: 53.12%] [G loss: 0.785390]\n",
      "epoch:8 step:8040 [D loss: 0.698643, acc.: 55.47%] [G loss: 0.827801]\n",
      "epoch:8 step:8041 [D loss: 0.680948, acc.: 58.59%] [G loss: 0.718161]\n",
      "epoch:8 step:8042 [D loss: 0.685305, acc.: 47.66%] [G loss: 0.783573]\n",
      "epoch:8 step:8043 [D loss: 0.696104, acc.: 58.59%] [G loss: 0.828279]\n",
      "epoch:8 step:8044 [D loss: 0.690906, acc.: 50.78%] [G loss: 0.781276]\n",
      "epoch:8 step:8045 [D loss: 0.687677, acc.: 57.03%] [G loss: 0.809418]\n",
      "epoch:8 step:8046 [D loss: 0.700581, acc.: 53.12%] [G loss: 0.753503]\n",
      "epoch:8 step:8047 [D loss: 0.696899, acc.: 47.66%] [G loss: 0.776787]\n",
      "epoch:8 step:8048 [D loss: 0.703501, acc.: 48.44%] [G loss: 0.748870]\n",
      "epoch:8 step:8049 [D loss: 0.678522, acc.: 53.91%] [G loss: 0.768478]\n",
      "epoch:8 step:8050 [D loss: 0.673609, acc.: 57.81%] [G loss: 0.819502]\n",
      "epoch:8 step:8051 [D loss: 0.654359, acc.: 59.38%] [G loss: 0.793232]\n",
      "epoch:8 step:8052 [D loss: 0.679552, acc.: 57.81%] [G loss: 0.906518]\n",
      "epoch:8 step:8053 [D loss: 0.711910, acc.: 50.00%] [G loss: 0.764611]\n",
      "epoch:8 step:8054 [D loss: 0.686754, acc.: 55.47%] [G loss: 0.761519]\n",
      "epoch:8 step:8055 [D loss: 0.669700, acc.: 58.59%] [G loss: 0.759293]\n",
      "epoch:8 step:8056 [D loss: 0.728409, acc.: 43.75%] [G loss: 0.763380]\n",
      "epoch:8 step:8057 [D loss: 0.678315, acc.: 57.81%] [G loss: 0.739634]\n",
      "epoch:8 step:8058 [D loss: 0.681566, acc.: 57.81%] [G loss: 0.789363]\n",
      "epoch:8 step:8059 [D loss: 0.709472, acc.: 48.44%] [G loss: 0.748312]\n",
      "epoch:8 step:8060 [D loss: 0.713325, acc.: 46.88%] [G loss: 0.750088]\n",
      "epoch:8 step:8061 [D loss: 0.696529, acc.: 46.09%] [G loss: 0.743751]\n",
      "epoch:8 step:8062 [D loss: 0.676131, acc.: 51.56%] [G loss: 0.749465]\n",
      "epoch:8 step:8063 [D loss: 0.680303, acc.: 56.25%] [G loss: 0.702089]\n",
      "epoch:8 step:8064 [D loss: 0.668000, acc.: 55.47%] [G loss: 0.706330]\n",
      "epoch:8 step:8065 [D loss: 0.658354, acc.: 60.16%] [G loss: 0.748327]\n",
      "epoch:8 step:8066 [D loss: 0.751550, acc.: 48.44%] [G loss: 0.751251]\n",
      "epoch:8 step:8067 [D loss: 0.694953, acc.: 48.44%] [G loss: 0.823359]\n",
      "epoch:8 step:8068 [D loss: 0.686162, acc.: 57.03%] [G loss: 0.728633]\n",
      "epoch:8 step:8069 [D loss: 0.688990, acc.: 54.69%] [G loss: 0.758389]\n",
      "epoch:8 step:8070 [D loss: 0.682037, acc.: 56.25%] [G loss: 0.775415]\n",
      "epoch:8 step:8071 [D loss: 0.754687, acc.: 50.00%] [G loss: 0.828481]\n",
      "epoch:8 step:8072 [D loss: 0.682550, acc.: 57.81%] [G loss: 0.799154]\n",
      "epoch:8 step:8073 [D loss: 0.701095, acc.: 53.12%] [G loss: 0.808223]\n",
      "epoch:8 step:8074 [D loss: 0.676670, acc.: 58.59%] [G loss: 0.820410]\n",
      "epoch:8 step:8075 [D loss: 0.710995, acc.: 54.69%] [G loss: 0.793944]\n",
      "epoch:8 step:8076 [D loss: 0.707146, acc.: 45.31%] [G loss: 0.785846]\n",
      "epoch:8 step:8077 [D loss: 0.671572, acc.: 60.94%] [G loss: 0.813611]\n",
      "epoch:8 step:8078 [D loss: 0.704334, acc.: 46.09%] [G loss: 0.776351]\n",
      "epoch:8 step:8079 [D loss: 0.700946, acc.: 48.44%] [G loss: 0.788128]\n",
      "epoch:8 step:8080 [D loss: 0.666473, acc.: 60.16%] [G loss: 0.744143]\n",
      "epoch:8 step:8081 [D loss: 0.654933, acc.: 63.28%] [G loss: 0.800287]\n",
      "epoch:8 step:8082 [D loss: 0.668951, acc.: 55.47%] [G loss: 0.728487]\n",
      "epoch:8 step:8083 [D loss: 0.675018, acc.: 56.25%] [G loss: 0.737817]\n",
      "epoch:8 step:8084 [D loss: 0.680713, acc.: 53.12%] [G loss: 0.757216]\n",
      "epoch:8 step:8085 [D loss: 0.706284, acc.: 47.66%] [G loss: 0.731800]\n",
      "epoch:8 step:8086 [D loss: 0.680106, acc.: 50.00%] [G loss: 0.742579]\n",
      "epoch:8 step:8087 [D loss: 0.656716, acc.: 61.72%] [G loss: 0.750481]\n",
      "epoch:8 step:8088 [D loss: 0.672660, acc.: 52.34%] [G loss: 0.731317]\n",
      "epoch:8 step:8089 [D loss: 0.679997, acc.: 47.66%] [G loss: 0.726235]\n",
      "epoch:8 step:8090 [D loss: 0.655499, acc.: 66.41%] [G loss: 0.727974]\n",
      "epoch:8 step:8091 [D loss: 0.692085, acc.: 55.47%] [G loss: 0.777912]\n",
      "epoch:8 step:8092 [D loss: 0.648221, acc.: 64.84%] [G loss: 0.769532]\n",
      "epoch:8 step:8093 [D loss: 0.683508, acc.: 53.91%] [G loss: 0.764603]\n",
      "epoch:8 step:8094 [D loss: 0.697237, acc.: 56.25%] [G loss: 0.755000]\n",
      "epoch:8 step:8095 [D loss: 0.674275, acc.: 60.16%] [G loss: 0.780598]\n",
      "epoch:8 step:8096 [D loss: 0.676641, acc.: 58.59%] [G loss: 0.789677]\n",
      "epoch:8 step:8097 [D loss: 0.706712, acc.: 42.97%] [G loss: 0.792650]\n",
      "epoch:8 step:8098 [D loss: 0.675395, acc.: 56.25%] [G loss: 0.806687]\n",
      "epoch:8 step:8099 [D loss: 0.674274, acc.: 56.25%] [G loss: 0.824670]\n",
      "epoch:8 step:8100 [D loss: 0.684972, acc.: 62.50%] [G loss: 0.797663]\n",
      "epoch:8 step:8101 [D loss: 0.681847, acc.: 48.44%] [G loss: 0.764557]\n",
      "epoch:8 step:8102 [D loss: 0.708336, acc.: 50.00%] [G loss: 0.758340]\n",
      "epoch:8 step:8103 [D loss: 0.677849, acc.: 56.25%] [G loss: 0.826100]\n",
      "epoch:8 step:8104 [D loss: 0.694629, acc.: 46.88%] [G loss: 0.803511]\n",
      "epoch:8 step:8105 [D loss: 0.687151, acc.: 50.00%] [G loss: 0.822855]\n",
      "epoch:8 step:8106 [D loss: 0.685621, acc.: 53.91%] [G loss: 0.812773]\n",
      "epoch:8 step:8107 [D loss: 0.671932, acc.: 57.81%] [G loss: 0.782422]\n",
      "epoch:8 step:8108 [D loss: 0.676102, acc.: 55.47%] [G loss: 0.773323]\n",
      "epoch:8 step:8109 [D loss: 0.688277, acc.: 50.78%] [G loss: 0.758600]\n",
      "epoch:8 step:8110 [D loss: 0.696960, acc.: 53.91%] [G loss: 0.699553]\n",
      "epoch:8 step:8111 [D loss: 0.691830, acc.: 58.59%] [G loss: 0.775460]\n",
      "epoch:8 step:8112 [D loss: 0.698918, acc.: 53.12%] [G loss: 0.787732]\n",
      "epoch:8 step:8113 [D loss: 0.681854, acc.: 50.00%] [G loss: 0.780306]\n",
      "epoch:8 step:8114 [D loss: 0.683338, acc.: 57.03%] [G loss: 0.803572]\n",
      "epoch:8 step:8115 [D loss: 0.665115, acc.: 57.81%] [G loss: 0.786285]\n",
      "epoch:8 step:8116 [D loss: 0.667224, acc.: 58.59%] [G loss: 0.802829]\n",
      "epoch:8 step:8117 [D loss: 0.698856, acc.: 53.12%] [G loss: 0.810034]\n",
      "epoch:8 step:8118 [D loss: 0.711641, acc.: 50.78%] [G loss: 0.811663]\n",
      "epoch:8 step:8119 [D loss: 0.684106, acc.: 59.38%] [G loss: 0.808874]\n",
      "epoch:8 step:8120 [D loss: 0.691791, acc.: 57.03%] [G loss: 0.848932]\n",
      "epoch:8 step:8121 [D loss: 0.712794, acc.: 45.31%] [G loss: 0.789686]\n",
      "epoch:8 step:8122 [D loss: 0.674941, acc.: 53.12%] [G loss: 0.766613]\n",
      "epoch:8 step:8123 [D loss: 0.694884, acc.: 52.34%] [G loss: 0.758654]\n",
      "epoch:8 step:8124 [D loss: 0.650518, acc.: 62.50%] [G loss: 0.833175]\n",
      "epoch:8 step:8125 [D loss: 0.670264, acc.: 59.38%] [G loss: 0.782470]\n",
      "epoch:8 step:8126 [D loss: 0.704733, acc.: 50.00%] [G loss: 0.811227]\n",
      "epoch:8 step:8127 [D loss: 0.691989, acc.: 49.22%] [G loss: 0.821334]\n",
      "epoch:8 step:8128 [D loss: 0.684846, acc.: 55.47%] [G loss: 0.821437]\n",
      "epoch:8 step:8129 [D loss: 0.680299, acc.: 55.47%] [G loss: 0.763702]\n",
      "epoch:8 step:8130 [D loss: 0.693918, acc.: 49.22%] [G loss: 0.763207]\n",
      "epoch:8 step:8131 [D loss: 0.666014, acc.: 60.94%] [G loss: 0.895687]\n",
      "epoch:8 step:8132 [D loss: 0.719647, acc.: 52.34%] [G loss: 0.773759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8133 [D loss: 0.723047, acc.: 43.75%] [G loss: 0.777419]\n",
      "epoch:8 step:8134 [D loss: 0.687110, acc.: 57.03%] [G loss: 0.849447]\n",
      "epoch:8 step:8135 [D loss: 0.682009, acc.: 53.91%] [G loss: 0.822287]\n",
      "epoch:8 step:8136 [D loss: 0.675292, acc.: 53.91%] [G loss: 0.853315]\n",
      "epoch:8 step:8137 [D loss: 0.683097, acc.: 51.56%] [G loss: 0.904680]\n",
      "epoch:8 step:8138 [D loss: 0.697944, acc.: 53.12%] [G loss: 0.854226]\n",
      "epoch:8 step:8139 [D loss: 0.687914, acc.: 49.22%] [G loss: 0.874755]\n",
      "epoch:8 step:8140 [D loss: 0.664744, acc.: 59.38%] [G loss: 0.862916]\n",
      "epoch:8 step:8141 [D loss: 0.673349, acc.: 53.91%] [G loss: 0.806403]\n",
      "epoch:8 step:8142 [D loss: 0.927046, acc.: 33.59%] [G loss: 0.800876]\n",
      "epoch:8 step:8143 [D loss: 0.663385, acc.: 64.84%] [G loss: 0.836145]\n",
      "epoch:8 step:8144 [D loss: 0.693546, acc.: 55.47%] [G loss: 0.809446]\n",
      "epoch:8 step:8145 [D loss: 0.679965, acc.: 53.91%] [G loss: 0.843727]\n",
      "epoch:8 step:8146 [D loss: 0.658948, acc.: 61.72%] [G loss: 0.816867]\n",
      "epoch:8 step:8147 [D loss: 0.699189, acc.: 52.34%] [G loss: 0.757608]\n",
      "epoch:8 step:8148 [D loss: 0.655691, acc.: 65.62%] [G loss: 0.813159]\n",
      "epoch:8 step:8149 [D loss: 0.680947, acc.: 53.12%] [G loss: 0.770663]\n",
      "epoch:8 step:8150 [D loss: 0.676995, acc.: 52.34%] [G loss: 0.764948]\n",
      "epoch:8 step:8151 [D loss: 0.678092, acc.: 55.47%] [G loss: 0.771587]\n",
      "epoch:8 step:8152 [D loss: 0.689553, acc.: 55.47%] [G loss: 0.796556]\n",
      "epoch:8 step:8153 [D loss: 0.669766, acc.: 58.59%] [G loss: 0.757938]\n",
      "epoch:8 step:8154 [D loss: 0.699186, acc.: 51.56%] [G loss: 0.720341]\n",
      "epoch:8 step:8155 [D loss: 0.708632, acc.: 58.59%] [G loss: 0.849881]\n",
      "epoch:8 step:8156 [D loss: 0.676606, acc.: 60.16%] [G loss: 0.759668]\n",
      "epoch:8 step:8157 [D loss: 0.694628, acc.: 52.34%] [G loss: 0.793990]\n",
      "epoch:8 step:8158 [D loss: 0.669994, acc.: 61.72%] [G loss: 0.777685]\n",
      "epoch:8 step:8159 [D loss: 0.674927, acc.: 57.03%] [G loss: 0.796659]\n",
      "epoch:8 step:8160 [D loss: 0.657312, acc.: 62.50%] [G loss: 0.742550]\n",
      "epoch:8 step:8161 [D loss: 0.688606, acc.: 60.16%] [G loss: 0.720325]\n",
      "epoch:8 step:8162 [D loss: 0.713928, acc.: 47.66%] [G loss: 0.771921]\n",
      "epoch:8 step:8163 [D loss: 0.709847, acc.: 46.88%] [G loss: 0.758558]\n",
      "epoch:8 step:8164 [D loss: 0.689119, acc.: 53.12%] [G loss: 0.726852]\n",
      "epoch:8 step:8165 [D loss: 0.647463, acc.: 64.84%] [G loss: 0.768151]\n",
      "epoch:8 step:8166 [D loss: 0.665020, acc.: 59.38%] [G loss: 0.859206]\n",
      "epoch:8 step:8167 [D loss: 0.698669, acc.: 46.09%] [G loss: 0.782616]\n",
      "epoch:8 step:8168 [D loss: 0.656725, acc.: 62.50%] [G loss: 0.787308]\n",
      "epoch:8 step:8169 [D loss: 0.695972, acc.: 51.56%] [G loss: 0.787872]\n",
      "epoch:8 step:8170 [D loss: 0.665103, acc.: 58.59%] [G loss: 0.797446]\n",
      "epoch:8 step:8171 [D loss: 0.667065, acc.: 61.72%] [G loss: 0.763747]\n",
      "epoch:8 step:8172 [D loss: 0.658272, acc.: 63.28%] [G loss: 0.827257]\n",
      "epoch:8 step:8173 [D loss: 0.677993, acc.: 52.34%] [G loss: 0.869557]\n",
      "epoch:8 step:8174 [D loss: 0.662115, acc.: 61.72%] [G loss: 0.874355]\n",
      "epoch:8 step:8175 [D loss: 0.654200, acc.: 63.28%] [G loss: 0.923925]\n",
      "epoch:8 step:8176 [D loss: 0.681168, acc.: 57.03%] [G loss: 0.832845]\n",
      "epoch:8 step:8177 [D loss: 0.705291, acc.: 52.34%] [G loss: 0.886114]\n",
      "epoch:8 step:8178 [D loss: 0.665173, acc.: 55.47%] [G loss: 0.824126]\n",
      "epoch:8 step:8179 [D loss: 0.692682, acc.: 54.69%] [G loss: 0.806321]\n",
      "epoch:8 step:8180 [D loss: 0.658344, acc.: 59.38%] [G loss: 0.794374]\n",
      "epoch:8 step:8181 [D loss: 0.720570, acc.: 43.75%] [G loss: 0.751925]\n",
      "epoch:8 step:8182 [D loss: 0.703599, acc.: 50.78%] [G loss: 0.812779]\n",
      "epoch:8 step:8183 [D loss: 0.693985, acc.: 53.91%] [G loss: 0.802348]\n",
      "epoch:8 step:8184 [D loss: 0.677853, acc.: 58.59%] [G loss: 0.749709]\n",
      "epoch:8 step:8185 [D loss: 0.716355, acc.: 43.75%] [G loss: 0.762608]\n",
      "epoch:8 step:8186 [D loss: 0.669273, acc.: 51.56%] [G loss: 0.805550]\n",
      "epoch:8 step:8187 [D loss: 0.681554, acc.: 52.34%] [G loss: 0.782854]\n",
      "epoch:8 step:8188 [D loss: 0.687408, acc.: 52.34%] [G loss: 0.780230]\n",
      "epoch:8 step:8189 [D loss: 0.733895, acc.: 46.09%] [G loss: 0.804093]\n",
      "epoch:8 step:8190 [D loss: 0.691210, acc.: 50.78%] [G loss: 0.797736]\n",
      "epoch:8 step:8191 [D loss: 0.666371, acc.: 63.28%] [G loss: 0.804276]\n",
      "epoch:8 step:8192 [D loss: 0.682002, acc.: 54.69%] [G loss: 0.742066]\n",
      "epoch:8 step:8193 [D loss: 0.683225, acc.: 53.12%] [G loss: 0.768805]\n",
      "epoch:8 step:8194 [D loss: 0.682665, acc.: 52.34%] [G loss: 0.721875]\n",
      "epoch:8 step:8195 [D loss: 0.704053, acc.: 46.09%] [G loss: 0.774933]\n",
      "epoch:8 step:8196 [D loss: 0.688769, acc.: 55.47%] [G loss: 0.784997]\n",
      "epoch:8 step:8197 [D loss: 0.668274, acc.: 61.72%] [G loss: 0.790231]\n",
      "epoch:8 step:8198 [D loss: 0.675732, acc.: 53.91%] [G loss: 0.810309]\n",
      "epoch:8 step:8199 [D loss: 0.698790, acc.: 50.78%] [G loss: 0.813872]\n",
      "epoch:8 step:8200 [D loss: 0.702877, acc.: 45.31%] [G loss: 0.771158]\n",
      "epoch:8 step:8201 [D loss: 0.705467, acc.: 53.91%] [G loss: 0.758259]\n",
      "epoch:8 step:8202 [D loss: 0.666052, acc.: 52.34%] [G loss: 0.779639]\n",
      "epoch:8 step:8203 [D loss: 0.652736, acc.: 60.16%] [G loss: 0.780899]\n",
      "epoch:8 step:8204 [D loss: 0.679792, acc.: 60.94%] [G loss: 0.792290]\n",
      "epoch:8 step:8205 [D loss: 0.664062, acc.: 63.28%] [G loss: 0.801036]\n",
      "epoch:8 step:8206 [D loss: 0.656667, acc.: 64.84%] [G loss: 0.755789]\n",
      "epoch:8 step:8207 [D loss: 0.685214, acc.: 55.47%] [G loss: 0.940491]\n",
      "epoch:8 step:8208 [D loss: 0.704191, acc.: 51.56%] [G loss: 0.806641]\n",
      "epoch:8 step:8209 [D loss: 0.716847, acc.: 54.69%] [G loss: 0.826666]\n",
      "epoch:8 step:8210 [D loss: 0.645149, acc.: 67.19%] [G loss: 0.796655]\n",
      "epoch:8 step:8211 [D loss: 0.710829, acc.: 51.56%] [G loss: 0.811865]\n",
      "epoch:8 step:8212 [D loss: 0.685732, acc.: 59.38%] [G loss: 0.876933]\n",
      "epoch:8 step:8213 [D loss: 0.675990, acc.: 57.81%] [G loss: 0.800000]\n",
      "epoch:8 step:8214 [D loss: 0.645927, acc.: 61.72%] [G loss: 0.803487]\n",
      "epoch:8 step:8215 [D loss: 0.675449, acc.: 53.12%] [G loss: 0.805625]\n",
      "epoch:8 step:8216 [D loss: 0.701640, acc.: 45.31%] [G loss: 0.861935]\n",
      "epoch:8 step:8217 [D loss: 0.658260, acc.: 57.81%] [G loss: 0.786169]\n",
      "epoch:8 step:8218 [D loss: 0.699579, acc.: 50.78%] [G loss: 0.835881]\n",
      "epoch:8 step:8219 [D loss: 0.674356, acc.: 59.38%] [G loss: 0.863030]\n",
      "epoch:8 step:8220 [D loss: 0.668997, acc.: 57.81%] [G loss: 0.867500]\n",
      "epoch:8 step:8221 [D loss: 0.699461, acc.: 52.34%] [G loss: 0.818027]\n",
      "epoch:8 step:8222 [D loss: 0.691113, acc.: 57.03%] [G loss: 0.832461]\n",
      "epoch:8 step:8223 [D loss: 0.738720, acc.: 41.41%] [G loss: 0.794285]\n",
      "epoch:8 step:8224 [D loss: 0.709546, acc.: 50.00%] [G loss: 0.795418]\n",
      "epoch:8 step:8225 [D loss: 0.656514, acc.: 62.50%] [G loss: 0.775100]\n",
      "epoch:8 step:8226 [D loss: 0.713045, acc.: 48.44%] [G loss: 0.745338]\n",
      "epoch:8 step:8227 [D loss: 0.668260, acc.: 63.28%] [G loss: 0.848458]\n",
      "epoch:8 step:8228 [D loss: 0.632407, acc.: 67.19%] [G loss: 0.854518]\n",
      "epoch:8 step:8229 [D loss: 0.656223, acc.: 57.81%] [G loss: 0.770423]\n",
      "epoch:8 step:8230 [D loss: 0.681614, acc.: 60.16%] [G loss: 0.795738]\n",
      "epoch:8 step:8231 [D loss: 0.703566, acc.: 50.78%] [G loss: 0.778315]\n",
      "epoch:8 step:8232 [D loss: 0.648022, acc.: 62.50%] [G loss: 0.883514]\n",
      "epoch:8 step:8233 [D loss: 0.676628, acc.: 52.34%] [G loss: 0.858727]\n",
      "epoch:8 step:8234 [D loss: 0.687993, acc.: 54.69%] [G loss: 0.877303]\n",
      "epoch:8 step:8235 [D loss: 0.702961, acc.: 47.66%] [G loss: 0.780512]\n",
      "epoch:8 step:8236 [D loss: 0.751695, acc.: 37.50%] [G loss: 0.837035]\n",
      "epoch:8 step:8237 [D loss: 0.659519, acc.: 57.03%] [G loss: 0.764723]\n",
      "epoch:8 step:8238 [D loss: 0.694469, acc.: 57.03%] [G loss: 0.788146]\n",
      "epoch:8 step:8239 [D loss: 0.681786, acc.: 50.00%] [G loss: 0.777022]\n",
      "epoch:8 step:8240 [D loss: 0.686713, acc.: 60.94%] [G loss: 0.765442]\n",
      "epoch:8 step:8241 [D loss: 0.685776, acc.: 48.44%] [G loss: 0.748529]\n",
      "epoch:8 step:8242 [D loss: 0.729659, acc.: 38.28%] [G loss: 0.748451]\n",
      "epoch:8 step:8243 [D loss: 0.698256, acc.: 49.22%] [G loss: 0.721791]\n",
      "epoch:8 step:8244 [D loss: 0.647303, acc.: 56.25%] [G loss: 0.777431]\n",
      "epoch:8 step:8245 [D loss: 0.669186, acc.: 61.72%] [G loss: 0.805936]\n",
      "epoch:8 step:8246 [D loss: 0.682970, acc.: 52.34%] [G loss: 0.783151]\n",
      "epoch:8 step:8247 [D loss: 0.658292, acc.: 54.69%] [G loss: 0.795214]\n",
      "epoch:8 step:8248 [D loss: 0.688215, acc.: 50.78%] [G loss: 0.734597]\n",
      "epoch:8 step:8249 [D loss: 0.663506, acc.: 68.75%] [G loss: 0.802876]\n",
      "epoch:8 step:8250 [D loss: 0.688761, acc.: 55.47%] [G loss: 0.827843]\n",
      "epoch:8 step:8251 [D loss: 0.690900, acc.: 50.78%] [G loss: 0.786852]\n",
      "epoch:8 step:8252 [D loss: 0.670816, acc.: 53.12%] [G loss: 0.745255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8253 [D loss: 0.676368, acc.: 57.81%] [G loss: 0.716517]\n",
      "epoch:8 step:8254 [D loss: 0.635771, acc.: 69.53%] [G loss: 0.807007]\n",
      "epoch:8 step:8255 [D loss: 0.685600, acc.: 54.69%] [G loss: 0.750857]\n",
      "epoch:8 step:8256 [D loss: 0.657413, acc.: 60.94%] [G loss: 0.740557]\n",
      "epoch:8 step:8257 [D loss: 0.685030, acc.: 58.59%] [G loss: 0.728331]\n",
      "epoch:8 step:8258 [D loss: 0.665245, acc.: 61.72%] [G loss: 0.776146]\n",
      "epoch:8 step:8259 [D loss: 0.623032, acc.: 64.06%] [G loss: 0.859314]\n",
      "epoch:8 step:8260 [D loss: 0.677248, acc.: 55.47%] [G loss: 0.804926]\n",
      "epoch:8 step:8261 [D loss: 0.679883, acc.: 57.81%] [G loss: 0.887521]\n",
      "epoch:8 step:8262 [D loss: 0.647946, acc.: 61.72%] [G loss: 0.884443]\n",
      "epoch:8 step:8263 [D loss: 0.684444, acc.: 57.81%] [G loss: 0.851458]\n",
      "epoch:8 step:8264 [D loss: 0.651823, acc.: 64.06%] [G loss: 0.867498]\n",
      "epoch:8 step:8265 [D loss: 0.689851, acc.: 50.00%] [G loss: 0.823610]\n",
      "epoch:8 step:8266 [D loss: 0.694819, acc.: 59.38%] [G loss: 0.756264]\n",
      "epoch:8 step:8267 [D loss: 0.685183, acc.: 56.25%] [G loss: 0.791812]\n",
      "epoch:8 step:8268 [D loss: 0.730064, acc.: 43.75%] [G loss: 0.849899]\n",
      "epoch:8 step:8269 [D loss: 0.692998, acc.: 50.00%] [G loss: 0.749722]\n",
      "epoch:8 step:8270 [D loss: 0.702423, acc.: 50.00%] [G loss: 0.810160]\n",
      "epoch:8 step:8271 [D loss: 0.684643, acc.: 60.16%] [G loss: 0.870535]\n",
      "epoch:8 step:8272 [D loss: 0.658274, acc.: 55.47%] [G loss: 0.791742]\n",
      "epoch:8 step:8273 [D loss: 0.724356, acc.: 50.00%] [G loss: 0.725531]\n",
      "epoch:8 step:8274 [D loss: 0.713726, acc.: 47.66%] [G loss: 0.836760]\n",
      "epoch:8 step:8275 [D loss: 0.710065, acc.: 46.09%] [G loss: 0.869692]\n",
      "epoch:8 step:8276 [D loss: 0.716412, acc.: 45.31%] [G loss: 0.860825]\n",
      "epoch:8 step:8277 [D loss: 0.657778, acc.: 57.03%] [G loss: 0.819623]\n",
      "epoch:8 step:8278 [D loss: 0.650213, acc.: 64.06%] [G loss: 0.850038]\n",
      "epoch:8 step:8279 [D loss: 0.656126, acc.: 57.03%] [G loss: 0.828605]\n",
      "epoch:8 step:8280 [D loss: 0.689336, acc.: 51.56%] [G loss: 0.817092]\n",
      "epoch:8 step:8281 [D loss: 0.738812, acc.: 36.72%] [G loss: 0.786084]\n",
      "epoch:8 step:8282 [D loss: 0.641042, acc.: 67.97%] [G loss: 0.840608]\n",
      "epoch:8 step:8283 [D loss: 0.676155, acc.: 57.03%] [G loss: 0.810481]\n",
      "epoch:8 step:8284 [D loss: 0.678066, acc.: 53.12%] [G loss: 0.753016]\n",
      "epoch:8 step:8285 [D loss: 0.672573, acc.: 57.03%] [G loss: 0.733605]\n",
      "epoch:8 step:8286 [D loss: 0.680872, acc.: 57.81%] [G loss: 0.794186]\n",
      "epoch:8 step:8287 [D loss: 0.688020, acc.: 57.81%] [G loss: 0.746608]\n",
      "epoch:8 step:8288 [D loss: 0.653354, acc.: 64.84%] [G loss: 0.800010]\n",
      "epoch:8 step:8289 [D loss: 0.675750, acc.: 57.03%] [G loss: 0.796094]\n",
      "epoch:8 step:8290 [D loss: 0.675289, acc.: 54.69%] [G loss: 0.822707]\n",
      "epoch:8 step:8291 [D loss: 0.674177, acc.: 50.78%] [G loss: 0.825309]\n",
      "epoch:8 step:8292 [D loss: 0.693128, acc.: 47.66%] [G loss: 0.806948]\n",
      "epoch:8 step:8293 [D loss: 0.699107, acc.: 54.69%] [G loss: 0.762768]\n",
      "epoch:8 step:8294 [D loss: 0.687024, acc.: 51.56%] [G loss: 0.792929]\n",
      "epoch:8 step:8295 [D loss: 0.693303, acc.: 47.66%] [G loss: 0.728111]\n",
      "epoch:8 step:8296 [D loss: 0.672380, acc.: 55.47%] [G loss: 0.759156]\n",
      "epoch:8 step:8297 [D loss: 0.667407, acc.: 53.12%] [G loss: 0.792589]\n",
      "epoch:8 step:8298 [D loss: 0.703039, acc.: 53.12%] [G loss: 0.734226]\n",
      "epoch:8 step:8299 [D loss: 0.679533, acc.: 53.91%] [G loss: 0.799436]\n",
      "epoch:8 step:8300 [D loss: 0.703819, acc.: 50.00%] [G loss: 0.753103]\n",
      "epoch:8 step:8301 [D loss: 0.673012, acc.: 61.72%] [G loss: 0.767387]\n",
      "epoch:8 step:8302 [D loss: 0.747489, acc.: 47.66%] [G loss: 0.768356]\n",
      "epoch:8 step:8303 [D loss: 0.667724, acc.: 62.50%] [G loss: 0.829681]\n",
      "epoch:8 step:8304 [D loss: 0.676515, acc.: 51.56%] [G loss: 0.724775]\n",
      "epoch:8 step:8305 [D loss: 0.695990, acc.: 52.34%] [G loss: 0.802812]\n",
      "epoch:8 step:8306 [D loss: 0.664838, acc.: 64.84%] [G loss: 0.770469]\n",
      "epoch:8 step:8307 [D loss: 0.706442, acc.: 44.53%] [G loss: 0.850236]\n",
      "epoch:8 step:8308 [D loss: 0.679647, acc.: 58.59%] [G loss: 0.754319]\n",
      "epoch:8 step:8309 [D loss: 0.705900, acc.: 52.34%] [G loss: 0.788810]\n",
      "epoch:8 step:8310 [D loss: 0.699056, acc.: 53.91%] [G loss: 0.808712]\n",
      "epoch:8 step:8311 [D loss: 0.682051, acc.: 53.91%] [G loss: 0.802934]\n",
      "epoch:8 step:8312 [D loss: 0.642774, acc.: 60.94%] [G loss: 0.796022]\n",
      "epoch:8 step:8313 [D loss: 0.691805, acc.: 51.56%] [G loss: 0.798949]\n",
      "epoch:8 step:8314 [D loss: 0.693492, acc.: 53.12%] [G loss: 0.844239]\n",
      "epoch:8 step:8315 [D loss: 0.687994, acc.: 57.03%] [G loss: 0.827945]\n",
      "epoch:8 step:8316 [D loss: 0.679079, acc.: 55.47%] [G loss: 0.855943]\n",
      "epoch:8 step:8317 [D loss: 0.637083, acc.: 60.94%] [G loss: 0.846528]\n",
      "epoch:8 step:8318 [D loss: 0.669653, acc.: 55.47%] [G loss: 0.785635]\n",
      "epoch:8 step:8319 [D loss: 0.701083, acc.: 53.12%] [G loss: 0.807932]\n",
      "epoch:8 step:8320 [D loss: 0.696007, acc.: 53.12%] [G loss: 0.792176]\n",
      "epoch:8 step:8321 [D loss: 0.691866, acc.: 44.53%] [G loss: 0.773976]\n",
      "epoch:8 step:8322 [D loss: 0.674970, acc.: 66.41%] [G loss: 0.742327]\n",
      "epoch:8 step:8323 [D loss: 0.699337, acc.: 55.47%] [G loss: 0.745479]\n",
      "epoch:8 step:8324 [D loss: 0.702098, acc.: 48.44%] [G loss: 0.784279]\n",
      "epoch:8 step:8325 [D loss: 0.675494, acc.: 57.81%] [G loss: 0.726872]\n",
      "epoch:8 step:8326 [D loss: 0.707848, acc.: 46.09%] [G loss: 0.714053]\n",
      "epoch:8 step:8327 [D loss: 0.709650, acc.: 46.09%] [G loss: 0.765284]\n",
      "epoch:8 step:8328 [D loss: 0.687010, acc.: 56.25%] [G loss: 0.751184]\n",
      "epoch:8 step:8329 [D loss: 0.680944, acc.: 53.91%] [G loss: 0.778811]\n",
      "epoch:8 step:8330 [D loss: 0.673355, acc.: 57.03%] [G loss: 0.770145]\n",
      "epoch:8 step:8331 [D loss: 0.683745, acc.: 57.03%] [G loss: 0.804373]\n",
      "epoch:8 step:8332 [D loss: 0.677108, acc.: 56.25%] [G loss: 0.771531]\n",
      "epoch:8 step:8333 [D loss: 0.690040, acc.: 45.31%] [G loss: 0.721865]\n",
      "epoch:8 step:8334 [D loss: 0.677719, acc.: 57.03%] [G loss: 0.802567]\n",
      "epoch:8 step:8335 [D loss: 0.678909, acc.: 51.56%] [G loss: 0.803528]\n",
      "epoch:8 step:8336 [D loss: 0.677818, acc.: 58.59%] [G loss: 0.712032]\n",
      "epoch:8 step:8337 [D loss: 0.710472, acc.: 53.12%] [G loss: 0.782870]\n",
      "epoch:8 step:8338 [D loss: 0.681852, acc.: 52.34%] [G loss: 0.820377]\n",
      "epoch:8 step:8339 [D loss: 0.703771, acc.: 46.88%] [G loss: 0.840270]\n",
      "epoch:8 step:8340 [D loss: 0.709351, acc.: 46.09%] [G loss: 0.781710]\n",
      "epoch:8 step:8341 [D loss: 0.693223, acc.: 50.78%] [G loss: 0.752448]\n",
      "epoch:8 step:8342 [D loss: 0.672925, acc.: 60.16%] [G loss: 0.779204]\n",
      "epoch:8 step:8343 [D loss: 0.627687, acc.: 64.84%] [G loss: 0.807094]\n",
      "epoch:8 step:8344 [D loss: 0.663201, acc.: 56.25%] [G loss: 0.872772]\n",
      "epoch:8 step:8345 [D loss: 0.665659, acc.: 63.28%] [G loss: 0.810186]\n",
      "epoch:8 step:8346 [D loss: 0.670951, acc.: 59.38%] [G loss: 0.835841]\n",
      "epoch:8 step:8347 [D loss: 0.676099, acc.: 53.91%] [G loss: 0.814493]\n",
      "epoch:8 step:8348 [D loss: 0.669977, acc.: 62.50%] [G loss: 0.750036]\n",
      "epoch:8 step:8349 [D loss: 0.718976, acc.: 50.78%] [G loss: 0.851007]\n",
      "epoch:8 step:8350 [D loss: 0.658763, acc.: 68.75%] [G loss: 0.839993]\n",
      "epoch:8 step:8351 [D loss: 0.679572, acc.: 52.34%] [G loss: 0.809513]\n",
      "epoch:8 step:8352 [D loss: 0.648151, acc.: 64.84%] [G loss: 0.799571]\n",
      "epoch:8 step:8353 [D loss: 0.687388, acc.: 53.12%] [G loss: 0.808239]\n",
      "epoch:8 step:8354 [D loss: 0.680647, acc.: 57.81%] [G loss: 0.757283]\n",
      "epoch:8 step:8355 [D loss: 0.668718, acc.: 60.94%] [G loss: 0.788350]\n",
      "epoch:8 step:8356 [D loss: 0.702963, acc.: 53.12%] [G loss: 0.787107]\n",
      "epoch:8 step:8357 [D loss: 0.667046, acc.: 59.38%] [G loss: 0.792221]\n",
      "epoch:8 step:8358 [D loss: 0.674200, acc.: 57.03%] [G loss: 0.826374]\n",
      "epoch:8 step:8359 [D loss: 0.685086, acc.: 56.25%] [G loss: 0.845385]\n",
      "epoch:8 step:8360 [D loss: 0.670849, acc.: 58.59%] [G loss: 0.720965]\n",
      "epoch:8 step:8361 [D loss: 0.676809, acc.: 57.03%] [G loss: 0.793342]\n",
      "epoch:8 step:8362 [D loss: 0.659139, acc.: 62.50%] [G loss: 0.785526]\n",
      "epoch:8 step:8363 [D loss: 0.688288, acc.: 57.03%] [G loss: 0.810524]\n",
      "epoch:8 step:8364 [D loss: 0.654838, acc.: 57.81%] [G loss: 0.838907]\n",
      "epoch:8 step:8365 [D loss: 0.700781, acc.: 50.00%] [G loss: 0.835827]\n",
      "epoch:8 step:8366 [D loss: 0.642937, acc.: 64.84%] [G loss: 0.738722]\n",
      "epoch:8 step:8367 [D loss: 0.692358, acc.: 47.66%] [G loss: 0.794125]\n",
      "epoch:8 step:8368 [D loss: 0.691743, acc.: 52.34%] [G loss: 0.840584]\n",
      "epoch:8 step:8369 [D loss: 0.673004, acc.: 57.81%] [G loss: 0.820049]\n",
      "epoch:8 step:8370 [D loss: 0.695200, acc.: 53.12%] [G loss: 0.870193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8371 [D loss: 0.699414, acc.: 53.12%] [G loss: 0.855437]\n",
      "epoch:8 step:8372 [D loss: 0.657237, acc.: 60.16%] [G loss: 0.783332]\n",
      "epoch:8 step:8373 [D loss: 0.682710, acc.: 53.12%] [G loss: 0.829510]\n",
      "epoch:8 step:8374 [D loss: 0.696069, acc.: 53.12%] [G loss: 0.867064]\n",
      "epoch:8 step:8375 [D loss: 0.667268, acc.: 56.25%] [G loss: 0.828249]\n",
      "epoch:8 step:8376 [D loss: 0.651719, acc.: 61.72%] [G loss: 0.835984]\n",
      "epoch:8 step:8377 [D loss: 0.725356, acc.: 49.22%] [G loss: 0.792758]\n",
      "epoch:8 step:8378 [D loss: 0.666909, acc.: 60.16%] [G loss: 0.772154]\n",
      "epoch:8 step:8379 [D loss: 0.678751, acc.: 55.47%] [G loss: 0.802878]\n",
      "epoch:8 step:8380 [D loss: 0.664017, acc.: 54.69%] [G loss: 0.837250]\n",
      "epoch:8 step:8381 [D loss: 0.663408, acc.: 64.06%] [G loss: 0.835848]\n",
      "epoch:8 step:8382 [D loss: 0.684973, acc.: 54.69%] [G loss: 0.772178]\n",
      "epoch:8 step:8383 [D loss: 0.654219, acc.: 60.16%] [G loss: 0.815790]\n",
      "epoch:8 step:8384 [D loss: 0.653133, acc.: 59.38%] [G loss: 0.850691]\n",
      "epoch:8 step:8385 [D loss: 0.673587, acc.: 53.12%] [G loss: 0.775251]\n",
      "epoch:8 step:8386 [D loss: 0.678579, acc.: 54.69%] [G loss: 0.852533]\n",
      "epoch:8 step:8387 [D loss: 0.662136, acc.: 57.03%] [G loss: 0.795722]\n",
      "epoch:8 step:8388 [D loss: 0.679155, acc.: 57.03%] [G loss: 0.828267]\n",
      "epoch:8 step:8389 [D loss: 0.671333, acc.: 57.81%] [G loss: 0.746777]\n",
      "epoch:8 step:8390 [D loss: 0.741718, acc.: 46.09%] [G loss: 0.811686]\n",
      "epoch:8 step:8391 [D loss: 0.644271, acc.: 62.50%] [G loss: 0.794391]\n",
      "epoch:8 step:8392 [D loss: 0.703742, acc.: 49.22%] [G loss: 0.880443]\n",
      "epoch:8 step:8393 [D loss: 0.703213, acc.: 53.12%] [G loss: 0.810355]\n",
      "epoch:8 step:8394 [D loss: 0.643479, acc.: 55.47%] [G loss: 0.794395]\n",
      "epoch:8 step:8395 [D loss: 0.663219, acc.: 57.03%] [G loss: 0.848506]\n",
      "epoch:8 step:8396 [D loss: 0.667052, acc.: 58.59%] [G loss: 0.881614]\n",
      "epoch:8 step:8397 [D loss: 0.700173, acc.: 52.34%] [G loss: 0.818431]\n",
      "epoch:8 step:8398 [D loss: 0.674831, acc.: 56.25%] [G loss: 0.811870]\n",
      "epoch:8 step:8399 [D loss: 0.674090, acc.: 54.69%] [G loss: 0.849253]\n",
      "epoch:8 step:8400 [D loss: 0.673356, acc.: 56.25%] [G loss: 0.802051]\n",
      "epoch:8 step:8401 [D loss: 0.682706, acc.: 56.25%] [G loss: 0.816824]\n",
      "epoch:8 step:8402 [D loss: 0.674276, acc.: 54.69%] [G loss: 0.776568]\n",
      "epoch:8 step:8403 [D loss: 0.684365, acc.: 60.16%] [G loss: 0.788080]\n",
      "epoch:8 step:8404 [D loss: 0.684554, acc.: 51.56%] [G loss: 0.793181]\n",
      "epoch:8 step:8405 [D loss: 0.666072, acc.: 59.38%] [G loss: 0.789323]\n",
      "epoch:8 step:8406 [D loss: 0.712279, acc.: 52.34%] [G loss: 0.808385]\n",
      "epoch:8 step:8407 [D loss: 0.714700, acc.: 53.91%] [G loss: 0.788777]\n",
      "epoch:8 step:8408 [D loss: 0.699038, acc.: 48.44%] [G loss: 0.881048]\n",
      "epoch:8 step:8409 [D loss: 0.659732, acc.: 64.06%] [G loss: 0.876756]\n",
      "epoch:8 step:8410 [D loss: 0.672518, acc.: 59.38%] [G loss: 0.830340]\n",
      "epoch:8 step:8411 [D loss: 0.686872, acc.: 53.12%] [G loss: 0.879422]\n",
      "epoch:8 step:8412 [D loss: 0.668212, acc.: 49.22%] [G loss: 0.793857]\n",
      "epoch:8 step:8413 [D loss: 0.687148, acc.: 54.69%] [G loss: 0.848456]\n",
      "epoch:8 step:8414 [D loss: 0.692046, acc.: 50.78%] [G loss: 0.821233]\n",
      "epoch:8 step:8415 [D loss: 0.704312, acc.: 49.22%] [G loss: 0.778528]\n",
      "epoch:8 step:8416 [D loss: 0.674972, acc.: 49.22%] [G loss: 0.823357]\n",
      "epoch:8 step:8417 [D loss: 0.668334, acc.: 63.28%] [G loss: 0.818032]\n",
      "epoch:8 step:8418 [D loss: 0.662538, acc.: 63.28%] [G loss: 0.790940]\n",
      "epoch:8 step:8419 [D loss: 0.640081, acc.: 64.84%] [G loss: 0.763353]\n",
      "epoch:8 step:8420 [D loss: 0.690660, acc.: 53.12%] [G loss: 0.844586]\n",
      "epoch:8 step:8421 [D loss: 0.709473, acc.: 50.78%] [G loss: 0.853806]\n",
      "epoch:8 step:8422 [D loss: 0.681406, acc.: 54.69%] [G loss: 0.820034]\n",
      "epoch:8 step:8423 [D loss: 0.674042, acc.: 53.12%] [G loss: 0.870539]\n",
      "epoch:8 step:8424 [D loss: 0.651887, acc.: 60.16%] [G loss: 0.820051]\n",
      "epoch:8 step:8425 [D loss: 0.719208, acc.: 56.25%] [G loss: 0.806859]\n",
      "epoch:8 step:8426 [D loss: 0.661516, acc.: 57.03%] [G loss: 0.825723]\n",
      "epoch:8 step:8427 [D loss: 0.689470, acc.: 50.00%] [G loss: 0.773419]\n",
      "epoch:8 step:8428 [D loss: 0.726903, acc.: 53.91%] [G loss: 0.717995]\n",
      "epoch:8 step:8429 [D loss: 0.677612, acc.: 57.81%] [G loss: 0.792511]\n",
      "epoch:8 step:8430 [D loss: 0.636007, acc.: 71.09%] [G loss: 0.836765]\n",
      "epoch:8 step:8431 [D loss: 0.673093, acc.: 59.38%] [G loss: 0.823983]\n",
      "epoch:8 step:8432 [D loss: 0.688522, acc.: 54.69%] [G loss: 0.809780]\n",
      "epoch:8 step:8433 [D loss: 0.656444, acc.: 59.38%] [G loss: 0.780162]\n",
      "epoch:9 step:8434 [D loss: 0.692799, acc.: 59.38%] [G loss: 0.758042]\n",
      "epoch:9 step:8435 [D loss: 0.674635, acc.: 56.25%] [G loss: 0.766810]\n",
      "epoch:9 step:8436 [D loss: 0.713253, acc.: 49.22%] [G loss: 0.774018]\n",
      "epoch:9 step:8437 [D loss: 0.710969, acc.: 56.25%] [G loss: 0.832292]\n",
      "epoch:9 step:8438 [D loss: 0.655808, acc.: 62.50%] [G loss: 0.765537]\n",
      "epoch:9 step:8439 [D loss: 0.643831, acc.: 58.59%] [G loss: 0.779842]\n",
      "epoch:9 step:8440 [D loss: 0.691942, acc.: 55.47%] [G loss: 0.759190]\n",
      "epoch:9 step:8441 [D loss: 0.686030, acc.: 58.59%] [G loss: 0.761223]\n",
      "epoch:9 step:8442 [D loss: 0.683366, acc.: 57.81%] [G loss: 0.781664]\n",
      "epoch:9 step:8443 [D loss: 0.654739, acc.: 59.38%] [G loss: 0.805075]\n",
      "epoch:9 step:8444 [D loss: 0.642172, acc.: 62.50%] [G loss: 0.814338]\n",
      "epoch:9 step:8445 [D loss: 0.647010, acc.: 66.41%] [G loss: 0.788934]\n",
      "epoch:9 step:8446 [D loss: 0.703197, acc.: 50.00%] [G loss: 0.784109]\n",
      "epoch:9 step:8447 [D loss: 0.656535, acc.: 64.84%] [G loss: 0.824817]\n",
      "epoch:9 step:8448 [D loss: 0.678980, acc.: 57.03%] [G loss: 0.788113]\n",
      "epoch:9 step:8449 [D loss: 0.673717, acc.: 54.69%] [G loss: 0.826312]\n",
      "epoch:9 step:8450 [D loss: 0.645616, acc.: 58.59%] [G loss: 0.788849]\n",
      "epoch:9 step:8451 [D loss: 0.687062, acc.: 57.03%] [G loss: 0.761356]\n",
      "epoch:9 step:8452 [D loss: 0.728390, acc.: 50.00%] [G loss: 0.761878]\n",
      "epoch:9 step:8453 [D loss: 0.681024, acc.: 52.34%] [G loss: 0.795509]\n",
      "epoch:9 step:8454 [D loss: 0.662978, acc.: 61.72%] [G loss: 0.781154]\n",
      "epoch:9 step:8455 [D loss: 0.643618, acc.: 67.19%] [G loss: 0.834002]\n",
      "epoch:9 step:8456 [D loss: 0.666328, acc.: 55.47%] [G loss: 0.804327]\n",
      "epoch:9 step:8457 [D loss: 0.627108, acc.: 64.84%] [G loss: 0.755166]\n",
      "epoch:9 step:8458 [D loss: 0.720989, acc.: 57.81%] [G loss: 0.857746]\n",
      "epoch:9 step:8459 [D loss: 0.681666, acc.: 51.56%] [G loss: 0.817436]\n",
      "epoch:9 step:8460 [D loss: 0.678638, acc.: 59.38%] [G loss: 0.777866]\n",
      "epoch:9 step:8461 [D loss: 0.698890, acc.: 55.47%] [G loss: 0.818812]\n",
      "epoch:9 step:8462 [D loss: 0.702231, acc.: 49.22%] [G loss: 0.781254]\n",
      "epoch:9 step:8463 [D loss: 0.698416, acc.: 52.34%] [G loss: 0.826538]\n",
      "epoch:9 step:8464 [D loss: 0.739424, acc.: 40.62%] [G loss: 0.871238]\n",
      "epoch:9 step:8465 [D loss: 0.669743, acc.: 56.25%] [G loss: 0.862487]\n",
      "epoch:9 step:8466 [D loss: 0.672935, acc.: 54.69%] [G loss: 0.888464]\n",
      "epoch:9 step:8467 [D loss: 0.680422, acc.: 55.47%] [G loss: 0.874550]\n",
      "epoch:9 step:8468 [D loss: 0.681046, acc.: 53.12%] [G loss: 0.901302]\n",
      "epoch:9 step:8469 [D loss: 0.683693, acc.: 50.78%] [G loss: 0.870137]\n",
      "epoch:9 step:8470 [D loss: 0.664749, acc.: 57.81%] [G loss: 0.886663]\n",
      "epoch:9 step:8471 [D loss: 0.672147, acc.: 56.25%] [G loss: 0.813262]\n",
      "epoch:9 step:8472 [D loss: 0.681130, acc.: 50.00%] [G loss: 1.025714]\n",
      "epoch:9 step:8473 [D loss: 0.699774, acc.: 51.56%] [G loss: 0.909423]\n",
      "epoch:9 step:8474 [D loss: 0.691883, acc.: 52.34%] [G loss: 0.823230]\n",
      "epoch:9 step:8475 [D loss: 0.695209, acc.: 49.22%] [G loss: 0.783836]\n",
      "epoch:9 step:8476 [D loss: 0.689092, acc.: 50.78%] [G loss: 0.745762]\n",
      "epoch:9 step:8477 [D loss: 0.710720, acc.: 50.78%] [G loss: 0.773442]\n",
      "epoch:9 step:8478 [D loss: 0.682758, acc.: 58.59%] [G loss: 0.759004]\n",
      "epoch:9 step:8479 [D loss: 0.689809, acc.: 51.56%] [G loss: 0.763508]\n",
      "epoch:9 step:8480 [D loss: 0.670171, acc.: 56.25%] [G loss: 0.819823]\n",
      "epoch:9 step:8481 [D loss: 0.653417, acc.: 62.50%] [G loss: 0.805425]\n",
      "epoch:9 step:8482 [D loss: 0.668032, acc.: 56.25%] [G loss: 0.843878]\n",
      "epoch:9 step:8483 [D loss: 0.654554, acc.: 64.84%] [G loss: 0.851989]\n",
      "epoch:9 step:8484 [D loss: 0.648959, acc.: 63.28%] [G loss: 0.834325]\n",
      "epoch:9 step:8485 [D loss: 0.601518, acc.: 75.00%] [G loss: 0.800638]\n",
      "epoch:9 step:8486 [D loss: 0.650270, acc.: 62.50%] [G loss: 0.844941]\n",
      "epoch:9 step:8487 [D loss: 0.702434, acc.: 51.56%] [G loss: 0.741103]\n",
      "epoch:9 step:8488 [D loss: 0.705861, acc.: 48.44%] [G loss: 0.774703]\n",
      "epoch:9 step:8489 [D loss: 0.629418, acc.: 64.84%] [G loss: 0.713174]\n",
      "epoch:9 step:8490 [D loss: 0.700182, acc.: 56.25%] [G loss: 0.761399]\n",
      "epoch:9 step:8491 [D loss: 0.693763, acc.: 56.25%] [G loss: 0.746034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8492 [D loss: 0.670736, acc.: 55.47%] [G loss: 0.727587]\n",
      "epoch:9 step:8493 [D loss: 0.702201, acc.: 50.00%] [G loss: 0.756448]\n",
      "epoch:9 step:8494 [D loss: 0.703060, acc.: 53.12%] [G loss: 0.722823]\n",
      "epoch:9 step:8495 [D loss: 0.655622, acc.: 60.94%] [G loss: 0.757607]\n",
      "epoch:9 step:8496 [D loss: 0.664021, acc.: 60.16%] [G loss: 0.832462]\n",
      "epoch:9 step:8497 [D loss: 0.681145, acc.: 53.91%] [G loss: 0.815745]\n",
      "epoch:9 step:8498 [D loss: 0.690556, acc.: 54.69%] [G loss: 0.896144]\n",
      "epoch:9 step:8499 [D loss: 0.665375, acc.: 54.69%] [G loss: 0.831592]\n",
      "epoch:9 step:8500 [D loss: 0.700431, acc.: 45.31%] [G loss: 0.878847]\n",
      "epoch:9 step:8501 [D loss: 0.695785, acc.: 52.34%] [G loss: 0.854361]\n",
      "epoch:9 step:8502 [D loss: 0.640233, acc.: 62.50%] [G loss: 0.815661]\n",
      "epoch:9 step:8503 [D loss: 0.668215, acc.: 60.16%] [G loss: 0.795299]\n",
      "epoch:9 step:8504 [D loss: 0.684414, acc.: 51.56%] [G loss: 0.807149]\n",
      "epoch:9 step:8505 [D loss: 0.647070, acc.: 57.81%] [G loss: 0.876252]\n",
      "epoch:9 step:8506 [D loss: 0.642397, acc.: 63.28%] [G loss: 0.773587]\n",
      "epoch:9 step:8507 [D loss: 0.693818, acc.: 46.88%] [G loss: 0.781719]\n",
      "epoch:9 step:8508 [D loss: 0.696543, acc.: 52.34%] [G loss: 0.825781]\n",
      "epoch:9 step:8509 [D loss: 0.709794, acc.: 45.31%] [G loss: 0.815591]\n",
      "epoch:9 step:8510 [D loss: 0.692283, acc.: 47.66%] [G loss: 0.928720]\n",
      "epoch:9 step:8511 [D loss: 0.674691, acc.: 57.81%] [G loss: 0.777656]\n",
      "epoch:9 step:8512 [D loss: 0.679594, acc.: 52.34%] [G loss: 0.806179]\n",
      "epoch:9 step:8513 [D loss: 0.707095, acc.: 58.59%] [G loss: 0.756332]\n",
      "epoch:9 step:8514 [D loss: 0.697823, acc.: 50.00%] [G loss: 0.833562]\n",
      "epoch:9 step:8515 [D loss: 0.702743, acc.: 50.78%] [G loss: 0.802518]\n",
      "epoch:9 step:8516 [D loss: 0.675173, acc.: 55.47%] [G loss: 0.837980]\n",
      "epoch:9 step:8517 [D loss: 0.688114, acc.: 51.56%] [G loss: 0.871151]\n",
      "epoch:9 step:8518 [D loss: 0.667457, acc.: 60.16%] [G loss: 0.788467]\n",
      "epoch:9 step:8519 [D loss: 0.646954, acc.: 64.06%] [G loss: 0.865159]\n",
      "epoch:9 step:8520 [D loss: 0.655793, acc.: 64.06%] [G loss: 0.860028]\n",
      "epoch:9 step:8521 [D loss: 0.684599, acc.: 57.03%] [G loss: 0.814253]\n",
      "epoch:9 step:8522 [D loss: 0.674762, acc.: 57.03%] [G loss: 0.801018]\n",
      "epoch:9 step:8523 [D loss: 0.664251, acc.: 59.38%] [G loss: 0.772682]\n",
      "epoch:9 step:8524 [D loss: 0.697392, acc.: 50.00%] [G loss: 0.741080]\n",
      "epoch:9 step:8525 [D loss: 0.660681, acc.: 57.81%] [G loss: 0.786446]\n",
      "epoch:9 step:8526 [D loss: 0.695430, acc.: 57.81%] [G loss: 0.786513]\n",
      "epoch:9 step:8527 [D loss: 0.698562, acc.: 50.00%] [G loss: 0.795989]\n",
      "epoch:9 step:8528 [D loss: 0.678616, acc.: 53.12%] [G loss: 0.839427]\n",
      "epoch:9 step:8529 [D loss: 0.687417, acc.: 55.47%] [G loss: 0.773113]\n",
      "epoch:9 step:8530 [D loss: 0.641088, acc.: 66.41%] [G loss: 0.728437]\n",
      "epoch:9 step:8531 [D loss: 0.721905, acc.: 50.78%] [G loss: 0.842725]\n",
      "epoch:9 step:8532 [D loss: 0.733901, acc.: 51.56%] [G loss: 0.844529]\n",
      "epoch:9 step:8533 [D loss: 0.665304, acc.: 67.19%] [G loss: 0.783376]\n",
      "epoch:9 step:8534 [D loss: 0.719715, acc.: 51.56%] [G loss: 0.795511]\n",
      "epoch:9 step:8535 [D loss: 0.689602, acc.: 53.91%] [G loss: 0.888069]\n",
      "epoch:9 step:8536 [D loss: 0.640144, acc.: 64.84%] [G loss: 1.028023]\n",
      "epoch:9 step:8537 [D loss: 0.701389, acc.: 52.34%] [G loss: 0.808847]\n",
      "epoch:9 step:8538 [D loss: 0.690253, acc.: 52.34%] [G loss: 0.816959]\n",
      "epoch:9 step:8539 [D loss: 0.669766, acc.: 59.38%] [G loss: 0.776401]\n",
      "epoch:9 step:8540 [D loss: 0.701910, acc.: 47.66%] [G loss: 0.770134]\n",
      "epoch:9 step:8541 [D loss: 0.617446, acc.: 69.53%] [G loss: 0.796528]\n",
      "epoch:9 step:8542 [D loss: 0.690244, acc.: 53.12%] [G loss: 0.753537]\n",
      "epoch:9 step:8543 [D loss: 0.666188, acc.: 57.81%] [G loss: 0.931004]\n",
      "epoch:9 step:8544 [D loss: 0.710237, acc.: 49.22%] [G loss: 0.772395]\n",
      "epoch:9 step:8545 [D loss: 0.683016, acc.: 49.22%] [G loss: 0.863178]\n",
      "epoch:9 step:8546 [D loss: 0.672666, acc.: 57.81%] [G loss: 0.863103]\n",
      "epoch:9 step:8547 [D loss: 0.683566, acc.: 56.25%] [G loss: 0.849298]\n",
      "epoch:9 step:8548 [D loss: 0.721084, acc.: 44.53%] [G loss: 0.824824]\n",
      "epoch:9 step:8549 [D loss: 0.690206, acc.: 53.12%] [G loss: 0.804295]\n",
      "epoch:9 step:8550 [D loss: 0.699773, acc.: 53.12%] [G loss: 0.861985]\n",
      "epoch:9 step:8551 [D loss: 0.678457, acc.: 57.03%] [G loss: 0.854095]\n",
      "epoch:9 step:8552 [D loss: 0.668048, acc.: 60.16%] [G loss: 0.880306]\n",
      "epoch:9 step:8553 [D loss: 0.660971, acc.: 53.12%] [G loss: 0.809971]\n",
      "epoch:9 step:8554 [D loss: 0.682672, acc.: 50.00%] [G loss: 0.810620]\n",
      "epoch:9 step:8555 [D loss: 0.694200, acc.: 54.69%] [G loss: 0.794466]\n",
      "epoch:9 step:8556 [D loss: 0.683024, acc.: 48.44%] [G loss: 0.825383]\n",
      "epoch:9 step:8557 [D loss: 0.670480, acc.: 54.69%] [G loss: 0.808794]\n",
      "epoch:9 step:8558 [D loss: 0.637577, acc.: 67.97%] [G loss: 0.843405]\n",
      "epoch:9 step:8559 [D loss: 0.697852, acc.: 52.34%] [G loss: 0.858245]\n",
      "epoch:9 step:8560 [D loss: 0.673248, acc.: 59.38%] [G loss: 0.785596]\n",
      "epoch:9 step:8561 [D loss: 0.725072, acc.: 46.88%] [G loss: 0.811797]\n",
      "epoch:9 step:8562 [D loss: 0.703809, acc.: 48.44%] [G loss: 0.816593]\n",
      "epoch:9 step:8563 [D loss: 0.714971, acc.: 40.62%] [G loss: 0.783600]\n",
      "epoch:9 step:8564 [D loss: 0.646888, acc.: 65.62%] [G loss: 0.806743]\n",
      "epoch:9 step:8565 [D loss: 0.697809, acc.: 48.44%] [G loss: 0.773414]\n",
      "epoch:9 step:8566 [D loss: 0.717129, acc.: 50.00%] [G loss: 0.831883]\n",
      "epoch:9 step:8567 [D loss: 0.666048, acc.: 60.16%] [G loss: 0.799046]\n",
      "epoch:9 step:8568 [D loss: 0.683531, acc.: 57.03%] [G loss: 0.755734]\n",
      "epoch:9 step:8569 [D loss: 0.685050, acc.: 57.81%] [G loss: 0.803344]\n",
      "epoch:9 step:8570 [D loss: 0.680245, acc.: 53.12%] [G loss: 0.771258]\n",
      "epoch:9 step:8571 [D loss: 0.713996, acc.: 53.91%] [G loss: 0.742058]\n",
      "epoch:9 step:8572 [D loss: 0.703456, acc.: 51.56%] [G loss: 0.797643]\n",
      "epoch:9 step:8573 [D loss: 0.637584, acc.: 62.50%] [G loss: 0.778089]\n",
      "epoch:9 step:8574 [D loss: 0.688471, acc.: 54.69%] [G loss: 0.784336]\n",
      "epoch:9 step:8575 [D loss: 0.687384, acc.: 57.81%] [G loss: 0.775475]\n",
      "epoch:9 step:8576 [D loss: 0.679616, acc.: 54.69%] [G loss: 0.833713]\n",
      "epoch:9 step:8577 [D loss: 0.637405, acc.: 60.16%] [G loss: 0.970283]\n",
      "epoch:9 step:8578 [D loss: 0.666759, acc.: 59.38%] [G loss: 0.809054]\n",
      "epoch:9 step:8579 [D loss: 0.648429, acc.: 66.41%] [G loss: 0.804196]\n",
      "epoch:9 step:8580 [D loss: 0.637830, acc.: 64.84%] [G loss: 0.857925]\n",
      "epoch:9 step:8581 [D loss: 0.678856, acc.: 57.03%] [G loss: 0.785141]\n",
      "epoch:9 step:8582 [D loss: 0.646024, acc.: 64.06%] [G loss: 0.801250]\n",
      "epoch:9 step:8583 [D loss: 0.683386, acc.: 51.56%] [G loss: 0.874609]\n",
      "epoch:9 step:8584 [D loss: 0.683574, acc.: 50.00%] [G loss: 0.810259]\n",
      "epoch:9 step:8585 [D loss: 0.697026, acc.: 57.03%] [G loss: 0.765529]\n",
      "epoch:9 step:8586 [D loss: 0.714094, acc.: 47.66%] [G loss: 0.762088]\n",
      "epoch:9 step:8587 [D loss: 0.680425, acc.: 55.47%] [G loss: 0.726900]\n",
      "epoch:9 step:8588 [D loss: 0.681911, acc.: 60.16%] [G loss: 0.793349]\n",
      "epoch:9 step:8589 [D loss: 0.645081, acc.: 64.06%] [G loss: 0.879631]\n",
      "epoch:9 step:8590 [D loss: 0.683366, acc.: 56.25%] [G loss: 0.788149]\n",
      "epoch:9 step:8591 [D loss: 0.690052, acc.: 52.34%] [G loss: 0.843145]\n",
      "epoch:9 step:8592 [D loss: 0.682791, acc.: 55.47%] [G loss: 0.777283]\n",
      "epoch:9 step:8593 [D loss: 0.674540, acc.: 57.03%] [G loss: 0.800596]\n",
      "epoch:9 step:8594 [D loss: 0.737550, acc.: 49.22%] [G loss: 0.775508]\n",
      "epoch:9 step:8595 [D loss: 0.712461, acc.: 50.00%] [G loss: 0.773011]\n",
      "epoch:9 step:8596 [D loss: 0.715315, acc.: 47.66%] [G loss: 0.764687]\n",
      "epoch:9 step:8597 [D loss: 0.679565, acc.: 47.66%] [G loss: 0.777048]\n",
      "epoch:9 step:8598 [D loss: 0.696969, acc.: 50.00%] [G loss: 0.786850]\n",
      "epoch:9 step:8599 [D loss: 0.710672, acc.: 47.66%] [G loss: 0.786785]\n",
      "epoch:9 step:8600 [D loss: 0.662711, acc.: 61.72%] [G loss: 0.822625]\n",
      "epoch:9 step:8601 [D loss: 0.709905, acc.: 46.88%] [G loss: 0.774236]\n",
      "epoch:9 step:8602 [D loss: 0.696023, acc.: 57.03%] [G loss: 0.787392]\n",
      "epoch:9 step:8603 [D loss: 0.683657, acc.: 57.81%] [G loss: 0.813926]\n",
      "epoch:9 step:8604 [D loss: 0.707248, acc.: 50.00%] [G loss: 0.781646]\n",
      "epoch:9 step:8605 [D loss: 0.689354, acc.: 50.78%] [G loss: 0.823913]\n",
      "epoch:9 step:8606 [D loss: 0.714830, acc.: 56.25%] [G loss: 0.773911]\n",
      "epoch:9 step:8607 [D loss: 0.679942, acc.: 57.03%] [G loss: 0.826247]\n",
      "epoch:9 step:8608 [D loss: 0.680380, acc.: 54.69%] [G loss: 0.791318]\n",
      "epoch:9 step:8609 [D loss: 0.696384, acc.: 53.12%] [G loss: 0.787175]\n",
      "epoch:9 step:8610 [D loss: 0.645265, acc.: 67.19%] [G loss: 0.805228]\n",
      "epoch:9 step:8611 [D loss: 0.646320, acc.: 63.28%] [G loss: 0.771058]\n",
      "epoch:9 step:8612 [D loss: 0.672400, acc.: 63.28%] [G loss: 0.803043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8613 [D loss: 0.689912, acc.: 51.56%] [G loss: 0.830390]\n",
      "epoch:9 step:8614 [D loss: 0.697217, acc.: 52.34%] [G loss: 0.770822]\n",
      "epoch:9 step:8615 [D loss: 0.679527, acc.: 60.94%] [G loss: 0.771923]\n",
      "epoch:9 step:8616 [D loss: 0.672846, acc.: 54.69%] [G loss: 0.770922]\n",
      "epoch:9 step:8617 [D loss: 0.666292, acc.: 60.16%] [G loss: 0.789639]\n",
      "epoch:9 step:8618 [D loss: 0.672617, acc.: 56.25%] [G loss: 0.771654]\n",
      "epoch:9 step:8619 [D loss: 0.689176, acc.: 50.00%] [G loss: 0.800857]\n",
      "epoch:9 step:8620 [D loss: 0.683222, acc.: 47.66%] [G loss: 0.737405]\n",
      "epoch:9 step:8621 [D loss: 0.678503, acc.: 59.38%] [G loss: 0.799075]\n",
      "epoch:9 step:8622 [D loss: 0.693303, acc.: 60.16%] [G loss: 0.773858]\n",
      "epoch:9 step:8623 [D loss: 0.668992, acc.: 53.91%] [G loss: 0.753692]\n",
      "epoch:9 step:8624 [D loss: 0.674713, acc.: 53.91%] [G loss: 0.790184]\n",
      "epoch:9 step:8625 [D loss: 0.717657, acc.: 50.00%] [G loss: 0.708326]\n",
      "epoch:9 step:8626 [D loss: 0.675324, acc.: 55.47%] [G loss: 0.795181]\n",
      "epoch:9 step:8627 [D loss: 0.655103, acc.: 64.06%] [G loss: 0.800785]\n",
      "epoch:9 step:8628 [D loss: 0.661345, acc.: 57.03%] [G loss: 0.754949]\n",
      "epoch:9 step:8629 [D loss: 0.714066, acc.: 43.75%] [G loss: 0.786490]\n",
      "epoch:9 step:8630 [D loss: 0.668028, acc.: 64.84%] [G loss: 0.793572]\n",
      "epoch:9 step:8631 [D loss: 0.708217, acc.: 56.25%] [G loss: 0.791045]\n",
      "epoch:9 step:8632 [D loss: 0.664295, acc.: 56.25%] [G loss: 0.789505]\n",
      "epoch:9 step:8633 [D loss: 0.685561, acc.: 53.12%] [G loss: 0.816489]\n",
      "epoch:9 step:8634 [D loss: 0.677313, acc.: 57.03%] [G loss: 0.811806]\n",
      "epoch:9 step:8635 [D loss: 0.684252, acc.: 62.50%] [G loss: 0.765034]\n",
      "epoch:9 step:8636 [D loss: 0.658888, acc.: 56.25%] [G loss: 0.746820]\n",
      "epoch:9 step:8637 [D loss: 0.653668, acc.: 61.72%] [G loss: 0.799608]\n",
      "epoch:9 step:8638 [D loss: 0.667589, acc.: 60.16%] [G loss: 0.769490]\n",
      "epoch:9 step:8639 [D loss: 0.696632, acc.: 53.91%] [G loss: 0.765679]\n",
      "epoch:9 step:8640 [D loss: 0.683633, acc.: 56.25%] [G loss: 0.780847]\n",
      "epoch:9 step:8641 [D loss: 0.705788, acc.: 46.88%] [G loss: 0.764292]\n",
      "epoch:9 step:8642 [D loss: 0.675008, acc.: 53.91%] [G loss: 0.863730]\n",
      "epoch:9 step:8643 [D loss: 0.653316, acc.: 60.16%] [G loss: 0.806632]\n",
      "epoch:9 step:8644 [D loss: 0.673216, acc.: 53.12%] [G loss: 0.861285]\n",
      "epoch:9 step:8645 [D loss: 0.692571, acc.: 48.44%] [G loss: 0.827012]\n",
      "epoch:9 step:8646 [D loss: 0.719607, acc.: 46.09%] [G loss: 0.753007]\n",
      "epoch:9 step:8647 [D loss: 0.676005, acc.: 55.47%] [G loss: 0.843882]\n",
      "epoch:9 step:8648 [D loss: 0.713535, acc.: 46.88%] [G loss: 0.783489]\n",
      "epoch:9 step:8649 [D loss: 0.674176, acc.: 55.47%] [G loss: 0.838935]\n",
      "epoch:9 step:8650 [D loss: 0.721875, acc.: 50.78%] [G loss: 0.751626]\n",
      "epoch:9 step:8651 [D loss: 0.686395, acc.: 57.81%] [G loss: 0.760854]\n",
      "epoch:9 step:8652 [D loss: 0.699175, acc.: 55.47%] [G loss: 0.753726]\n",
      "epoch:9 step:8653 [D loss: 0.714985, acc.: 50.00%] [G loss: 0.780355]\n",
      "epoch:9 step:8654 [D loss: 0.670436, acc.: 54.69%] [G loss: 0.795378]\n",
      "epoch:9 step:8655 [D loss: 0.666183, acc.: 56.25%] [G loss: 0.826704]\n",
      "epoch:9 step:8656 [D loss: 0.681836, acc.: 48.44%] [G loss: 0.842660]\n",
      "epoch:9 step:8657 [D loss: 0.694187, acc.: 50.00%] [G loss: 0.781639]\n",
      "epoch:9 step:8658 [D loss: 0.664227, acc.: 62.50%] [G loss: 0.758873]\n",
      "epoch:9 step:8659 [D loss: 0.705387, acc.: 47.66%] [G loss: 0.808343]\n",
      "epoch:9 step:8660 [D loss: 0.674774, acc.: 59.38%] [G loss: 0.776641]\n",
      "epoch:9 step:8661 [D loss: 0.686421, acc.: 56.25%] [G loss: 0.769744]\n",
      "epoch:9 step:8662 [D loss: 0.673575, acc.: 55.47%] [G loss: 0.724751]\n",
      "epoch:9 step:8663 [D loss: 0.679957, acc.: 56.25%] [G loss: 0.799249]\n",
      "epoch:9 step:8664 [D loss: 0.709054, acc.: 48.44%] [G loss: 0.769024]\n",
      "epoch:9 step:8665 [D loss: 0.675891, acc.: 55.47%] [G loss: 0.736574]\n",
      "epoch:9 step:8666 [D loss: 0.698048, acc.: 54.69%] [G loss: 0.799704]\n",
      "epoch:9 step:8667 [D loss: 0.675986, acc.: 59.38%] [G loss: 0.734052]\n",
      "epoch:9 step:8668 [D loss: 0.654860, acc.: 60.94%] [G loss: 0.743588]\n",
      "epoch:9 step:8669 [D loss: 0.664688, acc.: 60.94%] [G loss: 0.713920]\n",
      "epoch:9 step:8670 [D loss: 0.684365, acc.: 55.47%] [G loss: 0.785603]\n",
      "epoch:9 step:8671 [D loss: 0.681900, acc.: 55.47%] [G loss: 0.743550]\n",
      "epoch:9 step:8672 [D loss: 0.697954, acc.: 54.69%] [G loss: 0.749231]\n",
      "epoch:9 step:8673 [D loss: 0.699464, acc.: 50.78%] [G loss: 0.770910]\n",
      "epoch:9 step:8674 [D loss: 0.672756, acc.: 60.94%] [G loss: 0.736852]\n",
      "epoch:9 step:8675 [D loss: 0.659117, acc.: 61.72%] [G loss: 0.772898]\n",
      "epoch:9 step:8676 [D loss: 0.650817, acc.: 60.16%] [G loss: 0.861386]\n",
      "epoch:9 step:8677 [D loss: 0.663201, acc.: 64.06%] [G loss: 0.890333]\n",
      "epoch:9 step:8678 [D loss: 0.673713, acc.: 56.25%] [G loss: 0.828910]\n",
      "epoch:9 step:8679 [D loss: 0.676464, acc.: 55.47%] [G loss: 0.798730]\n",
      "epoch:9 step:8680 [D loss: 0.678433, acc.: 51.56%] [G loss: 0.869953]\n",
      "epoch:9 step:8681 [D loss: 0.710884, acc.: 50.00%] [G loss: 0.853849]\n",
      "epoch:9 step:8682 [D loss: 0.671157, acc.: 60.16%] [G loss: 0.789523]\n",
      "epoch:9 step:8683 [D loss: 0.691554, acc.: 53.91%] [G loss: 0.852369]\n",
      "epoch:9 step:8684 [D loss: 0.672713, acc.: 61.72%] [G loss: 0.796346]\n",
      "epoch:9 step:8685 [D loss: 0.676203, acc.: 58.59%] [G loss: 0.800536]\n",
      "epoch:9 step:8686 [D loss: 0.653136, acc.: 61.72%] [G loss: 0.806192]\n",
      "epoch:9 step:8687 [D loss: 0.685498, acc.: 56.25%] [G loss: 0.794224]\n",
      "epoch:9 step:8688 [D loss: 0.685935, acc.: 53.12%] [G loss: 0.821307]\n",
      "epoch:9 step:8689 [D loss: 0.666742, acc.: 56.25%] [G loss: 0.823854]\n",
      "epoch:9 step:8690 [D loss: 0.678629, acc.: 59.38%] [G loss: 0.773868]\n",
      "epoch:9 step:8691 [D loss: 0.679447, acc.: 57.81%] [G loss: 0.795109]\n",
      "epoch:9 step:8692 [D loss: 0.713999, acc.: 44.53%] [G loss: 0.796567]\n",
      "epoch:9 step:8693 [D loss: 0.667831, acc.: 61.72%] [G loss: 0.823804]\n",
      "epoch:9 step:8694 [D loss: 0.653419, acc.: 58.59%] [G loss: 0.787306]\n",
      "epoch:9 step:8695 [D loss: 0.679267, acc.: 58.59%] [G loss: 0.780686]\n",
      "epoch:9 step:8696 [D loss: 0.647933, acc.: 67.19%] [G loss: 0.779788]\n",
      "epoch:9 step:8697 [D loss: 0.675314, acc.: 60.16%] [G loss: 0.759460]\n",
      "epoch:9 step:8698 [D loss: 0.665406, acc.: 54.69%] [G loss: 0.816733]\n",
      "epoch:9 step:8699 [D loss: 0.645430, acc.: 57.81%] [G loss: 0.828310]\n",
      "epoch:9 step:8700 [D loss: 0.643042, acc.: 60.94%] [G loss: 0.893080]\n",
      "epoch:9 step:8701 [D loss: 0.634072, acc.: 66.41%] [G loss: 0.799915]\n",
      "epoch:9 step:8702 [D loss: 0.672584, acc.: 53.12%] [G loss: 0.840762]\n",
      "epoch:9 step:8703 [D loss: 0.655042, acc.: 63.28%] [G loss: 0.835780]\n",
      "epoch:9 step:8704 [D loss: 0.634988, acc.: 64.06%] [G loss: 0.934040]\n",
      "epoch:9 step:8705 [D loss: 0.674823, acc.: 52.34%] [G loss: 0.793779]\n",
      "epoch:9 step:8706 [D loss: 0.671239, acc.: 59.38%] [G loss: 0.904131]\n",
      "epoch:9 step:8707 [D loss: 0.673099, acc.: 55.47%] [G loss: 0.774442]\n",
      "epoch:9 step:8708 [D loss: 0.669546, acc.: 62.50%] [G loss: 0.806090]\n",
      "epoch:9 step:8709 [D loss: 0.671726, acc.: 59.38%] [G loss: 0.821341]\n",
      "epoch:9 step:8710 [D loss: 0.676368, acc.: 57.03%] [G loss: 0.794359]\n",
      "epoch:9 step:8711 [D loss: 0.684619, acc.: 56.25%] [G loss: 0.779762]\n",
      "epoch:9 step:8712 [D loss: 0.648432, acc.: 60.16%] [G loss: 0.828493]\n",
      "epoch:9 step:8713 [D loss: 0.682411, acc.: 59.38%] [G loss: 0.833186]\n",
      "epoch:9 step:8714 [D loss: 0.684252, acc.: 57.03%] [G loss: 0.863171]\n",
      "epoch:9 step:8715 [D loss: 0.640857, acc.: 64.06%] [G loss: 0.875160]\n",
      "epoch:9 step:8716 [D loss: 0.612137, acc.: 68.75%] [G loss: 0.890251]\n",
      "epoch:9 step:8717 [D loss: 0.650620, acc.: 60.16%] [G loss: 0.966284]\n",
      "epoch:9 step:8718 [D loss: 0.642163, acc.: 63.28%] [G loss: 0.959810]\n",
      "epoch:9 step:8719 [D loss: 0.676536, acc.: 54.69%] [G loss: 0.857241]\n",
      "epoch:9 step:8720 [D loss: 0.652886, acc.: 59.38%] [G loss: 0.897284]\n",
      "epoch:9 step:8721 [D loss: 0.729164, acc.: 50.78%] [G loss: 0.788319]\n",
      "epoch:9 step:8722 [D loss: 0.716496, acc.: 46.88%] [G loss: 0.744397]\n",
      "epoch:9 step:8723 [D loss: 0.619261, acc.: 66.41%] [G loss: 0.798774]\n",
      "epoch:9 step:8724 [D loss: 0.696724, acc.: 53.91%] [G loss: 0.800734]\n",
      "epoch:9 step:8725 [D loss: 0.670806, acc.: 61.72%] [G loss: 0.810687]\n",
      "epoch:9 step:8726 [D loss: 0.714985, acc.: 48.44%] [G loss: 0.873619]\n",
      "epoch:9 step:8727 [D loss: 0.668656, acc.: 60.94%] [G loss: 0.913505]\n",
      "epoch:9 step:8728 [D loss: 0.678799, acc.: 50.78%] [G loss: 0.894771]\n",
      "epoch:9 step:8729 [D loss: 0.659625, acc.: 54.69%] [G loss: 0.936576]\n",
      "epoch:9 step:8730 [D loss: 0.606466, acc.: 65.62%] [G loss: 0.947422]\n",
      "epoch:9 step:8731 [D loss: 0.702142, acc.: 53.12%] [G loss: 0.862066]\n",
      "epoch:9 step:8732 [D loss: 0.692948, acc.: 53.91%] [G loss: 0.830054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8733 [D loss: 0.653343, acc.: 54.69%] [G loss: 0.868340]\n",
      "epoch:9 step:8734 [D loss: 0.680845, acc.: 54.69%] [G loss: 0.779953]\n",
      "epoch:9 step:8735 [D loss: 0.668592, acc.: 61.72%] [G loss: 0.802175]\n",
      "epoch:9 step:8736 [D loss: 0.648094, acc.: 62.50%] [G loss: 0.878695]\n",
      "epoch:9 step:8737 [D loss: 0.761985, acc.: 40.62%] [G loss: 0.782055]\n",
      "epoch:9 step:8738 [D loss: 0.696275, acc.: 49.22%] [G loss: 0.834492]\n",
      "epoch:9 step:8739 [D loss: 0.731199, acc.: 41.41%] [G loss: 0.814134]\n",
      "epoch:9 step:8740 [D loss: 0.684828, acc.: 58.59%] [G loss: 0.827869]\n",
      "epoch:9 step:8741 [D loss: 0.675515, acc.: 50.78%] [G loss: 0.855549]\n",
      "epoch:9 step:8742 [D loss: 0.693495, acc.: 52.34%] [G loss: 0.943729]\n",
      "epoch:9 step:8743 [D loss: 0.675384, acc.: 57.03%] [G loss: 0.775035]\n",
      "epoch:9 step:8744 [D loss: 0.689448, acc.: 57.03%] [G loss: 0.818299]\n",
      "epoch:9 step:8745 [D loss: 0.661300, acc.: 59.38%] [G loss: 0.835853]\n",
      "epoch:9 step:8746 [D loss: 0.677646, acc.: 63.28%] [G loss: 0.794220]\n",
      "epoch:9 step:8747 [D loss: 0.707649, acc.: 55.47%] [G loss: 0.855266]\n",
      "epoch:9 step:8748 [D loss: 0.688563, acc.: 58.59%] [G loss: 0.931726]\n",
      "epoch:9 step:8749 [D loss: 0.674871, acc.: 57.81%] [G loss: 0.864973]\n",
      "epoch:9 step:8750 [D loss: 0.645318, acc.: 58.59%] [G loss: 0.885617]\n",
      "epoch:9 step:8751 [D loss: 0.680023, acc.: 53.91%] [G loss: 0.891383]\n",
      "epoch:9 step:8752 [D loss: 0.647042, acc.: 60.16%] [G loss: 0.967400]\n",
      "epoch:9 step:8753 [D loss: 0.628291, acc.: 67.19%] [G loss: 0.903329]\n",
      "epoch:9 step:8754 [D loss: 0.686276, acc.: 51.56%] [G loss: 0.830558]\n",
      "epoch:9 step:8755 [D loss: 0.657278, acc.: 62.50%] [G loss: 0.823135]\n",
      "epoch:9 step:8756 [D loss: 0.689870, acc.: 54.69%] [G loss: 0.783253]\n",
      "epoch:9 step:8757 [D loss: 0.633499, acc.: 54.69%] [G loss: 0.886194]\n",
      "epoch:9 step:8758 [D loss: 0.708669, acc.: 46.88%] [G loss: 0.786377]\n",
      "epoch:9 step:8759 [D loss: 0.699981, acc.: 53.91%] [G loss: 0.779699]\n",
      "epoch:9 step:8760 [D loss: 0.657509, acc.: 67.97%] [G loss: 0.778747]\n",
      "epoch:9 step:8761 [D loss: 0.650756, acc.: 60.16%] [G loss: 0.903566]\n",
      "epoch:9 step:8762 [D loss: 0.679259, acc.: 54.69%] [G loss: 0.918247]\n",
      "epoch:9 step:8763 [D loss: 0.703261, acc.: 50.00%] [G loss: 0.822943]\n",
      "epoch:9 step:8764 [D loss: 0.683370, acc.: 50.00%] [G loss: 0.885272]\n",
      "epoch:9 step:8765 [D loss: 0.701241, acc.: 53.91%] [G loss: 0.781290]\n",
      "epoch:9 step:8766 [D loss: 0.668007, acc.: 53.12%] [G loss: 0.773628]\n",
      "epoch:9 step:8767 [D loss: 0.661617, acc.: 60.94%] [G loss: 0.845837]\n",
      "epoch:9 step:8768 [D loss: 0.690509, acc.: 57.03%] [G loss: 0.801972]\n",
      "epoch:9 step:8769 [D loss: 0.668956, acc.: 58.59%] [G loss: 0.839442]\n",
      "epoch:9 step:8770 [D loss: 0.676601, acc.: 53.91%] [G loss: 0.844477]\n",
      "epoch:9 step:8771 [D loss: 0.662130, acc.: 60.16%] [G loss: 0.803734]\n",
      "epoch:9 step:8772 [D loss: 0.658979, acc.: 63.28%] [G loss: 0.772397]\n",
      "epoch:9 step:8773 [D loss: 0.712648, acc.: 49.22%] [G loss: 0.791192]\n",
      "epoch:9 step:8774 [D loss: 0.678450, acc.: 54.69%] [G loss: 0.833086]\n",
      "epoch:9 step:8775 [D loss: 0.676619, acc.: 58.59%] [G loss: 0.887488]\n",
      "epoch:9 step:8776 [D loss: 0.699073, acc.: 48.44%] [G loss: 0.833118]\n",
      "epoch:9 step:8777 [D loss: 0.658134, acc.: 61.72%] [G loss: 0.768174]\n",
      "epoch:9 step:8778 [D loss: 0.659050, acc.: 50.78%] [G loss: 0.775151]\n",
      "epoch:9 step:8779 [D loss: 0.659267, acc.: 58.59%] [G loss: 0.884896]\n",
      "epoch:9 step:8780 [D loss: 0.679802, acc.: 57.03%] [G loss: 0.868309]\n",
      "epoch:9 step:8781 [D loss: 0.711993, acc.: 42.97%] [G loss: 0.808683]\n",
      "epoch:9 step:8782 [D loss: 0.653406, acc.: 53.91%] [G loss: 0.800545]\n",
      "epoch:9 step:8783 [D loss: 0.664621, acc.: 60.94%] [G loss: 0.868154]\n",
      "epoch:9 step:8784 [D loss: 0.688038, acc.: 53.91%] [G loss: 0.826317]\n",
      "epoch:9 step:8785 [D loss: 0.675920, acc.: 52.34%] [G loss: 0.808359]\n",
      "epoch:9 step:8786 [D loss: 0.699169, acc.: 52.34%] [G loss: 0.905793]\n",
      "epoch:9 step:8787 [D loss: 0.732320, acc.: 50.78%] [G loss: 0.817533]\n",
      "epoch:9 step:8788 [D loss: 0.687038, acc.: 49.22%] [G loss: 0.851513]\n",
      "epoch:9 step:8789 [D loss: 0.696847, acc.: 48.44%] [G loss: 0.765552]\n",
      "epoch:9 step:8790 [D loss: 0.669745, acc.: 56.25%] [G loss: 0.819006]\n",
      "epoch:9 step:8791 [D loss: 0.659006, acc.: 60.94%] [G loss: 0.817658]\n",
      "epoch:9 step:8792 [D loss: 0.697391, acc.: 53.12%] [G loss: 0.784718]\n",
      "epoch:9 step:8793 [D loss: 0.670498, acc.: 53.91%] [G loss: 0.784175]\n",
      "epoch:9 step:8794 [D loss: 0.644610, acc.: 63.28%] [G loss: 0.879942]\n",
      "epoch:9 step:8795 [D loss: 0.665519, acc.: 50.78%] [G loss: 0.903425]\n",
      "epoch:9 step:8796 [D loss: 0.622907, acc.: 72.66%] [G loss: 0.826806]\n",
      "epoch:9 step:8797 [D loss: 0.705504, acc.: 46.88%] [G loss: 0.835264]\n",
      "epoch:9 step:8798 [D loss: 0.663058, acc.: 60.94%] [G loss: 0.894430]\n",
      "epoch:9 step:8799 [D loss: 0.655407, acc.: 63.28%] [G loss: 0.750091]\n",
      "epoch:9 step:8800 [D loss: 0.667538, acc.: 53.91%] [G loss: 0.906594]\n",
      "epoch:9 step:8801 [D loss: 0.639103, acc.: 63.28%] [G loss: 0.839831]\n",
      "epoch:9 step:8802 [D loss: 0.671120, acc.: 56.25%] [G loss: 0.953896]\n",
      "epoch:9 step:8803 [D loss: 0.677927, acc.: 50.78%] [G loss: 0.795863]\n",
      "epoch:9 step:8804 [D loss: 0.691504, acc.: 53.91%] [G loss: 0.796404]\n",
      "epoch:9 step:8805 [D loss: 0.663383, acc.: 56.25%] [G loss: 0.792844]\n",
      "epoch:9 step:8806 [D loss: 0.667079, acc.: 60.16%] [G loss: 0.745122]\n",
      "epoch:9 step:8807 [D loss: 0.675124, acc.: 50.78%] [G loss: 0.736095]\n",
      "epoch:9 step:8808 [D loss: 0.696869, acc.: 53.12%] [G loss: 0.777643]\n",
      "epoch:9 step:8809 [D loss: 0.659668, acc.: 62.50%] [G loss: 0.811396]\n",
      "epoch:9 step:8810 [D loss: 0.728013, acc.: 44.53%] [G loss: 0.829012]\n",
      "epoch:9 step:8811 [D loss: 0.697596, acc.: 47.66%] [G loss: 0.814570]\n",
      "epoch:9 step:8812 [D loss: 0.690147, acc.: 57.81%] [G loss: 0.783804]\n",
      "epoch:9 step:8813 [D loss: 0.711911, acc.: 50.00%] [G loss: 0.770865]\n",
      "epoch:9 step:8814 [D loss: 0.640124, acc.: 59.38%] [G loss: 0.901394]\n",
      "epoch:9 step:8815 [D loss: 0.669268, acc.: 55.47%] [G loss: 0.786988]\n",
      "epoch:9 step:8816 [D loss: 0.697721, acc.: 51.56%] [G loss: 0.784223]\n",
      "epoch:9 step:8817 [D loss: 0.701426, acc.: 47.66%] [G loss: 0.820713]\n",
      "epoch:9 step:8818 [D loss: 0.709332, acc.: 47.66%] [G loss: 0.828348]\n",
      "epoch:9 step:8819 [D loss: 0.681361, acc.: 53.91%] [G loss: 0.809193]\n",
      "epoch:9 step:8820 [D loss: 0.680011, acc.: 59.38%] [G loss: 0.819530]\n",
      "epoch:9 step:8821 [D loss: 0.702393, acc.: 49.22%] [G loss: 0.822209]\n",
      "epoch:9 step:8822 [D loss: 0.664187, acc.: 53.91%] [G loss: 0.823245]\n",
      "epoch:9 step:8823 [D loss: 0.642947, acc.: 60.94%] [G loss: 0.884584]\n",
      "epoch:9 step:8824 [D loss: 0.660076, acc.: 58.59%] [G loss: 0.797921]\n",
      "epoch:9 step:8825 [D loss: 0.632836, acc.: 60.16%] [G loss: 0.857087]\n",
      "epoch:9 step:8826 [D loss: 0.645064, acc.: 61.72%] [G loss: 0.902388]\n",
      "epoch:9 step:8827 [D loss: 0.680854, acc.: 51.56%] [G loss: 0.902162]\n",
      "epoch:9 step:8828 [D loss: 0.668873, acc.: 56.25%] [G loss: 1.002245]\n",
      "epoch:9 step:8829 [D loss: 0.657278, acc.: 60.16%] [G loss: 0.985861]\n",
      "epoch:9 step:8830 [D loss: 0.735482, acc.: 46.09%] [G loss: 0.836976]\n",
      "epoch:9 step:8831 [D loss: 0.648943, acc.: 60.94%] [G loss: 0.853534]\n",
      "epoch:9 step:8832 [D loss: 0.683480, acc.: 53.91%] [G loss: 0.849924]\n",
      "epoch:9 step:8833 [D loss: 0.617480, acc.: 66.41%] [G loss: 1.017639]\n",
      "epoch:9 step:8834 [D loss: 0.661444, acc.: 58.59%] [G loss: 0.884487]\n",
      "epoch:9 step:8835 [D loss: 0.713894, acc.: 49.22%] [G loss: 0.832516]\n",
      "epoch:9 step:8836 [D loss: 0.659960, acc.: 62.50%] [G loss: 0.829431]\n",
      "epoch:9 step:8837 [D loss: 0.674415, acc.: 61.72%] [G loss: 0.883844]\n",
      "epoch:9 step:8838 [D loss: 0.679298, acc.: 50.78%] [G loss: 0.894528]\n",
      "epoch:9 step:8839 [D loss: 0.629964, acc.: 60.94%] [G loss: 1.006653]\n",
      "epoch:9 step:8840 [D loss: 0.646242, acc.: 68.75%] [G loss: 0.816005]\n",
      "epoch:9 step:8841 [D loss: 0.651043, acc.: 62.50%] [G loss: 0.919448]\n",
      "epoch:9 step:8842 [D loss: 0.638718, acc.: 62.50%] [G loss: 0.865741]\n",
      "epoch:9 step:8843 [D loss: 0.642544, acc.: 57.81%] [G loss: 0.838557]\n",
      "epoch:9 step:8844 [D loss: 0.689902, acc.: 55.47%] [G loss: 0.947821]\n",
      "epoch:9 step:8845 [D loss: 0.713378, acc.: 52.34%] [G loss: 0.807470]\n",
      "epoch:9 step:8846 [D loss: 0.665371, acc.: 50.78%] [G loss: 0.904263]\n",
      "epoch:9 step:8847 [D loss: 0.647833, acc.: 59.38%] [G loss: 0.803686]\n",
      "epoch:9 step:8848 [D loss: 0.635806, acc.: 60.16%] [G loss: 0.830135]\n",
      "epoch:9 step:8849 [D loss: 0.656081, acc.: 59.38%] [G loss: 0.824761]\n",
      "epoch:9 step:8850 [D loss: 0.648740, acc.: 58.59%] [G loss: 0.831804]\n",
      "epoch:9 step:8851 [D loss: 0.688105, acc.: 60.16%] [G loss: 0.958677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8852 [D loss: 0.685990, acc.: 54.69%] [G loss: 1.081281]\n",
      "epoch:9 step:8853 [D loss: 0.704828, acc.: 53.91%] [G loss: 0.956459]\n",
      "epoch:9 step:8854 [D loss: 0.709204, acc.: 48.44%] [G loss: 0.905222]\n",
      "epoch:9 step:8855 [D loss: 0.700408, acc.: 54.69%] [G loss: 0.925123]\n",
      "epoch:9 step:8856 [D loss: 0.688116, acc.: 53.12%] [G loss: 0.905029]\n",
      "epoch:9 step:8857 [D loss: 0.683002, acc.: 61.72%] [G loss: 0.890366]\n",
      "epoch:9 step:8858 [D loss: 0.662336, acc.: 59.38%] [G loss: 0.787873]\n",
      "epoch:9 step:8859 [D loss: 0.652893, acc.: 63.28%] [G loss: 0.819860]\n",
      "epoch:9 step:8860 [D loss: 0.670013, acc.: 53.91%] [G loss: 0.816504]\n",
      "epoch:9 step:8861 [D loss: 0.674133, acc.: 55.47%] [G loss: 0.910096]\n",
      "epoch:9 step:8862 [D loss: 0.682950, acc.: 56.25%] [G loss: 0.833385]\n",
      "epoch:9 step:8863 [D loss: 0.680388, acc.: 60.94%] [G loss: 0.890223]\n",
      "epoch:9 step:8864 [D loss: 0.685142, acc.: 57.81%] [G loss: 0.806633]\n",
      "epoch:9 step:8865 [D loss: 0.715931, acc.: 44.53%] [G loss: 0.804917]\n",
      "epoch:9 step:8866 [D loss: 0.718834, acc.: 49.22%] [G loss: 0.854122]\n",
      "epoch:9 step:8867 [D loss: 0.657351, acc.: 60.16%] [G loss: 0.813291]\n",
      "epoch:9 step:8868 [D loss: 0.689733, acc.: 50.00%] [G loss: 0.917647]\n",
      "epoch:9 step:8869 [D loss: 0.703949, acc.: 51.56%] [G loss: 0.819270]\n",
      "epoch:9 step:8870 [D loss: 0.679806, acc.: 60.16%] [G loss: 0.817316]\n",
      "epoch:9 step:8871 [D loss: 0.665697, acc.: 57.81%] [G loss: 0.770950]\n",
      "epoch:9 step:8872 [D loss: 0.677867, acc.: 58.59%] [G loss: 0.765953]\n",
      "epoch:9 step:8873 [D loss: 0.647144, acc.: 58.59%] [G loss: 0.821057]\n",
      "epoch:9 step:8874 [D loss: 0.637445, acc.: 67.19%] [G loss: 0.807606]\n",
      "epoch:9 step:8875 [D loss: 0.642187, acc.: 61.72%] [G loss: 0.895739]\n",
      "epoch:9 step:8876 [D loss: 0.694662, acc.: 55.47%] [G loss: 0.809476]\n",
      "epoch:9 step:8877 [D loss: 0.694002, acc.: 57.03%] [G loss: 0.858519]\n",
      "epoch:9 step:8878 [D loss: 0.661512, acc.: 60.94%] [G loss: 0.844952]\n",
      "epoch:9 step:8879 [D loss: 0.670239, acc.: 57.81%] [G loss: 0.869267]\n",
      "epoch:9 step:8880 [D loss: 0.695969, acc.: 56.25%] [G loss: 0.872091]\n",
      "epoch:9 step:8881 [D loss: 0.788102, acc.: 39.06%] [G loss: 0.887320]\n",
      "epoch:9 step:8882 [D loss: 0.691763, acc.: 53.12%] [G loss: 0.948986]\n",
      "epoch:9 step:8883 [D loss: 0.700138, acc.: 58.59%] [G loss: 0.818936]\n",
      "epoch:9 step:8884 [D loss: 0.640350, acc.: 66.41%] [G loss: 0.767319]\n",
      "epoch:9 step:8885 [D loss: 0.690896, acc.: 50.78%] [G loss: 0.821409]\n",
      "epoch:9 step:8886 [D loss: 0.666757, acc.: 60.16%] [G loss: 0.752905]\n",
      "epoch:9 step:8887 [D loss: 0.648119, acc.: 57.81%] [G loss: 0.857661]\n",
      "epoch:9 step:8888 [D loss: 0.647949, acc.: 60.16%] [G loss: 0.862204]\n",
      "epoch:9 step:8889 [D loss: 0.656761, acc.: 64.06%] [G loss: 0.898346]\n",
      "epoch:9 step:8890 [D loss: 0.673084, acc.: 57.03%] [G loss: 0.887441]\n",
      "epoch:9 step:8891 [D loss: 0.693174, acc.: 55.47%] [G loss: 0.811865]\n",
      "epoch:9 step:8892 [D loss: 0.681427, acc.: 57.81%] [G loss: 0.791350]\n",
      "epoch:9 step:8893 [D loss: 0.700244, acc.: 49.22%] [G loss: 0.770760]\n",
      "epoch:9 step:8894 [D loss: 0.689628, acc.: 55.47%] [G loss: 0.780647]\n",
      "epoch:9 step:8895 [D loss: 0.681830, acc.: 55.47%] [G loss: 0.790272]\n",
      "epoch:9 step:8896 [D loss: 0.663575, acc.: 57.81%] [G loss: 0.856059]\n",
      "epoch:9 step:8897 [D loss: 0.655036, acc.: 64.06%] [G loss: 0.853285]\n",
      "epoch:9 step:8898 [D loss: 0.670101, acc.: 57.81%] [G loss: 0.834448]\n",
      "epoch:9 step:8899 [D loss: 0.671283, acc.: 59.38%] [G loss: 0.813542]\n",
      "epoch:9 step:8900 [D loss: 0.673062, acc.: 57.81%] [G loss: 0.822135]\n",
      "epoch:9 step:8901 [D loss: 0.661141, acc.: 54.69%] [G loss: 0.842984]\n",
      "epoch:9 step:8902 [D loss: 0.689099, acc.: 50.78%] [G loss: 0.828350]\n",
      "epoch:9 step:8903 [D loss: 0.681023, acc.: 59.38%] [G loss: 0.730634]\n",
      "epoch:9 step:8904 [D loss: 0.647053, acc.: 65.62%] [G loss: 0.818597]\n",
      "epoch:9 step:8905 [D loss: 0.706721, acc.: 54.69%] [G loss: 0.771132]\n",
      "epoch:9 step:8906 [D loss: 0.653780, acc.: 53.12%] [G loss: 0.827715]\n",
      "epoch:9 step:8907 [D loss: 0.647518, acc.: 64.06%] [G loss: 0.790512]\n",
      "epoch:9 step:8908 [D loss: 0.679465, acc.: 57.81%] [G loss: 0.787883]\n",
      "epoch:9 step:8909 [D loss: 0.676161, acc.: 59.38%] [G loss: 0.768519]\n",
      "epoch:9 step:8910 [D loss: 0.658739, acc.: 57.81%] [G loss: 0.795448]\n",
      "epoch:9 step:8911 [D loss: 0.678261, acc.: 54.69%] [G loss: 0.792519]\n",
      "epoch:9 step:8912 [D loss: 0.693019, acc.: 57.81%] [G loss: 0.742609]\n",
      "epoch:9 step:8913 [D loss: 0.712618, acc.: 49.22%] [G loss: 0.821105]\n",
      "epoch:9 step:8914 [D loss: 0.722279, acc.: 46.09%] [G loss: 0.812078]\n",
      "epoch:9 step:8915 [D loss: 0.655452, acc.: 62.50%] [G loss: 0.761598]\n",
      "epoch:9 step:8916 [D loss: 0.688418, acc.: 59.38%] [G loss: 0.819461]\n",
      "epoch:9 step:8917 [D loss: 0.638351, acc.: 62.50%] [G loss: 0.861744]\n",
      "epoch:9 step:8918 [D loss: 0.657678, acc.: 61.72%] [G loss: 0.841298]\n",
      "epoch:9 step:8919 [D loss: 0.693029, acc.: 58.59%] [G loss: 0.831812]\n",
      "epoch:9 step:8920 [D loss: 0.708737, acc.: 47.66%] [G loss: 0.871191]\n",
      "epoch:9 step:8921 [D loss: 0.646300, acc.: 69.53%] [G loss: 0.805591]\n",
      "epoch:9 step:8922 [D loss: 0.681090, acc.: 52.34%] [G loss: 0.820065]\n",
      "epoch:9 step:8923 [D loss: 0.660924, acc.: 53.91%] [G loss: 0.851155]\n",
      "epoch:9 step:8924 [D loss: 0.682390, acc.: 57.81%] [G loss: 0.756231]\n",
      "epoch:9 step:8925 [D loss: 0.679575, acc.: 52.34%] [G loss: 0.812045]\n",
      "epoch:9 step:8926 [D loss: 0.690909, acc.: 57.03%] [G loss: 0.831298]\n",
      "epoch:9 step:8927 [D loss: 0.664538, acc.: 60.94%] [G loss: 0.799383]\n",
      "epoch:9 step:8928 [D loss: 0.699316, acc.: 55.47%] [G loss: 0.884755]\n",
      "epoch:9 step:8929 [D loss: 0.658518, acc.: 62.50%] [G loss: 0.821195]\n",
      "epoch:9 step:8930 [D loss: 0.667545, acc.: 60.16%] [G loss: 0.799807]\n",
      "epoch:9 step:8931 [D loss: 0.686637, acc.: 57.03%] [G loss: 0.789255]\n",
      "epoch:9 step:8932 [D loss: 0.699347, acc.: 50.00%] [G loss: 0.814117]\n",
      "epoch:9 step:8933 [D loss: 0.685712, acc.: 55.47%] [G loss: 0.817611]\n",
      "epoch:9 step:8934 [D loss: 0.691766, acc.: 57.03%] [G loss: 0.763772]\n",
      "epoch:9 step:8935 [D loss: 0.664694, acc.: 59.38%] [G loss: 0.805210]\n",
      "epoch:9 step:8936 [D loss: 0.676829, acc.: 55.47%] [G loss: 0.792886]\n",
      "epoch:9 step:8937 [D loss: 0.700963, acc.: 50.78%] [G loss: 0.773493]\n",
      "epoch:9 step:8938 [D loss: 0.645563, acc.: 62.50%] [G loss: 0.870197]\n",
      "epoch:9 step:8939 [D loss: 0.645728, acc.: 61.72%] [G loss: 0.821437]\n",
      "epoch:9 step:8940 [D loss: 0.677882, acc.: 57.03%] [G loss: 0.853118]\n",
      "epoch:9 step:8941 [D loss: 0.655479, acc.: 59.38%] [G loss: 0.831177]\n",
      "epoch:9 step:8942 [D loss: 0.649194, acc.: 57.03%] [G loss: 0.817365]\n",
      "epoch:9 step:8943 [D loss: 0.660658, acc.: 53.12%] [G loss: 0.881567]\n",
      "epoch:9 step:8944 [D loss: 0.676815, acc.: 55.47%] [G loss: 0.848449]\n",
      "epoch:9 step:8945 [D loss: 0.698597, acc.: 52.34%] [G loss: 0.771169]\n",
      "epoch:9 step:8946 [D loss: 0.624814, acc.: 62.50%] [G loss: 0.899676]\n",
      "epoch:9 step:8947 [D loss: 0.670814, acc.: 61.72%] [G loss: 0.788896]\n",
      "epoch:9 step:8948 [D loss: 0.655543, acc.: 64.06%] [G loss: 0.824805]\n",
      "epoch:9 step:8949 [D loss: 0.665289, acc.: 64.84%] [G loss: 0.740054]\n",
      "epoch:9 step:8950 [D loss: 0.663748, acc.: 56.25%] [G loss: 0.764852]\n",
      "epoch:9 step:8951 [D loss: 0.665864, acc.: 59.38%] [G loss: 0.757497]\n",
      "epoch:9 step:8952 [D loss: 0.672511, acc.: 57.03%] [G loss: 0.861469]\n",
      "epoch:9 step:8953 [D loss: 0.645774, acc.: 55.47%] [G loss: 0.737935]\n",
      "epoch:9 step:8954 [D loss: 0.698138, acc.: 48.44%] [G loss: 0.770528]\n",
      "epoch:9 step:8955 [D loss: 0.722173, acc.: 45.31%] [G loss: 0.787580]\n",
      "epoch:9 step:8956 [D loss: 0.672445, acc.: 55.47%] [G loss: 0.859812]\n",
      "epoch:9 step:8957 [D loss: 0.640992, acc.: 62.50%] [G loss: 0.854531]\n",
      "epoch:9 step:8958 [D loss: 0.704155, acc.: 53.12%] [G loss: 1.006687]\n",
      "epoch:9 step:8959 [D loss: 0.699363, acc.: 55.47%] [G loss: 0.833460]\n",
      "epoch:9 step:8960 [D loss: 0.694751, acc.: 48.44%] [G loss: 0.860220]\n",
      "epoch:9 step:8961 [D loss: 0.688822, acc.: 50.00%] [G loss: 0.871336]\n",
      "epoch:9 step:8962 [D loss: 0.677680, acc.: 50.00%] [G loss: 0.852368]\n",
      "epoch:9 step:8963 [D loss: 0.686506, acc.: 58.59%] [G loss: 0.819801]\n",
      "epoch:9 step:8964 [D loss: 0.640972, acc.: 64.06%] [G loss: 0.923962]\n",
      "epoch:9 step:8965 [D loss: 0.663283, acc.: 59.38%] [G loss: 0.881619]\n",
      "epoch:9 step:8966 [D loss: 0.617811, acc.: 64.06%] [G loss: 0.787305]\n",
      "epoch:9 step:8967 [D loss: 0.668585, acc.: 57.03%] [G loss: 0.814281]\n",
      "epoch:9 step:8968 [D loss: 0.642454, acc.: 57.81%] [G loss: 0.793681]\n",
      "epoch:9 step:8969 [D loss: 0.684798, acc.: 52.34%] [G loss: 0.761985]\n",
      "epoch:9 step:8970 [D loss: 0.709298, acc.: 53.91%] [G loss: 0.793390]\n",
      "epoch:9 step:8971 [D loss: 0.689328, acc.: 61.72%] [G loss: 0.818056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8972 [D loss: 0.722324, acc.: 50.78%] [G loss: 0.720122]\n",
      "epoch:9 step:8973 [D loss: 0.690498, acc.: 52.34%] [G loss: 0.836642]\n",
      "epoch:9 step:8974 [D loss: 0.687258, acc.: 52.34%] [G loss: 0.856394]\n",
      "epoch:9 step:8975 [D loss: 0.715932, acc.: 49.22%] [G loss: 0.773399]\n",
      "epoch:9 step:8976 [D loss: 0.674009, acc.: 55.47%] [G loss: 0.848518]\n",
      "epoch:9 step:8977 [D loss: 0.616892, acc.: 72.66%] [G loss: 0.855257]\n",
      "epoch:9 step:8978 [D loss: 0.668051, acc.: 55.47%] [G loss: 0.847544]\n",
      "epoch:9 step:8979 [D loss: 0.666391, acc.: 55.47%] [G loss: 0.822170]\n",
      "epoch:9 step:8980 [D loss: 0.689125, acc.: 53.12%] [G loss: 0.831420]\n",
      "epoch:9 step:8981 [D loss: 0.694695, acc.: 53.12%] [G loss: 0.861839]\n",
      "epoch:9 step:8982 [D loss: 0.689556, acc.: 55.47%] [G loss: 0.809084]\n",
      "epoch:9 step:8983 [D loss: 0.644394, acc.: 58.59%] [G loss: 0.815926]\n",
      "epoch:9 step:8984 [D loss: 0.668258, acc.: 57.03%] [G loss: 0.889253]\n",
      "epoch:9 step:8985 [D loss: 0.661340, acc.: 60.16%] [G loss: 0.839619]\n",
      "epoch:9 step:8986 [D loss: 0.702626, acc.: 55.47%] [G loss: 0.900003]\n",
      "epoch:9 step:8987 [D loss: 0.670658, acc.: 55.47%] [G loss: 0.895231]\n",
      "epoch:9 step:8988 [D loss: 0.672911, acc.: 59.38%] [G loss: 0.859599]\n",
      "epoch:9 step:8989 [D loss: 0.681333, acc.: 56.25%] [G loss: 0.804892]\n",
      "epoch:9 step:8990 [D loss: 0.669993, acc.: 60.16%] [G loss: 0.787158]\n",
      "epoch:9 step:8991 [D loss: 0.662522, acc.: 62.50%] [G loss: 0.803320]\n",
      "epoch:9 step:8992 [D loss: 0.683272, acc.: 54.69%] [G loss: 0.793718]\n",
      "epoch:9 step:8993 [D loss: 0.691823, acc.: 53.91%] [G loss: 0.808505]\n",
      "epoch:9 step:8994 [D loss: 0.673642, acc.: 54.69%] [G loss: 0.870817]\n",
      "epoch:9 step:8995 [D loss: 0.669668, acc.: 58.59%] [G loss: 0.876655]\n",
      "epoch:9 step:8996 [D loss: 0.692660, acc.: 56.25%] [G loss: 0.834254]\n",
      "epoch:9 step:8997 [D loss: 0.696681, acc.: 50.78%] [G loss: 0.834178]\n",
      "epoch:9 step:8998 [D loss: 0.646899, acc.: 64.84%] [G loss: 0.838896]\n",
      "epoch:9 step:8999 [D loss: 0.647668, acc.: 61.72%] [G loss: 0.848940]\n",
      "epoch:9 step:9000 [D loss: 0.721500, acc.: 49.22%] [G loss: 0.814495]\n",
      "epoch:9 step:9001 [D loss: 0.659187, acc.: 57.81%] [G loss: 0.922319]\n",
      "epoch:9 step:9002 [D loss: 0.681503, acc.: 54.69%] [G loss: 0.873479]\n",
      "epoch:9 step:9003 [D loss: 0.701772, acc.: 50.78%] [G loss: 0.873825]\n",
      "epoch:9 step:9004 [D loss: 0.671235, acc.: 57.03%] [G loss: 0.887846]\n",
      "epoch:9 step:9005 [D loss: 0.636415, acc.: 64.06%] [G loss: 0.907970]\n",
      "epoch:9 step:9006 [D loss: 0.650891, acc.: 59.38%] [G loss: 0.871481]\n",
      "epoch:9 step:9007 [D loss: 0.700069, acc.: 53.12%] [G loss: 0.832630]\n",
      "epoch:9 step:9008 [D loss: 0.686653, acc.: 50.78%] [G loss: 0.838310]\n",
      "epoch:9 step:9009 [D loss: 0.660785, acc.: 62.50%] [G loss: 0.855384]\n",
      "epoch:9 step:9010 [D loss: 0.602444, acc.: 77.34%] [G loss: 0.856674]\n",
      "epoch:9 step:9011 [D loss: 0.667467, acc.: 60.94%] [G loss: 0.880817]\n",
      "epoch:9 step:9012 [D loss: 0.652752, acc.: 63.28%] [G loss: 0.802556]\n",
      "epoch:9 step:9013 [D loss: 0.656501, acc.: 56.25%] [G loss: 0.862449]\n",
      "epoch:9 step:9014 [D loss: 0.657511, acc.: 60.94%] [G loss: 0.852636]\n",
      "epoch:9 step:9015 [D loss: 0.656126, acc.: 62.50%] [G loss: 0.840311]\n",
      "epoch:9 step:9016 [D loss: 0.669821, acc.: 57.03%] [G loss: 0.809929]\n",
      "epoch:9 step:9017 [D loss: 0.645928, acc.: 64.84%] [G loss: 1.006423]\n",
      "epoch:9 step:9018 [D loss: 0.650372, acc.: 65.62%] [G loss: 0.823233]\n",
      "epoch:9 step:9019 [D loss: 0.680821, acc.: 50.00%] [G loss: 0.889209]\n",
      "epoch:9 step:9020 [D loss: 0.629550, acc.: 68.75%] [G loss: 0.904287]\n",
      "epoch:9 step:9021 [D loss: 0.655340, acc.: 60.94%] [G loss: 0.826277]\n",
      "epoch:9 step:9022 [D loss: 0.677578, acc.: 60.94%] [G loss: 0.969285]\n",
      "epoch:9 step:9023 [D loss: 0.694695, acc.: 59.38%] [G loss: 0.879247]\n",
      "epoch:9 step:9024 [D loss: 0.638363, acc.: 57.81%] [G loss: 0.828440]\n",
      "epoch:9 step:9025 [D loss: 0.614802, acc.: 65.62%] [G loss: 0.847733]\n",
      "epoch:9 step:9026 [D loss: 0.616456, acc.: 63.28%] [G loss: 0.944530]\n",
      "epoch:9 step:9027 [D loss: 0.659460, acc.: 60.16%] [G loss: 1.009876]\n",
      "epoch:9 step:9028 [D loss: 0.659366, acc.: 58.59%] [G loss: 0.942014]\n",
      "epoch:9 step:9029 [D loss: 0.679133, acc.: 61.72%] [G loss: 1.002048]\n",
      "epoch:9 step:9030 [D loss: 0.737415, acc.: 44.53%] [G loss: 1.016384]\n",
      "epoch:9 step:9031 [D loss: 0.660096, acc.: 63.28%] [G loss: 0.962221]\n",
      "epoch:9 step:9032 [D loss: 0.616096, acc.: 72.66%] [G loss: 0.815982]\n",
      "epoch:9 step:9033 [D loss: 0.794869, acc.: 42.97%] [G loss: 0.880323]\n",
      "epoch:9 step:9034 [D loss: 0.709197, acc.: 51.56%] [G loss: 0.903016]\n",
      "epoch:9 step:9035 [D loss: 0.642654, acc.: 65.62%] [G loss: 0.847571]\n",
      "epoch:9 step:9036 [D loss: 0.707016, acc.: 51.56%] [G loss: 0.859388]\n",
      "epoch:9 step:9037 [D loss: 0.656481, acc.: 62.50%] [G loss: 0.903872]\n",
      "epoch:9 step:9038 [D loss: 0.642442, acc.: 67.97%] [G loss: 0.760000]\n",
      "epoch:9 step:9039 [D loss: 0.703906, acc.: 56.25%] [G loss: 0.866083]\n",
      "epoch:9 step:9040 [D loss: 0.659519, acc.: 59.38%] [G loss: 0.848448]\n",
      "epoch:9 step:9041 [D loss: 0.674392, acc.: 65.62%] [G loss: 0.923169]\n",
      "epoch:9 step:9042 [D loss: 0.694224, acc.: 53.91%] [G loss: 0.852844]\n",
      "epoch:9 step:9043 [D loss: 0.712522, acc.: 47.66%] [G loss: 0.843253]\n",
      "epoch:9 step:9044 [D loss: 0.686419, acc.: 50.00%] [G loss: 0.834308]\n",
      "epoch:9 step:9045 [D loss: 0.683766, acc.: 59.38%] [G loss: 0.798955]\n",
      "epoch:9 step:9046 [D loss: 0.677357, acc.: 50.78%] [G loss: 0.743885]\n",
      "epoch:9 step:9047 [D loss: 0.618591, acc.: 69.53%] [G loss: 0.791423]\n",
      "epoch:9 step:9048 [D loss: 0.725134, acc.: 53.12%] [G loss: 0.854464]\n",
      "epoch:9 step:9049 [D loss: 0.696209, acc.: 51.56%] [G loss: 0.917430]\n",
      "epoch:9 step:9050 [D loss: 0.699358, acc.: 46.88%] [G loss: 0.883889]\n",
      "epoch:9 step:9051 [D loss: 0.687359, acc.: 60.16%] [G loss: 0.860493]\n",
      "epoch:9 step:9052 [D loss: 0.672977, acc.: 60.94%] [G loss: 0.891684]\n",
      "epoch:9 step:9053 [D loss: 0.616348, acc.: 67.19%] [G loss: 0.918209]\n",
      "epoch:9 step:9054 [D loss: 0.606323, acc.: 70.31%] [G loss: 0.935281]\n",
      "epoch:9 step:9055 [D loss: 0.634752, acc.: 69.53%] [G loss: 0.923145]\n",
      "epoch:9 step:9056 [D loss: 0.641907, acc.: 57.81%] [G loss: 1.000072]\n",
      "epoch:9 step:9057 [D loss: 0.639253, acc.: 57.81%] [G loss: 0.846113]\n",
      "epoch:9 step:9058 [D loss: 0.681005, acc.: 56.25%] [G loss: 0.863446]\n",
      "epoch:9 step:9059 [D loss: 0.695934, acc.: 46.88%] [G loss: 0.816512]\n",
      "epoch:9 step:9060 [D loss: 0.621080, acc.: 60.16%] [G loss: 0.986164]\n",
      "epoch:9 step:9061 [D loss: 0.705598, acc.: 59.38%] [G loss: 0.910376]\n",
      "epoch:9 step:9062 [D loss: 0.685272, acc.: 50.00%] [G loss: 0.763308]\n",
      "epoch:9 step:9063 [D loss: 0.667861, acc.: 57.81%] [G loss: 0.949063]\n",
      "epoch:9 step:9064 [D loss: 0.662622, acc.: 57.03%] [G loss: 0.855287]\n",
      "epoch:9 step:9065 [D loss: 0.683488, acc.: 48.44%] [G loss: 0.815390]\n",
      "epoch:9 step:9066 [D loss: 0.755230, acc.: 52.34%] [G loss: 0.949858]\n",
      "epoch:9 step:9067 [D loss: 0.727449, acc.: 50.00%] [G loss: 0.928442]\n",
      "epoch:9 step:9068 [D loss: 0.643953, acc.: 62.50%] [G loss: 1.092409]\n",
      "epoch:9 step:9069 [D loss: 0.680566, acc.: 60.94%] [G loss: 0.946039]\n",
      "epoch:9 step:9070 [D loss: 0.616046, acc.: 69.53%] [G loss: 0.954602]\n",
      "epoch:9 step:9071 [D loss: 0.663395, acc.: 61.72%] [G loss: 1.030502]\n",
      "epoch:9 step:9072 [D loss: 0.680647, acc.: 54.69%] [G loss: 0.910645]\n",
      "epoch:9 step:9073 [D loss: 0.626691, acc.: 64.84%] [G loss: 0.960581]\n",
      "epoch:9 step:9074 [D loss: 0.707224, acc.: 50.00%] [G loss: 0.954851]\n",
      "epoch:9 step:9075 [D loss: 0.688653, acc.: 54.69%] [G loss: 0.948823]\n",
      "epoch:9 step:9076 [D loss: 0.661262, acc.: 57.03%] [G loss: 0.884299]\n",
      "epoch:9 step:9077 [D loss: 0.631448, acc.: 64.06%] [G loss: 0.767745]\n",
      "epoch:9 step:9078 [D loss: 0.715703, acc.: 38.28%] [G loss: 0.889650]\n",
      "epoch:9 step:9079 [D loss: 0.862958, acc.: 39.06%] [G loss: 0.938052]\n",
      "epoch:9 step:9080 [D loss: 0.660683, acc.: 65.62%] [G loss: 0.900820]\n",
      "epoch:9 step:9081 [D loss: 0.671458, acc.: 60.94%] [G loss: 0.864684]\n",
      "epoch:9 step:9082 [D loss: 0.656225, acc.: 60.94%] [G loss: 0.897255]\n",
      "epoch:9 step:9083 [D loss: 0.634266, acc.: 61.72%] [G loss: 0.855721]\n",
      "epoch:9 step:9084 [D loss: 0.659664, acc.: 61.72%] [G loss: 0.850313]\n",
      "epoch:9 step:9085 [D loss: 0.674008, acc.: 57.81%] [G loss: 0.849962]\n",
      "epoch:9 step:9086 [D loss: 0.652225, acc.: 60.94%] [G loss: 0.844889]\n",
      "epoch:9 step:9087 [D loss: 0.657579, acc.: 57.81%] [G loss: 0.929917]\n",
      "epoch:9 step:9088 [D loss: 0.692201, acc.: 57.03%] [G loss: 0.790081]\n",
      "epoch:9 step:9089 [D loss: 0.643832, acc.: 63.28%] [G loss: 0.916026]\n",
      "epoch:9 step:9090 [D loss: 0.650364, acc.: 60.94%] [G loss: 0.803642]\n",
      "epoch:9 step:9091 [D loss: 0.676239, acc.: 58.59%] [G loss: 0.755174]\n",
      "epoch:9 step:9092 [D loss: 0.677857, acc.: 58.59%] [G loss: 0.765823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9093 [D loss: 0.684945, acc.: 50.00%] [G loss: 0.910574]\n",
      "epoch:9 step:9094 [D loss: 0.670547, acc.: 55.47%] [G loss: 0.729429]\n",
      "epoch:9 step:9095 [D loss: 0.678937, acc.: 57.03%] [G loss: 0.793095]\n",
      "epoch:9 step:9096 [D loss: 0.651569, acc.: 67.97%] [G loss: 0.868763]\n",
      "epoch:9 step:9097 [D loss: 0.660926, acc.: 61.72%] [G loss: 0.826996]\n",
      "epoch:9 step:9098 [D loss: 0.704070, acc.: 51.56%] [G loss: 0.784965]\n",
      "epoch:9 step:9099 [D loss: 0.671775, acc.: 50.78%] [G loss: 0.890170]\n",
      "epoch:9 step:9100 [D loss: 0.674897, acc.: 55.47%] [G loss: 0.859419]\n",
      "epoch:9 step:9101 [D loss: 0.673874, acc.: 53.91%] [G loss: 0.916126]\n",
      "epoch:9 step:9102 [D loss: 0.652729, acc.: 57.81%] [G loss: 0.951883]\n",
      "epoch:9 step:9103 [D loss: 0.650830, acc.: 62.50%] [G loss: 0.783274]\n",
      "epoch:9 step:9104 [D loss: 0.654628, acc.: 63.28%] [G loss: 0.811969]\n",
      "epoch:9 step:9105 [D loss: 0.665580, acc.: 53.12%] [G loss: 0.901454]\n",
      "epoch:9 step:9106 [D loss: 0.684722, acc.: 49.22%] [G loss: 0.861722]\n",
      "epoch:9 step:9107 [D loss: 0.638885, acc.: 64.84%] [G loss: 0.755120]\n",
      "epoch:9 step:9108 [D loss: 0.643465, acc.: 60.94%] [G loss: 0.879728]\n",
      "epoch:9 step:9109 [D loss: 0.665864, acc.: 56.25%] [G loss: 0.933310]\n",
      "epoch:9 step:9110 [D loss: 0.657322, acc.: 60.94%] [G loss: 0.835124]\n",
      "epoch:9 step:9111 [D loss: 0.694890, acc.: 48.44%] [G loss: 0.792250]\n",
      "epoch:9 step:9112 [D loss: 0.649354, acc.: 58.59%] [G loss: 0.901228]\n",
      "epoch:9 step:9113 [D loss: 0.720203, acc.: 44.53%] [G loss: 0.817779]\n",
      "epoch:9 step:9114 [D loss: 0.673204, acc.: 60.16%] [G loss: 0.908755]\n",
      "epoch:9 step:9115 [D loss: 0.678590, acc.: 53.12%] [G loss: 0.885063]\n",
      "epoch:9 step:9116 [D loss: 0.634428, acc.: 59.38%] [G loss: 0.831778]\n",
      "epoch:9 step:9117 [D loss: 0.638016, acc.: 64.06%] [G loss: 0.851477]\n",
      "epoch:9 step:9118 [D loss: 0.660235, acc.: 57.03%] [G loss: 0.907421]\n",
      "epoch:9 step:9119 [D loss: 0.690866, acc.: 54.69%] [G loss: 0.893467]\n",
      "epoch:9 step:9120 [D loss: 0.665422, acc.: 54.69%] [G loss: 0.889949]\n",
      "epoch:9 step:9121 [D loss: 0.663455, acc.: 54.69%] [G loss: 0.861917]\n",
      "epoch:9 step:9122 [D loss: 0.687058, acc.: 56.25%] [G loss: 0.872653]\n",
      "epoch:9 step:9123 [D loss: 0.688989, acc.: 50.78%] [G loss: 0.798817]\n",
      "epoch:9 step:9124 [D loss: 0.664948, acc.: 50.00%] [G loss: 0.871035]\n",
      "epoch:9 step:9125 [D loss: 0.648840, acc.: 60.16%] [G loss: 0.839601]\n",
      "epoch:9 step:9126 [D loss: 0.702194, acc.: 48.44%] [G loss: 0.765861]\n",
      "epoch:9 step:9127 [D loss: 0.677442, acc.: 58.59%] [G loss: 0.879064]\n",
      "epoch:9 step:9128 [D loss: 0.660255, acc.: 57.81%] [G loss: 0.886139]\n",
      "epoch:9 step:9129 [D loss: 0.734591, acc.: 47.66%] [G loss: 0.809390]\n",
      "epoch:9 step:9130 [D loss: 0.650603, acc.: 64.84%] [G loss: 0.812123]\n",
      "epoch:9 step:9131 [D loss: 0.721063, acc.: 51.56%] [G loss: 0.850848]\n",
      "epoch:9 step:9132 [D loss: 0.682023, acc.: 58.59%] [G loss: 0.774359]\n",
      "epoch:9 step:9133 [D loss: 0.679027, acc.: 52.34%] [G loss: 0.835338]\n",
      "epoch:9 step:9134 [D loss: 0.703876, acc.: 49.22%] [G loss: 0.928845]\n",
      "epoch:9 step:9135 [D loss: 0.660298, acc.: 57.03%] [G loss: 0.822616]\n",
      "epoch:9 step:9136 [D loss: 0.679646, acc.: 57.81%] [G loss: 0.852124]\n",
      "epoch:9 step:9137 [D loss: 0.669205, acc.: 62.50%] [G loss: 0.875232]\n",
      "epoch:9 step:9138 [D loss: 0.664949, acc.: 55.47%] [G loss: 0.869928]\n",
      "epoch:9 step:9139 [D loss: 0.643997, acc.: 67.97%] [G loss: 0.949333]\n",
      "epoch:9 step:9140 [D loss: 0.633686, acc.: 64.84%] [G loss: 0.871913]\n",
      "epoch:9 step:9141 [D loss: 0.633679, acc.: 64.84%] [G loss: 0.955489]\n",
      "epoch:9 step:9142 [D loss: 0.654640, acc.: 61.72%] [G loss: 0.945354]\n",
      "epoch:9 step:9143 [D loss: 0.661592, acc.: 57.03%] [G loss: 0.845500]\n",
      "epoch:9 step:9144 [D loss: 0.646258, acc.: 62.50%] [G loss: 0.843546]\n",
      "epoch:9 step:9145 [D loss: 0.668114, acc.: 60.16%] [G loss: 0.912110]\n",
      "epoch:9 step:9146 [D loss: 0.668363, acc.: 57.81%] [G loss: 0.907073]\n",
      "epoch:9 step:9147 [D loss: 0.619214, acc.: 70.31%] [G loss: 0.875808]\n",
      "epoch:9 step:9148 [D loss: 0.650195, acc.: 62.50%] [G loss: 0.818463]\n",
      "epoch:9 step:9149 [D loss: 0.711680, acc.: 52.34%] [G loss: 0.873937]\n",
      "epoch:9 step:9150 [D loss: 0.685690, acc.: 50.78%] [G loss: 0.867508]\n",
      "epoch:9 step:9151 [D loss: 0.612285, acc.: 64.06%] [G loss: 0.936232]\n",
      "epoch:9 step:9152 [D loss: 0.683019, acc.: 60.16%] [G loss: 0.936588]\n",
      "epoch:9 step:9153 [D loss: 0.679045, acc.: 59.38%] [G loss: 0.880534]\n",
      "epoch:9 step:9154 [D loss: 0.652195, acc.: 58.59%] [G loss: 0.996650]\n",
      "epoch:9 step:9155 [D loss: 0.647686, acc.: 64.06%] [G loss: 0.866919]\n",
      "epoch:9 step:9156 [D loss: 0.635810, acc.: 60.16%] [G loss: 0.957889]\n",
      "epoch:9 step:9157 [D loss: 0.625832, acc.: 61.72%] [G loss: 0.975897]\n",
      "epoch:9 step:9158 [D loss: 0.604306, acc.: 67.97%] [G loss: 0.855651]\n",
      "epoch:9 step:9159 [D loss: 0.642888, acc.: 62.50%] [G loss: 0.906064]\n",
      "epoch:9 step:9160 [D loss: 0.671324, acc.: 60.16%] [G loss: 0.813837]\n",
      "epoch:9 step:9161 [D loss: 0.642166, acc.: 61.72%] [G loss: 0.926196]\n",
      "epoch:9 step:9162 [D loss: 0.708160, acc.: 47.66%] [G loss: 0.967522]\n",
      "epoch:9 step:9163 [D loss: 0.691613, acc.: 51.56%] [G loss: 0.834008]\n",
      "epoch:9 step:9164 [D loss: 0.738494, acc.: 56.25%] [G loss: 1.011937]\n",
      "epoch:9 step:9165 [D loss: 0.651798, acc.: 62.50%] [G loss: 0.936918]\n",
      "epoch:9 step:9166 [D loss: 0.685881, acc.: 57.03%] [G loss: 1.377860]\n",
      "epoch:9 step:9167 [D loss: 0.662227, acc.: 70.31%] [G loss: 0.825840]\n",
      "epoch:9 step:9168 [D loss: 0.643937, acc.: 64.84%] [G loss: 0.874786]\n",
      "epoch:9 step:9169 [D loss: 0.651788, acc.: 67.97%] [G loss: 0.902405]\n",
      "epoch:9 step:9170 [D loss: 0.609178, acc.: 71.09%] [G loss: 0.953448]\n",
      "epoch:9 step:9171 [D loss: 0.636071, acc.: 64.06%] [G loss: 0.852348]\n",
      "epoch:9 step:9172 [D loss: 0.637685, acc.: 63.28%] [G loss: 1.015082]\n",
      "epoch:9 step:9173 [D loss: 0.667348, acc.: 65.62%] [G loss: 0.917627]\n",
      "epoch:9 step:9174 [D loss: 0.622963, acc.: 61.72%] [G loss: 1.051839]\n",
      "epoch:9 step:9175 [D loss: 0.610506, acc.: 67.19%] [G loss: 1.028122]\n",
      "epoch:9 step:9176 [D loss: 0.679543, acc.: 56.25%] [G loss: 1.136214]\n",
      "epoch:9 step:9177 [D loss: 0.641476, acc.: 64.84%] [G loss: 0.882960]\n",
      "epoch:9 step:9178 [D loss: 0.689765, acc.: 57.03%] [G loss: 0.929724]\n",
      "epoch:9 step:9179 [D loss: 0.721781, acc.: 46.09%] [G loss: 0.894750]\n",
      "epoch:9 step:9180 [D loss: 0.619330, acc.: 68.75%] [G loss: 0.872549]\n",
      "epoch:9 step:9181 [D loss: 0.612656, acc.: 62.50%] [G loss: 0.918194]\n",
      "epoch:9 step:9182 [D loss: 0.658050, acc.: 58.59%] [G loss: 1.026220]\n",
      "epoch:9 step:9183 [D loss: 0.683103, acc.: 52.34%] [G loss: 0.882101]\n",
      "epoch:9 step:9184 [D loss: 0.667300, acc.: 62.50%] [G loss: 1.004634]\n",
      "epoch:9 step:9185 [D loss: 0.639808, acc.: 64.84%] [G loss: 1.023702]\n",
      "epoch:9 step:9186 [D loss: 0.682619, acc.: 57.03%] [G loss: 0.877171]\n",
      "epoch:9 step:9187 [D loss: 0.667419, acc.: 60.16%] [G loss: 0.844705]\n",
      "epoch:9 step:9188 [D loss: 0.694046, acc.: 55.47%] [G loss: 0.822489]\n",
      "epoch:9 step:9189 [D loss: 0.654032, acc.: 61.72%] [G loss: 0.817206]\n",
      "epoch:9 step:9190 [D loss: 0.704592, acc.: 50.78%] [G loss: 0.871839]\n",
      "epoch:9 step:9191 [D loss: 0.667546, acc.: 62.50%] [G loss: 1.172675]\n",
      "epoch:9 step:9192 [D loss: 0.633782, acc.: 65.62%] [G loss: 0.864713]\n",
      "epoch:9 step:9193 [D loss: 0.641997, acc.: 68.75%] [G loss: 0.860212]\n",
      "epoch:9 step:9194 [D loss: 0.692162, acc.: 56.25%] [G loss: 0.868507]\n",
      "epoch:9 step:9195 [D loss: 0.603934, acc.: 70.31%] [G loss: 0.870419]\n",
      "epoch:9 step:9196 [D loss: 0.678089, acc.: 66.41%] [G loss: 0.843496]\n",
      "epoch:9 step:9197 [D loss: 0.577055, acc.: 75.78%] [G loss: 0.925914]\n",
      "epoch:9 step:9198 [D loss: 0.668770, acc.: 60.16%] [G loss: 0.889426]\n",
      "epoch:9 step:9199 [D loss: 0.682231, acc.: 57.03%] [G loss: 0.941519]\n",
      "epoch:9 step:9200 [D loss: 0.640619, acc.: 64.06%] [G loss: 1.009925]\n",
      "epoch:9 step:9201 [D loss: 0.629141, acc.: 60.16%] [G loss: 0.924200]\n",
      "epoch:9 step:9202 [D loss: 0.669398, acc.: 64.84%] [G loss: 0.992225]\n",
      "epoch:9 step:9203 [D loss: 0.619739, acc.: 61.72%] [G loss: 1.026770]\n",
      "epoch:9 step:9204 [D loss: 0.648571, acc.: 57.03%] [G loss: 1.032911]\n",
      "epoch:9 step:9205 [D loss: 0.676598, acc.: 56.25%] [G loss: 0.820035]\n",
      "epoch:9 step:9206 [D loss: 0.650988, acc.: 59.38%] [G loss: 0.857839]\n",
      "epoch:9 step:9207 [D loss: 0.703090, acc.: 53.12%] [G loss: 0.850064]\n",
      "epoch:9 step:9208 [D loss: 0.685055, acc.: 60.16%] [G loss: 0.873821]\n",
      "epoch:9 step:9209 [D loss: 0.649922, acc.: 59.38%] [G loss: 0.781154]\n",
      "epoch:9 step:9210 [D loss: 0.675198, acc.: 56.25%] [G loss: 0.869444]\n",
      "epoch:9 step:9211 [D loss: 0.651508, acc.: 54.69%] [G loss: 0.921666]\n",
      "epoch:9 step:9212 [D loss: 0.676545, acc.: 61.72%] [G loss: 0.868147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9213 [D loss: 0.683945, acc.: 55.47%] [G loss: 0.872814]\n",
      "epoch:9 step:9214 [D loss: 0.651924, acc.: 58.59%] [G loss: 0.860626]\n",
      "epoch:9 step:9215 [D loss: 0.615690, acc.: 71.09%] [G loss: 0.851564]\n",
      "epoch:9 step:9216 [D loss: 0.699994, acc.: 46.88%] [G loss: 0.796953]\n",
      "epoch:9 step:9217 [D loss: 0.673420, acc.: 57.03%] [G loss: 0.768667]\n",
      "epoch:9 step:9218 [D loss: 0.663893, acc.: 56.25%] [G loss: 0.844741]\n",
      "epoch:9 step:9219 [D loss: 0.690437, acc.: 48.44%] [G loss: 0.842890]\n",
      "epoch:9 step:9220 [D loss: 0.632865, acc.: 62.50%] [G loss: 0.869150]\n",
      "epoch:9 step:9221 [D loss: 0.670458, acc.: 58.59%] [G loss: 0.741411]\n",
      "epoch:9 step:9222 [D loss: 0.654182, acc.: 51.56%] [G loss: 0.796081]\n",
      "epoch:9 step:9223 [D loss: 0.636125, acc.: 61.72%] [G loss: 0.926359]\n",
      "epoch:9 step:9224 [D loss: 0.689244, acc.: 58.59%] [G loss: 0.854461]\n",
      "epoch:9 step:9225 [D loss: 0.640206, acc.: 64.84%] [G loss: 0.942502]\n",
      "epoch:9 step:9226 [D loss: 0.647051, acc.: 61.72%] [G loss: 1.034541]\n",
      "epoch:9 step:9227 [D loss: 0.645292, acc.: 55.47%] [G loss: 0.997929]\n",
      "epoch:9 step:9228 [D loss: 0.611913, acc.: 65.62%] [G loss: 1.037247]\n",
      "epoch:9 step:9229 [D loss: 0.638689, acc.: 61.72%] [G loss: 0.944608]\n",
      "epoch:9 step:9230 [D loss: 0.693444, acc.: 53.12%] [G loss: 0.871028]\n",
      "epoch:9 step:9231 [D loss: 0.707330, acc.: 50.78%] [G loss: 0.865070]\n",
      "epoch:9 step:9232 [D loss: 0.661594, acc.: 61.72%] [G loss: 0.787151]\n",
      "epoch:9 step:9233 [D loss: 0.699316, acc.: 55.47%] [G loss: 0.788120]\n",
      "epoch:9 step:9234 [D loss: 0.672141, acc.: 57.81%] [G loss: 0.835338]\n",
      "epoch:9 step:9235 [D loss: 0.670809, acc.: 56.25%] [G loss: 0.844929]\n",
      "epoch:9 step:9236 [D loss: 0.645736, acc.: 57.03%] [G loss: 0.843656]\n",
      "epoch:9 step:9237 [D loss: 0.709146, acc.: 56.25%] [G loss: 0.851423]\n",
      "epoch:9 step:9238 [D loss: 0.637340, acc.: 58.59%] [G loss: 0.939043]\n",
      "epoch:9 step:9239 [D loss: 0.657424, acc.: 64.06%] [G loss: 0.769470]\n",
      "epoch:9 step:9240 [D loss: 0.675748, acc.: 53.91%] [G loss: 0.957294]\n",
      "epoch:9 step:9241 [D loss: 0.667777, acc.: 60.16%] [G loss: 0.804669]\n",
      "epoch:9 step:9242 [D loss: 0.650878, acc.: 59.38%] [G loss: 0.792150]\n",
      "epoch:9 step:9243 [D loss: 0.675529, acc.: 58.59%] [G loss: 0.883729]\n",
      "epoch:9 step:9244 [D loss: 0.655579, acc.: 67.97%] [G loss: 0.856528]\n",
      "epoch:9 step:9245 [D loss: 0.693566, acc.: 55.47%] [G loss: 0.852411]\n",
      "epoch:9 step:9246 [D loss: 0.705223, acc.: 56.25%] [G loss: 0.926637]\n",
      "epoch:9 step:9247 [D loss: 0.638356, acc.: 60.16%] [G loss: 0.884623]\n",
      "epoch:9 step:9248 [D loss: 0.721744, acc.: 51.56%] [G loss: 0.818944]\n",
      "epoch:9 step:9249 [D loss: 0.682912, acc.: 57.81%] [G loss: 0.804263]\n",
      "epoch:9 step:9250 [D loss: 0.664598, acc.: 53.12%] [G loss: 0.838377]\n",
      "epoch:9 step:9251 [D loss: 0.694675, acc.: 45.31%] [G loss: 0.829042]\n",
      "epoch:9 step:9252 [D loss: 0.638149, acc.: 61.72%] [G loss: 0.864894]\n",
      "epoch:9 step:9253 [D loss: 0.654108, acc.: 62.50%] [G loss: 0.895521]\n",
      "epoch:9 step:9254 [D loss: 0.697636, acc.: 52.34%] [G loss: 0.892018]\n",
      "epoch:9 step:9255 [D loss: 0.660860, acc.: 60.94%] [G loss: 0.805920]\n",
      "epoch:9 step:9256 [D loss: 0.682200, acc.: 56.25%] [G loss: 0.938710]\n",
      "epoch:9 step:9257 [D loss: 0.707812, acc.: 50.78%] [G loss: 0.883493]\n",
      "epoch:9 step:9258 [D loss: 0.715503, acc.: 47.66%] [G loss: 0.914234]\n",
      "epoch:9 step:9259 [D loss: 0.650285, acc.: 66.41%] [G loss: 0.952059]\n",
      "epoch:9 step:9260 [D loss: 0.697457, acc.: 53.12%] [G loss: 0.943909]\n",
      "epoch:9 step:9261 [D loss: 0.754504, acc.: 48.44%] [G loss: 0.939358]\n",
      "epoch:9 step:9262 [D loss: 0.645637, acc.: 60.94%] [G loss: 0.965233]\n",
      "epoch:9 step:9263 [D loss: 0.692989, acc.: 55.47%] [G loss: 0.838545]\n",
      "epoch:9 step:9264 [D loss: 0.648923, acc.: 64.84%] [G loss: 0.882261]\n",
      "epoch:9 step:9265 [D loss: 0.689434, acc.: 56.25%] [G loss: 0.867044]\n",
      "epoch:9 step:9266 [D loss: 0.677300, acc.: 59.38%] [G loss: 0.963504]\n",
      "epoch:9 step:9267 [D loss: 0.659675, acc.: 55.47%] [G loss: 0.904483]\n",
      "epoch:9 step:9268 [D loss: 0.709756, acc.: 46.09%] [G loss: 0.838450]\n",
      "epoch:9 step:9269 [D loss: 0.667771, acc.: 57.81%] [G loss: 0.810466]\n",
      "epoch:9 step:9270 [D loss: 0.629885, acc.: 66.41%] [G loss: 1.126351]\n",
      "epoch:9 step:9271 [D loss: 0.630544, acc.: 66.41%] [G loss: 0.866070]\n",
      "epoch:9 step:9272 [D loss: 0.680845, acc.: 56.25%] [G loss: 0.783556]\n",
      "epoch:9 step:9273 [D loss: 0.654458, acc.: 58.59%] [G loss: 0.897768]\n",
      "epoch:9 step:9274 [D loss: 0.630242, acc.: 64.06%] [G loss: 0.857448]\n",
      "epoch:9 step:9275 [D loss: 0.649533, acc.: 60.94%] [G loss: 0.816554]\n",
      "epoch:9 step:9276 [D loss: 0.653034, acc.: 63.28%] [G loss: 0.877152]\n",
      "epoch:9 step:9277 [D loss: 0.718027, acc.: 50.00%] [G loss: 0.867112]\n",
      "epoch:9 step:9278 [D loss: 0.761233, acc.: 41.41%] [G loss: 0.809764]\n",
      "epoch:9 step:9279 [D loss: 0.650011, acc.: 57.03%] [G loss: 0.780900]\n",
      "epoch:9 step:9280 [D loss: 0.650133, acc.: 60.16%] [G loss: 0.880179]\n",
      "epoch:9 step:9281 [D loss: 0.669796, acc.: 60.94%] [G loss: 0.838599]\n",
      "epoch:9 step:9282 [D loss: 0.671384, acc.: 53.91%] [G loss: 0.824815]\n",
      "epoch:9 step:9283 [D loss: 0.636385, acc.: 64.84%] [G loss: 0.833201]\n",
      "epoch:9 step:9284 [D loss: 0.616746, acc.: 67.19%] [G loss: 0.909772]\n",
      "epoch:9 step:9285 [D loss: 0.650734, acc.: 63.28%] [G loss: 0.931803]\n",
      "epoch:9 step:9286 [D loss: 0.715726, acc.: 53.91%] [G loss: 0.863846]\n",
      "epoch:9 step:9287 [D loss: 0.646500, acc.: 62.50%] [G loss: 0.880509]\n",
      "epoch:9 step:9288 [D loss: 0.647002, acc.: 61.72%] [G loss: 0.882130]\n",
      "epoch:9 step:9289 [D loss: 0.683738, acc.: 55.47%] [G loss: 0.799514]\n",
      "epoch:9 step:9290 [D loss: 0.684392, acc.: 60.94%] [G loss: 0.778539]\n",
      "epoch:9 step:9291 [D loss: 0.681623, acc.: 52.34%] [G loss: 0.797522]\n",
      "epoch:9 step:9292 [D loss: 0.694801, acc.: 50.78%] [G loss: 0.930064]\n",
      "epoch:9 step:9293 [D loss: 0.672394, acc.: 56.25%] [G loss: 0.825820]\n",
      "epoch:9 step:9294 [D loss: 0.683174, acc.: 54.69%] [G loss: 0.796444]\n",
      "epoch:9 step:9295 [D loss: 0.707277, acc.: 47.66%] [G loss: 0.842110]\n",
      "epoch:9 step:9296 [D loss: 0.640355, acc.: 62.50%] [G loss: 0.842823]\n",
      "epoch:9 step:9297 [D loss: 0.674210, acc.: 53.91%] [G loss: 0.880647]\n",
      "epoch:9 step:9298 [D loss: 0.626409, acc.: 64.06%] [G loss: 0.851629]\n",
      "epoch:9 step:9299 [D loss: 0.596411, acc.: 67.97%] [G loss: 0.787380]\n",
      "epoch:9 step:9300 [D loss: 0.688442, acc.: 50.00%] [G loss: 0.813358]\n",
      "epoch:9 step:9301 [D loss: 0.638231, acc.: 68.75%] [G loss: 0.893376]\n",
      "epoch:9 step:9302 [D loss: 0.693042, acc.: 50.78%] [G loss: 0.901367]\n",
      "epoch:9 step:9303 [D loss: 0.664115, acc.: 57.81%] [G loss: 0.714044]\n",
      "epoch:9 step:9304 [D loss: 0.737617, acc.: 47.66%] [G loss: 0.842010]\n",
      "epoch:9 step:9305 [D loss: 0.696097, acc.: 51.56%] [G loss: 0.794991]\n",
      "epoch:9 step:9306 [D loss: 0.706305, acc.: 48.44%] [G loss: 0.834648]\n",
      "epoch:9 step:9307 [D loss: 0.657485, acc.: 58.59%] [G loss: 0.857888]\n",
      "epoch:9 step:9308 [D loss: 0.675965, acc.: 56.25%] [G loss: 0.885419]\n",
      "epoch:9 step:9309 [D loss: 0.678935, acc.: 56.25%] [G loss: 0.915809]\n",
      "epoch:9 step:9310 [D loss: 0.660635, acc.: 58.59%] [G loss: 0.891537]\n",
      "epoch:9 step:9311 [D loss: 0.679342, acc.: 52.34%] [G loss: 0.879873]\n",
      "epoch:9 step:9312 [D loss: 0.642891, acc.: 64.84%] [G loss: 0.996508]\n",
      "epoch:9 step:9313 [D loss: 0.654289, acc.: 62.50%] [G loss: 0.955373]\n",
      "epoch:9 step:9314 [D loss: 0.631270, acc.: 63.28%] [G loss: 0.948675]\n",
      "epoch:9 step:9315 [D loss: 0.604038, acc.: 68.75%] [G loss: 1.027264]\n",
      "epoch:9 step:9316 [D loss: 0.684211, acc.: 56.25%] [G loss: 0.870379]\n",
      "epoch:9 step:9317 [D loss: 0.615599, acc.: 66.41%] [G loss: 0.852182]\n",
      "epoch:9 step:9318 [D loss: 0.666871, acc.: 52.34%] [G loss: 0.809449]\n",
      "epoch:9 step:9319 [D loss: 0.692107, acc.: 57.81%] [G loss: 0.864323]\n",
      "epoch:9 step:9320 [D loss: 0.633431, acc.: 59.38%] [G loss: 0.761423]\n",
      "epoch:9 step:9321 [D loss: 0.620579, acc.: 66.41%] [G loss: 0.810552]\n",
      "epoch:9 step:9322 [D loss: 0.620690, acc.: 65.62%] [G loss: 0.913352]\n",
      "epoch:9 step:9323 [D loss: 0.668888, acc.: 58.59%] [G loss: 0.792963]\n",
      "epoch:9 step:9324 [D loss: 0.695863, acc.: 54.69%] [G loss: 0.797278]\n",
      "epoch:9 step:9325 [D loss: 0.708108, acc.: 50.78%] [G loss: 0.829329]\n",
      "epoch:9 step:9326 [D loss: 0.677524, acc.: 59.38%] [G loss: 0.934999]\n",
      "epoch:9 step:9327 [D loss: 0.625917, acc.: 67.97%] [G loss: 0.833161]\n",
      "epoch:9 step:9328 [D loss: 0.631516, acc.: 67.97%] [G loss: 0.795283]\n",
      "epoch:9 step:9329 [D loss: 0.651278, acc.: 64.06%] [G loss: 0.879476]\n",
      "epoch:9 step:9330 [D loss: 0.696554, acc.: 45.31%] [G loss: 0.896400]\n",
      "epoch:9 step:9331 [D loss: 0.637243, acc.: 68.75%] [G loss: 1.006679]\n",
      "epoch:9 step:9332 [D loss: 0.630434, acc.: 69.53%] [G loss: 0.882198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9333 [D loss: 0.627334, acc.: 66.41%] [G loss: 0.857129]\n",
      "epoch:9 step:9334 [D loss: 0.696620, acc.: 50.78%] [G loss: 1.059214]\n",
      "epoch:9 step:9335 [D loss: 0.650538, acc.: 61.72%] [G loss: 0.912260]\n",
      "epoch:9 step:9336 [D loss: 0.686822, acc.: 61.72%] [G loss: 0.925408]\n",
      "epoch:9 step:9337 [D loss: 0.669998, acc.: 57.81%] [G loss: 0.696826]\n",
      "epoch:9 step:9338 [D loss: 0.628534, acc.: 63.28%] [G loss: 0.867584]\n",
      "epoch:9 step:9339 [D loss: 0.666899, acc.: 54.69%] [G loss: 0.851464]\n",
      "epoch:9 step:9340 [D loss: 0.659803, acc.: 60.94%] [G loss: 0.786336]\n",
      "epoch:9 step:9341 [D loss: 0.766977, acc.: 49.22%] [G loss: 0.772825]\n",
      "epoch:9 step:9342 [D loss: 0.672076, acc.: 53.91%] [G loss: 0.987003]\n",
      "epoch:9 step:9343 [D loss: 0.694898, acc.: 46.88%] [G loss: 0.931201]\n",
      "epoch:9 step:9344 [D loss: 0.681615, acc.: 54.69%] [G loss: 0.795532]\n",
      "epoch:9 step:9345 [D loss: 0.731330, acc.: 48.44%] [G loss: 0.825336]\n",
      "epoch:9 step:9346 [D loss: 0.670278, acc.: 51.56%] [G loss: 0.862833]\n",
      "epoch:9 step:9347 [D loss: 0.700624, acc.: 52.34%] [G loss: 0.844954]\n",
      "epoch:9 step:9348 [D loss: 0.649587, acc.: 56.25%] [G loss: 0.840884]\n",
      "epoch:9 step:9349 [D loss: 0.674057, acc.: 58.59%] [G loss: 0.934249]\n",
      "epoch:9 step:9350 [D loss: 0.642326, acc.: 63.28%] [G loss: 0.872731]\n",
      "epoch:9 step:9351 [D loss: 0.609668, acc.: 66.41%] [G loss: 0.952319]\n",
      "epoch:9 step:9352 [D loss: 0.680831, acc.: 53.91%] [G loss: 1.440531]\n",
      "epoch:9 step:9353 [D loss: 0.657680, acc.: 57.03%] [G loss: 0.747948]\n",
      "epoch:9 step:9354 [D loss: 0.686723, acc.: 57.03%] [G loss: 0.791014]\n",
      "epoch:9 step:9355 [D loss: 0.647573, acc.: 57.81%] [G loss: 1.024789]\n",
      "epoch:9 step:9356 [D loss: 0.604493, acc.: 68.75%] [G loss: 0.790160]\n",
      "epoch:9 step:9357 [D loss: 0.659763, acc.: 57.81%] [G loss: 0.881719]\n",
      "epoch:9 step:9358 [D loss: 0.636423, acc.: 60.94%] [G loss: 0.919637]\n",
      "epoch:9 step:9359 [D loss: 0.631975, acc.: 64.06%] [G loss: 0.960434]\n",
      "epoch:9 step:9360 [D loss: 0.680675, acc.: 51.56%] [G loss: 0.904915]\n",
      "epoch:9 step:9361 [D loss: 0.631379, acc.: 61.72%] [G loss: 0.968804]\n",
      "epoch:9 step:9362 [D loss: 0.736559, acc.: 44.53%] [G loss: 0.927888]\n",
      "epoch:9 step:9363 [D loss: 0.669331, acc.: 56.25%] [G loss: 0.960110]\n",
      "epoch:9 step:9364 [D loss: 0.694723, acc.: 51.56%] [G loss: 1.054898]\n",
      "epoch:9 step:9365 [D loss: 0.650949, acc.: 57.81%] [G loss: 1.059036]\n",
      "epoch:9 step:9366 [D loss: 0.624377, acc.: 67.97%] [G loss: 0.936716]\n",
      "epoch:9 step:9367 [D loss: 0.641250, acc.: 61.72%] [G loss: 0.907348]\n",
      "epoch:9 step:9368 [D loss: 0.714035, acc.: 50.00%] [G loss: 0.866852]\n",
      "epoch:9 step:9369 [D loss: 0.684444, acc.: 58.59%] [G loss: 1.028622]\n",
      "epoch:9 step:9370 [D loss: 0.701961, acc.: 46.88%] [G loss: 0.937710]\n",
      "epoch:10 step:9371 [D loss: 0.656310, acc.: 62.50%] [G loss: 0.941228]\n",
      "epoch:10 step:9372 [D loss: 0.669536, acc.: 57.03%] [G loss: 0.906603]\n",
      "epoch:10 step:9373 [D loss: 0.639565, acc.: 53.12%] [G loss: 0.906571]\n",
      "epoch:10 step:9374 [D loss: 0.663554, acc.: 56.25%] [G loss: 0.858717]\n",
      "epoch:10 step:9375 [D loss: 0.618881, acc.: 67.19%] [G loss: 0.942894]\n",
      "epoch:10 step:9376 [D loss: 0.636698, acc.: 63.28%] [G loss: 0.965101]\n",
      "epoch:10 step:9377 [D loss: 0.702229, acc.: 60.16%] [G loss: 0.985879]\n",
      "epoch:10 step:9378 [D loss: 0.671520, acc.: 61.72%] [G loss: 0.954843]\n",
      "epoch:10 step:9379 [D loss: 0.656368, acc.: 64.84%] [G loss: 0.927069]\n",
      "epoch:10 step:9380 [D loss: 0.674367, acc.: 55.47%] [G loss: 0.926313]\n",
      "epoch:10 step:9381 [D loss: 0.661903, acc.: 62.50%] [G loss: 0.994179]\n",
      "epoch:10 step:9382 [D loss: 0.666477, acc.: 59.38%] [G loss: 0.980477]\n",
      "epoch:10 step:9383 [D loss: 0.679241, acc.: 62.50%] [G loss: 0.958699]\n",
      "epoch:10 step:9384 [D loss: 0.701853, acc.: 53.91%] [G loss: 0.887490]\n",
      "epoch:10 step:9385 [D loss: 0.653910, acc.: 64.84%] [G loss: 0.955564]\n",
      "epoch:10 step:9386 [D loss: 0.645751, acc.: 54.69%] [G loss: 0.880154]\n",
      "epoch:10 step:9387 [D loss: 0.621105, acc.: 66.41%] [G loss: 0.896348]\n",
      "epoch:10 step:9388 [D loss: 0.660563, acc.: 57.03%] [G loss: 0.772441]\n",
      "epoch:10 step:9389 [D loss: 0.671524, acc.: 54.69%] [G loss: 0.934064]\n",
      "epoch:10 step:9390 [D loss: 0.644220, acc.: 61.72%] [G loss: 0.846896]\n",
      "epoch:10 step:9391 [D loss: 0.644858, acc.: 61.72%] [G loss: 0.851697]\n",
      "epoch:10 step:9392 [D loss: 0.644815, acc.: 56.25%] [G loss: 0.887891]\n",
      "epoch:10 step:9393 [D loss: 0.643691, acc.: 60.16%] [G loss: 1.075171]\n",
      "epoch:10 step:9394 [D loss: 0.661544, acc.: 61.72%] [G loss: 0.927972]\n",
      "epoch:10 step:9395 [D loss: 0.639638, acc.: 63.28%] [G loss: 0.945754]\n",
      "epoch:10 step:9396 [D loss: 0.631410, acc.: 64.84%] [G loss: 0.959967]\n",
      "epoch:10 step:9397 [D loss: 0.653346, acc.: 59.38%] [G loss: 0.934320]\n",
      "epoch:10 step:9398 [D loss: 0.656543, acc.: 58.59%] [G loss: 0.937921]\n",
      "epoch:10 step:9399 [D loss: 0.661175, acc.: 57.81%] [G loss: 0.759999]\n",
      "epoch:10 step:9400 [D loss: 0.639838, acc.: 63.28%] [G loss: 0.925696]\n",
      "epoch:10 step:9401 [D loss: 0.639516, acc.: 58.59%] [G loss: 0.913284]\n",
      "epoch:10 step:9402 [D loss: 0.694029, acc.: 55.47%] [G loss: 0.899260]\n",
      "epoch:10 step:9403 [D loss: 0.633420, acc.: 69.53%] [G loss: 0.940363]\n",
      "epoch:10 step:9404 [D loss: 0.675891, acc.: 57.81%] [G loss: 0.892602]\n",
      "epoch:10 step:9405 [D loss: 0.705637, acc.: 52.34%] [G loss: 0.765287]\n",
      "epoch:10 step:9406 [D loss: 0.713504, acc.: 49.22%] [G loss: 0.877662]\n",
      "epoch:10 step:9407 [D loss: 0.707048, acc.: 56.25%] [G loss: 0.898890]\n",
      "epoch:10 step:9408 [D loss: 0.753269, acc.: 50.00%] [G loss: 0.910472]\n",
      "epoch:10 step:9409 [D loss: 0.651072, acc.: 62.50%] [G loss: 0.899656]\n",
      "epoch:10 step:9410 [D loss: 0.718612, acc.: 53.12%] [G loss: 0.964538]\n",
      "epoch:10 step:9411 [D loss: 0.670556, acc.: 58.59%] [G loss: 1.060807]\n",
      "epoch:10 step:9412 [D loss: 0.706020, acc.: 48.44%] [G loss: 0.866374]\n",
      "epoch:10 step:9413 [D loss: 0.621328, acc.: 70.31%] [G loss: 0.965112]\n",
      "epoch:10 step:9414 [D loss: 0.658762, acc.: 66.41%] [G loss: 0.791830]\n",
      "epoch:10 step:9415 [D loss: 0.706393, acc.: 51.56%] [G loss: 0.898881]\n",
      "epoch:10 step:9416 [D loss: 0.642515, acc.: 61.72%] [G loss: 0.855426]\n",
      "epoch:10 step:9417 [D loss: 0.677601, acc.: 53.12%] [G loss: 0.882198]\n",
      "epoch:10 step:9418 [D loss: 0.615594, acc.: 69.53%] [G loss: 0.900887]\n",
      "epoch:10 step:9419 [D loss: 0.644253, acc.: 54.69%] [G loss: 0.884974]\n",
      "epoch:10 step:9420 [D loss: 0.616984, acc.: 66.41%] [G loss: 0.950225]\n",
      "epoch:10 step:9421 [D loss: 0.676069, acc.: 54.69%] [G loss: 0.824824]\n",
      "epoch:10 step:9422 [D loss: 0.668469, acc.: 53.91%] [G loss: 0.876177]\n",
      "epoch:10 step:9423 [D loss: 0.699950, acc.: 47.66%] [G loss: 0.914008]\n",
      "epoch:10 step:9424 [D loss: 0.668611, acc.: 54.69%] [G loss: 0.955848]\n",
      "epoch:10 step:9425 [D loss: 0.660509, acc.: 54.69%] [G loss: 0.883483]\n",
      "epoch:10 step:9426 [D loss: 0.623289, acc.: 63.28%] [G loss: 0.799616]\n",
      "epoch:10 step:9427 [D loss: 0.699457, acc.: 59.38%] [G loss: 0.860740]\n",
      "epoch:10 step:9428 [D loss: 0.685821, acc.: 60.16%] [G loss: 0.858014]\n",
      "epoch:10 step:9429 [D loss: 0.637869, acc.: 67.97%] [G loss: 0.835812]\n",
      "epoch:10 step:9430 [D loss: 0.725256, acc.: 52.34%] [G loss: 0.833211]\n",
      "epoch:10 step:9431 [D loss: 0.692143, acc.: 54.69%] [G loss: 0.911201]\n",
      "epoch:10 step:9432 [D loss: 0.637159, acc.: 70.31%] [G loss: 0.894880]\n",
      "epoch:10 step:9433 [D loss: 0.665794, acc.: 55.47%] [G loss: 0.961176]\n",
      "epoch:10 step:9434 [D loss: 0.672607, acc.: 61.72%] [G loss: 0.992879]\n",
      "epoch:10 step:9435 [D loss: 0.613341, acc.: 64.06%] [G loss: 1.040261]\n",
      "epoch:10 step:9436 [D loss: 0.620137, acc.: 66.41%] [G loss: 0.964038]\n",
      "epoch:10 step:9437 [D loss: 0.675162, acc.: 55.47%] [G loss: 0.981603]\n",
      "epoch:10 step:9438 [D loss: 0.709451, acc.: 49.22%] [G loss: 0.936027]\n",
      "epoch:10 step:9439 [D loss: 0.622064, acc.: 69.53%] [G loss: 0.945370]\n",
      "epoch:10 step:9440 [D loss: 0.619964, acc.: 67.97%] [G loss: 1.038114]\n",
      "epoch:10 step:9441 [D loss: 0.692016, acc.: 50.78%] [G loss: 0.910242]\n",
      "epoch:10 step:9442 [D loss: 0.663079, acc.: 61.72%] [G loss: 0.856143]\n",
      "epoch:10 step:9443 [D loss: 0.685924, acc.: 53.12%] [G loss: 0.868315]\n",
      "epoch:10 step:9444 [D loss: 0.652947, acc.: 57.81%] [G loss: 0.910574]\n",
      "epoch:10 step:9445 [D loss: 0.665158, acc.: 57.81%] [G loss: 0.908038]\n",
      "epoch:10 step:9446 [D loss: 0.648744, acc.: 64.06%] [G loss: 0.938772]\n",
      "epoch:10 step:9447 [D loss: 0.702229, acc.: 53.12%] [G loss: 0.823199]\n",
      "epoch:10 step:9448 [D loss: 0.608847, acc.: 71.88%] [G loss: 0.853730]\n",
      "epoch:10 step:9449 [D loss: 0.654237, acc.: 56.25%] [G loss: 0.847170]\n",
      "epoch:10 step:9450 [D loss: 0.691152, acc.: 59.38%] [G loss: 0.863281]\n",
      "epoch:10 step:9451 [D loss: 0.636289, acc.: 62.50%] [G loss: 0.878095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9452 [D loss: 0.702858, acc.: 50.78%] [G loss: 0.881584]\n",
      "epoch:10 step:9453 [D loss: 0.616382, acc.: 75.00%] [G loss: 0.764684]\n",
      "epoch:10 step:9454 [D loss: 0.665540, acc.: 56.25%] [G loss: 0.812321]\n",
      "epoch:10 step:9455 [D loss: 0.656946, acc.: 62.50%] [G loss: 0.876569]\n",
      "epoch:10 step:9456 [D loss: 0.624177, acc.: 71.09%] [G loss: 0.862063]\n",
      "epoch:10 step:9457 [D loss: 0.651233, acc.: 60.16%] [G loss: 0.771681]\n",
      "epoch:10 step:9458 [D loss: 0.720698, acc.: 46.88%] [G loss: 0.861477]\n",
      "epoch:10 step:9459 [D loss: 0.678315, acc.: 55.47%] [G loss: 0.837982]\n",
      "epoch:10 step:9460 [D loss: 0.703812, acc.: 46.09%] [G loss: 0.825508]\n",
      "epoch:10 step:9461 [D loss: 0.665879, acc.: 59.38%] [G loss: 0.906154]\n",
      "epoch:10 step:9462 [D loss: 0.720768, acc.: 45.31%] [G loss: 0.788133]\n",
      "epoch:10 step:9463 [D loss: 0.627337, acc.: 60.16%] [G loss: 0.839912]\n",
      "epoch:10 step:9464 [D loss: 0.688853, acc.: 57.03%] [G loss: 0.821274]\n",
      "epoch:10 step:9465 [D loss: 0.706517, acc.: 47.66%] [G loss: 0.929656]\n",
      "epoch:10 step:9466 [D loss: 0.675531, acc.: 55.47%] [G loss: 0.854221]\n",
      "epoch:10 step:9467 [D loss: 0.660230, acc.: 56.25%] [G loss: 0.822005]\n",
      "epoch:10 step:9468 [D loss: 0.658491, acc.: 60.16%] [G loss: 0.874149]\n",
      "epoch:10 step:9469 [D loss: 0.672980, acc.: 58.59%] [G loss: 0.857531]\n",
      "epoch:10 step:9470 [D loss: 0.663043, acc.: 59.38%] [G loss: 0.966038]\n",
      "epoch:10 step:9471 [D loss: 0.716206, acc.: 46.88%] [G loss: 0.905507]\n",
      "epoch:10 step:9472 [D loss: 0.647029, acc.: 62.50%] [G loss: 0.842818]\n",
      "epoch:10 step:9473 [D loss: 0.657062, acc.: 60.16%] [G loss: 0.901901]\n",
      "epoch:10 step:9474 [D loss: 0.677461, acc.: 60.94%] [G loss: 0.942499]\n",
      "epoch:10 step:9475 [D loss: 0.636591, acc.: 67.19%] [G loss: 0.864605]\n",
      "epoch:10 step:9476 [D loss: 0.603272, acc.: 68.75%] [G loss: 0.879063]\n",
      "epoch:10 step:9477 [D loss: 0.686651, acc.: 55.47%] [G loss: 0.752936]\n",
      "epoch:10 step:9478 [D loss: 0.609747, acc.: 67.19%] [G loss: 0.841650]\n",
      "epoch:10 step:9479 [D loss: 0.692355, acc.: 55.47%] [G loss: 0.797643]\n",
      "epoch:10 step:9480 [D loss: 0.678797, acc.: 51.56%] [G loss: 0.911959]\n",
      "epoch:10 step:9481 [D loss: 0.733270, acc.: 50.00%] [G loss: 0.884814]\n",
      "epoch:10 step:9482 [D loss: 0.711553, acc.: 56.25%] [G loss: 0.879653]\n",
      "epoch:10 step:9483 [D loss: 0.648963, acc.: 60.16%] [G loss: 0.889326]\n",
      "epoch:10 step:9484 [D loss: 0.653148, acc.: 60.16%] [G loss: 0.798996]\n",
      "epoch:10 step:9485 [D loss: 0.707102, acc.: 48.44%] [G loss: 0.871580]\n",
      "epoch:10 step:9486 [D loss: 0.705784, acc.: 53.91%] [G loss: 0.923565]\n",
      "epoch:10 step:9487 [D loss: 0.676711, acc.: 60.94%] [G loss: 0.782457]\n",
      "epoch:10 step:9488 [D loss: 0.673193, acc.: 52.34%] [G loss: 0.853720]\n",
      "epoch:10 step:9489 [D loss: 0.636509, acc.: 67.19%] [G loss: 0.841017]\n",
      "epoch:10 step:9490 [D loss: 0.654124, acc.: 60.16%] [G loss: 0.973088]\n",
      "epoch:10 step:9491 [D loss: 0.690252, acc.: 51.56%] [G loss: 0.785908]\n",
      "epoch:10 step:9492 [D loss: 0.654055, acc.: 60.94%] [G loss: 0.958747]\n",
      "epoch:10 step:9493 [D loss: 0.659521, acc.: 53.91%] [G loss: 0.867286]\n",
      "epoch:10 step:9494 [D loss: 0.729431, acc.: 41.41%] [G loss: 0.872653]\n",
      "epoch:10 step:9495 [D loss: 0.622201, acc.: 70.31%] [G loss: 0.970394]\n",
      "epoch:10 step:9496 [D loss: 0.657222, acc.: 53.12%] [G loss: 0.980930]\n",
      "epoch:10 step:9497 [D loss: 0.659682, acc.: 58.59%] [G loss: 0.984016]\n",
      "epoch:10 step:9498 [D loss: 0.638013, acc.: 64.84%] [G loss: 0.976980]\n",
      "epoch:10 step:9499 [D loss: 0.646182, acc.: 59.38%] [G loss: 0.862782]\n",
      "epoch:10 step:9500 [D loss: 0.655125, acc.: 63.28%] [G loss: 0.910217]\n",
      "epoch:10 step:9501 [D loss: 0.629163, acc.: 64.84%] [G loss: 0.869417]\n",
      "epoch:10 step:9502 [D loss: 0.675266, acc.: 57.81%] [G loss: 0.836163]\n",
      "epoch:10 step:9503 [D loss: 0.695397, acc.: 53.12%] [G loss: 0.891151]\n",
      "epoch:10 step:9504 [D loss: 0.632984, acc.: 68.75%] [G loss: 0.876941]\n",
      "epoch:10 step:9505 [D loss: 0.632249, acc.: 65.62%] [G loss: 0.867951]\n",
      "epoch:10 step:9506 [D loss: 0.750723, acc.: 43.75%] [G loss: 0.827081]\n",
      "epoch:10 step:9507 [D loss: 0.660907, acc.: 58.59%] [G loss: 0.939199]\n",
      "epoch:10 step:9508 [D loss: 0.703080, acc.: 53.91%] [G loss: 0.814447]\n",
      "epoch:10 step:9509 [D loss: 0.656865, acc.: 63.28%] [G loss: 0.834600]\n",
      "epoch:10 step:9510 [D loss: 0.679986, acc.: 54.69%] [G loss: 0.835589]\n",
      "epoch:10 step:9511 [D loss: 0.720734, acc.: 52.34%] [G loss: 0.764349]\n",
      "epoch:10 step:9512 [D loss: 0.600922, acc.: 73.44%] [G loss: 0.911738]\n",
      "epoch:10 step:9513 [D loss: 0.690844, acc.: 57.81%] [G loss: 0.846167]\n",
      "epoch:10 step:9514 [D loss: 0.644528, acc.: 63.28%] [G loss: 0.812208]\n",
      "epoch:10 step:9515 [D loss: 0.660147, acc.: 58.59%] [G loss: 0.816427]\n",
      "epoch:10 step:9516 [D loss: 0.677426, acc.: 56.25%] [G loss: 0.842441]\n",
      "epoch:10 step:9517 [D loss: 0.666837, acc.: 55.47%] [G loss: 0.906089]\n",
      "epoch:10 step:9518 [D loss: 0.667667, acc.: 66.41%] [G loss: 0.859062]\n",
      "epoch:10 step:9519 [D loss: 0.631398, acc.: 67.97%] [G loss: 0.881071]\n",
      "epoch:10 step:9520 [D loss: 0.634074, acc.: 64.06%] [G loss: 0.888941]\n",
      "epoch:10 step:9521 [D loss: 0.639251, acc.: 68.75%] [G loss: 0.919025]\n",
      "epoch:10 step:9522 [D loss: 0.659458, acc.: 61.72%] [G loss: 0.877101]\n",
      "epoch:10 step:9523 [D loss: 0.716635, acc.: 53.12%] [G loss: 0.938565]\n",
      "epoch:10 step:9524 [D loss: 0.672794, acc.: 53.12%] [G loss: 0.853836]\n",
      "epoch:10 step:9525 [D loss: 0.667798, acc.: 57.81%] [G loss: 0.831282]\n",
      "epoch:10 step:9526 [D loss: 0.648238, acc.: 64.06%] [G loss: 0.894796]\n",
      "epoch:10 step:9527 [D loss: 0.643571, acc.: 61.72%] [G loss: 0.884682]\n",
      "epoch:10 step:9528 [D loss: 0.666444, acc.: 60.94%] [G loss: 0.892578]\n",
      "epoch:10 step:9529 [D loss: 0.709292, acc.: 54.69%] [G loss: 0.979983]\n",
      "epoch:10 step:9530 [D loss: 0.652251, acc.: 62.50%] [G loss: 0.865431]\n",
      "epoch:10 step:9531 [D loss: 0.694668, acc.: 56.25%] [G loss: 0.871719]\n",
      "epoch:10 step:9532 [D loss: 0.669162, acc.: 60.94%] [G loss: 0.876172]\n",
      "epoch:10 step:9533 [D loss: 0.697622, acc.: 47.66%] [G loss: 0.870919]\n",
      "epoch:10 step:9534 [D loss: 0.704741, acc.: 54.69%] [G loss: 0.890175]\n",
      "epoch:10 step:9535 [D loss: 0.669112, acc.: 57.81%] [G loss: 0.811921]\n",
      "epoch:10 step:9536 [D loss: 0.667664, acc.: 58.59%] [G loss: 0.917185]\n",
      "epoch:10 step:9537 [D loss: 0.623268, acc.: 68.75%] [G loss: 0.888592]\n",
      "epoch:10 step:9538 [D loss: 0.693665, acc.: 57.81%] [G loss: 0.803818]\n",
      "epoch:10 step:9539 [D loss: 0.682815, acc.: 53.91%] [G loss: 0.833193]\n",
      "epoch:10 step:9540 [D loss: 0.719535, acc.: 51.56%] [G loss: 0.865848]\n",
      "epoch:10 step:9541 [D loss: 0.676812, acc.: 53.91%] [G loss: 0.835433]\n",
      "epoch:10 step:9542 [D loss: 0.664524, acc.: 60.16%] [G loss: 0.815828]\n",
      "epoch:10 step:9543 [D loss: 0.646128, acc.: 60.94%] [G loss: 0.791123]\n",
      "epoch:10 step:9544 [D loss: 0.679864, acc.: 50.78%] [G loss: 0.757886]\n",
      "epoch:10 step:9545 [D loss: 0.727084, acc.: 45.31%] [G loss: 0.827067]\n",
      "epoch:10 step:9546 [D loss: 0.746774, acc.: 38.28%] [G loss: 0.796837]\n",
      "epoch:10 step:9547 [D loss: 0.663328, acc.: 56.25%] [G loss: 0.784959]\n",
      "epoch:10 step:9548 [D loss: 0.661914, acc.: 56.25%] [G loss: 0.778365]\n",
      "epoch:10 step:9549 [D loss: 0.694539, acc.: 55.47%] [G loss: 0.750373]\n",
      "epoch:10 step:9550 [D loss: 0.683270, acc.: 57.81%] [G loss: 0.759097]\n",
      "epoch:10 step:9551 [D loss: 0.671005, acc.: 55.47%] [G loss: 0.864278]\n",
      "epoch:10 step:9552 [D loss: 0.730793, acc.: 42.97%] [G loss: 0.787136]\n",
      "epoch:10 step:9553 [D loss: 0.682911, acc.: 53.12%] [G loss: 0.803090]\n",
      "epoch:10 step:9554 [D loss: 0.686391, acc.: 56.25%] [G loss: 0.752016]\n",
      "epoch:10 step:9555 [D loss: 0.745119, acc.: 35.94%] [G loss: 0.777826]\n",
      "epoch:10 step:9556 [D loss: 0.657584, acc.: 62.50%] [G loss: 0.808813]\n",
      "epoch:10 step:9557 [D loss: 0.654122, acc.: 65.62%] [G loss: 0.750094]\n",
      "epoch:10 step:9558 [D loss: 0.653573, acc.: 62.50%] [G loss: 0.909272]\n",
      "epoch:10 step:9559 [D loss: 0.698039, acc.: 60.16%] [G loss: 0.794975]\n",
      "epoch:10 step:9560 [D loss: 0.666003, acc.: 54.69%] [G loss: 0.855995]\n",
      "epoch:10 step:9561 [D loss: 0.679257, acc.: 53.91%] [G loss: 0.832906]\n",
      "epoch:10 step:9562 [D loss: 0.649833, acc.: 65.62%] [G loss: 0.844679]\n",
      "epoch:10 step:9563 [D loss: 0.681452, acc.: 53.91%] [G loss: 0.767775]\n",
      "epoch:10 step:9564 [D loss: 0.650702, acc.: 58.59%] [G loss: 0.837377]\n",
      "epoch:10 step:9565 [D loss: 0.669732, acc.: 56.25%] [G loss: 0.866836]\n",
      "epoch:10 step:9566 [D loss: 0.694258, acc.: 51.56%] [G loss: 0.815001]\n",
      "epoch:10 step:9567 [D loss: 0.642723, acc.: 61.72%] [G loss: 0.843788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9568 [D loss: 0.711068, acc.: 53.12%] [G loss: 0.793867]\n",
      "epoch:10 step:9569 [D loss: 0.648823, acc.: 63.28%] [G loss: 0.805773]\n",
      "epoch:10 step:9570 [D loss: 0.710665, acc.: 55.47%] [G loss: 0.746958]\n",
      "epoch:10 step:9571 [D loss: 0.724785, acc.: 43.75%] [G loss: 0.807202]\n",
      "epoch:10 step:9572 [D loss: 0.677952, acc.: 57.03%] [G loss: 0.839188]\n",
      "epoch:10 step:9573 [D loss: 0.677047, acc.: 54.69%] [G loss: 0.899846]\n",
      "epoch:10 step:9574 [D loss: 0.598049, acc.: 69.53%] [G loss: 0.841961]\n",
      "epoch:10 step:9575 [D loss: 0.691806, acc.: 50.00%] [G loss: 0.851120]\n",
      "epoch:10 step:9576 [D loss: 0.700578, acc.: 48.44%] [G loss: 0.911366]\n",
      "epoch:10 step:9577 [D loss: 0.658620, acc.: 59.38%] [G loss: 0.925271]\n",
      "epoch:10 step:9578 [D loss: 0.628480, acc.: 65.62%] [G loss: 0.898906]\n",
      "epoch:10 step:9579 [D loss: 0.690449, acc.: 46.88%] [G loss: 0.773401]\n",
      "epoch:10 step:9580 [D loss: 0.659563, acc.: 62.50%] [G loss: 0.812632]\n",
      "epoch:10 step:9581 [D loss: 0.659334, acc.: 61.72%] [G loss: 0.812554]\n",
      "epoch:10 step:9582 [D loss: 0.684710, acc.: 53.12%] [G loss: 0.880406]\n",
      "epoch:10 step:9583 [D loss: 0.684075, acc.: 57.03%] [G loss: 0.853844]\n",
      "epoch:10 step:9584 [D loss: 0.702277, acc.: 52.34%] [G loss: 0.851126]\n",
      "epoch:10 step:9585 [D loss: 0.689730, acc.: 56.25%] [G loss: 0.777668]\n",
      "epoch:10 step:9586 [D loss: 0.706753, acc.: 49.22%] [G loss: 0.818428]\n",
      "epoch:10 step:9587 [D loss: 0.708516, acc.: 53.91%] [G loss: 0.901515]\n",
      "epoch:10 step:9588 [D loss: 0.676220, acc.: 60.16%] [G loss: 0.750932]\n",
      "epoch:10 step:9589 [D loss: 0.643353, acc.: 64.06%] [G loss: 0.821643]\n",
      "epoch:10 step:9590 [D loss: 0.728980, acc.: 45.31%] [G loss: 0.802736]\n",
      "epoch:10 step:9591 [D loss: 0.694238, acc.: 49.22%] [G loss: 0.813549]\n",
      "epoch:10 step:9592 [D loss: 0.678911, acc.: 53.91%] [G loss: 0.809716]\n",
      "epoch:10 step:9593 [D loss: 0.674333, acc.: 60.16%] [G loss: 0.836724]\n",
      "epoch:10 step:9594 [D loss: 0.679332, acc.: 54.69%] [G loss: 0.822521]\n",
      "epoch:10 step:9595 [D loss: 0.697426, acc.: 50.00%] [G loss: 0.765419]\n",
      "epoch:10 step:9596 [D loss: 0.680444, acc.: 55.47%] [G loss: 0.835451]\n",
      "epoch:10 step:9597 [D loss: 0.669435, acc.: 58.59%] [G loss: 0.842306]\n",
      "epoch:10 step:9598 [D loss: 0.653146, acc.: 59.38%] [G loss: 0.801927]\n",
      "epoch:10 step:9599 [D loss: 0.648785, acc.: 67.97%] [G loss: 0.803252]\n",
      "epoch:10 step:9600 [D loss: 0.679022, acc.: 59.38%] [G loss: 0.752824]\n",
      "epoch:10 step:9601 [D loss: 0.672728, acc.: 57.03%] [G loss: 0.767661]\n",
      "epoch:10 step:9602 [D loss: 0.668224, acc.: 61.72%] [G loss: 0.876874]\n",
      "epoch:10 step:9603 [D loss: 0.723084, acc.: 40.62%] [G loss: 0.745635]\n",
      "epoch:10 step:9604 [D loss: 0.648919, acc.: 60.16%] [G loss: 0.847563]\n",
      "epoch:10 step:9605 [D loss: 0.609712, acc.: 75.78%] [G loss: 0.847801]\n",
      "epoch:10 step:9606 [D loss: 0.722145, acc.: 49.22%] [G loss: 0.833103]\n",
      "epoch:10 step:9607 [D loss: 0.643236, acc.: 64.06%] [G loss: 0.818083]\n",
      "epoch:10 step:9608 [D loss: 0.670888, acc.: 61.72%] [G loss: 0.830582]\n",
      "epoch:10 step:9609 [D loss: 0.652751, acc.: 59.38%] [G loss: 0.753424]\n",
      "epoch:10 step:9610 [D loss: 0.665497, acc.: 61.72%] [G loss: 0.778020]\n",
      "epoch:10 step:9611 [D loss: 0.609325, acc.: 64.84%] [G loss: 0.856933]\n",
      "epoch:10 step:9612 [D loss: 0.695138, acc.: 53.12%] [G loss: 0.711554]\n",
      "epoch:10 step:9613 [D loss: 0.673758, acc.: 57.03%] [G loss: 0.825669]\n",
      "epoch:10 step:9614 [D loss: 0.715086, acc.: 46.88%] [G loss: 0.724189]\n",
      "epoch:10 step:9615 [D loss: 0.713139, acc.: 46.88%] [G loss: 0.769138]\n",
      "epoch:10 step:9616 [D loss: 0.763200, acc.: 39.84%] [G loss: 0.762437]\n",
      "epoch:10 step:9617 [D loss: 0.692275, acc.: 56.25%] [G loss: 0.863386]\n",
      "epoch:10 step:9618 [D loss: 0.693328, acc.: 53.91%] [G loss: 0.868755]\n",
      "epoch:10 step:9619 [D loss: 0.692894, acc.: 56.25%] [G loss: 0.830639]\n",
      "epoch:10 step:9620 [D loss: 0.654590, acc.: 60.16%] [G loss: 0.849645]\n",
      "epoch:10 step:9621 [D loss: 0.663261, acc.: 60.94%] [G loss: 0.895206]\n",
      "epoch:10 step:9622 [D loss: 0.656030, acc.: 64.84%] [G loss: 0.830102]\n",
      "epoch:10 step:9623 [D loss: 0.649318, acc.: 61.72%] [G loss: 0.820103]\n",
      "epoch:10 step:9624 [D loss: 0.682811, acc.: 60.94%] [G loss: 0.936464]\n",
      "epoch:10 step:9625 [D loss: 0.678772, acc.: 56.25%] [G loss: 1.031759]\n",
      "epoch:10 step:9626 [D loss: 0.690514, acc.: 55.47%] [G loss: 0.846508]\n",
      "epoch:10 step:9627 [D loss: 0.642464, acc.: 65.62%] [G loss: 0.862209]\n",
      "epoch:10 step:9628 [D loss: 0.660911, acc.: 59.38%] [G loss: 0.774746]\n",
      "epoch:10 step:9629 [D loss: 0.665918, acc.: 59.38%] [G loss: 0.907023]\n",
      "epoch:10 step:9630 [D loss: 0.690446, acc.: 55.47%] [G loss: 0.821861]\n",
      "epoch:10 step:9631 [D loss: 0.673433, acc.: 57.81%] [G loss: 0.875732]\n",
      "epoch:10 step:9632 [D loss: 0.679884, acc.: 59.38%] [G loss: 0.789888]\n",
      "epoch:10 step:9633 [D loss: 0.633514, acc.: 64.84%] [G loss: 0.819382]\n",
      "epoch:10 step:9634 [D loss: 0.687356, acc.: 57.81%] [G loss: 0.800087]\n",
      "epoch:10 step:9635 [D loss: 0.685073, acc.: 49.22%] [G loss: 0.784461]\n",
      "epoch:10 step:9636 [D loss: 0.634319, acc.: 59.38%] [G loss: 0.753618]\n",
      "epoch:10 step:9637 [D loss: 0.669844, acc.: 56.25%] [G loss: 0.788132]\n",
      "epoch:10 step:9638 [D loss: 0.711142, acc.: 46.09%] [G loss: 0.775327]\n",
      "epoch:10 step:9639 [D loss: 0.709774, acc.: 46.88%] [G loss: 0.876549]\n",
      "epoch:10 step:9640 [D loss: 0.651524, acc.: 51.56%] [G loss: 0.833276]\n",
      "epoch:10 step:9641 [D loss: 0.627869, acc.: 63.28%] [G loss: 0.808056]\n",
      "epoch:10 step:9642 [D loss: 0.663851, acc.: 57.81%] [G loss: 0.811455]\n",
      "epoch:10 step:9643 [D loss: 0.633622, acc.: 69.53%] [G loss: 0.749247]\n",
      "epoch:10 step:9644 [D loss: 0.629681, acc.: 67.19%] [G loss: 0.764679]\n",
      "epoch:10 step:9645 [D loss: 0.687764, acc.: 53.12%] [G loss: 0.913997]\n",
      "epoch:10 step:9646 [D loss: 0.665317, acc.: 60.16%] [G loss: 0.839739]\n",
      "epoch:10 step:9647 [D loss: 0.727244, acc.: 45.31%] [G loss: 0.889890]\n",
      "epoch:10 step:9648 [D loss: 0.691117, acc.: 42.19%] [G loss: 0.813408]\n",
      "epoch:10 step:9649 [D loss: 0.685123, acc.: 53.91%] [G loss: 0.910114]\n",
      "epoch:10 step:9650 [D loss: 0.699103, acc.: 55.47%] [G loss: 0.788913]\n",
      "epoch:10 step:9651 [D loss: 0.708166, acc.: 51.56%] [G loss: 0.822273]\n",
      "epoch:10 step:9652 [D loss: 0.715545, acc.: 49.22%] [G loss: 0.811057]\n",
      "epoch:10 step:9653 [D loss: 0.636307, acc.: 65.62%] [G loss: 0.901793]\n",
      "epoch:10 step:9654 [D loss: 0.673958, acc.: 57.81%] [G loss: 0.925741]\n",
      "epoch:10 step:9655 [D loss: 0.686150, acc.: 52.34%] [G loss: 0.939239]\n",
      "epoch:10 step:9656 [D loss: 0.639300, acc.: 67.97%] [G loss: 0.859017]\n",
      "epoch:10 step:9657 [D loss: 0.683427, acc.: 57.03%] [G loss: 0.881791]\n",
      "epoch:10 step:9658 [D loss: 0.690640, acc.: 51.56%] [G loss: 0.826281]\n",
      "epoch:10 step:9659 [D loss: 0.683009, acc.: 52.34%] [G loss: 0.831049]\n",
      "epoch:10 step:9660 [D loss: 0.630747, acc.: 70.31%] [G loss: 0.864687]\n",
      "epoch:10 step:9661 [D loss: 0.700629, acc.: 50.00%] [G loss: 0.857790]\n",
      "epoch:10 step:9662 [D loss: 0.636520, acc.: 59.38%] [G loss: 0.883233]\n",
      "epoch:10 step:9663 [D loss: 0.660299, acc.: 59.38%] [G loss: 0.761317]\n",
      "epoch:10 step:9664 [D loss: 0.646511, acc.: 63.28%] [G loss: 0.852598]\n",
      "epoch:10 step:9665 [D loss: 0.683415, acc.: 50.00%] [G loss: 0.860506]\n",
      "epoch:10 step:9666 [D loss: 0.698039, acc.: 47.66%] [G loss: 0.830067]\n",
      "epoch:10 step:9667 [D loss: 0.608014, acc.: 67.19%] [G loss: 0.900085]\n",
      "epoch:10 step:9668 [D loss: 0.654879, acc.: 59.38%] [G loss: 0.889340]\n",
      "epoch:10 step:9669 [D loss: 0.634206, acc.: 67.19%] [G loss: 0.811550]\n",
      "epoch:10 step:9670 [D loss: 0.649615, acc.: 67.19%] [G loss: 0.857765]\n",
      "epoch:10 step:9671 [D loss: 0.653573, acc.: 62.50%] [G loss: 0.874410]\n",
      "epoch:10 step:9672 [D loss: 0.638975, acc.: 65.62%] [G loss: 0.962088]\n",
      "epoch:10 step:9673 [D loss: 0.672609, acc.: 56.25%] [G loss: 0.860398]\n",
      "epoch:10 step:9674 [D loss: 0.765756, acc.: 42.97%] [G loss: 0.769705]\n",
      "epoch:10 step:9675 [D loss: 0.692219, acc.: 52.34%] [G loss: 0.679671]\n",
      "epoch:10 step:9676 [D loss: 0.659388, acc.: 61.72%] [G loss: 0.878045]\n",
      "epoch:10 step:9677 [D loss: 0.663938, acc.: 53.91%] [G loss: 0.785688]\n",
      "epoch:10 step:9678 [D loss: 0.656059, acc.: 57.03%] [G loss: 0.783227]\n",
      "epoch:10 step:9679 [D loss: 0.680848, acc.: 53.12%] [G loss: 0.827207]\n",
      "epoch:10 step:9680 [D loss: 0.651831, acc.: 64.84%] [G loss: 0.940532]\n",
      "epoch:10 step:9681 [D loss: 0.637347, acc.: 57.81%] [G loss: 0.829098]\n",
      "epoch:10 step:9682 [D loss: 0.654676, acc.: 60.16%] [G loss: 0.926708]\n",
      "epoch:10 step:9683 [D loss: 0.677661, acc.: 60.94%] [G loss: 1.027644]\n",
      "epoch:10 step:9684 [D loss: 0.688087, acc.: 51.56%] [G loss: 0.899773]\n",
      "epoch:10 step:9685 [D loss: 0.576278, acc.: 75.78%] [G loss: 0.968951]\n",
      "epoch:10 step:9686 [D loss: 0.696405, acc.: 46.09%] [G loss: 0.902446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9687 [D loss: 0.628241, acc.: 63.28%] [G loss: 0.756997]\n",
      "epoch:10 step:9688 [D loss: 0.730967, acc.: 44.53%] [G loss: 0.817959]\n",
      "epoch:10 step:9689 [D loss: 0.616657, acc.: 62.50%] [G loss: 0.870537]\n",
      "epoch:10 step:9690 [D loss: 0.669866, acc.: 61.72%] [G loss: 0.837664]\n",
      "epoch:10 step:9691 [D loss: 0.762450, acc.: 38.28%] [G loss: 0.833162]\n",
      "epoch:10 step:9692 [D loss: 0.704858, acc.: 53.12%] [G loss: 0.841658]\n",
      "epoch:10 step:9693 [D loss: 0.695648, acc.: 50.78%] [G loss: 0.827839]\n",
      "epoch:10 step:9694 [D loss: 0.607871, acc.: 67.97%] [G loss: 0.908048]\n",
      "epoch:10 step:9695 [D loss: 0.730639, acc.: 48.44%] [G loss: 0.827939]\n",
      "epoch:10 step:9696 [D loss: 0.686090, acc.: 60.94%] [G loss: 0.821792]\n",
      "epoch:10 step:9697 [D loss: 0.655361, acc.: 57.81%] [G loss: 0.869776]\n",
      "epoch:10 step:9698 [D loss: 0.656001, acc.: 59.38%] [G loss: 0.888421]\n",
      "epoch:10 step:9699 [D loss: 0.670241, acc.: 57.81%] [G loss: 0.805216]\n",
      "epoch:10 step:9700 [D loss: 0.690475, acc.: 54.69%] [G loss: 0.773835]\n",
      "epoch:10 step:9701 [D loss: 0.703341, acc.: 51.56%] [G loss: 0.883206]\n",
      "epoch:10 step:9702 [D loss: 0.667347, acc.: 57.03%] [G loss: 0.821095]\n",
      "epoch:10 step:9703 [D loss: 0.680076, acc.: 60.16%] [G loss: 0.833339]\n",
      "epoch:10 step:9704 [D loss: 0.656688, acc.: 59.38%] [G loss: 0.758537]\n",
      "epoch:10 step:9705 [D loss: 0.625555, acc.: 67.19%] [G loss: 0.817101]\n",
      "epoch:10 step:9706 [D loss: 0.660290, acc.: 58.59%] [G loss: 0.725348]\n",
      "epoch:10 step:9707 [D loss: 0.596081, acc.: 66.41%] [G loss: 0.760667]\n",
      "epoch:10 step:9708 [D loss: 0.661944, acc.: 57.81%] [G loss: 0.754819]\n",
      "epoch:10 step:9709 [D loss: 0.632192, acc.: 67.19%] [G loss: 0.822797]\n",
      "epoch:10 step:9710 [D loss: 0.716699, acc.: 55.47%] [G loss: 0.817214]\n",
      "epoch:10 step:9711 [D loss: 0.654863, acc.: 62.50%] [G loss: 0.814481]\n",
      "epoch:10 step:9712 [D loss: 0.758719, acc.: 50.00%] [G loss: 0.809203]\n",
      "epoch:10 step:9713 [D loss: 0.694630, acc.: 54.69%] [G loss: 0.886536]\n",
      "epoch:10 step:9714 [D loss: 0.709746, acc.: 53.91%] [G loss: 0.878133]\n",
      "epoch:10 step:9715 [D loss: 0.627215, acc.: 61.72%] [G loss: 0.892832]\n",
      "epoch:10 step:9716 [D loss: 0.632732, acc.: 61.72%] [G loss: 1.584602]\n",
      "epoch:10 step:9717 [D loss: 0.647389, acc.: 62.50%] [G loss: 0.879802]\n",
      "epoch:10 step:9718 [D loss: 0.680485, acc.: 57.81%] [G loss: 0.838692]\n",
      "epoch:10 step:9719 [D loss: 0.647445, acc.: 60.16%] [G loss: 0.796873]\n",
      "epoch:10 step:9720 [D loss: 0.674057, acc.: 53.91%] [G loss: 0.827919]\n",
      "epoch:10 step:9721 [D loss: 0.681506, acc.: 53.12%] [G loss: 0.819440]\n",
      "epoch:10 step:9722 [D loss: 0.691157, acc.: 55.47%] [G loss: 0.846326]\n",
      "epoch:10 step:9723 [D loss: 0.668615, acc.: 50.78%] [G loss: 0.811016]\n",
      "epoch:10 step:9724 [D loss: 0.657099, acc.: 58.59%] [G loss: 0.778147]\n",
      "epoch:10 step:9725 [D loss: 0.633934, acc.: 67.19%] [G loss: 0.878812]\n",
      "epoch:10 step:9726 [D loss: 0.746174, acc.: 42.97%] [G loss: 0.831856]\n",
      "epoch:10 step:9727 [D loss: 0.650167, acc.: 60.16%] [G loss: 0.827847]\n",
      "epoch:10 step:9728 [D loss: 0.682751, acc.: 56.25%] [G loss: 0.825912]\n",
      "epoch:10 step:9729 [D loss: 0.665744, acc.: 61.72%] [G loss: 0.821658]\n",
      "epoch:10 step:9730 [D loss: 0.658923, acc.: 64.06%] [G loss: 0.860827]\n",
      "epoch:10 step:9731 [D loss: 0.709094, acc.: 50.00%] [G loss: 0.848054]\n",
      "epoch:10 step:9732 [D loss: 0.638542, acc.: 60.16%] [G loss: 0.828818]\n",
      "epoch:10 step:9733 [D loss: 0.649403, acc.: 54.69%] [G loss: 0.844260]\n",
      "epoch:10 step:9734 [D loss: 0.621550, acc.: 67.19%] [G loss: 0.770685]\n",
      "epoch:10 step:9735 [D loss: 0.655750, acc.: 60.16%] [G loss: 0.923498]\n",
      "epoch:10 step:9736 [D loss: 0.699504, acc.: 56.25%] [G loss: 0.862741]\n",
      "epoch:10 step:9737 [D loss: 0.677331, acc.: 56.25%] [G loss: 0.820027]\n",
      "epoch:10 step:9738 [D loss: 0.684489, acc.: 58.59%] [G loss: 0.848549]\n",
      "epoch:10 step:9739 [D loss: 0.650421, acc.: 67.19%] [G loss: 0.855599]\n",
      "epoch:10 step:9740 [D loss: 0.712946, acc.: 56.25%] [G loss: 0.849111]\n",
      "epoch:10 step:9741 [D loss: 0.669869, acc.: 57.03%] [G loss: 0.841442]\n",
      "epoch:10 step:9742 [D loss: 0.664771, acc.: 57.03%] [G loss: 0.824401]\n",
      "epoch:10 step:9743 [D loss: 0.692093, acc.: 47.66%] [G loss: 0.879321]\n",
      "epoch:10 step:9744 [D loss: 0.676527, acc.: 57.03%] [G loss: 0.836589]\n",
      "epoch:10 step:9745 [D loss: 0.643181, acc.: 62.50%] [G loss: 0.812455]\n",
      "epoch:10 step:9746 [D loss: 0.673621, acc.: 58.59%] [G loss: 0.916029]\n",
      "epoch:10 step:9747 [D loss: 0.683008, acc.: 56.25%] [G loss: 0.843215]\n",
      "epoch:10 step:9748 [D loss: 0.712166, acc.: 53.12%] [G loss: 0.802802]\n",
      "epoch:10 step:9749 [D loss: 0.667675, acc.: 60.16%] [G loss: 0.878984]\n",
      "epoch:10 step:9750 [D loss: 0.673714, acc.: 57.81%] [G loss: 0.819222]\n",
      "epoch:10 step:9751 [D loss: 0.650255, acc.: 57.81%] [G loss: 0.874876]\n",
      "epoch:10 step:9752 [D loss: 0.654332, acc.: 57.03%] [G loss: 0.894399]\n",
      "epoch:10 step:9753 [D loss: 0.691501, acc.: 54.69%] [G loss: 0.815030]\n",
      "epoch:10 step:9754 [D loss: 0.694443, acc.: 53.91%] [G loss: 0.844562]\n",
      "epoch:10 step:9755 [D loss: 0.734507, acc.: 50.78%] [G loss: 0.805449]\n",
      "epoch:10 step:9756 [D loss: 0.643223, acc.: 65.62%] [G loss: 0.803534]\n",
      "epoch:10 step:9757 [D loss: 0.646223, acc.: 61.72%] [G loss: 0.902956]\n",
      "epoch:10 step:9758 [D loss: 0.650865, acc.: 64.06%] [G loss: 0.865229]\n",
      "epoch:10 step:9759 [D loss: 0.709754, acc.: 49.22%] [G loss: 1.169069]\n",
      "epoch:10 step:9760 [D loss: 0.627108, acc.: 66.41%] [G loss: 0.885918]\n",
      "epoch:10 step:9761 [D loss: 0.674136, acc.: 60.94%] [G loss: 0.912286]\n",
      "epoch:10 step:9762 [D loss: 0.622978, acc.: 67.97%] [G loss: 0.786745]\n",
      "epoch:10 step:9763 [D loss: 0.649571, acc.: 63.28%] [G loss: 0.885487]\n",
      "epoch:10 step:9764 [D loss: 0.656114, acc.: 63.28%] [G loss: 0.918425]\n",
      "epoch:10 step:9765 [D loss: 0.658253, acc.: 56.25%] [G loss: 0.788639]\n",
      "epoch:10 step:9766 [D loss: 0.669884, acc.: 53.12%] [G loss: 0.881379]\n",
      "epoch:10 step:9767 [D loss: 0.681350, acc.: 58.59%] [G loss: 0.811128]\n",
      "epoch:10 step:9768 [D loss: 0.657173, acc.: 60.94%] [G loss: 0.903633]\n",
      "epoch:10 step:9769 [D loss: 0.676677, acc.: 57.81%] [G loss: 0.879951]\n",
      "epoch:10 step:9770 [D loss: 0.660607, acc.: 63.28%] [G loss: 0.844329]\n",
      "epoch:10 step:9771 [D loss: 0.647159, acc.: 63.28%] [G loss: 0.983636]\n",
      "epoch:10 step:9772 [D loss: 0.627972, acc.: 62.50%] [G loss: 0.843172]\n",
      "epoch:10 step:9773 [D loss: 0.644722, acc.: 67.19%] [G loss: 0.914936]\n",
      "epoch:10 step:9774 [D loss: 0.640884, acc.: 61.72%] [G loss: 0.981754]\n",
      "epoch:10 step:9775 [D loss: 0.634216, acc.: 60.16%] [G loss: 0.932803]\n",
      "epoch:10 step:9776 [D loss: 0.670860, acc.: 60.94%] [G loss: 0.916084]\n",
      "epoch:10 step:9777 [D loss: 0.643448, acc.: 64.84%] [G loss: 0.860280]\n",
      "epoch:10 step:9778 [D loss: 0.632401, acc.: 63.28%] [G loss: 0.993875]\n",
      "epoch:10 step:9779 [D loss: 0.631297, acc.: 61.72%] [G loss: 0.895140]\n",
      "epoch:10 step:9780 [D loss: 0.686350, acc.: 50.00%] [G loss: 0.855054]\n",
      "epoch:10 step:9781 [D loss: 0.705648, acc.: 50.00%] [G loss: 0.916563]\n",
      "epoch:10 step:9782 [D loss: 0.664421, acc.: 60.16%] [G loss: 0.838403]\n",
      "epoch:10 step:9783 [D loss: 0.738830, acc.: 50.00%] [G loss: 0.827811]\n",
      "epoch:10 step:9784 [D loss: 0.661929, acc.: 59.38%] [G loss: 0.879858]\n",
      "epoch:10 step:9785 [D loss: 0.674608, acc.: 60.94%] [G loss: 0.795036]\n",
      "epoch:10 step:9786 [D loss: 0.672974, acc.: 62.50%] [G loss: 0.796810]\n",
      "epoch:10 step:9787 [D loss: 0.630617, acc.: 67.19%] [G loss: 0.969741]\n",
      "epoch:10 step:9788 [D loss: 0.605247, acc.: 71.09%] [G loss: 0.838137]\n",
      "epoch:10 step:9789 [D loss: 0.697885, acc.: 54.69%] [G loss: 0.751749]\n",
      "epoch:10 step:9790 [D loss: 0.628096, acc.: 71.09%] [G loss: 0.827910]\n",
      "epoch:10 step:9791 [D loss: 0.676596, acc.: 60.16%] [G loss: 0.796038]\n",
      "epoch:10 step:9792 [D loss: 0.703328, acc.: 53.12%] [G loss: 0.827127]\n",
      "epoch:10 step:9793 [D loss: 0.695168, acc.: 55.47%] [G loss: 0.841735]\n",
      "epoch:10 step:9794 [D loss: 0.677651, acc.: 56.25%] [G loss: 0.931706]\n",
      "epoch:10 step:9795 [D loss: 0.639223, acc.: 62.50%] [G loss: 0.897304]\n",
      "epoch:10 step:9796 [D loss: 0.649344, acc.: 62.50%] [G loss: 0.886108]\n",
      "epoch:10 step:9797 [D loss: 0.733867, acc.: 50.78%] [G loss: 0.915826]\n",
      "epoch:10 step:9798 [D loss: 0.748180, acc.: 49.22%] [G loss: 0.850017]\n",
      "epoch:10 step:9799 [D loss: 0.676528, acc.: 55.47%] [G loss: 0.886348]\n",
      "epoch:10 step:9800 [D loss: 0.661776, acc.: 57.03%] [G loss: 0.762303]\n",
      "epoch:10 step:9801 [D loss: 0.622931, acc.: 68.75%] [G loss: 0.817042]\n",
      "epoch:10 step:9802 [D loss: 0.725483, acc.: 52.34%] [G loss: 0.816985]\n",
      "epoch:10 step:9803 [D loss: 0.675652, acc.: 58.59%] [G loss: 0.830802]\n",
      "epoch:10 step:9804 [D loss: 0.641703, acc.: 66.41%] [G loss: 0.828092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9805 [D loss: 0.699332, acc.: 55.47%] [G loss: 0.815697]\n",
      "epoch:10 step:9806 [D loss: 0.720333, acc.: 54.69%] [G loss: 0.801232]\n",
      "epoch:10 step:9807 [D loss: 0.779849, acc.: 49.22%] [G loss: 0.774888]\n",
      "epoch:10 step:9808 [D loss: 0.633808, acc.: 64.84%] [G loss: 0.903223]\n",
      "epoch:10 step:9809 [D loss: 0.637960, acc.: 65.62%] [G loss: 0.816762]\n",
      "epoch:10 step:9810 [D loss: 0.636210, acc.: 69.53%] [G loss: 0.808148]\n",
      "epoch:10 step:9811 [D loss: 0.725309, acc.: 47.66%] [G loss: 0.825879]\n",
      "epoch:10 step:9812 [D loss: 0.637320, acc.: 64.06%] [G loss: 0.803429]\n",
      "epoch:10 step:9813 [D loss: 0.695954, acc.: 48.44%] [G loss: 0.842701]\n",
      "epoch:10 step:9814 [D loss: 0.653899, acc.: 60.94%] [G loss: 0.805526]\n",
      "epoch:10 step:9815 [D loss: 0.697681, acc.: 53.91%] [G loss: 0.922887]\n",
      "epoch:10 step:9816 [D loss: 0.714167, acc.: 56.25%] [G loss: 0.855986]\n",
      "epoch:10 step:9817 [D loss: 0.705962, acc.: 50.00%] [G loss: 0.810851]\n",
      "epoch:10 step:9818 [D loss: 0.687701, acc.: 56.25%] [G loss: 0.785469]\n",
      "epoch:10 step:9819 [D loss: 0.702565, acc.: 57.03%] [G loss: 0.720138]\n",
      "epoch:10 step:9820 [D loss: 0.687770, acc.: 58.59%] [G loss: 0.839164]\n",
      "epoch:10 step:9821 [D loss: 0.664660, acc.: 57.03%] [G loss: 0.747953]\n",
      "epoch:10 step:9822 [D loss: 0.695253, acc.: 54.69%] [G loss: 0.804926]\n",
      "epoch:10 step:9823 [D loss: 0.679344, acc.: 57.03%] [G loss: 0.795005]\n",
      "epoch:10 step:9824 [D loss: 0.633194, acc.: 64.06%] [G loss: 0.850451]\n",
      "epoch:10 step:9825 [D loss: 0.674211, acc.: 60.94%] [G loss: 0.785423]\n",
      "epoch:10 step:9826 [D loss: 0.697699, acc.: 53.91%] [G loss: 0.782211]\n",
      "epoch:10 step:9827 [D loss: 0.681423, acc.: 57.81%] [G loss: 0.765084]\n",
      "epoch:10 step:9828 [D loss: 0.681292, acc.: 53.91%] [G loss: 0.826698]\n",
      "epoch:10 step:9829 [D loss: 0.632969, acc.: 67.19%] [G loss: 0.814461]\n",
      "epoch:10 step:9830 [D loss: 0.684391, acc.: 60.16%] [G loss: 0.886392]\n",
      "epoch:10 step:9831 [D loss: 0.661306, acc.: 60.16%] [G loss: 0.809850]\n",
      "epoch:10 step:9832 [D loss: 0.659616, acc.: 64.06%] [G loss: 0.825386]\n",
      "epoch:10 step:9833 [D loss: 0.630227, acc.: 63.28%] [G loss: 0.905320]\n",
      "epoch:10 step:9834 [D loss: 0.682448, acc.: 52.34%] [G loss: 0.868523]\n",
      "epoch:10 step:9835 [D loss: 0.651198, acc.: 60.94%] [G loss: 0.816545]\n",
      "epoch:10 step:9836 [D loss: 0.623370, acc.: 67.97%] [G loss: 0.822536]\n",
      "epoch:10 step:9837 [D loss: 0.682823, acc.: 53.91%] [G loss: 0.897000]\n",
      "epoch:10 step:9838 [D loss: 0.669180, acc.: 57.81%] [G loss: 0.911801]\n",
      "epoch:10 step:9839 [D loss: 0.611450, acc.: 73.44%] [G loss: 0.848004]\n",
      "epoch:10 step:9840 [D loss: 0.685030, acc.: 59.38%] [G loss: 0.806781]\n",
      "epoch:10 step:9841 [D loss: 0.663342, acc.: 62.50%] [G loss: 0.793238]\n",
      "epoch:10 step:9842 [D loss: 0.699625, acc.: 47.66%] [G loss: 0.768782]\n",
      "epoch:10 step:9843 [D loss: 0.618193, acc.: 67.19%] [G loss: 0.826805]\n",
      "epoch:10 step:9844 [D loss: 0.693121, acc.: 57.81%] [G loss: 0.907381]\n",
      "epoch:10 step:9845 [D loss: 0.617973, acc.: 70.31%] [G loss: 0.902124]\n",
      "epoch:10 step:9846 [D loss: 0.674770, acc.: 57.03%] [G loss: 0.970138]\n",
      "epoch:10 step:9847 [D loss: 0.722075, acc.: 46.09%] [G loss: 0.971306]\n",
      "epoch:10 step:9848 [D loss: 0.696170, acc.: 55.47%] [G loss: 0.918075]\n",
      "epoch:10 step:9849 [D loss: 0.708807, acc.: 53.12%] [G loss: 0.890950]\n",
      "epoch:10 step:9850 [D loss: 0.635781, acc.: 59.38%] [G loss: 0.834467]\n",
      "epoch:10 step:9851 [D loss: 0.786357, acc.: 35.16%] [G loss: 0.828648]\n",
      "epoch:10 step:9852 [D loss: 0.659096, acc.: 58.59%] [G loss: 0.914639]\n",
      "epoch:10 step:9853 [D loss: 0.668469, acc.: 61.72%] [G loss: 0.880579]\n",
      "epoch:10 step:9854 [D loss: 0.693788, acc.: 59.38%] [G loss: 0.899412]\n",
      "epoch:10 step:9855 [D loss: 0.697857, acc.: 50.00%] [G loss: 0.909418]\n",
      "epoch:10 step:9856 [D loss: 0.631458, acc.: 67.97%] [G loss: 0.946263]\n",
      "epoch:10 step:9857 [D loss: 0.654747, acc.: 58.59%] [G loss: 0.768916]\n",
      "epoch:10 step:9858 [D loss: 0.642311, acc.: 60.94%] [G loss: 0.845314]\n",
      "epoch:10 step:9859 [D loss: 0.681372, acc.: 59.38%] [G loss: 0.888451]\n",
      "epoch:10 step:9860 [D loss: 0.654946, acc.: 57.81%] [G loss: 0.824988]\n",
      "epoch:10 step:9861 [D loss: 0.710409, acc.: 50.00%] [G loss: 0.862505]\n",
      "epoch:10 step:9862 [D loss: 0.645853, acc.: 66.41%] [G loss: 0.878424]\n",
      "epoch:10 step:9863 [D loss: 0.669335, acc.: 57.03%] [G loss: 0.876157]\n",
      "epoch:10 step:9864 [D loss: 0.663045, acc.: 59.38%] [G loss: 0.950626]\n",
      "epoch:10 step:9865 [D loss: 0.696663, acc.: 56.25%] [G loss: 0.910121]\n",
      "epoch:10 step:9866 [D loss: 0.706392, acc.: 53.91%] [G loss: 0.840025]\n",
      "epoch:10 step:9867 [D loss: 0.671770, acc.: 57.03%] [G loss: 0.865097]\n",
      "epoch:10 step:9868 [D loss: 0.637416, acc.: 70.31%] [G loss: 0.859497]\n",
      "epoch:10 step:9869 [D loss: 0.680001, acc.: 56.25%] [G loss: 0.894477]\n",
      "epoch:10 step:9870 [D loss: 0.696759, acc.: 56.25%] [G loss: 0.866922]\n",
      "epoch:10 step:9871 [D loss: 0.695683, acc.: 49.22%] [G loss: 0.787985]\n",
      "epoch:10 step:9872 [D loss: 0.651008, acc.: 59.38%] [G loss: 0.865047]\n",
      "epoch:10 step:9873 [D loss: 0.723503, acc.: 47.66%] [G loss: 0.881329]\n",
      "epoch:10 step:9874 [D loss: 0.688421, acc.: 52.34%] [G loss: 0.949699]\n",
      "epoch:10 step:9875 [D loss: 0.670086, acc.: 57.03%] [G loss: 0.807176]\n",
      "epoch:10 step:9876 [D loss: 0.672644, acc.: 60.16%] [G loss: 0.873099]\n",
      "epoch:10 step:9877 [D loss: 0.676150, acc.: 55.47%] [G loss: 0.914057]\n",
      "epoch:10 step:9878 [D loss: 0.663250, acc.: 58.59%] [G loss: 0.810277]\n",
      "epoch:10 step:9879 [D loss: 0.677430, acc.: 57.03%] [G loss: 0.840939]\n",
      "epoch:10 step:9880 [D loss: 0.676343, acc.: 64.06%] [G loss: 0.825090]\n",
      "epoch:10 step:9881 [D loss: 0.700078, acc.: 48.44%] [G loss: 0.806428]\n",
      "epoch:10 step:9882 [D loss: 0.704259, acc.: 50.00%] [G loss: 0.845992]\n",
      "epoch:10 step:9883 [D loss: 0.677368, acc.: 55.47%] [G loss: 0.815743]\n",
      "epoch:10 step:9884 [D loss: 0.679217, acc.: 60.16%] [G loss: 0.813458]\n",
      "epoch:10 step:9885 [D loss: 0.707267, acc.: 49.22%] [G loss: 0.766378]\n",
      "epoch:10 step:9886 [D loss: 0.614170, acc.: 66.41%] [G loss: 0.799690]\n",
      "epoch:10 step:9887 [D loss: 0.631053, acc.: 64.06%] [G loss: 0.871135]\n",
      "epoch:10 step:9888 [D loss: 0.726714, acc.: 47.66%] [G loss: 0.841740]\n",
      "epoch:10 step:9889 [D loss: 0.664082, acc.: 57.81%] [G loss: 0.897831]\n",
      "epoch:10 step:9890 [D loss: 0.646628, acc.: 57.81%] [G loss: 0.869356]\n",
      "epoch:10 step:9891 [D loss: 0.696525, acc.: 49.22%] [G loss: 0.798471]\n",
      "epoch:10 step:9892 [D loss: 0.682280, acc.: 56.25%] [G loss: 0.812375]\n",
      "epoch:10 step:9893 [D loss: 0.656789, acc.: 59.38%] [G loss: 0.877973]\n",
      "epoch:10 step:9894 [D loss: 0.646149, acc.: 61.72%] [G loss: 0.788735]\n",
      "epoch:10 step:9895 [D loss: 0.700795, acc.: 51.56%] [G loss: 0.780553]\n",
      "epoch:10 step:9896 [D loss: 0.682679, acc.: 53.91%] [G loss: 0.801273]\n",
      "epoch:10 step:9897 [D loss: 0.642157, acc.: 62.50%] [G loss: 0.884224]\n",
      "epoch:10 step:9898 [D loss: 0.694084, acc.: 54.69%] [G loss: 0.736424]\n",
      "epoch:10 step:9899 [D loss: 0.638622, acc.: 57.03%] [G loss: 0.880567]\n",
      "epoch:10 step:9900 [D loss: 0.685266, acc.: 59.38%] [G loss: 0.795264]\n",
      "epoch:10 step:9901 [D loss: 0.660488, acc.: 56.25%] [G loss: 0.854744]\n",
      "epoch:10 step:9902 [D loss: 0.697731, acc.: 50.78%] [G loss: 0.831993]\n",
      "epoch:10 step:9903 [D loss: 0.704996, acc.: 49.22%] [G loss: 0.765090]\n",
      "epoch:10 step:9904 [D loss: 0.670102, acc.: 58.59%] [G loss: 0.895256]\n",
      "epoch:10 step:9905 [D loss: 0.662608, acc.: 58.59%] [G loss: 0.846703]\n",
      "epoch:10 step:9906 [D loss: 0.637897, acc.: 64.06%] [G loss: 0.836510]\n",
      "epoch:10 step:9907 [D loss: 0.705311, acc.: 50.78%] [G loss: 0.744919]\n",
      "epoch:10 step:9908 [D loss: 0.678305, acc.: 60.16%] [G loss: 0.786777]\n",
      "epoch:10 step:9909 [D loss: 0.711367, acc.: 57.81%] [G loss: 0.853008]\n",
      "epoch:10 step:9910 [D loss: 0.682190, acc.: 57.81%] [G loss: 0.778394]\n",
      "epoch:10 step:9911 [D loss: 0.741274, acc.: 40.62%] [G loss: 0.870213]\n",
      "epoch:10 step:9912 [D loss: 0.704753, acc.: 50.00%] [G loss: 0.810368]\n",
      "epoch:10 step:9913 [D loss: 0.683954, acc.: 53.91%] [G loss: 0.831070]\n",
      "epoch:10 step:9914 [D loss: 0.639870, acc.: 66.41%] [G loss: 0.795684]\n",
      "epoch:10 step:9915 [D loss: 0.696308, acc.: 53.91%] [G loss: 0.823910]\n",
      "epoch:10 step:9916 [D loss: 0.682503, acc.: 54.69%] [G loss: 0.850840]\n",
      "epoch:10 step:9917 [D loss: 0.661556, acc.: 64.84%] [G loss: 0.825900]\n",
      "epoch:10 step:9918 [D loss: 0.670267, acc.: 53.12%] [G loss: 0.854925]\n",
      "epoch:10 step:9919 [D loss: 0.720890, acc.: 45.31%] [G loss: 0.851284]\n",
      "epoch:10 step:9920 [D loss: 0.697449, acc.: 53.91%] [G loss: 0.938082]\n",
      "epoch:10 step:9921 [D loss: 0.701933, acc.: 46.88%] [G loss: 0.838443]\n",
      "epoch:10 step:9922 [D loss: 0.666609, acc.: 57.03%] [G loss: 0.850136]\n",
      "epoch:10 step:9923 [D loss: 0.685011, acc.: 53.12%] [G loss: 0.921854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9924 [D loss: 0.678091, acc.: 54.69%] [G loss: 0.855813]\n",
      "epoch:10 step:9925 [D loss: 0.653202, acc.: 57.03%] [G loss: 0.827587]\n",
      "epoch:10 step:9926 [D loss: 0.680438, acc.: 57.03%] [G loss: 0.821214]\n",
      "epoch:10 step:9927 [D loss: 0.690106, acc.: 54.69%] [G loss: 0.784303]\n",
      "epoch:10 step:9928 [D loss: 0.694580, acc.: 53.12%] [G loss: 0.752190]\n",
      "epoch:10 step:9929 [D loss: 0.670290, acc.: 59.38%] [G loss: 0.746542]\n",
      "epoch:10 step:9930 [D loss: 0.701570, acc.: 51.56%] [G loss: 0.800015]\n",
      "epoch:10 step:9931 [D loss: 0.634052, acc.: 71.09%] [G loss: 0.784331]\n",
      "epoch:10 step:9932 [D loss: 0.690004, acc.: 56.25%] [G loss: 0.845754]\n",
      "epoch:10 step:9933 [D loss: 0.695992, acc.: 47.66%] [G loss: 0.763338]\n",
      "epoch:10 step:9934 [D loss: 0.706055, acc.: 46.88%] [G loss: 0.817536]\n",
      "epoch:10 step:9935 [D loss: 0.691836, acc.: 46.09%] [G loss: 0.843161]\n",
      "epoch:10 step:9936 [D loss: 0.669227, acc.: 58.59%] [G loss: 0.797385]\n",
      "epoch:10 step:9937 [D loss: 0.707785, acc.: 55.47%] [G loss: 0.859621]\n",
      "epoch:10 step:9938 [D loss: 0.675500, acc.: 59.38%] [G loss: 0.843825]\n",
      "epoch:10 step:9939 [D loss: 0.671895, acc.: 57.03%] [G loss: 0.839272]\n",
      "epoch:10 step:9940 [D loss: 0.677752, acc.: 62.50%] [G loss: 0.836443]\n",
      "epoch:10 step:9941 [D loss: 0.665973, acc.: 56.25%] [G loss: 0.781097]\n",
      "epoch:10 step:9942 [D loss: 0.694669, acc.: 50.00%] [G loss: 0.764678]\n",
      "epoch:10 step:9943 [D loss: 0.660827, acc.: 60.16%] [G loss: 0.822647]\n",
      "epoch:10 step:9944 [D loss: 0.655571, acc.: 60.94%] [G loss: 0.787122]\n",
      "epoch:10 step:9945 [D loss: 0.669893, acc.: 57.03%] [G loss: 0.839254]\n",
      "epoch:10 step:9946 [D loss: 0.647345, acc.: 67.19%] [G loss: 0.808446]\n",
      "epoch:10 step:9947 [D loss: 0.691485, acc.: 53.12%] [G loss: 0.762342]\n",
      "epoch:10 step:9948 [D loss: 0.670075, acc.: 57.03%] [G loss: 0.748964]\n",
      "epoch:10 step:9949 [D loss: 0.673826, acc.: 51.56%] [G loss: 0.746431]\n",
      "epoch:10 step:9950 [D loss: 0.675546, acc.: 51.56%] [G loss: 0.828251]\n",
      "epoch:10 step:9951 [D loss: 0.651675, acc.: 65.62%] [G loss: 0.846485]\n",
      "epoch:10 step:9952 [D loss: 0.674394, acc.: 58.59%] [G loss: 0.855235]\n",
      "epoch:10 step:9953 [D loss: 0.741243, acc.: 40.62%] [G loss: 0.884378]\n",
      "epoch:10 step:9954 [D loss: 0.628291, acc.: 64.06%] [G loss: 0.837495]\n",
      "epoch:10 step:9955 [D loss: 0.677064, acc.: 58.59%] [G loss: 0.887822]\n",
      "epoch:10 step:9956 [D loss: 0.668899, acc.: 60.16%] [G loss: 0.893652]\n",
      "epoch:10 step:9957 [D loss: 0.669609, acc.: 58.59%] [G loss: 0.888334]\n",
      "epoch:10 step:9958 [D loss: 0.672089, acc.: 58.59%] [G loss: 1.061524]\n",
      "epoch:10 step:9959 [D loss: 0.692168, acc.: 52.34%] [G loss: 0.919871]\n",
      "epoch:10 step:9960 [D loss: 0.688094, acc.: 57.03%] [G loss: 0.924659]\n",
      "epoch:10 step:9961 [D loss: 0.664848, acc.: 59.38%] [G loss: 0.887951]\n",
      "epoch:10 step:9962 [D loss: 0.655293, acc.: 57.81%] [G loss: 0.878545]\n",
      "epoch:10 step:9963 [D loss: 0.742345, acc.: 43.75%] [G loss: 0.791065]\n",
      "epoch:10 step:9964 [D loss: 0.685564, acc.: 55.47%] [G loss: 0.851931]\n",
      "epoch:10 step:9965 [D loss: 0.643327, acc.: 58.59%] [G loss: 0.900408]\n",
      "epoch:10 step:9966 [D loss: 0.639543, acc.: 63.28%] [G loss: 0.813090]\n",
      "epoch:10 step:9967 [D loss: 0.675413, acc.: 53.91%] [G loss: 0.886125]\n",
      "epoch:10 step:9968 [D loss: 0.628580, acc.: 67.19%] [G loss: 0.911879]\n",
      "epoch:10 step:9969 [D loss: 0.648125, acc.: 61.72%] [G loss: 0.914535]\n",
      "epoch:10 step:9970 [D loss: 0.669864, acc.: 57.03%] [G loss: 0.796739]\n",
      "epoch:10 step:9971 [D loss: 0.631605, acc.: 65.62%] [G loss: 0.967629]\n",
      "epoch:10 step:9972 [D loss: 0.640261, acc.: 64.06%] [G loss: 0.833736]\n",
      "epoch:10 step:9973 [D loss: 0.707614, acc.: 51.56%] [G loss: 0.800589]\n",
      "epoch:10 step:9974 [D loss: 0.661853, acc.: 60.16%] [G loss: 0.751903]\n",
      "epoch:10 step:9975 [D loss: 0.677208, acc.: 60.16%] [G loss: 0.802735]\n",
      "epoch:10 step:9976 [D loss: 0.726249, acc.: 52.34%] [G loss: 0.828466]\n",
      "epoch:10 step:9977 [D loss: 0.670907, acc.: 57.03%] [G loss: 0.798637]\n",
      "epoch:10 step:9978 [D loss: 0.710352, acc.: 42.97%] [G loss: 0.833361]\n",
      "epoch:10 step:9979 [D loss: 0.632339, acc.: 67.97%] [G loss: 0.796750]\n",
      "epoch:10 step:9980 [D loss: 0.691223, acc.: 50.00%] [G loss: 0.821164]\n",
      "epoch:10 step:9981 [D loss: 0.662813, acc.: 60.16%] [G loss: 0.750735]\n",
      "epoch:10 step:9982 [D loss: 0.650902, acc.: 65.62%] [G loss: 0.822551]\n",
      "epoch:10 step:9983 [D loss: 0.687785, acc.: 47.66%] [G loss: 0.812924]\n",
      "epoch:10 step:9984 [D loss: 0.693049, acc.: 54.69%] [G loss: 0.814949]\n",
      "epoch:10 step:9985 [D loss: 0.702444, acc.: 53.91%] [G loss: 0.772942]\n",
      "epoch:10 step:9986 [D loss: 0.735068, acc.: 42.19%] [G loss: 0.872244]\n",
      "epoch:10 step:9987 [D loss: 0.692471, acc.: 50.78%] [G loss: 0.876252]\n",
      "epoch:10 step:9988 [D loss: 0.674997, acc.: 53.91%] [G loss: 0.809917]\n",
      "epoch:10 step:9989 [D loss: 0.677665, acc.: 51.56%] [G loss: 0.923524]\n",
      "epoch:10 step:9990 [D loss: 0.684631, acc.: 53.12%] [G loss: 0.894597]\n",
      "epoch:10 step:9991 [D loss: 0.679260, acc.: 56.25%] [G loss: 0.856206]\n",
      "epoch:10 step:9992 [D loss: 0.680817, acc.: 53.91%] [G loss: 0.872036]\n",
      "epoch:10 step:9993 [D loss: 0.672158, acc.: 57.81%] [G loss: 0.843166]\n",
      "epoch:10 step:9994 [D loss: 0.646284, acc.: 62.50%] [G loss: 0.837139]\n",
      "epoch:10 step:9995 [D loss: 0.709500, acc.: 52.34%] [G loss: 0.846846]\n",
      "epoch:10 step:9996 [D loss: 0.705219, acc.: 49.22%] [G loss: 0.886377]\n",
      "epoch:10 step:9997 [D loss: 0.653079, acc.: 58.59%] [G loss: 0.884424]\n",
      "epoch:10 step:9998 [D loss: 0.655550, acc.: 60.94%] [G loss: 0.886516]\n",
      "epoch:10 step:9999 [D loss: 0.674357, acc.: 56.25%] [G loss: 0.951348]\n",
      "epoch:10 step:10000 [D loss: 0.662009, acc.: 61.72%] [G loss: 0.885172]\n",
      "epoch:10 step:10001 [D loss: 0.673166, acc.: 53.91%] [G loss: 0.870689]\n",
      "epoch:10 step:10002 [D loss: 0.657649, acc.: 55.47%] [G loss: 0.902437]\n",
      "epoch:10 step:10003 [D loss: 0.748546, acc.: 46.88%] [G loss: 0.851034]\n",
      "epoch:10 step:10004 [D loss: 0.661412, acc.: 59.38%] [G loss: 0.880048]\n",
      "epoch:10 step:10005 [D loss: 0.631410, acc.: 58.59%] [G loss: 0.792658]\n",
      "epoch:10 step:10006 [D loss: 0.689502, acc.: 60.94%] [G loss: 0.830186]\n",
      "epoch:10 step:10007 [D loss: 0.668980, acc.: 57.03%] [G loss: 0.898148]\n",
      "epoch:10 step:10008 [D loss: 0.688197, acc.: 55.47%] [G loss: 0.869380]\n",
      "epoch:10 step:10009 [D loss: 0.662879, acc.: 61.72%] [G loss: 0.829614]\n",
      "epoch:10 step:10010 [D loss: 0.673983, acc.: 60.94%] [G loss: 0.943834]\n",
      "epoch:10 step:10011 [D loss: 0.625815, acc.: 66.41%] [G loss: 0.956725]\n",
      "epoch:10 step:10012 [D loss: 0.714805, acc.: 50.00%] [G loss: 0.817752]\n",
      "epoch:10 step:10013 [D loss: 0.662100, acc.: 60.94%] [G loss: 0.837324]\n",
      "epoch:10 step:10014 [D loss: 0.668420, acc.: 57.81%] [G loss: 0.771050]\n",
      "epoch:10 step:10015 [D loss: 0.711429, acc.: 42.19%] [G loss: 0.785164]\n",
      "epoch:10 step:10016 [D loss: 0.797029, acc.: 55.47%] [G loss: 0.917869]\n",
      "epoch:10 step:10017 [D loss: 0.642950, acc.: 63.28%] [G loss: 0.945720]\n",
      "epoch:10 step:10018 [D loss: 0.662794, acc.: 57.81%] [G loss: 0.973979]\n",
      "epoch:10 step:10019 [D loss: 0.674500, acc.: 56.25%] [G loss: 0.947654]\n",
      "epoch:10 step:10020 [D loss: 0.647674, acc.: 65.62%] [G loss: 0.872290]\n",
      "epoch:10 step:10021 [D loss: 0.720174, acc.: 53.91%] [G loss: 0.896083]\n",
      "epoch:10 step:10022 [D loss: 0.693928, acc.: 53.12%] [G loss: 0.812419]\n",
      "epoch:10 step:10023 [D loss: 0.609316, acc.: 69.53%] [G loss: 0.799510]\n",
      "epoch:10 step:10024 [D loss: 0.641499, acc.: 61.72%] [G loss: 0.846937]\n",
      "epoch:10 step:10025 [D loss: 0.698406, acc.: 53.12%] [G loss: 0.881765]\n",
      "epoch:10 step:10026 [D loss: 0.654302, acc.: 67.97%] [G loss: 0.843893]\n",
      "epoch:10 step:10027 [D loss: 0.626250, acc.: 65.62%] [G loss: 0.785231]\n",
      "epoch:10 step:10028 [D loss: 0.725872, acc.: 45.31%] [G loss: 0.811823]\n",
      "epoch:10 step:10029 [D loss: 0.648046, acc.: 57.81%] [G loss: 0.871213]\n",
      "epoch:10 step:10030 [D loss: 0.652012, acc.: 57.81%] [G loss: 0.780824]\n",
      "epoch:10 step:10031 [D loss: 0.652689, acc.: 57.03%] [G loss: 0.781878]\n",
      "epoch:10 step:10032 [D loss: 0.668548, acc.: 60.94%] [G loss: 0.792722]\n",
      "epoch:10 step:10033 [D loss: 0.695020, acc.: 55.47%] [G loss: 0.833871]\n",
      "epoch:10 step:10034 [D loss: 0.666233, acc.: 61.72%] [G loss: 0.800974]\n",
      "epoch:10 step:10035 [D loss: 0.692463, acc.: 51.56%] [G loss: 0.802094]\n",
      "epoch:10 step:10036 [D loss: 0.714955, acc.: 48.44%] [G loss: 0.888597]\n",
      "epoch:10 step:10037 [D loss: 0.668919, acc.: 57.03%] [G loss: 0.833676]\n",
      "epoch:10 step:10038 [D loss: 0.615431, acc.: 69.53%] [G loss: 0.866300]\n",
      "epoch:10 step:10039 [D loss: 0.678131, acc.: 54.69%] [G loss: 0.838513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10040 [D loss: 0.628377, acc.: 67.97%] [G loss: 0.841199]\n",
      "epoch:10 step:10041 [D loss: 0.701417, acc.: 49.22%] [G loss: 0.780178]\n",
      "epoch:10 step:10042 [D loss: 0.716928, acc.: 52.34%] [G loss: 0.793814]\n",
      "epoch:10 step:10043 [D loss: 0.699152, acc.: 53.91%] [G loss: 0.845405]\n",
      "epoch:10 step:10044 [D loss: 0.648630, acc.: 61.72%] [G loss: 0.804321]\n",
      "epoch:10 step:10045 [D loss: 0.694527, acc.: 54.69%] [G loss: 0.936983]\n",
      "epoch:10 step:10046 [D loss: 0.688079, acc.: 50.00%] [G loss: 0.886164]\n",
      "epoch:10 step:10047 [D loss: 0.689588, acc.: 50.00%] [G loss: 0.836308]\n",
      "epoch:10 step:10048 [D loss: 0.669040, acc.: 53.91%] [G loss: 0.828629]\n",
      "epoch:10 step:10049 [D loss: 0.676454, acc.: 60.94%] [G loss: 0.859037]\n",
      "epoch:10 step:10050 [D loss: 0.675566, acc.: 55.47%] [G loss: 0.847319]\n",
      "epoch:10 step:10051 [D loss: 0.688189, acc.: 59.38%] [G loss: 0.830114]\n",
      "epoch:10 step:10052 [D loss: 0.673796, acc.: 57.81%] [G loss: 0.820442]\n",
      "epoch:10 step:10053 [D loss: 0.663819, acc.: 55.47%] [G loss: 0.839029]\n",
      "epoch:10 step:10054 [D loss: 0.691058, acc.: 50.78%] [G loss: 0.903208]\n",
      "epoch:10 step:10055 [D loss: 0.694926, acc.: 48.44%] [G loss: 0.770156]\n",
      "epoch:10 step:10056 [D loss: 0.677694, acc.: 57.03%] [G loss: 0.786197]\n",
      "epoch:10 step:10057 [D loss: 0.709273, acc.: 47.66%] [G loss: 0.806467]\n",
      "epoch:10 step:10058 [D loss: 0.669922, acc.: 57.03%] [G loss: 0.764936]\n",
      "epoch:10 step:10059 [D loss: 0.714668, acc.: 46.09%] [G loss: 0.808672]\n",
      "epoch:10 step:10060 [D loss: 0.699237, acc.: 53.12%] [G loss: 0.812055]\n",
      "epoch:10 step:10061 [D loss: 0.650190, acc.: 64.06%] [G loss: 0.768386]\n",
      "epoch:10 step:10062 [D loss: 0.640402, acc.: 60.16%] [G loss: 0.807545]\n",
      "epoch:10 step:10063 [D loss: 0.683169, acc.: 51.56%] [G loss: 0.878951]\n",
      "epoch:10 step:10064 [D loss: 0.695574, acc.: 50.00%] [G loss: 0.823292]\n",
      "epoch:10 step:10065 [D loss: 0.697976, acc.: 53.91%] [G loss: 0.858936]\n",
      "epoch:10 step:10066 [D loss: 0.662534, acc.: 53.91%] [G loss: 0.842961]\n",
      "epoch:10 step:10067 [D loss: 0.638325, acc.: 60.94%] [G loss: 0.785213]\n",
      "epoch:10 step:10068 [D loss: 0.664625, acc.: 56.25%] [G loss: 0.788086]\n",
      "epoch:10 step:10069 [D loss: 0.677001, acc.: 55.47%] [G loss: 0.829599]\n",
      "epoch:10 step:10070 [D loss: 0.660271, acc.: 57.03%] [G loss: 0.829935]\n",
      "epoch:10 step:10071 [D loss: 0.691819, acc.: 50.78%] [G loss: 0.832692]\n",
      "epoch:10 step:10072 [D loss: 0.768219, acc.: 47.66%] [G loss: 0.719404]\n",
      "epoch:10 step:10073 [D loss: 0.678665, acc.: 53.91%] [G loss: 0.792254]\n",
      "epoch:10 step:10074 [D loss: 0.686044, acc.: 57.81%] [G loss: 0.810036]\n",
      "epoch:10 step:10075 [D loss: 0.703571, acc.: 53.12%] [G loss: 0.840618]\n",
      "epoch:10 step:10076 [D loss: 0.626429, acc.: 68.75%] [G loss: 0.770739]\n",
      "epoch:10 step:10077 [D loss: 0.627822, acc.: 60.94%] [G loss: 0.854183]\n",
      "epoch:10 step:10078 [D loss: 0.686886, acc.: 56.25%] [G loss: 0.790799]\n",
      "epoch:10 step:10079 [D loss: 0.671093, acc.: 57.03%] [G loss: 0.764509]\n",
      "epoch:10 step:10080 [D loss: 0.732863, acc.: 49.22%] [G loss: 0.906045]\n",
      "epoch:10 step:10081 [D loss: 0.643324, acc.: 57.81%] [G loss: 0.877156]\n",
      "epoch:10 step:10082 [D loss: 0.674097, acc.: 58.59%] [G loss: 0.763043]\n",
      "epoch:10 step:10083 [D loss: 0.703069, acc.: 52.34%] [G loss: 0.782672]\n",
      "epoch:10 step:10084 [D loss: 0.656143, acc.: 60.16%] [G loss: 0.790035]\n",
      "epoch:10 step:10085 [D loss: 0.702993, acc.: 45.31%] [G loss: 0.779587]\n",
      "epoch:10 step:10086 [D loss: 0.683979, acc.: 54.69%] [G loss: 0.754903]\n",
      "epoch:10 step:10087 [D loss: 0.650120, acc.: 64.84%] [G loss: 0.741301]\n",
      "epoch:10 step:10088 [D loss: 0.649764, acc.: 61.72%] [G loss: 0.839566]\n",
      "epoch:10 step:10089 [D loss: 0.652291, acc.: 60.94%] [G loss: 0.823186]\n",
      "epoch:10 step:10090 [D loss: 0.664945, acc.: 64.84%] [G loss: 0.795093]\n",
      "epoch:10 step:10091 [D loss: 0.702780, acc.: 56.25%] [G loss: 0.822098]\n",
      "epoch:10 step:10092 [D loss: 0.692619, acc.: 59.38%] [G loss: 0.840503]\n",
      "epoch:10 step:10093 [D loss: 0.666948, acc.: 59.38%] [G loss: 0.853428]\n",
      "epoch:10 step:10094 [D loss: 0.651937, acc.: 63.28%] [G loss: 0.821443]\n",
      "epoch:10 step:10095 [D loss: 0.688793, acc.: 53.91%] [G loss: 0.872346]\n",
      "epoch:10 step:10096 [D loss: 0.694282, acc.: 54.69%] [G loss: 0.818554]\n",
      "epoch:10 step:10097 [D loss: 0.662925, acc.: 58.59%] [G loss: 0.883809]\n",
      "epoch:10 step:10098 [D loss: 0.668706, acc.: 60.16%] [G loss: 0.759847]\n",
      "epoch:10 step:10099 [D loss: 0.644107, acc.: 71.09%] [G loss: 0.783180]\n",
      "epoch:10 step:10100 [D loss: 0.658641, acc.: 63.28%] [G loss: 0.756102]\n",
      "epoch:10 step:10101 [D loss: 0.691165, acc.: 55.47%] [G loss: 0.850635]\n",
      "epoch:10 step:10102 [D loss: 0.626115, acc.: 67.97%] [G loss: 0.864858]\n",
      "epoch:10 step:10103 [D loss: 0.632871, acc.: 63.28%] [G loss: 0.874976]\n",
      "epoch:10 step:10104 [D loss: 0.668614, acc.: 53.91%] [G loss: 0.814016]\n",
      "epoch:10 step:10105 [D loss: 0.651030, acc.: 63.28%] [G loss: 0.924110]\n",
      "epoch:10 step:10106 [D loss: 0.646367, acc.: 60.94%] [G loss: 0.867093]\n",
      "epoch:10 step:10107 [D loss: 0.613755, acc.: 72.66%] [G loss: 0.842220]\n",
      "epoch:10 step:10108 [D loss: 0.644255, acc.: 57.03%] [G loss: 0.822695]\n",
      "epoch:10 step:10109 [D loss: 0.675091, acc.: 56.25%] [G loss: 0.839638]\n",
      "epoch:10 step:10110 [D loss: 0.698610, acc.: 51.56%] [G loss: 0.830311]\n",
      "epoch:10 step:10111 [D loss: 0.682849, acc.: 55.47%] [G loss: 0.855146]\n",
      "epoch:10 step:10112 [D loss: 0.705002, acc.: 61.72%] [G loss: 0.903544]\n",
      "epoch:10 step:10113 [D loss: 0.688126, acc.: 56.25%] [G loss: 0.963318]\n",
      "epoch:10 step:10114 [D loss: 0.661814, acc.: 52.34%] [G loss: 0.853773]\n",
      "epoch:10 step:10115 [D loss: 0.662555, acc.: 63.28%] [G loss: 0.831209]\n",
      "epoch:10 step:10116 [D loss: 0.689444, acc.: 52.34%] [G loss: 0.782167]\n",
      "epoch:10 step:10117 [D loss: 0.716548, acc.: 47.66%] [G loss: 0.848400]\n",
      "epoch:10 step:10118 [D loss: 0.658167, acc.: 60.94%] [G loss: 0.895109]\n",
      "epoch:10 step:10119 [D loss: 0.677323, acc.: 57.81%] [G loss: 0.903720]\n",
      "epoch:10 step:10120 [D loss: 0.639464, acc.: 70.31%] [G loss: 0.768072]\n",
      "epoch:10 step:10121 [D loss: 0.686206, acc.: 53.91%] [G loss: 0.851264]\n",
      "epoch:10 step:10122 [D loss: 0.628278, acc.: 64.06%] [G loss: 0.848931]\n",
      "epoch:10 step:10123 [D loss: 0.655701, acc.: 60.94%] [G loss: 0.905333]\n",
      "epoch:10 step:10124 [D loss: 0.653968, acc.: 55.47%] [G loss: 0.859006]\n",
      "epoch:10 step:10125 [D loss: 0.690449, acc.: 53.12%] [G loss: 0.828058]\n",
      "epoch:10 step:10126 [D loss: 0.699841, acc.: 56.25%] [G loss: 0.858059]\n",
      "epoch:10 step:10127 [D loss: 0.726708, acc.: 55.47%] [G loss: 0.859625]\n",
      "epoch:10 step:10128 [D loss: 0.691158, acc.: 57.03%] [G loss: 0.808787]\n",
      "epoch:10 step:10129 [D loss: 0.701714, acc.: 53.91%] [G loss: 0.745887]\n",
      "epoch:10 step:10130 [D loss: 0.645175, acc.: 64.84%] [G loss: 0.799331]\n",
      "epoch:10 step:10131 [D loss: 0.676890, acc.: 55.47%] [G loss: 0.885799]\n",
      "epoch:10 step:10132 [D loss: 0.657431, acc.: 66.41%] [G loss: 0.899432]\n",
      "epoch:10 step:10133 [D loss: 0.675326, acc.: 60.94%] [G loss: 0.790545]\n",
      "epoch:10 step:10134 [D loss: 0.639030, acc.: 68.75%] [G loss: 0.855894]\n",
      "epoch:10 step:10135 [D loss: 0.700000, acc.: 52.34%] [G loss: 0.867307]\n",
      "epoch:10 step:10136 [D loss: 0.742503, acc.: 41.41%] [G loss: 0.797896]\n",
      "epoch:10 step:10137 [D loss: 0.684028, acc.: 57.81%] [G loss: 0.854715]\n",
      "epoch:10 step:10138 [D loss: 0.651213, acc.: 57.03%] [G loss: 0.843342]\n",
      "epoch:10 step:10139 [D loss: 0.648211, acc.: 66.41%] [G loss: 0.851697]\n",
      "epoch:10 step:10140 [D loss: 0.649835, acc.: 63.28%] [G loss: 0.853773]\n",
      "epoch:10 step:10141 [D loss: 0.683618, acc.: 43.75%] [G loss: 0.912576]\n",
      "epoch:10 step:10142 [D loss: 0.670682, acc.: 60.16%] [G loss: 0.831067]\n",
      "epoch:10 step:10143 [D loss: 0.643128, acc.: 67.97%] [G loss: 0.792037]\n",
      "epoch:10 step:10144 [D loss: 0.635222, acc.: 64.84%] [G loss: 0.865925]\n",
      "epoch:10 step:10145 [D loss: 0.658563, acc.: 58.59%] [G loss: 0.904114]\n",
      "epoch:10 step:10146 [D loss: 0.656019, acc.: 64.06%] [G loss: 0.842034]\n",
      "epoch:10 step:10147 [D loss: 0.705424, acc.: 50.00%] [G loss: 0.960569]\n",
      "epoch:10 step:10148 [D loss: 0.640912, acc.: 64.84%] [G loss: 0.784483]\n",
      "epoch:10 step:10149 [D loss: 0.713629, acc.: 50.78%] [G loss: 0.859477]\n",
      "epoch:10 step:10150 [D loss: 0.677647, acc.: 58.59%] [G loss: 0.861461]\n",
      "epoch:10 step:10151 [D loss: 0.654401, acc.: 61.72%] [G loss: 0.823443]\n",
      "epoch:10 step:10152 [D loss: 0.684538, acc.: 56.25%] [G loss: 0.834313]\n",
      "epoch:10 step:10153 [D loss: 0.692026, acc.: 48.44%] [G loss: 0.780169]\n",
      "epoch:10 step:10154 [D loss: 0.636457, acc.: 61.72%] [G loss: 0.760851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10155 [D loss: 0.729696, acc.: 41.41%] [G loss: 0.795998]\n",
      "epoch:10 step:10156 [D loss: 0.628861, acc.: 64.06%] [G loss: 0.838760]\n",
      "epoch:10 step:10157 [D loss: 0.667736, acc.: 54.69%] [G loss: 0.827319]\n",
      "epoch:10 step:10158 [D loss: 0.648436, acc.: 57.81%] [G loss: 0.835133]\n",
      "epoch:10 step:10159 [D loss: 0.649763, acc.: 63.28%] [G loss: 0.785794]\n",
      "epoch:10 step:10160 [D loss: 0.682072, acc.: 53.91%] [G loss: 0.779963]\n",
      "epoch:10 step:10161 [D loss: 0.680872, acc.: 53.91%] [G loss: 0.803740]\n",
      "epoch:10 step:10162 [D loss: 0.639980, acc.: 60.94%] [G loss: 0.850630]\n",
      "epoch:10 step:10163 [D loss: 0.711356, acc.: 47.66%] [G loss: 0.865261]\n",
      "epoch:10 step:10164 [D loss: 0.710545, acc.: 50.00%] [G loss: 0.856630]\n",
      "epoch:10 step:10165 [D loss: 0.694722, acc.: 55.47%] [G loss: 0.823445]\n",
      "epoch:10 step:10166 [D loss: 0.640679, acc.: 64.84%] [G loss: 0.945950]\n",
      "epoch:10 step:10167 [D loss: 0.659507, acc.: 57.03%] [G loss: 0.874367]\n",
      "epoch:10 step:10168 [D loss: 0.683065, acc.: 61.72%] [G loss: 0.841413]\n",
      "epoch:10 step:10169 [D loss: 0.666305, acc.: 57.81%] [G loss: 0.797428]\n",
      "epoch:10 step:10170 [D loss: 0.614768, acc.: 67.97%] [G loss: 0.855038]\n",
      "epoch:10 step:10171 [D loss: 0.645440, acc.: 59.38%] [G loss: 0.842068]\n",
      "epoch:10 step:10172 [D loss: 0.677918, acc.: 55.47%] [G loss: 0.838324]\n",
      "epoch:10 step:10173 [D loss: 0.584012, acc.: 72.66%] [G loss: 0.851600]\n",
      "epoch:10 step:10174 [D loss: 0.718820, acc.: 45.31%] [G loss: 0.841017]\n",
      "epoch:10 step:10175 [D loss: 0.685228, acc.: 57.81%] [G loss: 0.839680]\n",
      "epoch:10 step:10176 [D loss: 0.716018, acc.: 52.34%] [G loss: 0.848309]\n",
      "epoch:10 step:10177 [D loss: 0.706398, acc.: 51.56%] [G loss: 0.793180]\n",
      "epoch:10 step:10178 [D loss: 0.645574, acc.: 65.62%] [G loss: 0.791398]\n",
      "epoch:10 step:10179 [D loss: 0.636741, acc.: 60.94%] [G loss: 0.810073]\n",
      "epoch:10 step:10180 [D loss: 0.708701, acc.: 57.03%] [G loss: 0.774375]\n",
      "epoch:10 step:10181 [D loss: 0.683828, acc.: 53.12%] [G loss: 0.795126]\n",
      "epoch:10 step:10182 [D loss: 0.670778, acc.: 55.47%] [G loss: 0.814778]\n",
      "epoch:10 step:10183 [D loss: 0.754357, acc.: 49.22%] [G loss: 0.820616]\n",
      "epoch:10 step:10184 [D loss: 0.646230, acc.: 63.28%] [G loss: 0.789302]\n",
      "epoch:10 step:10185 [D loss: 0.678414, acc.: 58.59%] [G loss: 0.818382]\n",
      "epoch:10 step:10186 [D loss: 0.672813, acc.: 59.38%] [G loss: 0.687648]\n",
      "epoch:10 step:10187 [D loss: 0.669176, acc.: 61.72%] [G loss: 0.748562]\n",
      "epoch:10 step:10188 [D loss: 0.681850, acc.: 56.25%] [G loss: 0.781708]\n",
      "epoch:10 step:10189 [D loss: 0.673777, acc.: 57.03%] [G loss: 0.899168]\n",
      "epoch:10 step:10190 [D loss: 0.692371, acc.: 52.34%] [G loss: 0.849403]\n",
      "epoch:10 step:10191 [D loss: 0.640010, acc.: 62.50%] [G loss: 0.878118]\n",
      "epoch:10 step:10192 [D loss: 0.624048, acc.: 65.62%] [G loss: 0.880785]\n",
      "epoch:10 step:10193 [D loss: 0.669669, acc.: 54.69%] [G loss: 0.777852]\n",
      "epoch:10 step:10194 [D loss: 0.680678, acc.: 54.69%] [G loss: 0.818611]\n",
      "epoch:10 step:10195 [D loss: 0.629925, acc.: 67.19%] [G loss: 0.954335]\n",
      "epoch:10 step:10196 [D loss: 0.698146, acc.: 54.69%] [G loss: 0.802921]\n",
      "epoch:10 step:10197 [D loss: 0.686658, acc.: 54.69%] [G loss: 0.836912]\n",
      "epoch:10 step:10198 [D loss: 0.680545, acc.: 54.69%] [G loss: 0.779395]\n",
      "epoch:10 step:10199 [D loss: 0.675589, acc.: 61.72%] [G loss: 0.796918]\n",
      "epoch:10 step:10200 [D loss: 0.713577, acc.: 47.66%] [G loss: 0.781269]\n",
      "epoch:10 step:10201 [D loss: 0.668590, acc.: 58.59%] [G loss: 0.880469]\n",
      "epoch:10 step:10202 [D loss: 0.772896, acc.: 48.44%] [G loss: 0.825366]\n",
      "epoch:10 step:10203 [D loss: 0.684732, acc.: 48.44%] [G loss: 0.815235]\n",
      "epoch:10 step:10204 [D loss: 0.651409, acc.: 62.50%] [G loss: 0.798765]\n",
      "epoch:10 step:10205 [D loss: 0.718923, acc.: 46.09%] [G loss: 0.767561]\n",
      "epoch:10 step:10206 [D loss: 0.685949, acc.: 52.34%] [G loss: 0.797394]\n",
      "epoch:10 step:10207 [D loss: 0.710686, acc.: 44.53%] [G loss: 0.843593]\n",
      "epoch:10 step:10208 [D loss: 0.661282, acc.: 62.50%] [G loss: 0.825690]\n",
      "epoch:10 step:10209 [D loss: 0.648970, acc.: 66.41%] [G loss: 0.813955]\n",
      "epoch:10 step:10210 [D loss: 0.637917, acc.: 69.53%] [G loss: 0.769370]\n",
      "epoch:10 step:10211 [D loss: 0.669802, acc.: 57.03%] [G loss: 0.829712]\n",
      "epoch:10 step:10212 [D loss: 0.674377, acc.: 55.47%] [G loss: 0.781799]\n",
      "epoch:10 step:10213 [D loss: 0.690436, acc.: 52.34%] [G loss: 0.936530]\n",
      "epoch:10 step:10214 [D loss: 0.703561, acc.: 46.09%] [G loss: 0.838298]\n",
      "epoch:10 step:10215 [D loss: 0.707396, acc.: 47.66%] [G loss: 0.780123]\n",
      "epoch:10 step:10216 [D loss: 0.665856, acc.: 60.94%] [G loss: 0.762333]\n",
      "epoch:10 step:10217 [D loss: 0.666091, acc.: 60.94%] [G loss: 0.767466]\n",
      "epoch:10 step:10218 [D loss: 0.657433, acc.: 58.59%] [G loss: 0.822181]\n",
      "epoch:10 step:10219 [D loss: 0.650500, acc.: 65.62%] [G loss: 0.876124]\n",
      "epoch:10 step:10220 [D loss: 0.624484, acc.: 69.53%] [G loss: 0.837282]\n",
      "epoch:10 step:10221 [D loss: 0.660315, acc.: 60.16%] [G loss: 0.862305]\n",
      "epoch:10 step:10222 [D loss: 0.673361, acc.: 67.19%] [G loss: 0.875982]\n",
      "epoch:10 step:10223 [D loss: 0.657563, acc.: 64.06%] [G loss: 0.914216]\n",
      "epoch:10 step:10224 [D loss: 0.682223, acc.: 54.69%] [G loss: 0.853202]\n",
      "epoch:10 step:10225 [D loss: 0.673658, acc.: 50.78%] [G loss: 0.891406]\n",
      "epoch:10 step:10226 [D loss: 0.683467, acc.: 60.94%] [G loss: 0.845166]\n",
      "epoch:10 step:10227 [D loss: 0.727959, acc.: 47.66%] [G loss: 0.786804]\n",
      "epoch:10 step:10228 [D loss: 0.693794, acc.: 54.69%] [G loss: 0.747164]\n",
      "epoch:10 step:10229 [D loss: 0.697787, acc.: 50.78%] [G loss: 0.745533]\n",
      "epoch:10 step:10230 [D loss: 0.672381, acc.: 56.25%] [G loss: 0.769277]\n",
      "epoch:10 step:10231 [D loss: 0.711698, acc.: 49.22%] [G loss: 0.794485]\n",
      "epoch:10 step:10232 [D loss: 0.681865, acc.: 53.12%] [G loss: 0.739060]\n",
      "epoch:10 step:10233 [D loss: 0.657988, acc.: 58.59%] [G loss: 0.851301]\n",
      "epoch:10 step:10234 [D loss: 0.705297, acc.: 53.12%] [G loss: 0.760480]\n",
      "epoch:10 step:10235 [D loss: 0.603485, acc.: 72.66%] [G loss: 0.863922]\n",
      "epoch:10 step:10236 [D loss: 0.658507, acc.: 54.69%] [G loss: 0.868226]\n",
      "epoch:10 step:10237 [D loss: 0.696303, acc.: 48.44%] [G loss: 0.787388]\n",
      "epoch:10 step:10238 [D loss: 0.663629, acc.: 67.19%] [G loss: 0.926657]\n",
      "epoch:10 step:10239 [D loss: 0.689031, acc.: 50.78%] [G loss: 0.882518]\n",
      "epoch:10 step:10240 [D loss: 0.654722, acc.: 63.28%] [G loss: 0.934810]\n",
      "epoch:10 step:10241 [D loss: 0.760672, acc.: 36.72%] [G loss: 0.821499]\n",
      "epoch:10 step:10242 [D loss: 0.627820, acc.: 68.75%] [G loss: 0.819777]\n",
      "epoch:10 step:10243 [D loss: 0.672818, acc.: 53.91%] [G loss: 0.799205]\n",
      "epoch:10 step:10244 [D loss: 0.680035, acc.: 58.59%] [G loss: 0.913398]\n",
      "epoch:10 step:10245 [D loss: 0.673644, acc.: 54.69%] [G loss: 0.887607]\n",
      "epoch:10 step:10246 [D loss: 0.601938, acc.: 69.53%] [G loss: 0.909984]\n",
      "epoch:10 step:10247 [D loss: 0.698397, acc.: 44.53%] [G loss: 0.807897]\n",
      "epoch:10 step:10248 [D loss: 0.722789, acc.: 47.66%] [G loss: 0.912614]\n",
      "epoch:10 step:10249 [D loss: 0.646226, acc.: 65.62%] [G loss: 0.818717]\n",
      "epoch:10 step:10250 [D loss: 0.686548, acc.: 50.78%] [G loss: 0.823574]\n",
      "epoch:10 step:10251 [D loss: 0.647027, acc.: 62.50%] [G loss: 0.824612]\n",
      "epoch:10 step:10252 [D loss: 0.673298, acc.: 57.03%] [G loss: 0.766326]\n",
      "epoch:10 step:10253 [D loss: 0.645109, acc.: 62.50%] [G loss: 0.762773]\n",
      "epoch:10 step:10254 [D loss: 0.669556, acc.: 54.69%] [G loss: 0.783558]\n",
      "epoch:10 step:10255 [D loss: 0.658491, acc.: 57.81%] [G loss: 0.815167]\n",
      "epoch:10 step:10256 [D loss: 0.683180, acc.: 59.38%] [G loss: 0.785165]\n",
      "epoch:10 step:10257 [D loss: 0.648799, acc.: 63.28%] [G loss: 0.868946]\n",
      "epoch:10 step:10258 [D loss: 0.612881, acc.: 64.06%] [G loss: 0.796181]\n",
      "epoch:10 step:10259 [D loss: 0.643179, acc.: 64.06%] [G loss: 0.800254]\n",
      "epoch:10 step:10260 [D loss: 0.663110, acc.: 57.03%] [G loss: 0.776619]\n",
      "epoch:10 step:10261 [D loss: 0.672940, acc.: 57.03%] [G loss: 0.767365]\n",
      "epoch:10 step:10262 [D loss: 0.651879, acc.: 56.25%] [G loss: 0.806617]\n",
      "epoch:10 step:10263 [D loss: 0.692438, acc.: 48.44%] [G loss: 0.746180]\n",
      "epoch:10 step:10264 [D loss: 0.718456, acc.: 50.00%] [G loss: 0.757424]\n",
      "epoch:10 step:10265 [D loss: 0.652860, acc.: 60.94%] [G loss: 0.856669]\n",
      "epoch:10 step:10266 [D loss: 0.677883, acc.: 60.94%] [G loss: 0.838315]\n",
      "epoch:10 step:10267 [D loss: 0.661823, acc.: 60.94%] [G loss: 0.865941]\n",
      "epoch:10 step:10268 [D loss: 0.668053, acc.: 57.81%] [G loss: 0.860719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10269 [D loss: 0.659745, acc.: 63.28%] [G loss: 0.900459]\n",
      "epoch:10 step:10270 [D loss: 0.719483, acc.: 43.75%] [G loss: 0.973350]\n",
      "epoch:10 step:10271 [D loss: 0.696720, acc.: 47.66%] [G loss: 0.773217]\n",
      "epoch:10 step:10272 [D loss: 0.637321, acc.: 64.06%] [G loss: 0.861113]\n",
      "epoch:10 step:10273 [D loss: 0.691951, acc.: 56.25%] [G loss: 0.751206]\n",
      "epoch:10 step:10274 [D loss: 0.739303, acc.: 40.62%] [G loss: 0.927187]\n",
      "epoch:10 step:10275 [D loss: 0.673004, acc.: 60.16%] [G loss: 0.824805]\n",
      "epoch:10 step:10276 [D loss: 0.653958, acc.: 54.69%] [G loss: 0.820852]\n",
      "epoch:10 step:10277 [D loss: 0.661349, acc.: 59.38%] [G loss: 0.774661]\n",
      "epoch:10 step:10278 [D loss: 0.658371, acc.: 61.72%] [G loss: 0.758932]\n",
      "epoch:10 step:10279 [D loss: 0.646681, acc.: 64.06%] [G loss: 0.867664]\n",
      "epoch:10 step:10280 [D loss: 0.708251, acc.: 50.78%] [G loss: 0.867224]\n",
      "epoch:10 step:10281 [D loss: 0.618841, acc.: 66.41%] [G loss: 0.861512]\n",
      "epoch:10 step:10282 [D loss: 0.667029, acc.: 62.50%] [G loss: 0.835612]\n",
      "epoch:10 step:10283 [D loss: 0.633702, acc.: 65.62%] [G loss: 0.765139]\n",
      "epoch:10 step:10284 [D loss: 0.682016, acc.: 56.25%] [G loss: 0.857190]\n",
      "epoch:10 step:10285 [D loss: 0.678261, acc.: 57.03%] [G loss: 0.727091]\n",
      "epoch:10 step:10286 [D loss: 0.686166, acc.: 53.12%] [G loss: 0.781407]\n",
      "epoch:10 step:10287 [D loss: 0.717791, acc.: 53.12%] [G loss: 0.832319]\n",
      "epoch:10 step:10288 [D loss: 0.648714, acc.: 60.94%] [G loss: 0.741139]\n",
      "epoch:10 step:10289 [D loss: 0.761003, acc.: 53.91%] [G loss: 0.898623]\n",
      "epoch:10 step:10290 [D loss: 0.691629, acc.: 50.78%] [G loss: 0.820536]\n",
      "epoch:10 step:10291 [D loss: 0.665482, acc.: 62.50%] [G loss: 0.793974]\n",
      "epoch:10 step:10292 [D loss: 0.697784, acc.: 51.56%] [G loss: 0.815278]\n",
      "epoch:10 step:10293 [D loss: 0.682937, acc.: 57.03%] [G loss: 0.825373]\n",
      "epoch:10 step:10294 [D loss: 0.682575, acc.: 57.81%] [G loss: 0.819785]\n",
      "epoch:10 step:10295 [D loss: 0.665141, acc.: 57.81%] [G loss: 0.934764]\n",
      "epoch:10 step:10296 [D loss: 0.650090, acc.: 66.41%] [G loss: 0.829902]\n",
      "epoch:10 step:10297 [D loss: 0.672686, acc.: 57.81%] [G loss: 0.828171]\n",
      "epoch:10 step:10298 [D loss: 0.675158, acc.: 57.03%] [G loss: 0.841337]\n",
      "epoch:10 step:10299 [D loss: 0.693547, acc.: 53.91%] [G loss: 0.847425]\n",
      "epoch:10 step:10300 [D loss: 0.662406, acc.: 65.62%] [G loss: 0.767308]\n",
      "epoch:10 step:10301 [D loss: 0.668100, acc.: 63.28%] [G loss: 0.799112]\n",
      "epoch:10 step:10302 [D loss: 0.683617, acc.: 62.50%] [G loss: 0.854197]\n",
      "epoch:10 step:10303 [D loss: 0.685963, acc.: 60.16%] [G loss: 0.828444]\n",
      "epoch:10 step:10304 [D loss: 0.649726, acc.: 60.16%] [G loss: 0.876703]\n",
      "epoch:10 step:10305 [D loss: 0.644711, acc.: 58.59%] [G loss: 0.758514]\n",
      "epoch:10 step:10306 [D loss: 0.659405, acc.: 58.59%] [G loss: 0.855843]\n",
      "epoch:10 step:10307 [D loss: 0.706852, acc.: 53.12%] [G loss: 0.773642]\n",
      "epoch:11 step:10308 [D loss: 0.692600, acc.: 52.34%] [G loss: 0.848553]\n",
      "epoch:11 step:10309 [D loss: 0.661825, acc.: 64.06%] [G loss: 0.911848]\n",
      "epoch:11 step:10310 [D loss: 0.697545, acc.: 50.00%] [G loss: 0.876568]\n",
      "epoch:11 step:10311 [D loss: 0.662375, acc.: 55.47%] [G loss: 0.825612]\n",
      "epoch:11 step:10312 [D loss: 0.599040, acc.: 78.12%] [G loss: 0.797502]\n",
      "epoch:11 step:10313 [D loss: 0.651669, acc.: 61.72%] [G loss: 0.899197]\n",
      "epoch:11 step:10314 [D loss: 0.651531, acc.: 64.06%] [G loss: 0.788997]\n",
      "epoch:11 step:10315 [D loss: 0.679803, acc.: 49.22%] [G loss: 0.927619]\n",
      "epoch:11 step:10316 [D loss: 0.712416, acc.: 51.56%] [G loss: 0.863901]\n",
      "epoch:11 step:10317 [D loss: 0.693109, acc.: 53.91%] [G loss: 0.792599]\n",
      "epoch:11 step:10318 [D loss: 0.699502, acc.: 50.00%] [G loss: 0.739710]\n",
      "epoch:11 step:10319 [D loss: 0.690855, acc.: 52.34%] [G loss: 0.794620]\n",
      "epoch:11 step:10320 [D loss: 0.705774, acc.: 59.38%] [G loss: 0.861292]\n",
      "epoch:11 step:10321 [D loss: 0.701510, acc.: 50.00%] [G loss: 0.821023]\n",
      "epoch:11 step:10322 [D loss: 0.711414, acc.: 52.34%] [G loss: 0.798762]\n",
      "epoch:11 step:10323 [D loss: 0.684750, acc.: 57.81%] [G loss: 0.856005]\n",
      "epoch:11 step:10324 [D loss: 0.653734, acc.: 60.16%] [G loss: 0.893370]\n",
      "epoch:11 step:10325 [D loss: 0.658730, acc.: 60.16%] [G loss: 0.779012]\n",
      "epoch:11 step:10326 [D loss: 0.734557, acc.: 44.53%] [G loss: 0.813935]\n",
      "epoch:11 step:10327 [D loss: 0.678439, acc.: 57.03%] [G loss: 0.819610]\n",
      "epoch:11 step:10328 [D loss: 0.652756, acc.: 60.94%] [G loss: 0.816851]\n",
      "epoch:11 step:10329 [D loss: 0.673105, acc.: 57.03%] [G loss: 0.841584]\n",
      "epoch:11 step:10330 [D loss: 0.668331, acc.: 60.16%] [G loss: 0.832447]\n",
      "epoch:11 step:10331 [D loss: 0.674841, acc.: 55.47%] [G loss: 0.908169]\n",
      "epoch:11 step:10332 [D loss: 0.662212, acc.: 59.38%] [G loss: 0.841190]\n",
      "epoch:11 step:10333 [D loss: 0.688607, acc.: 53.12%] [G loss: 0.812495]\n",
      "epoch:11 step:10334 [D loss: 0.676003, acc.: 57.03%] [G loss: 0.790153]\n",
      "epoch:11 step:10335 [D loss: 0.676961, acc.: 58.59%] [G loss: 0.819675]\n",
      "epoch:11 step:10336 [D loss: 0.669119, acc.: 53.91%] [G loss: 0.841742]\n",
      "epoch:11 step:10337 [D loss: 0.674014, acc.: 57.03%] [G loss: 0.877835]\n",
      "epoch:11 step:10338 [D loss: 0.691656, acc.: 48.44%] [G loss: 0.897512]\n",
      "epoch:11 step:10339 [D loss: 0.670460, acc.: 56.25%] [G loss: 0.832461]\n",
      "epoch:11 step:10340 [D loss: 0.659733, acc.: 60.94%] [G loss: 0.879373]\n",
      "epoch:11 step:10341 [D loss: 0.645500, acc.: 57.81%] [G loss: 0.897734]\n",
      "epoch:11 step:10342 [D loss: 0.682510, acc.: 58.59%] [G loss: 0.862881]\n",
      "epoch:11 step:10343 [D loss: 0.666482, acc.: 60.94%] [G loss: 0.851182]\n",
      "epoch:11 step:10344 [D loss: 0.647737, acc.: 64.84%] [G loss: 0.832769]\n",
      "epoch:11 step:10345 [D loss: 0.659973, acc.: 54.69%] [G loss: 0.840440]\n",
      "epoch:11 step:10346 [D loss: 0.664929, acc.: 65.62%] [G loss: 0.829696]\n",
      "epoch:11 step:10347 [D loss: 0.677390, acc.: 57.81%] [G loss: 0.794916]\n",
      "epoch:11 step:10348 [D loss: 0.649793, acc.: 60.16%] [G loss: 0.891592]\n",
      "epoch:11 step:10349 [D loss: 0.713739, acc.: 46.88%] [G loss: 0.809653]\n",
      "epoch:11 step:10350 [D loss: 0.610871, acc.: 71.88%] [G loss: 0.815644]\n",
      "epoch:11 step:10351 [D loss: 0.656008, acc.: 61.72%] [G loss: 0.780985]\n",
      "epoch:11 step:10352 [D loss: 0.692749, acc.: 58.59%] [G loss: 0.895355]\n",
      "epoch:11 step:10353 [D loss: 0.656015, acc.: 61.72%] [G loss: 0.886878]\n",
      "epoch:11 step:10354 [D loss: 0.676970, acc.: 56.25%] [G loss: 0.853690]\n",
      "epoch:11 step:10355 [D loss: 0.696754, acc.: 47.66%] [G loss: 0.786128]\n",
      "epoch:11 step:10356 [D loss: 0.683628, acc.: 50.78%] [G loss: 0.820828]\n",
      "epoch:11 step:10357 [D loss: 0.614181, acc.: 69.53%] [G loss: 0.747458]\n",
      "epoch:11 step:10358 [D loss: 0.675418, acc.: 53.91%] [G loss: 0.790501]\n",
      "epoch:11 step:10359 [D loss: 0.623722, acc.: 67.97%] [G loss: 0.839630]\n",
      "epoch:11 step:10360 [D loss: 0.650252, acc.: 57.03%] [G loss: 0.888791]\n",
      "epoch:11 step:10361 [D loss: 0.737131, acc.: 41.41%] [G loss: 0.820945]\n",
      "epoch:11 step:10362 [D loss: 0.688159, acc.: 53.91%] [G loss: 0.854206]\n",
      "epoch:11 step:10363 [D loss: 0.648520, acc.: 61.72%] [G loss: 0.884143]\n",
      "epoch:11 step:10364 [D loss: 0.700857, acc.: 52.34%] [G loss: 0.870608]\n",
      "epoch:11 step:10365 [D loss: 0.644350, acc.: 68.75%] [G loss: 0.800238]\n",
      "epoch:11 step:10366 [D loss: 0.698498, acc.: 55.47%] [G loss: 0.820582]\n",
      "epoch:11 step:10367 [D loss: 0.708336, acc.: 53.12%] [G loss: 0.879628]\n",
      "epoch:11 step:10368 [D loss: 0.683484, acc.: 57.03%] [G loss: 0.797172]\n",
      "epoch:11 step:10369 [D loss: 0.645465, acc.: 60.94%] [G loss: 0.874583]\n",
      "epoch:11 step:10370 [D loss: 0.662399, acc.: 58.59%] [G loss: 0.852301]\n",
      "epoch:11 step:10371 [D loss: 0.643393, acc.: 67.97%] [G loss: 0.855797]\n",
      "epoch:11 step:10372 [D loss: 0.723232, acc.: 49.22%] [G loss: 0.858606]\n",
      "epoch:11 step:10373 [D loss: 0.682806, acc.: 60.94%] [G loss: 0.810661]\n",
      "epoch:11 step:10374 [D loss: 0.671138, acc.: 55.47%] [G loss: 0.798739]\n",
      "epoch:11 step:10375 [D loss: 0.698155, acc.: 46.88%] [G loss: 0.806201]\n",
      "epoch:11 step:10376 [D loss: 0.619609, acc.: 74.22%] [G loss: 0.835217]\n",
      "epoch:11 step:10377 [D loss: 0.671356, acc.: 58.59%] [G loss: 0.815629]\n",
      "epoch:11 step:10378 [D loss: 0.656552, acc.: 60.16%] [G loss: 0.908424]\n",
      "epoch:11 step:10379 [D loss: 0.658087, acc.: 58.59%] [G loss: 0.809126]\n",
      "epoch:11 step:10380 [D loss: 0.623314, acc.: 65.62%] [G loss: 0.867580]\n",
      "epoch:11 step:10381 [D loss: 0.665449, acc.: 57.81%] [G loss: 0.864242]\n",
      "epoch:11 step:10382 [D loss: 0.654508, acc.: 63.28%] [G loss: 0.807168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10383 [D loss: 0.711340, acc.: 47.66%] [G loss: 0.775958]\n",
      "epoch:11 step:10384 [D loss: 0.700436, acc.: 50.78%] [G loss: 0.732748]\n",
      "epoch:11 step:10385 [D loss: 0.652404, acc.: 62.50%] [G loss: 0.795301]\n",
      "epoch:11 step:10386 [D loss: 0.702542, acc.: 50.00%] [G loss: 0.814179]\n",
      "epoch:11 step:10387 [D loss: 0.699403, acc.: 50.00%] [G loss: 0.793797]\n",
      "epoch:11 step:10388 [D loss: 0.671820, acc.: 56.25%] [G loss: 0.835244]\n",
      "epoch:11 step:10389 [D loss: 0.742320, acc.: 46.09%] [G loss: 0.822715]\n",
      "epoch:11 step:10390 [D loss: 0.657641, acc.: 66.41%] [G loss: 0.880464]\n",
      "epoch:11 step:10391 [D loss: 0.717788, acc.: 48.44%] [G loss: 0.886388]\n",
      "epoch:11 step:10392 [D loss: 0.659118, acc.: 56.25%] [G loss: 0.944664]\n",
      "epoch:11 step:10393 [D loss: 0.633627, acc.: 70.31%] [G loss: 0.815574]\n",
      "epoch:11 step:10394 [D loss: 0.708384, acc.: 48.44%] [G loss: 0.816810]\n",
      "epoch:11 step:10395 [D loss: 0.693662, acc.: 56.25%] [G loss: 0.814262]\n",
      "epoch:11 step:10396 [D loss: 0.649974, acc.: 60.16%] [G loss: 0.787077]\n",
      "epoch:11 step:10397 [D loss: 0.701032, acc.: 47.66%] [G loss: 0.846135]\n",
      "epoch:11 step:10398 [D loss: 0.654755, acc.: 60.94%] [G loss: 0.847326]\n",
      "epoch:11 step:10399 [D loss: 0.663844, acc.: 57.81%] [G loss: 0.915141]\n",
      "epoch:11 step:10400 [D loss: 0.648717, acc.: 59.38%] [G loss: 0.833308]\n",
      "epoch:11 step:10401 [D loss: 0.688514, acc.: 55.47%] [G loss: 0.728042]\n",
      "epoch:11 step:10402 [D loss: 0.700381, acc.: 46.09%] [G loss: 0.865547]\n",
      "epoch:11 step:10403 [D loss: 0.682662, acc.: 59.38%] [G loss: 0.791715]\n",
      "epoch:11 step:10404 [D loss: 0.654809, acc.: 59.38%] [G loss: 0.790315]\n",
      "epoch:11 step:10405 [D loss: 0.628779, acc.: 63.28%] [G loss: 0.829075]\n",
      "epoch:11 step:10406 [D loss: 0.685481, acc.: 55.47%] [G loss: 0.852095]\n",
      "epoch:11 step:10407 [D loss: 0.665424, acc.: 60.94%] [G loss: 0.799433]\n",
      "epoch:11 step:10408 [D loss: 0.679168, acc.: 53.91%] [G loss: 0.814923]\n",
      "epoch:11 step:10409 [D loss: 0.634768, acc.: 66.41%] [G loss: 0.808549]\n",
      "epoch:11 step:10410 [D loss: 0.693681, acc.: 48.44%] [G loss: 0.831715]\n",
      "epoch:11 step:10411 [D loss: 0.685961, acc.: 50.78%] [G loss: 0.869927]\n",
      "epoch:11 step:10412 [D loss: 0.701373, acc.: 46.09%] [G loss: 0.829569]\n",
      "epoch:11 step:10413 [D loss: 0.684282, acc.: 56.25%] [G loss: 0.849580]\n",
      "epoch:11 step:10414 [D loss: 0.676682, acc.: 53.12%] [G loss: 0.861267]\n",
      "epoch:11 step:10415 [D loss: 0.693345, acc.: 57.03%] [G loss: 0.820705]\n",
      "epoch:11 step:10416 [D loss: 0.707425, acc.: 48.44%] [G loss: 0.810856]\n",
      "epoch:11 step:10417 [D loss: 0.716486, acc.: 50.00%] [G loss: 0.805014]\n",
      "epoch:11 step:10418 [D loss: 0.662700, acc.: 56.25%] [G loss: 0.828173]\n",
      "epoch:11 step:10419 [D loss: 0.692093, acc.: 52.34%] [G loss: 0.839454]\n",
      "epoch:11 step:10420 [D loss: 0.681303, acc.: 53.91%] [G loss: 0.792743]\n",
      "epoch:11 step:10421 [D loss: 0.698131, acc.: 57.81%] [G loss: 0.769303]\n",
      "epoch:11 step:10422 [D loss: 0.693340, acc.: 53.91%] [G loss: 0.812668]\n",
      "epoch:11 step:10423 [D loss: 0.682698, acc.: 55.47%] [G loss: 0.740416]\n",
      "epoch:11 step:10424 [D loss: 0.652607, acc.: 62.50%] [G loss: 0.767431]\n",
      "epoch:11 step:10425 [D loss: 0.659303, acc.: 60.94%] [G loss: 0.861195]\n",
      "epoch:11 step:10426 [D loss: 0.669377, acc.: 55.47%] [G loss: 0.772509]\n",
      "epoch:11 step:10427 [D loss: 0.706436, acc.: 50.78%] [G loss: 0.785458]\n",
      "epoch:11 step:10428 [D loss: 0.677109, acc.: 57.03%] [G loss: 0.805868]\n",
      "epoch:11 step:10429 [D loss: 0.672083, acc.: 55.47%] [G loss: 0.833810]\n",
      "epoch:11 step:10430 [D loss: 0.716339, acc.: 49.22%] [G loss: 0.731690]\n",
      "epoch:11 step:10431 [D loss: 0.704443, acc.: 47.66%] [G loss: 0.742999]\n",
      "epoch:11 step:10432 [D loss: 0.641814, acc.: 62.50%] [G loss: 0.800973]\n",
      "epoch:11 step:10433 [D loss: 0.703411, acc.: 46.88%] [G loss: 0.762103]\n",
      "epoch:11 step:10434 [D loss: 0.714106, acc.: 47.66%] [G loss: 0.782358]\n",
      "epoch:11 step:10435 [D loss: 0.722715, acc.: 50.00%] [G loss: 0.735975]\n",
      "epoch:11 step:10436 [D loss: 0.674579, acc.: 56.25%] [G loss: 0.807769]\n",
      "epoch:11 step:10437 [D loss: 0.672148, acc.: 58.59%] [G loss: 0.798883]\n",
      "epoch:11 step:10438 [D loss: 0.647446, acc.: 61.72%] [G loss: 0.819944]\n",
      "epoch:11 step:10439 [D loss: 0.712408, acc.: 40.62%] [G loss: 0.697939]\n",
      "epoch:11 step:10440 [D loss: 0.668951, acc.: 54.69%] [G loss: 0.946026]\n",
      "epoch:11 step:10441 [D loss: 0.669026, acc.: 60.16%] [G loss: 0.822843]\n",
      "epoch:11 step:10442 [D loss: 0.694551, acc.: 52.34%] [G loss: 0.802451]\n",
      "epoch:11 step:10443 [D loss: 0.693700, acc.: 45.31%] [G loss: 0.821435]\n",
      "epoch:11 step:10444 [D loss: 0.696500, acc.: 53.91%] [G loss: 0.851933]\n",
      "epoch:11 step:10445 [D loss: 0.645254, acc.: 63.28%] [G loss: 0.788575]\n",
      "epoch:11 step:10446 [D loss: 0.711651, acc.: 50.00%] [G loss: 0.818923]\n",
      "epoch:11 step:10447 [D loss: 0.713133, acc.: 52.34%] [G loss: 0.787177]\n",
      "epoch:11 step:10448 [D loss: 0.692408, acc.: 50.00%] [G loss: 0.838891]\n",
      "epoch:11 step:10449 [D loss: 0.680897, acc.: 52.34%] [G loss: 0.789778]\n",
      "epoch:11 step:10450 [D loss: 0.672371, acc.: 57.81%] [G loss: 0.753261]\n",
      "epoch:11 step:10451 [D loss: 0.664662, acc.: 54.69%] [G loss: 0.814716]\n",
      "epoch:11 step:10452 [D loss: 0.677712, acc.: 55.47%] [G loss: 0.746125]\n",
      "epoch:11 step:10453 [D loss: 0.683256, acc.: 53.91%] [G loss: 0.812021]\n",
      "epoch:11 step:10454 [D loss: 0.651141, acc.: 57.81%] [G loss: 0.809919]\n",
      "epoch:11 step:10455 [D loss: 0.652493, acc.: 57.81%] [G loss: 0.849220]\n",
      "epoch:11 step:10456 [D loss: 0.711260, acc.: 53.12%] [G loss: 0.859205]\n",
      "epoch:11 step:10457 [D loss: 0.655668, acc.: 58.59%] [G loss: 0.825197]\n",
      "epoch:11 step:10458 [D loss: 0.680496, acc.: 54.69%] [G loss: 0.840986]\n",
      "epoch:11 step:10459 [D loss: 0.679214, acc.: 61.72%] [G loss: 0.884391]\n",
      "epoch:11 step:10460 [D loss: 0.695533, acc.: 51.56%] [G loss: 0.765959]\n",
      "epoch:11 step:10461 [D loss: 0.715106, acc.: 45.31%] [G loss: 0.738986]\n",
      "epoch:11 step:10462 [D loss: 0.693340, acc.: 55.47%] [G loss: 0.772776]\n",
      "epoch:11 step:10463 [D loss: 0.656059, acc.: 63.28%] [G loss: 0.759884]\n",
      "epoch:11 step:10464 [D loss: 0.674850, acc.: 53.12%] [G loss: 0.741144]\n",
      "epoch:11 step:10465 [D loss: 0.646732, acc.: 60.16%] [G loss: 0.799035]\n",
      "epoch:11 step:10466 [D loss: 0.702039, acc.: 52.34%] [G loss: 0.807479]\n",
      "epoch:11 step:10467 [D loss: 0.648192, acc.: 62.50%] [G loss: 0.765742]\n",
      "epoch:11 step:10468 [D loss: 0.672922, acc.: 60.94%] [G loss: 0.767220]\n",
      "epoch:11 step:10469 [D loss: 0.684145, acc.: 54.69%] [G loss: 0.764373]\n",
      "epoch:11 step:10470 [D loss: 0.688160, acc.: 52.34%] [G loss: 0.823588]\n",
      "epoch:11 step:10471 [D loss: 0.685710, acc.: 50.78%] [G loss: 0.814039]\n",
      "epoch:11 step:10472 [D loss: 0.659361, acc.: 58.59%] [G loss: 0.834466]\n",
      "epoch:11 step:10473 [D loss: 0.675493, acc.: 66.41%] [G loss: 0.757876]\n",
      "epoch:11 step:10474 [D loss: 0.685859, acc.: 54.69%] [G loss: 0.851050]\n",
      "epoch:11 step:10475 [D loss: 0.737971, acc.: 46.09%] [G loss: 0.763054]\n",
      "epoch:11 step:10476 [D loss: 0.648709, acc.: 63.28%] [G loss: 0.843776]\n",
      "epoch:11 step:10477 [D loss: 0.688021, acc.: 50.78%] [G loss: 0.816010]\n",
      "epoch:11 step:10478 [D loss: 0.686075, acc.: 57.03%] [G loss: 0.848993]\n",
      "epoch:11 step:10479 [D loss: 0.663258, acc.: 61.72%] [G loss: 0.759963]\n",
      "epoch:11 step:10480 [D loss: 0.699858, acc.: 53.12%] [G loss: 0.801737]\n",
      "epoch:11 step:10481 [D loss: 0.681935, acc.: 50.78%] [G loss: 0.832853]\n",
      "epoch:11 step:10482 [D loss: 0.712810, acc.: 53.91%] [G loss: 0.796648]\n",
      "epoch:11 step:10483 [D loss: 0.641632, acc.: 64.06%] [G loss: 0.814990]\n",
      "epoch:11 step:10484 [D loss: 0.652561, acc.: 63.28%] [G loss: 0.801257]\n",
      "epoch:11 step:10485 [D loss: 0.670992, acc.: 64.06%] [G loss: 0.813883]\n",
      "epoch:11 step:10486 [D loss: 0.676655, acc.: 56.25%] [G loss: 0.866576]\n",
      "epoch:11 step:10487 [D loss: 0.654655, acc.: 60.16%] [G loss: 0.856688]\n",
      "epoch:11 step:10488 [D loss: 0.692588, acc.: 53.91%] [G loss: 0.788887]\n",
      "epoch:11 step:10489 [D loss: 0.675439, acc.: 54.69%] [G loss: 0.839990]\n",
      "epoch:11 step:10490 [D loss: 0.700197, acc.: 54.69%] [G loss: 0.857157]\n",
      "epoch:11 step:10491 [D loss: 0.657294, acc.: 62.50%] [G loss: 0.795756]\n",
      "epoch:11 step:10492 [D loss: 0.657799, acc.: 59.38%] [G loss: 0.814333]\n",
      "epoch:11 step:10493 [D loss: 0.719443, acc.: 53.12%] [G loss: 0.821458]\n",
      "epoch:11 step:10494 [D loss: 0.694862, acc.: 53.91%] [G loss: 0.782007]\n",
      "epoch:11 step:10495 [D loss: 0.689793, acc.: 52.34%] [G loss: 0.791650]\n",
      "epoch:11 step:10496 [D loss: 0.662615, acc.: 60.94%] [G loss: 0.832063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10497 [D loss: 0.679886, acc.: 52.34%] [G loss: 0.777405]\n",
      "epoch:11 step:10498 [D loss: 0.681488, acc.: 52.34%] [G loss: 0.855214]\n",
      "epoch:11 step:10499 [D loss: 0.690629, acc.: 57.03%] [G loss: 0.844800]\n",
      "epoch:11 step:10500 [D loss: 0.661258, acc.: 68.75%] [G loss: 0.817963]\n",
      "epoch:11 step:10501 [D loss: 0.629719, acc.: 62.50%] [G loss: 0.831565]\n",
      "epoch:11 step:10502 [D loss: 0.676810, acc.: 54.69%] [G loss: 0.771681]\n",
      "epoch:11 step:10503 [D loss: 0.693749, acc.: 46.88%] [G loss: 0.780021]\n",
      "epoch:11 step:10504 [D loss: 0.677837, acc.: 54.69%] [G loss: 0.780223]\n",
      "epoch:11 step:10505 [D loss: 0.676492, acc.: 55.47%] [G loss: 0.821699]\n",
      "epoch:11 step:10506 [D loss: 0.670051, acc.: 58.59%] [G loss: 0.799748]\n",
      "epoch:11 step:10507 [D loss: 0.611092, acc.: 69.53%] [G loss: 0.768310]\n",
      "epoch:11 step:10508 [D loss: 0.691368, acc.: 52.34%] [G loss: 0.811099]\n",
      "epoch:11 step:10509 [D loss: 0.656045, acc.: 64.84%] [G loss: 0.820958]\n",
      "epoch:11 step:10510 [D loss: 0.667352, acc.: 60.94%] [G loss: 0.807332]\n",
      "epoch:11 step:10511 [D loss: 0.616472, acc.: 67.19%] [G loss: 0.869266]\n",
      "epoch:11 step:10512 [D loss: 0.705021, acc.: 56.25%] [G loss: 0.762564]\n",
      "epoch:11 step:10513 [D loss: 0.725618, acc.: 46.88%] [G loss: 0.837816]\n",
      "epoch:11 step:10514 [D loss: 0.639033, acc.: 59.38%] [G loss: 0.809356]\n",
      "epoch:11 step:10515 [D loss: 0.655267, acc.: 61.72%] [G loss: 0.796157]\n",
      "epoch:11 step:10516 [D loss: 0.672620, acc.: 58.59%] [G loss: 0.924496]\n",
      "epoch:11 step:10517 [D loss: 0.656966, acc.: 60.16%] [G loss: 0.873933]\n",
      "epoch:11 step:10518 [D loss: 0.684268, acc.: 53.91%] [G loss: 0.824022]\n",
      "epoch:11 step:10519 [D loss: 0.662263, acc.: 64.06%] [G loss: 0.828059]\n",
      "epoch:11 step:10520 [D loss: 0.684238, acc.: 51.56%] [G loss: 0.879233]\n",
      "epoch:11 step:10521 [D loss: 0.719255, acc.: 50.78%] [G loss: 0.702526]\n",
      "epoch:11 step:10522 [D loss: 0.711970, acc.: 48.44%] [G loss: 0.862307]\n",
      "epoch:11 step:10523 [D loss: 0.683347, acc.: 53.12%] [G loss: 0.785003]\n",
      "epoch:11 step:10524 [D loss: 0.712933, acc.: 47.66%] [G loss: 0.782967]\n",
      "epoch:11 step:10525 [D loss: 0.726835, acc.: 46.09%] [G loss: 0.751993]\n",
      "epoch:11 step:10526 [D loss: 0.691234, acc.: 54.69%] [G loss: 0.838047]\n",
      "epoch:11 step:10527 [D loss: 0.676843, acc.: 53.12%] [G loss: 0.810350]\n",
      "epoch:11 step:10528 [D loss: 0.689467, acc.: 50.00%] [G loss: 0.832339]\n",
      "epoch:11 step:10529 [D loss: 0.692023, acc.: 50.78%] [G loss: 0.773396]\n",
      "epoch:11 step:10530 [D loss: 0.669543, acc.: 59.38%] [G loss: 0.770526]\n",
      "epoch:11 step:10531 [D loss: 0.651849, acc.: 64.06%] [G loss: 0.859376]\n",
      "epoch:11 step:10532 [D loss: 0.649839, acc.: 67.19%] [G loss: 0.811454]\n",
      "epoch:11 step:10533 [D loss: 0.679485, acc.: 56.25%] [G loss: 0.767127]\n",
      "epoch:11 step:10534 [D loss: 0.643169, acc.: 59.38%] [G loss: 0.770173]\n",
      "epoch:11 step:10535 [D loss: 0.705887, acc.: 50.78%] [G loss: 0.717948]\n",
      "epoch:11 step:10536 [D loss: 0.674540, acc.: 60.16%] [G loss: 0.767794]\n",
      "epoch:11 step:10537 [D loss: 0.680782, acc.: 58.59%] [G loss: 0.791641]\n",
      "epoch:11 step:10538 [D loss: 0.669942, acc.: 61.72%] [G loss: 0.801963]\n",
      "epoch:11 step:10539 [D loss: 0.721811, acc.: 53.12%] [G loss: 0.764734]\n",
      "epoch:11 step:10540 [D loss: 0.690193, acc.: 53.91%] [G loss: 0.820241]\n",
      "epoch:11 step:10541 [D loss: 0.691010, acc.: 57.03%] [G loss: 0.793442]\n",
      "epoch:11 step:10542 [D loss: 0.662936, acc.: 60.94%] [G loss: 0.830504]\n",
      "epoch:11 step:10543 [D loss: 0.679765, acc.: 52.34%] [G loss: 0.898658]\n",
      "epoch:11 step:10544 [D loss: 0.667091, acc.: 58.59%] [G loss: 0.829541]\n",
      "epoch:11 step:10545 [D loss: 0.680053, acc.: 58.59%] [G loss: 0.886704]\n",
      "epoch:11 step:10546 [D loss: 0.708929, acc.: 42.97%] [G loss: 0.830526]\n",
      "epoch:11 step:10547 [D loss: 0.681944, acc.: 56.25%] [G loss: 0.842321]\n",
      "epoch:11 step:10548 [D loss: 0.700161, acc.: 55.47%] [G loss: 0.808386]\n",
      "epoch:11 step:10549 [D loss: 0.637464, acc.: 68.75%] [G loss: 0.829555]\n",
      "epoch:11 step:10550 [D loss: 0.674816, acc.: 53.91%] [G loss: 0.827686]\n",
      "epoch:11 step:10551 [D loss: 0.666286, acc.: 57.81%] [G loss: 0.834544]\n",
      "epoch:11 step:10552 [D loss: 0.728930, acc.: 39.06%] [G loss: 0.798709]\n",
      "epoch:11 step:10553 [D loss: 0.665216, acc.: 58.59%] [G loss: 0.829469]\n",
      "epoch:11 step:10554 [D loss: 0.650875, acc.: 57.81%] [G loss: 0.795128]\n",
      "epoch:11 step:10555 [D loss: 0.645006, acc.: 62.50%] [G loss: 0.789088]\n",
      "epoch:11 step:10556 [D loss: 0.656851, acc.: 60.94%] [G loss: 0.810387]\n",
      "epoch:11 step:10557 [D loss: 0.671773, acc.: 56.25%] [G loss: 0.929166]\n",
      "epoch:11 step:10558 [D loss: 0.632449, acc.: 64.06%] [G loss: 0.803890]\n",
      "epoch:11 step:10559 [D loss: 0.650003, acc.: 62.50%] [G loss: 0.783781]\n",
      "epoch:11 step:10560 [D loss: 0.697164, acc.: 47.66%] [G loss: 0.818154]\n",
      "epoch:11 step:10561 [D loss: 0.678902, acc.: 53.91%] [G loss: 0.813274]\n",
      "epoch:11 step:10562 [D loss: 0.686815, acc.: 53.12%] [G loss: 0.792530]\n",
      "epoch:11 step:10563 [D loss: 0.645110, acc.: 67.19%] [G loss: 0.828771]\n",
      "epoch:11 step:10564 [D loss: 0.663018, acc.: 58.59%] [G loss: 0.795406]\n",
      "epoch:11 step:10565 [D loss: 0.649020, acc.: 63.28%] [G loss: 0.822782]\n",
      "epoch:11 step:10566 [D loss: 0.669970, acc.: 58.59%] [G loss: 0.880602]\n",
      "epoch:11 step:10567 [D loss: 0.710065, acc.: 49.22%] [G loss: 0.763960]\n",
      "epoch:11 step:10568 [D loss: 0.665966, acc.: 53.91%] [G loss: 0.772339]\n",
      "epoch:11 step:10569 [D loss: 0.658072, acc.: 60.16%] [G loss: 0.801877]\n",
      "epoch:11 step:10570 [D loss: 0.659244, acc.: 52.34%] [G loss: 0.834949]\n",
      "epoch:11 step:10571 [D loss: 0.717866, acc.: 47.66%] [G loss: 0.878217]\n",
      "epoch:11 step:10572 [D loss: 0.667079, acc.: 57.81%] [G loss: 0.838586]\n",
      "epoch:11 step:10573 [D loss: 0.678061, acc.: 54.69%] [G loss: 0.889497]\n",
      "epoch:11 step:10574 [D loss: 0.693781, acc.: 53.12%] [G loss: 0.876451]\n",
      "epoch:11 step:10575 [D loss: 0.680752, acc.: 53.12%] [G loss: 0.858686]\n",
      "epoch:11 step:10576 [D loss: 0.712565, acc.: 50.78%] [G loss: 0.760608]\n",
      "epoch:11 step:10577 [D loss: 0.623919, acc.: 62.50%] [G loss: 0.853562]\n",
      "epoch:11 step:10578 [D loss: 0.646854, acc.: 56.25%] [G loss: 0.807530]\n",
      "epoch:11 step:10579 [D loss: 0.613273, acc.: 65.62%] [G loss: 0.874694]\n",
      "epoch:11 step:10580 [D loss: 0.639877, acc.: 62.50%] [G loss: 0.845244]\n",
      "epoch:11 step:10581 [D loss: 0.632028, acc.: 63.28%] [G loss: 0.979769]\n",
      "epoch:11 step:10582 [D loss: 0.622709, acc.: 66.41%] [G loss: 0.883944]\n",
      "epoch:11 step:10583 [D loss: 0.685934, acc.: 54.69%] [G loss: 0.837607]\n",
      "epoch:11 step:10584 [D loss: 0.666303, acc.: 57.03%] [G loss: 0.883105]\n",
      "epoch:11 step:10585 [D loss: 0.699477, acc.: 50.00%] [G loss: 0.875781]\n",
      "epoch:11 step:10586 [D loss: 0.693671, acc.: 49.22%] [G loss: 0.926874]\n",
      "epoch:11 step:10587 [D loss: 0.703042, acc.: 51.56%] [G loss: 0.821757]\n",
      "epoch:11 step:10588 [D loss: 0.669304, acc.: 56.25%] [G loss: 0.863837]\n",
      "epoch:11 step:10589 [D loss: 0.676562, acc.: 54.69%] [G loss: 0.907265]\n",
      "epoch:11 step:10590 [D loss: 0.666578, acc.: 60.16%] [G loss: 0.868574]\n",
      "epoch:11 step:10591 [D loss: 0.693023, acc.: 54.69%] [G loss: 0.873995]\n",
      "epoch:11 step:10592 [D loss: 0.668273, acc.: 64.06%] [G loss: 0.807598]\n",
      "epoch:11 step:10593 [D loss: 0.658494, acc.: 57.81%] [G loss: 0.860070]\n",
      "epoch:11 step:10594 [D loss: 0.674634, acc.: 52.34%] [G loss: 0.841902]\n",
      "epoch:11 step:10595 [D loss: 0.707470, acc.: 50.78%] [G loss: 0.834006]\n",
      "epoch:11 step:10596 [D loss: 0.694572, acc.: 54.69%] [G loss: 0.913189]\n",
      "epoch:11 step:10597 [D loss: 0.647691, acc.: 63.28%] [G loss: 0.804066]\n",
      "epoch:11 step:10598 [D loss: 0.716876, acc.: 57.03%] [G loss: 0.836779]\n",
      "epoch:11 step:10599 [D loss: 0.643204, acc.: 64.84%] [G loss: 0.879701]\n",
      "epoch:11 step:10600 [D loss: 0.694423, acc.: 53.91%] [G loss: 0.913936]\n",
      "epoch:11 step:10601 [D loss: 0.680170, acc.: 53.12%] [G loss: 0.829036]\n",
      "epoch:11 step:10602 [D loss: 0.700568, acc.: 55.47%] [G loss: 0.914901]\n",
      "epoch:11 step:10603 [D loss: 0.655560, acc.: 63.28%] [G loss: 0.821910]\n",
      "epoch:11 step:10604 [D loss: 0.672607, acc.: 60.16%] [G loss: 0.897867]\n",
      "epoch:11 step:10605 [D loss: 0.667057, acc.: 55.47%] [G loss: 0.771661]\n",
      "epoch:11 step:10606 [D loss: 0.716715, acc.: 47.66%] [G loss: 0.816659]\n",
      "epoch:11 step:10607 [D loss: 0.677180, acc.: 56.25%] [G loss: 0.837964]\n",
      "epoch:11 step:10608 [D loss: 0.692759, acc.: 56.25%] [G loss: 0.798091]\n",
      "epoch:11 step:10609 [D loss: 0.703235, acc.: 53.12%] [G loss: 0.848450]\n",
      "epoch:11 step:10610 [D loss: 0.678474, acc.: 59.38%] [G loss: 0.861301]\n",
      "epoch:11 step:10611 [D loss: 0.733712, acc.: 44.53%] [G loss: 0.848857]\n",
      "epoch:11 step:10612 [D loss: 0.669761, acc.: 54.69%] [G loss: 0.839224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10613 [D loss: 0.649134, acc.: 57.81%] [G loss: 0.895548]\n",
      "epoch:11 step:10614 [D loss: 0.623989, acc.: 61.72%] [G loss: 0.910101]\n",
      "epoch:11 step:10615 [D loss: 0.674384, acc.: 56.25%] [G loss: 0.793034]\n",
      "epoch:11 step:10616 [D loss: 0.653112, acc.: 59.38%] [G loss: 0.861699]\n",
      "epoch:11 step:10617 [D loss: 0.671285, acc.: 58.59%] [G loss: 0.762070]\n",
      "epoch:11 step:10618 [D loss: 0.631286, acc.: 64.84%] [G loss: 0.788118]\n",
      "epoch:11 step:10619 [D loss: 0.695158, acc.: 51.56%] [G loss: 0.836865]\n",
      "epoch:11 step:10620 [D loss: 0.693181, acc.: 53.91%] [G loss: 0.935165]\n",
      "epoch:11 step:10621 [D loss: 0.701531, acc.: 59.38%] [G loss: 0.863900]\n",
      "epoch:11 step:10622 [D loss: 0.622573, acc.: 64.84%] [G loss: 0.795701]\n",
      "epoch:11 step:10623 [D loss: 0.689934, acc.: 46.88%] [G loss: 0.809571]\n",
      "epoch:11 step:10624 [D loss: 0.656148, acc.: 65.62%] [G loss: 0.774568]\n",
      "epoch:11 step:10625 [D loss: 0.676029, acc.: 58.59%] [G loss: 0.850570]\n",
      "epoch:11 step:10626 [D loss: 0.686583, acc.: 50.78%] [G loss: 0.786651]\n",
      "epoch:11 step:10627 [D loss: 0.674482, acc.: 60.16%] [G loss: 0.812653]\n",
      "epoch:11 step:10628 [D loss: 0.702856, acc.: 50.78%] [G loss: 0.753175]\n",
      "epoch:11 step:10629 [D loss: 0.662651, acc.: 61.72%] [G loss: 0.762099]\n",
      "epoch:11 step:10630 [D loss: 0.670597, acc.: 57.03%] [G loss: 0.718393]\n",
      "epoch:11 step:10631 [D loss: 0.625433, acc.: 66.41%] [G loss: 0.735996]\n",
      "epoch:11 step:10632 [D loss: 0.655248, acc.: 56.25%] [G loss: 0.784992]\n",
      "epoch:11 step:10633 [D loss: 0.768718, acc.: 42.19%] [G loss: 0.743737]\n",
      "epoch:11 step:10634 [D loss: 0.665413, acc.: 57.03%] [G loss: 1.364596]\n",
      "epoch:11 step:10635 [D loss: 0.648013, acc.: 62.50%] [G loss: 0.685171]\n",
      "epoch:11 step:10636 [D loss: 0.676918, acc.: 56.25%] [G loss: 0.814427]\n",
      "epoch:11 step:10637 [D loss: 0.674344, acc.: 53.91%] [G loss: 0.784139]\n",
      "epoch:11 step:10638 [D loss: 0.678184, acc.: 60.16%] [G loss: 0.754867]\n",
      "epoch:11 step:10639 [D loss: 0.699157, acc.: 53.91%] [G loss: 0.780936]\n",
      "epoch:11 step:10640 [D loss: 0.734395, acc.: 51.56%] [G loss: 0.751908]\n",
      "epoch:11 step:10641 [D loss: 0.675736, acc.: 56.25%] [G loss: 0.779340]\n",
      "epoch:11 step:10642 [D loss: 0.663707, acc.: 55.47%] [G loss: 0.740688]\n",
      "epoch:11 step:10643 [D loss: 0.697575, acc.: 54.69%] [G loss: 0.798789]\n",
      "epoch:11 step:10644 [D loss: 0.698079, acc.: 49.22%] [G loss: 0.795560]\n",
      "epoch:11 step:10645 [D loss: 0.692453, acc.: 56.25%] [G loss: 0.923764]\n",
      "epoch:11 step:10646 [D loss: 0.668000, acc.: 55.47%] [G loss: 0.851950]\n",
      "epoch:11 step:10647 [D loss: 0.697635, acc.: 56.25%] [G loss: 0.850990]\n",
      "epoch:11 step:10648 [D loss: 0.698103, acc.: 56.25%] [G loss: 0.851678]\n",
      "epoch:11 step:10649 [D loss: 0.657662, acc.: 63.28%] [G loss: 0.868770]\n",
      "epoch:11 step:10650 [D loss: 0.687033, acc.: 53.12%] [G loss: 0.902241]\n",
      "epoch:11 step:10651 [D loss: 0.685257, acc.: 53.91%] [G loss: 0.791089]\n",
      "epoch:11 step:10652 [D loss: 0.645617, acc.: 62.50%] [G loss: 0.904930]\n",
      "epoch:11 step:10653 [D loss: 0.675445, acc.: 52.34%] [G loss: 0.870358]\n",
      "epoch:11 step:10654 [D loss: 0.721755, acc.: 44.53%] [G loss: 0.841684]\n",
      "epoch:11 step:10655 [D loss: 0.672206, acc.: 59.38%] [G loss: 0.861212]\n",
      "epoch:11 step:10656 [D loss: 0.677400, acc.: 55.47%] [G loss: 0.938536]\n",
      "epoch:11 step:10657 [D loss: 0.688014, acc.: 54.69%] [G loss: 0.801914]\n",
      "epoch:11 step:10658 [D loss: 0.665841, acc.: 59.38%] [G loss: 0.843297]\n",
      "epoch:11 step:10659 [D loss: 0.698438, acc.: 46.09%] [G loss: 0.749788]\n",
      "epoch:11 step:10660 [D loss: 0.646304, acc.: 61.72%] [G loss: 0.828927]\n",
      "epoch:11 step:10661 [D loss: 0.691752, acc.: 56.25%] [G loss: 0.828045]\n",
      "epoch:11 step:10662 [D loss: 0.645237, acc.: 63.28%] [G loss: 0.887596]\n",
      "epoch:11 step:10663 [D loss: 0.687220, acc.: 53.12%] [G loss: 0.846759]\n",
      "epoch:11 step:10664 [D loss: 0.695445, acc.: 53.12%] [G loss: 0.781109]\n",
      "epoch:11 step:10665 [D loss: 0.687463, acc.: 55.47%] [G loss: 0.842315]\n",
      "epoch:11 step:10666 [D loss: 0.720249, acc.: 50.00%] [G loss: 0.784023]\n",
      "epoch:11 step:10667 [D loss: 0.652055, acc.: 60.94%] [G loss: 0.826588]\n",
      "epoch:11 step:10668 [D loss: 0.687342, acc.: 56.25%] [G loss: 0.784193]\n",
      "epoch:11 step:10669 [D loss: 0.699585, acc.: 45.31%] [G loss: 0.816070]\n",
      "epoch:11 step:10670 [D loss: 0.635372, acc.: 64.06%] [G loss: 0.781724]\n",
      "epoch:11 step:10671 [D loss: 0.664012, acc.: 54.69%] [G loss: 0.815083]\n",
      "epoch:11 step:10672 [D loss: 0.664612, acc.: 64.06%] [G loss: 0.713606]\n",
      "epoch:11 step:10673 [D loss: 0.691482, acc.: 50.00%] [G loss: 0.762400]\n",
      "epoch:11 step:10674 [D loss: 0.670923, acc.: 60.94%] [G loss: 0.787926]\n",
      "epoch:11 step:10675 [D loss: 0.701079, acc.: 48.44%] [G loss: 0.755430]\n",
      "epoch:11 step:10676 [D loss: 0.675088, acc.: 60.94%] [G loss: 0.790750]\n",
      "epoch:11 step:10677 [D loss: 0.664654, acc.: 54.69%] [G loss: 0.771882]\n",
      "epoch:11 step:10678 [D loss: 0.667016, acc.: 60.16%] [G loss: 0.785782]\n",
      "epoch:11 step:10679 [D loss: 0.665444, acc.: 59.38%] [G loss: 0.755291]\n",
      "epoch:11 step:10680 [D loss: 0.706878, acc.: 49.22%] [G loss: 0.763153]\n",
      "epoch:11 step:10681 [D loss: 0.706134, acc.: 51.56%] [G loss: 0.821600]\n",
      "epoch:11 step:10682 [D loss: 0.676207, acc.: 61.72%] [G loss: 0.800765]\n",
      "epoch:11 step:10683 [D loss: 0.648397, acc.: 65.62%] [G loss: 0.875610]\n",
      "epoch:11 step:10684 [D loss: 0.646596, acc.: 64.84%] [G loss: 0.814484]\n",
      "epoch:11 step:10685 [D loss: 0.647414, acc.: 61.72%] [G loss: 0.795901]\n",
      "epoch:11 step:10686 [D loss: 0.709884, acc.: 55.47%] [G loss: 0.837594]\n",
      "epoch:11 step:10687 [D loss: 0.679747, acc.: 48.44%] [G loss: 0.747931]\n",
      "epoch:11 step:10688 [D loss: 0.657211, acc.: 58.59%] [G loss: 0.830306]\n",
      "epoch:11 step:10689 [D loss: 0.648482, acc.: 64.06%] [G loss: 0.830415]\n",
      "epoch:11 step:10690 [D loss: 0.684229, acc.: 57.81%] [G loss: 0.718480]\n",
      "epoch:11 step:10691 [D loss: 0.691477, acc.: 51.56%] [G loss: 0.869799]\n",
      "epoch:11 step:10692 [D loss: 0.656965, acc.: 62.50%] [G loss: 0.750540]\n",
      "epoch:11 step:10693 [D loss: 0.670379, acc.: 60.94%] [G loss: 0.878068]\n",
      "epoch:11 step:10694 [D loss: 0.727342, acc.: 56.25%] [G loss: 0.736336]\n",
      "epoch:11 step:10695 [D loss: 0.675976, acc.: 56.25%] [G loss: 0.749862]\n",
      "epoch:11 step:10696 [D loss: 0.671066, acc.: 62.50%] [G loss: 0.766075]\n",
      "epoch:11 step:10697 [D loss: 0.722372, acc.: 44.53%] [G loss: 0.760579]\n",
      "epoch:11 step:10698 [D loss: 0.659758, acc.: 57.81%] [G loss: 0.775695]\n",
      "epoch:11 step:10699 [D loss: 0.679955, acc.: 49.22%] [G loss: 0.833358]\n",
      "epoch:11 step:10700 [D loss: 0.671312, acc.: 57.81%] [G loss: 0.781282]\n",
      "epoch:11 step:10701 [D loss: 0.678827, acc.: 55.47%] [G loss: 0.761263]\n",
      "epoch:11 step:10702 [D loss: 0.702425, acc.: 47.66%] [G loss: 0.799686]\n",
      "epoch:11 step:10703 [D loss: 0.685146, acc.: 54.69%] [G loss: 0.792161]\n",
      "epoch:11 step:10704 [D loss: 0.710232, acc.: 49.22%] [G loss: 0.825682]\n",
      "epoch:11 step:10705 [D loss: 0.673520, acc.: 57.81%] [G loss: 0.786967]\n",
      "epoch:11 step:10706 [D loss: 0.669660, acc.: 53.91%] [G loss: 0.812830]\n",
      "epoch:11 step:10707 [D loss: 0.655943, acc.: 55.47%] [G loss: 0.863009]\n",
      "epoch:11 step:10708 [D loss: 0.657729, acc.: 56.25%] [G loss: 0.869518]\n",
      "epoch:11 step:10709 [D loss: 0.690779, acc.: 61.72%] [G loss: 0.794157]\n",
      "epoch:11 step:10710 [D loss: 0.642315, acc.: 58.59%] [G loss: 0.809797]\n",
      "epoch:11 step:10711 [D loss: 0.655422, acc.: 60.16%] [G loss: 0.806178]\n",
      "epoch:11 step:10712 [D loss: 0.669750, acc.: 55.47%] [G loss: 0.882997]\n",
      "epoch:11 step:10713 [D loss: 0.679413, acc.: 53.12%] [G loss: 0.830235]\n",
      "epoch:11 step:10714 [D loss: 0.677271, acc.: 60.16%] [G loss: 0.819388]\n",
      "epoch:11 step:10715 [D loss: 0.690979, acc.: 50.00%] [G loss: 0.823386]\n",
      "epoch:11 step:10716 [D loss: 0.680718, acc.: 60.16%] [G loss: 0.826727]\n",
      "epoch:11 step:10717 [D loss: 0.674130, acc.: 57.03%] [G loss: 0.834701]\n",
      "epoch:11 step:10718 [D loss: 0.663880, acc.: 63.28%] [G loss: 0.868933]\n",
      "epoch:11 step:10719 [D loss: 0.705006, acc.: 51.56%] [G loss: 0.837916]\n",
      "epoch:11 step:10720 [D loss: 0.688707, acc.: 56.25%] [G loss: 0.799538]\n",
      "epoch:11 step:10721 [D loss: 0.618044, acc.: 66.41%] [G loss: 0.854488]\n",
      "epoch:11 step:10722 [D loss: 0.655665, acc.: 58.59%] [G loss: 0.799636]\n",
      "epoch:11 step:10723 [D loss: 0.687807, acc.: 54.69%] [G loss: 0.821015]\n",
      "epoch:11 step:10724 [D loss: 0.666217, acc.: 57.81%] [G loss: 0.825553]\n",
      "epoch:11 step:10725 [D loss: 0.689521, acc.: 57.03%] [G loss: 0.712105]\n",
      "epoch:11 step:10726 [D loss: 0.696512, acc.: 50.00%] [G loss: 0.754945]\n",
      "epoch:11 step:10727 [D loss: 0.656662, acc.: 61.72%] [G loss: 0.772870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10728 [D loss: 0.688364, acc.: 50.00%] [G loss: 0.776017]\n",
      "epoch:11 step:10729 [D loss: 0.665834, acc.: 52.34%] [G loss: 0.778931]\n",
      "epoch:11 step:10730 [D loss: 0.683235, acc.: 54.69%] [G loss: 0.820593]\n",
      "epoch:11 step:10731 [D loss: 0.677357, acc.: 61.72%] [G loss: 0.811165]\n",
      "epoch:11 step:10732 [D loss: 0.680913, acc.: 53.91%] [G loss: 0.878721]\n",
      "epoch:11 step:10733 [D loss: 0.679250, acc.: 56.25%] [G loss: 0.825832]\n",
      "epoch:11 step:10734 [D loss: 0.661519, acc.: 64.84%] [G loss: 0.853521]\n",
      "epoch:11 step:10735 [D loss: 0.672093, acc.: 55.47%] [G loss: 0.953205]\n",
      "epoch:11 step:10736 [D loss: 0.676957, acc.: 56.25%] [G loss: 0.764118]\n",
      "epoch:11 step:10737 [D loss: 0.658356, acc.: 61.72%] [G loss: 0.806670]\n",
      "epoch:11 step:10738 [D loss: 0.695256, acc.: 51.56%] [G loss: 0.851959]\n",
      "epoch:11 step:10739 [D loss: 0.697443, acc.: 45.31%] [G loss: 0.768374]\n",
      "epoch:11 step:10740 [D loss: 0.721489, acc.: 51.56%] [G loss: 0.848358]\n",
      "epoch:11 step:10741 [D loss: 0.698158, acc.: 52.34%] [G loss: 0.838392]\n",
      "epoch:11 step:10742 [D loss: 0.689005, acc.: 53.91%] [G loss: 0.720718]\n",
      "epoch:11 step:10743 [D loss: 0.678381, acc.: 59.38%] [G loss: 0.756821]\n",
      "epoch:11 step:10744 [D loss: 0.683725, acc.: 51.56%] [G loss: 0.793455]\n",
      "epoch:11 step:10745 [D loss: 0.665248, acc.: 54.69%] [G loss: 0.791070]\n",
      "epoch:11 step:10746 [D loss: 0.633819, acc.: 66.41%] [G loss: 0.805404]\n",
      "epoch:11 step:10747 [D loss: 0.674716, acc.: 55.47%] [G loss: 0.819231]\n",
      "epoch:11 step:10748 [D loss: 0.676032, acc.: 55.47%] [G loss: 0.826272]\n",
      "epoch:11 step:10749 [D loss: 0.667803, acc.: 56.25%] [G loss: 0.840542]\n",
      "epoch:11 step:10750 [D loss: 0.692991, acc.: 52.34%] [G loss: 0.828020]\n",
      "epoch:11 step:10751 [D loss: 0.681271, acc.: 51.56%] [G loss: 0.971137]\n",
      "epoch:11 step:10752 [D loss: 0.665601, acc.: 55.47%] [G loss: 0.835592]\n",
      "epoch:11 step:10753 [D loss: 0.671589, acc.: 59.38%] [G loss: 0.895963]\n",
      "epoch:11 step:10754 [D loss: 0.657709, acc.: 63.28%] [G loss: 0.838180]\n",
      "epoch:11 step:10755 [D loss: 0.689428, acc.: 57.03%] [G loss: 0.849173]\n",
      "epoch:11 step:10756 [D loss: 0.691011, acc.: 56.25%] [G loss: 0.838741]\n",
      "epoch:11 step:10757 [D loss: 0.691668, acc.: 54.69%] [G loss: 0.840314]\n",
      "epoch:11 step:10758 [D loss: 0.680165, acc.: 54.69%] [G loss: 0.793269]\n",
      "epoch:11 step:10759 [D loss: 0.689333, acc.: 57.81%] [G loss: 0.844022]\n",
      "epoch:11 step:10760 [D loss: 0.700386, acc.: 53.91%] [G loss: 0.812188]\n",
      "epoch:11 step:10761 [D loss: 0.702391, acc.: 52.34%] [G loss: 0.911055]\n",
      "epoch:11 step:10762 [D loss: 0.659293, acc.: 55.47%] [G loss: 0.763431]\n",
      "epoch:11 step:10763 [D loss: 0.696677, acc.: 50.78%] [G loss: 0.767719]\n",
      "epoch:11 step:10764 [D loss: 0.640753, acc.: 65.62%] [G loss: 0.783892]\n",
      "epoch:11 step:10765 [D loss: 0.671511, acc.: 54.69%] [G loss: 0.844386]\n",
      "epoch:11 step:10766 [D loss: 0.691170, acc.: 53.12%] [G loss: 0.841204]\n",
      "epoch:11 step:10767 [D loss: 0.663415, acc.: 56.25%] [G loss: 0.792732]\n",
      "epoch:11 step:10768 [D loss: 0.661072, acc.: 56.25%] [G loss: 0.812016]\n",
      "epoch:11 step:10769 [D loss: 0.682460, acc.: 54.69%] [G loss: 0.856724]\n",
      "epoch:11 step:10770 [D loss: 0.665197, acc.: 56.25%] [G loss: 0.840546]\n",
      "epoch:11 step:10771 [D loss: 0.708296, acc.: 48.44%] [G loss: 0.835685]\n",
      "epoch:11 step:10772 [D loss: 0.684081, acc.: 54.69%] [G loss: 0.773964]\n",
      "epoch:11 step:10773 [D loss: 0.675222, acc.: 55.47%] [G loss: 0.845417]\n",
      "epoch:11 step:10774 [D loss: 0.664727, acc.: 57.81%] [G loss: 0.866753]\n",
      "epoch:11 step:10775 [D loss: 0.674387, acc.: 54.69%] [G loss: 0.816854]\n",
      "epoch:11 step:10776 [D loss: 0.636430, acc.: 59.38%] [G loss: 0.826248]\n",
      "epoch:11 step:10777 [D loss: 0.700931, acc.: 49.22%] [G loss: 0.793277]\n",
      "epoch:11 step:10778 [D loss: 0.679370, acc.: 56.25%] [G loss: 0.772732]\n",
      "epoch:11 step:10779 [D loss: 0.675363, acc.: 57.03%] [G loss: 0.769668]\n",
      "epoch:11 step:10780 [D loss: 0.641520, acc.: 62.50%] [G loss: 0.852975]\n",
      "epoch:11 step:10781 [D loss: 0.636867, acc.: 64.84%] [G loss: 0.872481]\n",
      "epoch:11 step:10782 [D loss: 0.685652, acc.: 50.78%] [G loss: 0.785606]\n",
      "epoch:11 step:10783 [D loss: 0.695886, acc.: 52.34%] [G loss: 0.818255]\n",
      "epoch:11 step:10784 [D loss: 0.686766, acc.: 53.12%] [G loss: 0.877272]\n",
      "epoch:11 step:10785 [D loss: 0.642687, acc.: 63.28%] [G loss: 0.855281]\n",
      "epoch:11 step:10786 [D loss: 0.693780, acc.: 57.03%] [G loss: 1.010856]\n",
      "epoch:11 step:10787 [D loss: 0.710451, acc.: 47.66%] [G loss: 0.810253]\n",
      "epoch:11 step:10788 [D loss: 0.677043, acc.: 53.91%] [G loss: 0.817612]\n",
      "epoch:11 step:10789 [D loss: 0.632110, acc.: 65.62%] [G loss: 0.802944]\n",
      "epoch:11 step:10790 [D loss: 0.681643, acc.: 58.59%] [G loss: 0.801837]\n",
      "epoch:11 step:10791 [D loss: 0.705734, acc.: 57.81%] [G loss: 0.818338]\n",
      "epoch:11 step:10792 [D loss: 0.651683, acc.: 60.94%] [G loss: 0.848251]\n",
      "epoch:11 step:10793 [D loss: 0.671971, acc.: 53.12%] [G loss: 0.856791]\n",
      "epoch:11 step:10794 [D loss: 0.664491, acc.: 61.72%] [G loss: 0.798060]\n",
      "epoch:11 step:10795 [D loss: 0.651024, acc.: 67.19%] [G loss: 0.769938]\n",
      "epoch:11 step:10796 [D loss: 0.697531, acc.: 51.56%] [G loss: 0.802200]\n",
      "epoch:11 step:10797 [D loss: 0.686162, acc.: 51.56%] [G loss: 0.873573]\n",
      "epoch:11 step:10798 [D loss: 0.718712, acc.: 41.41%] [G loss: 0.803777]\n",
      "epoch:11 step:10799 [D loss: 0.672418, acc.: 53.12%] [G loss: 0.875571]\n",
      "epoch:11 step:10800 [D loss: 0.650852, acc.: 62.50%] [G loss: 0.852872]\n",
      "epoch:11 step:10801 [D loss: 0.681833, acc.: 57.03%] [G loss: 0.843500]\n",
      "epoch:11 step:10802 [D loss: 0.704348, acc.: 53.12%] [G loss: 0.793170]\n",
      "epoch:11 step:10803 [D loss: 0.634874, acc.: 64.84%] [G loss: 0.811674]\n",
      "epoch:11 step:10804 [D loss: 0.734061, acc.: 44.53%] [G loss: 0.804990]\n",
      "epoch:11 step:10805 [D loss: 0.683872, acc.: 57.03%] [G loss: 0.820866]\n",
      "epoch:11 step:10806 [D loss: 0.680787, acc.: 57.81%] [G loss: 0.893322]\n",
      "epoch:11 step:10807 [D loss: 0.693980, acc.: 50.00%] [G loss: 0.841221]\n",
      "epoch:11 step:10808 [D loss: 0.678200, acc.: 53.91%] [G loss: 0.879395]\n",
      "epoch:11 step:10809 [D loss: 0.664840, acc.: 57.03%] [G loss: 0.846962]\n",
      "epoch:11 step:10810 [D loss: 0.679094, acc.: 52.34%] [G loss: 0.897124]\n",
      "epoch:11 step:10811 [D loss: 0.676549, acc.: 56.25%] [G loss: 0.921685]\n",
      "epoch:11 step:10812 [D loss: 0.668421, acc.: 56.25%] [G loss: 0.830772]\n",
      "epoch:11 step:10813 [D loss: 0.672888, acc.: 55.47%] [G loss: 0.805678]\n",
      "epoch:11 step:10814 [D loss: 0.735505, acc.: 42.97%] [G loss: 0.767512]\n",
      "epoch:11 step:10815 [D loss: 0.645775, acc.: 58.59%] [G loss: 0.813470]\n",
      "epoch:11 step:10816 [D loss: 0.690659, acc.: 54.69%] [G loss: 0.805794]\n",
      "epoch:11 step:10817 [D loss: 0.666242, acc.: 58.59%] [G loss: 0.793421]\n",
      "epoch:11 step:10818 [D loss: 0.693579, acc.: 54.69%] [G loss: 0.766823]\n",
      "epoch:11 step:10819 [D loss: 0.670011, acc.: 54.69%] [G loss: 0.811848]\n",
      "epoch:11 step:10820 [D loss: 0.666762, acc.: 56.25%] [G loss: 0.820912]\n",
      "epoch:11 step:10821 [D loss: 0.690361, acc.: 53.12%] [G loss: 0.800882]\n",
      "epoch:11 step:10822 [D loss: 0.688470, acc.: 55.47%] [G loss: 0.795805]\n",
      "epoch:11 step:10823 [D loss: 0.629206, acc.: 64.06%] [G loss: 0.830208]\n",
      "epoch:11 step:10824 [D loss: 0.654000, acc.: 60.16%] [G loss: 0.823801]\n",
      "epoch:11 step:10825 [D loss: 0.658818, acc.: 60.94%] [G loss: 0.774543]\n",
      "epoch:11 step:10826 [D loss: 0.666375, acc.: 54.69%] [G loss: 0.859777]\n",
      "epoch:11 step:10827 [D loss: 0.675904, acc.: 60.16%] [G loss: 0.781481]\n",
      "epoch:11 step:10828 [D loss: 0.657877, acc.: 57.03%] [G loss: 0.828624]\n",
      "epoch:11 step:10829 [D loss: 0.678493, acc.: 55.47%] [G loss: 0.784990]\n",
      "epoch:11 step:10830 [D loss: 0.672022, acc.: 57.03%] [G loss: 0.930931]\n",
      "epoch:11 step:10831 [D loss: 0.680697, acc.: 56.25%] [G loss: 0.839950]\n",
      "epoch:11 step:10832 [D loss: 0.679239, acc.: 53.12%] [G loss: 0.786509]\n",
      "epoch:11 step:10833 [D loss: 0.682395, acc.: 55.47%] [G loss: 0.866357]\n",
      "epoch:11 step:10834 [D loss: 0.653736, acc.: 60.94%] [G loss: 0.771408]\n",
      "epoch:11 step:10835 [D loss: 0.680123, acc.: 53.12%] [G loss: 0.840233]\n",
      "epoch:11 step:10836 [D loss: 0.671740, acc.: 57.81%] [G loss: 0.880637]\n",
      "epoch:11 step:10837 [D loss: 0.669667, acc.: 61.72%] [G loss: 0.819772]\n",
      "epoch:11 step:10838 [D loss: 0.704665, acc.: 53.91%] [G loss: 0.792900]\n",
      "epoch:11 step:10839 [D loss: 0.640797, acc.: 64.06%] [G loss: 0.921448]\n",
      "epoch:11 step:10840 [D loss: 0.669437, acc.: 56.25%] [G loss: 0.813064]\n",
      "epoch:11 step:10841 [D loss: 0.677928, acc.: 58.59%] [G loss: 0.788904]\n",
      "epoch:11 step:10842 [D loss: 0.650845, acc.: 61.72%] [G loss: 0.826162]\n",
      "epoch:11 step:10843 [D loss: 0.661671, acc.: 64.06%] [G loss: 0.792711]\n",
      "epoch:11 step:10844 [D loss: 0.694477, acc.: 55.47%] [G loss: 0.784494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10845 [D loss: 0.677553, acc.: 59.38%] [G loss: 0.808671]\n",
      "epoch:11 step:10846 [D loss: 0.649114, acc.: 60.94%] [G loss: 0.747862]\n",
      "epoch:11 step:10847 [D loss: 0.629622, acc.: 70.31%] [G loss: 0.835284]\n",
      "epoch:11 step:10848 [D loss: 0.692032, acc.: 54.69%] [G loss: 0.821699]\n",
      "epoch:11 step:10849 [D loss: 0.738994, acc.: 39.84%] [G loss: 0.847902]\n",
      "epoch:11 step:10850 [D loss: 0.705014, acc.: 53.12%] [G loss: 0.772651]\n",
      "epoch:11 step:10851 [D loss: 0.693168, acc.: 58.59%] [G loss: 0.782892]\n",
      "epoch:11 step:10852 [D loss: 0.669990, acc.: 53.91%] [G loss: 0.803365]\n",
      "epoch:11 step:10853 [D loss: 0.687555, acc.: 50.00%] [G loss: 0.793220]\n",
      "epoch:11 step:10854 [D loss: 0.660524, acc.: 61.72%] [G loss: 0.824669]\n",
      "epoch:11 step:10855 [D loss: 0.698864, acc.: 47.66%] [G loss: 0.826486]\n",
      "epoch:11 step:10856 [D loss: 0.687583, acc.: 54.69%] [G loss: 0.789968]\n",
      "epoch:11 step:10857 [D loss: 0.720184, acc.: 47.66%] [G loss: 0.787940]\n",
      "epoch:11 step:10858 [D loss: 0.726184, acc.: 42.19%] [G loss: 0.793580]\n",
      "epoch:11 step:10859 [D loss: 0.671631, acc.: 58.59%] [G loss: 0.766594]\n",
      "epoch:11 step:10860 [D loss: 0.697700, acc.: 53.12%] [G loss: 0.809967]\n",
      "epoch:11 step:10861 [D loss: 0.691527, acc.: 57.81%] [G loss: 0.797208]\n",
      "epoch:11 step:10862 [D loss: 0.686972, acc.: 58.59%] [G loss: 0.820700]\n",
      "epoch:11 step:10863 [D loss: 0.682083, acc.: 56.25%] [G loss: 0.784070]\n",
      "epoch:11 step:10864 [D loss: 0.685712, acc.: 56.25%] [G loss: 0.749338]\n",
      "epoch:11 step:10865 [D loss: 0.680599, acc.: 55.47%] [G loss: 0.832779]\n",
      "epoch:11 step:10866 [D loss: 0.672200, acc.: 56.25%] [G loss: 0.760900]\n",
      "epoch:11 step:10867 [D loss: 0.675840, acc.: 53.91%] [G loss: 0.843116]\n",
      "epoch:11 step:10868 [D loss: 0.667495, acc.: 57.03%] [G loss: 0.791480]\n",
      "epoch:11 step:10869 [D loss: 0.653831, acc.: 57.81%] [G loss: 0.766691]\n",
      "epoch:11 step:10870 [D loss: 0.672279, acc.: 58.59%] [G loss: 0.835111]\n",
      "epoch:11 step:10871 [D loss: 0.696179, acc.: 49.22%] [G loss: 0.821746]\n",
      "epoch:11 step:10872 [D loss: 0.663249, acc.: 60.94%] [G loss: 0.748606]\n",
      "epoch:11 step:10873 [D loss: 0.667982, acc.: 57.03%] [G loss: 0.767642]\n",
      "epoch:11 step:10874 [D loss: 0.682535, acc.: 57.81%] [G loss: 0.784153]\n",
      "epoch:11 step:10875 [D loss: 0.678994, acc.: 59.38%] [G loss: 0.754349]\n",
      "epoch:11 step:10876 [D loss: 0.669011, acc.: 54.69%] [G loss: 0.751341]\n",
      "epoch:11 step:10877 [D loss: 0.707787, acc.: 47.66%] [G loss: 0.768305]\n",
      "epoch:11 step:10878 [D loss: 0.677302, acc.: 53.12%] [G loss: 0.759893]\n",
      "epoch:11 step:10879 [D loss: 0.698632, acc.: 48.44%] [G loss: 0.736580]\n",
      "epoch:11 step:10880 [D loss: 0.656296, acc.: 60.94%] [G loss: 0.787364]\n",
      "epoch:11 step:10881 [D loss: 0.702183, acc.: 53.91%] [G loss: 0.793586]\n",
      "epoch:11 step:10882 [D loss: 0.704519, acc.: 50.78%] [G loss: 0.821051]\n",
      "epoch:11 step:10883 [D loss: 0.658189, acc.: 64.06%] [G loss: 0.793291]\n",
      "epoch:11 step:10884 [D loss: 0.700219, acc.: 53.91%] [G loss: 0.732545]\n",
      "epoch:11 step:10885 [D loss: 0.652527, acc.: 62.50%] [G loss: 0.757829]\n",
      "epoch:11 step:10886 [D loss: 0.668137, acc.: 57.81%] [G loss: 0.762711]\n",
      "epoch:11 step:10887 [D loss: 0.680446, acc.: 57.81%] [G loss: 0.793247]\n",
      "epoch:11 step:10888 [D loss: 0.650572, acc.: 63.28%] [G loss: 0.809649]\n",
      "epoch:11 step:10889 [D loss: 0.681574, acc.: 55.47%] [G loss: 0.843749]\n",
      "epoch:11 step:10890 [D loss: 0.686173, acc.: 57.03%] [G loss: 0.779274]\n",
      "epoch:11 step:10891 [D loss: 0.666572, acc.: 58.59%] [G loss: 0.815924]\n",
      "epoch:11 step:10892 [D loss: 0.628842, acc.: 68.75%] [G loss: 0.802018]\n",
      "epoch:11 step:10893 [D loss: 0.687990, acc.: 55.47%] [G loss: 0.766306]\n",
      "epoch:11 step:10894 [D loss: 0.668513, acc.: 57.03%] [G loss: 0.788754]\n",
      "epoch:11 step:10895 [D loss: 0.676081, acc.: 53.91%] [G loss: 0.818850]\n",
      "epoch:11 step:10896 [D loss: 0.688914, acc.: 57.81%] [G loss: 0.803496]\n",
      "epoch:11 step:10897 [D loss: 0.693949, acc.: 51.56%] [G loss: 0.825081]\n",
      "epoch:11 step:10898 [D loss: 0.689437, acc.: 55.47%] [G loss: 0.831603]\n",
      "epoch:11 step:10899 [D loss: 0.664249, acc.: 62.50%] [G loss: 0.760780]\n",
      "epoch:11 step:10900 [D loss: 0.663490, acc.: 56.25%] [G loss: 0.819653]\n",
      "epoch:11 step:10901 [D loss: 0.677897, acc.: 59.38%] [G loss: 0.742663]\n",
      "epoch:11 step:10902 [D loss: 0.674994, acc.: 61.72%] [G loss: 0.812258]\n",
      "epoch:11 step:10903 [D loss: 0.653488, acc.: 64.06%] [G loss: 0.806070]\n",
      "epoch:11 step:10904 [D loss: 0.676533, acc.: 53.12%] [G loss: 0.787613]\n",
      "epoch:11 step:10905 [D loss: 0.656593, acc.: 61.72%] [G loss: 0.830004]\n",
      "epoch:11 step:10906 [D loss: 0.656134, acc.: 58.59%] [G loss: 0.860987]\n",
      "epoch:11 step:10907 [D loss: 0.705376, acc.: 46.88%] [G loss: 0.833006]\n",
      "epoch:11 step:10908 [D loss: 0.677494, acc.: 55.47%] [G loss: 0.833289]\n",
      "epoch:11 step:10909 [D loss: 0.663148, acc.: 58.59%] [G loss: 0.819399]\n",
      "epoch:11 step:10910 [D loss: 0.662119, acc.: 57.81%] [G loss: 0.814795]\n",
      "epoch:11 step:10911 [D loss: 0.662663, acc.: 61.72%] [G loss: 0.749212]\n",
      "epoch:11 step:10912 [D loss: 0.688228, acc.: 50.78%] [G loss: 0.907152]\n",
      "epoch:11 step:10913 [D loss: 0.688508, acc.: 58.59%] [G loss: 0.866823]\n",
      "epoch:11 step:10914 [D loss: 0.687452, acc.: 53.12%] [G loss: 0.790194]\n",
      "epoch:11 step:10915 [D loss: 0.687046, acc.: 55.47%] [G loss: 0.794565]\n",
      "epoch:11 step:10916 [D loss: 0.681710, acc.: 52.34%] [G loss: 0.947910]\n",
      "epoch:11 step:10917 [D loss: 0.676079, acc.: 50.00%] [G loss: 0.843985]\n",
      "epoch:11 step:10918 [D loss: 0.672149, acc.: 59.38%] [G loss: 0.934244]\n",
      "epoch:11 step:10919 [D loss: 0.682139, acc.: 52.34%] [G loss: 0.929960]\n",
      "epoch:11 step:10920 [D loss: 0.664192, acc.: 59.38%] [G loss: 0.875047]\n",
      "epoch:11 step:10921 [D loss: 0.681759, acc.: 50.78%] [G loss: 0.881387]\n",
      "epoch:11 step:10922 [D loss: 0.699042, acc.: 55.47%] [G loss: 0.798108]\n",
      "epoch:11 step:10923 [D loss: 0.695154, acc.: 53.91%] [G loss: 0.794875]\n",
      "epoch:11 step:10924 [D loss: 0.703704, acc.: 48.44%] [G loss: 0.846907]\n",
      "epoch:11 step:10925 [D loss: 0.716597, acc.: 43.75%] [G loss: 0.823095]\n",
      "epoch:11 step:10926 [D loss: 0.647035, acc.: 64.84%] [G loss: 0.808523]\n",
      "epoch:11 step:10927 [D loss: 0.653917, acc.: 62.50%] [G loss: 0.814925]\n",
      "epoch:11 step:10928 [D loss: 0.662180, acc.: 63.28%] [G loss: 0.751184]\n",
      "epoch:11 step:10929 [D loss: 0.660390, acc.: 55.47%] [G loss: 0.838182]\n",
      "epoch:11 step:10930 [D loss: 0.693679, acc.: 56.25%] [G loss: 0.838175]\n",
      "epoch:11 step:10931 [D loss: 0.691394, acc.: 52.34%] [G loss: 0.820577]\n",
      "epoch:11 step:10932 [D loss: 0.690753, acc.: 56.25%] [G loss: 0.781580]\n",
      "epoch:11 step:10933 [D loss: 0.700144, acc.: 52.34%] [G loss: 0.822723]\n",
      "epoch:11 step:10934 [D loss: 0.658147, acc.: 60.94%] [G loss: 0.791405]\n",
      "epoch:11 step:10935 [D loss: 0.674130, acc.: 57.03%] [G loss: 0.831040]\n",
      "epoch:11 step:10936 [D loss: 0.666357, acc.: 54.69%] [G loss: 0.799886]\n",
      "epoch:11 step:10937 [D loss: 0.694420, acc.: 49.22%] [G loss: 0.853099]\n",
      "epoch:11 step:10938 [D loss: 0.682535, acc.: 48.44%] [G loss: 0.787304]\n",
      "epoch:11 step:10939 [D loss: 0.667604, acc.: 59.38%] [G loss: 0.821197]\n",
      "epoch:11 step:10940 [D loss: 0.706376, acc.: 56.25%] [G loss: 0.765099]\n",
      "epoch:11 step:10941 [D loss: 0.641933, acc.: 60.94%] [G loss: 1.002761]\n",
      "epoch:11 step:10942 [D loss: 0.648082, acc.: 65.62%] [G loss: 0.800563]\n",
      "epoch:11 step:10943 [D loss: 0.654228, acc.: 58.59%] [G loss: 0.744492]\n",
      "epoch:11 step:10944 [D loss: 0.655332, acc.: 60.16%] [G loss: 0.892706]\n",
      "epoch:11 step:10945 [D loss: 0.660069, acc.: 50.00%] [G loss: 0.771408]\n",
      "epoch:11 step:10946 [D loss: 0.698470, acc.: 46.09%] [G loss: 1.006434]\n",
      "epoch:11 step:10947 [D loss: 0.648219, acc.: 60.94%] [G loss: 0.826919]\n",
      "epoch:11 step:10948 [D loss: 0.652703, acc.: 56.25%] [G loss: 0.809281]\n",
      "epoch:11 step:10949 [D loss: 0.696874, acc.: 49.22%] [G loss: 0.805188]\n",
      "epoch:11 step:10950 [D loss: 0.672332, acc.: 58.59%] [G loss: 0.810117]\n",
      "epoch:11 step:10951 [D loss: 0.691960, acc.: 55.47%] [G loss: 0.784335]\n",
      "epoch:11 step:10952 [D loss: 0.700561, acc.: 50.78%] [G loss: 0.832751]\n",
      "epoch:11 step:10953 [D loss: 0.821557, acc.: 50.00%] [G loss: 0.895269]\n",
      "epoch:11 step:10954 [D loss: 0.643512, acc.: 64.84%] [G loss: 0.901280]\n",
      "epoch:11 step:10955 [D loss: 0.691405, acc.: 57.03%] [G loss: 0.918425]\n",
      "epoch:11 step:10956 [D loss: 0.698145, acc.: 51.56%] [G loss: 0.983731]\n",
      "epoch:11 step:10957 [D loss: 0.657626, acc.: 64.06%] [G loss: 0.846239]\n",
      "epoch:11 step:10958 [D loss: 0.697906, acc.: 55.47%] [G loss: 0.833921]\n",
      "epoch:11 step:10959 [D loss: 0.659406, acc.: 57.81%] [G loss: 0.878690]\n",
      "epoch:11 step:10960 [D loss: 0.692067, acc.: 60.94%] [G loss: 0.770962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10961 [D loss: 0.626973, acc.: 67.97%] [G loss: 0.808458]\n",
      "epoch:11 step:10962 [D loss: 0.642309, acc.: 64.84%] [G loss: 0.786090]\n",
      "epoch:11 step:10963 [D loss: 0.674092, acc.: 50.78%] [G loss: 0.850278]\n",
      "epoch:11 step:10964 [D loss: 0.672148, acc.: 57.81%] [G loss: 0.839907]\n",
      "epoch:11 step:10965 [D loss: 0.708923, acc.: 52.34%] [G loss: 0.821702]\n",
      "epoch:11 step:10966 [D loss: 0.652494, acc.: 54.69%] [G loss: 0.883900]\n",
      "epoch:11 step:10967 [D loss: 0.677528, acc.: 58.59%] [G loss: 0.796879]\n",
      "epoch:11 step:10968 [D loss: 0.678055, acc.: 52.34%] [G loss: 0.797649]\n",
      "epoch:11 step:10969 [D loss: 0.688170, acc.: 56.25%] [G loss: 0.749098]\n",
      "epoch:11 step:10970 [D loss: 0.643696, acc.: 63.28%] [G loss: 0.836997]\n",
      "epoch:11 step:10971 [D loss: 0.673897, acc.: 57.03%] [G loss: 0.852465]\n",
      "epoch:11 step:10972 [D loss: 0.709231, acc.: 48.44%] [G loss: 0.860824]\n",
      "epoch:11 step:10973 [D loss: 0.654907, acc.: 59.38%] [G loss: 0.818372]\n",
      "epoch:11 step:10974 [D loss: 0.665118, acc.: 63.28%] [G loss: 0.868624]\n",
      "epoch:11 step:10975 [D loss: 0.639498, acc.: 63.28%] [G loss: 0.835902]\n",
      "epoch:11 step:10976 [D loss: 0.685412, acc.: 50.78%] [G loss: 0.802273]\n",
      "epoch:11 step:10977 [D loss: 0.654716, acc.: 60.16%] [G loss: 0.793324]\n",
      "epoch:11 step:10978 [D loss: 0.696937, acc.: 51.56%] [G loss: 0.817321]\n",
      "epoch:11 step:10979 [D loss: 0.608917, acc.: 72.66%] [G loss: 0.889701]\n",
      "epoch:11 step:10980 [D loss: 0.674744, acc.: 57.81%] [G loss: 0.867568]\n",
      "epoch:11 step:10981 [D loss: 0.696193, acc.: 51.56%] [G loss: 0.910577]\n",
      "epoch:11 step:10982 [D loss: 0.678340, acc.: 63.28%] [G loss: 0.859001]\n",
      "epoch:11 step:10983 [D loss: 0.655251, acc.: 62.50%] [G loss: 0.886465]\n",
      "epoch:11 step:10984 [D loss: 0.654500, acc.: 61.72%] [G loss: 1.002681]\n",
      "epoch:11 step:10985 [D loss: 0.692557, acc.: 56.25%] [G loss: 0.886521]\n",
      "epoch:11 step:10986 [D loss: 0.666018, acc.: 58.59%] [G loss: 0.860051]\n",
      "epoch:11 step:10987 [D loss: 0.654978, acc.: 59.38%] [G loss: 0.886297]\n",
      "epoch:11 step:10988 [D loss: 0.678834, acc.: 51.56%] [G loss: 0.889072]\n",
      "epoch:11 step:10989 [D loss: 0.687736, acc.: 52.34%] [G loss: 0.897399]\n",
      "epoch:11 step:10990 [D loss: 0.655800, acc.: 66.41%] [G loss: 0.883304]\n",
      "epoch:11 step:10991 [D loss: 0.675689, acc.: 56.25%] [G loss: 0.785333]\n",
      "epoch:11 step:10992 [D loss: 0.729230, acc.: 47.66%] [G loss: 0.876358]\n",
      "epoch:11 step:10993 [D loss: 0.678857, acc.: 57.03%] [G loss: 0.888222]\n",
      "epoch:11 step:10994 [D loss: 0.669472, acc.: 50.78%] [G loss: 0.855416]\n",
      "epoch:11 step:10995 [D loss: 0.636474, acc.: 69.53%] [G loss: 0.774454]\n",
      "epoch:11 step:10996 [D loss: 0.655983, acc.: 65.62%] [G loss: 0.851513]\n",
      "epoch:11 step:10997 [D loss: 0.690538, acc.: 54.69%] [G loss: 0.800044]\n",
      "epoch:11 step:10998 [D loss: 0.709160, acc.: 49.22%] [G loss: 0.737776]\n",
      "epoch:11 step:10999 [D loss: 0.652889, acc.: 63.28%] [G loss: 0.780447]\n",
      "epoch:11 step:11000 [D loss: 0.670939, acc.: 57.81%] [G loss: 0.828621]\n",
      "epoch:11 step:11001 [D loss: 0.695687, acc.: 60.16%] [G loss: 0.766228]\n",
      "epoch:11 step:11002 [D loss: 0.683535, acc.: 46.88%] [G loss: 0.734304]\n",
      "epoch:11 step:11003 [D loss: 0.599190, acc.: 71.88%] [G loss: 0.760731]\n",
      "epoch:11 step:11004 [D loss: 0.638789, acc.: 67.19%] [G loss: 0.756143]\n",
      "epoch:11 step:11005 [D loss: 0.664195, acc.: 52.34%] [G loss: 0.776649]\n",
      "epoch:11 step:11006 [D loss: 0.664582, acc.: 64.06%] [G loss: 0.752552]\n",
      "epoch:11 step:11007 [D loss: 0.642356, acc.: 61.72%] [G loss: 0.740085]\n",
      "epoch:11 step:11008 [D loss: 0.700440, acc.: 53.91%] [G loss: 0.822429]\n",
      "epoch:11 step:11009 [D loss: 0.659950, acc.: 57.81%] [G loss: 0.936865]\n",
      "epoch:11 step:11010 [D loss: 0.639329, acc.: 70.31%] [G loss: 0.850887]\n",
      "epoch:11 step:11011 [D loss: 0.662439, acc.: 56.25%] [G loss: 0.835265]\n",
      "epoch:11 step:11012 [D loss: 0.683587, acc.: 53.91%] [G loss: 0.812047]\n",
      "epoch:11 step:11013 [D loss: 0.644681, acc.: 64.06%] [G loss: 0.948995]\n",
      "epoch:11 step:11014 [D loss: 0.644609, acc.: 64.84%] [G loss: 0.843616]\n",
      "epoch:11 step:11015 [D loss: 0.687524, acc.: 54.69%] [G loss: 0.951818]\n",
      "epoch:11 step:11016 [D loss: 0.705368, acc.: 46.88%] [G loss: 0.920867]\n",
      "epoch:11 step:11017 [D loss: 0.706538, acc.: 51.56%] [G loss: 0.848628]\n",
      "epoch:11 step:11018 [D loss: 0.652979, acc.: 63.28%] [G loss: 0.814882]\n",
      "epoch:11 step:11019 [D loss: 0.644765, acc.: 65.62%] [G loss: 0.864654]\n",
      "epoch:11 step:11020 [D loss: 0.672558, acc.: 60.94%] [G loss: 0.945257]\n",
      "epoch:11 step:11021 [D loss: 0.694561, acc.: 55.47%] [G loss: 0.741652]\n",
      "epoch:11 step:11022 [D loss: 0.693609, acc.: 57.03%] [G loss: 0.851734]\n",
      "epoch:11 step:11023 [D loss: 0.646581, acc.: 57.81%] [G loss: 0.774368]\n",
      "epoch:11 step:11024 [D loss: 0.701266, acc.: 52.34%] [G loss: 0.902102]\n",
      "epoch:11 step:11025 [D loss: 0.627989, acc.: 70.31%] [G loss: 0.857960]\n",
      "epoch:11 step:11026 [D loss: 0.662224, acc.: 61.72%] [G loss: 0.865551]\n",
      "epoch:11 step:11027 [D loss: 0.641717, acc.: 63.28%] [G loss: 0.809297]\n",
      "epoch:11 step:11028 [D loss: 0.613000, acc.: 66.41%] [G loss: 0.999308]\n",
      "epoch:11 step:11029 [D loss: 0.679253, acc.: 54.69%] [G loss: 0.837134]\n",
      "epoch:11 step:11030 [D loss: 0.630787, acc.: 64.84%] [G loss: 0.889863]\n",
      "epoch:11 step:11031 [D loss: 0.679553, acc.: 59.38%] [G loss: 0.911944]\n",
      "epoch:11 step:11032 [D loss: 0.640618, acc.: 61.72%] [G loss: 0.840088]\n",
      "epoch:11 step:11033 [D loss: 0.681216, acc.: 58.59%] [G loss: 0.825212]\n",
      "epoch:11 step:11034 [D loss: 0.670966, acc.: 58.59%] [G loss: 0.855631]\n",
      "epoch:11 step:11035 [D loss: 0.671226, acc.: 57.81%] [G loss: 0.789215]\n",
      "epoch:11 step:11036 [D loss: 0.676519, acc.: 55.47%] [G loss: 0.761332]\n",
      "epoch:11 step:11037 [D loss: 0.708092, acc.: 50.00%] [G loss: 0.769624]\n",
      "epoch:11 step:11038 [D loss: 0.800723, acc.: 41.41%] [G loss: 0.741610]\n",
      "epoch:11 step:11039 [D loss: 0.605634, acc.: 68.75%] [G loss: 0.828072]\n",
      "epoch:11 step:11040 [D loss: 0.631982, acc.: 66.41%] [G loss: 0.829493]\n",
      "epoch:11 step:11041 [D loss: 0.652728, acc.: 63.28%] [G loss: 0.885434]\n",
      "epoch:11 step:11042 [D loss: 0.641336, acc.: 64.84%] [G loss: 0.803581]\n",
      "epoch:11 step:11043 [D loss: 0.615903, acc.: 66.41%] [G loss: 0.758501]\n",
      "epoch:11 step:11044 [D loss: 0.646578, acc.: 61.72%] [G loss: 0.824915]\n",
      "epoch:11 step:11045 [D loss: 0.645551, acc.: 62.50%] [G loss: 0.920540]\n",
      "epoch:11 step:11046 [D loss: 0.664795, acc.: 63.28%] [G loss: 0.794227]\n",
      "epoch:11 step:11047 [D loss: 0.690890, acc.: 50.78%] [G loss: 0.723722]\n",
      "epoch:11 step:11048 [D loss: 0.606313, acc.: 70.31%] [G loss: 0.919701]\n",
      "epoch:11 step:11049 [D loss: 0.618045, acc.: 67.19%] [G loss: 0.823069]\n",
      "epoch:11 step:11050 [D loss: 0.633231, acc.: 62.50%] [G loss: 0.910268]\n",
      "epoch:11 step:11051 [D loss: 0.676853, acc.: 59.38%] [G loss: 0.796384]\n",
      "epoch:11 step:11052 [D loss: 0.684821, acc.: 55.47%] [G loss: 0.935760]\n",
      "epoch:11 step:11053 [D loss: 0.689102, acc.: 54.69%] [G loss: 0.791518]\n",
      "epoch:11 step:11054 [D loss: 0.679586, acc.: 62.50%] [G loss: 0.803729]\n",
      "epoch:11 step:11055 [D loss: 0.639690, acc.: 63.28%] [G loss: 0.784864]\n",
      "epoch:11 step:11056 [D loss: 0.650306, acc.: 64.84%] [G loss: 0.900649]\n",
      "epoch:11 step:11057 [D loss: 0.632842, acc.: 62.50%] [G loss: 1.088391]\n",
      "epoch:11 step:11058 [D loss: 0.705489, acc.: 51.56%] [G loss: 0.917601]\n",
      "epoch:11 step:11059 [D loss: 0.707189, acc.: 49.22%] [G loss: 0.816527]\n",
      "epoch:11 step:11060 [D loss: 0.637164, acc.: 71.88%] [G loss: 0.783567]\n",
      "epoch:11 step:11061 [D loss: 0.689733, acc.: 53.91%] [G loss: 0.789993]\n",
      "epoch:11 step:11062 [D loss: 0.708310, acc.: 53.91%] [G loss: 0.799841]\n",
      "epoch:11 step:11063 [D loss: 0.685390, acc.: 61.72%] [G loss: 0.844616]\n",
      "epoch:11 step:11064 [D loss: 0.634189, acc.: 66.41%] [G loss: 0.892460]\n",
      "epoch:11 step:11065 [D loss: 0.673536, acc.: 56.25%] [G loss: 0.828266]\n",
      "epoch:11 step:11066 [D loss: 0.689134, acc.: 53.91%] [G loss: 0.759293]\n",
      "epoch:11 step:11067 [D loss: 0.670622, acc.: 57.81%] [G loss: 0.908435]\n",
      "epoch:11 step:11068 [D loss: 0.784920, acc.: 40.62%] [G loss: 0.727924]\n",
      "epoch:11 step:11069 [D loss: 0.649212, acc.: 55.47%] [G loss: 0.804268]\n",
      "epoch:11 step:11070 [D loss: 0.704637, acc.: 54.69%] [G loss: 0.838601]\n",
      "epoch:11 step:11071 [D loss: 0.631152, acc.: 61.72%] [G loss: 0.826179]\n",
      "epoch:11 step:11072 [D loss: 0.724421, acc.: 46.88%] [G loss: 0.876289]\n",
      "epoch:11 step:11073 [D loss: 0.638378, acc.: 61.72%] [G loss: 0.952269]\n",
      "epoch:11 step:11074 [D loss: 0.714609, acc.: 48.44%] [G loss: 0.760633]\n",
      "epoch:11 step:11075 [D loss: 0.645681, acc.: 60.16%] [G loss: 0.883186]\n",
      "epoch:11 step:11076 [D loss: 0.627565, acc.: 65.62%] [G loss: 0.861313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11077 [D loss: 0.626719, acc.: 64.84%] [G loss: 0.824332]\n",
      "epoch:11 step:11078 [D loss: 0.712988, acc.: 49.22%] [G loss: 0.812947]\n",
      "epoch:11 step:11079 [D loss: 0.710223, acc.: 48.44%] [G loss: 0.878569]\n",
      "epoch:11 step:11080 [D loss: 0.727928, acc.: 50.00%] [G loss: 0.847086]\n",
      "epoch:11 step:11081 [D loss: 0.648155, acc.: 63.28%] [G loss: 0.874769]\n",
      "epoch:11 step:11082 [D loss: 0.697409, acc.: 56.25%] [G loss: 0.834209]\n",
      "epoch:11 step:11083 [D loss: 0.652987, acc.: 54.69%] [G loss: 0.849262]\n",
      "epoch:11 step:11084 [D loss: 0.723995, acc.: 46.09%] [G loss: 0.852927]\n",
      "epoch:11 step:11085 [D loss: 0.685438, acc.: 56.25%] [G loss: 0.765717]\n",
      "epoch:11 step:11086 [D loss: 0.715298, acc.: 48.44%] [G loss: 0.931639]\n",
      "epoch:11 step:11087 [D loss: 0.680838, acc.: 52.34%] [G loss: 0.826796]\n",
      "epoch:11 step:11088 [D loss: 0.724279, acc.: 43.75%] [G loss: 0.834616]\n",
      "epoch:11 step:11089 [D loss: 0.683490, acc.: 53.12%] [G loss: 0.822072]\n",
      "epoch:11 step:11090 [D loss: 0.652284, acc.: 64.84%] [G loss: 0.819502]\n",
      "epoch:11 step:11091 [D loss: 0.635472, acc.: 57.81%] [G loss: 0.838751]\n",
      "epoch:11 step:11092 [D loss: 0.689488, acc.: 51.56%] [G loss: 0.814936]\n",
      "epoch:11 step:11093 [D loss: 0.646578, acc.: 54.69%] [G loss: 0.870737]\n",
      "epoch:11 step:11094 [D loss: 0.717438, acc.: 46.88%] [G loss: 0.843219]\n",
      "epoch:11 step:11095 [D loss: 0.646877, acc.: 60.16%] [G loss: 0.797313]\n",
      "epoch:11 step:11096 [D loss: 0.669982, acc.: 61.72%] [G loss: 0.825700]\n",
      "epoch:11 step:11097 [D loss: 0.683906, acc.: 54.69%] [G loss: 0.698739]\n",
      "epoch:11 step:11098 [D loss: 0.641979, acc.: 63.28%] [G loss: 0.760919]\n",
      "epoch:11 step:11099 [D loss: 0.664074, acc.: 54.69%] [G loss: 0.813388]\n",
      "epoch:11 step:11100 [D loss: 0.688137, acc.: 53.91%] [G loss: 0.885516]\n",
      "epoch:11 step:11101 [D loss: 0.629451, acc.: 64.84%] [G loss: 0.871857]\n",
      "epoch:11 step:11102 [D loss: 0.653155, acc.: 62.50%] [G loss: 0.787971]\n",
      "epoch:11 step:11103 [D loss: 0.714636, acc.: 51.56%] [G loss: 0.874587]\n",
      "epoch:11 step:11104 [D loss: 0.677084, acc.: 49.22%] [G loss: 0.734936]\n",
      "epoch:11 step:11105 [D loss: 0.681032, acc.: 46.09%] [G loss: 0.859854]\n",
      "epoch:11 step:11106 [D loss: 0.654158, acc.: 55.47%] [G loss: 0.866088]\n",
      "epoch:11 step:11107 [D loss: 0.672510, acc.: 57.03%] [G loss: 0.819319]\n",
      "epoch:11 step:11108 [D loss: 0.633420, acc.: 55.47%] [G loss: 0.849496]\n",
      "epoch:11 step:11109 [D loss: 0.745442, acc.: 39.84%] [G loss: 0.821243]\n",
      "epoch:11 step:11110 [D loss: 0.692422, acc.: 61.72%] [G loss: 0.810242]\n",
      "epoch:11 step:11111 [D loss: 0.676707, acc.: 53.91%] [G loss: 0.870466]\n",
      "epoch:11 step:11112 [D loss: 0.688279, acc.: 55.47%] [G loss: 0.788787]\n",
      "epoch:11 step:11113 [D loss: 0.715349, acc.: 50.00%] [G loss: 0.865239]\n",
      "epoch:11 step:11114 [D loss: 0.693190, acc.: 54.69%] [G loss: 0.785655]\n",
      "epoch:11 step:11115 [D loss: 0.669485, acc.: 60.16%] [G loss: 0.763735]\n",
      "epoch:11 step:11116 [D loss: 0.640680, acc.: 62.50%] [G loss: 0.836323]\n",
      "epoch:11 step:11117 [D loss: 0.671379, acc.: 58.59%] [G loss: 0.775349]\n",
      "epoch:11 step:11118 [D loss: 0.657434, acc.: 58.59%] [G loss: 0.756409]\n",
      "epoch:11 step:11119 [D loss: 0.676906, acc.: 50.78%] [G loss: 0.813142]\n",
      "epoch:11 step:11120 [D loss: 0.710528, acc.: 50.00%] [G loss: 0.782960]\n",
      "epoch:11 step:11121 [D loss: 0.684695, acc.: 63.28%] [G loss: 0.840795]\n",
      "epoch:11 step:11122 [D loss: 0.708434, acc.: 47.66%] [G loss: 0.835517]\n",
      "epoch:11 step:11123 [D loss: 0.667569, acc.: 53.91%] [G loss: 0.821050]\n",
      "epoch:11 step:11124 [D loss: 0.646969, acc.: 58.59%] [G loss: 0.850542]\n",
      "epoch:11 step:11125 [D loss: 0.693236, acc.: 55.47%] [G loss: 0.819295]\n",
      "epoch:11 step:11126 [D loss: 0.660885, acc.: 53.91%] [G loss: 0.817453]\n",
      "epoch:11 step:11127 [D loss: 0.643882, acc.: 66.41%] [G loss: 0.844124]\n",
      "epoch:11 step:11128 [D loss: 0.693195, acc.: 57.03%] [G loss: 0.895673]\n",
      "epoch:11 step:11129 [D loss: 0.674621, acc.: 56.25%] [G loss: 0.927413]\n",
      "epoch:11 step:11130 [D loss: 0.650502, acc.: 62.50%] [G loss: 0.856657]\n",
      "epoch:11 step:11131 [D loss: 0.682569, acc.: 58.59%] [G loss: 0.917592]\n",
      "epoch:11 step:11132 [D loss: 0.675474, acc.: 55.47%] [G loss: 0.846759]\n",
      "epoch:11 step:11133 [D loss: 0.646174, acc.: 63.28%] [G loss: 0.788214]\n",
      "epoch:11 step:11134 [D loss: 0.680044, acc.: 62.50%] [G loss: 0.802593]\n",
      "epoch:11 step:11135 [D loss: 0.678329, acc.: 57.81%] [G loss: 0.826676]\n",
      "epoch:11 step:11136 [D loss: 0.700889, acc.: 57.03%] [G loss: 0.745551]\n",
      "epoch:11 step:11137 [D loss: 0.722603, acc.: 46.09%] [G loss: 0.839899]\n",
      "epoch:11 step:11138 [D loss: 0.668802, acc.: 60.94%] [G loss: 0.791920]\n",
      "epoch:11 step:11139 [D loss: 0.689789, acc.: 57.81%] [G loss: 0.870714]\n",
      "epoch:11 step:11140 [D loss: 0.650784, acc.: 57.81%] [G loss: 1.018222]\n",
      "epoch:11 step:11141 [D loss: 0.648971, acc.: 58.59%] [G loss: 0.875148]\n",
      "epoch:11 step:11142 [D loss: 0.671507, acc.: 56.25%] [G loss: 0.834103]\n",
      "epoch:11 step:11143 [D loss: 0.680091, acc.: 57.81%] [G loss: 0.896142]\n",
      "epoch:11 step:11144 [D loss: 0.694281, acc.: 48.44%] [G loss: 0.874206]\n",
      "epoch:11 step:11145 [D loss: 0.687304, acc.: 48.44%] [G loss: 0.773138]\n",
      "epoch:11 step:11146 [D loss: 0.646070, acc.: 60.16%] [G loss: 0.861953]\n",
      "epoch:11 step:11147 [D loss: 0.711938, acc.: 42.97%] [G loss: 0.848857]\n",
      "epoch:11 step:11148 [D loss: 0.679124, acc.: 53.91%] [G loss: 0.864251]\n",
      "epoch:11 step:11149 [D loss: 0.683632, acc.: 60.16%] [G loss: 0.840656]\n",
      "epoch:11 step:11150 [D loss: 0.682430, acc.: 56.25%] [G loss: 0.822481]\n",
      "epoch:11 step:11151 [D loss: 0.689935, acc.: 48.44%] [G loss: 0.910489]\n",
      "epoch:11 step:11152 [D loss: 0.742378, acc.: 43.75%] [G loss: 0.752316]\n",
      "epoch:11 step:11153 [D loss: 0.637532, acc.: 61.72%] [G loss: 0.906455]\n",
      "epoch:11 step:11154 [D loss: 0.681837, acc.: 53.12%] [G loss: 0.808714]\n",
      "epoch:11 step:11155 [D loss: 0.682740, acc.: 55.47%] [G loss: 0.830908]\n",
      "epoch:11 step:11156 [D loss: 0.671111, acc.: 57.81%] [G loss: 0.853920]\n",
      "epoch:11 step:11157 [D loss: 0.650998, acc.: 65.62%] [G loss: 0.815828]\n",
      "epoch:11 step:11158 [D loss: 0.679939, acc.: 53.12%] [G loss: 0.817879]\n",
      "epoch:11 step:11159 [D loss: 0.677970, acc.: 55.47%] [G loss: 0.887495]\n",
      "epoch:11 step:11160 [D loss: 0.686470, acc.: 51.56%] [G loss: 0.880081]\n",
      "epoch:11 step:11161 [D loss: 0.732775, acc.: 44.53%] [G loss: 0.882968]\n",
      "epoch:11 step:11162 [D loss: 0.640785, acc.: 58.59%] [G loss: 0.853536]\n",
      "epoch:11 step:11163 [D loss: 0.652578, acc.: 60.16%] [G loss: 0.925512]\n",
      "epoch:11 step:11164 [D loss: 0.676783, acc.: 51.56%] [G loss: 0.815185]\n",
      "epoch:11 step:11165 [D loss: 0.657140, acc.: 57.03%] [G loss: 0.810165]\n",
      "epoch:11 step:11166 [D loss: 0.700887, acc.: 49.22%] [G loss: 0.763078]\n",
      "epoch:11 step:11167 [D loss: 0.613569, acc.: 69.53%] [G loss: 0.852956]\n",
      "epoch:11 step:11168 [D loss: 0.642351, acc.: 67.19%] [G loss: 0.839731]\n",
      "epoch:11 step:11169 [D loss: 0.619072, acc.: 69.53%] [G loss: 0.789503]\n",
      "epoch:11 step:11170 [D loss: 0.608281, acc.: 70.31%] [G loss: 0.777665]\n",
      "epoch:11 step:11171 [D loss: 0.676995, acc.: 49.22%] [G loss: 0.821024]\n",
      "epoch:11 step:11172 [D loss: 0.627952, acc.: 62.50%] [G loss: 0.877845]\n",
      "epoch:11 step:11173 [D loss: 0.624516, acc.: 60.94%] [G loss: 0.825551]\n",
      "epoch:11 step:11174 [D loss: 0.651175, acc.: 67.19%] [G loss: 0.876383]\n",
      "epoch:11 step:11175 [D loss: 0.655283, acc.: 62.50%] [G loss: 0.935802]\n",
      "epoch:11 step:11176 [D loss: 0.728880, acc.: 45.31%] [G loss: 0.817356]\n",
      "epoch:11 step:11177 [D loss: 0.644274, acc.: 57.81%] [G loss: 0.855424]\n",
      "epoch:11 step:11178 [D loss: 0.729476, acc.: 47.66%] [G loss: 0.782379]\n",
      "epoch:11 step:11179 [D loss: 0.691221, acc.: 52.34%] [G loss: 0.847407]\n",
      "epoch:11 step:11180 [D loss: 0.677739, acc.: 53.91%] [G loss: 0.862822]\n",
      "epoch:11 step:11181 [D loss: 0.660797, acc.: 55.47%] [G loss: 0.847882]\n",
      "epoch:11 step:11182 [D loss: 0.673368, acc.: 54.69%] [G loss: 0.842796]\n",
      "epoch:11 step:11183 [D loss: 0.631043, acc.: 63.28%] [G loss: 0.851022]\n",
      "epoch:11 step:11184 [D loss: 0.649728, acc.: 62.50%] [G loss: 0.937631]\n",
      "epoch:11 step:11185 [D loss: 0.722955, acc.: 46.88%] [G loss: 0.902582]\n",
      "epoch:11 step:11186 [D loss: 0.661027, acc.: 53.12%] [G loss: 0.924130]\n",
      "epoch:11 step:11187 [D loss: 0.683129, acc.: 55.47%] [G loss: 0.793411]\n",
      "epoch:11 step:11188 [D loss: 0.662348, acc.: 59.38%] [G loss: 0.869042]\n",
      "epoch:11 step:11189 [D loss: 0.643925, acc.: 60.16%] [G loss: 0.831525]\n",
      "epoch:11 step:11190 [D loss: 0.654825, acc.: 60.16%] [G loss: 0.858651]\n",
      "epoch:11 step:11191 [D loss: 0.650543, acc.: 60.94%] [G loss: 1.014073]\n",
      "epoch:11 step:11192 [D loss: 0.725362, acc.: 48.44%] [G loss: 0.798824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11193 [D loss: 0.657539, acc.: 57.81%] [G loss: 0.851434]\n",
      "epoch:11 step:11194 [D loss: 0.615062, acc.: 67.19%] [G loss: 0.878487]\n",
      "epoch:11 step:11195 [D loss: 0.700279, acc.: 52.34%] [G loss: 0.810619]\n",
      "epoch:11 step:11196 [D loss: 0.682442, acc.: 55.47%] [G loss: 0.907667]\n",
      "epoch:11 step:11197 [D loss: 0.656457, acc.: 57.81%] [G loss: 0.756295]\n",
      "epoch:11 step:11198 [D loss: 0.632577, acc.: 67.97%] [G loss: 0.814846]\n",
      "epoch:11 step:11199 [D loss: 0.676932, acc.: 57.03%] [G loss: 0.816852]\n",
      "epoch:11 step:11200 [D loss: 0.674692, acc.: 56.25%] [G loss: 0.781063]\n",
      "epoch:11 step:11201 [D loss: 0.677608, acc.: 51.56%] [G loss: 0.844340]\n",
      "epoch:11 step:11202 [D loss: 0.652148, acc.: 55.47%] [G loss: 0.902827]\n",
      "epoch:11 step:11203 [D loss: 0.679043, acc.: 57.03%] [G loss: 0.798144]\n",
      "epoch:11 step:11204 [D loss: 0.732899, acc.: 45.31%] [G loss: 0.852729]\n",
      "epoch:11 step:11205 [D loss: 0.643018, acc.: 59.38%] [G loss: 0.840789]\n",
      "epoch:11 step:11206 [D loss: 0.658977, acc.: 58.59%] [G loss: 0.815688]\n",
      "epoch:11 step:11207 [D loss: 0.667145, acc.: 62.50%] [G loss: 0.883551]\n",
      "epoch:11 step:11208 [D loss: 0.684231, acc.: 53.12%] [G loss: 0.900699]\n",
      "epoch:11 step:11209 [D loss: 0.678249, acc.: 53.91%] [G loss: 0.950998]\n",
      "epoch:11 step:11210 [D loss: 0.710005, acc.: 53.91%] [G loss: 0.816221]\n",
      "epoch:11 step:11211 [D loss: 0.720756, acc.: 42.97%] [G loss: 0.818233]\n",
      "epoch:11 step:11212 [D loss: 0.723898, acc.: 46.09%] [G loss: 0.838269]\n",
      "epoch:11 step:11213 [D loss: 0.703189, acc.: 48.44%] [G loss: 0.837309]\n",
      "epoch:11 step:11214 [D loss: 0.660936, acc.: 56.25%] [G loss: 0.823259]\n",
      "epoch:11 step:11215 [D loss: 0.654160, acc.: 57.03%] [G loss: 0.840583]\n",
      "epoch:11 step:11216 [D loss: 0.643798, acc.: 56.25%] [G loss: 0.821799]\n",
      "epoch:11 step:11217 [D loss: 0.701083, acc.: 49.22%] [G loss: 0.837304]\n",
      "epoch:11 step:11218 [D loss: 0.660858, acc.: 53.91%] [G loss: 0.768116]\n",
      "epoch:11 step:11219 [D loss: 0.668646, acc.: 57.03%] [G loss: 0.815575]\n",
      "epoch:11 step:11220 [D loss: 0.632565, acc.: 64.06%] [G loss: 0.776399]\n",
      "epoch:11 step:11221 [D loss: 0.670545, acc.: 60.94%] [G loss: 0.864651]\n",
      "epoch:11 step:11222 [D loss: 0.657915, acc.: 67.19%] [G loss: 0.896272]\n",
      "epoch:11 step:11223 [D loss: 0.647299, acc.: 60.16%] [G loss: 0.807926]\n",
      "epoch:11 step:11224 [D loss: 0.649290, acc.: 62.50%] [G loss: 0.846062]\n",
      "epoch:11 step:11225 [D loss: 0.625893, acc.: 64.84%] [G loss: 0.877059]\n",
      "epoch:11 step:11226 [D loss: 0.675242, acc.: 53.12%] [G loss: 0.824873]\n",
      "epoch:11 step:11227 [D loss: 0.637819, acc.: 64.84%] [G loss: 0.835709]\n",
      "epoch:11 step:11228 [D loss: 0.641109, acc.: 63.28%] [G loss: 0.892623]\n",
      "epoch:11 step:11229 [D loss: 0.645053, acc.: 60.16%] [G loss: 0.915161]\n",
      "epoch:11 step:11230 [D loss: 0.725942, acc.: 51.56%] [G loss: 0.686216]\n",
      "epoch:11 step:11231 [D loss: 0.641324, acc.: 60.94%] [G loss: 0.827203]\n",
      "epoch:11 step:11232 [D loss: 0.665373, acc.: 57.03%] [G loss: 0.876335]\n",
      "epoch:11 step:11233 [D loss: 0.683538, acc.: 54.69%] [G loss: 0.737474]\n",
      "epoch:11 step:11234 [D loss: 0.693937, acc.: 52.34%] [G loss: 0.882119]\n",
      "epoch:11 step:11235 [D loss: 0.646078, acc.: 57.81%] [G loss: 0.806598]\n",
      "epoch:11 step:11236 [D loss: 0.671775, acc.: 58.59%] [G loss: 0.874814]\n",
      "epoch:11 step:11237 [D loss: 0.637369, acc.: 59.38%] [G loss: 0.876544]\n",
      "epoch:11 step:11238 [D loss: 0.668185, acc.: 50.78%] [G loss: 0.818803]\n",
      "epoch:11 step:11239 [D loss: 0.685371, acc.: 53.12%] [G loss: 0.821407]\n",
      "epoch:11 step:11240 [D loss: 0.636173, acc.: 54.69%] [G loss: 0.873243]\n",
      "epoch:11 step:11241 [D loss: 0.682642, acc.: 60.94%] [G loss: 0.832015]\n",
      "epoch:11 step:11242 [D loss: 0.681010, acc.: 58.59%] [G loss: 0.808647]\n",
      "epoch:11 step:11243 [D loss: 0.640919, acc.: 62.50%] [G loss: 0.900521]\n",
      "epoch:11 step:11244 [D loss: 0.694958, acc.: 49.22%] [G loss: 0.907177]\n",
      "epoch:12 step:11245 [D loss: 0.739229, acc.: 57.03%] [G loss: 0.897139]\n",
      "epoch:12 step:11246 [D loss: 0.663996, acc.: 60.16%] [G loss: 0.854339]\n",
      "epoch:12 step:11247 [D loss: 0.689474, acc.: 55.47%] [G loss: 0.710244]\n",
      "epoch:12 step:11248 [D loss: 0.759012, acc.: 50.78%] [G loss: 0.781212]\n",
      "epoch:12 step:11249 [D loss: 0.619929, acc.: 67.19%] [G loss: 0.816572]\n",
      "epoch:12 step:11250 [D loss: 0.700212, acc.: 53.91%] [G loss: 0.881993]\n",
      "epoch:12 step:11251 [D loss: 0.699587, acc.: 56.25%] [G loss: 0.855611]\n",
      "epoch:12 step:11252 [D loss: 0.714365, acc.: 50.00%] [G loss: 0.811298]\n",
      "epoch:12 step:11253 [D loss: 0.641434, acc.: 62.50%] [G loss: 0.868757]\n",
      "epoch:12 step:11254 [D loss: 0.729647, acc.: 47.66%] [G loss: 0.863855]\n",
      "epoch:12 step:11255 [D loss: 0.668517, acc.: 57.03%] [G loss: 0.798628]\n",
      "epoch:12 step:11256 [D loss: 0.696235, acc.: 53.12%] [G loss: 0.790964]\n",
      "epoch:12 step:11257 [D loss: 0.687750, acc.: 49.22%] [G loss: 0.784850]\n",
      "epoch:12 step:11258 [D loss: 0.687880, acc.: 52.34%] [G loss: 0.839154]\n",
      "epoch:12 step:11259 [D loss: 0.696356, acc.: 54.69%] [G loss: 0.796959]\n",
      "epoch:12 step:11260 [D loss: 0.670562, acc.: 59.38%] [G loss: 0.930755]\n",
      "epoch:12 step:11261 [D loss: 0.618676, acc.: 64.06%] [G loss: 1.047247]\n",
      "epoch:12 step:11262 [D loss: 0.717910, acc.: 46.09%] [G loss: 0.885631]\n",
      "epoch:12 step:11263 [D loss: 0.685041, acc.: 53.91%] [G loss: 0.898535]\n",
      "epoch:12 step:11264 [D loss: 0.676883, acc.: 59.38%] [G loss: 0.873901]\n",
      "epoch:12 step:11265 [D loss: 0.702827, acc.: 48.44%] [G loss: 0.855356]\n",
      "epoch:12 step:11266 [D loss: 0.673045, acc.: 50.00%] [G loss: 0.841883]\n",
      "epoch:12 step:11267 [D loss: 0.653204, acc.: 52.34%] [G loss: 0.805940]\n",
      "epoch:12 step:11268 [D loss: 0.685718, acc.: 53.12%] [G loss: 0.892573]\n",
      "epoch:12 step:11269 [D loss: 0.679383, acc.: 54.69%] [G loss: 0.847255]\n",
      "epoch:12 step:11270 [D loss: 0.694417, acc.: 57.03%] [G loss: 0.862934]\n",
      "epoch:12 step:11271 [D loss: 0.687409, acc.: 56.25%] [G loss: 0.833849]\n",
      "epoch:12 step:11272 [D loss: 0.670621, acc.: 56.25%] [G loss: 0.824737]\n",
      "epoch:12 step:11273 [D loss: 0.671879, acc.: 57.81%] [G loss: 0.818601]\n",
      "epoch:12 step:11274 [D loss: 0.643322, acc.: 57.81%] [G loss: 0.778367]\n",
      "epoch:12 step:11275 [D loss: 0.710120, acc.: 45.31%] [G loss: 0.908877]\n",
      "epoch:12 step:11276 [D loss: 0.647977, acc.: 69.53%] [G loss: 0.864897]\n",
      "epoch:12 step:11277 [D loss: 0.667035, acc.: 58.59%] [G loss: 0.926999]\n",
      "epoch:12 step:11278 [D loss: 0.647386, acc.: 64.06%] [G loss: 0.887172]\n",
      "epoch:12 step:11279 [D loss: 0.663847, acc.: 63.28%] [G loss: 0.779188]\n",
      "epoch:12 step:11280 [D loss: 0.615749, acc.: 71.09%] [G loss: 0.913655]\n",
      "epoch:12 step:11281 [D loss: 0.681115, acc.: 60.94%] [G loss: 0.792475]\n",
      "epoch:12 step:11282 [D loss: 0.667734, acc.: 59.38%] [G loss: 0.909674]\n",
      "epoch:12 step:11283 [D loss: 0.651227, acc.: 66.41%] [G loss: 0.736710]\n",
      "epoch:12 step:11284 [D loss: 0.631935, acc.: 65.62%] [G loss: 0.772311]\n",
      "epoch:12 step:11285 [D loss: 0.668580, acc.: 60.16%] [G loss: 0.921393]\n",
      "epoch:12 step:11286 [D loss: 0.702033, acc.: 46.88%] [G loss: 0.812154]\n",
      "epoch:12 step:11287 [D loss: 0.676730, acc.: 61.72%] [G loss: 0.769595]\n",
      "epoch:12 step:11288 [D loss: 0.688306, acc.: 47.66%] [G loss: 0.807239]\n",
      "epoch:12 step:11289 [D loss: 0.660653, acc.: 56.25%] [G loss: 0.835865]\n",
      "epoch:12 step:11290 [D loss: 0.723871, acc.: 53.12%] [G loss: 0.743219]\n",
      "epoch:12 step:11291 [D loss: 0.702771, acc.: 51.56%] [G loss: 0.836438]\n",
      "epoch:12 step:11292 [D loss: 0.714110, acc.: 52.34%] [G loss: 0.808357]\n",
      "epoch:12 step:11293 [D loss: 0.630262, acc.: 74.22%] [G loss: 0.726694]\n",
      "epoch:12 step:11294 [D loss: 0.615685, acc.: 64.84%] [G loss: 0.728069]\n",
      "epoch:12 step:11295 [D loss: 0.679022, acc.: 53.91%] [G loss: 0.840003]\n",
      "epoch:12 step:11296 [D loss: 0.663795, acc.: 53.91%] [G loss: 0.906316]\n",
      "epoch:12 step:11297 [D loss: 0.617046, acc.: 60.94%] [G loss: 0.856126]\n",
      "epoch:12 step:11298 [D loss: 0.697528, acc.: 58.59%] [G loss: 0.968837]\n",
      "epoch:12 step:11299 [D loss: 0.624536, acc.: 64.84%] [G loss: 0.902750]\n",
      "epoch:12 step:11300 [D loss: 0.681636, acc.: 55.47%] [G loss: 0.854278]\n",
      "epoch:12 step:11301 [D loss: 0.641822, acc.: 61.72%] [G loss: 0.958569]\n",
      "epoch:12 step:11302 [D loss: 0.680767, acc.: 60.16%] [G loss: 0.849036]\n",
      "epoch:12 step:11303 [D loss: 0.654457, acc.: 63.28%] [G loss: 0.780794]\n",
      "epoch:12 step:11304 [D loss: 0.723537, acc.: 47.66%] [G loss: 0.825111]\n",
      "epoch:12 step:11305 [D loss: 0.630535, acc.: 62.50%] [G loss: 0.863219]\n",
      "epoch:12 step:11306 [D loss: 0.629635, acc.: 61.72%] [G loss: 0.866815]\n",
      "epoch:12 step:11307 [D loss: 0.671113, acc.: 59.38%] [G loss: 0.851322]\n",
      "epoch:12 step:11308 [D loss: 0.619871, acc.: 68.75%] [G loss: 0.860129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11309 [D loss: 0.643994, acc.: 60.94%] [G loss: 0.929195]\n",
      "epoch:12 step:11310 [D loss: 0.695957, acc.: 59.38%] [G loss: 0.838956]\n",
      "epoch:12 step:11311 [D loss: 0.668398, acc.: 56.25%] [G loss: 0.911342]\n",
      "epoch:12 step:11312 [D loss: 0.657187, acc.: 60.94%] [G loss: 0.868614]\n",
      "epoch:12 step:11313 [D loss: 0.599646, acc.: 74.22%] [G loss: 0.965249]\n",
      "epoch:12 step:11314 [D loss: 0.639664, acc.: 65.62%] [G loss: 0.839584]\n",
      "epoch:12 step:11315 [D loss: 0.706934, acc.: 47.66%] [G loss: 1.022648]\n",
      "epoch:12 step:11316 [D loss: 0.703412, acc.: 44.53%] [G loss: 0.890238]\n",
      "epoch:12 step:11317 [D loss: 0.662008, acc.: 60.16%] [G loss: 0.915016]\n",
      "epoch:12 step:11318 [D loss: 0.625010, acc.: 70.31%] [G loss: 0.889029]\n",
      "epoch:12 step:11319 [D loss: 0.657848, acc.: 64.84%] [G loss: 0.703406]\n",
      "epoch:12 step:11320 [D loss: 0.717986, acc.: 45.31%] [G loss: 0.791606]\n",
      "epoch:12 step:11321 [D loss: 0.712105, acc.: 46.88%] [G loss: 0.800024]\n",
      "epoch:12 step:11322 [D loss: 0.695804, acc.: 53.91%] [G loss: 0.872907]\n",
      "epoch:12 step:11323 [D loss: 0.667910, acc.: 59.38%] [G loss: 0.799592]\n",
      "epoch:12 step:11324 [D loss: 0.705391, acc.: 50.78%] [G loss: 0.766436]\n",
      "epoch:12 step:11325 [D loss: 0.694928, acc.: 48.44%] [G loss: 0.800674]\n",
      "epoch:12 step:11326 [D loss: 0.643055, acc.: 63.28%] [G loss: 0.767913]\n",
      "epoch:12 step:11327 [D loss: 0.663971, acc.: 54.69%] [G loss: 0.839856]\n",
      "epoch:12 step:11328 [D loss: 0.640330, acc.: 61.72%] [G loss: 0.784526]\n",
      "epoch:12 step:11329 [D loss: 0.633964, acc.: 64.84%] [G loss: 0.865756]\n",
      "epoch:12 step:11330 [D loss: 0.635903, acc.: 63.28%] [G loss: 0.945837]\n",
      "epoch:12 step:11331 [D loss: 0.641931, acc.: 67.19%] [G loss: 0.904202]\n",
      "epoch:12 step:11332 [D loss: 0.633935, acc.: 65.62%] [G loss: 0.782874]\n",
      "epoch:12 step:11333 [D loss: 0.622098, acc.: 71.88%] [G loss: 0.786644]\n",
      "epoch:12 step:11334 [D loss: 0.703714, acc.: 46.88%] [G loss: 0.880445]\n",
      "epoch:12 step:11335 [D loss: 0.687266, acc.: 46.88%] [G loss: 0.899207]\n",
      "epoch:12 step:11336 [D loss: 0.690369, acc.: 54.69%] [G loss: 0.969739]\n",
      "epoch:12 step:11337 [D loss: 0.656143, acc.: 59.38%] [G loss: 0.875052]\n",
      "epoch:12 step:11338 [D loss: 0.638863, acc.: 60.94%] [G loss: 0.865018]\n",
      "epoch:12 step:11339 [D loss: 0.769037, acc.: 33.59%] [G loss: 0.787724]\n",
      "epoch:12 step:11340 [D loss: 0.717280, acc.: 43.75%] [G loss: 0.753073]\n",
      "epoch:12 step:11341 [D loss: 0.632354, acc.: 58.59%] [G loss: 0.749547]\n",
      "epoch:12 step:11342 [D loss: 0.719833, acc.: 50.00%] [G loss: 0.840524]\n",
      "epoch:12 step:11343 [D loss: 0.666139, acc.: 58.59%] [G loss: 0.848603]\n",
      "epoch:12 step:11344 [D loss: 0.650989, acc.: 64.06%] [G loss: 0.802573]\n",
      "epoch:12 step:11345 [D loss: 0.682225, acc.: 46.88%] [G loss: 0.823873]\n",
      "epoch:12 step:11346 [D loss: 0.588338, acc.: 73.44%] [G loss: 0.762099]\n",
      "epoch:12 step:11347 [D loss: 0.709995, acc.: 53.91%] [G loss: 0.944885]\n",
      "epoch:12 step:11348 [D loss: 0.695789, acc.: 58.59%] [G loss: 0.787450]\n",
      "epoch:12 step:11349 [D loss: 0.659176, acc.: 60.16%] [G loss: 0.879172]\n",
      "epoch:12 step:11350 [D loss: 0.677112, acc.: 58.59%] [G loss: 0.742655]\n",
      "epoch:12 step:11351 [D loss: 0.717982, acc.: 46.09%] [G loss: 0.792532]\n",
      "epoch:12 step:11352 [D loss: 0.658389, acc.: 61.72%] [G loss: 0.814199]\n",
      "epoch:12 step:11353 [D loss: 0.695513, acc.: 46.88%] [G loss: 0.824948]\n",
      "epoch:12 step:11354 [D loss: 0.671254, acc.: 54.69%] [G loss: 0.796876]\n",
      "epoch:12 step:11355 [D loss: 0.697516, acc.: 49.22%] [G loss: 0.929931]\n",
      "epoch:12 step:11356 [D loss: 0.657618, acc.: 64.84%] [G loss: 0.751918]\n",
      "epoch:12 step:11357 [D loss: 0.669267, acc.: 54.69%] [G loss: 0.766832]\n",
      "epoch:12 step:11358 [D loss: 0.642667, acc.: 67.19%] [G loss: 0.834691]\n",
      "epoch:12 step:11359 [D loss: 0.675745, acc.: 61.72%] [G loss: 0.794475]\n",
      "epoch:12 step:11360 [D loss: 0.649471, acc.: 58.59%] [G loss: 0.805417]\n",
      "epoch:12 step:11361 [D loss: 0.635239, acc.: 64.06%] [G loss: 0.829175]\n",
      "epoch:12 step:11362 [D loss: 0.684395, acc.: 58.59%] [G loss: 0.802383]\n",
      "epoch:12 step:11363 [D loss: 0.607089, acc.: 65.62%] [G loss: 0.855873]\n",
      "epoch:12 step:11364 [D loss: 0.679533, acc.: 55.47%] [G loss: 0.764831]\n",
      "epoch:12 step:11365 [D loss: 0.686605, acc.: 53.91%] [G loss: 0.751217]\n",
      "epoch:12 step:11366 [D loss: 0.697685, acc.: 53.91%] [G loss: 0.758098]\n",
      "epoch:12 step:11367 [D loss: 0.658443, acc.: 60.16%] [G loss: 0.769072]\n",
      "epoch:12 step:11368 [D loss: 0.693371, acc.: 49.22%] [G loss: 0.805293]\n",
      "epoch:12 step:11369 [D loss: 0.660061, acc.: 62.50%] [G loss: 0.784855]\n",
      "epoch:12 step:11370 [D loss: 0.689900, acc.: 50.00%] [G loss: 0.855115]\n",
      "epoch:12 step:11371 [D loss: 0.666736, acc.: 54.69%] [G loss: 0.847592]\n",
      "epoch:12 step:11372 [D loss: 0.708599, acc.: 50.78%] [G loss: 0.849278]\n",
      "epoch:12 step:11373 [D loss: 0.662441, acc.: 58.59%] [G loss: 0.858851]\n",
      "epoch:12 step:11374 [D loss: 0.668233, acc.: 60.16%] [G loss: 0.836921]\n",
      "epoch:12 step:11375 [D loss: 0.638572, acc.: 64.06%] [G loss: 0.779077]\n",
      "epoch:12 step:11376 [D loss: 0.733150, acc.: 39.84%] [G loss: 0.827123]\n",
      "epoch:12 step:11377 [D loss: 0.716516, acc.: 50.78%] [G loss: 0.858913]\n",
      "epoch:12 step:11378 [D loss: 0.644774, acc.: 64.84%] [G loss: 0.874341]\n",
      "epoch:12 step:11379 [D loss: 0.688231, acc.: 55.47%] [G loss: 0.918436]\n",
      "epoch:12 step:11380 [D loss: 0.710793, acc.: 50.00%] [G loss: 0.887289]\n",
      "epoch:12 step:11381 [D loss: 0.687402, acc.: 53.12%] [G loss: 0.860270]\n",
      "epoch:12 step:11382 [D loss: 0.677642, acc.: 56.25%] [G loss: 0.922714]\n",
      "epoch:12 step:11383 [D loss: 0.690499, acc.: 53.91%] [G loss: 0.937314]\n",
      "epoch:12 step:11384 [D loss: 0.687445, acc.: 54.69%] [G loss: 0.812228]\n",
      "epoch:12 step:11385 [D loss: 0.737661, acc.: 50.00%] [G loss: 0.786194]\n",
      "epoch:12 step:11386 [D loss: 0.684964, acc.: 54.69%] [G loss: 0.806500]\n",
      "epoch:12 step:11387 [D loss: 0.695556, acc.: 51.56%] [G loss: 0.827333]\n",
      "epoch:12 step:11388 [D loss: 0.668229, acc.: 57.81%] [G loss: 0.810045]\n",
      "epoch:12 step:11389 [D loss: 0.663245, acc.: 67.19%] [G loss: 0.739810]\n",
      "epoch:12 step:11390 [D loss: 0.697495, acc.: 59.38%] [G loss: 0.851575]\n",
      "epoch:12 step:11391 [D loss: 0.655004, acc.: 60.94%] [G loss: 0.765783]\n",
      "epoch:12 step:11392 [D loss: 0.655202, acc.: 56.25%] [G loss: 0.831292]\n",
      "epoch:12 step:11393 [D loss: 0.692639, acc.: 52.34%] [G loss: 0.832753]\n",
      "epoch:12 step:11394 [D loss: 0.722357, acc.: 49.22%] [G loss: 0.992384]\n",
      "epoch:12 step:11395 [D loss: 0.712909, acc.: 51.56%] [G loss: 0.840729]\n",
      "epoch:12 step:11396 [D loss: 0.683378, acc.: 53.91%] [G loss: 0.817199]\n",
      "epoch:12 step:11397 [D loss: 0.636331, acc.: 63.28%] [G loss: 0.834520]\n",
      "epoch:12 step:11398 [D loss: 0.687270, acc.: 51.56%] [G loss: 0.789928]\n",
      "epoch:12 step:11399 [D loss: 0.716362, acc.: 43.75%] [G loss: 0.843583]\n",
      "epoch:12 step:11400 [D loss: 0.678236, acc.: 57.81%] [G loss: 0.862317]\n",
      "epoch:12 step:11401 [D loss: 0.698504, acc.: 50.00%] [G loss: 0.832493]\n",
      "epoch:12 step:11402 [D loss: 0.652377, acc.: 60.16%] [G loss: 0.894129]\n",
      "epoch:12 step:11403 [D loss: 0.651525, acc.: 60.94%] [G loss: 0.790364]\n",
      "epoch:12 step:11404 [D loss: 0.693997, acc.: 60.94%] [G loss: 0.732892]\n",
      "epoch:12 step:11405 [D loss: 0.683298, acc.: 57.03%] [G loss: 0.776246]\n",
      "epoch:12 step:11406 [D loss: 0.662059, acc.: 61.72%] [G loss: 0.830193]\n",
      "epoch:12 step:11407 [D loss: 0.685832, acc.: 54.69%] [G loss: 0.727445]\n",
      "epoch:12 step:11408 [D loss: 0.718426, acc.: 45.31%] [G loss: 0.802704]\n",
      "epoch:12 step:11409 [D loss: 0.671070, acc.: 62.50%] [G loss: 0.728925]\n",
      "epoch:12 step:11410 [D loss: 0.672731, acc.: 59.38%] [G loss: 0.783873]\n",
      "epoch:12 step:11411 [D loss: 0.711641, acc.: 46.88%] [G loss: 0.749633]\n",
      "epoch:12 step:11412 [D loss: 0.663806, acc.: 60.16%] [G loss: 0.758753]\n",
      "epoch:12 step:11413 [D loss: 0.688476, acc.: 52.34%] [G loss: 0.807006]\n",
      "epoch:12 step:11414 [D loss: 0.660849, acc.: 64.84%] [G loss: 0.800109]\n",
      "epoch:12 step:11415 [D loss: 0.719340, acc.: 50.78%] [G loss: 0.721646]\n",
      "epoch:12 step:11416 [D loss: 0.730350, acc.: 48.44%] [G loss: 0.792102]\n",
      "epoch:12 step:11417 [D loss: 0.685827, acc.: 52.34%] [G loss: 0.792954]\n",
      "epoch:12 step:11418 [D loss: 0.713473, acc.: 48.44%] [G loss: 0.756229]\n",
      "epoch:12 step:11419 [D loss: 0.707366, acc.: 53.91%] [G loss: 0.787860]\n",
      "epoch:12 step:11420 [D loss: 0.702443, acc.: 48.44%] [G loss: 0.798912]\n",
      "epoch:12 step:11421 [D loss: 0.674869, acc.: 59.38%] [G loss: 0.862650]\n",
      "epoch:12 step:11422 [D loss: 0.656370, acc.: 56.25%] [G loss: 0.839428]\n",
      "epoch:12 step:11423 [D loss: 0.637656, acc.: 64.06%] [G loss: 0.879360]\n",
      "epoch:12 step:11424 [D loss: 0.692252, acc.: 50.78%] [G loss: 0.960033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11425 [D loss: 0.702045, acc.: 46.88%] [G loss: 0.786685]\n",
      "epoch:12 step:11426 [D loss: 0.690856, acc.: 55.47%] [G loss: 0.764583]\n",
      "epoch:12 step:11427 [D loss: 0.642663, acc.: 62.50%] [G loss: 0.808515]\n",
      "epoch:12 step:11428 [D loss: 0.667247, acc.: 61.72%] [G loss: 0.807144]\n",
      "epoch:12 step:11429 [D loss: 0.703957, acc.: 51.56%] [G loss: 0.772241]\n",
      "epoch:12 step:11430 [D loss: 0.695774, acc.: 53.12%] [G loss: 0.733361]\n",
      "epoch:12 step:11431 [D loss: 0.712579, acc.: 39.06%] [G loss: 0.703967]\n",
      "epoch:12 step:11432 [D loss: 0.668085, acc.: 50.00%] [G loss: 0.811397]\n",
      "epoch:12 step:11433 [D loss: 0.647143, acc.: 62.50%] [G loss: 0.801881]\n",
      "epoch:12 step:11434 [D loss: 0.682705, acc.: 53.91%] [G loss: 0.785758]\n",
      "epoch:12 step:11435 [D loss: 0.686424, acc.: 52.34%] [G loss: 0.722414]\n",
      "epoch:12 step:11436 [D loss: 0.648276, acc.: 55.47%] [G loss: 0.755386]\n",
      "epoch:12 step:11437 [D loss: 0.680780, acc.: 60.16%] [G loss: 0.816331]\n",
      "epoch:12 step:11438 [D loss: 0.665531, acc.: 57.81%] [G loss: 0.805436]\n",
      "epoch:12 step:11439 [D loss: 0.661752, acc.: 60.94%] [G loss: 0.787954]\n",
      "epoch:12 step:11440 [D loss: 0.686843, acc.: 57.81%] [G loss: 0.799954]\n",
      "epoch:12 step:11441 [D loss: 0.638756, acc.: 63.28%] [G loss: 0.809465]\n",
      "epoch:12 step:11442 [D loss: 0.683751, acc.: 57.03%] [G loss: 0.803717]\n",
      "epoch:12 step:11443 [D loss: 0.689451, acc.: 53.12%] [G loss: 0.817339]\n",
      "epoch:12 step:11444 [D loss: 0.672677, acc.: 50.00%] [G loss: 0.803948]\n",
      "epoch:12 step:11445 [D loss: 0.668209, acc.: 55.47%] [G loss: 0.780821]\n",
      "epoch:12 step:11446 [D loss: 0.694376, acc.: 52.34%] [G loss: 0.797827]\n",
      "epoch:12 step:11447 [D loss: 0.665162, acc.: 59.38%] [G loss: 0.793284]\n",
      "epoch:12 step:11448 [D loss: 0.685115, acc.: 50.00%] [G loss: 0.816912]\n",
      "epoch:12 step:11449 [D loss: 0.671519, acc.: 60.16%] [G loss: 0.860320]\n",
      "epoch:12 step:11450 [D loss: 0.688436, acc.: 57.81%] [G loss: 0.843854]\n",
      "epoch:12 step:11451 [D loss: 0.693541, acc.: 55.47%] [G loss: 0.807357]\n",
      "epoch:12 step:11452 [D loss: 0.659019, acc.: 59.38%] [G loss: 0.841494]\n",
      "epoch:12 step:11453 [D loss: 0.704971, acc.: 48.44%] [G loss: 0.782474]\n",
      "epoch:12 step:11454 [D loss: 0.638778, acc.: 60.16%] [G loss: 0.864732]\n",
      "epoch:12 step:11455 [D loss: 0.675815, acc.: 57.81%] [G loss: 0.831423]\n",
      "epoch:12 step:11456 [D loss: 0.661901, acc.: 63.28%] [G loss: 0.872384]\n",
      "epoch:12 step:11457 [D loss: 0.676974, acc.: 55.47%] [G loss: 0.804941]\n",
      "epoch:12 step:11458 [D loss: 0.671268, acc.: 57.81%] [G loss: 0.820578]\n",
      "epoch:12 step:11459 [D loss: 0.677210, acc.: 53.91%] [G loss: 0.785629]\n",
      "epoch:12 step:11460 [D loss: 0.666013, acc.: 56.25%] [G loss: 0.825447]\n",
      "epoch:12 step:11461 [D loss: 0.713919, acc.: 45.31%] [G loss: 0.889885]\n",
      "epoch:12 step:11462 [D loss: 0.718268, acc.: 46.09%] [G loss: 0.858802]\n",
      "epoch:12 step:11463 [D loss: 0.643057, acc.: 63.28%] [G loss: 0.796344]\n",
      "epoch:12 step:11464 [D loss: 0.688548, acc.: 51.56%] [G loss: 0.814090]\n",
      "epoch:12 step:11465 [D loss: 0.656356, acc.: 58.59%] [G loss: 0.783777]\n",
      "epoch:12 step:11466 [D loss: 0.705131, acc.: 51.56%] [G loss: 0.735761]\n",
      "epoch:12 step:11467 [D loss: 0.713843, acc.: 46.88%] [G loss: 0.765202]\n",
      "epoch:12 step:11468 [D loss: 0.666496, acc.: 60.16%] [G loss: 0.785813]\n",
      "epoch:12 step:11469 [D loss: 0.724393, acc.: 39.84%] [G loss: 0.777104]\n",
      "epoch:12 step:11470 [D loss: 0.689089, acc.: 52.34%] [G loss: 0.815401]\n",
      "epoch:12 step:11471 [D loss: 0.673011, acc.: 60.16%] [G loss: 0.766577]\n",
      "epoch:12 step:11472 [D loss: 0.686028, acc.: 56.25%] [G loss: 0.752312]\n",
      "epoch:12 step:11473 [D loss: 0.688637, acc.: 54.69%] [G loss: 0.779344]\n",
      "epoch:12 step:11474 [D loss: 0.705775, acc.: 50.78%] [G loss: 0.817178]\n",
      "epoch:12 step:11475 [D loss: 0.667220, acc.: 53.91%] [G loss: 0.766460]\n",
      "epoch:12 step:11476 [D loss: 0.680195, acc.: 53.12%] [G loss: 0.811719]\n",
      "epoch:12 step:11477 [D loss: 0.695624, acc.: 53.12%] [G loss: 0.772083]\n",
      "epoch:12 step:11478 [D loss: 0.674098, acc.: 60.94%] [G loss: 0.760376]\n",
      "epoch:12 step:11479 [D loss: 0.711757, acc.: 40.62%] [G loss: 0.798689]\n",
      "epoch:12 step:11480 [D loss: 0.688814, acc.: 57.03%] [G loss: 0.780753]\n",
      "epoch:12 step:11481 [D loss: 0.645209, acc.: 64.06%] [G loss: 0.833005]\n",
      "epoch:12 step:11482 [D loss: 0.723300, acc.: 47.66%] [G loss: 0.733007]\n",
      "epoch:12 step:11483 [D loss: 0.660885, acc.: 60.94%] [G loss: 0.797038]\n",
      "epoch:12 step:11484 [D loss: 0.679559, acc.: 54.69%] [G loss: 0.876243]\n",
      "epoch:12 step:11485 [D loss: 0.668270, acc.: 60.16%] [G loss: 0.899162]\n",
      "epoch:12 step:11486 [D loss: 0.652254, acc.: 64.06%] [G loss: 0.805412]\n",
      "epoch:12 step:11487 [D loss: 0.670632, acc.: 57.81%] [G loss: 0.869969]\n",
      "epoch:12 step:11488 [D loss: 0.692125, acc.: 57.03%] [G loss: 0.809701]\n",
      "epoch:12 step:11489 [D loss: 0.674742, acc.: 57.03%] [G loss: 0.832190]\n",
      "epoch:12 step:11490 [D loss: 0.721107, acc.: 48.44%] [G loss: 0.771307]\n",
      "epoch:12 step:11491 [D loss: 0.676044, acc.: 52.34%] [G loss: 0.883573]\n",
      "epoch:12 step:11492 [D loss: 0.679006, acc.: 57.81%] [G loss: 0.815969]\n",
      "epoch:12 step:11493 [D loss: 0.639452, acc.: 63.28%] [G loss: 0.787478]\n",
      "epoch:12 step:11494 [D loss: 0.705679, acc.: 42.19%] [G loss: 0.885849]\n",
      "epoch:12 step:11495 [D loss: 0.678702, acc.: 53.91%] [G loss: 0.818294]\n",
      "epoch:12 step:11496 [D loss: 0.706559, acc.: 49.22%] [G loss: 0.794663]\n",
      "epoch:12 step:11497 [D loss: 0.672289, acc.: 57.81%] [G loss: 0.796457]\n",
      "epoch:12 step:11498 [D loss: 0.685230, acc.: 60.16%] [G loss: 0.786101]\n",
      "epoch:12 step:11499 [D loss: 0.682375, acc.: 57.81%] [G loss: 0.815357]\n",
      "epoch:12 step:11500 [D loss: 0.723240, acc.: 40.62%] [G loss: 0.764191]\n",
      "epoch:12 step:11501 [D loss: 0.689791, acc.: 51.56%] [G loss: 0.811980]\n",
      "epoch:12 step:11502 [D loss: 0.676326, acc.: 54.69%] [G loss: 0.779152]\n",
      "epoch:12 step:11503 [D loss: 0.652849, acc.: 66.41%] [G loss: 0.812823]\n",
      "epoch:12 step:11504 [D loss: 0.633965, acc.: 66.41%] [G loss: 0.831288]\n",
      "epoch:12 step:11505 [D loss: 0.658950, acc.: 58.59%] [G loss: 0.813169]\n",
      "epoch:12 step:11506 [D loss: 0.664317, acc.: 62.50%] [G loss: 0.802759]\n",
      "epoch:12 step:11507 [D loss: 0.623852, acc.: 70.31%] [G loss: 0.873304]\n",
      "epoch:12 step:11508 [D loss: 0.666205, acc.: 59.38%] [G loss: 0.839629]\n",
      "epoch:12 step:11509 [D loss: 0.681218, acc.: 55.47%] [G loss: 0.808089]\n",
      "epoch:12 step:11510 [D loss: 0.640580, acc.: 64.06%] [G loss: 0.925022]\n",
      "epoch:12 step:11511 [D loss: 0.717542, acc.: 44.53%] [G loss: 0.865608]\n",
      "epoch:12 step:11512 [D loss: 0.682023, acc.: 53.91%] [G loss: 0.797419]\n",
      "epoch:12 step:11513 [D loss: 0.704458, acc.: 53.91%] [G loss: 0.784964]\n",
      "epoch:12 step:11514 [D loss: 0.642091, acc.: 60.94%] [G loss: 0.897095]\n",
      "epoch:12 step:11515 [D loss: 0.632415, acc.: 68.75%] [G loss: 0.817281]\n",
      "epoch:12 step:11516 [D loss: 0.650882, acc.: 65.62%] [G loss: 0.789870]\n",
      "epoch:12 step:11517 [D loss: 0.641158, acc.: 65.62%] [G loss: 0.810667]\n",
      "epoch:12 step:11518 [D loss: 0.703576, acc.: 52.34%] [G loss: 0.825948]\n",
      "epoch:12 step:11519 [D loss: 0.685002, acc.: 54.69%] [G loss: 0.861110]\n",
      "epoch:12 step:11520 [D loss: 0.695526, acc.: 52.34%] [G loss: 0.863259]\n",
      "epoch:12 step:11521 [D loss: 0.676431, acc.: 61.72%] [G loss: 0.831363]\n",
      "epoch:12 step:11522 [D loss: 0.702041, acc.: 46.09%] [G loss: 0.723395]\n",
      "epoch:12 step:11523 [D loss: 0.638441, acc.: 62.50%] [G loss: 0.923663]\n",
      "epoch:12 step:11524 [D loss: 0.690212, acc.: 46.88%] [G loss: 0.795236]\n",
      "epoch:12 step:11525 [D loss: 0.706394, acc.: 53.91%] [G loss: 0.768923]\n",
      "epoch:12 step:11526 [D loss: 0.689540, acc.: 52.34%] [G loss: 0.797629]\n",
      "epoch:12 step:11527 [D loss: 0.664723, acc.: 61.72%] [G loss: 0.807838]\n",
      "epoch:12 step:11528 [D loss: 0.697072, acc.: 49.22%] [G loss: 0.815262]\n",
      "epoch:12 step:11529 [D loss: 0.681795, acc.: 57.03%] [G loss: 0.755291]\n",
      "epoch:12 step:11530 [D loss: 0.653844, acc.: 62.50%] [G loss: 0.813587]\n",
      "epoch:12 step:11531 [D loss: 0.693586, acc.: 49.22%] [G loss: 0.777255]\n",
      "epoch:12 step:11532 [D loss: 0.700133, acc.: 53.12%] [G loss: 0.813055]\n",
      "epoch:12 step:11533 [D loss: 0.691501, acc.: 50.78%] [G loss: 0.748874]\n",
      "epoch:12 step:11534 [D loss: 0.638275, acc.: 68.75%] [G loss: 0.773760]\n",
      "epoch:12 step:11535 [D loss: 0.677199, acc.: 56.25%] [G loss: 0.799149]\n",
      "epoch:12 step:11536 [D loss: 0.666153, acc.: 57.03%] [G loss: 0.822633]\n",
      "epoch:12 step:11537 [D loss: 0.695801, acc.: 48.44%] [G loss: 0.756998]\n",
      "epoch:12 step:11538 [D loss: 0.689656, acc.: 45.31%] [G loss: 0.794132]\n",
      "epoch:12 step:11539 [D loss: 0.752081, acc.: 47.66%] [G loss: 0.766553]\n",
      "epoch:12 step:11540 [D loss: 0.674892, acc.: 57.03%] [G loss: 0.886323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11541 [D loss: 0.681256, acc.: 53.91%] [G loss: 0.836777]\n",
      "epoch:12 step:11542 [D loss: 0.706626, acc.: 51.56%] [G loss: 0.808882]\n",
      "epoch:12 step:11543 [D loss: 0.682520, acc.: 57.03%] [G loss: 0.770442]\n",
      "epoch:12 step:11544 [D loss: 0.691320, acc.: 55.47%] [G loss: 0.785366]\n",
      "epoch:12 step:11545 [D loss: 0.673682, acc.: 53.12%] [G loss: 0.815542]\n",
      "epoch:12 step:11546 [D loss: 0.699044, acc.: 47.66%] [G loss: 0.796949]\n",
      "epoch:12 step:11547 [D loss: 0.660805, acc.: 60.16%] [G loss: 0.834759]\n",
      "epoch:12 step:11548 [D loss: 0.725632, acc.: 40.62%] [G loss: 0.821702]\n",
      "epoch:12 step:11549 [D loss: 0.700440, acc.: 53.91%] [G loss: 0.815405]\n",
      "epoch:12 step:11550 [D loss: 0.641044, acc.: 65.62%] [G loss: 0.786853]\n",
      "epoch:12 step:11551 [D loss: 0.664771, acc.: 60.16%] [G loss: 0.730893]\n",
      "epoch:12 step:11552 [D loss: 0.692576, acc.: 59.38%] [G loss: 0.857416]\n",
      "epoch:12 step:11553 [D loss: 0.693397, acc.: 49.22%] [G loss: 0.857604]\n",
      "epoch:12 step:11554 [D loss: 0.621001, acc.: 64.84%] [G loss: 0.805740]\n",
      "epoch:12 step:11555 [D loss: 0.667018, acc.: 53.91%] [G loss: 0.850710]\n",
      "epoch:12 step:11556 [D loss: 0.648819, acc.: 54.69%] [G loss: 0.801885]\n",
      "epoch:12 step:11557 [D loss: 0.645826, acc.: 61.72%] [G loss: 0.840651]\n",
      "epoch:12 step:11558 [D loss: 0.676060, acc.: 52.34%] [G loss: 0.848975]\n",
      "epoch:12 step:11559 [D loss: 0.687274, acc.: 53.91%] [G loss: 0.777605]\n",
      "epoch:12 step:11560 [D loss: 0.736571, acc.: 44.53%] [G loss: 0.826130]\n",
      "epoch:12 step:11561 [D loss: 0.664081, acc.: 60.16%] [G loss: 0.763423]\n",
      "epoch:12 step:11562 [D loss: 0.687116, acc.: 52.34%] [G loss: 0.832768]\n",
      "epoch:12 step:11563 [D loss: 0.670922, acc.: 57.81%] [G loss: 0.842490]\n",
      "epoch:12 step:11564 [D loss: 0.681764, acc.: 58.59%] [G loss: 0.793600]\n",
      "epoch:12 step:11565 [D loss: 0.750946, acc.: 32.81%] [G loss: 0.859584]\n",
      "epoch:12 step:11566 [D loss: 0.644111, acc.: 64.06%] [G loss: 0.860376]\n",
      "epoch:12 step:11567 [D loss: 0.661743, acc.: 63.28%] [G loss: 0.808033]\n",
      "epoch:12 step:11568 [D loss: 0.659013, acc.: 61.72%] [G loss: 0.889234]\n",
      "epoch:12 step:11569 [D loss: 0.715526, acc.: 46.09%] [G loss: 0.811174]\n",
      "epoch:12 step:11570 [D loss: 0.710329, acc.: 44.53%] [G loss: 0.830054]\n",
      "epoch:12 step:11571 [D loss: 0.652453, acc.: 67.97%] [G loss: 0.871037]\n",
      "epoch:12 step:11572 [D loss: 0.633750, acc.: 61.72%] [G loss: 0.842900]\n",
      "epoch:12 step:11573 [D loss: 0.642899, acc.: 64.06%] [G loss: 0.836650]\n",
      "epoch:12 step:11574 [D loss: 0.658237, acc.: 56.25%] [G loss: 0.840026]\n",
      "epoch:12 step:11575 [D loss: 0.680401, acc.: 62.50%] [G loss: 0.805852]\n",
      "epoch:12 step:11576 [D loss: 0.616516, acc.: 68.75%] [G loss: 0.792680]\n",
      "epoch:12 step:11577 [D loss: 0.647594, acc.: 61.72%] [G loss: 0.808233]\n",
      "epoch:12 step:11578 [D loss: 0.673174, acc.: 56.25%] [G loss: 0.804813]\n",
      "epoch:12 step:11579 [D loss: 0.638899, acc.: 61.72%] [G loss: 0.903798]\n",
      "epoch:12 step:11580 [D loss: 0.651499, acc.: 61.72%] [G loss: 0.861906]\n",
      "epoch:12 step:11581 [D loss: 0.735124, acc.: 47.66%] [G loss: 0.902947]\n",
      "epoch:12 step:11582 [D loss: 0.673388, acc.: 57.81%] [G loss: 0.848583]\n",
      "epoch:12 step:11583 [D loss: 0.734170, acc.: 42.97%] [G loss: 0.813113]\n",
      "epoch:12 step:11584 [D loss: 0.661510, acc.: 55.47%] [G loss: 0.825535]\n",
      "epoch:12 step:11585 [D loss: 0.670450, acc.: 52.34%] [G loss: 0.859029]\n",
      "epoch:12 step:11586 [D loss: 0.678252, acc.: 54.69%] [G loss: 0.833547]\n",
      "epoch:12 step:11587 [D loss: 0.642570, acc.: 62.50%] [G loss: 0.798322]\n",
      "epoch:12 step:11588 [D loss: 0.647353, acc.: 61.72%] [G loss: 0.747324]\n",
      "epoch:12 step:11589 [D loss: 0.735079, acc.: 47.66%] [G loss: 0.912195]\n",
      "epoch:12 step:11590 [D loss: 0.668531, acc.: 59.38%] [G loss: 0.722962]\n",
      "epoch:12 step:11591 [D loss: 0.680471, acc.: 55.47%] [G loss: 0.775099]\n",
      "epoch:12 step:11592 [D loss: 0.664560, acc.: 60.94%] [G loss: 0.821430]\n",
      "epoch:12 step:11593 [D loss: 0.643830, acc.: 67.97%] [G loss: 0.785137]\n",
      "epoch:12 step:11594 [D loss: 0.681199, acc.: 54.69%] [G loss: 0.759709]\n",
      "epoch:12 step:11595 [D loss: 0.730573, acc.: 42.19%] [G loss: 0.803956]\n",
      "epoch:12 step:11596 [D loss: 0.684337, acc.: 47.66%] [G loss: 0.856334]\n",
      "epoch:12 step:11597 [D loss: 0.740784, acc.: 50.00%] [G loss: 0.733852]\n",
      "epoch:12 step:11598 [D loss: 0.693477, acc.: 56.25%] [G loss: 0.813012]\n",
      "epoch:12 step:11599 [D loss: 0.683992, acc.: 53.91%] [G loss: 0.782303]\n",
      "epoch:12 step:11600 [D loss: 0.758515, acc.: 37.50%] [G loss: 0.852021]\n",
      "epoch:12 step:11601 [D loss: 0.663871, acc.: 53.12%] [G loss: 0.796764]\n",
      "epoch:12 step:11602 [D loss: 0.670193, acc.: 60.16%] [G loss: 0.768037]\n",
      "epoch:12 step:11603 [D loss: 0.690191, acc.: 57.03%] [G loss: 0.871125]\n",
      "epoch:12 step:11604 [D loss: 0.702551, acc.: 50.78%] [G loss: 1.006803]\n",
      "epoch:12 step:11605 [D loss: 0.686781, acc.: 57.81%] [G loss: 0.884697]\n",
      "epoch:12 step:11606 [D loss: 0.690484, acc.: 52.34%] [G loss: 0.780372]\n",
      "epoch:12 step:11607 [D loss: 0.657980, acc.: 61.72%] [G loss: 0.846790]\n",
      "epoch:12 step:11608 [D loss: 0.705445, acc.: 53.12%] [G loss: 0.846896]\n",
      "epoch:12 step:11609 [D loss: 0.653204, acc.: 56.25%] [G loss: 0.798447]\n",
      "epoch:12 step:11610 [D loss: 0.721964, acc.: 50.78%] [G loss: 0.843426]\n",
      "epoch:12 step:11611 [D loss: 0.659391, acc.: 57.81%] [G loss: 0.836283]\n",
      "epoch:12 step:11612 [D loss: 0.707100, acc.: 51.56%] [G loss: 0.820754]\n",
      "epoch:12 step:11613 [D loss: 0.670842, acc.: 60.16%] [G loss: 0.846614]\n",
      "epoch:12 step:11614 [D loss: 0.671327, acc.: 55.47%] [G loss: 0.773738]\n",
      "epoch:12 step:11615 [D loss: 0.663597, acc.: 55.47%] [G loss: 0.805602]\n",
      "epoch:12 step:11616 [D loss: 0.692777, acc.: 52.34%] [G loss: 0.820049]\n",
      "epoch:12 step:11617 [D loss: 0.711040, acc.: 48.44%] [G loss: 0.808373]\n",
      "epoch:12 step:11618 [D loss: 0.706531, acc.: 50.00%] [G loss: 0.779717]\n",
      "epoch:12 step:11619 [D loss: 0.674827, acc.: 51.56%] [G loss: 0.816033]\n",
      "epoch:12 step:11620 [D loss: 0.645135, acc.: 61.72%] [G loss: 0.818892]\n",
      "epoch:12 step:11621 [D loss: 0.679229, acc.: 55.47%] [G loss: 0.783376]\n",
      "epoch:12 step:11622 [D loss: 0.697541, acc.: 48.44%] [G loss: 0.876050]\n",
      "epoch:12 step:11623 [D loss: 0.679142, acc.: 60.16%] [G loss: 0.762272]\n",
      "epoch:12 step:11624 [D loss: 0.704710, acc.: 47.66%] [G loss: 0.818788]\n",
      "epoch:12 step:11625 [D loss: 0.723315, acc.: 45.31%] [G loss: 0.801358]\n",
      "epoch:12 step:11626 [D loss: 0.675941, acc.: 57.81%] [G loss: 0.814955]\n",
      "epoch:12 step:11627 [D loss: 0.688733, acc.: 54.69%] [G loss: 0.755274]\n",
      "epoch:12 step:11628 [D loss: 0.703013, acc.: 52.34%] [G loss: 0.849408]\n",
      "epoch:12 step:11629 [D loss: 0.697229, acc.: 50.00%] [G loss: 0.752644]\n",
      "epoch:12 step:11630 [D loss: 0.649454, acc.: 64.84%] [G loss: 0.762130]\n",
      "epoch:12 step:11631 [D loss: 0.671978, acc.: 59.38%] [G loss: 0.813176]\n",
      "epoch:12 step:11632 [D loss: 0.666336, acc.: 60.94%] [G loss: 0.797769]\n",
      "epoch:12 step:11633 [D loss: 0.721487, acc.: 42.19%] [G loss: 0.740765]\n",
      "epoch:12 step:11634 [D loss: 0.672811, acc.: 56.25%] [G loss: 0.770988]\n",
      "epoch:12 step:11635 [D loss: 0.676107, acc.: 52.34%] [G loss: 0.902633]\n",
      "epoch:12 step:11636 [D loss: 0.653386, acc.: 55.47%] [G loss: 0.772712]\n",
      "epoch:12 step:11637 [D loss: 0.682610, acc.: 55.47%] [G loss: 0.783816]\n",
      "epoch:12 step:11638 [D loss: 0.665952, acc.: 58.59%] [G loss: 0.814529]\n",
      "epoch:12 step:11639 [D loss: 0.672445, acc.: 53.12%] [G loss: 0.855209]\n",
      "epoch:12 step:11640 [D loss: 0.660146, acc.: 64.06%] [G loss: 0.855660]\n",
      "epoch:12 step:11641 [D loss: 0.688699, acc.: 60.16%] [G loss: 0.776659]\n",
      "epoch:12 step:11642 [D loss: 0.687610, acc.: 49.22%] [G loss: 0.821304]\n",
      "epoch:12 step:11643 [D loss: 0.654177, acc.: 57.03%] [G loss: 0.798044]\n",
      "epoch:12 step:11644 [D loss: 0.640347, acc.: 61.72%] [G loss: 0.865661]\n",
      "epoch:12 step:11645 [D loss: 0.654848, acc.: 57.03%] [G loss: 0.814927]\n",
      "epoch:12 step:11646 [D loss: 0.662890, acc.: 57.03%] [G loss: 0.858969]\n",
      "epoch:12 step:11647 [D loss: 0.638789, acc.: 69.53%] [G loss: 0.793560]\n",
      "epoch:12 step:11648 [D loss: 0.665352, acc.: 57.03%] [G loss: 0.799036]\n",
      "epoch:12 step:11649 [D loss: 0.690712, acc.: 59.38%] [G loss: 0.869934]\n",
      "epoch:12 step:11650 [D loss: 0.635099, acc.: 58.59%] [G loss: 0.824559]\n",
      "epoch:12 step:11651 [D loss: 0.665015, acc.: 57.03%] [G loss: 0.884854]\n",
      "epoch:12 step:11652 [D loss: 0.689083, acc.: 57.03%] [G loss: 0.789858]\n",
      "epoch:12 step:11653 [D loss: 0.701653, acc.: 50.78%] [G loss: 0.841567]\n",
      "epoch:12 step:11654 [D loss: 0.647487, acc.: 64.06%] [G loss: 0.873473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11655 [D loss: 0.686386, acc.: 51.56%] [G loss: 0.837383]\n",
      "epoch:12 step:11656 [D loss: 0.682922, acc.: 53.12%] [G loss: 0.797692]\n",
      "epoch:12 step:11657 [D loss: 0.691667, acc.: 47.66%] [G loss: 0.857489]\n",
      "epoch:12 step:11658 [D loss: 0.691849, acc.: 57.03%] [G loss: 0.839637]\n",
      "epoch:12 step:11659 [D loss: 0.644398, acc.: 60.16%] [G loss: 0.890179]\n",
      "epoch:12 step:11660 [D loss: 0.719411, acc.: 45.31%] [G loss: 0.809493]\n",
      "epoch:12 step:11661 [D loss: 0.672270, acc.: 51.56%] [G loss: 0.849948]\n",
      "epoch:12 step:11662 [D loss: 0.654215, acc.: 60.16%] [G loss: 0.809995]\n",
      "epoch:12 step:11663 [D loss: 0.689442, acc.: 53.91%] [G loss: 0.916691]\n",
      "epoch:12 step:11664 [D loss: 0.657871, acc.: 57.03%] [G loss: 0.871712]\n",
      "epoch:12 step:11665 [D loss: 0.676026, acc.: 57.03%] [G loss: 0.801451]\n",
      "epoch:12 step:11666 [D loss: 0.676820, acc.: 58.59%] [G loss: 0.809436]\n",
      "epoch:12 step:11667 [D loss: 0.690624, acc.: 54.69%] [G loss: 0.787095]\n",
      "epoch:12 step:11668 [D loss: 0.628366, acc.: 63.28%] [G loss: 0.793346]\n",
      "epoch:12 step:11669 [D loss: 0.676932, acc.: 57.81%] [G loss: 0.800534]\n",
      "epoch:12 step:11670 [D loss: 0.660044, acc.: 56.25%] [G loss: 0.797868]\n",
      "epoch:12 step:11671 [D loss: 0.645317, acc.: 63.28%] [G loss: 0.735048]\n",
      "epoch:12 step:11672 [D loss: 0.704285, acc.: 44.53%] [G loss: 0.851990]\n",
      "epoch:12 step:11673 [D loss: 0.690578, acc.: 54.69%] [G loss: 0.875510]\n",
      "epoch:12 step:11674 [D loss: 0.672799, acc.: 57.03%] [G loss: 0.878182]\n",
      "epoch:12 step:11675 [D loss: 0.651416, acc.: 61.72%] [G loss: 0.912671]\n",
      "epoch:12 step:11676 [D loss: 0.719372, acc.: 42.19%] [G loss: 0.829848]\n",
      "epoch:12 step:11677 [D loss: 0.673733, acc.: 59.38%] [G loss: 0.824772]\n",
      "epoch:12 step:11678 [D loss: 0.678706, acc.: 54.69%] [G loss: 0.835545]\n",
      "epoch:12 step:11679 [D loss: 0.703459, acc.: 54.69%] [G loss: 0.891488]\n",
      "epoch:12 step:11680 [D loss: 0.671920, acc.: 55.47%] [G loss: 0.766484]\n",
      "epoch:12 step:11681 [D loss: 0.689167, acc.: 53.12%] [G loss: 0.915024]\n",
      "epoch:12 step:11682 [D loss: 0.638428, acc.: 61.72%] [G loss: 0.820259]\n",
      "epoch:12 step:11683 [D loss: 0.660505, acc.: 53.91%] [G loss: 0.814749]\n",
      "epoch:12 step:11684 [D loss: 0.640795, acc.: 61.72%] [G loss: 0.880581]\n",
      "epoch:12 step:11685 [D loss: 0.659182, acc.: 60.16%] [G loss: 0.769801]\n",
      "epoch:12 step:11686 [D loss: 0.644273, acc.: 60.16%] [G loss: 0.883406]\n",
      "epoch:12 step:11687 [D loss: 0.707916, acc.: 50.78%] [G loss: 0.819527]\n",
      "epoch:12 step:11688 [D loss: 0.678859, acc.: 57.03%] [G loss: 0.795267]\n",
      "epoch:12 step:11689 [D loss: 0.684860, acc.: 53.91%] [G loss: 0.857017]\n",
      "epoch:12 step:11690 [D loss: 0.641299, acc.: 64.84%] [G loss: 0.881909]\n",
      "epoch:12 step:11691 [D loss: 0.689118, acc.: 53.12%] [G loss: 0.821227]\n",
      "epoch:12 step:11692 [D loss: 0.674566, acc.: 55.47%] [G loss: 0.773273]\n",
      "epoch:12 step:11693 [D loss: 0.694814, acc.: 51.56%] [G loss: 0.739412]\n",
      "epoch:12 step:11694 [D loss: 0.688648, acc.: 57.81%] [G loss: 0.790297]\n",
      "epoch:12 step:11695 [D loss: 0.669478, acc.: 60.16%] [G loss: 0.840353]\n",
      "epoch:12 step:11696 [D loss: 0.695068, acc.: 52.34%] [G loss: 0.822028]\n",
      "epoch:12 step:11697 [D loss: 0.685366, acc.: 50.78%] [G loss: 0.788265]\n",
      "epoch:12 step:11698 [D loss: 0.689406, acc.: 52.34%] [G loss: 0.799221]\n",
      "epoch:12 step:11699 [D loss: 0.653592, acc.: 62.50%] [G loss: 0.725342]\n",
      "epoch:12 step:11700 [D loss: 0.667049, acc.: 62.50%] [G loss: 0.826034]\n",
      "epoch:12 step:11701 [D loss: 0.687635, acc.: 52.34%] [G loss: 0.812290]\n",
      "epoch:12 step:11702 [D loss: 0.711890, acc.: 45.31%] [G loss: 0.842088]\n",
      "epoch:12 step:11703 [D loss: 0.693368, acc.: 48.44%] [G loss: 0.786761]\n",
      "epoch:12 step:11704 [D loss: 0.667348, acc.: 59.38%] [G loss: 0.828838]\n",
      "epoch:12 step:11705 [D loss: 0.728809, acc.: 44.53%] [G loss: 0.827627]\n",
      "epoch:12 step:11706 [D loss: 0.688246, acc.: 51.56%] [G loss: 0.827223]\n",
      "epoch:12 step:11707 [D loss: 0.641446, acc.: 64.06%] [G loss: 0.758605]\n",
      "epoch:12 step:11708 [D loss: 0.676568, acc.: 53.91%] [G loss: 0.735602]\n",
      "epoch:12 step:11709 [D loss: 0.659886, acc.: 54.69%] [G loss: 0.801160]\n",
      "epoch:12 step:11710 [D loss: 0.632211, acc.: 62.50%] [G loss: 0.815562]\n",
      "epoch:12 step:11711 [D loss: 0.698172, acc.: 45.31%] [G loss: 0.788463]\n",
      "epoch:12 step:11712 [D loss: 0.682929, acc.: 46.09%] [G loss: 0.812580]\n",
      "epoch:12 step:11713 [D loss: 0.681755, acc.: 55.47%] [G loss: 0.806768]\n",
      "epoch:12 step:11714 [D loss: 0.675502, acc.: 53.91%] [G loss: 0.800347]\n",
      "epoch:12 step:11715 [D loss: 0.642367, acc.: 65.62%] [G loss: 0.826670]\n",
      "epoch:12 step:11716 [D loss: 0.721335, acc.: 50.00%] [G loss: 0.849757]\n",
      "epoch:12 step:11717 [D loss: 0.672544, acc.: 57.03%] [G loss: 0.838851]\n",
      "epoch:12 step:11718 [D loss: 0.687844, acc.: 50.78%] [G loss: 0.820384]\n",
      "epoch:12 step:11719 [D loss: 0.673797, acc.: 54.69%] [G loss: 0.813367]\n",
      "epoch:12 step:11720 [D loss: 0.618848, acc.: 67.19%] [G loss: 0.809811]\n",
      "epoch:12 step:11721 [D loss: 0.679384, acc.: 63.28%] [G loss: 0.835770]\n",
      "epoch:12 step:11722 [D loss: 0.685416, acc.: 53.91%] [G loss: 0.887474]\n",
      "epoch:12 step:11723 [D loss: 0.626538, acc.: 61.72%] [G loss: 0.890470]\n",
      "epoch:12 step:11724 [D loss: 0.694566, acc.: 50.78%] [G loss: 0.845657]\n",
      "epoch:12 step:11725 [D loss: 0.679273, acc.: 57.81%] [G loss: 0.923521]\n",
      "epoch:12 step:11726 [D loss: 0.625152, acc.: 69.53%] [G loss: 0.809918]\n",
      "epoch:12 step:11727 [D loss: 0.660388, acc.: 60.94%] [G loss: 0.882608]\n",
      "epoch:12 step:11728 [D loss: 0.722595, acc.: 50.78%] [G loss: 0.911057]\n",
      "epoch:12 step:11729 [D loss: 0.659946, acc.: 67.19%] [G loss: 0.838854]\n",
      "epoch:12 step:11730 [D loss: 0.662719, acc.: 55.47%] [G loss: 0.842645]\n",
      "epoch:12 step:11731 [D loss: 0.656952, acc.: 60.16%] [G loss: 0.833501]\n",
      "epoch:12 step:11732 [D loss: 0.687387, acc.: 55.47%] [G loss: 0.808520]\n",
      "epoch:12 step:11733 [D loss: 0.696951, acc.: 53.91%] [G loss: 0.809182]\n",
      "epoch:12 step:11734 [D loss: 0.661131, acc.: 56.25%] [G loss: 0.815151]\n",
      "epoch:12 step:11735 [D loss: 0.689028, acc.: 53.12%] [G loss: 0.762587]\n",
      "epoch:12 step:11736 [D loss: 0.627244, acc.: 70.31%] [G loss: 0.879579]\n",
      "epoch:12 step:11737 [D loss: 0.678344, acc.: 63.28%] [G loss: 0.786160]\n",
      "epoch:12 step:11738 [D loss: 0.645546, acc.: 59.38%] [G loss: 0.739457]\n",
      "epoch:12 step:11739 [D loss: 0.662092, acc.: 58.59%] [G loss: 0.779438]\n",
      "epoch:12 step:11740 [D loss: 0.612184, acc.: 69.53%] [G loss: 0.815644]\n",
      "epoch:12 step:11741 [D loss: 0.703091, acc.: 54.69%] [G loss: 0.776160]\n",
      "epoch:12 step:11742 [D loss: 0.670087, acc.: 59.38%] [G loss: 0.735077]\n",
      "epoch:12 step:11743 [D loss: 0.696566, acc.: 58.59%] [G loss: 0.839972]\n",
      "epoch:12 step:11744 [D loss: 0.665424, acc.: 58.59%] [G loss: 0.852314]\n",
      "epoch:12 step:11745 [D loss: 0.683455, acc.: 51.56%] [G loss: 0.717313]\n",
      "epoch:12 step:11746 [D loss: 0.715694, acc.: 50.00%] [G loss: 0.813600]\n",
      "epoch:12 step:11747 [D loss: 0.741507, acc.: 39.06%] [G loss: 0.889363]\n",
      "epoch:12 step:11748 [D loss: 0.658478, acc.: 58.59%] [G loss: 0.812223]\n",
      "epoch:12 step:11749 [D loss: 0.706017, acc.: 47.66%] [G loss: 0.855538]\n",
      "epoch:12 step:11750 [D loss: 0.654387, acc.: 58.59%] [G loss: 0.871003]\n",
      "epoch:12 step:11751 [D loss: 0.721713, acc.: 48.44%] [G loss: 0.842014]\n",
      "epoch:12 step:11752 [D loss: 0.677488, acc.: 51.56%] [G loss: 0.851986]\n",
      "epoch:12 step:11753 [D loss: 0.660278, acc.: 60.16%] [G loss: 0.907740]\n",
      "epoch:12 step:11754 [D loss: 0.675928, acc.: 54.69%] [G loss: 0.773107]\n",
      "epoch:12 step:11755 [D loss: 0.619759, acc.: 68.75%] [G loss: 0.871743]\n",
      "epoch:12 step:11756 [D loss: 0.693103, acc.: 48.44%] [G loss: 0.805794]\n",
      "epoch:12 step:11757 [D loss: 0.691548, acc.: 58.59%] [G loss: 0.825978]\n",
      "epoch:12 step:11758 [D loss: 0.636284, acc.: 65.62%] [G loss: 0.719390]\n",
      "epoch:12 step:11759 [D loss: 0.659162, acc.: 59.38%] [G loss: 0.826315]\n",
      "epoch:12 step:11760 [D loss: 0.619183, acc.: 72.66%] [G loss: 0.827551]\n",
      "epoch:12 step:11761 [D loss: 0.654392, acc.: 60.16%] [G loss: 0.906721]\n",
      "epoch:12 step:11762 [D loss: 0.662310, acc.: 59.38%] [G loss: 0.849801]\n",
      "epoch:12 step:11763 [D loss: 0.727556, acc.: 49.22%] [G loss: 0.818971]\n",
      "epoch:12 step:11764 [D loss: 0.673951, acc.: 56.25%] [G loss: 0.838421]\n",
      "epoch:12 step:11765 [D loss: 0.639941, acc.: 67.97%] [G loss: 0.846759]\n",
      "epoch:12 step:11766 [D loss: 0.728215, acc.: 51.56%] [G loss: 0.711714]\n",
      "epoch:12 step:11767 [D loss: 0.636298, acc.: 64.06%] [G loss: 0.740559]\n",
      "epoch:12 step:11768 [D loss: 0.649273, acc.: 64.06%] [G loss: 0.728926]\n",
      "epoch:12 step:11769 [D loss: 0.730111, acc.: 46.09%] [G loss: 0.755705]\n",
      "epoch:12 step:11770 [D loss: 0.633558, acc.: 64.84%] [G loss: 0.749078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11771 [D loss: 0.721621, acc.: 48.44%] [G loss: 0.825165]\n",
      "epoch:12 step:11772 [D loss: 0.674789, acc.: 50.00%] [G loss: 0.768951]\n",
      "epoch:12 step:11773 [D loss: 0.675519, acc.: 54.69%] [G loss: 0.840877]\n",
      "epoch:12 step:11774 [D loss: 0.647313, acc.: 66.41%] [G loss: 0.822608]\n",
      "epoch:12 step:11775 [D loss: 0.657955, acc.: 64.06%] [G loss: 0.758861]\n",
      "epoch:12 step:11776 [D loss: 0.655328, acc.: 64.06%] [G loss: 0.724482]\n",
      "epoch:12 step:11777 [D loss: 0.703510, acc.: 49.22%] [G loss: 0.734014]\n",
      "epoch:12 step:11778 [D loss: 0.659174, acc.: 57.03%] [G loss: 0.801320]\n",
      "epoch:12 step:11779 [D loss: 0.675993, acc.: 54.69%] [G loss: 0.735494]\n",
      "epoch:12 step:11780 [D loss: 0.714035, acc.: 45.31%] [G loss: 0.710614]\n",
      "epoch:12 step:11781 [D loss: 0.616632, acc.: 70.31%] [G loss: 0.837150]\n",
      "epoch:12 step:11782 [D loss: 0.662935, acc.: 59.38%] [G loss: 0.713491]\n",
      "epoch:12 step:11783 [D loss: 0.725315, acc.: 43.75%] [G loss: 0.756043]\n",
      "epoch:12 step:11784 [D loss: 0.621319, acc.: 69.53%] [G loss: 0.715679]\n",
      "epoch:12 step:11785 [D loss: 0.663644, acc.: 64.06%] [G loss: 0.766185]\n",
      "epoch:12 step:11786 [D loss: 0.677405, acc.: 60.94%] [G loss: 0.799194]\n",
      "epoch:12 step:11787 [D loss: 0.631885, acc.: 61.72%] [G loss: 0.836387]\n",
      "epoch:12 step:11788 [D loss: 0.659795, acc.: 56.25%] [G loss: 0.779538]\n",
      "epoch:12 step:11789 [D loss: 0.684172, acc.: 58.59%] [G loss: 0.898985]\n",
      "epoch:12 step:11790 [D loss: 0.681678, acc.: 52.34%] [G loss: 0.904810]\n",
      "epoch:12 step:11791 [D loss: 0.671994, acc.: 64.84%] [G loss: 0.797736]\n",
      "epoch:12 step:11792 [D loss: 0.733405, acc.: 40.62%] [G loss: 0.702074]\n",
      "epoch:12 step:11793 [D loss: 0.718508, acc.: 49.22%] [G loss: 0.828482]\n",
      "epoch:12 step:11794 [D loss: 0.666226, acc.: 57.03%] [G loss: 0.777796]\n",
      "epoch:12 step:11795 [D loss: 0.719036, acc.: 39.06%] [G loss: 0.731050]\n",
      "epoch:12 step:11796 [D loss: 0.659610, acc.: 60.94%] [G loss: 0.755377]\n",
      "epoch:12 step:11797 [D loss: 0.765161, acc.: 45.31%] [G loss: 0.863730]\n",
      "epoch:12 step:11798 [D loss: 0.698962, acc.: 42.97%] [G loss: 0.913752]\n",
      "epoch:12 step:11799 [D loss: 0.684351, acc.: 51.56%] [G loss: 0.870175]\n",
      "epoch:12 step:11800 [D loss: 0.696738, acc.: 51.56%] [G loss: 0.875969]\n",
      "epoch:12 step:11801 [D loss: 0.742385, acc.: 39.84%] [G loss: 0.844011]\n",
      "epoch:12 step:11802 [D loss: 0.683799, acc.: 55.47%] [G loss: 0.814669]\n",
      "epoch:12 step:11803 [D loss: 0.688698, acc.: 48.44%] [G loss: 0.781773]\n",
      "epoch:12 step:11804 [D loss: 0.731461, acc.: 46.09%] [G loss: 0.782639]\n",
      "epoch:12 step:11805 [D loss: 0.725005, acc.: 46.09%] [G loss: 0.832012]\n",
      "epoch:12 step:11806 [D loss: 0.715113, acc.: 47.66%] [G loss: 0.849229]\n",
      "epoch:12 step:11807 [D loss: 0.676848, acc.: 56.25%] [G loss: 0.745421]\n",
      "epoch:12 step:11808 [D loss: 0.732605, acc.: 45.31%] [G loss: 0.862610]\n",
      "epoch:12 step:11809 [D loss: 0.717588, acc.: 42.97%] [G loss: 0.842043]\n",
      "epoch:12 step:11810 [D loss: 0.678397, acc.: 57.81%] [G loss: 0.888534]\n",
      "epoch:12 step:11811 [D loss: 0.675064, acc.: 55.47%] [G loss: 0.843221]\n",
      "epoch:12 step:11812 [D loss: 0.657264, acc.: 58.59%] [G loss: 0.816942]\n",
      "epoch:12 step:11813 [D loss: 0.650546, acc.: 67.19%] [G loss: 0.779189]\n",
      "epoch:12 step:11814 [D loss: 0.669670, acc.: 56.25%] [G loss: 0.833581]\n",
      "epoch:12 step:11815 [D loss: 0.668426, acc.: 55.47%] [G loss: 0.783601]\n",
      "epoch:12 step:11816 [D loss: 0.682459, acc.: 59.38%] [G loss: 0.688928]\n",
      "epoch:12 step:11817 [D loss: 0.677058, acc.: 57.03%] [G loss: 0.767808]\n",
      "epoch:12 step:11818 [D loss: 0.702267, acc.: 52.34%] [G loss: 0.774444]\n",
      "epoch:12 step:11819 [D loss: 0.692168, acc.: 58.59%] [G loss: 0.789980]\n",
      "epoch:12 step:11820 [D loss: 0.675861, acc.: 55.47%] [G loss: 0.805358]\n",
      "epoch:12 step:11821 [D loss: 0.704123, acc.: 50.78%] [G loss: 0.785174]\n",
      "epoch:12 step:11822 [D loss: 0.639288, acc.: 62.50%] [G loss: 0.831281]\n",
      "epoch:12 step:11823 [D loss: 0.684804, acc.: 56.25%] [G loss: 0.760107]\n",
      "epoch:12 step:11824 [D loss: 0.695629, acc.: 49.22%] [G loss: 0.814583]\n",
      "epoch:12 step:11825 [D loss: 0.670785, acc.: 59.38%] [G loss: 0.733951]\n",
      "epoch:12 step:11826 [D loss: 0.676582, acc.: 59.38%] [G loss: 0.789402]\n",
      "epoch:12 step:11827 [D loss: 0.707594, acc.: 52.34%] [G loss: 0.784954]\n",
      "epoch:12 step:11828 [D loss: 0.698293, acc.: 53.91%] [G loss: 0.826754]\n",
      "epoch:12 step:11829 [D loss: 0.668830, acc.: 54.69%] [G loss: 0.783291]\n",
      "epoch:12 step:11830 [D loss: 0.663123, acc.: 62.50%] [G loss: 0.782304]\n",
      "epoch:12 step:11831 [D loss: 0.676090, acc.: 55.47%] [G loss: 0.760692]\n",
      "epoch:12 step:11832 [D loss: 0.617789, acc.: 64.84%] [G loss: 0.776078]\n",
      "epoch:12 step:11833 [D loss: 0.671777, acc.: 59.38%] [G loss: 0.886755]\n",
      "epoch:12 step:11834 [D loss: 0.687703, acc.: 50.78%] [G loss: 0.845811]\n",
      "epoch:12 step:11835 [D loss: 0.689791, acc.: 57.03%] [G loss: 0.775251]\n",
      "epoch:12 step:11836 [D loss: 0.658704, acc.: 56.25%] [G loss: 0.768351]\n",
      "epoch:12 step:11837 [D loss: 0.735467, acc.: 43.75%] [G loss: 0.798683]\n",
      "epoch:12 step:11838 [D loss: 0.689113, acc.: 53.91%] [G loss: 0.810859]\n",
      "epoch:12 step:11839 [D loss: 0.678590, acc.: 53.91%] [G loss: 0.738170]\n",
      "epoch:12 step:11840 [D loss: 0.668835, acc.: 58.59%] [G loss: 0.772041]\n",
      "epoch:12 step:11841 [D loss: 0.700704, acc.: 46.09%] [G loss: 0.761442]\n",
      "epoch:12 step:11842 [D loss: 0.684222, acc.: 58.59%] [G loss: 0.777894]\n",
      "epoch:12 step:11843 [D loss: 0.723536, acc.: 42.97%] [G loss: 0.764366]\n",
      "epoch:12 step:11844 [D loss: 0.687184, acc.: 60.16%] [G loss: 1.508466]\n",
      "epoch:12 step:11845 [D loss: 0.680306, acc.: 53.91%] [G loss: 0.742911]\n",
      "epoch:12 step:11846 [D loss: 0.688883, acc.: 53.91%] [G loss: 0.794344]\n",
      "epoch:12 step:11847 [D loss: 0.629685, acc.: 69.53%] [G loss: 0.799574]\n",
      "epoch:12 step:11848 [D loss: 0.684500, acc.: 57.03%] [G loss: 0.755473]\n",
      "epoch:12 step:11849 [D loss: 0.671126, acc.: 61.72%] [G loss: 0.813716]\n",
      "epoch:12 step:11850 [D loss: 0.740186, acc.: 51.56%] [G loss: 0.855307]\n",
      "epoch:12 step:11851 [D loss: 0.640640, acc.: 67.97%] [G loss: 0.833048]\n",
      "epoch:12 step:11852 [D loss: 0.714761, acc.: 42.97%] [G loss: 0.951283]\n",
      "epoch:12 step:11853 [D loss: 0.663479, acc.: 53.91%] [G loss: 0.932278]\n",
      "epoch:12 step:11854 [D loss: 0.678554, acc.: 54.69%] [G loss: 0.796292]\n",
      "epoch:12 step:11855 [D loss: 0.679062, acc.: 61.72%] [G loss: 0.889410]\n",
      "epoch:12 step:11856 [D loss: 0.685401, acc.: 57.81%] [G loss: 0.907809]\n",
      "epoch:12 step:11857 [D loss: 0.660724, acc.: 60.94%] [G loss: 0.928276]\n",
      "epoch:12 step:11858 [D loss: 0.648575, acc.: 60.16%] [G loss: 0.794440]\n",
      "epoch:12 step:11859 [D loss: 0.661023, acc.: 64.84%] [G loss: 0.887754]\n",
      "epoch:12 step:11860 [D loss: 0.701629, acc.: 48.44%] [G loss: 0.896446]\n",
      "epoch:12 step:11861 [D loss: 0.665230, acc.: 59.38%] [G loss: 0.776576]\n",
      "epoch:12 step:11862 [D loss: 0.687831, acc.: 50.78%] [G loss: 0.811696]\n",
      "epoch:12 step:11863 [D loss: 0.684848, acc.: 54.69%] [G loss: 0.765622]\n",
      "epoch:12 step:11864 [D loss: 0.655773, acc.: 60.94%] [G loss: 0.892183]\n",
      "epoch:12 step:11865 [D loss: 0.619379, acc.: 78.12%] [G loss: 0.781668]\n",
      "epoch:12 step:11866 [D loss: 0.681208, acc.: 58.59%] [G loss: 0.854157]\n",
      "epoch:12 step:11867 [D loss: 0.679551, acc.: 48.44%] [G loss: 0.780864]\n",
      "epoch:12 step:11868 [D loss: 0.661518, acc.: 57.03%] [G loss: 0.821891]\n",
      "epoch:12 step:11869 [D loss: 0.672964, acc.: 60.16%] [G loss: 0.896419]\n",
      "epoch:12 step:11870 [D loss: 0.696198, acc.: 48.44%] [G loss: 0.818065]\n",
      "epoch:12 step:11871 [D loss: 0.654620, acc.: 63.28%] [G loss: 0.937083]\n",
      "epoch:12 step:11872 [D loss: 0.632847, acc.: 66.41%] [G loss: 0.819194]\n",
      "epoch:12 step:11873 [D loss: 0.635215, acc.: 57.81%] [G loss: 0.837363]\n",
      "epoch:12 step:11874 [D loss: 0.641946, acc.: 64.84%] [G loss: 0.852131]\n",
      "epoch:12 step:11875 [D loss: 0.672255, acc.: 57.81%] [G loss: 0.807871]\n",
      "epoch:12 step:11876 [D loss: 0.629900, acc.: 68.75%] [G loss: 0.921929]\n",
      "epoch:12 step:11877 [D loss: 0.660679, acc.: 65.62%] [G loss: 0.879475]\n",
      "epoch:12 step:11878 [D loss: 0.695254, acc.: 58.59%] [G loss: 0.872457]\n",
      "epoch:12 step:11879 [D loss: 0.639163, acc.: 66.41%] [G loss: 0.838831]\n",
      "epoch:12 step:11880 [D loss: 0.674704, acc.: 59.38%] [G loss: 0.857384]\n",
      "epoch:12 step:11881 [D loss: 0.664868, acc.: 56.25%] [G loss: 0.914506]\n",
      "epoch:12 step:11882 [D loss: 0.714293, acc.: 52.34%] [G loss: 0.929619]\n",
      "epoch:12 step:11883 [D loss: 0.693722, acc.: 50.78%] [G loss: 0.910761]\n",
      "epoch:12 step:11884 [D loss: 0.676951, acc.: 58.59%] [G loss: 0.900778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11885 [D loss: 0.624615, acc.: 63.28%] [G loss: 0.878883]\n",
      "epoch:12 step:11886 [D loss: 0.757113, acc.: 36.72%] [G loss: 0.860636]\n",
      "epoch:12 step:11887 [D loss: 0.666173, acc.: 59.38%] [G loss: 0.810122]\n",
      "epoch:12 step:11888 [D loss: 0.637427, acc.: 67.97%] [G loss: 0.796258]\n",
      "epoch:12 step:11889 [D loss: 0.700879, acc.: 48.44%] [G loss: 0.799887]\n",
      "epoch:12 step:11890 [D loss: 0.867322, acc.: 52.34%] [G loss: 0.865752]\n",
      "epoch:12 step:11891 [D loss: 0.657104, acc.: 61.72%] [G loss: 0.767407]\n",
      "epoch:12 step:11892 [D loss: 0.728053, acc.: 56.25%] [G loss: 0.808056]\n",
      "epoch:12 step:11893 [D loss: 0.701518, acc.: 52.34%] [G loss: 0.861308]\n",
      "epoch:12 step:11894 [D loss: 0.644943, acc.: 60.94%] [G loss: 0.847225]\n",
      "epoch:12 step:11895 [D loss: 0.683908, acc.: 53.12%] [G loss: 0.862956]\n",
      "epoch:12 step:11896 [D loss: 0.722818, acc.: 50.00%] [G loss: 0.937056]\n",
      "epoch:12 step:11897 [D loss: 0.660438, acc.: 59.38%] [G loss: 0.841731]\n",
      "epoch:12 step:11898 [D loss: 0.631124, acc.: 64.84%] [G loss: 0.737498]\n",
      "epoch:12 step:11899 [D loss: 0.679114, acc.: 63.28%] [G loss: 0.883200]\n",
      "epoch:12 step:11900 [D loss: 0.664537, acc.: 57.81%] [G loss: 0.775882]\n",
      "epoch:12 step:11901 [D loss: 0.696556, acc.: 49.22%] [G loss: 0.732136]\n",
      "epoch:12 step:11902 [D loss: 0.729407, acc.: 50.00%] [G loss: 0.731778]\n",
      "epoch:12 step:11903 [D loss: 0.678662, acc.: 53.12%] [G loss: 0.803101]\n",
      "epoch:12 step:11904 [D loss: 0.629722, acc.: 70.31%] [G loss: 0.773179]\n",
      "epoch:12 step:11905 [D loss: 0.672172, acc.: 54.69%] [G loss: 0.797045]\n",
      "epoch:12 step:11906 [D loss: 0.712884, acc.: 50.00%] [G loss: 0.793073]\n",
      "epoch:12 step:11907 [D loss: 0.667264, acc.: 58.59%] [G loss: 0.691704]\n",
      "epoch:12 step:11908 [D loss: 0.687802, acc.: 56.25%] [G loss: 0.753136]\n",
      "epoch:12 step:11909 [D loss: 0.688360, acc.: 56.25%] [G loss: 0.771088]\n",
      "epoch:12 step:11910 [D loss: 0.682021, acc.: 52.34%] [G loss: 0.711397]\n",
      "epoch:12 step:11911 [D loss: 0.688626, acc.: 51.56%] [G loss: 0.777361]\n",
      "epoch:12 step:11912 [D loss: 0.685750, acc.: 51.56%] [G loss: 0.766392]\n",
      "epoch:12 step:11913 [D loss: 0.655859, acc.: 63.28%] [G loss: 0.720288]\n",
      "epoch:12 step:11914 [D loss: 0.640963, acc.: 63.28%] [G loss: 0.736201]\n",
      "epoch:12 step:11915 [D loss: 0.662840, acc.: 60.94%] [G loss: 0.805368]\n",
      "epoch:12 step:11916 [D loss: 0.698673, acc.: 51.56%] [G loss: 0.914480]\n",
      "epoch:12 step:11917 [D loss: 0.662292, acc.: 54.69%] [G loss: 0.748575]\n",
      "epoch:12 step:11918 [D loss: 0.630909, acc.: 68.75%] [G loss: 0.718910]\n",
      "epoch:12 step:11919 [D loss: 0.725879, acc.: 53.12%] [G loss: 0.710385]\n",
      "epoch:12 step:11920 [D loss: 0.657229, acc.: 62.50%] [G loss: 0.787263]\n",
      "epoch:12 step:11921 [D loss: 0.665011, acc.: 60.16%] [G loss: 0.827580]\n",
      "epoch:12 step:11922 [D loss: 0.704026, acc.: 53.91%] [G loss: 0.732193]\n",
      "epoch:12 step:11923 [D loss: 0.664624, acc.: 62.50%] [G loss: 0.690405]\n",
      "epoch:12 step:11924 [D loss: 0.742985, acc.: 44.53%] [G loss: 0.798308]\n",
      "epoch:12 step:11925 [D loss: 0.657390, acc.: 58.59%] [G loss: 0.773796]\n",
      "epoch:12 step:11926 [D loss: 0.680178, acc.: 50.00%] [G loss: 0.857321]\n",
      "epoch:12 step:11927 [D loss: 0.690422, acc.: 53.91%] [G loss: 0.809434]\n",
      "epoch:12 step:11928 [D loss: 0.664836, acc.: 57.81%] [G loss: 0.885150]\n",
      "epoch:12 step:11929 [D loss: 0.644538, acc.: 65.62%] [G loss: 0.825897]\n",
      "epoch:12 step:11930 [D loss: 0.676932, acc.: 57.81%] [G loss: 0.867850]\n",
      "epoch:12 step:11931 [D loss: 0.688740, acc.: 53.91%] [G loss: 0.800244]\n",
      "epoch:12 step:11932 [D loss: 0.664495, acc.: 48.44%] [G loss: 0.867865]\n",
      "epoch:12 step:11933 [D loss: 0.692056, acc.: 47.66%] [G loss: 0.821268]\n",
      "epoch:12 step:11934 [D loss: 0.688522, acc.: 57.81%] [G loss: 0.816415]\n",
      "epoch:12 step:11935 [D loss: 0.670084, acc.: 53.91%] [G loss: 0.779150]\n",
      "epoch:12 step:11936 [D loss: 0.705364, acc.: 49.22%] [G loss: 0.796308]\n",
      "epoch:12 step:11937 [D loss: 0.678523, acc.: 52.34%] [G loss: 0.738674]\n",
      "epoch:12 step:11938 [D loss: 0.691883, acc.: 50.78%] [G loss: 0.789435]\n",
      "epoch:12 step:11939 [D loss: 0.681318, acc.: 50.78%] [G loss: 0.782019]\n",
      "epoch:12 step:11940 [D loss: 0.691044, acc.: 52.34%] [G loss: 0.846601]\n",
      "epoch:12 step:11941 [D loss: 0.689984, acc.: 54.69%] [G loss: 0.824336]\n",
      "epoch:12 step:11942 [D loss: 0.713581, acc.: 44.53%] [G loss: 0.805090]\n",
      "epoch:12 step:11943 [D loss: 0.684190, acc.: 50.00%] [G loss: 0.794245]\n",
      "epoch:12 step:11944 [D loss: 0.676574, acc.: 60.16%] [G loss: 0.814787]\n",
      "epoch:12 step:11945 [D loss: 0.656803, acc.: 64.06%] [G loss: 0.844836]\n",
      "epoch:12 step:11946 [D loss: 0.675249, acc.: 53.12%] [G loss: 0.935797]\n",
      "epoch:12 step:11947 [D loss: 0.712294, acc.: 49.22%] [G loss: 0.785022]\n",
      "epoch:12 step:11948 [D loss: 0.665384, acc.: 56.25%] [G loss: 0.820504]\n",
      "epoch:12 step:11949 [D loss: 0.611423, acc.: 66.41%] [G loss: 0.779502]\n",
      "epoch:12 step:11950 [D loss: 0.712440, acc.: 46.88%] [G loss: 0.848053]\n",
      "epoch:12 step:11951 [D loss: 0.673366, acc.: 58.59%] [G loss: 0.811372]\n",
      "epoch:12 step:11952 [D loss: 0.656861, acc.: 52.34%] [G loss: 0.849352]\n",
      "epoch:12 step:11953 [D loss: 0.693467, acc.: 53.12%] [G loss: 0.852824]\n",
      "epoch:12 step:11954 [D loss: 0.670339, acc.: 57.81%] [G loss: 0.740685]\n",
      "epoch:12 step:11955 [D loss: 0.672458, acc.: 57.03%] [G loss: 0.792928]\n",
      "epoch:12 step:11956 [D loss: 0.652474, acc.: 60.94%] [G loss: 0.844941]\n",
      "epoch:12 step:11957 [D loss: 0.628759, acc.: 69.53%] [G loss: 0.881732]\n",
      "epoch:12 step:11958 [D loss: 0.643302, acc.: 66.41%] [G loss: 0.858990]\n",
      "epoch:12 step:11959 [D loss: 0.671728, acc.: 60.94%] [G loss: 0.874660]\n",
      "epoch:12 step:11960 [D loss: 0.696618, acc.: 60.16%] [G loss: 0.849753]\n",
      "epoch:12 step:11961 [D loss: 0.676439, acc.: 57.03%] [G loss: 0.805691]\n",
      "epoch:12 step:11962 [D loss: 0.653226, acc.: 62.50%] [G loss: 0.784060]\n",
      "epoch:12 step:11963 [D loss: 0.620527, acc.: 62.50%] [G loss: 0.861934]\n",
      "epoch:12 step:11964 [D loss: 0.636183, acc.: 70.31%] [G loss: 0.855261]\n",
      "epoch:12 step:11965 [D loss: 0.664208, acc.: 60.94%] [G loss: 0.819438]\n",
      "epoch:12 step:11966 [D loss: 0.632834, acc.: 69.53%] [G loss: 0.778211]\n",
      "epoch:12 step:11967 [D loss: 0.667419, acc.: 58.59%] [G loss: 0.878476]\n",
      "epoch:12 step:11968 [D loss: 0.704355, acc.: 53.12%] [G loss: 0.820005]\n",
      "epoch:12 step:11969 [D loss: 0.720164, acc.: 47.66%] [G loss: 0.808482]\n",
      "epoch:12 step:11970 [D loss: 0.764627, acc.: 32.81%] [G loss: 0.732238]\n",
      "epoch:12 step:11971 [D loss: 0.670721, acc.: 53.12%] [G loss: 0.754306]\n",
      "epoch:12 step:11972 [D loss: 0.657601, acc.: 59.38%] [G loss: 0.785325]\n",
      "epoch:12 step:11973 [D loss: 0.714205, acc.: 57.03%] [G loss: 0.745003]\n",
      "epoch:12 step:11974 [D loss: 0.691498, acc.: 57.03%] [G loss: 0.809213]\n",
      "epoch:12 step:11975 [D loss: 0.720734, acc.: 47.66%] [G loss: 0.824430]\n",
      "epoch:12 step:11976 [D loss: 0.611678, acc.: 74.22%] [G loss: 0.846478]\n",
      "epoch:12 step:11977 [D loss: 0.655257, acc.: 56.25%] [G loss: 0.787537]\n",
      "epoch:12 step:11978 [D loss: 0.684382, acc.: 58.59%] [G loss: 0.830979]\n",
      "epoch:12 step:11979 [D loss: 0.686004, acc.: 57.03%] [G loss: 0.775581]\n",
      "epoch:12 step:11980 [D loss: 0.629199, acc.: 63.28%] [G loss: 0.752085]\n",
      "epoch:12 step:11981 [D loss: 0.611912, acc.: 69.53%] [G loss: 0.821562]\n",
      "epoch:12 step:11982 [D loss: 0.704477, acc.: 51.56%] [G loss: 0.718490]\n",
      "epoch:12 step:11983 [D loss: 0.696475, acc.: 49.22%] [G loss: 0.819673]\n",
      "epoch:12 step:11984 [D loss: 0.690443, acc.: 50.00%] [G loss: 0.747807]\n",
      "epoch:12 step:11985 [D loss: 0.667288, acc.: 64.84%] [G loss: 0.723204]\n",
      "epoch:12 step:11986 [D loss: 0.655371, acc.: 55.47%] [G loss: 0.819885]\n",
      "epoch:12 step:11987 [D loss: 0.683905, acc.: 58.59%] [G loss: 0.813450]\n",
      "epoch:12 step:11988 [D loss: 0.700757, acc.: 50.78%] [G loss: 0.867676]\n",
      "epoch:12 step:11989 [D loss: 0.683361, acc.: 51.56%] [G loss: 0.873193]\n",
      "epoch:12 step:11990 [D loss: 0.736429, acc.: 40.62%] [G loss: 0.813503]\n",
      "epoch:12 step:11991 [D loss: 0.688667, acc.: 53.91%] [G loss: 0.907327]\n",
      "epoch:12 step:11992 [D loss: 0.635284, acc.: 57.81%] [G loss: 0.920275]\n",
      "epoch:12 step:11993 [D loss: 0.659262, acc.: 59.38%] [G loss: 0.852026]\n",
      "epoch:12 step:11994 [D loss: 0.688547, acc.: 51.56%] [G loss: 0.788962]\n",
      "epoch:12 step:11995 [D loss: 0.651667, acc.: 58.59%] [G loss: 0.810591]\n",
      "epoch:12 step:11996 [D loss: 0.684967, acc.: 53.91%] [G loss: 0.886282]\n",
      "epoch:12 step:11997 [D loss: 0.712606, acc.: 49.22%] [G loss: 0.858937]\n",
      "epoch:12 step:11998 [D loss: 0.663584, acc.: 50.78%] [G loss: 0.819799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11999 [D loss: 0.717139, acc.: 53.91%] [G loss: 0.838850]\n",
      "epoch:12 step:12000 [D loss: 0.693910, acc.: 54.69%] [G loss: 0.813691]\n",
      "epoch:12 step:12001 [D loss: 0.646271, acc.: 67.97%] [G loss: 0.829440]\n",
      "epoch:12 step:12002 [D loss: 0.651061, acc.: 56.25%] [G loss: 0.834263]\n",
      "epoch:12 step:12003 [D loss: 0.667324, acc.: 60.94%] [G loss: 0.832383]\n",
      "epoch:12 step:12004 [D loss: 0.665820, acc.: 57.81%] [G loss: 0.876694]\n",
      "epoch:12 step:12005 [D loss: 0.644815, acc.: 64.06%] [G loss: 0.865423]\n",
      "epoch:12 step:12006 [D loss: 0.682744, acc.: 56.25%] [G loss: 0.903392]\n",
      "epoch:12 step:12007 [D loss: 0.676164, acc.: 57.81%] [G loss: 0.806291]\n",
      "epoch:12 step:12008 [D loss: 0.656783, acc.: 56.25%] [G loss: 0.774488]\n",
      "epoch:12 step:12009 [D loss: 0.660949, acc.: 59.38%] [G loss: 0.851804]\n",
      "epoch:12 step:12010 [D loss: 0.671148, acc.: 56.25%] [G loss: 0.900922]\n",
      "epoch:12 step:12011 [D loss: 0.696021, acc.: 45.31%] [G loss: 0.780119]\n",
      "epoch:12 step:12012 [D loss: 0.639939, acc.: 61.72%] [G loss: 0.904417]\n",
      "epoch:12 step:12013 [D loss: 0.655772, acc.: 62.50%] [G loss: 0.923932]\n",
      "epoch:12 step:12014 [D loss: 0.671816, acc.: 57.81%] [G loss: 0.893511]\n",
      "epoch:12 step:12015 [D loss: 0.661772, acc.: 59.38%] [G loss: 0.779389]\n",
      "epoch:12 step:12016 [D loss: 0.678360, acc.: 53.12%] [G loss: 0.870605]\n",
      "epoch:12 step:12017 [D loss: 0.666677, acc.: 59.38%] [G loss: 0.850490]\n",
      "epoch:12 step:12018 [D loss: 0.652461, acc.: 62.50%] [G loss: 0.852470]\n",
      "epoch:12 step:12019 [D loss: 0.639738, acc.: 64.06%] [G loss: 0.783109]\n",
      "epoch:12 step:12020 [D loss: 0.630162, acc.: 67.97%] [G loss: 0.810325]\n",
      "epoch:12 step:12021 [D loss: 0.651979, acc.: 60.16%] [G loss: 0.689338]\n",
      "epoch:12 step:12022 [D loss: 0.710362, acc.: 50.00%] [G loss: 0.862607]\n",
      "epoch:12 step:12023 [D loss: 0.686561, acc.: 57.03%] [G loss: 0.881872]\n",
      "epoch:12 step:12024 [D loss: 0.668573, acc.: 55.47%] [G loss: 0.792311]\n",
      "epoch:12 step:12025 [D loss: 0.645383, acc.: 64.84%] [G loss: 0.865778]\n",
      "epoch:12 step:12026 [D loss: 0.661786, acc.: 54.69%] [G loss: 0.795138]\n",
      "epoch:12 step:12027 [D loss: 0.688278, acc.: 49.22%] [G loss: 0.863176]\n",
      "epoch:12 step:12028 [D loss: 0.618675, acc.: 67.19%] [G loss: 0.905289]\n",
      "epoch:12 step:12029 [D loss: 0.692450, acc.: 55.47%] [G loss: 0.811207]\n",
      "epoch:12 step:12030 [D loss: 0.619362, acc.: 64.84%] [G loss: 0.837229]\n",
      "epoch:12 step:12031 [D loss: 0.718322, acc.: 49.22%] [G loss: 0.792332]\n",
      "epoch:12 step:12032 [D loss: 0.665572, acc.: 54.69%] [G loss: 0.836794]\n",
      "epoch:12 step:12033 [D loss: 0.697789, acc.: 52.34%] [G loss: 0.814716]\n",
      "epoch:12 step:12034 [D loss: 0.678091, acc.: 61.72%] [G loss: 0.786831]\n",
      "epoch:12 step:12035 [D loss: 0.674481, acc.: 57.81%] [G loss: 0.809832]\n",
      "epoch:12 step:12036 [D loss: 0.637722, acc.: 65.62%] [G loss: 0.770939]\n",
      "epoch:12 step:12037 [D loss: 0.647362, acc.: 60.16%] [G loss: 0.849453]\n",
      "epoch:12 step:12038 [D loss: 0.663119, acc.: 54.69%] [G loss: 0.898990]\n",
      "epoch:12 step:12039 [D loss: 0.687404, acc.: 48.44%] [G loss: 0.794827]\n",
      "epoch:12 step:12040 [D loss: 0.641524, acc.: 60.94%] [G loss: 0.855845]\n",
      "epoch:12 step:12041 [D loss: 0.686889, acc.: 49.22%] [G loss: 0.842096]\n",
      "epoch:12 step:12042 [D loss: 0.603575, acc.: 71.09%] [G loss: 0.804053]\n",
      "epoch:12 step:12043 [D loss: 0.671208, acc.: 57.81%] [G loss: 0.828382]\n",
      "epoch:12 step:12044 [D loss: 0.690912, acc.: 51.56%] [G loss: 0.869855]\n",
      "epoch:12 step:12045 [D loss: 0.640107, acc.: 69.53%] [G loss: 0.855697]\n",
      "epoch:12 step:12046 [D loss: 0.672760, acc.: 51.56%] [G loss: 0.736481]\n",
      "epoch:12 step:12047 [D loss: 0.616546, acc.: 65.62%] [G loss: 0.870221]\n",
      "epoch:12 step:12048 [D loss: 0.680172, acc.: 50.00%] [G loss: 0.805810]\n",
      "epoch:12 step:12049 [D loss: 0.683855, acc.: 56.25%] [G loss: 0.789618]\n",
      "epoch:12 step:12050 [D loss: 0.649350, acc.: 63.28%] [G loss: 0.804312]\n",
      "epoch:12 step:12051 [D loss: 0.666607, acc.: 54.69%] [G loss: 0.739064]\n",
      "epoch:12 step:12052 [D loss: 0.670343, acc.: 56.25%] [G loss: 0.824484]\n",
      "epoch:12 step:12053 [D loss: 0.674721, acc.: 57.81%] [G loss: 0.789903]\n",
      "epoch:12 step:12054 [D loss: 0.624143, acc.: 60.16%] [G loss: 0.860525]\n",
      "epoch:12 step:12055 [D loss: 0.710719, acc.: 52.34%] [G loss: 0.863573]\n",
      "epoch:12 step:12056 [D loss: 0.681044, acc.: 56.25%] [G loss: 0.858761]\n",
      "epoch:12 step:12057 [D loss: 0.736772, acc.: 41.41%] [G loss: 0.789260]\n",
      "epoch:12 step:12058 [D loss: 0.674303, acc.: 57.81%] [G loss: 0.766549]\n",
      "epoch:12 step:12059 [D loss: 0.668503, acc.: 51.56%] [G loss: 0.908323]\n",
      "epoch:12 step:12060 [D loss: 0.649299, acc.: 62.50%] [G loss: 0.821283]\n",
      "epoch:12 step:12061 [D loss: 0.718088, acc.: 55.47%] [G loss: 0.782098]\n",
      "epoch:12 step:12062 [D loss: 0.679201, acc.: 55.47%] [G loss: 0.783098]\n",
      "epoch:12 step:12063 [D loss: 0.639866, acc.: 66.41%] [G loss: 0.784812]\n",
      "epoch:12 step:12064 [D loss: 0.667470, acc.: 57.03%] [G loss: 0.858215]\n",
      "epoch:12 step:12065 [D loss: 0.665200, acc.: 61.72%] [G loss: 0.814062]\n",
      "epoch:12 step:12066 [D loss: 0.676213, acc.: 58.59%] [G loss: 0.892540]\n",
      "epoch:12 step:12067 [D loss: 0.696718, acc.: 51.56%] [G loss: 0.849457]\n",
      "epoch:12 step:12068 [D loss: 0.702613, acc.: 53.12%] [G loss: 0.790370]\n",
      "epoch:12 step:12069 [D loss: 0.714057, acc.: 49.22%] [G loss: 0.793116]\n",
      "epoch:12 step:12070 [D loss: 0.657218, acc.: 60.16%] [G loss: 0.853881]\n",
      "epoch:12 step:12071 [D loss: 0.665430, acc.: 58.59%] [G loss: 0.845043]\n",
      "epoch:12 step:12072 [D loss: 0.696128, acc.: 53.91%] [G loss: 0.771099]\n",
      "epoch:12 step:12073 [D loss: 0.729360, acc.: 46.88%] [G loss: 0.874196]\n",
      "epoch:12 step:12074 [D loss: 0.665716, acc.: 60.16%] [G loss: 0.845860]\n",
      "epoch:12 step:12075 [D loss: 0.666601, acc.: 60.16%] [G loss: 0.783983]\n",
      "epoch:12 step:12076 [D loss: 0.758236, acc.: 39.84%] [G loss: 0.759115]\n",
      "epoch:12 step:12077 [D loss: 0.637286, acc.: 70.31%] [G loss: 0.862633]\n",
      "epoch:12 step:12078 [D loss: 0.679087, acc.: 58.59%] [G loss: 0.815215]\n",
      "epoch:12 step:12079 [D loss: 0.674359, acc.: 54.69%] [G loss: 0.787205]\n",
      "epoch:12 step:12080 [D loss: 0.656962, acc.: 67.19%] [G loss: 0.837345]\n",
      "epoch:12 step:12081 [D loss: 0.683568, acc.: 51.56%] [G loss: 0.818123]\n",
      "epoch:12 step:12082 [D loss: 0.614092, acc.: 70.31%] [G loss: 0.817567]\n",
      "epoch:12 step:12083 [D loss: 0.661509, acc.: 64.06%] [G loss: 0.852589]\n",
      "epoch:12 step:12084 [D loss: 0.632070, acc.: 66.41%] [G loss: 0.867473]\n",
      "epoch:12 step:12085 [D loss: 0.678852, acc.: 50.00%] [G loss: 0.793440]\n",
      "epoch:12 step:12086 [D loss: 0.663380, acc.: 60.94%] [G loss: 0.892731]\n",
      "epoch:12 step:12087 [D loss: 0.650271, acc.: 63.28%] [G loss: 0.807395]\n",
      "epoch:12 step:12088 [D loss: 0.647032, acc.: 62.50%] [G loss: 0.797969]\n",
      "epoch:12 step:12089 [D loss: 0.724420, acc.: 50.78%] [G loss: 0.865255]\n",
      "epoch:12 step:12090 [D loss: 0.691048, acc.: 51.56%] [G loss: 0.820394]\n",
      "epoch:12 step:12091 [D loss: 0.720154, acc.: 53.91%] [G loss: 0.882223]\n",
      "epoch:12 step:12092 [D loss: 0.715182, acc.: 46.09%] [G loss: 0.763174]\n",
      "epoch:12 step:12093 [D loss: 0.657010, acc.: 56.25%] [G loss: 0.768186]\n",
      "epoch:12 step:12094 [D loss: 0.630220, acc.: 68.75%] [G loss: 0.791057]\n",
      "epoch:12 step:12095 [D loss: 0.636425, acc.: 63.28%] [G loss: 0.771064]\n",
      "epoch:12 step:12096 [D loss: 0.627237, acc.: 69.53%] [G loss: 0.902994]\n",
      "epoch:12 step:12097 [D loss: 0.699596, acc.: 54.69%] [G loss: 0.845731]\n",
      "epoch:12 step:12098 [D loss: 0.641262, acc.: 64.06%] [G loss: 0.833258]\n",
      "epoch:12 step:12099 [D loss: 0.670902, acc.: 54.69%] [G loss: 0.758701]\n",
      "epoch:12 step:12100 [D loss: 0.628813, acc.: 67.19%] [G loss: 0.859617]\n",
      "epoch:12 step:12101 [D loss: 0.695911, acc.: 49.22%] [G loss: 0.829486]\n",
      "epoch:12 step:12102 [D loss: 0.683641, acc.: 60.16%] [G loss: 0.838106]\n",
      "epoch:12 step:12103 [D loss: 0.671306, acc.: 61.72%] [G loss: 0.852115]\n",
      "epoch:12 step:12104 [D loss: 0.679715, acc.: 48.44%] [G loss: 0.810089]\n",
      "epoch:12 step:12105 [D loss: 0.725159, acc.: 48.44%] [G loss: 0.858447]\n",
      "epoch:12 step:12106 [D loss: 0.747484, acc.: 36.72%] [G loss: 0.824970]\n",
      "epoch:12 step:12107 [D loss: 0.645841, acc.: 64.84%] [G loss: 0.851709]\n",
      "epoch:12 step:12108 [D loss: 0.742889, acc.: 45.31%] [G loss: 0.870806]\n",
      "epoch:12 step:12109 [D loss: 0.656595, acc.: 53.12%] [G loss: 0.899856]\n",
      "epoch:12 step:12110 [D loss: 0.685500, acc.: 58.59%] [G loss: 0.812935]\n",
      "epoch:12 step:12111 [D loss: 0.679546, acc.: 60.16%] [G loss: 0.799651]\n",
      "epoch:12 step:12112 [D loss: 0.646328, acc.: 51.56%] [G loss: 0.872433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12113 [D loss: 0.638188, acc.: 61.72%] [G loss: 0.842750]\n",
      "epoch:12 step:12114 [D loss: 0.705751, acc.: 51.56%] [G loss: 0.816170]\n",
      "epoch:12 step:12115 [D loss: 0.614409, acc.: 67.19%] [G loss: 0.742965]\n",
      "epoch:12 step:12116 [D loss: 0.625476, acc.: 67.97%] [G loss: 0.775103]\n",
      "epoch:12 step:12117 [D loss: 0.691005, acc.: 47.66%] [G loss: 0.841325]\n",
      "epoch:12 step:12118 [D loss: 0.756801, acc.: 50.78%] [G loss: 0.859089]\n",
      "epoch:12 step:12119 [D loss: 0.626258, acc.: 60.16%] [G loss: 0.991738]\n",
      "epoch:12 step:12120 [D loss: 0.725950, acc.: 53.12%] [G loss: 0.855786]\n",
      "epoch:12 step:12121 [D loss: 0.665642, acc.: 57.03%] [G loss: 0.877008]\n",
      "epoch:12 step:12122 [D loss: 0.676080, acc.: 57.03%] [G loss: 0.809513]\n",
      "epoch:12 step:12123 [D loss: 0.699550, acc.: 47.66%] [G loss: 0.851078]\n",
      "epoch:12 step:12124 [D loss: 0.659937, acc.: 60.16%] [G loss: 0.783033]\n",
      "epoch:12 step:12125 [D loss: 0.615047, acc.: 71.09%] [G loss: 0.789032]\n",
      "epoch:12 step:12126 [D loss: 0.624943, acc.: 67.97%] [G loss: 0.791066]\n",
      "epoch:12 step:12127 [D loss: 0.608349, acc.: 64.06%] [G loss: 0.873513]\n",
      "epoch:12 step:12128 [D loss: 0.630554, acc.: 64.84%] [G loss: 0.899350]\n",
      "epoch:12 step:12129 [D loss: 0.663207, acc.: 57.81%] [G loss: 0.890122]\n",
      "epoch:12 step:12130 [D loss: 0.624626, acc.: 59.38%] [G loss: 0.904486]\n",
      "epoch:12 step:12131 [D loss: 0.656405, acc.: 54.69%] [G loss: 0.907925]\n",
      "epoch:12 step:12132 [D loss: 0.613881, acc.: 69.53%] [G loss: 0.828986]\n",
      "epoch:12 step:12133 [D loss: 0.672834, acc.: 57.81%] [G loss: 0.833522]\n",
      "epoch:12 step:12134 [D loss: 0.659884, acc.: 60.94%] [G loss: 0.738349]\n",
      "epoch:12 step:12135 [D loss: 0.661842, acc.: 57.81%] [G loss: 0.788791]\n",
      "epoch:12 step:12136 [D loss: 0.700751, acc.: 50.78%] [G loss: 0.735688]\n",
      "epoch:12 step:12137 [D loss: 0.646833, acc.: 63.28%] [G loss: 0.740476]\n",
      "epoch:12 step:12138 [D loss: 0.697990, acc.: 50.00%] [G loss: 0.771743]\n",
      "epoch:12 step:12139 [D loss: 0.633371, acc.: 63.28%] [G loss: 0.815096]\n",
      "epoch:12 step:12140 [D loss: 0.681648, acc.: 55.47%] [G loss: 0.809819]\n",
      "epoch:12 step:12141 [D loss: 0.654218, acc.: 64.84%] [G loss: 0.818719]\n",
      "epoch:12 step:12142 [D loss: 0.684994, acc.: 54.69%] [G loss: 0.812144]\n",
      "epoch:12 step:12143 [D loss: 0.714128, acc.: 45.31%] [G loss: 0.816778]\n",
      "epoch:12 step:12144 [D loss: 0.727075, acc.: 46.88%] [G loss: 0.848209]\n",
      "epoch:12 step:12145 [D loss: 0.722174, acc.: 48.44%] [G loss: 0.809647]\n",
      "epoch:12 step:12146 [D loss: 0.669098, acc.: 57.03%] [G loss: 0.846224]\n",
      "epoch:12 step:12147 [D loss: 0.722713, acc.: 44.53%] [G loss: 0.921428]\n",
      "epoch:12 step:12148 [D loss: 0.753483, acc.: 44.53%] [G loss: 0.822740]\n",
      "epoch:12 step:12149 [D loss: 0.715865, acc.: 47.66%] [G loss: 0.769828]\n",
      "epoch:12 step:12150 [D loss: 0.711781, acc.: 46.88%] [G loss: 0.812945]\n",
      "epoch:12 step:12151 [D loss: 0.648260, acc.: 60.16%] [G loss: 0.776150]\n",
      "epoch:12 step:12152 [D loss: 0.724921, acc.: 44.53%] [G loss: 0.710347]\n",
      "epoch:12 step:12153 [D loss: 0.660375, acc.: 60.94%] [G loss: 0.791562]\n",
      "epoch:12 step:12154 [D loss: 0.651867, acc.: 61.72%] [G loss: 0.811318]\n",
      "epoch:12 step:12155 [D loss: 0.688923, acc.: 50.78%] [G loss: 0.774164]\n",
      "epoch:12 step:12156 [D loss: 0.719488, acc.: 45.31%] [G loss: 0.738065]\n",
      "epoch:12 step:12157 [D loss: 0.660477, acc.: 53.91%] [G loss: 0.819615]\n",
      "epoch:12 step:12158 [D loss: 0.677440, acc.: 60.16%] [G loss: 0.748810]\n",
      "epoch:12 step:12159 [D loss: 0.637486, acc.: 67.19%] [G loss: 0.769789]\n",
      "epoch:12 step:12160 [D loss: 0.674383, acc.: 58.59%] [G loss: 0.769824]\n",
      "epoch:12 step:12161 [D loss: 0.673337, acc.: 55.47%] [G loss: 0.820535]\n",
      "epoch:12 step:12162 [D loss: 0.638867, acc.: 64.06%] [G loss: 0.883442]\n",
      "epoch:12 step:12163 [D loss: 0.681765, acc.: 59.38%] [G loss: 0.829576]\n",
      "epoch:12 step:12164 [D loss: 0.707581, acc.: 45.31%] [G loss: 0.722968]\n",
      "epoch:12 step:12165 [D loss: 0.751194, acc.: 39.06%] [G loss: 0.857521]\n",
      "epoch:12 step:12166 [D loss: 0.697246, acc.: 59.38%] [G loss: 0.784307]\n",
      "epoch:12 step:12167 [D loss: 0.644719, acc.: 60.94%] [G loss: 0.874762]\n",
      "epoch:12 step:12168 [D loss: 0.673016, acc.: 56.25%] [G loss: 0.797506]\n",
      "epoch:12 step:12169 [D loss: 0.650269, acc.: 61.72%] [G loss: 0.803649]\n",
      "epoch:12 step:12170 [D loss: 0.684243, acc.: 57.03%] [G loss: 0.861125]\n",
      "epoch:12 step:12171 [D loss: 0.700016, acc.: 49.22%] [G loss: 0.781469]\n",
      "epoch:12 step:12172 [D loss: 0.670890, acc.: 62.50%] [G loss: 0.834583]\n",
      "epoch:12 step:12173 [D loss: 0.688188, acc.: 51.56%] [G loss: 0.795578]\n",
      "epoch:12 step:12174 [D loss: 0.685166, acc.: 54.69%] [G loss: 0.876449]\n",
      "epoch:12 step:12175 [D loss: 0.694629, acc.: 50.00%] [G loss: 0.823900]\n",
      "epoch:12 step:12176 [D loss: 0.695704, acc.: 59.38%] [G loss: 0.819240]\n",
      "epoch:12 step:12177 [D loss: 0.655225, acc.: 60.94%] [G loss: 0.796612]\n",
      "epoch:12 step:12178 [D loss: 0.697780, acc.: 54.69%] [G loss: 0.787922]\n",
      "epoch:12 step:12179 [D loss: 0.634556, acc.: 76.56%] [G loss: 0.842522]\n",
      "epoch:12 step:12180 [D loss: 0.695127, acc.: 50.78%] [G loss: 0.817756]\n",
      "epoch:12 step:12181 [D loss: 0.685319, acc.: 53.91%] [G loss: 0.821563]\n",
      "epoch:13 step:12182 [D loss: 0.692693, acc.: 58.59%] [G loss: 0.786589]\n",
      "epoch:13 step:12183 [D loss: 0.718230, acc.: 54.69%] [G loss: 0.782101]\n",
      "epoch:13 step:12184 [D loss: 0.714418, acc.: 47.66%] [G loss: 0.806772]\n",
      "epoch:13 step:12185 [D loss: 0.706277, acc.: 50.78%] [G loss: 0.791052]\n",
      "epoch:13 step:12186 [D loss: 0.648035, acc.: 64.06%] [G loss: 0.791325]\n",
      "epoch:13 step:12187 [D loss: 0.678687, acc.: 52.34%] [G loss: 0.805013]\n",
      "epoch:13 step:12188 [D loss: 0.672217, acc.: 54.69%] [G loss: 0.827119]\n",
      "epoch:13 step:12189 [D loss: 0.686917, acc.: 53.91%] [G loss: 0.771106]\n",
      "epoch:13 step:12190 [D loss: 0.681357, acc.: 56.25%] [G loss: 0.783982]\n",
      "epoch:13 step:12191 [D loss: 0.674624, acc.: 56.25%] [G loss: 0.882528]\n",
      "epoch:13 step:12192 [D loss: 0.670933, acc.: 57.03%] [G loss: 0.819138]\n",
      "epoch:13 step:12193 [D loss: 0.676260, acc.: 56.25%] [G loss: 0.826372]\n",
      "epoch:13 step:12194 [D loss: 0.674002, acc.: 57.03%] [G loss: 0.752122]\n",
      "epoch:13 step:12195 [D loss: 0.668218, acc.: 62.50%] [G loss: 0.873172]\n",
      "epoch:13 step:12196 [D loss: 0.672685, acc.: 57.81%] [G loss: 0.879281]\n",
      "epoch:13 step:12197 [D loss: 0.651977, acc.: 62.50%] [G loss: 0.830302]\n",
      "epoch:13 step:12198 [D loss: 0.661568, acc.: 60.16%] [G loss: 0.880630]\n",
      "epoch:13 step:12199 [D loss: 0.670069, acc.: 53.12%] [G loss: 0.785153]\n",
      "epoch:13 step:12200 [D loss: 0.659912, acc.: 64.84%] [G loss: 0.801228]\n",
      "epoch:13 step:12201 [D loss: 0.719453, acc.: 47.66%] [G loss: 0.887120]\n",
      "epoch:13 step:12202 [D loss: 0.665394, acc.: 59.38%] [G loss: 0.875572]\n",
      "epoch:13 step:12203 [D loss: 0.618224, acc.: 67.97%] [G loss: 0.800241]\n",
      "epoch:13 step:12204 [D loss: 0.647895, acc.: 63.28%] [G loss: 0.905149]\n",
      "epoch:13 step:12205 [D loss: 0.697969, acc.: 54.69%] [G loss: 0.848982]\n",
      "epoch:13 step:12206 [D loss: 0.661429, acc.: 54.69%] [G loss: 0.771241]\n",
      "epoch:13 step:12207 [D loss: 0.729471, acc.: 42.19%] [G loss: 0.755534]\n",
      "epoch:13 step:12208 [D loss: 0.647594, acc.: 65.62%] [G loss: 0.871639]\n",
      "epoch:13 step:12209 [D loss: 0.682205, acc.: 47.66%] [G loss: 0.732275]\n",
      "epoch:13 step:12210 [D loss: 0.690447, acc.: 49.22%] [G loss: 0.799666]\n",
      "epoch:13 step:12211 [D loss: 0.642311, acc.: 58.59%] [G loss: 0.837153]\n",
      "epoch:13 step:12212 [D loss: 0.689026, acc.: 51.56%] [G loss: 0.799924]\n",
      "epoch:13 step:12213 [D loss: 0.633947, acc.: 67.97%] [G loss: 0.884147]\n",
      "epoch:13 step:12214 [D loss: 0.683891, acc.: 56.25%] [G loss: 0.870088]\n",
      "epoch:13 step:12215 [D loss: 0.660969, acc.: 61.72%] [G loss: 0.763732]\n",
      "epoch:13 step:12216 [D loss: 0.651004, acc.: 60.94%] [G loss: 0.702917]\n",
      "epoch:13 step:12217 [D loss: 0.668412, acc.: 61.72%] [G loss: 0.845097]\n",
      "epoch:13 step:12218 [D loss: 0.618524, acc.: 69.53%] [G loss: 0.832949]\n",
      "epoch:13 step:12219 [D loss: 0.715956, acc.: 57.81%] [G loss: 0.818132]\n",
      "epoch:13 step:12220 [D loss: 0.674499, acc.: 65.62%] [G loss: 0.840802]\n",
      "epoch:13 step:12221 [D loss: 0.679390, acc.: 56.25%] [G loss: 0.809465]\n",
      "epoch:13 step:12222 [D loss: 0.703918, acc.: 47.66%] [G loss: 0.846038]\n",
      "epoch:13 step:12223 [D loss: 0.700136, acc.: 49.22%] [G loss: 0.904093]\n",
      "epoch:13 step:12224 [D loss: 0.634648, acc.: 66.41%] [G loss: 1.686491]\n",
      "epoch:13 step:12225 [D loss: 0.687822, acc.: 64.06%] [G loss: 0.746777]\n",
      "epoch:13 step:12226 [D loss: 0.725918, acc.: 41.41%] [G loss: 0.747827]\n",
      "epoch:13 step:12227 [D loss: 0.656184, acc.: 55.47%] [G loss: 0.755287]\n",
      "epoch:13 step:12228 [D loss: 0.652892, acc.: 58.59%] [G loss: 0.782704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12229 [D loss: 0.675437, acc.: 57.81%] [G loss: 0.771820]\n",
      "epoch:13 step:12230 [D loss: 0.612708, acc.: 67.19%] [G loss: 1.002756]\n",
      "epoch:13 step:12231 [D loss: 0.617515, acc.: 67.97%] [G loss: 0.916158]\n",
      "epoch:13 step:12232 [D loss: 0.655861, acc.: 59.38%] [G loss: 0.869437]\n",
      "epoch:13 step:12233 [D loss: 0.555246, acc.: 71.88%] [G loss: 0.913475]\n",
      "epoch:13 step:12234 [D loss: 0.647105, acc.: 59.38%] [G loss: 0.716763]\n",
      "epoch:13 step:12235 [D loss: 0.701163, acc.: 52.34%] [G loss: 0.752312]\n",
      "epoch:13 step:12236 [D loss: 0.699933, acc.: 53.91%] [G loss: 0.710033]\n",
      "epoch:13 step:12237 [D loss: 0.650696, acc.: 60.94%] [G loss: 0.841832]\n",
      "epoch:13 step:12238 [D loss: 0.686012, acc.: 53.12%] [G loss: 0.809326]\n",
      "epoch:13 step:12239 [D loss: 0.647873, acc.: 67.19%] [G loss: 0.874644]\n",
      "epoch:13 step:12240 [D loss: 0.663249, acc.: 60.94%] [G loss: 0.852459]\n",
      "epoch:13 step:12241 [D loss: 0.650473, acc.: 60.94%] [G loss: 0.910312]\n",
      "epoch:13 step:12242 [D loss: 0.665240, acc.: 64.84%] [G loss: 0.837027]\n",
      "epoch:13 step:12243 [D loss: 0.657632, acc.: 60.16%] [G loss: 0.832112]\n",
      "epoch:13 step:12244 [D loss: 0.639284, acc.: 60.16%] [G loss: 1.072180]\n",
      "epoch:13 step:12245 [D loss: 0.693788, acc.: 54.69%] [G loss: 0.865075]\n",
      "epoch:13 step:12246 [D loss: 0.677308, acc.: 53.91%] [G loss: 0.848316]\n",
      "epoch:13 step:12247 [D loss: 0.715961, acc.: 55.47%] [G loss: 0.913444]\n",
      "epoch:13 step:12248 [D loss: 0.641931, acc.: 61.72%] [G loss: 0.900856]\n",
      "epoch:13 step:12249 [D loss: 0.659793, acc.: 54.69%] [G loss: 0.807366]\n",
      "epoch:13 step:12250 [D loss: 0.712548, acc.: 47.66%] [G loss: 0.779429]\n",
      "epoch:13 step:12251 [D loss: 0.636967, acc.: 63.28%] [G loss: 0.821928]\n",
      "epoch:13 step:12252 [D loss: 0.668992, acc.: 57.03%] [G loss: 0.851045]\n",
      "epoch:13 step:12253 [D loss: 0.744508, acc.: 46.88%] [G loss: 0.862486]\n",
      "epoch:13 step:12254 [D loss: 0.675166, acc.: 52.34%] [G loss: 1.232985]\n",
      "epoch:13 step:12255 [D loss: 0.723937, acc.: 46.09%] [G loss: 0.808953]\n",
      "epoch:13 step:12256 [D loss: 0.639477, acc.: 67.19%] [G loss: 0.802972]\n",
      "epoch:13 step:12257 [D loss: 0.699174, acc.: 50.78%] [G loss: 0.874280]\n",
      "epoch:13 step:12258 [D loss: 0.693822, acc.: 50.00%] [G loss: 0.881815]\n",
      "epoch:13 step:12259 [D loss: 0.635983, acc.: 63.28%] [G loss: 0.777780]\n",
      "epoch:13 step:12260 [D loss: 0.681366, acc.: 56.25%] [G loss: 0.726818]\n",
      "epoch:13 step:12261 [D loss: 0.673647, acc.: 61.72%] [G loss: 0.808040]\n",
      "epoch:13 step:12262 [D loss: 0.670675, acc.: 59.38%] [G loss: 0.739621]\n",
      "epoch:13 step:12263 [D loss: 0.652101, acc.: 58.59%] [G loss: 0.758915]\n",
      "epoch:13 step:12264 [D loss: 0.680636, acc.: 64.06%] [G loss: 0.759300]\n",
      "epoch:13 step:12265 [D loss: 0.630374, acc.: 69.53%] [G loss: 0.883717]\n",
      "epoch:13 step:12266 [D loss: 0.676336, acc.: 57.81%] [G loss: 0.833744]\n",
      "epoch:13 step:12267 [D loss: 0.689365, acc.: 53.91%] [G loss: 0.752529]\n",
      "epoch:13 step:12268 [D loss: 0.651748, acc.: 65.62%] [G loss: 0.874616]\n",
      "epoch:13 step:12269 [D loss: 0.727356, acc.: 42.97%] [G loss: 0.774029]\n",
      "epoch:13 step:12270 [D loss: 0.614787, acc.: 78.12%] [G loss: 0.843636]\n",
      "epoch:13 step:12271 [D loss: 0.666682, acc.: 57.03%] [G loss: 0.781591]\n",
      "epoch:13 step:12272 [D loss: 0.670684, acc.: 52.34%] [G loss: 0.837029]\n",
      "epoch:13 step:12273 [D loss: 0.695300, acc.: 54.69%] [G loss: 0.799875]\n",
      "epoch:13 step:12274 [D loss: 0.607290, acc.: 70.31%] [G loss: 0.907299]\n",
      "epoch:13 step:12275 [D loss: 0.623344, acc.: 64.84%] [G loss: 0.860420]\n",
      "epoch:13 step:12276 [D loss: 0.715154, acc.: 45.31%] [G loss: 0.909042]\n",
      "epoch:13 step:12277 [D loss: 0.649720, acc.: 57.81%] [G loss: 0.839887]\n",
      "epoch:13 step:12278 [D loss: 0.615798, acc.: 70.31%] [G loss: 0.816937]\n",
      "epoch:13 step:12279 [D loss: 0.658362, acc.: 64.84%] [G loss: 0.869778]\n",
      "epoch:13 step:12280 [D loss: 0.664014, acc.: 64.06%] [G loss: 0.917743]\n",
      "epoch:13 step:12281 [D loss: 0.613121, acc.: 69.53%] [G loss: 0.837487]\n",
      "epoch:13 step:12282 [D loss: 0.674435, acc.: 55.47%] [G loss: 0.897172]\n",
      "epoch:13 step:12283 [D loss: 0.692929, acc.: 52.34%] [G loss: 0.802326]\n",
      "epoch:13 step:12284 [D loss: 0.669129, acc.: 56.25%] [G loss: 0.893779]\n",
      "epoch:13 step:12285 [D loss: 0.687087, acc.: 50.00%] [G loss: 0.804828]\n",
      "epoch:13 step:12286 [D loss: 0.672135, acc.: 55.47%] [G loss: 0.810821]\n",
      "epoch:13 step:12287 [D loss: 0.683347, acc.: 58.59%] [G loss: 0.805773]\n",
      "epoch:13 step:12288 [D loss: 0.714526, acc.: 52.34%] [G loss: 0.793828]\n",
      "epoch:13 step:12289 [D loss: 0.627504, acc.: 60.16%] [G loss: 0.790736]\n",
      "epoch:13 step:12290 [D loss: 0.720978, acc.: 38.28%] [G loss: 0.829954]\n",
      "epoch:13 step:12291 [D loss: 0.697160, acc.: 46.88%] [G loss: 0.807612]\n",
      "epoch:13 step:12292 [D loss: 0.678052, acc.: 58.59%] [G loss: 0.801736]\n",
      "epoch:13 step:12293 [D loss: 0.694464, acc.: 51.56%] [G loss: 0.811299]\n",
      "epoch:13 step:12294 [D loss: 0.669128, acc.: 59.38%] [G loss: 0.851800]\n",
      "epoch:13 step:12295 [D loss: 0.689656, acc.: 55.47%] [G loss: 0.831041]\n",
      "epoch:13 step:12296 [D loss: 0.690866, acc.: 53.91%] [G loss: 0.781520]\n",
      "epoch:13 step:12297 [D loss: 0.677268, acc.: 56.25%] [G loss: 0.823966]\n",
      "epoch:13 step:12298 [D loss: 0.646587, acc.: 58.59%] [G loss: 0.850986]\n",
      "epoch:13 step:12299 [D loss: 0.674603, acc.: 59.38%] [G loss: 0.846372]\n",
      "epoch:13 step:12300 [D loss: 0.640809, acc.: 56.25%] [G loss: 0.969398]\n",
      "epoch:13 step:12301 [D loss: 0.697231, acc.: 49.22%] [G loss: 0.907464]\n",
      "epoch:13 step:12302 [D loss: 0.665726, acc.: 57.03%] [G loss: 0.800666]\n",
      "epoch:13 step:12303 [D loss: 0.667372, acc.: 53.91%] [G loss: 0.738948]\n",
      "epoch:13 step:12304 [D loss: 0.713033, acc.: 48.44%] [G loss: 0.794507]\n",
      "epoch:13 step:12305 [D loss: 0.684197, acc.: 51.56%] [G loss: 0.776028]\n",
      "epoch:13 step:12306 [D loss: 0.672630, acc.: 61.72%] [G loss: 0.721774]\n",
      "epoch:13 step:12307 [D loss: 0.696215, acc.: 48.44%] [G loss: 0.775287]\n",
      "epoch:13 step:12308 [D loss: 0.709400, acc.: 54.69%] [G loss: 0.781031]\n",
      "epoch:13 step:12309 [D loss: 0.689435, acc.: 48.44%] [G loss: 0.840917]\n",
      "epoch:13 step:12310 [D loss: 0.667123, acc.: 61.72%] [G loss: 0.843610]\n",
      "epoch:13 step:12311 [D loss: 0.665749, acc.: 60.16%] [G loss: 0.940587]\n",
      "epoch:13 step:12312 [D loss: 0.680230, acc.: 51.56%] [G loss: 0.932428]\n",
      "epoch:13 step:12313 [D loss: 0.686746, acc.: 53.12%] [G loss: 0.844627]\n",
      "epoch:13 step:12314 [D loss: 0.666255, acc.: 53.91%] [G loss: 0.773444]\n",
      "epoch:13 step:12315 [D loss: 0.651743, acc.: 53.12%] [G loss: 0.853803]\n",
      "epoch:13 step:12316 [D loss: 0.649970, acc.: 60.94%] [G loss: 0.873611]\n",
      "epoch:13 step:12317 [D loss: 0.708195, acc.: 54.69%] [G loss: 0.813793]\n",
      "epoch:13 step:12318 [D loss: 0.669619, acc.: 55.47%] [G loss: 0.853088]\n",
      "epoch:13 step:12319 [D loss: 0.731144, acc.: 51.56%] [G loss: 0.737283]\n",
      "epoch:13 step:12320 [D loss: 0.639022, acc.: 65.62%] [G loss: 0.752047]\n",
      "epoch:13 step:12321 [D loss: 0.706801, acc.: 52.34%] [G loss: 0.779466]\n",
      "epoch:13 step:12322 [D loss: 0.652262, acc.: 64.84%] [G loss: 0.774842]\n",
      "epoch:13 step:12323 [D loss: 0.698468, acc.: 53.12%] [G loss: 0.819322]\n",
      "epoch:13 step:12324 [D loss: 0.651977, acc.: 56.25%] [G loss: 0.907783]\n",
      "epoch:13 step:12325 [D loss: 0.659560, acc.: 59.38%] [G loss: 0.784762]\n",
      "epoch:13 step:12326 [D loss: 0.695525, acc.: 53.91%] [G loss: 0.767140]\n",
      "epoch:13 step:12327 [D loss: 0.672147, acc.: 57.03%] [G loss: 0.928213]\n",
      "epoch:13 step:12328 [D loss: 0.670062, acc.: 53.12%] [G loss: 0.836777]\n",
      "epoch:13 step:12329 [D loss: 0.671721, acc.: 57.03%] [G loss: 0.739256]\n",
      "epoch:13 step:12330 [D loss: 0.666393, acc.: 59.38%] [G loss: 0.892722]\n",
      "epoch:13 step:12331 [D loss: 0.629805, acc.: 65.62%] [G loss: 0.947772]\n",
      "epoch:13 step:12332 [D loss: 0.650990, acc.: 66.41%] [G loss: 0.873777]\n",
      "epoch:13 step:12333 [D loss: 0.688997, acc.: 53.12%] [G loss: 0.896411]\n",
      "epoch:13 step:12334 [D loss: 0.664087, acc.: 56.25%] [G loss: 0.801352]\n",
      "epoch:13 step:12335 [D loss: 0.671715, acc.: 54.69%] [G loss: 0.797148]\n",
      "epoch:13 step:12336 [D loss: 0.652362, acc.: 61.72%] [G loss: 0.841230]\n",
      "epoch:13 step:12337 [D loss: 0.648213, acc.: 57.81%] [G loss: 0.858487]\n",
      "epoch:13 step:12338 [D loss: 0.698317, acc.: 51.56%] [G loss: 0.846424]\n",
      "epoch:13 step:12339 [D loss: 0.620838, acc.: 67.19%] [G loss: 0.777149]\n",
      "epoch:13 step:12340 [D loss: 0.710826, acc.: 51.56%] [G loss: 0.707120]\n",
      "epoch:13 step:12341 [D loss: 0.695754, acc.: 57.03%] [G loss: 0.800942]\n",
      "epoch:13 step:12342 [D loss: 0.681027, acc.: 54.69%] [G loss: 0.829475]\n",
      "epoch:13 step:12343 [D loss: 0.673893, acc.: 55.47%] [G loss: 0.841285]\n",
      "epoch:13 step:12344 [D loss: 0.665217, acc.: 52.34%] [G loss: 0.854028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12345 [D loss: 0.663827, acc.: 56.25%] [G loss: 0.774556]\n",
      "epoch:13 step:12346 [D loss: 0.615590, acc.: 70.31%] [G loss: 0.980298]\n",
      "epoch:13 step:12347 [D loss: 0.667209, acc.: 57.03%] [G loss: 0.945965]\n",
      "epoch:13 step:12348 [D loss: 0.640960, acc.: 64.84%] [G loss: 0.849524]\n",
      "epoch:13 step:12349 [D loss: 0.672910, acc.: 60.16%] [G loss: 1.006947]\n",
      "epoch:13 step:12350 [D loss: 0.614985, acc.: 69.53%] [G loss: 0.900268]\n",
      "epoch:13 step:12351 [D loss: 0.630556, acc.: 62.50%] [G loss: 0.869947]\n",
      "epoch:13 step:12352 [D loss: 0.684320, acc.: 57.03%] [G loss: 0.843524]\n",
      "epoch:13 step:12353 [D loss: 0.724204, acc.: 51.56%] [G loss: 0.982714]\n",
      "epoch:13 step:12354 [D loss: 0.671636, acc.: 63.28%] [G loss: 0.809656]\n",
      "epoch:13 step:12355 [D loss: 0.608597, acc.: 74.22%] [G loss: 0.725944]\n",
      "epoch:13 step:12356 [D loss: 0.694015, acc.: 50.78%] [G loss: 0.882012]\n",
      "epoch:13 step:12357 [D loss: 0.689405, acc.: 59.38%] [G loss: 0.789716]\n",
      "epoch:13 step:12358 [D loss: 0.624908, acc.: 66.41%] [G loss: 0.878936]\n",
      "epoch:13 step:12359 [D loss: 0.695157, acc.: 54.69%] [G loss: 0.846840]\n",
      "epoch:13 step:12360 [D loss: 0.660385, acc.: 64.06%] [G loss: 0.892174]\n",
      "epoch:13 step:12361 [D loss: 0.628302, acc.: 67.19%] [G loss: 0.803588]\n",
      "epoch:13 step:12362 [D loss: 0.717636, acc.: 50.00%] [G loss: 0.811246]\n",
      "epoch:13 step:12363 [D loss: 0.613793, acc.: 71.88%] [G loss: 0.901967]\n",
      "epoch:13 step:12364 [D loss: 0.681664, acc.: 52.34%] [G loss: 0.926785]\n",
      "epoch:13 step:12365 [D loss: 0.696421, acc.: 58.59%] [G loss: 0.871297]\n",
      "epoch:13 step:12366 [D loss: 0.705033, acc.: 52.34%] [G loss: 0.850091]\n",
      "epoch:13 step:12367 [D loss: 0.627254, acc.: 69.53%] [G loss: 0.876207]\n",
      "epoch:13 step:12368 [D loss: 0.652043, acc.: 64.84%] [G loss: 1.073595]\n",
      "epoch:13 step:12369 [D loss: 0.642349, acc.: 60.16%] [G loss: 0.840841]\n",
      "epoch:13 step:12370 [D loss: 0.654565, acc.: 61.72%] [G loss: 0.852779]\n",
      "epoch:13 step:12371 [D loss: 0.654204, acc.: 59.38%] [G loss: 0.943694]\n",
      "epoch:13 step:12372 [D loss: 0.727554, acc.: 45.31%] [G loss: 0.757427]\n",
      "epoch:13 step:12373 [D loss: 0.701264, acc.: 53.91%] [G loss: 0.866416]\n",
      "epoch:13 step:12374 [D loss: 0.719110, acc.: 50.00%] [G loss: 0.780885]\n",
      "epoch:13 step:12375 [D loss: 0.709504, acc.: 44.53%] [G loss: 0.797638]\n",
      "epoch:13 step:12376 [D loss: 0.600982, acc.: 73.44%] [G loss: 0.857885]\n",
      "epoch:13 step:12377 [D loss: 0.688111, acc.: 53.12%] [G loss: 0.891199]\n",
      "epoch:13 step:12378 [D loss: 0.597707, acc.: 72.66%] [G loss: 0.797998]\n",
      "epoch:13 step:12379 [D loss: 0.677244, acc.: 53.91%] [G loss: 0.845312]\n",
      "epoch:13 step:12380 [D loss: 0.727358, acc.: 46.88%] [G loss: 0.833694]\n",
      "epoch:13 step:12381 [D loss: 0.627791, acc.: 67.19%] [G loss: 0.831550]\n",
      "epoch:13 step:12382 [D loss: 0.642880, acc.: 56.25%] [G loss: 0.810314]\n",
      "epoch:13 step:12383 [D loss: 0.678499, acc.: 63.28%] [G loss: 0.795114]\n",
      "epoch:13 step:12384 [D loss: 0.665977, acc.: 60.16%] [G loss: 0.793275]\n",
      "epoch:13 step:12385 [D loss: 0.632964, acc.: 63.28%] [G loss: 0.820981]\n",
      "epoch:13 step:12386 [D loss: 0.723550, acc.: 51.56%] [G loss: 1.017087]\n",
      "epoch:13 step:12387 [D loss: 0.735825, acc.: 47.66%] [G loss: 0.907051]\n",
      "epoch:13 step:12388 [D loss: 0.656926, acc.: 64.06%] [G loss: 0.767691]\n",
      "epoch:13 step:12389 [D loss: 0.636587, acc.: 60.94%] [G loss: 0.857738]\n",
      "epoch:13 step:12390 [D loss: 0.643096, acc.: 58.59%] [G loss: 0.977263]\n",
      "epoch:13 step:12391 [D loss: 0.607852, acc.: 74.22%] [G loss: 0.971151]\n",
      "epoch:13 step:12392 [D loss: 0.678074, acc.: 64.84%] [G loss: 0.976061]\n",
      "epoch:13 step:12393 [D loss: 0.689426, acc.: 53.91%] [G loss: 0.840147]\n",
      "epoch:13 step:12394 [D loss: 0.667379, acc.: 60.94%] [G loss: 0.921084]\n",
      "epoch:13 step:12395 [D loss: 0.669650, acc.: 57.81%] [G loss: 0.861854]\n",
      "epoch:13 step:12396 [D loss: 0.671414, acc.: 55.47%] [G loss: 0.876020]\n",
      "epoch:13 step:12397 [D loss: 0.628256, acc.: 66.41%] [G loss: 0.831694]\n",
      "epoch:13 step:12398 [D loss: 0.679457, acc.: 55.47%] [G loss: 0.883941]\n",
      "epoch:13 step:12399 [D loss: 0.678653, acc.: 57.03%] [G loss: 0.765394]\n",
      "epoch:13 step:12400 [D loss: 0.627292, acc.: 64.06%] [G loss: 0.823368]\n",
      "epoch:13 step:12401 [D loss: 0.675168, acc.: 52.34%] [G loss: 0.889429]\n",
      "epoch:13 step:12402 [D loss: 0.649070, acc.: 54.69%] [G loss: 0.805941]\n",
      "epoch:13 step:12403 [D loss: 0.699484, acc.: 53.91%] [G loss: 0.836540]\n",
      "epoch:13 step:12404 [D loss: 0.675612, acc.: 50.78%] [G loss: 0.702474]\n",
      "epoch:13 step:12405 [D loss: 0.594900, acc.: 70.31%] [G loss: 0.811606]\n",
      "epoch:13 step:12406 [D loss: 0.689397, acc.: 52.34%] [G loss: 0.799188]\n",
      "epoch:13 step:12407 [D loss: 0.710718, acc.: 53.12%] [G loss: 0.857606]\n",
      "epoch:13 step:12408 [D loss: 0.652103, acc.: 63.28%] [G loss: 0.843467]\n",
      "epoch:13 step:12409 [D loss: 0.641436, acc.: 61.72%] [G loss: 0.875895]\n",
      "epoch:13 step:12410 [D loss: 0.621509, acc.: 65.62%] [G loss: 1.009483]\n",
      "epoch:13 step:12411 [D loss: 0.710686, acc.: 52.34%] [G loss: 0.813271]\n",
      "epoch:13 step:12412 [D loss: 0.638550, acc.: 60.94%] [G loss: 0.807439]\n",
      "epoch:13 step:12413 [D loss: 0.686182, acc.: 57.03%] [G loss: 0.793288]\n",
      "epoch:13 step:12414 [D loss: 0.656602, acc.: 60.16%] [G loss: 0.899178]\n",
      "epoch:13 step:12415 [D loss: 0.673535, acc.: 60.16%] [G loss: 0.779326]\n",
      "epoch:13 step:12416 [D loss: 0.688493, acc.: 54.69%] [G loss: 0.816590]\n",
      "epoch:13 step:12417 [D loss: 0.695038, acc.: 51.56%] [G loss: 0.782188]\n",
      "epoch:13 step:12418 [D loss: 0.692794, acc.: 53.91%] [G loss: 0.762862]\n",
      "epoch:13 step:12419 [D loss: 0.691927, acc.: 64.06%] [G loss: 0.724337]\n",
      "epoch:13 step:12420 [D loss: 0.724345, acc.: 54.69%] [G loss: 0.801296]\n",
      "epoch:13 step:12421 [D loss: 0.704430, acc.: 50.78%] [G loss: 0.892429]\n",
      "epoch:13 step:12422 [D loss: 0.676162, acc.: 57.03%] [G loss: 0.799113]\n",
      "epoch:13 step:12423 [D loss: 0.665054, acc.: 58.59%] [G loss: 0.784966]\n",
      "epoch:13 step:12424 [D loss: 0.698597, acc.: 53.91%] [G loss: 0.910001]\n",
      "epoch:13 step:12425 [D loss: 0.724530, acc.: 50.78%] [G loss: 0.830697]\n",
      "epoch:13 step:12426 [D loss: 0.682905, acc.: 53.12%] [G loss: 0.834404]\n",
      "epoch:13 step:12427 [D loss: 0.659884, acc.: 59.38%] [G loss: 0.778498]\n",
      "epoch:13 step:12428 [D loss: 0.684417, acc.: 51.56%] [G loss: 0.926165]\n",
      "epoch:13 step:12429 [D loss: 0.676017, acc.: 56.25%] [G loss: 0.783971]\n",
      "epoch:13 step:12430 [D loss: 0.667448, acc.: 59.38%] [G loss: 0.798625]\n",
      "epoch:13 step:12431 [D loss: 0.656026, acc.: 64.84%] [G loss: 0.885228]\n",
      "epoch:13 step:12432 [D loss: 0.658122, acc.: 53.91%] [G loss: 0.808079]\n",
      "epoch:13 step:12433 [D loss: 0.696797, acc.: 50.78%] [G loss: 0.766318]\n",
      "epoch:13 step:12434 [D loss: 0.718409, acc.: 42.97%] [G loss: 0.790374]\n",
      "epoch:13 step:12435 [D loss: 0.683786, acc.: 52.34%] [G loss: 0.721094]\n",
      "epoch:13 step:12436 [D loss: 0.674961, acc.: 53.91%] [G loss: 0.778208]\n",
      "epoch:13 step:12437 [D loss: 0.680461, acc.: 50.78%] [G loss: 0.781456]\n",
      "epoch:13 step:12438 [D loss: 0.659887, acc.: 60.16%] [G loss: 0.762401]\n",
      "epoch:13 step:12439 [D loss: 0.661995, acc.: 57.81%] [G loss: 0.762505]\n",
      "epoch:13 step:12440 [D loss: 0.684819, acc.: 48.44%] [G loss: 0.850397]\n",
      "epoch:13 step:12441 [D loss: 0.652812, acc.: 60.16%] [G loss: 0.745798]\n",
      "epoch:13 step:12442 [D loss: 0.646934, acc.: 63.28%] [G loss: 1.028295]\n",
      "epoch:13 step:12443 [D loss: 0.640340, acc.: 64.06%] [G loss: 0.842090]\n",
      "epoch:13 step:12444 [D loss: 0.651093, acc.: 61.72%] [G loss: 0.817246]\n",
      "epoch:13 step:12445 [D loss: 0.684692, acc.: 52.34%] [G loss: 0.777275]\n",
      "epoch:13 step:12446 [D loss: 0.660768, acc.: 57.81%] [G loss: 0.773002]\n",
      "epoch:13 step:12447 [D loss: 0.656362, acc.: 56.25%] [G loss: 0.730629]\n",
      "epoch:13 step:12448 [D loss: 0.673103, acc.: 57.81%] [G loss: 0.766375]\n",
      "epoch:13 step:12449 [D loss: 0.628241, acc.: 67.19%] [G loss: 0.779644]\n",
      "epoch:13 step:12450 [D loss: 0.688765, acc.: 55.47%] [G loss: 0.863091]\n",
      "epoch:13 step:12451 [D loss: 0.639180, acc.: 52.34%] [G loss: 0.802884]\n",
      "epoch:13 step:12452 [D loss: 0.675240, acc.: 56.25%] [G loss: 0.776294]\n",
      "epoch:13 step:12453 [D loss: 0.679388, acc.: 60.94%] [G loss: 0.837766]\n",
      "epoch:13 step:12454 [D loss: 0.624870, acc.: 64.06%] [G loss: 0.841145]\n",
      "epoch:13 step:12455 [D loss: 0.635823, acc.: 67.97%] [G loss: 0.834154]\n",
      "epoch:13 step:12456 [D loss: 0.662486, acc.: 61.72%] [G loss: 0.982751]\n",
      "epoch:13 step:12457 [D loss: 0.699061, acc.: 56.25%] [G loss: 0.868713]\n",
      "epoch:13 step:12458 [D loss: 0.700085, acc.: 50.78%] [G loss: 0.914392]\n",
      "epoch:13 step:12459 [D loss: 0.651821, acc.: 60.94%] [G loss: 0.856104]\n",
      "epoch:13 step:12460 [D loss: 0.716671, acc.: 44.53%] [G loss: 0.817005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12461 [D loss: 0.715505, acc.: 48.44%] [G loss: 0.899647]\n",
      "epoch:13 step:12462 [D loss: 0.722696, acc.: 46.88%] [G loss: 0.797732]\n",
      "epoch:13 step:12463 [D loss: 0.677667, acc.: 60.16%] [G loss: 0.844755]\n",
      "epoch:13 step:12464 [D loss: 0.668623, acc.: 62.50%] [G loss: 0.831955]\n",
      "epoch:13 step:12465 [D loss: 0.714006, acc.: 56.25%] [G loss: 0.845225]\n",
      "epoch:13 step:12466 [D loss: 0.658176, acc.: 60.16%] [G loss: 0.866472]\n",
      "epoch:13 step:12467 [D loss: 0.599099, acc.: 68.75%] [G loss: 0.974063]\n",
      "epoch:13 step:12468 [D loss: 0.694688, acc.: 52.34%] [G loss: 0.886779]\n",
      "epoch:13 step:12469 [D loss: 0.705845, acc.: 49.22%] [G loss: 0.953421]\n",
      "epoch:13 step:12470 [D loss: 0.667504, acc.: 61.72%] [G loss: 0.892897]\n",
      "epoch:13 step:12471 [D loss: 0.646734, acc.: 65.62%] [G loss: 0.930829]\n",
      "epoch:13 step:12472 [D loss: 0.657850, acc.: 57.81%] [G loss: 0.822474]\n",
      "epoch:13 step:12473 [D loss: 0.731352, acc.: 45.31%] [G loss: 0.801791]\n",
      "epoch:13 step:12474 [D loss: 0.674613, acc.: 51.56%] [G loss: 0.810827]\n",
      "epoch:13 step:12475 [D loss: 0.662779, acc.: 59.38%] [G loss: 0.886023]\n",
      "epoch:13 step:12476 [D loss: 0.685658, acc.: 46.88%] [G loss: 0.826951]\n",
      "epoch:13 step:12477 [D loss: 0.652085, acc.: 63.28%] [G loss: 0.829973]\n",
      "epoch:13 step:12478 [D loss: 0.668122, acc.: 61.72%] [G loss: 0.768671]\n",
      "epoch:13 step:12479 [D loss: 0.657461, acc.: 60.16%] [G loss: 0.788029]\n",
      "epoch:13 step:12480 [D loss: 0.668537, acc.: 57.81%] [G loss: 0.808865]\n",
      "epoch:13 step:12481 [D loss: 0.681456, acc.: 53.91%] [G loss: 0.783692]\n",
      "epoch:13 step:12482 [D loss: 0.660692, acc.: 60.16%] [G loss: 0.766630]\n",
      "epoch:13 step:12483 [D loss: 0.601864, acc.: 71.09%] [G loss: 0.805834]\n",
      "epoch:13 step:12484 [D loss: 0.673451, acc.: 58.59%] [G loss: 0.786682]\n",
      "epoch:13 step:12485 [D loss: 0.744328, acc.: 39.84%] [G loss: 0.752065]\n",
      "epoch:13 step:12486 [D loss: 0.738518, acc.: 37.50%] [G loss: 0.805501]\n",
      "epoch:13 step:12487 [D loss: 0.658177, acc.: 67.97%] [G loss: 0.904534]\n",
      "epoch:13 step:12488 [D loss: 0.661449, acc.: 58.59%] [G loss: 0.770645]\n",
      "epoch:13 step:12489 [D loss: 0.676574, acc.: 60.16%] [G loss: 0.857864]\n",
      "epoch:13 step:12490 [D loss: 0.611277, acc.: 73.44%] [G loss: 0.795629]\n",
      "epoch:13 step:12491 [D loss: 0.642697, acc.: 64.06%] [G loss: 0.939170]\n",
      "epoch:13 step:12492 [D loss: 0.626017, acc.: 60.94%] [G loss: 0.958877]\n",
      "epoch:13 step:12493 [D loss: 0.651762, acc.: 60.94%] [G loss: 0.864570]\n",
      "epoch:13 step:12494 [D loss: 0.639336, acc.: 64.06%] [G loss: 0.901294]\n",
      "epoch:13 step:12495 [D loss: 0.664947, acc.: 58.59%] [G loss: 0.802585]\n",
      "epoch:13 step:12496 [D loss: 0.607130, acc.: 67.97%] [G loss: 0.882499]\n",
      "epoch:13 step:12497 [D loss: 0.727337, acc.: 38.28%] [G loss: 0.843827]\n",
      "epoch:13 step:12498 [D loss: 0.661399, acc.: 55.47%] [G loss: 0.835089]\n",
      "epoch:13 step:12499 [D loss: 0.638501, acc.: 66.41%] [G loss: 0.849959]\n",
      "epoch:13 step:12500 [D loss: 0.651760, acc.: 55.47%] [G loss: 0.867116]\n",
      "epoch:13 step:12501 [D loss: 0.730054, acc.: 50.00%] [G loss: 0.815326]\n",
      "epoch:13 step:12502 [D loss: 0.658526, acc.: 52.34%] [G loss: 0.895543]\n",
      "epoch:13 step:12503 [D loss: 0.709051, acc.: 53.91%] [G loss: 0.953768]\n",
      "epoch:13 step:12504 [D loss: 0.610073, acc.: 71.09%] [G loss: 0.886425]\n",
      "epoch:13 step:12505 [D loss: 0.611094, acc.: 62.50%] [G loss: 0.870578]\n",
      "epoch:13 step:12506 [D loss: 0.735826, acc.: 46.09%] [G loss: 0.808002]\n",
      "epoch:13 step:12507 [D loss: 0.700086, acc.: 57.03%] [G loss: 0.799139]\n",
      "epoch:13 step:12508 [D loss: 0.702847, acc.: 58.59%] [G loss: 0.837420]\n",
      "epoch:13 step:12509 [D loss: 0.645031, acc.: 64.84%] [G loss: 0.843119]\n",
      "epoch:13 step:12510 [D loss: 0.684348, acc.: 56.25%] [G loss: 0.869342]\n",
      "epoch:13 step:12511 [D loss: 0.727985, acc.: 44.53%] [G loss: 0.887860]\n",
      "epoch:13 step:12512 [D loss: 0.661291, acc.: 58.59%] [G loss: 0.892161]\n",
      "epoch:13 step:12513 [D loss: 0.669288, acc.: 60.94%] [G loss: 0.748415]\n",
      "epoch:13 step:12514 [D loss: 0.635635, acc.: 66.41%] [G loss: 0.834830]\n",
      "epoch:13 step:12515 [D loss: 0.681179, acc.: 54.69%] [G loss: 0.915125]\n",
      "epoch:13 step:12516 [D loss: 0.644166, acc.: 56.25%] [G loss: 0.894839]\n",
      "epoch:13 step:12517 [D loss: 0.623875, acc.: 64.06%] [G loss: 0.958713]\n",
      "epoch:13 step:12518 [D loss: 0.623685, acc.: 64.84%] [G loss: 0.885777]\n",
      "epoch:13 step:12519 [D loss: 0.672009, acc.: 56.25%] [G loss: 0.799552]\n",
      "epoch:13 step:12520 [D loss: 0.688421, acc.: 53.91%] [G loss: 0.891349]\n",
      "epoch:13 step:12521 [D loss: 0.675563, acc.: 55.47%] [G loss: 0.915985]\n",
      "epoch:13 step:12522 [D loss: 0.652047, acc.: 62.50%] [G loss: 0.829924]\n",
      "epoch:13 step:12523 [D loss: 0.702944, acc.: 57.03%] [G loss: 0.887978]\n",
      "epoch:13 step:12524 [D loss: 0.608724, acc.: 69.53%] [G loss: 0.847948]\n",
      "epoch:13 step:12525 [D loss: 0.649919, acc.: 62.50%] [G loss: 0.869393]\n",
      "epoch:13 step:12526 [D loss: 0.653028, acc.: 61.72%] [G loss: 0.869060]\n",
      "epoch:13 step:12527 [D loss: 0.651081, acc.: 60.16%] [G loss: 0.780148]\n",
      "epoch:13 step:12528 [D loss: 0.621411, acc.: 60.16%] [G loss: 0.912584]\n",
      "epoch:13 step:12529 [D loss: 0.636381, acc.: 63.28%] [G loss: 0.796544]\n",
      "epoch:13 step:12530 [D loss: 0.651087, acc.: 60.16%] [G loss: 0.916136]\n",
      "epoch:13 step:12531 [D loss: 0.620898, acc.: 71.88%] [G loss: 0.881271]\n",
      "epoch:13 step:12532 [D loss: 0.709069, acc.: 50.00%] [G loss: 0.855452]\n",
      "epoch:13 step:12533 [D loss: 0.712142, acc.: 50.78%] [G loss: 0.772136]\n",
      "epoch:13 step:12534 [D loss: 0.691036, acc.: 59.38%] [G loss: 0.902011]\n",
      "epoch:13 step:12535 [D loss: 0.685307, acc.: 58.59%] [G loss: 0.827472]\n",
      "epoch:13 step:12536 [D loss: 0.614022, acc.: 68.75%] [G loss: 0.858971]\n",
      "epoch:13 step:12537 [D loss: 0.705829, acc.: 51.56%] [G loss: 0.926819]\n",
      "epoch:13 step:12538 [D loss: 0.674557, acc.: 57.81%] [G loss: 0.861333]\n",
      "epoch:13 step:12539 [D loss: 0.676071, acc.: 55.47%] [G loss: 0.947604]\n",
      "epoch:13 step:12540 [D loss: 0.664283, acc.: 63.28%] [G loss: 0.926719]\n",
      "epoch:13 step:12541 [D loss: 0.650479, acc.: 64.06%] [G loss: 0.890674]\n",
      "epoch:13 step:12542 [D loss: 0.648450, acc.: 60.94%] [G loss: 0.929105]\n",
      "epoch:13 step:12543 [D loss: 0.735438, acc.: 51.56%] [G loss: 0.892093]\n",
      "epoch:13 step:12544 [D loss: 0.680112, acc.: 54.69%] [G loss: 0.835191]\n",
      "epoch:13 step:12545 [D loss: 0.699447, acc.: 53.91%] [G loss: 0.827564]\n",
      "epoch:13 step:12546 [D loss: 0.676714, acc.: 58.59%] [G loss: 0.912287]\n",
      "epoch:13 step:12547 [D loss: 0.673388, acc.: 57.81%] [G loss: 0.803777]\n",
      "epoch:13 step:12548 [D loss: 0.670205, acc.: 57.81%] [G loss: 0.899527]\n",
      "epoch:13 step:12549 [D loss: 0.619035, acc.: 65.62%] [G loss: 0.932031]\n",
      "epoch:13 step:12550 [D loss: 0.645551, acc.: 67.97%] [G loss: 0.831196]\n",
      "epoch:13 step:12551 [D loss: 0.719641, acc.: 43.75%] [G loss: 0.776556]\n",
      "epoch:13 step:12552 [D loss: 0.661382, acc.: 62.50%] [G loss: 0.804810]\n",
      "epoch:13 step:12553 [D loss: 0.688467, acc.: 49.22%] [G loss: 0.855795]\n",
      "epoch:13 step:12554 [D loss: 0.701534, acc.: 49.22%] [G loss: 0.768275]\n",
      "epoch:13 step:12555 [D loss: 0.719242, acc.: 50.78%] [G loss: 0.920533]\n",
      "epoch:13 step:12556 [D loss: 0.669190, acc.: 57.03%] [G loss: 0.905185]\n",
      "epoch:13 step:12557 [D loss: 0.638395, acc.: 62.50%] [G loss: 0.866885]\n",
      "epoch:13 step:12558 [D loss: 0.540071, acc.: 71.88%] [G loss: 0.840800]\n",
      "epoch:13 step:12559 [D loss: 0.671650, acc.: 60.94%] [G loss: 0.791864]\n",
      "epoch:13 step:12560 [D loss: 0.700222, acc.: 47.66%] [G loss: 0.742859]\n",
      "epoch:13 step:12561 [D loss: 0.693571, acc.: 57.81%] [G loss: 0.794321]\n",
      "epoch:13 step:12562 [D loss: 0.700524, acc.: 44.53%] [G loss: 0.826786]\n",
      "epoch:13 step:12563 [D loss: 0.643139, acc.: 64.06%] [G loss: 0.718729]\n",
      "epoch:13 step:12564 [D loss: 0.677927, acc.: 62.50%] [G loss: 0.847161]\n",
      "epoch:13 step:12565 [D loss: 0.671507, acc.: 55.47%] [G loss: 0.787189]\n",
      "epoch:13 step:12566 [D loss: 0.656152, acc.: 64.06%] [G loss: 0.731683]\n",
      "epoch:13 step:12567 [D loss: 0.675196, acc.: 59.38%] [G loss: 0.837769]\n",
      "epoch:13 step:12568 [D loss: 0.648663, acc.: 63.28%] [G loss: 0.867635]\n",
      "epoch:13 step:12569 [D loss: 0.673520, acc.: 59.38%] [G loss: 0.825401]\n",
      "epoch:13 step:12570 [D loss: 0.740258, acc.: 39.06%] [G loss: 0.787425]\n",
      "epoch:13 step:12571 [D loss: 0.649079, acc.: 60.94%] [G loss: 0.871436]\n",
      "epoch:13 step:12572 [D loss: 0.670985, acc.: 56.25%] [G loss: 0.793856]\n",
      "epoch:13 step:12573 [D loss: 0.656633, acc.: 55.47%] [G loss: 0.876334]\n",
      "epoch:13 step:12574 [D loss: 0.681678, acc.: 60.16%] [G loss: 0.811035]\n",
      "epoch:13 step:12575 [D loss: 0.671393, acc.: 53.12%] [G loss: 0.862383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12576 [D loss: 0.803539, acc.: 40.62%] [G loss: 0.763704]\n",
      "epoch:13 step:12577 [D loss: 0.630227, acc.: 64.06%] [G loss: 0.855912]\n",
      "epoch:13 step:12578 [D loss: 0.702886, acc.: 50.00%] [G loss: 0.742406]\n",
      "epoch:13 step:12579 [D loss: 0.651381, acc.: 60.94%] [G loss: 0.902844]\n",
      "epoch:13 step:12580 [D loss: 0.628240, acc.: 62.50%] [G loss: 0.849111]\n",
      "epoch:13 step:12581 [D loss: 0.640778, acc.: 63.28%] [G loss: 0.869131]\n",
      "epoch:13 step:12582 [D loss: 0.609805, acc.: 67.19%] [G loss: 0.817619]\n",
      "epoch:13 step:12583 [D loss: 0.631781, acc.: 66.41%] [G loss: 0.841114]\n",
      "epoch:13 step:12584 [D loss: 0.681593, acc.: 61.72%] [G loss: 0.819708]\n",
      "epoch:13 step:12585 [D loss: 0.720357, acc.: 47.66%] [G loss: 0.888114]\n",
      "epoch:13 step:12586 [D loss: 0.675894, acc.: 55.47%] [G loss: 0.921901]\n",
      "epoch:13 step:12587 [D loss: 0.625954, acc.: 61.72%] [G loss: 0.900882]\n",
      "epoch:13 step:12588 [D loss: 0.616557, acc.: 71.09%] [G loss: 0.950140]\n",
      "epoch:13 step:12589 [D loss: 0.663569, acc.: 58.59%] [G loss: 0.835966]\n",
      "epoch:13 step:12590 [D loss: 0.737451, acc.: 47.66%] [G loss: 0.763015]\n",
      "epoch:13 step:12591 [D loss: 0.643427, acc.: 67.19%] [G loss: 0.823968]\n",
      "epoch:13 step:12592 [D loss: 0.664490, acc.: 55.47%] [G loss: 0.849537]\n",
      "epoch:13 step:12593 [D loss: 0.712476, acc.: 50.00%] [G loss: 0.842535]\n",
      "epoch:13 step:12594 [D loss: 0.730989, acc.: 48.44%] [G loss: 0.895940]\n",
      "epoch:13 step:12595 [D loss: 0.685830, acc.: 53.91%] [G loss: 0.905790]\n",
      "epoch:13 step:12596 [D loss: 0.656426, acc.: 55.47%] [G loss: 0.888663]\n",
      "epoch:13 step:12597 [D loss: 0.690857, acc.: 55.47%] [G loss: 0.812698]\n",
      "epoch:13 step:12598 [D loss: 0.633582, acc.: 57.03%] [G loss: 0.771615]\n",
      "epoch:13 step:12599 [D loss: 0.642526, acc.: 60.94%] [G loss: 0.890268]\n",
      "epoch:13 step:12600 [D loss: 0.680104, acc.: 54.69%] [G loss: 0.878700]\n",
      "epoch:13 step:12601 [D loss: 0.637609, acc.: 64.06%] [G loss: 0.834872]\n",
      "epoch:13 step:12602 [D loss: 0.652022, acc.: 61.72%] [G loss: 0.863531]\n",
      "epoch:13 step:12603 [D loss: 0.659840, acc.: 55.47%] [G loss: 0.830259]\n",
      "epoch:13 step:12604 [D loss: 0.629969, acc.: 68.75%] [G loss: 0.891008]\n",
      "epoch:13 step:12605 [D loss: 0.706793, acc.: 50.78%] [G loss: 0.699182]\n",
      "epoch:13 step:12606 [D loss: 0.668846, acc.: 57.03%] [G loss: 0.852152]\n",
      "epoch:13 step:12607 [D loss: 0.628819, acc.: 60.94%] [G loss: 0.688533]\n",
      "epoch:13 step:12608 [D loss: 0.606730, acc.: 71.09%] [G loss: 0.946399]\n",
      "epoch:13 step:12609 [D loss: 0.681657, acc.: 54.69%] [G loss: 0.727557]\n",
      "epoch:13 step:12610 [D loss: 0.750468, acc.: 44.53%] [G loss: 0.871536]\n",
      "epoch:13 step:12611 [D loss: 0.679504, acc.: 57.03%] [G loss: 0.809627]\n",
      "epoch:13 step:12612 [D loss: 0.686938, acc.: 47.66%] [G loss: 0.886976]\n",
      "epoch:13 step:12613 [D loss: 0.712822, acc.: 57.03%] [G loss: 0.861634]\n",
      "epoch:13 step:12614 [D loss: 0.662408, acc.: 53.12%] [G loss: 0.688967]\n",
      "epoch:13 step:12615 [D loss: 0.693858, acc.: 53.91%] [G loss: 0.843288]\n",
      "epoch:13 step:12616 [D loss: 0.712333, acc.: 45.31%] [G loss: 0.801800]\n",
      "epoch:13 step:12617 [D loss: 0.682224, acc.: 54.69%] [G loss: 0.838174]\n",
      "epoch:13 step:12618 [D loss: 0.637243, acc.: 61.72%] [G loss: 0.794218]\n",
      "epoch:13 step:12619 [D loss: 0.635562, acc.: 67.19%] [G loss: 0.810987]\n",
      "epoch:13 step:12620 [D loss: 0.725509, acc.: 49.22%] [G loss: 0.821705]\n",
      "epoch:13 step:12621 [D loss: 0.671246, acc.: 56.25%] [G loss: 0.814730]\n",
      "epoch:13 step:12622 [D loss: 0.636327, acc.: 64.84%] [G loss: 0.883456]\n",
      "epoch:13 step:12623 [D loss: 0.663480, acc.: 58.59%] [G loss: 0.923155]\n",
      "epoch:13 step:12624 [D loss: 0.653389, acc.: 60.94%] [G loss: 0.858705]\n",
      "epoch:13 step:12625 [D loss: 0.618534, acc.: 69.53%] [G loss: 0.855527]\n",
      "epoch:13 step:12626 [D loss: 0.685049, acc.: 57.81%] [G loss: 0.744250]\n",
      "epoch:13 step:12627 [D loss: 0.672165, acc.: 53.91%] [G loss: 0.876599]\n",
      "epoch:13 step:12628 [D loss: 0.674855, acc.: 54.69%] [G loss: 0.840824]\n",
      "epoch:13 step:12629 [D loss: 0.703810, acc.: 53.91%] [G loss: 0.805009]\n",
      "epoch:13 step:12630 [D loss: 0.664170, acc.: 58.59%] [G loss: 0.757185]\n",
      "epoch:13 step:12631 [D loss: 0.714582, acc.: 49.22%] [G loss: 0.840219]\n",
      "epoch:13 step:12632 [D loss: 0.668076, acc.: 57.03%] [G loss: 0.876327]\n",
      "epoch:13 step:12633 [D loss: 0.676238, acc.: 54.69%] [G loss: 0.886117]\n",
      "epoch:13 step:12634 [D loss: 0.658887, acc.: 55.47%] [G loss: 0.779062]\n",
      "epoch:13 step:12635 [D loss: 0.670712, acc.: 64.84%] [G loss: 0.823374]\n",
      "epoch:13 step:12636 [D loss: 0.693502, acc.: 55.47%] [G loss: 0.860881]\n",
      "epoch:13 step:12637 [D loss: 0.650415, acc.: 54.69%] [G loss: 0.844038]\n",
      "epoch:13 step:12638 [D loss: 0.654228, acc.: 57.03%] [G loss: 0.725918]\n",
      "epoch:13 step:12639 [D loss: 0.731459, acc.: 46.09%] [G loss: 0.807185]\n",
      "epoch:13 step:12640 [D loss: 0.720183, acc.: 49.22%] [G loss: 0.852709]\n",
      "epoch:13 step:12641 [D loss: 0.658634, acc.: 64.84%] [G loss: 0.880046]\n",
      "epoch:13 step:12642 [D loss: 0.704921, acc.: 47.66%] [G loss: 0.886249]\n",
      "epoch:13 step:12643 [D loss: 0.676723, acc.: 58.59%] [G loss: 0.934638]\n",
      "epoch:13 step:12644 [D loss: 0.639200, acc.: 61.72%] [G loss: 0.888270]\n",
      "epoch:13 step:12645 [D loss: 0.671375, acc.: 53.12%] [G loss: 0.946836]\n",
      "epoch:13 step:12646 [D loss: 0.664779, acc.: 59.38%] [G loss: 0.833977]\n",
      "epoch:13 step:12647 [D loss: 0.638886, acc.: 60.94%] [G loss: 0.868112]\n",
      "epoch:13 step:12648 [D loss: 0.646373, acc.: 59.38%] [G loss: 0.817016]\n",
      "epoch:13 step:12649 [D loss: 0.643977, acc.: 55.47%] [G loss: 0.925901]\n",
      "epoch:13 step:12650 [D loss: 0.712638, acc.: 52.34%] [G loss: 0.914156]\n",
      "epoch:13 step:12651 [D loss: 0.691500, acc.: 56.25%] [G loss: 0.881160]\n",
      "epoch:13 step:12652 [D loss: 0.654340, acc.: 59.38%] [G loss: 0.935101]\n",
      "epoch:13 step:12653 [D loss: 0.714326, acc.: 43.75%] [G loss: 0.779164]\n",
      "epoch:13 step:12654 [D loss: 0.618175, acc.: 71.09%] [G loss: 0.881085]\n",
      "epoch:13 step:12655 [D loss: 0.674730, acc.: 53.91%] [G loss: 0.797690]\n",
      "epoch:13 step:12656 [D loss: 0.628036, acc.: 64.06%] [G loss: 0.883568]\n",
      "epoch:13 step:12657 [D loss: 0.650175, acc.: 60.16%] [G loss: 0.878517]\n",
      "epoch:13 step:12658 [D loss: 0.672396, acc.: 50.78%] [G loss: 0.930155]\n",
      "epoch:13 step:12659 [D loss: 0.623854, acc.: 66.41%] [G loss: 0.921170]\n",
      "epoch:13 step:12660 [D loss: 0.660455, acc.: 54.69%] [G loss: 0.830733]\n",
      "epoch:13 step:12661 [D loss: 0.684537, acc.: 56.25%] [G loss: 0.841924]\n",
      "epoch:13 step:12662 [D loss: 0.679899, acc.: 54.69%] [G loss: 0.838300]\n",
      "epoch:13 step:12663 [D loss: 0.646272, acc.: 59.38%] [G loss: 0.737966]\n",
      "epoch:13 step:12664 [D loss: 0.627581, acc.: 67.97%] [G loss: 0.829211]\n",
      "epoch:13 step:12665 [D loss: 0.640887, acc.: 64.84%] [G loss: 0.861370]\n",
      "epoch:13 step:12666 [D loss: 0.688983, acc.: 51.56%] [G loss: 0.826284]\n",
      "epoch:13 step:12667 [D loss: 0.699332, acc.: 42.97%] [G loss: 0.826253]\n",
      "epoch:13 step:12668 [D loss: 0.665220, acc.: 53.91%] [G loss: 0.837400]\n",
      "epoch:13 step:12669 [D loss: 0.675606, acc.: 59.38%] [G loss: 0.907396]\n",
      "epoch:13 step:12670 [D loss: 0.688328, acc.: 53.91%] [G loss: 0.907569]\n",
      "epoch:13 step:12671 [D loss: 0.634206, acc.: 63.28%] [G loss: 0.876627]\n",
      "epoch:13 step:12672 [D loss: 0.668601, acc.: 53.12%] [G loss: 0.977131]\n",
      "epoch:13 step:12673 [D loss: 0.674321, acc.: 55.47%] [G loss: 0.810659]\n",
      "epoch:13 step:12674 [D loss: 0.672430, acc.: 54.69%] [G loss: 0.938172]\n",
      "epoch:13 step:12675 [D loss: 0.692605, acc.: 60.16%] [G loss: 0.851069]\n",
      "epoch:13 step:12676 [D loss: 0.712773, acc.: 52.34%] [G loss: 0.788546]\n",
      "epoch:13 step:12677 [D loss: 0.629659, acc.: 71.88%] [G loss: 0.851555]\n",
      "epoch:13 step:12678 [D loss: 0.715283, acc.: 46.88%] [G loss: 0.792986]\n",
      "epoch:13 step:12679 [D loss: 0.695916, acc.: 60.94%] [G loss: 0.817341]\n",
      "epoch:13 step:12680 [D loss: 0.715878, acc.: 49.22%] [G loss: 0.748424]\n",
      "epoch:13 step:12681 [D loss: 0.738975, acc.: 40.62%] [G loss: 0.871001]\n",
      "epoch:13 step:12682 [D loss: 0.676369, acc.: 56.25%] [G loss: 1.025621]\n",
      "epoch:13 step:12683 [D loss: 0.679034, acc.: 59.38%] [G loss: 0.821295]\n",
      "epoch:13 step:12684 [D loss: 0.702405, acc.: 43.75%] [G loss: 0.815070]\n",
      "epoch:13 step:12685 [D loss: 0.692213, acc.: 46.88%] [G loss: 0.807950]\n",
      "epoch:13 step:12686 [D loss: 0.654453, acc.: 60.94%] [G loss: 0.803268]\n",
      "epoch:13 step:12687 [D loss: 0.697589, acc.: 53.12%] [G loss: 0.833368]\n",
      "epoch:13 step:12688 [D loss: 0.698649, acc.: 49.22%] [G loss: 0.815072]\n",
      "epoch:13 step:12689 [D loss: 0.704244, acc.: 46.88%] [G loss: 0.783504]\n",
      "epoch:13 step:12690 [D loss: 0.661488, acc.: 65.62%] [G loss: 0.819391]\n",
      "epoch:13 step:12691 [D loss: 0.656189, acc.: 60.16%] [G loss: 0.851507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12692 [D loss: 0.683325, acc.: 57.81%] [G loss: 0.811994]\n",
      "epoch:13 step:12693 [D loss: 0.683581, acc.: 57.03%] [G loss: 0.783513]\n",
      "epoch:13 step:12694 [D loss: 0.663491, acc.: 57.81%] [G loss: 0.752959]\n",
      "epoch:13 step:12695 [D loss: 0.631468, acc.: 64.84%] [G loss: 0.752169]\n",
      "epoch:13 step:12696 [D loss: 0.650555, acc.: 65.62%] [G loss: 0.887787]\n",
      "epoch:13 step:12697 [D loss: 0.650789, acc.: 61.72%] [G loss: 0.791618]\n",
      "epoch:13 step:12698 [D loss: 0.652485, acc.: 57.03%] [G loss: 0.791059]\n",
      "epoch:13 step:12699 [D loss: 0.695762, acc.: 53.91%] [G loss: 0.806376]\n",
      "epoch:13 step:12700 [D loss: 0.712023, acc.: 53.91%] [G loss: 0.799332]\n",
      "epoch:13 step:12701 [D loss: 0.663781, acc.: 56.25%] [G loss: 0.810890]\n",
      "epoch:13 step:12702 [D loss: 0.676149, acc.: 54.69%] [G loss: 0.766168]\n",
      "epoch:13 step:12703 [D loss: 0.721616, acc.: 55.47%] [G loss: 0.765139]\n",
      "epoch:13 step:12704 [D loss: 0.653943, acc.: 54.69%] [G loss: 1.006482]\n",
      "epoch:13 step:12705 [D loss: 0.665944, acc.: 53.91%] [G loss: 0.815181]\n",
      "epoch:13 step:12706 [D loss: 0.684778, acc.: 54.69%] [G loss: 0.822450]\n",
      "epoch:13 step:12707 [D loss: 0.654208, acc.: 58.59%] [G loss: 0.780983]\n",
      "epoch:13 step:12708 [D loss: 0.668424, acc.: 58.59%] [G loss: 0.770230]\n",
      "epoch:13 step:12709 [D loss: 0.639203, acc.: 60.94%] [G loss: 0.769054]\n",
      "epoch:13 step:12710 [D loss: 0.661996, acc.: 56.25%] [G loss: 0.898406]\n",
      "epoch:13 step:12711 [D loss: 0.693964, acc.: 52.34%] [G loss: 0.903796]\n",
      "epoch:13 step:12712 [D loss: 0.701636, acc.: 47.66%] [G loss: 0.872328]\n",
      "epoch:13 step:12713 [D loss: 0.645146, acc.: 59.38%] [G loss: 0.876630]\n",
      "epoch:13 step:12714 [D loss: 0.716471, acc.: 42.19%] [G loss: 0.818316]\n",
      "epoch:13 step:12715 [D loss: 0.711019, acc.: 52.34%] [G loss: 0.836726]\n",
      "epoch:13 step:12716 [D loss: 0.615793, acc.: 68.75%] [G loss: 0.933255]\n",
      "epoch:13 step:12717 [D loss: 0.656617, acc.: 56.25%] [G loss: 0.902153]\n",
      "epoch:13 step:12718 [D loss: 0.692094, acc.: 58.59%] [G loss: 0.850593]\n",
      "epoch:13 step:12719 [D loss: 0.657216, acc.: 59.38%] [G loss: 0.869799]\n",
      "epoch:13 step:12720 [D loss: 0.745461, acc.: 48.44%] [G loss: 0.822043]\n",
      "epoch:13 step:12721 [D loss: 0.632461, acc.: 70.31%] [G loss: 0.822847]\n",
      "epoch:13 step:12722 [D loss: 0.710322, acc.: 55.47%] [G loss: 0.863777]\n",
      "epoch:13 step:12723 [D loss: 0.708369, acc.: 46.88%] [G loss: 0.803173]\n",
      "epoch:13 step:12724 [D loss: 0.652250, acc.: 62.50%] [G loss: 0.766006]\n",
      "epoch:13 step:12725 [D loss: 0.609391, acc.: 67.97%] [G loss: 0.843378]\n",
      "epoch:13 step:12726 [D loss: 0.662820, acc.: 60.16%] [G loss: 0.784210]\n",
      "epoch:13 step:12727 [D loss: 0.691842, acc.: 52.34%] [G loss: 0.804967]\n",
      "epoch:13 step:12728 [D loss: 0.660967, acc.: 58.59%] [G loss: 0.926691]\n",
      "epoch:13 step:12729 [D loss: 0.693604, acc.: 55.47%] [G loss: 0.816823]\n",
      "epoch:13 step:12730 [D loss: 0.649731, acc.: 67.19%] [G loss: 0.834357]\n",
      "epoch:13 step:12731 [D loss: 0.666434, acc.: 63.28%] [G loss: 0.884355]\n",
      "epoch:13 step:12732 [D loss: 0.676126, acc.: 57.03%] [G loss: 0.889423]\n",
      "epoch:13 step:12733 [D loss: 0.679869, acc.: 53.91%] [G loss: 0.791886]\n",
      "epoch:13 step:12734 [D loss: 0.681281, acc.: 55.47%] [G loss: 0.836378]\n",
      "epoch:13 step:12735 [D loss: 0.697271, acc.: 48.44%] [G loss: 0.731416]\n",
      "epoch:13 step:12736 [D loss: 0.630836, acc.: 61.72%] [G loss: 0.837524]\n",
      "epoch:13 step:12737 [D loss: 0.693941, acc.: 45.31%] [G loss: 0.771890]\n",
      "epoch:13 step:12738 [D loss: 0.683131, acc.: 55.47%] [G loss: 0.813300]\n",
      "epoch:13 step:12739 [D loss: 0.690621, acc.: 53.12%] [G loss: 0.772640]\n",
      "epoch:13 step:12740 [D loss: 0.652895, acc.: 61.72%] [G loss: 0.873308]\n",
      "epoch:13 step:12741 [D loss: 0.655096, acc.: 64.06%] [G loss: 0.780214]\n",
      "epoch:13 step:12742 [D loss: 0.663032, acc.: 60.16%] [G loss: 0.780926]\n",
      "epoch:13 step:12743 [D loss: 0.676055, acc.: 57.03%] [G loss: 0.782073]\n",
      "epoch:13 step:12744 [D loss: 0.672333, acc.: 54.69%] [G loss: 0.754417]\n",
      "epoch:13 step:12745 [D loss: 0.684549, acc.: 51.56%] [G loss: 0.794188]\n",
      "epoch:13 step:12746 [D loss: 0.679135, acc.: 55.47%] [G loss: 0.779954]\n",
      "epoch:13 step:12747 [D loss: 0.693622, acc.: 63.28%] [G loss: 0.829787]\n",
      "epoch:13 step:12748 [D loss: 0.656714, acc.: 60.16%] [G loss: 0.770175]\n",
      "epoch:13 step:12749 [D loss: 0.648033, acc.: 57.03%] [G loss: 0.810169]\n",
      "epoch:13 step:12750 [D loss: 0.656551, acc.: 59.38%] [G loss: 0.855247]\n",
      "epoch:13 step:12751 [D loss: 0.710258, acc.: 56.25%] [G loss: 0.804773]\n",
      "epoch:13 step:12752 [D loss: 0.642279, acc.: 67.19%] [G loss: 0.791939]\n",
      "epoch:13 step:12753 [D loss: 0.645884, acc.: 61.72%] [G loss: 0.799081]\n",
      "epoch:13 step:12754 [D loss: 0.652379, acc.: 58.59%] [G loss: 0.875608]\n",
      "epoch:13 step:12755 [D loss: 0.726699, acc.: 44.53%] [G loss: 0.891215]\n",
      "epoch:13 step:12756 [D loss: 0.651334, acc.: 64.84%] [G loss: 0.887833]\n",
      "epoch:13 step:12757 [D loss: 0.621977, acc.: 68.75%] [G loss: 0.845410]\n",
      "epoch:13 step:12758 [D loss: 0.705105, acc.: 49.22%] [G loss: 0.869090]\n",
      "epoch:13 step:12759 [D loss: 0.657829, acc.: 57.81%] [G loss: 0.838585]\n",
      "epoch:13 step:12760 [D loss: 0.701651, acc.: 57.81%] [G loss: 0.767370]\n",
      "epoch:13 step:12761 [D loss: 0.674450, acc.: 49.22%] [G loss: 1.114632]\n",
      "epoch:13 step:12762 [D loss: 0.655109, acc.: 61.72%] [G loss: 0.881917]\n",
      "epoch:13 step:12763 [D loss: 0.644036, acc.: 65.62%] [G loss: 0.791556]\n",
      "epoch:13 step:12764 [D loss: 0.697145, acc.: 52.34%] [G loss: 0.710299]\n",
      "epoch:13 step:12765 [D loss: 0.666896, acc.: 57.81%] [G loss: 0.813015]\n",
      "epoch:13 step:12766 [D loss: 0.632683, acc.: 71.88%] [G loss: 0.802986]\n",
      "epoch:13 step:12767 [D loss: 0.672690, acc.: 51.56%] [G loss: 0.756935]\n",
      "epoch:13 step:12768 [D loss: 0.673161, acc.: 57.81%] [G loss: 0.705088]\n",
      "epoch:13 step:12769 [D loss: 0.667043, acc.: 57.81%] [G loss: 0.840206]\n",
      "epoch:13 step:12770 [D loss: 0.679735, acc.: 54.69%] [G loss: 0.742405]\n",
      "epoch:13 step:12771 [D loss: 0.714145, acc.: 49.22%] [G loss: 1.021342]\n",
      "epoch:13 step:12772 [D loss: 0.660660, acc.: 53.12%] [G loss: 0.848328]\n",
      "epoch:13 step:12773 [D loss: 0.651732, acc.: 60.94%] [G loss: 0.882147]\n",
      "epoch:13 step:12774 [D loss: 0.720202, acc.: 45.31%] [G loss: 0.802757]\n",
      "epoch:13 step:12775 [D loss: 0.682256, acc.: 60.16%] [G loss: 0.835179]\n",
      "epoch:13 step:12776 [D loss: 0.665709, acc.: 60.16%] [G loss: 0.761262]\n",
      "epoch:13 step:12777 [D loss: 0.639631, acc.: 63.28%] [G loss: 0.832307]\n",
      "epoch:13 step:12778 [D loss: 0.636919, acc.: 60.94%] [G loss: 0.896119]\n",
      "epoch:13 step:12779 [D loss: 0.672100, acc.: 64.06%] [G loss: 0.804479]\n",
      "epoch:13 step:12780 [D loss: 0.682598, acc.: 53.91%] [G loss: 0.810689]\n",
      "epoch:13 step:12781 [D loss: 0.660255, acc.: 61.72%] [G loss: 0.846544]\n",
      "epoch:13 step:12782 [D loss: 0.706278, acc.: 59.38%] [G loss: 0.837013]\n",
      "epoch:13 step:12783 [D loss: 0.667503, acc.: 58.59%] [G loss: 0.940910]\n",
      "epoch:13 step:12784 [D loss: 0.692688, acc.: 54.69%] [G loss: 0.807067]\n",
      "epoch:13 step:12785 [D loss: 0.680211, acc.: 53.12%] [G loss: 0.838265]\n",
      "epoch:13 step:12786 [D loss: 0.664461, acc.: 57.03%] [G loss: 0.765717]\n",
      "epoch:13 step:12787 [D loss: 0.720592, acc.: 46.88%] [G loss: 0.843951]\n",
      "epoch:13 step:12788 [D loss: 0.671874, acc.: 58.59%] [G loss: 0.836595]\n",
      "epoch:13 step:12789 [D loss: 0.653807, acc.: 59.38%] [G loss: 0.812475]\n",
      "epoch:13 step:12790 [D loss: 0.628883, acc.: 67.19%] [G loss: 0.903958]\n",
      "epoch:13 step:12791 [D loss: 0.686866, acc.: 50.78%] [G loss: 0.870570]\n",
      "epoch:13 step:12792 [D loss: 0.607969, acc.: 71.09%] [G loss: 0.796199]\n",
      "epoch:13 step:12793 [D loss: 0.657460, acc.: 58.59%] [G loss: 0.870966]\n",
      "epoch:13 step:12794 [D loss: 0.639514, acc.: 64.06%] [G loss: 0.898858]\n",
      "epoch:13 step:12795 [D loss: 0.679497, acc.: 58.59%] [G loss: 0.758864]\n",
      "epoch:13 step:12796 [D loss: 0.651297, acc.: 64.06%] [G loss: 0.903484]\n",
      "epoch:13 step:12797 [D loss: 0.644235, acc.: 61.72%] [G loss: 0.982413]\n",
      "epoch:13 step:12798 [D loss: 0.665799, acc.: 59.38%] [G loss: 0.886620]\n",
      "epoch:13 step:12799 [D loss: 0.710808, acc.: 52.34%] [G loss: 0.873089]\n",
      "epoch:13 step:12800 [D loss: 0.666760, acc.: 54.69%] [G loss: 0.845979]\n",
      "epoch:13 step:12801 [D loss: 0.694971, acc.: 55.47%] [G loss: 0.908589]\n",
      "epoch:13 step:12802 [D loss: 0.638995, acc.: 67.97%] [G loss: 0.859226]\n",
      "epoch:13 step:12803 [D loss: 0.653122, acc.: 60.16%] [G loss: 0.820809]\n",
      "epoch:13 step:12804 [D loss: 0.706032, acc.: 50.78%] [G loss: 0.740902]\n",
      "epoch:13 step:12805 [D loss: 0.656541, acc.: 59.38%] [G loss: 0.710141]\n",
      "epoch:13 step:12806 [D loss: 0.708891, acc.: 55.47%] [G loss: 0.783205]\n",
      "epoch:13 step:12807 [D loss: 0.654128, acc.: 51.56%] [G loss: 0.925649]\n",
      "epoch:13 step:12808 [D loss: 0.618947, acc.: 68.75%] [G loss: 0.873655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12809 [D loss: 0.615568, acc.: 73.44%] [G loss: 0.923102]\n",
      "epoch:13 step:12810 [D loss: 0.635413, acc.: 62.50%] [G loss: 0.756887]\n",
      "epoch:13 step:12811 [D loss: 0.648838, acc.: 62.50%] [G loss: 0.839782]\n",
      "epoch:13 step:12812 [D loss: 0.650814, acc.: 58.59%] [G loss: 0.830790]\n",
      "epoch:13 step:12813 [D loss: 0.613659, acc.: 67.97%] [G loss: 1.001762]\n",
      "epoch:13 step:12814 [D loss: 0.695533, acc.: 46.88%] [G loss: 0.832911]\n",
      "epoch:13 step:12815 [D loss: 0.667528, acc.: 67.19%] [G loss: 0.841814]\n",
      "epoch:13 step:12816 [D loss: 0.630354, acc.: 66.41%] [G loss: 0.879607]\n",
      "epoch:13 step:12817 [D loss: 0.677811, acc.: 60.16%] [G loss: 0.919292]\n",
      "epoch:13 step:12818 [D loss: 0.674325, acc.: 57.81%] [G loss: 0.813025]\n",
      "epoch:13 step:12819 [D loss: 0.702396, acc.: 49.22%] [G loss: 0.865236]\n",
      "epoch:13 step:12820 [D loss: 0.671087, acc.: 57.81%] [G loss: 0.816444]\n",
      "epoch:13 step:12821 [D loss: 0.633003, acc.: 68.75%] [G loss: 0.872953]\n",
      "epoch:13 step:12822 [D loss: 0.652734, acc.: 63.28%] [G loss: 0.880659]\n",
      "epoch:13 step:12823 [D loss: 0.714402, acc.: 53.91%] [G loss: 1.444430]\n",
      "epoch:13 step:12824 [D loss: 0.730444, acc.: 42.19%] [G loss: 0.988469]\n",
      "epoch:13 step:12825 [D loss: 0.602155, acc.: 69.53%] [G loss: 0.902424]\n",
      "epoch:13 step:12826 [D loss: 0.719133, acc.: 48.44%] [G loss: 0.903158]\n",
      "epoch:13 step:12827 [D loss: 0.855336, acc.: 55.47%] [G loss: 1.116701]\n",
      "epoch:13 step:12828 [D loss: 0.598970, acc.: 71.09%] [G loss: 0.845081]\n",
      "epoch:13 step:12829 [D loss: 0.604445, acc.: 66.41%] [G loss: 1.008309]\n",
      "epoch:13 step:12830 [D loss: 0.702498, acc.: 50.00%] [G loss: 1.009470]\n",
      "epoch:13 step:12831 [D loss: 0.637860, acc.: 60.94%] [G loss: 1.034040]\n",
      "epoch:13 step:12832 [D loss: 0.716369, acc.: 56.25%] [G loss: 0.858346]\n",
      "epoch:13 step:12833 [D loss: 0.699088, acc.: 53.12%] [G loss: 0.773175]\n",
      "epoch:13 step:12834 [D loss: 0.649673, acc.: 64.06%] [G loss: 0.844660]\n",
      "epoch:13 step:12835 [D loss: 0.642556, acc.: 57.03%] [G loss: 0.868003]\n",
      "epoch:13 step:12836 [D loss: 0.681913, acc.: 64.06%] [G loss: 0.962521]\n",
      "epoch:13 step:12837 [D loss: 0.607450, acc.: 72.66%] [G loss: 0.800366]\n",
      "epoch:13 step:12838 [D loss: 0.650304, acc.: 64.06%] [G loss: 0.976776]\n",
      "epoch:13 step:12839 [D loss: 0.695758, acc.: 59.38%] [G loss: 0.840515]\n",
      "epoch:13 step:12840 [D loss: 0.651400, acc.: 61.72%] [G loss: 0.864890]\n",
      "epoch:13 step:12841 [D loss: 0.640011, acc.: 60.94%] [G loss: 0.875154]\n",
      "epoch:13 step:12842 [D loss: 0.631955, acc.: 65.62%] [G loss: 0.836542]\n",
      "epoch:13 step:12843 [D loss: 0.691875, acc.: 53.12%] [G loss: 0.935052]\n",
      "epoch:13 step:12844 [D loss: 0.629599, acc.: 67.19%] [G loss: 0.932728]\n",
      "epoch:13 step:12845 [D loss: 0.679058, acc.: 60.16%] [G loss: 0.928862]\n",
      "epoch:13 step:12846 [D loss: 0.663423, acc.: 63.28%] [G loss: 0.919340]\n",
      "epoch:13 step:12847 [D loss: 0.693719, acc.: 50.00%] [G loss: 0.828431]\n",
      "epoch:13 step:12848 [D loss: 0.629332, acc.: 61.72%] [G loss: 0.905841]\n",
      "epoch:13 step:12849 [D loss: 0.642918, acc.: 64.84%] [G loss: 0.966888]\n",
      "epoch:13 step:12850 [D loss: 0.611086, acc.: 71.09%] [G loss: 0.712947]\n",
      "epoch:13 step:12851 [D loss: 0.566611, acc.: 76.56%] [G loss: 0.763767]\n",
      "epoch:13 step:12852 [D loss: 0.688118, acc.: 47.66%] [G loss: 0.844417]\n",
      "epoch:13 step:12853 [D loss: 0.664821, acc.: 52.34%] [G loss: 0.928792]\n",
      "epoch:13 step:12854 [D loss: 0.671560, acc.: 59.38%] [G loss: 0.778519]\n",
      "epoch:13 step:12855 [D loss: 0.657233, acc.: 63.28%] [G loss: 0.883360]\n",
      "epoch:13 step:12856 [D loss: 0.711094, acc.: 43.75%] [G loss: 0.837514]\n",
      "epoch:13 step:12857 [D loss: 0.654001, acc.: 64.06%] [G loss: 0.823180]\n",
      "epoch:13 step:12858 [D loss: 0.718416, acc.: 50.00%] [G loss: 0.813542]\n",
      "epoch:13 step:12859 [D loss: 0.741773, acc.: 46.09%] [G loss: 0.789975]\n",
      "epoch:13 step:12860 [D loss: 0.678125, acc.: 54.69%] [G loss: 0.776928]\n",
      "epoch:13 step:12861 [D loss: 0.714090, acc.: 53.12%] [G loss: 0.788951]\n",
      "epoch:13 step:12862 [D loss: 0.671883, acc.: 51.56%] [G loss: 0.919132]\n",
      "epoch:13 step:12863 [D loss: 0.691595, acc.: 56.25%] [G loss: 0.853442]\n",
      "epoch:13 step:12864 [D loss: 0.692535, acc.: 48.44%] [G loss: 0.870227]\n",
      "epoch:13 step:12865 [D loss: 0.773514, acc.: 35.94%] [G loss: 0.879053]\n",
      "epoch:13 step:12866 [D loss: 0.705022, acc.: 48.44%] [G loss: 0.880567]\n",
      "epoch:13 step:12867 [D loss: 0.695021, acc.: 51.56%] [G loss: 0.895349]\n",
      "epoch:13 step:12868 [D loss: 0.687670, acc.: 57.03%] [G loss: 0.838736]\n",
      "epoch:13 step:12869 [D loss: 0.639095, acc.: 60.16%] [G loss: 0.817772]\n",
      "epoch:13 step:12870 [D loss: 0.725185, acc.: 46.88%] [G loss: 0.746257]\n",
      "epoch:13 step:12871 [D loss: 0.688254, acc.: 57.03%] [G loss: 0.833321]\n",
      "epoch:13 step:12872 [D loss: 0.687667, acc.: 59.38%] [G loss: 0.819778]\n",
      "epoch:13 step:12873 [D loss: 0.704256, acc.: 52.34%] [G loss: 0.814006]\n",
      "epoch:13 step:12874 [D loss: 0.682108, acc.: 54.69%] [G loss: 0.766639]\n",
      "epoch:13 step:12875 [D loss: 0.680767, acc.: 57.03%] [G loss: 0.750602]\n",
      "epoch:13 step:12876 [D loss: 0.703736, acc.: 57.81%] [G loss: 0.792410]\n",
      "epoch:13 step:12877 [D loss: 0.660592, acc.: 60.94%] [G loss: 0.719712]\n",
      "epoch:13 step:12878 [D loss: 0.666971, acc.: 62.50%] [G loss: 0.741977]\n",
      "epoch:13 step:12879 [D loss: 0.615142, acc.: 75.00%] [G loss: 0.704951]\n",
      "epoch:13 step:12880 [D loss: 0.679084, acc.: 60.16%] [G loss: 0.723974]\n",
      "epoch:13 step:12881 [D loss: 0.672164, acc.: 60.16%] [G loss: 0.781835]\n",
      "epoch:13 step:12882 [D loss: 0.634936, acc.: 65.62%] [G loss: 0.787051]\n",
      "epoch:13 step:12883 [D loss: 0.649548, acc.: 59.38%] [G loss: 0.682253]\n",
      "epoch:13 step:12884 [D loss: 0.722103, acc.: 53.12%] [G loss: 0.863209]\n",
      "epoch:13 step:12885 [D loss: 0.640393, acc.: 70.31%] [G loss: 0.712062]\n",
      "epoch:13 step:12886 [D loss: 0.712363, acc.: 51.56%] [G loss: 0.794085]\n",
      "epoch:13 step:12887 [D loss: 0.660210, acc.: 60.16%] [G loss: 0.833214]\n",
      "epoch:13 step:12888 [D loss: 0.597544, acc.: 76.56%] [G loss: 0.880684]\n",
      "epoch:13 step:12889 [D loss: 0.630508, acc.: 69.53%] [G loss: 0.737875]\n",
      "epoch:13 step:12890 [D loss: 0.788235, acc.: 35.16%] [G loss: 0.841770]\n",
      "epoch:13 step:12891 [D loss: 0.637452, acc.: 64.06%] [G loss: 0.803522]\n",
      "epoch:13 step:12892 [D loss: 0.645423, acc.: 58.59%] [G loss: 0.977770]\n",
      "epoch:13 step:12893 [D loss: 0.634047, acc.: 64.06%] [G loss: 0.911334]\n",
      "epoch:13 step:12894 [D loss: 0.555392, acc.: 69.53%] [G loss: 0.924214]\n",
      "epoch:13 step:12895 [D loss: 0.576491, acc.: 73.44%] [G loss: 0.947835]\n",
      "epoch:13 step:12896 [D loss: 0.648937, acc.: 60.94%] [G loss: 0.884794]\n",
      "epoch:13 step:12897 [D loss: 0.720240, acc.: 55.47%] [G loss: 0.780773]\n",
      "epoch:13 step:12898 [D loss: 0.667630, acc.: 61.72%] [G loss: 0.922162]\n",
      "epoch:13 step:12899 [D loss: 0.661710, acc.: 54.69%] [G loss: 0.862909]\n",
      "epoch:13 step:12900 [D loss: 0.621640, acc.: 61.72%] [G loss: 0.875658]\n",
      "epoch:13 step:12901 [D loss: 0.719504, acc.: 45.31%] [G loss: 0.949109]\n",
      "epoch:13 step:12902 [D loss: 0.693798, acc.: 46.88%] [G loss: 0.825137]\n",
      "epoch:13 step:12903 [D loss: 0.690902, acc.: 50.78%] [G loss: 0.818473]\n",
      "epoch:13 step:12904 [D loss: 0.668799, acc.: 59.38%] [G loss: 0.916701]\n",
      "epoch:13 step:12905 [D loss: 0.691784, acc.: 52.34%] [G loss: 0.816887]\n",
      "epoch:13 step:12906 [D loss: 0.660695, acc.: 52.34%] [G loss: 0.837605]\n",
      "epoch:13 step:12907 [D loss: 0.685556, acc.: 52.34%] [G loss: 0.806633]\n",
      "epoch:13 step:12908 [D loss: 0.659458, acc.: 58.59%] [G loss: 0.826726]\n",
      "epoch:13 step:12909 [D loss: 0.606354, acc.: 67.19%] [G loss: 0.973821]\n",
      "epoch:13 step:12910 [D loss: 0.705188, acc.: 50.78%] [G loss: 0.768085]\n",
      "epoch:13 step:12911 [D loss: 0.731517, acc.: 49.22%] [G loss: 0.732810]\n",
      "epoch:13 step:12912 [D loss: 0.710688, acc.: 50.00%] [G loss: 0.798713]\n",
      "epoch:13 step:12913 [D loss: 0.690196, acc.: 54.69%] [G loss: 0.852266]\n",
      "epoch:13 step:12914 [D loss: 0.646625, acc.: 60.94%] [G loss: 0.731921]\n",
      "epoch:13 step:12915 [D loss: 0.707808, acc.: 53.12%] [G loss: 0.782146]\n",
      "epoch:13 step:12916 [D loss: 0.617698, acc.: 67.97%] [G loss: 0.825163]\n",
      "epoch:13 step:12917 [D loss: 0.620884, acc.: 64.06%] [G loss: 0.813801]\n",
      "epoch:13 step:12918 [D loss: 0.681724, acc.: 53.12%] [G loss: 0.791637]\n",
      "epoch:13 step:12919 [D loss: 0.708071, acc.: 46.88%] [G loss: 0.720177]\n",
      "epoch:13 step:12920 [D loss: 0.646318, acc.: 55.47%] [G loss: 0.824962]\n",
      "epoch:13 step:12921 [D loss: 0.699510, acc.: 57.81%] [G loss: 0.801637]\n",
      "epoch:13 step:12922 [D loss: 0.718383, acc.: 57.03%] [G loss: 0.825689]\n",
      "epoch:13 step:12923 [D loss: 0.614463, acc.: 72.66%] [G loss: 0.802105]\n",
      "epoch:13 step:12924 [D loss: 0.653220, acc.: 62.50%] [G loss: 0.792248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12925 [D loss: 0.602689, acc.: 71.09%] [G loss: 0.891103]\n",
      "epoch:13 step:12926 [D loss: 0.688780, acc.: 47.66%] [G loss: 0.822068]\n",
      "epoch:13 step:12927 [D loss: 0.752214, acc.: 46.88%] [G loss: 0.886890]\n",
      "epoch:13 step:12928 [D loss: 0.639162, acc.: 63.28%] [G loss: 0.782609]\n",
      "epoch:13 step:12929 [D loss: 0.665703, acc.: 62.50%] [G loss: 0.841273]\n",
      "epoch:13 step:12930 [D loss: 0.661666, acc.: 64.06%] [G loss: 0.943677]\n",
      "epoch:13 step:12931 [D loss: 0.618580, acc.: 73.44%] [G loss: 0.990611]\n",
      "epoch:13 step:12932 [D loss: 0.713819, acc.: 53.12%] [G loss: 0.906358]\n",
      "epoch:13 step:12933 [D loss: 0.641548, acc.: 64.06%] [G loss: 0.854476]\n",
      "epoch:13 step:12934 [D loss: 0.689156, acc.: 57.81%] [G loss: 0.919279]\n",
      "epoch:13 step:12935 [D loss: 0.633751, acc.: 65.62%] [G loss: 0.861867]\n",
      "epoch:13 step:12936 [D loss: 0.626451, acc.: 68.75%] [G loss: 0.960634]\n",
      "epoch:13 step:12937 [D loss: 0.710151, acc.: 52.34%] [G loss: 0.796330]\n",
      "epoch:13 step:12938 [D loss: 0.675881, acc.: 50.00%] [G loss: 0.929827]\n",
      "epoch:13 step:12939 [D loss: 0.732741, acc.: 52.34%] [G loss: 0.828530]\n",
      "epoch:13 step:12940 [D loss: 0.736480, acc.: 49.22%] [G loss: 0.752692]\n",
      "epoch:13 step:12941 [D loss: 0.705726, acc.: 51.56%] [G loss: 0.880400]\n",
      "epoch:13 step:12942 [D loss: 0.691874, acc.: 55.47%] [G loss: 0.863028]\n",
      "epoch:13 step:12943 [D loss: 0.643900, acc.: 64.06%] [G loss: 0.824665]\n",
      "epoch:13 step:12944 [D loss: 0.700751, acc.: 57.03%] [G loss: 0.854065]\n",
      "epoch:13 step:12945 [D loss: 0.617780, acc.: 77.34%] [G loss: 0.974153]\n",
      "epoch:13 step:12946 [D loss: 0.664852, acc.: 58.59%] [G loss: 0.866851]\n",
      "epoch:13 step:12947 [D loss: 0.685327, acc.: 52.34%] [G loss: 0.808896]\n",
      "epoch:13 step:12948 [D loss: 0.682482, acc.: 57.03%] [G loss: 0.866694]\n",
      "epoch:13 step:12949 [D loss: 0.633052, acc.: 64.06%] [G loss: 0.944132]\n",
      "epoch:13 step:12950 [D loss: 0.695557, acc.: 58.59%] [G loss: 0.941914]\n",
      "epoch:13 step:12951 [D loss: 0.640724, acc.: 58.59%] [G loss: 0.911851]\n",
      "epoch:13 step:12952 [D loss: 0.613249, acc.: 65.62%] [G loss: 0.902206]\n",
      "epoch:13 step:12953 [D loss: 0.668151, acc.: 58.59%] [G loss: 0.913023]\n",
      "epoch:13 step:12954 [D loss: 0.754717, acc.: 42.97%] [G loss: 0.871902]\n",
      "epoch:13 step:12955 [D loss: 0.697641, acc.: 48.44%] [G loss: 0.897717]\n",
      "epoch:13 step:12956 [D loss: 0.653511, acc.: 55.47%] [G loss: 0.901711]\n",
      "epoch:13 step:12957 [D loss: 0.705551, acc.: 56.25%] [G loss: 0.829087]\n",
      "epoch:13 step:12958 [D loss: 0.675766, acc.: 55.47%] [G loss: 0.962202]\n",
      "epoch:13 step:12959 [D loss: 0.683203, acc.: 53.91%] [G loss: 0.845501]\n",
      "epoch:13 step:12960 [D loss: 0.673391, acc.: 51.56%] [G loss: 0.784139]\n",
      "epoch:13 step:12961 [D loss: 0.630489, acc.: 66.41%] [G loss: 0.816603]\n",
      "epoch:13 step:12962 [D loss: 0.661095, acc.: 61.72%] [G loss: 0.883962]\n",
      "epoch:13 step:12963 [D loss: 0.680972, acc.: 51.56%] [G loss: 0.765472]\n",
      "epoch:13 step:12964 [D loss: 0.647812, acc.: 64.06%] [G loss: 0.836643]\n",
      "epoch:13 step:12965 [D loss: 0.638192, acc.: 63.28%] [G loss: 0.928420]\n",
      "epoch:13 step:12966 [D loss: 0.715487, acc.: 49.22%] [G loss: 0.980336]\n",
      "epoch:13 step:12967 [D loss: 0.630305, acc.: 64.06%] [G loss: 0.839970]\n",
      "epoch:13 step:12968 [D loss: 0.696381, acc.: 48.44%] [G loss: 0.738981]\n",
      "epoch:13 step:12969 [D loss: 0.658319, acc.: 63.28%] [G loss: 0.885923]\n",
      "epoch:13 step:12970 [D loss: 0.671367, acc.: 57.81%] [G loss: 0.802702]\n",
      "epoch:13 step:12971 [D loss: 0.669252, acc.: 55.47%] [G loss: 0.788582]\n",
      "epoch:13 step:12972 [D loss: 0.687352, acc.: 50.00%] [G loss: 0.850868]\n",
      "epoch:13 step:12973 [D loss: 0.634531, acc.: 67.97%] [G loss: 0.736358]\n",
      "epoch:13 step:12974 [D loss: 0.660427, acc.: 51.56%] [G loss: 0.839119]\n",
      "epoch:13 step:12975 [D loss: 0.626249, acc.: 66.41%] [G loss: 0.879106]\n",
      "epoch:13 step:12976 [D loss: 0.670430, acc.: 54.69%] [G loss: 0.781271]\n",
      "epoch:13 step:12977 [D loss: 0.636445, acc.: 60.94%] [G loss: 0.995002]\n",
      "epoch:13 step:12978 [D loss: 0.638930, acc.: 57.03%] [G loss: 0.897411]\n",
      "epoch:13 step:12979 [D loss: 0.650856, acc.: 64.84%] [G loss: 0.844566]\n",
      "epoch:13 step:12980 [D loss: 0.645075, acc.: 60.94%] [G loss: 0.833229]\n",
      "epoch:13 step:12981 [D loss: 0.623184, acc.: 67.19%] [G loss: 0.880373]\n",
      "epoch:13 step:12982 [D loss: 0.659037, acc.: 61.72%] [G loss: 0.788541]\n",
      "epoch:13 step:12983 [D loss: 0.739670, acc.: 49.22%] [G loss: 0.761821]\n",
      "epoch:13 step:12984 [D loss: 0.677903, acc.: 52.34%] [G loss: 0.828839]\n",
      "epoch:13 step:12985 [D loss: 0.650569, acc.: 61.72%] [G loss: 0.803557]\n",
      "epoch:13 step:12986 [D loss: 0.624527, acc.: 67.19%] [G loss: 0.857181]\n",
      "epoch:13 step:12987 [D loss: 0.737576, acc.: 41.41%] [G loss: 0.812472]\n",
      "epoch:13 step:12988 [D loss: 0.653971, acc.: 59.38%] [G loss: 0.817216]\n",
      "epoch:13 step:12989 [D loss: 0.703664, acc.: 50.78%] [G loss: 0.807234]\n",
      "epoch:13 step:12990 [D loss: 0.636425, acc.: 59.38%] [G loss: 0.913482]\n",
      "epoch:13 step:12991 [D loss: 0.686730, acc.: 53.91%] [G loss: 0.853040]\n",
      "epoch:13 step:12992 [D loss: 0.645126, acc.: 57.81%] [G loss: 0.852671]\n",
      "epoch:13 step:12993 [D loss: 0.663632, acc.: 57.81%] [G loss: 0.821676]\n",
      "epoch:13 step:12994 [D loss: 0.741002, acc.: 52.34%] [G loss: 0.721997]\n",
      "epoch:13 step:12995 [D loss: 0.713142, acc.: 49.22%] [G loss: 0.753175]\n",
      "epoch:13 step:12996 [D loss: 0.626971, acc.: 67.19%] [G loss: 0.840641]\n",
      "epoch:13 step:12997 [D loss: 0.685413, acc.: 53.12%] [G loss: 0.920655]\n",
      "epoch:13 step:12998 [D loss: 0.719741, acc.: 50.00%] [G loss: 0.837594]\n",
      "epoch:13 step:12999 [D loss: 0.687031, acc.: 47.66%] [G loss: 0.835223]\n",
      "epoch:13 step:13000 [D loss: 0.678041, acc.: 54.69%] [G loss: 0.777148]\n",
      "epoch:13 step:13001 [D loss: 0.650225, acc.: 62.50%] [G loss: 0.735918]\n",
      "epoch:13 step:13002 [D loss: 0.667406, acc.: 55.47%] [G loss: 0.851435]\n",
      "epoch:13 step:13003 [D loss: 0.645787, acc.: 59.38%] [G loss: 0.832775]\n",
      "epoch:13 step:13004 [D loss: 0.650520, acc.: 61.72%] [G loss: 0.851046]\n",
      "epoch:13 step:13005 [D loss: 0.729183, acc.: 53.12%] [G loss: 0.979737]\n",
      "epoch:13 step:13006 [D loss: 0.714919, acc.: 57.03%] [G loss: 0.813056]\n",
      "epoch:13 step:13007 [D loss: 0.659871, acc.: 57.03%] [G loss: 0.791445]\n",
      "epoch:13 step:13008 [D loss: 0.708419, acc.: 54.69%] [G loss: 0.819687]\n",
      "epoch:13 step:13009 [D loss: 0.700084, acc.: 51.56%] [G loss: 0.823116]\n",
      "epoch:13 step:13010 [D loss: 0.678213, acc.: 53.91%] [G loss: 0.756181]\n",
      "epoch:13 step:13011 [D loss: 0.627594, acc.: 67.19%] [G loss: 0.754470]\n",
      "epoch:13 step:13012 [D loss: 0.674774, acc.: 63.28%] [G loss: 0.786009]\n",
      "epoch:13 step:13013 [D loss: 0.670428, acc.: 52.34%] [G loss: 0.820189]\n",
      "epoch:13 step:13014 [D loss: 0.671718, acc.: 57.03%] [G loss: 0.764556]\n",
      "epoch:13 step:13015 [D loss: 0.677154, acc.: 52.34%] [G loss: 0.718258]\n",
      "epoch:13 step:13016 [D loss: 0.675950, acc.: 52.34%] [G loss: 0.866066]\n",
      "epoch:13 step:13017 [D loss: 0.700721, acc.: 50.78%] [G loss: 0.800374]\n",
      "epoch:13 step:13018 [D loss: 0.703303, acc.: 50.00%] [G loss: 0.786510]\n",
      "epoch:13 step:13019 [D loss: 0.686913, acc.: 53.12%] [G loss: 0.860217]\n",
      "epoch:13 step:13020 [D loss: 0.670812, acc.: 53.91%] [G loss: 0.864632]\n",
      "epoch:13 step:13021 [D loss: 0.644547, acc.: 61.72%] [G loss: 0.801433]\n",
      "epoch:13 step:13022 [D loss: 0.635587, acc.: 67.19%] [G loss: 0.829317]\n",
      "epoch:13 step:13023 [D loss: 0.692265, acc.: 59.38%] [G loss: 0.897788]\n",
      "epoch:13 step:13024 [D loss: 0.624329, acc.: 71.09%] [G loss: 0.804019]\n",
      "epoch:13 step:13025 [D loss: 0.671517, acc.: 55.47%] [G loss: 0.807782]\n",
      "epoch:13 step:13026 [D loss: 0.699420, acc.: 52.34%] [G loss: 0.904950]\n",
      "epoch:13 step:13027 [D loss: 0.646975, acc.: 61.72%] [G loss: 0.856796]\n",
      "epoch:13 step:13028 [D loss: 0.680399, acc.: 55.47%] [G loss: 0.833102]\n",
      "epoch:13 step:13029 [D loss: 0.612919, acc.: 70.31%] [G loss: 0.746539]\n",
      "epoch:13 step:13030 [D loss: 0.675458, acc.: 58.59%] [G loss: 0.719710]\n",
      "epoch:13 step:13031 [D loss: 0.638680, acc.: 64.06%] [G loss: 0.809193]\n",
      "epoch:13 step:13032 [D loss: 0.585674, acc.: 78.91%] [G loss: 0.776270]\n",
      "epoch:13 step:13033 [D loss: 0.711643, acc.: 60.16%] [G loss: 0.832788]\n",
      "epoch:13 step:13034 [D loss: 0.636198, acc.: 57.81%] [G loss: 0.704110]\n",
      "epoch:13 step:13035 [D loss: 0.740903, acc.: 42.97%] [G loss: 1.054280]\n",
      "epoch:13 step:13036 [D loss: 0.751780, acc.: 39.06%] [G loss: 0.905995]\n",
      "epoch:13 step:13037 [D loss: 0.677156, acc.: 56.25%] [G loss: 0.819625]\n",
      "epoch:13 step:13038 [D loss: 0.701137, acc.: 47.66%] [G loss: 0.819589]\n",
      "epoch:13 step:13039 [D loss: 0.721532, acc.: 44.53%] [G loss: 0.872326]\n",
      "epoch:13 step:13040 [D loss: 0.702380, acc.: 51.56%] [G loss: 0.841941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:13041 [D loss: 0.630316, acc.: 64.06%] [G loss: 0.961670]\n",
      "epoch:13 step:13042 [D loss: 0.648884, acc.: 62.50%] [G loss: 0.887879]\n",
      "epoch:13 step:13043 [D loss: 0.690369, acc.: 45.31%] [G loss: 0.863579]\n",
      "epoch:13 step:13044 [D loss: 0.647165, acc.: 53.12%] [G loss: 0.988794]\n",
      "epoch:13 step:13045 [D loss: 0.641502, acc.: 59.38%] [G loss: 0.875042]\n",
      "epoch:13 step:13046 [D loss: 0.661792, acc.: 57.81%] [G loss: 0.908952]\n",
      "epoch:13 step:13047 [D loss: 0.688523, acc.: 54.69%] [G loss: 0.830635]\n",
      "epoch:13 step:13048 [D loss: 0.661666, acc.: 58.59%] [G loss: 0.886596]\n",
      "epoch:13 step:13049 [D loss: 0.623396, acc.: 66.41%] [G loss: 0.769229]\n",
      "epoch:13 step:13050 [D loss: 0.641153, acc.: 61.72%] [G loss: 0.780628]\n",
      "epoch:13 step:13051 [D loss: 0.679336, acc.: 52.34%] [G loss: 0.991236]\n",
      "epoch:13 step:13052 [D loss: 0.749412, acc.: 46.88%] [G loss: 0.871464]\n",
      "epoch:13 step:13053 [D loss: 0.657665, acc.: 54.69%] [G loss: 0.856248]\n",
      "epoch:13 step:13054 [D loss: 0.707677, acc.: 53.12%] [G loss: 0.793314]\n",
      "epoch:13 step:13055 [D loss: 0.683069, acc.: 58.59%] [G loss: 0.757312]\n",
      "epoch:13 step:13056 [D loss: 0.689221, acc.: 58.59%] [G loss: 0.792576]\n",
      "epoch:13 step:13057 [D loss: 0.668155, acc.: 56.25%] [G loss: 0.787774]\n",
      "epoch:13 step:13058 [D loss: 0.671459, acc.: 49.22%] [G loss: 0.871302]\n",
      "epoch:13 step:13059 [D loss: 0.744535, acc.: 35.16%] [G loss: 0.782348]\n",
      "epoch:13 step:13060 [D loss: 0.646343, acc.: 63.28%] [G loss: 0.843333]\n",
      "epoch:13 step:13061 [D loss: 0.684106, acc.: 52.34%] [G loss: 0.858155]\n",
      "epoch:13 step:13062 [D loss: 0.615507, acc.: 63.28%] [G loss: 0.862819]\n",
      "epoch:13 step:13063 [D loss: 0.667723, acc.: 71.88%] [G loss: 0.817511]\n",
      "epoch:13 step:13064 [D loss: 0.722281, acc.: 52.34%] [G loss: 0.834996]\n",
      "epoch:13 step:13065 [D loss: 0.645733, acc.: 60.16%] [G loss: 0.839388]\n",
      "epoch:13 step:13066 [D loss: 0.666589, acc.: 60.94%] [G loss: 0.814461]\n",
      "epoch:13 step:13067 [D loss: 0.698614, acc.: 57.81%] [G loss: 0.830975]\n",
      "epoch:13 step:13068 [D loss: 0.635141, acc.: 64.06%] [G loss: 0.858209]\n",
      "epoch:13 step:13069 [D loss: 0.640132, acc.: 64.84%] [G loss: 0.864339]\n",
      "epoch:13 step:13070 [D loss: 0.620827, acc.: 69.53%] [G loss: 0.885074]\n",
      "epoch:13 step:13071 [D loss: 0.625871, acc.: 71.09%] [G loss: 0.892894]\n",
      "epoch:13 step:13072 [D loss: 0.615978, acc.: 72.66%] [G loss: 0.791995]\n",
      "epoch:13 step:13073 [D loss: 0.678304, acc.: 54.69%] [G loss: 0.732296]\n",
      "epoch:13 step:13074 [D loss: 0.656648, acc.: 60.16%] [G loss: 0.846236]\n",
      "epoch:13 step:13075 [D loss: 0.709666, acc.: 49.22%] [G loss: 0.769447]\n",
      "epoch:13 step:13076 [D loss: 0.669024, acc.: 50.78%] [G loss: 0.831081]\n",
      "epoch:13 step:13077 [D loss: 0.674013, acc.: 57.03%] [G loss: 0.726972]\n",
      "epoch:13 step:13078 [D loss: 0.618944, acc.: 72.66%] [G loss: 0.880984]\n",
      "epoch:13 step:13079 [D loss: 0.609448, acc.: 72.66%] [G loss: 0.915531]\n",
      "epoch:13 step:13080 [D loss: 0.641781, acc.: 61.72%] [G loss: 0.906031]\n",
      "epoch:13 step:13081 [D loss: 0.702331, acc.: 47.66%] [G loss: 0.791140]\n",
      "epoch:13 step:13082 [D loss: 0.646318, acc.: 63.28%] [G loss: 0.844767]\n",
      "epoch:13 step:13083 [D loss: 0.680034, acc.: 59.38%] [G loss: 0.914510]\n",
      "epoch:13 step:13084 [D loss: 0.689148, acc.: 51.56%] [G loss: 0.796403]\n",
      "epoch:13 step:13085 [D loss: 0.729142, acc.: 47.66%] [G loss: 0.716994]\n",
      "epoch:13 step:13086 [D loss: 0.685720, acc.: 53.91%] [G loss: 0.905317]\n",
      "epoch:13 step:13087 [D loss: 0.631735, acc.: 65.62%] [G loss: 0.880386]\n",
      "epoch:13 step:13088 [D loss: 0.658385, acc.: 59.38%] [G loss: 0.963259]\n",
      "epoch:13 step:13089 [D loss: 0.631594, acc.: 63.28%] [G loss: 0.759665]\n",
      "epoch:13 step:13090 [D loss: 0.663494, acc.: 57.81%] [G loss: 0.822883]\n",
      "epoch:13 step:13091 [D loss: 0.566454, acc.: 73.44%] [G loss: 0.924497]\n",
      "epoch:13 step:13092 [D loss: 0.691543, acc.: 56.25%] [G loss: 0.827265]\n",
      "epoch:13 step:13093 [D loss: 0.604045, acc.: 73.44%] [G loss: 0.819924]\n",
      "epoch:13 step:13094 [D loss: 0.604422, acc.: 68.75%] [G loss: 0.885153]\n",
      "epoch:13 step:13095 [D loss: 0.636631, acc.: 61.72%] [G loss: 0.837544]\n",
      "epoch:13 step:13096 [D loss: 0.646559, acc.: 56.25%] [G loss: 0.930348]\n",
      "epoch:13 step:13097 [D loss: 0.626939, acc.: 63.28%] [G loss: 0.935641]\n",
      "epoch:13 step:13098 [D loss: 0.614140, acc.: 67.97%] [G loss: 0.807278]\n",
      "epoch:13 step:13099 [D loss: 0.692514, acc.: 51.56%] [G loss: 0.891930]\n",
      "epoch:13 step:13100 [D loss: 0.712844, acc.: 58.59%] [G loss: 0.822320]\n",
      "epoch:13 step:13101 [D loss: 0.657998, acc.: 58.59%] [G loss: 0.756711]\n",
      "epoch:13 step:13102 [D loss: 0.738534, acc.: 43.75%] [G loss: 0.819472]\n",
      "epoch:13 step:13103 [D loss: 0.634952, acc.: 67.19%] [G loss: 0.727395]\n",
      "epoch:13 step:13104 [D loss: 0.663947, acc.: 60.16%] [G loss: 0.792317]\n",
      "epoch:13 step:13105 [D loss: 0.668003, acc.: 58.59%] [G loss: 0.755758]\n",
      "epoch:13 step:13106 [D loss: 0.706704, acc.: 50.78%] [G loss: 0.761845]\n",
      "epoch:13 step:13107 [D loss: 0.702538, acc.: 52.34%] [G loss: 0.831164]\n",
      "epoch:13 step:13108 [D loss: 0.675869, acc.: 53.12%] [G loss: 0.824072]\n",
      "epoch:13 step:13109 [D loss: 0.665453, acc.: 57.03%] [G loss: 0.890652]\n",
      "epoch:13 step:13110 [D loss: 0.696979, acc.: 50.78%] [G loss: 0.798579]\n",
      "epoch:13 step:13111 [D loss: 0.711820, acc.: 46.88%] [G loss: 0.963013]\n",
      "epoch:13 step:13112 [D loss: 0.674594, acc.: 52.34%] [G loss: 0.898180]\n",
      "epoch:13 step:13113 [D loss: 0.693555, acc.: 59.38%] [G loss: 0.835952]\n",
      "epoch:13 step:13114 [D loss: 0.703494, acc.: 53.12%] [G loss: 0.775026]\n",
      "epoch:13 step:13115 [D loss: 0.681679, acc.: 50.00%] [G loss: 0.916216]\n",
      "epoch:13 step:13116 [D loss: 0.625463, acc.: 67.19%] [G loss: 0.891473]\n",
      "epoch:13 step:13117 [D loss: 0.682726, acc.: 57.81%] [G loss: 0.988217]\n",
      "epoch:13 step:13118 [D loss: 0.683743, acc.: 57.81%] [G loss: 0.842499]\n",
      "epoch:14 step:13119 [D loss: 0.705530, acc.: 56.25%] [G loss: 0.829942]\n",
      "epoch:14 step:13120 [D loss: 0.671471, acc.: 59.38%] [G loss: 0.845762]\n",
      "epoch:14 step:13121 [D loss: 0.790741, acc.: 43.75%] [G loss: 0.888617]\n",
      "epoch:14 step:13122 [D loss: 0.679462, acc.: 61.72%] [G loss: 0.905275]\n",
      "epoch:14 step:13123 [D loss: 0.676139, acc.: 52.34%] [G loss: 0.775846]\n",
      "epoch:14 step:13124 [D loss: 0.691827, acc.: 47.66%] [G loss: 0.887953]\n",
      "epoch:14 step:13125 [D loss: 0.711755, acc.: 46.88%] [G loss: 0.840070]\n",
      "epoch:14 step:13126 [D loss: 0.777216, acc.: 36.72%] [G loss: 0.801487]\n",
      "epoch:14 step:13127 [D loss: 0.631830, acc.: 63.28%] [G loss: 0.747827]\n",
      "epoch:14 step:13128 [D loss: 0.704937, acc.: 57.03%] [G loss: 0.776487]\n",
      "epoch:14 step:13129 [D loss: 0.665705, acc.: 56.25%] [G loss: 0.853264]\n",
      "epoch:14 step:13130 [D loss: 0.659277, acc.: 60.16%] [G loss: 0.875783]\n",
      "epoch:14 step:13131 [D loss: 0.728754, acc.: 51.56%] [G loss: 0.852091]\n",
      "epoch:14 step:13132 [D loss: 0.718730, acc.: 51.56%] [G loss: 0.894217]\n",
      "epoch:14 step:13133 [D loss: 0.651850, acc.: 66.41%] [G loss: 0.900260]\n",
      "epoch:14 step:13134 [D loss: 0.711820, acc.: 57.81%] [G loss: 0.833415]\n",
      "epoch:14 step:13135 [D loss: 0.675038, acc.: 60.16%] [G loss: 0.835686]\n",
      "epoch:14 step:13136 [D loss: 0.674132, acc.: 55.47%] [G loss: 0.858742]\n",
      "epoch:14 step:13137 [D loss: 0.681702, acc.: 61.72%] [G loss: 0.750506]\n",
      "epoch:14 step:13138 [D loss: 0.635852, acc.: 68.75%] [G loss: 0.890500]\n",
      "epoch:14 step:13139 [D loss: 0.665907, acc.: 57.03%] [G loss: 0.809857]\n",
      "epoch:14 step:13140 [D loss: 0.634137, acc.: 64.84%] [G loss: 0.774331]\n",
      "epoch:14 step:13141 [D loss: 0.692652, acc.: 51.56%] [G loss: 0.809808]\n",
      "epoch:14 step:13142 [D loss: 0.659435, acc.: 61.72%] [G loss: 0.843946]\n",
      "epoch:14 step:13143 [D loss: 0.693800, acc.: 52.34%] [G loss: 0.826255]\n",
      "epoch:14 step:13144 [D loss: 0.675578, acc.: 57.81%] [G loss: 0.857091]\n",
      "epoch:14 step:13145 [D loss: 0.674917, acc.: 56.25%] [G loss: 0.821636]\n",
      "epoch:14 step:13146 [D loss: 0.661058, acc.: 56.25%] [G loss: 0.812197]\n",
      "epoch:14 step:13147 [D loss: 0.667379, acc.: 60.16%] [G loss: 0.761324]\n",
      "epoch:14 step:13148 [D loss: 0.684726, acc.: 52.34%] [G loss: 0.811571]\n",
      "epoch:14 step:13149 [D loss: 0.729387, acc.: 43.75%] [G loss: 0.974263]\n",
      "epoch:14 step:13150 [D loss: 0.676317, acc.: 59.38%] [G loss: 0.912767]\n",
      "epoch:14 step:13151 [D loss: 0.680462, acc.: 57.03%] [G loss: 0.836571]\n",
      "epoch:14 step:13152 [D loss: 0.719376, acc.: 48.44%] [G loss: 0.804703]\n",
      "epoch:14 step:13153 [D loss: 0.683546, acc.: 50.78%] [G loss: 0.784891]\n",
      "epoch:14 step:13154 [D loss: 0.686135, acc.: 47.66%] [G loss: 0.826970]\n",
      "epoch:14 step:13155 [D loss: 0.649008, acc.: 67.19%] [G loss: 0.837213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13156 [D loss: 0.675808, acc.: 52.34%] [G loss: 0.798162]\n",
      "epoch:14 step:13157 [D loss: 0.681009, acc.: 53.12%] [G loss: 0.826574]\n",
      "epoch:14 step:13158 [D loss: 0.709324, acc.: 42.97%] [G loss: 0.782109]\n",
      "epoch:14 step:13159 [D loss: 0.665582, acc.: 58.59%] [G loss: 0.821490]\n",
      "epoch:14 step:13160 [D loss: 0.722609, acc.: 48.44%] [G loss: 0.813327]\n",
      "epoch:14 step:13161 [D loss: 0.647209, acc.: 64.84%] [G loss: 0.876928]\n",
      "epoch:14 step:13162 [D loss: 0.675000, acc.: 54.69%] [G loss: 0.806035]\n",
      "epoch:14 step:13163 [D loss: 0.711606, acc.: 47.66%] [G loss: 0.783101]\n",
      "epoch:14 step:13164 [D loss: 0.635946, acc.: 64.06%] [G loss: 0.720316]\n",
      "epoch:14 step:13165 [D loss: 0.688166, acc.: 58.59%] [G loss: 0.781569]\n",
      "epoch:14 step:13166 [D loss: 0.666214, acc.: 58.59%] [G loss: 0.792219]\n",
      "epoch:14 step:13167 [D loss: 0.652105, acc.: 57.81%] [G loss: 0.829986]\n",
      "epoch:14 step:13168 [D loss: 0.680672, acc.: 52.34%] [G loss: 0.790379]\n",
      "epoch:14 step:13169 [D loss: 0.659222, acc.: 56.25%] [G loss: 0.870609]\n",
      "epoch:14 step:13170 [D loss: 0.632473, acc.: 64.06%] [G loss: 0.910789]\n",
      "epoch:14 step:13171 [D loss: 0.676423, acc.: 53.91%] [G loss: 0.842577]\n",
      "epoch:14 step:13172 [D loss: 0.658283, acc.: 56.25%] [G loss: 0.897968]\n",
      "epoch:14 step:13173 [D loss: 0.704894, acc.: 47.66%] [G loss: 0.831352]\n",
      "epoch:14 step:13174 [D loss: 0.623540, acc.: 63.28%] [G loss: 0.884194]\n",
      "epoch:14 step:13175 [D loss: 0.658568, acc.: 54.69%] [G loss: 0.856801]\n",
      "epoch:14 step:13176 [D loss: 0.657886, acc.: 61.72%] [G loss: 0.799709]\n",
      "epoch:14 step:13177 [D loss: 0.660151, acc.: 60.94%] [G loss: 0.742838]\n",
      "epoch:14 step:13178 [D loss: 0.691979, acc.: 53.91%] [G loss: 0.852745]\n",
      "epoch:14 step:13179 [D loss: 0.683645, acc.: 51.56%] [G loss: 0.833480]\n",
      "epoch:14 step:13180 [D loss: 0.709157, acc.: 58.59%] [G loss: 0.856354]\n",
      "epoch:14 step:13181 [D loss: 0.653749, acc.: 57.03%] [G loss: 0.917411]\n",
      "epoch:14 step:13182 [D loss: 0.670833, acc.: 58.59%] [G loss: 0.900921]\n",
      "epoch:14 step:13183 [D loss: 0.672452, acc.: 55.47%] [G loss: 0.820350]\n",
      "epoch:14 step:13184 [D loss: 0.644521, acc.: 64.06%] [G loss: 0.985038]\n",
      "epoch:14 step:13185 [D loss: 0.674347, acc.: 56.25%] [G loss: 0.809062]\n",
      "epoch:14 step:13186 [D loss: 0.648905, acc.: 60.16%] [G loss: 0.908586]\n",
      "epoch:14 step:13187 [D loss: 0.644631, acc.: 63.28%] [G loss: 0.856722]\n",
      "epoch:14 step:13188 [D loss: 0.686891, acc.: 46.09%] [G loss: 0.803721]\n",
      "epoch:14 step:13189 [D loss: 0.644350, acc.: 54.69%] [G loss: 0.904285]\n",
      "epoch:14 step:13190 [D loss: 0.682917, acc.: 53.91%] [G loss: 0.894088]\n",
      "epoch:14 step:13191 [D loss: 0.612260, acc.: 69.53%] [G loss: 0.912766]\n",
      "epoch:14 step:13192 [D loss: 0.664425, acc.: 53.91%] [G loss: 0.871902]\n",
      "epoch:14 step:13193 [D loss: 0.665615, acc.: 56.25%] [G loss: 1.088207]\n",
      "epoch:14 step:13194 [D loss: 0.702469, acc.: 56.25%] [G loss: 0.899981]\n",
      "epoch:14 step:13195 [D loss: 0.616168, acc.: 64.06%] [G loss: 0.931330]\n",
      "epoch:14 step:13196 [D loss: 0.596196, acc.: 64.84%] [G loss: 0.974321]\n",
      "epoch:14 step:13197 [D loss: 0.572180, acc.: 73.44%] [G loss: 0.900298]\n",
      "epoch:14 step:13198 [D loss: 0.677182, acc.: 53.12%] [G loss: 1.009555]\n",
      "epoch:14 step:13199 [D loss: 0.643904, acc.: 60.16%] [G loss: 0.945239]\n",
      "epoch:14 step:13200 [D loss: 0.703799, acc.: 50.78%] [G loss: 1.020113]\n",
      "epoch:14 step:13201 [D loss: 0.667287, acc.: 54.69%] [G loss: 0.988584]\n",
      "epoch:14 step:13202 [D loss: 0.648279, acc.: 65.62%] [G loss: 0.933564]\n",
      "epoch:14 step:13203 [D loss: 0.696778, acc.: 53.12%] [G loss: 0.784122]\n",
      "epoch:14 step:13204 [D loss: 0.641855, acc.: 63.28%] [G loss: 0.878269]\n",
      "epoch:14 step:13205 [D loss: 0.649518, acc.: 65.62%] [G loss: 0.870087]\n",
      "epoch:14 step:13206 [D loss: 0.664574, acc.: 60.16%] [G loss: 0.868598]\n",
      "epoch:14 step:13207 [D loss: 0.654847, acc.: 64.84%] [G loss: 0.769756]\n",
      "epoch:14 step:13208 [D loss: 0.691070, acc.: 56.25%] [G loss: 0.902957]\n",
      "epoch:14 step:13209 [D loss: 0.577337, acc.: 71.88%] [G loss: 0.888705]\n",
      "epoch:14 step:13210 [D loss: 0.732921, acc.: 42.19%] [G loss: 0.912186]\n",
      "epoch:14 step:13211 [D loss: 0.666205, acc.: 57.81%] [G loss: 0.820514]\n",
      "epoch:14 step:13212 [D loss: 0.646406, acc.: 57.81%] [G loss: 0.794020]\n",
      "epoch:14 step:13213 [D loss: 0.686949, acc.: 57.03%] [G loss: 0.859447]\n",
      "epoch:14 step:13214 [D loss: 0.628812, acc.: 66.41%] [G loss: 0.782816]\n",
      "epoch:14 step:13215 [D loss: 0.685038, acc.: 58.59%] [G loss: 0.791324]\n",
      "epoch:14 step:13216 [D loss: 0.670693, acc.: 53.12%] [G loss: 0.854967]\n",
      "epoch:14 step:13217 [D loss: 0.655492, acc.: 56.25%] [G loss: 0.811444]\n",
      "epoch:14 step:13218 [D loss: 0.651304, acc.: 60.94%] [G loss: 0.830617]\n",
      "epoch:14 step:13219 [D loss: 0.747081, acc.: 42.97%] [G loss: 0.909716]\n",
      "epoch:14 step:13220 [D loss: 0.653331, acc.: 59.38%] [G loss: 0.805150]\n",
      "epoch:14 step:13221 [D loss: 0.613499, acc.: 71.09%] [G loss: 0.814859]\n",
      "epoch:14 step:13222 [D loss: 0.668495, acc.: 66.41%] [G loss: 0.774087]\n",
      "epoch:14 step:13223 [D loss: 0.736184, acc.: 41.41%] [G loss: 0.776635]\n",
      "epoch:14 step:13224 [D loss: 0.678958, acc.: 53.91%] [G loss: 0.796902]\n",
      "epoch:14 step:13225 [D loss: 0.676275, acc.: 55.47%] [G loss: 0.698610]\n",
      "epoch:14 step:13226 [D loss: 0.672569, acc.: 52.34%] [G loss: 0.728602]\n",
      "epoch:14 step:13227 [D loss: 0.701113, acc.: 44.53%] [G loss: 0.749197]\n",
      "epoch:14 step:13228 [D loss: 0.692731, acc.: 60.94%] [G loss: 0.680291]\n",
      "epoch:14 step:13229 [D loss: 0.676352, acc.: 51.56%] [G loss: 0.721208]\n",
      "epoch:14 step:13230 [D loss: 0.693621, acc.: 53.12%] [G loss: 0.825197]\n",
      "epoch:14 step:13231 [D loss: 0.645883, acc.: 66.41%] [G loss: 0.808849]\n",
      "epoch:14 step:13232 [D loss: 0.640329, acc.: 66.41%] [G loss: 0.808684]\n",
      "epoch:14 step:13233 [D loss: 0.632613, acc.: 64.84%] [G loss: 0.842087]\n",
      "epoch:14 step:13234 [D loss: 0.669848, acc.: 60.94%] [G loss: 0.853148]\n",
      "epoch:14 step:13235 [D loss: 0.697059, acc.: 58.59%] [G loss: 1.032346]\n",
      "epoch:14 step:13236 [D loss: 0.699273, acc.: 52.34%] [G loss: 0.783432]\n",
      "epoch:14 step:13237 [D loss: 0.598224, acc.: 71.88%] [G loss: 0.808700]\n",
      "epoch:14 step:13238 [D loss: 0.711376, acc.: 50.78%] [G loss: 0.721276]\n",
      "epoch:14 step:13239 [D loss: 0.647661, acc.: 56.25%] [G loss: 0.851207]\n",
      "epoch:14 step:13240 [D loss: 0.665726, acc.: 57.81%] [G loss: 0.799152]\n",
      "epoch:14 step:13241 [D loss: 0.695762, acc.: 52.34%] [G loss: 0.883749]\n",
      "epoch:14 step:13242 [D loss: 0.733777, acc.: 44.53%] [G loss: 0.744401]\n",
      "epoch:14 step:13243 [D loss: 0.672315, acc.: 57.81%] [G loss: 0.729263]\n",
      "epoch:14 step:13244 [D loss: 0.684622, acc.: 49.22%] [G loss: 0.843682]\n",
      "epoch:14 step:13245 [D loss: 0.687528, acc.: 54.69%] [G loss: 0.775013]\n",
      "epoch:14 step:13246 [D loss: 0.668290, acc.: 57.03%] [G loss: 0.776479]\n",
      "epoch:14 step:13247 [D loss: 0.614173, acc.: 65.62%] [G loss: 0.813637]\n",
      "epoch:14 step:13248 [D loss: 0.638080, acc.: 64.84%] [G loss: 0.826589]\n",
      "epoch:14 step:13249 [D loss: 0.608881, acc.: 72.66%] [G loss: 0.764984]\n",
      "epoch:14 step:13250 [D loss: 0.717898, acc.: 46.88%] [G loss: 0.782176]\n",
      "epoch:14 step:13251 [D loss: 0.665391, acc.: 54.69%] [G loss: 0.785724]\n",
      "epoch:14 step:13252 [D loss: 0.687817, acc.: 53.91%] [G loss: 0.822736]\n",
      "epoch:14 step:13253 [D loss: 0.648010, acc.: 59.38%] [G loss: 0.805538]\n",
      "epoch:14 step:13254 [D loss: 0.740748, acc.: 46.88%] [G loss: 0.818658]\n",
      "epoch:14 step:13255 [D loss: 0.645258, acc.: 64.06%] [G loss: 0.928549]\n",
      "epoch:14 step:13256 [D loss: 0.604603, acc.: 68.75%] [G loss: 0.890765]\n",
      "epoch:14 step:13257 [D loss: 0.687520, acc.: 53.12%] [G loss: 0.800972]\n",
      "epoch:14 step:13258 [D loss: 0.638847, acc.: 57.03%] [G loss: 0.900780]\n",
      "epoch:14 step:13259 [D loss: 0.702776, acc.: 46.88%] [G loss: 0.810659]\n",
      "epoch:14 step:13260 [D loss: 0.653368, acc.: 57.03%] [G loss: 0.847602]\n",
      "epoch:14 step:13261 [D loss: 0.685183, acc.: 54.69%] [G loss: 0.878949]\n",
      "epoch:14 step:13262 [D loss: 0.681359, acc.: 53.12%] [G loss: 0.841462]\n",
      "epoch:14 step:13263 [D loss: 0.653849, acc.: 57.81%] [G loss: 0.810059]\n",
      "epoch:14 step:13264 [D loss: 0.706994, acc.: 55.47%] [G loss: 0.756976]\n",
      "epoch:14 step:13265 [D loss: 0.668579, acc.: 58.59%] [G loss: 0.933957]\n",
      "epoch:14 step:13266 [D loss: 0.677749, acc.: 56.25%] [G loss: 0.900002]\n",
      "epoch:14 step:13267 [D loss: 0.666925, acc.: 51.56%] [G loss: 0.915186]\n",
      "epoch:14 step:13268 [D loss: 0.649429, acc.: 56.25%] [G loss: 1.010030]\n",
      "epoch:14 step:13269 [D loss: 0.645977, acc.: 61.72%] [G loss: 0.864513]\n",
      "epoch:14 step:13270 [D loss: 0.702212, acc.: 56.25%] [G loss: 0.806639]\n",
      "epoch:14 step:13271 [D loss: 0.679347, acc.: 60.16%] [G loss: 0.869293]\n",
      "epoch:14 step:13272 [D loss: 0.645297, acc.: 64.06%] [G loss: 0.862866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13273 [D loss: 0.688413, acc.: 53.91%] [G loss: 0.836069]\n",
      "epoch:14 step:13274 [D loss: 0.589989, acc.: 68.75%] [G loss: 0.835780]\n",
      "epoch:14 step:13275 [D loss: 0.705153, acc.: 47.66%] [G loss: 0.788903]\n",
      "epoch:14 step:13276 [D loss: 0.640479, acc.: 59.38%] [G loss: 0.861409]\n",
      "epoch:14 step:13277 [D loss: 0.640423, acc.: 64.06%] [G loss: 0.810878]\n",
      "epoch:14 step:13278 [D loss: 0.583510, acc.: 77.34%] [G loss: 0.834053]\n",
      "epoch:14 step:13279 [D loss: 0.729079, acc.: 46.88%] [G loss: 0.797159]\n",
      "epoch:14 step:13280 [D loss: 0.739771, acc.: 46.88%] [G loss: 0.845829]\n",
      "epoch:14 step:13281 [D loss: 0.636462, acc.: 60.16%] [G loss: 0.885043]\n",
      "epoch:14 step:13282 [D loss: 0.633165, acc.: 60.94%] [G loss: 0.860846]\n",
      "epoch:14 step:13283 [D loss: 0.642250, acc.: 60.94%] [G loss: 0.866750]\n",
      "epoch:14 step:13284 [D loss: 0.667764, acc.: 63.28%] [G loss: 0.906310]\n",
      "epoch:14 step:13285 [D loss: 0.643794, acc.: 57.03%] [G loss: 0.815014]\n",
      "epoch:14 step:13286 [D loss: 0.707907, acc.: 53.91%] [G loss: 0.820030]\n",
      "epoch:14 step:13287 [D loss: 0.664036, acc.: 53.91%] [G loss: 0.786524]\n",
      "epoch:14 step:13288 [D loss: 0.670694, acc.: 55.47%] [G loss: 0.789016]\n",
      "epoch:14 step:13289 [D loss: 0.678078, acc.: 59.38%] [G loss: 0.932223]\n",
      "epoch:14 step:13290 [D loss: 0.670939, acc.: 58.59%] [G loss: 0.866617]\n",
      "epoch:14 step:13291 [D loss: 0.693914, acc.: 55.47%] [G loss: 0.826776]\n",
      "epoch:14 step:13292 [D loss: 0.683594, acc.: 53.12%] [G loss: 0.797894]\n",
      "epoch:14 step:13293 [D loss: 0.721103, acc.: 50.00%] [G loss: 0.802831]\n",
      "epoch:14 step:13294 [D loss: 0.703652, acc.: 52.34%] [G loss: 0.901397]\n",
      "epoch:14 step:13295 [D loss: 0.693858, acc.: 52.34%] [G loss: 0.899946]\n",
      "epoch:14 step:13296 [D loss: 0.637883, acc.: 62.50%] [G loss: 0.923338]\n",
      "epoch:14 step:13297 [D loss: 0.675292, acc.: 50.78%] [G loss: 0.827521]\n",
      "epoch:14 step:13298 [D loss: 0.657881, acc.: 60.16%] [G loss: 0.923598]\n",
      "epoch:14 step:13299 [D loss: 0.689481, acc.: 54.69%] [G loss: 0.877512]\n",
      "epoch:14 step:13300 [D loss: 0.675679, acc.: 58.59%] [G loss: 0.791503]\n",
      "epoch:14 step:13301 [D loss: 0.705057, acc.: 54.69%] [G loss: 0.848741]\n",
      "epoch:14 step:13302 [D loss: 0.640187, acc.: 64.06%] [G loss: 0.802907]\n",
      "epoch:14 step:13303 [D loss: 0.715226, acc.: 50.78%] [G loss: 0.811299]\n",
      "epoch:14 step:13304 [D loss: 0.676037, acc.: 60.94%] [G loss: 0.800287]\n",
      "epoch:14 step:13305 [D loss: 0.685468, acc.: 60.94%] [G loss: 0.830196]\n",
      "epoch:14 step:13306 [D loss: 0.665394, acc.: 54.69%] [G loss: 0.760445]\n",
      "epoch:14 step:13307 [D loss: 0.618122, acc.: 68.75%] [G loss: 0.804200]\n",
      "epoch:14 step:13308 [D loss: 0.669775, acc.: 59.38%] [G loss: 0.821228]\n",
      "epoch:14 step:13309 [D loss: 0.621965, acc.: 69.53%] [G loss: 0.780630]\n",
      "epoch:14 step:13310 [D loss: 0.660329, acc.: 55.47%] [G loss: 0.783293]\n",
      "epoch:14 step:13311 [D loss: 0.662093, acc.: 60.16%] [G loss: 0.768790]\n",
      "epoch:14 step:13312 [D loss: 0.665586, acc.: 59.38%] [G loss: 0.731119]\n",
      "epoch:14 step:13313 [D loss: 0.678153, acc.: 60.94%] [G loss: 0.779570]\n",
      "epoch:14 step:13314 [D loss: 0.724726, acc.: 49.22%] [G loss: 0.823928]\n",
      "epoch:14 step:13315 [D loss: 0.669194, acc.: 51.56%] [G loss: 0.859385]\n",
      "epoch:14 step:13316 [D loss: 0.732490, acc.: 44.53%] [G loss: 0.768723]\n",
      "epoch:14 step:13317 [D loss: 0.655912, acc.: 64.84%] [G loss: 0.821097]\n",
      "epoch:14 step:13318 [D loss: 0.649682, acc.: 62.50%] [G loss: 0.718682]\n",
      "epoch:14 step:13319 [D loss: 0.694252, acc.: 51.56%] [G loss: 0.837601]\n",
      "epoch:14 step:13320 [D loss: 0.708811, acc.: 51.56%] [G loss: 0.759209]\n",
      "epoch:14 step:13321 [D loss: 0.652296, acc.: 67.19%] [G loss: 0.779470]\n",
      "epoch:14 step:13322 [D loss: 0.638739, acc.: 67.19%] [G loss: 0.766965]\n",
      "epoch:14 step:13323 [D loss: 0.658745, acc.: 53.12%] [G loss: 0.765501]\n",
      "epoch:14 step:13324 [D loss: 0.674472, acc.: 56.25%] [G loss: 0.779155]\n",
      "epoch:14 step:13325 [D loss: 0.672002, acc.: 65.62%] [G loss: 0.756762]\n",
      "epoch:14 step:13326 [D loss: 0.695456, acc.: 53.12%] [G loss: 0.801138]\n",
      "epoch:14 step:13327 [D loss: 0.688599, acc.: 50.78%] [G loss: 0.834185]\n",
      "epoch:14 step:13328 [D loss: 0.660130, acc.: 61.72%] [G loss: 0.790212]\n",
      "epoch:14 step:13329 [D loss: 0.690361, acc.: 57.81%] [G loss: 0.829883]\n",
      "epoch:14 step:13330 [D loss: 0.680046, acc.: 51.56%] [G loss: 1.435969]\n",
      "epoch:14 step:13331 [D loss: 0.695568, acc.: 50.78%] [G loss: 0.822220]\n",
      "epoch:14 step:13332 [D loss: 0.707342, acc.: 43.75%] [G loss: 0.784323]\n",
      "epoch:14 step:13333 [D loss: 0.679269, acc.: 55.47%] [G loss: 0.737323]\n",
      "epoch:14 step:13334 [D loss: 0.681972, acc.: 55.47%] [G loss: 0.795885]\n",
      "epoch:14 step:13335 [D loss: 0.706148, acc.: 48.44%] [G loss: 0.851089]\n",
      "epoch:14 step:13336 [D loss: 0.723462, acc.: 45.31%] [G loss: 0.810847]\n",
      "epoch:14 step:13337 [D loss: 0.641375, acc.: 60.94%] [G loss: 0.787937]\n",
      "epoch:14 step:13338 [D loss: 0.647524, acc.: 57.81%] [G loss: 0.928983]\n",
      "epoch:14 step:13339 [D loss: 0.658920, acc.: 57.03%] [G loss: 0.806088]\n",
      "epoch:14 step:13340 [D loss: 0.638267, acc.: 63.28%] [G loss: 0.865349]\n",
      "epoch:14 step:13341 [D loss: 0.697754, acc.: 48.44%] [G loss: 0.863879]\n",
      "epoch:14 step:13342 [D loss: 0.682085, acc.: 57.81%] [G loss: 0.819233]\n",
      "epoch:14 step:13343 [D loss: 0.656143, acc.: 66.41%] [G loss: 0.781786]\n",
      "epoch:14 step:13344 [D loss: 0.682112, acc.: 57.81%] [G loss: 0.859007]\n",
      "epoch:14 step:13345 [D loss: 0.685514, acc.: 54.69%] [G loss: 0.877332]\n",
      "epoch:14 step:13346 [D loss: 0.681241, acc.: 57.03%] [G loss: 0.842576]\n",
      "epoch:14 step:13347 [D loss: 0.651059, acc.: 59.38%] [G loss: 0.853736]\n",
      "epoch:14 step:13348 [D loss: 0.683558, acc.: 49.22%] [G loss: 0.813952]\n",
      "epoch:14 step:13349 [D loss: 0.685728, acc.: 53.12%] [G loss: 0.814401]\n",
      "epoch:14 step:13350 [D loss: 0.717556, acc.: 46.09%] [G loss: 0.813872]\n",
      "epoch:14 step:13351 [D loss: 0.662103, acc.: 57.03%] [G loss: 0.843550]\n",
      "epoch:14 step:13352 [D loss: 0.714822, acc.: 51.56%] [G loss: 0.799735]\n",
      "epoch:14 step:13353 [D loss: 0.698529, acc.: 53.12%] [G loss: 0.846279]\n",
      "epoch:14 step:13354 [D loss: 0.663633, acc.: 57.03%] [G loss: 0.803377]\n",
      "epoch:14 step:13355 [D loss: 0.653150, acc.: 64.06%] [G loss: 0.848041]\n",
      "epoch:14 step:13356 [D loss: 0.687778, acc.: 55.47%] [G loss: 0.825626]\n",
      "epoch:14 step:13357 [D loss: 0.673605, acc.: 49.22%] [G loss: 0.842067]\n",
      "epoch:14 step:13358 [D loss: 0.655591, acc.: 58.59%] [G loss: 0.817726]\n",
      "epoch:14 step:13359 [D loss: 0.663148, acc.: 57.81%] [G loss: 0.762056]\n",
      "epoch:14 step:13360 [D loss: 0.682893, acc.: 53.12%] [G loss: 0.844304]\n",
      "epoch:14 step:13361 [D loss: 0.681032, acc.: 56.25%] [G loss: 0.787374]\n",
      "epoch:14 step:13362 [D loss: 0.696879, acc.: 49.22%] [G loss: 0.778071]\n",
      "epoch:14 step:13363 [D loss: 0.668162, acc.: 58.59%] [G loss: 0.781756]\n",
      "epoch:14 step:13364 [D loss: 0.692573, acc.: 51.56%] [G loss: 0.774268]\n",
      "epoch:14 step:13365 [D loss: 0.642896, acc.: 68.75%] [G loss: 0.853709]\n",
      "epoch:14 step:13366 [D loss: 0.689883, acc.: 52.34%] [G loss: 0.870068]\n",
      "epoch:14 step:13367 [D loss: 0.676952, acc.: 60.16%] [G loss: 0.858397]\n",
      "epoch:14 step:13368 [D loss: 0.664606, acc.: 57.81%] [G loss: 0.828301]\n",
      "epoch:14 step:13369 [D loss: 0.630767, acc.: 67.19%] [G loss: 0.860087]\n",
      "epoch:14 step:13370 [D loss: 0.622707, acc.: 64.84%] [G loss: 0.847724]\n",
      "epoch:14 step:13371 [D loss: 0.627048, acc.: 68.75%] [G loss: 0.812450]\n",
      "epoch:14 step:13372 [D loss: 0.686529, acc.: 56.25%] [G loss: 0.866532]\n",
      "epoch:14 step:13373 [D loss: 0.636017, acc.: 59.38%] [G loss: 0.844854]\n",
      "epoch:14 step:13374 [D loss: 0.722398, acc.: 42.19%] [G loss: 0.879425]\n",
      "epoch:14 step:13375 [D loss: 0.654795, acc.: 66.41%] [G loss: 0.853717]\n",
      "epoch:14 step:13376 [D loss: 0.662251, acc.: 58.59%] [G loss: 0.867370]\n",
      "epoch:14 step:13377 [D loss: 0.667493, acc.: 63.28%] [G loss: 0.869883]\n",
      "epoch:14 step:13378 [D loss: 0.668683, acc.: 60.94%] [G loss: 0.851363]\n",
      "epoch:14 step:13379 [D loss: 0.677349, acc.: 55.47%] [G loss: 0.872976]\n",
      "epoch:14 step:13380 [D loss: 0.660004, acc.: 60.16%] [G loss: 0.831757]\n",
      "epoch:14 step:13381 [D loss: 0.632777, acc.: 61.72%] [G loss: 0.937935]\n",
      "epoch:14 step:13382 [D loss: 0.689869, acc.: 56.25%] [G loss: 0.781319]\n",
      "epoch:14 step:13383 [D loss: 0.664323, acc.: 57.81%] [G loss: 0.804787]\n",
      "epoch:14 step:13384 [D loss: 0.630769, acc.: 63.28%] [G loss: 0.914596]\n",
      "epoch:14 step:13385 [D loss: 0.697477, acc.: 46.88%] [G loss: 0.925438]\n",
      "epoch:14 step:13386 [D loss: 0.626487, acc.: 62.50%] [G loss: 0.882980]\n",
      "epoch:14 step:13387 [D loss: 0.685606, acc.: 52.34%] [G loss: 0.773176]\n",
      "epoch:14 step:13388 [D loss: 0.621194, acc.: 62.50%] [G loss: 0.838411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13389 [D loss: 0.649339, acc.: 63.28%] [G loss: 0.805248]\n",
      "epoch:14 step:13390 [D loss: 0.606035, acc.: 70.31%] [G loss: 0.821965]\n",
      "epoch:14 step:13391 [D loss: 0.630790, acc.: 67.97%] [G loss: 0.936767]\n",
      "epoch:14 step:13392 [D loss: 0.647956, acc.: 56.25%] [G loss: 0.901696]\n",
      "epoch:14 step:13393 [D loss: 0.624005, acc.: 71.88%] [G loss: 0.937904]\n",
      "epoch:14 step:13394 [D loss: 0.667741, acc.: 51.56%] [G loss: 0.965588]\n",
      "epoch:14 step:13395 [D loss: 0.740455, acc.: 45.31%] [G loss: 0.942236]\n",
      "epoch:14 step:13396 [D loss: 0.631986, acc.: 60.16%] [G loss: 0.919672]\n",
      "epoch:14 step:13397 [D loss: 0.648224, acc.: 61.72%] [G loss: 0.912437]\n",
      "epoch:14 step:13398 [D loss: 0.655141, acc.: 60.16%] [G loss: 0.812732]\n",
      "epoch:14 step:13399 [D loss: 0.637071, acc.: 69.53%] [G loss: 0.908572]\n",
      "epoch:14 step:13400 [D loss: 0.687181, acc.: 51.56%] [G loss: 0.805761]\n",
      "epoch:14 step:13401 [D loss: 0.590207, acc.: 65.62%] [G loss: 0.873972]\n",
      "epoch:14 step:13402 [D loss: 0.661676, acc.: 52.34%] [G loss: 0.838183]\n",
      "epoch:14 step:13403 [D loss: 0.613043, acc.: 70.31%] [G loss: 0.981760]\n",
      "epoch:14 step:13404 [D loss: 0.659405, acc.: 54.69%] [G loss: 0.716185]\n",
      "epoch:14 step:13405 [D loss: 0.748467, acc.: 42.19%] [G loss: 0.743639]\n",
      "epoch:14 step:13406 [D loss: 0.698455, acc.: 53.91%] [G loss: 0.789966]\n",
      "epoch:14 step:13407 [D loss: 0.679634, acc.: 59.38%] [G loss: 0.846169]\n",
      "epoch:14 step:13408 [D loss: 0.623001, acc.: 61.72%] [G loss: 0.918814]\n",
      "epoch:14 step:13409 [D loss: 0.701281, acc.: 60.94%] [G loss: 0.836559]\n",
      "epoch:14 step:13410 [D loss: 0.680046, acc.: 50.78%] [G loss: 0.889313]\n",
      "epoch:14 step:13411 [D loss: 0.644114, acc.: 63.28%] [G loss: 0.858030]\n",
      "epoch:14 step:13412 [D loss: 0.667456, acc.: 51.56%] [G loss: 0.840454]\n",
      "epoch:14 step:13413 [D loss: 0.678224, acc.: 51.56%] [G loss: 0.790357]\n",
      "epoch:14 step:13414 [D loss: 0.656157, acc.: 62.50%] [G loss: 1.005691]\n",
      "epoch:14 step:13415 [D loss: 0.640426, acc.: 63.28%] [G loss: 0.836325]\n",
      "epoch:14 step:13416 [D loss: 0.675811, acc.: 53.12%] [G loss: 0.829495]\n",
      "epoch:14 step:13417 [D loss: 0.621140, acc.: 70.31%] [G loss: 0.899195]\n",
      "epoch:14 step:13418 [D loss: 0.641754, acc.: 57.81%] [G loss: 0.935159]\n",
      "epoch:14 step:13419 [D loss: 0.671499, acc.: 62.50%] [G loss: 0.984611]\n",
      "epoch:14 step:13420 [D loss: 0.636794, acc.: 61.72%] [G loss: 0.766415]\n",
      "epoch:14 step:13421 [D loss: 0.679905, acc.: 57.81%] [G loss: 0.771044]\n",
      "epoch:14 step:13422 [D loss: 0.754525, acc.: 42.19%] [G loss: 0.901730]\n",
      "epoch:14 step:13423 [D loss: 0.659061, acc.: 64.84%] [G loss: 0.887315]\n",
      "epoch:14 step:13424 [D loss: 0.676696, acc.: 54.69%] [G loss: 0.752679]\n",
      "epoch:14 step:13425 [D loss: 0.632298, acc.: 67.19%] [G loss: 0.868986]\n",
      "epoch:14 step:13426 [D loss: 0.693561, acc.: 50.78%] [G loss: 0.833192]\n",
      "epoch:14 step:13427 [D loss: 0.680791, acc.: 60.16%] [G loss: 0.777529]\n",
      "epoch:14 step:13428 [D loss: 0.706972, acc.: 50.78%] [G loss: 0.811757]\n",
      "epoch:14 step:13429 [D loss: 0.645563, acc.: 64.06%] [G loss: 0.855775]\n",
      "epoch:14 step:13430 [D loss: 0.591606, acc.: 68.75%] [G loss: 0.964022]\n",
      "epoch:14 step:13431 [D loss: 0.701877, acc.: 51.56%] [G loss: 0.777026]\n",
      "epoch:14 step:13432 [D loss: 0.682155, acc.: 54.69%] [G loss: 0.799817]\n",
      "epoch:14 step:13433 [D loss: 0.641381, acc.: 72.66%] [G loss: 0.879204]\n",
      "epoch:14 step:13434 [D loss: 0.693746, acc.: 48.44%] [G loss: 0.775717]\n",
      "epoch:14 step:13435 [D loss: 0.669466, acc.: 55.47%] [G loss: 0.754406]\n",
      "epoch:14 step:13436 [D loss: 0.679449, acc.: 53.91%] [G loss: 0.791978]\n",
      "epoch:14 step:13437 [D loss: 0.690105, acc.: 47.66%] [G loss: 0.927076]\n",
      "epoch:14 step:13438 [D loss: 0.639149, acc.: 69.53%] [G loss: 0.797915]\n",
      "epoch:14 step:13439 [D loss: 0.640890, acc.: 64.06%] [G loss: 0.779023]\n",
      "epoch:14 step:13440 [D loss: 0.699375, acc.: 53.91%] [G loss: 0.786355]\n",
      "epoch:14 step:13441 [D loss: 0.631915, acc.: 65.62%] [G loss: 0.840856]\n",
      "epoch:14 step:13442 [D loss: 0.609204, acc.: 67.19%] [G loss: 0.825855]\n",
      "epoch:14 step:13443 [D loss: 0.705811, acc.: 49.22%] [G loss: 0.897145]\n",
      "epoch:14 step:13444 [D loss: 0.733948, acc.: 50.00%] [G loss: 0.962335]\n",
      "epoch:14 step:13445 [D loss: 0.612105, acc.: 67.97%] [G loss: 0.942865]\n",
      "epoch:14 step:13446 [D loss: 0.621963, acc.: 67.19%] [G loss: 0.943007]\n",
      "epoch:14 step:13447 [D loss: 0.677591, acc.: 55.47%] [G loss: 0.917921]\n",
      "epoch:14 step:13448 [D loss: 0.683119, acc.: 52.34%] [G loss: 0.845626]\n",
      "epoch:14 step:13449 [D loss: 0.655397, acc.: 63.28%] [G loss: 0.808036]\n",
      "epoch:14 step:13450 [D loss: 0.666979, acc.: 57.81%] [G loss: 0.875899]\n",
      "epoch:14 step:13451 [D loss: 0.641689, acc.: 63.28%] [G loss: 0.889099]\n",
      "epoch:14 step:13452 [D loss: 0.653542, acc.: 58.59%] [G loss: 0.823869]\n",
      "epoch:14 step:13453 [D loss: 0.609360, acc.: 68.75%] [G loss: 0.900235]\n",
      "epoch:14 step:13454 [D loss: 0.679974, acc.: 55.47%] [G loss: 0.892960]\n",
      "epoch:14 step:13455 [D loss: 0.633122, acc.: 60.16%] [G loss: 0.873157]\n",
      "epoch:14 step:13456 [D loss: 0.608994, acc.: 74.22%] [G loss: 0.903448]\n",
      "epoch:14 step:13457 [D loss: 0.636878, acc.: 60.16%] [G loss: 0.834191]\n",
      "epoch:14 step:13458 [D loss: 0.707103, acc.: 50.78%] [G loss: 0.924227]\n",
      "epoch:14 step:13459 [D loss: 0.681866, acc.: 57.03%] [G loss: 0.799150]\n",
      "epoch:14 step:13460 [D loss: 0.741121, acc.: 44.53%] [G loss: 0.861404]\n",
      "epoch:14 step:13461 [D loss: 0.645324, acc.: 62.50%] [G loss: 0.926282]\n",
      "epoch:14 step:13462 [D loss: 0.648865, acc.: 59.38%] [G loss: 0.948200]\n",
      "epoch:14 step:13463 [D loss: 0.643358, acc.: 66.41%] [G loss: 0.879193]\n",
      "epoch:14 step:13464 [D loss: 0.621270, acc.: 61.72%] [G loss: 0.829316]\n",
      "epoch:14 step:13465 [D loss: 0.660003, acc.: 62.50%] [G loss: 0.811981]\n",
      "epoch:14 step:13466 [D loss: 0.652345, acc.: 58.59%] [G loss: 0.769717]\n",
      "epoch:14 step:13467 [D loss: 0.692923, acc.: 50.00%] [G loss: 0.775808]\n",
      "epoch:14 step:13468 [D loss: 0.710426, acc.: 53.91%] [G loss: 0.850842]\n",
      "epoch:14 step:13469 [D loss: 0.680388, acc.: 59.38%] [G loss: 0.892036]\n",
      "epoch:14 step:13470 [D loss: 0.647265, acc.: 63.28%] [G loss: 0.763372]\n",
      "epoch:14 step:13471 [D loss: 0.588495, acc.: 66.41%] [G loss: 0.936996]\n",
      "epoch:14 step:13472 [D loss: 0.774997, acc.: 39.84%] [G loss: 0.855693]\n",
      "epoch:14 step:13473 [D loss: 0.633635, acc.: 64.06%] [G loss: 0.965073]\n",
      "epoch:14 step:13474 [D loss: 0.747337, acc.: 39.84%] [G loss: 0.856396]\n",
      "epoch:14 step:13475 [D loss: 0.654391, acc.: 67.19%] [G loss: 0.782935]\n",
      "epoch:14 step:13476 [D loss: 0.635553, acc.: 60.16%] [G loss: 0.786629]\n",
      "epoch:14 step:13477 [D loss: 0.705288, acc.: 50.78%] [G loss: 0.827396]\n",
      "epoch:14 step:13478 [D loss: 0.686189, acc.: 52.34%] [G loss: 0.827192]\n",
      "epoch:14 step:13479 [D loss: 0.664316, acc.: 53.12%] [G loss: 0.809758]\n",
      "epoch:14 step:13480 [D loss: 0.712419, acc.: 57.81%] [G loss: 0.831231]\n",
      "epoch:14 step:13481 [D loss: 0.623967, acc.: 58.59%] [G loss: 0.883416]\n",
      "epoch:14 step:13482 [D loss: 0.691689, acc.: 55.47%] [G loss: 0.985326]\n",
      "epoch:14 step:13483 [D loss: 0.636613, acc.: 62.50%] [G loss: 0.837191]\n",
      "epoch:14 step:13484 [D loss: 0.674393, acc.: 62.50%] [G loss: 0.870185]\n",
      "epoch:14 step:13485 [D loss: 0.696275, acc.: 57.81%] [G loss: 0.816246]\n",
      "epoch:14 step:13486 [D loss: 0.750096, acc.: 35.94%] [G loss: 0.815866]\n",
      "epoch:14 step:13487 [D loss: 0.703717, acc.: 54.69%] [G loss: 0.830473]\n",
      "epoch:14 step:13488 [D loss: 0.678701, acc.: 54.69%] [G loss: 0.817496]\n",
      "epoch:14 step:13489 [D loss: 0.655111, acc.: 61.72%] [G loss: 0.774111]\n",
      "epoch:14 step:13490 [D loss: 0.607721, acc.: 69.53%] [G loss: 0.778388]\n",
      "epoch:14 step:13491 [D loss: 0.732076, acc.: 42.19%] [G loss: 0.754943]\n",
      "epoch:14 step:13492 [D loss: 0.643201, acc.: 61.72%] [G loss: 0.875467]\n",
      "epoch:14 step:13493 [D loss: 0.648733, acc.: 60.16%] [G loss: 0.870315]\n",
      "epoch:14 step:13494 [D loss: 0.650566, acc.: 64.84%] [G loss: 0.844818]\n",
      "epoch:14 step:13495 [D loss: 0.673359, acc.: 56.25%] [G loss: 0.808349]\n",
      "epoch:14 step:13496 [D loss: 0.682425, acc.: 59.38%] [G loss: 0.809390]\n",
      "epoch:14 step:13497 [D loss: 0.685056, acc.: 54.69%] [G loss: 0.763333]\n",
      "epoch:14 step:13498 [D loss: 0.660642, acc.: 60.16%] [G loss: 0.848633]\n",
      "epoch:14 step:13499 [D loss: 0.661037, acc.: 62.50%] [G loss: 0.745326]\n",
      "epoch:14 step:13500 [D loss: 0.593050, acc.: 69.53%] [G loss: 0.928514]\n",
      "epoch:14 step:13501 [D loss: 0.639791, acc.: 61.72%] [G loss: 0.866579]\n",
      "epoch:14 step:13502 [D loss: 0.745935, acc.: 39.06%] [G loss: 0.627172]\n",
      "epoch:14 step:13503 [D loss: 0.677059, acc.: 50.00%] [G loss: 0.777547]\n",
      "epoch:14 step:13504 [D loss: 0.743155, acc.: 43.75%] [G loss: 0.757238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13505 [D loss: 0.659128, acc.: 60.94%] [G loss: 0.748542]\n",
      "epoch:14 step:13506 [D loss: 0.696796, acc.: 46.88%] [G loss: 0.755293]\n",
      "epoch:14 step:13507 [D loss: 0.790943, acc.: 30.47%] [G loss: 0.795298]\n",
      "epoch:14 step:13508 [D loss: 0.688470, acc.: 51.56%] [G loss: 0.853567]\n",
      "epoch:14 step:13509 [D loss: 0.711576, acc.: 46.88%] [G loss: 0.854089]\n",
      "epoch:14 step:13510 [D loss: 0.717044, acc.: 56.25%] [G loss: 0.856280]\n",
      "epoch:14 step:13511 [D loss: 0.651213, acc.: 54.69%] [G loss: 0.922482]\n",
      "epoch:14 step:13512 [D loss: 0.692240, acc.: 49.22%] [G loss: 0.918562]\n",
      "epoch:14 step:13513 [D loss: 0.699053, acc.: 50.00%] [G loss: 0.919282]\n",
      "epoch:14 step:13514 [D loss: 0.647545, acc.: 59.38%] [G loss: 0.879375]\n",
      "epoch:14 step:13515 [D loss: 0.646543, acc.: 60.16%] [G loss: 0.824860]\n",
      "epoch:14 step:13516 [D loss: 0.702721, acc.: 50.78%] [G loss: 0.811225]\n",
      "epoch:14 step:13517 [D loss: 0.678316, acc.: 57.03%] [G loss: 0.821867]\n",
      "epoch:14 step:13518 [D loss: 0.668192, acc.: 50.78%] [G loss: 0.789397]\n",
      "epoch:14 step:13519 [D loss: 0.665334, acc.: 57.03%] [G loss: 0.778471]\n",
      "epoch:14 step:13520 [D loss: 0.637558, acc.: 64.06%] [G loss: 0.913171]\n",
      "epoch:14 step:13521 [D loss: 0.603823, acc.: 72.66%] [G loss: 0.863468]\n",
      "epoch:14 step:13522 [D loss: 0.673225, acc.: 62.50%] [G loss: 0.869454]\n",
      "epoch:14 step:13523 [D loss: 0.677463, acc.: 59.38%] [G loss: 0.804157]\n",
      "epoch:14 step:13524 [D loss: 0.662916, acc.: 57.81%] [G loss: 0.839017]\n",
      "epoch:14 step:13525 [D loss: 0.705581, acc.: 44.53%] [G loss: 0.782592]\n",
      "epoch:14 step:13526 [D loss: 0.660589, acc.: 59.38%] [G loss: 0.779594]\n",
      "epoch:14 step:13527 [D loss: 0.675326, acc.: 54.69%] [G loss: 0.815875]\n",
      "epoch:14 step:13528 [D loss: 0.637161, acc.: 62.50%] [G loss: 0.736478]\n",
      "epoch:14 step:13529 [D loss: 0.725133, acc.: 49.22%] [G loss: 0.734552]\n",
      "epoch:14 step:13530 [D loss: 0.663598, acc.: 60.94%] [G loss: 0.856771]\n",
      "epoch:14 step:13531 [D loss: 0.676300, acc.: 57.03%] [G loss: 0.850052]\n",
      "epoch:14 step:13532 [D loss: 0.662109, acc.: 58.59%] [G loss: 0.771408]\n",
      "epoch:14 step:13533 [D loss: 0.649335, acc.: 65.62%] [G loss: 0.818406]\n",
      "epoch:14 step:13534 [D loss: 0.704298, acc.: 53.91%] [G loss: 0.843480]\n",
      "epoch:14 step:13535 [D loss: 0.689021, acc.: 51.56%] [G loss: 0.776071]\n",
      "epoch:14 step:13536 [D loss: 0.637422, acc.: 64.84%] [G loss: 0.852046]\n",
      "epoch:14 step:13537 [D loss: 0.709599, acc.: 51.56%] [G loss: 0.792192]\n",
      "epoch:14 step:13538 [D loss: 0.662728, acc.: 60.16%] [G loss: 0.783593]\n",
      "epoch:14 step:13539 [D loss: 0.664269, acc.: 54.69%] [G loss: 0.819267]\n",
      "epoch:14 step:13540 [D loss: 0.690720, acc.: 53.12%] [G loss: 0.807902]\n",
      "epoch:14 step:13541 [D loss: 0.696515, acc.: 52.34%] [G loss: 0.760043]\n",
      "epoch:14 step:13542 [D loss: 0.668090, acc.: 62.50%] [G loss: 0.800820]\n",
      "epoch:14 step:13543 [D loss: 0.659482, acc.: 53.12%] [G loss: 0.815044]\n",
      "epoch:14 step:13544 [D loss: 0.636301, acc.: 62.50%] [G loss: 0.811077]\n",
      "epoch:14 step:13545 [D loss: 0.689288, acc.: 56.25%] [G loss: 0.815098]\n",
      "epoch:14 step:13546 [D loss: 0.653884, acc.: 63.28%] [G loss: 0.856504]\n",
      "epoch:14 step:13547 [D loss: 0.709030, acc.: 54.69%] [G loss: 0.807235]\n",
      "epoch:14 step:13548 [D loss: 0.671836, acc.: 60.16%] [G loss: 0.903558]\n",
      "epoch:14 step:13549 [D loss: 0.647444, acc.: 55.47%] [G loss: 0.857194]\n",
      "epoch:14 step:13550 [D loss: 0.674692, acc.: 58.59%] [G loss: 0.904936]\n",
      "epoch:14 step:13551 [D loss: 0.661810, acc.: 63.28%] [G loss: 0.844283]\n",
      "epoch:14 step:13552 [D loss: 0.636124, acc.: 66.41%] [G loss: 0.787575]\n",
      "epoch:14 step:13553 [D loss: 0.662565, acc.: 60.16%] [G loss: 0.880081]\n",
      "epoch:14 step:13554 [D loss: 0.667733, acc.: 56.25%] [G loss: 0.838936]\n",
      "epoch:14 step:13555 [D loss: 0.650928, acc.: 60.16%] [G loss: 0.788037]\n",
      "epoch:14 step:13556 [D loss: 0.584238, acc.: 74.22%] [G loss: 0.860203]\n",
      "epoch:14 step:13557 [D loss: 0.699610, acc.: 51.56%] [G loss: 0.838340]\n",
      "epoch:14 step:13558 [D loss: 0.700897, acc.: 49.22%] [G loss: 0.906137]\n",
      "epoch:14 step:13559 [D loss: 0.680951, acc.: 50.78%] [G loss: 0.844902]\n",
      "epoch:14 step:13560 [D loss: 0.614805, acc.: 64.06%] [G loss: 0.800017]\n",
      "epoch:14 step:13561 [D loss: 0.664617, acc.: 62.50%] [G loss: 0.842481]\n",
      "epoch:14 step:13562 [D loss: 0.659276, acc.: 59.38%] [G loss: 0.896758]\n",
      "epoch:14 step:13563 [D loss: 0.726825, acc.: 50.00%] [G loss: 0.867445]\n",
      "epoch:14 step:13564 [D loss: 0.678821, acc.: 57.03%] [G loss: 0.937646]\n",
      "epoch:14 step:13565 [D loss: 0.720096, acc.: 42.19%] [G loss: 1.017927]\n",
      "epoch:14 step:13566 [D loss: 0.644123, acc.: 57.81%] [G loss: 0.828412]\n",
      "epoch:14 step:13567 [D loss: 0.646486, acc.: 67.19%] [G loss: 0.834259]\n",
      "epoch:14 step:13568 [D loss: 0.700240, acc.: 54.69%] [G loss: 0.802632]\n",
      "epoch:14 step:13569 [D loss: 0.626451, acc.: 68.75%] [G loss: 0.786501]\n",
      "epoch:14 step:13570 [D loss: 0.700566, acc.: 51.56%] [G loss: 0.737902]\n",
      "epoch:14 step:13571 [D loss: 0.650898, acc.: 65.62%] [G loss: 0.746659]\n",
      "epoch:14 step:13572 [D loss: 0.673511, acc.: 62.50%] [G loss: 0.886934]\n",
      "epoch:14 step:13573 [D loss: 0.681556, acc.: 54.69%] [G loss: 0.792070]\n",
      "epoch:14 step:13574 [D loss: 0.636364, acc.: 64.84%] [G loss: 0.855304]\n",
      "epoch:14 step:13575 [D loss: 0.733985, acc.: 39.06%] [G loss: 0.694399]\n",
      "epoch:14 step:13576 [D loss: 0.599142, acc.: 70.31%] [G loss: 1.416235]\n",
      "epoch:14 step:13577 [D loss: 0.679983, acc.: 48.44%] [G loss: 0.922940]\n",
      "epoch:14 step:13578 [D loss: 0.661824, acc.: 59.38%] [G loss: 0.773698]\n",
      "epoch:14 step:13579 [D loss: 0.691254, acc.: 54.69%] [G loss: 0.944524]\n",
      "epoch:14 step:13580 [D loss: 0.679396, acc.: 55.47%] [G loss: 0.865357]\n",
      "epoch:14 step:13581 [D loss: 0.670500, acc.: 54.69%] [G loss: 0.809535]\n",
      "epoch:14 step:13582 [D loss: 0.645489, acc.: 57.81%] [G loss: 0.877760]\n",
      "epoch:14 step:13583 [D loss: 0.642165, acc.: 63.28%] [G loss: 0.924104]\n",
      "epoch:14 step:13584 [D loss: 0.627780, acc.: 64.06%] [G loss: 0.983231]\n",
      "epoch:14 step:13585 [D loss: 0.699085, acc.: 42.97%] [G loss: 0.879296]\n",
      "epoch:14 step:13586 [D loss: 0.687400, acc.: 52.34%] [G loss: 0.865963]\n",
      "epoch:14 step:13587 [D loss: 0.657706, acc.: 58.59%] [G loss: 0.879756]\n",
      "epoch:14 step:13588 [D loss: 0.736220, acc.: 42.97%] [G loss: 0.929573]\n",
      "epoch:14 step:13589 [D loss: 0.685865, acc.: 53.12%] [G loss: 0.838715]\n",
      "epoch:14 step:13590 [D loss: 0.672991, acc.: 50.00%] [G loss: 0.951337]\n",
      "epoch:14 step:13591 [D loss: 0.656757, acc.: 60.16%] [G loss: 0.938799]\n",
      "epoch:14 step:13592 [D loss: 0.632160, acc.: 70.31%] [G loss: 0.817495]\n",
      "epoch:14 step:13593 [D loss: 0.638863, acc.: 64.84%] [G loss: 0.956715]\n",
      "epoch:14 step:13594 [D loss: 0.610357, acc.: 67.19%] [G loss: 0.892325]\n",
      "epoch:14 step:13595 [D loss: 0.618067, acc.: 71.09%] [G loss: 0.748410]\n",
      "epoch:14 step:13596 [D loss: 0.711601, acc.: 50.78%] [G loss: 0.798184]\n",
      "epoch:14 step:13597 [D loss: 0.733986, acc.: 49.22%] [G loss: 0.913850]\n",
      "epoch:14 step:13598 [D loss: 0.696857, acc.: 54.69%] [G loss: 1.083371]\n",
      "epoch:14 step:13599 [D loss: 0.665448, acc.: 57.81%] [G loss: 0.895521]\n",
      "epoch:14 step:13600 [D loss: 0.591316, acc.: 79.69%] [G loss: 0.907616]\n",
      "epoch:14 step:13601 [D loss: 0.642707, acc.: 60.94%] [G loss: 0.972448]\n",
      "epoch:14 step:13602 [D loss: 0.611232, acc.: 68.75%] [G loss: 0.850178]\n",
      "epoch:14 step:13603 [D loss: 0.633131, acc.: 71.88%] [G loss: 0.888285]\n",
      "epoch:14 step:13604 [D loss: 0.670565, acc.: 53.12%] [G loss: 0.810239]\n",
      "epoch:14 step:13605 [D loss: 0.671365, acc.: 53.91%] [G loss: 0.779554]\n",
      "epoch:14 step:13606 [D loss: 0.639109, acc.: 64.84%] [G loss: 0.832217]\n",
      "epoch:14 step:13607 [D loss: 0.641347, acc.: 60.16%] [G loss: 0.805236]\n",
      "epoch:14 step:13608 [D loss: 0.664266, acc.: 65.62%] [G loss: 0.916241]\n",
      "epoch:14 step:13609 [D loss: 0.736673, acc.: 39.84%] [G loss: 0.920127]\n",
      "epoch:14 step:13610 [D loss: 0.688692, acc.: 55.47%] [G loss: 0.858550]\n",
      "epoch:14 step:13611 [D loss: 0.687250, acc.: 50.78%] [G loss: 0.830129]\n",
      "epoch:14 step:13612 [D loss: 0.603409, acc.: 64.84%] [G loss: 0.856485]\n",
      "epoch:14 step:13613 [D loss: 0.681083, acc.: 59.38%] [G loss: 0.909881]\n",
      "epoch:14 step:13614 [D loss: 0.635650, acc.: 62.50%] [G loss: 0.839102]\n",
      "epoch:14 step:13615 [D loss: 0.673359, acc.: 54.69%] [G loss: 0.844747]\n",
      "epoch:14 step:13616 [D loss: 0.676402, acc.: 65.62%] [G loss: 0.928527]\n",
      "epoch:14 step:13617 [D loss: 0.665675, acc.: 57.81%] [G loss: 0.836645]\n",
      "epoch:14 step:13618 [D loss: 0.683480, acc.: 51.56%] [G loss: 1.048282]\n",
      "epoch:14 step:13619 [D loss: 0.710189, acc.: 50.00%] [G loss: 0.810047]\n",
      "epoch:14 step:13620 [D loss: 0.649074, acc.: 64.06%] [G loss: 0.957835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13621 [D loss: 0.704098, acc.: 50.00%] [G loss: 0.799580]\n",
      "epoch:14 step:13622 [D loss: 0.641619, acc.: 60.94%] [G loss: 0.826145]\n",
      "epoch:14 step:13623 [D loss: 0.683464, acc.: 58.59%] [G loss: 0.883221]\n",
      "epoch:14 step:13624 [D loss: 0.678336, acc.: 60.94%] [G loss: 0.837804]\n",
      "epoch:14 step:13625 [D loss: 0.675758, acc.: 53.12%] [G loss: 0.864512]\n",
      "epoch:14 step:13626 [D loss: 0.581501, acc.: 69.53%] [G loss: 0.808889]\n",
      "epoch:14 step:13627 [D loss: 0.628566, acc.: 60.16%] [G loss: 0.889968]\n",
      "epoch:14 step:13628 [D loss: 0.638518, acc.: 67.19%] [G loss: 0.842526]\n",
      "epoch:14 step:13629 [D loss: 0.651258, acc.: 58.59%] [G loss: 0.812857]\n",
      "epoch:14 step:13630 [D loss: 0.657776, acc.: 63.28%] [G loss: 0.959073]\n",
      "epoch:14 step:13631 [D loss: 0.728843, acc.: 49.22%] [G loss: 0.820630]\n",
      "epoch:14 step:13632 [D loss: 0.721923, acc.: 53.91%] [G loss: 0.882185]\n",
      "epoch:14 step:13633 [D loss: 0.670845, acc.: 60.94%] [G loss: 0.940421]\n",
      "epoch:14 step:13634 [D loss: 0.691356, acc.: 56.25%] [G loss: 0.747139]\n",
      "epoch:14 step:13635 [D loss: 0.595728, acc.: 67.97%] [G loss: 0.850062]\n",
      "epoch:14 step:13636 [D loss: 0.717611, acc.: 50.00%] [G loss: 0.926730]\n",
      "epoch:14 step:13637 [D loss: 0.644478, acc.: 62.50%] [G loss: 0.885042]\n",
      "epoch:14 step:13638 [D loss: 0.675830, acc.: 52.34%] [G loss: 0.816172]\n",
      "epoch:14 step:13639 [D loss: 0.642822, acc.: 56.25%] [G loss: 0.827425]\n",
      "epoch:14 step:13640 [D loss: 0.721926, acc.: 56.25%] [G loss: 0.832060]\n",
      "epoch:14 step:13641 [D loss: 0.600413, acc.: 74.22%] [G loss: 0.970441]\n",
      "epoch:14 step:13642 [D loss: 0.610808, acc.: 71.09%] [G loss: 0.944486]\n",
      "epoch:14 step:13643 [D loss: 0.663945, acc.: 65.62%] [G loss: 0.885376]\n",
      "epoch:14 step:13644 [D loss: 0.667689, acc.: 55.47%] [G loss: 0.825577]\n",
      "epoch:14 step:13645 [D loss: 0.749178, acc.: 42.97%] [G loss: 0.899639]\n",
      "epoch:14 step:13646 [D loss: 0.677068, acc.: 55.47%] [G loss: 0.686889]\n",
      "epoch:14 step:13647 [D loss: 0.707858, acc.: 46.09%] [G loss: 0.843949]\n",
      "epoch:14 step:13648 [D loss: 0.709468, acc.: 50.78%] [G loss: 0.796584]\n",
      "epoch:14 step:13649 [D loss: 0.641393, acc.: 60.16%] [G loss: 0.766683]\n",
      "epoch:14 step:13650 [D loss: 0.703665, acc.: 57.03%] [G loss: 0.740828]\n",
      "epoch:14 step:13651 [D loss: 0.658631, acc.: 55.47%] [G loss: 0.839092]\n",
      "epoch:14 step:13652 [D loss: 0.638576, acc.: 67.97%] [G loss: 0.835812]\n",
      "epoch:14 step:13653 [D loss: 0.648469, acc.: 63.28%] [G loss: 0.811148]\n",
      "epoch:14 step:13654 [D loss: 0.667029, acc.: 60.16%] [G loss: 0.811407]\n",
      "epoch:14 step:13655 [D loss: 0.694775, acc.: 54.69%] [G loss: 0.768512]\n",
      "epoch:14 step:13656 [D loss: 0.630555, acc.: 71.09%] [G loss: 0.912041]\n",
      "epoch:14 step:13657 [D loss: 0.639343, acc.: 65.62%] [G loss: 0.894657]\n",
      "epoch:14 step:13658 [D loss: 0.675443, acc.: 53.91%] [G loss: 0.765920]\n",
      "epoch:14 step:13659 [D loss: 0.652300, acc.: 63.28%] [G loss: 0.947519]\n",
      "epoch:14 step:13660 [D loss: 0.711435, acc.: 52.34%] [G loss: 0.769638]\n",
      "epoch:14 step:13661 [D loss: 0.670022, acc.: 61.72%] [G loss: 0.957989]\n",
      "epoch:14 step:13662 [D loss: 0.649731, acc.: 61.72%] [G loss: 0.811074]\n",
      "epoch:14 step:13663 [D loss: 0.642941, acc.: 67.97%] [G loss: 0.899277]\n",
      "epoch:14 step:13664 [D loss: 0.633537, acc.: 69.53%] [G loss: 0.804417]\n",
      "epoch:14 step:13665 [D loss: 0.693827, acc.: 49.22%] [G loss: 0.826310]\n",
      "epoch:14 step:13666 [D loss: 0.728845, acc.: 40.62%] [G loss: 1.010627]\n",
      "epoch:14 step:13667 [D loss: 0.688198, acc.: 57.81%] [G loss: 0.759388]\n",
      "epoch:14 step:13668 [D loss: 0.695869, acc.: 56.25%] [G loss: 0.861738]\n",
      "epoch:14 step:13669 [D loss: 0.730470, acc.: 39.84%] [G loss: 0.828472]\n",
      "epoch:14 step:13670 [D loss: 0.642084, acc.: 61.72%] [G loss: 0.878371]\n",
      "epoch:14 step:13671 [D loss: 0.682871, acc.: 62.50%] [G loss: 0.899016]\n",
      "epoch:14 step:13672 [D loss: 0.715727, acc.: 50.00%] [G loss: 0.830271]\n",
      "epoch:14 step:13673 [D loss: 0.575472, acc.: 73.44%] [G loss: 0.809245]\n",
      "epoch:14 step:13674 [D loss: 0.720095, acc.: 48.44%] [G loss: 0.832471]\n",
      "epoch:14 step:13675 [D loss: 0.567108, acc.: 76.56%] [G loss: 0.935131]\n",
      "epoch:14 step:13676 [D loss: 0.657945, acc.: 60.94%] [G loss: 0.751956]\n",
      "epoch:14 step:13677 [D loss: 0.714224, acc.: 47.66%] [G loss: 0.872804]\n",
      "epoch:14 step:13678 [D loss: 0.724357, acc.: 47.66%] [G loss: 0.884175]\n",
      "epoch:14 step:13679 [D loss: 0.712051, acc.: 49.22%] [G loss: 0.773409]\n",
      "epoch:14 step:13680 [D loss: 0.708289, acc.: 50.00%] [G loss: 0.723489]\n",
      "epoch:14 step:13681 [D loss: 0.669553, acc.: 51.56%] [G loss: 0.811465]\n",
      "epoch:14 step:13682 [D loss: 0.668355, acc.: 58.59%] [G loss: 0.723519]\n",
      "epoch:14 step:13683 [D loss: 0.695548, acc.: 58.59%] [G loss: 0.797797]\n",
      "epoch:14 step:13684 [D loss: 0.618321, acc.: 64.06%] [G loss: 0.937635]\n",
      "epoch:14 step:13685 [D loss: 0.679682, acc.: 59.38%] [G loss: 0.848672]\n",
      "epoch:14 step:13686 [D loss: 0.633126, acc.: 67.97%] [G loss: 0.798760]\n",
      "epoch:14 step:13687 [D loss: 0.610189, acc.: 69.53%] [G loss: 0.760164]\n",
      "epoch:14 step:13688 [D loss: 0.690982, acc.: 54.69%] [G loss: 0.791653]\n",
      "epoch:14 step:13689 [D loss: 0.622867, acc.: 64.06%] [G loss: 0.790048]\n",
      "epoch:14 step:13690 [D loss: 0.616729, acc.: 67.97%] [G loss: 0.866346]\n",
      "epoch:14 step:13691 [D loss: 0.647765, acc.: 58.59%] [G loss: 0.936246]\n",
      "epoch:14 step:13692 [D loss: 0.756180, acc.: 46.88%] [G loss: 0.845098]\n",
      "epoch:14 step:13693 [D loss: 0.715178, acc.: 50.00%] [G loss: 0.769387]\n",
      "epoch:14 step:13694 [D loss: 0.607259, acc.: 73.44%] [G loss: 0.787278]\n",
      "epoch:14 step:13695 [D loss: 0.696684, acc.: 51.56%] [G loss: 0.783364]\n",
      "epoch:14 step:13696 [D loss: 0.673768, acc.: 57.03%] [G loss: 0.861177]\n",
      "epoch:14 step:13697 [D loss: 0.706169, acc.: 57.03%] [G loss: 0.906517]\n",
      "epoch:14 step:13698 [D loss: 0.660003, acc.: 64.84%] [G loss: 0.912591]\n",
      "epoch:14 step:13699 [D loss: 0.673184, acc.: 54.69%] [G loss: 0.894266]\n",
      "epoch:14 step:13700 [D loss: 0.702984, acc.: 53.91%] [G loss: 0.936191]\n",
      "epoch:14 step:13701 [D loss: 0.661063, acc.: 67.97%] [G loss: 0.894824]\n",
      "epoch:14 step:13702 [D loss: 0.690540, acc.: 51.56%] [G loss: 0.926485]\n",
      "epoch:14 step:13703 [D loss: 0.689132, acc.: 55.47%] [G loss: 0.820160]\n",
      "epoch:14 step:13704 [D loss: 0.662661, acc.: 61.72%] [G loss: 0.847088]\n",
      "epoch:14 step:13705 [D loss: 0.730991, acc.: 52.34%] [G loss: 0.821435]\n",
      "epoch:14 step:13706 [D loss: 0.692614, acc.: 57.81%] [G loss: 0.812034]\n",
      "epoch:14 step:13707 [D loss: 0.690339, acc.: 58.59%] [G loss: 0.866998]\n",
      "epoch:14 step:13708 [D loss: 0.691791, acc.: 54.69%] [G loss: 0.895022]\n",
      "epoch:14 step:13709 [D loss: 0.690332, acc.: 47.66%] [G loss: 0.844394]\n",
      "epoch:14 step:13710 [D loss: 0.602883, acc.: 63.28%] [G loss: 0.773466]\n",
      "epoch:14 step:13711 [D loss: 0.677513, acc.: 53.12%] [G loss: 0.764832]\n",
      "epoch:14 step:13712 [D loss: 0.600947, acc.: 71.88%] [G loss: 0.738617]\n",
      "epoch:14 step:13713 [D loss: 0.684779, acc.: 53.91%] [G loss: 0.754894]\n",
      "epoch:14 step:13714 [D loss: 0.687896, acc.: 53.12%] [G loss: 0.887182]\n",
      "epoch:14 step:13715 [D loss: 0.676333, acc.: 55.47%] [G loss: 0.772244]\n",
      "epoch:14 step:13716 [D loss: 0.712165, acc.: 44.53%] [G loss: 0.724941]\n",
      "epoch:14 step:13717 [D loss: 0.643960, acc.: 55.47%] [G loss: 0.805218]\n",
      "epoch:14 step:13718 [D loss: 0.748258, acc.: 50.78%] [G loss: 0.852819]\n",
      "epoch:14 step:13719 [D loss: 0.653976, acc.: 61.72%] [G loss: 0.883253]\n",
      "epoch:14 step:13720 [D loss: 0.662462, acc.: 57.81%] [G loss: 0.830912]\n",
      "epoch:14 step:13721 [D loss: 0.681731, acc.: 57.81%] [G loss: 0.827523]\n",
      "epoch:14 step:13722 [D loss: 0.638354, acc.: 64.06%] [G loss: 0.888338]\n",
      "epoch:14 step:13723 [D loss: 0.632799, acc.: 62.50%] [G loss: 0.891549]\n",
      "epoch:14 step:13724 [D loss: 0.683999, acc.: 53.12%] [G loss: 0.844229]\n",
      "epoch:14 step:13725 [D loss: 0.676600, acc.: 54.69%] [G loss: 0.828873]\n",
      "epoch:14 step:13726 [D loss: 0.688114, acc.: 49.22%] [G loss: 0.890300]\n",
      "epoch:14 step:13727 [D loss: 0.694557, acc.: 47.66%] [G loss: 0.941754]\n",
      "epoch:14 step:13728 [D loss: 0.691277, acc.: 50.00%] [G loss: 0.792514]\n",
      "epoch:14 step:13729 [D loss: 0.631226, acc.: 64.06%] [G loss: 0.873024]\n",
      "epoch:14 step:13730 [D loss: 0.711382, acc.: 47.66%] [G loss: 0.821262]\n",
      "epoch:14 step:13731 [D loss: 0.749306, acc.: 38.28%] [G loss: 0.805341]\n",
      "epoch:14 step:13732 [D loss: 0.681559, acc.: 58.59%] [G loss: 0.862867]\n",
      "epoch:14 step:13733 [D loss: 0.638300, acc.: 64.06%] [G loss: 0.779945]\n",
      "epoch:14 step:13734 [D loss: 0.694683, acc.: 52.34%] [G loss: 0.710770]\n",
      "epoch:14 step:13735 [D loss: 0.733050, acc.: 38.28%] [G loss: 0.758644]\n",
      "epoch:14 step:13736 [D loss: 0.616827, acc.: 68.75%] [G loss: 0.881640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13737 [D loss: 0.653103, acc.: 57.81%] [G loss: 0.886255]\n",
      "epoch:14 step:13738 [D loss: 0.626851, acc.: 68.75%] [G loss: 0.860085]\n",
      "epoch:14 step:13739 [D loss: 0.662864, acc.: 57.03%] [G loss: 0.858870]\n",
      "epoch:14 step:13740 [D loss: 0.686477, acc.: 51.56%] [G loss: 0.825404]\n",
      "epoch:14 step:13741 [D loss: 0.667124, acc.: 56.25%] [G loss: 0.908063]\n",
      "epoch:14 step:13742 [D loss: 0.692322, acc.: 58.59%] [G loss: 0.822449]\n",
      "epoch:14 step:13743 [D loss: 0.697381, acc.: 47.66%] [G loss: 0.869921]\n",
      "epoch:14 step:13744 [D loss: 0.688785, acc.: 51.56%] [G loss: 0.821065]\n",
      "epoch:14 step:13745 [D loss: 0.666050, acc.: 63.28%] [G loss: 0.849628]\n",
      "epoch:14 step:13746 [D loss: 0.633591, acc.: 69.53%] [G loss: 0.892845]\n",
      "epoch:14 step:13747 [D loss: 0.642466, acc.: 60.94%] [G loss: 0.836826]\n",
      "epoch:14 step:13748 [D loss: 0.636070, acc.: 62.50%] [G loss: 0.842002]\n",
      "epoch:14 step:13749 [D loss: 0.642118, acc.: 62.50%] [G loss: 0.866334]\n",
      "epoch:14 step:13750 [D loss: 0.662634, acc.: 59.38%] [G loss: 0.867981]\n",
      "epoch:14 step:13751 [D loss: 0.718957, acc.: 50.78%] [G loss: 0.823141]\n",
      "epoch:14 step:13752 [D loss: 0.706234, acc.: 60.16%] [G loss: 0.927849]\n",
      "epoch:14 step:13753 [D loss: 0.645520, acc.: 64.84%] [G loss: 0.957371]\n",
      "epoch:14 step:13754 [D loss: 0.653700, acc.: 59.38%] [G loss: 0.987906]\n",
      "epoch:14 step:13755 [D loss: 0.657205, acc.: 59.38%] [G loss: 0.858248]\n",
      "epoch:14 step:13756 [D loss: 0.664760, acc.: 60.16%] [G loss: 0.946466]\n",
      "epoch:14 step:13757 [D loss: 0.630688, acc.: 60.94%] [G loss: 1.013172]\n",
      "epoch:14 step:13758 [D loss: 0.646716, acc.: 57.81%] [G loss: 0.979779]\n",
      "epoch:14 step:13759 [D loss: 0.621034, acc.: 70.31%] [G loss: 1.059622]\n",
      "epoch:14 step:13760 [D loss: 0.766132, acc.: 43.75%] [G loss: 0.917532]\n",
      "epoch:14 step:13761 [D loss: 0.636080, acc.: 64.06%] [G loss: 0.946086]\n",
      "epoch:14 step:13762 [D loss: 0.659827, acc.: 57.81%] [G loss: 0.826627]\n",
      "epoch:14 step:13763 [D loss: 0.683141, acc.: 55.47%] [G loss: 1.020957]\n",
      "epoch:14 step:13764 [D loss: 0.974642, acc.: 39.06%] [G loss: 0.873787]\n",
      "epoch:14 step:13765 [D loss: 0.619345, acc.: 73.44%] [G loss: 0.854740]\n",
      "epoch:14 step:13766 [D loss: 0.676878, acc.: 57.81%] [G loss: 0.863807]\n",
      "epoch:14 step:13767 [D loss: 0.665948, acc.: 62.50%] [G loss: 0.877179]\n",
      "epoch:14 step:13768 [D loss: 0.661516, acc.: 62.50%] [G loss: 0.729369]\n",
      "epoch:14 step:13769 [D loss: 0.660131, acc.: 62.50%] [G loss: 0.888273]\n",
      "epoch:14 step:13770 [D loss: 0.669135, acc.: 61.72%] [G loss: 0.874845]\n",
      "epoch:14 step:13771 [D loss: 0.687263, acc.: 52.34%] [G loss: 0.934805]\n",
      "epoch:14 step:13772 [D loss: 0.625532, acc.: 64.84%] [G loss: 0.760874]\n",
      "epoch:14 step:13773 [D loss: 0.613485, acc.: 67.19%] [G loss: 0.933768]\n",
      "epoch:14 step:13774 [D loss: 0.685305, acc.: 54.69%] [G loss: 0.780559]\n",
      "epoch:14 step:13775 [D loss: 0.743609, acc.: 42.19%] [G loss: 0.725218]\n",
      "epoch:14 step:13776 [D loss: 0.782366, acc.: 31.25%] [G loss: 0.728631]\n",
      "epoch:14 step:13777 [D loss: 0.601764, acc.: 72.66%] [G loss: 0.774443]\n",
      "epoch:14 step:13778 [D loss: 0.634436, acc.: 64.06%] [G loss: 0.748278]\n",
      "epoch:14 step:13779 [D loss: 0.663393, acc.: 62.50%] [G loss: 0.876577]\n",
      "epoch:14 step:13780 [D loss: 0.662058, acc.: 53.91%] [G loss: 0.812131]\n",
      "epoch:14 step:13781 [D loss: 0.697363, acc.: 50.00%] [G loss: 0.846855]\n",
      "epoch:14 step:13782 [D loss: 0.624891, acc.: 69.53%] [G loss: 0.753687]\n",
      "epoch:14 step:13783 [D loss: 0.709383, acc.: 50.78%] [G loss: 0.728543]\n",
      "epoch:14 step:13784 [D loss: 0.675745, acc.: 59.38%] [G loss: 0.759382]\n",
      "epoch:14 step:13785 [D loss: 0.728517, acc.: 50.78%] [G loss: 0.840435]\n",
      "epoch:14 step:13786 [D loss: 0.726692, acc.: 40.62%] [G loss: 0.859438]\n",
      "epoch:14 step:13787 [D loss: 0.663078, acc.: 60.94%] [G loss: 0.877282]\n",
      "epoch:14 step:13788 [D loss: 0.631152, acc.: 72.66%] [G loss: 0.822903]\n",
      "epoch:14 step:13789 [D loss: 0.644553, acc.: 63.28%] [G loss: 0.888237]\n",
      "epoch:14 step:13790 [D loss: 0.672235, acc.: 63.28%] [G loss: 0.771698]\n",
      "epoch:14 step:13791 [D loss: 0.656668, acc.: 59.38%] [G loss: 0.820300]\n",
      "epoch:14 step:13792 [D loss: 0.667804, acc.: 54.69%] [G loss: 0.846857]\n",
      "epoch:14 step:13793 [D loss: 0.569599, acc.: 69.53%] [G loss: 0.891381]\n",
      "epoch:14 step:13794 [D loss: 0.600955, acc.: 75.00%] [G loss: 1.000408]\n",
      "epoch:14 step:13795 [D loss: 0.702734, acc.: 56.25%] [G loss: 0.896989]\n",
      "epoch:14 step:13796 [D loss: 0.710764, acc.: 53.12%] [G loss: 0.861880]\n",
      "epoch:14 step:13797 [D loss: 0.687383, acc.: 53.12%] [G loss: 0.951201]\n",
      "epoch:14 step:13798 [D loss: 0.664562, acc.: 55.47%] [G loss: 0.817642]\n",
      "epoch:14 step:13799 [D loss: 0.634209, acc.: 71.09%] [G loss: 0.858467]\n",
      "epoch:14 step:13800 [D loss: 0.701383, acc.: 46.09%] [G loss: 0.883047]\n",
      "epoch:14 step:13801 [D loss: 0.673728, acc.: 65.62%] [G loss: 0.912945]\n",
      "epoch:14 step:13802 [D loss: 0.659268, acc.: 61.72%] [G loss: 0.813468]\n",
      "epoch:14 step:13803 [D loss: 0.692010, acc.: 55.47%] [G loss: 0.945530]\n",
      "epoch:14 step:13804 [D loss: 0.666805, acc.: 59.38%] [G loss: 0.866579]\n",
      "epoch:14 step:13805 [D loss: 0.643485, acc.: 60.94%] [G loss: 0.841141]\n",
      "epoch:14 step:13806 [D loss: 0.658141, acc.: 55.47%] [G loss: 0.874043]\n",
      "epoch:14 step:13807 [D loss: 0.715321, acc.: 50.00%] [G loss: 0.826357]\n",
      "epoch:14 step:13808 [D loss: 0.714679, acc.: 46.09%] [G loss: 0.808346]\n",
      "epoch:14 step:13809 [D loss: 0.678792, acc.: 53.91%] [G loss: 0.752077]\n",
      "epoch:14 step:13810 [D loss: 0.576670, acc.: 70.31%] [G loss: 0.800269]\n",
      "epoch:14 step:13811 [D loss: 0.685621, acc.: 60.94%] [G loss: 0.690746]\n",
      "epoch:14 step:13812 [D loss: 0.728895, acc.: 48.44%] [G loss: 0.838963]\n",
      "epoch:14 step:13813 [D loss: 0.602341, acc.: 71.88%] [G loss: 0.811091]\n",
      "epoch:14 step:13814 [D loss: 0.682044, acc.: 53.91%] [G loss: 0.768513]\n",
      "epoch:14 step:13815 [D loss: 0.653555, acc.: 65.62%] [G loss: 0.856991]\n",
      "epoch:14 step:13816 [D loss: 0.653792, acc.: 60.94%] [G loss: 0.778645]\n",
      "epoch:14 step:13817 [D loss: 0.616067, acc.: 67.19%] [G loss: 0.803155]\n",
      "epoch:14 step:13818 [D loss: 0.647291, acc.: 59.38%] [G loss: 0.931097]\n",
      "epoch:14 step:13819 [D loss: 0.702940, acc.: 51.56%] [G loss: 0.753750]\n",
      "epoch:14 step:13820 [D loss: 0.636480, acc.: 60.16%] [G loss: 0.876509]\n",
      "epoch:14 step:13821 [D loss: 0.727329, acc.: 50.00%] [G loss: 0.856575]\n",
      "epoch:14 step:13822 [D loss: 0.675070, acc.: 50.78%] [G loss: 0.907005]\n",
      "epoch:14 step:13823 [D loss: 0.704884, acc.: 47.66%] [G loss: 0.805672]\n",
      "epoch:14 step:13824 [D loss: 0.557518, acc.: 65.62%] [G loss: 0.787558]\n",
      "epoch:14 step:13825 [D loss: 0.647777, acc.: 62.50%] [G loss: 0.872166]\n",
      "epoch:14 step:13826 [D loss: 0.659464, acc.: 54.69%] [G loss: 0.889649]\n",
      "epoch:14 step:13827 [D loss: 0.700883, acc.: 56.25%] [G loss: 0.809570]\n",
      "epoch:14 step:13828 [D loss: 0.603300, acc.: 69.53%] [G loss: 0.941746]\n",
      "epoch:14 step:13829 [D loss: 0.636096, acc.: 61.72%] [G loss: 0.920821]\n",
      "epoch:14 step:13830 [D loss: 0.674175, acc.: 60.94%] [G loss: 0.882573]\n",
      "epoch:14 step:13831 [D loss: 0.690284, acc.: 55.47%] [G loss: 0.882734]\n",
      "epoch:14 step:13832 [D loss: 0.626074, acc.: 66.41%] [G loss: 0.864524]\n",
      "epoch:14 step:13833 [D loss: 0.645512, acc.: 64.84%] [G loss: 0.834259]\n",
      "epoch:14 step:13834 [D loss: 0.673379, acc.: 53.91%] [G loss: 0.897411]\n",
      "epoch:14 step:13835 [D loss: 0.697135, acc.: 48.44%] [G loss: 0.744455]\n",
      "epoch:14 step:13836 [D loss: 0.658293, acc.: 59.38%] [G loss: 0.855749]\n",
      "epoch:14 step:13837 [D loss: 0.625464, acc.: 67.97%] [G loss: 0.865576]\n",
      "epoch:14 step:13838 [D loss: 0.641499, acc.: 68.75%] [G loss: 0.946625]\n",
      "epoch:14 step:13839 [D loss: 0.638429, acc.: 67.19%] [G loss: 0.853325]\n",
      "epoch:14 step:13840 [D loss: 0.620777, acc.: 70.31%] [G loss: 0.800606]\n",
      "epoch:14 step:13841 [D loss: 0.632172, acc.: 66.41%] [G loss: 0.763410]\n",
      "epoch:14 step:13842 [D loss: 0.675388, acc.: 63.28%] [G loss: 0.758799]\n",
      "epoch:14 step:13843 [D loss: 0.595828, acc.: 75.78%] [G loss: 1.067843]\n",
      "epoch:14 step:13844 [D loss: 0.703266, acc.: 48.44%] [G loss: 0.774236]\n",
      "epoch:14 step:13845 [D loss: 0.615007, acc.: 66.41%] [G loss: 0.915174]\n",
      "epoch:14 step:13846 [D loss: 0.606034, acc.: 67.97%] [G loss: 0.893036]\n",
      "epoch:14 step:13847 [D loss: 0.737463, acc.: 48.44%] [G loss: 0.971750]\n",
      "epoch:14 step:13848 [D loss: 0.694710, acc.: 47.66%] [G loss: 0.827078]\n",
      "epoch:14 step:13849 [D loss: 0.688521, acc.: 54.69%] [G loss: 0.924944]\n",
      "epoch:14 step:13850 [D loss: 0.613353, acc.: 69.53%] [G loss: 0.899509]\n",
      "epoch:14 step:13851 [D loss: 0.538030, acc.: 85.94%] [G loss: 0.991699]\n",
      "epoch:14 step:13852 [D loss: 0.764079, acc.: 47.66%] [G loss: 0.764018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13853 [D loss: 0.674590, acc.: 58.59%] [G loss: 0.900422]\n",
      "epoch:14 step:13854 [D loss: 0.625715, acc.: 57.81%] [G loss: 0.779245]\n",
      "epoch:14 step:13855 [D loss: 0.688772, acc.: 55.47%] [G loss: 0.798691]\n",
      "epoch:14 step:13856 [D loss: 0.663819, acc.: 57.81%] [G loss: 0.862791]\n",
      "epoch:14 step:13857 [D loss: 0.628161, acc.: 64.84%] [G loss: 0.809550]\n",
      "epoch:14 step:13858 [D loss: 0.627734, acc.: 63.28%] [G loss: 0.988971]\n",
      "epoch:14 step:13859 [D loss: 0.687719, acc.: 58.59%] [G loss: 0.916604]\n",
      "epoch:14 step:13860 [D loss: 0.714006, acc.: 50.78%] [G loss: 0.847126]\n",
      "epoch:14 step:13861 [D loss: 0.589762, acc.: 74.22%] [G loss: 0.729374]\n",
      "epoch:14 step:13862 [D loss: 0.674080, acc.: 51.56%] [G loss: 0.795923]\n",
      "epoch:14 step:13863 [D loss: 0.653984, acc.: 53.91%] [G loss: 0.756243]\n",
      "epoch:14 step:13864 [D loss: 0.722166, acc.: 47.66%] [G loss: 0.821868]\n",
      "epoch:14 step:13865 [D loss: 0.644267, acc.: 62.50%] [G loss: 0.798953]\n",
      "epoch:14 step:13866 [D loss: 0.618889, acc.: 68.75%] [G loss: 0.919532]\n",
      "epoch:14 step:13867 [D loss: 0.662995, acc.: 56.25%] [G loss: 0.884791]\n",
      "epoch:14 step:13868 [D loss: 0.694239, acc.: 54.69%] [G loss: 0.750082]\n",
      "epoch:14 step:13869 [D loss: 0.679188, acc.: 52.34%] [G loss: 0.856683]\n",
      "epoch:14 step:13870 [D loss: 0.688364, acc.: 48.44%] [G loss: 0.810726]\n",
      "epoch:14 step:13871 [D loss: 0.642472, acc.: 64.06%] [G loss: 0.948140]\n",
      "epoch:14 step:13872 [D loss: 0.684905, acc.: 50.78%] [G loss: 0.902252]\n",
      "epoch:14 step:13873 [D loss: 0.682705, acc.: 66.41%] [G loss: 0.834148]\n",
      "epoch:14 step:13874 [D loss: 0.696816, acc.: 53.12%] [G loss: 0.844819]\n",
      "epoch:14 step:13875 [D loss: 0.628501, acc.: 69.53%] [G loss: 0.955930]\n",
      "epoch:14 step:13876 [D loss: 0.615585, acc.: 69.53%] [G loss: 0.997492]\n",
      "epoch:14 step:13877 [D loss: 0.691965, acc.: 57.81%] [G loss: 0.897305]\n",
      "epoch:14 step:13878 [D loss: 0.608462, acc.: 67.97%] [G loss: 0.857190]\n",
      "epoch:14 step:13879 [D loss: 0.734190, acc.: 49.22%] [G loss: 0.820728]\n",
      "epoch:14 step:13880 [D loss: 0.644925, acc.: 65.62%] [G loss: 0.847605]\n",
      "epoch:14 step:13881 [D loss: 0.630372, acc.: 66.41%] [G loss: 0.939540]\n",
      "epoch:14 step:13882 [D loss: 0.603165, acc.: 70.31%] [G loss: 0.954449]\n",
      "epoch:14 step:13883 [D loss: 0.663400, acc.: 62.50%] [G loss: 0.928054]\n",
      "epoch:14 step:13884 [D loss: 0.626658, acc.: 67.97%] [G loss: 0.992343]\n",
      "epoch:14 step:13885 [D loss: 0.661609, acc.: 55.47%] [G loss: 0.856148]\n",
      "epoch:14 step:13886 [D loss: 0.652809, acc.: 58.59%] [G loss: 0.834484]\n",
      "epoch:14 step:13887 [D loss: 0.703272, acc.: 51.56%] [G loss: 0.845132]\n",
      "epoch:14 step:13888 [D loss: 0.677519, acc.: 56.25%] [G loss: 0.814484]\n",
      "epoch:14 step:13889 [D loss: 0.608001, acc.: 67.97%] [G loss: 0.761011]\n",
      "epoch:14 step:13890 [D loss: 0.621645, acc.: 64.84%] [G loss: 0.745435]\n",
      "epoch:14 step:13891 [D loss: 0.739607, acc.: 42.19%] [G loss: 0.808088]\n",
      "epoch:14 step:13892 [D loss: 0.647585, acc.: 57.03%] [G loss: 0.808750]\n",
      "epoch:14 step:13893 [D loss: 0.653077, acc.: 59.38%] [G loss: 0.862872]\n",
      "epoch:14 step:13894 [D loss: 0.656576, acc.: 54.69%] [G loss: 0.804421]\n",
      "epoch:14 step:13895 [D loss: 0.652749, acc.: 59.38%] [G loss: 0.892299]\n",
      "epoch:14 step:13896 [D loss: 0.631366, acc.: 65.62%] [G loss: 0.757338]\n",
      "epoch:14 step:13897 [D loss: 0.690446, acc.: 53.12%] [G loss: 0.740230]\n",
      "epoch:14 step:13898 [D loss: 0.635125, acc.: 61.72%] [G loss: 0.769574]\n",
      "epoch:14 step:13899 [D loss: 0.648438, acc.: 60.16%] [G loss: 0.980152]\n",
      "epoch:14 step:13900 [D loss: 0.669950, acc.: 57.81%] [G loss: 0.779417]\n",
      "epoch:14 step:13901 [D loss: 0.642099, acc.: 60.16%] [G loss: 0.810224]\n",
      "epoch:14 step:13902 [D loss: 0.633878, acc.: 67.19%] [G loss: 0.954987]\n",
      "epoch:14 step:13903 [D loss: 0.722286, acc.: 44.53%] [G loss: 0.791439]\n",
      "epoch:14 step:13904 [D loss: 0.632049, acc.: 64.84%] [G loss: 0.841679]\n",
      "epoch:14 step:13905 [D loss: 0.711371, acc.: 44.53%] [G loss: 0.896459]\n",
      "epoch:14 step:13906 [D loss: 0.601915, acc.: 68.75%] [G loss: 0.992087]\n",
      "epoch:14 step:13907 [D loss: 0.607361, acc.: 74.22%] [G loss: 0.987923]\n",
      "epoch:14 step:13908 [D loss: 0.609800, acc.: 73.44%] [G loss: 0.985775]\n",
      "epoch:14 step:13909 [D loss: 0.631656, acc.: 63.28%] [G loss: 0.818741]\n",
      "epoch:14 step:13910 [D loss: 0.596998, acc.: 72.66%] [G loss: 0.915487]\n",
      "epoch:14 step:13911 [D loss: 0.740119, acc.: 53.12%] [G loss: 1.046227]\n",
      "epoch:14 step:13912 [D loss: 0.711446, acc.: 42.97%] [G loss: 0.960610]\n",
      "epoch:14 step:13913 [D loss: 0.700452, acc.: 51.56%] [G loss: 1.045025]\n",
      "epoch:14 step:13914 [D loss: 0.695978, acc.: 52.34%] [G loss: 0.915387]\n",
      "epoch:14 step:13915 [D loss: 0.748987, acc.: 46.09%] [G loss: 0.855014]\n",
      "epoch:14 step:13916 [D loss: 0.618647, acc.: 74.22%] [G loss: 0.888884]\n",
      "epoch:14 step:13917 [D loss: 0.635365, acc.: 64.84%] [G loss: 0.926913]\n",
      "epoch:14 step:13918 [D loss: 0.626183, acc.: 65.62%] [G loss: 0.850337]\n",
      "epoch:14 step:13919 [D loss: 0.702920, acc.: 51.56%] [G loss: 0.951529]\n",
      "epoch:14 step:13920 [D loss: 0.790962, acc.: 30.47%] [G loss: 0.863017]\n",
      "epoch:14 step:13921 [D loss: 0.639941, acc.: 64.06%] [G loss: 0.929269]\n",
      "epoch:14 step:13922 [D loss: 0.674203, acc.: 52.34%] [G loss: 0.889820]\n",
      "epoch:14 step:13923 [D loss: 0.618884, acc.: 70.31%] [G loss: 0.936768]\n",
      "epoch:14 step:13924 [D loss: 0.703559, acc.: 54.69%] [G loss: 0.806845]\n",
      "epoch:14 step:13925 [D loss: 0.608672, acc.: 67.97%] [G loss: 0.875887]\n",
      "epoch:14 step:13926 [D loss: 0.698054, acc.: 50.78%] [G loss: 0.843846]\n",
      "epoch:14 step:13927 [D loss: 0.630663, acc.: 69.53%] [G loss: 0.846177]\n",
      "epoch:14 step:13928 [D loss: 0.634856, acc.: 64.06%] [G loss: 0.924773]\n",
      "epoch:14 step:13929 [D loss: 0.697351, acc.: 51.56%] [G loss: 0.967524]\n",
      "epoch:14 step:13930 [D loss: 0.660348, acc.: 59.38%] [G loss: 0.833019]\n",
      "epoch:14 step:13931 [D loss: 0.634161, acc.: 66.41%] [G loss: 1.029734]\n",
      "epoch:14 step:13932 [D loss: 0.613443, acc.: 63.28%] [G loss: 0.863255]\n",
      "epoch:14 step:13933 [D loss: 0.664098, acc.: 58.59%] [G loss: 1.011660]\n",
      "epoch:14 step:13934 [D loss: 0.611213, acc.: 64.84%] [G loss: 0.884926]\n",
      "epoch:14 step:13935 [D loss: 0.694207, acc.: 55.47%] [G loss: 0.759334]\n",
      "epoch:14 step:13936 [D loss: 0.684761, acc.: 50.78%] [G loss: 0.793488]\n",
      "epoch:14 step:13937 [D loss: 0.662033, acc.: 60.16%] [G loss: 0.862696]\n",
      "epoch:14 step:13938 [D loss: 0.636062, acc.: 60.94%] [G loss: 1.079838]\n",
      "epoch:14 step:13939 [D loss: 0.611297, acc.: 64.06%] [G loss: 0.871652]\n",
      "epoch:14 step:13940 [D loss: 0.702460, acc.: 51.56%] [G loss: 0.926191]\n",
      "epoch:14 step:13941 [D loss: 0.596734, acc.: 64.84%] [G loss: 1.038674]\n",
      "epoch:14 step:13942 [D loss: 0.752147, acc.: 47.66%] [G loss: 0.984434]\n",
      "epoch:14 step:13943 [D loss: 0.590934, acc.: 68.75%] [G loss: 0.868922]\n",
      "epoch:14 step:13944 [D loss: 0.757647, acc.: 43.75%] [G loss: 0.853394]\n",
      "epoch:14 step:13945 [D loss: 0.595071, acc.: 75.78%] [G loss: 0.883709]\n",
      "epoch:14 step:13946 [D loss: 0.617873, acc.: 73.44%] [G loss: 0.930537]\n",
      "epoch:14 step:13947 [D loss: 0.704056, acc.: 50.00%] [G loss: 0.851430]\n",
      "epoch:14 step:13948 [D loss: 0.655100, acc.: 60.94%] [G loss: 0.778973]\n",
      "epoch:14 step:13949 [D loss: 0.702660, acc.: 53.12%] [G loss: 0.766986]\n",
      "epoch:14 step:13950 [D loss: 0.658567, acc.: 54.69%] [G loss: 0.914487]\n",
      "epoch:14 step:13951 [D loss: 0.652804, acc.: 59.38%] [G loss: 0.872452]\n",
      "epoch:14 step:13952 [D loss: 0.678914, acc.: 58.59%] [G loss: 0.921946]\n",
      "epoch:14 step:13953 [D loss: 0.699023, acc.: 55.47%] [G loss: 0.838624]\n",
      "epoch:14 step:13954 [D loss: 0.684718, acc.: 50.78%] [G loss: 0.783060]\n",
      "epoch:14 step:13955 [D loss: 0.667091, acc.: 49.22%] [G loss: 0.850669]\n",
      "epoch:14 step:13956 [D loss: 0.675049, acc.: 52.34%] [G loss: 0.882755]\n",
      "epoch:14 step:13957 [D loss: 0.601034, acc.: 71.88%] [G loss: 0.901054]\n",
      "epoch:14 step:13958 [D loss: 0.708193, acc.: 45.31%] [G loss: 0.777220]\n",
      "epoch:14 step:13959 [D loss: 0.642244, acc.: 63.28%] [G loss: 0.926258]\n",
      "epoch:14 step:13960 [D loss: 0.687775, acc.: 56.25%] [G loss: 0.945338]\n",
      "epoch:14 step:13961 [D loss: 0.591309, acc.: 76.56%] [G loss: 0.851010]\n",
      "epoch:14 step:13962 [D loss: 0.675888, acc.: 56.25%] [G loss: 0.792157]\n",
      "epoch:14 step:13963 [D loss: 0.684862, acc.: 50.78%] [G loss: 0.839134]\n",
      "epoch:14 step:13964 [D loss: 0.660878, acc.: 56.25%] [G loss: 0.777476]\n",
      "epoch:14 step:13965 [D loss: 0.686652, acc.: 57.03%] [G loss: 0.817277]\n",
      "epoch:14 step:13966 [D loss: 0.690723, acc.: 53.12%] [G loss: 0.787225]\n",
      "epoch:14 step:13967 [D loss: 0.620461, acc.: 61.72%] [G loss: 0.856636]\n",
      "epoch:14 step:13968 [D loss: 0.675115, acc.: 53.91%] [G loss: 0.787314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13969 [D loss: 0.602658, acc.: 71.09%] [G loss: 0.898038]\n",
      "epoch:14 step:13970 [D loss: 0.616712, acc.: 65.62%] [G loss: 0.789529]\n",
      "epoch:14 step:13971 [D loss: 0.620042, acc.: 60.16%] [G loss: 0.806874]\n",
      "epoch:14 step:13972 [D loss: 0.631170, acc.: 62.50%] [G loss: 0.905652]\n",
      "epoch:14 step:13973 [D loss: 0.673140, acc.: 53.91%] [G loss: 1.022239]\n",
      "epoch:14 step:13974 [D loss: 0.674588, acc.: 46.88%] [G loss: 0.832607]\n",
      "epoch:14 step:13975 [D loss: 0.660467, acc.: 64.84%] [G loss: 0.842153]\n",
      "epoch:14 step:13976 [D loss: 0.707481, acc.: 40.62%] [G loss: 0.806733]\n",
      "epoch:14 step:13977 [D loss: 0.635473, acc.: 61.72%] [G loss: 0.801662]\n",
      "epoch:14 step:13978 [D loss: 0.650558, acc.: 66.41%] [G loss: 0.881827]\n",
      "epoch:14 step:13979 [D loss: 0.674882, acc.: 60.16%] [G loss: 0.998698]\n",
      "epoch:14 step:13980 [D loss: 0.597311, acc.: 76.56%] [G loss: 0.849251]\n",
      "epoch:14 step:13981 [D loss: 0.683031, acc.: 56.25%] [G loss: 0.901189]\n",
      "epoch:14 step:13982 [D loss: 0.665442, acc.: 56.25%] [G loss: 0.865340]\n",
      "epoch:14 step:13983 [D loss: 0.685525, acc.: 53.91%] [G loss: 0.825782]\n",
      "epoch:14 step:13984 [D loss: 0.621210, acc.: 67.19%] [G loss: 0.934381]\n",
      "epoch:14 step:13985 [D loss: 0.645632, acc.: 65.62%] [G loss: 0.744059]\n",
      "epoch:14 step:13986 [D loss: 0.645065, acc.: 61.72%] [G loss: 0.862045]\n",
      "epoch:14 step:13987 [D loss: 0.616048, acc.: 67.19%] [G loss: 0.875538]\n",
      "epoch:14 step:13988 [D loss: 0.661218, acc.: 58.59%] [G loss: 0.810970]\n",
      "epoch:14 step:13989 [D loss: 0.645849, acc.: 60.94%] [G loss: 0.831177]\n",
      "epoch:14 step:13990 [D loss: 0.663864, acc.: 64.06%] [G loss: 0.944593]\n",
      "epoch:14 step:13991 [D loss: 0.619988, acc.: 60.94%] [G loss: 1.046707]\n",
      "epoch:14 step:13992 [D loss: 0.647794, acc.: 57.81%] [G loss: 1.111321]\n",
      "epoch:14 step:13993 [D loss: 0.531807, acc.: 81.25%] [G loss: 1.017296]\n",
      "epoch:14 step:13994 [D loss: 0.651445, acc.: 64.06%] [G loss: 0.890160]\n",
      "epoch:14 step:13995 [D loss: 0.649491, acc.: 60.16%] [G loss: 0.906226]\n",
      "epoch:14 step:13996 [D loss: 0.672741, acc.: 57.03%] [G loss: 0.894970]\n",
      "epoch:14 step:13997 [D loss: 0.685379, acc.: 64.06%] [G loss: 0.887987]\n",
      "epoch:14 step:13998 [D loss: 0.704610, acc.: 53.12%] [G loss: 0.819175]\n",
      "epoch:14 step:13999 [D loss: 0.680202, acc.: 56.25%] [G loss: 0.820097]\n",
      "epoch:14 step:14000 [D loss: 0.593973, acc.: 71.88%] [G loss: 0.900367]\n",
      "epoch:14 step:14001 [D loss: 0.678883, acc.: 57.03%] [G loss: 1.098862]\n",
      "epoch:14 step:14002 [D loss: 0.580627, acc.: 74.22%] [G loss: 1.073487]\n",
      "epoch:14 step:14003 [D loss: 0.585930, acc.: 66.41%] [G loss: 0.835668]\n",
      "epoch:14 step:14004 [D loss: 0.546997, acc.: 76.56%] [G loss: 1.085965]\n",
      "epoch:14 step:14005 [D loss: 0.677379, acc.: 57.81%] [G loss: 0.986471]\n",
      "epoch:14 step:14006 [D loss: 0.550673, acc.: 74.22%] [G loss: 1.108710]\n",
      "epoch:14 step:14007 [D loss: 0.605961, acc.: 65.62%] [G loss: 0.969959]\n",
      "epoch:14 step:14008 [D loss: 0.579150, acc.: 75.00%] [G loss: 0.903660]\n",
      "epoch:14 step:14009 [D loss: 0.666474, acc.: 58.59%] [G loss: 0.948008]\n",
      "epoch:14 step:14010 [D loss: 0.632475, acc.: 64.06%] [G loss: 0.912046]\n",
      "epoch:14 step:14011 [D loss: 0.608263, acc.: 66.41%] [G loss: 0.738639]\n",
      "epoch:14 step:14012 [D loss: 0.707505, acc.: 60.16%] [G loss: 0.872129]\n",
      "epoch:14 step:14013 [D loss: 0.595290, acc.: 68.75%] [G loss: 1.055023]\n",
      "epoch:14 step:14014 [D loss: 0.645724, acc.: 61.72%] [G loss: 0.730203]\n",
      "epoch:14 step:14015 [D loss: 0.647948, acc.: 66.41%] [G loss: 1.100223]\n",
      "epoch:14 step:14016 [D loss: 0.644695, acc.: 60.94%] [G loss: 0.973540]\n",
      "epoch:14 step:14017 [D loss: 0.680195, acc.: 49.22%] [G loss: 0.996662]\n",
      "epoch:14 step:14018 [D loss: 0.662519, acc.: 58.59%] [G loss: 0.929223]\n",
      "epoch:14 step:14019 [D loss: 0.764055, acc.: 46.09%] [G loss: 0.991083]\n",
      "epoch:14 step:14020 [D loss: 0.631759, acc.: 74.22%] [G loss: 0.768174]\n",
      "epoch:14 step:14021 [D loss: 0.661140, acc.: 57.81%] [G loss: 0.852513]\n",
      "epoch:14 step:14022 [D loss: 0.835943, acc.: 31.25%] [G loss: 0.851480]\n",
      "epoch:14 step:14023 [D loss: 0.767555, acc.: 48.44%] [G loss: 0.894830]\n",
      "epoch:14 step:14024 [D loss: 0.680711, acc.: 53.12%] [G loss: 0.788747]\n",
      "epoch:14 step:14025 [D loss: 0.669966, acc.: 55.47%] [G loss: 0.908609]\n",
      "epoch:14 step:14026 [D loss: 0.651505, acc.: 61.72%] [G loss: 0.887615]\n",
      "epoch:14 step:14027 [D loss: 0.653002, acc.: 57.81%] [G loss: 0.861126]\n",
      "epoch:14 step:14028 [D loss: 0.691294, acc.: 53.91%] [G loss: 1.002133]\n",
      "epoch:14 step:14029 [D loss: 0.633944, acc.: 53.91%] [G loss: 0.836892]\n",
      "epoch:14 step:14030 [D loss: 0.633071, acc.: 67.97%] [G loss: 0.914575]\n",
      "epoch:14 step:14031 [D loss: 0.612102, acc.: 65.62%] [G loss: 0.872639]\n",
      "epoch:14 step:14032 [D loss: 0.602675, acc.: 64.06%] [G loss: 0.855620]\n",
      "epoch:14 step:14033 [D loss: 0.647998, acc.: 60.16%] [G loss: 1.048810]\n",
      "epoch:14 step:14034 [D loss: 0.614424, acc.: 66.41%] [G loss: 0.868672]\n",
      "epoch:14 step:14035 [D loss: 0.678260, acc.: 50.78%] [G loss: 0.866608]\n",
      "epoch:14 step:14036 [D loss: 0.681528, acc.: 58.59%] [G loss: 0.932400]\n",
      "epoch:14 step:14037 [D loss: 0.683449, acc.: 56.25%] [G loss: 0.954923]\n",
      "epoch:14 step:14038 [D loss: 0.694418, acc.: 55.47%] [G loss: 0.983316]\n",
      "epoch:14 step:14039 [D loss: 0.684589, acc.: 60.16%] [G loss: 0.900064]\n",
      "epoch:14 step:14040 [D loss: 0.695853, acc.: 59.38%] [G loss: 0.795675]\n",
      "epoch:14 step:14041 [D loss: 0.718121, acc.: 49.22%] [G loss: 0.804380]\n",
      "epoch:14 step:14042 [D loss: 0.635971, acc.: 66.41%] [G loss: 0.836142]\n",
      "epoch:14 step:14043 [D loss: 0.702921, acc.: 53.91%] [G loss: 0.956989]\n",
      "epoch:14 step:14044 [D loss: 0.681594, acc.: 51.56%] [G loss: 0.871132]\n",
      "epoch:14 step:14045 [D loss: 0.696470, acc.: 56.25%] [G loss: 0.879681]\n",
      "epoch:14 step:14046 [D loss: 0.678529, acc.: 56.25%] [G loss: 0.886814]\n",
      "epoch:14 step:14047 [D loss: 0.649087, acc.: 59.38%] [G loss: 0.728593]\n",
      "epoch:14 step:14048 [D loss: 0.724206, acc.: 50.78%] [G loss: 0.832394]\n",
      "epoch:14 step:14049 [D loss: 0.664289, acc.: 64.84%] [G loss: 0.828200]\n",
      "epoch:14 step:14050 [D loss: 0.678504, acc.: 58.59%] [G loss: 0.970202]\n",
      "epoch:14 step:14051 [D loss: 0.671215, acc.: 46.88%] [G loss: 0.909222]\n",
      "epoch:14 step:14052 [D loss: 0.674618, acc.: 51.56%] [G loss: 0.987040]\n",
      "epoch:14 step:14053 [D loss: 0.652956, acc.: 63.28%] [G loss: 0.749128]\n",
      "epoch:14 step:14054 [D loss: 0.653425, acc.: 65.62%] [G loss: 0.902748]\n",
      "epoch:14 step:14055 [D loss: 0.661404, acc.: 57.03%] [G loss: 0.919424]\n",
      "epoch:15 step:14056 [D loss: 0.647531, acc.: 60.16%] [G loss: 0.835280]\n",
      "epoch:15 step:14057 [D loss: 0.672405, acc.: 60.94%] [G loss: 0.787297]\n",
      "epoch:15 step:14058 [D loss: 0.630659, acc.: 53.91%] [G loss: 0.775986]\n",
      "epoch:15 step:14059 [D loss: 0.597476, acc.: 67.97%] [G loss: 0.918974]\n",
      "epoch:15 step:14060 [D loss: 0.617221, acc.: 64.06%] [G loss: 0.856805]\n",
      "epoch:15 step:14061 [D loss: 0.605536, acc.: 64.84%] [G loss: 0.819477]\n",
      "epoch:15 step:14062 [D loss: 0.718835, acc.: 45.31%] [G loss: 0.889444]\n",
      "epoch:15 step:14063 [D loss: 0.745615, acc.: 45.31%] [G loss: 1.002388]\n",
      "epoch:15 step:14064 [D loss: 0.698852, acc.: 57.03%] [G loss: 0.845037]\n",
      "epoch:15 step:14065 [D loss: 0.695798, acc.: 58.59%] [G loss: 0.816575]\n",
      "epoch:15 step:14066 [D loss: 0.697664, acc.: 50.78%] [G loss: 0.802679]\n",
      "epoch:15 step:14067 [D loss: 0.622067, acc.: 67.19%] [G loss: 0.828876]\n",
      "epoch:15 step:14068 [D loss: 0.616288, acc.: 69.53%] [G loss: 0.845496]\n",
      "epoch:15 step:14069 [D loss: 0.782895, acc.: 38.28%] [G loss: 0.886011]\n",
      "epoch:15 step:14070 [D loss: 0.678146, acc.: 57.03%] [G loss: 0.827073]\n",
      "epoch:15 step:14071 [D loss: 0.630526, acc.: 67.19%] [G loss: 0.893351]\n",
      "epoch:15 step:14072 [D loss: 0.647378, acc.: 64.06%] [G loss: 0.792610]\n",
      "epoch:15 step:14073 [D loss: 0.704827, acc.: 50.00%] [G loss: 0.805919]\n",
      "epoch:15 step:14074 [D loss: 0.729455, acc.: 56.25%] [G loss: 0.898559]\n",
      "epoch:15 step:14075 [D loss: 0.692477, acc.: 55.47%] [G loss: 0.945077]\n",
      "epoch:15 step:14076 [D loss: 0.643646, acc.: 57.81%] [G loss: 0.948138]\n",
      "epoch:15 step:14077 [D loss: 0.631266, acc.: 58.59%] [G loss: 0.996690]\n",
      "epoch:15 step:14078 [D loss: 0.647581, acc.: 58.59%] [G loss: 0.926487]\n",
      "epoch:15 step:14079 [D loss: 0.653450, acc.: 58.59%] [G loss: 0.994974]\n",
      "epoch:15 step:14080 [D loss: 0.659870, acc.: 60.94%] [G loss: 0.921561]\n",
      "epoch:15 step:14081 [D loss: 0.675360, acc.: 54.69%] [G loss: 0.924493]\n",
      "epoch:15 step:14082 [D loss: 0.646161, acc.: 64.06%] [G loss: 0.941853]\n",
      "epoch:15 step:14083 [D loss: 0.644556, acc.: 59.38%] [G loss: 0.970727]\n",
      "epoch:15 step:14084 [D loss: 0.690431, acc.: 50.78%] [G loss: 0.994742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14085 [D loss: 0.662733, acc.: 53.12%] [G loss: 1.052778]\n",
      "epoch:15 step:14086 [D loss: 0.638621, acc.: 63.28%] [G loss: 0.915241]\n",
      "epoch:15 step:14087 [D loss: 0.624624, acc.: 67.97%] [G loss: 0.993472]\n",
      "epoch:15 step:14088 [D loss: 0.687229, acc.: 55.47%] [G loss: 0.945349]\n",
      "epoch:15 step:14089 [D loss: 0.614228, acc.: 62.50%] [G loss: 0.823567]\n",
      "epoch:15 step:14090 [D loss: 0.639982, acc.: 66.41%] [G loss: 0.773180]\n",
      "epoch:15 step:14091 [D loss: 0.603586, acc.: 75.00%] [G loss: 0.786008]\n",
      "epoch:15 step:14092 [D loss: 0.665220, acc.: 57.81%] [G loss: 0.834235]\n",
      "epoch:15 step:14093 [D loss: 0.682381, acc.: 60.16%] [G loss: 0.933779]\n",
      "epoch:15 step:14094 [D loss: 0.729070, acc.: 52.34%] [G loss: 0.912685]\n",
      "epoch:15 step:14095 [D loss: 0.731255, acc.: 46.88%] [G loss: 1.018248]\n",
      "epoch:15 step:14096 [D loss: 0.680080, acc.: 53.12%] [G loss: 0.929347]\n",
      "epoch:15 step:14097 [D loss: 0.733816, acc.: 38.28%] [G loss: 0.826068]\n",
      "epoch:15 step:14098 [D loss: 0.628897, acc.: 70.31%] [G loss: 0.788641]\n",
      "epoch:15 step:14099 [D loss: 0.655408, acc.: 61.72%] [G loss: 0.883898]\n",
      "epoch:15 step:14100 [D loss: 0.701793, acc.: 54.69%] [G loss: 0.820298]\n",
      "epoch:15 step:14101 [D loss: 0.657314, acc.: 49.22%] [G loss: 0.797678]\n",
      "epoch:15 step:14102 [D loss: 0.643913, acc.: 60.94%] [G loss: 0.853842]\n",
      "epoch:15 step:14103 [D loss: 0.661339, acc.: 54.69%] [G loss: 0.817894]\n",
      "epoch:15 step:14104 [D loss: 0.604880, acc.: 65.62%] [G loss: 0.963786]\n",
      "epoch:15 step:14105 [D loss: 0.621095, acc.: 68.75%] [G loss: 0.770972]\n",
      "epoch:15 step:14106 [D loss: 0.634129, acc.: 65.62%] [G loss: 0.839877]\n",
      "epoch:15 step:14107 [D loss: 0.612298, acc.: 67.97%] [G loss: 0.892123]\n",
      "epoch:15 step:14108 [D loss: 0.623229, acc.: 67.97%] [G loss: 0.865739]\n",
      "epoch:15 step:14109 [D loss: 0.627844, acc.: 64.06%] [G loss: 0.914874]\n",
      "epoch:15 step:14110 [D loss: 0.600006, acc.: 73.44%] [G loss: 0.886021]\n",
      "epoch:15 step:14111 [D loss: 0.667776, acc.: 61.72%] [G loss: 0.745461]\n",
      "epoch:15 step:14112 [D loss: 0.724669, acc.: 43.75%] [G loss: 0.867106]\n",
      "epoch:15 step:14113 [D loss: 0.644847, acc.: 67.19%] [G loss: 0.862620]\n",
      "epoch:15 step:14114 [D loss: 0.700870, acc.: 58.59%] [G loss: 0.692365]\n",
      "epoch:15 step:14115 [D loss: 0.648435, acc.: 62.50%] [G loss: 0.870101]\n",
      "epoch:15 step:14116 [D loss: 0.691447, acc.: 57.03%] [G loss: 0.863528]\n",
      "epoch:15 step:14117 [D loss: 0.571472, acc.: 75.78%] [G loss: 0.962855]\n",
      "epoch:15 step:14118 [D loss: 0.626384, acc.: 69.53%] [G loss: 0.979071]\n",
      "epoch:15 step:14119 [D loss: 0.612598, acc.: 69.53%] [G loss: 1.042059]\n",
      "epoch:15 step:14120 [D loss: 0.620157, acc.: 62.50%] [G loss: 0.927886]\n",
      "epoch:15 step:14121 [D loss: 0.646355, acc.: 59.38%] [G loss: 0.845375]\n",
      "epoch:15 step:14122 [D loss: 0.631072, acc.: 65.62%] [G loss: 0.997413]\n",
      "epoch:15 step:14123 [D loss: 0.716385, acc.: 47.66%] [G loss: 0.837319]\n",
      "epoch:15 step:14124 [D loss: 0.662008, acc.: 57.03%] [G loss: 0.831798]\n",
      "epoch:15 step:14125 [D loss: 0.643031, acc.: 62.50%] [G loss: 0.819208]\n",
      "epoch:15 step:14126 [D loss: 0.681779, acc.: 55.47%] [G loss: 0.955475]\n",
      "epoch:15 step:14127 [D loss: 0.608328, acc.: 70.31%] [G loss: 1.000500]\n",
      "epoch:15 step:14128 [D loss: 0.665039, acc.: 57.03%] [G loss: 0.938063]\n",
      "epoch:15 step:14129 [D loss: 0.629363, acc.: 57.81%] [G loss: 1.075646]\n",
      "epoch:15 step:14130 [D loss: 0.628222, acc.: 64.84%] [G loss: 1.073329]\n",
      "epoch:15 step:14131 [D loss: 0.711742, acc.: 48.44%] [G loss: 0.910766]\n",
      "epoch:15 step:14132 [D loss: 0.625177, acc.: 67.19%] [G loss: 0.953678]\n",
      "epoch:15 step:14133 [D loss: 0.553921, acc.: 80.47%] [G loss: 0.849059]\n",
      "epoch:15 step:14134 [D loss: 0.677786, acc.: 55.47%] [G loss: 0.907961]\n",
      "epoch:15 step:14135 [D loss: 0.642972, acc.: 55.47%] [G loss: 0.936230]\n",
      "epoch:15 step:14136 [D loss: 0.611237, acc.: 69.53%] [G loss: 0.806129]\n",
      "epoch:15 step:14137 [D loss: 0.662217, acc.: 58.59%] [G loss: 0.957183]\n",
      "epoch:15 step:14138 [D loss: 0.692165, acc.: 52.34%] [G loss: 1.012250]\n",
      "epoch:15 step:14139 [D loss: 0.635915, acc.: 65.62%] [G loss: 0.849944]\n",
      "epoch:15 step:14140 [D loss: 0.709258, acc.: 51.56%] [G loss: 1.039990]\n",
      "epoch:15 step:14141 [D loss: 0.621601, acc.: 70.31%] [G loss: 0.839469]\n",
      "epoch:15 step:14142 [D loss: 0.573516, acc.: 71.88%] [G loss: 0.888470]\n",
      "epoch:15 step:14143 [D loss: 0.659456, acc.: 58.59%] [G loss: 0.822049]\n",
      "epoch:15 step:14144 [D loss: 0.580100, acc.: 72.66%] [G loss: 1.003038]\n",
      "epoch:15 step:14145 [D loss: 0.751661, acc.: 44.53%] [G loss: 0.855617]\n",
      "epoch:15 step:14146 [D loss: 0.536945, acc.: 68.75%] [G loss: 0.828451]\n",
      "epoch:15 step:14147 [D loss: 0.691440, acc.: 60.16%] [G loss: 0.811609]\n",
      "epoch:15 step:14148 [D loss: 0.567565, acc.: 73.44%] [G loss: 0.883329]\n",
      "epoch:15 step:14149 [D loss: 0.693829, acc.: 53.91%] [G loss: 0.989834]\n",
      "epoch:15 step:14150 [D loss: 0.639963, acc.: 64.84%] [G loss: 0.895677]\n",
      "epoch:15 step:14151 [D loss: 0.565334, acc.: 82.81%] [G loss: 0.910912]\n",
      "epoch:15 step:14152 [D loss: 0.644880, acc.: 64.84%] [G loss: 0.917151]\n",
      "epoch:15 step:14153 [D loss: 0.724265, acc.: 50.00%] [G loss: 0.844564]\n",
      "epoch:15 step:14154 [D loss: 0.540976, acc.: 73.44%] [G loss: 0.887941]\n",
      "epoch:15 step:14155 [D loss: 0.709283, acc.: 49.22%] [G loss: 0.797109]\n",
      "epoch:15 step:14156 [D loss: 0.648839, acc.: 64.84%] [G loss: 0.784009]\n",
      "epoch:15 step:14157 [D loss: 0.648153, acc.: 63.28%] [G loss: 0.908397]\n",
      "epoch:15 step:14158 [D loss: 0.693934, acc.: 41.41%] [G loss: 0.814557]\n",
      "epoch:15 step:14159 [D loss: 0.705751, acc.: 42.97%] [G loss: 0.771131]\n",
      "epoch:15 step:14160 [D loss: 0.673089, acc.: 49.22%] [G loss: 0.902916]\n",
      "epoch:15 step:14161 [D loss: 0.587189, acc.: 77.34%] [G loss: 0.733012]\n",
      "epoch:15 step:14162 [D loss: 0.670043, acc.: 60.94%] [G loss: 0.791333]\n",
      "epoch:15 step:14163 [D loss: 0.651577, acc.: 57.03%] [G loss: 0.820452]\n",
      "epoch:15 step:14164 [D loss: 0.634729, acc.: 61.72%] [G loss: 0.840019]\n",
      "epoch:15 step:14165 [D loss: 0.663465, acc.: 57.81%] [G loss: 0.955673]\n",
      "epoch:15 step:14166 [D loss: 0.680022, acc.: 54.69%] [G loss: 1.021016]\n",
      "epoch:15 step:14167 [D loss: 0.644183, acc.: 70.31%] [G loss: 0.835720]\n",
      "epoch:15 step:14168 [D loss: 0.626403, acc.: 60.16%] [G loss: 0.817910]\n",
      "epoch:15 step:14169 [D loss: 0.584285, acc.: 75.00%] [G loss: 0.880813]\n",
      "epoch:15 step:14170 [D loss: 0.629897, acc.: 67.97%] [G loss: 0.832782]\n",
      "epoch:15 step:14171 [D loss: 0.623828, acc.: 67.97%] [G loss: 0.954856]\n",
      "epoch:15 step:14172 [D loss: 0.651459, acc.: 65.62%] [G loss: 0.938712]\n",
      "epoch:15 step:14173 [D loss: 0.584247, acc.: 71.88%] [G loss: 1.001020]\n",
      "epoch:15 step:14174 [D loss: 0.579838, acc.: 67.97%] [G loss: 0.818054]\n",
      "epoch:15 step:14175 [D loss: 0.713580, acc.: 52.34%] [G loss: 1.030581]\n",
      "epoch:15 step:14176 [D loss: 0.541504, acc.: 81.25%] [G loss: 0.875666]\n",
      "epoch:15 step:14177 [D loss: 0.661155, acc.: 55.47%] [G loss: 0.839715]\n",
      "epoch:15 step:14178 [D loss: 0.611436, acc.: 60.94%] [G loss: 0.867306]\n",
      "epoch:15 step:14179 [D loss: 0.596028, acc.: 71.09%] [G loss: 0.840215]\n",
      "epoch:15 step:14180 [D loss: 0.578241, acc.: 75.00%] [G loss: 0.937819]\n",
      "epoch:15 step:14181 [D loss: 0.919400, acc.: 16.41%] [G loss: 0.999506]\n",
      "epoch:15 step:14182 [D loss: 0.627716, acc.: 67.19%] [G loss: 1.143784]\n",
      "epoch:15 step:14183 [D loss: 0.584410, acc.: 72.66%] [G loss: 0.942491]\n",
      "epoch:15 step:14184 [D loss: 0.631072, acc.: 64.84%] [G loss: 0.816274]\n",
      "epoch:15 step:14185 [D loss: 0.709112, acc.: 49.22%] [G loss: 0.977997]\n",
      "epoch:15 step:14186 [D loss: 0.690008, acc.: 56.25%] [G loss: 0.828570]\n",
      "epoch:15 step:14187 [D loss: 0.707210, acc.: 55.47%] [G loss: 0.981054]\n",
      "epoch:15 step:14188 [D loss: 0.701073, acc.: 57.03%] [G loss: 0.752800]\n",
      "epoch:15 step:14189 [D loss: 0.752140, acc.: 39.84%] [G loss: 0.820142]\n",
      "epoch:15 step:14190 [D loss: 0.740041, acc.: 49.22%] [G loss: 0.872579]\n",
      "epoch:15 step:14191 [D loss: 0.748118, acc.: 46.88%] [G loss: 0.859260]\n",
      "epoch:15 step:14192 [D loss: 0.661771, acc.: 58.59%] [G loss: 0.903549]\n",
      "epoch:15 step:14193 [D loss: 0.665497, acc.: 59.38%] [G loss: 0.932658]\n",
      "epoch:15 step:14194 [D loss: 0.615695, acc.: 63.28%] [G loss: 0.978717]\n",
      "epoch:15 step:14195 [D loss: 0.636312, acc.: 66.41%] [G loss: 0.813626]\n",
      "epoch:15 step:14196 [D loss: 0.606841, acc.: 67.19%] [G loss: 0.908972]\n",
      "epoch:15 step:14197 [D loss: 0.656868, acc.: 57.03%] [G loss: 0.813372]\n",
      "epoch:15 step:14198 [D loss: 0.575041, acc.: 71.09%] [G loss: 0.828164]\n",
      "epoch:15 step:14199 [D loss: 0.577951, acc.: 75.78%] [G loss: 0.822510]\n",
      "epoch:15 step:14200 [D loss: 0.586653, acc.: 72.66%] [G loss: 0.753744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14201 [D loss: 0.652907, acc.: 58.59%] [G loss: 0.951678]\n",
      "epoch:15 step:14202 [D loss: 0.624098, acc.: 60.94%] [G loss: 0.911675]\n",
      "epoch:15 step:14203 [D loss: 0.647197, acc.: 57.03%] [G loss: 0.866565]\n",
      "epoch:15 step:14204 [D loss: 0.639885, acc.: 57.81%] [G loss: 0.698851]\n",
      "epoch:15 step:14205 [D loss: 0.647503, acc.: 65.62%] [G loss: 0.949975]\n",
      "epoch:15 step:14206 [D loss: 0.799417, acc.: 39.06%] [G loss: 0.822921]\n",
      "epoch:15 step:14207 [D loss: 0.692003, acc.: 57.03%] [G loss: 0.997086]\n",
      "epoch:15 step:14208 [D loss: 0.674656, acc.: 64.06%] [G loss: 0.935545]\n",
      "epoch:15 step:14209 [D loss: 0.658608, acc.: 53.91%] [G loss: 0.874103]\n",
      "epoch:15 step:14210 [D loss: 0.719280, acc.: 50.78%] [G loss: 0.731599]\n",
      "epoch:15 step:14211 [D loss: 0.636168, acc.: 60.94%] [G loss: 0.831636]\n",
      "epoch:15 step:14212 [D loss: 0.700056, acc.: 50.78%] [G loss: 0.993382]\n",
      "epoch:15 step:14213 [D loss: 0.661689, acc.: 59.38%] [G loss: 0.863006]\n",
      "epoch:15 step:14214 [D loss: 0.712605, acc.: 52.34%] [G loss: 0.812310]\n",
      "epoch:15 step:14215 [D loss: 0.647695, acc.: 60.94%] [G loss: 1.198745]\n",
      "epoch:15 step:14216 [D loss: 0.685164, acc.: 53.91%] [G loss: 0.879847]\n",
      "epoch:15 step:14217 [D loss: 0.603133, acc.: 71.88%] [G loss: 0.888730]\n",
      "epoch:15 step:14218 [D loss: 0.665811, acc.: 57.81%] [G loss: 1.009591]\n",
      "epoch:15 step:14219 [D loss: 0.659959, acc.: 57.81%] [G loss: 0.904808]\n",
      "epoch:15 step:14220 [D loss: 0.670401, acc.: 61.72%] [G loss: 0.930602]\n",
      "epoch:15 step:14221 [D loss: 0.670300, acc.: 59.38%] [G loss: 1.087061]\n",
      "epoch:15 step:14222 [D loss: 0.644465, acc.: 66.41%] [G loss: 0.942249]\n",
      "epoch:15 step:14223 [D loss: 0.681023, acc.: 53.91%] [G loss: 0.952741]\n",
      "epoch:15 step:14224 [D loss: 0.711567, acc.: 53.91%] [G loss: 0.924510]\n",
      "epoch:15 step:14225 [D loss: 0.652475, acc.: 59.38%] [G loss: 0.911440]\n",
      "epoch:15 step:14226 [D loss: 0.682505, acc.: 53.91%] [G loss: 0.899970]\n",
      "epoch:15 step:14227 [D loss: 0.765714, acc.: 38.28%] [G loss: 0.842075]\n",
      "epoch:15 step:14228 [D loss: 0.590029, acc.: 72.66%] [G loss: 0.781948]\n",
      "epoch:15 step:14229 [D loss: 0.712877, acc.: 51.56%] [G loss: 0.989986]\n",
      "epoch:15 step:14230 [D loss: 0.657995, acc.: 58.59%] [G loss: 0.881026]\n",
      "epoch:15 step:14231 [D loss: 0.700797, acc.: 48.44%] [G loss: 0.968925]\n",
      "epoch:15 step:14232 [D loss: 0.619119, acc.: 64.84%] [G loss: 0.787864]\n",
      "epoch:15 step:14233 [D loss: 0.544565, acc.: 79.69%] [G loss: 0.858502]\n",
      "epoch:15 step:14234 [D loss: 0.616916, acc.: 70.31%] [G loss: 1.045092]\n",
      "epoch:15 step:14235 [D loss: 0.650397, acc.: 64.06%] [G loss: 0.879668]\n",
      "epoch:15 step:14236 [D loss: 0.716138, acc.: 54.69%] [G loss: 0.780301]\n",
      "epoch:15 step:14237 [D loss: 0.610835, acc.: 71.09%] [G loss: 0.845141]\n",
      "epoch:15 step:14238 [D loss: 0.665841, acc.: 53.12%] [G loss: 0.801808]\n",
      "epoch:15 step:14239 [D loss: 0.693185, acc.: 46.88%] [G loss: 0.821762]\n",
      "epoch:15 step:14240 [D loss: 0.689716, acc.: 52.34%] [G loss: 0.798390]\n",
      "epoch:15 step:14241 [D loss: 0.689415, acc.: 55.47%] [G loss: 0.861076]\n",
      "epoch:15 step:14242 [D loss: 0.650518, acc.: 59.38%] [G loss: 0.831933]\n",
      "epoch:15 step:14243 [D loss: 0.625906, acc.: 61.72%] [G loss: 0.956931]\n",
      "epoch:15 step:14244 [D loss: 0.659969, acc.: 57.81%] [G loss: 0.832001]\n",
      "epoch:15 step:14245 [D loss: 0.680335, acc.: 56.25%] [G loss: 0.862464]\n",
      "epoch:15 step:14246 [D loss: 0.593107, acc.: 73.44%] [G loss: 0.909402]\n",
      "epoch:15 step:14247 [D loss: 0.608421, acc.: 66.41%] [G loss: 0.890202]\n",
      "epoch:15 step:14248 [D loss: 0.671704, acc.: 56.25%] [G loss: 0.926524]\n",
      "epoch:15 step:14249 [D loss: 0.588832, acc.: 68.75%] [G loss: 0.849215]\n",
      "epoch:15 step:14250 [D loss: 0.621473, acc.: 64.06%] [G loss: 0.954969]\n",
      "epoch:15 step:14251 [D loss: 0.681416, acc.: 54.69%] [G loss: 0.896296]\n",
      "epoch:15 step:14252 [D loss: 0.671172, acc.: 59.38%] [G loss: 0.897844]\n",
      "epoch:15 step:14253 [D loss: 0.643436, acc.: 59.38%] [G loss: 0.781413]\n",
      "epoch:15 step:14254 [D loss: 0.633376, acc.: 67.19%] [G loss: 1.042964]\n",
      "epoch:15 step:14255 [D loss: 0.642760, acc.: 64.84%] [G loss: 0.705459]\n",
      "epoch:15 step:14256 [D loss: 0.741321, acc.: 48.44%] [G loss: 0.746740]\n",
      "epoch:15 step:14257 [D loss: 0.671079, acc.: 60.16%] [G loss: 0.753650]\n",
      "epoch:15 step:14258 [D loss: 0.678628, acc.: 57.03%] [G loss: 0.926020]\n",
      "epoch:15 step:14259 [D loss: 0.662527, acc.: 59.38%] [G loss: 0.926299]\n",
      "epoch:15 step:14260 [D loss: 0.606544, acc.: 71.09%] [G loss: 0.839373]\n",
      "epoch:15 step:14261 [D loss: 0.727270, acc.: 49.22%] [G loss: 0.811051]\n",
      "epoch:15 step:14262 [D loss: 0.799216, acc.: 42.19%] [G loss: 0.848866]\n",
      "epoch:15 step:14263 [D loss: 0.674003, acc.: 60.94%] [G loss: 0.778896]\n",
      "epoch:15 step:14264 [D loss: 0.739066, acc.: 45.31%] [G loss: 0.888022]\n",
      "epoch:15 step:14265 [D loss: 0.628041, acc.: 63.28%] [G loss: 0.916376]\n",
      "epoch:15 step:14266 [D loss: 0.636743, acc.: 66.41%] [G loss: 0.904796]\n",
      "epoch:15 step:14267 [D loss: 0.670548, acc.: 57.81%] [G loss: 0.945218]\n",
      "epoch:15 step:14268 [D loss: 0.698475, acc.: 51.56%] [G loss: 0.821457]\n",
      "epoch:15 step:14269 [D loss: 0.665435, acc.: 56.25%] [G loss: 0.867789]\n",
      "epoch:15 step:14270 [D loss: 0.663533, acc.: 61.72%] [G loss: 0.833806]\n",
      "epoch:15 step:14271 [D loss: 0.651353, acc.: 53.12%] [G loss: 1.019372]\n",
      "epoch:15 step:14272 [D loss: 0.712815, acc.: 53.12%] [G loss: 0.934409]\n",
      "epoch:15 step:14273 [D loss: 0.752454, acc.: 41.41%] [G loss: 0.832778]\n",
      "epoch:15 step:14274 [D loss: 0.643813, acc.: 66.41%] [G loss: 0.862829]\n",
      "epoch:15 step:14275 [D loss: 0.662713, acc.: 62.50%] [G loss: 0.908838]\n",
      "epoch:15 step:14276 [D loss: 0.607784, acc.: 67.97%] [G loss: 0.952307]\n",
      "epoch:15 step:14277 [D loss: 0.691469, acc.: 53.91%] [G loss: 1.006881]\n",
      "epoch:15 step:14278 [D loss: 0.679265, acc.: 52.34%] [G loss: 0.841532]\n",
      "epoch:15 step:14279 [D loss: 0.586764, acc.: 75.00%] [G loss: 1.048750]\n",
      "epoch:15 step:14280 [D loss: 0.627363, acc.: 65.62%] [G loss: 0.921157]\n",
      "epoch:15 step:14281 [D loss: 0.695903, acc.: 55.47%] [G loss: 0.896139]\n",
      "epoch:15 step:14282 [D loss: 0.608102, acc.: 67.19%] [G loss: 0.938396]\n",
      "epoch:15 step:14283 [D loss: 0.729194, acc.: 54.69%] [G loss: 0.918335]\n",
      "epoch:15 step:14284 [D loss: 0.630414, acc.: 64.84%] [G loss: 0.806215]\n",
      "epoch:15 step:14285 [D loss: 0.678863, acc.: 57.81%] [G loss: 0.976018]\n",
      "epoch:15 step:14286 [D loss: 0.600810, acc.: 70.31%] [G loss: 0.834722]\n",
      "epoch:15 step:14287 [D loss: 0.582293, acc.: 66.41%] [G loss: 0.876681]\n",
      "epoch:15 step:14288 [D loss: 0.762675, acc.: 43.75%] [G loss: 0.809307]\n",
      "epoch:15 step:14289 [D loss: 0.691226, acc.: 55.47%] [G loss: 0.921420]\n",
      "epoch:15 step:14290 [D loss: 0.660677, acc.: 59.38%] [G loss: 0.810559]\n",
      "epoch:15 step:14291 [D loss: 0.667029, acc.: 60.16%] [G loss: 0.956856]\n",
      "epoch:15 step:14292 [D loss: 0.670178, acc.: 57.81%] [G loss: 0.933982]\n",
      "epoch:15 step:14293 [D loss: 0.705104, acc.: 54.69%] [G loss: 0.858169]\n",
      "epoch:15 step:14294 [D loss: 0.665551, acc.: 61.72%] [G loss: 0.895288]\n",
      "epoch:15 step:14295 [D loss: 0.699070, acc.: 49.22%] [G loss: 0.892527]\n",
      "epoch:15 step:14296 [D loss: 0.665400, acc.: 56.25%] [G loss: 0.840185]\n",
      "epoch:15 step:14297 [D loss: 0.689443, acc.: 50.00%] [G loss: 0.888308]\n",
      "epoch:15 step:14298 [D loss: 0.615901, acc.: 66.41%] [G loss: 0.867107]\n",
      "epoch:15 step:14299 [D loss: 0.689798, acc.: 48.44%] [G loss: 0.918116]\n",
      "epoch:15 step:14300 [D loss: 0.698083, acc.: 53.12%] [G loss: 0.868037]\n",
      "epoch:15 step:14301 [D loss: 0.663026, acc.: 60.16%] [G loss: 0.877823]\n",
      "epoch:15 step:14302 [D loss: 0.670133, acc.: 51.56%] [G loss: 0.977368]\n",
      "epoch:15 step:14303 [D loss: 0.653756, acc.: 62.50%] [G loss: 0.942595]\n",
      "epoch:15 step:14304 [D loss: 0.540606, acc.: 74.22%] [G loss: 0.873234]\n",
      "epoch:15 step:14305 [D loss: 0.671420, acc.: 59.38%] [G loss: 0.864566]\n",
      "epoch:15 step:14306 [D loss: 0.625164, acc.: 67.19%] [G loss: 0.905568]\n",
      "epoch:15 step:14307 [D loss: 0.635464, acc.: 64.84%] [G loss: 0.947883]\n",
      "epoch:15 step:14308 [D loss: 0.682278, acc.: 59.38%] [G loss: 0.873900]\n",
      "epoch:15 step:14309 [D loss: 0.687816, acc.: 51.56%] [G loss: 0.879211]\n",
      "epoch:15 step:14310 [D loss: 0.630471, acc.: 67.19%] [G loss: 0.975958]\n",
      "epoch:15 step:14311 [D loss: 0.648369, acc.: 54.69%] [G loss: 0.878932]\n",
      "epoch:15 step:14312 [D loss: 0.642092, acc.: 67.19%] [G loss: 0.983704]\n",
      "epoch:15 step:14313 [D loss: 0.674934, acc.: 55.47%] [G loss: 1.008263]\n",
      "epoch:15 step:14314 [D loss: 0.666075, acc.: 55.47%] [G loss: 0.865915]\n",
      "epoch:15 step:14315 [D loss: 0.595516, acc.: 66.41%] [G loss: 0.987296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14316 [D loss: 0.633749, acc.: 68.75%] [G loss: 0.849430]\n",
      "epoch:15 step:14317 [D loss: 0.620420, acc.: 67.19%] [G loss: 0.843366]\n",
      "epoch:15 step:14318 [D loss: 0.578706, acc.: 71.88%] [G loss: 0.845610]\n",
      "epoch:15 step:14319 [D loss: 0.571556, acc.: 76.56%] [G loss: 0.931386]\n",
      "epoch:15 step:14320 [D loss: 0.662230, acc.: 57.81%] [G loss: 0.851990]\n",
      "epoch:15 step:14321 [D loss: 0.650457, acc.: 67.19%] [G loss: 0.977155]\n",
      "epoch:15 step:14322 [D loss: 0.601129, acc.: 70.31%] [G loss: 0.904512]\n",
      "epoch:15 step:14323 [D loss: 0.741261, acc.: 42.97%] [G loss: 0.928558]\n",
      "epoch:15 step:14324 [D loss: 0.689681, acc.: 49.22%] [G loss: 0.806274]\n",
      "epoch:15 step:14325 [D loss: 0.562771, acc.: 74.22%] [G loss: 0.737445]\n",
      "epoch:15 step:14326 [D loss: 0.554789, acc.: 80.47%] [G loss: 0.860397]\n",
      "epoch:15 step:14327 [D loss: 0.676276, acc.: 51.56%] [G loss: 0.865393]\n",
      "epoch:15 step:14328 [D loss: 0.498054, acc.: 86.72%] [G loss: 0.937002]\n",
      "epoch:15 step:14329 [D loss: 0.674767, acc.: 50.00%] [G loss: 0.760043]\n",
      "epoch:15 step:14330 [D loss: 0.578794, acc.: 74.22%] [G loss: 0.858646]\n",
      "epoch:15 step:14331 [D loss: 0.605206, acc.: 71.09%] [G loss: 0.954279]\n",
      "epoch:15 step:14332 [D loss: 0.762604, acc.: 41.41%] [G loss: 0.849959]\n",
      "epoch:15 step:14333 [D loss: 0.640736, acc.: 60.94%] [G loss: 0.868873]\n",
      "epoch:15 step:14334 [D loss: 0.616781, acc.: 66.41%] [G loss: 1.017001]\n",
      "epoch:15 step:14335 [D loss: 0.701598, acc.: 54.69%] [G loss: 0.845471]\n",
      "epoch:15 step:14336 [D loss: 0.721143, acc.: 50.78%] [G loss: 0.883829]\n",
      "epoch:15 step:14337 [D loss: 0.680699, acc.: 54.69%] [G loss: 0.937967]\n",
      "epoch:15 step:14338 [D loss: 0.641948, acc.: 58.59%] [G loss: 0.925212]\n",
      "epoch:15 step:14339 [D loss: 0.683245, acc.: 49.22%] [G loss: 1.032138]\n",
      "epoch:15 step:14340 [D loss: 0.668973, acc.: 57.03%] [G loss: 0.976498]\n",
      "epoch:15 step:14341 [D loss: 0.573851, acc.: 78.91%] [G loss: 1.025785]\n",
      "epoch:15 step:14342 [D loss: 0.616876, acc.: 70.31%] [G loss: 0.758466]\n",
      "epoch:15 step:14343 [D loss: 0.607971, acc.: 66.41%] [G loss: 0.798646]\n",
      "epoch:15 step:14344 [D loss: 0.681596, acc.: 55.47%] [G loss: 0.946922]\n",
      "epoch:15 step:14345 [D loss: 0.585567, acc.: 75.78%] [G loss: 0.942191]\n",
      "epoch:15 step:14346 [D loss: 0.571475, acc.: 71.09%] [G loss: 0.905808]\n",
      "epoch:15 step:14347 [D loss: 0.603075, acc.: 68.75%] [G loss: 1.156955]\n",
      "epoch:15 step:14348 [D loss: 0.708635, acc.: 50.00%] [G loss: 0.987962]\n",
      "epoch:15 step:14349 [D loss: 0.570094, acc.: 71.88%] [G loss: 0.914598]\n",
      "epoch:15 step:14350 [D loss: 0.590269, acc.: 73.44%] [G loss: 1.072911]\n",
      "epoch:15 step:14351 [D loss: 0.565173, acc.: 75.00%] [G loss: 0.984262]\n",
      "epoch:15 step:14352 [D loss: 0.510454, acc.: 76.56%] [G loss: 0.952478]\n",
      "epoch:15 step:14353 [D loss: 0.706422, acc.: 54.69%] [G loss: 0.856423]\n",
      "epoch:15 step:14354 [D loss: 0.688895, acc.: 55.47%] [G loss: 1.136827]\n",
      "epoch:15 step:14355 [D loss: 0.639073, acc.: 62.50%] [G loss: 1.043390]\n",
      "epoch:15 step:14356 [D loss: 0.564541, acc.: 71.09%] [G loss: 0.817976]\n",
      "epoch:15 step:14357 [D loss: 0.641299, acc.: 64.84%] [G loss: 1.047219]\n",
      "epoch:15 step:14358 [D loss: 0.685082, acc.: 58.59%] [G loss: 0.946432]\n",
      "epoch:15 step:14359 [D loss: 0.675379, acc.: 57.03%] [G loss: 1.036355]\n",
      "epoch:15 step:14360 [D loss: 0.692458, acc.: 51.56%] [G loss: 0.909752]\n",
      "epoch:15 step:14361 [D loss: 0.666344, acc.: 63.28%] [G loss: 0.994889]\n",
      "epoch:15 step:14362 [D loss: 0.583967, acc.: 71.88%] [G loss: 0.787520]\n",
      "epoch:15 step:14363 [D loss: 0.622185, acc.: 64.06%] [G loss: 0.990113]\n",
      "epoch:15 step:14364 [D loss: 0.655506, acc.: 60.16%] [G loss: 0.784258]\n",
      "epoch:15 step:14365 [D loss: 0.635227, acc.: 58.59%] [G loss: 0.847185]\n",
      "epoch:15 step:14366 [D loss: 0.594574, acc.: 66.41%] [G loss: 0.935877]\n",
      "epoch:15 step:14367 [D loss: 0.598756, acc.: 69.53%] [G loss: 0.943382]\n",
      "epoch:15 step:14368 [D loss: 0.617573, acc.: 60.16%] [G loss: 1.040291]\n",
      "epoch:15 step:14369 [D loss: 0.731692, acc.: 55.47%] [G loss: 0.913602]\n",
      "epoch:15 step:14370 [D loss: 0.673037, acc.: 56.25%] [G loss: 0.872236]\n",
      "epoch:15 step:14371 [D loss: 0.752625, acc.: 42.97%] [G loss: 0.916570]\n",
      "epoch:15 step:14372 [D loss: 0.709307, acc.: 50.78%] [G loss: 0.667787]\n",
      "epoch:15 step:14373 [D loss: 0.597067, acc.: 71.09%] [G loss: 0.876694]\n",
      "epoch:15 step:14374 [D loss: 0.636609, acc.: 67.19%] [G loss: 1.008856]\n",
      "epoch:15 step:14375 [D loss: 0.606683, acc.: 67.19%] [G loss: 0.914472]\n",
      "epoch:15 step:14376 [D loss: 0.707481, acc.: 47.66%] [G loss: 0.854100]\n",
      "epoch:15 step:14377 [D loss: 0.625914, acc.: 67.97%] [G loss: 0.951239]\n",
      "epoch:15 step:14378 [D loss: 0.718617, acc.: 50.78%] [G loss: 1.061051]\n",
      "epoch:15 step:14379 [D loss: 0.561991, acc.: 71.09%] [G loss: 0.894748]\n",
      "epoch:15 step:14380 [D loss: 0.679436, acc.: 59.38%] [G loss: 1.094470]\n",
      "epoch:15 step:14381 [D loss: 0.668127, acc.: 60.16%] [G loss: 0.985655]\n",
      "epoch:15 step:14382 [D loss: 0.633244, acc.: 67.97%] [G loss: 0.944418]\n",
      "epoch:15 step:14383 [D loss: 0.692561, acc.: 49.22%] [G loss: 0.871012]\n",
      "epoch:15 step:14384 [D loss: 0.626356, acc.: 60.16%] [G loss: 0.908886]\n",
      "epoch:15 step:14385 [D loss: 0.690172, acc.: 51.56%] [G loss: 0.888819]\n",
      "epoch:15 step:14386 [D loss: 0.670253, acc.: 59.38%] [G loss: 0.829277]\n",
      "epoch:15 step:14387 [D loss: 0.616949, acc.: 65.62%] [G loss: 0.987383]\n",
      "epoch:15 step:14388 [D loss: 0.691309, acc.: 50.00%] [G loss: 0.812748]\n",
      "epoch:15 step:14389 [D loss: 0.694216, acc.: 54.69%] [G loss: 0.739179]\n",
      "epoch:15 step:14390 [D loss: 0.668971, acc.: 52.34%] [G loss: 0.839701]\n",
      "epoch:15 step:14391 [D loss: 0.645963, acc.: 60.94%] [G loss: 0.839570]\n",
      "epoch:15 step:14392 [D loss: 0.648556, acc.: 57.81%] [G loss: 0.935957]\n",
      "epoch:15 step:14393 [D loss: 0.620143, acc.: 64.06%] [G loss: 0.793297]\n",
      "epoch:15 step:14394 [D loss: 0.606841, acc.: 60.94%] [G loss: 0.768636]\n",
      "epoch:15 step:14395 [D loss: 0.714938, acc.: 57.03%] [G loss: 0.937553]\n",
      "epoch:15 step:14396 [D loss: 0.674053, acc.: 64.06%] [G loss: 0.963384]\n",
      "epoch:15 step:14397 [D loss: 0.729428, acc.: 53.12%] [G loss: 0.908858]\n",
      "epoch:15 step:14398 [D loss: 0.680900, acc.: 52.34%] [G loss: 1.079199]\n",
      "epoch:15 step:14399 [D loss: 0.588827, acc.: 67.97%] [G loss: 0.993186]\n",
      "epoch:15 step:14400 [D loss: 0.718170, acc.: 50.00%] [G loss: 0.995003]\n",
      "epoch:15 step:14401 [D loss: 0.587496, acc.: 69.53%] [G loss: 0.838723]\n",
      "epoch:15 step:14402 [D loss: 0.688987, acc.: 60.94%] [G loss: 0.893911]\n",
      "epoch:15 step:14403 [D loss: 0.712998, acc.: 46.88%] [G loss: 0.918801]\n",
      "epoch:15 step:14404 [D loss: 0.688648, acc.: 54.69%] [G loss: 1.061285]\n",
      "epoch:15 step:14405 [D loss: 0.679287, acc.: 53.91%] [G loss: 0.862608]\n",
      "epoch:15 step:14406 [D loss: 0.644058, acc.: 58.59%] [G loss: 0.878517]\n",
      "epoch:15 step:14407 [D loss: 0.674702, acc.: 55.47%] [G loss: 0.956598]\n",
      "epoch:15 step:14408 [D loss: 0.696520, acc.: 45.31%] [G loss: 0.847094]\n",
      "epoch:15 step:14409 [D loss: 0.737025, acc.: 42.19%] [G loss: 0.752338]\n",
      "epoch:15 step:14410 [D loss: 0.623777, acc.: 67.19%] [G loss: 0.950144]\n",
      "epoch:15 step:14411 [D loss: 0.679763, acc.: 56.25%] [G loss: 0.817616]\n",
      "epoch:15 step:14412 [D loss: 0.645615, acc.: 60.94%] [G loss: 0.848708]\n",
      "epoch:15 step:14413 [D loss: 0.778720, acc.: 38.28%] [G loss: 1.043586]\n",
      "epoch:15 step:14414 [D loss: 0.716344, acc.: 57.03%] [G loss: 0.916401]\n",
      "epoch:15 step:14415 [D loss: 0.619995, acc.: 64.06%] [G loss: 0.922408]\n",
      "epoch:15 step:14416 [D loss: 0.692923, acc.: 54.69%] [G loss: 0.971742]\n",
      "epoch:15 step:14417 [D loss: 0.689560, acc.: 51.56%] [G loss: 0.861735]\n",
      "epoch:15 step:14418 [D loss: 0.619819, acc.: 59.38%] [G loss: 0.943997]\n",
      "epoch:15 step:14419 [D loss: 0.702984, acc.: 50.00%] [G loss: 0.947197]\n",
      "epoch:15 step:14420 [D loss: 0.589702, acc.: 75.78%] [G loss: 0.857289]\n",
      "epoch:15 step:14421 [D loss: 0.748585, acc.: 41.41%] [G loss: 1.043289]\n",
      "epoch:15 step:14422 [D loss: 0.620416, acc.: 71.88%] [G loss: 0.962563]\n",
      "epoch:15 step:14423 [D loss: 0.668565, acc.: 59.38%] [G loss: 0.955917]\n",
      "epoch:15 step:14424 [D loss: 0.655918, acc.: 64.84%] [G loss: 0.913487]\n",
      "epoch:15 step:14425 [D loss: 0.640991, acc.: 58.59%] [G loss: 0.867764]\n",
      "epoch:15 step:14426 [D loss: 0.641725, acc.: 64.84%] [G loss: 0.835136]\n",
      "epoch:15 step:14427 [D loss: 0.599564, acc.: 64.84%] [G loss: 0.813545]\n",
      "epoch:15 step:14428 [D loss: 0.633430, acc.: 68.75%] [G loss: 0.939668]\n",
      "epoch:15 step:14429 [D loss: 0.695433, acc.: 50.78%] [G loss: 1.079504]\n",
      "epoch:15 step:14430 [D loss: 0.686271, acc.: 52.34%] [G loss: 0.819488]\n",
      "epoch:15 step:14431 [D loss: 0.680906, acc.: 60.16%] [G loss: 0.955949]\n",
      "epoch:15 step:14432 [D loss: 0.654700, acc.: 67.19%] [G loss: 0.831523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14433 [D loss: 0.615587, acc.: 68.75%] [G loss: 0.800792]\n",
      "epoch:15 step:14434 [D loss: 0.737557, acc.: 49.22%] [G loss: 0.834823]\n",
      "epoch:15 step:14435 [D loss: 0.789175, acc.: 44.53%] [G loss: 0.673353]\n",
      "epoch:15 step:14436 [D loss: 0.632222, acc.: 63.28%] [G loss: 0.792985]\n",
      "epoch:15 step:14437 [D loss: 0.692539, acc.: 58.59%] [G loss: 0.802935]\n",
      "epoch:15 step:14438 [D loss: 0.674874, acc.: 51.56%] [G loss: 0.787950]\n",
      "epoch:15 step:14439 [D loss: 0.733127, acc.: 44.53%] [G loss: 0.871971]\n",
      "epoch:15 step:14440 [D loss: 0.701757, acc.: 59.38%] [G loss: 0.952817]\n",
      "epoch:15 step:14441 [D loss: 0.620618, acc.: 66.41%] [G loss: 0.997270]\n",
      "epoch:15 step:14442 [D loss: 0.633128, acc.: 63.28%] [G loss: 0.908888]\n",
      "epoch:15 step:14443 [D loss: 0.632782, acc.: 67.97%] [G loss: 0.822062]\n",
      "epoch:15 step:14444 [D loss: 0.688433, acc.: 56.25%] [G loss: 0.829140]\n",
      "epoch:15 step:14445 [D loss: 0.701477, acc.: 53.12%] [G loss: 0.771750]\n",
      "epoch:15 step:14446 [D loss: 0.673388, acc.: 57.81%] [G loss: 0.873032]\n",
      "epoch:15 step:14447 [D loss: 0.726033, acc.: 48.44%] [G loss: 0.789927]\n",
      "epoch:15 step:14448 [D loss: 0.663205, acc.: 54.69%] [G loss: 0.923142]\n",
      "epoch:15 step:14449 [D loss: 0.739200, acc.: 42.97%] [G loss: 0.737910]\n",
      "epoch:15 step:14450 [D loss: 0.633724, acc.: 64.06%] [G loss: 0.762549]\n",
      "epoch:15 step:14451 [D loss: 0.656176, acc.: 61.72%] [G loss: 0.909160]\n",
      "epoch:15 step:14452 [D loss: 0.727578, acc.: 47.66%] [G loss: 0.850067]\n",
      "epoch:15 step:14453 [D loss: 0.664535, acc.: 57.03%] [G loss: 0.958130]\n",
      "epoch:15 step:14454 [D loss: 0.675946, acc.: 53.91%] [G loss: 0.801796]\n",
      "epoch:15 step:14455 [D loss: 0.631979, acc.: 60.94%] [G loss: 0.776401]\n",
      "epoch:15 step:14456 [D loss: 0.726336, acc.: 43.75%] [G loss: 0.776309]\n",
      "epoch:15 step:14457 [D loss: 0.583492, acc.: 71.88%] [G loss: 0.827961]\n",
      "epoch:15 step:14458 [D loss: 0.689060, acc.: 55.47%] [G loss: 0.825506]\n",
      "epoch:15 step:14459 [D loss: 0.581308, acc.: 75.00%] [G loss: 0.826582]\n",
      "epoch:15 step:14460 [D loss: 0.670696, acc.: 61.72%] [G loss: 0.882802]\n",
      "epoch:15 step:14461 [D loss: 0.628053, acc.: 65.62%] [G loss: 0.839868]\n",
      "epoch:15 step:14462 [D loss: 0.762508, acc.: 39.84%] [G loss: 0.864015]\n",
      "epoch:15 step:14463 [D loss: 0.614901, acc.: 67.97%] [G loss: 0.848487]\n",
      "epoch:15 step:14464 [D loss: 0.618907, acc.: 64.84%] [G loss: 0.780761]\n",
      "epoch:15 step:14465 [D loss: 0.686677, acc.: 57.03%] [G loss: 0.944423]\n",
      "epoch:15 step:14466 [D loss: 0.660960, acc.: 63.28%] [G loss: 0.762974]\n",
      "epoch:15 step:14467 [D loss: 0.666126, acc.: 60.94%] [G loss: 0.883957]\n",
      "epoch:15 step:14468 [D loss: 0.737396, acc.: 52.34%] [G loss: 0.760579]\n",
      "epoch:15 step:14469 [D loss: 0.617591, acc.: 68.75%] [G loss: 0.864363]\n",
      "epoch:15 step:14470 [D loss: 0.629402, acc.: 65.62%] [G loss: 0.895248]\n",
      "epoch:15 step:14471 [D loss: 0.658732, acc.: 53.91%] [G loss: 0.906327]\n",
      "epoch:15 step:14472 [D loss: 0.678109, acc.: 60.94%] [G loss: 0.929661]\n",
      "epoch:15 step:14473 [D loss: 0.624582, acc.: 68.75%] [G loss: 0.779496]\n",
      "epoch:15 step:14474 [D loss: 0.709171, acc.: 49.22%] [G loss: 0.873962]\n",
      "epoch:15 step:14475 [D loss: 0.694340, acc.: 55.47%] [G loss: 0.777558]\n",
      "epoch:15 step:14476 [D loss: 0.651144, acc.: 61.72%] [G loss: 0.884112]\n",
      "epoch:15 step:14477 [D loss: 0.650316, acc.: 60.16%] [G loss: 0.764476]\n",
      "epoch:15 step:14478 [D loss: 0.686474, acc.: 52.34%] [G loss: 0.821784]\n",
      "epoch:15 step:14479 [D loss: 0.627068, acc.: 69.53%] [G loss: 0.964899]\n",
      "epoch:15 step:14480 [D loss: 0.677508, acc.: 58.59%] [G loss: 0.772290]\n",
      "epoch:15 step:14481 [D loss: 0.683185, acc.: 53.91%] [G loss: 0.771899]\n",
      "epoch:15 step:14482 [D loss: 0.620901, acc.: 66.41%] [G loss: 0.894272]\n",
      "epoch:15 step:14483 [D loss: 0.742674, acc.: 46.09%] [G loss: 0.870124]\n",
      "epoch:15 step:14484 [D loss: 0.652001, acc.: 58.59%] [G loss: 0.823041]\n",
      "epoch:15 step:14485 [D loss: 0.702917, acc.: 47.66%] [G loss: 0.840669]\n",
      "epoch:15 step:14486 [D loss: 0.658483, acc.: 57.81%] [G loss: 0.864312]\n",
      "epoch:15 step:14487 [D loss: 0.710427, acc.: 47.66%] [G loss: 0.852541]\n",
      "epoch:15 step:14488 [D loss: 0.685451, acc.: 54.69%] [G loss: 0.840156]\n",
      "epoch:15 step:14489 [D loss: 0.690186, acc.: 42.97%] [G loss: 0.811715]\n",
      "epoch:15 step:14490 [D loss: 0.627572, acc.: 64.84%] [G loss: 0.858014]\n",
      "epoch:15 step:14491 [D loss: 0.654156, acc.: 62.50%] [G loss: 0.896502]\n",
      "epoch:15 step:14492 [D loss: 0.690313, acc.: 57.03%] [G loss: 0.827330]\n",
      "epoch:15 step:14493 [D loss: 0.639388, acc.: 60.94%] [G loss: 1.019342]\n",
      "epoch:15 step:14494 [D loss: 0.614584, acc.: 67.19%] [G loss: 0.936227]\n",
      "epoch:15 step:14495 [D loss: 0.622825, acc.: 67.19%] [G loss: 0.919379]\n",
      "epoch:15 step:14496 [D loss: 0.659039, acc.: 60.94%] [G loss: 0.829073]\n",
      "epoch:15 step:14497 [D loss: 0.656485, acc.: 60.94%] [G loss: 0.937627]\n",
      "epoch:15 step:14498 [D loss: 0.667021, acc.: 58.59%] [G loss: 0.795298]\n",
      "epoch:15 step:14499 [D loss: 0.688705, acc.: 50.00%] [G loss: 0.827526]\n",
      "epoch:15 step:14500 [D loss: 0.641596, acc.: 62.50%] [G loss: 0.878424]\n",
      "epoch:15 step:14501 [D loss: 0.677245, acc.: 57.81%] [G loss: 1.000235]\n",
      "epoch:15 step:14502 [D loss: 0.718522, acc.: 54.69%] [G loss: 0.871940]\n",
      "epoch:15 step:14503 [D loss: 0.548188, acc.: 66.41%] [G loss: 0.945581]\n",
      "epoch:15 step:14504 [D loss: 0.621383, acc.: 68.75%] [G loss: 0.758826]\n",
      "epoch:15 step:14505 [D loss: 0.650420, acc.: 65.62%] [G loss: 0.904578]\n",
      "epoch:15 step:14506 [D loss: 0.647563, acc.: 60.94%] [G loss: 0.731559]\n",
      "epoch:15 step:14507 [D loss: 0.667368, acc.: 50.78%] [G loss: 0.914853]\n",
      "epoch:15 step:14508 [D loss: 0.652487, acc.: 64.84%] [G loss: 0.877193]\n",
      "epoch:15 step:14509 [D loss: 0.619107, acc.: 60.94%] [G loss: 0.814576]\n",
      "epoch:15 step:14510 [D loss: 0.658229, acc.: 54.69%] [G loss: 0.741799]\n",
      "epoch:15 step:14511 [D loss: 0.628607, acc.: 65.62%] [G loss: 0.817924]\n",
      "epoch:15 step:14512 [D loss: 0.637097, acc.: 61.72%] [G loss: 0.959480]\n",
      "epoch:15 step:14513 [D loss: 0.603961, acc.: 68.75%] [G loss: 0.969620]\n",
      "epoch:15 step:14514 [D loss: 0.654715, acc.: 57.03%] [G loss: 0.926800]\n",
      "epoch:15 step:14515 [D loss: 0.565615, acc.: 73.44%] [G loss: 0.942301]\n",
      "epoch:15 step:14516 [D loss: 0.722810, acc.: 47.66%] [G loss: 0.898865]\n",
      "epoch:15 step:14517 [D loss: 0.590723, acc.: 72.66%] [G loss: 0.917658]\n",
      "epoch:15 step:14518 [D loss: 0.671995, acc.: 56.25%] [G loss: 0.906668]\n",
      "epoch:15 step:14519 [D loss: 0.681463, acc.: 53.91%] [G loss: 0.823706]\n",
      "epoch:15 step:14520 [D loss: 0.680470, acc.: 55.47%] [G loss: 1.034313]\n",
      "epoch:15 step:14521 [D loss: 0.576730, acc.: 78.91%] [G loss: 0.865470]\n",
      "epoch:15 step:14522 [D loss: 0.640891, acc.: 60.16%] [G loss: 0.950211]\n",
      "epoch:15 step:14523 [D loss: 0.625214, acc.: 64.06%] [G loss: 0.868366]\n",
      "epoch:15 step:14524 [D loss: 0.624953, acc.: 60.16%] [G loss: 1.094185]\n",
      "epoch:15 step:14525 [D loss: 0.692588, acc.: 60.94%] [G loss: 0.774528]\n",
      "epoch:15 step:14526 [D loss: 0.673861, acc.: 55.47%] [G loss: 0.830371]\n",
      "epoch:15 step:14527 [D loss: 0.621006, acc.: 70.31%] [G loss: 0.892814]\n",
      "epoch:15 step:14528 [D loss: 0.573476, acc.: 74.22%] [G loss: 0.824531]\n",
      "epoch:15 step:14529 [D loss: 0.605692, acc.: 66.41%] [G loss: 1.241438]\n",
      "epoch:15 step:14530 [D loss: 0.545143, acc.: 76.56%] [G loss: 1.163190]\n",
      "epoch:15 step:14531 [D loss: 0.583004, acc.: 72.66%] [G loss: 1.162713]\n",
      "epoch:15 step:14532 [D loss: 0.475260, acc.: 67.97%] [G loss: 1.004427]\n",
      "epoch:15 step:14533 [D loss: 0.658441, acc.: 65.62%] [G loss: 0.900833]\n",
      "epoch:15 step:14534 [D loss: 0.700383, acc.: 55.47%] [G loss: 0.919170]\n",
      "epoch:15 step:14535 [D loss: 0.657750, acc.: 64.84%] [G loss: 0.973468]\n",
      "epoch:15 step:14536 [D loss: 0.633526, acc.: 65.62%] [G loss: 0.877422]\n",
      "epoch:15 step:14537 [D loss: 0.498159, acc.: 84.38%] [G loss: 0.968717]\n",
      "epoch:15 step:14538 [D loss: 0.673610, acc.: 64.84%] [G loss: 0.909160]\n",
      "epoch:15 step:14539 [D loss: 0.580445, acc.: 69.53%] [G loss: 0.996818]\n",
      "epoch:15 step:14540 [D loss: 0.570941, acc.: 75.00%] [G loss: 0.782553]\n",
      "epoch:15 step:14541 [D loss: 0.610056, acc.: 67.19%] [G loss: 1.101605]\n",
      "epoch:15 step:14542 [D loss: 0.631105, acc.: 64.84%] [G loss: 1.007485]\n",
      "epoch:15 step:14543 [D loss: 0.581530, acc.: 77.34%] [G loss: 0.799620]\n",
      "epoch:15 step:14544 [D loss: 0.633702, acc.: 60.16%] [G loss: 0.955214]\n",
      "epoch:15 step:14545 [D loss: 0.678573, acc.: 53.12%] [G loss: 0.924531]\n",
      "epoch:15 step:14546 [D loss: 0.713866, acc.: 52.34%] [G loss: 0.925313]\n",
      "epoch:15 step:14547 [D loss: 0.590210, acc.: 73.44%] [G loss: 1.142275]\n",
      "epoch:15 step:14548 [D loss: 0.595556, acc.: 65.62%] [G loss: 1.008072]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14549 [D loss: 0.588454, acc.: 67.97%] [G loss: 0.862260]\n",
      "epoch:15 step:14550 [D loss: 0.658987, acc.: 62.50%] [G loss: 0.798579]\n",
      "epoch:15 step:14551 [D loss: 0.649318, acc.: 60.94%] [G loss: 0.768264]\n",
      "epoch:15 step:14552 [D loss: 0.611771, acc.: 67.19%] [G loss: 0.876118]\n",
      "epoch:15 step:14553 [D loss: 0.650358, acc.: 65.62%] [G loss: 1.048049]\n",
      "epoch:15 step:14554 [D loss: 0.687029, acc.: 53.12%] [G loss: 0.847054]\n",
      "epoch:15 step:14555 [D loss: 0.637550, acc.: 64.06%] [G loss: 0.967656]\n",
      "epoch:15 step:14556 [D loss: 0.742864, acc.: 42.19%] [G loss: 0.970433]\n",
      "epoch:15 step:14557 [D loss: 0.617130, acc.: 64.06%] [G loss: 0.988623]\n",
      "epoch:15 step:14558 [D loss: 0.809399, acc.: 36.72%] [G loss: 0.823142]\n",
      "epoch:15 step:14559 [D loss: 0.703157, acc.: 53.12%] [G loss: 0.748732]\n",
      "epoch:15 step:14560 [D loss: 0.607924, acc.: 64.84%] [G loss: 0.780192]\n",
      "epoch:15 step:14561 [D loss: 0.686208, acc.: 53.12%] [G loss: 0.848686]\n",
      "epoch:15 step:14562 [D loss: 0.736026, acc.: 45.31%] [G loss: 0.972925]\n",
      "epoch:15 step:14563 [D loss: 0.636146, acc.: 60.94%] [G loss: 0.825153]\n",
      "epoch:15 step:14564 [D loss: 0.727746, acc.: 52.34%] [G loss: 0.762606]\n",
      "epoch:15 step:14565 [D loss: 0.582633, acc.: 76.56%] [G loss: 0.876708]\n",
      "epoch:15 step:14566 [D loss: 0.687224, acc.: 56.25%] [G loss: 0.934601]\n",
      "epoch:15 step:14567 [D loss: 0.782334, acc.: 34.38%] [G loss: 0.964751]\n",
      "epoch:15 step:14568 [D loss: 0.657512, acc.: 67.97%] [G loss: 0.900119]\n",
      "epoch:15 step:14569 [D loss: 0.693227, acc.: 62.50%] [G loss: 0.772670]\n",
      "epoch:15 step:14570 [D loss: 0.632650, acc.: 69.53%] [G loss: 0.786706]\n",
      "epoch:15 step:14571 [D loss: 0.603960, acc.: 70.31%] [G loss: 1.004151]\n",
      "epoch:15 step:14572 [D loss: 0.706119, acc.: 51.56%] [G loss: 0.832917]\n",
      "epoch:15 step:14573 [D loss: 0.652695, acc.: 61.72%] [G loss: 0.871946]\n",
      "epoch:15 step:14574 [D loss: 0.665507, acc.: 60.16%] [G loss: 1.083648]\n",
      "epoch:15 step:14575 [D loss: 0.667039, acc.: 59.38%] [G loss: 0.816420]\n",
      "epoch:15 step:14576 [D loss: 0.607115, acc.: 62.50%] [G loss: 0.801156]\n",
      "epoch:15 step:14577 [D loss: 0.577412, acc.: 74.22%] [G loss: 0.829268]\n",
      "epoch:15 step:14578 [D loss: 0.614603, acc.: 68.75%] [G loss: 0.834123]\n",
      "epoch:15 step:14579 [D loss: 0.547118, acc.: 76.56%] [G loss: 0.850903]\n",
      "epoch:15 step:14580 [D loss: 0.743479, acc.: 46.88%] [G loss: 0.772299]\n",
      "epoch:15 step:14581 [D loss: 0.684598, acc.: 53.91%] [G loss: 0.978013]\n",
      "epoch:15 step:14582 [D loss: 0.587853, acc.: 76.56%] [G loss: 0.927153]\n",
      "epoch:15 step:14583 [D loss: 0.632030, acc.: 66.41%] [G loss: 0.780385]\n",
      "epoch:15 step:14584 [D loss: 0.638947, acc.: 57.03%] [G loss: 0.973800]\n",
      "epoch:15 step:14585 [D loss: 0.546126, acc.: 82.03%] [G loss: 0.634186]\n",
      "epoch:15 step:14586 [D loss: 0.585715, acc.: 70.31%] [G loss: 0.908296]\n",
      "epoch:15 step:14587 [D loss: 0.599116, acc.: 68.75%] [G loss: 0.956037]\n",
      "epoch:15 step:14588 [D loss: 0.664753, acc.: 56.25%] [G loss: 0.848947]\n",
      "epoch:15 step:14589 [D loss: 0.605746, acc.: 67.19%] [G loss: 1.034367]\n",
      "epoch:15 step:14590 [D loss: 0.636129, acc.: 60.94%] [G loss: 0.804275]\n",
      "epoch:15 step:14591 [D loss: 0.620132, acc.: 66.41%] [G loss: 0.893528]\n",
      "epoch:15 step:14592 [D loss: 0.672432, acc.: 57.03%] [G loss: 0.808255]\n",
      "epoch:15 step:14593 [D loss: 0.614698, acc.: 68.75%] [G loss: 0.926212]\n",
      "epoch:15 step:14594 [D loss: 0.633960, acc.: 67.19%] [G loss: 1.016738]\n",
      "epoch:15 step:14595 [D loss: 0.684714, acc.: 56.25%] [G loss: 0.891105]\n",
      "epoch:15 step:14596 [D loss: 0.617089, acc.: 71.09%] [G loss: 0.998676]\n",
      "epoch:15 step:14597 [D loss: 0.655339, acc.: 64.06%] [G loss: 0.964858]\n",
      "epoch:15 step:14598 [D loss: 0.659929, acc.: 55.47%] [G loss: 1.015723]\n",
      "epoch:15 step:14599 [D loss: 0.566541, acc.: 70.31%] [G loss: 0.971722]\n",
      "epoch:15 step:14600 [D loss: 0.695411, acc.: 56.25%] [G loss: 0.948739]\n",
      "epoch:15 step:14601 [D loss: 0.614978, acc.: 64.06%] [G loss: 0.978791]\n",
      "epoch:15 step:14602 [D loss: 0.679395, acc.: 60.94%] [G loss: 1.040997]\n",
      "epoch:15 step:14603 [D loss: 0.722301, acc.: 44.53%] [G loss: 1.128905]\n",
      "epoch:15 step:14604 [D loss: 0.689663, acc.: 58.59%] [G loss: 1.204939]\n",
      "epoch:15 step:14605 [D loss: 0.770444, acc.: 42.19%] [G loss: 1.173602]\n",
      "epoch:15 step:14606 [D loss: 0.678707, acc.: 57.03%] [G loss: 0.939184]\n",
      "epoch:15 step:14607 [D loss: 0.656141, acc.: 64.84%] [G loss: 1.059996]\n",
      "epoch:15 step:14608 [D loss: 0.537163, acc.: 67.19%] [G loss: 1.007732]\n",
      "epoch:15 step:14609 [D loss: 0.733414, acc.: 50.00%] [G loss: 1.031093]\n",
      "epoch:15 step:14610 [D loss: 0.633817, acc.: 64.84%] [G loss: 0.962806]\n",
      "epoch:15 step:14611 [D loss: 0.715194, acc.: 50.78%] [G loss: 0.792315]\n",
      "epoch:15 step:14612 [D loss: 0.582396, acc.: 76.56%] [G loss: 0.763657]\n",
      "epoch:15 step:14613 [D loss: 0.680286, acc.: 62.50%] [G loss: 0.931122]\n",
      "epoch:15 step:14614 [D loss: 0.587244, acc.: 69.53%] [G loss: 1.028368]\n",
      "epoch:15 step:14615 [D loss: 0.702453, acc.: 54.69%] [G loss: 0.934162]\n",
      "epoch:15 step:14616 [D loss: 0.615132, acc.: 66.41%] [G loss: 0.887233]\n",
      "epoch:15 step:14617 [D loss: 0.752271, acc.: 41.41%] [G loss: 0.805354]\n",
      "epoch:15 step:14618 [D loss: 0.648538, acc.: 57.81%] [G loss: 0.762357]\n",
      "epoch:15 step:14619 [D loss: 0.738064, acc.: 47.66%] [G loss: 0.742448]\n",
      "epoch:15 step:14620 [D loss: 0.557230, acc.: 80.47%] [G loss: 0.943771]\n",
      "epoch:15 step:14621 [D loss: 0.728521, acc.: 47.66%] [G loss: 1.034979]\n",
      "epoch:15 step:14622 [D loss: 0.610972, acc.: 69.53%] [G loss: 0.841334]\n",
      "epoch:15 step:14623 [D loss: 0.542904, acc.: 78.91%] [G loss: 0.823115]\n",
      "epoch:15 step:14624 [D loss: 0.622672, acc.: 64.06%] [G loss: 1.100441]\n",
      "epoch:15 step:14625 [D loss: 0.640344, acc.: 65.62%] [G loss: 1.006841]\n",
      "epoch:15 step:14626 [D loss: 0.559997, acc.: 76.56%] [G loss: 0.753883]\n",
      "epoch:15 step:14627 [D loss: 0.631692, acc.: 64.06%] [G loss: 1.007070]\n",
      "epoch:15 step:14628 [D loss: 0.687666, acc.: 56.25%] [G loss: 1.058573]\n",
      "epoch:15 step:14629 [D loss: 0.786129, acc.: 37.50%] [G loss: 0.719360]\n",
      "epoch:15 step:14630 [D loss: 0.576802, acc.: 72.66%] [G loss: 1.121319]\n",
      "epoch:15 step:14631 [D loss: 0.667809, acc.: 57.81%] [G loss: 0.697024]\n",
      "epoch:15 step:14632 [D loss: 0.688139, acc.: 54.69%] [G loss: 0.659194]\n",
      "epoch:15 step:14633 [D loss: 0.544798, acc.: 77.34%] [G loss: 0.952871]\n",
      "epoch:15 step:14634 [D loss: 0.614240, acc.: 71.09%] [G loss: 0.754354]\n",
      "epoch:15 step:14635 [D loss: 0.755150, acc.: 41.41%] [G loss: 1.127395]\n",
      "epoch:15 step:14636 [D loss: 0.632789, acc.: 70.31%] [G loss: 0.913753]\n",
      "epoch:15 step:14637 [D loss: 0.674153, acc.: 56.25%] [G loss: 0.963074]\n",
      "epoch:15 step:14638 [D loss: 0.649064, acc.: 59.38%] [G loss: 0.797731]\n",
      "epoch:15 step:14639 [D loss: 0.598911, acc.: 67.19%] [G loss: 0.945897]\n",
      "epoch:15 step:14640 [D loss: 0.614872, acc.: 64.84%] [G loss: 0.887562]\n",
      "epoch:15 step:14641 [D loss: 0.606949, acc.: 67.19%] [G loss: 1.114628]\n",
      "epoch:15 step:14642 [D loss: 0.782167, acc.: 39.84%] [G loss: 0.969718]\n",
      "epoch:15 step:14643 [D loss: 0.588331, acc.: 62.50%] [G loss: 0.953838]\n",
      "epoch:15 step:14644 [D loss: 0.607320, acc.: 67.97%] [G loss: 0.930648]\n",
      "epoch:15 step:14645 [D loss: 0.618360, acc.: 64.06%] [G loss: 1.026298]\n",
      "epoch:15 step:14646 [D loss: 0.572327, acc.: 69.53%] [G loss: 0.903312]\n",
      "epoch:15 step:14647 [D loss: 0.598256, acc.: 71.88%] [G loss: 0.908363]\n",
      "epoch:15 step:14648 [D loss: 0.716286, acc.: 45.31%] [G loss: 0.859922]\n",
      "epoch:15 step:14649 [D loss: 0.621385, acc.: 68.75%] [G loss: 0.842746]\n",
      "epoch:15 step:14650 [D loss: 0.705567, acc.: 54.69%] [G loss: 0.751615]\n",
      "epoch:15 step:14651 [D loss: 0.601417, acc.: 70.31%] [G loss: 0.874958]\n",
      "epoch:15 step:14652 [D loss: 0.617276, acc.: 67.97%] [G loss: 1.014868]\n",
      "epoch:15 step:14653 [D loss: 0.629238, acc.: 60.94%] [G loss: 0.988285]\n",
      "epoch:15 step:14654 [D loss: 0.637624, acc.: 62.50%] [G loss: 0.958482]\n",
      "epoch:15 step:14655 [D loss: 0.619848, acc.: 63.28%] [G loss: 0.833437]\n",
      "epoch:15 step:14656 [D loss: 0.688775, acc.: 55.47%] [G loss: 0.999889]\n",
      "epoch:15 step:14657 [D loss: 0.584249, acc.: 71.88%] [G loss: 0.972769]\n",
      "epoch:15 step:14658 [D loss: 0.652564, acc.: 61.72%] [G loss: 1.019991]\n",
      "epoch:15 step:14659 [D loss: 0.627106, acc.: 64.84%] [G loss: 0.839606]\n",
      "epoch:15 step:14660 [D loss: 0.596462, acc.: 67.19%] [G loss: 0.911293]\n",
      "epoch:15 step:14661 [D loss: 0.708038, acc.: 53.91%] [G loss: 0.971443]\n",
      "epoch:15 step:14662 [D loss: 0.571975, acc.: 79.69%] [G loss: 0.882509]\n",
      "epoch:15 step:14663 [D loss: 0.834070, acc.: 40.62%] [G loss: 0.908375]\n",
      "epoch:15 step:14664 [D loss: 0.665764, acc.: 60.16%] [G loss: 0.917016]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14665 [D loss: 0.691480, acc.: 57.03%] [G loss: 0.774411]\n",
      "epoch:15 step:14666 [D loss: 0.724842, acc.: 59.38%] [G loss: 0.979642]\n",
      "epoch:15 step:14667 [D loss: 0.686591, acc.: 57.03%] [G loss: 1.282019]\n",
      "epoch:15 step:14668 [D loss: 0.594490, acc.: 71.09%] [G loss: 1.172997]\n",
      "epoch:15 step:14669 [D loss: 0.614382, acc.: 60.94%] [G loss: 1.330315]\n",
      "epoch:15 step:14670 [D loss: 0.568185, acc.: 74.22%] [G loss: 1.114615]\n",
      "epoch:15 step:14671 [D loss: 0.578357, acc.: 69.53%] [G loss: 0.956515]\n",
      "epoch:15 step:14672 [D loss: 0.588292, acc.: 69.53%] [G loss: 1.109372]\n",
      "epoch:15 step:14673 [D loss: 0.580266, acc.: 67.97%] [G loss: 1.044088]\n",
      "epoch:15 step:14674 [D loss: 0.708621, acc.: 52.34%] [G loss: 0.882166]\n",
      "epoch:15 step:14675 [D loss: 0.739877, acc.: 37.50%] [G loss: 0.989471]\n",
      "epoch:15 step:14676 [D loss: 0.602074, acc.: 67.97%] [G loss: 0.808101]\n",
      "epoch:15 step:14677 [D loss: 0.646788, acc.: 64.84%] [G loss: 0.885453]\n",
      "epoch:15 step:14678 [D loss: 0.790621, acc.: 41.41%] [G loss: 0.862600]\n",
      "epoch:15 step:14679 [D loss: 0.766365, acc.: 44.53%] [G loss: 0.831274]\n",
      "epoch:15 step:14680 [D loss: 0.690717, acc.: 55.47%] [G loss: 0.862071]\n",
      "epoch:15 step:14681 [D loss: 0.643513, acc.: 58.59%] [G loss: 0.847503]\n",
      "epoch:15 step:14682 [D loss: 0.587692, acc.: 69.53%] [G loss: 0.925973]\n",
      "epoch:15 step:14683 [D loss: 0.689816, acc.: 53.12%] [G loss: 0.923460]\n",
      "epoch:15 step:14684 [D loss: 0.649213, acc.: 53.91%] [G loss: 0.806612]\n",
      "epoch:15 step:14685 [D loss: 0.514632, acc.: 81.25%] [G loss: 1.029135]\n",
      "epoch:15 step:14686 [D loss: 0.719481, acc.: 57.81%] [G loss: 0.883445]\n",
      "epoch:15 step:14687 [D loss: 0.682693, acc.: 59.38%] [G loss: 0.884390]\n",
      "epoch:15 step:14688 [D loss: 0.694240, acc.: 56.25%] [G loss: 0.827096]\n",
      "epoch:15 step:14689 [D loss: 0.684215, acc.: 56.25%] [G loss: 0.880984]\n",
      "epoch:15 step:14690 [D loss: 0.678236, acc.: 64.84%] [G loss: 0.973393]\n",
      "epoch:15 step:14691 [D loss: 0.823988, acc.: 42.19%] [G loss: 0.976039]\n",
      "epoch:15 step:14692 [D loss: 0.674510, acc.: 54.69%] [G loss: 0.824791]\n",
      "epoch:15 step:14693 [D loss: 0.640465, acc.: 58.59%] [G loss: 0.844168]\n",
      "epoch:15 step:14694 [D loss: 0.730446, acc.: 51.56%] [G loss: 0.889811]\n",
      "epoch:15 step:14695 [D loss: 0.701046, acc.: 53.12%] [G loss: 0.974655]\n",
      "epoch:15 step:14696 [D loss: 0.707250, acc.: 54.69%] [G loss: 0.867847]\n",
      "epoch:15 step:14697 [D loss: 0.694429, acc.: 56.25%] [G loss: 0.868105]\n",
      "epoch:15 step:14698 [D loss: 0.735140, acc.: 44.53%] [G loss: 0.773995]\n",
      "epoch:15 step:14699 [D loss: 0.682651, acc.: 54.69%] [G loss: 0.852986]\n",
      "epoch:15 step:14700 [D loss: 0.718718, acc.: 42.19%] [G loss: 0.891774]\n",
      "epoch:15 step:14701 [D loss: 0.968427, acc.: 40.62%] [G loss: 1.005417]\n",
      "epoch:15 step:14702 [D loss: 0.567964, acc.: 75.78%] [G loss: 1.013815]\n",
      "epoch:15 step:14703 [D loss: 0.659415, acc.: 62.50%] [G loss: 1.030237]\n",
      "epoch:15 step:14704 [D loss: 0.622330, acc.: 64.84%] [G loss: 0.979063]\n",
      "epoch:15 step:14705 [D loss: 0.611573, acc.: 66.41%] [G loss: 1.039257]\n",
      "epoch:15 step:14706 [D loss: 0.621991, acc.: 62.50%] [G loss: 0.805339]\n",
      "epoch:15 step:14707 [D loss: 0.623776, acc.: 66.41%] [G loss: 0.903950]\n",
      "epoch:15 step:14708 [D loss: 0.660015, acc.: 63.28%] [G loss: 0.940326]\n",
      "epoch:15 step:14709 [D loss: 0.596970, acc.: 68.75%] [G loss: 0.887748]\n",
      "epoch:15 step:14710 [D loss: 0.629646, acc.: 64.84%] [G loss: 0.995609]\n",
      "epoch:15 step:14711 [D loss: 0.675082, acc.: 57.81%] [G loss: 0.880327]\n",
      "epoch:15 step:14712 [D loss: 0.688796, acc.: 53.12%] [G loss: 0.987415]\n",
      "epoch:15 step:14713 [D loss: 0.764248, acc.: 46.88%] [G loss: 0.863744]\n",
      "epoch:15 step:14714 [D loss: 0.602603, acc.: 66.41%] [G loss: 0.850097]\n",
      "epoch:15 step:14715 [D loss: 0.693153, acc.: 55.47%] [G loss: 0.931501]\n",
      "epoch:15 step:14716 [D loss: 0.644348, acc.: 63.28%] [G loss: 0.903719]\n",
      "epoch:15 step:14717 [D loss: 0.681527, acc.: 52.34%] [G loss: 0.874230]\n",
      "epoch:15 step:14718 [D loss: 0.576925, acc.: 71.09%] [G loss: 0.778710]\n",
      "epoch:15 step:14719 [D loss: 0.638624, acc.: 64.06%] [G loss: 0.887767]\n",
      "epoch:15 step:14720 [D loss: 0.653836, acc.: 66.41%] [G loss: 0.947590]\n",
      "epoch:15 step:14721 [D loss: 0.610906, acc.: 67.19%] [G loss: 0.763531]\n",
      "epoch:15 step:14722 [D loss: 0.654840, acc.: 59.38%] [G loss: 0.861000]\n",
      "epoch:15 step:14723 [D loss: 0.662095, acc.: 63.28%] [G loss: 0.909169]\n",
      "epoch:15 step:14724 [D loss: 0.687219, acc.: 52.34%] [G loss: 0.869212]\n",
      "epoch:15 step:14725 [D loss: 0.593980, acc.: 71.09%] [G loss: 0.783890]\n",
      "epoch:15 step:14726 [D loss: 0.626337, acc.: 60.94%] [G loss: 0.774587]\n",
      "epoch:15 step:14727 [D loss: 0.688292, acc.: 54.69%] [G loss: 0.824157]\n",
      "epoch:15 step:14728 [D loss: 0.686607, acc.: 50.78%] [G loss: 0.848842]\n",
      "epoch:15 step:14729 [D loss: 0.640463, acc.: 55.47%] [G loss: 0.802112]\n",
      "epoch:15 step:14730 [D loss: 0.803321, acc.: 64.84%] [G loss: 1.158086]\n",
      "epoch:15 step:14731 [D loss: 0.646214, acc.: 64.06%] [G loss: 1.034543]\n",
      "epoch:15 step:14732 [D loss: 0.625465, acc.: 67.97%] [G loss: 1.046432]\n",
      "epoch:15 step:14733 [D loss: 0.653919, acc.: 57.81%] [G loss: 0.898945]\n",
      "epoch:15 step:14734 [D loss: 0.616314, acc.: 66.41%] [G loss: 0.903842]\n",
      "epoch:15 step:14735 [D loss: 0.688998, acc.: 50.78%] [G loss: 0.846678]\n",
      "epoch:15 step:14736 [D loss: 0.644480, acc.: 61.72%] [G loss: 0.969786]\n",
      "epoch:15 step:14737 [D loss: 0.602461, acc.: 65.62%] [G loss: 0.888353]\n",
      "epoch:15 step:14738 [D loss: 0.687803, acc.: 54.69%] [G loss: 0.989553]\n",
      "epoch:15 step:14739 [D loss: 0.683367, acc.: 53.91%] [G loss: 0.893647]\n",
      "epoch:15 step:14740 [D loss: 0.654806, acc.: 57.03%] [G loss: 0.859301]\n",
      "epoch:15 step:14741 [D loss: 0.569558, acc.: 68.75%] [G loss: 0.869743]\n",
      "epoch:15 step:14742 [D loss: 0.749579, acc.: 39.84%] [G loss: 0.892222]\n",
      "epoch:15 step:14743 [D loss: 0.619720, acc.: 61.72%] [G loss: 0.947561]\n",
      "epoch:15 step:14744 [D loss: 0.704995, acc.: 47.66%] [G loss: 0.950171]\n",
      "epoch:15 step:14745 [D loss: 0.676736, acc.: 51.56%] [G loss: 0.968095]\n",
      "epoch:15 step:14746 [D loss: 0.683547, acc.: 53.91%] [G loss: 0.917738]\n",
      "epoch:15 step:14747 [D loss: 0.597006, acc.: 74.22%] [G loss: 1.028165]\n",
      "epoch:15 step:14748 [D loss: 0.691309, acc.: 57.81%] [G loss: 0.925738]\n",
      "epoch:15 step:14749 [D loss: 0.717893, acc.: 48.44%] [G loss: 0.855079]\n",
      "epoch:15 step:14750 [D loss: 0.693519, acc.: 58.59%] [G loss: 0.909209]\n",
      "epoch:15 step:14751 [D loss: 0.578803, acc.: 71.09%] [G loss: 0.856515]\n",
      "epoch:15 step:14752 [D loss: 0.642643, acc.: 62.50%] [G loss: 0.923125]\n",
      "epoch:15 step:14753 [D loss: 0.607636, acc.: 64.84%] [G loss: 0.983766]\n",
      "epoch:15 step:14754 [D loss: 0.632302, acc.: 67.19%] [G loss: 0.955379]\n",
      "epoch:15 step:14755 [D loss: 0.570916, acc.: 72.66%] [G loss: 0.936105]\n",
      "epoch:15 step:14756 [D loss: 0.654116, acc.: 65.62%] [G loss: 0.829477]\n",
      "epoch:15 step:14757 [D loss: 0.576370, acc.: 71.88%] [G loss: 0.857378]\n",
      "epoch:15 step:14758 [D loss: 0.550865, acc.: 64.06%] [G loss: 0.878983]\n",
      "epoch:15 step:14759 [D loss: 0.590284, acc.: 67.97%] [G loss: 0.984647]\n",
      "epoch:15 step:14760 [D loss: 0.591904, acc.: 68.75%] [G loss: 0.867852]\n",
      "epoch:15 step:14761 [D loss: 0.636972, acc.: 58.59%] [G loss: 0.957328]\n",
      "epoch:15 step:14762 [D loss: 0.612957, acc.: 71.09%] [G loss: 0.797911]\n",
      "epoch:15 step:14763 [D loss: 0.704544, acc.: 58.59%] [G loss: 0.737676]\n",
      "epoch:15 step:14764 [D loss: 0.597511, acc.: 73.44%] [G loss: 0.634216]\n",
      "epoch:15 step:14765 [D loss: 0.778381, acc.: 35.16%] [G loss: 0.818429]\n",
      "epoch:15 step:14766 [D loss: 0.659647, acc.: 54.69%] [G loss: 1.029211]\n",
      "epoch:15 step:14767 [D loss: 0.619094, acc.: 63.28%] [G loss: 0.901977]\n",
      "epoch:15 step:14768 [D loss: 0.644481, acc.: 62.50%] [G loss: 0.893171]\n",
      "epoch:15 step:14769 [D loss: 0.551376, acc.: 79.69%] [G loss: 0.864686]\n",
      "epoch:15 step:14770 [D loss: 0.758412, acc.: 45.31%] [G loss: 1.028386]\n",
      "epoch:15 step:14771 [D loss: 0.603005, acc.: 65.62%] [G loss: 0.877712]\n",
      "epoch:15 step:14772 [D loss: 0.800714, acc.: 37.50%] [G loss: 0.994729]\n",
      "epoch:15 step:14773 [D loss: 0.579738, acc.: 71.88%] [G loss: 0.846408]\n",
      "epoch:15 step:14774 [D loss: 0.572523, acc.: 75.00%] [G loss: 1.153786]\n",
      "epoch:15 step:14775 [D loss: 0.546222, acc.: 78.91%] [G loss: 1.053508]\n",
      "epoch:15 step:14776 [D loss: 0.585230, acc.: 68.75%] [G loss: 0.835721]\n",
      "epoch:15 step:14777 [D loss: 0.693706, acc.: 54.69%] [G loss: 1.005354]\n",
      "epoch:15 step:14778 [D loss: 0.706413, acc.: 53.91%] [G loss: 0.987412]\n",
      "epoch:15 step:14779 [D loss: 0.660193, acc.: 56.25%] [G loss: 0.969787]\n",
      "epoch:15 step:14780 [D loss: 0.681497, acc.: 53.12%] [G loss: 0.797890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14781 [D loss: 0.581291, acc.: 72.66%] [G loss: 1.029141]\n",
      "epoch:15 step:14782 [D loss: 0.607595, acc.: 71.88%] [G loss: 0.868547]\n",
      "epoch:15 step:14783 [D loss: 0.622981, acc.: 66.41%] [G loss: 0.973432]\n",
      "epoch:15 step:14784 [D loss: 0.647761, acc.: 60.16%] [G loss: 1.041002]\n",
      "epoch:15 step:14785 [D loss: 0.673247, acc.: 56.25%] [G loss: 1.009829]\n",
      "epoch:15 step:14786 [D loss: 0.698158, acc.: 54.69%] [G loss: 0.866309]\n",
      "epoch:15 step:14787 [D loss: 0.634101, acc.: 59.38%] [G loss: 0.773724]\n",
      "epoch:15 step:14788 [D loss: 0.638616, acc.: 60.16%] [G loss: 0.900895]\n",
      "epoch:15 step:14789 [D loss: 0.623429, acc.: 63.28%] [G loss: 1.027327]\n",
      "epoch:15 step:14790 [D loss: 0.576622, acc.: 69.53%] [G loss: 1.251194]\n",
      "epoch:15 step:14791 [D loss: 0.619434, acc.: 60.94%] [G loss: 1.100194]\n",
      "epoch:15 step:14792 [D loss: 0.602404, acc.: 65.62%] [G loss: 1.135613]\n",
      "epoch:15 step:14793 [D loss: 0.597110, acc.: 67.97%] [G loss: 1.031738]\n",
      "epoch:15 step:14794 [D loss: 0.625713, acc.: 63.28%] [G loss: 0.987653]\n",
      "epoch:15 step:14795 [D loss: 0.620962, acc.: 63.28%] [G loss: 1.070611]\n",
      "epoch:15 step:14796 [D loss: 0.631150, acc.: 65.62%] [G loss: 0.893732]\n",
      "epoch:15 step:14797 [D loss: 0.533965, acc.: 76.56%] [G loss: 1.138312]\n",
      "epoch:15 step:14798 [D loss: 0.548866, acc.: 76.56%] [G loss: 1.217039]\n",
      "epoch:15 step:14799 [D loss: 0.462532, acc.: 82.81%] [G loss: 0.887801]\n",
      "epoch:15 step:14800 [D loss: 0.641258, acc.: 55.47%] [G loss: 1.178501]\n",
      "epoch:15 step:14801 [D loss: 0.654623, acc.: 59.38%] [G loss: 0.871470]\n",
      "epoch:15 step:14802 [D loss: 0.544488, acc.: 75.78%] [G loss: 1.009759]\n",
      "epoch:15 step:14803 [D loss: 0.571448, acc.: 76.56%] [G loss: 1.093943]\n",
      "epoch:15 step:14804 [D loss: 0.523026, acc.: 78.12%] [G loss: 0.838779]\n",
      "epoch:15 step:14805 [D loss: 0.637191, acc.: 65.62%] [G loss: 0.957149]\n",
      "epoch:15 step:14806 [D loss: 0.658723, acc.: 61.72%] [G loss: 0.868638]\n",
      "epoch:15 step:14807 [D loss: 0.547859, acc.: 75.00%] [G loss: 0.723285]\n",
      "epoch:15 step:14808 [D loss: 0.761176, acc.: 50.00%] [G loss: 1.020124]\n",
      "epoch:15 step:14809 [D loss: 0.675549, acc.: 64.06%] [G loss: 0.975688]\n",
      "epoch:15 step:14810 [D loss: 0.487575, acc.: 81.25%] [G loss: 0.896766]\n",
      "epoch:15 step:14811 [D loss: 0.610767, acc.: 60.94%] [G loss: 0.827262]\n",
      "epoch:15 step:14812 [D loss: 0.670166, acc.: 55.47%] [G loss: 1.200795]\n",
      "epoch:15 step:14813 [D loss: 0.669262, acc.: 55.47%] [G loss: 1.032084]\n",
      "epoch:15 step:14814 [D loss: 0.769042, acc.: 48.44%] [G loss: 0.970300]\n",
      "epoch:15 step:14815 [D loss: 0.644019, acc.: 53.12%] [G loss: 0.784435]\n",
      "epoch:15 step:14816 [D loss: 0.609054, acc.: 68.75%] [G loss: 0.630046]\n",
      "epoch:15 step:14817 [D loss: 0.699180, acc.: 55.47%] [G loss: 1.104304]\n",
      "epoch:15 step:14818 [D loss: 0.664039, acc.: 54.69%] [G loss: 0.815533]\n",
      "epoch:15 step:14819 [D loss: 0.540743, acc.: 75.78%] [G loss: 1.024105]\n",
      "epoch:15 step:14820 [D loss: 0.690642, acc.: 58.59%] [G loss: 0.966393]\n",
      "epoch:15 step:14821 [D loss: 0.571356, acc.: 78.12%] [G loss: 0.926987]\n",
      "epoch:15 step:14822 [D loss: 0.509890, acc.: 75.00%] [G loss: 0.910769]\n",
      "epoch:15 step:14823 [D loss: 0.711570, acc.: 54.69%] [G loss: 0.907718]\n",
      "epoch:15 step:14824 [D loss: 0.691507, acc.: 55.47%] [G loss: 1.030719]\n",
      "epoch:15 step:14825 [D loss: 0.756161, acc.: 42.19%] [G loss: 1.030908]\n",
      "epoch:15 step:14826 [D loss: 0.615186, acc.: 66.41%] [G loss: 0.920879]\n",
      "epoch:15 step:14827 [D loss: 0.623541, acc.: 62.50%] [G loss: 0.822961]\n",
      "epoch:15 step:14828 [D loss: 0.702797, acc.: 50.78%] [G loss: 0.782662]\n",
      "epoch:15 step:14829 [D loss: 0.663226, acc.: 57.03%] [G loss: 0.851654]\n",
      "epoch:15 step:14830 [D loss: 0.588483, acc.: 67.19%] [G loss: 1.102443]\n",
      "epoch:15 step:14831 [D loss: 0.647099, acc.: 57.81%] [G loss: 0.757675]\n",
      "epoch:15 step:14832 [D loss: 0.665647, acc.: 53.91%] [G loss: 0.808784]\n",
      "epoch:15 step:14833 [D loss: 0.686227, acc.: 59.38%] [G loss: 1.094425]\n",
      "epoch:15 step:14834 [D loss: 0.714936, acc.: 52.34%] [G loss: 0.848458]\n",
      "epoch:15 step:14835 [D loss: 0.686347, acc.: 52.34%] [G loss: 1.068702]\n",
      "epoch:15 step:14836 [D loss: 0.630819, acc.: 60.94%] [G loss: 1.195272]\n",
      "epoch:15 step:14837 [D loss: 0.631312, acc.: 66.41%] [G loss: 1.041426]\n",
      "epoch:15 step:14838 [D loss: 0.606011, acc.: 68.75%] [G loss: 1.067260]\n",
      "epoch:15 step:14839 [D loss: 0.591897, acc.: 69.53%] [G loss: 1.013665]\n",
      "epoch:15 step:14840 [D loss: 0.734583, acc.: 55.47%] [G loss: 0.928626]\n",
      "epoch:15 step:14841 [D loss: 0.608435, acc.: 60.94%] [G loss: 1.063457]\n",
      "epoch:15 step:14842 [D loss: 0.694406, acc.: 50.00%] [G loss: 1.139465]\n",
      "epoch:15 step:14843 [D loss: 0.595988, acc.: 71.09%] [G loss: 0.966314]\n",
      "epoch:15 step:14844 [D loss: 0.680076, acc.: 62.50%] [G loss: 0.799926]\n",
      "epoch:15 step:14845 [D loss: 0.590725, acc.: 74.22%] [G loss: 0.844089]\n",
      "epoch:15 step:14846 [D loss: 0.633962, acc.: 63.28%] [G loss: 0.957742]\n",
      "epoch:15 step:14847 [D loss: 0.607170, acc.: 68.75%] [G loss: 0.984909]\n",
      "epoch:15 step:14848 [D loss: 0.718182, acc.: 46.88%] [G loss: 0.996326]\n",
      "epoch:15 step:14849 [D loss: 0.681289, acc.: 47.66%] [G loss: 1.007526]\n",
      "epoch:15 step:14850 [D loss: 0.494808, acc.: 86.72%] [G loss: 1.119056]\n",
      "epoch:15 step:14851 [D loss: 0.623269, acc.: 60.94%] [G loss: 1.084324]\n",
      "epoch:15 step:14852 [D loss: 0.657323, acc.: 55.47%] [G loss: 0.924892]\n",
      "epoch:15 step:14853 [D loss: 0.597911, acc.: 66.41%] [G loss: 0.855485]\n",
      "epoch:15 step:14854 [D loss: 0.589050, acc.: 70.31%] [G loss: 0.884709]\n",
      "epoch:15 step:14855 [D loss: 0.601029, acc.: 67.19%] [G loss: 0.854616]\n",
      "epoch:15 step:14856 [D loss: 0.560111, acc.: 70.31%] [G loss: 0.921228]\n",
      "epoch:15 step:14857 [D loss: 0.673482, acc.: 60.94%] [G loss: 1.251701]\n",
      "epoch:15 step:14858 [D loss: 0.565146, acc.: 74.22%] [G loss: 0.821485]\n",
      "epoch:15 step:14859 [D loss: 0.637286, acc.: 61.72%] [G loss: 0.898045]\n",
      "epoch:15 step:14860 [D loss: 0.647573, acc.: 60.94%] [G loss: 0.848193]\n",
      "epoch:15 step:14861 [D loss: 0.672034, acc.: 58.59%] [G loss: 0.926992]\n",
      "epoch:15 step:14862 [D loss: 0.702813, acc.: 51.56%] [G loss: 1.256990]\n",
      "epoch:15 step:14863 [D loss: 0.615048, acc.: 64.06%] [G loss: 0.930392]\n",
      "epoch:15 step:14864 [D loss: 0.538513, acc.: 80.47%] [G loss: 1.068477]\n",
      "epoch:15 step:14865 [D loss: 0.672115, acc.: 58.59%] [G loss: 1.037038]\n",
      "epoch:15 step:14866 [D loss: 0.675309, acc.: 56.25%] [G loss: 0.955529]\n",
      "epoch:15 step:14867 [D loss: 0.757655, acc.: 39.84%] [G loss: 0.950771]\n",
      "epoch:15 step:14868 [D loss: 0.629950, acc.: 68.75%] [G loss: 0.936538]\n",
      "epoch:15 step:14869 [D loss: 0.632665, acc.: 62.50%] [G loss: 0.970125]\n",
      "epoch:15 step:14870 [D loss: 0.601264, acc.: 72.66%] [G loss: 1.353491]\n",
      "epoch:15 step:14871 [D loss: 0.612700, acc.: 64.84%] [G loss: 0.893736]\n",
      "epoch:15 step:14872 [D loss: 0.557410, acc.: 72.66%] [G loss: 0.859768]\n",
      "epoch:15 step:14873 [D loss: 0.614557, acc.: 65.62%] [G loss: 0.818914]\n",
      "epoch:15 step:14874 [D loss: 0.676226, acc.: 60.16%] [G loss: 0.992765]\n",
      "epoch:15 step:14875 [D loss: 0.546405, acc.: 76.56%] [G loss: 1.039636]\n",
      "epoch:15 step:14876 [D loss: 0.569010, acc.: 75.00%] [G loss: 1.075928]\n",
      "epoch:15 step:14877 [D loss: 0.627819, acc.: 60.16%] [G loss: 1.096270]\n",
      "epoch:15 step:14878 [D loss: 0.685138, acc.: 52.34%] [G loss: 1.361959]\n",
      "epoch:15 step:14879 [D loss: 0.674523, acc.: 55.47%] [G loss: 1.181119]\n",
      "epoch:15 step:14880 [D loss: 0.565603, acc.: 77.34%] [G loss: 0.877121]\n",
      "epoch:15 step:14881 [D loss: 0.749629, acc.: 39.84%] [G loss: 1.260331]\n",
      "epoch:15 step:14882 [D loss: 0.601624, acc.: 67.97%] [G loss: 0.912156]\n",
      "epoch:15 step:14883 [D loss: 0.673109, acc.: 57.03%] [G loss: 0.950811]\n",
      "epoch:15 step:14884 [D loss: 0.622082, acc.: 71.09%] [G loss: 1.065977]\n",
      "epoch:15 step:14885 [D loss: 0.665240, acc.: 60.16%] [G loss: 0.982169]\n",
      "epoch:15 step:14886 [D loss: 0.609005, acc.: 70.31%] [G loss: 1.106903]\n",
      "epoch:15 step:14887 [D loss: 0.615550, acc.: 69.53%] [G loss: 0.909608]\n",
      "epoch:15 step:14888 [D loss: 0.614893, acc.: 67.19%] [G loss: 0.971466]\n",
      "epoch:15 step:14889 [D loss: 0.598395, acc.: 69.53%] [G loss: 0.906898]\n",
      "epoch:15 step:14890 [D loss: 0.720586, acc.: 50.78%] [G loss: 0.912958]\n",
      "epoch:15 step:14891 [D loss: 0.635044, acc.: 60.16%] [G loss: 0.887435]\n",
      "epoch:15 step:14892 [D loss: 0.648293, acc.: 57.03%] [G loss: 1.276577]\n",
      "epoch:15 step:14893 [D loss: 0.647427, acc.: 57.81%] [G loss: 0.855237]\n",
      "epoch:15 step:14894 [D loss: 0.626525, acc.: 65.62%] [G loss: 0.945873]\n",
      "epoch:15 step:14895 [D loss: 0.591583, acc.: 63.28%] [G loss: 0.996547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14896 [D loss: 0.613269, acc.: 74.22%] [G loss: 0.982367]\n",
      "epoch:15 step:14897 [D loss: 0.659034, acc.: 55.47%] [G loss: 1.045157]\n",
      "epoch:15 step:14898 [D loss: 0.619992, acc.: 70.31%] [G loss: 0.882820]\n",
      "epoch:15 step:14899 [D loss: 0.649818, acc.: 59.38%] [G loss: 1.009496]\n",
      "epoch:15 step:14900 [D loss: 0.701650, acc.: 49.22%] [G loss: 0.814570]\n",
      "epoch:15 step:14901 [D loss: 0.579288, acc.: 69.53%] [G loss: 1.099641]\n",
      "epoch:15 step:14902 [D loss: 0.724804, acc.: 49.22%] [G loss: 0.760356]\n",
      "epoch:15 step:14903 [D loss: 0.687085, acc.: 50.78%] [G loss: 1.140038]\n",
      "epoch:15 step:14904 [D loss: 0.630871, acc.: 57.03%] [G loss: 1.018289]\n",
      "epoch:15 step:14905 [D loss: 0.550126, acc.: 72.66%] [G loss: 0.924282]\n",
      "epoch:15 step:14906 [D loss: 0.670091, acc.: 53.12%] [G loss: 0.963987]\n",
      "epoch:15 step:14907 [D loss: 0.526928, acc.: 83.59%] [G loss: 1.059918]\n",
      "epoch:15 step:14908 [D loss: 0.595603, acc.: 64.06%] [G loss: 1.542859]\n",
      "epoch:15 step:14909 [D loss: 0.651129, acc.: 57.81%] [G loss: 1.005465]\n",
      "epoch:15 step:14910 [D loss: 0.569856, acc.: 75.78%] [G loss: 1.070163]\n",
      "epoch:15 step:14911 [D loss: 0.563367, acc.: 71.09%] [G loss: 1.003285]\n",
      "epoch:15 step:14912 [D loss: 0.615934, acc.: 66.41%] [G loss: 0.952686]\n",
      "epoch:15 step:14913 [D loss: 0.608832, acc.: 65.62%] [G loss: 0.960019]\n",
      "epoch:15 step:14914 [D loss: 0.698761, acc.: 53.12%] [G loss: 1.110394]\n",
      "epoch:15 step:14915 [D loss: 0.587146, acc.: 65.62%] [G loss: 1.099072]\n",
      "epoch:15 step:14916 [D loss: 0.605500, acc.: 67.97%] [G loss: 1.194548]\n",
      "epoch:15 step:14917 [D loss: 0.576268, acc.: 75.00%] [G loss: 1.126081]\n",
      "epoch:15 step:14918 [D loss: 0.600758, acc.: 71.09%] [G loss: 1.011425]\n",
      "epoch:15 step:14919 [D loss: 0.371453, acc.: 88.28%] [G loss: 1.181635]\n",
      "epoch:15 step:14920 [D loss: 0.620870, acc.: 67.97%] [G loss: 1.017787]\n",
      "epoch:15 step:14921 [D loss: 0.633232, acc.: 71.88%] [G loss: 1.245610]\n",
      "epoch:15 step:14922 [D loss: 0.709441, acc.: 50.78%] [G loss: 1.140346]\n",
      "epoch:15 step:14923 [D loss: 0.521992, acc.: 75.00%] [G loss: 1.134717]\n",
      "epoch:15 step:14924 [D loss: 0.557233, acc.: 69.53%] [G loss: 1.617157]\n",
      "epoch:15 step:14925 [D loss: 0.502015, acc.: 80.47%] [G loss: 1.576134]\n",
      "epoch:15 step:14926 [D loss: 0.630235, acc.: 63.28%] [G loss: 0.820028]\n",
      "epoch:15 step:14927 [D loss: 0.553878, acc.: 74.22%] [G loss: 1.070419]\n",
      "epoch:15 step:14928 [D loss: 0.535189, acc.: 77.34%] [G loss: 0.804482]\n",
      "epoch:15 step:14929 [D loss: 0.616779, acc.: 63.28%] [G loss: 1.025689]\n",
      "epoch:15 step:14930 [D loss: 0.539732, acc.: 78.12%] [G loss: 1.081611]\n",
      "epoch:15 step:14931 [D loss: 0.529158, acc.: 77.34%] [G loss: 0.707212]\n",
      "epoch:15 step:14932 [D loss: 0.692252, acc.: 58.59%] [G loss: 1.099843]\n",
      "epoch:15 step:14933 [D loss: 0.686852, acc.: 57.03%] [G loss: 0.920714]\n",
      "epoch:15 step:14934 [D loss: 0.598740, acc.: 69.53%] [G loss: 1.038972]\n",
      "epoch:15 step:14935 [D loss: 0.671453, acc.: 53.12%] [G loss: 1.352987]\n",
      "epoch:15 step:14936 [D loss: 0.644568, acc.: 57.81%] [G loss: 0.766661]\n",
      "epoch:15 step:14937 [D loss: 0.456621, acc.: 83.59%] [G loss: 0.864604]\n",
      "epoch:15 step:14938 [D loss: 0.633229, acc.: 67.19%] [G loss: 1.141751]\n",
      "epoch:15 step:14939 [D loss: 0.623319, acc.: 57.81%] [G loss: 1.047854]\n",
      "epoch:15 step:14940 [D loss: 0.804734, acc.: 31.25%] [G loss: 1.193766]\n",
      "epoch:15 step:14941 [D loss: 0.693339, acc.: 53.91%] [G loss: 1.201582]\n",
      "epoch:15 step:14942 [D loss: 0.592817, acc.: 71.88%] [G loss: 0.866635]\n",
      "epoch:15 step:14943 [D loss: 0.502678, acc.: 78.91%] [G loss: 1.267955]\n",
      "epoch:15 step:14944 [D loss: 0.606967, acc.: 70.31%] [G loss: 0.999006]\n",
      "epoch:15 step:14945 [D loss: 0.765807, acc.: 50.00%] [G loss: 1.550336]\n",
      "epoch:15 step:14946 [D loss: 0.626581, acc.: 62.50%] [G loss: 1.240007]\n",
      "epoch:15 step:14947 [D loss: 0.660895, acc.: 63.28%] [G loss: 0.901492]\n",
      "epoch:15 step:14948 [D loss: 0.641138, acc.: 64.06%] [G loss: 1.152932]\n",
      "epoch:15 step:14949 [D loss: 0.757823, acc.: 43.75%] [G loss: 1.006152]\n",
      "epoch:15 step:14950 [D loss: 0.713905, acc.: 51.56%] [G loss: 0.913270]\n",
      "epoch:15 step:14951 [D loss: 0.747612, acc.: 42.19%] [G loss: 0.972134]\n",
      "epoch:15 step:14952 [D loss: 0.700308, acc.: 53.12%] [G loss: 1.079458]\n",
      "epoch:15 step:14953 [D loss: 0.618147, acc.: 65.62%] [G loss: 0.822952]\n",
      "epoch:15 step:14954 [D loss: 0.741787, acc.: 42.97%] [G loss: 1.172717]\n",
      "epoch:15 step:14955 [D loss: 0.582626, acc.: 72.66%] [G loss: 0.986897]\n",
      "epoch:15 step:14956 [D loss: 0.712213, acc.: 50.78%] [G loss: 0.820884]\n",
      "epoch:15 step:14957 [D loss: 0.620655, acc.: 66.41%] [G loss: 0.926615]\n",
      "epoch:15 step:14958 [D loss: 0.644489, acc.: 63.28%] [G loss: 0.951140]\n",
      "epoch:15 step:14959 [D loss: 0.673931, acc.: 60.94%] [G loss: 0.937156]\n",
      "epoch:15 step:14960 [D loss: 0.652512, acc.: 59.38%] [G loss: 0.787463]\n",
      "epoch:15 step:14961 [D loss: 0.634781, acc.: 64.84%] [G loss: 0.783639]\n",
      "epoch:15 step:14962 [D loss: 0.490385, acc.: 85.16%] [G loss: 0.758017]\n",
      "epoch:15 step:14963 [D loss: 0.615885, acc.: 65.62%] [G loss: 0.953321]\n",
      "epoch:15 step:14964 [D loss: 0.559353, acc.: 75.00%] [G loss: 0.833500]\n",
      "epoch:15 step:14965 [D loss: 0.737295, acc.: 43.75%] [G loss: 1.253533]\n",
      "epoch:15 step:14966 [D loss: 0.600361, acc.: 68.75%] [G loss: 0.864392]\n",
      "epoch:15 step:14967 [D loss: 0.635151, acc.: 64.06%] [G loss: 0.947162]\n",
      "epoch:15 step:14968 [D loss: 0.731329, acc.: 53.91%] [G loss: 0.981811]\n",
      "epoch:15 step:14969 [D loss: 0.678889, acc.: 57.81%] [G loss: 0.810758]\n",
      "epoch:15 step:14970 [D loss: 0.639616, acc.: 65.62%] [G loss: 1.039564]\n",
      "epoch:15 step:14971 [D loss: 0.628676, acc.: 59.38%] [G loss: 0.906002]\n",
      "epoch:15 step:14972 [D loss: 0.557007, acc.: 78.91%] [G loss: 0.850351]\n",
      "epoch:15 step:14973 [D loss: 0.612565, acc.: 64.84%] [G loss: 0.896943]\n",
      "epoch:15 step:14974 [D loss: 0.694593, acc.: 51.56%] [G loss: 1.058152]\n",
      "epoch:15 step:14975 [D loss: 0.581673, acc.: 67.19%] [G loss: 0.930962]\n",
      "epoch:15 step:14976 [D loss: 0.658478, acc.: 60.16%] [G loss: 0.982913]\n",
      "epoch:15 step:14977 [D loss: 0.678096, acc.: 50.78%] [G loss: 1.034429]\n",
      "epoch:15 step:14978 [D loss: 0.700031, acc.: 58.59%] [G loss: 0.990361]\n",
      "epoch:15 step:14979 [D loss: 0.544033, acc.: 70.31%] [G loss: 0.944504]\n",
      "epoch:15 step:14980 [D loss: 0.627032, acc.: 64.06%] [G loss: 0.837271]\n",
      "epoch:15 step:14981 [D loss: 0.564634, acc.: 75.78%] [G loss: 1.061865]\n",
      "epoch:15 step:14982 [D loss: 0.735285, acc.: 48.44%] [G loss: 0.876799]\n",
      "epoch:15 step:14983 [D loss: 0.637576, acc.: 64.06%] [G loss: 0.991648]\n",
      "epoch:15 step:14984 [D loss: 0.611369, acc.: 68.75%] [G loss: 1.341216]\n",
      "epoch:15 step:14985 [D loss: 0.611684, acc.: 66.41%] [G loss: 1.020604]\n",
      "epoch:15 step:14986 [D loss: 0.676354, acc.: 59.38%] [G loss: 0.769529]\n",
      "epoch:15 step:14987 [D loss: 0.665466, acc.: 57.81%] [G loss: 0.871423]\n",
      "epoch:15 step:14988 [D loss: 0.669755, acc.: 56.25%] [G loss: 1.024412]\n",
      "epoch:15 step:14989 [D loss: 0.664637, acc.: 64.06%] [G loss: 0.976584]\n",
      "epoch:15 step:14990 [D loss: 0.671908, acc.: 52.34%] [G loss: 0.931524]\n",
      "epoch:15 step:14991 [D loss: 0.609954, acc.: 69.53%] [G loss: 0.853983]\n",
      "epoch:15 step:14992 [D loss: 0.771197, acc.: 42.19%] [G loss: 1.029787]\n",
      "epoch:16 step:14993 [D loss: 0.620741, acc.: 62.50%] [G loss: 0.868922]\n",
      "epoch:16 step:14994 [D loss: 0.674336, acc.: 55.47%] [G loss: 0.841405]\n",
      "epoch:16 step:14995 [D loss: 0.682873, acc.: 57.81%] [G loss: 0.951009]\n",
      "epoch:16 step:14996 [D loss: 0.755042, acc.: 44.53%] [G loss: 1.029902]\n",
      "epoch:16 step:14997 [D loss: 0.696615, acc.: 53.12%] [G loss: 0.904998]\n",
      "epoch:16 step:14998 [D loss: 0.668365, acc.: 53.12%] [G loss: 1.054714]\n",
      "epoch:16 step:14999 [D loss: 0.667628, acc.: 57.81%] [G loss: 1.152089]\n",
      "epoch:16 step:15000 [D loss: 0.645703, acc.: 58.59%] [G loss: 1.006653]\n",
      "epoch:16 step:15001 [D loss: 0.554181, acc.: 76.56%] [G loss: 1.067280]\n",
      "epoch:16 step:15002 [D loss: 0.701437, acc.: 50.78%] [G loss: 0.835174]\n",
      "epoch:16 step:15003 [D loss: 0.707425, acc.: 49.22%] [G loss: 1.086381]\n",
      "epoch:16 step:15004 [D loss: 0.597017, acc.: 67.97%] [G loss: 1.061365]\n",
      "epoch:16 step:15005 [D loss: 0.740769, acc.: 41.41%] [G loss: 1.034733]\n",
      "epoch:16 step:15006 [D loss: 0.680465, acc.: 57.03%] [G loss: 1.197464]\n",
      "epoch:16 step:15007 [D loss: 0.736154, acc.: 53.91%] [G loss: 0.919726]\n",
      "epoch:16 step:15008 [D loss: 0.604721, acc.: 70.31%] [G loss: 1.061069]\n",
      "epoch:16 step:15009 [D loss: 0.560824, acc.: 69.53%] [G loss: 1.136382]\n",
      "epoch:16 step:15010 [D loss: 0.716700, acc.: 52.34%] [G loss: 1.166195]\n",
      "epoch:16 step:15011 [D loss: 0.637262, acc.: 63.28%] [G loss: 0.921192]\n",
      "epoch:16 step:15012 [D loss: 0.718976, acc.: 57.81%] [G loss: 0.889198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15013 [D loss: 0.625285, acc.: 62.50%] [G loss: 0.906183]\n",
      "epoch:16 step:15014 [D loss: 0.694896, acc.: 51.56%] [G loss: 0.861606]\n",
      "epoch:16 step:15015 [D loss: 0.682735, acc.: 57.03%] [G loss: 0.969429]\n",
      "epoch:16 step:15016 [D loss: 0.656066, acc.: 64.84%] [G loss: 0.976946]\n",
      "epoch:16 step:15017 [D loss: 0.603013, acc.: 73.44%] [G loss: 1.241817]\n",
      "epoch:16 step:15018 [D loss: 0.613289, acc.: 65.62%] [G loss: 1.122478]\n",
      "epoch:16 step:15019 [D loss: 0.607326, acc.: 69.53%] [G loss: 0.964898]\n",
      "epoch:16 step:15020 [D loss: 0.631188, acc.: 58.59%] [G loss: 0.908244]\n",
      "epoch:16 step:15021 [D loss: 0.619112, acc.: 65.62%] [G loss: 0.896504]\n",
      "epoch:16 step:15022 [D loss: 0.649859, acc.: 64.06%] [G loss: 1.077937]\n",
      "epoch:16 step:15023 [D loss: 0.675470, acc.: 53.12%] [G loss: 1.019924]\n",
      "epoch:16 step:15024 [D loss: 0.581713, acc.: 72.66%] [G loss: 1.047738]\n",
      "epoch:16 step:15025 [D loss: 0.608454, acc.: 65.62%] [G loss: 1.072365]\n",
      "epoch:16 step:15026 [D loss: 0.559765, acc.: 71.09%] [G loss: 1.202583]\n",
      "epoch:16 step:15027 [D loss: 0.739377, acc.: 50.78%] [G loss: 0.855491]\n",
      "epoch:16 step:15028 [D loss: 0.673666, acc.: 60.94%] [G loss: 0.884125]\n",
      "epoch:16 step:15029 [D loss: 0.663206, acc.: 50.00%] [G loss: 0.933482]\n",
      "epoch:16 step:15030 [D loss: 0.493919, acc.: 72.66%] [G loss: 0.935451]\n",
      "epoch:16 step:15031 [D loss: 0.718124, acc.: 51.56%] [G loss: 1.100284]\n",
      "epoch:16 step:15032 [D loss: 0.723027, acc.: 50.78%] [G loss: 0.845998]\n",
      "epoch:16 step:15033 [D loss: 0.594859, acc.: 67.19%] [G loss: 1.044053]\n",
      "epoch:16 step:15034 [D loss: 0.660799, acc.: 57.03%] [G loss: 1.054709]\n",
      "epoch:16 step:15035 [D loss: 0.549530, acc.: 81.25%] [G loss: 1.037426]\n",
      "epoch:16 step:15036 [D loss: 0.716940, acc.: 61.72%] [G loss: 0.838832]\n",
      "epoch:16 step:15037 [D loss: 0.745401, acc.: 53.12%] [G loss: 0.968275]\n",
      "epoch:16 step:15038 [D loss: 0.801150, acc.: 48.44%] [G loss: 0.956386]\n",
      "epoch:16 step:15039 [D loss: 0.586230, acc.: 72.66%] [G loss: 0.938000]\n",
      "epoch:16 step:15040 [D loss: 0.626362, acc.: 60.16%] [G loss: 0.830918]\n",
      "epoch:16 step:15041 [D loss: 0.555017, acc.: 75.78%] [G loss: 0.816641]\n",
      "epoch:16 step:15042 [D loss: 0.622958, acc.: 69.53%] [G loss: 1.184030]\n",
      "epoch:16 step:15043 [D loss: 0.529632, acc.: 78.91%] [G loss: 0.932892]\n",
      "epoch:16 step:15044 [D loss: 0.552308, acc.: 76.56%] [G loss: 2.999575]\n",
      "epoch:16 step:15045 [D loss: 0.608057, acc.: 60.94%] [G loss: 0.865018]\n",
      "epoch:16 step:15046 [D loss: 0.661186, acc.: 59.38%] [G loss: 0.860593]\n",
      "epoch:16 step:15047 [D loss: 0.576873, acc.: 67.97%] [G loss: 1.024165]\n",
      "epoch:16 step:15048 [D loss: 0.648708, acc.: 61.72%] [G loss: 1.199218]\n",
      "epoch:16 step:15049 [D loss: 0.621461, acc.: 66.41%] [G loss: 0.841427]\n",
      "epoch:16 step:15050 [D loss: 0.547607, acc.: 77.34%] [G loss: 1.024568]\n",
      "epoch:16 step:15051 [D loss: 0.681967, acc.: 59.38%] [G loss: 0.918572]\n",
      "epoch:16 step:15052 [D loss: 0.629253, acc.: 61.72%] [G loss: 0.824389]\n",
      "epoch:16 step:15053 [D loss: 0.535629, acc.: 82.03%] [G loss: 0.933262]\n",
      "epoch:16 step:15054 [D loss: 0.643842, acc.: 60.16%] [G loss: 1.160888]\n",
      "epoch:16 step:15055 [D loss: 0.515025, acc.: 78.12%] [G loss: 1.140753]\n",
      "epoch:16 step:15056 [D loss: 0.565782, acc.: 62.50%] [G loss: 1.398725]\n",
      "epoch:16 step:15057 [D loss: 0.708817, acc.: 50.78%] [G loss: 1.214499]\n",
      "epoch:16 step:15058 [D loss: 0.564989, acc.: 72.66%] [G loss: 1.414827]\n",
      "epoch:16 step:15059 [D loss: 0.689144, acc.: 54.69%] [G loss: 0.843522]\n",
      "epoch:16 step:15060 [D loss: 0.562457, acc.: 69.53%] [G loss: 0.991975]\n",
      "epoch:16 step:15061 [D loss: 0.574256, acc.: 71.09%] [G loss: 0.996795]\n",
      "epoch:16 step:15062 [D loss: 0.696184, acc.: 53.91%] [G loss: 0.852239]\n",
      "epoch:16 step:15063 [D loss: 0.732420, acc.: 53.12%] [G loss: 0.945693]\n",
      "epoch:16 step:15064 [D loss: 0.660416, acc.: 64.84%] [G loss: 0.971688]\n",
      "epoch:16 step:15065 [D loss: 0.572266, acc.: 74.22%] [G loss: 1.090416]\n",
      "epoch:16 step:15066 [D loss: 0.662838, acc.: 60.16%] [G loss: 0.868327]\n",
      "epoch:16 step:15067 [D loss: 0.629710, acc.: 60.94%] [G loss: 0.935410]\n",
      "epoch:16 step:15068 [D loss: 0.620496, acc.: 61.72%] [G loss: 1.193062]\n",
      "epoch:16 step:15069 [D loss: 0.684756, acc.: 50.00%] [G loss: 1.197821]\n",
      "epoch:16 step:15070 [D loss: 0.633809, acc.: 61.72%] [G loss: 1.099355]\n",
      "epoch:16 step:15071 [D loss: 0.573037, acc.: 74.22%] [G loss: 1.096673]\n",
      "epoch:16 step:15072 [D loss: 0.633071, acc.: 67.19%] [G loss: 1.323948]\n",
      "epoch:16 step:15073 [D loss: 0.748197, acc.: 48.44%] [G loss: 0.839112]\n",
      "epoch:16 step:15074 [D loss: 0.773851, acc.: 45.31%] [G loss: 0.972209]\n",
      "epoch:16 step:15075 [D loss: 0.470244, acc.: 89.06%] [G loss: 0.750371]\n",
      "epoch:16 step:15076 [D loss: 0.644725, acc.: 62.50%] [G loss: 0.801399]\n",
      "epoch:16 step:15077 [D loss: 0.587659, acc.: 72.66%] [G loss: 0.797300]\n",
      "epoch:16 step:15078 [D loss: 0.576404, acc.: 74.22%] [G loss: 0.803408]\n",
      "epoch:16 step:15079 [D loss: 0.586770, acc.: 76.56%] [G loss: 1.344393]\n",
      "epoch:16 step:15080 [D loss: 0.682360, acc.: 53.91%] [G loss: 1.061587]\n",
      "epoch:16 step:15081 [D loss: 0.519517, acc.: 78.12%] [G loss: 1.210114]\n",
      "epoch:16 step:15082 [D loss: 0.738843, acc.: 47.66%] [G loss: 0.993815]\n",
      "epoch:16 step:15083 [D loss: 0.473594, acc.: 76.56%] [G loss: 1.235467]\n",
      "epoch:16 step:15084 [D loss: 0.653835, acc.: 61.72%] [G loss: 1.142568]\n",
      "epoch:16 step:15085 [D loss: 0.606653, acc.: 68.75%] [G loss: 0.903283]\n",
      "epoch:16 step:15086 [D loss: 0.506389, acc.: 78.91%] [G loss: 1.266810]\n",
      "epoch:16 step:15087 [D loss: 0.669875, acc.: 58.59%] [G loss: 0.855388]\n",
      "epoch:16 step:15088 [D loss: 0.569544, acc.: 66.41%] [G loss: 1.086453]\n",
      "epoch:16 step:15089 [D loss: 0.593335, acc.: 66.41%] [G loss: 1.002609]\n",
      "epoch:16 step:15090 [D loss: 0.706659, acc.: 51.56%] [G loss: 0.867163]\n",
      "epoch:16 step:15091 [D loss: 0.632439, acc.: 70.31%] [G loss: 0.992451]\n",
      "epoch:16 step:15092 [D loss: 0.665367, acc.: 57.81%] [G loss: 1.055114]\n",
      "epoch:16 step:15093 [D loss: 0.659308, acc.: 58.59%] [G loss: 1.003989]\n",
      "epoch:16 step:15094 [D loss: 0.513370, acc.: 81.25%] [G loss: 0.854989]\n",
      "epoch:16 step:15095 [D loss: 0.638039, acc.: 57.03%] [G loss: 1.190713]\n",
      "epoch:16 step:15096 [D loss: 0.647763, acc.: 60.94%] [G loss: 1.135540]\n",
      "epoch:16 step:15097 [D loss: 0.695381, acc.: 53.91%] [G loss: 0.938421]\n",
      "epoch:16 step:15098 [D loss: 0.655659, acc.: 57.03%] [G loss: 0.968972]\n",
      "epoch:16 step:15099 [D loss: 0.688802, acc.: 50.00%] [G loss: 1.112567]\n",
      "epoch:16 step:15100 [D loss: 0.558879, acc.: 71.88%] [G loss: 1.059294]\n",
      "epoch:16 step:15101 [D loss: 0.733681, acc.: 50.00%] [G loss: 0.957964]\n",
      "epoch:16 step:15102 [D loss: 0.620427, acc.: 65.62%] [G loss: 0.954577]\n",
      "epoch:16 step:15103 [D loss: 0.635934, acc.: 62.50%] [G loss: 0.941776]\n",
      "epoch:16 step:15104 [D loss: 0.614300, acc.: 67.97%] [G loss: 0.746071]\n",
      "epoch:16 step:15105 [D loss: 0.576181, acc.: 71.09%] [G loss: 0.915251]\n",
      "epoch:16 step:15106 [D loss: 0.670577, acc.: 54.69%] [G loss: 0.981336]\n",
      "epoch:16 step:15107 [D loss: 0.670775, acc.: 56.25%] [G loss: 1.015301]\n",
      "epoch:16 step:15108 [D loss: 0.576876, acc.: 68.75%] [G loss: 1.105019]\n",
      "epoch:16 step:15109 [D loss: 0.704104, acc.: 50.78%] [G loss: 0.897963]\n",
      "epoch:16 step:15110 [D loss: 0.612563, acc.: 61.72%] [G loss: 0.896222]\n",
      "epoch:16 step:15111 [D loss: 0.605380, acc.: 65.62%] [G loss: 0.863132]\n",
      "epoch:16 step:15112 [D loss: 0.690501, acc.: 55.47%] [G loss: 0.990136]\n",
      "epoch:16 step:15113 [D loss: 0.631268, acc.: 61.72%] [G loss: 0.999093]\n",
      "epoch:16 step:15114 [D loss: 0.642071, acc.: 60.94%] [G loss: 1.059543]\n",
      "epoch:16 step:15115 [D loss: 0.665917, acc.: 59.38%] [G loss: 0.934767]\n",
      "epoch:16 step:15116 [D loss: 0.623119, acc.: 65.62%] [G loss: 1.109353]\n",
      "epoch:16 step:15117 [D loss: 0.627701, acc.: 66.41%] [G loss: 1.064665]\n",
      "epoch:16 step:15118 [D loss: 0.771887, acc.: 42.19%] [G loss: 1.047244]\n",
      "epoch:16 step:15119 [D loss: 0.690144, acc.: 57.03%] [G loss: 1.054570]\n",
      "epoch:16 step:15120 [D loss: 0.631405, acc.: 58.59%] [G loss: 1.019276]\n",
      "epoch:16 step:15121 [D loss: 0.661008, acc.: 60.16%] [G loss: 0.858216]\n",
      "epoch:16 step:15122 [D loss: 0.615624, acc.: 66.41%] [G loss: 0.970947]\n",
      "epoch:16 step:15123 [D loss: 0.620413, acc.: 71.09%] [G loss: 0.765032]\n",
      "epoch:16 step:15124 [D loss: 0.671684, acc.: 53.91%] [G loss: 0.771971]\n",
      "epoch:16 step:15125 [D loss: 0.604140, acc.: 66.41%] [G loss: 0.818627]\n",
      "epoch:16 step:15126 [D loss: 0.596418, acc.: 75.00%] [G loss: 1.232413]\n",
      "epoch:16 step:15127 [D loss: 0.615604, acc.: 66.41%] [G loss: 1.054428]\n",
      "epoch:16 step:15128 [D loss: 0.617358, acc.: 68.75%] [G loss: 1.122889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15129 [D loss: 0.591115, acc.: 65.62%] [G loss: 0.824280]\n",
      "epoch:16 step:15130 [D loss: 0.742529, acc.: 42.19%] [G loss: 0.915325]\n",
      "epoch:16 step:15131 [D loss: 0.584782, acc.: 71.88%] [G loss: 1.012142]\n",
      "epoch:16 step:15132 [D loss: 0.654246, acc.: 63.28%] [G loss: 1.409661]\n",
      "epoch:16 step:15133 [D loss: 0.549916, acc.: 76.56%] [G loss: 1.071085]\n",
      "epoch:16 step:15134 [D loss: 0.631445, acc.: 67.19%] [G loss: 0.968905]\n",
      "epoch:16 step:15135 [D loss: 0.584450, acc.: 68.75%] [G loss: 1.009644]\n",
      "epoch:16 step:15136 [D loss: 0.492964, acc.: 80.47%] [G loss: 0.961526]\n",
      "epoch:16 step:15137 [D loss: 0.705337, acc.: 49.22%] [G loss: 0.943321]\n",
      "epoch:16 step:15138 [D loss: 0.542171, acc.: 75.78%] [G loss: 1.118695]\n",
      "epoch:16 step:15139 [D loss: 0.663771, acc.: 56.25%] [G loss: 0.921049]\n",
      "epoch:16 step:15140 [D loss: 0.534417, acc.: 77.34%] [G loss: 1.023570]\n",
      "epoch:16 step:15141 [D loss: 0.619056, acc.: 63.28%] [G loss: 0.922142]\n",
      "epoch:16 step:15142 [D loss: 0.606161, acc.: 65.62%] [G loss: 1.084795]\n",
      "epoch:16 step:15143 [D loss: 0.655392, acc.: 65.62%] [G loss: 1.179235]\n",
      "epoch:16 step:15144 [D loss: 0.793982, acc.: 42.19%] [G loss: 1.343558]\n",
      "epoch:16 step:15145 [D loss: 0.529425, acc.: 77.34%] [G loss: 1.015538]\n",
      "epoch:16 step:15146 [D loss: 0.683055, acc.: 53.91%] [G loss: 0.849050]\n",
      "epoch:16 step:15147 [D loss: 0.656724, acc.: 56.25%] [G loss: 1.045682]\n",
      "epoch:16 step:15148 [D loss: 0.681644, acc.: 54.69%] [G loss: 0.862592]\n",
      "epoch:16 step:15149 [D loss: 0.760239, acc.: 53.12%] [G loss: 1.033837]\n",
      "epoch:16 step:15150 [D loss: 0.628484, acc.: 65.62%] [G loss: 1.063923]\n",
      "epoch:16 step:15151 [D loss: 0.601806, acc.: 64.06%] [G loss: 1.010097]\n",
      "epoch:16 step:15152 [D loss: 0.595323, acc.: 71.09%] [G loss: 0.780564]\n",
      "epoch:16 step:15153 [D loss: 0.632149, acc.: 55.47%] [G loss: 0.958405]\n",
      "epoch:16 step:15154 [D loss: 0.624227, acc.: 65.62%] [G loss: 1.005478]\n",
      "epoch:16 step:15155 [D loss: 0.674992, acc.: 59.38%] [G loss: 0.855169]\n",
      "epoch:16 step:15156 [D loss: 0.763666, acc.: 48.44%] [G loss: 0.925872]\n",
      "epoch:16 step:15157 [D loss: 0.516634, acc.: 76.56%] [G loss: 0.777581]\n",
      "epoch:16 step:15158 [D loss: 0.654219, acc.: 63.28%] [G loss: 0.787361]\n",
      "epoch:16 step:15159 [D loss: 0.630046, acc.: 67.19%] [G loss: 0.895261]\n",
      "epoch:16 step:15160 [D loss: 0.762107, acc.: 40.62%] [G loss: 0.915262]\n",
      "epoch:16 step:15161 [D loss: 0.619202, acc.: 66.41%] [G loss: 1.172592]\n",
      "epoch:16 step:15162 [D loss: 0.594146, acc.: 74.22%] [G loss: 1.045048]\n",
      "epoch:16 step:15163 [D loss: 0.778932, acc.: 49.22%] [G loss: 0.820376]\n",
      "epoch:16 step:15164 [D loss: 0.780105, acc.: 43.75%] [G loss: 0.849033]\n",
      "epoch:16 step:15165 [D loss: 0.681275, acc.: 58.59%] [G loss: 1.082163]\n",
      "epoch:16 step:15166 [D loss: 0.670749, acc.: 53.12%] [G loss: 1.022970]\n",
      "epoch:16 step:15167 [D loss: 0.741702, acc.: 47.66%] [G loss: 0.905892]\n",
      "epoch:16 step:15168 [D loss: 0.644951, acc.: 60.94%] [G loss: 0.873415]\n",
      "epoch:16 step:15169 [D loss: 0.625224, acc.: 64.06%] [G loss: 1.028733]\n",
      "epoch:16 step:15170 [D loss: 0.553563, acc.: 71.09%] [G loss: 0.946782]\n",
      "epoch:16 step:15171 [D loss: 0.636425, acc.: 61.72%] [G loss: 1.090066]\n",
      "epoch:16 step:15172 [D loss: 0.637850, acc.: 60.16%] [G loss: 0.847228]\n",
      "epoch:16 step:15173 [D loss: 0.704944, acc.: 50.78%] [G loss: 1.039785]\n",
      "epoch:16 step:15174 [D loss: 0.606709, acc.: 69.53%] [G loss: 0.862922]\n",
      "epoch:16 step:15175 [D loss: 0.696840, acc.: 58.59%] [G loss: 0.918796]\n",
      "epoch:16 step:15176 [D loss: 0.684810, acc.: 60.94%] [G loss: 1.061087]\n",
      "epoch:16 step:15177 [D loss: 0.737331, acc.: 46.09%] [G loss: 1.044572]\n",
      "epoch:16 step:15178 [D loss: 0.608199, acc.: 65.62%] [G loss: 0.961816]\n",
      "epoch:16 step:15179 [D loss: 0.734463, acc.: 44.53%] [G loss: 0.975963]\n",
      "epoch:16 step:15180 [D loss: 0.667334, acc.: 52.34%] [G loss: 0.951526]\n",
      "epoch:16 step:15181 [D loss: 0.628244, acc.: 64.84%] [G loss: 0.918268]\n",
      "epoch:16 step:15182 [D loss: 0.571029, acc.: 68.75%] [G loss: 0.961247]\n",
      "epoch:16 step:15183 [D loss: 0.692001, acc.: 58.59%] [G loss: 0.954839]\n",
      "epoch:16 step:15184 [D loss: 0.636413, acc.: 58.59%] [G loss: 0.845372]\n",
      "epoch:16 step:15185 [D loss: 0.590012, acc.: 65.62%] [G loss: 0.975029]\n",
      "epoch:16 step:15186 [D loss: 0.666258, acc.: 56.25%] [G loss: 0.945241]\n",
      "epoch:16 step:15187 [D loss: 0.602734, acc.: 64.06%] [G loss: 0.892870]\n",
      "epoch:16 step:15188 [D loss: 0.609714, acc.: 67.97%] [G loss: 0.945384]\n",
      "epoch:16 step:15189 [D loss: 0.539865, acc.: 73.44%] [G loss: 0.962313]\n",
      "epoch:16 step:15190 [D loss: 0.633475, acc.: 65.62%] [G loss: 1.059609]\n",
      "epoch:16 step:15191 [D loss: 0.607999, acc.: 56.25%] [G loss: 1.016862]\n",
      "epoch:16 step:15192 [D loss: 0.664915, acc.: 60.16%] [G loss: 1.030782]\n",
      "epoch:16 step:15193 [D loss: 0.556614, acc.: 75.00%] [G loss: 0.933172]\n",
      "epoch:16 step:15194 [D loss: 0.609393, acc.: 69.53%] [G loss: 0.920312]\n",
      "epoch:16 step:15195 [D loss: 0.564398, acc.: 75.78%] [G loss: 0.988934]\n",
      "epoch:16 step:15196 [D loss: 0.680491, acc.: 56.25%] [G loss: 0.924111]\n",
      "epoch:16 step:15197 [D loss: 0.520223, acc.: 85.16%] [G loss: 0.903307]\n",
      "epoch:16 step:15198 [D loss: 0.773772, acc.: 44.53%] [G loss: 1.065498]\n",
      "epoch:16 step:15199 [D loss: 0.614429, acc.: 64.06%] [G loss: 0.933419]\n",
      "epoch:16 step:15200 [D loss: 0.479661, acc.: 85.16%] [G loss: 1.197377]\n",
      "epoch:16 step:15201 [D loss: 0.656332, acc.: 60.16%] [G loss: 1.010880]\n",
      "epoch:16 step:15202 [D loss: 0.612349, acc.: 62.50%] [G loss: 1.023646]\n",
      "epoch:16 step:15203 [D loss: 0.638615, acc.: 64.06%] [G loss: 1.048110]\n",
      "epoch:16 step:15204 [D loss: 0.621985, acc.: 65.62%] [G loss: 1.578127]\n",
      "epoch:16 step:15205 [D loss: 0.600542, acc.: 71.09%] [G loss: 0.907853]\n",
      "epoch:16 step:15206 [D loss: 0.765830, acc.: 50.00%] [G loss: 0.974762]\n",
      "epoch:16 step:15207 [D loss: 0.545750, acc.: 78.91%] [G loss: 1.025551]\n",
      "epoch:16 step:15208 [D loss: 0.654081, acc.: 64.84%] [G loss: 0.966944]\n",
      "epoch:16 step:15209 [D loss: 0.545948, acc.: 77.34%] [G loss: 1.344485]\n",
      "epoch:16 step:15210 [D loss: 0.732935, acc.: 48.44%] [G loss: 0.618204]\n",
      "epoch:16 step:15211 [D loss: 0.623259, acc.: 62.50%] [G loss: 1.019408]\n",
      "epoch:16 step:15212 [D loss: 0.646382, acc.: 60.94%] [G loss: 0.917247]\n",
      "epoch:16 step:15213 [D loss: 0.689808, acc.: 53.12%] [G loss: 1.169820]\n",
      "epoch:16 step:15214 [D loss: 0.659543, acc.: 56.25%] [G loss: 0.983012]\n",
      "epoch:16 step:15215 [D loss: 0.569768, acc.: 75.00%] [G loss: 0.915666]\n",
      "epoch:16 step:15216 [D loss: 0.640647, acc.: 66.41%] [G loss: 1.275700]\n",
      "epoch:16 step:15217 [D loss: 0.632865, acc.: 60.94%] [G loss: 1.155795]\n",
      "epoch:16 step:15218 [D loss: 0.667070, acc.: 60.94%] [G loss: 0.922049]\n",
      "epoch:16 step:15219 [D loss: 0.565558, acc.: 71.09%] [G loss: 1.200062]\n",
      "epoch:16 step:15220 [D loss: 0.708322, acc.: 53.12%] [G loss: 0.837911]\n",
      "epoch:16 step:15221 [D loss: 0.718872, acc.: 49.22%] [G loss: 0.815246]\n",
      "epoch:16 step:15222 [D loss: 0.571042, acc.: 69.53%] [G loss: 0.964558]\n",
      "epoch:16 step:15223 [D loss: 0.520243, acc.: 78.12%] [G loss: 0.833248]\n",
      "epoch:16 step:15224 [D loss: 0.649484, acc.: 57.03%] [G loss: 0.860559]\n",
      "epoch:16 step:15225 [D loss: 0.800070, acc.: 40.62%] [G loss: 0.846682]\n",
      "epoch:16 step:15226 [D loss: 0.640272, acc.: 64.06%] [G loss: 1.065933]\n",
      "epoch:16 step:15227 [D loss: 0.657227, acc.: 57.03%] [G loss: 1.100364]\n",
      "epoch:16 step:15228 [D loss: 0.650168, acc.: 65.62%] [G loss: 0.910483]\n",
      "epoch:16 step:15229 [D loss: 0.684902, acc.: 55.47%] [G loss: 0.987513]\n",
      "epoch:16 step:15230 [D loss: 0.600448, acc.: 69.53%] [G loss: 0.915415]\n",
      "epoch:16 step:15231 [D loss: 0.698169, acc.: 50.78%] [G loss: 0.862120]\n",
      "epoch:16 step:15232 [D loss: 0.775861, acc.: 38.28%] [G loss: 0.863206]\n",
      "epoch:16 step:15233 [D loss: 0.717085, acc.: 47.66%] [G loss: 0.930983]\n",
      "epoch:16 step:15234 [D loss: 0.587053, acc.: 71.09%] [G loss: 1.007365]\n",
      "epoch:16 step:15235 [D loss: 0.627253, acc.: 64.84%] [G loss: 0.880473]\n",
      "epoch:16 step:15236 [D loss: 0.677484, acc.: 55.47%] [G loss: 0.990640]\n",
      "epoch:16 step:15237 [D loss: 0.728837, acc.: 50.78%] [G loss: 0.802922]\n",
      "epoch:16 step:15238 [D loss: 0.644869, acc.: 59.38%] [G loss: 0.930798]\n",
      "epoch:16 step:15239 [D loss: 0.651938, acc.: 57.81%] [G loss: 1.022011]\n",
      "epoch:16 step:15240 [D loss: 0.643268, acc.: 55.47%] [G loss: 0.921282]\n",
      "epoch:16 step:15241 [D loss: 0.690516, acc.: 61.72%] [G loss: 1.013691]\n",
      "epoch:16 step:15242 [D loss: 0.666063, acc.: 58.59%] [G loss: 1.124742]\n",
      "epoch:16 step:15243 [D loss: 0.598181, acc.: 66.41%] [G loss: 0.950906]\n",
      "epoch:16 step:15244 [D loss: 0.642089, acc.: 64.84%] [G loss: 0.855838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15245 [D loss: 0.674198, acc.: 55.47%] [G loss: 0.831538]\n",
      "epoch:16 step:15246 [D loss: 0.692324, acc.: 52.34%] [G loss: 0.817186]\n",
      "epoch:16 step:15247 [D loss: 0.596649, acc.: 67.97%] [G loss: 0.913227]\n",
      "epoch:16 step:15248 [D loss: 0.663140, acc.: 56.25%] [G loss: 0.947193]\n",
      "epoch:16 step:15249 [D loss: 0.614284, acc.: 61.72%] [G loss: 1.059207]\n",
      "epoch:16 step:15250 [D loss: 0.643358, acc.: 58.59%] [G loss: 0.838816]\n",
      "epoch:16 step:15251 [D loss: 0.701841, acc.: 57.81%] [G loss: 0.836570]\n",
      "epoch:16 step:15252 [D loss: 0.622846, acc.: 65.62%] [G loss: 1.141139]\n",
      "epoch:16 step:15253 [D loss: 0.585546, acc.: 71.88%] [G loss: 0.963675]\n",
      "epoch:16 step:15254 [D loss: 0.603701, acc.: 65.62%] [G loss: 1.043857]\n",
      "epoch:16 step:15255 [D loss: 0.661386, acc.: 53.91%] [G loss: 0.970198]\n",
      "epoch:16 step:15256 [D loss: 0.695894, acc.: 59.38%] [G loss: 0.897019]\n",
      "epoch:16 step:15257 [D loss: 0.660695, acc.: 60.94%] [G loss: 0.874114]\n",
      "epoch:16 step:15258 [D loss: 0.544934, acc.: 73.44%] [G loss: 1.007382]\n",
      "epoch:16 step:15259 [D loss: 0.664589, acc.: 60.16%] [G loss: 0.858791]\n",
      "epoch:16 step:15260 [D loss: 0.635989, acc.: 63.28%] [G loss: 0.825994]\n",
      "epoch:16 step:15261 [D loss: 0.643011, acc.: 67.97%] [G loss: 1.021866]\n",
      "epoch:16 step:15262 [D loss: 0.580968, acc.: 68.75%] [G loss: 1.112074]\n",
      "epoch:16 step:15263 [D loss: 0.579552, acc.: 71.09%] [G loss: 1.074966]\n",
      "epoch:16 step:15264 [D loss: 0.687749, acc.: 54.69%] [G loss: 1.080335]\n",
      "epoch:16 step:15265 [D loss: 0.566398, acc.: 72.66%] [G loss: 1.078135]\n",
      "epoch:16 step:15266 [D loss: 0.695853, acc.: 57.81%] [G loss: 1.095610]\n",
      "epoch:16 step:15267 [D loss: 0.564617, acc.: 71.88%] [G loss: 1.086696]\n",
      "epoch:16 step:15268 [D loss: 0.665463, acc.: 66.41%] [G loss: 1.306108]\n",
      "epoch:16 step:15269 [D loss: 0.635492, acc.: 60.16%] [G loss: 1.148534]\n",
      "epoch:16 step:15270 [D loss: 0.673235, acc.: 55.47%] [G loss: 1.175366]\n",
      "epoch:16 step:15271 [D loss: 0.680933, acc.: 52.34%] [G loss: 1.228682]\n",
      "epoch:16 step:15272 [D loss: 0.624939, acc.: 64.84%] [G loss: 0.888488]\n",
      "epoch:16 step:15273 [D loss: 0.787483, acc.: 46.88%] [G loss: 0.999390]\n",
      "epoch:16 step:15274 [D loss: 0.732308, acc.: 45.31%] [G loss: 0.879403]\n",
      "epoch:16 step:15275 [D loss: 0.554022, acc.: 70.31%] [G loss: 0.914673]\n",
      "epoch:16 step:15276 [D loss: 0.626933, acc.: 66.41%] [G loss: 1.064219]\n",
      "epoch:16 step:15277 [D loss: 0.530040, acc.: 78.12%] [G loss: 1.037196]\n",
      "epoch:16 step:15278 [D loss: 0.560906, acc.: 78.12%] [G loss: 1.000383]\n",
      "epoch:16 step:15279 [D loss: 0.567357, acc.: 74.22%] [G loss: 0.840570]\n",
      "epoch:16 step:15280 [D loss: 0.606764, acc.: 66.41%] [G loss: 0.734906]\n",
      "epoch:16 step:15281 [D loss: 0.610915, acc.: 67.97%] [G loss: 1.063016]\n",
      "epoch:16 step:15282 [D loss: 0.586069, acc.: 71.88%] [G loss: 0.762890]\n",
      "epoch:16 step:15283 [D loss: 0.640811, acc.: 64.06%] [G loss: 1.056670]\n",
      "epoch:16 step:15284 [D loss: 0.738953, acc.: 46.09%] [G loss: 1.017279]\n",
      "epoch:16 step:15285 [D loss: 0.561586, acc.: 71.88%] [G loss: 0.951460]\n",
      "epoch:16 step:15286 [D loss: 0.638625, acc.: 61.72%] [G loss: 1.013278]\n",
      "epoch:16 step:15287 [D loss: 0.652937, acc.: 60.94%] [G loss: 1.188948]\n",
      "epoch:16 step:15288 [D loss: 0.599532, acc.: 63.28%] [G loss: 1.044115]\n",
      "epoch:16 step:15289 [D loss: 0.621914, acc.: 64.84%] [G loss: 0.947703]\n",
      "epoch:16 step:15290 [D loss: 0.678246, acc.: 55.47%] [G loss: 0.976966]\n",
      "epoch:16 step:15291 [D loss: 0.607897, acc.: 68.75%] [G loss: 1.206997]\n",
      "epoch:16 step:15292 [D loss: 0.680024, acc.: 52.34%] [G loss: 1.098864]\n",
      "epoch:16 step:15293 [D loss: 0.638563, acc.: 62.50%] [G loss: 0.839820]\n",
      "epoch:16 step:15294 [D loss: 0.594408, acc.: 68.75%] [G loss: 1.251218]\n",
      "epoch:16 step:15295 [D loss: 0.700862, acc.: 47.66%] [G loss: 0.840226]\n",
      "epoch:16 step:15296 [D loss: 0.743091, acc.: 50.78%] [G loss: 1.100035]\n",
      "epoch:16 step:15297 [D loss: 0.597342, acc.: 68.75%] [G loss: 1.111537]\n",
      "epoch:16 step:15298 [D loss: 0.594162, acc.: 71.88%] [G loss: 0.858675]\n",
      "epoch:16 step:15299 [D loss: 0.526065, acc.: 79.69%] [G loss: 1.157536]\n",
      "epoch:16 step:15300 [D loss: 0.575388, acc.: 69.53%] [G loss: 0.821758]\n",
      "epoch:16 step:15301 [D loss: 0.574853, acc.: 74.22%] [G loss: 1.028432]\n",
      "epoch:16 step:15302 [D loss: 0.511754, acc.: 79.69%] [G loss: 1.157422]\n",
      "epoch:16 step:15303 [D loss: 0.591233, acc.: 66.41%] [G loss: 1.002780]\n",
      "epoch:16 step:15304 [D loss: 0.578293, acc.: 77.34%] [G loss: 1.111831]\n",
      "epoch:16 step:15305 [D loss: 0.683896, acc.: 56.25%] [G loss: 0.709411]\n",
      "epoch:16 step:15306 [D loss: 0.876773, acc.: 37.50%] [G loss: 1.023564]\n",
      "epoch:16 step:15307 [D loss: 0.428211, acc.: 79.69%] [G loss: 0.983592]\n",
      "epoch:16 step:15308 [D loss: 0.787992, acc.: 42.19%] [G loss: 1.065372]\n",
      "epoch:16 step:15309 [D loss: 0.615926, acc.: 62.50%] [G loss: 1.026006]\n",
      "epoch:16 step:15310 [D loss: 0.671075, acc.: 55.47%] [G loss: 0.897036]\n",
      "epoch:16 step:15311 [D loss: 0.724017, acc.: 49.22%] [G loss: 0.773656]\n",
      "epoch:16 step:15312 [D loss: 0.573815, acc.: 66.41%] [G loss: 0.974127]\n",
      "epoch:16 step:15313 [D loss: 0.613017, acc.: 67.19%] [G loss: 0.871638]\n",
      "epoch:16 step:15314 [D loss: 0.609184, acc.: 59.38%] [G loss: 0.868669]\n",
      "epoch:16 step:15315 [D loss: 0.579183, acc.: 70.31%] [G loss: 0.842805]\n",
      "epoch:16 step:15316 [D loss: 0.651293, acc.: 58.59%] [G loss: 0.898702]\n",
      "epoch:16 step:15317 [D loss: 0.716467, acc.: 49.22%] [G loss: 1.056497]\n",
      "epoch:16 step:15318 [D loss: 0.698501, acc.: 49.22%] [G loss: 0.776799]\n",
      "epoch:16 step:15319 [D loss: 0.642650, acc.: 63.28%] [G loss: 0.855051]\n",
      "epoch:16 step:15320 [D loss: 0.549960, acc.: 74.22%] [G loss: 1.053424]\n",
      "epoch:16 step:15321 [D loss: 0.562738, acc.: 71.88%] [G loss: 0.893248]\n",
      "epoch:16 step:15322 [D loss: 0.682851, acc.: 58.59%] [G loss: 1.034904]\n",
      "epoch:16 step:15323 [D loss: 0.654966, acc.: 57.81%] [G loss: 0.773513]\n",
      "epoch:16 step:15324 [D loss: 0.655000, acc.: 64.84%] [G loss: 0.931151]\n",
      "epoch:16 step:15325 [D loss: 0.526146, acc.: 80.47%] [G loss: 1.263519]\n",
      "epoch:16 step:15326 [D loss: 0.583223, acc.: 70.31%] [G loss: 1.293639]\n",
      "epoch:16 step:15327 [D loss: 0.609164, acc.: 67.19%] [G loss: 1.287175]\n",
      "epoch:16 step:15328 [D loss: 0.648927, acc.: 58.59%] [G loss: 0.955298]\n",
      "epoch:16 step:15329 [D loss: 0.705440, acc.: 55.47%] [G loss: 0.985656]\n",
      "epoch:16 step:15330 [D loss: 0.625972, acc.: 67.19%] [G loss: 0.895970]\n",
      "epoch:16 step:15331 [D loss: 0.586436, acc.: 68.75%] [G loss: 1.299817]\n",
      "epoch:16 step:15332 [D loss: 0.818639, acc.: 40.62%] [G loss: 0.815589]\n",
      "epoch:16 step:15333 [D loss: 0.613465, acc.: 67.97%] [G loss: 1.149006]\n",
      "epoch:16 step:15334 [D loss: 0.741206, acc.: 46.88%] [G loss: 0.876155]\n",
      "epoch:16 step:15335 [D loss: 0.684442, acc.: 55.47%] [G loss: 0.871331]\n",
      "epoch:16 step:15336 [D loss: 0.678138, acc.: 56.25%] [G loss: 0.941874]\n",
      "epoch:16 step:15337 [D loss: 0.569070, acc.: 70.31%] [G loss: 1.045577]\n",
      "epoch:16 step:15338 [D loss: 0.613814, acc.: 70.31%] [G loss: 0.970826]\n",
      "epoch:16 step:15339 [D loss: 0.626667, acc.: 64.06%] [G loss: 1.154478]\n",
      "epoch:16 step:15340 [D loss: 0.619208, acc.: 67.19%] [G loss: 0.986195]\n",
      "epoch:16 step:15341 [D loss: 0.707019, acc.: 53.12%] [G loss: 1.018304]\n",
      "epoch:16 step:15342 [D loss: 0.592269, acc.: 67.97%] [G loss: 1.090786]\n",
      "epoch:16 step:15343 [D loss: 0.610999, acc.: 67.19%] [G loss: 1.119271]\n",
      "epoch:16 step:15344 [D loss: 0.747696, acc.: 50.78%] [G loss: 1.080104]\n",
      "epoch:16 step:15345 [D loss: 0.613080, acc.: 70.31%] [G loss: 0.837475]\n",
      "epoch:16 step:15346 [D loss: 0.681699, acc.: 52.34%] [G loss: 0.890702]\n",
      "epoch:16 step:15347 [D loss: 0.676223, acc.: 64.84%] [G loss: 0.926371]\n",
      "epoch:16 step:15348 [D loss: 0.757470, acc.: 44.53%] [G loss: 0.871728]\n",
      "epoch:16 step:15349 [D loss: 0.789698, acc.: 42.97%] [G loss: 0.741017]\n",
      "epoch:16 step:15350 [D loss: 0.618886, acc.: 65.62%] [G loss: 0.847682]\n",
      "epoch:16 step:15351 [D loss: 0.744528, acc.: 54.69%] [G loss: 0.859120]\n",
      "epoch:16 step:15352 [D loss: 0.596628, acc.: 67.97%] [G loss: 0.834427]\n",
      "epoch:16 step:15353 [D loss: 0.621364, acc.: 61.72%] [G loss: 0.867503]\n",
      "epoch:16 step:15354 [D loss: 0.629332, acc.: 68.75%] [G loss: 0.990293]\n",
      "epoch:16 step:15355 [D loss: 0.664783, acc.: 58.59%] [G loss: 0.823871]\n",
      "epoch:16 step:15356 [D loss: 0.652094, acc.: 64.84%] [G loss: 0.826020]\n",
      "epoch:16 step:15357 [D loss: 0.594742, acc.: 67.19%] [G loss: 1.156835]\n",
      "epoch:16 step:15358 [D loss: 0.627180, acc.: 64.06%] [G loss: 1.162830]\n",
      "epoch:16 step:15359 [D loss: 0.665140, acc.: 64.06%] [G loss: 1.185873]\n",
      "epoch:16 step:15360 [D loss: 0.605827, acc.: 68.75%] [G loss: 0.925493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15361 [D loss: 0.665824, acc.: 55.47%] [G loss: 1.024575]\n",
      "epoch:16 step:15362 [D loss: 0.746231, acc.: 51.56%] [G loss: 0.981849]\n",
      "epoch:16 step:15363 [D loss: 0.731761, acc.: 44.53%] [G loss: 0.958772]\n",
      "epoch:16 step:15364 [D loss: 0.652388, acc.: 64.84%] [G loss: 1.100599]\n",
      "epoch:16 step:15365 [D loss: 0.576393, acc.: 72.66%] [G loss: 0.798844]\n",
      "epoch:16 step:15366 [D loss: 0.682493, acc.: 56.25%] [G loss: 0.863149]\n",
      "epoch:16 step:15367 [D loss: 0.593775, acc.: 72.66%] [G loss: 0.877506]\n",
      "epoch:16 step:15368 [D loss: 0.617617, acc.: 67.97%] [G loss: 0.993709]\n",
      "epoch:16 step:15369 [D loss: 0.656617, acc.: 64.06%] [G loss: 1.022835]\n",
      "epoch:16 step:15370 [D loss: 0.605920, acc.: 66.41%] [G loss: 1.071231]\n",
      "epoch:16 step:15371 [D loss: 0.590631, acc.: 71.88%] [G loss: 0.968620]\n",
      "epoch:16 step:15372 [D loss: 0.685558, acc.: 60.94%] [G loss: 0.915904]\n",
      "epoch:16 step:15373 [D loss: 0.666464, acc.: 59.38%] [G loss: 0.874408]\n",
      "epoch:16 step:15374 [D loss: 0.644068, acc.: 60.16%] [G loss: 1.021142]\n",
      "epoch:16 step:15375 [D loss: 0.665139, acc.: 60.94%] [G loss: 0.815900]\n",
      "epoch:16 step:15376 [D loss: 0.819423, acc.: 38.28%] [G loss: 1.014341]\n",
      "epoch:16 step:15377 [D loss: 0.679367, acc.: 58.59%] [G loss: 0.835518]\n",
      "epoch:16 step:15378 [D loss: 0.700348, acc.: 53.91%] [G loss: 0.974150]\n",
      "epoch:16 step:15379 [D loss: 0.613987, acc.: 65.62%] [G loss: 0.804807]\n",
      "epoch:16 step:15380 [D loss: 0.580582, acc.: 75.78%] [G loss: 0.777207]\n",
      "epoch:16 step:15381 [D loss: 0.738538, acc.: 50.00%] [G loss: 0.866485]\n",
      "epoch:16 step:15382 [D loss: 0.611006, acc.: 70.31%] [G loss: 0.968349]\n",
      "epoch:16 step:15383 [D loss: 0.723823, acc.: 50.00%] [G loss: 0.843876]\n",
      "epoch:16 step:15384 [D loss: 0.660590, acc.: 57.81%] [G loss: 1.002634]\n",
      "epoch:16 step:15385 [D loss: 0.657074, acc.: 57.81%] [G loss: 1.061374]\n",
      "epoch:16 step:15386 [D loss: 0.687978, acc.: 53.91%] [G loss: 0.885392]\n",
      "epoch:16 step:15387 [D loss: 0.594925, acc.: 67.97%] [G loss: 0.926474]\n",
      "epoch:16 step:15388 [D loss: 0.670810, acc.: 54.69%] [G loss: 0.933984]\n",
      "epoch:16 step:15389 [D loss: 0.633657, acc.: 67.97%] [G loss: 0.949410]\n",
      "epoch:16 step:15390 [D loss: 0.694487, acc.: 53.91%] [G loss: 1.004587]\n",
      "epoch:16 step:15391 [D loss: 0.612292, acc.: 64.06%] [G loss: 0.930487]\n",
      "epoch:16 step:15392 [D loss: 0.641468, acc.: 60.94%] [G loss: 0.844321]\n",
      "epoch:16 step:15393 [D loss: 0.610756, acc.: 60.94%] [G loss: 0.841415]\n",
      "epoch:16 step:15394 [D loss: 0.621399, acc.: 64.84%] [G loss: 0.826212]\n",
      "epoch:16 step:15395 [D loss: 0.597161, acc.: 70.31%] [G loss: 0.766727]\n",
      "epoch:16 step:15396 [D loss: 0.712679, acc.: 45.31%] [G loss: 0.874474]\n",
      "epoch:16 step:15397 [D loss: 0.581708, acc.: 75.00%] [G loss: 0.911581]\n",
      "epoch:16 step:15398 [D loss: 0.640209, acc.: 58.59%] [G loss: 0.998699]\n",
      "epoch:16 step:15399 [D loss: 0.564986, acc.: 75.00%] [G loss: 0.969943]\n",
      "epoch:16 step:15400 [D loss: 0.608199, acc.: 62.50%] [G loss: 0.982935]\n",
      "epoch:16 step:15401 [D loss: 0.584954, acc.: 72.66%] [G loss: 1.037378]\n",
      "epoch:16 step:15402 [D loss: 0.658396, acc.: 60.16%] [G loss: 1.143720]\n",
      "epoch:16 step:15403 [D loss: 0.643294, acc.: 60.16%] [G loss: 0.869462]\n",
      "epoch:16 step:15404 [D loss: 0.711587, acc.: 50.00%] [G loss: 0.993508]\n",
      "epoch:16 step:15405 [D loss: 0.668399, acc.: 65.62%] [G loss: 0.913094]\n",
      "epoch:16 step:15406 [D loss: 0.614919, acc.: 58.59%] [G loss: 1.013965]\n",
      "epoch:16 step:15407 [D loss: 0.554667, acc.: 75.78%] [G loss: 1.197619]\n",
      "epoch:16 step:15408 [D loss: 0.708667, acc.: 49.22%] [G loss: 0.911060]\n",
      "epoch:16 step:15409 [D loss: 0.639383, acc.: 62.50%] [G loss: 1.119659]\n",
      "epoch:16 step:15410 [D loss: 0.613510, acc.: 64.84%] [G loss: 1.142689]\n",
      "epoch:16 step:15411 [D loss: 0.654034, acc.: 60.94%] [G loss: 1.080922]\n",
      "epoch:16 step:15412 [D loss: 0.753552, acc.: 50.78%] [G loss: 1.108635]\n",
      "epoch:16 step:15413 [D loss: 0.618934, acc.: 67.97%] [G loss: 1.047363]\n",
      "epoch:16 step:15414 [D loss: 0.675743, acc.: 56.25%] [G loss: 0.927245]\n",
      "epoch:16 step:15415 [D loss: 0.621221, acc.: 68.75%] [G loss: 0.900226]\n",
      "epoch:16 step:15416 [D loss: 0.606612, acc.: 67.19%] [G loss: 0.940482]\n",
      "epoch:16 step:15417 [D loss: 0.616304, acc.: 67.97%] [G loss: 0.958515]\n",
      "epoch:16 step:15418 [D loss: 0.693875, acc.: 50.78%] [G loss: 0.900477]\n",
      "epoch:16 step:15419 [D loss: 0.560987, acc.: 75.00%] [G loss: 0.834214]\n",
      "epoch:16 step:15420 [D loss: 0.644195, acc.: 62.50%] [G loss: 0.878783]\n",
      "epoch:16 step:15421 [D loss: 0.601440, acc.: 67.97%] [G loss: 1.055649]\n",
      "epoch:16 step:15422 [D loss: 0.596825, acc.: 68.75%] [G loss: 0.881568]\n",
      "epoch:16 step:15423 [D loss: 0.511734, acc.: 81.25%] [G loss: 1.417719]\n",
      "epoch:16 step:15424 [D loss: 0.733579, acc.: 50.00%] [G loss: 1.031995]\n",
      "epoch:16 step:15425 [D loss: 0.596801, acc.: 70.31%] [G loss: 0.961037]\n",
      "epoch:16 step:15426 [D loss: 0.593723, acc.: 68.75%] [G loss: 1.029875]\n",
      "epoch:16 step:15427 [D loss: 0.577082, acc.: 70.31%] [G loss: 1.154139]\n",
      "epoch:16 step:15428 [D loss: 0.607561, acc.: 64.84%] [G loss: 1.189111]\n",
      "epoch:16 step:15429 [D loss: 0.685545, acc.: 53.91%] [G loss: 0.973551]\n",
      "epoch:16 step:15430 [D loss: 0.664951, acc.: 57.81%] [G loss: 1.053152]\n",
      "epoch:16 step:15431 [D loss: 0.654338, acc.: 59.38%] [G loss: 1.020688]\n",
      "epoch:16 step:15432 [D loss: 0.580621, acc.: 78.12%] [G loss: 1.165513]\n",
      "epoch:16 step:15433 [D loss: 0.658063, acc.: 57.03%] [G loss: 0.888309]\n",
      "epoch:16 step:15434 [D loss: 0.526250, acc.: 72.66%] [G loss: 0.862775]\n",
      "epoch:16 step:15435 [D loss: 0.610931, acc.: 69.53%] [G loss: 0.808783]\n",
      "epoch:16 step:15436 [D loss: 0.658095, acc.: 55.47%] [G loss: 0.911317]\n",
      "epoch:16 step:15437 [D loss: 0.651928, acc.: 53.91%] [G loss: 1.065805]\n",
      "epoch:16 step:15438 [D loss: 0.645923, acc.: 65.62%] [G loss: 0.899080]\n",
      "epoch:16 step:15439 [D loss: 0.622863, acc.: 65.62%] [G loss: 0.730905]\n",
      "epoch:16 step:15440 [D loss: 0.584012, acc.: 69.53%] [G loss: 0.789173]\n",
      "epoch:16 step:15441 [D loss: 0.632734, acc.: 64.84%] [G loss: 0.893657]\n",
      "epoch:16 step:15442 [D loss: 0.560319, acc.: 70.31%] [G loss: 0.724281]\n",
      "epoch:16 step:15443 [D loss: 0.667078, acc.: 59.38%] [G loss: 0.738049]\n",
      "epoch:16 step:15444 [D loss: 0.636044, acc.: 67.19%] [G loss: 0.909603]\n",
      "epoch:16 step:15445 [D loss: 0.623790, acc.: 63.28%] [G loss: 0.987873]\n",
      "epoch:16 step:15446 [D loss: 0.771370, acc.: 53.12%] [G loss: 0.895630]\n",
      "epoch:16 step:15447 [D loss: 0.647807, acc.: 59.38%] [G loss: 0.949154]\n",
      "epoch:16 step:15448 [D loss: 0.624063, acc.: 62.50%] [G loss: 0.828030]\n",
      "epoch:16 step:15449 [D loss: 0.642748, acc.: 58.59%] [G loss: 0.848618]\n",
      "epoch:16 step:15450 [D loss: 0.641194, acc.: 53.91%] [G loss: 0.969796]\n",
      "epoch:16 step:15451 [D loss: 0.680364, acc.: 52.34%] [G loss: 0.894449]\n",
      "epoch:16 step:15452 [D loss: 0.587595, acc.: 64.06%] [G loss: 1.060189]\n",
      "epoch:16 step:15453 [D loss: 0.750056, acc.: 46.09%] [G loss: 1.022678]\n",
      "epoch:16 step:15454 [D loss: 0.625507, acc.: 63.28%] [G loss: 0.982594]\n",
      "epoch:16 step:15455 [D loss: 0.675490, acc.: 53.12%] [G loss: 0.906458]\n",
      "epoch:16 step:15456 [D loss: 0.610539, acc.: 64.06%] [G loss: 0.918101]\n",
      "epoch:16 step:15457 [D loss: 0.601607, acc.: 63.28%] [G loss: 0.777348]\n",
      "epoch:16 step:15458 [D loss: 0.636704, acc.: 64.06%] [G loss: 1.051865]\n",
      "epoch:16 step:15459 [D loss: 0.592683, acc.: 76.56%] [G loss: 1.034680]\n",
      "epoch:16 step:15460 [D loss: 0.633565, acc.: 58.59%] [G loss: 0.877071]\n",
      "epoch:16 step:15461 [D loss: 0.677233, acc.: 64.84%] [G loss: 0.873458]\n",
      "epoch:16 step:15462 [D loss: 0.714599, acc.: 52.34%] [G loss: 0.823907]\n",
      "epoch:16 step:15463 [D loss: 0.607096, acc.: 64.84%] [G loss: 1.135122]\n",
      "epoch:16 step:15464 [D loss: 0.606677, acc.: 70.31%] [G loss: 0.988729]\n",
      "epoch:16 step:15465 [D loss: 0.637523, acc.: 64.84%] [G loss: 1.076000]\n",
      "epoch:16 step:15466 [D loss: 0.561482, acc.: 78.12%] [G loss: 0.975098]\n",
      "epoch:16 step:15467 [D loss: 0.582265, acc.: 71.88%] [G loss: 0.718740]\n",
      "epoch:16 step:15468 [D loss: 0.556929, acc.: 71.88%] [G loss: 0.899545]\n",
      "epoch:16 step:15469 [D loss: 0.720948, acc.: 53.91%] [G loss: 0.864623]\n",
      "epoch:16 step:15470 [D loss: 0.745341, acc.: 44.53%] [G loss: 0.889791]\n",
      "epoch:16 step:15471 [D loss: 0.664828, acc.: 57.81%] [G loss: 0.819340]\n",
      "epoch:16 step:15472 [D loss: 0.650862, acc.: 53.91%] [G loss: 0.890145]\n",
      "epoch:16 step:15473 [D loss: 0.740091, acc.: 48.44%] [G loss: 0.929821]\n",
      "epoch:16 step:15474 [D loss: 0.533692, acc.: 78.12%] [G loss: 1.026357]\n",
      "epoch:16 step:15475 [D loss: 0.726290, acc.: 55.47%] [G loss: 0.902534]\n",
      "epoch:16 step:15476 [D loss: 0.679103, acc.: 54.69%] [G loss: 0.932657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15477 [D loss: 0.604497, acc.: 69.53%] [G loss: 0.926778]\n",
      "epoch:16 step:15478 [D loss: 0.654052, acc.: 64.06%] [G loss: 1.004720]\n",
      "epoch:16 step:15479 [D loss: 0.648605, acc.: 62.50%] [G loss: 1.017478]\n",
      "epoch:16 step:15480 [D loss: 0.698745, acc.: 57.03%] [G loss: 0.873467]\n",
      "epoch:16 step:15481 [D loss: 0.690984, acc.: 57.81%] [G loss: 0.932286]\n",
      "epoch:16 step:15482 [D loss: 0.612151, acc.: 63.28%] [G loss: 1.011293]\n",
      "epoch:16 step:15483 [D loss: 0.686344, acc.: 54.69%] [G loss: 0.862820]\n",
      "epoch:16 step:15484 [D loss: 0.629312, acc.: 64.06%] [G loss: 0.937079]\n",
      "epoch:16 step:15485 [D loss: 0.563451, acc.: 72.66%] [G loss: 1.065560]\n",
      "epoch:16 step:15486 [D loss: 0.582899, acc.: 71.09%] [G loss: 0.881215]\n",
      "epoch:16 step:15487 [D loss: 0.656662, acc.: 61.72%] [G loss: 0.869238]\n",
      "epoch:16 step:15488 [D loss: 0.604780, acc.: 69.53%] [G loss: 0.938185]\n",
      "epoch:16 step:15489 [D loss: 0.648915, acc.: 57.03%] [G loss: 0.803297]\n",
      "epoch:16 step:15490 [D loss: 0.614867, acc.: 71.09%] [G loss: 1.079814]\n",
      "epoch:16 step:15491 [D loss: 0.631135, acc.: 63.28%] [G loss: 0.941299]\n",
      "epoch:16 step:15492 [D loss: 0.715673, acc.: 46.88%] [G loss: 0.965924]\n",
      "epoch:16 step:15493 [D loss: 0.620589, acc.: 60.16%] [G loss: 0.743464]\n",
      "epoch:16 step:15494 [D loss: 0.604983, acc.: 63.28%] [G loss: 0.872275]\n",
      "epoch:16 step:15495 [D loss: 0.650937, acc.: 60.16%] [G loss: 0.928984]\n",
      "epoch:16 step:15496 [D loss: 0.599024, acc.: 64.84%] [G loss: 0.783711]\n",
      "epoch:16 step:15497 [D loss: 0.585760, acc.: 70.31%] [G loss: 0.876014]\n",
      "epoch:16 step:15498 [D loss: 0.616064, acc.: 68.75%] [G loss: 0.970420]\n",
      "epoch:16 step:15499 [D loss: 0.585018, acc.: 67.19%] [G loss: 0.920470]\n",
      "epoch:16 step:15500 [D loss: 0.596950, acc.: 68.75%] [G loss: 0.863872]\n",
      "epoch:16 step:15501 [D loss: 0.525801, acc.: 73.44%] [G loss: 1.103007]\n",
      "epoch:16 step:15502 [D loss: 0.712346, acc.: 54.69%] [G loss: 0.975720]\n",
      "epoch:16 step:15503 [D loss: 0.603480, acc.: 63.28%] [G loss: 0.893194]\n",
      "epoch:16 step:15504 [D loss: 0.587157, acc.: 71.09%] [G loss: 1.029660]\n",
      "epoch:16 step:15505 [D loss: 0.712835, acc.: 50.78%] [G loss: 0.780994]\n",
      "epoch:16 step:15506 [D loss: 0.640777, acc.: 61.72%] [G loss: 0.976665]\n",
      "epoch:16 step:15507 [D loss: 0.707175, acc.: 47.66%] [G loss: 0.925462]\n",
      "epoch:16 step:15508 [D loss: 0.629609, acc.: 65.62%] [G loss: 0.722546]\n",
      "epoch:16 step:15509 [D loss: 0.613868, acc.: 67.97%] [G loss: 1.374670]\n",
      "epoch:16 step:15510 [D loss: 0.635712, acc.: 61.72%] [G loss: 1.488822]\n",
      "epoch:16 step:15511 [D loss: 0.611756, acc.: 61.72%] [G loss: 0.898531]\n",
      "epoch:16 step:15512 [D loss: 0.754789, acc.: 47.66%] [G loss: 0.947306]\n",
      "epoch:16 step:15513 [D loss: 0.534881, acc.: 75.00%] [G loss: 1.099506]\n",
      "epoch:16 step:15514 [D loss: 0.630022, acc.: 65.62%] [G loss: 0.956026]\n",
      "epoch:16 step:15515 [D loss: 0.670744, acc.: 56.25%] [G loss: 0.914393]\n",
      "epoch:16 step:15516 [D loss: 0.689400, acc.: 57.03%] [G loss: 0.934134]\n",
      "epoch:16 step:15517 [D loss: 0.566848, acc.: 76.56%] [G loss: 0.930512]\n",
      "epoch:16 step:15518 [D loss: 0.616827, acc.: 67.19%] [G loss: 1.000720]\n",
      "epoch:16 step:15519 [D loss: 0.671657, acc.: 55.47%] [G loss: 0.905543]\n",
      "epoch:16 step:15520 [D loss: 0.585081, acc.: 67.97%] [G loss: 1.137744]\n",
      "epoch:16 step:15521 [D loss: 0.598527, acc.: 69.53%] [G loss: 1.055139]\n",
      "epoch:16 step:15522 [D loss: 0.743108, acc.: 42.19%] [G loss: 0.867982]\n",
      "epoch:16 step:15523 [D loss: 0.682025, acc.: 58.59%] [G loss: 0.927871]\n",
      "epoch:16 step:15524 [D loss: 0.651041, acc.: 56.25%] [G loss: 0.998502]\n",
      "epoch:16 step:15525 [D loss: 0.591415, acc.: 64.84%] [G loss: 1.036241]\n",
      "epoch:16 step:15526 [D loss: 0.567853, acc.: 68.75%] [G loss: 1.016392]\n",
      "epoch:16 step:15527 [D loss: 0.559771, acc.: 71.88%] [G loss: 1.098157]\n",
      "epoch:16 step:15528 [D loss: 0.710892, acc.: 47.66%] [G loss: 1.054929]\n",
      "epoch:16 step:15529 [D loss: 0.697836, acc.: 54.69%] [G loss: 0.874933]\n",
      "epoch:16 step:15530 [D loss: 0.695477, acc.: 48.44%] [G loss: 1.006661]\n",
      "epoch:16 step:15531 [D loss: 0.636789, acc.: 62.50%] [G loss: 1.023940]\n",
      "epoch:16 step:15532 [D loss: 0.602679, acc.: 67.97%] [G loss: 0.938851]\n",
      "epoch:16 step:15533 [D loss: 0.590042, acc.: 70.31%] [G loss: 1.032467]\n",
      "epoch:16 step:15534 [D loss: 0.752085, acc.: 50.78%] [G loss: 0.890436]\n",
      "epoch:16 step:15535 [D loss: 0.606327, acc.: 64.06%] [G loss: 0.866071]\n",
      "epoch:16 step:15536 [D loss: 0.620506, acc.: 61.72%] [G loss: 0.964239]\n",
      "epoch:16 step:15537 [D loss: 0.627164, acc.: 64.06%] [G loss: 0.895062]\n",
      "epoch:16 step:15538 [D loss: 0.718017, acc.: 52.34%] [G loss: 1.376202]\n",
      "epoch:16 step:15539 [D loss: 0.695835, acc.: 59.38%] [G loss: 0.960512]\n",
      "epoch:16 step:15540 [D loss: 0.678732, acc.: 57.03%] [G loss: 1.067138]\n",
      "epoch:16 step:15541 [D loss: 0.668860, acc.: 53.91%] [G loss: 0.891320]\n",
      "epoch:16 step:15542 [D loss: 0.699493, acc.: 46.09%] [G loss: 0.867097]\n",
      "epoch:16 step:15543 [D loss: 0.593192, acc.: 69.53%] [G loss: 0.946111]\n",
      "epoch:16 step:15544 [D loss: 0.645001, acc.: 60.16%] [G loss: 0.834856]\n",
      "epoch:16 step:15545 [D loss: 0.696971, acc.: 61.72%] [G loss: 0.891317]\n",
      "epoch:16 step:15546 [D loss: 0.617549, acc.: 68.75%] [G loss: 0.807814]\n",
      "epoch:16 step:15547 [D loss: 0.535600, acc.: 76.56%] [G loss: 1.153500]\n",
      "epoch:16 step:15548 [D loss: 0.797118, acc.: 43.75%] [G loss: 0.901097]\n",
      "epoch:16 step:15549 [D loss: 0.686979, acc.: 57.03%] [G loss: 0.855734]\n",
      "epoch:16 step:15550 [D loss: 0.666598, acc.: 60.94%] [G loss: 0.982703]\n",
      "epoch:16 step:15551 [D loss: 0.690713, acc.: 52.34%] [G loss: 0.957832]\n",
      "epoch:16 step:15552 [D loss: 0.609020, acc.: 69.53%] [G loss: 1.003877]\n",
      "epoch:16 step:15553 [D loss: 0.626183, acc.: 61.72%] [G loss: 1.064574]\n",
      "epoch:16 step:15554 [D loss: 0.604931, acc.: 65.62%] [G loss: 1.034623]\n",
      "epoch:16 step:15555 [D loss: 0.633465, acc.: 64.06%] [G loss: 0.988797]\n",
      "epoch:16 step:15556 [D loss: 0.636076, acc.: 61.72%] [G loss: 0.784785]\n",
      "epoch:16 step:15557 [D loss: 0.707331, acc.: 46.88%] [G loss: 0.985927]\n",
      "epoch:16 step:15558 [D loss: 0.597834, acc.: 65.62%] [G loss: 0.913532]\n",
      "epoch:16 step:15559 [D loss: 0.623211, acc.: 65.62%] [G loss: 1.117151]\n",
      "epoch:16 step:15560 [D loss: 0.798586, acc.: 44.53%] [G loss: 0.913874]\n",
      "epoch:16 step:15561 [D loss: 0.537529, acc.: 75.78%] [G loss: 0.848437]\n",
      "epoch:16 step:15562 [D loss: 0.601862, acc.: 67.97%] [G loss: 1.141448]\n",
      "epoch:16 step:15563 [D loss: 0.591880, acc.: 71.09%] [G loss: 1.117762]\n",
      "epoch:16 step:15564 [D loss: 0.525732, acc.: 79.69%] [G loss: 1.010617]\n",
      "epoch:16 step:15565 [D loss: 0.575706, acc.: 68.75%] [G loss: 1.094749]\n",
      "epoch:16 step:15566 [D loss: 0.667082, acc.: 58.59%] [G loss: 0.879046]\n",
      "epoch:16 step:15567 [D loss: 0.665914, acc.: 56.25%] [G loss: 0.771555]\n",
      "epoch:16 step:15568 [D loss: 0.519209, acc.: 75.78%] [G loss: 0.949539]\n",
      "epoch:16 step:15569 [D loss: 0.677037, acc.: 59.38%] [G loss: 0.845364]\n",
      "epoch:16 step:15570 [D loss: 0.666101, acc.: 55.47%] [G loss: 0.909480]\n",
      "epoch:16 step:15571 [D loss: 0.695572, acc.: 55.47%] [G loss: 0.979318]\n",
      "epoch:16 step:15572 [D loss: 0.620479, acc.: 63.28%] [G loss: 0.915928]\n",
      "epoch:16 step:15573 [D loss: 0.642294, acc.: 62.50%] [G loss: 1.000109]\n",
      "epoch:16 step:15574 [D loss: 0.581693, acc.: 71.88%] [G loss: 0.999521]\n",
      "epoch:16 step:15575 [D loss: 0.704438, acc.: 47.66%] [G loss: 0.901693]\n",
      "epoch:16 step:15576 [D loss: 0.618515, acc.: 64.84%] [G loss: 0.869787]\n",
      "epoch:16 step:15577 [D loss: 0.589629, acc.: 69.53%] [G loss: 0.804801]\n",
      "epoch:16 step:15578 [D loss: 0.592432, acc.: 69.53%] [G loss: 0.955340]\n",
      "epoch:16 step:15579 [D loss: 0.648072, acc.: 60.16%] [G loss: 0.819461]\n",
      "epoch:16 step:15580 [D loss: 0.523596, acc.: 78.91%] [G loss: 1.157617]\n",
      "epoch:16 step:15581 [D loss: 0.651363, acc.: 60.16%] [G loss: 1.301570]\n",
      "epoch:16 step:15582 [D loss: 0.760354, acc.: 44.53%] [G loss: 1.174804]\n",
      "epoch:16 step:15583 [D loss: 0.650356, acc.: 60.16%] [G loss: 0.820469]\n",
      "epoch:16 step:15584 [D loss: 0.532157, acc.: 81.25%] [G loss: 0.792013]\n",
      "epoch:16 step:15585 [D loss: 0.761585, acc.: 43.75%] [G loss: 0.990919]\n",
      "epoch:16 step:15586 [D loss: 0.605282, acc.: 66.41%] [G loss: 0.866056]\n",
      "epoch:16 step:15587 [D loss: 0.577051, acc.: 70.31%] [G loss: 0.820100]\n",
      "epoch:16 step:15588 [D loss: 0.582214, acc.: 74.22%] [G loss: 0.763312]\n",
      "epoch:16 step:15589 [D loss: 0.686513, acc.: 52.34%] [G loss: 0.951249]\n",
      "epoch:16 step:15590 [D loss: 0.680790, acc.: 53.12%] [G loss: 0.799086]\n",
      "epoch:16 step:15591 [D loss: 0.745705, acc.: 46.88%] [G loss: 1.016509]\n",
      "epoch:16 step:15592 [D loss: 0.672041, acc.: 60.16%] [G loss: 1.103252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15593 [D loss: 0.526525, acc.: 85.94%] [G loss: 0.930501]\n",
      "epoch:16 step:15594 [D loss: 0.569585, acc.: 69.53%] [G loss: 1.057267]\n",
      "epoch:16 step:15595 [D loss: 0.502378, acc.: 82.81%] [G loss: 0.839459]\n",
      "epoch:16 step:15596 [D loss: 0.698150, acc.: 51.56%] [G loss: 0.890766]\n",
      "epoch:16 step:15597 [D loss: 0.619128, acc.: 64.84%] [G loss: 0.981327]\n",
      "epoch:16 step:15598 [D loss: 0.723994, acc.: 51.56%] [G loss: 0.866431]\n",
      "epoch:16 step:15599 [D loss: 0.600921, acc.: 67.19%] [G loss: 1.046712]\n",
      "epoch:16 step:15600 [D loss: 0.698050, acc.: 55.47%] [G loss: 1.015903]\n",
      "epoch:16 step:15601 [D loss: 0.681454, acc.: 49.22%] [G loss: 0.925132]\n",
      "epoch:16 step:15602 [D loss: 0.693707, acc.: 54.69%] [G loss: 0.845339]\n",
      "epoch:16 step:15603 [D loss: 0.679178, acc.: 57.03%] [G loss: 0.982146]\n",
      "epoch:16 step:15604 [D loss: 0.708122, acc.: 50.00%] [G loss: 0.959818]\n",
      "epoch:16 step:15605 [D loss: 0.641803, acc.: 60.16%] [G loss: 0.895109]\n",
      "epoch:16 step:15606 [D loss: 0.596347, acc.: 67.97%] [G loss: 0.882263]\n",
      "epoch:16 step:15607 [D loss: 0.705153, acc.: 58.59%] [G loss: 0.901568]\n",
      "epoch:16 step:15608 [D loss: 0.746241, acc.: 43.75%] [G loss: 0.925245]\n",
      "epoch:16 step:15609 [D loss: 0.666535, acc.: 59.38%] [G loss: 0.820725]\n",
      "epoch:16 step:15610 [D loss: 0.615648, acc.: 67.19%] [G loss: 0.842025]\n",
      "epoch:16 step:15611 [D loss: 0.716682, acc.: 49.22%] [G loss: 0.890095]\n",
      "epoch:16 step:15612 [D loss: 0.721214, acc.: 43.75%] [G loss: 1.056815]\n",
      "epoch:16 step:15613 [D loss: 0.664296, acc.: 60.16%] [G loss: 0.955121]\n",
      "epoch:16 step:15614 [D loss: 0.708075, acc.: 45.31%] [G loss: 0.954685]\n",
      "epoch:16 step:15615 [D loss: 0.648979, acc.: 62.50%] [G loss: 0.923009]\n",
      "epoch:16 step:15616 [D loss: 0.621182, acc.: 63.28%] [G loss: 0.938881]\n",
      "epoch:16 step:15617 [D loss: 0.776591, acc.: 45.31%] [G loss: 1.118264]\n",
      "epoch:16 step:15618 [D loss: 0.620159, acc.: 59.38%] [G loss: 0.911186]\n",
      "epoch:16 step:15619 [D loss: 0.591287, acc.: 70.31%] [G loss: 0.951945]\n",
      "epoch:16 step:15620 [D loss: 0.601828, acc.: 63.28%] [G loss: 1.000422]\n",
      "epoch:16 step:15621 [D loss: 0.638788, acc.: 60.16%] [G loss: 0.917000]\n",
      "epoch:16 step:15622 [D loss: 0.606972, acc.: 67.19%] [G loss: 1.029551]\n",
      "epoch:16 step:15623 [D loss: 0.665239, acc.: 60.16%] [G loss: 1.154389]\n",
      "epoch:16 step:15624 [D loss: 0.663299, acc.: 60.94%] [G loss: 1.042453]\n",
      "epoch:16 step:15625 [D loss: 0.643730, acc.: 64.84%] [G loss: 1.014761]\n",
      "epoch:16 step:15626 [D loss: 0.582026, acc.: 69.53%] [G loss: 0.986753]\n",
      "epoch:16 step:15627 [D loss: 0.526454, acc.: 78.91%] [G loss: 1.015624]\n",
      "epoch:16 step:15628 [D loss: 0.571465, acc.: 73.44%] [G loss: 1.108612]\n",
      "epoch:16 step:15629 [D loss: 0.604981, acc.: 64.06%] [G loss: 1.185554]\n",
      "epoch:16 step:15630 [D loss: 0.684185, acc.: 50.00%] [G loss: 1.149426]\n",
      "epoch:16 step:15631 [D loss: 0.549438, acc.: 75.00%] [G loss: 1.155148]\n",
      "epoch:16 step:15632 [D loss: 0.568555, acc.: 73.44%] [G loss: 0.809828]\n",
      "epoch:16 step:15633 [D loss: 0.599574, acc.: 68.75%] [G loss: 0.982081]\n",
      "epoch:16 step:15634 [D loss: 0.741159, acc.: 46.88%] [G loss: 0.868419]\n",
      "epoch:16 step:15635 [D loss: 0.508662, acc.: 81.25%] [G loss: 0.993041]\n",
      "epoch:16 step:15636 [D loss: 0.639765, acc.: 59.38%] [G loss: 0.771535]\n",
      "epoch:16 step:15637 [D loss: 0.671665, acc.: 62.50%] [G loss: 1.194402]\n",
      "epoch:16 step:15638 [D loss: 0.692563, acc.: 61.72%] [G loss: 1.043500]\n",
      "epoch:16 step:15639 [D loss: 0.678569, acc.: 57.81%] [G loss: 1.293591]\n",
      "epoch:16 step:15640 [D loss: 0.668910, acc.: 60.16%] [G loss: 1.179571]\n",
      "epoch:16 step:15641 [D loss: 0.776592, acc.: 43.75%] [G loss: 1.110552]\n",
      "epoch:16 step:15642 [D loss: 0.636208, acc.: 60.94%] [G loss: 1.000822]\n",
      "epoch:16 step:15643 [D loss: 0.619171, acc.: 62.50%] [G loss: 1.050617]\n",
      "epoch:16 step:15644 [D loss: 0.646590, acc.: 63.28%] [G loss: 1.044853]\n",
      "epoch:16 step:15645 [D loss: 0.633381, acc.: 69.53%] [G loss: 1.106818]\n",
      "epoch:16 step:15646 [D loss: 0.573811, acc.: 64.06%] [G loss: 0.999391]\n",
      "epoch:16 step:15647 [D loss: 0.623284, acc.: 67.19%] [G loss: 0.996788]\n",
      "epoch:16 step:15648 [D loss: 0.696341, acc.: 51.56%] [G loss: 1.136457]\n",
      "epoch:16 step:15649 [D loss: 0.640252, acc.: 64.06%] [G loss: 0.896498]\n",
      "epoch:16 step:15650 [D loss: 0.737216, acc.: 44.53%] [G loss: 1.306165]\n",
      "epoch:16 step:15651 [D loss: 0.573012, acc.: 67.97%] [G loss: 0.971394]\n",
      "epoch:16 step:15652 [D loss: 0.552294, acc.: 74.22%] [G loss: 0.900402]\n",
      "epoch:16 step:15653 [D loss: 0.718372, acc.: 59.38%] [G loss: 1.196227]\n",
      "epoch:16 step:15654 [D loss: 0.675491, acc.: 54.69%] [G loss: 1.033030]\n",
      "epoch:16 step:15655 [D loss: 0.561214, acc.: 71.09%] [G loss: 1.174133]\n",
      "epoch:16 step:15656 [D loss: 0.590258, acc.: 73.44%] [G loss: 1.142750]\n",
      "epoch:16 step:15657 [D loss: 0.634170, acc.: 61.72%] [G loss: 1.219221]\n",
      "epoch:16 step:15658 [D loss: 0.674971, acc.: 58.59%] [G loss: 0.966741]\n",
      "epoch:16 step:15659 [D loss: 0.597854, acc.: 63.28%] [G loss: 0.821163]\n",
      "epoch:16 step:15660 [D loss: 0.674443, acc.: 59.38%] [G loss: 1.080844]\n",
      "epoch:16 step:15661 [D loss: 0.594758, acc.: 65.62%] [G loss: 0.930710]\n",
      "epoch:16 step:15662 [D loss: 0.596344, acc.: 65.62%] [G loss: 0.864071]\n",
      "epoch:16 step:15663 [D loss: 0.677599, acc.: 54.69%] [G loss: 0.827834]\n",
      "epoch:16 step:15664 [D loss: 0.690245, acc.: 57.03%] [G loss: 1.333143]\n",
      "epoch:16 step:15665 [D loss: 0.551584, acc.: 75.00%] [G loss: 0.855749]\n",
      "epoch:16 step:15666 [D loss: 0.590283, acc.: 71.88%] [G loss: 1.285670]\n",
      "epoch:16 step:15667 [D loss: 0.602005, acc.: 66.41%] [G loss: 0.909373]\n",
      "epoch:16 step:15668 [D loss: 0.559302, acc.: 71.09%] [G loss: 1.209831]\n",
      "epoch:16 step:15669 [D loss: 0.677716, acc.: 56.25%] [G loss: 0.938101]\n",
      "epoch:16 step:15670 [D loss: 0.659819, acc.: 60.94%] [G loss: 0.903664]\n",
      "epoch:16 step:15671 [D loss: 0.658065, acc.: 62.50%] [G loss: 1.115984]\n",
      "epoch:16 step:15672 [D loss: 0.686060, acc.: 54.69%] [G loss: 1.025044]\n",
      "epoch:16 step:15673 [D loss: 0.639812, acc.: 65.62%] [G loss: 0.862324]\n",
      "epoch:16 step:15674 [D loss: 0.614639, acc.: 64.06%] [G loss: 0.972544]\n",
      "epoch:16 step:15675 [D loss: 0.637921, acc.: 61.72%] [G loss: 1.285380]\n",
      "epoch:16 step:15676 [D loss: 0.682701, acc.: 51.56%] [G loss: 0.917677]\n",
      "epoch:16 step:15677 [D loss: 0.618519, acc.: 65.62%] [G loss: 0.906717]\n",
      "epoch:16 step:15678 [D loss: 0.605331, acc.: 67.97%] [G loss: 0.838984]\n",
      "epoch:16 step:15679 [D loss: 0.754184, acc.: 51.56%] [G loss: 0.867200]\n",
      "epoch:16 step:15680 [D loss: 0.647261, acc.: 62.50%] [G loss: 0.974345]\n",
      "epoch:16 step:15681 [D loss: 0.651448, acc.: 61.72%] [G loss: 0.939403]\n",
      "epoch:16 step:15682 [D loss: 0.626432, acc.: 64.06%] [G loss: 1.014414]\n",
      "epoch:16 step:15683 [D loss: 0.615110, acc.: 66.41%] [G loss: 1.069668]\n",
      "epoch:16 step:15684 [D loss: 0.599215, acc.: 64.06%] [G loss: 1.037326]\n",
      "epoch:16 step:15685 [D loss: 0.749482, acc.: 46.09%] [G loss: 0.852852]\n",
      "epoch:16 step:15686 [D loss: 0.782104, acc.: 46.09%] [G loss: 1.098471]\n",
      "epoch:16 step:15687 [D loss: 0.718777, acc.: 50.78%] [G loss: 1.014634]\n",
      "epoch:16 step:15688 [D loss: 0.605046, acc.: 64.06%] [G loss: 0.989045]\n",
      "epoch:16 step:15689 [D loss: 0.581825, acc.: 71.09%] [G loss: 1.099935]\n",
      "epoch:16 step:15690 [D loss: 0.625010, acc.: 69.53%] [G loss: 0.785422]\n",
      "epoch:16 step:15691 [D loss: 0.597090, acc.: 70.31%] [G loss: 0.865161]\n",
      "epoch:16 step:15692 [D loss: 0.616846, acc.: 60.16%] [G loss: 1.100495]\n",
      "epoch:16 step:15693 [D loss: 0.652444, acc.: 64.84%] [G loss: 1.023442]\n",
      "epoch:16 step:15694 [D loss: 0.671841, acc.: 58.59%] [G loss: 0.917605]\n",
      "epoch:16 step:15695 [D loss: 0.648650, acc.: 59.38%] [G loss: 0.887534]\n",
      "epoch:16 step:15696 [D loss: 0.612365, acc.: 71.88%] [G loss: 1.029023]\n",
      "epoch:16 step:15697 [D loss: 0.702550, acc.: 53.12%] [G loss: 0.806312]\n",
      "epoch:16 step:15698 [D loss: 0.635387, acc.: 64.06%] [G loss: 0.888088]\n",
      "epoch:16 step:15699 [D loss: 0.566557, acc.: 74.22%] [G loss: 0.919937]\n",
      "epoch:16 step:15700 [D loss: 0.679110, acc.: 59.38%] [G loss: 0.861955]\n",
      "epoch:16 step:15701 [D loss: 0.650911, acc.: 59.38%] [G loss: 0.858628]\n",
      "epoch:16 step:15702 [D loss: 0.626058, acc.: 64.06%] [G loss: 0.988828]\n",
      "epoch:16 step:15703 [D loss: 0.488952, acc.: 78.91%] [G loss: 0.944470]\n",
      "epoch:16 step:15704 [D loss: 0.627977, acc.: 65.62%] [G loss: 0.974503]\n",
      "epoch:16 step:15705 [D loss: 0.611261, acc.: 63.28%] [G loss: 1.047818]\n",
      "epoch:16 step:15706 [D loss: 0.615818, acc.: 64.06%] [G loss: 1.128760]\n",
      "epoch:16 step:15707 [D loss: 0.613574, acc.: 66.41%] [G loss: 0.874044]\n",
      "epoch:16 step:15708 [D loss: 0.704342, acc.: 56.25%] [G loss: 0.956371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15709 [D loss: 0.769936, acc.: 45.31%] [G loss: 1.039450]\n",
      "epoch:16 step:15710 [D loss: 0.663322, acc.: 60.94%] [G loss: 0.790602]\n",
      "epoch:16 step:15711 [D loss: 0.609202, acc.: 67.19%] [G loss: 0.945502]\n",
      "epoch:16 step:15712 [D loss: 0.701882, acc.: 50.00%] [G loss: 0.834408]\n",
      "epoch:16 step:15713 [D loss: 0.711311, acc.: 53.12%] [G loss: 0.998334]\n",
      "epoch:16 step:15714 [D loss: 0.645734, acc.: 61.72%] [G loss: 0.934681]\n",
      "epoch:16 step:15715 [D loss: 0.710730, acc.: 51.56%] [G loss: 0.983595]\n",
      "epoch:16 step:15716 [D loss: 0.694614, acc.: 51.56%] [G loss: 1.003017]\n",
      "epoch:16 step:15717 [D loss: 0.671344, acc.: 61.72%] [G loss: 0.900128]\n",
      "epoch:16 step:15718 [D loss: 0.712139, acc.: 51.56%] [G loss: 0.880320]\n",
      "epoch:16 step:15719 [D loss: 0.619940, acc.: 62.50%] [G loss: 0.863712]\n",
      "epoch:16 step:15720 [D loss: 0.723529, acc.: 46.88%] [G loss: 1.019440]\n",
      "epoch:16 step:15721 [D loss: 0.755231, acc.: 48.44%] [G loss: 0.955991]\n",
      "epoch:16 step:15722 [D loss: 0.602254, acc.: 73.44%] [G loss: 0.887644]\n",
      "epoch:16 step:15723 [D loss: 0.849165, acc.: 46.09%] [G loss: 0.868768]\n",
      "epoch:16 step:15724 [D loss: 0.689539, acc.: 56.25%] [G loss: 0.761758]\n",
      "epoch:16 step:15725 [D loss: 0.580355, acc.: 74.22%] [G loss: 0.970322]\n",
      "epoch:16 step:15726 [D loss: 0.701627, acc.: 53.91%] [G loss: 1.037071]\n",
      "epoch:16 step:15727 [D loss: 0.602570, acc.: 69.53%] [G loss: 0.832358]\n",
      "epoch:16 step:15728 [D loss: 0.587839, acc.: 71.09%] [G loss: 0.926794]\n",
      "epoch:16 step:15729 [D loss: 0.533420, acc.: 78.91%] [G loss: 0.950558]\n",
      "epoch:16 step:15730 [D loss: 0.609301, acc.: 68.75%] [G loss: 1.004080]\n",
      "epoch:16 step:15731 [D loss: 0.628492, acc.: 63.28%] [G loss: 0.860812]\n",
      "epoch:16 step:15732 [D loss: 0.620900, acc.: 64.06%] [G loss: 0.953068]\n",
      "epoch:16 step:15733 [D loss: 0.595641, acc.: 71.09%] [G loss: 0.955686]\n",
      "epoch:16 step:15734 [D loss: 0.507940, acc.: 82.03%] [G loss: 0.852941]\n",
      "epoch:16 step:15735 [D loss: 0.555023, acc.: 76.56%] [G loss: 1.067800]\n",
      "epoch:16 step:15736 [D loss: 0.611648, acc.: 66.41%] [G loss: 0.901852]\n",
      "epoch:16 step:15737 [D loss: 0.565637, acc.: 67.97%] [G loss: 1.125213]\n",
      "epoch:16 step:15738 [D loss: 0.628194, acc.: 63.28%] [G loss: 0.942910]\n",
      "epoch:16 step:15739 [D loss: 0.507044, acc.: 78.12%] [G loss: 0.965330]\n",
      "epoch:16 step:15740 [D loss: 0.536545, acc.: 76.56%] [G loss: 0.940101]\n",
      "epoch:16 step:15741 [D loss: 0.454087, acc.: 87.50%] [G loss: 0.900049]\n",
      "epoch:16 step:15742 [D loss: 0.724632, acc.: 48.44%] [G loss: 1.016731]\n",
      "epoch:16 step:15743 [D loss: 0.836682, acc.: 35.16%] [G loss: 0.877938]\n",
      "epoch:16 step:15744 [D loss: 0.768642, acc.: 45.31%] [G loss: 0.897040]\n",
      "epoch:16 step:15745 [D loss: 0.723145, acc.: 53.12%] [G loss: 1.118850]\n",
      "epoch:16 step:15746 [D loss: 0.632486, acc.: 68.75%] [G loss: 0.938412]\n",
      "epoch:16 step:15747 [D loss: 0.569202, acc.: 70.31%] [G loss: 0.824702]\n",
      "epoch:16 step:15748 [D loss: 0.652488, acc.: 55.47%] [G loss: 0.842671]\n",
      "epoch:16 step:15749 [D loss: 0.639611, acc.: 64.06%] [G loss: 1.191309]\n",
      "epoch:16 step:15750 [D loss: 0.600420, acc.: 69.53%] [G loss: 0.960647]\n",
      "epoch:16 step:15751 [D loss: 0.770651, acc.: 46.09%] [G loss: 0.849258]\n",
      "epoch:16 step:15752 [D loss: 0.673525, acc.: 58.59%] [G loss: 0.933773]\n",
      "epoch:16 step:15753 [D loss: 0.687412, acc.: 58.59%] [G loss: 0.964869]\n",
      "epoch:16 step:15754 [D loss: 0.613916, acc.: 70.31%] [G loss: 1.235678]\n",
      "epoch:16 step:15755 [D loss: 0.591606, acc.: 71.09%] [G loss: 1.140042]\n",
      "epoch:16 step:15756 [D loss: 0.594886, acc.: 64.06%] [G loss: 1.332004]\n",
      "epoch:16 step:15757 [D loss: 0.557931, acc.: 72.66%] [G loss: 0.955890]\n",
      "epoch:16 step:15758 [D loss: 0.630438, acc.: 62.50%] [G loss: 1.201374]\n",
      "epoch:16 step:15759 [D loss: 0.517419, acc.: 82.81%] [G loss: 1.213035]\n",
      "epoch:16 step:15760 [D loss: 0.710361, acc.: 54.69%] [G loss: 0.946055]\n",
      "epoch:16 step:15761 [D loss: 0.542271, acc.: 75.78%] [G loss: 0.896639]\n",
      "epoch:16 step:15762 [D loss: 0.599432, acc.: 69.53%] [G loss: 0.889934]\n",
      "epoch:16 step:15763 [D loss: 0.706226, acc.: 54.69%] [G loss: 1.012443]\n",
      "epoch:16 step:15764 [D loss: 0.621080, acc.: 64.84%] [G loss: 0.987371]\n",
      "epoch:16 step:15765 [D loss: 0.659918, acc.: 53.91%] [G loss: 1.072650]\n",
      "epoch:16 step:15766 [D loss: 0.810765, acc.: 38.28%] [G loss: 0.892751]\n",
      "epoch:16 step:15767 [D loss: 0.584964, acc.: 68.75%] [G loss: 0.805109]\n",
      "epoch:16 step:15768 [D loss: 0.663335, acc.: 53.91%] [G loss: 0.880925]\n",
      "epoch:16 step:15769 [D loss: 0.684270, acc.: 59.38%] [G loss: 1.442941]\n",
      "epoch:16 step:15770 [D loss: 0.673848, acc.: 53.12%] [G loss: 0.951128]\n",
      "epoch:16 step:15771 [D loss: 0.570364, acc.: 74.22%] [G loss: 0.776252]\n",
      "epoch:16 step:15772 [D loss: 0.603174, acc.: 67.19%] [G loss: 0.874552]\n",
      "epoch:16 step:15773 [D loss: 0.686359, acc.: 50.00%] [G loss: 1.028477]\n",
      "epoch:16 step:15774 [D loss: 0.643502, acc.: 63.28%] [G loss: 0.910662]\n",
      "epoch:16 step:15775 [D loss: 0.661761, acc.: 62.50%] [G loss: 1.027969]\n",
      "epoch:16 step:15776 [D loss: 0.603652, acc.: 67.19%] [G loss: 1.016407]\n",
      "epoch:16 step:15777 [D loss: 0.668748, acc.: 57.81%] [G loss: 0.972302]\n",
      "epoch:16 step:15778 [D loss: 0.641634, acc.: 60.16%] [G loss: 1.315899]\n",
      "epoch:16 step:15779 [D loss: 0.666711, acc.: 55.47%] [G loss: 1.018387]\n",
      "epoch:16 step:15780 [D loss: 0.639427, acc.: 67.19%] [G loss: 1.074424]\n",
      "epoch:16 step:15781 [D loss: 0.710839, acc.: 51.56%] [G loss: 1.110313]\n",
      "epoch:16 step:15782 [D loss: 0.711680, acc.: 53.91%] [G loss: 0.963888]\n",
      "epoch:16 step:15783 [D loss: 0.561177, acc.: 75.78%] [G loss: 1.028598]\n",
      "epoch:16 step:15784 [D loss: 0.590153, acc.: 72.66%] [G loss: 0.903584]\n",
      "epoch:16 step:15785 [D loss: 0.658595, acc.: 57.03%] [G loss: 0.988728]\n",
      "epoch:16 step:15786 [D loss: 0.638810, acc.: 64.84%] [G loss: 1.078329]\n",
      "epoch:16 step:15787 [D loss: 0.545843, acc.: 75.78%] [G loss: 0.836484]\n",
      "epoch:16 step:15788 [D loss: 0.640033, acc.: 66.41%] [G loss: 0.888787]\n",
      "epoch:16 step:15789 [D loss: 0.604208, acc.: 64.84%] [G loss: 0.817332]\n",
      "epoch:16 step:15790 [D loss: 0.606589, acc.: 67.97%] [G loss: 1.123872]\n",
      "epoch:16 step:15791 [D loss: 0.622943, acc.: 62.50%] [G loss: 1.076027]\n",
      "epoch:16 step:15792 [D loss: 0.599962, acc.: 70.31%] [G loss: 1.203430]\n",
      "epoch:16 step:15793 [D loss: 0.565510, acc.: 69.53%] [G loss: 0.840944]\n",
      "epoch:16 step:15794 [D loss: 0.741531, acc.: 42.19%] [G loss: 1.144039]\n",
      "epoch:16 step:15795 [D loss: 0.662301, acc.: 63.28%] [G loss: 0.984414]\n",
      "epoch:16 step:15796 [D loss: 0.608065, acc.: 66.41%] [G loss: 0.860240]\n",
      "epoch:16 step:15797 [D loss: 0.612635, acc.: 67.97%] [G loss: 1.396293]\n",
      "epoch:16 step:15798 [D loss: 0.717858, acc.: 53.91%] [G loss: 0.900492]\n",
      "epoch:16 step:15799 [D loss: 0.576768, acc.: 74.22%] [G loss: 0.929592]\n",
      "epoch:16 step:15800 [D loss: 0.541555, acc.: 75.00%] [G loss: 0.961729]\n",
      "epoch:16 step:15801 [D loss: 0.551816, acc.: 71.88%] [G loss: 1.138617]\n",
      "epoch:16 step:15802 [D loss: 0.614392, acc.: 64.06%] [G loss: 0.911240]\n",
      "epoch:16 step:15803 [D loss: 0.683683, acc.: 56.25%] [G loss: 0.943568]\n",
      "epoch:16 step:15804 [D loss: 0.627129, acc.: 66.41%] [G loss: 1.009547]\n",
      "epoch:16 step:15805 [D loss: 0.702685, acc.: 55.47%] [G loss: 1.053053]\n",
      "epoch:16 step:15806 [D loss: 0.649236, acc.: 55.47%] [G loss: 1.189796]\n",
      "epoch:16 step:15807 [D loss: 0.675448, acc.: 57.81%] [G loss: 0.928935]\n",
      "epoch:16 step:15808 [D loss: 0.648400, acc.: 58.59%] [G loss: 1.040431]\n",
      "epoch:16 step:15809 [D loss: 0.634865, acc.: 60.94%] [G loss: 1.012109]\n",
      "epoch:16 step:15810 [D loss: 0.627788, acc.: 66.41%] [G loss: 1.092192]\n",
      "epoch:16 step:15811 [D loss: 0.543591, acc.: 73.44%] [G loss: 1.102904]\n",
      "epoch:16 step:15812 [D loss: 0.572330, acc.: 72.66%] [G loss: 0.863091]\n",
      "epoch:16 step:15813 [D loss: 0.606402, acc.: 68.75%] [G loss: 1.093057]\n",
      "epoch:16 step:15814 [D loss: 0.612978, acc.: 67.97%] [G loss: 1.280331]\n",
      "epoch:16 step:15815 [D loss: 0.621570, acc.: 67.19%] [G loss: 1.116241]\n",
      "epoch:16 step:15816 [D loss: 0.563149, acc.: 72.66%] [G loss: 1.142730]\n",
      "epoch:16 step:15817 [D loss: 0.657115, acc.: 60.16%] [G loss: 1.216885]\n",
      "epoch:16 step:15818 [D loss: 0.568299, acc.: 71.09%] [G loss: 1.180574]\n",
      "epoch:16 step:15819 [D loss: 0.603717, acc.: 67.97%] [G loss: 0.981447]\n",
      "epoch:16 step:15820 [D loss: 0.646719, acc.: 55.47%] [G loss: 0.890675]\n",
      "epoch:16 step:15821 [D loss: 0.640548, acc.: 64.06%] [G loss: 1.116209]\n",
      "epoch:16 step:15822 [D loss: 0.618815, acc.: 61.72%] [G loss: 0.965509]\n",
      "epoch:16 step:15823 [D loss: 0.613937, acc.: 72.66%] [G loss: 1.083219]\n",
      "epoch:16 step:15824 [D loss: 0.662896, acc.: 60.16%] [G loss: 1.009589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15825 [D loss: 0.628371, acc.: 62.50%] [G loss: 1.157897]\n",
      "epoch:16 step:15826 [D loss: 0.613477, acc.: 64.06%] [G loss: 1.152332]\n",
      "epoch:16 step:15827 [D loss: 0.647767, acc.: 63.28%] [G loss: 1.055384]\n",
      "epoch:16 step:15828 [D loss: 0.634568, acc.: 65.62%] [G loss: 1.200383]\n",
      "epoch:16 step:15829 [D loss: 0.568970, acc.: 75.00%] [G loss: 0.970728]\n",
      "epoch:16 step:15830 [D loss: 0.556880, acc.: 80.47%] [G loss: 1.192847]\n",
      "epoch:16 step:15831 [D loss: 0.656720, acc.: 57.03%] [G loss: 1.000757]\n",
      "epoch:16 step:15832 [D loss: 0.693672, acc.: 53.12%] [G loss: 0.910542]\n",
      "epoch:16 step:15833 [D loss: 0.583555, acc.: 68.75%] [G loss: 0.967384]\n",
      "epoch:16 step:15834 [D loss: 0.573740, acc.: 71.09%] [G loss: 1.053061]\n",
      "epoch:16 step:15835 [D loss: 0.575844, acc.: 70.31%] [G loss: 1.018907]\n",
      "epoch:16 step:15836 [D loss: 0.687190, acc.: 53.91%] [G loss: 1.249401]\n",
      "epoch:16 step:15837 [D loss: 0.703757, acc.: 55.47%] [G loss: 0.739865]\n",
      "epoch:16 step:15838 [D loss: 0.652523, acc.: 60.16%] [G loss: 0.985458]\n",
      "epoch:16 step:15839 [D loss: 0.630392, acc.: 63.28%] [G loss: 0.823043]\n",
      "epoch:16 step:15840 [D loss: 0.702410, acc.: 53.91%] [G loss: 1.024759]\n",
      "epoch:16 step:15841 [D loss: 0.593662, acc.: 68.75%] [G loss: 0.910314]\n",
      "epoch:16 step:15842 [D loss: 0.532669, acc.: 78.91%] [G loss: 1.224802]\n",
      "epoch:16 step:15843 [D loss: 0.605648, acc.: 66.41%] [G loss: 0.770680]\n",
      "epoch:16 step:15844 [D loss: 0.635213, acc.: 67.19%] [G loss: 1.055806]\n",
      "epoch:16 step:15845 [D loss: 0.519453, acc.: 77.34%] [G loss: 1.034451]\n",
      "epoch:16 step:15846 [D loss: 0.598845, acc.: 64.06%] [G loss: 1.078815]\n",
      "epoch:16 step:15847 [D loss: 0.627502, acc.: 67.97%] [G loss: 0.828559]\n",
      "epoch:16 step:15848 [D loss: 0.478777, acc.: 79.69%] [G loss: 0.728427]\n",
      "epoch:16 step:15849 [D loss: 0.780533, acc.: 35.94%] [G loss: 0.876536]\n",
      "epoch:16 step:15850 [D loss: 0.634231, acc.: 56.25%] [G loss: 1.168153]\n",
      "epoch:16 step:15851 [D loss: 0.675946, acc.: 53.91%] [G loss: 0.930287]\n",
      "epoch:16 step:15852 [D loss: 0.621422, acc.: 64.06%] [G loss: 1.034207]\n",
      "epoch:16 step:15853 [D loss: 0.569614, acc.: 75.00%] [G loss: 1.187765]\n",
      "epoch:16 step:15854 [D loss: 0.625306, acc.: 72.66%] [G loss: 1.151769]\n",
      "epoch:16 step:15855 [D loss: 0.549891, acc.: 76.56%] [G loss: 1.108685]\n",
      "epoch:16 step:15856 [D loss: 0.575106, acc.: 73.44%] [G loss: 1.011250]\n",
      "epoch:16 step:15857 [D loss: 0.511954, acc.: 79.69%] [G loss: 0.954435]\n",
      "epoch:16 step:15858 [D loss: 0.588784, acc.: 67.19%] [G loss: 1.139912]\n",
      "epoch:16 step:15859 [D loss: 0.694915, acc.: 51.56%] [G loss: 0.907046]\n",
      "epoch:16 step:15860 [D loss: 0.591592, acc.: 68.75%] [G loss: 0.895893]\n",
      "epoch:16 step:15861 [D loss: 0.539754, acc.: 78.91%] [G loss: 0.990010]\n",
      "epoch:16 step:15862 [D loss: 0.519981, acc.: 82.03%] [G loss: 1.078988]\n",
      "epoch:16 step:15863 [D loss: 0.579943, acc.: 64.06%] [G loss: 0.987551]\n",
      "epoch:16 step:15864 [D loss: 0.539551, acc.: 70.31%] [G loss: 1.004930]\n",
      "epoch:16 step:15865 [D loss: 0.539254, acc.: 71.88%] [G loss: 1.369248]\n",
      "epoch:16 step:15866 [D loss: 0.701984, acc.: 56.25%] [G loss: 1.163559]\n",
      "epoch:16 step:15867 [D loss: 0.513852, acc.: 82.03%] [G loss: 0.974622]\n",
      "epoch:16 step:15868 [D loss: 0.658597, acc.: 58.59%] [G loss: 0.709837]\n",
      "epoch:16 step:15869 [D loss: 0.744548, acc.: 49.22%] [G loss: 1.012971]\n",
      "epoch:16 step:15870 [D loss: 0.666547, acc.: 57.03%] [G loss: 0.886036]\n",
      "epoch:16 step:15871 [D loss: 0.523856, acc.: 76.56%] [G loss: 0.959171]\n",
      "epoch:16 step:15872 [D loss: 0.652169, acc.: 53.91%] [G loss: 0.956041]\n",
      "epoch:16 step:15873 [D loss: 0.561835, acc.: 71.09%] [G loss: 1.505027]\n",
      "epoch:16 step:15874 [D loss: 0.472412, acc.: 85.94%] [G loss: 1.088899]\n",
      "epoch:16 step:15875 [D loss: 0.545379, acc.: 71.88%] [G loss: 1.465178]\n",
      "epoch:16 step:15876 [D loss: 0.567952, acc.: 73.44%] [G loss: 0.938032]\n",
      "epoch:16 step:15877 [D loss: 0.781941, acc.: 38.28%] [G loss: 0.967520]\n",
      "epoch:16 step:15878 [D loss: 0.649760, acc.: 64.06%] [G loss: 0.804110]\n",
      "epoch:16 step:15879 [D loss: 0.638965, acc.: 60.94%] [G loss: 1.517086]\n",
      "epoch:16 step:15880 [D loss: 0.800527, acc.: 45.31%] [G loss: 1.324474]\n",
      "epoch:16 step:15881 [D loss: 0.628341, acc.: 61.72%] [G loss: 0.958167]\n",
      "epoch:16 step:15882 [D loss: 0.528917, acc.: 75.00%] [G loss: 1.329713]\n",
      "epoch:16 step:15883 [D loss: 0.604312, acc.: 64.84%] [G loss: 1.158453]\n",
      "epoch:16 step:15884 [D loss: 0.515937, acc.: 75.00%] [G loss: 1.086017]\n",
      "epoch:16 step:15885 [D loss: 0.541745, acc.: 75.00%] [G loss: 1.036493]\n",
      "epoch:16 step:15886 [D loss: 0.581172, acc.: 69.53%] [G loss: 1.121879]\n",
      "epoch:16 step:15887 [D loss: 0.657956, acc.: 57.81%] [G loss: 1.124112]\n",
      "epoch:16 step:15888 [D loss: 0.613660, acc.: 64.84%] [G loss: 0.870995]\n",
      "epoch:16 step:15889 [D loss: 0.659625, acc.: 60.16%] [G loss: 0.948576]\n",
      "epoch:16 step:15890 [D loss: 0.595293, acc.: 69.53%] [G loss: 1.352264]\n",
      "epoch:16 step:15891 [D loss: 0.678944, acc.: 53.91%] [G loss: 1.077397]\n",
      "epoch:16 step:15892 [D loss: 0.627994, acc.: 60.16%] [G loss: 1.313172]\n",
      "epoch:16 step:15893 [D loss: 0.638648, acc.: 59.38%] [G loss: 1.091003]\n",
      "epoch:16 step:15894 [D loss: 0.643919, acc.: 64.06%] [G loss: 1.034493]\n",
      "epoch:16 step:15895 [D loss: 0.614771, acc.: 71.09%] [G loss: 1.148777]\n",
      "epoch:16 step:15896 [D loss: 0.875380, acc.: 35.94%] [G loss: 0.714221]\n",
      "epoch:16 step:15897 [D loss: 0.548765, acc.: 73.44%] [G loss: 0.847354]\n",
      "epoch:16 step:15898 [D loss: 0.543534, acc.: 74.22%] [G loss: 0.681498]\n",
      "epoch:16 step:15899 [D loss: 0.649921, acc.: 65.62%] [G loss: 0.779301]\n",
      "epoch:16 step:15900 [D loss: 0.786327, acc.: 37.50%] [G loss: 0.834167]\n",
      "epoch:16 step:15901 [D loss: 0.791507, acc.: 42.97%] [G loss: 0.970242]\n",
      "epoch:16 step:15902 [D loss: 0.777143, acc.: 49.22%] [G loss: 1.139666]\n",
      "epoch:16 step:15903 [D loss: 0.624239, acc.: 63.28%] [G loss: 1.197394]\n",
      "epoch:16 step:15904 [D loss: 0.615562, acc.: 59.38%] [G loss: 1.001402]\n",
      "epoch:16 step:15905 [D loss: 0.524774, acc.: 72.66%] [G loss: 1.124732]\n",
      "epoch:16 step:15906 [D loss: 0.476015, acc.: 83.59%] [G loss: 1.100314]\n",
      "epoch:16 step:15907 [D loss: 0.552730, acc.: 77.34%] [G loss: 0.860474]\n",
      "epoch:16 step:15908 [D loss: 0.455366, acc.: 86.72%] [G loss: 1.157637]\n",
      "epoch:16 step:15909 [D loss: 0.587122, acc.: 68.75%] [G loss: 1.016029]\n",
      "epoch:16 step:15910 [D loss: 0.641482, acc.: 67.19%] [G loss: 1.047184]\n",
      "epoch:16 step:15911 [D loss: 0.730451, acc.: 58.59%] [G loss: 0.778437]\n",
      "epoch:16 step:15912 [D loss: 0.724420, acc.: 48.44%] [G loss: 0.729958]\n",
      "epoch:16 step:15913 [D loss: 0.688647, acc.: 56.25%] [G loss: 0.822988]\n",
      "epoch:16 step:15914 [D loss: 0.676733, acc.: 54.69%] [G loss: 0.706993]\n",
      "epoch:16 step:15915 [D loss: 0.520948, acc.: 80.47%] [G loss: 1.281704]\n",
      "epoch:16 step:15916 [D loss: 0.564443, acc.: 70.31%] [G loss: 0.910011]\n",
      "epoch:16 step:15917 [D loss: 0.630059, acc.: 75.78%] [G loss: 0.843052]\n",
      "epoch:16 step:15918 [D loss: 0.421062, acc.: 89.84%] [G loss: 1.119766]\n",
      "epoch:16 step:15919 [D loss: 0.689759, acc.: 53.12%] [G loss: 0.862678]\n",
      "epoch:16 step:15920 [D loss: 0.705877, acc.: 57.81%] [G loss: 0.674034]\n",
      "epoch:16 step:15921 [D loss: 0.692644, acc.: 59.38%] [G loss: 0.858842]\n",
      "epoch:16 step:15922 [D loss: 0.577127, acc.: 74.22%] [G loss: 0.858772]\n",
      "epoch:16 step:15923 [D loss: 0.739925, acc.: 47.66%] [G loss: 1.437079]\n",
      "epoch:16 step:15924 [D loss: 0.694047, acc.: 59.38%] [G loss: 1.141611]\n",
      "epoch:16 step:15925 [D loss: 0.701085, acc.: 54.69%] [G loss: 1.157753]\n",
      "epoch:16 step:15926 [D loss: 0.541254, acc.: 72.66%] [G loss: 1.033695]\n",
      "epoch:16 step:15927 [D loss: 0.607850, acc.: 63.28%] [G loss: 1.187542]\n",
      "epoch:16 step:15928 [D loss: 0.559682, acc.: 74.22%] [G loss: 1.333102]\n",
      "epoch:16 step:15929 [D loss: 0.784063, acc.: 45.31%] [G loss: 1.242036]\n",
      "epoch:17 step:15930 [D loss: 0.528727, acc.: 78.12%] [G loss: 0.980036]\n",
      "epoch:17 step:15931 [D loss: 0.570791, acc.: 75.78%] [G loss: 1.077215]\n",
      "epoch:17 step:15932 [D loss: 0.842562, acc.: 36.72%] [G loss: 1.327600]\n",
      "epoch:17 step:15933 [D loss: 0.681254, acc.: 60.16%] [G loss: 1.007335]\n",
      "epoch:17 step:15934 [D loss: 0.507772, acc.: 80.47%] [G loss: 0.998982]\n",
      "epoch:17 step:15935 [D loss: 0.611277, acc.: 64.84%] [G loss: 0.790915]\n",
      "epoch:17 step:15936 [D loss: 0.629830, acc.: 60.94%] [G loss: 1.146016]\n",
      "epoch:17 step:15937 [D loss: 0.736226, acc.: 54.69%] [G loss: 1.082407]\n",
      "epoch:17 step:15938 [D loss: 0.660435, acc.: 59.38%] [G loss: 0.927523]\n",
      "epoch:17 step:15939 [D loss: 0.563791, acc.: 72.66%] [G loss: 1.129874]\n",
      "epoch:17 step:15940 [D loss: 0.611335, acc.: 62.50%] [G loss: 1.142033]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:15941 [D loss: 0.567013, acc.: 69.53%] [G loss: 1.126792]\n",
      "epoch:17 step:15942 [D loss: 0.588962, acc.: 69.53%] [G loss: 0.770204]\n",
      "epoch:17 step:15943 [D loss: 0.619889, acc.: 61.72%] [G loss: 1.202580]\n",
      "epoch:17 step:15944 [D loss: 0.533406, acc.: 72.66%] [G loss: 1.434048]\n",
      "epoch:17 step:15945 [D loss: 0.673346, acc.: 60.16%] [G loss: 1.134744]\n",
      "epoch:17 step:15946 [D loss: 0.593737, acc.: 64.84%] [G loss: 1.050577]\n",
      "epoch:17 step:15947 [D loss: 0.525760, acc.: 78.12%] [G loss: 0.923598]\n",
      "epoch:17 step:15948 [D loss: 0.638979, acc.: 65.62%] [G loss: 1.369208]\n",
      "epoch:17 step:15949 [D loss: 0.590622, acc.: 67.19%] [G loss: 1.212785]\n",
      "epoch:17 step:15950 [D loss: 0.667515, acc.: 59.38%] [G loss: 0.902523]\n",
      "epoch:17 step:15951 [D loss: 0.694079, acc.: 54.69%] [G loss: 0.974023]\n",
      "epoch:17 step:15952 [D loss: 0.573579, acc.: 68.75%] [G loss: 1.003404]\n",
      "epoch:17 step:15953 [D loss: 0.437366, acc.: 85.94%] [G loss: 1.310861]\n",
      "epoch:17 step:15954 [D loss: 0.659069, acc.: 57.81%] [G loss: 0.949459]\n",
      "epoch:17 step:15955 [D loss: 0.519836, acc.: 75.78%] [G loss: 0.970172]\n",
      "epoch:17 step:15956 [D loss: 0.679865, acc.: 60.16%] [G loss: 0.808248]\n",
      "epoch:17 step:15957 [D loss: 0.578936, acc.: 69.53%] [G loss: 1.335039]\n",
      "epoch:17 step:15958 [D loss: 0.638688, acc.: 57.03%] [G loss: 1.113433]\n",
      "epoch:17 step:15959 [D loss: 0.557865, acc.: 76.56%] [G loss: 0.888745]\n",
      "epoch:17 step:15960 [D loss: 0.686273, acc.: 60.94%] [G loss: 1.078479]\n",
      "epoch:17 step:15961 [D loss: 0.671042, acc.: 57.03%] [G loss: 1.212247]\n",
      "epoch:17 step:15962 [D loss: 0.536679, acc.: 72.66%] [G loss: 0.896097]\n",
      "epoch:17 step:15963 [D loss: 0.663306, acc.: 62.50%] [G loss: 1.309535]\n",
      "epoch:17 step:15964 [D loss: 0.655002, acc.: 57.81%] [G loss: 1.001018]\n",
      "epoch:17 step:15965 [D loss: 0.589387, acc.: 73.44%] [G loss: 1.247171]\n",
      "epoch:17 step:15966 [D loss: 0.537706, acc.: 79.69%] [G loss: 1.049277]\n",
      "epoch:17 step:15967 [D loss: 0.703779, acc.: 48.44%] [G loss: 1.180739]\n",
      "epoch:17 step:15968 [D loss: 0.650224, acc.: 59.38%] [G loss: 0.878633]\n",
      "epoch:17 step:15969 [D loss: 0.658636, acc.: 57.81%] [G loss: 0.900193]\n",
      "epoch:17 step:15970 [D loss: 0.666443, acc.: 57.03%] [G loss: 0.962058]\n",
      "epoch:17 step:15971 [D loss: 0.912443, acc.: 30.47%] [G loss: 0.939175]\n",
      "epoch:17 step:15972 [D loss: 0.534444, acc.: 78.12%] [G loss: 0.793905]\n",
      "epoch:17 step:15973 [D loss: 0.705328, acc.: 48.44%] [G loss: 1.212353]\n",
      "epoch:17 step:15974 [D loss: 0.606447, acc.: 67.97%] [G loss: 0.805532]\n",
      "epoch:17 step:15975 [D loss: 0.594639, acc.: 60.94%] [G loss: 1.044655]\n",
      "epoch:17 step:15976 [D loss: 0.651527, acc.: 63.28%] [G loss: 1.113099]\n",
      "epoch:17 step:15977 [D loss: 0.507683, acc.: 89.84%] [G loss: 0.931548]\n",
      "epoch:17 step:15978 [D loss: 0.554105, acc.: 77.34%] [G loss: 0.854206]\n",
      "epoch:17 step:15979 [D loss: 0.669988, acc.: 58.59%] [G loss: 0.890754]\n",
      "epoch:17 step:15980 [D loss: 0.477073, acc.: 83.59%] [G loss: 1.018289]\n",
      "epoch:17 step:15981 [D loss: 0.606367, acc.: 67.97%] [G loss: 0.996794]\n",
      "epoch:17 step:15982 [D loss: 0.616141, acc.: 64.06%] [G loss: 0.985545]\n",
      "epoch:17 step:15983 [D loss: 0.577777, acc.: 67.19%] [G loss: 0.730961]\n",
      "epoch:17 step:15984 [D loss: 0.510000, acc.: 80.47%] [G loss: 1.106955]\n",
      "epoch:17 step:15985 [D loss: 0.599322, acc.: 65.62%] [G loss: 0.944732]\n",
      "epoch:17 step:15986 [D loss: 0.594818, acc.: 61.72%] [G loss: 1.164949]\n",
      "epoch:17 step:15987 [D loss: 0.629971, acc.: 61.72%] [G loss: 1.032532]\n",
      "epoch:17 step:15988 [D loss: 0.648761, acc.: 56.25%] [G loss: 0.951886]\n",
      "epoch:17 step:15989 [D loss: 0.667301, acc.: 60.16%] [G loss: 0.979418]\n",
      "epoch:17 step:15990 [D loss: 0.512451, acc.: 78.12%] [G loss: 0.989622]\n",
      "epoch:17 step:15991 [D loss: 0.520155, acc.: 78.91%] [G loss: 1.255968]\n",
      "epoch:17 step:15992 [D loss: 0.464741, acc.: 86.72%] [G loss: 0.972604]\n",
      "epoch:17 step:15993 [D loss: 0.494029, acc.: 82.81%] [G loss: 1.224244]\n",
      "epoch:17 step:15994 [D loss: 0.769961, acc.: 39.06%] [G loss: 0.960470]\n",
      "epoch:17 step:15995 [D loss: 0.627968, acc.: 64.06%] [G loss: 0.777382]\n",
      "epoch:17 step:15996 [D loss: 0.591290, acc.: 67.19%] [G loss: 1.636769]\n",
      "epoch:17 step:15997 [D loss: 0.575794, acc.: 69.53%] [G loss: 1.164200]\n",
      "epoch:17 step:15998 [D loss: 0.599007, acc.: 69.53%] [G loss: 0.857584]\n",
      "epoch:17 step:15999 [D loss: 0.551075, acc.: 77.34%] [G loss: 1.182781]\n",
      "epoch:17 step:16000 [D loss: 0.754590, acc.: 46.88%] [G loss: 0.966029]\n",
      "epoch:17 step:16001 [D loss: 0.641692, acc.: 64.84%] [G loss: 0.974127]\n",
      "epoch:17 step:16002 [D loss: 0.444683, acc.: 92.97%] [G loss: 1.350067]\n",
      "epoch:17 step:16003 [D loss: 0.608986, acc.: 67.97%] [G loss: 0.963864]\n",
      "epoch:17 step:16004 [D loss: 0.634340, acc.: 60.94%] [G loss: 0.921677]\n",
      "epoch:17 step:16005 [D loss: 0.739065, acc.: 50.78%] [G loss: 0.842068]\n",
      "epoch:17 step:16006 [D loss: 0.733286, acc.: 55.47%] [G loss: 0.786396]\n",
      "epoch:17 step:16007 [D loss: 0.578798, acc.: 73.44%] [G loss: 0.978946]\n",
      "epoch:17 step:16008 [D loss: 0.578849, acc.: 65.62%] [G loss: 0.914460]\n",
      "epoch:17 step:16009 [D loss: 0.736436, acc.: 50.78%] [G loss: 0.950602]\n",
      "epoch:17 step:16010 [D loss: 0.592226, acc.: 67.19%] [G loss: 1.237211]\n",
      "epoch:17 step:16011 [D loss: 0.599981, acc.: 65.62%] [G loss: 0.928895]\n",
      "epoch:17 step:16012 [D loss: 0.480265, acc.: 84.38%] [G loss: 1.222102]\n",
      "epoch:17 step:16013 [D loss: 0.609894, acc.: 67.19%] [G loss: 1.269760]\n",
      "epoch:17 step:16014 [D loss: 0.507425, acc.: 72.66%] [G loss: 1.056136]\n",
      "epoch:17 step:16015 [D loss: 0.566704, acc.: 71.09%] [G loss: 1.091995]\n",
      "epoch:17 step:16016 [D loss: 0.447255, acc.: 85.16%] [G loss: 1.117573]\n",
      "epoch:17 step:16017 [D loss: 0.562604, acc.: 75.00%] [G loss: 0.820794]\n",
      "epoch:17 step:16018 [D loss: 0.455934, acc.: 83.59%] [G loss: 0.759704]\n",
      "epoch:17 step:16019 [D loss: 0.530027, acc.: 79.69%] [G loss: 1.056552]\n",
      "epoch:17 step:16020 [D loss: 0.613327, acc.: 68.75%] [G loss: 1.139400]\n",
      "epoch:17 step:16021 [D loss: 0.676989, acc.: 57.03%] [G loss: 0.956930]\n",
      "epoch:17 step:16022 [D loss: 0.730832, acc.: 47.66%] [G loss: 0.924473]\n",
      "epoch:17 step:16023 [D loss: 0.673597, acc.: 60.94%] [G loss: 0.952698]\n",
      "epoch:17 step:16024 [D loss: 0.614802, acc.: 65.62%] [G loss: 1.341473]\n",
      "epoch:17 step:16025 [D loss: 0.551640, acc.: 79.69%] [G loss: 1.673832]\n",
      "epoch:17 step:16026 [D loss: 0.719960, acc.: 52.34%] [G loss: 1.165410]\n",
      "epoch:17 step:16027 [D loss: 0.720745, acc.: 54.69%] [G loss: 1.140301]\n",
      "epoch:17 step:16028 [D loss: 0.499024, acc.: 75.78%] [G loss: 1.323262]\n",
      "epoch:17 step:16029 [D loss: 0.578936, acc.: 71.09%] [G loss: 0.734882]\n",
      "epoch:17 step:16030 [D loss: 0.635131, acc.: 65.62%] [G loss: 0.732540]\n",
      "epoch:17 step:16031 [D loss: 0.653881, acc.: 61.72%] [G loss: 0.957217]\n",
      "epoch:17 step:16032 [D loss: 0.612077, acc.: 64.06%] [G loss: 1.496347]\n",
      "epoch:17 step:16033 [D loss: 0.733254, acc.: 50.00%] [G loss: 0.943489]\n",
      "epoch:17 step:16034 [D loss: 0.676556, acc.: 54.69%] [G loss: 1.096560]\n",
      "epoch:17 step:16035 [D loss: 0.652835, acc.: 66.41%] [G loss: 1.096598]\n",
      "epoch:17 step:16036 [D loss: 0.667049, acc.: 65.62%] [G loss: 0.930663]\n",
      "epoch:17 step:16037 [D loss: 0.710992, acc.: 51.56%] [G loss: 0.877458]\n",
      "epoch:17 step:16038 [D loss: 0.687505, acc.: 58.59%] [G loss: 0.941077]\n",
      "epoch:17 step:16039 [D loss: 0.499620, acc.: 76.56%] [G loss: 0.851173]\n",
      "epoch:17 step:16040 [D loss: 0.714705, acc.: 44.53%] [G loss: 0.907687]\n",
      "epoch:17 step:16041 [D loss: 0.687109, acc.: 49.22%] [G loss: 1.093038]\n",
      "epoch:17 step:16042 [D loss: 0.682368, acc.: 56.25%] [G loss: 0.960074]\n",
      "epoch:17 step:16043 [D loss: 0.614532, acc.: 68.75%] [G loss: 1.007419]\n",
      "epoch:17 step:16044 [D loss: 0.726541, acc.: 50.78%] [G loss: 0.723846]\n",
      "epoch:17 step:16045 [D loss: 0.769239, acc.: 42.19%] [G loss: 0.715560]\n",
      "epoch:17 step:16046 [D loss: 0.719871, acc.: 50.78%] [G loss: 0.768819]\n",
      "epoch:17 step:16047 [D loss: 0.650189, acc.: 61.72%] [G loss: 0.897159]\n",
      "epoch:17 step:16048 [D loss: 0.667285, acc.: 53.91%] [G loss: 1.040843]\n",
      "epoch:17 step:16049 [D loss: 0.609834, acc.: 66.41%] [G loss: 0.945577]\n",
      "epoch:17 step:16050 [D loss: 0.562864, acc.: 68.75%] [G loss: 0.879840]\n",
      "epoch:17 step:16051 [D loss: 0.636016, acc.: 57.03%] [G loss: 1.398585]\n",
      "epoch:17 step:16052 [D loss: 0.594069, acc.: 74.22%] [G loss: 1.115156]\n",
      "epoch:17 step:16053 [D loss: 0.749652, acc.: 50.00%] [G loss: 0.681351]\n",
      "epoch:17 step:16054 [D loss: 0.632911, acc.: 62.50%] [G loss: 0.900570]\n",
      "epoch:17 step:16055 [D loss: 0.664613, acc.: 58.59%] [G loss: 0.831428]\n",
      "epoch:17 step:16056 [D loss: 0.684003, acc.: 54.69%] [G loss: 0.970723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16057 [D loss: 0.615496, acc.: 63.28%] [G loss: 0.857047]\n",
      "epoch:17 step:16058 [D loss: 0.642912, acc.: 59.38%] [G loss: 0.881221]\n",
      "epoch:17 step:16059 [D loss: 0.600470, acc.: 65.62%] [G loss: 0.978267]\n",
      "epoch:17 step:16060 [D loss: 0.647368, acc.: 66.41%] [G loss: 0.846282]\n",
      "epoch:17 step:16061 [D loss: 0.738570, acc.: 45.31%] [G loss: 0.884291]\n",
      "epoch:17 step:16062 [D loss: 0.638653, acc.: 59.38%] [G loss: 0.778493]\n",
      "epoch:17 step:16063 [D loss: 0.660658, acc.: 63.28%] [G loss: 0.882030]\n",
      "epoch:17 step:16064 [D loss: 0.634838, acc.: 64.84%] [G loss: 0.941984]\n",
      "epoch:17 step:16065 [D loss: 0.515050, acc.: 80.47%] [G loss: 0.868703]\n",
      "epoch:17 step:16066 [D loss: 0.587178, acc.: 71.09%] [G loss: 0.948388]\n",
      "epoch:17 step:16067 [D loss: 0.721529, acc.: 44.53%] [G loss: 0.754654]\n",
      "epoch:17 step:16068 [D loss: 0.576784, acc.: 75.78%] [G loss: 0.936485]\n",
      "epoch:17 step:16069 [D loss: 0.674781, acc.: 57.81%] [G loss: 1.121635]\n",
      "epoch:17 step:16070 [D loss: 0.597518, acc.: 64.06%] [G loss: 0.968666]\n",
      "epoch:17 step:16071 [D loss: 0.640217, acc.: 61.72%] [G loss: 0.808152]\n",
      "epoch:17 step:16072 [D loss: 0.623281, acc.: 67.97%] [G loss: 1.307392]\n",
      "epoch:17 step:16073 [D loss: 0.665842, acc.: 61.72%] [G loss: 1.038798]\n",
      "epoch:17 step:16074 [D loss: 0.618562, acc.: 71.88%] [G loss: 1.182962]\n",
      "epoch:17 step:16075 [D loss: 0.463593, acc.: 85.94%] [G loss: 1.106781]\n",
      "epoch:17 step:16076 [D loss: 0.634794, acc.: 60.94%] [G loss: 0.864507]\n",
      "epoch:17 step:16077 [D loss: 0.560378, acc.: 70.31%] [G loss: 1.257198]\n",
      "epoch:17 step:16078 [D loss: 0.547776, acc.: 72.66%] [G loss: 1.008081]\n",
      "epoch:17 step:16079 [D loss: 0.546352, acc.: 75.78%] [G loss: 1.279485]\n",
      "epoch:17 step:16080 [D loss: 0.848774, acc.: 46.09%] [G loss: 1.204592]\n",
      "epoch:17 step:16081 [D loss: 0.759828, acc.: 42.97%] [G loss: 1.106125]\n",
      "epoch:17 step:16082 [D loss: 0.677584, acc.: 58.59%] [G loss: 1.117440]\n",
      "epoch:17 step:16083 [D loss: 0.674816, acc.: 58.59%] [G loss: 1.013741]\n",
      "epoch:17 step:16084 [D loss: 0.581434, acc.: 72.66%] [G loss: 0.929134]\n",
      "epoch:17 step:16085 [D loss: 0.586955, acc.: 71.09%] [G loss: 1.137209]\n",
      "epoch:17 step:16086 [D loss: 0.664741, acc.: 52.34%] [G loss: 0.872169]\n",
      "epoch:17 step:16087 [D loss: 0.629279, acc.: 61.72%] [G loss: 1.106780]\n",
      "epoch:17 step:16088 [D loss: 0.663411, acc.: 61.72%] [G loss: 1.194586]\n",
      "epoch:17 step:16089 [D loss: 0.604298, acc.: 67.19%] [G loss: 0.910186]\n",
      "epoch:17 step:16090 [D loss: 0.638277, acc.: 63.28%] [G loss: 1.108190]\n",
      "epoch:17 step:16091 [D loss: 0.688057, acc.: 57.03%] [G loss: 0.926357]\n",
      "epoch:17 step:16092 [D loss: 0.677753, acc.: 57.03%] [G loss: 1.117418]\n",
      "epoch:17 step:16093 [D loss: 0.595684, acc.: 68.75%] [G loss: 1.080128]\n",
      "epoch:17 step:16094 [D loss: 0.658375, acc.: 57.03%] [G loss: 1.063644]\n",
      "epoch:17 step:16095 [D loss: 0.669853, acc.: 57.81%] [G loss: 0.788834]\n",
      "epoch:17 step:16096 [D loss: 0.669638, acc.: 58.59%] [G loss: 0.807834]\n",
      "epoch:17 step:16097 [D loss: 0.626189, acc.: 64.06%] [G loss: 1.068624]\n",
      "epoch:17 step:16098 [D loss: 0.668560, acc.: 58.59%] [G loss: 0.997700]\n",
      "epoch:17 step:16099 [D loss: 0.715788, acc.: 49.22%] [G loss: 1.112984]\n",
      "epoch:17 step:16100 [D loss: 0.606898, acc.: 67.97%] [G loss: 0.934503]\n",
      "epoch:17 step:16101 [D loss: 0.572670, acc.: 70.31%] [G loss: 1.164158]\n",
      "epoch:17 step:16102 [D loss: 0.733894, acc.: 47.66%] [G loss: 0.777315]\n",
      "epoch:17 step:16103 [D loss: 0.662516, acc.: 64.06%] [G loss: 1.164225]\n",
      "epoch:17 step:16104 [D loss: 0.565028, acc.: 72.66%] [G loss: 0.830531]\n",
      "epoch:17 step:16105 [D loss: 0.693185, acc.: 53.12%] [G loss: 0.708011]\n",
      "epoch:17 step:16106 [D loss: 0.598618, acc.: 67.19%] [G loss: 1.060489]\n",
      "epoch:17 step:16107 [D loss: 0.518301, acc.: 78.91%] [G loss: 1.032547]\n",
      "epoch:17 step:16108 [D loss: 0.716195, acc.: 46.88%] [G loss: 0.896163]\n",
      "epoch:17 step:16109 [D loss: 0.599245, acc.: 71.09%] [G loss: 1.075497]\n",
      "epoch:17 step:16110 [D loss: 0.601059, acc.: 62.50%] [G loss: 0.890661]\n",
      "epoch:17 step:16111 [D loss: 0.680255, acc.: 59.38%] [G loss: 1.344103]\n",
      "epoch:17 step:16112 [D loss: 0.566994, acc.: 66.41%] [G loss: 1.071380]\n",
      "epoch:17 step:16113 [D loss: 0.585593, acc.: 69.53%] [G loss: 0.787444]\n",
      "epoch:17 step:16114 [D loss: 0.530943, acc.: 74.22%] [G loss: 0.942621]\n",
      "epoch:17 step:16115 [D loss: 0.690809, acc.: 50.78%] [G loss: 1.239761]\n",
      "epoch:17 step:16116 [D loss: 0.711846, acc.: 53.12%] [G loss: 1.277759]\n",
      "epoch:17 step:16117 [D loss: 0.495602, acc.: 73.44%] [G loss: 0.969231]\n",
      "epoch:17 step:16118 [D loss: 0.472169, acc.: 69.53%] [G loss: 1.043769]\n",
      "epoch:17 step:16119 [D loss: 0.711850, acc.: 53.91%] [G loss: 1.326274]\n",
      "epoch:17 step:16120 [D loss: 0.675068, acc.: 57.81%] [G loss: 0.996369]\n",
      "epoch:17 step:16121 [D loss: 0.589120, acc.: 70.31%] [G loss: 0.927810]\n",
      "epoch:17 step:16122 [D loss: 0.711494, acc.: 50.00%] [G loss: 0.916913]\n",
      "epoch:17 step:16123 [D loss: 0.594389, acc.: 60.94%] [G loss: 0.769342]\n",
      "epoch:17 step:16124 [D loss: 0.701275, acc.: 55.47%] [G loss: 0.872555]\n",
      "epoch:17 step:16125 [D loss: 0.573368, acc.: 67.19%] [G loss: 0.929193]\n",
      "epoch:17 step:16126 [D loss: 0.620163, acc.: 65.62%] [G loss: 0.900399]\n",
      "epoch:17 step:16127 [D loss: 0.699870, acc.: 57.81%] [G loss: 0.938363]\n",
      "epoch:17 step:16128 [D loss: 0.621929, acc.: 66.41%] [G loss: 0.949994]\n",
      "epoch:17 step:16129 [D loss: 0.776323, acc.: 50.00%] [G loss: 0.909345]\n",
      "epoch:17 step:16130 [D loss: 0.675189, acc.: 59.38%] [G loss: 0.905936]\n",
      "epoch:17 step:16131 [D loss: 0.603035, acc.: 68.75%] [G loss: 0.792190]\n",
      "epoch:17 step:16132 [D loss: 0.777436, acc.: 43.75%] [G loss: 1.111821]\n",
      "epoch:17 step:16133 [D loss: 0.666069, acc.: 60.16%] [G loss: 1.104573]\n",
      "epoch:17 step:16134 [D loss: 0.548970, acc.: 71.88%] [G loss: 0.999575]\n",
      "epoch:17 step:16135 [D loss: 0.671797, acc.: 61.72%] [G loss: 0.922439]\n",
      "epoch:17 step:16136 [D loss: 0.543491, acc.: 78.12%] [G loss: 1.168939]\n",
      "epoch:17 step:16137 [D loss: 0.543884, acc.: 72.66%] [G loss: 0.890589]\n",
      "epoch:17 step:16138 [D loss: 0.656391, acc.: 62.50%] [G loss: 1.533538]\n",
      "epoch:17 step:16139 [D loss: 0.576310, acc.: 69.53%] [G loss: 1.109379]\n",
      "epoch:17 step:16140 [D loss: 0.647644, acc.: 63.28%] [G loss: 1.139952]\n",
      "epoch:17 step:16141 [D loss: 0.687730, acc.: 59.38%] [G loss: 1.066336]\n",
      "epoch:17 step:16142 [D loss: 0.519515, acc.: 76.56%] [G loss: 0.788799]\n",
      "epoch:17 step:16143 [D loss: 0.698961, acc.: 57.81%] [G loss: 0.702629]\n",
      "epoch:17 step:16144 [D loss: 0.650721, acc.: 57.03%] [G loss: 0.894457]\n",
      "epoch:17 step:16145 [D loss: 0.625909, acc.: 68.75%] [G loss: 0.847859]\n",
      "epoch:17 step:16146 [D loss: 0.531654, acc.: 78.91%] [G loss: 0.914556]\n",
      "epoch:17 step:16147 [D loss: 0.782285, acc.: 46.09%] [G loss: 0.891912]\n",
      "epoch:17 step:16148 [D loss: 0.636299, acc.: 64.84%] [G loss: 0.800369]\n",
      "epoch:17 step:16149 [D loss: 0.626591, acc.: 65.62%] [G loss: 1.121451]\n",
      "epoch:17 step:16150 [D loss: 0.638103, acc.: 68.75%] [G loss: 0.968254]\n",
      "epoch:17 step:16151 [D loss: 0.603011, acc.: 64.06%] [G loss: 1.010849]\n",
      "epoch:17 step:16152 [D loss: 0.678444, acc.: 53.91%] [G loss: 0.888983]\n",
      "epoch:17 step:16153 [D loss: 0.584198, acc.: 70.31%] [G loss: 0.919528]\n",
      "epoch:17 step:16154 [D loss: 0.607286, acc.: 71.09%] [G loss: 1.086169]\n",
      "epoch:17 step:16155 [D loss: 0.640284, acc.: 62.50%] [G loss: 1.043776]\n",
      "epoch:17 step:16156 [D loss: 0.522927, acc.: 78.91%] [G loss: 0.859096]\n",
      "epoch:17 step:16157 [D loss: 0.508319, acc.: 77.34%] [G loss: 1.127892]\n",
      "epoch:17 step:16158 [D loss: 0.591291, acc.: 71.09%] [G loss: 0.867066]\n",
      "epoch:17 step:16159 [D loss: 0.524503, acc.: 81.25%] [G loss: 1.018302]\n",
      "epoch:17 step:16160 [D loss: 0.607079, acc.: 68.75%] [G loss: 0.939850]\n",
      "epoch:17 step:16161 [D loss: 0.636486, acc.: 62.50%] [G loss: 1.080710]\n",
      "epoch:17 step:16162 [D loss: 0.661493, acc.: 61.72%] [G loss: 1.033430]\n",
      "epoch:17 step:16163 [D loss: 0.613991, acc.: 63.28%] [G loss: 1.238539]\n",
      "epoch:17 step:16164 [D loss: 0.659010, acc.: 58.59%] [G loss: 0.994922]\n",
      "epoch:17 step:16165 [D loss: 0.688308, acc.: 59.38%] [G loss: 1.164503]\n",
      "epoch:17 step:16166 [D loss: 0.610782, acc.: 63.28%] [G loss: 1.726138]\n",
      "epoch:17 step:16167 [D loss: 0.762570, acc.: 45.31%] [G loss: 1.019359]\n",
      "epoch:17 step:16168 [D loss: 0.644629, acc.: 64.06%] [G loss: 1.001710]\n",
      "epoch:17 step:16169 [D loss: 0.734698, acc.: 42.97%] [G loss: 1.181813]\n",
      "epoch:17 step:16170 [D loss: 0.650384, acc.: 58.59%] [G loss: 1.218764]\n",
      "epoch:17 step:16171 [D loss: 0.751522, acc.: 42.97%] [G loss: 1.115042]\n",
      "epoch:17 step:16172 [D loss: 0.579233, acc.: 70.31%] [G loss: 1.150013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16173 [D loss: 0.563364, acc.: 72.66%] [G loss: 1.216219]\n",
      "epoch:17 step:16174 [D loss: 0.753442, acc.: 47.66%] [G loss: 1.094044]\n",
      "epoch:17 step:16175 [D loss: 0.828597, acc.: 36.72%] [G loss: 0.945632]\n",
      "epoch:17 step:16176 [D loss: 0.677990, acc.: 55.47%] [G loss: 1.005756]\n",
      "epoch:17 step:16177 [D loss: 0.706742, acc.: 52.34%] [G loss: 1.056696]\n",
      "epoch:17 step:16178 [D loss: 0.680592, acc.: 58.59%] [G loss: 0.947018]\n",
      "epoch:17 step:16179 [D loss: 0.650528, acc.: 60.94%] [G loss: 0.996685]\n",
      "epoch:17 step:16180 [D loss: 0.659544, acc.: 63.28%] [G loss: 1.005508]\n",
      "epoch:17 step:16181 [D loss: 0.583011, acc.: 64.06%] [G loss: 1.322942]\n",
      "epoch:17 step:16182 [D loss: 0.657887, acc.: 57.81%] [G loss: 0.977455]\n",
      "epoch:17 step:16183 [D loss: 0.652853, acc.: 61.72%] [G loss: 0.806872]\n",
      "epoch:17 step:16184 [D loss: 0.627228, acc.: 59.38%] [G loss: 0.856313]\n",
      "epoch:17 step:16185 [D loss: 0.730022, acc.: 54.69%] [G loss: 1.105191]\n",
      "epoch:17 step:16186 [D loss: 0.673837, acc.: 53.91%] [G loss: 0.896159]\n",
      "epoch:17 step:16187 [D loss: 0.679619, acc.: 57.03%] [G loss: 0.916895]\n",
      "epoch:17 step:16188 [D loss: 0.630390, acc.: 57.03%] [G loss: 1.116527]\n",
      "epoch:17 step:16189 [D loss: 0.612267, acc.: 67.97%] [G loss: 0.872305]\n",
      "epoch:17 step:16190 [D loss: 0.723248, acc.: 50.78%] [G loss: 0.792963]\n",
      "epoch:17 step:16191 [D loss: 0.755892, acc.: 49.22%] [G loss: 0.981497]\n",
      "epoch:17 step:16192 [D loss: 0.561706, acc.: 73.44%] [G loss: 0.982583]\n",
      "epoch:17 step:16193 [D loss: 0.650943, acc.: 57.03%] [G loss: 0.942045]\n",
      "epoch:17 step:16194 [D loss: 0.641251, acc.: 63.28%] [G loss: 1.109660]\n",
      "epoch:17 step:16195 [D loss: 0.472486, acc.: 85.94%] [G loss: 1.063486]\n",
      "epoch:17 step:16196 [D loss: 0.630094, acc.: 60.94%] [G loss: 1.175889]\n",
      "epoch:17 step:16197 [D loss: 0.603115, acc.: 63.28%] [G loss: 1.171786]\n",
      "epoch:17 step:16198 [D loss: 0.571505, acc.: 71.88%] [G loss: 1.026008]\n",
      "epoch:17 step:16199 [D loss: 0.655861, acc.: 65.62%] [G loss: 0.898351]\n",
      "epoch:17 step:16200 [D loss: 0.545267, acc.: 76.56%] [G loss: 1.248415]\n",
      "epoch:17 step:16201 [D loss: 0.590450, acc.: 66.41%] [G loss: 1.107359]\n",
      "epoch:17 step:16202 [D loss: 0.557782, acc.: 69.53%] [G loss: 1.291939]\n",
      "epoch:17 step:16203 [D loss: 0.600478, acc.: 65.62%] [G loss: 1.095514]\n",
      "epoch:17 step:16204 [D loss: 0.684182, acc.: 57.03%] [G loss: 1.146668]\n",
      "epoch:17 step:16205 [D loss: 0.692271, acc.: 51.56%] [G loss: 1.131650]\n",
      "epoch:17 step:16206 [D loss: 0.772774, acc.: 45.31%] [G loss: 1.019963]\n",
      "epoch:17 step:16207 [D loss: 0.772213, acc.: 47.66%] [G loss: 1.060417]\n",
      "epoch:17 step:16208 [D loss: 0.591412, acc.: 66.41%] [G loss: 0.870964]\n",
      "epoch:17 step:16209 [D loss: 0.644687, acc.: 60.94%] [G loss: 1.006589]\n",
      "epoch:17 step:16210 [D loss: 0.596636, acc.: 66.41%] [G loss: 0.908570]\n",
      "epoch:17 step:16211 [D loss: 0.624844, acc.: 62.50%] [G loss: 1.137527]\n",
      "epoch:17 step:16212 [D loss: 0.562885, acc.: 71.09%] [G loss: 1.125876]\n",
      "epoch:17 step:16213 [D loss: 0.770225, acc.: 52.34%] [G loss: 0.969547]\n",
      "epoch:17 step:16214 [D loss: 0.677146, acc.: 57.03%] [G loss: 0.919795]\n",
      "epoch:17 step:16215 [D loss: 0.581325, acc.: 69.53%] [G loss: 0.906353]\n",
      "epoch:17 step:16216 [D loss: 0.685325, acc.: 55.47%] [G loss: 1.018641]\n",
      "epoch:17 step:16217 [D loss: 0.608679, acc.: 64.84%] [G loss: 0.861713]\n",
      "epoch:17 step:16218 [D loss: 0.709937, acc.: 52.34%] [G loss: 1.167733]\n",
      "epoch:17 step:16219 [D loss: 0.480276, acc.: 82.03%] [G loss: 1.287796]\n",
      "epoch:17 step:16220 [D loss: 0.550432, acc.: 72.66%] [G loss: 1.034621]\n",
      "epoch:17 step:16221 [D loss: 0.662997, acc.: 54.69%] [G loss: 1.193833]\n",
      "epoch:17 step:16222 [D loss: 0.581125, acc.: 67.97%] [G loss: 1.033168]\n",
      "epoch:17 step:16223 [D loss: 0.570158, acc.: 68.75%] [G loss: 0.908973]\n",
      "epoch:17 step:16224 [D loss: 0.571376, acc.: 72.66%] [G loss: 1.227706]\n",
      "epoch:17 step:16225 [D loss: 0.596841, acc.: 67.97%] [G loss: 1.052644]\n",
      "epoch:17 step:16226 [D loss: 0.473550, acc.: 85.94%] [G loss: 0.773330]\n",
      "epoch:17 step:16227 [D loss: 0.708019, acc.: 55.47%] [G loss: 1.127635]\n",
      "epoch:17 step:16228 [D loss: 0.623091, acc.: 65.62%] [G loss: 0.985948]\n",
      "epoch:17 step:16229 [D loss: 0.792089, acc.: 49.22%] [G loss: 0.826657]\n",
      "epoch:17 step:16230 [D loss: 0.672113, acc.: 62.50%] [G loss: 0.773211]\n",
      "epoch:17 step:16231 [D loss: 0.638646, acc.: 59.38%] [G loss: 0.943237]\n",
      "epoch:17 step:16232 [D loss: 0.694658, acc.: 53.12%] [G loss: 1.137027]\n",
      "epoch:17 step:16233 [D loss: 0.630890, acc.: 64.06%] [G loss: 0.962250]\n",
      "epoch:17 step:16234 [D loss: 0.710387, acc.: 48.44%] [G loss: 1.028894]\n",
      "epoch:17 step:16235 [D loss: 0.569235, acc.: 72.66%] [G loss: 0.988935]\n",
      "epoch:17 step:16236 [D loss: 0.609949, acc.: 68.75%] [G loss: 1.025101]\n",
      "epoch:17 step:16237 [D loss: 0.686149, acc.: 46.09%] [G loss: 1.097731]\n",
      "epoch:17 step:16238 [D loss: 0.704654, acc.: 50.78%] [G loss: 1.040045]\n",
      "epoch:17 step:16239 [D loss: 0.664328, acc.: 59.38%] [G loss: 1.095451]\n",
      "epoch:17 step:16240 [D loss: 0.627042, acc.: 64.06%] [G loss: 0.957437]\n",
      "epoch:17 step:16241 [D loss: 0.587752, acc.: 71.88%] [G loss: 0.914765]\n",
      "epoch:17 step:16242 [D loss: 0.605849, acc.: 68.75%] [G loss: 1.165511]\n",
      "epoch:17 step:16243 [D loss: 0.715248, acc.: 55.47%] [G loss: 0.956995]\n",
      "epoch:17 step:16244 [D loss: 0.682288, acc.: 56.25%] [G loss: 0.844434]\n",
      "epoch:17 step:16245 [D loss: 0.770989, acc.: 46.88%] [G loss: 0.974570]\n",
      "epoch:17 step:16246 [D loss: 0.689056, acc.: 58.59%] [G loss: 1.281262]\n",
      "epoch:17 step:16247 [D loss: 0.717176, acc.: 44.53%] [G loss: 1.008224]\n",
      "epoch:17 step:16248 [D loss: 0.620724, acc.: 64.84%] [G loss: 1.057177]\n",
      "epoch:17 step:16249 [D loss: 0.540049, acc.: 81.25%] [G loss: 0.833771]\n",
      "epoch:17 step:16250 [D loss: 0.625326, acc.: 66.41%] [G loss: 1.093141]\n",
      "epoch:17 step:16251 [D loss: 0.650708, acc.: 61.72%] [G loss: 1.013852]\n",
      "epoch:17 step:16252 [D loss: 0.603591, acc.: 62.50%] [G loss: 0.957919]\n",
      "epoch:17 step:16253 [D loss: 0.600225, acc.: 68.75%] [G loss: 1.115361]\n",
      "epoch:17 step:16254 [D loss: 0.623547, acc.: 64.84%] [G loss: 1.034825]\n",
      "epoch:17 step:16255 [D loss: 0.609669, acc.: 64.84%] [G loss: 1.144294]\n",
      "epoch:17 step:16256 [D loss: 0.537505, acc.: 78.12%] [G loss: 0.917620]\n",
      "epoch:17 step:16257 [D loss: 0.615438, acc.: 68.75%] [G loss: 1.239168]\n",
      "epoch:17 step:16258 [D loss: 0.597532, acc.: 72.66%] [G loss: 1.031712]\n",
      "epoch:17 step:16259 [D loss: 0.595644, acc.: 64.06%] [G loss: 0.970031]\n",
      "epoch:17 step:16260 [D loss: 0.580692, acc.: 80.47%] [G loss: 0.867497]\n",
      "epoch:17 step:16261 [D loss: 0.731956, acc.: 57.03%] [G loss: 0.931680]\n",
      "epoch:17 step:16262 [D loss: 0.672074, acc.: 53.12%] [G loss: 0.992676]\n",
      "epoch:17 step:16263 [D loss: 0.655280, acc.: 56.25%] [G loss: 1.031780]\n",
      "epoch:17 step:16264 [D loss: 0.628661, acc.: 64.84%] [G loss: 1.150358]\n",
      "epoch:17 step:16265 [D loss: 0.639555, acc.: 62.50%] [G loss: 1.101610]\n",
      "epoch:17 step:16266 [D loss: 0.666285, acc.: 53.91%] [G loss: 0.911578]\n",
      "epoch:17 step:16267 [D loss: 0.601820, acc.: 67.97%] [G loss: 1.119639]\n",
      "epoch:17 step:16268 [D loss: 0.563266, acc.: 73.44%] [G loss: 1.296168]\n",
      "epoch:17 step:16269 [D loss: 0.808217, acc.: 39.06%] [G loss: 0.848921]\n",
      "epoch:17 step:16270 [D loss: 0.623968, acc.: 64.84%] [G loss: 0.871637]\n",
      "epoch:17 step:16271 [D loss: 0.698584, acc.: 57.81%] [G loss: 0.812022]\n",
      "epoch:17 step:16272 [D loss: 0.577036, acc.: 66.41%] [G loss: 0.833558]\n",
      "epoch:17 step:16273 [D loss: 0.585899, acc.: 65.62%] [G loss: 0.837732]\n",
      "epoch:17 step:16274 [D loss: 0.594403, acc.: 67.97%] [G loss: 1.080507]\n",
      "epoch:17 step:16275 [D loss: 0.595482, acc.: 71.09%] [G loss: 1.110790]\n",
      "epoch:17 step:16276 [D loss: 0.562447, acc.: 71.88%] [G loss: 1.102402]\n",
      "epoch:17 step:16277 [D loss: 0.493498, acc.: 80.47%] [G loss: 0.924340]\n",
      "epoch:17 step:16278 [D loss: 0.666274, acc.: 60.94%] [G loss: 1.171555]\n",
      "epoch:17 step:16279 [D loss: 0.567598, acc.: 72.66%] [G loss: 1.164508]\n",
      "epoch:17 step:16280 [D loss: 0.572607, acc.: 67.97%] [G loss: 0.813222]\n",
      "epoch:17 step:16281 [D loss: 0.531229, acc.: 73.44%] [G loss: 0.993766]\n",
      "epoch:17 step:16282 [D loss: 0.453118, acc.: 79.69%] [G loss: 1.052803]\n",
      "epoch:17 step:16283 [D loss: 0.881216, acc.: 39.84%] [G loss: 0.836292]\n",
      "epoch:17 step:16284 [D loss: 0.584456, acc.: 69.53%] [G loss: 0.981869]\n",
      "epoch:17 step:16285 [D loss: 0.805364, acc.: 41.41%] [G loss: 0.931414]\n",
      "epoch:17 step:16286 [D loss: 0.734942, acc.: 54.69%] [G loss: 0.880991]\n",
      "epoch:17 step:16287 [D loss: 0.633972, acc.: 66.41%] [G loss: 0.808791]\n",
      "epoch:17 step:16288 [D loss: 0.679976, acc.: 60.94%] [G loss: 0.948540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16289 [D loss: 0.673067, acc.: 54.69%] [G loss: 1.023878]\n",
      "epoch:17 step:16290 [D loss: 0.603535, acc.: 65.62%] [G loss: 1.112143]\n",
      "epoch:17 step:16291 [D loss: 0.610364, acc.: 67.19%] [G loss: 1.115407]\n",
      "epoch:17 step:16292 [D loss: 0.646289, acc.: 63.28%] [G loss: 1.132849]\n",
      "epoch:17 step:16293 [D loss: 0.606966, acc.: 70.31%] [G loss: 0.853194]\n",
      "epoch:17 step:16294 [D loss: 0.566266, acc.: 76.56%] [G loss: 1.222564]\n",
      "epoch:17 step:16295 [D loss: 0.620202, acc.: 67.19%] [G loss: 1.280783]\n",
      "epoch:17 step:16296 [D loss: 0.536851, acc.: 76.56%] [G loss: 1.223232]\n",
      "epoch:17 step:16297 [D loss: 0.709080, acc.: 53.91%] [G loss: 0.988065]\n",
      "epoch:17 step:16298 [D loss: 0.561895, acc.: 75.00%] [G loss: 1.083591]\n",
      "epoch:17 step:16299 [D loss: 0.681094, acc.: 60.16%] [G loss: 1.014754]\n",
      "epoch:17 step:16300 [D loss: 0.646616, acc.: 60.16%] [G loss: 1.395350]\n",
      "epoch:17 step:16301 [D loss: 0.570578, acc.: 75.78%] [G loss: 1.065297]\n",
      "epoch:17 step:16302 [D loss: 0.557234, acc.: 75.78%] [G loss: 1.129894]\n",
      "epoch:17 step:16303 [D loss: 0.690371, acc.: 51.56%] [G loss: 1.029856]\n",
      "epoch:17 step:16304 [D loss: 0.585131, acc.: 67.19%] [G loss: 0.764189]\n",
      "epoch:17 step:16305 [D loss: 0.547844, acc.: 73.44%] [G loss: 1.046727]\n",
      "epoch:17 step:16306 [D loss: 0.607613, acc.: 68.75%] [G loss: 0.863265]\n",
      "epoch:17 step:16307 [D loss: 0.585249, acc.: 66.41%] [G loss: 1.359251]\n",
      "epoch:17 step:16308 [D loss: 0.588381, acc.: 67.19%] [G loss: 1.242831]\n",
      "epoch:17 step:16309 [D loss: 0.739501, acc.: 46.09%] [G loss: 1.015838]\n",
      "epoch:17 step:16310 [D loss: 0.583732, acc.: 70.31%] [G loss: 0.962379]\n",
      "epoch:17 step:16311 [D loss: 0.582035, acc.: 69.53%] [G loss: 0.955851]\n",
      "epoch:17 step:16312 [D loss: 0.708666, acc.: 46.88%] [G loss: 0.857724]\n",
      "epoch:17 step:16313 [D loss: 0.646927, acc.: 57.81%] [G loss: 1.302021]\n",
      "epoch:17 step:16314 [D loss: 0.682140, acc.: 60.16%] [G loss: 0.955865]\n",
      "epoch:17 step:16315 [D loss: 0.643637, acc.: 61.72%] [G loss: 0.990704]\n",
      "epoch:17 step:16316 [D loss: 0.595211, acc.: 63.28%] [G loss: 0.839803]\n",
      "epoch:17 step:16317 [D loss: 0.669246, acc.: 58.59%] [G loss: 1.098393]\n",
      "epoch:17 step:16318 [D loss: 0.725139, acc.: 52.34%] [G loss: 0.879669]\n",
      "epoch:17 step:16319 [D loss: 0.515862, acc.: 78.12%] [G loss: 0.963096]\n",
      "epoch:17 step:16320 [D loss: 0.605507, acc.: 66.41%] [G loss: 0.877405]\n",
      "epoch:17 step:16321 [D loss: 0.678759, acc.: 58.59%] [G loss: 0.966223]\n",
      "epoch:17 step:16322 [D loss: 0.721521, acc.: 53.12%] [G loss: 0.754732]\n",
      "epoch:17 step:16323 [D loss: 0.693624, acc.: 53.91%] [G loss: 0.961597]\n",
      "epoch:17 step:16324 [D loss: 0.662280, acc.: 60.94%] [G loss: 0.833922]\n",
      "epoch:17 step:16325 [D loss: 0.416427, acc.: 92.19%] [G loss: 0.810607]\n",
      "epoch:17 step:16326 [D loss: 0.702427, acc.: 49.22%] [G loss: 1.107173]\n",
      "epoch:17 step:16327 [D loss: 0.623165, acc.: 67.97%] [G loss: 1.078748]\n",
      "epoch:17 step:16328 [D loss: 0.680106, acc.: 57.03%] [G loss: 0.754648]\n",
      "epoch:17 step:16329 [D loss: 0.674902, acc.: 55.47%] [G loss: 0.778353]\n",
      "epoch:17 step:16330 [D loss: 0.712429, acc.: 49.22%] [G loss: 0.899680]\n",
      "epoch:17 step:16331 [D loss: 0.687122, acc.: 52.34%] [G loss: 0.898820]\n",
      "epoch:17 step:16332 [D loss: 0.632090, acc.: 57.81%] [G loss: 1.210815]\n",
      "epoch:17 step:16333 [D loss: 0.673782, acc.: 57.81%] [G loss: 0.971612]\n",
      "epoch:17 step:16334 [D loss: 0.671128, acc.: 52.34%] [G loss: 0.899057]\n",
      "epoch:17 step:16335 [D loss: 0.599630, acc.: 72.66%] [G loss: 0.831402]\n",
      "epoch:17 step:16336 [D loss: 0.677052, acc.: 55.47%] [G loss: 0.945865]\n",
      "epoch:17 step:16337 [D loss: 0.768559, acc.: 45.31%] [G loss: 1.102837]\n",
      "epoch:17 step:16338 [D loss: 0.572407, acc.: 74.22%] [G loss: 0.910009]\n",
      "epoch:17 step:16339 [D loss: 0.605959, acc.: 67.19%] [G loss: 0.959785]\n",
      "epoch:17 step:16340 [D loss: 0.764565, acc.: 41.41%] [G loss: 1.369850]\n",
      "epoch:17 step:16341 [D loss: 0.668007, acc.: 61.72%] [G loss: 0.803601]\n",
      "epoch:17 step:16342 [D loss: 0.597883, acc.: 63.28%] [G loss: 0.890337]\n",
      "epoch:17 step:16343 [D loss: 0.583573, acc.: 68.75%] [G loss: 1.080076]\n",
      "epoch:17 step:16344 [D loss: 0.560822, acc.: 72.66%] [G loss: 0.966397]\n",
      "epoch:17 step:16345 [D loss: 0.682699, acc.: 57.81%] [G loss: 0.991764]\n",
      "epoch:17 step:16346 [D loss: 0.748068, acc.: 50.00%] [G loss: 0.975471]\n",
      "epoch:17 step:16347 [D loss: 0.661684, acc.: 56.25%] [G loss: 0.905271]\n",
      "epoch:17 step:16348 [D loss: 0.595927, acc.: 67.97%] [G loss: 0.807597]\n",
      "epoch:17 step:16349 [D loss: 0.630982, acc.: 62.50%] [G loss: 0.847322]\n",
      "epoch:17 step:16350 [D loss: 0.493427, acc.: 85.94%] [G loss: 1.134327]\n",
      "epoch:17 step:16351 [D loss: 0.721308, acc.: 42.97%] [G loss: 0.959416]\n",
      "epoch:17 step:16352 [D loss: 0.715440, acc.: 54.69%] [G loss: 1.005018]\n",
      "epoch:17 step:16353 [D loss: 0.707970, acc.: 47.66%] [G loss: 0.971371]\n",
      "epoch:17 step:16354 [D loss: 0.582506, acc.: 70.31%] [G loss: 1.114763]\n",
      "epoch:17 step:16355 [D loss: 0.629799, acc.: 60.16%] [G loss: 0.837306]\n",
      "epoch:17 step:16356 [D loss: 0.619802, acc.: 71.09%] [G loss: 1.073181]\n",
      "epoch:17 step:16357 [D loss: 0.703876, acc.: 53.12%] [G loss: 0.967314]\n",
      "epoch:17 step:16358 [D loss: 0.688800, acc.: 53.91%] [G loss: 1.034740]\n",
      "epoch:17 step:16359 [D loss: 0.628708, acc.: 59.38%] [G loss: 1.098334]\n",
      "epoch:17 step:16360 [D loss: 0.634831, acc.: 67.19%] [G loss: 1.240079]\n",
      "epoch:17 step:16361 [D loss: 0.736714, acc.: 50.78%] [G loss: 0.968564]\n",
      "epoch:17 step:16362 [D loss: 0.609443, acc.: 67.19%] [G loss: 1.004970]\n",
      "epoch:17 step:16363 [D loss: 0.549425, acc.: 71.88%] [G loss: 1.211662]\n",
      "epoch:17 step:16364 [D loss: 0.570361, acc.: 75.78%] [G loss: 1.040025]\n",
      "epoch:17 step:16365 [D loss: 0.686045, acc.: 53.91%] [G loss: 0.940354]\n",
      "epoch:17 step:16366 [D loss: 0.755585, acc.: 52.34%] [G loss: 0.851645]\n",
      "epoch:17 step:16367 [D loss: 0.593705, acc.: 70.31%] [G loss: 0.883101]\n",
      "epoch:17 step:16368 [D loss: 0.602880, acc.: 71.09%] [G loss: 0.906278]\n",
      "epoch:17 step:16369 [D loss: 0.713965, acc.: 50.78%] [G loss: 0.843068]\n",
      "epoch:17 step:16370 [D loss: 0.674062, acc.: 53.91%] [G loss: 0.993253]\n",
      "epoch:17 step:16371 [D loss: 0.608921, acc.: 65.62%] [G loss: 1.083153]\n",
      "epoch:17 step:16372 [D loss: 0.633657, acc.: 64.06%] [G loss: 0.937569]\n",
      "epoch:17 step:16373 [D loss: 0.521174, acc.: 75.00%] [G loss: 1.187194]\n",
      "epoch:17 step:16374 [D loss: 0.709779, acc.: 50.78%] [G loss: 0.855179]\n",
      "epoch:17 step:16375 [D loss: 0.750394, acc.: 44.53%] [G loss: 0.858272]\n",
      "epoch:17 step:16376 [D loss: 0.640296, acc.: 63.28%] [G loss: 0.792163]\n",
      "epoch:17 step:16377 [D loss: 0.612750, acc.: 62.50%] [G loss: 0.963286]\n",
      "epoch:17 step:16378 [D loss: 0.612825, acc.: 72.66%] [G loss: 0.715033]\n",
      "epoch:17 step:16379 [D loss: 0.659970, acc.: 60.94%] [G loss: 0.874729]\n",
      "epoch:17 step:16380 [D loss: 0.628235, acc.: 67.97%] [G loss: 0.832232]\n",
      "epoch:17 step:16381 [D loss: 0.630365, acc.: 69.53%] [G loss: 0.894993]\n",
      "epoch:17 step:16382 [D loss: 0.627500, acc.: 62.50%] [G loss: 0.739646]\n",
      "epoch:17 step:16383 [D loss: 0.694563, acc.: 50.78%] [G loss: 0.950212]\n",
      "epoch:17 step:16384 [D loss: 0.570516, acc.: 71.88%] [G loss: 0.915905]\n",
      "epoch:17 step:16385 [D loss: 0.697538, acc.: 60.16%] [G loss: 0.862301]\n",
      "epoch:17 step:16386 [D loss: 0.604491, acc.: 63.28%] [G loss: 1.034289]\n",
      "epoch:17 step:16387 [D loss: 0.662651, acc.: 57.81%] [G loss: 1.067747]\n",
      "epoch:17 step:16388 [D loss: 0.614793, acc.: 67.97%] [G loss: 1.191598]\n",
      "epoch:17 step:16389 [D loss: 0.646175, acc.: 64.06%] [G loss: 0.927062]\n",
      "epoch:17 step:16390 [D loss: 0.825117, acc.: 36.72%] [G loss: 0.946749]\n",
      "epoch:17 step:16391 [D loss: 0.765610, acc.: 46.88%] [G loss: 0.729331]\n",
      "epoch:17 step:16392 [D loss: 0.609107, acc.: 66.41%] [G loss: 0.902344]\n",
      "epoch:17 step:16393 [D loss: 0.732825, acc.: 48.44%] [G loss: 0.813968]\n",
      "epoch:17 step:16394 [D loss: 0.626750, acc.: 66.41%] [G loss: 0.882036]\n",
      "epoch:17 step:16395 [D loss: 0.680012, acc.: 53.91%] [G loss: 0.936316]\n",
      "epoch:17 step:16396 [D loss: 0.665127, acc.: 56.25%] [G loss: 0.945829]\n",
      "epoch:17 step:16397 [D loss: 0.624743, acc.: 67.19%] [G loss: 1.042366]\n",
      "epoch:17 step:16398 [D loss: 0.583441, acc.: 71.88%] [G loss: 1.002548]\n",
      "epoch:17 step:16399 [D loss: 0.707039, acc.: 49.22%] [G loss: 0.903693]\n",
      "epoch:17 step:16400 [D loss: 0.584254, acc.: 77.34%] [G loss: 0.828029]\n",
      "epoch:17 step:16401 [D loss: 0.671243, acc.: 59.38%] [G loss: 0.888494]\n",
      "epoch:17 step:16402 [D loss: 0.624094, acc.: 61.72%] [G loss: 1.270859]\n",
      "epoch:17 step:16403 [D loss: 0.587378, acc.: 72.66%] [G loss: 0.807612]\n",
      "epoch:17 step:16404 [D loss: 0.643500, acc.: 63.28%] [G loss: 1.041854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16405 [D loss: 0.625797, acc.: 65.62%] [G loss: 1.288738]\n",
      "epoch:17 step:16406 [D loss: 0.636539, acc.: 63.28%] [G loss: 1.005083]\n",
      "epoch:17 step:16407 [D loss: 0.658812, acc.: 63.28%] [G loss: 0.993628]\n",
      "epoch:17 step:16408 [D loss: 0.517459, acc.: 77.34%] [G loss: 0.925159]\n",
      "epoch:17 step:16409 [D loss: 0.740706, acc.: 46.88%] [G loss: 0.892826]\n",
      "epoch:17 step:16410 [D loss: 0.667876, acc.: 58.59%] [G loss: 0.839040]\n",
      "epoch:17 step:16411 [D loss: 0.532499, acc.: 80.47%] [G loss: 1.079337]\n",
      "epoch:17 step:16412 [D loss: 0.518377, acc.: 78.91%] [G loss: 0.866740]\n",
      "epoch:17 step:16413 [D loss: 0.615117, acc.: 60.94%] [G loss: 1.298064]\n",
      "epoch:17 step:16414 [D loss: 0.611194, acc.: 64.84%] [G loss: 1.110517]\n",
      "epoch:17 step:16415 [D loss: 0.638846, acc.: 60.16%] [G loss: 0.943229]\n",
      "epoch:17 step:16416 [D loss: 0.636610, acc.: 58.59%] [G loss: 0.921726]\n",
      "epoch:17 step:16417 [D loss: 0.626718, acc.: 64.06%] [G loss: 1.203986]\n",
      "epoch:17 step:16418 [D loss: 0.617868, acc.: 70.31%] [G loss: 1.090265]\n",
      "epoch:17 step:16419 [D loss: 0.629966, acc.: 64.06%] [G loss: 1.044988]\n",
      "epoch:17 step:16420 [D loss: 0.698221, acc.: 54.69%] [G loss: 1.217367]\n",
      "epoch:17 step:16421 [D loss: 0.595595, acc.: 67.97%] [G loss: 1.169180]\n",
      "epoch:17 step:16422 [D loss: 0.755814, acc.: 53.12%] [G loss: 1.064064]\n",
      "epoch:17 step:16423 [D loss: 0.616122, acc.: 64.84%] [G loss: 1.135441]\n",
      "epoch:17 step:16424 [D loss: 0.591442, acc.: 68.75%] [G loss: 0.988409]\n",
      "epoch:17 step:16425 [D loss: 0.597795, acc.: 63.28%] [G loss: 1.032282]\n",
      "epoch:17 step:16426 [D loss: 0.634338, acc.: 61.72%] [G loss: 0.863179]\n",
      "epoch:17 step:16427 [D loss: 0.592244, acc.: 70.31%] [G loss: 1.020008]\n",
      "epoch:17 step:16428 [D loss: 0.595019, acc.: 71.88%] [G loss: 1.036264]\n",
      "epoch:17 step:16429 [D loss: 0.567864, acc.: 75.00%] [G loss: 0.879753]\n",
      "epoch:17 step:16430 [D loss: 0.586122, acc.: 68.75%] [G loss: 1.189136]\n",
      "epoch:17 step:16431 [D loss: 0.582839, acc.: 71.88%] [G loss: 0.892709]\n",
      "epoch:17 step:16432 [D loss: 0.608918, acc.: 66.41%] [G loss: 0.902683]\n",
      "epoch:17 step:16433 [D loss: 0.575618, acc.: 66.41%] [G loss: 0.799713]\n",
      "epoch:17 step:16434 [D loss: 0.622410, acc.: 70.31%] [G loss: 1.227187]\n",
      "epoch:17 step:16435 [D loss: 0.638237, acc.: 57.03%] [G loss: 1.199229]\n",
      "epoch:17 step:16436 [D loss: 0.724087, acc.: 44.53%] [G loss: 1.074767]\n",
      "epoch:17 step:16437 [D loss: 0.610099, acc.: 67.19%] [G loss: 1.022634]\n",
      "epoch:17 step:16438 [D loss: 0.611505, acc.: 69.53%] [G loss: 1.052449]\n",
      "epoch:17 step:16439 [D loss: 0.549328, acc.: 75.00%] [G loss: 1.288283]\n",
      "epoch:17 step:16440 [D loss: 0.678100, acc.: 56.25%] [G loss: 1.050485]\n",
      "epoch:17 step:16441 [D loss: 0.603888, acc.: 65.62%] [G loss: 1.186436]\n",
      "epoch:17 step:16442 [D loss: 0.581375, acc.: 72.66%] [G loss: 0.900390]\n",
      "epoch:17 step:16443 [D loss: 0.626397, acc.: 60.16%] [G loss: 1.068762]\n",
      "epoch:17 step:16444 [D loss: 0.599748, acc.: 69.53%] [G loss: 1.003782]\n",
      "epoch:17 step:16445 [D loss: 0.540957, acc.: 75.78%] [G loss: 0.912269]\n",
      "epoch:17 step:16446 [D loss: 0.601875, acc.: 71.88%] [G loss: 0.966892]\n",
      "epoch:17 step:16447 [D loss: 0.753342, acc.: 42.97%] [G loss: 1.207496]\n",
      "epoch:17 step:16448 [D loss: 0.717071, acc.: 52.34%] [G loss: 1.158429]\n",
      "epoch:17 step:16449 [D loss: 0.493486, acc.: 77.34%] [G loss: 0.881424]\n",
      "epoch:17 step:16450 [D loss: 0.538425, acc.: 79.69%] [G loss: 1.218725]\n",
      "epoch:17 step:16451 [D loss: 0.605994, acc.: 69.53%] [G loss: 1.061022]\n",
      "epoch:17 step:16452 [D loss: 0.655339, acc.: 60.16%] [G loss: 1.378755]\n",
      "epoch:17 step:16453 [D loss: 0.585294, acc.: 70.31%] [G loss: 1.106959]\n",
      "epoch:17 step:16454 [D loss: 0.641553, acc.: 61.72%] [G loss: 1.197423]\n",
      "epoch:17 step:16455 [D loss: 0.602686, acc.: 66.41%] [G loss: 0.971907]\n",
      "epoch:17 step:16456 [D loss: 0.684353, acc.: 62.50%] [G loss: 0.875228]\n",
      "epoch:17 step:16457 [D loss: 0.776934, acc.: 43.75%] [G loss: 0.937484]\n",
      "epoch:17 step:16458 [D loss: 0.548924, acc.: 73.44%] [G loss: 1.190750]\n",
      "epoch:17 step:16459 [D loss: 0.667797, acc.: 61.72%] [G loss: 0.802930]\n",
      "epoch:17 step:16460 [D loss: 0.582582, acc.: 78.12%] [G loss: 0.843885]\n",
      "epoch:17 step:16461 [D loss: 0.511829, acc.: 80.47%] [G loss: 0.907312]\n",
      "epoch:17 step:16462 [D loss: 0.667402, acc.: 60.16%] [G loss: 0.964326]\n",
      "epoch:17 step:16463 [D loss: 0.723582, acc.: 51.56%] [G loss: 1.059224]\n",
      "epoch:17 step:16464 [D loss: 0.624311, acc.: 62.50%] [G loss: 0.735677]\n",
      "epoch:17 step:16465 [D loss: 0.695577, acc.: 64.06%] [G loss: 1.089023]\n",
      "epoch:17 step:16466 [D loss: 0.659730, acc.: 55.47%] [G loss: 0.896404]\n",
      "epoch:17 step:16467 [D loss: 0.585700, acc.: 67.97%] [G loss: 1.260357]\n",
      "epoch:17 step:16468 [D loss: 0.616085, acc.: 62.50%] [G loss: 0.943154]\n",
      "epoch:17 step:16469 [D loss: 0.700604, acc.: 59.38%] [G loss: 1.309108]\n",
      "epoch:17 step:16470 [D loss: 0.645990, acc.: 64.84%] [G loss: 1.003532]\n",
      "epoch:17 step:16471 [D loss: 0.780509, acc.: 40.62%] [G loss: 0.969572]\n",
      "epoch:17 step:16472 [D loss: 0.638265, acc.: 59.38%] [G loss: 1.335160]\n",
      "epoch:17 step:16473 [D loss: 0.618359, acc.: 65.62%] [G loss: 1.144129]\n",
      "epoch:17 step:16474 [D loss: 0.652174, acc.: 60.16%] [G loss: 0.789091]\n",
      "epoch:17 step:16475 [D loss: 0.786311, acc.: 42.19%] [G loss: 1.068131]\n",
      "epoch:17 step:16476 [D loss: 0.646996, acc.: 61.72%] [G loss: 0.815311]\n",
      "epoch:17 step:16477 [D loss: 0.626943, acc.: 64.06%] [G loss: 1.451710]\n",
      "epoch:17 step:16478 [D loss: 0.520856, acc.: 83.59%] [G loss: 1.058344]\n",
      "epoch:17 step:16479 [D loss: 0.762327, acc.: 43.75%] [G loss: 1.118313]\n",
      "epoch:17 step:16480 [D loss: 0.731308, acc.: 47.66%] [G loss: 0.765044]\n",
      "epoch:17 step:16481 [D loss: 0.638887, acc.: 66.41%] [G loss: 1.122570]\n",
      "epoch:17 step:16482 [D loss: 0.594941, acc.: 67.19%] [G loss: 0.839929]\n",
      "epoch:17 step:16483 [D loss: 0.599317, acc.: 69.53%] [G loss: 0.809217]\n",
      "epoch:17 step:16484 [D loss: 0.530396, acc.: 80.47%] [G loss: 1.269266]\n",
      "epoch:17 step:16485 [D loss: 0.918770, acc.: 28.91%] [G loss: 0.789185]\n",
      "epoch:17 step:16486 [D loss: 0.745613, acc.: 54.69%] [G loss: 0.983262]\n",
      "epoch:17 step:16487 [D loss: 0.650245, acc.: 59.38%] [G loss: 0.847884]\n",
      "epoch:17 step:16488 [D loss: 0.809332, acc.: 40.62%] [G loss: 0.857998]\n",
      "epoch:17 step:16489 [D loss: 0.668697, acc.: 57.81%] [G loss: 0.962696]\n",
      "epoch:17 step:16490 [D loss: 0.582472, acc.: 68.75%] [G loss: 0.958826]\n",
      "epoch:17 step:16491 [D loss: 0.601970, acc.: 71.88%] [G loss: 0.840748]\n",
      "epoch:17 step:16492 [D loss: 0.655428, acc.: 60.94%] [G loss: 0.982931]\n",
      "epoch:17 step:16493 [D loss: 0.725223, acc.: 48.44%] [G loss: 0.878091]\n",
      "epoch:17 step:16494 [D loss: 0.619038, acc.: 68.75%] [G loss: 0.885523]\n",
      "epoch:17 step:16495 [D loss: 0.624231, acc.: 66.41%] [G loss: 0.962642]\n",
      "epoch:17 step:16496 [D loss: 0.713868, acc.: 49.22%] [G loss: 0.915822]\n",
      "epoch:17 step:16497 [D loss: 0.666890, acc.: 62.50%] [G loss: 0.926962]\n",
      "epoch:17 step:16498 [D loss: 0.653373, acc.: 60.94%] [G loss: 0.888861]\n",
      "epoch:17 step:16499 [D loss: 0.607173, acc.: 67.97%] [G loss: 0.932694]\n",
      "epoch:17 step:16500 [D loss: 0.611508, acc.: 62.50%] [G loss: 0.890804]\n",
      "epoch:17 step:16501 [D loss: 0.517937, acc.: 75.78%] [G loss: 1.406356]\n",
      "epoch:17 step:16502 [D loss: 0.557942, acc.: 75.00%] [G loss: 1.061883]\n",
      "epoch:17 step:16503 [D loss: 0.734046, acc.: 46.88%] [G loss: 0.977519]\n",
      "epoch:17 step:16504 [D loss: 0.716174, acc.: 46.88%] [G loss: 0.958121]\n",
      "epoch:17 step:16505 [D loss: 0.598399, acc.: 67.19%] [G loss: 0.887312]\n",
      "epoch:17 step:16506 [D loss: 0.508775, acc.: 77.34%] [G loss: 1.124081]\n",
      "epoch:17 step:16507 [D loss: 0.521716, acc.: 75.00%] [G loss: 1.068004]\n",
      "epoch:17 step:16508 [D loss: 0.548596, acc.: 81.25%] [G loss: 0.887426]\n",
      "epoch:17 step:16509 [D loss: 0.668710, acc.: 62.50%] [G loss: 0.921162]\n",
      "epoch:17 step:16510 [D loss: 0.632025, acc.: 64.84%] [G loss: 1.161833]\n",
      "epoch:17 step:16511 [D loss: 0.595976, acc.: 72.66%] [G loss: 1.051272]\n",
      "epoch:17 step:16512 [D loss: 0.621729, acc.: 63.28%] [G loss: 1.222385]\n",
      "epoch:17 step:16513 [D loss: 0.668129, acc.: 53.91%] [G loss: 0.824742]\n",
      "epoch:17 step:16514 [D loss: 0.633546, acc.: 62.50%] [G loss: 0.866052]\n",
      "epoch:17 step:16515 [D loss: 0.580152, acc.: 72.66%] [G loss: 1.094139]\n",
      "epoch:17 step:16516 [D loss: 0.642112, acc.: 71.09%] [G loss: 0.967562]\n",
      "epoch:17 step:16517 [D loss: 0.637527, acc.: 65.62%] [G loss: 0.903921]\n",
      "epoch:17 step:16518 [D loss: 0.575172, acc.: 68.75%] [G loss: 1.186517]\n",
      "epoch:17 step:16519 [D loss: 0.804903, acc.: 44.53%] [G loss: 0.992042]\n",
      "epoch:17 step:16520 [D loss: 0.587262, acc.: 67.19%] [G loss: 0.966510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16521 [D loss: 0.598320, acc.: 64.06%] [G loss: 1.022788]\n",
      "epoch:17 step:16522 [D loss: 0.640769, acc.: 64.06%] [G loss: 0.858787]\n",
      "epoch:17 step:16523 [D loss: 0.589691, acc.: 70.31%] [G loss: 0.944437]\n",
      "epoch:17 step:16524 [D loss: 0.747474, acc.: 51.56%] [G loss: 0.991101]\n",
      "epoch:17 step:16525 [D loss: 0.637387, acc.: 59.38%] [G loss: 0.896236]\n",
      "epoch:17 step:16526 [D loss: 0.674902, acc.: 54.69%] [G loss: 0.970703]\n",
      "epoch:17 step:16527 [D loss: 0.649262, acc.: 56.25%] [G loss: 0.924890]\n",
      "epoch:17 step:16528 [D loss: 0.772433, acc.: 44.53%] [G loss: 1.061234]\n",
      "epoch:17 step:16529 [D loss: 0.666445, acc.: 57.03%] [G loss: 0.874088]\n",
      "epoch:17 step:16530 [D loss: 0.580362, acc.: 73.44%] [G loss: 0.935358]\n",
      "epoch:17 step:16531 [D loss: 0.575676, acc.: 67.19%] [G loss: 0.918131]\n",
      "epoch:17 step:16532 [D loss: 0.573903, acc.: 74.22%] [G loss: 1.138404]\n",
      "epoch:17 step:16533 [D loss: 0.576643, acc.: 71.09%] [G loss: 0.901014]\n",
      "epoch:17 step:16534 [D loss: 0.608659, acc.: 67.97%] [G loss: 0.841573]\n",
      "epoch:17 step:16535 [D loss: 0.636211, acc.: 67.97%] [G loss: 0.911216]\n",
      "epoch:17 step:16536 [D loss: 0.594954, acc.: 71.88%] [G loss: 1.035453]\n",
      "epoch:17 step:16537 [D loss: 0.741311, acc.: 50.00%] [G loss: 1.018168]\n",
      "epoch:17 step:16538 [D loss: 0.628560, acc.: 64.84%] [G loss: 1.045216]\n",
      "epoch:17 step:16539 [D loss: 0.607058, acc.: 71.09%] [G loss: 0.881735]\n",
      "epoch:17 step:16540 [D loss: 0.590193, acc.: 75.78%] [G loss: 0.942130]\n",
      "epoch:17 step:16541 [D loss: 0.763062, acc.: 49.22%] [G loss: 0.883044]\n",
      "epoch:17 step:16542 [D loss: 0.658778, acc.: 58.59%] [G loss: 0.979868]\n",
      "epoch:17 step:16543 [D loss: 0.549565, acc.: 75.00%] [G loss: 1.233847]\n",
      "epoch:17 step:16544 [D loss: 0.613872, acc.: 69.53%] [G loss: 0.720057]\n",
      "epoch:17 step:16545 [D loss: 0.640102, acc.: 64.84%] [G loss: 1.051950]\n",
      "epoch:17 step:16546 [D loss: 0.549823, acc.: 74.22%] [G loss: 0.858899]\n",
      "epoch:17 step:16547 [D loss: 0.602208, acc.: 62.50%] [G loss: 0.701075]\n",
      "epoch:17 step:16548 [D loss: 0.668155, acc.: 59.38%] [G loss: 1.041485]\n",
      "epoch:17 step:16549 [D loss: 0.648960, acc.: 61.72%] [G loss: 0.862209]\n",
      "epoch:17 step:16550 [D loss: 0.646234, acc.: 64.06%] [G loss: 0.970190]\n",
      "epoch:17 step:16551 [D loss: 0.656720, acc.: 57.81%] [G loss: 0.961810]\n",
      "epoch:17 step:16552 [D loss: 0.755769, acc.: 49.22%] [G loss: 0.876709]\n",
      "epoch:17 step:16553 [D loss: 0.645424, acc.: 63.28%] [G loss: 0.984032]\n",
      "epoch:17 step:16554 [D loss: 0.709565, acc.: 48.44%] [G loss: 0.959351]\n",
      "epoch:17 step:16555 [D loss: 0.730780, acc.: 50.00%] [G loss: 1.253111]\n",
      "epoch:17 step:16556 [D loss: 0.584010, acc.: 67.19%] [G loss: 1.057202]\n",
      "epoch:17 step:16557 [D loss: 0.671257, acc.: 53.91%] [G loss: 1.024481]\n",
      "epoch:17 step:16558 [D loss: 0.633595, acc.: 64.06%] [G loss: 1.216423]\n",
      "epoch:17 step:16559 [D loss: 0.630524, acc.: 62.50%] [G loss: 0.970916]\n",
      "epoch:17 step:16560 [D loss: 0.606436, acc.: 66.41%] [G loss: 0.979704]\n",
      "epoch:17 step:16561 [D loss: 0.551650, acc.: 77.34%] [G loss: 0.997787]\n",
      "epoch:17 step:16562 [D loss: 0.601633, acc.: 71.88%] [G loss: 1.028350]\n",
      "epoch:17 step:16563 [D loss: 0.618803, acc.: 61.72%] [G loss: 0.945732]\n",
      "epoch:17 step:16564 [D loss: 0.580436, acc.: 66.41%] [G loss: 0.896555]\n",
      "epoch:17 step:16565 [D loss: 0.630511, acc.: 64.06%] [G loss: 1.019641]\n",
      "epoch:17 step:16566 [D loss: 0.653045, acc.: 57.03%] [G loss: 1.086786]\n",
      "epoch:17 step:16567 [D loss: 0.738218, acc.: 51.56%] [G loss: 1.073780]\n",
      "epoch:17 step:16568 [D loss: 0.641643, acc.: 60.16%] [G loss: 1.055643]\n",
      "epoch:17 step:16569 [D loss: 0.635128, acc.: 66.41%] [G loss: 0.993030]\n",
      "epoch:17 step:16570 [D loss: 0.640653, acc.: 59.38%] [G loss: 0.991749]\n",
      "epoch:17 step:16571 [D loss: 0.643050, acc.: 57.81%] [G loss: 1.024393]\n",
      "epoch:17 step:16572 [D loss: 0.679402, acc.: 57.03%] [G loss: 0.861765]\n",
      "epoch:17 step:16573 [D loss: 0.634562, acc.: 60.94%] [G loss: 0.963190]\n",
      "epoch:17 step:16574 [D loss: 0.666456, acc.: 61.72%] [G loss: 0.880016]\n",
      "epoch:17 step:16575 [D loss: 0.555058, acc.: 68.75%] [G loss: 1.002629]\n",
      "epoch:17 step:16576 [D loss: 0.514019, acc.: 81.25%] [G loss: 1.012429]\n",
      "epoch:17 step:16577 [D loss: 0.678849, acc.: 52.34%] [G loss: 0.811121]\n",
      "epoch:17 step:16578 [D loss: 0.568370, acc.: 69.53%] [G loss: 0.898637]\n",
      "epoch:17 step:16579 [D loss: 0.565503, acc.: 75.78%] [G loss: 0.917719]\n",
      "epoch:17 step:16580 [D loss: 0.568730, acc.: 78.12%] [G loss: 0.920352]\n",
      "epoch:17 step:16581 [D loss: 0.587303, acc.: 73.44%] [G loss: 0.939747]\n",
      "epoch:17 step:16582 [D loss: 0.600710, acc.: 66.41%] [G loss: 0.881270]\n",
      "epoch:17 step:16583 [D loss: 0.622148, acc.: 64.06%] [G loss: 0.866937]\n",
      "epoch:17 step:16584 [D loss: 0.636368, acc.: 61.72%] [G loss: 0.765725]\n",
      "epoch:17 step:16585 [D loss: 0.556339, acc.: 75.00%] [G loss: 1.013601]\n",
      "epoch:17 step:16586 [D loss: 0.691377, acc.: 52.34%] [G loss: 0.950065]\n",
      "epoch:17 step:16587 [D loss: 0.659305, acc.: 59.38%] [G loss: 1.204297]\n",
      "epoch:17 step:16588 [D loss: 0.481280, acc.: 82.81%] [G loss: 1.386039]\n",
      "epoch:17 step:16589 [D loss: 0.602442, acc.: 60.94%] [G loss: 0.876178]\n",
      "epoch:17 step:16590 [D loss: 0.705577, acc.: 55.47%] [G loss: 1.087213]\n",
      "epoch:17 step:16591 [D loss: 0.600232, acc.: 75.78%] [G loss: 1.065907]\n",
      "epoch:17 step:16592 [D loss: 0.692455, acc.: 54.69%] [G loss: 0.849992]\n",
      "epoch:17 step:16593 [D loss: 0.600602, acc.: 69.53%] [G loss: 1.179078]\n",
      "epoch:17 step:16594 [D loss: 0.542848, acc.: 78.91%] [G loss: 1.078521]\n",
      "epoch:17 step:16595 [D loss: 0.807114, acc.: 42.19%] [G loss: 0.844332]\n",
      "epoch:17 step:16596 [D loss: 0.547003, acc.: 74.22%] [G loss: 0.820734]\n",
      "epoch:17 step:16597 [D loss: 0.682244, acc.: 61.72%] [G loss: 0.891346]\n",
      "epoch:17 step:16598 [D loss: 0.663659, acc.: 54.69%] [G loss: 1.303996]\n",
      "epoch:17 step:16599 [D loss: 0.574041, acc.: 70.31%] [G loss: 1.045747]\n",
      "epoch:17 step:16600 [D loss: 0.639168, acc.: 63.28%] [G loss: 0.896152]\n",
      "epoch:17 step:16601 [D loss: 0.698586, acc.: 50.78%] [G loss: 1.239444]\n",
      "epoch:17 step:16602 [D loss: 0.618007, acc.: 64.84%] [G loss: 1.224195]\n",
      "epoch:17 step:16603 [D loss: 0.580771, acc.: 76.56%] [G loss: 1.532552]\n",
      "epoch:17 step:16604 [D loss: 0.647388, acc.: 66.41%] [G loss: 1.256562]\n",
      "epoch:17 step:16605 [D loss: 0.562365, acc.: 71.09%] [G loss: 1.015309]\n",
      "epoch:17 step:16606 [D loss: 0.608021, acc.: 62.50%] [G loss: 0.976499]\n",
      "epoch:17 step:16607 [D loss: 0.643824, acc.: 62.50%] [G loss: 0.968006]\n",
      "epoch:17 step:16608 [D loss: 0.681802, acc.: 57.03%] [G loss: 1.024786]\n",
      "epoch:17 step:16609 [D loss: 0.624378, acc.: 60.16%] [G loss: 1.126980]\n",
      "epoch:17 step:16610 [D loss: 0.617142, acc.: 64.84%] [G loss: 1.134893]\n",
      "epoch:17 step:16611 [D loss: 0.575732, acc.: 71.09%] [G loss: 1.278342]\n",
      "epoch:17 step:16612 [D loss: 0.705778, acc.: 54.69%] [G loss: 0.902124]\n",
      "epoch:17 step:16613 [D loss: 0.745099, acc.: 42.19%] [G loss: 1.270545]\n",
      "epoch:17 step:16614 [D loss: 0.439464, acc.: 88.28%] [G loss: 1.153833]\n",
      "epoch:17 step:16615 [D loss: 0.606863, acc.: 64.84%] [G loss: 1.292395]\n",
      "epoch:17 step:16616 [D loss: 0.580540, acc.: 66.41%] [G loss: 1.102081]\n",
      "epoch:17 step:16617 [D loss: 0.564083, acc.: 74.22%] [G loss: 1.277067]\n",
      "epoch:17 step:16618 [D loss: 0.715670, acc.: 54.69%] [G loss: 0.782685]\n",
      "epoch:17 step:16619 [D loss: 0.583655, acc.: 68.75%] [G loss: 1.127497]\n",
      "epoch:17 step:16620 [D loss: 0.618446, acc.: 66.41%] [G loss: 0.854301]\n",
      "epoch:17 step:16621 [D loss: 0.647087, acc.: 61.72%] [G loss: 0.814914]\n",
      "epoch:17 step:16622 [D loss: 0.655959, acc.: 66.41%] [G loss: 0.716550]\n",
      "epoch:17 step:16623 [D loss: 0.774548, acc.: 48.44%] [G loss: 0.889012]\n",
      "epoch:17 step:16624 [D loss: 0.644637, acc.: 65.62%] [G loss: 1.073272]\n",
      "epoch:17 step:16625 [D loss: 0.596143, acc.: 60.16%] [G loss: 1.100979]\n",
      "epoch:17 step:16626 [D loss: 0.682712, acc.: 53.91%] [G loss: 0.867947]\n",
      "epoch:17 step:16627 [D loss: 0.666653, acc.: 59.38%] [G loss: 0.878672]\n",
      "epoch:17 step:16628 [D loss: 0.709571, acc.: 57.81%] [G loss: 0.960347]\n",
      "epoch:17 step:16629 [D loss: 0.723115, acc.: 47.66%] [G loss: 0.844826]\n",
      "epoch:17 step:16630 [D loss: 0.659733, acc.: 55.47%] [G loss: 0.985220]\n",
      "epoch:17 step:16631 [D loss: 0.681405, acc.: 57.03%] [G loss: 0.954218]\n",
      "epoch:17 step:16632 [D loss: 0.855153, acc.: 39.84%] [G loss: 0.867726]\n",
      "epoch:17 step:16633 [D loss: 0.585112, acc.: 73.44%] [G loss: 1.012744]\n",
      "epoch:17 step:16634 [D loss: 0.718387, acc.: 53.91%] [G loss: 0.844638]\n",
      "epoch:17 step:16635 [D loss: 0.588318, acc.: 67.19%] [G loss: 0.715233]\n",
      "epoch:17 step:16636 [D loss: 0.623149, acc.: 65.62%] [G loss: 0.952968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16637 [D loss: 0.667221, acc.: 55.47%] [G loss: 0.914221]\n",
      "epoch:17 step:16638 [D loss: 0.651158, acc.: 62.50%] [G loss: 0.852907]\n",
      "epoch:17 step:16639 [D loss: 0.687899, acc.: 53.91%] [G loss: 0.907116]\n",
      "epoch:17 step:16640 [D loss: 0.647112, acc.: 60.16%] [G loss: 1.260987]\n",
      "epoch:17 step:16641 [D loss: 0.714580, acc.: 52.34%] [G loss: 1.088720]\n",
      "epoch:17 step:16642 [D loss: 0.608993, acc.: 63.28%] [G loss: 0.985203]\n",
      "epoch:17 step:16643 [D loss: 0.567211, acc.: 73.44%] [G loss: 1.129168]\n",
      "epoch:17 step:16644 [D loss: 0.684650, acc.: 57.03%] [G loss: 0.999274]\n",
      "epoch:17 step:16645 [D loss: 0.598275, acc.: 67.97%] [G loss: 1.132788]\n",
      "epoch:17 step:16646 [D loss: 0.848626, acc.: 41.41%] [G loss: 1.278872]\n",
      "epoch:17 step:16647 [D loss: 0.661594, acc.: 62.50%] [G loss: 0.886783]\n",
      "epoch:17 step:16648 [D loss: 0.602336, acc.: 67.19%] [G loss: 1.134701]\n",
      "epoch:17 step:16649 [D loss: 0.590384, acc.: 71.09%] [G loss: 1.206854]\n",
      "epoch:17 step:16650 [D loss: 0.641445, acc.: 59.38%] [G loss: 1.353520]\n",
      "epoch:17 step:16651 [D loss: 0.596773, acc.: 70.31%] [G loss: 1.083998]\n",
      "epoch:17 step:16652 [D loss: 0.672479, acc.: 47.66%] [G loss: 1.070757]\n",
      "epoch:17 step:16653 [D loss: 0.756437, acc.: 47.66%] [G loss: 0.778108]\n",
      "epoch:17 step:16654 [D loss: 0.671320, acc.: 59.38%] [G loss: 0.640038]\n",
      "epoch:17 step:16655 [D loss: 0.683022, acc.: 51.56%] [G loss: 1.139876]\n",
      "epoch:17 step:16656 [D loss: 0.463104, acc.: 88.28%] [G loss: 0.917289]\n",
      "epoch:17 step:16657 [D loss: 0.771529, acc.: 40.62%] [G loss: 0.916639]\n",
      "epoch:17 step:16658 [D loss: 0.645376, acc.: 56.25%] [G loss: 0.845360]\n",
      "epoch:17 step:16659 [D loss: 0.605523, acc.: 72.66%] [G loss: 0.807787]\n",
      "epoch:17 step:16660 [D loss: 0.719898, acc.: 47.66%] [G loss: 0.987738]\n",
      "epoch:17 step:16661 [D loss: 0.621729, acc.: 64.84%] [G loss: 1.297251]\n",
      "epoch:17 step:16662 [D loss: 0.549419, acc.: 77.34%] [G loss: 0.809703]\n",
      "epoch:17 step:16663 [D loss: 0.552437, acc.: 71.88%] [G loss: 0.958644]\n",
      "epoch:17 step:16664 [D loss: 0.689688, acc.: 55.47%] [G loss: 0.921995]\n",
      "epoch:17 step:16665 [D loss: 0.653434, acc.: 67.19%] [G loss: 1.193725]\n",
      "epoch:17 step:16666 [D loss: 0.499738, acc.: 83.59%] [G loss: 0.916529]\n",
      "epoch:17 step:16667 [D loss: 0.668358, acc.: 59.38%] [G loss: 1.122874]\n",
      "epoch:17 step:16668 [D loss: 0.507700, acc.: 84.38%] [G loss: 0.906235]\n",
      "epoch:17 step:16669 [D loss: 0.636091, acc.: 60.16%] [G loss: 0.967894]\n",
      "epoch:17 step:16670 [D loss: 0.719419, acc.: 53.12%] [G loss: 0.948522]\n",
      "epoch:17 step:16671 [D loss: 0.647556, acc.: 59.38%] [G loss: 1.067932]\n",
      "epoch:17 step:16672 [D loss: 0.527478, acc.: 78.12%] [G loss: 1.139084]\n",
      "epoch:17 step:16673 [D loss: 0.586129, acc.: 70.31%] [G loss: 0.859874]\n",
      "epoch:17 step:16674 [D loss: 0.506146, acc.: 75.78%] [G loss: 0.890023]\n",
      "epoch:17 step:16675 [D loss: 0.769109, acc.: 45.31%] [G loss: 0.736038]\n",
      "epoch:17 step:16676 [D loss: 0.567853, acc.: 71.88%] [G loss: 1.173763]\n",
      "epoch:17 step:16677 [D loss: 0.535188, acc.: 76.56%] [G loss: 0.934267]\n",
      "epoch:17 step:16678 [D loss: 0.577256, acc.: 67.97%] [G loss: 1.396935]\n",
      "epoch:17 step:16679 [D loss: 0.716927, acc.: 52.34%] [G loss: 0.969185]\n",
      "epoch:17 step:16680 [D loss: 0.818593, acc.: 33.59%] [G loss: 0.772798]\n",
      "epoch:17 step:16681 [D loss: 0.696823, acc.: 57.03%] [G loss: 0.881256]\n",
      "epoch:17 step:16682 [D loss: 0.689955, acc.: 48.44%] [G loss: 0.955266]\n",
      "epoch:17 step:16683 [D loss: 0.566430, acc.: 72.66%] [G loss: 1.067782]\n",
      "epoch:17 step:16684 [D loss: 0.721959, acc.: 49.22%] [G loss: 0.934821]\n",
      "epoch:17 step:16685 [D loss: 0.575822, acc.: 68.75%] [G loss: 0.950644]\n",
      "epoch:17 step:16686 [D loss: 0.552324, acc.: 78.12%] [G loss: 1.008539]\n",
      "epoch:17 step:16687 [D loss: 0.598703, acc.: 67.97%] [G loss: 0.978074]\n",
      "epoch:17 step:16688 [D loss: 0.700548, acc.: 54.69%] [G loss: 1.242948]\n",
      "epoch:17 step:16689 [D loss: 0.617733, acc.: 67.97%] [G loss: 1.252892]\n",
      "epoch:17 step:16690 [D loss: 0.654474, acc.: 59.38%] [G loss: 1.175347]\n",
      "epoch:17 step:16691 [D loss: 0.561855, acc.: 74.22%] [G loss: 1.028615]\n",
      "epoch:17 step:16692 [D loss: 0.647560, acc.: 66.41%] [G loss: 0.796643]\n",
      "epoch:17 step:16693 [D loss: 0.520586, acc.: 80.47%] [G loss: 1.656443]\n",
      "epoch:17 step:16694 [D loss: 0.541865, acc.: 73.44%] [G loss: 1.044634]\n",
      "epoch:17 step:16695 [D loss: 0.541390, acc.: 70.31%] [G loss: 1.018370]\n",
      "epoch:17 step:16696 [D loss: 0.643405, acc.: 67.19%] [G loss: 1.046051]\n",
      "epoch:17 step:16697 [D loss: 0.522195, acc.: 75.00%] [G loss: 1.103333]\n",
      "epoch:17 step:16698 [D loss: 0.594576, acc.: 64.84%] [G loss: 1.113730]\n",
      "epoch:17 step:16699 [D loss: 0.655451, acc.: 57.81%] [G loss: 1.061000]\n",
      "epoch:17 step:16700 [D loss: 0.466808, acc.: 85.94%] [G loss: 0.737248]\n",
      "epoch:17 step:16701 [D loss: 0.708868, acc.: 57.03%] [G loss: 0.932134]\n",
      "epoch:17 step:16702 [D loss: 0.528807, acc.: 74.22%] [G loss: 1.476171]\n",
      "epoch:17 step:16703 [D loss: 0.646859, acc.: 60.94%] [G loss: 1.273955]\n",
      "epoch:17 step:16704 [D loss: 0.594441, acc.: 68.75%] [G loss: 0.830209]\n",
      "epoch:17 step:16705 [D loss: 0.625476, acc.: 63.28%] [G loss: 1.244081]\n",
      "epoch:17 step:16706 [D loss: 0.561489, acc.: 71.88%] [G loss: 1.031180]\n",
      "epoch:17 step:16707 [D loss: 0.764379, acc.: 39.06%] [G loss: 1.106444]\n",
      "epoch:17 step:16708 [D loss: 0.940031, acc.: 27.34%] [G loss: 0.771989]\n",
      "epoch:17 step:16709 [D loss: 0.710899, acc.: 52.34%] [G loss: 1.085856]\n",
      "epoch:17 step:16710 [D loss: 0.577176, acc.: 69.53%] [G loss: 1.319164]\n",
      "epoch:17 step:16711 [D loss: 0.628913, acc.: 65.62%] [G loss: 1.403118]\n",
      "epoch:17 step:16712 [D loss: 0.622000, acc.: 64.84%] [G loss: 0.881137]\n",
      "epoch:17 step:16713 [D loss: 0.622846, acc.: 64.84%] [G loss: 0.992898]\n",
      "epoch:17 step:16714 [D loss: 0.625565, acc.: 63.28%] [G loss: 1.148018]\n",
      "epoch:17 step:16715 [D loss: 0.705410, acc.: 56.25%] [G loss: 0.966743]\n",
      "epoch:17 step:16716 [D loss: 0.658051, acc.: 60.16%] [G loss: 0.941567]\n",
      "epoch:17 step:16717 [D loss: 0.624498, acc.: 62.50%] [G loss: 1.053791]\n",
      "epoch:17 step:16718 [D loss: 0.622139, acc.: 62.50%] [G loss: 0.931059]\n",
      "epoch:17 step:16719 [D loss: 0.688315, acc.: 52.34%] [G loss: 1.090308]\n",
      "epoch:17 step:16720 [D loss: 0.466382, acc.: 90.62%] [G loss: 1.360050]\n",
      "epoch:17 step:16721 [D loss: 0.573837, acc.: 70.31%] [G loss: 0.935067]\n",
      "epoch:17 step:16722 [D loss: 0.484031, acc.: 76.56%] [G loss: 1.094184]\n",
      "epoch:17 step:16723 [D loss: 0.547623, acc.: 70.31%] [G loss: 1.345722]\n",
      "epoch:17 step:16724 [D loss: 0.578058, acc.: 67.97%] [G loss: 0.894408]\n",
      "epoch:17 step:16725 [D loss: 0.585231, acc.: 74.22%] [G loss: 0.949260]\n",
      "epoch:17 step:16726 [D loss: 0.653672, acc.: 55.47%] [G loss: 0.990836]\n",
      "epoch:17 step:16727 [D loss: 0.648775, acc.: 58.59%] [G loss: 1.034504]\n",
      "epoch:17 step:16728 [D loss: 0.564651, acc.: 73.44%] [G loss: 0.889760]\n",
      "epoch:17 step:16729 [D loss: 0.673298, acc.: 61.72%] [G loss: 1.263729]\n",
      "epoch:17 step:16730 [D loss: 0.621326, acc.: 63.28%] [G loss: 0.928709]\n",
      "epoch:17 step:16731 [D loss: 0.757841, acc.: 45.31%] [G loss: 0.948119]\n",
      "epoch:17 step:16732 [D loss: 0.476238, acc.: 82.03%] [G loss: 1.700486]\n",
      "epoch:17 step:16733 [D loss: 0.583606, acc.: 71.09%] [G loss: 1.173484]\n",
      "epoch:17 step:16734 [D loss: 0.545575, acc.: 81.25%] [G loss: 0.836534]\n",
      "epoch:17 step:16735 [D loss: 0.656583, acc.: 63.28%] [G loss: 0.813377]\n",
      "epoch:17 step:16736 [D loss: 0.681454, acc.: 62.50%] [G loss: 1.009806]\n",
      "epoch:17 step:16737 [D loss: 0.591088, acc.: 70.31%] [G loss: 1.040090]\n",
      "epoch:17 step:16738 [D loss: 0.684248, acc.: 57.03%] [G loss: 1.038715]\n",
      "epoch:17 step:16739 [D loss: 0.514756, acc.: 79.69%] [G loss: 0.932732]\n",
      "epoch:17 step:16740 [D loss: 0.581379, acc.: 76.56%] [G loss: 1.018808]\n",
      "epoch:17 step:16741 [D loss: 0.659256, acc.: 58.59%] [G loss: 0.949412]\n",
      "epoch:17 step:16742 [D loss: 0.628655, acc.: 66.41%] [G loss: 0.908676]\n",
      "epoch:17 step:16743 [D loss: 0.694212, acc.: 60.94%] [G loss: 0.997140]\n",
      "epoch:17 step:16744 [D loss: 0.705300, acc.: 54.69%] [G loss: 0.857729]\n",
      "epoch:17 step:16745 [D loss: 0.574837, acc.: 72.66%] [G loss: 1.021425]\n",
      "epoch:17 step:16746 [D loss: 0.657301, acc.: 60.94%] [G loss: 1.119383]\n",
      "epoch:17 step:16747 [D loss: 0.638187, acc.: 61.72%] [G loss: 0.929441]\n",
      "epoch:17 step:16748 [D loss: 0.605809, acc.: 64.84%] [G loss: 1.132160]\n",
      "epoch:17 step:16749 [D loss: 0.589097, acc.: 66.41%] [G loss: 1.036036]\n",
      "epoch:17 step:16750 [D loss: 0.600304, acc.: 68.75%] [G loss: 1.358940]\n",
      "epoch:17 step:16751 [D loss: 0.589574, acc.: 67.19%] [G loss: 1.047853]\n",
      "epoch:17 step:16752 [D loss: 0.578595, acc.: 69.53%] [G loss: 0.787769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16753 [D loss: 0.751748, acc.: 46.09%] [G loss: 0.960859]\n",
      "epoch:17 step:16754 [D loss: 0.616367, acc.: 63.28%] [G loss: 1.237629]\n",
      "epoch:17 step:16755 [D loss: 0.530282, acc.: 77.34%] [G loss: 1.144574]\n",
      "epoch:17 step:16756 [D loss: 0.519665, acc.: 82.03%] [G loss: 1.042591]\n",
      "epoch:17 step:16757 [D loss: 0.638712, acc.: 62.50%] [G loss: 1.250911]\n",
      "epoch:17 step:16758 [D loss: 0.514956, acc.: 76.56%] [G loss: 0.936704]\n",
      "epoch:17 step:16759 [D loss: 0.785646, acc.: 42.19%] [G loss: 0.710579]\n",
      "epoch:17 step:16760 [D loss: 0.654239, acc.: 66.41%] [G loss: 0.879195]\n",
      "epoch:17 step:16761 [D loss: 0.741430, acc.: 52.34%] [G loss: 1.235131]\n",
      "epoch:17 step:16762 [D loss: 0.731640, acc.: 55.47%] [G loss: 1.097016]\n",
      "epoch:17 step:16763 [D loss: 0.563615, acc.: 71.09%] [G loss: 0.773023]\n",
      "epoch:17 step:16764 [D loss: 0.732248, acc.: 43.75%] [G loss: 0.931251]\n",
      "epoch:17 step:16765 [D loss: 0.684083, acc.: 53.12%] [G loss: 0.763817]\n",
      "epoch:17 step:16766 [D loss: 0.739473, acc.: 46.09%] [G loss: 0.999080]\n",
      "epoch:17 step:16767 [D loss: 0.650344, acc.: 61.72%] [G loss: 0.933407]\n",
      "epoch:17 step:16768 [D loss: 0.694435, acc.: 50.00%] [G loss: 0.795077]\n",
      "epoch:17 step:16769 [D loss: 0.744757, acc.: 49.22%] [G loss: 1.072485]\n",
      "epoch:17 step:16770 [D loss: 0.573827, acc.: 75.78%] [G loss: 0.970849]\n",
      "epoch:17 step:16771 [D loss: 0.786552, acc.: 44.53%] [G loss: 0.846559]\n",
      "epoch:17 step:16772 [D loss: 0.643926, acc.: 63.28%] [G loss: 1.245656]\n",
      "epoch:17 step:16773 [D loss: 0.666387, acc.: 53.12%] [G loss: 1.141864]\n",
      "epoch:17 step:16774 [D loss: 0.715451, acc.: 51.56%] [G loss: 1.167010]\n",
      "epoch:17 step:16775 [D loss: 0.550022, acc.: 73.44%] [G loss: 0.689777]\n",
      "epoch:17 step:16776 [D loss: 0.714567, acc.: 61.72%] [G loss: 1.010484]\n",
      "epoch:17 step:16777 [D loss: 0.638525, acc.: 67.19%] [G loss: 1.176239]\n",
      "epoch:17 step:16778 [D loss: 0.622867, acc.: 60.16%] [G loss: 0.989726]\n",
      "epoch:17 step:16779 [D loss: 0.604767, acc.: 62.50%] [G loss: 1.062278]\n",
      "epoch:17 step:16780 [D loss: 0.691844, acc.: 52.34%] [G loss: 0.984894]\n",
      "epoch:17 step:16781 [D loss: 0.653283, acc.: 57.03%] [G loss: 0.837946]\n",
      "epoch:17 step:16782 [D loss: 0.709228, acc.: 53.91%] [G loss: 0.796899]\n",
      "epoch:17 step:16783 [D loss: 0.649105, acc.: 58.59%] [G loss: 0.862791]\n",
      "epoch:17 step:16784 [D loss: 0.622724, acc.: 70.31%] [G loss: 0.915727]\n",
      "epoch:17 step:16785 [D loss: 0.642320, acc.: 66.41%] [G loss: 0.884026]\n",
      "epoch:17 step:16786 [D loss: 0.683959, acc.: 56.25%] [G loss: 0.849447]\n",
      "epoch:17 step:16787 [D loss: 0.561595, acc.: 71.88%] [G loss: 1.067638]\n",
      "epoch:17 step:16788 [D loss: 0.638781, acc.: 54.69%] [G loss: 0.841451]\n",
      "epoch:17 step:16789 [D loss: 0.672542, acc.: 53.12%] [G loss: 0.878424]\n",
      "epoch:17 step:16790 [D loss: 0.794856, acc.: 43.75%] [G loss: 0.985640]\n",
      "epoch:17 step:16791 [D loss: 0.670009, acc.: 52.34%] [G loss: 1.064151]\n",
      "epoch:17 step:16792 [D loss: 0.566018, acc.: 74.22%] [G loss: 1.178729]\n",
      "epoch:17 step:16793 [D loss: 0.654519, acc.: 54.69%] [G loss: 1.077367]\n",
      "epoch:17 step:16794 [D loss: 0.734511, acc.: 53.12%] [G loss: 0.991603]\n",
      "epoch:17 step:16795 [D loss: 0.653429, acc.: 64.06%] [G loss: 0.902353]\n",
      "epoch:17 step:16796 [D loss: 0.611127, acc.: 60.94%] [G loss: 0.808242]\n",
      "epoch:17 step:16797 [D loss: 0.573330, acc.: 73.44%] [G loss: 0.878790]\n",
      "epoch:17 step:16798 [D loss: 0.630107, acc.: 65.62%] [G loss: 0.996502]\n",
      "epoch:17 step:16799 [D loss: 0.705840, acc.: 59.38%] [G loss: 0.837934]\n",
      "epoch:17 step:16800 [D loss: 0.580680, acc.: 66.41%] [G loss: 1.248839]\n",
      "epoch:17 step:16801 [D loss: 0.611207, acc.: 61.72%] [G loss: 0.989273]\n",
      "epoch:17 step:16802 [D loss: 0.848584, acc.: 34.38%] [G loss: 0.971588]\n",
      "epoch:17 step:16803 [D loss: 0.646341, acc.: 60.16%] [G loss: 0.982198]\n",
      "epoch:17 step:16804 [D loss: 0.756917, acc.: 33.59%] [G loss: 0.961026]\n",
      "epoch:17 step:16805 [D loss: 0.618734, acc.: 65.62%] [G loss: 0.915736]\n",
      "epoch:17 step:16806 [D loss: 0.622555, acc.: 60.16%] [G loss: 0.792847]\n",
      "epoch:17 step:16807 [D loss: 0.669275, acc.: 57.81%] [G loss: 0.855730]\n",
      "epoch:17 step:16808 [D loss: 0.533004, acc.: 71.88%] [G loss: 1.269531]\n",
      "epoch:17 step:16809 [D loss: 0.682456, acc.: 56.25%] [G loss: 0.949864]\n",
      "epoch:17 step:16810 [D loss: 0.536313, acc.: 78.12%] [G loss: 1.106272]\n",
      "epoch:17 step:16811 [D loss: 0.621716, acc.: 67.97%] [G loss: 0.786964]\n",
      "epoch:17 step:16812 [D loss: 0.547572, acc.: 77.34%] [G loss: 0.913226]\n",
      "epoch:17 step:16813 [D loss: 0.549274, acc.: 75.00%] [G loss: 1.052997]\n",
      "epoch:17 step:16814 [D loss: 0.626989, acc.: 69.53%] [G loss: 1.174472]\n",
      "epoch:17 step:16815 [D loss: 0.614350, acc.: 68.75%] [G loss: 1.014268]\n",
      "epoch:17 step:16816 [D loss: 0.599636, acc.: 68.75%] [G loss: 0.985824]\n",
      "epoch:17 step:16817 [D loss: 0.478973, acc.: 85.16%] [G loss: 1.117241]\n",
      "epoch:17 step:16818 [D loss: 0.541109, acc.: 78.91%] [G loss: 1.074153]\n",
      "epoch:17 step:16819 [D loss: 0.493917, acc.: 82.03%] [G loss: 0.943679]\n",
      "epoch:17 step:16820 [D loss: 0.519536, acc.: 76.56%] [G loss: 1.128211]\n",
      "epoch:17 step:16821 [D loss: 0.544485, acc.: 72.66%] [G loss: 1.184629]\n",
      "epoch:17 step:16822 [D loss: 0.641822, acc.: 66.41%] [G loss: 1.210651]\n",
      "epoch:17 step:16823 [D loss: 0.488668, acc.: 78.91%] [G loss: 1.163710]\n",
      "epoch:17 step:16824 [D loss: 0.581165, acc.: 71.88%] [G loss: 0.644756]\n",
      "epoch:17 step:16825 [D loss: 0.537367, acc.: 75.00%] [G loss: 1.115837]\n",
      "epoch:17 step:16826 [D loss: 0.684556, acc.: 57.81%] [G loss: 0.873593]\n",
      "epoch:17 step:16827 [D loss: 0.531569, acc.: 71.88%] [G loss: 0.918891]\n",
      "epoch:17 step:16828 [D loss: 0.733938, acc.: 48.44%] [G loss: 1.090695]\n",
      "epoch:17 step:16829 [D loss: 0.622426, acc.: 64.06%] [G loss: 0.756402]\n",
      "epoch:17 step:16830 [D loss: 0.683372, acc.: 55.47%] [G loss: 1.189666]\n",
      "epoch:17 step:16831 [D loss: 0.679981, acc.: 57.03%] [G loss: 1.043552]\n",
      "epoch:17 step:16832 [D loss: 0.707458, acc.: 54.69%] [G loss: 1.050981]\n",
      "epoch:17 step:16833 [D loss: 0.532885, acc.: 77.34%] [G loss: 1.113717]\n",
      "epoch:17 step:16834 [D loss: 0.618171, acc.: 67.19%] [G loss: 0.833648]\n",
      "epoch:17 step:16835 [D loss: 0.581423, acc.: 64.84%] [G loss: 0.918225]\n",
      "epoch:17 step:16836 [D loss: 0.561730, acc.: 78.12%] [G loss: 0.842511]\n",
      "epoch:17 step:16837 [D loss: 0.606155, acc.: 69.53%] [G loss: 0.716251]\n",
      "epoch:17 step:16838 [D loss: 0.503407, acc.: 83.59%] [G loss: 1.024003]\n",
      "epoch:17 step:16839 [D loss: 0.476855, acc.: 76.56%] [G loss: 1.322014]\n",
      "epoch:17 step:16840 [D loss: 0.684030, acc.: 53.91%] [G loss: 0.937839]\n",
      "epoch:17 step:16841 [D loss: 0.639956, acc.: 60.94%] [G loss: 1.279362]\n",
      "epoch:17 step:16842 [D loss: 0.576781, acc.: 66.41%] [G loss: 1.132528]\n",
      "epoch:17 step:16843 [D loss: 0.514782, acc.: 72.66%] [G loss: 1.144424]\n",
      "epoch:17 step:16844 [D loss: 0.668513, acc.: 55.47%] [G loss: 1.020587]\n",
      "epoch:17 step:16845 [D loss: 0.722404, acc.: 54.69%] [G loss: 1.027728]\n",
      "epoch:17 step:16846 [D loss: 0.567426, acc.: 71.88%] [G loss: 1.455162]\n",
      "epoch:17 step:16847 [D loss: 0.768811, acc.: 45.31%] [G loss: 0.903299]\n",
      "epoch:17 step:16848 [D loss: 0.531412, acc.: 78.12%] [G loss: 1.080984]\n",
      "epoch:17 step:16849 [D loss: 0.654621, acc.: 54.69%] [G loss: 1.192587]\n",
      "epoch:17 step:16850 [D loss: 0.834397, acc.: 35.94%] [G loss: 0.995503]\n",
      "epoch:17 step:16851 [D loss: 0.614197, acc.: 64.06%] [G loss: 0.789258]\n",
      "epoch:17 step:16852 [D loss: 0.776731, acc.: 48.44%] [G loss: 0.944080]\n",
      "epoch:17 step:16853 [D loss: 0.492771, acc.: 81.25%] [G loss: 1.239743]\n",
      "epoch:17 step:16854 [D loss: 0.567987, acc.: 72.66%] [G loss: 0.865723]\n",
      "epoch:17 step:16855 [D loss: 0.754212, acc.: 42.19%] [G loss: 0.714962]\n",
      "epoch:17 step:16856 [D loss: 0.672733, acc.: 57.03%] [G loss: 1.027062]\n",
      "epoch:17 step:16857 [D loss: 0.608641, acc.: 71.88%] [G loss: 0.765758]\n",
      "epoch:17 step:16858 [D loss: 0.714859, acc.: 54.69%] [G loss: 0.872993]\n",
      "epoch:17 step:16859 [D loss: 0.717233, acc.: 48.44%] [G loss: 1.284658]\n",
      "epoch:17 step:16860 [D loss: 0.726936, acc.: 52.34%] [G loss: 0.832304]\n",
      "epoch:17 step:16861 [D loss: 0.650233, acc.: 55.47%] [G loss: 1.169240]\n",
      "epoch:17 step:16862 [D loss: 0.651552, acc.: 64.06%] [G loss: 1.112786]\n",
      "epoch:17 step:16863 [D loss: 0.606194, acc.: 63.28%] [G loss: 0.889050]\n",
      "epoch:17 step:16864 [D loss: 0.609678, acc.: 62.50%] [G loss: 0.768283]\n",
      "epoch:17 step:16865 [D loss: 0.570843, acc.: 74.22%] [G loss: 0.946916]\n",
      "epoch:17 step:16866 [D loss: 0.788335, acc.: 49.22%] [G loss: 0.907226]\n",
      "epoch:18 step:16867 [D loss: 0.681278, acc.: 61.72%] [G loss: 0.952473]\n",
      "epoch:18 step:16868 [D loss: 0.610230, acc.: 66.41%] [G loss: 1.038811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16869 [D loss: 0.511152, acc.: 78.91%] [G loss: 1.307978]\n",
      "epoch:18 step:16870 [D loss: 0.634667, acc.: 61.72%] [G loss: 1.016342]\n",
      "epoch:18 step:16871 [D loss: 0.601867, acc.: 66.41%] [G loss: 1.019897]\n",
      "epoch:18 step:16872 [D loss: 0.532216, acc.: 73.44%] [G loss: 1.330230]\n",
      "epoch:18 step:16873 [D loss: 0.557114, acc.: 73.44%] [G loss: 1.155668]\n",
      "epoch:18 step:16874 [D loss: 0.647446, acc.: 60.94%] [G loss: 0.805850]\n",
      "epoch:18 step:16875 [D loss: 0.593653, acc.: 66.41%] [G loss: 0.673629]\n",
      "epoch:18 step:16876 [D loss: 0.732393, acc.: 44.53%] [G loss: 1.422395]\n",
      "epoch:18 step:16877 [D loss: 0.760931, acc.: 51.56%] [G loss: 1.037098]\n",
      "epoch:18 step:16878 [D loss: 0.631223, acc.: 62.50%] [G loss: 0.983572]\n",
      "epoch:18 step:16879 [D loss: 0.617925, acc.: 66.41%] [G loss: 0.739090]\n",
      "epoch:18 step:16880 [D loss: 0.614546, acc.: 65.62%] [G loss: 0.787080]\n",
      "epoch:18 step:16881 [D loss: 0.689342, acc.: 58.59%] [G loss: 1.008721]\n",
      "epoch:18 step:16882 [D loss: 0.681194, acc.: 60.16%] [G loss: 1.136891]\n",
      "epoch:18 step:16883 [D loss: 0.672631, acc.: 61.72%] [G loss: 1.232975]\n",
      "epoch:18 step:16884 [D loss: 0.526336, acc.: 77.34%] [G loss: 1.349291]\n",
      "epoch:18 step:16885 [D loss: 0.578944, acc.: 67.19%] [G loss: 1.073826]\n",
      "epoch:18 step:16886 [D loss: 0.561181, acc.: 69.53%] [G loss: 1.044460]\n",
      "epoch:18 step:16887 [D loss: 0.485820, acc.: 78.91%] [G loss: 1.283169]\n",
      "epoch:18 step:16888 [D loss: 0.709250, acc.: 53.12%] [G loss: 0.796078]\n",
      "epoch:18 step:16889 [D loss: 0.655722, acc.: 62.50%] [G loss: 0.938092]\n",
      "epoch:18 step:16890 [D loss: 0.558137, acc.: 72.66%] [G loss: 1.269821]\n",
      "epoch:18 step:16891 [D loss: 0.617258, acc.: 61.72%] [G loss: 1.006783]\n",
      "epoch:18 step:16892 [D loss: 0.859782, acc.: 33.59%] [G loss: 0.950832]\n",
      "epoch:18 step:16893 [D loss: 0.741014, acc.: 47.66%] [G loss: 0.837098]\n",
      "epoch:18 step:16894 [D loss: 0.641378, acc.: 62.50%] [G loss: 0.973974]\n",
      "epoch:18 step:16895 [D loss: 0.646696, acc.: 61.72%] [G loss: 0.878803]\n",
      "epoch:18 step:16896 [D loss: 0.655488, acc.: 57.03%] [G loss: 0.983461]\n",
      "epoch:18 step:16897 [D loss: 0.680215, acc.: 52.34%] [G loss: 0.960068]\n",
      "epoch:18 step:16898 [D loss: 0.686069, acc.: 57.81%] [G loss: 1.094970]\n",
      "epoch:18 step:16899 [D loss: 0.694737, acc.: 48.44%] [G loss: 0.994568]\n",
      "epoch:18 step:16900 [D loss: 0.627838, acc.: 64.06%] [G loss: 0.942101]\n",
      "epoch:18 step:16901 [D loss: 0.634149, acc.: 59.38%] [G loss: 1.043477]\n",
      "epoch:18 step:16902 [D loss: 0.717118, acc.: 53.91%] [G loss: 1.048612]\n",
      "epoch:18 step:16903 [D loss: 0.605367, acc.: 68.75%] [G loss: 1.115713]\n",
      "epoch:18 step:16904 [D loss: 0.646580, acc.: 64.06%] [G loss: 1.082187]\n",
      "epoch:18 step:16905 [D loss: 0.771754, acc.: 47.66%] [G loss: 0.975481]\n",
      "epoch:18 step:16906 [D loss: 0.655610, acc.: 61.72%] [G loss: 1.079585]\n",
      "epoch:18 step:16907 [D loss: 0.694896, acc.: 54.69%] [G loss: 1.030595]\n",
      "epoch:18 step:16908 [D loss: 0.647306, acc.: 65.62%] [G loss: 0.972039]\n",
      "epoch:18 step:16909 [D loss: 0.624140, acc.: 67.19%] [G loss: 1.004059]\n",
      "epoch:18 step:16910 [D loss: 0.732258, acc.: 54.69%] [G loss: 1.045530]\n",
      "epoch:18 step:16911 [D loss: 0.665390, acc.: 57.81%] [G loss: 1.015402]\n",
      "epoch:18 step:16912 [D loss: 0.676608, acc.: 51.56%] [G loss: 1.086268]\n",
      "epoch:18 step:16913 [D loss: 0.708747, acc.: 56.25%] [G loss: 0.802820]\n",
      "epoch:18 step:16914 [D loss: 0.574665, acc.: 72.66%] [G loss: 1.191847]\n",
      "epoch:18 step:16915 [D loss: 0.574427, acc.: 74.22%] [G loss: 1.354647]\n",
      "epoch:18 step:16916 [D loss: 0.699573, acc.: 51.56%] [G loss: 0.908477]\n",
      "epoch:18 step:16917 [D loss: 0.668678, acc.: 51.56%] [G loss: 1.000676]\n",
      "epoch:18 step:16918 [D loss: 0.617465, acc.: 72.66%] [G loss: 1.012917]\n",
      "epoch:18 step:16919 [D loss: 0.601304, acc.: 71.88%] [G loss: 1.140195]\n",
      "epoch:18 step:16920 [D loss: 0.695188, acc.: 55.47%] [G loss: 0.863730]\n",
      "epoch:18 step:16921 [D loss: 0.710224, acc.: 46.88%] [G loss: 0.999181]\n",
      "epoch:18 step:16922 [D loss: 0.583443, acc.: 66.41%] [G loss: 0.716428]\n",
      "epoch:18 step:16923 [D loss: 0.525762, acc.: 75.78%] [G loss: 1.020158]\n",
      "epoch:18 step:16924 [D loss: 0.618449, acc.: 68.75%] [G loss: 0.792638]\n",
      "epoch:18 step:16925 [D loss: 0.567576, acc.: 71.09%] [G loss: 1.066258]\n",
      "epoch:18 step:16926 [D loss: 0.674507, acc.: 58.59%] [G loss: 0.930400]\n",
      "epoch:18 step:16927 [D loss: 0.603536, acc.: 74.22%] [G loss: 0.881558]\n",
      "epoch:18 step:16928 [D loss: 0.629721, acc.: 65.62%] [G loss: 1.233635]\n",
      "epoch:18 step:16929 [D loss: 0.568802, acc.: 69.53%] [G loss: 1.131866]\n",
      "epoch:18 step:16930 [D loss: 0.561674, acc.: 70.31%] [G loss: 1.194137]\n",
      "epoch:18 step:16931 [D loss: 0.592030, acc.: 67.97%] [G loss: 1.076007]\n",
      "epoch:18 step:16932 [D loss: 0.588984, acc.: 67.97%] [G loss: 1.220634]\n",
      "epoch:18 step:16933 [D loss: 0.670365, acc.: 63.28%] [G loss: 1.201121]\n",
      "epoch:18 step:16934 [D loss: 0.548469, acc.: 71.88%] [G loss: 3.566080]\n",
      "epoch:18 step:16935 [D loss: 0.634916, acc.: 63.28%] [G loss: 1.258977]\n",
      "epoch:18 step:16936 [D loss: 0.545642, acc.: 72.66%] [G loss: 0.984295]\n",
      "epoch:18 step:16937 [D loss: 0.664643, acc.: 58.59%] [G loss: 1.017783]\n",
      "epoch:18 step:16938 [D loss: 0.586751, acc.: 72.66%] [G loss: 0.947564]\n",
      "epoch:18 step:16939 [D loss: 0.442750, acc.: 82.81%] [G loss: 1.244767]\n",
      "epoch:18 step:16940 [D loss: 0.665385, acc.: 56.25%] [G loss: 0.954480]\n",
      "epoch:18 step:16941 [D loss: 0.585912, acc.: 67.97%] [G loss: 1.056051]\n",
      "epoch:18 step:16942 [D loss: 0.592822, acc.: 73.44%] [G loss: 1.156945]\n",
      "epoch:18 step:16943 [D loss: 0.484739, acc.: 80.47%] [G loss: 1.236110]\n",
      "epoch:18 step:16944 [D loss: 0.481694, acc.: 82.81%] [G loss: 0.964361]\n",
      "epoch:18 step:16945 [D loss: 0.537333, acc.: 73.44%] [G loss: 1.452410]\n",
      "epoch:18 step:16946 [D loss: 0.536287, acc.: 78.91%] [G loss: 1.296910]\n",
      "epoch:18 step:16947 [D loss: 0.402123, acc.: 88.28%] [G loss: 0.736460]\n",
      "epoch:18 step:16948 [D loss: 0.737668, acc.: 53.12%] [G loss: 0.670504]\n",
      "epoch:18 step:16949 [D loss: 0.678835, acc.: 56.25%] [G loss: 0.785021]\n",
      "epoch:18 step:16950 [D loss: 0.674487, acc.: 59.38%] [G loss: 1.124662]\n",
      "epoch:18 step:16951 [D loss: 0.754624, acc.: 50.78%] [G loss: 1.259039]\n",
      "epoch:18 step:16952 [D loss: 0.654469, acc.: 60.94%] [G loss: 1.326339]\n",
      "epoch:18 step:16953 [D loss: 0.568845, acc.: 75.00%] [G loss: 0.886595]\n",
      "epoch:18 step:16954 [D loss: 0.642773, acc.: 57.03%] [G loss: 1.158233]\n",
      "epoch:18 step:16955 [D loss: 0.598292, acc.: 64.06%] [G loss: 0.918554]\n",
      "epoch:18 step:16956 [D loss: 0.559899, acc.: 71.88%] [G loss: 1.108490]\n",
      "epoch:18 step:16957 [D loss: 0.529728, acc.: 79.69%] [G loss: 0.918624]\n",
      "epoch:18 step:16958 [D loss: 0.574127, acc.: 67.19%] [G loss: 0.985184]\n",
      "epoch:18 step:16959 [D loss: 0.616976, acc.: 62.50%] [G loss: 0.845900]\n",
      "epoch:18 step:16960 [D loss: 0.656775, acc.: 57.03%] [G loss: 0.969270]\n",
      "epoch:18 step:16961 [D loss: 0.670263, acc.: 56.25%] [G loss: 1.006561]\n",
      "epoch:18 step:16962 [D loss: 0.461614, acc.: 85.94%] [G loss: 1.216514]\n",
      "epoch:18 step:16963 [D loss: 0.497386, acc.: 86.72%] [G loss: 1.103128]\n",
      "epoch:18 step:16964 [D loss: 0.639624, acc.: 62.50%] [G loss: 0.904365]\n",
      "epoch:18 step:16965 [D loss: 0.682266, acc.: 50.78%] [G loss: 1.319595]\n",
      "epoch:18 step:16966 [D loss: 0.628812, acc.: 62.50%] [G loss: 0.834649]\n",
      "epoch:18 step:16967 [D loss: 0.730708, acc.: 54.69%] [G loss: 1.277160]\n",
      "epoch:18 step:16968 [D loss: 0.643559, acc.: 65.62%] [G loss: 1.140737]\n",
      "epoch:18 step:16969 [D loss: 0.608005, acc.: 65.62%] [G loss: 0.918020]\n",
      "epoch:18 step:16970 [D loss: 0.557885, acc.: 68.75%] [G loss: 0.916363]\n",
      "epoch:18 step:16971 [D loss: 0.791674, acc.: 43.75%] [G loss: 1.256202]\n",
      "epoch:18 step:16972 [D loss: 0.612280, acc.: 66.41%] [G loss: 0.808155]\n",
      "epoch:18 step:16973 [D loss: 0.543437, acc.: 78.12%] [G loss: 0.745040]\n",
      "epoch:18 step:16974 [D loss: 0.554807, acc.: 71.88%] [G loss: 1.173933]\n",
      "epoch:18 step:16975 [D loss: 0.839809, acc.: 42.97%] [G loss: 0.983275]\n",
      "epoch:18 step:16976 [D loss: 0.624570, acc.: 60.94%] [G loss: 1.063018]\n",
      "epoch:18 step:16977 [D loss: 0.509660, acc.: 75.00%] [G loss: 1.442597]\n",
      "epoch:18 step:16978 [D loss: 0.692557, acc.: 57.03%] [G loss: 0.882723]\n",
      "epoch:18 step:16979 [D loss: 0.630402, acc.: 65.62%] [G loss: 1.064540]\n",
      "epoch:18 step:16980 [D loss: 0.500658, acc.: 78.91%] [G loss: 0.828504]\n",
      "epoch:18 step:16981 [D loss: 0.496273, acc.: 78.91%] [G loss: 0.921232]\n",
      "epoch:18 step:16982 [D loss: 0.599383, acc.: 69.53%] [G loss: 1.118814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16983 [D loss: 0.657641, acc.: 60.16%] [G loss: 0.721553]\n",
      "epoch:18 step:16984 [D loss: 0.564886, acc.: 72.66%] [G loss: 1.411649]\n",
      "epoch:18 step:16985 [D loss: 0.578970, acc.: 72.66%] [G loss: 1.809025]\n",
      "epoch:18 step:16986 [D loss: 0.775291, acc.: 50.00%] [G loss: 0.984012]\n",
      "epoch:18 step:16987 [D loss: 0.633743, acc.: 59.38%] [G loss: 1.145440]\n",
      "epoch:18 step:16988 [D loss: 0.786736, acc.: 53.12%] [G loss: 0.933564]\n",
      "epoch:18 step:16989 [D loss: 0.675344, acc.: 55.47%] [G loss: 1.212946]\n",
      "epoch:18 step:16990 [D loss: 0.691021, acc.: 56.25%] [G loss: 1.186488]\n",
      "epoch:18 step:16991 [D loss: 0.521383, acc.: 80.47%] [G loss: 0.780103]\n",
      "epoch:18 step:16992 [D loss: 0.698125, acc.: 54.69%] [G loss: 0.761734]\n",
      "epoch:18 step:16993 [D loss: 0.647012, acc.: 62.50%] [G loss: 0.784688]\n",
      "epoch:18 step:16994 [D loss: 0.566272, acc.: 71.09%] [G loss: 0.768752]\n",
      "epoch:18 step:16995 [D loss: 0.595835, acc.: 70.31%] [G loss: 0.939480]\n",
      "epoch:18 step:16996 [D loss: 0.619256, acc.: 64.06%] [G loss: 0.826081]\n",
      "epoch:18 step:16997 [D loss: 0.603595, acc.: 63.28%] [G loss: 0.807171]\n",
      "epoch:18 step:16998 [D loss: 0.861370, acc.: 40.62%] [G loss: 1.106464]\n",
      "epoch:18 step:16999 [D loss: 0.598221, acc.: 72.66%] [G loss: 0.829578]\n",
      "epoch:18 step:17000 [D loss: 0.735288, acc.: 53.12%] [G loss: 0.749261]\n",
      "epoch:18 step:17001 [D loss: 0.746060, acc.: 51.56%] [G loss: 0.932341]\n",
      "epoch:18 step:17002 [D loss: 0.671998, acc.: 57.03%] [G loss: 1.067735]\n",
      "epoch:18 step:17003 [D loss: 0.705684, acc.: 53.91%] [G loss: 0.948598]\n",
      "epoch:18 step:17004 [D loss: 0.735316, acc.: 49.22%] [G loss: 1.007860]\n",
      "epoch:18 step:17005 [D loss: 0.675728, acc.: 57.81%] [G loss: 0.991068]\n",
      "epoch:18 step:17006 [D loss: 0.634968, acc.: 65.62%] [G loss: 0.946136]\n",
      "epoch:18 step:17007 [D loss: 0.753754, acc.: 46.88%] [G loss: 0.833885]\n",
      "epoch:18 step:17008 [D loss: 0.618373, acc.: 65.62%] [G loss: 0.820024]\n",
      "epoch:18 step:17009 [D loss: 0.632258, acc.: 62.50%] [G loss: 0.956985]\n",
      "epoch:18 step:17010 [D loss: 0.705302, acc.: 56.25%] [G loss: 0.999973]\n",
      "epoch:18 step:17011 [D loss: 0.581527, acc.: 72.66%] [G loss: 0.797115]\n",
      "epoch:18 step:17012 [D loss: 0.717055, acc.: 51.56%] [G loss: 0.809471]\n",
      "epoch:18 step:17013 [D loss: 0.655605, acc.: 56.25%] [G loss: 0.952308]\n",
      "epoch:18 step:17014 [D loss: 0.646070, acc.: 60.16%] [G loss: 0.918992]\n",
      "epoch:18 step:17015 [D loss: 0.700582, acc.: 50.00%] [G loss: 0.836872]\n",
      "epoch:18 step:17016 [D loss: 0.668594, acc.: 60.94%] [G loss: 0.934000]\n",
      "epoch:18 step:17017 [D loss: 0.692765, acc.: 53.12%] [G loss: 1.030036]\n",
      "epoch:18 step:17018 [D loss: 0.743900, acc.: 53.12%] [G loss: 0.993090]\n",
      "epoch:18 step:17019 [D loss: 0.572467, acc.: 74.22%] [G loss: 0.953797]\n",
      "epoch:18 step:17020 [D loss: 0.645835, acc.: 60.16%] [G loss: 0.951954]\n",
      "epoch:18 step:17021 [D loss: 0.651404, acc.: 67.97%] [G loss: 0.839018]\n",
      "epoch:18 step:17022 [D loss: 0.574633, acc.: 70.31%] [G loss: 1.016357]\n",
      "epoch:18 step:17023 [D loss: 0.690440, acc.: 59.38%] [G loss: 1.063948]\n",
      "epoch:18 step:17024 [D loss: 0.764146, acc.: 46.88%] [G loss: 0.901725]\n",
      "epoch:18 step:17025 [D loss: 0.668564, acc.: 57.03%] [G loss: 0.858501]\n",
      "epoch:18 step:17026 [D loss: 0.560556, acc.: 73.44%] [G loss: 2.772120]\n",
      "epoch:18 step:17027 [D loss: 0.787006, acc.: 45.31%] [G loss: 0.855612]\n",
      "epoch:18 step:17028 [D loss: 0.659549, acc.: 55.47%] [G loss: 0.882968]\n",
      "epoch:18 step:17029 [D loss: 0.658246, acc.: 56.25%] [G loss: 0.890703]\n",
      "epoch:18 step:17030 [D loss: 0.624805, acc.: 64.84%] [G loss: 0.915430]\n",
      "epoch:18 step:17031 [D loss: 0.597917, acc.: 66.41%] [G loss: 0.957121]\n",
      "epoch:18 step:17032 [D loss: 0.618806, acc.: 65.62%] [G loss: 0.922934]\n",
      "epoch:18 step:17033 [D loss: 0.669667, acc.: 55.47%] [G loss: 0.914867]\n",
      "epoch:18 step:17034 [D loss: 0.494374, acc.: 82.81%] [G loss: 0.947971]\n",
      "epoch:18 step:17035 [D loss: 0.633971, acc.: 64.84%] [G loss: 0.870928]\n",
      "epoch:18 step:17036 [D loss: 0.500494, acc.: 78.91%] [G loss: 0.827205]\n",
      "epoch:18 step:17037 [D loss: 0.613876, acc.: 68.75%] [G loss: 0.891232]\n",
      "epoch:18 step:17038 [D loss: 0.603229, acc.: 67.97%] [G loss: 0.759323]\n",
      "epoch:18 step:17039 [D loss: 0.560603, acc.: 78.12%] [G loss: 0.728109]\n",
      "epoch:18 step:17040 [D loss: 0.643506, acc.: 60.94%] [G loss: 0.853833]\n",
      "epoch:18 step:17041 [D loss: 0.875639, acc.: 30.47%] [G loss: 1.159932]\n",
      "epoch:18 step:17042 [D loss: 0.656059, acc.: 60.94%] [G loss: 0.788604]\n",
      "epoch:18 step:17043 [D loss: 0.596022, acc.: 71.88%] [G loss: 0.852363]\n",
      "epoch:18 step:17044 [D loss: 0.654107, acc.: 63.28%] [G loss: 1.099152]\n",
      "epoch:18 step:17045 [D loss: 0.612772, acc.: 69.53%] [G loss: 1.299577]\n",
      "epoch:18 step:17046 [D loss: 0.620963, acc.: 62.50%] [G loss: 1.038643]\n",
      "epoch:18 step:17047 [D loss: 0.585363, acc.: 71.88%] [G loss: 1.284141]\n",
      "epoch:18 step:17048 [D loss: 0.589152, acc.: 69.53%] [G loss: 0.976086]\n",
      "epoch:18 step:17049 [D loss: 0.652778, acc.: 62.50%] [G loss: 1.484779]\n",
      "epoch:18 step:17050 [D loss: 0.685520, acc.: 60.16%] [G loss: 1.052280]\n",
      "epoch:18 step:17051 [D loss: 0.595464, acc.: 70.31%] [G loss: 1.094394]\n",
      "epoch:18 step:17052 [D loss: 0.681193, acc.: 53.12%] [G loss: 0.905017]\n",
      "epoch:18 step:17053 [D loss: 0.573042, acc.: 70.31%] [G loss: 1.085614]\n",
      "epoch:18 step:17054 [D loss: 0.629652, acc.: 61.72%] [G loss: 0.904586]\n",
      "epoch:18 step:17055 [D loss: 0.581945, acc.: 73.44%] [G loss: 0.815284]\n",
      "epoch:18 step:17056 [D loss: 0.710452, acc.: 49.22%] [G loss: 0.971725]\n",
      "epoch:18 step:17057 [D loss: 0.547847, acc.: 72.66%] [G loss: 1.005604]\n",
      "epoch:18 step:17058 [D loss: 0.533515, acc.: 75.00%] [G loss: 0.987134]\n",
      "epoch:18 step:17059 [D loss: 0.600318, acc.: 67.19%] [G loss: 1.012621]\n",
      "epoch:18 step:17060 [D loss: 0.774733, acc.: 49.22%] [G loss: 1.324088]\n",
      "epoch:18 step:17061 [D loss: 0.601066, acc.: 70.31%] [G loss: 0.953298]\n",
      "epoch:18 step:17062 [D loss: 0.631932, acc.: 69.53%] [G loss: 1.080716]\n",
      "epoch:18 step:17063 [D loss: 0.648500, acc.: 56.25%] [G loss: 1.079658]\n",
      "epoch:18 step:17064 [D loss: 0.711054, acc.: 55.47%] [G loss: 0.794250]\n",
      "epoch:18 step:17065 [D loss: 0.750526, acc.: 48.44%] [G loss: 0.834250]\n",
      "epoch:18 step:17066 [D loss: 0.664170, acc.: 59.38%] [G loss: 0.934033]\n",
      "epoch:18 step:17067 [D loss: 0.696382, acc.: 53.12%] [G loss: 0.841971]\n",
      "epoch:18 step:17068 [D loss: 0.639701, acc.: 65.62%] [G loss: 0.881842]\n",
      "epoch:18 step:17069 [D loss: 0.599861, acc.: 67.19%] [G loss: 1.127743]\n",
      "epoch:18 step:17070 [D loss: 0.508164, acc.: 82.03%] [G loss: 1.375571]\n",
      "epoch:18 step:17071 [D loss: 0.533523, acc.: 84.38%] [G loss: 0.858351]\n",
      "epoch:18 step:17072 [D loss: 0.787328, acc.: 44.53%] [G loss: 1.281169]\n",
      "epoch:18 step:17073 [D loss: 0.475977, acc.: 80.47%] [G loss: 1.008864]\n",
      "epoch:18 step:17074 [D loss: 0.620325, acc.: 63.28%] [G loss: 1.230877]\n",
      "epoch:18 step:17075 [D loss: 0.736209, acc.: 50.78%] [G loss: 0.983701]\n",
      "epoch:18 step:17076 [D loss: 0.606167, acc.: 68.75%] [G loss: 1.063025]\n",
      "epoch:18 step:17077 [D loss: 0.639353, acc.: 64.06%] [G loss: 1.020874]\n",
      "epoch:18 step:17078 [D loss: 0.826663, acc.: 48.44%] [G loss: 0.919613]\n",
      "epoch:18 step:17079 [D loss: 0.666147, acc.: 59.38%] [G loss: 0.867953]\n",
      "epoch:18 step:17080 [D loss: 0.748110, acc.: 48.44%] [G loss: 1.040640]\n",
      "epoch:18 step:17081 [D loss: 0.613346, acc.: 65.62%] [G loss: 1.163808]\n",
      "epoch:18 step:17082 [D loss: 0.646478, acc.: 61.72%] [G loss: 0.986995]\n",
      "epoch:18 step:17083 [D loss: 0.623145, acc.: 61.72%] [G loss: 0.930199]\n",
      "epoch:18 step:17084 [D loss: 0.640279, acc.: 61.72%] [G loss: 1.149406]\n",
      "epoch:18 step:17085 [D loss: 0.599854, acc.: 71.09%] [G loss: 0.865823]\n",
      "epoch:18 step:17086 [D loss: 0.651215, acc.: 67.19%] [G loss: 1.142329]\n",
      "epoch:18 step:17087 [D loss: 0.497994, acc.: 79.69%] [G loss: 0.953984]\n",
      "epoch:18 step:17088 [D loss: 0.717926, acc.: 48.44%] [G loss: 0.793261]\n",
      "epoch:18 step:17089 [D loss: 0.672808, acc.: 62.50%] [G loss: 0.998963]\n",
      "epoch:18 step:17090 [D loss: 0.531784, acc.: 76.56%] [G loss: 0.964537]\n",
      "epoch:18 step:17091 [D loss: 0.690803, acc.: 55.47%] [G loss: 1.000570]\n",
      "epoch:18 step:17092 [D loss: 0.621913, acc.: 62.50%] [G loss: 0.990729]\n",
      "epoch:18 step:17093 [D loss: 0.591830, acc.: 70.31%] [G loss: 0.898730]\n",
      "epoch:18 step:17094 [D loss: 0.652246, acc.: 59.38%] [G loss: 1.139999]\n",
      "epoch:18 step:17095 [D loss: 0.785704, acc.: 42.19%] [G loss: 0.953687]\n",
      "epoch:18 step:17096 [D loss: 0.468628, acc.: 83.59%] [G loss: 1.045598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17097 [D loss: 0.547545, acc.: 79.69%] [G loss: 0.991727]\n",
      "epoch:18 step:17098 [D loss: 0.684890, acc.: 57.81%] [G loss: 1.083595]\n",
      "epoch:18 step:17099 [D loss: 0.685533, acc.: 60.16%] [G loss: 1.024584]\n",
      "epoch:18 step:17100 [D loss: 0.827938, acc.: 37.50%] [G loss: 1.012293]\n",
      "epoch:18 step:17101 [D loss: 0.618699, acc.: 58.59%] [G loss: 1.034874]\n",
      "epoch:18 step:17102 [D loss: 0.579629, acc.: 75.78%] [G loss: 0.923012]\n",
      "epoch:18 step:17103 [D loss: 0.654062, acc.: 60.94%] [G loss: 0.801383]\n",
      "epoch:18 step:17104 [D loss: 0.651822, acc.: 60.94%] [G loss: 0.995246]\n",
      "epoch:18 step:17105 [D loss: 0.597312, acc.: 70.31%] [G loss: 0.878152]\n",
      "epoch:18 step:17106 [D loss: 0.622661, acc.: 63.28%] [G loss: 0.806260]\n",
      "epoch:18 step:17107 [D loss: 0.571333, acc.: 75.78%] [G loss: 1.049750]\n",
      "epoch:18 step:17108 [D loss: 0.601082, acc.: 67.97%] [G loss: 1.040283]\n",
      "epoch:18 step:17109 [D loss: 0.553584, acc.: 70.31%] [G loss: 1.171450]\n",
      "epoch:18 step:17110 [D loss: 0.621981, acc.: 70.31%] [G loss: 0.957621]\n",
      "epoch:18 step:17111 [D loss: 0.726599, acc.: 51.56%] [G loss: 1.016937]\n",
      "epoch:18 step:17112 [D loss: 0.706840, acc.: 57.03%] [G loss: 0.801396]\n",
      "epoch:18 step:17113 [D loss: 0.662278, acc.: 55.47%] [G loss: 1.088751]\n",
      "epoch:18 step:17114 [D loss: 0.640273, acc.: 63.28%] [G loss: 1.210981]\n",
      "epoch:18 step:17115 [D loss: 0.657251, acc.: 57.03%] [G loss: 1.046541]\n",
      "epoch:18 step:17116 [D loss: 0.698660, acc.: 52.34%] [G loss: 0.876303]\n",
      "epoch:18 step:17117 [D loss: 0.658006, acc.: 65.62%] [G loss: 1.000249]\n",
      "epoch:18 step:17118 [D loss: 0.634020, acc.: 64.06%] [G loss: 0.940875]\n",
      "epoch:18 step:17119 [D loss: 0.625917, acc.: 65.62%] [G loss: 0.933857]\n",
      "epoch:18 step:17120 [D loss: 0.665010, acc.: 60.16%] [G loss: 0.948739]\n",
      "epoch:18 step:17121 [D loss: 0.589136, acc.: 69.53%] [G loss: 1.150931]\n",
      "epoch:18 step:17122 [D loss: 0.640195, acc.: 65.62%] [G loss: 1.022338]\n",
      "epoch:18 step:17123 [D loss: 0.706570, acc.: 50.00%] [G loss: 0.855484]\n",
      "epoch:18 step:17124 [D loss: 0.673057, acc.: 55.47%] [G loss: 1.001016]\n",
      "epoch:18 step:17125 [D loss: 0.587539, acc.: 72.66%] [G loss: 1.018544]\n",
      "epoch:18 step:17126 [D loss: 0.581535, acc.: 72.66%] [G loss: 0.937606]\n",
      "epoch:18 step:17127 [D loss: 0.594838, acc.: 75.78%] [G loss: 1.003107]\n",
      "epoch:18 step:17128 [D loss: 0.610224, acc.: 65.62%] [G loss: 1.022821]\n",
      "epoch:18 step:17129 [D loss: 0.511837, acc.: 82.03%] [G loss: 0.940157]\n",
      "epoch:18 step:17130 [D loss: 0.656371, acc.: 60.16%] [G loss: 0.892540]\n",
      "epoch:18 step:17131 [D loss: 0.523903, acc.: 78.91%] [G loss: 0.794902]\n",
      "epoch:18 step:17132 [D loss: 0.530074, acc.: 76.56%] [G loss: 0.881652]\n",
      "epoch:18 step:17133 [D loss: 0.706580, acc.: 51.56%] [G loss: 1.070753]\n",
      "epoch:18 step:17134 [D loss: 0.716423, acc.: 51.56%] [G loss: 0.741878]\n",
      "epoch:18 step:17135 [D loss: 0.533319, acc.: 74.22%] [G loss: 0.948242]\n",
      "epoch:18 step:17136 [D loss: 0.614631, acc.: 69.53%] [G loss: 0.870306]\n",
      "epoch:18 step:17137 [D loss: 0.640717, acc.: 59.38%] [G loss: 1.004313]\n",
      "epoch:18 step:17138 [D loss: 0.547815, acc.: 79.69%] [G loss: 1.021768]\n",
      "epoch:18 step:17139 [D loss: 0.606470, acc.: 67.19%] [G loss: 1.098583]\n",
      "epoch:18 step:17140 [D loss: 0.506026, acc.: 85.94%] [G loss: 0.998690]\n",
      "epoch:18 step:17141 [D loss: 0.548968, acc.: 71.88%] [G loss: 0.961870]\n",
      "epoch:18 step:17142 [D loss: 0.666453, acc.: 54.69%] [G loss: 0.856680]\n",
      "epoch:18 step:17143 [D loss: 0.697072, acc.: 56.25%] [G loss: 1.214412]\n",
      "epoch:18 step:17144 [D loss: 0.484613, acc.: 71.09%] [G loss: 1.161771]\n",
      "epoch:18 step:17145 [D loss: 0.661039, acc.: 54.69%] [G loss: 1.310906]\n",
      "epoch:18 step:17146 [D loss: 0.637456, acc.: 67.19%] [G loss: 0.819493]\n",
      "epoch:18 step:17147 [D loss: 0.741118, acc.: 52.34%] [G loss: 0.837118]\n",
      "epoch:18 step:17148 [D loss: 0.509676, acc.: 75.00%] [G loss: 1.139921]\n",
      "epoch:18 step:17149 [D loss: 0.626486, acc.: 68.75%] [G loss: 0.882569]\n",
      "epoch:18 step:17150 [D loss: 0.610824, acc.: 66.41%] [G loss: 0.964506]\n",
      "epoch:18 step:17151 [D loss: 0.711519, acc.: 49.22%] [G loss: 1.038987]\n",
      "epoch:18 step:17152 [D loss: 0.723666, acc.: 53.91%] [G loss: 1.048017]\n",
      "epoch:18 step:17153 [D loss: 0.667122, acc.: 54.69%] [G loss: 0.831070]\n",
      "epoch:18 step:17154 [D loss: 0.703105, acc.: 52.34%] [G loss: 1.054703]\n",
      "epoch:18 step:17155 [D loss: 0.626635, acc.: 60.94%] [G loss: 0.838896]\n",
      "epoch:18 step:17156 [D loss: 0.628286, acc.: 67.97%] [G loss: 0.882369]\n",
      "epoch:18 step:17157 [D loss: 0.627916, acc.: 59.38%] [G loss: 0.829262]\n",
      "epoch:18 step:17158 [D loss: 0.613733, acc.: 67.19%] [G loss: 0.980987]\n",
      "epoch:18 step:17159 [D loss: 0.601676, acc.: 67.19%] [G loss: 0.951988]\n",
      "epoch:18 step:17160 [D loss: 0.720538, acc.: 56.25%] [G loss: 0.834030]\n",
      "epoch:18 step:17161 [D loss: 0.665907, acc.: 59.38%] [G loss: 0.934030]\n",
      "epoch:18 step:17162 [D loss: 0.652402, acc.: 60.94%] [G loss: 1.189986]\n",
      "epoch:18 step:17163 [D loss: 0.574830, acc.: 74.22%] [G loss: 1.102930]\n",
      "epoch:18 step:17164 [D loss: 0.750389, acc.: 46.88%] [G loss: 0.899035]\n",
      "epoch:18 step:17165 [D loss: 0.543119, acc.: 78.91%] [G loss: 1.154232]\n",
      "epoch:18 step:17166 [D loss: 0.706974, acc.: 52.34%] [G loss: 1.160137]\n",
      "epoch:18 step:17167 [D loss: 0.678496, acc.: 51.56%] [G loss: 1.023408]\n",
      "epoch:18 step:17168 [D loss: 0.600554, acc.: 67.97%] [G loss: 0.915664]\n",
      "epoch:18 step:17169 [D loss: 0.667385, acc.: 60.16%] [G loss: 0.924624]\n",
      "epoch:18 step:17170 [D loss: 0.605189, acc.: 71.09%] [G loss: 0.838668]\n",
      "epoch:18 step:17171 [D loss: 0.543582, acc.: 79.69%] [G loss: 0.774623]\n",
      "epoch:18 step:17172 [D loss: 0.648120, acc.: 60.16%] [G loss: 0.849418]\n",
      "epoch:18 step:17173 [D loss: 0.735267, acc.: 50.78%] [G loss: 0.864144]\n",
      "epoch:18 step:17174 [D loss: 0.591220, acc.: 70.31%] [G loss: 1.030430]\n",
      "epoch:18 step:17175 [D loss: 0.336631, acc.: 91.41%] [G loss: 0.861615]\n",
      "epoch:18 step:17176 [D loss: 0.529906, acc.: 76.56%] [G loss: 0.815846]\n",
      "epoch:18 step:17177 [D loss: 0.622827, acc.: 64.84%] [G loss: 1.095600]\n",
      "epoch:18 step:17178 [D loss: 0.591905, acc.: 68.75%] [G loss: 1.054235]\n",
      "epoch:18 step:17179 [D loss: 0.576823, acc.: 74.22%] [G loss: 1.083171]\n",
      "epoch:18 step:17180 [D loss: 0.699581, acc.: 53.91%] [G loss: 1.058350]\n",
      "epoch:18 step:17181 [D loss: 0.647896, acc.: 61.72%] [G loss: 1.088755]\n",
      "epoch:18 step:17182 [D loss: 0.588559, acc.: 73.44%] [G loss: 1.117930]\n",
      "epoch:18 step:17183 [D loss: 0.683355, acc.: 56.25%] [G loss: 0.840194]\n",
      "epoch:18 step:17184 [D loss: 0.568172, acc.: 71.88%] [G loss: 0.994441]\n",
      "epoch:18 step:17185 [D loss: 0.749431, acc.: 47.66%] [G loss: 1.009051]\n",
      "epoch:18 step:17186 [D loss: 0.754332, acc.: 50.00%] [G loss: 1.097226]\n",
      "epoch:18 step:17187 [D loss: 0.622050, acc.: 68.75%] [G loss: 1.477488]\n",
      "epoch:18 step:17188 [D loss: 0.564257, acc.: 75.00%] [G loss: 1.074262]\n",
      "epoch:18 step:17189 [D loss: 0.568432, acc.: 75.78%] [G loss: 0.905920]\n",
      "epoch:18 step:17190 [D loss: 0.516580, acc.: 79.69%] [G loss: 0.972482]\n",
      "epoch:18 step:17191 [D loss: 0.617919, acc.: 64.84%] [G loss: 1.251814]\n",
      "epoch:18 step:17192 [D loss: 0.605404, acc.: 67.19%] [G loss: 1.247917]\n",
      "epoch:18 step:17193 [D loss: 0.684114, acc.: 54.69%] [G loss: 1.147725]\n",
      "epoch:18 step:17194 [D loss: 0.593219, acc.: 71.09%] [G loss: 1.053989]\n",
      "epoch:18 step:17195 [D loss: 0.490754, acc.: 86.72%] [G loss: 1.207879]\n",
      "epoch:18 step:17196 [D loss: 0.600586, acc.: 64.06%] [G loss: 0.917944]\n",
      "epoch:18 step:17197 [D loss: 0.475336, acc.: 85.16%] [G loss: 0.859997]\n",
      "epoch:18 step:17198 [D loss: 0.503116, acc.: 82.03%] [G loss: 0.984452]\n",
      "epoch:18 step:17199 [D loss: 0.686418, acc.: 55.47%] [G loss: 1.097132]\n",
      "epoch:18 step:17200 [D loss: 0.840319, acc.: 35.94%] [G loss: 1.065655]\n",
      "epoch:18 step:17201 [D loss: 0.649711, acc.: 60.16%] [G loss: 0.992831]\n",
      "epoch:18 step:17202 [D loss: 0.704969, acc.: 50.00%] [G loss: 1.001472]\n",
      "epoch:18 step:17203 [D loss: 0.824619, acc.: 34.38%] [G loss: 0.759850]\n",
      "epoch:18 step:17204 [D loss: 0.618996, acc.: 66.41%] [G loss: 1.053589]\n",
      "epoch:18 step:17205 [D loss: 0.629719, acc.: 63.28%] [G loss: 0.876494]\n",
      "epoch:18 step:17206 [D loss: 0.789916, acc.: 42.19%] [G loss: 0.974878]\n",
      "epoch:18 step:17207 [D loss: 0.633523, acc.: 60.16%] [G loss: 0.960079]\n",
      "epoch:18 step:17208 [D loss: 0.693866, acc.: 52.34%] [G loss: 1.105355]\n",
      "epoch:18 step:17209 [D loss: 0.659587, acc.: 55.47%] [G loss: 0.986647]\n",
      "epoch:18 step:17210 [D loss: 0.560753, acc.: 71.88%] [G loss: 1.006939]\n",
      "epoch:18 step:17211 [D loss: 0.576084, acc.: 71.09%] [G loss: 0.969610]\n",
      "epoch:18 step:17212 [D loss: 0.582113, acc.: 71.88%] [G loss: 0.920064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17213 [D loss: 0.679546, acc.: 58.59%] [G loss: 0.970419]\n",
      "epoch:18 step:17214 [D loss: 0.645195, acc.: 63.28%] [G loss: 1.005004]\n",
      "epoch:18 step:17215 [D loss: 0.579284, acc.: 65.62%] [G loss: 1.038172]\n",
      "epoch:18 step:17216 [D loss: 0.625157, acc.: 64.06%] [G loss: 1.206839]\n",
      "epoch:18 step:17217 [D loss: 0.656338, acc.: 60.94%] [G loss: 1.030529]\n",
      "epoch:18 step:17218 [D loss: 0.590249, acc.: 64.84%] [G loss: 0.860984]\n",
      "epoch:18 step:17219 [D loss: 0.300718, acc.: 85.94%] [G loss: 0.845838]\n",
      "epoch:18 step:17220 [D loss: 0.706902, acc.: 60.94%] [G loss: 0.866254]\n",
      "epoch:18 step:17221 [D loss: 0.647285, acc.: 64.84%] [G loss: 0.858803]\n",
      "epoch:18 step:17222 [D loss: 0.719624, acc.: 53.91%] [G loss: 0.798661]\n",
      "epoch:18 step:17223 [D loss: 0.784400, acc.: 45.31%] [G loss: 1.083197]\n",
      "epoch:18 step:17224 [D loss: 0.641469, acc.: 61.72%] [G loss: 1.016570]\n",
      "epoch:18 step:17225 [D loss: 0.792406, acc.: 55.47%] [G loss: 1.104111]\n",
      "epoch:18 step:17226 [D loss: 0.687285, acc.: 54.69%] [G loss: 1.007887]\n",
      "epoch:18 step:17227 [D loss: 0.633012, acc.: 64.84%] [G loss: 1.121159]\n",
      "epoch:18 step:17228 [D loss: 0.615450, acc.: 64.84%] [G loss: 1.056355]\n",
      "epoch:18 step:17229 [D loss: 0.682573, acc.: 57.03%] [G loss: 0.800729]\n",
      "epoch:18 step:17230 [D loss: 0.579088, acc.: 73.44%] [G loss: 0.764777]\n",
      "epoch:18 step:17231 [D loss: 0.578334, acc.: 67.19%] [G loss: 0.936393]\n",
      "epoch:18 step:17232 [D loss: 0.659636, acc.: 53.12%] [G loss: 1.038221]\n",
      "epoch:18 step:17233 [D loss: 0.652519, acc.: 59.38%] [G loss: 0.863990]\n",
      "epoch:18 step:17234 [D loss: 0.571832, acc.: 75.00%] [G loss: 0.848123]\n",
      "epoch:18 step:17235 [D loss: 0.720302, acc.: 53.12%] [G loss: 0.897645]\n",
      "epoch:18 step:17236 [D loss: 0.549109, acc.: 76.56%] [G loss: 1.115261]\n",
      "epoch:18 step:17237 [D loss: 0.631719, acc.: 60.94%] [G loss: 0.921604]\n",
      "epoch:18 step:17238 [D loss: 0.502122, acc.: 82.03%] [G loss: 0.961539]\n",
      "epoch:18 step:17239 [D loss: 0.634937, acc.: 68.75%] [G loss: 1.325207]\n",
      "epoch:18 step:17240 [D loss: 0.765910, acc.: 48.44%] [G loss: 1.046305]\n",
      "epoch:18 step:17241 [D loss: 0.662851, acc.: 59.38%] [G loss: 0.805274]\n",
      "epoch:18 step:17242 [D loss: 0.642011, acc.: 58.59%] [G loss: 0.893811]\n",
      "epoch:18 step:17243 [D loss: 0.649554, acc.: 60.16%] [G loss: 0.977984]\n",
      "epoch:18 step:17244 [D loss: 0.589247, acc.: 73.44%] [G loss: 1.043980]\n",
      "epoch:18 step:17245 [D loss: 0.706438, acc.: 55.47%] [G loss: 0.842292]\n",
      "epoch:18 step:17246 [D loss: 0.746581, acc.: 50.00%] [G loss: 0.829978]\n",
      "epoch:18 step:17247 [D loss: 0.628619, acc.: 64.84%] [G loss: 0.925873]\n",
      "epoch:18 step:17248 [D loss: 0.528222, acc.: 79.69%] [G loss: 0.840226]\n",
      "epoch:18 step:17249 [D loss: 0.675234, acc.: 54.69%] [G loss: 0.982148]\n",
      "epoch:18 step:17250 [D loss: 0.628100, acc.: 62.50%] [G loss: 0.925447]\n",
      "epoch:18 step:17251 [D loss: 0.686153, acc.: 57.03%] [G loss: 1.034806]\n",
      "epoch:18 step:17252 [D loss: 0.709103, acc.: 49.22%] [G loss: 0.949724]\n",
      "epoch:18 step:17253 [D loss: 0.571281, acc.: 74.22%] [G loss: 1.057154]\n",
      "epoch:18 step:17254 [D loss: 0.689321, acc.: 57.81%] [G loss: 0.840869]\n",
      "epoch:18 step:17255 [D loss: 0.706862, acc.: 53.12%] [G loss: 0.744912]\n",
      "epoch:18 step:17256 [D loss: 0.514264, acc.: 79.69%] [G loss: 1.043142]\n",
      "epoch:18 step:17257 [D loss: 0.633378, acc.: 64.06%] [G loss: 1.015625]\n",
      "epoch:18 step:17258 [D loss: 0.556519, acc.: 75.00%] [G loss: 0.935611]\n",
      "epoch:18 step:17259 [D loss: 0.516246, acc.: 81.25%] [G loss: 1.044836]\n",
      "epoch:18 step:17260 [D loss: 0.545723, acc.: 76.56%] [G loss: 0.859125]\n",
      "epoch:18 step:17261 [D loss: 0.622200, acc.: 68.75%] [G loss: 1.004982]\n",
      "epoch:18 step:17262 [D loss: 0.532396, acc.: 81.25%] [G loss: 0.845393]\n",
      "epoch:18 step:17263 [D loss: 0.598538, acc.: 70.31%] [G loss: 0.804669]\n",
      "epoch:18 step:17264 [D loss: 0.659473, acc.: 59.38%] [G loss: 0.738043]\n",
      "epoch:18 step:17265 [D loss: 0.694066, acc.: 56.25%] [G loss: 0.809253]\n",
      "epoch:18 step:17266 [D loss: 0.597984, acc.: 61.72%] [G loss: 1.013650]\n",
      "epoch:18 step:17267 [D loss: 0.641391, acc.: 62.50%] [G loss: 1.223638]\n",
      "epoch:18 step:17268 [D loss: 0.545102, acc.: 77.34%] [G loss: 1.547637]\n",
      "epoch:18 step:17269 [D loss: 0.645336, acc.: 59.38%] [G loss: 1.688254]\n",
      "epoch:18 step:17270 [D loss: 0.616005, acc.: 64.06%] [G loss: 0.913933]\n",
      "epoch:18 step:17271 [D loss: 0.692088, acc.: 52.34%] [G loss: 0.937610]\n",
      "epoch:18 step:17272 [D loss: 0.599748, acc.: 61.72%] [G loss: 1.133371]\n",
      "epoch:18 step:17273 [D loss: 0.736954, acc.: 53.91%] [G loss: 1.201341]\n",
      "epoch:18 step:17274 [D loss: 0.538048, acc.: 80.47%] [G loss: 1.172408]\n",
      "epoch:18 step:17275 [D loss: 0.586818, acc.: 67.19%] [G loss: 1.126693]\n",
      "epoch:18 step:17276 [D loss: 0.677831, acc.: 60.94%] [G loss: 1.126197]\n",
      "epoch:18 step:17277 [D loss: 0.621572, acc.: 64.84%] [G loss: 0.955592]\n",
      "epoch:18 step:17278 [D loss: 0.755125, acc.: 44.53%] [G loss: 0.947578]\n",
      "epoch:18 step:17279 [D loss: 0.777149, acc.: 59.38%] [G loss: 1.281535]\n",
      "epoch:18 step:17280 [D loss: 0.632873, acc.: 57.03%] [G loss: 1.176953]\n",
      "epoch:18 step:17281 [D loss: 0.740477, acc.: 46.88%] [G loss: 0.883732]\n",
      "epoch:18 step:17282 [D loss: 0.672456, acc.: 52.34%] [G loss: 0.863105]\n",
      "epoch:18 step:17283 [D loss: 0.668185, acc.: 60.16%] [G loss: 1.093384]\n",
      "epoch:18 step:17284 [D loss: 0.635996, acc.: 59.38%] [G loss: 0.924496]\n",
      "epoch:18 step:17285 [D loss: 0.756657, acc.: 43.75%] [G loss: 1.207679]\n",
      "epoch:18 step:17286 [D loss: 0.588563, acc.: 75.00%] [G loss: 1.115723]\n",
      "epoch:18 step:17287 [D loss: 0.662247, acc.: 57.81%] [G loss: 0.866201]\n",
      "epoch:18 step:17288 [D loss: 0.682507, acc.: 59.38%] [G loss: 0.894128]\n",
      "epoch:18 step:17289 [D loss: 0.704120, acc.: 57.03%] [G loss: 0.810412]\n",
      "epoch:18 step:17290 [D loss: 0.695263, acc.: 57.81%] [G loss: 0.771699]\n",
      "epoch:18 step:17291 [D loss: 0.650010, acc.: 60.16%] [G loss: 0.833649]\n",
      "epoch:18 step:17292 [D loss: 0.617459, acc.: 65.62%] [G loss: 0.879918]\n",
      "epoch:18 step:17293 [D loss: 0.583556, acc.: 69.53%] [G loss: 1.045082]\n",
      "epoch:18 step:17294 [D loss: 0.556899, acc.: 70.31%] [G loss: 1.251777]\n",
      "epoch:18 step:17295 [D loss: 0.765118, acc.: 44.53%] [G loss: 1.193456]\n",
      "epoch:18 step:17296 [D loss: 0.646424, acc.: 58.59%] [G loss: 0.933993]\n",
      "epoch:18 step:17297 [D loss: 0.681825, acc.: 51.56%] [G loss: 0.987241]\n",
      "epoch:18 step:17298 [D loss: 0.694311, acc.: 52.34%] [G loss: 0.889368]\n",
      "epoch:18 step:17299 [D loss: 0.692073, acc.: 55.47%] [G loss: 0.745438]\n",
      "epoch:18 step:17300 [D loss: 0.588432, acc.: 75.00%] [G loss: 0.865785]\n",
      "epoch:18 step:17301 [D loss: 0.624310, acc.: 64.06%] [G loss: 1.007623]\n",
      "epoch:18 step:17302 [D loss: 0.645736, acc.: 64.06%] [G loss: 0.936012]\n",
      "epoch:18 step:17303 [D loss: 0.657664, acc.: 57.03%] [G loss: 0.814957]\n",
      "epoch:18 step:17304 [D loss: 0.577346, acc.: 76.56%] [G loss: 1.033689]\n",
      "epoch:18 step:17305 [D loss: 0.690842, acc.: 56.25%] [G loss: 1.007192]\n",
      "epoch:18 step:17306 [D loss: 0.618240, acc.: 67.19%] [G loss: 0.897546]\n",
      "epoch:18 step:17307 [D loss: 0.650743, acc.: 57.81%] [G loss: 1.027710]\n",
      "epoch:18 step:17308 [D loss: 0.652843, acc.: 60.94%] [G loss: 1.006725]\n",
      "epoch:18 step:17309 [D loss: 0.628710, acc.: 64.06%] [G loss: 1.032312]\n",
      "epoch:18 step:17310 [D loss: 0.584928, acc.: 72.66%] [G loss: 0.856173]\n",
      "epoch:18 step:17311 [D loss: 0.548302, acc.: 78.91%] [G loss: 0.941333]\n",
      "epoch:18 step:17312 [D loss: 0.669895, acc.: 58.59%] [G loss: 0.993059]\n",
      "epoch:18 step:17313 [D loss: 0.607432, acc.: 70.31%] [G loss: 1.107268]\n",
      "epoch:18 step:17314 [D loss: 0.724397, acc.: 50.00%] [G loss: 1.014184]\n",
      "epoch:18 step:17315 [D loss: 0.636619, acc.: 65.62%] [G loss: 1.152477]\n",
      "epoch:18 step:17316 [D loss: 0.611957, acc.: 70.31%] [G loss: 0.888905]\n",
      "epoch:18 step:17317 [D loss: 0.621275, acc.: 60.94%] [G loss: 0.981867]\n",
      "epoch:18 step:17318 [D loss: 0.700603, acc.: 53.12%] [G loss: 0.794680]\n",
      "epoch:18 step:17319 [D loss: 0.632713, acc.: 64.06%] [G loss: 0.849868]\n",
      "epoch:18 step:17320 [D loss: 0.579046, acc.: 76.56%] [G loss: 0.989486]\n",
      "epoch:18 step:17321 [D loss: 0.712768, acc.: 45.31%] [G loss: 1.140671]\n",
      "epoch:18 step:17322 [D loss: 0.616228, acc.: 67.19%] [G loss: 1.080185]\n",
      "epoch:18 step:17323 [D loss: 0.559992, acc.: 76.56%] [G loss: 0.853738]\n",
      "epoch:18 step:17324 [D loss: 0.611919, acc.: 64.06%] [G loss: 0.973073]\n",
      "epoch:18 step:17325 [D loss: 0.519453, acc.: 82.03%] [G loss: 0.995405]\n",
      "epoch:18 step:17326 [D loss: 0.468394, acc.: 85.16%] [G loss: 0.951354]\n",
      "epoch:18 step:17327 [D loss: 0.511644, acc.: 77.34%] [G loss: 0.950609]\n",
      "epoch:18 step:17328 [D loss: 0.597069, acc.: 65.62%] [G loss: 0.867260]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17329 [D loss: 0.715450, acc.: 56.25%] [G loss: 1.177843]\n",
      "epoch:18 step:17330 [D loss: 0.474141, acc.: 82.03%] [G loss: 1.183076]\n",
      "epoch:18 step:17331 [D loss: 0.601090, acc.: 69.53%] [G loss: 1.255388]\n",
      "epoch:18 step:17332 [D loss: 0.574538, acc.: 68.75%] [G loss: 1.008669]\n",
      "epoch:18 step:17333 [D loss: 0.509259, acc.: 83.59%] [G loss: 1.522121]\n",
      "epoch:18 step:17334 [D loss: 0.655921, acc.: 58.59%] [G loss: 1.063701]\n",
      "epoch:18 step:17335 [D loss: 0.646398, acc.: 55.47%] [G loss: 1.202697]\n",
      "epoch:18 step:17336 [D loss: 0.553413, acc.: 75.78%] [G loss: 1.139783]\n",
      "epoch:18 step:17337 [D loss: 0.606884, acc.: 68.75%] [G loss: 1.002264]\n",
      "epoch:18 step:17338 [D loss: 0.704921, acc.: 53.91%] [G loss: 1.141653]\n",
      "epoch:18 step:17339 [D loss: 0.492320, acc.: 83.59%] [G loss: 1.168450]\n",
      "epoch:18 step:17340 [D loss: 0.535977, acc.: 75.00%] [G loss: 1.070693]\n",
      "epoch:18 step:17341 [D loss: 0.718758, acc.: 46.88%] [G loss: 1.466076]\n",
      "epoch:18 step:17342 [D loss: 0.614110, acc.: 65.62%] [G loss: 1.135208]\n",
      "epoch:18 step:17343 [D loss: 0.614067, acc.: 67.19%] [G loss: 0.977012]\n",
      "epoch:18 step:17344 [D loss: 0.604326, acc.: 67.97%] [G loss: 0.848861]\n",
      "epoch:18 step:17345 [D loss: 0.707627, acc.: 48.44%] [G loss: 1.010455]\n",
      "epoch:18 step:17346 [D loss: 0.646576, acc.: 59.38%] [G loss: 1.044085]\n",
      "epoch:18 step:17347 [D loss: 0.672512, acc.: 56.25%] [G loss: 0.912295]\n",
      "epoch:18 step:17348 [D loss: 0.442017, acc.: 88.28%] [G loss: 1.427418]\n",
      "epoch:18 step:17349 [D loss: 0.539995, acc.: 76.56%] [G loss: 0.817311]\n",
      "epoch:18 step:17350 [D loss: 0.598662, acc.: 64.06%] [G loss: 1.396193]\n",
      "epoch:18 step:17351 [D loss: 0.628655, acc.: 60.16%] [G loss: 3.283518]\n",
      "epoch:18 step:17352 [D loss: 0.569620, acc.: 77.34%] [G loss: 1.545140]\n",
      "epoch:18 step:17353 [D loss: 0.735956, acc.: 49.22%] [G loss: 1.342315]\n",
      "epoch:18 step:17354 [D loss: 0.685539, acc.: 59.38%] [G loss: 0.851868]\n",
      "epoch:18 step:17355 [D loss: 0.822189, acc.: 39.06%] [G loss: 0.932470]\n",
      "epoch:18 step:17356 [D loss: 0.548387, acc.: 73.44%] [G loss: 1.357008]\n",
      "epoch:18 step:17357 [D loss: 1.048228, acc.: 21.88%] [G loss: 0.801584]\n",
      "epoch:18 step:17358 [D loss: 0.664698, acc.: 57.03%] [G loss: 0.870255]\n",
      "epoch:18 step:17359 [D loss: 0.755524, acc.: 41.41%] [G loss: 0.923174]\n",
      "epoch:18 step:17360 [D loss: 0.676350, acc.: 56.25%] [G loss: 1.007598]\n",
      "epoch:18 step:17361 [D loss: 0.477885, acc.: 85.94%] [G loss: 1.171997]\n",
      "epoch:18 step:17362 [D loss: 0.541636, acc.: 78.91%] [G loss: 0.997735]\n",
      "epoch:18 step:17363 [D loss: 0.535774, acc.: 77.34%] [G loss: 0.916153]\n",
      "epoch:18 step:17364 [D loss: 0.549205, acc.: 79.69%] [G loss: 1.021032]\n",
      "epoch:18 step:17365 [D loss: 0.583309, acc.: 67.97%] [G loss: 0.925507]\n",
      "epoch:18 step:17366 [D loss: 0.691042, acc.: 50.78%] [G loss: 0.902788]\n",
      "epoch:18 step:17367 [D loss: 0.758225, acc.: 50.00%] [G loss: 0.918855]\n",
      "epoch:18 step:17368 [D loss: 0.590871, acc.: 70.31%] [G loss: 0.983005]\n",
      "epoch:18 step:17369 [D loss: 0.818706, acc.: 40.62%] [G loss: 0.822341]\n",
      "epoch:18 step:17370 [D loss: 0.617570, acc.: 60.16%] [G loss: 0.991814]\n",
      "epoch:18 step:17371 [D loss: 0.492723, acc.: 79.69%] [G loss: 1.019257]\n",
      "epoch:18 step:17372 [D loss: 0.636380, acc.: 63.28%] [G loss: 0.938728]\n",
      "epoch:18 step:17373 [D loss: 0.720336, acc.: 53.12%] [G loss: 0.991370]\n",
      "epoch:18 step:17374 [D loss: 0.664220, acc.: 62.50%] [G loss: 1.076747]\n",
      "epoch:18 step:17375 [D loss: 0.624998, acc.: 61.72%] [G loss: 1.023449]\n",
      "epoch:18 step:17376 [D loss: 0.660152, acc.: 59.38%] [G loss: 0.944760]\n",
      "epoch:18 step:17377 [D loss: 0.581183, acc.: 71.88%] [G loss: 0.863674]\n",
      "epoch:18 step:17378 [D loss: 0.559277, acc.: 71.88%] [G loss: 1.040629]\n",
      "epoch:18 step:17379 [D loss: 0.606006, acc.: 68.75%] [G loss: 0.998773]\n",
      "epoch:18 step:17380 [D loss: 0.646086, acc.: 62.50%] [G loss: 0.766067]\n",
      "epoch:18 step:17381 [D loss: 0.592422, acc.: 68.75%] [G loss: 1.040630]\n",
      "epoch:18 step:17382 [D loss: 0.641782, acc.: 64.84%] [G loss: 0.936613]\n",
      "epoch:18 step:17383 [D loss: 0.515394, acc.: 78.91%] [G loss: 1.019467]\n",
      "epoch:18 step:17384 [D loss: 0.704251, acc.: 53.12%] [G loss: 1.197326]\n",
      "epoch:18 step:17385 [D loss: 0.566859, acc.: 70.31%] [G loss: 0.778742]\n",
      "epoch:18 step:17386 [D loss: 0.665803, acc.: 60.16%] [G loss: 1.238634]\n",
      "epoch:18 step:17387 [D loss: 0.581396, acc.: 71.88%] [G loss: 0.872731]\n",
      "epoch:18 step:17388 [D loss: 0.639175, acc.: 64.06%] [G loss: 0.939366]\n",
      "epoch:18 step:17389 [D loss: 0.613877, acc.: 67.97%] [G loss: 1.207433]\n",
      "epoch:18 step:17390 [D loss: 0.673632, acc.: 55.47%] [G loss: 1.147116]\n",
      "epoch:18 step:17391 [D loss: 0.627205, acc.: 64.84%] [G loss: 0.801553]\n",
      "epoch:18 step:17392 [D loss: 0.575559, acc.: 67.97%] [G loss: 1.211544]\n",
      "epoch:18 step:17393 [D loss: 0.584420, acc.: 72.66%] [G loss: 1.003026]\n",
      "epoch:18 step:17394 [D loss: 0.706928, acc.: 46.88%] [G loss: 0.979830]\n",
      "epoch:18 step:17395 [D loss: 0.541310, acc.: 76.56%] [G loss: 0.987410]\n",
      "epoch:18 step:17396 [D loss: 0.650888, acc.: 62.50%] [G loss: 1.080111]\n",
      "epoch:18 step:17397 [D loss: 0.629147, acc.: 68.75%] [G loss: 0.982340]\n",
      "epoch:18 step:17398 [D loss: 0.639928, acc.: 60.16%] [G loss: 0.975472]\n",
      "epoch:18 step:17399 [D loss: 0.614072, acc.: 67.19%] [G loss: 1.050372]\n",
      "epoch:18 step:17400 [D loss: 0.614888, acc.: 62.50%] [G loss: 0.832189]\n",
      "epoch:18 step:17401 [D loss: 0.571529, acc.: 72.66%] [G loss: 0.955902]\n",
      "epoch:18 step:17402 [D loss: 0.565295, acc.: 78.12%] [G loss: 1.143096]\n",
      "epoch:18 step:17403 [D loss: 0.606802, acc.: 64.06%] [G loss: 0.979267]\n",
      "epoch:18 step:17404 [D loss: 0.488576, acc.: 79.69%] [G loss: 1.203180]\n",
      "epoch:18 step:17405 [D loss: 0.650811, acc.: 63.28%] [G loss: 1.153768]\n",
      "epoch:18 step:17406 [D loss: 0.686209, acc.: 56.25%] [G loss: 1.506733]\n",
      "epoch:18 step:17407 [D loss: 0.501099, acc.: 79.69%] [G loss: 1.098021]\n",
      "epoch:18 step:17408 [D loss: 0.649604, acc.: 60.16%] [G loss: 0.836427]\n",
      "epoch:18 step:17409 [D loss: 0.443321, acc.: 82.81%] [G loss: 0.969269]\n",
      "epoch:18 step:17410 [D loss: 0.593589, acc.: 67.19%] [G loss: 1.061149]\n",
      "epoch:18 step:17411 [D loss: 0.514173, acc.: 78.91%] [G loss: 0.988037]\n",
      "epoch:18 step:17412 [D loss: 0.665301, acc.: 56.25%] [G loss: 1.033011]\n",
      "epoch:18 step:17413 [D loss: 0.588554, acc.: 66.41%] [G loss: 1.035465]\n",
      "epoch:18 step:17414 [D loss: 0.887955, acc.: 38.28%] [G loss: 0.907078]\n",
      "epoch:18 step:17415 [D loss: 0.612125, acc.: 69.53%] [G loss: 0.980282]\n",
      "epoch:18 step:17416 [D loss: 0.599526, acc.: 66.41%] [G loss: 1.028038]\n",
      "epoch:18 step:17417 [D loss: 0.706868, acc.: 44.53%] [G loss: 0.797256]\n",
      "epoch:18 step:17418 [D loss: 0.627127, acc.: 64.06%] [G loss: 0.808459]\n",
      "epoch:18 step:17419 [D loss: 0.575173, acc.: 72.66%] [G loss: 0.941689]\n",
      "epoch:18 step:17420 [D loss: 0.808684, acc.: 39.84%] [G loss: 0.781601]\n",
      "epoch:18 step:17421 [D loss: 0.649570, acc.: 60.94%] [G loss: 0.963185]\n",
      "epoch:18 step:17422 [D loss: 0.683407, acc.: 50.78%] [G loss: 0.991724]\n",
      "epoch:18 step:17423 [D loss: 0.564133, acc.: 70.31%] [G loss: 1.087766]\n",
      "epoch:18 step:17424 [D loss: 0.536602, acc.: 77.34%] [G loss: 1.113755]\n",
      "epoch:18 step:17425 [D loss: 0.638286, acc.: 63.28%] [G loss: 0.994346]\n",
      "epoch:18 step:17426 [D loss: 0.619664, acc.: 60.94%] [G loss: 0.944985]\n",
      "epoch:18 step:17427 [D loss: 0.649056, acc.: 62.50%] [G loss: 1.308724]\n",
      "epoch:18 step:17428 [D loss: 0.626985, acc.: 59.38%] [G loss: 0.908349]\n",
      "epoch:18 step:17429 [D loss: 0.576372, acc.: 73.44%] [G loss: 1.110560]\n",
      "epoch:18 step:17430 [D loss: 0.706911, acc.: 56.25%] [G loss: 0.917141]\n",
      "epoch:18 step:17431 [D loss: 0.477613, acc.: 86.72%] [G loss: 1.015181]\n",
      "epoch:18 step:17432 [D loss: 0.624430, acc.: 65.62%] [G loss: 0.946097]\n",
      "epoch:18 step:17433 [D loss: 0.719898, acc.: 54.69%] [G loss: 1.297941]\n",
      "epoch:18 step:17434 [D loss: 0.612619, acc.: 60.94%] [G loss: 1.139678]\n",
      "epoch:18 step:17435 [D loss: 0.592638, acc.: 68.75%] [G loss: 1.146342]\n",
      "epoch:18 step:17436 [D loss: 0.641481, acc.: 63.28%] [G loss: 0.886050]\n",
      "epoch:18 step:17437 [D loss: 0.591861, acc.: 68.75%] [G loss: 1.026272]\n",
      "epoch:18 step:17438 [D loss: 0.568347, acc.: 69.53%] [G loss: 1.089722]\n",
      "epoch:18 step:17439 [D loss: 0.589867, acc.: 63.28%] [G loss: 0.952459]\n",
      "epoch:18 step:17440 [D loss: 0.872150, acc.: 36.72%] [G loss: 1.141619]\n",
      "epoch:18 step:17441 [D loss: 0.583042, acc.: 67.97%] [G loss: 0.851219]\n",
      "epoch:18 step:17442 [D loss: 0.590400, acc.: 70.31%] [G loss: 1.031827]\n",
      "epoch:18 step:17443 [D loss: 0.613127, acc.: 69.53%] [G loss: 1.385766]\n",
      "epoch:18 step:17444 [D loss: 0.541203, acc.: 77.34%] [G loss: 1.137496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17445 [D loss: 0.701208, acc.: 56.25%] [G loss: 1.089187]\n",
      "epoch:18 step:17446 [D loss: 0.704567, acc.: 54.69%] [G loss: 0.831314]\n",
      "epoch:18 step:17447 [D loss: 0.632035, acc.: 70.31%] [G loss: 0.981583]\n",
      "epoch:18 step:17448 [D loss: 0.539456, acc.: 77.34%] [G loss: 1.019871]\n",
      "epoch:18 step:17449 [D loss: 0.718442, acc.: 49.22%] [G loss: 0.865024]\n",
      "epoch:18 step:17450 [D loss: 0.564550, acc.: 72.66%] [G loss: 0.861709]\n",
      "epoch:18 step:17451 [D loss: 0.528035, acc.: 75.78%] [G loss: 0.984677]\n",
      "epoch:18 step:17452 [D loss: 0.585337, acc.: 70.31%] [G loss: 1.127611]\n",
      "epoch:18 step:17453 [D loss: 0.792504, acc.: 39.06%] [G loss: 0.960993]\n",
      "epoch:18 step:17454 [D loss: 0.505109, acc.: 76.56%] [G loss: 0.884058]\n",
      "epoch:18 step:17455 [D loss: 0.763259, acc.: 44.53%] [G loss: 1.133039]\n",
      "epoch:18 step:17456 [D loss: 0.670485, acc.: 60.16%] [G loss: 1.143270]\n",
      "epoch:18 step:17457 [D loss: 0.755633, acc.: 52.34%] [G loss: 1.086859]\n",
      "epoch:18 step:17458 [D loss: 0.538488, acc.: 78.12%] [G loss: 0.801820]\n",
      "epoch:18 step:17459 [D loss: 0.649790, acc.: 60.94%] [G loss: 0.968848]\n",
      "epoch:18 step:17460 [D loss: 0.693994, acc.: 57.03%] [G loss: 1.120785]\n",
      "epoch:18 step:17461 [D loss: 0.563237, acc.: 70.31%] [G loss: 0.927235]\n",
      "epoch:18 step:17462 [D loss: 0.525882, acc.: 83.59%] [G loss: 0.909429]\n",
      "epoch:18 step:17463 [D loss: 0.543608, acc.: 75.00%] [G loss: 1.157790]\n",
      "epoch:18 step:17464 [D loss: 0.518309, acc.: 75.00%] [G loss: 0.943305]\n",
      "epoch:18 step:17465 [D loss: 0.778189, acc.: 42.97%] [G loss: 1.003455]\n",
      "epoch:18 step:17466 [D loss: 0.694197, acc.: 57.81%] [G loss: 1.060306]\n",
      "epoch:18 step:17467 [D loss: 0.696854, acc.: 54.69%] [G loss: 1.035345]\n",
      "epoch:18 step:17468 [D loss: 0.625378, acc.: 61.72%] [G loss: 1.360183]\n",
      "epoch:18 step:17469 [D loss: 0.680360, acc.: 63.28%] [G loss: 1.155442]\n",
      "epoch:18 step:17470 [D loss: 0.657777, acc.: 54.69%] [G loss: 0.942012]\n",
      "epoch:18 step:17471 [D loss: 0.491999, acc.: 82.81%] [G loss: 1.142089]\n",
      "epoch:18 step:17472 [D loss: 0.745029, acc.: 48.44%] [G loss: 1.081148]\n",
      "epoch:18 step:17473 [D loss: 0.591474, acc.: 68.75%] [G loss: 1.125412]\n",
      "epoch:18 step:17474 [D loss: 0.792612, acc.: 41.41%] [G loss: 1.160485]\n",
      "epoch:18 step:17475 [D loss: 0.672765, acc.: 60.16%] [G loss: 1.287382]\n",
      "epoch:18 step:17476 [D loss: 0.662153, acc.: 63.28%] [G loss: 0.888162]\n",
      "epoch:18 step:17477 [D loss: 0.675542, acc.: 53.91%] [G loss: 0.857682]\n",
      "epoch:18 step:17478 [D loss: 0.671779, acc.: 63.28%] [G loss: 0.868271]\n",
      "epoch:18 step:17479 [D loss: 0.589564, acc.: 71.09%] [G loss: 1.137889]\n",
      "epoch:18 step:17480 [D loss: 0.689679, acc.: 53.91%] [G loss: 1.365920]\n",
      "epoch:18 step:17481 [D loss: 0.549071, acc.: 78.91%] [G loss: 0.971537]\n",
      "epoch:18 step:17482 [D loss: 0.812179, acc.: 35.94%] [G loss: 0.974667]\n",
      "epoch:18 step:17483 [D loss: 0.675946, acc.: 53.91%] [G loss: 1.130998]\n",
      "epoch:18 step:17484 [D loss: 0.628832, acc.: 65.62%] [G loss: 0.718631]\n",
      "epoch:18 step:17485 [D loss: 0.577397, acc.: 68.75%] [G loss: 1.138620]\n",
      "epoch:18 step:17486 [D loss: 0.652992, acc.: 58.59%] [G loss: 1.278139]\n",
      "epoch:18 step:17487 [D loss: 0.543626, acc.: 73.44%] [G loss: 1.006819]\n",
      "epoch:18 step:17488 [D loss: 0.611678, acc.: 67.97%] [G loss: 1.067808]\n",
      "epoch:18 step:17489 [D loss: 0.852221, acc.: 31.25%] [G loss: 0.881914]\n",
      "epoch:18 step:17490 [D loss: 0.661693, acc.: 50.00%] [G loss: 0.921327]\n",
      "epoch:18 step:17491 [D loss: 0.659237, acc.: 65.62%] [G loss: 0.933952]\n",
      "epoch:18 step:17492 [D loss: 0.733230, acc.: 46.88%] [G loss: 0.715382]\n",
      "epoch:18 step:17493 [D loss: 0.640710, acc.: 60.16%] [G loss: 1.071228]\n",
      "epoch:18 step:17494 [D loss: 0.606092, acc.: 70.31%] [G loss: 0.919118]\n",
      "epoch:18 step:17495 [D loss: 0.632663, acc.: 64.06%] [G loss: 1.176475]\n",
      "epoch:18 step:17496 [D loss: 0.440102, acc.: 89.84%] [G loss: 1.168873]\n",
      "epoch:18 step:17497 [D loss: 0.725664, acc.: 46.09%] [G loss: 0.767917]\n",
      "epoch:18 step:17498 [D loss: 0.567319, acc.: 71.09%] [G loss: 0.897019]\n",
      "epoch:18 step:17499 [D loss: 0.577672, acc.: 73.44%] [G loss: 1.005801]\n",
      "epoch:18 step:17500 [D loss: 0.643986, acc.: 62.50%] [G loss: 0.967884]\n",
      "epoch:18 step:17501 [D loss: 0.584407, acc.: 71.09%] [G loss: 0.906277]\n",
      "epoch:18 step:17502 [D loss: 0.714959, acc.: 52.34%] [G loss: 0.970664]\n",
      "epoch:18 step:17503 [D loss: 0.618073, acc.: 65.62%] [G loss: 1.126370]\n",
      "epoch:18 step:17504 [D loss: 0.623682, acc.: 59.38%] [G loss: 0.979349]\n",
      "epoch:18 step:17505 [D loss: 0.621425, acc.: 67.19%] [G loss: 1.031487]\n",
      "epoch:18 step:17506 [D loss: 0.592608, acc.: 71.88%] [G loss: 0.871908]\n",
      "epoch:18 step:17507 [D loss: 0.652759, acc.: 60.16%] [G loss: 1.094438]\n",
      "epoch:18 step:17508 [D loss: 0.749007, acc.: 50.78%] [G loss: 0.954431]\n",
      "epoch:18 step:17509 [D loss: 0.524399, acc.: 80.47%] [G loss: 0.923296]\n",
      "epoch:18 step:17510 [D loss: 0.601593, acc.: 66.41%] [G loss: 1.106845]\n",
      "epoch:18 step:17511 [D loss: 0.599095, acc.: 68.75%] [G loss: 1.214818]\n",
      "epoch:18 step:17512 [D loss: 0.613517, acc.: 61.72%] [G loss: 0.831892]\n",
      "epoch:18 step:17513 [D loss: 0.667689, acc.: 60.94%] [G loss: 0.885816]\n",
      "epoch:18 step:17514 [D loss: 0.616930, acc.: 64.06%] [G loss: 1.025084]\n",
      "epoch:18 step:17515 [D loss: 0.739678, acc.: 57.03%] [G loss: 1.141434]\n",
      "epoch:18 step:17516 [D loss: 0.614129, acc.: 69.53%] [G loss: 0.929138]\n",
      "epoch:18 step:17517 [D loss: 0.733226, acc.: 53.12%] [G loss: 0.904747]\n",
      "epoch:18 step:17518 [D loss: 0.627388, acc.: 65.62%] [G loss: 1.027052]\n",
      "epoch:18 step:17519 [D loss: 0.718288, acc.: 48.44%] [G loss: 0.801280]\n",
      "epoch:18 step:17520 [D loss: 0.670871, acc.: 60.16%] [G loss: 1.098351]\n",
      "epoch:18 step:17521 [D loss: 0.605664, acc.: 71.09%] [G loss: 0.982880]\n",
      "epoch:18 step:17522 [D loss: 0.600623, acc.: 66.41%] [G loss: 0.905784]\n",
      "epoch:18 step:17523 [D loss: 0.681247, acc.: 58.59%] [G loss: 0.700854]\n",
      "epoch:18 step:17524 [D loss: 0.819676, acc.: 39.84%] [G loss: 0.814906]\n",
      "epoch:18 step:17525 [D loss: 0.579219, acc.: 71.88%] [G loss: 1.028718]\n",
      "epoch:18 step:17526 [D loss: 0.632852, acc.: 74.22%] [G loss: 1.024804]\n",
      "epoch:18 step:17527 [D loss: 0.673844, acc.: 55.47%] [G loss: 1.079295]\n",
      "epoch:18 step:17528 [D loss: 0.660292, acc.: 55.47%] [G loss: 1.086539]\n",
      "epoch:18 step:17529 [D loss: 0.569723, acc.: 70.31%] [G loss: 1.090263]\n",
      "epoch:18 step:17530 [D loss: 0.658332, acc.: 64.06%] [G loss: 0.944187]\n",
      "epoch:18 step:17531 [D loss: 0.508547, acc.: 81.25%] [G loss: 1.025685]\n",
      "epoch:18 step:17532 [D loss: 0.608504, acc.: 67.97%] [G loss: 0.970771]\n",
      "epoch:18 step:17533 [D loss: 0.499856, acc.: 80.47%] [G loss: 1.196472]\n",
      "epoch:18 step:17534 [D loss: 0.515238, acc.: 79.69%] [G loss: 1.090093]\n",
      "epoch:18 step:17535 [D loss: 0.560642, acc.: 72.66%] [G loss: 0.909616]\n",
      "epoch:18 step:17536 [D loss: 0.504695, acc.: 80.47%] [G loss: 1.064364]\n",
      "epoch:18 step:17537 [D loss: 0.570516, acc.: 68.75%] [G loss: 0.997970]\n",
      "epoch:18 step:17538 [D loss: 0.668679, acc.: 57.03%] [G loss: 1.113362]\n",
      "epoch:18 step:17539 [D loss: 0.479951, acc.: 82.03%] [G loss: 1.106617]\n",
      "epoch:18 step:17540 [D loss: 0.566249, acc.: 71.09%] [G loss: 0.870173]\n",
      "epoch:18 step:17541 [D loss: 0.610746, acc.: 65.62%] [G loss: 1.108825]\n",
      "epoch:18 step:17542 [D loss: 0.507779, acc.: 77.34%] [G loss: 1.211675]\n",
      "epoch:18 step:17543 [D loss: 0.672248, acc.: 53.12%] [G loss: 1.165492]\n",
      "epoch:18 step:17544 [D loss: 0.763811, acc.: 48.44%] [G loss: 0.955745]\n",
      "epoch:18 step:17545 [D loss: 0.607281, acc.: 67.97%] [G loss: 1.139786]\n",
      "epoch:18 step:17546 [D loss: 0.735314, acc.: 52.34%] [G loss: 1.071708]\n",
      "epoch:18 step:17547 [D loss: 0.548693, acc.: 75.78%] [G loss: 1.047005]\n",
      "epoch:18 step:17548 [D loss: 0.687741, acc.: 55.47%] [G loss: 0.990462]\n",
      "epoch:18 step:17549 [D loss: 0.859899, acc.: 35.16%] [G loss: 1.008003]\n",
      "epoch:18 step:17550 [D loss: 0.704689, acc.: 58.59%] [G loss: 1.159308]\n",
      "epoch:18 step:17551 [D loss: 0.583231, acc.: 71.09%] [G loss: 0.856236]\n",
      "epoch:18 step:17552 [D loss: 0.640992, acc.: 62.50%] [G loss: 1.000910]\n",
      "epoch:18 step:17553 [D loss: 0.555708, acc.: 72.66%] [G loss: 0.905174]\n",
      "epoch:18 step:17554 [D loss: 0.685997, acc.: 55.47%] [G loss: 1.128280]\n",
      "epoch:18 step:17555 [D loss: 0.604808, acc.: 66.41%] [G loss: 1.295596]\n",
      "epoch:18 step:17556 [D loss: 0.526517, acc.: 74.22%] [G loss: 1.049072]\n",
      "epoch:18 step:17557 [D loss: 0.592991, acc.: 75.78%] [G loss: 0.779776]\n",
      "epoch:18 step:17558 [D loss: 0.449172, acc.: 85.16%] [G loss: 1.132427]\n",
      "epoch:18 step:17559 [D loss: 0.608920, acc.: 64.06%] [G loss: 0.816404]\n",
      "epoch:18 step:17560 [D loss: 0.722124, acc.: 54.69%] [G loss: 1.500764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17561 [D loss: 0.769574, acc.: 46.88%] [G loss: 0.867616]\n",
      "epoch:18 step:17562 [D loss: 0.649862, acc.: 52.34%] [G loss: 0.794549]\n",
      "epoch:18 step:17563 [D loss: 0.661326, acc.: 59.38%] [G loss: 1.061609]\n",
      "epoch:18 step:17564 [D loss: 0.594370, acc.: 67.97%] [G loss: 0.843431]\n",
      "epoch:18 step:17565 [D loss: 0.718958, acc.: 57.03%] [G loss: 1.013356]\n",
      "epoch:18 step:17566 [D loss: 0.651547, acc.: 60.16%] [G loss: 0.820962]\n",
      "epoch:18 step:17567 [D loss: 0.605975, acc.: 72.66%] [G loss: 1.241494]\n",
      "epoch:18 step:17568 [D loss: 0.769784, acc.: 48.44%] [G loss: 0.862682]\n",
      "epoch:18 step:17569 [D loss: 0.742902, acc.: 50.00%] [G loss: 1.057100]\n",
      "epoch:18 step:17570 [D loss: 0.685074, acc.: 55.47%] [G loss: 0.896779]\n",
      "epoch:18 step:17571 [D loss: 0.666156, acc.: 64.06%] [G loss: 0.839271]\n",
      "epoch:18 step:17572 [D loss: 0.658868, acc.: 57.81%] [G loss: 0.912655]\n",
      "epoch:18 step:17573 [D loss: 0.579753, acc.: 74.22%] [G loss: 0.927891]\n",
      "epoch:18 step:17574 [D loss: 0.587088, acc.: 67.97%] [G loss: 0.875160]\n",
      "epoch:18 step:17575 [D loss: 0.600284, acc.: 70.31%] [G loss: 1.049581]\n",
      "epoch:18 step:17576 [D loss: 0.601093, acc.: 70.31%] [G loss: 1.034039]\n",
      "epoch:18 step:17577 [D loss: 0.603632, acc.: 73.44%] [G loss: 0.957917]\n",
      "epoch:18 step:17578 [D loss: 0.615921, acc.: 71.88%] [G loss: 1.047622]\n",
      "epoch:18 step:17579 [D loss: 0.702415, acc.: 52.34%] [G loss: 0.920310]\n",
      "epoch:18 step:17580 [D loss: 0.470419, acc.: 84.38%] [G loss: 0.972188]\n",
      "epoch:18 step:17581 [D loss: 0.616885, acc.: 61.72%] [G loss: 0.819535]\n",
      "epoch:18 step:17582 [D loss: 0.553610, acc.: 79.69%] [G loss: 0.815210]\n",
      "epoch:18 step:17583 [D loss: 0.753152, acc.: 47.66%] [G loss: 0.716251]\n",
      "epoch:18 step:17584 [D loss: 0.544343, acc.: 78.12%] [G loss: 0.923558]\n",
      "epoch:18 step:17585 [D loss: 0.591389, acc.: 72.66%] [G loss: 1.056527]\n",
      "epoch:18 step:17586 [D loss: 0.637542, acc.: 60.16%] [G loss: 0.702702]\n",
      "epoch:18 step:17587 [D loss: 0.675574, acc.: 62.50%] [G loss: 0.725446]\n",
      "epoch:18 step:17588 [D loss: 0.473681, acc.: 83.59%] [G loss: 1.059041]\n",
      "epoch:18 step:17589 [D loss: 0.588363, acc.: 64.84%] [G loss: 0.864050]\n",
      "epoch:18 step:17590 [D loss: 0.574878, acc.: 70.31%] [G loss: 1.412641]\n",
      "epoch:18 step:17591 [D loss: 0.587090, acc.: 71.09%] [G loss: 1.032705]\n",
      "epoch:18 step:17592 [D loss: 0.664193, acc.: 55.47%] [G loss: 0.924853]\n",
      "epoch:18 step:17593 [D loss: 0.493737, acc.: 84.38%] [G loss: 1.044023]\n",
      "epoch:18 step:17594 [D loss: 0.507320, acc.: 78.12%] [G loss: 0.900484]\n",
      "epoch:18 step:17595 [D loss: 0.614055, acc.: 64.84%] [G loss: 1.330698]\n",
      "epoch:18 step:17596 [D loss: 0.657200, acc.: 67.97%] [G loss: 1.155494]\n",
      "epoch:18 step:17597 [D loss: 0.670674, acc.: 57.81%] [G loss: 1.218014]\n",
      "epoch:18 step:17598 [D loss: 0.737678, acc.: 51.56%] [G loss: 1.055688]\n",
      "epoch:18 step:17599 [D loss: 0.610839, acc.: 67.97%] [G loss: 0.859352]\n",
      "epoch:18 step:17600 [D loss: 0.443264, acc.: 89.06%] [G loss: 0.811012]\n",
      "epoch:18 step:17601 [D loss: 0.674343, acc.: 53.12%] [G loss: 0.905088]\n",
      "epoch:18 step:17602 [D loss: 0.449460, acc.: 86.72%] [G loss: 0.987178]\n",
      "epoch:18 step:17603 [D loss: 0.514524, acc.: 82.81%] [G loss: 0.862438]\n",
      "epoch:18 step:17604 [D loss: 0.653866, acc.: 64.84%] [G loss: 1.135455]\n",
      "epoch:18 step:17605 [D loss: 0.675354, acc.: 57.81%] [G loss: 0.994656]\n",
      "epoch:18 step:17606 [D loss: 0.791133, acc.: 36.72%] [G loss: 0.916423]\n",
      "epoch:18 step:17607 [D loss: 0.545624, acc.: 75.78%] [G loss: 0.912167]\n",
      "epoch:18 step:17608 [D loss: 0.672068, acc.: 53.91%] [G loss: 1.179446]\n",
      "epoch:18 step:17609 [D loss: 0.808571, acc.: 49.22%] [G loss: 1.448876]\n",
      "epoch:18 step:17610 [D loss: 0.581742, acc.: 69.53%] [G loss: 1.485893]\n",
      "epoch:18 step:17611 [D loss: 0.533203, acc.: 75.00%] [G loss: 1.243669]\n",
      "epoch:18 step:17612 [D loss: 0.728652, acc.: 46.09%] [G loss: 1.229833]\n",
      "epoch:18 step:17613 [D loss: 0.619372, acc.: 64.06%] [G loss: 0.957734]\n",
      "epoch:18 step:17614 [D loss: 0.490451, acc.: 77.34%] [G loss: 1.033900]\n",
      "epoch:18 step:17615 [D loss: 0.565499, acc.: 76.56%] [G loss: 0.948889]\n",
      "epoch:18 step:17616 [D loss: 0.548420, acc.: 75.78%] [G loss: 0.903549]\n",
      "epoch:18 step:17617 [D loss: 0.635381, acc.: 64.84%] [G loss: 0.853550]\n",
      "epoch:18 step:17618 [D loss: 0.704422, acc.: 57.03%] [G loss: 1.411633]\n",
      "epoch:18 step:17619 [D loss: 0.644819, acc.: 61.72%] [G loss: 1.267900]\n",
      "epoch:18 step:17620 [D loss: 0.448329, acc.: 86.72%] [G loss: 1.322421]\n",
      "epoch:18 step:17621 [D loss: 0.753888, acc.: 48.44%] [G loss: 1.044743]\n",
      "epoch:18 step:17622 [D loss: 0.666809, acc.: 61.72%] [G loss: 1.116943]\n",
      "epoch:18 step:17623 [D loss: 0.573542, acc.: 72.66%] [G loss: 0.913148]\n",
      "epoch:18 step:17624 [D loss: 0.605593, acc.: 67.19%] [G loss: 1.202631]\n",
      "epoch:18 step:17625 [D loss: 0.646367, acc.: 59.38%] [G loss: 0.949032]\n",
      "epoch:18 step:17626 [D loss: 0.821845, acc.: 43.75%] [G loss: 1.301561]\n",
      "epoch:18 step:17627 [D loss: 0.678619, acc.: 57.03%] [G loss: 0.974512]\n",
      "epoch:18 step:17628 [D loss: 0.509242, acc.: 75.00%] [G loss: 1.019707]\n",
      "epoch:18 step:17629 [D loss: 0.424787, acc.: 83.59%] [G loss: 1.206266]\n",
      "epoch:18 step:17630 [D loss: 0.494710, acc.: 82.03%] [G loss: 1.018341]\n",
      "epoch:18 step:17631 [D loss: 0.510440, acc.: 72.66%] [G loss: 1.361693]\n",
      "epoch:18 step:17632 [D loss: 0.563043, acc.: 70.31%] [G loss: 1.061669]\n",
      "epoch:18 step:17633 [D loss: 0.611684, acc.: 67.19%] [G loss: 1.254852]\n",
      "epoch:18 step:17634 [D loss: 0.565965, acc.: 71.88%] [G loss: 1.134965]\n",
      "epoch:18 step:17635 [D loss: 0.509073, acc.: 74.22%] [G loss: 0.831658]\n",
      "epoch:18 step:17636 [D loss: 0.605792, acc.: 67.97%] [G loss: 1.339408]\n",
      "epoch:18 step:17637 [D loss: 0.445684, acc.: 87.50%] [G loss: 1.217842]\n",
      "epoch:18 step:17638 [D loss: 0.605417, acc.: 73.44%] [G loss: 1.132231]\n",
      "epoch:18 step:17639 [D loss: 0.780754, acc.: 46.88%] [G loss: 0.932511]\n",
      "epoch:18 step:17640 [D loss: 0.413899, acc.: 89.06%] [G loss: 1.209883]\n",
      "epoch:18 step:17641 [D loss: 0.657023, acc.: 60.16%] [G loss: 1.057255]\n",
      "epoch:18 step:17642 [D loss: 0.493138, acc.: 79.69%] [G loss: 1.029853]\n",
      "epoch:18 step:17643 [D loss: 0.925839, acc.: 32.81%] [G loss: 1.314960]\n",
      "epoch:18 step:17644 [D loss: 0.648754, acc.: 66.41%] [G loss: 0.932783]\n",
      "epoch:18 step:17645 [D loss: 0.601758, acc.: 67.97%] [G loss: 1.009008]\n",
      "epoch:18 step:17646 [D loss: 0.616581, acc.: 64.84%] [G loss: 1.155928]\n",
      "epoch:18 step:17647 [D loss: 0.743741, acc.: 50.00%] [G loss: 1.196281]\n",
      "epoch:18 step:17648 [D loss: 0.883974, acc.: 32.03%] [G loss: 0.796647]\n",
      "epoch:18 step:17649 [D loss: 0.594676, acc.: 65.62%] [G loss: 1.164112]\n",
      "epoch:18 step:17650 [D loss: 0.751035, acc.: 45.31%] [G loss: 0.917349]\n",
      "epoch:18 step:17651 [D loss: 0.523164, acc.: 84.38%] [G loss: 1.036776]\n",
      "epoch:18 step:17652 [D loss: 0.631098, acc.: 60.16%] [G loss: 1.135851]\n",
      "epoch:18 step:17653 [D loss: 0.638938, acc.: 57.81%] [G loss: 1.094850]\n",
      "epoch:18 step:17654 [D loss: 0.578942, acc.: 69.53%] [G loss: 1.219733]\n",
      "epoch:18 step:17655 [D loss: 0.538207, acc.: 76.56%] [G loss: 1.820564]\n",
      "epoch:18 step:17656 [D loss: 0.582996, acc.: 65.62%] [G loss: 1.224653]\n",
      "epoch:18 step:17657 [D loss: 0.464911, acc.: 85.16%] [G loss: 0.920909]\n",
      "epoch:18 step:17658 [D loss: 0.613926, acc.: 69.53%] [G loss: 0.998417]\n",
      "epoch:18 step:17659 [D loss: 0.567017, acc.: 70.31%] [G loss: 1.112379]\n",
      "epoch:18 step:17660 [D loss: 0.554988, acc.: 73.44%] [G loss: 0.911077]\n",
      "epoch:18 step:17661 [D loss: 0.596798, acc.: 74.22%] [G loss: 1.078438]\n",
      "epoch:18 step:17662 [D loss: 0.652035, acc.: 62.50%] [G loss: 1.158059]\n",
      "epoch:18 step:17663 [D loss: 0.804999, acc.: 47.66%] [G loss: 0.897216]\n",
      "epoch:18 step:17664 [D loss: 0.663998, acc.: 57.03%] [G loss: 1.167421]\n",
      "epoch:18 step:17665 [D loss: 0.556245, acc.: 70.31%] [G loss: 1.117000]\n",
      "epoch:18 step:17666 [D loss: 0.583501, acc.: 73.44%] [G loss: 1.571720]\n",
      "epoch:18 step:17667 [D loss: 0.535880, acc.: 71.09%] [G loss: 1.356626]\n",
      "epoch:18 step:17668 [D loss: 0.747363, acc.: 46.88%] [G loss: 0.963166]\n",
      "epoch:18 step:17669 [D loss: 0.585408, acc.: 71.88%] [G loss: 0.881809]\n",
      "epoch:18 step:17670 [D loss: 0.639346, acc.: 57.81%] [G loss: 1.286426]\n",
      "epoch:18 step:17671 [D loss: 0.483890, acc.: 88.28%] [G loss: 1.761618]\n",
      "epoch:18 step:17672 [D loss: 0.506665, acc.: 75.78%] [G loss: 0.982192]\n",
      "epoch:18 step:17673 [D loss: 0.595288, acc.: 64.06%] [G loss: 0.792933]\n",
      "epoch:18 step:17674 [D loss: 0.517892, acc.: 77.34%] [G loss: 1.050607]\n",
      "epoch:18 step:17675 [D loss: 0.455281, acc.: 85.94%] [G loss: 0.899083]\n",
      "epoch:18 step:17676 [D loss: 0.645037, acc.: 60.94%] [G loss: 0.798194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17677 [D loss: 0.586775, acc.: 70.31%] [G loss: 0.741576]\n",
      "epoch:18 step:17678 [D loss: 0.561073, acc.: 73.44%] [G loss: 0.997100]\n",
      "epoch:18 step:17679 [D loss: 0.662688, acc.: 61.72%] [G loss: 1.399153]\n",
      "epoch:18 step:17680 [D loss: 0.453405, acc.: 75.00%] [G loss: 1.376138]\n",
      "epoch:18 step:17681 [D loss: 0.642860, acc.: 61.72%] [G loss: 1.373867]\n",
      "epoch:18 step:17682 [D loss: 0.586591, acc.: 66.41%] [G loss: 1.345465]\n",
      "epoch:18 step:17683 [D loss: 0.586064, acc.: 66.41%] [G loss: 1.184460]\n",
      "epoch:18 step:17684 [D loss: 0.668811, acc.: 60.94%] [G loss: 1.001385]\n",
      "epoch:18 step:17685 [D loss: 0.588684, acc.: 66.41%] [G loss: 0.942326]\n",
      "epoch:18 step:17686 [D loss: 0.558084, acc.: 75.78%] [G loss: 1.385284]\n",
      "epoch:18 step:17687 [D loss: 0.620716, acc.: 69.53%] [G loss: 1.234837]\n",
      "epoch:18 step:17688 [D loss: 0.690929, acc.: 58.59%] [G loss: 1.213377]\n",
      "epoch:18 step:17689 [D loss: 0.846266, acc.: 32.03%] [G loss: 1.428498]\n",
      "epoch:18 step:17690 [D loss: 0.554695, acc.: 71.09%] [G loss: 1.057323]\n",
      "epoch:18 step:17691 [D loss: 0.625959, acc.: 62.50%] [G loss: 1.383320]\n",
      "epoch:18 step:17692 [D loss: 0.550475, acc.: 75.78%] [G loss: 1.273031]\n",
      "epoch:18 step:17693 [D loss: 0.391873, acc.: 86.72%] [G loss: 0.989483]\n",
      "epoch:18 step:17694 [D loss: 0.794585, acc.: 50.00%] [G loss: 0.918700]\n",
      "epoch:18 step:17695 [D loss: 0.728591, acc.: 51.56%] [G loss: 0.963664]\n",
      "epoch:18 step:17696 [D loss: 0.608676, acc.: 67.97%] [G loss: 0.965024]\n",
      "epoch:18 step:17697 [D loss: 0.767829, acc.: 46.09%] [G loss: 0.996898]\n",
      "epoch:18 step:17698 [D loss: 0.627447, acc.: 64.06%] [G loss: 1.010659]\n",
      "epoch:18 step:17699 [D loss: 0.580868, acc.: 67.97%] [G loss: 1.257736]\n",
      "epoch:18 step:17700 [D loss: 0.597912, acc.: 73.44%] [G loss: 0.991001]\n",
      "epoch:18 step:17701 [D loss: 0.554612, acc.: 72.66%] [G loss: 1.177088]\n",
      "epoch:18 step:17702 [D loss: 0.654277, acc.: 61.72%] [G loss: 1.198821]\n",
      "epoch:18 step:17703 [D loss: 0.597182, acc.: 68.75%] [G loss: 0.985439]\n",
      "epoch:18 step:17704 [D loss: 0.639061, acc.: 62.50%] [G loss: 1.394906]\n",
      "epoch:18 step:17705 [D loss: 0.505786, acc.: 79.69%] [G loss: 1.341472]\n",
      "epoch:18 step:17706 [D loss: 0.635252, acc.: 57.81%] [G loss: 0.809918]\n",
      "epoch:18 step:17707 [D loss: 0.555355, acc.: 70.31%] [G loss: 1.483196]\n",
      "epoch:18 step:17708 [D loss: 0.575864, acc.: 66.41%] [G loss: 1.895671]\n",
      "epoch:18 step:17709 [D loss: 0.458965, acc.: 83.59%] [G loss: 1.045810]\n",
      "epoch:18 step:17710 [D loss: 0.633607, acc.: 66.41%] [G loss: 1.273145]\n",
      "epoch:18 step:17711 [D loss: 0.523790, acc.: 79.69%] [G loss: 1.268538]\n",
      "epoch:18 step:17712 [D loss: 0.610293, acc.: 65.62%] [G loss: 1.016129]\n",
      "epoch:18 step:17713 [D loss: 0.654197, acc.: 64.06%] [G loss: 1.361573]\n",
      "epoch:18 step:17714 [D loss: 0.697371, acc.: 56.25%] [G loss: 0.742920]\n",
      "epoch:18 step:17715 [D loss: 0.648595, acc.: 61.72%] [G loss: 1.241882]\n",
      "epoch:18 step:17716 [D loss: 0.550690, acc.: 73.44%] [G loss: 1.454141]\n",
      "epoch:18 step:17717 [D loss: 0.610945, acc.: 65.62%] [G loss: 0.765951]\n",
      "epoch:18 step:17718 [D loss: 0.470158, acc.: 84.38%] [G loss: 0.783577]\n",
      "epoch:18 step:17719 [D loss: 0.616452, acc.: 67.19%] [G loss: 0.929924]\n",
      "epoch:18 step:17720 [D loss: 0.623036, acc.: 67.19%] [G loss: 1.033250]\n",
      "epoch:18 step:17721 [D loss: 0.695203, acc.: 60.16%] [G loss: 0.774317]\n",
      "epoch:18 step:17722 [D loss: 0.679866, acc.: 57.03%] [G loss: 0.919019]\n",
      "epoch:18 step:17723 [D loss: 0.560983, acc.: 68.75%] [G loss: 1.463575]\n",
      "epoch:18 step:17724 [D loss: 0.587686, acc.: 69.53%] [G loss: 0.889489]\n",
      "epoch:18 step:17725 [D loss: 0.684008, acc.: 58.59%] [G loss: 1.008502]\n",
      "epoch:18 step:17726 [D loss: 0.529131, acc.: 79.69%] [G loss: 0.915580]\n",
      "epoch:18 step:17727 [D loss: 0.582142, acc.: 70.31%] [G loss: 1.243355]\n",
      "epoch:18 step:17728 [D loss: 0.635140, acc.: 67.97%] [G loss: 1.164140]\n",
      "epoch:18 step:17729 [D loss: 0.438045, acc.: 82.03%] [G loss: 0.974095]\n",
      "epoch:18 step:17730 [D loss: 0.541820, acc.: 72.66%] [G loss: 1.493900]\n",
      "epoch:18 step:17731 [D loss: 0.696775, acc.: 55.47%] [G loss: 1.479324]\n",
      "epoch:18 step:17732 [D loss: 0.607428, acc.: 66.41%] [G loss: 0.920091]\n",
      "epoch:18 step:17733 [D loss: 0.557059, acc.: 73.44%] [G loss: 1.105363]\n",
      "epoch:18 step:17734 [D loss: 0.596031, acc.: 64.84%] [G loss: 1.176576]\n",
      "epoch:18 step:17735 [D loss: 0.625709, acc.: 59.38%] [G loss: 1.215646]\n",
      "epoch:18 step:17736 [D loss: 0.622393, acc.: 69.53%] [G loss: 1.017951]\n",
      "epoch:18 step:17737 [D loss: 0.638055, acc.: 64.06%] [G loss: 1.148516]\n",
      "epoch:18 step:17738 [D loss: 0.564529, acc.: 73.44%] [G loss: 1.388442]\n",
      "epoch:18 step:17739 [D loss: 0.528586, acc.: 80.47%] [G loss: 1.032376]\n",
      "epoch:18 step:17740 [D loss: 0.585337, acc.: 69.53%] [G loss: 1.074804]\n",
      "epoch:18 step:17741 [D loss: 0.761770, acc.: 50.78%] [G loss: 1.469491]\n",
      "epoch:18 step:17742 [D loss: 0.406439, acc.: 89.06%] [G loss: 1.684656]\n",
      "epoch:18 step:17743 [D loss: 0.639532, acc.: 61.72%] [G loss: 0.776480]\n",
      "epoch:18 step:17744 [D loss: 0.567379, acc.: 68.75%] [G loss: 1.396774]\n",
      "epoch:18 step:17745 [D loss: 0.581223, acc.: 65.62%] [G loss: 0.745259]\n",
      "epoch:18 step:17746 [D loss: 0.888087, acc.: 33.59%] [G loss: 0.755284]\n",
      "epoch:18 step:17747 [D loss: 0.520563, acc.: 71.09%] [G loss: 0.809801]\n",
      "epoch:18 step:17748 [D loss: 0.475370, acc.: 86.72%] [G loss: 0.916133]\n",
      "epoch:18 step:17749 [D loss: 0.574726, acc.: 71.88%] [G loss: 1.689830]\n",
      "epoch:18 step:17750 [D loss: 0.546803, acc.: 72.66%] [G loss: 1.135901]\n",
      "epoch:18 step:17751 [D loss: 0.502151, acc.: 78.12%] [G loss: 1.022183]\n",
      "epoch:18 step:17752 [D loss: 0.472760, acc.: 77.34%] [G loss: 0.677516]\n",
      "epoch:18 step:17753 [D loss: 0.759545, acc.: 53.91%] [G loss: 1.233479]\n",
      "epoch:18 step:17754 [D loss: 0.325970, acc.: 93.75%] [G loss: 1.196941]\n",
      "epoch:18 step:17755 [D loss: 0.639644, acc.: 63.28%] [G loss: 0.807078]\n",
      "epoch:18 step:17756 [D loss: 0.574472, acc.: 75.00%] [G loss: 0.967585]\n",
      "epoch:18 step:17757 [D loss: 0.611042, acc.: 68.75%] [G loss: 1.308474]\n",
      "epoch:18 step:17758 [D loss: 0.620573, acc.: 61.72%] [G loss: 0.781240]\n",
      "epoch:18 step:17759 [D loss: 0.441188, acc.: 86.72%] [G loss: 0.930903]\n",
      "epoch:18 step:17760 [D loss: 0.638788, acc.: 58.59%] [G loss: 1.516427]\n",
      "epoch:18 step:17761 [D loss: 0.656537, acc.: 65.62%] [G loss: 0.803021]\n",
      "epoch:18 step:17762 [D loss: 0.810710, acc.: 45.31%] [G loss: 0.887847]\n",
      "epoch:18 step:17763 [D loss: 0.712875, acc.: 55.47%] [G loss: 1.541241]\n",
      "epoch:18 step:17764 [D loss: 0.714658, acc.: 52.34%] [G loss: 1.508708]\n",
      "epoch:18 step:17765 [D loss: 0.515153, acc.: 74.22%] [G loss: 1.285139]\n",
      "epoch:18 step:17766 [D loss: 0.819376, acc.: 39.84%] [G loss: 1.418658]\n",
      "epoch:18 step:17767 [D loss: 0.919027, acc.: 35.16%] [G loss: 1.111974]\n",
      "epoch:18 step:17768 [D loss: 0.598237, acc.: 66.41%] [G loss: 1.406090]\n",
      "epoch:18 step:17769 [D loss: 0.656212, acc.: 57.03%] [G loss: 0.963517]\n",
      "epoch:18 step:17770 [D loss: 0.619544, acc.: 59.38%] [G loss: 1.372329]\n",
      "epoch:18 step:17771 [D loss: 0.585205, acc.: 74.22%] [G loss: 0.720296]\n",
      "epoch:18 step:17772 [D loss: 0.631499, acc.: 63.28%] [G loss: 1.473161]\n",
      "epoch:18 step:17773 [D loss: 0.511182, acc.: 78.12%] [G loss: 1.523220]\n",
      "epoch:18 step:17774 [D loss: 0.593773, acc.: 68.75%] [G loss: 1.267020]\n",
      "epoch:18 step:17775 [D loss: 0.462870, acc.: 82.03%] [G loss: 1.673755]\n",
      "epoch:18 step:17776 [D loss: 0.512985, acc.: 75.00%] [G loss: 1.600569]\n",
      "epoch:18 step:17777 [D loss: 0.562039, acc.: 77.34%] [G loss: 1.011633]\n",
      "epoch:18 step:17778 [D loss: 0.614034, acc.: 67.97%] [G loss: 1.500331]\n",
      "epoch:18 step:17779 [D loss: 0.482609, acc.: 77.34%] [G loss: 1.023245]\n",
      "epoch:18 step:17780 [D loss: 0.383855, acc.: 86.72%] [G loss: 1.198766]\n",
      "epoch:18 step:17781 [D loss: 0.604595, acc.: 69.53%] [G loss: 1.141421]\n",
      "epoch:18 step:17782 [D loss: 0.626141, acc.: 63.28%] [G loss: 1.498097]\n",
      "epoch:18 step:17783 [D loss: 0.449776, acc.: 82.81%] [G loss: 1.669349]\n",
      "epoch:18 step:17784 [D loss: 0.406245, acc.: 86.72%] [G loss: 1.344526]\n",
      "epoch:18 step:17785 [D loss: 0.424302, acc.: 89.06%] [G loss: 0.492350]\n",
      "epoch:18 step:17786 [D loss: 0.571358, acc.: 69.53%] [G loss: 1.211830]\n",
      "epoch:18 step:17787 [D loss: 0.529785, acc.: 75.78%] [G loss: 1.238898]\n",
      "epoch:18 step:17788 [D loss: 0.502709, acc.: 76.56%] [G loss: 1.097077]\n",
      "epoch:18 step:17789 [D loss: 1.052979, acc.: 27.34%] [G loss: 1.241842]\n",
      "epoch:18 step:17790 [D loss: 0.505693, acc.: 79.69%] [G loss: 1.150562]\n",
      "epoch:18 step:17791 [D loss: 0.497059, acc.: 79.69%] [G loss: 1.222877]\n",
      "epoch:18 step:17792 [D loss: 0.519140, acc.: 78.91%] [G loss: 1.118328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17793 [D loss: 0.478896, acc.: 77.34%] [G loss: 1.397690]\n",
      "epoch:18 step:17794 [D loss: 0.736279, acc.: 50.00%] [G loss: 1.097949]\n",
      "epoch:18 step:17795 [D loss: 0.742119, acc.: 49.22%] [G loss: 0.971446]\n",
      "epoch:18 step:17796 [D loss: 0.627035, acc.: 64.06%] [G loss: 1.448123]\n",
      "epoch:18 step:17797 [D loss: 0.761729, acc.: 55.47%] [G loss: 0.692464]\n",
      "epoch:18 step:17798 [D loss: 0.552057, acc.: 71.88%] [G loss: 0.459746]\n",
      "epoch:18 step:17799 [D loss: 0.503199, acc.: 78.91%] [G loss: 0.821242]\n",
      "epoch:18 step:17800 [D loss: 0.437634, acc.: 88.28%] [G loss: 0.720754]\n",
      "epoch:18 step:17801 [D loss: 0.853228, acc.: 43.75%] [G loss: 0.813734]\n",
      "epoch:18 step:17802 [D loss: 0.358471, acc.: 90.62%] [G loss: 1.232418]\n",
      "epoch:18 step:17803 [D loss: 0.749897, acc.: 46.88%] [G loss: 0.983927]\n",
      "epoch:19 step:17804 [D loss: 0.691625, acc.: 57.81%] [G loss: 1.039148]\n",
      "epoch:19 step:17805 [D loss: 0.558046, acc.: 64.06%] [G loss: 0.976193]\n",
      "epoch:19 step:17806 [D loss: 0.672658, acc.: 52.34%] [G loss: 1.365053]\n",
      "epoch:19 step:17807 [D loss: 0.674337, acc.: 58.59%] [G loss: 1.142939]\n",
      "epoch:19 step:17808 [D loss: 0.591061, acc.: 64.06%] [G loss: 0.924343]\n",
      "epoch:19 step:17809 [D loss: 0.611821, acc.: 68.75%] [G loss: 0.976742]\n",
      "epoch:19 step:17810 [D loss: 0.474278, acc.: 82.81%] [G loss: 1.504236]\n",
      "epoch:19 step:17811 [D loss: 0.492106, acc.: 77.34%] [G loss: 1.223557]\n",
      "epoch:19 step:17812 [D loss: 0.935812, acc.: 27.34%] [G loss: 1.509184]\n",
      "epoch:19 step:17813 [D loss: 0.775181, acc.: 46.88%] [G loss: 1.044551]\n",
      "epoch:19 step:17814 [D loss: 0.511375, acc.: 75.00%] [G loss: 1.547542]\n",
      "epoch:19 step:17815 [D loss: 0.659936, acc.: 57.03%] [G loss: 1.503379]\n",
      "epoch:19 step:17816 [D loss: 0.546555, acc.: 73.44%] [G loss: 1.249165]\n",
      "epoch:19 step:17817 [D loss: 0.609080, acc.: 67.19%] [G loss: 1.240061]\n",
      "epoch:19 step:17818 [D loss: 0.568470, acc.: 71.09%] [G loss: 1.661003]\n",
      "epoch:19 step:17819 [D loss: 0.460346, acc.: 81.25%] [G loss: 1.338533]\n",
      "epoch:19 step:17820 [D loss: 0.601586, acc.: 62.50%] [G loss: 0.970768]\n",
      "epoch:19 step:17821 [D loss: 0.624716, acc.: 66.41%] [G loss: 1.380154]\n",
      "epoch:19 step:17822 [D loss: 0.605684, acc.: 72.66%] [G loss: 2.138535]\n",
      "epoch:19 step:17823 [D loss: 0.511127, acc.: 78.12%] [G loss: 1.335415]\n",
      "epoch:19 step:17824 [D loss: 0.780311, acc.: 45.31%] [G loss: 1.162526]\n",
      "epoch:19 step:17825 [D loss: 0.766727, acc.: 55.47%] [G loss: 1.501742]\n",
      "epoch:19 step:17826 [D loss: 0.489951, acc.: 81.25%] [G loss: 1.496601]\n",
      "epoch:19 step:17827 [D loss: 0.688810, acc.: 53.91%] [G loss: 1.028562]\n",
      "epoch:19 step:17828 [D loss: 0.585756, acc.: 68.75%] [G loss: 1.577488]\n",
      "epoch:19 step:17829 [D loss: 0.852928, acc.: 38.28%] [G loss: 0.972889]\n",
      "epoch:19 step:17830 [D loss: 0.917271, acc.: 42.19%] [G loss: 1.454758]\n",
      "epoch:19 step:17831 [D loss: 0.587401, acc.: 67.19%] [G loss: 1.323725]\n",
      "epoch:19 step:17832 [D loss: 0.598704, acc.: 66.41%] [G loss: 1.058739]\n",
      "epoch:19 step:17833 [D loss: 0.478142, acc.: 75.00%] [G loss: 1.065973]\n",
      "epoch:19 step:17834 [D loss: 0.584417, acc.: 66.41%] [G loss: 1.307258]\n",
      "epoch:19 step:17835 [D loss: 0.510364, acc.: 75.78%] [G loss: 1.229281]\n",
      "epoch:19 step:17836 [D loss: 0.596726, acc.: 63.28%] [G loss: 0.917718]\n",
      "epoch:19 step:17837 [D loss: 0.543701, acc.: 74.22%] [G loss: 1.162012]\n",
      "epoch:19 step:17838 [D loss: 0.468553, acc.: 86.72%] [G loss: 1.418447]\n",
      "epoch:19 step:17839 [D loss: 0.614404, acc.: 61.72%] [G loss: 1.500846]\n",
      "epoch:19 step:17840 [D loss: 0.607549, acc.: 71.88%] [G loss: 0.932597]\n",
      "epoch:19 step:17841 [D loss: 0.610881, acc.: 64.06%] [G loss: 1.222376]\n",
      "epoch:19 step:17842 [D loss: 0.565958, acc.: 67.97%] [G loss: 1.439423]\n",
      "epoch:19 step:17843 [D loss: 0.675293, acc.: 60.94%] [G loss: 0.835490]\n",
      "epoch:19 step:17844 [D loss: 0.738810, acc.: 52.34%] [G loss: 1.300397]\n",
      "epoch:19 step:17845 [D loss: 0.660571, acc.: 57.81%] [G loss: 1.311707]\n",
      "epoch:19 step:17846 [D loss: 0.562249, acc.: 69.53%] [G loss: 1.039310]\n",
      "epoch:19 step:17847 [D loss: 0.723554, acc.: 53.91%] [G loss: 1.506380]\n",
      "epoch:19 step:17848 [D loss: 0.572017, acc.: 66.41%] [G loss: 1.498591]\n",
      "epoch:19 step:17849 [D loss: 0.576499, acc.: 71.09%] [G loss: 1.288072]\n",
      "epoch:19 step:17850 [D loss: 0.781297, acc.: 49.22%] [G loss: 1.194010]\n",
      "epoch:19 step:17851 [D loss: 0.616414, acc.: 67.19%] [G loss: 1.440927]\n",
      "epoch:19 step:17852 [D loss: 0.511719, acc.: 76.56%] [G loss: 1.145958]\n",
      "epoch:19 step:17853 [D loss: 0.474723, acc.: 78.91%] [G loss: 1.170836]\n",
      "epoch:19 step:17854 [D loss: 0.540827, acc.: 75.78%] [G loss: 1.348606]\n",
      "epoch:19 step:17855 [D loss: 0.534985, acc.: 76.56%] [G loss: 1.099904]\n",
      "epoch:19 step:17856 [D loss: 0.550645, acc.: 76.56%] [G loss: 1.007360]\n",
      "epoch:19 step:17857 [D loss: 0.559105, acc.: 68.75%] [G loss: 1.043448]\n",
      "epoch:19 step:17858 [D loss: 0.490797, acc.: 79.69%] [G loss: 0.865477]\n",
      "epoch:19 step:17859 [D loss: 0.520265, acc.: 78.12%] [G loss: 1.001849]\n",
      "epoch:19 step:17860 [D loss: 0.617308, acc.: 61.72%] [G loss: 1.518358]\n",
      "epoch:19 step:17861 [D loss: 0.488575, acc.: 78.91%] [G loss: 1.094113]\n",
      "epoch:19 step:17862 [D loss: 0.577175, acc.: 67.97%] [G loss: 0.908269]\n",
      "epoch:19 step:17863 [D loss: 0.501713, acc.: 72.66%] [G loss: 1.288833]\n",
      "epoch:19 step:17864 [D loss: 0.517922, acc.: 75.00%] [G loss: 1.410524]\n",
      "epoch:19 step:17865 [D loss: 0.570545, acc.: 75.00%] [G loss: 1.783631]\n",
      "epoch:19 step:17866 [D loss: 0.715117, acc.: 53.91%] [G loss: 1.286034]\n",
      "epoch:19 step:17867 [D loss: 0.719343, acc.: 53.12%] [G loss: 1.276787]\n",
      "epoch:19 step:17868 [D loss: 0.644550, acc.: 60.94%] [G loss: 1.342739]\n",
      "epoch:19 step:17869 [D loss: 0.655261, acc.: 57.03%] [G loss: 1.716486]\n",
      "epoch:19 step:17870 [D loss: 0.611032, acc.: 69.53%] [G loss: 1.565006]\n",
      "epoch:19 step:17871 [D loss: 0.569438, acc.: 71.88%] [G loss: 1.260248]\n",
      "epoch:19 step:17872 [D loss: 0.419819, acc.: 91.41%] [G loss: 1.691984]\n",
      "epoch:19 step:17873 [D loss: 0.597593, acc.: 69.53%] [G loss: 1.207405]\n",
      "epoch:19 step:17874 [D loss: 0.383837, acc.: 89.84%] [G loss: 1.759903]\n",
      "epoch:19 step:17875 [D loss: 0.632426, acc.: 64.84%] [G loss: 0.976131]\n",
      "epoch:19 step:17876 [D loss: 0.426874, acc.: 91.41%] [G loss: 1.185794]\n",
      "epoch:19 step:17877 [D loss: 0.410793, acc.: 89.84%] [G loss: 1.759527]\n",
      "epoch:19 step:17878 [D loss: 0.648447, acc.: 58.59%] [G loss: 1.364486]\n",
      "epoch:19 step:17879 [D loss: 0.503976, acc.: 78.91%] [G loss: 0.890101]\n",
      "epoch:19 step:17880 [D loss: 0.616265, acc.: 66.41%] [G loss: 1.072488]\n",
      "epoch:19 step:17881 [D loss: 0.549410, acc.: 75.00%] [G loss: 1.639629]\n",
      "epoch:19 step:17882 [D loss: 0.372759, acc.: 94.53%] [G loss: 1.964418]\n",
      "epoch:19 step:17883 [D loss: 0.607362, acc.: 67.19%] [G loss: 1.242224]\n",
      "epoch:19 step:17884 [D loss: 0.654220, acc.: 56.25%] [G loss: 0.842460]\n",
      "epoch:19 step:17885 [D loss: 0.665565, acc.: 62.50%] [G loss: 0.707595]\n",
      "epoch:19 step:17886 [D loss: 0.499297, acc.: 82.03%] [G loss: 0.806019]\n",
      "epoch:19 step:17887 [D loss: 0.423764, acc.: 84.38%] [G loss: 1.790760]\n",
      "epoch:19 step:17888 [D loss: 0.435255, acc.: 85.16%] [G loss: 1.932397]\n",
      "epoch:19 step:17889 [D loss: 0.744980, acc.: 49.22%] [G loss: 0.735858]\n",
      "epoch:19 step:17890 [D loss: 0.594134, acc.: 58.59%] [G loss: 1.809136]\n",
      "epoch:19 step:17891 [D loss: 0.666987, acc.: 59.38%] [G loss: 1.011914]\n",
      "epoch:19 step:17892 [D loss: 0.605437, acc.: 67.97%] [G loss: 0.865026]\n",
      "epoch:19 step:17893 [D loss: 0.416968, acc.: 90.62%] [G loss: 1.351322]\n",
      "epoch:19 step:17894 [D loss: 0.401339, acc.: 89.84%] [G loss: 0.669968]\n",
      "epoch:19 step:17895 [D loss: 0.393786, acc.: 89.84%] [G loss: 0.964982]\n",
      "epoch:19 step:17896 [D loss: 0.542383, acc.: 70.31%] [G loss: 0.900760]\n",
      "epoch:19 step:17897 [D loss: 0.603626, acc.: 64.06%] [G loss: 0.982173]\n",
      "epoch:19 step:17898 [D loss: 0.503811, acc.: 85.94%] [G loss: 1.052732]\n",
      "epoch:19 step:17899 [D loss: 0.349212, acc.: 94.53%] [G loss: 0.747869]\n",
      "epoch:19 step:17900 [D loss: 0.760656, acc.: 42.19%] [G loss: 0.957489]\n",
      "epoch:19 step:17901 [D loss: 0.447710, acc.: 75.78%] [G loss: 1.665168]\n",
      "epoch:19 step:17902 [D loss: 0.630388, acc.: 61.72%] [G loss: 1.582054]\n",
      "epoch:19 step:17903 [D loss: 0.328931, acc.: 91.41%] [G loss: 1.021867]\n",
      "epoch:19 step:17904 [D loss: 0.943054, acc.: 32.03%] [G loss: 0.895545]\n",
      "epoch:19 step:17905 [D loss: 0.332900, acc.: 93.75%] [G loss: 1.722823]\n",
      "epoch:19 step:17906 [D loss: 0.753910, acc.: 53.12%] [G loss: 1.661173]\n",
      "epoch:19 step:17907 [D loss: 0.501239, acc.: 75.78%] [G loss: 2.009801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17908 [D loss: 0.655291, acc.: 53.91%] [G loss: 0.703830]\n",
      "epoch:19 step:17909 [D loss: 0.760104, acc.: 43.75%] [G loss: 1.429100]\n",
      "epoch:19 step:17910 [D loss: 0.628315, acc.: 64.84%] [G loss: 0.822851]\n",
      "epoch:19 step:17911 [D loss: 0.641584, acc.: 65.62%] [G loss: 1.501893]\n",
      "epoch:19 step:17912 [D loss: 0.820929, acc.: 38.28%] [G loss: 0.839343]\n",
      "epoch:19 step:17913 [D loss: 0.591709, acc.: 64.84%] [G loss: 0.607850]\n",
      "epoch:19 step:17914 [D loss: 0.616601, acc.: 64.06%] [G loss: 0.904396]\n",
      "epoch:19 step:17915 [D loss: 0.410372, acc.: 86.72%] [G loss: 1.450676]\n",
      "epoch:19 step:17916 [D loss: 0.453439, acc.: 80.47%] [G loss: 1.417768]\n",
      "epoch:19 step:17917 [D loss: 0.713516, acc.: 56.25%] [G loss: 1.395784]\n",
      "epoch:19 step:17918 [D loss: 0.692993, acc.: 54.69%] [G loss: 0.923625]\n",
      "epoch:19 step:17919 [D loss: 0.482172, acc.: 84.38%] [G loss: 1.237179]\n",
      "epoch:19 step:17920 [D loss: 0.561462, acc.: 70.31%] [G loss: 1.487563]\n",
      "epoch:19 step:17921 [D loss: 0.592145, acc.: 66.41%] [G loss: 1.201692]\n",
      "epoch:19 step:17922 [D loss: 0.508784, acc.: 75.78%] [G loss: 1.223407]\n",
      "epoch:19 step:17923 [D loss: 0.935852, acc.: 35.16%] [G loss: 1.199710]\n",
      "epoch:19 step:17924 [D loss: 0.458913, acc.: 88.28%] [G loss: 1.144613]\n",
      "epoch:19 step:17925 [D loss: 0.478616, acc.: 79.69%] [G loss: 1.132379]\n",
      "epoch:19 step:17926 [D loss: 0.375401, acc.: 92.19%] [G loss: 1.015132]\n",
      "epoch:19 step:17927 [D loss: 0.478343, acc.: 82.03%] [G loss: 0.796975]\n",
      "epoch:19 step:17928 [D loss: 0.556227, acc.: 72.66%] [G loss: 1.623423]\n",
      "epoch:19 step:17929 [D loss: 0.958866, acc.: 25.78%] [G loss: 0.632522]\n",
      "epoch:19 step:17930 [D loss: 0.798557, acc.: 44.53%] [G loss: 0.860685]\n",
      "epoch:19 step:17931 [D loss: 0.430496, acc.: 85.94%] [G loss: 1.665043]\n",
      "epoch:19 step:17932 [D loss: 0.609480, acc.: 62.50%] [G loss: 0.752789]\n",
      "epoch:19 step:17933 [D loss: 0.491883, acc.: 79.69%] [G loss: 2.200488]\n",
      "epoch:19 step:17934 [D loss: 0.913679, acc.: 41.41%] [G loss: 1.361793]\n",
      "epoch:19 step:17935 [D loss: 0.475196, acc.: 84.38%] [G loss: 1.380409]\n",
      "epoch:19 step:17936 [D loss: 0.689445, acc.: 56.25%] [G loss: 0.810153]\n",
      "epoch:19 step:17937 [D loss: 0.629289, acc.: 60.94%] [G loss: 0.794129]\n",
      "epoch:19 step:17938 [D loss: 0.841615, acc.: 37.50%] [G loss: 0.967141]\n",
      "epoch:19 step:17939 [D loss: 0.719539, acc.: 57.81%] [G loss: 0.856623]\n",
      "epoch:19 step:17940 [D loss: 0.688839, acc.: 51.56%] [G loss: 1.453486]\n",
      "epoch:19 step:17941 [D loss: 0.608787, acc.: 69.53%] [G loss: 1.318413]\n",
      "epoch:19 step:17942 [D loss: 0.595143, acc.: 68.75%] [G loss: 0.871609]\n",
      "epoch:19 step:17943 [D loss: 0.708646, acc.: 56.25%] [G loss: 1.142274]\n",
      "epoch:19 step:17944 [D loss: 0.421699, acc.: 85.94%] [G loss: 0.784543]\n",
      "epoch:19 step:17945 [D loss: 0.700018, acc.: 58.59%] [G loss: 1.105176]\n",
      "epoch:19 step:17946 [D loss: 0.517202, acc.: 73.44%] [G loss: 1.374167]\n",
      "epoch:19 step:17947 [D loss: 0.458448, acc.: 89.06%] [G loss: 1.770973]\n",
      "epoch:19 step:17948 [D loss: 0.571904, acc.: 67.97%] [G loss: 1.481509]\n",
      "epoch:19 step:17949 [D loss: 0.774915, acc.: 44.53%] [G loss: 0.777297]\n",
      "epoch:19 step:17950 [D loss: 0.669297, acc.: 51.56%] [G loss: 0.898959]\n",
      "epoch:19 step:17951 [D loss: 0.679853, acc.: 59.38%] [G loss: 0.977119]\n",
      "epoch:19 step:17952 [D loss: 0.474816, acc.: 82.81%] [G loss: 0.925114]\n",
      "epoch:19 step:17953 [D loss: 0.554826, acc.: 75.00%] [G loss: 1.247763]\n",
      "epoch:19 step:17954 [D loss: 0.439925, acc.: 88.28%] [G loss: 1.169930]\n",
      "epoch:19 step:17955 [D loss: 0.529873, acc.: 75.00%] [G loss: 1.401334]\n",
      "epoch:19 step:17956 [D loss: 0.470847, acc.: 82.03%] [G loss: 1.249459]\n",
      "epoch:19 step:17957 [D loss: 0.795017, acc.: 40.62%] [G loss: 0.989209]\n",
      "epoch:19 step:17958 [D loss: 0.526970, acc.: 77.34%] [G loss: 1.230774]\n",
      "epoch:19 step:17959 [D loss: 0.544266, acc.: 71.88%] [G loss: 0.925704]\n",
      "epoch:19 step:17960 [D loss: 0.653319, acc.: 57.03%] [G loss: 1.550136]\n",
      "epoch:19 step:17961 [D loss: 0.582694, acc.: 67.19%] [G loss: 1.008080]\n",
      "epoch:19 step:17962 [D loss: 0.596530, acc.: 62.50%] [G loss: 0.849638]\n",
      "epoch:19 step:17963 [D loss: 0.486877, acc.: 78.12%] [G loss: 0.950196]\n",
      "epoch:19 step:17964 [D loss: 0.602556, acc.: 67.97%] [G loss: 1.057439]\n",
      "epoch:19 step:17965 [D loss: 0.587256, acc.: 71.09%] [G loss: 1.057534]\n",
      "epoch:19 step:17966 [D loss: 0.484907, acc.: 78.91%] [G loss: 1.528858]\n",
      "epoch:19 step:17967 [D loss: 0.470578, acc.: 81.25%] [G loss: 1.718704]\n",
      "epoch:19 step:17968 [D loss: 0.616382, acc.: 65.62%] [G loss: 1.120999]\n",
      "epoch:19 step:17969 [D loss: 0.494769, acc.: 75.00%] [G loss: 1.346328]\n",
      "epoch:19 step:17970 [D loss: 0.469615, acc.: 83.59%] [G loss: 1.292178]\n",
      "epoch:19 step:17971 [D loss: 0.548481, acc.: 72.66%] [G loss: 1.471065]\n",
      "epoch:19 step:17972 [D loss: 0.592223, acc.: 68.75%] [G loss: 0.769719]\n",
      "epoch:19 step:17973 [D loss: 0.656639, acc.: 55.47%] [G loss: 1.165228]\n",
      "epoch:19 step:17974 [D loss: 0.823528, acc.: 48.44%] [G loss: 1.445605]\n",
      "epoch:19 step:17975 [D loss: 0.639409, acc.: 60.94%] [G loss: 1.344280]\n",
      "epoch:19 step:17976 [D loss: 0.617720, acc.: 63.28%] [G loss: 0.651085]\n",
      "epoch:19 step:17977 [D loss: 0.717094, acc.: 53.12%] [G loss: 0.842088]\n",
      "epoch:19 step:17978 [D loss: 0.844195, acc.: 42.97%] [G loss: 1.088105]\n",
      "epoch:19 step:17979 [D loss: 0.878768, acc.: 39.84%] [G loss: 0.825224]\n",
      "epoch:19 step:17980 [D loss: 0.493538, acc.: 75.78%] [G loss: 1.481314]\n",
      "epoch:19 step:17981 [D loss: 0.385227, acc.: 88.28%] [G loss: 1.117934]\n",
      "epoch:19 step:17982 [D loss: 0.800153, acc.: 39.06%] [G loss: 1.158378]\n",
      "epoch:19 step:17983 [D loss: 0.631421, acc.: 69.53%] [G loss: 1.344411]\n",
      "epoch:19 step:17984 [D loss: 0.545766, acc.: 71.88%] [G loss: 1.437522]\n",
      "epoch:19 step:17985 [D loss: 0.605811, acc.: 66.41%] [G loss: 0.864587]\n",
      "epoch:19 step:17986 [D loss: 0.918245, acc.: 29.69%] [G loss: 1.034018]\n",
      "epoch:19 step:17987 [D loss: 0.647804, acc.: 63.28%] [G loss: 0.958723]\n",
      "epoch:19 step:17988 [D loss: 0.547495, acc.: 74.22%] [G loss: 0.930800]\n",
      "epoch:19 step:17989 [D loss: 0.717647, acc.: 60.16%] [G loss: 0.788722]\n",
      "epoch:19 step:17990 [D loss: 0.624028, acc.: 69.53%] [G loss: 1.177153]\n",
      "epoch:19 step:17991 [D loss: 0.611179, acc.: 63.28%] [G loss: 0.833155]\n",
      "epoch:19 step:17992 [D loss: 0.549050, acc.: 75.78%] [G loss: 1.641115]\n",
      "epoch:19 step:17993 [D loss: 0.585535, acc.: 73.44%] [G loss: 1.156184]\n",
      "epoch:19 step:17994 [D loss: 0.495769, acc.: 74.22%] [G loss: 0.903292]\n",
      "epoch:19 step:17995 [D loss: 0.663931, acc.: 64.84%] [G loss: 1.269997]\n",
      "epoch:19 step:17996 [D loss: 0.623403, acc.: 67.19%] [G loss: 0.976494]\n",
      "epoch:19 step:17997 [D loss: 0.661907, acc.: 64.06%] [G loss: 1.616491]\n",
      "epoch:19 step:17998 [D loss: 0.492687, acc.: 82.03%] [G loss: 1.147217]\n",
      "epoch:19 step:17999 [D loss: 0.571315, acc.: 69.53%] [G loss: 1.122461]\n",
      "epoch:19 step:18000 [D loss: 0.563247, acc.: 67.97%] [G loss: 0.874988]\n",
      "epoch:19 step:18001 [D loss: 0.386685, acc.: 88.28%] [G loss: 0.911249]\n",
      "epoch:19 step:18002 [D loss: 0.587009, acc.: 67.19%] [G loss: 1.267020]\n",
      "epoch:19 step:18003 [D loss: 0.390453, acc.: 89.84%] [G loss: 1.028432]\n",
      "epoch:19 step:18004 [D loss: 0.430686, acc.: 86.72%] [G loss: 1.030399]\n",
      "epoch:19 step:18005 [D loss: 0.527451, acc.: 70.31%] [G loss: 1.042407]\n",
      "epoch:19 step:18006 [D loss: 0.706109, acc.: 50.78%] [G loss: 1.585520]\n",
      "epoch:19 step:18007 [D loss: 0.426259, acc.: 87.50%] [G loss: 1.660071]\n",
      "epoch:19 step:18008 [D loss: 0.521448, acc.: 75.00%] [G loss: 1.271945]\n",
      "epoch:19 step:18009 [D loss: 0.771735, acc.: 46.09%] [G loss: 1.871359]\n",
      "epoch:19 step:18010 [D loss: 0.483216, acc.: 80.47%] [G loss: 1.408226]\n",
      "epoch:19 step:18011 [D loss: 0.524175, acc.: 71.88%] [G loss: 1.235822]\n",
      "epoch:19 step:18012 [D loss: 0.657049, acc.: 57.03%] [G loss: 1.291633]\n",
      "epoch:19 step:18013 [D loss: 0.659947, acc.: 57.81%] [G loss: 0.809473]\n",
      "epoch:19 step:18014 [D loss: 0.586837, acc.: 73.44%] [G loss: 0.952869]\n",
      "epoch:19 step:18015 [D loss: 0.846128, acc.: 48.44%] [G loss: 0.932898]\n",
      "epoch:19 step:18016 [D loss: 0.482624, acc.: 82.03%] [G loss: 0.591461]\n",
      "epoch:19 step:18017 [D loss: 0.605698, acc.: 70.31%] [G loss: 0.852914]\n",
      "epoch:19 step:18018 [D loss: 0.586657, acc.: 65.62%] [G loss: 1.441751]\n",
      "epoch:19 step:18019 [D loss: 0.358039, acc.: 92.97%] [G loss: 0.897087]\n",
      "epoch:19 step:18020 [D loss: 0.337003, acc.: 94.53%] [G loss: 0.809388]\n",
      "epoch:19 step:18021 [D loss: 0.599995, acc.: 71.09%] [G loss: 0.811562]\n",
      "epoch:19 step:18022 [D loss: 0.713505, acc.: 60.94%] [G loss: 0.988680]\n",
      "epoch:19 step:18023 [D loss: 0.699382, acc.: 57.03%] [G loss: 1.420554]\n",
      "epoch:19 step:18024 [D loss: 0.488073, acc.: 81.25%] [G loss: 1.021255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18025 [D loss: 0.821068, acc.: 40.62%] [G loss: 0.887978]\n",
      "epoch:19 step:18026 [D loss: 0.478868, acc.: 84.38%] [G loss: 0.835713]\n",
      "epoch:19 step:18027 [D loss: 0.292778, acc.: 95.31%] [G loss: 1.630159]\n",
      "epoch:19 step:18028 [D loss: 0.845593, acc.: 48.44%] [G loss: 0.647664]\n",
      "epoch:19 step:18029 [D loss: 0.588484, acc.: 66.41%] [G loss: 1.001738]\n",
      "epoch:19 step:18030 [D loss: 0.357857, acc.: 92.19%] [G loss: 1.455845]\n",
      "epoch:19 step:18031 [D loss: 0.564034, acc.: 75.00%] [G loss: 1.430422]\n",
      "epoch:19 step:18032 [D loss: 0.796304, acc.: 35.94%] [G loss: 1.472620]\n",
      "epoch:19 step:18033 [D loss: 0.430257, acc.: 87.50%] [G loss: 1.331747]\n",
      "epoch:19 step:18034 [D loss: 0.439485, acc.: 82.81%] [G loss: 1.367410]\n",
      "epoch:19 step:18035 [D loss: 0.542047, acc.: 71.88%] [G loss: 0.931628]\n",
      "epoch:19 step:18036 [D loss: 0.469491, acc.: 85.16%] [G loss: 1.385635]\n",
      "epoch:19 step:18037 [D loss: 0.436650, acc.: 82.03%] [G loss: 1.494215]\n",
      "epoch:19 step:18038 [D loss: 0.557748, acc.: 69.53%] [G loss: 1.237696]\n",
      "epoch:19 step:18039 [D loss: 0.625179, acc.: 64.06%] [G loss: 1.626211]\n",
      "epoch:19 step:18040 [D loss: 0.703580, acc.: 50.00%] [G loss: 2.445931]\n",
      "epoch:19 step:18041 [D loss: 0.750899, acc.: 50.00%] [G loss: 1.119417]\n",
      "epoch:19 step:18042 [D loss: 0.448991, acc.: 80.47%] [G loss: 1.288068]\n",
      "epoch:19 step:18043 [D loss: 0.732707, acc.: 53.91%] [G loss: 1.207031]\n",
      "epoch:19 step:18044 [D loss: 0.475961, acc.: 76.56%] [G loss: 1.648478]\n",
      "epoch:19 step:18045 [D loss: 0.504008, acc.: 81.25%] [G loss: 0.780802]\n",
      "epoch:19 step:18046 [D loss: 0.329496, acc.: 93.75%] [G loss: 2.158754]\n",
      "epoch:19 step:18047 [D loss: 0.473649, acc.: 81.25%] [G loss: 0.825229]\n",
      "epoch:19 step:18048 [D loss: 0.638286, acc.: 66.41%] [G loss: 1.261211]\n",
      "epoch:19 step:18049 [D loss: 0.933750, acc.: 45.31%] [G loss: 1.321489]\n",
      "epoch:19 step:18050 [D loss: 0.617790, acc.: 60.94%] [G loss: 1.033885]\n",
      "epoch:19 step:18051 [D loss: 0.619566, acc.: 58.59%] [G loss: 1.625607]\n",
      "epoch:19 step:18052 [D loss: 0.859528, acc.: 41.41%] [G loss: 1.086250]\n",
      "epoch:19 step:18053 [D loss: 0.647711, acc.: 59.38%] [G loss: 1.177839]\n",
      "epoch:19 step:18054 [D loss: 0.605083, acc.: 66.41%] [G loss: 1.207650]\n",
      "epoch:19 step:18055 [D loss: 0.619590, acc.: 64.84%] [G loss: 0.968864]\n",
      "epoch:19 step:18056 [D loss: 0.715887, acc.: 52.34%] [G loss: 2.334614]\n",
      "epoch:19 step:18057 [D loss: 0.533545, acc.: 73.44%] [G loss: 1.574444]\n",
      "epoch:19 step:18058 [D loss: 0.480326, acc.: 78.91%] [G loss: 1.437160]\n",
      "epoch:19 step:18059 [D loss: 0.522603, acc.: 71.09%] [G loss: 1.121642]\n",
      "epoch:19 step:18060 [D loss: 0.567159, acc.: 72.66%] [G loss: 1.386115]\n",
      "epoch:19 step:18061 [D loss: 0.640036, acc.: 58.59%] [G loss: 1.424780]\n",
      "epoch:19 step:18062 [D loss: 0.548644, acc.: 73.44%] [G loss: 2.014734]\n",
      "epoch:19 step:18063 [D loss: 0.578444, acc.: 67.97%] [G loss: 1.258768]\n",
      "epoch:19 step:18064 [D loss: 0.769027, acc.: 50.78%] [G loss: 1.284527]\n",
      "epoch:19 step:18065 [D loss: 0.537509, acc.: 76.56%] [G loss: 1.393704]\n",
      "epoch:19 step:18066 [D loss: 0.458782, acc.: 82.03%] [G loss: 1.604031]\n",
      "epoch:19 step:18067 [D loss: 0.502910, acc.: 74.22%] [G loss: 1.664909]\n",
      "epoch:19 step:18068 [D loss: 0.526927, acc.: 71.88%] [G loss: 1.443963]\n",
      "epoch:19 step:18069 [D loss: 0.567096, acc.: 73.44%] [G loss: 1.588903]\n",
      "epoch:19 step:18070 [D loss: 0.513902, acc.: 73.44%] [G loss: 1.118393]\n",
      "epoch:19 step:18071 [D loss: 0.544490, acc.: 75.00%] [G loss: 1.193321]\n",
      "epoch:19 step:18072 [D loss: 0.561936, acc.: 72.66%] [G loss: 1.022931]\n",
      "epoch:19 step:18073 [D loss: 0.613810, acc.: 76.56%] [G loss: 1.533064]\n",
      "epoch:19 step:18074 [D loss: 0.439168, acc.: 81.25%] [G loss: 1.319415]\n",
      "epoch:19 step:18075 [D loss: 0.435335, acc.: 87.50%] [G loss: 1.339934]\n",
      "epoch:19 step:18076 [D loss: 0.513217, acc.: 78.12%] [G loss: 1.338538]\n",
      "epoch:19 step:18077 [D loss: 0.618736, acc.: 62.50%] [G loss: 0.997446]\n",
      "epoch:19 step:18078 [D loss: 0.630400, acc.: 63.28%] [G loss: 1.402109]\n",
      "epoch:19 step:18079 [D loss: 0.603463, acc.: 71.09%] [G loss: 1.014222]\n",
      "epoch:19 step:18080 [D loss: 0.584195, acc.: 74.22%] [G loss: 1.221513]\n",
      "epoch:19 step:18081 [D loss: 0.520145, acc.: 71.88%] [G loss: 1.018774]\n",
      "epoch:19 step:18082 [D loss: 0.723745, acc.: 56.25%] [G loss: 1.295337]\n",
      "epoch:19 step:18083 [D loss: 0.549027, acc.: 67.19%] [G loss: 1.630690]\n",
      "epoch:19 step:18084 [D loss: 0.684264, acc.: 61.72%] [G loss: 0.887730]\n",
      "epoch:19 step:18085 [D loss: 0.561121, acc.: 69.53%] [G loss: 1.419932]\n",
      "epoch:19 step:18086 [D loss: 0.359351, acc.: 89.06%] [G loss: 1.650513]\n",
      "epoch:19 step:18087 [D loss: 0.532211, acc.: 71.88%] [G loss: 1.285766]\n",
      "epoch:19 step:18088 [D loss: 0.686226, acc.: 57.03%] [G loss: 1.696171]\n",
      "epoch:19 step:18089 [D loss: 0.331505, acc.: 89.06%] [G loss: 1.173464]\n",
      "epoch:19 step:18090 [D loss: 0.614268, acc.: 64.06%] [G loss: 1.803671]\n",
      "epoch:19 step:18091 [D loss: 0.614234, acc.: 62.50%] [G loss: 1.637287]\n",
      "epoch:19 step:18092 [D loss: 0.506410, acc.: 70.31%] [G loss: 0.839965]\n",
      "epoch:19 step:18093 [D loss: 0.618355, acc.: 63.28%] [G loss: 1.853361]\n",
      "epoch:19 step:18094 [D loss: 0.450404, acc.: 82.81%] [G loss: 1.073824]\n",
      "epoch:19 step:18095 [D loss: 0.503372, acc.: 80.47%] [G loss: 1.077250]\n",
      "epoch:19 step:18096 [D loss: 0.658748, acc.: 57.81%] [G loss: 1.176721]\n",
      "epoch:19 step:18097 [D loss: 0.500796, acc.: 77.34%] [G loss: 1.364685]\n",
      "epoch:19 step:18098 [D loss: 0.697682, acc.: 58.59%] [G loss: 0.848543]\n",
      "epoch:19 step:18099 [D loss: 0.601525, acc.: 65.62%] [G loss: 1.029009]\n",
      "epoch:19 step:18100 [D loss: 0.540536, acc.: 71.09%] [G loss: 0.971088]\n",
      "epoch:19 step:18101 [D loss: 0.663997, acc.: 67.19%] [G loss: 1.141908]\n",
      "epoch:19 step:18102 [D loss: 0.464104, acc.: 81.25%] [G loss: 1.146163]\n",
      "epoch:19 step:18103 [D loss: 0.633925, acc.: 61.72%] [G loss: 1.582689]\n",
      "epoch:19 step:18104 [D loss: 0.644746, acc.: 65.62%] [G loss: 0.936005]\n",
      "epoch:19 step:18105 [D loss: 0.520233, acc.: 75.78%] [G loss: 1.126929]\n",
      "epoch:19 step:18106 [D loss: 0.693682, acc.: 60.94%] [G loss: 1.483820]\n",
      "epoch:19 step:18107 [D loss: 0.547461, acc.: 75.78%] [G loss: 1.717736]\n",
      "epoch:19 step:18108 [D loss: 0.580419, acc.: 69.53%] [G loss: 1.700189]\n",
      "epoch:19 step:18109 [D loss: 0.461885, acc.: 82.03%] [G loss: 1.201132]\n",
      "epoch:19 step:18110 [D loss: 0.613630, acc.: 68.75%] [G loss: 1.565758]\n",
      "epoch:19 step:18111 [D loss: 0.545484, acc.: 71.88%] [G loss: 1.686862]\n",
      "epoch:19 step:18112 [D loss: 0.344372, acc.: 80.47%] [G loss: 0.782908]\n",
      "epoch:19 step:18113 [D loss: 0.793221, acc.: 50.00%] [G loss: 0.674895]\n",
      "epoch:19 step:18114 [D loss: 0.498255, acc.: 78.91%] [G loss: 1.068244]\n",
      "epoch:19 step:18115 [D loss: 0.493789, acc.: 78.12%] [G loss: 1.789533]\n",
      "epoch:19 step:18116 [D loss: 0.642227, acc.: 61.72%] [G loss: 1.290651]\n",
      "epoch:19 step:18117 [D loss: 0.699167, acc.: 52.34%] [G loss: 0.990999]\n",
      "epoch:19 step:18118 [D loss: 0.444743, acc.: 82.81%] [G loss: 1.056488]\n",
      "epoch:19 step:18119 [D loss: 0.887180, acc.: 38.28%] [G loss: 0.689521]\n",
      "epoch:19 step:18120 [D loss: 0.349347, acc.: 89.84%] [G loss: 0.848794]\n",
      "epoch:19 step:18121 [D loss: 0.520802, acc.: 72.66%] [G loss: 0.923768]\n",
      "epoch:19 step:18122 [D loss: 0.733721, acc.: 54.69%] [G loss: 1.063148]\n",
      "epoch:19 step:18123 [D loss: 0.548064, acc.: 71.88%] [G loss: 0.965790]\n",
      "epoch:19 step:18124 [D loss: 0.614607, acc.: 64.06%] [G loss: 1.158103]\n",
      "epoch:19 step:18125 [D loss: 0.383098, acc.: 89.84%] [G loss: 1.957362]\n",
      "epoch:19 step:18126 [D loss: 0.501803, acc.: 77.34%] [G loss: 1.273623]\n",
      "epoch:19 step:18127 [D loss: 0.307599, acc.: 97.66%] [G loss: 1.434461]\n",
      "epoch:19 step:18128 [D loss: 0.395443, acc.: 88.28%] [G loss: 1.598161]\n",
      "epoch:19 step:18129 [D loss: 0.517992, acc.: 75.78%] [G loss: 2.420203]\n",
      "epoch:19 step:18130 [D loss: 0.720118, acc.: 57.81%] [G loss: 1.126938]\n",
      "epoch:19 step:18131 [D loss: 0.524806, acc.: 79.69%] [G loss: 1.917134]\n",
      "epoch:19 step:18132 [D loss: 0.494329, acc.: 81.25%] [G loss: 1.656595]\n",
      "epoch:19 step:18133 [D loss: 0.545076, acc.: 75.00%] [G loss: 1.949141]\n",
      "epoch:19 step:18134 [D loss: 0.387196, acc.: 88.28%] [G loss: 2.141937]\n",
      "epoch:19 step:18135 [D loss: 0.433693, acc.: 81.25%] [G loss: 1.342238]\n",
      "epoch:19 step:18136 [D loss: 0.487234, acc.: 78.91%] [G loss: 0.802775]\n",
      "epoch:19 step:18137 [D loss: 0.530078, acc.: 75.00%] [G loss: 1.033212]\n",
      "epoch:19 step:18138 [D loss: 0.618917, acc.: 60.16%] [G loss: 0.649788]\n",
      "epoch:19 step:18139 [D loss: 0.483073, acc.: 75.00%] [G loss: 2.393555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18140 [D loss: 0.735981, acc.: 50.78%] [G loss: 1.191807]\n",
      "epoch:19 step:18141 [D loss: 0.377240, acc.: 88.28%] [G loss: 1.253602]\n",
      "epoch:19 step:18142 [D loss: 0.564836, acc.: 71.88%] [G loss: 1.158917]\n",
      "epoch:19 step:18143 [D loss: 1.027330, acc.: 32.81%] [G loss: 1.205669]\n",
      "epoch:19 step:18144 [D loss: 0.543478, acc.: 70.31%] [G loss: 1.896305]\n",
      "epoch:19 step:18145 [D loss: 0.582502, acc.: 65.62%] [G loss: 0.924744]\n",
      "epoch:19 step:18146 [D loss: 0.588654, acc.: 64.84%] [G loss: 1.002850]\n",
      "epoch:19 step:18147 [D loss: 0.621012, acc.: 58.59%] [G loss: 1.191499]\n",
      "epoch:19 step:18148 [D loss: 0.456248, acc.: 78.12%] [G loss: 1.174167]\n",
      "epoch:19 step:18149 [D loss: 0.701460, acc.: 60.16%] [G loss: 1.174226]\n",
      "epoch:19 step:18150 [D loss: 0.565069, acc.: 69.53%] [G loss: 1.371311]\n",
      "epoch:19 step:18151 [D loss: 0.625231, acc.: 60.94%] [G loss: 1.899893]\n",
      "epoch:19 step:18152 [D loss: 0.462859, acc.: 81.25%] [G loss: 1.372259]\n",
      "epoch:19 step:18153 [D loss: 0.631141, acc.: 57.03%] [G loss: 1.547272]\n",
      "epoch:19 step:18154 [D loss: 0.742472, acc.: 57.03%] [G loss: 1.335061]\n",
      "epoch:19 step:18155 [D loss: 0.750231, acc.: 46.88%] [G loss: 1.056309]\n",
      "epoch:19 step:18156 [D loss: 0.691021, acc.: 61.72%] [G loss: 1.369356]\n",
      "epoch:19 step:18157 [D loss: 0.385234, acc.: 85.16%] [G loss: 1.153106]\n",
      "epoch:19 step:18158 [D loss: 0.465438, acc.: 75.78%] [G loss: 1.591392]\n",
      "epoch:19 step:18159 [D loss: 0.933743, acc.: 35.16%] [G loss: 1.419142]\n",
      "epoch:19 step:18160 [D loss: 0.617644, acc.: 64.06%] [G loss: 1.163113]\n",
      "epoch:19 step:18161 [D loss: 0.639540, acc.: 60.16%] [G loss: 1.281647]\n",
      "epoch:19 step:18162 [D loss: 0.665000, acc.: 60.16%] [G loss: 1.133242]\n",
      "epoch:19 step:18163 [D loss: 0.534303, acc.: 76.56%] [G loss: 1.272769]\n",
      "epoch:19 step:18164 [D loss: 0.478667, acc.: 80.47%] [G loss: 1.219684]\n",
      "epoch:19 step:18165 [D loss: 0.621926, acc.: 73.44%] [G loss: 1.074479]\n",
      "epoch:19 step:18166 [D loss: 0.195877, acc.: 98.44%] [G loss: 1.306428]\n",
      "epoch:19 step:18167 [D loss: 0.610409, acc.: 64.84%] [G loss: 0.710012]\n",
      "epoch:19 step:18168 [D loss: 0.621200, acc.: 63.28%] [G loss: 1.081035]\n",
      "epoch:19 step:18169 [D loss: 0.589191, acc.: 66.41%] [G loss: 1.207926]\n",
      "epoch:19 step:18170 [D loss: 0.663284, acc.: 67.97%] [G loss: 2.024002]\n",
      "epoch:19 step:18171 [D loss: 0.715447, acc.: 57.81%] [G loss: 1.508053]\n",
      "epoch:19 step:18172 [D loss: 0.697578, acc.: 58.59%] [G loss: 1.252572]\n",
      "epoch:19 step:18173 [D loss: 0.463585, acc.: 78.12%] [G loss: 1.151153]\n",
      "epoch:19 step:18174 [D loss: 0.322125, acc.: 92.19%] [G loss: 2.182320]\n",
      "epoch:19 step:18175 [D loss: 0.424791, acc.: 84.38%] [G loss: 1.061239]\n",
      "epoch:19 step:18176 [D loss: 0.939447, acc.: 46.88%] [G loss: 1.122334]\n",
      "epoch:19 step:18177 [D loss: 0.676651, acc.: 57.03%] [G loss: 1.383925]\n",
      "epoch:19 step:18178 [D loss: 0.474221, acc.: 84.38%] [G loss: 1.216733]\n",
      "epoch:19 step:18179 [D loss: 0.583268, acc.: 67.19%] [G loss: 0.696404]\n",
      "epoch:19 step:18180 [D loss: 0.496146, acc.: 81.25%] [G loss: 1.709429]\n",
      "epoch:19 step:18181 [D loss: 0.844775, acc.: 41.41%] [G loss: 1.416327]\n",
      "epoch:19 step:18182 [D loss: 0.525328, acc.: 73.44%] [G loss: 1.285551]\n",
      "epoch:19 step:18183 [D loss: 0.736705, acc.: 59.38%] [G loss: 1.994877]\n",
      "epoch:19 step:18184 [D loss: 0.428360, acc.: 85.94%] [G loss: 1.068456]\n",
      "epoch:19 step:18185 [D loss: 1.057779, acc.: 31.25%] [G loss: 1.719588]\n",
      "epoch:19 step:18186 [D loss: 0.724008, acc.: 62.50%] [G loss: 1.412053]\n",
      "epoch:19 step:18187 [D loss: 0.677242, acc.: 56.25%] [G loss: 1.441915]\n",
      "epoch:19 step:18188 [D loss: 0.703115, acc.: 56.25%] [G loss: 1.380195]\n",
      "epoch:19 step:18189 [D loss: 0.746631, acc.: 51.56%] [G loss: 1.248627]\n",
      "epoch:19 step:18190 [D loss: 0.663265, acc.: 53.91%] [G loss: 0.923394]\n",
      "epoch:19 step:18191 [D loss: 0.497745, acc.: 81.25%] [G loss: 1.203082]\n",
      "epoch:19 step:18192 [D loss: 0.767146, acc.: 44.53%] [G loss: 0.969179]\n",
      "epoch:19 step:18193 [D loss: 0.586871, acc.: 67.97%] [G loss: 1.033979]\n",
      "epoch:19 step:18194 [D loss: 0.563246, acc.: 71.09%] [G loss: 1.111960]\n",
      "epoch:19 step:18195 [D loss: 0.915609, acc.: 34.38%] [G loss: 1.334144]\n",
      "epoch:19 step:18196 [D loss: 0.628069, acc.: 56.25%] [G loss: 1.383109]\n",
      "epoch:19 step:18197 [D loss: 0.733312, acc.: 50.78%] [G loss: 1.411317]\n",
      "epoch:19 step:18198 [D loss: 0.516502, acc.: 81.25%] [G loss: 1.161093]\n",
      "epoch:19 step:18199 [D loss: 0.720101, acc.: 43.75%] [G loss: 1.131483]\n",
      "epoch:19 step:18200 [D loss: 0.554105, acc.: 78.12%] [G loss: 1.219919]\n",
      "epoch:19 step:18201 [D loss: 0.643137, acc.: 60.16%] [G loss: 1.149430]\n",
      "epoch:19 step:18202 [D loss: 0.519303, acc.: 78.91%] [G loss: 1.538175]\n",
      "epoch:19 step:18203 [D loss: 0.668634, acc.: 57.03%] [G loss: 1.004026]\n",
      "epoch:19 step:18204 [D loss: 0.412528, acc.: 89.06%] [G loss: 1.417154]\n",
      "epoch:19 step:18205 [D loss: 0.685299, acc.: 64.84%] [G loss: 1.591574]\n",
      "epoch:19 step:18206 [D loss: 0.444561, acc.: 81.25%] [G loss: 1.333688]\n",
      "epoch:19 step:18207 [D loss: 0.421423, acc.: 84.38%] [G loss: 1.302671]\n",
      "epoch:19 step:18208 [D loss: 0.697088, acc.: 55.47%] [G loss: 1.021095]\n",
      "epoch:19 step:18209 [D loss: 0.549481, acc.: 72.66%] [G loss: 1.685135]\n",
      "epoch:19 step:18210 [D loss: 0.602991, acc.: 64.84%] [G loss: 1.072663]\n",
      "epoch:19 step:18211 [D loss: 0.573650, acc.: 66.41%] [G loss: 0.844974]\n",
      "epoch:19 step:18212 [D loss: 0.451208, acc.: 79.69%] [G loss: 1.287977]\n",
      "epoch:19 step:18213 [D loss: 0.802756, acc.: 42.97%] [G loss: 1.165163]\n",
      "epoch:19 step:18214 [D loss: 0.434648, acc.: 86.72%] [G loss: 0.910402]\n",
      "epoch:19 step:18215 [D loss: 0.732239, acc.: 50.78%] [G loss: 1.103565]\n",
      "epoch:19 step:18216 [D loss: 0.907380, acc.: 40.62%] [G loss: 1.249773]\n",
      "epoch:19 step:18217 [D loss: 0.525381, acc.: 74.22%] [G loss: 1.034729]\n",
      "epoch:19 step:18218 [D loss: 0.447084, acc.: 83.59%] [G loss: 1.409613]\n",
      "epoch:19 step:18219 [D loss: 0.521765, acc.: 76.56%] [G loss: 1.794273]\n",
      "epoch:19 step:18220 [D loss: 0.462114, acc.: 83.59%] [G loss: 0.880040]\n",
      "epoch:19 step:18221 [D loss: 0.727274, acc.: 50.78%] [G loss: 0.940288]\n",
      "epoch:19 step:18222 [D loss: 0.634275, acc.: 62.50%] [G loss: 1.244242]\n",
      "epoch:19 step:18223 [D loss: 0.616752, acc.: 64.84%] [G loss: 1.298293]\n",
      "epoch:19 step:18224 [D loss: 0.697255, acc.: 57.81%] [G loss: 1.659776]\n",
      "epoch:19 step:18225 [D loss: 0.592560, acc.: 65.62%] [G loss: 1.260524]\n",
      "epoch:19 step:18226 [D loss: 0.597865, acc.: 68.75%] [G loss: 1.319640]\n",
      "epoch:19 step:18227 [D loss: 0.623501, acc.: 62.50%] [G loss: 1.058084]\n",
      "epoch:19 step:18228 [D loss: 0.555786, acc.: 75.78%] [G loss: 1.331806]\n",
      "epoch:19 step:18229 [D loss: 0.426124, acc.: 85.16%] [G loss: 1.436245]\n",
      "epoch:19 step:18230 [D loss: 0.520495, acc.: 78.12%] [G loss: 1.247582]\n",
      "epoch:19 step:18231 [D loss: 0.569669, acc.: 72.66%] [G loss: 1.098108]\n",
      "epoch:19 step:18232 [D loss: 0.555484, acc.: 71.09%] [G loss: 1.089922]\n",
      "epoch:19 step:18233 [D loss: 0.670480, acc.: 57.03%] [G loss: 1.249009]\n",
      "epoch:19 step:18234 [D loss: 0.726387, acc.: 52.34%] [G loss: 0.900194]\n",
      "epoch:19 step:18235 [D loss: 0.860911, acc.: 46.88%] [G loss: 1.104784]\n",
      "epoch:19 step:18236 [D loss: 0.535363, acc.: 67.19%] [G loss: 1.089556]\n",
      "epoch:19 step:18237 [D loss: 0.697670, acc.: 58.59%] [G loss: 1.207300]\n",
      "epoch:19 step:18238 [D loss: 0.521518, acc.: 73.44%] [G loss: 1.370104]\n",
      "epoch:19 step:18239 [D loss: 0.615004, acc.: 69.53%] [G loss: 1.276235]\n",
      "epoch:19 step:18240 [D loss: 0.529285, acc.: 78.12%] [G loss: 1.154686]\n",
      "epoch:19 step:18241 [D loss: 0.654807, acc.: 60.94%] [G loss: 0.867668]\n",
      "epoch:19 step:18242 [D loss: 0.444529, acc.: 81.25%] [G loss: 0.910672]\n",
      "epoch:19 step:18243 [D loss: 0.556769, acc.: 71.09%] [G loss: 0.984865]\n",
      "epoch:19 step:18244 [D loss: 0.594766, acc.: 63.28%] [G loss: 1.317029]\n",
      "epoch:19 step:18245 [D loss: 0.557117, acc.: 72.66%] [G loss: 0.942494]\n",
      "epoch:19 step:18246 [D loss: 0.636945, acc.: 67.97%] [G loss: 1.023884]\n",
      "epoch:19 step:18247 [D loss: 0.676846, acc.: 54.69%] [G loss: 1.159891]\n",
      "epoch:19 step:18248 [D loss: 0.697754, acc.: 54.69%] [G loss: 1.132741]\n",
      "epoch:19 step:18249 [D loss: 0.683302, acc.: 57.03%] [G loss: 1.406411]\n",
      "epoch:19 step:18250 [D loss: 0.627617, acc.: 66.41%] [G loss: 1.498510]\n",
      "epoch:19 step:18251 [D loss: 0.485666, acc.: 72.66%] [G loss: 1.051971]\n",
      "epoch:19 step:18252 [D loss: 0.633940, acc.: 62.50%] [G loss: 0.949829]\n",
      "epoch:19 step:18253 [D loss: 0.534109, acc.: 71.09%] [G loss: 1.290989]\n",
      "epoch:19 step:18254 [D loss: 0.602309, acc.: 64.84%] [G loss: 1.298261]\n",
      "epoch:19 step:18255 [D loss: 0.676874, acc.: 59.38%] [G loss: 1.250262]\n",
      "epoch:19 step:18256 [D loss: 0.449506, acc.: 85.16%] [G loss: 1.116373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18257 [D loss: 0.494306, acc.: 78.91%] [G loss: 1.024413]\n",
      "epoch:19 step:18258 [D loss: 0.690415, acc.: 53.12%] [G loss: 1.020041]\n",
      "epoch:19 step:18259 [D loss: 0.628550, acc.: 66.41%] [G loss: 1.033306]\n",
      "epoch:19 step:18260 [D loss: 0.558892, acc.: 70.31%] [G loss: 1.186729]\n",
      "epoch:19 step:18261 [D loss: 0.695119, acc.: 50.78%] [G loss: 0.938933]\n",
      "epoch:19 step:18262 [D loss: 0.469469, acc.: 85.94%] [G loss: 1.265650]\n",
      "epoch:19 step:18263 [D loss: 0.467788, acc.: 83.59%] [G loss: 1.353872]\n",
      "epoch:19 step:18264 [D loss: 0.645228, acc.: 64.84%] [G loss: 1.384456]\n",
      "epoch:19 step:18265 [D loss: 0.473094, acc.: 83.59%] [G loss: 0.805637]\n",
      "epoch:19 step:18266 [D loss: 0.559969, acc.: 72.66%] [G loss: 0.999284]\n",
      "epoch:19 step:18267 [D loss: 0.449445, acc.: 83.59%] [G loss: 1.423955]\n",
      "epoch:19 step:18268 [D loss: 0.472961, acc.: 85.16%] [G loss: 1.056505]\n",
      "epoch:19 step:18269 [D loss: 0.481507, acc.: 78.91%] [G loss: 1.249544]\n",
      "epoch:19 step:18270 [D loss: 0.453262, acc.: 87.50%] [G loss: 1.160328]\n",
      "epoch:19 step:18271 [D loss: 0.499553, acc.: 84.38%] [G loss: 1.148027]\n",
      "epoch:19 step:18272 [D loss: 0.487722, acc.: 81.25%] [G loss: 1.465525]\n",
      "epoch:19 step:18273 [D loss: 0.495448, acc.: 82.03%] [G loss: 0.980556]\n",
      "epoch:19 step:18274 [D loss: 0.651912, acc.: 62.50%] [G loss: 1.125462]\n",
      "epoch:19 step:18275 [D loss: 0.461154, acc.: 82.03%] [G loss: 1.250789]\n",
      "epoch:19 step:18276 [D loss: 0.508331, acc.: 75.78%] [G loss: 0.872782]\n",
      "epoch:19 step:18277 [D loss: 0.521093, acc.: 80.47%] [G loss: 1.425463]\n",
      "epoch:19 step:18278 [D loss: 0.667610, acc.: 55.47%] [G loss: 0.804123]\n",
      "epoch:19 step:18279 [D loss: 0.626016, acc.: 63.28%] [G loss: 3.853308]\n",
      "epoch:19 step:18280 [D loss: 0.833238, acc.: 55.47%] [G loss: 0.994089]\n",
      "epoch:19 step:18281 [D loss: 0.654044, acc.: 60.94%] [G loss: 1.110882]\n",
      "epoch:19 step:18282 [D loss: 0.506681, acc.: 81.25%] [G loss: 1.112695]\n",
      "epoch:19 step:18283 [D loss: 0.680585, acc.: 59.38%] [G loss: 1.877942]\n",
      "epoch:19 step:18284 [D loss: 0.658572, acc.: 61.72%] [G loss: 1.426210]\n",
      "epoch:19 step:18285 [D loss: 0.382125, acc.: 88.28%] [G loss: 1.226478]\n",
      "epoch:19 step:18286 [D loss: 0.888172, acc.: 44.53%] [G loss: 1.397052]\n",
      "epoch:19 step:18287 [D loss: 0.808505, acc.: 46.09%] [G loss: 1.185948]\n",
      "epoch:19 step:18288 [D loss: 0.622830, acc.: 67.19%] [G loss: 1.090506]\n",
      "epoch:19 step:18289 [D loss: 0.578821, acc.: 68.75%] [G loss: 1.039421]\n",
      "epoch:19 step:18290 [D loss: 0.667471, acc.: 57.03%] [G loss: 1.234841]\n",
      "epoch:19 step:18291 [D loss: 0.486404, acc.: 78.91%] [G loss: 1.210867]\n",
      "epoch:19 step:18292 [D loss: 0.735733, acc.: 57.81%] [G loss: 1.164283]\n",
      "epoch:19 step:18293 [D loss: 0.616166, acc.: 64.06%] [G loss: 1.549698]\n",
      "epoch:19 step:18294 [D loss: 0.581410, acc.: 72.66%] [G loss: 1.492493]\n",
      "epoch:19 step:18295 [D loss: 0.730953, acc.: 46.09%] [G loss: 1.647627]\n",
      "epoch:19 step:18296 [D loss: 0.716198, acc.: 50.00%] [G loss: 1.159142]\n",
      "epoch:19 step:18297 [D loss: 0.490884, acc.: 79.69%] [G loss: 1.429139]\n",
      "epoch:19 step:18298 [D loss: 0.582984, acc.: 76.56%] [G loss: 1.137415]\n",
      "epoch:19 step:18299 [D loss: 0.498684, acc.: 82.03%] [G loss: 1.132966]\n",
      "epoch:19 step:18300 [D loss: 0.651274, acc.: 61.72%] [G loss: 1.255537]\n",
      "epoch:19 step:18301 [D loss: 0.494390, acc.: 79.69%] [G loss: 1.137827]\n",
      "epoch:19 step:18302 [D loss: 0.393280, acc.: 90.62%] [G loss: 1.443636]\n",
      "epoch:19 step:18303 [D loss: 0.612367, acc.: 65.62%] [G loss: 0.979241]\n",
      "epoch:19 step:18304 [D loss: 0.729450, acc.: 54.69%] [G loss: 1.068842]\n",
      "epoch:19 step:18305 [D loss: 0.551814, acc.: 75.00%] [G loss: 0.875513]\n",
      "epoch:19 step:18306 [D loss: 0.747585, acc.: 53.91%] [G loss: 1.726594]\n",
      "epoch:19 step:18307 [D loss: 0.495911, acc.: 81.25%] [G loss: 0.747430]\n",
      "epoch:19 step:18308 [D loss: 0.745952, acc.: 53.12%] [G loss: 2.139769]\n",
      "epoch:19 step:18309 [D loss: 0.743209, acc.: 57.81%] [G loss: 0.865076]\n",
      "epoch:19 step:18310 [D loss: 0.811091, acc.: 42.19%] [G loss: 1.038764]\n",
      "epoch:19 step:18311 [D loss: 0.548079, acc.: 74.22%] [G loss: 1.514107]\n",
      "epoch:19 step:18312 [D loss: 0.426871, acc.: 86.72%] [G loss: 1.277196]\n",
      "epoch:19 step:18313 [D loss: 0.418161, acc.: 87.50%] [G loss: 1.486448]\n",
      "epoch:19 step:18314 [D loss: 0.631075, acc.: 60.94%] [G loss: 1.320664]\n",
      "epoch:19 step:18315 [D loss: 0.789294, acc.: 49.22%] [G loss: 1.648192]\n",
      "epoch:19 step:18316 [D loss: 0.790958, acc.: 42.97%] [G loss: 1.421504]\n",
      "epoch:19 step:18317 [D loss: 0.570851, acc.: 71.09%] [G loss: 1.422480]\n",
      "epoch:19 step:18318 [D loss: 0.593200, acc.: 67.97%] [G loss: 1.366312]\n",
      "epoch:19 step:18319 [D loss: 0.644540, acc.: 62.50%] [G loss: 1.472547]\n",
      "epoch:19 step:18320 [D loss: 0.446130, acc.: 86.72%] [G loss: 1.359676]\n",
      "epoch:19 step:18321 [D loss: 0.417734, acc.: 88.28%] [G loss: 1.431429]\n",
      "epoch:19 step:18322 [D loss: 0.624262, acc.: 62.50%] [G loss: 1.244076]\n",
      "epoch:19 step:18323 [D loss: 0.755627, acc.: 53.91%] [G loss: 1.325240]\n",
      "epoch:19 step:18324 [D loss: 0.437383, acc.: 87.50%] [G loss: 1.439206]\n",
      "epoch:19 step:18325 [D loss: 0.592522, acc.: 65.62%] [G loss: 1.229761]\n",
      "epoch:19 step:18326 [D loss: 0.496561, acc.: 82.81%] [G loss: 1.383615]\n",
      "epoch:19 step:18327 [D loss: 0.461943, acc.: 82.81%] [G loss: 1.637390]\n",
      "epoch:19 step:18328 [D loss: 0.443971, acc.: 87.50%] [G loss: 0.938855]\n",
      "epoch:19 step:18329 [D loss: 0.462364, acc.: 83.59%] [G loss: 1.047127]\n",
      "epoch:19 step:18330 [D loss: 0.597813, acc.: 74.22%] [G loss: 1.155741]\n",
      "epoch:19 step:18331 [D loss: 0.553872, acc.: 67.97%] [G loss: 1.562060]\n",
      "epoch:19 step:18332 [D loss: 0.614345, acc.: 67.19%] [G loss: 1.097372]\n",
      "epoch:19 step:18333 [D loss: 0.655296, acc.: 61.72%] [G loss: 1.169764]\n",
      "epoch:19 step:18334 [D loss: 0.624703, acc.: 60.94%] [G loss: 1.137986]\n",
      "epoch:19 step:18335 [D loss: 0.473673, acc.: 82.03%] [G loss: 1.546154]\n",
      "epoch:19 step:18336 [D loss: 0.486738, acc.: 80.47%] [G loss: 0.829333]\n",
      "epoch:19 step:18337 [D loss: 0.621373, acc.: 67.97%] [G loss: 1.073461]\n",
      "epoch:19 step:18338 [D loss: 0.496583, acc.: 79.69%] [G loss: 1.519213]\n",
      "epoch:19 step:18339 [D loss: 0.631034, acc.: 64.84%] [G loss: 1.045519]\n",
      "epoch:19 step:18340 [D loss: 0.653579, acc.: 53.12%] [G loss: 0.804902]\n",
      "epoch:19 step:18341 [D loss: 0.459400, acc.: 82.81%] [G loss: 0.889699]\n",
      "epoch:19 step:18342 [D loss: 0.535706, acc.: 73.44%] [G loss: 1.868398]\n",
      "epoch:19 step:18343 [D loss: 0.431191, acc.: 85.16%] [G loss: 0.763976]\n",
      "epoch:19 step:18344 [D loss: 0.703624, acc.: 57.03%] [G loss: 1.592864]\n",
      "epoch:19 step:18345 [D loss: 0.829336, acc.: 49.22%] [G loss: 1.148758]\n",
      "epoch:19 step:18346 [D loss: 0.564201, acc.: 75.78%] [G loss: 1.141108]\n",
      "epoch:19 step:18347 [D loss: 0.669914, acc.: 60.16%] [G loss: 1.872004]\n",
      "epoch:19 step:18348 [D loss: 0.412803, acc.: 86.72%] [G loss: 0.982538]\n",
      "epoch:19 step:18349 [D loss: 0.546392, acc.: 73.44%] [G loss: 1.857487]\n",
      "epoch:19 step:18350 [D loss: 0.589552, acc.: 65.62%] [G loss: 1.212308]\n",
      "epoch:19 step:18351 [D loss: 0.613778, acc.: 65.62%] [G loss: 0.988765]\n",
      "epoch:19 step:18352 [D loss: 0.806999, acc.: 52.34%] [G loss: 1.162864]\n",
      "epoch:19 step:18353 [D loss: 0.760117, acc.: 46.88%] [G loss: 0.868941]\n",
      "epoch:19 step:18354 [D loss: 0.586288, acc.: 67.19%] [G loss: 0.805606]\n",
      "epoch:19 step:18355 [D loss: 0.776551, acc.: 46.88%] [G loss: 0.917529]\n",
      "epoch:19 step:18356 [D loss: 0.471833, acc.: 83.59%] [G loss: 1.350686]\n",
      "epoch:19 step:18357 [D loss: 0.621577, acc.: 64.06%] [G loss: 0.794554]\n",
      "epoch:19 step:18358 [D loss: 0.478970, acc.: 78.12%] [G loss: 0.868520]\n",
      "epoch:19 step:18359 [D loss: 0.392726, acc.: 88.28%] [G loss: 1.332627]\n",
      "epoch:19 step:18360 [D loss: 0.755779, acc.: 48.44%] [G loss: 1.466150]\n",
      "epoch:19 step:18361 [D loss: 0.596920, acc.: 64.06%] [G loss: 1.075725]\n",
      "epoch:19 step:18362 [D loss: 0.639896, acc.: 60.16%] [G loss: 1.151637]\n",
      "epoch:19 step:18363 [D loss: 0.745037, acc.: 56.25%] [G loss: 1.133389]\n",
      "epoch:19 step:18364 [D loss: 0.494500, acc.: 75.78%] [G loss: 1.588126]\n",
      "epoch:19 step:18365 [D loss: 0.615753, acc.: 64.06%] [G loss: 1.740451]\n",
      "epoch:19 step:18366 [D loss: 0.609681, acc.: 64.84%] [G loss: 1.075478]\n",
      "epoch:19 step:18367 [D loss: 0.508685, acc.: 77.34%] [G loss: 1.274326]\n",
      "epoch:19 step:18368 [D loss: 0.529493, acc.: 75.00%] [G loss: 1.393670]\n",
      "epoch:19 step:18369 [D loss: 0.652543, acc.: 62.50%] [G loss: 1.464106]\n",
      "epoch:19 step:18370 [D loss: 0.580810, acc.: 69.53%] [G loss: 1.419576]\n",
      "epoch:19 step:18371 [D loss: 0.547383, acc.: 71.88%] [G loss: 1.525151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18372 [D loss: 0.538570, acc.: 77.34%] [G loss: 1.954019]\n",
      "epoch:19 step:18373 [D loss: 0.719872, acc.: 53.12%] [G loss: 1.304538]\n",
      "epoch:19 step:18374 [D loss: 0.621338, acc.: 66.41%] [G loss: 1.203678]\n",
      "epoch:19 step:18375 [D loss: 0.534567, acc.: 72.66%] [G loss: 1.106251]\n",
      "epoch:19 step:18376 [D loss: 0.625707, acc.: 67.19%] [G loss: 1.786298]\n",
      "epoch:19 step:18377 [D loss: 0.836444, acc.: 42.19%] [G loss: 0.883213]\n",
      "epoch:19 step:18378 [D loss: 0.415423, acc.: 84.38%] [G loss: 0.973528]\n",
      "epoch:19 step:18379 [D loss: 0.493494, acc.: 77.34%] [G loss: 0.803550]\n",
      "epoch:19 step:18380 [D loss: 0.727038, acc.: 48.44%] [G loss: 1.148599]\n",
      "epoch:19 step:18381 [D loss: 0.500871, acc.: 79.69%] [G loss: 1.089961]\n",
      "epoch:19 step:18382 [D loss: 0.660209, acc.: 60.16%] [G loss: 1.052911]\n",
      "epoch:19 step:18383 [D loss: 0.784058, acc.: 50.78%] [G loss: 1.088261]\n",
      "epoch:19 step:18384 [D loss: 0.563101, acc.: 71.09%] [G loss: 1.227103]\n",
      "epoch:19 step:18385 [D loss: 0.550114, acc.: 71.09%] [G loss: 1.412048]\n",
      "epoch:19 step:18386 [D loss: 0.470168, acc.: 78.91%] [G loss: 1.155596]\n",
      "epoch:19 step:18387 [D loss: 0.608585, acc.: 65.62%] [G loss: 1.228546]\n",
      "epoch:19 step:18388 [D loss: 0.672412, acc.: 63.28%] [G loss: 1.630694]\n",
      "epoch:19 step:18389 [D loss: 0.445171, acc.: 82.03%] [G loss: 0.938357]\n",
      "epoch:19 step:18390 [D loss: 0.569147, acc.: 72.66%] [G loss: 1.354943]\n",
      "epoch:19 step:18391 [D loss: 0.481087, acc.: 81.25%] [G loss: 2.159819]\n",
      "epoch:19 step:18392 [D loss: 0.521447, acc.: 75.00%] [G loss: 1.237622]\n",
      "epoch:19 step:18393 [D loss: 0.637457, acc.: 57.81%] [G loss: 0.966670]\n",
      "epoch:19 step:18394 [D loss: 0.492270, acc.: 73.44%] [G loss: 1.146628]\n",
      "epoch:19 step:18395 [D loss: 0.553835, acc.: 71.88%] [G loss: 1.392555]\n",
      "epoch:19 step:18396 [D loss: 0.318943, acc.: 89.84%] [G loss: 1.323960]\n",
      "epoch:19 step:18397 [D loss: 0.653876, acc.: 57.81%] [G loss: 0.861571]\n",
      "epoch:19 step:18398 [D loss: 0.583550, acc.: 62.50%] [G loss: 1.217711]\n",
      "epoch:19 step:18399 [D loss: 0.456155, acc.: 83.59%] [G loss: 1.488517]\n",
      "epoch:19 step:18400 [D loss: 0.519730, acc.: 75.00%] [G loss: 1.692510]\n",
      "epoch:19 step:18401 [D loss: 0.461511, acc.: 82.81%] [G loss: 1.136883]\n",
      "epoch:19 step:18402 [D loss: 0.493641, acc.: 82.03%] [G loss: 1.057481]\n",
      "epoch:19 step:18403 [D loss: 0.656730, acc.: 60.94%] [G loss: 0.965970]\n",
      "epoch:19 step:18404 [D loss: 0.530306, acc.: 77.34%] [G loss: 1.694330]\n",
      "epoch:19 step:18405 [D loss: 0.318301, acc.: 89.84%] [G loss: 1.324193]\n",
      "epoch:19 step:18406 [D loss: 0.802277, acc.: 51.56%] [G loss: 1.260091]\n",
      "epoch:19 step:18407 [D loss: 0.831022, acc.: 43.75%] [G loss: 1.857175]\n",
      "epoch:19 step:18408 [D loss: 0.585259, acc.: 63.28%] [G loss: 1.873950]\n",
      "epoch:19 step:18409 [D loss: 0.698086, acc.: 60.94%] [G loss: 1.918655]\n",
      "epoch:19 step:18410 [D loss: 0.490555, acc.: 75.78%] [G loss: 1.409913]\n",
      "epoch:19 step:18411 [D loss: 0.669373, acc.: 57.81%] [G loss: 1.159354]\n",
      "epoch:19 step:18412 [D loss: 0.641224, acc.: 62.50%] [G loss: 1.662338]\n",
      "epoch:19 step:18413 [D loss: 0.429047, acc.: 84.38%] [G loss: 1.479514]\n",
      "epoch:19 step:18414 [D loss: 0.500264, acc.: 77.34%] [G loss: 1.452265]\n",
      "epoch:19 step:18415 [D loss: 0.349917, acc.: 90.62%] [G loss: 1.694038]\n",
      "epoch:19 step:18416 [D loss: 0.447011, acc.: 81.25%] [G loss: 1.140731]\n",
      "epoch:19 step:18417 [D loss: 0.421229, acc.: 86.72%] [G loss: 1.514024]\n",
      "epoch:19 step:18418 [D loss: 0.350175, acc.: 92.97%] [G loss: 1.881639]\n",
      "epoch:19 step:18419 [D loss: 0.411700, acc.: 86.72%] [G loss: 1.818275]\n",
      "epoch:19 step:18420 [D loss: 0.634723, acc.: 65.62%] [G loss: 1.221143]\n",
      "epoch:19 step:18421 [D loss: 0.335965, acc.: 91.41%] [G loss: 1.639043]\n",
      "epoch:19 step:18422 [D loss: 0.475417, acc.: 74.22%] [G loss: 1.102624]\n",
      "epoch:19 step:18423 [D loss: 0.414617, acc.: 85.16%] [G loss: 1.220218]\n",
      "epoch:19 step:18424 [D loss: 0.299176, acc.: 94.53%] [G loss: 1.561494]\n",
      "epoch:19 step:18425 [D loss: 0.623275, acc.: 65.62%] [G loss: 1.513649]\n",
      "epoch:19 step:18426 [D loss: 0.636472, acc.: 67.97%] [G loss: 1.604476]\n",
      "epoch:19 step:18427 [D loss: 0.468076, acc.: 77.34%] [G loss: 1.915117]\n",
      "epoch:19 step:18428 [D loss: 0.720385, acc.: 54.69%] [G loss: 1.519737]\n",
      "epoch:19 step:18429 [D loss: 0.582121, acc.: 68.75%] [G loss: 0.893565]\n",
      "epoch:19 step:18430 [D loss: 0.218361, acc.: 96.09%] [G loss: 0.856264]\n",
      "epoch:19 step:18431 [D loss: 0.610556, acc.: 68.75%] [G loss: 1.159339]\n",
      "epoch:19 step:18432 [D loss: 0.482823, acc.: 76.56%] [G loss: 1.799395]\n",
      "epoch:19 step:18433 [D loss: 0.361082, acc.: 86.72%] [G loss: 1.583376]\n",
      "epoch:19 step:18434 [D loss: 0.341208, acc.: 86.72%] [G loss: 0.789686]\n",
      "epoch:19 step:18435 [D loss: 0.642014, acc.: 61.72%] [G loss: 0.706286]\n",
      "epoch:19 step:18436 [D loss: 0.855870, acc.: 54.69%] [G loss: 1.239630]\n",
      "epoch:19 step:18437 [D loss: 0.770377, acc.: 48.44%] [G loss: 1.444937]\n",
      "epoch:19 step:18438 [D loss: 0.354128, acc.: 89.84%] [G loss: 1.298768]\n",
      "epoch:19 step:18439 [D loss: 0.852470, acc.: 50.00%] [G loss: 1.279219]\n",
      "epoch:19 step:18440 [D loss: 0.510210, acc.: 75.78%] [G loss: 1.372169]\n",
      "epoch:19 step:18441 [D loss: 0.629220, acc.: 64.84%] [G loss: 1.765323]\n",
      "epoch:19 step:18442 [D loss: 0.495861, acc.: 82.81%] [G loss: 1.787009]\n",
      "epoch:19 step:18443 [D loss: 0.685004, acc.: 60.16%] [G loss: 1.684546]\n",
      "epoch:19 step:18444 [D loss: 0.677664, acc.: 53.91%] [G loss: 1.462037]\n",
      "epoch:19 step:18445 [D loss: 0.632536, acc.: 66.41%] [G loss: 1.287591]\n",
      "epoch:19 step:18446 [D loss: 0.515546, acc.: 77.34%] [G loss: 0.636570]\n",
      "epoch:19 step:18447 [D loss: 0.685167, acc.: 52.34%] [G loss: 0.864004]\n",
      "epoch:19 step:18448 [D loss: 0.742408, acc.: 51.56%] [G loss: 0.805926]\n",
      "epoch:19 step:18449 [D loss: 1.067816, acc.: 51.56%] [G loss: 1.293938]\n",
      "epoch:19 step:18450 [D loss: 0.579400, acc.: 67.19%] [G loss: 1.279304]\n",
      "epoch:19 step:18451 [D loss: 0.440772, acc.: 74.22%] [G loss: 1.970431]\n",
      "epoch:19 step:18452 [D loss: 0.608449, acc.: 68.75%] [G loss: 1.331577]\n",
      "epoch:19 step:18453 [D loss: 0.697216, acc.: 54.69%] [G loss: 1.627491]\n",
      "epoch:19 step:18454 [D loss: 0.791271, acc.: 47.66%] [G loss: 1.112203]\n",
      "epoch:19 step:18455 [D loss: 0.467200, acc.: 77.34%] [G loss: 1.242717]\n",
      "epoch:19 step:18456 [D loss: 0.491054, acc.: 79.69%] [G loss: 1.408378]\n",
      "epoch:19 step:18457 [D loss: 0.459032, acc.: 78.91%] [G loss: 1.506206]\n",
      "epoch:19 step:18458 [D loss: 0.500283, acc.: 76.56%] [G loss: 1.588660]\n",
      "epoch:19 step:18459 [D loss: 0.751278, acc.: 42.97%] [G loss: 1.023258]\n",
      "epoch:19 step:18460 [D loss: 0.961471, acc.: 34.38%] [G loss: 1.414326]\n",
      "epoch:19 step:18461 [D loss: 0.766538, acc.: 53.12%] [G loss: 1.253134]\n",
      "epoch:19 step:18462 [D loss: 0.493154, acc.: 75.00%] [G loss: 1.291205]\n",
      "epoch:19 step:18463 [D loss: 0.270561, acc.: 97.66%] [G loss: 0.979546]\n",
      "epoch:19 step:18464 [D loss: 0.810633, acc.: 44.53%] [G loss: 1.369256]\n",
      "epoch:19 step:18465 [D loss: 0.648853, acc.: 53.12%] [G loss: 1.273335]\n",
      "epoch:19 step:18466 [D loss: 0.623643, acc.: 63.28%] [G loss: 1.227299]\n",
      "epoch:19 step:18467 [D loss: 0.472663, acc.: 83.59%] [G loss: 1.423018]\n",
      "epoch:19 step:18468 [D loss: 0.435240, acc.: 85.16%] [G loss: 1.328635]\n",
      "epoch:19 step:18469 [D loss: 0.485501, acc.: 81.25%] [G loss: 1.243132]\n",
      "epoch:19 step:18470 [D loss: 0.593951, acc.: 67.97%] [G loss: 1.614362]\n",
      "epoch:19 step:18471 [D loss: 0.582309, acc.: 66.41%] [G loss: 1.438471]\n",
      "epoch:19 step:18472 [D loss: 0.411815, acc.: 85.94%] [G loss: 1.310755]\n",
      "epoch:19 step:18473 [D loss: 0.557772, acc.: 70.31%] [G loss: 1.397187]\n",
      "epoch:19 step:18474 [D loss: 0.736185, acc.: 45.31%] [G loss: 1.479858]\n",
      "epoch:19 step:18475 [D loss: 0.742304, acc.: 57.03%] [G loss: 1.447121]\n",
      "epoch:19 step:18476 [D loss: 0.484034, acc.: 80.47%] [G loss: 1.432481]\n",
      "epoch:19 step:18477 [D loss: 0.492891, acc.: 75.00%] [G loss: 1.465271]\n",
      "epoch:19 step:18478 [D loss: 0.647360, acc.: 65.62%] [G loss: 1.336432]\n",
      "epoch:19 step:18479 [D loss: 0.447207, acc.: 86.72%] [G loss: 1.129824]\n",
      "epoch:19 step:18480 [D loss: 0.486372, acc.: 77.34%] [G loss: 1.342395]\n",
      "epoch:19 step:18481 [D loss: 0.515089, acc.: 80.47%] [G loss: 1.329965]\n",
      "epoch:19 step:18482 [D loss: 0.541321, acc.: 75.00%] [G loss: 1.157637]\n",
      "epoch:19 step:18483 [D loss: 0.298167, acc.: 96.09%] [G loss: 1.276165]\n",
      "epoch:19 step:18484 [D loss: 0.753366, acc.: 60.94%] [G loss: 1.623167]\n",
      "epoch:19 step:18485 [D loss: 0.341318, acc.: 90.62%] [G loss: 1.478629]\n",
      "epoch:19 step:18486 [D loss: 0.375461, acc.: 90.62%] [G loss: 1.917097]\n",
      "epoch:19 step:18487 [D loss: 0.653254, acc.: 60.16%] [G loss: 1.420408]\n",
      "epoch:19 step:18488 [D loss: 0.501343, acc.: 78.12%] [G loss: 1.560248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18489 [D loss: 0.710852, acc.: 56.25%] [G loss: 1.437891]\n",
      "epoch:19 step:18490 [D loss: 0.387452, acc.: 87.50%] [G loss: 1.110453]\n",
      "epoch:19 step:18491 [D loss: 0.741770, acc.: 54.69%] [G loss: 1.074693]\n",
      "epoch:19 step:18492 [D loss: 0.824001, acc.: 40.62%] [G loss: 1.309165]\n",
      "epoch:19 step:18493 [D loss: 0.438511, acc.: 84.38%] [G loss: 1.145459]\n",
      "epoch:19 step:18494 [D loss: 0.626484, acc.: 65.62%] [G loss: 0.980385]\n",
      "epoch:19 step:18495 [D loss: 0.589651, acc.: 69.53%] [G loss: 1.240438]\n",
      "epoch:19 step:18496 [D loss: 0.498692, acc.: 69.53%] [G loss: 0.849777]\n",
      "epoch:19 step:18497 [D loss: 0.779813, acc.: 53.12%] [G loss: 1.007969]\n",
      "epoch:19 step:18498 [D loss: 0.466356, acc.: 77.34%] [G loss: 0.631141]\n",
      "epoch:19 step:18499 [D loss: 0.736088, acc.: 53.91%] [G loss: 0.887595]\n",
      "epoch:19 step:18500 [D loss: 0.627993, acc.: 65.62%] [G loss: 1.185368]\n",
      "epoch:19 step:18501 [D loss: 0.571499, acc.: 64.06%] [G loss: 0.982856]\n",
      "epoch:19 step:18502 [D loss: 0.571566, acc.: 75.78%] [G loss: 1.689655]\n",
      "epoch:19 step:18503 [D loss: 0.749000, acc.: 60.16%] [G loss: 0.701539]\n",
      "epoch:19 step:18504 [D loss: 0.402126, acc.: 89.06%] [G loss: 1.895113]\n",
      "epoch:19 step:18505 [D loss: 0.441321, acc.: 82.03%] [G loss: 1.528353]\n",
      "epoch:19 step:18506 [D loss: 0.504238, acc.: 75.78%] [G loss: 2.135222]\n",
      "epoch:19 step:18507 [D loss: 0.674385, acc.: 60.94%] [G loss: 0.903554]\n",
      "epoch:19 step:18508 [D loss: 0.479896, acc.: 78.12%] [G loss: 1.690850]\n",
      "epoch:19 step:18509 [D loss: 1.043880, acc.: 28.91%] [G loss: 1.390517]\n",
      "epoch:19 step:18510 [D loss: 0.383861, acc.: 88.28%] [G loss: 1.254660]\n",
      "epoch:19 step:18511 [D loss: 0.595583, acc.: 66.41%] [G loss: 1.196688]\n",
      "epoch:19 step:18512 [D loss: 0.593538, acc.: 67.19%] [G loss: 0.975576]\n",
      "epoch:19 step:18513 [D loss: 0.703080, acc.: 60.16%] [G loss: 1.143533]\n",
      "epoch:19 step:18514 [D loss: 0.581186, acc.: 65.62%] [G loss: 1.377522]\n",
      "epoch:19 step:18515 [D loss: 0.560353, acc.: 65.62%] [G loss: 1.178575]\n",
      "epoch:19 step:18516 [D loss: 0.526637, acc.: 78.91%] [G loss: 1.615657]\n",
      "epoch:19 step:18517 [D loss: 0.714725, acc.: 50.00%] [G loss: 1.124865]\n",
      "epoch:19 step:18518 [D loss: 0.550082, acc.: 75.00%] [G loss: 0.886284]\n",
      "epoch:19 step:18519 [D loss: 0.673753, acc.: 61.72%] [G loss: 1.809608]\n",
      "epoch:19 step:18520 [D loss: 0.415600, acc.: 87.50%] [G loss: 0.924848]\n",
      "epoch:19 step:18521 [D loss: 0.584947, acc.: 65.62%] [G loss: 1.198234]\n",
      "epoch:19 step:18522 [D loss: 0.395757, acc.: 88.28%] [G loss: 1.344980]\n",
      "epoch:19 step:18523 [D loss: 0.578697, acc.: 70.31%] [G loss: 1.429898]\n",
      "epoch:19 step:18524 [D loss: 0.636469, acc.: 60.94%] [G loss: 1.081537]\n",
      "epoch:19 step:18525 [D loss: 0.458200, acc.: 78.12%] [G loss: 1.438391]\n",
      "epoch:19 step:18526 [D loss: 0.501591, acc.: 77.34%] [G loss: 1.032885]\n",
      "epoch:19 step:18527 [D loss: 0.850859, acc.: 46.09%] [G loss: 1.366831]\n",
      "epoch:19 step:18528 [D loss: 0.593087, acc.: 66.41%] [G loss: 1.326727]\n",
      "epoch:19 step:18529 [D loss: 0.678662, acc.: 56.25%] [G loss: 1.035879]\n",
      "epoch:19 step:18530 [D loss: 0.367013, acc.: 89.84%] [G loss: 1.221802]\n",
      "epoch:19 step:18531 [D loss: 0.584643, acc.: 64.84%] [G loss: 1.647656]\n",
      "epoch:19 step:18532 [D loss: 0.424993, acc.: 84.38%] [G loss: 1.038586]\n",
      "epoch:19 step:18533 [D loss: 0.453699, acc.: 87.50%] [G loss: 1.406908]\n",
      "epoch:19 step:18534 [D loss: 0.656787, acc.: 62.50%] [G loss: 1.266880]\n",
      "epoch:19 step:18535 [D loss: 0.396430, acc.: 88.28%] [G loss: 2.337065]\n",
      "epoch:19 step:18536 [D loss: 0.435618, acc.: 84.38%] [G loss: 1.420096]\n",
      "epoch:19 step:18537 [D loss: 0.783768, acc.: 51.56%] [G loss: 1.412860]\n",
      "epoch:19 step:18538 [D loss: 0.812853, acc.: 45.31%] [G loss: 1.425081]\n",
      "epoch:19 step:18539 [D loss: 0.502575, acc.: 81.25%] [G loss: 1.494299]\n",
      "epoch:19 step:18540 [D loss: 0.459201, acc.: 81.25%] [G loss: 1.860914]\n",
      "epoch:19 step:18541 [D loss: 0.793524, acc.: 47.66%] [G loss: 1.421124]\n",
      "epoch:19 step:18542 [D loss: 0.578126, acc.: 63.28%] [G loss: 1.811339]\n",
      "epoch:19 step:18543 [D loss: 0.477742, acc.: 87.50%] [G loss: 1.518351]\n",
      "epoch:19 step:18544 [D loss: 0.565743, acc.: 76.56%] [G loss: 1.697305]\n",
      "epoch:19 step:18545 [D loss: 0.410220, acc.: 83.59%] [G loss: 0.926184]\n",
      "epoch:19 step:18546 [D loss: 0.556861, acc.: 68.75%] [G loss: 1.424848]\n",
      "epoch:19 step:18547 [D loss: 0.408194, acc.: 89.06%] [G loss: 1.545975]\n",
      "epoch:19 step:18548 [D loss: 0.458910, acc.: 77.34%] [G loss: 1.294176]\n",
      "epoch:19 step:18549 [D loss: 0.583777, acc.: 67.97%] [G loss: 1.111838]\n",
      "epoch:19 step:18550 [D loss: 0.629989, acc.: 66.41%] [G loss: 1.191603]\n",
      "epoch:19 step:18551 [D loss: 0.302105, acc.: 94.53%] [G loss: 1.285240]\n",
      "epoch:19 step:18552 [D loss: 0.708741, acc.: 58.59%] [G loss: 1.625509]\n",
      "epoch:19 step:18553 [D loss: 0.408074, acc.: 85.94%] [G loss: 1.807888]\n",
      "epoch:19 step:18554 [D loss: 0.821295, acc.: 57.81%] [G loss: 0.857923]\n",
      "epoch:19 step:18555 [D loss: 0.479733, acc.: 79.69%] [G loss: 1.193196]\n",
      "epoch:19 step:18556 [D loss: 0.703175, acc.: 52.34%] [G loss: 1.444928]\n",
      "epoch:19 step:18557 [D loss: 0.420473, acc.: 88.28%] [G loss: 3.145144]\n",
      "epoch:19 step:18558 [D loss: 0.539127, acc.: 79.69%] [G loss: 1.283402]\n",
      "epoch:19 step:18559 [D loss: 0.422777, acc.: 82.03%] [G loss: 1.267644]\n",
      "epoch:19 step:18560 [D loss: 0.478346, acc.: 78.91%] [G loss: 1.243610]\n",
      "epoch:19 step:18561 [D loss: 0.616266, acc.: 64.84%] [G loss: 1.514368]\n",
      "epoch:19 step:18562 [D loss: 0.467570, acc.: 82.81%] [G loss: 1.999899]\n",
      "epoch:19 step:18563 [D loss: 0.503009, acc.: 77.34%] [G loss: 1.661782]\n",
      "epoch:19 step:18564 [D loss: 0.516162, acc.: 72.66%] [G loss: 1.601249]\n",
      "epoch:19 step:18565 [D loss: 0.390854, acc.: 85.94%] [G loss: 2.079362]\n",
      "epoch:19 step:18566 [D loss: 0.734933, acc.: 56.25%] [G loss: 1.636566]\n",
      "epoch:19 step:18567 [D loss: 0.433507, acc.: 85.94%] [G loss: 1.830739]\n",
      "epoch:19 step:18568 [D loss: 0.795990, acc.: 51.56%] [G loss: 1.606231]\n",
      "epoch:19 step:18569 [D loss: 0.626147, acc.: 64.06%] [G loss: 1.569498]\n",
      "epoch:19 step:18570 [D loss: 0.385744, acc.: 88.28%] [G loss: 0.997061]\n",
      "epoch:19 step:18571 [D loss: 0.549643, acc.: 69.53%] [G loss: 1.313856]\n",
      "epoch:19 step:18572 [D loss: 0.534453, acc.: 74.22%] [G loss: 1.330719]\n",
      "epoch:19 step:18573 [D loss: 0.629181, acc.: 63.28%] [G loss: 1.314825]\n",
      "epoch:19 step:18574 [D loss: 0.501672, acc.: 75.00%] [G loss: 1.581697]\n",
      "epoch:19 step:18575 [D loss: 0.416815, acc.: 86.72%] [G loss: 0.623936]\n",
      "epoch:19 step:18576 [D loss: 0.671158, acc.: 62.50%] [G loss: 1.431849]\n",
      "epoch:19 step:18577 [D loss: 0.291038, acc.: 92.19%] [G loss: 1.856504]\n",
      "epoch:19 step:18578 [D loss: 0.438585, acc.: 82.81%] [G loss: 0.674883]\n",
      "epoch:19 step:18579 [D loss: 0.321916, acc.: 93.75%] [G loss: 2.004225]\n",
      "epoch:19 step:18580 [D loss: 0.314338, acc.: 92.19%] [G loss: 1.708828]\n",
      "epoch:19 step:18581 [D loss: 0.277580, acc.: 92.19%] [G loss: 2.579569]\n",
      "epoch:19 step:18582 [D loss: 1.470514, acc.: 14.06%] [G loss: 1.418208]\n",
      "epoch:19 step:18583 [D loss: 0.542701, acc.: 75.78%] [G loss: 1.401160]\n",
      "epoch:19 step:18584 [D loss: 0.756094, acc.: 49.22%] [G loss: 1.118622]\n",
      "epoch:19 step:18585 [D loss: 0.721269, acc.: 60.94%] [G loss: 1.238383]\n",
      "epoch:19 step:18586 [D loss: 0.645438, acc.: 64.06%] [G loss: 0.976208]\n",
      "epoch:19 step:18587 [D loss: 0.543779, acc.: 70.31%] [G loss: 1.702637]\n",
      "epoch:19 step:18588 [D loss: 0.724857, acc.: 51.56%] [G loss: 1.017562]\n",
      "epoch:19 step:18589 [D loss: 0.364864, acc.: 88.28%] [G loss: 1.262854]\n",
      "epoch:19 step:18590 [D loss: 0.622859, acc.: 65.62%] [G loss: 2.300912]\n",
      "epoch:19 step:18591 [D loss: 0.580433, acc.: 66.41%] [G loss: 1.271392]\n",
      "epoch:19 step:18592 [D loss: 0.653184, acc.: 60.94%] [G loss: 1.385511]\n",
      "epoch:19 step:18593 [D loss: 0.704460, acc.: 53.12%] [G loss: 1.034906]\n",
      "epoch:19 step:18594 [D loss: 0.415246, acc.: 84.38%] [G loss: 1.470091]\n",
      "epoch:19 step:18595 [D loss: 0.349600, acc.: 92.97%] [G loss: 1.651606]\n",
      "epoch:19 step:18596 [D loss: 0.689905, acc.: 57.81%] [G loss: 1.354370]\n",
      "epoch:19 step:18597 [D loss: 0.438125, acc.: 82.81%] [G loss: 1.260246]\n",
      "epoch:19 step:18598 [D loss: 0.563711, acc.: 74.22%] [G loss: 1.307264]\n",
      "epoch:19 step:18599 [D loss: 0.769172, acc.: 42.97%] [G loss: 1.548804]\n",
      "epoch:19 step:18600 [D loss: 0.363579, acc.: 87.50%] [G loss: 1.134645]\n",
      "epoch:19 step:18601 [D loss: 0.749964, acc.: 54.69%] [G loss: 1.333839]\n",
      "epoch:19 step:18602 [D loss: 0.397485, acc.: 85.16%] [G loss: 0.694760]\n",
      "epoch:19 step:18603 [D loss: 0.435110, acc.: 85.94%] [G loss: 1.346946]\n",
      "epoch:19 step:18604 [D loss: 0.413763, acc.: 78.91%] [G loss: 1.491120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18605 [D loss: 1.047618, acc.: 40.62%] [G loss: 1.578669]\n",
      "epoch:19 step:18606 [D loss: 0.622355, acc.: 62.50%] [G loss: 2.404554]\n",
      "epoch:19 step:18607 [D loss: 0.389813, acc.: 90.62%] [G loss: 1.267318]\n",
      "epoch:19 step:18608 [D loss: 0.400333, acc.: 87.50%] [G loss: 1.425568]\n",
      "epoch:19 step:18609 [D loss: 0.789057, acc.: 54.69%] [G loss: 1.191732]\n",
      "epoch:19 step:18610 [D loss: 0.750305, acc.: 59.38%] [G loss: 1.540830]\n",
      "epoch:19 step:18611 [D loss: 0.565298, acc.: 65.62%] [G loss: 1.444782]\n",
      "epoch:19 step:18612 [D loss: 0.486267, acc.: 77.34%] [G loss: 1.946653]\n",
      "epoch:19 step:18613 [D loss: 0.540963, acc.: 73.44%] [G loss: 1.858897]\n",
      "epoch:19 step:18614 [D loss: 0.485182, acc.: 76.56%] [G loss: 0.776116]\n",
      "epoch:19 step:18615 [D loss: 0.715448, acc.: 50.00%] [G loss: 1.027406]\n",
      "epoch:19 step:18616 [D loss: 0.718457, acc.: 50.78%] [G loss: 1.398441]\n",
      "epoch:19 step:18617 [D loss: 0.427688, acc.: 82.81%] [G loss: 1.386774]\n",
      "epoch:19 step:18618 [D loss: 0.352380, acc.: 92.19%] [G loss: 1.546293]\n",
      "epoch:19 step:18619 [D loss: 0.759210, acc.: 57.03%] [G loss: 1.174035]\n",
      "epoch:19 step:18620 [D loss: 0.622992, acc.: 65.62%] [G loss: 1.373065]\n",
      "epoch:19 step:18621 [D loss: 0.719598, acc.: 56.25%] [G loss: 2.161955]\n",
      "epoch:19 step:18622 [D loss: 0.423290, acc.: 83.59%] [G loss: 0.967318]\n",
      "epoch:19 step:18623 [D loss: 0.738914, acc.: 56.25%] [G loss: 1.541700]\n",
      "epoch:19 step:18624 [D loss: 0.913871, acc.: 39.06%] [G loss: 1.260194]\n",
      "epoch:19 step:18625 [D loss: 0.535145, acc.: 72.66%] [G loss: 1.800519]\n",
      "epoch:19 step:18626 [D loss: 0.485496, acc.: 75.78%] [G loss: 1.388479]\n",
      "epoch:19 step:18627 [D loss: 0.662814, acc.: 58.59%] [G loss: 1.180820]\n",
      "epoch:19 step:18628 [D loss: 0.493487, acc.: 78.12%] [G loss: 1.574782]\n",
      "epoch:19 step:18629 [D loss: 0.395581, acc.: 88.28%] [G loss: 1.209903]\n",
      "epoch:19 step:18630 [D loss: 0.588177, acc.: 69.53%] [G loss: 1.206222]\n",
      "epoch:19 step:18631 [D loss: 0.592834, acc.: 69.53%] [G loss: 1.605040]\n",
      "epoch:19 step:18632 [D loss: 0.472886, acc.: 82.03%] [G loss: 1.143358]\n",
      "epoch:19 step:18633 [D loss: 0.681637, acc.: 58.59%] [G loss: 1.610544]\n",
      "epoch:19 step:18634 [D loss: 0.632610, acc.: 62.50%] [G loss: 1.495232]\n",
      "epoch:19 step:18635 [D loss: 0.601337, acc.: 67.19%] [G loss: 1.233636]\n",
      "epoch:19 step:18636 [D loss: 0.422233, acc.: 88.28%] [G loss: 0.824665]\n",
      "epoch:19 step:18637 [D loss: 0.507753, acc.: 75.00%] [G loss: 0.865312]\n",
      "epoch:19 step:18638 [D loss: 0.497788, acc.: 74.22%] [G loss: 0.790257]\n",
      "epoch:19 step:18639 [D loss: 0.683802, acc.: 54.69%] [G loss: 1.087886]\n",
      "epoch:19 step:18640 [D loss: 0.630283, acc.: 62.50%] [G loss: 0.902871]\n",
      "epoch:19 step:18641 [D loss: 0.677045, acc.: 58.59%] [G loss: 0.988429]\n",
      "epoch:19 step:18642 [D loss: 0.544614, acc.: 67.19%] [G loss: 1.247009]\n",
      "epoch:19 step:18643 [D loss: 0.587909, acc.: 68.75%] [G loss: 0.858545]\n",
      "epoch:19 step:18644 [D loss: 0.424176, acc.: 88.28%] [G loss: 1.007072]\n",
      "epoch:19 step:18645 [D loss: 0.821392, acc.: 43.75%] [G loss: 1.333432]\n",
      "epoch:19 step:18646 [D loss: 0.396951, acc.: 84.38%] [G loss: 1.151380]\n",
      "epoch:19 step:18647 [D loss: 0.492736, acc.: 77.34%] [G loss: 0.939133]\n",
      "epoch:19 step:18648 [D loss: 0.765924, acc.: 47.66%] [G loss: 1.211499]\n",
      "epoch:19 step:18649 [D loss: 0.456873, acc.: 82.03%] [G loss: 1.136074]\n",
      "epoch:19 step:18650 [D loss: 0.658906, acc.: 54.69%] [G loss: 1.423412]\n",
      "epoch:19 step:18651 [D loss: 0.417568, acc.: 89.84%] [G loss: 0.945584]\n",
      "epoch:19 step:18652 [D loss: 0.502950, acc.: 77.34%] [G loss: 1.646782]\n",
      "epoch:19 step:18653 [D loss: 0.703304, acc.: 55.47%] [G loss: 1.925332]\n",
      "epoch:19 step:18654 [D loss: 0.679265, acc.: 58.59%] [G loss: 1.675658]\n",
      "epoch:19 step:18655 [D loss: 0.580921, acc.: 75.78%] [G loss: 2.434531]\n",
      "epoch:19 step:18656 [D loss: 0.421491, acc.: 82.03%] [G loss: 1.066898]\n",
      "epoch:19 step:18657 [D loss: 0.531139, acc.: 73.44%] [G loss: 0.847277]\n",
      "epoch:19 step:18658 [D loss: 0.589715, acc.: 64.84%] [G loss: 0.988148]\n",
      "epoch:19 step:18659 [D loss: 0.521122, acc.: 72.66%] [G loss: 2.241122]\n",
      "epoch:19 step:18660 [D loss: 0.595074, acc.: 66.41%] [G loss: 1.648952]\n",
      "epoch:19 step:18661 [D loss: 0.558771, acc.: 65.62%] [G loss: 1.172475]\n",
      "epoch:19 step:18662 [D loss: 0.555116, acc.: 75.00%] [G loss: 1.681038]\n",
      "epoch:19 step:18663 [D loss: 0.396353, acc.: 86.72%] [G loss: 2.386911]\n",
      "epoch:19 step:18664 [D loss: 0.457728, acc.: 80.47%] [G loss: 1.176178]\n",
      "epoch:19 step:18665 [D loss: 0.503443, acc.: 82.03%] [G loss: 0.886628]\n",
      "epoch:19 step:18666 [D loss: 0.287079, acc.: 89.84%] [G loss: 1.052453]\n",
      "epoch:19 step:18667 [D loss: 0.507986, acc.: 78.12%] [G loss: 1.196806]\n",
      "epoch:19 step:18668 [D loss: 0.421079, acc.: 82.03%] [G loss: 1.746244]\n",
      "epoch:19 step:18669 [D loss: 0.316198, acc.: 92.19%] [G loss: 1.425079]\n",
      "epoch:19 step:18670 [D loss: 0.333481, acc.: 87.50%] [G loss: 1.805127]\n",
      "epoch:19 step:18671 [D loss: 0.379643, acc.: 85.16%] [G loss: 1.043204]\n",
      "epoch:19 step:18672 [D loss: 0.422340, acc.: 86.72%] [G loss: 1.109748]\n",
      "epoch:19 step:18673 [D loss: 0.377907, acc.: 86.72%] [G loss: 0.952803]\n",
      "epoch:19 step:18674 [D loss: 0.342839, acc.: 85.94%] [G loss: 1.146543]\n",
      "epoch:19 step:18675 [D loss: 0.375660, acc.: 87.50%] [G loss: 1.706035]\n",
      "epoch:19 step:18676 [D loss: 0.541785, acc.: 71.09%] [G loss: 1.571061]\n",
      "epoch:19 step:18677 [D loss: 0.795047, acc.: 50.00%] [G loss: 1.363449]\n",
      "epoch:19 step:18678 [D loss: 0.809706, acc.: 57.03%] [G loss: 0.786026]\n",
      "epoch:19 step:18679 [D loss: 0.555914, acc.: 68.75%] [G loss: 1.094840]\n",
      "epoch:19 step:18680 [D loss: 0.468795, acc.: 82.81%] [G loss: 1.315153]\n",
      "epoch:19 step:18681 [D loss: 0.436759, acc.: 82.03%] [G loss: 1.400002]\n",
      "epoch:19 step:18682 [D loss: 0.403398, acc.: 82.81%] [G loss: 1.681067]\n",
      "epoch:19 step:18683 [D loss: 0.509296, acc.: 73.44%] [G loss: 1.768980]\n",
      "epoch:19 step:18684 [D loss: 0.442476, acc.: 82.81%] [G loss: 1.102814]\n",
      "epoch:19 step:18685 [D loss: 0.449514, acc.: 80.47%] [G loss: 1.445925]\n",
      "epoch:19 step:18686 [D loss: 0.562776, acc.: 70.31%] [G loss: 2.238716]\n",
      "epoch:19 step:18687 [D loss: 0.546547, acc.: 71.09%] [G loss: 1.620078]\n",
      "epoch:19 step:18688 [D loss: 0.629461, acc.: 64.06%] [G loss: 2.481312]\n",
      "epoch:19 step:18689 [D loss: 0.734728, acc.: 52.34%] [G loss: 1.113255]\n",
      "epoch:19 step:18690 [D loss: 0.386131, acc.: 88.28%] [G loss: 1.015581]\n",
      "epoch:19 step:18691 [D loss: 0.840725, acc.: 47.66%] [G loss: 1.383764]\n",
      "epoch:19 step:18692 [D loss: 0.263698, acc.: 96.09%] [G loss: 1.702602]\n",
      "epoch:19 step:18693 [D loss: 0.635102, acc.: 67.19%] [G loss: 1.390528]\n",
      "epoch:19 step:18694 [D loss: 0.584903, acc.: 67.19%] [G loss: 0.779155]\n",
      "epoch:19 step:18695 [D loss: 0.513030, acc.: 75.78%] [G loss: 2.243287]\n",
      "epoch:19 step:18696 [D loss: 0.608810, acc.: 63.28%] [G loss: 1.346883]\n",
      "epoch:19 step:18697 [D loss: 0.687438, acc.: 58.59%] [G loss: 1.522735]\n",
      "epoch:19 step:18698 [D loss: 0.551123, acc.: 67.97%] [G loss: 2.070420]\n",
      "epoch:19 step:18699 [D loss: 0.289295, acc.: 90.62%] [G loss: 1.708660]\n",
      "epoch:19 step:18700 [D loss: 0.814866, acc.: 45.31%] [G loss: 1.379608]\n",
      "epoch:19 step:18701 [D loss: 0.594657, acc.: 67.97%] [G loss: 1.546077]\n",
      "epoch:19 step:18702 [D loss: 0.695014, acc.: 53.12%] [G loss: 1.066175]\n",
      "epoch:19 step:18703 [D loss: 0.663501, acc.: 59.38%] [G loss: 1.512048]\n",
      "epoch:19 step:18704 [D loss: 0.747017, acc.: 57.03%] [G loss: 1.205110]\n",
      "epoch:19 step:18705 [D loss: 0.708515, acc.: 57.81%] [G loss: 1.185104]\n",
      "epoch:19 step:18706 [D loss: 0.824394, acc.: 44.53%] [G loss: 1.320066]\n",
      "epoch:19 step:18707 [D loss: 0.480013, acc.: 82.81%] [G loss: 1.317940]\n",
      "epoch:19 step:18708 [D loss: 0.553282, acc.: 68.75%] [G loss: 1.092704]\n",
      "epoch:19 step:18709 [D loss: 0.476660, acc.: 79.69%] [G loss: 1.541984]\n",
      "epoch:19 step:18710 [D loss: 0.430771, acc.: 82.03%] [G loss: 1.794154]\n",
      "epoch:19 step:18711 [D loss: 0.681574, acc.: 60.16%] [G loss: 2.057143]\n",
      "epoch:19 step:18712 [D loss: 0.483953, acc.: 77.34%] [G loss: 0.992767]\n",
      "epoch:19 step:18713 [D loss: 0.271617, acc.: 94.53%] [G loss: 2.136118]\n",
      "epoch:19 step:18714 [D loss: 0.645737, acc.: 60.16%] [G loss: 1.603077]\n",
      "epoch:19 step:18715 [D loss: 0.486853, acc.: 75.78%] [G loss: 1.496483]\n",
      "epoch:19 step:18716 [D loss: 0.507078, acc.: 75.00%] [G loss: 0.927418]\n",
      "epoch:19 step:18717 [D loss: 0.483450, acc.: 78.12%] [G loss: 1.270439]\n",
      "epoch:19 step:18718 [D loss: 0.444449, acc.: 78.12%] [G loss: 2.072031]\n",
      "epoch:19 step:18719 [D loss: 0.518859, acc.: 77.34%] [G loss: 0.989393]\n",
      "epoch:19 step:18720 [D loss: 0.412453, acc.: 82.81%] [G loss: 2.701455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18721 [D loss: 0.644785, acc.: 64.84%] [G loss: 2.445108]\n",
      "epoch:19 step:18722 [D loss: 0.633537, acc.: 62.50%] [G loss: 1.643501]\n",
      "epoch:19 step:18723 [D loss: 0.648053, acc.: 53.91%] [G loss: 0.767614]\n",
      "epoch:19 step:18724 [D loss: 0.516300, acc.: 77.34%] [G loss: 0.718628]\n",
      "epoch:19 step:18725 [D loss: 0.641796, acc.: 66.41%] [G loss: 0.844651]\n",
      "epoch:19 step:18726 [D loss: 0.539227, acc.: 74.22%] [G loss: 1.124792]\n",
      "epoch:19 step:18727 [D loss: 0.841769, acc.: 51.56%] [G loss: 0.934670]\n",
      "epoch:19 step:18728 [D loss: 0.811308, acc.: 43.75%] [G loss: 0.951983]\n",
      "epoch:19 step:18729 [D loss: 0.339728, acc.: 89.84%] [G loss: 1.973959]\n",
      "epoch:19 step:18730 [D loss: 0.457651, acc.: 84.38%] [G loss: 1.246531]\n",
      "epoch:19 step:18731 [D loss: 0.448350, acc.: 82.03%] [G loss: 1.009542]\n",
      "epoch:19 step:18732 [D loss: 0.541098, acc.: 67.97%] [G loss: 1.937987]\n",
      "epoch:19 step:18733 [D loss: 0.446878, acc.: 82.81%] [G loss: 1.578846]\n",
      "epoch:19 step:18734 [D loss: 0.350658, acc.: 89.84%] [G loss: 1.166710]\n",
      "epoch:19 step:18735 [D loss: 0.446790, acc.: 82.81%] [G loss: 1.518051]\n",
      "epoch:19 step:18736 [D loss: 0.581587, acc.: 67.97%] [G loss: 1.861266]\n",
      "epoch:19 step:18737 [D loss: 0.610847, acc.: 62.50%] [G loss: 1.587009]\n",
      "epoch:19 step:18738 [D loss: 0.644198, acc.: 65.62%] [G loss: 1.686486]\n",
      "epoch:19 step:18739 [D loss: 0.318668, acc.: 92.19%] [G loss: 1.689308]\n",
      "epoch:19 step:18740 [D loss: 0.752437, acc.: 60.16%] [G loss: 1.460411]\n",
      "epoch:20 step:18741 [D loss: 0.465645, acc.: 81.25%] [G loss: 1.284872]\n",
      "epoch:20 step:18742 [D loss: 0.423544, acc.: 84.38%] [G loss: 1.243557]\n",
      "epoch:20 step:18743 [D loss: 0.648717, acc.: 64.84%] [G loss: 1.158681]\n",
      "epoch:20 step:18744 [D loss: 0.520420, acc.: 77.34%] [G loss: 1.046516]\n",
      "epoch:20 step:18745 [D loss: 0.550912, acc.: 70.31%] [G loss: 1.626225]\n",
      "epoch:20 step:18746 [D loss: 0.861225, acc.: 40.62%] [G loss: 1.208288]\n",
      "epoch:20 step:18747 [D loss: 0.595212, acc.: 67.19%] [G loss: 1.274535]\n",
      "epoch:20 step:18748 [D loss: 0.842623, acc.: 45.31%] [G loss: 1.110903]\n",
      "epoch:20 step:18749 [D loss: 0.835401, acc.: 40.62%] [G loss: 0.980407]\n",
      "epoch:20 step:18750 [D loss: 0.969159, acc.: 38.28%] [G loss: 0.875842]\n",
      "epoch:20 step:18751 [D loss: 0.521434, acc.: 78.12%] [G loss: 1.836071]\n",
      "epoch:20 step:18752 [D loss: 0.548581, acc.: 68.75%] [G loss: 1.141379]\n",
      "epoch:20 step:18753 [D loss: 0.659949, acc.: 61.72%] [G loss: 1.436293]\n",
      "epoch:20 step:18754 [D loss: 0.535572, acc.: 74.22%] [G loss: 1.834905]\n",
      "epoch:20 step:18755 [D loss: 0.708103, acc.: 57.81%] [G loss: 1.286099]\n",
      "epoch:20 step:18756 [D loss: 0.446330, acc.: 82.03%] [G loss: 1.415128]\n",
      "epoch:20 step:18757 [D loss: 0.654535, acc.: 59.38%] [G loss: 1.366734]\n",
      "epoch:20 step:18758 [D loss: 0.372460, acc.: 88.28%] [G loss: 1.338789]\n",
      "epoch:20 step:18759 [D loss: 0.589196, acc.: 66.41%] [G loss: 1.249790]\n",
      "epoch:20 step:18760 [D loss: 0.579917, acc.: 68.75%] [G loss: 1.739274]\n",
      "epoch:20 step:18761 [D loss: 0.446187, acc.: 79.69%] [G loss: 2.198944]\n",
      "epoch:20 step:18762 [D loss: 0.694959, acc.: 61.72%] [G loss: 1.617576]\n",
      "epoch:20 step:18763 [D loss: 0.693776, acc.: 58.59%] [G loss: 1.032501]\n",
      "epoch:20 step:18764 [D loss: 0.269649, acc.: 94.53%] [G loss: 1.302250]\n",
      "epoch:20 step:18765 [D loss: 0.441458, acc.: 79.69%] [G loss: 1.269907]\n",
      "epoch:20 step:18766 [D loss: 0.324551, acc.: 94.53%] [G loss: 1.579192]\n",
      "epoch:20 step:18767 [D loss: 0.639607, acc.: 64.84%] [G loss: 1.725055]\n",
      "epoch:20 step:18768 [D loss: 0.504532, acc.: 70.31%] [G loss: 1.603999]\n",
      "epoch:20 step:18769 [D loss: 0.666510, acc.: 56.25%] [G loss: 1.340213]\n",
      "epoch:20 step:18770 [D loss: 0.540007, acc.: 71.09%] [G loss: 1.557671]\n",
      "epoch:20 step:18771 [D loss: 0.601157, acc.: 70.31%] [G loss: 1.805434]\n",
      "epoch:20 step:18772 [D loss: 0.636536, acc.: 66.41%] [G loss: 1.961547]\n",
      "epoch:20 step:18773 [D loss: 0.425571, acc.: 81.25%] [G loss: 2.360874]\n",
      "epoch:20 step:18774 [D loss: 0.332960, acc.: 91.41%] [G loss: 1.878974]\n",
      "epoch:20 step:18775 [D loss: 0.958542, acc.: 31.25%] [G loss: 1.479562]\n",
      "epoch:20 step:18776 [D loss: 0.975731, acc.: 29.69%] [G loss: 1.222126]\n",
      "epoch:20 step:18777 [D loss: 0.538846, acc.: 71.09%] [G loss: 1.006499]\n",
      "epoch:20 step:18778 [D loss: 0.654315, acc.: 68.75%] [G loss: 1.376688]\n",
      "epoch:20 step:18779 [D loss: 0.526686, acc.: 71.09%] [G loss: 1.403115]\n",
      "epoch:20 step:18780 [D loss: 0.716404, acc.: 50.00%] [G loss: 0.962199]\n",
      "epoch:20 step:18781 [D loss: 0.546974, acc.: 70.31%] [G loss: 1.090242]\n",
      "epoch:20 step:18782 [D loss: 0.789553, acc.: 46.09%] [G loss: 1.226665]\n",
      "epoch:20 step:18783 [D loss: 0.493651, acc.: 75.00%] [G loss: 1.303682]\n",
      "epoch:20 step:18784 [D loss: 0.634175, acc.: 64.06%] [G loss: 1.593579]\n",
      "epoch:20 step:18785 [D loss: 0.754657, acc.: 46.88%] [G loss: 1.693650]\n",
      "epoch:20 step:18786 [D loss: 0.365010, acc.: 91.41%] [G loss: 2.253758]\n",
      "epoch:20 step:18787 [D loss: 0.540575, acc.: 71.88%] [G loss: 2.002016]\n",
      "epoch:20 step:18788 [D loss: 0.635462, acc.: 59.38%] [G loss: 1.668561]\n",
      "epoch:20 step:18789 [D loss: 0.340680, acc.: 90.62%] [G loss: 1.917986]\n",
      "epoch:20 step:18790 [D loss: 0.393568, acc.: 89.06%] [G loss: 1.507013]\n",
      "epoch:20 step:18791 [D loss: 0.404599, acc.: 84.38%] [G loss: 0.849291]\n",
      "epoch:20 step:18792 [D loss: 0.436868, acc.: 80.47%] [G loss: 2.115503]\n",
      "epoch:20 step:18793 [D loss: 0.538924, acc.: 71.88%] [G loss: 1.816771]\n",
      "epoch:20 step:18794 [D loss: 0.368683, acc.: 89.84%] [G loss: 1.434359]\n",
      "epoch:20 step:18795 [D loss: 0.497313, acc.: 78.12%] [G loss: 0.869824]\n",
      "epoch:20 step:18796 [D loss: 0.519735, acc.: 76.56%] [G loss: 1.852523]\n",
      "epoch:20 step:18797 [D loss: 0.668330, acc.: 64.84%] [G loss: 1.375561]\n",
      "epoch:20 step:18798 [D loss: 0.583897, acc.: 64.84%] [G loss: 1.487637]\n",
      "epoch:20 step:18799 [D loss: 0.396631, acc.: 88.28%] [G loss: 1.100074]\n",
      "epoch:20 step:18800 [D loss: 0.521127, acc.: 74.22%] [G loss: 1.903284]\n",
      "epoch:20 step:18801 [D loss: 0.653612, acc.: 60.94%] [G loss: 1.683094]\n",
      "epoch:20 step:18802 [D loss: 0.523761, acc.: 71.88%] [G loss: 1.290557]\n",
      "epoch:20 step:18803 [D loss: 0.318628, acc.: 92.97%] [G loss: 1.208681]\n",
      "epoch:20 step:18804 [D loss: 0.476541, acc.: 79.69%] [G loss: 1.853013]\n",
      "epoch:20 step:18805 [D loss: 0.321690, acc.: 93.75%] [G loss: 0.975267]\n",
      "epoch:20 step:18806 [D loss: 0.627606, acc.: 66.41%] [G loss: 1.852014]\n",
      "epoch:20 step:18807 [D loss: 0.563154, acc.: 71.88%] [G loss: 1.317239]\n",
      "epoch:20 step:18808 [D loss: 0.406081, acc.: 77.34%] [G loss: 1.614567]\n",
      "epoch:20 step:18809 [D loss: 0.393592, acc.: 84.38%] [G loss: 1.640455]\n",
      "epoch:20 step:18810 [D loss: 0.732844, acc.: 49.22%] [G loss: 1.773976]\n",
      "epoch:20 step:18811 [D loss: 0.364623, acc.: 89.06%] [G loss: 1.411876]\n",
      "epoch:20 step:18812 [D loss: 0.564074, acc.: 71.88%] [G loss: 1.470457]\n",
      "epoch:20 step:18813 [D loss: 0.246640, acc.: 94.53%] [G loss: 0.673490]\n",
      "epoch:20 step:18814 [D loss: 0.357020, acc.: 86.72%] [G loss: 1.802221]\n",
      "epoch:20 step:18815 [D loss: 0.367931, acc.: 88.28%] [G loss: 1.095818]\n",
      "epoch:20 step:18816 [D loss: 0.256034, acc.: 96.09%] [G loss: 1.309931]\n",
      "epoch:20 step:18817 [D loss: 0.327471, acc.: 93.75%] [G loss: 1.971100]\n",
      "epoch:20 step:18818 [D loss: 0.476846, acc.: 75.00%] [G loss: 2.090037]\n",
      "epoch:20 step:18819 [D loss: 0.176040, acc.: 96.88%] [G loss: 1.879543]\n",
      "epoch:20 step:18820 [D loss: 0.866367, acc.: 42.97%] [G loss: 1.890425]\n",
      "epoch:20 step:18821 [D loss: 0.773209, acc.: 59.38%] [G loss: 2.736809]\n",
      "epoch:20 step:18822 [D loss: 0.504333, acc.: 81.25%] [G loss: 1.119278]\n",
      "epoch:20 step:18823 [D loss: 0.299494, acc.: 92.97%] [G loss: 1.048400]\n",
      "epoch:20 step:18824 [D loss: 0.538908, acc.: 74.22%] [G loss: 1.498533]\n",
      "epoch:20 step:18825 [D loss: 0.513867, acc.: 69.53%] [G loss: 1.200948]\n",
      "epoch:20 step:18826 [D loss: 0.527540, acc.: 72.66%] [G loss: 1.157377]\n",
      "epoch:20 step:18827 [D loss: 0.449370, acc.: 77.34%] [G loss: 2.550589]\n",
      "epoch:20 step:18828 [D loss: 0.618704, acc.: 63.28%] [G loss: 2.591357]\n",
      "epoch:20 step:18829 [D loss: 0.817651, acc.: 47.66%] [G loss: 1.535317]\n",
      "epoch:20 step:18830 [D loss: 0.659710, acc.: 59.38%] [G loss: 1.097266]\n",
      "epoch:20 step:18831 [D loss: 0.350930, acc.: 89.06%] [G loss: 1.058578]\n",
      "epoch:20 step:18832 [D loss: 0.788677, acc.: 49.22%] [G loss: 0.613509]\n",
      "epoch:20 step:18833 [D loss: 0.448606, acc.: 78.12%] [G loss: 1.496802]\n",
      "epoch:20 step:18834 [D loss: 0.297129, acc.: 89.06%] [G loss: 1.147044]\n",
      "epoch:20 step:18835 [D loss: 0.730554, acc.: 50.00%] [G loss: 1.750813]\n",
      "epoch:20 step:18836 [D loss: 0.477536, acc.: 82.03%] [G loss: 1.759409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18837 [D loss: 0.455216, acc.: 80.47%] [G loss: 0.684879]\n",
      "epoch:20 step:18838 [D loss: 0.471202, acc.: 80.47%] [G loss: 1.186442]\n",
      "epoch:20 step:18839 [D loss: 0.580522, acc.: 69.53%] [G loss: 0.844907]\n",
      "epoch:20 step:18840 [D loss: 0.321964, acc.: 92.19%] [G loss: 1.693269]\n",
      "epoch:20 step:18841 [D loss: 0.566186, acc.: 71.09%] [G loss: 2.081702]\n",
      "epoch:20 step:18842 [D loss: 0.564519, acc.: 70.31%] [G loss: 0.745444]\n",
      "epoch:20 step:18843 [D loss: 0.362534, acc.: 85.94%] [G loss: 1.326579]\n",
      "epoch:20 step:18844 [D loss: 0.636429, acc.: 60.16%] [G loss: 1.215630]\n",
      "epoch:20 step:18845 [D loss: 0.347050, acc.: 90.62%] [G loss: 0.995181]\n",
      "epoch:20 step:18846 [D loss: 0.556173, acc.: 66.41%] [G loss: 1.943067]\n",
      "epoch:20 step:18847 [D loss: 0.446244, acc.: 82.81%] [G loss: 2.045275]\n",
      "epoch:20 step:18848 [D loss: 0.510384, acc.: 74.22%] [G loss: 1.270371]\n",
      "epoch:20 step:18849 [D loss: 0.445977, acc.: 85.94%] [G loss: 1.267362]\n",
      "epoch:20 step:18850 [D loss: 0.499778, acc.: 78.91%] [G loss: 1.624020]\n",
      "epoch:20 step:18851 [D loss: 0.485600, acc.: 79.69%] [G loss: 1.483986]\n",
      "epoch:20 step:18852 [D loss: 1.208997, acc.: 38.28%] [G loss: 1.617331]\n",
      "epoch:20 step:18853 [D loss: 0.722299, acc.: 64.06%] [G loss: 2.877072]\n",
      "epoch:20 step:18854 [D loss: 0.567921, acc.: 68.75%] [G loss: 1.908735]\n",
      "epoch:20 step:18855 [D loss: 0.773011, acc.: 50.00%] [G loss: 1.180843]\n",
      "epoch:20 step:18856 [D loss: 0.793385, acc.: 53.91%] [G loss: 1.656225]\n",
      "epoch:20 step:18857 [D loss: 0.496898, acc.: 77.34%] [G loss: 1.181956]\n",
      "epoch:20 step:18858 [D loss: 0.353596, acc.: 92.19%] [G loss: 1.200418]\n",
      "epoch:20 step:18859 [D loss: 0.658607, acc.: 59.38%] [G loss: 1.511081]\n",
      "epoch:20 step:18860 [D loss: 0.385615, acc.: 84.38%] [G loss: 1.759856]\n",
      "epoch:20 step:18861 [D loss: 0.385012, acc.: 89.06%] [G loss: 0.974577]\n",
      "epoch:20 step:18862 [D loss: 0.966481, acc.: 42.19%] [G loss: 2.133260]\n",
      "epoch:20 step:18863 [D loss: 0.609568, acc.: 64.84%] [G loss: 1.635404]\n",
      "epoch:20 step:18864 [D loss: 0.763084, acc.: 52.34%] [G loss: 1.212374]\n",
      "epoch:20 step:18865 [D loss: 0.555428, acc.: 71.09%] [G loss: 1.670134]\n",
      "epoch:20 step:18866 [D loss: 0.826867, acc.: 50.00%] [G loss: 1.994220]\n",
      "epoch:20 step:18867 [D loss: 0.636879, acc.: 64.84%] [G loss: 1.394938]\n",
      "epoch:20 step:18868 [D loss: 0.623850, acc.: 59.38%] [G loss: 1.898580]\n",
      "epoch:20 step:18869 [D loss: 0.426912, acc.: 79.69%] [G loss: 0.982376]\n",
      "epoch:20 step:18870 [D loss: 0.456965, acc.: 79.69%] [G loss: 1.205030]\n",
      "epoch:20 step:18871 [D loss: 0.713048, acc.: 50.78%] [G loss: 1.294962]\n",
      "epoch:20 step:18872 [D loss: 0.551682, acc.: 75.00%] [G loss: 1.536320]\n",
      "epoch:20 step:18873 [D loss: 0.492698, acc.: 75.78%] [G loss: 1.208778]\n",
      "epoch:20 step:18874 [D loss: 0.487106, acc.: 77.34%] [G loss: 1.213829]\n",
      "epoch:20 step:18875 [D loss: 0.506223, acc.: 75.78%] [G loss: 1.651181]\n",
      "epoch:20 step:18876 [D loss: 0.840621, acc.: 45.31%] [G loss: 0.907315]\n",
      "epoch:20 step:18877 [D loss: 0.547116, acc.: 71.09%] [G loss: 1.481145]\n",
      "epoch:20 step:18878 [D loss: 0.524490, acc.: 76.56%] [G loss: 2.199096]\n",
      "epoch:20 step:18879 [D loss: 0.496688, acc.: 76.56%] [G loss: 1.402318]\n",
      "epoch:20 step:18880 [D loss: 0.694813, acc.: 57.03%] [G loss: 1.746705]\n",
      "epoch:20 step:18881 [D loss: 0.628192, acc.: 67.19%] [G loss: 1.269849]\n",
      "epoch:20 step:18882 [D loss: 0.431759, acc.: 85.94%] [G loss: 1.252816]\n",
      "epoch:20 step:18883 [D loss: 0.451264, acc.: 80.47%] [G loss: 2.023255]\n",
      "epoch:20 step:18884 [D loss: 0.472418, acc.: 78.91%] [G loss: 1.427282]\n",
      "epoch:20 step:18885 [D loss: 0.443421, acc.: 82.81%] [G loss: 1.054437]\n",
      "epoch:20 step:18886 [D loss: 0.506894, acc.: 74.22%] [G loss: 1.738614]\n",
      "epoch:20 step:18887 [D loss: 0.507172, acc.: 75.00%] [G loss: 0.725753]\n",
      "epoch:20 step:18888 [D loss: 0.793562, acc.: 53.12%] [G loss: 0.783476]\n",
      "epoch:20 step:18889 [D loss: 0.631931, acc.: 64.06%] [G loss: 1.909375]\n",
      "epoch:20 step:18890 [D loss: 0.436974, acc.: 82.81%] [G loss: 2.394042]\n",
      "epoch:20 step:18891 [D loss: 0.800370, acc.: 45.31%] [G loss: 1.460927]\n",
      "epoch:20 step:18892 [D loss: 0.585788, acc.: 70.31%] [G loss: 1.524351]\n",
      "epoch:20 step:18893 [D loss: 0.563027, acc.: 74.22%] [G loss: 1.704012]\n",
      "epoch:20 step:18894 [D loss: 0.631685, acc.: 57.81%] [G loss: 2.052125]\n",
      "epoch:20 step:18895 [D loss: 0.485429, acc.: 78.12%] [G loss: 1.284330]\n",
      "epoch:20 step:18896 [D loss: 0.552777, acc.: 69.53%] [G loss: 1.369753]\n",
      "epoch:20 step:18897 [D loss: 0.307777, acc.: 92.19%] [G loss: 1.371924]\n",
      "epoch:20 step:18898 [D loss: 0.479657, acc.: 77.34%] [G loss: 1.734472]\n",
      "epoch:20 step:18899 [D loss: 0.644383, acc.: 60.94%] [G loss: 1.746710]\n",
      "epoch:20 step:18900 [D loss: 0.570267, acc.: 68.75%] [G loss: 1.079360]\n",
      "epoch:20 step:18901 [D loss: 0.609082, acc.: 63.28%] [G loss: 1.631757]\n",
      "epoch:20 step:18902 [D loss: 0.253463, acc.: 93.75%] [G loss: 1.227279]\n",
      "epoch:20 step:18903 [D loss: 0.750251, acc.: 57.03%] [G loss: 1.942165]\n",
      "epoch:20 step:18904 [D loss: 0.785999, acc.: 53.12%] [G loss: 0.870461]\n",
      "epoch:20 step:18905 [D loss: 0.717964, acc.: 60.94%] [G loss: 1.437086]\n",
      "epoch:20 step:18906 [D loss: 0.487486, acc.: 80.47%] [G loss: 1.641560]\n",
      "epoch:20 step:18907 [D loss: 0.526198, acc.: 77.34%] [G loss: 1.307728]\n",
      "epoch:20 step:18908 [D loss: 0.491451, acc.: 75.78%] [G loss: 1.328624]\n",
      "epoch:20 step:18909 [D loss: 0.627434, acc.: 60.94%] [G loss: 1.255298]\n",
      "epoch:20 step:18910 [D loss: 0.786759, acc.: 46.09%] [G loss: 1.661116]\n",
      "epoch:20 step:18911 [D loss: 0.477003, acc.: 81.25%] [G loss: 0.868842]\n",
      "epoch:20 step:18912 [D loss: 1.058589, acc.: 28.12%] [G loss: 1.804513]\n",
      "epoch:20 step:18913 [D loss: 0.545820, acc.: 71.88%] [G loss: 1.021260]\n",
      "epoch:20 step:18914 [D loss: 0.775653, acc.: 46.88%] [G loss: 1.799285]\n",
      "epoch:20 step:18915 [D loss: 0.550301, acc.: 67.19%] [G loss: 1.838588]\n",
      "epoch:20 step:18916 [D loss: 0.530915, acc.: 76.56%] [G loss: 0.797265]\n",
      "epoch:20 step:18917 [D loss: 0.423582, acc.: 84.38%] [G loss: 2.012436]\n",
      "epoch:20 step:18918 [D loss: 0.512301, acc.: 77.34%] [G loss: 1.166677]\n",
      "epoch:20 step:18919 [D loss: 0.410017, acc.: 90.62%] [G loss: 0.744242]\n",
      "epoch:20 step:18920 [D loss: 0.539834, acc.: 70.31%] [G loss: 0.738940]\n",
      "epoch:20 step:18921 [D loss: 0.713283, acc.: 57.03%] [G loss: 1.272910]\n",
      "epoch:20 step:18922 [D loss: 0.543764, acc.: 71.09%] [G loss: 1.788729]\n",
      "epoch:20 step:18923 [D loss: 0.556476, acc.: 71.88%] [G loss: 1.083153]\n",
      "epoch:20 step:18924 [D loss: 0.395074, acc.: 89.06%] [G loss: 0.933911]\n",
      "epoch:20 step:18925 [D loss: 0.730637, acc.: 54.69%] [G loss: 2.192435]\n",
      "epoch:20 step:18926 [D loss: 0.515462, acc.: 75.00%] [G loss: 1.968284]\n",
      "epoch:20 step:18927 [D loss: 0.563294, acc.: 69.53%] [G loss: 1.904965]\n",
      "epoch:20 step:18928 [D loss: 0.287598, acc.: 92.19%] [G loss: 1.437975]\n",
      "epoch:20 step:18929 [D loss: 0.574260, acc.: 72.66%] [G loss: 1.446989]\n",
      "epoch:20 step:18930 [D loss: 0.609365, acc.: 67.19%] [G loss: 2.132031]\n",
      "epoch:20 step:18931 [D loss: 0.642058, acc.: 66.41%] [G loss: 1.425220]\n",
      "epoch:20 step:18932 [D loss: 0.475938, acc.: 78.12%] [G loss: 0.816387]\n",
      "epoch:20 step:18933 [D loss: 0.836024, acc.: 53.91%] [G loss: 0.963664]\n",
      "epoch:20 step:18934 [D loss: 0.532742, acc.: 75.00%] [G loss: 2.045266]\n",
      "epoch:20 step:18935 [D loss: 0.436214, acc.: 82.81%] [G loss: 1.471068]\n",
      "epoch:20 step:18936 [D loss: 0.340690, acc.: 89.06%] [G loss: 1.610308]\n",
      "epoch:20 step:18937 [D loss: 0.543930, acc.: 72.66%] [G loss: 1.348745]\n",
      "epoch:20 step:18938 [D loss: 0.709052, acc.: 59.38%] [G loss: 1.075038]\n",
      "epoch:20 step:18939 [D loss: 0.849632, acc.: 41.41%] [G loss: 1.258928]\n",
      "epoch:20 step:18940 [D loss: 0.982003, acc.: 34.38%] [G loss: 1.369774]\n",
      "epoch:20 step:18941 [D loss: 0.496495, acc.: 75.00%] [G loss: 1.515306]\n",
      "epoch:20 step:18942 [D loss: 0.510434, acc.: 78.12%] [G loss: 1.380520]\n",
      "epoch:20 step:18943 [D loss: 0.348452, acc.: 90.62%] [G loss: 2.023069]\n",
      "epoch:20 step:18944 [D loss: 0.576737, acc.: 65.62%] [G loss: 1.274320]\n",
      "epoch:20 step:18945 [D loss: 0.334261, acc.: 91.41%] [G loss: 1.315759]\n",
      "epoch:20 step:18946 [D loss: 0.540334, acc.: 72.66%] [G loss: 2.455871]\n",
      "epoch:20 step:18947 [D loss: 0.460367, acc.: 82.03%] [G loss: 1.062339]\n",
      "epoch:20 step:18948 [D loss: 0.526356, acc.: 72.66%] [G loss: 0.718119]\n",
      "epoch:20 step:18949 [D loss: 0.707851, acc.: 50.00%] [G loss: 0.802754]\n",
      "epoch:20 step:18950 [D loss: 0.372146, acc.: 89.84%] [G loss: 2.241403]\n",
      "epoch:20 step:18951 [D loss: 0.206404, acc.: 97.66%] [G loss: 0.606895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18952 [D loss: 0.692777, acc.: 57.81%] [G loss: 1.723095]\n",
      "epoch:20 step:18953 [D loss: 0.907717, acc.: 45.31%] [G loss: 0.724673]\n",
      "epoch:20 step:18954 [D loss: 0.732502, acc.: 52.34%] [G loss: 0.773685]\n",
      "epoch:20 step:18955 [D loss: 0.453690, acc.: 82.81%] [G loss: 2.000072]\n",
      "epoch:20 step:18956 [D loss: 0.299295, acc.: 96.09%] [G loss: 0.821295]\n",
      "epoch:20 step:18957 [D loss: 0.410344, acc.: 84.38%] [G loss: 1.238014]\n",
      "epoch:20 step:18958 [D loss: 0.534393, acc.: 71.09%] [G loss: 2.187568]\n",
      "epoch:20 step:18959 [D loss: 0.416971, acc.: 87.50%] [G loss: 2.370556]\n",
      "epoch:20 step:18960 [D loss: 0.491544, acc.: 75.00%] [G loss: 1.300442]\n",
      "epoch:20 step:18961 [D loss: 0.426120, acc.: 85.94%] [G loss: 1.232517]\n",
      "epoch:20 step:18962 [D loss: 0.581515, acc.: 66.41%] [G loss: 2.492561]\n",
      "epoch:20 step:18963 [D loss: 0.488121, acc.: 71.09%] [G loss: 0.945637]\n",
      "epoch:20 step:18964 [D loss: 0.434277, acc.: 81.25%] [G loss: 1.219686]\n",
      "epoch:20 step:18965 [D loss: 0.452313, acc.: 78.91%] [G loss: 2.236274]\n",
      "epoch:20 step:18966 [D loss: 0.332658, acc.: 88.28%] [G loss: 1.702780]\n",
      "epoch:20 step:18967 [D loss: 0.651680, acc.: 61.72%] [G loss: 1.309978]\n",
      "epoch:20 step:18968 [D loss: 0.371712, acc.: 88.28%] [G loss: 1.185374]\n",
      "epoch:20 step:18969 [D loss: 0.531524, acc.: 72.66%] [G loss: 1.351300]\n",
      "epoch:20 step:18970 [D loss: 0.848046, acc.: 53.12%] [G loss: 1.358284]\n",
      "epoch:20 step:18971 [D loss: 0.600723, acc.: 70.31%] [G loss: 1.472964]\n",
      "epoch:20 step:18972 [D loss: 0.474223, acc.: 81.25%] [G loss: 1.219730]\n",
      "epoch:20 step:18973 [D loss: 0.831517, acc.: 46.88%] [G loss: 1.451562]\n",
      "epoch:20 step:18974 [D loss: 0.602062, acc.: 65.62%] [G loss: 0.961536]\n",
      "epoch:20 step:18975 [D loss: 0.452129, acc.: 86.72%] [G loss: 1.769824]\n",
      "epoch:20 step:18976 [D loss: 0.681403, acc.: 56.25%] [G loss: 1.208231]\n",
      "epoch:20 step:18977 [D loss: 0.591003, acc.: 68.75%] [G loss: 0.961675]\n",
      "epoch:20 step:18978 [D loss: 0.625113, acc.: 71.09%] [G loss: 1.481361]\n",
      "epoch:20 step:18979 [D loss: 0.799058, acc.: 54.69%] [G loss: 0.846752]\n",
      "epoch:20 step:18980 [D loss: 0.715415, acc.: 56.25%] [G loss: 1.631497]\n",
      "epoch:20 step:18981 [D loss: 0.660085, acc.: 68.75%] [G loss: 0.937950]\n",
      "epoch:20 step:18982 [D loss: 0.362830, acc.: 86.72%] [G loss: 1.016233]\n",
      "epoch:20 step:18983 [D loss: 0.484078, acc.: 82.03%] [G loss: 1.567847]\n",
      "epoch:20 step:18984 [D loss: 0.602928, acc.: 67.97%] [G loss: 1.538260]\n",
      "epoch:20 step:18985 [D loss: 0.732500, acc.: 55.47%] [G loss: 1.320679]\n",
      "epoch:20 step:18986 [D loss: 0.451943, acc.: 86.72%] [G loss: 1.510926]\n",
      "epoch:20 step:18987 [D loss: 0.584263, acc.: 71.09%] [G loss: 1.608409]\n",
      "epoch:20 step:18988 [D loss: 0.393860, acc.: 89.06%] [G loss: 1.453848]\n",
      "epoch:20 step:18989 [D loss: 0.618607, acc.: 66.41%] [G loss: 0.893760]\n",
      "epoch:20 step:18990 [D loss: 0.335298, acc.: 91.41%] [G loss: 1.271444]\n",
      "epoch:20 step:18991 [D loss: 0.393135, acc.: 88.28%] [G loss: 1.696788]\n",
      "epoch:20 step:18992 [D loss: 0.436590, acc.: 82.03%] [G loss: 1.483170]\n",
      "epoch:20 step:18993 [D loss: 0.491309, acc.: 71.88%] [G loss: 0.770705]\n",
      "epoch:20 step:18994 [D loss: 0.641347, acc.: 64.84%] [G loss: 1.234184]\n",
      "epoch:20 step:18995 [D loss: 0.500796, acc.: 74.22%] [G loss: 2.110756]\n",
      "epoch:20 step:18996 [D loss: 0.644028, acc.: 57.81%] [G loss: 1.557235]\n",
      "epoch:20 step:18997 [D loss: 0.886871, acc.: 49.22%] [G loss: 0.736745]\n",
      "epoch:20 step:18998 [D loss: 0.764035, acc.: 53.91%] [G loss: 1.958743]\n",
      "epoch:20 step:18999 [D loss: 0.557055, acc.: 68.75%] [G loss: 1.107210]\n",
      "epoch:20 step:19000 [D loss: 0.290260, acc.: 94.53%] [G loss: 1.989403]\n",
      "epoch:20 step:19001 [D loss: 0.543133, acc.: 69.53%] [G loss: 1.207347]\n",
      "epoch:20 step:19002 [D loss: 0.441675, acc.: 82.81%] [G loss: 1.573227]\n",
      "epoch:20 step:19003 [D loss: 0.470517, acc.: 76.56%] [G loss: 1.390546]\n",
      "epoch:20 step:19004 [D loss: 0.530295, acc.: 71.09%] [G loss: 2.139909]\n",
      "epoch:20 step:19005 [D loss: 0.518591, acc.: 76.56%] [G loss: 1.552324]\n",
      "epoch:20 step:19006 [D loss: 0.614412, acc.: 67.97%] [G loss: 1.421324]\n",
      "epoch:20 step:19007 [D loss: 0.493994, acc.: 77.34%] [G loss: 1.352182]\n",
      "epoch:20 step:19008 [D loss: 0.399702, acc.: 84.38%] [G loss: 1.191236]\n",
      "epoch:20 step:19009 [D loss: 0.456236, acc.: 82.03%] [G loss: 1.831502]\n",
      "epoch:20 step:19010 [D loss: 0.233513, acc.: 95.31%] [G loss: 1.239097]\n",
      "epoch:20 step:19011 [D loss: 0.504213, acc.: 71.88%] [G loss: 1.369500]\n",
      "epoch:20 step:19012 [D loss: 0.430449, acc.: 85.16%] [G loss: 1.325711]\n",
      "epoch:20 step:19013 [D loss: 0.645331, acc.: 59.38%] [G loss: 1.705368]\n",
      "epoch:20 step:19014 [D loss: 0.422122, acc.: 85.94%] [G loss: 1.862422]\n",
      "epoch:20 step:19015 [D loss: 0.617260, acc.: 66.41%] [G loss: 1.283692]\n",
      "epoch:20 step:19016 [D loss: 0.496679, acc.: 74.22%] [G loss: 1.219794]\n",
      "epoch:20 step:19017 [D loss: 0.534526, acc.: 75.00%] [G loss: 1.414840]\n",
      "epoch:20 step:19018 [D loss: 0.687744, acc.: 62.50%] [G loss: 1.383693]\n",
      "epoch:20 step:19019 [D loss: 0.737962, acc.: 53.91%] [G loss: 1.909173]\n",
      "epoch:20 step:19020 [D loss: 0.611822, acc.: 64.06%] [G loss: 0.948061]\n",
      "epoch:20 step:19021 [D loss: 0.711603, acc.: 61.72%] [G loss: 1.439591]\n",
      "epoch:20 step:19022 [D loss: 0.501771, acc.: 75.78%] [G loss: 1.669774]\n",
      "epoch:20 step:19023 [D loss: 0.525364, acc.: 71.88%] [G loss: 1.680832]\n",
      "epoch:20 step:19024 [D loss: 0.432004, acc.: 80.47%] [G loss: 1.227097]\n",
      "epoch:20 step:19025 [D loss: 0.323562, acc.: 92.97%] [G loss: 2.132947]\n",
      "epoch:20 step:19026 [D loss: 0.613407, acc.: 66.41%] [G loss: 1.140830]\n",
      "epoch:20 step:19027 [D loss: 0.625342, acc.: 63.28%] [G loss: 1.892028]\n",
      "epoch:20 step:19028 [D loss: 0.601147, acc.: 63.28%] [G loss: 2.680462]\n",
      "epoch:20 step:19029 [D loss: 0.500983, acc.: 77.34%] [G loss: 0.828374]\n",
      "epoch:20 step:19030 [D loss: 0.849621, acc.: 47.66%] [G loss: 1.121736]\n",
      "epoch:20 step:19031 [D loss: 0.420127, acc.: 87.50%] [G loss: 1.304601]\n",
      "epoch:20 step:19032 [D loss: 0.754482, acc.: 50.00%] [G loss: 1.738865]\n",
      "epoch:20 step:19033 [D loss: 1.078968, acc.: 43.75%] [G loss: 1.619107]\n",
      "epoch:20 step:19034 [D loss: 0.365523, acc.: 88.28%] [G loss: 1.146570]\n",
      "epoch:20 step:19035 [D loss: 0.835768, acc.: 45.31%] [G loss: 0.949434]\n",
      "epoch:20 step:19036 [D loss: 0.522084, acc.: 72.66%] [G loss: 1.883883]\n",
      "epoch:20 step:19037 [D loss: 0.485978, acc.: 77.34%] [G loss: 0.632949]\n",
      "epoch:20 step:19038 [D loss: 0.446838, acc.: 85.16%] [G loss: 1.946105]\n",
      "epoch:20 step:19039 [D loss: 0.530227, acc.: 75.00%] [G loss: 1.673935]\n",
      "epoch:20 step:19040 [D loss: 0.333290, acc.: 85.94%] [G loss: 1.888057]\n",
      "epoch:20 step:19041 [D loss: 0.298970, acc.: 91.41%] [G loss: 1.681637]\n",
      "epoch:20 step:19042 [D loss: 0.300189, acc.: 92.97%] [G loss: 1.050068]\n",
      "epoch:20 step:19043 [D loss: 0.748790, acc.: 60.94%] [G loss: 2.192646]\n",
      "epoch:20 step:19044 [D loss: 0.498516, acc.: 76.56%] [G loss: 1.002263]\n",
      "epoch:20 step:19045 [D loss: 0.463352, acc.: 77.34%] [G loss: 1.255382]\n",
      "epoch:20 step:19046 [D loss: 0.581068, acc.: 67.97%] [G loss: 1.688857]\n",
      "epoch:20 step:19047 [D loss: 0.637722, acc.: 65.62%] [G loss: 1.483063]\n",
      "epoch:20 step:19048 [D loss: 0.461079, acc.: 78.12%] [G loss: 1.296324]\n",
      "epoch:20 step:19049 [D loss: 0.387103, acc.: 85.16%] [G loss: 2.024617]\n",
      "epoch:20 step:19050 [D loss: 0.492010, acc.: 76.56%] [G loss: 1.583598]\n",
      "epoch:20 step:19051 [D loss: 0.415244, acc.: 82.03%] [G loss: 1.685437]\n",
      "epoch:20 step:19052 [D loss: 0.673188, acc.: 66.41%] [G loss: 1.431476]\n",
      "epoch:20 step:19053 [D loss: 0.449314, acc.: 81.25%] [G loss: 1.241447]\n",
      "epoch:20 step:19054 [D loss: 0.453574, acc.: 79.69%] [G loss: 1.591979]\n",
      "epoch:20 step:19055 [D loss: 0.568549, acc.: 73.44%] [G loss: 1.254082]\n",
      "epoch:20 step:19056 [D loss: 0.835736, acc.: 46.88%] [G loss: 1.288166]\n",
      "epoch:20 step:19057 [D loss: 0.612989, acc.: 66.41%] [G loss: 1.379840]\n",
      "epoch:20 step:19058 [D loss: 0.543946, acc.: 71.88%] [G loss: 0.892864]\n",
      "epoch:20 step:19059 [D loss: 0.519288, acc.: 75.00%] [G loss: 1.177829]\n",
      "epoch:20 step:19060 [D loss: 0.372939, acc.: 89.84%] [G loss: 1.422771]\n",
      "epoch:20 step:19061 [D loss: 0.480699, acc.: 75.78%] [G loss: 1.556830]\n",
      "epoch:20 step:19062 [D loss: 0.577714, acc.: 64.84%] [G loss: 1.398179]\n",
      "epoch:20 step:19063 [D loss: 0.483988, acc.: 75.78%] [G loss: 1.453018]\n",
      "epoch:20 step:19064 [D loss: 0.268862, acc.: 95.31%] [G loss: 2.051547]\n",
      "epoch:20 step:19065 [D loss: 0.786065, acc.: 53.91%] [G loss: 1.669608]\n",
      "epoch:20 step:19066 [D loss: 0.636336, acc.: 59.38%] [G loss: 1.589806]\n",
      "epoch:20 step:19067 [D loss: 0.849571, acc.: 39.84%] [G loss: 0.708688]\n",
      "epoch:20 step:19068 [D loss: 0.547737, acc.: 68.75%] [G loss: 0.866641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19069 [D loss: 0.345237, acc.: 90.62%] [G loss: 2.057619]\n",
      "epoch:20 step:19070 [D loss: 0.836292, acc.: 45.31%] [G loss: 1.721579]\n",
      "epoch:20 step:19071 [D loss: 0.394175, acc.: 90.62%] [G loss: 1.811925]\n",
      "epoch:20 step:19072 [D loss: 0.531155, acc.: 72.66%] [G loss: 1.170927]\n",
      "epoch:20 step:19073 [D loss: 0.631493, acc.: 63.28%] [G loss: 1.713952]\n",
      "epoch:20 step:19074 [D loss: 0.453474, acc.: 78.91%] [G loss: 2.176436]\n",
      "epoch:20 step:19075 [D loss: 0.483123, acc.: 70.31%] [G loss: 0.972581]\n",
      "epoch:20 step:19076 [D loss: 0.609332, acc.: 68.75%] [G loss: 1.073421]\n",
      "epoch:20 step:19077 [D loss: 0.518644, acc.: 76.56%] [G loss: 2.017160]\n",
      "epoch:20 step:19078 [D loss: 0.376574, acc.: 84.38%] [G loss: 1.610752]\n",
      "epoch:20 step:19079 [D loss: 0.597172, acc.: 71.88%] [G loss: 0.846168]\n",
      "epoch:20 step:19080 [D loss: 0.787910, acc.: 44.53%] [G loss: 0.622899]\n",
      "epoch:20 step:19081 [D loss: 0.331746, acc.: 92.97%] [G loss: 1.720382]\n",
      "epoch:20 step:19082 [D loss: 0.276696, acc.: 92.97%] [G loss: 0.928065]\n",
      "epoch:20 step:19083 [D loss: 1.172306, acc.: 22.66%] [G loss: 0.948693]\n",
      "epoch:20 step:19084 [D loss: 0.439785, acc.: 78.12%] [G loss: 1.415625]\n",
      "epoch:20 step:19085 [D loss: 0.827200, acc.: 49.22%] [G loss: 1.352192]\n",
      "epoch:20 step:19086 [D loss: 0.373198, acc.: 90.62%] [G loss: 1.058753]\n",
      "epoch:20 step:19087 [D loss: 0.259586, acc.: 96.09%] [G loss: 1.801927]\n",
      "epoch:20 step:19088 [D loss: 0.316320, acc.: 92.19%] [G loss: 2.154854]\n",
      "epoch:20 step:19089 [D loss: 0.526324, acc.: 83.59%] [G loss: 1.094880]\n",
      "epoch:20 step:19090 [D loss: 0.411990, acc.: 84.38%] [G loss: 1.665624]\n",
      "epoch:20 step:19091 [D loss: 0.908762, acc.: 45.31%] [G loss: 1.515792]\n",
      "epoch:20 step:19092 [D loss: 0.599679, acc.: 72.66%] [G loss: 1.483355]\n",
      "epoch:20 step:19093 [D loss: 0.517475, acc.: 73.44%] [G loss: 1.288757]\n",
      "epoch:20 step:19094 [D loss: 0.630147, acc.: 71.88%] [G loss: 0.724699]\n",
      "epoch:20 step:19095 [D loss: 0.480318, acc.: 81.25%] [G loss: 1.172093]\n",
      "epoch:20 step:19096 [D loss: 0.965826, acc.: 32.81%] [G loss: 0.653307]\n",
      "epoch:20 step:19097 [D loss: 0.420039, acc.: 85.16%] [G loss: 0.648381]\n",
      "epoch:20 step:19098 [D loss: 0.606651, acc.: 65.62%] [G loss: 1.413986]\n",
      "epoch:20 step:19099 [D loss: 0.802626, acc.: 57.03%] [G loss: 1.417857]\n",
      "epoch:20 step:19100 [D loss: 0.482871, acc.: 74.22%] [G loss: 1.509573]\n",
      "epoch:20 step:19101 [D loss: 0.604227, acc.: 68.75%] [G loss: 1.997110]\n",
      "epoch:20 step:19102 [D loss: 0.611959, acc.: 60.94%] [G loss: 1.648417]\n",
      "epoch:20 step:19103 [D loss: 0.329647, acc.: 88.28%] [G loss: 1.356403]\n",
      "epoch:20 step:19104 [D loss: 0.317200, acc.: 92.97%] [G loss: 0.896486]\n",
      "epoch:20 step:19105 [D loss: 0.407717, acc.: 85.16%] [G loss: 1.311527]\n",
      "epoch:20 step:19106 [D loss: 0.532129, acc.: 82.03%] [G loss: 0.774749]\n",
      "epoch:20 step:19107 [D loss: 0.918250, acc.: 39.84%] [G loss: 1.152254]\n",
      "epoch:20 step:19108 [D loss: 0.679758, acc.: 56.25%] [G loss: 1.327310]\n",
      "epoch:20 step:19109 [D loss: 0.396471, acc.: 88.28%] [G loss: 1.962583]\n",
      "epoch:20 step:19110 [D loss: 0.650766, acc.: 57.81%] [G loss: 1.306998]\n",
      "epoch:20 step:19111 [D loss: 0.614758, acc.: 60.94%] [G loss: 1.061290]\n",
      "epoch:20 step:19112 [D loss: 0.506743, acc.: 82.03%] [G loss: 1.337477]\n",
      "epoch:20 step:19113 [D loss: 0.379432, acc.: 88.28%] [G loss: 1.104437]\n",
      "epoch:20 step:19114 [D loss: 0.652072, acc.: 64.84%] [G loss: 1.486155]\n",
      "epoch:20 step:19115 [D loss: 0.675664, acc.: 60.16%] [G loss: 1.942723]\n",
      "epoch:20 step:19116 [D loss: 0.499434, acc.: 74.22%] [G loss: 1.878718]\n",
      "epoch:20 step:19117 [D loss: 0.567364, acc.: 70.31%] [G loss: 1.793962]\n",
      "epoch:20 step:19118 [D loss: 0.419983, acc.: 85.16%] [G loss: 1.160835]\n",
      "epoch:20 step:19119 [D loss: 0.750351, acc.: 54.69%] [G loss: 1.438675]\n",
      "epoch:20 step:19120 [D loss: 0.715430, acc.: 59.38%] [G loss: 1.203476]\n",
      "epoch:20 step:19121 [D loss: 0.479158, acc.: 75.78%] [G loss: 1.190712]\n",
      "epoch:20 step:19122 [D loss: 0.323728, acc.: 94.53%] [G loss: 1.107439]\n",
      "epoch:20 step:19123 [D loss: 0.322922, acc.: 94.53%] [G loss: 1.451954]\n",
      "epoch:20 step:19124 [D loss: 0.514801, acc.: 71.88%] [G loss: 1.348108]\n",
      "epoch:20 step:19125 [D loss: 0.360504, acc.: 90.62%] [G loss: 1.791535]\n",
      "epoch:20 step:19126 [D loss: 0.304398, acc.: 96.09%] [G loss: 1.975322]\n",
      "epoch:20 step:19127 [D loss: 0.566929, acc.: 74.22%] [G loss: 1.214285]\n",
      "epoch:20 step:19128 [D loss: 0.377787, acc.: 88.28%] [G loss: 1.613049]\n",
      "epoch:20 step:19129 [D loss: 0.782856, acc.: 50.78%] [G loss: 1.457728]\n",
      "epoch:20 step:19130 [D loss: 0.459537, acc.: 81.25%] [G loss: 2.015732]\n",
      "epoch:20 step:19131 [D loss: 1.226625, acc.: 21.88%] [G loss: 0.802446]\n",
      "epoch:20 step:19132 [D loss: 0.618666, acc.: 64.84%] [G loss: 2.552855]\n",
      "epoch:20 step:19133 [D loss: 0.424593, acc.: 83.59%] [G loss: 1.590790]\n",
      "epoch:20 step:19134 [D loss: 0.911394, acc.: 36.72%] [G loss: 1.342973]\n",
      "epoch:20 step:19135 [D loss: 0.323672, acc.: 89.06%] [G loss: 1.941361]\n",
      "epoch:20 step:19136 [D loss: 0.418644, acc.: 81.25%] [G loss: 1.763025]\n",
      "epoch:20 step:19137 [D loss: 0.940655, acc.: 43.75%] [G loss: 1.850395]\n",
      "epoch:20 step:19138 [D loss: 0.647301, acc.: 63.28%] [G loss: 1.811338]\n",
      "epoch:20 step:19139 [D loss: 0.497243, acc.: 75.78%] [G loss: 1.186064]\n",
      "epoch:20 step:19140 [D loss: 0.428095, acc.: 81.25%] [G loss: 1.428091]\n",
      "epoch:20 step:19141 [D loss: 0.755391, acc.: 49.22%] [G loss: 1.701564]\n",
      "epoch:20 step:19142 [D loss: 0.708315, acc.: 62.50%] [G loss: 1.629278]\n",
      "epoch:20 step:19143 [D loss: 0.520585, acc.: 73.44%] [G loss: 1.540452]\n",
      "epoch:20 step:19144 [D loss: 0.624056, acc.: 64.84%] [G loss: 1.357550]\n",
      "epoch:20 step:19145 [D loss: 0.558818, acc.: 66.41%] [G loss: 1.229541]\n",
      "epoch:20 step:19146 [D loss: 0.416378, acc.: 85.94%] [G loss: 1.547541]\n",
      "epoch:20 step:19147 [D loss: 0.757380, acc.: 57.03%] [G loss: 1.392814]\n",
      "epoch:20 step:19148 [D loss: 0.513375, acc.: 71.88%] [G loss: 1.843572]\n",
      "epoch:20 step:19149 [D loss: 0.491794, acc.: 76.56%] [G loss: 2.317685]\n",
      "epoch:20 step:19150 [D loss: 0.677463, acc.: 59.38%] [G loss: 1.324966]\n",
      "epoch:20 step:19151 [D loss: 0.531967, acc.: 74.22%] [G loss: 1.432553]\n",
      "epoch:20 step:19152 [D loss: 0.746797, acc.: 46.88%] [G loss: 1.564518]\n",
      "epoch:20 step:19153 [D loss: 0.834545, acc.: 46.88%] [G loss: 1.716962]\n",
      "epoch:20 step:19154 [D loss: 0.543809, acc.: 72.66%] [G loss: 1.379750]\n",
      "epoch:20 step:19155 [D loss: 0.461780, acc.: 79.69%] [G loss: 1.620917]\n",
      "epoch:20 step:19156 [D loss: 0.624226, acc.: 73.44%] [G loss: 1.617304]\n",
      "epoch:20 step:19157 [D loss: 0.495613, acc.: 78.91%] [G loss: 1.792165]\n",
      "epoch:20 step:19158 [D loss: 0.573875, acc.: 71.09%] [G loss: 1.291191]\n",
      "epoch:20 step:19159 [D loss: 0.423527, acc.: 83.59%] [G loss: 1.422538]\n",
      "epoch:20 step:19160 [D loss: 1.097136, acc.: 28.12%] [G loss: 1.367128]\n",
      "epoch:20 step:19161 [D loss: 0.387416, acc.: 91.41%] [G loss: 1.151536]\n",
      "epoch:20 step:19162 [D loss: 0.830060, acc.: 42.19%] [G loss: 0.858383]\n",
      "epoch:20 step:19163 [D loss: 0.543799, acc.: 75.78%] [G loss: 0.357878]\n",
      "epoch:20 step:19164 [D loss: 0.510246, acc.: 74.22%] [G loss: 1.029955]\n",
      "epoch:20 step:19165 [D loss: 0.441760, acc.: 79.69%] [G loss: 1.737952]\n",
      "epoch:20 step:19166 [D loss: 0.601676, acc.: 65.62%] [G loss: 0.889717]\n",
      "epoch:20 step:19167 [D loss: 0.554433, acc.: 73.44%] [G loss: 0.900010]\n",
      "epoch:20 step:19168 [D loss: 0.822615, acc.: 44.53%] [G loss: 1.058504]\n",
      "epoch:20 step:19169 [D loss: 0.459220, acc.: 82.81%] [G loss: 1.201670]\n",
      "epoch:20 step:19170 [D loss: 0.380157, acc.: 90.62%] [G loss: 1.455994]\n",
      "epoch:20 step:19171 [D loss: 1.261288, acc.: 25.78%] [G loss: 1.203163]\n",
      "epoch:20 step:19172 [D loss: 0.897627, acc.: 35.16%] [G loss: 1.445829]\n",
      "epoch:20 step:19173 [D loss: 0.453993, acc.: 82.81%] [G loss: 1.809221]\n",
      "epoch:20 step:19174 [D loss: 0.741882, acc.: 50.00%] [G loss: 0.952650]\n",
      "epoch:20 step:19175 [D loss: 0.619628, acc.: 62.50%] [G loss: 1.198675]\n",
      "epoch:20 step:19176 [D loss: 0.571903, acc.: 70.31%] [G loss: 1.365289]\n",
      "epoch:20 step:19177 [D loss: 0.786886, acc.: 53.12%] [G loss: 1.125747]\n",
      "epoch:20 step:19178 [D loss: 0.503800, acc.: 81.25%] [G loss: 1.313065]\n",
      "epoch:20 step:19179 [D loss: 0.564126, acc.: 71.09%] [G loss: 1.794203]\n",
      "epoch:20 step:19180 [D loss: 0.488677, acc.: 76.56%] [G loss: 1.703421]\n",
      "epoch:20 step:19181 [D loss: 0.493819, acc.: 78.12%] [G loss: 1.319794]\n",
      "epoch:20 step:19182 [D loss: 0.425860, acc.: 85.16%] [G loss: 1.491183]\n",
      "epoch:20 step:19183 [D loss: 0.552558, acc.: 76.56%] [G loss: 1.915722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19184 [D loss: 0.538678, acc.: 71.09%] [G loss: 2.018821]\n",
      "epoch:20 step:19185 [D loss: 0.533114, acc.: 75.00%] [G loss: 1.310486]\n",
      "epoch:20 step:19186 [D loss: 0.558207, acc.: 71.09%] [G loss: 1.515179]\n",
      "epoch:20 step:19187 [D loss: 0.836819, acc.: 57.81%] [G loss: 1.093995]\n",
      "epoch:20 step:19188 [D loss: 0.734708, acc.: 55.47%] [G loss: 0.834598]\n",
      "epoch:20 step:19189 [D loss: 0.411705, acc.: 88.28%] [G loss: 1.102651]\n",
      "epoch:20 step:19190 [D loss: 0.630244, acc.: 63.28%] [G loss: 0.646640]\n",
      "epoch:20 step:19191 [D loss: 0.524153, acc.: 75.78%] [G loss: 1.240559]\n",
      "epoch:20 step:19192 [D loss: 0.628411, acc.: 60.16%] [G loss: 0.929273]\n",
      "epoch:20 step:19193 [D loss: 0.642936, acc.: 65.62%] [G loss: 0.669327]\n",
      "epoch:20 step:19194 [D loss: 0.559368, acc.: 71.09%] [G loss: 1.802177]\n",
      "epoch:20 step:19195 [D loss: 0.473783, acc.: 85.16%] [G loss: 1.102601]\n",
      "epoch:20 step:19196 [D loss: 0.750134, acc.: 50.78%] [G loss: 1.275999]\n",
      "epoch:20 step:19197 [D loss: 0.616739, acc.: 64.84%] [G loss: 1.639565]\n",
      "epoch:20 step:19198 [D loss: 0.754355, acc.: 53.12%] [G loss: 0.771269]\n",
      "epoch:20 step:19199 [D loss: 0.469900, acc.: 81.25%] [G loss: 1.742719]\n",
      "epoch:20 step:19200 [D loss: 0.464001, acc.: 82.03%] [G loss: 1.501812]\n",
      "epoch:20 step:19201 [D loss: 0.636602, acc.: 62.50%] [G loss: 0.848716]\n",
      "epoch:20 step:19202 [D loss: 0.485208, acc.: 81.25%] [G loss: 1.618797]\n",
      "epoch:20 step:19203 [D loss: 0.664315, acc.: 55.47%] [G loss: 1.602536]\n",
      "epoch:20 step:19204 [D loss: 0.570836, acc.: 68.75%] [G loss: 1.468309]\n",
      "epoch:20 step:19205 [D loss: 0.607460, acc.: 67.19%] [G loss: 1.574121]\n",
      "epoch:20 step:19206 [D loss: 0.380282, acc.: 90.62%] [G loss: 1.037797]\n",
      "epoch:20 step:19207 [D loss: 0.711520, acc.: 56.25%] [G loss: 1.102469]\n",
      "epoch:20 step:19208 [D loss: 0.904631, acc.: 37.50%] [G loss: 1.055535]\n",
      "epoch:20 step:19209 [D loss: 0.514433, acc.: 75.78%] [G loss: 1.440829]\n",
      "epoch:20 step:19210 [D loss: 0.573288, acc.: 70.31%] [G loss: 1.594054]\n",
      "epoch:20 step:19211 [D loss: 0.293162, acc.: 95.31%] [G loss: 1.411907]\n",
      "epoch:20 step:19212 [D loss: 0.509038, acc.: 77.34%] [G loss: 1.381823]\n",
      "epoch:20 step:19213 [D loss: 0.307430, acc.: 93.75%] [G loss: 2.174607]\n",
      "epoch:20 step:19214 [D loss: 0.469631, acc.: 84.38%] [G loss: 0.922180]\n",
      "epoch:20 step:19215 [D loss: 0.777302, acc.: 47.66%] [G loss: 1.487387]\n",
      "epoch:20 step:19216 [D loss: 0.476960, acc.: 78.12%] [G loss: 2.039931]\n",
      "epoch:20 step:19217 [D loss: 0.423330, acc.: 85.94%] [G loss: 1.186163]\n",
      "epoch:20 step:19218 [D loss: 0.508377, acc.: 75.00%] [G loss: 1.513494]\n",
      "epoch:20 step:19219 [D loss: 0.529934, acc.: 77.34%] [G loss: 1.512699]\n",
      "epoch:20 step:19220 [D loss: 0.659384, acc.: 60.16%] [G loss: 1.793701]\n",
      "epoch:20 step:19221 [D loss: 0.421849, acc.: 87.50%] [G loss: 1.742330]\n",
      "epoch:20 step:19222 [D loss: 0.479930, acc.: 82.81%] [G loss: 1.992820]\n",
      "epoch:20 step:19223 [D loss: 0.961222, acc.: 46.09%] [G loss: 2.497725]\n",
      "epoch:20 step:19224 [D loss: 0.418363, acc.: 85.94%] [G loss: 1.898992]\n",
      "epoch:20 step:19225 [D loss: 0.433473, acc.: 82.03%] [G loss: 2.243454]\n",
      "epoch:20 step:19226 [D loss: 0.413978, acc.: 82.81%] [G loss: 1.302279]\n",
      "epoch:20 step:19227 [D loss: 0.486992, acc.: 78.91%] [G loss: 3.277610]\n",
      "epoch:20 step:19228 [D loss: 0.620867, acc.: 63.28%] [G loss: 1.958455]\n",
      "epoch:20 step:19229 [D loss: 0.599249, acc.: 66.41%] [G loss: 2.079576]\n",
      "epoch:20 step:19230 [D loss: 0.301347, acc.: 85.94%] [G loss: 1.035628]\n",
      "epoch:20 step:19231 [D loss: 0.904570, acc.: 34.38%] [G loss: 0.761236]\n",
      "epoch:20 step:19232 [D loss: 0.660112, acc.: 60.94%] [G loss: 1.744007]\n",
      "epoch:20 step:19233 [D loss: 0.621858, acc.: 60.94%] [G loss: 1.152366]\n",
      "epoch:20 step:19234 [D loss: 0.487386, acc.: 83.59%] [G loss: 1.482621]\n",
      "epoch:20 step:19235 [D loss: 0.565515, acc.: 71.09%] [G loss: 1.283668]\n",
      "epoch:20 step:19236 [D loss: 0.541405, acc.: 73.44%] [G loss: 2.200084]\n",
      "epoch:20 step:19237 [D loss: 0.765401, acc.: 53.91%] [G loss: 1.129402]\n",
      "epoch:20 step:19238 [D loss: 0.472374, acc.: 82.81%] [G loss: 1.150566]\n",
      "epoch:20 step:19239 [D loss: 0.650675, acc.: 63.28%] [G loss: 1.309650]\n",
      "epoch:20 step:19240 [D loss: 0.487726, acc.: 77.34%] [G loss: 1.081956]\n",
      "epoch:20 step:19241 [D loss: 0.519262, acc.: 76.56%] [G loss: 1.448194]\n",
      "epoch:20 step:19242 [D loss: 0.461590, acc.: 71.09%] [G loss: 2.280358]\n",
      "epoch:20 step:19243 [D loss: 0.928851, acc.: 50.00%] [G loss: 1.280491]\n",
      "epoch:20 step:19244 [D loss: 0.501064, acc.: 77.34%] [G loss: 1.087466]\n",
      "epoch:20 step:19245 [D loss: 0.396341, acc.: 83.59%] [G loss: 0.802919]\n",
      "epoch:20 step:19246 [D loss: 0.753281, acc.: 52.34%] [G loss: 1.536603]\n",
      "epoch:20 step:19247 [D loss: 0.507057, acc.: 76.56%] [G loss: 1.345662]\n",
      "epoch:20 step:19248 [D loss: 0.456821, acc.: 82.81%] [G loss: 1.287172]\n",
      "epoch:20 step:19249 [D loss: 0.355493, acc.: 91.41%] [G loss: 3.050207]\n",
      "epoch:20 step:19250 [D loss: 0.269933, acc.: 97.66%] [G loss: 1.863671]\n",
      "epoch:20 step:19251 [D loss: 0.423509, acc.: 83.59%] [G loss: 2.493453]\n",
      "epoch:20 step:19252 [D loss: 0.543500, acc.: 71.88%] [G loss: 1.843351]\n",
      "epoch:20 step:19253 [D loss: 0.323187, acc.: 92.19%] [G loss: 1.458890]\n",
      "epoch:20 step:19254 [D loss: 0.349859, acc.: 85.16%] [G loss: 2.011262]\n",
      "epoch:20 step:19255 [D loss: 0.735977, acc.: 56.25%] [G loss: 1.348970]\n",
      "epoch:20 step:19256 [D loss: 0.236373, acc.: 96.88%] [G loss: 1.535765]\n",
      "epoch:20 step:19257 [D loss: 0.236738, acc.: 95.31%] [G loss: 0.981968]\n",
      "epoch:20 step:19258 [D loss: 0.353789, acc.: 90.62%] [G loss: 0.416701]\n",
      "epoch:20 step:19259 [D loss: 1.042422, acc.: 46.88%] [G loss: 1.316349]\n",
      "epoch:20 step:19260 [D loss: 0.379350, acc.: 82.81%] [G loss: 2.099342]\n",
      "epoch:20 step:19261 [D loss: 0.414225, acc.: 78.91%] [G loss: 1.061973]\n",
      "epoch:20 step:19262 [D loss: 0.638577, acc.: 65.62%] [G loss: 1.640876]\n",
      "epoch:20 step:19263 [D loss: 0.494850, acc.: 73.44%] [G loss: 2.304323]\n",
      "epoch:20 step:19264 [D loss: 0.421283, acc.: 82.03%] [G loss: 1.057199]\n",
      "epoch:20 step:19265 [D loss: 0.435712, acc.: 78.12%] [G loss: 3.328335]\n",
      "epoch:20 step:19266 [D loss: 0.434624, acc.: 84.38%] [G loss: 1.345290]\n",
      "epoch:20 step:19267 [D loss: 0.536141, acc.: 73.44%] [G loss: 1.243571]\n",
      "epoch:20 step:19268 [D loss: 0.508379, acc.: 71.09%] [G loss: 2.786542]\n",
      "epoch:20 step:19269 [D loss: 0.259115, acc.: 94.53%] [G loss: 0.900561]\n",
      "epoch:20 step:19270 [D loss: 0.530044, acc.: 72.66%] [G loss: 1.053262]\n",
      "epoch:20 step:19271 [D loss: 0.332934, acc.: 92.19%] [G loss: 1.769209]\n",
      "epoch:20 step:19272 [D loss: 0.650331, acc.: 64.06%] [G loss: 1.021345]\n",
      "epoch:20 step:19273 [D loss: 0.204349, acc.: 96.09%] [G loss: 1.164575]\n",
      "epoch:20 step:19274 [D loss: 0.420293, acc.: 78.12%] [G loss: 0.705863]\n",
      "epoch:20 step:19275 [D loss: 0.348925, acc.: 88.28%] [G loss: 2.715494]\n",
      "epoch:20 step:19276 [D loss: 0.345729, acc.: 88.28%] [G loss: 0.766892]\n",
      "epoch:20 step:19277 [D loss: 0.335600, acc.: 82.81%] [G loss: 1.518705]\n",
      "epoch:20 step:19278 [D loss: 0.499678, acc.: 71.88%] [G loss: 2.299510]\n",
      "epoch:20 step:19279 [D loss: 0.702837, acc.: 62.50%] [G loss: 1.482650]\n",
      "epoch:20 step:19280 [D loss: 0.662942, acc.: 58.59%] [G loss: 1.418605]\n",
      "epoch:20 step:19281 [D loss: 0.485494, acc.: 80.47%] [G loss: 1.555625]\n",
      "epoch:20 step:19282 [D loss: 0.622939, acc.: 62.50%] [G loss: 1.547434]\n",
      "epoch:20 step:19283 [D loss: 0.476373, acc.: 80.47%] [G loss: 1.407447]\n",
      "epoch:20 step:19284 [D loss: 0.652597, acc.: 60.16%] [G loss: 1.452683]\n",
      "epoch:20 step:19285 [D loss: 0.490353, acc.: 76.56%] [G loss: 1.951278]\n",
      "epoch:20 step:19286 [D loss: 0.575769, acc.: 66.41%] [G loss: 1.963895]\n",
      "epoch:20 step:19287 [D loss: 0.297931, acc.: 89.84%] [G loss: 1.604476]\n",
      "epoch:20 step:19288 [D loss: 0.747539, acc.: 54.69%] [G loss: 2.549994]\n",
      "epoch:20 step:19289 [D loss: 0.265672, acc.: 92.19%] [G loss: 1.059188]\n",
      "epoch:20 step:19290 [D loss: 0.400797, acc.: 81.25%] [G loss: 2.170886]\n",
      "epoch:20 step:19291 [D loss: 0.699672, acc.: 53.12%] [G loss: 1.837937]\n",
      "epoch:20 step:19292 [D loss: 0.379823, acc.: 85.16%] [G loss: 0.785847]\n",
      "epoch:20 step:19293 [D loss: 0.640664, acc.: 63.28%] [G loss: 1.686461]\n",
      "epoch:20 step:19294 [D loss: 0.341903, acc.: 88.28%] [G loss: 2.510659]\n",
      "epoch:20 step:19295 [D loss: 0.330342, acc.: 90.62%] [G loss: 1.028310]\n",
      "epoch:20 step:19296 [D loss: 1.062618, acc.: 48.44%] [G loss: 1.109932]\n",
      "epoch:20 step:19297 [D loss: 0.319065, acc.: 92.19%] [G loss: 1.604089]\n",
      "epoch:20 step:19298 [D loss: 0.553463, acc.: 69.53%] [G loss: 1.818490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19299 [D loss: 1.109816, acc.: 28.12%] [G loss: 1.182449]\n",
      "epoch:20 step:19300 [D loss: 0.718294, acc.: 53.12%] [G loss: 1.232176]\n",
      "epoch:20 step:19301 [D loss: 0.521367, acc.: 76.56%] [G loss: 2.469127]\n",
      "epoch:20 step:19302 [D loss: 0.443782, acc.: 82.81%] [G loss: 2.168055]\n",
      "epoch:20 step:19303 [D loss: 0.702541, acc.: 62.50%] [G loss: 1.149207]\n",
      "epoch:20 step:19304 [D loss: 0.846663, acc.: 39.84%] [G loss: 1.246936]\n",
      "epoch:20 step:19305 [D loss: 0.506564, acc.: 74.22%] [G loss: 1.677703]\n",
      "epoch:20 step:19306 [D loss: 0.390664, acc.: 85.16%] [G loss: 1.288571]\n",
      "epoch:20 step:19307 [D loss: 0.656553, acc.: 63.28%] [G loss: 0.727853]\n",
      "epoch:20 step:19308 [D loss: 0.663863, acc.: 64.06%] [G loss: 1.458461]\n",
      "epoch:20 step:19309 [D loss: 0.651675, acc.: 57.03%] [G loss: 1.366940]\n",
      "epoch:20 step:19310 [D loss: 0.488637, acc.: 76.56%] [G loss: 1.322656]\n",
      "epoch:20 step:19311 [D loss: 0.467287, acc.: 81.25%] [G loss: 2.185842]\n",
      "epoch:20 step:19312 [D loss: 0.379688, acc.: 90.62%] [G loss: 1.735305]\n",
      "epoch:20 step:19313 [D loss: 0.354703, acc.: 80.47%] [G loss: 1.521999]\n",
      "epoch:20 step:19314 [D loss: 0.632548, acc.: 64.84%] [G loss: 0.839767]\n",
      "epoch:20 step:19315 [D loss: 0.558523, acc.: 69.53%] [G loss: 1.017522]\n",
      "epoch:20 step:19316 [D loss: 0.654431, acc.: 60.16%] [G loss: 1.106965]\n",
      "epoch:20 step:19317 [D loss: 0.368918, acc.: 85.16%] [G loss: 1.047394]\n",
      "epoch:20 step:19318 [D loss: 0.330408, acc.: 92.97%] [G loss: 1.685330]\n",
      "epoch:20 step:19319 [D loss: 0.443301, acc.: 82.81%] [G loss: 1.281059]\n",
      "epoch:20 step:19320 [D loss: 0.558328, acc.: 74.22%] [G loss: 0.965283]\n",
      "epoch:20 step:19321 [D loss: 0.904155, acc.: 44.53%] [G loss: 1.586235]\n",
      "epoch:20 step:19322 [D loss: 0.717884, acc.: 53.91%] [G loss: 1.215149]\n",
      "epoch:20 step:19323 [D loss: 0.443378, acc.: 85.16%] [G loss: 1.213902]\n",
      "epoch:20 step:19324 [D loss: 0.693470, acc.: 57.81%] [G loss: 0.751989]\n",
      "epoch:20 step:19325 [D loss: 0.527413, acc.: 72.66%] [G loss: 2.352299]\n",
      "epoch:20 step:19326 [D loss: 0.690625, acc.: 56.25%] [G loss: 1.092371]\n",
      "epoch:20 step:19327 [D loss: 0.476182, acc.: 80.47%] [G loss: 1.408693]\n",
      "epoch:20 step:19328 [D loss: 0.589970, acc.: 67.97%] [G loss: 0.864921]\n",
      "epoch:20 step:19329 [D loss: 0.479978, acc.: 78.12%] [G loss: 1.038647]\n",
      "epoch:20 step:19330 [D loss: 0.522160, acc.: 74.22%] [G loss: 0.935861]\n",
      "epoch:20 step:19331 [D loss: 0.466261, acc.: 71.88%] [G loss: 0.993159]\n",
      "epoch:20 step:19332 [D loss: 0.659107, acc.: 63.28%] [G loss: 2.095949]\n",
      "epoch:20 step:19333 [D loss: 0.365101, acc.: 87.50%] [G loss: 2.051968]\n",
      "epoch:20 step:19334 [D loss: 0.555169, acc.: 72.66%] [G loss: 1.649722]\n",
      "epoch:20 step:19335 [D loss: 0.394156, acc.: 83.59%] [G loss: 1.173677]\n",
      "epoch:20 step:19336 [D loss: 0.564965, acc.: 68.75%] [G loss: 1.479162]\n",
      "epoch:20 step:19337 [D loss: 0.396402, acc.: 92.19%] [G loss: 1.051446]\n",
      "epoch:20 step:19338 [D loss: 0.535410, acc.: 72.66%] [G loss: 1.089013]\n",
      "epoch:20 step:19339 [D loss: 0.695944, acc.: 50.78%] [G loss: 1.278402]\n",
      "epoch:20 step:19340 [D loss: 0.553760, acc.: 67.19%] [G loss: 2.581547]\n",
      "epoch:20 step:19341 [D loss: 0.413462, acc.: 91.41%] [G loss: 2.250889]\n",
      "epoch:20 step:19342 [D loss: 0.363795, acc.: 90.62%] [G loss: 1.440535]\n",
      "epoch:20 step:19343 [D loss: 0.427468, acc.: 75.00%] [G loss: 2.759054]\n",
      "epoch:20 step:19344 [D loss: 0.518984, acc.: 73.44%] [G loss: 1.798288]\n",
      "epoch:20 step:19345 [D loss: 0.548790, acc.: 62.50%] [G loss: 1.658262]\n",
      "epoch:20 step:19346 [D loss: 0.672017, acc.: 59.38%] [G loss: 1.741170]\n",
      "epoch:20 step:19347 [D loss: 0.442003, acc.: 86.72%] [G loss: 2.150285]\n",
      "epoch:20 step:19348 [D loss: 0.515772, acc.: 72.66%] [G loss: 0.857003]\n",
      "epoch:20 step:19349 [D loss: 1.000041, acc.: 34.38%] [G loss: 0.397370]\n",
      "epoch:20 step:19350 [D loss: 0.583993, acc.: 75.00%] [G loss: 0.987362]\n",
      "epoch:20 step:19351 [D loss: 0.801986, acc.: 55.47%] [G loss: 0.975581]\n",
      "epoch:20 step:19352 [D loss: 0.645042, acc.: 60.16%] [G loss: 1.492690]\n",
      "epoch:20 step:19353 [D loss: 0.681781, acc.: 60.94%] [G loss: 2.401681]\n",
      "epoch:20 step:19354 [D loss: 0.768522, acc.: 58.59%] [G loss: 1.395188]\n",
      "epoch:20 step:19355 [D loss: 0.416241, acc.: 89.84%] [G loss: 1.385897]\n",
      "epoch:20 step:19356 [D loss: 0.646076, acc.: 64.84%] [G loss: 0.948003]\n",
      "epoch:20 step:19357 [D loss: 0.396867, acc.: 87.50%] [G loss: 2.390306]\n",
      "epoch:20 step:19358 [D loss: 0.447617, acc.: 84.38%] [G loss: 1.383929]\n",
      "epoch:20 step:19359 [D loss: 0.345263, acc.: 91.41%] [G loss: 2.054859]\n",
      "epoch:20 step:19360 [D loss: 0.419149, acc.: 82.81%] [G loss: 1.079787]\n",
      "epoch:20 step:19361 [D loss: 0.440964, acc.: 85.16%] [G loss: 2.955147]\n",
      "epoch:20 step:19362 [D loss: 0.977330, acc.: 32.81%] [G loss: 0.964952]\n",
      "epoch:20 step:19363 [D loss: 0.599237, acc.: 65.62%] [G loss: 1.211113]\n",
      "epoch:20 step:19364 [D loss: 0.732578, acc.: 50.78%] [G loss: 1.072549]\n",
      "epoch:20 step:19365 [D loss: 0.697344, acc.: 55.47%] [G loss: 2.342416]\n",
      "epoch:20 step:19366 [D loss: 0.385144, acc.: 89.06%] [G loss: 1.753055]\n",
      "epoch:20 step:19367 [D loss: 0.422147, acc.: 82.03%] [G loss: 1.651713]\n",
      "epoch:20 step:19368 [D loss: 0.374064, acc.: 82.81%] [G loss: 1.628448]\n",
      "epoch:20 step:19369 [D loss: 0.715773, acc.: 60.16%] [G loss: 1.404329]\n",
      "epoch:20 step:19370 [D loss: 0.665971, acc.: 57.81%] [G loss: 1.084089]\n",
      "epoch:20 step:19371 [D loss: 0.392273, acc.: 88.28%] [G loss: 0.761286]\n",
      "epoch:20 step:19372 [D loss: 0.444839, acc.: 81.25%] [G loss: 0.887048]\n",
      "epoch:20 step:19373 [D loss: 0.513031, acc.: 71.09%] [G loss: 1.303472]\n",
      "epoch:20 step:19374 [D loss: 0.793460, acc.: 50.00%] [G loss: 1.041430]\n",
      "epoch:20 step:19375 [D loss: 0.232513, acc.: 94.53%] [G loss: 2.328688]\n",
      "epoch:20 step:19376 [D loss: 0.848098, acc.: 36.72%] [G loss: 1.686731]\n",
      "epoch:20 step:19377 [D loss: 0.414199, acc.: 83.59%] [G loss: 1.181771]\n",
      "epoch:20 step:19378 [D loss: 0.775298, acc.: 48.44%] [G loss: 1.821366]\n",
      "epoch:20 step:19379 [D loss: 0.569779, acc.: 65.62%] [G loss: 1.679459]\n",
      "epoch:20 step:19380 [D loss: 0.416084, acc.: 87.50%] [G loss: 1.899663]\n",
      "epoch:20 step:19381 [D loss: 0.585984, acc.: 60.16%] [G loss: 2.262668]\n",
      "epoch:20 step:19382 [D loss: 0.606009, acc.: 63.28%] [G loss: 2.200186]\n",
      "epoch:20 step:19383 [D loss: 0.569135, acc.: 71.09%] [G loss: 0.872162]\n",
      "epoch:20 step:19384 [D loss: 0.886223, acc.: 33.59%] [G loss: 1.557475]\n",
      "epoch:20 step:19385 [D loss: 0.729612, acc.: 54.69%] [G loss: 1.248795]\n",
      "epoch:20 step:19386 [D loss: 0.808258, acc.: 68.75%] [G loss: 1.590954]\n",
      "epoch:20 step:19387 [D loss: 0.814193, acc.: 58.59%] [G loss: 2.127237]\n",
      "epoch:20 step:19388 [D loss: 0.762448, acc.: 53.12%] [G loss: 1.881110]\n",
      "epoch:20 step:19389 [D loss: 0.862380, acc.: 37.50%] [G loss: 1.299430]\n",
      "epoch:20 step:19390 [D loss: 0.478047, acc.: 85.94%] [G loss: 1.604984]\n",
      "epoch:20 step:19391 [D loss: 0.316984, acc.: 95.31%] [G loss: 1.708408]\n",
      "epoch:20 step:19392 [D loss: 0.337745, acc.: 91.41%] [G loss: 1.177758]\n",
      "epoch:20 step:19393 [D loss: 0.529834, acc.: 72.66%] [G loss: 1.678330]\n",
      "epoch:20 step:19394 [D loss: 0.574269, acc.: 67.97%] [G loss: 1.767956]\n",
      "epoch:20 step:19395 [D loss: 0.396444, acc.: 89.06%] [G loss: 1.256573]\n",
      "epoch:20 step:19396 [D loss: 0.633200, acc.: 60.16%] [G loss: 1.406520]\n",
      "epoch:20 step:19397 [D loss: 1.027252, acc.: 36.72%] [G loss: 1.286529]\n",
      "epoch:20 step:19398 [D loss: 0.595543, acc.: 62.50%] [G loss: 1.900377]\n",
      "epoch:20 step:19399 [D loss: 0.497210, acc.: 75.78%] [G loss: 1.908674]\n",
      "epoch:20 step:19400 [D loss: 0.382407, acc.: 89.06%] [G loss: 0.899113]\n",
      "epoch:20 step:19401 [D loss: 0.443422, acc.: 82.03%] [G loss: 2.436968]\n",
      "epoch:20 step:19402 [D loss: 0.748606, acc.: 45.31%] [G loss: 1.301345]\n",
      "epoch:20 step:19403 [D loss: 0.328170, acc.: 94.53%] [G loss: 1.336606]\n",
      "epoch:20 step:19404 [D loss: 0.387095, acc.: 85.94%] [G loss: 1.728959]\n",
      "epoch:20 step:19405 [D loss: 0.353460, acc.: 90.62%] [G loss: 1.713555]\n",
      "epoch:20 step:19406 [D loss: 0.769077, acc.: 51.56%] [G loss: 1.804685]\n",
      "epoch:20 step:19407 [D loss: 0.433631, acc.: 81.25%] [G loss: 1.830021]\n",
      "epoch:20 step:19408 [D loss: 0.692970, acc.: 58.59%] [G loss: 1.815696]\n",
      "epoch:20 step:19409 [D loss: 0.548518, acc.: 70.31%] [G loss: 1.535536]\n",
      "epoch:20 step:19410 [D loss: 0.480703, acc.: 74.22%] [G loss: 1.272890]\n",
      "epoch:20 step:19411 [D loss: 0.431071, acc.: 71.88%] [G loss: 2.396628]\n",
      "epoch:20 step:19412 [D loss: 0.647571, acc.: 63.28%] [G loss: 1.099041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19413 [D loss: 0.409442, acc.: 89.06%] [G loss: 0.762404]\n",
      "epoch:20 step:19414 [D loss: 0.503017, acc.: 74.22%] [G loss: 1.121413]\n",
      "epoch:20 step:19415 [D loss: 0.361439, acc.: 88.28%] [G loss: 1.190109]\n",
      "epoch:20 step:19416 [D loss: 0.598588, acc.: 67.19%] [G loss: 2.515759]\n",
      "epoch:20 step:19417 [D loss: 1.154762, acc.: 23.44%] [G loss: 2.181226]\n",
      "epoch:20 step:19418 [D loss: 0.936269, acc.: 34.38%] [G loss: 1.566381]\n",
      "epoch:20 step:19419 [D loss: 0.488518, acc.: 80.47%] [G loss: 1.074694]\n",
      "epoch:20 step:19420 [D loss: 0.567407, acc.: 71.09%] [G loss: 1.049057]\n",
      "epoch:20 step:19421 [D loss: 0.316787, acc.: 92.19%] [G loss: 1.308476]\n",
      "epoch:20 step:19422 [D loss: 0.462979, acc.: 82.81%] [G loss: 0.869452]\n",
      "epoch:20 step:19423 [D loss: 0.945051, acc.: 34.38%] [G loss: 1.932306]\n",
      "epoch:20 step:19424 [D loss: 0.395061, acc.: 84.38%] [G loss: 1.776244]\n",
      "epoch:20 step:19425 [D loss: 0.486987, acc.: 84.38%] [G loss: 1.793179]\n",
      "epoch:20 step:19426 [D loss: 1.149318, acc.: 22.66%] [G loss: 0.744723]\n",
      "epoch:20 step:19427 [D loss: 0.405791, acc.: 89.06%] [G loss: 1.379306]\n",
      "epoch:20 step:19428 [D loss: 0.520479, acc.: 71.88%] [G loss: 1.359558]\n",
      "epoch:20 step:19429 [D loss: 0.365719, acc.: 84.38%] [G loss: 1.959187]\n",
      "epoch:20 step:19430 [D loss: 0.565055, acc.: 71.09%] [G loss: 1.196661]\n",
      "epoch:20 step:19431 [D loss: 0.422219, acc.: 86.72%] [G loss: 1.221354]\n",
      "epoch:20 step:19432 [D loss: 0.564454, acc.: 70.31%] [G loss: 1.092760]\n",
      "epoch:20 step:19433 [D loss: 0.524376, acc.: 68.75%] [G loss: 1.235258]\n",
      "epoch:20 step:19434 [D loss: 0.625465, acc.: 67.19%] [G loss: 1.074167]\n",
      "epoch:20 step:19435 [D loss: 0.614355, acc.: 63.28%] [G loss: 1.260918]\n",
      "epoch:20 step:19436 [D loss: 0.726347, acc.: 56.25%] [G loss: 0.848139]\n",
      "epoch:20 step:19437 [D loss: 0.392685, acc.: 85.94%] [G loss: 1.211073]\n",
      "epoch:20 step:19438 [D loss: 0.704061, acc.: 58.59%] [G loss: 0.957447]\n",
      "epoch:20 step:19439 [D loss: 0.311321, acc.: 93.75%] [G loss: 0.972291]\n",
      "epoch:20 step:19440 [D loss: 0.585091, acc.: 64.06%] [G loss: 1.493992]\n",
      "epoch:20 step:19441 [D loss: 0.635684, acc.: 64.84%] [G loss: 0.781311]\n",
      "epoch:20 step:19442 [D loss: 0.467804, acc.: 76.56%] [G loss: 0.912114]\n",
      "epoch:20 step:19443 [D loss: 0.523915, acc.: 75.78%] [G loss: 1.008175]\n",
      "epoch:20 step:19444 [D loss: 0.347964, acc.: 85.16%] [G loss: 0.699453]\n",
      "epoch:20 step:19445 [D loss: 0.647331, acc.: 64.84%] [G loss: 1.822788]\n",
      "epoch:20 step:19446 [D loss: 0.298556, acc.: 91.41%] [G loss: 1.247093]\n",
      "epoch:20 step:19447 [D loss: 0.241168, acc.: 95.31%] [G loss: 2.193216]\n",
      "epoch:20 step:19448 [D loss: 0.873171, acc.: 42.97%] [G loss: 1.447373]\n",
      "epoch:20 step:19449 [D loss: 0.408694, acc.: 84.38%] [G loss: 1.179268]\n",
      "epoch:20 step:19450 [D loss: 0.578948, acc.: 71.09%] [G loss: 1.429742]\n",
      "epoch:20 step:19451 [D loss: 0.364496, acc.: 92.19%] [G loss: 1.533254]\n",
      "epoch:20 step:19452 [D loss: 0.583437, acc.: 75.78%] [G loss: 0.859258]\n",
      "epoch:20 step:19453 [D loss: 0.644841, acc.: 64.84%] [G loss: 1.407226]\n",
      "epoch:20 step:19454 [D loss: 0.413206, acc.: 85.94%] [G loss: 1.744108]\n",
      "epoch:20 step:19455 [D loss: 0.553462, acc.: 68.75%] [G loss: 1.860821]\n",
      "epoch:20 step:19456 [D loss: 0.487093, acc.: 77.34%] [G loss: 1.579481]\n",
      "epoch:20 step:19457 [D loss: 0.694289, acc.: 60.16%] [G loss: 0.900934]\n",
      "epoch:20 step:19458 [D loss: 0.510672, acc.: 70.31%] [G loss: 1.066988]\n",
      "epoch:20 step:19459 [D loss: 0.476179, acc.: 76.56%] [G loss: 1.626055]\n",
      "epoch:20 step:19460 [D loss: 0.620488, acc.: 62.50%] [G loss: 1.667717]\n",
      "epoch:20 step:19461 [D loss: 0.467935, acc.: 76.56%] [G loss: 0.909415]\n",
      "epoch:20 step:19462 [D loss: 0.549891, acc.: 72.66%] [G loss: 1.529944]\n",
      "epoch:20 step:19463 [D loss: 0.419668, acc.: 85.94%] [G loss: 1.724454]\n",
      "epoch:20 step:19464 [D loss: 0.278649, acc.: 95.31%] [G loss: 1.416671]\n",
      "epoch:20 step:19465 [D loss: 0.657592, acc.: 70.31%] [G loss: 2.118073]\n",
      "epoch:20 step:19466 [D loss: 0.523246, acc.: 68.75%] [G loss: 1.971506]\n",
      "epoch:20 step:19467 [D loss: 0.427394, acc.: 85.16%] [G loss: 1.269615]\n",
      "epoch:20 step:19468 [D loss: 0.782591, acc.: 47.66%] [G loss: 1.493565]\n",
      "epoch:20 step:19469 [D loss: 0.370336, acc.: 89.84%] [G loss: 0.868162]\n",
      "epoch:20 step:19470 [D loss: 0.460935, acc.: 80.47%] [G loss: 1.916877]\n",
      "epoch:20 step:19471 [D loss: 0.506095, acc.: 75.00%] [G loss: 1.088073]\n",
      "epoch:20 step:19472 [D loss: 0.663676, acc.: 57.81%] [G loss: 2.159467]\n",
      "epoch:20 step:19473 [D loss: 0.872040, acc.: 38.28%] [G loss: 1.075988]\n",
      "epoch:20 step:19474 [D loss: 0.556483, acc.: 67.97%] [G loss: 1.549512]\n",
      "epoch:20 step:19475 [D loss: 0.338494, acc.: 91.41%] [G loss: 1.509825]\n",
      "epoch:20 step:19476 [D loss: 0.516024, acc.: 81.25%] [G loss: 2.088412]\n",
      "epoch:20 step:19477 [D loss: 0.401719, acc.: 86.72%] [G loss: 1.284340]\n",
      "epoch:20 step:19478 [D loss: 0.700150, acc.: 54.69%] [G loss: 1.632520]\n",
      "epoch:20 step:19479 [D loss: 0.640674, acc.: 63.28%] [G loss: 1.073111]\n",
      "epoch:20 step:19480 [D loss: 0.447608, acc.: 77.34%] [G loss: 1.565892]\n",
      "epoch:20 step:19481 [D loss: 0.486456, acc.: 78.12%] [G loss: 1.100160]\n",
      "epoch:20 step:19482 [D loss: 0.540446, acc.: 73.44%] [G loss: 1.137107]\n",
      "epoch:20 step:19483 [D loss: 0.427030, acc.: 82.81%] [G loss: 1.622395]\n",
      "epoch:20 step:19484 [D loss: 0.258725, acc.: 93.75%] [G loss: 2.004316]\n",
      "epoch:20 step:19485 [D loss: 0.483083, acc.: 77.34%] [G loss: 1.003261]\n",
      "epoch:20 step:19486 [D loss: 0.614220, acc.: 58.59%] [G loss: 1.106887]\n",
      "epoch:20 step:19487 [D loss: 0.675703, acc.: 58.59%] [G loss: 1.248099]\n",
      "epoch:20 step:19488 [D loss: 0.455974, acc.: 75.00%] [G loss: 3.066766]\n",
      "epoch:20 step:19489 [D loss: 0.282959, acc.: 96.09%] [G loss: 1.171081]\n",
      "epoch:20 step:19490 [D loss: 0.510802, acc.: 81.25%] [G loss: 2.117920]\n",
      "epoch:20 step:19491 [D loss: 0.683902, acc.: 60.94%] [G loss: 1.729199]\n",
      "epoch:20 step:19492 [D loss: 0.578233, acc.: 66.41%] [G loss: 1.307118]\n",
      "epoch:20 step:19493 [D loss: 0.357257, acc.: 91.41%] [G loss: 1.134133]\n",
      "epoch:20 step:19494 [D loss: 0.559891, acc.: 69.53%] [G loss: 0.877902]\n",
      "epoch:20 step:19495 [D loss: 0.790577, acc.: 46.09%] [G loss: 0.653036]\n",
      "epoch:20 step:19496 [D loss: 0.390924, acc.: 84.38%] [G loss: 1.452202]\n",
      "epoch:20 step:19497 [D loss: 0.496125, acc.: 79.69%] [G loss: 1.689362]\n",
      "epoch:20 step:19498 [D loss: 0.374913, acc.: 86.72%] [G loss: 1.904358]\n",
      "epoch:20 step:19499 [D loss: 0.452045, acc.: 84.38%] [G loss: 1.662742]\n",
      "epoch:20 step:19500 [D loss: 0.848485, acc.: 52.34%] [G loss: 2.054946]\n",
      "epoch:20 step:19501 [D loss: 0.467919, acc.: 82.81%] [G loss: 1.485049]\n",
      "epoch:20 step:19502 [D loss: 0.496020, acc.: 74.22%] [G loss: 1.900463]\n",
      "epoch:20 step:19503 [D loss: 0.343649, acc.: 87.50%] [G loss: 1.935846]\n",
      "epoch:20 step:19504 [D loss: 0.369450, acc.: 92.19%] [G loss: 1.819246]\n",
      "epoch:20 step:19505 [D loss: 0.663452, acc.: 60.94%] [G loss: 1.171653]\n",
      "epoch:20 step:19506 [D loss: 0.584567, acc.: 67.97%] [G loss: 1.455589]\n",
      "epoch:20 step:19507 [D loss: 0.391385, acc.: 88.28%] [G loss: 1.857727]\n",
      "epoch:20 step:19508 [D loss: 0.579511, acc.: 74.22%] [G loss: 2.052778]\n",
      "epoch:20 step:19509 [D loss: 0.399662, acc.: 86.72%] [G loss: 1.494694]\n",
      "epoch:20 step:19510 [D loss: 0.433848, acc.: 82.03%] [G loss: 2.844551]\n",
      "epoch:20 step:19511 [D loss: 0.491663, acc.: 81.25%] [G loss: 1.350478]\n",
      "epoch:20 step:19512 [D loss: 0.375623, acc.: 86.72%] [G loss: 1.741378]\n",
      "epoch:20 step:19513 [D loss: 0.568047, acc.: 64.84%] [G loss: 0.948189]\n",
      "epoch:20 step:19514 [D loss: 0.742939, acc.: 52.34%] [G loss: 1.755534]\n",
      "epoch:20 step:19515 [D loss: 0.842021, acc.: 53.12%] [G loss: 1.911668]\n",
      "epoch:20 step:19516 [D loss: 0.634255, acc.: 58.59%] [G loss: 1.761275]\n",
      "epoch:20 step:19517 [D loss: 0.374276, acc.: 85.16%] [G loss: 1.932547]\n",
      "epoch:20 step:19518 [D loss: 0.323120, acc.: 91.41%] [G loss: 2.264374]\n",
      "epoch:20 step:19519 [D loss: 0.665759, acc.: 60.16%] [G loss: 1.176384]\n",
      "epoch:20 step:19520 [D loss: 0.508983, acc.: 74.22%] [G loss: 1.246622]\n",
      "epoch:20 step:19521 [D loss: 0.350562, acc.: 89.84%] [G loss: 1.320222]\n",
      "epoch:20 step:19522 [D loss: 0.291101, acc.: 93.75%] [G loss: 2.443857]\n",
      "epoch:20 step:19523 [D loss: 0.401716, acc.: 81.25%] [G loss: 2.001228]\n",
      "epoch:20 step:19524 [D loss: 0.427195, acc.: 83.59%] [G loss: 1.227117]\n",
      "epoch:20 step:19525 [D loss: 0.409687, acc.: 82.81%] [G loss: 1.086460]\n",
      "epoch:20 step:19526 [D loss: 0.320679, acc.: 87.50%] [G loss: 0.815500]\n",
      "epoch:20 step:19527 [D loss: 1.218066, acc.: 28.91%] [G loss: 2.498336]\n",
      "epoch:20 step:19528 [D loss: 0.475180, acc.: 75.78%] [G loss: 1.743339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19529 [D loss: 0.407769, acc.: 84.38%] [G loss: 2.237494]\n",
      "epoch:20 step:19530 [D loss: 0.871338, acc.: 43.75%] [G loss: 2.434655]\n",
      "epoch:20 step:19531 [D loss: 0.277862, acc.: 89.84%] [G loss: 1.104037]\n",
      "epoch:20 step:19532 [D loss: 0.265076, acc.: 92.19%] [G loss: 2.579377]\n",
      "epoch:20 step:19533 [D loss: 0.209843, acc.: 93.75%] [G loss: 2.730585]\n",
      "epoch:20 step:19534 [D loss: 0.696572, acc.: 64.06%] [G loss: 1.000666]\n",
      "epoch:20 step:19535 [D loss: 0.738085, acc.: 60.94%] [G loss: 1.727657]\n",
      "epoch:20 step:19536 [D loss: 0.486551, acc.: 71.09%] [G loss: 1.912904]\n",
      "epoch:20 step:19537 [D loss: 0.617521, acc.: 73.44%] [G loss: 1.544372]\n",
      "epoch:20 step:19538 [D loss: 0.535464, acc.: 72.66%] [G loss: 2.170372]\n",
      "epoch:20 step:19539 [D loss: 0.294434, acc.: 93.75%] [G loss: 4.402770]\n",
      "epoch:20 step:19540 [D loss: 0.392211, acc.: 82.81%] [G loss: 1.705110]\n",
      "epoch:20 step:19541 [D loss: 0.671621, acc.: 63.28%] [G loss: 1.552637]\n",
      "epoch:20 step:19542 [D loss: 0.771897, acc.: 47.66%] [G loss: 1.620829]\n",
      "epoch:20 step:19543 [D loss: 0.471326, acc.: 78.91%] [G loss: 2.024086]\n",
      "epoch:20 step:19544 [D loss: 0.376504, acc.: 81.25%] [G loss: 1.031696]\n",
      "epoch:20 step:19545 [D loss: 0.646813, acc.: 57.81%] [G loss: 1.163769]\n",
      "epoch:20 step:19546 [D loss: 0.729189, acc.: 58.59%] [G loss: 1.597989]\n",
      "epoch:20 step:19547 [D loss: 0.642502, acc.: 66.41%] [G loss: 1.364469]\n",
      "epoch:20 step:19548 [D loss: 0.544062, acc.: 74.22%] [G loss: 0.742083]\n",
      "epoch:20 step:19549 [D loss: 0.578295, acc.: 66.41%] [G loss: 3.265484]\n",
      "epoch:20 step:19550 [D loss: 0.776244, acc.: 53.12%] [G loss: 0.974373]\n",
      "epoch:20 step:19551 [D loss: 0.560589, acc.: 69.53%] [G loss: 1.742743]\n",
      "epoch:20 step:19552 [D loss: 0.399702, acc.: 89.06%] [G loss: 0.610924]\n",
      "epoch:20 step:19553 [D loss: 0.369597, acc.: 85.16%] [G loss: 1.643804]\n",
      "epoch:20 step:19554 [D loss: 0.628479, acc.: 67.97%] [G loss: 1.100269]\n",
      "epoch:20 step:19555 [D loss: 0.458006, acc.: 80.47%] [G loss: 1.708853]\n",
      "epoch:20 step:19556 [D loss: 0.264250, acc.: 93.75%] [G loss: 0.983591]\n",
      "epoch:20 step:19557 [D loss: 0.540352, acc.: 73.44%] [G loss: 1.801327]\n",
      "epoch:20 step:19558 [D loss: 0.399175, acc.: 83.59%] [G loss: 1.226052]\n",
      "epoch:20 step:19559 [D loss: 0.328447, acc.: 89.06%] [G loss: 0.665178]\n",
      "epoch:20 step:19560 [D loss: 0.380279, acc.: 86.72%] [G loss: 1.023969]\n",
      "epoch:20 step:19561 [D loss: 0.946386, acc.: 46.88%] [G loss: 0.951939]\n",
      "epoch:20 step:19562 [D loss: 1.303680, acc.: 18.75%] [G loss: 1.532126]\n",
      "epoch:20 step:19563 [D loss: 0.705844, acc.: 57.03%] [G loss: 1.615517]\n",
      "epoch:20 step:19564 [D loss: 0.724214, acc.: 56.25%] [G loss: 2.260616]\n",
      "epoch:20 step:19565 [D loss: 0.459792, acc.: 81.25%] [G loss: 2.425308]\n",
      "epoch:20 step:19566 [D loss: 0.362487, acc.: 88.28%] [G loss: 2.320084]\n",
      "epoch:20 step:19567 [D loss: 0.355020, acc.: 89.06%] [G loss: 1.070757]\n",
      "epoch:20 step:19568 [D loss: 0.422905, acc.: 81.25%] [G loss: 1.678075]\n",
      "epoch:20 step:19569 [D loss: 0.341035, acc.: 89.84%] [G loss: 1.194844]\n",
      "epoch:20 step:19570 [D loss: 0.536650, acc.: 69.53%] [G loss: 1.868616]\n",
      "epoch:20 step:19571 [D loss: 0.475002, acc.: 73.44%] [G loss: 1.246637]\n",
      "epoch:20 step:19572 [D loss: 0.807946, acc.: 41.41%] [G loss: 0.774021]\n",
      "epoch:20 step:19573 [D loss: 0.453822, acc.: 78.12%] [G loss: 1.402048]\n",
      "epoch:20 step:19574 [D loss: 0.440217, acc.: 80.47%] [G loss: 1.884090]\n",
      "epoch:20 step:19575 [D loss: 0.394722, acc.: 89.06%] [G loss: 2.355631]\n",
      "epoch:20 step:19576 [D loss: 0.646288, acc.: 64.06%] [G loss: 1.647737]\n",
      "epoch:20 step:19577 [D loss: 0.569802, acc.: 74.22%] [G loss: 1.896649]\n",
      "epoch:20 step:19578 [D loss: 0.510191, acc.: 75.00%] [G loss: 1.521113]\n",
      "epoch:20 step:19579 [D loss: 0.483896, acc.: 77.34%] [G loss: 1.317780]\n",
      "epoch:20 step:19580 [D loss: 0.376130, acc.: 88.28%] [G loss: 2.680357]\n",
      "epoch:20 step:19581 [D loss: 0.383955, acc.: 88.28%] [G loss: 1.722608]\n",
      "epoch:20 step:19582 [D loss: 0.711835, acc.: 55.47%] [G loss: 1.544874]\n",
      "epoch:20 step:19583 [D loss: 0.344395, acc.: 88.28%] [G loss: 3.731663]\n",
      "epoch:20 step:19584 [D loss: 0.518614, acc.: 75.78%] [G loss: 1.447156]\n",
      "epoch:20 step:19585 [D loss: 0.613023, acc.: 69.53%] [G loss: 1.987590]\n",
      "epoch:20 step:19586 [D loss: 0.663350, acc.: 60.94%] [G loss: 1.481695]\n",
      "epoch:20 step:19587 [D loss: 0.669875, acc.: 57.81%] [G loss: 1.174639]\n",
      "epoch:20 step:19588 [D loss: 0.392204, acc.: 86.72%] [G loss: 1.397182]\n",
      "epoch:20 step:19589 [D loss: 0.824333, acc.: 45.31%] [G loss: 1.327093]\n",
      "epoch:20 step:19590 [D loss: 0.397794, acc.: 80.47%] [G loss: 0.901813]\n",
      "epoch:20 step:19591 [D loss: 0.636277, acc.: 65.62%] [G loss: 1.133290]\n",
      "epoch:20 step:19592 [D loss: 0.636880, acc.: 60.16%] [G loss: 0.717378]\n",
      "epoch:20 step:19593 [D loss: 0.592098, acc.: 61.72%] [G loss: 1.119010]\n",
      "epoch:20 step:19594 [D loss: 0.342092, acc.: 87.50%] [G loss: 0.618602]\n",
      "epoch:20 step:19595 [D loss: 0.615636, acc.: 66.41%] [G loss: 1.423520]\n",
      "epoch:20 step:19596 [D loss: 0.651765, acc.: 61.72%] [G loss: 1.031432]\n",
      "epoch:20 step:19597 [D loss: 0.324968, acc.: 91.41%] [G loss: 1.697088]\n",
      "epoch:20 step:19598 [D loss: 0.499175, acc.: 75.00%] [G loss: 1.513750]\n",
      "epoch:20 step:19599 [D loss: 0.469594, acc.: 75.00%] [G loss: 1.322244]\n",
      "epoch:20 step:19600 [D loss: 0.410662, acc.: 85.16%] [G loss: 1.077890]\n",
      "epoch:20 step:19601 [D loss: 0.441680, acc.: 82.03%] [G loss: 1.371448]\n",
      "epoch:20 step:19602 [D loss: 0.409835, acc.: 84.38%] [G loss: 1.274985]\n",
      "epoch:20 step:19603 [D loss: 0.454376, acc.: 78.91%] [G loss: 1.830301]\n",
      "epoch:20 step:19604 [D loss: 0.252196, acc.: 96.09%] [G loss: 1.646510]\n",
      "epoch:20 step:19605 [D loss: 0.315196, acc.: 92.97%] [G loss: 1.231254]\n",
      "epoch:20 step:19606 [D loss: 0.524168, acc.: 72.66%] [G loss: 1.306604]\n",
      "epoch:20 step:19607 [D loss: 0.538335, acc.: 74.22%] [G loss: 1.140736]\n",
      "epoch:20 step:19608 [D loss: 0.326861, acc.: 89.06%] [G loss: 1.706671]\n",
      "epoch:20 step:19609 [D loss: 0.665089, acc.: 58.59%] [G loss: 1.695706]\n",
      "epoch:20 step:19610 [D loss: 0.636303, acc.: 66.41%] [G loss: 0.988744]\n",
      "epoch:20 step:19611 [D loss: 0.700607, acc.: 57.81%] [G loss: 2.051772]\n",
      "epoch:20 step:19612 [D loss: 0.499757, acc.: 77.34%] [G loss: 2.556433]\n",
      "epoch:20 step:19613 [D loss: 0.246061, acc.: 94.53%] [G loss: 2.792901]\n",
      "epoch:20 step:19614 [D loss: 0.663182, acc.: 68.75%] [G loss: 1.317114]\n",
      "epoch:20 step:19615 [D loss: 0.513648, acc.: 71.09%] [G loss: 0.480830]\n",
      "epoch:20 step:19616 [D loss: 0.366560, acc.: 85.94%] [G loss: 1.199754]\n",
      "epoch:20 step:19617 [D loss: 0.417210, acc.: 75.78%] [G loss: 0.865112]\n",
      "epoch:20 step:19618 [D loss: 0.650618, acc.: 66.41%] [G loss: 3.102645]\n",
      "epoch:20 step:19619 [D loss: 0.905213, acc.: 46.09%] [G loss: 1.405596]\n",
      "epoch:20 step:19620 [D loss: 0.687486, acc.: 60.94%] [G loss: 1.441724]\n",
      "epoch:20 step:19621 [D loss: 0.461588, acc.: 73.44%] [G loss: 2.031473]\n",
      "epoch:20 step:19622 [D loss: 0.197436, acc.: 93.75%] [G loss: 3.247247]\n",
      "epoch:20 step:19623 [D loss: 0.787715, acc.: 53.91%] [G loss: 1.922779]\n",
      "epoch:20 step:19624 [D loss: 0.129345, acc.: 99.22%] [G loss: 2.232328]\n",
      "epoch:20 step:19625 [D loss: 0.810039, acc.: 57.81%] [G loss: 1.550583]\n",
      "epoch:20 step:19626 [D loss: 0.305933, acc.: 90.62%] [G loss: 1.112191]\n",
      "epoch:20 step:19627 [D loss: 0.450945, acc.: 78.91%] [G loss: 2.208460]\n",
      "epoch:20 step:19628 [D loss: 0.256971, acc.: 95.31%] [G loss: 2.817409]\n",
      "epoch:20 step:19629 [D loss: 0.259169, acc.: 93.75%] [G loss: 1.176567]\n",
      "epoch:20 step:19630 [D loss: 0.501817, acc.: 82.03%] [G loss: 3.187369]\n",
      "epoch:20 step:19631 [D loss: 0.637287, acc.: 60.16%] [G loss: 1.197869]\n",
      "epoch:20 step:19632 [D loss: 0.189580, acc.: 96.09%] [G loss: 0.828295]\n",
      "epoch:20 step:19633 [D loss: 0.611369, acc.: 62.50%] [G loss: 2.696268]\n",
      "epoch:20 step:19634 [D loss: 0.561271, acc.: 72.66%] [G loss: 2.674418]\n",
      "epoch:20 step:19635 [D loss: 0.465115, acc.: 74.22%] [G loss: 0.787473]\n",
      "epoch:20 step:19636 [D loss: 0.697905, acc.: 57.81%] [G loss: 1.266062]\n",
      "epoch:20 step:19637 [D loss: 0.231561, acc.: 95.31%] [G loss: 1.561841]\n",
      "epoch:20 step:19638 [D loss: 0.595340, acc.: 64.84%] [G loss: 1.391140]\n",
      "epoch:20 step:19639 [D loss: 0.149071, acc.: 96.88%] [G loss: 1.303757]\n",
      "epoch:20 step:19640 [D loss: 0.483207, acc.: 77.34%] [G loss: 3.438628]\n",
      "epoch:20 step:19641 [D loss: 0.489892, acc.: 75.78%] [G loss: 2.203006]\n",
      "epoch:20 step:19642 [D loss: 0.783139, acc.: 60.16%] [G loss: 2.891209]\n",
      "epoch:20 step:19643 [D loss: 0.538634, acc.: 75.78%] [G loss: 1.183668]\n",
      "epoch:20 step:19644 [D loss: 0.413530, acc.: 84.38%] [G loss: 2.236221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19645 [D loss: 0.849750, acc.: 39.06%] [G loss: 1.504550]\n",
      "epoch:20 step:19646 [D loss: 0.433985, acc.: 83.59%] [G loss: 1.751313]\n",
      "epoch:20 step:19647 [D loss: 0.669802, acc.: 64.06%] [G loss: 0.986958]\n",
      "epoch:20 step:19648 [D loss: 0.456525, acc.: 72.66%] [G loss: 1.944177]\n",
      "epoch:20 step:19649 [D loss: 0.499656, acc.: 74.22%] [G loss: 2.896807]\n",
      "epoch:20 step:19650 [D loss: 0.409013, acc.: 85.94%] [G loss: 2.056330]\n",
      "epoch:20 step:19651 [D loss: 0.623626, acc.: 63.28%] [G loss: 1.210350]\n",
      "epoch:20 step:19652 [D loss: 0.787095, acc.: 49.22%] [G loss: 1.998398]\n",
      "epoch:20 step:19653 [D loss: 0.751136, acc.: 50.00%] [G loss: 1.393637]\n",
      "epoch:20 step:19654 [D loss: 0.276285, acc.: 92.19%] [G loss: 1.587199]\n",
      "epoch:20 step:19655 [D loss: 0.293831, acc.: 91.41%] [G loss: 1.090564]\n",
      "epoch:20 step:19656 [D loss: 0.285095, acc.: 93.75%] [G loss: 1.362139]\n",
      "epoch:20 step:19657 [D loss: 0.419781, acc.: 82.03%] [G loss: 2.508751]\n",
      "epoch:20 step:19658 [D loss: 0.372080, acc.: 87.50%] [G loss: 2.844802]\n",
      "epoch:20 step:19659 [D loss: 0.408666, acc.: 87.50%] [G loss: 2.338696]\n",
      "epoch:20 step:19660 [D loss: 0.529607, acc.: 72.66%] [G loss: 2.483239]\n",
      "epoch:20 step:19661 [D loss: 0.550427, acc.: 72.66%] [G loss: 1.811111]\n",
      "epoch:20 step:19662 [D loss: 0.651502, acc.: 64.06%] [G loss: 1.811908]\n",
      "epoch:20 step:19663 [D loss: 0.321151, acc.: 91.41%] [G loss: 3.210504]\n",
      "epoch:20 step:19664 [D loss: 0.313015, acc.: 87.50%] [G loss: 3.002409]\n",
      "epoch:20 step:19665 [D loss: 0.258575, acc.: 93.75%] [G loss: 1.889768]\n",
      "epoch:20 step:19666 [D loss: 0.726243, acc.: 56.25%] [G loss: 1.804622]\n",
      "epoch:20 step:19667 [D loss: 0.385150, acc.: 80.47%] [G loss: 0.871698]\n",
      "epoch:20 step:19668 [D loss: 0.648798, acc.: 64.84%] [G loss: 1.891700]\n",
      "epoch:20 step:19669 [D loss: 0.367525, acc.: 86.72%] [G loss: 1.388787]\n",
      "epoch:20 step:19670 [D loss: 0.497067, acc.: 75.78%] [G loss: 1.889830]\n",
      "epoch:20 step:19671 [D loss: 0.253287, acc.: 96.88%] [G loss: 3.094755]\n",
      "epoch:20 step:19672 [D loss: 0.537686, acc.: 67.97%] [G loss: 1.364686]\n",
      "epoch:20 step:19673 [D loss: 0.741063, acc.: 57.03%] [G loss: 1.321720]\n",
      "epoch:20 step:19674 [D loss: 0.717258, acc.: 58.59%] [G loss: 1.306734]\n",
      "epoch:20 step:19675 [D loss: 0.277217, acc.: 94.53%] [G loss: 1.342182]\n",
      "epoch:20 step:19676 [D loss: 0.278215, acc.: 91.41%] [G loss: 1.127979]\n",
      "epoch:20 step:19677 [D loss: 0.725733, acc.: 59.38%] [G loss: 2.349568]\n",
      "epoch:21 step:19678 [D loss: 0.254847, acc.: 96.09%] [G loss: 2.388045]\n",
      "epoch:21 step:19679 [D loss: 0.199026, acc.: 99.22%] [G loss: 2.225142]\n",
      "epoch:21 step:19680 [D loss: 0.657628, acc.: 56.25%] [G loss: 2.468345]\n",
      "epoch:21 step:19681 [D loss: 0.500307, acc.: 79.69%] [G loss: 1.433670]\n",
      "epoch:21 step:19682 [D loss: 0.326250, acc.: 92.19%] [G loss: 1.792357]\n",
      "epoch:21 step:19683 [D loss: 0.297567, acc.: 90.62%] [G loss: 1.928304]\n",
      "epoch:21 step:19684 [D loss: 0.469334, acc.: 75.78%] [G loss: 2.477517]\n",
      "epoch:21 step:19685 [D loss: 0.703074, acc.: 57.81%] [G loss: 0.629011]\n",
      "epoch:21 step:19686 [D loss: 0.521427, acc.: 79.69%] [G loss: 0.745724]\n",
      "epoch:21 step:19687 [D loss: 0.858306, acc.: 55.47%] [G loss: 2.618545]\n",
      "epoch:21 step:19688 [D loss: 0.964298, acc.: 52.34%] [G loss: 0.687354]\n",
      "epoch:21 step:19689 [D loss: 0.656670, acc.: 60.16%] [G loss: 1.041759]\n",
      "epoch:21 step:19690 [D loss: 0.385513, acc.: 84.38%] [G loss: 1.317741]\n",
      "epoch:21 step:19691 [D loss: 0.297448, acc.: 94.53%] [G loss: 1.302003]\n",
      "epoch:21 step:19692 [D loss: 0.352220, acc.: 89.84%] [G loss: 1.209468]\n",
      "epoch:21 step:19693 [D loss: 0.421582, acc.: 76.56%] [G loss: 2.043903]\n",
      "epoch:21 step:19694 [D loss: 0.491845, acc.: 78.91%] [G loss: 0.848756]\n",
      "epoch:21 step:19695 [D loss: 0.269806, acc.: 93.75%] [G loss: 0.828374]\n",
      "epoch:21 step:19696 [D loss: 0.175843, acc.: 98.44%] [G loss: 1.546432]\n",
      "epoch:21 step:19697 [D loss: 0.674605, acc.: 61.72%] [G loss: 2.524883]\n",
      "epoch:21 step:19698 [D loss: 0.548580, acc.: 68.75%] [G loss: 3.293593]\n",
      "epoch:21 step:19699 [D loss: 0.812154, acc.: 47.66%] [G loss: 3.499007]\n",
      "epoch:21 step:19700 [D loss: 0.374377, acc.: 83.59%] [G loss: 1.116232]\n",
      "epoch:21 step:19701 [D loss: 0.836660, acc.: 46.88%] [G loss: 1.843287]\n",
      "epoch:21 step:19702 [D loss: 0.450233, acc.: 81.25%] [G loss: 1.724547]\n",
      "epoch:21 step:19703 [D loss: 1.154085, acc.: 50.78%] [G loss: 1.059260]\n",
      "epoch:21 step:19704 [D loss: 0.665410, acc.: 59.38%] [G loss: 1.674718]\n",
      "epoch:21 step:19705 [D loss: 1.078613, acc.: 55.47%] [G loss: 0.879043]\n",
      "epoch:21 step:19706 [D loss: 0.670569, acc.: 62.50%] [G loss: 1.825100]\n",
      "epoch:21 step:19707 [D loss: 0.463881, acc.: 77.34%] [G loss: 2.472233]\n",
      "epoch:21 step:19708 [D loss: 0.697322, acc.: 59.38%] [G loss: 0.999726]\n",
      "epoch:21 step:19709 [D loss: 0.355918, acc.: 91.41%] [G loss: 1.334903]\n",
      "epoch:21 step:19710 [D loss: 0.459160, acc.: 85.16%] [G loss: 3.330749]\n",
      "epoch:21 step:19711 [D loss: 0.546370, acc.: 68.75%] [G loss: 0.944356]\n",
      "epoch:21 step:19712 [D loss: 0.450046, acc.: 82.03%] [G loss: 2.125909]\n",
      "epoch:21 step:19713 [D loss: 0.791959, acc.: 49.22%] [G loss: 1.740958]\n",
      "epoch:21 step:19714 [D loss: 0.804448, acc.: 40.62%] [G loss: 1.347947]\n",
      "epoch:21 step:19715 [D loss: 0.535492, acc.: 75.78%] [G loss: 1.204918]\n",
      "epoch:21 step:19716 [D loss: 0.459194, acc.: 81.25%] [G loss: 1.206803]\n",
      "epoch:21 step:19717 [D loss: 0.457529, acc.: 70.31%] [G loss: 2.236495]\n",
      "epoch:21 step:19718 [D loss: 0.600997, acc.: 62.50%] [G loss: 1.874234]\n",
      "epoch:21 step:19719 [D loss: 0.576001, acc.: 61.72%] [G loss: 2.406006]\n",
      "epoch:21 step:19720 [D loss: 0.464076, acc.: 71.88%] [G loss: 2.641911]\n",
      "epoch:21 step:19721 [D loss: 0.477191, acc.: 75.00%] [G loss: 2.334701]\n",
      "epoch:21 step:19722 [D loss: 0.559788, acc.: 75.78%] [G loss: 1.664707]\n",
      "epoch:21 step:19723 [D loss: 0.338588, acc.: 89.84%] [G loss: 1.547933]\n",
      "epoch:21 step:19724 [D loss: 0.405687, acc.: 85.16%] [G loss: 1.623478]\n",
      "epoch:21 step:19725 [D loss: 0.840177, acc.: 50.78%] [G loss: 1.245317]\n",
      "epoch:21 step:19726 [D loss: 0.297104, acc.: 93.75%] [G loss: 1.858128]\n",
      "epoch:21 step:19727 [D loss: 0.446975, acc.: 84.38%] [G loss: 1.721993]\n",
      "epoch:21 step:19728 [D loss: 0.536734, acc.: 67.97%] [G loss: 2.284730]\n",
      "epoch:21 step:19729 [D loss: 0.423585, acc.: 81.25%] [G loss: 0.787754]\n",
      "epoch:21 step:19730 [D loss: 0.699188, acc.: 59.38%] [G loss: 0.754895]\n",
      "epoch:21 step:19731 [D loss: 0.557750, acc.: 67.97%] [G loss: 2.043561]\n",
      "epoch:21 step:19732 [D loss: 0.462681, acc.: 78.91%] [G loss: 2.431756]\n",
      "epoch:21 step:19733 [D loss: 0.293385, acc.: 90.62%] [G loss: 3.395987]\n",
      "epoch:21 step:19734 [D loss: 0.482453, acc.: 75.00%] [G loss: 1.539861]\n",
      "epoch:21 step:19735 [D loss: 0.327608, acc.: 89.84%] [G loss: 2.019434]\n",
      "epoch:21 step:19736 [D loss: 0.872171, acc.: 54.69%] [G loss: 2.100508]\n",
      "epoch:21 step:19737 [D loss: 0.332401, acc.: 87.50%] [G loss: 1.458354]\n",
      "epoch:21 step:19738 [D loss: 0.597402, acc.: 64.84%] [G loss: 2.651829]\n",
      "epoch:21 step:19739 [D loss: 0.298939, acc.: 91.41%] [G loss: 1.708677]\n",
      "epoch:21 step:19740 [D loss: 0.323186, acc.: 89.06%] [G loss: 2.767152]\n",
      "epoch:21 step:19741 [D loss: 0.492856, acc.: 75.00%] [G loss: 1.054387]\n",
      "epoch:21 step:19742 [D loss: 0.328029, acc.: 85.16%] [G loss: 2.264375]\n",
      "epoch:21 step:19743 [D loss: 0.373964, acc.: 82.81%] [G loss: 1.637129]\n",
      "epoch:21 step:19744 [D loss: 0.523098, acc.: 73.44%] [G loss: 1.977944]\n",
      "epoch:21 step:19745 [D loss: 0.361433, acc.: 88.28%] [G loss: 1.724181]\n",
      "epoch:21 step:19746 [D loss: 0.328056, acc.: 86.72%] [G loss: 0.733319]\n",
      "epoch:21 step:19747 [D loss: 0.248606, acc.: 93.75%] [G loss: 1.579545]\n",
      "epoch:21 step:19748 [D loss: 0.442659, acc.: 82.03%] [G loss: 2.025321]\n",
      "epoch:21 step:19749 [D loss: 0.351433, acc.: 87.50%] [G loss: 1.156178]\n",
      "epoch:21 step:19750 [D loss: 0.133338, acc.: 98.44%] [G loss: 1.856061]\n",
      "epoch:21 step:19751 [D loss: 0.227155, acc.: 95.31%] [G loss: 1.888350]\n",
      "epoch:21 step:19752 [D loss: 0.175377, acc.: 98.44%] [G loss: 0.920282]\n",
      "epoch:21 step:19753 [D loss: 0.735098, acc.: 53.91%] [G loss: 1.195172]\n",
      "epoch:21 step:19754 [D loss: 0.235409, acc.: 96.09%] [G loss: 2.245363]\n",
      "epoch:21 step:19755 [D loss: 0.484664, acc.: 78.12%] [G loss: 1.098085]\n",
      "epoch:21 step:19756 [D loss: 0.634419, acc.: 62.50%] [G loss: 1.172495]\n",
      "epoch:21 step:19757 [D loss: 0.486801, acc.: 76.56%] [G loss: 1.254942]\n",
      "epoch:21 step:19758 [D loss: 0.476161, acc.: 75.00%] [G loss: 1.627353]\n",
      "epoch:21 step:19759 [D loss: 0.532530, acc.: 71.88%] [G loss: 1.403454]\n",
      "epoch:21 step:19760 [D loss: 0.430019, acc.: 85.94%] [G loss: 1.003673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19761 [D loss: 0.292331, acc.: 92.97%] [G loss: 3.051725]\n",
      "epoch:21 step:19762 [D loss: 0.256729, acc.: 92.97%] [G loss: 1.319173]\n",
      "epoch:21 step:19763 [D loss: 0.268185, acc.: 92.19%] [G loss: 1.333130]\n",
      "epoch:21 step:19764 [D loss: 0.268386, acc.: 95.31%] [G loss: 2.129924]\n",
      "epoch:21 step:19765 [D loss: 0.653141, acc.: 62.50%] [G loss: 1.684858]\n",
      "epoch:21 step:19766 [D loss: 0.332077, acc.: 87.50%] [G loss: 2.185090]\n",
      "epoch:21 step:19767 [D loss: 0.245559, acc.: 96.88%] [G loss: 2.607476]\n",
      "epoch:21 step:19768 [D loss: 0.502031, acc.: 70.31%] [G loss: 1.190773]\n",
      "epoch:21 step:19769 [D loss: 0.781520, acc.: 50.78%] [G loss: 3.122786]\n",
      "epoch:21 step:19770 [D loss: 0.415668, acc.: 77.34%] [G loss: 0.994613]\n",
      "epoch:21 step:19771 [D loss: 0.500036, acc.: 77.34%] [G loss: 3.166785]\n",
      "epoch:21 step:19772 [D loss: 0.944640, acc.: 49.22%] [G loss: 1.911692]\n",
      "epoch:21 step:19773 [D loss: 0.375196, acc.: 86.72%] [G loss: 1.446636]\n",
      "epoch:21 step:19774 [D loss: 0.198110, acc.: 96.09%] [G loss: 1.835873]\n",
      "epoch:21 step:19775 [D loss: 0.367047, acc.: 89.06%] [G loss: 1.083726]\n",
      "epoch:21 step:19776 [D loss: 0.825318, acc.: 56.25%] [G loss: 1.999506]\n",
      "epoch:21 step:19777 [D loss: 0.514152, acc.: 71.09%] [G loss: 1.731343]\n",
      "epoch:21 step:19778 [D loss: 0.359142, acc.: 88.28%] [G loss: 2.158309]\n",
      "epoch:21 step:19779 [D loss: 0.498124, acc.: 76.56%] [G loss: 2.631040]\n",
      "epoch:21 step:19780 [D loss: 0.502579, acc.: 74.22%] [G loss: 2.361994]\n",
      "epoch:21 step:19781 [D loss: 0.701816, acc.: 54.69%] [G loss: 1.554438]\n",
      "epoch:21 step:19782 [D loss: 0.419063, acc.: 85.94%] [G loss: 2.129710]\n",
      "epoch:21 step:19783 [D loss: 0.815589, acc.: 53.91%] [G loss: 1.457466]\n",
      "epoch:21 step:19784 [D loss: 0.498972, acc.: 73.44%] [G loss: 0.704960]\n",
      "epoch:21 step:19785 [D loss: 0.857999, acc.: 56.25%] [G loss: 1.940201]\n",
      "epoch:21 step:19786 [D loss: 0.400662, acc.: 82.81%] [G loss: 2.989643]\n",
      "epoch:21 step:19787 [D loss: 0.386051, acc.: 86.72%] [G loss: 0.883114]\n",
      "epoch:21 step:19788 [D loss: 0.487039, acc.: 75.00%] [G loss: 1.818380]\n",
      "epoch:21 step:19789 [D loss: 0.590631, acc.: 65.62%] [G loss: 2.312198]\n",
      "epoch:21 step:19790 [D loss: 0.374925, acc.: 80.47%] [G loss: 1.378227]\n",
      "epoch:21 step:19791 [D loss: 0.740598, acc.: 61.72%] [G loss: 1.556929]\n",
      "epoch:21 step:19792 [D loss: 0.922646, acc.: 43.75%] [G loss: 1.072664]\n",
      "epoch:21 step:19793 [D loss: 0.630871, acc.: 64.84%] [G loss: 1.179917]\n",
      "epoch:21 step:19794 [D loss: 0.562432, acc.: 68.75%] [G loss: 1.367982]\n",
      "epoch:21 step:19795 [D loss: 0.453626, acc.: 77.34%] [G loss: 1.833537]\n",
      "epoch:21 step:19796 [D loss: 0.389226, acc.: 85.94%] [G loss: 0.878367]\n",
      "epoch:21 step:19797 [D loss: 1.613713, acc.: 26.56%] [G loss: 2.104743]\n",
      "epoch:21 step:19798 [D loss: 0.437636, acc.: 79.69%] [G loss: 2.143129]\n",
      "epoch:21 step:19799 [D loss: 0.758834, acc.: 54.69%] [G loss: 1.293725]\n",
      "epoch:21 step:19800 [D loss: 0.230955, acc.: 95.31%] [G loss: 0.635856]\n",
      "epoch:21 step:19801 [D loss: 0.609976, acc.: 64.84%] [G loss: 2.174985]\n",
      "epoch:21 step:19802 [D loss: 0.326010, acc.: 89.06%] [G loss: 1.522884]\n",
      "epoch:21 step:19803 [D loss: 0.568469, acc.: 71.88%] [G loss: 0.901012]\n",
      "epoch:21 step:19804 [D loss: 0.615799, acc.: 65.62%] [G loss: 1.250508]\n",
      "epoch:21 step:19805 [D loss: 0.420275, acc.: 84.38%] [G loss: 2.032967]\n",
      "epoch:21 step:19806 [D loss: 0.467354, acc.: 78.91%] [G loss: 1.485449]\n",
      "epoch:21 step:19807 [D loss: 0.771348, acc.: 58.59%] [G loss: 1.911010]\n",
      "epoch:21 step:19808 [D loss: 0.589454, acc.: 65.62%] [G loss: 1.214620]\n",
      "epoch:21 step:19809 [D loss: 0.461172, acc.: 76.56%] [G loss: 2.061560]\n",
      "epoch:21 step:19810 [D loss: 0.428937, acc.: 82.81%] [G loss: 1.786166]\n",
      "epoch:21 step:19811 [D loss: 0.570410, acc.: 69.53%] [G loss: 1.381319]\n",
      "epoch:21 step:19812 [D loss: 0.726612, acc.: 55.47%] [G loss: 0.869852]\n",
      "epoch:21 step:19813 [D loss: 0.748328, acc.: 50.78%] [G loss: 1.825635]\n",
      "epoch:21 step:19814 [D loss: 0.840720, acc.: 46.09%] [G loss: 1.153255]\n",
      "epoch:21 step:19815 [D loss: 0.596658, acc.: 67.19%] [G loss: 1.265339]\n",
      "epoch:21 step:19816 [D loss: 0.813139, acc.: 47.66%] [G loss: 1.799978]\n",
      "epoch:21 step:19817 [D loss: 0.872865, acc.: 53.12%] [G loss: 1.379092]\n",
      "epoch:21 step:19818 [D loss: 0.616249, acc.: 61.72%] [G loss: 1.648287]\n",
      "epoch:21 step:19819 [D loss: 0.431049, acc.: 87.50%] [G loss: 1.182066]\n",
      "epoch:21 step:19820 [D loss: 0.474185, acc.: 77.34%] [G loss: 1.352064]\n",
      "epoch:21 step:19821 [D loss: 0.539410, acc.: 72.66%] [G loss: 1.690878]\n",
      "epoch:21 step:19822 [D loss: 0.500514, acc.: 75.00%] [G loss: 2.202666]\n",
      "epoch:21 step:19823 [D loss: 0.938495, acc.: 39.06%] [G loss: 1.391459]\n",
      "epoch:21 step:19824 [D loss: 0.659407, acc.: 57.81%] [G loss: 0.928882]\n",
      "epoch:21 step:19825 [D loss: 0.533144, acc.: 71.09%] [G loss: 1.393261]\n",
      "epoch:21 step:19826 [D loss: 0.550179, acc.: 75.00%] [G loss: 1.293265]\n",
      "epoch:21 step:19827 [D loss: 0.370990, acc.: 82.81%] [G loss: 1.657036]\n",
      "epoch:21 step:19828 [D loss: 0.386912, acc.: 85.94%] [G loss: 1.819791]\n",
      "epoch:21 step:19829 [D loss: 0.399308, acc.: 89.06%] [G loss: 1.330833]\n",
      "epoch:21 step:19830 [D loss: 0.326814, acc.: 89.84%] [G loss: 1.368825]\n",
      "epoch:21 step:19831 [D loss: 0.451988, acc.: 78.91%] [G loss: 1.324692]\n",
      "epoch:21 step:19832 [D loss: 0.478129, acc.: 79.69%] [G loss: 2.340213]\n",
      "epoch:21 step:19833 [D loss: 0.272293, acc.: 98.44%] [G loss: 1.331595]\n",
      "epoch:21 step:19834 [D loss: 0.426358, acc.: 90.62%] [G loss: 1.871734]\n",
      "epoch:21 step:19835 [D loss: 0.397580, acc.: 86.72%] [G loss: 1.205248]\n",
      "epoch:21 step:19836 [D loss: 0.274685, acc.: 94.53%] [G loss: 1.530915]\n",
      "epoch:21 step:19837 [D loss: 0.449576, acc.: 79.69%] [G loss: 0.767933]\n",
      "epoch:21 step:19838 [D loss: 0.324393, acc.: 90.62%] [G loss: 1.216802]\n",
      "epoch:21 step:19839 [D loss: 0.378109, acc.: 89.84%] [G loss: 2.256734]\n",
      "epoch:21 step:19840 [D loss: 0.251501, acc.: 97.66%] [G loss: 2.115356]\n",
      "epoch:21 step:19841 [D loss: 0.403733, acc.: 79.69%] [G loss: 2.346742]\n",
      "epoch:21 step:19842 [D loss: 0.999376, acc.: 32.81%] [G loss: 0.916936]\n",
      "epoch:21 step:19843 [D loss: 0.564941, acc.: 70.31%] [G loss: 1.055163]\n",
      "epoch:21 step:19844 [D loss: 0.493958, acc.: 75.00%] [G loss: 2.039248]\n",
      "epoch:21 step:19845 [D loss: 0.301291, acc.: 87.50%] [G loss: 2.057629]\n",
      "epoch:21 step:19846 [D loss: 0.337628, acc.: 89.84%] [G loss: 1.579220]\n",
      "epoch:21 step:19847 [D loss: 0.519191, acc.: 71.88%] [G loss: 1.365336]\n",
      "epoch:21 step:19848 [D loss: 0.434379, acc.: 82.03%] [G loss: 1.863278]\n",
      "epoch:21 step:19849 [D loss: 0.447605, acc.: 80.47%] [G loss: 1.586820]\n",
      "epoch:21 step:19850 [D loss: 0.768989, acc.: 49.22%] [G loss: 2.311977]\n",
      "epoch:21 step:19851 [D loss: 0.335016, acc.: 89.06%] [G loss: 1.934150]\n",
      "epoch:21 step:19852 [D loss: 0.542400, acc.: 75.78%] [G loss: 2.116717]\n",
      "epoch:21 step:19853 [D loss: 0.776596, acc.: 52.34%] [G loss: 1.481204]\n",
      "epoch:21 step:19854 [D loss: 0.322195, acc.: 90.62%] [G loss: 1.522185]\n",
      "epoch:21 step:19855 [D loss: 0.430179, acc.: 84.38%] [G loss: 1.635076]\n",
      "epoch:21 step:19856 [D loss: 0.726550, acc.: 65.62%] [G loss: 2.285534]\n",
      "epoch:21 step:19857 [D loss: 0.387153, acc.: 83.59%] [G loss: 1.950627]\n",
      "epoch:21 step:19858 [D loss: 0.568312, acc.: 64.06%] [G loss: 1.996568]\n",
      "epoch:21 step:19859 [D loss: 0.351403, acc.: 83.59%] [G loss: 1.480293]\n",
      "epoch:21 step:19860 [D loss: 0.344255, acc.: 86.72%] [G loss: 1.490342]\n",
      "epoch:21 step:19861 [D loss: 0.480286, acc.: 81.25%] [G loss: 1.976581]\n",
      "epoch:21 step:19862 [D loss: 0.578476, acc.: 67.19%] [G loss: 1.660781]\n",
      "epoch:21 step:19863 [D loss: 0.447662, acc.: 79.69%] [G loss: 2.111846]\n",
      "epoch:21 step:19864 [D loss: 0.784208, acc.: 46.09%] [G loss: 1.520785]\n",
      "epoch:21 step:19865 [D loss: 0.495337, acc.: 72.66%] [G loss: 1.037707]\n",
      "epoch:21 step:19866 [D loss: 0.561300, acc.: 73.44%] [G loss: 1.720710]\n",
      "epoch:21 step:19867 [D loss: 0.531149, acc.: 67.97%] [G loss: 1.019034]\n",
      "epoch:21 step:19868 [D loss: 0.602491, acc.: 64.84%] [G loss: 1.419015]\n",
      "epoch:21 step:19869 [D loss: 0.693247, acc.: 61.72%] [G loss: 1.580956]\n",
      "epoch:21 step:19870 [D loss: 0.349647, acc.: 88.28%] [G loss: 1.588430]\n",
      "epoch:21 step:19871 [D loss: 0.547175, acc.: 71.09%] [G loss: 1.460211]\n",
      "epoch:21 step:19872 [D loss: 0.590882, acc.: 63.28%] [G loss: 1.604464]\n",
      "epoch:21 step:19873 [D loss: 0.169027, acc.: 99.22%] [G loss: 1.544578]\n",
      "epoch:21 step:19874 [D loss: 0.375385, acc.: 89.84%] [G loss: 1.754277]\n",
      "epoch:21 step:19875 [D loss: 0.315412, acc.: 91.41%] [G loss: 2.518063]\n",
      "epoch:21 step:19876 [D loss: 0.424450, acc.: 73.44%] [G loss: 1.974126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19877 [D loss: 0.470805, acc.: 75.00%] [G loss: 2.302019]\n",
      "epoch:21 step:19878 [D loss: 0.379006, acc.: 89.06%] [G loss: 1.576363]\n",
      "epoch:21 step:19879 [D loss: 0.539716, acc.: 77.34%] [G loss: 2.550546]\n",
      "epoch:21 step:19880 [D loss: 0.277559, acc.: 93.75%] [G loss: 1.628217]\n",
      "epoch:21 step:19881 [D loss: 1.128279, acc.: 43.75%] [G loss: 1.217227]\n",
      "epoch:21 step:19882 [D loss: 0.493532, acc.: 70.31%] [G loss: 2.033218]\n",
      "epoch:21 step:19883 [D loss: 0.288373, acc.: 94.53%] [G loss: 2.214208]\n",
      "epoch:21 step:19884 [D loss: 0.172125, acc.: 97.66%] [G loss: 1.298223]\n",
      "epoch:21 step:19885 [D loss: 0.532709, acc.: 70.31%] [G loss: 1.533253]\n",
      "epoch:21 step:19886 [D loss: 0.442920, acc.: 82.03%] [G loss: 1.829820]\n",
      "epoch:21 step:19887 [D loss: 0.599282, acc.: 64.06%] [G loss: 1.140603]\n",
      "epoch:21 step:19888 [D loss: 0.438229, acc.: 82.03%] [G loss: 2.220834]\n",
      "epoch:21 step:19889 [D loss: 1.111868, acc.: 37.50%] [G loss: 1.839131]\n",
      "epoch:21 step:19890 [D loss: 0.483732, acc.: 80.47%] [G loss: 2.656594]\n",
      "epoch:21 step:19891 [D loss: 0.601346, acc.: 62.50%] [G loss: 1.645531]\n",
      "epoch:21 step:19892 [D loss: 0.459527, acc.: 82.03%] [G loss: 2.158821]\n",
      "epoch:21 step:19893 [D loss: 0.325666, acc.: 94.53%] [G loss: 1.280333]\n",
      "epoch:21 step:19894 [D loss: 0.393259, acc.: 83.59%] [G loss: 2.015419]\n",
      "epoch:21 step:19895 [D loss: 0.374758, acc.: 82.03%] [G loss: 2.075951]\n",
      "epoch:21 step:19896 [D loss: 0.465963, acc.: 86.72%] [G loss: 1.562620]\n",
      "epoch:21 step:19897 [D loss: 0.392976, acc.: 86.72%] [G loss: 1.150399]\n",
      "epoch:21 step:19898 [D loss: 0.380932, acc.: 85.16%] [G loss: 0.533049]\n",
      "epoch:21 step:19899 [D loss: 0.811594, acc.: 50.00%] [G loss: 1.048229]\n",
      "epoch:21 step:19900 [D loss: 0.654874, acc.: 61.72%] [G loss: 1.324241]\n",
      "epoch:21 step:19901 [D loss: 0.290227, acc.: 92.97%] [G loss: 3.176193]\n",
      "epoch:21 step:19902 [D loss: 0.688633, acc.: 61.72%] [G loss: 1.298153]\n",
      "epoch:21 step:19903 [D loss: 0.856007, acc.: 45.31%] [G loss: 1.813221]\n",
      "epoch:21 step:19904 [D loss: 0.198996, acc.: 98.44%] [G loss: 1.656699]\n",
      "epoch:21 step:19905 [D loss: 0.640173, acc.: 60.94%] [G loss: 2.239689]\n",
      "epoch:21 step:19906 [D loss: 0.785391, acc.: 50.78%] [G loss: 1.245671]\n",
      "epoch:21 step:19907 [D loss: 0.365822, acc.: 85.94%] [G loss: 0.974824]\n",
      "epoch:21 step:19908 [D loss: 0.457529, acc.: 75.00%] [G loss: 0.613596]\n",
      "epoch:21 step:19909 [D loss: 0.429024, acc.: 82.81%] [G loss: 2.553843]\n",
      "epoch:21 step:19910 [D loss: 0.223256, acc.: 97.66%] [G loss: 1.975029]\n",
      "epoch:21 step:19911 [D loss: 0.305777, acc.: 89.84%] [G loss: 2.255210]\n",
      "epoch:21 step:19912 [D loss: 0.250675, acc.: 92.19%] [G loss: 1.510770]\n",
      "epoch:21 step:19913 [D loss: 0.371024, acc.: 89.06%] [G loss: 2.586026]\n",
      "epoch:21 step:19914 [D loss: 0.653201, acc.: 66.41%] [G loss: 0.878806]\n",
      "epoch:21 step:19915 [D loss: 0.539144, acc.: 75.00%] [G loss: 2.490705]\n",
      "epoch:21 step:19916 [D loss: 0.396805, acc.: 78.12%] [G loss: 1.960099]\n",
      "epoch:21 step:19917 [D loss: 0.373906, acc.: 80.47%] [G loss: 2.078302]\n",
      "epoch:21 step:19918 [D loss: 0.220993, acc.: 96.09%] [G loss: 1.283586]\n",
      "epoch:21 step:19919 [D loss: 0.366303, acc.: 85.94%] [G loss: 1.368674]\n",
      "epoch:21 step:19920 [D loss: 0.915566, acc.: 42.19%] [G loss: 1.989047]\n",
      "epoch:21 step:19921 [D loss: 0.375436, acc.: 82.81%] [G loss: 1.570092]\n",
      "epoch:21 step:19922 [D loss: 0.514098, acc.: 71.88%] [G loss: 2.455592]\n",
      "epoch:21 step:19923 [D loss: 0.297094, acc.: 93.75%] [G loss: 1.390170]\n",
      "epoch:21 step:19924 [D loss: 0.592929, acc.: 66.41%] [G loss: 0.681295]\n",
      "epoch:21 step:19925 [D loss: 0.173071, acc.: 98.44%] [G loss: 1.727681]\n",
      "epoch:21 step:19926 [D loss: 0.451712, acc.: 82.03%] [G loss: 1.037601]\n",
      "epoch:21 step:19927 [D loss: 0.552915, acc.: 75.00%] [G loss: 3.590442]\n",
      "epoch:21 step:19928 [D loss: 0.298680, acc.: 90.62%] [G loss: 2.141196]\n",
      "epoch:21 step:19929 [D loss: 1.202352, acc.: 32.81%] [G loss: 2.017502]\n",
      "epoch:21 step:19930 [D loss: 0.374150, acc.: 87.50%] [G loss: 1.592554]\n",
      "epoch:21 step:19931 [D loss: 0.220766, acc.: 92.97%] [G loss: 1.811585]\n",
      "epoch:21 step:19932 [D loss: 0.478593, acc.: 82.03%] [G loss: 2.450124]\n",
      "epoch:21 step:19933 [D loss: 0.409496, acc.: 77.34%] [G loss: 3.308596]\n",
      "epoch:21 step:19934 [D loss: 0.966574, acc.: 42.97%] [G loss: 1.170205]\n",
      "epoch:21 step:19935 [D loss: 0.452911, acc.: 79.69%] [G loss: 1.664258]\n",
      "epoch:21 step:19936 [D loss: 0.400214, acc.: 85.16%] [G loss: 2.735246]\n",
      "epoch:21 step:19937 [D loss: 0.363190, acc.: 88.28%] [G loss: 0.940046]\n",
      "epoch:21 step:19938 [D loss: 0.360517, acc.: 87.50%] [G loss: 1.194361]\n",
      "epoch:21 step:19939 [D loss: 0.354043, acc.: 85.94%] [G loss: 2.675516]\n",
      "epoch:21 step:19940 [D loss: 0.367691, acc.: 87.50%] [G loss: 3.310769]\n",
      "epoch:21 step:19941 [D loss: 0.422616, acc.: 75.78%] [G loss: 1.007607]\n",
      "epoch:21 step:19942 [D loss: 0.351026, acc.: 88.28%] [G loss: 1.844312]\n",
      "epoch:21 step:19943 [D loss: 0.423394, acc.: 82.81%] [G loss: 1.188550]\n",
      "epoch:21 step:19944 [D loss: 0.684102, acc.: 63.28%] [G loss: 1.970470]\n",
      "epoch:21 step:19945 [D loss: 0.526318, acc.: 71.88%] [G loss: 1.854225]\n",
      "epoch:21 step:19946 [D loss: 0.432886, acc.: 81.25%] [G loss: 2.908163]\n",
      "epoch:21 step:19947 [D loss: 0.495380, acc.: 78.91%] [G loss: 1.657355]\n",
      "epoch:21 step:19948 [D loss: 0.338855, acc.: 92.19%] [G loss: 1.166413]\n",
      "epoch:21 step:19949 [D loss: 0.359866, acc.: 85.16%] [G loss: 3.200617]\n",
      "epoch:21 step:19950 [D loss: 1.150256, acc.: 42.19%] [G loss: 1.546444]\n",
      "epoch:21 step:19951 [D loss: 0.198250, acc.: 97.66%] [G loss: 1.720397]\n",
      "epoch:21 step:19952 [D loss: 0.283706, acc.: 90.62%] [G loss: 1.488834]\n",
      "epoch:21 step:19953 [D loss: 0.418987, acc.: 78.91%] [G loss: 1.283777]\n",
      "epoch:21 step:19954 [D loss: 0.362653, acc.: 85.16%] [G loss: 1.791297]\n",
      "epoch:21 step:19955 [D loss: 0.289766, acc.: 93.75%] [G loss: 2.546240]\n",
      "epoch:21 step:19956 [D loss: 0.353623, acc.: 83.59%] [G loss: 2.099121]\n",
      "epoch:21 step:19957 [D loss: 0.307789, acc.: 95.31%] [G loss: 1.313343]\n",
      "epoch:21 step:19958 [D loss: 0.224663, acc.: 96.09%] [G loss: 2.674731]\n",
      "epoch:21 step:19959 [D loss: 1.285850, acc.: 35.94%] [G loss: 0.982608]\n",
      "epoch:21 step:19960 [D loss: 0.375711, acc.: 89.06%] [G loss: 1.262163]\n",
      "epoch:21 step:19961 [D loss: 0.458161, acc.: 80.47%] [G loss: 1.405299]\n",
      "epoch:21 step:19962 [D loss: 0.353662, acc.: 89.06%] [G loss: 1.982331]\n",
      "epoch:21 step:19963 [D loss: 0.315676, acc.: 90.62%] [G loss: 3.262473]\n",
      "epoch:21 step:19964 [D loss: 0.212373, acc.: 92.19%] [G loss: 2.562526]\n",
      "epoch:21 step:19965 [D loss: 0.173545, acc.: 96.88%] [G loss: 2.276686]\n",
      "epoch:21 step:19966 [D loss: 0.722753, acc.: 56.25%] [G loss: 3.392748]\n",
      "epoch:21 step:19967 [D loss: 0.670434, acc.: 64.84%] [G loss: 2.588864]\n",
      "epoch:21 step:19968 [D loss: 0.298494, acc.: 92.19%] [G loss: 2.570871]\n",
      "epoch:21 step:19969 [D loss: 0.270175, acc.: 92.97%] [G loss: 1.772619]\n",
      "epoch:21 step:19970 [D loss: 0.594898, acc.: 65.62%] [G loss: 1.975856]\n",
      "epoch:21 step:19971 [D loss: 0.573611, acc.: 73.44%] [G loss: 1.430420]\n",
      "epoch:21 step:19972 [D loss: 0.809887, acc.: 53.12%] [G loss: 0.747442]\n",
      "epoch:21 step:19973 [D loss: 0.527524, acc.: 69.53%] [G loss: 1.733984]\n",
      "epoch:21 step:19974 [D loss: 0.461588, acc.: 80.47%] [G loss: 1.935163]\n",
      "epoch:21 step:19975 [D loss: 0.510142, acc.: 73.44%] [G loss: 0.996047]\n",
      "epoch:21 step:19976 [D loss: 0.416021, acc.: 86.72%] [G loss: 2.153907]\n",
      "epoch:21 step:19977 [D loss: 1.018888, acc.: 39.84%] [G loss: 0.834236]\n",
      "epoch:21 step:19978 [D loss: 0.487620, acc.: 77.34%] [G loss: 2.176205]\n",
      "epoch:21 step:19979 [D loss: 0.323892, acc.: 92.97%] [G loss: 2.496581]\n",
      "epoch:21 step:19980 [D loss: 0.580784, acc.: 74.22%] [G loss: 2.528231]\n",
      "epoch:21 step:19981 [D loss: 0.479919, acc.: 75.00%] [G loss: 2.151643]\n",
      "epoch:21 step:19982 [D loss: 0.391617, acc.: 82.03%] [G loss: 1.508357]\n",
      "epoch:21 step:19983 [D loss: 0.889747, acc.: 57.81%] [G loss: 1.805796]\n",
      "epoch:21 step:19984 [D loss: 0.247527, acc.: 95.31%] [G loss: 2.462635]\n",
      "epoch:21 step:19985 [D loss: 0.472785, acc.: 75.78%] [G loss: 2.132576]\n",
      "epoch:21 step:19986 [D loss: 0.640350, acc.: 63.28%] [G loss: 1.875815]\n",
      "epoch:21 step:19987 [D loss: 0.469869, acc.: 79.69%] [G loss: 2.241642]\n",
      "epoch:21 step:19988 [D loss: 0.452871, acc.: 72.66%] [G loss: 1.159242]\n",
      "epoch:21 step:19989 [D loss: 0.462232, acc.: 83.59%] [G loss: 2.640754]\n",
      "epoch:21 step:19990 [D loss: 0.545120, acc.: 71.88%] [G loss: 0.949010]\n",
      "epoch:21 step:19991 [D loss: 0.525023, acc.: 78.12%] [G loss: 2.120444]\n",
      "epoch:21 step:19992 [D loss: 0.518642, acc.: 76.56%] [G loss: 1.790347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19993 [D loss: 0.511155, acc.: 75.00%] [G loss: 2.153892]\n",
      "epoch:21 step:19994 [D loss: 0.415816, acc.: 83.59%] [G loss: 2.247084]\n",
      "epoch:21 step:19995 [D loss: 0.198479, acc.: 96.09%] [G loss: 2.160391]\n",
      "epoch:21 step:19996 [D loss: 0.637750, acc.: 66.41%] [G loss: 0.677043]\n",
      "epoch:21 step:19997 [D loss: 0.581438, acc.: 64.84%] [G loss: 0.735901]\n",
      "epoch:21 step:19998 [D loss: 0.408953, acc.: 83.59%] [G loss: 0.959479]\n",
      "epoch:21 step:19999 [D loss: 0.466395, acc.: 67.97%] [G loss: 1.919946]\n",
      "epoch:21 step:20000 [D loss: 0.251504, acc.: 96.09%] [G loss: 1.136997]\n",
      "epoch:21 step:20001 [D loss: 0.476927, acc.: 67.97%] [G loss: 2.985633]\n",
      "epoch:21 step:20002 [D loss: 0.183411, acc.: 94.53%] [G loss: 1.440210]\n",
      "epoch:21 step:20003 [D loss: 0.502992, acc.: 72.66%] [G loss: 2.064346]\n",
      "epoch:21 step:20004 [D loss: 0.891879, acc.: 53.91%] [G loss: 1.517050]\n",
      "epoch:21 step:20005 [D loss: 0.235879, acc.: 95.31%] [G loss: 1.040505]\n",
      "epoch:21 step:20006 [D loss: 0.202183, acc.: 96.88%] [G loss: 2.177701]\n",
      "epoch:21 step:20007 [D loss: 0.523219, acc.: 69.53%] [G loss: 1.701917]\n",
      "epoch:21 step:20008 [D loss: 0.355521, acc.: 86.72%] [G loss: 1.728465]\n",
      "epoch:21 step:20009 [D loss: 0.689712, acc.: 63.28%] [G loss: 1.928413]\n",
      "epoch:21 step:20010 [D loss: 0.234096, acc.: 96.88%] [G loss: 1.979255]\n",
      "epoch:21 step:20011 [D loss: 0.313915, acc.: 90.62%] [G loss: 2.717763]\n",
      "epoch:21 step:20012 [D loss: 0.592105, acc.: 71.88%] [G loss: 0.778835]\n",
      "epoch:21 step:20013 [D loss: 0.175601, acc.: 96.88%] [G loss: 1.449916]\n",
      "epoch:21 step:20014 [D loss: 0.475623, acc.: 81.25%] [G loss: 2.382312]\n",
      "epoch:21 step:20015 [D loss: 0.519700, acc.: 71.88%] [G loss: 2.605129]\n",
      "epoch:21 step:20016 [D loss: 0.800593, acc.: 51.56%] [G loss: 2.189671]\n",
      "epoch:21 step:20017 [D loss: 0.428250, acc.: 86.72%] [G loss: 1.357006]\n",
      "epoch:21 step:20018 [D loss: 0.525935, acc.: 75.78%] [G loss: 0.877863]\n",
      "epoch:21 step:20019 [D loss: 0.303747, acc.: 92.97%] [G loss: 2.258260]\n",
      "epoch:21 step:20020 [D loss: 0.267933, acc.: 92.19%] [G loss: 2.334582]\n",
      "epoch:21 step:20021 [D loss: 0.583968, acc.: 67.97%] [G loss: 1.163553]\n",
      "epoch:21 step:20022 [D loss: 0.460941, acc.: 81.25%] [G loss: 0.733845]\n",
      "epoch:21 step:20023 [D loss: 0.310404, acc.: 92.19%] [G loss: 0.827550]\n",
      "epoch:21 step:20024 [D loss: 1.322633, acc.: 41.41%] [G loss: 1.400738]\n",
      "epoch:21 step:20025 [D loss: 0.433438, acc.: 81.25%] [G loss: 2.974608]\n",
      "epoch:21 step:20026 [D loss: 0.263964, acc.: 90.62%] [G loss: 1.086601]\n",
      "epoch:21 step:20027 [D loss: 0.686293, acc.: 52.34%] [G loss: 3.136938]\n",
      "epoch:21 step:20028 [D loss: 0.800322, acc.: 57.03%] [G loss: 1.914265]\n",
      "epoch:21 step:20029 [D loss: 0.424615, acc.: 89.84%] [G loss: 1.711212]\n",
      "epoch:21 step:20030 [D loss: 0.641678, acc.: 64.06%] [G loss: 3.152532]\n",
      "epoch:21 step:20031 [D loss: 0.652137, acc.: 60.16%] [G loss: 1.072069]\n",
      "epoch:21 step:20032 [D loss: 1.068444, acc.: 36.72%] [G loss: 1.472745]\n",
      "epoch:21 step:20033 [D loss: 0.787485, acc.: 52.34%] [G loss: 1.434551]\n",
      "epoch:21 step:20034 [D loss: 0.575891, acc.: 66.41%] [G loss: 1.054111]\n",
      "epoch:21 step:20035 [D loss: 0.641728, acc.: 63.28%] [G loss: 1.293695]\n",
      "epoch:21 step:20036 [D loss: 0.389079, acc.: 87.50%] [G loss: 1.252535]\n",
      "epoch:21 step:20037 [D loss: 0.351421, acc.: 89.06%] [G loss: 1.540208]\n",
      "epoch:21 step:20038 [D loss: 0.354087, acc.: 82.81%] [G loss: 2.008729]\n",
      "epoch:21 step:20039 [D loss: 0.359241, acc.: 89.84%] [G loss: 1.525752]\n",
      "epoch:21 step:20040 [D loss: 0.350543, acc.: 87.50%] [G loss: 1.599529]\n",
      "epoch:21 step:20041 [D loss: 0.531145, acc.: 74.22%] [G loss: 1.412672]\n",
      "epoch:21 step:20042 [D loss: 0.840558, acc.: 53.91%] [G loss: 1.575831]\n",
      "epoch:21 step:20043 [D loss: 0.313671, acc.: 92.19%] [G loss: 2.427330]\n",
      "epoch:21 step:20044 [D loss: 0.712819, acc.: 57.81%] [G loss: 0.866524]\n",
      "epoch:21 step:20045 [D loss: 0.391378, acc.: 87.50%] [G loss: 1.277584]\n",
      "epoch:21 step:20046 [D loss: 0.396401, acc.: 78.91%] [G loss: 1.733816]\n",
      "epoch:21 step:20047 [D loss: 0.349380, acc.: 89.84%] [G loss: 2.867499]\n",
      "epoch:21 step:20048 [D loss: 0.211867, acc.: 98.44%] [G loss: 1.847698]\n",
      "epoch:21 step:20049 [D loss: 0.335095, acc.: 89.06%] [G loss: 2.040454]\n",
      "epoch:21 step:20050 [D loss: 0.337844, acc.: 92.97%] [G loss: 5.074359]\n",
      "epoch:21 step:20051 [D loss: 1.063318, acc.: 28.12%] [G loss: 1.852536]\n",
      "epoch:21 step:20052 [D loss: 0.332871, acc.: 86.72%] [G loss: 1.698852]\n",
      "epoch:21 step:20053 [D loss: 0.903714, acc.: 42.97%] [G loss: 1.361173]\n",
      "epoch:21 step:20054 [D loss: 0.879829, acc.: 47.66%] [G loss: 2.372511]\n",
      "epoch:21 step:20055 [D loss: 0.357069, acc.: 86.72%] [G loss: 0.916514]\n",
      "epoch:21 step:20056 [D loss: 0.494451, acc.: 74.22%] [G loss: 1.700966]\n",
      "epoch:21 step:20057 [D loss: 0.673328, acc.: 60.16%] [G loss: 0.557956]\n",
      "epoch:21 step:20058 [D loss: 0.500517, acc.: 71.88%] [G loss: 0.994828]\n",
      "epoch:21 step:20059 [D loss: 0.238334, acc.: 96.88%] [G loss: 0.934517]\n",
      "epoch:21 step:20060 [D loss: 0.617865, acc.: 64.84%] [G loss: 2.282603]\n",
      "epoch:21 step:20061 [D loss: 0.626284, acc.: 64.84%] [G loss: 0.975142]\n",
      "epoch:21 step:20062 [D loss: 0.508856, acc.: 76.56%] [G loss: 0.727645]\n",
      "epoch:21 step:20063 [D loss: 0.525433, acc.: 77.34%] [G loss: 1.523964]\n",
      "epoch:21 step:20064 [D loss: 0.780020, acc.: 50.00%] [G loss: 1.299469]\n",
      "epoch:21 step:20065 [D loss: 0.412864, acc.: 78.91%] [G loss: 1.250050]\n",
      "epoch:21 step:20066 [D loss: 0.607026, acc.: 68.75%] [G loss: 2.758403]\n",
      "epoch:21 step:20067 [D loss: 1.106961, acc.: 32.03%] [G loss: 2.208054]\n",
      "epoch:21 step:20068 [D loss: 0.742770, acc.: 57.81%] [G loss: 1.416006]\n",
      "epoch:21 step:20069 [D loss: 0.528444, acc.: 73.44%] [G loss: 1.167681]\n",
      "epoch:21 step:20070 [D loss: 0.496965, acc.: 76.56%] [G loss: 1.008289]\n",
      "epoch:21 step:20071 [D loss: 0.270842, acc.: 95.31%] [G loss: 1.487619]\n",
      "epoch:21 step:20072 [D loss: 0.405591, acc.: 85.94%] [G loss: 1.344072]\n",
      "epoch:21 step:20073 [D loss: 0.434673, acc.: 78.91%] [G loss: 0.992746]\n",
      "epoch:21 step:20074 [D loss: 0.700864, acc.: 62.50%] [G loss: 1.692909]\n",
      "epoch:21 step:20075 [D loss: 0.729034, acc.: 50.78%] [G loss: 0.923964]\n",
      "epoch:21 step:20076 [D loss: 0.724747, acc.: 55.47%] [G loss: 1.217907]\n",
      "epoch:21 step:20077 [D loss: 0.473589, acc.: 75.00%] [G loss: 1.278545]\n",
      "epoch:21 step:20078 [D loss: 0.444667, acc.: 82.03%] [G loss: 3.383693]\n",
      "epoch:21 step:20079 [D loss: 1.166374, acc.: 27.34%] [G loss: 1.047854]\n",
      "epoch:21 step:20080 [D loss: 0.834589, acc.: 50.00%] [G loss: 1.706404]\n",
      "epoch:21 step:20081 [D loss: 0.569721, acc.: 68.75%] [G loss: 1.261118]\n",
      "epoch:21 step:20082 [D loss: 0.553491, acc.: 71.88%] [G loss: 1.601091]\n",
      "epoch:21 step:20083 [D loss: 0.529334, acc.: 67.97%] [G loss: 1.802977]\n",
      "epoch:21 step:20084 [D loss: 0.484128, acc.: 79.69%] [G loss: 1.594389]\n",
      "epoch:21 step:20085 [D loss: 0.658601, acc.: 63.28%] [G loss: 1.757393]\n",
      "epoch:21 step:20086 [D loss: 0.411177, acc.: 82.03%] [G loss: 1.718422]\n",
      "epoch:21 step:20087 [D loss: 0.597472, acc.: 69.53%] [G loss: 1.275681]\n",
      "epoch:21 step:20088 [D loss: 0.338693, acc.: 92.97%] [G loss: 1.941627]\n",
      "epoch:21 step:20089 [D loss: 0.726454, acc.: 57.81%] [G loss: 1.509649]\n",
      "epoch:21 step:20090 [D loss: 0.604439, acc.: 64.06%] [G loss: 1.502980]\n",
      "epoch:21 step:20091 [D loss: 0.389841, acc.: 91.41%] [G loss: 1.883470]\n",
      "epoch:21 step:20092 [D loss: 0.535000, acc.: 72.66%] [G loss: 2.320597]\n",
      "epoch:21 step:20093 [D loss: 0.767690, acc.: 50.00%] [G loss: 2.582229]\n",
      "epoch:21 step:20094 [D loss: 0.309153, acc.: 92.19%] [G loss: 2.916510]\n",
      "epoch:21 step:20095 [D loss: 0.263295, acc.: 94.53%] [G loss: 2.444635]\n",
      "epoch:21 step:20096 [D loss: 0.348172, acc.: 87.50%] [G loss: 2.225907]\n",
      "epoch:21 step:20097 [D loss: 0.718537, acc.: 55.47%] [G loss: 1.922916]\n",
      "epoch:21 step:20098 [D loss: 0.909516, acc.: 54.69%] [G loss: 1.839252]\n",
      "epoch:21 step:20099 [D loss: 0.423799, acc.: 84.38%] [G loss: 1.433601]\n",
      "epoch:21 step:20100 [D loss: 0.556764, acc.: 70.31%] [G loss: 1.047854]\n",
      "epoch:21 step:20101 [D loss: 0.703211, acc.: 60.16%] [G loss: 2.202117]\n",
      "epoch:21 step:20102 [D loss: 0.380998, acc.: 89.06%] [G loss: 2.308355]\n",
      "epoch:21 step:20103 [D loss: 0.975366, acc.: 41.41%] [G loss: 1.075614]\n",
      "epoch:21 step:20104 [D loss: 0.816095, acc.: 58.59%] [G loss: 1.259144]\n",
      "epoch:21 step:20105 [D loss: 0.628182, acc.: 58.59%] [G loss: 1.536177]\n",
      "epoch:21 step:20106 [D loss: 0.501290, acc.: 82.81%] [G loss: 1.950459]\n",
      "epoch:21 step:20107 [D loss: 0.483581, acc.: 82.81%] [G loss: 1.233103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20108 [D loss: 0.809424, acc.: 58.59%] [G loss: 0.916714]\n",
      "epoch:21 step:20109 [D loss: 1.162563, acc.: 35.94%] [G loss: 0.884082]\n",
      "epoch:21 step:20110 [D loss: 0.214925, acc.: 96.88%] [G loss: 1.485747]\n",
      "epoch:21 step:20111 [D loss: 0.434804, acc.: 85.16%] [G loss: 1.151876]\n",
      "epoch:21 step:20112 [D loss: 0.293449, acc.: 92.19%] [G loss: 1.560213]\n",
      "epoch:21 step:20113 [D loss: 0.300757, acc.: 94.53%] [G loss: 1.446749]\n",
      "epoch:21 step:20114 [D loss: 0.606242, acc.: 67.97%] [G loss: 1.443942]\n",
      "epoch:21 step:20115 [D loss: 0.331623, acc.: 90.62%] [G loss: 1.781297]\n",
      "epoch:21 step:20116 [D loss: 0.584196, acc.: 66.41%] [G loss: 1.733506]\n",
      "epoch:21 step:20117 [D loss: 0.401338, acc.: 81.25%] [G loss: 1.383930]\n",
      "epoch:21 step:20118 [D loss: 0.238267, acc.: 96.09%] [G loss: 2.526872]\n",
      "epoch:21 step:20119 [D loss: 0.364822, acc.: 89.06%] [G loss: 2.320708]\n",
      "epoch:21 step:20120 [D loss: 0.264833, acc.: 92.19%] [G loss: 1.963474]\n",
      "epoch:21 step:20121 [D loss: 0.444401, acc.: 76.56%] [G loss: 1.881968]\n",
      "epoch:21 step:20122 [D loss: 0.541554, acc.: 70.31%] [G loss: 0.756551]\n",
      "epoch:21 step:20123 [D loss: 0.791686, acc.: 64.06%] [G loss: 4.173120]\n",
      "epoch:21 step:20124 [D loss: 0.631004, acc.: 64.84%] [G loss: 2.175661]\n",
      "epoch:21 step:20125 [D loss: 1.017377, acc.: 46.88%] [G loss: 0.803009]\n",
      "epoch:21 step:20126 [D loss: 0.391332, acc.: 90.62%] [G loss: 1.409660]\n",
      "epoch:21 step:20127 [D loss: 0.434334, acc.: 83.59%] [G loss: 1.810138]\n",
      "epoch:21 step:20128 [D loss: 0.674742, acc.: 62.50%] [G loss: 0.760430]\n",
      "epoch:21 step:20129 [D loss: 0.349840, acc.: 90.62%] [G loss: 1.501751]\n",
      "epoch:21 step:20130 [D loss: 0.443002, acc.: 86.72%] [G loss: 2.840680]\n",
      "epoch:21 step:20131 [D loss: 0.213634, acc.: 96.88%] [G loss: 1.652806]\n",
      "epoch:21 step:20132 [D loss: 0.823621, acc.: 43.75%] [G loss: 2.130606]\n",
      "epoch:21 step:20133 [D loss: 0.684411, acc.: 57.81%] [G loss: 0.896003]\n",
      "epoch:21 step:20134 [D loss: 0.723914, acc.: 57.03%] [G loss: 1.738040]\n",
      "epoch:21 step:20135 [D loss: 0.558450, acc.: 69.53%] [G loss: 1.600652]\n",
      "epoch:21 step:20136 [D loss: 0.513985, acc.: 72.66%] [G loss: 1.689683]\n",
      "epoch:21 step:20137 [D loss: 0.769574, acc.: 56.25%] [G loss: 0.677460]\n",
      "epoch:21 step:20138 [D loss: 0.522363, acc.: 71.88%] [G loss: 2.086038]\n",
      "epoch:21 step:20139 [D loss: 0.654054, acc.: 60.94%] [G loss: 1.264041]\n",
      "epoch:21 step:20140 [D loss: 0.652633, acc.: 62.50%] [G loss: 1.177794]\n",
      "epoch:21 step:20141 [D loss: 0.506459, acc.: 72.66%] [G loss: 2.531208]\n",
      "epoch:21 step:20142 [D loss: 0.255785, acc.: 94.53%] [G loss: 1.669019]\n",
      "epoch:21 step:20143 [D loss: 0.290662, acc.: 93.75%] [G loss: 2.042885]\n",
      "epoch:21 step:20144 [D loss: 0.629888, acc.: 60.94%] [G loss: 1.213914]\n",
      "epoch:21 step:20145 [D loss: 0.856887, acc.: 53.12%] [G loss: 1.847178]\n",
      "epoch:21 step:20146 [D loss: 0.794772, acc.: 49.22%] [G loss: 1.153883]\n",
      "epoch:21 step:20147 [D loss: 0.583498, acc.: 68.75%] [G loss: 1.397412]\n",
      "epoch:21 step:20148 [D loss: 0.266794, acc.: 93.75%] [G loss: 1.103306]\n",
      "epoch:21 step:20149 [D loss: 0.549556, acc.: 69.53%] [G loss: 0.978557]\n",
      "epoch:21 step:20150 [D loss: 0.557288, acc.: 67.19%] [G loss: 0.785982]\n",
      "epoch:21 step:20151 [D loss: 0.437095, acc.: 75.00%] [G loss: 1.149208]\n",
      "epoch:21 step:20152 [D loss: 0.158864, acc.: 99.22%] [G loss: 0.997186]\n",
      "epoch:21 step:20153 [D loss: 0.313860, acc.: 91.41%] [G loss: 0.851471]\n",
      "epoch:21 step:20154 [D loss: 0.523798, acc.: 75.00%] [G loss: 1.129714]\n",
      "epoch:21 step:20155 [D loss: 0.407107, acc.: 82.03%] [G loss: 1.723287]\n",
      "epoch:21 step:20156 [D loss: 0.373309, acc.: 89.84%] [G loss: 1.288216]\n",
      "epoch:21 step:20157 [D loss: 0.536114, acc.: 70.31%] [G loss: 1.081225]\n",
      "epoch:21 step:20158 [D loss: 0.693935, acc.: 60.16%] [G loss: 1.156309]\n",
      "epoch:21 step:20159 [D loss: 0.337274, acc.: 92.19%] [G loss: 2.007210]\n",
      "epoch:21 step:20160 [D loss: 0.794452, acc.: 58.59%] [G loss: 1.778383]\n",
      "epoch:21 step:20161 [D loss: 0.442594, acc.: 73.44%] [G loss: 2.097461]\n",
      "epoch:21 step:20162 [D loss: 0.380063, acc.: 79.69%] [G loss: 1.280746]\n",
      "epoch:21 step:20163 [D loss: 0.528322, acc.: 74.22%] [G loss: 0.955939]\n",
      "epoch:21 step:20164 [D loss: 0.334942, acc.: 86.72%] [G loss: 2.035841]\n",
      "epoch:21 step:20165 [D loss: 0.591590, acc.: 67.97%] [G loss: 0.584337]\n",
      "epoch:21 step:20166 [D loss: 0.563672, acc.: 68.75%] [G loss: 1.709391]\n",
      "epoch:21 step:20167 [D loss: 0.355591, acc.: 89.06%] [G loss: 1.583345]\n",
      "epoch:21 step:20168 [D loss: 0.199158, acc.: 97.66%] [G loss: 0.993063]\n",
      "epoch:21 step:20169 [D loss: 0.357158, acc.: 83.59%] [G loss: 1.748725]\n",
      "epoch:21 step:20170 [D loss: 0.409640, acc.: 81.25%] [G loss: 1.508314]\n",
      "epoch:21 step:20171 [D loss: 0.394072, acc.: 88.28%] [G loss: 2.194427]\n",
      "epoch:21 step:20172 [D loss: 0.446233, acc.: 79.69%] [G loss: 1.749386]\n",
      "epoch:21 step:20173 [D loss: 0.217272, acc.: 98.44%] [G loss: 3.063080]\n",
      "epoch:21 step:20174 [D loss: 0.224290, acc.: 97.66%] [G loss: 1.943820]\n",
      "epoch:21 step:20175 [D loss: 0.766799, acc.: 53.12%] [G loss: 1.168021]\n",
      "epoch:21 step:20176 [D loss: 0.624184, acc.: 63.28%] [G loss: 1.152189]\n",
      "epoch:21 step:20177 [D loss: 0.862300, acc.: 38.28%] [G loss: 2.786819]\n",
      "epoch:21 step:20178 [D loss: 0.703714, acc.: 64.84%] [G loss: 1.442190]\n",
      "epoch:21 step:20179 [D loss: 0.461321, acc.: 76.56%] [G loss: 2.563240]\n",
      "epoch:21 step:20180 [D loss: 1.057035, acc.: 32.03%] [G loss: 1.101477]\n",
      "epoch:21 step:20181 [D loss: 0.584807, acc.: 64.06%] [G loss: 1.640426]\n",
      "epoch:21 step:20182 [D loss: 0.253350, acc.: 97.66%] [G loss: 1.110386]\n",
      "epoch:21 step:20183 [D loss: 0.491791, acc.: 74.22%] [G loss: 2.443938]\n",
      "epoch:21 step:20184 [D loss: 0.432990, acc.: 82.81%] [G loss: 1.601594]\n",
      "epoch:21 step:20185 [D loss: 0.467150, acc.: 79.69%] [G loss: 1.548245]\n",
      "epoch:21 step:20186 [D loss: 0.994701, acc.: 39.84%] [G loss: 0.601011]\n",
      "epoch:21 step:20187 [D loss: 0.289799, acc.: 89.84%] [G loss: 1.561258]\n",
      "epoch:21 step:20188 [D loss: 0.267499, acc.: 92.97%] [G loss: 1.970107]\n",
      "epoch:21 step:20189 [D loss: 0.346780, acc.: 88.28%] [G loss: 1.302824]\n",
      "epoch:21 step:20190 [D loss: 0.565781, acc.: 71.09%] [G loss: 1.061656]\n",
      "epoch:21 step:20191 [D loss: 0.382647, acc.: 85.94%] [G loss: 1.656133]\n",
      "epoch:21 step:20192 [D loss: 0.260045, acc.: 96.09%] [G loss: 2.320086]\n",
      "epoch:21 step:20193 [D loss: 0.390036, acc.: 85.16%] [G loss: 1.692587]\n",
      "epoch:21 step:20194 [D loss: 1.023295, acc.: 42.19%] [G loss: 0.772680]\n",
      "epoch:21 step:20195 [D loss: 0.436056, acc.: 84.38%] [G loss: 2.528584]\n",
      "epoch:21 step:20196 [D loss: 0.510326, acc.: 72.66%] [G loss: 2.627288]\n",
      "epoch:21 step:20197 [D loss: 0.380935, acc.: 87.50%] [G loss: 1.668500]\n",
      "epoch:21 step:20198 [D loss: 0.315001, acc.: 92.19%] [G loss: 1.274172]\n",
      "epoch:21 step:20199 [D loss: 0.321102, acc.: 89.84%] [G loss: 1.534620]\n",
      "epoch:21 step:20200 [D loss: 0.331501, acc.: 92.19%] [G loss: 1.279371]\n",
      "epoch:21 step:20201 [D loss: 0.518844, acc.: 76.56%] [G loss: 1.263625]\n",
      "epoch:21 step:20202 [D loss: 0.539210, acc.: 76.56%] [G loss: 1.414766]\n",
      "epoch:21 step:20203 [D loss: 0.729971, acc.: 59.38%] [G loss: 1.561461]\n",
      "epoch:21 step:20204 [D loss: 0.961299, acc.: 40.62%] [G loss: 2.115187]\n",
      "epoch:21 step:20205 [D loss: 0.621994, acc.: 68.75%] [G loss: 1.276446]\n",
      "epoch:21 step:20206 [D loss: 0.489023, acc.: 77.34%] [G loss: 2.216268]\n",
      "epoch:21 step:20207 [D loss: 0.389327, acc.: 86.72%] [G loss: 1.948073]\n",
      "epoch:21 step:20208 [D loss: 0.501168, acc.: 78.12%] [G loss: 0.512090]\n",
      "epoch:21 step:20209 [D loss: 0.691658, acc.: 56.25%] [G loss: 0.442525]\n",
      "epoch:21 step:20210 [D loss: 0.669538, acc.: 59.38%] [G loss: 1.557075]\n",
      "epoch:21 step:20211 [D loss: 0.751556, acc.: 46.09%] [G loss: 1.951202]\n",
      "epoch:21 step:20212 [D loss: 0.247051, acc.: 96.88%] [G loss: 0.706957]\n",
      "epoch:21 step:20213 [D loss: 0.573220, acc.: 67.97%] [G loss: 1.413704]\n",
      "epoch:21 step:20214 [D loss: 0.336059, acc.: 90.62%] [G loss: 1.723393]\n",
      "epoch:21 step:20215 [D loss: 0.544058, acc.: 72.66%] [G loss: 1.031923]\n",
      "epoch:21 step:20216 [D loss: 0.534580, acc.: 66.41%] [G loss: 1.439234]\n",
      "epoch:21 step:20217 [D loss: 0.709135, acc.: 57.03%] [G loss: 1.178385]\n",
      "epoch:21 step:20218 [D loss: 0.468733, acc.: 76.56%] [G loss: 2.053165]\n",
      "epoch:21 step:20219 [D loss: 0.789639, acc.: 54.69%] [G loss: 1.180578]\n",
      "epoch:21 step:20220 [D loss: 0.692858, acc.: 58.59%] [G loss: 1.233088]\n",
      "epoch:21 step:20221 [D loss: 0.261470, acc.: 94.53%] [G loss: 1.551970]\n",
      "epoch:21 step:20222 [D loss: 0.248632, acc.: 95.31%] [G loss: 1.402976]\n",
      "epoch:21 step:20223 [D loss: 0.710768, acc.: 61.72%] [G loss: 2.454659]\n",
      "epoch:21 step:20224 [D loss: 0.834428, acc.: 53.91%] [G loss: 2.429487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20225 [D loss: 0.576679, acc.: 71.88%] [G loss: 1.693482]\n",
      "epoch:21 step:20226 [D loss: 0.592979, acc.: 64.84%] [G loss: 0.733375]\n",
      "epoch:21 step:20227 [D loss: 0.756009, acc.: 50.00%] [G loss: 0.742483]\n",
      "epoch:21 step:20228 [D loss: 1.119067, acc.: 30.47%] [G loss: 2.191903]\n",
      "epoch:21 step:20229 [D loss: 0.428244, acc.: 85.16%] [G loss: 1.596163]\n",
      "epoch:21 step:20230 [D loss: 0.299707, acc.: 90.62%] [G loss: 1.726082]\n",
      "epoch:21 step:20231 [D loss: 0.500085, acc.: 80.47%] [G loss: 1.681382]\n",
      "epoch:21 step:20232 [D loss: 0.468368, acc.: 82.81%] [G loss: 1.521093]\n",
      "epoch:21 step:20233 [D loss: 0.477210, acc.: 79.69%] [G loss: 1.792815]\n",
      "epoch:21 step:20234 [D loss: 0.206216, acc.: 98.44%] [G loss: 1.070257]\n",
      "epoch:21 step:20235 [D loss: 0.469329, acc.: 72.66%] [G loss: 2.139306]\n",
      "epoch:21 step:20236 [D loss: 0.760400, acc.: 46.88%] [G loss: 1.155967]\n",
      "epoch:21 step:20237 [D loss: 0.637674, acc.: 61.72%] [G loss: 1.974691]\n",
      "epoch:21 step:20238 [D loss: 1.066703, acc.: 23.44%] [G loss: 1.402044]\n",
      "epoch:21 step:20239 [D loss: 0.666328, acc.: 50.00%] [G loss: 2.396761]\n",
      "epoch:21 step:20240 [D loss: 0.303567, acc.: 92.19%] [G loss: 0.702577]\n",
      "epoch:21 step:20241 [D loss: 0.799714, acc.: 41.41%] [G loss: 1.739360]\n",
      "epoch:21 step:20242 [D loss: 0.658743, acc.: 62.50%] [G loss: 1.502619]\n",
      "epoch:21 step:20243 [D loss: 0.413706, acc.: 80.47%] [G loss: 2.324464]\n",
      "epoch:21 step:20244 [D loss: 0.663880, acc.: 61.72%] [G loss: 1.139235]\n",
      "epoch:21 step:20245 [D loss: 0.700594, acc.: 57.81%] [G loss: 1.293234]\n",
      "epoch:21 step:20246 [D loss: 0.433411, acc.: 82.81%] [G loss: 1.211339]\n",
      "epoch:21 step:20247 [D loss: 0.552542, acc.: 71.88%] [G loss: 1.429844]\n",
      "epoch:21 step:20248 [D loss: 0.403723, acc.: 85.16%] [G loss: 1.501033]\n",
      "epoch:21 step:20249 [D loss: 0.312837, acc.: 93.75%] [G loss: 1.291899]\n",
      "epoch:21 step:20250 [D loss: 0.578098, acc.: 64.84%] [G loss: 1.145780]\n",
      "epoch:21 step:20251 [D loss: 1.011116, acc.: 50.00%] [G loss: 1.354841]\n",
      "epoch:21 step:20252 [D loss: 0.743990, acc.: 56.25%] [G loss: 1.131323]\n",
      "epoch:21 step:20253 [D loss: 0.791526, acc.: 59.38%] [G loss: 1.267564]\n",
      "epoch:21 step:20254 [D loss: 0.645045, acc.: 68.75%] [G loss: 1.669982]\n",
      "epoch:21 step:20255 [D loss: 0.731575, acc.: 54.69%] [G loss: 2.004460]\n",
      "epoch:21 step:20256 [D loss: 0.523634, acc.: 75.00%] [G loss: 1.568028]\n",
      "epoch:21 step:20257 [D loss: 0.730904, acc.: 56.25%] [G loss: 1.416786]\n",
      "epoch:21 step:20258 [D loss: 0.515112, acc.: 78.12%] [G loss: 1.279493]\n",
      "epoch:21 step:20259 [D loss: 0.449641, acc.: 82.81%] [G loss: 1.405872]\n",
      "epoch:21 step:20260 [D loss: 0.556612, acc.: 70.31%] [G loss: 1.250729]\n",
      "epoch:21 step:20261 [D loss: 0.570484, acc.: 70.31%] [G loss: 1.632316]\n",
      "epoch:21 step:20262 [D loss: 0.497209, acc.: 75.00%] [G loss: 1.379065]\n",
      "epoch:21 step:20263 [D loss: 0.542025, acc.: 67.19%] [G loss: 1.052151]\n",
      "epoch:21 step:20264 [D loss: 0.627809, acc.: 66.41%] [G loss: 0.739887]\n",
      "epoch:21 step:20265 [D loss: 0.492730, acc.: 77.34%] [G loss: 1.447291]\n",
      "epoch:21 step:20266 [D loss: 0.651725, acc.: 61.72%] [G loss: 1.215137]\n",
      "epoch:21 step:20267 [D loss: 0.592740, acc.: 67.19%] [G loss: 1.091942]\n",
      "epoch:21 step:20268 [D loss: 0.637705, acc.: 66.41%] [G loss: 0.943072]\n",
      "epoch:21 step:20269 [D loss: 0.327227, acc.: 90.62%] [G loss: 1.221615]\n",
      "epoch:21 step:20270 [D loss: 0.552682, acc.: 71.09%] [G loss: 1.253486]\n",
      "epoch:21 step:20271 [D loss: 0.300686, acc.: 93.75%] [G loss: 1.132967]\n",
      "epoch:21 step:20272 [D loss: 0.281596, acc.: 93.75%] [G loss: 1.522920]\n",
      "epoch:21 step:20273 [D loss: 0.648270, acc.: 63.28%] [G loss: 1.823302]\n",
      "epoch:21 step:20274 [D loss: 0.494731, acc.: 77.34%] [G loss: 1.694358]\n",
      "epoch:21 step:20275 [D loss: 0.654187, acc.: 56.25%] [G loss: 2.001464]\n",
      "epoch:21 step:20276 [D loss: 0.615656, acc.: 64.84%] [G loss: 2.479476]\n",
      "epoch:21 step:20277 [D loss: 0.687573, acc.: 63.28%] [G loss: 1.846226]\n",
      "epoch:21 step:20278 [D loss: 0.368091, acc.: 90.62%] [G loss: 1.116223]\n",
      "epoch:21 step:20279 [D loss: 0.311088, acc.: 93.75%] [G loss: 1.569891]\n",
      "epoch:21 step:20280 [D loss: 0.570514, acc.: 71.88%] [G loss: 0.936954]\n",
      "epoch:21 step:20281 [D loss: 0.449796, acc.: 79.69%] [G loss: 1.073803]\n",
      "epoch:21 step:20282 [D loss: 0.595862, acc.: 64.06%] [G loss: 1.633066]\n",
      "epoch:21 step:20283 [D loss: 0.267780, acc.: 96.09%] [G loss: 2.253374]\n",
      "epoch:21 step:20284 [D loss: 0.626062, acc.: 69.53%] [G loss: 2.163716]\n",
      "epoch:21 step:20285 [D loss: 0.991764, acc.: 46.88%] [G loss: 1.969038]\n",
      "epoch:21 step:20286 [D loss: 0.808102, acc.: 53.91%] [G loss: 1.448788]\n",
      "epoch:21 step:20287 [D loss: 0.436563, acc.: 82.03%] [G loss: 1.269161]\n",
      "epoch:21 step:20288 [D loss: 0.575522, acc.: 67.97%] [G loss: 2.356602]\n",
      "epoch:21 step:20289 [D loss: 0.551487, acc.: 70.31%] [G loss: 1.684961]\n",
      "epoch:21 step:20290 [D loss: 0.519919, acc.: 73.44%] [G loss: 1.545048]\n",
      "epoch:21 step:20291 [D loss: 0.460242, acc.: 75.78%] [G loss: 1.290900]\n",
      "epoch:21 step:20292 [D loss: 0.329274, acc.: 94.53%] [G loss: 1.701664]\n",
      "epoch:21 step:20293 [D loss: 0.723751, acc.: 56.25%] [G loss: 1.477465]\n",
      "epoch:21 step:20294 [D loss: 0.639627, acc.: 60.94%] [G loss: 0.848599]\n",
      "epoch:21 step:20295 [D loss: 0.562406, acc.: 66.41%] [G loss: 1.332977]\n",
      "epoch:21 step:20296 [D loss: 0.497509, acc.: 72.66%] [G loss: 1.115604]\n",
      "epoch:21 step:20297 [D loss: 0.420491, acc.: 82.03%] [G loss: 2.293490]\n",
      "epoch:21 step:20298 [D loss: 0.519386, acc.: 75.00%] [G loss: 1.258105]\n",
      "epoch:21 step:20299 [D loss: 0.574611, acc.: 67.97%] [G loss: 1.567257]\n",
      "epoch:21 step:20300 [D loss: 0.550597, acc.: 71.88%] [G loss: 0.785087]\n",
      "epoch:21 step:20301 [D loss: 0.424529, acc.: 80.47%] [G loss: 1.395131]\n",
      "epoch:21 step:20302 [D loss: 0.577395, acc.: 68.75%] [G loss: 0.986348]\n",
      "epoch:21 step:20303 [D loss: 0.594867, acc.: 69.53%] [G loss: 1.967349]\n",
      "epoch:21 step:20304 [D loss: 0.541902, acc.: 73.44%] [G loss: 2.511179]\n",
      "epoch:21 step:20305 [D loss: 0.364665, acc.: 84.38%] [G loss: 1.983965]\n",
      "epoch:21 step:20306 [D loss: 0.716689, acc.: 60.94%] [G loss: 2.877932]\n",
      "epoch:21 step:20307 [D loss: 0.308716, acc.: 89.84%] [G loss: 2.896353]\n",
      "epoch:21 step:20308 [D loss: 0.254998, acc.: 91.41%] [G loss: 1.654600]\n",
      "epoch:21 step:20309 [D loss: 0.540854, acc.: 71.88%] [G loss: 1.602065]\n",
      "epoch:21 step:20310 [D loss: 0.642085, acc.: 64.06%] [G loss: 1.703590]\n",
      "epoch:21 step:20311 [D loss: 0.638265, acc.: 57.81%] [G loss: 1.474320]\n",
      "epoch:21 step:20312 [D loss: 0.471170, acc.: 82.03%] [G loss: 1.890956]\n",
      "epoch:21 step:20313 [D loss: 0.718431, acc.: 62.50%] [G loss: 1.216533]\n",
      "epoch:21 step:20314 [D loss: 0.301602, acc.: 89.84%] [G loss: 1.440268]\n",
      "epoch:21 step:20315 [D loss: 0.465077, acc.: 80.47%] [G loss: 1.325292]\n",
      "epoch:21 step:20316 [D loss: 0.578904, acc.: 68.75%] [G loss: 0.764659]\n",
      "epoch:21 step:20317 [D loss: 1.550928, acc.: 16.41%] [G loss: 0.532806]\n",
      "epoch:21 step:20318 [D loss: 0.530228, acc.: 75.00%] [G loss: 2.007920]\n",
      "epoch:21 step:20319 [D loss: 0.427097, acc.: 86.72%] [G loss: 2.560168]\n",
      "epoch:21 step:20320 [D loss: 0.772146, acc.: 50.78%] [G loss: 1.311543]\n",
      "epoch:21 step:20321 [D loss: 0.365128, acc.: 91.41%] [G loss: 3.102035]\n",
      "epoch:21 step:20322 [D loss: 0.345273, acc.: 89.06%] [G loss: 1.927298]\n",
      "epoch:21 step:20323 [D loss: 1.107281, acc.: 56.25%] [G loss: 2.366328]\n",
      "epoch:21 step:20324 [D loss: 0.560183, acc.: 75.78%] [G loss: 2.230608]\n",
      "epoch:21 step:20325 [D loss: 0.596011, acc.: 67.97%] [G loss: 1.300710]\n",
      "epoch:21 step:20326 [D loss: 0.587135, acc.: 64.84%] [G loss: 0.820047]\n",
      "epoch:21 step:20327 [D loss: 0.419599, acc.: 85.94%] [G loss: 1.358258]\n",
      "epoch:21 step:20328 [D loss: 0.411850, acc.: 85.16%] [G loss: 1.225690]\n",
      "epoch:21 step:20329 [D loss: 0.472317, acc.: 77.34%] [G loss: 2.454316]\n",
      "epoch:21 step:20330 [D loss: 0.715368, acc.: 55.47%] [G loss: 2.131495]\n",
      "epoch:21 step:20331 [D loss: 0.654448, acc.: 56.25%] [G loss: 1.482880]\n",
      "epoch:21 step:20332 [D loss: 0.306195, acc.: 91.41%] [G loss: 1.679261]\n",
      "epoch:21 step:20333 [D loss: 0.541344, acc.: 73.44%] [G loss: 0.801564]\n",
      "epoch:21 step:20334 [D loss: 0.758641, acc.: 51.56%] [G loss: 1.358865]\n",
      "epoch:21 step:20335 [D loss: 0.596390, acc.: 71.09%] [G loss: 1.056303]\n",
      "epoch:21 step:20336 [D loss: 0.296774, acc.: 91.41%] [G loss: 2.073340]\n",
      "epoch:21 step:20337 [D loss: 0.460856, acc.: 82.81%] [G loss: 0.678977]\n",
      "epoch:21 step:20338 [D loss: 0.494530, acc.: 75.00%] [G loss: 1.572795]\n",
      "epoch:21 step:20339 [D loss: 0.368378, acc.: 87.50%] [G loss: 1.431909]\n",
      "epoch:21 step:20340 [D loss: 0.348469, acc.: 86.72%] [G loss: 1.036599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20341 [D loss: 0.412962, acc.: 88.28%] [G loss: 1.247475]\n",
      "epoch:21 step:20342 [D loss: 0.349802, acc.: 87.50%] [G loss: 2.049106]\n",
      "epoch:21 step:20343 [D loss: 0.371750, acc.: 85.94%] [G loss: 2.046116]\n",
      "epoch:21 step:20344 [D loss: 0.362707, acc.: 85.94%] [G loss: 1.503714]\n",
      "epoch:21 step:20345 [D loss: 0.183656, acc.: 98.44%] [G loss: 1.139047]\n",
      "epoch:21 step:20346 [D loss: 0.389542, acc.: 86.72%] [G loss: 0.624848]\n",
      "epoch:21 step:20347 [D loss: 0.779107, acc.: 53.91%] [G loss: 2.007426]\n",
      "epoch:21 step:20348 [D loss: 0.486877, acc.: 75.00%] [G loss: 1.592957]\n",
      "epoch:21 step:20349 [D loss: 0.564349, acc.: 75.00%] [G loss: 2.099896]\n",
      "epoch:21 step:20350 [D loss: 0.444999, acc.: 83.59%] [G loss: 2.400345]\n",
      "epoch:21 step:20351 [D loss: 0.227274, acc.: 96.09%] [G loss: 0.767660]\n",
      "epoch:21 step:20352 [D loss: 0.576595, acc.: 69.53%] [G loss: 1.964155]\n",
      "epoch:21 step:20353 [D loss: 0.313875, acc.: 91.41%] [G loss: 2.608574]\n",
      "epoch:21 step:20354 [D loss: 0.572739, acc.: 65.62%] [G loss: 1.774130]\n",
      "epoch:21 step:20355 [D loss: 0.827631, acc.: 42.97%] [G loss: 1.319328]\n",
      "epoch:21 step:20356 [D loss: 0.401350, acc.: 82.03%] [G loss: 1.974459]\n",
      "epoch:21 step:20357 [D loss: 0.777221, acc.: 60.94%] [G loss: 1.797708]\n",
      "epoch:21 step:20358 [D loss: 0.390157, acc.: 79.69%] [G loss: 1.439396]\n",
      "epoch:21 step:20359 [D loss: 0.466793, acc.: 78.91%] [G loss: 1.224629]\n",
      "epoch:21 step:20360 [D loss: 0.866305, acc.: 39.84%] [G loss: 1.500189]\n",
      "epoch:21 step:20361 [D loss: 0.722321, acc.: 57.03%] [G loss: 1.705815]\n",
      "epoch:21 step:20362 [D loss: 0.257431, acc.: 97.66%] [G loss: 1.379915]\n",
      "epoch:21 step:20363 [D loss: 0.510028, acc.: 72.66%] [G loss: 1.238958]\n",
      "epoch:21 step:20364 [D loss: 0.725637, acc.: 57.03%] [G loss: 2.682182]\n",
      "epoch:21 step:20365 [D loss: 0.363609, acc.: 84.38%] [G loss: 2.051025]\n",
      "epoch:21 step:20366 [D loss: 0.461124, acc.: 81.25%] [G loss: 3.066989]\n",
      "epoch:21 step:20367 [D loss: 0.917088, acc.: 54.69%] [G loss: 1.788840]\n",
      "epoch:21 step:20368 [D loss: 0.372451, acc.: 85.94%] [G loss: 1.765576]\n",
      "epoch:21 step:20369 [D loss: 0.301367, acc.: 92.19%] [G loss: 2.045884]\n",
      "epoch:21 step:20370 [D loss: 0.510625, acc.: 75.00%] [G loss: 1.037776]\n",
      "epoch:21 step:20371 [D loss: 0.601914, acc.: 64.84%] [G loss: 1.367111]\n",
      "epoch:21 step:20372 [D loss: 0.502246, acc.: 70.31%] [G loss: 0.887421]\n",
      "epoch:21 step:20373 [D loss: 0.791444, acc.: 54.69%] [G loss: 1.440287]\n",
      "epoch:21 step:20374 [D loss: 0.502056, acc.: 76.56%] [G loss: 1.120849]\n",
      "epoch:21 step:20375 [D loss: 0.716422, acc.: 55.47%] [G loss: 1.842617]\n",
      "epoch:21 step:20376 [D loss: 0.423481, acc.: 82.81%] [G loss: 1.720643]\n",
      "epoch:21 step:20377 [D loss: 0.731362, acc.: 49.22%] [G loss: 2.139262]\n",
      "epoch:21 step:20378 [D loss: 0.263667, acc.: 98.44%] [G loss: 1.919966]\n",
      "epoch:21 step:20379 [D loss: 0.495591, acc.: 80.47%] [G loss: 2.013709]\n",
      "epoch:21 step:20380 [D loss: 0.619722, acc.: 64.84%] [G loss: 1.190598]\n",
      "epoch:21 step:20381 [D loss: 0.583386, acc.: 69.53%] [G loss: 1.045907]\n",
      "epoch:21 step:20382 [D loss: 0.481041, acc.: 78.91%] [G loss: 1.570330]\n",
      "epoch:21 step:20383 [D loss: 0.377273, acc.: 86.72%] [G loss: 1.595825]\n",
      "epoch:21 step:20384 [D loss: 0.381668, acc.: 89.06%] [G loss: 2.242696]\n",
      "epoch:21 step:20385 [D loss: 0.533963, acc.: 71.09%] [G loss: 0.888243]\n",
      "epoch:21 step:20386 [D loss: 0.662845, acc.: 64.06%] [G loss: 1.461660]\n",
      "epoch:21 step:20387 [D loss: 0.344837, acc.: 93.75%] [G loss: 0.825941]\n",
      "epoch:21 step:20388 [D loss: 0.512516, acc.: 68.75%] [G loss: 1.523533]\n",
      "epoch:21 step:20389 [D loss: 0.409005, acc.: 85.16%] [G loss: 1.185494]\n",
      "epoch:21 step:20390 [D loss: 0.625550, acc.: 63.28%] [G loss: 1.925893]\n",
      "epoch:21 step:20391 [D loss: 0.266538, acc.: 92.97%] [G loss: 1.931598]\n",
      "epoch:21 step:20392 [D loss: 0.376643, acc.: 85.16%] [G loss: 1.039230]\n",
      "epoch:21 step:20393 [D loss: 0.443180, acc.: 81.25%] [G loss: 1.632963]\n",
      "epoch:21 step:20394 [D loss: 0.539316, acc.: 69.53%] [G loss: 1.288745]\n",
      "epoch:21 step:20395 [D loss: 0.258352, acc.: 94.53%] [G loss: 1.956404]\n",
      "epoch:21 step:20396 [D loss: 0.459754, acc.: 77.34%] [G loss: 1.128915]\n",
      "epoch:21 step:20397 [D loss: 0.395047, acc.: 89.84%] [G loss: 2.016018]\n",
      "epoch:21 step:20398 [D loss: 0.341777, acc.: 90.62%] [G loss: 2.004777]\n",
      "epoch:21 step:20399 [D loss: 0.563232, acc.: 73.44%] [G loss: 0.905090]\n",
      "epoch:21 step:20400 [D loss: 0.529656, acc.: 70.31%] [G loss: 2.468121]\n",
      "epoch:21 step:20401 [D loss: 0.503209, acc.: 74.22%] [G loss: 1.207013]\n",
      "epoch:21 step:20402 [D loss: 0.654680, acc.: 60.16%] [G loss: 1.585923]\n",
      "epoch:21 step:20403 [D loss: 1.016840, acc.: 42.19%] [G loss: 1.980779]\n",
      "epoch:21 step:20404 [D loss: 0.405326, acc.: 86.72%] [G loss: 1.883024]\n",
      "epoch:21 step:20405 [D loss: 0.284928, acc.: 94.53%] [G loss: 2.323574]\n",
      "epoch:21 step:20406 [D loss: 0.614097, acc.: 67.19%] [G loss: 1.779783]\n",
      "epoch:21 step:20407 [D loss: 0.278920, acc.: 92.97%] [G loss: 1.460926]\n",
      "epoch:21 step:20408 [D loss: 1.149696, acc.: 32.03%] [G loss: 0.861435]\n",
      "epoch:21 step:20409 [D loss: 0.256333, acc.: 93.75%] [G loss: 0.503274]\n",
      "epoch:21 step:20410 [D loss: 0.394956, acc.: 86.72%] [G loss: 0.723934]\n",
      "epoch:21 step:20411 [D loss: 0.686741, acc.: 57.81%] [G loss: 1.513651]\n",
      "epoch:21 step:20412 [D loss: 0.429912, acc.: 76.56%] [G loss: 2.235269]\n",
      "epoch:21 step:20413 [D loss: 0.936008, acc.: 39.84%] [G loss: 0.819027]\n",
      "epoch:21 step:20414 [D loss: 0.565331, acc.: 70.31%] [G loss: 1.141574]\n",
      "epoch:21 step:20415 [D loss: 0.359051, acc.: 87.50%] [G loss: 0.790708]\n",
      "epoch:21 step:20416 [D loss: 0.416657, acc.: 84.38%] [G loss: 1.265059]\n",
      "epoch:21 step:20417 [D loss: 0.448423, acc.: 78.91%] [G loss: 2.655863]\n",
      "epoch:21 step:20418 [D loss: 0.484478, acc.: 82.03%] [G loss: 1.461865]\n",
      "epoch:21 step:20419 [D loss: 0.554826, acc.: 66.41%] [G loss: 0.785143]\n",
      "epoch:21 step:20420 [D loss: 0.387134, acc.: 85.16%] [G loss: 0.623362]\n",
      "epoch:21 step:20421 [D loss: 0.239899, acc.: 95.31%] [G loss: 1.320577]\n",
      "epoch:21 step:20422 [D loss: 0.529530, acc.: 70.31%] [G loss: 2.210898]\n",
      "epoch:21 step:20423 [D loss: 0.608111, acc.: 64.84%] [G loss: 1.659889]\n",
      "epoch:21 step:20424 [D loss: 0.633749, acc.: 62.50%] [G loss: 1.213216]\n",
      "epoch:21 step:20425 [D loss: 0.460027, acc.: 77.34%] [G loss: 1.279128]\n",
      "epoch:21 step:20426 [D loss: 0.734005, acc.: 57.03%] [G loss: 2.057488]\n",
      "epoch:21 step:20427 [D loss: 0.253531, acc.: 92.97%] [G loss: 1.399921]\n",
      "epoch:21 step:20428 [D loss: 0.407417, acc.: 84.38%] [G loss: 1.766008]\n",
      "epoch:21 step:20429 [D loss: 0.905588, acc.: 38.28%] [G loss: 2.003375]\n",
      "epoch:21 step:20430 [D loss: 0.643104, acc.: 64.84%] [G loss: 3.170982]\n",
      "epoch:21 step:20431 [D loss: 0.616768, acc.: 67.97%] [G loss: 0.797383]\n",
      "epoch:21 step:20432 [D loss: 0.490624, acc.: 78.12%] [G loss: 1.033325]\n",
      "epoch:21 step:20433 [D loss: 0.546227, acc.: 72.66%] [G loss: 1.024149]\n",
      "epoch:21 step:20434 [D loss: 0.299286, acc.: 91.41%] [G loss: 1.880425]\n",
      "epoch:21 step:20435 [D loss: 0.350958, acc.: 85.94%] [G loss: 1.035726]\n",
      "epoch:21 step:20436 [D loss: 0.264569, acc.: 93.75%] [G loss: 1.935562]\n",
      "epoch:21 step:20437 [D loss: 0.784730, acc.: 48.44%] [G loss: 0.947180]\n",
      "epoch:21 step:20438 [D loss: 0.438696, acc.: 84.38%] [G loss: 1.291440]\n",
      "epoch:21 step:20439 [D loss: 0.273558, acc.: 94.53%] [G loss: 5.377645]\n",
      "epoch:21 step:20440 [D loss: 0.880022, acc.: 46.09%] [G loss: 1.195381]\n",
      "epoch:21 step:20441 [D loss: 0.243864, acc.: 96.88%] [G loss: 2.001400]\n",
      "epoch:21 step:20442 [D loss: 0.227372, acc.: 95.31%] [G loss: 2.509284]\n",
      "epoch:21 step:20443 [D loss: 0.214204, acc.: 94.53%] [G loss: 1.202354]\n",
      "epoch:21 step:20444 [D loss: 0.614312, acc.: 64.06%] [G loss: 1.193053]\n",
      "epoch:21 step:20445 [D loss: 0.647191, acc.: 62.50%] [G loss: 1.856211]\n",
      "epoch:21 step:20446 [D loss: 0.680990, acc.: 62.50%] [G loss: 1.879935]\n",
      "epoch:21 step:20447 [D loss: 0.447891, acc.: 83.59%] [G loss: 2.365551]\n",
      "epoch:21 step:20448 [D loss: 0.458105, acc.: 80.47%] [G loss: 1.377530]\n",
      "epoch:21 step:20449 [D loss: 0.334705, acc.: 90.62%] [G loss: 1.481261]\n",
      "epoch:21 step:20450 [D loss: 0.557040, acc.: 66.41%] [G loss: 3.847532]\n",
      "epoch:21 step:20451 [D loss: 0.145627, acc.: 98.44%] [G loss: 1.281402]\n",
      "epoch:21 step:20452 [D loss: 0.518220, acc.: 75.00%] [G loss: 3.048216]\n",
      "epoch:21 step:20453 [D loss: 0.495798, acc.: 78.12%] [G loss: 2.174864]\n",
      "epoch:21 step:20454 [D loss: 0.219960, acc.: 94.53%] [G loss: 1.613150]\n",
      "epoch:21 step:20455 [D loss: 0.405895, acc.: 82.81%] [G loss: 1.752187]\n",
      "epoch:21 step:20456 [D loss: 0.780773, acc.: 57.03%] [G loss: 3.004128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20457 [D loss: 0.482297, acc.: 72.66%] [G loss: 1.436569]\n",
      "epoch:21 step:20458 [D loss: 0.229366, acc.: 96.88%] [G loss: 1.619673]\n",
      "epoch:21 step:20459 [D loss: 0.733827, acc.: 60.16%] [G loss: 2.077822]\n",
      "epoch:21 step:20460 [D loss: 0.358105, acc.: 89.84%] [G loss: 1.083093]\n",
      "epoch:21 step:20461 [D loss: 0.416985, acc.: 82.81%] [G loss: 2.138508]\n",
      "epoch:21 step:20462 [D loss: 0.252057, acc.: 93.75%] [G loss: 2.047045]\n",
      "epoch:21 step:20463 [D loss: 0.408167, acc.: 86.72%] [G loss: 0.615566]\n",
      "epoch:21 step:20464 [D loss: 0.749557, acc.: 59.38%] [G loss: 1.424199]\n",
      "epoch:21 step:20465 [D loss: 0.847236, acc.: 55.47%] [G loss: 1.605903]\n",
      "epoch:21 step:20466 [D loss: 0.596197, acc.: 69.53%] [G loss: 0.814884]\n",
      "epoch:21 step:20467 [D loss: 0.769551, acc.: 53.91%] [G loss: 1.869031]\n",
      "epoch:21 step:20468 [D loss: 0.839787, acc.: 57.03%] [G loss: 3.200762]\n",
      "epoch:21 step:20469 [D loss: 0.521200, acc.: 72.66%] [G loss: 1.951064]\n",
      "epoch:21 step:20470 [D loss: 0.375682, acc.: 84.38%] [G loss: 2.627380]\n",
      "epoch:21 step:20471 [D loss: 0.873542, acc.: 40.62%] [G loss: 1.525112]\n",
      "epoch:21 step:20472 [D loss: 0.814286, acc.: 50.00%] [G loss: 2.048665]\n",
      "epoch:21 step:20473 [D loss: 0.678388, acc.: 60.16%] [G loss: 1.779111]\n",
      "epoch:21 step:20474 [D loss: 0.647230, acc.: 65.62%] [G loss: 1.326778]\n",
      "epoch:21 step:20475 [D loss: 0.923884, acc.: 36.72%] [G loss: 1.570372]\n",
      "epoch:21 step:20476 [D loss: 0.314326, acc.: 92.19%] [G loss: 0.986080]\n",
      "epoch:21 step:20477 [D loss: 0.713909, acc.: 62.50%] [G loss: 1.045952]\n",
      "epoch:21 step:20478 [D loss: 0.329786, acc.: 89.84%] [G loss: 1.484960]\n",
      "epoch:21 step:20479 [D loss: 0.916469, acc.: 55.47%] [G loss: 1.435661]\n",
      "epoch:21 step:20480 [D loss: 0.309740, acc.: 92.19%] [G loss: 1.709457]\n",
      "epoch:21 step:20481 [D loss: 0.543788, acc.: 74.22%] [G loss: 2.148728]\n",
      "epoch:21 step:20482 [D loss: 0.506696, acc.: 77.34%] [G loss: 2.538998]\n",
      "epoch:21 step:20483 [D loss: 1.046476, acc.: 33.59%] [G loss: 2.309322]\n",
      "epoch:21 step:20484 [D loss: 0.287917, acc.: 88.28%] [G loss: 2.600037]\n",
      "epoch:21 step:20485 [D loss: 0.592132, acc.: 68.75%] [G loss: 0.951610]\n",
      "epoch:21 step:20486 [D loss: 0.385820, acc.: 88.28%] [G loss: 1.388955]\n",
      "epoch:21 step:20487 [D loss: 0.745869, acc.: 60.16%] [G loss: 1.751946]\n",
      "epoch:21 step:20488 [D loss: 0.445077, acc.: 82.81%] [G loss: 1.712952]\n",
      "epoch:21 step:20489 [D loss: 0.820669, acc.: 42.97%] [G loss: 2.107867]\n",
      "epoch:21 step:20490 [D loss: 0.645598, acc.: 62.50%] [G loss: 1.731393]\n",
      "epoch:21 step:20491 [D loss: 0.658679, acc.: 62.50%] [G loss: 1.313221]\n",
      "epoch:21 step:20492 [D loss: 0.488404, acc.: 78.91%] [G loss: 1.171767]\n",
      "epoch:21 step:20493 [D loss: 0.480201, acc.: 75.00%] [G loss: 0.994347]\n",
      "epoch:21 step:20494 [D loss: 0.643744, acc.: 60.94%] [G loss: 1.410340]\n",
      "epoch:21 step:20495 [D loss: 0.457798, acc.: 75.00%] [G loss: 1.498299]\n",
      "epoch:21 step:20496 [D loss: 0.611093, acc.: 64.84%] [G loss: 0.646991]\n",
      "epoch:21 step:20497 [D loss: 0.452584, acc.: 81.25%] [G loss: 1.521766]\n",
      "epoch:21 step:20498 [D loss: 0.534544, acc.: 74.22%] [G loss: 2.655607]\n",
      "epoch:21 step:20499 [D loss: 0.753187, acc.: 55.47%] [G loss: 1.126571]\n",
      "epoch:21 step:20500 [D loss: 0.238433, acc.: 95.31%] [G loss: 0.827056]\n",
      "epoch:21 step:20501 [D loss: 0.898306, acc.: 37.50%] [G loss: 1.989431]\n",
      "epoch:21 step:20502 [D loss: 0.361303, acc.: 88.28%] [G loss: 1.050872]\n",
      "epoch:21 step:20503 [D loss: 0.791258, acc.: 53.12%] [G loss: 2.142855]\n",
      "epoch:21 step:20504 [D loss: 0.277865, acc.: 93.75%] [G loss: 2.774117]\n",
      "epoch:21 step:20505 [D loss: 0.393131, acc.: 83.59%] [G loss: 1.272749]\n",
      "epoch:21 step:20506 [D loss: 1.006433, acc.: 28.12%] [G loss: 2.769434]\n",
      "epoch:21 step:20507 [D loss: 0.594332, acc.: 63.28%] [G loss: 1.506967]\n",
      "epoch:21 step:20508 [D loss: 0.652657, acc.: 63.28%] [G loss: 0.903374]\n",
      "epoch:21 step:20509 [D loss: 0.348234, acc.: 89.06%] [G loss: 0.809085]\n",
      "epoch:21 step:20510 [D loss: 0.766877, acc.: 51.56%] [G loss: 1.634513]\n",
      "epoch:21 step:20511 [D loss: 0.498376, acc.: 73.44%] [G loss: 1.080683]\n",
      "epoch:21 step:20512 [D loss: 0.480013, acc.: 82.81%] [G loss: 1.184703]\n",
      "epoch:21 step:20513 [D loss: 0.314238, acc.: 89.84%] [G loss: 1.923367]\n",
      "epoch:21 step:20514 [D loss: 0.287439, acc.: 90.62%] [G loss: 0.993305]\n",
      "epoch:21 step:20515 [D loss: 0.449560, acc.: 82.81%] [G loss: 1.333570]\n",
      "epoch:21 step:20516 [D loss: 0.332606, acc.: 89.84%] [G loss: 1.216693]\n",
      "epoch:21 step:20517 [D loss: 0.437722, acc.: 82.03%] [G loss: 2.177082]\n",
      "epoch:21 step:20518 [D loss: 0.565059, acc.: 69.53%] [G loss: 1.889619]\n",
      "epoch:21 step:20519 [D loss: 0.516420, acc.: 75.00%] [G loss: 1.694013]\n",
      "epoch:21 step:20520 [D loss: 0.241539, acc.: 92.97%] [G loss: 1.431597]\n",
      "epoch:21 step:20521 [D loss: 0.912811, acc.: 57.03%] [G loss: 1.234444]\n",
      "epoch:21 step:20522 [D loss: 0.535112, acc.: 70.31%] [G loss: 0.751829]\n",
      "epoch:21 step:20523 [D loss: 0.596281, acc.: 63.28%] [G loss: 0.832722]\n",
      "epoch:21 step:20524 [D loss: 0.416493, acc.: 86.72%] [G loss: 1.442647]\n",
      "epoch:21 step:20525 [D loss: 0.796828, acc.: 52.34%] [G loss: 1.129377]\n",
      "epoch:21 step:20526 [D loss: 0.523314, acc.: 79.69%] [G loss: 1.508885]\n",
      "epoch:21 step:20527 [D loss: 0.314927, acc.: 87.50%] [G loss: 2.308182]\n",
      "epoch:21 step:20528 [D loss: 0.314638, acc.: 90.62%] [G loss: 1.886675]\n",
      "epoch:21 step:20529 [D loss: 0.376896, acc.: 88.28%] [G loss: 0.997958]\n",
      "epoch:21 step:20530 [D loss: 0.605873, acc.: 63.28%] [G loss: 0.693412]\n",
      "epoch:21 step:20531 [D loss: 0.596003, acc.: 66.41%] [G loss: 2.357348]\n",
      "epoch:21 step:20532 [D loss: 0.551378, acc.: 65.62%] [G loss: 1.746171]\n",
      "epoch:21 step:20533 [D loss: 0.504234, acc.: 78.12%] [G loss: 1.456834]\n",
      "epoch:21 step:20534 [D loss: 0.622543, acc.: 61.72%] [G loss: 2.425000]\n",
      "epoch:21 step:20535 [D loss: 0.614971, acc.: 64.84%] [G loss: 2.614663]\n",
      "epoch:21 step:20536 [D loss: 0.366588, acc.: 91.41%] [G loss: 2.468835]\n",
      "epoch:21 step:20537 [D loss: 0.406726, acc.: 79.69%] [G loss: 1.945555]\n",
      "epoch:21 step:20538 [D loss: 0.472698, acc.: 78.91%] [G loss: 1.284449]\n",
      "epoch:21 step:20539 [D loss: 0.609959, acc.: 62.50%] [G loss: 2.179226]\n",
      "epoch:21 step:20540 [D loss: 0.441220, acc.: 80.47%] [G loss: 1.294135]\n",
      "epoch:21 step:20541 [D loss: 0.599667, acc.: 67.97%] [G loss: 2.030571]\n",
      "epoch:21 step:20542 [D loss: 0.315563, acc.: 92.19%] [G loss: 1.006125]\n",
      "epoch:21 step:20543 [D loss: 0.365949, acc.: 89.06%] [G loss: 2.446328]\n",
      "epoch:21 step:20544 [D loss: 0.407200, acc.: 75.78%] [G loss: 1.803456]\n",
      "epoch:21 step:20545 [D loss: 0.390282, acc.: 89.06%] [G loss: 1.029979]\n",
      "epoch:21 step:20546 [D loss: 0.242302, acc.: 93.75%] [G loss: 2.670757]\n",
      "epoch:21 step:20547 [D loss: 0.709800, acc.: 57.81%] [G loss: 3.694116]\n",
      "epoch:21 step:20548 [D loss: 0.697595, acc.: 62.50%] [G loss: 0.697215]\n",
      "epoch:21 step:20549 [D loss: 0.539375, acc.: 71.09%] [G loss: 0.719055]\n",
      "epoch:21 step:20550 [D loss: 0.396328, acc.: 83.59%] [G loss: 1.917551]\n",
      "epoch:21 step:20551 [D loss: 0.589527, acc.: 74.22%] [G loss: 1.093530]\n",
      "epoch:21 step:20552 [D loss: 0.600565, acc.: 69.53%] [G loss: 1.678730]\n",
      "epoch:21 step:20553 [D loss: 0.498215, acc.: 72.66%] [G loss: 1.188351]\n",
      "epoch:21 step:20554 [D loss: 0.504003, acc.: 77.34%] [G loss: 3.183147]\n",
      "epoch:21 step:20555 [D loss: 0.503873, acc.: 75.78%] [G loss: 1.160159]\n",
      "epoch:21 step:20556 [D loss: 0.300703, acc.: 92.19%] [G loss: 2.903832]\n",
      "epoch:21 step:20557 [D loss: 0.723978, acc.: 55.47%] [G loss: 1.655626]\n",
      "epoch:21 step:20558 [D loss: 0.367880, acc.: 92.19%] [G loss: 1.782371]\n",
      "epoch:21 step:20559 [D loss: 0.584632, acc.: 71.09%] [G loss: 3.787567]\n",
      "epoch:21 step:20560 [D loss: 0.698166, acc.: 57.03%] [G loss: 2.051538]\n",
      "epoch:21 step:20561 [D loss: 0.259902, acc.: 90.62%] [G loss: 0.402362]\n",
      "epoch:21 step:20562 [D loss: 0.791042, acc.: 43.75%] [G loss: 0.940220]\n",
      "epoch:21 step:20563 [D loss: 0.793363, acc.: 51.56%] [G loss: 1.716906]\n",
      "epoch:21 step:20564 [D loss: 0.627666, acc.: 63.28%] [G loss: 2.254981]\n",
      "epoch:21 step:20565 [D loss: 0.392257, acc.: 85.94%] [G loss: 1.500153]\n",
      "epoch:21 step:20566 [D loss: 0.288063, acc.: 92.97%] [G loss: 2.378878]\n",
      "epoch:21 step:20567 [D loss: 0.579015, acc.: 69.53%] [G loss: 1.001231]\n",
      "epoch:21 step:20568 [D loss: 0.377303, acc.: 85.94%] [G loss: 1.382346]\n",
      "epoch:21 step:20569 [D loss: 0.366079, acc.: 89.84%] [G loss: 0.621801]\n",
      "epoch:21 step:20570 [D loss: 0.547574, acc.: 68.75%] [G loss: 1.882579]\n",
      "epoch:21 step:20571 [D loss: 0.639143, acc.: 71.88%] [G loss: 0.951047]\n",
      "epoch:21 step:20572 [D loss: 0.813514, acc.: 46.09%] [G loss: 1.754808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20573 [D loss: 0.425310, acc.: 84.38%] [G loss: 1.695601]\n",
      "epoch:21 step:20574 [D loss: 0.382400, acc.: 89.84%] [G loss: 1.552620]\n",
      "epoch:21 step:20575 [D loss: 0.723562, acc.: 58.59%] [G loss: 1.583122]\n",
      "epoch:21 step:20576 [D loss: 0.609557, acc.: 66.41%] [G loss: 1.088742]\n",
      "epoch:21 step:20577 [D loss: 0.576872, acc.: 67.97%] [G loss: 0.917975]\n",
      "epoch:21 step:20578 [D loss: 0.550712, acc.: 71.09%] [G loss: 1.707558]\n",
      "epoch:21 step:20579 [D loss: 0.310716, acc.: 93.75%] [G loss: 1.181673]\n",
      "epoch:21 step:20580 [D loss: 0.830768, acc.: 45.31%] [G loss: 1.333256]\n",
      "epoch:21 step:20581 [D loss: 0.454324, acc.: 76.56%] [G loss: 1.222194]\n",
      "epoch:21 step:20582 [D loss: 0.577294, acc.: 69.53%] [G loss: 1.545871]\n",
      "epoch:21 step:20583 [D loss: 0.344095, acc.: 84.38%] [G loss: 2.112181]\n",
      "epoch:21 step:20584 [D loss: 0.339031, acc.: 92.19%] [G loss: 1.193790]\n",
      "epoch:21 step:20585 [D loss: 0.413546, acc.: 85.16%] [G loss: 0.891510]\n",
      "epoch:21 step:20586 [D loss: 0.321694, acc.: 90.62%] [G loss: 1.704314]\n",
      "epoch:21 step:20587 [D loss: 0.509310, acc.: 74.22%] [G loss: 1.304935]\n",
      "epoch:21 step:20588 [D loss: 0.239952, acc.: 95.31%] [G loss: 2.096841]\n",
      "epoch:21 step:20589 [D loss: 0.429582, acc.: 83.59%] [G loss: 2.769247]\n",
      "epoch:21 step:20590 [D loss: 0.426789, acc.: 83.59%] [G loss: 1.267040]\n",
      "epoch:21 step:20591 [D loss: 0.323991, acc.: 86.72%] [G loss: 2.559218]\n",
      "epoch:21 step:20592 [D loss: 0.574190, acc.: 71.09%] [G loss: 3.147859]\n",
      "epoch:21 step:20593 [D loss: 0.565597, acc.: 73.44%] [G loss: 1.120061]\n",
      "epoch:21 step:20594 [D loss: 0.607400, acc.: 67.97%] [G loss: 1.011150]\n",
      "epoch:21 step:20595 [D loss: 0.974031, acc.: 52.34%] [G loss: 1.491414]\n",
      "epoch:21 step:20596 [D loss: 0.598235, acc.: 68.75%] [G loss: 2.438186]\n",
      "epoch:21 step:20597 [D loss: 0.333393, acc.: 78.91%] [G loss: 3.097974]\n",
      "epoch:21 step:20598 [D loss: 0.349635, acc.: 85.94%] [G loss: 1.375258]\n",
      "epoch:21 step:20599 [D loss: 1.091187, acc.: 45.31%] [G loss: 1.878479]\n",
      "epoch:21 step:20600 [D loss: 0.508882, acc.: 78.12%] [G loss: 3.081445]\n",
      "epoch:21 step:20601 [D loss: 0.448710, acc.: 84.38%] [G loss: 2.352374]\n",
      "epoch:21 step:20602 [D loss: 0.527767, acc.: 71.09%] [G loss: 2.518683]\n",
      "epoch:21 step:20603 [D loss: 0.647427, acc.: 60.16%] [G loss: 1.421395]\n",
      "epoch:21 step:20604 [D loss: 0.530973, acc.: 71.88%] [G loss: 2.577502]\n",
      "epoch:21 step:20605 [D loss: 0.381265, acc.: 89.06%] [G loss: 1.802949]\n",
      "epoch:21 step:20606 [D loss: 0.499399, acc.: 78.12%] [G loss: 1.156354]\n",
      "epoch:21 step:20607 [D loss: 0.451161, acc.: 79.69%] [G loss: 1.539638]\n",
      "epoch:21 step:20608 [D loss: 0.475016, acc.: 77.34%] [G loss: 2.408560]\n",
      "epoch:21 step:20609 [D loss: 0.528304, acc.: 74.22%] [G loss: 0.686225]\n",
      "epoch:21 step:20610 [D loss: 0.390764, acc.: 87.50%] [G loss: 1.722378]\n",
      "epoch:21 step:20611 [D loss: 0.320311, acc.: 92.97%] [G loss: 0.675443]\n",
      "epoch:21 step:20612 [D loss: 0.854334, acc.: 35.94%] [G loss: 1.797481]\n",
      "epoch:21 step:20613 [D loss: 0.572118, acc.: 69.53%] [G loss: 1.509019]\n",
      "epoch:21 step:20614 [D loss: 0.454857, acc.: 82.03%] [G loss: 1.808891]\n",
      "epoch:22 step:20615 [D loss: 0.207670, acc.: 97.66%] [G loss: 0.926429]\n",
      "epoch:22 step:20616 [D loss: 0.451125, acc.: 75.00%] [G loss: 1.098634]\n",
      "epoch:22 step:20617 [D loss: 0.878920, acc.: 44.53%] [G loss: 1.603195]\n",
      "epoch:22 step:20618 [D loss: 1.105102, acc.: 53.91%] [G loss: 0.836923]\n",
      "epoch:22 step:20619 [D loss: 0.291084, acc.: 93.75%] [G loss: 1.242737]\n",
      "epoch:22 step:20620 [D loss: 0.581316, acc.: 67.19%] [G loss: 2.163491]\n",
      "epoch:22 step:20621 [D loss: 0.369213, acc.: 89.06%] [G loss: 1.425861]\n",
      "epoch:22 step:20622 [D loss: 0.598308, acc.: 66.41%] [G loss: 1.042274]\n",
      "epoch:22 step:20623 [D loss: 0.438304, acc.: 88.28%] [G loss: 1.201651]\n",
      "epoch:22 step:20624 [D loss: 0.477506, acc.: 82.03%] [G loss: 0.905738]\n",
      "epoch:22 step:20625 [D loss: 0.567917, acc.: 70.31%] [G loss: 1.439844]\n",
      "epoch:22 step:20626 [D loss: 0.441289, acc.: 84.38%] [G loss: 1.266135]\n",
      "epoch:22 step:20627 [D loss: 0.657585, acc.: 63.28%] [G loss: 1.921893]\n",
      "epoch:22 step:20628 [D loss: 1.005807, acc.: 32.03%] [G loss: 1.760809]\n",
      "epoch:22 step:20629 [D loss: 0.743362, acc.: 57.03%] [G loss: 1.328311]\n",
      "epoch:22 step:20630 [D loss: 0.508712, acc.: 75.78%] [G loss: 1.179529]\n",
      "epoch:22 step:20631 [D loss: 0.468056, acc.: 78.12%] [G loss: 1.458728]\n",
      "epoch:22 step:20632 [D loss: 0.499763, acc.: 76.56%] [G loss: 1.504923]\n",
      "epoch:22 step:20633 [D loss: 0.527733, acc.: 75.78%] [G loss: 2.041797]\n",
      "epoch:22 step:20634 [D loss: 0.520666, acc.: 72.66%] [G loss: 1.117234]\n",
      "epoch:22 step:20635 [D loss: 0.351759, acc.: 87.50%] [G loss: 1.946170]\n",
      "epoch:22 step:20636 [D loss: 1.197933, acc.: 22.66%] [G loss: 1.401349]\n",
      "epoch:22 step:20637 [D loss: 0.565115, acc.: 71.09%] [G loss: 1.358213]\n",
      "epoch:22 step:20638 [D loss: 0.403240, acc.: 84.38%] [G loss: 1.538951]\n",
      "epoch:22 step:20639 [D loss: 0.461257, acc.: 82.81%] [G loss: 1.898686]\n",
      "epoch:22 step:20640 [D loss: 0.901376, acc.: 52.34%] [G loss: 0.989013]\n",
      "epoch:22 step:20641 [D loss: 0.559407, acc.: 73.44%] [G loss: 1.368407]\n",
      "epoch:22 step:20642 [D loss: 0.250961, acc.: 94.53%] [G loss: 1.965047]\n",
      "epoch:22 step:20643 [D loss: 0.528695, acc.: 71.88%] [G loss: 1.941000]\n",
      "epoch:22 step:20644 [D loss: 0.414409, acc.: 84.38%] [G loss: 0.919962]\n",
      "epoch:22 step:20645 [D loss: 0.378341, acc.: 88.28%] [G loss: 1.414361]\n",
      "epoch:22 step:20646 [D loss: 0.368057, acc.: 88.28%] [G loss: 0.934626]\n",
      "epoch:22 step:20647 [D loss: 0.292040, acc.: 92.19%] [G loss: 1.478647]\n",
      "epoch:22 step:20648 [D loss: 0.355344, acc.: 86.72%] [G loss: 1.993558]\n",
      "epoch:22 step:20649 [D loss: 0.379675, acc.: 89.06%] [G loss: 1.531373]\n",
      "epoch:22 step:20650 [D loss: 0.441854, acc.: 81.25%] [G loss: 1.249637]\n",
      "epoch:22 step:20651 [D loss: 0.443960, acc.: 82.03%] [G loss: 2.331709]\n",
      "epoch:22 step:20652 [D loss: 0.350305, acc.: 88.28%] [G loss: 2.016912]\n",
      "epoch:22 step:20653 [D loss: 0.313804, acc.: 94.53%] [G loss: 1.342025]\n",
      "epoch:22 step:20654 [D loss: 0.615298, acc.: 59.38%] [G loss: 2.178949]\n",
      "epoch:22 step:20655 [D loss: 0.699258, acc.: 59.38%] [G loss: 0.793879]\n",
      "epoch:22 step:20656 [D loss: 0.852194, acc.: 45.31%] [G loss: 1.306602]\n",
      "epoch:22 step:20657 [D loss: 0.856742, acc.: 35.94%] [G loss: 1.543810]\n",
      "epoch:22 step:20658 [D loss: 1.186286, acc.: 35.94%] [G loss: 2.017565]\n",
      "epoch:22 step:20659 [D loss: 0.511913, acc.: 74.22%] [G loss: 2.556835]\n",
      "epoch:22 step:20660 [D loss: 0.256778, acc.: 93.75%] [G loss: 1.096503]\n",
      "epoch:22 step:20661 [D loss: 0.649538, acc.: 60.94%] [G loss: 0.552563]\n",
      "epoch:22 step:20662 [D loss: 0.496345, acc.: 74.22%] [G loss: 1.311817]\n",
      "epoch:22 step:20663 [D loss: 0.554379, acc.: 67.19%] [G loss: 2.256768]\n",
      "epoch:22 step:20664 [D loss: 0.529902, acc.: 75.00%] [G loss: 1.561009]\n",
      "epoch:22 step:20665 [D loss: 0.544363, acc.: 67.97%] [G loss: 1.619437]\n",
      "epoch:22 step:20666 [D loss: 0.400917, acc.: 84.38%] [G loss: 2.097727]\n",
      "epoch:22 step:20667 [D loss: 0.504272, acc.: 75.00%] [G loss: 2.094801]\n",
      "epoch:22 step:20668 [D loss: 0.244015, acc.: 93.75%] [G loss: 2.015560]\n",
      "epoch:22 step:20669 [D loss: 0.465380, acc.: 75.00%] [G loss: 2.141231]\n",
      "epoch:22 step:20670 [D loss: 0.356796, acc.: 87.50%] [G loss: 0.908553]\n",
      "epoch:22 step:20671 [D loss: 0.619861, acc.: 65.62%] [G loss: 0.528880]\n",
      "epoch:22 step:20672 [D loss: 0.239145, acc.: 93.75%] [G loss: 1.039458]\n",
      "epoch:22 step:20673 [D loss: 0.574647, acc.: 70.31%] [G loss: 0.770408]\n",
      "epoch:22 step:20674 [D loss: 0.457656, acc.: 78.91%] [G loss: 1.056540]\n",
      "epoch:22 step:20675 [D loss: 0.693847, acc.: 57.03%] [G loss: 1.574414]\n",
      "epoch:22 step:20676 [D loss: 0.391839, acc.: 88.28%] [G loss: 1.488474]\n",
      "epoch:22 step:20677 [D loss: 0.769295, acc.: 51.56%] [G loss: 1.444539]\n",
      "epoch:22 step:20678 [D loss: 0.373515, acc.: 88.28%] [G loss: 2.789970]\n",
      "epoch:22 step:20679 [D loss: 0.542022, acc.: 71.09%] [G loss: 3.234515]\n",
      "epoch:22 step:20680 [D loss: 0.524722, acc.: 67.19%] [G loss: 1.022007]\n",
      "epoch:22 step:20681 [D loss: 0.579217, acc.: 71.09%] [G loss: 0.941105]\n",
      "epoch:22 step:20682 [D loss: 0.469922, acc.: 79.69%] [G loss: 2.168479]\n",
      "epoch:22 step:20683 [D loss: 0.486151, acc.: 73.44%] [G loss: 1.785024]\n",
      "epoch:22 step:20684 [D loss: 0.382826, acc.: 86.72%] [G loss: 1.153990]\n",
      "epoch:22 step:20685 [D loss: 0.577738, acc.: 67.97%] [G loss: 1.290448]\n",
      "epoch:22 step:20686 [D loss: 0.320262, acc.: 91.41%] [G loss: 1.285826]\n",
      "epoch:22 step:20687 [D loss: 0.262243, acc.: 95.31%] [G loss: 1.184996]\n",
      "epoch:22 step:20688 [D loss: 0.571832, acc.: 65.62%] [G loss: 1.662168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20689 [D loss: 0.682619, acc.: 61.72%] [G loss: 4.021233]\n",
      "epoch:22 step:20690 [D loss: 0.346774, acc.: 90.62%] [G loss: 1.257854]\n",
      "epoch:22 step:20691 [D loss: 0.172260, acc.: 98.44%] [G loss: 1.557714]\n",
      "epoch:22 step:20692 [D loss: 0.507292, acc.: 71.88%] [G loss: 3.370437]\n",
      "epoch:22 step:20693 [D loss: 0.791034, acc.: 56.25%] [G loss: 1.093174]\n",
      "epoch:22 step:20694 [D loss: 0.325610, acc.: 89.06%] [G loss: 1.897407]\n",
      "epoch:22 step:20695 [D loss: 0.943771, acc.: 43.75%] [G loss: 0.731600]\n",
      "epoch:22 step:20696 [D loss: 0.947913, acc.: 44.53%] [G loss: 1.213708]\n",
      "epoch:22 step:20697 [D loss: 0.469168, acc.: 75.00%] [G loss: 1.737237]\n",
      "epoch:22 step:20698 [D loss: 0.508757, acc.: 76.56%] [G loss: 1.725517]\n",
      "epoch:22 step:20699 [D loss: 0.355297, acc.: 92.19%] [G loss: 1.556211]\n",
      "epoch:22 step:20700 [D loss: 0.268231, acc.: 94.53%] [G loss: 2.141081]\n",
      "epoch:22 step:20701 [D loss: 0.625520, acc.: 64.84%] [G loss: 1.734495]\n",
      "epoch:22 step:20702 [D loss: 0.500803, acc.: 68.75%] [G loss: 1.017575]\n",
      "epoch:22 step:20703 [D loss: 0.150150, acc.: 97.66%] [G loss: 1.558819]\n",
      "epoch:22 step:20704 [D loss: 0.227440, acc.: 94.53%] [G loss: 1.631920]\n",
      "epoch:22 step:20705 [D loss: 0.222557, acc.: 93.75%] [G loss: 2.773821]\n",
      "epoch:22 step:20706 [D loss: 0.307876, acc.: 92.97%] [G loss: 1.508853]\n",
      "epoch:22 step:20707 [D loss: 0.235166, acc.: 94.53%] [G loss: 1.089650]\n",
      "epoch:22 step:20708 [D loss: 0.250689, acc.: 93.75%] [G loss: 2.955687]\n",
      "epoch:22 step:20709 [D loss: 0.206413, acc.: 95.31%] [G loss: 0.860841]\n",
      "epoch:22 step:20710 [D loss: 0.453686, acc.: 74.22%] [G loss: 1.908532]\n",
      "epoch:22 step:20711 [D loss: 0.247024, acc.: 95.31%] [G loss: 3.017780]\n",
      "epoch:22 step:20712 [D loss: 1.491126, acc.: 12.50%] [G loss: 0.713828]\n",
      "epoch:22 step:20713 [D loss: 0.231222, acc.: 92.97%] [G loss: 1.183298]\n",
      "epoch:22 step:20714 [D loss: 0.534654, acc.: 78.91%] [G loss: 0.638903]\n",
      "epoch:22 step:20715 [D loss: 0.862365, acc.: 42.19%] [G loss: 1.452496]\n",
      "epoch:22 step:20716 [D loss: 0.665293, acc.: 57.03%] [G loss: 1.329335]\n",
      "epoch:22 step:20717 [D loss: 0.602853, acc.: 64.06%] [G loss: 3.071583]\n",
      "epoch:22 step:20718 [D loss: 0.811713, acc.: 55.47%] [G loss: 1.062983]\n",
      "epoch:22 step:20719 [D loss: 0.446691, acc.: 78.12%] [G loss: 0.869486]\n",
      "epoch:22 step:20720 [D loss: 0.649781, acc.: 61.72%] [G loss: 2.030550]\n",
      "epoch:22 step:20721 [D loss: 0.768183, acc.: 57.81%] [G loss: 0.817576]\n",
      "epoch:22 step:20722 [D loss: 0.462940, acc.: 80.47%] [G loss: 1.871588]\n",
      "epoch:22 step:20723 [D loss: 0.390191, acc.: 92.97%] [G loss: 3.269418]\n",
      "epoch:22 step:20724 [D loss: 0.374382, acc.: 86.72%] [G loss: 2.578095]\n",
      "epoch:22 step:20725 [D loss: 0.536004, acc.: 70.31%] [G loss: 1.326468]\n",
      "epoch:22 step:20726 [D loss: 0.505113, acc.: 74.22%] [G loss: 1.291336]\n",
      "epoch:22 step:20727 [D loss: 0.599469, acc.: 64.06%] [G loss: 2.013387]\n",
      "epoch:22 step:20728 [D loss: 0.366361, acc.: 90.62%] [G loss: 2.994401]\n",
      "epoch:22 step:20729 [D loss: 0.656877, acc.: 62.50%] [G loss: 1.403014]\n",
      "epoch:22 step:20730 [D loss: 0.538010, acc.: 75.00%] [G loss: 1.383275]\n",
      "epoch:22 step:20731 [D loss: 0.247928, acc.: 94.53%] [G loss: 1.919351]\n",
      "epoch:22 step:20732 [D loss: 0.411148, acc.: 78.91%] [G loss: 2.223927]\n",
      "epoch:22 step:20733 [D loss: 0.656293, acc.: 57.81%] [G loss: 0.905473]\n",
      "epoch:22 step:20734 [D loss: 0.326409, acc.: 92.97%] [G loss: 2.125424]\n",
      "epoch:22 step:20735 [D loss: 0.478741, acc.: 78.91%] [G loss: 1.777667]\n",
      "epoch:22 step:20736 [D loss: 0.509656, acc.: 75.00%] [G loss: 1.303532]\n",
      "epoch:22 step:20737 [D loss: 0.452809, acc.: 79.69%] [G loss: 0.822053]\n",
      "epoch:22 step:20738 [D loss: 0.695582, acc.: 60.16%] [G loss: 1.338857]\n",
      "epoch:22 step:20739 [D loss: 0.477485, acc.: 81.25%] [G loss: 0.413520]\n",
      "epoch:22 step:20740 [D loss: 0.544370, acc.: 66.41%] [G loss: 1.679954]\n",
      "epoch:22 step:20741 [D loss: 1.122347, acc.: 21.09%] [G loss: 1.178427]\n",
      "epoch:22 step:20742 [D loss: 0.405482, acc.: 86.72%] [G loss: 2.970611]\n",
      "epoch:22 step:20743 [D loss: 0.357490, acc.: 89.06%] [G loss: 1.657138]\n",
      "epoch:22 step:20744 [D loss: 0.732284, acc.: 55.47%] [G loss: 2.564610]\n",
      "epoch:22 step:20745 [D loss: 0.375454, acc.: 89.84%] [G loss: 3.886440]\n",
      "epoch:22 step:20746 [D loss: 0.378257, acc.: 87.50%] [G loss: 1.886925]\n",
      "epoch:22 step:20747 [D loss: 0.248663, acc.: 96.09%] [G loss: 2.234149]\n",
      "epoch:22 step:20748 [D loss: 0.206265, acc.: 95.31%] [G loss: 1.667367]\n",
      "epoch:22 step:20749 [D loss: 0.659873, acc.: 64.84%] [G loss: 1.883264]\n",
      "epoch:22 step:20750 [D loss: 0.927655, acc.: 53.12%] [G loss: 0.954795]\n",
      "epoch:22 step:20751 [D loss: 0.354446, acc.: 90.62%] [G loss: 2.461394]\n",
      "epoch:22 step:20752 [D loss: 0.844214, acc.: 59.38%] [G loss: 1.343966]\n",
      "epoch:22 step:20753 [D loss: 0.490356, acc.: 75.78%] [G loss: 2.071979]\n",
      "epoch:22 step:20754 [D loss: 0.590150, acc.: 64.84%] [G loss: 1.416785]\n",
      "epoch:22 step:20755 [D loss: 0.684436, acc.: 60.94%] [G loss: 0.746030]\n",
      "epoch:22 step:20756 [D loss: 0.297584, acc.: 94.53%] [G loss: 1.600708]\n",
      "epoch:22 step:20757 [D loss: 0.755930, acc.: 49.22%] [G loss: 1.411760]\n",
      "epoch:22 step:20758 [D loss: 0.561496, acc.: 74.22%] [G loss: 1.240467]\n",
      "epoch:22 step:20759 [D loss: 0.400662, acc.: 89.06%] [G loss: 2.034809]\n",
      "epoch:22 step:20760 [D loss: 0.705242, acc.: 54.69%] [G loss: 2.156528]\n",
      "epoch:22 step:20761 [D loss: 0.264470, acc.: 94.53%] [G loss: 1.363870]\n",
      "epoch:22 step:20762 [D loss: 0.556488, acc.: 72.66%] [G loss: 2.054720]\n",
      "epoch:22 step:20763 [D loss: 0.538661, acc.: 75.78%] [G loss: 2.298669]\n",
      "epoch:22 step:20764 [D loss: 0.413638, acc.: 84.38%] [G loss: 1.198634]\n",
      "epoch:22 step:20765 [D loss: 0.814998, acc.: 50.00%] [G loss: 1.627423]\n",
      "epoch:22 step:20766 [D loss: 0.493516, acc.: 79.69%] [G loss: 1.548522]\n",
      "epoch:22 step:20767 [D loss: 0.437989, acc.: 84.38%] [G loss: 2.514397]\n",
      "epoch:22 step:20768 [D loss: 0.322471, acc.: 89.84%] [G loss: 1.906866]\n",
      "epoch:22 step:20769 [D loss: 0.240403, acc.: 91.41%] [G loss: 2.953603]\n",
      "epoch:22 step:20770 [D loss: 0.291483, acc.: 91.41%] [G loss: 1.676323]\n",
      "epoch:22 step:20771 [D loss: 0.990271, acc.: 38.28%] [G loss: 1.098736]\n",
      "epoch:22 step:20772 [D loss: 0.470218, acc.: 82.03%] [G loss: 0.961408]\n",
      "epoch:22 step:20773 [D loss: 0.241146, acc.: 96.88%] [G loss: 1.828104]\n",
      "epoch:22 step:20774 [D loss: 0.313452, acc.: 85.16%] [G loss: 0.933095]\n",
      "epoch:22 step:20775 [D loss: 0.382072, acc.: 87.50%] [G loss: 2.449298]\n",
      "epoch:22 step:20776 [D loss: 0.191328, acc.: 94.53%] [G loss: 0.651914]\n",
      "epoch:22 step:20777 [D loss: 0.217249, acc.: 97.66%] [G loss: 1.869746]\n",
      "epoch:22 step:20778 [D loss: 0.959318, acc.: 40.62%] [G loss: 2.995573]\n",
      "epoch:22 step:20779 [D loss: 1.157074, acc.: 22.66%] [G loss: 2.414641]\n",
      "epoch:22 step:20780 [D loss: 0.761112, acc.: 55.47%] [G loss: 1.115825]\n",
      "epoch:22 step:20781 [D loss: 0.210398, acc.: 96.88%] [G loss: 2.205897]\n",
      "epoch:22 step:20782 [D loss: 0.398098, acc.: 83.59%] [G loss: 1.062977]\n",
      "epoch:22 step:20783 [D loss: 0.705575, acc.: 57.03%] [G loss: 1.008800]\n",
      "epoch:22 step:20784 [D loss: 0.591733, acc.: 67.97%] [G loss: 1.242033]\n",
      "epoch:22 step:20785 [D loss: 0.484990, acc.: 75.00%] [G loss: 1.036097]\n",
      "epoch:22 step:20786 [D loss: 0.805480, acc.: 50.78%] [G loss: 2.336148]\n",
      "epoch:22 step:20787 [D loss: 0.925541, acc.: 43.75%] [G loss: 0.908927]\n",
      "epoch:22 step:20788 [D loss: 0.214196, acc.: 97.66%] [G loss: 1.151526]\n",
      "epoch:22 step:20789 [D loss: 0.479657, acc.: 77.34%] [G loss: 1.496521]\n",
      "epoch:22 step:20790 [D loss: 0.721647, acc.: 54.69%] [G loss: 1.595147]\n",
      "epoch:22 step:20791 [D loss: 0.447816, acc.: 79.69%] [G loss: 1.780125]\n",
      "epoch:22 step:20792 [D loss: 0.298261, acc.: 89.06%] [G loss: 1.888383]\n",
      "epoch:22 step:20793 [D loss: 0.878999, acc.: 48.44%] [G loss: 1.842902]\n",
      "epoch:22 step:20794 [D loss: 0.265643, acc.: 91.41%] [G loss: 1.145835]\n",
      "epoch:22 step:20795 [D loss: 0.779269, acc.: 52.34%] [G loss: 1.708531]\n",
      "epoch:22 step:20796 [D loss: 0.690657, acc.: 60.94%] [G loss: 1.444378]\n",
      "epoch:22 step:20797 [D loss: 0.961595, acc.: 36.72%] [G loss: 1.852224]\n",
      "epoch:22 step:20798 [D loss: 0.278210, acc.: 91.41%] [G loss: 1.701079]\n",
      "epoch:22 step:20799 [D loss: 0.398398, acc.: 89.06%] [G loss: 1.219162]\n",
      "epoch:22 step:20800 [D loss: 0.395632, acc.: 85.94%] [G loss: 0.875395]\n",
      "epoch:22 step:20801 [D loss: 0.380128, acc.: 88.28%] [G loss: 0.682948]\n",
      "epoch:22 step:20802 [D loss: 0.279565, acc.: 92.97%] [G loss: 1.129866]\n",
      "epoch:22 step:20803 [D loss: 0.622713, acc.: 64.06%] [G loss: 1.372187]\n",
      "epoch:22 step:20804 [D loss: 0.434057, acc.: 81.25%] [G loss: 2.524954]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20805 [D loss: 0.544597, acc.: 71.09%] [G loss: 1.677734]\n",
      "epoch:22 step:20806 [D loss: 0.444345, acc.: 82.03%] [G loss: 1.739905]\n",
      "epoch:22 step:20807 [D loss: 0.490218, acc.: 73.44%] [G loss: 1.463452]\n",
      "epoch:22 step:20808 [D loss: 0.454591, acc.: 75.00%] [G loss: 1.695749]\n",
      "epoch:22 step:20809 [D loss: 0.490472, acc.: 75.00%] [G loss: 2.266170]\n",
      "epoch:22 step:20810 [D loss: 0.242805, acc.: 93.75%] [G loss: 1.813598]\n",
      "epoch:22 step:20811 [D loss: 0.279438, acc.: 91.41%] [G loss: 2.368841]\n",
      "epoch:22 step:20812 [D loss: 0.366124, acc.: 82.03%] [G loss: 2.292675]\n",
      "epoch:22 step:20813 [D loss: 0.376083, acc.: 92.97%] [G loss: 1.388874]\n",
      "epoch:22 step:20814 [D loss: 0.629625, acc.: 65.62%] [G loss: 1.353477]\n",
      "epoch:22 step:20815 [D loss: 0.474029, acc.: 81.25%] [G loss: 1.643340]\n",
      "epoch:22 step:20816 [D loss: 0.399763, acc.: 85.94%] [G loss: 0.846014]\n",
      "epoch:22 step:20817 [D loss: 0.642155, acc.: 72.66%] [G loss: 1.506005]\n",
      "epoch:22 step:20818 [D loss: 0.798939, acc.: 50.00%] [G loss: 2.459843]\n",
      "epoch:22 step:20819 [D loss: 0.516044, acc.: 78.91%] [G loss: 2.493954]\n",
      "epoch:22 step:20820 [D loss: 0.474999, acc.: 82.81%] [G loss: 1.404921]\n",
      "epoch:22 step:20821 [D loss: 0.924220, acc.: 39.06%] [G loss: 2.293388]\n",
      "epoch:22 step:20822 [D loss: 1.003002, acc.: 36.72%] [G loss: 0.658922]\n",
      "epoch:22 step:20823 [D loss: 0.421021, acc.: 83.59%] [G loss: 0.739657]\n",
      "epoch:22 step:20824 [D loss: 0.170993, acc.: 94.53%] [G loss: 1.346626]\n",
      "epoch:22 step:20825 [D loss: 0.386087, acc.: 89.06%] [G loss: 1.918096]\n",
      "epoch:22 step:20826 [D loss: 0.466141, acc.: 75.00%] [G loss: 0.731360]\n",
      "epoch:22 step:20827 [D loss: 0.802128, acc.: 53.12%] [G loss: 1.242607]\n",
      "epoch:22 step:20828 [D loss: 0.354104, acc.: 87.50%] [G loss: 1.218455]\n",
      "epoch:22 step:20829 [D loss: 0.420387, acc.: 78.12%] [G loss: 1.158347]\n",
      "epoch:22 step:20830 [D loss: 0.435168, acc.: 84.38%] [G loss: 1.288487]\n",
      "epoch:22 step:20831 [D loss: 0.508611, acc.: 72.66%] [G loss: 1.735629]\n",
      "epoch:22 step:20832 [D loss: 0.652100, acc.: 64.06%] [G loss: 2.072486]\n",
      "epoch:22 step:20833 [D loss: 0.445393, acc.: 84.38%] [G loss: 1.861775]\n",
      "epoch:22 step:20834 [D loss: 0.900299, acc.: 52.34%] [G loss: 1.284851]\n",
      "epoch:22 step:20835 [D loss: 0.491008, acc.: 71.09%] [G loss: 1.621181]\n",
      "epoch:22 step:20836 [D loss: 0.417279, acc.: 84.38%] [G loss: 0.990981]\n",
      "epoch:22 step:20837 [D loss: 0.621175, acc.: 64.06%] [G loss: 1.557975]\n",
      "epoch:22 step:20838 [D loss: 0.320031, acc.: 91.41%] [G loss: 2.093471]\n",
      "epoch:22 step:20839 [D loss: 0.468871, acc.: 74.22%] [G loss: 1.845800]\n",
      "epoch:22 step:20840 [D loss: 0.564951, acc.: 70.31%] [G loss: 1.909148]\n",
      "epoch:22 step:20841 [D loss: 0.254343, acc.: 95.31%] [G loss: 1.416712]\n",
      "epoch:22 step:20842 [D loss: 0.551331, acc.: 72.66%] [G loss: 1.625927]\n",
      "epoch:22 step:20843 [D loss: 0.651500, acc.: 61.72%] [G loss: 1.073063]\n",
      "epoch:22 step:20844 [D loss: 0.319254, acc.: 92.97%] [G loss: 1.173501]\n",
      "epoch:22 step:20845 [D loss: 0.215533, acc.: 96.88%] [G loss: 1.409679]\n",
      "epoch:22 step:20846 [D loss: 0.345182, acc.: 88.28%] [G loss: 1.743837]\n",
      "epoch:22 step:20847 [D loss: 0.364934, acc.: 90.62%] [G loss: 1.185206]\n",
      "epoch:22 step:20848 [D loss: 0.271386, acc.: 96.88%] [G loss: 0.469931]\n",
      "epoch:22 step:20849 [D loss: 0.432073, acc.: 84.38%] [G loss: 1.584185]\n",
      "epoch:22 step:20850 [D loss: 0.263633, acc.: 92.97%] [G loss: 2.730792]\n",
      "epoch:22 step:20851 [D loss: 0.362236, acc.: 88.28%] [G loss: 3.249581]\n",
      "epoch:22 step:20852 [D loss: 0.648283, acc.: 68.75%] [G loss: 3.192406]\n",
      "epoch:22 step:20853 [D loss: 0.439933, acc.: 79.69%] [G loss: 1.176075]\n",
      "epoch:22 step:20854 [D loss: 0.297540, acc.: 91.41%] [G loss: 1.924282]\n",
      "epoch:22 step:20855 [D loss: 0.400967, acc.: 84.38%] [G loss: 1.285194]\n",
      "epoch:22 step:20856 [D loss: 0.481193, acc.: 79.69%] [G loss: 2.169285]\n",
      "epoch:22 step:20857 [D loss: 0.326143, acc.: 85.94%] [G loss: 0.530261]\n",
      "epoch:22 step:20858 [D loss: 0.184516, acc.: 98.44%] [G loss: 4.088689]\n",
      "epoch:22 step:20859 [D loss: 0.198845, acc.: 95.31%] [G loss: 3.239257]\n",
      "epoch:22 step:20860 [D loss: 0.672335, acc.: 54.69%] [G loss: 0.755222]\n",
      "epoch:22 step:20861 [D loss: 0.733121, acc.: 58.59%] [G loss: 0.762166]\n",
      "epoch:22 step:20862 [D loss: 0.599493, acc.: 65.62%] [G loss: 1.289711]\n",
      "epoch:22 step:20863 [D loss: 0.428406, acc.: 79.69%] [G loss: 2.134472]\n",
      "epoch:22 step:20864 [D loss: 0.478139, acc.: 78.12%] [G loss: 1.547001]\n",
      "epoch:22 step:20865 [D loss: 0.439183, acc.: 86.72%] [G loss: 0.763100]\n",
      "epoch:22 step:20866 [D loss: 0.398383, acc.: 82.03%] [G loss: 3.005007]\n",
      "epoch:22 step:20867 [D loss: 0.638017, acc.: 73.44%] [G loss: 2.073418]\n",
      "epoch:22 step:20868 [D loss: 0.460802, acc.: 78.12%] [G loss: 2.538258]\n",
      "epoch:22 step:20869 [D loss: 0.319991, acc.: 92.19%] [G loss: 2.256284]\n",
      "epoch:22 step:20870 [D loss: 0.942640, acc.: 35.16%] [G loss: 1.114261]\n",
      "epoch:22 step:20871 [D loss: 0.397823, acc.: 82.81%] [G loss: 0.812323]\n",
      "epoch:22 step:20872 [D loss: 0.388888, acc.: 80.47%] [G loss: 0.474296]\n",
      "epoch:22 step:20873 [D loss: 1.077238, acc.: 51.56%] [G loss: 2.573755]\n",
      "epoch:22 step:20874 [D loss: 0.219664, acc.: 92.19%] [G loss: 1.527052]\n",
      "epoch:22 step:20875 [D loss: 0.434631, acc.: 80.47%] [G loss: 0.706347]\n",
      "epoch:22 step:20876 [D loss: 0.447119, acc.: 78.12%] [G loss: 3.286677]\n",
      "epoch:22 step:20877 [D loss: 0.168082, acc.: 97.66%] [G loss: 1.124388]\n",
      "epoch:22 step:20878 [D loss: 0.283564, acc.: 91.41%] [G loss: 1.256362]\n",
      "epoch:22 step:20879 [D loss: 0.458728, acc.: 75.78%] [G loss: 1.587129]\n",
      "epoch:22 step:20880 [D loss: 0.537189, acc.: 71.88%] [G loss: 0.788041]\n",
      "epoch:22 step:20881 [D loss: 0.764328, acc.: 57.03%] [G loss: 3.472666]\n",
      "epoch:22 step:20882 [D loss: 1.095334, acc.: 29.69%] [G loss: 2.203217]\n",
      "epoch:22 step:20883 [D loss: 0.677297, acc.: 64.84%] [G loss: 1.094269]\n",
      "epoch:22 step:20884 [D loss: 0.195221, acc.: 94.53%] [G loss: 1.209325]\n",
      "epoch:22 step:20885 [D loss: 0.202572, acc.: 96.88%] [G loss: 1.510043]\n",
      "epoch:22 step:20886 [D loss: 0.846987, acc.: 57.81%] [G loss: 0.869285]\n",
      "epoch:22 step:20887 [D loss: 0.646307, acc.: 63.28%] [G loss: 1.434239]\n",
      "epoch:22 step:20888 [D loss: 0.342509, acc.: 85.94%] [G loss: 1.731951]\n",
      "epoch:22 step:20889 [D loss: 0.480797, acc.: 72.66%] [G loss: 2.074697]\n",
      "epoch:22 step:20890 [D loss: 0.576713, acc.: 74.22%] [G loss: 1.571677]\n",
      "epoch:22 step:20891 [D loss: 0.504665, acc.: 74.22%] [G loss: 2.001145]\n",
      "epoch:22 step:20892 [D loss: 0.559810, acc.: 62.50%] [G loss: 0.668845]\n",
      "epoch:22 step:20893 [D loss: 0.434297, acc.: 78.12%] [G loss: 1.425564]\n",
      "epoch:22 step:20894 [D loss: 0.452749, acc.: 75.78%] [G loss: 1.890632]\n",
      "epoch:22 step:20895 [D loss: 0.131557, acc.: 97.66%] [G loss: 1.516201]\n",
      "epoch:22 step:20896 [D loss: 0.661242, acc.: 60.94%] [G loss: 0.987720]\n",
      "epoch:22 step:20897 [D loss: 0.941342, acc.: 51.56%] [G loss: 1.134833]\n",
      "epoch:22 step:20898 [D loss: 0.759216, acc.: 55.47%] [G loss: 1.049042]\n",
      "epoch:22 step:20899 [D loss: 0.491524, acc.: 75.00%] [G loss: 1.299042]\n",
      "epoch:22 step:20900 [D loss: 0.295349, acc.: 96.09%] [G loss: 1.703791]\n",
      "epoch:22 step:20901 [D loss: 0.441104, acc.: 82.03%] [G loss: 1.276560]\n",
      "epoch:22 step:20902 [D loss: 0.762265, acc.: 56.25%] [G loss: 1.932860]\n",
      "epoch:22 step:20903 [D loss: 0.289017, acc.: 93.75%] [G loss: 1.186041]\n",
      "epoch:22 step:20904 [D loss: 0.290191, acc.: 92.19%] [G loss: 1.080901]\n",
      "epoch:22 step:20905 [D loss: 1.035667, acc.: 28.12%] [G loss: 1.138497]\n",
      "epoch:22 step:20906 [D loss: 0.622095, acc.: 67.19%] [G loss: 0.872054]\n",
      "epoch:22 step:20907 [D loss: 0.732709, acc.: 58.59%] [G loss: 1.202247]\n",
      "epoch:22 step:20908 [D loss: 0.601319, acc.: 63.28%] [G loss: 1.033491]\n",
      "epoch:22 step:20909 [D loss: 0.379415, acc.: 83.59%] [G loss: 1.569759]\n",
      "epoch:22 step:20910 [D loss: 0.441550, acc.: 82.81%] [G loss: 1.520633]\n",
      "epoch:22 step:20911 [D loss: 0.773749, acc.: 48.44%] [G loss: 1.560950]\n",
      "epoch:22 step:20912 [D loss: 0.420524, acc.: 82.03%] [G loss: 1.834856]\n",
      "epoch:22 step:20913 [D loss: 0.302788, acc.: 94.53%] [G loss: 0.848872]\n",
      "epoch:22 step:20914 [D loss: 0.650875, acc.: 62.50%] [G loss: 2.015804]\n",
      "epoch:22 step:20915 [D loss: 0.164710, acc.: 98.44%] [G loss: 1.907498]\n",
      "epoch:22 step:20916 [D loss: 0.665947, acc.: 60.16%] [G loss: 2.663465]\n",
      "epoch:22 step:20917 [D loss: 0.597825, acc.: 64.06%] [G loss: 2.393555]\n",
      "epoch:22 step:20918 [D loss: 0.623870, acc.: 60.16%] [G loss: 1.244720]\n",
      "epoch:22 step:20919 [D loss: 0.462951, acc.: 75.78%] [G loss: 1.139425]\n",
      "epoch:22 step:20920 [D loss: 0.628464, acc.: 62.50%] [G loss: 1.166866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20921 [D loss: 0.359645, acc.: 85.94%] [G loss: 1.193897]\n",
      "epoch:22 step:20922 [D loss: 0.236617, acc.: 92.19%] [G loss: 2.165111]\n",
      "epoch:22 step:20923 [D loss: 0.414219, acc.: 85.16%] [G loss: 1.397882]\n",
      "epoch:22 step:20924 [D loss: 0.916674, acc.: 51.56%] [G loss: 1.261487]\n",
      "epoch:22 step:20925 [D loss: 0.399748, acc.: 90.62%] [G loss: 2.964384]\n",
      "epoch:22 step:20926 [D loss: 0.335995, acc.: 83.59%] [G loss: 2.148252]\n",
      "epoch:22 step:20927 [D loss: 0.532938, acc.: 72.66%] [G loss: 2.031481]\n",
      "epoch:22 step:20928 [D loss: 0.429495, acc.: 75.78%] [G loss: 2.379717]\n",
      "epoch:22 step:20929 [D loss: 0.289428, acc.: 91.41%] [G loss: 1.921347]\n",
      "epoch:22 step:20930 [D loss: 0.532375, acc.: 73.44%] [G loss: 1.143056]\n",
      "epoch:22 step:20931 [D loss: 0.678427, acc.: 56.25%] [G loss: 1.496201]\n",
      "epoch:22 step:20932 [D loss: 0.203542, acc.: 96.09%] [G loss: 2.716468]\n",
      "epoch:22 step:20933 [D loss: 0.347106, acc.: 88.28%] [G loss: 1.368627]\n",
      "epoch:22 step:20934 [D loss: 0.161824, acc.: 98.44%] [G loss: 1.684800]\n",
      "epoch:22 step:20935 [D loss: 0.846496, acc.: 58.59%] [G loss: 2.482690]\n",
      "epoch:22 step:20936 [D loss: 0.426390, acc.: 79.69%] [G loss: 1.897994]\n",
      "epoch:22 step:20937 [D loss: 0.621424, acc.: 67.19%] [G loss: 2.123957]\n",
      "epoch:22 step:20938 [D loss: 0.238919, acc.: 95.31%] [G loss: 2.297899]\n",
      "epoch:22 step:20939 [D loss: 0.298531, acc.: 91.41%] [G loss: 2.152343]\n",
      "epoch:22 step:20940 [D loss: 0.578286, acc.: 67.97%] [G loss: 1.892532]\n",
      "epoch:22 step:20941 [D loss: 0.413358, acc.: 84.38%] [G loss: 1.725893]\n",
      "epoch:22 step:20942 [D loss: 0.401128, acc.: 75.00%] [G loss: 1.614509]\n",
      "epoch:22 step:20943 [D loss: 0.420919, acc.: 77.34%] [G loss: 1.227240]\n",
      "epoch:22 step:20944 [D loss: 0.549011, acc.: 67.97%] [G loss: 1.761688]\n",
      "epoch:22 step:20945 [D loss: 0.565698, acc.: 70.31%] [G loss: 1.743365]\n",
      "epoch:22 step:20946 [D loss: 0.545861, acc.: 72.66%] [G loss: 1.978940]\n",
      "epoch:22 step:20947 [D loss: 0.714587, acc.: 55.47%] [G loss: 1.832247]\n",
      "epoch:22 step:20948 [D loss: 0.405691, acc.: 82.81%] [G loss: 1.736989]\n",
      "epoch:22 step:20949 [D loss: 0.623925, acc.: 59.38%] [G loss: 1.433805]\n",
      "epoch:22 step:20950 [D loss: 0.513138, acc.: 72.66%] [G loss: 2.487518]\n",
      "epoch:22 step:20951 [D loss: 0.444471, acc.: 82.03%] [G loss: 1.589621]\n",
      "epoch:22 step:20952 [D loss: 0.350260, acc.: 86.72%] [G loss: 2.183130]\n",
      "epoch:22 step:20953 [D loss: 0.435554, acc.: 85.16%] [G loss: 2.226321]\n",
      "epoch:22 step:20954 [D loss: 0.841798, acc.: 58.59%] [G loss: 1.422658]\n",
      "epoch:22 step:20955 [D loss: 0.446061, acc.: 74.22%] [G loss: 2.201768]\n",
      "epoch:22 step:20956 [D loss: 0.566511, acc.: 76.56%] [G loss: 0.984210]\n",
      "epoch:22 step:20957 [D loss: 0.225981, acc.: 94.53%] [G loss: 2.666241]\n",
      "epoch:22 step:20958 [D loss: 0.264715, acc.: 92.97%] [G loss: 2.277785]\n",
      "epoch:22 step:20959 [D loss: 0.396676, acc.: 91.41%] [G loss: 1.676837]\n",
      "epoch:22 step:20960 [D loss: 0.374109, acc.: 86.72%] [G loss: 1.574695]\n",
      "epoch:22 step:20961 [D loss: 0.233211, acc.: 92.19%] [G loss: 1.824404]\n",
      "epoch:22 step:20962 [D loss: 0.490772, acc.: 73.44%] [G loss: 1.512704]\n",
      "epoch:22 step:20963 [D loss: 0.513068, acc.: 75.00%] [G loss: 2.188267]\n",
      "epoch:22 step:20964 [D loss: 0.532512, acc.: 68.75%] [G loss: 2.218313]\n",
      "epoch:22 step:20965 [D loss: 0.843510, acc.: 45.31%] [G loss: 2.189375]\n",
      "epoch:22 step:20966 [D loss: 0.904182, acc.: 42.19%] [G loss: 1.300022]\n",
      "epoch:22 step:20967 [D loss: 0.423175, acc.: 79.69%] [G loss: 1.399283]\n",
      "epoch:22 step:20968 [D loss: 0.433670, acc.: 82.81%] [G loss: 2.708162]\n",
      "epoch:22 step:20969 [D loss: 0.322002, acc.: 85.94%] [G loss: 1.548696]\n",
      "epoch:22 step:20970 [D loss: 0.461444, acc.: 78.91%] [G loss: 1.663656]\n",
      "epoch:22 step:20971 [D loss: 0.341781, acc.: 89.84%] [G loss: 0.851470]\n",
      "epoch:22 step:20972 [D loss: 0.412460, acc.: 85.16%] [G loss: 0.686207]\n",
      "epoch:22 step:20973 [D loss: 0.407155, acc.: 82.03%] [G loss: 1.008394]\n",
      "epoch:22 step:20974 [D loss: 0.171494, acc.: 96.09%] [G loss: 1.742217]\n",
      "epoch:22 step:20975 [D loss: 0.313355, acc.: 92.19%] [G loss: 1.133305]\n",
      "epoch:22 step:20976 [D loss: 0.317334, acc.: 89.06%] [G loss: 2.352550]\n",
      "epoch:22 step:20977 [D loss: 0.352263, acc.: 86.72%] [G loss: 1.999091]\n",
      "epoch:22 step:20978 [D loss: 0.498252, acc.: 75.78%] [G loss: 2.036562]\n",
      "epoch:22 step:20979 [D loss: 0.572180, acc.: 68.75%] [G loss: 1.764569]\n",
      "epoch:22 step:20980 [D loss: 0.229753, acc.: 95.31%] [G loss: 1.758255]\n",
      "epoch:22 step:20981 [D loss: 0.190772, acc.: 96.88%] [G loss: 1.949058]\n",
      "epoch:22 step:20982 [D loss: 0.368023, acc.: 84.38%] [G loss: 2.093819]\n",
      "epoch:22 step:20983 [D loss: 0.578080, acc.: 68.75%] [G loss: 1.612946]\n",
      "epoch:22 step:20984 [D loss: 0.292857, acc.: 93.75%] [G loss: 2.040759]\n",
      "epoch:22 step:20985 [D loss: 0.534540, acc.: 68.75%] [G loss: 3.776391]\n",
      "epoch:22 step:20986 [D loss: 0.204617, acc.: 96.88%] [G loss: 2.571964]\n",
      "epoch:22 step:20987 [D loss: 0.289368, acc.: 93.75%] [G loss: 3.092946]\n",
      "epoch:22 step:20988 [D loss: 0.593729, acc.: 63.28%] [G loss: 2.554371]\n",
      "epoch:22 step:20989 [D loss: 0.400256, acc.: 82.03%] [G loss: 3.481736]\n",
      "epoch:22 step:20990 [D loss: 0.974528, acc.: 35.94%] [G loss: 1.299403]\n",
      "epoch:22 step:20991 [D loss: 0.252085, acc.: 90.62%] [G loss: 1.046280]\n",
      "epoch:22 step:20992 [D loss: 0.574960, acc.: 65.62%] [G loss: 1.371108]\n",
      "epoch:22 step:20993 [D loss: 0.545660, acc.: 76.56%] [G loss: 3.567439]\n",
      "epoch:22 step:20994 [D loss: 0.563121, acc.: 68.75%] [G loss: 2.385782]\n",
      "epoch:22 step:20995 [D loss: 0.312067, acc.: 89.84%] [G loss: 2.725308]\n",
      "epoch:22 step:20996 [D loss: 0.324846, acc.: 92.97%] [G loss: 1.395304]\n",
      "epoch:22 step:20997 [D loss: 0.687045, acc.: 60.94%] [G loss: 2.546628]\n",
      "epoch:22 step:20998 [D loss: 0.859079, acc.: 55.47%] [G loss: 1.558179]\n",
      "epoch:22 step:20999 [D loss: 0.886886, acc.: 50.78%] [G loss: 1.104957]\n",
      "epoch:22 step:21000 [D loss: 0.382669, acc.: 87.50%] [G loss: 1.169214]\n",
      "epoch:22 step:21001 [D loss: 0.648940, acc.: 64.06%] [G loss: 2.151891]\n",
      "epoch:22 step:21002 [D loss: 0.241831, acc.: 96.09%] [G loss: 2.069115]\n",
      "epoch:22 step:21003 [D loss: 0.136786, acc.: 98.44%] [G loss: 2.643107]\n",
      "epoch:22 step:21004 [D loss: 0.392768, acc.: 85.94%] [G loss: 0.782227]\n",
      "epoch:22 step:21005 [D loss: 0.473263, acc.: 76.56%] [G loss: 1.410467]\n",
      "epoch:22 step:21006 [D loss: 0.698040, acc.: 64.84%] [G loss: 1.582155]\n",
      "epoch:22 step:21007 [D loss: 0.529423, acc.: 65.62%] [G loss: 3.149657]\n",
      "epoch:22 step:21008 [D loss: 0.683164, acc.: 60.16%] [G loss: 3.430234]\n",
      "epoch:22 step:21009 [D loss: 0.751560, acc.: 62.50%] [G loss: 3.239392]\n",
      "epoch:22 step:21010 [D loss: 0.443992, acc.: 82.03%] [G loss: 2.661945]\n",
      "epoch:22 step:21011 [D loss: 0.988340, acc.: 44.53%] [G loss: 2.563115]\n",
      "epoch:22 step:21012 [D loss: 0.365443, acc.: 85.16%] [G loss: 1.672006]\n",
      "epoch:22 step:21013 [D loss: 0.273894, acc.: 89.06%] [G loss: 1.919955]\n",
      "epoch:22 step:21014 [D loss: 0.362565, acc.: 78.12%] [G loss: 1.996576]\n",
      "epoch:22 step:21015 [D loss: 0.252762, acc.: 94.53%] [G loss: 0.814897]\n",
      "epoch:22 step:21016 [D loss: 0.407984, acc.: 84.38%] [G loss: 0.963736]\n",
      "epoch:22 step:21017 [D loss: 0.330135, acc.: 82.03%] [G loss: 2.138842]\n",
      "epoch:22 step:21018 [D loss: 0.793472, acc.: 50.78%] [G loss: 2.627226]\n",
      "epoch:22 step:21019 [D loss: 0.308602, acc.: 90.62%] [G loss: 1.136835]\n",
      "epoch:22 step:21020 [D loss: 0.487305, acc.: 83.59%] [G loss: 1.403326]\n",
      "epoch:22 step:21021 [D loss: 0.236893, acc.: 95.31%] [G loss: 1.282754]\n",
      "epoch:22 step:21022 [D loss: 0.625791, acc.: 63.28%] [G loss: 1.482761]\n",
      "epoch:22 step:21023 [D loss: 0.358763, acc.: 84.38%] [G loss: 2.065386]\n",
      "epoch:22 step:21024 [D loss: 0.188685, acc.: 98.44%] [G loss: 2.408160]\n",
      "epoch:22 step:21025 [D loss: 0.286837, acc.: 94.53%] [G loss: 0.711407]\n",
      "epoch:22 step:21026 [D loss: 0.451316, acc.: 82.03%] [G loss: 2.206131]\n",
      "epoch:22 step:21027 [D loss: 0.801891, acc.: 56.25%] [G loss: 0.894638]\n",
      "epoch:22 step:21028 [D loss: 0.422371, acc.: 88.28%] [G loss: 0.650046]\n",
      "epoch:22 step:21029 [D loss: 0.329048, acc.: 88.28%] [G loss: 1.274920]\n",
      "epoch:22 step:21030 [D loss: 1.673083, acc.: 11.72%] [G loss: 1.102991]\n",
      "epoch:22 step:21031 [D loss: 0.768227, acc.: 48.44%] [G loss: 0.977110]\n",
      "epoch:22 step:21032 [D loss: 0.331993, acc.: 86.72%] [G loss: 2.175295]\n",
      "epoch:22 step:21033 [D loss: 0.460551, acc.: 78.12%] [G loss: 1.841108]\n",
      "epoch:22 step:21034 [D loss: 0.637058, acc.: 62.50%] [G loss: 1.501169]\n",
      "epoch:22 step:21035 [D loss: 0.559887, acc.: 72.66%] [G loss: 1.396689]\n",
      "epoch:22 step:21036 [D loss: 0.281952, acc.: 88.28%] [G loss: 0.961923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21037 [D loss: 0.735939, acc.: 60.94%] [G loss: 1.819676]\n",
      "epoch:22 step:21038 [D loss: 0.496195, acc.: 78.12%] [G loss: 1.409758]\n",
      "epoch:22 step:21039 [D loss: 0.820596, acc.: 51.56%] [G loss: 2.275754]\n",
      "epoch:22 step:21040 [D loss: 0.389320, acc.: 86.72%] [G loss: 1.842960]\n",
      "epoch:22 step:21041 [D loss: 0.407074, acc.: 86.72%] [G loss: 2.473523]\n",
      "epoch:22 step:21042 [D loss: 0.717018, acc.: 56.25%] [G loss: 1.855468]\n",
      "epoch:22 step:21043 [D loss: 0.456330, acc.: 79.69%] [G loss: 1.819643]\n",
      "epoch:22 step:21044 [D loss: 0.580860, acc.: 62.50%] [G loss: 3.503693]\n",
      "epoch:22 step:21045 [D loss: 0.447476, acc.: 71.09%] [G loss: 1.139504]\n",
      "epoch:22 step:21046 [D loss: 1.450996, acc.: 14.84%] [G loss: 2.022159]\n",
      "epoch:22 step:21047 [D loss: 0.974157, acc.: 35.16%] [G loss: 1.115473]\n",
      "epoch:22 step:21048 [D loss: 1.010544, acc.: 36.72%] [G loss: 2.154506]\n",
      "epoch:22 step:21049 [D loss: 0.548361, acc.: 68.75%] [G loss: 1.327421]\n",
      "epoch:22 step:21050 [D loss: 0.467131, acc.: 77.34%] [G loss: 1.647413]\n",
      "epoch:22 step:21051 [D loss: 0.585300, acc.: 69.53%] [G loss: 1.654431]\n",
      "epoch:22 step:21052 [D loss: 0.301088, acc.: 92.97%] [G loss: 1.598337]\n",
      "epoch:22 step:21053 [D loss: 0.215106, acc.: 98.44%] [G loss: 2.053526]\n",
      "epoch:22 step:21054 [D loss: 0.306089, acc.: 93.75%] [G loss: 1.178406]\n",
      "epoch:22 step:21055 [D loss: 0.372132, acc.: 82.81%] [G loss: 1.669393]\n",
      "epoch:22 step:21056 [D loss: 0.386763, acc.: 89.06%] [G loss: 1.215075]\n",
      "epoch:22 step:21057 [D loss: 0.242560, acc.: 96.09%] [G loss: 1.505547]\n",
      "epoch:22 step:21058 [D loss: 0.551270, acc.: 72.66%] [G loss: 1.918489]\n",
      "epoch:22 step:21059 [D loss: 0.663699, acc.: 64.06%] [G loss: 2.832029]\n",
      "epoch:22 step:21060 [D loss: 0.302995, acc.: 85.16%] [G loss: 2.715561]\n",
      "epoch:22 step:21061 [D loss: 1.287752, acc.: 21.88%] [G loss: 1.037622]\n",
      "epoch:22 step:21062 [D loss: 0.533124, acc.: 71.88%] [G loss: 1.763040]\n",
      "epoch:22 step:21063 [D loss: 0.364011, acc.: 88.28%] [G loss: 1.887758]\n",
      "epoch:22 step:21064 [D loss: 0.339660, acc.: 89.06%] [G loss: 1.941491]\n",
      "epoch:22 step:21065 [D loss: 0.363547, acc.: 84.38%] [G loss: 2.537381]\n",
      "epoch:22 step:21066 [D loss: 0.581548, acc.: 72.66%] [G loss: 1.134888]\n",
      "epoch:22 step:21067 [D loss: 0.263444, acc.: 92.97%] [G loss: 1.869925]\n",
      "epoch:22 step:21068 [D loss: 0.579257, acc.: 69.53%] [G loss: 1.955724]\n",
      "epoch:22 step:21069 [D loss: 0.567726, acc.: 71.09%] [G loss: 2.155674]\n",
      "epoch:22 step:21070 [D loss: 0.262915, acc.: 94.53%] [G loss: 4.725109]\n",
      "epoch:22 step:21071 [D loss: 0.589440, acc.: 67.97%] [G loss: 1.323965]\n",
      "epoch:22 step:21072 [D loss: 0.278842, acc.: 98.44%] [G loss: 2.522034]\n",
      "epoch:22 step:21073 [D loss: 1.152543, acc.: 35.16%] [G loss: 0.667535]\n",
      "epoch:22 step:21074 [D loss: 0.127872, acc.: 100.00%] [G loss: 2.560205]\n",
      "epoch:22 step:21075 [D loss: 1.027045, acc.: 49.22%] [G loss: 1.476279]\n",
      "epoch:22 step:21076 [D loss: 0.533706, acc.: 71.88%] [G loss: 2.646232]\n",
      "epoch:22 step:21077 [D loss: 0.447676, acc.: 80.47%] [G loss: 1.644281]\n",
      "epoch:22 step:21078 [D loss: 0.409753, acc.: 89.06%] [G loss: 1.620802]\n",
      "epoch:22 step:21079 [D loss: 0.223065, acc.: 95.31%] [G loss: 2.688769]\n",
      "epoch:22 step:21080 [D loss: 0.569591, acc.: 64.06%] [G loss: 1.377797]\n",
      "epoch:22 step:21081 [D loss: 0.284808, acc.: 96.09%] [G loss: 2.495336]\n",
      "epoch:22 step:21082 [D loss: 0.264380, acc.: 93.75%] [G loss: 2.127493]\n",
      "epoch:22 step:21083 [D loss: 0.699337, acc.: 63.28%] [G loss: 1.233835]\n",
      "epoch:22 step:21084 [D loss: 0.441548, acc.: 79.69%] [G loss: 0.864619]\n",
      "epoch:22 step:21085 [D loss: 0.777896, acc.: 49.22%] [G loss: 1.228624]\n",
      "epoch:22 step:21086 [D loss: 0.944241, acc.: 38.28%] [G loss: 1.556448]\n",
      "epoch:22 step:21087 [D loss: 0.364319, acc.: 87.50%] [G loss: 1.649031]\n",
      "epoch:22 step:21088 [D loss: 0.496795, acc.: 76.56%] [G loss: 2.309076]\n",
      "epoch:22 step:21089 [D loss: 0.315524, acc.: 91.41%] [G loss: 2.521629]\n",
      "epoch:22 step:21090 [D loss: 0.578514, acc.: 69.53%] [G loss: 2.189351]\n",
      "epoch:22 step:21091 [D loss: 0.365257, acc.: 86.72%] [G loss: 1.806133]\n",
      "epoch:22 step:21092 [D loss: 0.301891, acc.: 92.19%] [G loss: 1.894519]\n",
      "epoch:22 step:21093 [D loss: 0.679584, acc.: 60.94%] [G loss: 1.146899]\n",
      "epoch:22 step:21094 [D loss: 0.599035, acc.: 64.84%] [G loss: 1.737579]\n",
      "epoch:22 step:21095 [D loss: 0.541838, acc.: 70.31%] [G loss: 0.573032]\n",
      "epoch:22 step:21096 [D loss: 0.642816, acc.: 62.50%] [G loss: 1.859120]\n",
      "epoch:22 step:21097 [D loss: 0.669020, acc.: 57.81%] [G loss: 1.667927]\n",
      "epoch:22 step:21098 [D loss: 0.328258, acc.: 86.72%] [G loss: 2.162565]\n",
      "epoch:22 step:21099 [D loss: 0.169131, acc.: 98.44%] [G loss: 3.523435]\n",
      "epoch:22 step:21100 [D loss: 0.275323, acc.: 92.19%] [G loss: 3.143897]\n",
      "epoch:22 step:21101 [D loss: 0.337195, acc.: 90.62%] [G loss: 2.271802]\n",
      "epoch:22 step:21102 [D loss: 0.450242, acc.: 84.38%] [G loss: 1.826606]\n",
      "epoch:22 step:21103 [D loss: 0.453569, acc.: 71.88%] [G loss: 1.074703]\n",
      "epoch:22 step:21104 [D loss: 0.448932, acc.: 85.16%] [G loss: 2.269178]\n",
      "epoch:22 step:21105 [D loss: 0.331195, acc.: 90.62%] [G loss: 2.131500]\n",
      "epoch:22 step:21106 [D loss: 0.748278, acc.: 57.03%] [G loss: 3.485061]\n",
      "epoch:22 step:21107 [D loss: 0.610784, acc.: 67.19%] [G loss: 1.262198]\n",
      "epoch:22 step:21108 [D loss: 0.529223, acc.: 75.78%] [G loss: 3.033077]\n",
      "epoch:22 step:21109 [D loss: 0.707259, acc.: 59.38%] [G loss: 1.167636]\n",
      "epoch:22 step:21110 [D loss: 0.199725, acc.: 96.88%] [G loss: 1.789402]\n",
      "epoch:22 step:21111 [D loss: 1.332483, acc.: 35.16%] [G loss: 2.194929]\n",
      "epoch:22 step:21112 [D loss: 0.351139, acc.: 83.59%] [G loss: 1.068016]\n",
      "epoch:22 step:21113 [D loss: 0.171879, acc.: 96.09%] [G loss: 1.105081]\n",
      "epoch:22 step:21114 [D loss: 0.590570, acc.: 70.31%] [G loss: 1.353635]\n",
      "epoch:22 step:21115 [D loss: 0.719053, acc.: 57.81%] [G loss: 0.568272]\n",
      "epoch:22 step:21116 [D loss: 0.164600, acc.: 97.66%] [G loss: 1.733000]\n",
      "epoch:22 step:21117 [D loss: 0.785552, acc.: 52.34%] [G loss: 0.559939]\n",
      "epoch:22 step:21118 [D loss: 1.057182, acc.: 29.69%] [G loss: 1.503327]\n",
      "epoch:22 step:21119 [D loss: 0.242078, acc.: 96.88%] [G loss: 2.752680]\n",
      "epoch:22 step:21120 [D loss: 0.405161, acc.: 81.25%] [G loss: 1.343492]\n",
      "epoch:22 step:21121 [D loss: 0.704912, acc.: 55.47%] [G loss: 2.102209]\n",
      "epoch:22 step:21122 [D loss: 0.211404, acc.: 96.88%] [G loss: 2.106041]\n",
      "epoch:22 step:21123 [D loss: 0.125908, acc.: 97.66%] [G loss: 2.439111]\n",
      "epoch:22 step:21124 [D loss: 0.504670, acc.: 73.44%] [G loss: 2.245207]\n",
      "epoch:22 step:21125 [D loss: 0.259658, acc.: 88.28%] [G loss: 0.891456]\n",
      "epoch:22 step:21126 [D loss: 0.233946, acc.: 92.19%] [G loss: 1.866906]\n",
      "epoch:22 step:21127 [D loss: 0.535781, acc.: 76.56%] [G loss: 1.029460]\n",
      "epoch:22 step:21128 [D loss: 0.547674, acc.: 75.00%] [G loss: 1.877934]\n",
      "epoch:22 step:21129 [D loss: 0.419888, acc.: 80.47%] [G loss: 0.904071]\n",
      "epoch:22 step:21130 [D loss: 0.350618, acc.: 86.72%] [G loss: 3.335902]\n",
      "epoch:22 step:21131 [D loss: 0.479225, acc.: 74.22%] [G loss: 1.697478]\n",
      "epoch:22 step:21132 [D loss: 0.340979, acc.: 86.72%] [G loss: 1.694973]\n",
      "epoch:22 step:21133 [D loss: 0.418821, acc.: 85.94%] [G loss: 2.054896]\n",
      "epoch:22 step:21134 [D loss: 0.616725, acc.: 64.06%] [G loss: 0.906956]\n",
      "epoch:22 step:21135 [D loss: 0.368977, acc.: 82.03%] [G loss: 1.595066]\n",
      "epoch:22 step:21136 [D loss: 0.229437, acc.: 93.75%] [G loss: 1.952951]\n",
      "epoch:22 step:21137 [D loss: 0.541637, acc.: 71.09%] [G loss: 1.619917]\n",
      "epoch:22 step:21138 [D loss: 0.481073, acc.: 76.56%] [G loss: 1.080263]\n",
      "epoch:22 step:21139 [D loss: 0.501221, acc.: 77.34%] [G loss: 1.564615]\n",
      "epoch:22 step:21140 [D loss: 0.425393, acc.: 84.38%] [G loss: 1.908168]\n",
      "epoch:22 step:21141 [D loss: 0.568560, acc.: 65.62%] [G loss: 1.355801]\n",
      "epoch:22 step:21142 [D loss: 0.669818, acc.: 67.19%] [G loss: 1.639855]\n",
      "epoch:22 step:21143 [D loss: 0.305476, acc.: 94.53%] [G loss: 1.978717]\n",
      "epoch:22 step:21144 [D loss: 0.363336, acc.: 88.28%] [G loss: 0.646154]\n",
      "epoch:22 step:21145 [D loss: 0.568942, acc.: 68.75%] [G loss: 1.804167]\n",
      "epoch:22 step:21146 [D loss: 0.219904, acc.: 94.53%] [G loss: 0.705576]\n",
      "epoch:22 step:21147 [D loss: 0.481415, acc.: 78.91%] [G loss: 3.216791]\n",
      "epoch:22 step:21148 [D loss: 0.169905, acc.: 98.44%] [G loss: 0.699410]\n",
      "epoch:22 step:21149 [D loss: 0.362074, acc.: 90.62%] [G loss: 1.888570]\n",
      "epoch:22 step:21150 [D loss: 1.029721, acc.: 41.41%] [G loss: 1.523897]\n",
      "epoch:22 step:21151 [D loss: 1.068852, acc.: 32.03%] [G loss: 2.779155]\n",
      "epoch:22 step:21152 [D loss: 0.456690, acc.: 74.22%] [G loss: 1.511731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21153 [D loss: 0.339948, acc.: 86.72%] [G loss: 0.802927]\n",
      "epoch:22 step:21154 [D loss: 0.657860, acc.: 61.72%] [G loss: 1.055278]\n",
      "epoch:22 step:21155 [D loss: 0.299685, acc.: 93.75%] [G loss: 1.610266]\n",
      "epoch:22 step:21156 [D loss: 0.640092, acc.: 75.00%] [G loss: 0.857031]\n",
      "epoch:22 step:21157 [D loss: 0.484246, acc.: 69.53%] [G loss: 1.642801]\n",
      "epoch:22 step:21158 [D loss: 0.176905, acc.: 95.31%] [G loss: 1.557283]\n",
      "epoch:22 step:21159 [D loss: 0.275717, acc.: 89.06%] [G loss: 1.838886]\n",
      "epoch:22 step:21160 [D loss: 0.493849, acc.: 80.47%] [G loss: 2.634205]\n",
      "epoch:22 step:21161 [D loss: 0.482515, acc.: 80.47%] [G loss: 0.421030]\n",
      "epoch:22 step:21162 [D loss: 0.731618, acc.: 56.25%] [G loss: 1.410613]\n",
      "epoch:22 step:21163 [D loss: 0.440079, acc.: 76.56%] [G loss: 1.179395]\n",
      "epoch:22 step:21164 [D loss: 0.367499, acc.: 91.41%] [G loss: 2.123271]\n",
      "epoch:22 step:21165 [D loss: 0.803911, acc.: 50.00%] [G loss: 0.770583]\n",
      "epoch:22 step:21166 [D loss: 0.552064, acc.: 69.53%] [G loss: 0.770411]\n",
      "epoch:22 step:21167 [D loss: 0.253875, acc.: 96.88%] [G loss: 1.887647]\n",
      "epoch:22 step:21168 [D loss: 0.519563, acc.: 71.88%] [G loss: 0.918779]\n",
      "epoch:22 step:21169 [D loss: 0.369976, acc.: 89.84%] [G loss: 1.268520]\n",
      "epoch:22 step:21170 [D loss: 0.708857, acc.: 53.91%] [G loss: 1.017606]\n",
      "epoch:22 step:21171 [D loss: 0.188551, acc.: 96.09%] [G loss: 3.269794]\n",
      "epoch:22 step:21172 [D loss: 0.360864, acc.: 83.59%] [G loss: 2.151099]\n",
      "epoch:22 step:21173 [D loss: 0.503348, acc.: 77.34%] [G loss: 1.625568]\n",
      "epoch:22 step:21174 [D loss: 0.749559, acc.: 55.47%] [G loss: 1.911312]\n",
      "epoch:22 step:21175 [D loss: 0.605277, acc.: 67.19%] [G loss: 2.039897]\n",
      "epoch:22 step:21176 [D loss: 0.351605, acc.: 88.28%] [G loss: 2.984566]\n",
      "epoch:22 step:21177 [D loss: 0.304448, acc.: 85.94%] [G loss: 1.319370]\n",
      "epoch:22 step:21178 [D loss: 0.357537, acc.: 84.38%] [G loss: 0.995065]\n",
      "epoch:22 step:21179 [D loss: 0.355864, acc.: 92.19%] [G loss: 1.029961]\n",
      "epoch:22 step:21180 [D loss: 0.520540, acc.: 78.12%] [G loss: 0.478543]\n",
      "epoch:22 step:21181 [D loss: 0.569401, acc.: 67.97%] [G loss: 0.706245]\n",
      "epoch:22 step:21182 [D loss: 0.431047, acc.: 84.38%] [G loss: 2.211951]\n",
      "epoch:22 step:21183 [D loss: 0.394181, acc.: 85.16%] [G loss: 2.483465]\n",
      "epoch:22 step:21184 [D loss: 1.129146, acc.: 34.38%] [G loss: 1.811086]\n",
      "epoch:22 step:21185 [D loss: 0.273694, acc.: 92.97%] [G loss: 1.130077]\n",
      "epoch:22 step:21186 [D loss: 0.308701, acc.: 91.41%] [G loss: 2.541717]\n",
      "epoch:22 step:21187 [D loss: 0.333557, acc.: 85.94%] [G loss: 3.892539]\n",
      "epoch:22 step:21188 [D loss: 0.500376, acc.: 71.88%] [G loss: 2.134747]\n",
      "epoch:22 step:21189 [D loss: 0.213269, acc.: 93.75%] [G loss: 2.565955]\n",
      "epoch:22 step:21190 [D loss: 0.278327, acc.: 89.84%] [G loss: 1.383563]\n",
      "epoch:22 step:21191 [D loss: 0.137500, acc.: 98.44%] [G loss: 1.978029]\n",
      "epoch:22 step:21192 [D loss: 0.403403, acc.: 83.59%] [G loss: 1.091308]\n",
      "epoch:22 step:21193 [D loss: 0.252380, acc.: 94.53%] [G loss: 0.838400]\n",
      "epoch:22 step:21194 [D loss: 0.483885, acc.: 74.22%] [G loss: 1.585472]\n",
      "epoch:22 step:21195 [D loss: 0.417314, acc.: 85.16%] [G loss: 0.583328]\n",
      "epoch:22 step:21196 [D loss: 0.265884, acc.: 92.19%] [G loss: 1.293744]\n",
      "epoch:22 step:21197 [D loss: 0.200302, acc.: 95.31%] [G loss: 1.981094]\n",
      "epoch:22 step:21198 [D loss: 0.537791, acc.: 73.44%] [G loss: 3.754842]\n",
      "epoch:22 step:21199 [D loss: 0.242583, acc.: 92.97%] [G loss: 1.680704]\n",
      "epoch:22 step:21200 [D loss: 0.703048, acc.: 61.72%] [G loss: 2.045777]\n",
      "epoch:22 step:21201 [D loss: 0.672438, acc.: 62.50%] [G loss: 2.627288]\n",
      "epoch:22 step:21202 [D loss: 0.194787, acc.: 96.88%] [G loss: 1.791045]\n",
      "epoch:22 step:21203 [D loss: 0.111214, acc.: 99.22%] [G loss: 1.257855]\n",
      "epoch:22 step:21204 [D loss: 0.631272, acc.: 63.28%] [G loss: 0.958371]\n",
      "epoch:22 step:21205 [D loss: 0.121175, acc.: 98.44%] [G loss: 3.657564]\n",
      "epoch:22 step:21206 [D loss: 0.196507, acc.: 96.09%] [G loss: 0.600646]\n",
      "epoch:22 step:21207 [D loss: 0.196020, acc.: 91.41%] [G loss: 1.161481]\n",
      "epoch:22 step:21208 [D loss: 0.827012, acc.: 49.22%] [G loss: 2.337824]\n",
      "epoch:22 step:21209 [D loss: 0.308646, acc.: 92.19%] [G loss: 1.894452]\n",
      "epoch:22 step:21210 [D loss: 0.876817, acc.: 41.41%] [G loss: 1.174919]\n",
      "epoch:22 step:21211 [D loss: 0.555741, acc.: 68.75%] [G loss: 2.112598]\n",
      "epoch:22 step:21212 [D loss: 0.549525, acc.: 65.62%] [G loss: 1.156639]\n",
      "epoch:22 step:21213 [D loss: 0.229085, acc.: 94.53%] [G loss: 1.033138]\n",
      "epoch:22 step:21214 [D loss: 0.656382, acc.: 65.62%] [G loss: 0.984466]\n",
      "epoch:22 step:21215 [D loss: 0.366643, acc.: 85.94%] [G loss: 4.429615]\n",
      "epoch:22 step:21216 [D loss: 0.244786, acc.: 93.75%] [G loss: 1.357925]\n",
      "epoch:22 step:21217 [D loss: 0.756645, acc.: 55.47%] [G loss: 3.499332]\n",
      "epoch:22 step:21218 [D loss: 0.538154, acc.: 71.88%] [G loss: 2.322312]\n",
      "epoch:22 step:21219 [D loss: 0.479110, acc.: 81.25%] [G loss: 2.787118]\n",
      "epoch:22 step:21220 [D loss: 0.659230, acc.: 50.78%] [G loss: 1.633104]\n",
      "epoch:22 step:21221 [D loss: 0.358398, acc.: 89.06%] [G loss: 1.869528]\n",
      "epoch:22 step:21222 [D loss: 0.583546, acc.: 66.41%] [G loss: 1.916775]\n",
      "epoch:22 step:21223 [D loss: 0.524747, acc.: 64.84%] [G loss: 1.496161]\n",
      "epoch:22 step:21224 [D loss: 0.278839, acc.: 93.75%] [G loss: 2.611112]\n",
      "epoch:22 step:21225 [D loss: 0.120194, acc.: 96.88%] [G loss: 1.392128]\n",
      "epoch:22 step:21226 [D loss: 0.528194, acc.: 71.88%] [G loss: 2.204150]\n",
      "epoch:22 step:21227 [D loss: 0.257272, acc.: 95.31%] [G loss: 3.379318]\n",
      "epoch:22 step:21228 [D loss: 0.412775, acc.: 83.59%] [G loss: 3.360140]\n",
      "epoch:22 step:21229 [D loss: 0.527766, acc.: 67.97%] [G loss: 1.032511]\n",
      "epoch:22 step:21230 [D loss: 0.500469, acc.: 78.12%] [G loss: 2.066374]\n",
      "epoch:22 step:21231 [D loss: 0.215529, acc.: 96.09%] [G loss: 1.781347]\n",
      "epoch:22 step:21232 [D loss: 0.098829, acc.: 97.66%] [G loss: 1.318948]\n",
      "epoch:22 step:21233 [D loss: 0.296179, acc.: 90.62%] [G loss: 2.412870]\n",
      "epoch:22 step:21234 [D loss: 0.476565, acc.: 71.09%] [G loss: 1.759061]\n",
      "epoch:22 step:21235 [D loss: 0.167491, acc.: 93.75%] [G loss: 2.394578]\n",
      "epoch:22 step:21236 [D loss: 0.906442, acc.: 48.44%] [G loss: 1.195285]\n",
      "epoch:22 step:21237 [D loss: 0.532086, acc.: 69.53%] [G loss: 1.628735]\n",
      "epoch:22 step:21238 [D loss: 0.626387, acc.: 64.06%] [G loss: 1.605901]\n",
      "epoch:22 step:21239 [D loss: 0.242238, acc.: 94.53%] [G loss: 1.391159]\n",
      "epoch:22 step:21240 [D loss: 0.770097, acc.: 60.16%] [G loss: 0.873004]\n",
      "epoch:22 step:21241 [D loss: 0.353072, acc.: 89.06%] [G loss: 3.845425]\n",
      "epoch:22 step:21242 [D loss: 0.307946, acc.: 89.06%] [G loss: 2.596790]\n",
      "epoch:22 step:21243 [D loss: 0.383219, acc.: 87.50%] [G loss: 5.135405]\n",
      "epoch:22 step:21244 [D loss: 0.487098, acc.: 74.22%] [G loss: 1.492404]\n",
      "epoch:22 step:21245 [D loss: 0.754917, acc.: 64.84%] [G loss: 1.281630]\n",
      "epoch:22 step:21246 [D loss: 0.486741, acc.: 76.56%] [G loss: 2.204840]\n",
      "epoch:22 step:21247 [D loss: 0.403818, acc.: 82.81%] [G loss: 1.874295]\n",
      "epoch:22 step:21248 [D loss: 0.273018, acc.: 90.62%] [G loss: 0.868189]\n",
      "epoch:22 step:21249 [D loss: 0.921056, acc.: 47.66%] [G loss: 2.744226]\n",
      "epoch:22 step:21250 [D loss: 0.162877, acc.: 97.66%] [G loss: 2.341270]\n",
      "epoch:22 step:21251 [D loss: 0.372733, acc.: 82.81%] [G loss: 4.022069]\n",
      "epoch:22 step:21252 [D loss: 0.301141, acc.: 90.62%] [G loss: 3.231369]\n",
      "epoch:22 step:21253 [D loss: 0.513516, acc.: 70.31%] [G loss: 2.218812]\n",
      "epoch:22 step:21254 [D loss: 0.319822, acc.: 85.16%] [G loss: 1.391281]\n",
      "epoch:22 step:21255 [D loss: 0.793025, acc.: 57.81%] [G loss: 2.185375]\n",
      "epoch:22 step:21256 [D loss: 0.407513, acc.: 78.91%] [G loss: 3.060428]\n",
      "epoch:22 step:21257 [D loss: 0.092299, acc.: 100.00%] [G loss: 1.629963]\n",
      "epoch:22 step:21258 [D loss: 0.335627, acc.: 85.16%] [G loss: 2.375707]\n",
      "epoch:22 step:21259 [D loss: 0.451806, acc.: 71.88%] [G loss: 2.673851]\n",
      "epoch:22 step:21260 [D loss: 0.372017, acc.: 84.38%] [G loss: 0.989907]\n",
      "epoch:22 step:21261 [D loss: 1.066019, acc.: 55.47%] [G loss: 2.690305]\n",
      "epoch:22 step:21262 [D loss: 0.526840, acc.: 71.09%] [G loss: 2.230764]\n",
      "epoch:22 step:21263 [D loss: 0.749138, acc.: 62.50%] [G loss: 3.257000]\n",
      "epoch:22 step:21264 [D loss: 0.475592, acc.: 80.47%] [G loss: 1.236711]\n",
      "epoch:22 step:21265 [D loss: 0.589608, acc.: 67.97%] [G loss: 1.800882]\n",
      "epoch:22 step:21266 [D loss: 0.923984, acc.: 42.97%] [G loss: 1.255833]\n",
      "epoch:22 step:21267 [D loss: 0.250884, acc.: 91.41%] [G loss: 1.629911]\n",
      "epoch:22 step:21268 [D loss: 0.312499, acc.: 92.19%] [G loss: 2.186839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21269 [D loss: 0.154275, acc.: 99.22%] [G loss: 2.288056]\n",
      "epoch:22 step:21270 [D loss: 0.274429, acc.: 95.31%] [G loss: 1.416489]\n",
      "epoch:22 step:21271 [D loss: 0.804911, acc.: 49.22%] [G loss: 2.158232]\n",
      "epoch:22 step:21272 [D loss: 0.663933, acc.: 62.50%] [G loss: 2.603195]\n",
      "epoch:22 step:21273 [D loss: 0.557305, acc.: 72.66%] [G loss: 1.334747]\n",
      "epoch:22 step:21274 [D loss: 0.282325, acc.: 92.19%] [G loss: 1.594070]\n",
      "epoch:22 step:21275 [D loss: 0.967010, acc.: 36.72%] [G loss: 1.649130]\n",
      "epoch:22 step:21276 [D loss: 0.338022, acc.: 88.28%] [G loss: 3.114835]\n",
      "epoch:22 step:21277 [D loss: 0.845942, acc.: 57.03%] [G loss: 0.817206]\n",
      "epoch:22 step:21278 [D loss: 0.244821, acc.: 96.09%] [G loss: 1.526609]\n",
      "epoch:22 step:21279 [D loss: 0.352139, acc.: 86.72%] [G loss: 0.941134]\n",
      "epoch:22 step:21280 [D loss: 0.796972, acc.: 59.38%] [G loss: 1.477230]\n",
      "epoch:22 step:21281 [D loss: 0.678078, acc.: 60.16%] [G loss: 1.320685]\n",
      "epoch:22 step:21282 [D loss: 0.583800, acc.: 64.06%] [G loss: 2.473315]\n",
      "epoch:22 step:21283 [D loss: 0.192835, acc.: 96.09%] [G loss: 2.404650]\n",
      "epoch:22 step:21284 [D loss: 0.116259, acc.: 98.44%] [G loss: 3.131647]\n",
      "epoch:22 step:21285 [D loss: 0.236315, acc.: 96.09%] [G loss: 2.032859]\n",
      "epoch:22 step:21286 [D loss: 0.470374, acc.: 83.59%] [G loss: 1.379517]\n",
      "epoch:22 step:21287 [D loss: 0.618727, acc.: 66.41%] [G loss: 3.450316]\n",
      "epoch:22 step:21288 [D loss: 0.190272, acc.: 96.88%] [G loss: 1.956773]\n",
      "epoch:22 step:21289 [D loss: 0.307208, acc.: 86.72%] [G loss: 2.297029]\n",
      "epoch:22 step:21290 [D loss: 0.214353, acc.: 96.88%] [G loss: 0.700502]\n",
      "epoch:22 step:21291 [D loss: 0.162555, acc.: 97.66%] [G loss: 2.027333]\n",
      "epoch:22 step:21292 [D loss: 0.914958, acc.: 50.00%] [G loss: 0.621374]\n",
      "epoch:22 step:21293 [D loss: 0.453021, acc.: 77.34%] [G loss: 2.123811]\n",
      "epoch:22 step:21294 [D loss: 0.791225, acc.: 56.25%] [G loss: 2.548156]\n",
      "epoch:22 step:21295 [D loss: 0.198518, acc.: 96.88%] [G loss: 1.181872]\n",
      "epoch:22 step:21296 [D loss: 0.713199, acc.: 57.81%] [G loss: 1.936036]\n",
      "epoch:22 step:21297 [D loss: 0.436072, acc.: 80.47%] [G loss: 2.261315]\n",
      "epoch:22 step:21298 [D loss: 0.868852, acc.: 47.66%] [G loss: 1.926094]\n",
      "epoch:22 step:21299 [D loss: 0.318289, acc.: 87.50%] [G loss: 3.411616]\n",
      "epoch:22 step:21300 [D loss: 0.543525, acc.: 68.75%] [G loss: 1.616276]\n",
      "epoch:22 step:21301 [D loss: 0.551488, acc.: 70.31%] [G loss: 0.495840]\n",
      "epoch:22 step:21302 [D loss: 0.486273, acc.: 78.12%] [G loss: 0.702839]\n",
      "epoch:22 step:21303 [D loss: 0.722461, acc.: 59.38%] [G loss: 0.807056]\n",
      "epoch:22 step:21304 [D loss: 0.391339, acc.: 81.25%] [G loss: 0.499177]\n",
      "epoch:22 step:21305 [D loss: 0.590259, acc.: 71.09%] [G loss: 1.057295]\n",
      "epoch:22 step:21306 [D loss: 0.434449, acc.: 82.03%] [G loss: 1.478693]\n",
      "epoch:22 step:21307 [D loss: 0.539648, acc.: 75.00%] [G loss: 1.758984]\n",
      "epoch:22 step:21308 [D loss: 0.946569, acc.: 36.72%] [G loss: 1.809480]\n",
      "epoch:22 step:21309 [D loss: 0.431903, acc.: 78.91%] [G loss: 2.236907]\n",
      "epoch:22 step:21310 [D loss: 0.464417, acc.: 80.47%] [G loss: 1.412772]\n",
      "epoch:22 step:21311 [D loss: 0.233997, acc.: 95.31%] [G loss: 3.693666]\n",
      "epoch:22 step:21312 [D loss: 0.541900, acc.: 71.88%] [G loss: 1.674217]\n",
      "epoch:22 step:21313 [D loss: 0.182945, acc.: 98.44%] [G loss: 0.624849]\n",
      "epoch:22 step:21314 [D loss: 0.444273, acc.: 78.12%] [G loss: 3.718497]\n",
      "epoch:22 step:21315 [D loss: 0.358966, acc.: 87.50%] [G loss: 2.295984]\n",
      "epoch:22 step:21316 [D loss: 0.412594, acc.: 79.69%] [G loss: 0.591103]\n",
      "epoch:22 step:21317 [D loss: 0.615534, acc.: 67.19%] [G loss: 1.054857]\n",
      "epoch:22 step:21318 [D loss: 0.505775, acc.: 75.78%] [G loss: 1.544450]\n",
      "epoch:22 step:21319 [D loss: 0.842528, acc.: 57.03%] [G loss: 0.817875]\n",
      "epoch:22 step:21320 [D loss: 0.184810, acc.: 93.75%] [G loss: 1.967322]\n",
      "epoch:22 step:21321 [D loss: 0.145560, acc.: 97.66%] [G loss: 1.569932]\n",
      "epoch:22 step:21322 [D loss: 0.529751, acc.: 68.75%] [G loss: 2.756495]\n",
      "epoch:22 step:21323 [D loss: 1.334430, acc.: 21.88%] [G loss: 2.105098]\n",
      "epoch:22 step:21324 [D loss: 0.260174, acc.: 92.19%] [G loss: 2.358598]\n",
      "epoch:22 step:21325 [D loss: 0.286377, acc.: 91.41%] [G loss: 1.296459]\n",
      "epoch:22 step:21326 [D loss: 0.266582, acc.: 90.62%] [G loss: 2.653308]\n",
      "epoch:22 step:21327 [D loss: 0.553961, acc.: 74.22%] [G loss: 2.378233]\n",
      "epoch:22 step:21328 [D loss: 0.266930, acc.: 92.97%] [G loss: 2.100106]\n",
      "epoch:22 step:21329 [D loss: 0.318173, acc.: 90.62%] [G loss: 1.881357]\n",
      "epoch:22 step:21330 [D loss: 1.073526, acc.: 48.44%] [G loss: 1.356551]\n",
      "epoch:22 step:21331 [D loss: 0.630782, acc.: 64.06%] [G loss: 0.740694]\n",
      "epoch:22 step:21332 [D loss: 0.599203, acc.: 62.50%] [G loss: 0.938605]\n",
      "epoch:22 step:21333 [D loss: 0.148947, acc.: 100.00%] [G loss: 1.243077]\n",
      "epoch:22 step:21334 [D loss: 0.286014, acc.: 93.75%] [G loss: 2.209114]\n",
      "epoch:22 step:21335 [D loss: 0.399550, acc.: 84.38%] [G loss: 2.195973]\n",
      "epoch:22 step:21336 [D loss: 0.510884, acc.: 68.75%] [G loss: 2.605773]\n",
      "epoch:22 step:21337 [D loss: 0.820392, acc.: 54.69%] [G loss: 1.431833]\n",
      "epoch:22 step:21338 [D loss: 0.434733, acc.: 81.25%] [G loss: 2.082584]\n",
      "epoch:22 step:21339 [D loss: 0.400319, acc.: 85.94%] [G loss: 2.165286]\n",
      "epoch:22 step:21340 [D loss: 0.578379, acc.: 71.09%] [G loss: 1.412696]\n",
      "epoch:22 step:21341 [D loss: 0.251246, acc.: 93.75%] [G loss: 2.168416]\n",
      "epoch:22 step:21342 [D loss: 0.149788, acc.: 98.44%] [G loss: 2.670566]\n",
      "epoch:22 step:21343 [D loss: 0.624123, acc.: 61.72%] [G loss: 2.709327]\n",
      "epoch:22 step:21344 [D loss: 0.801196, acc.: 53.91%] [G loss: 1.863327]\n",
      "epoch:22 step:21345 [D loss: 0.412036, acc.: 75.78%] [G loss: 2.724433]\n",
      "epoch:22 step:21346 [D loss: 0.185166, acc.: 96.09%] [G loss: 2.012774]\n",
      "epoch:22 step:21347 [D loss: 0.361928, acc.: 88.28%] [G loss: 1.653251]\n",
      "epoch:22 step:21348 [D loss: 0.305170, acc.: 90.62%] [G loss: 1.908944]\n",
      "epoch:22 step:21349 [D loss: 0.178453, acc.: 97.66%] [G loss: 1.701610]\n",
      "epoch:22 step:21350 [D loss: 0.225852, acc.: 95.31%] [G loss: 2.055492]\n",
      "epoch:22 step:21351 [D loss: 0.353256, acc.: 88.28%] [G loss: 1.681295]\n",
      "epoch:22 step:21352 [D loss: 0.463713, acc.: 81.25%] [G loss: 2.556307]\n",
      "epoch:22 step:21353 [D loss: 0.545972, acc.: 64.06%] [G loss: 1.213232]\n",
      "epoch:22 step:21354 [D loss: 0.404098, acc.: 83.59%] [G loss: 1.695935]\n",
      "epoch:22 step:21355 [D loss: 0.141911, acc.: 96.09%] [G loss: 1.088597]\n",
      "epoch:22 step:21356 [D loss: 0.551402, acc.: 69.53%] [G loss: 2.466658]\n",
      "epoch:22 step:21357 [D loss: 0.774679, acc.: 58.59%] [G loss: 1.525462]\n",
      "epoch:22 step:21358 [D loss: 0.726787, acc.: 59.38%] [G loss: 3.035636]\n",
      "epoch:22 step:21359 [D loss: 0.270262, acc.: 91.41%] [G loss: 2.666184]\n",
      "epoch:22 step:21360 [D loss: 0.328040, acc.: 90.62%] [G loss: 2.427815]\n",
      "epoch:22 step:21361 [D loss: 0.247721, acc.: 93.75%] [G loss: 2.662858]\n",
      "epoch:22 step:21362 [D loss: 0.068165, acc.: 99.22%] [G loss: 3.992643]\n",
      "epoch:22 step:21363 [D loss: 0.736836, acc.: 53.91%] [G loss: 1.827917]\n",
      "epoch:22 step:21364 [D loss: 0.123862, acc.: 99.22%] [G loss: 3.805416]\n",
      "epoch:22 step:21365 [D loss: 0.381458, acc.: 86.72%] [G loss: 2.611263]\n",
      "epoch:22 step:21366 [D loss: 0.394062, acc.: 81.25%] [G loss: 0.759921]\n",
      "epoch:22 step:21367 [D loss: 0.633791, acc.: 61.72%] [G loss: 2.951074]\n",
      "epoch:22 step:21368 [D loss: 0.168806, acc.: 94.53%] [G loss: 0.737499]\n",
      "epoch:22 step:21369 [D loss: 0.575233, acc.: 70.31%] [G loss: 1.467210]\n",
      "epoch:22 step:21370 [D loss: 0.655331, acc.: 63.28%] [G loss: 1.260916]\n",
      "epoch:22 step:21371 [D loss: 0.497339, acc.: 74.22%] [G loss: 2.451928]\n",
      "epoch:22 step:21372 [D loss: 0.179914, acc.: 98.44%] [G loss: 1.245997]\n",
      "epoch:22 step:21373 [D loss: 0.250468, acc.: 89.84%] [G loss: 3.013277]\n",
      "epoch:22 step:21374 [D loss: 0.401041, acc.: 75.78%] [G loss: 2.286993]\n",
      "epoch:22 step:21375 [D loss: 0.335482, acc.: 89.06%] [G loss: 1.742513]\n",
      "epoch:22 step:21376 [D loss: 0.221122, acc.: 92.97%] [G loss: 1.486774]\n",
      "epoch:22 step:21377 [D loss: 0.528659, acc.: 70.31%] [G loss: 2.043466]\n",
      "epoch:22 step:21378 [D loss: 0.253975, acc.: 95.31%] [G loss: 2.032648]\n",
      "epoch:22 step:21379 [D loss: 0.392035, acc.: 82.81%] [G loss: 2.855622]\n",
      "epoch:22 step:21380 [D loss: 0.566793, acc.: 71.09%] [G loss: 2.018310]\n",
      "epoch:22 step:21381 [D loss: 0.285335, acc.: 92.19%] [G loss: 2.123682]\n",
      "epoch:22 step:21382 [D loss: 0.277145, acc.: 90.62%] [G loss: 1.007750]\n",
      "epoch:22 step:21383 [D loss: 0.212846, acc.: 95.31%] [G loss: 2.219030]\n",
      "epoch:22 step:21384 [D loss: 0.632566, acc.: 64.84%] [G loss: 1.552933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21385 [D loss: 0.115757, acc.: 97.66%] [G loss: 0.527519]\n",
      "epoch:22 step:21386 [D loss: 0.240310, acc.: 93.75%] [G loss: 0.997858]\n",
      "epoch:22 step:21387 [D loss: 0.590212, acc.: 68.75%] [G loss: 1.938669]\n",
      "epoch:22 step:21388 [D loss: 0.559113, acc.: 65.62%] [G loss: 6.541965]\n",
      "epoch:22 step:21389 [D loss: 0.146639, acc.: 95.31%] [G loss: 3.628464]\n",
      "epoch:22 step:21390 [D loss: 0.191984, acc.: 93.75%] [G loss: 2.568577]\n",
      "epoch:22 step:21391 [D loss: 0.227140, acc.: 92.19%] [G loss: 0.744937]\n",
      "epoch:22 step:21392 [D loss: 0.983020, acc.: 46.88%] [G loss: 2.449702]\n",
      "epoch:22 step:21393 [D loss: 1.388350, acc.: 16.41%] [G loss: 2.942533]\n",
      "epoch:22 step:21394 [D loss: 0.462359, acc.: 80.47%] [G loss: 1.982211]\n",
      "epoch:22 step:21395 [D loss: 0.278256, acc.: 91.41%] [G loss: 2.735518]\n",
      "epoch:22 step:21396 [D loss: 0.241333, acc.: 93.75%] [G loss: 2.635666]\n",
      "epoch:22 step:21397 [D loss: 0.471455, acc.: 69.53%] [G loss: 2.034524]\n",
      "epoch:22 step:21398 [D loss: 0.893495, acc.: 42.19%] [G loss: 1.995811]\n",
      "epoch:22 step:21399 [D loss: 0.296240, acc.: 88.28%] [G loss: 1.074286]\n",
      "epoch:22 step:21400 [D loss: 0.745650, acc.: 59.38%] [G loss: 1.172316]\n",
      "epoch:22 step:21401 [D loss: 0.900063, acc.: 45.31%] [G loss: 1.357269]\n",
      "epoch:22 step:21402 [D loss: 0.442348, acc.: 80.47%] [G loss: 1.851987]\n",
      "epoch:22 step:21403 [D loss: 0.979932, acc.: 55.47%] [G loss: 1.027080]\n",
      "epoch:22 step:21404 [D loss: 0.347623, acc.: 89.84%] [G loss: 1.535158]\n",
      "epoch:22 step:21405 [D loss: 0.233582, acc.: 96.88%] [G loss: 0.931606]\n",
      "epoch:22 step:21406 [D loss: 0.646177, acc.: 64.06%] [G loss: 1.381171]\n",
      "epoch:22 step:21407 [D loss: 0.283094, acc.: 92.97%] [G loss: 2.755011]\n",
      "epoch:22 step:21408 [D loss: 0.445630, acc.: 79.69%] [G loss: 2.471620]\n",
      "epoch:22 step:21409 [D loss: 0.736303, acc.: 53.91%] [G loss: 1.558431]\n",
      "epoch:22 step:21410 [D loss: 0.316456, acc.: 90.62%] [G loss: 1.531677]\n",
      "epoch:22 step:21411 [D loss: 0.452358, acc.: 77.34%] [G loss: 1.434235]\n",
      "epoch:22 step:21412 [D loss: 0.376007, acc.: 85.16%] [G loss: 2.010115]\n",
      "epoch:22 step:21413 [D loss: 0.535001, acc.: 78.12%] [G loss: 1.242437]\n",
      "epoch:22 step:21414 [D loss: 0.271593, acc.: 91.41%] [G loss: 3.080136]\n",
      "epoch:22 step:21415 [D loss: 0.458517, acc.: 78.12%] [G loss: 2.960216]\n",
      "epoch:22 step:21416 [D loss: 0.649383, acc.: 60.94%] [G loss: 1.902867]\n",
      "epoch:22 step:21417 [D loss: 0.410944, acc.: 77.34%] [G loss: 1.616726]\n",
      "epoch:22 step:21418 [D loss: 0.367259, acc.: 89.84%] [G loss: 1.925571]\n",
      "epoch:22 step:21419 [D loss: 0.423360, acc.: 80.47%] [G loss: 2.614417]\n",
      "epoch:22 step:21420 [D loss: 0.440346, acc.: 81.25%] [G loss: 1.985824]\n",
      "epoch:22 step:21421 [D loss: 0.273488, acc.: 90.62%] [G loss: 3.154409]\n",
      "epoch:22 step:21422 [D loss: 0.133141, acc.: 99.22%] [G loss: 1.812812]\n",
      "epoch:22 step:21423 [D loss: 0.240086, acc.: 94.53%] [G loss: 1.449384]\n",
      "epoch:22 step:21424 [D loss: 0.305705, acc.: 85.94%] [G loss: 1.649938]\n",
      "epoch:22 step:21425 [D loss: 0.360728, acc.: 88.28%] [G loss: 1.399754]\n",
      "epoch:22 step:21426 [D loss: 0.200115, acc.: 96.09%] [G loss: 1.382854]\n",
      "epoch:22 step:21427 [D loss: 0.555958, acc.: 76.56%] [G loss: 3.048223]\n",
      "epoch:22 step:21428 [D loss: 0.219781, acc.: 93.75%] [G loss: 1.454592]\n",
      "epoch:22 step:21429 [D loss: 0.297584, acc.: 89.84%] [G loss: 0.409502]\n",
      "epoch:22 step:21430 [D loss: 0.611612, acc.: 65.62%] [G loss: 1.398654]\n",
      "epoch:22 step:21431 [D loss: 0.616736, acc.: 62.50%] [G loss: 3.408052]\n",
      "epoch:22 step:21432 [D loss: 0.627818, acc.: 64.06%] [G loss: 1.856778]\n",
      "epoch:22 step:21433 [D loss: 0.800784, acc.: 56.25%] [G loss: 2.305218]\n",
      "epoch:22 step:21434 [D loss: 0.342045, acc.: 89.84%] [G loss: 2.621737]\n",
      "epoch:22 step:21435 [D loss: 0.478581, acc.: 75.78%] [G loss: 2.251634]\n",
      "epoch:22 step:21436 [D loss: 0.695990, acc.: 63.28%] [G loss: 1.255490]\n",
      "epoch:22 step:21437 [D loss: 0.288606, acc.: 92.19%] [G loss: 1.483881]\n",
      "epoch:22 step:21438 [D loss: 0.670837, acc.: 58.59%] [G loss: 2.075769]\n",
      "epoch:22 step:21439 [D loss: 1.218555, acc.: 42.19%] [G loss: 1.990391]\n",
      "epoch:22 step:21440 [D loss: 0.281330, acc.: 87.50%] [G loss: 1.368597]\n",
      "epoch:22 step:21441 [D loss: 0.377576, acc.: 87.50%] [G loss: 1.134038]\n",
      "epoch:22 step:21442 [D loss: 0.930825, acc.: 55.47%] [G loss: 0.839913]\n",
      "epoch:22 step:21443 [D loss: 0.527512, acc.: 71.88%] [G loss: 1.028107]\n",
      "epoch:22 step:21444 [D loss: 1.332525, acc.: 25.78%] [G loss: 0.670869]\n",
      "epoch:22 step:21445 [D loss: 0.637098, acc.: 61.72%] [G loss: 4.010195]\n",
      "epoch:22 step:21446 [D loss: 0.370429, acc.: 84.38%] [G loss: 1.155436]\n",
      "epoch:22 step:21447 [D loss: 0.306552, acc.: 91.41%] [G loss: 1.249603]\n",
      "epoch:22 step:21448 [D loss: 0.425529, acc.: 82.03%] [G loss: 1.354636]\n",
      "epoch:22 step:21449 [D loss: 0.513335, acc.: 76.56%] [G loss: 1.815499]\n",
      "epoch:22 step:21450 [D loss: 0.465969, acc.: 80.47%] [G loss: 2.408382]\n",
      "epoch:22 step:21451 [D loss: 0.638724, acc.: 62.50%] [G loss: 1.150643]\n",
      "epoch:22 step:21452 [D loss: 0.594094, acc.: 70.31%] [G loss: 2.290539]\n",
      "epoch:22 step:21453 [D loss: 0.346428, acc.: 89.06%] [G loss: 2.423943]\n",
      "epoch:22 step:21454 [D loss: 0.503886, acc.: 77.34%] [G loss: 1.214846]\n",
      "epoch:22 step:21455 [D loss: 0.331170, acc.: 90.62%] [G loss: 0.925176]\n",
      "epoch:22 step:21456 [D loss: 0.764399, acc.: 59.38%] [G loss: 1.783462]\n",
      "epoch:22 step:21457 [D loss: 0.367118, acc.: 83.59%] [G loss: 1.822048]\n",
      "epoch:22 step:21458 [D loss: 0.766213, acc.: 51.56%] [G loss: 1.413268]\n",
      "epoch:22 step:21459 [D loss: 0.681249, acc.: 66.41%] [G loss: 2.712657]\n",
      "epoch:22 step:21460 [D loss: 0.386221, acc.: 83.59%] [G loss: 1.132138]\n",
      "epoch:22 step:21461 [D loss: 0.299630, acc.: 92.19%] [G loss: 2.664220]\n",
      "epoch:22 step:21462 [D loss: 0.520078, acc.: 72.66%] [G loss: 1.839928]\n",
      "epoch:22 step:21463 [D loss: 0.489818, acc.: 75.00%] [G loss: 2.374608]\n",
      "epoch:22 step:21464 [D loss: 0.312219, acc.: 88.28%] [G loss: 1.613572]\n",
      "epoch:22 step:21465 [D loss: 0.586791, acc.: 69.53%] [G loss: 2.784840]\n",
      "epoch:22 step:21466 [D loss: 0.533673, acc.: 70.31%] [G loss: 0.867142]\n",
      "epoch:22 step:21467 [D loss: 0.541125, acc.: 71.88%] [G loss: 2.022149]\n",
      "epoch:22 step:21468 [D loss: 0.374607, acc.: 83.59%] [G loss: 1.211170]\n",
      "epoch:22 step:21469 [D loss: 0.749317, acc.: 58.59%] [G loss: 0.461252]\n",
      "epoch:22 step:21470 [D loss: 0.557968, acc.: 68.75%] [G loss: 0.797187]\n",
      "epoch:22 step:21471 [D loss: 0.701336, acc.: 62.50%] [G loss: 0.706994]\n",
      "epoch:22 step:21472 [D loss: 0.971134, acc.: 53.12%] [G loss: 1.861290]\n",
      "epoch:22 step:21473 [D loss: 0.377180, acc.: 88.28%] [G loss: 1.578242]\n",
      "epoch:22 step:21474 [D loss: 0.398208, acc.: 78.12%] [G loss: 0.889416]\n",
      "epoch:22 step:21475 [D loss: 0.517517, acc.: 75.78%] [G loss: 1.870673]\n",
      "epoch:22 step:21476 [D loss: 0.331697, acc.: 89.84%] [G loss: 1.603819]\n",
      "epoch:22 step:21477 [D loss: 0.611959, acc.: 59.38%] [G loss: 1.518383]\n",
      "epoch:22 step:21478 [D loss: 0.740404, acc.: 58.59%] [G loss: 2.414210]\n",
      "epoch:22 step:21479 [D loss: 0.656930, acc.: 61.72%] [G loss: 1.597315]\n",
      "epoch:22 step:21480 [D loss: 0.797179, acc.: 53.91%] [G loss: 1.187801]\n",
      "epoch:22 step:21481 [D loss: 0.222803, acc.: 94.53%] [G loss: 1.579725]\n",
      "epoch:22 step:21482 [D loss: 0.378190, acc.: 83.59%] [G loss: 1.469593]\n",
      "epoch:22 step:21483 [D loss: 0.553967, acc.: 62.50%] [G loss: 1.634828]\n",
      "epoch:22 step:21484 [D loss: 0.313310, acc.: 89.06%] [G loss: 2.017217]\n",
      "epoch:22 step:21485 [D loss: 0.712161, acc.: 59.38%] [G loss: 2.919109]\n",
      "epoch:22 step:21486 [D loss: 0.508459, acc.: 75.00%] [G loss: 2.007957]\n",
      "epoch:22 step:21487 [D loss: 0.291283, acc.: 93.75%] [G loss: 2.115774]\n",
      "epoch:22 step:21488 [D loss: 0.344748, acc.: 91.41%] [G loss: 1.661881]\n",
      "epoch:22 step:21489 [D loss: 0.762123, acc.: 49.22%] [G loss: 2.148478]\n",
      "epoch:22 step:21490 [D loss: 0.538284, acc.: 71.09%] [G loss: 2.388635]\n",
      "epoch:22 step:21491 [D loss: 0.323569, acc.: 90.62%] [G loss: 2.679370]\n",
      "epoch:22 step:21492 [D loss: 0.820205, acc.: 42.19%] [G loss: 3.005526]\n",
      "epoch:22 step:21493 [D loss: 0.349124, acc.: 88.28%] [G loss: 1.196910]\n",
      "epoch:22 step:21494 [D loss: 0.308687, acc.: 91.41%] [G loss: 2.158698]\n",
      "epoch:22 step:21495 [D loss: 0.475277, acc.: 74.22%] [G loss: 1.586985]\n",
      "epoch:22 step:21496 [D loss: 0.184676, acc.: 98.44%] [G loss: 2.255372]\n",
      "epoch:22 step:21497 [D loss: 0.763065, acc.: 60.16%] [G loss: 1.476020]\n",
      "epoch:22 step:21498 [D loss: 0.156017, acc.: 98.44%] [G loss: 2.016180]\n",
      "epoch:22 step:21499 [D loss: 0.305211, acc.: 92.19%] [G loss: 1.466425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21500 [D loss: 0.458452, acc.: 78.12%] [G loss: 0.853806]\n",
      "epoch:22 step:21501 [D loss: 0.460953, acc.: 74.22%] [G loss: 1.624247]\n",
      "epoch:22 step:21502 [D loss: 0.503759, acc.: 65.62%] [G loss: 2.476831]\n",
      "epoch:22 step:21503 [D loss: 0.393031, acc.: 84.38%] [G loss: 1.306043]\n",
      "epoch:22 step:21504 [D loss: 0.601075, acc.: 63.28%] [G loss: 1.599486]\n",
      "epoch:22 step:21505 [D loss: 0.269471, acc.: 95.31%] [G loss: 0.658287]\n",
      "epoch:22 step:21506 [D loss: 0.769406, acc.: 53.91%] [G loss: 0.623404]\n",
      "epoch:22 step:21507 [D loss: 0.527418, acc.: 72.66%] [G loss: 1.467115]\n",
      "epoch:22 step:21508 [D loss: 0.313936, acc.: 86.72%] [G loss: 2.969034]\n",
      "epoch:22 step:21509 [D loss: 0.306724, acc.: 94.53%] [G loss: 1.563643]\n",
      "epoch:22 step:21510 [D loss: 0.222000, acc.: 94.53%] [G loss: 1.986524]\n",
      "epoch:22 step:21511 [D loss: 0.432002, acc.: 83.59%] [G loss: 0.536466]\n",
      "epoch:22 step:21512 [D loss: 0.626730, acc.: 60.94%] [G loss: 1.348468]\n",
      "epoch:22 step:21513 [D loss: 0.827312, acc.: 42.19%] [G loss: 1.882142]\n",
      "epoch:22 step:21514 [D loss: 0.638755, acc.: 64.84%] [G loss: 2.354320]\n",
      "epoch:22 step:21515 [D loss: 0.628170, acc.: 61.72%] [G loss: 1.310419]\n",
      "epoch:22 step:21516 [D loss: 0.447447, acc.: 76.56%] [G loss: 1.125240]\n",
      "epoch:22 step:21517 [D loss: 0.532012, acc.: 72.66%] [G loss: 1.193516]\n",
      "epoch:22 step:21518 [D loss: 0.220780, acc.: 96.09%] [G loss: 2.667341]\n",
      "epoch:22 step:21519 [D loss: 0.463945, acc.: 81.25%] [G loss: 1.889131]\n",
      "epoch:22 step:21520 [D loss: 0.480631, acc.: 71.88%] [G loss: 1.471401]\n",
      "epoch:22 step:21521 [D loss: 0.344208, acc.: 92.19%] [G loss: 1.292248]\n",
      "epoch:22 step:21522 [D loss: 0.169693, acc.: 97.66%] [G loss: 1.777600]\n",
      "epoch:22 step:21523 [D loss: 0.539794, acc.: 69.53%] [G loss: 1.786373]\n",
      "epoch:22 step:21524 [D loss: 0.515557, acc.: 73.44%] [G loss: 2.371271]\n",
      "epoch:22 step:21525 [D loss: 0.273127, acc.: 91.41%] [G loss: 1.544375]\n",
      "epoch:22 step:21526 [D loss: 0.527741, acc.: 75.00%] [G loss: 1.937857]\n",
      "epoch:22 step:21527 [D loss: 0.225535, acc.: 96.09%] [G loss: 2.979033]\n",
      "epoch:22 step:21528 [D loss: 0.223605, acc.: 95.31%] [G loss: 2.578596]\n",
      "epoch:22 step:21529 [D loss: 0.320673, acc.: 89.84%] [G loss: 2.212351]\n",
      "epoch:22 step:21530 [D loss: 0.270657, acc.: 92.97%] [G loss: 1.612440]\n",
      "epoch:22 step:21531 [D loss: 0.353395, acc.: 87.50%] [G loss: 1.675329]\n",
      "epoch:22 step:21532 [D loss: 0.497257, acc.: 76.56%] [G loss: 2.226895]\n",
      "epoch:22 step:21533 [D loss: 0.482786, acc.: 77.34%] [G loss: 1.487045]\n",
      "epoch:22 step:21534 [D loss: 0.316805, acc.: 85.16%] [G loss: 2.249571]\n",
      "epoch:22 step:21535 [D loss: 0.152453, acc.: 96.88%] [G loss: 1.057469]\n",
      "epoch:22 step:21536 [D loss: 1.048763, acc.: 49.22%] [G loss: 1.081488]\n",
      "epoch:22 step:21537 [D loss: 0.280694, acc.: 91.41%] [G loss: 2.345953]\n",
      "epoch:22 step:21538 [D loss: 0.759995, acc.: 61.72%] [G loss: 2.072142]\n",
      "epoch:22 step:21539 [D loss: 0.498008, acc.: 71.09%] [G loss: 1.334501]\n",
      "epoch:22 step:21540 [D loss: 0.177040, acc.: 98.44%] [G loss: 1.800343]\n",
      "epoch:22 step:21541 [D loss: 0.286879, acc.: 93.75%] [G loss: 1.160720]\n",
      "epoch:22 step:21542 [D loss: 0.307678, acc.: 89.84%] [G loss: 2.306800]\n",
      "epoch:22 step:21543 [D loss: 0.574804, acc.: 70.31%] [G loss: 2.278644]\n",
      "epoch:22 step:21544 [D loss: 0.124290, acc.: 98.44%] [G loss: 3.441281]\n",
      "epoch:22 step:21545 [D loss: 0.820494, acc.: 57.03%] [G loss: 2.557286]\n",
      "epoch:22 step:21546 [D loss: 1.042540, acc.: 39.06%] [G loss: 0.802874]\n",
      "epoch:22 step:21547 [D loss: 1.415631, acc.: 38.28%] [G loss: 1.089139]\n",
      "epoch:22 step:21548 [D loss: 0.385521, acc.: 81.25%] [G loss: 1.468434]\n",
      "epoch:22 step:21549 [D loss: 0.263404, acc.: 96.88%] [G loss: 1.302740]\n",
      "epoch:22 step:21550 [D loss: 0.411895, acc.: 84.38%] [G loss: 1.254446]\n",
      "epoch:22 step:21551 [D loss: 0.519112, acc.: 74.22%] [G loss: 0.945512]\n",
      "epoch:23 step:21552 [D loss: 0.763301, acc.: 57.03%] [G loss: 0.937238]\n",
      "epoch:23 step:21553 [D loss: 0.253753, acc.: 94.53%] [G loss: 3.119910]\n",
      "epoch:23 step:21554 [D loss: 0.583234, acc.: 70.31%] [G loss: 1.246236]\n",
      "epoch:23 step:21555 [D loss: 0.564885, acc.: 67.97%] [G loss: 1.221375]\n",
      "epoch:23 step:21556 [D loss: 0.351315, acc.: 85.94%] [G loss: 1.032452]\n",
      "epoch:23 step:21557 [D loss: 0.664223, acc.: 57.81%] [G loss: 0.665082]\n",
      "epoch:23 step:21558 [D loss: 0.260110, acc.: 94.53%] [G loss: 2.231212]\n",
      "epoch:23 step:21559 [D loss: 0.408342, acc.: 81.25%] [G loss: 1.676506]\n",
      "epoch:23 step:21560 [D loss: 0.277478, acc.: 89.84%] [G loss: 1.717700]\n",
      "epoch:23 step:21561 [D loss: 0.420331, acc.: 85.94%] [G loss: 2.931360]\n",
      "epoch:23 step:21562 [D loss: 0.377977, acc.: 79.69%] [G loss: 1.007795]\n",
      "epoch:23 step:21563 [D loss: 0.887547, acc.: 45.31%] [G loss: 0.502265]\n",
      "epoch:23 step:21564 [D loss: 0.293026, acc.: 90.62%] [G loss: 1.085162]\n",
      "epoch:23 step:21565 [D loss: 0.385416, acc.: 85.16%] [G loss: 1.826037]\n",
      "epoch:23 step:21566 [D loss: 0.268025, acc.: 93.75%] [G loss: 1.799276]\n",
      "epoch:23 step:21567 [D loss: 0.248123, acc.: 89.06%] [G loss: 2.897117]\n",
      "epoch:23 step:21568 [D loss: 0.286614, acc.: 90.62%] [G loss: 1.050827]\n",
      "epoch:23 step:21569 [D loss: 0.269877, acc.: 92.19%] [G loss: 1.944661]\n",
      "epoch:23 step:21570 [D loss: 0.705004, acc.: 60.94%] [G loss: 0.633894]\n",
      "epoch:23 step:21571 [D loss: 0.481441, acc.: 75.00%] [G loss: 1.094067]\n",
      "epoch:23 step:21572 [D loss: 0.389618, acc.: 84.38%] [G loss: 1.653844]\n",
      "epoch:23 step:21573 [D loss: 0.475008, acc.: 75.00%] [G loss: 1.057079]\n",
      "epoch:23 step:21574 [D loss: 0.605925, acc.: 65.62%] [G loss: 2.041346]\n",
      "epoch:23 step:21575 [D loss: 0.276279, acc.: 96.09%] [G loss: 0.812600]\n",
      "epoch:23 step:21576 [D loss: 0.436558, acc.: 79.69%] [G loss: 1.413464]\n",
      "epoch:23 step:21577 [D loss: 0.446385, acc.: 82.81%] [G loss: 0.745572]\n",
      "epoch:23 step:21578 [D loss: 0.256019, acc.: 94.53%] [G loss: 1.305452]\n",
      "epoch:23 step:21579 [D loss: 0.306310, acc.: 87.50%] [G loss: 2.832659]\n",
      "epoch:23 step:21580 [D loss: 0.679173, acc.: 60.94%] [G loss: 2.311725]\n",
      "epoch:23 step:21581 [D loss: 0.342613, acc.: 89.06%] [G loss: 2.280737]\n",
      "epoch:23 step:21582 [D loss: 0.589671, acc.: 68.75%] [G loss: 2.860524]\n",
      "epoch:23 step:21583 [D loss: 1.267024, acc.: 46.88%] [G loss: 1.683135]\n",
      "epoch:23 step:21584 [D loss: 0.104854, acc.: 98.44%] [G loss: 2.688631]\n",
      "epoch:23 step:21585 [D loss: 0.406771, acc.: 79.69%] [G loss: 1.830670]\n",
      "epoch:23 step:21586 [D loss: 0.355761, acc.: 80.47%] [G loss: 1.717469]\n",
      "epoch:23 step:21587 [D loss: 0.472219, acc.: 80.47%] [G loss: 0.460294]\n",
      "epoch:23 step:21588 [D loss: 0.688724, acc.: 62.50%] [G loss: 2.788167]\n",
      "epoch:23 step:21589 [D loss: 0.773651, acc.: 54.69%] [G loss: 0.910783]\n",
      "epoch:23 step:21590 [D loss: 0.508566, acc.: 72.66%] [G loss: 1.835304]\n",
      "epoch:23 step:21591 [D loss: 0.215120, acc.: 93.75%] [G loss: 1.963436]\n",
      "epoch:23 step:21592 [D loss: 0.557436, acc.: 68.75%] [G loss: 1.547125]\n",
      "epoch:23 step:21593 [D loss: 0.671395, acc.: 57.81%] [G loss: 1.087191]\n",
      "epoch:23 step:21594 [D loss: 0.339108, acc.: 89.84%] [G loss: 1.987581]\n",
      "epoch:23 step:21595 [D loss: 0.359398, acc.: 88.28%] [G loss: 2.183729]\n",
      "epoch:23 step:21596 [D loss: 0.590909, acc.: 66.41%] [G loss: 1.637550]\n",
      "epoch:23 step:21597 [D loss: 0.334560, acc.: 89.06%] [G loss: 2.397137]\n",
      "epoch:23 step:21598 [D loss: 0.656674, acc.: 56.25%] [G loss: 2.603960]\n",
      "epoch:23 step:21599 [D loss: 0.391401, acc.: 83.59%] [G loss: 1.595386]\n",
      "epoch:23 step:21600 [D loss: 0.332431, acc.: 89.06%] [G loss: 1.746850]\n",
      "epoch:23 step:21601 [D loss: 0.503721, acc.: 70.31%] [G loss: 2.179653]\n",
      "epoch:23 step:21602 [D loss: 0.318201, acc.: 93.75%] [G loss: 2.038455]\n",
      "epoch:23 step:21603 [D loss: 0.289917, acc.: 89.06%] [G loss: 1.305220]\n",
      "epoch:23 step:21604 [D loss: 0.241461, acc.: 95.31%] [G loss: 1.246564]\n",
      "epoch:23 step:21605 [D loss: 0.099757, acc.: 98.44%] [G loss: 3.053969]\n",
      "epoch:23 step:21606 [D loss: 0.589409, acc.: 69.53%] [G loss: 1.456243]\n",
      "epoch:23 step:21607 [D loss: 1.048325, acc.: 41.41%] [G loss: 1.918432]\n",
      "epoch:23 step:21608 [D loss: 0.728098, acc.: 56.25%] [G loss: 2.209456]\n",
      "epoch:23 step:21609 [D loss: 0.237167, acc.: 95.31%] [G loss: 1.657810]\n",
      "epoch:23 step:21610 [D loss: 0.357422, acc.: 90.62%] [G loss: 2.876447]\n",
      "epoch:23 step:21611 [D loss: 0.398527, acc.: 84.38%] [G loss: 1.751640]\n",
      "epoch:23 step:21612 [D loss: 0.319992, acc.: 86.72%] [G loss: 1.016516]\n",
      "epoch:23 step:21613 [D loss: 0.677774, acc.: 61.72%] [G loss: 0.763862]\n",
      "epoch:23 step:21614 [D loss: 0.282246, acc.: 92.19%] [G loss: 4.023403]\n",
      "epoch:23 step:21615 [D loss: 0.993772, acc.: 47.66%] [G loss: 1.433506]\n",
      "epoch:23 step:21616 [D loss: 0.433682, acc.: 78.12%] [G loss: 1.731841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21617 [D loss: 0.462981, acc.: 79.69%] [G loss: 3.663147]\n",
      "epoch:23 step:21618 [D loss: 0.421178, acc.: 88.28%] [G loss: 1.665921]\n",
      "epoch:23 step:21619 [D loss: 0.724007, acc.: 51.56%] [G loss: 2.786478]\n",
      "epoch:23 step:21620 [D loss: 0.289022, acc.: 92.97%] [G loss: 1.290104]\n",
      "epoch:23 step:21621 [D loss: 0.791815, acc.: 50.00%] [G loss: 2.180193]\n",
      "epoch:23 step:21622 [D loss: 1.186568, acc.: 41.41%] [G loss: 1.818152]\n",
      "epoch:23 step:21623 [D loss: 0.329153, acc.: 88.28%] [G loss: 1.214826]\n",
      "epoch:23 step:21624 [D loss: 0.252350, acc.: 93.75%] [G loss: 2.296735]\n",
      "epoch:23 step:21625 [D loss: 0.543309, acc.: 75.00%] [G loss: 1.428894]\n",
      "epoch:23 step:21626 [D loss: 0.945776, acc.: 47.66%] [G loss: 1.044314]\n",
      "epoch:23 step:21627 [D loss: 0.216362, acc.: 94.53%] [G loss: 1.303217]\n",
      "epoch:23 step:21628 [D loss: 0.226489, acc.: 93.75%] [G loss: 0.647605]\n",
      "epoch:23 step:21629 [D loss: 0.533458, acc.: 68.75%] [G loss: 3.229630]\n",
      "epoch:23 step:21630 [D loss: 0.223831, acc.: 94.53%] [G loss: 1.465188]\n",
      "epoch:23 step:21631 [D loss: 0.668561, acc.: 57.03%] [G loss: 3.155738]\n",
      "epoch:23 step:21632 [D loss: 0.153091, acc.: 98.44%] [G loss: 2.055799]\n",
      "epoch:23 step:21633 [D loss: 0.714900, acc.: 63.28%] [G loss: 1.783000]\n",
      "epoch:23 step:21634 [D loss: 0.210241, acc.: 96.09%] [G loss: 1.897746]\n",
      "epoch:23 step:21635 [D loss: 0.936343, acc.: 42.19%] [G loss: 1.057877]\n",
      "epoch:23 step:21636 [D loss: 0.247936, acc.: 94.53%] [G loss: 2.638108]\n",
      "epoch:23 step:21637 [D loss: 0.476890, acc.: 77.34%] [G loss: 3.228696]\n",
      "epoch:23 step:21638 [D loss: 0.490741, acc.: 79.69%] [G loss: 1.648571]\n",
      "epoch:23 step:21639 [D loss: 0.702115, acc.: 65.62%] [G loss: 1.492383]\n",
      "epoch:23 step:21640 [D loss: 0.313244, acc.: 89.06%] [G loss: 2.575956]\n",
      "epoch:23 step:21641 [D loss: 0.512450, acc.: 72.66%] [G loss: 1.694225]\n",
      "epoch:23 step:21642 [D loss: 0.212300, acc.: 93.75%] [G loss: 1.260100]\n",
      "epoch:23 step:21643 [D loss: 1.184890, acc.: 39.84%] [G loss: 0.558497]\n",
      "epoch:23 step:21644 [D loss: 0.419088, acc.: 82.03%] [G loss: 1.781057]\n",
      "epoch:23 step:21645 [D loss: 0.391641, acc.: 82.03%] [G loss: 2.634367]\n",
      "epoch:23 step:21646 [D loss: 0.265579, acc.: 96.09%] [G loss: 2.509908]\n",
      "epoch:23 step:21647 [D loss: 0.453004, acc.: 78.91%] [G loss: 2.566517]\n",
      "epoch:23 step:21648 [D loss: 0.273260, acc.: 92.97%] [G loss: 1.879954]\n",
      "epoch:23 step:21649 [D loss: 0.551468, acc.: 71.09%] [G loss: 1.163097]\n",
      "epoch:23 step:21650 [D loss: 0.317854, acc.: 89.06%] [G loss: 1.586356]\n",
      "epoch:23 step:21651 [D loss: 0.212592, acc.: 96.88%] [G loss: 1.740466]\n",
      "epoch:23 step:21652 [D loss: 0.698221, acc.: 59.38%] [G loss: 2.349739]\n",
      "epoch:23 step:21653 [D loss: 0.594034, acc.: 63.28%] [G loss: 2.255817]\n",
      "epoch:23 step:21654 [D loss: 0.362881, acc.: 83.59%] [G loss: 1.124113]\n",
      "epoch:23 step:21655 [D loss: 0.564579, acc.: 68.75%] [G loss: 2.578299]\n",
      "epoch:23 step:21656 [D loss: 0.107200, acc.: 100.00%] [G loss: 1.054096]\n",
      "epoch:23 step:21657 [D loss: 0.638625, acc.: 61.72%] [G loss: 2.704523]\n",
      "epoch:23 step:21658 [D loss: 0.709799, acc.: 59.38%] [G loss: 1.688036]\n",
      "epoch:23 step:21659 [D loss: 0.653741, acc.: 67.19%] [G loss: 2.889243]\n",
      "epoch:23 step:21660 [D loss: 0.447317, acc.: 80.47%] [G loss: 2.811140]\n",
      "epoch:23 step:21661 [D loss: 0.585050, acc.: 73.44%] [G loss: 0.869014]\n",
      "epoch:23 step:21662 [D loss: 0.849872, acc.: 53.91%] [G loss: 0.552521]\n",
      "epoch:23 step:21663 [D loss: 0.510029, acc.: 74.22%] [G loss: 1.000625]\n",
      "epoch:23 step:21664 [D loss: 0.251911, acc.: 89.06%] [G loss: 2.484164]\n",
      "epoch:23 step:21665 [D loss: 0.325357, acc.: 89.06%] [G loss: 1.374311]\n",
      "epoch:23 step:21666 [D loss: 0.411483, acc.: 81.25%] [G loss: 1.396444]\n",
      "epoch:23 step:21667 [D loss: 0.375413, acc.: 78.91%] [G loss: 1.834340]\n",
      "epoch:23 step:21668 [D loss: 0.652781, acc.: 65.62%] [G loss: 1.749582]\n",
      "epoch:23 step:21669 [D loss: 0.388953, acc.: 77.34%] [G loss: 2.409560]\n",
      "epoch:23 step:21670 [D loss: 0.378397, acc.: 88.28%] [G loss: 1.407211]\n",
      "epoch:23 step:21671 [D loss: 0.399746, acc.: 87.50%] [G loss: 1.453721]\n",
      "epoch:23 step:21672 [D loss: 0.467468, acc.: 75.78%] [G loss: 1.107504]\n",
      "epoch:23 step:21673 [D loss: 0.344017, acc.: 88.28%] [G loss: 2.752710]\n",
      "epoch:23 step:21674 [D loss: 0.390496, acc.: 84.38%] [G loss: 1.901217]\n",
      "epoch:23 step:21675 [D loss: 0.927582, acc.: 35.94%] [G loss: 1.797367]\n",
      "epoch:23 step:21676 [D loss: 0.513453, acc.: 73.44%] [G loss: 2.355192]\n",
      "epoch:23 step:21677 [D loss: 0.839889, acc.: 54.69%] [G loss: 1.693257]\n",
      "epoch:23 step:21678 [D loss: 0.535488, acc.: 76.56%] [G loss: 2.918081]\n",
      "epoch:23 step:21679 [D loss: 0.591284, acc.: 67.19%] [G loss: 2.312319]\n",
      "epoch:23 step:21680 [D loss: 0.377409, acc.: 89.06%] [G loss: 1.547647]\n",
      "epoch:23 step:21681 [D loss: 0.466851, acc.: 79.69%] [G loss: 2.268929]\n",
      "epoch:23 step:21682 [D loss: 0.428759, acc.: 82.03%] [G loss: 0.722274]\n",
      "epoch:23 step:21683 [D loss: 1.066746, acc.: 39.06%] [G loss: 2.028678]\n",
      "epoch:23 step:21684 [D loss: 0.597389, acc.: 64.84%] [G loss: 1.783744]\n",
      "epoch:23 step:21685 [D loss: 0.891342, acc.: 46.09%] [G loss: 1.245691]\n",
      "epoch:23 step:21686 [D loss: 0.725012, acc.: 53.91%] [G loss: 1.116084]\n",
      "epoch:23 step:21687 [D loss: 0.466879, acc.: 77.34%] [G loss: 1.344898]\n",
      "epoch:23 step:21688 [D loss: 0.348045, acc.: 85.94%] [G loss: 1.472638]\n",
      "epoch:23 step:21689 [D loss: 0.570518, acc.: 65.62%] [G loss: 1.411680]\n",
      "epoch:23 step:21690 [D loss: 0.553527, acc.: 77.34%] [G loss: 1.966232]\n",
      "epoch:23 step:21691 [D loss: 0.469407, acc.: 78.12%] [G loss: 1.948234]\n",
      "epoch:23 step:21692 [D loss: 0.225656, acc.: 94.53%] [G loss: 1.832366]\n",
      "epoch:23 step:21693 [D loss: 0.619607, acc.: 67.19%] [G loss: 1.585373]\n",
      "epoch:23 step:21694 [D loss: 0.288076, acc.: 94.53%] [G loss: 1.928258]\n",
      "epoch:23 step:21695 [D loss: 0.434758, acc.: 82.03%] [G loss: 2.088524]\n",
      "epoch:23 step:21696 [D loss: 0.424770, acc.: 75.78%] [G loss: 1.616218]\n",
      "epoch:23 step:21697 [D loss: 0.315435, acc.: 86.72%] [G loss: 1.767483]\n",
      "epoch:23 step:21698 [D loss: 0.439371, acc.: 81.25%] [G loss: 1.223280]\n",
      "epoch:23 step:21699 [D loss: 0.899270, acc.: 39.84%] [G loss: 0.963335]\n",
      "epoch:23 step:21700 [D loss: 0.305458, acc.: 92.19%] [G loss: 1.953784]\n",
      "epoch:23 step:21701 [D loss: 0.249489, acc.: 95.31%] [G loss: 0.912345]\n",
      "epoch:23 step:21702 [D loss: 1.172077, acc.: 23.44%] [G loss: 1.044395]\n",
      "epoch:23 step:21703 [D loss: 0.327319, acc.: 92.97%] [G loss: 1.970723]\n",
      "epoch:23 step:21704 [D loss: 0.918508, acc.: 50.00%] [G loss: 1.833520]\n",
      "epoch:23 step:21705 [D loss: 0.548325, acc.: 67.97%] [G loss: 1.552863]\n",
      "epoch:23 step:21706 [D loss: 0.250896, acc.: 94.53%] [G loss: 2.725051]\n",
      "epoch:23 step:21707 [D loss: 0.505523, acc.: 78.91%] [G loss: 2.338704]\n",
      "epoch:23 step:21708 [D loss: 0.706339, acc.: 57.03%] [G loss: 1.317259]\n",
      "epoch:23 step:21709 [D loss: 0.481150, acc.: 79.69%] [G loss: 1.783970]\n",
      "epoch:23 step:21710 [D loss: 0.439449, acc.: 80.47%] [G loss: 1.590040]\n",
      "epoch:23 step:21711 [D loss: 0.282228, acc.: 92.97%] [G loss: 2.803762]\n",
      "epoch:23 step:21712 [D loss: 0.595863, acc.: 64.84%] [G loss: 1.308143]\n",
      "epoch:23 step:21713 [D loss: 0.423433, acc.: 88.28%] [G loss: 0.969273]\n",
      "epoch:23 step:21714 [D loss: 0.185989, acc.: 95.31%] [G loss: 0.672158]\n",
      "epoch:23 step:21715 [D loss: 0.297469, acc.: 95.31%] [G loss: 2.496223]\n",
      "epoch:23 step:21716 [D loss: 0.342445, acc.: 87.50%] [G loss: 0.719717]\n",
      "epoch:23 step:21717 [D loss: 0.492573, acc.: 73.44%] [G loss: 1.201634]\n",
      "epoch:23 step:21718 [D loss: 0.190028, acc.: 97.66%] [G loss: 1.123222]\n",
      "epoch:23 step:21719 [D loss: 0.412255, acc.: 78.12%] [G loss: 1.324629]\n",
      "epoch:23 step:21720 [D loss: 0.354484, acc.: 89.06%] [G loss: 1.645230]\n",
      "epoch:23 step:21721 [D loss: 0.611143, acc.: 67.97%] [G loss: 0.603453]\n",
      "epoch:23 step:21722 [D loss: 1.006249, acc.: 35.16%] [G loss: 2.311646]\n",
      "epoch:23 step:21723 [D loss: 1.055052, acc.: 38.28%] [G loss: 0.948691]\n",
      "epoch:23 step:21724 [D loss: 0.632632, acc.: 64.84%] [G loss: 2.148237]\n",
      "epoch:23 step:21725 [D loss: 0.472673, acc.: 81.25%] [G loss: 1.208262]\n",
      "epoch:23 step:21726 [D loss: 0.340416, acc.: 89.84%] [G loss: 2.136365]\n",
      "epoch:23 step:21727 [D loss: 0.528609, acc.: 75.00%] [G loss: 0.602801]\n",
      "epoch:23 step:21728 [D loss: 0.454716, acc.: 83.59%] [G loss: 1.388824]\n",
      "epoch:23 step:21729 [D loss: 0.212047, acc.: 96.09%] [G loss: 2.417896]\n",
      "epoch:23 step:21730 [D loss: 0.355453, acc.: 87.50%] [G loss: 2.514574]\n",
      "epoch:23 step:21731 [D loss: 0.149166, acc.: 100.00%] [G loss: 2.329441]\n",
      "epoch:23 step:21732 [D loss: 0.383688, acc.: 85.94%] [G loss: 2.344269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21733 [D loss: 0.204715, acc.: 96.88%] [G loss: 2.807057]\n",
      "epoch:23 step:21734 [D loss: 0.391864, acc.: 84.38%] [G loss: 2.424862]\n",
      "epoch:23 step:21735 [D loss: 0.231880, acc.: 92.19%] [G loss: 1.037774]\n",
      "epoch:23 step:21736 [D loss: 0.311440, acc.: 92.97%] [G loss: 0.969411]\n",
      "epoch:23 step:21737 [D loss: 0.391066, acc.: 89.06%] [G loss: 1.993578]\n",
      "epoch:23 step:21738 [D loss: 0.405956, acc.: 79.69%] [G loss: 2.641654]\n",
      "epoch:23 step:21739 [D loss: 0.277283, acc.: 92.97%] [G loss: 2.979644]\n",
      "epoch:23 step:21740 [D loss: 0.256981, acc.: 94.53%] [G loss: 1.617833]\n",
      "epoch:23 step:21741 [D loss: 0.471969, acc.: 85.16%] [G loss: 2.529482]\n",
      "epoch:23 step:21742 [D loss: 0.588853, acc.: 69.53%] [G loss: 0.986942]\n",
      "epoch:23 step:21743 [D loss: 0.416373, acc.: 77.34%] [G loss: 3.135981]\n",
      "epoch:23 step:21744 [D loss: 0.371437, acc.: 84.38%] [G loss: 1.701292]\n",
      "epoch:23 step:21745 [D loss: 0.825092, acc.: 60.94%] [G loss: 1.896332]\n",
      "epoch:23 step:21746 [D loss: 0.725888, acc.: 61.72%] [G loss: 1.724917]\n",
      "epoch:23 step:21747 [D loss: 0.773810, acc.: 50.00%] [G loss: 2.686720]\n",
      "epoch:23 step:21748 [D loss: 0.408668, acc.: 88.28%] [G loss: 2.263129]\n",
      "epoch:23 step:21749 [D loss: 0.218242, acc.: 96.88%] [G loss: 0.634627]\n",
      "epoch:23 step:21750 [D loss: 0.468369, acc.: 71.09%] [G loss: 2.332367]\n",
      "epoch:23 step:21751 [D loss: 0.154361, acc.: 96.88%] [G loss: 2.651670]\n",
      "epoch:23 step:21752 [D loss: 0.256157, acc.: 92.97%] [G loss: 2.430896]\n",
      "epoch:23 step:21753 [D loss: 0.373977, acc.: 84.38%] [G loss: 1.837339]\n",
      "epoch:23 step:21754 [D loss: 0.433299, acc.: 82.81%] [G loss: 3.478618]\n",
      "epoch:23 step:21755 [D loss: 0.560442, acc.: 66.41%] [G loss: 1.615868]\n",
      "epoch:23 step:21756 [D loss: 0.315694, acc.: 83.59%] [G loss: 2.783753]\n",
      "epoch:23 step:21757 [D loss: 0.676257, acc.: 69.53%] [G loss: 1.658892]\n",
      "epoch:23 step:21758 [D loss: 0.424274, acc.: 86.72%] [G loss: 1.395375]\n",
      "epoch:23 step:21759 [D loss: 0.724394, acc.: 62.50%] [G loss: 1.097432]\n",
      "epoch:23 step:21760 [D loss: 0.850124, acc.: 40.62%] [G loss: 2.118051]\n",
      "epoch:23 step:21761 [D loss: 0.810981, acc.: 53.91%] [G loss: 1.976775]\n",
      "epoch:23 step:21762 [D loss: 0.733782, acc.: 60.94%] [G loss: 1.090946]\n",
      "epoch:23 step:21763 [D loss: 0.383178, acc.: 83.59%] [G loss: 1.528937]\n",
      "epoch:23 step:21764 [D loss: 0.430617, acc.: 82.03%] [G loss: 1.349585]\n",
      "epoch:23 step:21765 [D loss: 1.128071, acc.: 34.38%] [G loss: 1.777387]\n",
      "epoch:23 step:21766 [D loss: 0.401056, acc.: 81.25%] [G loss: 2.654805]\n",
      "epoch:23 step:21767 [D loss: 0.426647, acc.: 85.94%] [G loss: 1.993210]\n",
      "epoch:23 step:21768 [D loss: 0.618582, acc.: 64.84%] [G loss: 2.576669]\n",
      "epoch:23 step:21769 [D loss: 0.538042, acc.: 73.44%] [G loss: 1.742084]\n",
      "epoch:23 step:21770 [D loss: 0.610556, acc.: 69.53%] [G loss: 1.908749]\n",
      "epoch:23 step:21771 [D loss: 0.696286, acc.: 64.06%] [G loss: 1.087901]\n",
      "epoch:23 step:21772 [D loss: 0.818474, acc.: 56.25%] [G loss: 2.111076]\n",
      "epoch:23 step:21773 [D loss: 0.724783, acc.: 63.28%] [G loss: 1.825680]\n",
      "epoch:23 step:21774 [D loss: 0.139588, acc.: 98.44%] [G loss: 0.956851]\n",
      "epoch:23 step:21775 [D loss: 0.222938, acc.: 94.53%] [G loss: 1.072129]\n",
      "epoch:23 step:21776 [D loss: 0.501268, acc.: 77.34%] [G loss: 3.021161]\n",
      "epoch:23 step:21777 [D loss: 0.299478, acc.: 89.84%] [G loss: 1.998944]\n",
      "epoch:23 step:21778 [D loss: 0.554561, acc.: 67.19%] [G loss: 0.800798]\n",
      "epoch:23 step:21779 [D loss: 0.652171, acc.: 64.06%] [G loss: 0.910742]\n",
      "epoch:23 step:21780 [D loss: 0.888428, acc.: 50.00%] [G loss: 1.253668]\n",
      "epoch:23 step:21781 [D loss: 0.260562, acc.: 94.53%] [G loss: 3.458839]\n",
      "epoch:23 step:21782 [D loss: 0.247116, acc.: 96.09%] [G loss: 2.398576]\n",
      "epoch:23 step:21783 [D loss: 0.251056, acc.: 91.41%] [G loss: 2.411768]\n",
      "epoch:23 step:21784 [D loss: 0.471261, acc.: 79.69%] [G loss: 1.370565]\n",
      "epoch:23 step:21785 [D loss: 0.511127, acc.: 72.66%] [G loss: 3.689907]\n",
      "epoch:23 step:21786 [D loss: 0.484909, acc.: 78.91%] [G loss: 4.277485]\n",
      "epoch:23 step:21787 [D loss: 0.171106, acc.: 97.66%] [G loss: 1.610489]\n",
      "epoch:23 step:21788 [D loss: 0.402718, acc.: 79.69%] [G loss: 1.775867]\n",
      "epoch:23 step:21789 [D loss: 0.960191, acc.: 35.16%] [G loss: 0.883590]\n",
      "epoch:23 step:21790 [D loss: 0.989782, acc.: 43.75%] [G loss: 2.138390]\n",
      "epoch:23 step:21791 [D loss: 0.612129, acc.: 65.62%] [G loss: 1.920124]\n",
      "epoch:23 step:21792 [D loss: 1.268595, acc.: 22.66%] [G loss: 0.976008]\n",
      "epoch:23 step:21793 [D loss: 0.472605, acc.: 78.91%] [G loss: 0.891490]\n",
      "epoch:23 step:21794 [D loss: 0.564080, acc.: 68.75%] [G loss: 2.885540]\n",
      "epoch:23 step:21795 [D loss: 0.209016, acc.: 95.31%] [G loss: 1.369741]\n",
      "epoch:23 step:21796 [D loss: 0.904072, acc.: 42.19%] [G loss: 2.427977]\n",
      "epoch:23 step:21797 [D loss: 0.498527, acc.: 77.34%] [G loss: 2.025736]\n",
      "epoch:23 step:21798 [D loss: 0.426261, acc.: 82.81%] [G loss: 1.369350]\n",
      "epoch:23 step:21799 [D loss: 0.243376, acc.: 93.75%] [G loss: 1.915706]\n",
      "epoch:23 step:21800 [D loss: 0.421710, acc.: 85.16%] [G loss: 3.047898]\n",
      "epoch:23 step:21801 [D loss: 0.194993, acc.: 95.31%] [G loss: 1.521726]\n",
      "epoch:23 step:21802 [D loss: 0.461567, acc.: 83.59%] [G loss: 1.627656]\n",
      "epoch:23 step:21803 [D loss: 0.395814, acc.: 87.50%] [G loss: 0.767977]\n",
      "epoch:23 step:21804 [D loss: 1.023088, acc.: 33.59%] [G loss: 2.144880]\n",
      "epoch:23 step:21805 [D loss: 0.477230, acc.: 76.56%] [G loss: 1.508408]\n",
      "epoch:23 step:21806 [D loss: 0.361505, acc.: 87.50%] [G loss: 1.421078]\n",
      "epoch:23 step:21807 [D loss: 0.404873, acc.: 81.25%] [G loss: 1.839401]\n",
      "epoch:23 step:21808 [D loss: 0.503304, acc.: 79.69%] [G loss: 2.099326]\n",
      "epoch:23 step:21809 [D loss: 0.719820, acc.: 51.56%] [G loss: 1.570895]\n",
      "epoch:23 step:21810 [D loss: 0.462785, acc.: 77.34%] [G loss: 1.880853]\n",
      "epoch:23 step:21811 [D loss: 0.296785, acc.: 92.19%] [G loss: 0.793684]\n",
      "epoch:23 step:21812 [D loss: 0.335520, acc.: 89.84%] [G loss: 2.103126]\n",
      "epoch:23 step:21813 [D loss: 0.496246, acc.: 76.56%] [G loss: 1.313503]\n",
      "epoch:23 step:21814 [D loss: 0.271656, acc.: 91.41%] [G loss: 2.096950]\n",
      "epoch:23 step:21815 [D loss: 0.439980, acc.: 81.25%] [G loss: 3.158803]\n",
      "epoch:23 step:21816 [D loss: 0.349714, acc.: 89.06%] [G loss: 1.005573]\n",
      "epoch:23 step:21817 [D loss: 0.678905, acc.: 62.50%] [G loss: 2.537431]\n",
      "epoch:23 step:21818 [D loss: 0.623432, acc.: 65.62%] [G loss: 1.604089]\n",
      "epoch:23 step:21819 [D loss: 0.517739, acc.: 70.31%] [G loss: 1.674535]\n",
      "epoch:23 step:21820 [D loss: 0.728921, acc.: 58.59%] [G loss: 0.867852]\n",
      "epoch:23 step:21821 [D loss: 0.230143, acc.: 94.53%] [G loss: 1.508307]\n",
      "epoch:23 step:21822 [D loss: 0.300506, acc.: 91.41%] [G loss: 2.947092]\n",
      "epoch:23 step:21823 [D loss: 0.416082, acc.: 86.72%] [G loss: 0.831013]\n",
      "epoch:23 step:21824 [D loss: 0.269874, acc.: 94.53%] [G loss: 1.809096]\n",
      "epoch:23 step:21825 [D loss: 0.277343, acc.: 91.41%] [G loss: 3.216580]\n",
      "epoch:23 step:21826 [D loss: 0.337897, acc.: 86.72%] [G loss: 1.535202]\n",
      "epoch:23 step:21827 [D loss: 0.217392, acc.: 95.31%] [G loss: 1.858062]\n",
      "epoch:23 step:21828 [D loss: 0.600679, acc.: 61.72%] [G loss: 0.883153]\n",
      "epoch:23 step:21829 [D loss: 0.411283, acc.: 82.81%] [G loss: 1.151537]\n",
      "epoch:23 step:21830 [D loss: 0.793330, acc.: 51.56%] [G loss: 0.909254]\n",
      "epoch:23 step:21831 [D loss: 0.910213, acc.: 46.88%] [G loss: 1.706609]\n",
      "epoch:23 step:21832 [D loss: 0.417524, acc.: 85.16%] [G loss: 1.431084]\n",
      "epoch:23 step:21833 [D loss: 0.345574, acc.: 86.72%] [G loss: 2.007170]\n",
      "epoch:23 step:21834 [D loss: 0.374623, acc.: 86.72%] [G loss: 2.523739]\n",
      "epoch:23 step:21835 [D loss: 0.926484, acc.: 40.62%] [G loss: 2.306131]\n",
      "epoch:23 step:21836 [D loss: 0.585576, acc.: 64.84%] [G loss: 2.565937]\n",
      "epoch:23 step:21837 [D loss: 0.442806, acc.: 78.91%] [G loss: 1.417721]\n",
      "epoch:23 step:21838 [D loss: 0.793477, acc.: 47.66%] [G loss: 2.171786]\n",
      "epoch:23 step:21839 [D loss: 0.769108, acc.: 56.25%] [G loss: 2.138316]\n",
      "epoch:23 step:21840 [D loss: 0.346687, acc.: 88.28%] [G loss: 1.659157]\n",
      "epoch:23 step:21841 [D loss: 0.339490, acc.: 89.06%] [G loss: 0.965090]\n",
      "epoch:23 step:21842 [D loss: 0.650906, acc.: 64.84%] [G loss: 1.333538]\n",
      "epoch:23 step:21843 [D loss: 0.730115, acc.: 57.03%] [G loss: 1.398950]\n",
      "epoch:23 step:21844 [D loss: 0.743949, acc.: 58.59%] [G loss: 0.891119]\n",
      "epoch:23 step:21845 [D loss: 0.621060, acc.: 68.75%] [G loss: 0.778807]\n",
      "epoch:23 step:21846 [D loss: 0.423153, acc.: 85.94%] [G loss: 2.343271]\n",
      "epoch:23 step:21847 [D loss: 0.706419, acc.: 59.38%] [G loss: 1.352118]\n",
      "epoch:23 step:21848 [D loss: 0.462312, acc.: 75.00%] [G loss: 2.554619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21849 [D loss: 0.797132, acc.: 46.09%] [G loss: 0.987617]\n",
      "epoch:23 step:21850 [D loss: 0.266445, acc.: 93.75%] [G loss: 2.620829]\n",
      "epoch:23 step:21851 [D loss: 0.719350, acc.: 64.84%] [G loss: 1.610137]\n",
      "epoch:23 step:21852 [D loss: 0.282294, acc.: 92.19%] [G loss: 1.545443]\n",
      "epoch:23 step:21853 [D loss: 0.198694, acc.: 95.31%] [G loss: 1.753949]\n",
      "epoch:23 step:21854 [D loss: 0.340236, acc.: 85.16%] [G loss: 3.073389]\n",
      "epoch:23 step:21855 [D loss: 1.616176, acc.: 16.41%] [G loss: 1.027530]\n",
      "epoch:23 step:21856 [D loss: 0.731628, acc.: 57.81%] [G loss: 0.608640]\n",
      "epoch:23 step:21857 [D loss: 0.306212, acc.: 94.53%] [G loss: 2.059286]\n",
      "epoch:23 step:21858 [D loss: 0.415696, acc.: 84.38%] [G loss: 0.925836]\n",
      "epoch:23 step:21859 [D loss: 0.905845, acc.: 50.00%] [G loss: 1.330355]\n",
      "epoch:23 step:21860 [D loss: 0.436943, acc.: 78.91%] [G loss: 2.488111]\n",
      "epoch:23 step:21861 [D loss: 0.443898, acc.: 82.03%] [G loss: 1.872006]\n",
      "epoch:23 step:21862 [D loss: 0.236312, acc.: 92.97%] [G loss: 2.590587]\n",
      "epoch:23 step:21863 [D loss: 0.696549, acc.: 59.38%] [G loss: 2.940840]\n",
      "epoch:23 step:21864 [D loss: 0.281560, acc.: 92.97%] [G loss: 2.469452]\n",
      "epoch:23 step:21865 [D loss: 0.335029, acc.: 90.62%] [G loss: 1.930923]\n",
      "epoch:23 step:21866 [D loss: 0.650736, acc.: 61.72%] [G loss: 1.988423]\n",
      "epoch:23 step:21867 [D loss: 0.546094, acc.: 71.88%] [G loss: 1.829525]\n",
      "epoch:23 step:21868 [D loss: 0.635052, acc.: 64.06%] [G loss: 1.570788]\n",
      "epoch:23 step:21869 [D loss: 0.377769, acc.: 83.59%] [G loss: 1.625951]\n",
      "epoch:23 step:21870 [D loss: 0.512401, acc.: 76.56%] [G loss: 1.660684]\n",
      "epoch:23 step:21871 [D loss: 0.365042, acc.: 88.28%] [G loss: 1.180837]\n",
      "epoch:23 step:21872 [D loss: 0.711354, acc.: 58.59%] [G loss: 1.319189]\n",
      "epoch:23 step:21873 [D loss: 0.617098, acc.: 65.62%] [G loss: 1.063876]\n",
      "epoch:23 step:21874 [D loss: 0.374670, acc.: 87.50%] [G loss: 1.967770]\n",
      "epoch:23 step:21875 [D loss: 0.289404, acc.: 94.53%] [G loss: 1.842462]\n",
      "epoch:23 step:21876 [D loss: 0.429788, acc.: 84.38%] [G loss: 2.246438]\n",
      "epoch:23 step:21877 [D loss: 0.292403, acc.: 89.84%] [G loss: 1.647373]\n",
      "epoch:23 step:21878 [D loss: 0.300259, acc.: 94.53%] [G loss: 2.601556]\n",
      "epoch:23 step:21879 [D loss: 0.184156, acc.: 97.66%] [G loss: 1.469716]\n",
      "epoch:23 step:21880 [D loss: 0.364215, acc.: 89.06%] [G loss: 1.766617]\n",
      "epoch:23 step:21881 [D loss: 0.395835, acc.: 85.16%] [G loss: 2.725357]\n",
      "epoch:23 step:21882 [D loss: 0.470017, acc.: 81.25%] [G loss: 0.699629]\n",
      "epoch:23 step:21883 [D loss: 0.580499, acc.: 65.62%] [G loss: 0.951410]\n",
      "epoch:23 step:21884 [D loss: 0.314952, acc.: 90.62%] [G loss: 1.168256]\n",
      "epoch:23 step:21885 [D loss: 0.484745, acc.: 78.12%] [G loss: 2.586980]\n",
      "epoch:23 step:21886 [D loss: 0.444607, acc.: 81.25%] [G loss: 3.358253]\n",
      "epoch:23 step:21887 [D loss: 0.197061, acc.: 93.75%] [G loss: 3.328484]\n",
      "epoch:23 step:21888 [D loss: 0.344131, acc.: 92.19%] [G loss: 0.742034]\n",
      "epoch:23 step:21889 [D loss: 0.386862, acc.: 84.38%] [G loss: 2.034544]\n",
      "epoch:23 step:21890 [D loss: 0.407138, acc.: 85.16%] [G loss: 1.511642]\n",
      "epoch:23 step:21891 [D loss: 0.383331, acc.: 82.03%] [G loss: 3.208330]\n",
      "epoch:23 step:21892 [D loss: 0.324470, acc.: 92.97%] [G loss: 1.462058]\n",
      "epoch:23 step:21893 [D loss: 0.082206, acc.: 100.00%] [G loss: 0.996223]\n",
      "epoch:23 step:21894 [D loss: 0.838775, acc.: 53.91%] [G loss: 0.540218]\n",
      "epoch:23 step:21895 [D loss: 1.501300, acc.: 49.22%] [G loss: 1.876204]\n",
      "epoch:23 step:21896 [D loss: 0.224022, acc.: 95.31%] [G loss: 2.449853]\n",
      "epoch:23 step:21897 [D loss: 0.321266, acc.: 85.94%] [G loss: 0.779817]\n",
      "epoch:23 step:21898 [D loss: 0.863407, acc.: 55.47%] [G loss: 1.735478]\n",
      "epoch:23 step:21899 [D loss: 0.454731, acc.: 78.91%] [G loss: 1.359928]\n",
      "epoch:23 step:21900 [D loss: 0.401428, acc.: 83.59%] [G loss: 0.975238]\n",
      "epoch:23 step:21901 [D loss: 1.038443, acc.: 42.19%] [G loss: 2.412773]\n",
      "epoch:23 step:21902 [D loss: 0.563108, acc.: 68.75%] [G loss: 1.782403]\n",
      "epoch:23 step:21903 [D loss: 0.565781, acc.: 69.53%] [G loss: 1.015779]\n",
      "epoch:23 step:21904 [D loss: 0.478972, acc.: 71.09%] [G loss: 1.337858]\n",
      "epoch:23 step:21905 [D loss: 0.809903, acc.: 53.12%] [G loss: 0.868892]\n",
      "epoch:23 step:21906 [D loss: 0.505272, acc.: 74.22%] [G loss: 1.210086]\n",
      "epoch:23 step:21907 [D loss: 0.594752, acc.: 63.28%] [G loss: 1.342462]\n",
      "epoch:23 step:21908 [D loss: 0.331021, acc.: 89.06%] [G loss: 1.713316]\n",
      "epoch:23 step:21909 [D loss: 0.496898, acc.: 71.88%] [G loss: 1.856762]\n",
      "epoch:23 step:21910 [D loss: 0.556835, acc.: 73.44%] [G loss: 2.232967]\n",
      "epoch:23 step:21911 [D loss: 0.605388, acc.: 60.94%] [G loss: 0.981048]\n",
      "epoch:23 step:21912 [D loss: 0.423822, acc.: 79.69%] [G loss: 1.251655]\n",
      "epoch:23 step:21913 [D loss: 0.643079, acc.: 64.06%] [G loss: 1.459694]\n",
      "epoch:23 step:21914 [D loss: 0.345983, acc.: 89.84%] [G loss: 1.733238]\n",
      "epoch:23 step:21915 [D loss: 0.854652, acc.: 41.41%] [G loss: 2.878545]\n",
      "epoch:23 step:21916 [D loss: 0.680762, acc.: 60.94%] [G loss: 2.603419]\n",
      "epoch:23 step:21917 [D loss: 0.673462, acc.: 55.47%] [G loss: 2.933167]\n",
      "epoch:23 step:21918 [D loss: 1.120057, acc.: 36.72%] [G loss: 1.331605]\n",
      "epoch:23 step:21919 [D loss: 0.614503, acc.: 61.72%] [G loss: 1.228292]\n",
      "epoch:23 step:21920 [D loss: 0.189370, acc.: 95.31%] [G loss: 1.140419]\n",
      "epoch:23 step:21921 [D loss: 0.575582, acc.: 64.84%] [G loss: 1.593695]\n",
      "epoch:23 step:21922 [D loss: 0.440883, acc.: 82.81%] [G loss: 1.734804]\n",
      "epoch:23 step:21923 [D loss: 0.233746, acc.: 96.09%] [G loss: 2.761944]\n",
      "epoch:23 step:21924 [D loss: 0.183438, acc.: 97.66%] [G loss: 1.163658]\n",
      "epoch:23 step:21925 [D loss: 0.553199, acc.: 67.19%] [G loss: 1.200346]\n",
      "epoch:23 step:21926 [D loss: 0.458223, acc.: 79.69%] [G loss: 2.198309]\n",
      "epoch:23 step:21927 [D loss: 0.441873, acc.: 82.03%] [G loss: 2.214340]\n",
      "epoch:23 step:21928 [D loss: 0.804899, acc.: 57.03%] [G loss: 1.753496]\n",
      "epoch:23 step:21929 [D loss: 0.278667, acc.: 96.09%] [G loss: 1.719555]\n",
      "epoch:23 step:21930 [D loss: 0.284772, acc.: 89.84%] [G loss: 1.721282]\n",
      "epoch:23 step:21931 [D loss: 1.104983, acc.: 43.75%] [G loss: 0.655529]\n",
      "epoch:23 step:21932 [D loss: 0.302000, acc.: 85.16%] [G loss: 1.933892]\n",
      "epoch:23 step:21933 [D loss: 0.407169, acc.: 84.38%] [G loss: 1.795616]\n",
      "epoch:23 step:21934 [D loss: 1.047573, acc.: 32.81%] [G loss: 0.993876]\n",
      "epoch:23 step:21935 [D loss: 0.514449, acc.: 74.22%] [G loss: 1.161745]\n",
      "epoch:23 step:21936 [D loss: 0.664186, acc.: 67.97%] [G loss: 1.580488]\n",
      "epoch:23 step:21937 [D loss: 0.430359, acc.: 86.72%] [G loss: 2.260979]\n",
      "epoch:23 step:21938 [D loss: 0.277920, acc.: 96.09%] [G loss: 1.431774]\n",
      "epoch:23 step:21939 [D loss: 0.535078, acc.: 67.97%] [G loss: 1.376904]\n",
      "epoch:23 step:21940 [D loss: 0.590226, acc.: 66.41%] [G loss: 1.994356]\n",
      "epoch:23 step:21941 [D loss: 0.761567, acc.: 50.00%] [G loss: 1.080272]\n",
      "epoch:23 step:21942 [D loss: 0.397499, acc.: 83.59%] [G loss: 2.075373]\n",
      "epoch:23 step:21943 [D loss: 0.386362, acc.: 84.38%] [G loss: 1.056710]\n",
      "epoch:23 step:21944 [D loss: 0.576231, acc.: 67.97%] [G loss: 1.552330]\n",
      "epoch:23 step:21945 [D loss: 0.377002, acc.: 90.62%] [G loss: 1.725284]\n",
      "epoch:23 step:21946 [D loss: 0.289558, acc.: 90.62%] [G loss: 1.003499]\n",
      "epoch:23 step:21947 [D loss: 0.373303, acc.: 89.84%] [G loss: 1.858694]\n",
      "epoch:23 step:21948 [D loss: 0.218375, acc.: 94.53%] [G loss: 0.903113]\n",
      "epoch:23 step:21949 [D loss: 0.405474, acc.: 89.06%] [G loss: 1.246371]\n",
      "epoch:23 step:21950 [D loss: 0.652202, acc.: 60.94%] [G loss: 2.835777]\n",
      "epoch:23 step:21951 [D loss: 0.723325, acc.: 53.12%] [G loss: 2.115662]\n",
      "epoch:23 step:21952 [D loss: 0.371250, acc.: 84.38%] [G loss: 1.947485]\n",
      "epoch:23 step:21953 [D loss: 0.615009, acc.: 66.41%] [G loss: 1.513587]\n",
      "epoch:23 step:21954 [D loss: 0.280648, acc.: 91.41%] [G loss: 1.956352]\n",
      "epoch:23 step:21955 [D loss: 0.544632, acc.: 75.00%] [G loss: 0.749912]\n",
      "epoch:23 step:21956 [D loss: 0.621162, acc.: 65.62%] [G loss: 0.949548]\n",
      "epoch:23 step:21957 [D loss: 0.401570, acc.: 82.81%] [G loss: 2.111643]\n",
      "epoch:23 step:21958 [D loss: 0.360770, acc.: 87.50%] [G loss: 1.228569]\n",
      "epoch:23 step:21959 [D loss: 0.310902, acc.: 92.97%] [G loss: 1.730552]\n",
      "epoch:23 step:21960 [D loss: 0.391837, acc.: 82.81%] [G loss: 2.610260]\n",
      "epoch:23 step:21961 [D loss: 0.412580, acc.: 84.38%] [G loss: 0.687611]\n",
      "epoch:23 step:21962 [D loss: 0.328047, acc.: 89.06%] [G loss: 1.838018]\n",
      "epoch:23 step:21963 [D loss: 1.084519, acc.: 49.22%] [G loss: 1.939134]\n",
      "epoch:23 step:21964 [D loss: 0.415990, acc.: 80.47%] [G loss: 1.663758]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21965 [D loss: 0.420594, acc.: 85.16%] [G loss: 2.728249]\n",
      "epoch:23 step:21966 [D loss: 0.359442, acc.: 88.28%] [G loss: 1.824366]\n",
      "epoch:23 step:21967 [D loss: 0.364860, acc.: 86.72%] [G loss: 0.506188]\n",
      "epoch:23 step:21968 [D loss: 0.966588, acc.: 43.75%] [G loss: 0.976206]\n",
      "epoch:23 step:21969 [D loss: 0.217838, acc.: 96.88%] [G loss: 2.031349]\n",
      "epoch:23 step:21970 [D loss: 0.350179, acc.: 88.28%] [G loss: 2.185997]\n",
      "epoch:23 step:21971 [D loss: 0.805899, acc.: 45.31%] [G loss: 0.285472]\n",
      "epoch:23 step:21972 [D loss: 0.944787, acc.: 47.66%] [G loss: 2.959428]\n",
      "epoch:23 step:21973 [D loss: 0.612203, acc.: 67.97%] [G loss: 2.055852]\n",
      "epoch:23 step:21974 [D loss: 0.625879, acc.: 59.38%] [G loss: 1.659958]\n",
      "epoch:23 step:21975 [D loss: 0.742561, acc.: 65.62%] [G loss: 1.636082]\n",
      "epoch:23 step:21976 [D loss: 0.618983, acc.: 61.72%] [G loss: 2.311362]\n",
      "epoch:23 step:21977 [D loss: 0.613651, acc.: 64.06%] [G loss: 1.702591]\n",
      "epoch:23 step:21978 [D loss: 0.347897, acc.: 89.06%] [G loss: 2.774130]\n",
      "epoch:23 step:21979 [D loss: 0.556357, acc.: 67.97%] [G loss: 2.822759]\n",
      "epoch:23 step:21980 [D loss: 0.317538, acc.: 94.53%] [G loss: 2.433541]\n",
      "epoch:23 step:21981 [D loss: 0.732516, acc.: 58.59%] [G loss: 1.235759]\n",
      "epoch:23 step:21982 [D loss: 0.521016, acc.: 71.09%] [G loss: 1.994977]\n",
      "epoch:23 step:21983 [D loss: 0.618511, acc.: 60.94%] [G loss: 1.359643]\n",
      "epoch:23 step:21984 [D loss: 0.352075, acc.: 82.81%] [G loss: 0.812283]\n",
      "epoch:23 step:21985 [D loss: 0.509629, acc.: 73.44%] [G loss: 1.728640]\n",
      "epoch:23 step:21986 [D loss: 0.255225, acc.: 96.88%] [G loss: 2.549361]\n",
      "epoch:23 step:21987 [D loss: 0.482662, acc.: 77.34%] [G loss: 1.356401]\n",
      "epoch:23 step:21988 [D loss: 0.736287, acc.: 60.94%] [G loss: 1.349120]\n",
      "epoch:23 step:21989 [D loss: 0.486984, acc.: 83.59%] [G loss: 1.181659]\n",
      "epoch:23 step:21990 [D loss: 0.827055, acc.: 51.56%] [G loss: 1.308879]\n",
      "epoch:23 step:21991 [D loss: 0.423097, acc.: 74.22%] [G loss: 1.627621]\n",
      "epoch:23 step:21992 [D loss: 0.311299, acc.: 90.62%] [G loss: 1.444711]\n",
      "epoch:23 step:21993 [D loss: 0.264719, acc.: 95.31%] [G loss: 2.307627]\n",
      "epoch:23 step:21994 [D loss: 0.636349, acc.: 65.62%] [G loss: 2.075111]\n",
      "epoch:23 step:21995 [D loss: 0.543513, acc.: 69.53%] [G loss: 0.876133]\n",
      "epoch:23 step:21996 [D loss: 0.483669, acc.: 73.44%] [G loss: 3.020499]\n",
      "epoch:23 step:21997 [D loss: 0.312207, acc.: 92.19%] [G loss: 3.213662]\n",
      "epoch:23 step:21998 [D loss: 0.907549, acc.: 53.91%] [G loss: 1.237585]\n",
      "epoch:23 step:21999 [D loss: 0.295002, acc.: 92.19%] [G loss: 1.172412]\n",
      "epoch:23 step:22000 [D loss: 0.238263, acc.: 95.31%] [G loss: 2.778027]\n",
      "epoch:23 step:22001 [D loss: 0.620770, acc.: 64.84%] [G loss: 1.733268]\n",
      "epoch:23 step:22002 [D loss: 0.937608, acc.: 39.84%] [G loss: 2.211164]\n",
      "epoch:23 step:22003 [D loss: 0.593312, acc.: 62.50%] [G loss: 2.330334]\n",
      "epoch:23 step:22004 [D loss: 0.264717, acc.: 94.53%] [G loss: 1.911839]\n",
      "epoch:23 step:22005 [D loss: 0.366589, acc.: 85.94%] [G loss: 1.174974]\n",
      "epoch:23 step:22006 [D loss: 0.181108, acc.: 96.88%] [G loss: 1.487238]\n",
      "epoch:23 step:22007 [D loss: 0.394018, acc.: 80.47%] [G loss: 1.609277]\n",
      "epoch:23 step:22008 [D loss: 0.376067, acc.: 89.06%] [G loss: 0.814836]\n",
      "epoch:23 step:22009 [D loss: 0.739140, acc.: 55.47%] [G loss: 0.898613]\n",
      "epoch:23 step:22010 [D loss: 0.430437, acc.: 75.78%] [G loss: 3.612704]\n",
      "epoch:23 step:22011 [D loss: 0.639335, acc.: 64.06%] [G loss: 1.255490]\n",
      "epoch:23 step:22012 [D loss: 0.639982, acc.: 64.84%] [G loss: 1.347745]\n",
      "epoch:23 step:22013 [D loss: 0.338006, acc.: 86.72%] [G loss: 1.772518]\n",
      "epoch:23 step:22014 [D loss: 0.486419, acc.: 76.56%] [G loss: 1.902603]\n",
      "epoch:23 step:22015 [D loss: 0.207595, acc.: 96.88%] [G loss: 2.979797]\n",
      "epoch:23 step:22016 [D loss: 0.426920, acc.: 86.72%] [G loss: 0.774055]\n",
      "epoch:23 step:22017 [D loss: 0.331991, acc.: 84.38%] [G loss: 2.192791]\n",
      "epoch:23 step:22018 [D loss: 0.176262, acc.: 97.66%] [G loss: 2.165806]\n",
      "epoch:23 step:22019 [D loss: 0.237441, acc.: 93.75%] [G loss: 2.721588]\n",
      "epoch:23 step:22020 [D loss: 0.599129, acc.: 64.84%] [G loss: 1.978286]\n",
      "epoch:23 step:22021 [D loss: 0.701901, acc.: 61.72%] [G loss: 1.214597]\n",
      "epoch:23 step:22022 [D loss: 0.292324, acc.: 92.97%] [G loss: 1.554315]\n",
      "epoch:23 step:22023 [D loss: 0.434458, acc.: 80.47%] [G loss: 2.728179]\n",
      "epoch:23 step:22024 [D loss: 0.623050, acc.: 60.16%] [G loss: 2.868556]\n",
      "epoch:23 step:22025 [D loss: 0.410513, acc.: 86.72%] [G loss: 2.825396]\n",
      "epoch:23 step:22026 [D loss: 0.143450, acc.: 96.09%] [G loss: 1.872325]\n",
      "epoch:23 step:22027 [D loss: 0.366129, acc.: 83.59%] [G loss: 2.043139]\n",
      "epoch:23 step:22028 [D loss: 0.454537, acc.: 74.22%] [G loss: 2.824114]\n",
      "epoch:23 step:22029 [D loss: 0.439301, acc.: 82.03%] [G loss: 2.078597]\n",
      "epoch:23 step:22030 [D loss: 1.022943, acc.: 32.81%] [G loss: 1.656134]\n",
      "epoch:23 step:22031 [D loss: 0.509088, acc.: 73.44%] [G loss: 1.686343]\n",
      "epoch:23 step:22032 [D loss: 0.618592, acc.: 71.88%] [G loss: 1.643310]\n",
      "epoch:23 step:22033 [D loss: 0.366224, acc.: 88.28%] [G loss: 4.057058]\n",
      "epoch:23 step:22034 [D loss: 1.044482, acc.: 59.38%] [G loss: 2.073621]\n",
      "epoch:23 step:22035 [D loss: 0.342471, acc.: 90.62%] [G loss: 1.780612]\n",
      "epoch:23 step:22036 [D loss: 0.853021, acc.: 55.47%] [G loss: 1.728813]\n",
      "epoch:23 step:22037 [D loss: 0.440649, acc.: 83.59%] [G loss: 1.244457]\n",
      "epoch:23 step:22038 [D loss: 0.288769, acc.: 92.19%] [G loss: 2.764098]\n",
      "epoch:23 step:22039 [D loss: 0.427890, acc.: 78.91%] [G loss: 1.992474]\n",
      "epoch:23 step:22040 [D loss: 0.797040, acc.: 56.25%] [G loss: 1.125790]\n",
      "epoch:23 step:22041 [D loss: 0.312000, acc.: 92.97%] [G loss: 0.956237]\n",
      "epoch:23 step:22042 [D loss: 0.196981, acc.: 99.22%] [G loss: 1.950248]\n",
      "epoch:23 step:22043 [D loss: 0.576742, acc.: 67.19%] [G loss: 0.354008]\n",
      "epoch:23 step:22044 [D loss: 0.425137, acc.: 83.59%] [G loss: 0.951497]\n",
      "epoch:23 step:22045 [D loss: 0.621608, acc.: 59.38%] [G loss: 2.247930]\n",
      "epoch:23 step:22046 [D loss: 0.536103, acc.: 72.66%] [G loss: 2.484214]\n",
      "epoch:23 step:22047 [D loss: 0.251390, acc.: 97.66%] [G loss: 1.674809]\n",
      "epoch:23 step:22048 [D loss: 0.296594, acc.: 91.41%] [G loss: 0.982228]\n",
      "epoch:23 step:22049 [D loss: 0.474814, acc.: 74.22%] [G loss: 6.061315]\n",
      "epoch:23 step:22050 [D loss: 0.581350, acc.: 67.19%] [G loss: 1.993113]\n",
      "epoch:23 step:22051 [D loss: 0.708413, acc.: 56.25%] [G loss: 1.866307]\n",
      "epoch:23 step:22052 [D loss: 0.622904, acc.: 62.50%] [G loss: 2.266874]\n",
      "epoch:23 step:22053 [D loss: 0.393835, acc.: 85.94%] [G loss: 0.546399]\n",
      "epoch:23 step:22054 [D loss: 0.645608, acc.: 67.97%] [G loss: 1.091656]\n",
      "epoch:23 step:22055 [D loss: 0.359966, acc.: 86.72%] [G loss: 1.485535]\n",
      "epoch:23 step:22056 [D loss: 0.456857, acc.: 71.09%] [G loss: 1.511408]\n",
      "epoch:23 step:22057 [D loss: 0.406924, acc.: 84.38%] [G loss: 1.346013]\n",
      "epoch:23 step:22058 [D loss: 0.895996, acc.: 39.06%] [G loss: 0.997370]\n",
      "epoch:23 step:22059 [D loss: 0.832000, acc.: 54.69%] [G loss: 1.953233]\n",
      "epoch:23 step:22060 [D loss: 0.356844, acc.: 85.94%] [G loss: 2.768960]\n",
      "epoch:23 step:22061 [D loss: 0.334501, acc.: 82.03%] [G loss: 3.003176]\n",
      "epoch:23 step:22062 [D loss: 0.519229, acc.: 78.12%] [G loss: 1.501689]\n",
      "epoch:23 step:22063 [D loss: 0.401626, acc.: 85.16%] [G loss: 2.311356]\n",
      "epoch:23 step:22064 [D loss: 0.886151, acc.: 44.53%] [G loss: 2.268785]\n",
      "epoch:23 step:22065 [D loss: 0.475062, acc.: 73.44%] [G loss: 3.731235]\n",
      "epoch:23 step:22066 [D loss: 0.714330, acc.: 57.03%] [G loss: 2.019329]\n",
      "epoch:23 step:22067 [D loss: 0.276098, acc.: 92.19%] [G loss: 0.920109]\n",
      "epoch:23 step:22068 [D loss: 0.610061, acc.: 67.19%] [G loss: 1.696291]\n",
      "epoch:23 step:22069 [D loss: 0.625176, acc.: 66.41%] [G loss: 1.145458]\n",
      "epoch:23 step:22070 [D loss: 0.744000, acc.: 53.12%] [G loss: 2.675182]\n",
      "epoch:23 step:22071 [D loss: 0.455018, acc.: 83.59%] [G loss: 3.402067]\n",
      "epoch:23 step:22072 [D loss: 0.186198, acc.: 98.44%] [G loss: 1.278028]\n",
      "epoch:23 step:22073 [D loss: 0.249896, acc.: 94.53%] [G loss: 3.001899]\n",
      "epoch:23 step:22074 [D loss: 0.432936, acc.: 82.81%] [G loss: 0.640212]\n",
      "epoch:23 step:22075 [D loss: 0.351952, acc.: 86.72%] [G loss: 2.126408]\n",
      "epoch:23 step:22076 [D loss: 0.703072, acc.: 60.94%] [G loss: 2.274888]\n",
      "epoch:23 step:22077 [D loss: 0.270011, acc.: 94.53%] [G loss: 1.435064]\n",
      "epoch:23 step:22078 [D loss: 0.434318, acc.: 85.16%] [G loss: 2.482517]\n",
      "epoch:23 step:22079 [D loss: 0.432422, acc.: 81.25%] [G loss: 2.482432]\n",
      "epoch:23 step:22080 [D loss: 0.361156, acc.: 85.16%] [G loss: 2.919119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22081 [D loss: 0.647005, acc.: 60.94%] [G loss: 1.780907]\n",
      "epoch:23 step:22082 [D loss: 0.847230, acc.: 54.69%] [G loss: 1.470387]\n",
      "epoch:23 step:22083 [D loss: 0.472282, acc.: 78.91%] [G loss: 1.382627]\n",
      "epoch:23 step:22084 [D loss: 0.331449, acc.: 84.38%] [G loss: 1.250386]\n",
      "epoch:23 step:22085 [D loss: 0.327007, acc.: 89.84%] [G loss: 1.188075]\n",
      "epoch:23 step:22086 [D loss: 0.305695, acc.: 92.97%] [G loss: 1.900625]\n",
      "epoch:23 step:22087 [D loss: 0.223202, acc.: 93.75%] [G loss: 2.415254]\n",
      "epoch:23 step:22088 [D loss: 0.345609, acc.: 93.75%] [G loss: 2.095605]\n",
      "epoch:23 step:22089 [D loss: 0.660368, acc.: 60.16%] [G loss: 1.738798]\n",
      "epoch:23 step:22090 [D loss: 0.386796, acc.: 93.75%] [G loss: 1.679198]\n",
      "epoch:23 step:22091 [D loss: 0.395277, acc.: 88.28%] [G loss: 2.078027]\n",
      "epoch:23 step:22092 [D loss: 0.140215, acc.: 99.22%] [G loss: 1.564935]\n",
      "epoch:23 step:22093 [D loss: 0.508347, acc.: 75.78%] [G loss: 2.013143]\n",
      "epoch:23 step:22094 [D loss: 0.341268, acc.: 87.50%] [G loss: 1.342206]\n",
      "epoch:23 step:22095 [D loss: 0.289906, acc.: 91.41%] [G loss: 1.490313]\n",
      "epoch:23 step:22096 [D loss: 0.373109, acc.: 82.03%] [G loss: 0.943182]\n",
      "epoch:23 step:22097 [D loss: 0.146076, acc.: 99.22%] [G loss: 2.455092]\n",
      "epoch:23 step:22098 [D loss: 0.531982, acc.: 71.88%] [G loss: 1.079315]\n",
      "epoch:23 step:22099 [D loss: 0.382055, acc.: 85.94%] [G loss: 2.544095]\n",
      "epoch:23 step:22100 [D loss: 0.288431, acc.: 91.41%] [G loss: 3.443925]\n",
      "epoch:23 step:22101 [D loss: 0.509503, acc.: 76.56%] [G loss: 1.172686]\n",
      "epoch:23 step:22102 [D loss: 0.473570, acc.: 75.78%] [G loss: 1.442808]\n",
      "epoch:23 step:22103 [D loss: 0.617258, acc.: 63.28%] [G loss: 2.384517]\n",
      "epoch:23 step:22104 [D loss: 0.467644, acc.: 79.69%] [G loss: 0.674051]\n",
      "epoch:23 step:22105 [D loss: 0.156785, acc.: 97.66%] [G loss: 2.195449]\n",
      "epoch:23 step:22106 [D loss: 0.252477, acc.: 92.97%] [G loss: 2.690470]\n",
      "epoch:23 step:22107 [D loss: 0.223507, acc.: 94.53%] [G loss: 1.163864]\n",
      "epoch:23 step:22108 [D loss: 0.650728, acc.: 62.50%] [G loss: 1.815874]\n",
      "epoch:23 step:22109 [D loss: 0.258080, acc.: 90.62%] [G loss: 1.910081]\n",
      "epoch:23 step:22110 [D loss: 0.483717, acc.: 72.66%] [G loss: 2.212656]\n",
      "epoch:23 step:22111 [D loss: 0.615686, acc.: 61.72%] [G loss: 0.646883]\n",
      "epoch:23 step:22112 [D loss: 0.777159, acc.: 56.25%] [G loss: 1.867455]\n",
      "epoch:23 step:22113 [D loss: 0.209509, acc.: 95.31%] [G loss: 2.205396]\n",
      "epoch:23 step:22114 [D loss: 0.352806, acc.: 81.25%] [G loss: 1.952738]\n",
      "epoch:23 step:22115 [D loss: 1.302346, acc.: 35.16%] [G loss: 3.161728]\n",
      "epoch:23 step:22116 [D loss: 0.600444, acc.: 67.19%] [G loss: 1.546323]\n",
      "epoch:23 step:22117 [D loss: 0.798126, acc.: 58.59%] [G loss: 1.084537]\n",
      "epoch:23 step:22118 [D loss: 0.287912, acc.: 90.62%] [G loss: 2.392334]\n",
      "epoch:23 step:22119 [D loss: 1.635150, acc.: 30.47%] [G loss: 1.003658]\n",
      "epoch:23 step:22120 [D loss: 0.654970, acc.: 58.59%] [G loss: 2.254321]\n",
      "epoch:23 step:22121 [D loss: 0.500300, acc.: 78.12%] [G loss: 1.321149]\n",
      "epoch:23 step:22122 [D loss: 0.453455, acc.: 83.59%] [G loss: 2.589445]\n",
      "epoch:23 step:22123 [D loss: 0.803042, acc.: 46.09%] [G loss: 1.693183]\n",
      "epoch:23 step:22124 [D loss: 0.410136, acc.: 82.81%] [G loss: 1.546560]\n",
      "epoch:23 step:22125 [D loss: 0.391868, acc.: 86.72%] [G loss: 2.045289]\n",
      "epoch:23 step:22126 [D loss: 0.455747, acc.: 74.22%] [G loss: 1.905692]\n",
      "epoch:23 step:22127 [D loss: 0.250800, acc.: 94.53%] [G loss: 2.550770]\n",
      "epoch:23 step:22128 [D loss: 0.247313, acc.: 95.31%] [G loss: 1.743677]\n",
      "epoch:23 step:22129 [D loss: 0.216515, acc.: 96.88%] [G loss: 3.056968]\n",
      "epoch:23 step:22130 [D loss: 0.709589, acc.: 60.94%] [G loss: 2.654564]\n",
      "epoch:23 step:22131 [D loss: 0.425312, acc.: 84.38%] [G loss: 1.643930]\n",
      "epoch:23 step:22132 [D loss: 0.538131, acc.: 72.66%] [G loss: 1.239333]\n",
      "epoch:23 step:22133 [D loss: 0.421398, acc.: 82.81%] [G loss: 2.803874]\n",
      "epoch:23 step:22134 [D loss: 0.389208, acc.: 88.28%] [G loss: 1.665743]\n",
      "epoch:23 step:22135 [D loss: 0.161414, acc.: 99.22%] [G loss: 1.143314]\n",
      "epoch:23 step:22136 [D loss: 0.130465, acc.: 100.00%] [G loss: 2.540443]\n",
      "epoch:23 step:22137 [D loss: 0.494022, acc.: 80.47%] [G loss: 1.804435]\n",
      "epoch:23 step:22138 [D loss: 0.667063, acc.: 57.81%] [G loss: 1.206558]\n",
      "epoch:23 step:22139 [D loss: 1.138082, acc.: 36.72%] [G loss: 1.243092]\n",
      "epoch:23 step:22140 [D loss: 0.263386, acc.: 96.09%] [G loss: 1.799318]\n",
      "epoch:23 step:22141 [D loss: 0.414938, acc.: 80.47%] [G loss: 2.284294]\n",
      "epoch:23 step:22142 [D loss: 0.695549, acc.: 57.81%] [G loss: 0.458685]\n",
      "epoch:23 step:22143 [D loss: 0.436473, acc.: 75.78%] [G loss: 1.368542]\n",
      "epoch:23 step:22144 [D loss: 0.557916, acc.: 67.97%] [G loss: 1.973553]\n",
      "epoch:23 step:22145 [D loss: 0.568065, acc.: 68.75%] [G loss: 0.821485]\n",
      "epoch:23 step:22146 [D loss: 0.505839, acc.: 75.78%] [G loss: 2.356774]\n",
      "epoch:23 step:22147 [D loss: 0.247509, acc.: 96.09%] [G loss: 2.181357]\n",
      "epoch:23 step:22148 [D loss: 0.794392, acc.: 53.91%] [G loss: 0.927207]\n",
      "epoch:23 step:22149 [D loss: 0.430855, acc.: 82.03%] [G loss: 0.957633]\n",
      "epoch:23 step:22150 [D loss: 0.216116, acc.: 96.09%] [G loss: 1.858665]\n",
      "epoch:23 step:22151 [D loss: 0.277347, acc.: 92.19%] [G loss: 1.883924]\n",
      "epoch:23 step:22152 [D loss: 0.731560, acc.: 54.69%] [G loss: 1.836085]\n",
      "epoch:23 step:22153 [D loss: 0.661223, acc.: 59.38%] [G loss: 1.127801]\n",
      "epoch:23 step:22154 [D loss: 0.245848, acc.: 94.53%] [G loss: 2.269475]\n",
      "epoch:23 step:22155 [D loss: 1.003385, acc.: 39.84%] [G loss: 2.570101]\n",
      "epoch:23 step:22156 [D loss: 0.378831, acc.: 86.72%] [G loss: 1.802133]\n",
      "epoch:23 step:22157 [D loss: 0.615263, acc.: 64.84%] [G loss: 2.797988]\n",
      "epoch:23 step:22158 [D loss: 0.618244, acc.: 66.41%] [G loss: 2.873715]\n",
      "epoch:23 step:22159 [D loss: 0.284223, acc.: 92.97%] [G loss: 2.017253]\n",
      "epoch:23 step:22160 [D loss: 0.376826, acc.: 85.16%] [G loss: 2.206462]\n",
      "epoch:23 step:22161 [D loss: 0.508091, acc.: 71.88%] [G loss: 0.820804]\n",
      "epoch:23 step:22162 [D loss: 0.511391, acc.: 75.00%] [G loss: 2.088254]\n",
      "epoch:23 step:22163 [D loss: 0.416507, acc.: 84.38%] [G loss: 0.799891]\n",
      "epoch:23 step:22164 [D loss: 0.577350, acc.: 64.06%] [G loss: 2.875206]\n",
      "epoch:23 step:22165 [D loss: 0.372356, acc.: 87.50%] [G loss: 2.239404]\n",
      "epoch:23 step:22166 [D loss: 0.227042, acc.: 96.88%] [G loss: 2.522262]\n",
      "epoch:23 step:22167 [D loss: 0.656922, acc.: 53.91%] [G loss: 2.764366]\n",
      "epoch:23 step:22168 [D loss: 0.353551, acc.: 85.94%] [G loss: 0.568236]\n",
      "epoch:23 step:22169 [D loss: 0.313128, acc.: 89.06%] [G loss: 1.194043]\n",
      "epoch:23 step:22170 [D loss: 0.834541, acc.: 50.78%] [G loss: 1.790988]\n",
      "epoch:23 step:22171 [D loss: 0.336303, acc.: 87.50%] [G loss: 2.021224]\n",
      "epoch:23 step:22172 [D loss: 0.697990, acc.: 61.72%] [G loss: 1.836133]\n",
      "epoch:23 step:22173 [D loss: 0.214474, acc.: 93.75%] [G loss: 1.649047]\n",
      "epoch:23 step:22174 [D loss: 0.887034, acc.: 50.78%] [G loss: 2.231104]\n",
      "epoch:23 step:22175 [D loss: 0.812767, acc.: 48.44%] [G loss: 2.654367]\n",
      "epoch:23 step:22176 [D loss: 0.531851, acc.: 75.78%] [G loss: 1.269790]\n",
      "epoch:23 step:22177 [D loss: 0.482886, acc.: 71.88%] [G loss: 2.588228]\n",
      "epoch:23 step:22178 [D loss: 0.234378, acc.: 93.75%] [G loss: 0.920987]\n",
      "epoch:23 step:22179 [D loss: 0.446427, acc.: 84.38%] [G loss: 0.570399]\n",
      "epoch:23 step:22180 [D loss: 0.302636, acc.: 95.31%] [G loss: 1.738827]\n",
      "epoch:23 step:22181 [D loss: 0.271437, acc.: 95.31%] [G loss: 2.440946]\n",
      "epoch:23 step:22182 [D loss: 1.200362, acc.: 50.00%] [G loss: 1.738327]\n",
      "epoch:23 step:22183 [D loss: 0.345909, acc.: 91.41%] [G loss: 2.087589]\n",
      "epoch:23 step:22184 [D loss: 0.897454, acc.: 56.25%] [G loss: 1.880920]\n",
      "epoch:23 step:22185 [D loss: 0.811090, acc.: 50.78%] [G loss: 1.720628]\n",
      "epoch:23 step:22186 [D loss: 0.632463, acc.: 65.62%] [G loss: 2.601213]\n",
      "epoch:23 step:22187 [D loss: 0.338866, acc.: 89.84%] [G loss: 2.861634]\n",
      "epoch:23 step:22188 [D loss: 0.309690, acc.: 91.41%] [G loss: 2.401877]\n",
      "epoch:23 step:22189 [D loss: 0.359108, acc.: 86.72%] [G loss: 2.930842]\n",
      "epoch:23 step:22190 [D loss: 0.605732, acc.: 69.53%] [G loss: 2.710124]\n",
      "epoch:23 step:22191 [D loss: 0.341755, acc.: 90.62%] [G loss: 1.568989]\n",
      "epoch:23 step:22192 [D loss: 0.458393, acc.: 83.59%] [G loss: 2.438660]\n",
      "epoch:23 step:22193 [D loss: 0.228414, acc.: 95.31%] [G loss: 3.024091]\n",
      "epoch:23 step:22194 [D loss: 0.376886, acc.: 77.34%] [G loss: 3.074923]\n",
      "epoch:23 step:22195 [D loss: 0.282706, acc.: 92.19%] [G loss: 2.009937]\n",
      "epoch:23 step:22196 [D loss: 0.251749, acc.: 96.09%] [G loss: 3.390908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22197 [D loss: 0.370911, acc.: 82.81%] [G loss: 2.393810]\n",
      "epoch:23 step:22198 [D loss: 0.885246, acc.: 51.56%] [G loss: 1.489140]\n",
      "epoch:23 step:22199 [D loss: 0.227136, acc.: 95.31%] [G loss: 0.956102]\n",
      "epoch:23 step:22200 [D loss: 0.664198, acc.: 65.62%] [G loss: 1.473679]\n",
      "epoch:23 step:22201 [D loss: 0.459730, acc.: 82.81%] [G loss: 2.194168]\n",
      "epoch:23 step:22202 [D loss: 0.424285, acc.: 82.81%] [G loss: 1.586930]\n",
      "epoch:23 step:22203 [D loss: 0.244050, acc.: 93.75%] [G loss: 3.321182]\n",
      "epoch:23 step:22204 [D loss: 0.226219, acc.: 92.19%] [G loss: 2.578121]\n",
      "epoch:23 step:22205 [D loss: 0.219149, acc.: 95.31%] [G loss: 2.310525]\n",
      "epoch:23 step:22206 [D loss: 0.629993, acc.: 61.72%] [G loss: 1.845025]\n",
      "epoch:23 step:22207 [D loss: 0.225553, acc.: 95.31%] [G loss: 0.732266]\n",
      "epoch:23 step:22208 [D loss: 0.502542, acc.: 74.22%] [G loss: 1.854003]\n",
      "epoch:23 step:22209 [D loss: 0.613216, acc.: 64.06%] [G loss: 1.387736]\n",
      "epoch:23 step:22210 [D loss: 0.832916, acc.: 54.69%] [G loss: 2.487542]\n",
      "epoch:23 step:22211 [D loss: 0.677820, acc.: 60.94%] [G loss: 0.965264]\n",
      "epoch:23 step:22212 [D loss: 0.544044, acc.: 72.66%] [G loss: 0.965048]\n",
      "epoch:23 step:22213 [D loss: 0.187058, acc.: 96.09%] [G loss: 0.829142]\n",
      "epoch:23 step:22214 [D loss: 0.766596, acc.: 59.38%] [G loss: 2.118338]\n",
      "epoch:23 step:22215 [D loss: 0.491074, acc.: 73.44%] [G loss: 1.043217]\n",
      "epoch:23 step:22216 [D loss: 0.510646, acc.: 71.09%] [G loss: 2.485882]\n",
      "epoch:23 step:22217 [D loss: 0.543092, acc.: 67.19%] [G loss: 1.670874]\n",
      "epoch:23 step:22218 [D loss: 0.444592, acc.: 76.56%] [G loss: 2.800554]\n",
      "epoch:23 step:22219 [D loss: 0.525007, acc.: 77.34%] [G loss: 2.432260]\n",
      "epoch:23 step:22220 [D loss: 0.205346, acc.: 98.44%] [G loss: 2.204809]\n",
      "epoch:23 step:22221 [D loss: 0.308210, acc.: 79.69%] [G loss: 1.883800]\n",
      "epoch:23 step:22222 [D loss: 0.437671, acc.: 79.69%] [G loss: 2.428781]\n",
      "epoch:23 step:22223 [D loss: 0.332317, acc.: 87.50%] [G loss: 1.945303]\n",
      "epoch:23 step:22224 [D loss: 0.276660, acc.: 94.53%] [G loss: 2.300723]\n",
      "epoch:23 step:22225 [D loss: 0.411639, acc.: 85.16%] [G loss: 2.062286]\n",
      "epoch:23 step:22226 [D loss: 0.320973, acc.: 85.16%] [G loss: 1.852025]\n",
      "epoch:23 step:22227 [D loss: 0.391541, acc.: 86.72%] [G loss: 1.784713]\n",
      "epoch:23 step:22228 [D loss: 0.367280, acc.: 84.38%] [G loss: 2.005012]\n",
      "epoch:23 step:22229 [D loss: 0.546254, acc.: 75.78%] [G loss: 2.251244]\n",
      "epoch:23 step:22230 [D loss: 0.340560, acc.: 91.41%] [G loss: 0.792486]\n",
      "epoch:23 step:22231 [D loss: 0.970020, acc.: 50.00%] [G loss: 1.565862]\n",
      "epoch:23 step:22232 [D loss: 0.313483, acc.: 90.62%] [G loss: 1.791238]\n",
      "epoch:23 step:22233 [D loss: 0.333599, acc.: 85.94%] [G loss: 3.012685]\n",
      "epoch:23 step:22234 [D loss: 1.403226, acc.: 37.50%] [G loss: 0.767333]\n",
      "epoch:23 step:22235 [D loss: 0.489414, acc.: 79.69%] [G loss: 1.363104]\n",
      "epoch:23 step:22236 [D loss: 0.812460, acc.: 56.25%] [G loss: 2.077533]\n",
      "epoch:23 step:22237 [D loss: 0.469852, acc.: 80.47%] [G loss: 2.397719]\n",
      "epoch:23 step:22238 [D loss: 0.445572, acc.: 82.81%] [G loss: 1.632034]\n",
      "epoch:23 step:22239 [D loss: 0.192681, acc.: 95.31%] [G loss: 2.210554]\n",
      "epoch:23 step:22240 [D loss: 0.635143, acc.: 64.84%] [G loss: 1.714376]\n",
      "epoch:23 step:22241 [D loss: 0.733163, acc.: 58.59%] [G loss: 0.790861]\n",
      "epoch:23 step:22242 [D loss: 0.450105, acc.: 73.44%] [G loss: 2.571050]\n",
      "epoch:23 step:22243 [D loss: 0.639984, acc.: 67.97%] [G loss: 1.614773]\n",
      "epoch:23 step:22244 [D loss: 0.895239, acc.: 39.84%] [G loss: 2.959803]\n",
      "epoch:23 step:22245 [D loss: 0.732227, acc.: 58.59%] [G loss: 1.562411]\n",
      "epoch:23 step:22246 [D loss: 0.458815, acc.: 78.91%] [G loss: 1.942276]\n",
      "epoch:23 step:22247 [D loss: 0.447326, acc.: 77.34%] [G loss: 1.297076]\n",
      "epoch:23 step:22248 [D loss: 0.596186, acc.: 67.19%] [G loss: 2.134478]\n",
      "epoch:23 step:22249 [D loss: 0.174450, acc.: 96.88%] [G loss: 1.342178]\n",
      "epoch:23 step:22250 [D loss: 0.197539, acc.: 96.88%] [G loss: 1.340727]\n",
      "epoch:23 step:22251 [D loss: 0.470988, acc.: 77.34%] [G loss: 2.395354]\n",
      "epoch:23 step:22252 [D loss: 0.522848, acc.: 75.00%] [G loss: 0.625410]\n",
      "epoch:23 step:22253 [D loss: 0.645714, acc.: 66.41%] [G loss: 1.250667]\n",
      "epoch:23 step:22254 [D loss: 0.511314, acc.: 73.44%] [G loss: 1.383768]\n",
      "epoch:23 step:22255 [D loss: 0.312570, acc.: 93.75%] [G loss: 2.283334]\n",
      "epoch:23 step:22256 [D loss: 0.381243, acc.: 86.72%] [G loss: 1.403291]\n",
      "epoch:23 step:22257 [D loss: 0.709023, acc.: 55.47%] [G loss: 1.941716]\n",
      "epoch:23 step:22258 [D loss: 0.248770, acc.: 95.31%] [G loss: 2.200389]\n",
      "epoch:23 step:22259 [D loss: 0.601073, acc.: 67.97%] [G loss: 1.658992]\n",
      "epoch:23 step:22260 [D loss: 0.371541, acc.: 84.38%] [G loss: 1.171273]\n",
      "epoch:23 step:22261 [D loss: 0.187667, acc.: 98.44%] [G loss: 1.342954]\n",
      "epoch:23 step:22262 [D loss: 0.773005, acc.: 58.59%] [G loss: 1.349439]\n",
      "epoch:23 step:22263 [D loss: 0.699153, acc.: 60.94%] [G loss: 3.447751]\n",
      "epoch:23 step:22264 [D loss: 0.287616, acc.: 88.28%] [G loss: 3.209633]\n",
      "epoch:23 step:22265 [D loss: 0.314664, acc.: 88.28%] [G loss: 1.511675]\n",
      "epoch:23 step:22266 [D loss: 0.559905, acc.: 68.75%] [G loss: 1.135730]\n",
      "epoch:23 step:22267 [D loss: 0.503664, acc.: 71.88%] [G loss: 1.187061]\n",
      "epoch:23 step:22268 [D loss: 0.858310, acc.: 53.12%] [G loss: 1.669086]\n",
      "epoch:23 step:22269 [D loss: 1.203267, acc.: 52.34%] [G loss: 2.246359]\n",
      "epoch:23 step:22270 [D loss: 0.351873, acc.: 83.59%] [G loss: 1.845069]\n",
      "epoch:23 step:22271 [D loss: 0.738663, acc.: 61.72%] [G loss: 1.146552]\n",
      "epoch:23 step:22272 [D loss: 0.549167, acc.: 64.84%] [G loss: 1.811705]\n",
      "epoch:23 step:22273 [D loss: 0.527534, acc.: 71.09%] [G loss: 2.719727]\n",
      "epoch:23 step:22274 [D loss: 0.386271, acc.: 87.50%] [G loss: 2.086458]\n",
      "epoch:23 step:22275 [D loss: 0.535595, acc.: 75.00%] [G loss: 1.322338]\n",
      "epoch:23 step:22276 [D loss: 0.470583, acc.: 84.38%] [G loss: 3.637444]\n",
      "epoch:23 step:22277 [D loss: 0.554369, acc.: 66.41%] [G loss: 0.820412]\n",
      "epoch:23 step:22278 [D loss: 1.013928, acc.: 35.16%] [G loss: 1.730872]\n",
      "epoch:23 step:22279 [D loss: 0.768547, acc.: 59.38%] [G loss: 1.566492]\n",
      "epoch:23 step:22280 [D loss: 0.296468, acc.: 91.41%] [G loss: 2.429806]\n",
      "epoch:23 step:22281 [D loss: 0.572063, acc.: 69.53%] [G loss: 4.067540]\n",
      "epoch:23 step:22282 [D loss: 0.631625, acc.: 67.19%] [G loss: 1.081778]\n",
      "epoch:23 step:22283 [D loss: 0.399190, acc.: 80.47%] [G loss: 0.864973]\n",
      "epoch:23 step:22284 [D loss: 0.199399, acc.: 96.88%] [G loss: 0.770999]\n",
      "epoch:23 step:22285 [D loss: 0.529555, acc.: 73.44%] [G loss: 1.583899]\n",
      "epoch:23 step:22286 [D loss: 0.403185, acc.: 87.50%] [G loss: 2.998364]\n",
      "epoch:23 step:22287 [D loss: 0.225439, acc.: 96.09%] [G loss: 1.894910]\n",
      "epoch:23 step:22288 [D loss: 0.671669, acc.: 60.16%] [G loss: 1.256653]\n",
      "epoch:23 step:22289 [D loss: 0.744220, acc.: 57.03%] [G loss: 1.519416]\n",
      "epoch:23 step:22290 [D loss: 0.512074, acc.: 77.34%] [G loss: 2.244843]\n",
      "epoch:23 step:22291 [D loss: 0.652564, acc.: 61.72%] [G loss: 1.051153]\n",
      "epoch:23 step:22292 [D loss: 0.331670, acc.: 85.94%] [G loss: 2.852892]\n",
      "epoch:23 step:22293 [D loss: 0.319184, acc.: 92.97%] [G loss: 2.321817]\n",
      "epoch:23 step:22294 [D loss: 0.428991, acc.: 82.81%] [G loss: 3.217621]\n",
      "epoch:23 step:22295 [D loss: 0.442946, acc.: 83.59%] [G loss: 1.273846]\n",
      "epoch:23 step:22296 [D loss: 0.242255, acc.: 92.19%] [G loss: 1.820593]\n",
      "epoch:23 step:22297 [D loss: 0.243624, acc.: 93.75%] [G loss: 2.298769]\n",
      "epoch:23 step:22298 [D loss: 0.404285, acc.: 82.03%] [G loss: 1.502720]\n",
      "epoch:23 step:22299 [D loss: 0.328080, acc.: 91.41%] [G loss: 2.454374]\n",
      "epoch:23 step:22300 [D loss: 0.445892, acc.: 81.25%] [G loss: 2.751218]\n",
      "epoch:23 step:22301 [D loss: 0.274851, acc.: 92.19%] [G loss: 2.529323]\n",
      "epoch:23 step:22302 [D loss: 0.760225, acc.: 53.91%] [G loss: 1.658393]\n",
      "epoch:23 step:22303 [D loss: 0.373784, acc.: 86.72%] [G loss: 2.995604]\n",
      "epoch:23 step:22304 [D loss: 0.194223, acc.: 97.66%] [G loss: 2.983527]\n",
      "epoch:23 step:22305 [D loss: 0.222502, acc.: 94.53%] [G loss: 1.954062]\n",
      "epoch:23 step:22306 [D loss: 0.517588, acc.: 66.41%] [G loss: 1.200954]\n",
      "epoch:23 step:22307 [D loss: 0.277355, acc.: 94.53%] [G loss: 0.686840]\n",
      "epoch:23 step:22308 [D loss: 1.132674, acc.: 42.97%] [G loss: 2.849286]\n",
      "epoch:23 step:22309 [D loss: 0.560032, acc.: 71.09%] [G loss: 1.663988]\n",
      "epoch:23 step:22310 [D loss: 0.973598, acc.: 47.66%] [G loss: 1.515354]\n",
      "epoch:23 step:22311 [D loss: 0.485988, acc.: 65.62%] [G loss: 1.464991]\n",
      "epoch:23 step:22312 [D loss: 0.278980, acc.: 95.31%] [G loss: 2.032489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22313 [D loss: 0.317534, acc.: 91.41%] [G loss: 1.141076]\n",
      "epoch:23 step:22314 [D loss: 0.859044, acc.: 51.56%] [G loss: 1.365048]\n",
      "epoch:23 step:22315 [D loss: 0.521029, acc.: 69.53%] [G loss: 0.742160]\n",
      "epoch:23 step:22316 [D loss: 0.249212, acc.: 93.75%] [G loss: 2.041363]\n",
      "epoch:23 step:22317 [D loss: 0.515330, acc.: 80.47%] [G loss: 1.615175]\n",
      "epoch:23 step:22318 [D loss: 0.308744, acc.: 89.06%] [G loss: 2.014865]\n",
      "epoch:23 step:22319 [D loss: 0.346212, acc.: 90.62%] [G loss: 1.851377]\n",
      "epoch:23 step:22320 [D loss: 0.329777, acc.: 95.31%] [G loss: 2.110457]\n",
      "epoch:23 step:22321 [D loss: 0.615692, acc.: 68.75%] [G loss: 1.771009]\n",
      "epoch:23 step:22322 [D loss: 0.602359, acc.: 66.41%] [G loss: 2.557457]\n",
      "epoch:23 step:22323 [D loss: 0.235377, acc.: 95.31%] [G loss: 1.790861]\n",
      "epoch:23 step:22324 [D loss: 0.630071, acc.: 62.50%] [G loss: 3.348264]\n",
      "epoch:23 step:22325 [D loss: 0.261748, acc.: 91.41%] [G loss: 2.910933]\n",
      "epoch:23 step:22326 [D loss: 0.308002, acc.: 90.62%] [G loss: 2.312329]\n",
      "epoch:23 step:22327 [D loss: 0.247583, acc.: 92.97%] [G loss: 1.250319]\n",
      "epoch:23 step:22328 [D loss: 0.237533, acc.: 96.09%] [G loss: 3.149137]\n",
      "epoch:23 step:22329 [D loss: 0.457052, acc.: 78.91%] [G loss: 1.253139]\n",
      "epoch:23 step:22330 [D loss: 0.338000, acc.: 92.19%] [G loss: 2.285800]\n",
      "epoch:23 step:22331 [D loss: 0.228935, acc.: 97.66%] [G loss: 1.080475]\n",
      "epoch:23 step:22332 [D loss: 0.575725, acc.: 62.50%] [G loss: 1.162472]\n",
      "epoch:23 step:22333 [D loss: 0.324469, acc.: 88.28%] [G loss: 2.130221]\n",
      "epoch:23 step:22334 [D loss: 0.146226, acc.: 99.22%] [G loss: 2.932170]\n",
      "epoch:23 step:22335 [D loss: 0.470577, acc.: 72.66%] [G loss: 1.978527]\n",
      "epoch:23 step:22336 [D loss: 0.539122, acc.: 67.97%] [G loss: 2.421673]\n",
      "epoch:23 step:22337 [D loss: 0.277789, acc.: 91.41%] [G loss: 1.951824]\n",
      "epoch:23 step:22338 [D loss: 1.080482, acc.: 46.09%] [G loss: 1.106822]\n",
      "epoch:23 step:22339 [D loss: 0.403955, acc.: 82.03%] [G loss: 3.269883]\n",
      "epoch:23 step:22340 [D loss: 0.378772, acc.: 79.69%] [G loss: 3.070715]\n",
      "epoch:23 step:22341 [D loss: 0.288790, acc.: 94.53%] [G loss: 2.001923]\n",
      "epoch:23 step:22342 [D loss: 0.247712, acc.: 92.97%] [G loss: 2.767930]\n",
      "epoch:23 step:22343 [D loss: 0.333542, acc.: 89.06%] [G loss: 1.794035]\n",
      "epoch:23 step:22344 [D loss: 0.112931, acc.: 96.88%] [G loss: 0.820349]\n",
      "epoch:23 step:22345 [D loss: 0.486080, acc.: 79.69%] [G loss: 2.225889]\n",
      "epoch:23 step:22346 [D loss: 0.250668, acc.: 94.53%] [G loss: 1.795195]\n",
      "epoch:23 step:22347 [D loss: 1.062312, acc.: 28.12%] [G loss: 1.688941]\n",
      "epoch:23 step:22348 [D loss: 0.318182, acc.: 89.84%] [G loss: 2.152763]\n",
      "epoch:23 step:22349 [D loss: 0.356819, acc.: 86.72%] [G loss: 2.442811]\n",
      "epoch:23 step:22350 [D loss: 0.243856, acc.: 95.31%] [G loss: 1.442794]\n",
      "epoch:23 step:22351 [D loss: 0.453944, acc.: 81.25%] [G loss: 3.011176]\n",
      "epoch:23 step:22352 [D loss: 0.333055, acc.: 82.03%] [G loss: 2.199925]\n",
      "epoch:23 step:22353 [D loss: 0.672044, acc.: 60.94%] [G loss: 1.687840]\n",
      "epoch:23 step:22354 [D loss: 0.288259, acc.: 92.19%] [G loss: 2.356434]\n",
      "epoch:23 step:22355 [D loss: 0.771912, acc.: 53.12%] [G loss: 3.380093]\n",
      "epoch:23 step:22356 [D loss: 0.911709, acc.: 42.19%] [G loss: 1.904858]\n",
      "epoch:23 step:22357 [D loss: 1.062626, acc.: 52.34%] [G loss: 2.458673]\n",
      "epoch:23 step:22358 [D loss: 0.251330, acc.: 93.75%] [G loss: 3.157426]\n",
      "epoch:23 step:22359 [D loss: 0.284031, acc.: 91.41%] [G loss: 2.555286]\n",
      "epoch:23 step:22360 [D loss: 0.249841, acc.: 95.31%] [G loss: 1.535237]\n",
      "epoch:23 step:22361 [D loss: 0.582994, acc.: 67.19%] [G loss: 0.873041]\n",
      "epoch:23 step:22362 [D loss: 0.831898, acc.: 47.66%] [G loss: 1.544281]\n",
      "epoch:23 step:22363 [D loss: 0.967349, acc.: 46.09%] [G loss: 1.946560]\n",
      "epoch:23 step:22364 [D loss: 0.794594, acc.: 59.38%] [G loss: 1.691044]\n",
      "epoch:23 step:22365 [D loss: 0.967492, acc.: 46.09%] [G loss: 1.778838]\n",
      "epoch:23 step:22366 [D loss: 0.487557, acc.: 75.78%] [G loss: 1.909748]\n",
      "epoch:23 step:22367 [D loss: 0.500126, acc.: 75.78%] [G loss: 1.800714]\n",
      "epoch:23 step:22368 [D loss: 0.936827, acc.: 38.28%] [G loss: 1.344978]\n",
      "epoch:23 step:22369 [D loss: 0.868835, acc.: 56.25%] [G loss: 1.157041]\n",
      "epoch:23 step:22370 [D loss: 0.874355, acc.: 46.88%] [G loss: 1.199845]\n",
      "epoch:23 step:22371 [D loss: 0.383820, acc.: 89.84%] [G loss: 1.434255]\n",
      "epoch:23 step:22372 [D loss: 0.719822, acc.: 54.69%] [G loss: 1.660057]\n",
      "epoch:23 step:22373 [D loss: 0.631743, acc.: 60.94%] [G loss: 1.945258]\n",
      "epoch:23 step:22374 [D loss: 0.808432, acc.: 51.56%] [G loss: 1.978993]\n",
      "epoch:23 step:22375 [D loss: 0.853483, acc.: 47.66%] [G loss: 1.837789]\n",
      "epoch:23 step:22376 [D loss: 0.681115, acc.: 57.81%] [G loss: 1.318404]\n",
      "epoch:23 step:22377 [D loss: 0.227332, acc.: 96.09%] [G loss: 1.917247]\n",
      "epoch:23 step:22378 [D loss: 0.285767, acc.: 92.97%] [G loss: 1.227474]\n",
      "epoch:23 step:22379 [D loss: 0.355737, acc.: 88.28%] [G loss: 1.969048]\n",
      "epoch:23 step:22380 [D loss: 0.290150, acc.: 91.41%] [G loss: 0.808830]\n",
      "epoch:23 step:22381 [D loss: 0.788040, acc.: 50.78%] [G loss: 1.072455]\n",
      "epoch:23 step:22382 [D loss: 0.483613, acc.: 73.44%] [G loss: 1.793086]\n",
      "epoch:23 step:22383 [D loss: 0.392670, acc.: 84.38%] [G loss: 1.242236]\n",
      "epoch:23 step:22384 [D loss: 0.304735, acc.: 92.97%] [G loss: 1.692469]\n",
      "epoch:23 step:22385 [D loss: 0.412336, acc.: 85.16%] [G loss: 3.130540]\n",
      "epoch:23 step:22386 [D loss: 0.435064, acc.: 80.47%] [G loss: 3.303893]\n",
      "epoch:23 step:22387 [D loss: 0.458555, acc.: 79.69%] [G loss: 2.664406]\n",
      "epoch:23 step:22388 [D loss: 0.669134, acc.: 59.38%] [G loss: 2.715427]\n",
      "epoch:23 step:22389 [D loss: 0.246133, acc.: 90.62%] [G loss: 1.495752]\n",
      "epoch:23 step:22390 [D loss: 0.598759, acc.: 67.19%] [G loss: 2.706722]\n",
      "epoch:23 step:22391 [D loss: 0.429729, acc.: 80.47%] [G loss: 1.295754]\n",
      "epoch:23 step:22392 [D loss: 0.273292, acc.: 93.75%] [G loss: 2.912849]\n",
      "epoch:23 step:22393 [D loss: 0.347477, acc.: 90.62%] [G loss: 2.487642]\n",
      "epoch:23 step:22394 [D loss: 0.254571, acc.: 94.53%] [G loss: 2.922669]\n",
      "epoch:23 step:22395 [D loss: 0.507939, acc.: 77.34%] [G loss: 1.387679]\n",
      "epoch:23 step:22396 [D loss: 0.306827, acc.: 92.19%] [G loss: 1.840434]\n",
      "epoch:23 step:22397 [D loss: 0.452417, acc.: 75.78%] [G loss: 1.592114]\n",
      "epoch:23 step:22398 [D loss: 0.641708, acc.: 60.94%] [G loss: 2.743111]\n",
      "epoch:23 step:22399 [D loss: 0.318661, acc.: 92.97%] [G loss: 0.518307]\n",
      "epoch:23 step:22400 [D loss: 0.657891, acc.: 56.25%] [G loss: 1.928748]\n",
      "epoch:23 step:22401 [D loss: 0.304734, acc.: 90.62%] [G loss: 2.668970]\n",
      "epoch:23 step:22402 [D loss: 0.678023, acc.: 58.59%] [G loss: 0.875066]\n",
      "epoch:23 step:22403 [D loss: 0.317871, acc.: 88.28%] [G loss: 1.274784]\n",
      "epoch:23 step:22404 [D loss: 0.777389, acc.: 57.03%] [G loss: 1.218703]\n",
      "epoch:23 step:22405 [D loss: 0.969061, acc.: 35.16%] [G loss: 1.252878]\n",
      "epoch:23 step:22406 [D loss: 0.614308, acc.: 67.97%] [G loss: 0.866558]\n",
      "epoch:23 step:22407 [D loss: 0.432347, acc.: 78.12%] [G loss: 3.138400]\n",
      "epoch:23 step:22408 [D loss: 0.342774, acc.: 86.72%] [G loss: 2.405997]\n",
      "epoch:23 step:22409 [D loss: 0.840583, acc.: 42.97%] [G loss: 1.683605]\n",
      "epoch:23 step:22410 [D loss: 0.576932, acc.: 67.97%] [G loss: 1.338559]\n",
      "epoch:23 step:22411 [D loss: 0.273265, acc.: 91.41%] [G loss: 2.735731]\n",
      "epoch:23 step:22412 [D loss: 0.384295, acc.: 84.38%] [G loss: 1.330837]\n",
      "epoch:23 step:22413 [D loss: 0.256422, acc.: 94.53%] [G loss: 2.874296]\n",
      "epoch:23 step:22414 [D loss: 0.499503, acc.: 71.09%] [G loss: 1.872246]\n",
      "epoch:23 step:22415 [D loss: 0.158394, acc.: 97.66%] [G loss: 1.915702]\n",
      "epoch:23 step:22416 [D loss: 0.373403, acc.: 82.03%] [G loss: 1.864862]\n",
      "epoch:23 step:22417 [D loss: 0.117359, acc.: 99.22%] [G loss: 1.944986]\n",
      "epoch:23 step:22418 [D loss: 0.241517, acc.: 94.53%] [G loss: 1.225363]\n",
      "epoch:23 step:22419 [D loss: 0.526291, acc.: 70.31%] [G loss: 1.100220]\n",
      "epoch:23 step:22420 [D loss: 0.315079, acc.: 89.84%] [G loss: 2.455233]\n",
      "epoch:23 step:22421 [D loss: 0.253180, acc.: 96.88%] [G loss: 2.699830]\n",
      "epoch:23 step:22422 [D loss: 0.237651, acc.: 97.66%] [G loss: 2.515330]\n",
      "epoch:23 step:22423 [D loss: 0.631558, acc.: 60.94%] [G loss: 2.383921]\n",
      "epoch:23 step:22424 [D loss: 0.203199, acc.: 93.75%] [G loss: 1.433776]\n",
      "epoch:23 step:22425 [D loss: 0.341974, acc.: 89.84%] [G loss: 0.757000]\n",
      "epoch:23 step:22426 [D loss: 0.544558, acc.: 67.19%] [G loss: 2.382590]\n",
      "epoch:23 step:22427 [D loss: 0.175455, acc.: 96.88%] [G loss: 2.231233]\n",
      "epoch:23 step:22428 [D loss: 0.575664, acc.: 69.53%] [G loss: 2.307240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22429 [D loss: 0.778534, acc.: 49.22%] [G loss: 1.292494]\n",
      "epoch:23 step:22430 [D loss: 0.430770, acc.: 82.81%] [G loss: 1.938191]\n",
      "epoch:23 step:22431 [D loss: 0.543988, acc.: 64.84%] [G loss: 1.560822]\n",
      "epoch:23 step:22432 [D loss: 0.146041, acc.: 97.66%] [G loss: 0.725195]\n",
      "epoch:23 step:22433 [D loss: 0.258357, acc.: 92.97%] [G loss: 0.837883]\n",
      "epoch:23 step:22434 [D loss: 0.320157, acc.: 92.19%] [G loss: 1.039450]\n",
      "epoch:23 step:22435 [D loss: 0.093682, acc.: 100.00%] [G loss: 1.013951]\n",
      "epoch:23 step:22436 [D loss: 0.561548, acc.: 72.66%] [G loss: 0.861757]\n",
      "epoch:23 step:22437 [D loss: 0.516767, acc.: 72.66%] [G loss: 1.740636]\n",
      "epoch:23 step:22438 [D loss: 0.553946, acc.: 67.97%] [G loss: 1.269493]\n",
      "epoch:23 step:22439 [D loss: 0.365636, acc.: 83.59%] [G loss: 3.434627]\n",
      "epoch:23 step:22440 [D loss: 0.457369, acc.: 79.69%] [G loss: 1.281851]\n",
      "epoch:23 step:22441 [D loss: 0.852186, acc.: 50.00%] [G loss: 2.219985]\n",
      "epoch:23 step:22442 [D loss: 0.262670, acc.: 94.53%] [G loss: 2.607443]\n",
      "epoch:23 step:22443 [D loss: 0.536321, acc.: 67.97%] [G loss: 1.815148]\n",
      "epoch:23 step:22444 [D loss: 0.190715, acc.: 92.97%] [G loss: 3.080255]\n",
      "epoch:23 step:22445 [D loss: 0.748761, acc.: 64.84%] [G loss: 1.557621]\n",
      "epoch:23 step:22446 [D loss: 0.210889, acc.: 96.09%] [G loss: 1.031987]\n",
      "epoch:23 step:22447 [D loss: 0.556123, acc.: 74.22%] [G loss: 1.421384]\n",
      "epoch:23 step:22448 [D loss: 0.423373, acc.: 78.12%] [G loss: 1.837771]\n",
      "epoch:23 step:22449 [D loss: 0.513618, acc.: 75.00%] [G loss: 0.452617]\n",
      "epoch:23 step:22450 [D loss: 0.592012, acc.: 67.19%] [G loss: 2.272018]\n",
      "epoch:23 step:22451 [D loss: 0.795269, acc.: 50.78%] [G loss: 2.333577]\n",
      "epoch:23 step:22452 [D loss: 0.772250, acc.: 55.47%] [G loss: 2.683552]\n",
      "epoch:23 step:22453 [D loss: 0.300290, acc.: 89.06%] [G loss: 1.958095]\n",
      "epoch:23 step:22454 [D loss: 0.670202, acc.: 57.81%] [G loss: 2.816961]\n",
      "epoch:23 step:22455 [D loss: 0.256692, acc.: 92.97%] [G loss: 2.373530]\n",
      "epoch:23 step:22456 [D loss: 0.977418, acc.: 55.47%] [G loss: 2.169522]\n",
      "epoch:23 step:22457 [D loss: 0.601385, acc.: 69.53%] [G loss: 0.801042]\n",
      "epoch:23 step:22458 [D loss: 0.288107, acc.: 89.84%] [G loss: 1.419498]\n",
      "epoch:23 step:22459 [D loss: 0.812248, acc.: 57.03%] [G loss: 1.323977]\n",
      "epoch:23 step:22460 [D loss: 0.254716, acc.: 93.75%] [G loss: 2.393759]\n",
      "epoch:23 step:22461 [D loss: 0.307728, acc.: 92.19%] [G loss: 3.267472]\n",
      "epoch:23 step:22462 [D loss: 0.346332, acc.: 82.03%] [G loss: 2.987756]\n",
      "epoch:23 step:22463 [D loss: 0.274403, acc.: 92.19%] [G loss: 1.671672]\n",
      "epoch:23 step:22464 [D loss: 0.736207, acc.: 54.69%] [G loss: 2.277383]\n",
      "epoch:23 step:22465 [D loss: 0.407997, acc.: 80.47%] [G loss: 1.220671]\n",
      "epoch:23 step:22466 [D loss: 0.395740, acc.: 84.38%] [G loss: 2.177145]\n",
      "epoch:23 step:22467 [D loss: 0.408262, acc.: 80.47%] [G loss: 2.013695]\n",
      "epoch:23 step:22468 [D loss: 0.472216, acc.: 77.34%] [G loss: 1.561582]\n",
      "epoch:23 step:22469 [D loss: 0.482560, acc.: 72.66%] [G loss: 1.912466]\n",
      "epoch:23 step:22470 [D loss: 0.409927, acc.: 85.16%] [G loss: 1.986829]\n",
      "epoch:23 step:22471 [D loss: 0.350122, acc.: 91.41%] [G loss: 2.404784]\n",
      "epoch:23 step:22472 [D loss: 0.415027, acc.: 73.44%] [G loss: 3.697166]\n",
      "epoch:23 step:22473 [D loss: 0.341452, acc.: 83.59%] [G loss: 0.766258]\n",
      "epoch:23 step:22474 [D loss: 0.330671, acc.: 89.84%] [G loss: 1.135661]\n",
      "epoch:23 step:22475 [D loss: 0.455685, acc.: 78.91%] [G loss: 2.253849]\n",
      "epoch:23 step:22476 [D loss: 0.561426, acc.: 66.41%] [G loss: 2.129755]\n",
      "epoch:23 step:22477 [D loss: 0.262824, acc.: 92.97%] [G loss: 0.765089]\n",
      "epoch:23 step:22478 [D loss: 0.752514, acc.: 57.81%] [G loss: 2.508688]\n",
      "epoch:23 step:22479 [D loss: 1.027571, acc.: 50.00%] [G loss: 2.097104]\n",
      "epoch:23 step:22480 [D loss: 0.560166, acc.: 70.31%] [G loss: 1.205499]\n",
      "epoch:23 step:22481 [D loss: 0.179038, acc.: 96.09%] [G loss: 2.400826]\n",
      "epoch:23 step:22482 [D loss: 0.559679, acc.: 68.75%] [G loss: 0.988431]\n",
      "epoch:23 step:22483 [D loss: 0.559417, acc.: 74.22%] [G loss: 1.756257]\n",
      "epoch:23 step:22484 [D loss: 0.582851, acc.: 64.06%] [G loss: 1.490392]\n",
      "epoch:23 step:22485 [D loss: 0.621027, acc.: 64.84%] [G loss: 0.798696]\n",
      "epoch:23 step:22486 [D loss: 0.577334, acc.: 70.31%] [G loss: 0.543607]\n",
      "epoch:23 step:22487 [D loss: 0.749049, acc.: 52.34%] [G loss: 1.286590]\n",
      "epoch:23 step:22488 [D loss: 0.721478, acc.: 61.72%] [G loss: 3.118739]\n",
      "epoch:24 step:22489 [D loss: 0.253435, acc.: 90.62%] [G loss: 3.864586]\n",
      "epoch:24 step:22490 [D loss: 0.322586, acc.: 91.41%] [G loss: 3.091221]\n",
      "epoch:24 step:22491 [D loss: 0.306317, acc.: 88.28%] [G loss: 2.151424]\n",
      "epoch:24 step:22492 [D loss: 1.164248, acc.: 53.12%] [G loss: 1.513548]\n",
      "epoch:24 step:22493 [D loss: 0.605918, acc.: 62.50%] [G loss: 1.727005]\n",
      "epoch:24 step:22494 [D loss: 0.342463, acc.: 92.19%] [G loss: 1.392964]\n",
      "epoch:24 step:22495 [D loss: 0.561302, acc.: 67.97%] [G loss: 3.024763]\n",
      "epoch:24 step:22496 [D loss: 0.931183, acc.: 34.38%] [G loss: 2.157639]\n",
      "epoch:24 step:22497 [D loss: 0.747932, acc.: 53.12%] [G loss: 2.064984]\n",
      "epoch:24 step:22498 [D loss: 0.369958, acc.: 89.06%] [G loss: 2.325939]\n",
      "epoch:24 step:22499 [D loss: 0.243856, acc.: 90.62%] [G loss: 1.020855]\n",
      "epoch:24 step:22500 [D loss: 0.256121, acc.: 96.09%] [G loss: 1.172802]\n",
      "epoch:24 step:22501 [D loss: 0.623631, acc.: 64.06%] [G loss: 1.348791]\n",
      "epoch:24 step:22502 [D loss: 0.567855, acc.: 67.19%] [G loss: 1.687821]\n",
      "epoch:24 step:22503 [D loss: 0.605865, acc.: 65.62%] [G loss: 2.543801]\n",
      "epoch:24 step:22504 [D loss: 0.222950, acc.: 96.09%] [G loss: 1.842610]\n",
      "epoch:24 step:22505 [D loss: 0.183194, acc.: 96.09%] [G loss: 2.873837]\n",
      "epoch:24 step:22506 [D loss: 0.349610, acc.: 86.72%] [G loss: 2.609553]\n",
      "epoch:24 step:22507 [D loss: 0.353213, acc.: 92.19%] [G loss: 1.528050]\n",
      "epoch:24 step:22508 [D loss: 0.362810, acc.: 89.06%] [G loss: 1.394406]\n",
      "epoch:24 step:22509 [D loss: 1.037015, acc.: 35.16%] [G loss: 1.940895]\n",
      "epoch:24 step:22510 [D loss: 0.232320, acc.: 96.88%] [G loss: 0.660395]\n",
      "epoch:24 step:22511 [D loss: 0.560434, acc.: 67.19%] [G loss: 1.771838]\n",
      "epoch:24 step:22512 [D loss: 0.312330, acc.: 89.84%] [G loss: 2.742618]\n",
      "epoch:24 step:22513 [D loss: 0.635601, acc.: 64.84%] [G loss: 0.937663]\n",
      "epoch:24 step:22514 [D loss: 0.628531, acc.: 64.84%] [G loss: 2.871821]\n",
      "epoch:24 step:22515 [D loss: 0.535111, acc.: 76.56%] [G loss: 3.025481]\n",
      "epoch:24 step:22516 [D loss: 0.717395, acc.: 60.94%] [G loss: 0.761881]\n",
      "epoch:24 step:22517 [D loss: 1.016808, acc.: 37.50%] [G loss: 2.481137]\n",
      "epoch:24 step:22518 [D loss: 0.484999, acc.: 80.47%] [G loss: 1.327329]\n",
      "epoch:24 step:22519 [D loss: 0.504480, acc.: 72.66%] [G loss: 1.307757]\n",
      "epoch:24 step:22520 [D loss: 0.479701, acc.: 70.31%] [G loss: 1.143230]\n",
      "epoch:24 step:22521 [D loss: 0.648763, acc.: 62.50%] [G loss: 1.184987]\n",
      "epoch:24 step:22522 [D loss: 0.396905, acc.: 85.94%] [G loss: 2.074059]\n",
      "epoch:24 step:22523 [D loss: 0.268088, acc.: 92.97%] [G loss: 1.012090]\n",
      "epoch:24 step:22524 [D loss: 0.279546, acc.: 90.62%] [G loss: 1.529924]\n",
      "epoch:24 step:22525 [D loss: 0.448499, acc.: 78.12%] [G loss: 1.373585]\n",
      "epoch:24 step:22526 [D loss: 0.385824, acc.: 82.81%] [G loss: 1.880346]\n",
      "epoch:24 step:22527 [D loss: 0.485572, acc.: 79.69%] [G loss: 1.727504]\n",
      "epoch:24 step:22528 [D loss: 0.469469, acc.: 82.03%] [G loss: 1.696638]\n",
      "epoch:24 step:22529 [D loss: 0.508250, acc.: 71.88%] [G loss: 2.161922]\n",
      "epoch:24 step:22530 [D loss: 0.234176, acc.: 96.88%] [G loss: 1.999275]\n",
      "epoch:24 step:22531 [D loss: 0.284516, acc.: 92.97%] [G loss: 2.438165]\n",
      "epoch:24 step:22532 [D loss: 0.587411, acc.: 64.06%] [G loss: 2.419256]\n",
      "epoch:24 step:22533 [D loss: 0.242139, acc.: 95.31%] [G loss: 3.273266]\n",
      "epoch:24 step:22534 [D loss: 0.735365, acc.: 60.94%] [G loss: 2.865280]\n",
      "epoch:24 step:22535 [D loss: 0.393622, acc.: 84.38%] [G loss: 2.794127]\n",
      "epoch:24 step:22536 [D loss: 0.418797, acc.: 81.25%] [G loss: 3.065658]\n",
      "epoch:24 step:22537 [D loss: 0.177891, acc.: 96.88%] [G loss: 2.872735]\n",
      "epoch:24 step:22538 [D loss: 0.222369, acc.: 92.19%] [G loss: 2.882966]\n",
      "epoch:24 step:22539 [D loss: 0.385664, acc.: 85.94%] [G loss: 4.932511]\n",
      "epoch:24 step:22540 [D loss: 0.407606, acc.: 85.16%] [G loss: 2.441383]\n",
      "epoch:24 step:22541 [D loss: 0.113200, acc.: 99.22%] [G loss: 2.051353]\n",
      "epoch:24 step:22542 [D loss: 0.436711, acc.: 79.69%] [G loss: 2.696060]\n",
      "epoch:24 step:22543 [D loss: 0.177757, acc.: 96.09%] [G loss: 1.691525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22544 [D loss: 0.802481, acc.: 53.12%] [G loss: 2.969908]\n",
      "epoch:24 step:22545 [D loss: 0.509758, acc.: 68.75%] [G loss: 3.176903]\n",
      "epoch:24 step:22546 [D loss: 0.379975, acc.: 82.81%] [G loss: 1.509894]\n",
      "epoch:24 step:22547 [D loss: 0.331335, acc.: 82.81%] [G loss: 1.688482]\n",
      "epoch:24 step:22548 [D loss: 0.415875, acc.: 76.56%] [G loss: 2.326499]\n",
      "epoch:24 step:22549 [D loss: 0.358046, acc.: 86.72%] [G loss: 2.571259]\n",
      "epoch:24 step:22550 [D loss: 1.160710, acc.: 32.81%] [G loss: 1.382488]\n",
      "epoch:24 step:22551 [D loss: 0.388764, acc.: 88.28%] [G loss: 3.112815]\n",
      "epoch:24 step:22552 [D loss: 0.242993, acc.: 95.31%] [G loss: 2.708331]\n",
      "epoch:24 step:22553 [D loss: 0.294846, acc.: 92.19%] [G loss: 1.439690]\n",
      "epoch:24 step:22554 [D loss: 0.409449, acc.: 82.81%] [G loss: 2.172184]\n",
      "epoch:24 step:22555 [D loss: 0.455813, acc.: 84.38%] [G loss: 1.793499]\n",
      "epoch:24 step:22556 [D loss: 0.649082, acc.: 67.19%] [G loss: 2.396350]\n",
      "epoch:24 step:22557 [D loss: 0.167534, acc.: 98.44%] [G loss: 2.964888]\n",
      "epoch:24 step:22558 [D loss: 0.444101, acc.: 86.72%] [G loss: 3.761220]\n",
      "epoch:24 step:22559 [D loss: 0.789650, acc.: 54.69%] [G loss: 5.260184]\n",
      "epoch:24 step:22560 [D loss: 0.515383, acc.: 70.31%] [G loss: 2.816401]\n",
      "epoch:24 step:22561 [D loss: 0.279632, acc.: 92.19%] [G loss: 2.058389]\n",
      "epoch:24 step:22562 [D loss: 0.235067, acc.: 94.53%] [G loss: 2.506022]\n",
      "epoch:24 step:22563 [D loss: 0.429092, acc.: 83.59%] [G loss: 2.220647]\n",
      "epoch:24 step:22564 [D loss: 0.218599, acc.: 96.09%] [G loss: 2.010765]\n",
      "epoch:24 step:22565 [D loss: 0.971800, acc.: 41.41%] [G loss: 1.377998]\n",
      "epoch:24 step:22566 [D loss: 0.099732, acc.: 100.00%] [G loss: 1.382162]\n",
      "epoch:24 step:22567 [D loss: 0.260106, acc.: 92.97%] [G loss: 2.637962]\n",
      "epoch:24 step:22568 [D loss: 0.349382, acc.: 84.38%] [G loss: 0.996293]\n",
      "epoch:24 step:22569 [D loss: 0.092308, acc.: 98.44%] [G loss: 2.335980]\n",
      "epoch:24 step:22570 [D loss: 0.432972, acc.: 81.25%] [G loss: 0.190736]\n",
      "epoch:24 step:22571 [D loss: 0.246092, acc.: 94.53%] [G loss: 2.198417]\n",
      "epoch:24 step:22572 [D loss: 0.770752, acc.: 57.03%] [G loss: 1.880072]\n",
      "epoch:24 step:22573 [D loss: 0.547284, acc.: 66.41%] [G loss: 1.089698]\n",
      "epoch:24 step:22574 [D loss: 0.831395, acc.: 56.25%] [G loss: 1.276476]\n",
      "epoch:24 step:22575 [D loss: 0.222276, acc.: 92.97%] [G loss: 1.369212]\n",
      "epoch:24 step:22576 [D loss: 0.769186, acc.: 54.69%] [G loss: 2.744683]\n",
      "epoch:24 step:22577 [D loss: 0.403701, acc.: 85.94%] [G loss: 0.898118]\n",
      "epoch:24 step:22578 [D loss: 0.343164, acc.: 89.84%] [G loss: 1.946801]\n",
      "epoch:24 step:22579 [D loss: 0.472959, acc.: 75.00%] [G loss: 1.662158]\n",
      "epoch:24 step:22580 [D loss: 0.639178, acc.: 57.03%] [G loss: 3.989934]\n",
      "epoch:24 step:22581 [D loss: 0.535266, acc.: 77.34%] [G loss: 2.318544]\n",
      "epoch:24 step:22582 [D loss: 0.257111, acc.: 93.75%] [G loss: 2.129879]\n",
      "epoch:24 step:22583 [D loss: 0.183983, acc.: 96.88%] [G loss: 2.357982]\n",
      "epoch:24 step:22584 [D loss: 0.087418, acc.: 100.00%] [G loss: 2.689664]\n",
      "epoch:24 step:22585 [D loss: 0.913157, acc.: 54.69%] [G loss: 1.835120]\n",
      "epoch:24 step:22586 [D loss: 0.441714, acc.: 77.34%] [G loss: 2.717553]\n",
      "epoch:24 step:22587 [D loss: 0.330551, acc.: 87.50%] [G loss: 1.447245]\n",
      "epoch:24 step:22588 [D loss: 0.446381, acc.: 85.94%] [G loss: 2.843743]\n",
      "epoch:24 step:22589 [D loss: 0.912756, acc.: 52.34%] [G loss: 1.631065]\n",
      "epoch:24 step:22590 [D loss: 0.186794, acc.: 96.09%] [G loss: 2.513847]\n",
      "epoch:24 step:22591 [D loss: 0.275240, acc.: 92.97%] [G loss: 1.045548]\n",
      "epoch:24 step:22592 [D loss: 0.362580, acc.: 88.28%] [G loss: 2.139875]\n",
      "epoch:24 step:22593 [D loss: 0.355192, acc.: 90.62%] [G loss: 1.648394]\n",
      "epoch:24 step:22594 [D loss: 1.074703, acc.: 35.94%] [G loss: 3.426179]\n",
      "epoch:24 step:22595 [D loss: 0.208184, acc.: 95.31%] [G loss: 2.273952]\n",
      "epoch:24 step:22596 [D loss: 0.216552, acc.: 96.09%] [G loss: 3.710368]\n",
      "epoch:24 step:22597 [D loss: 0.477483, acc.: 70.31%] [G loss: 1.612163]\n",
      "epoch:24 step:22598 [D loss: 0.198296, acc.: 97.66%] [G loss: 3.701470]\n",
      "epoch:24 step:22599 [D loss: 0.744377, acc.: 59.38%] [G loss: 1.705426]\n",
      "epoch:24 step:22600 [D loss: 0.864669, acc.: 42.97%] [G loss: 1.332500]\n",
      "epoch:24 step:22601 [D loss: 0.507116, acc.: 69.53%] [G loss: 2.616451]\n",
      "epoch:24 step:22602 [D loss: 0.651189, acc.: 60.16%] [G loss: 1.625886]\n",
      "epoch:24 step:22603 [D loss: 0.160583, acc.: 95.31%] [G loss: 2.305783]\n",
      "epoch:24 step:22604 [D loss: 0.642305, acc.: 63.28%] [G loss: 2.276701]\n",
      "epoch:24 step:22605 [D loss: 0.326305, acc.: 89.84%] [G loss: 1.251183]\n",
      "epoch:24 step:22606 [D loss: 0.225442, acc.: 93.75%] [G loss: 1.377323]\n",
      "epoch:24 step:22607 [D loss: 0.285956, acc.: 92.97%] [G loss: 2.157523]\n",
      "epoch:24 step:22608 [D loss: 0.209003, acc.: 95.31%] [G loss: 1.867920]\n",
      "epoch:24 step:22609 [D loss: 0.284312, acc.: 88.28%] [G loss: 1.834463]\n",
      "epoch:24 step:22610 [D loss: 0.569241, acc.: 71.88%] [G loss: 0.935617]\n",
      "epoch:24 step:22611 [D loss: 0.276272, acc.: 89.84%] [G loss: 0.554374]\n",
      "epoch:24 step:22612 [D loss: 0.409398, acc.: 75.78%] [G loss: 1.254356]\n",
      "epoch:24 step:22613 [D loss: 0.204837, acc.: 96.09%] [G loss: 1.831709]\n",
      "epoch:24 step:22614 [D loss: 0.670353, acc.: 66.41%] [G loss: 0.764843]\n",
      "epoch:24 step:22615 [D loss: 0.210135, acc.: 96.09%] [G loss: 1.473760]\n",
      "epoch:24 step:22616 [D loss: 0.549592, acc.: 68.75%] [G loss: 1.409027]\n",
      "epoch:24 step:22617 [D loss: 0.448709, acc.: 75.78%] [G loss: 1.992188]\n",
      "epoch:24 step:22618 [D loss: 0.772652, acc.: 52.34%] [G loss: 2.500537]\n",
      "epoch:24 step:22619 [D loss: 0.190878, acc.: 96.88%] [G loss: 1.792578]\n",
      "epoch:24 step:22620 [D loss: 0.673392, acc.: 60.16%] [G loss: 1.919195]\n",
      "epoch:24 step:22621 [D loss: 0.174516, acc.: 97.66%] [G loss: 3.061486]\n",
      "epoch:24 step:22622 [D loss: 0.406562, acc.: 82.03%] [G loss: 1.831273]\n",
      "epoch:24 step:22623 [D loss: 0.336147, acc.: 89.84%] [G loss: 1.639006]\n",
      "epoch:24 step:22624 [D loss: 0.617057, acc.: 65.62%] [G loss: 0.441949]\n",
      "epoch:24 step:22625 [D loss: 0.340913, acc.: 92.19%] [G loss: 1.025532]\n",
      "epoch:24 step:22626 [D loss: 0.427680, acc.: 75.00%] [G loss: 1.977724]\n",
      "epoch:24 step:22627 [D loss: 0.304710, acc.: 89.06%] [G loss: 1.311549]\n",
      "epoch:24 step:22628 [D loss: 0.156570, acc.: 96.88%] [G loss: 1.477015]\n",
      "epoch:24 step:22629 [D loss: 0.104716, acc.: 99.22%] [G loss: 2.632136]\n",
      "epoch:24 step:22630 [D loss: 0.971720, acc.: 40.62%] [G loss: 1.001540]\n",
      "epoch:24 step:22631 [D loss: 0.428172, acc.: 75.78%] [G loss: 1.786409]\n",
      "epoch:24 step:22632 [D loss: 1.192294, acc.: 51.56%] [G loss: 2.003312]\n",
      "epoch:24 step:22633 [D loss: 0.561529, acc.: 69.53%] [G loss: 2.116539]\n",
      "epoch:24 step:22634 [D loss: 0.389612, acc.: 88.28%] [G loss: 1.848793]\n",
      "epoch:24 step:22635 [D loss: 0.704691, acc.: 60.16%] [G loss: 3.954991]\n",
      "epoch:24 step:22636 [D loss: 0.573059, acc.: 71.09%] [G loss: 5.264816]\n",
      "epoch:24 step:22637 [D loss: 0.176568, acc.: 93.75%] [G loss: 2.900778]\n",
      "epoch:24 step:22638 [D loss: 0.518871, acc.: 74.22%] [G loss: 2.135022]\n",
      "epoch:24 step:22639 [D loss: 0.314553, acc.: 92.19%] [G loss: 2.585104]\n",
      "epoch:24 step:22640 [D loss: 0.450434, acc.: 78.12%] [G loss: 2.175990]\n",
      "epoch:24 step:22641 [D loss: 0.663419, acc.: 58.59%] [G loss: 1.988158]\n",
      "epoch:24 step:22642 [D loss: 0.421284, acc.: 85.94%] [G loss: 2.531041]\n",
      "epoch:24 step:22643 [D loss: 1.262992, acc.: 50.00%] [G loss: 2.331606]\n",
      "epoch:24 step:22644 [D loss: 0.198767, acc.: 95.31%] [G loss: 2.019982]\n",
      "epoch:24 step:22645 [D loss: 0.513799, acc.: 65.62%] [G loss: 3.170057]\n",
      "epoch:24 step:22646 [D loss: 0.171308, acc.: 96.88%] [G loss: 3.562838]\n",
      "epoch:24 step:22647 [D loss: 0.130088, acc.: 99.22%] [G loss: 2.486206]\n",
      "epoch:24 step:22648 [D loss: 0.376297, acc.: 79.69%] [G loss: 1.753458]\n",
      "epoch:24 step:22649 [D loss: 0.200489, acc.: 93.75%] [G loss: 1.001747]\n",
      "epoch:24 step:22650 [D loss: 0.279942, acc.: 92.19%] [G loss: 2.030098]\n",
      "epoch:24 step:22651 [D loss: 0.166335, acc.: 96.88%] [G loss: 1.800918]\n",
      "epoch:24 step:22652 [D loss: 0.239147, acc.: 94.53%] [G loss: 2.436989]\n",
      "epoch:24 step:22653 [D loss: 0.104282, acc.: 99.22%] [G loss: 2.935074]\n",
      "epoch:24 step:22654 [D loss: 0.322944, acc.: 89.06%] [G loss: 1.301152]\n",
      "epoch:24 step:22655 [D loss: 0.108741, acc.: 99.22%] [G loss: 2.010297]\n",
      "epoch:24 step:22656 [D loss: 0.171931, acc.: 94.53%] [G loss: 2.192438]\n",
      "epoch:24 step:22657 [D loss: 0.124491, acc.: 99.22%] [G loss: 2.134109]\n",
      "epoch:24 step:22658 [D loss: 0.225144, acc.: 96.09%] [G loss: 3.250381]\n",
      "epoch:24 step:22659 [D loss: 0.343973, acc.: 82.03%] [G loss: 1.385888]\n",
      "epoch:24 step:22660 [D loss: 0.478236, acc.: 76.56%] [G loss: 2.565086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22661 [D loss: 2.017132, acc.: 31.25%] [G loss: 1.314208]\n",
      "epoch:24 step:22662 [D loss: 0.595787, acc.: 65.62%] [G loss: 0.712061]\n",
      "epoch:24 step:22663 [D loss: 1.014273, acc.: 43.75%] [G loss: 2.293467]\n",
      "epoch:24 step:22664 [D loss: 0.854274, acc.: 47.66%] [G loss: 1.581251]\n",
      "epoch:24 step:22665 [D loss: 1.225203, acc.: 51.56%] [G loss: 2.498480]\n",
      "epoch:24 step:22666 [D loss: 0.890794, acc.: 49.22%] [G loss: 0.871809]\n",
      "epoch:24 step:22667 [D loss: 0.835422, acc.: 59.38%] [G loss: 0.531644]\n",
      "epoch:24 step:22668 [D loss: 0.778823, acc.: 60.16%] [G loss: 1.615434]\n",
      "epoch:24 step:22669 [D loss: 0.340788, acc.: 92.19%] [G loss: 1.732495]\n",
      "epoch:24 step:22670 [D loss: 0.424157, acc.: 80.47%] [G loss: 1.747315]\n",
      "epoch:24 step:22671 [D loss: 0.861361, acc.: 37.50%] [G loss: 1.663754]\n",
      "epoch:24 step:22672 [D loss: 0.534367, acc.: 74.22%] [G loss: 1.889976]\n",
      "epoch:24 step:22673 [D loss: 0.683466, acc.: 70.31%] [G loss: 1.161577]\n",
      "epoch:24 step:22674 [D loss: 0.731581, acc.: 53.91%] [G loss: 1.576687]\n",
      "epoch:24 step:22675 [D loss: 0.510083, acc.: 73.44%] [G loss: 1.717179]\n",
      "epoch:24 step:22676 [D loss: 0.505161, acc.: 78.12%] [G loss: 2.173626]\n",
      "epoch:24 step:22677 [D loss: 0.520817, acc.: 76.56%] [G loss: 1.971035]\n",
      "epoch:24 step:22678 [D loss: 0.410747, acc.: 79.69%] [G loss: 1.407756]\n",
      "epoch:24 step:22679 [D loss: 0.308555, acc.: 85.94%] [G loss: 1.967561]\n",
      "epoch:24 step:22680 [D loss: 0.407871, acc.: 85.16%] [G loss: 1.574344]\n",
      "epoch:24 step:22681 [D loss: 0.344411, acc.: 85.94%] [G loss: 2.059231]\n",
      "epoch:24 step:22682 [D loss: 0.465271, acc.: 84.38%] [G loss: 2.471043]\n",
      "epoch:24 step:22683 [D loss: 0.694410, acc.: 61.72%] [G loss: 1.758498]\n",
      "epoch:24 step:22684 [D loss: 0.428039, acc.: 82.03%] [G loss: 1.682187]\n",
      "epoch:24 step:22685 [D loss: 0.788033, acc.: 59.38%] [G loss: 1.015361]\n",
      "epoch:24 step:22686 [D loss: 0.737705, acc.: 54.69%] [G loss: 1.406340]\n",
      "epoch:24 step:22687 [D loss: 0.574107, acc.: 66.41%] [G loss: 1.464305]\n",
      "epoch:24 step:22688 [D loss: 0.358589, acc.: 82.81%] [G loss: 2.400534]\n",
      "epoch:24 step:22689 [D loss: 0.923695, acc.: 36.72%] [G loss: 1.945086]\n",
      "epoch:24 step:22690 [D loss: 0.455353, acc.: 84.38%] [G loss: 1.262834]\n",
      "epoch:24 step:22691 [D loss: 1.049902, acc.: 32.81%] [G loss: 1.186890]\n",
      "epoch:24 step:22692 [D loss: 0.375657, acc.: 87.50%] [G loss: 1.775219]\n",
      "epoch:24 step:22693 [D loss: 0.414760, acc.: 86.72%] [G loss: 1.584623]\n",
      "epoch:24 step:22694 [D loss: 0.342347, acc.: 82.81%] [G loss: 1.437360]\n",
      "epoch:24 step:22695 [D loss: 0.521973, acc.: 76.56%] [G loss: 1.437486]\n",
      "epoch:24 step:22696 [D loss: 0.555926, acc.: 73.44%] [G loss: 1.853123]\n",
      "epoch:24 step:22697 [D loss: 0.707628, acc.: 60.94%] [G loss: 1.727501]\n",
      "epoch:24 step:22698 [D loss: 0.226611, acc.: 96.09%] [G loss: 1.586499]\n",
      "epoch:24 step:22699 [D loss: 0.629667, acc.: 60.94%] [G loss: 0.916824]\n",
      "epoch:24 step:22700 [D loss: 0.645652, acc.: 64.84%] [G loss: 1.214286]\n",
      "epoch:24 step:22701 [D loss: 0.421319, acc.: 81.25%] [G loss: 1.384772]\n",
      "epoch:24 step:22702 [D loss: 0.513949, acc.: 75.00%] [G loss: 1.058936]\n",
      "epoch:24 step:22703 [D loss: 0.275184, acc.: 92.97%] [G loss: 1.545251]\n",
      "epoch:24 step:22704 [D loss: 0.372793, acc.: 80.47%] [G loss: 2.923456]\n",
      "epoch:24 step:22705 [D loss: 0.278665, acc.: 92.97%] [G loss: 2.600884]\n",
      "epoch:24 step:22706 [D loss: 0.555226, acc.: 78.91%] [G loss: 2.498937]\n",
      "epoch:24 step:22707 [D loss: 0.261502, acc.: 89.84%] [G loss: 2.224771]\n",
      "epoch:24 step:22708 [D loss: 0.650236, acc.: 62.50%] [G loss: 1.013302]\n",
      "epoch:24 step:22709 [D loss: 0.214573, acc.: 96.09%] [G loss: 1.757929]\n",
      "epoch:24 step:22710 [D loss: 0.885333, acc.: 42.97%] [G loss: 0.919031]\n",
      "epoch:24 step:22711 [D loss: 0.399682, acc.: 86.72%] [G loss: 1.151944]\n",
      "epoch:24 step:22712 [D loss: 0.341623, acc.: 84.38%] [G loss: 1.801308]\n",
      "epoch:24 step:22713 [D loss: 0.320393, acc.: 92.97%] [G loss: 1.040585]\n",
      "epoch:24 step:22714 [D loss: 0.643290, acc.: 57.03%] [G loss: 1.385915]\n",
      "epoch:24 step:22715 [D loss: 0.294358, acc.: 89.06%] [G loss: 3.299103]\n",
      "epoch:24 step:22716 [D loss: 0.456526, acc.: 78.12%] [G loss: 0.947509]\n",
      "epoch:24 step:22717 [D loss: 0.862825, acc.: 46.09%] [G loss: 1.886429]\n",
      "epoch:24 step:22718 [D loss: 0.625639, acc.: 60.16%] [G loss: 1.654159]\n",
      "epoch:24 step:22719 [D loss: 0.472893, acc.: 74.22%] [G loss: 2.237749]\n",
      "epoch:24 step:22720 [D loss: 0.264612, acc.: 88.28%] [G loss: 1.710808]\n",
      "epoch:24 step:22721 [D loss: 0.330008, acc.: 90.62%] [G loss: 1.161663]\n",
      "epoch:24 step:22722 [D loss: 0.718482, acc.: 60.94%] [G loss: 1.534358]\n",
      "epoch:24 step:22723 [D loss: 0.566897, acc.: 63.28%] [G loss: 2.408528]\n",
      "epoch:24 step:22724 [D loss: 0.480885, acc.: 80.47%] [G loss: 0.814611]\n",
      "epoch:24 step:22725 [D loss: 0.351931, acc.: 88.28%] [G loss: 1.035292]\n",
      "epoch:24 step:22726 [D loss: 0.650976, acc.: 57.81%] [G loss: 2.163339]\n",
      "epoch:24 step:22727 [D loss: 0.196077, acc.: 96.09%] [G loss: 0.951769]\n",
      "epoch:24 step:22728 [D loss: 0.256814, acc.: 92.19%] [G loss: 1.798468]\n",
      "epoch:24 step:22729 [D loss: 0.620864, acc.: 70.31%] [G loss: 1.302813]\n",
      "epoch:24 step:22730 [D loss: 0.290036, acc.: 94.53%] [G loss: 2.495342]\n",
      "epoch:24 step:22731 [D loss: 0.479414, acc.: 75.78%] [G loss: 3.362099]\n",
      "epoch:24 step:22732 [D loss: 0.387960, acc.: 85.16%] [G loss: 0.694785]\n",
      "epoch:24 step:22733 [D loss: 0.582018, acc.: 74.22%] [G loss: 1.679172]\n",
      "epoch:24 step:22734 [D loss: 0.570477, acc.: 64.06%] [G loss: 2.820356]\n",
      "epoch:24 step:22735 [D loss: 0.824448, acc.: 58.59%] [G loss: 1.617814]\n",
      "epoch:24 step:22736 [D loss: 0.404301, acc.: 86.72%] [G loss: 3.163404]\n",
      "epoch:24 step:22737 [D loss: 0.575022, acc.: 71.09%] [G loss: 1.534195]\n",
      "epoch:24 step:22738 [D loss: 0.379631, acc.: 87.50%] [G loss: 1.490551]\n",
      "epoch:24 step:22739 [D loss: 0.320243, acc.: 90.62%] [G loss: 0.724009]\n",
      "epoch:24 step:22740 [D loss: 0.699994, acc.: 57.03%] [G loss: 1.199721]\n",
      "epoch:24 step:22741 [D loss: 0.630384, acc.: 65.62%] [G loss: 1.221174]\n",
      "epoch:24 step:22742 [D loss: 0.246743, acc.: 94.53%] [G loss: 1.986638]\n",
      "epoch:24 step:22743 [D loss: 0.754091, acc.: 56.25%] [G loss: 1.172113]\n",
      "epoch:24 step:22744 [D loss: 0.707890, acc.: 58.59%] [G loss: 1.620027]\n",
      "epoch:24 step:22745 [D loss: 1.094236, acc.: 50.00%] [G loss: 2.310632]\n",
      "epoch:24 step:22746 [D loss: 1.246188, acc.: 25.00%] [G loss: 0.496140]\n",
      "epoch:24 step:22747 [D loss: 0.355775, acc.: 85.16%] [G loss: 1.156026]\n",
      "epoch:24 step:22748 [D loss: 0.400833, acc.: 85.16%] [G loss: 1.159241]\n",
      "epoch:24 step:22749 [D loss: 0.499577, acc.: 70.31%] [G loss: 1.402087]\n",
      "epoch:24 step:22750 [D loss: 0.407906, acc.: 87.50%] [G loss: 2.955020]\n",
      "epoch:24 step:22751 [D loss: 0.528040, acc.: 78.91%] [G loss: 0.492001]\n",
      "epoch:24 step:22752 [D loss: 0.408979, acc.: 82.81%] [G loss: 0.670740]\n",
      "epoch:24 step:22753 [D loss: 0.381774, acc.: 78.91%] [G loss: 1.123219]\n",
      "epoch:24 step:22754 [D loss: 0.333236, acc.: 88.28%] [G loss: 2.961009]\n",
      "epoch:24 step:22755 [D loss: 0.443884, acc.: 83.59%] [G loss: 3.743816]\n",
      "epoch:24 step:22756 [D loss: 0.720739, acc.: 57.81%] [G loss: 1.048336]\n",
      "epoch:24 step:22757 [D loss: 0.468021, acc.: 77.34%] [G loss: 1.860073]\n",
      "epoch:24 step:22758 [D loss: 0.757934, acc.: 57.81%] [G loss: 1.578099]\n",
      "epoch:24 step:22759 [D loss: 0.298439, acc.: 89.84%] [G loss: 2.017948]\n",
      "epoch:24 step:22760 [D loss: 0.578470, acc.: 67.19%] [G loss: 2.132730]\n",
      "epoch:24 step:22761 [D loss: 0.597771, acc.: 67.19%] [G loss: 3.038994]\n",
      "epoch:24 step:22762 [D loss: 0.499120, acc.: 76.56%] [G loss: 1.858832]\n",
      "epoch:24 step:22763 [D loss: 0.260017, acc.: 92.19%] [G loss: 1.561386]\n",
      "epoch:24 step:22764 [D loss: 0.608827, acc.: 66.41%] [G loss: 0.831417]\n",
      "epoch:24 step:22765 [D loss: 0.720565, acc.: 57.03%] [G loss: 2.498569]\n",
      "epoch:24 step:22766 [D loss: 0.351440, acc.: 88.28%] [G loss: 1.126024]\n",
      "epoch:24 step:22767 [D loss: 0.790651, acc.: 50.78%] [G loss: 0.836509]\n",
      "epoch:24 step:22768 [D loss: 0.292329, acc.: 90.62%] [G loss: 1.145830]\n",
      "epoch:24 step:22769 [D loss: 0.720703, acc.: 61.72%] [G loss: 1.143055]\n",
      "epoch:24 step:22770 [D loss: 0.508194, acc.: 73.44%] [G loss: 1.136244]\n",
      "epoch:24 step:22771 [D loss: 0.840168, acc.: 46.09%] [G loss: 2.077271]\n",
      "epoch:24 step:22772 [D loss: 1.026222, acc.: 44.53%] [G loss: 0.824588]\n",
      "epoch:24 step:22773 [D loss: 0.402747, acc.: 93.75%] [G loss: 1.427834]\n",
      "epoch:24 step:22774 [D loss: 0.202262, acc.: 100.00%] [G loss: 1.554396]\n",
      "epoch:24 step:22775 [D loss: 0.510566, acc.: 77.34%] [G loss: 2.443166]\n",
      "epoch:24 step:22776 [D loss: 0.484708, acc.: 77.34%] [G loss: 1.645091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22777 [D loss: 0.252555, acc.: 93.75%] [G loss: 1.143177]\n",
      "epoch:24 step:22778 [D loss: 0.697313, acc.: 58.59%] [G loss: 1.856432]\n",
      "epoch:24 step:22779 [D loss: 0.124089, acc.: 98.44%] [G loss: 0.784155]\n",
      "epoch:24 step:22780 [D loss: 0.598621, acc.: 73.44%] [G loss: 0.979452]\n",
      "epoch:24 step:22781 [D loss: 0.325287, acc.: 90.62%] [G loss: 2.086962]\n",
      "epoch:24 step:22782 [D loss: 0.462859, acc.: 81.25%] [G loss: 2.820258]\n",
      "epoch:24 step:22783 [D loss: 0.519514, acc.: 75.78%] [G loss: 2.546480]\n",
      "epoch:24 step:22784 [D loss: 0.459550, acc.: 78.12%] [G loss: 1.429078]\n",
      "epoch:24 step:22785 [D loss: 0.335421, acc.: 89.06%] [G loss: 1.961802]\n",
      "epoch:24 step:22786 [D loss: 0.541639, acc.: 78.12%] [G loss: 1.354062]\n",
      "epoch:24 step:22787 [D loss: 0.142949, acc.: 96.09%] [G loss: 1.136283]\n",
      "epoch:24 step:22788 [D loss: 0.516876, acc.: 71.88%] [G loss: 1.235749]\n",
      "epoch:24 step:22789 [D loss: 0.351872, acc.: 85.94%] [G loss: 2.018541]\n",
      "epoch:24 step:22790 [D loss: 0.152721, acc.: 98.44%] [G loss: 1.506314]\n",
      "epoch:24 step:22791 [D loss: 0.521916, acc.: 70.31%] [G loss: 2.026934]\n",
      "epoch:24 step:22792 [D loss: 0.795836, acc.: 48.44%] [G loss: 1.447688]\n",
      "epoch:24 step:22793 [D loss: 0.281617, acc.: 94.53%] [G loss: 0.855005]\n",
      "epoch:24 step:22794 [D loss: 0.638906, acc.: 65.62%] [G loss: 1.830965]\n",
      "epoch:24 step:22795 [D loss: 0.548771, acc.: 71.88%] [G loss: 4.276808]\n",
      "epoch:24 step:22796 [D loss: 0.380006, acc.: 80.47%] [G loss: 2.306721]\n",
      "epoch:24 step:22797 [D loss: 0.481627, acc.: 75.78%] [G loss: 0.529354]\n",
      "epoch:24 step:22798 [D loss: 0.917351, acc.: 55.47%] [G loss: 1.179044]\n",
      "epoch:24 step:22799 [D loss: 0.279127, acc.: 94.53%] [G loss: 3.493510]\n",
      "epoch:24 step:22800 [D loss: 0.338524, acc.: 89.84%] [G loss: 1.598746]\n",
      "epoch:24 step:22801 [D loss: 0.340783, acc.: 92.97%] [G loss: 0.924474]\n",
      "epoch:24 step:22802 [D loss: 0.828848, acc.: 61.72%] [G loss: 2.186820]\n",
      "epoch:24 step:22803 [D loss: 0.194051, acc.: 97.66%] [G loss: 2.774749]\n",
      "epoch:24 step:22804 [D loss: 0.813415, acc.: 50.00%] [G loss: 1.253856]\n",
      "epoch:24 step:22805 [D loss: 0.333070, acc.: 84.38%] [G loss: 1.684552]\n",
      "epoch:24 step:22806 [D loss: 0.462199, acc.: 82.81%] [G loss: 2.402538]\n",
      "epoch:24 step:22807 [D loss: 0.999728, acc.: 39.06%] [G loss: 1.838158]\n",
      "epoch:24 step:22808 [D loss: 0.206815, acc.: 96.88%] [G loss: 1.469221]\n",
      "epoch:24 step:22809 [D loss: 0.847571, acc.: 56.25%] [G loss: 1.748639]\n",
      "epoch:24 step:22810 [D loss: 0.563109, acc.: 71.09%] [G loss: 2.073983]\n",
      "epoch:24 step:22811 [D loss: 0.467150, acc.: 78.91%] [G loss: 1.762518]\n",
      "epoch:24 step:22812 [D loss: 0.372102, acc.: 85.94%] [G loss: 0.944945]\n",
      "epoch:24 step:22813 [D loss: 0.477257, acc.: 78.12%] [G loss: 1.188851]\n",
      "epoch:24 step:22814 [D loss: 0.621079, acc.: 64.84%] [G loss: 1.710699]\n",
      "epoch:24 step:22815 [D loss: 0.633509, acc.: 64.06%] [G loss: 2.904894]\n",
      "epoch:24 step:22816 [D loss: 0.707309, acc.: 56.25%] [G loss: 0.990142]\n",
      "epoch:24 step:22817 [D loss: 0.507309, acc.: 74.22%] [G loss: 2.481552]\n",
      "epoch:24 step:22818 [D loss: 0.768527, acc.: 52.34%] [G loss: 3.094280]\n",
      "epoch:24 step:22819 [D loss: 0.327959, acc.: 85.94%] [G loss: 1.195019]\n",
      "epoch:24 step:22820 [D loss: 0.944047, acc.: 42.97%] [G loss: 0.777789]\n",
      "epoch:24 step:22821 [D loss: 0.325689, acc.: 90.62%] [G loss: 0.529952]\n",
      "epoch:24 step:22822 [D loss: 0.874813, acc.: 51.56%] [G loss: 0.794197]\n",
      "epoch:24 step:22823 [D loss: 0.735415, acc.: 55.47%] [G loss: 1.050328]\n",
      "epoch:24 step:22824 [D loss: 0.169098, acc.: 98.44%] [G loss: 1.142348]\n",
      "epoch:24 step:22825 [D loss: 0.481860, acc.: 68.75%] [G loss: 1.859241]\n",
      "epoch:24 step:22826 [D loss: 0.360048, acc.: 88.28%] [G loss: 1.164335]\n",
      "epoch:24 step:22827 [D loss: 0.645435, acc.: 67.19%] [G loss: 0.988298]\n",
      "epoch:24 step:22828 [D loss: 1.015254, acc.: 30.47%] [G loss: 0.630313]\n",
      "epoch:24 step:22829 [D loss: 0.294997, acc.: 93.75%] [G loss: 1.251927]\n",
      "epoch:24 step:22830 [D loss: 0.704891, acc.: 63.28%] [G loss: 1.136940]\n",
      "epoch:24 step:22831 [D loss: 0.434585, acc.: 79.69%] [G loss: 1.379617]\n",
      "epoch:24 step:22832 [D loss: 0.331123, acc.: 87.50%] [G loss: 1.248280]\n",
      "epoch:24 step:22833 [D loss: 0.355165, acc.: 90.62%] [G loss: 0.920658]\n",
      "epoch:24 step:22834 [D loss: 0.503974, acc.: 82.03%] [G loss: 0.607502]\n",
      "epoch:24 step:22835 [D loss: 0.311738, acc.: 92.97%] [G loss: 2.675812]\n",
      "epoch:24 step:22836 [D loss: 0.205395, acc.: 96.88%] [G loss: 1.470187]\n",
      "epoch:24 step:22837 [D loss: 1.155950, acc.: 21.88%] [G loss: 1.214587]\n",
      "epoch:24 step:22838 [D loss: 0.323755, acc.: 86.72%] [G loss: 0.920619]\n",
      "epoch:24 step:22839 [D loss: 1.368317, acc.: 17.19%] [G loss: 1.363384]\n",
      "epoch:24 step:22840 [D loss: 0.406820, acc.: 84.38%] [G loss: 3.097365]\n",
      "epoch:24 step:22841 [D loss: 0.384630, acc.: 81.25%] [G loss: 1.465421]\n",
      "epoch:24 step:22842 [D loss: 0.415544, acc.: 88.28%] [G loss: 1.961225]\n",
      "epoch:24 step:22843 [D loss: 0.534721, acc.: 68.75%] [G loss: 0.540771]\n",
      "epoch:24 step:22844 [D loss: 1.414337, acc.: 18.75%] [G loss: 1.592185]\n",
      "epoch:24 step:22845 [D loss: 0.803604, acc.: 53.91%] [G loss: 1.282666]\n",
      "epoch:24 step:22846 [D loss: 0.758604, acc.: 54.69%] [G loss: 1.881412]\n",
      "epoch:24 step:22847 [D loss: 0.418946, acc.: 81.25%] [G loss: 2.022163]\n",
      "epoch:24 step:22848 [D loss: 0.491413, acc.: 79.69%] [G loss: 1.655357]\n",
      "epoch:24 step:22849 [D loss: 0.544246, acc.: 70.31%] [G loss: 1.607372]\n",
      "epoch:24 step:22850 [D loss: 0.403678, acc.: 86.72%] [G loss: 2.113050]\n",
      "epoch:24 step:22851 [D loss: 0.343944, acc.: 92.19%] [G loss: 1.517954]\n",
      "epoch:24 step:22852 [D loss: 0.542754, acc.: 72.66%] [G loss: 1.208672]\n",
      "epoch:24 step:22853 [D loss: 0.366772, acc.: 89.06%] [G loss: 1.582872]\n",
      "epoch:24 step:22854 [D loss: 0.483298, acc.: 75.00%] [G loss: 1.873836]\n",
      "epoch:24 step:22855 [D loss: 0.814668, acc.: 53.91%] [G loss: 1.264221]\n",
      "epoch:24 step:22856 [D loss: 0.326454, acc.: 90.62%] [G loss: 2.159678]\n",
      "epoch:24 step:22857 [D loss: 0.444130, acc.: 75.00%] [G loss: 2.216992]\n",
      "epoch:24 step:22858 [D loss: 0.361422, acc.: 89.06%] [G loss: 1.730703]\n",
      "epoch:24 step:22859 [D loss: 0.280323, acc.: 91.41%] [G loss: 1.471321]\n",
      "epoch:24 step:22860 [D loss: 0.267501, acc.: 93.75%] [G loss: 2.222953]\n",
      "epoch:24 step:22861 [D loss: 0.331925, acc.: 88.28%] [G loss: 1.563612]\n",
      "epoch:24 step:22862 [D loss: 0.389806, acc.: 79.69%] [G loss: 2.016089]\n",
      "epoch:24 step:22863 [D loss: 0.502068, acc.: 75.78%] [G loss: 1.110588]\n",
      "epoch:24 step:22864 [D loss: 0.304472, acc.: 93.75%] [G loss: 1.544959]\n",
      "epoch:24 step:22865 [D loss: 1.106698, acc.: 28.12%] [G loss: 1.853014]\n",
      "epoch:24 step:22866 [D loss: 0.276125, acc.: 93.75%] [G loss: 0.766772]\n",
      "epoch:24 step:22867 [D loss: 0.579599, acc.: 63.28%] [G loss: 0.952281]\n",
      "epoch:24 step:22868 [D loss: 0.791067, acc.: 48.44%] [G loss: 0.430029]\n",
      "epoch:24 step:22869 [D loss: 0.531949, acc.: 67.97%] [G loss: 0.707866]\n",
      "epoch:24 step:22870 [D loss: 0.361220, acc.: 90.62%] [G loss: 1.919100]\n",
      "epoch:24 step:22871 [D loss: 0.536136, acc.: 75.00%] [G loss: 2.013075]\n",
      "epoch:24 step:22872 [D loss: 0.450450, acc.: 80.47%] [G loss: 1.592404]\n",
      "epoch:24 step:22873 [D loss: 0.755998, acc.: 53.12%] [G loss: 1.873123]\n",
      "epoch:24 step:22874 [D loss: 0.675418, acc.: 63.28%] [G loss: 1.350106]\n",
      "epoch:24 step:22875 [D loss: 0.615194, acc.: 71.88%] [G loss: 1.180100]\n",
      "epoch:24 step:22876 [D loss: 0.545568, acc.: 73.44%] [G loss: 1.062868]\n",
      "epoch:24 step:22877 [D loss: 0.475900, acc.: 82.03%] [G loss: 1.966506]\n",
      "epoch:24 step:22878 [D loss: 0.410030, acc.: 86.72%] [G loss: 0.900833]\n",
      "epoch:24 step:22879 [D loss: 0.795278, acc.: 50.78%] [G loss: 1.620356]\n",
      "epoch:24 step:22880 [D loss: 0.425371, acc.: 83.59%] [G loss: 1.763619]\n",
      "epoch:24 step:22881 [D loss: 0.507997, acc.: 78.12%] [G loss: 1.390818]\n",
      "epoch:24 step:22882 [D loss: 0.489571, acc.: 78.12%] [G loss: 2.110775]\n",
      "epoch:24 step:22883 [D loss: 0.487956, acc.: 76.56%] [G loss: 1.289027]\n",
      "epoch:24 step:22884 [D loss: 0.532834, acc.: 76.56%] [G loss: 2.837569]\n",
      "epoch:24 step:22885 [D loss: 0.799082, acc.: 47.66%] [G loss: 2.210931]\n",
      "epoch:24 step:22886 [D loss: 0.217218, acc.: 97.66%] [G loss: 1.560125]\n",
      "epoch:24 step:22887 [D loss: 0.447597, acc.: 78.12%] [G loss: 1.806726]\n",
      "epoch:24 step:22888 [D loss: 0.598106, acc.: 68.75%] [G loss: 1.452216]\n",
      "epoch:24 step:22889 [D loss: 0.387115, acc.: 80.47%] [G loss: 1.705150]\n",
      "epoch:24 step:22890 [D loss: 0.473784, acc.: 77.34%] [G loss: 1.488909]\n",
      "epoch:24 step:22891 [D loss: 0.410918, acc.: 84.38%] [G loss: 0.941823]\n",
      "epoch:24 step:22892 [D loss: 0.445684, acc.: 82.81%] [G loss: 1.303317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22893 [D loss: 0.505509, acc.: 75.78%] [G loss: 1.728282]\n",
      "epoch:24 step:22894 [D loss: 0.339319, acc.: 88.28%] [G loss: 1.088096]\n",
      "epoch:24 step:22895 [D loss: 0.356563, acc.: 86.72%] [G loss: 1.820855]\n",
      "epoch:24 step:22896 [D loss: 0.352542, acc.: 88.28%] [G loss: 1.526785]\n",
      "epoch:24 step:22897 [D loss: 0.819402, acc.: 54.69%] [G loss: 1.496627]\n",
      "epoch:24 step:22898 [D loss: 0.740983, acc.: 59.38%] [G loss: 1.410927]\n",
      "epoch:24 step:22899 [D loss: 0.656696, acc.: 62.50%] [G loss: 1.634041]\n",
      "epoch:24 step:22900 [D loss: 0.501817, acc.: 75.78%] [G loss: 1.562067]\n",
      "epoch:24 step:22901 [D loss: 0.196917, acc.: 96.88%] [G loss: 2.820842]\n",
      "epoch:24 step:22902 [D loss: 0.561582, acc.: 67.19%] [G loss: 1.503585]\n",
      "epoch:24 step:22903 [D loss: 1.055647, acc.: 33.59%] [G loss: 1.920449]\n",
      "epoch:24 step:22904 [D loss: 0.757597, acc.: 59.38%] [G loss: 1.007558]\n",
      "epoch:24 step:22905 [D loss: 0.573777, acc.: 67.19%] [G loss: 3.101551]\n",
      "epoch:24 step:22906 [D loss: 0.429055, acc.: 82.03%] [G loss: 1.043392]\n",
      "epoch:24 step:22907 [D loss: 0.207619, acc.: 96.88%] [G loss: 2.709935]\n",
      "epoch:24 step:22908 [D loss: 0.611841, acc.: 71.09%] [G loss: 1.887035]\n",
      "epoch:24 step:22909 [D loss: 0.917753, acc.: 53.91%] [G loss: 1.451118]\n",
      "epoch:24 step:22910 [D loss: 0.440932, acc.: 80.47%] [G loss: 2.295633]\n",
      "epoch:24 step:22911 [D loss: 0.705226, acc.: 57.81%] [G loss: 1.604541]\n",
      "epoch:24 step:22912 [D loss: 0.300974, acc.: 82.81%] [G loss: 1.901205]\n",
      "epoch:24 step:22913 [D loss: 0.405888, acc.: 88.28%] [G loss: 1.852086]\n",
      "epoch:24 step:22914 [D loss: 0.563897, acc.: 64.06%] [G loss: 1.757528]\n",
      "epoch:24 step:22915 [D loss: 0.165417, acc.: 99.22%] [G loss: 1.306744]\n",
      "epoch:24 step:22916 [D loss: 0.283910, acc.: 92.19%] [G loss: 1.813804]\n",
      "epoch:24 step:22917 [D loss: 0.902596, acc.: 46.88%] [G loss: 0.760145]\n",
      "epoch:24 step:22918 [D loss: 0.819228, acc.: 52.34%] [G loss: 1.050995]\n",
      "epoch:24 step:22919 [D loss: 0.703572, acc.: 59.38%] [G loss: 1.181585]\n",
      "epoch:24 step:22920 [D loss: 0.656133, acc.: 62.50%] [G loss: 1.510998]\n",
      "epoch:24 step:22921 [D loss: 0.552791, acc.: 75.00%] [G loss: 1.548179]\n",
      "epoch:24 step:22922 [D loss: 0.428388, acc.: 81.25%] [G loss: 0.598977]\n",
      "epoch:24 step:22923 [D loss: 0.642635, acc.: 63.28%] [G loss: 1.259378]\n",
      "epoch:24 step:22924 [D loss: 0.581342, acc.: 68.75%] [G loss: 1.411116]\n",
      "epoch:24 step:22925 [D loss: 0.465975, acc.: 82.03%] [G loss: 1.021524]\n",
      "epoch:24 step:22926 [D loss: 0.529418, acc.: 76.56%] [G loss: 1.970452]\n",
      "epoch:24 step:22927 [D loss: 0.359925, acc.: 85.16%] [G loss: 1.060343]\n",
      "epoch:24 step:22928 [D loss: 0.796700, acc.: 50.78%] [G loss: 1.877599]\n",
      "epoch:24 step:22929 [D loss: 0.416526, acc.: 84.38%] [G loss: 2.511160]\n",
      "epoch:24 step:22930 [D loss: 0.633817, acc.: 64.84%] [G loss: 1.054505]\n",
      "epoch:24 step:22931 [D loss: 0.430021, acc.: 78.91%] [G loss: 2.362525]\n",
      "epoch:24 step:22932 [D loss: 0.599860, acc.: 64.84%] [G loss: 1.675299]\n",
      "epoch:24 step:22933 [D loss: 0.528316, acc.: 67.97%] [G loss: 1.229051]\n",
      "epoch:24 step:22934 [D loss: 0.613111, acc.: 66.41%] [G loss: 1.865562]\n",
      "epoch:24 step:22935 [D loss: 0.541898, acc.: 69.53%] [G loss: 0.822909]\n",
      "epoch:24 step:22936 [D loss: 0.561367, acc.: 70.31%] [G loss: 1.007561]\n",
      "epoch:24 step:22937 [D loss: 0.649321, acc.: 61.72%] [G loss: 1.811047]\n",
      "epoch:24 step:22938 [D loss: 0.569343, acc.: 74.22%] [G loss: 2.474738]\n",
      "epoch:24 step:22939 [D loss: 0.530454, acc.: 74.22%] [G loss: 1.119576]\n",
      "epoch:24 step:22940 [D loss: 0.246339, acc.: 93.75%] [G loss: 1.181837]\n",
      "epoch:24 step:22941 [D loss: 0.388719, acc.: 84.38%] [G loss: 1.496149]\n",
      "epoch:24 step:22942 [D loss: 0.710101, acc.: 57.03%] [G loss: 2.487896]\n",
      "epoch:24 step:22943 [D loss: 0.298753, acc.: 92.19%] [G loss: 1.416286]\n",
      "epoch:24 step:22944 [D loss: 0.285925, acc.: 88.28%] [G loss: 1.187223]\n",
      "epoch:24 step:22945 [D loss: 0.845202, acc.: 47.66%] [G loss: 2.224176]\n",
      "epoch:24 step:22946 [D loss: 0.621267, acc.: 67.97%] [G loss: 2.645152]\n",
      "epoch:24 step:22947 [D loss: 0.386497, acc.: 85.16%] [G loss: 1.537427]\n",
      "epoch:24 step:22948 [D loss: 0.621762, acc.: 66.41%] [G loss: 1.329489]\n",
      "epoch:24 step:22949 [D loss: 0.608146, acc.: 68.75%] [G loss: 1.531610]\n",
      "epoch:24 step:22950 [D loss: 0.628139, acc.: 65.62%] [G loss: 1.761673]\n",
      "epoch:24 step:22951 [D loss: 0.393959, acc.: 87.50%] [G loss: 2.606143]\n",
      "epoch:24 step:22952 [D loss: 0.474045, acc.: 75.78%] [G loss: 1.854041]\n",
      "epoch:24 step:22953 [D loss: 0.554003, acc.: 70.31%] [G loss: 2.272561]\n",
      "epoch:24 step:22954 [D loss: 0.278304, acc.: 96.88%] [G loss: 2.605726]\n",
      "epoch:24 step:22955 [D loss: 0.247782, acc.: 95.31%] [G loss: 2.083026]\n",
      "epoch:24 step:22956 [D loss: 0.440126, acc.: 82.81%] [G loss: 1.557986]\n",
      "epoch:24 step:22957 [D loss: 0.497698, acc.: 73.44%] [G loss: 2.756447]\n",
      "epoch:24 step:22958 [D loss: 0.270154, acc.: 90.62%] [G loss: 2.688406]\n",
      "epoch:24 step:22959 [D loss: 0.624506, acc.: 64.84%] [G loss: 2.661158]\n",
      "epoch:24 step:22960 [D loss: 0.713963, acc.: 62.50%] [G loss: 1.674742]\n",
      "epoch:24 step:22961 [D loss: 0.285522, acc.: 89.06%] [G loss: 1.595435]\n",
      "epoch:24 step:22962 [D loss: 0.281943, acc.: 94.53%] [G loss: 2.154872]\n",
      "epoch:24 step:22963 [D loss: 0.094872, acc.: 99.22%] [G loss: 1.303369]\n",
      "epoch:24 step:22964 [D loss: 0.236043, acc.: 95.31%] [G loss: 1.735962]\n",
      "epoch:24 step:22965 [D loss: 0.259301, acc.: 95.31%] [G loss: 2.396933]\n",
      "epoch:24 step:22966 [D loss: 0.497425, acc.: 79.69%] [G loss: 2.754731]\n",
      "epoch:24 step:22967 [D loss: 0.519111, acc.: 76.56%] [G loss: 2.416415]\n",
      "epoch:24 step:22968 [D loss: 0.222724, acc.: 95.31%] [G loss: 1.563044]\n",
      "epoch:24 step:22969 [D loss: 0.424013, acc.: 80.47%] [G loss: 1.117886]\n",
      "epoch:24 step:22970 [D loss: 0.148937, acc.: 98.44%] [G loss: 2.738405]\n",
      "epoch:24 step:22971 [D loss: 0.655225, acc.: 66.41%] [G loss: 1.157187]\n",
      "epoch:24 step:22972 [D loss: 0.438152, acc.: 75.78%] [G loss: 1.478049]\n",
      "epoch:24 step:22973 [D loss: 0.261775, acc.: 94.53%] [G loss: 1.385133]\n",
      "epoch:24 step:22974 [D loss: 0.838842, acc.: 50.78%] [G loss: 2.473305]\n",
      "epoch:24 step:22975 [D loss: 0.225635, acc.: 94.53%] [G loss: 3.180531]\n",
      "epoch:24 step:22976 [D loss: 0.387679, acc.: 87.50%] [G loss: 2.481274]\n",
      "epoch:24 step:22977 [D loss: 0.600209, acc.: 69.53%] [G loss: 1.788021]\n",
      "epoch:24 step:22978 [D loss: 0.420605, acc.: 79.69%] [G loss: 1.862951]\n",
      "epoch:24 step:22979 [D loss: 0.312971, acc.: 88.28%] [G loss: 1.804922]\n",
      "epoch:24 step:22980 [D loss: 0.629893, acc.: 62.50%] [G loss: 3.099552]\n",
      "epoch:24 step:22981 [D loss: 0.763716, acc.: 52.34%] [G loss: 1.091995]\n",
      "epoch:24 step:22982 [D loss: 0.372262, acc.: 89.06%] [G loss: 1.843399]\n",
      "epoch:24 step:22983 [D loss: 0.407317, acc.: 86.72%] [G loss: 2.092947]\n",
      "epoch:24 step:22984 [D loss: 0.333481, acc.: 85.16%] [G loss: 1.540297]\n",
      "epoch:24 step:22985 [D loss: 0.349250, acc.: 88.28%] [G loss: 1.410711]\n",
      "epoch:24 step:22986 [D loss: 0.260038, acc.: 95.31%] [G loss: 2.374228]\n",
      "epoch:24 step:22987 [D loss: 0.393418, acc.: 85.16%] [G loss: 2.778511]\n",
      "epoch:24 step:22988 [D loss: 0.529231, acc.: 69.53%] [G loss: 1.478119]\n",
      "epoch:24 step:22989 [D loss: 0.597144, acc.: 65.62%] [G loss: 2.127770]\n",
      "epoch:24 step:22990 [D loss: 0.229493, acc.: 93.75%] [G loss: 1.112577]\n",
      "epoch:24 step:22991 [D loss: 1.042024, acc.: 35.16%] [G loss: 1.322087]\n",
      "epoch:24 step:22992 [D loss: 0.570981, acc.: 72.66%] [G loss: 1.169231]\n",
      "epoch:24 step:22993 [D loss: 0.392957, acc.: 82.81%] [G loss: 0.702403]\n",
      "epoch:24 step:22994 [D loss: 0.483104, acc.: 78.12%] [G loss: 1.467991]\n",
      "epoch:24 step:22995 [D loss: 0.703423, acc.: 60.16%] [G loss: 2.011716]\n",
      "epoch:24 step:22996 [D loss: 0.369200, acc.: 85.94%] [G loss: 1.483109]\n",
      "epoch:24 step:22997 [D loss: 0.636223, acc.: 67.19%] [G loss: 1.233308]\n",
      "epoch:24 step:22998 [D loss: 0.403632, acc.: 86.72%] [G loss: 2.699051]\n",
      "epoch:24 step:22999 [D loss: 0.528097, acc.: 75.78%] [G loss: 2.496432]\n",
      "epoch:24 step:23000 [D loss: 0.750384, acc.: 59.38%] [G loss: 0.561715]\n",
      "epoch:24 step:23001 [D loss: 0.450645, acc.: 78.12%] [G loss: 2.236208]\n",
      "epoch:24 step:23002 [D loss: 0.231081, acc.: 92.97%] [G loss: 0.611836]\n",
      "epoch:24 step:23003 [D loss: 0.891744, acc.: 46.09%] [G loss: 1.623953]\n",
      "epoch:24 step:23004 [D loss: 0.251713, acc.: 91.41%] [G loss: 2.417682]\n",
      "epoch:24 step:23005 [D loss: 0.398140, acc.: 82.81%] [G loss: 1.774505]\n",
      "epoch:24 step:23006 [D loss: 0.790164, acc.: 50.78%] [G loss: 1.666377]\n",
      "epoch:24 step:23007 [D loss: 0.730209, acc.: 55.47%] [G loss: 0.638545]\n",
      "epoch:24 step:23008 [D loss: 0.763632, acc.: 55.47%] [G loss: 2.573759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23009 [D loss: 0.366618, acc.: 86.72%] [G loss: 3.096741]\n",
      "epoch:24 step:23010 [D loss: 0.316893, acc.: 90.62%] [G loss: 3.475541]\n",
      "epoch:24 step:23011 [D loss: 0.672175, acc.: 57.81%] [G loss: 3.267196]\n",
      "epoch:24 step:23012 [D loss: 0.451287, acc.: 78.12%] [G loss: 1.264297]\n",
      "epoch:24 step:23013 [D loss: 0.802857, acc.: 53.91%] [G loss: 1.164182]\n",
      "epoch:24 step:23014 [D loss: 0.654795, acc.: 64.84%] [G loss: 3.085347]\n",
      "epoch:24 step:23015 [D loss: 0.499319, acc.: 77.34%] [G loss: 2.798119]\n",
      "epoch:24 step:23016 [D loss: 0.928596, acc.: 39.06%] [G loss: 1.899882]\n",
      "epoch:24 step:23017 [D loss: 0.849574, acc.: 43.75%] [G loss: 1.543447]\n",
      "epoch:24 step:23018 [D loss: 0.748838, acc.: 54.69%] [G loss: 1.537986]\n",
      "epoch:24 step:23019 [D loss: 0.459342, acc.: 85.94%] [G loss: 2.237508]\n",
      "epoch:24 step:23020 [D loss: 0.454744, acc.: 81.25%] [G loss: 2.042074]\n",
      "epoch:24 step:23021 [D loss: 0.840667, acc.: 42.97%] [G loss: 1.931816]\n",
      "epoch:24 step:23022 [D loss: 0.265895, acc.: 96.88%] [G loss: 1.413329]\n",
      "epoch:24 step:23023 [D loss: 0.778583, acc.: 46.88%] [G loss: 0.583157]\n",
      "epoch:24 step:23024 [D loss: 0.272537, acc.: 92.19%] [G loss: 1.831189]\n",
      "epoch:24 step:23025 [D loss: 1.067201, acc.: 26.56%] [G loss: 3.619214]\n",
      "epoch:24 step:23026 [D loss: 0.507933, acc.: 75.78%] [G loss: 1.219674]\n",
      "epoch:24 step:23027 [D loss: 0.235674, acc.: 96.88%] [G loss: 1.019308]\n",
      "epoch:24 step:23028 [D loss: 0.244854, acc.: 93.75%] [G loss: 0.649480]\n",
      "epoch:24 step:23029 [D loss: 0.804371, acc.: 49.22%] [G loss: 0.920039]\n",
      "epoch:24 step:23030 [D loss: 0.773203, acc.: 50.78%] [G loss: 2.104925]\n",
      "epoch:24 step:23031 [D loss: 0.332045, acc.: 84.38%] [G loss: 2.813638]\n",
      "epoch:24 step:23032 [D loss: 0.482968, acc.: 78.91%] [G loss: 1.940207]\n",
      "epoch:24 step:23033 [D loss: 0.289066, acc.: 94.53%] [G loss: 2.215719]\n",
      "epoch:24 step:23034 [D loss: 0.266060, acc.: 96.09%] [G loss: 1.243987]\n",
      "epoch:24 step:23035 [D loss: 0.656112, acc.: 65.62%] [G loss: 1.507084]\n",
      "epoch:24 step:23036 [D loss: 0.869981, acc.: 44.53%] [G loss: 0.610227]\n",
      "epoch:24 step:23037 [D loss: 0.341754, acc.: 91.41%] [G loss: 1.012689]\n",
      "epoch:24 step:23038 [D loss: 0.696084, acc.: 50.00%] [G loss: 0.854558]\n",
      "epoch:24 step:23039 [D loss: 0.882416, acc.: 46.09%] [G loss: 2.033304]\n",
      "epoch:24 step:23040 [D loss: 0.777915, acc.: 52.34%] [G loss: 2.192866]\n",
      "epoch:24 step:23041 [D loss: 0.773721, acc.: 52.34%] [G loss: 1.518635]\n",
      "epoch:24 step:23042 [D loss: 0.668217, acc.: 61.72%] [G loss: 1.487738]\n",
      "epoch:24 step:23043 [D loss: 0.398029, acc.: 88.28%] [G loss: 2.086935]\n",
      "epoch:24 step:23044 [D loss: 0.759598, acc.: 54.69%] [G loss: 2.754872]\n",
      "epoch:24 step:23045 [D loss: 0.794936, acc.: 53.91%] [G loss: 1.922925]\n",
      "epoch:24 step:23046 [D loss: 0.306028, acc.: 91.41%] [G loss: 3.517147]\n",
      "epoch:24 step:23047 [D loss: 0.699551, acc.: 56.25%] [G loss: 2.657585]\n",
      "epoch:24 step:23048 [D loss: 0.806235, acc.: 47.66%] [G loss: 2.285776]\n",
      "epoch:24 step:23049 [D loss: 0.329248, acc.: 89.84%] [G loss: 2.337356]\n",
      "epoch:24 step:23050 [D loss: 0.331030, acc.: 92.19%] [G loss: 2.234998]\n",
      "epoch:24 step:23051 [D loss: 0.242967, acc.: 94.53%] [G loss: 1.661008]\n",
      "epoch:24 step:23052 [D loss: 0.696061, acc.: 59.38%] [G loss: 0.645582]\n",
      "epoch:24 step:23053 [D loss: 0.439132, acc.: 82.03%] [G loss: 1.125533]\n",
      "epoch:24 step:23054 [D loss: 0.635381, acc.: 64.84%] [G loss: 1.765908]\n",
      "epoch:24 step:23055 [D loss: 0.937906, acc.: 35.94%] [G loss: 1.350141]\n",
      "epoch:24 step:23056 [D loss: 0.472643, acc.: 83.59%] [G loss: 1.789464]\n",
      "epoch:24 step:23057 [D loss: 0.178001, acc.: 98.44%] [G loss: 1.288583]\n",
      "epoch:24 step:23058 [D loss: 0.401484, acc.: 82.81%] [G loss: 1.680879]\n",
      "epoch:24 step:23059 [D loss: 0.326677, acc.: 89.84%] [G loss: 2.315254]\n",
      "epoch:24 step:23060 [D loss: 0.355076, acc.: 84.38%] [G loss: 2.160119]\n",
      "epoch:24 step:23061 [D loss: 0.469878, acc.: 80.47%] [G loss: 2.520943]\n",
      "epoch:24 step:23062 [D loss: 0.634845, acc.: 64.84%] [G loss: 2.155680]\n",
      "epoch:24 step:23063 [D loss: 0.703638, acc.: 62.50%] [G loss: 2.180908]\n",
      "epoch:24 step:23064 [D loss: 0.492613, acc.: 75.00%] [G loss: 1.745873]\n",
      "epoch:24 step:23065 [D loss: 0.188909, acc.: 95.31%] [G loss: 1.867277]\n",
      "epoch:24 step:23066 [D loss: 0.326447, acc.: 92.19%] [G loss: 2.151566]\n",
      "epoch:24 step:23067 [D loss: 0.172508, acc.: 98.44%] [G loss: 1.315477]\n",
      "epoch:24 step:23068 [D loss: 0.844470, acc.: 52.34%] [G loss: 0.659056]\n",
      "epoch:24 step:23069 [D loss: 0.291905, acc.: 90.62%] [G loss: 3.457988]\n",
      "epoch:24 step:23070 [D loss: 0.420482, acc.: 84.38%] [G loss: 1.768443]\n",
      "epoch:24 step:23071 [D loss: 1.027332, acc.: 48.44%] [G loss: 1.431760]\n",
      "epoch:24 step:23072 [D loss: 0.380179, acc.: 88.28%] [G loss: 2.385167]\n",
      "epoch:24 step:23073 [D loss: 0.714301, acc.: 57.03%] [G loss: 1.636808]\n",
      "epoch:24 step:23074 [D loss: 0.250129, acc.: 92.97%] [G loss: 2.458994]\n",
      "epoch:24 step:23075 [D loss: 0.367257, acc.: 85.16%] [G loss: 1.573346]\n",
      "epoch:24 step:23076 [D loss: 0.785664, acc.: 49.22%] [G loss: 2.118901]\n",
      "epoch:24 step:23077 [D loss: 0.569872, acc.: 71.88%] [G loss: 1.073708]\n",
      "epoch:24 step:23078 [D loss: 0.770628, acc.: 59.38%] [G loss: 1.727127]\n",
      "epoch:24 step:23079 [D loss: 0.199470, acc.: 97.66%] [G loss: 1.875460]\n",
      "epoch:24 step:23080 [D loss: 0.368671, acc.: 86.72%] [G loss: 0.956132]\n",
      "epoch:24 step:23081 [D loss: 0.208467, acc.: 96.88%] [G loss: 2.105099]\n",
      "epoch:24 step:23082 [D loss: 0.568298, acc.: 70.31%] [G loss: 1.050833]\n",
      "epoch:24 step:23083 [D loss: 0.889310, acc.: 56.25%] [G loss: 1.140061]\n",
      "epoch:24 step:23084 [D loss: 0.353538, acc.: 87.50%] [G loss: 1.362024]\n",
      "epoch:24 step:23085 [D loss: 0.276941, acc.: 93.75%] [G loss: 1.846256]\n",
      "epoch:24 step:23086 [D loss: 0.366389, acc.: 84.38%] [G loss: 2.857595]\n",
      "epoch:24 step:23087 [D loss: 0.333214, acc.: 81.25%] [G loss: 1.973443]\n",
      "epoch:24 step:23088 [D loss: 0.473385, acc.: 75.78%] [G loss: 0.768971]\n",
      "epoch:24 step:23089 [D loss: 0.181656, acc.: 95.31%] [G loss: 2.855657]\n",
      "epoch:24 step:23090 [D loss: 0.230744, acc.: 96.88%] [G loss: 1.695437]\n",
      "epoch:24 step:23091 [D loss: 0.893781, acc.: 47.66%] [G loss: 2.538378]\n",
      "epoch:24 step:23092 [D loss: 0.831328, acc.: 48.44%] [G loss: 2.424595]\n",
      "epoch:24 step:23093 [D loss: 0.251182, acc.: 94.53%] [G loss: 1.564233]\n",
      "epoch:24 step:23094 [D loss: 0.649182, acc.: 67.19%] [G loss: 1.360678]\n",
      "epoch:24 step:23095 [D loss: 0.371700, acc.: 86.72%] [G loss: 2.680969]\n",
      "epoch:24 step:23096 [D loss: 0.704453, acc.: 56.25%] [G loss: 1.704643]\n",
      "epoch:24 step:23097 [D loss: 0.654242, acc.: 63.28%] [G loss: 2.632934]\n",
      "epoch:24 step:23098 [D loss: 0.587622, acc.: 68.75%] [G loss: 2.862091]\n",
      "epoch:24 step:23099 [D loss: 1.092127, acc.: 36.72%] [G loss: 1.943940]\n",
      "epoch:24 step:23100 [D loss: 0.388197, acc.: 84.38%] [G loss: 1.478503]\n",
      "epoch:24 step:23101 [D loss: 0.261087, acc.: 93.75%] [G loss: 1.413735]\n",
      "epoch:24 step:23102 [D loss: 0.227154, acc.: 95.31%] [G loss: 1.749671]\n",
      "epoch:24 step:23103 [D loss: 0.276215, acc.: 92.19%] [G loss: 1.467660]\n",
      "epoch:24 step:23104 [D loss: 0.533467, acc.: 65.62%] [G loss: 3.053522]\n",
      "epoch:24 step:23105 [D loss: 0.299015, acc.: 92.97%] [G loss: 1.653297]\n",
      "epoch:24 step:23106 [D loss: 0.476819, acc.: 74.22%] [G loss: 1.039373]\n",
      "epoch:24 step:23107 [D loss: 0.467098, acc.: 80.47%] [G loss: 1.100434]\n",
      "epoch:24 step:23108 [D loss: 0.789620, acc.: 57.03%] [G loss: 1.007314]\n",
      "epoch:24 step:23109 [D loss: 0.252433, acc.: 93.75%] [G loss: 1.255551]\n",
      "epoch:24 step:23110 [D loss: 0.960567, acc.: 53.12%] [G loss: 1.516117]\n",
      "epoch:24 step:23111 [D loss: 0.625311, acc.: 66.41%] [G loss: 1.257588]\n",
      "epoch:24 step:23112 [D loss: 0.605591, acc.: 67.97%] [G loss: 1.452952]\n",
      "epoch:24 step:23113 [D loss: 0.647844, acc.: 58.59%] [G loss: 1.218676]\n",
      "epoch:24 step:23114 [D loss: 0.359516, acc.: 84.38%] [G loss: 1.016200]\n",
      "epoch:24 step:23115 [D loss: 0.844137, acc.: 46.09%] [G loss: 0.586359]\n",
      "epoch:24 step:23116 [D loss: 0.339942, acc.: 85.16%] [G loss: 2.247441]\n",
      "epoch:24 step:23117 [D loss: 0.422998, acc.: 79.69%] [G loss: 2.614033]\n",
      "epoch:24 step:23118 [D loss: 0.498647, acc.: 73.44%] [G loss: 1.515254]\n",
      "epoch:24 step:23119 [D loss: 0.369183, acc.: 85.94%] [G loss: 0.794802]\n",
      "epoch:24 step:23120 [D loss: 0.185579, acc.: 98.44%] [G loss: 2.825305]\n",
      "epoch:24 step:23121 [D loss: 0.361172, acc.: 91.41%] [G loss: 1.108523]\n",
      "epoch:24 step:23122 [D loss: 0.791800, acc.: 56.25%] [G loss: 2.209056]\n",
      "epoch:24 step:23123 [D loss: 0.482231, acc.: 75.00%] [G loss: 0.896275]\n",
      "epoch:24 step:23124 [D loss: 0.498016, acc.: 75.78%] [G loss: 2.376981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23125 [D loss: 0.448394, acc.: 79.69%] [G loss: 1.524909]\n",
      "epoch:24 step:23126 [D loss: 0.323220, acc.: 87.50%] [G loss: 1.424531]\n",
      "epoch:24 step:23127 [D loss: 0.508372, acc.: 75.78%] [G loss: 1.573364]\n",
      "epoch:24 step:23128 [D loss: 0.473192, acc.: 77.34%] [G loss: 2.546834]\n",
      "epoch:24 step:23129 [D loss: 0.364736, acc.: 87.50%] [G loss: 1.944574]\n",
      "epoch:24 step:23130 [D loss: 0.276748, acc.: 94.53%] [G loss: 2.249246]\n",
      "epoch:24 step:23131 [D loss: 0.663848, acc.: 60.16%] [G loss: 1.129157]\n",
      "epoch:24 step:23132 [D loss: 0.601071, acc.: 67.19%] [G loss: 1.500963]\n",
      "epoch:24 step:23133 [D loss: 0.681298, acc.: 63.28%] [G loss: 1.386771]\n",
      "epoch:24 step:23134 [D loss: 0.115119, acc.: 94.53%] [G loss: 1.580509]\n",
      "epoch:24 step:23135 [D loss: 0.439768, acc.: 82.81%] [G loss: 3.276141]\n",
      "epoch:24 step:23136 [D loss: 0.566942, acc.: 69.53%] [G loss: 1.094204]\n",
      "epoch:24 step:23137 [D loss: 0.395848, acc.: 83.59%] [G loss: 1.979914]\n",
      "epoch:24 step:23138 [D loss: 0.273791, acc.: 93.75%] [G loss: 1.531894]\n",
      "epoch:24 step:23139 [D loss: 0.230060, acc.: 96.88%] [G loss: 1.248450]\n",
      "epoch:24 step:23140 [D loss: 0.343445, acc.: 92.19%] [G loss: 2.054628]\n",
      "epoch:24 step:23141 [D loss: 0.228455, acc.: 94.53%] [G loss: 3.585755]\n",
      "epoch:24 step:23142 [D loss: 0.343104, acc.: 91.41%] [G loss: 1.547534]\n",
      "epoch:24 step:23143 [D loss: 0.197624, acc.: 96.09%] [G loss: 0.846966]\n",
      "epoch:24 step:23144 [D loss: 0.278617, acc.: 89.06%] [G loss: 0.962321]\n",
      "epoch:24 step:23145 [D loss: 0.450169, acc.: 75.78%] [G loss: 1.712726]\n",
      "epoch:24 step:23146 [D loss: 0.516907, acc.: 76.56%] [G loss: 3.510017]\n",
      "epoch:24 step:23147 [D loss: 0.181571, acc.: 94.53%] [G loss: 1.377107]\n",
      "epoch:24 step:23148 [D loss: 0.524545, acc.: 71.09%] [G loss: 2.261203]\n",
      "epoch:24 step:23149 [D loss: 0.893457, acc.: 44.53%] [G loss: 1.615061]\n",
      "epoch:24 step:23150 [D loss: 0.253310, acc.: 93.75%] [G loss: 2.095185]\n",
      "epoch:24 step:23151 [D loss: 0.397605, acc.: 75.00%] [G loss: 2.183480]\n",
      "epoch:24 step:23152 [D loss: 0.287906, acc.: 93.75%] [G loss: 2.864990]\n",
      "epoch:24 step:23153 [D loss: 0.177420, acc.: 96.88%] [G loss: 0.915078]\n",
      "epoch:24 step:23154 [D loss: 0.259106, acc.: 96.09%] [G loss: 1.726876]\n",
      "epoch:24 step:23155 [D loss: 0.204956, acc.: 96.09%] [G loss: 4.387887]\n",
      "epoch:24 step:23156 [D loss: 0.480377, acc.: 71.09%] [G loss: 1.741660]\n",
      "epoch:24 step:23157 [D loss: 0.145052, acc.: 95.31%] [G loss: 4.041339]\n",
      "epoch:24 step:23158 [D loss: 0.679493, acc.: 66.41%] [G loss: 0.993048]\n",
      "epoch:24 step:23159 [D loss: 0.417632, acc.: 82.03%] [G loss: 0.490509]\n",
      "epoch:24 step:23160 [D loss: 0.713903, acc.: 59.38%] [G loss: 2.820620]\n",
      "epoch:24 step:23161 [D loss: 0.340067, acc.: 89.06%] [G loss: 3.154367]\n",
      "epoch:24 step:23162 [D loss: 0.356821, acc.: 88.28%] [G loss: 2.704724]\n",
      "epoch:24 step:23163 [D loss: 0.388452, acc.: 81.25%] [G loss: 2.434032]\n",
      "epoch:24 step:23164 [D loss: 0.471829, acc.: 77.34%] [G loss: 3.059258]\n",
      "epoch:24 step:23165 [D loss: 0.334674, acc.: 91.41%] [G loss: 1.564246]\n",
      "epoch:24 step:23166 [D loss: 0.609892, acc.: 70.31%] [G loss: 1.678720]\n",
      "epoch:24 step:23167 [D loss: 0.967409, acc.: 47.66%] [G loss: 0.926062]\n",
      "epoch:24 step:23168 [D loss: 0.586015, acc.: 73.44%] [G loss: 1.809737]\n",
      "epoch:24 step:23169 [D loss: 0.777998, acc.: 58.59%] [G loss: 1.607830]\n",
      "epoch:24 step:23170 [D loss: 0.459036, acc.: 73.44%] [G loss: 2.874733]\n",
      "epoch:24 step:23171 [D loss: 0.887579, acc.: 49.22%] [G loss: 1.570467]\n",
      "epoch:24 step:23172 [D loss: 0.336424, acc.: 91.41%] [G loss: 1.701268]\n",
      "epoch:24 step:23173 [D loss: 0.212229, acc.: 96.09%] [G loss: 1.875776]\n",
      "epoch:24 step:23174 [D loss: 0.600192, acc.: 67.97%] [G loss: 1.469548]\n",
      "epoch:24 step:23175 [D loss: 0.407843, acc.: 86.72%] [G loss: 1.640659]\n",
      "epoch:24 step:23176 [D loss: 0.378438, acc.: 88.28%] [G loss: 1.757334]\n",
      "epoch:24 step:23177 [D loss: 0.282190, acc.: 91.41%] [G loss: 0.558316]\n",
      "epoch:24 step:23178 [D loss: 0.203777, acc.: 96.88%] [G loss: 1.240248]\n",
      "epoch:24 step:23179 [D loss: 0.245689, acc.: 91.41%] [G loss: 1.659477]\n",
      "epoch:24 step:23180 [D loss: 0.173372, acc.: 96.09%] [G loss: 1.851431]\n",
      "epoch:24 step:23181 [D loss: 0.881075, acc.: 41.41%] [G loss: 1.760770]\n",
      "epoch:24 step:23182 [D loss: 0.378323, acc.: 78.12%] [G loss: 0.925945]\n",
      "epoch:24 step:23183 [D loss: 0.929965, acc.: 51.56%] [G loss: 1.672076]\n",
      "epoch:24 step:23184 [D loss: 0.194581, acc.: 96.09%] [G loss: 1.595529]\n",
      "epoch:24 step:23185 [D loss: 0.291588, acc.: 91.41%] [G loss: 0.509698]\n",
      "epoch:24 step:23186 [D loss: 0.257208, acc.: 91.41%] [G loss: 1.044015]\n",
      "epoch:24 step:23187 [D loss: 0.149945, acc.: 99.22%] [G loss: 1.043222]\n",
      "epoch:24 step:23188 [D loss: 1.060735, acc.: 45.31%] [G loss: 2.024240]\n",
      "epoch:24 step:23189 [D loss: 0.292950, acc.: 89.84%] [G loss: 4.181143]\n",
      "epoch:24 step:23190 [D loss: 0.421183, acc.: 80.47%] [G loss: 1.763021]\n",
      "epoch:24 step:23191 [D loss: 0.300427, acc.: 92.19%] [G loss: 0.260288]\n",
      "epoch:24 step:23192 [D loss: 0.338003, acc.: 90.62%] [G loss: 2.693108]\n",
      "epoch:24 step:23193 [D loss: 0.119665, acc.: 98.44%] [G loss: 3.614476]\n",
      "epoch:24 step:23194 [D loss: 0.709522, acc.: 60.94%] [G loss: 1.741966]\n",
      "epoch:24 step:23195 [D loss: 0.167949, acc.: 96.88%] [G loss: 2.046462]\n",
      "epoch:24 step:23196 [D loss: 0.508827, acc.: 75.00%] [G loss: 3.533153]\n",
      "epoch:24 step:23197 [D loss: 0.773724, acc.: 59.38%] [G loss: 4.124169]\n",
      "epoch:24 step:23198 [D loss: 0.457029, acc.: 75.00%] [G loss: 3.271812]\n",
      "epoch:24 step:23199 [D loss: 0.262715, acc.: 94.53%] [G loss: 3.519037]\n",
      "epoch:24 step:23200 [D loss: 0.506679, acc.: 75.78%] [G loss: 0.922049]\n",
      "epoch:24 step:23201 [D loss: 0.230199, acc.: 93.75%] [G loss: 0.649866]\n",
      "epoch:24 step:23202 [D loss: 0.184681, acc.: 97.66%] [G loss: 2.230361]\n",
      "epoch:24 step:23203 [D loss: 0.242283, acc.: 95.31%] [G loss: 1.447773]\n",
      "epoch:24 step:23204 [D loss: 0.431466, acc.: 78.12%] [G loss: 1.369285]\n",
      "epoch:24 step:23205 [D loss: 0.722270, acc.: 58.59%] [G loss: 1.038397]\n",
      "epoch:24 step:23206 [D loss: 0.417770, acc.: 75.00%] [G loss: 1.362224]\n",
      "epoch:24 step:23207 [D loss: 0.506532, acc.: 72.66%] [G loss: 3.156424]\n",
      "epoch:24 step:23208 [D loss: 0.599452, acc.: 66.41%] [G loss: 3.452834]\n",
      "epoch:24 step:23209 [D loss: 0.417356, acc.: 85.16%] [G loss: 2.966573]\n",
      "epoch:24 step:23210 [D loss: 0.563361, acc.: 67.97%] [G loss: 3.093574]\n",
      "epoch:24 step:23211 [D loss: 0.474893, acc.: 77.34%] [G loss: 1.649843]\n",
      "epoch:24 step:23212 [D loss: 0.168167, acc.: 96.88%] [G loss: 2.145379]\n",
      "epoch:24 step:23213 [D loss: 0.109368, acc.: 96.09%] [G loss: 3.339353]\n",
      "epoch:24 step:23214 [D loss: 0.554725, acc.: 68.75%] [G loss: 0.685755]\n",
      "epoch:24 step:23215 [D loss: 0.354838, acc.: 89.84%] [G loss: 3.681347]\n",
      "epoch:24 step:23216 [D loss: 0.467020, acc.: 72.66%] [G loss: 2.938945]\n",
      "epoch:24 step:23217 [D loss: 0.237177, acc.: 92.97%] [G loss: 2.982866]\n",
      "epoch:24 step:23218 [D loss: 0.868999, acc.: 57.81%] [G loss: 2.023233]\n",
      "epoch:24 step:23219 [D loss: 0.520802, acc.: 74.22%] [G loss: 1.828613]\n",
      "epoch:24 step:23220 [D loss: 0.310226, acc.: 84.38%] [G loss: 1.823245]\n",
      "epoch:24 step:23221 [D loss: 0.400390, acc.: 85.16%] [G loss: 2.683842]\n",
      "epoch:24 step:23222 [D loss: 0.374450, acc.: 85.16%] [G loss: 0.966272]\n",
      "epoch:24 step:23223 [D loss: 0.445059, acc.: 80.47%] [G loss: 2.739119]\n",
      "epoch:24 step:23224 [D loss: 0.642280, acc.: 61.72%] [G loss: 1.740327]\n",
      "epoch:24 step:23225 [D loss: 0.407789, acc.: 82.03%] [G loss: 1.942903]\n",
      "epoch:24 step:23226 [D loss: 0.989427, acc.: 59.38%] [G loss: 2.484416]\n",
      "epoch:24 step:23227 [D loss: 0.843210, acc.: 48.44%] [G loss: 1.004571]\n",
      "epoch:24 step:23228 [D loss: 0.436822, acc.: 86.72%] [G loss: 0.981087]\n",
      "epoch:24 step:23229 [D loss: 0.359581, acc.: 88.28%] [G loss: 2.173470]\n",
      "epoch:24 step:23230 [D loss: 0.280772, acc.: 89.84%] [G loss: 1.165923]\n",
      "epoch:24 step:23231 [D loss: 0.201134, acc.: 93.75%] [G loss: 0.998358]\n",
      "epoch:24 step:23232 [D loss: 0.380223, acc.: 85.94%] [G loss: 0.340009]\n",
      "epoch:24 step:23233 [D loss: 1.134055, acc.: 46.88%] [G loss: 1.339460]\n",
      "epoch:24 step:23234 [D loss: 0.249383, acc.: 96.09%] [G loss: 1.359691]\n",
      "epoch:24 step:23235 [D loss: 0.457581, acc.: 83.59%] [G loss: 3.052483]\n",
      "epoch:24 step:23236 [D loss: 0.288920, acc.: 89.84%] [G loss: 1.997337]\n",
      "epoch:24 step:23237 [D loss: 0.245923, acc.: 92.97%] [G loss: 1.084099]\n",
      "epoch:24 step:23238 [D loss: 0.410459, acc.: 85.16%] [G loss: 1.913707]\n",
      "epoch:24 step:23239 [D loss: 0.722276, acc.: 58.59%] [G loss: 1.566834]\n",
      "epoch:24 step:23240 [D loss: 0.132113, acc.: 98.44%] [G loss: 1.123612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23241 [D loss: 0.181961, acc.: 98.44%] [G loss: 0.989134]\n",
      "epoch:24 step:23242 [D loss: 0.356701, acc.: 82.03%] [G loss: 1.602687]\n",
      "epoch:24 step:23243 [D loss: 0.645564, acc.: 64.06%] [G loss: 0.727277]\n",
      "epoch:24 step:23244 [D loss: 0.335961, acc.: 88.28%] [G loss: 1.173261]\n",
      "epoch:24 step:23245 [D loss: 0.237042, acc.: 95.31%] [G loss: 1.445136]\n",
      "epoch:24 step:23246 [D loss: 1.321857, acc.: 27.34%] [G loss: 0.762605]\n",
      "epoch:24 step:23247 [D loss: 0.430901, acc.: 77.34%] [G loss: 1.227005]\n",
      "epoch:24 step:23248 [D loss: 0.487952, acc.: 82.03%] [G loss: 1.001688]\n",
      "epoch:24 step:23249 [D loss: 0.213419, acc.: 94.53%] [G loss: 1.664163]\n",
      "epoch:24 step:23250 [D loss: 0.485932, acc.: 70.31%] [G loss: 2.092026]\n",
      "epoch:24 step:23251 [D loss: 0.292107, acc.: 92.19%] [G loss: 1.751835]\n",
      "epoch:24 step:23252 [D loss: 0.745367, acc.: 60.16%] [G loss: 1.823332]\n",
      "epoch:24 step:23253 [D loss: 0.487960, acc.: 71.09%] [G loss: 2.365274]\n",
      "epoch:24 step:23254 [D loss: 0.321340, acc.: 88.28%] [G loss: 2.371131]\n",
      "epoch:24 step:23255 [D loss: 0.180981, acc.: 97.66%] [G loss: 1.972544]\n",
      "epoch:24 step:23256 [D loss: 0.332953, acc.: 90.62%] [G loss: 2.113203]\n",
      "epoch:24 step:23257 [D loss: 0.367727, acc.: 87.50%] [G loss: 1.687210]\n",
      "epoch:24 step:23258 [D loss: 0.794664, acc.: 54.69%] [G loss: 2.181724]\n",
      "epoch:24 step:23259 [D loss: 0.372957, acc.: 85.94%] [G loss: 2.242265]\n",
      "epoch:24 step:23260 [D loss: 0.266067, acc.: 96.88%] [G loss: 1.611280]\n",
      "epoch:24 step:23261 [D loss: 1.312551, acc.: 45.31%] [G loss: 1.963718]\n",
      "epoch:24 step:23262 [D loss: 0.283136, acc.: 87.50%] [G loss: 1.958035]\n",
      "epoch:24 step:23263 [D loss: 0.369874, acc.: 89.84%] [G loss: 1.291806]\n",
      "epoch:24 step:23264 [D loss: 0.483662, acc.: 76.56%] [G loss: 1.864315]\n",
      "epoch:24 step:23265 [D loss: 0.231482, acc.: 92.19%] [G loss: 2.905352]\n",
      "epoch:24 step:23266 [D loss: 0.292745, acc.: 92.19%] [G loss: 1.988733]\n",
      "epoch:24 step:23267 [D loss: 0.428333, acc.: 83.59%] [G loss: 1.903874]\n",
      "epoch:24 step:23268 [D loss: 0.287757, acc.: 91.41%] [G loss: 1.764056]\n",
      "epoch:24 step:23269 [D loss: 0.407663, acc.: 82.03%] [G loss: 1.329322]\n",
      "epoch:24 step:23270 [D loss: 0.329450, acc.: 90.62%] [G loss: 1.380427]\n",
      "epoch:24 step:23271 [D loss: 0.561238, acc.: 67.97%] [G loss: 3.413572]\n",
      "epoch:24 step:23272 [D loss: 0.459265, acc.: 73.44%] [G loss: 0.976333]\n",
      "epoch:24 step:23273 [D loss: 0.428059, acc.: 79.69%] [G loss: 2.597301]\n",
      "epoch:24 step:23274 [D loss: 0.172993, acc.: 96.09%] [G loss: 1.514017]\n",
      "epoch:24 step:23275 [D loss: 0.328736, acc.: 88.28%] [G loss: 1.700101]\n",
      "epoch:24 step:23276 [D loss: 0.317305, acc.: 90.62%] [G loss: 1.902666]\n",
      "epoch:24 step:23277 [D loss: 0.690727, acc.: 60.16%] [G loss: 0.977160]\n",
      "epoch:24 step:23278 [D loss: 0.563086, acc.: 67.19%] [G loss: 0.448762]\n",
      "epoch:24 step:23279 [D loss: 0.297809, acc.: 92.19%] [G loss: 2.409125]\n",
      "epoch:24 step:23280 [D loss: 0.260998, acc.: 92.97%] [G loss: 1.551505]\n",
      "epoch:24 step:23281 [D loss: 0.536499, acc.: 76.56%] [G loss: 1.265040]\n",
      "epoch:24 step:23282 [D loss: 0.581448, acc.: 68.75%] [G loss: 2.579651]\n",
      "epoch:24 step:23283 [D loss: 0.346288, acc.: 89.06%] [G loss: 2.742405]\n",
      "epoch:24 step:23284 [D loss: 0.448048, acc.: 78.12%] [G loss: 2.391765]\n",
      "epoch:24 step:23285 [D loss: 0.972666, acc.: 49.22%] [G loss: 1.027454]\n",
      "epoch:24 step:23286 [D loss: 0.471020, acc.: 79.69%] [G loss: 1.660754]\n",
      "epoch:24 step:23287 [D loss: 0.162779, acc.: 96.09%] [G loss: 1.912240]\n",
      "epoch:24 step:23288 [D loss: 0.513648, acc.: 73.44%] [G loss: 1.682907]\n",
      "epoch:24 step:23289 [D loss: 0.501719, acc.: 77.34%] [G loss: 0.909182]\n",
      "epoch:24 step:23290 [D loss: 0.523719, acc.: 66.41%] [G loss: 2.037151]\n",
      "epoch:24 step:23291 [D loss: 0.358400, acc.: 84.38%] [G loss: 1.808518]\n",
      "epoch:24 step:23292 [D loss: 0.404409, acc.: 84.38%] [G loss: 0.707485]\n",
      "epoch:24 step:23293 [D loss: 1.211656, acc.: 50.78%] [G loss: 1.989537]\n",
      "epoch:24 step:23294 [D loss: 0.816632, acc.: 53.12%] [G loss: 2.246597]\n",
      "epoch:24 step:23295 [D loss: 0.320733, acc.: 85.94%] [G loss: 1.872819]\n",
      "epoch:24 step:23296 [D loss: 0.413102, acc.: 85.16%] [G loss: 2.258209]\n",
      "epoch:24 step:23297 [D loss: 0.406638, acc.: 84.38%] [G loss: 1.038461]\n",
      "epoch:24 step:23298 [D loss: 0.285890, acc.: 94.53%] [G loss: 2.994391]\n",
      "epoch:24 step:23299 [D loss: 0.482598, acc.: 75.78%] [G loss: 2.449708]\n",
      "epoch:24 step:23300 [D loss: 0.274182, acc.: 93.75%] [G loss: 1.579632]\n",
      "epoch:24 step:23301 [D loss: 0.689013, acc.: 62.50%] [G loss: 2.869113]\n",
      "epoch:24 step:23302 [D loss: 0.457032, acc.: 76.56%] [G loss: 1.963223]\n",
      "epoch:24 step:23303 [D loss: 0.401437, acc.: 82.81%] [G loss: 1.052408]\n",
      "epoch:24 step:23304 [D loss: 0.581389, acc.: 70.31%] [G loss: 1.924887]\n",
      "epoch:24 step:23305 [D loss: 0.417034, acc.: 78.12%] [G loss: 1.650416]\n",
      "epoch:24 step:23306 [D loss: 0.534594, acc.: 73.44%] [G loss: 1.659689]\n",
      "epoch:24 step:23307 [D loss: 0.315189, acc.: 89.84%] [G loss: 1.307106]\n",
      "epoch:24 step:23308 [D loss: 0.148568, acc.: 97.66%] [G loss: 1.693926]\n",
      "epoch:24 step:23309 [D loss: 0.759881, acc.: 60.94%] [G loss: 2.259875]\n",
      "epoch:24 step:23310 [D loss: 0.312424, acc.: 92.19%] [G loss: 1.959929]\n",
      "epoch:24 step:23311 [D loss: 0.323705, acc.: 89.84%] [G loss: 1.633310]\n",
      "epoch:24 step:23312 [D loss: 0.457335, acc.: 75.00%] [G loss: 2.460912]\n",
      "epoch:24 step:23313 [D loss: 0.476077, acc.: 73.44%] [G loss: 1.709911]\n",
      "epoch:24 step:23314 [D loss: 0.589867, acc.: 64.84%] [G loss: 2.900125]\n",
      "epoch:24 step:23315 [D loss: 0.335743, acc.: 90.62%] [G loss: 2.177730]\n",
      "epoch:24 step:23316 [D loss: 0.291176, acc.: 83.59%] [G loss: 1.588938]\n",
      "epoch:24 step:23317 [D loss: 0.470350, acc.: 83.59%] [G loss: 0.773715]\n",
      "epoch:24 step:23318 [D loss: 0.166529, acc.: 94.53%] [G loss: 2.141784]\n",
      "epoch:24 step:23319 [D loss: 0.350987, acc.: 91.41%] [G loss: 2.939015]\n",
      "epoch:24 step:23320 [D loss: 0.654369, acc.: 64.84%] [G loss: 2.732209]\n",
      "epoch:24 step:23321 [D loss: 0.251514, acc.: 92.97%] [G loss: 0.592850]\n",
      "epoch:24 step:23322 [D loss: 0.246398, acc.: 92.97%] [G loss: 2.136396]\n",
      "epoch:24 step:23323 [D loss: 0.996858, acc.: 39.06%] [G loss: 3.014888]\n",
      "epoch:24 step:23324 [D loss: 0.128393, acc.: 98.44%] [G loss: 2.706934]\n",
      "epoch:24 step:23325 [D loss: 0.696434, acc.: 57.03%] [G loss: 2.303566]\n",
      "epoch:24 step:23326 [D loss: 0.394784, acc.: 85.94%] [G loss: 2.630289]\n",
      "epoch:24 step:23327 [D loss: 0.430347, acc.: 85.94%] [G loss: 2.461916]\n",
      "epoch:24 step:23328 [D loss: 0.180362, acc.: 95.31%] [G loss: 2.136656]\n",
      "epoch:24 step:23329 [D loss: 0.253775, acc.: 94.53%] [G loss: 0.922081]\n",
      "epoch:24 step:23330 [D loss: 0.517280, acc.: 72.66%] [G loss: 1.178126]\n",
      "epoch:24 step:23331 [D loss: 0.502770, acc.: 78.91%] [G loss: 1.544919]\n",
      "epoch:24 step:23332 [D loss: 0.265434, acc.: 94.53%] [G loss: 2.748395]\n",
      "epoch:24 step:23333 [D loss: 0.157826, acc.: 97.66%] [G loss: 1.491134]\n",
      "epoch:24 step:23334 [D loss: 0.984323, acc.: 50.78%] [G loss: 1.634915]\n",
      "epoch:24 step:23335 [D loss: 0.343670, acc.: 85.94%] [G loss: 1.733004]\n",
      "epoch:24 step:23336 [D loss: 0.184922, acc.: 95.31%] [G loss: 1.916875]\n",
      "epoch:24 step:23337 [D loss: 0.485863, acc.: 76.56%] [G loss: 3.211863]\n",
      "epoch:24 step:23338 [D loss: 0.423137, acc.: 74.22%] [G loss: 2.886784]\n",
      "epoch:24 step:23339 [D loss: 1.196746, acc.: 53.12%] [G loss: 1.567738]\n",
      "epoch:24 step:23340 [D loss: 0.612911, acc.: 67.97%] [G loss: 1.126623]\n",
      "epoch:24 step:23341 [D loss: 0.230155, acc.: 93.75%] [G loss: 2.025226]\n",
      "epoch:24 step:23342 [D loss: 0.837761, acc.: 56.25%] [G loss: 1.170882]\n",
      "epoch:24 step:23343 [D loss: 0.373286, acc.: 82.03%] [G loss: 3.373529]\n",
      "epoch:24 step:23344 [D loss: 0.522620, acc.: 66.41%] [G loss: 2.665583]\n",
      "epoch:24 step:23345 [D loss: 1.163103, acc.: 44.53%] [G loss: 2.256472]\n",
      "epoch:24 step:23346 [D loss: 0.263381, acc.: 92.19%] [G loss: 2.297597]\n",
      "epoch:24 step:23347 [D loss: 0.292088, acc.: 91.41%] [G loss: 1.223073]\n",
      "epoch:24 step:23348 [D loss: 0.696361, acc.: 58.59%] [G loss: 2.305184]\n",
      "epoch:24 step:23349 [D loss: 0.283711, acc.: 92.97%] [G loss: 2.824828]\n",
      "epoch:24 step:23350 [D loss: 0.506783, acc.: 75.00%] [G loss: 2.070294]\n",
      "epoch:24 step:23351 [D loss: 0.303976, acc.: 92.97%] [G loss: 2.085679]\n",
      "epoch:24 step:23352 [D loss: 0.274999, acc.: 91.41%] [G loss: 3.012450]\n",
      "epoch:24 step:23353 [D loss: 0.520125, acc.: 72.66%] [G loss: 2.057318]\n",
      "epoch:24 step:23354 [D loss: 0.345789, acc.: 90.62%] [G loss: 0.725869]\n",
      "epoch:24 step:23355 [D loss: 0.450707, acc.: 82.03%] [G loss: 0.690736]\n",
      "epoch:24 step:23356 [D loss: 0.141365, acc.: 98.44%] [G loss: 1.932444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23357 [D loss: 0.244918, acc.: 93.75%] [G loss: 3.395914]\n",
      "epoch:24 step:23358 [D loss: 0.488729, acc.: 79.69%] [G loss: 4.331329]\n",
      "epoch:24 step:23359 [D loss: 1.022176, acc.: 32.03%] [G loss: 1.442269]\n",
      "epoch:24 step:23360 [D loss: 0.465423, acc.: 72.66%] [G loss: 2.206985]\n",
      "epoch:24 step:23361 [D loss: 0.187909, acc.: 95.31%] [G loss: 1.249176]\n",
      "epoch:24 step:23362 [D loss: 0.443330, acc.: 83.59%] [G loss: 1.625303]\n",
      "epoch:24 step:23363 [D loss: 0.078588, acc.: 99.22%] [G loss: 0.309280]\n",
      "epoch:24 step:23364 [D loss: 0.529269, acc.: 67.19%] [G loss: 3.245736]\n",
      "epoch:24 step:23365 [D loss: 0.295419, acc.: 93.75%] [G loss: 2.667622]\n",
      "epoch:24 step:23366 [D loss: 1.177444, acc.: 53.91%] [G loss: 0.817777]\n",
      "epoch:24 step:23367 [D loss: 0.453730, acc.: 76.56%] [G loss: 1.937580]\n",
      "epoch:24 step:23368 [D loss: 0.203780, acc.: 96.09%] [G loss: 2.211920]\n",
      "epoch:24 step:23369 [D loss: 0.335734, acc.: 83.59%] [G loss: 0.870032]\n",
      "epoch:24 step:23370 [D loss: 0.292500, acc.: 92.97%] [G loss: 3.348443]\n",
      "epoch:24 step:23371 [D loss: 0.410234, acc.: 79.69%] [G loss: 1.747734]\n",
      "epoch:24 step:23372 [D loss: 0.169804, acc.: 95.31%] [G loss: 2.445141]\n",
      "epoch:24 step:23373 [D loss: 0.516667, acc.: 78.12%] [G loss: 1.132983]\n",
      "epoch:24 step:23374 [D loss: 0.297732, acc.: 89.84%] [G loss: 1.503754]\n",
      "epoch:24 step:23375 [D loss: 0.350262, acc.: 85.16%] [G loss: 2.354509]\n",
      "epoch:24 step:23376 [D loss: 0.372346, acc.: 85.16%] [G loss: 0.832825]\n",
      "epoch:24 step:23377 [D loss: 0.481402, acc.: 77.34%] [G loss: 1.340533]\n",
      "epoch:24 step:23378 [D loss: 0.710499, acc.: 56.25%] [G loss: 1.989057]\n",
      "epoch:24 step:23379 [D loss: 0.164009, acc.: 96.09%] [G loss: 2.813875]\n",
      "epoch:24 step:23380 [D loss: 0.417316, acc.: 82.03%] [G loss: 1.360258]\n",
      "epoch:24 step:23381 [D loss: 0.508916, acc.: 72.66%] [G loss: 2.024108]\n",
      "epoch:24 step:23382 [D loss: 0.191489, acc.: 93.75%] [G loss: 3.136354]\n",
      "epoch:24 step:23383 [D loss: 0.605001, acc.: 64.84%] [G loss: 3.388451]\n",
      "epoch:24 step:23384 [D loss: 0.281773, acc.: 90.62%] [G loss: 2.759608]\n",
      "epoch:24 step:23385 [D loss: 0.202420, acc.: 96.09%] [G loss: 2.740154]\n",
      "epoch:24 step:23386 [D loss: 0.360605, acc.: 88.28%] [G loss: 1.998168]\n",
      "epoch:24 step:23387 [D loss: 0.574434, acc.: 67.97%] [G loss: 2.152223]\n",
      "epoch:24 step:23388 [D loss: 0.605649, acc.: 65.62%] [G loss: 1.753664]\n",
      "epoch:24 step:23389 [D loss: 0.240941, acc.: 92.97%] [G loss: 0.729877]\n",
      "epoch:24 step:23390 [D loss: 0.202783, acc.: 94.53%] [G loss: 0.851645]\n",
      "epoch:24 step:23391 [D loss: 0.255446, acc.: 93.75%] [G loss: 3.245670]\n",
      "epoch:24 step:23392 [D loss: 0.190908, acc.: 97.66%] [G loss: 2.543099]\n",
      "epoch:24 step:23393 [D loss: 0.314807, acc.: 91.41%] [G loss: 3.740242]\n",
      "epoch:24 step:23394 [D loss: 0.314571, acc.: 90.62%] [G loss: 2.916458]\n",
      "epoch:24 step:23395 [D loss: 0.708068, acc.: 60.94%] [G loss: 1.280458]\n",
      "epoch:24 step:23396 [D loss: 0.157280, acc.: 96.88%] [G loss: 0.969797]\n",
      "epoch:24 step:23397 [D loss: 0.414455, acc.: 83.59%] [G loss: 1.064494]\n",
      "epoch:24 step:23398 [D loss: 0.248879, acc.: 93.75%] [G loss: 2.935628]\n",
      "epoch:24 step:23399 [D loss: 0.214797, acc.: 96.88%] [G loss: 4.988552]\n",
      "epoch:24 step:23400 [D loss: 0.460776, acc.: 73.44%] [G loss: 0.948100]\n",
      "epoch:24 step:23401 [D loss: 0.297141, acc.: 92.19%] [G loss: 2.528080]\n",
      "epoch:24 step:23402 [D loss: 0.219860, acc.: 92.19%] [G loss: 1.495784]\n",
      "epoch:24 step:23403 [D loss: 0.266607, acc.: 91.41%] [G loss: 1.116068]\n",
      "epoch:24 step:23404 [D loss: 1.176004, acc.: 46.88%] [G loss: 2.530299]\n",
      "epoch:24 step:23405 [D loss: 0.737771, acc.: 63.28%] [G loss: 1.396783]\n",
      "epoch:24 step:23406 [D loss: 0.351784, acc.: 83.59%] [G loss: 2.340683]\n",
      "epoch:24 step:23407 [D loss: 0.580240, acc.: 64.84%] [G loss: 2.676163]\n",
      "epoch:24 step:23408 [D loss: 0.444831, acc.: 76.56%] [G loss: 0.856047]\n",
      "epoch:24 step:23409 [D loss: 0.258736, acc.: 88.28%] [G loss: 1.754285]\n",
      "epoch:24 step:23410 [D loss: 0.791159, acc.: 56.25%] [G loss: 2.243648]\n",
      "epoch:24 step:23411 [D loss: 0.212125, acc.: 92.97%] [G loss: 1.047317]\n",
      "epoch:24 step:23412 [D loss: 0.440039, acc.: 76.56%] [G loss: 0.389677]\n",
      "epoch:24 step:23413 [D loss: 0.382320, acc.: 86.72%] [G loss: 1.171133]\n",
      "epoch:24 step:23414 [D loss: 0.381146, acc.: 78.12%] [G loss: 0.942208]\n",
      "epoch:24 step:23415 [D loss: 0.973056, acc.: 55.47%] [G loss: 3.330994]\n",
      "epoch:24 step:23416 [D loss: 0.116492, acc.: 99.22%] [G loss: 4.159825]\n",
      "epoch:24 step:23417 [D loss: 0.411892, acc.: 85.94%] [G loss: 1.445418]\n",
      "epoch:24 step:23418 [D loss: 0.314397, acc.: 85.94%] [G loss: 2.016497]\n",
      "epoch:24 step:23419 [D loss: 0.203968, acc.: 90.62%] [G loss: 1.133720]\n",
      "epoch:24 step:23420 [D loss: 0.438206, acc.: 81.25%] [G loss: 3.227887]\n",
      "epoch:24 step:23421 [D loss: 0.415226, acc.: 84.38%] [G loss: 1.197848]\n",
      "epoch:24 step:23422 [D loss: 0.393922, acc.: 88.28%] [G loss: 1.101107]\n",
      "epoch:24 step:23423 [D loss: 0.600047, acc.: 66.41%] [G loss: 1.254872]\n",
      "epoch:24 step:23424 [D loss: 0.353653, acc.: 86.72%] [G loss: 1.604245]\n",
      "epoch:24 step:23425 [D loss: 0.765294, acc.: 57.03%] [G loss: 2.341753]\n",
      "epoch:25 step:23426 [D loss: 0.377322, acc.: 89.06%] [G loss: 0.688616]\n",
      "epoch:25 step:23427 [D loss: 0.111250, acc.: 100.00%] [G loss: 0.707729]\n",
      "epoch:25 step:23428 [D loss: 0.107269, acc.: 98.44%] [G loss: 1.401272]\n",
      "epoch:25 step:23429 [D loss: 0.301229, acc.: 91.41%] [G loss: 3.953100]\n",
      "epoch:25 step:23430 [D loss: 0.273969, acc.: 92.19%] [G loss: 1.636032]\n",
      "epoch:25 step:23431 [D loss: 0.656121, acc.: 62.50%] [G loss: 2.413151]\n",
      "epoch:25 step:23432 [D loss: 0.348857, acc.: 87.50%] [G loss: 1.528384]\n",
      "epoch:25 step:23433 [D loss: 0.427313, acc.: 79.69%] [G loss: 1.049133]\n",
      "epoch:25 step:23434 [D loss: 0.370114, acc.: 82.03%] [G loss: 2.113945]\n",
      "epoch:25 step:23435 [D loss: 0.958365, acc.: 50.78%] [G loss: 0.986040]\n",
      "epoch:25 step:23436 [D loss: 0.344103, acc.: 88.28%] [G loss: 0.528410]\n",
      "epoch:25 step:23437 [D loss: 0.263559, acc.: 91.41%] [G loss: 3.896300]\n",
      "epoch:25 step:23438 [D loss: 0.333691, acc.: 87.50%] [G loss: 2.058511]\n",
      "epoch:25 step:23439 [D loss: 0.254294, acc.: 92.19%] [G loss: 1.103064]\n",
      "epoch:25 step:23440 [D loss: 0.683825, acc.: 61.72%] [G loss: 3.055897]\n",
      "epoch:25 step:23441 [D loss: 0.267638, acc.: 92.19%] [G loss: 2.633880]\n",
      "epoch:25 step:23442 [D loss: 0.272302, acc.: 89.06%] [G loss: 1.935012]\n",
      "epoch:25 step:23443 [D loss: 0.448417, acc.: 81.25%] [G loss: 1.208500]\n",
      "epoch:25 step:23444 [D loss: 0.325998, acc.: 83.59%] [G loss: 0.921230]\n",
      "epoch:25 step:23445 [D loss: 0.419451, acc.: 75.78%] [G loss: 1.807605]\n",
      "epoch:25 step:23446 [D loss: 0.347172, acc.: 87.50%] [G loss: 1.920017]\n",
      "epoch:25 step:23447 [D loss: 0.877343, acc.: 49.22%] [G loss: 0.436105]\n",
      "epoch:25 step:23448 [D loss: 0.157709, acc.: 97.66%] [G loss: 1.759965]\n",
      "epoch:25 step:23449 [D loss: 0.137129, acc.: 97.66%] [G loss: 1.484718]\n",
      "epoch:25 step:23450 [D loss: 0.224101, acc.: 92.97%] [G loss: 1.129930]\n",
      "epoch:25 step:23451 [D loss: 0.145439, acc.: 96.88%] [G loss: 4.058092]\n",
      "epoch:25 step:23452 [D loss: 0.349212, acc.: 85.16%] [G loss: 2.910825]\n",
      "epoch:25 step:23453 [D loss: 0.291752, acc.: 89.84%] [G loss: 0.970087]\n",
      "epoch:25 step:23454 [D loss: 0.243029, acc.: 92.97%] [G loss: 2.707861]\n",
      "epoch:25 step:23455 [D loss: 0.193092, acc.: 94.53%] [G loss: 0.578278]\n",
      "epoch:25 step:23456 [D loss: 0.934875, acc.: 51.56%] [G loss: 0.635381]\n",
      "epoch:25 step:23457 [D loss: 0.118818, acc.: 98.44%] [G loss: 0.224232]\n",
      "epoch:25 step:23458 [D loss: 0.088944, acc.: 100.00%] [G loss: 1.328615]\n",
      "epoch:25 step:23459 [D loss: 0.312559, acc.: 83.59%] [G loss: 2.684505]\n",
      "epoch:25 step:23460 [D loss: 0.791997, acc.: 57.03%] [G loss: 1.286205]\n",
      "epoch:25 step:23461 [D loss: 0.717437, acc.: 59.38%] [G loss: 2.411380]\n",
      "epoch:25 step:23462 [D loss: 0.561619, acc.: 71.09%] [G loss: 2.140615]\n",
      "epoch:25 step:23463 [D loss: 0.335958, acc.: 87.50%] [G loss: 1.097370]\n",
      "epoch:25 step:23464 [D loss: 0.459808, acc.: 75.00%] [G loss: 0.749318]\n",
      "epoch:25 step:23465 [D loss: 0.680129, acc.: 65.62%] [G loss: 1.456020]\n",
      "epoch:25 step:23466 [D loss: 0.760605, acc.: 58.59%] [G loss: 3.617215]\n",
      "epoch:25 step:23467 [D loss: 1.125430, acc.: 46.09%] [G loss: 1.349253]\n",
      "epoch:25 step:23468 [D loss: 0.221834, acc.: 92.97%] [G loss: 1.847056]\n",
      "epoch:25 step:23469 [D loss: 0.584988, acc.: 74.22%] [G loss: 4.043479]\n",
      "epoch:25 step:23470 [D loss: 0.326431, acc.: 83.59%] [G loss: 3.606425]\n",
      "epoch:25 step:23471 [D loss: 0.321480, acc.: 83.59%] [G loss: 0.759979]\n",
      "epoch:25 step:23472 [D loss: 0.315332, acc.: 90.62%] [G loss: 2.019504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23473 [D loss: 1.052022, acc.: 36.72%] [G loss: 1.416415]\n",
      "epoch:25 step:23474 [D loss: 0.989602, acc.: 53.91%] [G loss: 2.633612]\n",
      "epoch:25 step:23475 [D loss: 0.480867, acc.: 73.44%] [G loss: 2.461423]\n",
      "epoch:25 step:23476 [D loss: 0.132726, acc.: 97.66%] [G loss: 3.185917]\n",
      "epoch:25 step:23477 [D loss: 0.239462, acc.: 92.97%] [G loss: 1.221454]\n",
      "epoch:25 step:23478 [D loss: 0.552939, acc.: 73.44%] [G loss: 0.908108]\n",
      "epoch:25 step:23479 [D loss: 0.356951, acc.: 86.72%] [G loss: 2.070187]\n",
      "epoch:25 step:23480 [D loss: 0.221489, acc.: 94.53%] [G loss: 2.731574]\n",
      "epoch:25 step:23481 [D loss: 0.549580, acc.: 71.88%] [G loss: 2.082574]\n",
      "epoch:25 step:23482 [D loss: 0.104210, acc.: 100.00%] [G loss: 1.735857]\n",
      "epoch:25 step:23483 [D loss: 0.459154, acc.: 81.25%] [G loss: 0.774912]\n",
      "epoch:25 step:23484 [D loss: 0.212698, acc.: 96.88%] [G loss: 2.601670]\n",
      "epoch:25 step:23485 [D loss: 0.099923, acc.: 100.00%] [G loss: 4.430447]\n",
      "epoch:25 step:23486 [D loss: 0.271658, acc.: 94.53%] [G loss: 1.304342]\n",
      "epoch:25 step:23487 [D loss: 0.506247, acc.: 74.22%] [G loss: 2.338511]\n",
      "epoch:25 step:23488 [D loss: 0.279299, acc.: 92.97%] [G loss: 1.185426]\n",
      "epoch:25 step:23489 [D loss: 0.318539, acc.: 90.62%] [G loss: 1.872792]\n",
      "epoch:25 step:23490 [D loss: 0.281705, acc.: 85.94%] [G loss: 2.661059]\n",
      "epoch:25 step:23491 [D loss: 0.438524, acc.: 78.91%] [G loss: 2.814652]\n",
      "epoch:25 step:23492 [D loss: 0.292302, acc.: 89.84%] [G loss: 1.573141]\n",
      "epoch:25 step:23493 [D loss: 0.333158, acc.: 89.84%] [G loss: 2.294483]\n",
      "epoch:25 step:23494 [D loss: 0.333799, acc.: 95.31%] [G loss: 1.093159]\n",
      "epoch:25 step:23495 [D loss: 0.328574, acc.: 88.28%] [G loss: 3.076056]\n",
      "epoch:25 step:23496 [D loss: 0.731441, acc.: 52.34%] [G loss: 1.534397]\n",
      "epoch:25 step:23497 [D loss: 0.556225, acc.: 72.66%] [G loss: 1.676812]\n",
      "epoch:25 step:23498 [D loss: 0.598026, acc.: 67.19%] [G loss: 3.621065]\n",
      "epoch:25 step:23499 [D loss: 0.478327, acc.: 71.09%] [G loss: 1.976515]\n",
      "epoch:25 step:23500 [D loss: 0.081881, acc.: 99.22%] [G loss: 1.578747]\n",
      "epoch:25 step:23501 [D loss: 0.353140, acc.: 85.16%] [G loss: 1.878267]\n",
      "epoch:25 step:23502 [D loss: 0.618832, acc.: 63.28%] [G loss: 1.426362]\n",
      "epoch:25 step:23503 [D loss: 0.396047, acc.: 78.91%] [G loss: 3.142385]\n",
      "epoch:25 step:23504 [D loss: 0.334095, acc.: 85.94%] [G loss: 1.640948]\n",
      "epoch:25 step:23505 [D loss: 0.387059, acc.: 82.81%] [G loss: 1.639850]\n",
      "epoch:25 step:23506 [D loss: 0.276703, acc.: 85.16%] [G loss: 2.384789]\n",
      "epoch:25 step:23507 [D loss: 0.217929, acc.: 94.53%] [G loss: 0.741584]\n",
      "epoch:25 step:23508 [D loss: 0.121639, acc.: 99.22%] [G loss: 3.851171]\n",
      "epoch:25 step:23509 [D loss: 0.637367, acc.: 67.19%] [G loss: 2.100280]\n",
      "epoch:25 step:23510 [D loss: 0.549565, acc.: 68.75%] [G loss: 2.580105]\n",
      "epoch:25 step:23511 [D loss: 0.253848, acc.: 93.75%] [G loss: 1.103547]\n",
      "epoch:25 step:23512 [D loss: 0.345177, acc.: 88.28%] [G loss: 3.510870]\n",
      "epoch:25 step:23513 [D loss: 0.617783, acc.: 68.75%] [G loss: 1.643031]\n",
      "epoch:25 step:23514 [D loss: 0.750213, acc.: 55.47%] [G loss: 0.826168]\n",
      "epoch:25 step:23515 [D loss: 0.421252, acc.: 85.16%] [G loss: 2.101007]\n",
      "epoch:25 step:23516 [D loss: 0.141561, acc.: 95.31%] [G loss: 1.644430]\n",
      "epoch:25 step:23517 [D loss: 0.179798, acc.: 96.88%] [G loss: 4.188339]\n",
      "epoch:25 step:23518 [D loss: 0.287980, acc.: 92.19%] [G loss: 0.529796]\n",
      "epoch:25 step:23519 [D loss: 1.277887, acc.: 52.34%] [G loss: 4.088877]\n",
      "epoch:25 step:23520 [D loss: 0.158278, acc.: 96.09%] [G loss: 2.743727]\n",
      "epoch:25 step:23521 [D loss: 0.229308, acc.: 92.19%] [G loss: 3.045473]\n",
      "epoch:25 step:23522 [D loss: 0.212903, acc.: 94.53%] [G loss: 2.077533]\n",
      "epoch:25 step:23523 [D loss: 1.077198, acc.: 55.47%] [G loss: 2.852824]\n",
      "epoch:25 step:23524 [D loss: 0.352188, acc.: 88.28%] [G loss: 0.678196]\n",
      "epoch:25 step:23525 [D loss: 0.241933, acc.: 92.97%] [G loss: 1.380407]\n",
      "epoch:25 step:23526 [D loss: 0.561778, acc.: 73.44%] [G loss: 1.366677]\n",
      "epoch:25 step:23527 [D loss: 0.738874, acc.: 59.38%] [G loss: 2.379040]\n",
      "epoch:25 step:23528 [D loss: 0.504708, acc.: 75.00%] [G loss: 2.225604]\n",
      "epoch:25 step:23529 [D loss: 0.791087, acc.: 57.81%] [G loss: 1.593190]\n",
      "epoch:25 step:23530 [D loss: 0.228388, acc.: 96.09%] [G loss: 0.738171]\n",
      "epoch:25 step:23531 [D loss: 0.634356, acc.: 61.72%] [G loss: 2.941565]\n",
      "epoch:25 step:23532 [D loss: 0.335397, acc.: 87.50%] [G loss: 1.701619]\n",
      "epoch:25 step:23533 [D loss: 0.480994, acc.: 77.34%] [G loss: 3.135491]\n",
      "epoch:25 step:23534 [D loss: 0.301459, acc.: 87.50%] [G loss: 2.973142]\n",
      "epoch:25 step:23535 [D loss: 0.160579, acc.: 97.66%] [G loss: 1.702660]\n",
      "epoch:25 step:23536 [D loss: 0.436265, acc.: 79.69%] [G loss: 0.966665]\n",
      "epoch:25 step:23537 [D loss: 0.495675, acc.: 76.56%] [G loss: 0.783524]\n",
      "epoch:25 step:23538 [D loss: 0.472301, acc.: 79.69%] [G loss: 2.106067]\n",
      "epoch:25 step:23539 [D loss: 0.314095, acc.: 87.50%] [G loss: 2.532501]\n",
      "epoch:25 step:23540 [D loss: 0.063021, acc.: 99.22%] [G loss: 1.235857]\n",
      "epoch:25 step:23541 [D loss: 0.508879, acc.: 69.53%] [G loss: 1.172508]\n",
      "epoch:25 step:23542 [D loss: 0.226053, acc.: 94.53%] [G loss: 3.090137]\n",
      "epoch:25 step:23543 [D loss: 0.082779, acc.: 97.66%] [G loss: 2.419030]\n",
      "epoch:25 step:23544 [D loss: 0.295704, acc.: 87.50%] [G loss: 0.592358]\n",
      "epoch:25 step:23545 [D loss: 0.394725, acc.: 82.03%] [G loss: 1.303846]\n",
      "epoch:25 step:23546 [D loss: 0.739087, acc.: 55.47%] [G loss: 2.163204]\n",
      "epoch:25 step:23547 [D loss: 0.410033, acc.: 81.25%] [G loss: 2.780612]\n",
      "epoch:25 step:23548 [D loss: 0.299741, acc.: 90.62%] [G loss: 1.544242]\n",
      "epoch:25 step:23549 [D loss: 0.817848, acc.: 58.59%] [G loss: 1.459239]\n",
      "epoch:25 step:23550 [D loss: 0.249051, acc.: 91.41%] [G loss: 3.859591]\n",
      "epoch:25 step:23551 [D loss: 0.660527, acc.: 57.81%] [G loss: 1.713477]\n",
      "epoch:25 step:23552 [D loss: 0.687549, acc.: 60.16%] [G loss: 2.049604]\n",
      "epoch:25 step:23553 [D loss: 0.585485, acc.: 75.00%] [G loss: 1.339663]\n",
      "epoch:25 step:23554 [D loss: 0.539595, acc.: 68.75%] [G loss: 3.278792]\n",
      "epoch:25 step:23555 [D loss: 0.462746, acc.: 80.47%] [G loss: 1.460539]\n",
      "epoch:25 step:23556 [D loss: 0.474644, acc.: 75.78%] [G loss: 1.975549]\n",
      "epoch:25 step:23557 [D loss: 1.043105, acc.: 35.94%] [G loss: 1.946284]\n",
      "epoch:25 step:23558 [D loss: 0.326920, acc.: 89.06%] [G loss: 1.615797]\n",
      "epoch:25 step:23559 [D loss: 0.466323, acc.: 76.56%] [G loss: 0.872172]\n",
      "epoch:25 step:23560 [D loss: 0.374918, acc.: 81.25%] [G loss: 1.269750]\n",
      "epoch:25 step:23561 [D loss: 0.138923, acc.: 96.88%] [G loss: 3.142034]\n",
      "epoch:25 step:23562 [D loss: 0.205869, acc.: 96.88%] [G loss: 2.306882]\n",
      "epoch:25 step:23563 [D loss: 0.270384, acc.: 90.62%] [G loss: 3.046467]\n",
      "epoch:25 step:23564 [D loss: 0.138426, acc.: 98.44%] [G loss: 3.087075]\n",
      "epoch:25 step:23565 [D loss: 0.629963, acc.: 67.97%] [G loss: 2.901254]\n",
      "epoch:25 step:23566 [D loss: 0.522098, acc.: 69.53%] [G loss: 3.104540]\n",
      "epoch:25 step:23567 [D loss: 0.462517, acc.: 77.34%] [G loss: 0.843121]\n",
      "epoch:25 step:23568 [D loss: 0.870866, acc.: 50.78%] [G loss: 1.319436]\n",
      "epoch:25 step:23569 [D loss: 0.272993, acc.: 95.31%] [G loss: 4.121285]\n",
      "epoch:25 step:23570 [D loss: 0.508631, acc.: 78.91%] [G loss: 1.499605]\n",
      "epoch:25 step:23571 [D loss: 0.298880, acc.: 91.41%] [G loss: 1.865496]\n",
      "epoch:25 step:23572 [D loss: 0.656014, acc.: 58.59%] [G loss: 0.973017]\n",
      "epoch:25 step:23573 [D loss: 0.513719, acc.: 79.69%] [G loss: 0.882599]\n",
      "epoch:25 step:23574 [D loss: 0.368175, acc.: 80.47%] [G loss: 1.402904]\n",
      "epoch:25 step:23575 [D loss: 0.561401, acc.: 68.75%] [G loss: 1.125945]\n",
      "epoch:25 step:23576 [D loss: 0.275585, acc.: 93.75%] [G loss: 0.906715]\n",
      "epoch:25 step:23577 [D loss: 0.278539, acc.: 92.19%] [G loss: 2.354899]\n",
      "epoch:25 step:23578 [D loss: 0.138789, acc.: 98.44%] [G loss: 1.477777]\n",
      "epoch:25 step:23579 [D loss: 0.569592, acc.: 67.19%] [G loss: 1.023124]\n",
      "epoch:25 step:23580 [D loss: 0.302816, acc.: 92.19%] [G loss: 1.355437]\n",
      "epoch:25 step:23581 [D loss: 0.387092, acc.: 83.59%] [G loss: 2.669871]\n",
      "epoch:25 step:23582 [D loss: 0.167497, acc.: 96.88%] [G loss: 1.246591]\n",
      "epoch:25 step:23583 [D loss: 0.280779, acc.: 92.19%] [G loss: 0.472558]\n",
      "epoch:25 step:23584 [D loss: 0.105834, acc.: 99.22%] [G loss: 1.735420]\n",
      "epoch:25 step:23585 [D loss: 0.137187, acc.: 98.44%] [G loss: 2.068347]\n",
      "epoch:25 step:23586 [D loss: 1.115765, acc.: 35.94%] [G loss: 2.655046]\n",
      "epoch:25 step:23587 [D loss: 0.933477, acc.: 54.69%] [G loss: 2.096915]\n",
      "epoch:25 step:23588 [D loss: 0.571649, acc.: 70.31%] [G loss: 3.572231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23589 [D loss: 0.228183, acc.: 90.62%] [G loss: 3.166805]\n",
      "epoch:25 step:23590 [D loss: 1.141511, acc.: 44.53%] [G loss: 2.676540]\n",
      "epoch:25 step:23591 [D loss: 0.438720, acc.: 81.25%] [G loss: 0.944953]\n",
      "epoch:25 step:23592 [D loss: 0.181427, acc.: 96.09%] [G loss: 3.030142]\n",
      "epoch:25 step:23593 [D loss: 0.407151, acc.: 82.81%] [G loss: 2.108240]\n",
      "epoch:25 step:23594 [D loss: 0.320455, acc.: 92.19%] [G loss: 0.875277]\n",
      "epoch:25 step:23595 [D loss: 0.198187, acc.: 95.31%] [G loss: 1.902533]\n",
      "epoch:25 step:23596 [D loss: 0.713017, acc.: 59.38%] [G loss: 2.374945]\n",
      "epoch:25 step:23597 [D loss: 0.356225, acc.: 88.28%] [G loss: 1.305482]\n",
      "epoch:25 step:23598 [D loss: 0.759124, acc.: 57.03%] [G loss: 1.156010]\n",
      "epoch:25 step:23599 [D loss: 0.162623, acc.: 96.88%] [G loss: 0.771968]\n",
      "epoch:25 step:23600 [D loss: 0.458804, acc.: 79.69%] [G loss: 0.948925]\n",
      "epoch:25 step:23601 [D loss: 0.193147, acc.: 96.09%] [G loss: 1.745693]\n",
      "epoch:25 step:23602 [D loss: 0.172874, acc.: 96.09%] [G loss: 1.636228]\n",
      "epoch:25 step:23603 [D loss: 0.361193, acc.: 87.50%] [G loss: 1.188900]\n",
      "epoch:25 step:23604 [D loss: 0.230431, acc.: 96.09%] [G loss: 2.935573]\n",
      "epoch:25 step:23605 [D loss: 0.724041, acc.: 60.16%] [G loss: 1.322625]\n",
      "epoch:25 step:23606 [D loss: 0.212685, acc.: 96.09%] [G loss: 1.763608]\n",
      "epoch:25 step:23607 [D loss: 0.631984, acc.: 60.16%] [G loss: 3.904548]\n",
      "epoch:25 step:23608 [D loss: 0.407018, acc.: 78.12%] [G loss: 0.932673]\n",
      "epoch:25 step:23609 [D loss: 0.478222, acc.: 72.66%] [G loss: 0.544124]\n",
      "epoch:25 step:23610 [D loss: 0.347367, acc.: 82.81%] [G loss: 3.691191]\n",
      "epoch:25 step:23611 [D loss: 0.190318, acc.: 97.66%] [G loss: 4.012061]\n",
      "epoch:25 step:23612 [D loss: 0.374019, acc.: 85.94%] [G loss: 2.143856]\n",
      "epoch:25 step:23613 [D loss: 0.409948, acc.: 85.94%] [G loss: 2.251354]\n",
      "epoch:25 step:23614 [D loss: 0.131014, acc.: 98.44%] [G loss: 2.747932]\n",
      "epoch:25 step:23615 [D loss: 0.686055, acc.: 60.16%] [G loss: 3.727186]\n",
      "epoch:25 step:23616 [D loss: 0.296552, acc.: 92.19%] [G loss: 3.083869]\n",
      "epoch:25 step:23617 [D loss: 0.360186, acc.: 83.59%] [G loss: 1.503389]\n",
      "epoch:25 step:23618 [D loss: 0.671231, acc.: 60.16%] [G loss: 1.426281]\n",
      "epoch:25 step:23619 [D loss: 0.435446, acc.: 82.03%] [G loss: 2.168414]\n",
      "epoch:25 step:23620 [D loss: 0.545283, acc.: 68.75%] [G loss: 2.260607]\n",
      "epoch:25 step:23621 [D loss: 0.335897, acc.: 85.16%] [G loss: 1.537460]\n",
      "epoch:25 step:23622 [D loss: 0.471416, acc.: 72.66%] [G loss: 1.265701]\n",
      "epoch:25 step:23623 [D loss: 0.613372, acc.: 62.50%] [G loss: 4.070312]\n",
      "epoch:25 step:23624 [D loss: 0.214284, acc.: 93.75%] [G loss: 3.983999]\n",
      "epoch:25 step:23625 [D loss: 0.349223, acc.: 84.38%] [G loss: 3.890661]\n",
      "epoch:25 step:23626 [D loss: 0.109303, acc.: 99.22%] [G loss: 4.954862]\n",
      "epoch:25 step:23627 [D loss: 0.147924, acc.: 96.09%] [G loss: 3.895691]\n",
      "epoch:25 step:23628 [D loss: 0.192747, acc.: 96.88%] [G loss: 1.364431]\n",
      "epoch:25 step:23629 [D loss: 0.442164, acc.: 78.91%] [G loss: 2.889969]\n",
      "epoch:25 step:23630 [D loss: 0.821245, acc.: 47.66%] [G loss: 0.930926]\n",
      "epoch:25 step:23631 [D loss: 0.161092, acc.: 96.09%] [G loss: 3.215382]\n",
      "epoch:25 step:23632 [D loss: 0.142640, acc.: 96.88%] [G loss: 2.697192]\n",
      "epoch:25 step:23633 [D loss: 0.346400, acc.: 87.50%] [G loss: 0.712020]\n",
      "epoch:25 step:23634 [D loss: 0.491205, acc.: 69.53%] [G loss: 1.857515]\n",
      "epoch:25 step:23635 [D loss: 0.594205, acc.: 65.62%] [G loss: 0.763235]\n",
      "epoch:25 step:23636 [D loss: 0.190666, acc.: 93.75%] [G loss: 1.087819]\n",
      "epoch:25 step:23637 [D loss: 0.913139, acc.: 50.78%] [G loss: 0.824168]\n",
      "epoch:25 step:23638 [D loss: 0.721565, acc.: 63.28%] [G loss: 0.571166]\n",
      "epoch:25 step:23639 [D loss: 0.238832, acc.: 94.53%] [G loss: 0.756790]\n",
      "epoch:25 step:23640 [D loss: 0.690091, acc.: 62.50%] [G loss: 2.496934]\n",
      "epoch:25 step:23641 [D loss: 0.291184, acc.: 90.62%] [G loss: 2.919806]\n",
      "epoch:25 step:23642 [D loss: 0.112181, acc.: 99.22%] [G loss: 2.937704]\n",
      "epoch:25 step:23643 [D loss: 0.305877, acc.: 89.84%] [G loss: 1.689965]\n",
      "epoch:25 step:23644 [D loss: 0.388383, acc.: 85.16%] [G loss: 5.061679]\n",
      "epoch:25 step:23645 [D loss: 0.602335, acc.: 71.88%] [G loss: 2.101772]\n",
      "epoch:25 step:23646 [D loss: 0.271862, acc.: 94.53%] [G loss: 3.829160]\n",
      "epoch:25 step:23647 [D loss: 0.381535, acc.: 87.50%] [G loss: 2.935729]\n",
      "epoch:25 step:23648 [D loss: 0.478459, acc.: 76.56%] [G loss: 3.271295]\n",
      "epoch:25 step:23649 [D loss: 0.187844, acc.: 94.53%] [G loss: 2.263532]\n",
      "epoch:25 step:23650 [D loss: 0.306284, acc.: 89.06%] [G loss: 1.335908]\n",
      "epoch:25 step:23651 [D loss: 0.386433, acc.: 78.12%] [G loss: 2.180875]\n",
      "epoch:25 step:23652 [D loss: 0.332839, acc.: 91.41%] [G loss: 3.171276]\n",
      "epoch:25 step:23653 [D loss: 0.230170, acc.: 95.31%] [G loss: 2.363181]\n",
      "epoch:25 step:23654 [D loss: 0.441743, acc.: 79.69%] [G loss: 1.690839]\n",
      "epoch:25 step:23655 [D loss: 0.370564, acc.: 85.94%] [G loss: 0.701981]\n",
      "epoch:25 step:23656 [D loss: 0.485526, acc.: 70.31%] [G loss: 3.203779]\n",
      "epoch:25 step:23657 [D loss: 0.719736, acc.: 64.84%] [G loss: 1.663556]\n",
      "epoch:25 step:23658 [D loss: 0.405266, acc.: 83.59%] [G loss: 1.449875]\n",
      "epoch:25 step:23659 [D loss: 0.312984, acc.: 85.16%] [G loss: 2.328443]\n",
      "epoch:25 step:23660 [D loss: 0.642656, acc.: 69.53%] [G loss: 2.206559]\n",
      "epoch:25 step:23661 [D loss: 0.319948, acc.: 89.84%] [G loss: 2.146271]\n",
      "epoch:25 step:23662 [D loss: 0.438987, acc.: 79.69%] [G loss: 4.781019]\n",
      "epoch:25 step:23663 [D loss: 1.063729, acc.: 39.06%] [G loss: 3.033531]\n",
      "epoch:25 step:23664 [D loss: 0.551901, acc.: 75.00%] [G loss: 4.190226]\n",
      "epoch:25 step:23665 [D loss: 0.876486, acc.: 58.59%] [G loss: 2.917085]\n",
      "epoch:25 step:23666 [D loss: 1.092989, acc.: 39.06%] [G loss: 1.482409]\n",
      "epoch:25 step:23667 [D loss: 0.205053, acc.: 96.88%] [G loss: 2.813976]\n",
      "epoch:25 step:23668 [D loss: 0.179106, acc.: 96.09%] [G loss: 3.119081]\n",
      "epoch:25 step:23669 [D loss: 0.119553, acc.: 96.88%] [G loss: 2.202291]\n",
      "epoch:25 step:23670 [D loss: 0.654361, acc.: 61.72%] [G loss: 1.239768]\n",
      "epoch:25 step:23671 [D loss: 1.276127, acc.: 25.78%] [G loss: 2.103342]\n",
      "epoch:25 step:23672 [D loss: 0.876163, acc.: 62.50%] [G loss: 1.521412]\n",
      "epoch:25 step:23673 [D loss: 0.113011, acc.: 99.22%] [G loss: 3.920823]\n",
      "epoch:25 step:23674 [D loss: 0.540116, acc.: 67.19%] [G loss: 2.604732]\n",
      "epoch:25 step:23675 [D loss: 0.322584, acc.: 89.06%] [G loss: 2.581917]\n",
      "epoch:25 step:23676 [D loss: 0.597447, acc.: 71.88%] [G loss: 2.216022]\n",
      "epoch:25 step:23677 [D loss: 0.218200, acc.: 92.97%] [G loss: 1.046863]\n",
      "epoch:25 step:23678 [D loss: 0.417295, acc.: 78.91%] [G loss: 1.480453]\n",
      "epoch:25 step:23679 [D loss: 0.149214, acc.: 98.44%] [G loss: 1.723218]\n",
      "epoch:25 step:23680 [D loss: 0.479168, acc.: 78.91%] [G loss: 1.716773]\n",
      "epoch:25 step:23681 [D loss: 0.714410, acc.: 64.06%] [G loss: 1.377836]\n",
      "epoch:25 step:23682 [D loss: 0.474768, acc.: 79.69%] [G loss: 2.952854]\n",
      "epoch:25 step:23683 [D loss: 1.194074, acc.: 27.34%] [G loss: 1.170966]\n",
      "epoch:25 step:23684 [D loss: 0.500659, acc.: 71.88%] [G loss: 1.638593]\n",
      "epoch:25 step:23685 [D loss: 0.269701, acc.: 92.97%] [G loss: 1.022005]\n",
      "epoch:25 step:23686 [D loss: 0.160764, acc.: 98.44%] [G loss: 1.505769]\n",
      "epoch:25 step:23687 [D loss: 0.537451, acc.: 69.53%] [G loss: 2.901483]\n",
      "epoch:25 step:23688 [D loss: 0.556929, acc.: 64.84%] [G loss: 2.409900]\n",
      "epoch:25 step:23689 [D loss: 0.692791, acc.: 62.50%] [G loss: 1.353358]\n",
      "epoch:25 step:23690 [D loss: 0.836930, acc.: 50.00%] [G loss: 2.900399]\n",
      "epoch:25 step:23691 [D loss: 0.312720, acc.: 85.16%] [G loss: 0.831873]\n",
      "epoch:25 step:23692 [D loss: 0.374904, acc.: 84.38%] [G loss: 3.248263]\n",
      "epoch:25 step:23693 [D loss: 0.967787, acc.: 42.19%] [G loss: 1.388979]\n",
      "epoch:25 step:23694 [D loss: 0.378522, acc.: 82.03%] [G loss: 0.868754]\n",
      "epoch:25 step:23695 [D loss: 0.351696, acc.: 89.84%] [G loss: 2.371690]\n",
      "epoch:25 step:23696 [D loss: 0.410944, acc.: 82.81%] [G loss: 1.465840]\n",
      "epoch:25 step:23697 [D loss: 0.427831, acc.: 82.03%] [G loss: 2.576200]\n",
      "epoch:25 step:23698 [D loss: 0.254235, acc.: 96.09%] [G loss: 2.330452]\n",
      "epoch:25 step:23699 [D loss: 0.221053, acc.: 95.31%] [G loss: 2.663949]\n",
      "epoch:25 step:23700 [D loss: 0.345136, acc.: 85.94%] [G loss: 4.476467]\n",
      "epoch:25 step:23701 [D loss: 0.946174, acc.: 39.06%] [G loss: 1.911793]\n",
      "epoch:25 step:23702 [D loss: 0.284172, acc.: 88.28%] [G loss: 1.512012]\n",
      "epoch:25 step:23703 [D loss: 0.352403, acc.: 85.16%] [G loss: 0.876557]\n",
      "epoch:25 step:23704 [D loss: 0.473628, acc.: 76.56%] [G loss: 0.674720]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23705 [D loss: 0.233775, acc.: 94.53%] [G loss: 1.679756]\n",
      "epoch:25 step:23706 [D loss: 0.468720, acc.: 83.59%] [G loss: 1.739077]\n",
      "epoch:25 step:23707 [D loss: 0.316782, acc.: 89.06%] [G loss: 2.030193]\n",
      "epoch:25 step:23708 [D loss: 0.362098, acc.: 83.59%] [G loss: 2.197570]\n",
      "epoch:25 step:23709 [D loss: 0.575620, acc.: 65.62%] [G loss: 2.476774]\n",
      "epoch:25 step:23710 [D loss: 0.202983, acc.: 94.53%] [G loss: 1.864896]\n",
      "epoch:25 step:23711 [D loss: 0.212425, acc.: 96.09%] [G loss: 2.399830]\n",
      "epoch:25 step:23712 [D loss: 0.182009, acc.: 98.44%] [G loss: 1.385172]\n",
      "epoch:25 step:23713 [D loss: 1.076929, acc.: 39.84%] [G loss: 1.832709]\n",
      "epoch:25 step:23714 [D loss: 0.389012, acc.: 81.25%] [G loss: 1.040325]\n",
      "epoch:25 step:23715 [D loss: 0.122923, acc.: 97.66%] [G loss: 2.170099]\n",
      "epoch:25 step:23716 [D loss: 0.440101, acc.: 80.47%] [G loss: 3.761922]\n",
      "epoch:25 step:23717 [D loss: 0.591185, acc.: 66.41%] [G loss: 1.663099]\n",
      "epoch:25 step:23718 [D loss: 0.420778, acc.: 79.69%] [G loss: 1.707039]\n",
      "epoch:25 step:23719 [D loss: 0.524021, acc.: 70.31%] [G loss: 1.321792]\n",
      "epoch:25 step:23720 [D loss: 0.660648, acc.: 60.16%] [G loss: 1.792921]\n",
      "epoch:25 step:23721 [D loss: 0.169149, acc.: 98.44%] [G loss: 1.683370]\n",
      "epoch:25 step:23722 [D loss: 0.525187, acc.: 75.78%] [G loss: 2.412948]\n",
      "epoch:25 step:23723 [D loss: 0.607902, acc.: 68.75%] [G loss: 0.944542]\n",
      "epoch:25 step:23724 [D loss: 0.500505, acc.: 78.91%] [G loss: 0.729394]\n",
      "epoch:25 step:23725 [D loss: 0.659236, acc.: 67.19%] [G loss: 1.247810]\n",
      "epoch:25 step:23726 [D loss: 0.369286, acc.: 78.12%] [G loss: 1.732850]\n",
      "epoch:25 step:23727 [D loss: 1.739848, acc.: 36.72%] [G loss: 0.919153]\n",
      "epoch:25 step:23728 [D loss: 0.579576, acc.: 67.97%] [G loss: 1.796837]\n",
      "epoch:25 step:23729 [D loss: 0.668579, acc.: 61.72%] [G loss: 1.873002]\n",
      "epoch:25 step:23730 [D loss: 0.459272, acc.: 77.34%] [G loss: 3.695825]\n",
      "epoch:25 step:23731 [D loss: 0.924510, acc.: 58.59%] [G loss: 1.180529]\n",
      "epoch:25 step:23732 [D loss: 0.540461, acc.: 81.25%] [G loss: 1.522181]\n",
      "epoch:25 step:23733 [D loss: 0.464457, acc.: 70.31%] [G loss: 1.745909]\n",
      "epoch:25 step:23734 [D loss: 0.387045, acc.: 85.94%] [G loss: 1.061953]\n",
      "epoch:25 step:23735 [D loss: 0.228741, acc.: 95.31%] [G loss: 2.066044]\n",
      "epoch:25 step:23736 [D loss: 0.300542, acc.: 89.84%] [G loss: 2.885058]\n",
      "epoch:25 step:23737 [D loss: 0.503377, acc.: 78.91%] [G loss: 1.595356]\n",
      "epoch:25 step:23738 [D loss: 0.831122, acc.: 46.09%] [G loss: 0.997907]\n",
      "epoch:25 step:23739 [D loss: 0.452490, acc.: 78.12%] [G loss: 0.876986]\n",
      "epoch:25 step:23740 [D loss: 0.353132, acc.: 85.94%] [G loss: 1.120267]\n",
      "epoch:25 step:23741 [D loss: 0.132871, acc.: 98.44%] [G loss: 1.367560]\n",
      "epoch:25 step:23742 [D loss: 0.347295, acc.: 85.94%] [G loss: 1.800443]\n",
      "epoch:25 step:23743 [D loss: 0.565156, acc.: 70.31%] [G loss: 0.566094]\n",
      "epoch:25 step:23744 [D loss: 0.398230, acc.: 87.50%] [G loss: 3.382542]\n",
      "epoch:25 step:23745 [D loss: 0.429552, acc.: 70.31%] [G loss: 3.306733]\n",
      "epoch:25 step:23746 [D loss: 0.511469, acc.: 76.56%] [G loss: 1.707800]\n",
      "epoch:25 step:23747 [D loss: 0.154221, acc.: 97.66%] [G loss: 1.674886]\n",
      "epoch:25 step:23748 [D loss: 0.105786, acc.: 98.44%] [G loss: 0.773478]\n",
      "epoch:25 step:23749 [D loss: 0.219655, acc.: 94.53%] [G loss: 3.089561]\n",
      "epoch:25 step:23750 [D loss: 0.114044, acc.: 99.22%] [G loss: 1.590850]\n",
      "epoch:25 step:23751 [D loss: 0.475055, acc.: 75.78%] [G loss: 2.698839]\n",
      "epoch:25 step:23752 [D loss: 0.555589, acc.: 62.50%] [G loss: 1.758328]\n",
      "epoch:25 step:23753 [D loss: 0.191646, acc.: 96.88%] [G loss: 3.097465]\n",
      "epoch:25 step:23754 [D loss: 0.448125, acc.: 76.56%] [G loss: 2.597324]\n",
      "epoch:25 step:23755 [D loss: 0.320849, acc.: 89.84%] [G loss: 1.310881]\n",
      "epoch:25 step:23756 [D loss: 0.344089, acc.: 90.62%] [G loss: 2.937348]\n",
      "epoch:25 step:23757 [D loss: 0.220751, acc.: 97.66%] [G loss: 2.082770]\n",
      "epoch:25 step:23758 [D loss: 0.476625, acc.: 72.66%] [G loss: 2.514027]\n",
      "epoch:25 step:23759 [D loss: 0.115114, acc.: 100.00%] [G loss: 0.787669]\n",
      "epoch:25 step:23760 [D loss: 0.792857, acc.: 53.12%] [G loss: 1.593190]\n",
      "epoch:25 step:23761 [D loss: 0.301730, acc.: 87.50%] [G loss: 1.391641]\n",
      "epoch:25 step:23762 [D loss: 0.317369, acc.: 84.38%] [G loss: 1.366643]\n",
      "epoch:25 step:23763 [D loss: 0.438452, acc.: 78.91%] [G loss: 1.542326]\n",
      "epoch:25 step:23764 [D loss: 0.823158, acc.: 55.47%] [G loss: 2.177269]\n",
      "epoch:25 step:23765 [D loss: 0.981043, acc.: 50.78%] [G loss: 2.159214]\n",
      "epoch:25 step:23766 [D loss: 0.289320, acc.: 89.84%] [G loss: 2.269547]\n",
      "epoch:25 step:23767 [D loss: 0.774028, acc.: 58.59%] [G loss: 2.145635]\n",
      "epoch:25 step:23768 [D loss: 0.374118, acc.: 87.50%] [G loss: 0.521498]\n",
      "epoch:25 step:23769 [D loss: 0.802520, acc.: 57.81%] [G loss: 1.998509]\n",
      "epoch:25 step:23770 [D loss: 0.581400, acc.: 62.50%] [G loss: 0.893704]\n",
      "epoch:25 step:23771 [D loss: 0.259777, acc.: 95.31%] [G loss: 2.426296]\n",
      "epoch:25 step:23772 [D loss: 0.563664, acc.: 72.66%] [G loss: 3.583049]\n",
      "epoch:25 step:23773 [D loss: 0.147486, acc.: 97.66%] [G loss: 3.488256]\n",
      "epoch:25 step:23774 [D loss: 0.612087, acc.: 67.19%] [G loss: 2.616763]\n",
      "epoch:25 step:23775 [D loss: 0.268164, acc.: 90.62%] [G loss: 2.182249]\n",
      "epoch:25 step:23776 [D loss: 0.387236, acc.: 77.34%] [G loss: 1.196397]\n",
      "epoch:25 step:23777 [D loss: 0.925402, acc.: 57.03%] [G loss: 1.519926]\n",
      "epoch:25 step:23778 [D loss: 0.157734, acc.: 96.88%] [G loss: 0.932850]\n",
      "epoch:25 step:23779 [D loss: 0.350090, acc.: 85.16%] [G loss: 1.422922]\n",
      "epoch:25 step:23780 [D loss: 0.234248, acc.: 92.97%] [G loss: 1.416423]\n",
      "epoch:25 step:23781 [D loss: 0.348446, acc.: 89.06%] [G loss: 2.049868]\n",
      "epoch:25 step:23782 [D loss: 0.547590, acc.: 76.56%] [G loss: 1.961397]\n",
      "epoch:25 step:23783 [D loss: 0.581933, acc.: 68.75%] [G loss: 3.735817]\n",
      "epoch:25 step:23784 [D loss: 0.401447, acc.: 81.25%] [G loss: 1.287308]\n",
      "epoch:25 step:23785 [D loss: 0.430600, acc.: 80.47%] [G loss: 3.090825]\n",
      "epoch:25 step:23786 [D loss: 0.504786, acc.: 75.78%] [G loss: 2.540036]\n",
      "epoch:25 step:23787 [D loss: 0.299919, acc.: 88.28%] [G loss: 2.115295]\n",
      "epoch:25 step:23788 [D loss: 0.380774, acc.: 82.81%] [G loss: 3.687843]\n",
      "epoch:25 step:23789 [D loss: 1.456470, acc.: 25.78%] [G loss: 1.366586]\n",
      "epoch:25 step:23790 [D loss: 0.845985, acc.: 55.47%] [G loss: 2.290556]\n",
      "epoch:25 step:23791 [D loss: 0.408306, acc.: 83.59%] [G loss: 0.563669]\n",
      "epoch:25 step:23792 [D loss: 0.597021, acc.: 74.22%] [G loss: 4.429911]\n",
      "epoch:25 step:23793 [D loss: 0.211006, acc.: 95.31%] [G loss: 1.565899]\n",
      "epoch:25 step:23794 [D loss: 0.205866, acc.: 97.66%] [G loss: 1.896997]\n",
      "epoch:25 step:23795 [D loss: 0.524378, acc.: 75.00%] [G loss: 0.637471]\n",
      "epoch:25 step:23796 [D loss: 0.598669, acc.: 65.62%] [G loss: 1.029632]\n",
      "epoch:25 step:23797 [D loss: 0.261474, acc.: 92.97%] [G loss: 3.084494]\n",
      "epoch:25 step:23798 [D loss: 0.279774, acc.: 89.06%] [G loss: 4.156433]\n",
      "epoch:25 step:23799 [D loss: 0.417198, acc.: 83.59%] [G loss: 2.949257]\n",
      "epoch:25 step:23800 [D loss: 0.724949, acc.: 56.25%] [G loss: 1.796650]\n",
      "epoch:25 step:23801 [D loss: 0.488077, acc.: 75.00%] [G loss: 2.390421]\n",
      "epoch:25 step:23802 [D loss: 0.627849, acc.: 64.84%] [G loss: 2.036571]\n",
      "epoch:25 step:23803 [D loss: 0.659072, acc.: 62.50%] [G loss: 1.273782]\n",
      "epoch:25 step:23804 [D loss: 0.424028, acc.: 82.03%] [G loss: 1.059401]\n",
      "epoch:25 step:23805 [D loss: 0.945152, acc.: 43.75%] [G loss: 1.346609]\n",
      "epoch:25 step:23806 [D loss: 0.671034, acc.: 61.72%] [G loss: 3.160429]\n",
      "epoch:25 step:23807 [D loss: 0.246144, acc.: 92.97%] [G loss: 0.964854]\n",
      "epoch:25 step:23808 [D loss: 0.215935, acc.: 93.75%] [G loss: 1.142668]\n",
      "epoch:25 step:23809 [D loss: 0.646316, acc.: 59.38%] [G loss: 2.434880]\n",
      "epoch:25 step:23810 [D loss: 0.487940, acc.: 79.69%] [G loss: 2.509863]\n",
      "epoch:25 step:23811 [D loss: 0.545777, acc.: 69.53%] [G loss: 0.697742]\n",
      "epoch:25 step:23812 [D loss: 0.247641, acc.: 97.66%] [G loss: 1.482012]\n",
      "epoch:25 step:23813 [D loss: 0.502326, acc.: 77.34%] [G loss: 2.681798]\n",
      "epoch:25 step:23814 [D loss: 0.523060, acc.: 77.34%] [G loss: 1.895215]\n",
      "epoch:25 step:23815 [D loss: 0.237264, acc.: 95.31%] [G loss: 2.145241]\n",
      "epoch:25 step:23816 [D loss: 0.481158, acc.: 82.03%] [G loss: 2.081409]\n",
      "epoch:25 step:23817 [D loss: 0.434459, acc.: 81.25%] [G loss: 0.764537]\n",
      "epoch:25 step:23818 [D loss: 0.444593, acc.: 83.59%] [G loss: 1.170657]\n",
      "epoch:25 step:23819 [D loss: 0.308433, acc.: 85.16%] [G loss: 2.193879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23820 [D loss: 0.238946, acc.: 96.09%] [G loss: 1.116133]\n",
      "epoch:25 step:23821 [D loss: 0.413434, acc.: 75.00%] [G loss: 0.757218]\n",
      "epoch:25 step:23822 [D loss: 0.629395, acc.: 65.62%] [G loss: 2.434762]\n",
      "epoch:25 step:23823 [D loss: 0.661669, acc.: 64.06%] [G loss: 1.759531]\n",
      "epoch:25 step:23824 [D loss: 0.694748, acc.: 64.06%] [G loss: 4.975546]\n",
      "epoch:25 step:23825 [D loss: 0.249329, acc.: 92.97%] [G loss: 1.175827]\n",
      "epoch:25 step:23826 [D loss: 0.646095, acc.: 66.41%] [G loss: 2.113127]\n",
      "epoch:25 step:23827 [D loss: 0.378821, acc.: 85.94%] [G loss: 4.233401]\n",
      "epoch:25 step:23828 [D loss: 0.495569, acc.: 80.47%] [G loss: 1.132960]\n",
      "epoch:25 step:23829 [D loss: 0.355240, acc.: 86.72%] [G loss: 3.133407]\n",
      "epoch:25 step:23830 [D loss: 0.575255, acc.: 72.66%] [G loss: 1.746646]\n",
      "epoch:25 step:23831 [D loss: 0.446405, acc.: 77.34%] [G loss: 4.566667]\n",
      "epoch:25 step:23832 [D loss: 0.692237, acc.: 58.59%] [G loss: 1.743606]\n",
      "epoch:25 step:23833 [D loss: 0.455317, acc.: 75.78%] [G loss: 0.837755]\n",
      "epoch:25 step:23834 [D loss: 0.398390, acc.: 85.94%] [G loss: 2.280310]\n",
      "epoch:25 step:23835 [D loss: 0.616595, acc.: 66.41%] [G loss: 3.074159]\n",
      "epoch:25 step:23836 [D loss: 0.374655, acc.: 86.72%] [G loss: 1.193160]\n",
      "epoch:25 step:23837 [D loss: 0.889125, acc.: 40.62%] [G loss: 1.440602]\n",
      "epoch:25 step:23838 [D loss: 0.336099, acc.: 85.94%] [G loss: 1.519665]\n",
      "epoch:25 step:23839 [D loss: 0.234168, acc.: 93.75%] [G loss: 2.498097]\n",
      "epoch:25 step:23840 [D loss: 0.507905, acc.: 73.44%] [G loss: 2.127648]\n",
      "epoch:25 step:23841 [D loss: 0.390886, acc.: 84.38%] [G loss: 1.954950]\n",
      "epoch:25 step:23842 [D loss: 0.444425, acc.: 79.69%] [G loss: 1.908610]\n",
      "epoch:25 step:23843 [D loss: 0.487390, acc.: 75.00%] [G loss: 2.743788]\n",
      "epoch:25 step:23844 [D loss: 0.736263, acc.: 53.91%] [G loss: 2.467983]\n",
      "epoch:25 step:23845 [D loss: 0.347807, acc.: 82.03%] [G loss: 1.942812]\n",
      "epoch:25 step:23846 [D loss: 0.778028, acc.: 58.59%] [G loss: 2.383224]\n",
      "epoch:25 step:23847 [D loss: 0.765117, acc.: 48.44%] [G loss: 2.807127]\n",
      "epoch:25 step:23848 [D loss: 0.705358, acc.: 64.84%] [G loss: 1.862951]\n",
      "epoch:25 step:23849 [D loss: 0.431761, acc.: 85.16%] [G loss: 1.819241]\n",
      "epoch:25 step:23850 [D loss: 0.571174, acc.: 61.72%] [G loss: 0.893856]\n",
      "epoch:25 step:23851 [D loss: 0.773398, acc.: 53.91%] [G loss: 1.742595]\n",
      "epoch:25 step:23852 [D loss: 0.401556, acc.: 78.12%] [G loss: 1.455964]\n",
      "epoch:25 step:23853 [D loss: 0.447335, acc.: 82.03%] [G loss: 0.869018]\n",
      "epoch:25 step:23854 [D loss: 0.506591, acc.: 74.22%] [G loss: 1.981584]\n",
      "epoch:25 step:23855 [D loss: 0.809558, acc.: 60.94%] [G loss: 1.656478]\n",
      "epoch:25 step:23856 [D loss: 0.299558, acc.: 90.62%] [G loss: 1.157798]\n",
      "epoch:25 step:23857 [D loss: 0.475215, acc.: 78.12%] [G loss: 1.558730]\n",
      "epoch:25 step:23858 [D loss: 0.581887, acc.: 73.44%] [G loss: 1.349047]\n",
      "epoch:25 step:23859 [D loss: 0.294700, acc.: 93.75%] [G loss: 1.138805]\n",
      "epoch:25 step:23860 [D loss: 0.294351, acc.: 94.53%] [G loss: 0.956277]\n",
      "epoch:25 step:23861 [D loss: 0.738752, acc.: 60.94%] [G loss: 0.272217]\n",
      "epoch:25 step:23862 [D loss: 0.242778, acc.: 92.97%] [G loss: 0.978180]\n",
      "epoch:25 step:23863 [D loss: 0.235983, acc.: 93.75%] [G loss: 1.410662]\n",
      "epoch:25 step:23864 [D loss: 0.537778, acc.: 76.56%] [G loss: 0.524059]\n",
      "epoch:25 step:23865 [D loss: 0.316996, acc.: 86.72%] [G loss: 2.794349]\n",
      "epoch:25 step:23866 [D loss: 0.511665, acc.: 72.66%] [G loss: 2.594395]\n",
      "epoch:25 step:23867 [D loss: 0.443054, acc.: 81.25%] [G loss: 3.533606]\n",
      "epoch:25 step:23868 [D loss: 1.050236, acc.: 39.84%] [G loss: 3.350312]\n",
      "epoch:25 step:23869 [D loss: 0.268704, acc.: 91.41%] [G loss: 2.690701]\n",
      "epoch:25 step:23870 [D loss: 0.651709, acc.: 64.84%] [G loss: 1.557876]\n",
      "epoch:25 step:23871 [D loss: 0.864218, acc.: 57.03%] [G loss: 1.431322]\n",
      "epoch:25 step:23872 [D loss: 0.551038, acc.: 71.88%] [G loss: 3.371886]\n",
      "epoch:25 step:23873 [D loss: 1.214821, acc.: 25.78%] [G loss: 0.950923]\n",
      "epoch:25 step:23874 [D loss: 0.147605, acc.: 98.44%] [G loss: 1.276361]\n",
      "epoch:25 step:23875 [D loss: 0.154176, acc.: 99.22%] [G loss: 1.895748]\n",
      "epoch:25 step:23876 [D loss: 0.486269, acc.: 74.22%] [G loss: 1.756423]\n",
      "epoch:25 step:23877 [D loss: 0.209932, acc.: 97.66%] [G loss: 0.582727]\n",
      "epoch:25 step:23878 [D loss: 0.349013, acc.: 86.72%] [G loss: 1.916477]\n",
      "epoch:25 step:23879 [D loss: 0.548416, acc.: 71.88%] [G loss: 1.177938]\n",
      "epoch:25 step:23880 [D loss: 0.627398, acc.: 61.72%] [G loss: 1.776870]\n",
      "epoch:25 step:23881 [D loss: 0.130892, acc.: 98.44%] [G loss: 2.381443]\n",
      "epoch:25 step:23882 [D loss: 0.333982, acc.: 89.06%] [G loss: 2.459723]\n",
      "epoch:25 step:23883 [D loss: 0.616650, acc.: 71.09%] [G loss: 2.127465]\n",
      "epoch:25 step:23884 [D loss: 0.482643, acc.: 81.25%] [G loss: 1.850881]\n",
      "epoch:25 step:23885 [D loss: 0.331535, acc.: 88.28%] [G loss: 2.561619]\n",
      "epoch:25 step:23886 [D loss: 0.539079, acc.: 69.53%] [G loss: 1.203994]\n",
      "epoch:25 step:23887 [D loss: 0.296031, acc.: 89.84%] [G loss: 2.146279]\n",
      "epoch:25 step:23888 [D loss: 0.380590, acc.: 87.50%] [G loss: 1.365802]\n",
      "epoch:25 step:23889 [D loss: 0.332847, acc.: 82.81%] [G loss: 2.956817]\n",
      "epoch:25 step:23890 [D loss: 0.612785, acc.: 65.62%] [G loss: 1.616272]\n",
      "epoch:25 step:23891 [D loss: 0.169030, acc.: 100.00%] [G loss: 2.654667]\n",
      "epoch:25 step:23892 [D loss: 0.271612, acc.: 93.75%] [G loss: 1.951005]\n",
      "epoch:25 step:23893 [D loss: 0.478828, acc.: 75.00%] [G loss: 2.101097]\n",
      "epoch:25 step:23894 [D loss: 0.374921, acc.: 83.59%] [G loss: 1.618170]\n",
      "epoch:25 step:23895 [D loss: 0.171388, acc.: 96.09%] [G loss: 3.868879]\n",
      "epoch:25 step:23896 [D loss: 0.390892, acc.: 78.12%] [G loss: 1.937717]\n",
      "epoch:25 step:23897 [D loss: 0.238999, acc.: 94.53%] [G loss: 0.821038]\n",
      "epoch:25 step:23898 [D loss: 0.268484, acc.: 89.84%] [G loss: 1.038020]\n",
      "epoch:25 step:23899 [D loss: 0.148123, acc.: 96.88%] [G loss: 2.373304]\n",
      "epoch:25 step:23900 [D loss: 0.444332, acc.: 68.75%] [G loss: 1.887191]\n",
      "epoch:25 step:23901 [D loss: 0.270508, acc.: 93.75%] [G loss: 1.017578]\n",
      "epoch:25 step:23902 [D loss: 0.505551, acc.: 78.12%] [G loss: 3.206149]\n",
      "epoch:25 step:23903 [D loss: 0.499717, acc.: 73.44%] [G loss: 1.045259]\n",
      "epoch:25 step:23904 [D loss: 0.433449, acc.: 86.72%] [G loss: 1.478608]\n",
      "epoch:25 step:23905 [D loss: 0.242750, acc.: 92.97%] [G loss: 1.342315]\n",
      "epoch:25 step:23906 [D loss: 0.863927, acc.: 42.19%] [G loss: 2.276233]\n",
      "epoch:25 step:23907 [D loss: 0.169022, acc.: 96.09%] [G loss: 0.854718]\n",
      "epoch:25 step:23908 [D loss: 0.104876, acc.: 98.44%] [G loss: 2.971370]\n",
      "epoch:25 step:23909 [D loss: 0.538258, acc.: 67.97%] [G loss: 0.440080]\n",
      "epoch:25 step:23910 [D loss: 0.636429, acc.: 64.06%] [G loss: 2.279308]\n",
      "epoch:25 step:23911 [D loss: 0.482183, acc.: 79.69%] [G loss: 0.605095]\n",
      "epoch:25 step:23912 [D loss: 0.104573, acc.: 100.00%] [G loss: 3.193033]\n",
      "epoch:25 step:23913 [D loss: 0.534961, acc.: 71.09%] [G loss: 2.422696]\n",
      "epoch:25 step:23914 [D loss: 1.042619, acc.: 46.88%] [G loss: 2.829462]\n",
      "epoch:25 step:23915 [D loss: 0.177934, acc.: 96.88%] [G loss: 1.039554]\n",
      "epoch:25 step:23916 [D loss: 0.982358, acc.: 47.66%] [G loss: 1.499471]\n",
      "epoch:25 step:23917 [D loss: 0.564511, acc.: 64.06%] [G loss: 1.302616]\n",
      "epoch:25 step:23918 [D loss: 0.188938, acc.: 96.09%] [G loss: 2.275968]\n",
      "epoch:25 step:23919 [D loss: 0.976849, acc.: 38.28%] [G loss: 3.300652]\n",
      "epoch:25 step:23920 [D loss: 0.490166, acc.: 77.34%] [G loss: 3.361612]\n",
      "epoch:25 step:23921 [D loss: 0.305665, acc.: 91.41%] [G loss: 1.659621]\n",
      "epoch:25 step:23922 [D loss: 0.149152, acc.: 96.88%] [G loss: 1.504980]\n",
      "epoch:25 step:23923 [D loss: 0.535459, acc.: 72.66%] [G loss: 1.647037]\n",
      "epoch:25 step:23924 [D loss: 0.295741, acc.: 93.75%] [G loss: 1.827323]\n",
      "epoch:25 step:23925 [D loss: 0.683021, acc.: 60.94%] [G loss: 2.141888]\n",
      "epoch:25 step:23926 [D loss: 0.790962, acc.: 55.47%] [G loss: 0.891169]\n",
      "epoch:25 step:23927 [D loss: 0.255608, acc.: 87.50%] [G loss: 2.061730]\n",
      "epoch:25 step:23928 [D loss: 1.436498, acc.: 32.81%] [G loss: 2.138545]\n",
      "epoch:25 step:23929 [D loss: 0.623134, acc.: 71.88%] [G loss: 1.044530]\n",
      "epoch:25 step:23930 [D loss: 0.497282, acc.: 72.66%] [G loss: 0.687622]\n",
      "epoch:25 step:23931 [D loss: 0.571539, acc.: 69.53%] [G loss: 2.602782]\n",
      "epoch:25 step:23932 [D loss: 0.502149, acc.: 77.34%] [G loss: 3.170263]\n",
      "epoch:25 step:23933 [D loss: 0.293690, acc.: 88.28%] [G loss: 2.531090]\n",
      "epoch:25 step:23934 [D loss: 0.338113, acc.: 89.06%] [G loss: 3.477561]\n",
      "epoch:25 step:23935 [D loss: 0.510897, acc.: 71.88%] [G loss: 2.838820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23936 [D loss: 0.327549, acc.: 84.38%] [G loss: 1.457799]\n",
      "epoch:25 step:23937 [D loss: 0.883411, acc.: 39.84%] [G loss: 0.647446]\n",
      "epoch:25 step:23938 [D loss: 0.493298, acc.: 75.78%] [G loss: 1.072822]\n",
      "epoch:25 step:23939 [D loss: 0.543821, acc.: 71.88%] [G loss: 2.056175]\n",
      "epoch:25 step:23940 [D loss: 0.480919, acc.: 76.56%] [G loss: 2.693878]\n",
      "epoch:25 step:23941 [D loss: 0.293283, acc.: 90.62%] [G loss: 2.693820]\n",
      "epoch:25 step:23942 [D loss: 0.252784, acc.: 93.75%] [G loss: 2.610867]\n",
      "epoch:25 step:23943 [D loss: 0.660429, acc.: 71.88%] [G loss: 0.787883]\n",
      "epoch:25 step:23944 [D loss: 0.374701, acc.: 86.72%] [G loss: 1.324152]\n",
      "epoch:25 step:23945 [D loss: 0.614610, acc.: 64.84%] [G loss: 0.962304]\n",
      "epoch:25 step:23946 [D loss: 0.264132, acc.: 89.06%] [G loss: 1.516405]\n",
      "epoch:25 step:23947 [D loss: 0.236377, acc.: 88.28%] [G loss: 1.257996]\n",
      "epoch:25 step:23948 [D loss: 1.354776, acc.: 24.22%] [G loss: 1.974082]\n",
      "epoch:25 step:23949 [D loss: 0.471856, acc.: 78.91%] [G loss: 2.355691]\n",
      "epoch:25 step:23950 [D loss: 0.269237, acc.: 93.75%] [G loss: 1.865093]\n",
      "epoch:25 step:23951 [D loss: 0.304250, acc.: 85.16%] [G loss: 1.418280]\n",
      "epoch:25 step:23952 [D loss: 0.412082, acc.: 83.59%] [G loss: 2.084572]\n",
      "epoch:25 step:23953 [D loss: 0.532243, acc.: 70.31%] [G loss: 1.689323]\n",
      "epoch:25 step:23954 [D loss: 0.232420, acc.: 93.75%] [G loss: 1.417664]\n",
      "epoch:25 step:23955 [D loss: 1.050822, acc.: 48.44%] [G loss: 0.985214]\n",
      "epoch:25 step:23956 [D loss: 0.418071, acc.: 81.25%] [G loss: 1.127189]\n",
      "epoch:25 step:23957 [D loss: 0.229601, acc.: 94.53%] [G loss: 2.182115]\n",
      "epoch:25 step:23958 [D loss: 0.329250, acc.: 90.62%] [G loss: 2.664715]\n",
      "epoch:25 step:23959 [D loss: 0.290910, acc.: 91.41%] [G loss: 1.114154]\n",
      "epoch:25 step:23960 [D loss: 0.143203, acc.: 100.00%] [G loss: 2.507131]\n",
      "epoch:25 step:23961 [D loss: 0.560971, acc.: 74.22%] [G loss: 0.670763]\n",
      "epoch:25 step:23962 [D loss: 0.360974, acc.: 91.41%] [G loss: 0.742311]\n",
      "epoch:25 step:23963 [D loss: 0.239650, acc.: 91.41%] [G loss: 1.048764]\n",
      "epoch:25 step:23964 [D loss: 0.171583, acc.: 98.44%] [G loss: 2.137734]\n",
      "epoch:25 step:23965 [D loss: 0.867081, acc.: 54.69%] [G loss: 3.370042]\n",
      "epoch:25 step:23966 [D loss: 0.224150, acc.: 95.31%] [G loss: 1.737677]\n",
      "epoch:25 step:23967 [D loss: 0.678027, acc.: 65.62%] [G loss: 3.553626]\n",
      "epoch:25 step:23968 [D loss: 0.086861, acc.: 100.00%] [G loss: 3.116714]\n",
      "epoch:25 step:23969 [D loss: 0.220734, acc.: 92.19%] [G loss: 1.484724]\n",
      "epoch:25 step:23970 [D loss: 0.366230, acc.: 85.94%] [G loss: 2.128047]\n",
      "epoch:25 step:23971 [D loss: 0.503506, acc.: 71.09%] [G loss: 2.156759]\n",
      "epoch:25 step:23972 [D loss: 0.382388, acc.: 83.59%] [G loss: 1.586183]\n",
      "epoch:25 step:23973 [D loss: 0.502284, acc.: 72.66%] [G loss: 2.280105]\n",
      "epoch:25 step:23974 [D loss: 0.415810, acc.: 72.66%] [G loss: 3.079049]\n",
      "epoch:25 step:23975 [D loss: 0.349883, acc.: 89.06%] [G loss: 1.749988]\n",
      "epoch:25 step:23976 [D loss: 0.529837, acc.: 68.75%] [G loss: 2.825228]\n",
      "epoch:25 step:23977 [D loss: 0.514252, acc.: 74.22%] [G loss: 1.317103]\n",
      "epoch:25 step:23978 [D loss: 0.137704, acc.: 95.31%] [G loss: 1.349921]\n",
      "epoch:25 step:23979 [D loss: 0.169675, acc.: 95.31%] [G loss: 1.346585]\n",
      "epoch:25 step:23980 [D loss: 0.460898, acc.: 81.25%] [G loss: 1.214237]\n",
      "epoch:25 step:23981 [D loss: 1.046865, acc.: 26.56%] [G loss: 1.499814]\n",
      "epoch:25 step:23982 [D loss: 0.555859, acc.: 69.53%] [G loss: 2.342168]\n",
      "epoch:25 step:23983 [D loss: 0.401380, acc.: 84.38%] [G loss: 3.076763]\n",
      "epoch:25 step:23984 [D loss: 0.771227, acc.: 50.00%] [G loss: 2.072321]\n",
      "epoch:25 step:23985 [D loss: 0.330752, acc.: 85.16%] [G loss: 1.994712]\n",
      "epoch:25 step:23986 [D loss: 0.394895, acc.: 78.12%] [G loss: 1.259029]\n",
      "epoch:25 step:23987 [D loss: 0.430390, acc.: 78.91%] [G loss: 1.008733]\n",
      "epoch:25 step:23988 [D loss: 0.420180, acc.: 82.03%] [G loss: 2.146989]\n",
      "epoch:25 step:23989 [D loss: 0.351125, acc.: 89.84%] [G loss: 2.498271]\n",
      "epoch:25 step:23990 [D loss: 0.122199, acc.: 98.44%] [G loss: 2.393842]\n",
      "epoch:25 step:23991 [D loss: 0.255272, acc.: 90.62%] [G loss: 1.035694]\n",
      "epoch:25 step:23992 [D loss: 0.802345, acc.: 48.44%] [G loss: 2.453959]\n",
      "epoch:25 step:23993 [D loss: 0.653459, acc.: 71.09%] [G loss: 0.703140]\n",
      "epoch:25 step:23994 [D loss: 0.208748, acc.: 97.66%] [G loss: 2.208744]\n",
      "epoch:25 step:23995 [D loss: 0.514953, acc.: 75.00%] [G loss: 1.777367]\n",
      "epoch:25 step:23996 [D loss: 0.134396, acc.: 99.22%] [G loss: 0.916638]\n",
      "epoch:25 step:23997 [D loss: 0.518944, acc.: 70.31%] [G loss: 1.738264]\n",
      "epoch:25 step:23998 [D loss: 0.149923, acc.: 96.88%] [G loss: 1.175240]\n",
      "epoch:25 step:23999 [D loss: 0.944786, acc.: 53.12%] [G loss: 1.127023]\n",
      "epoch:25 step:24000 [D loss: 0.457688, acc.: 78.91%] [G loss: 0.942341]\n",
      "epoch:25 step:24001 [D loss: 0.429885, acc.: 80.47%] [G loss: 2.248736]\n",
      "epoch:25 step:24002 [D loss: 0.164428, acc.: 95.31%] [G loss: 1.301950]\n",
      "epoch:25 step:24003 [D loss: 0.309423, acc.: 90.62%] [G loss: 2.562720]\n",
      "epoch:25 step:24004 [D loss: 0.287449, acc.: 92.97%] [G loss: 1.503658]\n",
      "epoch:25 step:24005 [D loss: 0.739659, acc.: 56.25%] [G loss: 0.996493]\n",
      "epoch:25 step:24006 [D loss: 0.673063, acc.: 62.50%] [G loss: 2.675101]\n",
      "epoch:25 step:24007 [D loss: 0.244812, acc.: 93.75%] [G loss: 2.206924]\n",
      "epoch:25 step:24008 [D loss: 0.243806, acc.: 94.53%] [G loss: 1.138871]\n",
      "epoch:25 step:24009 [D loss: 0.588938, acc.: 67.19%] [G loss: 1.748370]\n",
      "epoch:25 step:24010 [D loss: 0.171833, acc.: 96.09%] [G loss: 2.238739]\n",
      "epoch:25 step:24011 [D loss: 0.259370, acc.: 92.19%] [G loss: 1.403244]\n",
      "epoch:25 step:24012 [D loss: 0.199933, acc.: 97.66%] [G loss: 2.631141]\n",
      "epoch:25 step:24013 [D loss: 0.223177, acc.: 92.97%] [G loss: 3.317034]\n",
      "epoch:25 step:24014 [D loss: 0.535025, acc.: 74.22%] [G loss: 0.907209]\n",
      "epoch:25 step:24015 [D loss: 1.093878, acc.: 41.41%] [G loss: 2.608192]\n",
      "epoch:25 step:24016 [D loss: 0.190424, acc.: 93.75%] [G loss: 2.323288]\n",
      "epoch:25 step:24017 [D loss: 0.667530, acc.: 64.84%] [G loss: 3.772291]\n",
      "epoch:25 step:24018 [D loss: 0.397915, acc.: 81.25%] [G loss: 2.357643]\n",
      "epoch:25 step:24019 [D loss: 0.324380, acc.: 85.94%] [G loss: 2.755939]\n",
      "epoch:25 step:24020 [D loss: 0.313713, acc.: 87.50%] [G loss: 1.168582]\n",
      "epoch:25 step:24021 [D loss: 0.126986, acc.: 96.88%] [G loss: 2.128547]\n",
      "epoch:25 step:24022 [D loss: 0.436638, acc.: 82.81%] [G loss: 1.881693]\n",
      "epoch:25 step:24023 [D loss: 0.489909, acc.: 69.53%] [G loss: 3.279744]\n",
      "epoch:25 step:24024 [D loss: 0.299655, acc.: 86.72%] [G loss: 6.573050]\n",
      "epoch:25 step:24025 [D loss: 0.447489, acc.: 78.12%] [G loss: 2.573519]\n",
      "epoch:25 step:24026 [D loss: 0.739906, acc.: 56.25%] [G loss: 3.564690]\n",
      "epoch:25 step:24027 [D loss: 0.435822, acc.: 78.91%] [G loss: 3.163612]\n",
      "epoch:25 step:24028 [D loss: 0.238520, acc.: 94.53%] [G loss: 1.079951]\n",
      "epoch:25 step:24029 [D loss: 0.636576, acc.: 66.41%] [G loss: 1.166194]\n",
      "epoch:25 step:24030 [D loss: 0.152197, acc.: 96.88%] [G loss: 4.942878]\n",
      "epoch:25 step:24031 [D loss: 0.311769, acc.: 84.38%] [G loss: 2.021118]\n",
      "epoch:25 step:24032 [D loss: 0.227119, acc.: 89.06%] [G loss: 3.345172]\n",
      "epoch:25 step:24033 [D loss: 0.710054, acc.: 55.47%] [G loss: 1.777029]\n",
      "epoch:25 step:24034 [D loss: 0.367593, acc.: 84.38%] [G loss: 2.221119]\n",
      "epoch:25 step:24035 [D loss: 0.481250, acc.: 78.12%] [G loss: 3.296286]\n",
      "epoch:25 step:24036 [D loss: 0.428786, acc.: 80.47%] [G loss: 3.366780]\n",
      "epoch:25 step:24037 [D loss: 0.406615, acc.: 78.91%] [G loss: 0.947962]\n",
      "epoch:25 step:24038 [D loss: 0.052452, acc.: 100.00%] [G loss: 1.235179]\n",
      "epoch:25 step:24039 [D loss: 0.473710, acc.: 82.03%] [G loss: 3.045185]\n",
      "epoch:25 step:24040 [D loss: 0.095451, acc.: 96.88%] [G loss: 2.376517]\n",
      "epoch:25 step:24041 [D loss: 0.518650, acc.: 70.31%] [G loss: 2.523989]\n",
      "epoch:25 step:24042 [D loss: 0.193098, acc.: 98.44%] [G loss: 1.565389]\n",
      "epoch:25 step:24043 [D loss: 0.670037, acc.: 64.06%] [G loss: 0.442460]\n",
      "epoch:25 step:24044 [D loss: 0.283603, acc.: 89.84%] [G loss: 1.446330]\n",
      "epoch:25 step:24045 [D loss: 0.194052, acc.: 96.09%] [G loss: 1.781145]\n",
      "epoch:25 step:24046 [D loss: 0.355047, acc.: 80.47%] [G loss: 1.169337]\n",
      "epoch:25 step:24047 [D loss: 0.276557, acc.: 89.06%] [G loss: 3.878119]\n",
      "epoch:25 step:24048 [D loss: 0.200097, acc.: 93.75%] [G loss: 1.921936]\n",
      "epoch:25 step:24049 [D loss: 0.920381, acc.: 54.69%] [G loss: 2.006316]\n",
      "epoch:25 step:24050 [D loss: 0.198386, acc.: 94.53%] [G loss: 1.669106]\n",
      "epoch:25 step:24051 [D loss: 0.310393, acc.: 85.94%] [G loss: 4.691154]\n",
      "epoch:25 step:24052 [D loss: 0.251617, acc.: 92.19%] [G loss: 3.879189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24053 [D loss: 0.101482, acc.: 99.22%] [G loss: 2.562410]\n",
      "epoch:25 step:24054 [D loss: 0.714528, acc.: 58.59%] [G loss: 1.586226]\n",
      "epoch:25 step:24055 [D loss: 0.393475, acc.: 82.03%] [G loss: 2.914537]\n",
      "epoch:25 step:24056 [D loss: 0.184395, acc.: 93.75%] [G loss: 3.584469]\n",
      "epoch:25 step:24057 [D loss: 0.074279, acc.: 99.22%] [G loss: 1.378986]\n",
      "epoch:25 step:24058 [D loss: 0.388506, acc.: 81.25%] [G loss: 1.671038]\n",
      "epoch:25 step:24059 [D loss: 0.193940, acc.: 96.09%] [G loss: 1.095064]\n",
      "epoch:25 step:24060 [D loss: 0.109764, acc.: 98.44%] [G loss: 1.647969]\n",
      "epoch:25 step:24061 [D loss: 0.437811, acc.: 75.00%] [G loss: 3.916133]\n",
      "epoch:25 step:24062 [D loss: 0.255594, acc.: 92.19%] [G loss: 2.715875]\n",
      "epoch:25 step:24063 [D loss: 0.376390, acc.: 80.47%] [G loss: 4.197021]\n",
      "epoch:25 step:24064 [D loss: 0.543294, acc.: 70.31%] [G loss: 3.437654]\n",
      "epoch:25 step:24065 [D loss: 0.162598, acc.: 97.66%] [G loss: 4.540224]\n",
      "epoch:25 step:24066 [D loss: 0.228386, acc.: 92.97%] [G loss: 5.328989]\n",
      "epoch:25 step:24067 [D loss: 0.652696, acc.: 64.06%] [G loss: 4.023833]\n",
      "epoch:25 step:24068 [D loss: 0.272417, acc.: 93.75%] [G loss: 4.177837]\n",
      "epoch:25 step:24069 [D loss: 0.341215, acc.: 86.72%] [G loss: 5.061976]\n",
      "epoch:25 step:24070 [D loss: 0.161781, acc.: 96.09%] [G loss: 2.563558]\n",
      "epoch:25 step:24071 [D loss: 0.431737, acc.: 79.69%] [G loss: 1.942190]\n",
      "epoch:25 step:24072 [D loss: 0.352055, acc.: 86.72%] [G loss: 2.216284]\n",
      "epoch:25 step:24073 [D loss: 0.086143, acc.: 98.44%] [G loss: 2.480302]\n",
      "epoch:25 step:24074 [D loss: 0.333771, acc.: 86.72%] [G loss: 0.818403]\n",
      "epoch:25 step:24075 [D loss: 0.500045, acc.: 71.88%] [G loss: 1.472600]\n",
      "epoch:25 step:24076 [D loss: 0.329161, acc.: 90.62%] [G loss: 2.175461]\n",
      "epoch:25 step:24077 [D loss: 0.884204, acc.: 54.69%] [G loss: 0.849732]\n",
      "epoch:25 step:24078 [D loss: 0.894651, acc.: 56.25%] [G loss: 2.712511]\n",
      "epoch:25 step:24079 [D loss: 0.417982, acc.: 79.69%] [G loss: 2.720399]\n",
      "epoch:25 step:24080 [D loss: 0.152946, acc.: 96.09%] [G loss: 2.186779]\n",
      "epoch:25 step:24081 [D loss: 0.312564, acc.: 88.28%] [G loss: 1.573654]\n",
      "epoch:25 step:24082 [D loss: 0.824206, acc.: 56.25%] [G loss: 1.715086]\n",
      "epoch:25 step:24083 [D loss: 0.981209, acc.: 44.53%] [G loss: 2.900687]\n",
      "epoch:25 step:24084 [D loss: 0.553279, acc.: 71.88%] [G loss: 2.282807]\n",
      "epoch:25 step:24085 [D loss: 0.171291, acc.: 96.09%] [G loss: 4.169565]\n",
      "epoch:25 step:24086 [D loss: 0.492080, acc.: 76.56%] [G loss: 6.432193]\n",
      "epoch:25 step:24087 [D loss: 0.231904, acc.: 93.75%] [G loss: 4.001307]\n",
      "epoch:25 step:24088 [D loss: 1.452916, acc.: 50.00%] [G loss: 1.061674]\n",
      "epoch:25 step:24089 [D loss: 0.959115, acc.: 48.44%] [G loss: 3.756598]\n",
      "epoch:25 step:24090 [D loss: 0.735709, acc.: 57.81%] [G loss: 2.842810]\n",
      "epoch:25 step:24091 [D loss: 0.515947, acc.: 73.44%] [G loss: 1.868522]\n",
      "epoch:25 step:24092 [D loss: 0.468485, acc.: 77.34%] [G loss: 2.753576]\n",
      "epoch:25 step:24093 [D loss: 0.870551, acc.: 55.47%] [G loss: 1.953509]\n",
      "epoch:25 step:24094 [D loss: 0.313180, acc.: 85.94%] [G loss: 2.814660]\n",
      "epoch:25 step:24095 [D loss: 0.361518, acc.: 86.72%] [G loss: 1.224139]\n",
      "epoch:25 step:24096 [D loss: 0.330972, acc.: 89.84%] [G loss: 1.002144]\n",
      "epoch:25 step:24097 [D loss: 0.573902, acc.: 68.75%] [G loss: 1.416321]\n",
      "epoch:25 step:24098 [D loss: 0.379484, acc.: 84.38%] [G loss: 2.098616]\n",
      "epoch:25 step:24099 [D loss: 0.237894, acc.: 95.31%] [G loss: 2.342401]\n",
      "epoch:25 step:24100 [D loss: 0.208121, acc.: 97.66%] [G loss: 1.244266]\n",
      "epoch:25 step:24101 [D loss: 0.198980, acc.: 96.88%] [G loss: 1.852597]\n",
      "epoch:25 step:24102 [D loss: 0.632190, acc.: 64.84%] [G loss: 1.710215]\n",
      "epoch:25 step:24103 [D loss: 0.562770, acc.: 69.53%] [G loss: 3.438285]\n",
      "epoch:25 step:24104 [D loss: 0.405929, acc.: 78.12%] [G loss: 2.621381]\n",
      "epoch:25 step:24105 [D loss: 0.429897, acc.: 81.25%] [G loss: 4.256702]\n",
      "epoch:25 step:24106 [D loss: 0.217716, acc.: 96.09%] [G loss: 0.623971]\n",
      "epoch:25 step:24107 [D loss: 1.196291, acc.: 29.69%] [G loss: 2.810714]\n",
      "epoch:25 step:24108 [D loss: 0.221917, acc.: 95.31%] [G loss: 2.955046]\n",
      "epoch:25 step:24109 [D loss: 0.208295, acc.: 94.53%] [G loss: 2.463231]\n",
      "epoch:25 step:24110 [D loss: 0.427321, acc.: 84.38%] [G loss: 2.564475]\n",
      "epoch:25 step:24111 [D loss: 0.625271, acc.: 66.41%] [G loss: 1.261071]\n",
      "epoch:25 step:24112 [D loss: 0.311673, acc.: 90.62%] [G loss: 1.866138]\n",
      "epoch:25 step:24113 [D loss: 0.125636, acc.: 98.44%] [G loss: 2.454666]\n",
      "epoch:25 step:24114 [D loss: 0.785969, acc.: 57.81%] [G loss: 1.375924]\n",
      "epoch:25 step:24115 [D loss: 0.769359, acc.: 58.59%] [G loss: 2.719867]\n",
      "epoch:25 step:24116 [D loss: 0.453620, acc.: 71.88%] [G loss: 2.071475]\n",
      "epoch:25 step:24117 [D loss: 0.419355, acc.: 75.78%] [G loss: 1.101106]\n",
      "epoch:25 step:24118 [D loss: 0.920321, acc.: 39.84%] [G loss: 0.754198]\n",
      "epoch:25 step:24119 [D loss: 0.322555, acc.: 93.75%] [G loss: 1.791938]\n",
      "epoch:25 step:24120 [D loss: 0.515890, acc.: 73.44%] [G loss: 1.297038]\n",
      "epoch:25 step:24121 [D loss: 0.170443, acc.: 97.66%] [G loss: 1.157424]\n",
      "epoch:25 step:24122 [D loss: 0.396818, acc.: 83.59%] [G loss: 1.754599]\n",
      "epoch:25 step:24123 [D loss: 0.303148, acc.: 96.88%] [G loss: 0.959518]\n",
      "epoch:25 step:24124 [D loss: 0.900051, acc.: 55.47%] [G loss: 1.497984]\n",
      "epoch:25 step:24125 [D loss: 0.234217, acc.: 93.75%] [G loss: 2.788442]\n",
      "epoch:25 step:24126 [D loss: 0.538563, acc.: 67.97%] [G loss: 0.988127]\n",
      "epoch:25 step:24127 [D loss: 0.313792, acc.: 83.59%] [G loss: 0.726069]\n",
      "epoch:25 step:24128 [D loss: 1.038118, acc.: 42.19%] [G loss: 0.917475]\n",
      "epoch:25 step:24129 [D loss: 0.377376, acc.: 85.16%] [G loss: 0.903803]\n",
      "epoch:25 step:24130 [D loss: 0.359006, acc.: 85.94%] [G loss: 1.191566]\n",
      "epoch:25 step:24131 [D loss: 0.233442, acc.: 94.53%] [G loss: 2.295061]\n",
      "epoch:25 step:24132 [D loss: 0.271184, acc.: 94.53%] [G loss: 1.375306]\n",
      "epoch:25 step:24133 [D loss: 0.324396, acc.: 94.53%] [G loss: 1.750848]\n",
      "epoch:25 step:24134 [D loss: 0.443170, acc.: 75.00%] [G loss: 1.830074]\n",
      "epoch:25 step:24135 [D loss: 0.283973, acc.: 85.94%] [G loss: 2.193277]\n",
      "epoch:25 step:24136 [D loss: 0.418780, acc.: 85.16%] [G loss: 1.727809]\n",
      "epoch:25 step:24137 [D loss: 0.318575, acc.: 91.41%] [G loss: 2.571265]\n",
      "epoch:25 step:24138 [D loss: 0.279340, acc.: 91.41%] [G loss: 3.447444]\n",
      "epoch:25 step:24139 [D loss: 0.200527, acc.: 96.09%] [G loss: 2.024514]\n",
      "epoch:25 step:24140 [D loss: 0.331904, acc.: 82.81%] [G loss: 1.593271]\n",
      "epoch:25 step:24141 [D loss: 0.643078, acc.: 62.50%] [G loss: 1.016438]\n",
      "epoch:25 step:24142 [D loss: 0.915788, acc.: 39.84%] [G loss: 1.738181]\n",
      "epoch:25 step:24143 [D loss: 0.219847, acc.: 92.97%] [G loss: 0.874154]\n",
      "epoch:25 step:24144 [D loss: 0.227687, acc.: 94.53%] [G loss: 2.720894]\n",
      "epoch:25 step:24145 [D loss: 0.524502, acc.: 71.09%] [G loss: 2.136712]\n",
      "epoch:25 step:24146 [D loss: 1.090209, acc.: 53.12%] [G loss: 2.885786]\n",
      "epoch:25 step:24147 [D loss: 0.427425, acc.: 76.56%] [G loss: 1.961342]\n",
      "epoch:25 step:24148 [D loss: 0.215754, acc.: 94.53%] [G loss: 3.628893]\n",
      "epoch:25 step:24149 [D loss: 0.241647, acc.: 93.75%] [G loss: 0.530130]\n",
      "epoch:25 step:24150 [D loss: 0.613382, acc.: 64.84%] [G loss: 0.692962]\n",
      "epoch:25 step:24151 [D loss: 0.344654, acc.: 89.84%] [G loss: 1.769165]\n",
      "epoch:25 step:24152 [D loss: 0.238245, acc.: 92.97%] [G loss: 2.936913]\n",
      "epoch:25 step:24153 [D loss: 0.781660, acc.: 60.16%] [G loss: 1.161479]\n",
      "epoch:25 step:24154 [D loss: 0.343540, acc.: 82.03%] [G loss: 1.696325]\n",
      "epoch:25 step:24155 [D loss: 0.735224, acc.: 53.12%] [G loss: 1.034784]\n",
      "epoch:25 step:24156 [D loss: 0.415195, acc.: 85.16%] [G loss: 0.853562]\n",
      "epoch:25 step:24157 [D loss: 0.275864, acc.: 92.19%] [G loss: 2.616825]\n",
      "epoch:25 step:24158 [D loss: 0.394126, acc.: 85.16%] [G loss: 1.856237]\n",
      "epoch:25 step:24159 [D loss: 0.190855, acc.: 97.66%] [G loss: 1.840519]\n",
      "epoch:25 step:24160 [D loss: 0.264628, acc.: 92.97%] [G loss: 1.995310]\n",
      "epoch:25 step:24161 [D loss: 0.493466, acc.: 72.66%] [G loss: 1.295075]\n",
      "epoch:25 step:24162 [D loss: 0.282588, acc.: 91.41%] [G loss: 2.269066]\n",
      "epoch:25 step:24163 [D loss: 1.138663, acc.: 53.91%] [G loss: 0.880456]\n",
      "epoch:25 step:24164 [D loss: 0.188752, acc.: 96.88%] [G loss: 1.068987]\n",
      "epoch:25 step:24165 [D loss: 0.472103, acc.: 76.56%] [G loss: 1.817645]\n",
      "epoch:25 step:24166 [D loss: 0.223129, acc.: 96.09%] [G loss: 3.844855]\n",
      "epoch:25 step:24167 [D loss: 0.258347, acc.: 93.75%] [G loss: 2.237846]\n",
      "epoch:25 step:24168 [D loss: 0.189654, acc.: 96.88%] [G loss: 4.176109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24169 [D loss: 0.203354, acc.: 94.53%] [G loss: 1.034280]\n",
      "epoch:25 step:24170 [D loss: 0.764366, acc.: 55.47%] [G loss: 1.367371]\n",
      "epoch:25 step:24171 [D loss: 0.769498, acc.: 60.16%] [G loss: 1.531781]\n",
      "epoch:25 step:24172 [D loss: 0.587684, acc.: 65.62%] [G loss: 2.924165]\n",
      "epoch:25 step:24173 [D loss: 0.237230, acc.: 96.88%] [G loss: 1.506165]\n",
      "epoch:25 step:24174 [D loss: 0.279386, acc.: 95.31%] [G loss: 2.724204]\n",
      "epoch:25 step:24175 [D loss: 0.142565, acc.: 95.31%] [G loss: 2.426713]\n",
      "epoch:25 step:24176 [D loss: 0.481112, acc.: 75.00%] [G loss: 1.717921]\n",
      "epoch:25 step:24177 [D loss: 0.416879, acc.: 72.66%] [G loss: 2.048329]\n",
      "epoch:25 step:24178 [D loss: 0.217931, acc.: 96.09%] [G loss: 1.783494]\n",
      "epoch:25 step:24179 [D loss: 0.611718, acc.: 64.84%] [G loss: 2.095705]\n",
      "epoch:25 step:24180 [D loss: 0.625798, acc.: 60.16%] [G loss: 0.310247]\n",
      "epoch:25 step:24181 [D loss: 0.461260, acc.: 71.88%] [G loss: 2.821479]\n",
      "epoch:25 step:24182 [D loss: 0.884264, acc.: 42.19%] [G loss: 0.727369]\n",
      "epoch:25 step:24183 [D loss: 0.215315, acc.: 95.31%] [G loss: 1.602661]\n",
      "epoch:25 step:24184 [D loss: 0.981325, acc.: 50.78%] [G loss: 3.397929]\n",
      "epoch:25 step:24185 [D loss: 0.281612, acc.: 92.97%] [G loss: 2.890532]\n",
      "epoch:25 step:24186 [D loss: 0.539081, acc.: 72.66%] [G loss: 4.025147]\n",
      "epoch:25 step:24187 [D loss: 0.373147, acc.: 87.50%] [G loss: 2.901292]\n",
      "epoch:25 step:24188 [D loss: 0.959562, acc.: 56.25%] [G loss: 1.255966]\n",
      "epoch:25 step:24189 [D loss: 0.392529, acc.: 82.03%] [G loss: 1.633660]\n",
      "epoch:25 step:24190 [D loss: 0.367042, acc.: 85.94%] [G loss: 1.024408]\n",
      "epoch:25 step:24191 [D loss: 0.147419, acc.: 96.88%] [G loss: 2.641896]\n",
      "epoch:25 step:24192 [D loss: 0.091147, acc.: 99.22%] [G loss: 2.028669]\n",
      "epoch:25 step:24193 [D loss: 0.563157, acc.: 72.66%] [G loss: 2.460791]\n",
      "epoch:25 step:24194 [D loss: 0.232833, acc.: 95.31%] [G loss: 3.338731]\n",
      "epoch:25 step:24195 [D loss: 0.309499, acc.: 90.62%] [G loss: 3.618350]\n",
      "epoch:25 step:24196 [D loss: 0.118380, acc.: 99.22%] [G loss: 1.346509]\n",
      "epoch:25 step:24197 [D loss: 0.095476, acc.: 98.44%] [G loss: 1.174201]\n",
      "epoch:25 step:24198 [D loss: 0.243942, acc.: 95.31%] [G loss: 1.305857]\n",
      "epoch:25 step:24199 [D loss: 0.417752, acc.: 83.59%] [G loss: 3.103667]\n",
      "epoch:25 step:24200 [D loss: 0.138059, acc.: 97.66%] [G loss: 1.789044]\n",
      "epoch:25 step:24201 [D loss: 0.125552, acc.: 97.66%] [G loss: 2.047918]\n",
      "epoch:25 step:24202 [D loss: 0.106599, acc.: 99.22%] [G loss: 2.126623]\n",
      "epoch:25 step:24203 [D loss: 0.247809, acc.: 92.97%] [G loss: 1.532740]\n",
      "epoch:25 step:24204 [D loss: 1.567341, acc.: 16.41%] [G loss: 2.701767]\n",
      "epoch:25 step:24205 [D loss: 0.466522, acc.: 75.00%] [G loss: 2.031977]\n",
      "epoch:25 step:24206 [D loss: 0.225960, acc.: 92.19%] [G loss: 2.534541]\n",
      "epoch:25 step:24207 [D loss: 0.132500, acc.: 98.44%] [G loss: 0.866611]\n",
      "epoch:25 step:24208 [D loss: 0.212039, acc.: 94.53%] [G loss: 2.732910]\n",
      "epoch:25 step:24209 [D loss: 0.310019, acc.: 86.72%] [G loss: 0.638037]\n",
      "epoch:25 step:24210 [D loss: 0.711751, acc.: 59.38%] [G loss: 2.497927]\n",
      "epoch:25 step:24211 [D loss: 0.281525, acc.: 95.31%] [G loss: 2.967450]\n",
      "epoch:25 step:24212 [D loss: 0.528948, acc.: 73.44%] [G loss: 0.441603]\n",
      "epoch:25 step:24213 [D loss: 0.284836, acc.: 92.97%] [G loss: 2.615439]\n",
      "epoch:25 step:24214 [D loss: 0.650703, acc.: 63.28%] [G loss: 1.901868]\n",
      "epoch:25 step:24215 [D loss: 0.217649, acc.: 96.88%] [G loss: 3.142653]\n",
      "epoch:25 step:24216 [D loss: 0.693677, acc.: 58.59%] [G loss: 3.874105]\n",
      "epoch:25 step:24217 [D loss: 0.492376, acc.: 75.78%] [G loss: 3.188429]\n",
      "epoch:25 step:24218 [D loss: 0.346898, acc.: 84.38%] [G loss: 2.541962]\n",
      "epoch:25 step:24219 [D loss: 0.816445, acc.: 58.59%] [G loss: 2.547161]\n",
      "epoch:25 step:24220 [D loss: 0.366108, acc.: 85.16%] [G loss: 0.831490]\n",
      "epoch:25 step:24221 [D loss: 0.209957, acc.: 96.09%] [G loss: 1.522926]\n",
      "epoch:25 step:24222 [D loss: 0.164615, acc.: 95.31%] [G loss: 2.409618]\n",
      "epoch:25 step:24223 [D loss: 0.305202, acc.: 86.72%] [G loss: 1.053535]\n",
      "epoch:25 step:24224 [D loss: 0.385204, acc.: 88.28%] [G loss: 2.440269]\n",
      "epoch:25 step:24225 [D loss: 0.488068, acc.: 71.09%] [G loss: 3.766468]\n",
      "epoch:25 step:24226 [D loss: 0.365104, acc.: 86.72%] [G loss: 1.510933]\n",
      "epoch:25 step:24227 [D loss: 0.375664, acc.: 87.50%] [G loss: 4.136921]\n",
      "epoch:25 step:24228 [D loss: 0.109965, acc.: 98.44%] [G loss: 1.698266]\n",
      "epoch:25 step:24229 [D loss: 0.925031, acc.: 39.84%] [G loss: 4.546838]\n",
      "epoch:25 step:24230 [D loss: 0.291443, acc.: 90.62%] [G loss: 2.011695]\n",
      "epoch:25 step:24231 [D loss: 0.462105, acc.: 81.25%] [G loss: 1.473333]\n",
      "epoch:25 step:24232 [D loss: 0.862026, acc.: 53.91%] [G loss: 0.902427]\n",
      "epoch:25 step:24233 [D loss: 0.193778, acc.: 97.66%] [G loss: 2.204815]\n",
      "epoch:25 step:24234 [D loss: 0.353045, acc.: 84.38%] [G loss: 1.683149]\n",
      "epoch:25 step:24235 [D loss: 0.276231, acc.: 92.19%] [G loss: 1.022242]\n",
      "epoch:25 step:24236 [D loss: 1.189506, acc.: 32.81%] [G loss: 1.119388]\n",
      "epoch:25 step:24237 [D loss: 0.778058, acc.: 56.25%] [G loss: 0.633131]\n",
      "epoch:25 step:24238 [D loss: 0.336496, acc.: 86.72%] [G loss: 0.943244]\n",
      "epoch:25 step:24239 [D loss: 0.281254, acc.: 89.84%] [G loss: 1.173949]\n",
      "epoch:25 step:24240 [D loss: 0.539846, acc.: 71.09%] [G loss: 0.398774]\n",
      "epoch:25 step:24241 [D loss: 0.909391, acc.: 50.78%] [G loss: 1.258467]\n",
      "epoch:25 step:24242 [D loss: 0.160048, acc.: 98.44%] [G loss: 3.518335]\n",
      "epoch:25 step:24243 [D loss: 0.710071, acc.: 59.38%] [G loss: 2.359544]\n",
      "epoch:25 step:24244 [D loss: 0.325028, acc.: 90.62%] [G loss: 1.379387]\n",
      "epoch:25 step:24245 [D loss: 0.268201, acc.: 92.97%] [G loss: 2.618271]\n",
      "epoch:25 step:24246 [D loss: 0.284346, acc.: 95.31%] [G loss: 3.512550]\n",
      "epoch:25 step:24247 [D loss: 0.531987, acc.: 75.00%] [G loss: 3.543105]\n",
      "epoch:25 step:24248 [D loss: 0.543527, acc.: 66.41%] [G loss: 3.802273]\n",
      "epoch:25 step:24249 [D loss: 0.287145, acc.: 89.84%] [G loss: 3.113915]\n",
      "epoch:25 step:24250 [D loss: 0.744167, acc.: 60.16%] [G loss: 4.094910]\n",
      "epoch:25 step:24251 [D loss: 0.251419, acc.: 95.31%] [G loss: 1.084925]\n",
      "epoch:25 step:24252 [D loss: 0.166905, acc.: 97.66%] [G loss: 1.067497]\n",
      "epoch:25 step:24253 [D loss: 0.162643, acc.: 96.88%] [G loss: 1.488014]\n",
      "epoch:25 step:24254 [D loss: 0.834440, acc.: 47.66%] [G loss: 0.918972]\n",
      "epoch:25 step:24255 [D loss: 0.197180, acc.: 96.09%] [G loss: 1.266542]\n",
      "epoch:25 step:24256 [D loss: 0.230308, acc.: 91.41%] [G loss: 1.489475]\n",
      "epoch:25 step:24257 [D loss: 0.547919, acc.: 67.19%] [G loss: 1.177715]\n",
      "epoch:25 step:24258 [D loss: 0.298351, acc.: 89.06%] [G loss: 1.186200]\n",
      "epoch:25 step:24259 [D loss: 0.362853, acc.: 81.25%] [G loss: 4.179013]\n",
      "epoch:25 step:24260 [D loss: 0.144751, acc.: 99.22%] [G loss: 3.417236]\n",
      "epoch:25 step:24261 [D loss: 0.099556, acc.: 97.66%] [G loss: 2.954702]\n",
      "epoch:25 step:24262 [D loss: 0.407008, acc.: 82.81%] [G loss: 3.235868]\n",
      "epoch:25 step:24263 [D loss: 0.235209, acc.: 92.97%] [G loss: 3.497428]\n",
      "epoch:25 step:24264 [D loss: 0.156797, acc.: 98.44%] [G loss: 4.141147]\n",
      "epoch:25 step:24265 [D loss: 0.311344, acc.: 85.16%] [G loss: 0.936139]\n",
      "epoch:25 step:24266 [D loss: 0.819489, acc.: 58.59%] [G loss: 3.603263]\n",
      "epoch:25 step:24267 [D loss: 0.442606, acc.: 70.31%] [G loss: 1.957610]\n",
      "epoch:25 step:24268 [D loss: 0.332958, acc.: 82.81%] [G loss: 1.850392]\n",
      "epoch:25 step:24269 [D loss: 0.401196, acc.: 88.28%] [G loss: 1.802445]\n",
      "epoch:25 step:24270 [D loss: 0.747333, acc.: 60.16%] [G loss: 1.941469]\n",
      "epoch:25 step:24271 [D loss: 0.284038, acc.: 89.84%] [G loss: 0.749064]\n",
      "epoch:25 step:24272 [D loss: 0.778196, acc.: 60.16%] [G loss: 2.433094]\n",
      "epoch:25 step:24273 [D loss: 0.344489, acc.: 88.28%] [G loss: 2.071031]\n",
      "epoch:25 step:24274 [D loss: 0.154804, acc.: 98.44%] [G loss: 1.407309]\n",
      "epoch:25 step:24275 [D loss: 0.402864, acc.: 78.91%] [G loss: 3.186282]\n",
      "epoch:25 step:24276 [D loss: 1.314816, acc.: 35.94%] [G loss: 2.160322]\n",
      "epoch:25 step:24277 [D loss: 0.451270, acc.: 78.12%] [G loss: 1.119995]\n",
      "epoch:25 step:24278 [D loss: 0.326768, acc.: 89.84%] [G loss: 2.906599]\n",
      "epoch:25 step:24279 [D loss: 0.307744, acc.: 92.97%] [G loss: 1.600974]\n",
      "epoch:25 step:24280 [D loss: 0.894625, acc.: 32.03%] [G loss: 0.716439]\n",
      "epoch:25 step:24281 [D loss: 0.724597, acc.: 61.72%] [G loss: 2.094672]\n",
      "epoch:25 step:24282 [D loss: 0.319730, acc.: 89.06%] [G loss: 2.910691]\n",
      "epoch:25 step:24283 [D loss: 0.445146, acc.: 76.56%] [G loss: 1.595646]\n",
      "epoch:25 step:24284 [D loss: 0.681374, acc.: 51.56%] [G loss: 1.237170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24285 [D loss: 0.252946, acc.: 94.53%] [G loss: 2.043580]\n",
      "epoch:25 step:24286 [D loss: 0.152121, acc.: 97.66%] [G loss: 0.399313]\n",
      "epoch:25 step:24287 [D loss: 0.560649, acc.: 66.41%] [G loss: 1.687808]\n",
      "epoch:25 step:24288 [D loss: 0.424618, acc.: 76.56%] [G loss: 1.039758]\n",
      "epoch:25 step:24289 [D loss: 0.339963, acc.: 84.38%] [G loss: 2.061882]\n",
      "epoch:25 step:24290 [D loss: 0.215044, acc.: 95.31%] [G loss: 1.896313]\n",
      "epoch:25 step:24291 [D loss: 0.323182, acc.: 88.28%] [G loss: 2.414316]\n",
      "epoch:25 step:24292 [D loss: 0.294487, acc.: 92.19%] [G loss: 0.669569]\n",
      "epoch:25 step:24293 [D loss: 0.132977, acc.: 97.66%] [G loss: 2.798920]\n",
      "epoch:25 step:24294 [D loss: 0.181501, acc.: 98.44%] [G loss: 1.064054]\n",
      "epoch:25 step:24295 [D loss: 0.220918, acc.: 95.31%] [G loss: 1.094694]\n",
      "epoch:25 step:24296 [D loss: 0.782782, acc.: 53.12%] [G loss: 0.611159]\n",
      "epoch:25 step:24297 [D loss: 0.207216, acc.: 92.19%] [G loss: 0.593556]\n",
      "epoch:25 step:24298 [D loss: 0.689594, acc.: 60.94%] [G loss: 1.142811]\n",
      "epoch:25 step:24299 [D loss: 0.207465, acc.: 95.31%] [G loss: 0.919373]\n",
      "epoch:25 step:24300 [D loss: 0.646635, acc.: 64.84%] [G loss: 1.413399]\n",
      "epoch:25 step:24301 [D loss: 0.729667, acc.: 57.81%] [G loss: 0.687715]\n",
      "epoch:25 step:24302 [D loss: 0.091612, acc.: 98.44%] [G loss: 2.646583]\n",
      "epoch:25 step:24303 [D loss: 0.380045, acc.: 86.72%] [G loss: 0.442757]\n",
      "epoch:25 step:24304 [D loss: 0.719694, acc.: 57.03%] [G loss: 1.046406]\n",
      "epoch:25 step:24305 [D loss: 0.408924, acc.: 83.59%] [G loss: 3.488734]\n",
      "epoch:25 step:24306 [D loss: 0.359895, acc.: 83.59%] [G loss: 1.059119]\n",
      "epoch:25 step:24307 [D loss: 0.728877, acc.: 63.28%] [G loss: 1.081491]\n",
      "epoch:25 step:24308 [D loss: 0.338718, acc.: 87.50%] [G loss: 1.967121]\n",
      "epoch:25 step:24309 [D loss: 0.358229, acc.: 78.12%] [G loss: 1.320699]\n",
      "epoch:25 step:24310 [D loss: 0.358683, acc.: 82.03%] [G loss: 2.200243]\n",
      "epoch:25 step:24311 [D loss: 0.189441, acc.: 98.44%] [G loss: 1.111108]\n",
      "epoch:25 step:24312 [D loss: 0.125616, acc.: 97.66%] [G loss: 1.162439]\n",
      "epoch:25 step:24313 [D loss: 0.233890, acc.: 96.09%] [G loss: 0.520642]\n",
      "epoch:25 step:24314 [D loss: 0.899487, acc.: 52.34%] [G loss: 1.687022]\n",
      "epoch:25 step:24315 [D loss: 0.446310, acc.: 75.00%] [G loss: 0.838068]\n",
      "epoch:25 step:24316 [D loss: 0.445482, acc.: 82.81%] [G loss: 0.739972]\n",
      "epoch:25 step:24317 [D loss: 0.463086, acc.: 72.66%] [G loss: 1.816280]\n",
      "epoch:25 step:24318 [D loss: 0.150867, acc.: 96.09%] [G loss: 4.745394]\n",
      "epoch:25 step:24319 [D loss: 0.733831, acc.: 61.72%] [G loss: 4.580579]\n",
      "epoch:25 step:24320 [D loss: 0.187118, acc.: 92.97%] [G loss: 2.542778]\n",
      "epoch:25 step:24321 [D loss: 0.283361, acc.: 89.84%] [G loss: 1.475560]\n",
      "epoch:25 step:24322 [D loss: 0.534583, acc.: 72.66%] [G loss: 1.458532]\n",
      "epoch:25 step:24323 [D loss: 1.421896, acc.: 48.44%] [G loss: 0.774272]\n",
      "epoch:25 step:24324 [D loss: 0.538100, acc.: 68.75%] [G loss: 0.747530]\n",
      "epoch:25 step:24325 [D loss: 0.780805, acc.: 55.47%] [G loss: 2.256989]\n",
      "epoch:25 step:24326 [D loss: 0.188790, acc.: 98.44%] [G loss: 2.105775]\n",
      "epoch:25 step:24327 [D loss: 0.112998, acc.: 98.44%] [G loss: 4.599439]\n",
      "epoch:25 step:24328 [D loss: 0.307287, acc.: 85.16%] [G loss: 2.386624]\n",
      "epoch:25 step:24329 [D loss: 0.182179, acc.: 98.44%] [G loss: 0.696768]\n",
      "epoch:25 step:24330 [D loss: 0.676029, acc.: 64.06%] [G loss: 0.587145]\n",
      "epoch:25 step:24331 [D loss: 0.185524, acc.: 94.53%] [G loss: 1.294181]\n",
      "epoch:25 step:24332 [D loss: 0.166475, acc.: 96.09%] [G loss: 1.424798]\n",
      "epoch:25 step:24333 [D loss: 1.335780, acc.: 53.12%] [G loss: 0.808026]\n",
      "epoch:25 step:24334 [D loss: 0.291857, acc.: 92.97%] [G loss: 1.636184]\n",
      "epoch:25 step:24335 [D loss: 0.237976, acc.: 95.31%] [G loss: 3.555647]\n",
      "epoch:25 step:24336 [D loss: 0.477906, acc.: 78.12%] [G loss: 2.223581]\n",
      "epoch:25 step:24337 [D loss: 0.493772, acc.: 75.78%] [G loss: 1.616601]\n",
      "epoch:25 step:24338 [D loss: 0.444464, acc.: 82.81%] [G loss: 3.663559]\n",
      "epoch:25 step:24339 [D loss: 0.345268, acc.: 90.62%] [G loss: 2.069325]\n",
      "epoch:25 step:24340 [D loss: 0.652077, acc.: 63.28%] [G loss: 0.956192]\n",
      "epoch:25 step:24341 [D loss: 0.214546, acc.: 93.75%] [G loss: 3.866283]\n",
      "epoch:25 step:24342 [D loss: 0.323624, acc.: 88.28%] [G loss: 1.152120]\n",
      "epoch:25 step:24343 [D loss: 0.155579, acc.: 97.66%] [G loss: 2.576540]\n",
      "epoch:25 step:24344 [D loss: 0.459816, acc.: 75.00%] [G loss: 2.203384]\n",
      "epoch:25 step:24345 [D loss: 0.497271, acc.: 78.12%] [G loss: 2.150260]\n",
      "epoch:25 step:24346 [D loss: 0.881126, acc.: 48.44%] [G loss: 3.580064]\n",
      "epoch:25 step:24347 [D loss: 0.303994, acc.: 89.06%] [G loss: 3.142664]\n",
      "epoch:25 step:24348 [D loss: 0.724095, acc.: 61.72%] [G loss: 3.384626]\n",
      "epoch:25 step:24349 [D loss: 0.322829, acc.: 88.28%] [G loss: 1.173562]\n",
      "epoch:25 step:24350 [D loss: 0.385887, acc.: 85.94%] [G loss: 1.845724]\n",
      "epoch:25 step:24351 [D loss: 0.378773, acc.: 85.16%] [G loss: 1.484443]\n",
      "epoch:25 step:24352 [D loss: 0.239913, acc.: 91.41%] [G loss: 0.980569]\n",
      "epoch:25 step:24353 [D loss: 0.284036, acc.: 93.75%] [G loss: 2.840938]\n",
      "epoch:25 step:24354 [D loss: 0.258671, acc.: 94.53%] [G loss: 2.117259]\n",
      "epoch:25 step:24355 [D loss: 0.629616, acc.: 57.03%] [G loss: 2.187476]\n",
      "epoch:25 step:24356 [D loss: 0.221689, acc.: 92.19%] [G loss: 3.697507]\n",
      "epoch:25 step:24357 [D loss: 0.446971, acc.: 73.44%] [G loss: 3.234857]\n",
      "epoch:25 step:24358 [D loss: 0.482983, acc.: 79.69%] [G loss: 1.007593]\n",
      "epoch:25 step:24359 [D loss: 0.400113, acc.: 84.38%] [G loss: 3.174699]\n",
      "epoch:25 step:24360 [D loss: 0.381242, acc.: 79.69%] [G loss: 1.263491]\n",
      "epoch:25 step:24361 [D loss: 0.168906, acc.: 96.88%] [G loss: 4.018357]\n",
      "epoch:25 step:24362 [D loss: 0.575099, acc.: 65.62%] [G loss: 1.405150]\n",
      "epoch:26 step:24363 [D loss: 0.228553, acc.: 94.53%] [G loss: 3.014365]\n",
      "epoch:26 step:24364 [D loss: 0.490315, acc.: 74.22%] [G loss: 5.014904]\n",
      "epoch:26 step:24365 [D loss: 0.200997, acc.: 96.88%] [G loss: 2.775157]\n",
      "epoch:26 step:24366 [D loss: 1.466195, acc.: 47.66%] [G loss: 1.630534]\n",
      "epoch:26 step:24367 [D loss: 0.568775, acc.: 71.09%] [G loss: 1.554018]\n",
      "epoch:26 step:24368 [D loss: 0.758619, acc.: 56.25%] [G loss: 3.426760]\n",
      "epoch:26 step:24369 [D loss: 0.268370, acc.: 89.84%] [G loss: 2.443871]\n",
      "epoch:26 step:24370 [D loss: 0.341573, acc.: 88.28%] [G loss: 1.854047]\n",
      "epoch:26 step:24371 [D loss: 0.627632, acc.: 64.84%] [G loss: 3.038659]\n",
      "epoch:26 step:24372 [D loss: 0.199270, acc.: 92.97%] [G loss: 1.982677]\n",
      "epoch:26 step:24373 [D loss: 0.837000, acc.: 53.12%] [G loss: 2.265758]\n",
      "epoch:26 step:24374 [D loss: 0.344629, acc.: 85.16%] [G loss: 1.883330]\n",
      "epoch:26 step:24375 [D loss: 1.051543, acc.: 53.91%] [G loss: 2.287616]\n",
      "epoch:26 step:24376 [D loss: 0.209021, acc.: 96.88%] [G loss: 1.092089]\n",
      "epoch:26 step:24377 [D loss: 0.431992, acc.: 76.56%] [G loss: 3.540617]\n",
      "epoch:26 step:24378 [D loss: 0.281478, acc.: 92.19%] [G loss: 2.272091]\n",
      "epoch:26 step:24379 [D loss: 0.632159, acc.: 65.62%] [G loss: 1.042592]\n",
      "epoch:26 step:24380 [D loss: 0.194436, acc.: 96.09%] [G loss: 4.399871]\n",
      "epoch:26 step:24381 [D loss: 0.427599, acc.: 75.00%] [G loss: 1.095556]\n",
      "epoch:26 step:24382 [D loss: 0.299548, acc.: 89.06%] [G loss: 3.650137]\n",
      "epoch:26 step:24383 [D loss: 0.157693, acc.: 96.09%] [G loss: 2.759998]\n",
      "epoch:26 step:24384 [D loss: 0.983150, acc.: 44.53%] [G loss: 2.401078]\n",
      "epoch:26 step:24385 [D loss: 0.503633, acc.: 72.66%] [G loss: 1.755138]\n",
      "epoch:26 step:24386 [D loss: 0.210229, acc.: 94.53%] [G loss: 2.320858]\n",
      "epoch:26 step:24387 [D loss: 1.048062, acc.: 44.53%] [G loss: 0.597082]\n",
      "epoch:26 step:24388 [D loss: 0.723744, acc.: 59.38%] [G loss: 1.941658]\n",
      "epoch:26 step:24389 [D loss: 0.394530, acc.: 85.94%] [G loss: 3.311431]\n",
      "epoch:26 step:24390 [D loss: 0.455086, acc.: 82.81%] [G loss: 1.807312]\n",
      "epoch:26 step:24391 [D loss: 0.302879, acc.: 85.94%] [G loss: 1.892105]\n",
      "epoch:26 step:24392 [D loss: 0.309004, acc.: 93.75%] [G loss: 1.007611]\n",
      "epoch:26 step:24393 [D loss: 0.754957, acc.: 55.47%] [G loss: 1.890700]\n",
      "epoch:26 step:24394 [D loss: 0.308768, acc.: 90.62%] [G loss: 1.432890]\n",
      "epoch:26 step:24395 [D loss: 0.146782, acc.: 98.44%] [G loss: 2.697584]\n",
      "epoch:26 step:24396 [D loss: 0.278172, acc.: 85.94%] [G loss: 2.620263]\n",
      "epoch:26 step:24397 [D loss: 0.752949, acc.: 57.81%] [G loss: 1.055043]\n",
      "epoch:26 step:24398 [D loss: 1.012118, acc.: 50.00%] [G loss: 2.268869]\n",
      "epoch:26 step:24399 [D loss: 0.627425, acc.: 61.72%] [G loss: 1.327868]\n",
      "epoch:26 step:24400 [D loss: 0.715869, acc.: 64.06%] [G loss: 3.209302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24401 [D loss: 0.185331, acc.: 92.97%] [G loss: 2.430186]\n",
      "epoch:26 step:24402 [D loss: 0.551872, acc.: 72.66%] [G loss: 1.978945]\n",
      "epoch:26 step:24403 [D loss: 0.571570, acc.: 71.88%] [G loss: 0.588166]\n",
      "epoch:26 step:24404 [D loss: 0.849188, acc.: 46.09%] [G loss: 2.985379]\n",
      "epoch:26 step:24405 [D loss: 0.221687, acc.: 93.75%] [G loss: 1.679577]\n",
      "epoch:26 step:24406 [D loss: 0.641297, acc.: 63.28%] [G loss: 2.637995]\n",
      "epoch:26 step:24407 [D loss: 0.727791, acc.: 58.59%] [G loss: 1.768156]\n",
      "epoch:26 step:24408 [D loss: 0.489911, acc.: 67.19%] [G loss: 2.906075]\n",
      "epoch:26 step:24409 [D loss: 0.954438, acc.: 54.69%] [G loss: 2.429477]\n",
      "epoch:26 step:24410 [D loss: 0.445834, acc.: 82.03%] [G loss: 1.270770]\n",
      "epoch:26 step:24411 [D loss: 0.279232, acc.: 92.19%] [G loss: 2.699425]\n",
      "epoch:26 step:24412 [D loss: 0.432753, acc.: 75.00%] [G loss: 4.152254]\n",
      "epoch:26 step:24413 [D loss: 0.368626, acc.: 89.06%] [G loss: 2.112199]\n",
      "epoch:26 step:24414 [D loss: 0.187614, acc.: 97.66%] [G loss: 1.655130]\n",
      "epoch:26 step:24415 [D loss: 0.462954, acc.: 77.34%] [G loss: 1.750757]\n",
      "epoch:26 step:24416 [D loss: 0.114223, acc.: 99.22%] [G loss: 2.705660]\n",
      "epoch:26 step:24417 [D loss: 0.732082, acc.: 60.16%] [G loss: 2.087636]\n",
      "epoch:26 step:24418 [D loss: 0.558284, acc.: 72.66%] [G loss: 2.972904]\n",
      "epoch:26 step:24419 [D loss: 0.383368, acc.: 85.16%] [G loss: 2.409681]\n",
      "epoch:26 step:24420 [D loss: 0.215346, acc.: 95.31%] [G loss: 2.476749]\n",
      "epoch:26 step:24421 [D loss: 0.204689, acc.: 96.09%] [G loss: 2.100502]\n",
      "epoch:26 step:24422 [D loss: 0.617199, acc.: 60.94%] [G loss: 0.917139]\n",
      "epoch:26 step:24423 [D loss: 0.389097, acc.: 75.78%] [G loss: 2.706870]\n",
      "epoch:26 step:24424 [D loss: 0.241644, acc.: 96.88%] [G loss: 2.392652]\n",
      "epoch:26 step:24425 [D loss: 0.199876, acc.: 96.88%] [G loss: 3.479437]\n",
      "epoch:26 step:24426 [D loss: 0.323363, acc.: 92.19%] [G loss: 1.399410]\n",
      "epoch:26 step:24427 [D loss: 0.923304, acc.: 32.81%] [G loss: 1.595888]\n",
      "epoch:26 step:24428 [D loss: 0.795972, acc.: 46.09%] [G loss: 2.542605]\n",
      "epoch:26 step:24429 [D loss: 0.176514, acc.: 98.44%] [G loss: 2.506559]\n",
      "epoch:26 step:24430 [D loss: 0.634362, acc.: 60.16%] [G loss: 2.558840]\n",
      "epoch:26 step:24431 [D loss: 0.198776, acc.: 97.66%] [G loss: 1.995326]\n",
      "epoch:26 step:24432 [D loss: 0.857761, acc.: 39.06%] [G loss: 2.427590]\n",
      "epoch:26 step:24433 [D loss: 0.514433, acc.: 71.09%] [G loss: 4.003671]\n",
      "epoch:26 step:24434 [D loss: 0.451372, acc.: 78.12%] [G loss: 1.933805]\n",
      "epoch:26 step:24435 [D loss: 0.336257, acc.: 89.84%] [G loss: 1.616025]\n",
      "epoch:26 step:24436 [D loss: 0.482026, acc.: 72.66%] [G loss: 1.435280]\n",
      "epoch:26 step:24437 [D loss: 0.408933, acc.: 79.69%] [G loss: 2.028269]\n",
      "epoch:26 step:24438 [D loss: 0.554725, acc.: 72.66%] [G loss: 1.530368]\n",
      "epoch:26 step:24439 [D loss: 0.116654, acc.: 98.44%] [G loss: 2.472669]\n",
      "epoch:26 step:24440 [D loss: 0.195194, acc.: 96.09%] [G loss: 2.365401]\n",
      "epoch:26 step:24441 [D loss: 0.252375, acc.: 96.09%] [G loss: 2.042652]\n",
      "epoch:26 step:24442 [D loss: 0.466676, acc.: 78.12%] [G loss: 2.199948]\n",
      "epoch:26 step:24443 [D loss: 0.170109, acc.: 98.44%] [G loss: 2.219972]\n",
      "epoch:26 step:24444 [D loss: 0.403065, acc.: 85.94%] [G loss: 2.287536]\n",
      "epoch:26 step:24445 [D loss: 0.219269, acc.: 96.88%] [G loss: 1.834608]\n",
      "epoch:26 step:24446 [D loss: 0.190577, acc.: 96.88%] [G loss: 3.435613]\n",
      "epoch:26 step:24447 [D loss: 0.192639, acc.: 96.09%] [G loss: 1.286852]\n",
      "epoch:26 step:24448 [D loss: 0.126392, acc.: 98.44%] [G loss: 3.773566]\n",
      "epoch:26 step:24449 [D loss: 0.195669, acc.: 97.66%] [G loss: 2.030419]\n",
      "epoch:26 step:24450 [D loss: 0.644899, acc.: 63.28%] [G loss: 4.174397]\n",
      "epoch:26 step:24451 [D loss: 0.079934, acc.: 98.44%] [G loss: 3.661886]\n",
      "epoch:26 step:24452 [D loss: 0.704090, acc.: 57.81%] [G loss: 1.893481]\n",
      "epoch:26 step:24453 [D loss: 0.364942, acc.: 84.38%] [G loss: 3.706995]\n",
      "epoch:26 step:24454 [D loss: 0.238861, acc.: 94.53%] [G loss: 1.715880]\n",
      "epoch:26 step:24455 [D loss: 0.326246, acc.: 89.06%] [G loss: 0.688254]\n",
      "epoch:26 step:24456 [D loss: 0.246323, acc.: 94.53%] [G loss: 1.615902]\n",
      "epoch:26 step:24457 [D loss: 0.748163, acc.: 59.38%] [G loss: 1.730094]\n",
      "epoch:26 step:24458 [D loss: 0.190730, acc.: 98.44%] [G loss: 3.021792]\n",
      "epoch:26 step:24459 [D loss: 0.086109, acc.: 99.22%] [G loss: 3.047510]\n",
      "epoch:26 step:24460 [D loss: 0.685549, acc.: 64.84%] [G loss: 2.813210]\n",
      "epoch:26 step:24461 [D loss: 0.310363, acc.: 88.28%] [G loss: 2.475452]\n",
      "epoch:26 step:24462 [D loss: 0.523720, acc.: 79.69%] [G loss: 3.061049]\n",
      "epoch:26 step:24463 [D loss: 0.996968, acc.: 48.44%] [G loss: 2.015212]\n",
      "epoch:26 step:24464 [D loss: 0.299339, acc.: 92.19%] [G loss: 0.672217]\n",
      "epoch:26 step:24465 [D loss: 0.552567, acc.: 67.19%] [G loss: 2.356564]\n",
      "epoch:26 step:24466 [D loss: 0.396025, acc.: 85.94%] [G loss: 2.036730]\n",
      "epoch:26 step:24467 [D loss: 0.371725, acc.: 80.47%] [G loss: 1.623934]\n",
      "epoch:26 step:24468 [D loss: 0.345971, acc.: 86.72%] [G loss: 2.774043]\n",
      "epoch:26 step:24469 [D loss: 0.144753, acc.: 99.22%] [G loss: 3.475314]\n",
      "epoch:26 step:24470 [D loss: 0.412467, acc.: 83.59%] [G loss: 2.341631]\n",
      "epoch:26 step:24471 [D loss: 0.240074, acc.: 93.75%] [G loss: 2.064272]\n",
      "epoch:26 step:24472 [D loss: 0.431810, acc.: 82.81%] [G loss: 3.164835]\n",
      "epoch:26 step:24473 [D loss: 0.324205, acc.: 85.94%] [G loss: 4.597506]\n",
      "epoch:26 step:24474 [D loss: 0.415767, acc.: 80.47%] [G loss: 2.704629]\n",
      "epoch:26 step:24475 [D loss: 0.355334, acc.: 85.16%] [G loss: 1.960013]\n",
      "epoch:26 step:24476 [D loss: 1.531601, acc.: 32.03%] [G loss: 0.795349]\n",
      "epoch:26 step:24477 [D loss: 0.364651, acc.: 81.25%] [G loss: 1.689190]\n",
      "epoch:26 step:24478 [D loss: 0.191995, acc.: 93.75%] [G loss: 1.987827]\n",
      "epoch:26 step:24479 [D loss: 0.272289, acc.: 89.06%] [G loss: 2.994287]\n",
      "epoch:26 step:24480 [D loss: 0.346451, acc.: 86.72%] [G loss: 1.875712]\n",
      "epoch:26 step:24481 [D loss: 0.228060, acc.: 94.53%] [G loss: 3.411809]\n",
      "epoch:26 step:24482 [D loss: 0.558817, acc.: 71.09%] [G loss: 1.376923]\n",
      "epoch:26 step:24483 [D loss: 0.273113, acc.: 91.41%] [G loss: 1.668828]\n",
      "epoch:26 step:24484 [D loss: 0.459443, acc.: 73.44%] [G loss: 2.726317]\n",
      "epoch:26 step:24485 [D loss: 0.068377, acc.: 99.22%] [G loss: 2.529033]\n",
      "epoch:26 step:24486 [D loss: 0.911861, acc.: 40.62%] [G loss: 1.032089]\n",
      "epoch:26 step:24487 [D loss: 0.141055, acc.: 96.88%] [G loss: 2.573303]\n",
      "epoch:26 step:24488 [D loss: 0.718479, acc.: 59.38%] [G loss: 2.191607]\n",
      "epoch:26 step:24489 [D loss: 0.220947, acc.: 95.31%] [G loss: 2.173739]\n",
      "epoch:26 step:24490 [D loss: 0.532387, acc.: 70.31%] [G loss: 2.054008]\n",
      "epoch:26 step:24491 [D loss: 0.727663, acc.: 64.06%] [G loss: 2.943883]\n",
      "epoch:26 step:24492 [D loss: 0.333657, acc.: 85.94%] [G loss: 1.604961]\n",
      "epoch:26 step:24493 [D loss: 0.228962, acc.: 95.31%] [G loss: 2.677661]\n",
      "epoch:26 step:24494 [D loss: 0.222618, acc.: 92.97%] [G loss: 2.141521]\n",
      "epoch:26 step:24495 [D loss: 0.301163, acc.: 92.97%] [G loss: 1.438197]\n",
      "epoch:26 step:24496 [D loss: 0.353269, acc.: 85.16%] [G loss: 1.508019]\n",
      "epoch:26 step:24497 [D loss: 0.755210, acc.: 57.81%] [G loss: 3.269066]\n",
      "epoch:26 step:24498 [D loss: 0.277729, acc.: 91.41%] [G loss: 1.949520]\n",
      "epoch:26 step:24499 [D loss: 0.391518, acc.: 83.59%] [G loss: 1.364084]\n",
      "epoch:26 step:24500 [D loss: 0.294329, acc.: 92.19%] [G loss: 0.961920]\n",
      "epoch:26 step:24501 [D loss: 0.149481, acc.: 96.09%] [G loss: 3.104532]\n",
      "epoch:26 step:24502 [D loss: 0.581724, acc.: 66.41%] [G loss: 2.664395]\n",
      "epoch:26 step:24503 [D loss: 0.569208, acc.: 65.62%] [G loss: 1.317018]\n",
      "epoch:26 step:24504 [D loss: 0.180998, acc.: 97.66%] [G loss: 2.056770]\n",
      "epoch:26 step:24505 [D loss: 1.057848, acc.: 50.00%] [G loss: 3.739222]\n",
      "epoch:26 step:24506 [D loss: 0.252462, acc.: 91.41%] [G loss: 3.229646]\n",
      "epoch:26 step:24507 [D loss: 0.796512, acc.: 51.56%] [G loss: 2.833151]\n",
      "epoch:26 step:24508 [D loss: 0.397260, acc.: 82.03%] [G loss: 1.853508]\n",
      "epoch:26 step:24509 [D loss: 0.555946, acc.: 68.75%] [G loss: 1.102239]\n",
      "epoch:26 step:24510 [D loss: 0.757312, acc.: 53.91%] [G loss: 1.733732]\n",
      "epoch:26 step:24511 [D loss: 0.642660, acc.: 61.72%] [G loss: 2.437514]\n",
      "epoch:26 step:24512 [D loss: 0.329064, acc.: 85.94%] [G loss: 0.862621]\n",
      "epoch:26 step:24513 [D loss: 0.737148, acc.: 68.75%] [G loss: 1.265821]\n",
      "epoch:26 step:24514 [D loss: 0.480984, acc.: 74.22%] [G loss: 1.623772]\n",
      "epoch:26 step:24515 [D loss: 0.313920, acc.: 89.84%] [G loss: 3.032645]\n",
      "epoch:26 step:24516 [D loss: 0.140832, acc.: 96.88%] [G loss: 1.879556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24517 [D loss: 0.212116, acc.: 95.31%] [G loss: 0.786551]\n",
      "epoch:26 step:24518 [D loss: 0.195899, acc.: 96.09%] [G loss: 2.228129]\n",
      "epoch:26 step:24519 [D loss: 0.413276, acc.: 82.81%] [G loss: 1.503181]\n",
      "epoch:26 step:24520 [D loss: 0.233218, acc.: 95.31%] [G loss: 3.355330]\n",
      "epoch:26 step:24521 [D loss: 0.520053, acc.: 67.19%] [G loss: 2.345272]\n",
      "epoch:26 step:24522 [D loss: 0.602551, acc.: 64.06%] [G loss: 1.793645]\n",
      "epoch:26 step:24523 [D loss: 0.506629, acc.: 77.34%] [G loss: 1.156923]\n",
      "epoch:26 step:24524 [D loss: 0.094967, acc.: 98.44%] [G loss: 2.722978]\n",
      "epoch:26 step:24525 [D loss: 0.673951, acc.: 62.50%] [G loss: 2.775093]\n",
      "epoch:26 step:24526 [D loss: 0.221463, acc.: 94.53%] [G loss: 3.616073]\n",
      "epoch:26 step:24527 [D loss: 0.254842, acc.: 92.19%] [G loss: 3.704981]\n",
      "epoch:26 step:24528 [D loss: 0.748208, acc.: 57.81%] [G loss: 1.278062]\n",
      "epoch:26 step:24529 [D loss: 0.327668, acc.: 91.41%] [G loss: 0.927118]\n",
      "epoch:26 step:24530 [D loss: 0.191110, acc.: 97.66%] [G loss: 0.923477]\n",
      "epoch:26 step:24531 [D loss: 0.438391, acc.: 74.22%] [G loss: 4.019946]\n",
      "epoch:26 step:24532 [D loss: 0.262031, acc.: 96.09%] [G loss: 2.259695]\n",
      "epoch:26 step:24533 [D loss: 0.515084, acc.: 78.91%] [G loss: 2.315974]\n",
      "epoch:26 step:24534 [D loss: 0.486384, acc.: 76.56%] [G loss: 2.386938]\n",
      "epoch:26 step:24535 [D loss: 0.649068, acc.: 62.50%] [G loss: 1.232331]\n",
      "epoch:26 step:24536 [D loss: 0.499618, acc.: 78.12%] [G loss: 1.415621]\n",
      "epoch:26 step:24537 [D loss: 0.218606, acc.: 93.75%] [G loss: 1.322699]\n",
      "epoch:26 step:24538 [D loss: 0.311206, acc.: 94.53%] [G loss: 0.975235]\n",
      "epoch:26 step:24539 [D loss: 0.531352, acc.: 67.19%] [G loss: 1.425247]\n",
      "epoch:26 step:24540 [D loss: 0.610536, acc.: 71.88%] [G loss: 2.278592]\n",
      "epoch:26 step:24541 [D loss: 0.085319, acc.: 99.22%] [G loss: 3.553699]\n",
      "epoch:26 step:24542 [D loss: 0.140699, acc.: 99.22%] [G loss: 2.738712]\n",
      "epoch:26 step:24543 [D loss: 0.574935, acc.: 63.28%] [G loss: 3.352201]\n",
      "epoch:26 step:24544 [D loss: 0.139985, acc.: 98.44%] [G loss: 0.936560]\n",
      "epoch:26 step:24545 [D loss: 1.037423, acc.: 55.47%] [G loss: 2.819041]\n",
      "epoch:26 step:24546 [D loss: 0.930044, acc.: 49.22%] [G loss: 0.889399]\n",
      "epoch:26 step:24547 [D loss: 0.352607, acc.: 85.16%] [G loss: 1.008085]\n",
      "epoch:26 step:24548 [D loss: 0.119785, acc.: 97.66%] [G loss: 3.173432]\n",
      "epoch:26 step:24549 [D loss: 0.821203, acc.: 42.97%] [G loss: 3.192358]\n",
      "epoch:26 step:24550 [D loss: 0.326070, acc.: 92.19%] [G loss: 3.609051]\n",
      "epoch:26 step:24551 [D loss: 0.689633, acc.: 56.25%] [G loss: 2.298751]\n",
      "epoch:26 step:24552 [D loss: 0.211588, acc.: 97.66%] [G loss: 3.807879]\n",
      "epoch:26 step:24553 [D loss: 0.530988, acc.: 71.09%] [G loss: 2.805694]\n",
      "epoch:26 step:24554 [D loss: 0.177981, acc.: 96.88%] [G loss: 2.750472]\n",
      "epoch:26 step:24555 [D loss: 0.107194, acc.: 98.44%] [G loss: 4.521306]\n",
      "epoch:26 step:24556 [D loss: 0.331520, acc.: 86.72%] [G loss: 1.406913]\n",
      "epoch:26 step:24557 [D loss: 0.417620, acc.: 78.91%] [G loss: 3.066622]\n",
      "epoch:26 step:24558 [D loss: 0.508286, acc.: 71.09%] [G loss: 1.572175]\n",
      "epoch:26 step:24559 [D loss: 0.210105, acc.: 95.31%] [G loss: 2.038129]\n",
      "epoch:26 step:24560 [D loss: 0.643827, acc.: 61.72%] [G loss: 2.710728]\n",
      "epoch:26 step:24561 [D loss: 0.288968, acc.: 85.94%] [G loss: 2.637805]\n",
      "epoch:26 step:24562 [D loss: 0.142206, acc.: 97.66%] [G loss: 3.352755]\n",
      "epoch:26 step:24563 [D loss: 0.278576, acc.: 92.19%] [G loss: 4.380291]\n",
      "epoch:26 step:24564 [D loss: 0.428224, acc.: 75.00%] [G loss: 1.231207]\n",
      "epoch:26 step:24565 [D loss: 0.396784, acc.: 89.06%] [G loss: 0.909739]\n",
      "epoch:26 step:24566 [D loss: 0.387963, acc.: 83.59%] [G loss: 0.691193]\n",
      "epoch:26 step:24567 [D loss: 0.106851, acc.: 97.66%] [G loss: 2.910431]\n",
      "epoch:26 step:24568 [D loss: 0.142712, acc.: 98.44%] [G loss: 1.017910]\n",
      "epoch:26 step:24569 [D loss: 0.681365, acc.: 60.16%] [G loss: 0.510293]\n",
      "epoch:26 step:24570 [D loss: 0.905496, acc.: 57.03%] [G loss: 1.847549]\n",
      "epoch:26 step:24571 [D loss: 0.312001, acc.: 90.62%] [G loss: 1.433596]\n",
      "epoch:26 step:24572 [D loss: 0.089709, acc.: 99.22%] [G loss: 2.620581]\n",
      "epoch:26 step:24573 [D loss: 0.278088, acc.: 87.50%] [G loss: 1.840528]\n",
      "epoch:26 step:24574 [D loss: 0.627599, acc.: 63.28%] [G loss: 1.021500]\n",
      "epoch:26 step:24575 [D loss: 0.601077, acc.: 61.72%] [G loss: 2.836890]\n",
      "epoch:26 step:24576 [D loss: 0.240259, acc.: 94.53%] [G loss: 1.215956]\n",
      "epoch:26 step:24577 [D loss: 0.048153, acc.: 100.00%] [G loss: 2.218222]\n",
      "epoch:26 step:24578 [D loss: 0.245563, acc.: 96.88%] [G loss: 3.889375]\n",
      "epoch:26 step:24579 [D loss: 0.210803, acc.: 96.09%] [G loss: 2.778099]\n",
      "epoch:26 step:24580 [D loss: 0.403986, acc.: 78.91%] [G loss: 1.950263]\n",
      "epoch:26 step:24581 [D loss: 0.318578, acc.: 88.28%] [G loss: 4.213673]\n",
      "epoch:26 step:24582 [D loss: 0.672055, acc.: 63.28%] [G loss: 4.358589]\n",
      "epoch:26 step:24583 [D loss: 0.608036, acc.: 63.28%] [G loss: 1.878891]\n",
      "epoch:26 step:24584 [D loss: 0.234869, acc.: 93.75%] [G loss: 1.614030]\n",
      "epoch:26 step:24585 [D loss: 0.126855, acc.: 96.88%] [G loss: 0.870828]\n",
      "epoch:26 step:24586 [D loss: 0.084380, acc.: 100.00%] [G loss: 3.220729]\n",
      "epoch:26 step:24587 [D loss: 0.130046, acc.: 97.66%] [G loss: 0.618464]\n",
      "epoch:26 step:24588 [D loss: 0.181750, acc.: 95.31%] [G loss: 1.769206]\n",
      "epoch:26 step:24589 [D loss: 0.600672, acc.: 64.84%] [G loss: 2.695497]\n",
      "epoch:26 step:24590 [D loss: 0.231498, acc.: 92.19%] [G loss: 1.202597]\n",
      "epoch:26 step:24591 [D loss: 0.182769, acc.: 94.53%] [G loss: 4.186596]\n",
      "epoch:26 step:24592 [D loss: 0.394733, acc.: 92.19%] [G loss: 1.098090]\n",
      "epoch:26 step:24593 [D loss: 0.200029, acc.: 92.97%] [G loss: 3.665812]\n",
      "epoch:26 step:24594 [D loss: 0.104459, acc.: 98.44%] [G loss: 1.994932]\n",
      "epoch:26 step:24595 [D loss: 0.323554, acc.: 89.84%] [G loss: 1.846510]\n",
      "epoch:26 step:24596 [D loss: 0.255370, acc.: 93.75%] [G loss: 3.268174]\n",
      "epoch:26 step:24597 [D loss: 0.360445, acc.: 90.62%] [G loss: 2.504959]\n",
      "epoch:26 step:24598 [D loss: 0.437037, acc.: 80.47%] [G loss: 1.100812]\n",
      "epoch:26 step:24599 [D loss: 0.263788, acc.: 96.09%] [G loss: 3.976057]\n",
      "epoch:26 step:24600 [D loss: 0.737489, acc.: 55.47%] [G loss: 2.883440]\n",
      "epoch:26 step:24601 [D loss: 0.828480, acc.: 53.12%] [G loss: 4.598608]\n",
      "epoch:26 step:24602 [D loss: 0.221428, acc.: 92.97%] [G loss: 3.063218]\n",
      "epoch:26 step:24603 [D loss: 0.425521, acc.: 75.78%] [G loss: 2.901941]\n",
      "epoch:26 step:24604 [D loss: 0.503254, acc.: 70.31%] [G loss: 1.603918]\n",
      "epoch:26 step:24605 [D loss: 0.365980, acc.: 80.47%] [G loss: 0.650007]\n",
      "epoch:26 step:24606 [D loss: 0.155582, acc.: 96.88%] [G loss: 4.475084]\n",
      "epoch:26 step:24607 [D loss: 0.112080, acc.: 99.22%] [G loss: 3.716133]\n",
      "epoch:26 step:24608 [D loss: 0.180456, acc.: 96.09%] [G loss: 2.636259]\n",
      "epoch:26 step:24609 [D loss: 0.719472, acc.: 61.72%] [G loss: 1.345219]\n",
      "epoch:26 step:24610 [D loss: 1.063910, acc.: 57.03%] [G loss: 1.512596]\n",
      "epoch:26 step:24611 [D loss: 0.121604, acc.: 97.66%] [G loss: 1.716384]\n",
      "epoch:26 step:24612 [D loss: 0.084650, acc.: 100.00%] [G loss: 4.770148]\n",
      "epoch:26 step:24613 [D loss: 0.653896, acc.: 65.62%] [G loss: 3.200815]\n",
      "epoch:26 step:24614 [D loss: 0.121892, acc.: 97.66%] [G loss: 2.383141]\n",
      "epoch:26 step:24615 [D loss: 0.815494, acc.: 53.12%] [G loss: 0.955813]\n",
      "epoch:26 step:24616 [D loss: 0.337387, acc.: 90.62%] [G loss: 3.249453]\n",
      "epoch:26 step:24617 [D loss: 0.372710, acc.: 85.16%] [G loss: 0.776735]\n",
      "epoch:26 step:24618 [D loss: 0.153018, acc.: 96.09%] [G loss: 2.377856]\n",
      "epoch:26 step:24619 [D loss: 0.376151, acc.: 93.75%] [G loss: 0.808777]\n",
      "epoch:26 step:24620 [D loss: 0.521011, acc.: 71.88%] [G loss: 3.757216]\n",
      "epoch:26 step:24621 [D loss: 0.584827, acc.: 65.62%] [G loss: 3.110072]\n",
      "epoch:26 step:24622 [D loss: 0.447799, acc.: 75.78%] [G loss: 2.443367]\n",
      "epoch:26 step:24623 [D loss: 0.495188, acc.: 73.44%] [G loss: 2.351005]\n",
      "epoch:26 step:24624 [D loss: 0.377777, acc.: 85.94%] [G loss: 1.732139]\n",
      "epoch:26 step:24625 [D loss: 0.226406, acc.: 95.31%] [G loss: 2.692598]\n",
      "epoch:26 step:24626 [D loss: 0.262545, acc.: 97.66%] [G loss: 2.561269]\n",
      "epoch:26 step:24627 [D loss: 0.120345, acc.: 99.22%] [G loss: 4.279121]\n",
      "epoch:26 step:24628 [D loss: 0.362047, acc.: 80.47%] [G loss: 1.466661]\n",
      "epoch:26 step:24629 [D loss: 0.666631, acc.: 64.84%] [G loss: 1.564468]\n",
      "epoch:26 step:24630 [D loss: 0.785552, acc.: 55.47%] [G loss: 1.624955]\n",
      "epoch:26 step:24631 [D loss: 0.130457, acc.: 96.88%] [G loss: 1.128444]\n",
      "epoch:26 step:24632 [D loss: 0.145057, acc.: 96.88%] [G loss: 3.348513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24633 [D loss: 1.258375, acc.: 24.22%] [G loss: 2.478625]\n",
      "epoch:26 step:24634 [D loss: 1.032280, acc.: 39.06%] [G loss: 1.462005]\n",
      "epoch:26 step:24635 [D loss: 0.377214, acc.: 85.94%] [G loss: 1.123768]\n",
      "epoch:26 step:24636 [D loss: 0.329660, acc.: 89.06%] [G loss: 3.489365]\n",
      "epoch:26 step:24637 [D loss: 0.252737, acc.: 91.41%] [G loss: 1.116391]\n",
      "epoch:26 step:24638 [D loss: 0.267629, acc.: 96.09%] [G loss: 2.051544]\n",
      "epoch:26 step:24639 [D loss: 0.184762, acc.: 94.53%] [G loss: 1.314536]\n",
      "epoch:26 step:24640 [D loss: 0.352573, acc.: 90.62%] [G loss: 1.579793]\n",
      "epoch:26 step:24641 [D loss: 0.157342, acc.: 96.88%] [G loss: 2.850043]\n",
      "epoch:26 step:24642 [D loss: 0.124635, acc.: 99.22%] [G loss: 2.252215]\n",
      "epoch:26 step:24643 [D loss: 0.147423, acc.: 97.66%] [G loss: 2.756000]\n",
      "epoch:26 step:24644 [D loss: 0.627109, acc.: 59.38%] [G loss: 2.806204]\n",
      "epoch:26 step:24645 [D loss: 0.088629, acc.: 99.22%] [G loss: 2.299043]\n",
      "epoch:26 step:24646 [D loss: 0.447588, acc.: 80.47%] [G loss: 1.640963]\n",
      "epoch:26 step:24647 [D loss: 0.274547, acc.: 92.97%] [G loss: 3.057931]\n",
      "epoch:26 step:24648 [D loss: 0.043461, acc.: 99.22%] [G loss: 2.281504]\n",
      "epoch:26 step:24649 [D loss: 0.160517, acc.: 96.88%] [G loss: 2.634674]\n",
      "epoch:26 step:24650 [D loss: 0.508647, acc.: 67.19%] [G loss: 2.130306]\n",
      "epoch:26 step:24651 [D loss: 0.762273, acc.: 60.16%] [G loss: 1.251376]\n",
      "epoch:26 step:24652 [D loss: 0.132415, acc.: 96.88%] [G loss: 2.679481]\n",
      "epoch:26 step:24653 [D loss: 0.251047, acc.: 91.41%] [G loss: 2.381522]\n",
      "epoch:26 step:24654 [D loss: 0.478029, acc.: 78.12%] [G loss: 3.631749]\n",
      "epoch:26 step:24655 [D loss: 0.536777, acc.: 74.22%] [G loss: 0.905671]\n",
      "epoch:26 step:24656 [D loss: 0.189032, acc.: 96.09%] [G loss: 1.269159]\n",
      "epoch:26 step:24657 [D loss: 0.750254, acc.: 52.34%] [G loss: 2.804485]\n",
      "epoch:26 step:24658 [D loss: 0.375791, acc.: 86.72%] [G loss: 2.108458]\n",
      "epoch:26 step:24659 [D loss: 0.969320, acc.: 35.94%] [G loss: 1.277099]\n",
      "epoch:26 step:24660 [D loss: 0.421402, acc.: 76.56%] [G loss: 0.792353]\n",
      "epoch:26 step:24661 [D loss: 0.226507, acc.: 96.88%] [G loss: 0.951973]\n",
      "epoch:26 step:24662 [D loss: 1.072696, acc.: 34.38%] [G loss: 0.615928]\n",
      "epoch:26 step:24663 [D loss: 0.235368, acc.: 95.31%] [G loss: 1.374502]\n",
      "epoch:26 step:24664 [D loss: 0.339688, acc.: 82.81%] [G loss: 2.595147]\n",
      "epoch:26 step:24665 [D loss: 0.263900, acc.: 95.31%] [G loss: 2.861323]\n",
      "epoch:26 step:24666 [D loss: 0.592312, acc.: 70.31%] [G loss: 2.428123]\n",
      "epoch:26 step:24667 [D loss: 0.639277, acc.: 58.59%] [G loss: 1.319226]\n",
      "epoch:26 step:24668 [D loss: 0.339939, acc.: 83.59%] [G loss: 0.923960]\n",
      "epoch:26 step:24669 [D loss: 0.500911, acc.: 73.44%] [G loss: 3.646899]\n",
      "epoch:26 step:24670 [D loss: 0.613432, acc.: 67.19%] [G loss: 1.263049]\n",
      "epoch:26 step:24671 [D loss: 0.566992, acc.: 71.09%] [G loss: 4.087318]\n",
      "epoch:26 step:24672 [D loss: 0.123501, acc.: 97.66%] [G loss: 4.713062]\n",
      "epoch:26 step:24673 [D loss: 0.199610, acc.: 98.44%] [G loss: 2.733506]\n",
      "epoch:26 step:24674 [D loss: 0.300825, acc.: 87.50%] [G loss: 2.237561]\n",
      "epoch:26 step:24675 [D loss: 0.388238, acc.: 87.50%] [G loss: 2.947024]\n",
      "epoch:26 step:24676 [D loss: 0.305081, acc.: 92.19%] [G loss: 2.371820]\n",
      "epoch:26 step:24677 [D loss: 0.580921, acc.: 66.41%] [G loss: 2.206278]\n",
      "epoch:26 step:24678 [D loss: 0.278449, acc.: 86.72%] [G loss: 3.012294]\n",
      "epoch:26 step:24679 [D loss: 0.291304, acc.: 93.75%] [G loss: 1.335850]\n",
      "epoch:26 step:24680 [D loss: 0.633582, acc.: 62.50%] [G loss: 1.143348]\n",
      "epoch:26 step:24681 [D loss: 1.531312, acc.: 32.81%] [G loss: 1.445791]\n",
      "epoch:26 step:24682 [D loss: 0.227836, acc.: 92.97%] [G loss: 3.309595]\n",
      "epoch:26 step:24683 [D loss: 0.160066, acc.: 95.31%] [G loss: 1.737959]\n",
      "epoch:26 step:24684 [D loss: 0.180739, acc.: 96.09%] [G loss: 2.059778]\n",
      "epoch:26 step:24685 [D loss: 0.114461, acc.: 97.66%] [G loss: 1.600470]\n",
      "epoch:26 step:24686 [D loss: 0.634381, acc.: 65.62%] [G loss: 3.146348]\n",
      "epoch:26 step:24687 [D loss: 0.379437, acc.: 89.06%] [G loss: 2.277450]\n",
      "epoch:26 step:24688 [D loss: 0.674506, acc.: 69.53%] [G loss: 1.420360]\n",
      "epoch:26 step:24689 [D loss: 0.206562, acc.: 93.75%] [G loss: 2.963562]\n",
      "epoch:26 step:24690 [D loss: 0.499505, acc.: 73.44%] [G loss: 4.881794]\n",
      "epoch:26 step:24691 [D loss: 1.026965, acc.: 42.97%] [G loss: 0.558429]\n",
      "epoch:26 step:24692 [D loss: 0.097710, acc.: 99.22%] [G loss: 1.658336]\n",
      "epoch:26 step:24693 [D loss: 0.711512, acc.: 62.50%] [G loss: 1.036403]\n",
      "epoch:26 step:24694 [D loss: 0.461911, acc.: 72.66%] [G loss: 1.811430]\n",
      "epoch:26 step:24695 [D loss: 0.407003, acc.: 83.59%] [G loss: 1.976555]\n",
      "epoch:26 step:24696 [D loss: 0.745497, acc.: 57.81%] [G loss: 2.027470]\n",
      "epoch:26 step:24697 [D loss: 0.245424, acc.: 90.62%] [G loss: 1.537146]\n",
      "epoch:26 step:24698 [D loss: 0.336675, acc.: 88.28%] [G loss: 1.022419]\n",
      "epoch:26 step:24699 [D loss: 0.177110, acc.: 98.44%] [G loss: 1.459897]\n",
      "epoch:26 step:24700 [D loss: 0.445865, acc.: 72.66%] [G loss: 1.388494]\n",
      "epoch:26 step:24701 [D loss: 0.535457, acc.: 73.44%] [G loss: 1.803557]\n",
      "epoch:26 step:24702 [D loss: 1.310936, acc.: 53.91%] [G loss: 1.401537]\n",
      "epoch:26 step:24703 [D loss: 0.893842, acc.: 53.12%] [G loss: 2.276299]\n",
      "epoch:26 step:24704 [D loss: 0.231938, acc.: 93.75%] [G loss: 2.023427]\n",
      "epoch:26 step:24705 [D loss: 0.923912, acc.: 39.84%] [G loss: 2.903535]\n",
      "epoch:26 step:24706 [D loss: 0.827593, acc.: 52.34%] [G loss: 1.636070]\n",
      "epoch:26 step:24707 [D loss: 0.298811, acc.: 86.72%] [G loss: 1.017164]\n",
      "epoch:26 step:24708 [D loss: 0.249667, acc.: 90.62%] [G loss: 1.749537]\n",
      "epoch:26 step:24709 [D loss: 0.709051, acc.: 50.00%] [G loss: 2.729989]\n",
      "epoch:26 step:24710 [D loss: 0.401359, acc.: 87.50%] [G loss: 3.365020]\n",
      "epoch:26 step:24711 [D loss: 0.482528, acc.: 78.91%] [G loss: 1.416869]\n",
      "epoch:26 step:24712 [D loss: 0.537487, acc.: 67.19%] [G loss: 2.117827]\n",
      "epoch:26 step:24713 [D loss: 0.877928, acc.: 56.25%] [G loss: 0.650387]\n",
      "epoch:26 step:24714 [D loss: 0.300492, acc.: 93.75%] [G loss: 3.323918]\n",
      "epoch:26 step:24715 [D loss: 0.090252, acc.: 97.66%] [G loss: 2.299318]\n",
      "epoch:26 step:24716 [D loss: 0.451533, acc.: 78.12%] [G loss: 1.822516]\n",
      "epoch:26 step:24717 [D loss: 0.311900, acc.: 89.84%] [G loss: 1.851393]\n",
      "epoch:26 step:24718 [D loss: 0.228016, acc.: 96.09%] [G loss: 0.465439]\n",
      "epoch:26 step:24719 [D loss: 0.374529, acc.: 85.94%] [G loss: 1.068638]\n",
      "epoch:26 step:24720 [D loss: 0.733515, acc.: 55.47%] [G loss: 2.710476]\n",
      "epoch:26 step:24721 [D loss: 0.391682, acc.: 72.66%] [G loss: 0.825489]\n",
      "epoch:26 step:24722 [D loss: 0.666079, acc.: 55.47%] [G loss: 1.414453]\n",
      "epoch:26 step:24723 [D loss: 0.338133, acc.: 89.06%] [G loss: 2.693680]\n",
      "epoch:26 step:24724 [D loss: 0.666914, acc.: 62.50%] [G loss: 2.468773]\n",
      "epoch:26 step:24725 [D loss: 0.409045, acc.: 78.91%] [G loss: 1.905963]\n",
      "epoch:26 step:24726 [D loss: 0.692848, acc.: 58.59%] [G loss: 0.828406]\n",
      "epoch:26 step:24727 [D loss: 0.607750, acc.: 64.84%] [G loss: 2.255087]\n",
      "epoch:26 step:24728 [D loss: 0.384772, acc.: 77.34%] [G loss: 3.587204]\n",
      "epoch:26 step:24729 [D loss: 0.703417, acc.: 63.28%] [G loss: 2.210395]\n",
      "epoch:26 step:24730 [D loss: 0.320639, acc.: 89.06%] [G loss: 1.856755]\n",
      "epoch:26 step:24731 [D loss: 0.168803, acc.: 98.44%] [G loss: 3.736368]\n",
      "epoch:26 step:24732 [D loss: 0.280888, acc.: 94.53%] [G loss: 2.529434]\n",
      "epoch:26 step:24733 [D loss: 0.583271, acc.: 65.62%] [G loss: 0.973285]\n",
      "epoch:26 step:24734 [D loss: 0.095392, acc.: 98.44%] [G loss: 2.513312]\n",
      "epoch:26 step:24735 [D loss: 0.707262, acc.: 61.72%] [G loss: 2.183505]\n",
      "epoch:26 step:24736 [D loss: 0.747543, acc.: 51.56%] [G loss: 3.078802]\n",
      "epoch:26 step:24737 [D loss: 0.167286, acc.: 97.66%] [G loss: 2.787491]\n",
      "epoch:26 step:24738 [D loss: 0.276504, acc.: 85.16%] [G loss: 2.050230]\n",
      "epoch:26 step:24739 [D loss: 0.703502, acc.: 57.81%] [G loss: 1.124070]\n",
      "epoch:26 step:24740 [D loss: 0.397009, acc.: 78.12%] [G loss: 1.550323]\n",
      "epoch:26 step:24741 [D loss: 0.683592, acc.: 67.97%] [G loss: 2.061897]\n",
      "epoch:26 step:24742 [D loss: 0.463631, acc.: 80.47%] [G loss: 2.540223]\n",
      "epoch:26 step:24743 [D loss: 0.268597, acc.: 93.75%] [G loss: 2.584721]\n",
      "epoch:26 step:24744 [D loss: 0.282610, acc.: 94.53%] [G loss: 1.751833]\n",
      "epoch:26 step:24745 [D loss: 0.100484, acc.: 99.22%] [G loss: 1.565673]\n",
      "epoch:26 step:24746 [D loss: 0.742615, acc.: 59.38%] [G loss: 2.290850]\n",
      "epoch:26 step:24747 [D loss: 0.618209, acc.: 62.50%] [G loss: 1.682641]\n",
      "epoch:26 step:24748 [D loss: 0.377546, acc.: 83.59%] [G loss: 1.423439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24749 [D loss: 0.254487, acc.: 94.53%] [G loss: 3.626245]\n",
      "epoch:26 step:24750 [D loss: 0.784006, acc.: 53.91%] [G loss: 0.382967]\n",
      "epoch:26 step:24751 [D loss: 0.430857, acc.: 82.81%] [G loss: 3.294558]\n",
      "epoch:26 step:24752 [D loss: 0.228274, acc.: 94.53%] [G loss: 1.082257]\n",
      "epoch:26 step:24753 [D loss: 0.385409, acc.: 82.81%] [G loss: 0.961778]\n",
      "epoch:26 step:24754 [D loss: 0.742253, acc.: 54.69%] [G loss: 0.896020]\n",
      "epoch:26 step:24755 [D loss: 0.367788, acc.: 90.62%] [G loss: 2.775457]\n",
      "epoch:26 step:24756 [D loss: 0.219077, acc.: 95.31%] [G loss: 2.365066]\n",
      "epoch:26 step:24757 [D loss: 0.523205, acc.: 69.53%] [G loss: 1.418473]\n",
      "epoch:26 step:24758 [D loss: 0.194409, acc.: 96.09%] [G loss: 1.316007]\n",
      "epoch:26 step:24759 [D loss: 0.136882, acc.: 97.66%] [G loss: 0.478596]\n",
      "epoch:26 step:24760 [D loss: 0.187856, acc.: 95.31%] [G loss: 2.260436]\n",
      "epoch:26 step:24761 [D loss: 0.069620, acc.: 100.00%] [G loss: 0.243130]\n",
      "epoch:26 step:24762 [D loss: 0.155082, acc.: 97.66%] [G loss: 0.420039]\n",
      "epoch:26 step:24763 [D loss: 0.458365, acc.: 80.47%] [G loss: 0.896605]\n",
      "epoch:26 step:24764 [D loss: 0.168045, acc.: 96.09%] [G loss: 0.899187]\n",
      "epoch:26 step:24765 [D loss: 0.182799, acc.: 95.31%] [G loss: 0.901635]\n",
      "epoch:26 step:24766 [D loss: 0.150179, acc.: 98.44%] [G loss: 0.587112]\n",
      "epoch:26 step:24767 [D loss: 0.375319, acc.: 82.81%] [G loss: 0.186960]\n",
      "epoch:26 step:24768 [D loss: 0.271268, acc.: 92.97%] [G loss: 2.700448]\n",
      "epoch:26 step:24769 [D loss: 0.092796, acc.: 100.00%] [G loss: 0.471306]\n",
      "epoch:26 step:24770 [D loss: 0.275522, acc.: 96.09%] [G loss: 3.672978]\n",
      "epoch:26 step:24771 [D loss: 0.779544, acc.: 46.09%] [G loss: 0.810018]\n",
      "epoch:26 step:24772 [D loss: 1.490833, acc.: 33.59%] [G loss: 4.709736]\n",
      "epoch:26 step:24773 [D loss: 0.384138, acc.: 78.91%] [G loss: 3.676182]\n",
      "epoch:26 step:24774 [D loss: 1.472574, acc.: 42.19%] [G loss: 1.807533]\n",
      "epoch:26 step:24775 [D loss: 0.338515, acc.: 85.94%] [G loss: 1.917216]\n",
      "epoch:26 step:24776 [D loss: 0.173594, acc.: 93.75%] [G loss: 2.870263]\n",
      "epoch:26 step:24777 [D loss: 0.567696, acc.: 68.75%] [G loss: 2.767986]\n",
      "epoch:26 step:24778 [D loss: 0.465852, acc.: 78.12%] [G loss: 2.862857]\n",
      "epoch:26 step:24779 [D loss: 0.284890, acc.: 89.06%] [G loss: 3.239407]\n",
      "epoch:26 step:24780 [D loss: 0.403661, acc.: 78.91%] [G loss: 2.212959]\n",
      "epoch:26 step:24781 [D loss: 0.240368, acc.: 95.31%] [G loss: 1.074311]\n",
      "epoch:26 step:24782 [D loss: 0.874633, acc.: 46.09%] [G loss: 0.352316]\n",
      "epoch:26 step:24783 [D loss: 0.217968, acc.: 96.88%] [G loss: 1.377491]\n",
      "epoch:26 step:24784 [D loss: 0.475555, acc.: 74.22%] [G loss: 1.436583]\n",
      "epoch:26 step:24785 [D loss: 0.155657, acc.: 96.88%] [G loss: 1.620110]\n",
      "epoch:26 step:24786 [D loss: 0.602976, acc.: 64.84%] [G loss: 0.901705]\n",
      "epoch:26 step:24787 [D loss: 0.299338, acc.: 92.19%] [G loss: 1.717437]\n",
      "epoch:26 step:24788 [D loss: 0.446623, acc.: 77.34%] [G loss: 2.431026]\n",
      "epoch:26 step:24789 [D loss: 0.355503, acc.: 81.25%] [G loss: 1.474302]\n",
      "epoch:26 step:24790 [D loss: 0.388231, acc.: 76.56%] [G loss: 2.000973]\n",
      "epoch:26 step:24791 [D loss: 0.681949, acc.: 60.94%] [G loss: 1.927063]\n",
      "epoch:26 step:24792 [D loss: 0.221794, acc.: 94.53%] [G loss: 3.329398]\n",
      "epoch:26 step:24793 [D loss: 0.198806, acc.: 96.88%] [G loss: 3.714088]\n",
      "epoch:26 step:24794 [D loss: 0.745233, acc.: 49.22%] [G loss: 1.681552]\n",
      "epoch:26 step:24795 [D loss: 0.418460, acc.: 87.50%] [G loss: 1.340406]\n",
      "epoch:26 step:24796 [D loss: 0.555151, acc.: 72.66%] [G loss: 2.252819]\n",
      "epoch:26 step:24797 [D loss: 0.512562, acc.: 73.44%] [G loss: 2.063207]\n",
      "epoch:26 step:24798 [D loss: 0.270860, acc.: 90.62%] [G loss: 1.868102]\n",
      "epoch:26 step:24799 [D loss: 0.206500, acc.: 96.88%] [G loss: 1.428883]\n",
      "epoch:26 step:24800 [D loss: 0.494095, acc.: 64.84%] [G loss: 2.109243]\n",
      "epoch:26 step:24801 [D loss: 0.694925, acc.: 57.81%] [G loss: 1.710471]\n",
      "epoch:26 step:24802 [D loss: 0.697259, acc.: 62.50%] [G loss: 3.423875]\n",
      "epoch:26 step:24803 [D loss: 0.323893, acc.: 85.16%] [G loss: 1.749813]\n",
      "epoch:26 step:24804 [D loss: 0.124242, acc.: 98.44%] [G loss: 2.421053]\n",
      "epoch:26 step:24805 [D loss: 0.361488, acc.: 82.81%] [G loss: 2.961137]\n",
      "epoch:26 step:24806 [D loss: 0.216406, acc.: 96.09%] [G loss: 2.007442]\n",
      "epoch:26 step:24807 [D loss: 0.784316, acc.: 57.81%] [G loss: 1.709589]\n",
      "epoch:26 step:24808 [D loss: 0.471569, acc.: 81.25%] [G loss: 1.241672]\n",
      "epoch:26 step:24809 [D loss: 0.428931, acc.: 82.03%] [G loss: 0.849010]\n",
      "epoch:26 step:24810 [D loss: 0.324987, acc.: 92.97%] [G loss: 0.470314]\n",
      "epoch:26 step:24811 [D loss: 0.398674, acc.: 88.28%] [G loss: 0.902180]\n",
      "epoch:26 step:24812 [D loss: 0.476728, acc.: 75.00%] [G loss: 2.120901]\n",
      "epoch:26 step:24813 [D loss: 0.260282, acc.: 91.41%] [G loss: 2.033232]\n",
      "epoch:26 step:24814 [D loss: 0.473285, acc.: 80.47%] [G loss: 2.200575]\n",
      "epoch:26 step:24815 [D loss: 0.338393, acc.: 87.50%] [G loss: 3.884093]\n",
      "epoch:26 step:24816 [D loss: 0.289727, acc.: 90.62%] [G loss: 1.767073]\n",
      "epoch:26 step:24817 [D loss: 0.081133, acc.: 100.00%] [G loss: 1.563695]\n",
      "epoch:26 step:24818 [D loss: 0.080228, acc.: 99.22%] [G loss: 3.288072]\n",
      "epoch:26 step:24819 [D loss: 0.148354, acc.: 99.22%] [G loss: 1.361309]\n",
      "epoch:26 step:24820 [D loss: 0.163855, acc.: 96.09%] [G loss: 3.622376]\n",
      "epoch:26 step:24821 [D loss: 0.147958, acc.: 96.88%] [G loss: 3.256883]\n",
      "epoch:26 step:24822 [D loss: 0.126171, acc.: 98.44%] [G loss: 2.484042]\n",
      "epoch:26 step:24823 [D loss: 0.192249, acc.: 96.09%] [G loss: 2.705004]\n",
      "epoch:26 step:24824 [D loss: 0.130683, acc.: 98.44%] [G loss: 1.802945]\n",
      "epoch:26 step:24825 [D loss: 0.177194, acc.: 96.88%] [G loss: 0.984684]\n",
      "epoch:26 step:24826 [D loss: 0.240626, acc.: 95.31%] [G loss: 1.316537]\n",
      "epoch:26 step:24827 [D loss: 0.184443, acc.: 97.66%] [G loss: 3.341401]\n",
      "epoch:26 step:24828 [D loss: 0.040432, acc.: 100.00%] [G loss: 3.023043]\n",
      "epoch:26 step:24829 [D loss: 0.107669, acc.: 97.66%] [G loss: 1.267513]\n",
      "epoch:26 step:24830 [D loss: 0.227526, acc.: 93.75%] [G loss: 1.411099]\n",
      "epoch:26 step:24831 [D loss: 0.111638, acc.: 97.66%] [G loss: 0.711799]\n",
      "epoch:26 step:24832 [D loss: 0.105227, acc.: 99.22%] [G loss: 2.102693]\n",
      "epoch:26 step:24833 [D loss: 0.940542, acc.: 45.31%] [G loss: 3.461614]\n",
      "epoch:26 step:24834 [D loss: 0.132721, acc.: 98.44%] [G loss: 0.494761]\n",
      "epoch:26 step:24835 [D loss: 0.132238, acc.: 96.09%] [G loss: 2.929975]\n",
      "epoch:26 step:24836 [D loss: 0.051830, acc.: 99.22%] [G loss: 2.074899]\n",
      "epoch:26 step:24837 [D loss: 0.144842, acc.: 97.66%] [G loss: 1.575847]\n",
      "epoch:26 step:24838 [D loss: 0.050657, acc.: 99.22%] [G loss: 1.349757]\n",
      "epoch:26 step:24839 [D loss: 0.120803, acc.: 98.44%] [G loss: 0.522160]\n",
      "epoch:26 step:24840 [D loss: 0.635175, acc.: 64.06%] [G loss: 4.033601]\n",
      "epoch:26 step:24841 [D loss: 0.418549, acc.: 77.34%] [G loss: 1.151551]\n",
      "epoch:26 step:24842 [D loss: 0.549111, acc.: 71.09%] [G loss: 2.689840]\n",
      "epoch:26 step:24843 [D loss: 0.240582, acc.: 92.97%] [G loss: 3.146339]\n",
      "epoch:26 step:24844 [D loss: 0.180211, acc.: 96.88%] [G loss: 0.680063]\n",
      "epoch:26 step:24845 [D loss: 0.363638, acc.: 83.59%] [G loss: 1.581609]\n",
      "epoch:26 step:24846 [D loss: 0.162366, acc.: 95.31%] [G loss: 3.040583]\n",
      "epoch:26 step:24847 [D loss: 1.375530, acc.: 21.88%] [G loss: 2.113969]\n",
      "epoch:26 step:24848 [D loss: 0.126656, acc.: 96.88%] [G loss: 3.299434]\n",
      "epoch:26 step:24849 [D loss: 0.132937, acc.: 97.66%] [G loss: 3.766522]\n",
      "epoch:26 step:24850 [D loss: 0.179139, acc.: 97.66%] [G loss: 2.778127]\n",
      "epoch:26 step:24851 [D loss: 1.028173, acc.: 56.25%] [G loss: 1.294242]\n",
      "epoch:26 step:24852 [D loss: 1.693617, acc.: 41.41%] [G loss: 2.830520]\n",
      "epoch:26 step:24853 [D loss: 1.328856, acc.: 21.88%] [G loss: 1.456024]\n",
      "epoch:26 step:24854 [D loss: 0.205696, acc.: 95.31%] [G loss: 3.465308]\n",
      "epoch:26 step:24855 [D loss: 0.367100, acc.: 85.94%] [G loss: 2.883324]\n",
      "epoch:26 step:24856 [D loss: 0.114827, acc.: 96.09%] [G loss: 2.850837]\n",
      "epoch:26 step:24857 [D loss: 0.365322, acc.: 84.38%] [G loss: 3.005460]\n",
      "epoch:26 step:24858 [D loss: 0.260221, acc.: 92.19%] [G loss: 3.116970]\n",
      "epoch:26 step:24859 [D loss: 0.472251, acc.: 71.09%] [G loss: 1.582876]\n",
      "epoch:26 step:24860 [D loss: 0.216656, acc.: 92.97%] [G loss: 3.887962]\n",
      "epoch:26 step:24861 [D loss: 0.139425, acc.: 99.22%] [G loss: 1.145935]\n",
      "epoch:26 step:24862 [D loss: 0.316583, acc.: 85.16%] [G loss: 2.450646]\n",
      "epoch:26 step:24863 [D loss: 0.584436, acc.: 67.97%] [G loss: 2.702306]\n",
      "epoch:26 step:24864 [D loss: 0.477183, acc.: 72.66%] [G loss: 2.466615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24865 [D loss: 1.222269, acc.: 25.78%] [G loss: 1.454868]\n",
      "epoch:26 step:24866 [D loss: 0.157569, acc.: 96.09%] [G loss: 0.896604]\n",
      "epoch:26 step:24867 [D loss: 0.174959, acc.: 97.66%] [G loss: 3.337712]\n",
      "epoch:26 step:24868 [D loss: 0.823846, acc.: 52.34%] [G loss: 1.662392]\n",
      "epoch:26 step:24869 [D loss: 0.645224, acc.: 67.19%] [G loss: 1.735036]\n",
      "epoch:26 step:24870 [D loss: 0.179138, acc.: 98.44%] [G loss: 3.225377]\n",
      "epoch:26 step:24871 [D loss: 0.120565, acc.: 98.44%] [G loss: 2.811285]\n",
      "epoch:26 step:24872 [D loss: 0.246762, acc.: 92.97%] [G loss: 2.524380]\n",
      "epoch:26 step:24873 [D loss: 0.232462, acc.: 94.53%] [G loss: 2.851693]\n",
      "epoch:26 step:24874 [D loss: 0.405053, acc.: 86.72%] [G loss: 3.335514]\n",
      "epoch:26 step:24875 [D loss: 0.380349, acc.: 87.50%] [G loss: 3.491678]\n",
      "epoch:26 step:24876 [D loss: 0.816899, acc.: 52.34%] [G loss: 1.347292]\n",
      "epoch:26 step:24877 [D loss: 0.745207, acc.: 54.69%] [G loss: 1.032285]\n",
      "epoch:26 step:24878 [D loss: 0.251484, acc.: 92.97%] [G loss: 1.822586]\n",
      "epoch:26 step:24879 [D loss: 0.415653, acc.: 74.22%] [G loss: 2.714114]\n",
      "epoch:26 step:24880 [D loss: 0.416348, acc.: 79.69%] [G loss: 2.722694]\n",
      "epoch:26 step:24881 [D loss: 0.336706, acc.: 92.97%] [G loss: 0.637686]\n",
      "epoch:26 step:24882 [D loss: 0.181178, acc.: 97.66%] [G loss: 2.543392]\n",
      "epoch:26 step:24883 [D loss: 0.171047, acc.: 96.88%] [G loss: 2.690090]\n",
      "epoch:26 step:24884 [D loss: 0.540271, acc.: 68.75%] [G loss: 2.665951]\n",
      "epoch:26 step:24885 [D loss: 0.178405, acc.: 95.31%] [G loss: 0.734844]\n",
      "epoch:26 step:24886 [D loss: 0.093681, acc.: 99.22%] [G loss: 0.666301]\n",
      "epoch:26 step:24887 [D loss: 0.872800, acc.: 56.25%] [G loss: 2.896863]\n",
      "epoch:26 step:24888 [D loss: 0.338836, acc.: 87.50%] [G loss: 2.988296]\n",
      "epoch:26 step:24889 [D loss: 0.121822, acc.: 97.66%] [G loss: 2.497928]\n",
      "epoch:26 step:24890 [D loss: 0.157429, acc.: 96.09%] [G loss: 2.101984]\n",
      "epoch:26 step:24891 [D loss: 0.234249, acc.: 96.09%] [G loss: 1.568974]\n",
      "epoch:26 step:24892 [D loss: 0.587792, acc.: 66.41%] [G loss: 1.446910]\n",
      "epoch:26 step:24893 [D loss: 0.745508, acc.: 60.16%] [G loss: 2.726336]\n",
      "epoch:26 step:24894 [D loss: 0.169109, acc.: 96.09%] [G loss: 2.872583]\n",
      "epoch:26 step:24895 [D loss: 0.740803, acc.: 61.72%] [G loss: 0.374137]\n",
      "epoch:26 step:24896 [D loss: 0.664883, acc.: 61.72%] [G loss: 2.318509]\n",
      "epoch:26 step:24897 [D loss: 0.375722, acc.: 82.03%] [G loss: 2.038870]\n",
      "epoch:26 step:24898 [D loss: 0.525797, acc.: 77.34%] [G loss: 2.136950]\n",
      "epoch:26 step:24899 [D loss: 0.203603, acc.: 94.53%] [G loss: 0.922519]\n",
      "epoch:26 step:24900 [D loss: 0.374386, acc.: 84.38%] [G loss: 2.348017]\n",
      "epoch:26 step:24901 [D loss: 0.514977, acc.: 71.09%] [G loss: 2.903423]\n",
      "epoch:26 step:24902 [D loss: 0.415679, acc.: 75.78%] [G loss: 3.210562]\n",
      "epoch:26 step:24903 [D loss: 0.823329, acc.: 57.03%] [G loss: 2.098976]\n",
      "epoch:26 step:24904 [D loss: 0.320474, acc.: 87.50%] [G loss: 3.365582]\n",
      "epoch:26 step:24905 [D loss: 0.166538, acc.: 97.66%] [G loss: 1.911383]\n",
      "epoch:26 step:24906 [D loss: 0.163385, acc.: 96.09%] [G loss: 1.920827]\n",
      "epoch:26 step:24907 [D loss: 0.702609, acc.: 60.94%] [G loss: 0.888365]\n",
      "epoch:26 step:24908 [D loss: 0.183499, acc.: 96.88%] [G loss: 2.466496]\n",
      "epoch:26 step:24909 [D loss: 0.391967, acc.: 78.91%] [G loss: 2.571877]\n",
      "epoch:26 step:24910 [D loss: 0.227175, acc.: 92.97%] [G loss: 1.908988]\n",
      "epoch:26 step:24911 [D loss: 0.966806, acc.: 48.44%] [G loss: 0.469017]\n",
      "epoch:26 step:24912 [D loss: 0.815982, acc.: 55.47%] [G loss: 2.427350]\n",
      "epoch:26 step:24913 [D loss: 0.529175, acc.: 72.66%] [G loss: 3.158052]\n",
      "epoch:26 step:24914 [D loss: 0.248649, acc.: 92.97%] [G loss: 1.346932]\n",
      "epoch:26 step:24915 [D loss: 0.351227, acc.: 82.81%] [G loss: 1.885893]\n",
      "epoch:26 step:24916 [D loss: 0.095173, acc.: 99.22%] [G loss: 2.998279]\n",
      "epoch:26 step:24917 [D loss: 0.623343, acc.: 65.62%] [G loss: 2.539344]\n",
      "epoch:26 step:24918 [D loss: 0.632214, acc.: 65.62%] [G loss: 1.250302]\n",
      "epoch:26 step:24919 [D loss: 0.483854, acc.: 81.25%] [G loss: 1.953456]\n",
      "epoch:26 step:24920 [D loss: 0.233171, acc.: 94.53%] [G loss: 2.073447]\n",
      "epoch:26 step:24921 [D loss: 1.041909, acc.: 55.47%] [G loss: 3.080728]\n",
      "epoch:26 step:24922 [D loss: 0.589145, acc.: 75.78%] [G loss: 2.900456]\n",
      "epoch:26 step:24923 [D loss: 0.166529, acc.: 95.31%] [G loss: 3.064041]\n",
      "epoch:26 step:24924 [D loss: 0.532081, acc.: 69.53%] [G loss: 1.944922]\n",
      "epoch:26 step:24925 [D loss: 0.174465, acc.: 94.53%] [G loss: 3.086525]\n",
      "epoch:26 step:24926 [D loss: 0.274083, acc.: 92.19%] [G loss: 2.328140]\n",
      "epoch:26 step:24927 [D loss: 0.110455, acc.: 96.88%] [G loss: 0.715456]\n",
      "epoch:26 step:24928 [D loss: 0.283779, acc.: 95.31%] [G loss: 2.937085]\n",
      "epoch:26 step:24929 [D loss: 0.296880, acc.: 89.06%] [G loss: 2.502405]\n",
      "epoch:26 step:24930 [D loss: 0.380912, acc.: 84.38%] [G loss: 0.834959]\n",
      "epoch:26 step:24931 [D loss: 0.142126, acc.: 97.66%] [G loss: 0.564132]\n",
      "epoch:26 step:24932 [D loss: 0.448982, acc.: 76.56%] [G loss: 2.417841]\n",
      "epoch:26 step:24933 [D loss: 0.790080, acc.: 57.81%] [G loss: 1.732994]\n",
      "epoch:26 step:24934 [D loss: 0.051719, acc.: 100.00%] [G loss: 1.176250]\n",
      "epoch:26 step:24935 [D loss: 0.175515, acc.: 99.22%] [G loss: 0.978273]\n",
      "epoch:26 step:24936 [D loss: 1.457292, acc.: 21.09%] [G loss: 0.928704]\n",
      "epoch:26 step:24937 [D loss: 0.129656, acc.: 96.88%] [G loss: 3.304056]\n",
      "epoch:26 step:24938 [D loss: 0.348168, acc.: 85.16%] [G loss: 0.773949]\n",
      "epoch:26 step:24939 [D loss: 0.369957, acc.: 80.47%] [G loss: 1.226532]\n",
      "epoch:26 step:24940 [D loss: 0.331923, acc.: 84.38%] [G loss: 0.559804]\n",
      "epoch:26 step:24941 [D loss: 1.069977, acc.: 45.31%] [G loss: 5.031053]\n",
      "epoch:26 step:24942 [D loss: 0.526664, acc.: 78.12%] [G loss: 1.238077]\n",
      "epoch:26 step:24943 [D loss: 0.319549, acc.: 86.72%] [G loss: 3.779763]\n",
      "epoch:26 step:24944 [D loss: 0.169465, acc.: 97.66%] [G loss: 1.779049]\n",
      "epoch:26 step:24945 [D loss: 1.037216, acc.: 51.56%] [G loss: 1.259611]\n",
      "epoch:26 step:24946 [D loss: 0.863628, acc.: 46.09%] [G loss: 1.351409]\n",
      "epoch:26 step:24947 [D loss: 0.517812, acc.: 70.31%] [G loss: 2.128072]\n",
      "epoch:26 step:24948 [D loss: 0.351945, acc.: 83.59%] [G loss: 2.952836]\n",
      "epoch:26 step:24949 [D loss: 0.254729, acc.: 92.97%] [G loss: 2.530737]\n",
      "epoch:26 step:24950 [D loss: 0.107397, acc.: 98.44%] [G loss: 2.105543]\n",
      "epoch:26 step:24951 [D loss: 0.437961, acc.: 82.03%] [G loss: 2.886643]\n",
      "epoch:26 step:24952 [D loss: 1.017544, acc.: 39.84%] [G loss: 3.979903]\n",
      "epoch:26 step:24953 [D loss: 0.638896, acc.: 60.94%] [G loss: 1.975070]\n",
      "epoch:26 step:24954 [D loss: 0.652711, acc.: 58.59%] [G loss: 2.564330]\n",
      "epoch:26 step:24955 [D loss: 0.327793, acc.: 88.28%] [G loss: 2.163016]\n",
      "epoch:26 step:24956 [D loss: 0.444549, acc.: 77.34%] [G loss: 2.520637]\n",
      "epoch:26 step:24957 [D loss: 0.139418, acc.: 98.44%] [G loss: 1.552227]\n",
      "epoch:26 step:24958 [D loss: 0.240657, acc.: 92.97%] [G loss: 3.138425]\n",
      "epoch:26 step:24959 [D loss: 0.434110, acc.: 82.81%] [G loss: 2.441516]\n",
      "epoch:26 step:24960 [D loss: 0.254638, acc.: 92.19%] [G loss: 1.627717]\n",
      "epoch:26 step:24961 [D loss: 0.078530, acc.: 97.66%] [G loss: 3.195663]\n",
      "epoch:26 step:24962 [D loss: 0.381652, acc.: 85.16%] [G loss: 1.951729]\n",
      "epoch:26 step:24963 [D loss: 0.469257, acc.: 78.12%] [G loss: 3.173389]\n",
      "epoch:26 step:24964 [D loss: 0.279418, acc.: 93.75%] [G loss: 2.459334]\n",
      "epoch:26 step:24965 [D loss: 0.139442, acc.: 99.22%] [G loss: 2.331637]\n",
      "epoch:26 step:24966 [D loss: 0.584194, acc.: 70.31%] [G loss: 1.929484]\n",
      "epoch:26 step:24967 [D loss: 0.318869, acc.: 83.59%] [G loss: 2.283209]\n",
      "epoch:26 step:24968 [D loss: 0.299936, acc.: 89.84%] [G loss: 4.420601]\n",
      "epoch:26 step:24969 [D loss: 0.286670, acc.: 90.62%] [G loss: 2.974086]\n",
      "epoch:26 step:24970 [D loss: 0.572900, acc.: 67.97%] [G loss: 3.305939]\n",
      "epoch:26 step:24971 [D loss: 0.629274, acc.: 60.16%] [G loss: 1.895525]\n",
      "epoch:26 step:24972 [D loss: 0.141188, acc.: 98.44%] [G loss: 3.944985]\n",
      "epoch:26 step:24973 [D loss: 0.245924, acc.: 96.88%] [G loss: 1.854645]\n",
      "epoch:26 step:24974 [D loss: 0.189659, acc.: 94.53%] [G loss: 1.284036]\n",
      "epoch:26 step:24975 [D loss: 0.204345, acc.: 96.09%] [G loss: 2.432876]\n",
      "epoch:26 step:24976 [D loss: 0.769376, acc.: 57.03%] [G loss: 3.092109]\n",
      "epoch:26 step:24977 [D loss: 0.146925, acc.: 96.88%] [G loss: 2.939426]\n",
      "epoch:26 step:24978 [D loss: 0.600536, acc.: 68.75%] [G loss: 5.374618]\n",
      "epoch:26 step:24979 [D loss: 0.694830, acc.: 65.62%] [G loss: 0.999891]\n",
      "epoch:26 step:24980 [D loss: 0.722607, acc.: 55.47%] [G loss: 1.253078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24981 [D loss: 0.753206, acc.: 60.16%] [G loss: 1.902134]\n",
      "epoch:26 step:24982 [D loss: 0.259832, acc.: 87.50%] [G loss: 2.551550]\n",
      "epoch:26 step:24983 [D loss: 0.316716, acc.: 90.62%] [G loss: 2.310398]\n",
      "epoch:26 step:24984 [D loss: 0.292743, acc.: 92.97%] [G loss: 3.418402]\n",
      "epoch:26 step:24985 [D loss: 0.244001, acc.: 95.31%] [G loss: 2.741320]\n",
      "epoch:26 step:24986 [D loss: 0.338481, acc.: 91.41%] [G loss: 1.066311]\n",
      "epoch:26 step:24987 [D loss: 0.564864, acc.: 68.75%] [G loss: 2.658402]\n",
      "epoch:26 step:24988 [D loss: 0.132833, acc.: 99.22%] [G loss: 5.424968]\n",
      "epoch:26 step:24989 [D loss: 0.400554, acc.: 84.38%] [G loss: 5.069298]\n",
      "epoch:26 step:24990 [D loss: 0.196504, acc.: 94.53%] [G loss: 3.207033]\n",
      "epoch:26 step:24991 [D loss: 0.374958, acc.: 83.59%] [G loss: 3.210790]\n",
      "epoch:26 step:24992 [D loss: 0.895968, acc.: 57.81%] [G loss: 1.450442]\n",
      "epoch:26 step:24993 [D loss: 0.565993, acc.: 74.22%] [G loss: 1.987423]\n",
      "epoch:26 step:24994 [D loss: 0.122605, acc.: 99.22%] [G loss: 1.458639]\n",
      "epoch:26 step:24995 [D loss: 0.145013, acc.: 98.44%] [G loss: 1.788268]\n",
      "epoch:26 step:24996 [D loss: 0.149208, acc.: 96.88%] [G loss: 3.269739]\n",
      "epoch:26 step:24997 [D loss: 0.245755, acc.: 93.75%] [G loss: 1.600075]\n",
      "epoch:26 step:24998 [D loss: 0.093553, acc.: 100.00%] [G loss: 4.697808]\n",
      "epoch:26 step:24999 [D loss: 0.207944, acc.: 98.44%] [G loss: 1.593202]\n",
      "epoch:26 step:25000 [D loss: 0.210615, acc.: 94.53%] [G loss: 3.069803]\n",
      "epoch:26 step:25001 [D loss: 0.241940, acc.: 94.53%] [G loss: 2.627961]\n",
      "epoch:26 step:25002 [D loss: 0.084228, acc.: 99.22%] [G loss: 2.398969]\n",
      "epoch:26 step:25003 [D loss: 0.139989, acc.: 96.09%] [G loss: 2.381511]\n",
      "epoch:26 step:25004 [D loss: 0.335707, acc.: 89.84%] [G loss: 3.327107]\n",
      "epoch:26 step:25005 [D loss: 0.068410, acc.: 99.22%] [G loss: 3.676596]\n",
      "epoch:26 step:25006 [D loss: 0.597910, acc.: 67.19%] [G loss: 4.873615]\n",
      "epoch:26 step:25007 [D loss: 0.452469, acc.: 79.69%] [G loss: 2.607652]\n",
      "epoch:26 step:25008 [D loss: 0.385721, acc.: 82.81%] [G loss: 2.579758]\n",
      "epoch:26 step:25009 [D loss: 0.128541, acc.: 96.88%] [G loss: 1.580618]\n",
      "epoch:26 step:25010 [D loss: 0.544166, acc.: 77.34%] [G loss: 3.557876]\n",
      "epoch:26 step:25011 [D loss: 0.398812, acc.: 78.91%] [G loss: 1.354458]\n",
      "epoch:26 step:25012 [D loss: 0.243795, acc.: 89.06%] [G loss: 1.284080]\n",
      "epoch:26 step:25013 [D loss: 0.818466, acc.: 57.81%] [G loss: 2.155710]\n",
      "epoch:26 step:25014 [D loss: 0.446462, acc.: 80.47%] [G loss: 2.677891]\n",
      "epoch:26 step:25015 [D loss: 0.274664, acc.: 91.41%] [G loss: 4.094678]\n",
      "epoch:26 step:25016 [D loss: 0.315170, acc.: 85.16%] [G loss: 3.532888]\n",
      "epoch:26 step:25017 [D loss: 0.643017, acc.: 59.38%] [G loss: 3.994070]\n",
      "epoch:26 step:25018 [D loss: 0.156565, acc.: 96.09%] [G loss: 2.425267]\n",
      "epoch:26 step:25019 [D loss: 1.481384, acc.: 48.44%] [G loss: 1.369822]\n",
      "epoch:26 step:25020 [D loss: 0.415809, acc.: 82.03%] [G loss: 0.571094]\n",
      "epoch:26 step:25021 [D loss: 0.657252, acc.: 61.72%] [G loss: 2.526260]\n",
      "epoch:26 step:25022 [D loss: 0.419676, acc.: 80.47%] [G loss: 2.061027]\n",
      "epoch:26 step:25023 [D loss: 0.161194, acc.: 95.31%] [G loss: 0.559870]\n",
      "epoch:26 step:25024 [D loss: 0.166339, acc.: 98.44%] [G loss: 2.761818]\n",
      "epoch:26 step:25025 [D loss: 0.634923, acc.: 60.16%] [G loss: 0.643828]\n",
      "epoch:26 step:25026 [D loss: 0.294109, acc.: 86.72%] [G loss: 2.644291]\n",
      "epoch:26 step:25027 [D loss: 0.407343, acc.: 85.94%] [G loss: 2.458503]\n",
      "epoch:26 step:25028 [D loss: 0.299209, acc.: 87.50%] [G loss: 0.942825]\n",
      "epoch:26 step:25029 [D loss: 0.492112, acc.: 71.88%] [G loss: 3.035619]\n",
      "epoch:26 step:25030 [D loss: 0.171356, acc.: 96.88%] [G loss: 4.452556]\n",
      "epoch:26 step:25031 [D loss: 0.291170, acc.: 95.31%] [G loss: 2.594133]\n",
      "epoch:26 step:25032 [D loss: 0.112531, acc.: 98.44%] [G loss: 2.142026]\n",
      "epoch:26 step:25033 [D loss: 0.493928, acc.: 72.66%] [G loss: 2.729187]\n",
      "epoch:26 step:25034 [D loss: 0.191821, acc.: 96.88%] [G loss: 2.303091]\n",
      "epoch:26 step:25035 [D loss: 0.518592, acc.: 71.09%] [G loss: 1.319136]\n",
      "epoch:26 step:25036 [D loss: 0.613419, acc.: 64.84%] [G loss: 1.860799]\n",
      "epoch:26 step:25037 [D loss: 0.267496, acc.: 89.06%] [G loss: 2.683274]\n",
      "epoch:26 step:25038 [D loss: 0.478753, acc.: 75.78%] [G loss: 3.540881]\n",
      "epoch:26 step:25039 [D loss: 0.424788, acc.: 82.03%] [G loss: 1.431161]\n",
      "epoch:26 step:25040 [D loss: 0.230275, acc.: 93.75%] [G loss: 2.186987]\n",
      "epoch:26 step:25041 [D loss: 0.153928, acc.: 97.66%] [G loss: 0.939248]\n",
      "epoch:26 step:25042 [D loss: 0.146183, acc.: 96.88%] [G loss: 1.581174]\n",
      "epoch:26 step:25043 [D loss: 0.098995, acc.: 97.66%] [G loss: 1.031160]\n",
      "epoch:26 step:25044 [D loss: 0.982250, acc.: 41.41%] [G loss: 0.497236]\n",
      "epoch:26 step:25045 [D loss: 0.316732, acc.: 89.06%] [G loss: 2.518438]\n",
      "epoch:26 step:25046 [D loss: 0.175946, acc.: 98.44%] [G loss: 3.615241]\n",
      "epoch:26 step:25047 [D loss: 0.376311, acc.: 79.69%] [G loss: 3.189774]\n",
      "epoch:26 step:25048 [D loss: 0.602257, acc.: 71.09%] [G loss: 2.641661]\n",
      "epoch:26 step:25049 [D loss: 0.776861, acc.: 56.25%] [G loss: 1.563019]\n",
      "epoch:26 step:25050 [D loss: 0.894534, acc.: 47.66%] [G loss: 1.913774]\n",
      "epoch:26 step:25051 [D loss: 0.547054, acc.: 71.09%] [G loss: 2.564725]\n",
      "epoch:26 step:25052 [D loss: 0.257278, acc.: 95.31%] [G loss: 1.921481]\n",
      "epoch:26 step:25053 [D loss: 0.093733, acc.: 97.66%] [G loss: 1.834512]\n",
      "epoch:26 step:25054 [D loss: 0.327181, acc.: 86.72%] [G loss: 1.171883]\n",
      "epoch:26 step:25055 [D loss: 0.313223, acc.: 88.28%] [G loss: 3.490364]\n",
      "epoch:26 step:25056 [D loss: 0.430589, acc.: 78.91%] [G loss: 2.847642]\n",
      "epoch:26 step:25057 [D loss: 0.300639, acc.: 89.06%] [G loss: 2.746103]\n",
      "epoch:26 step:25058 [D loss: 0.430466, acc.: 74.22%] [G loss: 2.369375]\n",
      "epoch:26 step:25059 [D loss: 0.199467, acc.: 93.75%] [G loss: 1.683953]\n",
      "epoch:26 step:25060 [D loss: 0.643158, acc.: 64.06%] [G loss: 2.490511]\n",
      "epoch:26 step:25061 [D loss: 1.085040, acc.: 46.88%] [G loss: 0.745715]\n",
      "epoch:26 step:25062 [D loss: 0.520216, acc.: 73.44%] [G loss: 0.637592]\n",
      "epoch:26 step:25063 [D loss: 0.439287, acc.: 77.34%] [G loss: 1.666236]\n",
      "epoch:26 step:25064 [D loss: 0.739665, acc.: 55.47%] [G loss: 4.410196]\n",
      "epoch:26 step:25065 [D loss: 0.592783, acc.: 66.41%] [G loss: 2.899189]\n",
      "epoch:26 step:25066 [D loss: 0.586408, acc.: 63.28%] [G loss: 2.860909]\n",
      "epoch:26 step:25067 [D loss: 0.196110, acc.: 96.09%] [G loss: 2.494542]\n",
      "epoch:26 step:25068 [D loss: 0.831318, acc.: 53.12%] [G loss: 1.849150]\n",
      "epoch:26 step:25069 [D loss: 0.172729, acc.: 95.31%] [G loss: 3.050672]\n",
      "epoch:26 step:25070 [D loss: 0.334525, acc.: 88.28%] [G loss: 2.698682]\n",
      "epoch:26 step:25071 [D loss: 0.164769, acc.: 96.88%] [G loss: 0.327985]\n",
      "epoch:26 step:25072 [D loss: 0.464453, acc.: 69.53%] [G loss: 1.656864]\n",
      "epoch:26 step:25073 [D loss: 0.260050, acc.: 93.75%] [G loss: 1.666470]\n",
      "epoch:26 step:25074 [D loss: 0.605117, acc.: 61.72%] [G loss: 3.037836]\n",
      "epoch:26 step:25075 [D loss: 0.306914, acc.: 91.41%] [G loss: 1.415897]\n",
      "epoch:26 step:25076 [D loss: 0.177314, acc.: 96.88%] [G loss: 0.714862]\n",
      "epoch:26 step:25077 [D loss: 0.186657, acc.: 96.09%] [G loss: 1.921237]\n",
      "epoch:26 step:25078 [D loss: 0.647663, acc.: 62.50%] [G loss: 2.042580]\n",
      "epoch:26 step:25079 [D loss: 0.225097, acc.: 94.53%] [G loss: 1.713190]\n",
      "epoch:26 step:25080 [D loss: 0.289460, acc.: 85.16%] [G loss: 1.052180]\n",
      "epoch:26 step:25081 [D loss: 0.993944, acc.: 54.69%] [G loss: 0.798980]\n",
      "epoch:26 step:25082 [D loss: 0.245168, acc.: 90.62%] [G loss: 3.102997]\n",
      "epoch:26 step:25083 [D loss: 0.239645, acc.: 92.19%] [G loss: 2.712552]\n",
      "epoch:26 step:25084 [D loss: 0.183608, acc.: 98.44%] [G loss: 4.080169]\n",
      "epoch:26 step:25085 [D loss: 0.309013, acc.: 90.62%] [G loss: 1.015415]\n",
      "epoch:26 step:25086 [D loss: 0.187575, acc.: 97.66%] [G loss: 1.796709]\n",
      "epoch:26 step:25087 [D loss: 0.328502, acc.: 94.53%] [G loss: 1.993620]\n",
      "epoch:26 step:25088 [D loss: 1.528589, acc.: 48.44%] [G loss: 2.548590]\n",
      "epoch:26 step:25089 [D loss: 0.521163, acc.: 69.53%] [G loss: 3.159292]\n",
      "epoch:26 step:25090 [D loss: 0.064418, acc.: 100.00%] [G loss: 3.992153]\n",
      "epoch:26 step:25091 [D loss: 0.238682, acc.: 94.53%] [G loss: 2.327135]\n",
      "epoch:26 step:25092 [D loss: 0.508751, acc.: 82.03%] [G loss: 2.129359]\n",
      "epoch:26 step:25093 [D loss: 0.144738, acc.: 94.53%] [G loss: 3.583354]\n",
      "epoch:26 step:25094 [D loss: 0.164498, acc.: 98.44%] [G loss: 2.775321]\n",
      "epoch:26 step:25095 [D loss: 0.369228, acc.: 87.50%] [G loss: 1.064682]\n",
      "epoch:26 step:25096 [D loss: 0.407879, acc.: 82.03%] [G loss: 2.419967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25097 [D loss: 0.303244, acc.: 89.06%] [G loss: 2.968160]\n",
      "epoch:26 step:25098 [D loss: 0.135902, acc.: 97.66%] [G loss: 2.114141]\n",
      "epoch:26 step:25099 [D loss: 0.071924, acc.: 98.44%] [G loss: 0.992353]\n",
      "epoch:26 step:25100 [D loss: 0.837048, acc.: 44.53%] [G loss: 1.471303]\n",
      "epoch:26 step:25101 [D loss: 0.248551, acc.: 89.06%] [G loss: 1.265182]\n",
      "epoch:26 step:25102 [D loss: 0.361080, acc.: 85.94%] [G loss: 1.372238]\n",
      "epoch:26 step:25103 [D loss: 0.225452, acc.: 92.97%] [G loss: 2.628510]\n",
      "epoch:26 step:25104 [D loss: 0.254638, acc.: 94.53%] [G loss: 1.404687]\n",
      "epoch:26 step:25105 [D loss: 0.603263, acc.: 64.84%] [G loss: 2.566596]\n",
      "epoch:26 step:25106 [D loss: 0.396878, acc.: 81.25%] [G loss: 5.464973]\n",
      "epoch:26 step:25107 [D loss: 0.474651, acc.: 75.78%] [G loss: 2.724745]\n",
      "epoch:26 step:25108 [D loss: 0.348840, acc.: 82.81%] [G loss: 4.038140]\n",
      "epoch:26 step:25109 [D loss: 0.342626, acc.: 85.94%] [G loss: 2.111243]\n",
      "epoch:26 step:25110 [D loss: 0.120303, acc.: 97.66%] [G loss: 1.376497]\n",
      "epoch:26 step:25111 [D loss: 0.482703, acc.: 74.22%] [G loss: 1.719207]\n",
      "epoch:26 step:25112 [D loss: 0.329705, acc.: 91.41%] [G loss: 2.932036]\n",
      "epoch:26 step:25113 [D loss: 0.676789, acc.: 59.38%] [G loss: 0.572899]\n",
      "epoch:26 step:25114 [D loss: 0.569945, acc.: 70.31%] [G loss: 2.613735]\n",
      "epoch:26 step:25115 [D loss: 0.169027, acc.: 97.66%] [G loss: 2.389366]\n",
      "epoch:26 step:25116 [D loss: 0.238535, acc.: 92.19%] [G loss: 3.122653]\n",
      "epoch:26 step:25117 [D loss: 0.517676, acc.: 75.00%] [G loss: 3.075491]\n",
      "epoch:26 step:25118 [D loss: 0.467949, acc.: 76.56%] [G loss: 2.201289]\n",
      "epoch:26 step:25119 [D loss: 0.333960, acc.: 86.72%] [G loss: 4.215854]\n",
      "epoch:26 step:25120 [D loss: 0.112920, acc.: 97.66%] [G loss: 2.736579]\n",
      "epoch:26 step:25121 [D loss: 0.233160, acc.: 95.31%] [G loss: 2.116251]\n",
      "epoch:26 step:25122 [D loss: 0.579808, acc.: 60.16%] [G loss: 2.288239]\n",
      "epoch:26 step:25123 [D loss: 0.214638, acc.: 92.19%] [G loss: 3.026659]\n",
      "epoch:26 step:25124 [D loss: 0.215240, acc.: 96.88%] [G loss: 2.649118]\n",
      "epoch:26 step:25125 [D loss: 0.119666, acc.: 100.00%] [G loss: 2.966456]\n",
      "epoch:26 step:25126 [D loss: 0.352979, acc.: 82.81%] [G loss: 5.038907]\n",
      "epoch:26 step:25127 [D loss: 0.518003, acc.: 74.22%] [G loss: 5.341744]\n",
      "epoch:26 step:25128 [D loss: 0.190727, acc.: 96.88%] [G loss: 4.085326]\n",
      "epoch:26 step:25129 [D loss: 0.261510, acc.: 92.19%] [G loss: 6.065221]\n",
      "epoch:26 step:25130 [D loss: 0.838830, acc.: 57.03%] [G loss: 3.219327]\n",
      "epoch:26 step:25131 [D loss: 0.117954, acc.: 96.88%] [G loss: 3.972871]\n",
      "epoch:26 step:25132 [D loss: 0.081251, acc.: 98.44%] [G loss: 4.300170]\n",
      "epoch:26 step:25133 [D loss: 0.095045, acc.: 99.22%] [G loss: 2.676198]\n",
      "epoch:26 step:25134 [D loss: 0.320856, acc.: 87.50%] [G loss: 3.103585]\n",
      "epoch:26 step:25135 [D loss: 0.552474, acc.: 75.00%] [G loss: 3.157293]\n",
      "epoch:26 step:25136 [D loss: 0.127160, acc.: 97.66%] [G loss: 2.527489]\n",
      "epoch:26 step:25137 [D loss: 0.196034, acc.: 96.88%] [G loss: 4.497754]\n",
      "epoch:26 step:25138 [D loss: 0.142055, acc.: 96.88%] [G loss: 2.195006]\n",
      "epoch:26 step:25139 [D loss: 0.126544, acc.: 99.22%] [G loss: 3.642229]\n",
      "epoch:26 step:25140 [D loss: 0.174337, acc.: 95.31%] [G loss: 3.078407]\n",
      "epoch:26 step:25141 [D loss: 0.527491, acc.: 68.75%] [G loss: 1.642405]\n",
      "epoch:26 step:25142 [D loss: 0.931893, acc.: 57.03%] [G loss: 1.042696]\n",
      "epoch:26 step:25143 [D loss: 0.211727, acc.: 96.88%] [G loss: 4.332511]\n",
      "epoch:26 step:25144 [D loss: 0.928811, acc.: 51.56%] [G loss: 2.845880]\n",
      "epoch:26 step:25145 [D loss: 0.285003, acc.: 89.06%] [G loss: 1.624215]\n",
      "epoch:26 step:25146 [D loss: 0.843305, acc.: 56.25%] [G loss: 2.428185]\n",
      "epoch:26 step:25147 [D loss: 0.393208, acc.: 82.03%] [G loss: 2.624336]\n",
      "epoch:26 step:25148 [D loss: 0.466737, acc.: 72.66%] [G loss: 3.645897]\n",
      "epoch:26 step:25149 [D loss: 0.184543, acc.: 93.75%] [G loss: 3.878863]\n",
      "epoch:26 step:25150 [D loss: 0.252108, acc.: 93.75%] [G loss: 3.868623]\n",
      "epoch:26 step:25151 [D loss: 0.581311, acc.: 70.31%] [G loss: 0.367100]\n",
      "epoch:26 step:25152 [D loss: 0.186710, acc.: 95.31%] [G loss: 2.171877]\n",
      "epoch:26 step:25153 [D loss: 0.106544, acc.: 98.44%] [G loss: 1.517529]\n",
      "epoch:26 step:25154 [D loss: 0.179056, acc.: 95.31%] [G loss: 1.364908]\n",
      "epoch:26 step:25155 [D loss: 0.744234, acc.: 59.38%] [G loss: 3.304222]\n",
      "epoch:26 step:25156 [D loss: 0.347163, acc.: 79.69%] [G loss: 0.490142]\n",
      "epoch:26 step:25157 [D loss: 0.163206, acc.: 95.31%] [G loss: 1.753675]\n",
      "epoch:26 step:25158 [D loss: 1.402699, acc.: 50.78%] [G loss: 3.006055]\n",
      "epoch:26 step:25159 [D loss: 0.116534, acc.: 96.88%] [G loss: 3.891926]\n",
      "epoch:26 step:25160 [D loss: 0.223070, acc.: 92.97%] [G loss: 4.457977]\n",
      "epoch:26 step:25161 [D loss: 0.469640, acc.: 72.66%] [G loss: 3.589548]\n",
      "epoch:26 step:25162 [D loss: 0.652226, acc.: 60.16%] [G loss: 1.801092]\n",
      "epoch:26 step:25163 [D loss: 0.094932, acc.: 99.22%] [G loss: 3.760192]\n",
      "epoch:26 step:25164 [D loss: 0.081173, acc.: 99.22%] [G loss: 3.517458]\n",
      "epoch:26 step:25165 [D loss: 0.059398, acc.: 100.00%] [G loss: 5.107049]\n",
      "epoch:26 step:25166 [D loss: 0.097889, acc.: 99.22%] [G loss: 2.735546]\n",
      "epoch:26 step:25167 [D loss: 0.427863, acc.: 80.47%] [G loss: 3.118031]\n",
      "epoch:26 step:25168 [D loss: 0.077822, acc.: 99.22%] [G loss: 2.706810]\n",
      "epoch:26 step:25169 [D loss: 0.095928, acc.: 98.44%] [G loss: 3.306688]\n",
      "epoch:26 step:25170 [D loss: 0.137060, acc.: 99.22%] [G loss: 2.732794]\n",
      "epoch:26 step:25171 [D loss: 0.366819, acc.: 82.03%] [G loss: 2.689569]\n",
      "epoch:26 step:25172 [D loss: 0.206283, acc.: 91.41%] [G loss: 1.349623]\n",
      "epoch:26 step:25173 [D loss: 0.243541, acc.: 92.19%] [G loss: 2.257071]\n",
      "epoch:26 step:25174 [D loss: 0.132622, acc.: 98.44%] [G loss: 2.502767]\n",
      "epoch:26 step:25175 [D loss: 1.588971, acc.: 52.34%] [G loss: 3.542051]\n",
      "epoch:26 step:25176 [D loss: 0.576650, acc.: 69.53%] [G loss: 2.144311]\n",
      "epoch:26 step:25177 [D loss: 0.238729, acc.: 94.53%] [G loss: 3.237319]\n",
      "epoch:26 step:25178 [D loss: 0.917127, acc.: 56.25%] [G loss: 2.878028]\n",
      "epoch:26 step:25179 [D loss: 1.026167, acc.: 46.09%] [G loss: 1.185742]\n",
      "epoch:26 step:25180 [D loss: 0.272071, acc.: 91.41%] [G loss: 2.985274]\n",
      "epoch:26 step:25181 [D loss: 0.200059, acc.: 95.31%] [G loss: 3.504825]\n",
      "epoch:26 step:25182 [D loss: 0.181163, acc.: 96.09%] [G loss: 3.037863]\n",
      "epoch:26 step:25183 [D loss: 0.464374, acc.: 76.56%] [G loss: 2.398934]\n",
      "epoch:26 step:25184 [D loss: 0.339537, acc.: 85.94%] [G loss: 0.939838]\n",
      "epoch:26 step:25185 [D loss: 0.911599, acc.: 50.78%] [G loss: 2.480886]\n",
      "epoch:26 step:25186 [D loss: 0.822149, acc.: 53.12%] [G loss: 1.248011]\n",
      "epoch:26 step:25187 [D loss: 0.171074, acc.: 95.31%] [G loss: 3.017006]\n",
      "epoch:26 step:25188 [D loss: 0.407331, acc.: 87.50%] [G loss: 3.048569]\n",
      "epoch:26 step:25189 [D loss: 0.162013, acc.: 98.44%] [G loss: 2.091969]\n",
      "epoch:26 step:25190 [D loss: 0.219700, acc.: 95.31%] [G loss: 1.095992]\n",
      "epoch:26 step:25191 [D loss: 1.233631, acc.: 52.34%] [G loss: 2.494822]\n",
      "epoch:26 step:25192 [D loss: 0.698140, acc.: 57.03%] [G loss: 3.318643]\n",
      "epoch:26 step:25193 [D loss: 0.977552, acc.: 55.47%] [G loss: 3.430446]\n",
      "epoch:26 step:25194 [D loss: 0.543762, acc.: 74.22%] [G loss: 2.379747]\n",
      "epoch:26 step:25195 [D loss: 0.275386, acc.: 89.84%] [G loss: 2.447537]\n",
      "epoch:26 step:25196 [D loss: 0.272938, acc.: 95.31%] [G loss: 2.694049]\n",
      "epoch:26 step:25197 [D loss: 1.058441, acc.: 49.22%] [G loss: 3.164526]\n",
      "epoch:26 step:25198 [D loss: 0.106295, acc.: 98.44%] [G loss: 3.540731]\n",
      "epoch:26 step:25199 [D loss: 0.173144, acc.: 97.66%] [G loss: 1.839001]\n",
      "epoch:26 step:25200 [D loss: 0.248176, acc.: 96.88%] [G loss: 3.143481]\n",
      "epoch:26 step:25201 [D loss: 0.105020, acc.: 100.00%] [G loss: 1.952159]\n",
      "epoch:26 step:25202 [D loss: 0.436586, acc.: 75.00%] [G loss: 1.477828]\n",
      "epoch:26 step:25203 [D loss: 0.364930, acc.: 88.28%] [G loss: 1.843787]\n",
      "epoch:26 step:25204 [D loss: 0.079824, acc.: 99.22%] [G loss: 1.038163]\n",
      "epoch:26 step:25205 [D loss: 0.134368, acc.: 98.44%] [G loss: 2.196958]\n",
      "epoch:26 step:25206 [D loss: 0.301356, acc.: 93.75%] [G loss: 1.180747]\n",
      "epoch:26 step:25207 [D loss: 1.254681, acc.: 18.75%] [G loss: 0.937218]\n",
      "epoch:26 step:25208 [D loss: 0.535680, acc.: 68.75%] [G loss: 2.627787]\n",
      "epoch:26 step:25209 [D loss: 0.318631, acc.: 86.72%] [G loss: 3.202508]\n",
      "epoch:26 step:25210 [D loss: 0.572957, acc.: 67.19%] [G loss: 3.163376]\n",
      "epoch:26 step:25211 [D loss: 0.524689, acc.: 71.88%] [G loss: 1.540316]\n",
      "epoch:26 step:25212 [D loss: 0.130402, acc.: 96.88%] [G loss: 3.948120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25213 [D loss: 0.201759, acc.: 94.53%] [G loss: 1.913727]\n",
      "epoch:26 step:25214 [D loss: 0.433221, acc.: 80.47%] [G loss: 1.663081]\n",
      "epoch:26 step:25215 [D loss: 0.118629, acc.: 97.66%] [G loss: 1.833502]\n",
      "epoch:26 step:25216 [D loss: 0.313958, acc.: 93.75%] [G loss: 1.332088]\n",
      "epoch:26 step:25217 [D loss: 0.759156, acc.: 53.12%] [G loss: 4.045032]\n",
      "epoch:26 step:25218 [D loss: 1.778636, acc.: 16.41%] [G loss: 2.978872]\n",
      "epoch:26 step:25219 [D loss: 0.322837, acc.: 92.19%] [G loss: 1.168598]\n",
      "epoch:26 step:25220 [D loss: 0.134407, acc.: 97.66%] [G loss: 1.090599]\n",
      "epoch:26 step:25221 [D loss: 0.496574, acc.: 70.31%] [G loss: 3.962194]\n",
      "epoch:26 step:25222 [D loss: 0.261329, acc.: 91.41%] [G loss: 2.136034]\n",
      "epoch:26 step:25223 [D loss: 0.338919, acc.: 89.06%] [G loss: 1.045544]\n",
      "epoch:26 step:25224 [D loss: 1.129608, acc.: 37.50%] [G loss: 0.600487]\n",
      "epoch:26 step:25225 [D loss: 0.712699, acc.: 59.38%] [G loss: 1.008118]\n",
      "epoch:26 step:25226 [D loss: 0.268245, acc.: 93.75%] [G loss: 1.368851]\n",
      "epoch:26 step:25227 [D loss: 0.310285, acc.: 90.62%] [G loss: 3.181046]\n",
      "epoch:26 step:25228 [D loss: 0.439208, acc.: 77.34%] [G loss: 3.690743]\n",
      "epoch:26 step:25229 [D loss: 0.099754, acc.: 98.44%] [G loss: 0.804188]\n",
      "epoch:26 step:25230 [D loss: 0.183851, acc.: 98.44%] [G loss: 2.603602]\n",
      "epoch:26 step:25231 [D loss: 0.152846, acc.: 97.66%] [G loss: 0.534542]\n",
      "epoch:26 step:25232 [D loss: 0.579417, acc.: 68.75%] [G loss: 1.594751]\n",
      "epoch:26 step:25233 [D loss: 0.327281, acc.: 78.12%] [G loss: 0.789132]\n",
      "epoch:26 step:25234 [D loss: 0.451204, acc.: 76.56%] [G loss: 0.443917]\n",
      "epoch:26 step:25235 [D loss: 1.193406, acc.: 22.66%] [G loss: 2.142778]\n",
      "epoch:26 step:25236 [D loss: 0.324893, acc.: 92.19%] [G loss: 3.673846]\n",
      "epoch:26 step:25237 [D loss: 0.844274, acc.: 54.69%] [G loss: 1.445695]\n",
      "epoch:26 step:25238 [D loss: 0.098566, acc.: 98.44%] [G loss: 2.173969]\n",
      "epoch:26 step:25239 [D loss: 0.678596, acc.: 61.72%] [G loss: 0.827858]\n",
      "epoch:26 step:25240 [D loss: 0.270250, acc.: 95.31%] [G loss: 2.566602]\n",
      "epoch:26 step:25241 [D loss: 0.436261, acc.: 78.91%] [G loss: 4.148157]\n",
      "epoch:26 step:25242 [D loss: 0.364778, acc.: 82.03%] [G loss: 3.011115]\n",
      "epoch:26 step:25243 [D loss: 0.103035, acc.: 99.22%] [G loss: 4.156529]\n",
      "epoch:26 step:25244 [D loss: 0.333102, acc.: 92.97%] [G loss: 1.230127]\n",
      "epoch:26 step:25245 [D loss: 0.296470, acc.: 85.16%] [G loss: 2.275668]\n",
      "epoch:26 step:25246 [D loss: 0.162989, acc.: 98.44%] [G loss: 1.420880]\n",
      "epoch:26 step:25247 [D loss: 0.235267, acc.: 95.31%] [G loss: 2.423030]\n",
      "epoch:26 step:25248 [D loss: 0.115040, acc.: 96.88%] [G loss: 1.751352]\n",
      "epoch:26 step:25249 [D loss: 0.133007, acc.: 97.66%] [G loss: 2.468726]\n",
      "epoch:26 step:25250 [D loss: 0.089265, acc.: 99.22%] [G loss: 2.008827]\n",
      "epoch:26 step:25251 [D loss: 0.133820, acc.: 98.44%] [G loss: 4.477726]\n",
      "epoch:26 step:25252 [D loss: 0.555939, acc.: 68.75%] [G loss: 2.498229]\n",
      "epoch:26 step:25253 [D loss: 0.267081, acc.: 93.75%] [G loss: 2.767801]\n",
      "epoch:26 step:25254 [D loss: 0.551988, acc.: 68.75%] [G loss: 2.629334]\n",
      "epoch:26 step:25255 [D loss: 0.222607, acc.: 94.53%] [G loss: 3.225617]\n",
      "epoch:26 step:25256 [D loss: 0.285508, acc.: 93.75%] [G loss: 3.012601]\n",
      "epoch:26 step:25257 [D loss: 0.170403, acc.: 96.88%] [G loss: 2.508352]\n",
      "epoch:26 step:25258 [D loss: 0.474799, acc.: 71.09%] [G loss: 3.703689]\n",
      "epoch:26 step:25259 [D loss: 0.109957, acc.: 99.22%] [G loss: 1.714469]\n",
      "epoch:26 step:25260 [D loss: 0.134811, acc.: 97.66%] [G loss: 3.495546]\n",
      "epoch:26 step:25261 [D loss: 0.982016, acc.: 42.97%] [G loss: 2.789961]\n",
      "epoch:26 step:25262 [D loss: 0.790404, acc.: 58.59%] [G loss: 1.960047]\n",
      "epoch:26 step:25263 [D loss: 0.593263, acc.: 67.19%] [G loss: 1.885603]\n",
      "epoch:26 step:25264 [D loss: 0.130124, acc.: 99.22%] [G loss: 2.331879]\n",
      "epoch:26 step:25265 [D loss: 0.810911, acc.: 45.31%] [G loss: 3.115445]\n",
      "epoch:26 step:25266 [D loss: 0.105947, acc.: 99.22%] [G loss: 2.939907]\n",
      "epoch:26 step:25267 [D loss: 0.230277, acc.: 93.75%] [G loss: 1.064633]\n",
      "epoch:26 step:25268 [D loss: 0.689716, acc.: 67.97%] [G loss: 0.748559]\n",
      "epoch:26 step:25269 [D loss: 0.730492, acc.: 60.16%] [G loss: 1.935467]\n",
      "epoch:26 step:25270 [D loss: 0.855870, acc.: 43.75%] [G loss: 5.028636]\n",
      "epoch:26 step:25271 [D loss: 0.368764, acc.: 86.72%] [G loss: 2.250718]\n",
      "epoch:26 step:25272 [D loss: 0.075468, acc.: 99.22%] [G loss: 3.049464]\n",
      "epoch:26 step:25273 [D loss: 0.345934, acc.: 83.59%] [G loss: 2.748851]\n",
      "epoch:26 step:25274 [D loss: 0.304058, acc.: 89.84%] [G loss: 2.590112]\n",
      "epoch:26 step:25275 [D loss: 0.669216, acc.: 60.16%] [G loss: 1.768756]\n",
      "epoch:26 step:25276 [D loss: 0.151905, acc.: 96.88%] [G loss: 4.738567]\n",
      "epoch:26 step:25277 [D loss: 0.406994, acc.: 78.91%] [G loss: 3.864810]\n",
      "epoch:26 step:25278 [D loss: 0.606401, acc.: 68.75%] [G loss: 2.836422]\n",
      "epoch:26 step:25279 [D loss: 0.681405, acc.: 66.41%] [G loss: 1.549916]\n",
      "epoch:26 step:25280 [D loss: 0.418775, acc.: 78.91%] [G loss: 3.382534]\n",
      "epoch:26 step:25281 [D loss: 1.478485, acc.: 50.00%] [G loss: 1.699039]\n",
      "epoch:26 step:25282 [D loss: 0.590799, acc.: 61.72%] [G loss: 3.005328]\n",
      "epoch:26 step:25283 [D loss: 0.233215, acc.: 93.75%] [G loss: 1.242970]\n",
      "epoch:26 step:25284 [D loss: 0.301202, acc.: 90.62%] [G loss: 0.995497]\n",
      "epoch:26 step:25285 [D loss: 0.439468, acc.: 78.91%] [G loss: 2.383793]\n",
      "epoch:26 step:25286 [D loss: 0.443082, acc.: 75.78%] [G loss: 2.084666]\n",
      "epoch:26 step:25287 [D loss: 0.214587, acc.: 96.09%] [G loss: 1.679691]\n",
      "epoch:26 step:25288 [D loss: 0.208618, acc.: 95.31%] [G loss: 3.563492]\n",
      "epoch:26 step:25289 [D loss: 0.606039, acc.: 67.97%] [G loss: 4.117857]\n",
      "epoch:26 step:25290 [D loss: 0.701828, acc.: 64.84%] [G loss: 1.247367]\n",
      "epoch:26 step:25291 [D loss: 0.333949, acc.: 88.28%] [G loss: 1.855252]\n",
      "epoch:26 step:25292 [D loss: 0.408856, acc.: 82.03%] [G loss: 2.972604]\n",
      "epoch:26 step:25293 [D loss: 0.193273, acc.: 92.97%] [G loss: 3.741196]\n",
      "epoch:26 step:25294 [D loss: 0.201844, acc.: 94.53%] [G loss: 1.414617]\n",
      "epoch:26 step:25295 [D loss: 0.605735, acc.: 67.19%] [G loss: 0.591061]\n",
      "epoch:26 step:25296 [D loss: 0.431188, acc.: 82.03%] [G loss: 2.668262]\n",
      "epoch:26 step:25297 [D loss: 0.155529, acc.: 97.66%] [G loss: 3.180123]\n",
      "epoch:26 step:25298 [D loss: 0.359808, acc.: 85.94%] [G loss: 2.765026]\n",
      "epoch:26 step:25299 [D loss: 0.218659, acc.: 95.31%] [G loss: 2.796489]\n",
      "epoch:27 step:25300 [D loss: 0.773613, acc.: 57.81%] [G loss: 3.146399]\n",
      "epoch:27 step:25301 [D loss: 0.549354, acc.: 67.19%] [G loss: 3.013601]\n",
      "epoch:27 step:25302 [D loss: 0.283379, acc.: 91.41%] [G loss: 5.079403]\n",
      "epoch:27 step:25303 [D loss: 0.930082, acc.: 64.06%] [G loss: 2.588707]\n",
      "epoch:27 step:25304 [D loss: 0.813629, acc.: 53.12%] [G loss: 3.304543]\n",
      "epoch:27 step:25305 [D loss: 0.683004, acc.: 55.47%] [G loss: 2.745991]\n",
      "epoch:27 step:25306 [D loss: 0.809174, acc.: 56.25%] [G loss: 2.812256]\n",
      "epoch:27 step:25307 [D loss: 0.974547, acc.: 35.16%] [G loss: 2.114781]\n",
      "epoch:27 step:25308 [D loss: 0.288221, acc.: 94.53%] [G loss: 0.759152]\n",
      "epoch:27 step:25309 [D loss: 0.694318, acc.: 57.81%] [G loss: 2.580297]\n",
      "epoch:27 step:25310 [D loss: 0.857378, acc.: 53.12%] [G loss: 2.880915]\n",
      "epoch:27 step:25311 [D loss: 0.149829, acc.: 98.44%] [G loss: 2.866557]\n",
      "epoch:27 step:25312 [D loss: 0.344052, acc.: 85.16%] [G loss: 1.717602]\n",
      "epoch:27 step:25313 [D loss: 0.351350, acc.: 85.16%] [G loss: 3.124672]\n",
      "epoch:27 step:25314 [D loss: 0.184216, acc.: 96.88%] [G loss: 3.232159]\n",
      "epoch:27 step:25315 [D loss: 0.149318, acc.: 98.44%] [G loss: 2.957473]\n",
      "epoch:27 step:25316 [D loss: 0.210267, acc.: 96.09%] [G loss: 2.544897]\n",
      "epoch:27 step:25317 [D loss: 0.203142, acc.: 94.53%] [G loss: 2.194151]\n",
      "epoch:27 step:25318 [D loss: 0.085230, acc.: 99.22%] [G loss: 0.697782]\n",
      "epoch:27 step:25319 [D loss: 0.257532, acc.: 92.97%] [G loss: 1.745066]\n",
      "epoch:27 step:25320 [D loss: 0.329671, acc.: 83.59%] [G loss: 1.419775]\n",
      "epoch:27 step:25321 [D loss: 0.915202, acc.: 58.59%] [G loss: 1.975902]\n",
      "epoch:27 step:25322 [D loss: 0.279667, acc.: 90.62%] [G loss: 3.334358]\n",
      "epoch:27 step:25323 [D loss: 0.118780, acc.: 99.22%] [G loss: 4.265163]\n",
      "epoch:27 step:25324 [D loss: 1.171812, acc.: 35.94%] [G loss: 3.445549]\n",
      "epoch:27 step:25325 [D loss: 0.599823, acc.: 66.41%] [G loss: 1.745368]\n",
      "epoch:27 step:25326 [D loss: 0.242267, acc.: 95.31%] [G loss: 1.369871]\n",
      "epoch:27 step:25327 [D loss: 0.421887, acc.: 75.78%] [G loss: 0.966350]\n",
      "epoch:27 step:25328 [D loss: 0.413514, acc.: 71.09%] [G loss: 1.338853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25329 [D loss: 0.533368, acc.: 65.62%] [G loss: 2.179345]\n",
      "epoch:27 step:25330 [D loss: 0.765528, acc.: 60.94%] [G loss: 2.327156]\n",
      "epoch:27 step:25331 [D loss: 0.186731, acc.: 98.44%] [G loss: 0.509747]\n",
      "epoch:27 step:25332 [D loss: 0.426529, acc.: 75.78%] [G loss: 2.429670]\n",
      "epoch:27 step:25333 [D loss: 0.370311, acc.: 80.47%] [G loss: 1.914490]\n",
      "epoch:27 step:25334 [D loss: 0.554277, acc.: 68.75%] [G loss: 2.237573]\n",
      "epoch:27 step:25335 [D loss: 0.211215, acc.: 93.75%] [G loss: 2.387604]\n",
      "epoch:27 step:25336 [D loss: 0.255978, acc.: 93.75%] [G loss: 2.665925]\n",
      "epoch:27 step:25337 [D loss: 0.402727, acc.: 80.47%] [G loss: 3.125457]\n",
      "epoch:27 step:25338 [D loss: 0.337553, acc.: 86.72%] [G loss: 3.570442]\n",
      "epoch:27 step:25339 [D loss: 0.382972, acc.: 87.50%] [G loss: 1.984853]\n",
      "epoch:27 step:25340 [D loss: 0.246442, acc.: 92.97%] [G loss: 1.874999]\n",
      "epoch:27 step:25341 [D loss: 0.262319, acc.: 96.09%] [G loss: 1.797904]\n",
      "epoch:27 step:25342 [D loss: 0.576380, acc.: 67.19%] [G loss: 0.791198]\n",
      "epoch:27 step:25343 [D loss: 0.223764, acc.: 94.53%] [G loss: 0.864193]\n",
      "epoch:27 step:25344 [D loss: 0.250826, acc.: 92.97%] [G loss: 3.822348]\n",
      "epoch:27 step:25345 [D loss: 0.335431, acc.: 82.81%] [G loss: 2.278416]\n",
      "epoch:27 step:25346 [D loss: 0.238275, acc.: 90.62%] [G loss: 1.967415]\n",
      "epoch:27 step:25347 [D loss: 0.688205, acc.: 62.50%] [G loss: 3.224660]\n",
      "epoch:27 step:25348 [D loss: 0.613478, acc.: 67.19%] [G loss: 0.680770]\n",
      "epoch:27 step:25349 [D loss: 0.295265, acc.: 89.84%] [G loss: 1.147231]\n",
      "epoch:27 step:25350 [D loss: 0.438885, acc.: 73.44%] [G loss: 1.320475]\n",
      "epoch:27 step:25351 [D loss: 0.295527, acc.: 89.06%] [G loss: 1.532869]\n",
      "epoch:27 step:25352 [D loss: 0.336262, acc.: 81.25%] [G loss: 2.039569]\n",
      "epoch:27 step:25353 [D loss: 0.206278, acc.: 96.09%] [G loss: 1.042568]\n",
      "epoch:27 step:25354 [D loss: 0.214276, acc.: 94.53%] [G loss: 2.205801]\n",
      "epoch:27 step:25355 [D loss: 0.484892, acc.: 80.47%] [G loss: 1.066038]\n",
      "epoch:27 step:25356 [D loss: 0.212601, acc.: 95.31%] [G loss: 1.201337]\n",
      "epoch:27 step:25357 [D loss: 0.414270, acc.: 79.69%] [G loss: 2.145388]\n",
      "epoch:27 step:25358 [D loss: 0.152907, acc.: 96.09%] [G loss: 1.382278]\n",
      "epoch:27 step:25359 [D loss: 0.241590, acc.: 95.31%] [G loss: 3.105364]\n",
      "epoch:27 step:25360 [D loss: 0.223560, acc.: 95.31%] [G loss: 4.095209]\n",
      "epoch:27 step:25361 [D loss: 0.057709, acc.: 100.00%] [G loss: 3.222706]\n",
      "epoch:27 step:25362 [D loss: 0.103774, acc.: 99.22%] [G loss: 4.095709]\n",
      "epoch:27 step:25363 [D loss: 0.387894, acc.: 88.28%] [G loss: 3.615256]\n",
      "epoch:27 step:25364 [D loss: 0.149853, acc.: 98.44%] [G loss: 3.640108]\n",
      "epoch:27 step:25365 [D loss: 0.257241, acc.: 93.75%] [G loss: 2.629240]\n",
      "epoch:27 step:25366 [D loss: 0.724114, acc.: 56.25%] [G loss: 3.052652]\n",
      "epoch:27 step:25367 [D loss: 0.274011, acc.: 83.59%] [G loss: 1.377605]\n",
      "epoch:27 step:25368 [D loss: 0.213218, acc.: 92.97%] [G loss: 1.781389]\n",
      "epoch:27 step:25369 [D loss: 0.462067, acc.: 69.53%] [G loss: 0.755141]\n",
      "epoch:27 step:25370 [D loss: 0.787798, acc.: 54.69%] [G loss: 1.683829]\n",
      "epoch:27 step:25371 [D loss: 0.227437, acc.: 93.75%] [G loss: 4.080503]\n",
      "epoch:27 step:25372 [D loss: 0.047816, acc.: 100.00%] [G loss: 1.391780]\n",
      "epoch:27 step:25373 [D loss: 0.065736, acc.: 98.44%] [G loss: 1.355259]\n",
      "epoch:27 step:25374 [D loss: 0.168869, acc.: 97.66%] [G loss: 1.824332]\n",
      "epoch:27 step:25375 [D loss: 0.301863, acc.: 92.97%] [G loss: 3.202730]\n",
      "epoch:27 step:25376 [D loss: 1.726805, acc.: 22.66%] [G loss: 2.129818]\n",
      "epoch:27 step:25377 [D loss: 0.488208, acc.: 67.19%] [G loss: 3.832335]\n",
      "epoch:27 step:25378 [D loss: 0.095991, acc.: 99.22%] [G loss: 2.667808]\n",
      "epoch:27 step:25379 [D loss: 0.734344, acc.: 60.94%] [G loss: 5.327968]\n",
      "epoch:27 step:25380 [D loss: 0.199485, acc.: 96.09%] [G loss: 3.301103]\n",
      "epoch:27 step:25381 [D loss: 0.183198, acc.: 95.31%] [G loss: 1.428603]\n",
      "epoch:27 step:25382 [D loss: 0.196159, acc.: 96.09%] [G loss: 1.848456]\n",
      "epoch:27 step:25383 [D loss: 0.122190, acc.: 97.66%] [G loss: 3.129095]\n",
      "epoch:27 step:25384 [D loss: 0.320151, acc.: 85.94%] [G loss: 3.207286]\n",
      "epoch:27 step:25385 [D loss: 0.048737, acc.: 100.00%] [G loss: 0.456383]\n",
      "epoch:27 step:25386 [D loss: 0.314879, acc.: 89.84%] [G loss: 4.380393]\n",
      "epoch:27 step:25387 [D loss: 0.934449, acc.: 51.56%] [G loss: 1.268664]\n",
      "epoch:27 step:25388 [D loss: 0.386719, acc.: 79.69%] [G loss: 2.776530]\n",
      "epoch:27 step:25389 [D loss: 0.183105, acc.: 97.66%] [G loss: 2.648102]\n",
      "epoch:27 step:25390 [D loss: 0.100001, acc.: 100.00%] [G loss: 1.786073]\n",
      "epoch:27 step:25391 [D loss: 0.246613, acc.: 91.41%] [G loss: 3.471169]\n",
      "epoch:27 step:25392 [D loss: 0.605437, acc.: 63.28%] [G loss: 2.042877]\n",
      "epoch:27 step:25393 [D loss: 0.101771, acc.: 97.66%] [G loss: 3.261631]\n",
      "epoch:27 step:25394 [D loss: 0.475266, acc.: 79.69%] [G loss: 2.054162]\n",
      "epoch:27 step:25395 [D loss: 0.260152, acc.: 93.75%] [G loss: 2.565961]\n",
      "epoch:27 step:25396 [D loss: 0.360570, acc.: 80.47%] [G loss: 0.844521]\n",
      "epoch:27 step:25397 [D loss: 0.237919, acc.: 96.09%] [G loss: 4.881044]\n",
      "epoch:27 step:25398 [D loss: 1.014919, acc.: 42.97%] [G loss: 2.716731]\n",
      "epoch:27 step:25399 [D loss: 0.241162, acc.: 93.75%] [G loss: 3.650082]\n",
      "epoch:27 step:25400 [D loss: 0.903777, acc.: 53.12%] [G loss: 2.644065]\n",
      "epoch:27 step:25401 [D loss: 0.443921, acc.: 76.56%] [G loss: 1.702062]\n",
      "epoch:27 step:25402 [D loss: 0.959355, acc.: 54.69%] [G loss: 2.604269]\n",
      "epoch:27 step:25403 [D loss: 0.689881, acc.: 61.72%] [G loss: 3.012721]\n",
      "epoch:27 step:25404 [D loss: 0.125623, acc.: 97.66%] [G loss: 3.229062]\n",
      "epoch:27 step:25405 [D loss: 0.642368, acc.: 60.94%] [G loss: 1.330015]\n",
      "epoch:27 step:25406 [D loss: 0.564976, acc.: 71.09%] [G loss: 1.475792]\n",
      "epoch:27 step:25407 [D loss: 1.025747, acc.: 50.78%] [G loss: 0.559215]\n",
      "epoch:27 step:25408 [D loss: 0.265947, acc.: 90.62%] [G loss: 2.283461]\n",
      "epoch:27 step:25409 [D loss: 0.110575, acc.: 98.44%] [G loss: 1.511185]\n",
      "epoch:27 step:25410 [D loss: 0.267302, acc.: 91.41%] [G loss: 2.871732]\n",
      "epoch:27 step:25411 [D loss: 0.689568, acc.: 58.59%] [G loss: 2.574385]\n",
      "epoch:27 step:25412 [D loss: 0.999137, acc.: 39.84%] [G loss: 1.150069]\n",
      "epoch:27 step:25413 [D loss: 0.516297, acc.: 72.66%] [G loss: 2.914827]\n",
      "epoch:27 step:25414 [D loss: 0.302849, acc.: 88.28%] [G loss: 3.102632]\n",
      "epoch:27 step:25415 [D loss: 0.750842, acc.: 61.72%] [G loss: 1.002399]\n",
      "epoch:27 step:25416 [D loss: 0.585163, acc.: 65.62%] [G loss: 2.746752]\n",
      "epoch:27 step:25417 [D loss: 0.637539, acc.: 64.84%] [G loss: 3.823313]\n",
      "epoch:27 step:25418 [D loss: 0.222362, acc.: 92.97%] [G loss: 0.961634]\n",
      "epoch:27 step:25419 [D loss: 0.374002, acc.: 84.38%] [G loss: 0.934185]\n",
      "epoch:27 step:25420 [D loss: 0.521501, acc.: 67.97%] [G loss: 2.800427]\n",
      "epoch:27 step:25421 [D loss: 0.170262, acc.: 97.66%] [G loss: 2.132581]\n",
      "epoch:27 step:25422 [D loss: 0.149504, acc.: 96.09%] [G loss: 1.996816]\n",
      "epoch:27 step:25423 [D loss: 0.360407, acc.: 86.72%] [G loss: 2.451370]\n",
      "epoch:27 step:25424 [D loss: 0.317975, acc.: 88.28%] [G loss: 2.046890]\n",
      "epoch:27 step:25425 [D loss: 0.935414, acc.: 48.44%] [G loss: 3.677488]\n",
      "epoch:27 step:25426 [D loss: 0.503130, acc.: 76.56%] [G loss: 1.471071]\n",
      "epoch:27 step:25427 [D loss: 0.382701, acc.: 82.81%] [G loss: 1.929601]\n",
      "epoch:27 step:25428 [D loss: 0.390700, acc.: 82.81%] [G loss: 3.206914]\n",
      "epoch:27 step:25429 [D loss: 0.079864, acc.: 99.22%] [G loss: 1.553131]\n",
      "epoch:27 step:25430 [D loss: 0.442497, acc.: 78.91%] [G loss: 2.137968]\n",
      "epoch:27 step:25431 [D loss: 0.234833, acc.: 94.53%] [G loss: 1.950562]\n",
      "epoch:27 step:25432 [D loss: 0.433001, acc.: 71.09%] [G loss: 4.054588]\n",
      "epoch:27 step:25433 [D loss: 0.694120, acc.: 60.16%] [G loss: 3.365853]\n",
      "epoch:27 step:25434 [D loss: 0.318674, acc.: 90.62%] [G loss: 1.330227]\n",
      "epoch:27 step:25435 [D loss: 0.338008, acc.: 88.28%] [G loss: 2.034774]\n",
      "epoch:27 step:25436 [D loss: 0.151295, acc.: 100.00%] [G loss: 3.849016]\n",
      "epoch:27 step:25437 [D loss: 0.342727, acc.: 86.72%] [G loss: 2.463923]\n",
      "epoch:27 step:25438 [D loss: 0.459605, acc.: 74.22%] [G loss: 2.160678]\n",
      "epoch:27 step:25439 [D loss: 0.464355, acc.: 74.22%] [G loss: 0.786120]\n",
      "epoch:27 step:25440 [D loss: 0.707550, acc.: 60.16%] [G loss: 1.571651]\n",
      "epoch:27 step:25441 [D loss: 0.176499, acc.: 95.31%] [G loss: 3.007739]\n",
      "epoch:27 step:25442 [D loss: 0.249419, acc.: 93.75%] [G loss: 1.300287]\n",
      "epoch:27 step:25443 [D loss: 0.329101, acc.: 86.72%] [G loss: 1.371055]\n",
      "epoch:27 step:25444 [D loss: 0.132747, acc.: 97.66%] [G loss: 4.867745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25445 [D loss: 0.749733, acc.: 53.91%] [G loss: 3.263840]\n",
      "epoch:27 step:25446 [D loss: 0.147302, acc.: 96.88%] [G loss: 2.938074]\n",
      "epoch:27 step:25447 [D loss: 0.170675, acc.: 97.66%] [G loss: 1.890697]\n",
      "epoch:27 step:25448 [D loss: 0.275611, acc.: 90.62%] [G loss: 3.634384]\n",
      "epoch:27 step:25449 [D loss: 0.277309, acc.: 90.62%] [G loss: 3.102355]\n",
      "epoch:27 step:25450 [D loss: 0.304891, acc.: 89.84%] [G loss: 3.035649]\n",
      "epoch:27 step:25451 [D loss: 0.211181, acc.: 95.31%] [G loss: 2.134835]\n",
      "epoch:27 step:25452 [D loss: 0.303151, acc.: 92.19%] [G loss: 2.182429]\n",
      "epoch:27 step:25453 [D loss: 0.113563, acc.: 98.44%] [G loss: 3.387839]\n",
      "epoch:27 step:25454 [D loss: 0.456774, acc.: 81.25%] [G loss: 3.286135]\n",
      "epoch:27 step:25455 [D loss: 0.103822, acc.: 99.22%] [G loss: 2.484802]\n",
      "epoch:27 step:25456 [D loss: 0.510457, acc.: 72.66%] [G loss: 4.671257]\n",
      "epoch:27 step:25457 [D loss: 0.375928, acc.: 82.03%] [G loss: 2.346565]\n",
      "epoch:27 step:25458 [D loss: 0.209436, acc.: 94.53%] [G loss: 4.265372]\n",
      "epoch:27 step:25459 [D loss: 0.182859, acc.: 95.31%] [G loss: 0.915095]\n",
      "epoch:27 step:25460 [D loss: 0.313276, acc.: 89.84%] [G loss: 3.567774]\n",
      "epoch:27 step:25461 [D loss: 0.080183, acc.: 100.00%] [G loss: 1.925402]\n",
      "epoch:27 step:25462 [D loss: 0.518818, acc.: 71.88%] [G loss: 3.661498]\n",
      "epoch:27 step:25463 [D loss: 0.279299, acc.: 85.94%] [G loss: 3.925405]\n",
      "epoch:27 step:25464 [D loss: 0.340224, acc.: 92.19%] [G loss: 3.738275]\n",
      "epoch:27 step:25465 [D loss: 0.213590, acc.: 95.31%] [G loss: 3.499359]\n",
      "epoch:27 step:25466 [D loss: 0.321420, acc.: 85.94%] [G loss: 2.607287]\n",
      "epoch:27 step:25467 [D loss: 0.467398, acc.: 79.69%] [G loss: 1.116983]\n",
      "epoch:27 step:25468 [D loss: 1.297099, acc.: 22.66%] [G loss: 1.990406]\n",
      "epoch:27 step:25469 [D loss: 0.187243, acc.: 95.31%] [G loss: 3.522075]\n",
      "epoch:27 step:25470 [D loss: 0.653587, acc.: 64.84%] [G loss: 1.630099]\n",
      "epoch:27 step:25471 [D loss: 0.569926, acc.: 69.53%] [G loss: 1.905016]\n",
      "epoch:27 step:25472 [D loss: 0.439995, acc.: 78.12%] [G loss: 2.086158]\n",
      "epoch:27 step:25473 [D loss: 0.211857, acc.: 93.75%] [G loss: 1.755038]\n",
      "epoch:27 step:25474 [D loss: 0.230523, acc.: 94.53%] [G loss: 1.384371]\n",
      "epoch:27 step:25475 [D loss: 0.664245, acc.: 60.94%] [G loss: 0.410021]\n",
      "epoch:27 step:25476 [D loss: 0.787334, acc.: 59.38%] [G loss: 1.393465]\n",
      "epoch:27 step:25477 [D loss: 0.181925, acc.: 94.53%] [G loss: 1.562296]\n",
      "epoch:27 step:25478 [D loss: 0.442950, acc.: 75.78%] [G loss: 2.538207]\n",
      "epoch:27 step:25479 [D loss: 0.496718, acc.: 67.19%] [G loss: 2.628022]\n",
      "epoch:27 step:25480 [D loss: 0.250023, acc.: 93.75%] [G loss: 1.873405]\n",
      "epoch:27 step:25481 [D loss: 0.207697, acc.: 96.09%] [G loss: 2.420882]\n",
      "epoch:27 step:25482 [D loss: 0.910082, acc.: 50.78%] [G loss: 1.735624]\n",
      "epoch:27 step:25483 [D loss: 0.249965, acc.: 93.75%] [G loss: 1.803889]\n",
      "epoch:27 step:25484 [D loss: 0.358259, acc.: 86.72%] [G loss: 1.946789]\n",
      "epoch:27 step:25485 [D loss: 0.243325, acc.: 92.19%] [G loss: 1.960491]\n",
      "epoch:27 step:25486 [D loss: 0.368644, acc.: 82.03%] [G loss: 3.909833]\n",
      "epoch:27 step:25487 [D loss: 0.176035, acc.: 94.53%] [G loss: 0.897687]\n",
      "epoch:27 step:25488 [D loss: 0.390181, acc.: 83.59%] [G loss: 1.768960]\n",
      "epoch:27 step:25489 [D loss: 0.842207, acc.: 57.03%] [G loss: 1.545211]\n",
      "epoch:27 step:25490 [D loss: 0.179338, acc.: 95.31%] [G loss: 2.953964]\n",
      "epoch:27 step:25491 [D loss: 0.381672, acc.: 89.06%] [G loss: 1.363914]\n",
      "epoch:27 step:25492 [D loss: 0.106818, acc.: 97.66%] [G loss: 3.253336]\n",
      "epoch:27 step:25493 [D loss: 0.264046, acc.: 90.62%] [G loss: 3.844281]\n",
      "epoch:27 step:25494 [D loss: 0.473873, acc.: 72.66%] [G loss: 3.885919]\n",
      "epoch:27 step:25495 [D loss: 0.191225, acc.: 96.09%] [G loss: 3.835630]\n",
      "epoch:27 step:25496 [D loss: 0.356105, acc.: 85.94%] [G loss: 3.450101]\n",
      "epoch:27 step:25497 [D loss: 0.073520, acc.: 99.22%] [G loss: 3.547276]\n",
      "epoch:27 step:25498 [D loss: 0.169330, acc.: 96.09%] [G loss: 0.927315]\n",
      "epoch:27 step:25499 [D loss: 0.173390, acc.: 95.31%] [G loss: 2.921301]\n",
      "epoch:27 step:25500 [D loss: 0.166135, acc.: 97.66%] [G loss: 0.910046]\n",
      "epoch:27 step:25501 [D loss: 0.823774, acc.: 56.25%] [G loss: 1.982827]\n",
      "epoch:27 step:25502 [D loss: 0.467890, acc.: 71.88%] [G loss: 4.319375]\n",
      "epoch:27 step:25503 [D loss: 1.085454, acc.: 35.16%] [G loss: 3.785697]\n",
      "epoch:27 step:25504 [D loss: 0.290325, acc.: 86.72%] [G loss: 2.000981]\n",
      "epoch:27 step:25505 [D loss: 0.266763, acc.: 90.62%] [G loss: 2.715460]\n",
      "epoch:27 step:25506 [D loss: 1.275211, acc.: 27.34%] [G loss: 1.713471]\n",
      "epoch:27 step:25507 [D loss: 0.354775, acc.: 87.50%] [G loss: 1.725699]\n",
      "epoch:27 step:25508 [D loss: 0.672620, acc.: 62.50%] [G loss: 1.585475]\n",
      "epoch:27 step:25509 [D loss: 0.084564, acc.: 99.22%] [G loss: 3.910816]\n",
      "epoch:27 step:25510 [D loss: 0.516768, acc.: 69.53%] [G loss: 0.939257]\n",
      "epoch:27 step:25511 [D loss: 0.597250, acc.: 64.84%] [G loss: 0.683403]\n",
      "epoch:27 step:25512 [D loss: 0.897320, acc.: 56.25%] [G loss: 1.098102]\n",
      "epoch:27 step:25513 [D loss: 0.165163, acc.: 95.31%] [G loss: 2.308025]\n",
      "epoch:27 step:25514 [D loss: 0.383934, acc.: 79.69%] [G loss: 2.335374]\n",
      "epoch:27 step:25515 [D loss: 0.117078, acc.: 98.44%] [G loss: 1.879804]\n",
      "epoch:27 step:25516 [D loss: 0.152212, acc.: 96.88%] [G loss: 2.271432]\n",
      "epoch:27 step:25517 [D loss: 0.330482, acc.: 86.72%] [G loss: 2.609140]\n",
      "epoch:27 step:25518 [D loss: 0.319997, acc.: 84.38%] [G loss: 2.346051]\n",
      "epoch:27 step:25519 [D loss: 0.350994, acc.: 83.59%] [G loss: 1.970991]\n",
      "epoch:27 step:25520 [D loss: 0.953304, acc.: 42.19%] [G loss: 0.807092]\n",
      "epoch:27 step:25521 [D loss: 0.252704, acc.: 94.53%] [G loss: 3.446534]\n",
      "epoch:27 step:25522 [D loss: 0.801826, acc.: 56.25%] [G loss: 2.255825]\n",
      "epoch:27 step:25523 [D loss: 0.216405, acc.: 93.75%] [G loss: 2.514315]\n",
      "epoch:27 step:25524 [D loss: 0.366175, acc.: 81.25%] [G loss: 2.753879]\n",
      "epoch:27 step:25525 [D loss: 0.101980, acc.: 96.88%] [G loss: 3.798326]\n",
      "epoch:27 step:25526 [D loss: 0.290192, acc.: 91.41%] [G loss: 2.315503]\n",
      "epoch:27 step:25527 [D loss: 0.816863, acc.: 56.25%] [G loss: 0.797124]\n",
      "epoch:27 step:25528 [D loss: 0.222378, acc.: 97.66%] [G loss: 0.903630]\n",
      "epoch:27 step:25529 [D loss: 0.579822, acc.: 66.41%] [G loss: 2.189662]\n",
      "epoch:27 step:25530 [D loss: 0.061871, acc.: 99.22%] [G loss: 2.956874]\n",
      "epoch:27 step:25531 [D loss: 0.871031, acc.: 59.38%] [G loss: 2.177482]\n",
      "epoch:27 step:25532 [D loss: 0.524075, acc.: 71.09%] [G loss: 0.204954]\n",
      "epoch:27 step:25533 [D loss: 0.266255, acc.: 86.72%] [G loss: 0.675285]\n",
      "epoch:27 step:25534 [D loss: 0.412558, acc.: 85.94%] [G loss: 0.376447]\n",
      "epoch:27 step:25535 [D loss: 0.341149, acc.: 85.16%] [G loss: 3.061067]\n",
      "epoch:27 step:25536 [D loss: 0.367557, acc.: 88.28%] [G loss: 3.227945]\n",
      "epoch:27 step:25537 [D loss: 0.200363, acc.: 97.66%] [G loss: 3.148893]\n",
      "epoch:27 step:25538 [D loss: 0.555127, acc.: 73.44%] [G loss: 2.798811]\n",
      "epoch:27 step:25539 [D loss: 0.671198, acc.: 59.38%] [G loss: 1.464355]\n",
      "epoch:27 step:25540 [D loss: 0.239829, acc.: 90.62%] [G loss: 1.948435]\n",
      "epoch:27 step:25541 [D loss: 0.811172, acc.: 58.59%] [G loss: 0.859224]\n",
      "epoch:27 step:25542 [D loss: 0.433375, acc.: 80.47%] [G loss: 1.242813]\n",
      "epoch:27 step:25543 [D loss: 0.230351, acc.: 94.53%] [G loss: 3.338726]\n",
      "epoch:27 step:25544 [D loss: 0.104827, acc.: 99.22%] [G loss: 2.129445]\n",
      "epoch:27 step:25545 [D loss: 0.213390, acc.: 96.88%] [G loss: 0.602884]\n",
      "epoch:27 step:25546 [D loss: 0.692132, acc.: 56.25%] [G loss: 0.496839]\n",
      "epoch:27 step:25547 [D loss: 0.531342, acc.: 70.31%] [G loss: 2.302707]\n",
      "epoch:27 step:25548 [D loss: 1.171184, acc.: 52.34%] [G loss: 2.251796]\n",
      "epoch:27 step:25549 [D loss: 1.119976, acc.: 33.59%] [G loss: 2.112759]\n",
      "epoch:27 step:25550 [D loss: 0.287606, acc.: 92.97%] [G loss: 1.878917]\n",
      "epoch:27 step:25551 [D loss: 0.978306, acc.: 53.12%] [G loss: 2.217685]\n",
      "epoch:27 step:25552 [D loss: 0.499925, acc.: 74.22%] [G loss: 4.741491]\n",
      "epoch:27 step:25553 [D loss: 0.873006, acc.: 57.03%] [G loss: 1.351685]\n",
      "epoch:27 step:25554 [D loss: 0.652772, acc.: 60.16%] [G loss: 4.334563]\n",
      "epoch:27 step:25555 [D loss: 0.721759, acc.: 58.59%] [G loss: 1.170077]\n",
      "epoch:27 step:25556 [D loss: 1.030748, acc.: 30.47%] [G loss: 1.656067]\n",
      "epoch:27 step:25557 [D loss: 1.387866, acc.: 21.09%] [G loss: 0.961750]\n",
      "epoch:27 step:25558 [D loss: 0.544833, acc.: 77.34%] [G loss: 2.351308]\n",
      "epoch:27 step:25559 [D loss: 0.337150, acc.: 91.41%] [G loss: 3.035996]\n",
      "epoch:27 step:25560 [D loss: 0.208256, acc.: 97.66%] [G loss: 1.475305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25561 [D loss: 0.389863, acc.: 79.69%] [G loss: 0.780578]\n",
      "epoch:27 step:25562 [D loss: 0.431519, acc.: 78.12%] [G loss: 1.339513]\n",
      "epoch:27 step:25563 [D loss: 0.199131, acc.: 96.09%] [G loss: 1.792489]\n",
      "epoch:27 step:25564 [D loss: 0.108771, acc.: 100.00%] [G loss: 1.706953]\n",
      "epoch:27 step:25565 [D loss: 0.246997, acc.: 95.31%] [G loss: 1.641426]\n",
      "epoch:27 step:25566 [D loss: 0.276808, acc.: 93.75%] [G loss: 2.994656]\n",
      "epoch:27 step:25567 [D loss: 0.774754, acc.: 54.69%] [G loss: 1.386550]\n",
      "epoch:27 step:25568 [D loss: 0.909663, acc.: 53.91%] [G loss: 2.054302]\n",
      "epoch:27 step:25569 [D loss: 0.323716, acc.: 89.84%] [G loss: 3.244481]\n",
      "epoch:27 step:25570 [D loss: 0.297775, acc.: 92.19%] [G loss: 3.276293]\n",
      "epoch:27 step:25571 [D loss: 0.373265, acc.: 82.81%] [G loss: 1.959143]\n",
      "epoch:27 step:25572 [D loss: 0.329723, acc.: 91.41%] [G loss: 1.208092]\n",
      "epoch:27 step:25573 [D loss: 0.240433, acc.: 92.97%] [G loss: 1.411174]\n",
      "epoch:27 step:25574 [D loss: 0.341727, acc.: 92.97%] [G loss: 2.158123]\n",
      "epoch:27 step:25575 [D loss: 0.175397, acc.: 96.88%] [G loss: 3.099870]\n",
      "epoch:27 step:25576 [D loss: 0.288804, acc.: 90.62%] [G loss: 1.709036]\n",
      "epoch:27 step:25577 [D loss: 0.207002, acc.: 96.09%] [G loss: 2.067330]\n",
      "epoch:27 step:25578 [D loss: 0.235999, acc.: 98.44%] [G loss: 1.236819]\n",
      "epoch:27 step:25579 [D loss: 0.251391, acc.: 92.19%] [G loss: 1.506839]\n",
      "epoch:27 step:25580 [D loss: 0.228451, acc.: 92.19%] [G loss: 1.548442]\n",
      "epoch:27 step:25581 [D loss: 0.196700, acc.: 98.44%] [G loss: 2.970486]\n",
      "epoch:27 step:25582 [D loss: 0.199823, acc.: 96.09%] [G loss: 0.589845]\n",
      "epoch:27 step:25583 [D loss: 0.626582, acc.: 67.19%] [G loss: 1.925043]\n",
      "epoch:27 step:25584 [D loss: 0.754790, acc.: 65.62%] [G loss: 1.725123]\n",
      "epoch:27 step:25585 [D loss: 0.193887, acc.: 92.19%] [G loss: 2.935791]\n",
      "epoch:27 step:25586 [D loss: 0.576734, acc.: 67.19%] [G loss: 2.202971]\n",
      "epoch:27 step:25587 [D loss: 0.240838, acc.: 96.09%] [G loss: 1.079119]\n",
      "epoch:27 step:25588 [D loss: 0.276861, acc.: 92.97%] [G loss: 2.816207]\n",
      "epoch:27 step:25589 [D loss: 0.254118, acc.: 95.31%] [G loss: 1.199036]\n",
      "epoch:27 step:25590 [D loss: 0.208156, acc.: 96.09%] [G loss: 0.867286]\n",
      "epoch:27 step:25591 [D loss: 0.562383, acc.: 71.09%] [G loss: 2.022332]\n",
      "epoch:27 step:25592 [D loss: 0.326234, acc.: 86.72%] [G loss: 0.977019]\n",
      "epoch:27 step:25593 [D loss: 0.268656, acc.: 89.06%] [G loss: 1.654363]\n",
      "epoch:27 step:25594 [D loss: 0.407814, acc.: 82.81%] [G loss: 1.579407]\n",
      "epoch:27 step:25595 [D loss: 0.495275, acc.: 81.25%] [G loss: 0.404640]\n",
      "epoch:27 step:25596 [D loss: 0.678452, acc.: 64.06%] [G loss: 3.112375]\n",
      "epoch:27 step:25597 [D loss: 0.486365, acc.: 78.12%] [G loss: 2.440602]\n",
      "epoch:27 step:25598 [D loss: 0.084612, acc.: 98.44%] [G loss: 0.670308]\n",
      "epoch:27 step:25599 [D loss: 0.316872, acc.: 88.28%] [G loss: 2.313588]\n",
      "epoch:27 step:25600 [D loss: 0.295248, acc.: 96.88%] [G loss: 1.526486]\n",
      "epoch:27 step:25601 [D loss: 0.072408, acc.: 100.00%] [G loss: 5.400421]\n",
      "epoch:27 step:25602 [D loss: 0.215752, acc.: 97.66%] [G loss: 1.644075]\n",
      "epoch:27 step:25603 [D loss: 0.473775, acc.: 79.69%] [G loss: 1.093121]\n",
      "epoch:27 step:25604 [D loss: 0.153222, acc.: 97.66%] [G loss: 3.438655]\n",
      "epoch:27 step:25605 [D loss: 0.119287, acc.: 99.22%] [G loss: 2.273218]\n",
      "epoch:27 step:25606 [D loss: 0.122407, acc.: 99.22%] [G loss: 1.008241]\n",
      "epoch:27 step:25607 [D loss: 0.343479, acc.: 78.91%] [G loss: 0.606031]\n",
      "epoch:27 step:25608 [D loss: 0.565974, acc.: 68.75%] [G loss: 2.123112]\n",
      "epoch:27 step:25609 [D loss: 0.903642, acc.: 56.25%] [G loss: 3.316554]\n",
      "epoch:27 step:25610 [D loss: 0.179293, acc.: 96.09%] [G loss: 2.663800]\n",
      "epoch:27 step:25611 [D loss: 0.341834, acc.: 89.06%] [G loss: 0.863634]\n",
      "epoch:27 step:25612 [D loss: 0.337993, acc.: 90.62%] [G loss: 1.560087]\n",
      "epoch:27 step:25613 [D loss: 0.578630, acc.: 71.88%] [G loss: 0.786155]\n",
      "epoch:27 step:25614 [D loss: 0.685554, acc.: 59.38%] [G loss: 1.730144]\n",
      "epoch:27 step:25615 [D loss: 0.735642, acc.: 59.38%] [G loss: 4.854441]\n",
      "epoch:27 step:25616 [D loss: 1.048792, acc.: 39.06%] [G loss: 1.561550]\n",
      "epoch:27 step:25617 [D loss: 0.858213, acc.: 57.03%] [G loss: 4.250769]\n",
      "epoch:27 step:25618 [D loss: 0.645172, acc.: 65.62%] [G loss: 2.618702]\n",
      "epoch:27 step:25619 [D loss: 0.309531, acc.: 93.75%] [G loss: 3.527592]\n",
      "epoch:27 step:25620 [D loss: 0.410712, acc.: 83.59%] [G loss: 1.882523]\n",
      "epoch:27 step:25621 [D loss: 0.492036, acc.: 74.22%] [G loss: 1.410708]\n",
      "epoch:27 step:25622 [D loss: 0.426614, acc.: 79.69%] [G loss: 3.749098]\n",
      "epoch:27 step:25623 [D loss: 0.060858, acc.: 100.00%] [G loss: 1.168435]\n",
      "epoch:27 step:25624 [D loss: 0.436908, acc.: 81.25%] [G loss: 2.153841]\n",
      "epoch:27 step:25625 [D loss: 0.491823, acc.: 67.97%] [G loss: 2.677602]\n",
      "epoch:27 step:25626 [D loss: 0.252999, acc.: 95.31%] [G loss: 2.067710]\n",
      "epoch:27 step:25627 [D loss: 0.189301, acc.: 98.44%] [G loss: 2.697540]\n",
      "epoch:27 step:25628 [D loss: 0.673904, acc.: 60.16%] [G loss: 2.245676]\n",
      "epoch:27 step:25629 [D loss: 0.341926, acc.: 88.28%] [G loss: 2.486077]\n",
      "epoch:27 step:25630 [D loss: 0.208857, acc.: 95.31%] [G loss: 2.111855]\n",
      "epoch:27 step:25631 [D loss: 0.144792, acc.: 100.00%] [G loss: 2.252773]\n",
      "epoch:27 step:25632 [D loss: 0.134459, acc.: 96.88%] [G loss: 2.628039]\n",
      "epoch:27 step:25633 [D loss: 0.103928, acc.: 96.88%] [G loss: 0.360171]\n",
      "epoch:27 step:25634 [D loss: 0.432942, acc.: 80.47%] [G loss: 1.435946]\n",
      "epoch:27 step:25635 [D loss: 0.644757, acc.: 60.94%] [G loss: 1.003626]\n",
      "epoch:27 step:25636 [D loss: 0.445769, acc.: 79.69%] [G loss: 4.457346]\n",
      "epoch:27 step:25637 [D loss: 1.236471, acc.: 36.72%] [G loss: 1.496289]\n",
      "epoch:27 step:25638 [D loss: 0.355685, acc.: 84.38%] [G loss: 1.040030]\n",
      "epoch:27 step:25639 [D loss: 0.699719, acc.: 55.47%] [G loss: 1.056209]\n",
      "epoch:27 step:25640 [D loss: 0.247259, acc.: 92.97%] [G loss: 1.947543]\n",
      "epoch:27 step:25641 [D loss: 0.281723, acc.: 91.41%] [G loss: 1.864130]\n",
      "epoch:27 step:25642 [D loss: 0.316007, acc.: 89.06%] [G loss: 2.358741]\n",
      "epoch:27 step:25643 [D loss: 0.847203, acc.: 60.16%] [G loss: 3.556186]\n",
      "epoch:27 step:25644 [D loss: 0.074126, acc.: 98.44%] [G loss: 2.853004]\n",
      "epoch:27 step:25645 [D loss: 0.171765, acc.: 94.53%] [G loss: 4.978413]\n",
      "epoch:27 step:25646 [D loss: 0.429588, acc.: 82.81%] [G loss: 1.186416]\n",
      "epoch:27 step:25647 [D loss: 0.351628, acc.: 89.06%] [G loss: 3.533263]\n",
      "epoch:27 step:25648 [D loss: 0.260800, acc.: 94.53%] [G loss: 2.081408]\n",
      "epoch:27 step:25649 [D loss: 0.178391, acc.: 95.31%] [G loss: 3.077276]\n",
      "epoch:27 step:25650 [D loss: 0.787808, acc.: 50.78%] [G loss: 1.750898]\n",
      "epoch:27 step:25651 [D loss: 0.232650, acc.: 92.19%] [G loss: 2.797461]\n",
      "epoch:27 step:25652 [D loss: 0.308349, acc.: 82.81%] [G loss: 3.095232]\n",
      "epoch:27 step:25653 [D loss: 0.095785, acc.: 99.22%] [G loss: 3.267622]\n",
      "epoch:27 step:25654 [D loss: 0.205652, acc.: 96.09%] [G loss: 1.057182]\n",
      "epoch:27 step:25655 [D loss: 0.242677, acc.: 90.62%] [G loss: 3.642583]\n",
      "epoch:27 step:25656 [D loss: 0.578800, acc.: 65.62%] [G loss: 2.055337]\n",
      "epoch:27 step:25657 [D loss: 1.003596, acc.: 52.34%] [G loss: 3.891056]\n",
      "epoch:27 step:25658 [D loss: 0.418304, acc.: 82.03%] [G loss: 2.021846]\n",
      "epoch:27 step:25659 [D loss: 0.352336, acc.: 84.38%] [G loss: 2.797374]\n",
      "epoch:27 step:25660 [D loss: 0.295255, acc.: 88.28%] [G loss: 3.698743]\n",
      "epoch:27 step:25661 [D loss: 0.147441, acc.: 96.88%] [G loss: 2.607720]\n",
      "epoch:27 step:25662 [D loss: 0.326457, acc.: 85.16%] [G loss: 2.492928]\n",
      "epoch:27 step:25663 [D loss: 0.132168, acc.: 96.09%] [G loss: 4.875426]\n",
      "epoch:27 step:25664 [D loss: 0.278426, acc.: 92.19%] [G loss: 2.993525]\n",
      "epoch:27 step:25665 [D loss: 0.286242, acc.: 92.19%] [G loss: 2.086274]\n",
      "epoch:27 step:25666 [D loss: 0.569442, acc.: 70.31%] [G loss: 3.157732]\n",
      "epoch:27 step:25667 [D loss: 0.569011, acc.: 75.78%] [G loss: 1.134744]\n",
      "epoch:27 step:25668 [D loss: 0.178735, acc.: 96.88%] [G loss: 2.367192]\n",
      "epoch:27 step:25669 [D loss: 0.208210, acc.: 94.53%] [G loss: 1.117961]\n",
      "epoch:27 step:25670 [D loss: 1.041793, acc.: 44.53%] [G loss: 1.522799]\n",
      "epoch:27 step:25671 [D loss: 0.355651, acc.: 93.75%] [G loss: 0.981960]\n",
      "epoch:27 step:25672 [D loss: 0.106672, acc.: 98.44%] [G loss: 0.904264]\n",
      "epoch:27 step:25673 [D loss: 0.294617, acc.: 89.84%] [G loss: 1.772370]\n",
      "epoch:27 step:25674 [D loss: 0.309188, acc.: 89.06%] [G loss: 0.754144]\n",
      "epoch:27 step:25675 [D loss: 0.132154, acc.: 98.44%] [G loss: 2.836940]\n",
      "epoch:27 step:25676 [D loss: 0.285739, acc.: 89.84%] [G loss: 2.399973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25677 [D loss: 0.094031, acc.: 97.66%] [G loss: 3.633559]\n",
      "epoch:27 step:25678 [D loss: 1.082863, acc.: 28.91%] [G loss: 0.749220]\n",
      "epoch:27 step:25679 [D loss: 0.218140, acc.: 96.09%] [G loss: 1.908652]\n",
      "epoch:27 step:25680 [D loss: 0.150027, acc.: 98.44%] [G loss: 2.082273]\n",
      "epoch:27 step:25681 [D loss: 1.365007, acc.: 21.88%] [G loss: 2.070867]\n",
      "epoch:27 step:25682 [D loss: 0.075246, acc.: 99.22%] [G loss: 3.112477]\n",
      "epoch:27 step:25683 [D loss: 0.506011, acc.: 73.44%] [G loss: 3.540972]\n",
      "epoch:27 step:25684 [D loss: 0.169876, acc.: 95.31%] [G loss: 1.383015]\n",
      "epoch:27 step:25685 [D loss: 0.608660, acc.: 64.84%] [G loss: 1.593554]\n",
      "epoch:27 step:25686 [D loss: 0.246266, acc.: 89.06%] [G loss: 3.369582]\n",
      "epoch:27 step:25687 [D loss: 0.218921, acc.: 97.66%] [G loss: 3.703780]\n",
      "epoch:27 step:25688 [D loss: 0.532059, acc.: 67.97%] [G loss: 1.253171]\n",
      "epoch:27 step:25689 [D loss: 0.080606, acc.: 98.44%] [G loss: 2.839483]\n",
      "epoch:27 step:25690 [D loss: 0.418847, acc.: 78.12%] [G loss: 1.452055]\n",
      "epoch:27 step:25691 [D loss: 0.358631, acc.: 80.47%] [G loss: 3.155374]\n",
      "epoch:27 step:25692 [D loss: 0.324086, acc.: 91.41%] [G loss: 2.836910]\n",
      "epoch:27 step:25693 [D loss: 0.282027, acc.: 96.09%] [G loss: 1.470044]\n",
      "epoch:27 step:25694 [D loss: 0.191342, acc.: 94.53%] [G loss: 2.915353]\n",
      "epoch:27 step:25695 [D loss: 0.279400, acc.: 96.09%] [G loss: 1.859369]\n",
      "epoch:27 step:25696 [D loss: 0.364950, acc.: 91.41%] [G loss: 4.526558]\n",
      "epoch:27 step:25697 [D loss: 0.175535, acc.: 99.22%] [G loss: 2.247888]\n",
      "epoch:27 step:25698 [D loss: 0.208102, acc.: 97.66%] [G loss: 2.404720]\n",
      "epoch:27 step:25699 [D loss: 0.432473, acc.: 76.56%] [G loss: 2.615588]\n",
      "epoch:27 step:25700 [D loss: 0.551053, acc.: 70.31%] [G loss: 1.439912]\n",
      "epoch:27 step:25701 [D loss: 0.301449, acc.: 92.19%] [G loss: 3.774694]\n",
      "epoch:27 step:25702 [D loss: 0.252227, acc.: 89.06%] [G loss: 3.847110]\n",
      "epoch:27 step:25703 [D loss: 1.459539, acc.: 19.53%] [G loss: 1.618029]\n",
      "epoch:27 step:25704 [D loss: 0.325098, acc.: 85.94%] [G loss: 5.858755]\n",
      "epoch:27 step:25705 [D loss: 0.544102, acc.: 74.22%] [G loss: 4.069031]\n",
      "epoch:27 step:25706 [D loss: 0.276137, acc.: 88.28%] [G loss: 2.067063]\n",
      "epoch:27 step:25707 [D loss: 0.270720, acc.: 88.28%] [G loss: 0.854212]\n",
      "epoch:27 step:25708 [D loss: 0.631594, acc.: 64.06%] [G loss: 1.916276]\n",
      "epoch:27 step:25709 [D loss: 0.238113, acc.: 92.97%] [G loss: 2.719735]\n",
      "epoch:27 step:25710 [D loss: 0.149884, acc.: 98.44%] [G loss: 5.012359]\n",
      "epoch:27 step:25711 [D loss: 0.246834, acc.: 96.09%] [G loss: 2.405203]\n",
      "epoch:27 step:25712 [D loss: 0.200337, acc.: 92.97%] [G loss: 4.056553]\n",
      "epoch:27 step:25713 [D loss: 0.230381, acc.: 94.53%] [G loss: 1.150206]\n",
      "epoch:27 step:25714 [D loss: 0.255552, acc.: 92.19%] [G loss: 2.631228]\n",
      "epoch:27 step:25715 [D loss: 0.682078, acc.: 63.28%] [G loss: 3.797935]\n",
      "epoch:27 step:25716 [D loss: 1.266215, acc.: 23.44%] [G loss: 1.905610]\n",
      "epoch:27 step:25717 [D loss: 0.117940, acc.: 97.66%] [G loss: 3.044780]\n",
      "epoch:27 step:25718 [D loss: 1.342208, acc.: 53.91%] [G loss: 2.012189]\n",
      "epoch:27 step:25719 [D loss: 0.351471, acc.: 89.06%] [G loss: 0.863472]\n",
      "epoch:27 step:25720 [D loss: 0.842747, acc.: 57.03%] [G loss: 0.844786]\n",
      "epoch:27 step:25721 [D loss: 0.103385, acc.: 97.66%] [G loss: 0.477362]\n",
      "epoch:27 step:25722 [D loss: 0.266852, acc.: 92.97%] [G loss: 1.211693]\n",
      "epoch:27 step:25723 [D loss: 0.228524, acc.: 90.62%] [G loss: 1.474531]\n",
      "epoch:27 step:25724 [D loss: 0.816350, acc.: 57.03%] [G loss: 1.406422]\n",
      "epoch:27 step:25725 [D loss: 0.317497, acc.: 91.41%] [G loss: 2.612714]\n",
      "epoch:27 step:25726 [D loss: 0.251472, acc.: 96.09%] [G loss: 4.351664]\n",
      "epoch:27 step:25727 [D loss: 0.575910, acc.: 65.62%] [G loss: 2.754122]\n",
      "epoch:27 step:25728 [D loss: 0.820415, acc.: 53.12%] [G loss: 0.722625]\n",
      "epoch:27 step:25729 [D loss: 0.265261, acc.: 94.53%] [G loss: 1.318168]\n",
      "epoch:27 step:25730 [D loss: 0.917898, acc.: 48.44%] [G loss: 2.627747]\n",
      "epoch:27 step:25731 [D loss: 0.296536, acc.: 91.41%] [G loss: 1.236055]\n",
      "epoch:27 step:25732 [D loss: 0.198418, acc.: 97.66%] [G loss: 2.380918]\n",
      "epoch:27 step:25733 [D loss: 0.361990, acc.: 87.50%] [G loss: 2.226471]\n",
      "epoch:27 step:25734 [D loss: 0.220600, acc.: 96.88%] [G loss: 3.580082]\n",
      "epoch:27 step:25735 [D loss: 0.589666, acc.: 71.09%] [G loss: 1.819588]\n",
      "epoch:27 step:25736 [D loss: 0.832006, acc.: 52.34%] [G loss: 1.530124]\n",
      "epoch:27 step:25737 [D loss: 0.379412, acc.: 82.81%] [G loss: 2.595701]\n",
      "epoch:27 step:25738 [D loss: 0.386011, acc.: 87.50%] [G loss: 2.882551]\n",
      "epoch:27 step:25739 [D loss: 0.457362, acc.: 83.59%] [G loss: 2.946746]\n",
      "epoch:27 step:25740 [D loss: 0.262451, acc.: 92.97%] [G loss: 2.186868]\n",
      "epoch:27 step:25741 [D loss: 0.762511, acc.: 56.25%] [G loss: 4.869540]\n",
      "epoch:27 step:25742 [D loss: 0.139747, acc.: 96.88%] [G loss: 3.355324]\n",
      "epoch:27 step:25743 [D loss: 0.579549, acc.: 69.53%] [G loss: 2.208830]\n",
      "epoch:27 step:25744 [D loss: 0.233103, acc.: 92.97%] [G loss: 2.322028]\n",
      "epoch:27 step:25745 [D loss: 0.683478, acc.: 62.50%] [G loss: 0.410802]\n",
      "epoch:27 step:25746 [D loss: 0.691734, acc.: 58.59%] [G loss: 2.235374]\n",
      "epoch:27 step:25747 [D loss: 0.524677, acc.: 74.22%] [G loss: 2.695555]\n",
      "epoch:27 step:25748 [D loss: 0.236309, acc.: 95.31%] [G loss: 2.188507]\n",
      "epoch:27 step:25749 [D loss: 0.196446, acc.: 94.53%] [G loss: 3.210849]\n",
      "epoch:27 step:25750 [D loss: 1.032124, acc.: 48.44%] [G loss: 0.753820]\n",
      "epoch:27 step:25751 [D loss: 0.103682, acc.: 98.44%] [G loss: 1.703290]\n",
      "epoch:27 step:25752 [D loss: 0.072513, acc.: 98.44%] [G loss: 0.319892]\n",
      "epoch:27 step:25753 [D loss: 0.247090, acc.: 93.75%] [G loss: 1.120244]\n",
      "epoch:27 step:25754 [D loss: 0.709366, acc.: 57.03%] [G loss: 1.638723]\n",
      "epoch:27 step:25755 [D loss: 0.574165, acc.: 67.97%] [G loss: 2.713401]\n",
      "epoch:27 step:25756 [D loss: 0.685861, acc.: 60.16%] [G loss: 1.853454]\n",
      "epoch:27 step:25757 [D loss: 0.268198, acc.: 92.19%] [G loss: 2.684713]\n",
      "epoch:27 step:25758 [D loss: 0.320658, acc.: 88.28%] [G loss: 3.280468]\n",
      "epoch:27 step:25759 [D loss: 0.383658, acc.: 88.28%] [G loss: 1.407951]\n",
      "epoch:27 step:25760 [D loss: 0.315850, acc.: 88.28%] [G loss: 1.818567]\n",
      "epoch:27 step:25761 [D loss: 0.198108, acc.: 94.53%] [G loss: 1.263704]\n",
      "epoch:27 step:25762 [D loss: 0.295173, acc.: 89.84%] [G loss: 4.453085]\n",
      "epoch:27 step:25763 [D loss: 0.143724, acc.: 94.53%] [G loss: 2.407786]\n",
      "epoch:27 step:25764 [D loss: 0.835781, acc.: 57.03%] [G loss: 3.339198]\n",
      "epoch:27 step:25765 [D loss: 0.262674, acc.: 88.28%] [G loss: 2.085859]\n",
      "epoch:27 step:25766 [D loss: 0.248994, acc.: 93.75%] [G loss: 3.586733]\n",
      "epoch:27 step:25767 [D loss: 0.748497, acc.: 59.38%] [G loss: 1.288667]\n",
      "epoch:27 step:25768 [D loss: 0.378312, acc.: 78.91%] [G loss: 2.130454]\n",
      "epoch:27 step:25769 [D loss: 0.754411, acc.: 49.22%] [G loss: 1.464886]\n",
      "epoch:27 step:25770 [D loss: 0.629230, acc.: 58.59%] [G loss: 3.451631]\n",
      "epoch:27 step:25771 [D loss: 0.420000, acc.: 80.47%] [G loss: 2.053703]\n",
      "epoch:27 step:25772 [D loss: 0.101653, acc.: 96.88%] [G loss: 3.900148]\n",
      "epoch:27 step:25773 [D loss: 0.157421, acc.: 96.88%] [G loss: 3.316915]\n",
      "epoch:27 step:25774 [D loss: 0.123373, acc.: 98.44%] [G loss: 1.453894]\n",
      "epoch:27 step:25775 [D loss: 0.048290, acc.: 99.22%] [G loss: 2.329232]\n",
      "epoch:27 step:25776 [D loss: 0.203269, acc.: 98.44%] [G loss: 0.476924]\n",
      "epoch:27 step:25777 [D loss: 0.580168, acc.: 67.97%] [G loss: 2.266467]\n",
      "epoch:27 step:25778 [D loss: 0.469900, acc.: 77.34%] [G loss: 1.342623]\n",
      "epoch:27 step:25779 [D loss: 0.564168, acc.: 67.19%] [G loss: 4.389155]\n",
      "epoch:27 step:25780 [D loss: 0.888192, acc.: 46.09%] [G loss: 1.587331]\n",
      "epoch:27 step:25781 [D loss: 0.323335, acc.: 85.94%] [G loss: 2.874699]\n",
      "epoch:27 step:25782 [D loss: 0.356988, acc.: 85.94%] [G loss: 3.585176]\n",
      "epoch:27 step:25783 [D loss: 0.436629, acc.: 82.81%] [G loss: 2.407639]\n",
      "epoch:27 step:25784 [D loss: 0.078842, acc.: 98.44%] [G loss: 3.823806]\n",
      "epoch:27 step:25785 [D loss: 0.896207, acc.: 57.81%] [G loss: 2.566144]\n",
      "epoch:27 step:25786 [D loss: 0.177014, acc.: 94.53%] [G loss: 2.511988]\n",
      "epoch:27 step:25787 [D loss: 0.218908, acc.: 92.97%] [G loss: 3.232304]\n",
      "epoch:27 step:25788 [D loss: 0.793643, acc.: 55.47%] [G loss: 0.976580]\n",
      "epoch:27 step:25789 [D loss: 0.197796, acc.: 96.09%] [G loss: 5.121867]\n",
      "epoch:27 step:25790 [D loss: 1.251683, acc.: 50.00%] [G loss: 0.435805]\n",
      "epoch:27 step:25791 [D loss: 0.445635, acc.: 77.34%] [G loss: 1.403978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25792 [D loss: 0.289679, acc.: 92.97%] [G loss: 1.519095]\n",
      "epoch:27 step:25793 [D loss: 0.089114, acc.: 99.22%] [G loss: 1.950104]\n",
      "epoch:27 step:25794 [D loss: 0.265933, acc.: 89.06%] [G loss: 1.241199]\n",
      "epoch:27 step:25795 [D loss: 0.607131, acc.: 66.41%] [G loss: 0.498115]\n",
      "epoch:27 step:25796 [D loss: 0.376445, acc.: 80.47%] [G loss: 0.851379]\n",
      "epoch:27 step:25797 [D loss: 0.399163, acc.: 78.12%] [G loss: 1.876951]\n",
      "epoch:27 step:25798 [D loss: 0.287663, acc.: 90.62%] [G loss: 2.743724]\n",
      "epoch:27 step:25799 [D loss: 0.456269, acc.: 78.91%] [G loss: 3.587940]\n",
      "epoch:27 step:25800 [D loss: 0.885073, acc.: 47.66%] [G loss: 2.467661]\n",
      "epoch:27 step:25801 [D loss: 0.293089, acc.: 94.53%] [G loss: 2.779282]\n",
      "epoch:27 step:25802 [D loss: 0.961055, acc.: 48.44%] [G loss: 1.720162]\n",
      "epoch:27 step:25803 [D loss: 0.712047, acc.: 62.50%] [G loss: 2.085603]\n",
      "epoch:27 step:25804 [D loss: 0.084004, acc.: 98.44%] [G loss: 1.688211]\n",
      "epoch:27 step:25805 [D loss: 0.414410, acc.: 81.25%] [G loss: 5.185923]\n",
      "epoch:27 step:25806 [D loss: 0.862643, acc.: 50.78%] [G loss: 1.616812]\n",
      "epoch:27 step:25807 [D loss: 0.229946, acc.: 97.66%] [G loss: 5.441167]\n",
      "epoch:27 step:25808 [D loss: 0.247251, acc.: 92.19%] [G loss: 2.483764]\n",
      "epoch:27 step:25809 [D loss: 0.166144, acc.: 97.66%] [G loss: 1.865655]\n",
      "epoch:27 step:25810 [D loss: 0.919880, acc.: 56.25%] [G loss: 3.180484]\n",
      "epoch:27 step:25811 [D loss: 0.337956, acc.: 85.16%] [G loss: 3.858646]\n",
      "epoch:27 step:25812 [D loss: 0.396468, acc.: 81.25%] [G loss: 1.390511]\n",
      "epoch:27 step:25813 [D loss: 0.523114, acc.: 75.00%] [G loss: 2.388344]\n",
      "epoch:27 step:25814 [D loss: 0.456641, acc.: 85.16%] [G loss: 2.472878]\n",
      "epoch:27 step:25815 [D loss: 0.123580, acc.: 100.00%] [G loss: 2.042967]\n",
      "epoch:27 step:25816 [D loss: 0.171313, acc.: 96.09%] [G loss: 1.916294]\n",
      "epoch:27 step:25817 [D loss: 0.461566, acc.: 78.91%] [G loss: 3.649367]\n",
      "epoch:27 step:25818 [D loss: 0.182010, acc.: 98.44%] [G loss: 0.380766]\n",
      "epoch:27 step:25819 [D loss: 0.532809, acc.: 75.00%] [G loss: 1.137230]\n",
      "epoch:27 step:25820 [D loss: 0.241596, acc.: 92.19%] [G loss: 1.346085]\n",
      "epoch:27 step:25821 [D loss: 0.379867, acc.: 80.47%] [G loss: 2.266320]\n",
      "epoch:27 step:25822 [D loss: 0.186270, acc.: 98.44%] [G loss: 3.305756]\n",
      "epoch:27 step:25823 [D loss: 0.188900, acc.: 95.31%] [G loss: 1.358059]\n",
      "epoch:27 step:25824 [D loss: 0.107385, acc.: 96.88%] [G loss: 1.945074]\n",
      "epoch:27 step:25825 [D loss: 0.560260, acc.: 66.41%] [G loss: 1.892843]\n",
      "epoch:27 step:25826 [D loss: 1.081815, acc.: 49.22%] [G loss: 2.396905]\n",
      "epoch:27 step:25827 [D loss: 0.404972, acc.: 79.69%] [G loss: 2.687193]\n",
      "epoch:27 step:25828 [D loss: 0.267166, acc.: 94.53%] [G loss: 2.968812]\n",
      "epoch:27 step:25829 [D loss: 0.193313, acc.: 97.66%] [G loss: 2.082232]\n",
      "epoch:27 step:25830 [D loss: 0.122992, acc.: 100.00%] [G loss: 3.350365]\n",
      "epoch:27 step:25831 [D loss: 0.174026, acc.: 96.09%] [G loss: 2.421727]\n",
      "epoch:27 step:25832 [D loss: 0.145164, acc.: 96.88%] [G loss: 2.152686]\n",
      "epoch:27 step:25833 [D loss: 0.243598, acc.: 92.19%] [G loss: 4.102161]\n",
      "epoch:27 step:25834 [D loss: 0.141836, acc.: 98.44%] [G loss: 0.651594]\n",
      "epoch:27 step:25835 [D loss: 0.487541, acc.: 74.22%] [G loss: 0.673797]\n",
      "epoch:27 step:25836 [D loss: 0.072558, acc.: 100.00%] [G loss: 1.288273]\n",
      "epoch:27 step:25837 [D loss: 0.318468, acc.: 79.69%] [G loss: 3.414961]\n",
      "epoch:27 step:25838 [D loss: 0.420975, acc.: 73.44%] [G loss: 3.381976]\n",
      "epoch:27 step:25839 [D loss: 0.293250, acc.: 92.19%] [G loss: 2.348086]\n",
      "epoch:27 step:25840 [D loss: 0.313266, acc.: 86.72%] [G loss: 5.804513]\n",
      "epoch:27 step:25841 [D loss: 0.568606, acc.: 68.75%] [G loss: 3.960953]\n",
      "epoch:27 step:25842 [D loss: 0.072522, acc.: 100.00%] [G loss: 0.529156]\n",
      "epoch:27 step:25843 [D loss: 0.765517, acc.: 55.47%] [G loss: 4.834484]\n",
      "epoch:27 step:25844 [D loss: 0.141592, acc.: 98.44%] [G loss: 3.152650]\n",
      "epoch:27 step:25845 [D loss: 0.835633, acc.: 58.59%] [G loss: 0.433139]\n",
      "epoch:27 step:25846 [D loss: 1.295616, acc.: 38.28%] [G loss: 0.788706]\n",
      "epoch:27 step:25847 [D loss: 0.703327, acc.: 63.28%] [G loss: 3.102627]\n",
      "epoch:27 step:25848 [D loss: 0.604231, acc.: 64.06%] [G loss: 2.228271]\n",
      "epoch:27 step:25849 [D loss: 0.578903, acc.: 67.19%] [G loss: 1.190041]\n",
      "epoch:27 step:25850 [D loss: 0.697701, acc.: 53.12%] [G loss: 2.522091]\n",
      "epoch:27 step:25851 [D loss: 0.707766, acc.: 60.94%] [G loss: 3.179967]\n",
      "epoch:27 step:25852 [D loss: 0.220305, acc.: 94.53%] [G loss: 4.037908]\n",
      "epoch:27 step:25853 [D loss: 0.447087, acc.: 75.00%] [G loss: 1.796674]\n",
      "epoch:27 step:25854 [D loss: 0.390963, acc.: 88.28%] [G loss: 1.703506]\n",
      "epoch:27 step:25855 [D loss: 0.469947, acc.: 73.44%] [G loss: 2.610838]\n",
      "epoch:27 step:25856 [D loss: 0.261589, acc.: 93.75%] [G loss: 1.531983]\n",
      "epoch:27 step:25857 [D loss: 0.303317, acc.: 87.50%] [G loss: 2.457189]\n",
      "epoch:27 step:25858 [D loss: 0.429678, acc.: 86.72%] [G loss: 1.344866]\n",
      "epoch:27 step:25859 [D loss: 0.719109, acc.: 59.38%] [G loss: 4.454301]\n",
      "epoch:27 step:25860 [D loss: 0.343972, acc.: 89.06%] [G loss: 2.055120]\n",
      "epoch:27 step:25861 [D loss: 0.603568, acc.: 65.62%] [G loss: 1.362930]\n",
      "epoch:27 step:25862 [D loss: 0.324978, acc.: 89.06%] [G loss: 3.583182]\n",
      "epoch:27 step:25863 [D loss: 0.291194, acc.: 84.38%] [G loss: 2.040688]\n",
      "epoch:27 step:25864 [D loss: 0.951931, acc.: 53.91%] [G loss: 1.709594]\n",
      "epoch:27 step:25865 [D loss: 0.102278, acc.: 97.66%] [G loss: 3.518289]\n",
      "epoch:27 step:25866 [D loss: 0.269409, acc.: 89.06%] [G loss: 3.715310]\n",
      "epoch:27 step:25867 [D loss: 0.692604, acc.: 61.72%] [G loss: 1.652148]\n",
      "epoch:27 step:25868 [D loss: 0.331738, acc.: 83.59%] [G loss: 1.458746]\n",
      "epoch:27 step:25869 [D loss: 0.392453, acc.: 83.59%] [G loss: 1.560928]\n",
      "epoch:27 step:25870 [D loss: 0.113625, acc.: 98.44%] [G loss: 3.025649]\n",
      "epoch:27 step:25871 [D loss: 0.296056, acc.: 91.41%] [G loss: 1.822865]\n",
      "epoch:27 step:25872 [D loss: 0.264664, acc.: 90.62%] [G loss: 2.842443]\n",
      "epoch:27 step:25873 [D loss: 0.217017, acc.: 95.31%] [G loss: 2.405323]\n",
      "epoch:27 step:25874 [D loss: 0.171634, acc.: 95.31%] [G loss: 2.898097]\n",
      "epoch:27 step:25875 [D loss: 0.082932, acc.: 100.00%] [G loss: 1.849627]\n",
      "epoch:27 step:25876 [D loss: 0.506012, acc.: 71.88%] [G loss: 1.251656]\n",
      "epoch:27 step:25877 [D loss: 0.125012, acc.: 96.88%] [G loss: 3.044436]\n",
      "epoch:27 step:25878 [D loss: 0.238309, acc.: 90.62%] [G loss: 2.644743]\n",
      "epoch:27 step:25879 [D loss: 0.236684, acc.: 92.97%] [G loss: 3.864255]\n",
      "epoch:27 step:25880 [D loss: 0.172251, acc.: 95.31%] [G loss: 3.616621]\n",
      "epoch:27 step:25881 [D loss: 0.638356, acc.: 67.19%] [G loss: 2.346892]\n",
      "epoch:27 step:25882 [D loss: 0.330920, acc.: 90.62%] [G loss: 3.800725]\n",
      "epoch:27 step:25883 [D loss: 0.317945, acc.: 89.84%] [G loss: 4.038944]\n",
      "epoch:27 step:25884 [D loss: 0.406198, acc.: 85.16%] [G loss: 1.552523]\n",
      "epoch:27 step:25885 [D loss: 0.610978, acc.: 64.06%] [G loss: 3.977901]\n",
      "epoch:27 step:25886 [D loss: 0.883166, acc.: 55.47%] [G loss: 2.956960]\n",
      "epoch:27 step:25887 [D loss: 0.350571, acc.: 86.72%] [G loss: 3.062820]\n",
      "epoch:27 step:25888 [D loss: 0.386378, acc.: 80.47%] [G loss: 2.312676]\n",
      "epoch:27 step:25889 [D loss: 0.636068, acc.: 64.06%] [G loss: 5.214512]\n",
      "epoch:27 step:25890 [D loss: 0.239639, acc.: 92.19%] [G loss: 3.827805]\n",
      "epoch:27 step:25891 [D loss: 0.218361, acc.: 95.31%] [G loss: 2.804248]\n",
      "epoch:27 step:25892 [D loss: 0.150347, acc.: 97.66%] [G loss: 2.434516]\n",
      "epoch:27 step:25893 [D loss: 0.787551, acc.: 57.03%] [G loss: 2.767673]\n",
      "epoch:27 step:25894 [D loss: 0.086203, acc.: 97.66%] [G loss: 3.578137]\n",
      "epoch:27 step:25895 [D loss: 0.383250, acc.: 81.25%] [G loss: 2.372133]\n",
      "epoch:27 step:25896 [D loss: 0.269059, acc.: 90.62%] [G loss: 1.341373]\n",
      "epoch:27 step:25897 [D loss: 0.130462, acc.: 98.44%] [G loss: 0.458219]\n",
      "epoch:27 step:25898 [D loss: 0.126526, acc.: 99.22%] [G loss: 1.666613]\n",
      "epoch:27 step:25899 [D loss: 0.227534, acc.: 92.19%] [G loss: 1.275290]\n",
      "epoch:27 step:25900 [D loss: 1.236327, acc.: 28.12%] [G loss: 0.692139]\n",
      "epoch:27 step:25901 [D loss: 0.109157, acc.: 96.88%] [G loss: 3.444333]\n",
      "epoch:27 step:25902 [D loss: 0.458396, acc.: 73.44%] [G loss: 3.101935]\n",
      "epoch:27 step:25903 [D loss: 0.681177, acc.: 58.59%] [G loss: 1.913283]\n",
      "epoch:27 step:25904 [D loss: 0.582858, acc.: 68.75%] [G loss: 2.602536]\n",
      "epoch:27 step:25905 [D loss: 0.297592, acc.: 88.28%] [G loss: 0.860578]\n",
      "epoch:27 step:25906 [D loss: 0.629717, acc.: 67.97%] [G loss: 1.717834]\n",
      "epoch:27 step:25907 [D loss: 0.264944, acc.: 93.75%] [G loss: 3.295739]\n",
      "epoch:27 step:25908 [D loss: 0.857744, acc.: 56.25%] [G loss: 4.428232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25909 [D loss: 0.144574, acc.: 96.88%] [G loss: 3.944890]\n",
      "epoch:27 step:25910 [D loss: 1.422838, acc.: 39.84%] [G loss: 2.747239]\n",
      "epoch:27 step:25911 [D loss: 0.217852, acc.: 94.53%] [G loss: 0.628008]\n",
      "epoch:27 step:25912 [D loss: 0.584561, acc.: 68.75%] [G loss: 4.282302]\n",
      "epoch:27 step:25913 [D loss: 0.161085, acc.: 95.31%] [G loss: 4.190064]\n",
      "epoch:27 step:25914 [D loss: 0.776912, acc.: 60.94%] [G loss: 2.268208]\n",
      "epoch:27 step:25915 [D loss: 0.266575, acc.: 85.94%] [G loss: 3.908795]\n",
      "epoch:27 step:25916 [D loss: 0.225860, acc.: 96.88%] [G loss: 1.540741]\n",
      "epoch:27 step:25917 [D loss: 0.278885, acc.: 86.72%] [G loss: 4.639655]\n",
      "epoch:27 step:25918 [D loss: 0.795352, acc.: 60.94%] [G loss: 2.298444]\n",
      "epoch:27 step:25919 [D loss: 0.336035, acc.: 87.50%] [G loss: 2.355416]\n",
      "epoch:27 step:25920 [D loss: 0.766083, acc.: 59.38%] [G loss: 2.796715]\n",
      "epoch:27 step:25921 [D loss: 0.292441, acc.: 90.62%] [G loss: 4.292908]\n",
      "epoch:27 step:25922 [D loss: 0.525194, acc.: 71.09%] [G loss: 2.460039]\n",
      "epoch:27 step:25923 [D loss: 1.640069, acc.: 23.44%] [G loss: 0.573065]\n",
      "epoch:27 step:25924 [D loss: 0.271056, acc.: 85.16%] [G loss: 2.942604]\n",
      "epoch:27 step:25925 [D loss: 0.068655, acc.: 98.44%] [G loss: 1.569960]\n",
      "epoch:27 step:25926 [D loss: 0.049505, acc.: 100.00%] [G loss: 2.410215]\n",
      "epoch:27 step:25927 [D loss: 0.091562, acc.: 99.22%] [G loss: 2.977863]\n",
      "epoch:27 step:25928 [D loss: 0.408373, acc.: 75.00%] [G loss: 4.208038]\n",
      "epoch:27 step:25929 [D loss: 0.283428, acc.: 88.28%] [G loss: 2.399820]\n",
      "epoch:27 step:25930 [D loss: 0.412150, acc.: 77.34%] [G loss: 1.597213]\n",
      "epoch:27 step:25931 [D loss: 0.088867, acc.: 100.00%] [G loss: 4.269318]\n",
      "epoch:27 step:25932 [D loss: 0.320107, acc.: 88.28%] [G loss: 1.683846]\n",
      "epoch:27 step:25933 [D loss: 0.660755, acc.: 64.06%] [G loss: 3.323753]\n",
      "epoch:27 step:25934 [D loss: 0.513085, acc.: 78.12%] [G loss: 5.063411]\n",
      "epoch:27 step:25935 [D loss: 0.259272, acc.: 90.62%] [G loss: 4.630643]\n",
      "epoch:27 step:25936 [D loss: 0.224246, acc.: 92.97%] [G loss: 2.114872]\n",
      "epoch:27 step:25937 [D loss: 0.763942, acc.: 69.53%] [G loss: 3.680789]\n",
      "epoch:27 step:25938 [D loss: 0.290011, acc.: 89.84%] [G loss: 2.567190]\n",
      "epoch:27 step:25939 [D loss: 0.358673, acc.: 78.91%] [G loss: 2.968159]\n",
      "epoch:27 step:25940 [D loss: 0.418574, acc.: 78.12%] [G loss: 3.020650]\n",
      "epoch:27 step:25941 [D loss: 0.157202, acc.: 95.31%] [G loss: 2.052739]\n",
      "epoch:27 step:25942 [D loss: 0.312816, acc.: 94.53%] [G loss: 3.527256]\n",
      "epoch:27 step:25943 [D loss: 0.314984, acc.: 90.62%] [G loss: 2.430618]\n",
      "epoch:27 step:25944 [D loss: 0.270809, acc.: 92.19%] [G loss: 3.668851]\n",
      "epoch:27 step:25945 [D loss: 0.476721, acc.: 81.25%] [G loss: 2.001463]\n",
      "epoch:27 step:25946 [D loss: 0.251556, acc.: 93.75%] [G loss: 0.545220]\n",
      "epoch:27 step:25947 [D loss: 0.388859, acc.: 83.59%] [G loss: 2.425985]\n",
      "epoch:27 step:25948 [D loss: 0.215846, acc.: 95.31%] [G loss: 1.392881]\n",
      "epoch:27 step:25949 [D loss: 0.943937, acc.: 57.81%] [G loss: 2.463185]\n",
      "epoch:27 step:25950 [D loss: 0.864065, acc.: 57.03%] [G loss: 5.482303]\n",
      "epoch:27 step:25951 [D loss: 0.343485, acc.: 81.25%] [G loss: 3.076949]\n",
      "epoch:27 step:25952 [D loss: 0.911770, acc.: 53.12%] [G loss: 4.489652]\n",
      "epoch:27 step:25953 [D loss: 0.747072, acc.: 53.12%] [G loss: 3.121479]\n",
      "epoch:27 step:25954 [D loss: 0.218980, acc.: 92.19%] [G loss: 2.770764]\n",
      "epoch:27 step:25955 [D loss: 0.131709, acc.: 96.88%] [G loss: 2.255767]\n",
      "epoch:27 step:25956 [D loss: 1.136060, acc.: 55.47%] [G loss: 3.631108]\n",
      "epoch:27 step:25957 [D loss: 0.435311, acc.: 80.47%] [G loss: 1.677103]\n",
      "epoch:27 step:25958 [D loss: 0.188540, acc.: 94.53%] [G loss: 3.304325]\n",
      "epoch:27 step:25959 [D loss: 0.455745, acc.: 82.81%] [G loss: 3.196870]\n",
      "epoch:27 step:25960 [D loss: 0.259221, acc.: 94.53%] [G loss: 1.663182]\n",
      "epoch:27 step:25961 [D loss: 0.556274, acc.: 65.62%] [G loss: 1.968737]\n",
      "epoch:27 step:25962 [D loss: 0.186240, acc.: 94.53%] [G loss: 4.228509]\n",
      "epoch:27 step:25963 [D loss: 0.154890, acc.: 97.66%] [G loss: 3.507571]\n",
      "epoch:27 step:25964 [D loss: 0.765493, acc.: 61.72%] [G loss: 1.582775]\n",
      "epoch:27 step:25965 [D loss: 0.358728, acc.: 83.59%] [G loss: 1.523564]\n",
      "epoch:27 step:25966 [D loss: 0.295364, acc.: 89.06%] [G loss: 2.059834]\n",
      "epoch:27 step:25967 [D loss: 0.528433, acc.: 73.44%] [G loss: 2.558820]\n",
      "epoch:27 step:25968 [D loss: 0.489832, acc.: 71.88%] [G loss: 2.487808]\n",
      "epoch:27 step:25969 [D loss: 0.219502, acc.: 92.19%] [G loss: 4.014924]\n",
      "epoch:27 step:25970 [D loss: 0.198513, acc.: 93.75%] [G loss: 3.400784]\n",
      "epoch:27 step:25971 [D loss: 0.251647, acc.: 95.31%] [G loss: 1.110898]\n",
      "epoch:27 step:25972 [D loss: 0.470216, acc.: 75.78%] [G loss: 2.289336]\n",
      "epoch:27 step:25973 [D loss: 0.278635, acc.: 90.62%] [G loss: 2.556901]\n",
      "epoch:27 step:25974 [D loss: 0.534668, acc.: 79.69%] [G loss: 3.699331]\n",
      "epoch:27 step:25975 [D loss: 0.355238, acc.: 79.69%] [G loss: 2.948416]\n",
      "epoch:27 step:25976 [D loss: 0.243553, acc.: 92.19%] [G loss: 3.383375]\n",
      "epoch:27 step:25977 [D loss: 0.202566, acc.: 94.53%] [G loss: 2.251187]\n",
      "epoch:27 step:25978 [D loss: 0.217564, acc.: 95.31%] [G loss: 1.977609]\n",
      "epoch:27 step:25979 [D loss: 0.555823, acc.: 65.62%] [G loss: 1.708385]\n",
      "epoch:27 step:25980 [D loss: 0.403796, acc.: 84.38%] [G loss: 1.496723]\n",
      "epoch:27 step:25981 [D loss: 0.323349, acc.: 87.50%] [G loss: 2.494246]\n",
      "epoch:27 step:25982 [D loss: 0.101222, acc.: 98.44%] [G loss: 0.789472]\n",
      "epoch:27 step:25983 [D loss: 0.566254, acc.: 70.31%] [G loss: 2.605397]\n",
      "epoch:27 step:25984 [D loss: 0.132924, acc.: 97.66%] [G loss: 2.425117]\n",
      "epoch:27 step:25985 [D loss: 0.464318, acc.: 80.47%] [G loss: 1.605863]\n",
      "epoch:27 step:25986 [D loss: 0.267816, acc.: 87.50%] [G loss: 3.337566]\n",
      "epoch:27 step:25987 [D loss: 0.146241, acc.: 96.88%] [G loss: 2.811646]\n",
      "epoch:27 step:25988 [D loss: 0.877983, acc.: 57.81%] [G loss: 2.763786]\n",
      "epoch:27 step:25989 [D loss: 0.289658, acc.: 89.84%] [G loss: 1.399696]\n",
      "epoch:27 step:25990 [D loss: 0.046951, acc.: 100.00%] [G loss: 2.919687]\n",
      "epoch:27 step:25991 [D loss: 1.164101, acc.: 35.16%] [G loss: 1.834211]\n",
      "epoch:27 step:25992 [D loss: 0.519900, acc.: 67.97%] [G loss: 2.526642]\n",
      "epoch:27 step:25993 [D loss: 0.731361, acc.: 61.72%] [G loss: 2.735195]\n",
      "epoch:27 step:25994 [D loss: 0.425983, acc.: 84.38%] [G loss: 3.912504]\n",
      "epoch:27 step:25995 [D loss: 0.209174, acc.: 93.75%] [G loss: 1.421071]\n",
      "epoch:27 step:25996 [D loss: 1.265517, acc.: 25.78%] [G loss: 1.997922]\n",
      "epoch:27 step:25997 [D loss: 0.198714, acc.: 96.09%] [G loss: 1.890372]\n",
      "epoch:27 step:25998 [D loss: 0.546415, acc.: 67.19%] [G loss: 2.760150]\n",
      "epoch:27 step:25999 [D loss: 0.320239, acc.: 89.06%] [G loss: 2.660901]\n",
      "epoch:27 step:26000 [D loss: 0.711720, acc.: 59.38%] [G loss: 1.527442]\n",
      "epoch:27 step:26001 [D loss: 0.506761, acc.: 71.09%] [G loss: 1.275973]\n",
      "epoch:27 step:26002 [D loss: 0.229583, acc.: 96.09%] [G loss: 1.840914]\n",
      "epoch:27 step:26003 [D loss: 0.188788, acc.: 95.31%] [G loss: 1.463558]\n",
      "epoch:27 step:26004 [D loss: 0.590582, acc.: 68.75%] [G loss: 1.220176]\n",
      "epoch:27 step:26005 [D loss: 0.068706, acc.: 100.00%] [G loss: 2.087489]\n",
      "epoch:27 step:26006 [D loss: 0.070185, acc.: 99.22%] [G loss: 1.465671]\n",
      "epoch:27 step:26007 [D loss: 0.326166, acc.: 91.41%] [G loss: 4.460423]\n",
      "epoch:27 step:26008 [D loss: 1.143334, acc.: 27.34%] [G loss: 0.998914]\n",
      "epoch:27 step:26009 [D loss: 0.441770, acc.: 76.56%] [G loss: 1.975403]\n",
      "epoch:27 step:26010 [D loss: 0.334293, acc.: 89.06%] [G loss: 2.617100]\n",
      "epoch:27 step:26011 [D loss: 0.239778, acc.: 97.66%] [G loss: 3.072847]\n",
      "epoch:27 step:26012 [D loss: 0.400960, acc.: 83.59%] [G loss: 2.779814]\n",
      "epoch:27 step:26013 [D loss: 0.337408, acc.: 82.81%] [G loss: 1.911695]\n",
      "epoch:27 step:26014 [D loss: 0.243067, acc.: 92.19%] [G loss: 3.194678]\n",
      "epoch:27 step:26015 [D loss: 0.520262, acc.: 70.31%] [G loss: 4.466555]\n",
      "epoch:27 step:26016 [D loss: 0.376765, acc.: 82.81%] [G loss: 1.091126]\n",
      "epoch:27 step:26017 [D loss: 0.342640, acc.: 82.03%] [G loss: 2.614082]\n",
      "epoch:27 step:26018 [D loss: 0.215579, acc.: 94.53%] [G loss: 2.147164]\n",
      "epoch:27 step:26019 [D loss: 0.144861, acc.: 98.44%] [G loss: 1.585829]\n",
      "epoch:27 step:26020 [D loss: 0.242739, acc.: 92.97%] [G loss: 1.353031]\n",
      "epoch:27 step:26021 [D loss: 0.650254, acc.: 65.62%] [G loss: 1.329842]\n",
      "epoch:27 step:26022 [D loss: 1.047654, acc.: 53.91%] [G loss: 4.128601]\n",
      "epoch:27 step:26023 [D loss: 0.799948, acc.: 56.25%] [G loss: 2.962169]\n",
      "epoch:27 step:26024 [D loss: 1.281424, acc.: 31.25%] [G loss: 1.698029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26025 [D loss: 0.405481, acc.: 73.44%] [G loss: 2.146623]\n",
      "epoch:27 step:26026 [D loss: 0.141790, acc.: 97.66%] [G loss: 5.137283]\n",
      "epoch:27 step:26027 [D loss: 0.125903, acc.: 96.88%] [G loss: 2.603245]\n",
      "epoch:27 step:26028 [D loss: 0.445227, acc.: 75.00%] [G loss: 2.848251]\n",
      "epoch:27 step:26029 [D loss: 0.561880, acc.: 64.84%] [G loss: 1.706063]\n",
      "epoch:27 step:26030 [D loss: 0.664982, acc.: 64.84%] [G loss: 2.597477]\n",
      "epoch:27 step:26031 [D loss: 0.405144, acc.: 82.03%] [G loss: 1.522315]\n",
      "epoch:27 step:26032 [D loss: 0.121567, acc.: 98.44%] [G loss: 2.851207]\n",
      "epoch:27 step:26033 [D loss: 0.630256, acc.: 64.06%] [G loss: 2.594583]\n",
      "epoch:27 step:26034 [D loss: 0.235748, acc.: 95.31%] [G loss: 3.672758]\n",
      "epoch:27 step:26035 [D loss: 0.317125, acc.: 91.41%] [G loss: 1.354414]\n",
      "epoch:27 step:26036 [D loss: 0.259636, acc.: 94.53%] [G loss: 2.924314]\n",
      "epoch:27 step:26037 [D loss: 0.368464, acc.: 88.28%] [G loss: 3.746013]\n",
      "epoch:27 step:26038 [D loss: 0.200456, acc.: 97.66%] [G loss: 2.571396]\n",
      "epoch:27 step:26039 [D loss: 0.484638, acc.: 76.56%] [G loss: 1.977565]\n",
      "epoch:27 step:26040 [D loss: 0.598222, acc.: 67.19%] [G loss: 3.413553]\n",
      "epoch:27 step:26041 [D loss: 0.186441, acc.: 96.88%] [G loss: 3.259872]\n",
      "epoch:27 step:26042 [D loss: 0.116359, acc.: 99.22%] [G loss: 2.454862]\n",
      "epoch:27 step:26043 [D loss: 0.357070, acc.: 78.91%] [G loss: 2.848131]\n",
      "epoch:27 step:26044 [D loss: 0.724143, acc.: 55.47%] [G loss: 2.443212]\n",
      "epoch:27 step:26045 [D loss: 0.160355, acc.: 98.44%] [G loss: 5.126273]\n",
      "epoch:27 step:26046 [D loss: 0.128017, acc.: 96.88%] [G loss: 4.329763]\n",
      "epoch:27 step:26047 [D loss: 0.427477, acc.: 79.69%] [G loss: 3.515069]\n",
      "epoch:27 step:26048 [D loss: 0.245599, acc.: 92.19%] [G loss: 3.048914]\n",
      "epoch:27 step:26049 [D loss: 0.380086, acc.: 75.78%] [G loss: 1.691510]\n",
      "epoch:27 step:26050 [D loss: 0.522423, acc.: 75.78%] [G loss: 2.222511]\n",
      "epoch:27 step:26051 [D loss: 0.354054, acc.: 85.16%] [G loss: 2.931060]\n",
      "epoch:27 step:26052 [D loss: 0.119061, acc.: 97.66%] [G loss: 1.187923]\n",
      "epoch:27 step:26053 [D loss: 0.366648, acc.: 85.16%] [G loss: 3.585758]\n",
      "epoch:27 step:26054 [D loss: 0.125305, acc.: 98.44%] [G loss: 3.420927]\n",
      "epoch:27 step:26055 [D loss: 0.136756, acc.: 96.88%] [G loss: 1.969236]\n",
      "epoch:27 step:26056 [D loss: 1.267384, acc.: 39.84%] [G loss: 0.802842]\n",
      "epoch:27 step:26057 [D loss: 0.330995, acc.: 87.50%] [G loss: 1.830691]\n",
      "epoch:27 step:26058 [D loss: 0.218922, acc.: 95.31%] [G loss: 2.100085]\n",
      "epoch:27 step:26059 [D loss: 0.466468, acc.: 72.66%] [G loss: 0.433812]\n",
      "epoch:27 step:26060 [D loss: 0.192146, acc.: 96.09%] [G loss: 3.747418]\n",
      "epoch:27 step:26061 [D loss: 0.287377, acc.: 90.62%] [G loss: 2.059990]\n",
      "epoch:27 step:26062 [D loss: 0.220062, acc.: 94.53%] [G loss: 2.690538]\n",
      "epoch:27 step:26063 [D loss: 0.306557, acc.: 84.38%] [G loss: 3.198062]\n",
      "epoch:27 step:26064 [D loss: 0.481935, acc.: 79.69%] [G loss: 3.119032]\n",
      "epoch:27 step:26065 [D loss: 0.490662, acc.: 78.91%] [G loss: 3.113443]\n",
      "epoch:27 step:26066 [D loss: 0.149314, acc.: 98.44%] [G loss: 3.146492]\n",
      "epoch:27 step:26067 [D loss: 0.256235, acc.: 94.53%] [G loss: 3.830228]\n",
      "epoch:27 step:26068 [D loss: 0.393216, acc.: 85.94%] [G loss: 1.412921]\n",
      "epoch:27 step:26069 [D loss: 0.184281, acc.: 96.09%] [G loss: 2.810472]\n",
      "epoch:27 step:26070 [D loss: 0.500132, acc.: 75.78%] [G loss: 1.225051]\n",
      "epoch:27 step:26071 [D loss: 0.237107, acc.: 89.84%] [G loss: 1.566259]\n",
      "epoch:27 step:26072 [D loss: 0.428168, acc.: 78.12%] [G loss: 1.682680]\n",
      "epoch:27 step:26073 [D loss: 0.144829, acc.: 98.44%] [G loss: 1.291965]\n",
      "epoch:27 step:26074 [D loss: 0.533691, acc.: 77.34%] [G loss: 2.642056]\n",
      "epoch:27 step:26075 [D loss: 0.443005, acc.: 82.03%] [G loss: 2.907835]\n",
      "epoch:27 step:26076 [D loss: 0.525734, acc.: 67.97%] [G loss: 3.456163]\n",
      "epoch:27 step:26077 [D loss: 0.091179, acc.: 98.44%] [G loss: 3.619499]\n",
      "epoch:27 step:26078 [D loss: 0.311280, acc.: 89.84%] [G loss: 4.172054]\n",
      "epoch:27 step:26079 [D loss: 0.423871, acc.: 77.34%] [G loss: 3.336564]\n",
      "epoch:27 step:26080 [D loss: 0.282793, acc.: 92.19%] [G loss: 1.920335]\n",
      "epoch:27 step:26081 [D loss: 0.209906, acc.: 92.97%] [G loss: 2.550806]\n",
      "epoch:27 step:26082 [D loss: 0.259516, acc.: 89.84%] [G loss: 3.519267]\n",
      "epoch:27 step:26083 [D loss: 0.679134, acc.: 60.16%] [G loss: 1.898142]\n",
      "epoch:27 step:26084 [D loss: 0.393226, acc.: 81.25%] [G loss: 2.806785]\n",
      "epoch:27 step:26085 [D loss: 0.156583, acc.: 96.09%] [G loss: 4.819075]\n",
      "epoch:27 step:26086 [D loss: 0.197775, acc.: 95.31%] [G loss: 3.003984]\n",
      "epoch:27 step:26087 [D loss: 0.381842, acc.: 82.03%] [G loss: 3.609558]\n",
      "epoch:27 step:26088 [D loss: 0.182862, acc.: 97.66%] [G loss: 2.549505]\n",
      "epoch:27 step:26089 [D loss: 0.647642, acc.: 68.75%] [G loss: 2.722074]\n",
      "epoch:27 step:26090 [D loss: 0.069049, acc.: 99.22%] [G loss: 1.898410]\n",
      "epoch:27 step:26091 [D loss: 0.341237, acc.: 80.47%] [G loss: 3.669533]\n",
      "epoch:27 step:26092 [D loss: 0.084928, acc.: 98.44%] [G loss: 10.550547]\n",
      "epoch:27 step:26093 [D loss: 0.588787, acc.: 71.09%] [G loss: 3.139402]\n",
      "epoch:27 step:26094 [D loss: 0.548185, acc.: 70.31%] [G loss: 1.949081]\n",
      "epoch:27 step:26095 [D loss: 0.100093, acc.: 98.44%] [G loss: 2.923704]\n",
      "epoch:27 step:26096 [D loss: 0.426916, acc.: 71.88%] [G loss: 2.110270]\n",
      "epoch:27 step:26097 [D loss: 0.741697, acc.: 64.06%] [G loss: 3.131669]\n",
      "epoch:27 step:26098 [D loss: 0.307088, acc.: 88.28%] [G loss: 3.235880]\n",
      "epoch:27 step:26099 [D loss: 0.155221, acc.: 94.53%] [G loss: 1.789404]\n",
      "epoch:27 step:26100 [D loss: 0.105977, acc.: 98.44%] [G loss: 1.210226]\n",
      "epoch:27 step:26101 [D loss: 0.603218, acc.: 66.41%] [G loss: 2.169670]\n",
      "epoch:27 step:26102 [D loss: 0.570556, acc.: 64.06%] [G loss: 4.729750]\n",
      "epoch:27 step:26103 [D loss: 0.169139, acc.: 96.88%] [G loss: 4.800309]\n",
      "epoch:27 step:26104 [D loss: 0.373028, acc.: 82.81%] [G loss: 3.187782]\n",
      "epoch:27 step:26105 [D loss: 0.119626, acc.: 96.88%] [G loss: 4.713904]\n",
      "epoch:27 step:26106 [D loss: 0.112285, acc.: 98.44%] [G loss: 3.552659]\n",
      "epoch:27 step:26107 [D loss: 0.793991, acc.: 43.75%] [G loss: 2.364571]\n",
      "epoch:27 step:26108 [D loss: 0.181596, acc.: 96.88%] [G loss: 4.383575]\n",
      "epoch:27 step:26109 [D loss: 0.628560, acc.: 64.06%] [G loss: 2.536626]\n",
      "epoch:27 step:26110 [D loss: 0.352512, acc.: 83.59%] [G loss: 2.054440]\n",
      "epoch:27 step:26111 [D loss: 0.215614, acc.: 94.53%] [G loss: 1.749682]\n",
      "epoch:27 step:26112 [D loss: 0.592317, acc.: 69.53%] [G loss: 1.838607]\n",
      "epoch:27 step:26113 [D loss: 0.344299, acc.: 85.16%] [G loss: 0.567847]\n",
      "epoch:27 step:26114 [D loss: 0.321407, acc.: 85.16%] [G loss: 2.330486]\n",
      "epoch:27 step:26115 [D loss: 0.802483, acc.: 55.47%] [G loss: 3.188094]\n",
      "epoch:27 step:26116 [D loss: 0.252519, acc.: 94.53%] [G loss: 0.753686]\n",
      "epoch:27 step:26117 [D loss: 0.821991, acc.: 50.78%] [G loss: 4.370658]\n",
      "epoch:27 step:26118 [D loss: 0.268142, acc.: 93.75%] [G loss: 3.300886]\n",
      "epoch:27 step:26119 [D loss: 0.404335, acc.: 82.03%] [G loss: 3.806177]\n",
      "epoch:27 step:26120 [D loss: 0.302634, acc.: 88.28%] [G loss: 0.780394]\n",
      "epoch:27 step:26121 [D loss: 0.429096, acc.: 82.03%] [G loss: 3.222256]\n",
      "epoch:27 step:26122 [D loss: 0.456468, acc.: 75.78%] [G loss: 2.358020]\n",
      "epoch:27 step:26123 [D loss: 0.419420, acc.: 78.12%] [G loss: 2.724518]\n",
      "epoch:27 step:26124 [D loss: 0.337080, acc.: 89.84%] [G loss: 3.296424]\n",
      "epoch:27 step:26125 [D loss: 0.270419, acc.: 92.19%] [G loss: 2.819767]\n",
      "epoch:27 step:26126 [D loss: 0.100669, acc.: 98.44%] [G loss: 3.588292]\n",
      "epoch:27 step:26127 [D loss: 0.092243, acc.: 97.66%] [G loss: 0.913153]\n",
      "epoch:27 step:26128 [D loss: 0.345382, acc.: 82.03%] [G loss: 2.482896]\n",
      "epoch:27 step:26129 [D loss: 0.348394, acc.: 82.81%] [G loss: 2.277745]\n",
      "epoch:27 step:26130 [D loss: 0.237452, acc.: 90.62%] [G loss: 1.879337]\n",
      "epoch:27 step:26131 [D loss: 1.484996, acc.: 28.91%] [G loss: 2.079512]\n",
      "epoch:27 step:26132 [D loss: 0.612694, acc.: 69.53%] [G loss: 1.584692]\n",
      "epoch:27 step:26133 [D loss: 0.097616, acc.: 99.22%] [G loss: 4.207515]\n",
      "epoch:27 step:26134 [D loss: 0.714749, acc.: 55.47%] [G loss: 3.747978]\n",
      "epoch:27 step:26135 [D loss: 0.239092, acc.: 92.19%] [G loss: 3.996059]\n",
      "epoch:27 step:26136 [D loss: 0.390025, acc.: 85.94%] [G loss: 4.637744]\n",
      "epoch:27 step:26137 [D loss: 0.189971, acc.: 96.09%] [G loss: 1.106881]\n",
      "epoch:27 step:26138 [D loss: 0.194354, acc.: 96.88%] [G loss: 2.674206]\n",
      "epoch:27 step:26139 [D loss: 0.121443, acc.: 97.66%] [G loss: 2.211096]\n",
      "epoch:27 step:26140 [D loss: 0.133516, acc.: 96.88%] [G loss: 4.384912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26141 [D loss: 0.152877, acc.: 96.09%] [G loss: 4.376235]\n",
      "epoch:27 step:26142 [D loss: 0.232366, acc.: 91.41%] [G loss: 3.738185]\n",
      "epoch:27 step:26143 [D loss: 0.151635, acc.: 99.22%] [G loss: 4.332110]\n",
      "epoch:27 step:26144 [D loss: 0.288697, acc.: 88.28%] [G loss: 1.951440]\n",
      "epoch:27 step:26145 [D loss: 0.238521, acc.: 94.53%] [G loss: 2.687158]\n",
      "epoch:27 step:26146 [D loss: 0.468676, acc.: 78.12%] [G loss: 3.502195]\n",
      "epoch:27 step:26147 [D loss: 0.205496, acc.: 96.09%] [G loss: 1.892722]\n",
      "epoch:27 step:26148 [D loss: 0.174458, acc.: 97.66%] [G loss: 2.661658]\n",
      "epoch:27 step:26149 [D loss: 0.189931, acc.: 95.31%] [G loss: 1.960596]\n",
      "epoch:27 step:26150 [D loss: 0.127961, acc.: 97.66%] [G loss: 2.726282]\n",
      "epoch:27 step:26151 [D loss: 0.059824, acc.: 100.00%] [G loss: 1.447574]\n",
      "epoch:27 step:26152 [D loss: 0.419508, acc.: 78.12%] [G loss: 4.873776]\n",
      "epoch:27 step:26153 [D loss: 0.239152, acc.: 92.19%] [G loss: 3.124419]\n",
      "epoch:27 step:26154 [D loss: 0.118936, acc.: 98.44%] [G loss: 4.014311]\n",
      "epoch:27 step:26155 [D loss: 0.194597, acc.: 96.09%] [G loss: 2.264057]\n",
      "epoch:27 step:26156 [D loss: 0.209740, acc.: 95.31%] [G loss: 3.514318]\n",
      "epoch:27 step:26157 [D loss: 1.097306, acc.: 40.62%] [G loss: 1.917315]\n",
      "epoch:27 step:26158 [D loss: 0.197824, acc.: 91.41%] [G loss: 4.650383]\n",
      "epoch:27 step:26159 [D loss: 0.183779, acc.: 97.66%] [G loss: 3.472871]\n",
      "epoch:27 step:26160 [D loss: 0.152842, acc.: 96.09%] [G loss: 3.783153]\n",
      "epoch:27 step:26161 [D loss: 0.209557, acc.: 93.75%] [G loss: 0.578239]\n",
      "epoch:27 step:26162 [D loss: 0.101710, acc.: 99.22%] [G loss: 1.335448]\n",
      "epoch:27 step:26163 [D loss: 0.135886, acc.: 99.22%] [G loss: 2.630965]\n",
      "epoch:27 step:26164 [D loss: 0.064069, acc.: 99.22%] [G loss: 3.675690]\n",
      "epoch:27 step:26165 [D loss: 0.394246, acc.: 78.12%] [G loss: 4.937557]\n",
      "epoch:27 step:26166 [D loss: 0.296631, acc.: 81.25%] [G loss: 1.935725]\n",
      "epoch:27 step:26167 [D loss: 0.310017, acc.: 87.50%] [G loss: 1.654863]\n",
      "epoch:27 step:26168 [D loss: 0.061084, acc.: 99.22%] [G loss: 5.208730]\n",
      "epoch:27 step:26169 [D loss: 0.389225, acc.: 83.59%] [G loss: 0.891660]\n",
      "epoch:27 step:26170 [D loss: 0.049679, acc.: 99.22%] [G loss: 4.676457]\n",
      "epoch:27 step:26171 [D loss: 0.224580, acc.: 95.31%] [G loss: 2.899832]\n",
      "epoch:27 step:26172 [D loss: 0.128935, acc.: 98.44%] [G loss: 5.028686]\n",
      "epoch:27 step:26173 [D loss: 1.493570, acc.: 51.56%] [G loss: 1.938686]\n",
      "epoch:27 step:26174 [D loss: 0.299112, acc.: 93.75%] [G loss: 3.079491]\n",
      "epoch:27 step:26175 [D loss: 0.070166, acc.: 99.22%] [G loss: 2.224264]\n",
      "epoch:27 step:26176 [D loss: 0.071961, acc.: 99.22%] [G loss: 3.099628]\n",
      "epoch:27 step:26177 [D loss: 0.117101, acc.: 96.88%] [G loss: 1.233378]\n",
      "epoch:27 step:26178 [D loss: 0.534867, acc.: 64.06%] [G loss: 3.687920]\n",
      "epoch:27 step:26179 [D loss: 0.431498, acc.: 78.91%] [G loss: 4.728917]\n",
      "epoch:27 step:26180 [D loss: 0.224341, acc.: 90.62%] [G loss: 3.085724]\n",
      "epoch:27 step:26181 [D loss: 0.122481, acc.: 97.66%] [G loss: 4.120795]\n",
      "epoch:27 step:26182 [D loss: 0.454926, acc.: 75.78%] [G loss: 2.486147]\n",
      "epoch:27 step:26183 [D loss: 0.504254, acc.: 70.31%] [G loss: 4.221347]\n",
      "epoch:27 step:26184 [D loss: 0.191077, acc.: 93.75%] [G loss: 2.918709]\n",
      "epoch:27 step:26185 [D loss: 0.117396, acc.: 98.44%] [G loss: 3.811741]\n",
      "epoch:27 step:26186 [D loss: 0.208603, acc.: 97.66%] [G loss: 3.390402]\n",
      "epoch:27 step:26187 [D loss: 0.155809, acc.: 96.09%] [G loss: 5.266488]\n",
      "epoch:27 step:26188 [D loss: 0.186972, acc.: 96.88%] [G loss: 3.235638]\n",
      "epoch:27 step:26189 [D loss: 0.106294, acc.: 100.00%] [G loss: 1.713788]\n",
      "epoch:27 step:26190 [D loss: 1.203293, acc.: 53.12%] [G loss: 1.913477]\n",
      "epoch:27 step:26191 [D loss: 0.202279, acc.: 96.88%] [G loss: 3.907537]\n",
      "epoch:27 step:26192 [D loss: 0.157600, acc.: 95.31%] [G loss: 4.818868]\n",
      "epoch:27 step:26193 [D loss: 0.119047, acc.: 96.09%] [G loss: 3.222427]\n",
      "epoch:27 step:26194 [D loss: 0.392684, acc.: 78.12%] [G loss: 4.658915]\n",
      "epoch:27 step:26195 [D loss: 0.211846, acc.: 95.31%] [G loss: 4.072979]\n",
      "epoch:27 step:26196 [D loss: 0.328552, acc.: 91.41%] [G loss: 0.941913]\n",
      "epoch:27 step:26197 [D loss: 0.303233, acc.: 83.59%] [G loss: 3.562406]\n",
      "epoch:27 step:26198 [D loss: 0.061930, acc.: 98.44%] [G loss: 4.504463]\n",
      "epoch:27 step:26199 [D loss: 0.620736, acc.: 64.06%] [G loss: 6.430344]\n",
      "epoch:27 step:26200 [D loss: 0.366796, acc.: 76.56%] [G loss: 3.463244]\n",
      "epoch:27 step:26201 [D loss: 0.066274, acc.: 99.22%] [G loss: 2.008771]\n",
      "epoch:27 step:26202 [D loss: 0.174284, acc.: 97.66%] [G loss: 3.225487]\n",
      "epoch:27 step:26203 [D loss: 0.127354, acc.: 98.44%] [G loss: 3.403092]\n",
      "epoch:27 step:26204 [D loss: 1.347110, acc.: 19.53%] [G loss: 2.496883]\n",
      "epoch:27 step:26205 [D loss: 0.441718, acc.: 85.16%] [G loss: 4.421546]\n",
      "epoch:27 step:26206 [D loss: 1.135712, acc.: 38.28%] [G loss: 3.250763]\n",
      "epoch:27 step:26207 [D loss: 0.297345, acc.: 85.16%] [G loss: 2.297094]\n",
      "epoch:27 step:26208 [D loss: 0.096619, acc.: 96.88%] [G loss: 2.230215]\n",
      "epoch:27 step:26209 [D loss: 0.058944, acc.: 100.00%] [G loss: 1.875119]\n",
      "epoch:27 step:26210 [D loss: 0.239403, acc.: 88.28%] [G loss: 1.280762]\n",
      "epoch:27 step:26211 [D loss: 0.097422, acc.: 99.22%] [G loss: 1.660895]\n",
      "epoch:27 step:26212 [D loss: 0.090652, acc.: 98.44%] [G loss: 2.357547]\n",
      "epoch:27 step:26213 [D loss: 0.386476, acc.: 78.12%] [G loss: 4.650038]\n",
      "epoch:27 step:26214 [D loss: 0.141394, acc.: 96.88%] [G loss: 4.990487]\n",
      "epoch:27 step:26215 [D loss: 0.124973, acc.: 96.88%] [G loss: 4.093225]\n",
      "epoch:27 step:26216 [D loss: 0.083670, acc.: 100.00%] [G loss: 5.084262]\n",
      "epoch:27 step:26217 [D loss: 1.712265, acc.: 16.41%] [G loss: 0.938756]\n",
      "epoch:27 step:26218 [D loss: 0.702253, acc.: 64.84%] [G loss: 2.371811]\n",
      "epoch:27 step:26219 [D loss: 0.983186, acc.: 54.69%] [G loss: 2.641741]\n",
      "epoch:27 step:26220 [D loss: 0.251885, acc.: 90.62%] [G loss: 1.749082]\n",
      "epoch:27 step:26221 [D loss: 0.265865, acc.: 92.97%] [G loss: 2.762083]\n",
      "epoch:27 step:26222 [D loss: 0.139030, acc.: 98.44%] [G loss: 2.481696]\n",
      "epoch:27 step:26223 [D loss: 0.482815, acc.: 79.69%] [G loss: 1.784159]\n",
      "epoch:27 step:26224 [D loss: 0.193344, acc.: 94.53%] [G loss: 2.387553]\n",
      "epoch:27 step:26225 [D loss: 0.751662, acc.: 54.69%] [G loss: 2.473269]\n",
      "epoch:27 step:26226 [D loss: 0.382896, acc.: 83.59%] [G loss: 2.969470]\n",
      "epoch:27 step:26227 [D loss: 0.078114, acc.: 98.44%] [G loss: 4.373314]\n",
      "epoch:27 step:26228 [D loss: 0.473702, acc.: 77.34%] [G loss: 1.467198]\n",
      "epoch:27 step:26229 [D loss: 0.188583, acc.: 96.88%] [G loss: 2.358230]\n",
      "epoch:27 step:26230 [D loss: 0.533982, acc.: 71.09%] [G loss: 3.208581]\n",
      "epoch:27 step:26231 [D loss: 0.169798, acc.: 96.88%] [G loss: 1.308846]\n",
      "epoch:27 step:26232 [D loss: 0.063695, acc.: 98.44%] [G loss: 2.019756]\n",
      "epoch:27 step:26233 [D loss: 0.651752, acc.: 62.50%] [G loss: 1.819468]\n",
      "epoch:27 step:26234 [D loss: 0.224459, acc.: 94.53%] [G loss: 0.637016]\n",
      "epoch:27 step:26235 [D loss: 0.147386, acc.: 97.66%] [G loss: 3.018384]\n",
      "epoch:27 step:26236 [D loss: 0.409863, acc.: 82.03%] [G loss: 3.418423]\n",
      "epoch:28 step:26237 [D loss: 0.743892, acc.: 57.81%] [G loss: 4.073557]\n",
      "epoch:28 step:26238 [D loss: 0.053512, acc.: 100.00%] [G loss: 0.956602]\n",
      "epoch:28 step:26239 [D loss: 0.282486, acc.: 89.84%] [G loss: 2.956200]\n",
      "epoch:28 step:26240 [D loss: 0.150076, acc.: 93.75%] [G loss: 2.711199]\n",
      "epoch:28 step:26241 [D loss: 0.208440, acc.: 92.19%] [G loss: 2.106142]\n",
      "epoch:28 step:26242 [D loss: 0.179835, acc.: 94.53%] [G loss: 1.173913]\n",
      "epoch:28 step:26243 [D loss: 0.067517, acc.: 100.00%] [G loss: 4.556415]\n",
      "epoch:28 step:26244 [D loss: 0.205908, acc.: 97.66%] [G loss: 4.450372]\n",
      "epoch:28 step:26245 [D loss: 0.699032, acc.: 59.38%] [G loss: 2.516641]\n",
      "epoch:28 step:26246 [D loss: 0.143467, acc.: 95.31%] [G loss: 1.997044]\n",
      "epoch:28 step:26247 [D loss: 0.264522, acc.: 87.50%] [G loss: 0.793092]\n",
      "epoch:28 step:26248 [D loss: 0.240149, acc.: 89.84%] [G loss: 1.731559]\n",
      "epoch:28 step:26249 [D loss: 0.354920, acc.: 87.50%] [G loss: 3.230490]\n",
      "epoch:28 step:26250 [D loss: 0.415263, acc.: 72.66%] [G loss: 4.521165]\n",
      "epoch:28 step:26251 [D loss: 0.458371, acc.: 73.44%] [G loss: 1.795808]\n",
      "epoch:28 step:26252 [D loss: 0.405570, acc.: 76.56%] [G loss: 1.894021]\n",
      "epoch:28 step:26253 [D loss: 0.358143, acc.: 89.06%] [G loss: 4.144855]\n",
      "epoch:28 step:26254 [D loss: 0.078676, acc.: 99.22%] [G loss: 3.263573]\n",
      "epoch:28 step:26255 [D loss: 0.183467, acc.: 96.88%] [G loss: 3.779801]\n",
      "epoch:28 step:26256 [D loss: 0.076919, acc.: 98.44%] [G loss: 2.331816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26257 [D loss: 0.108755, acc.: 98.44%] [G loss: 2.200522]\n",
      "epoch:28 step:26258 [D loss: 0.564037, acc.: 72.66%] [G loss: 3.108025]\n",
      "epoch:28 step:26259 [D loss: 0.083313, acc.: 98.44%] [G loss: 4.511440]\n",
      "epoch:28 step:26260 [D loss: 0.095799, acc.: 98.44%] [G loss: 2.428294]\n",
      "epoch:28 step:26261 [D loss: 0.109620, acc.: 98.44%] [G loss: 3.179240]\n",
      "epoch:28 step:26262 [D loss: 0.216578, acc.: 94.53%] [G loss: 2.529029]\n",
      "epoch:28 step:26263 [D loss: 0.134182, acc.: 98.44%] [G loss: 2.746836]\n",
      "epoch:28 step:26264 [D loss: 0.141631, acc.: 98.44%] [G loss: 3.173886]\n",
      "epoch:28 step:26265 [D loss: 0.934281, acc.: 48.44%] [G loss: 1.381088]\n",
      "epoch:28 step:26266 [D loss: 0.544906, acc.: 71.09%] [G loss: 3.442718]\n",
      "epoch:28 step:26267 [D loss: 0.295072, acc.: 89.84%] [G loss: 3.511450]\n",
      "epoch:28 step:26268 [D loss: 0.150634, acc.: 99.22%] [G loss: 2.194370]\n",
      "epoch:28 step:26269 [D loss: 0.530486, acc.: 73.44%] [G loss: 2.860685]\n",
      "epoch:28 step:26270 [D loss: 0.573774, acc.: 71.09%] [G loss: 3.815949]\n",
      "epoch:28 step:26271 [D loss: 0.415714, acc.: 77.34%] [G loss: 3.768432]\n",
      "epoch:28 step:26272 [D loss: 0.180719, acc.: 96.09%] [G loss: 2.009066]\n",
      "epoch:28 step:26273 [D loss: 0.244459, acc.: 96.09%] [G loss: 4.034328]\n",
      "epoch:28 step:26274 [D loss: 0.293123, acc.: 85.94%] [G loss: 3.343446]\n",
      "epoch:28 step:26275 [D loss: 0.603528, acc.: 67.19%] [G loss: 4.058035]\n",
      "epoch:28 step:26276 [D loss: 0.221126, acc.: 89.84%] [G loss: 2.579484]\n",
      "epoch:28 step:26277 [D loss: 0.537549, acc.: 69.53%] [G loss: 3.422858]\n",
      "epoch:28 step:26278 [D loss: 0.440690, acc.: 78.12%] [G loss: 1.749431]\n",
      "epoch:28 step:26279 [D loss: 0.302101, acc.: 87.50%] [G loss: 2.044924]\n",
      "epoch:28 step:26280 [D loss: 0.235317, acc.: 93.75%] [G loss: 3.319155]\n",
      "epoch:28 step:26281 [D loss: 0.077900, acc.: 98.44%] [G loss: 3.578860]\n",
      "epoch:28 step:26282 [D loss: 0.190748, acc.: 93.75%] [G loss: 4.575398]\n",
      "epoch:28 step:26283 [D loss: 0.876500, acc.: 57.03%] [G loss: 3.584831]\n",
      "epoch:28 step:26284 [D loss: 0.744299, acc.: 57.81%] [G loss: 1.791024]\n",
      "epoch:28 step:26285 [D loss: 0.328820, acc.: 82.03%] [G loss: 1.595157]\n",
      "epoch:28 step:26286 [D loss: 0.117164, acc.: 98.44%] [G loss: 1.103712]\n",
      "epoch:28 step:26287 [D loss: 0.297723, acc.: 88.28%] [G loss: 3.075216]\n",
      "epoch:28 step:26288 [D loss: 0.428844, acc.: 78.91%] [G loss: 1.865875]\n",
      "epoch:28 step:26289 [D loss: 0.441906, acc.: 78.12%] [G loss: 1.419794]\n",
      "epoch:28 step:26290 [D loss: 0.144324, acc.: 97.66%] [G loss: 0.616093]\n",
      "epoch:28 step:26291 [D loss: 0.132398, acc.: 97.66%] [G loss: 1.521094]\n",
      "epoch:28 step:26292 [D loss: 0.398766, acc.: 84.38%] [G loss: 3.470972]\n",
      "epoch:28 step:26293 [D loss: 0.837697, acc.: 50.00%] [G loss: 1.875783]\n",
      "epoch:28 step:26294 [D loss: 0.965096, acc.: 61.72%] [G loss: 5.185740]\n",
      "epoch:28 step:26295 [D loss: 0.365961, acc.: 78.91%] [G loss: 4.698086]\n",
      "epoch:28 step:26296 [D loss: 0.833176, acc.: 55.47%] [G loss: 4.869495]\n",
      "epoch:28 step:26297 [D loss: 0.169476, acc.: 97.66%] [G loss: 5.280643]\n",
      "epoch:28 step:26298 [D loss: 0.435076, acc.: 81.25%] [G loss: 2.452108]\n",
      "epoch:28 step:26299 [D loss: 0.244090, acc.: 92.97%] [G loss: 4.007467]\n",
      "epoch:28 step:26300 [D loss: 0.626158, acc.: 64.06%] [G loss: 1.544105]\n",
      "epoch:28 step:26301 [D loss: 0.239925, acc.: 98.44%] [G loss: 3.414899]\n",
      "epoch:28 step:26302 [D loss: 0.098557, acc.: 99.22%] [G loss: 2.176125]\n",
      "epoch:28 step:26303 [D loss: 0.381207, acc.: 75.00%] [G loss: 3.108283]\n",
      "epoch:28 step:26304 [D loss: 0.500364, acc.: 77.34%] [G loss: 4.009574]\n",
      "epoch:28 step:26305 [D loss: 0.220561, acc.: 92.97%] [G loss: 4.321710]\n",
      "epoch:28 step:26306 [D loss: 0.080044, acc.: 99.22%] [G loss: 3.868872]\n",
      "epoch:28 step:26307 [D loss: 0.161228, acc.: 93.75%] [G loss: 2.176804]\n",
      "epoch:28 step:26308 [D loss: 0.207915, acc.: 96.09%] [G loss: 2.452573]\n",
      "epoch:28 step:26309 [D loss: 0.045151, acc.: 100.00%] [G loss: 1.529372]\n",
      "epoch:28 step:26310 [D loss: 0.347573, acc.: 81.25%] [G loss: 3.427029]\n",
      "epoch:28 step:26311 [D loss: 0.403940, acc.: 82.03%] [G loss: 3.926601]\n",
      "epoch:28 step:26312 [D loss: 0.295383, acc.: 88.28%] [G loss: 2.295288]\n",
      "epoch:28 step:26313 [D loss: 0.346631, acc.: 86.72%] [G loss: 2.785665]\n",
      "epoch:28 step:26314 [D loss: 0.054683, acc.: 100.00%] [G loss: 2.733181]\n",
      "epoch:28 step:26315 [D loss: 0.179022, acc.: 94.53%] [G loss: 4.006860]\n",
      "epoch:28 step:26316 [D loss: 0.466294, acc.: 81.25%] [G loss: 2.379576]\n",
      "epoch:28 step:26317 [D loss: 0.044652, acc.: 99.22%] [G loss: 3.847790]\n",
      "epoch:28 step:26318 [D loss: 0.340729, acc.: 86.72%] [G loss: 1.508551]\n",
      "epoch:28 step:26319 [D loss: 0.097277, acc.: 98.44%] [G loss: 3.275547]\n",
      "epoch:28 step:26320 [D loss: 0.342840, acc.: 89.06%] [G loss: 1.707515]\n",
      "epoch:28 step:26321 [D loss: 0.225672, acc.: 93.75%] [G loss: 2.984159]\n",
      "epoch:28 step:26322 [D loss: 0.030048, acc.: 99.22%] [G loss: 6.445804]\n",
      "epoch:28 step:26323 [D loss: 0.209233, acc.: 93.75%] [G loss: 4.041586]\n",
      "epoch:28 step:26324 [D loss: 0.286332, acc.: 89.06%] [G loss: 2.147894]\n",
      "epoch:28 step:26325 [D loss: 1.641518, acc.: 29.69%] [G loss: 0.730445]\n",
      "epoch:28 step:26326 [D loss: 0.336994, acc.: 87.50%] [G loss: 3.262972]\n",
      "epoch:28 step:26327 [D loss: 0.108560, acc.: 97.66%] [G loss: 2.083333]\n",
      "epoch:28 step:26328 [D loss: 0.142695, acc.: 98.44%] [G loss: 4.862474]\n",
      "epoch:28 step:26329 [D loss: 0.593992, acc.: 63.28%] [G loss: 2.638750]\n",
      "epoch:28 step:26330 [D loss: 0.489252, acc.: 71.09%] [G loss: 4.330284]\n",
      "epoch:28 step:26331 [D loss: 0.397542, acc.: 85.16%] [G loss: 2.925951]\n",
      "epoch:28 step:26332 [D loss: 0.244973, acc.: 92.97%] [G loss: 1.467054]\n",
      "epoch:28 step:26333 [D loss: 0.319245, acc.: 82.03%] [G loss: 3.176363]\n",
      "epoch:28 step:26334 [D loss: 0.151312, acc.: 96.09%] [G loss: 4.736239]\n",
      "epoch:28 step:26335 [D loss: 0.272642, acc.: 89.06%] [G loss: 4.382549]\n",
      "epoch:28 step:26336 [D loss: 0.195816, acc.: 96.88%] [G loss: 1.305825]\n",
      "epoch:28 step:26337 [D loss: 0.180011, acc.: 95.31%] [G loss: 1.755787]\n",
      "epoch:28 step:26338 [D loss: 0.113715, acc.: 96.09%] [G loss: 3.068015]\n",
      "epoch:28 step:26339 [D loss: 0.071862, acc.: 98.44%] [G loss: 1.300267]\n",
      "epoch:28 step:26340 [D loss: 1.172669, acc.: 34.38%] [G loss: 3.417238]\n",
      "epoch:28 step:26341 [D loss: 0.823630, acc.: 56.25%] [G loss: 4.153484]\n",
      "epoch:28 step:26342 [D loss: 0.610167, acc.: 68.75%] [G loss: 1.817880]\n",
      "epoch:28 step:26343 [D loss: 0.194662, acc.: 95.31%] [G loss: 3.831295]\n",
      "epoch:28 step:26344 [D loss: 0.271065, acc.: 89.06%] [G loss: 1.145088]\n",
      "epoch:28 step:26345 [D loss: 0.481357, acc.: 73.44%] [G loss: 3.830099]\n",
      "epoch:28 step:26346 [D loss: 0.021000, acc.: 100.00%] [G loss: 3.388533]\n",
      "epoch:28 step:26347 [D loss: 0.709920, acc.: 57.81%] [G loss: 4.762244]\n",
      "epoch:28 step:26348 [D loss: 0.892806, acc.: 46.88%] [G loss: 3.914246]\n",
      "epoch:28 step:26349 [D loss: 1.360706, acc.: 50.00%] [G loss: 2.611094]\n",
      "epoch:28 step:26350 [D loss: 0.058520, acc.: 99.22%] [G loss: 1.143488]\n",
      "epoch:28 step:26351 [D loss: 0.344141, acc.: 83.59%] [G loss: 4.803047]\n",
      "epoch:28 step:26352 [D loss: 0.143217, acc.: 96.09%] [G loss: 2.427857]\n",
      "epoch:28 step:26353 [D loss: 0.062477, acc.: 99.22%] [G loss: 3.250994]\n",
      "epoch:28 step:26354 [D loss: 0.092242, acc.: 96.88%] [G loss: 0.525377]\n",
      "epoch:28 step:26355 [D loss: 1.203224, acc.: 50.78%] [G loss: 2.690583]\n",
      "epoch:28 step:26356 [D loss: 0.758262, acc.: 50.78%] [G loss: 3.480810]\n",
      "epoch:28 step:26357 [D loss: 0.316532, acc.: 89.06%] [G loss: 2.065405]\n",
      "epoch:28 step:26358 [D loss: 0.879011, acc.: 59.38%] [G loss: 3.846295]\n",
      "epoch:28 step:26359 [D loss: 0.299971, acc.: 93.75%] [G loss: 3.159910]\n",
      "epoch:28 step:26360 [D loss: 0.220099, acc.: 93.75%] [G loss: 1.523263]\n",
      "epoch:28 step:26361 [D loss: 0.288031, acc.: 88.28%] [G loss: 2.942473]\n",
      "epoch:28 step:26362 [D loss: 0.501584, acc.: 67.97%] [G loss: 3.684822]\n",
      "epoch:28 step:26363 [D loss: 0.634708, acc.: 62.50%] [G loss: 2.448009]\n",
      "epoch:28 step:26364 [D loss: 0.232797, acc.: 92.19%] [G loss: 3.721539]\n",
      "epoch:28 step:26365 [D loss: 0.142612, acc.: 96.09%] [G loss: 3.423203]\n",
      "epoch:28 step:26366 [D loss: 0.694727, acc.: 64.06%] [G loss: 1.494028]\n",
      "epoch:28 step:26367 [D loss: 0.208384, acc.: 93.75%] [G loss: 1.830520]\n",
      "epoch:28 step:26368 [D loss: 0.412441, acc.: 77.34%] [G loss: 1.095081]\n",
      "epoch:28 step:26369 [D loss: 0.329611, acc.: 79.69%] [G loss: 5.020868]\n",
      "epoch:28 step:26370 [D loss: 0.899225, acc.: 42.97%] [G loss: 2.799405]\n",
      "epoch:28 step:26371 [D loss: 0.137262, acc.: 97.66%] [G loss: 2.994969]\n",
      "epoch:28 step:26372 [D loss: 0.100330, acc.: 97.66%] [G loss: 1.451293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26373 [D loss: 1.248412, acc.: 50.00%] [G loss: 1.431154]\n",
      "epoch:28 step:26374 [D loss: 0.534693, acc.: 66.41%] [G loss: 0.727015]\n",
      "epoch:28 step:26375 [D loss: 0.193871, acc.: 92.97%] [G loss: 0.677252]\n",
      "epoch:28 step:26376 [D loss: 0.236942, acc.: 89.84%] [G loss: 2.275935]\n",
      "epoch:28 step:26377 [D loss: 0.110876, acc.: 96.88%] [G loss: 5.901474]\n",
      "epoch:28 step:26378 [D loss: 0.418890, acc.: 74.22%] [G loss: 3.174093]\n",
      "epoch:28 step:26379 [D loss: 0.174357, acc.: 96.88%] [G loss: 6.508295]\n",
      "epoch:28 step:26380 [D loss: 0.169103, acc.: 97.66%] [G loss: 2.523258]\n",
      "epoch:28 step:26381 [D loss: 1.304518, acc.: 26.56%] [G loss: 1.434561]\n",
      "epoch:28 step:26382 [D loss: 0.377116, acc.: 87.50%] [G loss: 1.508693]\n",
      "epoch:28 step:26383 [D loss: 0.166812, acc.: 96.09%] [G loss: 1.182073]\n",
      "epoch:28 step:26384 [D loss: 0.229895, acc.: 95.31%] [G loss: 1.037324]\n",
      "epoch:28 step:26385 [D loss: 0.162369, acc.: 97.66%] [G loss: 2.384757]\n",
      "epoch:28 step:26386 [D loss: 0.084699, acc.: 98.44%] [G loss: 1.869826]\n",
      "epoch:28 step:26387 [D loss: 0.522518, acc.: 68.75%] [G loss: 2.141727]\n",
      "epoch:28 step:26388 [D loss: 0.366455, acc.: 82.81%] [G loss: 2.642349]\n",
      "epoch:28 step:26389 [D loss: 1.183687, acc.: 50.78%] [G loss: 3.815357]\n",
      "epoch:28 step:26390 [D loss: 0.589700, acc.: 63.28%] [G loss: 2.989528]\n",
      "epoch:28 step:26391 [D loss: 0.786456, acc.: 59.38%] [G loss: 1.951213]\n",
      "epoch:28 step:26392 [D loss: 0.288191, acc.: 85.16%] [G loss: 5.961111]\n",
      "epoch:28 step:26393 [D loss: 0.361632, acc.: 81.25%] [G loss: 2.796647]\n",
      "epoch:28 step:26394 [D loss: 0.735858, acc.: 58.59%] [G loss: 2.547556]\n",
      "epoch:28 step:26395 [D loss: 0.313524, acc.: 84.38%] [G loss: 4.565804]\n",
      "epoch:28 step:26396 [D loss: 0.647888, acc.: 60.16%] [G loss: 2.907876]\n",
      "epoch:28 step:26397 [D loss: 0.460514, acc.: 78.91%] [G loss: 2.301756]\n",
      "epoch:28 step:26398 [D loss: 0.239920, acc.: 94.53%] [G loss: 1.884529]\n",
      "epoch:28 step:26399 [D loss: 0.380074, acc.: 83.59%] [G loss: 2.795406]\n",
      "epoch:28 step:26400 [D loss: 0.428177, acc.: 78.91%] [G loss: 0.450356]\n",
      "epoch:28 step:26401 [D loss: 0.337371, acc.: 94.53%] [G loss: 2.344992]\n",
      "epoch:28 step:26402 [D loss: 0.979172, acc.: 39.06%] [G loss: 2.883511]\n",
      "epoch:28 step:26403 [D loss: 0.175048, acc.: 94.53%] [G loss: 3.514751]\n",
      "epoch:28 step:26404 [D loss: 0.315599, acc.: 92.97%] [G loss: 3.882561]\n",
      "epoch:28 step:26405 [D loss: 0.312800, acc.: 82.81%] [G loss: 3.606643]\n",
      "epoch:28 step:26406 [D loss: 0.406346, acc.: 85.16%] [G loss: 3.373796]\n",
      "epoch:28 step:26407 [D loss: 0.440276, acc.: 81.25%] [G loss: 3.399251]\n",
      "epoch:28 step:26408 [D loss: 0.450620, acc.: 74.22%] [G loss: 1.098225]\n",
      "epoch:28 step:26409 [D loss: 0.208736, acc.: 96.88%] [G loss: 2.206573]\n",
      "epoch:28 step:26410 [D loss: 0.749025, acc.: 60.16%] [G loss: 3.884604]\n",
      "epoch:28 step:26411 [D loss: 0.124355, acc.: 96.88%] [G loss: 3.405868]\n",
      "epoch:28 step:26412 [D loss: 0.602162, acc.: 66.41%] [G loss: 3.369615]\n",
      "epoch:28 step:26413 [D loss: 0.062027, acc.: 98.44%] [G loss: 1.263641]\n",
      "epoch:28 step:26414 [D loss: 0.052310, acc.: 100.00%] [G loss: 1.397423]\n",
      "epoch:28 step:26415 [D loss: 0.087053, acc.: 98.44%] [G loss: 0.648052]\n",
      "epoch:28 step:26416 [D loss: 0.044696, acc.: 99.22%] [G loss: 2.925164]\n",
      "epoch:28 step:26417 [D loss: 0.050440, acc.: 99.22%] [G loss: 2.279736]\n",
      "epoch:28 step:26418 [D loss: 0.222024, acc.: 94.53%] [G loss: 3.655253]\n",
      "epoch:28 step:26419 [D loss: 1.067769, acc.: 37.50%] [G loss: 2.632884]\n",
      "epoch:28 step:26420 [D loss: 1.147270, acc.: 40.62%] [G loss: 4.642129]\n",
      "epoch:28 step:26421 [D loss: 0.502984, acc.: 75.00%] [G loss: 4.995362]\n",
      "epoch:28 step:26422 [D loss: 0.094882, acc.: 97.66%] [G loss: 3.927406]\n",
      "epoch:28 step:26423 [D loss: 0.526235, acc.: 72.66%] [G loss: 3.088950]\n",
      "epoch:28 step:26424 [D loss: 0.233392, acc.: 95.31%] [G loss: 2.604180]\n",
      "epoch:28 step:26425 [D loss: 0.538688, acc.: 67.19%] [G loss: 1.459836]\n",
      "epoch:28 step:26426 [D loss: 0.133021, acc.: 97.66%] [G loss: 1.597506]\n",
      "epoch:28 step:26427 [D loss: 0.347428, acc.: 84.38%] [G loss: 0.866784]\n",
      "epoch:28 step:26428 [D loss: 0.155788, acc.: 95.31%] [G loss: 1.829963]\n",
      "epoch:28 step:26429 [D loss: 0.239438, acc.: 92.97%] [G loss: 1.709734]\n",
      "epoch:28 step:26430 [D loss: 0.164262, acc.: 97.66%] [G loss: 3.832022]\n",
      "epoch:28 step:26431 [D loss: 0.546668, acc.: 68.75%] [G loss: 1.546721]\n",
      "epoch:28 step:26432 [D loss: 0.099436, acc.: 98.44%] [G loss: 2.269743]\n",
      "epoch:28 step:26433 [D loss: 0.547781, acc.: 71.09%] [G loss: 4.296704]\n",
      "epoch:28 step:26434 [D loss: 0.332566, acc.: 87.50%] [G loss: 4.664318]\n",
      "epoch:28 step:26435 [D loss: 1.206758, acc.: 32.03%] [G loss: 3.689850]\n",
      "epoch:28 step:26436 [D loss: 0.559181, acc.: 68.75%] [G loss: 3.326933]\n",
      "epoch:28 step:26437 [D loss: 0.694554, acc.: 62.50%] [G loss: 1.957714]\n",
      "epoch:28 step:26438 [D loss: 0.180309, acc.: 95.31%] [G loss: 0.674185]\n",
      "epoch:28 step:26439 [D loss: 0.266879, acc.: 94.53%] [G loss: 0.597783]\n",
      "epoch:28 step:26440 [D loss: 0.346226, acc.: 93.75%] [G loss: 1.713031]\n",
      "epoch:28 step:26441 [D loss: 0.972894, acc.: 55.47%] [G loss: 0.801346]\n",
      "epoch:28 step:26442 [D loss: 0.441682, acc.: 77.34%] [G loss: 3.304321]\n",
      "epoch:28 step:26443 [D loss: 0.223098, acc.: 89.84%] [G loss: 4.564601]\n",
      "epoch:28 step:26444 [D loss: 0.282744, acc.: 89.06%] [G loss: 3.981215]\n",
      "epoch:28 step:26445 [D loss: 0.233314, acc.: 93.75%] [G loss: 3.927143]\n",
      "epoch:28 step:26446 [D loss: 0.085404, acc.: 98.44%] [G loss: 1.779014]\n",
      "epoch:28 step:26447 [D loss: 0.918324, acc.: 38.28%] [G loss: 3.008071]\n",
      "epoch:28 step:26448 [D loss: 1.223827, acc.: 22.66%] [G loss: 3.110612]\n",
      "epoch:28 step:26449 [D loss: 0.197899, acc.: 93.75%] [G loss: 2.305866]\n",
      "epoch:28 step:26450 [D loss: 0.621539, acc.: 71.09%] [G loss: 2.134106]\n",
      "epoch:28 step:26451 [D loss: 0.206033, acc.: 95.31%] [G loss: 2.758075]\n",
      "epoch:28 step:26452 [D loss: 0.516867, acc.: 70.31%] [G loss: 1.774719]\n",
      "epoch:28 step:26453 [D loss: 0.123661, acc.: 99.22%] [G loss: 2.177777]\n",
      "epoch:28 step:26454 [D loss: 0.355431, acc.: 82.81%] [G loss: 0.897076]\n",
      "epoch:28 step:26455 [D loss: 0.158202, acc.: 96.88%] [G loss: 2.769677]\n",
      "epoch:28 step:26456 [D loss: 0.184964, acc.: 95.31%] [G loss: 2.583932]\n",
      "epoch:28 step:26457 [D loss: 0.095373, acc.: 97.66%] [G loss: 1.904447]\n",
      "epoch:28 step:26458 [D loss: 0.401012, acc.: 80.47%] [G loss: 1.316900]\n",
      "epoch:28 step:26459 [D loss: 0.677451, acc.: 67.97%] [G loss: 1.768535]\n",
      "epoch:28 step:26460 [D loss: 0.265143, acc.: 95.31%] [G loss: 0.989802]\n",
      "epoch:28 step:26461 [D loss: 0.523357, acc.: 74.22%] [G loss: 1.605446]\n",
      "epoch:28 step:26462 [D loss: 0.272505, acc.: 89.84%] [G loss: 3.876144]\n",
      "epoch:28 step:26463 [D loss: 0.176770, acc.: 96.09%] [G loss: 3.189186]\n",
      "epoch:28 step:26464 [D loss: 0.401346, acc.: 86.72%] [G loss: 4.010210]\n",
      "epoch:28 step:26465 [D loss: 0.517642, acc.: 75.78%] [G loss: 1.702814]\n",
      "epoch:28 step:26466 [D loss: 0.752417, acc.: 60.94%] [G loss: 1.753560]\n",
      "epoch:28 step:26467 [D loss: 0.124314, acc.: 98.44%] [G loss: 0.770388]\n",
      "epoch:28 step:26468 [D loss: 0.642604, acc.: 60.16%] [G loss: 0.832692]\n",
      "epoch:28 step:26469 [D loss: 0.809501, acc.: 53.91%] [G loss: 1.863716]\n",
      "epoch:28 step:26470 [D loss: 0.147330, acc.: 98.44%] [G loss: 2.910005]\n",
      "epoch:28 step:26471 [D loss: 0.113162, acc.: 98.44%] [G loss: 3.352309]\n",
      "epoch:28 step:26472 [D loss: 0.615927, acc.: 71.09%] [G loss: 0.641550]\n",
      "epoch:28 step:26473 [D loss: 0.321135, acc.: 84.38%] [G loss: 4.153425]\n",
      "epoch:28 step:26474 [D loss: 0.385799, acc.: 89.06%] [G loss: 1.692026]\n",
      "epoch:28 step:26475 [D loss: 0.211575, acc.: 96.88%] [G loss: 2.077833]\n",
      "epoch:28 step:26476 [D loss: 0.168965, acc.: 97.66%] [G loss: 4.256299]\n",
      "epoch:28 step:26477 [D loss: 0.302490, acc.: 91.41%] [G loss: 2.975841]\n",
      "epoch:28 step:26478 [D loss: 1.271528, acc.: 24.22%] [G loss: 3.637178]\n",
      "epoch:28 step:26479 [D loss: 0.948608, acc.: 53.91%] [G loss: 2.448085]\n",
      "epoch:28 step:26480 [D loss: 0.330234, acc.: 87.50%] [G loss: 3.284654]\n",
      "epoch:28 step:26481 [D loss: 0.175390, acc.: 96.09%] [G loss: 3.694402]\n",
      "epoch:28 step:26482 [D loss: 0.216119, acc.: 95.31%] [G loss: 2.424368]\n",
      "epoch:28 step:26483 [D loss: 0.378513, acc.: 78.12%] [G loss: 4.130079]\n",
      "epoch:28 step:26484 [D loss: 0.067671, acc.: 99.22%] [G loss: 1.276101]\n",
      "epoch:28 step:26485 [D loss: 0.124131, acc.: 96.88%] [G loss: 2.917892]\n",
      "epoch:28 step:26486 [D loss: 0.429084, acc.: 78.12%] [G loss: 3.079188]\n",
      "epoch:28 step:26487 [D loss: 0.858065, acc.: 59.38%] [G loss: 0.737769]\n",
      "epoch:28 step:26488 [D loss: 0.141809, acc.: 98.44%] [G loss: 0.714586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26489 [D loss: 0.072250, acc.: 100.00%] [G loss: 0.994910]\n",
      "epoch:28 step:26490 [D loss: 0.263765, acc.: 92.19%] [G loss: 1.131554]\n",
      "epoch:28 step:26491 [D loss: 0.139576, acc.: 100.00%] [G loss: 1.310322]\n",
      "epoch:28 step:26492 [D loss: 0.198638, acc.: 96.09%] [G loss: 3.739123]\n",
      "epoch:28 step:26493 [D loss: 0.687614, acc.: 51.56%] [G loss: 1.219042]\n",
      "epoch:28 step:26494 [D loss: 0.084187, acc.: 99.22%] [G loss: 1.949968]\n",
      "epoch:28 step:26495 [D loss: 0.188012, acc.: 97.66%] [G loss: 2.745642]\n",
      "epoch:28 step:26496 [D loss: 0.366898, acc.: 82.03%] [G loss: 2.696428]\n",
      "epoch:28 step:26497 [D loss: 0.403131, acc.: 84.38%] [G loss: 2.166596]\n",
      "epoch:28 step:26498 [D loss: 0.110783, acc.: 97.66%] [G loss: 1.694588]\n",
      "epoch:28 step:26499 [D loss: 0.236149, acc.: 92.97%] [G loss: 3.835916]\n",
      "epoch:28 step:26500 [D loss: 0.270073, acc.: 91.41%] [G loss: 4.146502]\n",
      "epoch:28 step:26501 [D loss: 0.272245, acc.: 89.84%] [G loss: 2.861688]\n",
      "epoch:28 step:26502 [D loss: 0.130945, acc.: 96.09%] [G loss: 3.730328]\n",
      "epoch:28 step:26503 [D loss: 0.505578, acc.: 75.00%] [G loss: 3.022753]\n",
      "epoch:28 step:26504 [D loss: 0.453288, acc.: 80.47%] [G loss: 0.918670]\n",
      "epoch:28 step:26505 [D loss: 0.269671, acc.: 89.06%] [G loss: 1.751159]\n",
      "epoch:28 step:26506 [D loss: 0.142834, acc.: 95.31%] [G loss: 0.645222]\n",
      "epoch:28 step:26507 [D loss: 0.180481, acc.: 96.09%] [G loss: 2.898895]\n",
      "epoch:28 step:26508 [D loss: 0.476565, acc.: 71.09%] [G loss: 3.337844]\n",
      "epoch:28 step:26509 [D loss: 0.148569, acc.: 97.66%] [G loss: 2.766979]\n",
      "epoch:28 step:26510 [D loss: 0.190002, acc.: 96.09%] [G loss: 4.340051]\n",
      "epoch:28 step:26511 [D loss: 0.324521, acc.: 89.06%] [G loss: 2.631787]\n",
      "epoch:28 step:26512 [D loss: 0.283597, acc.: 87.50%] [G loss: 1.680135]\n",
      "epoch:28 step:26513 [D loss: 0.411403, acc.: 81.25%] [G loss: 4.045444]\n",
      "epoch:28 step:26514 [D loss: 0.624942, acc.: 67.97%] [G loss: 0.563135]\n",
      "epoch:28 step:26515 [D loss: 0.804512, acc.: 58.59%] [G loss: 2.586103]\n",
      "epoch:28 step:26516 [D loss: 0.204597, acc.: 95.31%] [G loss: 4.795139]\n",
      "epoch:28 step:26517 [D loss: 0.091038, acc.: 97.66%] [G loss: 2.790922]\n",
      "epoch:28 step:26518 [D loss: 0.409480, acc.: 81.25%] [G loss: 2.419797]\n",
      "epoch:28 step:26519 [D loss: 0.135595, acc.: 99.22%] [G loss: 3.122728]\n",
      "epoch:28 step:26520 [D loss: 0.200313, acc.: 93.75%] [G loss: 4.054047]\n",
      "epoch:28 step:26521 [D loss: 0.217863, acc.: 91.41%] [G loss: 1.303158]\n",
      "epoch:28 step:26522 [D loss: 0.055671, acc.: 98.44%] [G loss: 3.919150]\n",
      "epoch:28 step:26523 [D loss: 0.920532, acc.: 55.47%] [G loss: 0.870479]\n",
      "epoch:28 step:26524 [D loss: 0.574810, acc.: 67.19%] [G loss: 1.539175]\n",
      "epoch:28 step:26525 [D loss: 0.191535, acc.: 92.97%] [G loss: 3.486227]\n",
      "epoch:28 step:26526 [D loss: 0.244759, acc.: 90.62%] [G loss: 3.858670]\n",
      "epoch:28 step:26527 [D loss: 0.167680, acc.: 96.09%] [G loss: 3.230832]\n",
      "epoch:28 step:26528 [D loss: 0.268549, acc.: 87.50%] [G loss: 0.841621]\n",
      "epoch:28 step:26529 [D loss: 0.367290, acc.: 82.81%] [G loss: 1.645450]\n",
      "epoch:28 step:26530 [D loss: 0.322283, acc.: 89.84%] [G loss: 1.619064]\n",
      "epoch:28 step:26531 [D loss: 0.373023, acc.: 83.59%] [G loss: 0.492772]\n",
      "epoch:28 step:26532 [D loss: 0.194147, acc.: 92.97%] [G loss: 2.237206]\n",
      "epoch:28 step:26533 [D loss: 0.094709, acc.: 96.88%] [G loss: 4.594330]\n",
      "epoch:28 step:26534 [D loss: 0.114653, acc.: 97.66%] [G loss: 3.140454]\n",
      "epoch:28 step:26535 [D loss: 0.663817, acc.: 63.28%] [G loss: 3.242811]\n",
      "epoch:28 step:26536 [D loss: 0.085268, acc.: 97.66%] [G loss: 2.841282]\n",
      "epoch:28 step:26537 [D loss: 0.101593, acc.: 98.44%] [G loss: 1.767114]\n",
      "epoch:28 step:26538 [D loss: 0.427400, acc.: 82.03%] [G loss: 4.027979]\n",
      "epoch:28 step:26539 [D loss: 0.087449, acc.: 97.66%] [G loss: 4.458664]\n",
      "epoch:28 step:26540 [D loss: 0.738548, acc.: 55.47%] [G loss: 3.344415]\n",
      "epoch:28 step:26541 [D loss: 0.073814, acc.: 100.00%] [G loss: 2.045840]\n",
      "epoch:28 step:26542 [D loss: 0.192467, acc.: 96.09%] [G loss: 1.857550]\n",
      "epoch:28 step:26543 [D loss: 0.193086, acc.: 95.31%] [G loss: 4.105418]\n",
      "epoch:28 step:26544 [D loss: 0.454496, acc.: 78.12%] [G loss: 1.893338]\n",
      "epoch:28 step:26545 [D loss: 0.298540, acc.: 89.84%] [G loss: 2.870473]\n",
      "epoch:28 step:26546 [D loss: 0.783662, acc.: 60.16%] [G loss: 2.459684]\n",
      "epoch:28 step:26547 [D loss: 0.096331, acc.: 99.22%] [G loss: 2.059790]\n",
      "epoch:28 step:26548 [D loss: 0.207908, acc.: 94.53%] [G loss: 1.258178]\n",
      "epoch:28 step:26549 [D loss: 0.222014, acc.: 92.97%] [G loss: 4.334530]\n",
      "epoch:28 step:26550 [D loss: 0.811168, acc.: 59.38%] [G loss: 4.220329]\n",
      "epoch:28 step:26551 [D loss: 0.141623, acc.: 96.09%] [G loss: 4.296805]\n",
      "epoch:28 step:26552 [D loss: 0.153834, acc.: 96.88%] [G loss: 4.369989]\n",
      "epoch:28 step:26553 [D loss: 0.316500, acc.: 89.84%] [G loss: 2.621551]\n",
      "epoch:28 step:26554 [D loss: 0.398597, acc.: 78.91%] [G loss: 2.628354]\n",
      "epoch:28 step:26555 [D loss: 0.520782, acc.: 69.53%] [G loss: 2.950259]\n",
      "epoch:28 step:26556 [D loss: 0.035593, acc.: 99.22%] [G loss: 2.565570]\n",
      "epoch:28 step:26557 [D loss: 0.106239, acc.: 99.22%] [G loss: 3.209798]\n",
      "epoch:28 step:26558 [D loss: 0.086209, acc.: 100.00%] [G loss: 4.377065]\n",
      "epoch:28 step:26559 [D loss: 0.117977, acc.: 99.22%] [G loss: 1.246801]\n",
      "epoch:28 step:26560 [D loss: 0.121430, acc.: 99.22%] [G loss: 2.274404]\n",
      "epoch:28 step:26561 [D loss: 0.477302, acc.: 75.78%] [G loss: 3.525833]\n",
      "epoch:28 step:26562 [D loss: 0.411400, acc.: 75.00%] [G loss: 3.732694]\n",
      "epoch:28 step:26563 [D loss: 0.075550, acc.: 99.22%] [G loss: 4.474591]\n",
      "epoch:28 step:26564 [D loss: 0.039215, acc.: 99.22%] [G loss: 4.398699]\n",
      "epoch:28 step:26565 [D loss: 1.023162, acc.: 38.28%] [G loss: 2.009422]\n",
      "epoch:28 step:26566 [D loss: 0.137558, acc.: 95.31%] [G loss: 3.003187]\n",
      "epoch:28 step:26567 [D loss: 0.213717, acc.: 94.53%] [G loss: 4.383177]\n",
      "epoch:28 step:26568 [D loss: 0.154582, acc.: 97.66%] [G loss: 2.174408]\n",
      "epoch:28 step:26569 [D loss: 0.285798, acc.: 84.38%] [G loss: 0.493687]\n",
      "epoch:28 step:26570 [D loss: 0.836197, acc.: 56.25%] [G loss: 2.071244]\n",
      "epoch:28 step:26571 [D loss: 0.340176, acc.: 86.72%] [G loss: 3.198105]\n",
      "epoch:28 step:26572 [D loss: 0.752485, acc.: 60.16%] [G loss: 1.950261]\n",
      "epoch:28 step:26573 [D loss: 0.174411, acc.: 96.88%] [G loss: 1.984332]\n",
      "epoch:28 step:26574 [D loss: 0.539238, acc.: 74.22%] [G loss: 1.154360]\n",
      "epoch:28 step:26575 [D loss: 0.549242, acc.: 67.19%] [G loss: 3.015013]\n",
      "epoch:28 step:26576 [D loss: 0.212998, acc.: 95.31%] [G loss: 2.447332]\n",
      "epoch:28 step:26577 [D loss: 0.308148, acc.: 92.19%] [G loss: 3.971880]\n",
      "epoch:28 step:26578 [D loss: 0.122547, acc.: 99.22%] [G loss: 1.805790]\n",
      "epoch:28 step:26579 [D loss: 0.237704, acc.: 89.06%] [G loss: 1.758331]\n",
      "epoch:28 step:26580 [D loss: 0.424993, acc.: 81.25%] [G loss: 4.522937]\n",
      "epoch:28 step:26581 [D loss: 0.194733, acc.: 91.41%] [G loss: 4.704994]\n",
      "epoch:28 step:26582 [D loss: 0.196822, acc.: 92.19%] [G loss: 1.836520]\n",
      "epoch:28 step:26583 [D loss: 1.010374, acc.: 50.00%] [G loss: 2.511732]\n",
      "epoch:28 step:26584 [D loss: 0.423104, acc.: 76.56%] [G loss: 1.612161]\n",
      "epoch:28 step:26585 [D loss: 0.388610, acc.: 82.03%] [G loss: 2.524735]\n",
      "epoch:28 step:26586 [D loss: 0.143931, acc.: 96.09%] [G loss: 5.715077]\n",
      "epoch:28 step:26587 [D loss: 0.285962, acc.: 85.94%] [G loss: 2.865062]\n",
      "epoch:28 step:26588 [D loss: 0.183959, acc.: 96.09%] [G loss: 1.378038]\n",
      "epoch:28 step:26589 [D loss: 0.059566, acc.: 99.22%] [G loss: 2.058114]\n",
      "epoch:28 step:26590 [D loss: 0.489419, acc.: 78.91%] [G loss: 0.779184]\n",
      "epoch:28 step:26591 [D loss: 0.666783, acc.: 63.28%] [G loss: 1.378848]\n",
      "epoch:28 step:26592 [D loss: 0.106192, acc.: 99.22%] [G loss: 1.853765]\n",
      "epoch:28 step:26593 [D loss: 0.150015, acc.: 95.31%] [G loss: 2.942236]\n",
      "epoch:28 step:26594 [D loss: 0.879501, acc.: 59.38%] [G loss: 2.034323]\n",
      "epoch:28 step:26595 [D loss: 0.235298, acc.: 91.41%] [G loss: 1.267131]\n",
      "epoch:28 step:26596 [D loss: 0.123213, acc.: 96.88%] [G loss: 4.351145]\n",
      "epoch:28 step:26597 [D loss: 0.217040, acc.: 91.41%] [G loss: 2.314330]\n",
      "epoch:28 step:26598 [D loss: 0.030782, acc.: 99.22%] [G loss: 0.983219]\n",
      "epoch:28 step:26599 [D loss: 0.221221, acc.: 96.09%] [G loss: 2.588952]\n",
      "epoch:28 step:26600 [D loss: 0.965615, acc.: 36.72%] [G loss: 2.089983]\n",
      "epoch:28 step:26601 [D loss: 0.405661, acc.: 76.56%] [G loss: 4.774318]\n",
      "epoch:28 step:26602 [D loss: 0.648633, acc.: 66.41%] [G loss: 2.505630]\n",
      "epoch:28 step:26603 [D loss: 0.417930, acc.: 85.16%] [G loss: 2.554429]\n",
      "epoch:28 step:26604 [D loss: 0.728135, acc.: 57.81%] [G loss: 4.994022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26605 [D loss: 0.169217, acc.: 97.66%] [G loss: 3.572146]\n",
      "epoch:28 step:26606 [D loss: 0.140410, acc.: 98.44%] [G loss: 2.680519]\n",
      "epoch:28 step:26607 [D loss: 0.295581, acc.: 90.62%] [G loss: 4.000708]\n",
      "epoch:28 step:26608 [D loss: 0.036320, acc.: 98.44%] [G loss: 4.919112]\n",
      "epoch:28 step:26609 [D loss: 0.272652, acc.: 92.19%] [G loss: 4.244944]\n",
      "epoch:28 step:26610 [D loss: 0.552123, acc.: 71.88%] [G loss: 3.291970]\n",
      "epoch:28 step:26611 [D loss: 0.041138, acc.: 100.00%] [G loss: 4.075132]\n",
      "epoch:28 step:26612 [D loss: 0.578965, acc.: 70.31%] [G loss: 1.319177]\n",
      "epoch:28 step:26613 [D loss: 0.168954, acc.: 97.66%] [G loss: 1.924829]\n",
      "epoch:28 step:26614 [D loss: 1.252588, acc.: 21.09%] [G loss: 4.012460]\n",
      "epoch:28 step:26615 [D loss: 0.300395, acc.: 83.59%] [G loss: 2.225800]\n",
      "epoch:28 step:26616 [D loss: 1.938549, acc.: 42.97%] [G loss: 1.389160]\n",
      "epoch:28 step:26617 [D loss: 1.114863, acc.: 55.47%] [G loss: 1.955454]\n",
      "epoch:28 step:26618 [D loss: 0.463214, acc.: 77.34%] [G loss: 1.867712]\n",
      "epoch:28 step:26619 [D loss: 0.324328, acc.: 86.72%] [G loss: 2.537290]\n",
      "epoch:28 step:26620 [D loss: 0.574026, acc.: 71.88%] [G loss: 3.109688]\n",
      "epoch:28 step:26621 [D loss: 0.307020, acc.: 91.41%] [G loss: 0.987641]\n",
      "epoch:28 step:26622 [D loss: 0.322069, acc.: 86.72%] [G loss: 1.892821]\n",
      "epoch:28 step:26623 [D loss: 0.149884, acc.: 98.44%] [G loss: 0.964358]\n",
      "epoch:28 step:26624 [D loss: 1.227285, acc.: 25.00%] [G loss: 0.280946]\n",
      "epoch:28 step:26625 [D loss: 0.083841, acc.: 100.00%] [G loss: 3.967196]\n",
      "epoch:28 step:26626 [D loss: 0.248279, acc.: 95.31%] [G loss: 1.756699]\n",
      "epoch:28 step:26627 [D loss: 0.413885, acc.: 79.69%] [G loss: 1.484432]\n",
      "epoch:28 step:26628 [D loss: 0.257898, acc.: 92.97%] [G loss: 4.265900]\n",
      "epoch:28 step:26629 [D loss: 0.292586, acc.: 84.38%] [G loss: 2.960405]\n",
      "epoch:28 step:26630 [D loss: 0.089465, acc.: 99.22%] [G loss: 0.576249]\n",
      "epoch:28 step:26631 [D loss: 0.338065, acc.: 87.50%] [G loss: 3.954411]\n",
      "epoch:28 step:26632 [D loss: 0.080475, acc.: 99.22%] [G loss: 1.264766]\n",
      "epoch:28 step:26633 [D loss: 2.327661, acc.: 44.53%] [G loss: 0.736526]\n",
      "epoch:28 step:26634 [D loss: 0.177753, acc.: 95.31%] [G loss: 0.805689]\n",
      "epoch:28 step:26635 [D loss: 0.275719, acc.: 90.62%] [G loss: 2.978256]\n",
      "epoch:28 step:26636 [D loss: 0.863918, acc.: 48.44%] [G loss: 2.188775]\n",
      "epoch:28 step:26637 [D loss: 0.388013, acc.: 89.84%] [G loss: 2.835843]\n",
      "epoch:28 step:26638 [D loss: 0.275234, acc.: 93.75%] [G loss: 3.546843]\n",
      "epoch:28 step:26639 [D loss: 0.527523, acc.: 75.00%] [G loss: 2.860654]\n",
      "epoch:28 step:26640 [D loss: 0.228663, acc.: 92.97%] [G loss: 2.867400]\n",
      "epoch:28 step:26641 [D loss: 0.053673, acc.: 100.00%] [G loss: 5.008743]\n",
      "epoch:28 step:26642 [D loss: 0.114581, acc.: 99.22%] [G loss: 2.493601]\n",
      "epoch:28 step:26643 [D loss: 0.195995, acc.: 96.88%] [G loss: 2.663926]\n",
      "epoch:28 step:26644 [D loss: 0.143261, acc.: 96.88%] [G loss: 3.209870]\n",
      "epoch:28 step:26645 [D loss: 0.441766, acc.: 80.47%] [G loss: 3.079339]\n",
      "epoch:28 step:26646 [D loss: 0.194200, acc.: 95.31%] [G loss: 4.294364]\n",
      "epoch:28 step:26647 [D loss: 0.272293, acc.: 90.62%] [G loss: 3.974372]\n",
      "epoch:28 step:26648 [D loss: 0.589538, acc.: 68.75%] [G loss: 1.311162]\n",
      "epoch:28 step:26649 [D loss: 0.943695, acc.: 57.03%] [G loss: 1.579028]\n",
      "epoch:28 step:26650 [D loss: 0.152133, acc.: 97.66%] [G loss: 2.749527]\n",
      "epoch:28 step:26651 [D loss: 0.249522, acc.: 93.75%] [G loss: 0.972950]\n",
      "epoch:28 step:26652 [D loss: 0.339035, acc.: 91.41%] [G loss: 1.672058]\n",
      "epoch:28 step:26653 [D loss: 0.529670, acc.: 71.09%] [G loss: 2.389980]\n",
      "epoch:28 step:26654 [D loss: 0.438821, acc.: 79.69%] [G loss: 1.979018]\n",
      "epoch:28 step:26655 [D loss: 0.896572, acc.: 60.16%] [G loss: 4.490406]\n",
      "epoch:28 step:26656 [D loss: 0.111519, acc.: 98.44%] [G loss: 2.758442]\n",
      "epoch:28 step:26657 [D loss: 0.323989, acc.: 91.41%] [G loss: 3.896290]\n",
      "epoch:28 step:26658 [D loss: 0.123124, acc.: 99.22%] [G loss: 3.065241]\n",
      "epoch:28 step:26659 [D loss: 0.394678, acc.: 74.22%] [G loss: 2.349181]\n",
      "epoch:28 step:26660 [D loss: 0.156310, acc.: 97.66%] [G loss: 2.433993]\n",
      "epoch:28 step:26661 [D loss: 0.125980, acc.: 98.44%] [G loss: 2.892115]\n",
      "epoch:28 step:26662 [D loss: 0.357198, acc.: 84.38%] [G loss: 1.165015]\n",
      "epoch:28 step:26663 [D loss: 0.091992, acc.: 98.44%] [G loss: 2.497871]\n",
      "epoch:28 step:26664 [D loss: 0.432431, acc.: 73.44%] [G loss: 3.743181]\n",
      "epoch:28 step:26665 [D loss: 0.597291, acc.: 66.41%] [G loss: 4.239943]\n",
      "epoch:28 step:26666 [D loss: 0.895391, acc.: 42.97%] [G loss: 1.701410]\n",
      "epoch:28 step:26667 [D loss: 0.550903, acc.: 71.09%] [G loss: 3.990480]\n",
      "epoch:28 step:26668 [D loss: 0.781691, acc.: 56.25%] [G loss: 3.567688]\n",
      "epoch:28 step:26669 [D loss: 0.162763, acc.: 97.66%] [G loss: 2.162686]\n",
      "epoch:28 step:26670 [D loss: 0.161415, acc.: 96.88%] [G loss: 2.215588]\n",
      "epoch:28 step:26671 [D loss: 0.139048, acc.: 97.66%] [G loss: 2.433460]\n",
      "epoch:28 step:26672 [D loss: 0.712711, acc.: 64.06%] [G loss: 3.028775]\n",
      "epoch:28 step:26673 [D loss: 0.675690, acc.: 60.94%] [G loss: 0.263515]\n",
      "epoch:28 step:26674 [D loss: 0.223050, acc.: 92.97%] [G loss: 2.502526]\n",
      "epoch:28 step:26675 [D loss: 0.136734, acc.: 99.22%] [G loss: 3.113196]\n",
      "epoch:28 step:26676 [D loss: 0.354552, acc.: 87.50%] [G loss: 2.449147]\n",
      "epoch:28 step:26677 [D loss: 0.467362, acc.: 76.56%] [G loss: 1.546029]\n",
      "epoch:28 step:26678 [D loss: 0.472906, acc.: 73.44%] [G loss: 1.627417]\n",
      "epoch:28 step:26679 [D loss: 0.433583, acc.: 78.91%] [G loss: 1.549743]\n",
      "epoch:28 step:26680 [D loss: 0.389586, acc.: 81.25%] [G loss: 2.060338]\n",
      "epoch:28 step:26681 [D loss: 1.102208, acc.: 54.69%] [G loss: 1.940146]\n",
      "epoch:28 step:26682 [D loss: 1.262300, acc.: 39.84%] [G loss: 4.457886]\n",
      "epoch:28 step:26683 [D loss: 0.573982, acc.: 65.62%] [G loss: 1.195912]\n",
      "epoch:28 step:26684 [D loss: 0.433157, acc.: 77.34%] [G loss: 2.733852]\n",
      "epoch:28 step:26685 [D loss: 0.157821, acc.: 98.44%] [G loss: 1.061654]\n",
      "epoch:28 step:26686 [D loss: 0.264065, acc.: 89.06%] [G loss: 0.812157]\n",
      "epoch:28 step:26687 [D loss: 0.637850, acc.: 63.28%] [G loss: 3.736446]\n",
      "epoch:28 step:26688 [D loss: 0.680525, acc.: 60.94%] [G loss: 1.955775]\n",
      "epoch:28 step:26689 [D loss: 0.337092, acc.: 89.84%] [G loss: 2.015420]\n",
      "epoch:28 step:26690 [D loss: 0.543617, acc.: 72.66%] [G loss: 2.403678]\n",
      "epoch:28 step:26691 [D loss: 0.523312, acc.: 69.53%] [G loss: 2.136701]\n",
      "epoch:28 step:26692 [D loss: 0.105191, acc.: 99.22%] [G loss: 2.160542]\n",
      "epoch:28 step:26693 [D loss: 0.241497, acc.: 92.19%] [G loss: 2.797838]\n",
      "epoch:28 step:26694 [D loss: 0.226668, acc.: 95.31%] [G loss: 2.562021]\n",
      "epoch:28 step:26695 [D loss: 0.096463, acc.: 99.22%] [G loss: 1.972261]\n",
      "epoch:28 step:26696 [D loss: 0.415275, acc.: 82.03%] [G loss: 2.552711]\n",
      "epoch:28 step:26697 [D loss: 0.333619, acc.: 85.94%] [G loss: 2.912996]\n",
      "epoch:28 step:26698 [D loss: 0.065767, acc.: 100.00%] [G loss: 1.957846]\n",
      "epoch:28 step:26699 [D loss: 0.107545, acc.: 98.44%] [G loss: 2.612143]\n",
      "epoch:28 step:26700 [D loss: 0.114487, acc.: 97.66%] [G loss: 2.238933]\n",
      "epoch:28 step:26701 [D loss: 0.143627, acc.: 97.66%] [G loss: 1.450169]\n",
      "epoch:28 step:26702 [D loss: 0.906727, acc.: 50.00%] [G loss: 2.896867]\n",
      "epoch:28 step:26703 [D loss: 0.098580, acc.: 98.44%] [G loss: 6.588154]\n",
      "epoch:28 step:26704 [D loss: 0.681848, acc.: 62.50%] [G loss: 1.653187]\n",
      "epoch:28 step:26705 [D loss: 0.224442, acc.: 93.75%] [G loss: 4.012420]\n",
      "epoch:28 step:26706 [D loss: 0.727932, acc.: 56.25%] [G loss: 2.472070]\n",
      "epoch:28 step:26707 [D loss: 0.210028, acc.: 94.53%] [G loss: 2.132835]\n",
      "epoch:28 step:26708 [D loss: 1.367829, acc.: 41.41%] [G loss: 2.555689]\n",
      "epoch:28 step:26709 [D loss: 0.029972, acc.: 100.00%] [G loss: 2.743454]\n",
      "epoch:28 step:26710 [D loss: 0.583677, acc.: 75.78%] [G loss: 1.465747]\n",
      "epoch:28 step:26711 [D loss: 0.355270, acc.: 86.72%] [G loss: 3.219895]\n",
      "epoch:28 step:26712 [D loss: 0.478845, acc.: 73.44%] [G loss: 3.864903]\n",
      "epoch:28 step:26713 [D loss: 0.452710, acc.: 75.78%] [G loss: 2.485866]\n",
      "epoch:28 step:26714 [D loss: 0.332429, acc.: 89.84%] [G loss: 3.177256]\n",
      "epoch:28 step:26715 [D loss: 0.430259, acc.: 74.22%] [G loss: 5.058443]\n",
      "epoch:28 step:26716 [D loss: 0.580276, acc.: 67.19%] [G loss: 1.168948]\n",
      "epoch:28 step:26717 [D loss: 0.181194, acc.: 96.88%] [G loss: 3.546265]\n",
      "epoch:28 step:26718 [D loss: 0.781125, acc.: 57.03%] [G loss: 3.221451]\n",
      "epoch:28 step:26719 [D loss: 0.307096, acc.: 86.72%] [G loss: 3.172636]\n",
      "epoch:28 step:26720 [D loss: 0.221991, acc.: 95.31%] [G loss: 3.952232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26721 [D loss: 0.351472, acc.: 79.69%] [G loss: 5.029066]\n",
      "epoch:28 step:26722 [D loss: 0.396543, acc.: 82.81%] [G loss: 1.503670]\n",
      "epoch:28 step:26723 [D loss: 0.357078, acc.: 88.28%] [G loss: 3.127100]\n",
      "epoch:28 step:26724 [D loss: 0.365919, acc.: 79.69%] [G loss: 2.228367]\n",
      "epoch:28 step:26725 [D loss: 0.442636, acc.: 78.91%] [G loss: 2.133663]\n",
      "epoch:28 step:26726 [D loss: 0.484564, acc.: 67.97%] [G loss: 3.575683]\n",
      "epoch:28 step:26727 [D loss: 0.370252, acc.: 85.16%] [G loss: 0.524885]\n",
      "epoch:28 step:26728 [D loss: 0.070153, acc.: 97.66%] [G loss: 1.639417]\n",
      "epoch:28 step:26729 [D loss: 0.305520, acc.: 91.41%] [G loss: 0.556325]\n",
      "epoch:28 step:26730 [D loss: 0.560219, acc.: 74.22%] [G loss: 2.520919]\n",
      "epoch:28 step:26731 [D loss: 0.464668, acc.: 84.38%] [G loss: 1.617749]\n",
      "epoch:28 step:26732 [D loss: 0.195268, acc.: 93.75%] [G loss: 3.400558]\n",
      "epoch:28 step:26733 [D loss: 0.812187, acc.: 49.22%] [G loss: 1.838037]\n",
      "epoch:28 step:26734 [D loss: 0.069848, acc.: 100.00%] [G loss: 1.753956]\n",
      "epoch:28 step:26735 [D loss: 0.475622, acc.: 78.12%] [G loss: 2.047271]\n",
      "epoch:28 step:26736 [D loss: 0.233173, acc.: 92.97%] [G loss: 2.587138]\n",
      "epoch:28 step:26737 [D loss: 0.751268, acc.: 57.81%] [G loss: 1.709426]\n",
      "epoch:28 step:26738 [D loss: 0.193469, acc.: 95.31%] [G loss: 1.926500]\n",
      "epoch:28 step:26739 [D loss: 0.763190, acc.: 55.47%] [G loss: 0.860534]\n",
      "epoch:28 step:26740 [D loss: 0.460682, acc.: 75.78%] [G loss: 2.786209]\n",
      "epoch:28 step:26741 [D loss: 0.177345, acc.: 94.53%] [G loss: 1.940620]\n",
      "epoch:28 step:26742 [D loss: 0.374398, acc.: 87.50%] [G loss: 2.578376]\n",
      "epoch:28 step:26743 [D loss: 0.401723, acc.: 82.81%] [G loss: 3.083097]\n",
      "epoch:28 step:26744 [D loss: 0.117024, acc.: 96.09%] [G loss: 2.333977]\n",
      "epoch:28 step:26745 [D loss: 0.301084, acc.: 91.41%] [G loss: 2.284845]\n",
      "epoch:28 step:26746 [D loss: 0.059245, acc.: 100.00%] [G loss: 1.814456]\n",
      "epoch:28 step:26747 [D loss: 0.614560, acc.: 60.94%] [G loss: 4.840566]\n",
      "epoch:28 step:26748 [D loss: 0.357142, acc.: 87.50%] [G loss: 3.661388]\n",
      "epoch:28 step:26749 [D loss: 0.248111, acc.: 89.06%] [G loss: 3.664802]\n",
      "epoch:28 step:26750 [D loss: 0.800429, acc.: 57.81%] [G loss: 0.935694]\n",
      "epoch:28 step:26751 [D loss: 0.170884, acc.: 97.66%] [G loss: 0.742156]\n",
      "epoch:28 step:26752 [D loss: 0.325486, acc.: 90.62%] [G loss: 1.210064]\n",
      "epoch:28 step:26753 [D loss: 0.299060, acc.: 91.41%] [G loss: 1.012554]\n",
      "epoch:28 step:26754 [D loss: 0.273385, acc.: 92.97%] [G loss: 1.574182]\n",
      "epoch:28 step:26755 [D loss: 0.408126, acc.: 78.12%] [G loss: 2.784516]\n",
      "epoch:28 step:26756 [D loss: 0.115005, acc.: 98.44%] [G loss: 4.458913]\n",
      "epoch:28 step:26757 [D loss: 0.253503, acc.: 95.31%] [G loss: 3.010675]\n",
      "epoch:28 step:26758 [D loss: 0.078977, acc.: 99.22%] [G loss: 2.654348]\n",
      "epoch:28 step:26759 [D loss: 0.549236, acc.: 71.88%] [G loss: 4.520675]\n",
      "epoch:28 step:26760 [D loss: 0.063866, acc.: 99.22%] [G loss: 4.342575]\n",
      "epoch:28 step:26761 [D loss: 0.196749, acc.: 95.31%] [G loss: 4.296323]\n",
      "epoch:28 step:26762 [D loss: 0.421589, acc.: 77.34%] [G loss: 4.268991]\n",
      "epoch:28 step:26763 [D loss: 0.594954, acc.: 71.88%] [G loss: 2.710438]\n",
      "epoch:28 step:26764 [D loss: 1.169139, acc.: 31.25%] [G loss: 2.002746]\n",
      "epoch:28 step:26765 [D loss: 0.140612, acc.: 97.66%] [G loss: 3.525988]\n",
      "epoch:28 step:26766 [D loss: 0.303944, acc.: 88.28%] [G loss: 1.850450]\n",
      "epoch:28 step:26767 [D loss: 0.160797, acc.: 95.31%] [G loss: 1.945303]\n",
      "epoch:28 step:26768 [D loss: 0.662549, acc.: 65.62%] [G loss: 2.092185]\n",
      "epoch:28 step:26769 [D loss: 0.233500, acc.: 92.97%] [G loss: 4.354025]\n",
      "epoch:28 step:26770 [D loss: 0.080127, acc.: 100.00%] [G loss: 3.674166]\n",
      "epoch:28 step:26771 [D loss: 0.155704, acc.: 96.88%] [G loss: 2.798632]\n",
      "epoch:28 step:26772 [D loss: 0.042805, acc.: 100.00%] [G loss: 3.096496]\n",
      "epoch:28 step:26773 [D loss: 0.190967, acc.: 96.88%] [G loss: 2.176473]\n",
      "epoch:28 step:26774 [D loss: 0.078829, acc.: 99.22%] [G loss: 0.687851]\n",
      "epoch:28 step:26775 [D loss: 0.080316, acc.: 99.22%] [G loss: 3.398939]\n",
      "epoch:28 step:26776 [D loss: 0.133417, acc.: 97.66%] [G loss: 3.191563]\n",
      "epoch:28 step:26777 [D loss: 0.118828, acc.: 99.22%] [G loss: 0.591027]\n",
      "epoch:28 step:26778 [D loss: 0.247954, acc.: 90.62%] [G loss: 1.036609]\n",
      "epoch:28 step:26779 [D loss: 0.027701, acc.: 100.00%] [G loss: 2.738733]\n",
      "epoch:28 step:26780 [D loss: 0.235127, acc.: 92.97%] [G loss: 1.842214]\n",
      "epoch:28 step:26781 [D loss: 0.602506, acc.: 66.41%] [G loss: 3.782804]\n",
      "epoch:28 step:26782 [D loss: 0.146683, acc.: 98.44%] [G loss: 6.560359]\n",
      "epoch:28 step:26783 [D loss: 1.952161, acc.: 21.09%] [G loss: 4.269978]\n",
      "epoch:28 step:26784 [D loss: 0.267598, acc.: 91.41%] [G loss: 0.467577]\n",
      "epoch:28 step:26785 [D loss: 0.637485, acc.: 60.16%] [G loss: 1.558320]\n",
      "epoch:28 step:26786 [D loss: 0.031359, acc.: 100.00%] [G loss: 0.703364]\n",
      "epoch:28 step:26787 [D loss: 0.246226, acc.: 95.31%] [G loss: 4.403971]\n",
      "epoch:28 step:26788 [D loss: 0.204824, acc.: 93.75%] [G loss: 1.647420]\n",
      "epoch:28 step:26789 [D loss: 0.041622, acc.: 100.00%] [G loss: 0.677130]\n",
      "epoch:28 step:26790 [D loss: 1.148732, acc.: 51.56%] [G loss: 3.535282]\n",
      "epoch:28 step:26791 [D loss: 0.367363, acc.: 82.03%] [G loss: 4.670881]\n",
      "epoch:28 step:26792 [D loss: 0.719820, acc.: 62.50%] [G loss: 2.958086]\n",
      "epoch:28 step:26793 [D loss: 0.314561, acc.: 81.25%] [G loss: 2.965164]\n",
      "epoch:28 step:26794 [D loss: 0.292409, acc.: 89.84%] [G loss: 2.569027]\n",
      "epoch:28 step:26795 [D loss: 0.594750, acc.: 71.09%] [G loss: 1.665462]\n",
      "epoch:28 step:26796 [D loss: 0.632877, acc.: 65.62%] [G loss: 1.451612]\n",
      "epoch:28 step:26797 [D loss: 0.237806, acc.: 92.19%] [G loss: 0.735913]\n",
      "epoch:28 step:26798 [D loss: 0.195491, acc.: 94.53%] [G loss: 1.640643]\n",
      "epoch:28 step:26799 [D loss: 0.092362, acc.: 99.22%] [G loss: 2.850668]\n",
      "epoch:28 step:26800 [D loss: 0.422645, acc.: 84.38%] [G loss: 2.715492]\n",
      "epoch:28 step:26801 [D loss: 0.983743, acc.: 53.91%] [G loss: 2.791822]\n",
      "epoch:28 step:26802 [D loss: 0.093655, acc.: 98.44%] [G loss: 1.943927]\n",
      "epoch:28 step:26803 [D loss: 0.600954, acc.: 64.06%] [G loss: 2.679780]\n",
      "epoch:28 step:26804 [D loss: 0.320881, acc.: 85.16%] [G loss: 3.977962]\n",
      "epoch:28 step:26805 [D loss: 0.085756, acc.: 99.22%] [G loss: 1.644574]\n",
      "epoch:28 step:26806 [D loss: 0.459660, acc.: 80.47%] [G loss: 0.578978]\n",
      "epoch:28 step:26807 [D loss: 0.092306, acc.: 99.22%] [G loss: 2.751192]\n",
      "epoch:28 step:26808 [D loss: 0.132866, acc.: 96.88%] [G loss: 1.094450]\n",
      "epoch:28 step:26809 [D loss: 0.216430, acc.: 96.88%] [G loss: 2.848349]\n",
      "epoch:28 step:26810 [D loss: 0.284509, acc.: 90.62%] [G loss: 0.798710]\n",
      "epoch:28 step:26811 [D loss: 0.232915, acc.: 92.19%] [G loss: 1.691396]\n",
      "epoch:28 step:26812 [D loss: 0.030870, acc.: 100.00%] [G loss: 2.909034]\n",
      "epoch:28 step:26813 [D loss: 0.459847, acc.: 67.97%] [G loss: 3.486305]\n",
      "epoch:28 step:26814 [D loss: 0.935467, acc.: 42.97%] [G loss: 4.247274]\n",
      "epoch:28 step:26815 [D loss: 0.883594, acc.: 55.47%] [G loss: 0.750231]\n",
      "epoch:28 step:26816 [D loss: 0.272852, acc.: 94.53%] [G loss: 1.604731]\n",
      "epoch:28 step:26817 [D loss: 0.268767, acc.: 89.84%] [G loss: 2.413385]\n",
      "epoch:28 step:26818 [D loss: 0.433064, acc.: 76.56%] [G loss: 0.250626]\n",
      "epoch:28 step:26819 [D loss: 0.484762, acc.: 76.56%] [G loss: 3.762994]\n",
      "epoch:28 step:26820 [D loss: 1.035226, acc.: 54.69%] [G loss: 2.084814]\n",
      "epoch:28 step:26821 [D loss: 0.156746, acc.: 96.09%] [G loss: 1.428166]\n",
      "epoch:28 step:26822 [D loss: 0.089947, acc.: 98.44%] [G loss: 3.293090]\n",
      "epoch:28 step:26823 [D loss: 0.416785, acc.: 85.16%] [G loss: 3.232675]\n",
      "epoch:28 step:26824 [D loss: 0.063790, acc.: 98.44%] [G loss: 4.090333]\n",
      "epoch:28 step:26825 [D loss: 0.268986, acc.: 92.19%] [G loss: 2.130894]\n",
      "epoch:28 step:26826 [D loss: 1.047127, acc.: 43.75%] [G loss: 4.160491]\n",
      "epoch:28 step:26827 [D loss: 0.376301, acc.: 84.38%] [G loss: 1.521380]\n",
      "epoch:28 step:26828 [D loss: 0.260432, acc.: 87.50%] [G loss: 4.419604]\n",
      "epoch:28 step:26829 [D loss: 0.179902, acc.: 92.19%] [G loss: 1.958660]\n",
      "epoch:28 step:26830 [D loss: 0.271820, acc.: 93.75%] [G loss: 4.282499]\n",
      "epoch:28 step:26831 [D loss: 0.190910, acc.: 94.53%] [G loss: 1.556463]\n",
      "epoch:28 step:26832 [D loss: 0.153129, acc.: 96.09%] [G loss: 0.941345]\n",
      "epoch:28 step:26833 [D loss: 0.166844, acc.: 95.31%] [G loss: 2.462332]\n",
      "epoch:28 step:26834 [D loss: 0.226332, acc.: 93.75%] [G loss: 1.597115]\n",
      "epoch:28 step:26835 [D loss: 0.066330, acc.: 100.00%] [G loss: 1.115220]\n",
      "epoch:28 step:26836 [D loss: 0.271714, acc.: 87.50%] [G loss: 2.768250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26837 [D loss: 0.399536, acc.: 76.56%] [G loss: 2.559369]\n",
      "epoch:28 step:26838 [D loss: 0.057189, acc.: 99.22%] [G loss: 2.583531]\n",
      "epoch:28 step:26839 [D loss: 0.475563, acc.: 77.34%] [G loss: 2.140879]\n",
      "epoch:28 step:26840 [D loss: 0.241399, acc.: 91.41%] [G loss: 3.221895]\n",
      "epoch:28 step:26841 [D loss: 0.119223, acc.: 99.22%] [G loss: 1.954069]\n",
      "epoch:28 step:26842 [D loss: 0.604794, acc.: 72.66%] [G loss: 2.486697]\n",
      "epoch:28 step:26843 [D loss: 0.054666, acc.: 100.00%] [G loss: 0.878045]\n",
      "epoch:28 step:26844 [D loss: 1.429302, acc.: 21.09%] [G loss: 1.465760]\n",
      "epoch:28 step:26845 [D loss: 0.071280, acc.: 100.00%] [G loss: 2.350179]\n",
      "epoch:28 step:26846 [D loss: 0.548855, acc.: 68.75%] [G loss: 1.319702]\n",
      "epoch:28 step:26847 [D loss: 0.282954, acc.: 85.94%] [G loss: 2.771062]\n",
      "epoch:28 step:26848 [D loss: 0.337148, acc.: 93.75%] [G loss: 1.642497]\n",
      "epoch:28 step:26849 [D loss: 0.228381, acc.: 95.31%] [G loss: 2.747072]\n",
      "epoch:28 step:26850 [D loss: 0.028161, acc.: 100.00%] [G loss: 2.733694]\n",
      "epoch:28 step:26851 [D loss: 0.106271, acc.: 97.66%] [G loss: 3.769742]\n",
      "epoch:28 step:26852 [D loss: 0.503749, acc.: 73.44%] [G loss: 1.191839]\n",
      "epoch:28 step:26853 [D loss: 0.621815, acc.: 66.41%] [G loss: 5.518397]\n",
      "epoch:28 step:26854 [D loss: 0.322208, acc.: 92.97%] [G loss: 4.148147]\n",
      "epoch:28 step:26855 [D loss: 0.501009, acc.: 73.44%] [G loss: 0.960388]\n",
      "epoch:28 step:26856 [D loss: 0.583513, acc.: 63.28%] [G loss: 1.614898]\n",
      "epoch:28 step:26857 [D loss: 0.146009, acc.: 96.88%] [G loss: 1.214246]\n",
      "epoch:28 step:26858 [D loss: 0.047567, acc.: 100.00%] [G loss: 2.377274]\n",
      "epoch:28 step:26859 [D loss: 0.757399, acc.: 56.25%] [G loss: 1.501247]\n",
      "epoch:28 step:26860 [D loss: 0.177165, acc.: 94.53%] [G loss: 1.198213]\n",
      "epoch:28 step:26861 [D loss: 0.284381, acc.: 90.62%] [G loss: 1.977699]\n",
      "epoch:28 step:26862 [D loss: 0.316488, acc.: 92.97%] [G loss: 1.542489]\n",
      "epoch:28 step:26863 [D loss: 0.763518, acc.: 56.25%] [G loss: 3.510666]\n",
      "epoch:28 step:26864 [D loss: 0.242663, acc.: 87.50%] [G loss: 5.934564]\n",
      "epoch:28 step:26865 [D loss: 0.216429, acc.: 90.62%] [G loss: 5.752688]\n",
      "epoch:28 step:26866 [D loss: 0.235259, acc.: 92.19%] [G loss: 3.434761]\n",
      "epoch:28 step:26867 [D loss: 1.257299, acc.: 17.97%] [G loss: 2.442898]\n",
      "epoch:28 step:26868 [D loss: 0.090093, acc.: 98.44%] [G loss: 4.467223]\n",
      "epoch:28 step:26869 [D loss: 0.251265, acc.: 93.75%] [G loss: 4.600396]\n",
      "epoch:28 step:26870 [D loss: 0.214611, acc.: 95.31%] [G loss: 4.227339]\n",
      "epoch:28 step:26871 [D loss: 0.353159, acc.: 81.25%] [G loss: 2.997947]\n",
      "epoch:28 step:26872 [D loss: 0.115700, acc.: 98.44%] [G loss: 2.897645]\n",
      "epoch:28 step:26873 [D loss: 0.161957, acc.: 96.09%] [G loss: 3.234235]\n",
      "epoch:28 step:26874 [D loss: 0.220239, acc.: 92.97%] [G loss: 4.206228]\n",
      "epoch:28 step:26875 [D loss: 0.213729, acc.: 95.31%] [G loss: 4.456328]\n",
      "epoch:28 step:26876 [D loss: 0.393233, acc.: 85.94%] [G loss: 5.663698]\n",
      "epoch:28 step:26877 [D loss: 0.198715, acc.: 92.19%] [G loss: 2.210718]\n",
      "epoch:28 step:26878 [D loss: 0.287372, acc.: 91.41%] [G loss: 1.368430]\n",
      "epoch:28 step:26879 [D loss: 0.266931, acc.: 88.28%] [G loss: 4.351443]\n",
      "epoch:28 step:26880 [D loss: 0.381361, acc.: 78.12%] [G loss: 2.774400]\n",
      "epoch:28 step:26881 [D loss: 0.208571, acc.: 94.53%] [G loss: 3.544477]\n",
      "epoch:28 step:26882 [D loss: 0.512037, acc.: 81.25%] [G loss: 1.854166]\n",
      "epoch:28 step:26883 [D loss: 0.501151, acc.: 75.78%] [G loss: 1.886770]\n",
      "epoch:28 step:26884 [D loss: 0.514492, acc.: 71.09%] [G loss: 2.369463]\n",
      "epoch:28 step:26885 [D loss: 0.545139, acc.: 77.34%] [G loss: 2.556094]\n",
      "epoch:28 step:26886 [D loss: 0.295461, acc.: 92.97%] [G loss: 5.121969]\n",
      "epoch:28 step:26887 [D loss: 0.433507, acc.: 75.00%] [G loss: 4.323547]\n",
      "epoch:28 step:26888 [D loss: 0.554484, acc.: 64.84%] [G loss: 0.807741]\n",
      "epoch:28 step:26889 [D loss: 0.289879, acc.: 89.84%] [G loss: 2.060799]\n",
      "epoch:28 step:26890 [D loss: 0.245668, acc.: 92.97%] [G loss: 3.353870]\n",
      "epoch:28 step:26891 [D loss: 1.033680, acc.: 45.31%] [G loss: 2.210335]\n",
      "epoch:28 step:26892 [D loss: 0.126852, acc.: 97.66%] [G loss: 1.511671]\n",
      "epoch:28 step:26893 [D loss: 0.106540, acc.: 98.44%] [G loss: 1.008818]\n",
      "epoch:28 step:26894 [D loss: 0.125489, acc.: 99.22%] [G loss: 1.464615]\n",
      "epoch:28 step:26895 [D loss: 0.064991, acc.: 100.00%] [G loss: 2.851853]\n",
      "epoch:28 step:26896 [D loss: 0.215670, acc.: 92.19%] [G loss: 2.687760]\n",
      "epoch:28 step:26897 [D loss: 0.549916, acc.: 81.25%] [G loss: 1.997709]\n",
      "epoch:28 step:26898 [D loss: 0.223774, acc.: 91.41%] [G loss: 2.187019]\n",
      "epoch:28 step:26899 [D loss: 0.123658, acc.: 99.22%] [G loss: 1.101428]\n",
      "epoch:28 step:26900 [D loss: 0.196868, acc.: 93.75%] [G loss: 2.654793]\n",
      "epoch:28 step:26901 [D loss: 0.342013, acc.: 85.16%] [G loss: 1.129547]\n",
      "epoch:28 step:26902 [D loss: 0.592591, acc.: 71.09%] [G loss: 0.858665]\n",
      "epoch:28 step:26903 [D loss: 0.207915, acc.: 96.09%] [G loss: 2.596251]\n",
      "epoch:28 step:26904 [D loss: 0.964532, acc.: 42.97%] [G loss: 1.376295]\n",
      "epoch:28 step:26905 [D loss: 0.192662, acc.: 97.66%] [G loss: 2.408416]\n",
      "epoch:28 step:26906 [D loss: 0.146874, acc.: 96.88%] [G loss: 3.990385]\n",
      "epoch:28 step:26907 [D loss: 0.048830, acc.: 100.00%] [G loss: 3.607154]\n",
      "epoch:28 step:26908 [D loss: 0.231916, acc.: 89.84%] [G loss: 2.080595]\n",
      "epoch:28 step:26909 [D loss: 0.876598, acc.: 46.88%] [G loss: 1.323188]\n",
      "epoch:28 step:26910 [D loss: 0.053912, acc.: 99.22%] [G loss: 1.841823]\n",
      "epoch:28 step:26911 [D loss: 0.124981, acc.: 96.09%] [G loss: 3.084039]\n",
      "epoch:28 step:26912 [D loss: 0.237299, acc.: 90.62%] [G loss: 3.763733]\n",
      "epoch:28 step:26913 [D loss: 0.297516, acc.: 88.28%] [G loss: 4.357008]\n",
      "epoch:28 step:26914 [D loss: 0.305656, acc.: 89.06%] [G loss: 4.972256]\n",
      "epoch:28 step:26915 [D loss: 0.061448, acc.: 99.22%] [G loss: 3.670960]\n",
      "epoch:28 step:26916 [D loss: 0.622237, acc.: 65.62%] [G loss: 1.360842]\n",
      "epoch:28 step:26917 [D loss: 0.373113, acc.: 78.12%] [G loss: 3.195456]\n",
      "epoch:28 step:26918 [D loss: 0.108438, acc.: 98.44%] [G loss: 2.465021]\n",
      "epoch:28 step:26919 [D loss: 1.667706, acc.: 47.66%] [G loss: 1.326198]\n",
      "epoch:28 step:26920 [D loss: 0.678106, acc.: 64.84%] [G loss: 0.267068]\n",
      "epoch:28 step:26921 [D loss: 0.165036, acc.: 95.31%] [G loss: 1.341189]\n",
      "epoch:28 step:26922 [D loss: 0.313446, acc.: 88.28%] [G loss: 1.396026]\n",
      "epoch:28 step:26923 [D loss: 0.399778, acc.: 78.12%] [G loss: 3.348135]\n",
      "epoch:28 step:26924 [D loss: 0.135824, acc.: 96.09%] [G loss: 3.860491]\n",
      "epoch:28 step:26925 [D loss: 0.380014, acc.: 86.72%] [G loss: 2.233504]\n",
      "epoch:28 step:26926 [D loss: 0.179613, acc.: 96.09%] [G loss: 3.470295]\n",
      "epoch:28 step:26927 [D loss: 0.059951, acc.: 99.22%] [G loss: 2.336103]\n",
      "epoch:28 step:26928 [D loss: 0.418170, acc.: 76.56%] [G loss: 2.538345]\n",
      "epoch:28 step:26929 [D loss: 0.973055, acc.: 39.06%] [G loss: 3.078091]\n",
      "epoch:28 step:26930 [D loss: 0.117698, acc.: 96.88%] [G loss: 2.956492]\n",
      "epoch:28 step:26931 [D loss: 0.164674, acc.: 96.09%] [G loss: 2.447489]\n",
      "epoch:28 step:26932 [D loss: 1.422477, acc.: 35.16%] [G loss: 4.386759]\n",
      "epoch:28 step:26933 [D loss: 0.063573, acc.: 98.44%] [G loss: 2.080143]\n",
      "epoch:28 step:26934 [D loss: 0.310092, acc.: 86.72%] [G loss: 3.882123]\n",
      "epoch:28 step:26935 [D loss: 0.031928, acc.: 99.22%] [G loss: 4.048262]\n",
      "epoch:28 step:26936 [D loss: 0.345275, acc.: 85.16%] [G loss: 3.432443]\n",
      "epoch:28 step:26937 [D loss: 0.268630, acc.: 92.19%] [G loss: 2.963576]\n",
      "epoch:28 step:26938 [D loss: 0.171494, acc.: 95.31%] [G loss: 2.019945]\n",
      "epoch:28 step:26939 [D loss: 1.504399, acc.: 35.16%] [G loss: 1.601339]\n",
      "epoch:28 step:26940 [D loss: 0.067210, acc.: 100.00%] [G loss: 0.886611]\n",
      "epoch:28 step:26941 [D loss: 0.615166, acc.: 65.62%] [G loss: 2.548272]\n",
      "epoch:28 step:26942 [D loss: 0.034895, acc.: 99.22%] [G loss: 3.546970]\n",
      "epoch:28 step:26943 [D loss: 0.052675, acc.: 100.00%] [G loss: 1.118068]\n",
      "epoch:28 step:26944 [D loss: 0.975431, acc.: 51.56%] [G loss: 2.762363]\n",
      "epoch:28 step:26945 [D loss: 0.231017, acc.: 93.75%] [G loss: 1.577841]\n",
      "epoch:28 step:26946 [D loss: 0.146610, acc.: 96.09%] [G loss: 2.580276]\n",
      "epoch:28 step:26947 [D loss: 0.132010, acc.: 99.22%] [G loss: 2.634714]\n",
      "epoch:28 step:26948 [D loss: 0.171323, acc.: 96.09%] [G loss: 2.194221]\n",
      "epoch:28 step:26949 [D loss: 0.242163, acc.: 90.62%] [G loss: 2.405540]\n",
      "epoch:28 step:26950 [D loss: 0.059903, acc.: 99.22%] [G loss: 2.978620]\n",
      "epoch:28 step:26951 [D loss: 0.085070, acc.: 98.44%] [G loss: 3.019704]\n",
      "epoch:28 step:26952 [D loss: 0.120704, acc.: 98.44%] [G loss: 2.184625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26953 [D loss: 0.916059, acc.: 54.69%] [G loss: 2.031673]\n",
      "epoch:28 step:26954 [D loss: 0.073384, acc.: 99.22%] [G loss: 5.308262]\n",
      "epoch:28 step:26955 [D loss: 0.353414, acc.: 84.38%] [G loss: 3.163610]\n",
      "epoch:28 step:26956 [D loss: 0.508910, acc.: 72.66%] [G loss: 1.684207]\n",
      "epoch:28 step:26957 [D loss: 0.902790, acc.: 39.06%] [G loss: 1.051396]\n",
      "epoch:28 step:26958 [D loss: 0.049795, acc.: 100.00%] [G loss: 1.754183]\n",
      "epoch:28 step:26959 [D loss: 0.300173, acc.: 92.97%] [G loss: 2.935513]\n",
      "epoch:28 step:26960 [D loss: 0.374009, acc.: 78.91%] [G loss: 3.507954]\n",
      "epoch:28 step:26961 [D loss: 0.292375, acc.: 89.06%] [G loss: 1.307691]\n",
      "epoch:28 step:26962 [D loss: 0.333128, acc.: 94.53%] [G loss: 0.724411]\n",
      "epoch:28 step:26963 [D loss: 0.100281, acc.: 96.88%] [G loss: 1.932242]\n",
      "epoch:28 step:26964 [D loss: 0.515168, acc.: 66.41%] [G loss: 1.909549]\n",
      "epoch:28 step:26965 [D loss: 0.152723, acc.: 98.44%] [G loss: 3.500644]\n",
      "epoch:28 step:26966 [D loss: 0.628017, acc.: 64.06%] [G loss: 2.525593]\n",
      "epoch:28 step:26967 [D loss: 0.193345, acc.: 95.31%] [G loss: 2.373593]\n",
      "epoch:28 step:26968 [D loss: 0.122693, acc.: 98.44%] [G loss: 1.190595]\n",
      "epoch:28 step:26969 [D loss: 0.122494, acc.: 98.44%] [G loss: 3.322436]\n",
      "epoch:28 step:26970 [D loss: 0.101604, acc.: 98.44%] [G loss: 1.831230]\n",
      "epoch:28 step:26971 [D loss: 0.398225, acc.: 83.59%] [G loss: 2.973458]\n",
      "epoch:28 step:26972 [D loss: 0.055479, acc.: 99.22%] [G loss: 3.118912]\n",
      "epoch:28 step:26973 [D loss: 0.423644, acc.: 73.44%] [G loss: 1.812901]\n",
      "epoch:28 step:26974 [D loss: 0.515606, acc.: 69.53%] [G loss: 2.643970]\n",
      "epoch:28 step:26975 [D loss: 0.427713, acc.: 78.12%] [G loss: 1.824261]\n",
      "epoch:28 step:26976 [D loss: 0.555415, acc.: 69.53%] [G loss: 4.352592]\n",
      "epoch:28 step:26977 [D loss: 0.426683, acc.: 82.03%] [G loss: 1.514904]\n",
      "epoch:28 step:26978 [D loss: 0.158935, acc.: 96.09%] [G loss: 2.665642]\n",
      "epoch:28 step:26979 [D loss: 0.388129, acc.: 80.47%] [G loss: 2.072372]\n",
      "epoch:28 step:26980 [D loss: 0.097375, acc.: 97.66%] [G loss: 3.437438]\n",
      "epoch:28 step:26981 [D loss: 0.492825, acc.: 75.00%] [G loss: 3.223387]\n",
      "epoch:28 step:26982 [D loss: 0.127241, acc.: 96.88%] [G loss: 0.454039]\n",
      "epoch:28 step:26983 [D loss: 0.253740, acc.: 92.97%] [G loss: 2.211649]\n",
      "epoch:28 step:26984 [D loss: 0.541378, acc.: 73.44%] [G loss: 0.473164]\n",
      "epoch:28 step:26985 [D loss: 0.577403, acc.: 64.06%] [G loss: 3.737353]\n",
      "epoch:28 step:26986 [D loss: 0.061469, acc.: 99.22%] [G loss: 1.309173]\n",
      "epoch:28 step:26987 [D loss: 0.225564, acc.: 96.09%] [G loss: 1.273752]\n",
      "epoch:28 step:26988 [D loss: 0.174422, acc.: 97.66%] [G loss: 1.628042]\n",
      "epoch:28 step:26989 [D loss: 0.137006, acc.: 97.66%] [G loss: 3.196761]\n",
      "epoch:28 step:26990 [D loss: 0.473853, acc.: 75.00%] [G loss: 1.397584]\n",
      "epoch:28 step:26991 [D loss: 0.530423, acc.: 66.41%] [G loss: 7.208090]\n",
      "epoch:28 step:26992 [D loss: 0.123270, acc.: 97.66%] [G loss: 1.839645]\n",
      "epoch:28 step:26993 [D loss: 1.432695, acc.: 17.97%] [G loss: 3.412928]\n",
      "epoch:28 step:26994 [D loss: 0.372104, acc.: 77.34%] [G loss: 1.417394]\n",
      "epoch:28 step:26995 [D loss: 0.281806, acc.: 90.62%] [G loss: 1.100403]\n",
      "epoch:28 step:26996 [D loss: 0.042634, acc.: 99.22%] [G loss: 5.361032]\n",
      "epoch:28 step:26997 [D loss: 0.084792, acc.: 98.44%] [G loss: 1.403505]\n",
      "epoch:28 step:26998 [D loss: 0.349012, acc.: 85.16%] [G loss: 1.137566]\n",
      "epoch:28 step:26999 [D loss: 1.474745, acc.: 31.25%] [G loss: 1.401417]\n",
      "epoch:28 step:27000 [D loss: 0.338801, acc.: 82.03%] [G loss: 2.306879]\n",
      "epoch:28 step:27001 [D loss: 0.509520, acc.: 71.09%] [G loss: 3.210747]\n",
      "epoch:28 step:27002 [D loss: 0.104837, acc.: 97.66%] [G loss: 2.900123]\n",
      "epoch:28 step:27003 [D loss: 0.336064, acc.: 87.50%] [G loss: 2.649389]\n",
      "epoch:28 step:27004 [D loss: 0.254183, acc.: 91.41%] [G loss: 3.714806]\n",
      "epoch:28 step:27005 [D loss: 0.115256, acc.: 99.22%] [G loss: 1.751268]\n",
      "epoch:28 step:27006 [D loss: 0.350488, acc.: 85.94%] [G loss: 3.016664]\n",
      "epoch:28 step:27007 [D loss: 0.121231, acc.: 99.22%] [G loss: 2.680587]\n",
      "epoch:28 step:27008 [D loss: 0.084503, acc.: 99.22%] [G loss: 4.394313]\n",
      "epoch:28 step:27009 [D loss: 0.402990, acc.: 78.12%] [G loss: 1.073119]\n",
      "epoch:28 step:27010 [D loss: 0.166512, acc.: 97.66%] [G loss: 1.229003]\n",
      "epoch:28 step:27011 [D loss: 0.652741, acc.: 62.50%] [G loss: 5.304030]\n",
      "epoch:28 step:27012 [D loss: 0.063281, acc.: 99.22%] [G loss: 3.228150]\n",
      "epoch:28 step:27013 [D loss: 0.158450, acc.: 96.09%] [G loss: 5.233266]\n",
      "epoch:28 step:27014 [D loss: 0.118238, acc.: 98.44%] [G loss: 4.342498]\n",
      "epoch:28 step:27015 [D loss: 0.993028, acc.: 30.47%] [G loss: 3.965983]\n",
      "epoch:28 step:27016 [D loss: 0.364678, acc.: 82.03%] [G loss: 1.976571]\n",
      "epoch:28 step:27017 [D loss: 0.078140, acc.: 100.00%] [G loss: 2.309767]\n",
      "epoch:28 step:27018 [D loss: 0.093556, acc.: 99.22%] [G loss: 1.402174]\n",
      "epoch:28 step:27019 [D loss: 0.445484, acc.: 75.00%] [G loss: 3.705249]\n",
      "epoch:28 step:27020 [D loss: 0.186976, acc.: 96.09%] [G loss: 3.232736]\n",
      "epoch:28 step:27021 [D loss: 1.041486, acc.: 47.66%] [G loss: 3.056052]\n",
      "epoch:28 step:27022 [D loss: 0.539608, acc.: 71.88%] [G loss: 1.520899]\n",
      "epoch:28 step:27023 [D loss: 1.123423, acc.: 56.25%] [G loss: 2.340874]\n",
      "epoch:28 step:27024 [D loss: 0.403284, acc.: 77.34%] [G loss: 1.875300]\n",
      "epoch:28 step:27025 [D loss: 0.351935, acc.: 88.28%] [G loss: 0.825400]\n",
      "epoch:28 step:27026 [D loss: 0.097478, acc.: 97.66%] [G loss: 1.288850]\n",
      "epoch:28 step:27027 [D loss: 0.087211, acc.: 99.22%] [G loss: 2.056414]\n",
      "epoch:28 step:27028 [D loss: 0.491747, acc.: 72.66%] [G loss: 2.281683]\n",
      "epoch:28 step:27029 [D loss: 0.047940, acc.: 99.22%] [G loss: 3.047427]\n",
      "epoch:28 step:27030 [D loss: 0.282286, acc.: 89.06%] [G loss: 2.601710]\n",
      "epoch:28 step:27031 [D loss: 0.562398, acc.: 73.44%] [G loss: 1.221117]\n",
      "epoch:28 step:27032 [D loss: 0.866768, acc.: 56.25%] [G loss: 3.770400]\n",
      "epoch:28 step:27033 [D loss: 0.184861, acc.: 92.19%] [G loss: 2.833735]\n",
      "epoch:28 step:27034 [D loss: 0.140422, acc.: 97.66%] [G loss: 4.645274]\n",
      "epoch:28 step:27035 [D loss: 0.761065, acc.: 60.16%] [G loss: 1.766906]\n",
      "epoch:28 step:27036 [D loss: 0.152338, acc.: 97.66%] [G loss: 1.995140]\n",
      "epoch:28 step:27037 [D loss: 0.133725, acc.: 96.09%] [G loss: 1.973555]\n",
      "epoch:28 step:27038 [D loss: 0.296926, acc.: 88.28%] [G loss: 4.855766]\n",
      "epoch:28 step:27039 [D loss: 0.432111, acc.: 81.25%] [G loss: 1.569929]\n",
      "epoch:28 step:27040 [D loss: 0.446767, acc.: 81.25%] [G loss: 1.353750]\n",
      "epoch:28 step:27041 [D loss: 0.204639, acc.: 95.31%] [G loss: 3.248080]\n",
      "epoch:28 step:27042 [D loss: 0.152149, acc.: 96.88%] [G loss: 2.125008]\n",
      "epoch:28 step:27043 [D loss: 0.361296, acc.: 89.06%] [G loss: 1.948365]\n",
      "epoch:28 step:27044 [D loss: 0.321111, acc.: 89.06%] [G loss: 1.988260]\n",
      "epoch:28 step:27045 [D loss: 0.196314, acc.: 97.66%] [G loss: 3.211413]\n",
      "epoch:28 step:27046 [D loss: 0.477557, acc.: 75.78%] [G loss: 3.522147]\n",
      "epoch:28 step:27047 [D loss: 0.321411, acc.: 84.38%] [G loss: 1.641144]\n",
      "epoch:28 step:27048 [D loss: 0.205189, acc.: 98.44%] [G loss: 0.303290]\n",
      "epoch:28 step:27049 [D loss: 0.514898, acc.: 67.19%] [G loss: 2.951463]\n",
      "epoch:28 step:27050 [D loss: 0.035870, acc.: 100.00%] [G loss: 1.275845]\n",
      "epoch:28 step:27051 [D loss: 0.188694, acc.: 95.31%] [G loss: 1.840378]\n",
      "epoch:28 step:27052 [D loss: 0.752977, acc.: 60.16%] [G loss: 2.873036]\n",
      "epoch:28 step:27053 [D loss: 0.100402, acc.: 98.44%] [G loss: 2.176656]\n",
      "epoch:28 step:27054 [D loss: 0.350570, acc.: 78.12%] [G loss: 3.331303]\n",
      "epoch:28 step:27055 [D loss: 0.274085, acc.: 91.41%] [G loss: 4.604295]\n",
      "epoch:28 step:27056 [D loss: 0.560371, acc.: 75.00%] [G loss: 0.942895]\n",
      "epoch:28 step:27057 [D loss: 0.048549, acc.: 99.22%] [G loss: 3.792827]\n",
      "epoch:28 step:27058 [D loss: 0.382149, acc.: 80.47%] [G loss: 0.831957]\n",
      "epoch:28 step:27059 [D loss: 0.085140, acc.: 99.22%] [G loss: 1.428335]\n",
      "epoch:28 step:27060 [D loss: 0.541471, acc.: 64.84%] [G loss: 0.410393]\n",
      "epoch:28 step:27061 [D loss: 0.357574, acc.: 82.81%] [G loss: 5.370785]\n",
      "epoch:28 step:27062 [D loss: 0.434217, acc.: 69.53%] [G loss: 3.666143]\n",
      "epoch:28 step:27063 [D loss: 0.181528, acc.: 96.88%] [G loss: 2.297094]\n",
      "epoch:28 step:27064 [D loss: 0.154665, acc.: 96.88%] [G loss: 4.324131]\n",
      "epoch:28 step:27065 [D loss: 0.328442, acc.: 85.94%] [G loss: 5.191150]\n",
      "epoch:28 step:27066 [D loss: 1.490068, acc.: 51.56%] [G loss: 1.592714]\n",
      "epoch:28 step:27067 [D loss: 0.051228, acc.: 100.00%] [G loss: 0.725894]\n",
      "epoch:28 step:27068 [D loss: 0.591117, acc.: 67.97%] [G loss: 2.499659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:27069 [D loss: 0.098117, acc.: 98.44%] [G loss: 3.096228]\n",
      "epoch:28 step:27070 [D loss: 0.097218, acc.: 99.22%] [G loss: 1.625994]\n",
      "epoch:28 step:27071 [D loss: 0.144030, acc.: 96.09%] [G loss: 3.389915]\n",
      "epoch:28 step:27072 [D loss: 0.182190, acc.: 97.66%] [G loss: 2.970523]\n",
      "epoch:28 step:27073 [D loss: 0.126407, acc.: 99.22%] [G loss: 3.643259]\n",
      "epoch:28 step:27074 [D loss: 0.136954, acc.: 98.44%] [G loss: 2.560547]\n",
      "epoch:28 step:27075 [D loss: 0.049032, acc.: 98.44%] [G loss: 1.877689]\n",
      "epoch:28 step:27076 [D loss: 0.232204, acc.: 91.41%] [G loss: 2.321118]\n",
      "epoch:28 step:27077 [D loss: 0.183489, acc.: 96.09%] [G loss: 1.814840]\n",
      "epoch:28 step:27078 [D loss: 1.069626, acc.: 44.53%] [G loss: 3.129894]\n",
      "epoch:28 step:27079 [D loss: 0.620451, acc.: 60.94%] [G loss: 1.862768]\n",
      "epoch:28 step:27080 [D loss: 0.239815, acc.: 93.75%] [G loss: 3.020318]\n",
      "epoch:28 step:27081 [D loss: 0.232968, acc.: 91.41%] [G loss: 2.253319]\n",
      "epoch:28 step:27082 [D loss: 0.143562, acc.: 96.09%] [G loss: 2.960430]\n",
      "epoch:28 step:27083 [D loss: 0.191166, acc.: 96.88%] [G loss: 2.110562]\n",
      "epoch:28 step:27084 [D loss: 0.287214, acc.: 89.06%] [G loss: 4.624260]\n",
      "epoch:28 step:27085 [D loss: 0.282417, acc.: 92.97%] [G loss: 3.893956]\n",
      "epoch:28 step:27086 [D loss: 0.308742, acc.: 89.84%] [G loss: 3.557852]\n",
      "epoch:28 step:27087 [D loss: 0.696828, acc.: 63.28%] [G loss: 1.406372]\n",
      "epoch:28 step:27088 [D loss: 1.155020, acc.: 55.47%] [G loss: 1.862408]\n",
      "epoch:28 step:27089 [D loss: 0.424367, acc.: 83.59%] [G loss: 1.730362]\n",
      "epoch:28 step:27090 [D loss: 0.732230, acc.: 57.81%] [G loss: 1.948318]\n",
      "epoch:28 step:27091 [D loss: 0.251963, acc.: 95.31%] [G loss: 3.931195]\n",
      "epoch:28 step:27092 [D loss: 0.358148, acc.: 81.25%] [G loss: 2.722725]\n",
      "epoch:28 step:27093 [D loss: 0.486672, acc.: 71.09%] [G loss: 5.564279]\n",
      "epoch:28 step:27094 [D loss: 0.104455, acc.: 98.44%] [G loss: 2.558328]\n",
      "epoch:28 step:27095 [D loss: 0.162951, acc.: 98.44%] [G loss: 3.259674]\n",
      "epoch:28 step:27096 [D loss: 0.175506, acc.: 98.44%] [G loss: 3.455814]\n",
      "epoch:28 step:27097 [D loss: 0.484520, acc.: 75.00%] [G loss: 6.330207]\n",
      "epoch:28 step:27098 [D loss: 0.416558, acc.: 76.56%] [G loss: 1.160785]\n",
      "epoch:28 step:27099 [D loss: 0.122286, acc.: 96.09%] [G loss: 4.574440]\n",
      "epoch:28 step:27100 [D loss: 0.154232, acc.: 96.88%] [G loss: 3.198457]\n",
      "epoch:28 step:27101 [D loss: 0.697345, acc.: 62.50%] [G loss: 2.326816]\n",
      "epoch:28 step:27102 [D loss: 0.613539, acc.: 68.75%] [G loss: 2.891522]\n",
      "epoch:28 step:27103 [D loss: 0.638028, acc.: 60.16%] [G loss: 2.934465]\n",
      "epoch:28 step:27104 [D loss: 0.062062, acc.: 99.22%] [G loss: 4.869400]\n",
      "epoch:28 step:27105 [D loss: 0.071469, acc.: 98.44%] [G loss: 4.189966]\n",
      "epoch:28 step:27106 [D loss: 0.153829, acc.: 97.66%] [G loss: 4.977036]\n",
      "epoch:28 step:27107 [D loss: 0.155777, acc.: 97.66%] [G loss: 5.890543]\n",
      "epoch:28 step:27108 [D loss: 0.280905, acc.: 85.94%] [G loss: 1.940383]\n",
      "epoch:28 step:27109 [D loss: 0.223985, acc.: 93.75%] [G loss: 2.710599]\n",
      "epoch:28 step:27110 [D loss: 0.168644, acc.: 96.09%] [G loss: 5.563444]\n",
      "epoch:28 step:27111 [D loss: 1.060778, acc.: 45.31%] [G loss: 1.594217]\n",
      "epoch:28 step:27112 [D loss: 0.087602, acc.: 100.00%] [G loss: 5.090778]\n",
      "epoch:28 step:27113 [D loss: 0.085357, acc.: 99.22%] [G loss: 4.047405]\n",
      "epoch:28 step:27114 [D loss: 0.351768, acc.: 89.06%] [G loss: 6.277692]\n",
      "epoch:28 step:27115 [D loss: 0.302991, acc.: 92.19%] [G loss: 2.632186]\n",
      "epoch:28 step:27116 [D loss: 0.479628, acc.: 68.75%] [G loss: 4.953369]\n",
      "epoch:28 step:27117 [D loss: 0.248833, acc.: 94.53%] [G loss: 4.067607]\n",
      "epoch:28 step:27118 [D loss: 0.190518, acc.: 93.75%] [G loss: 4.617283]\n",
      "epoch:28 step:27119 [D loss: 0.190015, acc.: 94.53%] [G loss: 1.867639]\n",
      "epoch:28 step:27120 [D loss: 0.522014, acc.: 75.78%] [G loss: 1.005361]\n",
      "epoch:28 step:27121 [D loss: 0.132921, acc.: 96.09%] [G loss: 2.896984]\n",
      "epoch:28 step:27122 [D loss: 0.127627, acc.: 96.88%] [G loss: 4.232580]\n",
      "epoch:28 step:27123 [D loss: 0.224088, acc.: 95.31%] [G loss: 4.789874]\n",
      "epoch:28 step:27124 [D loss: 0.043952, acc.: 99.22%] [G loss: 2.690131]\n",
      "epoch:28 step:27125 [D loss: 0.144366, acc.: 97.66%] [G loss: 2.674981]\n",
      "epoch:28 step:27126 [D loss: 0.141888, acc.: 97.66%] [G loss: 4.337138]\n",
      "epoch:28 step:27127 [D loss: 0.194873, acc.: 94.53%] [G loss: 3.365983]\n",
      "epoch:28 step:27128 [D loss: 0.322798, acc.: 89.84%] [G loss: 2.557603]\n",
      "epoch:28 step:27129 [D loss: 0.131615, acc.: 99.22%] [G loss: 5.428626]\n",
      "epoch:28 step:27130 [D loss: 0.138712, acc.: 96.88%] [G loss: 4.341728]\n",
      "epoch:28 step:27131 [D loss: 0.634995, acc.: 64.06%] [G loss: 4.070522]\n",
      "epoch:28 step:27132 [D loss: 0.127700, acc.: 96.09%] [G loss: 2.216439]\n",
      "epoch:28 step:27133 [D loss: 0.170724, acc.: 96.09%] [G loss: 5.827265]\n",
      "epoch:28 step:27134 [D loss: 0.708648, acc.: 60.94%] [G loss: 1.905015]\n",
      "epoch:28 step:27135 [D loss: 0.976670, acc.: 40.62%] [G loss: 0.869411]\n",
      "epoch:28 step:27136 [D loss: 0.449009, acc.: 73.44%] [G loss: 2.362441]\n",
      "epoch:28 step:27137 [D loss: 0.311143, acc.: 89.06%] [G loss: 3.073865]\n",
      "epoch:28 step:27138 [D loss: 0.141390, acc.: 96.09%] [G loss: 1.249925]\n",
      "epoch:28 step:27139 [D loss: 0.247314, acc.: 96.09%] [G loss: 1.684628]\n",
      "epoch:28 step:27140 [D loss: 0.243690, acc.: 89.06%] [G loss: 4.167649]\n",
      "epoch:28 step:27141 [D loss: 0.475296, acc.: 71.88%] [G loss: 0.767442]\n",
      "epoch:28 step:27142 [D loss: 0.152014, acc.: 97.66%] [G loss: 1.545636]\n",
      "epoch:28 step:27143 [D loss: 0.424885, acc.: 74.22%] [G loss: 2.251383]\n",
      "epoch:28 step:27144 [D loss: 0.380342, acc.: 85.16%] [G loss: 2.566393]\n",
      "epoch:28 step:27145 [D loss: 0.129159, acc.: 97.66%] [G loss: 0.259741]\n",
      "epoch:28 step:27146 [D loss: 0.160456, acc.: 96.09%] [G loss: 5.193824]\n",
      "epoch:28 step:27147 [D loss: 0.243814, acc.: 92.97%] [G loss: 2.953262]\n",
      "epoch:28 step:27148 [D loss: 0.325009, acc.: 82.03%] [G loss: 0.788063]\n",
      "epoch:28 step:27149 [D loss: 0.651947, acc.: 64.06%] [G loss: 0.727585]\n",
      "epoch:28 step:27150 [D loss: 0.101421, acc.: 97.66%] [G loss: 0.998966]\n",
      "epoch:28 step:27151 [D loss: 0.486025, acc.: 80.47%] [G loss: 0.636810]\n",
      "epoch:28 step:27152 [D loss: 0.709296, acc.: 58.59%] [G loss: 3.679204]\n",
      "epoch:28 step:27153 [D loss: 0.050764, acc.: 99.22%] [G loss: 0.362749]\n",
      "epoch:28 step:27154 [D loss: 0.665125, acc.: 64.84%] [G loss: 7.855486]\n",
      "epoch:28 step:27155 [D loss: 0.186008, acc.: 91.41%] [G loss: 1.321458]\n",
      "epoch:28 step:27156 [D loss: 0.810919, acc.: 50.78%] [G loss: 7.530002]\n",
      "epoch:28 step:27157 [D loss: 0.461012, acc.: 75.00%] [G loss: 5.389801]\n",
      "epoch:28 step:27158 [D loss: 0.612985, acc.: 63.28%] [G loss: 2.015728]\n",
      "epoch:28 step:27159 [D loss: 0.092144, acc.: 100.00%] [G loss: 5.341329]\n",
      "epoch:28 step:27160 [D loss: 0.284687, acc.: 88.28%] [G loss: 3.282113]\n",
      "epoch:28 step:27161 [D loss: 0.232366, acc.: 92.97%] [G loss: 2.199888]\n",
      "epoch:28 step:27162 [D loss: 1.666803, acc.: 30.47%] [G loss: 1.082763]\n",
      "epoch:28 step:27163 [D loss: 0.643744, acc.: 66.41%] [G loss: 2.838892]\n",
      "epoch:28 step:27164 [D loss: 0.412228, acc.: 77.34%] [G loss: 2.191114]\n",
      "epoch:28 step:27165 [D loss: 0.090926, acc.: 98.44%] [G loss: 1.175253]\n",
      "epoch:28 step:27166 [D loss: 0.048640, acc.: 98.44%] [G loss: 1.609258]\n",
      "epoch:28 step:27167 [D loss: 0.373141, acc.: 78.91%] [G loss: 5.688005]\n",
      "epoch:28 step:27168 [D loss: 0.618227, acc.: 64.84%] [G loss: 1.908134]\n",
      "epoch:28 step:27169 [D loss: 0.925175, acc.: 46.88%] [G loss: 3.552852]\n",
      "epoch:28 step:27170 [D loss: 0.484865, acc.: 72.66%] [G loss: 1.208138]\n",
      "epoch:28 step:27171 [D loss: 0.424744, acc.: 78.91%] [G loss: 1.142108]\n",
      "epoch:28 step:27172 [D loss: 0.285202, acc.: 90.62%] [G loss: 3.150398]\n",
      "epoch:28 step:27173 [D loss: 0.415001, acc.: 79.69%] [G loss: 3.024827]\n",
      "epoch:29 step:27174 [D loss: 0.209110, acc.: 95.31%] [G loss: 3.174903]\n",
      "epoch:29 step:27175 [D loss: 0.819860, acc.: 57.81%] [G loss: 6.374317]\n",
      "epoch:29 step:27176 [D loss: 0.932558, acc.: 62.50%] [G loss: 3.264742]\n",
      "epoch:29 step:27177 [D loss: 1.313874, acc.: 42.97%] [G loss: 2.537024]\n",
      "epoch:29 step:27178 [D loss: 0.099163, acc.: 99.22%] [G loss: 3.815419]\n",
      "epoch:29 step:27179 [D loss: 0.977562, acc.: 43.75%] [G loss: 4.381508]\n",
      "epoch:29 step:27180 [D loss: 0.154260, acc.: 98.44%] [G loss: 3.857006]\n",
      "epoch:29 step:27181 [D loss: 0.343307, acc.: 84.38%] [G loss: 3.680972]\n",
      "epoch:29 step:27182 [D loss: 0.321631, acc.: 85.94%] [G loss: 3.097116]\n",
      "epoch:29 step:27183 [D loss: 0.462447, acc.: 76.56%] [G loss: 2.477772]\n",
      "epoch:29 step:27184 [D loss: 0.971796, acc.: 35.94%] [G loss: 1.628746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27185 [D loss: 0.247029, acc.: 91.41%] [G loss: 2.463004]\n",
      "epoch:29 step:27186 [D loss: 1.020800, acc.: 53.12%] [G loss: 1.512128]\n",
      "epoch:29 step:27187 [D loss: 0.409525, acc.: 75.78%] [G loss: 2.241059]\n",
      "epoch:29 step:27188 [D loss: 0.187671, acc.: 93.75%] [G loss: 4.536275]\n",
      "epoch:29 step:27189 [D loss: 0.234115, acc.: 92.97%] [G loss: 3.413599]\n",
      "epoch:29 step:27190 [D loss: 0.196180, acc.: 96.09%] [G loss: 2.949903]\n",
      "epoch:29 step:27191 [D loss: 0.244998, acc.: 89.84%] [G loss: 3.782085]\n",
      "epoch:29 step:27192 [D loss: 1.052560, acc.: 28.12%] [G loss: 1.776290]\n",
      "epoch:29 step:27193 [D loss: 0.187702, acc.: 96.09%] [G loss: 2.470993]\n",
      "epoch:29 step:27194 [D loss: 0.208169, acc.: 96.88%] [G loss: 1.518448]\n",
      "epoch:29 step:27195 [D loss: 0.352942, acc.: 82.81%] [G loss: 1.123848]\n",
      "epoch:29 step:27196 [D loss: 0.622220, acc.: 68.75%] [G loss: 1.265155]\n",
      "epoch:29 step:27197 [D loss: 0.331881, acc.: 92.19%] [G loss: 3.527187]\n",
      "epoch:29 step:27198 [D loss: 1.032672, acc.: 53.12%] [G loss: 1.525653]\n",
      "epoch:29 step:27199 [D loss: 0.520846, acc.: 68.75%] [G loss: 1.515109]\n",
      "epoch:29 step:27200 [D loss: 0.171254, acc.: 96.09%] [G loss: 1.563734]\n",
      "epoch:29 step:27201 [D loss: 0.454979, acc.: 78.12%] [G loss: 1.512137]\n",
      "epoch:29 step:27202 [D loss: 0.485292, acc.: 65.62%] [G loss: 1.352036]\n",
      "epoch:29 step:27203 [D loss: 0.207728, acc.: 97.66%] [G loss: 1.317952]\n",
      "epoch:29 step:27204 [D loss: 0.287263, acc.: 92.97%] [G loss: 2.955727]\n",
      "epoch:29 step:27205 [D loss: 0.515375, acc.: 80.47%] [G loss: 2.962433]\n",
      "epoch:29 step:27206 [D loss: 0.878944, acc.: 57.81%] [G loss: 4.179029]\n",
      "epoch:29 step:27207 [D loss: 0.183374, acc.: 92.97%] [G loss: 2.308110]\n",
      "epoch:29 step:27208 [D loss: 0.511236, acc.: 78.91%] [G loss: 3.585263]\n",
      "epoch:29 step:27209 [D loss: 0.211649, acc.: 99.22%] [G loss: 0.852500]\n",
      "epoch:29 step:27210 [D loss: 0.106475, acc.: 100.00%] [G loss: 3.310964]\n",
      "epoch:29 step:27211 [D loss: 0.823317, acc.: 60.94%] [G loss: 1.225368]\n",
      "epoch:29 step:27212 [D loss: 0.153519, acc.: 96.88%] [G loss: 1.116074]\n",
      "epoch:29 step:27213 [D loss: 0.413134, acc.: 84.38%] [G loss: 1.984917]\n",
      "epoch:29 step:27214 [D loss: 0.597299, acc.: 67.19%] [G loss: 2.301257]\n",
      "epoch:29 step:27215 [D loss: 0.200202, acc.: 97.66%] [G loss: 2.028691]\n",
      "epoch:29 step:27216 [D loss: 0.273287, acc.: 91.41%] [G loss: 1.786350]\n",
      "epoch:29 step:27217 [D loss: 1.188282, acc.: 30.47%] [G loss: 2.077963]\n",
      "epoch:29 step:27218 [D loss: 0.385100, acc.: 82.03%] [G loss: 1.165396]\n",
      "epoch:29 step:27219 [D loss: 0.446468, acc.: 75.00%] [G loss: 2.327342]\n",
      "epoch:29 step:27220 [D loss: 0.633698, acc.: 61.72%] [G loss: 1.471778]\n",
      "epoch:29 step:27221 [D loss: 0.641260, acc.: 69.53%] [G loss: 1.554458]\n",
      "epoch:29 step:27222 [D loss: 0.230899, acc.: 93.75%] [G loss: 2.588245]\n",
      "epoch:29 step:27223 [D loss: 0.179709, acc.: 96.09%] [G loss: 1.438328]\n",
      "epoch:29 step:27224 [D loss: 0.479474, acc.: 75.00%] [G loss: 3.654201]\n",
      "epoch:29 step:27225 [D loss: 0.349143, acc.: 79.69%] [G loss: 2.401170]\n",
      "epoch:29 step:27226 [D loss: 0.358902, acc.: 82.81%] [G loss: 0.385989]\n",
      "epoch:29 step:27227 [D loss: 0.497928, acc.: 71.88%] [G loss: 2.097777]\n",
      "epoch:29 step:27228 [D loss: 0.173970, acc.: 95.31%] [G loss: 1.752873]\n",
      "epoch:29 step:27229 [D loss: 0.168773, acc.: 98.44%] [G loss: 4.076066]\n",
      "epoch:29 step:27230 [D loss: 0.398833, acc.: 78.91%] [G loss: 1.437830]\n",
      "epoch:29 step:27231 [D loss: 0.092314, acc.: 99.22%] [G loss: 3.240365]\n",
      "epoch:29 step:27232 [D loss: 0.379522, acc.: 83.59%] [G loss: 1.593411]\n",
      "epoch:29 step:27233 [D loss: 0.285492, acc.: 93.75%] [G loss: 2.492585]\n",
      "epoch:29 step:27234 [D loss: 0.470527, acc.: 74.22%] [G loss: 2.986130]\n",
      "epoch:29 step:27235 [D loss: 0.254568, acc.: 89.06%] [G loss: 3.723657]\n",
      "epoch:29 step:27236 [D loss: 0.032339, acc.: 100.00%] [G loss: 2.839218]\n",
      "epoch:29 step:27237 [D loss: 0.159936, acc.: 96.88%] [G loss: 1.303436]\n",
      "epoch:29 step:27238 [D loss: 0.129788, acc.: 96.88%] [G loss: 3.845630]\n",
      "epoch:29 step:27239 [D loss: 0.147464, acc.: 96.09%] [G loss: 1.106896]\n",
      "epoch:29 step:27240 [D loss: 0.126976, acc.: 99.22%] [G loss: 4.747445]\n",
      "epoch:29 step:27241 [D loss: 0.347504, acc.: 89.84%] [G loss: 1.784643]\n",
      "epoch:29 step:27242 [D loss: 0.213815, acc.: 92.19%] [G loss: 4.534426]\n",
      "epoch:29 step:27243 [D loss: 0.170973, acc.: 96.88%] [G loss: 1.484102]\n",
      "epoch:29 step:27244 [D loss: 1.037021, acc.: 35.94%] [G loss: 1.419212]\n",
      "epoch:29 step:27245 [D loss: 0.230854, acc.: 93.75%] [G loss: 4.514323]\n",
      "epoch:29 step:27246 [D loss: 0.164392, acc.: 95.31%] [G loss: 4.126178]\n",
      "epoch:29 step:27247 [D loss: 0.525086, acc.: 67.19%] [G loss: 3.857874]\n",
      "epoch:29 step:27248 [D loss: 0.627853, acc.: 63.28%] [G loss: 2.741542]\n",
      "epoch:29 step:27249 [D loss: 0.389830, acc.: 78.91%] [G loss: 4.363755]\n",
      "epoch:29 step:27250 [D loss: 0.067006, acc.: 100.00%] [G loss: 5.719966]\n",
      "epoch:29 step:27251 [D loss: 0.177069, acc.: 98.44%] [G loss: 3.732167]\n",
      "epoch:29 step:27252 [D loss: 0.967662, acc.: 54.69%] [G loss: 1.452719]\n",
      "epoch:29 step:27253 [D loss: 0.189269, acc.: 96.09%] [G loss: 3.870587]\n",
      "epoch:29 step:27254 [D loss: 0.211556, acc.: 92.97%] [G loss: 1.359929]\n",
      "epoch:29 step:27255 [D loss: 0.394290, acc.: 85.16%] [G loss: 1.765845]\n",
      "epoch:29 step:27256 [D loss: 1.502985, acc.: 51.56%] [G loss: 1.545921]\n",
      "epoch:29 step:27257 [D loss: 0.295153, acc.: 91.41%] [G loss: 2.090377]\n",
      "epoch:29 step:27258 [D loss: 0.146314, acc.: 95.31%] [G loss: 2.651335]\n",
      "epoch:29 step:27259 [D loss: 0.130684, acc.: 96.88%] [G loss: 4.441484]\n",
      "epoch:29 step:27260 [D loss: 0.502861, acc.: 69.53%] [G loss: 3.564955]\n",
      "epoch:29 step:27261 [D loss: 0.306471, acc.: 90.62%] [G loss: 2.164449]\n",
      "epoch:29 step:27262 [D loss: 0.058158, acc.: 100.00%] [G loss: 2.754192]\n",
      "epoch:29 step:27263 [D loss: 0.586168, acc.: 64.06%] [G loss: 1.023174]\n",
      "epoch:29 step:27264 [D loss: 0.583039, acc.: 73.44%] [G loss: 2.641438]\n",
      "epoch:29 step:27265 [D loss: 0.066479, acc.: 99.22%] [G loss: 1.438366]\n",
      "epoch:29 step:27266 [D loss: 0.095975, acc.: 99.22%] [G loss: 2.240182]\n",
      "epoch:29 step:27267 [D loss: 0.042737, acc.: 100.00%] [G loss: 4.211679]\n",
      "epoch:29 step:27268 [D loss: 0.155457, acc.: 96.88%] [G loss: 5.560618]\n",
      "epoch:29 step:27269 [D loss: 0.684986, acc.: 62.50%] [G loss: 1.446534]\n",
      "epoch:29 step:27270 [D loss: 0.107454, acc.: 97.66%] [G loss: 3.603048]\n",
      "epoch:29 step:27271 [D loss: 0.259291, acc.: 89.84%] [G loss: 1.931647]\n",
      "epoch:29 step:27272 [D loss: 0.182588, acc.: 95.31%] [G loss: 2.052276]\n",
      "epoch:29 step:27273 [D loss: 0.136699, acc.: 100.00%] [G loss: 3.085488]\n",
      "epoch:29 step:27274 [D loss: 0.271478, acc.: 89.84%] [G loss: 3.192059]\n",
      "epoch:29 step:27275 [D loss: 0.272877, acc.: 89.06%] [G loss: 2.357225]\n",
      "epoch:29 step:27276 [D loss: 0.113546, acc.: 99.22%] [G loss: 1.877285]\n",
      "epoch:29 step:27277 [D loss: 0.121888, acc.: 96.88%] [G loss: 2.793610]\n",
      "epoch:29 step:27278 [D loss: 0.176679, acc.: 96.09%] [G loss: 0.771685]\n",
      "epoch:29 step:27279 [D loss: 0.629757, acc.: 64.84%] [G loss: 2.055706]\n",
      "epoch:29 step:27280 [D loss: 0.074855, acc.: 100.00%] [G loss: 3.552274]\n",
      "epoch:29 step:27281 [D loss: 0.480126, acc.: 77.34%] [G loss: 0.758426]\n",
      "epoch:29 step:27282 [D loss: 0.173411, acc.: 97.66%] [G loss: 1.828743]\n",
      "epoch:29 step:27283 [D loss: 0.213116, acc.: 95.31%] [G loss: 1.618412]\n",
      "epoch:29 step:27284 [D loss: 0.123303, acc.: 97.66%] [G loss: 2.892657]\n",
      "epoch:29 step:27285 [D loss: 0.300911, acc.: 85.94%] [G loss: 1.250692]\n",
      "epoch:29 step:27286 [D loss: 0.285402, acc.: 84.38%] [G loss: 2.612707]\n",
      "epoch:29 step:27287 [D loss: 0.110869, acc.: 96.09%] [G loss: 1.615495]\n",
      "epoch:29 step:27288 [D loss: 0.330151, acc.: 88.28%] [G loss: 1.984957]\n",
      "epoch:29 step:27289 [D loss: 0.074013, acc.: 99.22%] [G loss: 1.474441]\n",
      "epoch:29 step:27290 [D loss: 0.170064, acc.: 94.53%] [G loss: 8.727205]\n",
      "epoch:29 step:27291 [D loss: 0.968843, acc.: 45.31%] [G loss: 4.315039]\n",
      "epoch:29 step:27292 [D loss: 0.081597, acc.: 98.44%] [G loss: 2.185364]\n",
      "epoch:29 step:27293 [D loss: 0.430654, acc.: 74.22%] [G loss: 0.713137]\n",
      "epoch:29 step:27294 [D loss: 0.081408, acc.: 100.00%] [G loss: 0.644234]\n",
      "epoch:29 step:27295 [D loss: 0.068082, acc.: 98.44%] [G loss: 2.180818]\n",
      "epoch:29 step:27296 [D loss: 0.056188, acc.: 100.00%] [G loss: 4.087313]\n",
      "epoch:29 step:27297 [D loss: 0.949057, acc.: 39.84%] [G loss: 1.112088]\n",
      "epoch:29 step:27298 [D loss: 0.187817, acc.: 91.41%] [G loss: 2.269127]\n",
      "epoch:29 step:27299 [D loss: 1.416300, acc.: 50.00%] [G loss: 4.923172]\n",
      "epoch:29 step:27300 [D loss: 0.315238, acc.: 88.28%] [G loss: 3.603533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27301 [D loss: 0.884702, acc.: 48.44%] [G loss: 3.353713]\n",
      "epoch:29 step:27302 [D loss: 0.135323, acc.: 98.44%] [G loss: 2.925302]\n",
      "epoch:29 step:27303 [D loss: 0.250142, acc.: 89.84%] [G loss: 1.411436]\n",
      "epoch:29 step:27304 [D loss: 0.321963, acc.: 89.06%] [G loss: 0.790077]\n",
      "epoch:29 step:27305 [D loss: 0.271665, acc.: 93.75%] [G loss: 2.464430]\n",
      "epoch:29 step:27306 [D loss: 0.096160, acc.: 98.44%] [G loss: 3.867604]\n",
      "epoch:29 step:27307 [D loss: 0.452708, acc.: 79.69%] [G loss: 2.974256]\n",
      "epoch:29 step:27308 [D loss: 0.177012, acc.: 97.66%] [G loss: 2.751295]\n",
      "epoch:29 step:27309 [D loss: 0.385406, acc.: 86.72%] [G loss: 2.118211]\n",
      "epoch:29 step:27310 [D loss: 0.159787, acc.: 99.22%] [G loss: 0.686894]\n",
      "epoch:29 step:27311 [D loss: 0.522198, acc.: 71.88%] [G loss: 0.641269]\n",
      "epoch:29 step:27312 [D loss: 0.125300, acc.: 97.66%] [G loss: 1.663624]\n",
      "epoch:29 step:27313 [D loss: 0.834000, acc.: 61.72%] [G loss: 2.883845]\n",
      "epoch:29 step:27314 [D loss: 0.167802, acc.: 95.31%] [G loss: 2.408948]\n",
      "epoch:29 step:27315 [D loss: 0.430770, acc.: 81.25%] [G loss: 0.408142]\n",
      "epoch:29 step:27316 [D loss: 0.386328, acc.: 84.38%] [G loss: 1.141913]\n",
      "epoch:29 step:27317 [D loss: 0.908095, acc.: 53.12%] [G loss: 3.566915]\n",
      "epoch:29 step:27318 [D loss: 0.265195, acc.: 94.53%] [G loss: 2.233616]\n",
      "epoch:29 step:27319 [D loss: 0.087554, acc.: 98.44%] [G loss: 3.503009]\n",
      "epoch:29 step:27320 [D loss: 0.352907, acc.: 89.06%] [G loss: 0.898674]\n",
      "epoch:29 step:27321 [D loss: 0.186408, acc.: 96.09%] [G loss: 1.669358]\n",
      "epoch:29 step:27322 [D loss: 0.698984, acc.: 59.38%] [G loss: 2.844544]\n",
      "epoch:29 step:27323 [D loss: 0.158620, acc.: 96.09%] [G loss: 1.590348]\n",
      "epoch:29 step:27324 [D loss: 0.332561, acc.: 79.69%] [G loss: 0.349490]\n",
      "epoch:29 step:27325 [D loss: 0.838778, acc.: 52.34%] [G loss: 5.393231]\n",
      "epoch:29 step:27326 [D loss: 0.250526, acc.: 90.62%] [G loss: 2.558859]\n",
      "epoch:29 step:27327 [D loss: 0.744601, acc.: 60.94%] [G loss: 4.142346]\n",
      "epoch:29 step:27328 [D loss: 0.049815, acc.: 100.00%] [G loss: 3.222240]\n",
      "epoch:29 step:27329 [D loss: 0.628689, acc.: 64.84%] [G loss: 0.935816]\n",
      "epoch:29 step:27330 [D loss: 0.136092, acc.: 96.09%] [G loss: 1.877462]\n",
      "epoch:29 step:27331 [D loss: 0.086525, acc.: 97.66%] [G loss: 1.385138]\n",
      "epoch:29 step:27332 [D loss: 0.551353, acc.: 70.31%] [G loss: 1.233152]\n",
      "epoch:29 step:27333 [D loss: 0.456610, acc.: 83.59%] [G loss: 2.707842]\n",
      "epoch:29 step:27334 [D loss: 0.634271, acc.: 63.28%] [G loss: 3.415931]\n",
      "epoch:29 step:27335 [D loss: 0.818982, acc.: 60.94%] [G loss: 0.931581]\n",
      "epoch:29 step:27336 [D loss: 0.293490, acc.: 89.06%] [G loss: 3.233640]\n",
      "epoch:29 step:27337 [D loss: 0.254073, acc.: 92.97%] [G loss: 4.637733]\n",
      "epoch:29 step:27338 [D loss: 0.286543, acc.: 87.50%] [G loss: 1.965453]\n",
      "epoch:29 step:27339 [D loss: 0.308468, acc.: 87.50%] [G loss: 2.594289]\n",
      "epoch:29 step:27340 [D loss: 0.519032, acc.: 73.44%] [G loss: 4.705695]\n",
      "epoch:29 step:27341 [D loss: 0.531469, acc.: 69.53%] [G loss: 2.097109]\n",
      "epoch:29 step:27342 [D loss: 0.380510, acc.: 83.59%] [G loss: 2.807097]\n",
      "epoch:29 step:27343 [D loss: 0.259495, acc.: 92.19%] [G loss: 2.900467]\n",
      "epoch:29 step:27344 [D loss: 0.345724, acc.: 88.28%] [G loss: 3.345024]\n",
      "epoch:29 step:27345 [D loss: 0.496119, acc.: 73.44%] [G loss: 2.656045]\n",
      "epoch:29 step:27346 [D loss: 0.912609, acc.: 55.47%] [G loss: 5.003726]\n",
      "epoch:29 step:27347 [D loss: 0.124438, acc.: 96.09%] [G loss: 3.228694]\n",
      "epoch:29 step:27348 [D loss: 0.297345, acc.: 90.62%] [G loss: 2.656821]\n",
      "epoch:29 step:27349 [D loss: 0.551929, acc.: 65.62%] [G loss: 4.706738]\n",
      "epoch:29 step:27350 [D loss: 0.279952, acc.: 91.41%] [G loss: 1.136140]\n",
      "epoch:29 step:27351 [D loss: 0.197749, acc.: 96.88%] [G loss: 2.045420]\n",
      "epoch:29 step:27352 [D loss: 0.531447, acc.: 70.31%] [G loss: 4.057271]\n",
      "epoch:29 step:27353 [D loss: 0.155593, acc.: 96.88%] [G loss: 4.004054]\n",
      "epoch:29 step:27354 [D loss: 0.163161, acc.: 96.88%] [G loss: 5.484918]\n",
      "epoch:29 step:27355 [D loss: 0.871399, acc.: 52.34%] [G loss: 2.412976]\n",
      "epoch:29 step:27356 [D loss: 0.389404, acc.: 85.16%] [G loss: 2.399460]\n",
      "epoch:29 step:27357 [D loss: 1.037045, acc.: 42.97%] [G loss: 3.005581]\n",
      "epoch:29 step:27358 [D loss: 0.471322, acc.: 72.66%] [G loss: 3.029488]\n",
      "epoch:29 step:27359 [D loss: 0.222232, acc.: 92.97%] [G loss: 5.035341]\n",
      "epoch:29 step:27360 [D loss: 0.820885, acc.: 60.94%] [G loss: 3.367749]\n",
      "epoch:29 step:27361 [D loss: 0.221156, acc.: 94.53%] [G loss: 1.891281]\n",
      "epoch:29 step:27362 [D loss: 0.227325, acc.: 91.41%] [G loss: 4.060323]\n",
      "epoch:29 step:27363 [D loss: 0.330705, acc.: 81.25%] [G loss: 2.890894]\n",
      "epoch:29 step:27364 [D loss: 0.177366, acc.: 95.31%] [G loss: 2.985091]\n",
      "epoch:29 step:27365 [D loss: 0.252923, acc.: 92.19%] [G loss: 2.186389]\n",
      "epoch:29 step:27366 [D loss: 0.156913, acc.: 95.31%] [G loss: 3.719385]\n",
      "epoch:29 step:27367 [D loss: 0.554208, acc.: 72.66%] [G loss: 1.911829]\n",
      "epoch:29 step:27368 [D loss: 0.161230, acc.: 98.44%] [G loss: 2.146178]\n",
      "epoch:29 step:27369 [D loss: 0.481602, acc.: 69.53%] [G loss: 1.630792]\n",
      "epoch:29 step:27370 [D loss: 0.475576, acc.: 72.66%] [G loss: 7.929627]\n",
      "epoch:29 step:27371 [D loss: 0.103735, acc.: 99.22%] [G loss: 2.896593]\n",
      "epoch:29 step:27372 [D loss: 0.260828, acc.: 92.19%] [G loss: 3.426888]\n",
      "epoch:29 step:27373 [D loss: 0.217705, acc.: 92.97%] [G loss: 0.565586]\n",
      "epoch:29 step:27374 [D loss: 0.087612, acc.: 100.00%] [G loss: 2.524754]\n",
      "epoch:29 step:27375 [D loss: 0.134549, acc.: 96.88%] [G loss: 1.518010]\n",
      "epoch:29 step:27376 [D loss: 0.048579, acc.: 99.22%] [G loss: 3.824107]\n",
      "epoch:29 step:27377 [D loss: 0.343639, acc.: 86.72%] [G loss: 3.174471]\n",
      "epoch:29 step:27378 [D loss: 0.603698, acc.: 67.97%] [G loss: 1.704983]\n",
      "epoch:29 step:27379 [D loss: 0.542523, acc.: 73.44%] [G loss: 3.070421]\n",
      "epoch:29 step:27380 [D loss: 0.360235, acc.: 85.94%] [G loss: 1.276301]\n",
      "epoch:29 step:27381 [D loss: 0.787993, acc.: 60.16%] [G loss: 2.454411]\n",
      "epoch:29 step:27382 [D loss: 0.917692, acc.: 55.47%] [G loss: 0.853232]\n",
      "epoch:29 step:27383 [D loss: 0.078461, acc.: 99.22%] [G loss: 1.680240]\n",
      "epoch:29 step:27384 [D loss: 0.624812, acc.: 70.31%] [G loss: 2.017493]\n",
      "epoch:29 step:27385 [D loss: 1.104930, acc.: 50.00%] [G loss: 1.419086]\n",
      "epoch:29 step:27386 [D loss: 0.543019, acc.: 73.44%] [G loss: 1.675931]\n",
      "epoch:29 step:27387 [D loss: 0.196872, acc.: 97.66%] [G loss: 2.297813]\n",
      "epoch:29 step:27388 [D loss: 0.158520, acc.: 93.75%] [G loss: 0.755211]\n",
      "epoch:29 step:27389 [D loss: 0.379742, acc.: 86.72%] [G loss: 1.657091]\n",
      "epoch:29 step:27390 [D loss: 0.278425, acc.: 92.19%] [G loss: 1.318689]\n",
      "epoch:29 step:27391 [D loss: 0.306274, acc.: 86.72%] [G loss: 2.490488]\n",
      "epoch:29 step:27392 [D loss: 0.069316, acc.: 99.22%] [G loss: 6.077816]\n",
      "epoch:29 step:27393 [D loss: 1.422959, acc.: 42.97%] [G loss: 0.712446]\n",
      "epoch:29 step:27394 [D loss: 0.348363, acc.: 78.91%] [G loss: 1.501385]\n",
      "epoch:29 step:27395 [D loss: 0.309846, acc.: 91.41%] [G loss: 0.769382]\n",
      "epoch:29 step:27396 [D loss: 0.157906, acc.: 96.88%] [G loss: 3.885648]\n",
      "epoch:29 step:27397 [D loss: 0.182024, acc.: 96.09%] [G loss: 2.299116]\n",
      "epoch:29 step:27398 [D loss: 0.261791, acc.: 92.19%] [G loss: 2.884204]\n",
      "epoch:29 step:27399 [D loss: 0.203039, acc.: 96.09%] [G loss: 2.434882]\n",
      "epoch:29 step:27400 [D loss: 0.084627, acc.: 100.00%] [G loss: 2.205813]\n",
      "epoch:29 step:27401 [D loss: 0.680644, acc.: 59.38%] [G loss: 3.575678]\n",
      "epoch:29 step:27402 [D loss: 0.099286, acc.: 98.44%] [G loss: 2.990365]\n",
      "epoch:29 step:27403 [D loss: 0.647399, acc.: 67.19%] [G loss: 0.605652]\n",
      "epoch:29 step:27404 [D loss: 0.105810, acc.: 98.44%] [G loss: 2.368704]\n",
      "epoch:29 step:27405 [D loss: 0.412200, acc.: 82.03%] [G loss: 1.794695]\n",
      "epoch:29 step:27406 [D loss: 0.327732, acc.: 85.94%] [G loss: 2.545214]\n",
      "epoch:29 step:27407 [D loss: 0.289318, acc.: 85.94%] [G loss: 2.292549]\n",
      "epoch:29 step:27408 [D loss: 0.501843, acc.: 71.09%] [G loss: 4.685743]\n",
      "epoch:29 step:27409 [D loss: 0.064097, acc.: 100.00%] [G loss: 4.203826]\n",
      "epoch:29 step:27410 [D loss: 0.435856, acc.: 81.25%] [G loss: 2.793439]\n",
      "epoch:29 step:27411 [D loss: 0.447009, acc.: 80.47%] [G loss: 1.679080]\n",
      "epoch:29 step:27412 [D loss: 0.232478, acc.: 93.75%] [G loss: 3.540935]\n",
      "epoch:29 step:27413 [D loss: 0.253828, acc.: 90.62%] [G loss: 1.368260]\n",
      "epoch:29 step:27414 [D loss: 0.390333, acc.: 81.25%] [G loss: 1.678612]\n",
      "epoch:29 step:27415 [D loss: 0.415035, acc.: 78.91%] [G loss: 2.222962]\n",
      "epoch:29 step:27416 [D loss: 0.291138, acc.: 89.06%] [G loss: 3.849171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27417 [D loss: 0.154253, acc.: 93.75%] [G loss: 1.772789]\n",
      "epoch:29 step:27418 [D loss: 0.209018, acc.: 96.88%] [G loss: 2.210876]\n",
      "epoch:29 step:27419 [D loss: 0.359158, acc.: 82.03%] [G loss: 1.647342]\n",
      "epoch:29 step:27420 [D loss: 0.337851, acc.: 85.94%] [G loss: 1.822167]\n",
      "epoch:29 step:27421 [D loss: 0.654236, acc.: 61.72%] [G loss: 3.559617]\n",
      "epoch:29 step:27422 [D loss: 0.154297, acc.: 97.66%] [G loss: 2.386924]\n",
      "epoch:29 step:27423 [D loss: 0.109661, acc.: 98.44%] [G loss: 3.265203]\n",
      "epoch:29 step:27424 [D loss: 0.541702, acc.: 76.56%] [G loss: 4.332376]\n",
      "epoch:29 step:27425 [D loss: 0.153156, acc.: 99.22%] [G loss: 7.574528]\n",
      "epoch:29 step:27426 [D loss: 0.646380, acc.: 60.94%] [G loss: 3.157312]\n",
      "epoch:29 step:27427 [D loss: 0.319633, acc.: 88.28%] [G loss: 3.843264]\n",
      "epoch:29 step:27428 [D loss: 0.176162, acc.: 95.31%] [G loss: 2.894904]\n",
      "epoch:29 step:27429 [D loss: 0.280139, acc.: 89.06%] [G loss: 3.702150]\n",
      "epoch:29 step:27430 [D loss: 0.205517, acc.: 91.41%] [G loss: 3.876011]\n",
      "epoch:29 step:27431 [D loss: 0.435608, acc.: 79.69%] [G loss: 3.311361]\n",
      "epoch:29 step:27432 [D loss: 0.270088, acc.: 92.97%] [G loss: 4.921243]\n",
      "epoch:29 step:27433 [D loss: 0.135786, acc.: 96.09%] [G loss: 2.645307]\n",
      "epoch:29 step:27434 [D loss: 0.206919, acc.: 95.31%] [G loss: 3.937062]\n",
      "epoch:29 step:27435 [D loss: 0.070692, acc.: 98.44%] [G loss: 2.368902]\n",
      "epoch:29 step:27436 [D loss: 0.542149, acc.: 69.53%] [G loss: 3.993189]\n",
      "epoch:29 step:27437 [D loss: 0.174418, acc.: 92.19%] [G loss: 4.578586]\n",
      "epoch:29 step:27438 [D loss: 0.322295, acc.: 82.03%] [G loss: 2.604502]\n",
      "epoch:29 step:27439 [D loss: 0.175188, acc.: 97.66%] [G loss: 3.404107]\n",
      "epoch:29 step:27440 [D loss: 0.229856, acc.: 95.31%] [G loss: 2.761976]\n",
      "epoch:29 step:27441 [D loss: 0.149848, acc.: 98.44%] [G loss: 4.036526]\n",
      "epoch:29 step:27442 [D loss: 0.161749, acc.: 96.88%] [G loss: 2.136323]\n",
      "epoch:29 step:27443 [D loss: 0.159056, acc.: 96.09%] [G loss: 3.420084]\n",
      "epoch:29 step:27444 [D loss: 0.027049, acc.: 100.00%] [G loss: 2.400844]\n",
      "epoch:29 step:27445 [D loss: 0.137015, acc.: 99.22%] [G loss: 1.227327]\n",
      "epoch:29 step:27446 [D loss: 0.055943, acc.: 100.00%] [G loss: 2.751907]\n",
      "epoch:29 step:27447 [D loss: 0.182985, acc.: 97.66%] [G loss: 4.960199]\n",
      "epoch:29 step:27448 [D loss: 0.208495, acc.: 92.97%] [G loss: 2.159017]\n",
      "epoch:29 step:27449 [D loss: 0.278713, acc.: 88.28%] [G loss: 2.839336]\n",
      "epoch:29 step:27450 [D loss: 0.108375, acc.: 97.66%] [G loss: 4.000772]\n",
      "epoch:29 step:27451 [D loss: 0.202046, acc.: 94.53%] [G loss: 3.260497]\n",
      "epoch:29 step:27452 [D loss: 0.269002, acc.: 91.41%] [G loss: 1.267121]\n",
      "epoch:29 step:27453 [D loss: 0.289019, acc.: 90.62%] [G loss: 2.194252]\n",
      "epoch:29 step:27454 [D loss: 0.382632, acc.: 89.06%] [G loss: 0.497781]\n",
      "epoch:29 step:27455 [D loss: 0.057086, acc.: 100.00%] [G loss: 6.081918]\n",
      "epoch:29 step:27456 [D loss: 0.188955, acc.: 92.97%] [G loss: 3.492393]\n",
      "epoch:29 step:27457 [D loss: 1.407643, acc.: 44.53%] [G loss: 2.622303]\n",
      "epoch:29 step:27458 [D loss: 0.030993, acc.: 100.00%] [G loss: 5.361612]\n",
      "epoch:29 step:27459 [D loss: 0.079749, acc.: 97.66%] [G loss: 4.477290]\n",
      "epoch:29 step:27460 [D loss: 0.072234, acc.: 98.44%] [G loss: 2.484754]\n",
      "epoch:29 step:27461 [D loss: 0.185024, acc.: 93.75%] [G loss: 4.847079]\n",
      "epoch:29 step:27462 [D loss: 0.358153, acc.: 81.25%] [G loss: 5.037141]\n",
      "epoch:29 step:27463 [D loss: 0.118196, acc.: 97.66%] [G loss: 1.544622]\n",
      "epoch:29 step:27464 [D loss: 0.050179, acc.: 99.22%] [G loss: 0.473336]\n",
      "epoch:29 step:27465 [D loss: 0.182175, acc.: 96.09%] [G loss: 3.672466]\n",
      "epoch:29 step:27466 [D loss: 0.309937, acc.: 92.19%] [G loss: 3.982883]\n",
      "epoch:29 step:27467 [D loss: 0.409943, acc.: 82.81%] [G loss: 0.917971]\n",
      "epoch:29 step:27468 [D loss: 0.307237, acc.: 84.38%] [G loss: 3.529799]\n",
      "epoch:29 step:27469 [D loss: 0.101889, acc.: 97.66%] [G loss: 2.839376]\n",
      "epoch:29 step:27470 [D loss: 0.292353, acc.: 85.16%] [G loss: 0.953449]\n",
      "epoch:29 step:27471 [D loss: 0.078983, acc.: 98.44%] [G loss: 2.462939]\n",
      "epoch:29 step:27472 [D loss: 0.738623, acc.: 57.81%] [G loss: 5.931544]\n",
      "epoch:29 step:27473 [D loss: 0.857921, acc.: 52.34%] [G loss: 2.252916]\n",
      "epoch:29 step:27474 [D loss: 0.313689, acc.: 85.16%] [G loss: 0.553446]\n",
      "epoch:29 step:27475 [D loss: 0.586158, acc.: 67.19%] [G loss: 2.753292]\n",
      "epoch:29 step:27476 [D loss: 0.050121, acc.: 99.22%] [G loss: 2.951284]\n",
      "epoch:29 step:27477 [D loss: 0.114735, acc.: 98.44%] [G loss: 1.698459]\n",
      "epoch:29 step:27478 [D loss: 0.179856, acc.: 96.09%] [G loss: 0.270878]\n",
      "epoch:29 step:27479 [D loss: 0.064175, acc.: 99.22%] [G loss: 3.274867]\n",
      "epoch:29 step:27480 [D loss: 0.357826, acc.: 79.69%] [G loss: 3.237085]\n",
      "epoch:29 step:27481 [D loss: 0.420480, acc.: 78.12%] [G loss: 2.914970]\n",
      "epoch:29 step:27482 [D loss: 0.509097, acc.: 71.09%] [G loss: 5.127512]\n",
      "epoch:29 step:27483 [D loss: 0.052434, acc.: 99.22%] [G loss: 0.412078]\n",
      "epoch:29 step:27484 [D loss: 0.328215, acc.: 89.06%] [G loss: 6.226751]\n",
      "epoch:29 step:27485 [D loss: 0.824739, acc.: 56.25%] [G loss: 2.834288]\n",
      "epoch:29 step:27486 [D loss: 0.160776, acc.: 96.88%] [G loss: 4.690049]\n",
      "epoch:29 step:27487 [D loss: 0.264401, acc.: 89.84%] [G loss: 0.421153]\n",
      "epoch:29 step:27488 [D loss: 1.106875, acc.: 42.19%] [G loss: 1.689691]\n",
      "epoch:29 step:27489 [D loss: 0.335153, acc.: 84.38%] [G loss: 7.630019]\n",
      "epoch:29 step:27490 [D loss: 0.342850, acc.: 87.50%] [G loss: 3.483222]\n",
      "epoch:29 step:27491 [D loss: 0.381643, acc.: 82.81%] [G loss: 0.616376]\n",
      "epoch:29 step:27492 [D loss: 0.231038, acc.: 91.41%] [G loss: 1.932430]\n",
      "epoch:29 step:27493 [D loss: 0.091039, acc.: 99.22%] [G loss: 2.226820]\n",
      "epoch:29 step:27494 [D loss: 0.290326, acc.: 87.50%] [G loss: 3.645111]\n",
      "epoch:29 step:27495 [D loss: 0.184782, acc.: 93.75%] [G loss: 6.600132]\n",
      "epoch:29 step:27496 [D loss: 0.093626, acc.: 98.44%] [G loss: 3.947359]\n",
      "epoch:29 step:27497 [D loss: 0.179943, acc.: 95.31%] [G loss: 2.197340]\n",
      "epoch:29 step:27498 [D loss: 0.445097, acc.: 71.09%] [G loss: 2.947934]\n",
      "epoch:29 step:27499 [D loss: 0.196716, acc.: 97.66%] [G loss: 1.474217]\n",
      "epoch:29 step:27500 [D loss: 0.118507, acc.: 98.44%] [G loss: 2.821282]\n",
      "epoch:29 step:27501 [D loss: 0.115113, acc.: 98.44%] [G loss: 3.075660]\n",
      "epoch:29 step:27502 [D loss: 0.487048, acc.: 74.22%] [G loss: 2.719136]\n",
      "epoch:29 step:27503 [D loss: 0.052306, acc.: 100.00%] [G loss: 4.401794]\n",
      "epoch:29 step:27504 [D loss: 0.963043, acc.: 54.69%] [G loss: 1.867237]\n",
      "epoch:29 step:27505 [D loss: 1.320038, acc.: 48.44%] [G loss: 2.323204]\n",
      "epoch:29 step:27506 [D loss: 0.070095, acc.: 98.44%] [G loss: 2.789948]\n",
      "epoch:29 step:27507 [D loss: 0.039252, acc.: 99.22%] [G loss: 3.706969]\n",
      "epoch:29 step:27508 [D loss: 0.676171, acc.: 65.62%] [G loss: 1.411652]\n",
      "epoch:29 step:27509 [D loss: 0.310073, acc.: 86.72%] [G loss: 1.850657]\n",
      "epoch:29 step:27510 [D loss: 0.083166, acc.: 98.44%] [G loss: 0.712572]\n",
      "epoch:29 step:27511 [D loss: 0.511720, acc.: 70.31%] [G loss: 1.744238]\n",
      "epoch:29 step:27512 [D loss: 0.038414, acc.: 99.22%] [G loss: 2.883947]\n",
      "epoch:29 step:27513 [D loss: 0.437992, acc.: 81.25%] [G loss: 1.722227]\n",
      "epoch:29 step:27514 [D loss: 0.518746, acc.: 67.97%] [G loss: 1.421846]\n",
      "epoch:29 step:27515 [D loss: 0.055876, acc.: 98.44%] [G loss: 3.216767]\n",
      "epoch:29 step:27516 [D loss: 0.147149, acc.: 96.88%] [G loss: 4.580323]\n",
      "epoch:29 step:27517 [D loss: 0.030071, acc.: 100.00%] [G loss: 3.618868]\n",
      "epoch:29 step:27518 [D loss: 0.433855, acc.: 74.22%] [G loss: 0.378958]\n",
      "epoch:29 step:27519 [D loss: 0.280070, acc.: 90.62%] [G loss: 3.382099]\n",
      "epoch:29 step:27520 [D loss: 0.407535, acc.: 78.12%] [G loss: 6.218187]\n",
      "epoch:29 step:27521 [D loss: 0.157061, acc.: 96.88%] [G loss: 4.113840]\n",
      "epoch:29 step:27522 [D loss: 0.732002, acc.: 63.28%] [G loss: 0.559674]\n",
      "epoch:29 step:27523 [D loss: 0.137344, acc.: 95.31%] [G loss: 0.688214]\n",
      "epoch:29 step:27524 [D loss: 1.345961, acc.: 53.12%] [G loss: 5.945460]\n",
      "epoch:29 step:27525 [D loss: 0.276910, acc.: 95.31%] [G loss: 3.784626]\n",
      "epoch:29 step:27526 [D loss: 0.774858, acc.: 58.59%] [G loss: 2.024516]\n",
      "epoch:29 step:27527 [D loss: 0.529002, acc.: 74.22%] [G loss: 4.323253]\n",
      "epoch:29 step:27528 [D loss: 0.129738, acc.: 97.66%] [G loss: 5.484651]\n",
      "epoch:29 step:27529 [D loss: 0.220266, acc.: 93.75%] [G loss: 2.394315]\n",
      "epoch:29 step:27530 [D loss: 0.651039, acc.: 67.19%] [G loss: 2.194912]\n",
      "epoch:29 step:27531 [D loss: 0.199970, acc.: 93.75%] [G loss: 1.471279]\n",
      "epoch:29 step:27532 [D loss: 0.505736, acc.: 75.78%] [G loss: 3.194444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27533 [D loss: 0.254045, acc.: 91.41%] [G loss: 1.980537]\n",
      "epoch:29 step:27534 [D loss: 0.062857, acc.: 98.44%] [G loss: 0.327895]\n",
      "epoch:29 step:27535 [D loss: 0.355479, acc.: 88.28%] [G loss: 2.807620]\n",
      "epoch:29 step:27536 [D loss: 0.211659, acc.: 94.53%] [G loss: 0.196316]\n",
      "epoch:29 step:27537 [D loss: 0.188593, acc.: 96.09%] [G loss: 0.388158]\n",
      "epoch:29 step:27538 [D loss: 0.304895, acc.: 85.16%] [G loss: 1.967858]\n",
      "epoch:29 step:27539 [D loss: 0.191248, acc.: 96.09%] [G loss: 1.429093]\n",
      "epoch:29 step:27540 [D loss: 0.070250, acc.: 98.44%] [G loss: 5.151479]\n",
      "epoch:29 step:27541 [D loss: 0.294667, acc.: 91.41%] [G loss: 1.830682]\n",
      "epoch:29 step:27542 [D loss: 0.167642, acc.: 96.88%] [G loss: 4.758034]\n",
      "epoch:29 step:27543 [D loss: 0.467476, acc.: 73.44%] [G loss: 4.497118]\n",
      "epoch:29 step:27544 [D loss: 0.079451, acc.: 97.66%] [G loss: 6.045187]\n",
      "epoch:29 step:27545 [D loss: 0.151950, acc.: 97.66%] [G loss: 0.998843]\n",
      "epoch:29 step:27546 [D loss: 0.211298, acc.: 93.75%] [G loss: 2.468903]\n",
      "epoch:29 step:27547 [D loss: 0.104817, acc.: 98.44%] [G loss: 0.322174]\n",
      "epoch:29 step:27548 [D loss: 0.589426, acc.: 65.62%] [G loss: 5.276724]\n",
      "epoch:29 step:27549 [D loss: 0.394463, acc.: 87.50%] [G loss: 3.516204]\n",
      "epoch:29 step:27550 [D loss: 1.254701, acc.: 53.12%] [G loss: 6.014119]\n",
      "epoch:29 step:27551 [D loss: 0.814000, acc.: 57.81%] [G loss: 6.283749]\n",
      "epoch:29 step:27552 [D loss: 0.333294, acc.: 82.81%] [G loss: 1.456543]\n",
      "epoch:29 step:27553 [D loss: 0.592843, acc.: 67.19%] [G loss: 2.072020]\n",
      "epoch:29 step:27554 [D loss: 0.941496, acc.: 57.81%] [G loss: 3.006836]\n",
      "epoch:29 step:27555 [D loss: 0.201730, acc.: 92.97%] [G loss: 3.600820]\n",
      "epoch:29 step:27556 [D loss: 0.752127, acc.: 57.03%] [G loss: 2.561142]\n",
      "epoch:29 step:27557 [D loss: 0.849007, acc.: 42.97%] [G loss: 1.547856]\n",
      "epoch:29 step:27558 [D loss: 0.140347, acc.: 98.44%] [G loss: 4.834095]\n",
      "epoch:29 step:27559 [D loss: 0.365798, acc.: 89.06%] [G loss: 3.609804]\n",
      "epoch:29 step:27560 [D loss: 0.172443, acc.: 96.88%] [G loss: 3.728876]\n",
      "epoch:29 step:27561 [D loss: 0.945145, acc.: 51.56%] [G loss: 2.286839]\n",
      "epoch:29 step:27562 [D loss: 0.391007, acc.: 80.47%] [G loss: 0.936586]\n",
      "epoch:29 step:27563 [D loss: 0.286647, acc.: 93.75%] [G loss: 7.395474]\n",
      "epoch:29 step:27564 [D loss: 0.338562, acc.: 87.50%] [G loss: 2.494525]\n",
      "epoch:29 step:27565 [D loss: 0.121460, acc.: 98.44%] [G loss: 4.001381]\n",
      "epoch:29 step:27566 [D loss: 0.557714, acc.: 70.31%] [G loss: 3.968641]\n",
      "epoch:29 step:27567 [D loss: 0.189069, acc.: 92.19%] [G loss: 3.346482]\n",
      "epoch:29 step:27568 [D loss: 0.286998, acc.: 89.06%] [G loss: 1.886515]\n",
      "epoch:29 step:27569 [D loss: 0.433773, acc.: 75.00%] [G loss: 1.272465]\n",
      "epoch:29 step:27570 [D loss: 0.275184, acc.: 94.53%] [G loss: 1.884026]\n",
      "epoch:29 step:27571 [D loss: 0.088217, acc.: 100.00%] [G loss: 0.814137]\n",
      "epoch:29 step:27572 [D loss: 0.383908, acc.: 77.34%] [G loss: 2.481483]\n",
      "epoch:29 step:27573 [D loss: 0.085783, acc.: 99.22%] [G loss: 4.943587]\n",
      "epoch:29 step:27574 [D loss: 0.108703, acc.: 99.22%] [G loss: 4.929424]\n",
      "epoch:29 step:27575 [D loss: 0.165879, acc.: 98.44%] [G loss: 2.497321]\n",
      "epoch:29 step:27576 [D loss: 0.302996, acc.: 89.06%] [G loss: 1.674868]\n",
      "epoch:29 step:27577 [D loss: 0.079973, acc.: 100.00%] [G loss: 2.903339]\n",
      "epoch:29 step:27578 [D loss: 0.103539, acc.: 100.00%] [G loss: 3.160117]\n",
      "epoch:29 step:27579 [D loss: 0.281293, acc.: 89.84%] [G loss: 2.347605]\n",
      "epoch:29 step:27580 [D loss: 0.125118, acc.: 97.66%] [G loss: 3.040214]\n",
      "epoch:29 step:27581 [D loss: 0.254037, acc.: 95.31%] [G loss: 5.292406]\n",
      "epoch:29 step:27582 [D loss: 0.589501, acc.: 64.06%] [G loss: 3.004822]\n",
      "epoch:29 step:27583 [D loss: 0.142806, acc.: 93.75%] [G loss: 3.471533]\n",
      "epoch:29 step:27584 [D loss: 0.222196, acc.: 93.75%] [G loss: 1.150507]\n",
      "epoch:29 step:27585 [D loss: 0.331289, acc.: 88.28%] [G loss: 2.642824]\n",
      "epoch:29 step:27586 [D loss: 0.219759, acc.: 93.75%] [G loss: 2.699836]\n",
      "epoch:29 step:27587 [D loss: 0.172129, acc.: 93.75%] [G loss: 2.404665]\n",
      "epoch:29 step:27588 [D loss: 0.035986, acc.: 100.00%] [G loss: 2.725967]\n",
      "epoch:29 step:27589 [D loss: 0.766568, acc.: 59.38%] [G loss: 3.342787]\n",
      "epoch:29 step:27590 [D loss: 0.553367, acc.: 70.31%] [G loss: 4.052060]\n",
      "epoch:29 step:27591 [D loss: 0.142194, acc.: 96.09%] [G loss: 2.454931]\n",
      "epoch:29 step:27592 [D loss: 0.088431, acc.: 99.22%] [G loss: 2.886019]\n",
      "epoch:29 step:27593 [D loss: 0.831464, acc.: 57.81%] [G loss: 3.111610]\n",
      "epoch:29 step:27594 [D loss: 0.545269, acc.: 64.84%] [G loss: 1.926174]\n",
      "epoch:29 step:27595 [D loss: 0.155084, acc.: 95.31%] [G loss: 4.794522]\n",
      "epoch:29 step:27596 [D loss: 0.386777, acc.: 77.34%] [G loss: 1.793632]\n",
      "epoch:29 step:27597 [D loss: 0.108869, acc.: 97.66%] [G loss: 0.727969]\n",
      "epoch:29 step:27598 [D loss: 0.143374, acc.: 98.44%] [G loss: 1.449596]\n",
      "epoch:29 step:27599 [D loss: 0.238341, acc.: 93.75%] [G loss: 2.040631]\n",
      "epoch:29 step:27600 [D loss: 0.040668, acc.: 100.00%] [G loss: 6.715570]\n",
      "epoch:29 step:27601 [D loss: 0.058811, acc.: 99.22%] [G loss: 4.294759]\n",
      "epoch:29 step:27602 [D loss: 0.211368, acc.: 93.75%] [G loss: 2.857243]\n",
      "epoch:29 step:27603 [D loss: 0.048600, acc.: 99.22%] [G loss: 5.860349]\n",
      "epoch:29 step:27604 [D loss: 0.824630, acc.: 56.25%] [G loss: 4.370240]\n",
      "epoch:29 step:27605 [D loss: 0.083259, acc.: 99.22%] [G loss: 2.814434]\n",
      "epoch:29 step:27606 [D loss: 0.050082, acc.: 100.00%] [G loss: 3.108735]\n",
      "epoch:29 step:27607 [D loss: 0.249017, acc.: 93.75%] [G loss: 3.017841]\n",
      "epoch:29 step:27608 [D loss: 0.252805, acc.: 89.84%] [G loss: 4.482724]\n",
      "epoch:29 step:27609 [D loss: 0.067001, acc.: 98.44%] [G loss: 2.107209]\n",
      "epoch:29 step:27610 [D loss: 0.468266, acc.: 79.69%] [G loss: 5.271723]\n",
      "epoch:29 step:27611 [D loss: 0.154389, acc.: 97.66%] [G loss: 4.616105]\n",
      "epoch:29 step:27612 [D loss: 0.785197, acc.: 55.47%] [G loss: 2.720683]\n",
      "epoch:29 step:27613 [D loss: 0.124504, acc.: 98.44%] [G loss: 2.727364]\n",
      "epoch:29 step:27614 [D loss: 0.120096, acc.: 99.22%] [G loss: 5.016785]\n",
      "epoch:29 step:27615 [D loss: 0.104342, acc.: 99.22%] [G loss: 1.993518]\n",
      "epoch:29 step:27616 [D loss: 0.398655, acc.: 83.59%] [G loss: 0.460223]\n",
      "epoch:29 step:27617 [D loss: 0.104102, acc.: 98.44%] [G loss: 6.137249]\n",
      "epoch:29 step:27618 [D loss: 0.112070, acc.: 98.44%] [G loss: 2.315454]\n",
      "epoch:29 step:27619 [D loss: 0.209606, acc.: 95.31%] [G loss: 2.128540]\n",
      "epoch:29 step:27620 [D loss: 0.676127, acc.: 60.94%] [G loss: 3.985580]\n",
      "epoch:29 step:27621 [D loss: 0.200278, acc.: 93.75%] [G loss: 0.646806]\n",
      "epoch:29 step:27622 [D loss: 0.532771, acc.: 62.50%] [G loss: 4.939171]\n",
      "epoch:29 step:27623 [D loss: 0.060697, acc.: 100.00%] [G loss: 4.001153]\n",
      "epoch:29 step:27624 [D loss: 0.556399, acc.: 73.44%] [G loss: 1.488475]\n",
      "epoch:29 step:27625 [D loss: 0.354213, acc.: 82.81%] [G loss: 3.783301]\n",
      "epoch:29 step:27626 [D loss: 0.107591, acc.: 99.22%] [G loss: 5.011412]\n",
      "epoch:29 step:27627 [D loss: 0.175734, acc.: 96.09%] [G loss: 4.229172]\n",
      "epoch:29 step:27628 [D loss: 0.133109, acc.: 97.66%] [G loss: 0.665228]\n",
      "epoch:29 step:27629 [D loss: 0.074004, acc.: 100.00%] [G loss: 0.597706]\n",
      "epoch:29 step:27630 [D loss: 0.176302, acc.: 95.31%] [G loss: 4.279881]\n",
      "epoch:29 step:27631 [D loss: 0.037435, acc.: 100.00%] [G loss: 4.794543]\n",
      "epoch:29 step:27632 [D loss: 0.167817, acc.: 96.88%] [G loss: 5.604672]\n",
      "epoch:29 step:27633 [D loss: 0.272065, acc.: 95.31%] [G loss: 4.498037]\n",
      "epoch:29 step:27634 [D loss: 0.051349, acc.: 100.00%] [G loss: 4.018358]\n",
      "epoch:29 step:27635 [D loss: 0.051070, acc.: 100.00%] [G loss: 4.629056]\n",
      "epoch:29 step:27636 [D loss: 0.063032, acc.: 98.44%] [G loss: 0.754031]\n",
      "epoch:29 step:27637 [D loss: 0.167507, acc.: 96.88%] [G loss: 4.698365]\n",
      "epoch:29 step:27638 [D loss: 0.642439, acc.: 60.16%] [G loss: 5.617583]\n",
      "epoch:29 step:27639 [D loss: 0.145780, acc.: 96.09%] [G loss: 8.539335]\n",
      "epoch:29 step:27640 [D loss: 0.077620, acc.: 98.44%] [G loss: 9.191460]\n",
      "epoch:29 step:27641 [D loss: 0.778602, acc.: 56.25%] [G loss: 4.336013]\n",
      "epoch:29 step:27642 [D loss: 0.054271, acc.: 99.22%] [G loss: 3.082136]\n",
      "epoch:29 step:27643 [D loss: 0.029937, acc.: 100.00%] [G loss: 2.857058]\n",
      "epoch:29 step:27644 [D loss: 0.209384, acc.: 94.53%] [G loss: 3.847413]\n",
      "epoch:29 step:27645 [D loss: 0.026822, acc.: 100.00%] [G loss: 2.561347]\n",
      "epoch:29 step:27646 [D loss: 0.116772, acc.: 97.66%] [G loss: 2.545479]\n",
      "epoch:29 step:27647 [D loss: 0.037885, acc.: 99.22%] [G loss: 6.278585]\n",
      "epoch:29 step:27648 [D loss: 0.032398, acc.: 100.00%] [G loss: 2.667464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27649 [D loss: 0.034342, acc.: 99.22%] [G loss: 3.699917]\n",
      "epoch:29 step:27650 [D loss: 0.276500, acc.: 88.28%] [G loss: 0.625999]\n",
      "epoch:29 step:27651 [D loss: 0.766006, acc.: 50.78%] [G loss: 4.460218]\n",
      "epoch:29 step:27652 [D loss: 0.090589, acc.: 97.66%] [G loss: 4.043887]\n",
      "epoch:29 step:27653 [D loss: 0.423682, acc.: 77.34%] [G loss: 5.892015]\n",
      "epoch:29 step:27654 [D loss: 0.731405, acc.: 59.38%] [G loss: 1.730843]\n",
      "epoch:29 step:27655 [D loss: 0.461430, acc.: 68.75%] [G loss: 2.493239]\n",
      "epoch:29 step:27656 [D loss: 0.174401, acc.: 92.97%] [G loss: 4.048344]\n",
      "epoch:29 step:27657 [D loss: 0.053935, acc.: 99.22%] [G loss: 3.105659]\n",
      "epoch:29 step:27658 [D loss: 0.113802, acc.: 99.22%] [G loss: 2.079471]\n",
      "epoch:29 step:27659 [D loss: 0.849935, acc.: 57.81%] [G loss: 6.043136]\n",
      "epoch:29 step:27660 [D loss: 0.171242, acc.: 95.31%] [G loss: 5.402974]\n",
      "epoch:29 step:27661 [D loss: 0.425202, acc.: 77.34%] [G loss: 4.107500]\n",
      "epoch:29 step:27662 [D loss: 0.040313, acc.: 99.22%] [G loss: 3.716883]\n",
      "epoch:29 step:27663 [D loss: 0.156073, acc.: 97.66%] [G loss: 1.865435]\n",
      "epoch:29 step:27664 [D loss: 0.252168, acc.: 89.84%] [G loss: 4.195512]\n",
      "epoch:29 step:27665 [D loss: 0.132195, acc.: 97.66%] [G loss: 2.454753]\n",
      "epoch:29 step:27666 [D loss: 0.088330, acc.: 98.44%] [G loss: 0.561099]\n",
      "epoch:29 step:27667 [D loss: 1.101051, acc.: 53.91%] [G loss: 5.279849]\n",
      "epoch:29 step:27668 [D loss: 1.171077, acc.: 53.91%] [G loss: 6.229518]\n",
      "epoch:29 step:27669 [D loss: 0.268209, acc.: 83.59%] [G loss: 6.259807]\n",
      "epoch:29 step:27670 [D loss: 0.105985, acc.: 97.66%] [G loss: 4.447412]\n",
      "epoch:29 step:27671 [D loss: 0.086115, acc.: 98.44%] [G loss: 5.086633]\n",
      "epoch:29 step:27672 [D loss: 0.329824, acc.: 81.25%] [G loss: 2.643909]\n",
      "epoch:29 step:27673 [D loss: 0.355524, acc.: 84.38%] [G loss: 3.468287]\n",
      "epoch:29 step:27674 [D loss: 0.064408, acc.: 100.00%] [G loss: 2.738050]\n",
      "epoch:29 step:27675 [D loss: 0.127141, acc.: 97.66%] [G loss: 3.616073]\n",
      "epoch:29 step:27676 [D loss: 0.750675, acc.: 57.81%] [G loss: 0.868189]\n",
      "epoch:29 step:27677 [D loss: 0.083295, acc.: 98.44%] [G loss: 4.233544]\n",
      "epoch:29 step:27678 [D loss: 0.143424, acc.: 99.22%] [G loss: 2.560057]\n",
      "epoch:29 step:27679 [D loss: 0.325240, acc.: 82.81%] [G loss: 1.902537]\n",
      "epoch:29 step:27680 [D loss: 0.220119, acc.: 95.31%] [G loss: 1.746218]\n",
      "epoch:29 step:27681 [D loss: 0.088292, acc.: 99.22%] [G loss: 1.326782]\n",
      "epoch:29 step:27682 [D loss: 0.015599, acc.: 100.00%] [G loss: 1.902266]\n",
      "epoch:29 step:27683 [D loss: 0.138581, acc.: 98.44%] [G loss: 0.982216]\n",
      "epoch:29 step:27684 [D loss: 0.109073, acc.: 99.22%] [G loss: 4.447364]\n",
      "epoch:29 step:27685 [D loss: 0.045239, acc.: 100.00%] [G loss: 5.898616]\n",
      "epoch:29 step:27686 [D loss: 0.173390, acc.: 96.09%] [G loss: 1.561709]\n",
      "epoch:29 step:27687 [D loss: 0.074029, acc.: 97.66%] [G loss: 5.109995]\n",
      "epoch:29 step:27688 [D loss: 0.116722, acc.: 96.09%] [G loss: 3.794981]\n",
      "epoch:29 step:27689 [D loss: 0.126863, acc.: 99.22%] [G loss: 4.747091]\n",
      "epoch:29 step:27690 [D loss: 0.029807, acc.: 99.22%] [G loss: 4.012159]\n",
      "epoch:29 step:27691 [D loss: 0.074824, acc.: 99.22%] [G loss: 0.746354]\n",
      "epoch:29 step:27692 [D loss: 0.356780, acc.: 87.50%] [G loss: 3.569969]\n",
      "epoch:29 step:27693 [D loss: 0.715926, acc.: 59.38%] [G loss: 7.945384]\n",
      "epoch:29 step:27694 [D loss: 0.052694, acc.: 100.00%] [G loss: 3.813505]\n",
      "epoch:29 step:27695 [D loss: 0.054627, acc.: 99.22%] [G loss: 5.641402]\n",
      "epoch:29 step:27696 [D loss: 0.686422, acc.: 60.16%] [G loss: 2.768846]\n",
      "epoch:29 step:27697 [D loss: 0.046056, acc.: 100.00%] [G loss: 2.938137]\n",
      "epoch:29 step:27698 [D loss: 0.123403, acc.: 96.88%] [G loss: 1.885752]\n",
      "epoch:29 step:27699 [D loss: 0.089993, acc.: 97.66%] [G loss: 0.885498]\n",
      "epoch:29 step:27700 [D loss: 0.211924, acc.: 96.88%] [G loss: 4.859859]\n",
      "epoch:29 step:27701 [D loss: 0.349427, acc.: 78.12%] [G loss: 1.489954]\n",
      "epoch:29 step:27702 [D loss: 0.382126, acc.: 81.25%] [G loss: 2.275154]\n",
      "epoch:29 step:27703 [D loss: 0.462016, acc.: 74.22%] [G loss: 3.844054]\n",
      "epoch:29 step:27704 [D loss: 0.825634, acc.: 59.38%] [G loss: 0.488623]\n",
      "epoch:29 step:27705 [D loss: 0.032034, acc.: 100.00%] [G loss: 0.681918]\n",
      "epoch:29 step:27706 [D loss: 0.076500, acc.: 97.66%] [G loss: 3.780254]\n",
      "epoch:29 step:27707 [D loss: 0.692625, acc.: 58.59%] [G loss: 4.413071]\n",
      "epoch:29 step:27708 [D loss: 0.043797, acc.: 98.44%] [G loss: 4.154479]\n",
      "epoch:29 step:27709 [D loss: 0.890652, acc.: 58.59%] [G loss: 1.854767]\n",
      "epoch:29 step:27710 [D loss: 0.267687, acc.: 91.41%] [G loss: 2.101497]\n",
      "epoch:29 step:27711 [D loss: 0.037353, acc.: 99.22%] [G loss: 2.940696]\n",
      "epoch:29 step:27712 [D loss: 0.345327, acc.: 90.62%] [G loss: 2.145756]\n",
      "epoch:29 step:27713 [D loss: 0.738110, acc.: 57.81%] [G loss: 1.570615]\n",
      "epoch:29 step:27714 [D loss: 0.053950, acc.: 100.00%] [G loss: 1.444432]\n",
      "epoch:29 step:27715 [D loss: 1.836189, acc.: 17.19%] [G loss: 1.926818]\n",
      "epoch:29 step:27716 [D loss: 0.160901, acc.: 95.31%] [G loss: 1.581825]\n",
      "epoch:29 step:27717 [D loss: 0.668991, acc.: 57.81%] [G loss: 3.046697]\n",
      "epoch:29 step:27718 [D loss: 0.174201, acc.: 94.53%] [G loss: 4.610613]\n",
      "epoch:29 step:27719 [D loss: 0.295269, acc.: 88.28%] [G loss: 1.390369]\n",
      "epoch:29 step:27720 [D loss: 0.455276, acc.: 80.47%] [G loss: 3.355955]\n",
      "epoch:29 step:27721 [D loss: 0.081259, acc.: 99.22%] [G loss: 1.231824]\n",
      "epoch:29 step:27722 [D loss: 0.977031, acc.: 53.91%] [G loss: 1.729864]\n",
      "epoch:29 step:27723 [D loss: 0.830096, acc.: 54.69%] [G loss: 0.707296]\n",
      "epoch:29 step:27724 [D loss: 0.974226, acc.: 53.91%] [G loss: 3.661774]\n",
      "epoch:29 step:27725 [D loss: 0.081395, acc.: 99.22%] [G loss: 0.530902]\n",
      "epoch:29 step:27726 [D loss: 0.559887, acc.: 70.31%] [G loss: 0.691754]\n",
      "epoch:29 step:27727 [D loss: 0.075081, acc.: 98.44%] [G loss: 5.294712]\n",
      "epoch:29 step:27728 [D loss: 0.263121, acc.: 91.41%] [G loss: 1.358752]\n",
      "epoch:29 step:27729 [D loss: 0.175351, acc.: 98.44%] [G loss: 2.695171]\n",
      "epoch:29 step:27730 [D loss: 0.413971, acc.: 78.91%] [G loss: 2.990621]\n",
      "epoch:29 step:27731 [D loss: 0.097487, acc.: 99.22%] [G loss: 3.250676]\n",
      "epoch:29 step:27732 [D loss: 0.978922, acc.: 48.44%] [G loss: 3.139950]\n",
      "epoch:29 step:27733 [D loss: 1.201412, acc.: 51.56%] [G loss: 2.337877]\n",
      "epoch:29 step:27734 [D loss: 1.630299, acc.: 16.41%] [G loss: 2.610781]\n",
      "epoch:29 step:27735 [D loss: 0.324888, acc.: 94.53%] [G loss: 3.715496]\n",
      "epoch:29 step:27736 [D loss: 0.514989, acc.: 69.53%] [G loss: 3.528930]\n",
      "epoch:29 step:27737 [D loss: 0.140612, acc.: 97.66%] [G loss: 0.856610]\n",
      "epoch:29 step:27738 [D loss: 0.290030, acc.: 85.94%] [G loss: 2.727541]\n",
      "epoch:29 step:27739 [D loss: 0.287337, acc.: 92.19%] [G loss: 3.789233]\n",
      "epoch:29 step:27740 [D loss: 0.527705, acc.: 67.19%] [G loss: 3.989140]\n",
      "epoch:29 step:27741 [D loss: 0.062778, acc.: 99.22%] [G loss: 3.894207]\n",
      "epoch:29 step:27742 [D loss: 0.074489, acc.: 100.00%] [G loss: 2.498488]\n",
      "epoch:29 step:27743 [D loss: 0.302143, acc.: 88.28%] [G loss: 1.471823]\n",
      "epoch:29 step:27744 [D loss: 0.470460, acc.: 71.88%] [G loss: 3.220169]\n",
      "epoch:29 step:27745 [D loss: 0.219219, acc.: 94.53%] [G loss: 2.254805]\n",
      "epoch:29 step:27746 [D loss: 0.260906, acc.: 90.62%] [G loss: 2.459144]\n",
      "epoch:29 step:27747 [D loss: 0.256091, acc.: 91.41%] [G loss: 2.040467]\n",
      "epoch:29 step:27748 [D loss: 0.103176, acc.: 98.44%] [G loss: 0.235511]\n",
      "epoch:29 step:27749 [D loss: 0.287132, acc.: 89.84%] [G loss: 2.621123]\n",
      "epoch:29 step:27750 [D loss: 0.076488, acc.: 98.44%] [G loss: 1.028846]\n",
      "epoch:29 step:27751 [D loss: 0.112110, acc.: 98.44%] [G loss: 2.225140]\n",
      "epoch:29 step:27752 [D loss: 0.588970, acc.: 60.16%] [G loss: 0.931527]\n",
      "epoch:29 step:27753 [D loss: 0.115055, acc.: 96.09%] [G loss: 2.763605]\n",
      "epoch:29 step:27754 [D loss: 0.277820, acc.: 89.06%] [G loss: 2.805386]\n",
      "epoch:29 step:27755 [D loss: 0.568625, acc.: 65.62%] [G loss: 2.683518]\n",
      "epoch:29 step:27756 [D loss: 0.377904, acc.: 81.25%] [G loss: 1.107802]\n",
      "epoch:29 step:27757 [D loss: 0.415628, acc.: 75.78%] [G loss: 3.640300]\n",
      "epoch:29 step:27758 [D loss: 0.277839, acc.: 91.41%] [G loss: 1.280296]\n",
      "epoch:29 step:27759 [D loss: 0.297160, acc.: 89.84%] [G loss: 4.887738]\n",
      "epoch:29 step:27760 [D loss: 0.118417, acc.: 96.88%] [G loss: 6.202936]\n",
      "epoch:29 step:27761 [D loss: 0.280258, acc.: 95.31%] [G loss: 1.119702]\n",
      "epoch:29 step:27762 [D loss: 0.312238, acc.: 92.19%] [G loss: 3.684897]\n",
      "epoch:29 step:27763 [D loss: 0.158849, acc.: 94.53%] [G loss: 1.755310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27764 [D loss: 0.297955, acc.: 87.50%] [G loss: 2.841442]\n",
      "epoch:29 step:27765 [D loss: 0.204434, acc.: 92.97%] [G loss: 2.735679]\n",
      "epoch:29 step:27766 [D loss: 0.112432, acc.: 97.66%] [G loss: 5.741598]\n",
      "epoch:29 step:27767 [D loss: 0.062792, acc.: 98.44%] [G loss: 4.257365]\n",
      "epoch:29 step:27768 [D loss: 0.369363, acc.: 87.50%] [G loss: 1.605408]\n",
      "epoch:29 step:27769 [D loss: 0.465288, acc.: 72.66%] [G loss: 1.016722]\n",
      "epoch:29 step:27770 [D loss: 1.180096, acc.: 52.34%] [G loss: 2.195630]\n",
      "epoch:29 step:27771 [D loss: 0.242520, acc.: 92.19%] [G loss: 1.966000]\n",
      "epoch:29 step:27772 [D loss: 0.398760, acc.: 83.59%] [G loss: 2.191066]\n",
      "epoch:29 step:27773 [D loss: 0.331819, acc.: 88.28%] [G loss: 2.721129]\n",
      "epoch:29 step:27774 [D loss: 0.147963, acc.: 96.09%] [G loss: 5.009689]\n",
      "epoch:29 step:27775 [D loss: 0.227587, acc.: 93.75%] [G loss: 1.171760]\n",
      "epoch:29 step:27776 [D loss: 0.173809, acc.: 97.66%] [G loss: 4.778565]\n",
      "epoch:29 step:27777 [D loss: 0.437426, acc.: 88.28%] [G loss: 3.455614]\n",
      "epoch:29 step:27778 [D loss: 0.059133, acc.: 100.00%] [G loss: 3.823775]\n",
      "epoch:29 step:27779 [D loss: 0.232848, acc.: 95.31%] [G loss: 5.641108]\n",
      "epoch:29 step:27780 [D loss: 0.140335, acc.: 96.88%] [G loss: 2.475513]\n",
      "epoch:29 step:27781 [D loss: 0.151434, acc.: 97.66%] [G loss: 1.626236]\n",
      "epoch:29 step:27782 [D loss: 0.274982, acc.: 90.62%] [G loss: 2.346546]\n",
      "epoch:29 step:27783 [D loss: 0.193675, acc.: 95.31%] [G loss: 0.449998]\n",
      "epoch:29 step:27784 [D loss: 1.805379, acc.: 23.44%] [G loss: 2.246149]\n",
      "epoch:29 step:27785 [D loss: 0.219501, acc.: 95.31%] [G loss: 5.230833]\n",
      "epoch:29 step:27786 [D loss: 0.384324, acc.: 87.50%] [G loss: 4.267654]\n",
      "epoch:29 step:27787 [D loss: 0.138092, acc.: 96.88%] [G loss: 0.583251]\n",
      "epoch:29 step:27788 [D loss: 0.204444, acc.: 96.88%] [G loss: 3.736320]\n",
      "epoch:29 step:27789 [D loss: 0.322581, acc.: 87.50%] [G loss: 2.217472]\n",
      "epoch:29 step:27790 [D loss: 0.340780, acc.: 85.94%] [G loss: 3.993670]\n",
      "epoch:29 step:27791 [D loss: 0.116503, acc.: 97.66%] [G loss: 3.472084]\n",
      "epoch:29 step:27792 [D loss: 0.332924, acc.: 89.84%] [G loss: 4.164084]\n",
      "epoch:29 step:27793 [D loss: 0.236211, acc.: 94.53%] [G loss: 0.430882]\n",
      "epoch:29 step:27794 [D loss: 0.137767, acc.: 96.88%] [G loss: 3.312458]\n",
      "epoch:29 step:27795 [D loss: 0.261074, acc.: 92.97%] [G loss: 1.959338]\n",
      "epoch:29 step:27796 [D loss: 0.423342, acc.: 82.81%] [G loss: 1.928702]\n",
      "epoch:29 step:27797 [D loss: 0.892374, acc.: 57.81%] [G loss: 2.955200]\n",
      "epoch:29 step:27798 [D loss: 0.476840, acc.: 78.91%] [G loss: 2.946395]\n",
      "epoch:29 step:27799 [D loss: 0.095629, acc.: 98.44%] [G loss: 1.237497]\n",
      "epoch:29 step:27800 [D loss: 0.154200, acc.: 96.09%] [G loss: 2.949209]\n",
      "epoch:29 step:27801 [D loss: 0.293558, acc.: 91.41%] [G loss: 2.402229]\n",
      "epoch:29 step:27802 [D loss: 0.184942, acc.: 93.75%] [G loss: 2.950359]\n",
      "epoch:29 step:27803 [D loss: 0.098273, acc.: 98.44%] [G loss: 4.153168]\n",
      "epoch:29 step:27804 [D loss: 0.344256, acc.: 81.25%] [G loss: 2.355983]\n",
      "epoch:29 step:27805 [D loss: 0.888353, acc.: 55.47%] [G loss: 2.915773]\n",
      "epoch:29 step:27806 [D loss: 0.112521, acc.: 99.22%] [G loss: 3.725334]\n",
      "epoch:29 step:27807 [D loss: 0.389039, acc.: 78.91%] [G loss: 4.608335]\n",
      "epoch:29 step:27808 [D loss: 0.308383, acc.: 87.50%] [G loss: 2.395036]\n",
      "epoch:29 step:27809 [D loss: 0.167070, acc.: 96.09%] [G loss: 5.682010]\n",
      "epoch:29 step:27810 [D loss: 0.139832, acc.: 96.88%] [G loss: 2.651683]\n",
      "epoch:29 step:27811 [D loss: 1.064405, acc.: 37.50%] [G loss: 3.281164]\n",
      "epoch:29 step:27812 [D loss: 0.482434, acc.: 74.22%] [G loss: 1.759831]\n",
      "epoch:29 step:27813 [D loss: 0.320376, acc.: 91.41%] [G loss: 4.249496]\n",
      "epoch:29 step:27814 [D loss: 0.363493, acc.: 85.94%] [G loss: 1.265949]\n",
      "epoch:29 step:27815 [D loss: 0.160433, acc.: 96.09%] [G loss: 5.158428]\n",
      "epoch:29 step:27816 [D loss: 0.221057, acc.: 93.75%] [G loss: 4.543716]\n",
      "epoch:29 step:27817 [D loss: 0.755203, acc.: 60.94%] [G loss: 4.468078]\n",
      "epoch:29 step:27818 [D loss: 0.363644, acc.: 86.72%] [G loss: 0.827538]\n",
      "epoch:29 step:27819 [D loss: 0.045060, acc.: 97.66%] [G loss: 5.767089]\n",
      "epoch:29 step:27820 [D loss: 0.973711, acc.: 54.69%] [G loss: 4.005159]\n",
      "epoch:29 step:27821 [D loss: 0.472704, acc.: 70.31%] [G loss: 1.426328]\n",
      "epoch:29 step:27822 [D loss: 0.157638, acc.: 96.88%] [G loss: 2.068048]\n",
      "epoch:29 step:27823 [D loss: 0.533296, acc.: 80.47%] [G loss: 2.338685]\n",
      "epoch:29 step:27824 [D loss: 0.065406, acc.: 100.00%] [G loss: 4.447244]\n",
      "epoch:29 step:27825 [D loss: 0.106804, acc.: 97.66%] [G loss: 0.459552]\n",
      "epoch:29 step:27826 [D loss: 0.040140, acc.: 99.22%] [G loss: 3.177562]\n",
      "epoch:29 step:27827 [D loss: 0.291415, acc.: 92.19%] [G loss: 1.731667]\n",
      "epoch:29 step:27828 [D loss: 0.061913, acc.: 99.22%] [G loss: 3.290134]\n",
      "epoch:29 step:27829 [D loss: 0.960613, acc.: 57.03%] [G loss: 2.143280]\n",
      "epoch:29 step:27830 [D loss: 0.221598, acc.: 89.84%] [G loss: 0.982019]\n",
      "epoch:29 step:27831 [D loss: 1.505538, acc.: 17.97%] [G loss: 3.265018]\n",
      "epoch:29 step:27832 [D loss: 0.625124, acc.: 61.72%] [G loss: 1.222093]\n",
      "epoch:29 step:27833 [D loss: 0.191421, acc.: 97.66%] [G loss: 1.013401]\n",
      "epoch:29 step:27834 [D loss: 0.236261, acc.: 94.53%] [G loss: 1.570896]\n",
      "epoch:29 step:27835 [D loss: 0.187463, acc.: 98.44%] [G loss: 5.455988]\n",
      "epoch:29 step:27836 [D loss: 0.134210, acc.: 98.44%] [G loss: 1.405841]\n",
      "epoch:29 step:27837 [D loss: 0.116406, acc.: 98.44%] [G loss: 4.112060]\n",
      "epoch:29 step:27838 [D loss: 0.236802, acc.: 93.75%] [G loss: 5.606903]\n",
      "epoch:29 step:27839 [D loss: 0.204588, acc.: 93.75%] [G loss: 1.297807]\n",
      "epoch:29 step:27840 [D loss: 0.741232, acc.: 52.34%] [G loss: 1.877538]\n",
      "epoch:29 step:27841 [D loss: 0.391710, acc.: 86.72%] [G loss: 2.860941]\n",
      "epoch:29 step:27842 [D loss: 0.056003, acc.: 98.44%] [G loss: 2.460299]\n",
      "epoch:29 step:27843 [D loss: 0.108736, acc.: 99.22%] [G loss: 2.711833]\n",
      "epoch:29 step:27844 [D loss: 0.150862, acc.: 98.44%] [G loss: 2.472075]\n",
      "epoch:29 step:27845 [D loss: 0.827992, acc.: 59.38%] [G loss: 1.746102]\n",
      "epoch:29 step:27846 [D loss: 0.487678, acc.: 70.31%] [G loss: 2.873060]\n",
      "epoch:29 step:27847 [D loss: 0.126403, acc.: 99.22%] [G loss: 3.309496]\n",
      "epoch:29 step:27848 [D loss: 0.582548, acc.: 74.22%] [G loss: 3.385954]\n",
      "epoch:29 step:27849 [D loss: 0.575057, acc.: 67.19%] [G loss: 2.486907]\n",
      "epoch:29 step:27850 [D loss: 0.387751, acc.: 82.03%] [G loss: 3.389983]\n",
      "epoch:29 step:27851 [D loss: 0.243314, acc.: 94.53%] [G loss: 2.024772]\n",
      "epoch:29 step:27852 [D loss: 0.075341, acc.: 99.22%] [G loss: 7.165049]\n",
      "epoch:29 step:27853 [D loss: 0.190163, acc.: 96.09%] [G loss: 3.052510]\n",
      "epoch:29 step:27854 [D loss: 0.213723, acc.: 95.31%] [G loss: 2.307103]\n",
      "epoch:29 step:27855 [D loss: 0.549115, acc.: 63.28%] [G loss: 4.603023]\n",
      "epoch:29 step:27856 [D loss: 0.113040, acc.: 97.66%] [G loss: 4.224242]\n",
      "epoch:29 step:27857 [D loss: 0.392726, acc.: 86.72%] [G loss: 0.926433]\n",
      "epoch:29 step:27858 [D loss: 0.069725, acc.: 98.44%] [G loss: 2.295515]\n",
      "epoch:29 step:27859 [D loss: 1.001106, acc.: 53.91%] [G loss: 2.548318]\n",
      "epoch:29 step:27860 [D loss: 0.177678, acc.: 97.66%] [G loss: 3.439852]\n",
      "epoch:29 step:27861 [D loss: 0.417586, acc.: 75.00%] [G loss: 0.232861]\n",
      "epoch:29 step:27862 [D loss: 0.454591, acc.: 78.91%] [G loss: 4.556613]\n",
      "epoch:29 step:27863 [D loss: 0.256436, acc.: 91.41%] [G loss: 3.164801]\n",
      "epoch:29 step:27864 [D loss: 0.142534, acc.: 97.66%] [G loss: 1.801383]\n",
      "epoch:29 step:27865 [D loss: 0.332707, acc.: 88.28%] [G loss: 0.210204]\n",
      "epoch:29 step:27866 [D loss: 0.404227, acc.: 81.25%] [G loss: 1.400911]\n",
      "epoch:29 step:27867 [D loss: 0.631739, acc.: 67.19%] [G loss: 1.674124]\n",
      "epoch:29 step:27868 [D loss: 1.513297, acc.: 51.56%] [G loss: 3.724242]\n",
      "epoch:29 step:27869 [D loss: 0.172925, acc.: 92.97%] [G loss: 4.669405]\n",
      "epoch:29 step:27870 [D loss: 0.431087, acc.: 78.12%] [G loss: 1.552798]\n",
      "epoch:29 step:27871 [D loss: 0.247730, acc.: 91.41%] [G loss: 3.358631]\n",
      "epoch:29 step:27872 [D loss: 0.108659, acc.: 99.22%] [G loss: 1.886927]\n",
      "epoch:29 step:27873 [D loss: 0.543095, acc.: 69.53%] [G loss: 1.191306]\n",
      "epoch:29 step:27874 [D loss: 0.741466, acc.: 59.38%] [G loss: 2.587153]\n",
      "epoch:29 step:27875 [D loss: 0.227798, acc.: 92.19%] [G loss: 3.324496]\n",
      "epoch:29 step:27876 [D loss: 0.349384, acc.: 83.59%] [G loss: 5.330451]\n",
      "epoch:29 step:27877 [D loss: 0.524440, acc.: 76.56%] [G loss: 4.349535]\n",
      "epoch:29 step:27878 [D loss: 0.758470, acc.: 50.00%] [G loss: 2.190816]\n",
      "epoch:29 step:27879 [D loss: 0.453558, acc.: 78.91%] [G loss: 3.480645]\n",
      "epoch:29 step:27880 [D loss: 0.122966, acc.: 97.66%] [G loss: 4.270765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27881 [D loss: 1.215052, acc.: 52.34%] [G loss: 2.491472]\n",
      "epoch:29 step:27882 [D loss: 0.478553, acc.: 80.47%] [G loss: 1.359410]\n",
      "epoch:29 step:27883 [D loss: 0.151123, acc.: 99.22%] [G loss: 4.502777]\n",
      "epoch:29 step:27884 [D loss: 0.223453, acc.: 94.53%] [G loss: 4.102260]\n",
      "epoch:29 step:27885 [D loss: 0.283359, acc.: 90.62%] [G loss: 2.307506]\n",
      "epoch:29 step:27886 [D loss: 0.143278, acc.: 99.22%] [G loss: 1.821452]\n",
      "epoch:29 step:27887 [D loss: 0.099896, acc.: 99.22%] [G loss: 3.501472]\n",
      "epoch:29 step:27888 [D loss: 0.165943, acc.: 98.44%] [G loss: 1.665550]\n",
      "epoch:29 step:27889 [D loss: 0.973079, acc.: 48.44%] [G loss: 3.330930]\n",
      "epoch:29 step:27890 [D loss: 0.566639, acc.: 73.44%] [G loss: 2.736735]\n",
      "epoch:29 step:27891 [D loss: 0.091413, acc.: 99.22%] [G loss: 2.531950]\n",
      "epoch:29 step:27892 [D loss: 0.060616, acc.: 100.00%] [G loss: 1.320741]\n",
      "epoch:29 step:27893 [D loss: 0.230504, acc.: 96.09%] [G loss: 0.869193]\n",
      "epoch:29 step:27894 [D loss: 0.051556, acc.: 100.00%] [G loss: 3.607176]\n",
      "epoch:29 step:27895 [D loss: 0.503651, acc.: 65.62%] [G loss: 2.558157]\n",
      "epoch:29 step:27896 [D loss: 0.521912, acc.: 71.88%] [G loss: 2.371075]\n",
      "epoch:29 step:27897 [D loss: 0.475348, acc.: 75.78%] [G loss: 3.266106]\n",
      "epoch:29 step:27898 [D loss: 0.283161, acc.: 93.75%] [G loss: 2.295994]\n",
      "epoch:29 step:27899 [D loss: 0.324690, acc.: 85.94%] [G loss: 2.106661]\n",
      "epoch:29 step:27900 [D loss: 0.024356, acc.: 100.00%] [G loss: 1.419595]\n",
      "epoch:29 step:27901 [D loss: 0.546192, acc.: 71.09%] [G loss: 1.892507]\n",
      "epoch:29 step:27902 [D loss: 0.089687, acc.: 99.22%] [G loss: 3.213645]\n",
      "epoch:29 step:27903 [D loss: 0.592079, acc.: 63.28%] [G loss: 1.536357]\n",
      "epoch:29 step:27904 [D loss: 0.268310, acc.: 90.62%] [G loss: 1.787262]\n",
      "epoch:29 step:27905 [D loss: 0.086749, acc.: 99.22%] [G loss: 2.156900]\n",
      "epoch:29 step:27906 [D loss: 0.472419, acc.: 73.44%] [G loss: 3.517278]\n",
      "epoch:29 step:27907 [D loss: 0.185031, acc.: 94.53%] [G loss: 5.552408]\n",
      "epoch:29 step:27908 [D loss: 0.706056, acc.: 61.72%] [G loss: 1.313856]\n",
      "epoch:29 step:27909 [D loss: 0.316919, acc.: 88.28%] [G loss: 4.753631]\n",
      "epoch:29 step:27910 [D loss: 0.032145, acc.: 100.00%] [G loss: 4.916779]\n",
      "epoch:29 step:27911 [D loss: 0.229670, acc.: 93.75%] [G loss: 4.414361]\n",
      "epoch:29 step:27912 [D loss: 0.081034, acc.: 99.22%] [G loss: 1.187811]\n",
      "epoch:29 step:27913 [D loss: 0.254691, acc.: 93.75%] [G loss: 3.247924]\n",
      "epoch:29 step:27914 [D loss: 0.140984, acc.: 95.31%] [G loss: 3.187429]\n",
      "epoch:29 step:27915 [D loss: 0.420109, acc.: 78.91%] [G loss: 1.941507]\n",
      "epoch:29 step:27916 [D loss: 1.002574, acc.: 58.59%] [G loss: 5.210902]\n",
      "epoch:29 step:27917 [D loss: 0.043303, acc.: 100.00%] [G loss: 5.011708]\n",
      "epoch:29 step:27918 [D loss: 0.081692, acc.: 99.22%] [G loss: 5.523830]\n",
      "epoch:29 step:27919 [D loss: 0.285977, acc.: 90.62%] [G loss: 1.231215]\n",
      "epoch:29 step:27920 [D loss: 0.743581, acc.: 59.38%] [G loss: 0.193131]\n",
      "epoch:29 step:27921 [D loss: 0.201850, acc.: 92.19%] [G loss: 0.149883]\n",
      "epoch:29 step:27922 [D loss: 0.487103, acc.: 75.78%] [G loss: 0.703393]\n",
      "epoch:29 step:27923 [D loss: 0.087065, acc.: 97.66%] [G loss: 6.543422]\n",
      "epoch:29 step:27924 [D loss: 0.499107, acc.: 74.22%] [G loss: 1.556180]\n",
      "epoch:29 step:27925 [D loss: 1.228183, acc.: 53.12%] [G loss: 3.231320]\n",
      "epoch:29 step:27926 [D loss: 1.026928, acc.: 55.47%] [G loss: 0.607647]\n",
      "epoch:29 step:27927 [D loss: 0.203450, acc.: 92.97%] [G loss: 6.412804]\n",
      "epoch:29 step:27928 [D loss: 0.611438, acc.: 67.97%] [G loss: 1.223691]\n",
      "epoch:29 step:27929 [D loss: 0.166017, acc.: 96.88%] [G loss: 6.886151]\n",
      "epoch:29 step:27930 [D loss: 1.338498, acc.: 53.12%] [G loss: 5.315478]\n",
      "epoch:29 step:27931 [D loss: 0.199223, acc.: 96.88%] [G loss: 3.453703]\n",
      "epoch:29 step:27932 [D loss: 0.331238, acc.: 88.28%] [G loss: 4.721420]\n",
      "epoch:29 step:27933 [D loss: 0.446475, acc.: 75.00%] [G loss: 2.665674]\n",
      "epoch:29 step:27934 [D loss: 0.215995, acc.: 92.19%] [G loss: 2.196008]\n",
      "epoch:29 step:27935 [D loss: 0.432453, acc.: 73.44%] [G loss: 1.923632]\n",
      "epoch:29 step:27936 [D loss: 0.275545, acc.: 92.19%] [G loss: 1.808991]\n",
      "epoch:29 step:27937 [D loss: 0.121283, acc.: 97.66%] [G loss: 2.638071]\n",
      "epoch:29 step:27938 [D loss: 0.111991, acc.: 99.22%] [G loss: 3.971132]\n",
      "epoch:29 step:27939 [D loss: 0.204789, acc.: 95.31%] [G loss: 3.851986]\n",
      "epoch:29 step:27940 [D loss: 0.426782, acc.: 81.25%] [G loss: 3.275263]\n",
      "epoch:29 step:27941 [D loss: 0.345275, acc.: 84.38%] [G loss: 2.274772]\n",
      "epoch:29 step:27942 [D loss: 0.110446, acc.: 97.66%] [G loss: 3.200254]\n",
      "epoch:29 step:27943 [D loss: 0.141778, acc.: 96.88%] [G loss: 4.717834]\n",
      "epoch:29 step:27944 [D loss: 0.702824, acc.: 57.03%] [G loss: 0.633294]\n",
      "epoch:29 step:27945 [D loss: 0.219273, acc.: 96.88%] [G loss: 1.546234]\n",
      "epoch:29 step:27946 [D loss: 0.489532, acc.: 75.78%] [G loss: 6.197293]\n",
      "epoch:29 step:27947 [D loss: 0.083120, acc.: 97.66%] [G loss: 1.954463]\n",
      "epoch:29 step:27948 [D loss: 0.196966, acc.: 96.88%] [G loss: 1.789174]\n",
      "epoch:29 step:27949 [D loss: 0.333668, acc.: 85.94%] [G loss: 1.370100]\n",
      "epoch:29 step:27950 [D loss: 0.166532, acc.: 95.31%] [G loss: 0.696868]\n",
      "epoch:29 step:27951 [D loss: 0.609205, acc.: 67.97%] [G loss: 2.365993]\n",
      "epoch:29 step:27952 [D loss: 0.527164, acc.: 67.19%] [G loss: 7.465186]\n",
      "epoch:29 step:27953 [D loss: 0.186233, acc.: 98.44%] [G loss: 6.463202]\n",
      "epoch:29 step:27954 [D loss: 0.250754, acc.: 91.41%] [G loss: 2.980695]\n",
      "epoch:29 step:27955 [D loss: 0.257090, acc.: 92.19%] [G loss: 2.310705]\n",
      "epoch:29 step:27956 [D loss: 0.276913, acc.: 92.97%] [G loss: 2.334001]\n",
      "epoch:29 step:27957 [D loss: 0.101652, acc.: 96.88%] [G loss: 2.440391]\n",
      "epoch:29 step:27958 [D loss: 0.048878, acc.: 100.00%] [G loss: 2.770565]\n",
      "epoch:29 step:27959 [D loss: 0.383367, acc.: 80.47%] [G loss: 2.854117]\n",
      "epoch:29 step:27960 [D loss: 0.323957, acc.: 93.75%] [G loss: 1.143398]\n",
      "epoch:29 step:27961 [D loss: 1.825796, acc.: 51.56%] [G loss: 1.888579]\n",
      "epoch:29 step:27962 [D loss: 0.904827, acc.: 41.41%] [G loss: 3.385362]\n",
      "epoch:29 step:27963 [D loss: 0.572642, acc.: 67.97%] [G loss: 1.395885]\n",
      "epoch:29 step:27964 [D loss: 0.071427, acc.: 99.22%] [G loss: 2.518469]\n",
      "epoch:29 step:27965 [D loss: 0.400248, acc.: 83.59%] [G loss: 5.131222]\n",
      "epoch:29 step:27966 [D loss: 0.195332, acc.: 96.09%] [G loss: 4.795469]\n",
      "epoch:29 step:27967 [D loss: 0.840599, acc.: 53.12%] [G loss: 3.423784]\n",
      "epoch:29 step:27968 [D loss: 1.658110, acc.: 13.28%] [G loss: 2.722749]\n",
      "epoch:29 step:27969 [D loss: 0.419273, acc.: 83.59%] [G loss: 1.499815]\n",
      "epoch:29 step:27970 [D loss: 0.319165, acc.: 84.38%] [G loss: 3.192210]\n",
      "epoch:29 step:27971 [D loss: 0.077008, acc.: 99.22%] [G loss: 2.395819]\n",
      "epoch:29 step:27972 [D loss: 0.215808, acc.: 94.53%] [G loss: 3.467286]\n",
      "epoch:29 step:27973 [D loss: 0.184892, acc.: 94.53%] [G loss: 10.533175]\n",
      "epoch:29 step:27974 [D loss: 0.396645, acc.: 81.25%] [G loss: 1.893430]\n",
      "epoch:29 step:27975 [D loss: 0.891165, acc.: 52.34%] [G loss: 1.404205]\n",
      "epoch:29 step:27976 [D loss: 0.311063, acc.: 91.41%] [G loss: 1.693148]\n",
      "epoch:29 step:27977 [D loss: 0.960047, acc.: 40.62%] [G loss: 2.538264]\n",
      "epoch:29 step:27978 [D loss: 1.022262, acc.: 46.09%] [G loss: 3.112226]\n",
      "epoch:29 step:27979 [D loss: 0.753715, acc.: 60.94%] [G loss: 1.370133]\n",
      "epoch:29 step:27980 [D loss: 0.145230, acc.: 96.88%] [G loss: 2.203251]\n",
      "epoch:29 step:27981 [D loss: 0.219198, acc.: 98.44%] [G loss: 1.146240]\n",
      "epoch:29 step:27982 [D loss: 0.414814, acc.: 77.34%] [G loss: 1.124071]\n",
      "epoch:29 step:27983 [D loss: 0.494356, acc.: 77.34%] [G loss: 3.054461]\n",
      "epoch:29 step:27984 [D loss: 0.238275, acc.: 95.31%] [G loss: 3.332619]\n",
      "epoch:29 step:27985 [D loss: 0.306950, acc.: 91.41%] [G loss: 1.214028]\n",
      "epoch:29 step:27986 [D loss: 0.139878, acc.: 97.66%] [G loss: 4.757819]\n",
      "epoch:29 step:27987 [D loss: 0.272473, acc.: 89.06%] [G loss: 3.161104]\n",
      "epoch:29 step:27988 [D loss: 0.210552, acc.: 95.31%] [G loss: 1.815001]\n",
      "epoch:29 step:27989 [D loss: 0.345338, acc.: 82.81%] [G loss: 2.136229]\n",
      "epoch:29 step:27990 [D loss: 0.198083, acc.: 95.31%] [G loss: 3.168687]\n",
      "epoch:29 step:27991 [D loss: 0.060156, acc.: 100.00%] [G loss: 4.979350]\n",
      "epoch:29 step:27992 [D loss: 0.332649, acc.: 82.81%] [G loss: 3.428443]\n",
      "epoch:29 step:27993 [D loss: 0.089525, acc.: 100.00%] [G loss: 2.956480]\n",
      "epoch:29 step:27994 [D loss: 0.091825, acc.: 98.44%] [G loss: 3.489487]\n",
      "epoch:29 step:27995 [D loss: 0.566096, acc.: 67.19%] [G loss: 2.981673]\n",
      "epoch:29 step:27996 [D loss: 0.296874, acc.: 86.72%] [G loss: 2.959279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27997 [D loss: 0.246986, acc.: 96.09%] [G loss: 1.495594]\n",
      "epoch:29 step:27998 [D loss: 0.494129, acc.: 77.34%] [G loss: 2.615564]\n",
      "epoch:29 step:27999 [D loss: 0.190511, acc.: 96.88%] [G loss: 2.206542]\n",
      "epoch:29 step:28000 [D loss: 0.357845, acc.: 87.50%] [G loss: 1.848732]\n",
      "epoch:29 step:28001 [D loss: 0.116950, acc.: 97.66%] [G loss: 2.491147]\n",
      "epoch:29 step:28002 [D loss: 0.317568, acc.: 88.28%] [G loss: 4.145391]\n",
      "epoch:29 step:28003 [D loss: 0.410547, acc.: 75.78%] [G loss: 2.363547]\n",
      "epoch:29 step:28004 [D loss: 0.489250, acc.: 80.47%] [G loss: 4.631886]\n",
      "epoch:29 step:28005 [D loss: 0.138226, acc.: 96.88%] [G loss: 2.204810]\n",
      "epoch:29 step:28006 [D loss: 0.718430, acc.: 64.06%] [G loss: 4.402982]\n",
      "epoch:29 step:28007 [D loss: 0.298496, acc.: 89.84%] [G loss: 2.788580]\n",
      "epoch:29 step:28008 [D loss: 0.627619, acc.: 62.50%] [G loss: 1.482629]\n",
      "epoch:29 step:28009 [D loss: 0.215614, acc.: 91.41%] [G loss: 3.689041]\n",
      "epoch:29 step:28010 [D loss: 0.100737, acc.: 98.44%] [G loss: 2.976975]\n",
      "epoch:29 step:28011 [D loss: 0.123389, acc.: 98.44%] [G loss: 1.265763]\n",
      "epoch:29 step:28012 [D loss: 0.137865, acc.: 99.22%] [G loss: 3.463264]\n",
      "epoch:29 step:28013 [D loss: 0.372419, acc.: 84.38%] [G loss: 3.153749]\n",
      "epoch:29 step:28014 [D loss: 0.556570, acc.: 67.97%] [G loss: 0.640182]\n",
      "epoch:29 step:28015 [D loss: 0.249774, acc.: 89.06%] [G loss: 2.237319]\n",
      "epoch:29 step:28016 [D loss: 0.276334, acc.: 91.41%] [G loss: 5.067209]\n",
      "epoch:29 step:28017 [D loss: 0.335609, acc.: 85.94%] [G loss: 1.723776]\n",
      "epoch:29 step:28018 [D loss: 0.257252, acc.: 92.19%] [G loss: 1.394250]\n",
      "epoch:29 step:28019 [D loss: 0.064642, acc.: 98.44%] [G loss: 4.339074]\n",
      "epoch:29 step:28020 [D loss: 0.180600, acc.: 91.41%] [G loss: 2.185240]\n",
      "epoch:29 step:28021 [D loss: 0.449190, acc.: 82.81%] [G loss: 1.937112]\n",
      "epoch:29 step:28022 [D loss: 0.126023, acc.: 99.22%] [G loss: 1.487547]\n",
      "epoch:29 step:28023 [D loss: 0.207473, acc.: 95.31%] [G loss: 3.350383]\n",
      "epoch:29 step:28024 [D loss: 0.206770, acc.: 92.97%] [G loss: 3.471833]\n",
      "epoch:29 step:28025 [D loss: 0.137999, acc.: 96.88%] [G loss: 1.874153]\n",
      "epoch:29 step:28026 [D loss: 0.929405, acc.: 52.34%] [G loss: 3.328158]\n",
      "epoch:29 step:28027 [D loss: 0.964243, acc.: 57.03%] [G loss: 2.047736]\n",
      "epoch:29 step:28028 [D loss: 0.863551, acc.: 47.66%] [G loss: 2.405391]\n",
      "epoch:29 step:28029 [D loss: 0.523908, acc.: 65.62%] [G loss: 1.954175]\n",
      "epoch:29 step:28030 [D loss: 0.071633, acc.: 99.22%] [G loss: 2.390826]\n",
      "epoch:29 step:28031 [D loss: 0.321290, acc.: 89.84%] [G loss: 2.528160]\n",
      "epoch:29 step:28032 [D loss: 0.178918, acc.: 96.09%] [G loss: 2.021941]\n",
      "epoch:29 step:28033 [D loss: 0.101711, acc.: 97.66%] [G loss: 1.873281]\n",
      "epoch:29 step:28034 [D loss: 0.239377, acc.: 92.19%] [G loss: 1.475623]\n",
      "epoch:29 step:28035 [D loss: 0.163515, acc.: 98.44%] [G loss: 2.935687]\n",
      "epoch:29 step:28036 [D loss: 0.124485, acc.: 99.22%] [G loss: 3.586776]\n",
      "epoch:29 step:28037 [D loss: 0.364232, acc.: 82.81%] [G loss: 1.033570]\n",
      "epoch:29 step:28038 [D loss: 0.062570, acc.: 100.00%] [G loss: 4.071362]\n",
      "epoch:29 step:28039 [D loss: 0.828551, acc.: 60.16%] [G loss: 2.175483]\n",
      "epoch:29 step:28040 [D loss: 0.410791, acc.: 88.28%] [G loss: 2.714743]\n",
      "epoch:29 step:28041 [D loss: 0.675003, acc.: 60.16%] [G loss: 4.348739]\n",
      "epoch:29 step:28042 [D loss: 0.193855, acc.: 96.09%] [G loss: 6.871070]\n",
      "epoch:29 step:28043 [D loss: 0.645822, acc.: 64.06%] [G loss: 0.682660]\n",
      "epoch:29 step:28044 [D loss: 0.337886, acc.: 81.25%] [G loss: 2.927718]\n",
      "epoch:29 step:28045 [D loss: 0.109181, acc.: 100.00%] [G loss: 3.357131]\n",
      "epoch:29 step:28046 [D loss: 0.116271, acc.: 98.44%] [G loss: 3.288288]\n",
      "epoch:29 step:28047 [D loss: 1.112011, acc.: 53.12%] [G loss: 2.880216]\n",
      "epoch:29 step:28048 [D loss: 0.543238, acc.: 65.62%] [G loss: 2.309169]\n",
      "epoch:29 step:28049 [D loss: 0.672325, acc.: 63.28%] [G loss: 0.759036]\n",
      "epoch:29 step:28050 [D loss: 0.523481, acc.: 66.41%] [G loss: 2.979721]\n",
      "epoch:29 step:28051 [D loss: 0.200374, acc.: 94.53%] [G loss: 5.667296]\n",
      "epoch:29 step:28052 [D loss: 0.556892, acc.: 66.41%] [G loss: 2.401433]\n",
      "epoch:29 step:28053 [D loss: 0.302004, acc.: 90.62%] [G loss: 1.669335]\n",
      "epoch:29 step:28054 [D loss: 0.124017, acc.: 97.66%] [G loss: 2.181791]\n",
      "epoch:29 step:28055 [D loss: 0.346126, acc.: 85.16%] [G loss: 3.227849]\n",
      "epoch:29 step:28056 [D loss: 0.455828, acc.: 85.16%] [G loss: 2.518262]\n",
      "epoch:29 step:28057 [D loss: 0.266187, acc.: 90.62%] [G loss: 3.852090]\n",
      "epoch:29 step:28058 [D loss: 0.412938, acc.: 79.69%] [G loss: 1.767429]\n",
      "epoch:29 step:28059 [D loss: 0.394323, acc.: 78.91%] [G loss: 2.038434]\n",
      "epoch:29 step:28060 [D loss: 0.144613, acc.: 98.44%] [G loss: 5.165860]\n",
      "epoch:29 step:28061 [D loss: 0.121676, acc.: 97.66%] [G loss: 4.316953]\n",
      "epoch:29 step:28062 [D loss: 0.157101, acc.: 95.31%] [G loss: 1.713901]\n",
      "epoch:29 step:28063 [D loss: 0.098116, acc.: 99.22%] [G loss: 2.965582]\n",
      "epoch:29 step:28064 [D loss: 0.103114, acc.: 98.44%] [G loss: 2.165042]\n",
      "epoch:29 step:28065 [D loss: 0.383578, acc.: 75.00%] [G loss: 3.442440]\n",
      "epoch:29 step:28066 [D loss: 0.341785, acc.: 83.59%] [G loss: 3.089077]\n",
      "epoch:29 step:28067 [D loss: 0.291438, acc.: 88.28%] [G loss: 1.630669]\n",
      "epoch:29 step:28068 [D loss: 0.102225, acc.: 98.44%] [G loss: 5.012823]\n",
      "epoch:29 step:28069 [D loss: 0.172121, acc.: 96.09%] [G loss: 4.055288]\n",
      "epoch:29 step:28070 [D loss: 0.899438, acc.: 57.03%] [G loss: 5.017909]\n",
      "epoch:29 step:28071 [D loss: 0.185156, acc.: 95.31%] [G loss: 2.481666]\n",
      "epoch:29 step:28072 [D loss: 0.282999, acc.: 94.53%] [G loss: 3.689235]\n",
      "epoch:29 step:28073 [D loss: 0.126010, acc.: 99.22%] [G loss: 1.194757]\n",
      "epoch:29 step:28074 [D loss: 0.186843, acc.: 97.66%] [G loss: 2.394999]\n",
      "epoch:29 step:28075 [D loss: 0.060939, acc.: 99.22%] [G loss: 0.828057]\n",
      "epoch:29 step:28076 [D loss: 0.170182, acc.: 99.22%] [G loss: 1.026392]\n",
      "epoch:29 step:28077 [D loss: 0.073419, acc.: 100.00%] [G loss: 5.869154]\n",
      "epoch:29 step:28078 [D loss: 1.139378, acc.: 46.88%] [G loss: 2.774765]\n",
      "epoch:29 step:28079 [D loss: 0.111137, acc.: 97.66%] [G loss: 2.551157]\n",
      "epoch:29 step:28080 [D loss: 0.433380, acc.: 82.03%] [G loss: 2.341644]\n",
      "epoch:29 step:28081 [D loss: 0.080798, acc.: 99.22%] [G loss: 1.731454]\n",
      "epoch:29 step:28082 [D loss: 0.179778, acc.: 96.88%] [G loss: 4.415867]\n",
      "epoch:29 step:28083 [D loss: 0.258202, acc.: 89.84%] [G loss: 1.877590]\n",
      "epoch:29 step:28084 [D loss: 0.107954, acc.: 97.66%] [G loss: 3.969949]\n",
      "epoch:29 step:28085 [D loss: 1.308536, acc.: 53.91%] [G loss: 0.881491]\n",
      "epoch:29 step:28086 [D loss: 0.219639, acc.: 96.09%] [G loss: 0.790524]\n",
      "epoch:29 step:28087 [D loss: 0.633106, acc.: 65.62%] [G loss: 1.053340]\n",
      "epoch:29 step:28088 [D loss: 0.957320, acc.: 42.97%] [G loss: 3.223675]\n",
      "epoch:29 step:28089 [D loss: 0.342027, acc.: 78.12%] [G loss: 3.118085]\n",
      "epoch:29 step:28090 [D loss: 0.038922, acc.: 100.00%] [G loss: 1.767892]\n",
      "epoch:29 step:28091 [D loss: 0.076801, acc.: 99.22%] [G loss: 1.419772]\n",
      "epoch:29 step:28092 [D loss: 0.105090, acc.: 99.22%] [G loss: 1.934228]\n",
      "epoch:29 step:28093 [D loss: 0.366220, acc.: 84.38%] [G loss: 2.363547]\n",
      "epoch:29 step:28094 [D loss: 0.365370, acc.: 84.38%] [G loss: 2.775262]\n",
      "epoch:29 step:28095 [D loss: 0.317072, acc.: 90.62%] [G loss: 4.086682]\n",
      "epoch:29 step:28096 [D loss: 0.184203, acc.: 96.88%] [G loss: 6.819626]\n",
      "epoch:29 step:28097 [D loss: 0.034702, acc.: 99.22%] [G loss: 2.524181]\n",
      "epoch:29 step:28098 [D loss: 0.412263, acc.: 78.12%] [G loss: 2.714603]\n",
      "epoch:29 step:28099 [D loss: 0.323445, acc.: 83.59%] [G loss: 3.334119]\n",
      "epoch:29 step:28100 [D loss: 0.081563, acc.: 99.22%] [G loss: 2.471941]\n",
      "epoch:29 step:28101 [D loss: 0.444268, acc.: 77.34%] [G loss: 0.927126]\n",
      "epoch:29 step:28102 [D loss: 0.294192, acc.: 87.50%] [G loss: 3.331372]\n",
      "epoch:29 step:28103 [D loss: 0.160040, acc.: 96.88%] [G loss: 2.688081]\n",
      "epoch:29 step:28104 [D loss: 0.548887, acc.: 67.19%] [G loss: 5.114218]\n",
      "epoch:29 step:28105 [D loss: 0.509183, acc.: 66.41%] [G loss: 1.383537]\n",
      "epoch:29 step:28106 [D loss: 0.258817, acc.: 92.97%] [G loss: 1.749166]\n",
      "epoch:29 step:28107 [D loss: 0.181285, acc.: 95.31%] [G loss: 1.122822]\n",
      "epoch:29 step:28108 [D loss: 0.080432, acc.: 100.00%] [G loss: 1.751918]\n",
      "epoch:29 step:28109 [D loss: 0.393323, acc.: 79.69%] [G loss: 3.204125]\n",
      "epoch:29 step:28110 [D loss: 0.422571, acc.: 76.56%] [G loss: 2.948463]\n",
      "epoch:30 step:28111 [D loss: 0.422772, acc.: 82.03%] [G loss: 2.020533]\n",
      "epoch:30 step:28112 [D loss: 0.224056, acc.: 92.97%] [G loss: 1.184540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28113 [D loss: 0.218961, acc.: 91.41%] [G loss: 3.513739]\n",
      "epoch:30 step:28114 [D loss: 0.389502, acc.: 87.50%] [G loss: 1.472392]\n",
      "epoch:30 step:28115 [D loss: 0.114493, acc.: 100.00%] [G loss: 2.367190]\n",
      "epoch:30 step:28116 [D loss: 0.707464, acc.: 62.50%] [G loss: 4.418817]\n",
      "epoch:30 step:28117 [D loss: 0.445841, acc.: 75.78%] [G loss: 3.544427]\n",
      "epoch:30 step:28118 [D loss: 0.790732, acc.: 53.12%] [G loss: 1.689669]\n",
      "epoch:30 step:28119 [D loss: 0.419772, acc.: 76.56%] [G loss: 0.475854]\n",
      "epoch:30 step:28120 [D loss: 0.322180, acc.: 89.06%] [G loss: 1.545055]\n",
      "epoch:30 step:28121 [D loss: 0.159466, acc.: 96.88%] [G loss: 4.083942]\n",
      "epoch:30 step:28122 [D loss: 0.197585, acc.: 96.09%] [G loss: 1.660554]\n",
      "epoch:30 step:28123 [D loss: 0.341745, acc.: 85.94%] [G loss: 2.185379]\n",
      "epoch:30 step:28124 [D loss: 0.316547, acc.: 88.28%] [G loss: 3.240232]\n",
      "epoch:30 step:28125 [D loss: 0.179077, acc.: 97.66%] [G loss: 2.538917]\n",
      "epoch:30 step:28126 [D loss: 0.046800, acc.: 98.44%] [G loss: 5.564130]\n",
      "epoch:30 step:28127 [D loss: 0.014978, acc.: 100.00%] [G loss: 3.175543]\n",
      "epoch:30 step:28128 [D loss: 0.362962, acc.: 84.38%] [G loss: 2.777485]\n",
      "epoch:30 step:28129 [D loss: 0.163749, acc.: 98.44%] [G loss: 2.191348]\n",
      "epoch:30 step:28130 [D loss: 0.114890, acc.: 98.44%] [G loss: 1.355169]\n",
      "epoch:30 step:28131 [D loss: 0.084819, acc.: 98.44%] [G loss: 5.060368]\n",
      "epoch:30 step:28132 [D loss: 0.507176, acc.: 73.44%] [G loss: 3.029650]\n",
      "epoch:30 step:28133 [D loss: 0.060312, acc.: 98.44%] [G loss: 4.372219]\n",
      "epoch:30 step:28134 [D loss: 0.100257, acc.: 99.22%] [G loss: 0.748737]\n",
      "epoch:30 step:28135 [D loss: 0.162662, acc.: 94.53%] [G loss: 1.497396]\n",
      "epoch:30 step:28136 [D loss: 0.176819, acc.: 95.31%] [G loss: 0.397864]\n",
      "epoch:30 step:28137 [D loss: 0.391083, acc.: 83.59%] [G loss: 0.762209]\n",
      "epoch:30 step:28138 [D loss: 0.483186, acc.: 79.69%] [G loss: 3.212011]\n",
      "epoch:30 step:28139 [D loss: 0.135710, acc.: 98.44%] [G loss: 2.411599]\n",
      "epoch:30 step:28140 [D loss: 0.333519, acc.: 80.47%] [G loss: 1.868021]\n",
      "epoch:30 step:28141 [D loss: 0.327277, acc.: 84.38%] [G loss: 2.695255]\n",
      "epoch:30 step:28142 [D loss: 0.055274, acc.: 98.44%] [G loss: 2.578689]\n",
      "epoch:30 step:28143 [D loss: 0.450645, acc.: 75.78%] [G loss: 2.635396]\n",
      "epoch:30 step:28144 [D loss: 0.203440, acc.: 95.31%] [G loss: 4.162961]\n",
      "epoch:30 step:28145 [D loss: 0.130141, acc.: 98.44%] [G loss: 3.751480]\n",
      "epoch:30 step:28146 [D loss: 0.034855, acc.: 100.00%] [G loss: 4.542032]\n",
      "epoch:30 step:28147 [D loss: 0.377076, acc.: 82.03%] [G loss: 3.320499]\n",
      "epoch:30 step:28148 [D loss: 0.082496, acc.: 97.66%] [G loss: 6.300556]\n",
      "epoch:30 step:28149 [D loss: 0.288127, acc.: 92.97%] [G loss: 3.234063]\n",
      "epoch:30 step:28150 [D loss: 0.173100, acc.: 95.31%] [G loss: 4.080647]\n",
      "epoch:30 step:28151 [D loss: 2.131434, acc.: 14.84%] [G loss: 2.233636]\n",
      "epoch:30 step:28152 [D loss: 0.241411, acc.: 95.31%] [G loss: 3.202544]\n",
      "epoch:30 step:28153 [D loss: 0.483620, acc.: 68.75%] [G loss: 1.574455]\n",
      "epoch:30 step:28154 [D loss: 0.271766, acc.: 90.62%] [G loss: 4.794185]\n",
      "epoch:30 step:28155 [D loss: 1.739086, acc.: 28.12%] [G loss: 5.084649]\n",
      "epoch:30 step:28156 [D loss: 0.073398, acc.: 98.44%] [G loss: 2.362790]\n",
      "epoch:30 step:28157 [D loss: 1.114619, acc.: 51.56%] [G loss: 1.692146]\n",
      "epoch:30 step:28158 [D loss: 0.280333, acc.: 89.06%] [G loss: 1.900212]\n",
      "epoch:30 step:28159 [D loss: 0.118963, acc.: 99.22%] [G loss: 2.523481]\n",
      "epoch:30 step:28160 [D loss: 0.273846, acc.: 92.97%] [G loss: 2.606169]\n",
      "epoch:30 step:28161 [D loss: 0.159033, acc.: 96.88%] [G loss: 2.782098]\n",
      "epoch:30 step:28162 [D loss: 0.346757, acc.: 87.50%] [G loss: 5.091225]\n",
      "epoch:30 step:28163 [D loss: 0.202297, acc.: 95.31%] [G loss: 2.323891]\n",
      "epoch:30 step:28164 [D loss: 0.014303, acc.: 100.00%] [G loss: 4.075412]\n",
      "epoch:30 step:28165 [D loss: 0.099025, acc.: 98.44%] [G loss: 0.799717]\n",
      "epoch:30 step:28166 [D loss: 0.189288, acc.: 93.75%] [G loss: 4.845417]\n",
      "epoch:30 step:28167 [D loss: 0.200314, acc.: 94.53%] [G loss: 3.685383]\n",
      "epoch:30 step:28168 [D loss: 0.057934, acc.: 99.22%] [G loss: 3.692429]\n",
      "epoch:30 step:28169 [D loss: 0.198936, acc.: 96.09%] [G loss: 2.440356]\n",
      "epoch:30 step:28170 [D loss: 0.075680, acc.: 99.22%] [G loss: 3.822530]\n",
      "epoch:30 step:28171 [D loss: 0.065162, acc.: 100.00%] [G loss: 2.317651]\n",
      "epoch:30 step:28172 [D loss: 0.930961, acc.: 54.69%] [G loss: 4.147337]\n",
      "epoch:30 step:28173 [D loss: 0.056939, acc.: 99.22%] [G loss: 5.729694]\n",
      "epoch:30 step:28174 [D loss: 0.655361, acc.: 65.62%] [G loss: 3.236253]\n",
      "epoch:30 step:28175 [D loss: 0.550542, acc.: 72.66%] [G loss: 1.254105]\n",
      "epoch:30 step:28176 [D loss: 0.532183, acc.: 68.75%] [G loss: 2.006305]\n",
      "epoch:30 step:28177 [D loss: 0.196070, acc.: 97.66%] [G loss: 2.589572]\n",
      "epoch:30 step:28178 [D loss: 0.231417, acc.: 92.19%] [G loss: 1.214438]\n",
      "epoch:30 step:28179 [D loss: 0.582904, acc.: 67.19%] [G loss: 3.488923]\n",
      "epoch:30 step:28180 [D loss: 0.079550, acc.: 99.22%] [G loss: 5.392628]\n",
      "epoch:30 step:28181 [D loss: 0.188481, acc.: 97.66%] [G loss: 3.498644]\n",
      "epoch:30 step:28182 [D loss: 0.305645, acc.: 88.28%] [G loss: 3.605252]\n",
      "epoch:30 step:28183 [D loss: 0.059876, acc.: 100.00%] [G loss: 2.179926]\n",
      "epoch:30 step:28184 [D loss: 0.142675, acc.: 99.22%] [G loss: 1.243183]\n",
      "epoch:30 step:28185 [D loss: 0.498209, acc.: 75.78%] [G loss: 3.723100]\n",
      "epoch:30 step:28186 [D loss: 0.209733, acc.: 95.31%] [G loss: 1.231072]\n",
      "epoch:30 step:28187 [D loss: 0.668848, acc.: 62.50%] [G loss: 1.685784]\n",
      "epoch:30 step:28188 [D loss: 0.316588, acc.: 89.84%] [G loss: 2.867527]\n",
      "epoch:30 step:28189 [D loss: 0.290720, acc.: 89.84%] [G loss: 0.888538]\n",
      "epoch:30 step:28190 [D loss: 1.296070, acc.: 41.41%] [G loss: 1.539605]\n",
      "epoch:30 step:28191 [D loss: 0.008332, acc.: 100.00%] [G loss: 6.988144]\n",
      "epoch:30 step:28192 [D loss: 0.136054, acc.: 96.88%] [G loss: 4.299642]\n",
      "epoch:30 step:28193 [D loss: 0.324805, acc.: 85.16%] [G loss: 3.927902]\n",
      "epoch:30 step:28194 [D loss: 0.108290, acc.: 98.44%] [G loss: 3.551823]\n",
      "epoch:30 step:28195 [D loss: 0.198688, acc.: 93.75%] [G loss: 4.340600]\n",
      "epoch:30 step:28196 [D loss: 0.789615, acc.: 53.91%] [G loss: 2.241957]\n",
      "epoch:30 step:28197 [D loss: 0.704510, acc.: 57.81%] [G loss: 6.447948]\n",
      "epoch:30 step:28198 [D loss: 0.197080, acc.: 93.75%] [G loss: 4.892457]\n",
      "epoch:30 step:28199 [D loss: 0.268157, acc.: 86.72%] [G loss: 4.077473]\n",
      "epoch:30 step:28200 [D loss: 0.081829, acc.: 99.22%] [G loss: 1.925281]\n",
      "epoch:30 step:28201 [D loss: 0.145280, acc.: 96.09%] [G loss: 2.310471]\n",
      "epoch:30 step:28202 [D loss: 0.183780, acc.: 94.53%] [G loss: 2.815037]\n",
      "epoch:30 step:28203 [D loss: 0.405876, acc.: 88.28%] [G loss: 2.563036]\n",
      "epoch:30 step:28204 [D loss: 0.147580, acc.: 98.44%] [G loss: 1.717237]\n",
      "epoch:30 step:28205 [D loss: 0.576618, acc.: 65.62%] [G loss: 3.457788]\n",
      "epoch:30 step:28206 [D loss: 0.354767, acc.: 86.72%] [G loss: 2.987871]\n",
      "epoch:30 step:28207 [D loss: 0.087466, acc.: 98.44%] [G loss: 1.758724]\n",
      "epoch:30 step:28208 [D loss: 0.979288, acc.: 45.31%] [G loss: 1.176557]\n",
      "epoch:30 step:28209 [D loss: 1.277496, acc.: 51.56%] [G loss: 1.883643]\n",
      "epoch:30 step:28210 [D loss: 0.648107, acc.: 67.97%] [G loss: 0.706327]\n",
      "epoch:30 step:28211 [D loss: 1.076411, acc.: 51.56%] [G loss: 1.458911]\n",
      "epoch:30 step:28212 [D loss: 0.193596, acc.: 94.53%] [G loss: 3.694260]\n",
      "epoch:30 step:28213 [D loss: 0.142990, acc.: 96.88%] [G loss: 1.951912]\n",
      "epoch:30 step:28214 [D loss: 0.266792, acc.: 92.19%] [G loss: 1.228310]\n",
      "epoch:30 step:28215 [D loss: 0.083919, acc.: 99.22%] [G loss: 2.282304]\n",
      "epoch:30 step:28216 [D loss: 0.056005, acc.: 99.22%] [G loss: 3.614513]\n",
      "epoch:30 step:28217 [D loss: 0.265891, acc.: 92.19%] [G loss: 4.634204]\n",
      "epoch:30 step:28218 [D loss: 0.490334, acc.: 73.44%] [G loss: 1.189476]\n",
      "epoch:30 step:28219 [D loss: 0.336705, acc.: 92.19%] [G loss: 1.734138]\n",
      "epoch:30 step:28220 [D loss: 0.209867, acc.: 96.88%] [G loss: 3.760914]\n",
      "epoch:30 step:28221 [D loss: 0.054374, acc.: 100.00%] [G loss: 3.204160]\n",
      "epoch:30 step:28222 [D loss: 1.007193, acc.: 53.91%] [G loss: 2.078370]\n",
      "epoch:30 step:28223 [D loss: 0.026873, acc.: 100.00%] [G loss: 1.351341]\n",
      "epoch:30 step:28224 [D loss: 0.568378, acc.: 67.19%] [G loss: 1.813721]\n",
      "epoch:30 step:28225 [D loss: 0.254858, acc.: 91.41%] [G loss: 1.801837]\n",
      "epoch:30 step:28226 [D loss: 0.131757, acc.: 98.44%] [G loss: 1.446808]\n",
      "epoch:30 step:28227 [D loss: 0.245793, acc.: 96.09%] [G loss: 3.500091]\n",
      "epoch:30 step:28228 [D loss: 0.221254, acc.: 95.31%] [G loss: 3.000086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28229 [D loss: 0.405068, acc.: 85.16%] [G loss: 2.435973]\n",
      "epoch:30 step:28230 [D loss: 0.264889, acc.: 91.41%] [G loss: 1.868492]\n",
      "epoch:30 step:28231 [D loss: 0.187240, acc.: 96.88%] [G loss: 2.830255]\n",
      "epoch:30 step:28232 [D loss: 0.609515, acc.: 73.44%] [G loss: 1.443032]\n",
      "epoch:30 step:28233 [D loss: 0.180766, acc.: 98.44%] [G loss: 0.804594]\n",
      "epoch:30 step:28234 [D loss: 0.135109, acc.: 97.66%] [G loss: 3.910052]\n",
      "epoch:30 step:28235 [D loss: 0.348446, acc.: 78.91%] [G loss: 2.605020]\n",
      "epoch:30 step:28236 [D loss: 0.411648, acc.: 85.16%] [G loss: 14.766286]\n",
      "epoch:30 step:28237 [D loss: 0.331022, acc.: 80.47%] [G loss: 2.516092]\n",
      "epoch:30 step:28238 [D loss: 0.104890, acc.: 97.66%] [G loss: 2.187970]\n",
      "epoch:30 step:28239 [D loss: 0.681657, acc.: 59.38%] [G loss: 5.312723]\n",
      "epoch:30 step:28240 [D loss: 0.252919, acc.: 95.31%] [G loss: 0.945034]\n",
      "epoch:30 step:28241 [D loss: 1.279609, acc.: 28.12%] [G loss: 3.583213]\n",
      "epoch:30 step:28242 [D loss: 0.276909, acc.: 88.28%] [G loss: 1.266911]\n",
      "epoch:30 step:28243 [D loss: 0.040761, acc.: 99.22%] [G loss: 1.917898]\n",
      "epoch:30 step:28244 [D loss: 0.147453, acc.: 96.09%] [G loss: 2.029169]\n",
      "epoch:30 step:28245 [D loss: 0.286851, acc.: 92.97%] [G loss: 1.330743]\n",
      "epoch:30 step:28246 [D loss: 0.773196, acc.: 56.25%] [G loss: 7.040321]\n",
      "epoch:30 step:28247 [D loss: 0.320241, acc.: 86.72%] [G loss: 3.364491]\n",
      "epoch:30 step:28248 [D loss: 0.617472, acc.: 62.50%] [G loss: 2.875773]\n",
      "epoch:30 step:28249 [D loss: 0.087961, acc.: 99.22%] [G loss: 3.638488]\n",
      "epoch:30 step:28250 [D loss: 0.227266, acc.: 94.53%] [G loss: 1.817103]\n",
      "epoch:30 step:28251 [D loss: 0.143876, acc.: 96.88%] [G loss: 4.857557]\n",
      "epoch:30 step:28252 [D loss: 0.245338, acc.: 92.97%] [G loss: 1.498542]\n",
      "epoch:30 step:28253 [D loss: 0.490880, acc.: 78.12%] [G loss: 2.386124]\n",
      "epoch:30 step:28254 [D loss: 0.120947, acc.: 98.44%] [G loss: 3.661559]\n",
      "epoch:30 step:28255 [D loss: 0.291141, acc.: 92.97%] [G loss: 2.133349]\n",
      "epoch:30 step:28256 [D loss: 0.821821, acc.: 42.19%] [G loss: 1.930661]\n",
      "epoch:30 step:28257 [D loss: 0.145895, acc.: 98.44%] [G loss: 2.673332]\n",
      "epoch:30 step:28258 [D loss: 0.149321, acc.: 95.31%] [G loss: 2.036964]\n",
      "epoch:30 step:28259 [D loss: 0.212970, acc.: 92.19%] [G loss: 3.943087]\n",
      "epoch:30 step:28260 [D loss: 0.293406, acc.: 86.72%] [G loss: 2.252114]\n",
      "epoch:30 step:28261 [D loss: 0.532766, acc.: 75.78%] [G loss: 3.064056]\n",
      "epoch:30 step:28262 [D loss: 0.115131, acc.: 96.09%] [G loss: 4.493068]\n",
      "epoch:30 step:28263 [D loss: 0.044207, acc.: 100.00%] [G loss: 2.897645]\n",
      "epoch:30 step:28264 [D loss: 0.132004, acc.: 99.22%] [G loss: 0.996348]\n",
      "epoch:30 step:28265 [D loss: 0.207348, acc.: 93.75%] [G loss: 5.344989]\n",
      "epoch:30 step:28266 [D loss: 0.133339, acc.: 96.88%] [G loss: 2.778018]\n",
      "epoch:30 step:28267 [D loss: 0.133251, acc.: 97.66%] [G loss: 3.067690]\n",
      "epoch:30 step:28268 [D loss: 0.220523, acc.: 94.53%] [G loss: 1.396432]\n",
      "epoch:30 step:28269 [D loss: 0.313875, acc.: 83.59%] [G loss: 4.528265]\n",
      "epoch:30 step:28270 [D loss: 0.102705, acc.: 98.44%] [G loss: 4.421504]\n",
      "epoch:30 step:28271 [D loss: 0.223067, acc.: 94.53%] [G loss: 2.604301]\n",
      "epoch:30 step:28272 [D loss: 0.010622, acc.: 100.00%] [G loss: 2.215815]\n",
      "epoch:30 step:28273 [D loss: 0.122585, acc.: 96.88%] [G loss: 1.878396]\n",
      "epoch:30 step:28274 [D loss: 0.261117, acc.: 88.28%] [G loss: 5.632858]\n",
      "epoch:30 step:28275 [D loss: 0.221246, acc.: 92.97%] [G loss: 4.720450]\n",
      "epoch:30 step:28276 [D loss: 0.893165, acc.: 57.03%] [G loss: 0.695056]\n",
      "epoch:30 step:28277 [D loss: 0.155008, acc.: 96.88%] [G loss: 2.224732]\n",
      "epoch:30 step:28278 [D loss: 1.222918, acc.: 54.69%] [G loss: 0.864857]\n",
      "epoch:30 step:28279 [D loss: 0.096477, acc.: 97.66%] [G loss: 5.414184]\n",
      "epoch:30 step:28280 [D loss: 0.298196, acc.: 89.06%] [G loss: 3.895155]\n",
      "epoch:30 step:28281 [D loss: 0.484201, acc.: 68.75%] [G loss: 3.252842]\n",
      "epoch:30 step:28282 [D loss: 0.380956, acc.: 85.94%] [G loss: 0.836700]\n",
      "epoch:30 step:28283 [D loss: 0.425021, acc.: 79.69%] [G loss: 1.699362]\n",
      "epoch:30 step:28284 [D loss: 1.472770, acc.: 49.22%] [G loss: 3.559157]\n",
      "epoch:30 step:28285 [D loss: 0.062781, acc.: 98.44%] [G loss: 4.327480]\n",
      "epoch:30 step:28286 [D loss: 0.134482, acc.: 97.66%] [G loss: 4.268746]\n",
      "epoch:30 step:28287 [D loss: 0.047572, acc.: 99.22%] [G loss: 2.987404]\n",
      "epoch:30 step:28288 [D loss: 0.248706, acc.: 93.75%] [G loss: 4.042456]\n",
      "epoch:30 step:28289 [D loss: 0.921627, acc.: 54.69%] [G loss: 1.351593]\n",
      "epoch:30 step:28290 [D loss: 0.076166, acc.: 100.00%] [G loss: 1.264096]\n",
      "epoch:30 step:28291 [D loss: 0.126042, acc.: 97.66%] [G loss: 2.068390]\n",
      "epoch:30 step:28292 [D loss: 0.199785, acc.: 98.44%] [G loss: 2.185841]\n",
      "epoch:30 step:28293 [D loss: 0.176864, acc.: 96.88%] [G loss: 1.609051]\n",
      "epoch:30 step:28294 [D loss: 0.131155, acc.: 97.66%] [G loss: 0.973878]\n",
      "epoch:30 step:28295 [D loss: 0.194970, acc.: 95.31%] [G loss: 1.457421]\n",
      "epoch:30 step:28296 [D loss: 0.330187, acc.: 80.47%] [G loss: 2.687812]\n",
      "epoch:30 step:28297 [D loss: 0.110398, acc.: 98.44%] [G loss: 3.132965]\n",
      "epoch:30 step:28298 [D loss: 0.031230, acc.: 100.00%] [G loss: 4.797316]\n",
      "epoch:30 step:28299 [D loss: 0.119972, acc.: 98.44%] [G loss: 1.220534]\n",
      "epoch:30 step:28300 [D loss: 0.158941, acc.: 97.66%] [G loss: 2.499343]\n",
      "epoch:30 step:28301 [D loss: 0.741929, acc.: 60.94%] [G loss: 1.185210]\n",
      "epoch:30 step:28302 [D loss: 0.214374, acc.: 92.97%] [G loss: 5.462774]\n",
      "epoch:30 step:28303 [D loss: 0.121163, acc.: 98.44%] [G loss: 1.548822]\n",
      "epoch:30 step:28304 [D loss: 0.442777, acc.: 72.66%] [G loss: 3.360790]\n",
      "epoch:30 step:28305 [D loss: 0.077080, acc.: 98.44%] [G loss: 5.365827]\n",
      "epoch:30 step:28306 [D loss: 0.529653, acc.: 66.41%] [G loss: 3.680830]\n",
      "epoch:30 step:28307 [D loss: 0.131783, acc.: 98.44%] [G loss: 4.604674]\n",
      "epoch:30 step:28308 [D loss: 0.258479, acc.: 91.41%] [G loss: 3.897765]\n",
      "epoch:30 step:28309 [D loss: 1.677010, acc.: 21.88%] [G loss: 3.445830]\n",
      "epoch:30 step:28310 [D loss: 0.153432, acc.: 94.53%] [G loss: 1.548958]\n",
      "epoch:30 step:28311 [D loss: 0.016447, acc.: 100.00%] [G loss: 2.691731]\n",
      "epoch:30 step:28312 [D loss: 0.472578, acc.: 76.56%] [G loss: 2.234270]\n",
      "epoch:30 step:28313 [D loss: 0.741760, acc.: 59.38%] [G loss: 0.873735]\n",
      "epoch:30 step:28314 [D loss: 0.135935, acc.: 97.66%] [G loss: 2.092346]\n",
      "epoch:30 step:28315 [D loss: 0.702907, acc.: 60.94%] [G loss: 2.031732]\n",
      "epoch:30 step:28316 [D loss: 0.107344, acc.: 97.66%] [G loss: 1.959970]\n",
      "epoch:30 step:28317 [D loss: 0.774961, acc.: 58.59%] [G loss: 3.533857]\n",
      "epoch:30 step:28318 [D loss: 0.484309, acc.: 76.56%] [G loss: 5.036700]\n",
      "epoch:30 step:28319 [D loss: 0.553477, acc.: 67.97%] [G loss: 2.657005]\n",
      "epoch:30 step:28320 [D loss: 0.219149, acc.: 89.84%] [G loss: 0.531564]\n",
      "epoch:30 step:28321 [D loss: 0.499808, acc.: 76.56%] [G loss: 1.944663]\n",
      "epoch:30 step:28322 [D loss: 0.329802, acc.: 88.28%] [G loss: 5.251953]\n",
      "epoch:30 step:28323 [D loss: 0.453819, acc.: 77.34%] [G loss: 3.388479]\n",
      "epoch:30 step:28324 [D loss: 0.380224, acc.: 78.91%] [G loss: 3.835941]\n",
      "epoch:30 step:28325 [D loss: 0.302824, acc.: 82.81%] [G loss: 2.147498]\n",
      "epoch:30 step:28326 [D loss: 0.045468, acc.: 99.22%] [G loss: 3.537873]\n",
      "epoch:30 step:28327 [D loss: 0.259768, acc.: 91.41%] [G loss: 3.171540]\n",
      "epoch:30 step:28328 [D loss: 0.278629, acc.: 85.94%] [G loss: 4.307869]\n",
      "epoch:30 step:28329 [D loss: 0.151711, acc.: 98.44%] [G loss: 4.072434]\n",
      "epoch:30 step:28330 [D loss: 0.450483, acc.: 74.22%] [G loss: 2.774321]\n",
      "epoch:30 step:28331 [D loss: 0.170944, acc.: 96.09%] [G loss: 1.411288]\n",
      "epoch:30 step:28332 [D loss: 0.170011, acc.: 96.88%] [G loss: 3.590979]\n",
      "epoch:30 step:28333 [D loss: 0.071279, acc.: 98.44%] [G loss: 1.264595]\n",
      "epoch:30 step:28334 [D loss: 0.161835, acc.: 96.09%] [G loss: 4.816242]\n",
      "epoch:30 step:28335 [D loss: 0.181212, acc.: 96.88%] [G loss: 4.242144]\n",
      "epoch:30 step:28336 [D loss: 0.365219, acc.: 87.50%] [G loss: 3.188504]\n",
      "epoch:30 step:28337 [D loss: 0.058133, acc.: 99.22%] [G loss: 2.675412]\n",
      "epoch:30 step:28338 [D loss: 0.768013, acc.: 60.16%] [G loss: 2.471746]\n",
      "epoch:30 step:28339 [D loss: 0.364814, acc.: 80.47%] [G loss: 1.277124]\n",
      "epoch:30 step:28340 [D loss: 0.183347, acc.: 96.88%] [G loss: 2.817226]\n",
      "epoch:30 step:28341 [D loss: 0.051299, acc.: 100.00%] [G loss: 2.695109]\n",
      "epoch:30 step:28342 [D loss: 0.316700, acc.: 82.81%] [G loss: 6.441432]\n",
      "epoch:30 step:28343 [D loss: 0.132667, acc.: 97.66%] [G loss: 4.359664]\n",
      "epoch:30 step:28344 [D loss: 0.104528, acc.: 97.66%] [G loss: 1.226778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28345 [D loss: 0.245070, acc.: 91.41%] [G loss: 2.769300]\n",
      "epoch:30 step:28346 [D loss: 0.941981, acc.: 53.12%] [G loss: 4.289835]\n",
      "epoch:30 step:28347 [D loss: 0.223807, acc.: 93.75%] [G loss: 8.088945]\n",
      "epoch:30 step:28348 [D loss: 0.877195, acc.: 56.25%] [G loss: 4.993052]\n",
      "epoch:30 step:28349 [D loss: 0.141682, acc.: 98.44%] [G loss: 4.512860]\n",
      "epoch:30 step:28350 [D loss: 0.107243, acc.: 100.00%] [G loss: 1.113783]\n",
      "epoch:30 step:28351 [D loss: 0.232005, acc.: 91.41%] [G loss: 3.461699]\n",
      "epoch:30 step:28352 [D loss: 0.391406, acc.: 83.59%] [G loss: 4.486886]\n",
      "epoch:30 step:28353 [D loss: 0.107745, acc.: 97.66%] [G loss: 2.753082]\n",
      "epoch:30 step:28354 [D loss: 0.202650, acc.: 95.31%] [G loss: 1.873002]\n",
      "epoch:30 step:28355 [D loss: 0.336682, acc.: 79.69%] [G loss: 3.777697]\n",
      "epoch:30 step:28356 [D loss: 0.088646, acc.: 99.22%] [G loss: 1.400288]\n",
      "epoch:30 step:28357 [D loss: 0.170371, acc.: 97.66%] [G loss: 1.097213]\n",
      "epoch:30 step:28358 [D loss: 0.119069, acc.: 98.44%] [G loss: 1.303306]\n",
      "epoch:30 step:28359 [D loss: 0.440334, acc.: 76.56%] [G loss: 1.882815]\n",
      "epoch:30 step:28360 [D loss: 0.164237, acc.: 97.66%] [G loss: 1.045242]\n",
      "epoch:30 step:28361 [D loss: 0.131948, acc.: 98.44%] [G loss: 2.129826]\n",
      "epoch:30 step:28362 [D loss: 0.042893, acc.: 100.00%] [G loss: 2.691793]\n",
      "epoch:30 step:28363 [D loss: 0.165260, acc.: 96.88%] [G loss: 3.574700]\n",
      "epoch:30 step:28364 [D loss: 0.106555, acc.: 98.44%] [G loss: 1.903120]\n",
      "epoch:30 step:28365 [D loss: 0.214629, acc.: 96.88%] [G loss: 1.690374]\n",
      "epoch:30 step:28366 [D loss: 0.167670, acc.: 96.88%] [G loss: 2.215621]\n",
      "epoch:30 step:28367 [D loss: 0.061846, acc.: 100.00%] [G loss: 2.469128]\n",
      "epoch:30 step:28368 [D loss: 0.671877, acc.: 59.38%] [G loss: 1.102443]\n",
      "epoch:30 step:28369 [D loss: 0.144406, acc.: 97.66%] [G loss: 5.136147]\n",
      "epoch:30 step:28370 [D loss: 0.107558, acc.: 99.22%] [G loss: 1.154692]\n",
      "epoch:30 step:28371 [D loss: 0.439526, acc.: 71.88%] [G loss: 0.792764]\n",
      "epoch:30 step:28372 [D loss: 0.077450, acc.: 97.66%] [G loss: 0.802521]\n",
      "epoch:30 step:28373 [D loss: 0.113179, acc.: 98.44%] [G loss: 1.344960]\n",
      "epoch:30 step:28374 [D loss: 0.539098, acc.: 64.84%] [G loss: 0.292025]\n",
      "epoch:30 step:28375 [D loss: 0.051699, acc.: 99.22%] [G loss: 4.191187]\n",
      "epoch:30 step:28376 [D loss: 0.218992, acc.: 92.19%] [G loss: 0.979842]\n",
      "epoch:30 step:28377 [D loss: 0.144466, acc.: 97.66%] [G loss: 1.152044]\n",
      "epoch:30 step:28378 [D loss: 0.243123, acc.: 91.41%] [G loss: 1.236137]\n",
      "epoch:30 step:28379 [D loss: 0.031627, acc.: 100.00%] [G loss: 0.891394]\n",
      "epoch:30 step:28380 [D loss: 0.200240, acc.: 94.53%] [G loss: 3.059058]\n",
      "epoch:30 step:28381 [D loss: 0.625328, acc.: 61.72%] [G loss: 5.054504]\n",
      "epoch:30 step:28382 [D loss: 0.339202, acc.: 78.12%] [G loss: 4.459635]\n",
      "epoch:30 step:28383 [D loss: 0.240273, acc.: 90.62%] [G loss: 0.826456]\n",
      "epoch:30 step:28384 [D loss: 0.271263, acc.: 91.41%] [G loss: 2.629204]\n",
      "epoch:30 step:28385 [D loss: 0.888110, acc.: 53.91%] [G loss: 1.591136]\n",
      "epoch:30 step:28386 [D loss: 0.331945, acc.: 91.41%] [G loss: 3.656646]\n",
      "epoch:30 step:28387 [D loss: 2.237611, acc.: 14.06%] [G loss: 2.504064]\n",
      "epoch:30 step:28388 [D loss: 0.083740, acc.: 97.66%] [G loss: 4.775450]\n",
      "epoch:30 step:28389 [D loss: 0.026791, acc.: 100.00%] [G loss: 0.389107]\n",
      "epoch:30 step:28390 [D loss: 0.216452, acc.: 92.97%] [G loss: 3.962028]\n",
      "epoch:30 step:28391 [D loss: 0.119789, acc.: 95.31%] [G loss: 5.710620]\n",
      "epoch:30 step:28392 [D loss: 0.218349, acc.: 93.75%] [G loss: 1.944808]\n",
      "epoch:30 step:28393 [D loss: 0.030380, acc.: 100.00%] [G loss: 3.392508]\n",
      "epoch:30 step:28394 [D loss: 0.631506, acc.: 65.62%] [G loss: 2.553205]\n",
      "epoch:30 step:28395 [D loss: 0.068733, acc.: 100.00%] [G loss: 3.458647]\n",
      "epoch:30 step:28396 [D loss: 0.638245, acc.: 60.94%] [G loss: 4.969687]\n",
      "epoch:30 step:28397 [D loss: 0.085159, acc.: 98.44%] [G loss: 1.763023]\n",
      "epoch:30 step:28398 [D loss: 0.257631, acc.: 85.16%] [G loss: 4.480214]\n",
      "epoch:30 step:28399 [D loss: 0.130131, acc.: 96.09%] [G loss: 4.093905]\n",
      "epoch:30 step:28400 [D loss: 0.056808, acc.: 99.22%] [G loss: 4.984975]\n",
      "epoch:30 step:28401 [D loss: 0.067111, acc.: 99.22%] [G loss: 2.572772]\n",
      "epoch:30 step:28402 [D loss: 0.340336, acc.: 92.97%] [G loss: 0.195557]\n",
      "epoch:30 step:28403 [D loss: 0.851020, acc.: 53.91%] [G loss: 2.820452]\n",
      "epoch:30 step:28404 [D loss: 0.168925, acc.: 93.75%] [G loss: 5.174541]\n",
      "epoch:30 step:28405 [D loss: 0.208350, acc.: 94.53%] [G loss: 5.911528]\n",
      "epoch:30 step:28406 [D loss: 0.077421, acc.: 98.44%] [G loss: 3.122290]\n",
      "epoch:30 step:28407 [D loss: 0.463506, acc.: 79.69%] [G loss: 4.517322]\n",
      "epoch:30 step:28408 [D loss: 1.507030, acc.: 15.62%] [G loss: 2.752036]\n",
      "epoch:30 step:28409 [D loss: 0.234849, acc.: 92.97%] [G loss: 1.348170]\n",
      "epoch:30 step:28410 [D loss: 0.288496, acc.: 84.38%] [G loss: 2.693979]\n",
      "epoch:30 step:28411 [D loss: 0.322815, acc.: 83.59%] [G loss: 3.149651]\n",
      "epoch:30 step:28412 [D loss: 0.655140, acc.: 60.94%] [G loss: 3.031264]\n",
      "epoch:30 step:28413 [D loss: 0.313270, acc.: 87.50%] [G loss: 4.984642]\n",
      "epoch:30 step:28414 [D loss: 0.542140, acc.: 67.19%] [G loss: 3.048112]\n",
      "epoch:30 step:28415 [D loss: 0.044529, acc.: 99.22%] [G loss: 3.799212]\n",
      "epoch:30 step:28416 [D loss: 0.185127, acc.: 96.09%] [G loss: 3.617219]\n",
      "epoch:30 step:28417 [D loss: 0.076831, acc.: 99.22%] [G loss: 3.608609]\n",
      "epoch:30 step:28418 [D loss: 0.100018, acc.: 99.22%] [G loss: 4.743848]\n",
      "epoch:30 step:28419 [D loss: 1.125050, acc.: 36.72%] [G loss: 3.017444]\n",
      "epoch:30 step:28420 [D loss: 0.526578, acc.: 72.66%] [G loss: 6.773811]\n",
      "epoch:30 step:28421 [D loss: 0.402022, acc.: 75.78%] [G loss: 2.918507]\n",
      "epoch:30 step:28422 [D loss: 0.282079, acc.: 85.94%] [G loss: 4.141883]\n",
      "epoch:30 step:28423 [D loss: 1.173730, acc.: 46.88%] [G loss: 2.717321]\n",
      "epoch:30 step:28424 [D loss: 0.406878, acc.: 76.56%] [G loss: 2.646564]\n",
      "epoch:30 step:28425 [D loss: 0.072402, acc.: 97.66%] [G loss: 5.485775]\n",
      "epoch:30 step:28426 [D loss: 0.040175, acc.: 100.00%] [G loss: 4.650245]\n",
      "epoch:30 step:28427 [D loss: 0.119224, acc.: 99.22%] [G loss: 1.228925]\n",
      "epoch:30 step:28428 [D loss: 0.210979, acc.: 89.84%] [G loss: 1.278998]\n",
      "epoch:30 step:28429 [D loss: 0.085188, acc.: 100.00%] [G loss: 2.887789]\n",
      "epoch:30 step:28430 [D loss: 0.207223, acc.: 94.53%] [G loss: 2.019358]\n",
      "epoch:30 step:28431 [D loss: 0.147765, acc.: 96.88%] [G loss: 2.632300]\n",
      "epoch:30 step:28432 [D loss: 0.062522, acc.: 99.22%] [G loss: 2.397947]\n",
      "epoch:30 step:28433 [D loss: 0.090422, acc.: 98.44%] [G loss: 1.454915]\n",
      "epoch:30 step:28434 [D loss: 0.381877, acc.: 78.12%] [G loss: 3.350254]\n",
      "epoch:30 step:28435 [D loss: 0.089123, acc.: 100.00%] [G loss: 4.639291]\n",
      "epoch:30 step:28436 [D loss: 0.497519, acc.: 72.66%] [G loss: 5.395430]\n",
      "epoch:30 step:28437 [D loss: 0.064006, acc.: 99.22%] [G loss: 4.737653]\n",
      "epoch:30 step:28438 [D loss: 0.258912, acc.: 92.19%] [G loss: 2.680626]\n",
      "epoch:30 step:28439 [D loss: 0.117504, acc.: 98.44%] [G loss: 3.468529]\n",
      "epoch:30 step:28440 [D loss: 0.353003, acc.: 89.06%] [G loss: 0.341650]\n",
      "epoch:30 step:28441 [D loss: 0.520903, acc.: 66.41%] [G loss: 1.239950]\n",
      "epoch:30 step:28442 [D loss: 1.397455, acc.: 52.34%] [G loss: 4.695537]\n",
      "epoch:30 step:28443 [D loss: 0.104819, acc.: 100.00%] [G loss: 2.584270]\n",
      "epoch:30 step:28444 [D loss: 0.550868, acc.: 71.09%] [G loss: 4.146123]\n",
      "epoch:30 step:28445 [D loss: 0.618805, acc.: 73.44%] [G loss: 1.166869]\n",
      "epoch:30 step:28446 [D loss: 0.203138, acc.: 92.97%] [G loss: 1.521103]\n",
      "epoch:30 step:28447 [D loss: 0.414349, acc.: 71.09%] [G loss: 1.878540]\n",
      "epoch:30 step:28448 [D loss: 0.213849, acc.: 94.53%] [G loss: 3.982924]\n",
      "epoch:30 step:28449 [D loss: 0.733678, acc.: 61.72%] [G loss: 2.215281]\n",
      "epoch:30 step:28450 [D loss: 0.692404, acc.: 61.72%] [G loss: 1.365850]\n",
      "epoch:30 step:28451 [D loss: 0.737334, acc.: 59.38%] [G loss: 2.746140]\n",
      "epoch:30 step:28452 [D loss: 0.136216, acc.: 96.88%] [G loss: 3.359485]\n",
      "epoch:30 step:28453 [D loss: 0.317988, acc.: 90.62%] [G loss: 2.322843]\n",
      "epoch:30 step:28454 [D loss: 0.196498, acc.: 95.31%] [G loss: 1.497034]\n",
      "epoch:30 step:28455 [D loss: 0.134360, acc.: 98.44%] [G loss: 1.981895]\n",
      "epoch:30 step:28456 [D loss: 0.158082, acc.: 96.88%] [G loss: 3.495773]\n",
      "epoch:30 step:28457 [D loss: 1.004053, acc.: 53.91%] [G loss: 0.906360]\n",
      "epoch:30 step:28458 [D loss: 0.502540, acc.: 77.34%] [G loss: 0.427728]\n",
      "epoch:30 step:28459 [D loss: 0.365110, acc.: 85.94%] [G loss: 2.881351]\n",
      "epoch:30 step:28460 [D loss: 0.208958, acc.: 93.75%] [G loss: 3.842129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28461 [D loss: 0.177264, acc.: 96.88%] [G loss: 2.756523]\n",
      "epoch:30 step:28462 [D loss: 0.155763, acc.: 96.88%] [G loss: 1.520803]\n",
      "epoch:30 step:28463 [D loss: 0.455950, acc.: 76.56%] [G loss: 3.197348]\n",
      "epoch:30 step:28464 [D loss: 0.247373, acc.: 89.84%] [G loss: 1.611807]\n",
      "epoch:30 step:28465 [D loss: 0.513916, acc.: 72.66%] [G loss: 5.545403]\n",
      "epoch:30 step:28466 [D loss: 0.500680, acc.: 75.78%] [G loss: 0.723772]\n",
      "epoch:30 step:28467 [D loss: 0.615766, acc.: 67.19%] [G loss: 0.599888]\n",
      "epoch:30 step:28468 [D loss: 0.303135, acc.: 86.72%] [G loss: 0.805796]\n",
      "epoch:30 step:28469 [D loss: 0.282334, acc.: 92.19%] [G loss: 2.454536]\n",
      "epoch:30 step:28470 [D loss: 0.088350, acc.: 98.44%] [G loss: 1.338579]\n",
      "epoch:30 step:28471 [D loss: 0.404788, acc.: 82.03%] [G loss: 3.867701]\n",
      "epoch:30 step:28472 [D loss: 0.690742, acc.: 60.94%] [G loss: 3.316324]\n",
      "epoch:30 step:28473 [D loss: 0.604848, acc.: 65.62%] [G loss: 2.086037]\n",
      "epoch:30 step:28474 [D loss: 0.070519, acc.: 99.22%] [G loss: 5.322564]\n",
      "epoch:30 step:28475 [D loss: 0.124830, acc.: 97.66%] [G loss: 1.963759]\n",
      "epoch:30 step:28476 [D loss: 0.843186, acc.: 53.12%] [G loss: 1.609116]\n",
      "epoch:30 step:28477 [D loss: 0.257084, acc.: 91.41%] [G loss: 0.171211]\n",
      "epoch:30 step:28478 [D loss: 0.304395, acc.: 90.62%] [G loss: 1.412037]\n",
      "epoch:30 step:28479 [D loss: 0.306738, acc.: 89.84%] [G loss: 1.203377]\n",
      "epoch:30 step:28480 [D loss: 0.142513, acc.: 97.66%] [G loss: 1.458144]\n",
      "epoch:30 step:28481 [D loss: 0.038793, acc.: 100.00%] [G loss: 0.543790]\n",
      "epoch:30 step:28482 [D loss: 0.622731, acc.: 65.62%] [G loss: 3.114880]\n",
      "epoch:30 step:28483 [D loss: 0.315344, acc.: 85.94%] [G loss: 1.841712]\n",
      "epoch:30 step:28484 [D loss: 0.581505, acc.: 63.28%] [G loss: 1.895789]\n",
      "epoch:30 step:28485 [D loss: 0.170490, acc.: 94.53%] [G loss: 1.880940]\n",
      "epoch:30 step:28486 [D loss: 0.987974, acc.: 38.28%] [G loss: 3.236993]\n",
      "epoch:30 step:28487 [D loss: 0.947933, acc.: 45.31%] [G loss: 2.442780]\n",
      "epoch:30 step:28488 [D loss: 0.141157, acc.: 98.44%] [G loss: 1.917588]\n",
      "epoch:30 step:28489 [D loss: 0.538319, acc.: 78.91%] [G loss: 2.940859]\n",
      "epoch:30 step:28490 [D loss: 1.004062, acc.: 50.78%] [G loss: 0.985044]\n",
      "epoch:30 step:28491 [D loss: 0.198481, acc.: 96.88%] [G loss: 2.183984]\n",
      "epoch:30 step:28492 [D loss: 0.186157, acc.: 94.53%] [G loss: 2.212485]\n",
      "epoch:30 step:28493 [D loss: 0.100218, acc.: 97.66%] [G loss: 2.215946]\n",
      "epoch:30 step:28494 [D loss: 0.309566, acc.: 94.53%] [G loss: 0.725002]\n",
      "epoch:30 step:28495 [D loss: 0.316406, acc.: 89.84%] [G loss: 4.633242]\n",
      "epoch:30 step:28496 [D loss: 0.320392, acc.: 86.72%] [G loss: 2.215640]\n",
      "epoch:30 step:28497 [D loss: 0.391944, acc.: 84.38%] [G loss: 0.983739]\n",
      "epoch:30 step:28498 [D loss: 0.272872, acc.: 92.19%] [G loss: 1.134016]\n",
      "epoch:30 step:28499 [D loss: 0.222513, acc.: 92.19%] [G loss: 3.484553]\n",
      "epoch:30 step:28500 [D loss: 0.296189, acc.: 87.50%] [G loss: 3.805291]\n",
      "epoch:30 step:28501 [D loss: 0.427336, acc.: 78.91%] [G loss: 1.216519]\n",
      "epoch:30 step:28502 [D loss: 0.089380, acc.: 97.66%] [G loss: 0.387692]\n",
      "epoch:30 step:28503 [D loss: 0.608032, acc.: 62.50%] [G loss: 1.969086]\n",
      "epoch:30 step:28504 [D loss: 1.001363, acc.: 46.09%] [G loss: 1.838847]\n",
      "epoch:30 step:28505 [D loss: 0.189792, acc.: 95.31%] [G loss: 1.487828]\n",
      "epoch:30 step:28506 [D loss: 0.529120, acc.: 75.00%] [G loss: 0.597871]\n",
      "epoch:30 step:28507 [D loss: 0.710696, acc.: 60.94%] [G loss: 2.270856]\n",
      "epoch:30 step:28508 [D loss: 0.339913, acc.: 88.28%] [G loss: 2.084802]\n",
      "epoch:30 step:28509 [D loss: 0.126791, acc.: 97.66%] [G loss: 0.566409]\n",
      "epoch:30 step:28510 [D loss: 0.200199, acc.: 97.66%] [G loss: 1.044188]\n",
      "epoch:30 step:28511 [D loss: 0.387388, acc.: 79.69%] [G loss: 0.404025]\n",
      "epoch:30 step:28512 [D loss: 0.359173, acc.: 80.47%] [G loss: 1.122136]\n",
      "epoch:30 step:28513 [D loss: 0.182282, acc.: 94.53%] [G loss: 3.262820]\n",
      "epoch:30 step:28514 [D loss: 0.951920, acc.: 38.28%] [G loss: 3.319249]\n",
      "epoch:30 step:28515 [D loss: 0.369626, acc.: 82.81%] [G loss: 2.627108]\n",
      "epoch:30 step:28516 [D loss: 0.274389, acc.: 89.84%] [G loss: 2.314349]\n",
      "epoch:30 step:28517 [D loss: 0.061395, acc.: 97.66%] [G loss: 2.624999]\n",
      "epoch:30 step:28518 [D loss: 0.241965, acc.: 94.53%] [G loss: 0.897340]\n",
      "epoch:30 step:28519 [D loss: 0.184202, acc.: 96.09%] [G loss: 0.839884]\n",
      "epoch:30 step:28520 [D loss: 0.945414, acc.: 55.47%] [G loss: 2.768365]\n",
      "epoch:30 step:28521 [D loss: 0.123072, acc.: 99.22%] [G loss: 3.153460]\n",
      "epoch:30 step:28522 [D loss: 0.590354, acc.: 71.09%] [G loss: 6.260229]\n",
      "epoch:30 step:28523 [D loss: 0.806015, acc.: 60.94%] [G loss: 1.653953]\n",
      "epoch:30 step:28524 [D loss: 0.571033, acc.: 67.19%] [G loss: 4.460897]\n",
      "epoch:30 step:28525 [D loss: 0.190381, acc.: 96.09%] [G loss: 3.130270]\n",
      "epoch:30 step:28526 [D loss: 0.212294, acc.: 94.53%] [G loss: 2.586347]\n",
      "epoch:30 step:28527 [D loss: 0.551444, acc.: 71.09%] [G loss: 2.058133]\n",
      "epoch:30 step:28528 [D loss: 0.182917, acc.: 96.09%] [G loss: 2.118581]\n",
      "epoch:30 step:28529 [D loss: 0.103918, acc.: 97.66%] [G loss: 2.960262]\n",
      "epoch:30 step:28530 [D loss: 0.465996, acc.: 75.00%] [G loss: 4.286236]\n",
      "epoch:30 step:28531 [D loss: 0.481675, acc.: 71.88%] [G loss: 1.408069]\n",
      "epoch:30 step:28532 [D loss: 0.307677, acc.: 89.06%] [G loss: 1.009292]\n",
      "epoch:30 step:28533 [D loss: 0.262796, acc.: 90.62%] [G loss: 2.300430]\n",
      "epoch:30 step:28534 [D loss: 0.213335, acc.: 94.53%] [G loss: 3.152219]\n",
      "epoch:30 step:28535 [D loss: 0.277445, acc.: 91.41%] [G loss: 3.952441]\n",
      "epoch:30 step:28536 [D loss: 0.063082, acc.: 97.66%] [G loss: 4.553727]\n",
      "epoch:30 step:28537 [D loss: 0.085937, acc.: 99.22%] [G loss: 3.218562]\n",
      "epoch:30 step:28538 [D loss: 0.026121, acc.: 99.22%] [G loss: 2.443393]\n",
      "epoch:30 step:28539 [D loss: 0.383613, acc.: 78.91%] [G loss: 5.296457]\n",
      "epoch:30 step:28540 [D loss: 0.200955, acc.: 95.31%] [G loss: 2.828072]\n",
      "epoch:30 step:28541 [D loss: 0.272246, acc.: 94.53%] [G loss: 5.836817]\n",
      "epoch:30 step:28542 [D loss: 0.388926, acc.: 77.34%] [G loss: 2.330626]\n",
      "epoch:30 step:28543 [D loss: 0.429763, acc.: 76.56%] [G loss: 5.419593]\n",
      "epoch:30 step:28544 [D loss: 0.904083, acc.: 50.78%] [G loss: 2.122891]\n",
      "epoch:30 step:28545 [D loss: 0.745393, acc.: 64.06%] [G loss: 3.129038]\n",
      "epoch:30 step:28546 [D loss: 0.282022, acc.: 87.50%] [G loss: 4.453113]\n",
      "epoch:30 step:28547 [D loss: 0.784873, acc.: 58.59%] [G loss: 0.325836]\n",
      "epoch:30 step:28548 [D loss: 0.231845, acc.: 96.09%] [G loss: 1.586130]\n",
      "epoch:30 step:28549 [D loss: 0.100907, acc.: 96.88%] [G loss: 1.448255]\n",
      "epoch:30 step:28550 [D loss: 0.339193, acc.: 82.81%] [G loss: 1.916082]\n",
      "epoch:30 step:28551 [D loss: 0.407380, acc.: 76.56%] [G loss: 2.191725]\n",
      "epoch:30 step:28552 [D loss: 0.328365, acc.: 84.38%] [G loss: 1.888076]\n",
      "epoch:30 step:28553 [D loss: 0.464890, acc.: 70.31%] [G loss: 2.407707]\n",
      "epoch:30 step:28554 [D loss: 0.360791, acc.: 86.72%] [G loss: 4.229445]\n",
      "epoch:30 step:28555 [D loss: 0.178914, acc.: 93.75%] [G loss: 2.247533]\n",
      "epoch:30 step:28556 [D loss: 1.118212, acc.: 55.47%] [G loss: 0.985391]\n",
      "epoch:30 step:28557 [D loss: 0.338657, acc.: 89.84%] [G loss: 2.454418]\n",
      "epoch:30 step:28558 [D loss: 1.332720, acc.: 28.91%] [G loss: 0.420401]\n",
      "epoch:30 step:28559 [D loss: 0.206480, acc.: 93.75%] [G loss: 2.547172]\n",
      "epoch:30 step:28560 [D loss: 0.052036, acc.: 100.00%] [G loss: 4.341619]\n",
      "epoch:30 step:28561 [D loss: 0.596825, acc.: 64.84%] [G loss: 1.932056]\n",
      "epoch:30 step:28562 [D loss: 0.942419, acc.: 55.47%] [G loss: 1.529797]\n",
      "epoch:30 step:28563 [D loss: 0.595976, acc.: 74.22%] [G loss: 2.058352]\n",
      "epoch:30 step:28564 [D loss: 0.453833, acc.: 83.59%] [G loss: 0.856183]\n",
      "epoch:30 step:28565 [D loss: 0.224209, acc.: 92.97%] [G loss: 3.189506]\n",
      "epoch:30 step:28566 [D loss: 0.414720, acc.: 77.34%] [G loss: 5.000086]\n",
      "epoch:30 step:28567 [D loss: 0.236043, acc.: 92.97%] [G loss: 2.300219]\n",
      "epoch:30 step:28568 [D loss: 0.092248, acc.: 98.44%] [G loss: 2.523408]\n",
      "epoch:30 step:28569 [D loss: 0.149289, acc.: 96.88%] [G loss: 3.218590]\n",
      "epoch:30 step:28570 [D loss: 0.680493, acc.: 61.72%] [G loss: 4.746929]\n",
      "epoch:30 step:28571 [D loss: 0.625046, acc.: 66.41%] [G loss: 3.244482]\n",
      "epoch:30 step:28572 [D loss: 0.059161, acc.: 99.22%] [G loss: 0.843539]\n",
      "epoch:30 step:28573 [D loss: 0.115280, acc.: 98.44%] [G loss: 1.420945]\n",
      "epoch:30 step:28574 [D loss: 0.204493, acc.: 97.66%] [G loss: 3.037683]\n",
      "epoch:30 step:28575 [D loss: 0.291245, acc.: 88.28%] [G loss: 3.275153]\n",
      "epoch:30 step:28576 [D loss: 0.089087, acc.: 100.00%] [G loss: 2.984304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28577 [D loss: 0.384860, acc.: 85.16%] [G loss: 3.229926]\n",
      "epoch:30 step:28578 [D loss: 0.166315, acc.: 97.66%] [G loss: 4.158896]\n",
      "epoch:30 step:28579 [D loss: 0.038681, acc.: 99.22%] [G loss: 5.117205]\n",
      "epoch:30 step:28580 [D loss: 0.036865, acc.: 100.00%] [G loss: 2.330986]\n",
      "epoch:30 step:28581 [D loss: 0.407879, acc.: 82.03%] [G loss: 4.086238]\n",
      "epoch:30 step:28582 [D loss: 0.427840, acc.: 84.38%] [G loss: 1.297068]\n",
      "epoch:30 step:28583 [D loss: 0.334602, acc.: 84.38%] [G loss: 3.690388]\n",
      "epoch:30 step:28584 [D loss: 0.161640, acc.: 96.09%] [G loss: 1.243544]\n",
      "epoch:30 step:28585 [D loss: 0.114706, acc.: 99.22%] [G loss: 3.250159]\n",
      "epoch:30 step:28586 [D loss: 0.125164, acc.: 96.88%] [G loss: 1.422745]\n",
      "epoch:30 step:28587 [D loss: 0.049575, acc.: 99.22%] [G loss: 5.132874]\n",
      "epoch:30 step:28588 [D loss: 0.198417, acc.: 97.66%] [G loss: 3.054930]\n",
      "epoch:30 step:28589 [D loss: 0.216367, acc.: 95.31%] [G loss: 1.221538]\n",
      "epoch:30 step:28590 [D loss: 1.309019, acc.: 21.09%] [G loss: 2.637668]\n",
      "epoch:30 step:28591 [D loss: 0.110497, acc.: 99.22%] [G loss: 3.476804]\n",
      "epoch:30 step:28592 [D loss: 0.133223, acc.: 97.66%] [G loss: 2.277198]\n",
      "epoch:30 step:28593 [D loss: 0.962031, acc.: 49.22%] [G loss: 2.420330]\n",
      "epoch:30 step:28594 [D loss: 0.334773, acc.: 84.38%] [G loss: 1.922465]\n",
      "epoch:30 step:28595 [D loss: 0.075895, acc.: 98.44%] [G loss: 4.609962]\n",
      "epoch:30 step:28596 [D loss: 0.182432, acc.: 96.09%] [G loss: 1.046407]\n",
      "epoch:30 step:28597 [D loss: 0.204917, acc.: 96.09%] [G loss: 1.459181]\n",
      "epoch:30 step:28598 [D loss: 0.015429, acc.: 100.00%] [G loss: 3.150589]\n",
      "epoch:30 step:28599 [D loss: 0.932242, acc.: 49.22%] [G loss: 3.334767]\n",
      "epoch:30 step:28600 [D loss: 0.146441, acc.: 99.22%] [G loss: 3.845963]\n",
      "epoch:30 step:28601 [D loss: 2.155215, acc.: 33.59%] [G loss: 1.356050]\n",
      "epoch:30 step:28602 [D loss: 0.368365, acc.: 89.06%] [G loss: 1.166707]\n",
      "epoch:30 step:28603 [D loss: 0.252496, acc.: 89.06%] [G loss: 2.305807]\n",
      "epoch:30 step:28604 [D loss: 0.100486, acc.: 98.44%] [G loss: 3.422100]\n",
      "epoch:30 step:28605 [D loss: 0.420504, acc.: 80.47%] [G loss: 3.360209]\n",
      "epoch:30 step:28606 [D loss: 0.176618, acc.: 95.31%] [G loss: 2.172640]\n",
      "epoch:30 step:28607 [D loss: 0.690416, acc.: 57.81%] [G loss: 2.327433]\n",
      "epoch:30 step:28608 [D loss: 0.147661, acc.: 96.88%] [G loss: 3.090259]\n",
      "epoch:30 step:28609 [D loss: 0.154399, acc.: 97.66%] [G loss: 1.042567]\n",
      "epoch:30 step:28610 [D loss: 0.818628, acc.: 55.47%] [G loss: 1.951088]\n",
      "epoch:30 step:28611 [D loss: 0.170481, acc.: 98.44%] [G loss: 3.405914]\n",
      "epoch:30 step:28612 [D loss: 0.640623, acc.: 71.88%] [G loss: 2.325525]\n",
      "epoch:30 step:28613 [D loss: 0.746121, acc.: 59.38%] [G loss: 2.220575]\n",
      "epoch:30 step:28614 [D loss: 0.507536, acc.: 70.31%] [G loss: 4.303097]\n",
      "epoch:30 step:28615 [D loss: 0.414906, acc.: 85.16%] [G loss: 1.568459]\n",
      "epoch:30 step:28616 [D loss: 0.556713, acc.: 69.53%] [G loss: 2.270248]\n",
      "epoch:30 step:28617 [D loss: 0.577864, acc.: 66.41%] [G loss: 1.824296]\n",
      "epoch:30 step:28618 [D loss: 0.079830, acc.: 99.22%] [G loss: 4.053847]\n",
      "epoch:30 step:28619 [D loss: 0.129543, acc.: 98.44%] [G loss: 2.322950]\n",
      "epoch:30 step:28620 [D loss: 0.509642, acc.: 85.16%] [G loss: 1.886024]\n",
      "epoch:30 step:28621 [D loss: 0.364286, acc.: 79.69%] [G loss: 1.813737]\n",
      "epoch:30 step:28622 [D loss: 0.305519, acc.: 90.62%] [G loss: 2.944139]\n",
      "epoch:30 step:28623 [D loss: 0.080108, acc.: 100.00%] [G loss: 5.050385]\n",
      "epoch:30 step:28624 [D loss: 0.644940, acc.: 59.38%] [G loss: 2.155684]\n",
      "epoch:30 step:28625 [D loss: 0.233817, acc.: 89.84%] [G loss: 4.276359]\n",
      "epoch:30 step:28626 [D loss: 0.456391, acc.: 78.91%] [G loss: 3.177136]\n",
      "epoch:30 step:28627 [D loss: 0.189933, acc.: 96.09%] [G loss: 1.389593]\n",
      "epoch:30 step:28628 [D loss: 0.415572, acc.: 78.12%] [G loss: 2.126376]\n",
      "epoch:30 step:28629 [D loss: 0.331437, acc.: 88.28%] [G loss: 1.499670]\n",
      "epoch:30 step:28630 [D loss: 0.994122, acc.: 51.56%] [G loss: 2.287434]\n",
      "epoch:30 step:28631 [D loss: 0.215143, acc.: 93.75%] [G loss: 3.320252]\n",
      "epoch:30 step:28632 [D loss: 0.163976, acc.: 97.66%] [G loss: 2.024754]\n",
      "epoch:30 step:28633 [D loss: 0.256855, acc.: 91.41%] [G loss: 3.038380]\n",
      "epoch:30 step:28634 [D loss: 0.351302, acc.: 86.72%] [G loss: 4.594137]\n",
      "epoch:30 step:28635 [D loss: 0.161823, acc.: 97.66%] [G loss: 3.935727]\n",
      "epoch:30 step:28636 [D loss: 0.170855, acc.: 98.44%] [G loss: 3.381469]\n",
      "epoch:30 step:28637 [D loss: 0.414025, acc.: 76.56%] [G loss: 1.760923]\n",
      "epoch:30 step:28638 [D loss: 0.696734, acc.: 60.94%] [G loss: 0.348283]\n",
      "epoch:30 step:28639 [D loss: 1.202764, acc.: 53.91%] [G loss: 0.619867]\n",
      "epoch:30 step:28640 [D loss: 0.157158, acc.: 97.66%] [G loss: 4.587618]\n",
      "epoch:30 step:28641 [D loss: 0.382479, acc.: 79.69%] [G loss: 4.102180]\n",
      "epoch:30 step:28642 [D loss: 0.251047, acc.: 92.19%] [G loss: 3.287672]\n",
      "epoch:30 step:28643 [D loss: 0.060342, acc.: 99.22%] [G loss: 5.878087]\n",
      "epoch:30 step:28644 [D loss: 0.189326, acc.: 96.88%] [G loss: 3.232710]\n",
      "epoch:30 step:28645 [D loss: 0.363509, acc.: 85.16%] [G loss: 5.029146]\n",
      "epoch:30 step:28646 [D loss: 0.381685, acc.: 82.03%] [G loss: 3.831852]\n",
      "epoch:30 step:28647 [D loss: 0.484521, acc.: 77.34%] [G loss: 2.729121]\n",
      "epoch:30 step:28648 [D loss: 0.082844, acc.: 98.44%] [G loss: 5.776031]\n",
      "epoch:30 step:28649 [D loss: 0.208763, acc.: 96.88%] [G loss: 4.406767]\n",
      "epoch:30 step:28650 [D loss: 0.113125, acc.: 99.22%] [G loss: 3.518501]\n",
      "epoch:30 step:28651 [D loss: 0.585140, acc.: 63.28%] [G loss: 3.640978]\n",
      "epoch:30 step:28652 [D loss: 0.344411, acc.: 86.72%] [G loss: 0.773914]\n",
      "epoch:30 step:28653 [D loss: 0.013023, acc.: 100.00%] [G loss: 1.665860]\n",
      "epoch:30 step:28654 [D loss: 0.084585, acc.: 99.22%] [G loss: 1.322482]\n",
      "epoch:30 step:28655 [D loss: 0.529220, acc.: 71.88%] [G loss: 1.801975]\n",
      "epoch:30 step:28656 [D loss: 0.090484, acc.: 99.22%] [G loss: 3.386448]\n",
      "epoch:30 step:28657 [D loss: 0.461205, acc.: 79.69%] [G loss: 3.714188]\n",
      "epoch:30 step:28658 [D loss: 0.553620, acc.: 71.88%] [G loss: 0.304863]\n",
      "epoch:30 step:28659 [D loss: 0.246025, acc.: 96.09%] [G loss: 3.237618]\n",
      "epoch:30 step:28660 [D loss: 0.080774, acc.: 99.22%] [G loss: 1.796501]\n",
      "epoch:30 step:28661 [D loss: 0.546535, acc.: 73.44%] [G loss: 4.306586]\n",
      "epoch:30 step:28662 [D loss: 1.200681, acc.: 35.94%] [G loss: 4.757154]\n",
      "epoch:30 step:28663 [D loss: 0.078253, acc.: 98.44%] [G loss: 2.542898]\n",
      "epoch:30 step:28664 [D loss: 1.166355, acc.: 39.06%] [G loss: 4.182622]\n",
      "epoch:30 step:28665 [D loss: 0.079474, acc.: 99.22%] [G loss: 4.460069]\n",
      "epoch:30 step:28666 [D loss: 0.935106, acc.: 53.91%] [G loss: 2.292946]\n",
      "epoch:30 step:28667 [D loss: 0.557376, acc.: 73.44%] [G loss: 0.651307]\n",
      "epoch:30 step:28668 [D loss: 0.255421, acc.: 90.62%] [G loss: 0.955364]\n",
      "epoch:30 step:28669 [D loss: 0.219747, acc.: 91.41%] [G loss: 3.377053]\n",
      "epoch:30 step:28670 [D loss: 0.044579, acc.: 100.00%] [G loss: 0.730276]\n",
      "epoch:30 step:28671 [D loss: 0.244000, acc.: 91.41%] [G loss: 1.118305]\n",
      "epoch:30 step:28672 [D loss: 0.049301, acc.: 100.00%] [G loss: 2.211246]\n",
      "epoch:30 step:28673 [D loss: 0.266884, acc.: 93.75%] [G loss: 4.232615]\n",
      "epoch:30 step:28674 [D loss: 0.133700, acc.: 97.66%] [G loss: 3.400029]\n",
      "epoch:30 step:28675 [D loss: 0.215781, acc.: 93.75%] [G loss: 3.283972]\n",
      "epoch:30 step:28676 [D loss: 0.117017, acc.: 96.88%] [G loss: 3.394600]\n",
      "epoch:30 step:28677 [D loss: 0.387520, acc.: 78.12%] [G loss: 4.453473]\n",
      "epoch:30 step:28678 [D loss: 0.074541, acc.: 99.22%] [G loss: 1.782100]\n",
      "epoch:30 step:28679 [D loss: 0.080505, acc.: 98.44%] [G loss: 3.728360]\n",
      "epoch:30 step:28680 [D loss: 0.718682, acc.: 55.47%] [G loss: 1.239933]\n",
      "epoch:30 step:28681 [D loss: 0.075150, acc.: 100.00%] [G loss: 1.714374]\n",
      "epoch:30 step:28682 [D loss: 0.335924, acc.: 89.84%] [G loss: 2.018896]\n",
      "epoch:30 step:28683 [D loss: 0.157355, acc.: 96.88%] [G loss: 1.682819]\n",
      "epoch:30 step:28684 [D loss: 1.205689, acc.: 37.50%] [G loss: 1.509182]\n",
      "epoch:30 step:28685 [D loss: 0.191312, acc.: 94.53%] [G loss: 2.287018]\n",
      "epoch:30 step:28686 [D loss: 0.054634, acc.: 99.22%] [G loss: 4.692564]\n",
      "epoch:30 step:28687 [D loss: 0.243619, acc.: 92.19%] [G loss: 1.559821]\n",
      "epoch:30 step:28688 [D loss: 0.259739, acc.: 92.97%] [G loss: 1.126507]\n",
      "epoch:30 step:28689 [D loss: 0.320949, acc.: 89.06%] [G loss: 4.060433]\n",
      "epoch:30 step:28690 [D loss: 0.066419, acc.: 100.00%] [G loss: 1.998191]\n",
      "epoch:30 step:28691 [D loss: 0.830405, acc.: 59.38%] [G loss: 3.480888]\n",
      "epoch:30 step:28692 [D loss: 0.119698, acc.: 96.88%] [G loss: 1.164004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28693 [D loss: 0.867357, acc.: 57.81%] [G loss: 3.380510]\n",
      "epoch:30 step:28694 [D loss: 0.242829, acc.: 96.09%] [G loss: 2.102653]\n",
      "epoch:30 step:28695 [D loss: 0.237695, acc.: 88.28%] [G loss: 1.521050]\n",
      "epoch:30 step:28696 [D loss: 0.066021, acc.: 99.22%] [G loss: 2.544043]\n",
      "epoch:30 step:28697 [D loss: 0.346646, acc.: 85.16%] [G loss: 1.544191]\n",
      "epoch:30 step:28698 [D loss: 0.217996, acc.: 92.97%] [G loss: 4.282768]\n",
      "epoch:30 step:28699 [D loss: 0.465727, acc.: 77.34%] [G loss: 1.835139]\n",
      "epoch:30 step:28700 [D loss: 1.649583, acc.: 20.31%] [G loss: 3.420453]\n",
      "epoch:30 step:28701 [D loss: 0.093535, acc.: 100.00%] [G loss: 1.586336]\n",
      "epoch:30 step:28702 [D loss: 0.326826, acc.: 89.84%] [G loss: 1.872085]\n",
      "epoch:30 step:28703 [D loss: 0.170162, acc.: 97.66%] [G loss: 3.918878]\n",
      "epoch:30 step:28704 [D loss: 0.156128, acc.: 96.88%] [G loss: 2.685031]\n",
      "epoch:30 step:28705 [D loss: 0.179857, acc.: 96.88%] [G loss: 1.655482]\n",
      "epoch:30 step:28706 [D loss: 0.331895, acc.: 89.84%] [G loss: 4.312818]\n",
      "epoch:30 step:28707 [D loss: 0.360024, acc.: 89.84%] [G loss: 5.370472]\n",
      "epoch:30 step:28708 [D loss: 0.759883, acc.: 58.59%] [G loss: 1.438891]\n",
      "epoch:30 step:28709 [D loss: 0.077638, acc.: 98.44%] [G loss: 1.603441]\n",
      "epoch:30 step:28710 [D loss: 0.404297, acc.: 77.34%] [G loss: 3.311022]\n",
      "epoch:30 step:28711 [D loss: 0.128761, acc.: 99.22%] [G loss: 2.631234]\n",
      "epoch:30 step:28712 [D loss: 0.046200, acc.: 99.22%] [G loss: 3.348656]\n",
      "epoch:30 step:28713 [D loss: 0.264089, acc.: 89.06%] [G loss: 1.515031]\n",
      "epoch:30 step:28714 [D loss: 0.596586, acc.: 66.41%] [G loss: 3.123962]\n",
      "epoch:30 step:28715 [D loss: 0.296446, acc.: 85.16%] [G loss: 3.398325]\n",
      "epoch:30 step:28716 [D loss: 0.698259, acc.: 60.94%] [G loss: 3.194843]\n",
      "epoch:30 step:28717 [D loss: 0.050367, acc.: 100.00%] [G loss: 9.919807]\n",
      "epoch:30 step:28718 [D loss: 1.014821, acc.: 28.91%] [G loss: 2.149484]\n",
      "epoch:30 step:28719 [D loss: 0.056532, acc.: 99.22%] [G loss: 4.742671]\n",
      "epoch:30 step:28720 [D loss: 0.194248, acc.: 97.66%] [G loss: 5.600019]\n",
      "epoch:30 step:28721 [D loss: 0.461997, acc.: 75.00%] [G loss: 2.159037]\n",
      "epoch:30 step:28722 [D loss: 0.838638, acc.: 46.09%] [G loss: 0.242340]\n",
      "epoch:30 step:28723 [D loss: 0.362426, acc.: 82.03%] [G loss: 4.384880]\n",
      "epoch:30 step:28724 [D loss: 0.068949, acc.: 98.44%] [G loss: 1.378137]\n",
      "epoch:30 step:28725 [D loss: 0.175275, acc.: 97.66%] [G loss: 4.821182]\n",
      "epoch:30 step:28726 [D loss: 0.251914, acc.: 90.62%] [G loss: 4.712582]\n",
      "epoch:30 step:28727 [D loss: 0.288076, acc.: 89.84%] [G loss: 1.986316]\n",
      "epoch:30 step:28728 [D loss: 0.150212, acc.: 96.09%] [G loss: 1.693085]\n",
      "epoch:30 step:28729 [D loss: 0.084848, acc.: 97.66%] [G loss: 2.053842]\n",
      "epoch:30 step:28730 [D loss: 0.128686, acc.: 97.66%] [G loss: 1.687246]\n",
      "epoch:30 step:28731 [D loss: 0.673568, acc.: 58.59%] [G loss: 3.843346]\n",
      "epoch:30 step:28732 [D loss: 0.519865, acc.: 71.88%] [G loss: 2.928450]\n",
      "epoch:30 step:28733 [D loss: 0.921870, acc.: 60.16%] [G loss: 4.514213]\n",
      "epoch:30 step:28734 [D loss: 0.481048, acc.: 77.34%] [G loss: 3.487164]\n",
      "epoch:30 step:28735 [D loss: 0.062656, acc.: 99.22%] [G loss: 3.633674]\n",
      "epoch:30 step:28736 [D loss: 0.208141, acc.: 92.19%] [G loss: 1.332426]\n",
      "epoch:30 step:28737 [D loss: 0.226177, acc.: 93.75%] [G loss: 2.250724]\n",
      "epoch:30 step:28738 [D loss: 0.235273, acc.: 92.19%] [G loss: 4.822604]\n",
      "epoch:30 step:28739 [D loss: 0.339633, acc.: 85.94%] [G loss: 3.343566]\n",
      "epoch:30 step:28740 [D loss: 0.247059, acc.: 93.75%] [G loss: 3.794193]\n",
      "epoch:30 step:28741 [D loss: 0.357799, acc.: 81.25%] [G loss: 1.088238]\n",
      "epoch:30 step:28742 [D loss: 0.081451, acc.: 100.00%] [G loss: 2.465525]\n",
      "epoch:30 step:28743 [D loss: 0.815123, acc.: 56.25%] [G loss: 1.629438]\n",
      "epoch:30 step:28744 [D loss: 0.738688, acc.: 56.25%] [G loss: 5.088794]\n",
      "epoch:30 step:28745 [D loss: 0.811128, acc.: 61.72%] [G loss: 3.048818]\n",
      "epoch:30 step:28746 [D loss: 0.080709, acc.: 99.22%] [G loss: 3.912371]\n",
      "epoch:30 step:28747 [D loss: 0.252808, acc.: 92.97%] [G loss: 4.870700]\n",
      "epoch:30 step:28748 [D loss: 0.401114, acc.: 84.38%] [G loss: 2.748254]\n",
      "epoch:30 step:28749 [D loss: 0.283379, acc.: 82.81%] [G loss: 3.351216]\n",
      "epoch:30 step:28750 [D loss: 0.115622, acc.: 98.44%] [G loss: 4.022391]\n",
      "epoch:30 step:28751 [D loss: 0.149173, acc.: 97.66%] [G loss: 1.609815]\n",
      "epoch:30 step:28752 [D loss: 0.204893, acc.: 96.09%] [G loss: 2.579357]\n",
      "epoch:30 step:28753 [D loss: 0.169762, acc.: 96.88%] [G loss: 1.586490]\n",
      "epoch:30 step:28754 [D loss: 0.049114, acc.: 99.22%] [G loss: 3.217638]\n",
      "epoch:30 step:28755 [D loss: 0.278872, acc.: 90.62%] [G loss: 3.393356]\n",
      "epoch:30 step:28756 [D loss: 0.091611, acc.: 97.66%] [G loss: 4.827248]\n",
      "epoch:30 step:28757 [D loss: 0.032602, acc.: 100.00%] [G loss: 3.709675]\n",
      "epoch:30 step:28758 [D loss: 0.395232, acc.: 86.72%] [G loss: 4.205163]\n",
      "epoch:30 step:28759 [D loss: 0.413528, acc.: 77.34%] [G loss: 2.054884]\n",
      "epoch:30 step:28760 [D loss: 0.382077, acc.: 78.12%] [G loss: 2.763753]\n",
      "epoch:30 step:28761 [D loss: 0.776340, acc.: 53.91%] [G loss: 3.417122]\n",
      "epoch:30 step:28762 [D loss: 0.477438, acc.: 87.50%] [G loss: 3.428428]\n",
      "epoch:30 step:28763 [D loss: 0.123173, acc.: 97.66%] [G loss: 1.962228]\n",
      "epoch:30 step:28764 [D loss: 0.343748, acc.: 81.25%] [G loss: 4.262891]\n",
      "epoch:30 step:28765 [D loss: 0.254777, acc.: 94.53%] [G loss: 2.521869]\n",
      "epoch:30 step:28766 [D loss: 0.144864, acc.: 95.31%] [G loss: 2.282456]\n",
      "epoch:30 step:28767 [D loss: 0.458538, acc.: 78.91%] [G loss: 5.374741]\n",
      "epoch:30 step:28768 [D loss: 0.900249, acc.: 43.75%] [G loss: 1.115744]\n",
      "epoch:30 step:28769 [D loss: 0.030077, acc.: 99.22%] [G loss: 1.375362]\n",
      "epoch:30 step:28770 [D loss: 0.233942, acc.: 91.41%] [G loss: 2.019291]\n",
      "epoch:30 step:28771 [D loss: 0.175896, acc.: 96.88%] [G loss: 4.420961]\n",
      "epoch:30 step:28772 [D loss: 0.435045, acc.: 78.12%] [G loss: 3.840164]\n",
      "epoch:30 step:28773 [D loss: 0.103452, acc.: 96.88%] [G loss: 6.493149]\n",
      "epoch:30 step:28774 [D loss: 0.363113, acc.: 76.56%] [G loss: 2.251871]\n",
      "epoch:30 step:28775 [D loss: 0.296678, acc.: 93.75%] [G loss: 4.284489]\n",
      "epoch:30 step:28776 [D loss: 0.577067, acc.: 67.19%] [G loss: 3.179117]\n",
      "epoch:30 step:28777 [D loss: 0.375174, acc.: 85.16%] [G loss: 4.576017]\n",
      "epoch:30 step:28778 [D loss: 0.592179, acc.: 67.19%] [G loss: 1.511790]\n",
      "epoch:30 step:28779 [D loss: 0.084671, acc.: 99.22%] [G loss: 1.888279]\n",
      "epoch:30 step:28780 [D loss: 0.378103, acc.: 75.78%] [G loss: 3.737921]\n",
      "epoch:30 step:28781 [D loss: 0.479432, acc.: 78.12%] [G loss: 2.710953]\n",
      "epoch:30 step:28782 [D loss: 0.165044, acc.: 94.53%] [G loss: 1.322024]\n",
      "epoch:30 step:28783 [D loss: 0.213042, acc.: 93.75%] [G loss: 1.912513]\n",
      "epoch:30 step:28784 [D loss: 0.094671, acc.: 98.44%] [G loss: 1.075626]\n",
      "epoch:30 step:28785 [D loss: 0.415633, acc.: 78.91%] [G loss: 2.105237]\n",
      "epoch:30 step:28786 [D loss: 0.454296, acc.: 71.09%] [G loss: 5.072657]\n",
      "epoch:30 step:28787 [D loss: 0.319383, acc.: 83.59%] [G loss: 4.254238]\n",
      "epoch:30 step:28788 [D loss: 0.221921, acc.: 89.84%] [G loss: 5.095677]\n",
      "epoch:30 step:28789 [D loss: 0.138515, acc.: 94.53%] [G loss: 1.447527]\n",
      "epoch:30 step:28790 [D loss: 0.068525, acc.: 100.00%] [G loss: 2.072475]\n",
      "epoch:30 step:28791 [D loss: 0.253298, acc.: 92.97%] [G loss: 1.276523]\n",
      "epoch:30 step:28792 [D loss: 0.290354, acc.: 89.84%] [G loss: 1.821901]\n",
      "epoch:30 step:28793 [D loss: 0.184298, acc.: 93.75%] [G loss: 5.673986]\n",
      "epoch:30 step:28794 [D loss: 0.546782, acc.: 72.66%] [G loss: 2.132947]\n",
      "epoch:30 step:28795 [D loss: 0.440847, acc.: 74.22%] [G loss: 2.080994]\n",
      "epoch:30 step:28796 [D loss: 0.319797, acc.: 92.97%] [G loss: 2.083231]\n",
      "epoch:30 step:28797 [D loss: 0.413510, acc.: 76.56%] [G loss: 2.619040]\n",
      "epoch:30 step:28798 [D loss: 0.783154, acc.: 60.16%] [G loss: 2.577309]\n",
      "epoch:30 step:28799 [D loss: 0.628682, acc.: 66.41%] [G loss: 4.439094]\n",
      "epoch:30 step:28800 [D loss: 0.099009, acc.: 99.22%] [G loss: 4.065776]\n",
      "epoch:30 step:28801 [D loss: 0.089809, acc.: 97.66%] [G loss: 7.224523]\n",
      "epoch:30 step:28802 [D loss: 1.540095, acc.: 49.22%] [G loss: 1.738000]\n",
      "epoch:30 step:28803 [D loss: 0.299245, acc.: 90.62%] [G loss: 1.147698]\n",
      "epoch:30 step:28804 [D loss: 1.385065, acc.: 52.34%] [G loss: 2.660275]\n",
      "epoch:30 step:28805 [D loss: 0.362714, acc.: 86.72%] [G loss: 2.173001]\n",
      "epoch:30 step:28806 [D loss: 0.296110, acc.: 83.59%] [G loss: 2.726053]\n",
      "epoch:30 step:28807 [D loss: 0.424779, acc.: 81.25%] [G loss: 2.943634]\n",
      "epoch:30 step:28808 [D loss: 0.529114, acc.: 69.53%] [G loss: 3.554755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28809 [D loss: 0.207629, acc.: 96.09%] [G loss: 5.481092]\n",
      "epoch:30 step:28810 [D loss: 0.321255, acc.: 83.59%] [G loss: 2.086036]\n",
      "epoch:30 step:28811 [D loss: 0.113465, acc.: 96.88%] [G loss: 4.741735]\n",
      "epoch:30 step:28812 [D loss: 0.562495, acc.: 72.66%] [G loss: 1.636900]\n",
      "epoch:30 step:28813 [D loss: 0.744652, acc.: 55.47%] [G loss: 2.210132]\n",
      "epoch:30 step:28814 [D loss: 0.812382, acc.: 61.72%] [G loss: 3.724950]\n",
      "epoch:30 step:28815 [D loss: 0.381455, acc.: 78.12%] [G loss: 3.320552]\n",
      "epoch:30 step:28816 [D loss: 0.658348, acc.: 67.19%] [G loss: 4.512743]\n",
      "epoch:30 step:28817 [D loss: 0.094082, acc.: 98.44%] [G loss: 2.370799]\n",
      "epoch:30 step:28818 [D loss: 0.293335, acc.: 89.06%] [G loss: 3.058828]\n",
      "epoch:30 step:28819 [D loss: 0.963806, acc.: 56.25%] [G loss: 1.308362]\n",
      "epoch:30 step:28820 [D loss: 0.119809, acc.: 96.88%] [G loss: 0.363834]\n",
      "epoch:30 step:28821 [D loss: 0.470613, acc.: 78.12%] [G loss: 4.263994]\n",
      "epoch:30 step:28822 [D loss: 0.261429, acc.: 90.62%] [G loss: 3.224638]\n",
      "epoch:30 step:28823 [D loss: 0.216939, acc.: 96.88%] [G loss: 3.946627]\n",
      "epoch:30 step:28824 [D loss: 0.060196, acc.: 100.00%] [G loss: 4.031241]\n",
      "epoch:30 step:28825 [D loss: 0.156031, acc.: 97.66%] [G loss: 3.757423]\n",
      "epoch:30 step:28826 [D loss: 0.373151, acc.: 85.94%] [G loss: 3.741457]\n",
      "epoch:30 step:28827 [D loss: 0.223663, acc.: 93.75%] [G loss: 4.030016]\n",
      "epoch:30 step:28828 [D loss: 0.381603, acc.: 83.59%] [G loss: 1.911639]\n",
      "epoch:30 step:28829 [D loss: 0.276903, acc.: 86.72%] [G loss: 3.749644]\n",
      "epoch:30 step:28830 [D loss: 0.418147, acc.: 76.56%] [G loss: 3.541386]\n",
      "epoch:30 step:28831 [D loss: 0.176432, acc.: 95.31%] [G loss: 0.657417]\n",
      "epoch:30 step:28832 [D loss: 0.074512, acc.: 99.22%] [G loss: 1.463076]\n",
      "epoch:30 step:28833 [D loss: 0.344183, acc.: 79.69%] [G loss: 2.251976]\n",
      "epoch:30 step:28834 [D loss: 0.467263, acc.: 79.69%] [G loss: 1.912050]\n",
      "epoch:30 step:28835 [D loss: 0.580151, acc.: 67.97%] [G loss: 1.579776]\n",
      "epoch:30 step:28836 [D loss: 1.046102, acc.: 33.59%] [G loss: 4.033906]\n",
      "epoch:30 step:28837 [D loss: 0.783233, acc.: 57.03%] [G loss: 6.645635]\n",
      "epoch:30 step:28838 [D loss: 0.046241, acc.: 99.22%] [G loss: 5.800529]\n",
      "epoch:30 step:28839 [D loss: 0.758442, acc.: 61.72%] [G loss: 1.847330]\n",
      "epoch:30 step:28840 [D loss: 0.397514, acc.: 72.66%] [G loss: 2.170658]\n",
      "epoch:30 step:28841 [D loss: 0.572539, acc.: 70.31%] [G loss: 2.780225]\n",
      "epoch:30 step:28842 [D loss: 0.269343, acc.: 92.19%] [G loss: 0.536196]\n",
      "epoch:30 step:28843 [D loss: 0.149644, acc.: 96.09%] [G loss: 2.313061]\n",
      "epoch:30 step:28844 [D loss: 0.359359, acc.: 80.47%] [G loss: 0.928763]\n",
      "epoch:30 step:28845 [D loss: 0.504428, acc.: 67.19%] [G loss: 2.540273]\n",
      "epoch:30 step:28846 [D loss: 0.071356, acc.: 99.22%] [G loss: 3.027890]\n",
      "epoch:30 step:28847 [D loss: 0.117558, acc.: 97.66%] [G loss: 3.192528]\n",
      "epoch:30 step:28848 [D loss: 1.037261, acc.: 42.97%] [G loss: 1.283339]\n",
      "epoch:30 step:28849 [D loss: 0.133435, acc.: 100.00%] [G loss: 1.231683]\n",
      "epoch:30 step:28850 [D loss: 0.036571, acc.: 100.00%] [G loss: 3.297104]\n",
      "epoch:30 step:28851 [D loss: 0.748525, acc.: 59.38%] [G loss: 0.522286]\n",
      "epoch:30 step:28852 [D loss: 0.420690, acc.: 82.03%] [G loss: 2.175850]\n",
      "epoch:30 step:28853 [D loss: 0.212257, acc.: 92.97%] [G loss: 2.439332]\n",
      "epoch:30 step:28854 [D loss: 0.024833, acc.: 100.00%] [G loss: 1.355561]\n",
      "epoch:30 step:28855 [D loss: 0.419938, acc.: 82.03%] [G loss: 3.653652]\n",
      "epoch:30 step:28856 [D loss: 0.043171, acc.: 99.22%] [G loss: 0.448846]\n",
      "epoch:30 step:28857 [D loss: 0.622721, acc.: 64.84%] [G loss: 3.814346]\n",
      "epoch:30 step:28858 [D loss: 0.261299, acc.: 94.53%] [G loss: 2.786233]\n",
      "epoch:30 step:28859 [D loss: 0.137397, acc.: 98.44%] [G loss: 3.764713]\n",
      "epoch:30 step:28860 [D loss: 0.229376, acc.: 97.66%] [G loss: 1.889635]\n",
      "epoch:30 step:28861 [D loss: 0.124675, acc.: 97.66%] [G loss: 3.202420]\n",
      "epoch:30 step:28862 [D loss: 0.258454, acc.: 95.31%] [G loss: 3.520418]\n",
      "epoch:30 step:28863 [D loss: 0.235448, acc.: 93.75%] [G loss: 3.118162]\n",
      "epoch:30 step:28864 [D loss: 0.120762, acc.: 97.66%] [G loss: 1.332588]\n",
      "epoch:30 step:28865 [D loss: 0.098558, acc.: 97.66%] [G loss: 0.398093]\n",
      "epoch:30 step:28866 [D loss: 0.098250, acc.: 99.22%] [G loss: 2.863110]\n",
      "epoch:30 step:28867 [D loss: 0.452536, acc.: 77.34%] [G loss: 0.683351]\n",
      "epoch:30 step:28868 [D loss: 0.093315, acc.: 99.22%] [G loss: 0.695915]\n",
      "epoch:30 step:28869 [D loss: 0.678584, acc.: 57.03%] [G loss: 4.772688]\n",
      "epoch:30 step:28870 [D loss: 0.364996, acc.: 77.34%] [G loss: 3.406632]\n",
      "epoch:30 step:28871 [D loss: 0.760812, acc.: 55.47%] [G loss: 5.539161]\n",
      "epoch:30 step:28872 [D loss: 0.620412, acc.: 64.84%] [G loss: 2.743406]\n",
      "epoch:30 step:28873 [D loss: 0.778843, acc.: 46.09%] [G loss: 3.734805]\n",
      "epoch:30 step:28874 [D loss: 0.163780, acc.: 97.66%] [G loss: 2.198106]\n",
      "epoch:30 step:28875 [D loss: 0.105387, acc.: 98.44%] [G loss: 0.639969]\n",
      "epoch:30 step:28876 [D loss: 0.236113, acc.: 91.41%] [G loss: 1.911614]\n",
      "epoch:30 step:28877 [D loss: 0.354410, acc.: 92.97%] [G loss: 2.252712]\n",
      "epoch:30 step:28878 [D loss: 0.598527, acc.: 65.62%] [G loss: 2.889809]\n",
      "epoch:30 step:28879 [D loss: 0.195493, acc.: 95.31%] [G loss: 1.432523]\n",
      "epoch:30 step:28880 [D loss: 0.021955, acc.: 100.00%] [G loss: 0.611671]\n",
      "epoch:30 step:28881 [D loss: 0.251720, acc.: 95.31%] [G loss: 2.461602]\n",
      "epoch:30 step:28882 [D loss: 0.138542, acc.: 99.22%] [G loss: 2.726684]\n",
      "epoch:30 step:28883 [D loss: 0.435823, acc.: 76.56%] [G loss: 1.499460]\n",
      "epoch:30 step:28884 [D loss: 0.108660, acc.: 98.44%] [G loss: 3.082360]\n",
      "epoch:30 step:28885 [D loss: 0.598227, acc.: 65.62%] [G loss: 3.634634]\n",
      "epoch:30 step:28886 [D loss: 0.042470, acc.: 98.44%] [G loss: 2.142350]\n",
      "epoch:30 step:28887 [D loss: 0.035922, acc.: 100.00%] [G loss: 3.807197]\n",
      "epoch:30 step:28888 [D loss: 0.157260, acc.: 96.88%] [G loss: 1.733150]\n",
      "epoch:30 step:28889 [D loss: 0.334876, acc.: 82.03%] [G loss: 2.592479]\n",
      "epoch:30 step:28890 [D loss: 0.130273, acc.: 99.22%] [G loss: 3.263176]\n",
      "epoch:30 step:28891 [D loss: 0.204839, acc.: 98.44%] [G loss: 3.218565]\n",
      "epoch:30 step:28892 [D loss: 0.173864, acc.: 96.09%] [G loss: 4.506682]\n",
      "epoch:30 step:28893 [D loss: 0.278998, acc.: 92.97%] [G loss: 3.605220]\n",
      "epoch:30 step:28894 [D loss: 0.140000, acc.: 98.44%] [G loss: 1.316429]\n",
      "epoch:30 step:28895 [D loss: 0.117197, acc.: 97.66%] [G loss: 3.880497]\n",
      "epoch:30 step:28896 [D loss: 0.137398, acc.: 96.88%] [G loss: 0.849424]\n",
      "epoch:30 step:28897 [D loss: 0.466156, acc.: 71.09%] [G loss: 4.935395]\n",
      "epoch:30 step:28898 [D loss: 0.253586, acc.: 92.19%] [G loss: 2.484837]\n",
      "epoch:30 step:28899 [D loss: 0.376615, acc.: 87.50%] [G loss: 6.466403]\n",
      "epoch:30 step:28900 [D loss: 0.081875, acc.: 100.00%] [G loss: 2.085997]\n",
      "epoch:30 step:28901 [D loss: 0.053894, acc.: 100.00%] [G loss: 4.721920]\n",
      "epoch:30 step:28902 [D loss: 0.533362, acc.: 74.22%] [G loss: 1.989710]\n",
      "epoch:30 step:28903 [D loss: 0.142942, acc.: 97.66%] [G loss: 4.482427]\n",
      "epoch:30 step:28904 [D loss: 0.496827, acc.: 78.12%] [G loss: 4.050618]\n",
      "epoch:30 step:28905 [D loss: 0.070772, acc.: 98.44%] [G loss: 4.075361]\n",
      "epoch:30 step:28906 [D loss: 0.314722, acc.: 91.41%] [G loss: 4.284314]\n",
      "epoch:30 step:28907 [D loss: 0.101494, acc.: 99.22%] [G loss: 1.385777]\n",
      "epoch:30 step:28908 [D loss: 0.800095, acc.: 53.91%] [G loss: 0.925882]\n",
      "epoch:30 step:28909 [D loss: 0.310650, acc.: 86.72%] [G loss: 2.128373]\n",
      "epoch:30 step:28910 [D loss: 0.244561, acc.: 89.06%] [G loss: 5.417343]\n",
      "epoch:30 step:28911 [D loss: 0.675294, acc.: 62.50%] [G loss: 4.619786]\n",
      "epoch:30 step:28912 [D loss: 0.208381, acc.: 93.75%] [G loss: 5.298339]\n",
      "epoch:30 step:28913 [D loss: 1.158123, acc.: 54.69%] [G loss: 2.728092]\n",
      "epoch:30 step:28914 [D loss: 0.361383, acc.: 81.25%] [G loss: 2.863346]\n",
      "epoch:30 step:28915 [D loss: 0.617092, acc.: 64.06%] [G loss: 2.900041]\n",
      "epoch:30 step:28916 [D loss: 0.246351, acc.: 92.19%] [G loss: 4.393857]\n",
      "epoch:30 step:28917 [D loss: 0.513635, acc.: 71.09%] [G loss: 2.975807]\n",
      "epoch:30 step:28918 [D loss: 0.818179, acc.: 57.03%] [G loss: 6.189738]\n",
      "epoch:30 step:28919 [D loss: 0.558530, acc.: 67.97%] [G loss: 6.079429]\n",
      "epoch:30 step:28920 [D loss: 0.372094, acc.: 85.94%] [G loss: 6.192960]\n",
      "epoch:30 step:28921 [D loss: 0.632336, acc.: 64.84%] [G loss: 1.378400]\n",
      "epoch:30 step:28922 [D loss: 0.053846, acc.: 99.22%] [G loss: 2.742298]\n",
      "epoch:30 step:28923 [D loss: 0.097395, acc.: 98.44%] [G loss: 4.913452]\n",
      "epoch:30 step:28924 [D loss: 0.467456, acc.: 74.22%] [G loss: 2.122747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:28925 [D loss: 0.485530, acc.: 76.56%] [G loss: 7.508241]\n",
      "epoch:30 step:28926 [D loss: 0.148529, acc.: 96.88%] [G loss: 4.316127]\n",
      "epoch:30 step:28927 [D loss: 0.232890, acc.: 89.84%] [G loss: 2.524602]\n",
      "epoch:30 step:28928 [D loss: 0.706225, acc.: 61.72%] [G loss: 2.629218]\n",
      "epoch:30 step:28929 [D loss: 0.056131, acc.: 100.00%] [G loss: 4.153812]\n",
      "epoch:30 step:28930 [D loss: 0.056062, acc.: 100.00%] [G loss: 4.503658]\n",
      "epoch:30 step:28931 [D loss: 0.094736, acc.: 99.22%] [G loss: 3.600339]\n",
      "epoch:30 step:28932 [D loss: 0.168169, acc.: 96.88%] [G loss: 5.200176]\n",
      "epoch:30 step:28933 [D loss: 0.396515, acc.: 82.03%] [G loss: 5.172569]\n",
      "epoch:30 step:28934 [D loss: 1.310414, acc.: 31.25%] [G loss: 3.902006]\n",
      "epoch:30 step:28935 [D loss: 0.359993, acc.: 81.25%] [G loss: 2.899081]\n",
      "epoch:30 step:28936 [D loss: 0.145472, acc.: 96.09%] [G loss: 1.371960]\n",
      "epoch:30 step:28937 [D loss: 0.044668, acc.: 99.22%] [G loss: 2.081678]\n",
      "epoch:30 step:28938 [D loss: 0.101741, acc.: 97.66%] [G loss: 4.970024]\n",
      "epoch:30 step:28939 [D loss: 0.829767, acc.: 53.91%] [G loss: 0.905923]\n",
      "epoch:30 step:28940 [D loss: 0.118006, acc.: 97.66%] [G loss: 1.261898]\n",
      "epoch:30 step:28941 [D loss: 1.043508, acc.: 49.22%] [G loss: 1.213067]\n",
      "epoch:30 step:28942 [D loss: 0.279240, acc.: 88.28%] [G loss: 3.200327]\n",
      "epoch:30 step:28943 [D loss: 0.198941, acc.: 97.66%] [G loss: 4.500847]\n",
      "epoch:30 step:28944 [D loss: 0.376969, acc.: 84.38%] [G loss: 0.907508]\n",
      "epoch:30 step:28945 [D loss: 0.040648, acc.: 100.00%] [G loss: 2.162729]\n",
      "epoch:30 step:28946 [D loss: 0.231739, acc.: 92.19%] [G loss: 2.453287]\n",
      "epoch:30 step:28947 [D loss: 0.055342, acc.: 100.00%] [G loss: 3.799467]\n",
      "epoch:30 step:28948 [D loss: 0.437559, acc.: 71.09%] [G loss: 1.034966]\n",
      "epoch:30 step:28949 [D loss: 0.172342, acc.: 95.31%] [G loss: 2.064685]\n",
      "epoch:30 step:28950 [D loss: 0.185464, acc.: 95.31%] [G loss: 2.191180]\n",
      "epoch:30 step:28951 [D loss: 0.290837, acc.: 86.72%] [G loss: 4.277778]\n",
      "epoch:30 step:28952 [D loss: 0.285397, acc.: 88.28%] [G loss: 5.901858]\n",
      "epoch:30 step:28953 [D loss: 0.020278, acc.: 100.00%] [G loss: 7.605257]\n",
      "epoch:30 step:28954 [D loss: 0.589116, acc.: 72.66%] [G loss: 3.477496]\n",
      "epoch:30 step:28955 [D loss: 0.162185, acc.: 98.44%] [G loss: 8.222556]\n",
      "epoch:30 step:28956 [D loss: 0.071984, acc.: 100.00%] [G loss: 2.419863]\n",
      "epoch:30 step:28957 [D loss: 0.109748, acc.: 96.88%] [G loss: 1.965410]\n",
      "epoch:30 step:28958 [D loss: 0.253591, acc.: 90.62%] [G loss: 3.976640]\n",
      "epoch:30 step:28959 [D loss: 0.099190, acc.: 99.22%] [G loss: 1.458294]\n",
      "epoch:30 step:28960 [D loss: 0.293227, acc.: 90.62%] [G loss: 2.056135]\n",
      "epoch:30 step:28961 [D loss: 0.217479, acc.: 92.19%] [G loss: 3.424762]\n",
      "epoch:30 step:28962 [D loss: 0.265356, acc.: 93.75%] [G loss: 2.638590]\n",
      "epoch:30 step:28963 [D loss: 0.220513, acc.: 93.75%] [G loss: 4.722876]\n",
      "epoch:30 step:28964 [D loss: 0.170576, acc.: 96.09%] [G loss: 3.669143]\n",
      "epoch:30 step:28965 [D loss: 0.067434, acc.: 100.00%] [G loss: 4.569294]\n",
      "epoch:30 step:28966 [D loss: 0.114578, acc.: 99.22%] [G loss: 3.560049]\n",
      "epoch:30 step:28967 [D loss: 1.435411, acc.: 45.31%] [G loss: 3.591644]\n",
      "epoch:30 step:28968 [D loss: 1.102910, acc.: 45.31%] [G loss: 2.702633]\n",
      "epoch:30 step:28969 [D loss: 0.472445, acc.: 75.78%] [G loss: 3.705972]\n",
      "epoch:30 step:28970 [D loss: 0.756351, acc.: 58.59%] [G loss: 1.179826]\n",
      "epoch:30 step:28971 [D loss: 0.150427, acc.: 96.88%] [G loss: 1.912282]\n",
      "epoch:30 step:28972 [D loss: 0.447517, acc.: 79.69%] [G loss: 3.010943]\n",
      "epoch:30 step:28973 [D loss: 0.155176, acc.: 97.66%] [G loss: 5.121615]\n",
      "epoch:30 step:28974 [D loss: 0.382488, acc.: 78.12%] [G loss: 2.375720]\n",
      "epoch:30 step:28975 [D loss: 0.291017, acc.: 89.06%] [G loss: 2.359303]\n",
      "epoch:30 step:28976 [D loss: 0.629238, acc.: 63.28%] [G loss: 3.644042]\n",
      "epoch:30 step:28977 [D loss: 0.331762, acc.: 85.94%] [G loss: 2.972198]\n",
      "epoch:30 step:28978 [D loss: 0.051050, acc.: 100.00%] [G loss: 5.258427]\n",
      "epoch:30 step:28979 [D loss: 0.358808, acc.: 87.50%] [G loss: 5.330803]\n",
      "epoch:30 step:28980 [D loss: 0.488936, acc.: 76.56%] [G loss: 6.113298]\n",
      "epoch:30 step:28981 [D loss: 0.243332, acc.: 89.84%] [G loss: 2.740834]\n",
      "epoch:30 step:28982 [D loss: 0.030034, acc.: 100.00%] [G loss: 5.862041]\n",
      "epoch:30 step:28983 [D loss: 0.167744, acc.: 95.31%] [G loss: 3.907131]\n",
      "epoch:30 step:28984 [D loss: 0.419600, acc.: 79.69%] [G loss: 5.018414]\n",
      "epoch:30 step:28985 [D loss: 0.438869, acc.: 73.44%] [G loss: 2.148322]\n",
      "epoch:30 step:28986 [D loss: 0.073175, acc.: 99.22%] [G loss: 2.207448]\n",
      "epoch:30 step:28987 [D loss: 0.114329, acc.: 98.44%] [G loss: 2.291991]\n",
      "epoch:30 step:28988 [D loss: 0.712007, acc.: 59.38%] [G loss: 2.189779]\n",
      "epoch:30 step:28989 [D loss: 0.069601, acc.: 100.00%] [G loss: 4.610036]\n",
      "epoch:30 step:28990 [D loss: 0.410382, acc.: 82.81%] [G loss: 2.795336]\n",
      "epoch:30 step:28991 [D loss: 0.263355, acc.: 93.75%] [G loss: 3.714760]\n",
      "epoch:30 step:28992 [D loss: 0.192853, acc.: 92.97%] [G loss: 2.732755]\n",
      "epoch:30 step:28993 [D loss: 0.284287, acc.: 92.97%] [G loss: 2.720562]\n",
      "epoch:30 step:28994 [D loss: 0.135796, acc.: 97.66%] [G loss: 3.252819]\n",
      "epoch:30 step:28995 [D loss: 0.321941, acc.: 92.97%] [G loss: 4.992244]\n",
      "epoch:30 step:28996 [D loss: 0.255531, acc.: 95.31%] [G loss: 4.692663]\n",
      "epoch:30 step:28997 [D loss: 0.547179, acc.: 69.53%] [G loss: 3.126405]\n",
      "epoch:30 step:28998 [D loss: 0.759313, acc.: 57.03%] [G loss: 3.045243]\n",
      "epoch:30 step:28999 [D loss: 0.198928, acc.: 89.84%] [G loss: 2.946931]\n",
      "epoch:30 step:29000 [D loss: 0.438880, acc.: 71.88%] [G loss: 2.232580]\n",
      "epoch:30 step:29001 [D loss: 0.219934, acc.: 89.84%] [G loss: 4.290603]\n",
      "epoch:30 step:29002 [D loss: 0.153801, acc.: 96.88%] [G loss: 3.874685]\n",
      "epoch:30 step:29003 [D loss: 0.019523, acc.: 100.00%] [G loss: 9.217017]\n",
      "epoch:30 step:29004 [D loss: 0.628459, acc.: 62.50%] [G loss: 2.071114]\n",
      "epoch:30 step:29005 [D loss: 0.203763, acc.: 95.31%] [G loss: 3.426425]\n",
      "epoch:30 step:29006 [D loss: 0.780227, acc.: 61.72%] [G loss: 1.559804]\n",
      "epoch:30 step:29007 [D loss: 0.353705, acc.: 84.38%] [G loss: 6.244833]\n",
      "epoch:30 step:29008 [D loss: 0.062983, acc.: 99.22%] [G loss: 2.873569]\n",
      "epoch:30 step:29009 [D loss: 0.072874, acc.: 98.44%] [G loss: 3.064391]\n",
      "epoch:30 step:29010 [D loss: 0.287013, acc.: 92.19%] [G loss: 4.606232]\n",
      "epoch:30 step:29011 [D loss: 0.332182, acc.: 84.38%] [G loss: 5.448405]\n",
      "epoch:30 step:29012 [D loss: 0.135909, acc.: 96.88%] [G loss: 0.878858]\n",
      "epoch:30 step:29013 [D loss: 0.958435, acc.: 55.47%] [G loss: 6.183182]\n",
      "epoch:30 step:29014 [D loss: 0.315628, acc.: 90.62%] [G loss: 2.162763]\n",
      "epoch:30 step:29015 [D loss: 0.857445, acc.: 58.59%] [G loss: 3.259166]\n",
      "epoch:30 step:29016 [D loss: 0.293005, acc.: 90.62%] [G loss: 0.590809]\n",
      "epoch:30 step:29017 [D loss: 0.151912, acc.: 98.44%] [G loss: 1.557663]\n",
      "epoch:30 step:29018 [D loss: 0.420257, acc.: 84.38%] [G loss: 1.801445]\n",
      "epoch:30 step:29019 [D loss: 0.534159, acc.: 72.66%] [G loss: 4.892661]\n",
      "epoch:30 step:29020 [D loss: 0.011792, acc.: 100.00%] [G loss: 3.162171]\n",
      "epoch:30 step:29021 [D loss: 0.161203, acc.: 96.88%] [G loss: 2.968112]\n",
      "epoch:30 step:29022 [D loss: 0.407286, acc.: 79.69%] [G loss: 2.999322]\n",
      "epoch:30 step:29023 [D loss: 0.239922, acc.: 93.75%] [G loss: 2.219021]\n",
      "epoch:30 step:29024 [D loss: 0.359119, acc.: 82.81%] [G loss: 3.968953]\n",
      "epoch:30 step:29025 [D loss: 0.308146, acc.: 93.75%] [G loss: 5.078520]\n",
      "epoch:30 step:29026 [D loss: 0.675233, acc.: 65.62%] [G loss: 3.378825]\n",
      "epoch:30 step:29027 [D loss: 0.303342, acc.: 90.62%] [G loss: 5.409581]\n",
      "epoch:30 step:29028 [D loss: 0.418713, acc.: 74.22%] [G loss: 3.403498]\n",
      "epoch:30 step:29029 [D loss: 1.579557, acc.: 40.62%] [G loss: 2.523663]\n",
      "epoch:30 step:29030 [D loss: 0.246159, acc.: 92.19%] [G loss: 1.304866]\n",
      "epoch:30 step:29031 [D loss: 0.243212, acc.: 96.09%] [G loss: 2.066763]\n",
      "epoch:30 step:29032 [D loss: 0.091681, acc.: 100.00%] [G loss: 1.286861]\n",
      "epoch:30 step:29033 [D loss: 0.222816, acc.: 92.19%] [G loss: 1.848991]\n",
      "epoch:30 step:29034 [D loss: 0.718416, acc.: 60.16%] [G loss: 5.844679]\n",
      "epoch:30 step:29035 [D loss: 0.114000, acc.: 97.66%] [G loss: 6.134402]\n",
      "epoch:30 step:29036 [D loss: 0.561635, acc.: 72.66%] [G loss: 3.726206]\n",
      "epoch:30 step:29037 [D loss: 0.364948, acc.: 84.38%] [G loss: 1.542297]\n",
      "epoch:30 step:29038 [D loss: 0.896066, acc.: 51.56%] [G loss: 4.624469]\n",
      "epoch:30 step:29039 [D loss: 0.156180, acc.: 92.97%] [G loss: 5.316442]\n",
      "epoch:30 step:29040 [D loss: 0.088581, acc.: 96.88%] [G loss: 3.619385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:29041 [D loss: 0.229046, acc.: 89.84%] [G loss: 4.451653]\n",
      "epoch:30 step:29042 [D loss: 0.131514, acc.: 97.66%] [G loss: 2.684486]\n",
      "epoch:30 step:29043 [D loss: 0.086758, acc.: 99.22%] [G loss: 4.555716]\n",
      "epoch:30 step:29044 [D loss: 0.542602, acc.: 68.75%] [G loss: 4.181722]\n",
      "epoch:30 step:29045 [D loss: 0.473897, acc.: 75.78%] [G loss: 1.915492]\n",
      "epoch:30 step:29046 [D loss: 0.066972, acc.: 99.22%] [G loss: 4.856309]\n",
      "epoch:30 step:29047 [D loss: 0.081081, acc.: 98.44%] [G loss: 2.715577]\n",
      "epoch:31 step:29048 [D loss: 0.342693, acc.: 85.94%] [G loss: 3.740392]\n",
      "epoch:31 step:29049 [D loss: 0.112585, acc.: 97.66%] [G loss: 2.468815]\n",
      "epoch:31 step:29050 [D loss: 0.057147, acc.: 99.22%] [G loss: 4.896787]\n",
      "epoch:31 step:29051 [D loss: 0.644932, acc.: 68.75%] [G loss: 1.765354]\n",
      "epoch:31 step:29052 [D loss: 1.096795, acc.: 50.00%] [G loss: 2.695849]\n",
      "epoch:31 step:29053 [D loss: 0.729529, acc.: 58.59%] [G loss: 4.834586]\n",
      "epoch:31 step:29054 [D loss: 0.243280, acc.: 87.50%] [G loss: 2.701608]\n",
      "epoch:31 step:29055 [D loss: 0.827384, acc.: 54.69%] [G loss: 2.508764]\n",
      "epoch:31 step:29056 [D loss: 0.314114, acc.: 90.62%] [G loss: 2.965785]\n",
      "epoch:31 step:29057 [D loss: 1.190762, acc.: 51.56%] [G loss: 0.461107]\n",
      "epoch:31 step:29058 [D loss: 0.254631, acc.: 89.84%] [G loss: 1.750170]\n",
      "epoch:31 step:29059 [D loss: 0.780248, acc.: 57.81%] [G loss: 0.400239]\n",
      "epoch:31 step:29060 [D loss: 0.311294, acc.: 85.16%] [G loss: 1.116385]\n",
      "epoch:31 step:29061 [D loss: 0.305115, acc.: 90.62%] [G loss: 2.415971]\n",
      "epoch:31 step:29062 [D loss: 0.113347, acc.: 98.44%] [G loss: 0.753620]\n",
      "epoch:31 step:29063 [D loss: 0.141537, acc.: 97.66%] [G loss: 1.553153]\n",
      "epoch:31 step:29064 [D loss: 0.193508, acc.: 92.97%] [G loss: 3.826375]\n",
      "epoch:31 step:29065 [D loss: 0.112053, acc.: 99.22%] [G loss: 2.397968]\n",
      "epoch:31 step:29066 [D loss: 0.205154, acc.: 96.09%] [G loss: 3.137280]\n",
      "epoch:31 step:29067 [D loss: 0.084112, acc.: 99.22%] [G loss: 2.986365]\n",
      "epoch:31 step:29068 [D loss: 0.023752, acc.: 100.00%] [G loss: 3.182913]\n",
      "epoch:31 step:29069 [D loss: 0.328398, acc.: 89.06%] [G loss: 3.362965]\n",
      "epoch:31 step:29070 [D loss: 0.376490, acc.: 76.56%] [G loss: 1.737760]\n",
      "epoch:31 step:29071 [D loss: 0.048599, acc.: 98.44%] [G loss: 3.496270]\n",
      "epoch:31 step:29072 [D loss: 0.166687, acc.: 97.66%] [G loss: 1.999338]\n",
      "epoch:31 step:29073 [D loss: 0.335324, acc.: 81.25%] [G loss: 6.172234]\n",
      "epoch:31 step:29074 [D loss: 0.204631, acc.: 93.75%] [G loss: 3.778879]\n",
      "epoch:31 step:29075 [D loss: 1.252414, acc.: 25.00%] [G loss: 3.518632]\n",
      "epoch:31 step:29076 [D loss: 0.237555, acc.: 94.53%] [G loss: 1.067987]\n",
      "epoch:31 step:29077 [D loss: 0.065176, acc.: 100.00%] [G loss: 2.062234]\n",
      "epoch:31 step:29078 [D loss: 0.344393, acc.: 80.47%] [G loss: 4.627599]\n",
      "epoch:31 step:29079 [D loss: 0.144142, acc.: 96.88%] [G loss: 6.338468]\n",
      "epoch:31 step:29080 [D loss: 0.092097, acc.: 98.44%] [G loss: 4.442193]\n",
      "epoch:31 step:29081 [D loss: 0.372272, acc.: 85.16%] [G loss: 3.861368]\n",
      "epoch:31 step:29082 [D loss: 0.337252, acc.: 85.94%] [G loss: 1.412374]\n",
      "epoch:31 step:29083 [D loss: 0.318787, acc.: 85.94%] [G loss: 2.644573]\n",
      "epoch:31 step:29084 [D loss: 0.503871, acc.: 73.44%] [G loss: 1.481309]\n",
      "epoch:31 step:29085 [D loss: 0.026737, acc.: 99.22%] [G loss: 4.986763]\n",
      "epoch:31 step:29086 [D loss: 0.204161, acc.: 88.28%] [G loss: 5.910205]\n",
      "epoch:31 step:29087 [D loss: 0.277555, acc.: 92.97%] [G loss: 1.357937]\n",
      "epoch:31 step:29088 [D loss: 0.081038, acc.: 98.44%] [G loss: 3.220068]\n",
      "epoch:31 step:29089 [D loss: 0.119571, acc.: 98.44%] [G loss: 1.947391]\n",
      "epoch:31 step:29090 [D loss: 0.083390, acc.: 98.44%] [G loss: 0.549533]\n",
      "epoch:31 step:29091 [D loss: 0.352774, acc.: 89.06%] [G loss: 2.843876]\n",
      "epoch:31 step:29092 [D loss: 0.442257, acc.: 84.38%] [G loss: 2.883290]\n",
      "epoch:31 step:29093 [D loss: 0.501433, acc.: 64.06%] [G loss: 2.163026]\n",
      "epoch:31 step:29094 [D loss: 0.068314, acc.: 98.44%] [G loss: 4.097135]\n",
      "epoch:31 step:29095 [D loss: 1.315121, acc.: 48.44%] [G loss: 2.464200]\n",
      "epoch:31 step:29096 [D loss: 0.319264, acc.: 82.81%] [G loss: 1.852645]\n",
      "epoch:31 step:29097 [D loss: 0.239105, acc.: 89.84%] [G loss: 1.672537]\n",
      "epoch:31 step:29098 [D loss: 0.060456, acc.: 100.00%] [G loss: 2.546129]\n",
      "epoch:31 step:29099 [D loss: 0.082101, acc.: 98.44%] [G loss: 3.232700]\n",
      "epoch:31 step:29100 [D loss: 0.471953, acc.: 78.91%] [G loss: 1.389356]\n",
      "epoch:31 step:29101 [D loss: 0.788234, acc.: 53.12%] [G loss: 1.436597]\n",
      "epoch:31 step:29102 [D loss: 0.015932, acc.: 100.00%] [G loss: 4.310944]\n",
      "epoch:31 step:29103 [D loss: 0.854436, acc.: 57.03%] [G loss: 0.800057]\n",
      "epoch:31 step:29104 [D loss: 0.899683, acc.: 49.22%] [G loss: 3.618658]\n",
      "epoch:31 step:29105 [D loss: 0.120999, acc.: 98.44%] [G loss: 1.905477]\n",
      "epoch:31 step:29106 [D loss: 0.266142, acc.: 85.94%] [G loss: 2.504396]\n",
      "epoch:31 step:29107 [D loss: 0.155015, acc.: 97.66%] [G loss: 0.540462]\n",
      "epoch:31 step:29108 [D loss: 0.551664, acc.: 67.97%] [G loss: 1.654909]\n",
      "epoch:31 step:29109 [D loss: 0.880748, acc.: 62.50%] [G loss: 0.949528]\n",
      "epoch:31 step:29110 [D loss: 0.228527, acc.: 90.62%] [G loss: 3.652425]\n",
      "epoch:31 step:29111 [D loss: 0.211080, acc.: 95.31%] [G loss: 3.629099]\n",
      "epoch:31 step:29112 [D loss: 0.208445, acc.: 94.53%] [G loss: 2.108981]\n",
      "epoch:31 step:29113 [D loss: 0.624902, acc.: 65.62%] [G loss: 4.361971]\n",
      "epoch:31 step:29114 [D loss: 0.085460, acc.: 99.22%] [G loss: 3.309873]\n",
      "epoch:31 step:29115 [D loss: 0.198177, acc.: 95.31%] [G loss: 3.925458]\n",
      "epoch:31 step:29116 [D loss: 0.177169, acc.: 99.22%] [G loss: 0.541867]\n",
      "epoch:31 step:29117 [D loss: 0.052240, acc.: 100.00%] [G loss: 3.567847]\n",
      "epoch:31 step:29118 [D loss: 0.256452, acc.: 94.53%] [G loss: 2.396664]\n",
      "epoch:31 step:29119 [D loss: 0.454744, acc.: 79.69%] [G loss: 1.925454]\n",
      "epoch:31 step:29120 [D loss: 0.085513, acc.: 99.22%] [G loss: 1.748870]\n",
      "epoch:31 step:29121 [D loss: 0.314914, acc.: 89.84%] [G loss: 2.414131]\n",
      "epoch:31 step:29122 [D loss: 0.351097, acc.: 84.38%] [G loss: 5.543274]\n",
      "epoch:31 step:29123 [D loss: 0.522813, acc.: 71.09%] [G loss: 4.946362]\n",
      "epoch:31 step:29124 [D loss: 0.341982, acc.: 89.06%] [G loss: 3.283521]\n",
      "epoch:31 step:29125 [D loss: 0.205869, acc.: 92.19%] [G loss: 5.513755]\n",
      "epoch:31 step:29126 [D loss: 0.052906, acc.: 98.44%] [G loss: 2.521162]\n",
      "epoch:31 step:29127 [D loss: 0.360743, acc.: 85.16%] [G loss: 4.378441]\n",
      "epoch:31 step:29128 [D loss: 0.089251, acc.: 97.66%] [G loss: 2.097879]\n",
      "epoch:31 step:29129 [D loss: 0.293734, acc.: 91.41%] [G loss: 3.778676]\n",
      "epoch:31 step:29130 [D loss: 0.189056, acc.: 93.75%] [G loss: 3.998498]\n",
      "epoch:31 step:29131 [D loss: 0.098772, acc.: 97.66%] [G loss: 4.244354]\n",
      "epoch:31 step:29132 [D loss: 1.500147, acc.: 44.53%] [G loss: 1.678154]\n",
      "epoch:31 step:29133 [D loss: 0.087565, acc.: 98.44%] [G loss: 1.834331]\n",
      "epoch:31 step:29134 [D loss: 0.248815, acc.: 87.50%] [G loss: 4.993589]\n",
      "epoch:31 step:29135 [D loss: 0.713039, acc.: 55.47%] [G loss: 0.725713]\n",
      "epoch:31 step:29136 [D loss: 0.219833, acc.: 92.97%] [G loss: 4.408756]\n",
      "epoch:31 step:29137 [D loss: 0.608913, acc.: 66.41%] [G loss: 1.911465]\n",
      "epoch:31 step:29138 [D loss: 0.158409, acc.: 93.75%] [G loss: 3.065088]\n",
      "epoch:31 step:29139 [D loss: 0.124182, acc.: 97.66%] [G loss: 1.931113]\n",
      "epoch:31 step:29140 [D loss: 0.198501, acc.: 96.09%] [G loss: 5.278820]\n",
      "epoch:31 step:29141 [D loss: 0.533846, acc.: 68.75%] [G loss: 3.842425]\n",
      "epoch:31 step:29142 [D loss: 0.124570, acc.: 98.44%] [G loss: 8.035331]\n",
      "epoch:31 step:29143 [D loss: 0.205203, acc.: 94.53%] [G loss: 3.271948]\n",
      "epoch:31 step:29144 [D loss: 0.036349, acc.: 100.00%] [G loss: 2.499785]\n",
      "epoch:31 step:29145 [D loss: 0.134318, acc.: 96.09%] [G loss: 5.335969]\n",
      "epoch:31 step:29146 [D loss: 0.938902, acc.: 57.03%] [G loss: 3.476672]\n",
      "epoch:31 step:29147 [D loss: 1.002582, acc.: 53.12%] [G loss: 0.992155]\n",
      "epoch:31 step:29148 [D loss: 0.273318, acc.: 89.84%] [G loss: 1.266508]\n",
      "epoch:31 step:29149 [D loss: 0.858148, acc.: 50.78%] [G loss: 1.719978]\n",
      "epoch:31 step:29150 [D loss: 0.218438, acc.: 93.75%] [G loss: 2.452093]\n",
      "epoch:31 step:29151 [D loss: 0.459654, acc.: 74.22%] [G loss: 4.675233]\n",
      "epoch:31 step:29152 [D loss: 0.196882, acc.: 96.88%] [G loss: 1.415517]\n",
      "epoch:31 step:29153 [D loss: 0.305301, acc.: 85.16%] [G loss: 3.652858]\n",
      "epoch:31 step:29154 [D loss: 0.380804, acc.: 86.72%] [G loss: 3.572842]\n",
      "epoch:31 step:29155 [D loss: 0.108981, acc.: 97.66%] [G loss: 3.161194]\n",
      "epoch:31 step:29156 [D loss: 0.338946, acc.: 83.59%] [G loss: 2.028138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29157 [D loss: 0.648783, acc.: 63.28%] [G loss: 4.210823]\n",
      "epoch:31 step:29158 [D loss: 0.252006, acc.: 96.09%] [G loss: 2.801659]\n",
      "epoch:31 step:29159 [D loss: 0.497433, acc.: 70.31%] [G loss: 1.214062]\n",
      "epoch:31 step:29160 [D loss: 0.544905, acc.: 71.09%] [G loss: 3.203209]\n",
      "epoch:31 step:29161 [D loss: 0.163694, acc.: 96.09%] [G loss: 1.635280]\n",
      "epoch:31 step:29162 [D loss: 0.094836, acc.: 100.00%] [G loss: 2.867537]\n",
      "epoch:31 step:29163 [D loss: 0.209088, acc.: 92.97%] [G loss: 4.061337]\n",
      "epoch:31 step:29164 [D loss: 0.379659, acc.: 82.03%] [G loss: 1.920923]\n",
      "epoch:31 step:29165 [D loss: 0.366178, acc.: 86.72%] [G loss: 3.040202]\n",
      "epoch:31 step:29166 [D loss: 0.278304, acc.: 86.72%] [G loss: 2.354334]\n",
      "epoch:31 step:29167 [D loss: 0.102891, acc.: 99.22%] [G loss: 3.867932]\n",
      "epoch:31 step:29168 [D loss: 0.110248, acc.: 97.66%] [G loss: 2.148595]\n",
      "epoch:31 step:29169 [D loss: 0.474066, acc.: 75.00%] [G loss: 1.291863]\n",
      "epoch:31 step:29170 [D loss: 0.081832, acc.: 99.22%] [G loss: 0.799315]\n",
      "epoch:31 step:29171 [D loss: 0.068496, acc.: 99.22%] [G loss: 0.269671]\n",
      "epoch:31 step:29172 [D loss: 0.032416, acc.: 100.00%] [G loss: 4.014809]\n",
      "epoch:31 step:29173 [D loss: 0.972012, acc.: 55.47%] [G loss: 3.576690]\n",
      "epoch:31 step:29174 [D loss: 0.310270, acc.: 86.72%] [G loss: 4.684990]\n",
      "epoch:31 step:29175 [D loss: 0.167893, acc.: 96.09%] [G loss: 2.125169]\n",
      "epoch:31 step:29176 [D loss: 0.168243, acc.: 96.88%] [G loss: 1.660727]\n",
      "epoch:31 step:29177 [D loss: 0.267469, acc.: 87.50%] [G loss: 1.263582]\n",
      "epoch:31 step:29178 [D loss: 0.229596, acc.: 91.41%] [G loss: 3.398529]\n",
      "epoch:31 step:29179 [D loss: 0.437719, acc.: 79.69%] [G loss: 2.422959]\n",
      "epoch:31 step:29180 [D loss: 0.426472, acc.: 76.56%] [G loss: 1.937041]\n",
      "epoch:31 step:29181 [D loss: 0.757966, acc.: 56.25%] [G loss: 0.860238]\n",
      "epoch:31 step:29182 [D loss: 0.112480, acc.: 98.44%] [G loss: 2.176765]\n",
      "epoch:31 step:29183 [D loss: 0.127744, acc.: 96.09%] [G loss: 2.259526]\n",
      "epoch:31 step:29184 [D loss: 0.036518, acc.: 100.00%] [G loss: 0.889850]\n",
      "epoch:31 step:29185 [D loss: 0.319213, acc.: 85.94%] [G loss: 1.937991]\n",
      "epoch:31 step:29186 [D loss: 0.051899, acc.: 99.22%] [G loss: 4.575752]\n",
      "epoch:31 step:29187 [D loss: 0.201263, acc.: 93.75%] [G loss: 1.493569]\n",
      "epoch:31 step:29188 [D loss: 0.835648, acc.: 57.03%] [G loss: 1.884026]\n",
      "epoch:31 step:29189 [D loss: 0.090823, acc.: 97.66%] [G loss: 6.053694]\n",
      "epoch:31 step:29190 [D loss: 0.363810, acc.: 79.69%] [G loss: 4.021564]\n",
      "epoch:31 step:29191 [D loss: 0.523491, acc.: 74.22%] [G loss: 3.054953]\n",
      "epoch:31 step:29192 [D loss: 0.432422, acc.: 75.78%] [G loss: 0.870919]\n",
      "epoch:31 step:29193 [D loss: 0.482396, acc.: 76.56%] [G loss: 5.509055]\n",
      "epoch:31 step:29194 [D loss: 0.778554, acc.: 57.03%] [G loss: 5.693143]\n",
      "epoch:31 step:29195 [D loss: 0.165434, acc.: 95.31%] [G loss: 1.312461]\n",
      "epoch:31 step:29196 [D loss: 0.241755, acc.: 92.19%] [G loss: 1.603709]\n",
      "epoch:31 step:29197 [D loss: 1.149872, acc.: 32.03%] [G loss: 2.541818]\n",
      "epoch:31 step:29198 [D loss: 0.114364, acc.: 100.00%] [G loss: 1.071954]\n",
      "epoch:31 step:29199 [D loss: 0.523256, acc.: 68.75%] [G loss: 2.791817]\n",
      "epoch:31 step:29200 [D loss: 0.116102, acc.: 99.22%] [G loss: 4.486679]\n",
      "epoch:31 step:29201 [D loss: 0.185505, acc.: 93.75%] [G loss: 4.171624]\n",
      "epoch:31 step:29202 [D loss: 1.122922, acc.: 32.81%] [G loss: 1.937875]\n",
      "epoch:31 step:29203 [D loss: 0.179533, acc.: 93.75%] [G loss: 1.190392]\n",
      "epoch:31 step:29204 [D loss: 0.160867, acc.: 97.66%] [G loss: 1.115673]\n",
      "epoch:31 step:29205 [D loss: 0.096959, acc.: 96.88%] [G loss: 2.414925]\n",
      "epoch:31 step:29206 [D loss: 0.078302, acc.: 100.00%] [G loss: 2.066205]\n",
      "epoch:31 step:29207 [D loss: 0.436675, acc.: 84.38%] [G loss: 2.752161]\n",
      "epoch:31 step:29208 [D loss: 0.115884, acc.: 98.44%] [G loss: 0.750989]\n",
      "epoch:31 step:29209 [D loss: 0.040864, acc.: 100.00%] [G loss: 4.046152]\n",
      "epoch:31 step:29210 [D loss: 0.169210, acc.: 96.09%] [G loss: 2.331157]\n",
      "epoch:31 step:29211 [D loss: 0.160202, acc.: 98.44%] [G loss: 2.071660]\n",
      "epoch:31 step:29212 [D loss: 1.230543, acc.: 50.78%] [G loss: 1.622556]\n",
      "epoch:31 step:29213 [D loss: 0.073492, acc.: 100.00%] [G loss: 3.874142]\n",
      "epoch:31 step:29214 [D loss: 0.177823, acc.: 95.31%] [G loss: 2.108513]\n",
      "epoch:31 step:29215 [D loss: 1.113636, acc.: 43.75%] [G loss: 1.417993]\n",
      "epoch:31 step:29216 [D loss: 0.571625, acc.: 73.44%] [G loss: 1.457717]\n",
      "epoch:31 step:29217 [D loss: 1.142068, acc.: 35.94%] [G loss: 4.160754]\n",
      "epoch:31 step:29218 [D loss: 0.158526, acc.: 97.66%] [G loss: 1.995573]\n",
      "epoch:31 step:29219 [D loss: 0.700221, acc.: 59.38%] [G loss: 1.781616]\n",
      "epoch:31 step:29220 [D loss: 0.315639, acc.: 81.25%] [G loss: 1.043942]\n",
      "epoch:31 step:29221 [D loss: 0.258033, acc.: 92.19%] [G loss: 1.527670]\n",
      "epoch:31 step:29222 [D loss: 0.276950, acc.: 90.62%] [G loss: 1.178564]\n",
      "epoch:31 step:29223 [D loss: 0.205782, acc.: 96.09%] [G loss: 3.089295]\n",
      "epoch:31 step:29224 [D loss: 0.068462, acc.: 99.22%] [G loss: 5.365939]\n",
      "epoch:31 step:29225 [D loss: 0.126165, acc.: 99.22%] [G loss: 1.122756]\n",
      "epoch:31 step:29226 [D loss: 0.463464, acc.: 70.31%] [G loss: 1.688978]\n",
      "epoch:31 step:29227 [D loss: 0.182071, acc.: 96.09%] [G loss: 3.453128]\n",
      "epoch:31 step:29228 [D loss: 0.147405, acc.: 96.88%] [G loss: 3.824262]\n",
      "epoch:31 step:29229 [D loss: 0.051059, acc.: 99.22%] [G loss: 3.552056]\n",
      "epoch:31 step:29230 [D loss: 0.491555, acc.: 79.69%] [G loss: 1.302905]\n",
      "epoch:31 step:29231 [D loss: 0.386182, acc.: 77.34%] [G loss: 2.207658]\n",
      "epoch:31 step:29232 [D loss: 0.265277, acc.: 89.84%] [G loss: 0.297477]\n",
      "epoch:31 step:29233 [D loss: 0.125372, acc.: 96.88%] [G loss: 0.322326]\n",
      "epoch:31 step:29234 [D loss: 0.148724, acc.: 96.09%] [G loss: 0.847403]\n",
      "epoch:31 step:29235 [D loss: 0.417923, acc.: 75.78%] [G loss: 2.304796]\n",
      "epoch:31 step:29236 [D loss: 0.101534, acc.: 100.00%] [G loss: 3.100694]\n",
      "epoch:31 step:29237 [D loss: 0.207455, acc.: 96.09%] [G loss: 2.985825]\n",
      "epoch:31 step:29238 [D loss: 0.114586, acc.: 97.66%] [G loss: 2.522711]\n",
      "epoch:31 step:29239 [D loss: 0.100152, acc.: 99.22%] [G loss: 2.979280]\n",
      "epoch:31 step:29240 [D loss: 0.460563, acc.: 68.75%] [G loss: 4.054571]\n",
      "epoch:31 step:29241 [D loss: 0.361405, acc.: 80.47%] [G loss: 0.604114]\n",
      "epoch:31 step:29242 [D loss: 0.483490, acc.: 78.12%] [G loss: 1.691041]\n",
      "epoch:31 step:29243 [D loss: 0.287518, acc.: 93.75%] [G loss: 2.836382]\n",
      "epoch:31 step:29244 [D loss: 0.272252, acc.: 91.41%] [G loss: 5.559233]\n",
      "epoch:31 step:29245 [D loss: 0.104071, acc.: 98.44%] [G loss: 3.987023]\n",
      "epoch:31 step:29246 [D loss: 0.158031, acc.: 97.66%] [G loss: 3.283485]\n",
      "epoch:31 step:29247 [D loss: 1.237505, acc.: 56.25%] [G loss: 2.346154]\n",
      "epoch:31 step:29248 [D loss: 0.171072, acc.: 94.53%] [G loss: 2.472246]\n",
      "epoch:31 step:29249 [D loss: 0.332333, acc.: 80.47%] [G loss: 4.689754]\n",
      "epoch:31 step:29250 [D loss: 0.238260, acc.: 94.53%] [G loss: 0.897188]\n",
      "epoch:31 step:29251 [D loss: 0.083231, acc.: 99.22%] [G loss: 2.945055]\n",
      "epoch:31 step:29252 [D loss: 1.199549, acc.: 38.28%] [G loss: 2.874683]\n",
      "epoch:31 step:29253 [D loss: 0.030745, acc.: 100.00%] [G loss: 5.883922]\n",
      "epoch:31 step:29254 [D loss: 0.316046, acc.: 81.25%] [G loss: 2.331391]\n",
      "epoch:31 step:29255 [D loss: 0.157729, acc.: 97.66%] [G loss: 3.900805]\n",
      "epoch:31 step:29256 [D loss: 0.146208, acc.: 98.44%] [G loss: 5.772673]\n",
      "epoch:31 step:29257 [D loss: 0.188148, acc.: 93.75%] [G loss: 0.907182]\n",
      "epoch:31 step:29258 [D loss: 0.786998, acc.: 58.59%] [G loss: 3.240703]\n",
      "epoch:31 step:29259 [D loss: 1.661381, acc.: 27.34%] [G loss: 0.480466]\n",
      "epoch:31 step:29260 [D loss: 0.624692, acc.: 60.16%] [G loss: 4.486877]\n",
      "epoch:31 step:29261 [D loss: 0.341956, acc.: 88.28%] [G loss: 1.158606]\n",
      "epoch:31 step:29262 [D loss: 0.074143, acc.: 100.00%] [G loss: 2.142143]\n",
      "epoch:31 step:29263 [D loss: 0.367172, acc.: 84.38%] [G loss: 3.293464]\n",
      "epoch:31 step:29264 [D loss: 0.104222, acc.: 97.66%] [G loss: 2.452843]\n",
      "epoch:31 step:29265 [D loss: 0.202914, acc.: 95.31%] [G loss: 2.770473]\n",
      "epoch:31 step:29266 [D loss: 0.177941, acc.: 94.53%] [G loss: 3.691317]\n",
      "epoch:31 step:29267 [D loss: 0.149036, acc.: 94.53%] [G loss: 2.170011]\n",
      "epoch:31 step:29268 [D loss: 0.308334, acc.: 82.81%] [G loss: 3.192425]\n",
      "epoch:31 step:29269 [D loss: 0.573205, acc.: 66.41%] [G loss: 2.874107]\n",
      "epoch:31 step:29270 [D loss: 1.643653, acc.: 39.84%] [G loss: 4.622771]\n",
      "epoch:31 step:29271 [D loss: 0.028920, acc.: 100.00%] [G loss: 2.731415]\n",
      "epoch:31 step:29272 [D loss: 0.141313, acc.: 96.88%] [G loss: 1.946594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29273 [D loss: 0.260827, acc.: 92.97%] [G loss: 1.848671]\n",
      "epoch:31 step:29274 [D loss: 0.308622, acc.: 91.41%] [G loss: 3.297621]\n",
      "epoch:31 step:29275 [D loss: 0.638762, acc.: 60.16%] [G loss: 5.137354]\n",
      "epoch:31 step:29276 [D loss: 0.255452, acc.: 93.75%] [G loss: 2.300446]\n",
      "epoch:31 step:29277 [D loss: 0.182866, acc.: 97.66%] [G loss: 5.102607]\n",
      "epoch:31 step:29278 [D loss: 0.062937, acc.: 100.00%] [G loss: 2.655994]\n",
      "epoch:31 step:29279 [D loss: 0.761035, acc.: 51.56%] [G loss: 4.959136]\n",
      "epoch:31 step:29280 [D loss: 0.148863, acc.: 97.66%] [G loss: 1.124002]\n",
      "epoch:31 step:29281 [D loss: 0.149649, acc.: 99.22%] [G loss: 2.853498]\n",
      "epoch:31 step:29282 [D loss: 0.223720, acc.: 90.62%] [G loss: 1.704786]\n",
      "epoch:31 step:29283 [D loss: 0.096458, acc.: 98.44%] [G loss: 2.038157]\n",
      "epoch:31 step:29284 [D loss: 0.385124, acc.: 79.69%] [G loss: 3.392864]\n",
      "epoch:31 step:29285 [D loss: 0.069082, acc.: 100.00%] [G loss: 0.737129]\n",
      "epoch:31 step:29286 [D loss: 0.086937, acc.: 98.44%] [G loss: 3.498109]\n",
      "epoch:31 step:29287 [D loss: 0.157165, acc.: 98.44%] [G loss: 5.490679]\n",
      "epoch:31 step:29288 [D loss: 0.312545, acc.: 85.16%] [G loss: 2.379562]\n",
      "epoch:31 step:29289 [D loss: 0.154227, acc.: 96.09%] [G loss: 2.821869]\n",
      "epoch:31 step:29290 [D loss: 0.466664, acc.: 75.78%] [G loss: 2.349818]\n",
      "epoch:31 step:29291 [D loss: 0.924702, acc.: 41.41%] [G loss: 3.620752]\n",
      "epoch:31 step:29292 [D loss: 0.831613, acc.: 60.94%] [G loss: 1.672363]\n",
      "epoch:31 step:29293 [D loss: 0.216244, acc.: 93.75%] [G loss: 1.937794]\n",
      "epoch:31 step:29294 [D loss: 0.280522, acc.: 90.62%] [G loss: 1.951684]\n",
      "epoch:31 step:29295 [D loss: 0.055865, acc.: 100.00%] [G loss: 1.465168]\n",
      "epoch:31 step:29296 [D loss: 0.077916, acc.: 99.22%] [G loss: 7.478134]\n",
      "epoch:31 step:29297 [D loss: 0.347907, acc.: 85.16%] [G loss: 5.084964]\n",
      "epoch:31 step:29298 [D loss: 0.316205, acc.: 89.06%] [G loss: 4.464419]\n",
      "epoch:31 step:29299 [D loss: 0.076688, acc.: 97.66%] [G loss: 3.452435]\n",
      "epoch:31 step:29300 [D loss: 0.457399, acc.: 81.25%] [G loss: 4.757424]\n",
      "epoch:31 step:29301 [D loss: 0.090144, acc.: 98.44%] [G loss: 1.534324]\n",
      "epoch:31 step:29302 [D loss: 0.165208, acc.: 95.31%] [G loss: 2.707536]\n",
      "epoch:31 step:29303 [D loss: 0.220772, acc.: 93.75%] [G loss: 7.552517]\n",
      "epoch:31 step:29304 [D loss: 1.125917, acc.: 53.12%] [G loss: 0.936803]\n",
      "epoch:31 step:29305 [D loss: 0.438177, acc.: 83.59%] [G loss: 3.257551]\n",
      "epoch:31 step:29306 [D loss: 0.757010, acc.: 61.72%] [G loss: 3.263417]\n",
      "epoch:31 step:29307 [D loss: 0.075698, acc.: 99.22%] [G loss: 5.143994]\n",
      "epoch:31 step:29308 [D loss: 0.177900, acc.: 93.75%] [G loss: 3.634782]\n",
      "epoch:31 step:29309 [D loss: 0.646622, acc.: 64.06%] [G loss: 2.147252]\n",
      "epoch:31 step:29310 [D loss: 0.069223, acc.: 99.22%] [G loss: 3.155747]\n",
      "epoch:31 step:29311 [D loss: 1.100808, acc.: 34.38%] [G loss: 3.413446]\n",
      "epoch:31 step:29312 [D loss: 0.331373, acc.: 79.69%] [G loss: 6.285134]\n",
      "epoch:31 step:29313 [D loss: 0.060469, acc.: 98.44%] [G loss: 1.142290]\n",
      "epoch:31 step:29314 [D loss: 0.270088, acc.: 95.31%] [G loss: 0.975522]\n",
      "epoch:31 step:29315 [D loss: 0.513430, acc.: 68.75%] [G loss: 0.854136]\n",
      "epoch:31 step:29316 [D loss: 0.721432, acc.: 59.38%] [G loss: 1.051287]\n",
      "epoch:31 step:29317 [D loss: 0.109384, acc.: 99.22%] [G loss: 2.800080]\n",
      "epoch:31 step:29318 [D loss: 0.165675, acc.: 96.88%] [G loss: 3.123236]\n",
      "epoch:31 step:29319 [D loss: 0.281891, acc.: 86.72%] [G loss: 1.797276]\n",
      "epoch:31 step:29320 [D loss: 0.121217, acc.: 98.44%] [G loss: 5.002604]\n",
      "epoch:31 step:29321 [D loss: 0.171983, acc.: 93.75%] [G loss: 2.978432]\n",
      "epoch:31 step:29322 [D loss: 0.060081, acc.: 100.00%] [G loss: 3.136118]\n",
      "epoch:31 step:29323 [D loss: 0.256744, acc.: 89.84%] [G loss: 2.392836]\n",
      "epoch:31 step:29324 [D loss: 0.457269, acc.: 85.94%] [G loss: 2.608337]\n",
      "epoch:31 step:29325 [D loss: 0.153934, acc.: 98.44%] [G loss: 1.735362]\n",
      "epoch:31 step:29326 [D loss: 0.609640, acc.: 68.75%] [G loss: 0.750135]\n",
      "epoch:31 step:29327 [D loss: 0.052441, acc.: 100.00%] [G loss: 1.928081]\n",
      "epoch:31 step:29328 [D loss: 0.432934, acc.: 74.22%] [G loss: 2.537167]\n",
      "epoch:31 step:29329 [D loss: 0.058995, acc.: 99.22%] [G loss: 3.238386]\n",
      "epoch:31 step:29330 [D loss: 0.557119, acc.: 67.19%] [G loss: 1.650116]\n",
      "epoch:31 step:29331 [D loss: 0.422047, acc.: 76.56%] [G loss: 4.770689]\n",
      "epoch:31 step:29332 [D loss: 0.536204, acc.: 72.66%] [G loss: 5.859309]\n",
      "epoch:31 step:29333 [D loss: 0.078202, acc.: 99.22%] [G loss: 4.870538]\n",
      "epoch:31 step:29334 [D loss: 0.669078, acc.: 63.28%] [G loss: 4.493215]\n",
      "epoch:31 step:29335 [D loss: 0.226701, acc.: 94.53%] [G loss: 5.076848]\n",
      "epoch:31 step:29336 [D loss: 0.534953, acc.: 69.53%] [G loss: 3.651633]\n",
      "epoch:31 step:29337 [D loss: 0.123399, acc.: 98.44%] [G loss: 3.470702]\n",
      "epoch:31 step:29338 [D loss: 0.031558, acc.: 100.00%] [G loss: 2.320295]\n",
      "epoch:31 step:29339 [D loss: 0.340529, acc.: 87.50%] [G loss: 2.523054]\n",
      "epoch:31 step:29340 [D loss: 0.206698, acc.: 95.31%] [G loss: 2.177891]\n",
      "epoch:31 step:29341 [D loss: 0.382536, acc.: 89.06%] [G loss: 4.083916]\n",
      "epoch:31 step:29342 [D loss: 0.082584, acc.: 98.44%] [G loss: 4.761539]\n",
      "epoch:31 step:29343 [D loss: 0.236866, acc.: 92.19%] [G loss: 0.525625]\n",
      "epoch:31 step:29344 [D loss: 0.131881, acc.: 96.88%] [G loss: 6.166790]\n",
      "epoch:31 step:29345 [D loss: 0.927174, acc.: 48.44%] [G loss: 2.375927]\n",
      "epoch:31 step:29346 [D loss: 0.215865, acc.: 93.75%] [G loss: 2.933691]\n",
      "epoch:31 step:29347 [D loss: 0.334500, acc.: 83.59%] [G loss: 2.523150]\n",
      "epoch:31 step:29348 [D loss: 0.044867, acc.: 100.00%] [G loss: 2.941491]\n",
      "epoch:31 step:29349 [D loss: 0.495733, acc.: 75.78%] [G loss: 1.924842]\n",
      "epoch:31 step:29350 [D loss: 0.540605, acc.: 70.31%] [G loss: 3.469572]\n",
      "epoch:31 step:29351 [D loss: 0.107984, acc.: 99.22%] [G loss: 2.612080]\n",
      "epoch:31 step:29352 [D loss: 0.024857, acc.: 100.00%] [G loss: 2.999427]\n",
      "epoch:31 step:29353 [D loss: 0.288479, acc.: 86.72%] [G loss: 5.182239]\n",
      "epoch:31 step:29354 [D loss: 0.740532, acc.: 60.16%] [G loss: 2.986229]\n",
      "epoch:31 step:29355 [D loss: 0.277022, acc.: 88.28%] [G loss: 4.728770]\n",
      "epoch:31 step:29356 [D loss: 0.177182, acc.: 95.31%] [G loss: 2.447222]\n",
      "epoch:31 step:29357 [D loss: 0.230217, acc.: 91.41%] [G loss: 3.175490]\n",
      "epoch:31 step:29358 [D loss: 0.073205, acc.: 100.00%] [G loss: 2.889170]\n",
      "epoch:31 step:29359 [D loss: 0.365562, acc.: 85.94%] [G loss: 3.739742]\n",
      "epoch:31 step:29360 [D loss: 0.129586, acc.: 98.44%] [G loss: 4.385767]\n",
      "epoch:31 step:29361 [D loss: 0.348944, acc.: 82.03%] [G loss: 1.119358]\n",
      "epoch:31 step:29362 [D loss: 0.027322, acc.: 100.00%] [G loss: 1.421024]\n",
      "epoch:31 step:29363 [D loss: 0.045363, acc.: 100.00%] [G loss: 2.669803]\n",
      "epoch:31 step:29364 [D loss: 1.105623, acc.: 40.62%] [G loss: 2.266477]\n",
      "epoch:31 step:29365 [D loss: 0.268143, acc.: 87.50%] [G loss: 3.745815]\n",
      "epoch:31 step:29366 [D loss: 0.624461, acc.: 63.28%] [G loss: 1.581328]\n",
      "epoch:31 step:29367 [D loss: 0.216229, acc.: 95.31%] [G loss: 1.999038]\n",
      "epoch:31 step:29368 [D loss: 0.295232, acc.: 89.84%] [G loss: 0.675541]\n",
      "epoch:31 step:29369 [D loss: 1.124940, acc.: 53.12%] [G loss: 3.450261]\n",
      "epoch:31 step:29370 [D loss: 0.037290, acc.: 100.00%] [G loss: 3.694879]\n",
      "epoch:31 step:29371 [D loss: 0.634009, acc.: 59.38%] [G loss: 6.000941]\n",
      "epoch:31 step:29372 [D loss: 0.472583, acc.: 72.66%] [G loss: 4.226254]\n",
      "epoch:31 step:29373 [D loss: 0.229878, acc.: 92.19%] [G loss: 1.476857]\n",
      "epoch:31 step:29374 [D loss: 0.059368, acc.: 99.22%] [G loss: 3.361342]\n",
      "epoch:31 step:29375 [D loss: 0.195554, acc.: 97.66%] [G loss: 6.024927]\n",
      "epoch:31 step:29376 [D loss: 0.146927, acc.: 97.66%] [G loss: 3.129853]\n",
      "epoch:31 step:29377 [D loss: 0.079766, acc.: 99.22%] [G loss: 3.398903]\n",
      "epoch:31 step:29378 [D loss: 0.350778, acc.: 78.91%] [G loss: 2.763211]\n",
      "epoch:31 step:29379 [D loss: 0.139763, acc.: 97.66%] [G loss: 2.509897]\n",
      "epoch:31 step:29380 [D loss: 0.331564, acc.: 79.69%] [G loss: 3.597816]\n",
      "epoch:31 step:29381 [D loss: 0.049148, acc.: 100.00%] [G loss: 1.065786]\n",
      "epoch:31 step:29382 [D loss: 0.477556, acc.: 77.34%] [G loss: 1.286140]\n",
      "epoch:31 step:29383 [D loss: 0.474745, acc.: 78.91%] [G loss: 6.520580]\n",
      "epoch:31 step:29384 [D loss: 0.337309, acc.: 86.72%] [G loss: 2.320466]\n",
      "epoch:31 step:29385 [D loss: 0.157569, acc.: 98.44%] [G loss: 2.996489]\n",
      "epoch:31 step:29386 [D loss: 0.393232, acc.: 82.03%] [G loss: 2.868291]\n",
      "epoch:31 step:29387 [D loss: 0.161271, acc.: 96.88%] [G loss: 1.114774]\n",
      "epoch:31 step:29388 [D loss: 0.131855, acc.: 97.66%] [G loss: 3.300946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29389 [D loss: 0.184397, acc.: 93.75%] [G loss: 1.596941]\n",
      "epoch:31 step:29390 [D loss: 0.547574, acc.: 66.41%] [G loss: 1.302533]\n",
      "epoch:31 step:29391 [D loss: 0.059610, acc.: 98.44%] [G loss: 5.052672]\n",
      "epoch:31 step:29392 [D loss: 0.169091, acc.: 95.31%] [G loss: 6.573481]\n",
      "epoch:31 step:29393 [D loss: 0.140499, acc.: 96.09%] [G loss: 5.972642]\n",
      "epoch:31 step:29394 [D loss: 0.256147, acc.: 89.06%] [G loss: 4.061583]\n",
      "epoch:31 step:29395 [D loss: 0.152265, acc.: 99.22%] [G loss: 0.701237]\n",
      "epoch:31 step:29396 [D loss: 0.534473, acc.: 69.53%] [G loss: 0.892955]\n",
      "epoch:31 step:29397 [D loss: 0.947836, acc.: 56.25%] [G loss: 1.805182]\n",
      "epoch:31 step:29398 [D loss: 0.285345, acc.: 90.62%] [G loss: 1.950216]\n",
      "epoch:31 step:29399 [D loss: 0.170740, acc.: 95.31%] [G loss: 3.073340]\n",
      "epoch:31 step:29400 [D loss: 0.128106, acc.: 98.44%] [G loss: 3.040629]\n",
      "epoch:31 step:29401 [D loss: 0.132626, acc.: 99.22%] [G loss: 4.337400]\n",
      "epoch:31 step:29402 [D loss: 0.711672, acc.: 60.94%] [G loss: 1.707385]\n",
      "epoch:31 step:29403 [D loss: 0.175282, acc.: 96.88%] [G loss: 1.077414]\n",
      "epoch:31 step:29404 [D loss: 1.594151, acc.: 24.22%] [G loss: 2.872123]\n",
      "epoch:31 step:29405 [D loss: 0.452668, acc.: 73.44%] [G loss: 2.015934]\n",
      "epoch:31 step:29406 [D loss: 0.377145, acc.: 82.81%] [G loss: 0.971716]\n",
      "epoch:31 step:29407 [D loss: 0.649399, acc.: 62.50%] [G loss: 1.953126]\n",
      "epoch:31 step:29408 [D loss: 0.187800, acc.: 94.53%] [G loss: 3.067705]\n",
      "epoch:31 step:29409 [D loss: 0.252596, acc.: 88.28%] [G loss: 3.125697]\n",
      "epoch:31 step:29410 [D loss: 0.260286, acc.: 89.06%] [G loss: 3.676225]\n",
      "epoch:31 step:29411 [D loss: 0.107883, acc.: 96.88%] [G loss: 4.117615]\n",
      "epoch:31 step:29412 [D loss: 0.236095, acc.: 96.88%] [G loss: 2.323891]\n",
      "epoch:31 step:29413 [D loss: 0.417509, acc.: 84.38%] [G loss: 2.051835]\n",
      "epoch:31 step:29414 [D loss: 0.016283, acc.: 100.00%] [G loss: 5.253793]\n",
      "epoch:31 step:29415 [D loss: 0.171910, acc.: 95.31%] [G loss: 2.847426]\n",
      "epoch:31 step:29416 [D loss: 0.040600, acc.: 99.22%] [G loss: 4.240316]\n",
      "epoch:31 step:29417 [D loss: 0.464722, acc.: 76.56%] [G loss: 2.875870]\n",
      "epoch:31 step:29418 [D loss: 0.039107, acc.: 100.00%] [G loss: 4.356304]\n",
      "epoch:31 step:29419 [D loss: 0.206261, acc.: 96.09%] [G loss: 2.965765]\n",
      "epoch:31 step:29420 [D loss: 0.273228, acc.: 95.31%] [G loss: 2.579103]\n",
      "epoch:31 step:29421 [D loss: 0.436740, acc.: 83.59%] [G loss: 4.380006]\n",
      "epoch:31 step:29422 [D loss: 0.050595, acc.: 99.22%] [G loss: 4.850842]\n",
      "epoch:31 step:29423 [D loss: 0.285409, acc.: 92.19%] [G loss: 4.846234]\n",
      "epoch:31 step:29424 [D loss: 0.142254, acc.: 96.09%] [G loss: 3.606145]\n",
      "epoch:31 step:29425 [D loss: 0.125384, acc.: 95.31%] [G loss: 2.468015]\n",
      "epoch:31 step:29426 [D loss: 0.443814, acc.: 81.25%] [G loss: 6.661156]\n",
      "epoch:31 step:29427 [D loss: 1.444997, acc.: 45.31%] [G loss: 0.965321]\n",
      "epoch:31 step:29428 [D loss: 0.324738, acc.: 83.59%] [G loss: 5.468937]\n",
      "epoch:31 step:29429 [D loss: 0.148681, acc.: 96.09%] [G loss: 1.469402]\n",
      "epoch:31 step:29430 [D loss: 0.097688, acc.: 99.22%] [G loss: 6.079589]\n",
      "epoch:31 step:29431 [D loss: 0.924598, acc.: 48.44%] [G loss: 2.141964]\n",
      "epoch:31 step:29432 [D loss: 0.058012, acc.: 99.22%] [G loss: 0.459694]\n",
      "epoch:31 step:29433 [D loss: 0.132275, acc.: 98.44%] [G loss: 1.739141]\n",
      "epoch:31 step:29434 [D loss: 0.316934, acc.: 89.84%] [G loss: 1.647512]\n",
      "epoch:31 step:29435 [D loss: 0.511630, acc.: 72.66%] [G loss: 3.149228]\n",
      "epoch:31 step:29436 [D loss: 0.128784, acc.: 97.66%] [G loss: 0.936117]\n",
      "epoch:31 step:29437 [D loss: 0.256574, acc.: 93.75%] [G loss: 4.405393]\n",
      "epoch:31 step:29438 [D loss: 0.516825, acc.: 74.22%] [G loss: 4.331147]\n",
      "epoch:31 step:29439 [D loss: 0.107117, acc.: 96.88%] [G loss: 1.510948]\n",
      "epoch:31 step:29440 [D loss: 1.125441, acc.: 56.25%] [G loss: 2.429483]\n",
      "epoch:31 step:29441 [D loss: 0.270790, acc.: 92.97%] [G loss: 6.316041]\n",
      "epoch:31 step:29442 [D loss: 0.603826, acc.: 72.66%] [G loss: 2.825740]\n",
      "epoch:31 step:29443 [D loss: 0.230936, acc.: 92.19%] [G loss: 1.208508]\n",
      "epoch:31 step:29444 [D loss: 0.312784, acc.: 82.81%] [G loss: 3.883728]\n",
      "epoch:31 step:29445 [D loss: 0.153035, acc.: 95.31%] [G loss: 1.446306]\n",
      "epoch:31 step:29446 [D loss: 0.591215, acc.: 68.75%] [G loss: 1.264867]\n",
      "epoch:31 step:29447 [D loss: 0.485611, acc.: 74.22%] [G loss: 0.765805]\n",
      "epoch:31 step:29448 [D loss: 0.365031, acc.: 85.94%] [G loss: 3.033854]\n",
      "epoch:31 step:29449 [D loss: 0.224016, acc.: 92.97%] [G loss: 3.414059]\n",
      "epoch:31 step:29450 [D loss: 0.043832, acc.: 100.00%] [G loss: 3.253709]\n",
      "epoch:31 step:29451 [D loss: 0.095054, acc.: 97.66%] [G loss: 3.281434]\n",
      "epoch:31 step:29452 [D loss: 0.052932, acc.: 100.00%] [G loss: 3.462454]\n",
      "epoch:31 step:29453 [D loss: 0.361011, acc.: 87.50%] [G loss: 3.174824]\n",
      "epoch:31 step:29454 [D loss: 0.155383, acc.: 96.88%] [G loss: 2.219143]\n",
      "epoch:31 step:29455 [D loss: 0.149946, acc.: 98.44%] [G loss: 5.890218]\n",
      "epoch:31 step:29456 [D loss: 0.300893, acc.: 89.84%] [G loss: 3.538014]\n",
      "epoch:31 step:29457 [D loss: 0.936032, acc.: 39.06%] [G loss: 3.816237]\n",
      "epoch:31 step:29458 [D loss: 0.541342, acc.: 75.78%] [G loss: 5.202419]\n",
      "epoch:31 step:29459 [D loss: 0.716835, acc.: 59.38%] [G loss: 3.681855]\n",
      "epoch:31 step:29460 [D loss: 0.388268, acc.: 81.25%] [G loss: 5.998354]\n",
      "epoch:31 step:29461 [D loss: 0.056705, acc.: 99.22%] [G loss: 4.900930]\n",
      "epoch:31 step:29462 [D loss: 0.187223, acc.: 94.53%] [G loss: 2.988694]\n",
      "epoch:31 step:29463 [D loss: 0.056743, acc.: 99.22%] [G loss: 3.616015]\n",
      "epoch:31 step:29464 [D loss: 0.084688, acc.: 98.44%] [G loss: 4.412400]\n",
      "epoch:31 step:29465 [D loss: 0.040191, acc.: 100.00%] [G loss: 2.538101]\n",
      "epoch:31 step:29466 [D loss: 0.237793, acc.: 89.84%] [G loss: 4.574407]\n",
      "epoch:31 step:29467 [D loss: 0.857250, acc.: 58.59%] [G loss: 1.146890]\n",
      "epoch:31 step:29468 [D loss: 0.038999, acc.: 100.00%] [G loss: 1.354850]\n",
      "epoch:31 step:29469 [D loss: 0.555990, acc.: 64.06%] [G loss: 1.619877]\n",
      "epoch:31 step:29470 [D loss: 0.214401, acc.: 91.41%] [G loss: 4.430565]\n",
      "epoch:31 step:29471 [D loss: 0.150690, acc.: 94.53%] [G loss: 1.424568]\n",
      "epoch:31 step:29472 [D loss: 0.218270, acc.: 94.53%] [G loss: 3.831737]\n",
      "epoch:31 step:29473 [D loss: 0.103678, acc.: 96.09%] [G loss: 1.685977]\n",
      "epoch:31 step:29474 [D loss: 0.027294, acc.: 99.22%] [G loss: 3.571546]\n",
      "epoch:31 step:29475 [D loss: 0.104554, acc.: 96.09%] [G loss: 2.444338]\n",
      "epoch:31 step:29476 [D loss: 0.220725, acc.: 92.19%] [G loss: 1.442761]\n",
      "epoch:31 step:29477 [D loss: 0.128715, acc.: 99.22%] [G loss: 1.206508]\n",
      "epoch:31 step:29478 [D loss: 0.443958, acc.: 71.88%] [G loss: 6.099966]\n",
      "epoch:31 step:29479 [D loss: 1.709762, acc.: 46.09%] [G loss: 2.758264]\n",
      "epoch:31 step:29480 [D loss: 0.045178, acc.: 99.22%] [G loss: 1.897810]\n",
      "epoch:31 step:29481 [D loss: 0.244424, acc.: 94.53%] [G loss: 1.512491]\n",
      "epoch:31 step:29482 [D loss: 0.272964, acc.: 88.28%] [G loss: 5.799757]\n",
      "epoch:31 step:29483 [D loss: 0.118794, acc.: 96.88%] [G loss: 2.386908]\n",
      "epoch:31 step:29484 [D loss: 0.371211, acc.: 83.59%] [G loss: 4.307312]\n",
      "epoch:31 step:29485 [D loss: 0.075043, acc.: 99.22%] [G loss: 2.135342]\n",
      "epoch:31 step:29486 [D loss: 0.299486, acc.: 91.41%] [G loss: 3.569979]\n",
      "epoch:31 step:29487 [D loss: 0.470446, acc.: 75.78%] [G loss: 0.433315]\n",
      "epoch:31 step:29488 [D loss: 0.545483, acc.: 71.09%] [G loss: 0.908446]\n",
      "epoch:31 step:29489 [D loss: 0.462081, acc.: 75.00%] [G loss: 2.440572]\n",
      "epoch:31 step:29490 [D loss: 0.260949, acc.: 93.75%] [G loss: 3.378303]\n",
      "epoch:31 step:29491 [D loss: 0.454375, acc.: 83.59%] [G loss: 3.342607]\n",
      "epoch:31 step:29492 [D loss: 0.260016, acc.: 88.28%] [G loss: 2.023481]\n",
      "epoch:31 step:29493 [D loss: 0.953065, acc.: 53.12%] [G loss: 2.181915]\n",
      "epoch:31 step:29494 [D loss: 0.513481, acc.: 69.53%] [G loss: 2.732314]\n",
      "epoch:31 step:29495 [D loss: 0.491784, acc.: 78.12%] [G loss: 3.168075]\n",
      "epoch:31 step:29496 [D loss: 0.212242, acc.: 93.75%] [G loss: 2.515461]\n",
      "epoch:31 step:29497 [D loss: 0.341492, acc.: 82.81%] [G loss: 4.089378]\n",
      "epoch:31 step:29498 [D loss: 0.448681, acc.: 75.00%] [G loss: 2.828811]\n",
      "epoch:31 step:29499 [D loss: 1.028791, acc.: 46.88%] [G loss: 2.257355]\n",
      "epoch:31 step:29500 [D loss: 0.292639, acc.: 85.16%] [G loss: 2.760232]\n",
      "epoch:31 step:29501 [D loss: 0.918442, acc.: 45.31%] [G loss: 4.738028]\n",
      "epoch:31 step:29502 [D loss: 0.078341, acc.: 99.22%] [G loss: 6.696795]\n",
      "epoch:31 step:29503 [D loss: 0.235956, acc.: 92.97%] [G loss: 6.497482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29504 [D loss: 0.578269, acc.: 64.84%] [G loss: 1.968277]\n",
      "epoch:31 step:29505 [D loss: 0.187415, acc.: 92.97%] [G loss: 2.173033]\n",
      "epoch:31 step:29506 [D loss: 0.068356, acc.: 99.22%] [G loss: 2.850618]\n",
      "epoch:31 step:29507 [D loss: 0.379048, acc.: 87.50%] [G loss: 3.069467]\n",
      "epoch:31 step:29508 [D loss: 0.187018, acc.: 96.09%] [G loss: 5.088788]\n",
      "epoch:31 step:29509 [D loss: 0.183641, acc.: 96.09%] [G loss: 2.947438]\n",
      "epoch:31 step:29510 [D loss: 0.492011, acc.: 76.56%] [G loss: 1.053357]\n",
      "epoch:31 step:29511 [D loss: 0.041579, acc.: 100.00%] [G loss: 0.866674]\n",
      "epoch:31 step:29512 [D loss: 0.530791, acc.: 74.22%] [G loss: 3.962543]\n",
      "epoch:31 step:29513 [D loss: 0.045857, acc.: 100.00%] [G loss: 6.741259]\n",
      "epoch:31 step:29514 [D loss: 0.101320, acc.: 98.44%] [G loss: 2.990127]\n",
      "epoch:31 step:29515 [D loss: 0.069926, acc.: 100.00%] [G loss: 2.564901]\n",
      "epoch:31 step:29516 [D loss: 0.228199, acc.: 91.41%] [G loss: 4.719706]\n",
      "epoch:31 step:29517 [D loss: 0.314949, acc.: 88.28%] [G loss: 4.692992]\n",
      "epoch:31 step:29518 [D loss: 0.143247, acc.: 98.44%] [G loss: 2.737711]\n",
      "epoch:31 step:29519 [D loss: 0.333322, acc.: 80.47%] [G loss: 5.633918]\n",
      "epoch:31 step:29520 [D loss: 0.124874, acc.: 98.44%] [G loss: 4.806564]\n",
      "epoch:31 step:29521 [D loss: 0.460447, acc.: 80.47%] [G loss: 3.290456]\n",
      "epoch:31 step:29522 [D loss: 0.026075, acc.: 100.00%] [G loss: 5.078508]\n",
      "epoch:31 step:29523 [D loss: 0.092861, acc.: 96.88%] [G loss: 1.067836]\n",
      "epoch:31 step:29524 [D loss: 0.057919, acc.: 99.22%] [G loss: 2.623257]\n",
      "epoch:31 step:29525 [D loss: 0.487875, acc.: 71.09%] [G loss: 3.930292]\n",
      "epoch:31 step:29526 [D loss: 0.181586, acc.: 94.53%] [G loss: 3.306299]\n",
      "epoch:31 step:29527 [D loss: 0.048926, acc.: 100.00%] [G loss: 2.764981]\n",
      "epoch:31 step:29528 [D loss: 0.185915, acc.: 92.97%] [G loss: 2.570941]\n",
      "epoch:31 step:29529 [D loss: 0.132234, acc.: 96.88%] [G loss: 3.235149]\n",
      "epoch:31 step:29530 [D loss: 0.880149, acc.: 53.12%] [G loss: 4.587492]\n",
      "epoch:31 step:29531 [D loss: 0.186145, acc.: 92.97%] [G loss: 2.557174]\n",
      "epoch:31 step:29532 [D loss: 0.923275, acc.: 61.72%] [G loss: 2.174135]\n",
      "epoch:31 step:29533 [D loss: 0.146671, acc.: 97.66%] [G loss: 0.294022]\n",
      "epoch:31 step:29534 [D loss: 0.118742, acc.: 96.88%] [G loss: 1.968141]\n",
      "epoch:31 step:29535 [D loss: 0.281582, acc.: 88.28%] [G loss: 1.948266]\n",
      "epoch:31 step:29536 [D loss: 0.188152, acc.: 96.09%] [G loss: 4.040771]\n",
      "epoch:31 step:29537 [D loss: 0.367340, acc.: 78.91%] [G loss: 3.724878]\n",
      "epoch:31 step:29538 [D loss: 0.375030, acc.: 76.56%] [G loss: 0.780969]\n",
      "epoch:31 step:29539 [D loss: 0.265857, acc.: 94.53%] [G loss: 2.619987]\n",
      "epoch:31 step:29540 [D loss: 0.019675, acc.: 100.00%] [G loss: 0.862084]\n",
      "epoch:31 step:29541 [D loss: 0.156511, acc.: 97.66%] [G loss: 1.814450]\n",
      "epoch:31 step:29542 [D loss: 0.056623, acc.: 100.00%] [G loss: 0.834281]\n",
      "epoch:31 step:29543 [D loss: 0.177632, acc.: 96.09%] [G loss: 2.629082]\n",
      "epoch:31 step:29544 [D loss: 0.285796, acc.: 85.16%] [G loss: 1.907965]\n",
      "epoch:31 step:29545 [D loss: 0.212548, acc.: 95.31%] [G loss: 2.504936]\n",
      "epoch:31 step:29546 [D loss: 0.121728, acc.: 99.22%] [G loss: 2.180641]\n",
      "epoch:31 step:29547 [D loss: 0.640174, acc.: 63.28%] [G loss: 2.247586]\n",
      "epoch:31 step:29548 [D loss: 0.185904, acc.: 93.75%] [G loss: 3.183402]\n",
      "epoch:31 step:29549 [D loss: 0.141411, acc.: 97.66%] [G loss: 4.043301]\n",
      "epoch:31 step:29550 [D loss: 0.710540, acc.: 61.72%] [G loss: 1.678533]\n",
      "epoch:31 step:29551 [D loss: 4.458527, acc.: 1.56%] [G loss: 0.892047]\n",
      "epoch:31 step:29552 [D loss: 0.070697, acc.: 99.22%] [G loss: 2.477273]\n",
      "epoch:31 step:29553 [D loss: 0.370456, acc.: 82.81%] [G loss: 3.914824]\n",
      "epoch:31 step:29554 [D loss: 0.539228, acc.: 71.88%] [G loss: 1.425212]\n",
      "epoch:31 step:29555 [D loss: 0.235197, acc.: 90.62%] [G loss: 2.643208]\n",
      "epoch:31 step:29556 [D loss: 0.066283, acc.: 99.22%] [G loss: 3.990002]\n",
      "epoch:31 step:29557 [D loss: 0.087461, acc.: 100.00%] [G loss: 3.579556]\n",
      "epoch:31 step:29558 [D loss: 0.197349, acc.: 96.09%] [G loss: 0.767220]\n",
      "epoch:31 step:29559 [D loss: 0.527732, acc.: 71.88%] [G loss: 2.013530]\n",
      "epoch:31 step:29560 [D loss: 0.227558, acc.: 91.41%] [G loss: 1.960606]\n",
      "epoch:31 step:29561 [D loss: 0.097150, acc.: 98.44%] [G loss: 2.396065]\n",
      "epoch:31 step:29562 [D loss: 0.084783, acc.: 98.44%] [G loss: 0.491462]\n",
      "epoch:31 step:29563 [D loss: 0.421391, acc.: 75.78%] [G loss: 2.558404]\n",
      "epoch:31 step:29564 [D loss: 0.065297, acc.: 99.22%] [G loss: 5.286666]\n",
      "epoch:31 step:29565 [D loss: 0.093313, acc.: 99.22%] [G loss: 1.837728]\n",
      "epoch:31 step:29566 [D loss: 0.818426, acc.: 53.91%] [G loss: 2.851391]\n",
      "epoch:31 step:29567 [D loss: 0.211897, acc.: 95.31%] [G loss: 3.544312]\n",
      "epoch:31 step:29568 [D loss: 0.096942, acc.: 100.00%] [G loss: 3.509365]\n",
      "epoch:31 step:29569 [D loss: 0.067997, acc.: 99.22%] [G loss: 1.635330]\n",
      "epoch:31 step:29570 [D loss: 0.208918, acc.: 94.53%] [G loss: 3.132204]\n",
      "epoch:31 step:29571 [D loss: 0.131275, acc.: 98.44%] [G loss: 2.627223]\n",
      "epoch:31 step:29572 [D loss: 0.273696, acc.: 90.62%] [G loss: 3.207182]\n",
      "epoch:31 step:29573 [D loss: 0.285550, acc.: 94.53%] [G loss: 1.940876]\n",
      "epoch:31 step:29574 [D loss: 0.051155, acc.: 99.22%] [G loss: 0.988836]\n",
      "epoch:31 step:29575 [D loss: 0.077338, acc.: 98.44%] [G loss: 2.243218]\n",
      "epoch:31 step:29576 [D loss: 0.166177, acc.: 96.88%] [G loss: 1.236493]\n",
      "epoch:31 step:29577 [D loss: 0.500736, acc.: 66.41%] [G loss: 3.855692]\n",
      "epoch:31 step:29578 [D loss: 0.060659, acc.: 99.22%] [G loss: 4.758033]\n",
      "epoch:31 step:29579 [D loss: 0.292101, acc.: 89.06%] [G loss: 5.263037]\n",
      "epoch:31 step:29580 [D loss: 0.117392, acc.: 97.66%] [G loss: 3.550862]\n",
      "epoch:31 step:29581 [D loss: 0.232421, acc.: 92.19%] [G loss: 1.704814]\n",
      "epoch:31 step:29582 [D loss: 0.418063, acc.: 74.22%] [G loss: 6.769260]\n",
      "epoch:31 step:29583 [D loss: 0.197700, acc.: 92.97%] [G loss: 4.286373]\n",
      "epoch:31 step:29584 [D loss: 0.234301, acc.: 92.19%] [G loss: 2.680039]\n",
      "epoch:31 step:29585 [D loss: 0.075713, acc.: 100.00%] [G loss: 3.794865]\n",
      "epoch:31 step:29586 [D loss: 0.068945, acc.: 99.22%] [G loss: 3.215681]\n",
      "epoch:31 step:29587 [D loss: 0.537092, acc.: 72.66%] [G loss: 3.234744]\n",
      "epoch:31 step:29588 [D loss: 0.403695, acc.: 80.47%] [G loss: 2.114840]\n",
      "epoch:31 step:29589 [D loss: 0.107524, acc.: 98.44%] [G loss: 3.534242]\n",
      "epoch:31 step:29590 [D loss: 0.031439, acc.: 100.00%] [G loss: 2.906521]\n",
      "epoch:31 step:29591 [D loss: 0.262535, acc.: 91.41%] [G loss: 4.477270]\n",
      "epoch:31 step:29592 [D loss: 0.243328, acc.: 92.19%] [G loss: 3.227909]\n",
      "epoch:31 step:29593 [D loss: 0.446526, acc.: 74.22%] [G loss: 2.098165]\n",
      "epoch:31 step:29594 [D loss: 0.066267, acc.: 99.22%] [G loss: 6.613126]\n",
      "epoch:31 step:29595 [D loss: 0.166704, acc.: 97.66%] [G loss: 2.032906]\n",
      "epoch:31 step:29596 [D loss: 0.115160, acc.: 98.44%] [G loss: 5.052961]\n",
      "epoch:31 step:29597 [D loss: 0.283263, acc.: 90.62%] [G loss: 5.142356]\n",
      "epoch:31 step:29598 [D loss: 1.378761, acc.: 47.66%] [G loss: 5.029843]\n",
      "epoch:31 step:29599 [D loss: 0.283320, acc.: 87.50%] [G loss: 1.983816]\n",
      "epoch:31 step:29600 [D loss: 0.079050, acc.: 100.00%] [G loss: 2.335877]\n",
      "epoch:31 step:29601 [D loss: 0.277194, acc.: 89.84%] [G loss: 3.839358]\n",
      "epoch:31 step:29602 [D loss: 0.475141, acc.: 76.56%] [G loss: 1.138778]\n",
      "epoch:31 step:29603 [D loss: 1.284991, acc.: 51.56%] [G loss: 1.820789]\n",
      "epoch:31 step:29604 [D loss: 0.217572, acc.: 94.53%] [G loss: 3.182617]\n",
      "epoch:31 step:29605 [D loss: 0.856564, acc.: 60.94%] [G loss: 3.696892]\n",
      "epoch:31 step:29606 [D loss: 0.496215, acc.: 72.66%] [G loss: 2.542200]\n",
      "epoch:31 step:29607 [D loss: 0.580280, acc.: 67.97%] [G loss: 2.673457]\n",
      "epoch:31 step:29608 [D loss: 0.177301, acc.: 96.88%] [G loss: 4.750421]\n",
      "epoch:31 step:29609 [D loss: 0.177354, acc.: 94.53%] [G loss: 3.051795]\n",
      "epoch:31 step:29610 [D loss: 0.296486, acc.: 91.41%] [G loss: 2.043355]\n",
      "epoch:31 step:29611 [D loss: 0.129450, acc.: 96.88%] [G loss: 1.910055]\n",
      "epoch:31 step:29612 [D loss: 0.273285, acc.: 90.62%] [G loss: 3.270813]\n",
      "epoch:31 step:29613 [D loss: 0.437339, acc.: 78.12%] [G loss: 0.760287]\n",
      "epoch:31 step:29614 [D loss: 1.339497, acc.: 32.81%] [G loss: 3.421823]\n",
      "epoch:31 step:29615 [D loss: 0.284205, acc.: 91.41%] [G loss: 1.296736]\n",
      "epoch:31 step:29616 [D loss: 0.886340, acc.: 55.47%] [G loss: 1.944826]\n",
      "epoch:31 step:29617 [D loss: 0.802120, acc.: 60.94%] [G loss: 0.881298]\n",
      "epoch:31 step:29618 [D loss: 1.796868, acc.: 53.91%] [G loss: 2.558565]\n",
      "epoch:31 step:29619 [D loss: 0.054983, acc.: 99.22%] [G loss: 5.556429]\n",
      "epoch:31 step:29620 [D loss: 0.672247, acc.: 64.06%] [G loss: 1.604868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29621 [D loss: 0.162164, acc.: 96.09%] [G loss: 3.723837]\n",
      "epoch:31 step:29622 [D loss: 0.098684, acc.: 98.44%] [G loss: 5.058446]\n",
      "epoch:31 step:29623 [D loss: 0.072861, acc.: 99.22%] [G loss: 4.383954]\n",
      "epoch:31 step:29624 [D loss: 0.032358, acc.: 100.00%] [G loss: 2.406847]\n",
      "epoch:31 step:29625 [D loss: 0.144893, acc.: 97.66%] [G loss: 3.996209]\n",
      "epoch:31 step:29626 [D loss: 0.266625, acc.: 88.28%] [G loss: 2.713558]\n",
      "epoch:31 step:29627 [D loss: 0.160854, acc.: 97.66%] [G loss: 2.345124]\n",
      "epoch:31 step:29628 [D loss: 0.296970, acc.: 94.53%] [G loss: 3.187589]\n",
      "epoch:31 step:29629 [D loss: 0.689918, acc.: 58.59%] [G loss: 1.960800]\n",
      "epoch:31 step:29630 [D loss: 0.037470, acc.: 100.00%] [G loss: 3.186007]\n",
      "epoch:31 step:29631 [D loss: 0.257352, acc.: 92.97%] [G loss: 3.789145]\n",
      "epoch:31 step:29632 [D loss: 0.054517, acc.: 99.22%] [G loss: 3.170930]\n",
      "epoch:31 step:29633 [D loss: 0.091445, acc.: 97.66%] [G loss: 5.315858]\n",
      "epoch:31 step:29634 [D loss: 0.325734, acc.: 88.28%] [G loss: 2.556191]\n",
      "epoch:31 step:29635 [D loss: 0.325207, acc.: 89.84%] [G loss: 2.817688]\n",
      "epoch:31 step:29636 [D loss: 0.088040, acc.: 99.22%] [G loss: 1.574347]\n",
      "epoch:31 step:29637 [D loss: 1.210375, acc.: 38.28%] [G loss: 0.830469]\n",
      "epoch:31 step:29638 [D loss: 0.275374, acc.: 92.97%] [G loss: 4.934818]\n",
      "epoch:31 step:29639 [D loss: 0.233834, acc.: 93.75%] [G loss: 4.120677]\n",
      "epoch:31 step:29640 [D loss: 0.154410, acc.: 96.09%] [G loss: 1.945564]\n",
      "epoch:31 step:29641 [D loss: 0.100841, acc.: 99.22%] [G loss: 3.308797]\n",
      "epoch:31 step:29642 [D loss: 0.525234, acc.: 80.47%] [G loss: 3.962537]\n",
      "epoch:31 step:29643 [D loss: 0.133572, acc.: 98.44%] [G loss: 3.133587]\n",
      "epoch:31 step:29644 [D loss: 0.052392, acc.: 98.44%] [G loss: 1.590532]\n",
      "epoch:31 step:29645 [D loss: 0.325902, acc.: 83.59%] [G loss: 1.606652]\n",
      "epoch:31 step:29646 [D loss: 0.051284, acc.: 100.00%] [G loss: 1.175665]\n",
      "epoch:31 step:29647 [D loss: 0.315506, acc.: 89.84%] [G loss: 3.880010]\n",
      "epoch:31 step:29648 [D loss: 0.270197, acc.: 93.75%] [G loss: 2.895627]\n",
      "epoch:31 step:29649 [D loss: 1.551645, acc.: 51.56%] [G loss: 1.562419]\n",
      "epoch:31 step:29650 [D loss: 0.776217, acc.: 57.81%] [G loss: 2.117234]\n",
      "epoch:31 step:29651 [D loss: 1.165953, acc.: 51.56%] [G loss: 4.324947]\n",
      "epoch:31 step:29652 [D loss: 0.459146, acc.: 73.44%] [G loss: 1.030679]\n",
      "epoch:31 step:29653 [D loss: 0.231921, acc.: 92.19%] [G loss: 4.031882]\n",
      "epoch:31 step:29654 [D loss: 0.403834, acc.: 74.22%] [G loss: 1.452749]\n",
      "epoch:31 step:29655 [D loss: 0.254003, acc.: 97.66%] [G loss: 1.653475]\n",
      "epoch:31 step:29656 [D loss: 0.174138, acc.: 97.66%] [G loss: 1.989423]\n",
      "epoch:31 step:29657 [D loss: 0.837567, acc.: 43.75%] [G loss: 2.685054]\n",
      "epoch:31 step:29658 [D loss: 0.197968, acc.: 95.31%] [G loss: 1.861321]\n",
      "epoch:31 step:29659 [D loss: 0.825509, acc.: 53.91%] [G loss: 1.430091]\n",
      "epoch:31 step:29660 [D loss: 0.094291, acc.: 99.22%] [G loss: 0.928166]\n",
      "epoch:31 step:29661 [D loss: 0.132052, acc.: 97.66%] [G loss: 2.326535]\n",
      "epoch:31 step:29662 [D loss: 0.211421, acc.: 91.41%] [G loss: 3.342344]\n",
      "epoch:31 step:29663 [D loss: 0.507901, acc.: 74.22%] [G loss: 2.732704]\n",
      "epoch:31 step:29664 [D loss: 0.189982, acc.: 96.09%] [G loss: 2.832121]\n",
      "epoch:31 step:29665 [D loss: 0.634073, acc.: 64.06%] [G loss: 1.098104]\n",
      "epoch:31 step:29666 [D loss: 0.362426, acc.: 86.72%] [G loss: 2.325350]\n",
      "epoch:31 step:29667 [D loss: 0.036007, acc.: 100.00%] [G loss: 3.304477]\n",
      "epoch:31 step:29668 [D loss: 0.097098, acc.: 98.44%] [G loss: 1.279054]\n",
      "epoch:31 step:29669 [D loss: 0.084146, acc.: 99.22%] [G loss: 1.227989]\n",
      "epoch:31 step:29670 [D loss: 1.092126, acc.: 40.62%] [G loss: 3.044034]\n",
      "epoch:31 step:29671 [D loss: 0.150346, acc.: 97.66%] [G loss: 0.939294]\n",
      "epoch:31 step:29672 [D loss: 0.778155, acc.: 57.81%] [G loss: 3.192970]\n",
      "epoch:31 step:29673 [D loss: 0.407122, acc.: 85.94%] [G loss: 4.854860]\n",
      "epoch:31 step:29674 [D loss: 0.214889, acc.: 92.19%] [G loss: 4.805928]\n",
      "epoch:31 step:29675 [D loss: 0.241351, acc.: 95.31%] [G loss: 0.636581]\n",
      "epoch:31 step:29676 [D loss: 0.087676, acc.: 96.88%] [G loss: 1.458961]\n",
      "epoch:31 step:29677 [D loss: 0.169388, acc.: 96.09%] [G loss: 3.845430]\n",
      "epoch:31 step:29678 [D loss: 0.172560, acc.: 96.88%] [G loss: 3.422119]\n",
      "epoch:31 step:29679 [D loss: 0.565116, acc.: 66.41%] [G loss: 3.938203]\n",
      "epoch:31 step:29680 [D loss: 0.208080, acc.: 92.19%] [G loss: 4.403821]\n",
      "epoch:31 step:29681 [D loss: 0.048930, acc.: 100.00%] [G loss: 2.803333]\n",
      "epoch:31 step:29682 [D loss: 0.097882, acc.: 99.22%] [G loss: 3.106343]\n",
      "epoch:31 step:29683 [D loss: 0.100557, acc.: 99.22%] [G loss: 3.402037]\n",
      "epoch:31 step:29684 [D loss: 0.478118, acc.: 67.97%] [G loss: 3.869920]\n",
      "epoch:31 step:29685 [D loss: 0.337980, acc.: 82.81%] [G loss: 1.161971]\n",
      "epoch:31 step:29686 [D loss: 0.143843, acc.: 98.44%] [G loss: 2.959182]\n",
      "epoch:31 step:29687 [D loss: 0.143088, acc.: 95.31%] [G loss: 4.247033]\n",
      "epoch:31 step:29688 [D loss: 0.106365, acc.: 99.22%] [G loss: 3.565327]\n",
      "epoch:31 step:29689 [D loss: 0.712258, acc.: 58.59%] [G loss: 3.383658]\n",
      "epoch:31 step:29690 [D loss: 0.134350, acc.: 99.22%] [G loss: 3.980373]\n",
      "epoch:31 step:29691 [D loss: 0.319243, acc.: 86.72%] [G loss: 5.999430]\n",
      "epoch:31 step:29692 [D loss: 0.975378, acc.: 53.12%] [G loss: 4.135003]\n",
      "epoch:31 step:29693 [D loss: 0.050996, acc.: 96.88%] [G loss: 2.324271]\n",
      "epoch:31 step:29694 [D loss: 0.127929, acc.: 97.66%] [G loss: 1.468446]\n",
      "epoch:31 step:29695 [D loss: 0.523658, acc.: 73.44%] [G loss: 2.956517]\n",
      "epoch:31 step:29696 [D loss: 0.106756, acc.: 96.88%] [G loss: 4.883970]\n",
      "epoch:31 step:29697 [D loss: 0.608317, acc.: 67.19%] [G loss: 3.934396]\n",
      "epoch:31 step:29698 [D loss: 0.061106, acc.: 99.22%] [G loss: 0.324038]\n",
      "epoch:31 step:29699 [D loss: 0.142089, acc.: 98.44%] [G loss: 4.268782]\n",
      "epoch:31 step:29700 [D loss: 0.240991, acc.: 93.75%] [G loss: 4.195381]\n",
      "epoch:31 step:29701 [D loss: 0.174159, acc.: 95.31%] [G loss: 2.732117]\n",
      "epoch:31 step:29702 [D loss: 0.349897, acc.: 89.06%] [G loss: 4.175004]\n",
      "epoch:31 step:29703 [D loss: 0.280530, acc.: 84.38%] [G loss: 1.046636]\n",
      "epoch:31 step:29704 [D loss: 0.219370, acc.: 90.62%] [G loss: 2.525775]\n",
      "epoch:31 step:29705 [D loss: 0.812383, acc.: 56.25%] [G loss: 2.733899]\n",
      "epoch:31 step:29706 [D loss: 0.296568, acc.: 89.84%] [G loss: 3.762672]\n",
      "epoch:31 step:29707 [D loss: 0.036544, acc.: 100.00%] [G loss: 4.090075]\n",
      "epoch:31 step:29708 [D loss: 0.087526, acc.: 97.66%] [G loss: 5.812895]\n",
      "epoch:31 step:29709 [D loss: 0.163380, acc.: 96.88%] [G loss: 4.397524]\n",
      "epoch:31 step:29710 [D loss: 0.128566, acc.: 96.09%] [G loss: 3.828780]\n",
      "epoch:31 step:29711 [D loss: 0.026997, acc.: 99.22%] [G loss: 1.328692]\n",
      "epoch:31 step:29712 [D loss: 0.059429, acc.: 100.00%] [G loss: 1.850898]\n",
      "epoch:31 step:29713 [D loss: 0.188504, acc.: 95.31%] [G loss: 1.754014]\n",
      "epoch:31 step:29714 [D loss: 0.459614, acc.: 78.91%] [G loss: 1.984948]\n",
      "epoch:31 step:29715 [D loss: 0.523225, acc.: 68.75%] [G loss: 3.488866]\n",
      "epoch:31 step:29716 [D loss: 0.387093, acc.: 75.78%] [G loss: 4.671693]\n",
      "epoch:31 step:29717 [D loss: 0.095488, acc.: 98.44%] [G loss: 5.189935]\n",
      "epoch:31 step:29718 [D loss: 1.283088, acc.: 32.03%] [G loss: 2.581145]\n",
      "epoch:31 step:29719 [D loss: 0.387255, acc.: 81.25%] [G loss: 0.528351]\n",
      "epoch:31 step:29720 [D loss: 0.322154, acc.: 89.06%] [G loss: 3.604270]\n",
      "epoch:31 step:29721 [D loss: 0.323571, acc.: 83.59%] [G loss: 3.430536]\n",
      "epoch:31 step:29722 [D loss: 0.451347, acc.: 78.12%] [G loss: 3.538924]\n",
      "epoch:31 step:29723 [D loss: 0.467162, acc.: 76.56%] [G loss: 2.986965]\n",
      "epoch:31 step:29724 [D loss: 0.187841, acc.: 96.88%] [G loss: 5.934746]\n",
      "epoch:31 step:29725 [D loss: 0.633121, acc.: 67.19%] [G loss: 3.115591]\n",
      "epoch:31 step:29726 [D loss: 0.331363, acc.: 88.28%] [G loss: 6.398625]\n",
      "epoch:31 step:29727 [D loss: 0.111609, acc.: 98.44%] [G loss: 3.895642]\n",
      "epoch:31 step:29728 [D loss: 1.097688, acc.: 54.69%] [G loss: 2.569584]\n",
      "epoch:31 step:29729 [D loss: 0.081786, acc.: 98.44%] [G loss: 2.251908]\n",
      "epoch:31 step:29730 [D loss: 0.199904, acc.: 96.09%] [G loss: 1.086935]\n",
      "epoch:31 step:29731 [D loss: 0.404952, acc.: 83.59%] [G loss: 2.696686]\n",
      "epoch:31 step:29732 [D loss: 0.025161, acc.: 100.00%] [G loss: 3.681492]\n",
      "epoch:31 step:29733 [D loss: 0.536940, acc.: 68.75%] [G loss: 5.124013]\n",
      "epoch:31 step:29734 [D loss: 0.018550, acc.: 100.00%] [G loss: 3.684393]\n",
      "epoch:31 step:29735 [D loss: 0.692135, acc.: 67.97%] [G loss: 2.422289]\n",
      "epoch:31 step:29736 [D loss: 0.290171, acc.: 89.06%] [G loss: 4.305297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29737 [D loss: 0.133463, acc.: 99.22%] [G loss: 5.328279]\n",
      "epoch:31 step:29738 [D loss: 0.112028, acc.: 98.44%] [G loss: 1.888201]\n",
      "epoch:31 step:29739 [D loss: 0.030783, acc.: 100.00%] [G loss: 4.032495]\n",
      "epoch:31 step:29740 [D loss: 0.714820, acc.: 63.28%] [G loss: 3.885771]\n",
      "epoch:31 step:29741 [D loss: 0.089510, acc.: 98.44%] [G loss: 3.718791]\n",
      "epoch:31 step:29742 [D loss: 0.449854, acc.: 73.44%] [G loss: 3.883948]\n",
      "epoch:31 step:29743 [D loss: 0.050829, acc.: 99.22%] [G loss: 3.742582]\n",
      "epoch:31 step:29744 [D loss: 0.328901, acc.: 90.62%] [G loss: 3.920421]\n",
      "epoch:31 step:29745 [D loss: 0.093644, acc.: 97.66%] [G loss: 5.623455]\n",
      "epoch:31 step:29746 [D loss: 0.314176, acc.: 81.25%] [G loss: 5.190276]\n",
      "epoch:31 step:29747 [D loss: 0.805421, acc.: 58.59%] [G loss: 4.100182]\n",
      "epoch:31 step:29748 [D loss: 0.248936, acc.: 93.75%] [G loss: 2.493677]\n",
      "epoch:31 step:29749 [D loss: 0.580611, acc.: 71.88%] [G loss: 3.223806]\n",
      "epoch:31 step:29750 [D loss: 0.927725, acc.: 42.19%] [G loss: 3.714968]\n",
      "epoch:31 step:29751 [D loss: 0.585718, acc.: 63.28%] [G loss: 6.505119]\n",
      "epoch:31 step:29752 [D loss: 0.585167, acc.: 67.19%] [G loss: 6.600089]\n",
      "epoch:31 step:29753 [D loss: 0.045813, acc.: 99.22%] [G loss: 0.900799]\n",
      "epoch:31 step:29754 [D loss: 0.343732, acc.: 83.59%] [G loss: 4.858300]\n",
      "epoch:31 step:29755 [D loss: 0.422576, acc.: 80.47%] [G loss: 4.245019]\n",
      "epoch:31 step:29756 [D loss: 0.126126, acc.: 99.22%] [G loss: 1.587259]\n",
      "epoch:31 step:29757 [D loss: 0.062265, acc.: 98.44%] [G loss: 1.666850]\n",
      "epoch:31 step:29758 [D loss: 0.190929, acc.: 93.75%] [G loss: 1.402104]\n",
      "epoch:31 step:29759 [D loss: 0.254787, acc.: 90.62%] [G loss: 1.896365]\n",
      "epoch:31 step:29760 [D loss: 0.068685, acc.: 99.22%] [G loss: 2.769346]\n",
      "epoch:31 step:29761 [D loss: 0.020577, acc.: 100.00%] [G loss: 2.829873]\n",
      "epoch:31 step:29762 [D loss: 0.448136, acc.: 68.75%] [G loss: 2.830375]\n",
      "epoch:31 step:29763 [D loss: 0.305303, acc.: 84.38%] [G loss: 1.828466]\n",
      "epoch:31 step:29764 [D loss: 0.091909, acc.: 97.66%] [G loss: 3.866347]\n",
      "epoch:31 step:29765 [D loss: 0.058669, acc.: 99.22%] [G loss: 3.028951]\n",
      "epoch:31 step:29766 [D loss: 0.233147, acc.: 92.97%] [G loss: 2.704730]\n",
      "epoch:31 step:29767 [D loss: 0.627276, acc.: 67.19%] [G loss: 6.507518]\n",
      "epoch:31 step:29768 [D loss: 0.022690, acc.: 100.00%] [G loss: 2.669507]\n",
      "epoch:31 step:29769 [D loss: 0.032050, acc.: 100.00%] [G loss: 4.746569]\n",
      "epoch:31 step:29770 [D loss: 0.096356, acc.: 99.22%] [G loss: 1.855167]\n",
      "epoch:31 step:29771 [D loss: 0.503722, acc.: 79.69%] [G loss: 3.036746]\n",
      "epoch:31 step:29772 [D loss: 0.360295, acc.: 80.47%] [G loss: 3.254300]\n",
      "epoch:31 step:29773 [D loss: 0.221779, acc.: 96.09%] [G loss: 3.904819]\n",
      "epoch:31 step:29774 [D loss: 0.012753, acc.: 100.00%] [G loss: 5.373323]\n",
      "epoch:31 step:29775 [D loss: 0.916421, acc.: 55.47%] [G loss: 3.400550]\n",
      "epoch:31 step:29776 [D loss: 0.049302, acc.: 100.00%] [G loss: 4.225943]\n",
      "epoch:31 step:29777 [D loss: 0.282437, acc.: 86.72%] [G loss: 4.519614]\n",
      "epoch:31 step:29778 [D loss: 0.321415, acc.: 85.16%] [G loss: 3.621388]\n",
      "epoch:31 step:29779 [D loss: 0.198554, acc.: 96.88%] [G loss: 3.932058]\n",
      "epoch:31 step:29780 [D loss: 0.318043, acc.: 90.62%] [G loss: 5.044565]\n",
      "epoch:31 step:29781 [D loss: 0.517426, acc.: 72.66%] [G loss: 3.342440]\n",
      "epoch:31 step:29782 [D loss: 0.497854, acc.: 70.31%] [G loss: 3.583253]\n",
      "epoch:31 step:29783 [D loss: 0.535148, acc.: 68.75%] [G loss: 3.294420]\n",
      "epoch:31 step:29784 [D loss: 0.033276, acc.: 99.22%] [G loss: 3.904940]\n",
      "epoch:31 step:29785 [D loss: 0.100116, acc.: 99.22%] [G loss: 1.963105]\n",
      "epoch:31 step:29786 [D loss: 0.096672, acc.: 98.44%] [G loss: 2.605322]\n",
      "epoch:31 step:29787 [D loss: 0.245205, acc.: 90.62%] [G loss: 3.194390]\n",
      "epoch:31 step:29788 [D loss: 0.568466, acc.: 68.75%] [G loss: 5.079519]\n",
      "epoch:31 step:29789 [D loss: 0.056808, acc.: 100.00%] [G loss: 6.860674]\n",
      "epoch:31 step:29790 [D loss: 0.038728, acc.: 100.00%] [G loss: 4.326984]\n",
      "epoch:31 step:29791 [D loss: 0.046074, acc.: 99.22%] [G loss: 1.799964]\n",
      "epoch:31 step:29792 [D loss: 0.034846, acc.: 100.00%] [G loss: 3.506452]\n",
      "epoch:31 step:29793 [D loss: 0.146330, acc.: 98.44%] [G loss: 3.145283]\n",
      "epoch:31 step:29794 [D loss: 0.653070, acc.: 64.84%] [G loss: 3.136206]\n",
      "epoch:31 step:29795 [D loss: 0.503780, acc.: 65.62%] [G loss: 3.094878]\n",
      "epoch:31 step:29796 [D loss: 0.079839, acc.: 100.00%] [G loss: 5.257468]\n",
      "epoch:31 step:29797 [D loss: 0.040796, acc.: 99.22%] [G loss: 5.265731]\n",
      "epoch:31 step:29798 [D loss: 0.390456, acc.: 78.91%] [G loss: 4.145925]\n",
      "epoch:31 step:29799 [D loss: 0.628861, acc.: 63.28%] [G loss: 2.597972]\n",
      "epoch:31 step:29800 [D loss: 0.266162, acc.: 86.72%] [G loss: 2.245560]\n",
      "epoch:31 step:29801 [D loss: 0.187865, acc.: 91.41%] [G loss: 4.390123]\n",
      "epoch:31 step:29802 [D loss: 0.205198, acc.: 95.31%] [G loss: 6.084410]\n",
      "epoch:31 step:29803 [D loss: 0.048543, acc.: 100.00%] [G loss: 3.544347]\n",
      "epoch:31 step:29804 [D loss: 0.846118, acc.: 54.69%] [G loss: 4.154903]\n",
      "epoch:31 step:29805 [D loss: 0.035025, acc.: 99.22%] [G loss: 0.515683]\n",
      "epoch:31 step:29806 [D loss: 0.093757, acc.: 97.66%] [G loss: 3.196340]\n",
      "epoch:31 step:29807 [D loss: 0.910233, acc.: 57.03%] [G loss: 4.149665]\n",
      "epoch:31 step:29808 [D loss: 0.469577, acc.: 72.66%] [G loss: 1.806661]\n",
      "epoch:31 step:29809 [D loss: 0.073970, acc.: 98.44%] [G loss: 3.248135]\n",
      "epoch:31 step:29810 [D loss: 0.108729, acc.: 98.44%] [G loss: 3.612628]\n",
      "epoch:31 step:29811 [D loss: 0.370128, acc.: 81.25%] [G loss: 3.752171]\n",
      "epoch:31 step:29812 [D loss: 0.224820, acc.: 93.75%] [G loss: 3.546902]\n",
      "epoch:31 step:29813 [D loss: 0.472628, acc.: 74.22%] [G loss: 0.667327]\n",
      "epoch:31 step:29814 [D loss: 0.070497, acc.: 99.22%] [G loss: 4.599849]\n",
      "epoch:31 step:29815 [D loss: 0.200020, acc.: 89.84%] [G loss: 3.045701]\n",
      "epoch:31 step:29816 [D loss: 0.420403, acc.: 85.94%] [G loss: 1.117951]\n",
      "epoch:31 step:29817 [D loss: 0.130013, acc.: 98.44%] [G loss: 2.325016]\n",
      "epoch:31 step:29818 [D loss: 0.128588, acc.: 97.66%] [G loss: 2.535503]\n",
      "epoch:31 step:29819 [D loss: 0.331827, acc.: 87.50%] [G loss: 3.902483]\n",
      "epoch:31 step:29820 [D loss: 0.825345, acc.: 58.59%] [G loss: 1.123427]\n",
      "epoch:31 step:29821 [D loss: 0.102857, acc.: 99.22%] [G loss: 4.416260]\n",
      "epoch:31 step:29822 [D loss: 0.413443, acc.: 81.25%] [G loss: 3.684656]\n",
      "epoch:31 step:29823 [D loss: 0.539007, acc.: 67.97%] [G loss: 4.396411]\n",
      "epoch:31 step:29824 [D loss: 0.081932, acc.: 99.22%] [G loss: 3.563560]\n",
      "epoch:31 step:29825 [D loss: 0.363301, acc.: 89.06%] [G loss: 6.109299]\n",
      "epoch:31 step:29826 [D loss: 0.299804, acc.: 93.75%] [G loss: 2.793225]\n",
      "epoch:31 step:29827 [D loss: 0.059510, acc.: 100.00%] [G loss: 2.282073]\n",
      "epoch:31 step:29828 [D loss: 0.064538, acc.: 99.22%] [G loss: 3.604860]\n",
      "epoch:31 step:29829 [D loss: 0.200789, acc.: 96.88%] [G loss: 4.120204]\n",
      "epoch:31 step:29830 [D loss: 0.054936, acc.: 100.00%] [G loss: 5.587255]\n",
      "epoch:31 step:29831 [D loss: 0.215122, acc.: 96.09%] [G loss: 1.297178]\n",
      "epoch:31 step:29832 [D loss: 0.185167, acc.: 93.75%] [G loss: 3.709865]\n",
      "epoch:31 step:29833 [D loss: 0.152473, acc.: 96.09%] [G loss: 6.149942]\n",
      "epoch:31 step:29834 [D loss: 0.257906, acc.: 92.19%] [G loss: 2.946543]\n",
      "epoch:31 step:29835 [D loss: 0.234263, acc.: 94.53%] [G loss: 2.851520]\n",
      "epoch:31 step:29836 [D loss: 0.459647, acc.: 78.91%] [G loss: 4.361748]\n",
      "epoch:31 step:29837 [D loss: 0.125660, acc.: 96.09%] [G loss: 2.178400]\n",
      "epoch:31 step:29838 [D loss: 0.041482, acc.: 99.22%] [G loss: 3.546484]\n",
      "epoch:31 step:29839 [D loss: 0.132744, acc.: 97.66%] [G loss: 3.147061]\n",
      "epoch:31 step:29840 [D loss: 0.024513, acc.: 100.00%] [G loss: 3.333537]\n",
      "epoch:31 step:29841 [D loss: 0.341719, acc.: 87.50%] [G loss: 2.245333]\n",
      "epoch:31 step:29842 [D loss: 0.233206, acc.: 91.41%] [G loss: 5.107508]\n",
      "epoch:31 step:29843 [D loss: 0.259488, acc.: 93.75%] [G loss: 3.821940]\n",
      "epoch:31 step:29844 [D loss: 0.199748, acc.: 93.75%] [G loss: 5.023366]\n",
      "epoch:31 step:29845 [D loss: 0.878889, acc.: 47.66%] [G loss: 3.375213]\n",
      "epoch:31 step:29846 [D loss: 0.084567, acc.: 100.00%] [G loss: 5.583876]\n",
      "epoch:31 step:29847 [D loss: 0.109048, acc.: 99.22%] [G loss: 4.584283]\n",
      "epoch:31 step:29848 [D loss: 0.102720, acc.: 96.88%] [G loss: 2.778157]\n",
      "epoch:31 step:29849 [D loss: 0.297682, acc.: 84.38%] [G loss: 3.457967]\n",
      "epoch:31 step:29850 [D loss: 0.174669, acc.: 93.75%] [G loss: 2.549014]\n",
      "epoch:31 step:29851 [D loss: 0.014806, acc.: 100.00%] [G loss: 3.564231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29852 [D loss: 0.034234, acc.: 100.00%] [G loss: 3.892297]\n",
      "epoch:31 step:29853 [D loss: 0.070399, acc.: 100.00%] [G loss: 5.267875]\n",
      "epoch:31 step:29854 [D loss: 0.509368, acc.: 65.62%] [G loss: 3.917188]\n",
      "epoch:31 step:29855 [D loss: 1.070807, acc.: 50.78%] [G loss: 5.606351]\n",
      "epoch:31 step:29856 [D loss: 0.758794, acc.: 56.25%] [G loss: 3.523527]\n",
      "epoch:31 step:29857 [D loss: 0.209094, acc.: 96.88%] [G loss: 4.848656]\n",
      "epoch:31 step:29858 [D loss: 0.173579, acc.: 96.88%] [G loss: 4.168183]\n",
      "epoch:31 step:29859 [D loss: 0.074544, acc.: 98.44%] [G loss: 3.811934]\n",
      "epoch:31 step:29860 [D loss: 0.514306, acc.: 73.44%] [G loss: 2.721739]\n",
      "epoch:31 step:29861 [D loss: 0.274148, acc.: 85.94%] [G loss: 6.001771]\n",
      "epoch:31 step:29862 [D loss: 0.132959, acc.: 96.09%] [G loss: 1.440675]\n",
      "epoch:31 step:29863 [D loss: 0.133801, acc.: 97.66%] [G loss: 1.249236]\n",
      "epoch:31 step:29864 [D loss: 0.089795, acc.: 98.44%] [G loss: 1.824537]\n",
      "epoch:31 step:29865 [D loss: 0.150137, acc.: 96.88%] [G loss: 0.164118]\n",
      "epoch:31 step:29866 [D loss: 0.329155, acc.: 82.81%] [G loss: 1.881565]\n",
      "epoch:31 step:29867 [D loss: 0.237508, acc.: 94.53%] [G loss: 4.346139]\n",
      "epoch:31 step:29868 [D loss: 0.337892, acc.: 86.72%] [G loss: 2.854189]\n",
      "epoch:31 step:29869 [D loss: 0.197509, acc.: 93.75%] [G loss: 1.125479]\n",
      "epoch:31 step:29870 [D loss: 0.075283, acc.: 99.22%] [G loss: 3.929655]\n",
      "epoch:31 step:29871 [D loss: 0.605434, acc.: 65.62%] [G loss: 1.443837]\n",
      "epoch:31 step:29872 [D loss: 0.259236, acc.: 90.62%] [G loss: 4.458961]\n",
      "epoch:31 step:29873 [D loss: 0.008841, acc.: 100.00%] [G loss: 2.735632]\n",
      "epoch:31 step:29874 [D loss: 0.090756, acc.: 98.44%] [G loss: 3.333268]\n",
      "epoch:31 step:29875 [D loss: 0.188858, acc.: 96.09%] [G loss: 1.138116]\n",
      "epoch:31 step:29876 [D loss: 0.523876, acc.: 71.88%] [G loss: 4.200592]\n",
      "epoch:31 step:29877 [D loss: 0.216699, acc.: 92.19%] [G loss: 4.351226]\n",
      "epoch:31 step:29878 [D loss: 0.614882, acc.: 66.41%] [G loss: 0.954927]\n",
      "epoch:31 step:29879 [D loss: 0.423690, acc.: 78.91%] [G loss: 2.759583]\n",
      "epoch:31 step:29880 [D loss: 0.776800, acc.: 59.38%] [G loss: 3.741509]\n",
      "epoch:31 step:29881 [D loss: 0.101385, acc.: 97.66%] [G loss: 3.132651]\n",
      "epoch:31 step:29882 [D loss: 0.368725, acc.: 81.25%] [G loss: 2.893784]\n",
      "epoch:31 step:29883 [D loss: 0.667448, acc.: 60.94%] [G loss: 3.957155]\n",
      "epoch:31 step:29884 [D loss: 0.379066, acc.: 85.94%] [G loss: 4.501087]\n",
      "epoch:31 step:29885 [D loss: 0.029782, acc.: 100.00%] [G loss: 3.986095]\n",
      "epoch:31 step:29886 [D loss: 0.072301, acc.: 99.22%] [G loss: 5.483640]\n",
      "epoch:31 step:29887 [D loss: 0.285089, acc.: 84.38%] [G loss: 2.624629]\n",
      "epoch:31 step:29888 [D loss: 0.089390, acc.: 100.00%] [G loss: 2.611468]\n",
      "epoch:31 step:29889 [D loss: 0.063885, acc.: 99.22%] [G loss: 3.010883]\n",
      "epoch:31 step:29890 [D loss: 0.059709, acc.: 99.22%] [G loss: 2.361275]\n",
      "epoch:31 step:29891 [D loss: 0.038961, acc.: 100.00%] [G loss: 2.454801]\n",
      "epoch:31 step:29892 [D loss: 0.296241, acc.: 89.06%] [G loss: 1.629571]\n",
      "epoch:31 step:29893 [D loss: 0.307554, acc.: 87.50%] [G loss: 2.443118]\n",
      "epoch:31 step:29894 [D loss: 0.103396, acc.: 97.66%] [G loss: 3.064509]\n",
      "epoch:31 step:29895 [D loss: 0.668640, acc.: 63.28%] [G loss: 1.182913]\n",
      "epoch:31 step:29896 [D loss: 0.149815, acc.: 98.44%] [G loss: 4.152644]\n",
      "epoch:31 step:29897 [D loss: 0.071346, acc.: 98.44%] [G loss: 4.584100]\n",
      "epoch:31 step:29898 [D loss: 0.246583, acc.: 90.62%] [G loss: 4.158610]\n",
      "epoch:31 step:29899 [D loss: 0.109618, acc.: 98.44%] [G loss: 5.236546]\n",
      "epoch:31 step:29900 [D loss: 0.246072, acc.: 90.62%] [G loss: 0.867727]\n",
      "epoch:31 step:29901 [D loss: 0.368876, acc.: 89.84%] [G loss: 1.978540]\n",
      "epoch:31 step:29902 [D loss: 1.548556, acc.: 50.78%] [G loss: 3.665626]\n",
      "epoch:31 step:29903 [D loss: 0.215519, acc.: 92.97%] [G loss: 3.402330]\n",
      "epoch:31 step:29904 [D loss: 0.203839, acc.: 93.75%] [G loss: 3.444201]\n",
      "epoch:31 step:29905 [D loss: 0.553518, acc.: 68.75%] [G loss: 2.333804]\n",
      "epoch:31 step:29906 [D loss: 0.213562, acc.: 96.09%] [G loss: 3.322038]\n",
      "epoch:31 step:29907 [D loss: 0.148972, acc.: 96.88%] [G loss: 0.701682]\n",
      "epoch:31 step:29908 [D loss: 0.153109, acc.: 96.88%] [G loss: 1.634218]\n",
      "epoch:31 step:29909 [D loss: 0.417557, acc.: 82.03%] [G loss: 0.708917]\n",
      "epoch:31 step:29910 [D loss: 0.278323, acc.: 86.72%] [G loss: 0.322369]\n",
      "epoch:31 step:29911 [D loss: 0.147464, acc.: 96.09%] [G loss: 4.251031]\n",
      "epoch:31 step:29912 [D loss: 0.883313, acc.: 55.47%] [G loss: 2.513989]\n",
      "epoch:31 step:29913 [D loss: 0.274881, acc.: 85.94%] [G loss: 3.735240]\n",
      "epoch:31 step:29914 [D loss: 0.162170, acc.: 96.88%] [G loss: 3.905398]\n",
      "epoch:31 step:29915 [D loss: 0.097794, acc.: 97.66%] [G loss: 3.758837]\n",
      "epoch:31 step:29916 [D loss: 0.115749, acc.: 96.88%] [G loss: 5.023602]\n",
      "epoch:31 step:29917 [D loss: 0.483493, acc.: 74.22%] [G loss: 3.919210]\n",
      "epoch:31 step:29918 [D loss: 0.535502, acc.: 69.53%] [G loss: 4.061467]\n",
      "epoch:31 step:29919 [D loss: 0.330230, acc.: 78.91%] [G loss: 1.945868]\n",
      "epoch:31 step:29920 [D loss: 0.174786, acc.: 95.31%] [G loss: 2.433217]\n",
      "epoch:31 step:29921 [D loss: 0.162519, acc.: 96.09%] [G loss: 3.602963]\n",
      "epoch:31 step:29922 [D loss: 0.056350, acc.: 99.22%] [G loss: 5.095317]\n",
      "epoch:31 step:29923 [D loss: 0.299701, acc.: 88.28%] [G loss: 2.633865]\n",
      "epoch:31 step:29924 [D loss: 0.167512, acc.: 96.09%] [G loss: 1.680623]\n",
      "epoch:31 step:29925 [D loss: 0.135752, acc.: 99.22%] [G loss: 1.917597]\n",
      "epoch:31 step:29926 [D loss: 0.081685, acc.: 99.22%] [G loss: 0.952704]\n",
      "epoch:31 step:29927 [D loss: 0.553963, acc.: 67.97%] [G loss: 2.142511]\n",
      "epoch:31 step:29928 [D loss: 0.231549, acc.: 92.19%] [G loss: 2.044494]\n",
      "epoch:31 step:29929 [D loss: 0.092259, acc.: 96.88%] [G loss: 6.055986]\n",
      "epoch:31 step:29930 [D loss: 0.322727, acc.: 85.16%] [G loss: 1.792626]\n",
      "epoch:31 step:29931 [D loss: 0.588838, acc.: 64.06%] [G loss: 3.594211]\n",
      "epoch:31 step:29932 [D loss: 0.226998, acc.: 94.53%] [G loss: 7.606124]\n",
      "epoch:31 step:29933 [D loss: 0.164790, acc.: 95.31%] [G loss: 2.443595]\n",
      "epoch:31 step:29934 [D loss: 0.022923, acc.: 100.00%] [G loss: 1.206524]\n",
      "epoch:31 step:29935 [D loss: 0.445675, acc.: 72.66%] [G loss: 1.807561]\n",
      "epoch:31 step:29936 [D loss: 0.242720, acc.: 92.19%] [G loss: 3.607495]\n",
      "epoch:31 step:29937 [D loss: 0.350293, acc.: 81.25%] [G loss: 1.041440]\n",
      "epoch:31 step:29938 [D loss: 0.410290, acc.: 74.22%] [G loss: 4.564011]\n",
      "epoch:31 step:29939 [D loss: 0.736352, acc.: 57.03%] [G loss: 2.256103]\n",
      "epoch:31 step:29940 [D loss: 0.079263, acc.: 99.22%] [G loss: 3.777130]\n",
      "epoch:31 step:29941 [D loss: 0.056735, acc.: 99.22%] [G loss: 2.720025]\n",
      "epoch:31 step:29942 [D loss: 0.460834, acc.: 73.44%] [G loss: 6.082925]\n",
      "epoch:31 step:29943 [D loss: 0.239190, acc.: 88.28%] [G loss: 5.553555]\n",
      "epoch:31 step:29944 [D loss: 0.122931, acc.: 97.66%] [G loss: 5.705889]\n",
      "epoch:31 step:29945 [D loss: 0.022202, acc.: 99.22%] [G loss: 3.427900]\n",
      "epoch:31 step:29946 [D loss: 0.569353, acc.: 73.44%] [G loss: 1.197186]\n",
      "epoch:31 step:29947 [D loss: 1.299754, acc.: 23.44%] [G loss: 2.499953]\n",
      "epoch:31 step:29948 [D loss: 0.528230, acc.: 67.19%] [G loss: 2.981331]\n",
      "epoch:31 step:29949 [D loss: 0.047836, acc.: 100.00%] [G loss: 6.182362]\n",
      "epoch:31 step:29950 [D loss: 0.163992, acc.: 97.66%] [G loss: 5.681622]\n",
      "epoch:31 step:29951 [D loss: 0.129016, acc.: 96.88%] [G loss: 4.986336]\n",
      "epoch:31 step:29952 [D loss: 1.235240, acc.: 31.25%] [G loss: 5.133048]\n",
      "epoch:31 step:29953 [D loss: 0.497567, acc.: 71.88%] [G loss: 3.025715]\n",
      "epoch:31 step:29954 [D loss: 0.111387, acc.: 97.66%] [G loss: 1.389946]\n",
      "epoch:31 step:29955 [D loss: 0.766158, acc.: 60.94%] [G loss: 3.673793]\n",
      "epoch:31 step:29956 [D loss: 1.271510, acc.: 54.69%] [G loss: 1.795805]\n",
      "epoch:31 step:29957 [D loss: 0.201340, acc.: 92.19%] [G loss: 3.596261]\n",
      "epoch:31 step:29958 [D loss: 0.408721, acc.: 73.44%] [G loss: 4.146736]\n",
      "epoch:31 step:29959 [D loss: 0.273289, acc.: 92.19%] [G loss: 3.670585]\n",
      "epoch:31 step:29960 [D loss: 0.391766, acc.: 89.84%] [G loss: 0.816971]\n",
      "epoch:31 step:29961 [D loss: 0.431455, acc.: 78.91%] [G loss: 4.200195]\n",
      "epoch:31 step:29962 [D loss: 0.075046, acc.: 100.00%] [G loss: 3.742514]\n",
      "epoch:31 step:29963 [D loss: 0.129810, acc.: 97.66%] [G loss: 2.925919]\n",
      "epoch:31 step:29964 [D loss: 0.255730, acc.: 89.06%] [G loss: 4.532630]\n",
      "epoch:31 step:29965 [D loss: 0.068446, acc.: 100.00%] [G loss: 2.348670]\n",
      "epoch:31 step:29966 [D loss: 0.066704, acc.: 99.22%] [G loss: 2.002417]\n",
      "epoch:31 step:29967 [D loss: 0.415503, acc.: 72.66%] [G loss: 5.415883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:29968 [D loss: 0.325761, acc.: 84.38%] [G loss: 3.936151]\n",
      "epoch:31 step:29969 [D loss: 0.064921, acc.: 97.66%] [G loss: 4.162013]\n",
      "epoch:31 step:29970 [D loss: 0.212257, acc.: 94.53%] [G loss: 3.833115]\n",
      "epoch:31 step:29971 [D loss: 0.189126, acc.: 97.66%] [G loss: 2.898684]\n",
      "epoch:31 step:29972 [D loss: 0.176821, acc.: 95.31%] [G loss: 4.584996]\n",
      "epoch:31 step:29973 [D loss: 0.128390, acc.: 100.00%] [G loss: 3.548456]\n",
      "epoch:31 step:29974 [D loss: 0.187382, acc.: 96.88%] [G loss: 4.972662]\n",
      "epoch:31 step:29975 [D loss: 0.167759, acc.: 95.31%] [G loss: 3.310874]\n",
      "epoch:31 step:29976 [D loss: 0.618973, acc.: 66.41%] [G loss: 2.893117]\n",
      "epoch:31 step:29977 [D loss: 0.126222, acc.: 96.88%] [G loss: 2.676778]\n",
      "epoch:31 step:29978 [D loss: 0.060808, acc.: 100.00%] [G loss: 2.454506]\n",
      "epoch:31 step:29979 [D loss: 0.264746, acc.: 88.28%] [G loss: 2.617701]\n",
      "epoch:31 step:29980 [D loss: 0.097037, acc.: 98.44%] [G loss: 3.616006]\n",
      "epoch:31 step:29981 [D loss: 0.238075, acc.: 92.19%] [G loss: 3.173130]\n",
      "epoch:31 step:29982 [D loss: 0.940096, acc.: 50.78%] [G loss: 4.335146]\n",
      "epoch:31 step:29983 [D loss: 0.055445, acc.: 100.00%] [G loss: 1.999030]\n",
      "epoch:31 step:29984 [D loss: 0.072334, acc.: 97.66%] [G loss: 1.447056]\n",
      "epoch:32 step:29985 [D loss: 0.344684, acc.: 85.16%] [G loss: 0.746020]\n",
      "epoch:32 step:29986 [D loss: 0.029768, acc.: 100.00%] [G loss: 1.376808]\n",
      "epoch:32 step:29987 [D loss: 0.281011, acc.: 88.28%] [G loss: 3.997649]\n",
      "epoch:32 step:29988 [D loss: 0.020262, acc.: 100.00%] [G loss: 1.374812]\n",
      "epoch:32 step:29989 [D loss: 0.749597, acc.: 58.59%] [G loss: 0.129942]\n",
      "epoch:32 step:29990 [D loss: 0.212725, acc.: 96.09%] [G loss: 2.616421]\n",
      "epoch:32 step:29991 [D loss: 0.095431, acc.: 98.44%] [G loss: 3.749616]\n",
      "epoch:32 step:29992 [D loss: 0.787488, acc.: 60.94%] [G loss: 3.712197]\n",
      "epoch:32 step:29993 [D loss: 1.116403, acc.: 33.59%] [G loss: 2.844247]\n",
      "epoch:32 step:29994 [D loss: 0.952361, acc.: 57.03%] [G loss: 1.913334]\n",
      "epoch:32 step:29995 [D loss: 0.284379, acc.: 91.41%] [G loss: 1.732524]\n",
      "epoch:32 step:29996 [D loss: 0.138818, acc.: 96.09%] [G loss: 1.779491]\n",
      "epoch:32 step:29997 [D loss: 0.844780, acc.: 66.41%] [G loss: 1.390934]\n",
      "epoch:32 step:29998 [D loss: 0.390384, acc.: 81.25%] [G loss: 4.157136]\n",
      "epoch:32 step:29999 [D loss: 0.222942, acc.: 86.72%] [G loss: 0.574746]\n",
      "epoch:32 step:30000 [D loss: 0.610008, acc.: 65.62%] [G loss: 2.504328]\n",
      "epoch:32 step:30001 [D loss: 0.263406, acc.: 89.06%] [G loss: 4.862009]\n",
      "epoch:32 step:30002 [D loss: 0.182280, acc.: 95.31%] [G loss: 4.255111]\n",
      "epoch:32 step:30003 [D loss: 0.255232, acc.: 92.97%] [G loss: 4.897138]\n",
      "epoch:32 step:30004 [D loss: 0.171671, acc.: 96.09%] [G loss: 4.069907]\n",
      "epoch:32 step:30005 [D loss: 0.309452, acc.: 84.38%] [G loss: 0.358434]\n",
      "epoch:32 step:30006 [D loss: 1.199352, acc.: 43.75%] [G loss: 3.235101]\n",
      "epoch:32 step:30007 [D loss: 0.262310, acc.: 89.84%] [G loss: 4.308419]\n",
      "epoch:32 step:30008 [D loss: 0.342210, acc.: 90.62%] [G loss: 3.572854]\n",
      "epoch:32 step:30009 [D loss: 0.548891, acc.: 74.22%] [G loss: 4.873551]\n",
      "epoch:32 step:30010 [D loss: 0.222505, acc.: 92.97%] [G loss: 2.848401]\n",
      "epoch:32 step:30011 [D loss: 0.426837, acc.: 76.56%] [G loss: 4.150551]\n",
      "epoch:32 step:30012 [D loss: 0.820861, acc.: 60.16%] [G loss: 4.881320]\n",
      "epoch:32 step:30013 [D loss: 0.104634, acc.: 97.66%] [G loss: 4.321092]\n",
      "epoch:32 step:30014 [D loss: 0.468477, acc.: 78.91%] [G loss: 1.616173]\n",
      "epoch:32 step:30015 [D loss: 0.359269, acc.: 79.69%] [G loss: 2.203611]\n",
      "epoch:32 step:30016 [D loss: 0.334971, acc.: 87.50%] [G loss: 4.119280]\n",
      "epoch:32 step:30017 [D loss: 0.299203, acc.: 85.94%] [G loss: 3.282242]\n",
      "epoch:32 step:30018 [D loss: 0.125582, acc.: 97.66%] [G loss: 4.334998]\n",
      "epoch:32 step:30019 [D loss: 1.437161, acc.: 50.00%] [G loss: 1.588080]\n",
      "epoch:32 step:30020 [D loss: 0.399058, acc.: 80.47%] [G loss: 3.022882]\n",
      "epoch:32 step:30021 [D loss: 0.038513, acc.: 99.22%] [G loss: 1.200984]\n",
      "epoch:32 step:30022 [D loss: 0.220807, acc.: 95.31%] [G loss: 3.283397]\n",
      "epoch:32 step:30023 [D loss: 0.077789, acc.: 99.22%] [G loss: 5.107078]\n",
      "epoch:32 step:30024 [D loss: 0.046339, acc.: 100.00%] [G loss: 3.767054]\n",
      "epoch:32 step:30025 [D loss: 0.124970, acc.: 96.88%] [G loss: 2.287805]\n",
      "epoch:32 step:30026 [D loss: 0.124970, acc.: 98.44%] [G loss: 2.503540]\n",
      "epoch:32 step:30027 [D loss: 0.345101, acc.: 85.16%] [G loss: 1.790170]\n",
      "epoch:32 step:30028 [D loss: 0.385816, acc.: 81.25%] [G loss: 2.207723]\n",
      "epoch:32 step:30029 [D loss: 0.243190, acc.: 95.31%] [G loss: 1.495226]\n",
      "epoch:32 step:30030 [D loss: 0.479424, acc.: 79.69%] [G loss: 2.920387]\n",
      "epoch:32 step:30031 [D loss: 0.234804, acc.: 92.97%] [G loss: 1.427792]\n",
      "epoch:32 step:30032 [D loss: 0.079129, acc.: 99.22%] [G loss: 6.108979]\n",
      "epoch:32 step:30033 [D loss: 0.503401, acc.: 67.97%] [G loss: 4.234618]\n",
      "epoch:32 step:30034 [D loss: 0.108112, acc.: 99.22%] [G loss: 6.891376]\n",
      "epoch:32 step:30035 [D loss: 0.192231, acc.: 94.53%] [G loss: 2.671629]\n",
      "epoch:32 step:30036 [D loss: 0.163225, acc.: 96.09%] [G loss: 1.928520]\n",
      "epoch:32 step:30037 [D loss: 0.240121, acc.: 93.75%] [G loss: 5.423667]\n",
      "epoch:32 step:30038 [D loss: 0.268695, acc.: 92.97%] [G loss: 3.909494]\n",
      "epoch:32 step:30039 [D loss: 1.068582, acc.: 55.47%] [G loss: 2.595912]\n",
      "epoch:32 step:30040 [D loss: 0.032414, acc.: 100.00%] [G loss: 6.874138]\n",
      "epoch:32 step:30041 [D loss: 0.454696, acc.: 78.12%] [G loss: 4.135834]\n",
      "epoch:32 step:30042 [D loss: 0.175757, acc.: 96.09%] [G loss: 1.794316]\n",
      "epoch:32 step:30043 [D loss: 0.052103, acc.: 100.00%] [G loss: 3.720232]\n",
      "epoch:32 step:30044 [D loss: 0.560174, acc.: 71.09%] [G loss: 2.699366]\n",
      "epoch:32 step:30045 [D loss: 0.694946, acc.: 63.28%] [G loss: 1.544976]\n",
      "epoch:32 step:30046 [D loss: 0.403423, acc.: 78.12%] [G loss: 1.729129]\n",
      "epoch:32 step:30047 [D loss: 0.567835, acc.: 71.09%] [G loss: 2.409124]\n",
      "epoch:32 step:30048 [D loss: 0.053095, acc.: 100.00%] [G loss: 2.997865]\n",
      "epoch:32 step:30049 [D loss: 0.090001, acc.: 100.00%] [G loss: 1.958279]\n",
      "epoch:32 step:30050 [D loss: 0.408884, acc.: 78.91%] [G loss: 0.969200]\n",
      "epoch:32 step:30051 [D loss: 0.272227, acc.: 94.53%] [G loss: 0.978150]\n",
      "epoch:32 step:30052 [D loss: 0.171466, acc.: 97.66%] [G loss: 2.020487]\n",
      "epoch:32 step:30053 [D loss: 0.784234, acc.: 63.28%] [G loss: 1.708061]\n",
      "epoch:32 step:30054 [D loss: 0.240737, acc.: 86.72%] [G loss: 4.169368]\n",
      "epoch:32 step:30055 [D loss: 0.765166, acc.: 53.12%] [G loss: 1.233399]\n",
      "epoch:32 step:30056 [D loss: 0.093169, acc.: 97.66%] [G loss: 3.583570]\n",
      "epoch:32 step:30057 [D loss: 0.365270, acc.: 78.91%] [G loss: 0.889697]\n",
      "epoch:32 step:30058 [D loss: 0.043051, acc.: 99.22%] [G loss: 2.168174]\n",
      "epoch:32 step:30059 [D loss: 0.065135, acc.: 99.22%] [G loss: 3.189718]\n",
      "epoch:32 step:30060 [D loss: 0.684092, acc.: 63.28%] [G loss: 2.210067]\n",
      "epoch:32 step:30061 [D loss: 0.522576, acc.: 72.66%] [G loss: 4.484913]\n",
      "epoch:32 step:30062 [D loss: 0.089513, acc.: 100.00%] [G loss: 5.520271]\n",
      "epoch:32 step:30063 [D loss: 1.253653, acc.: 52.34%] [G loss: 4.466087]\n",
      "epoch:32 step:30064 [D loss: 0.101569, acc.: 100.00%] [G loss: 0.877523]\n",
      "epoch:32 step:30065 [D loss: 0.126889, acc.: 97.66%] [G loss: 4.018631]\n",
      "epoch:32 step:30066 [D loss: 0.218068, acc.: 94.53%] [G loss: 1.555901]\n",
      "epoch:32 step:30067 [D loss: 0.189727, acc.: 93.75%] [G loss: 0.664984]\n",
      "epoch:32 step:30068 [D loss: 0.130760, acc.: 98.44%] [G loss: 5.140970]\n",
      "epoch:32 step:30069 [D loss: 0.635986, acc.: 64.84%] [G loss: 4.890496]\n",
      "epoch:32 step:30070 [D loss: 0.127835, acc.: 99.22%] [G loss: 2.684843]\n",
      "epoch:32 step:30071 [D loss: 0.553639, acc.: 69.53%] [G loss: 4.419998]\n",
      "epoch:32 step:30072 [D loss: 0.984053, acc.: 57.03%] [G loss: 1.425159]\n",
      "epoch:32 step:30073 [D loss: 0.207730, acc.: 95.31%] [G loss: 5.110320]\n",
      "epoch:32 step:30074 [D loss: 0.058101, acc.: 99.22%] [G loss: 2.773026]\n",
      "epoch:32 step:30075 [D loss: 0.231159, acc.: 91.41%] [G loss: 2.847857]\n",
      "epoch:32 step:30076 [D loss: 0.250985, acc.: 91.41%] [G loss: 4.263104]\n",
      "epoch:32 step:30077 [D loss: 0.205919, acc.: 95.31%] [G loss: 4.704408]\n",
      "epoch:32 step:30078 [D loss: 1.948922, acc.: 39.84%] [G loss: 3.328095]\n",
      "epoch:32 step:30079 [D loss: 0.072731, acc.: 99.22%] [G loss: 2.538884]\n",
      "epoch:32 step:30080 [D loss: 0.107323, acc.: 97.66%] [G loss: 1.334023]\n",
      "epoch:32 step:30081 [D loss: 0.154030, acc.: 96.88%] [G loss: 2.029485]\n",
      "epoch:32 step:30082 [D loss: 0.411389, acc.: 77.34%] [G loss: 2.766140]\n",
      "epoch:32 step:30083 [D loss: 0.459869, acc.: 82.81%] [G loss: 1.836097]\n",
      "epoch:32 step:30084 [D loss: 0.082739, acc.: 99.22%] [G loss: 3.363411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30085 [D loss: 1.335336, acc.: 50.78%] [G loss: 0.914878]\n",
      "epoch:32 step:30086 [D loss: 0.251396, acc.: 92.97%] [G loss: 2.330595]\n",
      "epoch:32 step:30087 [D loss: 1.257944, acc.: 53.12%] [G loss: 2.144222]\n",
      "epoch:32 step:30088 [D loss: 0.200915, acc.: 92.19%] [G loss: 2.093123]\n",
      "epoch:32 step:30089 [D loss: 0.165543, acc.: 97.66%] [G loss: 3.784629]\n",
      "epoch:32 step:30090 [D loss: 0.540497, acc.: 68.75%] [G loss: 2.724228]\n",
      "epoch:32 step:30091 [D loss: 0.509752, acc.: 72.66%] [G loss: 3.595164]\n",
      "epoch:32 step:30092 [D loss: 0.588985, acc.: 67.97%] [G loss: 2.052613]\n",
      "epoch:32 step:30093 [D loss: 0.693969, acc.: 61.72%] [G loss: 2.310756]\n",
      "epoch:32 step:30094 [D loss: 0.108950, acc.: 98.44%] [G loss: 1.365804]\n",
      "epoch:32 step:30095 [D loss: 0.269602, acc.: 86.72%] [G loss: 1.855179]\n",
      "epoch:32 step:30096 [D loss: 0.345073, acc.: 84.38%] [G loss: 2.660645]\n",
      "epoch:32 step:30097 [D loss: 0.108182, acc.: 98.44%] [G loss: 1.359745]\n",
      "epoch:32 step:30098 [D loss: 0.730067, acc.: 64.84%] [G loss: 2.799574]\n",
      "epoch:32 step:30099 [D loss: 0.500966, acc.: 75.78%] [G loss: 0.871562]\n",
      "epoch:32 step:30100 [D loss: 0.389076, acc.: 73.44%] [G loss: 3.260072]\n",
      "epoch:32 step:30101 [D loss: 0.113248, acc.: 98.44%] [G loss: 2.149591]\n",
      "epoch:32 step:30102 [D loss: 0.850209, acc.: 59.38%] [G loss: 2.617398]\n",
      "epoch:32 step:30103 [D loss: 0.730725, acc.: 57.81%] [G loss: 2.267099]\n",
      "epoch:32 step:30104 [D loss: 0.523651, acc.: 70.31%] [G loss: 1.408248]\n",
      "epoch:32 step:30105 [D loss: 0.045155, acc.: 100.00%] [G loss: 1.881513]\n",
      "epoch:32 step:30106 [D loss: 0.185708, acc.: 94.53%] [G loss: 0.394260]\n",
      "epoch:32 step:30107 [D loss: 0.094674, acc.: 100.00%] [G loss: 4.584465]\n",
      "epoch:32 step:30108 [D loss: 0.271048, acc.: 90.62%] [G loss: 0.475044]\n",
      "epoch:32 step:30109 [D loss: 0.122055, acc.: 97.66%] [G loss: 1.519084]\n",
      "epoch:32 step:30110 [D loss: 0.131589, acc.: 97.66%] [G loss: 2.824211]\n",
      "epoch:32 step:30111 [D loss: 0.265073, acc.: 93.75%] [G loss: 2.356444]\n",
      "epoch:32 step:30112 [D loss: 0.140228, acc.: 98.44%] [G loss: 4.108047]\n",
      "epoch:32 step:30113 [D loss: 0.052599, acc.: 99.22%] [G loss: 5.122496]\n",
      "epoch:32 step:30114 [D loss: 0.509465, acc.: 78.12%] [G loss: 0.721354]\n",
      "epoch:32 step:30115 [D loss: 0.274521, acc.: 89.06%] [G loss: 2.181914]\n",
      "epoch:32 step:30116 [D loss: 0.176664, acc.: 96.09%] [G loss: 2.178280]\n",
      "epoch:32 step:30117 [D loss: 0.087407, acc.: 99.22%] [G loss: 1.904747]\n",
      "epoch:32 step:30118 [D loss: 0.157803, acc.: 98.44%] [G loss: 0.814780]\n",
      "epoch:32 step:30119 [D loss: 0.151209, acc.: 97.66%] [G loss: 1.650983]\n",
      "epoch:32 step:30120 [D loss: 0.131386, acc.: 97.66%] [G loss: 2.507877]\n",
      "epoch:32 step:30121 [D loss: 0.182004, acc.: 94.53%] [G loss: 3.694068]\n",
      "epoch:32 step:30122 [D loss: 0.096988, acc.: 99.22%] [G loss: 2.657863]\n",
      "epoch:32 step:30123 [D loss: 0.355894, acc.: 87.50%] [G loss: 0.921454]\n",
      "epoch:32 step:30124 [D loss: 0.106723, acc.: 98.44%] [G loss: 5.301763]\n",
      "epoch:32 step:30125 [D loss: 0.090119, acc.: 98.44%] [G loss: 3.490541]\n",
      "epoch:32 step:30126 [D loss: 0.260675, acc.: 91.41%] [G loss: 0.281613]\n",
      "epoch:32 step:30127 [D loss: 0.242937, acc.: 89.84%] [G loss: 3.012201]\n",
      "epoch:32 step:30128 [D loss: 0.230561, acc.: 94.53%] [G loss: 1.534604]\n",
      "epoch:32 step:30129 [D loss: 0.195902, acc.: 96.09%] [G loss: 1.987470]\n",
      "epoch:32 step:30130 [D loss: 0.302306, acc.: 88.28%] [G loss: 3.041038]\n",
      "epoch:32 step:30131 [D loss: 0.308200, acc.: 85.94%] [G loss: 1.233078]\n",
      "epoch:32 step:30132 [D loss: 0.070631, acc.: 98.44%] [G loss: 3.921158]\n",
      "epoch:32 step:30133 [D loss: 0.089506, acc.: 100.00%] [G loss: 2.112022]\n",
      "epoch:32 step:30134 [D loss: 1.065434, acc.: 41.41%] [G loss: 5.110271]\n",
      "epoch:32 step:30135 [D loss: 0.310257, acc.: 85.16%] [G loss: 4.499873]\n",
      "epoch:32 step:30136 [D loss: 0.049761, acc.: 99.22%] [G loss: 4.214310]\n",
      "epoch:32 step:30137 [D loss: 0.180306, acc.: 93.75%] [G loss: 2.143344]\n",
      "epoch:32 step:30138 [D loss: 0.155344, acc.: 97.66%] [G loss: 2.059534]\n",
      "epoch:32 step:30139 [D loss: 0.365170, acc.: 85.16%] [G loss: 2.649402]\n",
      "epoch:32 step:30140 [D loss: 0.054567, acc.: 99.22%] [G loss: 1.742942]\n",
      "epoch:32 step:30141 [D loss: 0.746495, acc.: 56.25%] [G loss: 1.320578]\n",
      "epoch:32 step:30142 [D loss: 0.137205, acc.: 96.88%] [G loss: 2.970589]\n",
      "epoch:32 step:30143 [D loss: 0.126064, acc.: 95.31%] [G loss: 3.879819]\n",
      "epoch:32 step:30144 [D loss: 0.084771, acc.: 97.66%] [G loss: 6.142284]\n",
      "epoch:32 step:30145 [D loss: 0.163782, acc.: 96.88%] [G loss: 3.336582]\n",
      "epoch:32 step:30146 [D loss: 0.176176, acc.: 96.09%] [G loss: 3.509408]\n",
      "epoch:32 step:30147 [D loss: 0.046048, acc.: 99.22%] [G loss: 7.102661]\n",
      "epoch:32 step:30148 [D loss: 0.020888, acc.: 100.00%] [G loss: 4.545115]\n",
      "epoch:32 step:30149 [D loss: 1.494299, acc.: 50.78%] [G loss: 4.524892]\n",
      "epoch:32 step:30150 [D loss: 0.293948, acc.: 85.16%] [G loss: 4.366630]\n",
      "epoch:32 step:30151 [D loss: 0.173377, acc.: 96.09%] [G loss: 3.299097]\n",
      "epoch:32 step:30152 [D loss: 0.547348, acc.: 67.97%] [G loss: 3.146026]\n",
      "epoch:32 step:30153 [D loss: 0.243341, acc.: 92.19%] [G loss: 6.289004]\n",
      "epoch:32 step:30154 [D loss: 0.293094, acc.: 85.94%] [G loss: 5.222224]\n",
      "epoch:32 step:30155 [D loss: 0.212087, acc.: 93.75%] [G loss: 4.659093]\n",
      "epoch:32 step:30156 [D loss: 0.425930, acc.: 79.69%] [G loss: 2.507845]\n",
      "epoch:32 step:30157 [D loss: 0.139929, acc.: 96.09%] [G loss: 2.360267]\n",
      "epoch:32 step:30158 [D loss: 0.298099, acc.: 90.62%] [G loss: 3.277934]\n",
      "epoch:32 step:30159 [D loss: 0.184967, acc.: 97.66%] [G loss: 1.766115]\n",
      "epoch:32 step:30160 [D loss: 0.174878, acc.: 95.31%] [G loss: 1.250499]\n",
      "epoch:32 step:30161 [D loss: 0.343269, acc.: 78.12%] [G loss: 4.155515]\n",
      "epoch:32 step:30162 [D loss: 0.102158, acc.: 98.44%] [G loss: 4.517810]\n",
      "epoch:32 step:30163 [D loss: 0.056273, acc.: 100.00%] [G loss: 0.795760]\n",
      "epoch:32 step:30164 [D loss: 0.048230, acc.: 100.00%] [G loss: 3.737507]\n",
      "epoch:32 step:30165 [D loss: 0.101992, acc.: 99.22%] [G loss: 2.609121]\n",
      "epoch:32 step:30166 [D loss: 0.071995, acc.: 100.00%] [G loss: 4.289351]\n",
      "epoch:32 step:30167 [D loss: 0.215267, acc.: 92.97%] [G loss: 2.059550]\n",
      "epoch:32 step:30168 [D loss: 0.114470, acc.: 99.22%] [G loss: 1.999812]\n",
      "epoch:32 step:30169 [D loss: 0.105250, acc.: 99.22%] [G loss: 1.946631]\n",
      "epoch:32 step:30170 [D loss: 0.066795, acc.: 98.44%] [G loss: 0.835794]\n",
      "epoch:32 step:30171 [D loss: 0.057352, acc.: 99.22%] [G loss: 2.851880]\n",
      "epoch:32 step:30172 [D loss: 0.160209, acc.: 98.44%] [G loss: 2.326372]\n",
      "epoch:32 step:30173 [D loss: 0.469641, acc.: 78.12%] [G loss: 2.788460]\n",
      "epoch:32 step:30174 [D loss: 0.409165, acc.: 83.59%] [G loss: 1.162461]\n",
      "epoch:32 step:30175 [D loss: 0.038875, acc.: 100.00%] [G loss: 3.877374]\n",
      "epoch:32 step:30176 [D loss: 0.475951, acc.: 67.97%] [G loss: 1.124509]\n",
      "epoch:32 step:30177 [D loss: 0.101422, acc.: 97.66%] [G loss: 3.504542]\n",
      "epoch:32 step:30178 [D loss: 0.088193, acc.: 98.44%] [G loss: 0.176319]\n",
      "epoch:32 step:30179 [D loss: 0.806187, acc.: 58.59%] [G loss: 5.152948]\n",
      "epoch:32 step:30180 [D loss: 0.417433, acc.: 76.56%] [G loss: 1.904341]\n",
      "epoch:32 step:30181 [D loss: 0.141752, acc.: 95.31%] [G loss: 4.642970]\n",
      "epoch:32 step:30182 [D loss: 0.019039, acc.: 100.00%] [G loss: 4.365254]\n",
      "epoch:32 step:30183 [D loss: 0.158192, acc.: 96.09%] [G loss: 2.550852]\n",
      "epoch:32 step:30184 [D loss: 0.598116, acc.: 65.62%] [G loss: 4.935637]\n",
      "epoch:32 step:30185 [D loss: 0.342509, acc.: 83.59%] [G loss: 1.109951]\n",
      "epoch:32 step:30186 [D loss: 0.835763, acc.: 59.38%] [G loss: 2.010710]\n",
      "epoch:32 step:30187 [D loss: 0.023782, acc.: 100.00%] [G loss: 6.270377]\n",
      "epoch:32 step:30188 [D loss: 0.097516, acc.: 97.66%] [G loss: 4.519679]\n",
      "epoch:32 step:30189 [D loss: 0.228883, acc.: 92.19%] [G loss: 6.298967]\n",
      "epoch:32 step:30190 [D loss: 0.420711, acc.: 74.22%] [G loss: 1.292903]\n",
      "epoch:32 step:30191 [D loss: 0.056529, acc.: 100.00%] [G loss: 3.715811]\n",
      "epoch:32 step:30192 [D loss: 0.311273, acc.: 84.38%] [G loss: 4.305212]\n",
      "epoch:32 step:30193 [D loss: 0.020828, acc.: 99.22%] [G loss: 3.221999]\n",
      "epoch:32 step:30194 [D loss: 0.028167, acc.: 100.00%] [G loss: 1.960731]\n",
      "epoch:32 step:30195 [D loss: 0.353063, acc.: 89.84%] [G loss: 1.948314]\n",
      "epoch:32 step:30196 [D loss: 0.250544, acc.: 93.75%] [G loss: 4.990422]\n",
      "epoch:32 step:30197 [D loss: 0.237813, acc.: 96.88%] [G loss: 2.334635]\n",
      "epoch:32 step:30198 [D loss: 0.141633, acc.: 97.66%] [G loss: 4.124155]\n",
      "epoch:32 step:30199 [D loss: 0.220707, acc.: 95.31%] [G loss: 5.487324]\n",
      "epoch:32 step:30200 [D loss: 0.063833, acc.: 97.66%] [G loss: 1.851001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30201 [D loss: 0.100920, acc.: 97.66%] [G loss: 2.938206]\n",
      "epoch:32 step:30202 [D loss: 0.673316, acc.: 60.16%] [G loss: 3.626158]\n",
      "epoch:32 step:30203 [D loss: 0.065075, acc.: 99.22%] [G loss: 3.465687]\n",
      "epoch:32 step:30204 [D loss: 0.437407, acc.: 78.12%] [G loss: 2.478236]\n",
      "epoch:32 step:30205 [D loss: 0.203564, acc.: 96.09%] [G loss: 4.521060]\n",
      "epoch:32 step:30206 [D loss: 0.075972, acc.: 99.22%] [G loss: 4.687679]\n",
      "epoch:32 step:30207 [D loss: 0.068646, acc.: 100.00%] [G loss: 2.019973]\n",
      "epoch:32 step:30208 [D loss: 0.057509, acc.: 100.00%] [G loss: 4.771860]\n",
      "epoch:32 step:30209 [D loss: 0.334148, acc.: 82.81%] [G loss: 3.204129]\n",
      "epoch:32 step:30210 [D loss: 0.070531, acc.: 99.22%] [G loss: 3.975219]\n",
      "epoch:32 step:30211 [D loss: 0.073430, acc.: 99.22%] [G loss: 4.214454]\n",
      "epoch:32 step:30212 [D loss: 0.299780, acc.: 86.72%] [G loss: 1.103821]\n",
      "epoch:32 step:30213 [D loss: 0.092956, acc.: 97.66%] [G loss: 0.472793]\n",
      "epoch:32 step:30214 [D loss: 0.078968, acc.: 99.22%] [G loss: 2.406801]\n",
      "epoch:32 step:30215 [D loss: 0.069269, acc.: 99.22%] [G loss: 0.996999]\n",
      "epoch:32 step:30216 [D loss: 0.034931, acc.: 100.00%] [G loss: 1.194885]\n",
      "epoch:32 step:30217 [D loss: 0.505796, acc.: 68.75%] [G loss: 0.544733]\n",
      "epoch:32 step:30218 [D loss: 0.152947, acc.: 97.66%] [G loss: 1.113024]\n",
      "epoch:32 step:30219 [D loss: 0.447904, acc.: 71.88%] [G loss: 2.934874]\n",
      "epoch:32 step:30220 [D loss: 0.035911, acc.: 100.00%] [G loss: 6.055664]\n",
      "epoch:32 step:30221 [D loss: 0.381573, acc.: 84.38%] [G loss: 4.967489]\n",
      "epoch:32 step:30222 [D loss: 0.481990, acc.: 72.66%] [G loss: 5.327527]\n",
      "epoch:32 step:30223 [D loss: 0.157693, acc.: 96.09%] [G loss: 6.064455]\n",
      "epoch:32 step:30224 [D loss: 0.163068, acc.: 97.66%] [G loss: 2.175701]\n",
      "epoch:32 step:30225 [D loss: 0.117291, acc.: 98.44%] [G loss: 5.531331]\n",
      "epoch:32 step:30226 [D loss: 0.284268, acc.: 84.38%] [G loss: 5.293853]\n",
      "epoch:32 step:30227 [D loss: 0.508444, acc.: 68.75%] [G loss: 1.892006]\n",
      "epoch:32 step:30228 [D loss: 0.200002, acc.: 93.75%] [G loss: 2.008610]\n",
      "epoch:32 step:30229 [D loss: 0.142307, acc.: 96.09%] [G loss: 1.564562]\n",
      "epoch:32 step:30230 [D loss: 0.069210, acc.: 99.22%] [G loss: 4.729282]\n",
      "epoch:32 step:30231 [D loss: 0.214628, acc.: 91.41%] [G loss: 5.443974]\n",
      "epoch:32 step:30232 [D loss: 0.057567, acc.: 99.22%] [G loss: 5.048302]\n",
      "epoch:32 step:30233 [D loss: 0.585009, acc.: 75.78%] [G loss: 2.329334]\n",
      "epoch:32 step:30234 [D loss: 0.022765, acc.: 100.00%] [G loss: 1.928625]\n",
      "epoch:32 step:30235 [D loss: 0.063078, acc.: 97.66%] [G loss: 2.950574]\n",
      "epoch:32 step:30236 [D loss: 0.069209, acc.: 98.44%] [G loss: 4.731178]\n",
      "epoch:32 step:30237 [D loss: 0.437078, acc.: 78.91%] [G loss: 5.574901]\n",
      "epoch:32 step:30238 [D loss: 0.188869, acc.: 94.53%] [G loss: 4.239264]\n",
      "epoch:32 step:30239 [D loss: 0.321614, acc.: 87.50%] [G loss: 6.140311]\n",
      "epoch:32 step:30240 [D loss: 0.061456, acc.: 99.22%] [G loss: 6.809357]\n",
      "epoch:32 step:30241 [D loss: 0.209544, acc.: 94.53%] [G loss: 2.109357]\n",
      "epoch:32 step:30242 [D loss: 0.669693, acc.: 64.06%] [G loss: 2.889371]\n",
      "epoch:32 step:30243 [D loss: 0.124586, acc.: 96.88%] [G loss: 6.831427]\n",
      "epoch:32 step:30244 [D loss: 0.061146, acc.: 98.44%] [G loss: 3.644538]\n",
      "epoch:32 step:30245 [D loss: 1.885429, acc.: 22.66%] [G loss: 2.744526]\n",
      "epoch:32 step:30246 [D loss: 0.136323, acc.: 96.88%] [G loss: 3.124296]\n",
      "epoch:32 step:30247 [D loss: 0.261647, acc.: 89.06%] [G loss: 2.340947]\n",
      "epoch:32 step:30248 [D loss: 0.441263, acc.: 75.00%] [G loss: 3.950243]\n",
      "epoch:32 step:30249 [D loss: 0.270221, acc.: 92.97%] [G loss: 1.222079]\n",
      "epoch:32 step:30250 [D loss: 0.117993, acc.: 96.88%] [G loss: 3.276783]\n",
      "epoch:32 step:30251 [D loss: 0.104205, acc.: 98.44%] [G loss: 3.195303]\n",
      "epoch:32 step:30252 [D loss: 0.116595, acc.: 97.66%] [G loss: 0.961649]\n",
      "epoch:32 step:30253 [D loss: 0.244243, acc.: 87.50%] [G loss: 1.787178]\n",
      "epoch:32 step:30254 [D loss: 0.107630, acc.: 98.44%] [G loss: 2.217369]\n",
      "epoch:32 step:30255 [D loss: 0.307317, acc.: 86.72%] [G loss: 3.538201]\n",
      "epoch:32 step:30256 [D loss: 0.091653, acc.: 98.44%] [G loss: 0.460154]\n",
      "epoch:32 step:30257 [D loss: 0.047963, acc.: 99.22%] [G loss: 4.048099]\n",
      "epoch:32 step:30258 [D loss: 0.204464, acc.: 95.31%] [G loss: 3.022059]\n",
      "epoch:32 step:30259 [D loss: 0.068305, acc.: 98.44%] [G loss: 1.306722]\n",
      "epoch:32 step:30260 [D loss: 1.410529, acc.: 31.25%] [G loss: 2.702494]\n",
      "epoch:32 step:30261 [D loss: 0.088742, acc.: 98.44%] [G loss: 5.837221]\n",
      "epoch:32 step:30262 [D loss: 0.097856, acc.: 99.22%] [G loss: 1.995255]\n",
      "epoch:32 step:30263 [D loss: 0.191950, acc.: 92.97%] [G loss: 5.596267]\n",
      "epoch:32 step:30264 [D loss: 3.597498, acc.: 6.25%] [G loss: 1.449424]\n",
      "epoch:32 step:30265 [D loss: 0.086882, acc.: 100.00%] [G loss: 0.884409]\n",
      "epoch:32 step:30266 [D loss: 0.187085, acc.: 93.75%] [G loss: 2.672472]\n",
      "epoch:32 step:30267 [D loss: 0.140721, acc.: 98.44%] [G loss: 1.954360]\n",
      "epoch:32 step:30268 [D loss: 0.183664, acc.: 96.88%] [G loss: 2.393645]\n",
      "epoch:32 step:30269 [D loss: 0.124206, acc.: 96.88%] [G loss: 2.020721]\n",
      "epoch:32 step:30270 [D loss: 0.179751, acc.: 95.31%] [G loss: 4.054950]\n",
      "epoch:32 step:30271 [D loss: 0.102803, acc.: 98.44%] [G loss: 3.368488]\n",
      "epoch:32 step:30272 [D loss: 0.035583, acc.: 100.00%] [G loss: 2.079609]\n",
      "epoch:32 step:30273 [D loss: 0.324027, acc.: 87.50%] [G loss: 3.735185]\n",
      "epoch:32 step:30274 [D loss: 0.117489, acc.: 97.66%] [G loss: 1.965484]\n",
      "epoch:32 step:30275 [D loss: 0.141187, acc.: 96.09%] [G loss: 1.361023]\n",
      "epoch:32 step:30276 [D loss: 0.497972, acc.: 70.31%] [G loss: 0.576983]\n",
      "epoch:32 step:30277 [D loss: 0.294185, acc.: 88.28%] [G loss: 3.786485]\n",
      "epoch:32 step:30278 [D loss: 0.400790, acc.: 76.56%] [G loss: 3.392272]\n",
      "epoch:32 step:30279 [D loss: 0.582441, acc.: 74.22%] [G loss: 0.425061]\n",
      "epoch:32 step:30280 [D loss: 0.283679, acc.: 86.72%] [G loss: 3.732539]\n",
      "epoch:32 step:30281 [D loss: 0.571103, acc.: 65.62%] [G loss: 3.403801]\n",
      "epoch:32 step:30282 [D loss: 0.335448, acc.: 90.62%] [G loss: 4.424590]\n",
      "epoch:32 step:30283 [D loss: 0.841612, acc.: 58.59%] [G loss: 2.850633]\n",
      "epoch:32 step:30284 [D loss: 0.271050, acc.: 92.97%] [G loss: 3.370194]\n",
      "epoch:32 step:30285 [D loss: 0.126736, acc.: 98.44%] [G loss: 3.246307]\n",
      "epoch:32 step:30286 [D loss: 0.044712, acc.: 99.22%] [G loss: 2.859071]\n",
      "epoch:32 step:30287 [D loss: 0.226810, acc.: 93.75%] [G loss: 1.538924]\n",
      "epoch:32 step:30288 [D loss: 0.083637, acc.: 99.22%] [G loss: 2.165038]\n",
      "epoch:32 step:30289 [D loss: 0.034939, acc.: 99.22%] [G loss: 4.346355]\n",
      "epoch:32 step:30290 [D loss: 0.610028, acc.: 64.06%] [G loss: 0.447633]\n",
      "epoch:32 step:30291 [D loss: 0.349159, acc.: 77.34%] [G loss: 2.783022]\n",
      "epoch:32 step:30292 [D loss: 0.120858, acc.: 96.09%] [G loss: 2.558666]\n",
      "epoch:32 step:30293 [D loss: 0.065016, acc.: 99.22%] [G loss: 2.394529]\n",
      "epoch:32 step:30294 [D loss: 0.414038, acc.: 76.56%] [G loss: 3.593454]\n",
      "epoch:32 step:30295 [D loss: 0.111678, acc.: 96.88%] [G loss: 4.367676]\n",
      "epoch:32 step:30296 [D loss: 0.361702, acc.: 90.62%] [G loss: 2.634295]\n",
      "epoch:32 step:30297 [D loss: 0.155623, acc.: 96.88%] [G loss: 4.539616]\n",
      "epoch:32 step:30298 [D loss: 0.739597, acc.: 59.38%] [G loss: 1.038150]\n",
      "epoch:32 step:30299 [D loss: 0.040577, acc.: 98.44%] [G loss: 3.482214]\n",
      "epoch:32 step:30300 [D loss: 0.113188, acc.: 96.09%] [G loss: 2.041564]\n",
      "epoch:32 step:30301 [D loss: 0.759100, acc.: 59.38%] [G loss: 1.604714]\n",
      "epoch:32 step:30302 [D loss: 0.199454, acc.: 92.97%] [G loss: 1.752451]\n",
      "epoch:32 step:30303 [D loss: 0.110983, acc.: 96.88%] [G loss: 2.236441]\n",
      "epoch:32 step:30304 [D loss: 0.594327, acc.: 71.88%] [G loss: 4.499499]\n",
      "epoch:32 step:30305 [D loss: 0.231561, acc.: 95.31%] [G loss: 5.168602]\n",
      "epoch:32 step:30306 [D loss: 0.123282, acc.: 97.66%] [G loss: 5.227654]\n",
      "epoch:32 step:30307 [D loss: 0.233868, acc.: 92.97%] [G loss: 2.383011]\n",
      "epoch:32 step:30308 [D loss: 0.190981, acc.: 96.88%] [G loss: 2.931193]\n",
      "epoch:32 step:30309 [D loss: 0.306350, acc.: 88.28%] [G loss: 2.553069]\n",
      "epoch:32 step:30310 [D loss: 0.206959, acc.: 96.09%] [G loss: 2.013060]\n",
      "epoch:32 step:30311 [D loss: 0.510744, acc.: 73.44%] [G loss: 5.501915]\n",
      "epoch:32 step:30312 [D loss: 0.033372, acc.: 100.00%] [G loss: 3.925379]\n",
      "epoch:32 step:30313 [D loss: 0.369418, acc.: 78.91%] [G loss: 2.613044]\n",
      "epoch:32 step:30314 [D loss: 0.923889, acc.: 55.47%] [G loss: 3.117013]\n",
      "epoch:32 step:30315 [D loss: 0.527079, acc.: 70.31%] [G loss: 5.056815]\n",
      "epoch:32 step:30316 [D loss: 0.208388, acc.: 96.09%] [G loss: 2.023799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30317 [D loss: 0.197488, acc.: 95.31%] [G loss: 1.012337]\n",
      "epoch:32 step:30318 [D loss: 0.189243, acc.: 92.19%] [G loss: 1.946443]\n",
      "epoch:32 step:30319 [D loss: 0.980563, acc.: 53.91%] [G loss: 1.006029]\n",
      "epoch:32 step:30320 [D loss: 0.133502, acc.: 97.66%] [G loss: 4.552892]\n",
      "epoch:32 step:30321 [D loss: 0.378610, acc.: 82.81%] [G loss: 3.808789]\n",
      "epoch:32 step:30322 [D loss: 0.224936, acc.: 89.84%] [G loss: 5.188984]\n",
      "epoch:32 step:30323 [D loss: 0.708630, acc.: 65.62%] [G loss: 6.322435]\n",
      "epoch:32 step:30324 [D loss: 0.851812, acc.: 60.16%] [G loss: 0.902538]\n",
      "epoch:32 step:30325 [D loss: 0.544560, acc.: 64.06%] [G loss: 2.918988]\n",
      "epoch:32 step:30326 [D loss: 0.709581, acc.: 65.62%] [G loss: 5.522918]\n",
      "epoch:32 step:30327 [D loss: 0.746941, acc.: 60.94%] [G loss: 1.629314]\n",
      "epoch:32 step:30328 [D loss: 0.065387, acc.: 98.44%] [G loss: 1.652781]\n",
      "epoch:32 step:30329 [D loss: 0.058056, acc.: 100.00%] [G loss: 0.691117]\n",
      "epoch:32 step:30330 [D loss: 0.515837, acc.: 75.78%] [G loss: 1.587176]\n",
      "epoch:32 step:30331 [D loss: 0.177052, acc.: 96.88%] [G loss: 3.931224]\n",
      "epoch:32 step:30332 [D loss: 0.042153, acc.: 99.22%] [G loss: 1.285661]\n",
      "epoch:32 step:30333 [D loss: 0.729538, acc.: 62.50%] [G loss: 1.924655]\n",
      "epoch:32 step:30334 [D loss: 0.044814, acc.: 99.22%] [G loss: 2.785362]\n",
      "epoch:32 step:30335 [D loss: 0.234712, acc.: 96.88%] [G loss: 3.151657]\n",
      "epoch:32 step:30336 [D loss: 0.315431, acc.: 82.81%] [G loss: 1.160367]\n",
      "epoch:32 step:30337 [D loss: 0.041094, acc.: 99.22%] [G loss: 1.365558]\n",
      "epoch:32 step:30338 [D loss: 0.285849, acc.: 90.62%] [G loss: 1.491792]\n",
      "epoch:32 step:30339 [D loss: 0.094800, acc.: 100.00%] [G loss: 0.199178]\n",
      "epoch:32 step:30340 [D loss: 0.872112, acc.: 54.69%] [G loss: 1.141791]\n",
      "epoch:32 step:30341 [D loss: 0.337721, acc.: 80.47%] [G loss: 3.457824]\n",
      "epoch:32 step:30342 [D loss: 0.261179, acc.: 92.19%] [G loss: 2.476618]\n",
      "epoch:32 step:30343 [D loss: 0.055647, acc.: 98.44%] [G loss: 4.044435]\n",
      "epoch:32 step:30344 [D loss: 0.241506, acc.: 95.31%] [G loss: 1.114486]\n",
      "epoch:32 step:30345 [D loss: 1.107018, acc.: 55.47%] [G loss: 3.480431]\n",
      "epoch:32 step:30346 [D loss: 0.178497, acc.: 94.53%] [G loss: 2.233334]\n",
      "epoch:32 step:30347 [D loss: 0.888249, acc.: 57.81%] [G loss: 2.270185]\n",
      "epoch:32 step:30348 [D loss: 0.261942, acc.: 90.62%] [G loss: 0.758193]\n",
      "epoch:32 step:30349 [D loss: 0.299586, acc.: 90.62%] [G loss: 2.288927]\n",
      "epoch:32 step:30350 [D loss: 0.119547, acc.: 98.44%] [G loss: 1.508624]\n",
      "epoch:32 step:30351 [D loss: 0.051038, acc.: 100.00%] [G loss: 2.246693]\n",
      "epoch:32 step:30352 [D loss: 0.239859, acc.: 93.75%] [G loss: 2.642063]\n",
      "epoch:32 step:30353 [D loss: 0.123049, acc.: 96.88%] [G loss: 5.113343]\n",
      "epoch:32 step:30354 [D loss: 0.103734, acc.: 97.66%] [G loss: 1.752656]\n",
      "epoch:32 step:30355 [D loss: 0.030458, acc.: 100.00%] [G loss: 4.621809]\n",
      "epoch:32 step:30356 [D loss: 0.274492, acc.: 89.06%] [G loss: 5.680563]\n",
      "epoch:32 step:30357 [D loss: 0.544636, acc.: 71.88%] [G loss: 5.250989]\n",
      "epoch:32 step:30358 [D loss: 0.263326, acc.: 86.72%] [G loss: 3.761014]\n",
      "epoch:32 step:30359 [D loss: 0.038134, acc.: 99.22%] [G loss: 2.858940]\n",
      "epoch:32 step:30360 [D loss: 0.307809, acc.: 85.94%] [G loss: 1.319970]\n",
      "epoch:32 step:30361 [D loss: 0.121520, acc.: 96.88%] [G loss: 2.521548]\n",
      "epoch:32 step:30362 [D loss: 0.034151, acc.: 100.00%] [G loss: 3.202302]\n",
      "epoch:32 step:30363 [D loss: 0.128336, acc.: 100.00%] [G loss: 4.856628]\n",
      "epoch:32 step:30364 [D loss: 0.456834, acc.: 73.44%] [G loss: 1.999610]\n",
      "epoch:32 step:30365 [D loss: 0.333379, acc.: 82.81%] [G loss: 3.559964]\n",
      "epoch:32 step:30366 [D loss: 0.106363, acc.: 97.66%] [G loss: 5.709847]\n",
      "epoch:32 step:30367 [D loss: 0.137837, acc.: 96.88%] [G loss: 4.001904]\n",
      "epoch:32 step:30368 [D loss: 0.469070, acc.: 80.47%] [G loss: 5.107470]\n",
      "epoch:32 step:30369 [D loss: 0.508635, acc.: 72.66%] [G loss: 4.222637]\n",
      "epoch:32 step:30370 [D loss: 0.231517, acc.: 93.75%] [G loss: 5.581219]\n",
      "epoch:32 step:30371 [D loss: 0.199911, acc.: 94.53%] [G loss: 1.365457]\n",
      "epoch:32 step:30372 [D loss: 1.076058, acc.: 35.16%] [G loss: 2.668228]\n",
      "epoch:32 step:30373 [D loss: 0.214165, acc.: 96.09%] [G loss: 1.493852]\n",
      "epoch:32 step:30374 [D loss: 0.152934, acc.: 96.88%] [G loss: 3.245329]\n",
      "epoch:32 step:30375 [D loss: 0.171810, acc.: 96.88%] [G loss: 0.904234]\n",
      "epoch:32 step:30376 [D loss: 0.291048, acc.: 87.50%] [G loss: 4.310378]\n",
      "epoch:32 step:30377 [D loss: 0.071301, acc.: 97.66%] [G loss: 3.552284]\n",
      "epoch:32 step:30378 [D loss: 0.228200, acc.: 94.53%] [G loss: 4.140922]\n",
      "epoch:32 step:30379 [D loss: 0.050436, acc.: 100.00%] [G loss: 2.744782]\n",
      "epoch:32 step:30380 [D loss: 0.032461, acc.: 100.00%] [G loss: 3.282213]\n",
      "epoch:32 step:30381 [D loss: 1.166760, acc.: 55.47%] [G loss: 0.444216]\n",
      "epoch:32 step:30382 [D loss: 0.728069, acc.: 59.38%] [G loss: 1.360787]\n",
      "epoch:32 step:30383 [D loss: 0.033101, acc.: 99.22%] [G loss: 5.730699]\n",
      "epoch:32 step:30384 [D loss: 0.168136, acc.: 95.31%] [G loss: 6.182807]\n",
      "epoch:32 step:30385 [D loss: 0.137017, acc.: 99.22%] [G loss: 3.009837]\n",
      "epoch:32 step:30386 [D loss: 0.270673, acc.: 86.72%] [G loss: 5.017004]\n",
      "epoch:32 step:30387 [D loss: 0.200388, acc.: 95.31%] [G loss: 3.075330]\n",
      "epoch:32 step:30388 [D loss: 0.252543, acc.: 91.41%] [G loss: 2.362590]\n",
      "epoch:32 step:30389 [D loss: 0.036874, acc.: 100.00%] [G loss: 2.283875]\n",
      "epoch:32 step:30390 [D loss: 0.065917, acc.: 99.22%] [G loss: 4.691105]\n",
      "epoch:32 step:30391 [D loss: 0.072063, acc.: 98.44%] [G loss: 3.225010]\n",
      "epoch:32 step:30392 [D loss: 0.138981, acc.: 97.66%] [G loss: 1.911176]\n",
      "epoch:32 step:30393 [D loss: 0.291980, acc.: 87.50%] [G loss: 4.513123]\n",
      "epoch:32 step:30394 [D loss: 0.244022, acc.: 85.16%] [G loss: 3.549254]\n",
      "epoch:32 step:30395 [D loss: 0.180866, acc.: 94.53%] [G loss: 4.451501]\n",
      "epoch:32 step:30396 [D loss: 0.428883, acc.: 70.31%] [G loss: 1.348128]\n",
      "epoch:32 step:30397 [D loss: 0.048526, acc.: 100.00%] [G loss: 0.618024]\n",
      "epoch:32 step:30398 [D loss: 0.669705, acc.: 65.62%] [G loss: 3.978165]\n",
      "epoch:32 step:30399 [D loss: 0.493866, acc.: 75.78%] [G loss: 6.271197]\n",
      "epoch:32 step:30400 [D loss: 0.458291, acc.: 71.09%] [G loss: 5.439075]\n",
      "epoch:32 step:30401 [D loss: 0.619974, acc.: 65.62%] [G loss: 3.579156]\n",
      "epoch:32 step:30402 [D loss: 0.198545, acc.: 95.31%] [G loss: 1.466134]\n",
      "epoch:32 step:30403 [D loss: 0.198311, acc.: 97.66%] [G loss: 1.466176]\n",
      "epoch:32 step:30404 [D loss: 0.292493, acc.: 91.41%] [G loss: 0.467329]\n",
      "epoch:32 step:30405 [D loss: 1.122350, acc.: 54.69%] [G loss: 0.883020]\n",
      "epoch:32 step:30406 [D loss: 0.044629, acc.: 99.22%] [G loss: 2.983321]\n",
      "epoch:32 step:30407 [D loss: 0.075740, acc.: 99.22%] [G loss: 1.736040]\n",
      "epoch:32 step:30408 [D loss: 0.292772, acc.: 88.28%] [G loss: 2.729130]\n",
      "epoch:32 step:30409 [D loss: 0.441257, acc.: 76.56%] [G loss: 3.246385]\n",
      "epoch:32 step:30410 [D loss: 0.344798, acc.: 88.28%] [G loss: 0.660852]\n",
      "epoch:32 step:30411 [D loss: 1.225194, acc.: 52.34%] [G loss: 3.876418]\n",
      "epoch:32 step:30412 [D loss: 0.324000, acc.: 86.72%] [G loss: 3.886012]\n",
      "epoch:32 step:30413 [D loss: 0.271736, acc.: 90.62%] [G loss: 4.590782]\n",
      "epoch:32 step:30414 [D loss: 0.312595, acc.: 86.72%] [G loss: 1.666080]\n",
      "epoch:32 step:30415 [D loss: 0.277143, acc.: 91.41%] [G loss: 2.349221]\n",
      "epoch:32 step:30416 [D loss: 1.336874, acc.: 24.22%] [G loss: 3.657100]\n",
      "epoch:32 step:30417 [D loss: 0.869043, acc.: 48.44%] [G loss: 1.091595]\n",
      "epoch:32 step:30418 [D loss: 0.092725, acc.: 96.88%] [G loss: 1.328341]\n",
      "epoch:32 step:30419 [D loss: 0.048999, acc.: 99.22%] [G loss: 1.031690]\n",
      "epoch:32 step:30420 [D loss: 0.854622, acc.: 58.59%] [G loss: 1.628085]\n",
      "epoch:32 step:30421 [D loss: 0.139437, acc.: 96.09%] [G loss: 2.192175]\n",
      "epoch:32 step:30422 [D loss: 0.106900, acc.: 99.22%] [G loss: 0.302913]\n",
      "epoch:32 step:30423 [D loss: 0.200968, acc.: 92.97%] [G loss: 0.908055]\n",
      "epoch:32 step:30424 [D loss: 0.939504, acc.: 55.47%] [G loss: 2.611610]\n",
      "epoch:32 step:30425 [D loss: 0.093633, acc.: 98.44%] [G loss: 1.883415]\n",
      "epoch:32 step:30426 [D loss: 0.208034, acc.: 92.97%] [G loss: 2.470820]\n",
      "epoch:32 step:30427 [D loss: 0.202547, acc.: 96.09%] [G loss: 2.243968]\n",
      "epoch:32 step:30428 [D loss: 0.254622, acc.: 93.75%] [G loss: 3.185597]\n",
      "epoch:32 step:30429 [D loss: 0.338253, acc.: 78.91%] [G loss: 1.758496]\n",
      "epoch:32 step:30430 [D loss: 0.184346, acc.: 92.97%] [G loss: 1.976417]\n",
      "epoch:32 step:30431 [D loss: 0.334201, acc.: 86.72%] [G loss: 2.071318]\n",
      "epoch:32 step:30432 [D loss: 0.387340, acc.: 78.91%] [G loss: 1.122257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30433 [D loss: 0.025395, acc.: 99.22%] [G loss: 1.261419]\n",
      "epoch:32 step:30434 [D loss: 0.072739, acc.: 100.00%] [G loss: 0.308895]\n",
      "epoch:32 step:30435 [D loss: 0.658301, acc.: 63.28%] [G loss: 3.965903]\n",
      "epoch:32 step:30436 [D loss: 0.212610, acc.: 95.31%] [G loss: 4.495848]\n",
      "epoch:32 step:30437 [D loss: 0.443637, acc.: 72.66%] [G loss: 6.136315]\n",
      "epoch:32 step:30438 [D loss: 0.278274, acc.: 89.84%] [G loss: 6.526875]\n",
      "epoch:32 step:30439 [D loss: 0.252945, acc.: 86.72%] [G loss: 4.305770]\n",
      "epoch:32 step:30440 [D loss: 0.060121, acc.: 99.22%] [G loss: 3.584079]\n",
      "epoch:32 step:30441 [D loss: 0.266975, acc.: 87.50%] [G loss: 2.696172]\n",
      "epoch:32 step:30442 [D loss: 0.162972, acc.: 96.09%] [G loss: 5.290620]\n",
      "epoch:32 step:30443 [D loss: 0.074572, acc.: 99.22%] [G loss: 1.640926]\n",
      "epoch:32 step:30444 [D loss: 0.377506, acc.: 79.69%] [G loss: 1.106395]\n",
      "epoch:32 step:30445 [D loss: 0.475953, acc.: 75.00%] [G loss: 1.813465]\n",
      "epoch:32 step:30446 [D loss: 0.046863, acc.: 100.00%] [G loss: 5.119104]\n",
      "epoch:32 step:30447 [D loss: 0.141476, acc.: 96.09%] [G loss: 2.789558]\n",
      "epoch:32 step:30448 [D loss: 0.039883, acc.: 99.22%] [G loss: 4.622452]\n",
      "epoch:32 step:30449 [D loss: 0.524507, acc.: 82.81%] [G loss: 0.832321]\n",
      "epoch:32 step:30450 [D loss: 0.035503, acc.: 100.00%] [G loss: 3.735676]\n",
      "epoch:32 step:30451 [D loss: 0.499794, acc.: 67.97%] [G loss: 4.684134]\n",
      "epoch:32 step:30452 [D loss: 0.112181, acc.: 98.44%] [G loss: 3.336976]\n",
      "epoch:32 step:30453 [D loss: 0.478485, acc.: 75.78%] [G loss: 5.380105]\n",
      "epoch:32 step:30454 [D loss: 0.081310, acc.: 99.22%] [G loss: 4.948462]\n",
      "epoch:32 step:30455 [D loss: 0.076281, acc.: 99.22%] [G loss: 3.044136]\n",
      "epoch:32 step:30456 [D loss: 0.104347, acc.: 99.22%] [G loss: 2.288657]\n",
      "epoch:32 step:30457 [D loss: 0.076988, acc.: 99.22%] [G loss: 3.593514]\n",
      "epoch:32 step:30458 [D loss: 0.040943, acc.: 100.00%] [G loss: 6.712181]\n",
      "epoch:32 step:30459 [D loss: 0.080181, acc.: 99.22%] [G loss: 4.093856]\n",
      "epoch:32 step:30460 [D loss: 0.221556, acc.: 91.41%] [G loss: 6.345029]\n",
      "epoch:32 step:30461 [D loss: 0.126954, acc.: 96.88%] [G loss: 2.107783]\n",
      "epoch:32 step:30462 [D loss: 0.515734, acc.: 64.84%] [G loss: 3.818234]\n",
      "epoch:32 step:30463 [D loss: 0.108668, acc.: 94.53%] [G loss: 4.067031]\n",
      "epoch:32 step:30464 [D loss: 0.174103, acc.: 97.66%] [G loss: 7.356884]\n",
      "epoch:32 step:30465 [D loss: 0.309651, acc.: 92.19%] [G loss: 6.282884]\n",
      "epoch:32 step:30466 [D loss: 0.079635, acc.: 100.00%] [G loss: 7.578167]\n",
      "epoch:32 step:30467 [D loss: 0.313405, acc.: 83.59%] [G loss: 5.253886]\n",
      "epoch:32 step:30468 [D loss: 0.032860, acc.: 100.00%] [G loss: 5.594097]\n",
      "epoch:32 step:30469 [D loss: 0.238468, acc.: 90.62%] [G loss: 1.861966]\n",
      "epoch:32 step:30470 [D loss: 0.747100, acc.: 55.47%] [G loss: 1.907234]\n",
      "epoch:32 step:30471 [D loss: 0.020202, acc.: 100.00%] [G loss: 3.727053]\n",
      "epoch:32 step:30472 [D loss: 0.058206, acc.: 99.22%] [G loss: 5.490096]\n",
      "epoch:32 step:30473 [D loss: 0.288352, acc.: 84.38%] [G loss: 4.285429]\n",
      "epoch:32 step:30474 [D loss: 0.108444, acc.: 97.66%] [G loss: 1.471862]\n",
      "epoch:32 step:30475 [D loss: 0.835450, acc.: 59.38%] [G loss: 5.487870]\n",
      "epoch:32 step:30476 [D loss: 0.020955, acc.: 100.00%] [G loss: 3.799806]\n",
      "epoch:32 step:30477 [D loss: 0.239328, acc.: 89.06%] [G loss: 5.570695]\n",
      "epoch:32 step:30478 [D loss: 0.065474, acc.: 100.00%] [G loss: 5.785804]\n",
      "epoch:32 step:30479 [D loss: 0.165728, acc.: 95.31%] [G loss: 6.814891]\n",
      "epoch:32 step:30480 [D loss: 0.137877, acc.: 98.44%] [G loss: 4.172447]\n",
      "epoch:32 step:30481 [D loss: 0.194103, acc.: 90.62%] [G loss: 2.374580]\n",
      "epoch:32 step:30482 [D loss: 0.141300, acc.: 96.88%] [G loss: 3.426042]\n",
      "epoch:32 step:30483 [D loss: 0.279005, acc.: 91.41%] [G loss: 4.031549]\n",
      "epoch:32 step:30484 [D loss: 0.053531, acc.: 100.00%] [G loss: 3.671078]\n",
      "epoch:32 step:30485 [D loss: 2.033293, acc.: 52.34%] [G loss: 0.344776]\n",
      "epoch:32 step:30486 [D loss: 0.173135, acc.: 95.31%] [G loss: 2.530026]\n",
      "epoch:32 step:30487 [D loss: 0.816040, acc.: 52.34%] [G loss: 0.532342]\n",
      "epoch:32 step:30488 [D loss: 0.036605, acc.: 100.00%] [G loss: 1.314668]\n",
      "epoch:32 step:30489 [D loss: 0.086140, acc.: 97.66%] [G loss: 2.783946]\n",
      "epoch:32 step:30490 [D loss: 0.034611, acc.: 98.44%] [G loss: 1.338462]\n",
      "epoch:32 step:30491 [D loss: 0.667713, acc.: 64.84%] [G loss: 5.666702]\n",
      "epoch:32 step:30492 [D loss: 0.071535, acc.: 99.22%] [G loss: 3.590470]\n",
      "epoch:32 step:30493 [D loss: 0.409998, acc.: 86.72%] [G loss: 4.541430]\n",
      "epoch:32 step:30494 [D loss: 0.373757, acc.: 76.56%] [G loss: 5.682631]\n",
      "epoch:32 step:30495 [D loss: 0.135712, acc.: 96.09%] [G loss: 2.346495]\n",
      "epoch:32 step:30496 [D loss: 0.257302, acc.: 94.53%] [G loss: 3.308947]\n",
      "epoch:32 step:30497 [D loss: 0.412026, acc.: 76.56%] [G loss: 2.249476]\n",
      "epoch:32 step:30498 [D loss: 0.130556, acc.: 97.66%] [G loss: 5.142825]\n",
      "epoch:32 step:30499 [D loss: 0.359709, acc.: 89.84%] [G loss: 5.286311]\n",
      "epoch:32 step:30500 [D loss: 0.207883, acc.: 91.41%] [G loss: 2.382574]\n",
      "epoch:32 step:30501 [D loss: 0.110409, acc.: 98.44%] [G loss: 3.571110]\n",
      "epoch:32 step:30502 [D loss: 0.407548, acc.: 82.81%] [G loss: 1.788869]\n",
      "epoch:32 step:30503 [D loss: 0.335805, acc.: 83.59%] [G loss: 4.614945]\n",
      "epoch:32 step:30504 [D loss: 0.078598, acc.: 99.22%] [G loss: 4.580347]\n",
      "epoch:32 step:30505 [D loss: 0.059037, acc.: 100.00%] [G loss: 4.028651]\n",
      "epoch:32 step:30506 [D loss: 0.209460, acc.: 92.97%] [G loss: 3.669816]\n",
      "epoch:32 step:30507 [D loss: 0.340743, acc.: 80.47%] [G loss: 0.568813]\n",
      "epoch:32 step:30508 [D loss: 0.300718, acc.: 86.72%] [G loss: 1.925650]\n",
      "epoch:32 step:30509 [D loss: 1.949154, acc.: 18.75%] [G loss: 0.965940]\n",
      "epoch:32 step:30510 [D loss: 0.535627, acc.: 73.44%] [G loss: 3.051373]\n",
      "epoch:32 step:30511 [D loss: 0.151202, acc.: 93.75%] [G loss: 4.064146]\n",
      "epoch:32 step:30512 [D loss: 0.103724, acc.: 97.66%] [G loss: 1.258730]\n",
      "epoch:32 step:30513 [D loss: 0.071977, acc.: 100.00%] [G loss: 2.364543]\n",
      "epoch:32 step:30514 [D loss: 0.394354, acc.: 80.47%] [G loss: 1.379371]\n",
      "epoch:32 step:30515 [D loss: 0.552347, acc.: 65.62%] [G loss: 3.590895]\n",
      "epoch:32 step:30516 [D loss: 0.309913, acc.: 92.19%] [G loss: 2.953744]\n",
      "epoch:32 step:30517 [D loss: 0.139994, acc.: 96.09%] [G loss: 2.669904]\n",
      "epoch:32 step:30518 [D loss: 0.439365, acc.: 82.81%] [G loss: 3.730530]\n",
      "epoch:32 step:30519 [D loss: 0.065903, acc.: 100.00%] [G loss: 2.738651]\n",
      "epoch:32 step:30520 [D loss: 0.113637, acc.: 97.66%] [G loss: 2.611712]\n",
      "epoch:32 step:30521 [D loss: 0.018066, acc.: 100.00%] [G loss: 1.308016]\n",
      "epoch:32 step:30522 [D loss: 0.123973, acc.: 96.88%] [G loss: 5.029551]\n",
      "epoch:32 step:30523 [D loss: 0.188339, acc.: 97.66%] [G loss: 1.684326]\n",
      "epoch:32 step:30524 [D loss: 0.471325, acc.: 80.47%] [G loss: 3.463788]\n",
      "epoch:32 step:30525 [D loss: 0.100011, acc.: 97.66%] [G loss: 3.671485]\n",
      "epoch:32 step:30526 [D loss: 0.227854, acc.: 91.41%] [G loss: 3.304905]\n",
      "epoch:32 step:30527 [D loss: 0.043179, acc.: 99.22%] [G loss: 2.515054]\n",
      "epoch:32 step:30528 [D loss: 0.166861, acc.: 96.09%] [G loss: 3.731265]\n",
      "epoch:32 step:30529 [D loss: 0.789065, acc.: 58.59%] [G loss: 5.903519]\n",
      "epoch:32 step:30530 [D loss: 0.587861, acc.: 70.31%] [G loss: 1.716612]\n",
      "epoch:32 step:30531 [D loss: 0.925285, acc.: 57.81%] [G loss: 1.204188]\n",
      "epoch:32 step:30532 [D loss: 0.358346, acc.: 82.03%] [G loss: 2.172086]\n",
      "epoch:32 step:30533 [D loss: 0.276071, acc.: 88.28%] [G loss: 4.513434]\n",
      "epoch:32 step:30534 [D loss: 0.754297, acc.: 61.72%] [G loss: 3.442417]\n",
      "epoch:32 step:30535 [D loss: 1.105731, acc.: 47.66%] [G loss: 4.331159]\n",
      "epoch:32 step:30536 [D loss: 0.841749, acc.: 57.03%] [G loss: 1.626731]\n",
      "epoch:32 step:30537 [D loss: 0.398464, acc.: 84.38%] [G loss: 6.870230]\n",
      "epoch:32 step:30538 [D loss: 0.856577, acc.: 39.84%] [G loss: 1.676207]\n",
      "epoch:32 step:30539 [D loss: 0.189295, acc.: 92.19%] [G loss: 1.218928]\n",
      "epoch:32 step:30540 [D loss: 0.962955, acc.: 35.94%] [G loss: 1.812992]\n",
      "epoch:32 step:30541 [D loss: 0.051844, acc.: 100.00%] [G loss: 1.496147]\n",
      "epoch:32 step:30542 [D loss: 0.074948, acc.: 99.22%] [G loss: 1.358099]\n",
      "epoch:32 step:30543 [D loss: 0.085946, acc.: 100.00%] [G loss: 5.213075]\n",
      "epoch:32 step:30544 [D loss: 0.132472, acc.: 98.44%] [G loss: 5.089406]\n",
      "epoch:32 step:30545 [D loss: 0.176659, acc.: 94.53%] [G loss: 2.246893]\n",
      "epoch:32 step:30546 [D loss: 0.020925, acc.: 100.00%] [G loss: 0.827466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30547 [D loss: 0.150000, acc.: 96.88%] [G loss: 0.549733]\n",
      "epoch:32 step:30548 [D loss: 0.168948, acc.: 96.09%] [G loss: 3.379278]\n",
      "epoch:32 step:30549 [D loss: 0.079604, acc.: 98.44%] [G loss: 1.392012]\n",
      "epoch:32 step:30550 [D loss: 0.070599, acc.: 99.22%] [G loss: 4.989539]\n",
      "epoch:32 step:30551 [D loss: 0.104914, acc.: 100.00%] [G loss: 0.188420]\n",
      "epoch:32 step:30552 [D loss: 0.100037, acc.: 98.44%] [G loss: 4.255105]\n",
      "epoch:32 step:30553 [D loss: 0.112041, acc.: 97.66%] [G loss: 1.332348]\n",
      "epoch:32 step:30554 [D loss: 0.628468, acc.: 63.28%] [G loss: 0.619530]\n",
      "epoch:32 step:30555 [D loss: 0.321887, acc.: 89.84%] [G loss: 1.999535]\n",
      "epoch:32 step:30556 [D loss: 0.103048, acc.: 99.22%] [G loss: 0.926937]\n",
      "epoch:32 step:30557 [D loss: 0.349434, acc.: 85.16%] [G loss: 1.854742]\n",
      "epoch:32 step:30558 [D loss: 0.767889, acc.: 61.72%] [G loss: 3.232020]\n",
      "epoch:32 step:30559 [D loss: 0.253176, acc.: 87.50%] [G loss: 5.333557]\n",
      "epoch:32 step:30560 [D loss: 1.056519, acc.: 56.25%] [G loss: 5.344661]\n",
      "epoch:32 step:30561 [D loss: 0.074966, acc.: 98.44%] [G loss: 4.087379]\n",
      "epoch:32 step:30562 [D loss: 0.190676, acc.: 90.62%] [G loss: 6.287548]\n",
      "epoch:32 step:30563 [D loss: 0.157536, acc.: 96.88%] [G loss: 2.276241]\n",
      "epoch:32 step:30564 [D loss: 0.395435, acc.: 89.84%] [G loss: 2.050410]\n",
      "epoch:32 step:30565 [D loss: 0.417883, acc.: 80.47%] [G loss: 4.694754]\n",
      "epoch:32 step:30566 [D loss: 0.464430, acc.: 70.31%] [G loss: 2.104856]\n",
      "epoch:32 step:30567 [D loss: 0.754755, acc.: 56.25%] [G loss: 4.167781]\n",
      "epoch:32 step:30568 [D loss: 0.689641, acc.: 62.50%] [G loss: 1.732816]\n",
      "epoch:32 step:30569 [D loss: 0.444180, acc.: 77.34%] [G loss: 3.118367]\n",
      "epoch:32 step:30570 [D loss: 0.034152, acc.: 100.00%] [G loss: 4.483817]\n",
      "epoch:32 step:30571 [D loss: 1.128533, acc.: 52.34%] [G loss: 2.742565]\n",
      "epoch:32 step:30572 [D loss: 0.163166, acc.: 95.31%] [G loss: 4.625309]\n",
      "epoch:32 step:30573 [D loss: 0.382133, acc.: 85.94%] [G loss: 1.193813]\n",
      "epoch:32 step:30574 [D loss: 0.096469, acc.: 96.88%] [G loss: 2.119382]\n",
      "epoch:32 step:30575 [D loss: 1.579224, acc.: 51.56%] [G loss: 1.915499]\n",
      "epoch:32 step:30576 [D loss: 0.997800, acc.: 53.91%] [G loss: 3.299155]\n",
      "epoch:32 step:30577 [D loss: 0.211280, acc.: 93.75%] [G loss: 2.030956]\n",
      "epoch:32 step:30578 [D loss: 0.630372, acc.: 67.19%] [G loss: 1.495667]\n",
      "epoch:32 step:30579 [D loss: 0.446962, acc.: 75.78%] [G loss: 3.394591]\n",
      "epoch:32 step:30580 [D loss: 1.441761, acc.: 35.16%] [G loss: 2.286096]\n",
      "epoch:32 step:30581 [D loss: 0.355173, acc.: 85.94%] [G loss: 2.422154]\n",
      "epoch:32 step:30582 [D loss: 0.261856, acc.: 89.84%] [G loss: 3.763213]\n",
      "epoch:32 step:30583 [D loss: 0.516281, acc.: 71.88%] [G loss: 3.252831]\n",
      "epoch:32 step:30584 [D loss: 0.257185, acc.: 93.75%] [G loss: 2.053607]\n",
      "epoch:32 step:30585 [D loss: 0.171290, acc.: 95.31%] [G loss: 4.333801]\n",
      "epoch:32 step:30586 [D loss: 0.213259, acc.: 93.75%] [G loss: 5.947459]\n",
      "epoch:32 step:30587 [D loss: 0.535796, acc.: 75.78%] [G loss: 4.064278]\n",
      "epoch:32 step:30588 [D loss: 0.175782, acc.: 98.44%] [G loss: 1.990080]\n",
      "epoch:32 step:30589 [D loss: 0.329576, acc.: 89.84%] [G loss: 1.892990]\n",
      "epoch:32 step:30590 [D loss: 1.051518, acc.: 54.69%] [G loss: 1.603878]\n",
      "epoch:32 step:30591 [D loss: 0.216408, acc.: 91.41%] [G loss: 0.848838]\n",
      "epoch:32 step:30592 [D loss: 0.463764, acc.: 72.66%] [G loss: 2.673856]\n",
      "epoch:32 step:30593 [D loss: 0.139347, acc.: 96.88%] [G loss: 4.070395]\n",
      "epoch:32 step:30594 [D loss: 0.211033, acc.: 96.88%] [G loss: 2.071359]\n",
      "epoch:32 step:30595 [D loss: 0.427478, acc.: 81.25%] [G loss: 4.549260]\n",
      "epoch:32 step:30596 [D loss: 0.453692, acc.: 69.53%] [G loss: 3.639483]\n",
      "epoch:32 step:30597 [D loss: 0.023971, acc.: 100.00%] [G loss: 5.835040]\n",
      "epoch:32 step:30598 [D loss: 0.087157, acc.: 96.88%] [G loss: 5.009721]\n",
      "epoch:32 step:30599 [D loss: 0.068866, acc.: 99.22%] [G loss: 2.848209]\n",
      "epoch:32 step:30600 [D loss: 0.317305, acc.: 92.97%] [G loss: 2.182792]\n",
      "epoch:32 step:30601 [D loss: 0.178136, acc.: 97.66%] [G loss: 3.835093]\n",
      "epoch:32 step:30602 [D loss: 0.251961, acc.: 94.53%] [G loss: 4.294450]\n",
      "epoch:32 step:30603 [D loss: 0.327037, acc.: 88.28%] [G loss: 4.526890]\n",
      "epoch:32 step:30604 [D loss: 0.472530, acc.: 75.00%] [G loss: 2.145062]\n",
      "epoch:32 step:30605 [D loss: 0.112175, acc.: 100.00%] [G loss: 2.564003]\n",
      "epoch:32 step:30606 [D loss: 0.085624, acc.: 99.22%] [G loss: 2.277197]\n",
      "epoch:32 step:30607 [D loss: 0.135774, acc.: 96.09%] [G loss: 3.082032]\n",
      "epoch:32 step:30608 [D loss: 0.212031, acc.: 95.31%] [G loss: 2.442461]\n",
      "epoch:32 step:30609 [D loss: 0.129473, acc.: 99.22%] [G loss: 2.209475]\n",
      "epoch:32 step:30610 [D loss: 0.230399, acc.: 94.53%] [G loss: 1.034160]\n",
      "epoch:32 step:30611 [D loss: 0.303370, acc.: 82.03%] [G loss: 1.090560]\n",
      "epoch:32 step:30612 [D loss: 0.168303, acc.: 93.75%] [G loss: 1.548293]\n",
      "epoch:32 step:30613 [D loss: 0.063020, acc.: 99.22%] [G loss: 2.836166]\n",
      "epoch:32 step:30614 [D loss: 0.232891, acc.: 91.41%] [G loss: 1.317414]\n",
      "epoch:32 step:30615 [D loss: 0.325459, acc.: 88.28%] [G loss: 2.418405]\n",
      "epoch:32 step:30616 [D loss: 0.130543, acc.: 96.88%] [G loss: 0.799118]\n",
      "epoch:32 step:30617 [D loss: 0.242600, acc.: 87.50%] [G loss: 5.776882]\n",
      "epoch:32 step:30618 [D loss: 0.310784, acc.: 87.50%] [G loss: 4.648660]\n",
      "epoch:32 step:30619 [D loss: 0.126759, acc.: 96.88%] [G loss: 1.310744]\n",
      "epoch:32 step:30620 [D loss: 0.208435, acc.: 98.44%] [G loss: 4.477607]\n",
      "epoch:32 step:30621 [D loss: 0.164690, acc.: 96.09%] [G loss: 3.985956]\n",
      "epoch:32 step:30622 [D loss: 0.294462, acc.: 89.06%] [G loss: 5.119397]\n",
      "epoch:32 step:30623 [D loss: 0.151234, acc.: 96.09%] [G loss: 4.137765]\n",
      "epoch:32 step:30624 [D loss: 1.491023, acc.: 21.88%] [G loss: 3.637107]\n",
      "epoch:32 step:30625 [D loss: 0.200287, acc.: 93.75%] [G loss: 4.657091]\n",
      "epoch:32 step:30626 [D loss: 0.138653, acc.: 96.09%] [G loss: 2.758116]\n",
      "epoch:32 step:30627 [D loss: 0.085148, acc.: 99.22%] [G loss: 4.832403]\n",
      "epoch:32 step:30628 [D loss: 0.096413, acc.: 100.00%] [G loss: 3.032609]\n",
      "epoch:32 step:30629 [D loss: 0.149814, acc.: 96.88%] [G loss: 4.599534]\n",
      "epoch:32 step:30630 [D loss: 0.163880, acc.: 95.31%] [G loss: 3.816355]\n",
      "epoch:32 step:30631 [D loss: 0.432575, acc.: 85.94%] [G loss: 2.218561]\n",
      "epoch:32 step:30632 [D loss: 0.098077, acc.: 97.66%] [G loss: 5.045701]\n",
      "epoch:32 step:30633 [D loss: 0.169759, acc.: 96.88%] [G loss: 1.318087]\n",
      "epoch:32 step:30634 [D loss: 0.305605, acc.: 89.06%] [G loss: 2.560428]\n",
      "epoch:32 step:30635 [D loss: 0.837835, acc.: 53.91%] [G loss: 5.917391]\n",
      "epoch:32 step:30636 [D loss: 0.693857, acc.: 64.06%] [G loss: 7.377013]\n",
      "epoch:32 step:30637 [D loss: 0.142705, acc.: 98.44%] [G loss: 4.819023]\n",
      "epoch:32 step:30638 [D loss: 0.280318, acc.: 89.06%] [G loss: 4.422383]\n",
      "epoch:32 step:30639 [D loss: 0.025131, acc.: 100.00%] [G loss: 1.664221]\n",
      "epoch:32 step:30640 [D loss: 0.053096, acc.: 98.44%] [G loss: 4.511038]\n",
      "epoch:32 step:30641 [D loss: 0.085391, acc.: 98.44%] [G loss: 3.045162]\n",
      "epoch:32 step:30642 [D loss: 0.793203, acc.: 59.38%] [G loss: 4.943546]\n",
      "epoch:32 step:30643 [D loss: 0.659957, acc.: 62.50%] [G loss: 4.750709]\n",
      "epoch:32 step:30644 [D loss: 0.048503, acc.: 100.00%] [G loss: 1.505381]\n",
      "epoch:32 step:30645 [D loss: 0.128964, acc.: 98.44%] [G loss: 2.736042]\n",
      "epoch:32 step:30646 [D loss: 0.684527, acc.: 65.62%] [G loss: 7.098110]\n",
      "epoch:32 step:30647 [D loss: 0.125536, acc.: 97.66%] [G loss: 2.583035]\n",
      "epoch:32 step:30648 [D loss: 0.587648, acc.: 67.97%] [G loss: 1.518827]\n",
      "epoch:32 step:30649 [D loss: 0.117359, acc.: 97.66%] [G loss: 1.873594]\n",
      "epoch:32 step:30650 [D loss: 0.961176, acc.: 50.78%] [G loss: 0.584544]\n",
      "epoch:32 step:30651 [D loss: 0.468655, acc.: 76.56%] [G loss: 2.361207]\n",
      "epoch:32 step:30652 [D loss: 0.411281, acc.: 78.12%] [G loss: 1.971576]\n",
      "epoch:32 step:30653 [D loss: 0.225276, acc.: 92.97%] [G loss: 3.574728]\n",
      "epoch:32 step:30654 [D loss: 0.031154, acc.: 99.22%] [G loss: 3.901758]\n",
      "epoch:32 step:30655 [D loss: 0.154793, acc.: 98.44%] [G loss: 1.576012]\n",
      "epoch:32 step:30656 [D loss: 0.085589, acc.: 99.22%] [G loss: 4.117632]\n",
      "epoch:32 step:30657 [D loss: 0.976380, acc.: 41.41%] [G loss: 3.177749]\n",
      "epoch:32 step:30658 [D loss: 0.126967, acc.: 96.88%] [G loss: 2.705475]\n",
      "epoch:32 step:30659 [D loss: 0.198201, acc.: 92.19%] [G loss: 2.309878]\n",
      "epoch:32 step:30660 [D loss: 0.052475, acc.: 100.00%] [G loss: 1.960821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30661 [D loss: 0.077334, acc.: 99.22%] [G loss: 1.018028]\n",
      "epoch:32 step:30662 [D loss: 0.144671, acc.: 97.66%] [G loss: 2.772384]\n",
      "epoch:32 step:30663 [D loss: 0.096992, acc.: 98.44%] [G loss: 1.334020]\n",
      "epoch:32 step:30664 [D loss: 0.529716, acc.: 67.19%] [G loss: 2.741694]\n",
      "epoch:32 step:30665 [D loss: 0.630736, acc.: 64.06%] [G loss: 2.080157]\n",
      "epoch:32 step:30666 [D loss: 0.112284, acc.: 98.44%] [G loss: 2.871848]\n",
      "epoch:32 step:30667 [D loss: 0.387106, acc.: 81.25%] [G loss: 1.886606]\n",
      "epoch:32 step:30668 [D loss: 0.708754, acc.: 62.50%] [G loss: 6.254666]\n",
      "epoch:32 step:30669 [D loss: 0.046184, acc.: 99.22%] [G loss: 4.162181]\n",
      "epoch:32 step:30670 [D loss: 0.610002, acc.: 66.41%] [G loss: 2.886115]\n",
      "epoch:32 step:30671 [D loss: 0.069073, acc.: 99.22%] [G loss: 1.854894]\n",
      "epoch:32 step:30672 [D loss: 0.405211, acc.: 82.81%] [G loss: 2.663764]\n",
      "epoch:32 step:30673 [D loss: 0.692630, acc.: 59.38%] [G loss: 2.410564]\n",
      "epoch:32 step:30674 [D loss: 0.075651, acc.: 99.22%] [G loss: 2.974917]\n",
      "epoch:32 step:30675 [D loss: 0.155420, acc.: 96.88%] [G loss: 4.756919]\n",
      "epoch:32 step:30676 [D loss: 0.169936, acc.: 94.53%] [G loss: 2.985944]\n",
      "epoch:32 step:30677 [D loss: 0.513589, acc.: 72.66%] [G loss: 2.249834]\n",
      "epoch:32 step:30678 [D loss: 0.490576, acc.: 73.44%] [G loss: 2.161718]\n",
      "epoch:32 step:30679 [D loss: 0.411544, acc.: 78.12%] [G loss: 2.911748]\n",
      "epoch:32 step:30680 [D loss: 0.110132, acc.: 98.44%] [G loss: 2.609640]\n",
      "epoch:32 step:30681 [D loss: 0.014251, acc.: 100.00%] [G loss: 2.814618]\n",
      "epoch:32 step:30682 [D loss: 1.352170, acc.: 41.41%] [G loss: 1.622359]\n",
      "epoch:32 step:30683 [D loss: 0.043994, acc.: 100.00%] [G loss: 4.072752]\n",
      "epoch:32 step:30684 [D loss: 0.689434, acc.: 62.50%] [G loss: 1.900296]\n",
      "epoch:32 step:30685 [D loss: 0.127932, acc.: 97.66%] [G loss: 2.650839]\n",
      "epoch:32 step:30686 [D loss: 0.371034, acc.: 81.25%] [G loss: 1.489137]\n",
      "epoch:32 step:30687 [D loss: 0.415657, acc.: 83.59%] [G loss: 0.553447]\n",
      "epoch:32 step:30688 [D loss: 0.146615, acc.: 99.22%] [G loss: 2.481373]\n",
      "epoch:32 step:30689 [D loss: 0.064739, acc.: 99.22%] [G loss: 1.726174]\n",
      "epoch:32 step:30690 [D loss: 0.103642, acc.: 99.22%] [G loss: 2.241809]\n",
      "epoch:32 step:30691 [D loss: 0.037910, acc.: 99.22%] [G loss: 3.565884]\n",
      "epoch:32 step:30692 [D loss: 0.361278, acc.: 85.16%] [G loss: 1.026649]\n",
      "epoch:32 step:30693 [D loss: 0.517129, acc.: 63.28%] [G loss: 2.122424]\n",
      "epoch:32 step:30694 [D loss: 0.814906, acc.: 55.47%] [G loss: 1.502210]\n",
      "epoch:32 step:30695 [D loss: 0.528927, acc.: 66.41%] [G loss: 3.465058]\n",
      "epoch:32 step:30696 [D loss: 0.188002, acc.: 94.53%] [G loss: 3.038203]\n",
      "epoch:32 step:30697 [D loss: 0.332823, acc.: 85.16%] [G loss: 4.405457]\n",
      "epoch:32 step:30698 [D loss: 0.037138, acc.: 100.00%] [G loss: 3.052578]\n",
      "epoch:32 step:30699 [D loss: 0.050812, acc.: 100.00%] [G loss: 2.915376]\n",
      "epoch:32 step:30700 [D loss: 0.403624, acc.: 75.78%] [G loss: 1.655413]\n",
      "epoch:32 step:30701 [D loss: 0.225106, acc.: 90.62%] [G loss: 4.993587]\n",
      "epoch:32 step:30702 [D loss: 0.156959, acc.: 98.44%] [G loss: 7.646297]\n",
      "epoch:32 step:30703 [D loss: 0.624385, acc.: 63.28%] [G loss: 0.941075]\n",
      "epoch:32 step:30704 [D loss: 0.399995, acc.: 82.03%] [G loss: 2.985768]\n",
      "epoch:32 step:30705 [D loss: 0.243282, acc.: 91.41%] [G loss: 2.099625]\n",
      "epoch:32 step:30706 [D loss: 0.332941, acc.: 85.16%] [G loss: 4.152280]\n",
      "epoch:32 step:30707 [D loss: 0.335020, acc.: 84.38%] [G loss: 1.798450]\n",
      "epoch:32 step:30708 [D loss: 0.243954, acc.: 92.19%] [G loss: 2.380625]\n",
      "epoch:32 step:30709 [D loss: 0.617898, acc.: 65.62%] [G loss: 2.977668]\n",
      "epoch:32 step:30710 [D loss: 0.135390, acc.: 99.22%] [G loss: 0.662308]\n",
      "epoch:32 step:30711 [D loss: 0.166550, acc.: 96.88%] [G loss: 1.030784]\n",
      "epoch:32 step:30712 [D loss: 0.718850, acc.: 60.94%] [G loss: 3.524570]\n",
      "epoch:32 step:30713 [D loss: 0.124703, acc.: 96.09%] [G loss: 2.784348]\n",
      "epoch:32 step:30714 [D loss: 0.315473, acc.: 87.50%] [G loss: 3.177805]\n",
      "epoch:32 step:30715 [D loss: 0.357870, acc.: 82.81%] [G loss: 3.388804]\n",
      "epoch:32 step:30716 [D loss: 1.230513, acc.: 31.25%] [G loss: 2.194804]\n",
      "epoch:32 step:30717 [D loss: 0.051235, acc.: 99.22%] [G loss: 2.146252]\n",
      "epoch:32 step:30718 [D loss: 0.030790, acc.: 99.22%] [G loss: 4.038224]\n",
      "epoch:32 step:30719 [D loss: 0.219115, acc.: 94.53%] [G loss: 1.281253]\n",
      "epoch:32 step:30720 [D loss: 0.389959, acc.: 78.12%] [G loss: 0.688439]\n",
      "epoch:32 step:30721 [D loss: 0.107311, acc.: 99.22%] [G loss: 3.672601]\n",
      "epoch:32 step:30722 [D loss: 0.691056, acc.: 59.38%] [G loss: 4.559001]\n",
      "epoch:32 step:30723 [D loss: 0.098661, acc.: 96.88%] [G loss: 1.957013]\n",
      "epoch:32 step:30724 [D loss: 0.379427, acc.: 83.59%] [G loss: 2.425685]\n",
      "epoch:32 step:30725 [D loss: 0.277798, acc.: 89.84%] [G loss: 1.400933]\n",
      "epoch:32 step:30726 [D loss: 0.310292, acc.: 84.38%] [G loss: 3.087779]\n",
      "epoch:32 step:30727 [D loss: 0.426952, acc.: 77.34%] [G loss: 1.489134]\n",
      "epoch:32 step:30728 [D loss: 0.582774, acc.: 68.75%] [G loss: 0.614027]\n",
      "epoch:32 step:30729 [D loss: 0.399536, acc.: 81.25%] [G loss: 5.355887]\n",
      "epoch:32 step:30730 [D loss: 0.963012, acc.: 50.00%] [G loss: 3.279936]\n",
      "epoch:32 step:30731 [D loss: 0.318052, acc.: 83.59%] [G loss: 3.757042]\n",
      "epoch:32 step:30732 [D loss: 0.168949, acc.: 96.88%] [G loss: 5.888996]\n",
      "epoch:32 step:30733 [D loss: 0.173014, acc.: 95.31%] [G loss: 2.812949]\n",
      "epoch:32 step:30734 [D loss: 0.047034, acc.: 100.00%] [G loss: 4.582382]\n",
      "epoch:32 step:30735 [D loss: 0.145623, acc.: 94.53%] [G loss: 5.953233]\n",
      "epoch:32 step:30736 [D loss: 0.277876, acc.: 89.84%] [G loss: 4.603586]\n",
      "epoch:32 step:30737 [D loss: 0.119548, acc.: 96.88%] [G loss: 1.276206]\n",
      "epoch:32 step:30738 [D loss: 0.319648, acc.: 83.59%] [G loss: 4.346872]\n",
      "epoch:32 step:30739 [D loss: 1.720032, acc.: 17.19%] [G loss: 2.438891]\n",
      "epoch:32 step:30740 [D loss: 0.054694, acc.: 99.22%] [G loss: 5.931242]\n",
      "epoch:32 step:30741 [D loss: 0.281511, acc.: 89.06%] [G loss: 3.245762]\n",
      "epoch:32 step:30742 [D loss: 0.037062, acc.: 98.44%] [G loss: 4.149319]\n",
      "epoch:32 step:30743 [D loss: 0.519453, acc.: 70.31%] [G loss: 2.210059]\n",
      "epoch:32 step:30744 [D loss: 0.162866, acc.: 96.88%] [G loss: 2.860142]\n",
      "epoch:32 step:30745 [D loss: 0.718521, acc.: 57.03%] [G loss: 1.403615]\n",
      "epoch:32 step:30746 [D loss: 0.253375, acc.: 85.94%] [G loss: 5.447988]\n",
      "epoch:32 step:30747 [D loss: 0.986272, acc.: 44.53%] [G loss: 5.564671]\n",
      "epoch:32 step:30748 [D loss: 0.088661, acc.: 98.44%] [G loss: 4.431911]\n",
      "epoch:32 step:30749 [D loss: 0.258643, acc.: 88.28%] [G loss: 3.980226]\n",
      "epoch:32 step:30750 [D loss: 0.141364, acc.: 96.88%] [G loss: 0.979715]\n",
      "epoch:32 step:30751 [D loss: 0.186309, acc.: 94.53%] [G loss: 4.897795]\n",
      "epoch:32 step:30752 [D loss: 0.344963, acc.: 84.38%] [G loss: 4.567748]\n",
      "epoch:32 step:30753 [D loss: 0.282237, acc.: 86.72%] [G loss: 1.391935]\n",
      "epoch:32 step:30754 [D loss: 0.118981, acc.: 96.88%] [G loss: 4.842882]\n",
      "epoch:32 step:30755 [D loss: 0.180347, acc.: 96.09%] [G loss: 4.866444]\n",
      "epoch:32 step:30756 [D loss: 0.217194, acc.: 96.88%] [G loss: 2.857284]\n",
      "epoch:32 step:30757 [D loss: 0.203122, acc.: 94.53%] [G loss: 0.792877]\n",
      "epoch:32 step:30758 [D loss: 0.199365, acc.: 97.66%] [G loss: 3.873597]\n",
      "epoch:32 step:30759 [D loss: 0.109467, acc.: 100.00%] [G loss: 5.117948]\n",
      "epoch:32 step:30760 [D loss: 0.109579, acc.: 99.22%] [G loss: 2.568727]\n",
      "epoch:32 step:30761 [D loss: 0.379754, acc.: 85.94%] [G loss: 2.063176]\n",
      "epoch:32 step:30762 [D loss: 0.125936, acc.: 99.22%] [G loss: 5.640139]\n",
      "epoch:32 step:30763 [D loss: 1.089224, acc.: 54.69%] [G loss: 3.630093]\n",
      "epoch:32 step:30764 [D loss: 0.096999, acc.: 97.66%] [G loss: 2.427605]\n",
      "epoch:32 step:30765 [D loss: 0.254022, acc.: 88.28%] [G loss: 2.210286]\n",
      "epoch:32 step:30766 [D loss: 0.071440, acc.: 100.00%] [G loss: 1.363630]\n",
      "epoch:32 step:30767 [D loss: 0.457809, acc.: 73.44%] [G loss: 2.504234]\n",
      "epoch:32 step:30768 [D loss: 0.337853, acc.: 83.59%] [G loss: 4.215445]\n",
      "epoch:32 step:30769 [D loss: 0.316358, acc.: 83.59%] [G loss: 3.897861]\n",
      "epoch:32 step:30770 [D loss: 0.037222, acc.: 99.22%] [G loss: 4.589380]\n",
      "epoch:32 step:30771 [D loss: 0.498040, acc.: 78.91%] [G loss: 4.396106]\n",
      "epoch:32 step:30772 [D loss: 0.298487, acc.: 91.41%] [G loss: 2.987614]\n",
      "epoch:32 step:30773 [D loss: 1.755168, acc.: 50.00%] [G loss: 2.440359]\n",
      "epoch:32 step:30774 [D loss: 0.832114, acc.: 57.03%] [G loss: 2.434157]\n",
      "epoch:32 step:30775 [D loss: 0.707320, acc.: 60.16%] [G loss: 0.921506]\n",
      "epoch:32 step:30776 [D loss: 0.254890, acc.: 92.97%] [G loss: 1.694989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30777 [D loss: 0.299946, acc.: 89.84%] [G loss: 1.345147]\n",
      "epoch:32 step:30778 [D loss: 0.961708, acc.: 50.78%] [G loss: 4.248686]\n",
      "epoch:32 step:30779 [D loss: 0.392262, acc.: 85.94%] [G loss: 2.866621]\n",
      "epoch:32 step:30780 [D loss: 0.228960, acc.: 89.84%] [G loss: 3.413481]\n",
      "epoch:32 step:30781 [D loss: 0.146302, acc.: 95.31%] [G loss: 2.024253]\n",
      "epoch:32 step:30782 [D loss: 0.211940, acc.: 91.41%] [G loss: 0.643448]\n",
      "epoch:32 step:30783 [D loss: 0.287754, acc.: 92.97%] [G loss: 3.409958]\n",
      "epoch:32 step:30784 [D loss: 0.243935, acc.: 92.97%] [G loss: 3.036980]\n",
      "epoch:32 step:30785 [D loss: 0.059496, acc.: 100.00%] [G loss: 2.924489]\n",
      "epoch:32 step:30786 [D loss: 0.181436, acc.: 96.09%] [G loss: 1.827840]\n",
      "epoch:32 step:30787 [D loss: 0.042742, acc.: 100.00%] [G loss: 3.443528]\n",
      "epoch:32 step:30788 [D loss: 1.421727, acc.: 51.56%] [G loss: 3.324536]\n",
      "epoch:32 step:30789 [D loss: 0.113047, acc.: 99.22%] [G loss: 1.817224]\n",
      "epoch:32 step:30790 [D loss: 0.387153, acc.: 73.44%] [G loss: 5.052633]\n",
      "epoch:32 step:30791 [D loss: 0.366820, acc.: 82.81%] [G loss: 0.738036]\n",
      "epoch:32 step:30792 [D loss: 0.233007, acc.: 90.62%] [G loss: 1.196533]\n",
      "epoch:32 step:30793 [D loss: 0.117438, acc.: 99.22%] [G loss: 2.574558]\n",
      "epoch:32 step:30794 [D loss: 0.059124, acc.: 99.22%] [G loss: 1.511409]\n",
      "epoch:32 step:30795 [D loss: 0.236059, acc.: 93.75%] [G loss: 2.468282]\n",
      "epoch:32 step:30796 [D loss: 0.025918, acc.: 100.00%] [G loss: 4.452803]\n",
      "epoch:32 step:30797 [D loss: 0.154662, acc.: 97.66%] [G loss: 1.896419]\n",
      "epoch:32 step:30798 [D loss: 0.113625, acc.: 99.22%] [G loss: 0.598581]\n",
      "epoch:32 step:30799 [D loss: 0.227719, acc.: 95.31%] [G loss: 2.030427]\n",
      "epoch:32 step:30800 [D loss: 0.232716, acc.: 95.31%] [G loss: 3.459367]\n",
      "epoch:32 step:30801 [D loss: 0.340550, acc.: 85.16%] [G loss: 1.872035]\n",
      "epoch:32 step:30802 [D loss: 0.356583, acc.: 85.16%] [G loss: 4.837179]\n",
      "epoch:32 step:30803 [D loss: 0.183739, acc.: 94.53%] [G loss: 4.134915]\n",
      "epoch:32 step:30804 [D loss: 0.181338, acc.: 96.09%] [G loss: 3.630819]\n",
      "epoch:32 step:30805 [D loss: 0.398980, acc.: 85.16%] [G loss: 5.251228]\n",
      "epoch:32 step:30806 [D loss: 0.561412, acc.: 68.75%] [G loss: 2.885433]\n",
      "epoch:32 step:30807 [D loss: 0.213563, acc.: 94.53%] [G loss: 2.158337]\n",
      "epoch:32 step:30808 [D loss: 0.659519, acc.: 62.50%] [G loss: 3.863789]\n",
      "epoch:32 step:30809 [D loss: 0.641007, acc.: 66.41%] [G loss: 4.004070]\n",
      "epoch:32 step:30810 [D loss: 0.209399, acc.: 94.53%] [G loss: 0.653001]\n",
      "epoch:32 step:30811 [D loss: 0.194811, acc.: 96.09%] [G loss: 1.504819]\n",
      "epoch:32 step:30812 [D loss: 0.239801, acc.: 94.53%] [G loss: 4.546882]\n",
      "epoch:32 step:30813 [D loss: 0.159802, acc.: 96.88%] [G loss: 3.023601]\n",
      "epoch:32 step:30814 [D loss: 0.129190, acc.: 98.44%] [G loss: 2.418469]\n",
      "epoch:32 step:30815 [D loss: 0.310429, acc.: 87.50%] [G loss: 3.794870]\n",
      "epoch:32 step:30816 [D loss: 0.170599, acc.: 93.75%] [G loss: 3.576280]\n",
      "epoch:32 step:30817 [D loss: 0.395941, acc.: 79.69%] [G loss: 5.138104]\n",
      "epoch:32 step:30818 [D loss: 0.353046, acc.: 83.59%] [G loss: 2.360267]\n",
      "epoch:32 step:30819 [D loss: 0.108416, acc.: 96.88%] [G loss: 2.311918]\n",
      "epoch:32 step:30820 [D loss: 0.374005, acc.: 82.03%] [G loss: 1.870016]\n",
      "epoch:32 step:30821 [D loss: 0.050982, acc.: 100.00%] [G loss: 1.101571]\n",
      "epoch:32 step:30822 [D loss: 0.033149, acc.: 100.00%] [G loss: 2.699865]\n",
      "epoch:32 step:30823 [D loss: 0.120525, acc.: 98.44%] [G loss: 2.792468]\n",
      "epoch:32 step:30824 [D loss: 0.130275, acc.: 96.09%] [G loss: 3.743003]\n",
      "epoch:32 step:30825 [D loss: 0.020418, acc.: 99.22%] [G loss: 1.809927]\n",
      "epoch:32 step:30826 [D loss: 0.106741, acc.: 99.22%] [G loss: 3.243942]\n",
      "epoch:32 step:30827 [D loss: 0.049165, acc.: 100.00%] [G loss: 4.129670]\n",
      "epoch:32 step:30828 [D loss: 0.373299, acc.: 75.78%] [G loss: 3.850577]\n",
      "epoch:32 step:30829 [D loss: 0.050026, acc.: 100.00%] [G loss: 3.272788]\n",
      "epoch:32 step:30830 [D loss: 0.105430, acc.: 97.66%] [G loss: 4.684442]\n",
      "epoch:32 step:30831 [D loss: 0.077329, acc.: 98.44%] [G loss: 2.710954]\n",
      "epoch:32 step:30832 [D loss: 0.135541, acc.: 97.66%] [G loss: 4.492354]\n",
      "epoch:32 step:30833 [D loss: 0.355210, acc.: 82.81%] [G loss: 2.700203]\n",
      "epoch:32 step:30834 [D loss: 0.716675, acc.: 62.50%] [G loss: 5.838540]\n",
      "epoch:32 step:30835 [D loss: 0.156928, acc.: 96.09%] [G loss: 3.564119]\n",
      "epoch:32 step:30836 [D loss: 0.739578, acc.: 57.81%] [G loss: 1.761117]\n",
      "epoch:32 step:30837 [D loss: 0.194918, acc.: 93.75%] [G loss: 4.133705]\n",
      "epoch:32 step:30838 [D loss: 0.320153, acc.: 91.41%] [G loss: 3.825503]\n",
      "epoch:32 step:30839 [D loss: 0.219900, acc.: 95.31%] [G loss: 2.443218]\n",
      "epoch:32 step:30840 [D loss: 1.242773, acc.: 32.81%] [G loss: 2.793328]\n",
      "epoch:32 step:30841 [D loss: 0.255702, acc.: 91.41%] [G loss: 2.124462]\n",
      "epoch:32 step:30842 [D loss: 0.163750, acc.: 96.88%] [G loss: 3.524203]\n",
      "epoch:32 step:30843 [D loss: 0.129857, acc.: 96.09%] [G loss: 2.424489]\n",
      "epoch:32 step:30844 [D loss: 0.209221, acc.: 97.66%] [G loss: 1.928159]\n",
      "epoch:32 step:30845 [D loss: 0.810170, acc.: 54.69%] [G loss: 6.642128]\n",
      "epoch:32 step:30846 [D loss: 0.449922, acc.: 83.59%] [G loss: 3.867383]\n",
      "epoch:32 step:30847 [D loss: 0.138723, acc.: 96.88%] [G loss: 2.482976]\n",
      "epoch:32 step:30848 [D loss: 0.263383, acc.: 85.94%] [G loss: 4.706935]\n",
      "epoch:32 step:30849 [D loss: 0.119483, acc.: 96.88%] [G loss: 1.407227]\n",
      "epoch:32 step:30850 [D loss: 0.218151, acc.: 95.31%] [G loss: 2.267663]\n",
      "epoch:32 step:30851 [D loss: 0.176042, acc.: 95.31%] [G loss: 5.765027]\n",
      "epoch:32 step:30852 [D loss: 0.364163, acc.: 82.03%] [G loss: 1.964701]\n",
      "epoch:32 step:30853 [D loss: 0.054955, acc.: 98.44%] [G loss: 1.684781]\n",
      "epoch:32 step:30854 [D loss: 0.198180, acc.: 92.19%] [G loss: 2.437833]\n",
      "epoch:32 step:30855 [D loss: 0.036640, acc.: 99.22%] [G loss: 2.343480]\n",
      "epoch:32 step:30856 [D loss: 0.195369, acc.: 93.75%] [G loss: 3.105338]\n",
      "epoch:32 step:30857 [D loss: 0.084736, acc.: 98.44%] [G loss: 4.241812]\n",
      "epoch:32 step:30858 [D loss: 1.604762, acc.: 32.03%] [G loss: 3.856206]\n",
      "epoch:32 step:30859 [D loss: 0.110957, acc.: 98.44%] [G loss: 2.866597]\n",
      "epoch:32 step:30860 [D loss: 0.193328, acc.: 92.19%] [G loss: 2.579926]\n",
      "epoch:32 step:30861 [D loss: 0.181489, acc.: 94.53%] [G loss: 3.934122]\n",
      "epoch:32 step:30862 [D loss: 0.335439, acc.: 81.25%] [G loss: 3.582871]\n",
      "epoch:32 step:30863 [D loss: 0.321319, acc.: 87.50%] [G loss: 2.329559]\n",
      "epoch:32 step:30864 [D loss: 0.305469, acc.: 82.03%] [G loss: 1.458906]\n",
      "epoch:32 step:30865 [D loss: 0.235452, acc.: 89.06%] [G loss: 2.555981]\n",
      "epoch:32 step:30866 [D loss: 0.149661, acc.: 96.88%] [G loss: 0.956558]\n",
      "epoch:32 step:30867 [D loss: 0.385641, acc.: 77.34%] [G loss: 0.417840]\n",
      "epoch:32 step:30868 [D loss: 0.152433, acc.: 96.09%] [G loss: 5.531321]\n",
      "epoch:32 step:30869 [D loss: 1.022200, acc.: 57.81%] [G loss: 4.101505]\n",
      "epoch:32 step:30870 [D loss: 0.150915, acc.: 93.75%] [G loss: 3.287238]\n",
      "epoch:32 step:30871 [D loss: 0.184547, acc.: 93.75%] [G loss: 2.062497]\n",
      "epoch:32 step:30872 [D loss: 0.027006, acc.: 100.00%] [G loss: 3.032077]\n",
      "epoch:32 step:30873 [D loss: 0.728534, acc.: 60.16%] [G loss: 5.093914]\n",
      "epoch:32 step:30874 [D loss: 0.159382, acc.: 96.88%] [G loss: 6.276197]\n",
      "epoch:32 step:30875 [D loss: 0.057228, acc.: 100.00%] [G loss: 2.284460]\n",
      "epoch:32 step:30876 [D loss: 0.050238, acc.: 100.00%] [G loss: 4.985278]\n",
      "epoch:32 step:30877 [D loss: 0.877284, acc.: 45.31%] [G loss: 4.728021]\n",
      "epoch:32 step:30878 [D loss: 0.189946, acc.: 96.09%] [G loss: 2.923867]\n",
      "epoch:32 step:30879 [D loss: 0.054887, acc.: 99.22%] [G loss: 1.288443]\n",
      "epoch:32 step:30880 [D loss: 0.007422, acc.: 100.00%] [G loss: 7.907050]\n",
      "epoch:32 step:30881 [D loss: 1.032147, acc.: 50.00%] [G loss: 3.175512]\n",
      "epoch:32 step:30882 [D loss: 0.046223, acc.: 100.00%] [G loss: 8.820182]\n",
      "epoch:32 step:30883 [D loss: 0.564110, acc.: 64.84%] [G loss: 0.800681]\n",
      "epoch:32 step:30884 [D loss: 0.480914, acc.: 72.66%] [G loss: 3.168955]\n",
      "epoch:32 step:30885 [D loss: 0.607034, acc.: 67.19%] [G loss: 1.957492]\n",
      "epoch:32 step:30886 [D loss: 0.501323, acc.: 71.88%] [G loss: 2.508183]\n",
      "epoch:32 step:30887 [D loss: 0.056278, acc.: 100.00%] [G loss: 3.484056]\n",
      "epoch:32 step:30888 [D loss: 0.117454, acc.: 96.88%] [G loss: 0.981196]\n",
      "epoch:32 step:30889 [D loss: 0.886924, acc.: 40.62%] [G loss: 0.468584]\n",
      "epoch:32 step:30890 [D loss: 0.296177, acc.: 90.62%] [G loss: 0.575624]\n",
      "epoch:32 step:30891 [D loss: 0.769857, acc.: 61.72%] [G loss: 3.269339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:30892 [D loss: 0.317358, acc.: 92.97%] [G loss: 7.008674]\n",
      "epoch:32 step:30893 [D loss: 0.095109, acc.: 98.44%] [G loss: 4.806131]\n",
      "epoch:32 step:30894 [D loss: 0.211683, acc.: 94.53%] [G loss: 4.229549]\n",
      "epoch:32 step:30895 [D loss: 0.198534, acc.: 93.75%] [G loss: 6.248675]\n",
      "epoch:32 step:30896 [D loss: 0.379044, acc.: 82.03%] [G loss: 3.355497]\n",
      "epoch:32 step:30897 [D loss: 0.186620, acc.: 94.53%] [G loss: 2.032796]\n",
      "epoch:32 step:30898 [D loss: 0.269821, acc.: 93.75%] [G loss: 0.201578]\n",
      "epoch:32 step:30899 [D loss: 0.198972, acc.: 96.88%] [G loss: 5.033530]\n",
      "epoch:32 step:30900 [D loss: 0.076459, acc.: 99.22%] [G loss: 4.654626]\n",
      "epoch:32 step:30901 [D loss: 0.111182, acc.: 96.88%] [G loss: 3.159848]\n",
      "epoch:32 step:30902 [D loss: 0.197306, acc.: 94.53%] [G loss: 3.552802]\n",
      "epoch:32 step:30903 [D loss: 0.395475, acc.: 75.78%] [G loss: 1.473229]\n",
      "epoch:32 step:30904 [D loss: 0.046589, acc.: 100.00%] [G loss: 2.816152]\n",
      "epoch:32 step:30905 [D loss: 1.193975, acc.: 53.12%] [G loss: 3.794301]\n",
      "epoch:32 step:30906 [D loss: 0.173362, acc.: 95.31%] [G loss: 2.065096]\n",
      "epoch:32 step:30907 [D loss: 0.474515, acc.: 75.00%] [G loss: 4.194318]\n",
      "epoch:32 step:30908 [D loss: 0.101033, acc.: 96.88%] [G loss: 2.382883]\n",
      "epoch:32 step:30909 [D loss: 0.158272, acc.: 96.09%] [G loss: 2.414964]\n",
      "epoch:32 step:30910 [D loss: 0.234656, acc.: 90.62%] [G loss: 0.360356]\n",
      "epoch:32 step:30911 [D loss: 0.047076, acc.: 100.00%] [G loss: 1.546933]\n",
      "epoch:32 step:30912 [D loss: 0.562656, acc.: 71.88%] [G loss: 1.172042]\n",
      "epoch:32 step:30913 [D loss: 0.183293, acc.: 96.09%] [G loss: 1.484021]\n",
      "epoch:32 step:30914 [D loss: 0.376529, acc.: 82.81%] [G loss: 6.464956]\n",
      "epoch:32 step:30915 [D loss: 0.106474, acc.: 99.22%] [G loss: 1.454864]\n",
      "epoch:32 step:30916 [D loss: 0.229590, acc.: 96.09%] [G loss: 3.679858]\n",
      "epoch:32 step:30917 [D loss: 0.173794, acc.: 95.31%] [G loss: 4.765815]\n",
      "epoch:32 step:30918 [D loss: 0.365965, acc.: 81.25%] [G loss: 2.896899]\n",
      "epoch:32 step:30919 [D loss: 0.238212, acc.: 90.62%] [G loss: 3.260366]\n",
      "epoch:32 step:30920 [D loss: 0.168346, acc.: 95.31%] [G loss: 2.450923]\n",
      "epoch:32 step:30921 [D loss: 0.231994, acc.: 90.62%] [G loss: 5.868473]\n",
      "epoch:33 step:30922 [D loss: 0.296616, acc.: 85.94%] [G loss: 1.843736]\n",
      "epoch:33 step:30923 [D loss: 0.540899, acc.: 65.62%] [G loss: 2.288555]\n",
      "epoch:33 step:30924 [D loss: 0.164888, acc.: 96.09%] [G loss: 4.875490]\n",
      "epoch:33 step:30925 [D loss: 0.082827, acc.: 96.88%] [G loss: 0.637264]\n",
      "epoch:33 step:30926 [D loss: 0.363294, acc.: 85.94%] [G loss: 4.137763]\n",
      "epoch:33 step:30927 [D loss: 0.703065, acc.: 62.50%] [G loss: 2.077364]\n",
      "epoch:33 step:30928 [D loss: 0.345481, acc.: 85.94%] [G loss: 0.555257]\n",
      "epoch:33 step:30929 [D loss: 0.224039, acc.: 91.41%] [G loss: 0.332662]\n",
      "epoch:33 step:30930 [D loss: 0.153714, acc.: 94.53%] [G loss: 5.169779]\n",
      "epoch:33 step:30931 [D loss: 0.058296, acc.: 99.22%] [G loss: 2.467746]\n",
      "epoch:33 step:30932 [D loss: 0.433897, acc.: 78.91%] [G loss: 0.366169]\n",
      "epoch:33 step:30933 [D loss: 1.470433, acc.: 50.00%] [G loss: 0.597073]\n",
      "epoch:33 step:30934 [D loss: 0.064237, acc.: 99.22%] [G loss: 5.106240]\n",
      "epoch:33 step:30935 [D loss: 0.142202, acc.: 98.44%] [G loss: 8.112727]\n",
      "epoch:33 step:30936 [D loss: 0.052625, acc.: 99.22%] [G loss: 6.342765]\n",
      "epoch:33 step:30937 [D loss: 0.526012, acc.: 74.22%] [G loss: 5.647391]\n",
      "epoch:33 step:30938 [D loss: 0.159978, acc.: 97.66%] [G loss: 4.484744]\n",
      "epoch:33 step:30939 [D loss: 0.114778, acc.: 96.88%] [G loss: 3.303669]\n",
      "epoch:33 step:30940 [D loss: 0.133996, acc.: 97.66%] [G loss: 3.434868]\n",
      "epoch:33 step:30941 [D loss: 0.044526, acc.: 100.00%] [G loss: 4.363820]\n",
      "epoch:33 step:30942 [D loss: 0.061916, acc.: 99.22%] [G loss: 3.692723]\n",
      "epoch:33 step:30943 [D loss: 0.183506, acc.: 96.88%] [G loss: 3.483202]\n",
      "epoch:33 step:30944 [D loss: 0.194323, acc.: 92.97%] [G loss: 3.195463]\n",
      "epoch:33 step:30945 [D loss: 0.120740, acc.: 96.88%] [G loss: 2.893419]\n",
      "epoch:33 step:30946 [D loss: 0.111284, acc.: 97.66%] [G loss: 1.374849]\n",
      "epoch:33 step:30947 [D loss: 0.118282, acc.: 99.22%] [G loss: 2.337389]\n",
      "epoch:33 step:30948 [D loss: 0.054908, acc.: 98.44%] [G loss: 2.835385]\n",
      "epoch:33 step:30949 [D loss: 1.826637, acc.: 19.53%] [G loss: 3.936285]\n",
      "epoch:33 step:30950 [D loss: 0.045036, acc.: 100.00%] [G loss: 4.574120]\n",
      "epoch:33 step:30951 [D loss: 0.120881, acc.: 98.44%] [G loss: 3.624889]\n",
      "epoch:33 step:30952 [D loss: 0.581386, acc.: 70.31%] [G loss: 3.911310]\n",
      "epoch:33 step:30953 [D loss: 0.230771, acc.: 92.19%] [G loss: 2.752015]\n",
      "epoch:33 step:30954 [D loss: 0.241272, acc.: 93.75%] [G loss: 2.992865]\n",
      "epoch:33 step:30955 [D loss: 0.028414, acc.: 100.00%] [G loss: 2.837080]\n",
      "epoch:33 step:30956 [D loss: 0.106811, acc.: 100.00%] [G loss: 3.035132]\n",
      "epoch:33 step:30957 [D loss: 0.119753, acc.: 97.66%] [G loss: 4.638527]\n",
      "epoch:33 step:30958 [D loss: 0.251961, acc.: 89.84%] [G loss: 8.296635]\n",
      "epoch:33 step:30959 [D loss: 0.250189, acc.: 93.75%] [G loss: 0.674989]\n",
      "epoch:33 step:30960 [D loss: 0.588764, acc.: 67.19%] [G loss: 2.855176]\n",
      "epoch:33 step:30961 [D loss: 0.107135, acc.: 99.22%] [G loss: 3.508196]\n",
      "epoch:33 step:30962 [D loss: 0.345799, acc.: 82.03%] [G loss: 2.347776]\n",
      "epoch:33 step:30963 [D loss: 0.174327, acc.: 96.09%] [G loss: 1.894743]\n",
      "epoch:33 step:30964 [D loss: 0.084513, acc.: 98.44%] [G loss: 3.594325]\n",
      "epoch:33 step:30965 [D loss: 0.275224, acc.: 89.06%] [G loss: 3.562343]\n",
      "epoch:33 step:30966 [D loss: 0.143023, acc.: 98.44%] [G loss: 0.595926]\n",
      "epoch:33 step:30967 [D loss: 0.203818, acc.: 93.75%] [G loss: 1.462606]\n",
      "epoch:33 step:30968 [D loss: 1.226417, acc.: 39.06%] [G loss: 3.160688]\n",
      "epoch:33 step:30969 [D loss: 0.104679, acc.: 98.44%] [G loss: 3.526744]\n",
      "epoch:33 step:30970 [D loss: 0.055490, acc.: 100.00%] [G loss: 1.127557]\n",
      "epoch:33 step:30971 [D loss: 0.961324, acc.: 39.84%] [G loss: 0.847022]\n",
      "epoch:33 step:30972 [D loss: 0.100199, acc.: 98.44%] [G loss: 2.330354]\n",
      "epoch:33 step:30973 [D loss: 0.047029, acc.: 98.44%] [G loss: 2.917189]\n",
      "epoch:33 step:30974 [D loss: 1.706933, acc.: 32.81%] [G loss: 2.367985]\n",
      "epoch:33 step:30975 [D loss: 0.169081, acc.: 97.66%] [G loss: 1.481771]\n",
      "epoch:33 step:30976 [D loss: 0.183213, acc.: 96.88%] [G loss: 2.049158]\n",
      "epoch:33 step:30977 [D loss: 0.187952, acc.: 95.31%] [G loss: 3.634703]\n",
      "epoch:33 step:30978 [D loss: 0.054099, acc.: 100.00%] [G loss: 5.020341]\n",
      "epoch:33 step:30979 [D loss: 0.401860, acc.: 84.38%] [G loss: 1.577750]\n",
      "epoch:33 step:30980 [D loss: 0.046490, acc.: 100.00%] [G loss: 3.750398]\n",
      "epoch:33 step:30981 [D loss: 0.753961, acc.: 63.28%] [G loss: 5.794358]\n",
      "epoch:33 step:30982 [D loss: 0.629586, acc.: 67.19%] [G loss: 2.969666]\n",
      "epoch:33 step:30983 [D loss: 0.160494, acc.: 96.09%] [G loss: 2.750903]\n",
      "epoch:33 step:30984 [D loss: 0.281632, acc.: 87.50%] [G loss: 5.434064]\n",
      "epoch:33 step:30985 [D loss: 0.316754, acc.: 88.28%] [G loss: 4.053535]\n",
      "epoch:33 step:30986 [D loss: 0.881683, acc.: 57.81%] [G loss: 2.379517]\n",
      "epoch:33 step:30987 [D loss: 0.770203, acc.: 63.28%] [G loss: 2.179628]\n",
      "epoch:33 step:30988 [D loss: 0.039596, acc.: 100.00%] [G loss: 1.575248]\n",
      "epoch:33 step:30989 [D loss: 0.124146, acc.: 98.44%] [G loss: 3.055523]\n",
      "epoch:33 step:30990 [D loss: 0.181505, acc.: 96.09%] [G loss: 1.875882]\n",
      "epoch:33 step:30991 [D loss: 0.131361, acc.: 97.66%] [G loss: 0.690933]\n",
      "epoch:33 step:30992 [D loss: 0.447272, acc.: 77.34%] [G loss: 2.262008]\n",
      "epoch:33 step:30993 [D loss: 0.814538, acc.: 50.78%] [G loss: 3.799181]\n",
      "epoch:33 step:30994 [D loss: 0.026398, acc.: 100.00%] [G loss: 3.700585]\n",
      "epoch:33 step:30995 [D loss: 0.144012, acc.: 96.88%] [G loss: 2.095495]\n",
      "epoch:33 step:30996 [D loss: 1.041828, acc.: 34.38%] [G loss: 2.965103]\n",
      "epoch:33 step:30997 [D loss: 0.334768, acc.: 86.72%] [G loss: 3.097594]\n",
      "epoch:33 step:30998 [D loss: 0.364211, acc.: 76.56%] [G loss: 4.875762]\n",
      "epoch:33 step:30999 [D loss: 0.066055, acc.: 99.22%] [G loss: 1.155765]\n",
      "epoch:33 step:31000 [D loss: 0.487639, acc.: 75.78%] [G loss: 1.843186]\n",
      "epoch:33 step:31001 [D loss: 0.165372, acc.: 98.44%] [G loss: 5.093631]\n",
      "epoch:33 step:31002 [D loss: 0.008492, acc.: 100.00%] [G loss: 3.786923]\n",
      "epoch:33 step:31003 [D loss: 0.483667, acc.: 74.22%] [G loss: 2.474864]\n",
      "epoch:33 step:31004 [D loss: 0.139075, acc.: 97.66%] [G loss: 2.972613]\n",
      "epoch:33 step:31005 [D loss: 0.167740, acc.: 96.09%] [G loss: 3.415627]\n",
      "epoch:33 step:31006 [D loss: 0.197040, acc.: 96.88%] [G loss: 3.244393]\n",
      "epoch:33 step:31007 [D loss: 0.030335, acc.: 100.00%] [G loss: 3.446417]\n",
      "epoch:33 step:31008 [D loss: 0.181580, acc.: 95.31%] [G loss: 2.638002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31009 [D loss: 0.718922, acc.: 60.16%] [G loss: 3.541018]\n",
      "epoch:33 step:31010 [D loss: 0.281480, acc.: 82.03%] [G loss: 2.098863]\n",
      "epoch:33 step:31011 [D loss: 0.087957, acc.: 98.44%] [G loss: 3.972417]\n",
      "epoch:33 step:31012 [D loss: 0.055385, acc.: 100.00%] [G loss: 5.300926]\n",
      "epoch:33 step:31013 [D loss: 0.175445, acc.: 92.97%] [G loss: 3.919425]\n",
      "epoch:33 step:31014 [D loss: 0.202166, acc.: 96.88%] [G loss: 5.510478]\n",
      "epoch:33 step:31015 [D loss: 0.031654, acc.: 100.00%] [G loss: 2.577096]\n",
      "epoch:33 step:31016 [D loss: 0.528265, acc.: 68.75%] [G loss: 4.972088]\n",
      "epoch:33 step:31017 [D loss: 0.028783, acc.: 100.00%] [G loss: 2.676501]\n",
      "epoch:33 step:31018 [D loss: 0.041958, acc.: 99.22%] [G loss: 5.569287]\n",
      "epoch:33 step:31019 [D loss: 0.366320, acc.: 89.84%] [G loss: 1.504768]\n",
      "epoch:33 step:31020 [D loss: 0.500728, acc.: 76.56%] [G loss: 7.738045]\n",
      "epoch:33 step:31021 [D loss: 0.019284, acc.: 100.00%] [G loss: 4.722929]\n",
      "epoch:33 step:31022 [D loss: 0.318150, acc.: 82.03%] [G loss: 2.137368]\n",
      "epoch:33 step:31023 [D loss: 0.114142, acc.: 98.44%] [G loss: 2.258945]\n",
      "epoch:33 step:31024 [D loss: 0.096441, acc.: 96.88%] [G loss: 3.581656]\n",
      "epoch:33 step:31025 [D loss: 0.100069, acc.: 98.44%] [G loss: 1.653985]\n",
      "epoch:33 step:31026 [D loss: 0.418476, acc.: 75.00%] [G loss: 6.251992]\n",
      "epoch:33 step:31027 [D loss: 0.259186, acc.: 92.97%] [G loss: 2.995647]\n",
      "epoch:33 step:31028 [D loss: 1.165367, acc.: 52.34%] [G loss: 3.345790]\n",
      "epoch:33 step:31029 [D loss: 0.282490, acc.: 91.41%] [G loss: 1.743799]\n",
      "epoch:33 step:31030 [D loss: 0.497709, acc.: 74.22%] [G loss: 0.758345]\n",
      "epoch:33 step:31031 [D loss: 0.475691, acc.: 75.78%] [G loss: 3.415171]\n",
      "epoch:33 step:31032 [D loss: 0.516855, acc.: 75.78%] [G loss: 1.615620]\n",
      "epoch:33 step:31033 [D loss: 0.147429, acc.: 96.09%] [G loss: 2.538718]\n",
      "epoch:33 step:31034 [D loss: 0.205918, acc.: 96.88%] [G loss: 4.405508]\n",
      "epoch:33 step:31035 [D loss: 0.387958, acc.: 80.47%] [G loss: 2.495708]\n",
      "epoch:33 step:31036 [D loss: 0.150177, acc.: 98.44%] [G loss: 2.811163]\n",
      "epoch:33 step:31037 [D loss: 0.828341, acc.: 46.88%] [G loss: 1.921704]\n",
      "epoch:33 step:31038 [D loss: 1.278013, acc.: 51.56%] [G loss: 3.514315]\n",
      "epoch:33 step:31039 [D loss: 0.319212, acc.: 90.62%] [G loss: 5.243595]\n",
      "epoch:33 step:31040 [D loss: 0.194595, acc.: 92.19%] [G loss: 4.085627]\n",
      "epoch:33 step:31041 [D loss: 1.207464, acc.: 47.66%] [G loss: 1.138829]\n",
      "epoch:33 step:31042 [D loss: 0.756972, acc.: 61.72%] [G loss: 1.787360]\n",
      "epoch:33 step:31043 [D loss: 0.108446, acc.: 97.66%] [G loss: 0.739424]\n",
      "epoch:33 step:31044 [D loss: 0.322747, acc.: 82.81%] [G loss: 4.371173]\n",
      "epoch:33 step:31045 [D loss: 0.270774, acc.: 89.84%] [G loss: 0.700049]\n",
      "epoch:33 step:31046 [D loss: 0.381577, acc.: 80.47%] [G loss: 3.702984]\n",
      "epoch:33 step:31047 [D loss: 0.091594, acc.: 98.44%] [G loss: 6.128782]\n",
      "epoch:33 step:31048 [D loss: 0.173239, acc.: 95.31%] [G loss: 4.413230]\n",
      "epoch:33 step:31049 [D loss: 0.045946, acc.: 100.00%] [G loss: 1.495368]\n",
      "epoch:33 step:31050 [D loss: 0.091013, acc.: 99.22%] [G loss: 2.544219]\n",
      "epoch:33 step:31051 [D loss: 0.081263, acc.: 98.44%] [G loss: 3.025525]\n",
      "epoch:33 step:31052 [D loss: 0.518349, acc.: 64.06%] [G loss: 1.792530]\n",
      "epoch:33 step:31053 [D loss: 0.867376, acc.: 45.31%] [G loss: 0.679431]\n",
      "epoch:33 step:31054 [D loss: 0.081109, acc.: 99.22%] [G loss: 5.359115]\n",
      "epoch:33 step:31055 [D loss: 0.272193, acc.: 92.19%] [G loss: 4.390271]\n",
      "epoch:33 step:31056 [D loss: 0.330145, acc.: 90.62%] [G loss: 4.223202]\n",
      "epoch:33 step:31057 [D loss: 0.708877, acc.: 62.50%] [G loss: 3.083865]\n",
      "epoch:33 step:31058 [D loss: 0.161898, acc.: 96.88%] [G loss: 9.601143]\n",
      "epoch:33 step:31059 [D loss: 0.299381, acc.: 86.72%] [G loss: 2.226553]\n",
      "epoch:33 step:31060 [D loss: 0.983781, acc.: 52.34%] [G loss: 3.557014]\n",
      "epoch:33 step:31061 [D loss: 0.155661, acc.: 96.09%] [G loss: 9.073644]\n",
      "epoch:33 step:31062 [D loss: 0.153529, acc.: 98.44%] [G loss: 6.264111]\n",
      "epoch:33 step:31063 [D loss: 0.339474, acc.: 83.59%] [G loss: 4.519496]\n",
      "epoch:33 step:31064 [D loss: 0.517547, acc.: 72.66%] [G loss: 2.060927]\n",
      "epoch:33 step:31065 [D loss: 0.282711, acc.: 93.75%] [G loss: 1.531890]\n",
      "epoch:33 step:31066 [D loss: 0.476416, acc.: 75.00%] [G loss: 1.109387]\n",
      "epoch:33 step:31067 [D loss: 0.298578, acc.: 92.97%] [G loss: 3.029450]\n",
      "epoch:33 step:31068 [D loss: 0.083642, acc.: 97.66%] [G loss: 3.559917]\n",
      "epoch:33 step:31069 [D loss: 0.244574, acc.: 89.06%] [G loss: 5.272893]\n",
      "epoch:33 step:31070 [D loss: 0.441098, acc.: 85.16%] [G loss: 2.359578]\n",
      "epoch:33 step:31071 [D loss: 0.119106, acc.: 97.66%] [G loss: 3.282768]\n",
      "epoch:33 step:31072 [D loss: 0.102537, acc.: 96.88%] [G loss: 2.708513]\n",
      "epoch:33 step:31073 [D loss: 0.069610, acc.: 98.44%] [G loss: 1.987263]\n",
      "epoch:33 step:31074 [D loss: 0.441072, acc.: 77.34%] [G loss: 2.313187]\n",
      "epoch:33 step:31075 [D loss: 0.195625, acc.: 98.44%] [G loss: 2.263218]\n",
      "epoch:33 step:31076 [D loss: 0.590146, acc.: 69.53%] [G loss: 1.905417]\n",
      "epoch:33 step:31077 [D loss: 0.288915, acc.: 86.72%] [G loss: 4.229372]\n",
      "epoch:33 step:31078 [D loss: 0.319874, acc.: 80.47%] [G loss: 5.186332]\n",
      "epoch:33 step:31079 [D loss: 0.315172, acc.: 89.84%] [G loss: 3.585353]\n",
      "epoch:33 step:31080 [D loss: 0.046448, acc.: 99.22%] [G loss: 3.393015]\n",
      "epoch:33 step:31081 [D loss: 0.131276, acc.: 96.88%] [G loss: 2.257414]\n",
      "epoch:33 step:31082 [D loss: 0.414041, acc.: 79.69%] [G loss: 4.086749]\n",
      "epoch:33 step:31083 [D loss: 0.122733, acc.: 99.22%] [G loss: 1.073307]\n",
      "epoch:33 step:31084 [D loss: 0.106853, acc.: 96.88%] [G loss: 0.585261]\n",
      "epoch:33 step:31085 [D loss: 0.237690, acc.: 91.41%] [G loss: 2.493406]\n",
      "epoch:33 step:31086 [D loss: 0.392764, acc.: 75.00%] [G loss: 4.066388]\n",
      "epoch:33 step:31087 [D loss: 0.208792, acc.: 92.97%] [G loss: 4.254150]\n",
      "epoch:33 step:31088 [D loss: 0.025934, acc.: 100.00%] [G loss: 2.020174]\n",
      "epoch:33 step:31089 [D loss: 0.068785, acc.: 99.22%] [G loss: 4.148155]\n",
      "epoch:33 step:31090 [D loss: 0.505831, acc.: 69.53%] [G loss: 1.034735]\n",
      "epoch:33 step:31091 [D loss: 1.048912, acc.: 54.69%] [G loss: 3.225387]\n",
      "epoch:33 step:31092 [D loss: 0.070864, acc.: 100.00%] [G loss: 5.088117]\n",
      "epoch:33 step:31093 [D loss: 0.123573, acc.: 97.66%] [G loss: 3.715462]\n",
      "epoch:33 step:31094 [D loss: 0.165532, acc.: 95.31%] [G loss: 3.224167]\n",
      "epoch:33 step:31095 [D loss: 0.222897, acc.: 96.09%] [G loss: 2.758229]\n",
      "epoch:33 step:31096 [D loss: 0.119920, acc.: 99.22%] [G loss: 1.333177]\n",
      "epoch:33 step:31097 [D loss: 0.270278, acc.: 85.16%] [G loss: 4.160151]\n",
      "epoch:33 step:31098 [D loss: 0.189228, acc.: 93.75%] [G loss: 2.628328]\n",
      "epoch:33 step:31099 [D loss: 0.203122, acc.: 97.66%] [G loss: 2.927059]\n",
      "epoch:33 step:31100 [D loss: 0.243467, acc.: 95.31%] [G loss: 2.103989]\n",
      "epoch:33 step:31101 [D loss: 0.239994, acc.: 88.28%] [G loss: 1.420617]\n",
      "epoch:33 step:31102 [D loss: 0.138371, acc.: 96.88%] [G loss: 1.639359]\n",
      "epoch:33 step:31103 [D loss: 0.237795, acc.: 89.06%] [G loss: 3.743949]\n",
      "epoch:33 step:31104 [D loss: 1.019225, acc.: 56.25%] [G loss: 4.075778]\n",
      "epoch:33 step:31105 [D loss: 0.550994, acc.: 71.09%] [G loss: 3.253814]\n",
      "epoch:33 step:31106 [D loss: 0.299030, acc.: 86.72%] [G loss: 3.722451]\n",
      "epoch:33 step:31107 [D loss: 0.073726, acc.: 98.44%] [G loss: 1.791985]\n",
      "epoch:33 step:31108 [D loss: 0.035527, acc.: 100.00%] [G loss: 3.952476]\n",
      "epoch:33 step:31109 [D loss: 0.349699, acc.: 82.03%] [G loss: 3.583848]\n",
      "epoch:33 step:31110 [D loss: 0.190385, acc.: 95.31%] [G loss: 6.325389]\n",
      "epoch:33 step:31111 [D loss: 0.159258, acc.: 96.09%] [G loss: 4.358691]\n",
      "epoch:33 step:31112 [D loss: 0.078154, acc.: 100.00%] [G loss: 5.984708]\n",
      "epoch:33 step:31113 [D loss: 0.103949, acc.: 98.44%] [G loss: 3.891720]\n",
      "epoch:33 step:31114 [D loss: 0.067052, acc.: 98.44%] [G loss: 1.477962]\n",
      "epoch:33 step:31115 [D loss: 0.668806, acc.: 63.28%] [G loss: 1.925229]\n",
      "epoch:33 step:31116 [D loss: 0.202329, acc.: 91.41%] [G loss: 3.087706]\n",
      "epoch:33 step:31117 [D loss: 0.620121, acc.: 64.06%] [G loss: 5.085906]\n",
      "epoch:33 step:31118 [D loss: 0.290977, acc.: 85.16%] [G loss: 5.300900]\n",
      "epoch:33 step:31119 [D loss: 0.042217, acc.: 100.00%] [G loss: 5.789869]\n",
      "epoch:33 step:31120 [D loss: 0.235810, acc.: 92.97%] [G loss: 4.634568]\n",
      "epoch:33 step:31121 [D loss: 0.213657, acc.: 89.84%] [G loss: 2.086474]\n",
      "epoch:33 step:31122 [D loss: 0.167051, acc.: 96.88%] [G loss: 5.175459]\n",
      "epoch:33 step:31123 [D loss: 0.113523, acc.: 96.88%] [G loss: 2.183024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31124 [D loss: 0.317571, acc.: 81.25%] [G loss: 2.789737]\n",
      "epoch:33 step:31125 [D loss: 0.814991, acc.: 44.53%] [G loss: 1.250213]\n",
      "epoch:33 step:31126 [D loss: 0.130587, acc.: 97.66%] [G loss: 4.432323]\n",
      "epoch:33 step:31127 [D loss: 0.052019, acc.: 99.22%] [G loss: 3.909781]\n",
      "epoch:33 step:31128 [D loss: 0.361289, acc.: 80.47%] [G loss: 2.851025]\n",
      "epoch:33 step:31129 [D loss: 0.176098, acc.: 96.88%] [G loss: 2.796657]\n",
      "epoch:33 step:31130 [D loss: 1.965344, acc.: 42.97%] [G loss: 1.920381]\n",
      "epoch:33 step:31131 [D loss: 0.379104, acc.: 84.38%] [G loss: 2.548598]\n",
      "epoch:33 step:31132 [D loss: 0.652590, acc.: 64.84%] [G loss: 1.938291]\n",
      "epoch:33 step:31133 [D loss: 0.702293, acc.: 58.59%] [G loss: 2.424102]\n",
      "epoch:33 step:31134 [D loss: 0.148366, acc.: 96.09%] [G loss: 4.279989]\n",
      "epoch:33 step:31135 [D loss: 0.357687, acc.: 76.56%] [G loss: 3.428322]\n",
      "epoch:33 step:31136 [D loss: 0.101119, acc.: 98.44%] [G loss: 3.541235]\n",
      "epoch:33 step:31137 [D loss: 1.281646, acc.: 50.00%] [G loss: 4.532818]\n",
      "epoch:33 step:31138 [D loss: 0.062819, acc.: 97.66%] [G loss: 6.506239]\n",
      "epoch:33 step:31139 [D loss: 0.355590, acc.: 83.59%] [G loss: 4.576583]\n",
      "epoch:33 step:31140 [D loss: 0.106775, acc.: 97.66%] [G loss: 4.236997]\n",
      "epoch:33 step:31141 [D loss: 0.649931, acc.: 67.97%] [G loss: 1.809452]\n",
      "epoch:33 step:31142 [D loss: 0.247919, acc.: 89.06%] [G loss: 2.164285]\n",
      "epoch:33 step:31143 [D loss: 0.148919, acc.: 96.88%] [G loss: 3.249691]\n",
      "epoch:33 step:31144 [D loss: 0.102710, acc.: 96.88%] [G loss: 3.900918]\n",
      "epoch:33 step:31145 [D loss: 0.075993, acc.: 99.22%] [G loss: 2.836357]\n",
      "epoch:33 step:31146 [D loss: 0.113023, acc.: 98.44%] [G loss: 3.634324]\n",
      "epoch:33 step:31147 [D loss: 0.088410, acc.: 99.22%] [G loss: 2.060120]\n",
      "epoch:33 step:31148 [D loss: 0.496064, acc.: 70.31%] [G loss: 4.117422]\n",
      "epoch:33 step:31149 [D loss: 0.416557, acc.: 82.03%] [G loss: 5.714637]\n",
      "epoch:33 step:31150 [D loss: 0.285567, acc.: 87.50%] [G loss: 3.699903]\n",
      "epoch:33 step:31151 [D loss: 0.420167, acc.: 75.78%] [G loss: 3.662725]\n",
      "epoch:33 step:31152 [D loss: 0.052870, acc.: 98.44%] [G loss: 3.244021]\n",
      "epoch:33 step:31153 [D loss: 0.331158, acc.: 89.84%] [G loss: 3.382555]\n",
      "epoch:33 step:31154 [D loss: 0.136562, acc.: 96.88%] [G loss: 3.306636]\n",
      "epoch:33 step:31155 [D loss: 0.091015, acc.: 98.44%] [G loss: 0.601132]\n",
      "epoch:33 step:31156 [D loss: 0.115632, acc.: 97.66%] [G loss: 5.771032]\n",
      "epoch:33 step:31157 [D loss: 0.035138, acc.: 99.22%] [G loss: 4.018818]\n",
      "epoch:33 step:31158 [D loss: 1.162072, acc.: 47.66%] [G loss: 2.494552]\n",
      "epoch:33 step:31159 [D loss: 0.181758, acc.: 96.09%] [G loss: 3.435406]\n",
      "epoch:33 step:31160 [D loss: 0.138063, acc.: 99.22%] [G loss: 5.696499]\n",
      "epoch:33 step:31161 [D loss: 0.355530, acc.: 80.47%] [G loss: 2.596489]\n",
      "epoch:33 step:31162 [D loss: 0.370279, acc.: 86.72%] [G loss: 3.754207]\n",
      "epoch:33 step:31163 [D loss: 0.103009, acc.: 97.66%] [G loss: 0.953077]\n",
      "epoch:33 step:31164 [D loss: 0.150240, acc.: 98.44%] [G loss: 4.535636]\n",
      "epoch:33 step:31165 [D loss: 0.113896, acc.: 97.66%] [G loss: 2.086981]\n",
      "epoch:33 step:31166 [D loss: 0.398079, acc.: 72.66%] [G loss: 3.329247]\n",
      "epoch:33 step:31167 [D loss: 0.045736, acc.: 99.22%] [G loss: 0.880530]\n",
      "epoch:33 step:31168 [D loss: 0.580452, acc.: 67.19%] [G loss: 5.966599]\n",
      "epoch:33 step:31169 [D loss: 0.041047, acc.: 100.00%] [G loss: 2.693588]\n",
      "epoch:33 step:31170 [D loss: 0.877895, acc.: 47.66%] [G loss: 3.301549]\n",
      "epoch:33 step:31171 [D loss: 0.044961, acc.: 99.22%] [G loss: 2.110136]\n",
      "epoch:33 step:31172 [D loss: 0.059083, acc.: 99.22%] [G loss: 2.366797]\n",
      "epoch:33 step:31173 [D loss: 0.281735, acc.: 88.28%] [G loss: 0.928321]\n",
      "epoch:33 step:31174 [D loss: 0.103075, acc.: 98.44%] [G loss: 3.232396]\n",
      "epoch:33 step:31175 [D loss: 0.103237, acc.: 99.22%] [G loss: 2.452517]\n",
      "epoch:33 step:31176 [D loss: 0.464489, acc.: 81.25%] [G loss: 0.924460]\n",
      "epoch:33 step:31177 [D loss: 0.091808, acc.: 98.44%] [G loss: 0.559205]\n",
      "epoch:33 step:31178 [D loss: 0.155220, acc.: 96.88%] [G loss: 3.193358]\n",
      "epoch:33 step:31179 [D loss: 0.339968, acc.: 86.72%] [G loss: 3.034558]\n",
      "epoch:33 step:31180 [D loss: 0.342685, acc.: 83.59%] [G loss: 1.490974]\n",
      "epoch:33 step:31181 [D loss: 0.449621, acc.: 79.69%] [G loss: 1.983345]\n",
      "epoch:33 step:31182 [D loss: 0.017472, acc.: 100.00%] [G loss: 3.403789]\n",
      "epoch:33 step:31183 [D loss: 0.140284, acc.: 96.88%] [G loss: 0.792980]\n",
      "epoch:33 step:31184 [D loss: 0.111509, acc.: 97.66%] [G loss: 0.837634]\n",
      "epoch:33 step:31185 [D loss: 0.342890, acc.: 87.50%] [G loss: 1.679043]\n",
      "epoch:33 step:31186 [D loss: 0.150640, acc.: 96.09%] [G loss: 6.404083]\n",
      "epoch:33 step:31187 [D loss: 0.521507, acc.: 70.31%] [G loss: 2.900835]\n",
      "epoch:33 step:31188 [D loss: 0.176161, acc.: 92.19%] [G loss: 5.494662]\n",
      "epoch:33 step:31189 [D loss: 0.950518, acc.: 60.94%] [G loss: 2.390523]\n",
      "epoch:33 step:31190 [D loss: 0.024031, acc.: 100.00%] [G loss: 3.902308]\n",
      "epoch:33 step:31191 [D loss: 0.027178, acc.: 100.00%] [G loss: 2.244356]\n",
      "epoch:33 step:31192 [D loss: 0.041622, acc.: 99.22%] [G loss: 3.140071]\n",
      "epoch:33 step:31193 [D loss: 0.083584, acc.: 100.00%] [G loss: 1.624484]\n",
      "epoch:33 step:31194 [D loss: 0.613860, acc.: 62.50%] [G loss: 2.152717]\n",
      "epoch:33 step:31195 [D loss: 0.026563, acc.: 99.22%] [G loss: 2.713047]\n",
      "epoch:33 step:31196 [D loss: 0.075391, acc.: 97.66%] [G loss: 4.371184]\n",
      "epoch:33 step:31197 [D loss: 0.264455, acc.: 92.19%] [G loss: 3.916664]\n",
      "epoch:33 step:31198 [D loss: 0.104990, acc.: 98.44%] [G loss: 3.021149]\n",
      "epoch:33 step:31199 [D loss: 0.123351, acc.: 99.22%] [G loss: 2.261662]\n",
      "epoch:33 step:31200 [D loss: 0.031350, acc.: 99.22%] [G loss: 0.747575]\n",
      "epoch:33 step:31201 [D loss: 0.038748, acc.: 99.22%] [G loss: 3.701508]\n",
      "epoch:33 step:31202 [D loss: 0.262425, acc.: 87.50%] [G loss: 5.357556]\n",
      "epoch:33 step:31203 [D loss: 0.132444, acc.: 96.88%] [G loss: 6.544231]\n",
      "epoch:33 step:31204 [D loss: 0.727075, acc.: 64.84%] [G loss: 2.928203]\n",
      "epoch:33 step:31205 [D loss: 0.382653, acc.: 87.50%] [G loss: 1.563198]\n",
      "epoch:33 step:31206 [D loss: 1.203189, acc.: 43.75%] [G loss: 2.734924]\n",
      "epoch:33 step:31207 [D loss: 0.726815, acc.: 56.25%] [G loss: 4.034072]\n",
      "epoch:33 step:31208 [D loss: 0.298186, acc.: 88.28%] [G loss: 3.058774]\n",
      "epoch:33 step:31209 [D loss: 0.228955, acc.: 92.97%] [G loss: 4.040606]\n",
      "epoch:33 step:31210 [D loss: 0.543341, acc.: 67.97%] [G loss: 2.969479]\n",
      "epoch:33 step:31211 [D loss: 0.274723, acc.: 85.16%] [G loss: 4.354797]\n",
      "epoch:33 step:31212 [D loss: 0.166262, acc.: 96.88%] [G loss: 1.249040]\n",
      "epoch:33 step:31213 [D loss: 0.061035, acc.: 100.00%] [G loss: 1.662809]\n",
      "epoch:33 step:31214 [D loss: 0.399206, acc.: 78.12%] [G loss: 4.065122]\n",
      "epoch:33 step:31215 [D loss: 0.296770, acc.: 88.28%] [G loss: 2.144277]\n",
      "epoch:33 step:31216 [D loss: 0.326471, acc.: 85.16%] [G loss: 1.093297]\n",
      "epoch:33 step:31217 [D loss: 0.123819, acc.: 96.09%] [G loss: 0.525780]\n",
      "epoch:33 step:31218 [D loss: 0.328274, acc.: 78.91%] [G loss: 2.339088]\n",
      "epoch:33 step:31219 [D loss: 0.086076, acc.: 99.22%] [G loss: 1.157602]\n",
      "epoch:33 step:31220 [D loss: 0.215912, acc.: 93.75%] [G loss: 0.390049]\n",
      "epoch:33 step:31221 [D loss: 0.343460, acc.: 85.16%] [G loss: 3.275673]\n",
      "epoch:33 step:31222 [D loss: 1.247026, acc.: 52.34%] [G loss: 1.664796]\n",
      "epoch:33 step:31223 [D loss: 0.598603, acc.: 71.09%] [G loss: 4.218488]\n",
      "epoch:33 step:31224 [D loss: 0.273859, acc.: 85.16%] [G loss: 3.140948]\n",
      "epoch:33 step:31225 [D loss: 0.041454, acc.: 99.22%] [G loss: 5.662712]\n",
      "epoch:33 step:31226 [D loss: 0.107063, acc.: 99.22%] [G loss: 1.237035]\n",
      "epoch:33 step:31227 [D loss: 0.761142, acc.: 58.59%] [G loss: 2.298195]\n",
      "epoch:33 step:31228 [D loss: 0.084408, acc.: 99.22%] [G loss: 2.556401]\n",
      "epoch:33 step:31229 [D loss: 0.737227, acc.: 60.16%] [G loss: 0.697800]\n",
      "epoch:33 step:31230 [D loss: 0.028400, acc.: 100.00%] [G loss: 1.659626]\n",
      "epoch:33 step:31231 [D loss: 0.087726, acc.: 97.66%] [G loss: 1.901325]\n",
      "epoch:33 step:31232 [D loss: 0.475872, acc.: 78.91%] [G loss: 1.358871]\n",
      "epoch:33 step:31233 [D loss: 0.172071, acc.: 97.66%] [G loss: 5.380329]\n",
      "epoch:33 step:31234 [D loss: 0.199461, acc.: 96.09%] [G loss: 1.540133]\n",
      "epoch:33 step:31235 [D loss: 0.593856, acc.: 67.19%] [G loss: 5.945023]\n",
      "epoch:33 step:31236 [D loss: 0.279605, acc.: 89.06%] [G loss: 3.937400]\n",
      "epoch:33 step:31237 [D loss: 0.237215, acc.: 88.28%] [G loss: 5.248036]\n",
      "epoch:33 step:31238 [D loss: 0.217547, acc.: 90.62%] [G loss: 3.077477]\n",
      "epoch:33 step:31239 [D loss: 0.318294, acc.: 82.81%] [G loss: 3.781417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31240 [D loss: 0.269848, acc.: 91.41%] [G loss: 1.695239]\n",
      "epoch:33 step:31241 [D loss: 0.886794, acc.: 57.81%] [G loss: 5.130570]\n",
      "epoch:33 step:31242 [D loss: 0.328385, acc.: 88.28%] [G loss: 3.352786]\n",
      "epoch:33 step:31243 [D loss: 0.221361, acc.: 92.97%] [G loss: 6.482398]\n",
      "epoch:33 step:31244 [D loss: 0.088608, acc.: 99.22%] [G loss: 3.479265]\n",
      "epoch:33 step:31245 [D loss: 0.083109, acc.: 99.22%] [G loss: 3.415667]\n",
      "epoch:33 step:31246 [D loss: 0.109193, acc.: 98.44%] [G loss: 2.044128]\n",
      "epoch:33 step:31247 [D loss: 0.226602, acc.: 96.09%] [G loss: 5.654333]\n",
      "epoch:33 step:31248 [D loss: 0.097075, acc.: 99.22%] [G loss: 2.483194]\n",
      "epoch:33 step:31249 [D loss: 0.154579, acc.: 96.88%] [G loss: 1.955945]\n",
      "epoch:33 step:31250 [D loss: 1.106752, acc.: 32.03%] [G loss: 3.026165]\n",
      "epoch:33 step:31251 [D loss: 0.107140, acc.: 99.22%] [G loss: 1.579633]\n",
      "epoch:33 step:31252 [D loss: 0.054358, acc.: 100.00%] [G loss: 0.418415]\n",
      "epoch:33 step:31253 [D loss: 0.058851, acc.: 99.22%] [G loss: 0.773665]\n",
      "epoch:33 step:31254 [D loss: 0.608559, acc.: 69.53%] [G loss: 3.184084]\n",
      "epoch:33 step:31255 [D loss: 0.386001, acc.: 85.16%] [G loss: 3.542458]\n",
      "epoch:33 step:31256 [D loss: 0.443816, acc.: 76.56%] [G loss: 3.592643]\n",
      "epoch:33 step:31257 [D loss: 0.069516, acc.: 98.44%] [G loss: 2.293156]\n",
      "epoch:33 step:31258 [D loss: 0.161860, acc.: 98.44%] [G loss: 0.934721]\n",
      "epoch:33 step:31259 [D loss: 0.046671, acc.: 98.44%] [G loss: 4.846151]\n",
      "epoch:33 step:31260 [D loss: 0.710830, acc.: 63.28%] [G loss: 0.825536]\n",
      "epoch:33 step:31261 [D loss: 0.282979, acc.: 89.06%] [G loss: 1.728148]\n",
      "epoch:33 step:31262 [D loss: 0.088147, acc.: 99.22%] [G loss: 2.545839]\n",
      "epoch:33 step:31263 [D loss: 0.065691, acc.: 100.00%] [G loss: 1.500794]\n",
      "epoch:33 step:31264 [D loss: 0.471481, acc.: 71.88%] [G loss: 4.459828]\n",
      "epoch:33 step:31265 [D loss: 0.064863, acc.: 99.22%] [G loss: 2.454545]\n",
      "epoch:33 step:31266 [D loss: 0.212542, acc.: 90.62%] [G loss: 4.863071]\n",
      "epoch:33 step:31267 [D loss: 0.629747, acc.: 60.94%] [G loss: 3.406717]\n",
      "epoch:33 step:31268 [D loss: 0.398491, acc.: 78.12%] [G loss: 3.192348]\n",
      "epoch:33 step:31269 [D loss: 0.116498, acc.: 96.88%] [G loss: 0.753696]\n",
      "epoch:33 step:31270 [D loss: 1.266134, acc.: 37.50%] [G loss: 2.256958]\n",
      "epoch:33 step:31271 [D loss: 0.054197, acc.: 98.44%] [G loss: 1.438446]\n",
      "epoch:33 step:31272 [D loss: 0.113240, acc.: 97.66%] [G loss: 2.896379]\n",
      "epoch:33 step:31273 [D loss: 0.192548, acc.: 92.97%] [G loss: 5.096037]\n",
      "epoch:33 step:31274 [D loss: 0.222647, acc.: 93.75%] [G loss: 0.968069]\n",
      "epoch:33 step:31275 [D loss: 0.220083, acc.: 96.09%] [G loss: 1.121899]\n",
      "epoch:33 step:31276 [D loss: 0.129040, acc.: 98.44%] [G loss: 3.688511]\n",
      "epoch:33 step:31277 [D loss: 0.016492, acc.: 100.00%] [G loss: 3.380859]\n",
      "epoch:33 step:31278 [D loss: 0.231738, acc.: 96.09%] [G loss: 1.166118]\n",
      "epoch:33 step:31279 [D loss: 1.022782, acc.: 52.34%] [G loss: 3.541217]\n",
      "epoch:33 step:31280 [D loss: 0.689306, acc.: 72.66%] [G loss: 4.191610]\n",
      "epoch:33 step:31281 [D loss: 0.287809, acc.: 92.19%] [G loss: 4.336073]\n",
      "epoch:33 step:31282 [D loss: 0.388993, acc.: 87.50%] [G loss: 2.216851]\n",
      "epoch:33 step:31283 [D loss: 0.140980, acc.: 96.88%] [G loss: 2.454405]\n",
      "epoch:33 step:31284 [D loss: 0.053884, acc.: 97.66%] [G loss: 3.951500]\n",
      "epoch:33 step:31285 [D loss: 0.079554, acc.: 99.22%] [G loss: 2.406405]\n",
      "epoch:33 step:31286 [D loss: 0.346386, acc.: 84.38%] [G loss: 2.194960]\n",
      "epoch:33 step:31287 [D loss: 0.283324, acc.: 86.72%] [G loss: 2.333240]\n",
      "epoch:33 step:31288 [D loss: 0.170506, acc.: 96.88%] [G loss: 2.242737]\n",
      "epoch:33 step:31289 [D loss: 0.045161, acc.: 100.00%] [G loss: 2.298598]\n",
      "epoch:33 step:31290 [D loss: 1.017951, acc.: 46.09%] [G loss: 1.088260]\n",
      "epoch:33 step:31291 [D loss: 0.082319, acc.: 97.66%] [G loss: 4.413178]\n",
      "epoch:33 step:31292 [D loss: 0.154027, acc.: 95.31%] [G loss: 5.190756]\n",
      "epoch:33 step:31293 [D loss: 0.327538, acc.: 82.81%] [G loss: 4.997285]\n",
      "epoch:33 step:31294 [D loss: 0.184516, acc.: 98.44%] [G loss: 3.427683]\n",
      "epoch:33 step:31295 [D loss: 0.288687, acc.: 84.38%] [G loss: 4.922038]\n",
      "epoch:33 step:31296 [D loss: 0.055056, acc.: 98.44%] [G loss: 2.403248]\n",
      "epoch:33 step:31297 [D loss: 0.097886, acc.: 99.22%] [G loss: 3.628139]\n",
      "epoch:33 step:31298 [D loss: 0.037324, acc.: 100.00%] [G loss: 1.856392]\n",
      "epoch:33 step:31299 [D loss: 0.375404, acc.: 84.38%] [G loss: 0.904994]\n",
      "epoch:33 step:31300 [D loss: 0.293183, acc.: 92.97%] [G loss: 1.377721]\n",
      "epoch:33 step:31301 [D loss: 0.396803, acc.: 82.81%] [G loss: 2.877735]\n",
      "epoch:33 step:31302 [D loss: 0.093760, acc.: 96.09%] [G loss: 5.803598]\n",
      "epoch:33 step:31303 [D loss: 0.031875, acc.: 99.22%] [G loss: 4.010766]\n",
      "epoch:33 step:31304 [D loss: 0.407297, acc.: 87.50%] [G loss: 4.962467]\n",
      "epoch:33 step:31305 [D loss: 0.374889, acc.: 82.81%] [G loss: 2.281358]\n",
      "epoch:33 step:31306 [D loss: 0.261602, acc.: 89.84%] [G loss: 2.370619]\n",
      "epoch:33 step:31307 [D loss: 0.399645, acc.: 75.78%] [G loss: 2.244907]\n",
      "epoch:33 step:31308 [D loss: 0.097419, acc.: 97.66%] [G loss: 1.476397]\n",
      "epoch:33 step:31309 [D loss: 0.184532, acc.: 96.09%] [G loss: 2.530038]\n",
      "epoch:33 step:31310 [D loss: 0.567373, acc.: 64.84%] [G loss: 4.138733]\n",
      "epoch:33 step:31311 [D loss: 0.062856, acc.: 99.22%] [G loss: 6.644574]\n",
      "epoch:33 step:31312 [D loss: 0.206759, acc.: 93.75%] [G loss: 3.001681]\n",
      "epoch:33 step:31313 [D loss: 0.104907, acc.: 97.66%] [G loss: 3.097398]\n",
      "epoch:33 step:31314 [D loss: 0.101845, acc.: 98.44%] [G loss: 1.838690]\n",
      "epoch:33 step:31315 [D loss: 0.820433, acc.: 53.12%] [G loss: 1.791585]\n",
      "epoch:33 step:31316 [D loss: 0.258691, acc.: 86.72%] [G loss: 4.823489]\n",
      "epoch:33 step:31317 [D loss: 0.123727, acc.: 96.88%] [G loss: 2.058362]\n",
      "epoch:33 step:31318 [D loss: 0.561852, acc.: 67.19%] [G loss: 2.736058]\n",
      "epoch:33 step:31319 [D loss: 0.172906, acc.: 96.88%] [G loss: 1.004748]\n",
      "epoch:33 step:31320 [D loss: 0.491202, acc.: 74.22%] [G loss: 5.477192]\n",
      "epoch:33 step:31321 [D loss: 0.308715, acc.: 89.06%] [G loss: 2.456776]\n",
      "epoch:33 step:31322 [D loss: 0.073115, acc.: 99.22%] [G loss: 1.793763]\n",
      "epoch:33 step:31323 [D loss: 0.188492, acc.: 93.75%] [G loss: 1.137176]\n",
      "epoch:33 step:31324 [D loss: 0.386898, acc.: 89.06%] [G loss: 1.161553]\n",
      "epoch:33 step:31325 [D loss: 0.185734, acc.: 96.09%] [G loss: 1.997658]\n",
      "epoch:33 step:31326 [D loss: 0.518327, acc.: 74.22%] [G loss: 2.324837]\n",
      "epoch:33 step:31327 [D loss: 0.202964, acc.: 95.31%] [G loss: 2.117384]\n",
      "epoch:33 step:31328 [D loss: 0.241947, acc.: 92.97%] [G loss: 4.977546]\n",
      "epoch:33 step:31329 [D loss: 0.111910, acc.: 96.09%] [G loss: 3.969281]\n",
      "epoch:33 step:31330 [D loss: 0.226362, acc.: 94.53%] [G loss: 2.000906]\n",
      "epoch:33 step:31331 [D loss: 0.624949, acc.: 58.59%] [G loss: 3.890146]\n",
      "epoch:33 step:31332 [D loss: 0.234718, acc.: 91.41%] [G loss: 5.271815]\n",
      "epoch:33 step:31333 [D loss: 0.047289, acc.: 100.00%] [G loss: 0.541321]\n",
      "epoch:33 step:31334 [D loss: 0.149875, acc.: 96.09%] [G loss: 3.444320]\n",
      "epoch:33 step:31335 [D loss: 0.188814, acc.: 93.75%] [G loss: 2.237715]\n",
      "epoch:33 step:31336 [D loss: 0.474324, acc.: 68.75%] [G loss: 4.864663]\n",
      "epoch:33 step:31337 [D loss: 0.078278, acc.: 100.00%] [G loss: 5.992718]\n",
      "epoch:33 step:31338 [D loss: 0.114043, acc.: 96.88%] [G loss: 4.542311]\n",
      "epoch:33 step:31339 [D loss: 0.036234, acc.: 99.22%] [G loss: 4.994864]\n",
      "epoch:33 step:31340 [D loss: 0.858479, acc.: 55.47%] [G loss: 3.070977]\n",
      "epoch:33 step:31341 [D loss: 0.863877, acc.: 57.03%] [G loss: 1.005198]\n",
      "epoch:33 step:31342 [D loss: 0.250288, acc.: 90.62%] [G loss: 1.702826]\n",
      "epoch:33 step:31343 [D loss: 0.974126, acc.: 48.44%] [G loss: 4.061627]\n",
      "epoch:33 step:31344 [D loss: 0.166053, acc.: 95.31%] [G loss: 2.012005]\n",
      "epoch:33 step:31345 [D loss: 0.584010, acc.: 71.88%] [G loss: 2.350759]\n",
      "epoch:33 step:31346 [D loss: 0.288005, acc.: 91.41%] [G loss: 1.283991]\n",
      "epoch:33 step:31347 [D loss: 0.104127, acc.: 99.22%] [G loss: 4.423842]\n",
      "epoch:33 step:31348 [D loss: 0.057109, acc.: 99.22%] [G loss: 4.683828]\n",
      "epoch:33 step:31349 [D loss: 0.666646, acc.: 60.94%] [G loss: 4.286891]\n",
      "epoch:33 step:31350 [D loss: 0.123138, acc.: 98.44%] [G loss: 2.329999]\n",
      "epoch:33 step:31351 [D loss: 0.083017, acc.: 99.22%] [G loss: 3.107007]\n",
      "epoch:33 step:31352 [D loss: 0.249476, acc.: 91.41%] [G loss: 2.200837]\n",
      "epoch:33 step:31353 [D loss: 0.307668, acc.: 86.72%] [G loss: 0.552369]\n",
      "epoch:33 step:31354 [D loss: 0.079301, acc.: 99.22%] [G loss: 0.979534]\n",
      "epoch:33 step:31355 [D loss: 0.738374, acc.: 60.16%] [G loss: 1.936819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31356 [D loss: 0.080311, acc.: 97.66%] [G loss: 4.502955]\n",
      "epoch:33 step:31357 [D loss: 0.080592, acc.: 97.66%] [G loss: 7.519611]\n",
      "epoch:33 step:31358 [D loss: 0.457227, acc.: 82.03%] [G loss: 1.027526]\n",
      "epoch:33 step:31359 [D loss: 0.178731, acc.: 96.88%] [G loss: 1.054287]\n",
      "epoch:33 step:31360 [D loss: 0.066660, acc.: 98.44%] [G loss: 1.706395]\n",
      "epoch:33 step:31361 [D loss: 0.096618, acc.: 98.44%] [G loss: 2.166425]\n",
      "epoch:33 step:31362 [D loss: 0.057964, acc.: 100.00%] [G loss: 2.496935]\n",
      "epoch:33 step:31363 [D loss: 0.768152, acc.: 57.81%] [G loss: 1.632172]\n",
      "epoch:33 step:31364 [D loss: 0.315328, acc.: 92.97%] [G loss: 4.627897]\n",
      "epoch:33 step:31365 [D loss: 0.376643, acc.: 81.25%] [G loss: 4.214920]\n",
      "epoch:33 step:31366 [D loss: 0.920308, acc.: 48.44%] [G loss: 2.080517]\n",
      "epoch:33 step:31367 [D loss: 0.131044, acc.: 97.66%] [G loss: 1.249645]\n",
      "epoch:33 step:31368 [D loss: 0.169195, acc.: 96.88%] [G loss: 1.226681]\n",
      "epoch:33 step:31369 [D loss: 1.231421, acc.: 52.34%] [G loss: 3.786452]\n",
      "epoch:33 step:31370 [D loss: 0.066700, acc.: 99.22%] [G loss: 5.293694]\n",
      "epoch:33 step:31371 [D loss: 0.616788, acc.: 69.53%] [G loss: 3.549598]\n",
      "epoch:33 step:31372 [D loss: 0.064953, acc.: 99.22%] [G loss: 1.307691]\n",
      "epoch:33 step:31373 [D loss: 0.087776, acc.: 99.22%] [G loss: 3.705166]\n",
      "epoch:33 step:31374 [D loss: 0.353087, acc.: 81.25%] [G loss: 2.279990]\n",
      "epoch:33 step:31375 [D loss: 0.761628, acc.: 62.50%] [G loss: 6.452505]\n",
      "epoch:33 step:31376 [D loss: 0.232448, acc.: 89.06%] [G loss: 3.074592]\n",
      "epoch:33 step:31377 [D loss: 0.139854, acc.: 97.66%] [G loss: 3.522319]\n",
      "epoch:33 step:31378 [D loss: 0.236521, acc.: 91.41%] [G loss: 7.421061]\n",
      "epoch:33 step:31379 [D loss: 0.174179, acc.: 98.44%] [G loss: 4.035894]\n",
      "epoch:33 step:31380 [D loss: 0.371003, acc.: 91.41%] [G loss: 2.258197]\n",
      "epoch:33 step:31381 [D loss: 0.115589, acc.: 98.44%] [G loss: 2.753730]\n",
      "epoch:33 step:31382 [D loss: 1.471478, acc.: 23.44%] [G loss: 2.784439]\n",
      "epoch:33 step:31383 [D loss: 0.015419, acc.: 99.22%] [G loss: 5.013405]\n",
      "epoch:33 step:31384 [D loss: 0.714608, acc.: 63.28%] [G loss: 2.020416]\n",
      "epoch:33 step:31385 [D loss: 0.368651, acc.: 78.12%] [G loss: 1.451606]\n",
      "epoch:33 step:31386 [D loss: 0.015708, acc.: 100.00%] [G loss: 2.906252]\n",
      "epoch:33 step:31387 [D loss: 0.135884, acc.: 95.31%] [G loss: 3.245602]\n",
      "epoch:33 step:31388 [D loss: 0.077703, acc.: 98.44%] [G loss: 2.841810]\n",
      "epoch:33 step:31389 [D loss: 0.137690, acc.: 98.44%] [G loss: 3.604485]\n",
      "epoch:33 step:31390 [D loss: 0.088159, acc.: 97.66%] [G loss: 3.844398]\n",
      "epoch:33 step:31391 [D loss: 0.692034, acc.: 61.72%] [G loss: 2.553395]\n",
      "epoch:33 step:31392 [D loss: 0.357253, acc.: 82.03%] [G loss: 2.508440]\n",
      "epoch:33 step:31393 [D loss: 0.232572, acc.: 92.97%] [G loss: 1.789842]\n",
      "epoch:33 step:31394 [D loss: 0.274708, acc.: 85.94%] [G loss: 2.791261]\n",
      "epoch:33 step:31395 [D loss: 0.071923, acc.: 99.22%] [G loss: 4.413834]\n",
      "epoch:33 step:31396 [D loss: 0.067872, acc.: 99.22%] [G loss: 5.996969]\n",
      "epoch:33 step:31397 [D loss: 0.037651, acc.: 99.22%] [G loss: 3.970321]\n",
      "epoch:33 step:31398 [D loss: 0.092784, acc.: 98.44%] [G loss: 2.521246]\n",
      "epoch:33 step:31399 [D loss: 0.199603, acc.: 98.44%] [G loss: 2.799900]\n",
      "epoch:33 step:31400 [D loss: 0.849332, acc.: 61.72%] [G loss: 3.990776]\n",
      "epoch:33 step:31401 [D loss: 0.287890, acc.: 82.81%] [G loss: 4.272380]\n",
      "epoch:33 step:31402 [D loss: 0.146539, acc.: 97.66%] [G loss: 3.940752]\n",
      "epoch:33 step:31403 [D loss: 0.017216, acc.: 99.22%] [G loss: 4.971255]\n",
      "epoch:33 step:31404 [D loss: 0.026170, acc.: 99.22%] [G loss: 4.258026]\n",
      "epoch:33 step:31405 [D loss: 0.065015, acc.: 99.22%] [G loss: 1.433836]\n",
      "epoch:33 step:31406 [D loss: 0.145749, acc.: 97.66%] [G loss: 1.457394]\n",
      "epoch:33 step:31407 [D loss: 0.359932, acc.: 80.47%] [G loss: 2.960978]\n",
      "epoch:33 step:31408 [D loss: 0.028822, acc.: 100.00%] [G loss: 1.554832]\n",
      "epoch:33 step:31409 [D loss: 0.543232, acc.: 65.62%] [G loss: 2.055908]\n",
      "epoch:33 step:31410 [D loss: 0.046892, acc.: 99.22%] [G loss: 4.870202]\n",
      "epoch:33 step:31411 [D loss: 0.179878, acc.: 96.09%] [G loss: 2.417722]\n",
      "epoch:33 step:31412 [D loss: 0.170565, acc.: 94.53%] [G loss: 0.875776]\n",
      "epoch:33 step:31413 [D loss: 0.049455, acc.: 99.22%] [G loss: 0.691514]\n",
      "epoch:33 step:31414 [D loss: 0.065154, acc.: 98.44%] [G loss: 0.696986]\n",
      "epoch:33 step:31415 [D loss: 0.057242, acc.: 100.00%] [G loss: 5.251511]\n",
      "epoch:33 step:31416 [D loss: 0.282747, acc.: 89.06%] [G loss: 1.026257]\n",
      "epoch:33 step:31417 [D loss: 0.382185, acc.: 87.50%] [G loss: 6.312727]\n",
      "epoch:33 step:31418 [D loss: 0.049173, acc.: 100.00%] [G loss: 3.263883]\n",
      "epoch:33 step:31419 [D loss: 1.212183, acc.: 39.84%] [G loss: 2.024967]\n",
      "epoch:33 step:31420 [D loss: 0.027450, acc.: 99.22%] [G loss: 2.521722]\n",
      "epoch:33 step:31421 [D loss: 0.430617, acc.: 74.22%] [G loss: 3.728166]\n",
      "epoch:33 step:31422 [D loss: 0.305718, acc.: 87.50%] [G loss: 0.462819]\n",
      "epoch:33 step:31423 [D loss: 0.280008, acc.: 88.28%] [G loss: 2.253301]\n",
      "epoch:33 step:31424 [D loss: 0.185774, acc.: 96.09%] [G loss: 2.852013]\n",
      "epoch:33 step:31425 [D loss: 0.961248, acc.: 46.09%] [G loss: 2.476807]\n",
      "epoch:33 step:31426 [D loss: 0.309499, acc.: 84.38%] [G loss: 4.412342]\n",
      "epoch:33 step:31427 [D loss: 0.103079, acc.: 97.66%] [G loss: 5.319756]\n",
      "epoch:33 step:31428 [D loss: 0.262788, acc.: 91.41%] [G loss: 2.547164]\n",
      "epoch:33 step:31429 [D loss: 0.045105, acc.: 100.00%] [G loss: 3.196309]\n",
      "epoch:33 step:31430 [D loss: 0.298405, acc.: 86.72%] [G loss: 6.693218]\n",
      "epoch:33 step:31431 [D loss: 0.148422, acc.: 96.88%] [G loss: 3.026258]\n",
      "epoch:33 step:31432 [D loss: 1.047163, acc.: 48.44%] [G loss: 4.769473]\n",
      "epoch:33 step:31433 [D loss: 0.794404, acc.: 56.25%] [G loss: 1.211564]\n",
      "epoch:33 step:31434 [D loss: 0.053339, acc.: 99.22%] [G loss: 1.069970]\n",
      "epoch:33 step:31435 [D loss: 0.583947, acc.: 66.41%] [G loss: 5.037786]\n",
      "epoch:33 step:31436 [D loss: 0.033806, acc.: 100.00%] [G loss: 4.232095]\n",
      "epoch:33 step:31437 [D loss: 0.215840, acc.: 92.19%] [G loss: 6.378796]\n",
      "epoch:33 step:31438 [D loss: 0.549007, acc.: 77.34%] [G loss: 2.125095]\n",
      "epoch:33 step:31439 [D loss: 0.685651, acc.: 59.38%] [G loss: 2.372916]\n",
      "epoch:33 step:31440 [D loss: 0.281524, acc.: 85.16%] [G loss: 1.444194]\n",
      "epoch:33 step:31441 [D loss: 0.383376, acc.: 85.94%] [G loss: 3.943817]\n",
      "epoch:33 step:31442 [D loss: 0.138659, acc.: 96.88%] [G loss: 5.560579]\n",
      "epoch:33 step:31443 [D loss: 0.133007, acc.: 96.88%] [G loss: 4.899496]\n",
      "epoch:33 step:31444 [D loss: 0.113335, acc.: 98.44%] [G loss: 2.976563]\n",
      "epoch:33 step:31445 [D loss: 0.240869, acc.: 92.97%] [G loss: 3.419753]\n",
      "epoch:33 step:31446 [D loss: 0.452746, acc.: 81.25%] [G loss: 3.381771]\n",
      "epoch:33 step:31447 [D loss: 0.509572, acc.: 75.00%] [G loss: 3.835233]\n",
      "epoch:33 step:31448 [D loss: 0.310538, acc.: 93.75%] [G loss: 2.394086]\n",
      "epoch:33 step:31449 [D loss: 0.136155, acc.: 96.88%] [G loss: 5.769312]\n",
      "epoch:33 step:31450 [D loss: 0.612540, acc.: 67.19%] [G loss: 2.317771]\n",
      "epoch:33 step:31451 [D loss: 0.056663, acc.: 99.22%] [G loss: 3.442611]\n",
      "epoch:33 step:31452 [D loss: 0.057476, acc.: 100.00%] [G loss: 4.424052]\n",
      "epoch:33 step:31453 [D loss: 0.057495, acc.: 99.22%] [G loss: 5.036765]\n",
      "epoch:33 step:31454 [D loss: 0.170340, acc.: 97.66%] [G loss: 1.205172]\n",
      "epoch:33 step:31455 [D loss: 0.267834, acc.: 84.38%] [G loss: 4.791159]\n",
      "epoch:33 step:31456 [D loss: 0.180814, acc.: 92.19%] [G loss: 4.391953]\n",
      "epoch:33 step:31457 [D loss: 0.200894, acc.: 94.53%] [G loss: 3.491518]\n",
      "epoch:33 step:31458 [D loss: 0.784615, acc.: 54.69%] [G loss: 4.659372]\n",
      "epoch:33 step:31459 [D loss: 0.094174, acc.: 99.22%] [G loss: 2.665089]\n",
      "epoch:33 step:31460 [D loss: 0.110647, acc.: 99.22%] [G loss: 9.092285]\n",
      "epoch:33 step:31461 [D loss: 0.281866, acc.: 86.72%] [G loss: 2.515265]\n",
      "epoch:33 step:31462 [D loss: 0.686137, acc.: 62.50%] [G loss: 4.430357]\n",
      "epoch:33 step:31463 [D loss: 0.081942, acc.: 98.44%] [G loss: 2.171473]\n",
      "epoch:33 step:31464 [D loss: 0.097209, acc.: 97.66%] [G loss: 2.341651]\n",
      "epoch:33 step:31465 [D loss: 0.535276, acc.: 69.53%] [G loss: 1.585173]\n",
      "epoch:33 step:31466 [D loss: 0.752015, acc.: 60.16%] [G loss: 5.714611]\n",
      "epoch:33 step:31467 [D loss: 0.353838, acc.: 88.28%] [G loss: 6.560549]\n",
      "epoch:33 step:31468 [D loss: 0.168430, acc.: 95.31%] [G loss: 6.541946]\n",
      "epoch:33 step:31469 [D loss: 0.040388, acc.: 100.00%] [G loss: 3.945475]\n",
      "epoch:33 step:31470 [D loss: 0.200506, acc.: 92.97%] [G loss: 6.652247]\n",
      "epoch:33 step:31471 [D loss: 0.378879, acc.: 85.16%] [G loss: 2.585269]\n",
      "epoch:33 step:31472 [D loss: 0.145655, acc.: 98.44%] [G loss: 3.347476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31473 [D loss: 0.163656, acc.: 97.66%] [G loss: 4.510800]\n",
      "epoch:33 step:31474 [D loss: 0.587952, acc.: 67.97%] [G loss: 4.303591]\n",
      "epoch:33 step:31475 [D loss: 1.851211, acc.: 22.66%] [G loss: 0.542185]\n",
      "epoch:33 step:31476 [D loss: 0.398270, acc.: 85.16%] [G loss: 4.823485]\n",
      "epoch:33 step:31477 [D loss: 0.108453, acc.: 97.66%] [G loss: 6.238839]\n",
      "epoch:33 step:31478 [D loss: 0.387048, acc.: 83.59%] [G loss: 4.540213]\n",
      "epoch:33 step:31479 [D loss: 0.150043, acc.: 96.88%] [G loss: 3.009356]\n",
      "epoch:33 step:31480 [D loss: 0.251822, acc.: 93.75%] [G loss: 4.458404]\n",
      "epoch:33 step:31481 [D loss: 0.409687, acc.: 82.03%] [G loss: 3.992918]\n",
      "epoch:33 step:31482 [D loss: 0.320641, acc.: 83.59%] [G loss: 0.946352]\n",
      "epoch:33 step:31483 [D loss: 0.037450, acc.: 99.22%] [G loss: 2.447337]\n",
      "epoch:33 step:31484 [D loss: 0.088189, acc.: 100.00%] [G loss: 3.100298]\n",
      "epoch:33 step:31485 [D loss: 0.501047, acc.: 69.53%] [G loss: 2.810002]\n",
      "epoch:33 step:31486 [D loss: 0.053110, acc.: 99.22%] [G loss: 3.155695]\n",
      "epoch:33 step:31487 [D loss: 0.081096, acc.: 98.44%] [G loss: 4.729520]\n",
      "epoch:33 step:31488 [D loss: 0.384777, acc.: 83.59%] [G loss: 2.898329]\n",
      "epoch:33 step:31489 [D loss: 1.879964, acc.: 48.44%] [G loss: 2.059118]\n",
      "epoch:33 step:31490 [D loss: 0.241961, acc.: 89.84%] [G loss: 1.090241]\n",
      "epoch:33 step:31491 [D loss: 0.360165, acc.: 80.47%] [G loss: 2.090767]\n",
      "epoch:33 step:31492 [D loss: 0.463405, acc.: 76.56%] [G loss: 4.703313]\n",
      "epoch:33 step:31493 [D loss: 0.364976, acc.: 85.94%] [G loss: 2.828706]\n",
      "epoch:33 step:31494 [D loss: 0.160147, acc.: 94.53%] [G loss: 6.986601]\n",
      "epoch:33 step:31495 [D loss: 0.382896, acc.: 85.16%] [G loss: 4.008557]\n",
      "epoch:33 step:31496 [D loss: 0.042919, acc.: 98.44%] [G loss: 1.427193]\n",
      "epoch:33 step:31497 [D loss: 0.068747, acc.: 98.44%] [G loss: 2.241930]\n",
      "epoch:33 step:31498 [D loss: 0.113876, acc.: 100.00%] [G loss: 3.261780]\n",
      "epoch:33 step:31499 [D loss: 0.163279, acc.: 96.09%] [G loss: 3.851613]\n",
      "epoch:33 step:31500 [D loss: 0.077834, acc.: 99.22%] [G loss: 1.585470]\n",
      "epoch:33 step:31501 [D loss: 0.337891, acc.: 80.47%] [G loss: 2.105760]\n",
      "epoch:33 step:31502 [D loss: 0.106852, acc.: 97.66%] [G loss: 5.132629]\n",
      "epoch:33 step:31503 [D loss: 0.116142, acc.: 97.66%] [G loss: 5.160869]\n",
      "epoch:33 step:31504 [D loss: 0.115323, acc.: 97.66%] [G loss: 6.147630]\n",
      "epoch:33 step:31505 [D loss: 0.198507, acc.: 94.53%] [G loss: 5.675649]\n",
      "epoch:33 step:31506 [D loss: 0.061021, acc.: 100.00%] [G loss: 3.963681]\n",
      "epoch:33 step:31507 [D loss: 0.220166, acc.: 98.44%] [G loss: 1.441400]\n",
      "epoch:33 step:31508 [D loss: 0.372225, acc.: 83.59%] [G loss: 3.785009]\n",
      "epoch:33 step:31509 [D loss: 0.133542, acc.: 97.66%] [G loss: 5.307131]\n",
      "epoch:33 step:31510 [D loss: 0.037089, acc.: 99.22%] [G loss: 3.877354]\n",
      "epoch:33 step:31511 [D loss: 0.372880, acc.: 80.47%] [G loss: 2.190044]\n",
      "epoch:33 step:31512 [D loss: 0.118363, acc.: 96.09%] [G loss: 3.312032]\n",
      "epoch:33 step:31513 [D loss: 0.078346, acc.: 98.44%] [G loss: 2.102212]\n",
      "epoch:33 step:31514 [D loss: 0.019082, acc.: 100.00%] [G loss: 1.111583]\n",
      "epoch:33 step:31515 [D loss: 0.102965, acc.: 97.66%] [G loss: 0.986840]\n",
      "epoch:33 step:31516 [D loss: 0.616133, acc.: 64.06%] [G loss: 1.419569]\n",
      "epoch:33 step:31517 [D loss: 0.044048, acc.: 100.00%] [G loss: 3.241745]\n",
      "epoch:33 step:31518 [D loss: 0.080411, acc.: 98.44%] [G loss: 2.670946]\n",
      "epoch:33 step:31519 [D loss: 0.529218, acc.: 79.69%] [G loss: 4.797277]\n",
      "epoch:33 step:31520 [D loss: 0.175571, acc.: 95.31%] [G loss: 3.402634]\n",
      "epoch:33 step:31521 [D loss: 0.679538, acc.: 59.38%] [G loss: 4.659240]\n",
      "epoch:33 step:31522 [D loss: 0.013994, acc.: 100.00%] [G loss: 5.110903]\n",
      "epoch:33 step:31523 [D loss: 0.185931, acc.: 94.53%] [G loss: 1.775445]\n",
      "epoch:33 step:31524 [D loss: 0.154749, acc.: 95.31%] [G loss: 0.582244]\n",
      "epoch:33 step:31525 [D loss: 0.058901, acc.: 99.22%] [G loss: 3.435302]\n",
      "epoch:33 step:31526 [D loss: 0.324262, acc.: 82.81%] [G loss: 5.902716]\n",
      "epoch:33 step:31527 [D loss: 0.108039, acc.: 97.66%] [G loss: 6.390532]\n",
      "epoch:33 step:31528 [D loss: 0.777338, acc.: 54.69%] [G loss: 5.138786]\n",
      "epoch:33 step:31529 [D loss: 0.433277, acc.: 75.00%] [G loss: 3.632507]\n",
      "epoch:33 step:31530 [D loss: 0.115943, acc.: 97.66%] [G loss: 2.011983]\n",
      "epoch:33 step:31531 [D loss: 0.035343, acc.: 99.22%] [G loss: 2.945656]\n",
      "epoch:33 step:31532 [D loss: 0.060740, acc.: 100.00%] [G loss: 1.419853]\n",
      "epoch:33 step:31533 [D loss: 1.036812, acc.: 53.12%] [G loss: 1.041928]\n",
      "epoch:33 step:31534 [D loss: 0.090099, acc.: 98.44%] [G loss: 1.855459]\n",
      "epoch:33 step:31535 [D loss: 0.050070, acc.: 98.44%] [G loss: 2.916023]\n",
      "epoch:33 step:31536 [D loss: 0.338985, acc.: 79.69%] [G loss: 4.753581]\n",
      "epoch:33 step:31537 [D loss: 0.119233, acc.: 100.00%] [G loss: 3.656107]\n",
      "epoch:33 step:31538 [D loss: 0.242910, acc.: 92.19%] [G loss: 9.113609]\n",
      "epoch:33 step:31539 [D loss: 0.449764, acc.: 71.88%] [G loss: 4.297081]\n",
      "epoch:33 step:31540 [D loss: 0.025921, acc.: 99.22%] [G loss: 4.728125]\n",
      "epoch:33 step:31541 [D loss: 0.016122, acc.: 99.22%] [G loss: 1.131797]\n",
      "epoch:33 step:31542 [D loss: 0.579320, acc.: 66.41%] [G loss: 7.599187]\n",
      "epoch:33 step:31543 [D loss: 0.419056, acc.: 75.78%] [G loss: 3.325494]\n",
      "epoch:33 step:31544 [D loss: 0.190214, acc.: 92.19%] [G loss: 3.551030]\n",
      "epoch:33 step:31545 [D loss: 0.144417, acc.: 98.44%] [G loss: 3.469073]\n",
      "epoch:33 step:31546 [D loss: 0.069165, acc.: 98.44%] [G loss: 4.188697]\n",
      "epoch:33 step:31547 [D loss: 0.162243, acc.: 98.44%] [G loss: 2.293065]\n",
      "epoch:33 step:31548 [D loss: 0.581200, acc.: 70.31%] [G loss: 1.209702]\n",
      "epoch:33 step:31549 [D loss: 0.259820, acc.: 92.19%] [G loss: 4.540888]\n",
      "epoch:33 step:31550 [D loss: 0.416530, acc.: 76.56%] [G loss: 2.500366]\n",
      "epoch:33 step:31551 [D loss: 0.248959, acc.: 92.97%] [G loss: 1.104356]\n",
      "epoch:33 step:31552 [D loss: 0.417762, acc.: 80.47%] [G loss: 1.847236]\n",
      "epoch:33 step:31553 [D loss: 0.132437, acc.: 96.09%] [G loss: 0.787931]\n",
      "epoch:33 step:31554 [D loss: 0.212833, acc.: 94.53%] [G loss: 1.288138]\n",
      "epoch:33 step:31555 [D loss: 0.164490, acc.: 96.09%] [G loss: 5.855098]\n",
      "epoch:33 step:31556 [D loss: 0.935125, acc.: 56.25%] [G loss: 4.909888]\n",
      "epoch:33 step:31557 [D loss: 0.220484, acc.: 89.06%] [G loss: 1.833645]\n",
      "epoch:33 step:31558 [D loss: 0.677421, acc.: 65.62%] [G loss: 2.703989]\n",
      "epoch:33 step:31559 [D loss: 0.189129, acc.: 94.53%] [G loss: 4.379109]\n",
      "epoch:33 step:31560 [D loss: 0.062464, acc.: 98.44%] [G loss: 4.068006]\n",
      "epoch:33 step:31561 [D loss: 0.323686, acc.: 89.06%] [G loss: 6.729009]\n",
      "epoch:33 step:31562 [D loss: 0.448079, acc.: 77.34%] [G loss: 5.591765]\n",
      "epoch:33 step:31563 [D loss: 0.082076, acc.: 100.00%] [G loss: 9.423222]\n",
      "epoch:33 step:31564 [D loss: 1.032185, acc.: 53.12%] [G loss: 2.223595]\n",
      "epoch:33 step:31565 [D loss: 0.278791, acc.: 89.06%] [G loss: 4.357118]\n",
      "epoch:33 step:31566 [D loss: 0.422349, acc.: 79.69%] [G loss: 1.448058]\n",
      "epoch:33 step:31567 [D loss: 0.443717, acc.: 84.38%] [G loss: 1.204891]\n",
      "epoch:33 step:31568 [D loss: 0.026998, acc.: 100.00%] [G loss: 2.311804]\n",
      "epoch:33 step:31569 [D loss: 0.071190, acc.: 97.66%] [G loss: 0.939828]\n",
      "epoch:33 step:31570 [D loss: 0.506206, acc.: 73.44%] [G loss: 0.975266]\n",
      "epoch:33 step:31571 [D loss: 2.529752, acc.: 49.22%] [G loss: 0.751681]\n",
      "epoch:33 step:31572 [D loss: 0.083196, acc.: 98.44%] [G loss: 3.359184]\n",
      "epoch:33 step:31573 [D loss: 0.141050, acc.: 96.88%] [G loss: 2.896283]\n",
      "epoch:33 step:31574 [D loss: 0.355717, acc.: 86.72%] [G loss: 4.278372]\n",
      "epoch:33 step:31575 [D loss: 0.345988, acc.: 89.06%] [G loss: 4.260579]\n",
      "epoch:33 step:31576 [D loss: 0.860986, acc.: 60.16%] [G loss: 2.536656]\n",
      "epoch:33 step:31577 [D loss: 0.197076, acc.: 94.53%] [G loss: 1.645248]\n",
      "epoch:33 step:31578 [D loss: 0.108034, acc.: 97.66%] [G loss: 3.396567]\n",
      "epoch:33 step:31579 [D loss: 0.071840, acc.: 98.44%] [G loss: 6.620454]\n",
      "epoch:33 step:31580 [D loss: 0.154632, acc.: 97.66%] [G loss: 1.004690]\n",
      "epoch:33 step:31581 [D loss: 0.031212, acc.: 100.00%] [G loss: 2.257848]\n",
      "epoch:33 step:31582 [D loss: 0.397490, acc.: 76.56%] [G loss: 1.603871]\n",
      "epoch:33 step:31583 [D loss: 0.189850, acc.: 97.66%] [G loss: 1.885239]\n",
      "epoch:33 step:31584 [D loss: 0.170358, acc.: 95.31%] [G loss: 5.151913]\n",
      "epoch:33 step:31585 [D loss: 0.138530, acc.: 97.66%] [G loss: 4.916238]\n",
      "epoch:33 step:31586 [D loss: 0.373713, acc.: 85.94%] [G loss: 2.898801]\n",
      "epoch:33 step:31587 [D loss: 0.434538, acc.: 80.47%] [G loss: 0.541908]\n",
      "epoch:33 step:31588 [D loss: 0.292656, acc.: 92.97%] [G loss: 3.645931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31589 [D loss: 0.300804, acc.: 92.97%] [G loss: 3.467953]\n",
      "epoch:33 step:31590 [D loss: 0.527235, acc.: 64.06%] [G loss: 1.316130]\n",
      "epoch:33 step:31591 [D loss: 0.048791, acc.: 98.44%] [G loss: 14.710348]\n",
      "epoch:33 step:31592 [D loss: 0.157946, acc.: 96.88%] [G loss: 3.116844]\n",
      "epoch:33 step:31593 [D loss: 1.613526, acc.: 42.97%] [G loss: 1.941130]\n",
      "epoch:33 step:31594 [D loss: 0.238311, acc.: 96.09%] [G loss: 0.581545]\n",
      "epoch:33 step:31595 [D loss: 0.409618, acc.: 80.47%] [G loss: 3.237968]\n",
      "epoch:33 step:31596 [D loss: 0.248538, acc.: 89.84%] [G loss: 5.780019]\n",
      "epoch:33 step:31597 [D loss: 0.793379, acc.: 60.16%] [G loss: 0.532690]\n",
      "epoch:33 step:31598 [D loss: 0.080638, acc.: 97.66%] [G loss: 2.593246]\n",
      "epoch:33 step:31599 [D loss: 0.225684, acc.: 90.62%] [G loss: 2.612771]\n",
      "epoch:33 step:31600 [D loss: 1.040399, acc.: 53.91%] [G loss: 2.882663]\n",
      "epoch:33 step:31601 [D loss: 0.183756, acc.: 94.53%] [G loss: 2.018437]\n",
      "epoch:33 step:31602 [D loss: 0.481943, acc.: 74.22%] [G loss: 4.989674]\n",
      "epoch:33 step:31603 [D loss: 0.191419, acc.: 96.09%] [G loss: 3.402032]\n",
      "epoch:33 step:31604 [D loss: 0.130267, acc.: 96.88%] [G loss: 1.091928]\n",
      "epoch:33 step:31605 [D loss: 0.198320, acc.: 94.53%] [G loss: 3.728641]\n",
      "epoch:33 step:31606 [D loss: 0.584371, acc.: 62.50%] [G loss: 3.092661]\n",
      "epoch:33 step:31607 [D loss: 0.072553, acc.: 98.44%] [G loss: 2.301836]\n",
      "epoch:33 step:31608 [D loss: 0.029438, acc.: 100.00%] [G loss: 3.468075]\n",
      "epoch:33 step:31609 [D loss: 0.093531, acc.: 98.44%] [G loss: 4.407228]\n",
      "epoch:33 step:31610 [D loss: 0.484158, acc.: 82.81%] [G loss: 4.785376]\n",
      "epoch:33 step:31611 [D loss: 0.469152, acc.: 75.00%] [G loss: 6.432230]\n",
      "epoch:33 step:31612 [D loss: 0.214403, acc.: 94.53%] [G loss: 5.223537]\n",
      "epoch:33 step:31613 [D loss: 0.128325, acc.: 98.44%] [G loss: 1.583862]\n",
      "epoch:33 step:31614 [D loss: 0.304825, acc.: 89.06%] [G loss: 0.632502]\n",
      "epoch:33 step:31615 [D loss: 0.093353, acc.: 96.09%] [G loss: 5.285063]\n",
      "epoch:33 step:31616 [D loss: 0.523428, acc.: 70.31%] [G loss: 1.205978]\n",
      "epoch:33 step:31617 [D loss: 0.408291, acc.: 84.38%] [G loss: 5.415244]\n",
      "epoch:33 step:31618 [D loss: 0.334507, acc.: 81.25%] [G loss: 1.791065]\n",
      "epoch:33 step:31619 [D loss: 0.153600, acc.: 97.66%] [G loss: 4.776083]\n",
      "epoch:33 step:31620 [D loss: 0.070063, acc.: 98.44%] [G loss: 3.650764]\n",
      "epoch:33 step:31621 [D loss: 0.441105, acc.: 76.56%] [G loss: 1.614657]\n",
      "epoch:33 step:31622 [D loss: 0.520116, acc.: 75.00%] [G loss: 3.568801]\n",
      "epoch:33 step:31623 [D loss: 0.295107, acc.: 84.38%] [G loss: 4.729277]\n",
      "epoch:33 step:31624 [D loss: 0.682482, acc.: 61.72%] [G loss: 2.608562]\n",
      "epoch:33 step:31625 [D loss: 0.093083, acc.: 98.44%] [G loss: 4.044406]\n",
      "epoch:33 step:31626 [D loss: 0.800279, acc.: 52.34%] [G loss: 2.750190]\n",
      "epoch:33 step:31627 [D loss: 0.033033, acc.: 100.00%] [G loss: 4.425202]\n",
      "epoch:33 step:31628 [D loss: 0.103535, acc.: 97.66%] [G loss: 4.395721]\n",
      "epoch:33 step:31629 [D loss: 0.222850, acc.: 96.09%] [G loss: 2.463968]\n",
      "epoch:33 step:31630 [D loss: 0.152509, acc.: 96.88%] [G loss: 3.547755]\n",
      "epoch:33 step:31631 [D loss: 0.108838, acc.: 99.22%] [G loss: 2.656770]\n",
      "epoch:33 step:31632 [D loss: 0.129281, acc.: 97.66%] [G loss: 0.578433]\n",
      "epoch:33 step:31633 [D loss: 0.205079, acc.: 96.09%] [G loss: 2.058109]\n",
      "epoch:33 step:31634 [D loss: 0.412142, acc.: 77.34%] [G loss: 4.117158]\n",
      "epoch:33 step:31635 [D loss: 0.077366, acc.: 99.22%] [G loss: 1.123968]\n",
      "epoch:33 step:31636 [D loss: 0.319393, acc.: 85.94%] [G loss: 2.107684]\n",
      "epoch:33 step:31637 [D loss: 0.430735, acc.: 84.38%] [G loss: 4.239618]\n",
      "epoch:33 step:31638 [D loss: 0.786351, acc.: 59.38%] [G loss: 1.325478]\n",
      "epoch:33 step:31639 [D loss: 0.141673, acc.: 97.66%] [G loss: 3.830200]\n",
      "epoch:33 step:31640 [D loss: 0.995999, acc.: 52.34%] [G loss: 2.550395]\n",
      "epoch:33 step:31641 [D loss: 0.080128, acc.: 97.66%] [G loss: 2.298517]\n",
      "epoch:33 step:31642 [D loss: 0.287495, acc.: 89.06%] [G loss: 0.392621]\n",
      "epoch:33 step:31643 [D loss: 0.632279, acc.: 63.28%] [G loss: 0.646940]\n",
      "epoch:33 step:31644 [D loss: 0.678357, acc.: 63.28%] [G loss: 1.466618]\n",
      "epoch:33 step:31645 [D loss: 0.534218, acc.: 60.16%] [G loss: 3.447890]\n",
      "epoch:33 step:31646 [D loss: 0.655887, acc.: 60.94%] [G loss: 2.448413]\n",
      "epoch:33 step:31647 [D loss: 0.212336, acc.: 97.66%] [G loss: 1.737274]\n",
      "epoch:33 step:31648 [D loss: 0.081623, acc.: 100.00%] [G loss: 3.239542]\n",
      "epoch:33 step:31649 [D loss: 0.087617, acc.: 98.44%] [G loss: 4.561747]\n",
      "epoch:33 step:31650 [D loss: 0.455622, acc.: 69.53%] [G loss: 5.365236]\n",
      "epoch:33 step:31651 [D loss: 0.185810, acc.: 96.09%] [G loss: 7.532233]\n",
      "epoch:33 step:31652 [D loss: 0.362201, acc.: 78.12%] [G loss: 3.132025]\n",
      "epoch:33 step:31653 [D loss: 0.078671, acc.: 98.44%] [G loss: 5.201692]\n",
      "epoch:33 step:31654 [D loss: 0.072178, acc.: 98.44%] [G loss: 0.983712]\n",
      "epoch:33 step:31655 [D loss: 0.033821, acc.: 99.22%] [G loss: 5.047136]\n",
      "epoch:33 step:31656 [D loss: 0.151832, acc.: 93.75%] [G loss: 3.223847]\n",
      "epoch:33 step:31657 [D loss: 0.136717, acc.: 98.44%] [G loss: 2.100725]\n",
      "epoch:33 step:31658 [D loss: 0.342600, acc.: 87.50%] [G loss: 4.033505]\n",
      "epoch:33 step:31659 [D loss: 0.195438, acc.: 93.75%] [G loss: 0.755290]\n",
      "epoch:33 step:31660 [D loss: 0.168046, acc.: 98.44%] [G loss: 3.898915]\n",
      "epoch:33 step:31661 [D loss: 0.268831, acc.: 86.72%] [G loss: 1.706425]\n",
      "epoch:33 step:31662 [D loss: 0.123365, acc.: 98.44%] [G loss: 3.041147]\n",
      "epoch:33 step:31663 [D loss: 0.650701, acc.: 61.72%] [G loss: 4.116142]\n",
      "epoch:33 step:31664 [D loss: 0.076739, acc.: 98.44%] [G loss: 0.759768]\n",
      "epoch:33 step:31665 [D loss: 0.982182, acc.: 57.81%] [G loss: 5.237733]\n",
      "epoch:33 step:31666 [D loss: 0.179442, acc.: 98.44%] [G loss: 4.147156]\n",
      "epoch:33 step:31667 [D loss: 0.223315, acc.: 94.53%] [G loss: 0.370329]\n",
      "epoch:33 step:31668 [D loss: 0.044701, acc.: 100.00%] [G loss: 4.638094]\n",
      "epoch:33 step:31669 [D loss: 0.096635, acc.: 99.22%] [G loss: 4.333830]\n",
      "epoch:33 step:31670 [D loss: 0.465358, acc.: 74.22%] [G loss: 3.806303]\n",
      "epoch:33 step:31671 [D loss: 0.073291, acc.: 98.44%] [G loss: 6.832169]\n",
      "epoch:33 step:31672 [D loss: 1.455178, acc.: 53.12%] [G loss: 0.495392]\n",
      "epoch:33 step:31673 [D loss: 0.431814, acc.: 71.09%] [G loss: 1.206918]\n",
      "epoch:33 step:31674 [D loss: 0.115067, acc.: 98.44%] [G loss: 6.416194]\n",
      "epoch:33 step:31675 [D loss: 0.200721, acc.: 95.31%] [G loss: 3.752955]\n",
      "epoch:33 step:31676 [D loss: 0.118281, acc.: 97.66%] [G loss: 4.535037]\n",
      "epoch:33 step:31677 [D loss: 0.025185, acc.: 100.00%] [G loss: 3.803894]\n",
      "epoch:33 step:31678 [D loss: 0.241310, acc.: 93.75%] [G loss: 3.861403]\n",
      "epoch:33 step:31679 [D loss: 0.322873, acc.: 86.72%] [G loss: 2.088783]\n",
      "epoch:33 step:31680 [D loss: 0.471577, acc.: 68.75%] [G loss: 2.139584]\n",
      "epoch:33 step:31681 [D loss: 0.273505, acc.: 85.16%] [G loss: 2.151015]\n",
      "epoch:33 step:31682 [D loss: 0.077125, acc.: 99.22%] [G loss: 2.647823]\n",
      "epoch:33 step:31683 [D loss: 0.033057, acc.: 100.00%] [G loss: 2.703836]\n",
      "epoch:33 step:31684 [D loss: 0.178934, acc.: 96.88%] [G loss: 1.945140]\n",
      "epoch:33 step:31685 [D loss: 0.206855, acc.: 94.53%] [G loss: 5.508974]\n",
      "epoch:33 step:31686 [D loss: 0.027262, acc.: 100.00%] [G loss: 0.919010]\n",
      "epoch:33 step:31687 [D loss: 0.031154, acc.: 100.00%] [G loss: 0.642812]\n",
      "epoch:33 step:31688 [D loss: 0.124596, acc.: 98.44%] [G loss: 0.329000]\n",
      "epoch:33 step:31689 [D loss: 0.052708, acc.: 99.22%] [G loss: 3.208360]\n",
      "epoch:33 step:31690 [D loss: 0.683231, acc.: 58.59%] [G loss: 1.151031]\n",
      "epoch:33 step:31691 [D loss: 0.333829, acc.: 87.50%] [G loss: 0.257842]\n",
      "epoch:33 step:31692 [D loss: 0.966041, acc.: 53.91%] [G loss: 0.565197]\n",
      "epoch:33 step:31693 [D loss: 0.081450, acc.: 99.22%] [G loss: 2.250543]\n",
      "epoch:33 step:31694 [D loss: 1.041232, acc.: 54.69%] [G loss: 1.286124]\n",
      "epoch:33 step:31695 [D loss: 0.819244, acc.: 57.03%] [G loss: 2.734253]\n",
      "epoch:33 step:31696 [D loss: 0.259173, acc.: 92.97%] [G loss: 1.357828]\n",
      "epoch:33 step:31697 [D loss: 0.086203, acc.: 98.44%] [G loss: 3.488383]\n",
      "epoch:33 step:31698 [D loss: 0.265077, acc.: 92.97%] [G loss: 4.308871]\n",
      "epoch:33 step:31699 [D loss: 0.154203, acc.: 99.22%] [G loss: 4.488240]\n",
      "epoch:33 step:31700 [D loss: 0.425398, acc.: 73.44%] [G loss: 3.564417]\n",
      "epoch:33 step:31701 [D loss: 0.399707, acc.: 80.47%] [G loss: 1.555393]\n",
      "epoch:33 step:31702 [D loss: 0.218576, acc.: 92.19%] [G loss: 2.157107]\n",
      "epoch:33 step:31703 [D loss: 0.403571, acc.: 72.66%] [G loss: 4.386120]\n",
      "epoch:33 step:31704 [D loss: 0.502953, acc.: 69.53%] [G loss: 3.616562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31705 [D loss: 0.381297, acc.: 82.81%] [G loss: 2.082134]\n",
      "epoch:33 step:31706 [D loss: 0.046699, acc.: 100.00%] [G loss: 2.038391]\n",
      "epoch:33 step:31707 [D loss: 0.119788, acc.: 97.66%] [G loss: 0.721992]\n",
      "epoch:33 step:31708 [D loss: 0.150759, acc.: 95.31%] [G loss: 2.723802]\n",
      "epoch:33 step:31709 [D loss: 0.744014, acc.: 56.25%] [G loss: 4.426999]\n",
      "epoch:33 step:31710 [D loss: 0.609475, acc.: 67.19%] [G loss: 0.997248]\n",
      "epoch:33 step:31711 [D loss: 0.102146, acc.: 99.22%] [G loss: 3.986048]\n",
      "epoch:33 step:31712 [D loss: 0.131560, acc.: 96.88%] [G loss: 5.091311]\n",
      "epoch:33 step:31713 [D loss: 0.111349, acc.: 99.22%] [G loss: 2.405499]\n",
      "epoch:33 step:31714 [D loss: 0.061275, acc.: 100.00%] [G loss: 5.347346]\n",
      "epoch:33 step:31715 [D loss: 0.088827, acc.: 100.00%] [G loss: 0.738608]\n",
      "epoch:33 step:31716 [D loss: 0.141473, acc.: 97.66%] [G loss: 4.235135]\n",
      "epoch:33 step:31717 [D loss: 0.118635, acc.: 99.22%] [G loss: 1.476557]\n",
      "epoch:33 step:31718 [D loss: 0.069486, acc.: 98.44%] [G loss: 1.597944]\n",
      "epoch:33 step:31719 [D loss: 0.679896, acc.: 62.50%] [G loss: 5.194390]\n",
      "epoch:33 step:31720 [D loss: 0.076543, acc.: 98.44%] [G loss: 1.217176]\n",
      "epoch:33 step:31721 [D loss: 0.629115, acc.: 67.19%] [G loss: 4.092388]\n",
      "epoch:33 step:31722 [D loss: 1.186641, acc.: 53.91%] [G loss: 3.605706]\n",
      "epoch:33 step:31723 [D loss: 0.246384, acc.: 92.19%] [G loss: 3.703598]\n",
      "epoch:33 step:31724 [D loss: 0.319585, acc.: 96.09%] [G loss: 1.680932]\n",
      "epoch:33 step:31725 [D loss: 0.604612, acc.: 67.19%] [G loss: 2.826100]\n",
      "epoch:33 step:31726 [D loss: 0.375961, acc.: 80.47%] [G loss: 2.325068]\n",
      "epoch:33 step:31727 [D loss: 1.093904, acc.: 46.09%] [G loss: 1.347241]\n",
      "epoch:33 step:31728 [D loss: 0.086304, acc.: 99.22%] [G loss: 2.583043]\n",
      "epoch:33 step:31729 [D loss: 0.591709, acc.: 71.09%] [G loss: 3.327363]\n",
      "epoch:33 step:31730 [D loss: 0.269060, acc.: 91.41%] [G loss: 4.084154]\n",
      "epoch:33 step:31731 [D loss: 0.131664, acc.: 98.44%] [G loss: 4.099404]\n",
      "epoch:33 step:31732 [D loss: 0.193845, acc.: 93.75%] [G loss: 3.119278]\n",
      "epoch:33 step:31733 [D loss: 0.294987, acc.: 86.72%] [G loss: 2.357494]\n",
      "epoch:33 step:31734 [D loss: 0.281080, acc.: 90.62%] [G loss: 1.501022]\n",
      "epoch:33 step:31735 [D loss: 0.197287, acc.: 95.31%] [G loss: 1.048263]\n",
      "epoch:33 step:31736 [D loss: 0.446139, acc.: 76.56%] [G loss: 2.566968]\n",
      "epoch:33 step:31737 [D loss: 0.225449, acc.: 93.75%] [G loss: 2.955160]\n",
      "epoch:33 step:31738 [D loss: 1.544155, acc.: 51.56%] [G loss: 0.276995]\n",
      "epoch:33 step:31739 [D loss: 0.248472, acc.: 92.97%] [G loss: 4.339763]\n",
      "epoch:33 step:31740 [D loss: 0.264013, acc.: 92.19%] [G loss: 3.382699]\n",
      "epoch:33 step:31741 [D loss: 0.300493, acc.: 88.28%] [G loss: 3.475718]\n",
      "epoch:33 step:31742 [D loss: 0.224478, acc.: 96.88%] [G loss: 1.922299]\n",
      "epoch:33 step:31743 [D loss: 0.114767, acc.: 97.66%] [G loss: 1.548662]\n",
      "epoch:33 step:31744 [D loss: 0.037749, acc.: 100.00%] [G loss: 2.275542]\n",
      "epoch:33 step:31745 [D loss: 0.331341, acc.: 84.38%] [G loss: 3.507386]\n",
      "epoch:33 step:31746 [D loss: 0.293531, acc.: 89.06%] [G loss: 1.862175]\n",
      "epoch:33 step:31747 [D loss: 0.273052, acc.: 87.50%] [G loss: 4.452018]\n",
      "epoch:33 step:31748 [D loss: 0.174783, acc.: 92.19%] [G loss: 4.390338]\n",
      "epoch:33 step:31749 [D loss: 0.285601, acc.: 89.84%] [G loss: 4.629823]\n",
      "epoch:33 step:31750 [D loss: 1.296040, acc.: 51.56%] [G loss: 0.700654]\n",
      "epoch:33 step:31751 [D loss: 0.585680, acc.: 67.19%] [G loss: 0.908133]\n",
      "epoch:33 step:31752 [D loss: 0.642059, acc.: 60.94%] [G loss: 2.746995]\n",
      "epoch:33 step:31753 [D loss: 0.375661, acc.: 82.03%] [G loss: 2.153199]\n",
      "epoch:33 step:31754 [D loss: 0.327013, acc.: 85.94%] [G loss: 2.553093]\n",
      "epoch:33 step:31755 [D loss: 0.043123, acc.: 99.22%] [G loss: 4.661039]\n",
      "epoch:33 step:31756 [D loss: 0.309927, acc.: 88.28%] [G loss: 3.035286]\n",
      "epoch:33 step:31757 [D loss: 0.116654, acc.: 98.44%] [G loss: 1.320278]\n",
      "epoch:33 step:31758 [D loss: 0.264087, acc.: 93.75%] [G loss: 1.870003]\n",
      "epoch:33 step:31759 [D loss: 0.168227, acc.: 95.31%] [G loss: 3.547395]\n",
      "epoch:33 step:31760 [D loss: 0.361117, acc.: 81.25%] [G loss: 2.334136]\n",
      "epoch:33 step:31761 [D loss: 0.159230, acc.: 96.09%] [G loss: 2.966310]\n",
      "epoch:33 step:31762 [D loss: 0.134883, acc.: 97.66%] [G loss: 0.955729]\n",
      "epoch:33 step:31763 [D loss: 0.317040, acc.: 89.84%] [G loss: 3.575871]\n",
      "epoch:33 step:31764 [D loss: 0.175903, acc.: 96.88%] [G loss: 1.387760]\n",
      "epoch:33 step:31765 [D loss: 0.221408, acc.: 96.88%] [G loss: 3.403580]\n",
      "epoch:33 step:31766 [D loss: 0.723651, acc.: 59.38%] [G loss: 3.186733]\n",
      "epoch:33 step:31767 [D loss: 0.200406, acc.: 96.88%] [G loss: 2.782763]\n",
      "epoch:33 step:31768 [D loss: 0.130087, acc.: 94.53%] [G loss: 3.638572]\n",
      "epoch:33 step:31769 [D loss: 0.418340, acc.: 78.91%] [G loss: 0.866588]\n",
      "epoch:33 step:31770 [D loss: 0.077146, acc.: 99.22%] [G loss: 3.106449]\n",
      "epoch:33 step:31771 [D loss: 0.161126, acc.: 97.66%] [G loss: 4.346301]\n",
      "epoch:33 step:31772 [D loss: 0.413160, acc.: 78.91%] [G loss: 1.998651]\n",
      "epoch:33 step:31773 [D loss: 0.457350, acc.: 67.97%] [G loss: 1.153381]\n",
      "epoch:33 step:31774 [D loss: 0.908172, acc.: 57.81%] [G loss: 2.079030]\n",
      "epoch:33 step:31775 [D loss: 0.040222, acc.: 98.44%] [G loss: 1.886799]\n",
      "epoch:33 step:31776 [D loss: 0.602845, acc.: 61.72%] [G loss: 3.748312]\n",
      "epoch:33 step:31777 [D loss: 0.095334, acc.: 97.66%] [G loss: 3.865080]\n",
      "epoch:33 step:31778 [D loss: 0.210423, acc.: 94.53%] [G loss: 4.060644]\n",
      "epoch:33 step:31779 [D loss: 0.214768, acc.: 92.97%] [G loss: 4.942876]\n",
      "epoch:33 step:31780 [D loss: 0.720617, acc.: 62.50%] [G loss: 5.092926]\n",
      "epoch:33 step:31781 [D loss: 0.299739, acc.: 96.09%] [G loss: 1.870728]\n",
      "epoch:33 step:31782 [D loss: 0.135925, acc.: 98.44%] [G loss: 2.814016]\n",
      "epoch:33 step:31783 [D loss: 0.190100, acc.: 96.88%] [G loss: 2.301919]\n",
      "epoch:33 step:31784 [D loss: 0.018200, acc.: 100.00%] [G loss: 3.649104]\n",
      "epoch:33 step:31785 [D loss: 0.084112, acc.: 99.22%] [G loss: 3.125798]\n",
      "epoch:33 step:31786 [D loss: 0.018446, acc.: 100.00%] [G loss: 2.705460]\n",
      "epoch:33 step:31787 [D loss: 0.384622, acc.: 85.16%] [G loss: 1.131602]\n",
      "epoch:33 step:31788 [D loss: 0.338410, acc.: 80.47%] [G loss: 2.151654]\n",
      "epoch:33 step:31789 [D loss: 0.189776, acc.: 94.53%] [G loss: 6.927100]\n",
      "epoch:33 step:31790 [D loss: 0.775590, acc.: 54.69%] [G loss: 6.794547]\n",
      "epoch:33 step:31791 [D loss: 0.526963, acc.: 71.88%] [G loss: 3.720444]\n",
      "epoch:33 step:31792 [D loss: 0.034180, acc.: 100.00%] [G loss: 4.797334]\n",
      "epoch:33 step:31793 [D loss: 0.096494, acc.: 99.22%] [G loss: 6.845607]\n",
      "epoch:33 step:31794 [D loss: 0.344777, acc.: 89.06%] [G loss: 5.502650]\n",
      "epoch:33 step:31795 [D loss: 0.297873, acc.: 89.84%] [G loss: 1.726851]\n",
      "epoch:33 step:31796 [D loss: 0.125342, acc.: 99.22%] [G loss: 3.881389]\n",
      "epoch:33 step:31797 [D loss: 0.027614, acc.: 100.00%] [G loss: 5.855217]\n",
      "epoch:33 step:31798 [D loss: 0.106956, acc.: 98.44%] [G loss: 2.096963]\n",
      "epoch:33 step:31799 [D loss: 0.131342, acc.: 98.44%] [G loss: 3.044036]\n",
      "epoch:33 step:31800 [D loss: 0.082057, acc.: 98.44%] [G loss: 1.452707]\n",
      "epoch:33 step:31801 [D loss: 1.219273, acc.: 47.66%] [G loss: 1.410029]\n",
      "epoch:33 step:31802 [D loss: 0.062519, acc.: 98.44%] [G loss: 6.605205]\n",
      "epoch:33 step:31803 [D loss: 0.451246, acc.: 82.03%] [G loss: 5.888348]\n",
      "epoch:33 step:31804 [D loss: 0.736661, acc.: 57.81%] [G loss: 4.051695]\n",
      "epoch:33 step:31805 [D loss: 0.068560, acc.: 99.22%] [G loss: 2.425125]\n",
      "epoch:33 step:31806 [D loss: 0.364913, acc.: 81.25%] [G loss: 1.724554]\n",
      "epoch:33 step:31807 [D loss: 0.179598, acc.: 96.88%] [G loss: 5.340751]\n",
      "epoch:33 step:31808 [D loss: 0.634669, acc.: 65.62%] [G loss: 0.790903]\n",
      "epoch:33 step:31809 [D loss: 0.191296, acc.: 97.66%] [G loss: 2.107865]\n",
      "epoch:33 step:31810 [D loss: 0.203877, acc.: 94.53%] [G loss: 1.462908]\n",
      "epoch:33 step:31811 [D loss: 0.154157, acc.: 96.09%] [G loss: 1.990961]\n",
      "epoch:33 step:31812 [D loss: 0.016151, acc.: 100.00%] [G loss: 2.471177]\n",
      "epoch:33 step:31813 [D loss: 0.045475, acc.: 100.00%] [G loss: 6.684166]\n",
      "epoch:33 step:31814 [D loss: 0.099414, acc.: 98.44%] [G loss: 2.251212]\n",
      "epoch:33 step:31815 [D loss: 0.133795, acc.: 98.44%] [G loss: 5.336636]\n",
      "epoch:33 step:31816 [D loss: 0.038503, acc.: 100.00%] [G loss: 6.001611]\n",
      "epoch:33 step:31817 [D loss: 1.605478, acc.: 46.88%] [G loss: 1.348467]\n",
      "epoch:33 step:31818 [D loss: 0.376298, acc.: 83.59%] [G loss: 1.497704]\n",
      "epoch:33 step:31819 [D loss: 0.247947, acc.: 92.19%] [G loss: 1.614058]\n",
      "epoch:33 step:31820 [D loss: 0.121526, acc.: 99.22%] [G loss: 2.479564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:31821 [D loss: 0.256337, acc.: 95.31%] [G loss: 5.352212]\n",
      "epoch:33 step:31822 [D loss: 0.063902, acc.: 98.44%] [G loss: 3.169697]\n",
      "epoch:33 step:31823 [D loss: 0.130429, acc.: 98.44%] [G loss: 1.404751]\n",
      "epoch:33 step:31824 [D loss: 0.044878, acc.: 99.22%] [G loss: 3.598409]\n",
      "epoch:33 step:31825 [D loss: 0.664159, acc.: 63.28%] [G loss: 3.992855]\n",
      "epoch:33 step:31826 [D loss: 1.017637, acc.: 55.47%] [G loss: 1.107834]\n",
      "epoch:33 step:31827 [D loss: 0.328391, acc.: 81.25%] [G loss: 1.298579]\n",
      "epoch:33 step:31828 [D loss: 0.680379, acc.: 63.28%] [G loss: 0.259214]\n",
      "epoch:33 step:31829 [D loss: 0.529586, acc.: 78.12%] [G loss: 5.539366]\n",
      "epoch:33 step:31830 [D loss: 0.173292, acc.: 92.97%] [G loss: 2.819177]\n",
      "epoch:33 step:31831 [D loss: 0.282224, acc.: 91.41%] [G loss: 2.618994]\n",
      "epoch:33 step:31832 [D loss: 0.396177, acc.: 75.00%] [G loss: 4.944676]\n",
      "epoch:33 step:31833 [D loss: 0.578441, acc.: 67.97%] [G loss: 4.589135]\n",
      "epoch:33 step:31834 [D loss: 0.314946, acc.: 85.16%] [G loss: 1.868432]\n",
      "epoch:33 step:31835 [D loss: 0.605330, acc.: 66.41%] [G loss: 1.489839]\n",
      "epoch:33 step:31836 [D loss: 0.100012, acc.: 98.44%] [G loss: 2.679836]\n",
      "epoch:33 step:31837 [D loss: 0.572332, acc.: 62.50%] [G loss: 2.799670]\n",
      "epoch:33 step:31838 [D loss: 0.367488, acc.: 80.47%] [G loss: 2.271323]\n",
      "epoch:33 step:31839 [D loss: 0.151707, acc.: 98.44%] [G loss: 2.415477]\n",
      "epoch:33 step:31840 [D loss: 0.113090, acc.: 99.22%] [G loss: 1.803624]\n",
      "epoch:33 step:31841 [D loss: 1.001494, acc.: 53.91%] [G loss: 3.674434]\n",
      "epoch:33 step:31842 [D loss: 0.789024, acc.: 57.03%] [G loss: 4.152033]\n",
      "epoch:33 step:31843 [D loss: 0.328970, acc.: 82.03%] [G loss: 5.357998]\n",
      "epoch:33 step:31844 [D loss: 0.236433, acc.: 87.50%] [G loss: 4.806728]\n",
      "epoch:33 step:31845 [D loss: 0.566094, acc.: 66.41%] [G loss: 3.314219]\n",
      "epoch:33 step:31846 [D loss: 0.095577, acc.: 97.66%] [G loss: 1.285763]\n",
      "epoch:33 step:31847 [D loss: 0.305531, acc.: 87.50%] [G loss: 0.568320]\n",
      "epoch:33 step:31848 [D loss: 0.909217, acc.: 56.25%] [G loss: 2.120769]\n",
      "epoch:33 step:31849 [D loss: 0.405204, acc.: 75.78%] [G loss: 1.469911]\n",
      "epoch:33 step:31850 [D loss: 0.681450, acc.: 61.72%] [G loss: 1.860839]\n",
      "epoch:33 step:31851 [D loss: 0.253225, acc.: 89.06%] [G loss: 3.936753]\n",
      "epoch:33 step:31852 [D loss: 0.141113, acc.: 97.66%] [G loss: 4.388853]\n",
      "epoch:33 step:31853 [D loss: 0.218736, acc.: 92.97%] [G loss: 4.192499]\n",
      "epoch:33 step:31854 [D loss: 0.604355, acc.: 67.19%] [G loss: 3.299382]\n",
      "epoch:33 step:31855 [D loss: 0.400200, acc.: 78.12%] [G loss: 4.323279]\n",
      "epoch:33 step:31856 [D loss: 0.251718, acc.: 90.62%] [G loss: 1.796474]\n",
      "epoch:33 step:31857 [D loss: 0.313715, acc.: 89.06%] [G loss: 2.177472]\n",
      "epoch:33 step:31858 [D loss: 0.605453, acc.: 65.62%] [G loss: 1.845861]\n",
      "epoch:34 step:31859 [D loss: 0.088986, acc.: 99.22%] [G loss: 1.010580]\n",
      "epoch:34 step:31860 [D loss: 0.522178, acc.: 70.31%] [G loss: 2.491922]\n",
      "epoch:34 step:31861 [D loss: 0.160905, acc.: 96.88%] [G loss: 0.631863]\n",
      "epoch:34 step:31862 [D loss: 0.096815, acc.: 96.88%] [G loss: 3.748065]\n",
      "epoch:34 step:31863 [D loss: 0.132662, acc.: 96.88%] [G loss: 5.023356]\n",
      "epoch:34 step:31864 [D loss: 0.195408, acc.: 93.75%] [G loss: 1.639230]\n",
      "epoch:34 step:31865 [D loss: 1.409185, acc.: 46.09%] [G loss: 2.137480]\n",
      "epoch:34 step:31866 [D loss: 0.098923, acc.: 98.44%] [G loss: 3.421703]\n",
      "epoch:34 step:31867 [D loss: 0.468773, acc.: 72.66%] [G loss: 4.246079]\n",
      "epoch:34 step:31868 [D loss: 0.155578, acc.: 98.44%] [G loss: 3.264826]\n",
      "epoch:34 step:31869 [D loss: 0.069192, acc.: 100.00%] [G loss: 0.715725]\n",
      "epoch:34 step:31870 [D loss: 1.263324, acc.: 46.88%] [G loss: 0.778396]\n",
      "epoch:34 step:31871 [D loss: 0.786197, acc.: 54.69%] [G loss: 3.810452]\n",
      "epoch:34 step:31872 [D loss: 0.431306, acc.: 75.00%] [G loss: 2.952157]\n",
      "epoch:34 step:31873 [D loss: 0.279756, acc.: 91.41%] [G loss: 3.257001]\n",
      "epoch:34 step:31874 [D loss: 0.147476, acc.: 96.09%] [G loss: 3.035651]\n",
      "epoch:34 step:31875 [D loss: 0.228975, acc.: 93.75%] [G loss: 3.084555]\n",
      "epoch:34 step:31876 [D loss: 0.507643, acc.: 66.41%] [G loss: 4.856579]\n",
      "epoch:34 step:31877 [D loss: 0.047512, acc.: 100.00%] [G loss: 1.460526]\n",
      "epoch:34 step:31878 [D loss: 0.150417, acc.: 96.88%] [G loss: 1.701950]\n",
      "epoch:34 step:31879 [D loss: 0.082748, acc.: 96.88%] [G loss: 3.594494]\n",
      "epoch:34 step:31880 [D loss: 0.339104, acc.: 84.38%] [G loss: 2.068980]\n",
      "epoch:34 step:31881 [D loss: 0.332422, acc.: 81.25%] [G loss: 1.259722]\n",
      "epoch:34 step:31882 [D loss: 0.092829, acc.: 99.22%] [G loss: 1.264767]\n",
      "epoch:34 step:31883 [D loss: 0.039459, acc.: 100.00%] [G loss: 1.436153]\n",
      "epoch:34 step:31884 [D loss: 0.185956, acc.: 96.09%] [G loss: 3.519842]\n",
      "epoch:34 step:31885 [D loss: 0.419908, acc.: 75.00%] [G loss: 3.159639]\n",
      "epoch:34 step:31886 [D loss: 0.107037, acc.: 98.44%] [G loss: 2.972688]\n",
      "epoch:34 step:31887 [D loss: 0.661658, acc.: 63.28%] [G loss: 2.580035]\n",
      "epoch:34 step:31888 [D loss: 0.189429, acc.: 95.31%] [G loss: 3.248431]\n",
      "epoch:34 step:31889 [D loss: 0.459902, acc.: 72.66%] [G loss: 2.079021]\n",
      "epoch:34 step:31890 [D loss: 0.062198, acc.: 99.22%] [G loss: 4.182837]\n",
      "epoch:34 step:31891 [D loss: 0.041337, acc.: 100.00%] [G loss: 2.619221]\n",
      "epoch:34 step:31892 [D loss: 0.337415, acc.: 80.47%] [G loss: 2.745405]\n",
      "epoch:34 step:31893 [D loss: 0.436954, acc.: 84.38%] [G loss: 3.540529]\n",
      "epoch:34 step:31894 [D loss: 0.082861, acc.: 99.22%] [G loss: 2.649365]\n",
      "epoch:34 step:31895 [D loss: 0.145475, acc.: 98.44%] [G loss: 1.792900]\n",
      "epoch:34 step:31896 [D loss: 0.065989, acc.: 96.88%] [G loss: 2.814045]\n",
      "epoch:34 step:31897 [D loss: 0.229544, acc.: 91.41%] [G loss: 3.440187]\n",
      "epoch:34 step:31898 [D loss: 0.568645, acc.: 68.75%] [G loss: 3.816946]\n",
      "epoch:34 step:31899 [D loss: 0.236502, acc.: 92.19%] [G loss: 2.525644]\n",
      "epoch:34 step:31900 [D loss: 0.191137, acc.: 96.09%] [G loss: 1.686990]\n",
      "epoch:34 step:31901 [D loss: 0.047027, acc.: 99.22%] [G loss: 2.065049]\n",
      "epoch:34 step:31902 [D loss: 0.651852, acc.: 68.75%] [G loss: 0.594447]\n",
      "epoch:34 step:31903 [D loss: 0.143717, acc.: 98.44%] [G loss: 1.437558]\n",
      "epoch:34 step:31904 [D loss: 0.188706, acc.: 97.66%] [G loss: 0.269251]\n",
      "epoch:34 step:31905 [D loss: 0.066204, acc.: 99.22%] [G loss: 1.127416]\n",
      "epoch:34 step:31906 [D loss: 0.249257, acc.: 89.84%] [G loss: 1.373174]\n",
      "epoch:34 step:31907 [D loss: 0.060629, acc.: 99.22%] [G loss: 2.286074]\n",
      "epoch:34 step:31908 [D loss: 0.224329, acc.: 91.41%] [G loss: 4.322612]\n",
      "epoch:34 step:31909 [D loss: 0.108319, acc.: 97.66%] [G loss: 6.512368]\n",
      "epoch:34 step:31910 [D loss: 0.250704, acc.: 92.97%] [G loss: 3.711243]\n",
      "epoch:34 step:31911 [D loss: 0.074931, acc.: 100.00%] [G loss: 7.076437]\n",
      "epoch:34 step:31912 [D loss: 0.764394, acc.: 59.38%] [G loss: 1.847335]\n",
      "epoch:34 step:31913 [D loss: 0.574761, acc.: 68.75%] [G loss: 1.421256]\n",
      "epoch:34 step:31914 [D loss: 0.685217, acc.: 64.84%] [G loss: 3.196702]\n",
      "epoch:34 step:31915 [D loss: 1.133021, acc.: 27.34%] [G loss: 1.321647]\n",
      "epoch:34 step:31916 [D loss: 0.048834, acc.: 100.00%] [G loss: 1.679354]\n",
      "epoch:34 step:31917 [D loss: 0.450821, acc.: 74.22%] [G loss: 2.583873]\n",
      "epoch:34 step:31918 [D loss: 0.392036, acc.: 77.34%] [G loss: 4.228468]\n",
      "epoch:34 step:31919 [D loss: 0.503798, acc.: 75.78%] [G loss: 4.425598]\n",
      "epoch:34 step:31920 [D loss: 0.451297, acc.: 79.69%] [G loss: 2.862237]\n",
      "epoch:34 step:31921 [D loss: 0.778105, acc.: 56.25%] [G loss: 1.846491]\n",
      "epoch:34 step:31922 [D loss: 0.023542, acc.: 100.00%] [G loss: 3.743057]\n",
      "epoch:34 step:31923 [D loss: 0.110039, acc.: 99.22%] [G loss: 2.505772]\n",
      "epoch:34 step:31924 [D loss: 0.140084, acc.: 95.31%] [G loss: 1.112921]\n",
      "epoch:34 step:31925 [D loss: 0.092415, acc.: 97.66%] [G loss: 0.977385]\n",
      "epoch:34 step:31926 [D loss: 0.054337, acc.: 99.22%] [G loss: 2.288007]\n",
      "epoch:34 step:31927 [D loss: 0.268136, acc.: 90.62%] [G loss: 2.789444]\n",
      "epoch:34 step:31928 [D loss: 0.255961, acc.: 88.28%] [G loss: 1.942885]\n",
      "epoch:34 step:31929 [D loss: 0.543172, acc.: 73.44%] [G loss: 3.305410]\n",
      "epoch:34 step:31930 [D loss: 0.094107, acc.: 100.00%] [G loss: 1.673219]\n",
      "epoch:34 step:31931 [D loss: 0.271565, acc.: 92.97%] [G loss: 3.363652]\n",
      "epoch:34 step:31932 [D loss: 0.031886, acc.: 100.00%] [G loss: 1.925512]\n",
      "epoch:34 step:31933 [D loss: 0.610491, acc.: 67.97%] [G loss: 2.164350]\n",
      "epoch:34 step:31934 [D loss: 0.650738, acc.: 62.50%] [G loss: 5.359591]\n",
      "epoch:34 step:31935 [D loss: 0.245721, acc.: 94.53%] [G loss: 0.671259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:31936 [D loss: 0.019748, acc.: 100.00%] [G loss: 3.715863]\n",
      "epoch:34 step:31937 [D loss: 0.651601, acc.: 66.41%] [G loss: 1.480165]\n",
      "epoch:34 step:31938 [D loss: 0.069923, acc.: 99.22%] [G loss: 4.322306]\n",
      "epoch:34 step:31939 [D loss: 0.453340, acc.: 68.75%] [G loss: 3.661978]\n",
      "epoch:34 step:31940 [D loss: 0.743302, acc.: 64.06%] [G loss: 3.380076]\n",
      "epoch:34 step:31941 [D loss: 0.611314, acc.: 62.50%] [G loss: 4.946361]\n",
      "epoch:34 step:31942 [D loss: 0.582059, acc.: 69.53%] [G loss: 1.836451]\n",
      "epoch:34 step:31943 [D loss: 0.080653, acc.: 99.22%] [G loss: 2.647126]\n",
      "epoch:34 step:31944 [D loss: 0.173275, acc.: 96.88%] [G loss: 3.072506]\n",
      "epoch:34 step:31945 [D loss: 1.220127, acc.: 35.94%] [G loss: 3.980794]\n",
      "epoch:34 step:31946 [D loss: 0.074430, acc.: 100.00%] [G loss: 2.851264]\n",
      "epoch:34 step:31947 [D loss: 0.144852, acc.: 95.31%] [G loss: 2.696762]\n",
      "epoch:34 step:31948 [D loss: 0.881566, acc.: 53.91%] [G loss: 1.590051]\n",
      "epoch:34 step:31949 [D loss: 0.670885, acc.: 64.84%] [G loss: 2.282073]\n",
      "epoch:34 step:31950 [D loss: 0.188778, acc.: 96.09%] [G loss: 3.244194]\n",
      "epoch:34 step:31951 [D loss: 0.051759, acc.: 100.00%] [G loss: 3.240774]\n",
      "epoch:34 step:31952 [D loss: 0.190460, acc.: 93.75%] [G loss: 2.004811]\n",
      "epoch:34 step:31953 [D loss: 0.092710, acc.: 98.44%] [G loss: 1.823613]\n",
      "epoch:34 step:31954 [D loss: 0.423387, acc.: 85.94%] [G loss: 5.633428]\n",
      "epoch:34 step:31955 [D loss: 0.156520, acc.: 96.09%] [G loss: 3.114745]\n",
      "epoch:34 step:31956 [D loss: 0.164301, acc.: 95.31%] [G loss: 3.928411]\n",
      "epoch:34 step:31957 [D loss: 0.469146, acc.: 73.44%] [G loss: 1.149921]\n",
      "epoch:34 step:31958 [D loss: 0.314851, acc.: 81.25%] [G loss: 1.051581]\n",
      "epoch:34 step:31959 [D loss: 0.386553, acc.: 80.47%] [G loss: 2.074810]\n",
      "epoch:34 step:31960 [D loss: 0.328759, acc.: 88.28%] [G loss: 4.165975]\n",
      "epoch:34 step:31961 [D loss: 0.564903, acc.: 67.97%] [G loss: 3.010135]\n",
      "epoch:34 step:31962 [D loss: 0.132242, acc.: 98.44%] [G loss: 3.941557]\n",
      "epoch:34 step:31963 [D loss: 0.163811, acc.: 96.09%] [G loss: 2.430886]\n",
      "epoch:34 step:31964 [D loss: 0.241184, acc.: 94.53%] [G loss: 3.786842]\n",
      "epoch:34 step:31965 [D loss: 0.325854, acc.: 83.59%] [G loss: 3.558409]\n",
      "epoch:34 step:31966 [D loss: 0.258705, acc.: 91.41%] [G loss: 3.093631]\n",
      "epoch:34 step:31967 [D loss: 0.194828, acc.: 96.09%] [G loss: 3.498400]\n",
      "epoch:34 step:31968 [D loss: 0.140200, acc.: 96.88%] [G loss: 0.827446]\n",
      "epoch:34 step:31969 [D loss: 0.105317, acc.: 98.44%] [G loss: 1.078713]\n",
      "epoch:34 step:31970 [D loss: 0.270778, acc.: 92.97%] [G loss: 2.370294]\n",
      "epoch:34 step:31971 [D loss: 0.142567, acc.: 96.88%] [G loss: 1.125367]\n",
      "epoch:34 step:31972 [D loss: 0.317459, acc.: 85.16%] [G loss: 2.066806]\n",
      "epoch:34 step:31973 [D loss: 0.177364, acc.: 94.53%] [G loss: 1.283985]\n",
      "epoch:34 step:31974 [D loss: 0.282961, acc.: 88.28%] [G loss: 4.383450]\n",
      "epoch:34 step:31975 [D loss: 0.336476, acc.: 82.81%] [G loss: 2.319013]\n",
      "epoch:34 step:31976 [D loss: 1.253514, acc.: 30.47%] [G loss: 1.248349]\n",
      "epoch:34 step:31977 [D loss: 0.220690, acc.: 95.31%] [G loss: 5.977529]\n",
      "epoch:34 step:31978 [D loss: 0.120407, acc.: 97.66%] [G loss: 3.081036]\n",
      "epoch:34 step:31979 [D loss: 0.100688, acc.: 99.22%] [G loss: 2.701231]\n",
      "epoch:34 step:31980 [D loss: 0.190649, acc.: 95.31%] [G loss: 5.126921]\n",
      "epoch:34 step:31981 [D loss: 0.579960, acc.: 70.31%] [G loss: 1.507725]\n",
      "epoch:34 step:31982 [D loss: 0.309820, acc.: 84.38%] [G loss: 1.913202]\n",
      "epoch:34 step:31983 [D loss: 0.919399, acc.: 59.38%] [G loss: 1.057752]\n",
      "epoch:34 step:31984 [D loss: 0.070378, acc.: 99.22%] [G loss: 1.206469]\n",
      "epoch:34 step:31985 [D loss: 0.136318, acc.: 96.09%] [G loss: 2.458011]\n",
      "epoch:34 step:31986 [D loss: 0.461435, acc.: 79.69%] [G loss: 3.794782]\n",
      "epoch:34 step:31987 [D loss: 0.239090, acc.: 94.53%] [G loss: 2.274951]\n",
      "epoch:34 step:31988 [D loss: 1.215662, acc.: 47.66%] [G loss: 2.685459]\n",
      "epoch:34 step:31989 [D loss: 0.389857, acc.: 81.25%] [G loss: 4.901985]\n",
      "epoch:34 step:31990 [D loss: 0.304974, acc.: 87.50%] [G loss: 1.516072]\n",
      "epoch:34 step:31991 [D loss: 0.099370, acc.: 98.44%] [G loss: 1.509610]\n",
      "epoch:34 step:31992 [D loss: 0.445591, acc.: 78.12%] [G loss: 2.142841]\n",
      "epoch:34 step:31993 [D loss: 0.056895, acc.: 100.00%] [G loss: 2.669992]\n",
      "epoch:34 step:31994 [D loss: 0.820734, acc.: 54.69%] [G loss: 1.176656]\n",
      "epoch:34 step:31995 [D loss: 0.404248, acc.: 78.12%] [G loss: 0.542998]\n",
      "epoch:34 step:31996 [D loss: 0.277501, acc.: 88.28%] [G loss: 0.599875]\n",
      "epoch:34 step:31997 [D loss: 0.462620, acc.: 78.91%] [G loss: 2.185924]\n",
      "epoch:34 step:31998 [D loss: 0.321890, acc.: 85.16%] [G loss: 1.135479]\n",
      "epoch:34 step:31999 [D loss: 0.416206, acc.: 82.81%] [G loss: 4.722252]\n",
      "epoch:34 step:32000 [D loss: 0.314284, acc.: 87.50%] [G loss: 0.811303]\n",
      "epoch:34 step:32001 [D loss: 0.101689, acc.: 96.88%] [G loss: 2.843166]\n",
      "epoch:34 step:32002 [D loss: 0.790463, acc.: 57.03%] [G loss: 4.651263]\n",
      "epoch:34 step:32003 [D loss: 1.123715, acc.: 32.03%] [G loss: 3.666252]\n",
      "epoch:34 step:32004 [D loss: 0.270473, acc.: 92.97%] [G loss: 2.280346]\n",
      "epoch:34 step:32005 [D loss: 0.142521, acc.: 97.66%] [G loss: 2.073963]\n",
      "epoch:34 step:32006 [D loss: 0.058721, acc.: 99.22%] [G loss: 3.717505]\n",
      "epoch:34 step:32007 [D loss: 0.445608, acc.: 81.25%] [G loss: 2.959381]\n",
      "epoch:34 step:32008 [D loss: 0.414800, acc.: 82.81%] [G loss: 1.706040]\n",
      "epoch:34 step:32009 [D loss: 0.333378, acc.: 92.97%] [G loss: 4.674509]\n",
      "epoch:34 step:32010 [D loss: 0.038506, acc.: 100.00%] [G loss: 0.944867]\n",
      "epoch:34 step:32011 [D loss: 0.517564, acc.: 68.75%] [G loss: 3.992102]\n",
      "epoch:34 step:32012 [D loss: 0.185562, acc.: 95.31%] [G loss: 1.934032]\n",
      "epoch:34 step:32013 [D loss: 0.045815, acc.: 99.22%] [G loss: 3.822598]\n",
      "epoch:34 step:32014 [D loss: 0.420031, acc.: 80.47%] [G loss: 1.358074]\n",
      "epoch:34 step:32015 [D loss: 0.136150, acc.: 96.09%] [G loss: 0.836694]\n",
      "epoch:34 step:32016 [D loss: 0.980541, acc.: 53.12%] [G loss: 0.596905]\n",
      "epoch:34 step:32017 [D loss: 0.045983, acc.: 99.22%] [G loss: 4.386349]\n",
      "epoch:34 step:32018 [D loss: 0.042244, acc.: 100.00%] [G loss: 3.708558]\n",
      "epoch:34 step:32019 [D loss: 1.050063, acc.: 54.69%] [G loss: 3.052289]\n",
      "epoch:34 step:32020 [D loss: 0.074872, acc.: 100.00%] [G loss: 0.946249]\n",
      "epoch:34 step:32021 [D loss: 0.686471, acc.: 63.28%] [G loss: 4.526981]\n",
      "epoch:34 step:32022 [D loss: 0.410427, acc.: 82.81%] [G loss: 2.002544]\n",
      "epoch:34 step:32023 [D loss: 0.721676, acc.: 63.28%] [G loss: 1.004432]\n",
      "epoch:34 step:32024 [D loss: 0.690413, acc.: 65.62%] [G loss: 1.684570]\n",
      "epoch:34 step:32025 [D loss: 0.243055, acc.: 93.75%] [G loss: 1.835033]\n",
      "epoch:34 step:32026 [D loss: 0.117959, acc.: 98.44%] [G loss: 3.295685]\n",
      "epoch:34 step:32027 [D loss: 0.170019, acc.: 97.66%] [G loss: 4.391453]\n",
      "epoch:34 step:32028 [D loss: 0.241649, acc.: 93.75%] [G loss: 1.924181]\n",
      "epoch:34 step:32029 [D loss: 0.077147, acc.: 98.44%] [G loss: 3.217141]\n",
      "epoch:34 step:32030 [D loss: 0.142333, acc.: 97.66%] [G loss: 2.082474]\n",
      "epoch:34 step:32031 [D loss: 0.507200, acc.: 75.78%] [G loss: 2.190280]\n",
      "epoch:34 step:32032 [D loss: 0.226666, acc.: 91.41%] [G loss: 4.486228]\n",
      "epoch:34 step:32033 [D loss: 0.539522, acc.: 73.44%] [G loss: 6.622509]\n",
      "epoch:34 step:32034 [D loss: 0.277323, acc.: 96.09%] [G loss: 5.987950]\n",
      "epoch:34 step:32035 [D loss: 0.083269, acc.: 100.00%] [G loss: 2.610195]\n",
      "epoch:34 step:32036 [D loss: 0.125748, acc.: 97.66%] [G loss: 3.062847]\n",
      "epoch:34 step:32037 [D loss: 0.105539, acc.: 98.44%] [G loss: 3.137308]\n",
      "epoch:34 step:32038 [D loss: 0.232099, acc.: 93.75%] [G loss: 1.848490]\n",
      "epoch:34 step:32039 [D loss: 1.302000, acc.: 21.09%] [G loss: 2.601331]\n",
      "epoch:34 step:32040 [D loss: 0.490861, acc.: 74.22%] [G loss: 1.161113]\n",
      "epoch:34 step:32041 [D loss: 0.217796, acc.: 95.31%] [G loss: 5.897943]\n",
      "epoch:34 step:32042 [D loss: 0.894800, acc.: 59.38%] [G loss: 2.100626]\n",
      "epoch:34 step:32043 [D loss: 0.069692, acc.: 98.44%] [G loss: 2.467237]\n",
      "epoch:34 step:32044 [D loss: 0.423337, acc.: 78.12%] [G loss: 1.766350]\n",
      "epoch:34 step:32045 [D loss: 0.364370, acc.: 84.38%] [G loss: 0.910930]\n",
      "epoch:34 step:32046 [D loss: 0.778517, acc.: 60.94%] [G loss: 2.293766]\n",
      "epoch:34 step:32047 [D loss: 0.196995, acc.: 95.31%] [G loss: 2.360199]\n",
      "epoch:34 step:32048 [D loss: 0.640298, acc.: 61.72%] [G loss: 3.506101]\n",
      "epoch:34 step:32049 [D loss: 0.472098, acc.: 71.88%] [G loss: 2.915710]\n",
      "epoch:34 step:32050 [D loss: 0.120081, acc.: 99.22%] [G loss: 2.582059]\n",
      "epoch:34 step:32051 [D loss: 0.119988, acc.: 97.66%] [G loss: 3.668814]\n",
      "epoch:34 step:32052 [D loss: 0.350802, acc.: 87.50%] [G loss: 6.176148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32053 [D loss: 0.180475, acc.: 97.66%] [G loss: 0.913746]\n",
      "epoch:34 step:32054 [D loss: 0.281621, acc.: 90.62%] [G loss: 2.912858]\n",
      "epoch:34 step:32055 [D loss: 0.177970, acc.: 98.44%] [G loss: 3.345206]\n",
      "epoch:34 step:32056 [D loss: 0.232537, acc.: 89.84%] [G loss: 2.317314]\n",
      "epoch:34 step:32057 [D loss: 0.312897, acc.: 85.94%] [G loss: 1.651057]\n",
      "epoch:34 step:32058 [D loss: 0.517131, acc.: 72.66%] [G loss: 3.872701]\n",
      "epoch:34 step:32059 [D loss: 0.421448, acc.: 78.91%] [G loss: 2.958187]\n",
      "epoch:34 step:32060 [D loss: 0.765406, acc.: 64.06%] [G loss: 6.421778]\n",
      "epoch:34 step:32061 [D loss: 0.408622, acc.: 85.94%] [G loss: 3.188000]\n",
      "epoch:34 step:32062 [D loss: 0.969686, acc.: 55.47%] [G loss: 3.269220]\n",
      "epoch:34 step:32063 [D loss: 0.122931, acc.: 96.88%] [G loss: 1.132420]\n",
      "epoch:34 step:32064 [D loss: 0.065087, acc.: 99.22%] [G loss: 2.355460]\n",
      "epoch:34 step:32065 [D loss: 0.144033, acc.: 96.88%] [G loss: 1.866042]\n",
      "epoch:34 step:32066 [D loss: 0.078158, acc.: 98.44%] [G loss: 3.629141]\n",
      "epoch:34 step:32067 [D loss: 0.081408, acc.: 98.44%] [G loss: 1.808938]\n",
      "epoch:34 step:32068 [D loss: 0.165726, acc.: 97.66%] [G loss: 1.044739]\n",
      "epoch:34 step:32069 [D loss: 0.255558, acc.: 94.53%] [G loss: 6.265559]\n",
      "epoch:34 step:32070 [D loss: 0.316986, acc.: 81.25%] [G loss: 2.807047]\n",
      "epoch:34 step:32071 [D loss: 1.458182, acc.: 39.06%] [G loss: 2.609696]\n",
      "epoch:34 step:32072 [D loss: 0.749227, acc.: 59.38%] [G loss: 2.364602]\n",
      "epoch:34 step:32073 [D loss: 0.062497, acc.: 99.22%] [G loss: 1.471304]\n",
      "epoch:34 step:32074 [D loss: 0.171530, acc.: 96.88%] [G loss: 4.511580]\n",
      "epoch:34 step:32075 [D loss: 0.150378, acc.: 97.66%] [G loss: 3.929797]\n",
      "epoch:34 step:32076 [D loss: 0.144767, acc.: 97.66%] [G loss: 3.856617]\n",
      "epoch:34 step:32077 [D loss: 0.638969, acc.: 61.72%] [G loss: 3.107921]\n",
      "epoch:34 step:32078 [D loss: 0.295791, acc.: 85.94%] [G loss: 5.963175]\n",
      "epoch:34 step:32079 [D loss: 0.182094, acc.: 95.31%] [G loss: 3.515104]\n",
      "epoch:34 step:32080 [D loss: 0.242700, acc.: 94.53%] [G loss: 1.907565]\n",
      "epoch:34 step:32081 [D loss: 0.702496, acc.: 64.06%] [G loss: 0.486877]\n",
      "epoch:34 step:32082 [D loss: 0.516671, acc.: 70.31%] [G loss: 1.250191]\n",
      "epoch:34 step:32083 [D loss: 0.452619, acc.: 82.03%] [G loss: 1.830502]\n",
      "epoch:34 step:32084 [D loss: 0.176611, acc.: 96.09%] [G loss: 2.272660]\n",
      "epoch:34 step:32085 [D loss: 0.110616, acc.: 98.44%] [G loss: 4.563716]\n",
      "epoch:34 step:32086 [D loss: 0.158659, acc.: 96.09%] [G loss: 7.235920]\n",
      "epoch:34 step:32087 [D loss: 0.439875, acc.: 78.12%] [G loss: 2.272851]\n",
      "epoch:34 step:32088 [D loss: 0.372104, acc.: 86.72%] [G loss: 1.413237]\n",
      "epoch:34 step:32089 [D loss: 0.169716, acc.: 93.75%] [G loss: 2.205802]\n",
      "epoch:34 step:32090 [D loss: 0.348897, acc.: 78.91%] [G loss: 1.787282]\n",
      "epoch:34 step:32091 [D loss: 0.192737, acc.: 96.88%] [G loss: 1.765849]\n",
      "epoch:34 step:32092 [D loss: 0.332189, acc.: 82.03%] [G loss: 1.109563]\n",
      "epoch:34 step:32093 [D loss: 0.401713, acc.: 75.78%] [G loss: 3.087422]\n",
      "epoch:34 step:32094 [D loss: 0.061303, acc.: 100.00%] [G loss: 0.647401]\n",
      "epoch:34 step:32095 [D loss: 0.749149, acc.: 56.25%] [G loss: 2.065200]\n",
      "epoch:34 step:32096 [D loss: 0.238491, acc.: 94.53%] [G loss: 1.100594]\n",
      "epoch:34 step:32097 [D loss: 0.073409, acc.: 99.22%] [G loss: 2.754185]\n",
      "epoch:34 step:32098 [D loss: 0.325703, acc.: 82.81%] [G loss: 1.374146]\n",
      "epoch:34 step:32099 [D loss: 0.396989, acc.: 81.25%] [G loss: 4.176662]\n",
      "epoch:34 step:32100 [D loss: 0.192600, acc.: 93.75%] [G loss: 2.770308]\n",
      "epoch:34 step:32101 [D loss: 0.022838, acc.: 100.00%] [G loss: 2.426562]\n",
      "epoch:34 step:32102 [D loss: 0.174802, acc.: 95.31%] [G loss: 2.887295]\n",
      "epoch:34 step:32103 [D loss: 0.293994, acc.: 85.94%] [G loss: 1.680691]\n",
      "epoch:34 step:32104 [D loss: 0.063513, acc.: 99.22%] [G loss: 2.232373]\n",
      "epoch:34 step:32105 [D loss: 0.076999, acc.: 96.88%] [G loss: 2.110485]\n",
      "epoch:34 step:32106 [D loss: 0.098238, acc.: 99.22%] [G loss: 2.834150]\n",
      "epoch:34 step:32107 [D loss: 0.776466, acc.: 55.47%] [G loss: 3.293645]\n",
      "epoch:34 step:32108 [D loss: 0.073589, acc.: 97.66%] [G loss: 3.114692]\n",
      "epoch:34 step:32109 [D loss: 0.235103, acc.: 91.41%] [G loss: 3.684409]\n",
      "epoch:34 step:32110 [D loss: 0.134156, acc.: 99.22%] [G loss: 2.742844]\n",
      "epoch:34 step:32111 [D loss: 0.132135, acc.: 98.44%] [G loss: 3.188718]\n",
      "epoch:34 step:32112 [D loss: 0.091399, acc.: 99.22%] [G loss: 6.594422]\n",
      "epoch:34 step:32113 [D loss: 0.305343, acc.: 95.31%] [G loss: 2.657827]\n",
      "epoch:34 step:32114 [D loss: 0.315452, acc.: 86.72%] [G loss: 6.927727]\n",
      "epoch:34 step:32115 [D loss: 0.125091, acc.: 97.66%] [G loss: 6.494365]\n",
      "epoch:34 step:32116 [D loss: 0.507612, acc.: 75.00%] [G loss: 3.091040]\n",
      "epoch:34 step:32117 [D loss: 0.482524, acc.: 73.44%] [G loss: 3.354382]\n",
      "epoch:34 step:32118 [D loss: 0.256067, acc.: 92.97%] [G loss: 2.738861]\n",
      "epoch:34 step:32119 [D loss: 0.350477, acc.: 89.84%] [G loss: 3.854224]\n",
      "epoch:34 step:32120 [D loss: 0.567488, acc.: 71.88%] [G loss: 4.359975]\n",
      "epoch:34 step:32121 [D loss: 0.094145, acc.: 97.66%] [G loss: 1.136611]\n",
      "epoch:34 step:32122 [D loss: 1.160250, acc.: 45.31%] [G loss: 3.381372]\n",
      "epoch:34 step:32123 [D loss: 0.582846, acc.: 72.66%] [G loss: 0.792615]\n",
      "epoch:34 step:32124 [D loss: 0.503011, acc.: 76.56%] [G loss: 2.024934]\n",
      "epoch:34 step:32125 [D loss: 0.252883, acc.: 89.06%] [G loss: 3.655799]\n",
      "epoch:34 step:32126 [D loss: 0.158886, acc.: 96.09%] [G loss: 2.313088]\n",
      "epoch:34 step:32127 [D loss: 0.793128, acc.: 62.50%] [G loss: 2.907704]\n",
      "epoch:34 step:32128 [D loss: 0.026708, acc.: 99.22%] [G loss: 6.965178]\n",
      "epoch:34 step:32129 [D loss: 0.434541, acc.: 71.09%] [G loss: 4.632013]\n",
      "epoch:34 step:32130 [D loss: 0.067706, acc.: 99.22%] [G loss: 3.891824]\n",
      "epoch:34 step:32131 [D loss: 0.161401, acc.: 97.66%] [G loss: 2.001833]\n",
      "epoch:34 step:32132 [D loss: 0.088787, acc.: 97.66%] [G loss: 4.039416]\n",
      "epoch:34 step:32133 [D loss: 0.040558, acc.: 100.00%] [G loss: 2.959370]\n",
      "epoch:34 step:32134 [D loss: 0.248276, acc.: 89.06%] [G loss: 4.732949]\n",
      "epoch:34 step:32135 [D loss: 0.090994, acc.: 99.22%] [G loss: 3.016889]\n",
      "epoch:34 step:32136 [D loss: 0.113881, acc.: 96.88%] [G loss: 2.649168]\n",
      "epoch:34 step:32137 [D loss: 0.051375, acc.: 99.22%] [G loss: 5.278519]\n",
      "epoch:34 step:32138 [D loss: 0.128907, acc.: 99.22%] [G loss: 2.216274]\n",
      "epoch:34 step:32139 [D loss: 0.106395, acc.: 96.88%] [G loss: 2.585607]\n",
      "epoch:34 step:32140 [D loss: 0.120854, acc.: 97.66%] [G loss: 3.243502]\n",
      "epoch:34 step:32141 [D loss: 0.122105, acc.: 97.66%] [G loss: 3.273311]\n",
      "epoch:34 step:32142 [D loss: 0.685965, acc.: 60.94%] [G loss: 2.970296]\n",
      "epoch:34 step:32143 [D loss: 0.124279, acc.: 94.53%] [G loss: 3.557022]\n",
      "epoch:34 step:32144 [D loss: 0.025662, acc.: 100.00%] [G loss: 5.733440]\n",
      "epoch:34 step:32145 [D loss: 0.683916, acc.: 59.38%] [G loss: 2.212105]\n",
      "epoch:34 step:32146 [D loss: 0.158430, acc.: 94.53%] [G loss: 3.494144]\n",
      "epoch:34 step:32147 [D loss: 0.124257, acc.: 96.09%] [G loss: 2.791718]\n",
      "epoch:34 step:32148 [D loss: 0.055937, acc.: 99.22%] [G loss: 3.352319]\n",
      "epoch:34 step:32149 [D loss: 0.064858, acc.: 99.22%] [G loss: 4.677026]\n",
      "epoch:34 step:32150 [D loss: 0.093094, acc.: 97.66%] [G loss: 1.604265]\n",
      "epoch:34 step:32151 [D loss: 0.125377, acc.: 97.66%] [G loss: 3.140436]\n",
      "epoch:34 step:32152 [D loss: 0.232171, acc.: 93.75%] [G loss: 1.905887]\n",
      "epoch:34 step:32153 [D loss: 0.285350, acc.: 93.75%] [G loss: 2.891414]\n",
      "epoch:34 step:32154 [D loss: 0.014136, acc.: 100.00%] [G loss: 2.301653]\n",
      "epoch:34 step:32155 [D loss: 0.575034, acc.: 64.84%] [G loss: 0.944949]\n",
      "epoch:34 step:32156 [D loss: 0.355479, acc.: 82.03%] [G loss: 2.026061]\n",
      "epoch:34 step:32157 [D loss: 2.211299, acc.: 9.38%] [G loss: 2.006415]\n",
      "epoch:34 step:32158 [D loss: 0.250414, acc.: 92.97%] [G loss: 1.625222]\n",
      "epoch:34 step:32159 [D loss: 0.364167, acc.: 84.38%] [G loss: 4.048009]\n",
      "epoch:34 step:32160 [D loss: 0.573163, acc.: 67.19%] [G loss: 2.417940]\n",
      "epoch:34 step:32161 [D loss: 0.175327, acc.: 92.97%] [G loss: 3.793285]\n",
      "epoch:34 step:32162 [D loss: 0.668255, acc.: 64.06%] [G loss: 3.682549]\n",
      "epoch:34 step:32163 [D loss: 0.052452, acc.: 99.22%] [G loss: 1.593297]\n",
      "epoch:34 step:32164 [D loss: 0.525722, acc.: 71.09%] [G loss: 2.224049]\n",
      "epoch:34 step:32165 [D loss: 0.119216, acc.: 99.22%] [G loss: 3.825927]\n",
      "epoch:34 step:32166 [D loss: 0.071760, acc.: 98.44%] [G loss: 9.210608]\n",
      "epoch:34 step:32167 [D loss: 0.035645, acc.: 100.00%] [G loss: 4.762567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32168 [D loss: 0.042733, acc.: 100.00%] [G loss: 2.150955]\n",
      "epoch:34 step:32169 [D loss: 0.703510, acc.: 56.25%] [G loss: 2.101793]\n",
      "epoch:34 step:32170 [D loss: 0.284393, acc.: 89.84%] [G loss: 1.248236]\n",
      "epoch:34 step:32171 [D loss: 0.558827, acc.: 71.88%] [G loss: 1.271186]\n",
      "epoch:34 step:32172 [D loss: 0.053836, acc.: 98.44%] [G loss: 3.079030]\n",
      "epoch:34 step:32173 [D loss: 0.032963, acc.: 100.00%] [G loss: 1.669645]\n",
      "epoch:34 step:32174 [D loss: 0.290538, acc.: 92.97%] [G loss: 4.414044]\n",
      "epoch:34 step:32175 [D loss: 0.530071, acc.: 68.75%] [G loss: 2.355206]\n",
      "epoch:34 step:32176 [D loss: 0.222124, acc.: 94.53%] [G loss: 2.423444]\n",
      "epoch:34 step:32177 [D loss: 0.152308, acc.: 96.09%] [G loss: 5.322766]\n",
      "epoch:34 step:32178 [D loss: 0.677538, acc.: 62.50%] [G loss: 3.672123]\n",
      "epoch:34 step:32179 [D loss: 0.155958, acc.: 97.66%] [G loss: 4.042889]\n",
      "epoch:34 step:32180 [D loss: 0.053023, acc.: 98.44%] [G loss: 3.098226]\n",
      "epoch:34 step:32181 [D loss: 0.073388, acc.: 98.44%] [G loss: 4.834497]\n",
      "epoch:34 step:32182 [D loss: 0.071615, acc.: 99.22%] [G loss: 4.243937]\n",
      "epoch:34 step:32183 [D loss: 0.177239, acc.: 97.66%] [G loss: 2.970387]\n",
      "epoch:34 step:32184 [D loss: 0.138690, acc.: 96.88%] [G loss: 2.190214]\n",
      "epoch:34 step:32185 [D loss: 1.059622, acc.: 51.56%] [G loss: 4.690220]\n",
      "epoch:34 step:32186 [D loss: 0.558235, acc.: 71.88%] [G loss: 2.718835]\n",
      "epoch:34 step:32187 [D loss: 0.136953, acc.: 96.88%] [G loss: 5.762153]\n",
      "epoch:34 step:32188 [D loss: 0.208443, acc.: 92.97%] [G loss: 3.798347]\n",
      "epoch:34 step:32189 [D loss: 0.294645, acc.: 89.06%] [G loss: 6.818316]\n",
      "epoch:34 step:32190 [D loss: 0.064358, acc.: 99.22%] [G loss: 0.147591]\n",
      "epoch:34 step:32191 [D loss: 0.175932, acc.: 98.44%] [G loss: 0.483606]\n",
      "epoch:34 step:32192 [D loss: 0.187999, acc.: 93.75%] [G loss: 1.317129]\n",
      "epoch:34 step:32193 [D loss: 0.341257, acc.: 88.28%] [G loss: 2.210337]\n",
      "epoch:34 step:32194 [D loss: 0.224838, acc.: 89.06%] [G loss: 0.850667]\n",
      "epoch:34 step:32195 [D loss: 0.234669, acc.: 95.31%] [G loss: 1.622611]\n",
      "epoch:34 step:32196 [D loss: 0.216309, acc.: 94.53%] [G loss: 1.442637]\n",
      "epoch:34 step:32197 [D loss: 0.390324, acc.: 80.47%] [G loss: 1.698094]\n",
      "epoch:34 step:32198 [D loss: 0.564856, acc.: 65.62%] [G loss: 2.290397]\n",
      "epoch:34 step:32199 [D loss: 0.531660, acc.: 71.09%] [G loss: 5.791857]\n",
      "epoch:34 step:32200 [D loss: 0.296299, acc.: 89.06%] [G loss: 6.087047]\n",
      "epoch:34 step:32201 [D loss: 0.160211, acc.: 95.31%] [G loss: 3.390992]\n",
      "epoch:34 step:32202 [D loss: 0.152609, acc.: 95.31%] [G loss: 3.708044]\n",
      "epoch:34 step:32203 [D loss: 0.383367, acc.: 75.00%] [G loss: 3.079919]\n",
      "epoch:34 step:32204 [D loss: 0.016482, acc.: 100.00%] [G loss: 0.603591]\n",
      "epoch:34 step:32205 [D loss: 0.296572, acc.: 85.94%] [G loss: 1.876031]\n",
      "epoch:34 step:32206 [D loss: 0.107935, acc.: 96.88%] [G loss: 2.718251]\n",
      "epoch:34 step:32207 [D loss: 0.072081, acc.: 97.66%] [G loss: 4.120119]\n",
      "epoch:34 step:32208 [D loss: 0.139214, acc.: 97.66%] [G loss: 1.715918]\n",
      "epoch:34 step:32209 [D loss: 0.248766, acc.: 91.41%] [G loss: 3.530677]\n",
      "epoch:34 step:32210 [D loss: 0.203365, acc.: 95.31%] [G loss: 2.380970]\n",
      "epoch:34 step:32211 [D loss: 0.014984, acc.: 99.22%] [G loss: 0.906361]\n",
      "epoch:34 step:32212 [D loss: 0.186319, acc.: 94.53%] [G loss: 1.931594]\n",
      "epoch:34 step:32213 [D loss: 0.058836, acc.: 99.22%] [G loss: 5.164024]\n",
      "epoch:34 step:32214 [D loss: 2.277422, acc.: 8.59%] [G loss: 0.492167]\n",
      "epoch:34 step:32215 [D loss: 0.133874, acc.: 96.09%] [G loss: 0.829986]\n",
      "epoch:34 step:32216 [D loss: 0.230809, acc.: 91.41%] [G loss: 4.055220]\n",
      "epoch:34 step:32217 [D loss: 0.467893, acc.: 70.31%] [G loss: 3.503026]\n",
      "epoch:34 step:32218 [D loss: 0.246392, acc.: 92.97%] [G loss: 5.671628]\n",
      "epoch:34 step:32219 [D loss: 0.295972, acc.: 87.50%] [G loss: 2.463390]\n",
      "epoch:34 step:32220 [D loss: 0.212476, acc.: 95.31%] [G loss: 2.048166]\n",
      "epoch:34 step:32221 [D loss: 0.097661, acc.: 97.66%] [G loss: 1.542009]\n",
      "epoch:34 step:32222 [D loss: 0.180590, acc.: 93.75%] [G loss: 1.780916]\n",
      "epoch:34 step:32223 [D loss: 0.172144, acc.: 96.88%] [G loss: 0.959499]\n",
      "epoch:34 step:32224 [D loss: 0.245230, acc.: 89.06%] [G loss: 1.823224]\n",
      "epoch:34 step:32225 [D loss: 0.264113, acc.: 94.53%] [G loss: 4.935577]\n",
      "epoch:34 step:32226 [D loss: 0.293133, acc.: 87.50%] [G loss: 1.578981]\n",
      "epoch:34 step:32227 [D loss: 0.221978, acc.: 92.97%] [G loss: 1.879220]\n",
      "epoch:34 step:32228 [D loss: 0.278190, acc.: 90.62%] [G loss: 2.509700]\n",
      "epoch:34 step:32229 [D loss: 0.133231, acc.: 98.44%] [G loss: 4.772175]\n",
      "epoch:34 step:32230 [D loss: 0.829277, acc.: 60.16%] [G loss: 3.145906]\n",
      "epoch:34 step:32231 [D loss: 0.059348, acc.: 100.00%] [G loss: 1.530021]\n",
      "epoch:34 step:32232 [D loss: 0.354051, acc.: 85.16%] [G loss: 4.650808]\n",
      "epoch:34 step:32233 [D loss: 0.056622, acc.: 99.22%] [G loss: 6.439305]\n",
      "epoch:34 step:32234 [D loss: 0.106214, acc.: 96.09%] [G loss: 3.507162]\n",
      "epoch:34 step:32235 [D loss: 0.319557, acc.: 79.69%] [G loss: 1.405153]\n",
      "epoch:34 step:32236 [D loss: 0.757619, acc.: 61.72%] [G loss: 0.129331]\n",
      "epoch:34 step:32237 [D loss: 0.618293, acc.: 65.62%] [G loss: 3.391561]\n",
      "epoch:34 step:32238 [D loss: 0.496640, acc.: 72.66%] [G loss: 3.983685]\n",
      "epoch:34 step:32239 [D loss: 0.265950, acc.: 89.06%] [G loss: 3.086086]\n",
      "epoch:34 step:32240 [D loss: 0.223388, acc.: 89.84%] [G loss: 4.437837]\n",
      "epoch:34 step:32241 [D loss: 0.355472, acc.: 89.06%] [G loss: 0.120484]\n",
      "epoch:34 step:32242 [D loss: 0.227689, acc.: 92.97%] [G loss: 3.121069]\n",
      "epoch:34 step:32243 [D loss: 0.829255, acc.: 47.66%] [G loss: 3.185523]\n",
      "epoch:34 step:32244 [D loss: 0.113675, acc.: 98.44%] [G loss: 5.531410]\n",
      "epoch:34 step:32245 [D loss: 0.189521, acc.: 97.66%] [G loss: 3.492155]\n",
      "epoch:34 step:32246 [D loss: 0.078758, acc.: 100.00%] [G loss: 0.596535]\n",
      "epoch:34 step:32247 [D loss: 0.092361, acc.: 100.00%] [G loss: 0.742695]\n",
      "epoch:34 step:32248 [D loss: 1.822284, acc.: 50.78%] [G loss: 4.542209]\n",
      "epoch:34 step:32249 [D loss: 0.242728, acc.: 92.97%] [G loss: 4.367121]\n",
      "epoch:34 step:32250 [D loss: 0.660894, acc.: 67.19%] [G loss: 4.194644]\n",
      "epoch:34 step:32251 [D loss: 0.464404, acc.: 78.12%] [G loss: 4.133354]\n",
      "epoch:34 step:32252 [D loss: 0.345478, acc.: 84.38%] [G loss: 1.163920]\n",
      "epoch:34 step:32253 [D loss: 0.318686, acc.: 86.72%] [G loss: 6.341001]\n",
      "epoch:34 step:32254 [D loss: 0.600136, acc.: 64.06%] [G loss: 4.347483]\n",
      "epoch:34 step:32255 [D loss: 1.942357, acc.: 52.34%] [G loss: 4.365086]\n",
      "epoch:34 step:32256 [D loss: 0.198311, acc.: 96.09%] [G loss: 1.458822]\n",
      "epoch:34 step:32257 [D loss: 0.250788, acc.: 89.06%] [G loss: 2.178321]\n",
      "epoch:34 step:32258 [D loss: 0.359521, acc.: 81.25%] [G loss: 2.588704]\n",
      "epoch:34 step:32259 [D loss: 0.426825, acc.: 72.66%] [G loss: 5.224214]\n",
      "epoch:34 step:32260 [D loss: 0.296995, acc.: 92.19%] [G loss: 3.186505]\n",
      "epoch:34 step:32261 [D loss: 0.051938, acc.: 97.66%] [G loss: 1.497769]\n",
      "epoch:34 step:32262 [D loss: 0.049493, acc.: 100.00%] [G loss: 3.155788]\n",
      "epoch:34 step:32263 [D loss: 0.321416, acc.: 89.06%] [G loss: 1.054283]\n",
      "epoch:34 step:32264 [D loss: 0.044656, acc.: 100.00%] [G loss: 3.007148]\n",
      "epoch:34 step:32265 [D loss: 0.052945, acc.: 99.22%] [G loss: 3.262242]\n",
      "epoch:34 step:32266 [D loss: 0.433958, acc.: 71.09%] [G loss: 4.152602]\n",
      "epoch:34 step:32267 [D loss: 0.343366, acc.: 89.06%] [G loss: 4.629803]\n",
      "epoch:34 step:32268 [D loss: 0.180524, acc.: 96.88%] [G loss: 3.862283]\n",
      "epoch:34 step:32269 [D loss: 0.141815, acc.: 97.66%] [G loss: 3.854953]\n",
      "epoch:34 step:32270 [D loss: 0.162633, acc.: 96.88%] [G loss: 3.095030]\n",
      "epoch:34 step:32271 [D loss: 0.475910, acc.: 72.66%] [G loss: 3.355047]\n",
      "epoch:34 step:32272 [D loss: 0.322884, acc.: 82.81%] [G loss: 4.126542]\n",
      "epoch:34 step:32273 [D loss: 0.406859, acc.: 82.81%] [G loss: 2.350127]\n",
      "epoch:34 step:32274 [D loss: 0.173065, acc.: 97.66%] [G loss: 5.213224]\n",
      "epoch:34 step:32275 [D loss: 0.137850, acc.: 96.09%] [G loss: 3.781538]\n",
      "epoch:34 step:32276 [D loss: 0.271438, acc.: 89.84%] [G loss: 4.510852]\n",
      "epoch:34 step:32277 [D loss: 0.181746, acc.: 92.97%] [G loss: 2.030392]\n",
      "epoch:34 step:32278 [D loss: 0.404995, acc.: 77.34%] [G loss: 4.196959]\n",
      "epoch:34 step:32279 [D loss: 0.060954, acc.: 100.00%] [G loss: 1.131778]\n",
      "epoch:34 step:32280 [D loss: 0.038211, acc.: 100.00%] [G loss: 4.425349]\n",
      "epoch:34 step:32281 [D loss: 1.299510, acc.: 36.72%] [G loss: 4.570983]\n",
      "epoch:34 step:32282 [D loss: 0.431119, acc.: 78.91%] [G loss: 6.313209]\n",
      "epoch:34 step:32283 [D loss: 0.129057, acc.: 96.88%] [G loss: 3.164245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32284 [D loss: 0.055884, acc.: 100.00%] [G loss: 4.972844]\n",
      "epoch:34 step:32285 [D loss: 0.056531, acc.: 99.22%] [G loss: 4.848322]\n",
      "epoch:34 step:32286 [D loss: 0.222696, acc.: 92.19%] [G loss: 3.039738]\n",
      "epoch:34 step:32287 [D loss: 0.465820, acc.: 81.25%] [G loss: 1.486339]\n",
      "epoch:34 step:32288 [D loss: 0.025789, acc.: 99.22%] [G loss: 1.567474]\n",
      "epoch:34 step:32289 [D loss: 0.392792, acc.: 79.69%] [G loss: 6.051295]\n",
      "epoch:34 step:32290 [D loss: 0.096329, acc.: 99.22%] [G loss: 2.427894]\n",
      "epoch:34 step:32291 [D loss: 0.216669, acc.: 91.41%] [G loss: 6.465066]\n",
      "epoch:34 step:32292 [D loss: 0.248479, acc.: 92.97%] [G loss: 0.387975]\n",
      "epoch:34 step:32293 [D loss: 0.094280, acc.: 97.66%] [G loss: 3.531158]\n",
      "epoch:34 step:32294 [D loss: 0.278607, acc.: 89.06%] [G loss: 1.467952]\n",
      "epoch:34 step:32295 [D loss: 0.124077, acc.: 97.66%] [G loss: 3.675599]\n",
      "epoch:34 step:32296 [D loss: 0.042214, acc.: 100.00%] [G loss: 6.327722]\n",
      "epoch:34 step:32297 [D loss: 0.102759, acc.: 99.22%] [G loss: 3.850182]\n",
      "epoch:34 step:32298 [D loss: 0.041761, acc.: 100.00%] [G loss: 1.888190]\n",
      "epoch:34 step:32299 [D loss: 0.196968, acc.: 92.19%] [G loss: 1.565863]\n",
      "epoch:34 step:32300 [D loss: 0.425189, acc.: 84.38%] [G loss: 1.945793]\n",
      "epoch:34 step:32301 [D loss: 0.318074, acc.: 83.59%] [G loss: 4.123365]\n",
      "epoch:34 step:32302 [D loss: 0.436814, acc.: 71.88%] [G loss: 1.832379]\n",
      "epoch:34 step:32303 [D loss: 0.692651, acc.: 62.50%] [G loss: 5.031351]\n",
      "epoch:34 step:32304 [D loss: 0.353569, acc.: 80.47%] [G loss: 3.521841]\n",
      "epoch:34 step:32305 [D loss: 0.278696, acc.: 92.97%] [G loss: 2.414551]\n",
      "epoch:34 step:32306 [D loss: 0.117112, acc.: 98.44%] [G loss: 2.214795]\n",
      "epoch:34 step:32307 [D loss: 0.055766, acc.: 97.66%] [G loss: 2.319070]\n",
      "epoch:34 step:32308 [D loss: 0.129298, acc.: 98.44%] [G loss: 2.140175]\n",
      "epoch:34 step:32309 [D loss: 0.395877, acc.: 79.69%] [G loss: 2.101674]\n",
      "epoch:34 step:32310 [D loss: 0.744041, acc.: 59.38%] [G loss: 5.169168]\n",
      "epoch:34 step:32311 [D loss: 0.949592, acc.: 59.38%] [G loss: 2.990155]\n",
      "epoch:34 step:32312 [D loss: 0.685605, acc.: 65.62%] [G loss: 1.699331]\n",
      "epoch:34 step:32313 [D loss: 0.372685, acc.: 78.91%] [G loss: 2.381099]\n",
      "epoch:34 step:32314 [D loss: 0.167297, acc.: 97.66%] [G loss: 3.259851]\n",
      "epoch:34 step:32315 [D loss: 0.896058, acc.: 51.56%] [G loss: 0.850593]\n",
      "epoch:34 step:32316 [D loss: 0.170184, acc.: 93.75%] [G loss: 4.797473]\n",
      "epoch:34 step:32317 [D loss: 1.766072, acc.: 46.09%] [G loss: 1.243209]\n",
      "epoch:34 step:32318 [D loss: 0.986880, acc.: 50.00%] [G loss: 1.455765]\n",
      "epoch:34 step:32319 [D loss: 0.074639, acc.: 99.22%] [G loss: 4.165713]\n",
      "epoch:34 step:32320 [D loss: 0.097128, acc.: 98.44%] [G loss: 2.101438]\n",
      "epoch:34 step:32321 [D loss: 0.098027, acc.: 97.66%] [G loss: 3.587849]\n",
      "epoch:34 step:32322 [D loss: 0.256781, acc.: 92.19%] [G loss: 2.272729]\n",
      "epoch:34 step:32323 [D loss: 1.315929, acc.: 51.56%] [G loss: 4.844305]\n",
      "epoch:34 step:32324 [D loss: 0.052781, acc.: 100.00%] [G loss: 5.149746]\n",
      "epoch:34 step:32325 [D loss: 0.666982, acc.: 59.38%] [G loss: 1.142650]\n",
      "epoch:34 step:32326 [D loss: 0.555607, acc.: 67.19%] [G loss: 2.924720]\n",
      "epoch:34 step:32327 [D loss: 0.040232, acc.: 99.22%] [G loss: 3.607727]\n",
      "epoch:34 step:32328 [D loss: 0.455507, acc.: 81.25%] [G loss: 1.283029]\n",
      "epoch:34 step:32329 [D loss: 0.091228, acc.: 98.44%] [G loss: 1.849457]\n",
      "epoch:34 step:32330 [D loss: 0.163594, acc.: 96.09%] [G loss: 1.034811]\n",
      "epoch:34 step:32331 [D loss: 0.226298, acc.: 92.97%] [G loss: 0.631278]\n",
      "epoch:34 step:32332 [D loss: 0.118916, acc.: 98.44%] [G loss: 5.478291]\n",
      "epoch:34 step:32333 [D loss: 0.191872, acc.: 98.44%] [G loss: 1.307015]\n",
      "epoch:34 step:32334 [D loss: 0.231323, acc.: 95.31%] [G loss: 1.471615]\n",
      "epoch:34 step:32335 [D loss: 0.031117, acc.: 100.00%] [G loss: 2.341698]\n",
      "epoch:34 step:32336 [D loss: 0.056893, acc.: 100.00%] [G loss: 4.260302]\n",
      "epoch:34 step:32337 [D loss: 0.174874, acc.: 94.53%] [G loss: 2.526914]\n",
      "epoch:34 step:32338 [D loss: 0.155765, acc.: 99.22%] [G loss: 2.813534]\n",
      "epoch:34 step:32339 [D loss: 0.289646, acc.: 94.53%] [G loss: 2.425945]\n",
      "epoch:34 step:32340 [D loss: 0.482220, acc.: 71.09%] [G loss: 4.396626]\n",
      "epoch:34 step:32341 [D loss: 0.428255, acc.: 81.25%] [G loss: 3.210994]\n",
      "epoch:34 step:32342 [D loss: 1.002748, acc.: 47.66%] [G loss: 3.247791]\n",
      "epoch:34 step:32343 [D loss: 0.132353, acc.: 96.09%] [G loss: 3.910517]\n",
      "epoch:34 step:32344 [D loss: 0.393499, acc.: 85.16%] [G loss: 2.755502]\n",
      "epoch:34 step:32345 [D loss: 0.218197, acc.: 93.75%] [G loss: 2.967294]\n",
      "epoch:34 step:32346 [D loss: 0.241127, acc.: 95.31%] [G loss: 1.414001]\n",
      "epoch:34 step:32347 [D loss: 0.104514, acc.: 97.66%] [G loss: 3.036143]\n",
      "epoch:34 step:32348 [D loss: 0.273180, acc.: 90.62%] [G loss: 1.603634]\n",
      "epoch:34 step:32349 [D loss: 0.395030, acc.: 80.47%] [G loss: 2.749886]\n",
      "epoch:34 step:32350 [D loss: 0.305299, acc.: 90.62%] [G loss: 1.529781]\n",
      "epoch:34 step:32351 [D loss: 0.896365, acc.: 46.88%] [G loss: 3.461272]\n",
      "epoch:34 step:32352 [D loss: 0.748835, acc.: 56.25%] [G loss: 2.805211]\n",
      "epoch:34 step:32353 [D loss: 0.101106, acc.: 96.88%] [G loss: 0.808616]\n",
      "epoch:34 step:32354 [D loss: 0.169219, acc.: 96.09%] [G loss: 1.921402]\n",
      "epoch:34 step:32355 [D loss: 1.152176, acc.: 47.66%] [G loss: 3.241671]\n",
      "epoch:34 step:32356 [D loss: 0.068113, acc.: 98.44%] [G loss: 0.825871]\n",
      "epoch:34 step:32357 [D loss: 0.354151, acc.: 86.72%] [G loss: 1.665448]\n",
      "epoch:34 step:32358 [D loss: 0.107377, acc.: 96.88%] [G loss: 4.562142]\n",
      "epoch:34 step:32359 [D loss: 1.235312, acc.: 53.12%] [G loss: 1.856243]\n",
      "epoch:34 step:32360 [D loss: 0.150231, acc.: 94.53%] [G loss: 1.572577]\n",
      "epoch:34 step:32361 [D loss: 0.066484, acc.: 99.22%] [G loss: 2.919736]\n",
      "epoch:34 step:32362 [D loss: 0.110520, acc.: 98.44%] [G loss: 0.871278]\n",
      "epoch:34 step:32363 [D loss: 0.231602, acc.: 95.31%] [G loss: 2.603504]\n",
      "epoch:34 step:32364 [D loss: 0.430463, acc.: 79.69%] [G loss: 1.134003]\n",
      "epoch:34 step:32365 [D loss: 1.374970, acc.: 34.38%] [G loss: 1.730353]\n",
      "epoch:34 step:32366 [D loss: 0.302290, acc.: 80.47%] [G loss: 0.950836]\n",
      "epoch:34 step:32367 [D loss: 0.272391, acc.: 93.75%] [G loss: 2.206217]\n",
      "epoch:34 step:32368 [D loss: 0.114824, acc.: 98.44%] [G loss: 3.796958]\n",
      "epoch:34 step:32369 [D loss: 0.039909, acc.: 99.22%] [G loss: 1.563503]\n",
      "epoch:34 step:32370 [D loss: 0.071332, acc.: 99.22%] [G loss: 4.132992]\n",
      "epoch:34 step:32371 [D loss: 0.267612, acc.: 94.53%] [G loss: 3.263713]\n",
      "epoch:34 step:32372 [D loss: 0.493116, acc.: 80.47%] [G loss: 0.203242]\n",
      "epoch:34 step:32373 [D loss: 0.163748, acc.: 97.66%] [G loss: 0.542944]\n",
      "epoch:34 step:32374 [D loss: 0.187311, acc.: 95.31%] [G loss: 2.527164]\n",
      "epoch:34 step:32375 [D loss: 0.100707, acc.: 98.44%] [G loss: 2.474205]\n",
      "epoch:34 step:32376 [D loss: 0.046126, acc.: 100.00%] [G loss: 2.150024]\n",
      "epoch:34 step:32377 [D loss: 0.298142, acc.: 89.06%] [G loss: 1.521502]\n",
      "epoch:34 step:32378 [D loss: 0.264908, acc.: 90.62%] [G loss: 2.393038]\n",
      "epoch:34 step:32379 [D loss: 0.074016, acc.: 100.00%] [G loss: 2.167822]\n",
      "epoch:34 step:32380 [D loss: 0.656951, acc.: 60.16%] [G loss: 2.912811]\n",
      "epoch:34 step:32381 [D loss: 0.337721, acc.: 80.47%] [G loss: 6.087137]\n",
      "epoch:34 step:32382 [D loss: 0.321777, acc.: 82.81%] [G loss: 4.717785]\n",
      "epoch:34 step:32383 [D loss: 0.117556, acc.: 98.44%] [G loss: 1.448989]\n",
      "epoch:34 step:32384 [D loss: 0.464283, acc.: 74.22%] [G loss: 4.559723]\n",
      "epoch:34 step:32385 [D loss: 0.071843, acc.: 100.00%] [G loss: 1.123424]\n",
      "epoch:34 step:32386 [D loss: 0.218269, acc.: 92.97%] [G loss: 1.282266]\n",
      "epoch:34 step:32387 [D loss: 0.035489, acc.: 100.00%] [G loss: 4.423068]\n",
      "epoch:34 step:32388 [D loss: 0.265771, acc.: 88.28%] [G loss: 1.369209]\n",
      "epoch:34 step:32389 [D loss: 0.032092, acc.: 99.22%] [G loss: 4.109671]\n",
      "epoch:34 step:32390 [D loss: 1.068604, acc.: 33.59%] [G loss: 1.817771]\n",
      "epoch:34 step:32391 [D loss: 0.202396, acc.: 97.66%] [G loss: 1.903220]\n",
      "epoch:34 step:32392 [D loss: 0.493922, acc.: 75.78%] [G loss: 2.754378]\n",
      "epoch:34 step:32393 [D loss: 0.198337, acc.: 92.19%] [G loss: 2.932831]\n",
      "epoch:34 step:32394 [D loss: 0.380344, acc.: 76.56%] [G loss: 3.813221]\n",
      "epoch:34 step:32395 [D loss: 0.882986, acc.: 46.09%] [G loss: 2.445897]\n",
      "epoch:34 step:32396 [D loss: 0.316060, acc.: 91.41%] [G loss: 5.312562]\n",
      "epoch:34 step:32397 [D loss: 0.185915, acc.: 98.44%] [G loss: 5.706005]\n",
      "epoch:34 step:32398 [D loss: 1.313562, acc.: 51.56%] [G loss: 2.184431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32399 [D loss: 0.131690, acc.: 98.44%] [G loss: 1.196500]\n",
      "epoch:34 step:32400 [D loss: 0.129056, acc.: 98.44%] [G loss: 3.251667]\n",
      "epoch:34 step:32401 [D loss: 0.428285, acc.: 73.44%] [G loss: 3.397941]\n",
      "epoch:34 step:32402 [D loss: 0.137109, acc.: 98.44%] [G loss: 4.448590]\n",
      "epoch:34 step:32403 [D loss: 0.337770, acc.: 80.47%] [G loss: 5.527244]\n",
      "epoch:34 step:32404 [D loss: 0.299025, acc.: 89.06%] [G loss: 3.518652]\n",
      "epoch:34 step:32405 [D loss: 0.233641, acc.: 92.97%] [G loss: 1.879268]\n",
      "epoch:34 step:32406 [D loss: 0.162675, acc.: 97.66%] [G loss: 4.436808]\n",
      "epoch:34 step:32407 [D loss: 0.379665, acc.: 90.62%] [G loss: 3.799922]\n",
      "epoch:34 step:32408 [D loss: 0.151555, acc.: 97.66%] [G loss: 4.576002]\n",
      "epoch:34 step:32409 [D loss: 0.153345, acc.: 99.22%] [G loss: 1.811809]\n",
      "epoch:34 step:32410 [D loss: 0.204185, acc.: 95.31%] [G loss: 1.647503]\n",
      "epoch:34 step:32411 [D loss: 0.586800, acc.: 66.41%] [G loss: 5.881605]\n",
      "epoch:34 step:32412 [D loss: 0.604442, acc.: 67.19%] [G loss: 1.279385]\n",
      "epoch:34 step:32413 [D loss: 0.124971, acc.: 100.00%] [G loss: 0.789797]\n",
      "epoch:34 step:32414 [D loss: 0.106109, acc.: 99.22%] [G loss: 3.958412]\n",
      "epoch:34 step:32415 [D loss: 0.299284, acc.: 86.72%] [G loss: 3.073949]\n",
      "epoch:34 step:32416 [D loss: 0.040885, acc.: 99.22%] [G loss: 0.892318]\n",
      "epoch:34 step:32417 [D loss: 0.419457, acc.: 73.44%] [G loss: 3.039171]\n",
      "epoch:34 step:32418 [D loss: 0.141830, acc.: 97.66%] [G loss: 1.671258]\n",
      "epoch:34 step:32419 [D loss: 0.083094, acc.: 100.00%] [G loss: 1.950987]\n",
      "epoch:34 step:32420 [D loss: 0.101951, acc.: 98.44%] [G loss: 1.001081]\n",
      "epoch:34 step:32421 [D loss: 0.041879, acc.: 99.22%] [G loss: 1.153468]\n",
      "epoch:34 step:32422 [D loss: 0.514009, acc.: 68.75%] [G loss: 1.853370]\n",
      "epoch:34 step:32423 [D loss: 0.087305, acc.: 99.22%] [G loss: 3.447532]\n",
      "epoch:34 step:32424 [D loss: 0.097415, acc.: 97.66%] [G loss: 2.867218]\n",
      "epoch:34 step:32425 [D loss: 0.451125, acc.: 78.91%] [G loss: 4.867018]\n",
      "epoch:34 step:32426 [D loss: 0.096337, acc.: 98.44%] [G loss: 4.933864]\n",
      "epoch:34 step:32427 [D loss: 0.318977, acc.: 92.19%] [G loss: 3.381545]\n",
      "epoch:34 step:32428 [D loss: 1.300438, acc.: 46.88%] [G loss: 2.645714]\n",
      "epoch:34 step:32429 [D loss: 0.168094, acc.: 99.22%] [G loss: 2.857196]\n",
      "epoch:34 step:32430 [D loss: 0.238675, acc.: 89.84%] [G loss: 1.451821]\n",
      "epoch:34 step:32431 [D loss: 0.153856, acc.: 96.09%] [G loss: 5.739162]\n",
      "epoch:34 step:32432 [D loss: 0.097666, acc.: 97.66%] [G loss: 1.064416]\n",
      "epoch:34 step:32433 [D loss: 0.030365, acc.: 99.22%] [G loss: 2.034529]\n",
      "epoch:34 step:32434 [D loss: 0.421397, acc.: 80.47%] [G loss: 0.960764]\n",
      "epoch:34 step:32435 [D loss: 0.142238, acc.: 95.31%] [G loss: 1.882647]\n",
      "epoch:34 step:32436 [D loss: 0.510221, acc.: 73.44%] [G loss: 4.444792]\n",
      "epoch:34 step:32437 [D loss: 0.238066, acc.: 92.19%] [G loss: 0.711730]\n",
      "epoch:34 step:32438 [D loss: 0.128532, acc.: 96.88%] [G loss: 0.773211]\n",
      "epoch:34 step:32439 [D loss: 0.597452, acc.: 67.19%] [G loss: 1.181093]\n",
      "epoch:34 step:32440 [D loss: 0.111912, acc.: 97.66%] [G loss: 1.897900]\n",
      "epoch:34 step:32441 [D loss: 0.570429, acc.: 64.84%] [G loss: 3.807003]\n",
      "epoch:34 step:32442 [D loss: 1.952061, acc.: 9.38%] [G loss: 1.702510]\n",
      "epoch:34 step:32443 [D loss: 0.076522, acc.: 99.22%] [G loss: 3.038507]\n",
      "epoch:34 step:32444 [D loss: 0.107064, acc.: 98.44%] [G loss: 2.880840]\n",
      "epoch:34 step:32445 [D loss: 0.128301, acc.: 98.44%] [G loss: 2.107957]\n",
      "epoch:34 step:32446 [D loss: 0.444406, acc.: 87.50%] [G loss: 2.989343]\n",
      "epoch:34 step:32447 [D loss: 0.628145, acc.: 64.84%] [G loss: 3.555782]\n",
      "epoch:34 step:32448 [D loss: 0.907321, acc.: 58.59%] [G loss: 0.776844]\n",
      "epoch:34 step:32449 [D loss: 0.632651, acc.: 70.31%] [G loss: 3.259626]\n",
      "epoch:34 step:32450 [D loss: 0.156900, acc.: 96.09%] [G loss: 1.784752]\n",
      "epoch:34 step:32451 [D loss: 0.039580, acc.: 100.00%] [G loss: 1.505530]\n",
      "epoch:34 step:32452 [D loss: 0.113907, acc.: 99.22%] [G loss: 2.809554]\n",
      "epoch:34 step:32453 [D loss: 0.372878, acc.: 84.38%] [G loss: 1.848331]\n",
      "epoch:34 step:32454 [D loss: 0.110763, acc.: 99.22%] [G loss: 2.935699]\n",
      "epoch:34 step:32455 [D loss: 0.681424, acc.: 62.50%] [G loss: 0.727522]\n",
      "epoch:34 step:32456 [D loss: 0.137335, acc.: 98.44%] [G loss: 2.848013]\n",
      "epoch:34 step:32457 [D loss: 0.286427, acc.: 84.38%] [G loss: 0.451651]\n",
      "epoch:34 step:32458 [D loss: 0.117113, acc.: 96.88%] [G loss: 4.091608]\n",
      "epoch:34 step:32459 [D loss: 0.374280, acc.: 78.91%] [G loss: 2.357476]\n",
      "epoch:34 step:32460 [D loss: 0.022640, acc.: 100.00%] [G loss: 2.563876]\n",
      "epoch:34 step:32461 [D loss: 0.062228, acc.: 99.22%] [G loss: 3.187236]\n",
      "epoch:34 step:32462 [D loss: 0.233956, acc.: 89.84%] [G loss: 0.828530]\n",
      "epoch:34 step:32463 [D loss: 0.132454, acc.: 96.88%] [G loss: 1.955705]\n",
      "epoch:34 step:32464 [D loss: 1.037816, acc.: 51.56%] [G loss: 0.836158]\n",
      "epoch:34 step:32465 [D loss: 0.077845, acc.: 98.44%] [G loss: 4.015635]\n",
      "epoch:34 step:32466 [D loss: 1.091029, acc.: 53.12%] [G loss: 5.042316]\n",
      "epoch:34 step:32467 [D loss: 1.617139, acc.: 24.22%] [G loss: 3.176555]\n",
      "epoch:34 step:32468 [D loss: 0.191461, acc.: 98.44%] [G loss: 0.358325]\n",
      "epoch:34 step:32469 [D loss: 0.164347, acc.: 96.09%] [G loss: 1.646919]\n",
      "epoch:34 step:32470 [D loss: 0.486127, acc.: 74.22%] [G loss: 2.787945]\n",
      "epoch:34 step:32471 [D loss: 1.073089, acc.: 55.47%] [G loss: 4.989276]\n",
      "epoch:34 step:32472 [D loss: 0.087470, acc.: 99.22%] [G loss: 1.241668]\n",
      "epoch:34 step:32473 [D loss: 0.269788, acc.: 89.84%] [G loss: 3.160283]\n",
      "epoch:34 step:32474 [D loss: 0.244064, acc.: 92.97%] [G loss: 4.090882]\n",
      "epoch:34 step:32475 [D loss: 0.568600, acc.: 68.75%] [G loss: 3.801490]\n",
      "epoch:34 step:32476 [D loss: 0.565201, acc.: 71.09%] [G loss: 2.013127]\n",
      "epoch:34 step:32477 [D loss: 0.449538, acc.: 82.81%] [G loss: 1.182578]\n",
      "epoch:34 step:32478 [D loss: 0.233570, acc.: 93.75%] [G loss: 1.364621]\n",
      "epoch:34 step:32479 [D loss: 0.272669, acc.: 90.62%] [G loss: 0.390465]\n",
      "epoch:34 step:32480 [D loss: 0.414946, acc.: 77.34%] [G loss: 4.191653]\n",
      "epoch:34 step:32481 [D loss: 0.374591, acc.: 88.28%] [G loss: 3.769316]\n",
      "epoch:34 step:32482 [D loss: 1.221981, acc.: 57.03%] [G loss: 1.271937]\n",
      "epoch:34 step:32483 [D loss: 1.073585, acc.: 55.47%] [G loss: 1.615106]\n",
      "epoch:34 step:32484 [D loss: 0.200374, acc.: 94.53%] [G loss: 2.934225]\n",
      "epoch:34 step:32485 [D loss: 0.310632, acc.: 85.94%] [G loss: 2.126976]\n",
      "epoch:34 step:32486 [D loss: 0.164946, acc.: 94.53%] [G loss: 4.064746]\n",
      "epoch:34 step:32487 [D loss: 0.129150, acc.: 97.66%] [G loss: 5.896952]\n",
      "epoch:34 step:32488 [D loss: 0.582698, acc.: 71.88%] [G loss: 4.186620]\n",
      "epoch:34 step:32489 [D loss: 0.064444, acc.: 99.22%] [G loss: 3.621594]\n",
      "epoch:34 step:32490 [D loss: 0.285126, acc.: 86.72%] [G loss: 5.099609]\n",
      "epoch:34 step:32491 [D loss: 0.421607, acc.: 81.25%] [G loss: 2.908262]\n",
      "epoch:34 step:32492 [D loss: 0.313262, acc.: 86.72%] [G loss: 2.214568]\n",
      "epoch:34 step:32493 [D loss: 0.077650, acc.: 100.00%] [G loss: 1.653255]\n",
      "epoch:34 step:32494 [D loss: 0.477168, acc.: 72.66%] [G loss: 2.208115]\n",
      "epoch:34 step:32495 [D loss: 0.098514, acc.: 99.22%] [G loss: 2.429726]\n",
      "epoch:34 step:32496 [D loss: 0.095114, acc.: 96.88%] [G loss: 2.892117]\n",
      "epoch:34 step:32497 [D loss: 0.071643, acc.: 100.00%] [G loss: 2.180457]\n",
      "epoch:34 step:32498 [D loss: 1.560858, acc.: 50.00%] [G loss: 2.833977]\n",
      "epoch:34 step:32499 [D loss: 0.656619, acc.: 71.88%] [G loss: 0.474303]\n",
      "epoch:34 step:32500 [D loss: 0.100989, acc.: 98.44%] [G loss: 1.026092]\n",
      "epoch:34 step:32501 [D loss: 0.086014, acc.: 96.88%] [G loss: 0.341745]\n",
      "epoch:34 step:32502 [D loss: 0.066619, acc.: 99.22%] [G loss: 1.244240]\n",
      "epoch:34 step:32503 [D loss: 0.980107, acc.: 50.00%] [G loss: 1.002284]\n",
      "epoch:34 step:32504 [D loss: 0.079241, acc.: 96.88%] [G loss: 5.825164]\n",
      "epoch:34 step:32505 [D loss: 1.499485, acc.: 17.97%] [G loss: 4.704324]\n",
      "epoch:34 step:32506 [D loss: 0.439278, acc.: 80.47%] [G loss: 2.126095]\n",
      "epoch:34 step:32507 [D loss: 0.327424, acc.: 92.19%] [G loss: 3.540153]\n",
      "epoch:34 step:32508 [D loss: 0.422607, acc.: 77.34%] [G loss: 2.096405]\n",
      "epoch:34 step:32509 [D loss: 0.365932, acc.: 87.50%] [G loss: 4.077787]\n",
      "epoch:34 step:32510 [D loss: 0.260529, acc.: 93.75%] [G loss: 3.036375]\n",
      "epoch:34 step:32511 [D loss: 0.234991, acc.: 90.62%] [G loss: 3.885705]\n",
      "epoch:34 step:32512 [D loss: 0.105097, acc.: 99.22%] [G loss: 5.161503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32513 [D loss: 0.362059, acc.: 79.69%] [G loss: 1.948212]\n",
      "epoch:34 step:32514 [D loss: 0.245735, acc.: 92.97%] [G loss: 3.859712]\n",
      "epoch:34 step:32515 [D loss: 1.048374, acc.: 55.47%] [G loss: 2.187745]\n",
      "epoch:34 step:32516 [D loss: 0.222948, acc.: 95.31%] [G loss: 2.658977]\n",
      "epoch:34 step:32517 [D loss: 0.041090, acc.: 99.22%] [G loss: 2.540930]\n",
      "epoch:34 step:32518 [D loss: 0.228650, acc.: 92.19%] [G loss: 2.498529]\n",
      "epoch:34 step:32519 [D loss: 0.283138, acc.: 94.53%] [G loss: 3.303144]\n",
      "epoch:34 step:32520 [D loss: 0.566588, acc.: 69.53%] [G loss: 2.552344]\n",
      "epoch:34 step:32521 [D loss: 0.171640, acc.: 97.66%] [G loss: 2.937716]\n",
      "epoch:34 step:32522 [D loss: 0.112723, acc.: 98.44%] [G loss: 3.432296]\n",
      "epoch:34 step:32523 [D loss: 0.115362, acc.: 98.44%] [G loss: 2.317091]\n",
      "epoch:34 step:32524 [D loss: 0.345449, acc.: 89.84%] [G loss: 4.069513]\n",
      "epoch:34 step:32525 [D loss: 0.346496, acc.: 78.12%] [G loss: 2.520779]\n",
      "epoch:34 step:32526 [D loss: 1.311989, acc.: 33.59%] [G loss: 2.683502]\n",
      "epoch:34 step:32527 [D loss: 0.091724, acc.: 98.44%] [G loss: 2.295098]\n",
      "epoch:34 step:32528 [D loss: 0.032600, acc.: 100.00%] [G loss: 3.893225]\n",
      "epoch:34 step:32529 [D loss: 0.756594, acc.: 57.03%] [G loss: 2.866842]\n",
      "epoch:34 step:32530 [D loss: 0.546070, acc.: 73.44%] [G loss: 3.351341]\n",
      "epoch:34 step:32531 [D loss: 0.510136, acc.: 69.53%] [G loss: 3.819734]\n",
      "epoch:34 step:32532 [D loss: 0.148461, acc.: 96.09%] [G loss: 2.405237]\n",
      "epoch:34 step:32533 [D loss: 0.282167, acc.: 89.06%] [G loss: 4.639922]\n",
      "epoch:34 step:32534 [D loss: 0.080961, acc.: 99.22%] [G loss: 2.055451]\n",
      "epoch:34 step:32535 [D loss: 0.304888, acc.: 89.84%] [G loss: 1.698334]\n",
      "epoch:34 step:32536 [D loss: 0.424302, acc.: 71.88%] [G loss: 3.043536]\n",
      "epoch:34 step:32537 [D loss: 0.177081, acc.: 92.97%] [G loss: 1.108728]\n",
      "epoch:34 step:32538 [D loss: 0.218552, acc.: 96.09%] [G loss: 2.876220]\n",
      "epoch:34 step:32539 [D loss: 0.214607, acc.: 92.97%] [G loss: 1.423867]\n",
      "epoch:34 step:32540 [D loss: 0.060543, acc.: 100.00%] [G loss: 1.752718]\n",
      "epoch:34 step:32541 [D loss: 0.958282, acc.: 51.56%] [G loss: 0.373005]\n",
      "epoch:34 step:32542 [D loss: 0.194930, acc.: 93.75%] [G loss: 4.137940]\n",
      "epoch:34 step:32543 [D loss: 0.179189, acc.: 96.88%] [G loss: 1.931736]\n",
      "epoch:34 step:32544 [D loss: 0.182481, acc.: 96.88%] [G loss: 1.105344]\n",
      "epoch:34 step:32545 [D loss: 0.174038, acc.: 95.31%] [G loss: 2.179282]\n",
      "epoch:34 step:32546 [D loss: 0.500915, acc.: 74.22%] [G loss: 1.912862]\n",
      "epoch:34 step:32547 [D loss: 0.649221, acc.: 62.50%] [G loss: 2.369526]\n",
      "epoch:34 step:32548 [D loss: 0.078839, acc.: 99.22%] [G loss: 0.332778]\n",
      "epoch:34 step:32549 [D loss: 0.407645, acc.: 73.44%] [G loss: 1.127652]\n",
      "epoch:34 step:32550 [D loss: 0.551517, acc.: 71.09%] [G loss: 2.736328]\n",
      "epoch:34 step:32551 [D loss: 0.586975, acc.: 70.31%] [G loss: 3.230923]\n",
      "epoch:34 step:32552 [D loss: 0.076573, acc.: 99.22%] [G loss: 3.966605]\n",
      "epoch:34 step:32553 [D loss: 0.150393, acc.: 98.44%] [G loss: 2.616860]\n",
      "epoch:34 step:32554 [D loss: 0.695098, acc.: 59.38%] [G loss: 3.754957]\n",
      "epoch:34 step:32555 [D loss: 0.124826, acc.: 100.00%] [G loss: 3.221192]\n",
      "epoch:34 step:32556 [D loss: 0.115503, acc.: 96.88%] [G loss: 0.320203]\n",
      "epoch:34 step:32557 [D loss: 0.031688, acc.: 99.22%] [G loss: 4.770249]\n",
      "epoch:34 step:32558 [D loss: 0.207722, acc.: 94.53%] [G loss: 2.790280]\n",
      "epoch:34 step:32559 [D loss: 0.325950, acc.: 86.72%] [G loss: 4.186063]\n",
      "epoch:34 step:32560 [D loss: 0.557963, acc.: 75.78%] [G loss: 3.825111]\n",
      "epoch:34 step:32561 [D loss: 0.188367, acc.: 94.53%] [G loss: 4.139500]\n",
      "epoch:34 step:32562 [D loss: 0.381893, acc.: 80.47%] [G loss: 0.574106]\n",
      "epoch:34 step:32563 [D loss: 0.627960, acc.: 71.88%] [G loss: 2.547103]\n",
      "epoch:34 step:32564 [D loss: 0.204269, acc.: 92.97%] [G loss: 3.308614]\n",
      "epoch:34 step:32565 [D loss: 0.043894, acc.: 100.00%] [G loss: 3.916517]\n",
      "epoch:34 step:32566 [D loss: 0.177705, acc.: 93.75%] [G loss: 2.900408]\n",
      "epoch:34 step:32567 [D loss: 1.477976, acc.: 21.09%] [G loss: 0.645601]\n",
      "epoch:34 step:32568 [D loss: 0.083301, acc.: 98.44%] [G loss: 1.723203]\n",
      "epoch:34 step:32569 [D loss: 0.681580, acc.: 64.84%] [G loss: 5.253066]\n",
      "epoch:34 step:32570 [D loss: 0.270280, acc.: 86.72%] [G loss: 3.770980]\n",
      "epoch:34 step:32571 [D loss: 0.052581, acc.: 99.22%] [G loss: 3.404182]\n",
      "epoch:34 step:32572 [D loss: 0.283738, acc.: 89.06%] [G loss: 6.860199]\n",
      "epoch:34 step:32573 [D loss: 0.166660, acc.: 92.97%] [G loss: 4.571582]\n",
      "epoch:34 step:32574 [D loss: 0.847516, acc.: 52.34%] [G loss: 2.509068]\n",
      "epoch:34 step:32575 [D loss: 0.256996, acc.: 95.31%] [G loss: 1.222541]\n",
      "epoch:34 step:32576 [D loss: 0.171762, acc.: 93.75%] [G loss: 2.159078]\n",
      "epoch:34 step:32577 [D loss: 0.431779, acc.: 73.44%] [G loss: 3.245876]\n",
      "epoch:34 step:32578 [D loss: 0.296614, acc.: 92.97%] [G loss: 2.554211]\n",
      "epoch:34 step:32579 [D loss: 0.519287, acc.: 75.78%] [G loss: 1.368598]\n",
      "epoch:34 step:32580 [D loss: 0.240872, acc.: 90.62%] [G loss: 2.684228]\n",
      "epoch:34 step:32581 [D loss: 0.127924, acc.: 97.66%] [G loss: 2.842894]\n",
      "epoch:34 step:32582 [D loss: 0.219289, acc.: 97.66%] [G loss: 3.097643]\n",
      "epoch:34 step:32583 [D loss: 0.206496, acc.: 92.97%] [G loss: 3.515071]\n",
      "epoch:34 step:32584 [D loss: 0.401939, acc.: 85.16%] [G loss: 3.324528]\n",
      "epoch:34 step:32585 [D loss: 0.132130, acc.: 98.44%] [G loss: 4.066161]\n",
      "epoch:34 step:32586 [D loss: 0.080746, acc.: 99.22%] [G loss: 4.834702]\n",
      "epoch:34 step:32587 [D loss: 0.011303, acc.: 100.00%] [G loss: 0.888445]\n",
      "epoch:34 step:32588 [D loss: 0.312503, acc.: 82.81%] [G loss: 2.949205]\n",
      "epoch:34 step:32589 [D loss: 0.176417, acc.: 93.75%] [G loss: 3.750736]\n",
      "epoch:34 step:32590 [D loss: 0.290673, acc.: 91.41%] [G loss: 2.501915]\n",
      "epoch:34 step:32591 [D loss: 0.273181, acc.: 92.97%] [G loss: 3.010991]\n",
      "epoch:34 step:32592 [D loss: 0.184474, acc.: 92.97%] [G loss: 4.257728]\n",
      "epoch:34 step:32593 [D loss: 0.065123, acc.: 98.44%] [G loss: 3.094913]\n",
      "epoch:34 step:32594 [D loss: 0.734601, acc.: 60.94%] [G loss: 3.311888]\n",
      "epoch:34 step:32595 [D loss: 0.410235, acc.: 76.56%] [G loss: 5.122604]\n",
      "epoch:34 step:32596 [D loss: 0.180326, acc.: 95.31%] [G loss: 2.535057]\n",
      "epoch:34 step:32597 [D loss: 0.254414, acc.: 90.62%] [G loss: 4.715449]\n",
      "epoch:34 step:32598 [D loss: 0.113473, acc.: 98.44%] [G loss: 2.700684]\n",
      "epoch:34 step:32599 [D loss: 0.236661, acc.: 93.75%] [G loss: 4.210486]\n",
      "epoch:34 step:32600 [D loss: 0.578738, acc.: 67.97%] [G loss: 4.713160]\n",
      "epoch:34 step:32601 [D loss: 0.135423, acc.: 96.88%] [G loss: 2.238869]\n",
      "epoch:34 step:32602 [D loss: 0.078823, acc.: 98.44%] [G loss: 3.040627]\n",
      "epoch:34 step:32603 [D loss: 0.175085, acc.: 94.53%] [G loss: 3.379802]\n",
      "epoch:34 step:32604 [D loss: 0.076298, acc.: 99.22%] [G loss: 3.346608]\n",
      "epoch:34 step:32605 [D loss: 0.146961, acc.: 96.88%] [G loss: 4.530078]\n",
      "epoch:34 step:32606 [D loss: 0.212968, acc.: 95.31%] [G loss: 1.982677]\n",
      "epoch:34 step:32607 [D loss: 0.059590, acc.: 99.22%] [G loss: 1.547770]\n",
      "epoch:34 step:32608 [D loss: 0.137641, acc.: 96.88%] [G loss: 4.684448]\n",
      "epoch:34 step:32609 [D loss: 0.249880, acc.: 90.62%] [G loss: 3.688346]\n",
      "epoch:34 step:32610 [D loss: 0.911834, acc.: 56.25%] [G loss: 4.108191]\n",
      "epoch:34 step:32611 [D loss: 0.510619, acc.: 67.19%] [G loss: 1.418265]\n",
      "epoch:34 step:32612 [D loss: 0.104968, acc.: 97.66%] [G loss: 4.968827]\n",
      "epoch:34 step:32613 [D loss: 0.934560, acc.: 45.31%] [G loss: 4.816961]\n",
      "epoch:34 step:32614 [D loss: 0.074907, acc.: 99.22%] [G loss: 3.859021]\n",
      "epoch:34 step:32615 [D loss: 0.470530, acc.: 78.91%] [G loss: 1.869147]\n",
      "epoch:34 step:32616 [D loss: 0.026987, acc.: 100.00%] [G loss: 3.780407]\n",
      "epoch:34 step:32617 [D loss: 0.265002, acc.: 88.28%] [G loss: 5.242429]\n",
      "epoch:34 step:32618 [D loss: 0.176384, acc.: 97.66%] [G loss: 4.594547]\n",
      "epoch:34 step:32619 [D loss: 0.087705, acc.: 99.22%] [G loss: 2.425402]\n",
      "epoch:34 step:32620 [D loss: 0.234340, acc.: 93.75%] [G loss: 0.924406]\n",
      "epoch:34 step:32621 [D loss: 0.310896, acc.: 88.28%] [G loss: 3.626624]\n",
      "epoch:34 step:32622 [D loss: 0.157590, acc.: 96.09%] [G loss: 1.600966]\n",
      "epoch:34 step:32623 [D loss: 0.703133, acc.: 68.75%] [G loss: 6.697007]\n",
      "epoch:34 step:32624 [D loss: 0.094418, acc.: 99.22%] [G loss: 3.339032]\n",
      "epoch:34 step:32625 [D loss: 0.181065, acc.: 96.09%] [G loss: 6.517211]\n",
      "epoch:34 step:32626 [D loss: 0.681457, acc.: 64.84%] [G loss: 2.448922]\n",
      "epoch:34 step:32627 [D loss: 0.524006, acc.: 69.53%] [G loss: 1.484944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32628 [D loss: 0.079591, acc.: 98.44%] [G loss: 4.130515]\n",
      "epoch:34 step:32629 [D loss: 0.236773, acc.: 91.41%] [G loss: 3.942457]\n",
      "epoch:34 step:32630 [D loss: 0.496096, acc.: 68.75%] [G loss: 2.814955]\n",
      "epoch:34 step:32631 [D loss: 1.273149, acc.: 53.12%] [G loss: 3.779176]\n",
      "epoch:34 step:32632 [D loss: 0.079012, acc.: 99.22%] [G loss: 3.120925]\n",
      "epoch:34 step:32633 [D loss: 0.757651, acc.: 60.16%] [G loss: 3.263195]\n",
      "epoch:34 step:32634 [D loss: 0.036509, acc.: 99.22%] [G loss: 4.958281]\n",
      "epoch:34 step:32635 [D loss: 0.056363, acc.: 99.22%] [G loss: 3.767194]\n",
      "epoch:34 step:32636 [D loss: 0.326210, acc.: 83.59%] [G loss: 3.247481]\n",
      "epoch:34 step:32637 [D loss: 0.970162, acc.: 53.91%] [G loss: 0.559061]\n",
      "epoch:34 step:32638 [D loss: 0.530940, acc.: 69.53%] [G loss: 4.275595]\n",
      "epoch:34 step:32639 [D loss: 0.127203, acc.: 97.66%] [G loss: 1.816491]\n",
      "epoch:34 step:32640 [D loss: 0.053761, acc.: 98.44%] [G loss: 2.219780]\n",
      "epoch:34 step:32641 [D loss: 0.027998, acc.: 100.00%] [G loss: 3.936265]\n",
      "epoch:34 step:32642 [D loss: 0.122444, acc.: 98.44%] [G loss: 0.818999]\n",
      "epoch:34 step:32643 [D loss: 0.431638, acc.: 73.44%] [G loss: 1.436495]\n",
      "epoch:34 step:32644 [D loss: 0.264462, acc.: 91.41%] [G loss: 1.751402]\n",
      "epoch:34 step:32645 [D loss: 0.081926, acc.: 97.66%] [G loss: 4.180452]\n",
      "epoch:34 step:32646 [D loss: 0.193848, acc.: 91.41%] [G loss: 2.253684]\n",
      "epoch:34 step:32647 [D loss: 0.435226, acc.: 70.31%] [G loss: 3.073700]\n",
      "epoch:34 step:32648 [D loss: 0.355659, acc.: 78.91%] [G loss: 0.355065]\n",
      "epoch:34 step:32649 [D loss: 0.329462, acc.: 83.59%] [G loss: 4.626309]\n",
      "epoch:34 step:32650 [D loss: 0.540885, acc.: 64.84%] [G loss: 4.173160]\n",
      "epoch:34 step:32651 [D loss: 0.030287, acc.: 100.00%] [G loss: 5.114758]\n",
      "epoch:34 step:32652 [D loss: 0.486621, acc.: 77.34%] [G loss: 4.393352]\n",
      "epoch:34 step:32653 [D loss: 0.303154, acc.: 89.06%] [G loss: 3.359672]\n",
      "epoch:34 step:32654 [D loss: 0.045389, acc.: 100.00%] [G loss: 3.782802]\n",
      "epoch:34 step:32655 [D loss: 0.399766, acc.: 79.69%] [G loss: 3.845781]\n",
      "epoch:34 step:32656 [D loss: 0.720054, acc.: 59.38%] [G loss: 6.937473]\n",
      "epoch:34 step:32657 [D loss: 0.855822, acc.: 57.03%] [G loss: 3.960715]\n",
      "epoch:34 step:32658 [D loss: 0.095431, acc.: 98.44%] [G loss: 2.458065]\n",
      "epoch:34 step:32659 [D loss: 0.120114, acc.: 98.44%] [G loss: 3.023703]\n",
      "epoch:34 step:32660 [D loss: 0.310057, acc.: 89.84%] [G loss: 2.934969]\n",
      "epoch:34 step:32661 [D loss: 0.151615, acc.: 96.88%] [G loss: 2.654996]\n",
      "epoch:34 step:32662 [D loss: 0.223926, acc.: 95.31%] [G loss: 1.430614]\n",
      "epoch:34 step:32663 [D loss: 0.179193, acc.: 96.88%] [G loss: 1.991748]\n",
      "epoch:34 step:32664 [D loss: 0.241395, acc.: 91.41%] [G loss: 2.010584]\n",
      "epoch:34 step:32665 [D loss: 0.076030, acc.: 100.00%] [G loss: 3.491880]\n",
      "epoch:34 step:32666 [D loss: 0.097140, acc.: 100.00%] [G loss: 1.580124]\n",
      "epoch:34 step:32667 [D loss: 0.407113, acc.: 82.03%] [G loss: 1.204048]\n",
      "epoch:34 step:32668 [D loss: 0.428848, acc.: 74.22%] [G loss: 1.029570]\n",
      "epoch:34 step:32669 [D loss: 0.050506, acc.: 100.00%] [G loss: 5.253300]\n",
      "epoch:34 step:32670 [D loss: 0.479785, acc.: 72.66%] [G loss: 2.294142]\n",
      "epoch:34 step:32671 [D loss: 0.480013, acc.: 78.91%] [G loss: 3.333503]\n",
      "epoch:34 step:32672 [D loss: 0.103848, acc.: 98.44%] [G loss: 1.195167]\n",
      "epoch:34 step:32673 [D loss: 0.312173, acc.: 90.62%] [G loss: 3.450560]\n",
      "epoch:34 step:32674 [D loss: 0.171882, acc.: 98.44%] [G loss: 4.963492]\n",
      "epoch:34 step:32675 [D loss: 0.374361, acc.: 86.72%] [G loss: 3.799467]\n",
      "epoch:34 step:32676 [D loss: 0.095210, acc.: 97.66%] [G loss: 4.614211]\n",
      "epoch:34 step:32677 [D loss: 0.460360, acc.: 72.66%] [G loss: 4.750941]\n",
      "epoch:34 step:32678 [D loss: 0.053551, acc.: 99.22%] [G loss: 1.626028]\n",
      "epoch:34 step:32679 [D loss: 0.338103, acc.: 82.81%] [G loss: 0.316422]\n",
      "epoch:34 step:32680 [D loss: 0.080107, acc.: 99.22%] [G loss: 1.545853]\n",
      "epoch:34 step:32681 [D loss: 0.043677, acc.: 100.00%] [G loss: 2.697326]\n",
      "epoch:34 step:32682 [D loss: 1.862713, acc.: 21.09%] [G loss: 2.007333]\n",
      "epoch:34 step:32683 [D loss: 0.132985, acc.: 96.88%] [G loss: 4.681561]\n",
      "epoch:34 step:32684 [D loss: 0.179503, acc.: 92.97%] [G loss: 4.924702]\n",
      "epoch:34 step:32685 [D loss: 0.060686, acc.: 97.66%] [G loss: 4.676608]\n",
      "epoch:34 step:32686 [D loss: 0.276794, acc.: 89.06%] [G loss: 2.015893]\n",
      "epoch:34 step:32687 [D loss: 0.116449, acc.: 98.44%] [G loss: 2.096863]\n",
      "epoch:34 step:32688 [D loss: 0.597789, acc.: 67.19%] [G loss: 1.425068]\n",
      "epoch:34 step:32689 [D loss: 0.220374, acc.: 97.66%] [G loss: 0.664509]\n",
      "epoch:34 step:32690 [D loss: 0.351316, acc.: 82.03%] [G loss: 5.426073]\n",
      "epoch:34 step:32691 [D loss: 0.216768, acc.: 93.75%] [G loss: 2.078510]\n",
      "epoch:34 step:32692 [D loss: 0.528425, acc.: 71.88%] [G loss: 2.644502]\n",
      "epoch:34 step:32693 [D loss: 0.145681, acc.: 94.53%] [G loss: 5.803599]\n",
      "epoch:34 step:32694 [D loss: 0.059736, acc.: 100.00%] [G loss: 5.372214]\n",
      "epoch:34 step:32695 [D loss: 0.126404, acc.: 96.09%] [G loss: 2.076819]\n",
      "epoch:34 step:32696 [D loss: 0.065828, acc.: 99.22%] [G loss: 2.683843]\n",
      "epoch:34 step:32697 [D loss: 0.102159, acc.: 99.22%] [G loss: 3.970005]\n",
      "epoch:34 step:32698 [D loss: 0.138093, acc.: 96.88%] [G loss: 4.305045]\n",
      "epoch:34 step:32699 [D loss: 0.209027, acc.: 92.97%] [G loss: 4.409068]\n",
      "epoch:34 step:32700 [D loss: 0.339618, acc.: 80.47%] [G loss: 5.406346]\n",
      "epoch:34 step:32701 [D loss: 0.098780, acc.: 99.22%] [G loss: 3.627171]\n",
      "epoch:34 step:32702 [D loss: 0.850530, acc.: 53.12%] [G loss: 5.361395]\n",
      "epoch:34 step:32703 [D loss: 0.042519, acc.: 100.00%] [G loss: 5.342985]\n",
      "epoch:34 step:32704 [D loss: 0.160359, acc.: 97.66%] [G loss: 2.833469]\n",
      "epoch:34 step:32705 [D loss: 0.326784, acc.: 88.28%] [G loss: 1.182990]\n",
      "epoch:34 step:32706 [D loss: 0.160434, acc.: 97.66%] [G loss: 1.714950]\n",
      "epoch:34 step:32707 [D loss: 0.466855, acc.: 69.53%] [G loss: 1.813815]\n",
      "epoch:34 step:32708 [D loss: 0.230984, acc.: 92.97%] [G loss: 2.749075]\n",
      "epoch:34 step:32709 [D loss: 1.350316, acc.: 47.66%] [G loss: 1.456295]\n",
      "epoch:34 step:32710 [D loss: 0.190660, acc.: 96.09%] [G loss: 0.294029]\n",
      "epoch:34 step:32711 [D loss: 0.057590, acc.: 100.00%] [G loss: 2.179927]\n",
      "epoch:34 step:32712 [D loss: 0.467236, acc.: 82.03%] [G loss: 2.612854]\n",
      "epoch:34 step:32713 [D loss: 0.215700, acc.: 94.53%] [G loss: 1.318123]\n",
      "epoch:34 step:32714 [D loss: 0.177589, acc.: 93.75%] [G loss: 1.844919]\n",
      "epoch:34 step:32715 [D loss: 0.175389, acc.: 93.75%] [G loss: 1.612809]\n",
      "epoch:34 step:32716 [D loss: 0.187907, acc.: 91.41%] [G loss: 2.672188]\n",
      "epoch:34 step:32717 [D loss: 0.515763, acc.: 72.66%] [G loss: 1.467311]\n",
      "epoch:34 step:32718 [D loss: 0.281110, acc.: 87.50%] [G loss: 0.571881]\n",
      "epoch:34 step:32719 [D loss: 0.036381, acc.: 100.00%] [G loss: 1.539526]\n",
      "epoch:34 step:32720 [D loss: 0.085770, acc.: 98.44%] [G loss: 4.335111]\n",
      "epoch:34 step:32721 [D loss: 0.049733, acc.: 100.00%] [G loss: 1.309092]\n",
      "epoch:34 step:32722 [D loss: 0.200662, acc.: 92.97%] [G loss: 7.424366]\n",
      "epoch:34 step:32723 [D loss: 0.184119, acc.: 92.97%] [G loss: 0.501437]\n",
      "epoch:34 step:32724 [D loss: 0.418753, acc.: 75.00%] [G loss: 2.737785]\n",
      "epoch:34 step:32725 [D loss: 0.114573, acc.: 98.44%] [G loss: 2.761817]\n",
      "epoch:34 step:32726 [D loss: 0.244903, acc.: 92.97%] [G loss: 2.083664]\n",
      "epoch:34 step:32727 [D loss: 0.132635, acc.: 97.66%] [G loss: 3.148410]\n",
      "epoch:34 step:32728 [D loss: 0.454450, acc.: 69.53%] [G loss: 4.472930]\n",
      "epoch:34 step:32729 [D loss: 0.045659, acc.: 100.00%] [G loss: 8.252615]\n",
      "epoch:34 step:32730 [D loss: 0.957054, acc.: 55.47%] [G loss: 6.281489]\n",
      "epoch:34 step:32731 [D loss: 0.126933, acc.: 97.66%] [G loss: 6.622363]\n",
      "epoch:34 step:32732 [D loss: 0.209662, acc.: 93.75%] [G loss: 3.576340]\n",
      "epoch:34 step:32733 [D loss: 0.523676, acc.: 66.41%] [G loss: 1.227293]\n",
      "epoch:34 step:32734 [D loss: 0.049467, acc.: 100.00%] [G loss: 2.807141]\n",
      "epoch:34 step:32735 [D loss: 0.572879, acc.: 70.31%] [G loss: 2.955250]\n",
      "epoch:34 step:32736 [D loss: 0.852452, acc.: 60.16%] [G loss: 5.758215]\n",
      "epoch:34 step:32737 [D loss: 0.111338, acc.: 98.44%] [G loss: 6.279764]\n",
      "epoch:34 step:32738 [D loss: 0.867443, acc.: 59.38%] [G loss: 4.279025]\n",
      "epoch:34 step:32739 [D loss: 0.064167, acc.: 98.44%] [G loss: 4.309501]\n",
      "epoch:34 step:32740 [D loss: 0.037689, acc.: 99.22%] [G loss: 0.876014]\n",
      "epoch:34 step:32741 [D loss: 0.151976, acc.: 95.31%] [G loss: 2.162799]\n",
      "epoch:34 step:32742 [D loss: 0.118998, acc.: 98.44%] [G loss: 3.197326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:32743 [D loss: 0.877622, acc.: 53.91%] [G loss: 2.738580]\n",
      "epoch:34 step:32744 [D loss: 0.587318, acc.: 67.97%] [G loss: 3.863334]\n",
      "epoch:34 step:32745 [D loss: 0.227709, acc.: 92.97%] [G loss: 5.653765]\n",
      "epoch:34 step:32746 [D loss: 0.036408, acc.: 99.22%] [G loss: 2.711916]\n",
      "epoch:34 step:32747 [D loss: 0.102172, acc.: 97.66%] [G loss: 6.945001]\n",
      "epoch:34 step:32748 [D loss: 0.073098, acc.: 100.00%] [G loss: 4.849446]\n",
      "epoch:34 step:32749 [D loss: 0.033315, acc.: 100.00%] [G loss: 1.390942]\n",
      "epoch:34 step:32750 [D loss: 0.353609, acc.: 84.38%] [G loss: 4.489732]\n",
      "epoch:34 step:32751 [D loss: 0.062232, acc.: 98.44%] [G loss: 1.509449]\n",
      "epoch:34 step:32752 [D loss: 0.320820, acc.: 87.50%] [G loss: 3.679485]\n",
      "epoch:34 step:32753 [D loss: 0.058022, acc.: 100.00%] [G loss: 3.162035]\n",
      "epoch:34 step:32754 [D loss: 0.074651, acc.: 99.22%] [G loss: 5.167184]\n",
      "epoch:34 step:32755 [D loss: 0.261662, acc.: 92.97%] [G loss: 4.012213]\n",
      "epoch:34 step:32756 [D loss: 0.048651, acc.: 99.22%] [G loss: 0.863204]\n",
      "epoch:34 step:32757 [D loss: 0.177902, acc.: 95.31%] [G loss: 1.871772]\n",
      "epoch:34 step:32758 [D loss: 0.153844, acc.: 97.66%] [G loss: 2.418582]\n",
      "epoch:34 step:32759 [D loss: 0.248602, acc.: 96.09%] [G loss: 1.100466]\n",
      "epoch:34 step:32760 [D loss: 0.714729, acc.: 60.94%] [G loss: 3.836699]\n",
      "epoch:34 step:32761 [D loss: 0.497070, acc.: 69.53%] [G loss: 4.383414]\n",
      "epoch:34 step:32762 [D loss: 0.158175, acc.: 94.53%] [G loss: 3.317116]\n",
      "epoch:34 step:32763 [D loss: 0.667305, acc.: 61.72%] [G loss: 2.327708]\n",
      "epoch:34 step:32764 [D loss: 0.353282, acc.: 81.25%] [G loss: 2.818491]\n",
      "epoch:34 step:32765 [D loss: 0.453767, acc.: 74.22%] [G loss: 4.014983]\n",
      "epoch:34 step:32766 [D loss: 0.114963, acc.: 97.66%] [G loss: 4.778008]\n",
      "epoch:34 step:32767 [D loss: 0.067262, acc.: 98.44%] [G loss: 3.509930]\n",
      "epoch:34 step:32768 [D loss: 0.135093, acc.: 96.09%] [G loss: 5.148978]\n",
      "epoch:34 step:32769 [D loss: 0.077756, acc.: 100.00%] [G loss: 4.555279]\n",
      "epoch:34 step:32770 [D loss: 0.186274, acc.: 96.09%] [G loss: 0.230364]\n",
      "epoch:34 step:32771 [D loss: 0.151957, acc.: 97.66%] [G loss: 3.098112]\n",
      "epoch:34 step:32772 [D loss: 0.197978, acc.: 95.31%] [G loss: 2.724681]\n",
      "epoch:34 step:32773 [D loss: 0.041771, acc.: 100.00%] [G loss: 4.807849]\n",
      "epoch:34 step:32774 [D loss: 0.133211, acc.: 99.22%] [G loss: 1.950553]\n",
      "epoch:34 step:32775 [D loss: 0.289104, acc.: 89.84%] [G loss: 5.755481]\n",
      "epoch:34 step:32776 [D loss: 0.179503, acc.: 94.53%] [G loss: 5.792223]\n",
      "epoch:34 step:32777 [D loss: 0.577509, acc.: 71.09%] [G loss: 5.016759]\n",
      "epoch:34 step:32778 [D loss: 0.265463, acc.: 88.28%] [G loss: 3.461979]\n",
      "epoch:34 step:32779 [D loss: 0.508809, acc.: 69.53%] [G loss: 1.934968]\n",
      "epoch:34 step:32780 [D loss: 0.303173, acc.: 85.94%] [G loss: 1.678735]\n",
      "epoch:34 step:32781 [D loss: 0.070741, acc.: 99.22%] [G loss: 3.877034]\n",
      "epoch:34 step:32782 [D loss: 0.251587, acc.: 97.66%] [G loss: 2.560754]\n",
      "epoch:34 step:32783 [D loss: 0.037823, acc.: 99.22%] [G loss: 3.998467]\n",
      "epoch:34 step:32784 [D loss: 0.154264, acc.: 97.66%] [G loss: 4.668151]\n",
      "epoch:34 step:32785 [D loss: 0.266448, acc.: 89.84%] [G loss: 0.119576]\n",
      "epoch:34 step:32786 [D loss: 0.129513, acc.: 96.09%] [G loss: 2.625968]\n",
      "epoch:34 step:32787 [D loss: 0.469054, acc.: 76.56%] [G loss: 3.045778]\n",
      "epoch:34 step:32788 [D loss: 0.064210, acc.: 99.22%] [G loss: 6.331120]\n",
      "epoch:34 step:32789 [D loss: 1.516379, acc.: 53.12%] [G loss: 0.791586]\n",
      "epoch:34 step:32790 [D loss: 0.108099, acc.: 97.66%] [G loss: 4.918712]\n",
      "epoch:34 step:32791 [D loss: 0.295891, acc.: 87.50%] [G loss: 3.711233]\n",
      "epoch:34 step:32792 [D loss: 0.240859, acc.: 92.97%] [G loss: 3.799320]\n",
      "epoch:34 step:32793 [D loss: 0.233717, acc.: 88.28%] [G loss: 3.369956]\n",
      "epoch:34 step:32794 [D loss: 0.159113, acc.: 95.31%] [G loss: 2.955714]\n",
      "epoch:34 step:32795 [D loss: 0.880157, acc.: 57.03%] [G loss: 1.473133]\n",
      "epoch:35 step:32796 [D loss: 0.476424, acc.: 72.66%] [G loss: 3.052607]\n",
      "epoch:35 step:32797 [D loss: 0.081647, acc.: 98.44%] [G loss: 4.375832]\n",
      "epoch:35 step:32798 [D loss: 0.283920, acc.: 87.50%] [G loss: 6.562759]\n",
      "epoch:35 step:32799 [D loss: 0.102899, acc.: 96.88%] [G loss: 6.581477]\n",
      "epoch:35 step:32800 [D loss: 0.805886, acc.: 59.38%] [G loss: 4.622215]\n",
      "epoch:35 step:32801 [D loss: 0.280125, acc.: 89.84%] [G loss: 1.904154]\n",
      "epoch:35 step:32802 [D loss: 0.259675, acc.: 89.84%] [G loss: 3.890116]\n",
      "epoch:35 step:32803 [D loss: 0.054962, acc.: 100.00%] [G loss: 1.764462]\n",
      "epoch:35 step:32804 [D loss: 0.094734, acc.: 99.22%] [G loss: 2.960875]\n",
      "epoch:35 step:32805 [D loss: 0.449334, acc.: 80.47%] [G loss: 4.503305]\n",
      "epoch:35 step:32806 [D loss: 0.362335, acc.: 81.25%] [G loss: 4.919792]\n",
      "epoch:35 step:32807 [D loss: 0.083824, acc.: 100.00%] [G loss: 3.691876]\n",
      "epoch:35 step:32808 [D loss: 0.310876, acc.: 88.28%] [G loss: 5.225256]\n",
      "epoch:35 step:32809 [D loss: 0.058288, acc.: 99.22%] [G loss: 3.289774]\n",
      "epoch:35 step:32810 [D loss: 0.039827, acc.: 99.22%] [G loss: 3.232350]\n",
      "epoch:35 step:32811 [D loss: 0.044317, acc.: 99.22%] [G loss: 2.792937]\n",
      "epoch:35 step:32812 [D loss: 0.096906, acc.: 97.66%] [G loss: 2.572506]\n",
      "epoch:35 step:32813 [D loss: 0.033187, acc.: 100.00%] [G loss: 0.811187]\n",
      "epoch:35 step:32814 [D loss: 0.520180, acc.: 67.97%] [G loss: 5.489932]\n",
      "epoch:35 step:32815 [D loss: 0.545990, acc.: 68.75%] [G loss: 0.821102]\n",
      "epoch:35 step:32816 [D loss: 0.102022, acc.: 100.00%] [G loss: 0.287293]\n",
      "epoch:35 step:32817 [D loss: 0.181919, acc.: 93.75%] [G loss: 5.184563]\n",
      "epoch:35 step:32818 [D loss: 0.656678, acc.: 66.41%] [G loss: 6.003456]\n",
      "epoch:35 step:32819 [D loss: 0.043161, acc.: 100.00%] [G loss: 6.317878]\n",
      "epoch:35 step:32820 [D loss: 0.291746, acc.: 83.59%] [G loss: 4.247893]\n",
      "epoch:35 step:32821 [D loss: 0.047441, acc.: 100.00%] [G loss: 4.095907]\n",
      "epoch:35 step:32822 [D loss: 2.400668, acc.: 11.72%] [G loss: 2.728179]\n",
      "epoch:35 step:32823 [D loss: 0.038140, acc.: 98.44%] [G loss: 3.321594]\n",
      "epoch:35 step:32824 [D loss: 0.868751, acc.: 57.03%] [G loss: 4.184601]\n",
      "epoch:35 step:32825 [D loss: 0.128824, acc.: 98.44%] [G loss: 5.086752]\n",
      "epoch:35 step:32826 [D loss: 1.391784, acc.: 50.78%] [G loss: 4.165251]\n",
      "epoch:35 step:32827 [D loss: 0.266966, acc.: 91.41%] [G loss: 3.544542]\n",
      "epoch:35 step:32828 [D loss: 0.246917, acc.: 93.75%] [G loss: 0.561659]\n",
      "epoch:35 step:32829 [D loss: 0.074020, acc.: 100.00%] [G loss: 3.235398]\n",
      "epoch:35 step:32830 [D loss: 0.380921, acc.: 89.06%] [G loss: 3.015342]\n",
      "epoch:35 step:32831 [D loss: 0.114946, acc.: 100.00%] [G loss: 3.161535]\n",
      "epoch:35 step:32832 [D loss: 0.220888, acc.: 94.53%] [G loss: 4.368793]\n",
      "epoch:35 step:32833 [D loss: 0.236625, acc.: 89.84%] [G loss: 2.851679]\n",
      "epoch:35 step:32834 [D loss: 1.135750, acc.: 47.66%] [G loss: 1.647606]\n",
      "epoch:35 step:32835 [D loss: 0.084844, acc.: 99.22%] [G loss: 3.058821]\n",
      "epoch:35 step:32836 [D loss: 0.045027, acc.: 100.00%] [G loss: 1.246290]\n",
      "epoch:35 step:32837 [D loss: 0.148975, acc.: 97.66%] [G loss: 2.007807]\n",
      "epoch:35 step:32838 [D loss: 0.097273, acc.: 98.44%] [G loss: 2.442546]\n",
      "epoch:35 step:32839 [D loss: 0.988169, acc.: 53.91%] [G loss: 3.140084]\n",
      "epoch:35 step:32840 [D loss: 0.732292, acc.: 57.03%] [G loss: 1.122671]\n",
      "epoch:35 step:32841 [D loss: 0.269388, acc.: 87.50%] [G loss: 1.075554]\n",
      "epoch:35 step:32842 [D loss: 0.159983, acc.: 94.53%] [G loss: 3.707136]\n",
      "epoch:35 step:32843 [D loss: 0.130713, acc.: 99.22%] [G loss: 1.115199]\n",
      "epoch:35 step:32844 [D loss: 0.676377, acc.: 61.72%] [G loss: 4.592974]\n",
      "epoch:35 step:32845 [D loss: 0.031629, acc.: 100.00%] [G loss: 4.072465]\n",
      "epoch:35 step:32846 [D loss: 0.031477, acc.: 100.00%] [G loss: 3.069355]\n",
      "epoch:35 step:32847 [D loss: 0.025112, acc.: 100.00%] [G loss: 1.171833]\n",
      "epoch:35 step:32848 [D loss: 0.212791, acc.: 96.09%] [G loss: 1.449763]\n",
      "epoch:35 step:32849 [D loss: 0.905948, acc.: 53.91%] [G loss: 7.057896]\n",
      "epoch:35 step:32850 [D loss: 0.332702, acc.: 87.50%] [G loss: 2.548623]\n",
      "epoch:35 step:32851 [D loss: 0.070177, acc.: 99.22%] [G loss: 4.260128]\n",
      "epoch:35 step:32852 [D loss: 0.277970, acc.: 88.28%] [G loss: 3.953998]\n",
      "epoch:35 step:32853 [D loss: 0.257613, acc.: 92.97%] [G loss: 1.210059]\n",
      "epoch:35 step:32854 [D loss: 0.105237, acc.: 97.66%] [G loss: 3.397993]\n",
      "epoch:35 step:32855 [D loss: 0.939518, acc.: 53.91%] [G loss: 2.173072]\n",
      "epoch:35 step:32856 [D loss: 0.300227, acc.: 87.50%] [G loss: 0.134523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:32857 [D loss: 0.139923, acc.: 94.53%] [G loss: 1.323939]\n",
      "epoch:35 step:32858 [D loss: 0.112195, acc.: 99.22%] [G loss: 2.098347]\n",
      "epoch:35 step:32859 [D loss: 1.589038, acc.: 50.78%] [G loss: 3.596741]\n",
      "epoch:35 step:32860 [D loss: 0.201884, acc.: 96.09%] [G loss: 2.389235]\n",
      "epoch:35 step:32861 [D loss: 0.114422, acc.: 96.88%] [G loss: 4.084085]\n",
      "epoch:35 step:32862 [D loss: 0.615792, acc.: 67.19%] [G loss: 2.505677]\n",
      "epoch:35 step:32863 [D loss: 0.466470, acc.: 73.44%] [G loss: 1.639200]\n",
      "epoch:35 step:32864 [D loss: 0.036258, acc.: 100.00%] [G loss: 3.418218]\n",
      "epoch:35 step:32865 [D loss: 0.311444, acc.: 80.47%] [G loss: 0.788929]\n",
      "epoch:35 step:32866 [D loss: 0.126413, acc.: 96.88%] [G loss: 1.530191]\n",
      "epoch:35 step:32867 [D loss: 0.045165, acc.: 100.00%] [G loss: 2.704464]\n",
      "epoch:35 step:32868 [D loss: 0.219863, acc.: 91.41%] [G loss: 2.348646]\n",
      "epoch:35 step:32869 [D loss: 0.068494, acc.: 99.22%] [G loss: 3.764586]\n",
      "epoch:35 step:32870 [D loss: 0.097670, acc.: 97.66%] [G loss: 2.210193]\n",
      "epoch:35 step:32871 [D loss: 0.532823, acc.: 67.97%] [G loss: 2.054419]\n",
      "epoch:35 step:32872 [D loss: 0.082289, acc.: 98.44%] [G loss: 6.442632]\n",
      "epoch:35 step:32873 [D loss: 0.157674, acc.: 98.44%] [G loss: 3.332421]\n",
      "epoch:35 step:32874 [D loss: 0.070031, acc.: 100.00%] [G loss: 3.943373]\n",
      "epoch:35 step:32875 [D loss: 0.281467, acc.: 95.31%] [G loss: 3.594889]\n",
      "epoch:35 step:32876 [D loss: 0.032340, acc.: 99.22%] [G loss: 5.298028]\n",
      "epoch:35 step:32877 [D loss: 0.248148, acc.: 88.28%] [G loss: 2.455715]\n",
      "epoch:35 step:32878 [D loss: 0.077684, acc.: 100.00%] [G loss: 3.528344]\n",
      "epoch:35 step:32879 [D loss: 1.306838, acc.: 53.91%] [G loss: 5.116653]\n",
      "epoch:35 step:32880 [D loss: 0.084151, acc.: 99.22%] [G loss: 5.044271]\n",
      "epoch:35 step:32881 [D loss: 0.135404, acc.: 96.09%] [G loss: 3.294646]\n",
      "epoch:35 step:32882 [D loss: 0.213766, acc.: 96.09%] [G loss: 3.252882]\n",
      "epoch:35 step:32883 [D loss: 0.228562, acc.: 91.41%] [G loss: 2.363440]\n",
      "epoch:35 step:32884 [D loss: 0.043743, acc.: 100.00%] [G loss: 2.835572]\n",
      "epoch:35 step:32885 [D loss: 0.223283, acc.: 96.09%] [G loss: 3.827870]\n",
      "epoch:35 step:32886 [D loss: 0.027975, acc.: 100.00%] [G loss: 6.322814]\n",
      "epoch:35 step:32887 [D loss: 0.140815, acc.: 97.66%] [G loss: 2.471862]\n",
      "epoch:35 step:32888 [D loss: 0.122657, acc.: 100.00%] [G loss: 3.947797]\n",
      "epoch:35 step:32889 [D loss: 0.405559, acc.: 76.56%] [G loss: 2.566420]\n",
      "epoch:35 step:32890 [D loss: 0.272919, acc.: 95.31%] [G loss: 1.577021]\n",
      "epoch:35 step:32891 [D loss: 0.225352, acc.: 95.31%] [G loss: 5.418123]\n",
      "epoch:35 step:32892 [D loss: 0.075574, acc.: 99.22%] [G loss: 5.732820]\n",
      "epoch:35 step:32893 [D loss: 0.027158, acc.: 99.22%] [G loss: 3.134269]\n",
      "epoch:35 step:32894 [D loss: 0.594630, acc.: 66.41%] [G loss: 1.296443]\n",
      "epoch:35 step:32895 [D loss: 0.625746, acc.: 64.06%] [G loss: 2.032274]\n",
      "epoch:35 step:32896 [D loss: 0.251683, acc.: 96.09%] [G loss: 3.447842]\n",
      "epoch:35 step:32897 [D loss: 0.271238, acc.: 86.72%] [G loss: 2.016051]\n",
      "epoch:35 step:32898 [D loss: 0.316157, acc.: 83.59%] [G loss: 3.748096]\n",
      "epoch:35 step:32899 [D loss: 0.311246, acc.: 87.50%] [G loss: 2.902744]\n",
      "epoch:35 step:32900 [D loss: 1.047486, acc.: 46.09%] [G loss: 4.273374]\n",
      "epoch:35 step:32901 [D loss: 0.308151, acc.: 85.16%] [G loss: 2.445683]\n",
      "epoch:35 step:32902 [D loss: 0.071265, acc.: 97.66%] [G loss: 2.631024]\n",
      "epoch:35 step:32903 [D loss: 0.182105, acc.: 93.75%] [G loss: 3.259675]\n",
      "epoch:35 step:32904 [D loss: 1.543157, acc.: 30.47%] [G loss: 0.617504]\n",
      "epoch:35 step:32905 [D loss: 0.629996, acc.: 63.28%] [G loss: 1.079715]\n",
      "epoch:35 step:32906 [D loss: 1.028471, acc.: 55.47%] [G loss: 5.467375]\n",
      "epoch:35 step:32907 [D loss: 0.189113, acc.: 92.97%] [G loss: 3.866234]\n",
      "epoch:35 step:32908 [D loss: 0.113356, acc.: 98.44%] [G loss: 5.397020]\n",
      "epoch:35 step:32909 [D loss: 0.887331, acc.: 57.03%] [G loss: 5.184927]\n",
      "epoch:35 step:32910 [D loss: 0.379642, acc.: 79.69%] [G loss: 1.106104]\n",
      "epoch:35 step:32911 [D loss: 0.554346, acc.: 73.44%] [G loss: 3.886922]\n",
      "epoch:35 step:32912 [D loss: 0.257409, acc.: 89.06%] [G loss: 1.261492]\n",
      "epoch:35 step:32913 [D loss: 0.149074, acc.: 97.66%] [G loss: 2.035498]\n",
      "epoch:35 step:32914 [D loss: 0.626276, acc.: 68.75%] [G loss: 0.440687]\n",
      "epoch:35 step:32915 [D loss: 0.329681, acc.: 89.06%] [G loss: 3.979172]\n",
      "epoch:35 step:32916 [D loss: 0.336786, acc.: 87.50%] [G loss: 3.338323]\n",
      "epoch:35 step:32917 [D loss: 0.476374, acc.: 73.44%] [G loss: 1.850958]\n",
      "epoch:35 step:32918 [D loss: 0.109319, acc.: 99.22%] [G loss: 4.047031]\n",
      "epoch:35 step:32919 [D loss: 0.319800, acc.: 89.84%] [G loss: 4.199217]\n",
      "epoch:35 step:32920 [D loss: 0.022275, acc.: 100.00%] [G loss: 1.272627]\n",
      "epoch:35 step:32921 [D loss: 0.086421, acc.: 99.22%] [G loss: 6.208638]\n",
      "epoch:35 step:32922 [D loss: 0.547751, acc.: 69.53%] [G loss: 2.182055]\n",
      "epoch:35 step:32923 [D loss: 0.166155, acc.: 96.09%] [G loss: 1.634527]\n",
      "epoch:35 step:32924 [D loss: 0.167885, acc.: 96.88%] [G loss: 2.103806]\n",
      "epoch:35 step:32925 [D loss: 0.244714, acc.: 87.50%] [G loss: 0.989999]\n",
      "epoch:35 step:32926 [D loss: 0.075694, acc.: 97.66%] [G loss: 1.649839]\n",
      "epoch:35 step:32927 [D loss: 0.242399, acc.: 94.53%] [G loss: 3.426300]\n",
      "epoch:35 step:32928 [D loss: 0.009912, acc.: 100.00%] [G loss: 5.201708]\n",
      "epoch:35 step:32929 [D loss: 0.207917, acc.: 92.19%] [G loss: 2.056048]\n",
      "epoch:35 step:32930 [D loss: 0.148162, acc.: 96.88%] [G loss: 1.036892]\n",
      "epoch:35 step:32931 [D loss: 0.080955, acc.: 100.00%] [G loss: 1.138255]\n",
      "epoch:35 step:32932 [D loss: 0.042628, acc.: 100.00%] [G loss: 4.013758]\n",
      "epoch:35 step:32933 [D loss: 0.540275, acc.: 67.19%] [G loss: 2.373689]\n",
      "epoch:35 step:32934 [D loss: 0.025435, acc.: 100.00%] [G loss: 4.646721]\n",
      "epoch:35 step:32935 [D loss: 0.235080, acc.: 93.75%] [G loss: 1.276929]\n",
      "epoch:35 step:32936 [D loss: 0.141365, acc.: 97.66%] [G loss: 4.029066]\n",
      "epoch:35 step:32937 [D loss: 0.255929, acc.: 91.41%] [G loss: 1.699068]\n",
      "epoch:35 step:32938 [D loss: 0.302768, acc.: 80.47%] [G loss: 0.483105]\n",
      "epoch:35 step:32939 [D loss: 0.104631, acc.: 99.22%] [G loss: 1.533434]\n",
      "epoch:35 step:32940 [D loss: 0.136008, acc.: 96.88%] [G loss: 3.767342]\n",
      "epoch:35 step:32941 [D loss: 1.265887, acc.: 51.56%] [G loss: 4.513496]\n",
      "epoch:35 step:32942 [D loss: 0.347692, acc.: 80.47%] [G loss: 3.964525]\n",
      "epoch:35 step:32943 [D loss: 0.275768, acc.: 88.28%] [G loss: 3.565827]\n",
      "epoch:35 step:32944 [D loss: 0.274180, acc.: 92.19%] [G loss: 1.147724]\n",
      "epoch:35 step:32945 [D loss: 0.204577, acc.: 93.75%] [G loss: 0.223085]\n",
      "epoch:35 step:32946 [D loss: 0.238645, acc.: 86.72%] [G loss: 2.943073]\n",
      "epoch:35 step:32947 [D loss: 0.115969, acc.: 97.66%] [G loss: 4.326196]\n",
      "epoch:35 step:32948 [D loss: 0.322723, acc.: 90.62%] [G loss: 5.181692]\n",
      "epoch:35 step:32949 [D loss: 0.204677, acc.: 92.19%] [G loss: 3.856576]\n",
      "epoch:35 step:32950 [D loss: 0.162640, acc.: 97.66%] [G loss: 3.213401]\n",
      "epoch:35 step:32951 [D loss: 0.094887, acc.: 98.44%] [G loss: 4.177216]\n",
      "epoch:35 step:32952 [D loss: 0.501288, acc.: 71.09%] [G loss: 4.882598]\n",
      "epoch:35 step:32953 [D loss: 0.355566, acc.: 86.72%] [G loss: 1.549338]\n",
      "epoch:35 step:32954 [D loss: 0.079804, acc.: 99.22%] [G loss: 7.047482]\n",
      "epoch:35 step:32955 [D loss: 0.809121, acc.: 57.81%] [G loss: 3.108723]\n",
      "epoch:35 step:32956 [D loss: 0.662061, acc.: 67.19%] [G loss: 3.985040]\n",
      "epoch:35 step:32957 [D loss: 0.234572, acc.: 95.31%] [G loss: 4.981273]\n",
      "epoch:35 step:32958 [D loss: 0.077962, acc.: 99.22%] [G loss: 1.718231]\n",
      "epoch:35 step:32959 [D loss: 0.322676, acc.: 83.59%] [G loss: 2.482500]\n",
      "epoch:35 step:32960 [D loss: 0.140893, acc.: 95.31%] [G loss: 0.294088]\n",
      "epoch:35 step:32961 [D loss: 0.850576, acc.: 53.12%] [G loss: 2.052477]\n",
      "epoch:35 step:32962 [D loss: 0.204351, acc.: 96.88%] [G loss: 4.983634]\n",
      "epoch:35 step:32963 [D loss: 0.406119, acc.: 80.47%] [G loss: 2.780166]\n",
      "epoch:35 step:32964 [D loss: 0.147011, acc.: 97.66%] [G loss: 3.292819]\n",
      "epoch:35 step:32965 [D loss: 0.299711, acc.: 82.81%] [G loss: 2.261980]\n",
      "epoch:35 step:32966 [D loss: 1.293251, acc.: 32.81%] [G loss: 3.562337]\n",
      "epoch:35 step:32967 [D loss: 0.102431, acc.: 99.22%] [G loss: 2.797937]\n",
      "epoch:35 step:32968 [D loss: 0.105871, acc.: 97.66%] [G loss: 2.161340]\n",
      "epoch:35 step:32969 [D loss: 0.100436, acc.: 99.22%] [G loss: 1.530906]\n",
      "epoch:35 step:32970 [D loss: 0.080350, acc.: 97.66%] [G loss: 3.441590]\n",
      "epoch:35 step:32971 [D loss: 0.680175, acc.: 60.16%] [G loss: 3.017028]\n",
      "epoch:35 step:32972 [D loss: 0.139855, acc.: 97.66%] [G loss: 4.579259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:32973 [D loss: 0.644744, acc.: 64.84%] [G loss: 2.916591]\n",
      "epoch:35 step:32974 [D loss: 0.090353, acc.: 100.00%] [G loss: 4.377579]\n",
      "epoch:35 step:32975 [D loss: 0.254617, acc.: 90.62%] [G loss: 3.012697]\n",
      "epoch:35 step:32976 [D loss: 0.444562, acc.: 78.12%] [G loss: 1.579911]\n",
      "epoch:35 step:32977 [D loss: 0.119757, acc.: 96.88%] [G loss: 2.022745]\n",
      "epoch:35 step:32978 [D loss: 0.046264, acc.: 99.22%] [G loss: 4.117432]\n",
      "epoch:35 step:32979 [D loss: 0.667962, acc.: 64.84%] [G loss: 2.757456]\n",
      "epoch:35 step:32980 [D loss: 0.131087, acc.: 98.44%] [G loss: 1.226166]\n",
      "epoch:35 step:32981 [D loss: 0.189782, acc.: 92.19%] [G loss: 1.829474]\n",
      "epoch:35 step:32982 [D loss: 0.348319, acc.: 77.34%] [G loss: 0.991009]\n",
      "epoch:35 step:32983 [D loss: 0.049976, acc.: 100.00%] [G loss: 5.208425]\n",
      "epoch:35 step:32984 [D loss: 0.154831, acc.: 92.97%] [G loss: 3.799981]\n",
      "epoch:35 step:32985 [D loss: 0.340814, acc.: 85.94%] [G loss: 5.152555]\n",
      "epoch:35 step:32986 [D loss: 0.360527, acc.: 80.47%] [G loss: 2.072438]\n",
      "epoch:35 step:32987 [D loss: 0.085021, acc.: 96.09%] [G loss: 2.283472]\n",
      "epoch:35 step:32988 [D loss: 0.363325, acc.: 75.00%] [G loss: 6.052607]\n",
      "epoch:35 step:32989 [D loss: 0.237870, acc.: 93.75%] [G loss: 8.187387]\n",
      "epoch:35 step:32990 [D loss: 0.144753, acc.: 96.88%] [G loss: 4.535413]\n",
      "epoch:35 step:32991 [D loss: 0.473618, acc.: 71.88%] [G loss: 1.447016]\n",
      "epoch:35 step:32992 [D loss: 0.502701, acc.: 70.31%] [G loss: 6.653712]\n",
      "epoch:35 step:32993 [D loss: 0.160409, acc.: 94.53%] [G loss: 4.495165]\n",
      "epoch:35 step:32994 [D loss: 0.100536, acc.: 96.88%] [G loss: 3.885132]\n",
      "epoch:35 step:32995 [D loss: 0.012523, acc.: 100.00%] [G loss: 4.010273]\n",
      "epoch:35 step:32996 [D loss: 0.069720, acc.: 98.44%] [G loss: 2.517800]\n",
      "epoch:35 step:32997 [D loss: 0.117972, acc.: 97.66%] [G loss: 2.317707]\n",
      "epoch:35 step:32998 [D loss: 0.134520, acc.: 98.44%] [G loss: 5.442842]\n",
      "epoch:35 step:32999 [D loss: 0.121721, acc.: 98.44%] [G loss: 3.896103]\n",
      "epoch:35 step:33000 [D loss: 0.432803, acc.: 82.03%] [G loss: 2.036221]\n",
      "epoch:35 step:33001 [D loss: 0.082497, acc.: 99.22%] [G loss: 0.782613]\n",
      "epoch:35 step:33002 [D loss: 0.054395, acc.: 98.44%] [G loss: 5.700089]\n",
      "epoch:35 step:33003 [D loss: 1.016157, acc.: 56.25%] [G loss: 2.067688]\n",
      "epoch:35 step:33004 [D loss: 1.143345, acc.: 53.91%] [G loss: 1.995882]\n",
      "epoch:35 step:33005 [D loss: 0.020185, acc.: 100.00%] [G loss: 3.655936]\n",
      "epoch:35 step:33006 [D loss: 0.118016, acc.: 96.09%] [G loss: 1.605278]\n",
      "epoch:35 step:33007 [D loss: 0.652797, acc.: 68.75%] [G loss: 2.374306]\n",
      "epoch:35 step:33008 [D loss: 0.141057, acc.: 97.66%] [G loss: 1.413691]\n",
      "epoch:35 step:33009 [D loss: 0.231064, acc.: 89.84%] [G loss: 1.879336]\n",
      "epoch:35 step:33010 [D loss: 0.132845, acc.: 98.44%] [G loss: 5.192430]\n",
      "epoch:35 step:33011 [D loss: 0.393371, acc.: 86.72%] [G loss: 1.959576]\n",
      "epoch:35 step:33012 [D loss: 0.142158, acc.: 97.66%] [G loss: 2.038578]\n",
      "epoch:35 step:33013 [D loss: 0.541016, acc.: 67.97%] [G loss: 6.621964]\n",
      "epoch:35 step:33014 [D loss: 0.346948, acc.: 88.28%] [G loss: 2.678570]\n",
      "epoch:35 step:33015 [D loss: 0.853184, acc.: 58.59%] [G loss: 2.342335]\n",
      "epoch:35 step:33016 [D loss: 0.361369, acc.: 83.59%] [G loss: 2.899170]\n",
      "epoch:35 step:33017 [D loss: 0.346521, acc.: 83.59%] [G loss: 2.787451]\n",
      "epoch:35 step:33018 [D loss: 0.091333, acc.: 97.66%] [G loss: 3.440094]\n",
      "epoch:35 step:33019 [D loss: 0.034990, acc.: 99.22%] [G loss: 3.735928]\n",
      "epoch:35 step:33020 [D loss: 0.087377, acc.: 99.22%] [G loss: 2.459060]\n",
      "epoch:35 step:33021 [D loss: 0.389380, acc.: 82.03%] [G loss: 2.879424]\n",
      "epoch:35 step:33022 [D loss: 0.309393, acc.: 94.53%] [G loss: 3.335177]\n",
      "epoch:35 step:33023 [D loss: 0.328519, acc.: 84.38%] [G loss: 3.130981]\n",
      "epoch:35 step:33024 [D loss: 0.153006, acc.: 95.31%] [G loss: 4.253079]\n",
      "epoch:35 step:33025 [D loss: 0.173647, acc.: 96.09%] [G loss: 0.862383]\n",
      "epoch:35 step:33026 [D loss: 0.046913, acc.: 99.22%] [G loss: 5.468588]\n",
      "epoch:35 step:33027 [D loss: 0.029714, acc.: 99.22%] [G loss: 2.983801]\n",
      "epoch:35 step:33028 [D loss: 0.024797, acc.: 100.00%] [G loss: 2.247885]\n",
      "epoch:35 step:33029 [D loss: 0.371677, acc.: 78.91%] [G loss: 4.971740]\n",
      "epoch:35 step:33030 [D loss: 0.108119, acc.: 96.88%] [G loss: 0.883516]\n",
      "epoch:35 step:33031 [D loss: 0.042109, acc.: 98.44%] [G loss: 4.172662]\n",
      "epoch:35 step:33032 [D loss: 0.086686, acc.: 98.44%] [G loss: 3.072474]\n",
      "epoch:35 step:33033 [D loss: 0.063730, acc.: 100.00%] [G loss: 4.501510]\n",
      "epoch:35 step:33034 [D loss: 0.081658, acc.: 100.00%] [G loss: 6.804643]\n",
      "epoch:35 step:33035 [D loss: 0.267960, acc.: 94.53%] [G loss: 1.815738]\n",
      "epoch:35 step:33036 [D loss: 0.483012, acc.: 70.31%] [G loss: 1.645378]\n",
      "epoch:35 step:33037 [D loss: 0.871435, acc.: 54.69%] [G loss: 3.148587]\n",
      "epoch:35 step:33038 [D loss: 0.130819, acc.: 97.66%] [G loss: 6.368389]\n",
      "epoch:35 step:33039 [D loss: 0.576417, acc.: 67.19%] [G loss: 1.813806]\n",
      "epoch:35 step:33040 [D loss: 0.063176, acc.: 100.00%] [G loss: 2.448603]\n",
      "epoch:35 step:33041 [D loss: 0.170344, acc.: 96.09%] [G loss: 0.278691]\n",
      "epoch:35 step:33042 [D loss: 1.260525, acc.: 53.91%] [G loss: 3.622754]\n",
      "epoch:35 step:33043 [D loss: 0.145697, acc.: 96.88%] [G loss: 1.156150]\n",
      "epoch:35 step:33044 [D loss: 0.341163, acc.: 82.03%] [G loss: 5.436910]\n",
      "epoch:35 step:33045 [D loss: 0.082271, acc.: 98.44%] [G loss: 1.068352]\n",
      "epoch:35 step:33046 [D loss: 0.024801, acc.: 100.00%] [G loss: 3.181836]\n",
      "epoch:35 step:33047 [D loss: 0.052624, acc.: 100.00%] [G loss: 2.906041]\n",
      "epoch:35 step:33048 [D loss: 0.103338, acc.: 100.00%] [G loss: 3.113573]\n",
      "epoch:35 step:33049 [D loss: 0.857574, acc.: 49.22%] [G loss: 0.968943]\n",
      "epoch:35 step:33050 [D loss: 0.197503, acc.: 94.53%] [G loss: 3.950829]\n",
      "epoch:35 step:33051 [D loss: 0.417756, acc.: 88.28%] [G loss: 2.881493]\n",
      "epoch:35 step:33052 [D loss: 0.407372, acc.: 76.56%] [G loss: 1.043569]\n",
      "epoch:35 step:33053 [D loss: 0.093943, acc.: 98.44%] [G loss: 4.409487]\n",
      "epoch:35 step:33054 [D loss: 0.969534, acc.: 57.03%] [G loss: 2.223811]\n",
      "epoch:35 step:33055 [D loss: 0.033760, acc.: 100.00%] [G loss: 4.987017]\n",
      "epoch:35 step:33056 [D loss: 0.797160, acc.: 57.03%] [G loss: 0.967363]\n",
      "epoch:35 step:33057 [D loss: 0.129251, acc.: 97.66%] [G loss: 3.664145]\n",
      "epoch:35 step:33058 [D loss: 0.247368, acc.: 92.19%] [G loss: 2.712814]\n",
      "epoch:35 step:33059 [D loss: 0.033583, acc.: 100.00%] [G loss: 1.943313]\n",
      "epoch:35 step:33060 [D loss: 0.066274, acc.: 100.00%] [G loss: 3.276105]\n",
      "epoch:35 step:33061 [D loss: 0.066080, acc.: 99.22%] [G loss: 3.051336]\n",
      "epoch:35 step:33062 [D loss: 0.092970, acc.: 99.22%] [G loss: 3.162490]\n",
      "epoch:35 step:33063 [D loss: 0.531652, acc.: 75.00%] [G loss: 1.871260]\n",
      "epoch:35 step:33064 [D loss: 0.043474, acc.: 99.22%] [G loss: 2.181053]\n",
      "epoch:35 step:33065 [D loss: 0.326637, acc.: 88.28%] [G loss: 3.270772]\n",
      "epoch:35 step:33066 [D loss: 0.192846, acc.: 94.53%] [G loss: 5.009100]\n",
      "epoch:35 step:33067 [D loss: 0.071680, acc.: 98.44%] [G loss: 0.365971]\n",
      "epoch:35 step:33068 [D loss: 0.057374, acc.: 100.00%] [G loss: 0.669645]\n",
      "epoch:35 step:33069 [D loss: 0.100622, acc.: 98.44%] [G loss: 2.864130]\n",
      "epoch:35 step:33070 [D loss: 0.084271, acc.: 100.00%] [G loss: 3.757192]\n",
      "epoch:35 step:33071 [D loss: 0.089419, acc.: 97.66%] [G loss: 4.924547]\n",
      "epoch:35 step:33072 [D loss: 0.069601, acc.: 98.44%] [G loss: 5.928702]\n",
      "epoch:35 step:33073 [D loss: 0.690877, acc.: 62.50%] [G loss: 1.933498]\n",
      "epoch:35 step:33074 [D loss: 0.232873, acc.: 92.19%] [G loss: 2.098896]\n",
      "epoch:35 step:33075 [D loss: 0.048044, acc.: 100.00%] [G loss: 2.260628]\n",
      "epoch:35 step:33076 [D loss: 0.669679, acc.: 64.06%] [G loss: 2.191246]\n",
      "epoch:35 step:33077 [D loss: 0.172459, acc.: 97.66%] [G loss: 2.454664]\n",
      "epoch:35 step:33078 [D loss: 0.127439, acc.: 96.09%] [G loss: 3.277167]\n",
      "epoch:35 step:33079 [D loss: 0.228331, acc.: 93.75%] [G loss: 0.935902]\n",
      "epoch:35 step:33080 [D loss: 0.537580, acc.: 72.66%] [G loss: 2.308173]\n",
      "epoch:35 step:33081 [D loss: 0.095948, acc.: 100.00%] [G loss: 3.237805]\n",
      "epoch:35 step:33082 [D loss: 0.121501, acc.: 97.66%] [G loss: 6.099164]\n",
      "epoch:35 step:33083 [D loss: 0.164607, acc.: 96.88%] [G loss: 5.764520]\n",
      "epoch:35 step:33084 [D loss: 1.151153, acc.: 51.56%] [G loss: 3.299970]\n",
      "epoch:35 step:33085 [D loss: 0.227837, acc.: 91.41%] [G loss: 2.872054]\n",
      "epoch:35 step:33086 [D loss: 0.012248, acc.: 100.00%] [G loss: 3.269807]\n",
      "epoch:35 step:33087 [D loss: 0.401111, acc.: 75.00%] [G loss: 2.673179]\n",
      "epoch:35 step:33088 [D loss: 0.257361, acc.: 93.75%] [G loss: 4.875895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33089 [D loss: 1.959644, acc.: 10.94%] [G loss: 0.648255]\n",
      "epoch:35 step:33090 [D loss: 0.235061, acc.: 92.19%] [G loss: 1.555258]\n",
      "epoch:35 step:33091 [D loss: 0.314856, acc.: 85.16%] [G loss: 3.122515]\n",
      "epoch:35 step:33092 [D loss: 0.225289, acc.: 96.09%] [G loss: 1.132365]\n",
      "epoch:35 step:33093 [D loss: 0.298802, acc.: 85.16%] [G loss: 2.391765]\n",
      "epoch:35 step:33094 [D loss: 0.544020, acc.: 67.97%] [G loss: 2.212389]\n",
      "epoch:35 step:33095 [D loss: 0.555208, acc.: 71.88%] [G loss: 1.534832]\n",
      "epoch:35 step:33096 [D loss: 0.200548, acc.: 92.19%] [G loss: 1.062743]\n",
      "epoch:35 step:33097 [D loss: 0.175192, acc.: 96.88%] [G loss: 1.059143]\n",
      "epoch:35 step:33098 [D loss: 0.113847, acc.: 97.66%] [G loss: 1.639296]\n",
      "epoch:35 step:33099 [D loss: 0.346595, acc.: 86.72%] [G loss: 1.171458]\n",
      "epoch:35 step:33100 [D loss: 0.810247, acc.: 57.03%] [G loss: 3.795776]\n",
      "epoch:35 step:33101 [D loss: 0.164415, acc.: 95.31%] [G loss: 5.339987]\n",
      "epoch:35 step:33102 [D loss: 0.184412, acc.: 96.88%] [G loss: 3.455363]\n",
      "epoch:35 step:33103 [D loss: 0.567171, acc.: 68.75%] [G loss: 3.405740]\n",
      "epoch:35 step:33104 [D loss: 0.134933, acc.: 96.88%] [G loss: 2.194144]\n",
      "epoch:35 step:33105 [D loss: 0.197585, acc.: 92.19%] [G loss: 1.944152]\n",
      "epoch:35 step:33106 [D loss: 0.027531, acc.: 99.22%] [G loss: 2.649802]\n",
      "epoch:35 step:33107 [D loss: 0.207844, acc.: 95.31%] [G loss: 0.821418]\n",
      "epoch:35 step:33108 [D loss: 0.065221, acc.: 99.22%] [G loss: 3.247867]\n",
      "epoch:35 step:33109 [D loss: 0.046863, acc.: 99.22%] [G loss: 5.435428]\n",
      "epoch:35 step:33110 [D loss: 0.260209, acc.: 91.41%] [G loss: 2.782975]\n",
      "epoch:35 step:33111 [D loss: 0.040148, acc.: 98.44%] [G loss: 4.455408]\n",
      "epoch:35 step:33112 [D loss: 0.469059, acc.: 77.34%] [G loss: 4.459454]\n",
      "epoch:35 step:33113 [D loss: 0.541209, acc.: 71.09%] [G loss: 2.198627]\n",
      "epoch:35 step:33114 [D loss: 0.074179, acc.: 99.22%] [G loss: 2.345067]\n",
      "epoch:35 step:33115 [D loss: 0.037406, acc.: 99.22%] [G loss: 1.571804]\n",
      "epoch:35 step:33116 [D loss: 0.323335, acc.: 89.84%] [G loss: 2.421449]\n",
      "epoch:35 step:33117 [D loss: 0.062803, acc.: 98.44%] [G loss: 2.941919]\n",
      "epoch:35 step:33118 [D loss: 0.069657, acc.: 99.22%] [G loss: 3.414067]\n",
      "epoch:35 step:33119 [D loss: 0.683482, acc.: 66.41%] [G loss: 8.450319]\n",
      "epoch:35 step:33120 [D loss: 0.319531, acc.: 82.03%] [G loss: 2.101263]\n",
      "epoch:35 step:33121 [D loss: 0.139509, acc.: 98.44%] [G loss: 2.433567]\n",
      "epoch:35 step:33122 [D loss: 1.277324, acc.: 53.12%] [G loss: 0.662690]\n",
      "epoch:35 step:33123 [D loss: 0.290446, acc.: 84.38%] [G loss: 0.785089]\n",
      "epoch:35 step:33124 [D loss: 0.244081, acc.: 94.53%] [G loss: 2.205499]\n",
      "epoch:35 step:33125 [D loss: 0.020772, acc.: 100.00%] [G loss: 4.577522]\n",
      "epoch:35 step:33126 [D loss: 0.026752, acc.: 100.00%] [G loss: 2.062410]\n",
      "epoch:35 step:33127 [D loss: 0.137451, acc.: 96.88%] [G loss: 4.464844]\n",
      "epoch:35 step:33128 [D loss: 0.834614, acc.: 59.38%] [G loss: 2.009487]\n",
      "epoch:35 step:33129 [D loss: 0.074776, acc.: 99.22%] [G loss: 3.583840]\n",
      "epoch:35 step:33130 [D loss: 0.776938, acc.: 58.59%] [G loss: 3.492451]\n",
      "epoch:35 step:33131 [D loss: 0.502249, acc.: 68.75%] [G loss: 2.679035]\n",
      "epoch:35 step:33132 [D loss: 0.180224, acc.: 95.31%] [G loss: 6.881096]\n",
      "epoch:35 step:33133 [D loss: 0.246330, acc.: 93.75%] [G loss: 3.285554]\n",
      "epoch:35 step:33134 [D loss: 0.573711, acc.: 67.97%] [G loss: 1.800817]\n",
      "epoch:35 step:33135 [D loss: 0.673874, acc.: 61.72%] [G loss: 1.261313]\n",
      "epoch:35 step:33136 [D loss: 0.437109, acc.: 71.88%] [G loss: 3.666439]\n",
      "epoch:35 step:33137 [D loss: 0.741332, acc.: 59.38%] [G loss: 2.810723]\n",
      "epoch:35 step:33138 [D loss: 0.027503, acc.: 99.22%] [G loss: 2.777001]\n",
      "epoch:35 step:33139 [D loss: 0.054080, acc.: 98.44%] [G loss: 3.575438]\n",
      "epoch:35 step:33140 [D loss: 0.203498, acc.: 95.31%] [G loss: 1.947643]\n",
      "epoch:35 step:33141 [D loss: 0.012233, acc.: 100.00%] [G loss: 3.235897]\n",
      "epoch:35 step:33142 [D loss: 0.126863, acc.: 97.66%] [G loss: 4.360359]\n",
      "epoch:35 step:33143 [D loss: 0.131915, acc.: 97.66%] [G loss: 3.008572]\n",
      "epoch:35 step:33144 [D loss: 0.538948, acc.: 71.09%] [G loss: 2.919569]\n",
      "epoch:35 step:33145 [D loss: 0.133752, acc.: 96.88%] [G loss: 2.782760]\n",
      "epoch:35 step:33146 [D loss: 0.172647, acc.: 97.66%] [G loss: 0.848770]\n",
      "epoch:35 step:33147 [D loss: 0.122514, acc.: 96.88%] [G loss: 2.438444]\n",
      "epoch:35 step:33148 [D loss: 0.086876, acc.: 99.22%] [G loss: 2.483504]\n",
      "epoch:35 step:33149 [D loss: 0.617419, acc.: 68.75%] [G loss: 3.641334]\n",
      "epoch:35 step:33150 [D loss: 0.400286, acc.: 75.78%] [G loss: 4.155304]\n",
      "epoch:35 step:33151 [D loss: 0.587114, acc.: 64.06%] [G loss: 2.884628]\n",
      "epoch:35 step:33152 [D loss: 0.154647, acc.: 97.66%] [G loss: 4.750308]\n",
      "epoch:35 step:33153 [D loss: 0.213167, acc.: 94.53%] [G loss: 1.788766]\n",
      "epoch:35 step:33154 [D loss: 0.122406, acc.: 99.22%] [G loss: 4.707273]\n",
      "epoch:35 step:33155 [D loss: 0.142667, acc.: 93.75%] [G loss: 3.854824]\n",
      "epoch:35 step:33156 [D loss: 0.067314, acc.: 99.22%] [G loss: 2.179327]\n",
      "epoch:35 step:33157 [D loss: 0.157076, acc.: 95.31%] [G loss: 3.090423]\n",
      "epoch:35 step:33158 [D loss: 0.181838, acc.: 95.31%] [G loss: 1.034639]\n",
      "epoch:35 step:33159 [D loss: 0.780868, acc.: 53.12%] [G loss: 1.192839]\n",
      "epoch:35 step:33160 [D loss: 0.307310, acc.: 85.94%] [G loss: 0.669576]\n",
      "epoch:35 step:33161 [D loss: 0.048506, acc.: 98.44%] [G loss: 0.558860]\n",
      "epoch:35 step:33162 [D loss: 0.175247, acc.: 93.75%] [G loss: 5.253452]\n",
      "epoch:35 step:33163 [D loss: 0.056520, acc.: 100.00%] [G loss: 3.776517]\n",
      "epoch:35 step:33164 [D loss: 0.079368, acc.: 99.22%] [G loss: 3.633143]\n",
      "epoch:35 step:33165 [D loss: 0.556932, acc.: 67.19%] [G loss: 1.954347]\n",
      "epoch:35 step:33166 [D loss: 0.072204, acc.: 99.22%] [G loss: 2.915456]\n",
      "epoch:35 step:33167 [D loss: 0.026919, acc.: 100.00%] [G loss: 4.122210]\n",
      "epoch:35 step:33168 [D loss: 0.073529, acc.: 99.22%] [G loss: 5.277249]\n",
      "epoch:35 step:33169 [D loss: 0.231378, acc.: 95.31%] [G loss: 2.940398]\n",
      "epoch:35 step:33170 [D loss: 0.533682, acc.: 71.88%] [G loss: 2.891189]\n",
      "epoch:35 step:33171 [D loss: 1.473017, acc.: 49.22%] [G loss: 6.064169]\n",
      "epoch:35 step:33172 [D loss: 0.297718, acc.: 89.84%] [G loss: 5.564246]\n",
      "epoch:35 step:33173 [D loss: 0.465273, acc.: 71.09%] [G loss: 3.952497]\n",
      "epoch:35 step:33174 [D loss: 0.114927, acc.: 98.44%] [G loss: 1.594622]\n",
      "epoch:35 step:33175 [D loss: 0.656524, acc.: 62.50%] [G loss: 4.153919]\n",
      "epoch:35 step:33176 [D loss: 0.428357, acc.: 80.47%] [G loss: 2.788765]\n",
      "epoch:35 step:33177 [D loss: 0.388541, acc.: 75.78%] [G loss: 3.752805]\n",
      "epoch:35 step:33178 [D loss: 0.057205, acc.: 99.22%] [G loss: 2.611959]\n",
      "epoch:35 step:33179 [D loss: 0.989457, acc.: 49.22%] [G loss: 3.046779]\n",
      "epoch:35 step:33180 [D loss: 0.064437, acc.: 99.22%] [G loss: 2.192868]\n",
      "epoch:35 step:33181 [D loss: 0.153725, acc.: 99.22%] [G loss: 3.401147]\n",
      "epoch:35 step:33182 [D loss: 0.120005, acc.: 98.44%] [G loss: 6.607463]\n",
      "epoch:35 step:33183 [D loss: 0.305921, acc.: 87.50%] [G loss: 2.203803]\n",
      "epoch:35 step:33184 [D loss: 0.706256, acc.: 63.28%] [G loss: 4.235756]\n",
      "epoch:35 step:33185 [D loss: 0.160750, acc.: 96.88%] [G loss: 5.534285]\n",
      "epoch:35 step:33186 [D loss: 0.942974, acc.: 59.38%] [G loss: 2.254603]\n",
      "epoch:35 step:33187 [D loss: 0.106260, acc.: 97.66%] [G loss: 1.545877]\n",
      "epoch:35 step:33188 [D loss: 0.229091, acc.: 92.19%] [G loss: 0.992788]\n",
      "epoch:35 step:33189 [D loss: 0.149631, acc.: 95.31%] [G loss: 1.393485]\n",
      "epoch:35 step:33190 [D loss: 0.359729, acc.: 87.50%] [G loss: 2.432345]\n",
      "epoch:35 step:33191 [D loss: 0.060055, acc.: 100.00%] [G loss: 6.423427]\n",
      "epoch:35 step:33192 [D loss: 1.113980, acc.: 51.56%] [G loss: 1.993921]\n",
      "epoch:35 step:33193 [D loss: 0.135167, acc.: 98.44%] [G loss: 2.372201]\n",
      "epoch:35 step:33194 [D loss: 0.212623, acc.: 95.31%] [G loss: 2.652437]\n",
      "epoch:35 step:33195 [D loss: 0.355787, acc.: 85.94%] [G loss: 2.756333]\n",
      "epoch:35 step:33196 [D loss: 0.243470, acc.: 90.62%] [G loss: 4.254085]\n",
      "epoch:35 step:33197 [D loss: 0.194179, acc.: 93.75%] [G loss: 4.147977]\n",
      "epoch:35 step:33198 [D loss: 0.326396, acc.: 84.38%] [G loss: 3.332471]\n",
      "epoch:35 step:33199 [D loss: 0.118362, acc.: 97.66%] [G loss: 2.208239]\n",
      "epoch:35 step:33200 [D loss: 0.091926, acc.: 97.66%] [G loss: 3.081671]\n",
      "epoch:35 step:33201 [D loss: 0.138739, acc.: 97.66%] [G loss: 2.192257]\n",
      "epoch:35 step:33202 [D loss: 0.047617, acc.: 98.44%] [G loss: 6.985281]\n",
      "epoch:35 step:33203 [D loss: 0.061376, acc.: 100.00%] [G loss: 1.571456]\n",
      "epoch:35 step:33204 [D loss: 0.064270, acc.: 99.22%] [G loss: 3.104492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33205 [D loss: 0.395075, acc.: 85.94%] [G loss: 1.457967]\n",
      "epoch:35 step:33206 [D loss: 0.133630, acc.: 98.44%] [G loss: 2.786158]\n",
      "epoch:35 step:33207 [D loss: 0.260358, acc.: 85.94%] [G loss: 5.908596]\n",
      "epoch:35 step:33208 [D loss: 0.025078, acc.: 99.22%] [G loss: 0.842014]\n",
      "epoch:35 step:33209 [D loss: 0.115632, acc.: 96.88%] [G loss: 4.684701]\n",
      "epoch:35 step:33210 [D loss: 0.096763, acc.: 99.22%] [G loss: 3.433840]\n",
      "epoch:35 step:33211 [D loss: 0.059833, acc.: 100.00%] [G loss: 3.531376]\n",
      "epoch:35 step:33212 [D loss: 0.242029, acc.: 94.53%] [G loss: 3.949348]\n",
      "epoch:35 step:33213 [D loss: 0.063250, acc.: 99.22%] [G loss: 1.807078]\n",
      "epoch:35 step:33214 [D loss: 0.044765, acc.: 98.44%] [G loss: 3.323889]\n",
      "epoch:35 step:33215 [D loss: 0.266312, acc.: 87.50%] [G loss: 3.072078]\n",
      "epoch:35 step:33216 [D loss: 0.582100, acc.: 61.72%] [G loss: 7.281324]\n",
      "epoch:35 step:33217 [D loss: 0.397667, acc.: 75.78%] [G loss: 5.797770]\n",
      "epoch:35 step:33218 [D loss: 0.234156, acc.: 90.62%] [G loss: 4.995321]\n",
      "epoch:35 step:33219 [D loss: 0.493305, acc.: 73.44%] [G loss: 1.938733]\n",
      "epoch:35 step:33220 [D loss: 0.209058, acc.: 92.19%] [G loss: 0.869443]\n",
      "epoch:35 step:33221 [D loss: 0.128122, acc.: 98.44%] [G loss: 4.089043]\n",
      "epoch:35 step:33222 [D loss: 0.073538, acc.: 97.66%] [G loss: 0.898750]\n",
      "epoch:35 step:33223 [D loss: 1.115238, acc.: 57.81%] [G loss: 3.744288]\n",
      "epoch:35 step:33224 [D loss: 0.730308, acc.: 62.50%] [G loss: 0.833857]\n",
      "epoch:35 step:33225 [D loss: 0.085926, acc.: 100.00%] [G loss: 3.327987]\n",
      "epoch:35 step:33226 [D loss: 0.225681, acc.: 90.62%] [G loss: 3.508834]\n",
      "epoch:35 step:33227 [D loss: 0.144559, acc.: 95.31%] [G loss: 0.362793]\n",
      "epoch:35 step:33228 [D loss: 0.679018, acc.: 62.50%] [G loss: 3.340085]\n",
      "epoch:35 step:33229 [D loss: 0.303420, acc.: 89.84%] [G loss: 3.070409]\n",
      "epoch:35 step:33230 [D loss: 0.160622, acc.: 97.66%] [G loss: 4.435541]\n",
      "epoch:35 step:33231 [D loss: 0.269071, acc.: 91.41%] [G loss: 4.573312]\n",
      "epoch:35 step:33232 [D loss: 0.026082, acc.: 100.00%] [G loss: 3.396736]\n",
      "epoch:35 step:33233 [D loss: 0.071039, acc.: 100.00%] [G loss: 2.062758]\n",
      "epoch:35 step:33234 [D loss: 0.128063, acc.: 97.66%] [G loss: 2.761028]\n",
      "epoch:35 step:33235 [D loss: 0.807017, acc.: 57.03%] [G loss: 2.716111]\n",
      "epoch:35 step:33236 [D loss: 0.076815, acc.: 99.22%] [G loss: 1.588210]\n",
      "epoch:35 step:33237 [D loss: 0.290951, acc.: 90.62%] [G loss: 1.621294]\n",
      "epoch:35 step:33238 [D loss: 0.053531, acc.: 100.00%] [G loss: 1.866385]\n",
      "epoch:35 step:33239 [D loss: 0.452599, acc.: 76.56%] [G loss: 2.906577]\n",
      "epoch:35 step:33240 [D loss: 0.545351, acc.: 69.53%] [G loss: 1.680877]\n",
      "epoch:35 step:33241 [D loss: 0.253536, acc.: 92.19%] [G loss: 1.996280]\n",
      "epoch:35 step:33242 [D loss: 0.099797, acc.: 99.22%] [G loss: 4.380259]\n",
      "epoch:35 step:33243 [D loss: 0.256127, acc.: 91.41%] [G loss: 4.580999]\n",
      "epoch:35 step:33244 [D loss: 0.493085, acc.: 76.56%] [G loss: 2.251601]\n",
      "epoch:35 step:33245 [D loss: 0.290737, acc.: 89.84%] [G loss: 3.037240]\n",
      "epoch:35 step:33246 [D loss: 0.204288, acc.: 95.31%] [G loss: 0.201146]\n",
      "epoch:35 step:33247 [D loss: 0.045149, acc.: 98.44%] [G loss: 5.578281]\n",
      "epoch:35 step:33248 [D loss: 0.190419, acc.: 95.31%] [G loss: 3.096649]\n",
      "epoch:35 step:33249 [D loss: 0.224571, acc.: 96.09%] [G loss: 2.897778]\n",
      "epoch:35 step:33250 [D loss: 1.017815, acc.: 53.12%] [G loss: 5.106581]\n",
      "epoch:35 step:33251 [D loss: 0.303970, acc.: 84.38%] [G loss: 3.121742]\n",
      "epoch:35 step:33252 [D loss: 1.062829, acc.: 59.38%] [G loss: 2.637829]\n",
      "epoch:35 step:33253 [D loss: 0.166121, acc.: 95.31%] [G loss: 1.753734]\n",
      "epoch:35 step:33254 [D loss: 0.361690, acc.: 77.34%] [G loss: 3.715656]\n",
      "epoch:35 step:33255 [D loss: 0.292200, acc.: 89.84%] [G loss: 3.201767]\n",
      "epoch:35 step:33256 [D loss: 0.143036, acc.: 98.44%] [G loss: 4.042687]\n",
      "epoch:35 step:33257 [D loss: 0.110215, acc.: 96.09%] [G loss: 2.251457]\n",
      "epoch:35 step:33258 [D loss: 0.596751, acc.: 71.09%] [G loss: 5.480900]\n",
      "epoch:35 step:33259 [D loss: 0.415304, acc.: 75.78%] [G loss: 5.034238]\n",
      "epoch:35 step:33260 [D loss: 0.502882, acc.: 72.66%] [G loss: 1.898894]\n",
      "epoch:35 step:33261 [D loss: 0.030923, acc.: 100.00%] [G loss: 1.680682]\n",
      "epoch:35 step:33262 [D loss: 0.024076, acc.: 100.00%] [G loss: 3.306396]\n",
      "epoch:35 step:33263 [D loss: 0.160555, acc.: 94.53%] [G loss: 4.689353]\n",
      "epoch:35 step:33264 [D loss: 0.045806, acc.: 100.00%] [G loss: 4.068934]\n",
      "epoch:35 step:33265 [D loss: 0.122169, acc.: 99.22%] [G loss: 1.649282]\n",
      "epoch:35 step:33266 [D loss: 0.138941, acc.: 95.31%] [G loss: 1.398232]\n",
      "epoch:35 step:33267 [D loss: 0.529600, acc.: 75.00%] [G loss: 3.151086]\n",
      "epoch:35 step:33268 [D loss: 0.101752, acc.: 99.22%] [G loss: 1.324192]\n",
      "epoch:35 step:33269 [D loss: 0.653977, acc.: 64.84%] [G loss: 2.482201]\n",
      "epoch:35 step:33270 [D loss: 0.615193, acc.: 64.06%] [G loss: 4.560810]\n",
      "epoch:35 step:33271 [D loss: 0.053113, acc.: 97.66%] [G loss: 3.841952]\n",
      "epoch:35 step:33272 [D loss: 0.009563, acc.: 100.00%] [G loss: 6.208805]\n",
      "epoch:35 step:33273 [D loss: 0.055335, acc.: 99.22%] [G loss: 4.159529]\n",
      "epoch:35 step:33274 [D loss: 0.316103, acc.: 84.38%] [G loss: 0.576288]\n",
      "epoch:35 step:33275 [D loss: 0.191313, acc.: 95.31%] [G loss: 1.868427]\n",
      "epoch:35 step:33276 [D loss: 0.111852, acc.: 98.44%] [G loss: 5.705682]\n",
      "epoch:35 step:33277 [D loss: 0.283544, acc.: 95.31%] [G loss: 2.828671]\n",
      "epoch:35 step:33278 [D loss: 0.682621, acc.: 63.28%] [G loss: 2.949573]\n",
      "epoch:35 step:33279 [D loss: 0.021414, acc.: 99.22%] [G loss: 5.788385]\n",
      "epoch:35 step:33280 [D loss: 0.251617, acc.: 92.97%] [G loss: 1.071810]\n",
      "epoch:35 step:33281 [D loss: 0.126264, acc.: 97.66%] [G loss: 1.964913]\n",
      "epoch:35 step:33282 [D loss: 0.506737, acc.: 71.09%] [G loss: 1.315054]\n",
      "epoch:35 step:33283 [D loss: 0.225474, acc.: 95.31%] [G loss: 4.177213]\n",
      "epoch:35 step:33284 [D loss: 1.005409, acc.: 36.72%] [G loss: 8.291679]\n",
      "epoch:35 step:33285 [D loss: 0.036347, acc.: 100.00%] [G loss: 5.022464]\n",
      "epoch:35 step:33286 [D loss: 0.200255, acc.: 96.09%] [G loss: 3.147645]\n",
      "epoch:35 step:33287 [D loss: 0.046778, acc.: 99.22%] [G loss: 1.614225]\n",
      "epoch:35 step:33288 [D loss: 0.499080, acc.: 74.22%] [G loss: 2.759677]\n",
      "epoch:35 step:33289 [D loss: 0.716839, acc.: 58.59%] [G loss: 1.510848]\n",
      "epoch:35 step:33290 [D loss: 0.025322, acc.: 99.22%] [G loss: 3.281531]\n",
      "epoch:35 step:33291 [D loss: 0.132230, acc.: 95.31%] [G loss: 3.949425]\n",
      "epoch:35 step:33292 [D loss: 0.110096, acc.: 98.44%] [G loss: 1.752886]\n",
      "epoch:35 step:33293 [D loss: 0.326075, acc.: 80.47%] [G loss: 1.619863]\n",
      "epoch:35 step:33294 [D loss: 0.106077, acc.: 96.88%] [G loss: 4.680043]\n",
      "epoch:35 step:33295 [D loss: 0.161037, acc.: 97.66%] [G loss: 1.806738]\n",
      "epoch:35 step:33296 [D loss: 1.529046, acc.: 50.78%] [G loss: 3.634632]\n",
      "epoch:35 step:33297 [D loss: 0.578005, acc.: 67.97%] [G loss: 1.884171]\n",
      "epoch:35 step:33298 [D loss: 0.241965, acc.: 90.62%] [G loss: 0.429068]\n",
      "epoch:35 step:33299 [D loss: 0.135844, acc.: 98.44%] [G loss: 5.156913]\n",
      "epoch:35 step:33300 [D loss: 0.008388, acc.: 100.00%] [G loss: 1.768615]\n",
      "epoch:35 step:33301 [D loss: 0.233406, acc.: 86.72%] [G loss: 2.259589]\n",
      "epoch:35 step:33302 [D loss: 0.111924, acc.: 99.22%] [G loss: 4.869955]\n",
      "epoch:35 step:33303 [D loss: 0.739666, acc.: 61.72%] [G loss: 7.660951]\n",
      "epoch:35 step:33304 [D loss: 0.374421, acc.: 78.12%] [G loss: 6.981615]\n",
      "epoch:35 step:33305 [D loss: 0.019660, acc.: 100.00%] [G loss: 3.988043]\n",
      "epoch:35 step:33306 [D loss: 1.682942, acc.: 31.25%] [G loss: 2.430365]\n",
      "epoch:35 step:33307 [D loss: 0.127533, acc.: 100.00%] [G loss: 1.892378]\n",
      "epoch:35 step:33308 [D loss: 0.155009, acc.: 96.88%] [G loss: 2.931864]\n",
      "epoch:35 step:33309 [D loss: 0.307743, acc.: 87.50%] [G loss: 5.221726]\n",
      "epoch:35 step:33310 [D loss: 0.271071, acc.: 83.59%] [G loss: 5.179414]\n",
      "epoch:35 step:33311 [D loss: 0.451744, acc.: 83.59%] [G loss: 3.498817]\n",
      "epoch:35 step:33312 [D loss: 0.039979, acc.: 100.00%] [G loss: 3.347504]\n",
      "epoch:35 step:33313 [D loss: 0.407021, acc.: 81.25%] [G loss: 1.608479]\n",
      "epoch:35 step:33314 [D loss: 0.393409, acc.: 85.94%] [G loss: 1.432672]\n",
      "epoch:35 step:33315 [D loss: 0.035314, acc.: 100.00%] [G loss: 2.551127]\n",
      "epoch:35 step:33316 [D loss: 0.349542, acc.: 86.72%] [G loss: 3.735528]\n",
      "epoch:35 step:33317 [D loss: 0.090510, acc.: 98.44%] [G loss: 2.556657]\n",
      "epoch:35 step:33318 [D loss: 0.348651, acc.: 85.94%] [G loss: 5.010859]\n",
      "epoch:35 step:33319 [D loss: 0.389039, acc.: 82.03%] [G loss: 4.750209]\n",
      "epoch:35 step:33320 [D loss: 0.105553, acc.: 98.44%] [G loss: 5.871239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33321 [D loss: 0.386145, acc.: 81.25%] [G loss: 0.823521]\n",
      "epoch:35 step:33322 [D loss: 0.271563, acc.: 88.28%] [G loss: 4.443670]\n",
      "epoch:35 step:33323 [D loss: 0.065955, acc.: 99.22%] [G loss: 3.602727]\n",
      "epoch:35 step:33324 [D loss: 0.123829, acc.: 99.22%] [G loss: 3.013761]\n",
      "epoch:35 step:33325 [D loss: 0.283596, acc.: 83.59%] [G loss: 2.136061]\n",
      "epoch:35 step:33326 [D loss: 0.620980, acc.: 70.31%] [G loss: 6.000933]\n",
      "epoch:35 step:33327 [D loss: 0.085498, acc.: 98.44%] [G loss: 4.293965]\n",
      "epoch:35 step:33328 [D loss: 0.150317, acc.: 94.53%] [G loss: 6.470469]\n",
      "epoch:35 step:33329 [D loss: 0.550634, acc.: 70.31%] [G loss: 3.803528]\n",
      "epoch:35 step:33330 [D loss: 0.177177, acc.: 96.88%] [G loss: 5.299110]\n",
      "epoch:35 step:33331 [D loss: 0.125512, acc.: 98.44%] [G loss: 5.127851]\n",
      "epoch:35 step:33332 [D loss: 0.154128, acc.: 94.53%] [G loss: 1.762331]\n",
      "epoch:35 step:33333 [D loss: 0.057329, acc.: 99.22%] [G loss: 1.659713]\n",
      "epoch:35 step:33334 [D loss: 0.019841, acc.: 100.00%] [G loss: 4.076879]\n",
      "epoch:35 step:33335 [D loss: 0.440476, acc.: 85.16%] [G loss: 3.525038]\n",
      "epoch:35 step:33336 [D loss: 0.554399, acc.: 72.66%] [G loss: 1.309477]\n",
      "epoch:35 step:33337 [D loss: 0.745295, acc.: 57.03%] [G loss: 1.311299]\n",
      "epoch:35 step:33338 [D loss: 0.925198, acc.: 56.25%] [G loss: 3.604069]\n",
      "epoch:35 step:33339 [D loss: 0.226529, acc.: 91.41%] [G loss: 12.125908]\n",
      "epoch:35 step:33340 [D loss: 0.242402, acc.: 87.50%] [G loss: 7.992116]\n",
      "epoch:35 step:33341 [D loss: 0.349418, acc.: 79.69%] [G loss: 4.467767]\n",
      "epoch:35 step:33342 [D loss: 0.301929, acc.: 83.59%] [G loss: 1.971315]\n",
      "epoch:35 step:33343 [D loss: 0.177893, acc.: 94.53%] [G loss: 3.071793]\n",
      "epoch:35 step:33344 [D loss: 0.191629, acc.: 92.97%] [G loss: 3.598209]\n",
      "epoch:35 step:33345 [D loss: 0.222065, acc.: 92.97%] [G loss: 1.959760]\n",
      "epoch:35 step:33346 [D loss: 0.424931, acc.: 85.16%] [G loss: 3.739814]\n",
      "epoch:35 step:33347 [D loss: 0.265184, acc.: 89.06%] [G loss: 3.372258]\n",
      "epoch:35 step:33348 [D loss: 0.253919, acc.: 93.75%] [G loss: 2.136765]\n",
      "epoch:35 step:33349 [D loss: 0.186585, acc.: 93.75%] [G loss: 2.275409]\n",
      "epoch:35 step:33350 [D loss: 0.494480, acc.: 73.44%] [G loss: 0.813797]\n",
      "epoch:35 step:33351 [D loss: 0.496878, acc.: 67.97%] [G loss: 3.197732]\n",
      "epoch:35 step:33352 [D loss: 0.092659, acc.: 97.66%] [G loss: 0.814872]\n",
      "epoch:35 step:33353 [D loss: 0.096872, acc.: 97.66%] [G loss: 5.933807]\n",
      "epoch:35 step:33354 [D loss: 0.114706, acc.: 99.22%] [G loss: 0.806348]\n",
      "epoch:35 step:33355 [D loss: 0.425597, acc.: 78.91%] [G loss: 2.647514]\n",
      "epoch:35 step:33356 [D loss: 0.439114, acc.: 72.66%] [G loss: 1.054462]\n",
      "epoch:35 step:33357 [D loss: 0.331891, acc.: 82.03%] [G loss: 2.127848]\n",
      "epoch:35 step:33358 [D loss: 0.090572, acc.: 99.22%] [G loss: 0.771347]\n",
      "epoch:35 step:33359 [D loss: 0.159328, acc.: 96.88%] [G loss: 5.038540]\n",
      "epoch:35 step:33360 [D loss: 0.194468, acc.: 93.75%] [G loss: 0.549617]\n",
      "epoch:35 step:33361 [D loss: 1.156054, acc.: 53.12%] [G loss: 4.065668]\n",
      "epoch:35 step:33362 [D loss: 0.120352, acc.: 96.88%] [G loss: 5.556203]\n",
      "epoch:35 step:33363 [D loss: 0.153684, acc.: 95.31%] [G loss: 7.852365]\n",
      "epoch:35 step:33364 [D loss: 0.056349, acc.: 100.00%] [G loss: 7.737725]\n",
      "epoch:35 step:33365 [D loss: 0.925928, acc.: 57.81%] [G loss: 1.493556]\n",
      "epoch:35 step:33366 [D loss: 0.037791, acc.: 99.22%] [G loss: 3.071482]\n",
      "epoch:35 step:33367 [D loss: 0.396215, acc.: 78.91%] [G loss: 0.569019]\n",
      "epoch:35 step:33368 [D loss: 0.111222, acc.: 97.66%] [G loss: 4.417505]\n",
      "epoch:35 step:33369 [D loss: 0.704220, acc.: 67.19%] [G loss: 0.902311]\n",
      "epoch:35 step:33370 [D loss: 0.063474, acc.: 99.22%] [G loss: 7.693275]\n",
      "epoch:35 step:33371 [D loss: 0.076627, acc.: 99.22%] [G loss: 4.543751]\n",
      "epoch:35 step:33372 [D loss: 0.175787, acc.: 94.53%] [G loss: 3.281658]\n",
      "epoch:35 step:33373 [D loss: 2.585754, acc.: 10.94%] [G loss: 1.358036]\n",
      "epoch:35 step:33374 [D loss: 0.049475, acc.: 99.22%] [G loss: 4.536154]\n",
      "epoch:35 step:33375 [D loss: 0.241321, acc.: 92.97%] [G loss: 7.086973]\n",
      "epoch:35 step:33376 [D loss: 0.131748, acc.: 98.44%] [G loss: 2.920131]\n",
      "epoch:35 step:33377 [D loss: 0.236891, acc.: 93.75%] [G loss: 4.162089]\n",
      "epoch:35 step:33378 [D loss: 0.137039, acc.: 97.66%] [G loss: 4.157513]\n",
      "epoch:35 step:33379 [D loss: 1.183698, acc.: 30.47%] [G loss: 4.267294]\n",
      "epoch:35 step:33380 [D loss: 0.303601, acc.: 83.59%] [G loss: 5.074893]\n",
      "epoch:35 step:33381 [D loss: 0.042300, acc.: 99.22%] [G loss: 3.737920]\n",
      "epoch:35 step:33382 [D loss: 0.053536, acc.: 100.00%] [G loss: 5.822870]\n",
      "epoch:35 step:33383 [D loss: 0.228104, acc.: 90.62%] [G loss: 4.119969]\n",
      "epoch:35 step:33384 [D loss: 1.016914, acc.: 53.91%] [G loss: 4.105630]\n",
      "epoch:35 step:33385 [D loss: 0.189660, acc.: 92.97%] [G loss: 4.293315]\n",
      "epoch:35 step:33386 [D loss: 0.172080, acc.: 96.09%] [G loss: 1.534533]\n",
      "epoch:35 step:33387 [D loss: 0.202222, acc.: 95.31%] [G loss: 3.751988]\n",
      "epoch:35 step:33388 [D loss: 0.130134, acc.: 95.31%] [G loss: 3.740471]\n",
      "epoch:35 step:33389 [D loss: 0.264533, acc.: 92.19%] [G loss: 3.317359]\n",
      "epoch:35 step:33390 [D loss: 0.033478, acc.: 100.00%] [G loss: 3.221699]\n",
      "epoch:35 step:33391 [D loss: 0.098392, acc.: 99.22%] [G loss: 4.470225]\n",
      "epoch:35 step:33392 [D loss: 0.531264, acc.: 76.56%] [G loss: 2.025152]\n",
      "epoch:35 step:33393 [D loss: 0.125325, acc.: 99.22%] [G loss: 1.736230]\n",
      "epoch:35 step:33394 [D loss: 0.037218, acc.: 100.00%] [G loss: 2.983774]\n",
      "epoch:35 step:33395 [D loss: 0.187873, acc.: 96.09%] [G loss: 4.411798]\n",
      "epoch:35 step:33396 [D loss: 0.299094, acc.: 88.28%] [G loss: 2.123678]\n",
      "epoch:35 step:33397 [D loss: 0.035251, acc.: 100.00%] [G loss: 3.117557]\n",
      "epoch:35 step:33398 [D loss: 0.609161, acc.: 69.53%] [G loss: 1.796328]\n",
      "epoch:35 step:33399 [D loss: 0.202772, acc.: 96.09%] [G loss: 2.352999]\n",
      "epoch:35 step:33400 [D loss: 0.414871, acc.: 73.44%] [G loss: 5.167879]\n",
      "epoch:35 step:33401 [D loss: 0.349098, acc.: 82.81%] [G loss: 2.841846]\n",
      "epoch:35 step:33402 [D loss: 0.293940, acc.: 92.19%] [G loss: 4.129027]\n",
      "epoch:35 step:33403 [D loss: 0.363166, acc.: 79.69%] [G loss: 6.157572]\n",
      "epoch:35 step:33404 [D loss: 0.165932, acc.: 96.09%] [G loss: 5.345779]\n",
      "epoch:35 step:33405 [D loss: 0.135829, acc.: 98.44%] [G loss: 3.053085]\n",
      "epoch:35 step:33406 [D loss: 0.650798, acc.: 62.50%] [G loss: 1.908775]\n",
      "epoch:35 step:33407 [D loss: 0.070285, acc.: 99.22%] [G loss: 2.431137]\n",
      "epoch:35 step:33408 [D loss: 0.127555, acc.: 98.44%] [G loss: 2.014516]\n",
      "epoch:35 step:33409 [D loss: 0.096969, acc.: 99.22%] [G loss: 2.630524]\n",
      "epoch:35 step:33410 [D loss: 0.110234, acc.: 96.88%] [G loss: 4.502439]\n",
      "epoch:35 step:33411 [D loss: 0.085579, acc.: 98.44%] [G loss: 2.798334]\n",
      "epoch:35 step:33412 [D loss: 0.353836, acc.: 81.25%] [G loss: 1.701321]\n",
      "epoch:35 step:33413 [D loss: 0.570612, acc.: 64.84%] [G loss: 6.029418]\n",
      "epoch:35 step:33414 [D loss: 0.356559, acc.: 79.69%] [G loss: 2.777403]\n",
      "epoch:35 step:33415 [D loss: 0.184602, acc.: 93.75%] [G loss: 6.706231]\n",
      "epoch:35 step:33416 [D loss: 0.137597, acc.: 93.75%] [G loss: 4.689521]\n",
      "epoch:35 step:33417 [D loss: 0.083009, acc.: 99.22%] [G loss: 3.167397]\n",
      "epoch:35 step:33418 [D loss: 0.810438, acc.: 57.03%] [G loss: 2.670272]\n",
      "epoch:35 step:33419 [D loss: 0.309163, acc.: 91.41%] [G loss: 3.930186]\n",
      "epoch:35 step:33420 [D loss: 0.603827, acc.: 67.19%] [G loss: 3.973607]\n",
      "epoch:35 step:33421 [D loss: 1.290038, acc.: 34.38%] [G loss: 5.764489]\n",
      "epoch:35 step:33422 [D loss: 0.759185, acc.: 64.06%] [G loss: 3.995960]\n",
      "epoch:35 step:33423 [D loss: 0.101624, acc.: 99.22%] [G loss: 3.910058]\n",
      "epoch:35 step:33424 [D loss: 0.240485, acc.: 85.94%] [G loss: 3.246387]\n",
      "epoch:35 step:33425 [D loss: 0.214795, acc.: 94.53%] [G loss: 1.076205]\n",
      "epoch:35 step:33426 [D loss: 0.651926, acc.: 67.97%] [G loss: 1.056608]\n",
      "epoch:35 step:33427 [D loss: 0.153204, acc.: 96.88%] [G loss: 1.842723]\n",
      "epoch:35 step:33428 [D loss: 0.190274, acc.: 96.09%] [G loss: 0.775838]\n",
      "epoch:35 step:33429 [D loss: 0.720453, acc.: 62.50%] [G loss: 2.646274]\n",
      "epoch:35 step:33430 [D loss: 0.507343, acc.: 71.09%] [G loss: 1.636258]\n",
      "epoch:35 step:33431 [D loss: 0.290015, acc.: 91.41%] [G loss: 1.768496]\n",
      "epoch:35 step:33432 [D loss: 0.550159, acc.: 67.97%] [G loss: 2.075231]\n",
      "epoch:35 step:33433 [D loss: 0.078419, acc.: 97.66%] [G loss: 4.800302]\n",
      "epoch:35 step:33434 [D loss: 0.225674, acc.: 91.41%] [G loss: 1.716512]\n",
      "epoch:35 step:33435 [D loss: 0.299298, acc.: 88.28%] [G loss: 2.263911]\n",
      "epoch:35 step:33436 [D loss: 0.136605, acc.: 98.44%] [G loss: 2.171439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33437 [D loss: 0.778714, acc.: 59.38%] [G loss: 1.831737]\n",
      "epoch:35 step:33438 [D loss: 0.097447, acc.: 97.66%] [G loss: 2.019992]\n",
      "epoch:35 step:33439 [D loss: 0.064537, acc.: 100.00%] [G loss: 2.002103]\n",
      "epoch:35 step:33440 [D loss: 0.060139, acc.: 99.22%] [G loss: 3.594063]\n",
      "epoch:35 step:33441 [D loss: 0.373516, acc.: 91.41%] [G loss: 2.462273]\n",
      "epoch:35 step:33442 [D loss: 0.101158, acc.: 99.22%] [G loss: 3.911153]\n",
      "epoch:35 step:33443 [D loss: 1.315885, acc.: 33.59%] [G loss: 5.005909]\n",
      "epoch:35 step:33444 [D loss: 0.076822, acc.: 99.22%] [G loss: 1.329891]\n",
      "epoch:35 step:33445 [D loss: 0.101267, acc.: 97.66%] [G loss: 0.702620]\n",
      "epoch:35 step:33446 [D loss: 0.133207, acc.: 99.22%] [G loss: 0.764042]\n",
      "epoch:35 step:33447 [D loss: 0.091525, acc.: 99.22%] [G loss: 3.075341]\n",
      "epoch:35 step:33448 [D loss: 0.443867, acc.: 75.00%] [G loss: 4.111077]\n",
      "epoch:35 step:33449 [D loss: 0.510460, acc.: 78.12%] [G loss: 2.787099]\n",
      "epoch:35 step:33450 [D loss: 0.371920, acc.: 75.78%] [G loss: 1.290461]\n",
      "epoch:35 step:33451 [D loss: 0.239946, acc.: 92.97%] [G loss: 3.186921]\n",
      "epoch:35 step:33452 [D loss: 0.680207, acc.: 65.62%] [G loss: 4.339870]\n",
      "epoch:35 step:33453 [D loss: 0.404800, acc.: 84.38%] [G loss: 2.342207]\n",
      "epoch:35 step:33454 [D loss: 0.037026, acc.: 100.00%] [G loss: 1.111595]\n",
      "epoch:35 step:33455 [D loss: 0.361732, acc.: 83.59%] [G loss: 4.313376]\n",
      "epoch:35 step:33456 [D loss: 0.223462, acc.: 95.31%] [G loss: 5.310977]\n",
      "epoch:35 step:33457 [D loss: 0.072498, acc.: 100.00%] [G loss: 4.471645]\n",
      "epoch:35 step:33458 [D loss: 0.062187, acc.: 99.22%] [G loss: 2.380740]\n",
      "epoch:35 step:33459 [D loss: 0.146511, acc.: 98.44%] [G loss: 6.020995]\n",
      "epoch:35 step:33460 [D loss: 0.152427, acc.: 97.66%] [G loss: 4.788272]\n",
      "epoch:35 step:33461 [D loss: 0.343097, acc.: 83.59%] [G loss: 2.161678]\n",
      "epoch:35 step:33462 [D loss: 0.683614, acc.: 60.16%] [G loss: 2.745340]\n",
      "epoch:35 step:33463 [D loss: 0.199548, acc.: 96.09%] [G loss: 4.093790]\n",
      "epoch:35 step:33464 [D loss: 0.313201, acc.: 89.84%] [G loss: 5.335249]\n",
      "epoch:35 step:33465 [D loss: 0.150814, acc.: 96.09%] [G loss: 1.592062]\n",
      "epoch:35 step:33466 [D loss: 0.078505, acc.: 99.22%] [G loss: 3.960619]\n",
      "epoch:35 step:33467 [D loss: 0.082115, acc.: 97.66%] [G loss: 2.342356]\n",
      "epoch:35 step:33468 [D loss: 0.102117, acc.: 97.66%] [G loss: 5.074848]\n",
      "epoch:35 step:33469 [D loss: 0.037621, acc.: 100.00%] [G loss: 2.879013]\n",
      "epoch:35 step:33470 [D loss: 0.034629, acc.: 99.22%] [G loss: 2.688057]\n",
      "epoch:35 step:33471 [D loss: 0.923807, acc.: 57.03%] [G loss: 2.267414]\n",
      "epoch:35 step:33472 [D loss: 0.807316, acc.: 56.25%] [G loss: 0.666559]\n",
      "epoch:35 step:33473 [D loss: 0.336007, acc.: 85.94%] [G loss: 1.174170]\n",
      "epoch:35 step:33474 [D loss: 0.084651, acc.: 99.22%] [G loss: 4.385828]\n",
      "epoch:35 step:33475 [D loss: 0.275420, acc.: 89.84%] [G loss: 4.509298]\n",
      "epoch:35 step:33476 [D loss: 0.289170, acc.: 89.84%] [G loss: 2.443061]\n",
      "epoch:35 step:33477 [D loss: 0.481425, acc.: 78.12%] [G loss: 2.418173]\n",
      "epoch:35 step:33478 [D loss: 0.601380, acc.: 68.75%] [G loss: 0.427449]\n",
      "epoch:35 step:33479 [D loss: 0.220951, acc.: 91.41%] [G loss: 3.417062]\n",
      "epoch:35 step:33480 [D loss: 0.623227, acc.: 60.16%] [G loss: 3.988125]\n",
      "epoch:35 step:33481 [D loss: 0.195809, acc.: 96.09%] [G loss: 1.510427]\n",
      "epoch:35 step:33482 [D loss: 0.102596, acc.: 99.22%] [G loss: 3.517904]\n",
      "epoch:35 step:33483 [D loss: 0.570652, acc.: 69.53%] [G loss: 3.796698]\n",
      "epoch:35 step:33484 [D loss: 0.057343, acc.: 98.44%] [G loss: 2.809001]\n",
      "epoch:35 step:33485 [D loss: 0.170645, acc.: 93.75%] [G loss: 3.068612]\n",
      "epoch:35 step:33486 [D loss: 0.024430, acc.: 100.00%] [G loss: 0.537110]\n",
      "epoch:35 step:33487 [D loss: 0.072456, acc.: 98.44%] [G loss: 3.099095]\n",
      "epoch:35 step:33488 [D loss: 0.139420, acc.: 97.66%] [G loss: 2.464448]\n",
      "epoch:35 step:33489 [D loss: 0.211147, acc.: 96.88%] [G loss: 0.574064]\n",
      "epoch:35 step:33490 [D loss: 0.190288, acc.: 96.09%] [G loss: 1.850701]\n",
      "epoch:35 step:33491 [D loss: 0.672696, acc.: 63.28%] [G loss: 1.684752]\n",
      "epoch:35 step:33492 [D loss: 0.172917, acc.: 96.09%] [G loss: 2.879591]\n",
      "epoch:35 step:33493 [D loss: 0.167065, acc.: 96.88%] [G loss: 1.320601]\n",
      "epoch:35 step:33494 [D loss: 0.187235, acc.: 95.31%] [G loss: 5.011292]\n",
      "epoch:35 step:33495 [D loss: 0.265188, acc.: 88.28%] [G loss: 2.684543]\n",
      "epoch:35 step:33496 [D loss: 0.447941, acc.: 78.12%] [G loss: 6.970357]\n",
      "epoch:35 step:33497 [D loss: 0.423292, acc.: 78.91%] [G loss: 2.083458]\n",
      "epoch:35 step:33498 [D loss: 0.314371, acc.: 82.81%] [G loss: 5.079398]\n",
      "epoch:35 step:33499 [D loss: 0.114484, acc.: 96.88%] [G loss: 2.784067]\n",
      "epoch:35 step:33500 [D loss: 0.369620, acc.: 88.28%] [G loss: 3.988560]\n",
      "epoch:35 step:33501 [D loss: 0.248537, acc.: 94.53%] [G loss: 1.206513]\n",
      "epoch:35 step:33502 [D loss: 0.020661, acc.: 99.22%] [G loss: 2.220960]\n",
      "epoch:35 step:33503 [D loss: 0.096030, acc.: 100.00%] [G loss: 1.013371]\n",
      "epoch:35 step:33504 [D loss: 0.216104, acc.: 92.19%] [G loss: 1.058999]\n",
      "epoch:35 step:33505 [D loss: 0.725963, acc.: 58.59%] [G loss: 5.637570]\n",
      "epoch:35 step:33506 [D loss: 0.487446, acc.: 79.69%] [G loss: 5.721058]\n",
      "epoch:35 step:33507 [D loss: 1.250266, acc.: 53.12%] [G loss: 6.570658]\n",
      "epoch:35 step:33508 [D loss: 0.532207, acc.: 74.22%] [G loss: 0.953032]\n",
      "epoch:35 step:33509 [D loss: 0.697594, acc.: 56.25%] [G loss: 4.817675]\n",
      "epoch:35 step:33510 [D loss: 0.488655, acc.: 77.34%] [G loss: 0.891307]\n",
      "epoch:35 step:33511 [D loss: 0.042180, acc.: 100.00%] [G loss: 2.643356]\n",
      "epoch:35 step:33512 [D loss: 0.056809, acc.: 99.22%] [G loss: 3.045281]\n",
      "epoch:35 step:33513 [D loss: 0.245038, acc.: 92.19%] [G loss: 2.136179]\n",
      "epoch:35 step:33514 [D loss: 0.273771, acc.: 92.97%] [G loss: 3.850269]\n",
      "epoch:35 step:33515 [D loss: 0.191401, acc.: 96.09%] [G loss: 4.736463]\n",
      "epoch:35 step:33516 [D loss: 0.066673, acc.: 99.22%] [G loss: 2.924551]\n",
      "epoch:35 step:33517 [D loss: 0.046608, acc.: 99.22%] [G loss: 8.730783]\n",
      "epoch:35 step:33518 [D loss: 0.372924, acc.: 85.94%] [G loss: 3.294656]\n",
      "epoch:35 step:33519 [D loss: 0.573080, acc.: 62.50%] [G loss: 4.505489]\n",
      "epoch:35 step:33520 [D loss: 0.574952, acc.: 67.97%] [G loss: 1.528562]\n",
      "epoch:35 step:33521 [D loss: 0.911965, acc.: 58.59%] [G loss: 3.015250]\n",
      "epoch:35 step:33522 [D loss: 0.098561, acc.: 98.44%] [G loss: 4.719621]\n",
      "epoch:35 step:33523 [D loss: 0.052714, acc.: 99.22%] [G loss: 2.585226]\n",
      "epoch:35 step:33524 [D loss: 0.049520, acc.: 99.22%] [G loss: 2.296201]\n",
      "epoch:35 step:33525 [D loss: 0.268123, acc.: 89.84%] [G loss: 2.324723]\n",
      "epoch:35 step:33526 [D loss: 0.392843, acc.: 88.28%] [G loss: 2.086596]\n",
      "epoch:35 step:33527 [D loss: 0.135830, acc.: 98.44%] [G loss: 3.215233]\n",
      "epoch:35 step:33528 [D loss: 0.341001, acc.: 91.41%] [G loss: 1.995516]\n",
      "epoch:35 step:33529 [D loss: 0.053092, acc.: 98.44%] [G loss: 5.047026]\n",
      "epoch:35 step:33530 [D loss: 0.206499, acc.: 97.66%] [G loss: 2.785566]\n",
      "epoch:35 step:33531 [D loss: 0.231388, acc.: 96.88%] [G loss: 5.283070]\n",
      "epoch:35 step:33532 [D loss: 0.178941, acc.: 96.09%] [G loss: 4.075130]\n",
      "epoch:35 step:33533 [D loss: 0.402089, acc.: 84.38%] [G loss: 2.689117]\n",
      "epoch:35 step:33534 [D loss: 1.258638, acc.: 32.81%] [G loss: 2.434575]\n",
      "epoch:35 step:33535 [D loss: 0.044203, acc.: 99.22%] [G loss: 1.622442]\n",
      "epoch:35 step:33536 [D loss: 0.428474, acc.: 80.47%] [G loss: 5.340272]\n",
      "epoch:35 step:33537 [D loss: 0.208104, acc.: 95.31%] [G loss: 2.811867]\n",
      "epoch:35 step:33538 [D loss: 0.400244, acc.: 78.12%] [G loss: 3.648519]\n",
      "epoch:35 step:33539 [D loss: 0.131093, acc.: 96.09%] [G loss: 3.141363]\n",
      "epoch:35 step:33540 [D loss: 0.307572, acc.: 85.94%] [G loss: 1.813946]\n",
      "epoch:35 step:33541 [D loss: 0.086881, acc.: 99.22%] [G loss: 2.946192]\n",
      "epoch:35 step:33542 [D loss: 0.094850, acc.: 98.44%] [G loss: 3.586605]\n",
      "epoch:35 step:33543 [D loss: 0.063337, acc.: 99.22%] [G loss: 2.350243]\n",
      "epoch:35 step:33544 [D loss: 0.699788, acc.: 62.50%] [G loss: 3.423068]\n",
      "epoch:35 step:33545 [D loss: 0.055414, acc.: 100.00%] [G loss: 4.517471]\n",
      "epoch:35 step:33546 [D loss: 0.538631, acc.: 65.62%] [G loss: 4.061323]\n",
      "epoch:35 step:33547 [D loss: 0.144552, acc.: 97.66%] [G loss: 1.620007]\n",
      "epoch:35 step:33548 [D loss: 0.054173, acc.: 100.00%] [G loss: 1.916697]\n",
      "epoch:35 step:33549 [D loss: 0.112947, acc.: 99.22%] [G loss: 0.733770]\n",
      "epoch:35 step:33550 [D loss: 0.799628, acc.: 59.38%] [G loss: 1.968753]\n",
      "epoch:35 step:33551 [D loss: 0.159898, acc.: 95.31%] [G loss: 1.650629]\n",
      "epoch:35 step:33552 [D loss: 0.180591, acc.: 98.44%] [G loss: 2.524580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33553 [D loss: 0.472196, acc.: 70.31%] [G loss: 3.040432]\n",
      "epoch:35 step:33554 [D loss: 0.221321, acc.: 92.97%] [G loss: 3.745424]\n",
      "epoch:35 step:33555 [D loss: 0.227429, acc.: 89.84%] [G loss: 5.657888]\n",
      "epoch:35 step:33556 [D loss: 0.073052, acc.: 100.00%] [G loss: 6.187847]\n",
      "epoch:35 step:33557 [D loss: 0.135503, acc.: 96.09%] [G loss: 5.953565]\n",
      "epoch:35 step:33558 [D loss: 0.723388, acc.: 52.34%] [G loss: 7.219535]\n",
      "epoch:35 step:33559 [D loss: 0.043071, acc.: 98.44%] [G loss: 4.734001]\n",
      "epoch:35 step:33560 [D loss: 0.043489, acc.: 100.00%] [G loss: 2.200424]\n",
      "epoch:35 step:33561 [D loss: 0.312893, acc.: 89.84%] [G loss: 3.031753]\n",
      "epoch:35 step:33562 [D loss: 0.080010, acc.: 99.22%] [G loss: 1.341420]\n",
      "epoch:35 step:33563 [D loss: 0.035899, acc.: 100.00%] [G loss: 4.653595]\n",
      "epoch:35 step:33564 [D loss: 0.172315, acc.: 96.88%] [G loss: 1.745074]\n",
      "epoch:35 step:33565 [D loss: 0.186969, acc.: 96.88%] [G loss: 4.242579]\n",
      "epoch:35 step:33566 [D loss: 0.472964, acc.: 71.88%] [G loss: 5.938272]\n",
      "epoch:35 step:33567 [D loss: 0.899099, acc.: 57.03%] [G loss: 2.150422]\n",
      "epoch:35 step:33568 [D loss: 0.072782, acc.: 98.44%] [G loss: 2.793839]\n",
      "epoch:35 step:33569 [D loss: 0.160686, acc.: 98.44%] [G loss: 5.499065]\n",
      "epoch:35 step:33570 [D loss: 0.279176, acc.: 93.75%] [G loss: 0.787207]\n",
      "epoch:35 step:33571 [D loss: 0.038581, acc.: 99.22%] [G loss: 0.456686]\n",
      "epoch:35 step:33572 [D loss: 0.064012, acc.: 99.22%] [G loss: 5.148362]\n",
      "epoch:35 step:33573 [D loss: 0.078631, acc.: 99.22%] [G loss: 4.430082]\n",
      "epoch:35 step:33574 [D loss: 0.648325, acc.: 61.72%] [G loss: 2.191900]\n",
      "epoch:35 step:33575 [D loss: 0.240437, acc.: 92.97%] [G loss: 0.408670]\n",
      "epoch:35 step:33576 [D loss: 0.636728, acc.: 65.62%] [G loss: 7.779346]\n",
      "epoch:35 step:33577 [D loss: 0.421177, acc.: 77.34%] [G loss: 4.154519]\n",
      "epoch:35 step:33578 [D loss: 0.209771, acc.: 93.75%] [G loss: 4.264236]\n",
      "epoch:35 step:33579 [D loss: 0.415947, acc.: 80.47%] [G loss: 3.250940]\n",
      "epoch:35 step:33580 [D loss: 0.060821, acc.: 99.22%] [G loss: 2.123622]\n",
      "epoch:35 step:33581 [D loss: 0.025056, acc.: 99.22%] [G loss: 0.546290]\n",
      "epoch:35 step:33582 [D loss: 0.574998, acc.: 73.44%] [G loss: 5.078303]\n",
      "epoch:35 step:33583 [D loss: 0.360057, acc.: 86.72%] [G loss: 5.262372]\n",
      "epoch:35 step:33584 [D loss: 0.699827, acc.: 64.06%] [G loss: 2.264146]\n",
      "epoch:35 step:33585 [D loss: 0.601203, acc.: 66.41%] [G loss: 2.100469]\n",
      "epoch:35 step:33586 [D loss: 0.418699, acc.: 75.78%] [G loss: 1.654833]\n",
      "epoch:35 step:33587 [D loss: 0.427200, acc.: 77.34%] [G loss: 0.657991]\n",
      "epoch:35 step:33588 [D loss: 0.081808, acc.: 99.22%] [G loss: 1.646707]\n",
      "epoch:35 step:33589 [D loss: 0.152577, acc.: 97.66%] [G loss: 1.991960]\n",
      "epoch:35 step:33590 [D loss: 0.098208, acc.: 97.66%] [G loss: 3.561129]\n",
      "epoch:35 step:33591 [D loss: 0.220845, acc.: 92.97%] [G loss: 4.137527]\n",
      "epoch:35 step:33592 [D loss: 0.306656, acc.: 84.38%] [G loss: 5.661875]\n",
      "epoch:35 step:33593 [D loss: 0.135475, acc.: 96.88%] [G loss: 3.964980]\n",
      "epoch:35 step:33594 [D loss: 0.080591, acc.: 98.44%] [G loss: 3.714261]\n",
      "epoch:35 step:33595 [D loss: 0.135128, acc.: 98.44%] [G loss: 4.991052]\n",
      "epoch:35 step:33596 [D loss: 0.205935, acc.: 90.62%] [G loss: 3.505075]\n",
      "epoch:35 step:33597 [D loss: 0.280100, acc.: 83.59%] [G loss: 2.127823]\n",
      "epoch:35 step:33598 [D loss: 0.200204, acc.: 96.88%] [G loss: 2.051288]\n",
      "epoch:35 step:33599 [D loss: 0.987815, acc.: 35.94%] [G loss: 1.990364]\n",
      "epoch:35 step:33600 [D loss: 0.044244, acc.: 100.00%] [G loss: 2.540396]\n",
      "epoch:35 step:33601 [D loss: 0.394877, acc.: 80.47%] [G loss: 2.585291]\n",
      "epoch:35 step:33602 [D loss: 0.149079, acc.: 97.66%] [G loss: 5.316612]\n",
      "epoch:35 step:33603 [D loss: 0.023469, acc.: 100.00%] [G loss: 3.067428]\n",
      "epoch:35 step:33604 [D loss: 0.091274, acc.: 99.22%] [G loss: 2.982112]\n",
      "epoch:35 step:33605 [D loss: 0.336083, acc.: 86.72%] [G loss: 3.097920]\n",
      "epoch:35 step:33606 [D loss: 0.660703, acc.: 56.25%] [G loss: 3.632132]\n",
      "epoch:35 step:33607 [D loss: 0.254846, acc.: 92.19%] [G loss: 3.805778]\n",
      "epoch:35 step:33608 [D loss: 1.152027, acc.: 35.94%] [G loss: 0.981268]\n",
      "epoch:35 step:33609 [D loss: 0.019963, acc.: 99.22%] [G loss: 4.262058]\n",
      "epoch:35 step:33610 [D loss: 0.059427, acc.: 97.66%] [G loss: 3.895895]\n",
      "epoch:35 step:33611 [D loss: 0.027426, acc.: 100.00%] [G loss: 4.337380]\n",
      "epoch:35 step:33612 [D loss: 0.047842, acc.: 98.44%] [G loss: 2.956036]\n",
      "epoch:35 step:33613 [D loss: 0.237267, acc.: 95.31%] [G loss: 4.874845]\n",
      "epoch:35 step:33614 [D loss: 0.275750, acc.: 88.28%] [G loss: 3.895064]\n",
      "epoch:35 step:33615 [D loss: 0.098994, acc.: 97.66%] [G loss: 7.742407]\n",
      "epoch:35 step:33616 [D loss: 0.216405, acc.: 90.62%] [G loss: 3.530243]\n",
      "epoch:35 step:33617 [D loss: 0.024944, acc.: 100.00%] [G loss: 5.088000]\n",
      "epoch:35 step:33618 [D loss: 0.032638, acc.: 100.00%] [G loss: 2.375588]\n",
      "epoch:35 step:33619 [D loss: 0.756087, acc.: 52.34%] [G loss: 4.708421]\n",
      "epoch:35 step:33620 [D loss: 0.330851, acc.: 82.81%] [G loss: 5.009472]\n",
      "epoch:35 step:33621 [D loss: 0.013642, acc.: 100.00%] [G loss: 3.003831]\n",
      "epoch:35 step:33622 [D loss: 0.034840, acc.: 100.00%] [G loss: 5.641056]\n",
      "epoch:35 step:33623 [D loss: 0.311713, acc.: 89.84%] [G loss: 5.251911]\n",
      "epoch:35 step:33624 [D loss: 0.047658, acc.: 100.00%] [G loss: 7.155474]\n",
      "epoch:35 step:33625 [D loss: 0.110223, acc.: 97.66%] [G loss: 3.495440]\n",
      "epoch:35 step:33626 [D loss: 0.204980, acc.: 94.53%] [G loss: 5.278301]\n",
      "epoch:35 step:33627 [D loss: 0.102737, acc.: 99.22%] [G loss: 6.112548]\n",
      "epoch:35 step:33628 [D loss: 0.138886, acc.: 96.88%] [G loss: 5.276359]\n",
      "epoch:35 step:33629 [D loss: 0.116644, acc.: 97.66%] [G loss: 3.433319]\n",
      "epoch:35 step:33630 [D loss: 0.275296, acc.: 93.75%] [G loss: 2.112743]\n",
      "epoch:35 step:33631 [D loss: 0.056631, acc.: 100.00%] [G loss: 3.261696]\n",
      "epoch:35 step:33632 [D loss: 0.065108, acc.: 100.00%] [G loss: 3.907093]\n",
      "epoch:35 step:33633 [D loss: 0.354606, acc.: 78.91%] [G loss: 3.976620]\n",
      "epoch:35 step:33634 [D loss: 0.012092, acc.: 100.00%] [G loss: 5.998123]\n",
      "epoch:35 step:33635 [D loss: 0.212553, acc.: 92.97%] [G loss: 5.481024]\n",
      "epoch:35 step:33636 [D loss: 0.035367, acc.: 100.00%] [G loss: 7.030829]\n",
      "epoch:35 step:33637 [D loss: 0.041877, acc.: 99.22%] [G loss: 4.174876]\n",
      "epoch:35 step:33638 [D loss: 0.042211, acc.: 99.22%] [G loss: 3.886393]\n",
      "epoch:35 step:33639 [D loss: 0.234131, acc.: 90.62%] [G loss: 1.908233]\n",
      "epoch:35 step:33640 [D loss: 0.341078, acc.: 89.84%] [G loss: 1.256109]\n",
      "epoch:35 step:33641 [D loss: 0.099462, acc.: 96.88%] [G loss: 3.051634]\n",
      "epoch:35 step:33642 [D loss: 0.051445, acc.: 100.00%] [G loss: 5.065711]\n",
      "epoch:35 step:33643 [D loss: 0.235705, acc.: 92.97%] [G loss: 7.495263]\n",
      "epoch:35 step:33644 [D loss: 0.105262, acc.: 96.88%] [G loss: 5.072943]\n",
      "epoch:35 step:33645 [D loss: 0.167770, acc.: 96.09%] [G loss: 6.643147]\n",
      "epoch:35 step:33646 [D loss: 0.022534, acc.: 100.00%] [G loss: 3.394559]\n",
      "epoch:35 step:33647 [D loss: 0.054124, acc.: 99.22%] [G loss: 5.299723]\n",
      "epoch:35 step:33648 [D loss: 0.103521, acc.: 96.88%] [G loss: 5.327808]\n",
      "epoch:35 step:33649 [D loss: 0.177516, acc.: 95.31%] [G loss: 2.701297]\n",
      "epoch:35 step:33650 [D loss: 0.176840, acc.: 97.66%] [G loss: 1.616235]\n",
      "epoch:35 step:33651 [D loss: 0.482898, acc.: 67.97%] [G loss: 4.816903]\n",
      "epoch:35 step:33652 [D loss: 0.276397, acc.: 89.84%] [G loss: 3.402467]\n",
      "epoch:35 step:33653 [D loss: 0.176185, acc.: 94.53%] [G loss: 7.176022]\n",
      "epoch:35 step:33654 [D loss: 0.071509, acc.: 98.44%] [G loss: 1.022613]\n",
      "epoch:35 step:33655 [D loss: 0.048922, acc.: 100.00%] [G loss: 7.719058]\n",
      "epoch:35 step:33656 [D loss: 0.036081, acc.: 99.22%] [G loss: 2.308370]\n",
      "epoch:35 step:33657 [D loss: 0.008152, acc.: 100.00%] [G loss: 7.483094]\n",
      "epoch:35 step:33658 [D loss: 0.210803, acc.: 97.66%] [G loss: 5.225675]\n",
      "epoch:35 step:33659 [D loss: 0.923628, acc.: 57.03%] [G loss: 9.260913]\n",
      "epoch:35 step:33660 [D loss: 0.017247, acc.: 100.00%] [G loss: 14.426314]\n",
      "epoch:35 step:33661 [D loss: 1.883106, acc.: 57.03%] [G loss: 6.068291]\n",
      "epoch:35 step:33662 [D loss: 0.308215, acc.: 84.38%] [G loss: 5.041080]\n",
      "epoch:35 step:33663 [D loss: 0.104201, acc.: 96.09%] [G loss: 5.903942]\n",
      "epoch:35 step:33664 [D loss: 0.040789, acc.: 100.00%] [G loss: 5.885841]\n",
      "epoch:35 step:33665 [D loss: 0.108782, acc.: 96.09%] [G loss: 3.765145]\n",
      "epoch:35 step:33666 [D loss: 0.338061, acc.: 81.25%] [G loss: 3.103025]\n",
      "epoch:35 step:33667 [D loss: 0.020688, acc.: 100.00%] [G loss: 4.640438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:33668 [D loss: 0.185717, acc.: 93.75%] [G loss: 7.055080]\n",
      "epoch:35 step:33669 [D loss: 0.535531, acc.: 72.66%] [G loss: 4.135572]\n",
      "epoch:35 step:33670 [D loss: 0.691774, acc.: 58.59%] [G loss: 4.554326]\n",
      "epoch:35 step:33671 [D loss: 0.265278, acc.: 89.06%] [G loss: 3.977143]\n",
      "epoch:35 step:33672 [D loss: 0.114427, acc.: 96.88%] [G loss: 3.796160]\n",
      "epoch:35 step:33673 [D loss: 0.205923, acc.: 91.41%] [G loss: 2.524347]\n",
      "epoch:35 step:33674 [D loss: 0.127186, acc.: 96.09%] [G loss: 3.224759]\n",
      "epoch:35 step:33675 [D loss: 0.595641, acc.: 66.41%] [G loss: 1.503870]\n",
      "epoch:35 step:33676 [D loss: 0.084565, acc.: 96.88%] [G loss: 1.233208]\n",
      "epoch:35 step:33677 [D loss: 0.303395, acc.: 82.81%] [G loss: 4.575877]\n",
      "epoch:35 step:33678 [D loss: 0.132083, acc.: 96.88%] [G loss: 4.901283]\n",
      "epoch:35 step:33679 [D loss: 0.262534, acc.: 96.09%] [G loss: 2.901699]\n",
      "epoch:35 step:33680 [D loss: 0.464456, acc.: 79.69%] [G loss: 1.197125]\n",
      "epoch:35 step:33681 [D loss: 0.037030, acc.: 99.22%] [G loss: 5.866684]\n",
      "epoch:35 step:33682 [D loss: 0.291646, acc.: 82.81%] [G loss: 2.941347]\n",
      "epoch:35 step:33683 [D loss: 0.024062, acc.: 100.00%] [G loss: 3.021840]\n",
      "epoch:35 step:33684 [D loss: 0.062587, acc.: 98.44%] [G loss: 3.949202]\n",
      "epoch:35 step:33685 [D loss: 0.399524, acc.: 84.38%] [G loss: 6.579647]\n",
      "epoch:35 step:33686 [D loss: 0.019267, acc.: 100.00%] [G loss: 3.096210]\n",
      "epoch:35 step:33687 [D loss: 0.240178, acc.: 90.62%] [G loss: 3.727555]\n",
      "epoch:35 step:33688 [D loss: 0.050555, acc.: 100.00%] [G loss: 3.663526]\n",
      "epoch:35 step:33689 [D loss: 0.101997, acc.: 95.31%] [G loss: 1.897260]\n",
      "epoch:35 step:33690 [D loss: 0.517233, acc.: 71.09%] [G loss: 6.096737]\n",
      "epoch:35 step:33691 [D loss: 0.012025, acc.: 100.00%] [G loss: 6.842559]\n",
      "epoch:35 step:33692 [D loss: 0.304506, acc.: 89.06%] [G loss: 5.547200]\n",
      "epoch:35 step:33693 [D loss: 0.254546, acc.: 89.84%] [G loss: 6.185954]\n",
      "epoch:35 step:33694 [D loss: 0.276154, acc.: 83.59%] [G loss: 2.902407]\n",
      "epoch:35 step:33695 [D loss: 0.074142, acc.: 98.44%] [G loss: 5.816979]\n",
      "epoch:35 step:33696 [D loss: 0.176271, acc.: 93.75%] [G loss: 4.183764]\n",
      "epoch:35 step:33697 [D loss: 0.066431, acc.: 98.44%] [G loss: 4.591372]\n",
      "epoch:35 step:33698 [D loss: 0.045622, acc.: 99.22%] [G loss: 2.252417]\n",
      "epoch:35 step:33699 [D loss: 0.037174, acc.: 99.22%] [G loss: 5.300686]\n",
      "epoch:35 step:33700 [D loss: 0.158506, acc.: 96.88%] [G loss: 1.804470]\n",
      "epoch:35 step:33701 [D loss: 0.049615, acc.: 99.22%] [G loss: 2.501065]\n",
      "epoch:35 step:33702 [D loss: 0.058246, acc.: 99.22%] [G loss: 0.902367]\n",
      "epoch:35 step:33703 [D loss: 1.169753, acc.: 49.22%] [G loss: 3.472734]\n",
      "epoch:35 step:33704 [D loss: 0.028307, acc.: 99.22%] [G loss: 6.225152]\n",
      "epoch:35 step:33705 [D loss: 0.115340, acc.: 97.66%] [G loss: 3.379206]\n",
      "epoch:35 step:33706 [D loss: 0.182590, acc.: 94.53%] [G loss: 0.831044]\n",
      "epoch:35 step:33707 [D loss: 0.618846, acc.: 61.72%] [G loss: 3.308336]\n",
      "epoch:35 step:33708 [D loss: 0.141620, acc.: 96.09%] [G loss: 3.176866]\n",
      "epoch:35 step:33709 [D loss: 1.135925, acc.: 60.16%] [G loss: 2.065113]\n",
      "epoch:35 step:33710 [D loss: 0.290524, acc.: 83.59%] [G loss: 0.754146]\n",
      "epoch:35 step:33711 [D loss: 0.064671, acc.: 99.22%] [G loss: 1.987872]\n",
      "epoch:35 step:33712 [D loss: 0.018147, acc.: 99.22%] [G loss: 2.401636]\n",
      "epoch:35 step:33713 [D loss: 0.101287, acc.: 96.88%] [G loss: 0.830385]\n",
      "epoch:35 step:33714 [D loss: 0.262441, acc.: 88.28%] [G loss: 2.591044]\n",
      "epoch:35 step:33715 [D loss: 0.237271, acc.: 95.31%] [G loss: 0.861135]\n",
      "epoch:35 step:33716 [D loss: 0.174816, acc.: 95.31%] [G loss: 2.671185]\n",
      "epoch:35 step:33717 [D loss: 0.283421, acc.: 92.19%] [G loss: 3.371988]\n",
      "epoch:35 step:33718 [D loss: 0.149578, acc.: 96.09%] [G loss: 1.165388]\n",
      "epoch:35 step:33719 [D loss: 0.100343, acc.: 99.22%] [G loss: 3.952507]\n",
      "epoch:35 step:33720 [D loss: 0.449069, acc.: 82.81%] [G loss: 4.176593]\n",
      "epoch:35 step:33721 [D loss: 0.108391, acc.: 97.66%] [G loss: 1.259226]\n",
      "epoch:35 step:33722 [D loss: 0.013749, acc.: 99.22%] [G loss: 3.581566]\n",
      "epoch:35 step:33723 [D loss: 0.405919, acc.: 78.12%] [G loss: 1.016836]\n",
      "epoch:35 step:33724 [D loss: 0.070831, acc.: 100.00%] [G loss: 4.097644]\n",
      "epoch:35 step:33725 [D loss: 0.032015, acc.: 100.00%] [G loss: 4.308148]\n",
      "epoch:35 step:33726 [D loss: 0.302828, acc.: 90.62%] [G loss: 2.141691]\n",
      "epoch:35 step:33727 [D loss: 0.247615, acc.: 88.28%] [G loss: 2.578649]\n",
      "epoch:35 step:33728 [D loss: 0.115803, acc.: 96.88%] [G loss: 1.033629]\n",
      "epoch:35 step:33729 [D loss: 0.049749, acc.: 98.44%] [G loss: 1.559175]\n",
      "epoch:35 step:33730 [D loss: 0.493663, acc.: 76.56%] [G loss: 1.179024]\n",
      "epoch:35 step:33731 [D loss: 0.144164, acc.: 96.88%] [G loss: 4.276408]\n",
      "epoch:35 step:33732 [D loss: 0.054364, acc.: 100.00%] [G loss: 4.403780]\n",
      "epoch:36 step:33733 [D loss: 0.345077, acc.: 82.03%] [G loss: 4.110989]\n",
      "epoch:36 step:33734 [D loss: 0.075358, acc.: 99.22%] [G loss: 4.655273]\n",
      "epoch:36 step:33735 [D loss: 0.025558, acc.: 100.00%] [G loss: 4.555313]\n",
      "epoch:36 step:33736 [D loss: 0.028476, acc.: 99.22%] [G loss: 6.811257]\n",
      "epoch:36 step:33737 [D loss: 0.102692, acc.: 97.66%] [G loss: 7.046627]\n",
      "epoch:36 step:33738 [D loss: 0.110224, acc.: 99.22%] [G loss: 6.407394]\n",
      "epoch:36 step:33739 [D loss: 0.008488, acc.: 100.00%] [G loss: 1.201936]\n",
      "epoch:36 step:33740 [D loss: 0.438629, acc.: 74.22%] [G loss: 2.776155]\n",
      "epoch:36 step:33741 [D loss: 0.113526, acc.: 96.09%] [G loss: 1.585135]\n",
      "epoch:36 step:33742 [D loss: 0.355653, acc.: 80.47%] [G loss: 3.288411]\n",
      "epoch:36 step:33743 [D loss: 1.340939, acc.: 55.47%] [G loss: 6.924179]\n",
      "epoch:36 step:33744 [D loss: 0.058814, acc.: 98.44%] [G loss: 1.575432]\n",
      "epoch:36 step:33745 [D loss: 0.126237, acc.: 96.09%] [G loss: 6.548255]\n",
      "epoch:36 step:33746 [D loss: 0.032749, acc.: 99.22%] [G loss: 3.055439]\n",
      "epoch:36 step:33747 [D loss: 1.721157, acc.: 52.34%] [G loss: 0.701250]\n",
      "epoch:36 step:33748 [D loss: 1.670241, acc.: 55.47%] [G loss: 1.916798]\n",
      "epoch:36 step:33749 [D loss: 0.064967, acc.: 99.22%] [G loss: 3.399122]\n",
      "epoch:36 step:33750 [D loss: 0.030345, acc.: 99.22%] [G loss: 4.591148]\n",
      "epoch:36 step:33751 [D loss: 0.047599, acc.: 100.00%] [G loss: 5.389349]\n",
      "epoch:36 step:33752 [D loss: 0.129617, acc.: 97.66%] [G loss: 3.946171]\n",
      "epoch:36 step:33753 [D loss: 0.278422, acc.: 89.06%] [G loss: 2.437462]\n",
      "epoch:36 step:33754 [D loss: 0.238334, acc.: 89.06%] [G loss: 4.963755]\n",
      "epoch:36 step:33755 [D loss: 0.043252, acc.: 99.22%] [G loss: 2.247882]\n",
      "epoch:36 step:33756 [D loss: 0.375842, acc.: 78.91%] [G loss: 5.350434]\n",
      "epoch:36 step:33757 [D loss: 0.025548, acc.: 100.00%] [G loss: 4.154054]\n",
      "epoch:36 step:33758 [D loss: 0.364239, acc.: 80.47%] [G loss: 6.499153]\n",
      "epoch:36 step:33759 [D loss: 0.044942, acc.: 100.00%] [G loss: 8.629078]\n",
      "epoch:36 step:33760 [D loss: 0.075858, acc.: 98.44%] [G loss: 7.411116]\n",
      "epoch:36 step:33761 [D loss: 0.329348, acc.: 85.16%] [G loss: 5.582758]\n",
      "epoch:36 step:33762 [D loss: 0.053790, acc.: 100.00%] [G loss: 2.281474]\n",
      "epoch:36 step:33763 [D loss: 0.306269, acc.: 91.41%] [G loss: 1.989462]\n",
      "epoch:36 step:33764 [D loss: 0.018545, acc.: 99.22%] [G loss: 4.985759]\n",
      "epoch:36 step:33765 [D loss: 0.171562, acc.: 97.66%] [G loss: 4.381824]\n",
      "epoch:36 step:33766 [D loss: 0.027788, acc.: 99.22%] [G loss: 0.860783]\n",
      "epoch:36 step:33767 [D loss: 0.308576, acc.: 86.72%] [G loss: 4.844314]\n",
      "epoch:36 step:33768 [D loss: 0.035827, acc.: 99.22%] [G loss: 4.364974]\n",
      "epoch:36 step:33769 [D loss: 0.295614, acc.: 89.06%] [G loss: 2.316473]\n",
      "epoch:36 step:33770 [D loss: 0.242384, acc.: 89.84%] [G loss: 5.202177]\n",
      "epoch:36 step:33771 [D loss: 0.947814, acc.: 46.88%] [G loss: 1.617449]\n",
      "epoch:36 step:33772 [D loss: 1.155134, acc.: 56.25%] [G loss: 3.430068]\n",
      "epoch:36 step:33773 [D loss: 0.311013, acc.: 88.28%] [G loss: 3.880065]\n",
      "epoch:36 step:33774 [D loss: 0.166815, acc.: 93.75%] [G loss: 5.089975]\n",
      "epoch:36 step:33775 [D loss: 0.037048, acc.: 99.22%] [G loss: 5.244704]\n",
      "epoch:36 step:33776 [D loss: 0.915382, acc.: 43.75%] [G loss: 5.321795]\n",
      "epoch:36 step:33777 [D loss: 0.073188, acc.: 100.00%] [G loss: 5.333473]\n",
      "epoch:36 step:33778 [D loss: 0.011148, acc.: 100.00%] [G loss: 3.991154]\n",
      "epoch:36 step:33779 [D loss: 0.031022, acc.: 100.00%] [G loss: 2.699272]\n",
      "epoch:36 step:33780 [D loss: 0.159773, acc.: 96.88%] [G loss: 4.619493]\n",
      "epoch:36 step:33781 [D loss: 0.369261, acc.: 82.81%] [G loss: 3.502592]\n",
      "epoch:36 step:33782 [D loss: 0.033524, acc.: 100.00%] [G loss: 2.623795]\n",
      "epoch:36 step:33783 [D loss: 0.072728, acc.: 98.44%] [G loss: 1.919724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:33784 [D loss: 0.112964, acc.: 96.09%] [G loss: 2.715112]\n",
      "epoch:36 step:33785 [D loss: 1.180791, acc.: 55.47%] [G loss: 4.662910]\n",
      "epoch:36 step:33786 [D loss: 0.293806, acc.: 90.62%] [G loss: 4.218513]\n",
      "epoch:36 step:33787 [D loss: 0.133003, acc.: 97.66%] [G loss: 4.675487]\n",
      "epoch:36 step:33788 [D loss: 0.090726, acc.: 99.22%] [G loss: 7.773057]\n",
      "epoch:36 step:33789 [D loss: 0.116400, acc.: 98.44%] [G loss: 5.406659]\n",
      "epoch:36 step:33790 [D loss: 0.024648, acc.: 100.00%] [G loss: 4.077474]\n",
      "epoch:36 step:33791 [D loss: 0.137549, acc.: 94.53%] [G loss: 8.096025]\n",
      "epoch:36 step:33792 [D loss: 0.460334, acc.: 79.69%] [G loss: 4.886641]\n",
      "epoch:36 step:33793 [D loss: 0.409884, acc.: 79.69%] [G loss: 4.267229]\n",
      "epoch:36 step:33794 [D loss: 0.376182, acc.: 79.69%] [G loss: 3.354002]\n",
      "epoch:36 step:33795 [D loss: 0.081191, acc.: 98.44%] [G loss: 4.260810]\n",
      "epoch:36 step:33796 [D loss: 0.080709, acc.: 99.22%] [G loss: 3.931224]\n",
      "epoch:36 step:33797 [D loss: 0.050395, acc.: 100.00%] [G loss: 4.590610]\n",
      "epoch:36 step:33798 [D loss: 0.140185, acc.: 97.66%] [G loss: 1.604387]\n",
      "epoch:36 step:33799 [D loss: 0.093117, acc.: 98.44%] [G loss: 5.044951]\n",
      "epoch:36 step:33800 [D loss: 0.023110, acc.: 100.00%] [G loss: 1.840094]\n",
      "epoch:36 step:33801 [D loss: 0.126704, acc.: 97.66%] [G loss: 4.168252]\n",
      "epoch:36 step:33802 [D loss: 0.272283, acc.: 85.16%] [G loss: 4.819446]\n",
      "epoch:36 step:33803 [D loss: 0.351415, acc.: 88.28%] [G loss: 1.717578]\n",
      "epoch:36 step:33804 [D loss: 0.189259, acc.: 95.31%] [G loss: 3.760521]\n",
      "epoch:36 step:33805 [D loss: 0.160256, acc.: 96.88%] [G loss: 6.151799]\n",
      "epoch:36 step:33806 [D loss: 0.093979, acc.: 100.00%] [G loss: 0.407134]\n",
      "epoch:36 step:33807 [D loss: 0.112285, acc.: 98.44%] [G loss: 2.272694]\n",
      "epoch:36 step:33808 [D loss: 0.102346, acc.: 98.44%] [G loss: 2.561568]\n",
      "epoch:36 step:33809 [D loss: 0.200600, acc.: 96.09%] [G loss: 1.291067]\n",
      "epoch:36 step:33810 [D loss: 0.193129, acc.: 94.53%] [G loss: 6.333085]\n",
      "epoch:36 step:33811 [D loss: 0.203907, acc.: 92.19%] [G loss: 5.574917]\n",
      "epoch:36 step:33812 [D loss: 0.090460, acc.: 97.66%] [G loss: 4.256708]\n",
      "epoch:36 step:33813 [D loss: 0.779395, acc.: 63.28%] [G loss: 1.413843]\n",
      "epoch:36 step:33814 [D loss: 0.026708, acc.: 100.00%] [G loss: 3.109528]\n",
      "epoch:36 step:33815 [D loss: 0.028653, acc.: 100.00%] [G loss: 4.830227]\n",
      "epoch:36 step:33816 [D loss: 0.384528, acc.: 78.12%] [G loss: 2.075492]\n",
      "epoch:36 step:33817 [D loss: 0.010994, acc.: 100.00%] [G loss: 3.446436]\n",
      "epoch:36 step:33818 [D loss: 0.148322, acc.: 98.44%] [G loss: 1.172270]\n",
      "epoch:36 step:33819 [D loss: 0.152280, acc.: 96.88%] [G loss: 3.023716]\n",
      "epoch:36 step:33820 [D loss: 0.261442, acc.: 91.41%] [G loss: 1.557604]\n",
      "epoch:36 step:33821 [D loss: 0.314325, acc.: 83.59%] [G loss: 4.867654]\n",
      "epoch:36 step:33822 [D loss: 0.013597, acc.: 100.00%] [G loss: 4.083396]\n",
      "epoch:36 step:33823 [D loss: 0.024989, acc.: 100.00%] [G loss: 1.408266]\n",
      "epoch:36 step:33824 [D loss: 0.244358, acc.: 91.41%] [G loss: 5.047496]\n",
      "epoch:36 step:33825 [D loss: 2.372771, acc.: 44.53%] [G loss: 4.362853]\n",
      "epoch:36 step:33826 [D loss: 0.020235, acc.: 100.00%] [G loss: 3.297978]\n",
      "epoch:36 step:33827 [D loss: 0.109652, acc.: 98.44%] [G loss: 6.844756]\n",
      "epoch:36 step:33828 [D loss: 0.250183, acc.: 90.62%] [G loss: 1.963646]\n",
      "epoch:36 step:33829 [D loss: 0.155062, acc.: 96.88%] [G loss: 3.671139]\n",
      "epoch:36 step:33830 [D loss: 0.088316, acc.: 98.44%] [G loss: 4.229208]\n",
      "epoch:36 step:33831 [D loss: 0.254955, acc.: 87.50%] [G loss: 3.522620]\n",
      "epoch:36 step:33832 [D loss: 0.160282, acc.: 96.09%] [G loss: 1.007916]\n",
      "epoch:36 step:33833 [D loss: 0.464775, acc.: 74.22%] [G loss: 5.579226]\n",
      "epoch:36 step:33834 [D loss: 0.153157, acc.: 96.09%] [G loss: 1.954117]\n",
      "epoch:36 step:33835 [D loss: 0.581665, acc.: 72.66%] [G loss: 5.384743]\n",
      "epoch:36 step:33836 [D loss: 0.113664, acc.: 96.09%] [G loss: 4.692530]\n",
      "epoch:36 step:33837 [D loss: 0.645095, acc.: 66.41%] [G loss: 2.378150]\n",
      "epoch:36 step:33838 [D loss: 0.051636, acc.: 100.00%] [G loss: 2.102053]\n",
      "epoch:36 step:33839 [D loss: 0.113625, acc.: 98.44%] [G loss: 5.893758]\n",
      "epoch:36 step:33840 [D loss: 0.073638, acc.: 98.44%] [G loss: 1.426934]\n",
      "epoch:36 step:33841 [D loss: 0.598225, acc.: 67.19%] [G loss: 3.880914]\n",
      "epoch:36 step:33842 [D loss: 0.032788, acc.: 99.22%] [G loss: 5.080333]\n",
      "epoch:36 step:33843 [D loss: 0.233183, acc.: 93.75%] [G loss: 7.829181]\n",
      "epoch:36 step:33844 [D loss: 0.032417, acc.: 99.22%] [G loss: 2.703322]\n",
      "epoch:36 step:33845 [D loss: 0.399149, acc.: 78.12%] [G loss: 4.142393]\n",
      "epoch:36 step:33846 [D loss: 0.457091, acc.: 70.31%] [G loss: 2.754422]\n",
      "epoch:36 step:33847 [D loss: 0.105657, acc.: 97.66%] [G loss: 5.763409]\n",
      "epoch:36 step:33848 [D loss: 0.393805, acc.: 85.94%] [G loss: 6.771633]\n",
      "epoch:36 step:33849 [D loss: 0.204808, acc.: 95.31%] [G loss: 2.862044]\n",
      "epoch:36 step:33850 [D loss: 0.124208, acc.: 96.88%] [G loss: 6.502571]\n",
      "epoch:36 step:33851 [D loss: 0.461482, acc.: 78.12%] [G loss: 3.525795]\n",
      "epoch:36 step:33852 [D loss: 0.129457, acc.: 96.88%] [G loss: 3.722947]\n",
      "epoch:36 step:33853 [D loss: 0.014329, acc.: 100.00%] [G loss: 4.673844]\n",
      "epoch:36 step:33854 [D loss: 0.091956, acc.: 97.66%] [G loss: 4.217636]\n",
      "epoch:36 step:33855 [D loss: 0.055334, acc.: 100.00%] [G loss: 6.392681]\n",
      "epoch:36 step:33856 [D loss: 0.055631, acc.: 100.00%] [G loss: 2.136615]\n",
      "epoch:36 step:33857 [D loss: 0.125055, acc.: 95.31%] [G loss: 1.859905]\n",
      "epoch:36 step:33858 [D loss: 0.105674, acc.: 99.22%] [G loss: 1.647940]\n",
      "epoch:36 step:33859 [D loss: 0.010062, acc.: 100.00%] [G loss: 5.839281]\n",
      "epoch:36 step:33860 [D loss: 0.317425, acc.: 90.62%] [G loss: 2.248529]\n",
      "epoch:36 step:33861 [D loss: 0.273774, acc.: 89.06%] [G loss: 5.700191]\n",
      "epoch:36 step:33862 [D loss: 0.144292, acc.: 96.09%] [G loss: 4.457620]\n",
      "epoch:36 step:33863 [D loss: 0.029998, acc.: 99.22%] [G loss: 6.809253]\n",
      "epoch:36 step:33864 [D loss: 0.168942, acc.: 93.75%] [G loss: 3.733391]\n",
      "epoch:36 step:33865 [D loss: 0.582232, acc.: 65.62%] [G loss: 7.833714]\n",
      "epoch:36 step:33866 [D loss: 0.047840, acc.: 99.22%] [G loss: 9.258213]\n",
      "epoch:36 step:33867 [D loss: 0.347224, acc.: 82.03%] [G loss: 4.511861]\n",
      "epoch:36 step:33868 [D loss: 1.039617, acc.: 55.47%] [G loss: 4.526791]\n",
      "epoch:36 step:33869 [D loss: 0.119153, acc.: 96.88%] [G loss: 2.975433]\n",
      "epoch:36 step:33870 [D loss: 0.073103, acc.: 97.66%] [G loss: 1.591959]\n",
      "epoch:36 step:33871 [D loss: 0.024636, acc.: 100.00%] [G loss: 4.019596]\n",
      "epoch:36 step:33872 [D loss: 0.011651, acc.: 99.22%] [G loss: 3.994013]\n",
      "epoch:36 step:33873 [D loss: 0.149740, acc.: 96.09%] [G loss: 1.521076]\n",
      "epoch:36 step:33874 [D loss: 0.014082, acc.: 100.00%] [G loss: 2.415878]\n",
      "epoch:36 step:33875 [D loss: 0.500565, acc.: 73.44%] [G loss: 4.272012]\n",
      "epoch:36 step:33876 [D loss: 0.196650, acc.: 96.09%] [G loss: 3.476590]\n",
      "epoch:36 step:33877 [D loss: 0.480530, acc.: 69.53%] [G loss: 1.740379]\n",
      "epoch:36 step:33878 [D loss: 0.273693, acc.: 93.75%] [G loss: 3.947158]\n",
      "epoch:36 step:33879 [D loss: 0.943685, acc.: 58.59%] [G loss: 3.934801]\n",
      "epoch:36 step:33880 [D loss: 0.093949, acc.: 98.44%] [G loss: 7.769893]\n",
      "epoch:36 step:33881 [D loss: 0.328043, acc.: 81.25%] [G loss: 3.947577]\n",
      "epoch:36 step:33882 [D loss: 0.053256, acc.: 99.22%] [G loss: 3.305910]\n",
      "epoch:36 step:33883 [D loss: 0.690532, acc.: 63.28%] [G loss: 1.102090]\n",
      "epoch:36 step:33884 [D loss: 0.044741, acc.: 99.22%] [G loss: 4.323954]\n",
      "epoch:36 step:33885 [D loss: 1.017529, acc.: 58.59%] [G loss: 6.374828]\n",
      "epoch:36 step:33886 [D loss: 0.113367, acc.: 96.88%] [G loss: 0.735096]\n",
      "epoch:36 step:33887 [D loss: 0.132707, acc.: 96.09%] [G loss: 1.026757]\n",
      "epoch:36 step:33888 [D loss: 0.042877, acc.: 100.00%] [G loss: 5.188953]\n",
      "epoch:36 step:33889 [D loss: 0.064321, acc.: 99.22%] [G loss: 5.857759]\n",
      "epoch:36 step:33890 [D loss: 0.156133, acc.: 96.88%] [G loss: 7.227723]\n",
      "epoch:36 step:33891 [D loss: 0.078263, acc.: 98.44%] [G loss: 5.763360]\n",
      "epoch:36 step:33892 [D loss: 0.256543, acc.: 90.62%] [G loss: 3.432645]\n",
      "epoch:36 step:33893 [D loss: 0.206003, acc.: 91.41%] [G loss: 6.302335]\n",
      "epoch:36 step:33894 [D loss: 0.608018, acc.: 60.94%] [G loss: 4.231713]\n",
      "epoch:36 step:33895 [D loss: 0.072427, acc.: 98.44%] [G loss: 2.443471]\n",
      "epoch:36 step:33896 [D loss: 0.239760, acc.: 96.88%] [G loss: 1.750466]\n",
      "epoch:36 step:33897 [D loss: 0.035553, acc.: 100.00%] [G loss: 6.319554]\n",
      "epoch:36 step:33898 [D loss: 0.312052, acc.: 90.62%] [G loss: 2.768763]\n",
      "epoch:36 step:33899 [D loss: 0.114770, acc.: 97.66%] [G loss: 5.835889]\n",
      "epoch:36 step:33900 [D loss: 0.217980, acc.: 96.09%] [G loss: 2.406970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:33901 [D loss: 0.166154, acc.: 96.88%] [G loss: 4.388200]\n",
      "epoch:36 step:33902 [D loss: 0.010422, acc.: 100.00%] [G loss: 6.040467]\n",
      "epoch:36 step:33903 [D loss: 0.067057, acc.: 99.22%] [G loss: 4.082372]\n",
      "epoch:36 step:33904 [D loss: 0.115869, acc.: 98.44%] [G loss: 3.114578]\n",
      "epoch:36 step:33905 [D loss: 0.453808, acc.: 68.75%] [G loss: 3.888913]\n",
      "epoch:36 step:33906 [D loss: 0.100643, acc.: 98.44%] [G loss: 6.480559]\n",
      "epoch:36 step:33907 [D loss: 0.463336, acc.: 67.19%] [G loss: 3.186080]\n",
      "epoch:36 step:33908 [D loss: 0.455096, acc.: 78.91%] [G loss: 4.127370]\n",
      "epoch:36 step:33909 [D loss: 0.541285, acc.: 64.84%] [G loss: 4.170580]\n",
      "epoch:36 step:33910 [D loss: 0.166673, acc.: 97.66%] [G loss: 4.350885]\n",
      "epoch:36 step:33911 [D loss: 0.104506, acc.: 99.22%] [G loss: 1.490704]\n",
      "epoch:36 step:33912 [D loss: 0.159422, acc.: 96.09%] [G loss: 5.577194]\n",
      "epoch:36 step:33913 [D loss: 0.423291, acc.: 84.38%] [G loss: 2.714211]\n",
      "epoch:36 step:33914 [D loss: 0.040998, acc.: 99.22%] [G loss: 4.744877]\n",
      "epoch:36 step:33915 [D loss: 0.053152, acc.: 98.44%] [G loss: 2.804135]\n",
      "epoch:36 step:33916 [D loss: 0.129978, acc.: 95.31%] [G loss: 7.607017]\n",
      "epoch:36 step:33917 [D loss: 0.014343, acc.: 100.00%] [G loss: 1.120015]\n",
      "epoch:36 step:33918 [D loss: 0.098912, acc.: 99.22%] [G loss: 4.314744]\n",
      "epoch:36 step:33919 [D loss: 0.010328, acc.: 100.00%] [G loss: 6.598605]\n",
      "epoch:36 step:33920 [D loss: 0.399183, acc.: 80.47%] [G loss: 3.323196]\n",
      "epoch:36 step:33921 [D loss: 0.129578, acc.: 96.88%] [G loss: 4.756474]\n",
      "epoch:36 step:33922 [D loss: 0.326086, acc.: 86.72%] [G loss: 5.446814]\n",
      "epoch:36 step:33923 [D loss: 0.046055, acc.: 99.22%] [G loss: 5.273022]\n",
      "epoch:36 step:33924 [D loss: 0.019715, acc.: 100.00%] [G loss: 4.986953]\n",
      "epoch:36 step:33925 [D loss: 0.632364, acc.: 67.19%] [G loss: 2.495383]\n",
      "epoch:36 step:33926 [D loss: 0.183788, acc.: 92.97%] [G loss: 4.579432]\n",
      "epoch:36 step:33927 [D loss: 0.455872, acc.: 71.09%] [G loss: 4.117616]\n",
      "epoch:36 step:33928 [D loss: 0.089286, acc.: 97.66%] [G loss: 5.829588]\n",
      "epoch:36 step:33929 [D loss: 0.046345, acc.: 98.44%] [G loss: 6.644004]\n",
      "epoch:36 step:33930 [D loss: 0.162560, acc.: 96.88%] [G loss: 7.380209]\n",
      "epoch:36 step:33931 [D loss: 0.019531, acc.: 99.22%] [G loss: 8.367033]\n",
      "epoch:36 step:33932 [D loss: 0.105114, acc.: 96.88%] [G loss: 3.058227]\n",
      "epoch:36 step:33933 [D loss: 0.085893, acc.: 98.44%] [G loss: 2.148418]\n",
      "epoch:36 step:33934 [D loss: 0.478206, acc.: 76.56%] [G loss: 3.277367]\n",
      "epoch:36 step:33935 [D loss: 0.013454, acc.: 100.00%] [G loss: 6.052739]\n",
      "epoch:36 step:33936 [D loss: 0.089403, acc.: 98.44%] [G loss: 3.382598]\n",
      "epoch:36 step:33937 [D loss: 0.930533, acc.: 57.03%] [G loss: 0.705955]\n",
      "epoch:36 step:33938 [D loss: 0.856393, acc.: 59.38%] [G loss: 1.604682]\n",
      "epoch:36 step:33939 [D loss: 0.090221, acc.: 96.88%] [G loss: 5.572118]\n",
      "epoch:36 step:33940 [D loss: 0.072764, acc.: 98.44%] [G loss: 2.025326]\n",
      "epoch:36 step:33941 [D loss: 0.055344, acc.: 100.00%] [G loss: 2.719628]\n",
      "epoch:36 step:33942 [D loss: 0.031208, acc.: 100.00%] [G loss: 2.333345]\n",
      "epoch:36 step:33943 [D loss: 0.243223, acc.: 89.84%] [G loss: 6.423190]\n",
      "epoch:36 step:33944 [D loss: 0.921826, acc.: 49.22%] [G loss: 2.481241]\n",
      "epoch:36 step:33945 [D loss: 0.514081, acc.: 69.53%] [G loss: 1.994975]\n",
      "epoch:36 step:33946 [D loss: 0.018932, acc.: 99.22%] [G loss: 3.818870]\n",
      "epoch:36 step:33947 [D loss: 0.480588, acc.: 78.12%] [G loss: 4.610303]\n",
      "epoch:36 step:33948 [D loss: 0.251039, acc.: 89.06%] [G loss: 3.487099]\n",
      "epoch:36 step:33949 [D loss: 0.060660, acc.: 98.44%] [G loss: 4.933966]\n",
      "epoch:36 step:33950 [D loss: 0.046046, acc.: 99.22%] [G loss: 4.001570]\n",
      "epoch:36 step:33951 [D loss: 0.163650, acc.: 93.75%] [G loss: 4.255197]\n",
      "epoch:36 step:33952 [D loss: 0.486406, acc.: 73.44%] [G loss: 5.322791]\n",
      "epoch:36 step:33953 [D loss: 0.156165, acc.: 95.31%] [G loss: 3.777128]\n",
      "epoch:36 step:33954 [D loss: 0.067795, acc.: 99.22%] [G loss: 4.252522]\n",
      "epoch:36 step:33955 [D loss: 0.062848, acc.: 100.00%] [G loss: 2.665248]\n",
      "epoch:36 step:33956 [D loss: 0.029114, acc.: 100.00%] [G loss: 1.443781]\n",
      "epoch:36 step:33957 [D loss: 0.036009, acc.: 100.00%] [G loss: 2.737544]\n",
      "epoch:36 step:33958 [D loss: 0.025746, acc.: 100.00%] [G loss: 2.268154]\n",
      "epoch:36 step:33959 [D loss: 0.523762, acc.: 67.97%] [G loss: 6.727897]\n",
      "epoch:36 step:33960 [D loss: 0.131434, acc.: 96.09%] [G loss: 4.384206]\n",
      "epoch:36 step:33961 [D loss: 0.246790, acc.: 89.06%] [G loss: 6.467518]\n",
      "epoch:36 step:33962 [D loss: 0.987681, acc.: 58.59%] [G loss: 0.977353]\n",
      "epoch:36 step:33963 [D loss: 0.059204, acc.: 100.00%] [G loss: 2.768870]\n",
      "epoch:36 step:33964 [D loss: 0.116839, acc.: 98.44%] [G loss: 1.457689]\n",
      "epoch:36 step:33965 [D loss: 0.031273, acc.: 100.00%] [G loss: 4.232888]\n",
      "epoch:36 step:33966 [D loss: 0.785881, acc.: 57.81%] [G loss: 1.947561]\n",
      "epoch:36 step:33967 [D loss: 0.232986, acc.: 91.41%] [G loss: 3.127395]\n",
      "epoch:36 step:33968 [D loss: 0.104363, acc.: 97.66%] [G loss: 4.607670]\n",
      "epoch:36 step:33969 [D loss: 0.059039, acc.: 98.44%] [G loss: 8.455436]\n",
      "epoch:36 step:33970 [D loss: 0.133939, acc.: 96.88%] [G loss: 6.292137]\n",
      "epoch:36 step:33971 [D loss: 0.119940, acc.: 97.66%] [G loss: 3.625253]\n",
      "epoch:36 step:33972 [D loss: 0.117244, acc.: 97.66%] [G loss: 4.756647]\n",
      "epoch:36 step:33973 [D loss: 0.027784, acc.: 100.00%] [G loss: 4.745235]\n",
      "epoch:36 step:33974 [D loss: 0.150180, acc.: 97.66%] [G loss: 3.360346]\n",
      "epoch:36 step:33975 [D loss: 0.041835, acc.: 98.44%] [G loss: 4.629234]\n",
      "epoch:36 step:33976 [D loss: 0.399440, acc.: 82.03%] [G loss: 4.199664]\n",
      "epoch:36 step:33977 [D loss: 0.439047, acc.: 81.25%] [G loss: 4.580501]\n",
      "epoch:36 step:33978 [D loss: 1.175833, acc.: 52.34%] [G loss: 5.972906]\n",
      "epoch:36 step:33979 [D loss: 0.021305, acc.: 100.00%] [G loss: 6.293195]\n",
      "epoch:36 step:33980 [D loss: 0.073017, acc.: 99.22%] [G loss: 7.105002]\n",
      "epoch:36 step:33981 [D loss: 0.046518, acc.: 99.22%] [G loss: 6.961975]\n",
      "epoch:36 step:33982 [D loss: 0.023496, acc.: 100.00%] [G loss: 5.259915]\n",
      "epoch:36 step:33983 [D loss: 0.034890, acc.: 100.00%] [G loss: 3.252085]\n",
      "epoch:36 step:33984 [D loss: 0.039531, acc.: 100.00%] [G loss: 6.041710]\n",
      "epoch:36 step:33985 [D loss: 0.020853, acc.: 100.00%] [G loss: 6.409005]\n",
      "epoch:36 step:33986 [D loss: 0.122428, acc.: 97.66%] [G loss: 6.078762]\n",
      "epoch:36 step:33987 [D loss: 0.011174, acc.: 100.00%] [G loss: 4.774575]\n",
      "epoch:36 step:33988 [D loss: 0.195343, acc.: 97.66%] [G loss: 5.269509]\n",
      "epoch:36 step:33989 [D loss: 0.210577, acc.: 90.62%] [G loss: 2.283509]\n",
      "epoch:36 step:33990 [D loss: 0.030333, acc.: 100.00%] [G loss: 3.560574]\n",
      "epoch:36 step:33991 [D loss: 0.105884, acc.: 97.66%] [G loss: 7.472330]\n",
      "epoch:36 step:33992 [D loss: 0.024255, acc.: 100.00%] [G loss: 7.915497]\n",
      "epoch:36 step:33993 [D loss: 0.028786, acc.: 100.00%] [G loss: 2.638616]\n",
      "epoch:36 step:33994 [D loss: 0.090818, acc.: 100.00%] [G loss: 3.733193]\n",
      "epoch:36 step:33995 [D loss: 0.176988, acc.: 94.53%] [G loss: 2.168673]\n",
      "epoch:36 step:33996 [D loss: 0.939524, acc.: 58.59%] [G loss: 6.776743]\n",
      "epoch:36 step:33997 [D loss: 0.016951, acc.: 100.00%] [G loss: 11.626383]\n",
      "epoch:36 step:33998 [D loss: 1.020335, acc.: 57.03%] [G loss: 3.848035]\n",
      "epoch:36 step:33999 [D loss: 0.061574, acc.: 99.22%] [G loss: 4.090972]\n",
      "epoch:36 step:34000 [D loss: 0.381139, acc.: 86.72%] [G loss: 2.224514]\n",
      "epoch:36 step:34001 [D loss: 0.041869, acc.: 99.22%] [G loss: 4.186796]\n",
      "epoch:36 step:34002 [D loss: 0.011231, acc.: 100.00%] [G loss: 1.879566]\n",
      "epoch:36 step:34003 [D loss: 0.141108, acc.: 96.09%] [G loss: 3.956164]\n",
      "epoch:36 step:34004 [D loss: 0.112819, acc.: 97.66%] [G loss: 4.274335]\n",
      "epoch:36 step:34005 [D loss: 0.010807, acc.: 100.00%] [G loss: 2.163196]\n",
      "epoch:36 step:34006 [D loss: 0.073114, acc.: 99.22%] [G loss: 3.089008]\n",
      "epoch:36 step:34007 [D loss: 0.231716, acc.: 91.41%] [G loss: 3.410027]\n",
      "epoch:36 step:34008 [D loss: 0.040393, acc.: 100.00%] [G loss: 3.830636]\n",
      "epoch:36 step:34009 [D loss: 0.126747, acc.: 97.66%] [G loss: 3.300844]\n",
      "epoch:36 step:34010 [D loss: 0.741828, acc.: 57.81%] [G loss: 8.028189]\n",
      "epoch:36 step:34011 [D loss: 0.203039, acc.: 92.97%] [G loss: 3.515061]\n",
      "epoch:36 step:34012 [D loss: 0.121281, acc.: 99.22%] [G loss: 8.866804]\n",
      "epoch:36 step:34013 [D loss: 0.123478, acc.: 99.22%] [G loss: 8.408286]\n",
      "epoch:36 step:34014 [D loss: 0.300805, acc.: 83.59%] [G loss: 3.649383]\n",
      "epoch:36 step:34015 [D loss: 0.092610, acc.: 99.22%] [G loss: 7.528818]\n",
      "epoch:36 step:34016 [D loss: 1.650436, acc.: 14.84%] [G loss: 3.826681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34017 [D loss: 0.246216, acc.: 89.06%] [G loss: 6.191831]\n",
      "epoch:36 step:34018 [D loss: 0.060278, acc.: 99.22%] [G loss: 7.476393]\n",
      "epoch:36 step:34019 [D loss: 0.044291, acc.: 100.00%] [G loss: 3.889858]\n",
      "epoch:36 step:34020 [D loss: 0.052581, acc.: 100.00%] [G loss: 3.850692]\n",
      "epoch:36 step:34021 [D loss: 0.156083, acc.: 98.44%] [G loss: 1.398997]\n",
      "epoch:36 step:34022 [D loss: 0.022366, acc.: 100.00%] [G loss: 4.458623]\n",
      "epoch:36 step:34023 [D loss: 0.124945, acc.: 98.44%] [G loss: 2.133179]\n",
      "epoch:36 step:34024 [D loss: 0.276597, acc.: 86.72%] [G loss: 6.215667]\n",
      "epoch:36 step:34025 [D loss: 0.342566, acc.: 85.94%] [G loss: 3.400590]\n",
      "epoch:36 step:34026 [D loss: 0.596080, acc.: 67.19%] [G loss: 1.342036]\n",
      "epoch:36 step:34027 [D loss: 0.092907, acc.: 96.88%] [G loss: 1.980751]\n",
      "epoch:36 step:34028 [D loss: 0.457931, acc.: 68.75%] [G loss: 1.711414]\n",
      "epoch:36 step:34029 [D loss: 0.024930, acc.: 99.22%] [G loss: 5.911015]\n",
      "epoch:36 step:34030 [D loss: 0.274472, acc.: 85.94%] [G loss: 6.996114]\n",
      "epoch:36 step:34031 [D loss: 0.643910, acc.: 66.41%] [G loss: 4.141483]\n",
      "epoch:36 step:34032 [D loss: 0.151599, acc.: 96.88%] [G loss: 2.611753]\n",
      "epoch:36 step:34033 [D loss: 0.060235, acc.: 98.44%] [G loss: 3.798139]\n",
      "epoch:36 step:34034 [D loss: 0.134602, acc.: 96.09%] [G loss: 2.401819]\n",
      "epoch:36 step:34035 [D loss: 0.302120, acc.: 87.50%] [G loss: 5.756869]\n",
      "epoch:36 step:34036 [D loss: 0.568570, acc.: 66.41%] [G loss: 4.441826]\n",
      "epoch:36 step:34037 [D loss: 0.167835, acc.: 94.53%] [G loss: 4.082965]\n",
      "epoch:36 step:34038 [D loss: 0.135113, acc.: 98.44%] [G loss: 2.193421]\n",
      "epoch:36 step:34039 [D loss: 0.036580, acc.: 99.22%] [G loss: 1.526809]\n",
      "epoch:36 step:34040 [D loss: 0.178455, acc.: 95.31%] [G loss: 3.761983]\n",
      "epoch:36 step:34041 [D loss: 0.217678, acc.: 96.09%] [G loss: 3.591650]\n",
      "epoch:36 step:34042 [D loss: 0.211643, acc.: 93.75%] [G loss: 8.607419]\n",
      "epoch:36 step:34043 [D loss: 0.143876, acc.: 97.66%] [G loss: 5.092926]\n",
      "epoch:36 step:34044 [D loss: 0.774597, acc.: 51.56%] [G loss: 3.147444]\n",
      "epoch:36 step:34045 [D loss: 0.941539, acc.: 55.47%] [G loss: 3.581419]\n",
      "epoch:36 step:34046 [D loss: 0.152388, acc.: 93.75%] [G loss: 2.197893]\n",
      "epoch:36 step:34047 [D loss: 0.107415, acc.: 96.88%] [G loss: 4.664138]\n",
      "epoch:36 step:34048 [D loss: 0.461995, acc.: 71.09%] [G loss: 4.458815]\n",
      "epoch:36 step:34049 [D loss: 0.148448, acc.: 95.31%] [G loss: 4.839128]\n",
      "epoch:36 step:34050 [D loss: 1.058875, acc.: 54.69%] [G loss: 1.350206]\n",
      "epoch:36 step:34051 [D loss: 0.103778, acc.: 98.44%] [G loss: 6.943723]\n",
      "epoch:36 step:34052 [D loss: 1.519744, acc.: 53.91%] [G loss: 1.492161]\n",
      "epoch:36 step:34053 [D loss: 0.107709, acc.: 96.88%] [G loss: 3.217608]\n",
      "epoch:36 step:34054 [D loss: 0.035019, acc.: 100.00%] [G loss: 5.993876]\n",
      "epoch:36 step:34055 [D loss: 0.100260, acc.: 97.66%] [G loss: 4.113664]\n",
      "epoch:36 step:34056 [D loss: 0.057187, acc.: 99.22%] [G loss: 4.346074]\n",
      "epoch:36 step:34057 [D loss: 0.417681, acc.: 82.03%] [G loss: 0.943174]\n",
      "epoch:36 step:34058 [D loss: 0.146513, acc.: 95.31%] [G loss: 3.991859]\n",
      "epoch:36 step:34059 [D loss: 0.062343, acc.: 98.44%] [G loss: 2.808145]\n",
      "epoch:36 step:34060 [D loss: 0.863260, acc.: 53.91%] [G loss: 2.073676]\n",
      "epoch:36 step:34061 [D loss: 0.055383, acc.: 99.22%] [G loss: 3.049693]\n",
      "epoch:36 step:34062 [D loss: 0.325680, acc.: 82.03%] [G loss: 4.912029]\n",
      "epoch:36 step:34063 [D loss: 0.730354, acc.: 60.16%] [G loss: 2.695279]\n",
      "epoch:36 step:34064 [D loss: 0.083684, acc.: 98.44%] [G loss: 2.736655]\n",
      "epoch:36 step:34065 [D loss: 0.110270, acc.: 96.88%] [G loss: 7.019567]\n",
      "epoch:36 step:34066 [D loss: 0.219754, acc.: 92.19%] [G loss: 4.500391]\n",
      "epoch:36 step:34067 [D loss: 0.449513, acc.: 73.44%] [G loss: 4.693972]\n",
      "epoch:36 step:34068 [D loss: 0.398546, acc.: 82.81%] [G loss: 1.304803]\n",
      "epoch:36 step:34069 [D loss: 0.053382, acc.: 99.22%] [G loss: 1.979548]\n",
      "epoch:36 step:34070 [D loss: 0.016708, acc.: 100.00%] [G loss: 3.938733]\n",
      "epoch:36 step:34071 [D loss: 0.228437, acc.: 90.62%] [G loss: 4.012822]\n",
      "epoch:36 step:34072 [D loss: 0.214632, acc.: 92.19%] [G loss: 1.855707]\n",
      "epoch:36 step:34073 [D loss: 0.124774, acc.: 97.66%] [G loss: 2.721305]\n",
      "epoch:36 step:34074 [D loss: 0.458326, acc.: 78.91%] [G loss: 3.156654]\n",
      "epoch:36 step:34075 [D loss: 0.385953, acc.: 75.00%] [G loss: 3.436142]\n",
      "epoch:36 step:34076 [D loss: 0.144240, acc.: 96.88%] [G loss: 4.762932]\n",
      "epoch:36 step:34077 [D loss: 0.234555, acc.: 88.28%] [G loss: 6.129491]\n",
      "epoch:36 step:34078 [D loss: 0.009359, acc.: 100.00%] [G loss: 6.029339]\n",
      "epoch:36 step:34079 [D loss: 0.303106, acc.: 85.94%] [G loss: 2.496341]\n",
      "epoch:36 step:34080 [D loss: 0.822923, acc.: 55.47%] [G loss: 0.890749]\n",
      "epoch:36 step:34081 [D loss: 0.081505, acc.: 99.22%] [G loss: 6.363452]\n",
      "epoch:36 step:34082 [D loss: 0.038340, acc.: 99.22%] [G loss: 1.622007]\n",
      "epoch:36 step:34083 [D loss: 0.253908, acc.: 87.50%] [G loss: 2.767237]\n",
      "epoch:36 step:34084 [D loss: 0.309895, acc.: 88.28%] [G loss: 3.412844]\n",
      "epoch:36 step:34085 [D loss: 0.024672, acc.: 99.22%] [G loss: 4.859268]\n",
      "epoch:36 step:34086 [D loss: 0.392758, acc.: 80.47%] [G loss: 3.783277]\n",
      "epoch:36 step:34087 [D loss: 0.026438, acc.: 99.22%] [G loss: 1.664793]\n",
      "epoch:36 step:34088 [D loss: 0.405987, acc.: 78.91%] [G loss: 4.142582]\n",
      "epoch:36 step:34089 [D loss: 0.278654, acc.: 89.84%] [G loss: 1.212320]\n",
      "epoch:36 step:34090 [D loss: 0.075448, acc.: 100.00%] [G loss: 1.610805]\n",
      "epoch:36 step:34091 [D loss: 0.102400, acc.: 98.44%] [G loss: 1.968718]\n",
      "epoch:36 step:34092 [D loss: 0.088186, acc.: 97.66%] [G loss: 5.206679]\n",
      "epoch:36 step:34093 [D loss: 0.385176, acc.: 79.69%] [G loss: 3.748171]\n",
      "epoch:36 step:34094 [D loss: 0.116325, acc.: 96.88%] [G loss: 4.446236]\n",
      "epoch:36 step:34095 [D loss: 0.383484, acc.: 78.91%] [G loss: 2.310136]\n",
      "epoch:36 step:34096 [D loss: 0.021187, acc.: 100.00%] [G loss: 3.283320]\n",
      "epoch:36 step:34097 [D loss: 0.212782, acc.: 92.19%] [G loss: 2.926900]\n",
      "epoch:36 step:34098 [D loss: 0.054514, acc.: 99.22%] [G loss: 1.180486]\n",
      "epoch:36 step:34099 [D loss: 0.235007, acc.: 92.97%] [G loss: 1.670387]\n",
      "epoch:36 step:34100 [D loss: 0.168201, acc.: 98.44%] [G loss: 5.163380]\n",
      "epoch:36 step:34101 [D loss: 1.288016, acc.: 30.47%] [G loss: 1.945076]\n",
      "epoch:36 step:34102 [D loss: 0.102171, acc.: 100.00%] [G loss: 1.566246]\n",
      "epoch:36 step:34103 [D loss: 0.008823, acc.: 100.00%] [G loss: 2.240894]\n",
      "epoch:36 step:34104 [D loss: 0.006045, acc.: 100.00%] [G loss: 1.052484]\n",
      "epoch:36 step:34105 [D loss: 0.106724, acc.: 97.66%] [G loss: 3.508784]\n",
      "epoch:36 step:34106 [D loss: 0.186990, acc.: 96.09%] [G loss: 1.165377]\n",
      "epoch:36 step:34107 [D loss: 0.065864, acc.: 100.00%] [G loss: 6.234346]\n",
      "epoch:36 step:34108 [D loss: 0.124878, acc.: 96.09%] [G loss: 2.209243]\n",
      "epoch:36 step:34109 [D loss: 1.130048, acc.: 38.28%] [G loss: 1.128131]\n",
      "epoch:36 step:34110 [D loss: 0.522103, acc.: 71.88%] [G loss: 5.417534]\n",
      "epoch:36 step:34111 [D loss: 1.363756, acc.: 39.84%] [G loss: 6.322312]\n",
      "epoch:36 step:34112 [D loss: 0.287659, acc.: 94.53%] [G loss: 4.043114]\n",
      "epoch:36 step:34113 [D loss: 0.032439, acc.: 99.22%] [G loss: 6.802719]\n",
      "epoch:36 step:34114 [D loss: 0.498646, acc.: 72.66%] [G loss: 3.058982]\n",
      "epoch:36 step:34115 [D loss: 0.051108, acc.: 98.44%] [G loss: 2.524410]\n",
      "epoch:36 step:34116 [D loss: 0.242825, acc.: 92.19%] [G loss: 0.832115]\n",
      "epoch:36 step:34117 [D loss: 0.557307, acc.: 63.28%] [G loss: 1.537624]\n",
      "epoch:36 step:34118 [D loss: 0.294686, acc.: 91.41%] [G loss: 3.713634]\n",
      "epoch:36 step:34119 [D loss: 0.094733, acc.: 98.44%] [G loss: 5.543692]\n",
      "epoch:36 step:34120 [D loss: 0.909518, acc.: 57.81%] [G loss: 1.138846]\n",
      "epoch:36 step:34121 [D loss: 0.034518, acc.: 100.00%] [G loss: 0.464316]\n",
      "epoch:36 step:34122 [D loss: 0.215188, acc.: 94.53%] [G loss: 6.535720]\n",
      "epoch:36 step:34123 [D loss: 0.071181, acc.: 98.44%] [G loss: 1.218590]\n",
      "epoch:36 step:34124 [D loss: 0.539541, acc.: 71.88%] [G loss: 3.664476]\n",
      "epoch:36 step:34125 [D loss: 0.176180, acc.: 96.88%] [G loss: 1.934016]\n",
      "epoch:36 step:34126 [D loss: 0.118751, acc.: 98.44%] [G loss: 2.269117]\n",
      "epoch:36 step:34127 [D loss: 0.528853, acc.: 69.53%] [G loss: 3.905523]\n",
      "epoch:36 step:34128 [D loss: 0.286382, acc.: 86.72%] [G loss: 3.816811]\n",
      "epoch:36 step:34129 [D loss: 0.256465, acc.: 89.84%] [G loss: 0.300910]\n",
      "epoch:36 step:34130 [D loss: 0.216674, acc.: 94.53%] [G loss: 4.405169]\n",
      "epoch:36 step:34131 [D loss: 0.750981, acc.: 64.06%] [G loss: 8.104790]\n",
      "epoch:36 step:34132 [D loss: 0.015367, acc.: 100.00%] [G loss: 5.070295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34133 [D loss: 0.082694, acc.: 99.22%] [G loss: 3.212219]\n",
      "epoch:36 step:34134 [D loss: 0.079678, acc.: 100.00%] [G loss: 9.018648]\n",
      "epoch:36 step:34135 [D loss: 0.036834, acc.: 99.22%] [G loss: 5.011771]\n",
      "epoch:36 step:34136 [D loss: 0.380680, acc.: 80.47%] [G loss: 1.355430]\n",
      "epoch:36 step:34137 [D loss: 0.296601, acc.: 90.62%] [G loss: 6.376517]\n",
      "epoch:36 step:34138 [D loss: 0.524498, acc.: 72.66%] [G loss: 2.826356]\n",
      "epoch:36 step:34139 [D loss: 0.165931, acc.: 95.31%] [G loss: 0.699746]\n",
      "epoch:36 step:34140 [D loss: 0.078897, acc.: 97.66%] [G loss: 2.351995]\n",
      "epoch:36 step:34141 [D loss: 0.218628, acc.: 89.84%] [G loss: 4.840105]\n",
      "epoch:36 step:34142 [D loss: 0.341785, acc.: 80.47%] [G loss: 3.327756]\n",
      "epoch:36 step:34143 [D loss: 0.021175, acc.: 100.00%] [G loss: 4.342581]\n",
      "epoch:36 step:34144 [D loss: 0.066801, acc.: 99.22%] [G loss: 2.144780]\n",
      "epoch:36 step:34145 [D loss: 0.260543, acc.: 91.41%] [G loss: 3.894557]\n",
      "epoch:36 step:34146 [D loss: 0.025797, acc.: 99.22%] [G loss: 6.754109]\n",
      "epoch:36 step:34147 [D loss: 0.102808, acc.: 99.22%] [G loss: 3.603758]\n",
      "epoch:36 step:34148 [D loss: 0.020254, acc.: 100.00%] [G loss: 6.069265]\n",
      "epoch:36 step:34149 [D loss: 0.040904, acc.: 100.00%] [G loss: 8.088746]\n",
      "epoch:36 step:34150 [D loss: 0.440962, acc.: 76.56%] [G loss: 2.031850]\n",
      "epoch:36 step:34151 [D loss: 0.027389, acc.: 100.00%] [G loss: 3.938433]\n",
      "epoch:36 step:34152 [D loss: 0.269296, acc.: 85.94%] [G loss: 3.621452]\n",
      "epoch:36 step:34153 [D loss: 0.301473, acc.: 93.75%] [G loss: 2.305058]\n",
      "epoch:36 step:34154 [D loss: 0.195352, acc.: 89.84%] [G loss: 5.070896]\n",
      "epoch:36 step:34155 [D loss: 0.066308, acc.: 100.00%] [G loss: 3.839486]\n",
      "epoch:36 step:34156 [D loss: 0.095062, acc.: 97.66%] [G loss: 3.576262]\n",
      "epoch:36 step:34157 [D loss: 0.147135, acc.: 97.66%] [G loss: 2.708158]\n",
      "epoch:36 step:34158 [D loss: 0.123137, acc.: 96.09%] [G loss: 1.083602]\n",
      "epoch:36 step:34159 [D loss: 0.098639, acc.: 97.66%] [G loss: 0.897258]\n",
      "epoch:36 step:34160 [D loss: 0.043367, acc.: 100.00%] [G loss: 4.139837]\n",
      "epoch:36 step:34161 [D loss: 0.274890, acc.: 84.38%] [G loss: 8.117914]\n",
      "epoch:36 step:34162 [D loss: 0.087875, acc.: 97.66%] [G loss: 9.907308]\n",
      "epoch:36 step:34163 [D loss: 0.013713, acc.: 100.00%] [G loss: 2.951266]\n",
      "epoch:36 step:34164 [D loss: 1.665712, acc.: 50.78%] [G loss: 3.289883]\n",
      "epoch:36 step:34165 [D loss: 0.253822, acc.: 89.06%] [G loss: 1.873581]\n",
      "epoch:36 step:34166 [D loss: 0.411900, acc.: 77.34%] [G loss: 1.388165]\n",
      "epoch:36 step:34167 [D loss: 0.214367, acc.: 94.53%] [G loss: 5.641091]\n",
      "epoch:36 step:34168 [D loss: 0.320648, acc.: 85.94%] [G loss: 3.359380]\n",
      "epoch:36 step:34169 [D loss: 0.081868, acc.: 98.44%] [G loss: 4.661517]\n",
      "epoch:36 step:34170 [D loss: 0.154208, acc.: 98.44%] [G loss: 7.325114]\n",
      "epoch:36 step:34171 [D loss: 0.213394, acc.: 95.31%] [G loss: 2.806062]\n",
      "epoch:36 step:34172 [D loss: 0.969419, acc.: 44.53%] [G loss: 3.162296]\n",
      "epoch:36 step:34173 [D loss: 0.053287, acc.: 99.22%] [G loss: 3.903710]\n",
      "epoch:36 step:34174 [D loss: 0.086448, acc.: 98.44%] [G loss: 2.905713]\n",
      "epoch:36 step:34175 [D loss: 0.491992, acc.: 72.66%] [G loss: 1.630086]\n",
      "epoch:36 step:34176 [D loss: 0.144782, acc.: 94.53%] [G loss: 2.741169]\n",
      "epoch:36 step:34177 [D loss: 0.696645, acc.: 60.94%] [G loss: 4.292164]\n",
      "epoch:36 step:34178 [D loss: 0.591557, acc.: 66.41%] [G loss: 1.396736]\n",
      "epoch:36 step:34179 [D loss: 0.199856, acc.: 95.31%] [G loss: 1.592089]\n",
      "epoch:36 step:34180 [D loss: 0.449418, acc.: 76.56%] [G loss: 3.718710]\n",
      "epoch:36 step:34181 [D loss: 0.149764, acc.: 96.88%] [G loss: 1.748226]\n",
      "epoch:36 step:34182 [D loss: 0.121940, acc.: 98.44%] [G loss: 4.398905]\n",
      "epoch:36 step:34183 [D loss: 0.162319, acc.: 96.09%] [G loss: 6.331559]\n",
      "epoch:36 step:34184 [D loss: 0.351813, acc.: 92.19%] [G loss: 2.481850]\n",
      "epoch:36 step:34185 [D loss: 0.862530, acc.: 52.34%] [G loss: 4.767195]\n",
      "epoch:36 step:34186 [D loss: 0.038590, acc.: 100.00%] [G loss: 4.674506]\n",
      "epoch:36 step:34187 [D loss: 0.809815, acc.: 60.94%] [G loss: 2.929049]\n",
      "epoch:36 step:34188 [D loss: 0.123249, acc.: 97.66%] [G loss: 9.432926]\n",
      "epoch:36 step:34189 [D loss: 0.216770, acc.: 94.53%] [G loss: 4.534559]\n",
      "epoch:36 step:34190 [D loss: 0.282095, acc.: 89.06%] [G loss: 2.820189]\n",
      "epoch:36 step:34191 [D loss: 0.418729, acc.: 74.22%] [G loss: 2.382909]\n",
      "epoch:36 step:34192 [D loss: 1.192450, acc.: 52.34%] [G loss: 5.198293]\n",
      "epoch:36 step:34193 [D loss: 0.185132, acc.: 91.41%] [G loss: 5.413462]\n",
      "epoch:36 step:34194 [D loss: 0.581625, acc.: 64.06%] [G loss: 0.616273]\n",
      "epoch:36 step:34195 [D loss: 0.216650, acc.: 91.41%] [G loss: 2.995400]\n",
      "epoch:36 step:34196 [D loss: 0.198023, acc.: 94.53%] [G loss: 0.363850]\n",
      "epoch:36 step:34197 [D loss: 0.068250, acc.: 96.88%] [G loss: 3.989455]\n",
      "epoch:36 step:34198 [D loss: 0.310839, acc.: 82.03%] [G loss: 6.385664]\n",
      "epoch:36 step:34199 [D loss: 0.093153, acc.: 98.44%] [G loss: 3.116170]\n",
      "epoch:36 step:34200 [D loss: 0.409405, acc.: 86.72%] [G loss: 3.484797]\n",
      "epoch:36 step:34201 [D loss: 0.123990, acc.: 98.44%] [G loss: 4.717155]\n",
      "epoch:36 step:34202 [D loss: 0.040749, acc.: 100.00%] [G loss: 2.812943]\n",
      "epoch:36 step:34203 [D loss: 0.079188, acc.: 97.66%] [G loss: 6.032130]\n",
      "epoch:36 step:34204 [D loss: 0.454789, acc.: 78.91%] [G loss: 5.561816]\n",
      "epoch:36 step:34205 [D loss: 0.004124, acc.: 100.00%] [G loss: 5.797635]\n",
      "epoch:36 step:34206 [D loss: 0.078127, acc.: 99.22%] [G loss: 3.797332]\n",
      "epoch:36 step:34207 [D loss: 0.091748, acc.: 98.44%] [G loss: 4.363426]\n",
      "epoch:36 step:34208 [D loss: 0.035941, acc.: 100.00%] [G loss: 5.464963]\n",
      "epoch:36 step:34209 [D loss: 0.012835, acc.: 100.00%] [G loss: 4.774087]\n",
      "epoch:36 step:34210 [D loss: 0.250899, acc.: 92.97%] [G loss: 4.870277]\n",
      "epoch:36 step:34211 [D loss: 0.716930, acc.: 60.16%] [G loss: 6.642340]\n",
      "epoch:36 step:34212 [D loss: 0.356642, acc.: 78.91%] [G loss: 2.779825]\n",
      "epoch:36 step:34213 [D loss: 0.027588, acc.: 100.00%] [G loss: 0.752932]\n",
      "epoch:36 step:34214 [D loss: 0.760666, acc.: 57.81%] [G loss: 4.348833]\n",
      "epoch:36 step:34215 [D loss: 0.236743, acc.: 89.84%] [G loss: 5.174133]\n",
      "epoch:36 step:34216 [D loss: 0.153671, acc.: 95.31%] [G loss: 5.515305]\n",
      "epoch:36 step:34217 [D loss: 0.500492, acc.: 77.34%] [G loss: 2.553161]\n",
      "epoch:36 step:34218 [D loss: 0.484322, acc.: 70.31%] [G loss: 1.797080]\n",
      "epoch:36 step:34219 [D loss: 0.697411, acc.: 55.47%] [G loss: 5.198600]\n",
      "epoch:36 step:34220 [D loss: 0.094523, acc.: 100.00%] [G loss: 1.175548]\n",
      "epoch:36 step:34221 [D loss: 0.213442, acc.: 93.75%] [G loss: 4.835190]\n",
      "epoch:36 step:34222 [D loss: 0.159882, acc.: 96.09%] [G loss: 3.092433]\n",
      "epoch:36 step:34223 [D loss: 0.370795, acc.: 78.12%] [G loss: 5.660067]\n",
      "epoch:36 step:34224 [D loss: 0.409245, acc.: 81.25%] [G loss: 5.573679]\n",
      "epoch:36 step:34225 [D loss: 0.036739, acc.: 99.22%] [G loss: 2.448088]\n",
      "epoch:36 step:34226 [D loss: 0.080093, acc.: 97.66%] [G loss: 2.610754]\n",
      "epoch:36 step:34227 [D loss: 0.217433, acc.: 92.97%] [G loss: 6.309393]\n",
      "epoch:36 step:34228 [D loss: 0.067693, acc.: 99.22%] [G loss: 2.165457]\n",
      "epoch:36 step:34229 [D loss: 0.721382, acc.: 60.16%] [G loss: 2.468750]\n",
      "epoch:36 step:34230 [D loss: 0.272575, acc.: 85.16%] [G loss: 2.889437]\n",
      "epoch:36 step:34231 [D loss: 0.015603, acc.: 100.00%] [G loss: 6.526626]\n",
      "epoch:36 step:34232 [D loss: 0.008164, acc.: 100.00%] [G loss: 2.083558]\n",
      "epoch:36 step:34233 [D loss: 0.237487, acc.: 88.28%] [G loss: 1.451442]\n",
      "epoch:36 step:34234 [D loss: 0.420675, acc.: 80.47%] [G loss: 4.330881]\n",
      "epoch:36 step:34235 [D loss: 0.614082, acc.: 67.19%] [G loss: 1.039281]\n",
      "epoch:36 step:34236 [D loss: 0.244102, acc.: 91.41%] [G loss: 2.240520]\n",
      "epoch:36 step:34237 [D loss: 0.453844, acc.: 73.44%] [G loss: 3.180091]\n",
      "epoch:36 step:34238 [D loss: 0.176643, acc.: 93.75%] [G loss: 2.295170]\n",
      "epoch:36 step:34239 [D loss: 0.062177, acc.: 100.00%] [G loss: 1.566000]\n",
      "epoch:36 step:34240 [D loss: 0.320296, acc.: 88.28%] [G loss: 3.531513]\n",
      "epoch:36 step:34241 [D loss: 0.048563, acc.: 99.22%] [G loss: 2.128852]\n",
      "epoch:36 step:34242 [D loss: 0.048708, acc.: 98.44%] [G loss: 0.141244]\n",
      "epoch:36 step:34243 [D loss: 0.461490, acc.: 78.12%] [G loss: 0.657882]\n",
      "epoch:36 step:34244 [D loss: 0.286303, acc.: 90.62%] [G loss: 1.181309]\n",
      "epoch:36 step:34245 [D loss: 1.106185, acc.: 57.03%] [G loss: 5.979251]\n",
      "epoch:36 step:34246 [D loss: 0.258371, acc.: 85.94%] [G loss: 2.079327]\n",
      "epoch:36 step:34247 [D loss: 0.324459, acc.: 82.03%] [G loss: 5.966056]\n",
      "epoch:36 step:34248 [D loss: 0.135901, acc.: 92.97%] [G loss: 4.499568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34249 [D loss: 0.088500, acc.: 98.44%] [G loss: 3.419617]\n",
      "epoch:36 step:34250 [D loss: 0.058594, acc.: 98.44%] [G loss: 2.546421]\n",
      "epoch:36 step:34251 [D loss: 0.009627, acc.: 100.00%] [G loss: 2.152002]\n",
      "epoch:36 step:34252 [D loss: 0.094970, acc.: 98.44%] [G loss: 1.500973]\n",
      "epoch:36 step:34253 [D loss: 0.003838, acc.: 100.00%] [G loss: 1.325899]\n",
      "epoch:36 step:34254 [D loss: 0.737508, acc.: 63.28%] [G loss: 3.295077]\n",
      "epoch:36 step:34255 [D loss: 0.052123, acc.: 99.22%] [G loss: 0.581662]\n",
      "epoch:36 step:34256 [D loss: 0.625565, acc.: 64.06%] [G loss: 0.670781]\n",
      "epoch:36 step:34257 [D loss: 0.004893, acc.: 100.00%] [G loss: 3.446235]\n",
      "epoch:36 step:34258 [D loss: 0.076756, acc.: 99.22%] [G loss: 1.227454]\n",
      "epoch:36 step:34259 [D loss: 0.084795, acc.: 99.22%] [G loss: 0.217021]\n",
      "epoch:36 step:34260 [D loss: 0.196504, acc.: 96.09%] [G loss: 0.293871]\n",
      "epoch:36 step:34261 [D loss: 0.882479, acc.: 58.59%] [G loss: 6.423828]\n",
      "epoch:36 step:34262 [D loss: 0.220206, acc.: 95.31%] [G loss: 5.705406]\n",
      "epoch:36 step:34263 [D loss: 0.068211, acc.: 99.22%] [G loss: 4.603017]\n",
      "epoch:36 step:34264 [D loss: 0.651756, acc.: 65.62%] [G loss: 0.577974]\n",
      "epoch:36 step:34265 [D loss: 0.038380, acc.: 99.22%] [G loss: 0.495335]\n",
      "epoch:36 step:34266 [D loss: 0.012560, acc.: 100.00%] [G loss: 2.372032]\n",
      "epoch:36 step:34267 [D loss: 0.876298, acc.: 57.03%] [G loss: 5.240261]\n",
      "epoch:36 step:34268 [D loss: 0.830652, acc.: 52.34%] [G loss: 5.314823]\n",
      "epoch:36 step:34269 [D loss: 0.078447, acc.: 98.44%] [G loss: 10.113828]\n",
      "epoch:36 step:34270 [D loss: 0.038568, acc.: 98.44%] [G loss: 5.084579]\n",
      "epoch:36 step:34271 [D loss: 0.653630, acc.: 59.38%] [G loss: 5.008200]\n",
      "epoch:36 step:34272 [D loss: 0.719133, acc.: 55.47%] [G loss: 4.891679]\n",
      "epoch:36 step:34273 [D loss: 0.092836, acc.: 97.66%] [G loss: 3.061503]\n",
      "epoch:36 step:34274 [D loss: 0.573986, acc.: 68.75%] [G loss: 4.850206]\n",
      "epoch:36 step:34275 [D loss: 0.053924, acc.: 98.44%] [G loss: 2.260036]\n",
      "epoch:36 step:34276 [D loss: 0.033101, acc.: 100.00%] [G loss: 2.728544]\n",
      "epoch:36 step:34277 [D loss: 0.028822, acc.: 100.00%] [G loss: 2.164022]\n",
      "epoch:36 step:34278 [D loss: 0.397733, acc.: 82.03%] [G loss: 3.514007]\n",
      "epoch:36 step:34279 [D loss: 0.111706, acc.: 97.66%] [G loss: 3.017966]\n",
      "epoch:36 step:34280 [D loss: 0.062456, acc.: 98.44%] [G loss: 6.553505]\n",
      "epoch:36 step:34281 [D loss: 0.287160, acc.: 92.19%] [G loss: 5.230325]\n",
      "epoch:36 step:34282 [D loss: 0.028134, acc.: 100.00%] [G loss: 5.294987]\n",
      "epoch:36 step:34283 [D loss: 0.047504, acc.: 99.22%] [G loss: 2.654002]\n",
      "epoch:36 step:34284 [D loss: 0.518480, acc.: 75.00%] [G loss: 3.491491]\n",
      "epoch:36 step:34285 [D loss: 0.059346, acc.: 99.22%] [G loss: 3.150063]\n",
      "epoch:36 step:34286 [D loss: 0.147783, acc.: 96.09%] [G loss: 6.356241]\n",
      "epoch:36 step:34287 [D loss: 0.013611, acc.: 100.00%] [G loss: 5.205323]\n",
      "epoch:36 step:34288 [D loss: 0.289541, acc.: 89.84%] [G loss: 5.583492]\n",
      "epoch:36 step:34289 [D loss: 1.199573, acc.: 35.16%] [G loss: 4.911143]\n",
      "epoch:36 step:34290 [D loss: 0.055437, acc.: 98.44%] [G loss: 3.450646]\n",
      "epoch:36 step:34291 [D loss: 0.096337, acc.: 97.66%] [G loss: 2.252145]\n",
      "epoch:36 step:34292 [D loss: 0.053441, acc.: 99.22%] [G loss: 4.764729]\n",
      "epoch:36 step:34293 [D loss: 0.045053, acc.: 100.00%] [G loss: 4.788100]\n",
      "epoch:36 step:34294 [D loss: 0.041480, acc.: 99.22%] [G loss: 3.292135]\n",
      "epoch:36 step:34295 [D loss: 1.186331, acc.: 28.91%] [G loss: 3.664581]\n",
      "epoch:36 step:34296 [D loss: 0.030828, acc.: 98.44%] [G loss: 5.522386]\n",
      "epoch:36 step:34297 [D loss: 0.650294, acc.: 60.16%] [G loss: 2.384188]\n",
      "epoch:36 step:34298 [D loss: 0.048507, acc.: 99.22%] [G loss: 5.668094]\n",
      "epoch:36 step:34299 [D loss: 0.195748, acc.: 93.75%] [G loss: 4.006348]\n",
      "epoch:36 step:34300 [D loss: 0.933983, acc.: 53.91%] [G loss: 1.871154]\n",
      "epoch:36 step:34301 [D loss: 0.075418, acc.: 100.00%] [G loss: 2.877195]\n",
      "epoch:36 step:34302 [D loss: 0.067579, acc.: 98.44%] [G loss: 7.160861]\n",
      "epoch:36 step:34303 [D loss: 0.165954, acc.: 95.31%] [G loss: 2.697178]\n",
      "epoch:36 step:34304 [D loss: 0.011570, acc.: 100.00%] [G loss: 3.063434]\n",
      "epoch:36 step:34305 [D loss: 0.691607, acc.: 58.59%] [G loss: 3.568112]\n",
      "epoch:36 step:34306 [D loss: 1.086860, acc.: 53.91%] [G loss: 4.343780]\n",
      "epoch:36 step:34307 [D loss: 0.014647, acc.: 100.00%] [G loss: 1.616873]\n",
      "epoch:36 step:34308 [D loss: 0.170525, acc.: 97.66%] [G loss: 5.077251]\n",
      "epoch:36 step:34309 [D loss: 0.037418, acc.: 99.22%] [G loss: 3.437407]\n",
      "epoch:36 step:34310 [D loss: 0.253681, acc.: 88.28%] [G loss: 6.164239]\n",
      "epoch:36 step:34311 [D loss: 0.404497, acc.: 82.81%] [G loss: 4.124254]\n",
      "epoch:36 step:34312 [D loss: 0.076110, acc.: 99.22%] [G loss: 2.120772]\n",
      "epoch:36 step:34313 [D loss: 0.017559, acc.: 100.00%] [G loss: 2.313567]\n",
      "epoch:36 step:34314 [D loss: 0.179432, acc.: 94.53%] [G loss: 4.162190]\n",
      "epoch:36 step:34315 [D loss: 0.105922, acc.: 99.22%] [G loss: 1.965013]\n",
      "epoch:36 step:34316 [D loss: 0.518667, acc.: 74.22%] [G loss: 5.370706]\n",
      "epoch:36 step:34317 [D loss: 0.053991, acc.: 100.00%] [G loss: 5.906419]\n",
      "epoch:36 step:34318 [D loss: 0.662901, acc.: 60.94%] [G loss: 2.575953]\n",
      "epoch:36 step:34319 [D loss: 0.430486, acc.: 72.66%] [G loss: 7.167138]\n",
      "epoch:36 step:34320 [D loss: 0.934255, acc.: 56.25%] [G loss: 4.818564]\n",
      "epoch:36 step:34321 [D loss: 0.906858, acc.: 49.22%] [G loss: 2.340927]\n",
      "epoch:36 step:34322 [D loss: 0.406141, acc.: 75.00%] [G loss: 4.988123]\n",
      "epoch:36 step:34323 [D loss: 0.510147, acc.: 71.88%] [G loss: 5.815340]\n",
      "epoch:36 step:34324 [D loss: 0.087049, acc.: 98.44%] [G loss: 4.138613]\n",
      "epoch:36 step:34325 [D loss: 0.050476, acc.: 98.44%] [G loss: 3.468430]\n",
      "epoch:36 step:34326 [D loss: 0.465728, acc.: 69.53%] [G loss: 5.378372]\n",
      "epoch:36 step:34327 [D loss: 0.244370, acc.: 86.72%] [G loss: 3.094044]\n",
      "epoch:36 step:34328 [D loss: 0.109735, acc.: 98.44%] [G loss: 4.186531]\n",
      "epoch:36 step:34329 [D loss: 0.248394, acc.: 91.41%] [G loss: 6.372140]\n",
      "epoch:36 step:34330 [D loss: 0.112407, acc.: 99.22%] [G loss: 6.934417]\n",
      "epoch:36 step:34331 [D loss: 0.087457, acc.: 100.00%] [G loss: 5.159588]\n",
      "epoch:36 step:34332 [D loss: 0.020335, acc.: 100.00%] [G loss: 6.705839]\n",
      "epoch:36 step:34333 [D loss: 0.051808, acc.: 99.22%] [G loss: 4.370046]\n",
      "epoch:36 step:34334 [D loss: 0.059361, acc.: 100.00%] [G loss: 6.461810]\n",
      "epoch:36 step:34335 [D loss: 0.275743, acc.: 89.06%] [G loss: 3.721586]\n",
      "epoch:36 step:34336 [D loss: 0.206179, acc.: 96.88%] [G loss: 2.100659]\n",
      "epoch:36 step:34337 [D loss: 0.050305, acc.: 100.00%] [G loss: 5.301974]\n",
      "epoch:36 step:34338 [D loss: 0.410261, acc.: 75.78%] [G loss: 4.646214]\n",
      "epoch:36 step:34339 [D loss: 0.041235, acc.: 100.00%] [G loss: 4.463900]\n",
      "epoch:36 step:34340 [D loss: 0.067788, acc.: 99.22%] [G loss: 5.298791]\n",
      "epoch:36 step:34341 [D loss: 0.106713, acc.: 99.22%] [G loss: 1.143725]\n",
      "epoch:36 step:34342 [D loss: 0.008111, acc.: 100.00%] [G loss: 4.214591]\n",
      "epoch:36 step:34343 [D loss: 0.134498, acc.: 98.44%] [G loss: 5.839390]\n",
      "epoch:36 step:34344 [D loss: 0.199878, acc.: 95.31%] [G loss: 2.294941]\n",
      "epoch:36 step:34345 [D loss: 0.730851, acc.: 53.91%] [G loss: 2.113786]\n",
      "epoch:36 step:34346 [D loss: 0.534272, acc.: 64.84%] [G loss: 3.334155]\n",
      "epoch:36 step:34347 [D loss: 0.039541, acc.: 100.00%] [G loss: 2.554210]\n",
      "epoch:36 step:34348 [D loss: 0.135287, acc.: 98.44%] [G loss: 3.425305]\n",
      "epoch:36 step:34349 [D loss: 0.160262, acc.: 98.44%] [G loss: 1.963747]\n",
      "epoch:36 step:34350 [D loss: 1.025035, acc.: 54.69%] [G loss: 2.931461]\n",
      "epoch:36 step:34351 [D loss: 0.320986, acc.: 84.38%] [G loss: 5.676102]\n",
      "epoch:36 step:34352 [D loss: 0.521117, acc.: 71.09%] [G loss: 3.701162]\n",
      "epoch:36 step:34353 [D loss: 0.036903, acc.: 99.22%] [G loss: 0.993199]\n",
      "epoch:36 step:34354 [D loss: 0.082123, acc.: 100.00%] [G loss: 7.038840]\n",
      "epoch:36 step:34355 [D loss: 0.316911, acc.: 89.84%] [G loss: 5.196348]\n",
      "epoch:36 step:34356 [D loss: 0.407348, acc.: 80.47%] [G loss: 3.086620]\n",
      "epoch:36 step:34357 [D loss: 0.161935, acc.: 96.88%] [G loss: 4.114000]\n",
      "epoch:36 step:34358 [D loss: 0.048988, acc.: 100.00%] [G loss: 2.113311]\n",
      "epoch:36 step:34359 [D loss: 0.452240, acc.: 80.47%] [G loss: 1.106081]\n",
      "epoch:36 step:34360 [D loss: 0.133842, acc.: 96.88%] [G loss: 4.619894]\n",
      "epoch:36 step:34361 [D loss: 0.478580, acc.: 78.91%] [G loss: 2.815207]\n",
      "epoch:36 step:34362 [D loss: 0.360354, acc.: 77.34%] [G loss: 6.475764]\n",
      "epoch:36 step:34363 [D loss: 0.195002, acc.: 92.97%] [G loss: 4.136795]\n",
      "epoch:36 step:34364 [D loss: 0.398851, acc.: 82.03%] [G loss: 2.270950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34365 [D loss: 0.305733, acc.: 88.28%] [G loss: 1.193739]\n",
      "epoch:36 step:34366 [D loss: 0.025618, acc.: 100.00%] [G loss: 4.468705]\n",
      "epoch:36 step:34367 [D loss: 0.085785, acc.: 99.22%] [G loss: 3.407530]\n",
      "epoch:36 step:34368 [D loss: 0.256958, acc.: 89.84%] [G loss: 1.449412]\n",
      "epoch:36 step:34369 [D loss: 0.137669, acc.: 96.88%] [G loss: 2.900100]\n",
      "epoch:36 step:34370 [D loss: 0.130180, acc.: 98.44%] [G loss: 4.232129]\n",
      "epoch:36 step:34371 [D loss: 0.138920, acc.: 97.66%] [G loss: 4.170098]\n",
      "epoch:36 step:34372 [D loss: 0.139169, acc.: 96.88%] [G loss: 3.591171]\n",
      "epoch:36 step:34373 [D loss: 0.369785, acc.: 79.69%] [G loss: 11.268298]\n",
      "epoch:36 step:34374 [D loss: 0.304910, acc.: 82.03%] [G loss: 3.983848]\n",
      "epoch:36 step:34375 [D loss: 0.049824, acc.: 99.22%] [G loss: 4.492777]\n",
      "epoch:36 step:34376 [D loss: 0.028782, acc.: 99.22%] [G loss: 5.220764]\n",
      "epoch:36 step:34377 [D loss: 0.258896, acc.: 89.84%] [G loss: 7.525315]\n",
      "epoch:36 step:34378 [D loss: 0.185098, acc.: 89.06%] [G loss: 5.678772]\n",
      "epoch:36 step:34379 [D loss: 0.189934, acc.: 92.19%] [G loss: 5.432738]\n",
      "epoch:36 step:34380 [D loss: 0.147099, acc.: 96.09%] [G loss: 4.183943]\n",
      "epoch:36 step:34381 [D loss: 0.059308, acc.: 99.22%] [G loss: 7.091954]\n",
      "epoch:36 step:34382 [D loss: 0.103724, acc.: 99.22%] [G loss: 6.385526]\n",
      "epoch:36 step:34383 [D loss: 0.813116, acc.: 60.16%] [G loss: 7.136364]\n",
      "epoch:36 step:34384 [D loss: 0.151358, acc.: 96.88%] [G loss: 8.546190]\n",
      "epoch:36 step:34385 [D loss: 0.135897, acc.: 96.09%] [G loss: 4.807789]\n",
      "epoch:36 step:34386 [D loss: 0.139709, acc.: 96.88%] [G loss: 5.933650]\n",
      "epoch:36 step:34387 [D loss: 1.640645, acc.: 51.56%] [G loss: 1.901164]\n",
      "epoch:36 step:34388 [D loss: 0.235206, acc.: 88.28%] [G loss: 5.667868]\n",
      "epoch:36 step:34389 [D loss: 0.558417, acc.: 71.09%] [G loss: 3.146539]\n",
      "epoch:36 step:34390 [D loss: 0.147739, acc.: 96.88%] [G loss: 4.405491]\n",
      "epoch:36 step:34391 [D loss: 0.395750, acc.: 85.94%] [G loss: 3.307966]\n",
      "epoch:36 step:34392 [D loss: 0.037614, acc.: 100.00%] [G loss: 5.683249]\n",
      "epoch:36 step:34393 [D loss: 0.015879, acc.: 100.00%] [G loss: 7.476991]\n",
      "epoch:36 step:34394 [D loss: 0.150874, acc.: 96.09%] [G loss: 4.570763]\n",
      "epoch:36 step:34395 [D loss: 0.064914, acc.: 99.22%] [G loss: 6.431687]\n",
      "epoch:36 step:34396 [D loss: 0.007010, acc.: 100.00%] [G loss: 4.728280]\n",
      "epoch:36 step:34397 [D loss: 0.044011, acc.: 100.00%] [G loss: 6.979479]\n",
      "epoch:36 step:34398 [D loss: 0.263081, acc.: 93.75%] [G loss: 2.223199]\n",
      "epoch:36 step:34399 [D loss: 0.659761, acc.: 60.94%] [G loss: 4.896227]\n",
      "epoch:36 step:34400 [D loss: 0.170995, acc.: 96.09%] [G loss: 7.274653]\n",
      "epoch:36 step:34401 [D loss: 0.032067, acc.: 99.22%] [G loss: 2.442692]\n",
      "epoch:36 step:34402 [D loss: 0.262852, acc.: 85.94%] [G loss: 6.559283]\n",
      "epoch:36 step:34403 [D loss: 0.256534, acc.: 92.19%] [G loss: 5.983914]\n",
      "epoch:36 step:34404 [D loss: 0.292306, acc.: 85.16%] [G loss: 3.893449]\n",
      "epoch:36 step:34405 [D loss: 0.756801, acc.: 59.38%] [G loss: 0.646256]\n",
      "epoch:36 step:34406 [D loss: 0.030237, acc.: 100.00%] [G loss: 2.932923]\n",
      "epoch:36 step:34407 [D loss: 0.025334, acc.: 99.22%] [G loss: 3.984411]\n",
      "epoch:36 step:34408 [D loss: 0.055280, acc.: 99.22%] [G loss: 2.588173]\n",
      "epoch:36 step:34409 [D loss: 0.155987, acc.: 97.66%] [G loss: 5.447228]\n",
      "epoch:36 step:34410 [D loss: 0.141615, acc.: 97.66%] [G loss: 7.832594]\n",
      "epoch:36 step:34411 [D loss: 0.152986, acc.: 96.88%] [G loss: 0.831173]\n",
      "epoch:36 step:34412 [D loss: 0.073470, acc.: 97.66%] [G loss: 4.370284]\n",
      "epoch:36 step:34413 [D loss: 1.263677, acc.: 36.72%] [G loss: 1.087562]\n",
      "epoch:36 step:34414 [D loss: 0.247076, acc.: 90.62%] [G loss: 2.675164]\n",
      "epoch:36 step:34415 [D loss: 0.067254, acc.: 100.00%] [G loss: 0.634846]\n",
      "epoch:36 step:34416 [D loss: 0.019144, acc.: 100.00%] [G loss: 1.128996]\n",
      "epoch:36 step:34417 [D loss: 0.047536, acc.: 99.22%] [G loss: 1.201208]\n",
      "epoch:36 step:34418 [D loss: 0.073785, acc.: 97.66%] [G loss: 3.352778]\n",
      "epoch:36 step:34419 [D loss: 0.017606, acc.: 100.00%] [G loss: 3.295345]\n",
      "epoch:36 step:34420 [D loss: 0.075309, acc.: 99.22%] [G loss: 4.246421]\n",
      "epoch:36 step:34421 [D loss: 0.149274, acc.: 97.66%] [G loss: 1.223557]\n",
      "epoch:36 step:34422 [D loss: 0.280097, acc.: 90.62%] [G loss: 1.355618]\n",
      "epoch:36 step:34423 [D loss: 0.414528, acc.: 75.00%] [G loss: 5.535110]\n",
      "epoch:36 step:34424 [D loss: 0.226306, acc.: 94.53%] [G loss: 1.897255]\n",
      "epoch:36 step:34425 [D loss: 0.554211, acc.: 67.97%] [G loss: 2.241215]\n",
      "epoch:36 step:34426 [D loss: 0.034766, acc.: 98.44%] [G loss: 2.401723]\n",
      "epoch:36 step:34427 [D loss: 0.053494, acc.: 99.22%] [G loss: 4.731698]\n",
      "epoch:36 step:34428 [D loss: 0.014268, acc.: 100.00%] [G loss: 1.256637]\n",
      "epoch:36 step:34429 [D loss: 0.050667, acc.: 100.00%] [G loss: 1.802122]\n",
      "epoch:36 step:34430 [D loss: 0.040040, acc.: 99.22%] [G loss: 0.304159]\n",
      "epoch:36 step:34431 [D loss: 0.021538, acc.: 100.00%] [G loss: 3.753422]\n",
      "epoch:36 step:34432 [D loss: 0.860081, acc.: 58.59%] [G loss: 3.211911]\n",
      "epoch:36 step:34433 [D loss: 0.041049, acc.: 98.44%] [G loss: 2.577239]\n",
      "epoch:36 step:34434 [D loss: 0.544991, acc.: 65.62%] [G loss: 1.120673]\n",
      "epoch:36 step:34435 [D loss: 1.237144, acc.: 40.62%] [G loss: 2.078147]\n",
      "epoch:36 step:34436 [D loss: 0.093689, acc.: 97.66%] [G loss: 2.688216]\n",
      "epoch:36 step:34437 [D loss: 0.279081, acc.: 83.59%] [G loss: 0.694655]\n",
      "epoch:36 step:34438 [D loss: 0.020902, acc.: 100.00%] [G loss: 3.753162]\n",
      "epoch:36 step:34439 [D loss: 0.208197, acc.: 93.75%] [G loss: 6.324804]\n",
      "epoch:36 step:34440 [D loss: 0.064442, acc.: 97.66%] [G loss: 5.886734]\n",
      "epoch:36 step:34441 [D loss: 0.119161, acc.: 97.66%] [G loss: 5.781573]\n",
      "epoch:36 step:34442 [D loss: 0.401938, acc.: 76.56%] [G loss: 5.425826]\n",
      "epoch:36 step:34443 [D loss: 0.069957, acc.: 98.44%] [G loss: 7.391353]\n",
      "epoch:36 step:34444 [D loss: 0.038812, acc.: 100.00%] [G loss: 6.172132]\n",
      "epoch:36 step:34445 [D loss: 0.303926, acc.: 82.81%] [G loss: 0.982030]\n",
      "epoch:36 step:34446 [D loss: 0.069157, acc.: 100.00%] [G loss: 4.429599]\n",
      "epoch:36 step:34447 [D loss: 0.030819, acc.: 100.00%] [G loss: 2.084519]\n",
      "epoch:36 step:34448 [D loss: 0.147354, acc.: 97.66%] [G loss: 4.136382]\n",
      "epoch:36 step:34449 [D loss: 0.211256, acc.: 98.44%] [G loss: 5.205706]\n",
      "epoch:36 step:34450 [D loss: 0.062843, acc.: 97.66%] [G loss: 4.787521]\n",
      "epoch:36 step:34451 [D loss: 0.035080, acc.: 99.22%] [G loss: 6.002829]\n",
      "epoch:36 step:34452 [D loss: 0.082968, acc.: 98.44%] [G loss: 3.625910]\n",
      "epoch:36 step:34453 [D loss: 0.046093, acc.: 100.00%] [G loss: 0.999834]\n",
      "epoch:36 step:34454 [D loss: 0.225602, acc.: 88.28%] [G loss: 3.984246]\n",
      "epoch:36 step:34455 [D loss: 0.090858, acc.: 99.22%] [G loss: 4.533744]\n",
      "epoch:36 step:34456 [D loss: 0.064575, acc.: 99.22%] [G loss: 3.343464]\n",
      "epoch:36 step:34457 [D loss: 1.097032, acc.: 57.81%] [G loss: 0.792218]\n",
      "epoch:36 step:34458 [D loss: 0.524919, acc.: 68.75%] [G loss: 2.653196]\n",
      "epoch:36 step:34459 [D loss: 0.191020, acc.: 94.53%] [G loss: 2.984333]\n",
      "epoch:36 step:34460 [D loss: 0.098999, acc.: 98.44%] [G loss: 4.289258]\n",
      "epoch:36 step:34461 [D loss: 0.003861, acc.: 100.00%] [G loss: 5.767340]\n",
      "epoch:36 step:34462 [D loss: 0.075319, acc.: 100.00%] [G loss: 3.445724]\n",
      "epoch:36 step:34463 [D loss: 1.143467, acc.: 51.56%] [G loss: 2.714924]\n",
      "epoch:36 step:34464 [D loss: 0.122550, acc.: 97.66%] [G loss: 2.685439]\n",
      "epoch:36 step:34465 [D loss: 0.110209, acc.: 96.88%] [G loss: 6.488841]\n",
      "epoch:36 step:34466 [D loss: 0.498034, acc.: 70.31%] [G loss: 5.009860]\n",
      "epoch:36 step:34467 [D loss: 0.031965, acc.: 100.00%] [G loss: 2.937745]\n",
      "epoch:36 step:34468 [D loss: 0.158036, acc.: 96.88%] [G loss: 8.754766]\n",
      "epoch:36 step:34469 [D loss: 0.355905, acc.: 86.72%] [G loss: 9.188311]\n",
      "epoch:36 step:34470 [D loss: 0.379157, acc.: 75.78%] [G loss: 2.830729]\n",
      "epoch:36 step:34471 [D loss: 0.131824, acc.: 98.44%] [G loss: 2.103303]\n",
      "epoch:36 step:34472 [D loss: 0.050237, acc.: 100.00%] [G loss: 4.440770]\n",
      "epoch:36 step:34473 [D loss: 0.455757, acc.: 78.91%] [G loss: 4.082722]\n",
      "epoch:36 step:34474 [D loss: 0.092733, acc.: 98.44%] [G loss: 6.480322]\n",
      "epoch:36 step:34475 [D loss: 0.214478, acc.: 96.88%] [G loss: 4.598352]\n",
      "epoch:36 step:34476 [D loss: 0.028380, acc.: 100.00%] [G loss: 6.001330]\n",
      "epoch:36 step:34477 [D loss: 0.231519, acc.: 92.19%] [G loss: 3.571905]\n",
      "epoch:36 step:34478 [D loss: 0.155516, acc.: 98.44%] [G loss: 2.820191]\n",
      "epoch:36 step:34479 [D loss: 0.467647, acc.: 82.03%] [G loss: 2.430561]\n",
      "epoch:36 step:34480 [D loss: 0.033919, acc.: 100.00%] [G loss: 4.319482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34481 [D loss: 0.453595, acc.: 78.12%] [G loss: 5.377439]\n",
      "epoch:36 step:34482 [D loss: 0.469452, acc.: 70.31%] [G loss: 6.415966]\n",
      "epoch:36 step:34483 [D loss: 0.437466, acc.: 74.22%] [G loss: 3.985361]\n",
      "epoch:36 step:34484 [D loss: 0.871939, acc.: 46.88%] [G loss: 3.014020]\n",
      "epoch:36 step:34485 [D loss: 0.043375, acc.: 100.00%] [G loss: 6.582998]\n",
      "epoch:36 step:34486 [D loss: 0.104029, acc.: 97.66%] [G loss: 9.358337]\n",
      "epoch:36 step:34487 [D loss: 0.155428, acc.: 96.09%] [G loss: 5.230802]\n",
      "epoch:36 step:34488 [D loss: 0.204568, acc.: 97.66%] [G loss: 3.930658]\n",
      "epoch:36 step:34489 [D loss: 0.615222, acc.: 64.06%] [G loss: 1.441391]\n",
      "epoch:36 step:34490 [D loss: 0.276632, acc.: 89.84%] [G loss: 4.368911]\n",
      "epoch:36 step:34491 [D loss: 0.108261, acc.: 96.09%] [G loss: 4.137043]\n",
      "epoch:36 step:34492 [D loss: 0.149907, acc.: 95.31%] [G loss: 7.245437]\n",
      "epoch:36 step:34493 [D loss: 0.231238, acc.: 92.19%] [G loss: 4.735967]\n",
      "epoch:36 step:34494 [D loss: 0.252035, acc.: 92.97%] [G loss: 2.992341]\n",
      "epoch:36 step:34495 [D loss: 0.217549, acc.: 93.75%] [G loss: 5.404496]\n",
      "epoch:36 step:34496 [D loss: 0.263150, acc.: 90.62%] [G loss: 7.683940]\n",
      "epoch:36 step:34497 [D loss: 0.016076, acc.: 100.00%] [G loss: 4.256577]\n",
      "epoch:36 step:34498 [D loss: 0.082295, acc.: 97.66%] [G loss: 5.565159]\n",
      "epoch:36 step:34499 [D loss: 0.142123, acc.: 94.53%] [G loss: 3.502875]\n",
      "epoch:36 step:34500 [D loss: 0.012730, acc.: 100.00%] [G loss: 3.925582]\n",
      "epoch:36 step:34501 [D loss: 0.064616, acc.: 98.44%] [G loss: 4.393865]\n",
      "epoch:36 step:34502 [D loss: 0.122126, acc.: 98.44%] [G loss: 3.677275]\n",
      "epoch:36 step:34503 [D loss: 0.185952, acc.: 95.31%] [G loss: 4.332290]\n",
      "epoch:36 step:34504 [D loss: 0.357834, acc.: 79.69%] [G loss: 5.031132]\n",
      "epoch:36 step:34505 [D loss: 0.154039, acc.: 96.88%] [G loss: 2.860430]\n",
      "epoch:36 step:34506 [D loss: 0.338654, acc.: 90.62%] [G loss: 4.303503]\n",
      "epoch:36 step:34507 [D loss: 0.239444, acc.: 94.53%] [G loss: 3.744736]\n",
      "epoch:36 step:34508 [D loss: 0.412560, acc.: 85.94%] [G loss: 3.761906]\n",
      "epoch:36 step:34509 [D loss: 0.054040, acc.: 99.22%] [G loss: 3.452802]\n",
      "epoch:36 step:34510 [D loss: 0.071316, acc.: 98.44%] [G loss: 4.675058]\n",
      "epoch:36 step:34511 [D loss: 1.055555, acc.: 42.97%] [G loss: 2.493372]\n",
      "epoch:36 step:34512 [D loss: 0.224779, acc.: 95.31%] [G loss: 5.561991]\n",
      "epoch:36 step:34513 [D loss: 0.554816, acc.: 70.31%] [G loss: 2.669213]\n",
      "epoch:36 step:34514 [D loss: 0.071124, acc.: 100.00%] [G loss: 3.452564]\n",
      "epoch:36 step:34515 [D loss: 0.376737, acc.: 76.56%] [G loss: 1.591054]\n",
      "epoch:36 step:34516 [D loss: 0.017727, acc.: 100.00%] [G loss: 1.713464]\n",
      "epoch:36 step:34517 [D loss: 0.684573, acc.: 59.38%] [G loss: 0.993063]\n",
      "epoch:36 step:34518 [D loss: 0.446640, acc.: 71.88%] [G loss: 3.144297]\n",
      "epoch:36 step:34519 [D loss: 0.356907, acc.: 81.25%] [G loss: 6.053641]\n",
      "epoch:36 step:34520 [D loss: 0.460554, acc.: 73.44%] [G loss: 7.453110]\n",
      "epoch:36 step:34521 [D loss: 0.739144, acc.: 60.16%] [G loss: 2.385185]\n",
      "epoch:36 step:34522 [D loss: 0.532962, acc.: 71.09%] [G loss: 3.134208]\n",
      "epoch:36 step:34523 [D loss: 0.009129, acc.: 100.00%] [G loss: 5.970712]\n",
      "epoch:36 step:34524 [D loss: 0.049269, acc.: 99.22%] [G loss: 2.718380]\n",
      "epoch:36 step:34525 [D loss: 0.160111, acc.: 93.75%] [G loss: 6.435896]\n",
      "epoch:36 step:34526 [D loss: 0.223025, acc.: 91.41%] [G loss: 2.084073]\n",
      "epoch:36 step:34527 [D loss: 0.120082, acc.: 99.22%] [G loss: 4.205295]\n",
      "epoch:36 step:34528 [D loss: 0.076654, acc.: 99.22%] [G loss: 6.022758]\n",
      "epoch:36 step:34529 [D loss: 1.429735, acc.: 31.25%] [G loss: 4.411043]\n",
      "epoch:36 step:34530 [D loss: 0.201193, acc.: 93.75%] [G loss: 6.397234]\n",
      "epoch:36 step:34531 [D loss: 0.102096, acc.: 98.44%] [G loss: 0.720579]\n",
      "epoch:36 step:34532 [D loss: 0.228520, acc.: 92.19%] [G loss: 3.919609]\n",
      "epoch:36 step:34533 [D loss: 1.222048, acc.: 52.34%] [G loss: 2.175104]\n",
      "epoch:36 step:34534 [D loss: 0.164676, acc.: 96.09%] [G loss: 5.162482]\n",
      "epoch:36 step:34535 [D loss: 0.043749, acc.: 100.00%] [G loss: 4.941899]\n",
      "epoch:36 step:34536 [D loss: 0.038815, acc.: 99.22%] [G loss: 1.930050]\n",
      "epoch:36 step:34537 [D loss: 0.098499, acc.: 98.44%] [G loss: 1.009419]\n",
      "epoch:36 step:34538 [D loss: 0.338652, acc.: 78.91%] [G loss: 0.924054]\n",
      "epoch:36 step:34539 [D loss: 0.098714, acc.: 98.44%] [G loss: 4.782375]\n",
      "epoch:36 step:34540 [D loss: 0.635604, acc.: 65.62%] [G loss: 2.396492]\n",
      "epoch:36 step:34541 [D loss: 0.249904, acc.: 87.50%] [G loss: 3.428309]\n",
      "epoch:36 step:34542 [D loss: 0.144614, acc.: 96.88%] [G loss: 1.538902]\n",
      "epoch:36 step:34543 [D loss: 0.030715, acc.: 100.00%] [G loss: 2.035167]\n",
      "epoch:36 step:34544 [D loss: 0.865884, acc.: 57.03%] [G loss: 0.398842]\n",
      "epoch:36 step:34545 [D loss: 1.213309, acc.: 50.78%] [G loss: 1.394249]\n",
      "epoch:36 step:34546 [D loss: 0.565205, acc.: 68.75%] [G loss: 2.507071]\n",
      "epoch:36 step:34547 [D loss: 0.238916, acc.: 90.62%] [G loss: 4.084926]\n",
      "epoch:36 step:34548 [D loss: 0.116283, acc.: 98.44%] [G loss: 3.838957]\n",
      "epoch:36 step:34549 [D loss: 0.992591, acc.: 54.69%] [G loss: 4.343579]\n",
      "epoch:36 step:34550 [D loss: 0.334800, acc.: 82.03%] [G loss: 3.983147]\n",
      "epoch:36 step:34551 [D loss: 0.070879, acc.: 99.22%] [G loss: 4.563780]\n",
      "epoch:36 step:34552 [D loss: 0.059740, acc.: 100.00%] [G loss: 4.348914]\n",
      "epoch:36 step:34553 [D loss: 0.197659, acc.: 97.66%] [G loss: 3.896566]\n",
      "epoch:36 step:34554 [D loss: 0.070343, acc.: 99.22%] [G loss: 2.029203]\n",
      "epoch:36 step:34555 [D loss: 0.200456, acc.: 95.31%] [G loss: 0.969690]\n",
      "epoch:36 step:34556 [D loss: 0.166413, acc.: 97.66%] [G loss: 7.041372]\n",
      "epoch:36 step:34557 [D loss: 0.482056, acc.: 78.91%] [G loss: 5.072058]\n",
      "epoch:36 step:34558 [D loss: 0.138662, acc.: 96.09%] [G loss: 5.334531]\n",
      "epoch:36 step:34559 [D loss: 0.165740, acc.: 96.88%] [G loss: 3.124444]\n",
      "epoch:36 step:34560 [D loss: 0.110567, acc.: 98.44%] [G loss: 1.309313]\n",
      "epoch:36 step:34561 [D loss: 0.166797, acc.: 93.75%] [G loss: 3.567253]\n",
      "epoch:36 step:34562 [D loss: 0.159038, acc.: 96.09%] [G loss: 3.897980]\n",
      "epoch:36 step:34563 [D loss: 0.259033, acc.: 94.53%] [G loss: 2.948597]\n",
      "epoch:36 step:34564 [D loss: 0.120515, acc.: 96.88%] [G loss: 5.633090]\n",
      "epoch:36 step:34565 [D loss: 0.054033, acc.: 99.22%] [G loss: 3.728112]\n",
      "epoch:36 step:34566 [D loss: 0.526655, acc.: 66.41%] [G loss: 2.231986]\n",
      "epoch:36 step:34567 [D loss: 0.305185, acc.: 85.16%] [G loss: 1.386674]\n",
      "epoch:36 step:34568 [D loss: 0.023700, acc.: 100.00%] [G loss: 6.731198]\n",
      "epoch:36 step:34569 [D loss: 0.082241, acc.: 99.22%] [G loss: 3.380690]\n",
      "epoch:36 step:34570 [D loss: 0.206888, acc.: 91.41%] [G loss: 3.714761]\n",
      "epoch:36 step:34571 [D loss: 0.158298, acc.: 95.31%] [G loss: 1.447975]\n",
      "epoch:36 step:34572 [D loss: 0.055416, acc.: 100.00%] [G loss: 1.737407]\n",
      "epoch:36 step:34573 [D loss: 0.181449, acc.: 95.31%] [G loss: 3.454174]\n",
      "epoch:36 step:34574 [D loss: 0.475152, acc.: 78.12%] [G loss: 1.957618]\n",
      "epoch:36 step:34575 [D loss: 0.207469, acc.: 92.19%] [G loss: 3.670091]\n",
      "epoch:36 step:34576 [D loss: 0.367592, acc.: 80.47%] [G loss: 3.880177]\n",
      "epoch:36 step:34577 [D loss: 0.071859, acc.: 99.22%] [G loss: 0.986410]\n",
      "epoch:36 step:34578 [D loss: 0.302640, acc.: 92.97%] [G loss: 0.543005]\n",
      "epoch:36 step:34579 [D loss: 0.280253, acc.: 89.06%] [G loss: 2.390877]\n",
      "epoch:36 step:34580 [D loss: 0.196473, acc.: 92.19%] [G loss: 2.295575]\n",
      "epoch:36 step:34581 [D loss: 0.073293, acc.: 99.22%] [G loss: 6.887612]\n",
      "epoch:36 step:34582 [D loss: 0.081089, acc.: 97.66%] [G loss: 4.419028]\n",
      "epoch:36 step:34583 [D loss: 0.028955, acc.: 100.00%] [G loss: 2.273731]\n",
      "epoch:36 step:34584 [D loss: 0.033632, acc.: 98.44%] [G loss: 3.123344]\n",
      "epoch:36 step:34585 [D loss: 0.179131, acc.: 97.66%] [G loss: 7.607137]\n",
      "epoch:36 step:34586 [D loss: 0.060892, acc.: 99.22%] [G loss: 3.443322]\n",
      "epoch:36 step:34587 [D loss: 0.141516, acc.: 96.88%] [G loss: 7.567946]\n",
      "epoch:36 step:34588 [D loss: 0.046016, acc.: 98.44%] [G loss: 2.390869]\n",
      "epoch:36 step:34589 [D loss: 0.047959, acc.: 99.22%] [G loss: 3.737659]\n",
      "epoch:36 step:34590 [D loss: 0.009772, acc.: 100.00%] [G loss: 1.395668]\n",
      "epoch:36 step:34591 [D loss: 0.113609, acc.: 98.44%] [G loss: 2.678535]\n",
      "epoch:36 step:34592 [D loss: 0.263049, acc.: 90.62%] [G loss: 5.520037]\n",
      "epoch:36 step:34593 [D loss: 0.118557, acc.: 97.66%] [G loss: 5.427134]\n",
      "epoch:36 step:34594 [D loss: 0.050575, acc.: 99.22%] [G loss: 4.740955]\n",
      "epoch:36 step:34595 [D loss: 0.063553, acc.: 99.22%] [G loss: 5.340058]\n",
      "epoch:36 step:34596 [D loss: 0.065340, acc.: 97.66%] [G loss: 3.482377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:34597 [D loss: 0.619736, acc.: 64.06%] [G loss: 1.306881]\n",
      "epoch:36 step:34598 [D loss: 0.081281, acc.: 100.00%] [G loss: 2.050266]\n",
      "epoch:36 step:34599 [D loss: 1.255188, acc.: 53.91%] [G loss: 4.197844]\n",
      "epoch:36 step:34600 [D loss: 0.175092, acc.: 96.09%] [G loss: 1.282891]\n",
      "epoch:36 step:34601 [D loss: 0.041793, acc.: 100.00%] [G loss: 4.995246]\n",
      "epoch:36 step:34602 [D loss: 0.256313, acc.: 85.94%] [G loss: 2.064577]\n",
      "epoch:36 step:34603 [D loss: 1.753109, acc.: 52.34%] [G loss: 0.185515]\n",
      "epoch:36 step:34604 [D loss: 1.090515, acc.: 53.91%] [G loss: 1.891332]\n",
      "epoch:36 step:34605 [D loss: 0.137520, acc.: 96.09%] [G loss: 6.305216]\n",
      "epoch:36 step:34606 [D loss: 0.976667, acc.: 49.22%] [G loss: 3.143937]\n",
      "epoch:36 step:34607 [D loss: 0.305954, acc.: 83.59%] [G loss: 5.523860]\n",
      "epoch:36 step:34608 [D loss: 0.895314, acc.: 57.81%] [G loss: 5.296973]\n",
      "epoch:36 step:34609 [D loss: 0.099988, acc.: 98.44%] [G loss: 2.026118]\n",
      "epoch:36 step:34610 [D loss: 0.131935, acc.: 98.44%] [G loss: 4.802793]\n",
      "epoch:36 step:34611 [D loss: 0.037417, acc.: 99.22%] [G loss: 6.268647]\n",
      "epoch:36 step:34612 [D loss: 0.099385, acc.: 96.09%] [G loss: 4.594808]\n",
      "epoch:36 step:34613 [D loss: 0.198519, acc.: 89.84%] [G loss: 2.562178]\n",
      "epoch:36 step:34614 [D loss: 0.091715, acc.: 97.66%] [G loss: 2.624344]\n",
      "epoch:36 step:34615 [D loss: 0.306416, acc.: 87.50%] [G loss: 1.035444]\n",
      "epoch:36 step:34616 [D loss: 0.103573, acc.: 99.22%] [G loss: 3.699273]\n",
      "epoch:36 step:34617 [D loss: 0.363861, acc.: 85.94%] [G loss: 1.530271]\n",
      "epoch:36 step:34618 [D loss: 0.238591, acc.: 88.28%] [G loss: 2.276144]\n",
      "epoch:36 step:34619 [D loss: 0.083714, acc.: 97.66%] [G loss: 1.393479]\n",
      "epoch:36 step:34620 [D loss: 0.147025, acc.: 98.44%] [G loss: 2.491167]\n",
      "epoch:36 step:34621 [D loss: 0.088058, acc.: 98.44%] [G loss: 3.141591]\n",
      "epoch:36 step:34622 [D loss: 0.672939, acc.: 61.72%] [G loss: 0.654709]\n",
      "epoch:36 step:34623 [D loss: 0.096949, acc.: 97.66%] [G loss: 2.722579]\n",
      "epoch:36 step:34624 [D loss: 0.257780, acc.: 90.62%] [G loss: 2.405066]\n",
      "epoch:36 step:34625 [D loss: 0.309829, acc.: 83.59%] [G loss: 4.301998]\n",
      "epoch:36 step:34626 [D loss: 0.308838, acc.: 91.41%] [G loss: 3.525283]\n",
      "epoch:36 step:34627 [D loss: 0.406656, acc.: 81.25%] [G loss: 3.710226]\n",
      "epoch:36 step:34628 [D loss: 0.084454, acc.: 98.44%] [G loss: 3.247315]\n",
      "epoch:36 step:34629 [D loss: 0.678173, acc.: 57.03%] [G loss: 4.261365]\n",
      "epoch:36 step:34630 [D loss: 0.260801, acc.: 89.06%] [G loss: 3.395729]\n",
      "epoch:36 step:34631 [D loss: 0.137562, acc.: 97.66%] [G loss: 2.413236]\n",
      "epoch:36 step:34632 [D loss: 0.213847, acc.: 92.97%] [G loss: 3.238482]\n",
      "epoch:36 step:34633 [D loss: 0.059123, acc.: 99.22%] [G loss: 2.462359]\n",
      "epoch:36 step:34634 [D loss: 0.854428, acc.: 55.47%] [G loss: 0.817087]\n",
      "epoch:36 step:34635 [D loss: 0.146317, acc.: 98.44%] [G loss: 3.505535]\n",
      "epoch:36 step:34636 [D loss: 0.037776, acc.: 100.00%] [G loss: 8.796518]\n",
      "epoch:36 step:34637 [D loss: 0.386815, acc.: 82.03%] [G loss: 2.072217]\n",
      "epoch:36 step:34638 [D loss: 1.130192, acc.: 42.19%] [G loss: 0.543732]\n",
      "epoch:36 step:34639 [D loss: 0.537485, acc.: 67.19%] [G loss: 2.916904]\n",
      "epoch:36 step:34640 [D loss: 0.911021, acc.: 59.38%] [G loss: 1.680394]\n",
      "epoch:36 step:34641 [D loss: 0.180464, acc.: 95.31%] [G loss: 3.264081]\n",
      "epoch:36 step:34642 [D loss: 0.138141, acc.: 97.66%] [G loss: 0.453299]\n",
      "epoch:36 step:34643 [D loss: 0.104689, acc.: 99.22%] [G loss: 8.540997]\n",
      "epoch:36 step:34644 [D loss: 0.213075, acc.: 94.53%] [G loss: 2.949709]\n",
      "epoch:36 step:34645 [D loss: 0.180998, acc.: 96.09%] [G loss: 3.908402]\n",
      "epoch:36 step:34646 [D loss: 0.060045, acc.: 100.00%] [G loss: 3.385468]\n",
      "epoch:36 step:34647 [D loss: 0.351450, acc.: 80.47%] [G loss: 2.895590]\n",
      "epoch:36 step:34648 [D loss: 0.741531, acc.: 55.47%] [G loss: 1.828701]\n",
      "epoch:36 step:34649 [D loss: 0.169203, acc.: 96.09%] [G loss: 3.202809]\n",
      "epoch:36 step:34650 [D loss: 0.035196, acc.: 100.00%] [G loss: 5.443642]\n",
      "epoch:36 step:34651 [D loss: 0.172406, acc.: 96.88%] [G loss: 2.996054]\n",
      "epoch:36 step:34652 [D loss: 0.135574, acc.: 96.88%] [G loss: 5.804073]\n",
      "epoch:36 step:34653 [D loss: 0.127149, acc.: 97.66%] [G loss: 0.778681]\n",
      "epoch:36 step:34654 [D loss: 0.296699, acc.: 95.31%] [G loss: 1.539908]\n",
      "epoch:36 step:34655 [D loss: 0.255978, acc.: 94.53%] [G loss: 0.588474]\n",
      "epoch:36 step:34656 [D loss: 0.270462, acc.: 89.84%] [G loss: 2.356042]\n",
      "epoch:36 step:34657 [D loss: 0.072725, acc.: 100.00%] [G loss: 4.630713]\n",
      "epoch:36 step:34658 [D loss: 0.016059, acc.: 100.00%] [G loss: 4.163956]\n",
      "epoch:36 step:34659 [D loss: 0.704592, acc.: 57.81%] [G loss: 5.419147]\n",
      "epoch:36 step:34660 [D loss: 0.116582, acc.: 98.44%] [G loss: 4.174060]\n",
      "epoch:36 step:34661 [D loss: 1.078818, acc.: 42.97%] [G loss: 2.114102]\n",
      "epoch:36 step:34662 [D loss: 0.075555, acc.: 98.44%] [G loss: 6.349237]\n",
      "epoch:36 step:34663 [D loss: 0.132517, acc.: 96.88%] [G loss: 6.422956]\n",
      "epoch:36 step:34664 [D loss: 0.129577, acc.: 99.22%] [G loss: 2.996867]\n",
      "epoch:36 step:34665 [D loss: 0.256139, acc.: 89.84%] [G loss: 2.129628]\n",
      "epoch:36 step:34666 [D loss: 0.173791, acc.: 95.31%] [G loss: 7.747777]\n",
      "epoch:36 step:34667 [D loss: 0.052510, acc.: 100.00%] [G loss: 2.670127]\n",
      "epoch:36 step:34668 [D loss: 0.048533, acc.: 99.22%] [G loss: 2.202943]\n",
      "epoch:36 step:34669 [D loss: 0.129803, acc.: 96.88%] [G loss: 5.017712]\n",
      "epoch:37 step:34670 [D loss: 0.701153, acc.: 60.94%] [G loss: 2.632576]\n",
      "epoch:37 step:34671 [D loss: 0.323955, acc.: 80.47%] [G loss: 1.886293]\n",
      "epoch:37 step:34672 [D loss: 0.068541, acc.: 97.66%] [G loss: 5.399519]\n",
      "epoch:37 step:34673 [D loss: 0.274487, acc.: 89.06%] [G loss: 4.077140]\n",
      "epoch:37 step:34674 [D loss: 0.122695, acc.: 99.22%] [G loss: 5.220614]\n",
      "epoch:37 step:34675 [D loss: 0.081419, acc.: 100.00%] [G loss: 4.369092]\n",
      "epoch:37 step:34676 [D loss: 0.104575, acc.: 98.44%] [G loss: 2.229697]\n",
      "epoch:37 step:34677 [D loss: 0.539647, acc.: 65.62%] [G loss: 3.750139]\n",
      "epoch:37 step:34678 [D loss: 0.505140, acc.: 66.41%] [G loss: 4.685757]\n",
      "epoch:37 step:34679 [D loss: 0.103945, acc.: 98.44%] [G loss: 4.823726]\n",
      "epoch:37 step:34680 [D loss: 0.231547, acc.: 89.84%] [G loss: 0.822769]\n",
      "epoch:37 step:34681 [D loss: 0.041653, acc.: 100.00%] [G loss: 4.641714]\n",
      "epoch:37 step:34682 [D loss: 0.011346, acc.: 100.00%] [G loss: 7.270308]\n",
      "epoch:37 step:34683 [D loss: 0.061507, acc.: 99.22%] [G loss: 3.217749]\n",
      "epoch:37 step:34684 [D loss: 0.047623, acc.: 99.22%] [G loss: 5.370506]\n",
      "epoch:37 step:34685 [D loss: 0.977484, acc.: 54.69%] [G loss: 2.799129]\n",
      "epoch:37 step:34686 [D loss: 0.015429, acc.: 100.00%] [G loss: 8.343101]\n",
      "epoch:37 step:34687 [D loss: 0.139030, acc.: 98.44%] [G loss: 4.345961]\n",
      "epoch:37 step:34688 [D loss: 0.116710, acc.: 96.88%] [G loss: 6.954801]\n",
      "epoch:37 step:34689 [D loss: 0.128479, acc.: 98.44%] [G loss: 5.223852]\n",
      "epoch:37 step:34690 [D loss: 0.065357, acc.: 100.00%] [G loss: 4.845906]\n",
      "epoch:37 step:34691 [D loss: 0.078337, acc.: 100.00%] [G loss: 5.691106]\n",
      "epoch:37 step:34692 [D loss: 0.218852, acc.: 90.62%] [G loss: 4.338154]\n",
      "epoch:37 step:34693 [D loss: 0.113270, acc.: 96.88%] [G loss: 3.739331]\n",
      "epoch:37 step:34694 [D loss: 0.198023, acc.: 91.41%] [G loss: 2.878985]\n",
      "epoch:37 step:34695 [D loss: 0.192340, acc.: 94.53%] [G loss: 1.273137]\n",
      "epoch:37 step:34696 [D loss: 0.103165, acc.: 99.22%] [G loss: 3.955314]\n",
      "epoch:37 step:34697 [D loss: 0.191022, acc.: 96.09%] [G loss: 4.656427]\n",
      "epoch:37 step:34698 [D loss: 0.027203, acc.: 100.00%] [G loss: 2.265687]\n",
      "epoch:37 step:34699 [D loss: 0.724448, acc.: 57.81%] [G loss: 3.867287]\n",
      "epoch:37 step:34700 [D loss: 1.142303, acc.: 48.44%] [G loss: 2.643946]\n",
      "epoch:37 step:34701 [D loss: 0.008380, acc.: 100.00%] [G loss: 4.684562]\n",
      "epoch:37 step:34702 [D loss: 0.036873, acc.: 100.00%] [G loss: 3.622161]\n",
      "epoch:37 step:34703 [D loss: 0.084818, acc.: 98.44%] [G loss: 4.668470]\n",
      "epoch:37 step:34704 [D loss: 0.066897, acc.: 100.00%] [G loss: 3.634820]\n",
      "epoch:37 step:34705 [D loss: 0.005874, acc.: 100.00%] [G loss: 5.611934]\n",
      "epoch:37 step:34706 [D loss: 0.005877, acc.: 100.00%] [G loss: 2.894019]\n",
      "epoch:37 step:34707 [D loss: 0.029472, acc.: 100.00%] [G loss: 3.480247]\n",
      "epoch:37 step:34708 [D loss: 0.047578, acc.: 100.00%] [G loss: 7.012152]\n",
      "epoch:37 step:34709 [D loss: 0.022018, acc.: 100.00%] [G loss: 5.674980]\n",
      "epoch:37 step:34710 [D loss: 0.009401, acc.: 100.00%] [G loss: 4.341165]\n",
      "epoch:37 step:34711 [D loss: 0.288371, acc.: 84.38%] [G loss: 0.699503]\n",
      "epoch:37 step:34712 [D loss: 0.100964, acc.: 99.22%] [G loss: 3.180603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:34713 [D loss: 0.086361, acc.: 99.22%] [G loss: 1.999021]\n",
      "epoch:37 step:34714 [D loss: 0.064959, acc.: 99.22%] [G loss: 1.442310]\n",
      "epoch:37 step:34715 [D loss: 0.100567, acc.: 96.88%] [G loss: 2.629580]\n",
      "epoch:37 step:34716 [D loss: 0.091892, acc.: 99.22%] [G loss: 3.369949]\n",
      "epoch:37 step:34717 [D loss: 0.046277, acc.: 100.00%] [G loss: 4.917002]\n",
      "epoch:37 step:34718 [D loss: 0.153227, acc.: 97.66%] [G loss: 3.141804]\n",
      "epoch:37 step:34719 [D loss: 0.032787, acc.: 99.22%] [G loss: 3.602437]\n",
      "epoch:37 step:34720 [D loss: 0.012108, acc.: 100.00%] [G loss: 5.840429]\n",
      "epoch:37 step:34721 [D loss: 0.105658, acc.: 100.00%] [G loss: 5.018174]\n",
      "epoch:37 step:34722 [D loss: 0.182225, acc.: 93.75%] [G loss: 3.802694]\n",
      "epoch:37 step:34723 [D loss: 0.381953, acc.: 82.03%] [G loss: 5.642308]\n",
      "epoch:37 step:34724 [D loss: 0.025032, acc.: 100.00%] [G loss: 6.360213]\n",
      "epoch:37 step:34725 [D loss: 0.030929, acc.: 99.22%] [G loss: 1.847279]\n",
      "epoch:37 step:34726 [D loss: 0.038276, acc.: 100.00%] [G loss: 6.408585]\n",
      "epoch:37 step:34727 [D loss: 0.014304, acc.: 100.00%] [G loss: 5.168723]\n",
      "epoch:37 step:34728 [D loss: 0.095018, acc.: 99.22%] [G loss: 5.264814]\n",
      "epoch:37 step:34729 [D loss: 0.006908, acc.: 100.00%] [G loss: 3.711617]\n",
      "epoch:37 step:34730 [D loss: 0.235726, acc.: 92.19%] [G loss: 4.280250]\n",
      "epoch:37 step:34731 [D loss: 0.020367, acc.: 100.00%] [G loss: 5.258365]\n",
      "epoch:37 step:34732 [D loss: 0.006218, acc.: 100.00%] [G loss: 1.765679]\n",
      "epoch:37 step:34733 [D loss: 0.065689, acc.: 97.66%] [G loss: 5.143971]\n",
      "epoch:37 step:34734 [D loss: 1.067083, acc.: 46.09%] [G loss: 2.515025]\n",
      "epoch:37 step:34735 [D loss: 0.037727, acc.: 100.00%] [G loss: 5.547166]\n",
      "epoch:37 step:34736 [D loss: 0.372316, acc.: 84.38%] [G loss: 5.699945]\n",
      "epoch:37 step:34737 [D loss: 0.146578, acc.: 95.31%] [G loss: 4.352357]\n",
      "epoch:37 step:34738 [D loss: 0.428842, acc.: 82.03%] [G loss: 8.290412]\n",
      "epoch:37 step:34739 [D loss: 0.090336, acc.: 97.66%] [G loss: 5.823409]\n",
      "epoch:37 step:34740 [D loss: 0.829283, acc.: 53.91%] [G loss: 2.115876]\n",
      "epoch:37 step:34741 [D loss: 0.279526, acc.: 92.19%] [G loss: 3.251920]\n",
      "epoch:37 step:34742 [D loss: 0.018230, acc.: 100.00%] [G loss: 1.565237]\n",
      "epoch:37 step:34743 [D loss: 0.029594, acc.: 100.00%] [G loss: 3.090360]\n",
      "epoch:37 step:34744 [D loss: 0.100422, acc.: 98.44%] [G loss: 3.021440]\n",
      "epoch:37 step:34745 [D loss: 0.451914, acc.: 76.56%] [G loss: 0.889214]\n",
      "epoch:37 step:34746 [D loss: 0.155738, acc.: 94.53%] [G loss: 5.892525]\n",
      "epoch:37 step:34747 [D loss: 0.028888, acc.: 99.22%] [G loss: 4.106510]\n",
      "epoch:37 step:34748 [D loss: 0.009161, acc.: 100.00%] [G loss: 5.440060]\n",
      "epoch:37 step:34749 [D loss: 0.430270, acc.: 80.47%] [G loss: 5.758564]\n",
      "epoch:37 step:34750 [D loss: 0.007446, acc.: 100.00%] [G loss: 7.511839]\n",
      "epoch:37 step:34751 [D loss: 0.347041, acc.: 79.69%] [G loss: 2.986574]\n",
      "epoch:37 step:34752 [D loss: 0.070404, acc.: 99.22%] [G loss: 6.453361]\n",
      "epoch:37 step:34753 [D loss: 0.037811, acc.: 100.00%] [G loss: 4.131104]\n",
      "epoch:37 step:34754 [D loss: 0.239402, acc.: 94.53%] [G loss: 0.683182]\n",
      "epoch:37 step:34755 [D loss: 0.021118, acc.: 100.00%] [G loss: 3.492959]\n",
      "epoch:37 step:34756 [D loss: 0.362555, acc.: 83.59%] [G loss: 1.520964]\n",
      "epoch:37 step:34757 [D loss: 1.290690, acc.: 41.41%] [G loss: 4.470162]\n",
      "epoch:37 step:34758 [D loss: 0.098198, acc.: 98.44%] [G loss: 3.953379]\n",
      "epoch:37 step:34759 [D loss: 0.044701, acc.: 100.00%] [G loss: 0.685799]\n",
      "epoch:37 step:34760 [D loss: 0.079212, acc.: 97.66%] [G loss: 0.084162]\n",
      "epoch:37 step:34761 [D loss: 0.082261, acc.: 99.22%] [G loss: 3.479249]\n",
      "epoch:37 step:34762 [D loss: 0.434370, acc.: 72.66%] [G loss: 5.998108]\n",
      "epoch:37 step:34763 [D loss: 0.115422, acc.: 97.66%] [G loss: 4.267741]\n",
      "epoch:37 step:34764 [D loss: 0.624446, acc.: 64.84%] [G loss: 7.664986]\n",
      "epoch:37 step:34765 [D loss: 0.782767, acc.: 57.03%] [G loss: 3.805717]\n",
      "epoch:37 step:34766 [D loss: 0.054322, acc.: 100.00%] [G loss: 5.204987]\n",
      "epoch:37 step:34767 [D loss: 0.034367, acc.: 100.00%] [G loss: 2.334230]\n",
      "epoch:37 step:34768 [D loss: 0.430709, acc.: 73.44%] [G loss: 5.173584]\n",
      "epoch:37 step:34769 [D loss: 1.162069, acc.: 53.91%] [G loss: 1.788472]\n",
      "epoch:37 step:34770 [D loss: 0.031550, acc.: 100.00%] [G loss: 6.965768]\n",
      "epoch:37 step:34771 [D loss: 0.063972, acc.: 99.22%] [G loss: 6.089950]\n",
      "epoch:37 step:34772 [D loss: 0.081543, acc.: 98.44%] [G loss: 0.773070]\n",
      "epoch:37 step:34773 [D loss: 0.106161, acc.: 98.44%] [G loss: 5.757092]\n",
      "epoch:37 step:34774 [D loss: 0.061664, acc.: 99.22%] [G loss: 4.086603]\n",
      "epoch:37 step:34775 [D loss: 0.212364, acc.: 92.97%] [G loss: 2.219032]\n",
      "epoch:37 step:34776 [D loss: 0.544613, acc.: 67.97%] [G loss: 2.697239]\n",
      "epoch:37 step:34777 [D loss: 1.326889, acc.: 49.22%] [G loss: 2.679945]\n",
      "epoch:37 step:34778 [D loss: 0.435334, acc.: 74.22%] [G loss: 3.103735]\n",
      "epoch:37 step:34779 [D loss: 0.216346, acc.: 92.19%] [G loss: 7.742039]\n",
      "epoch:37 step:34780 [D loss: 0.218038, acc.: 95.31%] [G loss: 4.151239]\n",
      "epoch:37 step:34781 [D loss: 0.021241, acc.: 100.00%] [G loss: 3.365686]\n",
      "epoch:37 step:34782 [D loss: 0.014186, acc.: 100.00%] [G loss: 3.605307]\n",
      "epoch:37 step:34783 [D loss: 0.422079, acc.: 82.03%] [G loss: 3.077495]\n",
      "epoch:37 step:34784 [D loss: 0.100792, acc.: 98.44%] [G loss: 4.166682]\n",
      "epoch:37 step:34785 [D loss: 0.011561, acc.: 100.00%] [G loss: 5.064232]\n",
      "epoch:37 step:34786 [D loss: 0.023377, acc.: 99.22%] [G loss: 1.547656]\n",
      "epoch:37 step:34787 [D loss: 2.040854, acc.: 11.72%] [G loss: 3.939104]\n",
      "epoch:37 step:34788 [D loss: 0.026505, acc.: 100.00%] [G loss: 3.785665]\n",
      "epoch:37 step:34789 [D loss: 0.215789, acc.: 93.75%] [G loss: 6.173748]\n",
      "epoch:37 step:34790 [D loss: 0.111880, acc.: 97.66%] [G loss: 3.395182]\n",
      "epoch:37 step:34791 [D loss: 0.632738, acc.: 68.75%] [G loss: 1.059392]\n",
      "epoch:37 step:34792 [D loss: 0.423099, acc.: 72.66%] [G loss: 3.335450]\n",
      "epoch:37 step:34793 [D loss: 0.066314, acc.: 100.00%] [G loss: 5.506148]\n",
      "epoch:37 step:34794 [D loss: 0.009042, acc.: 100.00%] [G loss: 6.391963]\n",
      "epoch:37 step:34795 [D loss: 0.279617, acc.: 94.53%] [G loss: 5.152876]\n",
      "epoch:37 step:34796 [D loss: 0.018633, acc.: 100.00%] [G loss: 1.292125]\n",
      "epoch:37 step:34797 [D loss: 0.401873, acc.: 73.44%] [G loss: 2.743951]\n",
      "epoch:37 step:34798 [D loss: 0.098419, acc.: 99.22%] [G loss: 6.733278]\n",
      "epoch:37 step:34799 [D loss: 0.772902, acc.: 53.12%] [G loss: 2.936722]\n",
      "epoch:37 step:34800 [D loss: 0.265619, acc.: 93.75%] [G loss: 4.900367]\n",
      "epoch:37 step:34801 [D loss: 0.483939, acc.: 74.22%] [G loss: 6.097430]\n",
      "epoch:37 step:34802 [D loss: 0.182089, acc.: 96.09%] [G loss: 4.956217]\n",
      "epoch:37 step:34803 [D loss: 0.229943, acc.: 92.19%] [G loss: 5.209572]\n",
      "epoch:37 step:34804 [D loss: 0.111751, acc.: 99.22%] [G loss: 3.934851]\n",
      "epoch:37 step:34805 [D loss: 0.258508, acc.: 89.06%] [G loss: 4.941187]\n",
      "epoch:37 step:34806 [D loss: 0.017668, acc.: 100.00%] [G loss: 7.667216]\n",
      "epoch:37 step:34807 [D loss: 0.125443, acc.: 98.44%] [G loss: 4.250280]\n",
      "epoch:37 step:34808 [D loss: 0.138067, acc.: 96.09%] [G loss: 2.596724]\n",
      "epoch:37 step:34809 [D loss: 0.750764, acc.: 61.72%] [G loss: 2.004126]\n",
      "epoch:37 step:34810 [D loss: 0.028834, acc.: 99.22%] [G loss: 2.681755]\n",
      "epoch:37 step:34811 [D loss: 0.071363, acc.: 97.66%] [G loss: 1.559738]\n",
      "epoch:37 step:34812 [D loss: 1.374503, acc.: 51.56%] [G loss: 0.895277]\n",
      "epoch:37 step:34813 [D loss: 0.098892, acc.: 97.66%] [G loss: 7.551291]\n",
      "epoch:37 step:34814 [D loss: 0.296567, acc.: 84.38%] [G loss: 4.973130]\n",
      "epoch:37 step:34815 [D loss: 0.063703, acc.: 100.00%] [G loss: 3.821045]\n",
      "epoch:37 step:34816 [D loss: 0.248088, acc.: 92.97%] [G loss: 2.739373]\n",
      "epoch:37 step:34817 [D loss: 0.021946, acc.: 100.00%] [G loss: 2.325069]\n",
      "epoch:37 step:34818 [D loss: 0.016873, acc.: 100.00%] [G loss: 5.849147]\n",
      "epoch:37 step:34819 [D loss: 0.124094, acc.: 98.44%] [G loss: 2.779596]\n",
      "epoch:37 step:34820 [D loss: 0.114207, acc.: 98.44%] [G loss: 1.453689]\n",
      "epoch:37 step:34821 [D loss: 0.120738, acc.: 97.66%] [G loss: 1.808853]\n",
      "epoch:37 step:34822 [D loss: 0.193854, acc.: 93.75%] [G loss: 4.257763]\n",
      "epoch:37 step:34823 [D loss: 0.112987, acc.: 99.22%] [G loss: 5.542027]\n",
      "epoch:37 step:34824 [D loss: 0.832927, acc.: 57.81%] [G loss: 2.098645]\n",
      "epoch:37 step:34825 [D loss: 0.131737, acc.: 96.88%] [G loss: 2.854269]\n",
      "epoch:37 step:34826 [D loss: 0.197340, acc.: 96.88%] [G loss: 1.947343]\n",
      "epoch:37 step:34827 [D loss: 0.695606, acc.: 57.03%] [G loss: 5.830606]\n",
      "epoch:37 step:34828 [D loss: 0.088969, acc.: 97.66%] [G loss: 3.785139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:34829 [D loss: 0.112363, acc.: 96.88%] [G loss: 4.687004]\n",
      "epoch:37 step:34830 [D loss: 0.088343, acc.: 97.66%] [G loss: 3.886771]\n",
      "epoch:37 step:34831 [D loss: 0.033815, acc.: 100.00%] [G loss: 3.733775]\n",
      "epoch:37 step:34832 [D loss: 0.140917, acc.: 96.88%] [G loss: 2.242061]\n",
      "epoch:37 step:34833 [D loss: 0.083271, acc.: 98.44%] [G loss: 2.227120]\n",
      "epoch:37 step:34834 [D loss: 0.043279, acc.: 99.22%] [G loss: 2.327573]\n",
      "epoch:37 step:34835 [D loss: 0.126441, acc.: 96.88%] [G loss: 4.446175]\n",
      "epoch:37 step:34836 [D loss: 0.065555, acc.: 99.22%] [G loss: 5.720098]\n",
      "epoch:37 step:34837 [D loss: 0.072292, acc.: 100.00%] [G loss: 3.167855]\n",
      "epoch:37 step:34838 [D loss: 0.284809, acc.: 88.28%] [G loss: 3.542542]\n",
      "epoch:37 step:34839 [D loss: 0.057555, acc.: 100.00%] [G loss: 1.254833]\n",
      "epoch:37 step:34840 [D loss: 0.041516, acc.: 99.22%] [G loss: 3.784031]\n",
      "epoch:37 step:34841 [D loss: 0.795972, acc.: 59.38%] [G loss: 5.449752]\n",
      "epoch:37 step:34842 [D loss: 0.029729, acc.: 100.00%] [G loss: 1.128150]\n",
      "epoch:37 step:34843 [D loss: 0.356589, acc.: 78.12%] [G loss: 7.603277]\n",
      "epoch:37 step:34844 [D loss: 0.014979, acc.: 100.00%] [G loss: 8.704233]\n",
      "epoch:37 step:34845 [D loss: 0.060266, acc.: 100.00%] [G loss: 6.552354]\n",
      "epoch:37 step:34846 [D loss: 0.036008, acc.: 99.22%] [G loss: 7.134725]\n",
      "epoch:37 step:34847 [D loss: 0.040524, acc.: 100.00%] [G loss: 7.141761]\n",
      "epoch:37 step:34848 [D loss: 1.161732, acc.: 50.78%] [G loss: 3.859142]\n",
      "epoch:37 step:34849 [D loss: 0.347755, acc.: 87.50%] [G loss: 5.707634]\n",
      "epoch:37 step:34850 [D loss: 0.461363, acc.: 78.91%] [G loss: 1.484601]\n",
      "epoch:37 step:34851 [D loss: 0.160519, acc.: 95.31%] [G loss: 3.707789]\n",
      "epoch:37 step:34852 [D loss: 0.075854, acc.: 98.44%] [G loss: 0.273604]\n",
      "epoch:37 step:34853 [D loss: 0.209555, acc.: 94.53%] [G loss: 7.019582]\n",
      "epoch:37 step:34854 [D loss: 0.239070, acc.: 95.31%] [G loss: 2.192063]\n",
      "epoch:37 step:34855 [D loss: 0.696187, acc.: 60.16%] [G loss: 2.802035]\n",
      "epoch:37 step:34856 [D loss: 0.046418, acc.: 100.00%] [G loss: 1.716098]\n",
      "epoch:37 step:34857 [D loss: 0.090754, acc.: 97.66%] [G loss: 3.457316]\n",
      "epoch:37 step:34858 [D loss: 0.215936, acc.: 96.09%] [G loss: 7.825171]\n",
      "epoch:37 step:34859 [D loss: 0.087098, acc.: 99.22%] [G loss: 3.935770]\n",
      "epoch:37 step:34860 [D loss: 0.149345, acc.: 97.66%] [G loss: 7.456437]\n",
      "epoch:37 step:34861 [D loss: 0.139943, acc.: 98.44%] [G loss: 8.843525]\n",
      "epoch:37 step:34862 [D loss: 0.189261, acc.: 92.97%] [G loss: 1.122703]\n",
      "epoch:37 step:34863 [D loss: 0.152096, acc.: 97.66%] [G loss: 5.001588]\n",
      "epoch:37 step:34864 [D loss: 0.470941, acc.: 78.91%] [G loss: 3.400000]\n",
      "epoch:37 step:34865 [D loss: 0.648984, acc.: 60.16%] [G loss: 2.136654]\n",
      "epoch:37 step:34866 [D loss: 1.877942, acc.: 50.78%] [G loss: 2.959420]\n",
      "epoch:37 step:34867 [D loss: 0.094250, acc.: 96.09%] [G loss: 5.886624]\n",
      "epoch:37 step:34868 [D loss: 0.454506, acc.: 75.00%] [G loss: 2.513978]\n",
      "epoch:37 step:34869 [D loss: 0.623762, acc.: 67.19%] [G loss: 4.464251]\n",
      "epoch:37 step:34870 [D loss: 0.189070, acc.: 95.31%] [G loss: 3.400275]\n",
      "epoch:37 step:34871 [D loss: 0.448745, acc.: 77.34%] [G loss: 1.887310]\n",
      "epoch:37 step:34872 [D loss: 0.019304, acc.: 100.00%] [G loss: 4.359350]\n",
      "epoch:37 step:34873 [D loss: 0.584205, acc.: 66.41%] [G loss: 6.591395]\n",
      "epoch:37 step:34874 [D loss: 0.029113, acc.: 99.22%] [G loss: 5.552618]\n",
      "epoch:37 step:34875 [D loss: 0.054954, acc.: 100.00%] [G loss: 8.008064]\n",
      "epoch:37 step:34876 [D loss: 0.157460, acc.: 94.53%] [G loss: 6.357732]\n",
      "epoch:37 step:34877 [D loss: 0.604043, acc.: 60.94%] [G loss: 1.838815]\n",
      "epoch:37 step:34878 [D loss: 0.059461, acc.: 100.00%] [G loss: 4.353728]\n",
      "epoch:37 step:34879 [D loss: 0.465173, acc.: 76.56%] [G loss: 3.000000]\n",
      "epoch:37 step:34880 [D loss: 0.325685, acc.: 91.41%] [G loss: 5.360688]\n",
      "epoch:37 step:34881 [D loss: 0.606300, acc.: 64.84%] [G loss: 5.348847]\n",
      "epoch:37 step:34882 [D loss: 0.141581, acc.: 96.09%] [G loss: 4.945265]\n",
      "epoch:37 step:34883 [D loss: 0.046033, acc.: 100.00%] [G loss: 3.390019]\n",
      "epoch:37 step:34884 [D loss: 0.088416, acc.: 99.22%] [G loss: 3.196627]\n",
      "epoch:37 step:34885 [D loss: 0.112964, acc.: 98.44%] [G loss: 2.188365]\n",
      "epoch:37 step:34886 [D loss: 0.042459, acc.: 100.00%] [G loss: 4.766049]\n",
      "epoch:37 step:34887 [D loss: 0.415966, acc.: 82.81%] [G loss: 2.657988]\n",
      "epoch:37 step:34888 [D loss: 0.087196, acc.: 97.66%] [G loss: 1.991706]\n",
      "epoch:37 step:34889 [D loss: 0.143825, acc.: 97.66%] [G loss: 6.552324]\n",
      "epoch:37 step:34890 [D loss: 0.066191, acc.: 100.00%] [G loss: 2.523368]\n",
      "epoch:37 step:34891 [D loss: 0.021146, acc.: 99.22%] [G loss: 5.125319]\n",
      "epoch:37 step:34892 [D loss: 0.103555, acc.: 98.44%] [G loss: 2.942420]\n",
      "epoch:37 step:34893 [D loss: 0.158138, acc.: 96.09%] [G loss: 4.197904]\n",
      "epoch:37 step:34894 [D loss: 0.622890, acc.: 59.38%] [G loss: 2.829398]\n",
      "epoch:37 step:34895 [D loss: 0.237537, acc.: 92.19%] [G loss: 3.120832]\n",
      "epoch:37 step:34896 [D loss: 0.597087, acc.: 64.84%] [G loss: 5.557333]\n",
      "epoch:37 step:34897 [D loss: 0.150861, acc.: 96.09%] [G loss: 3.763068]\n",
      "epoch:37 step:34898 [D loss: 0.096377, acc.: 98.44%] [G loss: 5.763963]\n",
      "epoch:37 step:34899 [D loss: 0.043208, acc.: 100.00%] [G loss: 6.459167]\n",
      "epoch:37 step:34900 [D loss: 0.029973, acc.: 100.00%] [G loss: 6.970521]\n",
      "epoch:37 step:34901 [D loss: 0.315696, acc.: 88.28%] [G loss: 2.473041]\n",
      "epoch:37 step:34902 [D loss: 0.109222, acc.: 98.44%] [G loss: 5.836998]\n",
      "epoch:37 step:34903 [D loss: 0.093659, acc.: 98.44%] [G loss: 3.880031]\n",
      "epoch:37 step:34904 [D loss: 0.115022, acc.: 98.44%] [G loss: 2.272039]\n",
      "epoch:37 step:34905 [D loss: 0.170928, acc.: 95.31%] [G loss: 2.147953]\n",
      "epoch:37 step:34906 [D loss: 0.143652, acc.: 96.88%] [G loss: 3.934082]\n",
      "epoch:37 step:34907 [D loss: 0.127146, acc.: 97.66%] [G loss: 1.384737]\n",
      "epoch:37 step:34908 [D loss: 0.368115, acc.: 85.94%] [G loss: 4.724356]\n",
      "epoch:37 step:34909 [D loss: 1.078219, acc.: 48.44%] [G loss: 5.673865]\n",
      "epoch:37 step:34910 [D loss: 0.465954, acc.: 72.66%] [G loss: 2.551690]\n",
      "epoch:37 step:34911 [D loss: 0.034018, acc.: 99.22%] [G loss: 7.589045]\n",
      "epoch:37 step:34912 [D loss: 0.106158, acc.: 99.22%] [G loss: 2.928632]\n",
      "epoch:37 step:34913 [D loss: 0.016420, acc.: 100.00%] [G loss: 3.404932]\n",
      "epoch:37 step:34914 [D loss: 0.323999, acc.: 89.06%] [G loss: 3.933783]\n",
      "epoch:37 step:34915 [D loss: 0.453347, acc.: 74.22%] [G loss: 2.799023]\n",
      "epoch:37 step:34916 [D loss: 0.576212, acc.: 67.97%] [G loss: 2.278847]\n",
      "epoch:37 step:34917 [D loss: 0.036980, acc.: 100.00%] [G loss: 4.448122]\n",
      "epoch:37 step:34918 [D loss: 0.244949, acc.: 87.50%] [G loss: 1.733430]\n",
      "epoch:37 step:34919 [D loss: 0.562468, acc.: 71.88%] [G loss: 3.260078]\n",
      "epoch:37 step:34920 [D loss: 0.308727, acc.: 85.16%] [G loss: 1.878649]\n",
      "epoch:37 step:34921 [D loss: 0.015170, acc.: 100.00%] [G loss: 4.157822]\n",
      "epoch:37 step:34922 [D loss: 0.017714, acc.: 100.00%] [G loss: 1.112920]\n",
      "epoch:37 step:34923 [D loss: 1.068709, acc.: 36.72%] [G loss: 3.879276]\n",
      "epoch:37 step:34924 [D loss: 0.045098, acc.: 99.22%] [G loss: 3.966356]\n",
      "epoch:37 step:34925 [D loss: 0.117336, acc.: 98.44%] [G loss: 6.096896]\n",
      "epoch:37 step:34926 [D loss: 0.512474, acc.: 70.31%] [G loss: 1.505930]\n",
      "epoch:37 step:34927 [D loss: 1.163286, acc.: 34.38%] [G loss: 6.017220]\n",
      "epoch:37 step:34928 [D loss: 0.020787, acc.: 100.00%] [G loss: 6.143478]\n",
      "epoch:37 step:34929 [D loss: 0.110074, acc.: 96.88%] [G loss: 3.598695]\n",
      "epoch:37 step:34930 [D loss: 0.038788, acc.: 100.00%] [G loss: 4.565609]\n",
      "epoch:37 step:34931 [D loss: 0.071475, acc.: 98.44%] [G loss: 4.563545]\n",
      "epoch:37 step:34932 [D loss: 0.116008, acc.: 96.09%] [G loss: 3.590742]\n",
      "epoch:37 step:34933 [D loss: 0.428978, acc.: 74.22%] [G loss: 4.687658]\n",
      "epoch:37 step:34934 [D loss: 0.393830, acc.: 82.03%] [G loss: 6.852196]\n",
      "epoch:37 step:34935 [D loss: 0.041361, acc.: 100.00%] [G loss: 4.304873]\n",
      "epoch:37 step:34936 [D loss: 0.438341, acc.: 70.31%] [G loss: 4.830453]\n",
      "epoch:37 step:34937 [D loss: 0.584721, acc.: 65.62%] [G loss: 3.716836]\n",
      "epoch:37 step:34938 [D loss: 0.262233, acc.: 88.28%] [G loss: 4.275004]\n",
      "epoch:37 step:34939 [D loss: 0.033301, acc.: 99.22%] [G loss: 6.361419]\n",
      "epoch:37 step:34940 [D loss: 0.333506, acc.: 80.47%] [G loss: 4.327243]\n",
      "epoch:37 step:34941 [D loss: 0.102075, acc.: 97.66%] [G loss: 7.308927]\n",
      "epoch:37 step:34942 [D loss: 0.019253, acc.: 100.00%] [G loss: 7.398998]\n",
      "epoch:37 step:34943 [D loss: 0.087711, acc.: 99.22%] [G loss: 4.276055]\n",
      "epoch:37 step:34944 [D loss: 0.103841, acc.: 100.00%] [G loss: 1.483293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:34945 [D loss: 0.024179, acc.: 100.00%] [G loss: 3.181540]\n",
      "epoch:37 step:34946 [D loss: 0.550404, acc.: 66.41%] [G loss: 3.083097]\n",
      "epoch:37 step:34947 [D loss: 0.401396, acc.: 79.69%] [G loss: 4.755812]\n",
      "epoch:37 step:34948 [D loss: 0.036089, acc.: 100.00%] [G loss: 2.741083]\n",
      "epoch:37 step:34949 [D loss: 0.164545, acc.: 96.88%] [G loss: 5.986740]\n",
      "epoch:37 step:34950 [D loss: 0.014195, acc.: 100.00%] [G loss: 7.812653]\n",
      "epoch:37 step:34951 [D loss: 0.288943, acc.: 85.16%] [G loss: 1.693360]\n",
      "epoch:37 step:34952 [D loss: 0.031423, acc.: 99.22%] [G loss: 5.469607]\n",
      "epoch:37 step:34953 [D loss: 0.097722, acc.: 98.44%] [G loss: 2.844464]\n",
      "epoch:37 step:34954 [D loss: 0.001785, acc.: 100.00%] [G loss: 4.552381]\n",
      "epoch:37 step:34955 [D loss: 0.599790, acc.: 61.72%] [G loss: 3.510358]\n",
      "epoch:37 step:34956 [D loss: 0.104954, acc.: 98.44%] [G loss: 7.422695]\n",
      "epoch:37 step:34957 [D loss: 0.103216, acc.: 99.22%] [G loss: 2.375664]\n",
      "epoch:37 step:34958 [D loss: 0.175832, acc.: 95.31%] [G loss: 6.453645]\n",
      "epoch:37 step:34959 [D loss: 0.062134, acc.: 99.22%] [G loss: 5.813228]\n",
      "epoch:37 step:34960 [D loss: 0.310283, acc.: 82.03%] [G loss: 2.970857]\n",
      "epoch:37 step:34961 [D loss: 0.032809, acc.: 100.00%] [G loss: 1.038828]\n",
      "epoch:37 step:34962 [D loss: 0.168587, acc.: 98.44%] [G loss: 2.805669]\n",
      "epoch:37 step:34963 [D loss: 1.163240, acc.: 50.78%] [G loss: 3.447201]\n",
      "epoch:37 step:34964 [D loss: 0.171648, acc.: 96.88%] [G loss: 4.221218]\n",
      "epoch:37 step:34965 [D loss: 0.902687, acc.: 55.47%] [G loss: 5.015944]\n",
      "epoch:37 step:34966 [D loss: 0.111754, acc.: 96.88%] [G loss: 0.794730]\n",
      "epoch:37 step:34967 [D loss: 0.206131, acc.: 94.53%] [G loss: 1.004979]\n",
      "epoch:37 step:34968 [D loss: 0.166406, acc.: 97.66%] [G loss: 3.837351]\n",
      "epoch:37 step:34969 [D loss: 0.500613, acc.: 76.56%] [G loss: 3.691290]\n",
      "epoch:37 step:34970 [D loss: 0.153995, acc.: 98.44%] [G loss: 2.079296]\n",
      "epoch:37 step:34971 [D loss: 0.402136, acc.: 75.78%] [G loss: 3.593256]\n",
      "epoch:37 step:34972 [D loss: 0.477814, acc.: 78.12%] [G loss: 2.280547]\n",
      "epoch:37 step:34973 [D loss: 0.478863, acc.: 78.12%] [G loss: 3.562866]\n",
      "epoch:37 step:34974 [D loss: 0.091487, acc.: 98.44%] [G loss: 2.268206]\n",
      "epoch:37 step:34975 [D loss: 0.024313, acc.: 99.22%] [G loss: 5.623721]\n",
      "epoch:37 step:34976 [D loss: 0.151060, acc.: 97.66%] [G loss: 4.667068]\n",
      "epoch:37 step:34977 [D loss: 0.103552, acc.: 96.88%] [G loss: 6.401081]\n",
      "epoch:37 step:34978 [D loss: 0.196129, acc.: 97.66%] [G loss: 4.619620]\n",
      "epoch:37 step:34979 [D loss: 0.068463, acc.: 100.00%] [G loss: 4.217804]\n",
      "epoch:37 step:34980 [D loss: 0.038037, acc.: 99.22%] [G loss: 2.086040]\n",
      "epoch:37 step:34981 [D loss: 0.125572, acc.: 97.66%] [G loss: 5.548410]\n",
      "epoch:37 step:34982 [D loss: 0.224906, acc.: 92.97%] [G loss: 5.665234]\n",
      "epoch:37 step:34983 [D loss: 0.450949, acc.: 75.00%] [G loss: 2.864607]\n",
      "epoch:37 step:34984 [D loss: 0.119043, acc.: 96.88%] [G loss: 2.571888]\n",
      "epoch:37 step:34985 [D loss: 0.077489, acc.: 97.66%] [G loss: 2.565369]\n",
      "epoch:37 step:34986 [D loss: 0.068499, acc.: 98.44%] [G loss: 5.744522]\n",
      "epoch:37 step:34987 [D loss: 0.087156, acc.: 96.88%] [G loss: 6.641105]\n",
      "epoch:37 step:34988 [D loss: 0.070707, acc.: 100.00%] [G loss: 3.310677]\n",
      "epoch:37 step:34989 [D loss: 0.013527, acc.: 100.00%] [G loss: 2.013693]\n",
      "epoch:37 step:34990 [D loss: 0.164298, acc.: 97.66%] [G loss: 1.065882]\n",
      "epoch:37 step:34991 [D loss: 0.012427, acc.: 100.00%] [G loss: 4.843702]\n",
      "epoch:37 step:34992 [D loss: 0.011255, acc.: 100.00%] [G loss: 1.180829]\n",
      "epoch:37 step:34993 [D loss: 0.147224, acc.: 96.09%] [G loss: 2.373850]\n",
      "epoch:37 step:34994 [D loss: 0.399960, acc.: 78.12%] [G loss: 2.247704]\n",
      "epoch:37 step:34995 [D loss: 1.839850, acc.: 51.56%] [G loss: 2.342834]\n",
      "epoch:37 step:34996 [D loss: 0.022803, acc.: 100.00%] [G loss: 3.279794]\n",
      "epoch:37 step:34997 [D loss: 0.059857, acc.: 99.22%] [G loss: 5.516170]\n",
      "epoch:37 step:34998 [D loss: 0.954674, acc.: 55.47%] [G loss: 1.963684]\n",
      "epoch:37 step:34999 [D loss: 0.060617, acc.: 100.00%] [G loss: 1.580639]\n",
      "epoch:37 step:35000 [D loss: 0.227542, acc.: 92.97%] [G loss: 2.413591]\n",
      "epoch:37 step:35001 [D loss: 0.643359, acc.: 63.28%] [G loss: 3.352600]\n",
      "epoch:37 step:35002 [D loss: 0.282299, acc.: 89.06%] [G loss: 5.950853]\n",
      "epoch:37 step:35003 [D loss: 0.176140, acc.: 96.88%] [G loss: 4.320710]\n",
      "epoch:37 step:35004 [D loss: 0.362443, acc.: 78.91%] [G loss: 4.216535]\n",
      "epoch:37 step:35005 [D loss: 0.460539, acc.: 68.75%] [G loss: 1.439825]\n",
      "epoch:37 step:35006 [D loss: 0.014606, acc.: 100.00%] [G loss: 5.029313]\n",
      "epoch:37 step:35007 [D loss: 0.148907, acc.: 97.66%] [G loss: 2.867202]\n",
      "epoch:37 step:35008 [D loss: 0.708122, acc.: 55.47%] [G loss: 2.851400]\n",
      "epoch:37 step:35009 [D loss: 0.384867, acc.: 82.81%] [G loss: 3.335760]\n",
      "epoch:37 step:35010 [D loss: 0.079655, acc.: 100.00%] [G loss: 6.587450]\n",
      "epoch:37 step:35011 [D loss: 0.083320, acc.: 99.22%] [G loss: 5.160857]\n",
      "epoch:37 step:35012 [D loss: 0.043031, acc.: 100.00%] [G loss: 5.736427]\n",
      "epoch:37 step:35013 [D loss: 0.041822, acc.: 98.44%] [G loss: 1.475178]\n",
      "epoch:37 step:35014 [D loss: 1.091473, acc.: 57.03%] [G loss: 3.410323]\n",
      "epoch:37 step:35015 [D loss: 0.060433, acc.: 99.22%] [G loss: 2.609021]\n",
      "epoch:37 step:35016 [D loss: 1.404440, acc.: 50.78%] [G loss: 3.910392]\n",
      "epoch:37 step:35017 [D loss: 0.063721, acc.: 99.22%] [G loss: 3.919187]\n",
      "epoch:37 step:35018 [D loss: 1.053340, acc.: 42.97%] [G loss: 5.091109]\n",
      "epoch:37 step:35019 [D loss: 0.222026, acc.: 89.84%] [G loss: 6.394737]\n",
      "epoch:37 step:35020 [D loss: 0.342508, acc.: 82.03%] [G loss: 5.180234]\n",
      "epoch:37 step:35021 [D loss: 0.102670, acc.: 98.44%] [G loss: 3.277064]\n",
      "epoch:37 step:35022 [D loss: 0.060544, acc.: 99.22%] [G loss: 4.758793]\n",
      "epoch:37 step:35023 [D loss: 0.123376, acc.: 96.88%] [G loss: 2.095633]\n",
      "epoch:37 step:35024 [D loss: 0.224357, acc.: 93.75%] [G loss: 4.172575]\n",
      "epoch:37 step:35025 [D loss: 0.049057, acc.: 100.00%] [G loss: 5.429706]\n",
      "epoch:37 step:35026 [D loss: 0.739688, acc.: 57.81%] [G loss: 3.348409]\n",
      "epoch:37 step:35027 [D loss: 0.043955, acc.: 100.00%] [G loss: 6.722076]\n",
      "epoch:37 step:35028 [D loss: 0.160519, acc.: 98.44%] [G loss: 4.106947]\n",
      "epoch:37 step:35029 [D loss: 0.269543, acc.: 93.75%] [G loss: 4.194687]\n",
      "epoch:37 step:35030 [D loss: 0.152127, acc.: 96.88%] [G loss: 3.984087]\n",
      "epoch:37 step:35031 [D loss: 0.091291, acc.: 98.44%] [G loss: 2.973385]\n",
      "epoch:37 step:35032 [D loss: 0.231512, acc.: 94.53%] [G loss: 4.301450]\n",
      "epoch:37 step:35033 [D loss: 0.299125, acc.: 89.84%] [G loss: 5.405303]\n",
      "epoch:37 step:35034 [D loss: 0.193636, acc.: 96.09%] [G loss: 4.981385]\n",
      "epoch:37 step:35035 [D loss: 0.083853, acc.: 98.44%] [G loss: 2.546894]\n",
      "epoch:37 step:35036 [D loss: 0.008031, acc.: 100.00%] [G loss: 3.196009]\n",
      "epoch:37 step:35037 [D loss: 0.076587, acc.: 98.44%] [G loss: 2.147909]\n",
      "epoch:37 step:35038 [D loss: 0.173780, acc.: 94.53%] [G loss: 5.288634]\n",
      "epoch:37 step:35039 [D loss: 0.057980, acc.: 100.00%] [G loss: 4.426163]\n",
      "epoch:37 step:35040 [D loss: 0.015893, acc.: 100.00%] [G loss: 1.219361]\n",
      "epoch:37 step:35041 [D loss: 0.288651, acc.: 89.06%] [G loss: 5.103349]\n",
      "epoch:37 step:35042 [D loss: 0.372083, acc.: 82.03%] [G loss: 2.006633]\n",
      "epoch:37 step:35043 [D loss: 0.190131, acc.: 93.75%] [G loss: 4.643705]\n",
      "epoch:37 step:35044 [D loss: 0.190567, acc.: 96.09%] [G loss: 3.768749]\n",
      "epoch:37 step:35045 [D loss: 0.069195, acc.: 98.44%] [G loss: 0.998010]\n",
      "epoch:37 step:35046 [D loss: 0.505793, acc.: 72.66%] [G loss: 1.281677]\n",
      "epoch:37 step:35047 [D loss: 0.080177, acc.: 100.00%] [G loss: 3.804459]\n",
      "epoch:37 step:35048 [D loss: 0.263486, acc.: 90.62%] [G loss: 2.196958]\n",
      "epoch:37 step:35049 [D loss: 0.065831, acc.: 100.00%] [G loss: 5.462644]\n",
      "epoch:37 step:35050 [D loss: 0.055017, acc.: 99.22%] [G loss: 1.937617]\n",
      "epoch:37 step:35051 [D loss: 0.018477, acc.: 100.00%] [G loss: 3.995090]\n",
      "epoch:37 step:35052 [D loss: 0.195648, acc.: 96.09%] [G loss: 0.749647]\n",
      "epoch:37 step:35053 [D loss: 0.345449, acc.: 88.28%] [G loss: 2.331426]\n",
      "epoch:37 step:35054 [D loss: 1.101308, acc.: 38.28%] [G loss: 3.330221]\n",
      "epoch:37 step:35055 [D loss: 0.228723, acc.: 90.62%] [G loss: 5.910436]\n",
      "epoch:37 step:35056 [D loss: 0.022630, acc.: 99.22%] [G loss: 7.333127]\n",
      "epoch:37 step:35057 [D loss: 0.474020, acc.: 79.69%] [G loss: 1.525182]\n",
      "epoch:37 step:35058 [D loss: 0.025373, acc.: 100.00%] [G loss: 1.826046]\n",
      "epoch:37 step:35059 [D loss: 1.134158, acc.: 32.81%] [G loss: 3.551251]\n",
      "epoch:37 step:35060 [D loss: 0.163396, acc.: 93.75%] [G loss: 3.009394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:35061 [D loss: 0.215263, acc.: 96.88%] [G loss: 3.135190]\n",
      "epoch:37 step:35062 [D loss: 0.101606, acc.: 99.22%] [G loss: 3.819787]\n",
      "epoch:37 step:35063 [D loss: 0.082048, acc.: 99.22%] [G loss: 2.728381]\n",
      "epoch:37 step:35064 [D loss: 0.008509, acc.: 100.00%] [G loss: 3.266918]\n",
      "epoch:37 step:35065 [D loss: 0.024934, acc.: 100.00%] [G loss: 3.453976]\n",
      "epoch:37 step:35066 [D loss: 1.605443, acc.: 14.84%] [G loss: 4.205546]\n",
      "epoch:37 step:35067 [D loss: 0.760166, acc.: 57.81%] [G loss: 2.635867]\n",
      "epoch:37 step:35068 [D loss: 0.055853, acc.: 100.00%] [G loss: 6.117586]\n",
      "epoch:37 step:35069 [D loss: 0.118543, acc.: 98.44%] [G loss: 2.040391]\n",
      "epoch:37 step:35070 [D loss: 0.176820, acc.: 96.88%] [G loss: 2.425508]\n",
      "epoch:37 step:35071 [D loss: 0.022745, acc.: 100.00%] [G loss: 2.864424]\n",
      "epoch:37 step:35072 [D loss: 0.030764, acc.: 99.22%] [G loss: 3.886993]\n",
      "epoch:37 step:35073 [D loss: 0.075922, acc.: 97.66%] [G loss: 2.975408]\n",
      "epoch:37 step:35074 [D loss: 0.130850, acc.: 96.88%] [G loss: 2.444568]\n",
      "epoch:37 step:35075 [D loss: 0.022896, acc.: 100.00%] [G loss: 5.753849]\n",
      "epoch:37 step:35076 [D loss: 0.130945, acc.: 97.66%] [G loss: 4.187988]\n",
      "epoch:37 step:35077 [D loss: 0.126617, acc.: 99.22%] [G loss: 3.998699]\n",
      "epoch:37 step:35078 [D loss: 0.079072, acc.: 96.88%] [G loss: 4.656366]\n",
      "epoch:37 step:35079 [D loss: 0.049254, acc.: 98.44%] [G loss: 3.212749]\n",
      "epoch:37 step:35080 [D loss: 0.508529, acc.: 64.06%] [G loss: 4.341806]\n",
      "epoch:37 step:35081 [D loss: 0.137841, acc.: 97.66%] [G loss: 6.384466]\n",
      "epoch:37 step:35082 [D loss: 0.181722, acc.: 92.19%] [G loss: 1.911257]\n",
      "epoch:37 step:35083 [D loss: 0.257524, acc.: 89.06%] [G loss: 3.633527]\n",
      "epoch:37 step:35084 [D loss: 0.289979, acc.: 87.50%] [G loss: 5.182059]\n",
      "epoch:37 step:35085 [D loss: 0.047216, acc.: 100.00%] [G loss: 6.653042]\n",
      "epoch:37 step:35086 [D loss: 0.131581, acc.: 97.66%] [G loss: 4.282026]\n",
      "epoch:37 step:35087 [D loss: 0.086780, acc.: 97.66%] [G loss: 4.885873]\n",
      "epoch:37 step:35088 [D loss: 0.490516, acc.: 79.69%] [G loss: 1.812306]\n",
      "epoch:37 step:35089 [D loss: 0.392266, acc.: 85.16%] [G loss: 1.704349]\n",
      "epoch:37 step:35090 [D loss: 1.177635, acc.: 51.56%] [G loss: 4.238278]\n",
      "epoch:37 step:35091 [D loss: 0.022955, acc.: 100.00%] [G loss: 3.143161]\n",
      "epoch:37 step:35092 [D loss: 0.240448, acc.: 95.31%] [G loss: 2.866254]\n",
      "epoch:37 step:35093 [D loss: 0.146859, acc.: 94.53%] [G loss: 1.851196]\n",
      "epoch:37 step:35094 [D loss: 0.025617, acc.: 100.00%] [G loss: 2.069808]\n",
      "epoch:37 step:35095 [D loss: 0.131317, acc.: 98.44%] [G loss: 3.637109]\n",
      "epoch:37 step:35096 [D loss: 0.077333, acc.: 98.44%] [G loss: 5.282548]\n",
      "epoch:37 step:35097 [D loss: 0.112495, acc.: 98.44%] [G loss: 4.679652]\n",
      "epoch:37 step:35098 [D loss: 0.419356, acc.: 75.78%] [G loss: 4.910795]\n",
      "epoch:37 step:35099 [D loss: 0.057780, acc.: 99.22%] [G loss: 4.766941]\n",
      "epoch:37 step:35100 [D loss: 2.395714, acc.: 50.00%] [G loss: 0.474139]\n",
      "epoch:37 step:35101 [D loss: 0.513798, acc.: 64.84%] [G loss: 0.272143]\n",
      "epoch:37 step:35102 [D loss: 0.148985, acc.: 96.88%] [G loss: 5.741356]\n",
      "epoch:37 step:35103 [D loss: 0.271483, acc.: 89.06%] [G loss: 3.309691]\n",
      "epoch:37 step:35104 [D loss: 0.120874, acc.: 98.44%] [G loss: 5.804596]\n",
      "epoch:37 step:35105 [D loss: 0.076803, acc.: 100.00%] [G loss: 2.718067]\n",
      "epoch:37 step:35106 [D loss: 0.125637, acc.: 99.22%] [G loss: 3.951288]\n",
      "epoch:37 step:35107 [D loss: 0.183660, acc.: 97.66%] [G loss: 3.749388]\n",
      "epoch:37 step:35108 [D loss: 0.246378, acc.: 91.41%] [G loss: 2.567281]\n",
      "epoch:37 step:35109 [D loss: 0.752338, acc.: 60.16%] [G loss: 1.925742]\n",
      "epoch:37 step:35110 [D loss: 0.204041, acc.: 92.97%] [G loss: 3.405030]\n",
      "epoch:37 step:35111 [D loss: 0.106031, acc.: 99.22%] [G loss: 4.869585]\n",
      "epoch:37 step:35112 [D loss: 0.169851, acc.: 99.22%] [G loss: 5.771689]\n",
      "epoch:37 step:35113 [D loss: 0.202736, acc.: 96.09%] [G loss: 2.444350]\n",
      "epoch:37 step:35114 [D loss: 0.064391, acc.: 100.00%] [G loss: 1.355773]\n",
      "epoch:37 step:35115 [D loss: 0.045904, acc.: 100.00%] [G loss: 1.920951]\n",
      "epoch:37 step:35116 [D loss: 0.104545, acc.: 98.44%] [G loss: 4.039509]\n",
      "epoch:37 step:35117 [D loss: 0.116340, acc.: 98.44%] [G loss: 2.285825]\n",
      "epoch:37 step:35118 [D loss: 0.085740, acc.: 98.44%] [G loss: 5.090490]\n",
      "epoch:37 step:35119 [D loss: 0.069083, acc.: 99.22%] [G loss: 2.654199]\n",
      "epoch:37 step:35120 [D loss: 0.148296, acc.: 96.88%] [G loss: 2.045884]\n",
      "epoch:37 step:35121 [D loss: 0.050317, acc.: 99.22%] [G loss: 3.887986]\n",
      "epoch:37 step:35122 [D loss: 0.026766, acc.: 100.00%] [G loss: 1.946271]\n",
      "epoch:37 step:35123 [D loss: 0.049267, acc.: 99.22%] [G loss: 1.135037]\n",
      "epoch:37 step:35124 [D loss: 0.106601, acc.: 100.00%] [G loss: 3.923541]\n",
      "epoch:37 step:35125 [D loss: 0.391026, acc.: 76.56%] [G loss: 4.629786]\n",
      "epoch:37 step:35126 [D loss: 0.107299, acc.: 97.66%] [G loss: 3.885804]\n",
      "epoch:37 step:35127 [D loss: 0.001889, acc.: 100.00%] [G loss: 9.345955]\n",
      "epoch:37 step:35128 [D loss: 0.029332, acc.: 99.22%] [G loss: 7.946115]\n",
      "epoch:37 step:35129 [D loss: 0.068632, acc.: 98.44%] [G loss: 4.130174]\n",
      "epoch:37 step:35130 [D loss: 0.081233, acc.: 96.09%] [G loss: 2.198452]\n",
      "epoch:37 step:35131 [D loss: 0.045217, acc.: 99.22%] [G loss: 6.806596]\n",
      "epoch:37 step:35132 [D loss: 0.018981, acc.: 100.00%] [G loss: 3.444706]\n",
      "epoch:37 step:35133 [D loss: 0.063153, acc.: 100.00%] [G loss: 7.775029]\n",
      "epoch:37 step:35134 [D loss: 0.134830, acc.: 96.88%] [G loss: 3.515304]\n",
      "epoch:37 step:35135 [D loss: 0.631123, acc.: 58.59%] [G loss: 5.455147]\n",
      "epoch:37 step:35136 [D loss: 0.013060, acc.: 100.00%] [G loss: 8.729801]\n",
      "epoch:37 step:35137 [D loss: 0.589578, acc.: 66.41%] [G loss: 4.221235]\n",
      "epoch:37 step:35138 [D loss: 0.387134, acc.: 84.38%] [G loss: 5.862636]\n",
      "epoch:37 step:35139 [D loss: 0.033381, acc.: 100.00%] [G loss: 3.862070]\n",
      "epoch:37 step:35140 [D loss: 0.174877, acc.: 94.53%] [G loss: 5.458341]\n",
      "epoch:37 step:35141 [D loss: 0.690187, acc.: 52.34%] [G loss: 1.458546]\n",
      "epoch:37 step:35142 [D loss: 0.008945, acc.: 100.00%] [G loss: 7.128119]\n",
      "epoch:37 step:35143 [D loss: 0.002842, acc.: 100.00%] [G loss: 6.288129]\n",
      "epoch:37 step:35144 [D loss: 0.021091, acc.: 100.00%] [G loss: 4.101424]\n",
      "epoch:37 step:35145 [D loss: 0.077194, acc.: 98.44%] [G loss: 6.045393]\n",
      "epoch:37 step:35146 [D loss: 0.015160, acc.: 100.00%] [G loss: 3.310425]\n",
      "epoch:37 step:35147 [D loss: 0.035754, acc.: 100.00%] [G loss: 4.646478]\n",
      "epoch:37 step:35148 [D loss: 0.037823, acc.: 98.44%] [G loss: 2.027799]\n",
      "epoch:37 step:35149 [D loss: 1.716165, acc.: 52.34%] [G loss: 6.815486]\n",
      "epoch:37 step:35150 [D loss: 0.234615, acc.: 92.19%] [G loss: 5.458474]\n",
      "epoch:37 step:35151 [D loss: 0.636987, acc.: 64.06%] [G loss: 5.619319]\n",
      "epoch:37 step:35152 [D loss: 1.170526, acc.: 61.72%] [G loss: 5.508139]\n",
      "epoch:37 step:35153 [D loss: 0.061983, acc.: 97.66%] [G loss: 5.352328]\n",
      "epoch:37 step:35154 [D loss: 0.061144, acc.: 99.22%] [G loss: 7.997182]\n",
      "epoch:37 step:35155 [D loss: 0.290424, acc.: 84.38%] [G loss: 0.739741]\n",
      "epoch:37 step:35156 [D loss: 0.187847, acc.: 94.53%] [G loss: 2.259982]\n",
      "epoch:37 step:35157 [D loss: 0.116947, acc.: 98.44%] [G loss: 1.674293]\n",
      "epoch:37 step:35158 [D loss: 0.036942, acc.: 100.00%] [G loss: 5.472035]\n",
      "epoch:37 step:35159 [D loss: 0.130794, acc.: 95.31%] [G loss: 5.185835]\n",
      "epoch:37 step:35160 [D loss: 1.559266, acc.: 31.25%] [G loss: 2.505708]\n",
      "epoch:37 step:35161 [D loss: 0.932041, acc.: 54.69%] [G loss: 3.102541]\n",
      "epoch:37 step:35162 [D loss: 0.013834, acc.: 100.00%] [G loss: 3.872846]\n",
      "epoch:37 step:35163 [D loss: 0.028295, acc.: 100.00%] [G loss: 2.165950]\n",
      "epoch:37 step:35164 [D loss: 0.439733, acc.: 82.03%] [G loss: 3.442315]\n",
      "epoch:37 step:35165 [D loss: 0.044329, acc.: 99.22%] [G loss: 0.686342]\n",
      "epoch:37 step:35166 [D loss: 0.057766, acc.: 100.00%] [G loss: 2.091716]\n",
      "epoch:37 step:35167 [D loss: 0.656587, acc.: 63.28%] [G loss: 5.757416]\n",
      "epoch:37 step:35168 [D loss: 0.233616, acc.: 90.62%] [G loss: 4.797461]\n",
      "epoch:37 step:35169 [D loss: 0.085983, acc.: 99.22%] [G loss: 7.489411]\n",
      "epoch:37 step:35170 [D loss: 0.879067, acc.: 56.25%] [G loss: 4.629190]\n",
      "epoch:37 step:35171 [D loss: 0.140791, acc.: 95.31%] [G loss: 1.254365]\n",
      "epoch:37 step:35172 [D loss: 0.137382, acc.: 99.22%] [G loss: 4.537624]\n",
      "epoch:37 step:35173 [D loss: 0.410178, acc.: 78.12%] [G loss: 4.111804]\n",
      "epoch:37 step:35174 [D loss: 0.134756, acc.: 98.44%] [G loss: 1.982685]\n",
      "epoch:37 step:35175 [D loss: 0.057711, acc.: 99.22%] [G loss: 3.061553]\n",
      "epoch:37 step:35176 [D loss: 0.297356, acc.: 92.19%] [G loss: 3.036963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:35177 [D loss: 0.259535, acc.: 89.84%] [G loss: 3.128846]\n",
      "epoch:37 step:35178 [D loss: 0.047206, acc.: 99.22%] [G loss: 6.961318]\n",
      "epoch:37 step:35179 [D loss: 0.036558, acc.: 100.00%] [G loss: 3.813543]\n",
      "epoch:37 step:35180 [D loss: 0.023801, acc.: 100.00%] [G loss: 2.809088]\n",
      "epoch:37 step:35181 [D loss: 0.287437, acc.: 86.72%] [G loss: 2.929208]\n",
      "epoch:37 step:35182 [D loss: 0.081644, acc.: 99.22%] [G loss: 4.103754]\n",
      "epoch:37 step:35183 [D loss: 0.252628, acc.: 89.06%] [G loss: 3.742861]\n",
      "epoch:37 step:35184 [D loss: 0.310593, acc.: 91.41%] [G loss: 6.106613]\n",
      "epoch:37 step:35185 [D loss: 0.600268, acc.: 63.28%] [G loss: 7.306295]\n",
      "epoch:37 step:35186 [D loss: 0.056477, acc.: 98.44%] [G loss: 4.029384]\n",
      "epoch:37 step:35187 [D loss: 0.809638, acc.: 59.38%] [G loss: 5.535199]\n",
      "epoch:37 step:35188 [D loss: 0.663237, acc.: 60.94%] [G loss: 2.706263]\n",
      "epoch:37 step:35189 [D loss: 0.109447, acc.: 98.44%] [G loss: 2.324734]\n",
      "epoch:37 step:35190 [D loss: 0.447432, acc.: 75.00%] [G loss: 6.046170]\n",
      "epoch:37 step:35191 [D loss: 0.073672, acc.: 98.44%] [G loss: 2.961045]\n",
      "epoch:37 step:35192 [D loss: 0.395626, acc.: 74.22%] [G loss: 4.941989]\n",
      "epoch:37 step:35193 [D loss: 0.022172, acc.: 100.00%] [G loss: 7.280596]\n",
      "epoch:37 step:35194 [D loss: 0.935061, acc.: 57.03%] [G loss: 4.694522]\n",
      "epoch:37 step:35195 [D loss: 0.018017, acc.: 100.00%] [G loss: 3.232321]\n",
      "epoch:37 step:35196 [D loss: 0.189111, acc.: 96.09%] [G loss: 2.516658]\n",
      "epoch:37 step:35197 [D loss: 0.338785, acc.: 90.62%] [G loss: 1.307467]\n",
      "epoch:37 step:35198 [D loss: 0.274781, acc.: 86.72%] [G loss: 3.365007]\n",
      "epoch:37 step:35199 [D loss: 0.039708, acc.: 100.00%] [G loss: 2.925745]\n",
      "epoch:37 step:35200 [D loss: 0.106854, acc.: 100.00%] [G loss: 1.523065]\n",
      "epoch:37 step:35201 [D loss: 0.052201, acc.: 100.00%] [G loss: 5.928469]\n",
      "epoch:37 step:35202 [D loss: 0.033857, acc.: 98.44%] [G loss: 2.833921]\n",
      "epoch:37 step:35203 [D loss: 0.076406, acc.: 100.00%] [G loss: 3.988273]\n",
      "epoch:37 step:35204 [D loss: 0.014756, acc.: 100.00%] [G loss: 3.095883]\n",
      "epoch:37 step:35205 [D loss: 0.187175, acc.: 92.97%] [G loss: 2.506210]\n",
      "epoch:37 step:35206 [D loss: 0.025589, acc.: 100.00%] [G loss: 1.685078]\n",
      "epoch:37 step:35207 [D loss: 0.544441, acc.: 71.88%] [G loss: 3.843438]\n",
      "epoch:37 step:35208 [D loss: 0.036335, acc.: 99.22%] [G loss: 3.486701]\n",
      "epoch:37 step:35209 [D loss: 0.395505, acc.: 85.94%] [G loss: 5.378243]\n",
      "epoch:37 step:35210 [D loss: 0.168006, acc.: 95.31%] [G loss: 5.661811]\n",
      "epoch:37 step:35211 [D loss: 0.736477, acc.: 57.03%] [G loss: 4.252138]\n",
      "epoch:37 step:35212 [D loss: 0.190711, acc.: 95.31%] [G loss: 4.916559]\n",
      "epoch:37 step:35213 [D loss: 0.118574, acc.: 99.22%] [G loss: 7.935575]\n",
      "epoch:37 step:35214 [D loss: 0.251873, acc.: 89.06%] [G loss: 4.261426]\n",
      "epoch:37 step:35215 [D loss: 0.235014, acc.: 94.53%] [G loss: 3.993271]\n",
      "epoch:37 step:35216 [D loss: 0.222233, acc.: 92.19%] [G loss: 2.426837]\n",
      "epoch:37 step:35217 [D loss: 0.449628, acc.: 71.88%] [G loss: 2.260058]\n",
      "epoch:37 step:35218 [D loss: 0.062841, acc.: 100.00%] [G loss: 4.226307]\n",
      "epoch:37 step:35219 [D loss: 0.231144, acc.: 95.31%] [G loss: 1.481197]\n",
      "epoch:37 step:35220 [D loss: 0.915003, acc.: 56.25%] [G loss: 1.170894]\n",
      "epoch:37 step:35221 [D loss: 0.892887, acc.: 55.47%] [G loss: 2.957807]\n",
      "epoch:37 step:35222 [D loss: 0.037281, acc.: 100.00%] [G loss: 1.624326]\n",
      "epoch:37 step:35223 [D loss: 0.045158, acc.: 100.00%] [G loss: 3.393211]\n",
      "epoch:37 step:35224 [D loss: 0.086489, acc.: 100.00%] [G loss: 2.544790]\n",
      "epoch:37 step:35225 [D loss: 0.082052, acc.: 99.22%] [G loss: 4.519818]\n",
      "epoch:37 step:35226 [D loss: 0.210225, acc.: 94.53%] [G loss: 3.467213]\n",
      "epoch:37 step:35227 [D loss: 0.055888, acc.: 99.22%] [G loss: 3.998256]\n",
      "epoch:37 step:35228 [D loss: 0.262933, acc.: 90.62%] [G loss: 4.056052]\n",
      "epoch:37 step:35229 [D loss: 0.284336, acc.: 84.38%] [G loss: 4.924842]\n",
      "epoch:37 step:35230 [D loss: 0.074438, acc.: 99.22%] [G loss: 0.552780]\n",
      "epoch:37 step:35231 [D loss: 0.125412, acc.: 97.66%] [G loss: 2.729405]\n",
      "epoch:37 step:35232 [D loss: 0.024972, acc.: 99.22%] [G loss: 4.596963]\n",
      "epoch:37 step:35233 [D loss: 0.807404, acc.: 58.59%] [G loss: 5.219818]\n",
      "epoch:37 step:35234 [D loss: 0.075378, acc.: 98.44%] [G loss: 5.256509]\n",
      "epoch:37 step:35235 [D loss: 0.037616, acc.: 100.00%] [G loss: 5.608439]\n",
      "epoch:37 step:35236 [D loss: 0.082602, acc.: 97.66%] [G loss: 6.917882]\n",
      "epoch:37 step:35237 [D loss: 0.320127, acc.: 81.25%] [G loss: 2.143751]\n",
      "epoch:37 step:35238 [D loss: 0.011694, acc.: 100.00%] [G loss: 5.169143]\n",
      "epoch:37 step:35239 [D loss: 0.417285, acc.: 83.59%] [G loss: 2.105158]\n",
      "epoch:37 step:35240 [D loss: 0.048745, acc.: 100.00%] [G loss: 1.627796]\n",
      "epoch:37 step:35241 [D loss: 0.051523, acc.: 99.22%] [G loss: 2.279810]\n",
      "epoch:37 step:35242 [D loss: 0.040495, acc.: 100.00%] [G loss: 3.157029]\n",
      "epoch:37 step:35243 [D loss: 0.172733, acc.: 97.66%] [G loss: 2.336725]\n",
      "epoch:37 step:35244 [D loss: 0.072903, acc.: 100.00%] [G loss: 4.557275]\n",
      "epoch:37 step:35245 [D loss: 0.114726, acc.: 96.88%] [G loss: 4.552320]\n",
      "epoch:37 step:35246 [D loss: 0.496988, acc.: 77.34%] [G loss: 3.404275]\n",
      "epoch:37 step:35247 [D loss: 0.047441, acc.: 100.00%] [G loss: 1.678546]\n",
      "epoch:37 step:35248 [D loss: 0.142053, acc.: 98.44%] [G loss: 3.257477]\n",
      "epoch:37 step:35249 [D loss: 1.160388, acc.: 53.12%] [G loss: 6.464658]\n",
      "epoch:37 step:35250 [D loss: 0.011639, acc.: 100.00%] [G loss: 5.379209]\n",
      "epoch:37 step:35251 [D loss: 0.105385, acc.: 98.44%] [G loss: 6.524721]\n",
      "epoch:37 step:35252 [D loss: 0.173376, acc.: 95.31%] [G loss: 2.043781]\n",
      "epoch:37 step:35253 [D loss: 2.037140, acc.: 49.22%] [G loss: 4.370283]\n",
      "epoch:37 step:35254 [D loss: 0.245540, acc.: 88.28%] [G loss: 2.278636]\n",
      "epoch:37 step:35255 [D loss: 0.078864, acc.: 100.00%] [G loss: 6.550374]\n",
      "epoch:37 step:35256 [D loss: 0.012560, acc.: 100.00%] [G loss: 4.852828]\n",
      "epoch:37 step:35257 [D loss: 0.029792, acc.: 100.00%] [G loss: 2.121468]\n",
      "epoch:37 step:35258 [D loss: 0.931594, acc.: 55.47%] [G loss: 6.396332]\n",
      "epoch:37 step:35259 [D loss: 0.838406, acc.: 62.50%] [G loss: 5.986424]\n",
      "epoch:37 step:35260 [D loss: 0.224432, acc.: 89.06%] [G loss: 4.176154]\n",
      "epoch:37 step:35261 [D loss: 0.010048, acc.: 100.00%] [G loss: 6.068607]\n",
      "epoch:37 step:35262 [D loss: 0.048872, acc.: 99.22%] [G loss: 3.666154]\n",
      "epoch:37 step:35263 [D loss: 0.082649, acc.: 99.22%] [G loss: 3.641047]\n",
      "epoch:37 step:35264 [D loss: 0.495432, acc.: 78.12%] [G loss: 2.561594]\n",
      "epoch:37 step:35265 [D loss: 0.132162, acc.: 98.44%] [G loss: 4.655816]\n",
      "epoch:37 step:35266 [D loss: 0.035007, acc.: 100.00%] [G loss: 0.962887]\n",
      "epoch:37 step:35267 [D loss: 0.064734, acc.: 99.22%] [G loss: 1.855730]\n",
      "epoch:37 step:35268 [D loss: 0.145305, acc.: 94.53%] [G loss: 0.867533]\n",
      "epoch:37 step:35269 [D loss: 0.039204, acc.: 99.22%] [G loss: 2.873242]\n",
      "epoch:37 step:35270 [D loss: 0.511623, acc.: 73.44%] [G loss: 4.811775]\n",
      "epoch:37 step:35271 [D loss: 0.037133, acc.: 100.00%] [G loss: 1.230236]\n",
      "epoch:37 step:35272 [D loss: 0.028383, acc.: 99.22%] [G loss: 3.433393]\n",
      "epoch:37 step:35273 [D loss: 0.271176, acc.: 88.28%] [G loss: 0.246049]\n",
      "epoch:37 step:35274 [D loss: 0.160731, acc.: 96.88%] [G loss: 0.651862]\n",
      "epoch:37 step:35275 [D loss: 0.343776, acc.: 81.25%] [G loss: 3.089478]\n",
      "epoch:37 step:35276 [D loss: 0.269099, acc.: 90.62%] [G loss: 4.044398]\n",
      "epoch:37 step:35277 [D loss: 0.213413, acc.: 93.75%] [G loss: 0.555047]\n",
      "epoch:37 step:35278 [D loss: 0.172158, acc.: 92.19%] [G loss: 3.341786]\n",
      "epoch:37 step:35279 [D loss: 1.014605, acc.: 54.69%] [G loss: 5.233227]\n",
      "epoch:37 step:35280 [D loss: 0.376074, acc.: 78.12%] [G loss: 2.059460]\n",
      "epoch:37 step:35281 [D loss: 0.657332, acc.: 64.06%] [G loss: 0.291979]\n",
      "epoch:37 step:35282 [D loss: 0.098085, acc.: 98.44%] [G loss: 3.318645]\n",
      "epoch:37 step:35283 [D loss: 0.215896, acc.: 92.19%] [G loss: 1.952594]\n",
      "epoch:37 step:35284 [D loss: 0.269044, acc.: 84.38%] [G loss: 4.271091]\n",
      "epoch:37 step:35285 [D loss: 0.053570, acc.: 97.66%] [G loss: 4.576160]\n",
      "epoch:37 step:35286 [D loss: 0.257557, acc.: 92.97%] [G loss: 3.826558]\n",
      "epoch:37 step:35287 [D loss: 0.246703, acc.: 93.75%] [G loss: 3.420398]\n",
      "epoch:37 step:35288 [D loss: 0.213923, acc.: 90.62%] [G loss: 1.975312]\n",
      "epoch:37 step:35289 [D loss: 0.062968, acc.: 98.44%] [G loss: 6.849295]\n",
      "epoch:37 step:35290 [D loss: 0.051581, acc.: 100.00%] [G loss: 2.456036]\n",
      "epoch:37 step:35291 [D loss: 0.011907, acc.: 100.00%] [G loss: 4.033312]\n",
      "epoch:37 step:35292 [D loss: 0.029640, acc.: 99.22%] [G loss: 3.095862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:35293 [D loss: 0.023989, acc.: 99.22%] [G loss: 1.721071]\n",
      "epoch:37 step:35294 [D loss: 0.117694, acc.: 97.66%] [G loss: 2.794301]\n",
      "epoch:37 step:35295 [D loss: 0.079461, acc.: 99.22%] [G loss: 3.275146]\n",
      "epoch:37 step:35296 [D loss: 0.016496, acc.: 100.00%] [G loss: 7.177217]\n",
      "epoch:37 step:35297 [D loss: 0.038889, acc.: 100.00%] [G loss: 4.121074]\n",
      "epoch:37 step:35298 [D loss: 0.036952, acc.: 100.00%] [G loss: 2.540658]\n",
      "epoch:37 step:35299 [D loss: 0.308591, acc.: 85.16%] [G loss: 2.684683]\n",
      "epoch:37 step:35300 [D loss: 0.143420, acc.: 98.44%] [G loss: 4.422467]\n",
      "epoch:37 step:35301 [D loss: 0.046609, acc.: 99.22%] [G loss: 6.475960]\n",
      "epoch:37 step:35302 [D loss: 0.230019, acc.: 89.06%] [G loss: 4.251784]\n",
      "epoch:37 step:35303 [D loss: 0.058713, acc.: 98.44%] [G loss: 1.696392]\n",
      "epoch:37 step:35304 [D loss: 0.029926, acc.: 100.00%] [G loss: 3.696640]\n",
      "epoch:37 step:35305 [D loss: 0.725833, acc.: 57.03%] [G loss: 3.316684]\n",
      "epoch:37 step:35306 [D loss: 0.130731, acc.: 96.09%] [G loss: 7.406935]\n",
      "epoch:37 step:35307 [D loss: 0.108619, acc.: 96.09%] [G loss: 3.794430]\n",
      "epoch:37 step:35308 [D loss: 0.073720, acc.: 98.44%] [G loss: 6.068538]\n",
      "epoch:37 step:35309 [D loss: 0.057109, acc.: 99.22%] [G loss: 5.516576]\n",
      "epoch:37 step:35310 [D loss: 0.201174, acc.: 89.06%] [G loss: 4.530332]\n",
      "epoch:37 step:35311 [D loss: 0.321336, acc.: 82.03%] [G loss: 2.399834]\n",
      "epoch:37 step:35312 [D loss: 0.223565, acc.: 92.19%] [G loss: 5.402012]\n",
      "epoch:37 step:35313 [D loss: 0.087234, acc.: 98.44%] [G loss: 2.709026]\n",
      "epoch:37 step:35314 [D loss: 0.237136, acc.: 96.09%] [G loss: 2.443059]\n",
      "epoch:37 step:35315 [D loss: 0.122659, acc.: 95.31%] [G loss: 4.356237]\n",
      "epoch:37 step:35316 [D loss: 0.156436, acc.: 92.97%] [G loss: 3.727365]\n",
      "epoch:37 step:35317 [D loss: 0.106892, acc.: 98.44%] [G loss: 3.323786]\n",
      "epoch:37 step:35318 [D loss: 0.033519, acc.: 100.00%] [G loss: 5.946375]\n",
      "epoch:37 step:35319 [D loss: 0.461294, acc.: 75.00%] [G loss: 3.038940]\n",
      "epoch:37 step:35320 [D loss: 1.226773, acc.: 54.69%] [G loss: 4.693849]\n",
      "epoch:37 step:35321 [D loss: 0.048543, acc.: 100.00%] [G loss: 4.524734]\n",
      "epoch:37 step:35322 [D loss: 0.127051, acc.: 96.09%] [G loss: 5.981655]\n",
      "epoch:37 step:35323 [D loss: 0.074890, acc.: 99.22%] [G loss: 6.623804]\n",
      "epoch:37 step:35324 [D loss: 0.651267, acc.: 63.28%] [G loss: 6.005575]\n",
      "epoch:37 step:35325 [D loss: 0.610562, acc.: 70.31%] [G loss: 1.565346]\n",
      "epoch:37 step:35326 [D loss: 0.147109, acc.: 96.88%] [G loss: 3.153967]\n",
      "epoch:37 step:35327 [D loss: 0.403654, acc.: 75.00%] [G loss: 5.626583]\n",
      "epoch:37 step:35328 [D loss: 0.047111, acc.: 100.00%] [G loss: 3.495558]\n",
      "epoch:37 step:35329 [D loss: 0.089273, acc.: 98.44%] [G loss: 2.790241]\n",
      "epoch:37 step:35330 [D loss: 0.072731, acc.: 96.88%] [G loss: 2.067019]\n",
      "epoch:37 step:35331 [D loss: 0.150854, acc.: 95.31%] [G loss: 0.868492]\n",
      "epoch:37 step:35332 [D loss: 0.097321, acc.: 97.66%] [G loss: 1.141388]\n",
      "epoch:37 step:35333 [D loss: 0.039828, acc.: 100.00%] [G loss: 1.011371]\n",
      "epoch:37 step:35334 [D loss: 0.028308, acc.: 100.00%] [G loss: 5.278629]\n",
      "epoch:37 step:35335 [D loss: 0.095138, acc.: 99.22%] [G loss: 2.763263]\n",
      "epoch:37 step:35336 [D loss: 0.012997, acc.: 100.00%] [G loss: 1.906071]\n",
      "epoch:37 step:35337 [D loss: 0.468743, acc.: 72.66%] [G loss: 0.581290]\n",
      "epoch:37 step:35338 [D loss: 1.224241, acc.: 53.12%] [G loss: 1.402444]\n",
      "epoch:37 step:35339 [D loss: 0.058175, acc.: 99.22%] [G loss: 5.579250]\n",
      "epoch:37 step:35340 [D loss: 0.356493, acc.: 87.50%] [G loss: 3.261038]\n",
      "epoch:37 step:35341 [D loss: 0.123396, acc.: 96.88%] [G loss: 4.043089]\n",
      "epoch:37 step:35342 [D loss: 0.165492, acc.: 94.53%] [G loss: 4.103089]\n",
      "epoch:37 step:35343 [D loss: 0.880516, acc.: 56.25%] [G loss: 3.622321]\n",
      "epoch:37 step:35344 [D loss: 0.069869, acc.: 98.44%] [G loss: 3.810855]\n",
      "epoch:37 step:35345 [D loss: 0.045396, acc.: 100.00%] [G loss: 1.728077]\n",
      "epoch:37 step:35346 [D loss: 0.171249, acc.: 92.97%] [G loss: 5.841618]\n",
      "epoch:37 step:35347 [D loss: 1.177607, acc.: 41.41%] [G loss: 5.541586]\n",
      "epoch:37 step:35348 [D loss: 0.710865, acc.: 60.94%] [G loss: 2.923179]\n",
      "epoch:37 step:35349 [D loss: 0.394494, acc.: 78.12%] [G loss: 1.333158]\n",
      "epoch:37 step:35350 [D loss: 0.789731, acc.: 57.03%] [G loss: 1.906307]\n",
      "epoch:37 step:35351 [D loss: 0.190551, acc.: 96.09%] [G loss: 5.118761]\n",
      "epoch:37 step:35352 [D loss: 0.081650, acc.: 100.00%] [G loss: 2.870080]\n",
      "epoch:37 step:35353 [D loss: 0.116836, acc.: 98.44%] [G loss: 2.078934]\n",
      "epoch:37 step:35354 [D loss: 0.064839, acc.: 99.22%] [G loss: 1.628414]\n",
      "epoch:37 step:35355 [D loss: 0.174767, acc.: 97.66%] [G loss: 3.274498]\n",
      "epoch:37 step:35356 [D loss: 0.164924, acc.: 96.88%] [G loss: 6.210812]\n",
      "epoch:37 step:35357 [D loss: 0.076316, acc.: 99.22%] [G loss: 2.150312]\n",
      "epoch:37 step:35358 [D loss: 0.220549, acc.: 92.19%] [G loss: 3.367645]\n",
      "epoch:37 step:35359 [D loss: 0.120021, acc.: 98.44%] [G loss: 3.864677]\n",
      "epoch:37 step:35360 [D loss: 0.137302, acc.: 97.66%] [G loss: 5.013700]\n",
      "epoch:37 step:35361 [D loss: 0.353901, acc.: 79.69%] [G loss: 5.167752]\n",
      "epoch:37 step:35362 [D loss: 0.056445, acc.: 100.00%] [G loss: 3.550220]\n",
      "epoch:37 step:35363 [D loss: 0.011504, acc.: 100.00%] [G loss: 1.014178]\n",
      "epoch:37 step:35364 [D loss: 0.019625, acc.: 100.00%] [G loss: 1.987682]\n",
      "epoch:37 step:35365 [D loss: 0.482009, acc.: 66.41%] [G loss: 2.848589]\n",
      "epoch:37 step:35366 [D loss: 0.047694, acc.: 100.00%] [G loss: 5.665579]\n",
      "epoch:37 step:35367 [D loss: 0.060515, acc.: 99.22%] [G loss: 5.300614]\n",
      "epoch:37 step:35368 [D loss: 0.004935, acc.: 100.00%] [G loss: 4.204185]\n",
      "epoch:37 step:35369 [D loss: 0.061520, acc.: 99.22%] [G loss: 4.567118]\n",
      "epoch:37 step:35370 [D loss: 0.180505, acc.: 96.09%] [G loss: 5.986102]\n",
      "epoch:37 step:35371 [D loss: 0.229309, acc.: 90.62%] [G loss: 4.319732]\n",
      "epoch:37 step:35372 [D loss: 0.349890, acc.: 84.38%] [G loss: 3.395002]\n",
      "epoch:37 step:35373 [D loss: 0.310376, acc.: 85.16%] [G loss: 2.210804]\n",
      "epoch:37 step:35374 [D loss: 0.219219, acc.: 92.97%] [G loss: 3.758892]\n",
      "epoch:37 step:35375 [D loss: 0.337824, acc.: 78.91%] [G loss: 4.707048]\n",
      "epoch:37 step:35376 [D loss: 0.069887, acc.: 99.22%] [G loss: 3.626421]\n",
      "epoch:37 step:35377 [D loss: 0.551664, acc.: 70.31%] [G loss: 1.983976]\n",
      "epoch:37 step:35378 [D loss: 0.132014, acc.: 96.88%] [G loss: 3.108137]\n",
      "epoch:37 step:35379 [D loss: 0.134349, acc.: 96.88%] [G loss: 2.810545]\n",
      "epoch:37 step:35380 [D loss: 0.042451, acc.: 100.00%] [G loss: 1.656657]\n",
      "epoch:37 step:35381 [D loss: 0.033461, acc.: 99.22%] [G loss: 1.665831]\n",
      "epoch:37 step:35382 [D loss: 0.274250, acc.: 85.16%] [G loss: 2.077229]\n",
      "epoch:37 step:35383 [D loss: 0.043530, acc.: 98.44%] [G loss: 3.798817]\n",
      "epoch:37 step:35384 [D loss: 0.048878, acc.: 100.00%] [G loss: 5.553324]\n",
      "epoch:37 step:35385 [D loss: 0.787156, acc.: 57.81%] [G loss: 3.611471]\n",
      "epoch:37 step:35386 [D loss: 0.055859, acc.: 100.00%] [G loss: 4.277350]\n",
      "epoch:37 step:35387 [D loss: 0.417686, acc.: 75.00%] [G loss: 4.267341]\n",
      "epoch:37 step:35388 [D loss: 0.173747, acc.: 95.31%] [G loss: 3.602576]\n",
      "epoch:37 step:35389 [D loss: 0.111799, acc.: 98.44%] [G loss: 8.741899]\n",
      "epoch:37 step:35390 [D loss: 0.579034, acc.: 67.97%] [G loss: 4.659502]\n",
      "epoch:37 step:35391 [D loss: 0.055822, acc.: 98.44%] [G loss: 0.174808]\n",
      "epoch:37 step:35392 [D loss: 0.133341, acc.: 98.44%] [G loss: 2.306437]\n",
      "epoch:37 step:35393 [D loss: 1.730735, acc.: 50.78%] [G loss: 5.220351]\n",
      "epoch:37 step:35394 [D loss: 0.033723, acc.: 100.00%] [G loss: 5.553665]\n",
      "epoch:37 step:35395 [D loss: 0.079229, acc.: 99.22%] [G loss: 4.174689]\n",
      "epoch:37 step:35396 [D loss: 0.032457, acc.: 100.00%] [G loss: 3.113742]\n",
      "epoch:37 step:35397 [D loss: 0.228673, acc.: 97.66%] [G loss: 6.798356]\n",
      "epoch:37 step:35398 [D loss: 0.034188, acc.: 100.00%] [G loss: 4.264426]\n",
      "epoch:37 step:35399 [D loss: 1.429265, acc.: 52.34%] [G loss: 1.516508]\n",
      "epoch:37 step:35400 [D loss: 0.062988, acc.: 98.44%] [G loss: 2.726244]\n",
      "epoch:37 step:35401 [D loss: 0.668006, acc.: 61.72%] [G loss: 5.320839]\n",
      "epoch:37 step:35402 [D loss: 0.085622, acc.: 98.44%] [G loss: 4.713970]\n",
      "epoch:37 step:35403 [D loss: 0.090797, acc.: 98.44%] [G loss: 6.100692]\n",
      "epoch:37 step:35404 [D loss: 0.280342, acc.: 93.75%] [G loss: 4.808223]\n",
      "epoch:37 step:35405 [D loss: 0.839914, acc.: 63.28%] [G loss: 1.169902]\n",
      "epoch:37 step:35406 [D loss: 0.063700, acc.: 100.00%] [G loss: 4.502424]\n",
      "epoch:37 step:35407 [D loss: 0.101397, acc.: 98.44%] [G loss: 1.381392]\n",
      "epoch:37 step:35408 [D loss: 0.098522, acc.: 100.00%] [G loss: 1.386983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:35409 [D loss: 0.148254, acc.: 96.88%] [G loss: 2.402355]\n",
      "epoch:37 step:35410 [D loss: 0.668797, acc.: 67.19%] [G loss: 2.083508]\n",
      "epoch:37 step:35411 [D loss: 0.135207, acc.: 96.88%] [G loss: 3.858436]\n",
      "epoch:37 step:35412 [D loss: 0.156400, acc.: 97.66%] [G loss: 2.844336]\n",
      "epoch:37 step:35413 [D loss: 0.067735, acc.: 99.22%] [G loss: 3.324028]\n",
      "epoch:37 step:35414 [D loss: 0.014360, acc.: 100.00%] [G loss: 6.306699]\n",
      "epoch:37 step:35415 [D loss: 0.397420, acc.: 79.69%] [G loss: 6.896380]\n",
      "epoch:37 step:35416 [D loss: 0.616884, acc.: 64.84%] [G loss: 3.140708]\n",
      "epoch:37 step:35417 [D loss: 0.458445, acc.: 76.56%] [G loss: 1.816137]\n",
      "epoch:37 step:35418 [D loss: 0.025305, acc.: 99.22%] [G loss: 2.854738]\n",
      "epoch:37 step:35419 [D loss: 1.176665, acc.: 59.38%] [G loss: 6.290246]\n",
      "epoch:37 step:35420 [D loss: 0.154356, acc.: 96.09%] [G loss: 1.608756]\n",
      "epoch:37 step:35421 [D loss: 0.308251, acc.: 84.38%] [G loss: 2.028857]\n",
      "epoch:37 step:35422 [D loss: 0.084661, acc.: 99.22%] [G loss: 4.030960]\n",
      "epoch:37 step:35423 [D loss: 0.196968, acc.: 95.31%] [G loss: 1.162983]\n",
      "epoch:37 step:35424 [D loss: 0.049816, acc.: 98.44%] [G loss: 3.762662]\n",
      "epoch:37 step:35425 [D loss: 0.108594, acc.: 99.22%] [G loss: 6.273758]\n",
      "epoch:37 step:35426 [D loss: 0.113040, acc.: 99.22%] [G loss: 1.355175]\n",
      "epoch:37 step:35427 [D loss: 1.584386, acc.: 45.31%] [G loss: 5.369230]\n",
      "epoch:37 step:35428 [D loss: 0.408920, acc.: 76.56%] [G loss: 3.868475]\n",
      "epoch:37 step:35429 [D loss: 0.129572, acc.: 95.31%] [G loss: 2.682118]\n",
      "epoch:37 step:35430 [D loss: 0.178332, acc.: 96.88%] [G loss: 3.263688]\n",
      "epoch:37 step:35431 [D loss: 0.577639, acc.: 71.09%] [G loss: 4.118340]\n",
      "epoch:37 step:35432 [D loss: 0.066254, acc.: 100.00%] [G loss: 6.441078]\n",
      "epoch:37 step:35433 [D loss: 1.221202, acc.: 34.38%] [G loss: 1.462987]\n",
      "epoch:37 step:35434 [D loss: 0.080459, acc.: 98.44%] [G loss: 5.904451]\n",
      "epoch:37 step:35435 [D loss: 0.212804, acc.: 95.31%] [G loss: 1.329706]\n",
      "epoch:37 step:35436 [D loss: 0.049243, acc.: 100.00%] [G loss: 3.733472]\n",
      "epoch:37 step:35437 [D loss: 0.050010, acc.: 100.00%] [G loss: 1.827665]\n",
      "epoch:37 step:35438 [D loss: 0.275064, acc.: 89.84%] [G loss: 3.643196]\n",
      "epoch:37 step:35439 [D loss: 0.081592, acc.: 100.00%] [G loss: 3.384238]\n",
      "epoch:37 step:35440 [D loss: 0.083015, acc.: 99.22%] [G loss: 6.618495]\n",
      "epoch:37 step:35441 [D loss: 0.128639, acc.: 95.31%] [G loss: 3.305335]\n",
      "epoch:37 step:35442 [D loss: 0.120745, acc.: 99.22%] [G loss: 2.892312]\n",
      "epoch:37 step:35443 [D loss: 0.214628, acc.: 92.97%] [G loss: 3.498826]\n",
      "epoch:37 step:35444 [D loss: 0.172413, acc.: 94.53%] [G loss: 4.098158]\n",
      "epoch:37 step:35445 [D loss: 0.086161, acc.: 98.44%] [G loss: 1.507672]\n",
      "epoch:37 step:35446 [D loss: 0.043736, acc.: 100.00%] [G loss: 2.121886]\n",
      "epoch:37 step:35447 [D loss: 0.228088, acc.: 90.62%] [G loss: 3.365719]\n",
      "epoch:37 step:35448 [D loss: 0.529641, acc.: 67.97%] [G loss: 4.478434]\n",
      "epoch:37 step:35449 [D loss: 0.086623, acc.: 99.22%] [G loss: 2.564260]\n",
      "epoch:37 step:35450 [D loss: 0.535887, acc.: 70.31%] [G loss: 3.210416]\n",
      "epoch:37 step:35451 [D loss: 0.008310, acc.: 100.00%] [G loss: 2.640196]\n",
      "epoch:37 step:35452 [D loss: 0.122504, acc.: 97.66%] [G loss: 4.662425]\n",
      "epoch:37 step:35453 [D loss: 0.351464, acc.: 87.50%] [G loss: 3.947732]\n",
      "epoch:37 step:35454 [D loss: 0.125279, acc.: 96.88%] [G loss: 5.752110]\n",
      "epoch:37 step:35455 [D loss: 0.274595, acc.: 92.19%] [G loss: 3.460096]\n",
      "epoch:37 step:35456 [D loss: 0.423104, acc.: 76.56%] [G loss: 7.722203]\n",
      "epoch:37 step:35457 [D loss: 0.773963, acc.: 57.81%] [G loss: 3.915435]\n",
      "epoch:37 step:35458 [D loss: 0.144381, acc.: 97.66%] [G loss: 2.438028]\n",
      "epoch:37 step:35459 [D loss: 1.142152, acc.: 55.47%] [G loss: 3.922979]\n",
      "epoch:37 step:35460 [D loss: 0.140234, acc.: 97.66%] [G loss: 6.515922]\n",
      "epoch:37 step:35461 [D loss: 0.110342, acc.: 96.09%] [G loss: 6.743158]\n",
      "epoch:37 step:35462 [D loss: 0.643409, acc.: 62.50%] [G loss: 4.735663]\n",
      "epoch:37 step:35463 [D loss: 0.138057, acc.: 94.53%] [G loss: 3.416257]\n",
      "epoch:37 step:35464 [D loss: 0.028674, acc.: 100.00%] [G loss: 3.377975]\n",
      "epoch:37 step:35465 [D loss: 0.452021, acc.: 74.22%] [G loss: 3.201899]\n",
      "epoch:37 step:35466 [D loss: 0.348130, acc.: 82.81%] [G loss: 6.027901]\n",
      "epoch:37 step:35467 [D loss: 0.305334, acc.: 82.03%] [G loss: 7.352621]\n",
      "epoch:37 step:35468 [D loss: 0.138949, acc.: 96.88%] [G loss: 6.209746]\n",
      "epoch:37 step:35469 [D loss: 0.176053, acc.: 97.66%] [G loss: 4.577324]\n",
      "epoch:37 step:35470 [D loss: 0.375754, acc.: 82.81%] [G loss: 3.352625]\n",
      "epoch:37 step:35471 [D loss: 0.063770, acc.: 99.22%] [G loss: 4.684684]\n",
      "epoch:37 step:35472 [D loss: 0.868203, acc.: 47.66%] [G loss: 5.861729]\n",
      "epoch:37 step:35473 [D loss: 0.056489, acc.: 99.22%] [G loss: 2.111851]\n",
      "epoch:37 step:35474 [D loss: 0.200161, acc.: 96.88%] [G loss: 3.973755]\n",
      "epoch:37 step:35475 [D loss: 0.248273, acc.: 90.62%] [G loss: 3.215116]\n",
      "epoch:37 step:35476 [D loss: 0.452678, acc.: 72.66%] [G loss: 4.565621]\n",
      "epoch:37 step:35477 [D loss: 0.315024, acc.: 87.50%] [G loss: 4.824930]\n",
      "epoch:37 step:35478 [D loss: 0.123717, acc.: 96.88%] [G loss: 3.091472]\n",
      "epoch:37 step:35479 [D loss: 0.010520, acc.: 100.00%] [G loss: 4.731250]\n",
      "epoch:37 step:35480 [D loss: 0.248309, acc.: 95.31%] [G loss: 4.454700]\n",
      "epoch:37 step:35481 [D loss: 0.516532, acc.: 67.19%] [G loss: 3.186376]\n",
      "epoch:37 step:35482 [D loss: 0.061885, acc.: 99.22%] [G loss: 7.702392]\n",
      "epoch:37 step:35483 [D loss: 0.320503, acc.: 78.91%] [G loss: 4.388911]\n",
      "epoch:37 step:35484 [D loss: 0.028470, acc.: 100.00%] [G loss: 4.693084]\n",
      "epoch:37 step:35485 [D loss: 0.060720, acc.: 99.22%] [G loss: 3.678350]\n",
      "epoch:37 step:35486 [D loss: 0.719978, acc.: 61.72%] [G loss: 1.739166]\n",
      "epoch:37 step:35487 [D loss: 0.306860, acc.: 90.62%] [G loss: 4.353804]\n",
      "epoch:37 step:35488 [D loss: 0.063528, acc.: 98.44%] [G loss: 3.116606]\n",
      "epoch:37 step:35489 [D loss: 0.086413, acc.: 99.22%] [G loss: 2.129013]\n",
      "epoch:37 step:35490 [D loss: 0.038441, acc.: 99.22%] [G loss: 2.431118]\n",
      "epoch:37 step:35491 [D loss: 0.033652, acc.: 99.22%] [G loss: 0.566039]\n",
      "epoch:37 step:35492 [D loss: 0.018443, acc.: 100.00%] [G loss: 6.186675]\n",
      "epoch:37 step:35493 [D loss: 0.093048, acc.: 98.44%] [G loss: 0.113350]\n",
      "epoch:37 step:35494 [D loss: 0.085413, acc.: 100.00%] [G loss: 2.635222]\n",
      "epoch:37 step:35495 [D loss: 0.134930, acc.: 98.44%] [G loss: 0.786559]\n",
      "epoch:37 step:35496 [D loss: 0.045665, acc.: 99.22%] [G loss: 3.014361]\n",
      "epoch:37 step:35497 [D loss: 0.079014, acc.: 97.66%] [G loss: 2.811364]\n",
      "epoch:37 step:35498 [D loss: 0.127437, acc.: 97.66%] [G loss: 4.120446]\n",
      "epoch:37 step:35499 [D loss: 1.660619, acc.: 50.00%] [G loss: 3.263425]\n",
      "epoch:37 step:35500 [D loss: 0.042135, acc.: 100.00%] [G loss: 6.779434]\n",
      "epoch:37 step:35501 [D loss: 0.248458, acc.: 90.62%] [G loss: 3.004878]\n",
      "epoch:37 step:35502 [D loss: 0.159930, acc.: 97.66%] [G loss: 3.071376]\n",
      "epoch:37 step:35503 [D loss: 0.703716, acc.: 58.59%] [G loss: 2.554110]\n",
      "epoch:37 step:35504 [D loss: 0.157056, acc.: 97.66%] [G loss: 3.773133]\n",
      "epoch:37 step:35505 [D loss: 0.085521, acc.: 100.00%] [G loss: 1.664955]\n",
      "epoch:37 step:35506 [D loss: 0.019000, acc.: 100.00%] [G loss: 2.330966]\n",
      "epoch:37 step:35507 [D loss: 0.966509, acc.: 53.12%] [G loss: 5.098709]\n",
      "epoch:37 step:35508 [D loss: 0.071240, acc.: 99.22%] [G loss: 4.674569]\n",
      "epoch:37 step:35509 [D loss: 0.818688, acc.: 57.81%] [G loss: 3.624862]\n",
      "epoch:37 step:35510 [D loss: 0.363452, acc.: 88.28%] [G loss: 4.775548]\n",
      "epoch:37 step:35511 [D loss: 0.347803, acc.: 89.84%] [G loss: 5.174047]\n",
      "epoch:37 step:35512 [D loss: 0.039766, acc.: 100.00%] [G loss: 2.432836]\n",
      "epoch:37 step:35513 [D loss: 0.162001, acc.: 96.09%] [G loss: 4.692181]\n",
      "epoch:37 step:35514 [D loss: 0.273723, acc.: 91.41%] [G loss: 3.771088]\n",
      "epoch:37 step:35515 [D loss: 0.303769, acc.: 86.72%] [G loss: 4.048652]\n",
      "epoch:37 step:35516 [D loss: 0.119475, acc.: 96.09%] [G loss: 3.942783]\n",
      "epoch:37 step:35517 [D loss: 0.114281, acc.: 100.00%] [G loss: 1.427520]\n",
      "epoch:37 step:35518 [D loss: 0.132519, acc.: 98.44%] [G loss: 2.105131]\n",
      "epoch:37 step:35519 [D loss: 0.106152, acc.: 97.66%] [G loss: 4.825950]\n",
      "epoch:37 step:35520 [D loss: 0.490885, acc.: 67.97%] [G loss: 0.385396]\n",
      "epoch:37 step:35521 [D loss: 0.101813, acc.: 97.66%] [G loss: 4.234983]\n",
      "epoch:37 step:35522 [D loss: 0.022755, acc.: 99.22%] [G loss: 1.916848]\n",
      "epoch:37 step:35523 [D loss: 0.382473, acc.: 82.81%] [G loss: 0.952202]\n",
      "epoch:37 step:35524 [D loss: 2.127312, acc.: 50.00%] [G loss: 5.138268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:35525 [D loss: 0.056963, acc.: 100.00%] [G loss: 3.658802]\n",
      "epoch:37 step:35526 [D loss: 0.389949, acc.: 77.34%] [G loss: 3.483414]\n",
      "epoch:37 step:35527 [D loss: 0.052757, acc.: 99.22%] [G loss: 0.207830]\n",
      "epoch:37 step:35528 [D loss: 0.158019, acc.: 96.88%] [G loss: 4.171317]\n",
      "epoch:37 step:35529 [D loss: 0.056859, acc.: 100.00%] [G loss: 0.419293]\n",
      "epoch:37 step:35530 [D loss: 1.393221, acc.: 52.34%] [G loss: 1.454891]\n",
      "epoch:37 step:35531 [D loss: 0.661186, acc.: 60.94%] [G loss: 2.773845]\n",
      "epoch:37 step:35532 [D loss: 0.359807, acc.: 75.78%] [G loss: 5.260839]\n",
      "epoch:37 step:35533 [D loss: 0.091951, acc.: 99.22%] [G loss: 3.666294]\n",
      "epoch:37 step:35534 [D loss: 0.998218, acc.: 52.34%] [G loss: 4.255659]\n",
      "epoch:37 step:35535 [D loss: 0.042734, acc.: 99.22%] [G loss: 3.744248]\n",
      "epoch:37 step:35536 [D loss: 0.481558, acc.: 71.09%] [G loss: 1.557766]\n",
      "epoch:37 step:35537 [D loss: 0.083010, acc.: 100.00%] [G loss: 3.161424]\n",
      "epoch:37 step:35538 [D loss: 0.122114, acc.: 98.44%] [G loss: 2.835702]\n",
      "epoch:37 step:35539 [D loss: 0.017125, acc.: 100.00%] [G loss: 4.227751]\n",
      "epoch:37 step:35540 [D loss: 0.032183, acc.: 100.00%] [G loss: 3.454331]\n",
      "epoch:37 step:35541 [D loss: 0.191821, acc.: 95.31%] [G loss: 3.216646]\n",
      "epoch:37 step:35542 [D loss: 0.202317, acc.: 97.66%] [G loss: 2.431323]\n",
      "epoch:37 step:35543 [D loss: 0.093706, acc.: 98.44%] [G loss: 7.098586]\n",
      "epoch:37 step:35544 [D loss: 0.068911, acc.: 100.00%] [G loss: 6.128264]\n",
      "epoch:37 step:35545 [D loss: 0.037974, acc.: 99.22%] [G loss: 3.081849]\n",
      "epoch:37 step:35546 [D loss: 0.252678, acc.: 90.62%] [G loss: 5.939073]\n",
      "epoch:37 step:35547 [D loss: 1.247245, acc.: 44.53%] [G loss: 3.070556]\n",
      "epoch:37 step:35548 [D loss: 0.217789, acc.: 94.53%] [G loss: 3.672000]\n",
      "epoch:37 step:35549 [D loss: 0.089466, acc.: 99.22%] [G loss: 4.079249]\n",
      "epoch:37 step:35550 [D loss: 0.078701, acc.: 99.22%] [G loss: 5.630145]\n",
      "epoch:37 step:35551 [D loss: 0.025151, acc.: 100.00%] [G loss: 2.744813]\n",
      "epoch:37 step:35552 [D loss: 0.118210, acc.: 99.22%] [G loss: 1.773281]\n",
      "epoch:37 step:35553 [D loss: 0.120963, acc.: 99.22%] [G loss: 5.228511]\n",
      "epoch:37 step:35554 [D loss: 0.075178, acc.: 100.00%] [G loss: 3.104334]\n",
      "epoch:37 step:35555 [D loss: 0.028774, acc.: 100.00%] [G loss: 3.304530]\n",
      "epoch:37 step:35556 [D loss: 0.164833, acc.: 95.31%] [G loss: 0.619929]\n",
      "epoch:37 step:35557 [D loss: 0.024246, acc.: 100.00%] [G loss: 2.753075]\n",
      "epoch:37 step:35558 [D loss: 0.087108, acc.: 100.00%] [G loss: 1.290215]\n",
      "epoch:37 step:35559 [D loss: 0.334002, acc.: 89.06%] [G loss: 3.899575]\n",
      "epoch:37 step:35560 [D loss: 0.044561, acc.: 100.00%] [G loss: 4.380964]\n",
      "epoch:37 step:35561 [D loss: 0.685719, acc.: 67.19%] [G loss: 3.020432]\n",
      "epoch:37 step:35562 [D loss: 0.087755, acc.: 98.44%] [G loss: 5.103601]\n",
      "epoch:37 step:35563 [D loss: 0.013058, acc.: 100.00%] [G loss: 5.305740]\n",
      "epoch:37 step:35564 [D loss: 0.037356, acc.: 100.00%] [G loss: 4.044185]\n",
      "epoch:37 step:35565 [D loss: 0.102288, acc.: 99.22%] [G loss: 4.796844]\n",
      "epoch:37 step:35566 [D loss: 0.168859, acc.: 93.75%] [G loss: 2.806981]\n",
      "epoch:37 step:35567 [D loss: 0.060888, acc.: 99.22%] [G loss: 2.731823]\n",
      "epoch:37 step:35568 [D loss: 0.176997, acc.: 96.88%] [G loss: 1.462658]\n",
      "epoch:37 step:35569 [D loss: 0.060336, acc.: 100.00%] [G loss: 5.074911]\n",
      "epoch:37 step:35570 [D loss: 0.079856, acc.: 99.22%] [G loss: 1.297886]\n",
      "epoch:37 step:35571 [D loss: 0.005153, acc.: 100.00%] [G loss: 3.666149]\n",
      "epoch:37 step:35572 [D loss: 0.070739, acc.: 98.44%] [G loss: 3.332929]\n",
      "epoch:37 step:35573 [D loss: 0.094714, acc.: 98.44%] [G loss: 2.880393]\n",
      "epoch:37 step:35574 [D loss: 0.125311, acc.: 98.44%] [G loss: 2.707928]\n",
      "epoch:37 step:35575 [D loss: 0.118098, acc.: 97.66%] [G loss: 2.913780]\n",
      "epoch:37 step:35576 [D loss: 0.056829, acc.: 99.22%] [G loss: 3.703986]\n",
      "epoch:37 step:35577 [D loss: 0.106761, acc.: 97.66%] [G loss: 2.574280]\n",
      "epoch:37 step:35578 [D loss: 0.083356, acc.: 100.00%] [G loss: 1.896272]\n",
      "epoch:37 step:35579 [D loss: 0.173645, acc.: 93.75%] [G loss: 1.907713]\n",
      "epoch:37 step:35580 [D loss: 0.349417, acc.: 89.84%] [G loss: 2.748352]\n",
      "epoch:37 step:35581 [D loss: 0.061791, acc.: 98.44%] [G loss: 2.460589]\n",
      "epoch:37 step:35582 [D loss: 0.603318, acc.: 66.41%] [G loss: 0.998568]\n",
      "epoch:37 step:35583 [D loss: 0.073442, acc.: 98.44%] [G loss: 4.789974]\n",
      "epoch:37 step:35584 [D loss: 0.097994, acc.: 96.88%] [G loss: 3.954470]\n",
      "epoch:37 step:35585 [D loss: 0.147347, acc.: 96.88%] [G loss: 3.778313]\n",
      "epoch:37 step:35586 [D loss: 0.547485, acc.: 68.75%] [G loss: 2.798555]\n",
      "epoch:37 step:35587 [D loss: 0.175319, acc.: 91.41%] [G loss: 6.808258]\n",
      "epoch:37 step:35588 [D loss: 0.163240, acc.: 96.88%] [G loss: 5.649209]\n",
      "epoch:37 step:35589 [D loss: 0.180089, acc.: 95.31%] [G loss: 2.451291]\n",
      "epoch:37 step:35590 [D loss: 0.261219, acc.: 92.19%] [G loss: 2.844280]\n",
      "epoch:37 step:35591 [D loss: 0.518819, acc.: 71.88%] [G loss: 0.970005]\n",
      "epoch:37 step:35592 [D loss: 0.123603, acc.: 98.44%] [G loss: 1.327851]\n",
      "epoch:37 step:35593 [D loss: 0.175095, acc.: 94.53%] [G loss: 6.128031]\n",
      "epoch:37 step:35594 [D loss: 0.062993, acc.: 98.44%] [G loss: 1.674860]\n",
      "epoch:37 step:35595 [D loss: 0.202714, acc.: 96.09%] [G loss: 4.532119]\n",
      "epoch:37 step:35596 [D loss: 0.340689, acc.: 89.84%] [G loss: 2.144135]\n",
      "epoch:37 step:35597 [D loss: 0.083284, acc.: 99.22%] [G loss: 4.881016]\n",
      "epoch:37 step:35598 [D loss: 0.194235, acc.: 92.19%] [G loss: 5.846029]\n",
      "epoch:37 step:35599 [D loss: 0.113124, acc.: 99.22%] [G loss: 4.938241]\n",
      "epoch:37 step:35600 [D loss: 0.316654, acc.: 86.72%] [G loss: 5.731184]\n",
      "epoch:37 step:35601 [D loss: 0.157491, acc.: 92.97%] [G loss: 5.682713]\n",
      "epoch:37 step:35602 [D loss: 0.066912, acc.: 99.22%] [G loss: 2.408491]\n",
      "epoch:37 step:35603 [D loss: 0.109657, acc.: 98.44%] [G loss: 3.866092]\n",
      "epoch:37 step:35604 [D loss: 0.017180, acc.: 100.00%] [G loss: 1.901067]\n",
      "epoch:37 step:35605 [D loss: 0.055087, acc.: 100.00%] [G loss: 5.530054]\n",
      "epoch:37 step:35606 [D loss: 0.183572, acc.: 92.19%] [G loss: 2.898350]\n",
      "epoch:38 step:35607 [D loss: 0.121487, acc.: 98.44%] [G loss: 0.733345]\n",
      "epoch:38 step:35608 [D loss: 0.011787, acc.: 100.00%] [G loss: 3.288828]\n",
      "epoch:38 step:35609 [D loss: 0.522234, acc.: 76.56%] [G loss: 2.306436]\n",
      "epoch:38 step:35610 [D loss: 0.121111, acc.: 98.44%] [G loss: 1.519406]\n",
      "epoch:38 step:35611 [D loss: 0.158302, acc.: 96.09%] [G loss: 4.966150]\n",
      "epoch:38 step:35612 [D loss: 0.460694, acc.: 76.56%] [G loss: 2.355464]\n",
      "epoch:38 step:35613 [D loss: 0.454282, acc.: 72.66%] [G loss: 4.560900]\n",
      "epoch:38 step:35614 [D loss: 0.265176, acc.: 92.97%] [G loss: 4.171506]\n",
      "epoch:38 step:35615 [D loss: 0.045127, acc.: 100.00%] [G loss: 3.897518]\n",
      "epoch:38 step:35616 [D loss: 0.052639, acc.: 99.22%] [G loss: 4.810380]\n",
      "epoch:38 step:35617 [D loss: 0.238463, acc.: 91.41%] [G loss: 5.816875]\n",
      "epoch:38 step:35618 [D loss: 0.144932, acc.: 96.88%] [G loss: 5.054753]\n",
      "epoch:38 step:35619 [D loss: 1.497559, acc.: 28.91%] [G loss: 6.537039]\n",
      "epoch:38 step:35620 [D loss: 0.870592, acc.: 57.03%] [G loss: 4.167139]\n",
      "epoch:38 step:35621 [D loss: 0.079826, acc.: 98.44%] [G loss: 4.980966]\n",
      "epoch:38 step:35622 [D loss: 0.044128, acc.: 100.00%] [G loss: 3.457009]\n",
      "epoch:38 step:35623 [D loss: 0.035102, acc.: 99.22%] [G loss: 2.167997]\n",
      "epoch:38 step:35624 [D loss: 0.056892, acc.: 99.22%] [G loss: 2.269958]\n",
      "epoch:38 step:35625 [D loss: 0.360395, acc.: 81.25%] [G loss: 4.360811]\n",
      "epoch:38 step:35626 [D loss: 0.030361, acc.: 100.00%] [G loss: 4.790575]\n",
      "epoch:38 step:35627 [D loss: 0.069742, acc.: 98.44%] [G loss: 4.107848]\n",
      "epoch:38 step:35628 [D loss: 1.570736, acc.: 24.22%] [G loss: 4.119275]\n",
      "epoch:38 step:35629 [D loss: 0.138585, acc.: 94.53%] [G loss: 3.026850]\n",
      "epoch:38 step:35630 [D loss: 0.058943, acc.: 99.22%] [G loss: 1.851052]\n",
      "epoch:38 step:35631 [D loss: 0.136121, acc.: 95.31%] [G loss: 0.633911]\n",
      "epoch:38 step:35632 [D loss: 0.340717, acc.: 84.38%] [G loss: 4.679113]\n",
      "epoch:38 step:35633 [D loss: 0.101122, acc.: 97.66%] [G loss: 7.558605]\n",
      "epoch:38 step:35634 [D loss: 0.185792, acc.: 95.31%] [G loss: 3.507231]\n",
      "epoch:38 step:35635 [D loss: 0.339002, acc.: 86.72%] [G loss: 5.619472]\n",
      "epoch:38 step:35636 [D loss: 0.028085, acc.: 100.00%] [G loss: 6.440071]\n",
      "epoch:38 step:35637 [D loss: 0.130246, acc.: 96.09%] [G loss: 3.848377]\n",
      "epoch:38 step:35638 [D loss: 0.051144, acc.: 98.44%] [G loss: 5.042368]\n",
      "epoch:38 step:35639 [D loss: 0.024345, acc.: 99.22%] [G loss: 2.025599]\n",
      "epoch:38 step:35640 [D loss: 0.052769, acc.: 99.22%] [G loss: 5.925900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:35641 [D loss: 0.003563, acc.: 100.00%] [G loss: 3.694224]\n",
      "epoch:38 step:35642 [D loss: 0.121502, acc.: 97.66%] [G loss: 1.343601]\n",
      "epoch:38 step:35643 [D loss: 0.024571, acc.: 100.00%] [G loss: 5.035206]\n",
      "epoch:38 step:35644 [D loss: 0.030560, acc.: 99.22%] [G loss: 0.852531]\n",
      "epoch:38 step:35645 [D loss: 1.060180, acc.: 53.91%] [G loss: 4.019968]\n",
      "epoch:38 step:35646 [D loss: 0.303586, acc.: 93.75%] [G loss: 8.643415]\n",
      "epoch:38 step:35647 [D loss: 0.102561, acc.: 97.66%] [G loss: 9.169699]\n",
      "epoch:38 step:35648 [D loss: 0.135303, acc.: 96.88%] [G loss: 6.668439]\n",
      "epoch:38 step:35649 [D loss: 0.375434, acc.: 85.16%] [G loss: 7.337236]\n",
      "epoch:38 step:35650 [D loss: 1.400457, acc.: 52.34%] [G loss: 2.028673]\n",
      "epoch:38 step:35651 [D loss: 2.191358, acc.: 13.28%] [G loss: 0.594158]\n",
      "epoch:38 step:35652 [D loss: 0.837694, acc.: 59.38%] [G loss: 3.930694]\n",
      "epoch:38 step:35653 [D loss: 0.058728, acc.: 100.00%] [G loss: 4.056170]\n",
      "epoch:38 step:35654 [D loss: 0.133201, acc.: 95.31%] [G loss: 4.297935]\n",
      "epoch:38 step:35655 [D loss: 0.093363, acc.: 98.44%] [G loss: 2.839316]\n",
      "epoch:38 step:35656 [D loss: 0.022456, acc.: 100.00%] [G loss: 3.110380]\n",
      "epoch:38 step:35657 [D loss: 0.101576, acc.: 96.88%] [G loss: 4.159517]\n",
      "epoch:38 step:35658 [D loss: 0.190009, acc.: 94.53%] [G loss: 4.637459]\n",
      "epoch:38 step:35659 [D loss: 0.263471, acc.: 92.19%] [G loss: 4.415764]\n",
      "epoch:38 step:35660 [D loss: 0.027699, acc.: 100.00%] [G loss: 3.514429]\n",
      "epoch:38 step:35661 [D loss: 0.020179, acc.: 100.00%] [G loss: 4.374810]\n",
      "epoch:38 step:35662 [D loss: 0.274705, acc.: 89.84%] [G loss: 3.593509]\n",
      "epoch:38 step:35663 [D loss: 0.422493, acc.: 72.66%] [G loss: 4.677301]\n",
      "epoch:38 step:35664 [D loss: 0.102387, acc.: 99.22%] [G loss: 1.663134]\n",
      "epoch:38 step:35665 [D loss: 0.336709, acc.: 88.28%] [G loss: 2.309345]\n",
      "epoch:38 step:35666 [D loss: 0.515151, acc.: 69.53%] [G loss: 1.394566]\n",
      "epoch:38 step:35667 [D loss: 0.082970, acc.: 99.22%] [G loss: 3.853516]\n",
      "epoch:38 step:35668 [D loss: 0.186484, acc.: 95.31%] [G loss: 4.376373]\n",
      "epoch:38 step:35669 [D loss: 0.016799, acc.: 100.00%] [G loss: 1.729274]\n",
      "epoch:38 step:35670 [D loss: 0.263455, acc.: 90.62%] [G loss: 3.667124]\n",
      "epoch:38 step:35671 [D loss: 0.273672, acc.: 92.19%] [G loss: 4.535576]\n",
      "epoch:38 step:35672 [D loss: 0.703340, acc.: 61.72%] [G loss: 2.279463]\n",
      "epoch:38 step:35673 [D loss: 0.197609, acc.: 94.53%] [G loss: 2.966901]\n",
      "epoch:38 step:35674 [D loss: 0.340693, acc.: 85.94%] [G loss: 3.397163]\n",
      "epoch:38 step:35675 [D loss: 0.242504, acc.: 93.75%] [G loss: 2.735668]\n",
      "epoch:38 step:35676 [D loss: 0.522575, acc.: 66.41%] [G loss: 3.423000]\n",
      "epoch:38 step:35677 [D loss: 0.169054, acc.: 96.88%] [G loss: 1.958867]\n",
      "epoch:38 step:35678 [D loss: 0.072769, acc.: 97.66%] [G loss: 2.482275]\n",
      "epoch:38 step:35679 [D loss: 0.005436, acc.: 100.00%] [G loss: 3.876494]\n",
      "epoch:38 step:35680 [D loss: 0.151861, acc.: 96.88%] [G loss: 2.912731]\n",
      "epoch:38 step:35681 [D loss: 0.055148, acc.: 100.00%] [G loss: 2.505791]\n",
      "epoch:38 step:35682 [D loss: 0.389719, acc.: 84.38%] [G loss: 2.347864]\n",
      "epoch:38 step:35683 [D loss: 0.448093, acc.: 77.34%] [G loss: 1.070974]\n",
      "epoch:38 step:35684 [D loss: 0.023890, acc.: 100.00%] [G loss: 4.675105]\n",
      "epoch:38 step:35685 [D loss: 0.056936, acc.: 99.22%] [G loss: 3.644369]\n",
      "epoch:38 step:35686 [D loss: 0.050439, acc.: 100.00%] [G loss: 5.654390]\n",
      "epoch:38 step:35687 [D loss: 0.088389, acc.: 97.66%] [G loss: 3.902539]\n",
      "epoch:38 step:35688 [D loss: 0.130948, acc.: 99.22%] [G loss: 3.234354]\n",
      "epoch:38 step:35689 [D loss: 0.287766, acc.: 85.16%] [G loss: 1.732450]\n",
      "epoch:38 step:35690 [D loss: 0.282016, acc.: 82.81%] [G loss: 3.028101]\n",
      "epoch:38 step:35691 [D loss: 0.132187, acc.: 96.88%] [G loss: 0.628905]\n",
      "epoch:38 step:35692 [D loss: 0.032032, acc.: 100.00%] [G loss: 4.574864]\n",
      "epoch:38 step:35693 [D loss: 0.180464, acc.: 98.44%] [G loss: 2.466022]\n",
      "epoch:38 step:35694 [D loss: 0.203928, acc.: 95.31%] [G loss: 5.653868]\n",
      "epoch:38 step:35695 [D loss: 0.229897, acc.: 94.53%] [G loss: 7.614189]\n",
      "epoch:38 step:35696 [D loss: 0.218924, acc.: 91.41%] [G loss: 1.474841]\n",
      "epoch:38 step:35697 [D loss: 0.182142, acc.: 92.97%] [G loss: 1.432589]\n",
      "epoch:38 step:35698 [D loss: 0.039330, acc.: 100.00%] [G loss: 5.850035]\n",
      "epoch:38 step:35699 [D loss: 0.149388, acc.: 96.09%] [G loss: 1.647462]\n",
      "epoch:38 step:35700 [D loss: 1.149840, acc.: 55.47%] [G loss: 2.499123]\n",
      "epoch:38 step:35701 [D loss: 0.084218, acc.: 99.22%] [G loss: 2.260547]\n",
      "epoch:38 step:35702 [D loss: 0.322751, acc.: 90.62%] [G loss: 3.609956]\n",
      "epoch:38 step:35703 [D loss: 1.007883, acc.: 55.47%] [G loss: 2.633270]\n",
      "epoch:38 step:35704 [D loss: 0.043892, acc.: 100.00%] [G loss: 6.103388]\n",
      "epoch:38 step:35705 [D loss: 0.787419, acc.: 56.25%] [G loss: 5.163997]\n",
      "epoch:38 step:35706 [D loss: 0.390369, acc.: 76.56%] [G loss: 2.420345]\n",
      "epoch:38 step:35707 [D loss: 0.351930, acc.: 89.06%] [G loss: 0.924382]\n",
      "epoch:38 step:35708 [D loss: 0.080549, acc.: 98.44%] [G loss: 1.767206]\n",
      "epoch:38 step:35709 [D loss: 0.026272, acc.: 100.00%] [G loss: 1.613287]\n",
      "epoch:38 step:35710 [D loss: 0.077700, acc.: 100.00%] [G loss: 2.867227]\n",
      "epoch:38 step:35711 [D loss: 0.035945, acc.: 100.00%] [G loss: 4.874195]\n",
      "epoch:38 step:35712 [D loss: 0.435096, acc.: 78.91%] [G loss: 0.830161]\n",
      "epoch:38 step:35713 [D loss: 0.209511, acc.: 92.97%] [G loss: 1.162685]\n",
      "epoch:38 step:35714 [D loss: 0.188936, acc.: 96.09%] [G loss: 2.507955]\n",
      "epoch:38 step:35715 [D loss: 0.140894, acc.: 99.22%] [G loss: 3.275025]\n",
      "epoch:38 step:35716 [D loss: 0.163629, acc.: 94.53%] [G loss: 6.037855]\n",
      "epoch:38 step:35717 [D loss: 0.103704, acc.: 99.22%] [G loss: 6.030777]\n",
      "epoch:38 step:35718 [D loss: 0.056990, acc.: 100.00%] [G loss: 2.016644]\n",
      "epoch:38 step:35719 [D loss: 0.238763, acc.: 88.28%] [G loss: 7.546897]\n",
      "epoch:38 step:35720 [D loss: 0.190460, acc.: 96.88%] [G loss: 1.819690]\n",
      "epoch:38 step:35721 [D loss: 0.339447, acc.: 83.59%] [G loss: 5.857882]\n",
      "epoch:38 step:35722 [D loss: 0.103624, acc.: 97.66%] [G loss: 2.279222]\n",
      "epoch:38 step:35723 [D loss: 0.305271, acc.: 87.50%] [G loss: 5.156194]\n",
      "epoch:38 step:35724 [D loss: 0.046031, acc.: 100.00%] [G loss: 4.671163]\n",
      "epoch:38 step:35725 [D loss: 0.071011, acc.: 98.44%] [G loss: 5.150134]\n",
      "epoch:38 step:35726 [D loss: 0.025008, acc.: 99.22%] [G loss: 3.562014]\n",
      "epoch:38 step:35727 [D loss: 0.400793, acc.: 81.25%] [G loss: 3.930254]\n",
      "epoch:38 step:35728 [D loss: 0.522101, acc.: 72.66%] [G loss: 0.213841]\n",
      "epoch:38 step:35729 [D loss: 0.266114, acc.: 89.06%] [G loss: 4.565770]\n",
      "epoch:38 step:35730 [D loss: 0.167394, acc.: 96.88%] [G loss: 6.919363]\n",
      "epoch:38 step:35731 [D loss: 0.838720, acc.: 49.22%] [G loss: 6.572210]\n",
      "epoch:38 step:35732 [D loss: 0.048727, acc.: 99.22%] [G loss: 3.520571]\n",
      "epoch:38 step:35733 [D loss: 0.032754, acc.: 99.22%] [G loss: 5.628219]\n",
      "epoch:38 step:35734 [D loss: 0.080999, acc.: 99.22%] [G loss: 0.838594]\n",
      "epoch:38 step:35735 [D loss: 0.041076, acc.: 99.22%] [G loss: 2.478726]\n",
      "epoch:38 step:35736 [D loss: 0.006652, acc.: 100.00%] [G loss: 4.183632]\n",
      "epoch:38 step:35737 [D loss: 0.713702, acc.: 64.06%] [G loss: 1.383130]\n",
      "epoch:38 step:35738 [D loss: 0.132090, acc.: 96.88%] [G loss: 0.747746]\n",
      "epoch:38 step:35739 [D loss: 1.083551, acc.: 50.00%] [G loss: 1.110814]\n",
      "epoch:38 step:35740 [D loss: 0.372294, acc.: 78.12%] [G loss: 2.203115]\n",
      "epoch:38 step:35741 [D loss: 0.241205, acc.: 87.50%] [G loss: 4.857228]\n",
      "epoch:38 step:35742 [D loss: 0.014387, acc.: 100.00%] [G loss: 4.676549]\n",
      "epoch:38 step:35743 [D loss: 0.030167, acc.: 99.22%] [G loss: 2.423667]\n",
      "epoch:38 step:35744 [D loss: 0.586546, acc.: 64.06%] [G loss: 3.922189]\n",
      "epoch:38 step:35745 [D loss: 1.030462, acc.: 53.12%] [G loss: 2.337385]\n",
      "epoch:38 step:35746 [D loss: 0.056275, acc.: 98.44%] [G loss: 2.788741]\n",
      "epoch:38 step:35747 [D loss: 0.610322, acc.: 69.53%] [G loss: 4.961553]\n",
      "epoch:38 step:35748 [D loss: 0.162953, acc.: 96.09%] [G loss: 3.064373]\n",
      "epoch:38 step:35749 [D loss: 0.057573, acc.: 99.22%] [G loss: 4.089523]\n",
      "epoch:38 step:35750 [D loss: 0.330492, acc.: 84.38%] [G loss: 1.960393]\n",
      "epoch:38 step:35751 [D loss: 0.158196, acc.: 96.09%] [G loss: 3.082287]\n",
      "epoch:38 step:35752 [D loss: 0.043285, acc.: 100.00%] [G loss: 0.928056]\n",
      "epoch:38 step:35753 [D loss: 0.372559, acc.: 87.50%] [G loss: 2.724868]\n",
      "epoch:38 step:35754 [D loss: 0.069629, acc.: 100.00%] [G loss: 3.089588]\n",
      "epoch:38 step:35755 [D loss: 0.134448, acc.: 95.31%] [G loss: 3.847214]\n",
      "epoch:38 step:35756 [D loss: 0.005197, acc.: 100.00%] [G loss: 5.181679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:35757 [D loss: 0.173754, acc.: 95.31%] [G loss: 9.110140]\n",
      "epoch:38 step:35758 [D loss: 0.622838, acc.: 64.06%] [G loss: 4.027579]\n",
      "epoch:38 step:35759 [D loss: 0.489057, acc.: 76.56%] [G loss: 4.154244]\n",
      "epoch:38 step:35760 [D loss: 0.076570, acc.: 98.44%] [G loss: 0.831868]\n",
      "epoch:38 step:35761 [D loss: 0.045723, acc.: 99.22%] [G loss: 3.434985]\n",
      "epoch:38 step:35762 [D loss: 0.157448, acc.: 98.44%] [G loss: 3.722836]\n",
      "epoch:38 step:35763 [D loss: 0.084882, acc.: 100.00%] [G loss: 2.174058]\n",
      "epoch:38 step:35764 [D loss: 0.056741, acc.: 99.22%] [G loss: 6.931284]\n",
      "epoch:38 step:35765 [D loss: 0.019095, acc.: 100.00%] [G loss: 5.005054]\n",
      "epoch:38 step:35766 [D loss: 0.526972, acc.: 68.75%] [G loss: 1.704532]\n",
      "epoch:38 step:35767 [D loss: 1.463846, acc.: 53.91%] [G loss: 2.397616]\n",
      "epoch:38 step:35768 [D loss: 0.078632, acc.: 98.44%] [G loss: 4.383904]\n",
      "epoch:38 step:35769 [D loss: 0.039593, acc.: 98.44%] [G loss: 3.709929]\n",
      "epoch:38 step:35770 [D loss: 0.088490, acc.: 98.44%] [G loss: 1.953545]\n",
      "epoch:38 step:35771 [D loss: 0.779030, acc.: 55.47%] [G loss: 6.316377]\n",
      "epoch:38 step:35772 [D loss: 0.420198, acc.: 79.69%] [G loss: 5.525955]\n",
      "epoch:38 step:35773 [D loss: 0.026402, acc.: 100.00%] [G loss: 3.560566]\n",
      "epoch:38 step:35774 [D loss: 0.112523, acc.: 98.44%] [G loss: 4.202777]\n",
      "epoch:38 step:35775 [D loss: 0.043112, acc.: 100.00%] [G loss: 3.756337]\n",
      "epoch:38 step:35776 [D loss: 0.090042, acc.: 98.44%] [G loss: 6.769414]\n",
      "epoch:38 step:35777 [D loss: 0.933442, acc.: 42.97%] [G loss: 1.350828]\n",
      "epoch:38 step:35778 [D loss: 0.509837, acc.: 66.41%] [G loss: 4.780276]\n",
      "epoch:38 step:35779 [D loss: 0.405956, acc.: 75.78%] [G loss: 2.292503]\n",
      "epoch:38 step:35780 [D loss: 0.111415, acc.: 98.44%] [G loss: 2.346687]\n",
      "epoch:38 step:35781 [D loss: 0.093482, acc.: 98.44%] [G loss: 1.207840]\n",
      "epoch:38 step:35782 [D loss: 0.313877, acc.: 80.47%] [G loss: 2.930402]\n",
      "epoch:38 step:35783 [D loss: 0.097862, acc.: 96.88%] [G loss: 3.772420]\n",
      "epoch:38 step:35784 [D loss: 0.149731, acc.: 97.66%] [G loss: 2.506462]\n",
      "epoch:38 step:35785 [D loss: 0.254598, acc.: 86.72%] [G loss: 3.439520]\n",
      "epoch:38 step:35786 [D loss: 0.132425, acc.: 98.44%] [G loss: 2.218362]\n",
      "epoch:38 step:35787 [D loss: 0.323676, acc.: 91.41%] [G loss: 4.286079]\n",
      "epoch:38 step:35788 [D loss: 0.152000, acc.: 98.44%] [G loss: 6.410094]\n",
      "epoch:38 step:35789 [D loss: 0.329576, acc.: 90.62%] [G loss: 5.186587]\n",
      "epoch:38 step:35790 [D loss: 0.479170, acc.: 75.78%] [G loss: 4.161294]\n",
      "epoch:38 step:35791 [D loss: 0.353206, acc.: 87.50%] [G loss: 8.230990]\n",
      "epoch:38 step:35792 [D loss: 0.094825, acc.: 98.44%] [G loss: 2.129867]\n",
      "epoch:38 step:35793 [D loss: 0.021070, acc.: 100.00%] [G loss: 3.972774]\n",
      "epoch:38 step:35794 [D loss: 0.688503, acc.: 61.72%] [G loss: 5.012329]\n",
      "epoch:38 step:35795 [D loss: 0.486952, acc.: 74.22%] [G loss: 3.779387]\n",
      "epoch:38 step:35796 [D loss: 0.441264, acc.: 76.56%] [G loss: 4.219971]\n",
      "epoch:38 step:35797 [D loss: 0.108674, acc.: 99.22%] [G loss: 2.394832]\n",
      "epoch:38 step:35798 [D loss: 0.695640, acc.: 57.03%] [G loss: 3.864833]\n",
      "epoch:38 step:35799 [D loss: 0.504452, acc.: 81.25%] [G loss: 2.879264]\n",
      "epoch:38 step:35800 [D loss: 1.256599, acc.: 53.91%] [G loss: 1.650402]\n",
      "epoch:38 step:35801 [D loss: 0.415613, acc.: 71.09%] [G loss: 1.286572]\n",
      "epoch:38 step:35802 [D loss: 0.150308, acc.: 97.66%] [G loss: 1.495654]\n",
      "epoch:38 step:35803 [D loss: 0.435963, acc.: 70.31%] [G loss: 4.081113]\n",
      "epoch:38 step:35804 [D loss: 0.059196, acc.: 99.22%] [G loss: 3.642295]\n",
      "epoch:38 step:35805 [D loss: 0.018064, acc.: 100.00%] [G loss: 1.362964]\n",
      "epoch:38 step:35806 [D loss: 0.010337, acc.: 100.00%] [G loss: 3.541290]\n",
      "epoch:38 step:35807 [D loss: 0.030078, acc.: 100.00%] [G loss: 4.955765]\n",
      "epoch:38 step:35808 [D loss: 0.159009, acc.: 98.44%] [G loss: 3.583138]\n",
      "epoch:38 step:35809 [D loss: 0.227553, acc.: 89.84%] [G loss: 1.592379]\n",
      "epoch:38 step:35810 [D loss: 0.017117, acc.: 100.00%] [G loss: 4.860903]\n",
      "epoch:38 step:35811 [D loss: 0.107974, acc.: 98.44%] [G loss: 3.239311]\n",
      "epoch:38 step:35812 [D loss: 0.933053, acc.: 48.44%] [G loss: 4.144470]\n",
      "epoch:38 step:35813 [D loss: 0.078213, acc.: 99.22%] [G loss: 3.518748]\n",
      "epoch:38 step:35814 [D loss: 0.179967, acc.: 96.88%] [G loss: 3.486229]\n",
      "epoch:38 step:35815 [D loss: 0.065435, acc.: 100.00%] [G loss: 1.645378]\n",
      "epoch:38 step:35816 [D loss: 0.033636, acc.: 99.22%] [G loss: 2.569881]\n",
      "epoch:38 step:35817 [D loss: 0.350358, acc.: 83.59%] [G loss: 2.446548]\n",
      "epoch:38 step:35818 [D loss: 0.038077, acc.: 100.00%] [G loss: 4.334913]\n",
      "epoch:38 step:35819 [D loss: 0.147991, acc.: 95.31%] [G loss: 3.515218]\n",
      "epoch:38 step:35820 [D loss: 0.153247, acc.: 96.09%] [G loss: 3.840143]\n",
      "epoch:38 step:35821 [D loss: 0.261622, acc.: 91.41%] [G loss: 3.627989]\n",
      "epoch:38 step:35822 [D loss: 0.361776, acc.: 78.12%] [G loss: 3.690350]\n",
      "epoch:38 step:35823 [D loss: 0.189502, acc.: 93.75%] [G loss: 3.954715]\n",
      "epoch:38 step:35824 [D loss: 0.346810, acc.: 82.03%] [G loss: 5.331499]\n",
      "epoch:38 step:35825 [D loss: 0.159337, acc.: 96.88%] [G loss: 7.469378]\n",
      "epoch:38 step:35826 [D loss: 0.141942, acc.: 97.66%] [G loss: 1.560003]\n",
      "epoch:38 step:35827 [D loss: 0.101627, acc.: 98.44%] [G loss: 4.896735]\n",
      "epoch:38 step:35828 [D loss: 0.661870, acc.: 67.97%] [G loss: 2.754931]\n",
      "epoch:38 step:35829 [D loss: 1.042585, acc.: 58.59%] [G loss: 2.731715]\n",
      "epoch:38 step:35830 [D loss: 2.591425, acc.: 25.78%] [G loss: 3.740273]\n",
      "epoch:38 step:35831 [D loss: 0.024054, acc.: 100.00%] [G loss: 10.506688]\n",
      "epoch:38 step:35832 [D loss: 0.386313, acc.: 82.03%] [G loss: 2.117474]\n",
      "epoch:38 step:35833 [D loss: 0.324354, acc.: 82.81%] [G loss: 5.123556]\n",
      "epoch:38 step:35834 [D loss: 0.302323, acc.: 82.81%] [G loss: 4.970191]\n",
      "epoch:38 step:35835 [D loss: 0.369934, acc.: 82.81%] [G loss: 1.633685]\n",
      "epoch:38 step:35836 [D loss: 0.141014, acc.: 96.09%] [G loss: 6.132848]\n",
      "epoch:38 step:35837 [D loss: 1.289463, acc.: 51.56%] [G loss: 5.594948]\n",
      "epoch:38 step:35838 [D loss: 0.435576, acc.: 74.22%] [G loss: 4.603367]\n",
      "epoch:38 step:35839 [D loss: 0.128642, acc.: 96.09%] [G loss: 5.641346]\n",
      "epoch:38 step:35840 [D loss: 0.204200, acc.: 92.97%] [G loss: 5.871059]\n",
      "epoch:38 step:35841 [D loss: 0.216063, acc.: 92.97%] [G loss: 8.134117]\n",
      "epoch:38 step:35842 [D loss: 0.104570, acc.: 96.88%] [G loss: 3.945398]\n",
      "epoch:38 step:35843 [D loss: 0.540494, acc.: 71.09%] [G loss: 5.629161]\n",
      "epoch:38 step:35844 [D loss: 0.543827, acc.: 67.19%] [G loss: 5.364357]\n",
      "epoch:38 step:35845 [D loss: 0.260405, acc.: 90.62%] [G loss: 2.953285]\n",
      "epoch:38 step:35846 [D loss: 0.101887, acc.: 99.22%] [G loss: 0.414345]\n",
      "epoch:38 step:35847 [D loss: 0.316304, acc.: 82.03%] [G loss: 3.520495]\n",
      "epoch:38 step:35848 [D loss: 0.220757, acc.: 92.19%] [G loss: 1.372587]\n",
      "epoch:38 step:35849 [D loss: 0.740019, acc.: 59.38%] [G loss: 3.898878]\n",
      "epoch:38 step:35850 [D loss: 0.194033, acc.: 93.75%] [G loss: 1.661011]\n",
      "epoch:38 step:35851 [D loss: 0.084552, acc.: 99.22%] [G loss: 2.974355]\n",
      "epoch:38 step:35852 [D loss: 0.122986, acc.: 98.44%] [G loss: 5.056129]\n",
      "epoch:38 step:35853 [D loss: 1.071695, acc.: 35.16%] [G loss: 3.146823]\n",
      "epoch:38 step:35854 [D loss: 0.074504, acc.: 99.22%] [G loss: 3.395764]\n",
      "epoch:38 step:35855 [D loss: 0.043419, acc.: 100.00%] [G loss: 1.814329]\n",
      "epoch:38 step:35856 [D loss: 0.059584, acc.: 99.22%] [G loss: 1.817069]\n",
      "epoch:38 step:35857 [D loss: 0.086708, acc.: 98.44%] [G loss: 5.266102]\n",
      "epoch:38 step:35858 [D loss: 0.073438, acc.: 99.22%] [G loss: 2.317062]\n",
      "epoch:38 step:35859 [D loss: 0.084453, acc.: 97.66%] [G loss: 3.061989]\n",
      "epoch:38 step:35860 [D loss: 0.066499, acc.: 100.00%] [G loss: 4.064388]\n",
      "epoch:38 step:35861 [D loss: 0.569009, acc.: 64.84%] [G loss: 4.133790]\n",
      "epoch:38 step:35862 [D loss: 0.116158, acc.: 96.88%] [G loss: 4.610883]\n",
      "epoch:38 step:35863 [D loss: 0.374176, acc.: 78.91%] [G loss: 2.217855]\n",
      "epoch:38 step:35864 [D loss: 0.040420, acc.: 100.00%] [G loss: 4.025542]\n",
      "epoch:38 step:35865 [D loss: 0.106278, acc.: 99.22%] [G loss: 5.123616]\n",
      "epoch:38 step:35866 [D loss: 0.747772, acc.: 55.47%] [G loss: 2.354072]\n",
      "epoch:38 step:35867 [D loss: 0.402590, acc.: 78.91%] [G loss: 4.123122]\n",
      "epoch:38 step:35868 [D loss: 0.105725, acc.: 97.66%] [G loss: 2.837743]\n",
      "epoch:38 step:35869 [D loss: 0.345050, acc.: 86.72%] [G loss: 1.312249]\n",
      "epoch:38 step:35870 [D loss: 0.136213, acc.: 96.09%] [G loss: 2.028423]\n",
      "epoch:38 step:35871 [D loss: 0.175273, acc.: 96.88%] [G loss: 1.887568]\n",
      "epoch:38 step:35872 [D loss: 0.273525, acc.: 90.62%] [G loss: 3.328054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:35873 [D loss: 0.040800, acc.: 100.00%] [G loss: 3.554687]\n",
      "epoch:38 step:35874 [D loss: 0.264944, acc.: 89.84%] [G loss: 2.969985]\n",
      "epoch:38 step:35875 [D loss: 0.397227, acc.: 75.78%] [G loss: 3.123051]\n",
      "epoch:38 step:35876 [D loss: 0.150132, acc.: 94.53%] [G loss: 1.517546]\n",
      "epoch:38 step:35877 [D loss: 0.241086, acc.: 91.41%] [G loss: 6.212176]\n",
      "epoch:38 step:35878 [D loss: 0.058286, acc.: 99.22%] [G loss: 4.034688]\n",
      "epoch:38 step:35879 [D loss: 0.140633, acc.: 95.31%] [G loss: 4.324792]\n",
      "epoch:38 step:35880 [D loss: 0.113912, acc.: 97.66%] [G loss: 2.454450]\n",
      "epoch:38 step:35881 [D loss: 0.060683, acc.: 99.22%] [G loss: 4.866803]\n",
      "epoch:38 step:35882 [D loss: 0.877113, acc.: 51.56%] [G loss: 3.908491]\n",
      "epoch:38 step:35883 [D loss: 0.063104, acc.: 99.22%] [G loss: 5.361193]\n",
      "epoch:38 step:35884 [D loss: 1.718243, acc.: 30.47%] [G loss: 5.706203]\n",
      "epoch:38 step:35885 [D loss: 0.331998, acc.: 82.81%] [G loss: 5.462115]\n",
      "epoch:38 step:35886 [D loss: 0.201983, acc.: 92.97%] [G loss: 5.311175]\n",
      "epoch:38 step:35887 [D loss: 0.165323, acc.: 96.09%] [G loss: 1.498980]\n",
      "epoch:38 step:35888 [D loss: 0.227686, acc.: 92.19%] [G loss: 2.551753]\n",
      "epoch:38 step:35889 [D loss: 0.360484, acc.: 89.06%] [G loss: 5.634106]\n",
      "epoch:38 step:35890 [D loss: 0.189734, acc.: 96.88%] [G loss: 1.926365]\n",
      "epoch:38 step:35891 [D loss: 0.232493, acc.: 89.06%] [G loss: 3.469055]\n",
      "epoch:38 step:35892 [D loss: 0.091464, acc.: 96.88%] [G loss: 4.910545]\n",
      "epoch:38 step:35893 [D loss: 0.093407, acc.: 98.44%] [G loss: 4.905196]\n",
      "epoch:38 step:35894 [D loss: 0.090673, acc.: 100.00%] [G loss: 3.648031]\n",
      "epoch:38 step:35895 [D loss: 0.207199, acc.: 96.09%] [G loss: 1.732631]\n",
      "epoch:38 step:35896 [D loss: 0.232848, acc.: 96.09%] [G loss: 4.240924]\n",
      "epoch:38 step:35897 [D loss: 0.085256, acc.: 100.00%] [G loss: 3.645478]\n",
      "epoch:38 step:35898 [D loss: 0.326975, acc.: 87.50%] [G loss: 3.664940]\n",
      "epoch:38 step:35899 [D loss: 0.506632, acc.: 75.00%] [G loss: 3.168113]\n",
      "epoch:38 step:35900 [D loss: 0.075964, acc.: 100.00%] [G loss: 1.725973]\n",
      "epoch:38 step:35901 [D loss: 0.176893, acc.: 96.88%] [G loss: 2.071169]\n",
      "epoch:38 step:35902 [D loss: 0.382035, acc.: 86.72%] [G loss: 4.227189]\n",
      "epoch:38 step:35903 [D loss: 0.045867, acc.: 99.22%] [G loss: 1.295932]\n",
      "epoch:38 step:35904 [D loss: 0.151382, acc.: 96.09%] [G loss: 5.393677]\n",
      "epoch:38 step:35905 [D loss: 0.421258, acc.: 80.47%] [G loss: 1.149915]\n",
      "epoch:38 step:35906 [D loss: 0.211209, acc.: 96.09%] [G loss: 2.471659]\n",
      "epoch:38 step:35907 [D loss: 0.074741, acc.: 100.00%] [G loss: 4.043169]\n",
      "epoch:38 step:35908 [D loss: 0.072539, acc.: 99.22%] [G loss: 2.357948]\n",
      "epoch:38 step:35909 [D loss: 0.394209, acc.: 76.56%] [G loss: 3.522576]\n",
      "epoch:38 step:35910 [D loss: 0.088962, acc.: 100.00%] [G loss: 5.002489]\n",
      "epoch:38 step:35911 [D loss: 0.061450, acc.: 100.00%] [G loss: 1.674241]\n",
      "epoch:38 step:35912 [D loss: 0.057617, acc.: 99.22%] [G loss: 5.086972]\n",
      "epoch:38 step:35913 [D loss: 0.115994, acc.: 99.22%] [G loss: 4.025834]\n",
      "epoch:38 step:35914 [D loss: 0.152416, acc.: 94.53%] [G loss: 1.899290]\n",
      "epoch:38 step:35915 [D loss: 0.156094, acc.: 99.22%] [G loss: 1.194428]\n",
      "epoch:38 step:35916 [D loss: 0.094452, acc.: 99.22%] [G loss: 1.381938]\n",
      "epoch:38 step:35917 [D loss: 0.345481, acc.: 80.47%] [G loss: 5.090378]\n",
      "epoch:38 step:35918 [D loss: 0.834471, acc.: 49.22%] [G loss: 1.824401]\n",
      "epoch:38 step:35919 [D loss: 0.200321, acc.: 92.97%] [G loss: 3.985802]\n",
      "epoch:38 step:35920 [D loss: 1.100089, acc.: 53.91%] [G loss: 2.751475]\n",
      "epoch:38 step:35921 [D loss: 0.008856, acc.: 100.00%] [G loss: 1.328704]\n",
      "epoch:38 step:35922 [D loss: 0.070434, acc.: 99.22%] [G loss: 3.904071]\n",
      "epoch:38 step:35923 [D loss: 0.055826, acc.: 100.00%] [G loss: 2.733099]\n",
      "epoch:38 step:35924 [D loss: 0.055260, acc.: 100.00%] [G loss: 3.303554]\n",
      "epoch:38 step:35925 [D loss: 1.206800, acc.: 52.34%] [G loss: 6.764881]\n",
      "epoch:38 step:35926 [D loss: 0.030912, acc.: 100.00%] [G loss: 5.060562]\n",
      "epoch:38 step:35927 [D loss: 0.789381, acc.: 55.47%] [G loss: 5.100033]\n",
      "epoch:38 step:35928 [D loss: 0.472765, acc.: 75.78%] [G loss: 6.222192]\n",
      "epoch:38 step:35929 [D loss: 0.042772, acc.: 100.00%] [G loss: 4.811282]\n",
      "epoch:38 step:35930 [D loss: 0.164397, acc.: 98.44%] [G loss: 4.249434]\n",
      "epoch:38 step:35931 [D loss: 0.138932, acc.: 96.88%] [G loss: 4.049137]\n",
      "epoch:38 step:35932 [D loss: 0.017737, acc.: 100.00%] [G loss: 3.158630]\n",
      "epoch:38 step:35933 [D loss: 1.443455, acc.: 53.12%] [G loss: 1.775714]\n",
      "epoch:38 step:35934 [D loss: 0.028130, acc.: 99.22%] [G loss: 6.536954]\n",
      "epoch:38 step:35935 [D loss: 0.278055, acc.: 94.53%] [G loss: 4.562992]\n",
      "epoch:38 step:35936 [D loss: 0.147795, acc.: 97.66%] [G loss: 8.003700]\n",
      "epoch:38 step:35937 [D loss: 0.085724, acc.: 100.00%] [G loss: 2.830340]\n",
      "epoch:38 step:35938 [D loss: 0.315525, acc.: 82.81%] [G loss: 4.989586]\n",
      "epoch:38 step:35939 [D loss: 0.610981, acc.: 70.31%] [G loss: 3.175723]\n",
      "epoch:38 step:35940 [D loss: 0.393644, acc.: 76.56%] [G loss: 2.765581]\n",
      "epoch:38 step:35941 [D loss: 0.033363, acc.: 100.00%] [G loss: 3.113338]\n",
      "epoch:38 step:35942 [D loss: 0.050486, acc.: 100.00%] [G loss: 2.974489]\n",
      "epoch:38 step:35943 [D loss: 0.182378, acc.: 97.66%] [G loss: 3.092922]\n",
      "epoch:38 step:35944 [D loss: 0.286013, acc.: 91.41%] [G loss: 4.226706]\n",
      "epoch:38 step:35945 [D loss: 0.142539, acc.: 97.66%] [G loss: 0.169444]\n",
      "epoch:38 step:35946 [D loss: 1.315545, acc.: 43.75%] [G loss: 0.847028]\n",
      "epoch:38 step:35947 [D loss: 1.210329, acc.: 50.78%] [G loss: 3.652297]\n",
      "epoch:38 step:35948 [D loss: 0.049625, acc.: 99.22%] [G loss: 2.972155]\n",
      "epoch:38 step:35949 [D loss: 0.065252, acc.: 98.44%] [G loss: 3.926628]\n",
      "epoch:38 step:35950 [D loss: 0.029126, acc.: 100.00%] [G loss: 4.351960]\n",
      "epoch:38 step:35951 [D loss: 0.238484, acc.: 89.06%] [G loss: 3.648232]\n",
      "epoch:38 step:35952 [D loss: 0.635181, acc.: 66.41%] [G loss: 3.326865]\n",
      "epoch:38 step:35953 [D loss: 0.100829, acc.: 98.44%] [G loss: 3.627988]\n",
      "epoch:38 step:35954 [D loss: 0.079901, acc.: 99.22%] [G loss: 2.913546]\n",
      "epoch:38 step:35955 [D loss: 0.110136, acc.: 98.44%] [G loss: 5.054271]\n",
      "epoch:38 step:35956 [D loss: 0.019878, acc.: 100.00%] [G loss: 3.854131]\n",
      "epoch:38 step:35957 [D loss: 0.333680, acc.: 85.16%] [G loss: 2.818515]\n",
      "epoch:38 step:35958 [D loss: 0.370826, acc.: 79.69%] [G loss: 4.850873]\n",
      "epoch:38 step:35959 [D loss: 0.005270, acc.: 100.00%] [G loss: 2.662564]\n",
      "epoch:38 step:35960 [D loss: 0.383171, acc.: 81.25%] [G loss: 0.816681]\n",
      "epoch:38 step:35961 [D loss: 0.499528, acc.: 76.56%] [G loss: 6.120615]\n",
      "epoch:38 step:35962 [D loss: 0.096523, acc.: 98.44%] [G loss: 4.515903]\n",
      "epoch:38 step:35963 [D loss: 0.060135, acc.: 99.22%] [G loss: 4.890540]\n",
      "epoch:38 step:35964 [D loss: 0.408436, acc.: 76.56%] [G loss: 2.844071]\n",
      "epoch:38 step:35965 [D loss: 0.199868, acc.: 92.19%] [G loss: 3.957000]\n",
      "epoch:38 step:35966 [D loss: 0.756756, acc.: 59.38%] [G loss: 4.060388]\n",
      "epoch:38 step:35967 [D loss: 0.103010, acc.: 98.44%] [G loss: 3.839643]\n",
      "epoch:38 step:35968 [D loss: 0.023520, acc.: 100.00%] [G loss: 5.618876]\n",
      "epoch:38 step:35969 [D loss: 0.107128, acc.: 97.66%] [G loss: 2.936507]\n",
      "epoch:38 step:35970 [D loss: 0.030518, acc.: 99.22%] [G loss: 6.034049]\n",
      "epoch:38 step:35971 [D loss: 0.024594, acc.: 100.00%] [G loss: 6.627552]\n",
      "epoch:38 step:35972 [D loss: 0.033968, acc.: 100.00%] [G loss: 4.014644]\n",
      "epoch:38 step:35973 [D loss: 0.230640, acc.: 94.53%] [G loss: 4.267193]\n",
      "epoch:38 step:35974 [D loss: 0.619299, acc.: 64.06%] [G loss: 4.938072]\n",
      "epoch:38 step:35975 [D loss: 0.016276, acc.: 100.00%] [G loss: 4.131783]\n",
      "epoch:38 step:35976 [D loss: 0.023145, acc.: 100.00%] [G loss: 3.059439]\n",
      "epoch:38 step:35977 [D loss: 0.014260, acc.: 100.00%] [G loss: 1.170695]\n",
      "epoch:38 step:35978 [D loss: 0.011733, acc.: 100.00%] [G loss: 2.172151]\n",
      "epoch:38 step:35979 [D loss: 0.018754, acc.: 100.00%] [G loss: 3.656156]\n",
      "epoch:38 step:35980 [D loss: 0.041986, acc.: 100.00%] [G loss: 3.858699]\n",
      "epoch:38 step:35981 [D loss: 0.429718, acc.: 71.09%] [G loss: 2.761056]\n",
      "epoch:38 step:35982 [D loss: 0.471817, acc.: 80.47%] [G loss: 6.824081]\n",
      "epoch:38 step:35983 [D loss: 0.063067, acc.: 99.22%] [G loss: 4.027574]\n",
      "epoch:38 step:35984 [D loss: 0.150044, acc.: 96.09%] [G loss: 5.767300]\n",
      "epoch:38 step:35985 [D loss: 0.896125, acc.: 55.47%] [G loss: 5.189232]\n",
      "epoch:38 step:35986 [D loss: 0.396214, acc.: 82.81%] [G loss: 0.804307]\n",
      "epoch:38 step:35987 [D loss: 0.009862, acc.: 100.00%] [G loss: 2.101507]\n",
      "epoch:38 step:35988 [D loss: 0.085377, acc.: 98.44%] [G loss: 5.343781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:35989 [D loss: 0.012391, acc.: 100.00%] [G loss: 0.535195]\n",
      "epoch:38 step:35990 [D loss: 0.073698, acc.: 99.22%] [G loss: 2.227850]\n",
      "epoch:38 step:35991 [D loss: 0.128349, acc.: 99.22%] [G loss: 4.254846]\n",
      "epoch:38 step:35992 [D loss: 0.432046, acc.: 80.47%] [G loss: 2.899882]\n",
      "epoch:38 step:35993 [D loss: 0.379838, acc.: 76.56%] [G loss: 4.303402]\n",
      "epoch:38 step:35994 [D loss: 0.792373, acc.: 53.91%] [G loss: 4.487653]\n",
      "epoch:38 step:35995 [D loss: 0.214285, acc.: 96.88%] [G loss: 5.215226]\n",
      "epoch:38 step:35996 [D loss: 0.062325, acc.: 98.44%] [G loss: 5.032908]\n",
      "epoch:38 step:35997 [D loss: 0.080706, acc.: 98.44%] [G loss: 3.907111]\n",
      "epoch:38 step:35998 [D loss: 0.079431, acc.: 98.44%] [G loss: 2.833716]\n",
      "epoch:38 step:35999 [D loss: 0.200020, acc.: 92.97%] [G loss: 4.531917]\n",
      "epoch:38 step:36000 [D loss: 0.050371, acc.: 100.00%] [G loss: 4.336016]\n",
      "epoch:38 step:36001 [D loss: 0.012162, acc.: 100.00%] [G loss: 3.835834]\n",
      "epoch:38 step:36002 [D loss: 0.170581, acc.: 95.31%] [G loss: 4.602364]\n",
      "epoch:38 step:36003 [D loss: 0.080607, acc.: 98.44%] [G loss: 5.931320]\n",
      "epoch:38 step:36004 [D loss: 0.028237, acc.: 99.22%] [G loss: 3.858778]\n",
      "epoch:38 step:36005 [D loss: 0.189954, acc.: 95.31%] [G loss: 5.937847]\n",
      "epoch:38 step:36006 [D loss: 0.114516, acc.: 99.22%] [G loss: 5.490748]\n",
      "epoch:38 step:36007 [D loss: 0.315291, acc.: 83.59%] [G loss: 2.802253]\n",
      "epoch:38 step:36008 [D loss: 0.011306, acc.: 100.00%] [G loss: 4.866973]\n",
      "epoch:38 step:36009 [D loss: 0.030199, acc.: 99.22%] [G loss: 0.596532]\n",
      "epoch:38 step:36010 [D loss: 0.153503, acc.: 95.31%] [G loss: 1.778664]\n",
      "epoch:38 step:36011 [D loss: 0.026166, acc.: 100.00%] [G loss: 5.829582]\n",
      "epoch:38 step:36012 [D loss: 0.013082, acc.: 100.00%] [G loss: 6.530698]\n",
      "epoch:38 step:36013 [D loss: 0.054864, acc.: 100.00%] [G loss: 0.559672]\n",
      "epoch:38 step:36014 [D loss: 0.033418, acc.: 100.00%] [G loss: 4.093979]\n",
      "epoch:38 step:36015 [D loss: 0.338822, acc.: 89.84%] [G loss: 5.829208]\n",
      "epoch:38 step:36016 [D loss: 0.020128, acc.: 100.00%] [G loss: 4.813317]\n",
      "epoch:38 step:36017 [D loss: 0.072710, acc.: 99.22%] [G loss: 2.803122]\n",
      "epoch:38 step:36018 [D loss: 0.133897, acc.: 96.88%] [G loss: 4.774784]\n",
      "epoch:38 step:36019 [D loss: 0.243986, acc.: 92.97%] [G loss: 3.502418]\n",
      "epoch:38 step:36020 [D loss: 0.538688, acc.: 76.56%] [G loss: 5.378789]\n",
      "epoch:38 step:36021 [D loss: 0.977905, acc.: 54.69%] [G loss: 2.431640]\n",
      "epoch:38 step:36022 [D loss: 0.050257, acc.: 99.22%] [G loss: 8.145825]\n",
      "epoch:38 step:36023 [D loss: 0.312170, acc.: 87.50%] [G loss: 3.716658]\n",
      "epoch:38 step:36024 [D loss: 0.057647, acc.: 99.22%] [G loss: 2.710682]\n",
      "epoch:38 step:36025 [D loss: 0.033142, acc.: 100.00%] [G loss: 2.076324]\n",
      "epoch:38 step:36026 [D loss: 0.308374, acc.: 85.16%] [G loss: 2.320182]\n",
      "epoch:38 step:36027 [D loss: 0.164987, acc.: 98.44%] [G loss: 4.498756]\n",
      "epoch:38 step:36028 [D loss: 0.173862, acc.: 93.75%] [G loss: 0.767645]\n",
      "epoch:38 step:36029 [D loss: 0.083529, acc.: 100.00%] [G loss: 2.616426]\n",
      "epoch:38 step:36030 [D loss: 0.033277, acc.: 100.00%] [G loss: 2.704039]\n",
      "epoch:38 step:36031 [D loss: 0.096953, acc.: 98.44%] [G loss: 4.659742]\n",
      "epoch:38 step:36032 [D loss: 0.116998, acc.: 96.88%] [G loss: 3.981216]\n",
      "epoch:38 step:36033 [D loss: 0.089312, acc.: 97.66%] [G loss: 0.670942]\n",
      "epoch:38 step:36034 [D loss: 0.062998, acc.: 97.66%] [G loss: 1.806346]\n",
      "epoch:38 step:36035 [D loss: 0.309897, acc.: 87.50%] [G loss: 3.596743]\n",
      "epoch:38 step:36036 [D loss: 0.055102, acc.: 99.22%] [G loss: 2.402660]\n",
      "epoch:38 step:36037 [D loss: 1.145380, acc.: 31.25%] [G loss: 7.122866]\n",
      "epoch:38 step:36038 [D loss: 0.074197, acc.: 99.22%] [G loss: 5.364297]\n",
      "epoch:38 step:36039 [D loss: 0.110988, acc.: 98.44%] [G loss: 5.247590]\n",
      "epoch:38 step:36040 [D loss: 0.133661, acc.: 96.88%] [G loss: 1.961950]\n",
      "epoch:38 step:36041 [D loss: 0.167897, acc.: 93.75%] [G loss: 4.455347]\n",
      "epoch:38 step:36042 [D loss: 0.493981, acc.: 71.09%] [G loss: 2.925403]\n",
      "epoch:38 step:36043 [D loss: 0.193676, acc.: 93.75%] [G loss: 1.747313]\n",
      "epoch:38 step:36044 [D loss: 0.118300, acc.: 96.09%] [G loss: 4.428631]\n",
      "epoch:38 step:36045 [D loss: 0.186534, acc.: 95.31%] [G loss: 4.408046]\n",
      "epoch:38 step:36046 [D loss: 0.130675, acc.: 96.88%] [G loss: 0.504370]\n",
      "epoch:38 step:36047 [D loss: 0.022965, acc.: 100.00%] [G loss: 3.180353]\n",
      "epoch:38 step:36048 [D loss: 0.113258, acc.: 96.88%] [G loss: 2.808752]\n",
      "epoch:38 step:36049 [D loss: 0.026303, acc.: 100.00%] [G loss: 4.569537]\n",
      "epoch:38 step:36050 [D loss: 0.992984, acc.: 56.25%] [G loss: 5.544604]\n",
      "epoch:38 step:36051 [D loss: 0.187844, acc.: 95.31%] [G loss: 5.993078]\n",
      "epoch:38 step:36052 [D loss: 2.049901, acc.: 51.56%] [G loss: 3.776700]\n",
      "epoch:38 step:36053 [D loss: 0.854198, acc.: 57.03%] [G loss: 4.453057]\n",
      "epoch:38 step:36054 [D loss: 0.315797, acc.: 91.41%] [G loss: 0.453912]\n",
      "epoch:38 step:36055 [D loss: 0.190712, acc.: 96.09%] [G loss: 1.249014]\n",
      "epoch:38 step:36056 [D loss: 0.609550, acc.: 64.84%] [G loss: 1.218467]\n",
      "epoch:38 step:36057 [D loss: 1.299778, acc.: 51.56%] [G loss: 2.340239]\n",
      "epoch:38 step:36058 [D loss: 1.126941, acc.: 53.12%] [G loss: 0.774259]\n",
      "epoch:38 step:36059 [D loss: 0.067586, acc.: 100.00%] [G loss: 3.751232]\n",
      "epoch:38 step:36060 [D loss: 0.081938, acc.: 100.00%] [G loss: 2.341335]\n",
      "epoch:38 step:36061 [D loss: 0.326957, acc.: 86.72%] [G loss: 3.126834]\n",
      "epoch:38 step:36062 [D loss: 0.270393, acc.: 86.72%] [G loss: 4.418378]\n",
      "epoch:38 step:36063 [D loss: 0.013841, acc.: 100.00%] [G loss: 3.439035]\n",
      "epoch:38 step:36064 [D loss: 0.204884, acc.: 92.97%] [G loss: 3.986386]\n",
      "epoch:38 step:36065 [D loss: 0.042395, acc.: 99.22%] [G loss: 2.865615]\n",
      "epoch:38 step:36066 [D loss: 0.099319, acc.: 97.66%] [G loss: 3.840259]\n",
      "epoch:38 step:36067 [D loss: 0.720313, acc.: 62.50%] [G loss: 1.518029]\n",
      "epoch:38 step:36068 [D loss: 0.478221, acc.: 68.75%] [G loss: 1.995204]\n",
      "epoch:38 step:36069 [D loss: 0.050162, acc.: 99.22%] [G loss: 5.924040]\n",
      "epoch:38 step:36070 [D loss: 0.038004, acc.: 99.22%] [G loss: 3.875483]\n",
      "epoch:38 step:36071 [D loss: 0.260806, acc.: 95.31%] [G loss: 2.901492]\n",
      "epoch:38 step:36072 [D loss: 0.023242, acc.: 100.00%] [G loss: 3.685429]\n",
      "epoch:38 step:36073 [D loss: 0.040888, acc.: 99.22%] [G loss: 6.357221]\n",
      "epoch:38 step:36074 [D loss: 0.023417, acc.: 100.00%] [G loss: 6.000142]\n",
      "epoch:38 step:36075 [D loss: 0.061401, acc.: 99.22%] [G loss: 3.612663]\n",
      "epoch:38 step:36076 [D loss: 0.091685, acc.: 98.44%] [G loss: 2.204748]\n",
      "epoch:38 step:36077 [D loss: 0.083466, acc.: 98.44%] [G loss: 3.649168]\n",
      "epoch:38 step:36078 [D loss: 0.097203, acc.: 99.22%] [G loss: 4.030036]\n",
      "epoch:38 step:36079 [D loss: 0.130566, acc.: 97.66%] [G loss: 2.513388]\n",
      "epoch:38 step:36080 [D loss: 0.033063, acc.: 100.00%] [G loss: 2.804549]\n",
      "epoch:38 step:36081 [D loss: 0.020998, acc.: 100.00%] [G loss: 5.288204]\n",
      "epoch:38 step:36082 [D loss: 0.055637, acc.: 100.00%] [G loss: 2.883976]\n",
      "epoch:38 step:36083 [D loss: 0.245340, acc.: 92.19%] [G loss: 1.492266]\n",
      "epoch:38 step:36084 [D loss: 0.075685, acc.: 99.22%] [G loss: 1.214717]\n",
      "epoch:38 step:36085 [D loss: 0.170374, acc.: 95.31%] [G loss: 1.099014]\n",
      "epoch:38 step:36086 [D loss: 0.027141, acc.: 100.00%] [G loss: 3.895068]\n",
      "epoch:38 step:36087 [D loss: 1.385603, acc.: 24.22%] [G loss: 1.348884]\n",
      "epoch:38 step:36088 [D loss: 0.260139, acc.: 84.38%] [G loss: 2.546799]\n",
      "epoch:38 step:36089 [D loss: 0.115578, acc.: 99.22%] [G loss: 4.680709]\n",
      "epoch:38 step:36090 [D loss: 0.063847, acc.: 100.00%] [G loss: 2.990324]\n",
      "epoch:38 step:36091 [D loss: 0.053165, acc.: 99.22%] [G loss: 2.951742]\n",
      "epoch:38 step:36092 [D loss: 0.089188, acc.: 100.00%] [G loss: 2.249228]\n",
      "epoch:38 step:36093 [D loss: 0.013729, acc.: 100.00%] [G loss: 2.506812]\n",
      "epoch:38 step:36094 [D loss: 0.764960, acc.: 61.72%] [G loss: 6.567614]\n",
      "epoch:38 step:36095 [D loss: 0.021396, acc.: 99.22%] [G loss: 6.094419]\n",
      "epoch:38 step:36096 [D loss: 0.792512, acc.: 60.94%] [G loss: 5.338115]\n",
      "epoch:38 step:36097 [D loss: 2.493464, acc.: 36.72%] [G loss: 1.044771]\n",
      "epoch:38 step:36098 [D loss: 0.154601, acc.: 98.44%] [G loss: 1.221330]\n",
      "epoch:38 step:36099 [D loss: 0.068546, acc.: 99.22%] [G loss: 1.916866]\n",
      "epoch:38 step:36100 [D loss: 0.050431, acc.: 99.22%] [G loss: 1.937041]\n",
      "epoch:38 step:36101 [D loss: 0.070224, acc.: 99.22%] [G loss: 2.330744]\n",
      "epoch:38 step:36102 [D loss: 0.086819, acc.: 99.22%] [G loss: 2.764933]\n",
      "epoch:38 step:36103 [D loss: 0.464605, acc.: 67.97%] [G loss: 4.252643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:36104 [D loss: 0.055749, acc.: 98.44%] [G loss: 3.415351]\n",
      "epoch:38 step:36105 [D loss: 0.086249, acc.: 100.00%] [G loss: 1.864983]\n",
      "epoch:38 step:36106 [D loss: 0.119431, acc.: 99.22%] [G loss: 5.013246]\n",
      "epoch:38 step:36107 [D loss: 0.720023, acc.: 57.03%] [G loss: 1.855790]\n",
      "epoch:38 step:36108 [D loss: 0.245696, acc.: 85.94%] [G loss: 2.224070]\n",
      "epoch:38 step:36109 [D loss: 0.354224, acc.: 88.28%] [G loss: 1.581182]\n",
      "epoch:38 step:36110 [D loss: 0.525515, acc.: 64.06%] [G loss: 1.383555]\n",
      "epoch:38 step:36111 [D loss: 0.126089, acc.: 96.88%] [G loss: 4.623177]\n",
      "epoch:38 step:36112 [D loss: 0.014689, acc.: 100.00%] [G loss: 4.648451]\n",
      "epoch:38 step:36113 [D loss: 0.170783, acc.: 96.09%] [G loss: 4.469370]\n",
      "epoch:38 step:36114 [D loss: 0.039529, acc.: 100.00%] [G loss: 2.544937]\n",
      "epoch:38 step:36115 [D loss: 0.066701, acc.: 99.22%] [G loss: 0.913556]\n",
      "epoch:38 step:36116 [D loss: 0.174498, acc.: 96.09%] [G loss: 5.531077]\n",
      "epoch:38 step:36117 [D loss: 0.050956, acc.: 99.22%] [G loss: 0.928645]\n",
      "epoch:38 step:36118 [D loss: 0.266315, acc.: 91.41%] [G loss: 3.396274]\n",
      "epoch:38 step:36119 [D loss: 0.090814, acc.: 100.00%] [G loss: 2.253914]\n",
      "epoch:38 step:36120 [D loss: 1.305449, acc.: 24.22%] [G loss: 3.613760]\n",
      "epoch:38 step:36121 [D loss: 0.040466, acc.: 99.22%] [G loss: 3.457439]\n",
      "epoch:38 step:36122 [D loss: 0.277239, acc.: 88.28%] [G loss: 3.557939]\n",
      "epoch:38 step:36123 [D loss: 0.079220, acc.: 100.00%] [G loss: 2.044466]\n",
      "epoch:38 step:36124 [D loss: 0.187480, acc.: 97.66%] [G loss: 1.557492]\n",
      "epoch:38 step:36125 [D loss: 0.239495, acc.: 95.31%] [G loss: 2.685937]\n",
      "epoch:38 step:36126 [D loss: 0.254082, acc.: 89.06%] [G loss: 3.344610]\n",
      "epoch:38 step:36127 [D loss: 0.057325, acc.: 99.22%] [G loss: 7.296389]\n",
      "epoch:38 step:36128 [D loss: 0.369748, acc.: 82.81%] [G loss: 6.453551]\n",
      "epoch:38 step:36129 [D loss: 0.051414, acc.: 100.00%] [G loss: 1.848113]\n",
      "epoch:38 step:36130 [D loss: 0.046178, acc.: 100.00%] [G loss: 1.836737]\n",
      "epoch:38 step:36131 [D loss: 0.140326, acc.: 99.22%] [G loss: 0.861477]\n",
      "epoch:38 step:36132 [D loss: 0.094764, acc.: 99.22%] [G loss: 4.884764]\n",
      "epoch:38 step:36133 [D loss: 0.450801, acc.: 79.69%] [G loss: 4.184097]\n",
      "epoch:38 step:36134 [D loss: 0.290708, acc.: 88.28%] [G loss: 1.091227]\n",
      "epoch:38 step:36135 [D loss: 1.426533, acc.: 46.09%] [G loss: 5.364202]\n",
      "epoch:38 step:36136 [D loss: 0.061518, acc.: 98.44%] [G loss: 3.227721]\n",
      "epoch:38 step:36137 [D loss: 0.325090, acc.: 79.69%] [G loss: 4.326860]\n",
      "epoch:38 step:36138 [D loss: 0.063737, acc.: 99.22%] [G loss: 4.727853]\n",
      "epoch:38 step:36139 [D loss: 0.072765, acc.: 99.22%] [G loss: 4.176049]\n",
      "epoch:38 step:36140 [D loss: 0.061458, acc.: 100.00%] [G loss: 2.298359]\n",
      "epoch:38 step:36141 [D loss: 0.221829, acc.: 95.31%] [G loss: 3.307565]\n",
      "epoch:38 step:36142 [D loss: 0.187725, acc.: 95.31%] [G loss: 4.549495]\n",
      "epoch:38 step:36143 [D loss: 0.058164, acc.: 100.00%] [G loss: 4.725723]\n",
      "epoch:38 step:36144 [D loss: 0.157355, acc.: 95.31%] [G loss: 3.759643]\n",
      "epoch:38 step:36145 [D loss: 0.047480, acc.: 100.00%] [G loss: 3.747698]\n",
      "epoch:38 step:36146 [D loss: 0.360438, acc.: 81.25%] [G loss: 3.230959]\n",
      "epoch:38 step:36147 [D loss: 0.041144, acc.: 99.22%] [G loss: 1.177388]\n",
      "epoch:38 step:36148 [D loss: 1.006127, acc.: 54.69%] [G loss: 3.203105]\n",
      "epoch:38 step:36149 [D loss: 0.421252, acc.: 75.00%] [G loss: 0.651965]\n",
      "epoch:38 step:36150 [D loss: 0.988555, acc.: 55.47%] [G loss: 4.969631]\n",
      "epoch:38 step:36151 [D loss: 0.045264, acc.: 99.22%] [G loss: 5.164071]\n",
      "epoch:38 step:36152 [D loss: 0.074482, acc.: 98.44%] [G loss: 4.686607]\n",
      "epoch:38 step:36153 [D loss: 0.699535, acc.: 64.84%] [G loss: 0.935727]\n",
      "epoch:38 step:36154 [D loss: 0.040477, acc.: 100.00%] [G loss: 4.370087]\n",
      "epoch:38 step:36155 [D loss: 0.073871, acc.: 100.00%] [G loss: 2.768865]\n",
      "epoch:38 step:36156 [D loss: 0.034948, acc.: 100.00%] [G loss: 1.207725]\n",
      "epoch:38 step:36157 [D loss: 0.149954, acc.: 96.88%] [G loss: 2.655040]\n",
      "epoch:38 step:36158 [D loss: 0.284135, acc.: 87.50%] [G loss: 3.453934]\n",
      "epoch:38 step:36159 [D loss: 0.214378, acc.: 93.75%] [G loss: 0.464763]\n",
      "epoch:38 step:36160 [D loss: 0.043285, acc.: 100.00%] [G loss: 0.386449]\n",
      "epoch:38 step:36161 [D loss: 0.584861, acc.: 71.09%] [G loss: 3.488163]\n",
      "epoch:38 step:36162 [D loss: 0.069119, acc.: 99.22%] [G loss: 8.315350]\n",
      "epoch:38 step:36163 [D loss: 0.027486, acc.: 100.00%] [G loss: 4.264847]\n",
      "epoch:38 step:36164 [D loss: 0.081075, acc.: 98.44%] [G loss: 1.417450]\n",
      "epoch:38 step:36165 [D loss: 0.428445, acc.: 83.59%] [G loss: 2.761437]\n",
      "epoch:38 step:36166 [D loss: 0.169771, acc.: 98.44%] [G loss: 3.207839]\n",
      "epoch:38 step:36167 [D loss: 0.081712, acc.: 99.22%] [G loss: 5.266562]\n",
      "epoch:38 step:36168 [D loss: 0.063439, acc.: 98.44%] [G loss: 4.158534]\n",
      "epoch:38 step:36169 [D loss: 0.040613, acc.: 99.22%] [G loss: 3.891167]\n",
      "epoch:38 step:36170 [D loss: 0.073666, acc.: 98.44%] [G loss: 5.236387]\n",
      "epoch:38 step:36171 [D loss: 0.068041, acc.: 100.00%] [G loss: 4.426052]\n",
      "epoch:38 step:36172 [D loss: 0.180520, acc.: 92.97%] [G loss: 3.739795]\n",
      "epoch:38 step:36173 [D loss: 0.548093, acc.: 66.41%] [G loss: 4.357359]\n",
      "epoch:38 step:36174 [D loss: 0.184849, acc.: 93.75%] [G loss: 4.455244]\n",
      "epoch:38 step:36175 [D loss: 0.053116, acc.: 96.88%] [G loss: 3.294378]\n",
      "epoch:38 step:36176 [D loss: 0.065569, acc.: 99.22%] [G loss: 2.686589]\n",
      "epoch:38 step:36177 [D loss: 0.318923, acc.: 85.16%] [G loss: 2.373427]\n",
      "epoch:38 step:36178 [D loss: 0.132619, acc.: 96.88%] [G loss: 2.715698]\n",
      "epoch:38 step:36179 [D loss: 0.126980, acc.: 98.44%] [G loss: 2.971375]\n",
      "epoch:38 step:36180 [D loss: 0.155064, acc.: 98.44%] [G loss: 3.562061]\n",
      "epoch:38 step:36181 [D loss: 0.141170, acc.: 94.53%] [G loss: 4.597873]\n",
      "epoch:38 step:36182 [D loss: 0.154191, acc.: 96.88%] [G loss: 3.974867]\n",
      "epoch:38 step:36183 [D loss: 0.039092, acc.: 100.00%] [G loss: 1.548333]\n",
      "epoch:38 step:36184 [D loss: 0.079709, acc.: 99.22%] [G loss: 5.776060]\n",
      "epoch:38 step:36185 [D loss: 0.075850, acc.: 99.22%] [G loss: 3.120334]\n",
      "epoch:38 step:36186 [D loss: 2.627354, acc.: 14.84%] [G loss: 1.200396]\n",
      "epoch:38 step:36187 [D loss: 0.042510, acc.: 100.00%] [G loss: 1.394229]\n",
      "epoch:38 step:36188 [D loss: 0.143350, acc.: 99.22%] [G loss: 1.344523]\n",
      "epoch:38 step:36189 [D loss: 0.162666, acc.: 96.88%] [G loss: 2.446282]\n",
      "epoch:38 step:36190 [D loss: 0.108208, acc.: 98.44%] [G loss: 2.454126]\n",
      "epoch:38 step:36191 [D loss: 0.082514, acc.: 100.00%] [G loss: 0.981372]\n",
      "epoch:38 step:36192 [D loss: 0.027209, acc.: 100.00%] [G loss: 5.381044]\n",
      "epoch:38 step:36193 [D loss: 0.259736, acc.: 88.28%] [G loss: 6.480970]\n",
      "epoch:38 step:36194 [D loss: 0.038955, acc.: 99.22%] [G loss: 1.941055]\n",
      "epoch:38 step:36195 [D loss: 0.284471, acc.: 86.72%] [G loss: 5.130939]\n",
      "epoch:38 step:36196 [D loss: 0.129230, acc.: 96.09%] [G loss: 3.683437]\n",
      "epoch:38 step:36197 [D loss: 0.061717, acc.: 99.22%] [G loss: 3.266984]\n",
      "epoch:38 step:36198 [D loss: 0.300652, acc.: 85.94%] [G loss: 3.653820]\n",
      "epoch:38 step:36199 [D loss: 0.306808, acc.: 85.16%] [G loss: 0.907943]\n",
      "epoch:38 step:36200 [D loss: 1.481203, acc.: 49.22%] [G loss: 4.382814]\n",
      "epoch:38 step:36201 [D loss: 0.124530, acc.: 98.44%] [G loss: 5.619382]\n",
      "epoch:38 step:36202 [D loss: 0.368970, acc.: 81.25%] [G loss: 2.961254]\n",
      "epoch:38 step:36203 [D loss: 0.835712, acc.: 49.22%] [G loss: 4.369773]\n",
      "epoch:38 step:36204 [D loss: 0.124232, acc.: 96.09%] [G loss: 4.360686]\n",
      "epoch:38 step:36205 [D loss: 0.473597, acc.: 64.84%] [G loss: 4.056843]\n",
      "epoch:38 step:36206 [D loss: 0.668201, acc.: 64.84%] [G loss: 1.191646]\n",
      "epoch:38 step:36207 [D loss: 1.439220, acc.: 28.12%] [G loss: 2.534364]\n",
      "epoch:38 step:36208 [D loss: 0.093756, acc.: 100.00%] [G loss: 4.762892]\n",
      "epoch:38 step:36209 [D loss: 0.046608, acc.: 100.00%] [G loss: 4.418957]\n",
      "epoch:38 step:36210 [D loss: 0.164509, acc.: 93.75%] [G loss: 3.736049]\n",
      "epoch:38 step:36211 [D loss: 0.067066, acc.: 98.44%] [G loss: 3.964689]\n",
      "epoch:38 step:36212 [D loss: 0.050656, acc.: 100.00%] [G loss: 3.188063]\n",
      "epoch:38 step:36213 [D loss: 0.272967, acc.: 85.94%] [G loss: 1.691626]\n",
      "epoch:38 step:36214 [D loss: 0.276296, acc.: 89.06%] [G loss: 2.196261]\n",
      "epoch:38 step:36215 [D loss: 0.207927, acc.: 93.75%] [G loss: 1.664851]\n",
      "epoch:38 step:36216 [D loss: 0.239288, acc.: 92.97%] [G loss: 4.454979]\n",
      "epoch:38 step:36217 [D loss: 0.103306, acc.: 98.44%] [G loss: 3.174784]\n",
      "epoch:38 step:36218 [D loss: 0.035786, acc.: 99.22%] [G loss: 3.212855]\n",
      "epoch:38 step:36219 [D loss: 0.031925, acc.: 100.00%] [G loss: 2.555334]\n",
      "epoch:38 step:36220 [D loss: 0.256850, acc.: 96.09%] [G loss: 4.973083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:36221 [D loss: 0.306332, acc.: 82.03%] [G loss: 5.192491]\n",
      "epoch:38 step:36222 [D loss: 0.158376, acc.: 97.66%] [G loss: 3.123857]\n",
      "epoch:38 step:36223 [D loss: 0.085878, acc.: 99.22%] [G loss: 4.458034]\n",
      "epoch:38 step:36224 [D loss: 0.302155, acc.: 94.53%] [G loss: 3.946333]\n",
      "epoch:38 step:36225 [D loss: 0.147172, acc.: 98.44%] [G loss: 3.666555]\n",
      "epoch:38 step:36226 [D loss: 0.116341, acc.: 96.88%] [G loss: 1.898850]\n",
      "epoch:38 step:36227 [D loss: 0.243718, acc.: 89.84%] [G loss: 4.283599]\n",
      "epoch:38 step:36228 [D loss: 0.350870, acc.: 92.19%] [G loss: 1.126978]\n",
      "epoch:38 step:36229 [D loss: 0.080174, acc.: 100.00%] [G loss: 3.501507]\n",
      "epoch:38 step:36230 [D loss: 0.557788, acc.: 65.62%] [G loss: 2.808647]\n",
      "epoch:38 step:36231 [D loss: 0.323838, acc.: 88.28%] [G loss: 7.662333]\n",
      "epoch:38 step:36232 [D loss: 0.090614, acc.: 98.44%] [G loss: 3.117682]\n",
      "epoch:38 step:36233 [D loss: 0.023596, acc.: 100.00%] [G loss: 6.446659]\n",
      "epoch:38 step:36234 [D loss: 0.072263, acc.: 97.66%] [G loss: 4.469193]\n",
      "epoch:38 step:36235 [D loss: 0.172906, acc.: 94.53%] [G loss: 5.142049]\n",
      "epoch:38 step:36236 [D loss: 0.103067, acc.: 99.22%] [G loss: 3.871566]\n",
      "epoch:38 step:36237 [D loss: 0.347814, acc.: 85.16%] [G loss: 2.029970]\n",
      "epoch:38 step:36238 [D loss: 0.117596, acc.: 98.44%] [G loss: 2.760424]\n",
      "epoch:38 step:36239 [D loss: 0.250484, acc.: 93.75%] [G loss: 4.591011]\n",
      "epoch:38 step:36240 [D loss: 0.035184, acc.: 100.00%] [G loss: 3.183554]\n",
      "epoch:38 step:36241 [D loss: 1.455688, acc.: 52.34%] [G loss: 5.244750]\n",
      "epoch:38 step:36242 [D loss: 0.030845, acc.: 99.22%] [G loss: 6.505451]\n",
      "epoch:38 step:36243 [D loss: 0.033958, acc.: 98.44%] [G loss: 4.522381]\n",
      "epoch:38 step:36244 [D loss: 0.151870, acc.: 92.97%] [G loss: 6.773939]\n",
      "epoch:38 step:36245 [D loss: 0.323842, acc.: 85.94%] [G loss: 4.262429]\n",
      "epoch:38 step:36246 [D loss: 0.024985, acc.: 100.00%] [G loss: 4.185594]\n",
      "epoch:38 step:36247 [D loss: 0.120658, acc.: 97.66%] [G loss: 2.640265]\n",
      "epoch:38 step:36248 [D loss: 0.008257, acc.: 100.00%] [G loss: 2.986622]\n",
      "epoch:38 step:36249 [D loss: 0.133431, acc.: 98.44%] [G loss: 4.045622]\n",
      "epoch:38 step:36250 [D loss: 0.083357, acc.: 98.44%] [G loss: 3.921563]\n",
      "epoch:38 step:36251 [D loss: 0.081003, acc.: 100.00%] [G loss: 1.574029]\n",
      "epoch:38 step:36252 [D loss: 0.423868, acc.: 85.94%] [G loss: 14.507581]\n",
      "epoch:38 step:36253 [D loss: 0.350238, acc.: 75.78%] [G loss: 3.105439]\n",
      "epoch:38 step:36254 [D loss: 0.077479, acc.: 100.00%] [G loss: 8.642385]\n",
      "epoch:38 step:36255 [D loss: 0.112507, acc.: 96.88%] [G loss: 5.622526]\n",
      "epoch:38 step:36256 [D loss: 0.762002, acc.: 70.31%] [G loss: 4.153275]\n",
      "epoch:38 step:36257 [D loss: 0.634645, acc.: 65.62%] [G loss: 4.190213]\n",
      "epoch:38 step:36258 [D loss: 1.124285, acc.: 32.81%] [G loss: 3.314630]\n",
      "epoch:38 step:36259 [D loss: 0.303187, acc.: 86.72%] [G loss: 1.054375]\n",
      "epoch:38 step:36260 [D loss: 0.177019, acc.: 95.31%] [G loss: 2.953473]\n",
      "epoch:38 step:36261 [D loss: 0.068813, acc.: 100.00%] [G loss: 4.728524]\n",
      "epoch:38 step:36262 [D loss: 0.386333, acc.: 81.25%] [G loss: 4.680477]\n",
      "epoch:38 step:36263 [D loss: 0.145217, acc.: 96.88%] [G loss: 1.468332]\n",
      "epoch:38 step:36264 [D loss: 0.865700, acc.: 52.34%] [G loss: 2.906286]\n",
      "epoch:38 step:36265 [D loss: 0.082306, acc.: 98.44%] [G loss: 1.480516]\n",
      "epoch:38 step:36266 [D loss: 1.276113, acc.: 55.47%] [G loss: 5.887486]\n",
      "epoch:38 step:36267 [D loss: 0.117471, acc.: 97.66%] [G loss: 5.883882]\n",
      "epoch:38 step:36268 [D loss: 0.800767, acc.: 59.38%] [G loss: 4.467409]\n",
      "epoch:38 step:36269 [D loss: 0.124073, acc.: 97.66%] [G loss: 4.816992]\n",
      "epoch:38 step:36270 [D loss: 0.099428, acc.: 97.66%] [G loss: 4.104722]\n",
      "epoch:38 step:36271 [D loss: 0.094480, acc.: 100.00%] [G loss: 2.542743]\n",
      "epoch:38 step:36272 [D loss: 0.339887, acc.: 86.72%] [G loss: 2.560106]\n",
      "epoch:38 step:36273 [D loss: 0.022172, acc.: 100.00%] [G loss: 5.462537]\n",
      "epoch:38 step:36274 [D loss: 0.547575, acc.: 63.28%] [G loss: 1.086594]\n",
      "epoch:38 step:36275 [D loss: 0.410273, acc.: 82.03%] [G loss: 2.110670]\n",
      "epoch:38 step:36276 [D loss: 0.063265, acc.: 99.22%] [G loss: 3.549011]\n",
      "epoch:38 step:36277 [D loss: 0.167800, acc.: 94.53%] [G loss: 4.058491]\n",
      "epoch:38 step:36278 [D loss: 0.099421, acc.: 98.44%] [G loss: 3.212746]\n",
      "epoch:38 step:36279 [D loss: 0.669330, acc.: 63.28%] [G loss: 5.572393]\n",
      "epoch:38 step:36280 [D loss: 0.086111, acc.: 98.44%] [G loss: 5.344725]\n",
      "epoch:38 step:36281 [D loss: 0.108373, acc.: 97.66%] [G loss: 2.381471]\n",
      "epoch:38 step:36282 [D loss: 0.102648, acc.: 98.44%] [G loss: 2.351155]\n",
      "epoch:38 step:36283 [D loss: 0.083306, acc.: 99.22%] [G loss: 4.872650]\n",
      "epoch:38 step:36284 [D loss: 0.042582, acc.: 100.00%] [G loss: 4.993141]\n",
      "epoch:38 step:36285 [D loss: 0.329934, acc.: 93.75%] [G loss: 3.991864]\n",
      "epoch:38 step:36286 [D loss: 0.302579, acc.: 91.41%] [G loss: 4.787809]\n",
      "epoch:38 step:36287 [D loss: 0.062056, acc.: 100.00%] [G loss: 2.870136]\n",
      "epoch:38 step:36288 [D loss: 0.213981, acc.: 94.53%] [G loss: 1.983007]\n",
      "epoch:38 step:36289 [D loss: 0.311353, acc.: 83.59%] [G loss: 3.992881]\n",
      "epoch:38 step:36290 [D loss: 0.859730, acc.: 40.62%] [G loss: 5.455432]\n",
      "epoch:38 step:36291 [D loss: 0.180478, acc.: 96.09%] [G loss: 3.561459]\n",
      "epoch:38 step:36292 [D loss: 0.285805, acc.: 92.97%] [G loss: 2.303458]\n",
      "epoch:38 step:36293 [D loss: 0.093099, acc.: 98.44%] [G loss: 3.455428]\n",
      "epoch:38 step:36294 [D loss: 0.156543, acc.: 96.88%] [G loss: 4.415093]\n",
      "epoch:38 step:36295 [D loss: 0.407412, acc.: 80.47%] [G loss: 3.799784]\n",
      "epoch:38 step:36296 [D loss: 1.009852, acc.: 53.91%] [G loss: 1.162560]\n",
      "epoch:38 step:36297 [D loss: 0.299580, acc.: 83.59%] [G loss: 2.476714]\n",
      "epoch:38 step:36298 [D loss: 0.045865, acc.: 100.00%] [G loss: 1.678187]\n",
      "epoch:38 step:36299 [D loss: 0.547035, acc.: 71.09%] [G loss: 0.502333]\n",
      "epoch:38 step:36300 [D loss: 0.508500, acc.: 74.22%] [G loss: 0.798519]\n",
      "epoch:38 step:36301 [D loss: 0.241483, acc.: 90.62%] [G loss: 1.939017]\n",
      "epoch:38 step:36302 [D loss: 0.022279, acc.: 100.00%] [G loss: 2.399040]\n",
      "epoch:38 step:36303 [D loss: 0.136550, acc.: 96.09%] [G loss: 2.166010]\n",
      "epoch:38 step:36304 [D loss: 0.036958, acc.: 100.00%] [G loss: 0.142951]\n",
      "epoch:38 step:36305 [D loss: 0.078419, acc.: 99.22%] [G loss: 2.502176]\n",
      "epoch:38 step:36306 [D loss: 0.153023, acc.: 96.88%] [G loss: 2.648514]\n",
      "epoch:38 step:36307 [D loss: 0.373133, acc.: 80.47%] [G loss: 2.790608]\n",
      "epoch:38 step:36308 [D loss: 0.136286, acc.: 97.66%] [G loss: 3.065982]\n",
      "epoch:38 step:36309 [D loss: 1.104167, acc.: 52.34%] [G loss: 5.384099]\n",
      "epoch:38 step:36310 [D loss: 0.820129, acc.: 62.50%] [G loss: 2.599236]\n",
      "epoch:38 step:36311 [D loss: 0.191526, acc.: 97.66%] [G loss: 4.214863]\n",
      "epoch:38 step:36312 [D loss: 0.366125, acc.: 82.03%] [G loss: 5.145965]\n",
      "epoch:38 step:36313 [D loss: 0.009754, acc.: 100.00%] [G loss: 0.765175]\n",
      "epoch:38 step:36314 [D loss: 0.176889, acc.: 95.31%] [G loss: 4.488372]\n",
      "epoch:38 step:36315 [D loss: 0.333112, acc.: 79.69%] [G loss: 5.812500]\n",
      "epoch:38 step:36316 [D loss: 0.062920, acc.: 98.44%] [G loss: 4.173406]\n",
      "epoch:38 step:36317 [D loss: 0.054154, acc.: 100.00%] [G loss: 2.348894]\n",
      "epoch:38 step:36318 [D loss: 0.017295, acc.: 100.00%] [G loss: 4.663028]\n",
      "epoch:38 step:36319 [D loss: 0.084153, acc.: 96.88%] [G loss: 1.159954]\n",
      "epoch:38 step:36320 [D loss: 0.156902, acc.: 93.75%] [G loss: 5.048700]\n",
      "epoch:38 step:36321 [D loss: 0.055372, acc.: 100.00%] [G loss: 4.989381]\n",
      "epoch:38 step:36322 [D loss: 0.026119, acc.: 100.00%] [G loss: 6.419561]\n",
      "epoch:38 step:36323 [D loss: 0.670429, acc.: 66.41%] [G loss: 2.279863]\n",
      "epoch:38 step:36324 [D loss: 0.528457, acc.: 67.97%] [G loss: 5.128043]\n",
      "epoch:38 step:36325 [D loss: 0.075814, acc.: 99.22%] [G loss: 4.848449]\n",
      "epoch:38 step:36326 [D loss: 0.041458, acc.: 100.00%] [G loss: 6.668563]\n",
      "epoch:38 step:36327 [D loss: 0.081718, acc.: 98.44%] [G loss: 5.266869]\n",
      "epoch:38 step:36328 [D loss: 1.453188, acc.: 50.78%] [G loss: 3.610642]\n",
      "epoch:38 step:36329 [D loss: 0.100108, acc.: 96.88%] [G loss: 0.741860]\n",
      "epoch:38 step:36330 [D loss: 0.491408, acc.: 65.62%] [G loss: 3.590483]\n",
      "epoch:38 step:36331 [D loss: 0.087504, acc.: 99.22%] [G loss: 2.087831]\n",
      "epoch:38 step:36332 [D loss: 0.276946, acc.: 92.97%] [G loss: 4.858768]\n",
      "epoch:38 step:36333 [D loss: 0.060110, acc.: 99.22%] [G loss: 3.909153]\n",
      "epoch:38 step:36334 [D loss: 0.063660, acc.: 99.22%] [G loss: 4.124561]\n",
      "epoch:38 step:36335 [D loss: 0.946554, acc.: 60.16%] [G loss: 4.076454]\n",
      "epoch:38 step:36336 [D loss: 0.262100, acc.: 89.84%] [G loss: 1.143090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:36337 [D loss: 0.058972, acc.: 97.66%] [G loss: 1.932509]\n",
      "epoch:38 step:36338 [D loss: 0.747174, acc.: 63.28%] [G loss: 1.795867]\n",
      "epoch:38 step:36339 [D loss: 0.037324, acc.: 100.00%] [G loss: 4.767522]\n",
      "epoch:38 step:36340 [D loss: 0.048825, acc.: 98.44%] [G loss: 3.820650]\n",
      "epoch:38 step:36341 [D loss: 0.046519, acc.: 99.22%] [G loss: 3.807819]\n",
      "epoch:38 step:36342 [D loss: 0.091769, acc.: 97.66%] [G loss: 3.705716]\n",
      "epoch:38 step:36343 [D loss: 0.146879, acc.: 95.31%] [G loss: 1.893743]\n",
      "epoch:38 step:36344 [D loss: 0.363686, acc.: 89.84%] [G loss: 3.203927]\n",
      "epoch:38 step:36345 [D loss: 0.370854, acc.: 82.03%] [G loss: 3.420120]\n",
      "epoch:38 step:36346 [D loss: 0.190846, acc.: 95.31%] [G loss: 4.267236]\n",
      "epoch:38 step:36347 [D loss: 0.335204, acc.: 86.72%] [G loss: 3.895901]\n",
      "epoch:38 step:36348 [D loss: 0.046915, acc.: 99.22%] [G loss: 3.203996]\n",
      "epoch:38 step:36349 [D loss: 0.080486, acc.: 98.44%] [G loss: 1.210023]\n",
      "epoch:38 step:36350 [D loss: 0.092754, acc.: 98.44%] [G loss: 2.035087]\n",
      "epoch:38 step:36351 [D loss: 0.170183, acc.: 93.75%] [G loss: 3.132020]\n",
      "epoch:38 step:36352 [D loss: 0.611376, acc.: 70.31%] [G loss: 5.166242]\n",
      "epoch:38 step:36353 [D loss: 0.059049, acc.: 98.44%] [G loss: 4.160078]\n",
      "epoch:38 step:36354 [D loss: 0.128134, acc.: 98.44%] [G loss: 5.034718]\n",
      "epoch:38 step:36355 [D loss: 0.030592, acc.: 100.00%] [G loss: 1.863150]\n",
      "epoch:38 step:36356 [D loss: 0.129771, acc.: 96.09%] [G loss: 4.778756]\n",
      "epoch:38 step:36357 [D loss: 0.045876, acc.: 99.22%] [G loss: 3.266630]\n",
      "epoch:38 step:36358 [D loss: 1.011372, acc.: 38.28%] [G loss: 3.379076]\n",
      "epoch:38 step:36359 [D loss: 0.088193, acc.: 97.66%] [G loss: 4.974560]\n",
      "epoch:38 step:36360 [D loss: 0.161556, acc.: 95.31%] [G loss: 3.812070]\n",
      "epoch:38 step:36361 [D loss: 0.063767, acc.: 100.00%] [G loss: 3.363792]\n",
      "epoch:38 step:36362 [D loss: 0.254963, acc.: 91.41%] [G loss: 1.392665]\n",
      "epoch:38 step:36363 [D loss: 0.039198, acc.: 100.00%] [G loss: 3.546308]\n",
      "epoch:38 step:36364 [D loss: 0.037669, acc.: 100.00%] [G loss: 1.595409]\n",
      "epoch:38 step:36365 [D loss: 0.286538, acc.: 85.94%] [G loss: 3.170146]\n",
      "epoch:38 step:36366 [D loss: 0.077899, acc.: 98.44%] [G loss: 3.137883]\n",
      "epoch:38 step:36367 [D loss: 0.069757, acc.: 99.22%] [G loss: 4.283466]\n",
      "epoch:38 step:36368 [D loss: 0.090434, acc.: 99.22%] [G loss: 3.444607]\n",
      "epoch:38 step:36369 [D loss: 0.094723, acc.: 98.44%] [G loss: 6.465423]\n",
      "epoch:38 step:36370 [D loss: 0.199368, acc.: 93.75%] [G loss: 3.431424]\n",
      "epoch:38 step:36371 [D loss: 0.034556, acc.: 99.22%] [G loss: 5.029185]\n",
      "epoch:38 step:36372 [D loss: 0.342866, acc.: 83.59%] [G loss: 1.663708]\n",
      "epoch:38 step:36373 [D loss: 0.040059, acc.: 100.00%] [G loss: 4.486969]\n",
      "epoch:38 step:36374 [D loss: 0.189145, acc.: 94.53%] [G loss: 4.880571]\n",
      "epoch:38 step:36375 [D loss: 0.093459, acc.: 99.22%] [G loss: 5.390213]\n",
      "epoch:38 step:36376 [D loss: 0.019636, acc.: 100.00%] [G loss: 6.507220]\n",
      "epoch:38 step:36377 [D loss: 0.032648, acc.: 100.00%] [G loss: 1.800143]\n",
      "epoch:38 step:36378 [D loss: 0.083794, acc.: 99.22%] [G loss: 2.334715]\n",
      "epoch:38 step:36379 [D loss: 0.077029, acc.: 98.44%] [G loss: 3.897135]\n",
      "epoch:38 step:36380 [D loss: 0.203113, acc.: 97.66%] [G loss: 2.005711]\n",
      "epoch:38 step:36381 [D loss: 0.333715, acc.: 83.59%] [G loss: 3.942780]\n",
      "epoch:38 step:36382 [D loss: 0.039005, acc.: 99.22%] [G loss: 1.011944]\n",
      "epoch:38 step:36383 [D loss: 0.392545, acc.: 81.25%] [G loss: 2.533242]\n",
      "epoch:38 step:36384 [D loss: 1.820855, acc.: 52.34%] [G loss: 5.097140]\n",
      "epoch:38 step:36385 [D loss: 0.086065, acc.: 99.22%] [G loss: 2.066091]\n",
      "epoch:38 step:36386 [D loss: 0.051584, acc.: 99.22%] [G loss: 6.289300]\n",
      "epoch:38 step:36387 [D loss: 0.420734, acc.: 78.12%] [G loss: 1.317350]\n",
      "epoch:38 step:36388 [D loss: 0.229645, acc.: 91.41%] [G loss: 8.709500]\n",
      "epoch:38 step:36389 [D loss: 0.294274, acc.: 91.41%] [G loss: 3.582765]\n",
      "epoch:38 step:36390 [D loss: 0.037897, acc.: 100.00%] [G loss: 7.182328]\n",
      "epoch:38 step:36391 [D loss: 0.023596, acc.: 99.22%] [G loss: 4.076139]\n",
      "epoch:38 step:36392 [D loss: 0.305221, acc.: 86.72%] [G loss: 4.972055]\n",
      "epoch:38 step:36393 [D loss: 0.093084, acc.: 98.44%] [G loss: 2.875103]\n",
      "epoch:38 step:36394 [D loss: 0.533164, acc.: 68.75%] [G loss: 1.259723]\n",
      "epoch:38 step:36395 [D loss: 0.250645, acc.: 87.50%] [G loss: 5.479004]\n",
      "epoch:38 step:36396 [D loss: 0.101483, acc.: 98.44%] [G loss: 5.369575]\n",
      "epoch:38 step:36397 [D loss: 1.494188, acc.: 34.38%] [G loss: 2.019235]\n",
      "epoch:38 step:36398 [D loss: 0.164608, acc.: 96.88%] [G loss: 3.941005]\n",
      "epoch:38 step:36399 [D loss: 0.288365, acc.: 84.38%] [G loss: 2.299008]\n",
      "epoch:38 step:36400 [D loss: 0.205044, acc.: 94.53%] [G loss: 2.607239]\n",
      "epoch:38 step:36401 [D loss: 0.116901, acc.: 97.66%] [G loss: 5.155810]\n",
      "epoch:38 step:36402 [D loss: 0.298293, acc.: 85.94%] [G loss: 1.514584]\n",
      "epoch:38 step:36403 [D loss: 0.037194, acc.: 100.00%] [G loss: 3.371235]\n",
      "epoch:38 step:36404 [D loss: 0.149731, acc.: 97.66%] [G loss: 3.576088]\n",
      "epoch:38 step:36405 [D loss: 0.411778, acc.: 72.66%] [G loss: 6.214642]\n",
      "epoch:38 step:36406 [D loss: 0.033307, acc.: 100.00%] [G loss: 4.203878]\n",
      "epoch:38 step:36407 [D loss: 0.181169, acc.: 97.66%] [G loss: 4.199149]\n",
      "epoch:38 step:36408 [D loss: 0.321456, acc.: 88.28%] [G loss: 1.244629]\n",
      "epoch:38 step:36409 [D loss: 0.045080, acc.: 100.00%] [G loss: 3.264264]\n",
      "epoch:38 step:36410 [D loss: 0.083208, acc.: 99.22%] [G loss: 1.829176]\n",
      "epoch:38 step:36411 [D loss: 0.197053, acc.: 96.88%] [G loss: 1.240585]\n",
      "epoch:38 step:36412 [D loss: 0.773946, acc.: 57.03%] [G loss: 3.695060]\n",
      "epoch:38 step:36413 [D loss: 1.227798, acc.: 52.34%] [G loss: 2.338554]\n",
      "epoch:38 step:36414 [D loss: 0.484484, acc.: 74.22%] [G loss: 4.418418]\n",
      "epoch:38 step:36415 [D loss: 0.086347, acc.: 98.44%] [G loss: 1.561761]\n",
      "epoch:38 step:36416 [D loss: 0.121615, acc.: 98.44%] [G loss: 3.827836]\n",
      "epoch:38 step:36417 [D loss: 0.216278, acc.: 92.97%] [G loss: 1.571636]\n",
      "epoch:38 step:36418 [D loss: 0.127652, acc.: 96.88%] [G loss: 2.798043]\n",
      "epoch:38 step:36419 [D loss: 0.480886, acc.: 75.78%] [G loss: 2.094760]\n",
      "epoch:38 step:36420 [D loss: 0.095632, acc.: 99.22%] [G loss: 3.079283]\n",
      "epoch:38 step:36421 [D loss: 0.056552, acc.: 100.00%] [G loss: 3.920211]\n",
      "epoch:38 step:36422 [D loss: 0.555210, acc.: 72.66%] [G loss: 3.215117]\n",
      "epoch:38 step:36423 [D loss: 0.059214, acc.: 98.44%] [G loss: 2.736865]\n",
      "epoch:38 step:36424 [D loss: 0.128404, acc.: 97.66%] [G loss: 0.866808]\n",
      "epoch:38 step:36425 [D loss: 0.361333, acc.: 80.47%] [G loss: 3.489760]\n",
      "epoch:38 step:36426 [D loss: 0.139442, acc.: 96.09%] [G loss: 1.139588]\n",
      "epoch:38 step:36427 [D loss: 0.105609, acc.: 98.44%] [G loss: 5.488162]\n",
      "epoch:38 step:36428 [D loss: 0.050954, acc.: 99.22%] [G loss: 3.164629]\n",
      "epoch:38 step:36429 [D loss: 0.100329, acc.: 98.44%] [G loss: 4.523600]\n",
      "epoch:38 step:36430 [D loss: 0.265744, acc.: 91.41%] [G loss: 0.656455]\n",
      "epoch:38 step:36431 [D loss: 0.088216, acc.: 98.44%] [G loss: 5.139157]\n",
      "epoch:38 step:36432 [D loss: 0.082569, acc.: 100.00%] [G loss: 2.849189]\n",
      "epoch:38 step:36433 [D loss: 0.082982, acc.: 98.44%] [G loss: 3.896788]\n",
      "epoch:38 step:36434 [D loss: 0.178807, acc.: 96.09%] [G loss: 5.430429]\n",
      "epoch:38 step:36435 [D loss: 0.088779, acc.: 97.66%] [G loss: 5.006104]\n",
      "epoch:38 step:36436 [D loss: 0.198224, acc.: 91.41%] [G loss: 2.791305]\n",
      "epoch:38 step:36437 [D loss: 0.334604, acc.: 83.59%] [G loss: 3.888200]\n",
      "epoch:38 step:36438 [D loss: 0.005946, acc.: 100.00%] [G loss: 3.435976]\n",
      "epoch:38 step:36439 [D loss: 0.291652, acc.: 86.72%] [G loss: 1.964196]\n",
      "epoch:38 step:36440 [D loss: 0.010010, acc.: 100.00%] [G loss: 8.463642]\n",
      "epoch:38 step:36441 [D loss: 0.132429, acc.: 96.09%] [G loss: 7.693076]\n",
      "epoch:38 step:36442 [D loss: 0.012212, acc.: 100.00%] [G loss: 1.436383]\n",
      "epoch:38 step:36443 [D loss: 0.127554, acc.: 99.22%] [G loss: 7.842013]\n",
      "epoch:38 step:36444 [D loss: 0.124349, acc.: 98.44%] [G loss: 1.574658]\n",
      "epoch:38 step:36445 [D loss: 0.055088, acc.: 99.22%] [G loss: 4.702657]\n",
      "epoch:38 step:36446 [D loss: 0.014427, acc.: 100.00%] [G loss: 7.238791]\n",
      "epoch:38 step:36447 [D loss: 0.056855, acc.: 97.66%] [G loss: 8.223539]\n",
      "epoch:38 step:36448 [D loss: 0.258142, acc.: 90.62%] [G loss: 4.623682]\n",
      "epoch:38 step:36449 [D loss: 0.393188, acc.: 75.00%] [G loss: 8.151724]\n",
      "epoch:38 step:36450 [D loss: 0.117452, acc.: 97.66%] [G loss: 3.934269]\n",
      "epoch:38 step:36451 [D loss: 0.138726, acc.: 96.09%] [G loss: 4.567901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:36452 [D loss: 0.049623, acc.: 100.00%] [G loss: 2.935151]\n",
      "epoch:38 step:36453 [D loss: 0.170534, acc.: 95.31%] [G loss: 3.095034]\n",
      "epoch:38 step:36454 [D loss: 0.191209, acc.: 92.19%] [G loss: 0.684148]\n",
      "epoch:38 step:36455 [D loss: 0.450295, acc.: 78.12%] [G loss: 5.766402]\n",
      "epoch:38 step:36456 [D loss: 0.373234, acc.: 82.03%] [G loss: 3.178896]\n",
      "epoch:38 step:36457 [D loss: 0.120712, acc.: 96.88%] [G loss: 3.260281]\n",
      "epoch:38 step:36458 [D loss: 0.177214, acc.: 96.09%] [G loss: 7.566381]\n",
      "epoch:38 step:36459 [D loss: 0.083570, acc.: 98.44%] [G loss: 4.068460]\n",
      "epoch:38 step:36460 [D loss: 0.742840, acc.: 50.00%] [G loss: 2.306437]\n",
      "epoch:38 step:36461 [D loss: 0.081831, acc.: 98.44%] [G loss: 5.664683]\n",
      "epoch:38 step:36462 [D loss: 0.103026, acc.: 99.22%] [G loss: 5.425869]\n",
      "epoch:38 step:36463 [D loss: 0.147525, acc.: 97.66%] [G loss: 5.415030]\n",
      "epoch:38 step:36464 [D loss: 0.075484, acc.: 99.22%] [G loss: 5.319265]\n",
      "epoch:38 step:36465 [D loss: 0.755474, acc.: 60.94%] [G loss: 7.339508]\n",
      "epoch:38 step:36466 [D loss: 0.330285, acc.: 81.25%] [G loss: 3.970528]\n",
      "epoch:38 step:36467 [D loss: 0.052202, acc.: 100.00%] [G loss: 9.382351]\n",
      "epoch:38 step:36468 [D loss: 0.019295, acc.: 100.00%] [G loss: 4.925723]\n",
      "epoch:38 step:36469 [D loss: 0.070231, acc.: 98.44%] [G loss: 7.524791]\n",
      "epoch:38 step:36470 [D loss: 0.007259, acc.: 100.00%] [G loss: 4.803444]\n",
      "epoch:38 step:36471 [D loss: 0.062235, acc.: 99.22%] [G loss: 2.628469]\n",
      "epoch:38 step:36472 [D loss: 0.023621, acc.: 100.00%] [G loss: 1.547650]\n",
      "epoch:38 step:36473 [D loss: 0.023193, acc.: 100.00%] [G loss: 2.748811]\n",
      "epoch:38 step:36474 [D loss: 0.194423, acc.: 93.75%] [G loss: 3.240345]\n",
      "epoch:38 step:36475 [D loss: 0.030039, acc.: 100.00%] [G loss: 7.028775]\n",
      "epoch:38 step:36476 [D loss: 0.016450, acc.: 100.00%] [G loss: 3.394403]\n",
      "epoch:38 step:36477 [D loss: 0.451120, acc.: 74.22%] [G loss: 6.585967]\n",
      "epoch:38 step:36478 [D loss: 0.382985, acc.: 78.91%] [G loss: 3.952322]\n",
      "epoch:38 step:36479 [D loss: 0.034878, acc.: 99.22%] [G loss: 3.020271]\n",
      "epoch:38 step:36480 [D loss: 0.050939, acc.: 99.22%] [G loss: 4.588590]\n",
      "epoch:38 step:36481 [D loss: 0.756373, acc.: 59.38%] [G loss: 2.824129]\n",
      "epoch:38 step:36482 [D loss: 0.155173, acc.: 96.88%] [G loss: 4.641798]\n",
      "epoch:38 step:36483 [D loss: 0.227917, acc.: 89.06%] [G loss: 8.606659]\n",
      "epoch:38 step:36484 [D loss: 0.060208, acc.: 100.00%] [G loss: 5.802315]\n",
      "epoch:38 step:36485 [D loss: 0.107940, acc.: 97.66%] [G loss: 7.401704]\n",
      "epoch:38 step:36486 [D loss: 0.288818, acc.: 85.94%] [G loss: 4.723555]\n",
      "epoch:38 step:36487 [D loss: 0.006702, acc.: 100.00%] [G loss: 4.348944]\n",
      "epoch:38 step:36488 [D loss: 0.116756, acc.: 97.66%] [G loss: 3.584226]\n",
      "epoch:38 step:36489 [D loss: 0.112468, acc.: 98.44%] [G loss: 7.423250]\n",
      "epoch:38 step:36490 [D loss: 0.463566, acc.: 80.47%] [G loss: 3.062959]\n",
      "epoch:38 step:36491 [D loss: 0.125871, acc.: 98.44%] [G loss: 2.698902]\n",
      "epoch:38 step:36492 [D loss: 0.655286, acc.: 58.59%] [G loss: 0.747851]\n",
      "epoch:38 step:36493 [D loss: 0.140910, acc.: 98.44%] [G loss: 6.629373]\n",
      "epoch:38 step:36494 [D loss: 0.355654, acc.: 77.34%] [G loss: 3.059137]\n",
      "epoch:38 step:36495 [D loss: 0.118961, acc.: 97.66%] [G loss: 4.408296]\n",
      "epoch:38 step:36496 [D loss: 0.016783, acc.: 100.00%] [G loss: 1.996541]\n",
      "epoch:38 step:36497 [D loss: 0.009951, acc.: 100.00%] [G loss: 0.679676]\n",
      "epoch:38 step:36498 [D loss: 0.023963, acc.: 100.00%] [G loss: 1.937201]\n",
      "epoch:38 step:36499 [D loss: 0.256658, acc.: 87.50%] [G loss: 2.233961]\n",
      "epoch:38 step:36500 [D loss: 0.131509, acc.: 96.09%] [G loss: 5.897536]\n",
      "epoch:38 step:36501 [D loss: 0.080323, acc.: 98.44%] [G loss: 4.647401]\n",
      "epoch:38 step:36502 [D loss: 0.087184, acc.: 97.66%] [G loss: 2.196327]\n",
      "epoch:38 step:36503 [D loss: 0.096646, acc.: 97.66%] [G loss: 3.094880]\n",
      "epoch:38 step:36504 [D loss: 0.695375, acc.: 67.19%] [G loss: 3.277018]\n",
      "epoch:38 step:36505 [D loss: 0.041070, acc.: 100.00%] [G loss: 1.190168]\n",
      "epoch:38 step:36506 [D loss: 0.424550, acc.: 75.00%] [G loss: 2.348014]\n",
      "epoch:38 step:36507 [D loss: 0.106384, acc.: 98.44%] [G loss: 1.512790]\n",
      "epoch:38 step:36508 [D loss: 0.689425, acc.: 60.94%] [G loss: 3.033365]\n",
      "epoch:38 step:36509 [D loss: 0.446209, acc.: 74.22%] [G loss: 3.494283]\n",
      "epoch:38 step:36510 [D loss: 0.042506, acc.: 99.22%] [G loss: 1.032482]\n",
      "epoch:38 step:36511 [D loss: 0.176209, acc.: 95.31%] [G loss: 3.594540]\n",
      "epoch:38 step:36512 [D loss: 0.278228, acc.: 85.94%] [G loss: 6.336917]\n",
      "epoch:38 step:36513 [D loss: 0.085109, acc.: 98.44%] [G loss: 4.940452]\n",
      "epoch:38 step:36514 [D loss: 0.217618, acc.: 96.88%] [G loss: 5.151767]\n",
      "epoch:38 step:36515 [D loss: 0.031123, acc.: 99.22%] [G loss: 3.110820]\n",
      "epoch:38 step:36516 [D loss: 0.521744, acc.: 78.12%] [G loss: 6.177643]\n",
      "epoch:38 step:36517 [D loss: 0.199438, acc.: 91.41%] [G loss: 5.584824]\n",
      "epoch:38 step:36518 [D loss: 0.218365, acc.: 94.53%] [G loss: 4.395741]\n",
      "epoch:38 step:36519 [D loss: 0.122813, acc.: 98.44%] [G loss: 3.562944]\n",
      "epoch:38 step:36520 [D loss: 0.137509, acc.: 96.88%] [G loss: 3.755422]\n",
      "epoch:38 step:36521 [D loss: 0.027058, acc.: 100.00%] [G loss: 7.636042]\n",
      "epoch:38 step:36522 [D loss: 0.296612, acc.: 86.72%] [G loss: 2.733557]\n",
      "epoch:38 step:36523 [D loss: 0.086118, acc.: 98.44%] [G loss: 2.315063]\n",
      "epoch:38 step:36524 [D loss: 0.039444, acc.: 99.22%] [G loss: 8.441695]\n",
      "epoch:38 step:36525 [D loss: 0.193830, acc.: 91.41%] [G loss: 4.065886]\n",
      "epoch:38 step:36526 [D loss: 0.187718, acc.: 96.09%] [G loss: 7.609612]\n",
      "epoch:38 step:36527 [D loss: 0.097390, acc.: 100.00%] [G loss: 5.504041]\n",
      "epoch:38 step:36528 [D loss: 0.121375, acc.: 97.66%] [G loss: 0.162533]\n",
      "epoch:38 step:36529 [D loss: 0.136568, acc.: 97.66%] [G loss: 0.849650]\n",
      "epoch:38 step:36530 [D loss: 0.084041, acc.: 98.44%] [G loss: 3.616422]\n",
      "epoch:38 step:36531 [D loss: 0.403069, acc.: 74.22%] [G loss: 5.723795]\n",
      "epoch:38 step:36532 [D loss: 0.037278, acc.: 100.00%] [G loss: 4.702970]\n",
      "epoch:38 step:36533 [D loss: 1.110002, acc.: 54.69%] [G loss: 0.852341]\n",
      "epoch:38 step:36534 [D loss: 0.539035, acc.: 69.53%] [G loss: 4.219153]\n",
      "epoch:38 step:36535 [D loss: 0.098224, acc.: 99.22%] [G loss: 6.895541]\n",
      "epoch:38 step:36536 [D loss: 0.011688, acc.: 99.22%] [G loss: 7.812829]\n",
      "epoch:38 step:36537 [D loss: 0.861428, acc.: 60.94%] [G loss: 7.529548]\n",
      "epoch:38 step:36538 [D loss: 0.418171, acc.: 76.56%] [G loss: 5.058934]\n",
      "epoch:38 step:36539 [D loss: 0.152191, acc.: 96.09%] [G loss: 2.490937]\n",
      "epoch:38 step:36540 [D loss: 0.017163, acc.: 100.00%] [G loss: 6.186342]\n",
      "epoch:38 step:36541 [D loss: 0.113842, acc.: 96.88%] [G loss: 6.847372]\n",
      "epoch:38 step:36542 [D loss: 0.026615, acc.: 100.00%] [G loss: 5.703652]\n",
      "epoch:38 step:36543 [D loss: 0.222655, acc.: 91.41%] [G loss: 4.132352]\n",
      "epoch:39 step:36544 [D loss: 0.189016, acc.: 94.53%] [G loss: 2.710386]\n",
      "epoch:39 step:36545 [D loss: 0.058022, acc.: 99.22%] [G loss: 5.046794]\n",
      "epoch:39 step:36546 [D loss: 0.347024, acc.: 87.50%] [G loss: 5.221252]\n",
      "epoch:39 step:36547 [D loss: 0.161444, acc.: 95.31%] [G loss: 6.588665]\n",
      "epoch:39 step:36548 [D loss: 0.304876, acc.: 81.25%] [G loss: 3.879349]\n",
      "epoch:39 step:36549 [D loss: 0.187079, acc.: 96.09%] [G loss: 3.240875]\n",
      "epoch:39 step:36550 [D loss: 0.098852, acc.: 99.22%] [G loss: 1.649280]\n",
      "epoch:39 step:36551 [D loss: 0.062926, acc.: 99.22%] [G loss: 3.701500]\n",
      "epoch:39 step:36552 [D loss: 0.369515, acc.: 85.16%] [G loss: 0.578359]\n",
      "epoch:39 step:36553 [D loss: 0.712613, acc.: 59.38%] [G loss: 5.708389]\n",
      "epoch:39 step:36554 [D loss: 0.141090, acc.: 96.09%] [G loss: 3.536822]\n",
      "epoch:39 step:36555 [D loss: 0.080029, acc.: 98.44%] [G loss: 6.948970]\n",
      "epoch:39 step:36556 [D loss: 0.255537, acc.: 92.97%] [G loss: 6.502381]\n",
      "epoch:39 step:36557 [D loss: 0.010500, acc.: 100.00%] [G loss: 6.044036]\n",
      "epoch:39 step:36558 [D loss: 0.250042, acc.: 92.19%] [G loss: 2.192382]\n",
      "epoch:39 step:36559 [D loss: 0.387693, acc.: 89.84%] [G loss: 0.539331]\n",
      "epoch:39 step:36560 [D loss: 0.768977, acc.: 60.16%] [G loss: 3.170971]\n",
      "epoch:39 step:36561 [D loss: 0.654572, acc.: 63.28%] [G loss: 4.177633]\n",
      "epoch:39 step:36562 [D loss: 0.331782, acc.: 83.59%] [G loss: 1.209729]\n",
      "epoch:39 step:36563 [D loss: 0.611696, acc.: 69.53%] [G loss: 5.158507]\n",
      "epoch:39 step:36564 [D loss: 0.038013, acc.: 98.44%] [G loss: 8.666941]\n",
      "epoch:39 step:36565 [D loss: 0.145745, acc.: 96.88%] [G loss: 5.165021]\n",
      "epoch:39 step:36566 [D loss: 0.193881, acc.: 89.84%] [G loss: 5.794681]\n",
      "epoch:39 step:36567 [D loss: 0.197923, acc.: 94.53%] [G loss: 5.737206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:36568 [D loss: 0.046571, acc.: 99.22%] [G loss: 4.013046]\n",
      "epoch:39 step:36569 [D loss: 0.133305, acc.: 96.88%] [G loss: 1.209395]\n",
      "epoch:39 step:36570 [D loss: 0.695921, acc.: 60.16%] [G loss: 2.724766]\n",
      "epoch:39 step:36571 [D loss: 0.203372, acc.: 90.62%] [G loss: 5.841577]\n",
      "epoch:39 step:36572 [D loss: 0.064789, acc.: 100.00%] [G loss: 4.945513]\n",
      "epoch:39 step:36573 [D loss: 0.117258, acc.: 96.88%] [G loss: 4.215225]\n",
      "epoch:39 step:36574 [D loss: 0.078556, acc.: 99.22%] [G loss: 4.515322]\n",
      "epoch:39 step:36575 [D loss: 0.063917, acc.: 99.22%] [G loss: 5.239552]\n",
      "epoch:39 step:36576 [D loss: 0.029873, acc.: 100.00%] [G loss: 7.489134]\n",
      "epoch:39 step:36577 [D loss: 0.706011, acc.: 64.06%] [G loss: 0.735098]\n",
      "epoch:39 step:36578 [D loss: 0.315430, acc.: 84.38%] [G loss: 4.611594]\n",
      "epoch:39 step:36579 [D loss: 0.047484, acc.: 99.22%] [G loss: 3.273651]\n",
      "epoch:39 step:36580 [D loss: 0.623494, acc.: 66.41%] [G loss: 6.550611]\n",
      "epoch:39 step:36581 [D loss: 0.057591, acc.: 98.44%] [G loss: 7.647126]\n",
      "epoch:39 step:36582 [D loss: 0.622929, acc.: 65.62%] [G loss: 2.272962]\n",
      "epoch:39 step:36583 [D loss: 0.094245, acc.: 97.66%] [G loss: 2.526492]\n",
      "epoch:39 step:36584 [D loss: 0.558758, acc.: 66.41%] [G loss: 5.779528]\n",
      "epoch:39 step:36585 [D loss: 0.051365, acc.: 100.00%] [G loss: 4.435575]\n",
      "epoch:39 step:36586 [D loss: 0.044635, acc.: 99.22%] [G loss: 8.706452]\n",
      "epoch:39 step:36587 [D loss: 0.362487, acc.: 83.59%] [G loss: 7.222260]\n",
      "epoch:39 step:36588 [D loss: 0.446008, acc.: 77.34%] [G loss: 1.315812]\n",
      "epoch:39 step:36589 [D loss: 0.113401, acc.: 96.88%] [G loss: 5.881804]\n",
      "epoch:39 step:36590 [D loss: 0.108044, acc.: 97.66%] [G loss: 2.746267]\n",
      "epoch:39 step:36591 [D loss: 0.051002, acc.: 99.22%] [G loss: 3.158813]\n",
      "epoch:39 step:36592 [D loss: 0.086421, acc.: 97.66%] [G loss: 0.356307]\n",
      "epoch:39 step:36593 [D loss: 0.495540, acc.: 70.31%] [G loss: 1.180404]\n",
      "epoch:39 step:36594 [D loss: 0.005113, acc.: 100.00%] [G loss: 8.407207]\n",
      "epoch:39 step:36595 [D loss: 1.866230, acc.: 57.03%] [G loss: 1.221640]\n",
      "epoch:39 step:36596 [D loss: 0.340153, acc.: 85.94%] [G loss: 1.393466]\n",
      "epoch:39 step:36597 [D loss: 0.855209, acc.: 57.81%] [G loss: 3.325648]\n",
      "epoch:39 step:36598 [D loss: 0.012375, acc.: 100.00%] [G loss: 3.950041]\n",
      "epoch:39 step:36599 [D loss: 0.045432, acc.: 98.44%] [G loss: 8.188333]\n",
      "epoch:39 step:36600 [D loss: 0.166269, acc.: 96.09%] [G loss: 7.053591]\n",
      "epoch:39 step:36601 [D loss: 0.014191, acc.: 100.00%] [G loss: 2.681299]\n",
      "epoch:39 step:36602 [D loss: 0.055882, acc.: 100.00%] [G loss: 1.675565]\n",
      "epoch:39 step:36603 [D loss: 0.027348, acc.: 100.00%] [G loss: 4.900281]\n",
      "epoch:39 step:36604 [D loss: 0.233123, acc.: 94.53%] [G loss: 1.313968]\n",
      "epoch:39 step:36605 [D loss: 0.064165, acc.: 98.44%] [G loss: 1.635643]\n",
      "epoch:39 step:36606 [D loss: 0.090129, acc.: 96.88%] [G loss: 2.318905]\n",
      "epoch:39 step:36607 [D loss: 0.168390, acc.: 96.88%] [G loss: 4.296793]\n",
      "epoch:39 step:36608 [D loss: 0.086972, acc.: 99.22%] [G loss: 1.041699]\n",
      "epoch:39 step:36609 [D loss: 0.016652, acc.: 99.22%] [G loss: 1.328127]\n",
      "epoch:39 step:36610 [D loss: 0.299784, acc.: 86.72%] [G loss: 5.030049]\n",
      "epoch:39 step:36611 [D loss: 0.591648, acc.: 65.62%] [G loss: 5.077354]\n",
      "epoch:39 step:36612 [D loss: 0.173985, acc.: 92.97%] [G loss: 2.706183]\n",
      "epoch:39 step:36613 [D loss: 0.071540, acc.: 99.22%] [G loss: 4.095757]\n",
      "epoch:39 step:36614 [D loss: 1.022096, acc.: 51.56%] [G loss: 5.001493]\n",
      "epoch:39 step:36615 [D loss: 0.008287, acc.: 99.22%] [G loss: 6.135942]\n",
      "epoch:39 step:36616 [D loss: 0.123295, acc.: 97.66%] [G loss: 4.599308]\n",
      "epoch:39 step:36617 [D loss: 1.312384, acc.: 49.22%] [G loss: 2.039568]\n",
      "epoch:39 step:36618 [D loss: 0.192485, acc.: 91.41%] [G loss: 0.543647]\n",
      "epoch:39 step:36619 [D loss: 0.210933, acc.: 95.31%] [G loss: 6.077626]\n",
      "epoch:39 step:36620 [D loss: 0.904516, acc.: 56.25%] [G loss: 4.672809]\n",
      "epoch:39 step:36621 [D loss: 0.059168, acc.: 97.66%] [G loss: 5.527779]\n",
      "epoch:39 step:36622 [D loss: 0.041820, acc.: 99.22%] [G loss: 6.313061]\n",
      "epoch:39 step:36623 [D loss: 0.088036, acc.: 99.22%] [G loss: 4.184122]\n",
      "epoch:39 step:36624 [D loss: 0.102994, acc.: 99.22%] [G loss: 2.227419]\n",
      "epoch:39 step:36625 [D loss: 0.191984, acc.: 94.53%] [G loss: 6.228488]\n",
      "epoch:39 step:36626 [D loss: 0.055092, acc.: 97.66%] [G loss: 4.528287]\n",
      "epoch:39 step:36627 [D loss: 0.475975, acc.: 72.66%] [G loss: 2.290804]\n",
      "epoch:39 step:36628 [D loss: 0.060257, acc.: 100.00%] [G loss: 2.749694]\n",
      "epoch:39 step:36629 [D loss: 0.030831, acc.: 100.00%] [G loss: 6.091650]\n",
      "epoch:39 step:36630 [D loss: 0.046597, acc.: 100.00%] [G loss: 7.655268]\n",
      "epoch:39 step:36631 [D loss: 0.322858, acc.: 90.62%] [G loss: 4.839962]\n",
      "epoch:39 step:36632 [D loss: 0.032213, acc.: 100.00%] [G loss: 6.026143]\n",
      "epoch:39 step:36633 [D loss: 0.080320, acc.: 99.22%] [G loss: 1.329221]\n",
      "epoch:39 step:36634 [D loss: 0.015354, acc.: 100.00%] [G loss: 6.360040]\n",
      "epoch:39 step:36635 [D loss: 0.544225, acc.: 65.62%] [G loss: 5.064549]\n",
      "epoch:39 step:36636 [D loss: 0.791739, acc.: 58.59%] [G loss: 2.249943]\n",
      "epoch:39 step:36637 [D loss: 0.270559, acc.: 87.50%] [G loss: 9.489038]\n",
      "epoch:39 step:36638 [D loss: 0.075622, acc.: 98.44%] [G loss: 5.747239]\n",
      "epoch:39 step:36639 [D loss: 0.139855, acc.: 96.88%] [G loss: 5.431528]\n",
      "epoch:39 step:36640 [D loss: 0.049623, acc.: 98.44%] [G loss: 9.919514]\n",
      "epoch:39 step:36641 [D loss: 0.110560, acc.: 96.09%] [G loss: 8.354564]\n",
      "epoch:39 step:36642 [D loss: 0.188203, acc.: 93.75%] [G loss: 1.200217]\n",
      "epoch:39 step:36643 [D loss: 0.413299, acc.: 78.91%] [G loss: 4.675561]\n",
      "epoch:39 step:36644 [D loss: 0.388693, acc.: 85.16%] [G loss: 3.052642]\n",
      "epoch:39 step:36645 [D loss: 0.217575, acc.: 94.53%] [G loss: 2.875145]\n",
      "epoch:39 step:36646 [D loss: 0.524950, acc.: 67.19%] [G loss: 3.731763]\n",
      "epoch:39 step:36647 [D loss: 0.050815, acc.: 100.00%] [G loss: 2.984542]\n",
      "epoch:39 step:36648 [D loss: 0.108324, acc.: 97.66%] [G loss: 1.643437]\n",
      "epoch:39 step:36649 [D loss: 0.108660, acc.: 98.44%] [G loss: 4.758398]\n",
      "epoch:39 step:36650 [D loss: 0.004096, acc.: 100.00%] [G loss: 6.629512]\n",
      "epoch:39 step:36651 [D loss: 0.084929, acc.: 98.44%] [G loss: 2.981003]\n",
      "epoch:39 step:36652 [D loss: 0.407362, acc.: 71.09%] [G loss: 0.858843]\n",
      "epoch:39 step:36653 [D loss: 0.215276, acc.: 96.09%] [G loss: 3.664576]\n",
      "epoch:39 step:36654 [D loss: 0.130735, acc.: 98.44%] [G loss: 6.484073]\n",
      "epoch:39 step:36655 [D loss: 0.101766, acc.: 97.66%] [G loss: 2.643101]\n",
      "epoch:39 step:36656 [D loss: 0.223074, acc.: 90.62%] [G loss: 1.325140]\n",
      "epoch:39 step:36657 [D loss: 0.116659, acc.: 97.66%] [G loss: 7.411663]\n",
      "epoch:39 step:36658 [D loss: 0.257939, acc.: 92.19%] [G loss: 4.951622]\n",
      "epoch:39 step:36659 [D loss: 1.182123, acc.: 54.69%] [G loss: 6.600867]\n",
      "epoch:39 step:36660 [D loss: 0.479718, acc.: 72.66%] [G loss: 8.029467]\n",
      "epoch:39 step:36661 [D loss: 0.821953, acc.: 58.59%] [G loss: 3.780257]\n",
      "epoch:39 step:36662 [D loss: 0.040049, acc.: 99.22%] [G loss: 2.193102]\n",
      "epoch:39 step:36663 [D loss: 0.342543, acc.: 78.91%] [G loss: 4.678811]\n",
      "epoch:39 step:36664 [D loss: 0.094750, acc.: 97.66%] [G loss: 2.985507]\n",
      "epoch:39 step:36665 [D loss: 0.054977, acc.: 99.22%] [G loss: 4.700123]\n",
      "epoch:39 step:36666 [D loss: 0.003615, acc.: 100.00%] [G loss: 3.812380]\n",
      "epoch:39 step:36667 [D loss: 0.249269, acc.: 95.31%] [G loss: 3.751690]\n",
      "epoch:39 step:36668 [D loss: 0.066122, acc.: 100.00%] [G loss: 5.118760]\n",
      "epoch:39 step:36669 [D loss: 0.810177, acc.: 50.00%] [G loss: 4.210182]\n",
      "epoch:39 step:36670 [D loss: 0.073325, acc.: 98.44%] [G loss: 6.217846]\n",
      "epoch:39 step:36671 [D loss: 0.079266, acc.: 98.44%] [G loss: 3.589358]\n",
      "epoch:39 step:36672 [D loss: 0.057854, acc.: 99.22%] [G loss: 4.666116]\n",
      "epoch:39 step:36673 [D loss: 0.295918, acc.: 89.06%] [G loss: 2.655236]\n",
      "epoch:39 step:36674 [D loss: 0.050404, acc.: 97.66%] [G loss: 4.846633]\n",
      "epoch:39 step:36675 [D loss: 1.196660, acc.: 30.47%] [G loss: 6.757629]\n",
      "epoch:39 step:36676 [D loss: 0.154290, acc.: 98.44%] [G loss: 4.825228]\n",
      "epoch:39 step:36677 [D loss: 0.948601, acc.: 59.38%] [G loss: 2.644716]\n",
      "epoch:39 step:36678 [D loss: 0.031762, acc.: 100.00%] [G loss: 2.801595]\n",
      "epoch:39 step:36679 [D loss: 0.017045, acc.: 100.00%] [G loss: 1.665864]\n",
      "epoch:39 step:36680 [D loss: 0.296940, acc.: 85.16%] [G loss: 2.203905]\n",
      "epoch:39 step:36681 [D loss: 0.133438, acc.: 97.66%] [G loss: 4.701278]\n",
      "epoch:39 step:36682 [D loss: 0.098513, acc.: 98.44%] [G loss: 2.265396]\n",
      "epoch:39 step:36683 [D loss: 0.082247, acc.: 99.22%] [G loss: 1.284506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:36684 [D loss: 0.125438, acc.: 98.44%] [G loss: 5.535139]\n",
      "epoch:39 step:36685 [D loss: 0.214762, acc.: 93.75%] [G loss: 0.970528]\n",
      "epoch:39 step:36686 [D loss: 0.544760, acc.: 75.00%] [G loss: 1.802207]\n",
      "epoch:39 step:36687 [D loss: 0.845296, acc.: 57.03%] [G loss: 6.523662]\n",
      "epoch:39 step:36688 [D loss: 0.074882, acc.: 99.22%] [G loss: 4.021806]\n",
      "epoch:39 step:36689 [D loss: 0.190043, acc.: 92.97%] [G loss: 4.605103]\n",
      "epoch:39 step:36690 [D loss: 0.357887, acc.: 76.56%] [G loss: 3.671791]\n",
      "epoch:39 step:36691 [D loss: 0.139919, acc.: 96.09%] [G loss: 3.478405]\n",
      "epoch:39 step:36692 [D loss: 0.169046, acc.: 95.31%] [G loss: 5.651604]\n",
      "epoch:39 step:36693 [D loss: 0.004449, acc.: 100.00%] [G loss: 1.689421]\n",
      "epoch:39 step:36694 [D loss: 0.011357, acc.: 100.00%] [G loss: 0.510632]\n",
      "epoch:39 step:36695 [D loss: 0.024671, acc.: 100.00%] [G loss: 7.527359]\n",
      "epoch:39 step:36696 [D loss: 0.241962, acc.: 94.53%] [G loss: 4.162216]\n",
      "epoch:39 step:36697 [D loss: 0.028381, acc.: 100.00%] [G loss: 6.945764]\n",
      "epoch:39 step:36698 [D loss: 0.039358, acc.: 99.22%] [G loss: 2.185763]\n",
      "epoch:39 step:36699 [D loss: 0.048420, acc.: 100.00%] [G loss: 5.382286]\n",
      "epoch:39 step:36700 [D loss: 0.439564, acc.: 74.22%] [G loss: 3.894903]\n",
      "epoch:39 step:36701 [D loss: 0.251898, acc.: 87.50%] [G loss: 0.681774]\n",
      "epoch:39 step:36702 [D loss: 0.021804, acc.: 100.00%] [G loss: 2.577212]\n",
      "epoch:39 step:36703 [D loss: 0.445168, acc.: 75.78%] [G loss: 7.341051]\n",
      "epoch:39 step:36704 [D loss: 0.506610, acc.: 75.78%] [G loss: 2.618299]\n",
      "epoch:39 step:36705 [D loss: 0.056905, acc.: 99.22%] [G loss: 3.361561]\n",
      "epoch:39 step:36706 [D loss: 0.122856, acc.: 96.88%] [G loss: 2.185856]\n",
      "epoch:39 step:36707 [D loss: 0.039971, acc.: 99.22%] [G loss: 4.765459]\n",
      "epoch:39 step:36708 [D loss: 0.052179, acc.: 99.22%] [G loss: 3.477806]\n",
      "epoch:39 step:36709 [D loss: 0.096944, acc.: 98.44%] [G loss: 0.766319]\n",
      "epoch:39 step:36710 [D loss: 1.892063, acc.: 50.00%] [G loss: 5.006186]\n",
      "epoch:39 step:36711 [D loss: 0.452941, acc.: 71.88%] [G loss: 2.051716]\n",
      "epoch:39 step:36712 [D loss: 0.534407, acc.: 69.53%] [G loss: 3.383756]\n",
      "epoch:39 step:36713 [D loss: 0.098451, acc.: 100.00%] [G loss: 1.801606]\n",
      "epoch:39 step:36714 [D loss: 0.708924, acc.: 59.38%] [G loss: 1.618002]\n",
      "epoch:39 step:36715 [D loss: 0.599318, acc.: 67.19%] [G loss: 4.332872]\n",
      "epoch:39 step:36716 [D loss: 0.138068, acc.: 96.88%] [G loss: 7.879531]\n",
      "epoch:39 step:36717 [D loss: 0.084614, acc.: 98.44%] [G loss: 8.891243]\n",
      "epoch:39 step:36718 [D loss: 0.012339, acc.: 100.00%] [G loss: 5.929556]\n",
      "epoch:39 step:36719 [D loss: 0.140027, acc.: 99.22%] [G loss: 3.510492]\n",
      "epoch:39 step:36720 [D loss: 0.259492, acc.: 93.75%] [G loss: 4.365224]\n",
      "epoch:39 step:36721 [D loss: 0.032889, acc.: 99.22%] [G loss: 4.093243]\n",
      "epoch:39 step:36722 [D loss: 0.015402, acc.: 100.00%] [G loss: 6.801467]\n",
      "epoch:39 step:36723 [D loss: 0.062753, acc.: 100.00%] [G loss: 2.011431]\n",
      "epoch:39 step:36724 [D loss: 0.321067, acc.: 82.03%] [G loss: 2.737415]\n",
      "epoch:39 step:36725 [D loss: 0.107305, acc.: 97.66%] [G loss: 3.270871]\n",
      "epoch:39 step:36726 [D loss: 0.225179, acc.: 92.97%] [G loss: 3.453635]\n",
      "epoch:39 step:36727 [D loss: 1.593952, acc.: 50.78%] [G loss: 3.801957]\n",
      "epoch:39 step:36728 [D loss: 0.096888, acc.: 99.22%] [G loss: 4.214641]\n",
      "epoch:39 step:36729 [D loss: 0.073445, acc.: 98.44%] [G loss: 6.028332]\n",
      "epoch:39 step:36730 [D loss: 0.675444, acc.: 54.69%] [G loss: 3.484847]\n",
      "epoch:39 step:36731 [D loss: 0.057663, acc.: 100.00%] [G loss: 2.443953]\n",
      "epoch:39 step:36732 [D loss: 0.054492, acc.: 100.00%] [G loss: 5.459479]\n",
      "epoch:39 step:36733 [D loss: 0.125297, acc.: 97.66%] [G loss: 5.937149]\n",
      "epoch:39 step:36734 [D loss: 0.079963, acc.: 98.44%] [G loss: 4.150297]\n",
      "epoch:39 step:36735 [D loss: 0.344606, acc.: 85.16%] [G loss: 4.395710]\n",
      "epoch:39 step:36736 [D loss: 0.160431, acc.: 96.88%] [G loss: 3.148179]\n",
      "epoch:39 step:36737 [D loss: 0.158420, acc.: 96.09%] [G loss: 2.115820]\n",
      "epoch:39 step:36738 [D loss: 0.503440, acc.: 65.62%] [G loss: 5.065611]\n",
      "epoch:39 step:36739 [D loss: 0.152525, acc.: 96.88%] [G loss: 2.220227]\n",
      "epoch:39 step:36740 [D loss: 0.047077, acc.: 100.00%] [G loss: 6.247727]\n",
      "epoch:39 step:36741 [D loss: 0.327186, acc.: 84.38%] [G loss: 4.730130]\n",
      "epoch:39 step:36742 [D loss: 0.164704, acc.: 98.44%] [G loss: 3.587646]\n",
      "epoch:39 step:36743 [D loss: 0.035260, acc.: 99.22%] [G loss: 2.229743]\n",
      "epoch:39 step:36744 [D loss: 0.085826, acc.: 98.44%] [G loss: 2.840289]\n",
      "epoch:39 step:36745 [D loss: 0.153449, acc.: 96.09%] [G loss: 4.283444]\n",
      "epoch:39 step:36746 [D loss: 0.563670, acc.: 66.41%] [G loss: 3.154631]\n",
      "epoch:39 step:36747 [D loss: 0.024044, acc.: 100.00%] [G loss: 6.735752]\n",
      "epoch:39 step:36748 [D loss: 0.041470, acc.: 99.22%] [G loss: 3.960171]\n",
      "epoch:39 step:36749 [D loss: 0.113299, acc.: 98.44%] [G loss: 4.835053]\n",
      "epoch:39 step:36750 [D loss: 0.142708, acc.: 98.44%] [G loss: 5.124521]\n",
      "epoch:39 step:36751 [D loss: 0.426490, acc.: 73.44%] [G loss: 1.663784]\n",
      "epoch:39 step:36752 [D loss: 0.147965, acc.: 95.31%] [G loss: 0.946695]\n",
      "epoch:39 step:36753 [D loss: 0.132075, acc.: 96.09%] [G loss: 4.461576]\n",
      "epoch:39 step:36754 [D loss: 0.391213, acc.: 83.59%] [G loss: 3.098047]\n",
      "epoch:39 step:36755 [D loss: 0.027509, acc.: 99.22%] [G loss: 3.935717]\n",
      "epoch:39 step:36756 [D loss: 0.147845, acc.: 95.31%] [G loss: 4.364069]\n",
      "epoch:39 step:36757 [D loss: 0.176629, acc.: 96.88%] [G loss: 0.520139]\n",
      "epoch:39 step:36758 [D loss: 0.268727, acc.: 89.84%] [G loss: 1.686597]\n",
      "epoch:39 step:36759 [D loss: 0.163504, acc.: 95.31%] [G loss: 0.887432]\n",
      "epoch:39 step:36760 [D loss: 0.156513, acc.: 97.66%] [G loss: 1.683614]\n",
      "epoch:39 step:36761 [D loss: 0.029331, acc.: 100.00%] [G loss: 0.986220]\n",
      "epoch:39 step:36762 [D loss: 0.077609, acc.: 100.00%] [G loss: 2.049236]\n",
      "epoch:39 step:36763 [D loss: 0.567588, acc.: 67.19%] [G loss: 4.872610]\n",
      "epoch:39 step:36764 [D loss: 0.328426, acc.: 85.16%] [G loss: 4.362090]\n",
      "epoch:39 step:36765 [D loss: 0.319936, acc.: 89.84%] [G loss: 3.803187]\n",
      "epoch:39 step:36766 [D loss: 0.121989, acc.: 99.22%] [G loss: 5.303799]\n",
      "epoch:39 step:36767 [D loss: 2.405114, acc.: 50.78%] [G loss: 4.596012]\n",
      "epoch:39 step:36768 [D loss: 0.093271, acc.: 100.00%] [G loss: 2.964770]\n",
      "epoch:39 step:36769 [D loss: 0.050824, acc.: 100.00%] [G loss: 0.692556]\n",
      "epoch:39 step:36770 [D loss: 0.020235, acc.: 100.00%] [G loss: 2.272406]\n",
      "epoch:39 step:36771 [D loss: 0.599649, acc.: 67.97%] [G loss: 1.613526]\n",
      "epoch:39 step:36772 [D loss: 0.120318, acc.: 97.66%] [G loss: 1.845739]\n",
      "epoch:39 step:36773 [D loss: 0.095352, acc.: 100.00%] [G loss: 2.018537]\n",
      "epoch:39 step:36774 [D loss: 0.198629, acc.: 93.75%] [G loss: 4.243076]\n",
      "epoch:39 step:36775 [D loss: 0.006566, acc.: 100.00%] [G loss: 4.699185]\n",
      "epoch:39 step:36776 [D loss: 0.025757, acc.: 100.00%] [G loss: 2.579603]\n",
      "epoch:39 step:36777 [D loss: 0.212593, acc.: 93.75%] [G loss: 4.600559]\n",
      "epoch:39 step:36778 [D loss: 0.197256, acc.: 92.97%] [G loss: 2.736831]\n",
      "epoch:39 step:36779 [D loss: 0.346005, acc.: 78.91%] [G loss: 1.965577]\n",
      "epoch:39 step:36780 [D loss: 0.081212, acc.: 99.22%] [G loss: 1.725311]\n",
      "epoch:39 step:36781 [D loss: 0.099663, acc.: 97.66%] [G loss: 4.730837]\n",
      "epoch:39 step:36782 [D loss: 0.540180, acc.: 70.31%] [G loss: 3.238727]\n",
      "epoch:39 step:36783 [D loss: 0.071007, acc.: 98.44%] [G loss: 0.473822]\n",
      "epoch:39 step:36784 [D loss: 0.073362, acc.: 100.00%] [G loss: 2.544649]\n",
      "epoch:39 step:36785 [D loss: 0.959267, acc.: 52.34%] [G loss: 1.221235]\n",
      "epoch:39 step:36786 [D loss: 0.874307, acc.: 59.38%] [G loss: 1.167338]\n",
      "epoch:39 step:36787 [D loss: 0.056141, acc.: 99.22%] [G loss: 3.846958]\n",
      "epoch:39 step:36788 [D loss: 0.101877, acc.: 98.44%] [G loss: 4.575336]\n",
      "epoch:39 step:36789 [D loss: 0.262733, acc.: 89.84%] [G loss: 2.651384]\n",
      "epoch:39 step:36790 [D loss: 0.016745, acc.: 100.00%] [G loss: 4.267889]\n",
      "epoch:39 step:36791 [D loss: 0.051245, acc.: 99.22%] [G loss: 4.091045]\n",
      "epoch:39 step:36792 [D loss: 0.094218, acc.: 99.22%] [G loss: 6.076803]\n",
      "epoch:39 step:36793 [D loss: 0.039681, acc.: 100.00%] [G loss: 2.163436]\n",
      "epoch:39 step:36794 [D loss: 0.284675, acc.: 93.75%] [G loss: 2.442034]\n",
      "epoch:39 step:36795 [D loss: 0.278584, acc.: 85.16%] [G loss: 3.118706]\n",
      "epoch:39 step:36796 [D loss: 0.004646, acc.: 100.00%] [G loss: 5.751063]\n",
      "epoch:39 step:36797 [D loss: 0.347791, acc.: 84.38%] [G loss: 4.337986]\n",
      "epoch:39 step:36798 [D loss: 0.058190, acc.: 97.66%] [G loss: 5.378408]\n",
      "epoch:39 step:36799 [D loss: 0.227099, acc.: 89.06%] [G loss: 2.289795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:36800 [D loss: 0.076560, acc.: 99.22%] [G loss: 1.552954]\n",
      "epoch:39 step:36801 [D loss: 0.178809, acc.: 92.97%] [G loss: 3.320430]\n",
      "epoch:39 step:36802 [D loss: 0.358267, acc.: 84.38%] [G loss: 6.751790]\n",
      "epoch:39 step:36803 [D loss: 0.111330, acc.: 98.44%] [G loss: 4.458791]\n",
      "epoch:39 step:36804 [D loss: 0.299298, acc.: 90.62%] [G loss: 11.262780]\n",
      "epoch:39 step:36805 [D loss: 0.067076, acc.: 97.66%] [G loss: 5.458358]\n",
      "epoch:39 step:36806 [D loss: 0.887879, acc.: 55.47%] [G loss: 4.684927]\n",
      "epoch:39 step:36807 [D loss: 0.164498, acc.: 94.53%] [G loss: 5.257071]\n",
      "epoch:39 step:36808 [D loss: 0.119371, acc.: 96.88%] [G loss: 3.700089]\n",
      "epoch:39 step:36809 [D loss: 0.046136, acc.: 99.22%] [G loss: 1.420753]\n",
      "epoch:39 step:36810 [D loss: 0.050340, acc.: 100.00%] [G loss: 6.324836]\n",
      "epoch:39 step:36811 [D loss: 0.183504, acc.: 96.09%] [G loss: 2.707515]\n",
      "epoch:39 step:36812 [D loss: 0.362111, acc.: 76.56%] [G loss: 2.345936]\n",
      "epoch:39 step:36813 [D loss: 0.322947, acc.: 88.28%] [G loss: 5.858404]\n",
      "epoch:39 step:36814 [D loss: 0.054884, acc.: 100.00%] [G loss: 7.061176]\n",
      "epoch:39 step:36815 [D loss: 0.348576, acc.: 81.25%] [G loss: 8.529442]\n",
      "epoch:39 step:36816 [D loss: 0.290281, acc.: 86.72%] [G loss: 2.400961]\n",
      "epoch:39 step:36817 [D loss: 0.423364, acc.: 74.22%] [G loss: 3.962542]\n",
      "epoch:39 step:36818 [D loss: 0.197739, acc.: 95.31%] [G loss: 10.144268]\n",
      "epoch:39 step:36819 [D loss: 0.183263, acc.: 92.97%] [G loss: 5.082193]\n",
      "epoch:39 step:36820 [D loss: 0.130988, acc.: 96.88%] [G loss: 5.362118]\n",
      "epoch:39 step:36821 [D loss: 0.054422, acc.: 99.22%] [G loss: 5.180450]\n",
      "epoch:39 step:36822 [D loss: 0.031103, acc.: 99.22%] [G loss: 3.930325]\n",
      "epoch:39 step:36823 [D loss: 0.209485, acc.: 95.31%] [G loss: 1.816065]\n",
      "epoch:39 step:36824 [D loss: 0.042817, acc.: 99.22%] [G loss: 3.383663]\n",
      "epoch:39 step:36825 [D loss: 0.064207, acc.: 98.44%] [G loss: 6.301102]\n",
      "epoch:39 step:36826 [D loss: 0.119192, acc.: 96.09%] [G loss: 1.099029]\n",
      "epoch:39 step:36827 [D loss: 0.249012, acc.: 89.06%] [G loss: 4.611772]\n",
      "epoch:39 step:36828 [D loss: 0.057131, acc.: 98.44%] [G loss: 5.212248]\n",
      "epoch:39 step:36829 [D loss: 0.218450, acc.: 89.06%] [G loss: 2.866875]\n",
      "epoch:39 step:36830 [D loss: 0.449770, acc.: 77.34%] [G loss: 4.262681]\n",
      "epoch:39 step:36831 [D loss: 0.152675, acc.: 96.09%] [G loss: 5.327513]\n",
      "epoch:39 step:36832 [D loss: 0.279212, acc.: 90.62%] [G loss: 6.669567]\n",
      "epoch:39 step:36833 [D loss: 0.264550, acc.: 89.84%] [G loss: 2.775353]\n",
      "epoch:39 step:36834 [D loss: 0.058112, acc.: 100.00%] [G loss: 4.715314]\n",
      "epoch:39 step:36835 [D loss: 0.190336, acc.: 95.31%] [G loss: 4.857968]\n",
      "epoch:39 step:36836 [D loss: 0.053710, acc.: 99.22%] [G loss: 4.515275]\n",
      "epoch:39 step:36837 [D loss: 0.193919, acc.: 92.19%] [G loss: 4.388002]\n",
      "epoch:39 step:36838 [D loss: 0.669300, acc.: 64.06%] [G loss: 0.872657]\n",
      "epoch:39 step:36839 [D loss: 0.062242, acc.: 99.22%] [G loss: 6.423162]\n",
      "epoch:39 step:36840 [D loss: 0.019675, acc.: 99.22%] [G loss: 6.172463]\n",
      "epoch:39 step:36841 [D loss: 0.725513, acc.: 63.28%] [G loss: 4.005456]\n",
      "epoch:39 step:36842 [D loss: 1.172612, acc.: 51.56%] [G loss: 0.623886]\n",
      "epoch:39 step:36843 [D loss: 0.259107, acc.: 89.06%] [G loss: 6.663549]\n",
      "epoch:39 step:36844 [D loss: 0.364712, acc.: 82.03%] [G loss: 4.149128]\n",
      "epoch:39 step:36845 [D loss: 0.205507, acc.: 92.19%] [G loss: 1.627419]\n",
      "epoch:39 step:36846 [D loss: 0.404707, acc.: 75.78%] [G loss: 6.716353]\n",
      "epoch:39 step:36847 [D loss: 0.189111, acc.: 92.19%] [G loss: 5.464835]\n",
      "epoch:39 step:36848 [D loss: 0.051346, acc.: 98.44%] [G loss: 5.818370]\n",
      "epoch:39 step:36849 [D loss: 0.186086, acc.: 96.88%] [G loss: 6.836497]\n",
      "epoch:39 step:36850 [D loss: 0.069475, acc.: 99.22%] [G loss: 3.220501]\n",
      "epoch:39 step:36851 [D loss: 0.000730, acc.: 100.00%] [G loss: 4.174234]\n",
      "epoch:39 step:36852 [D loss: 0.036684, acc.: 100.00%] [G loss: 3.897358]\n",
      "epoch:39 step:36853 [D loss: 0.094103, acc.: 99.22%] [G loss: 5.414445]\n",
      "epoch:39 step:36854 [D loss: 0.039759, acc.: 100.00%] [G loss: 6.112693]\n",
      "epoch:39 step:36855 [D loss: 0.010479, acc.: 100.00%] [G loss: 2.455893]\n",
      "epoch:39 step:36856 [D loss: 0.542485, acc.: 68.75%] [G loss: 1.296451]\n",
      "epoch:39 step:36857 [D loss: 0.512002, acc.: 67.19%] [G loss: 4.126771]\n",
      "epoch:39 step:36858 [D loss: 0.076076, acc.: 99.22%] [G loss: 6.729463]\n",
      "epoch:39 step:36859 [D loss: 0.003786, acc.: 100.00%] [G loss: 5.979087]\n",
      "epoch:39 step:36860 [D loss: 0.551239, acc.: 67.19%] [G loss: 3.192393]\n",
      "epoch:39 step:36861 [D loss: 0.018217, acc.: 100.00%] [G loss: 0.301975]\n",
      "epoch:39 step:36862 [D loss: 0.068361, acc.: 100.00%] [G loss: 1.733505]\n",
      "epoch:39 step:36863 [D loss: 0.138729, acc.: 97.66%] [G loss: 3.613695]\n",
      "epoch:39 step:36864 [D loss: 0.532341, acc.: 71.09%] [G loss: 5.282601]\n",
      "epoch:39 step:36865 [D loss: 0.030725, acc.: 100.00%] [G loss: 3.553723]\n",
      "epoch:39 step:36866 [D loss: 0.022000, acc.: 100.00%] [G loss: 3.731059]\n",
      "epoch:39 step:36867 [D loss: 0.370208, acc.: 79.69%] [G loss: 5.085344]\n",
      "epoch:39 step:36868 [D loss: 0.016221, acc.: 100.00%] [G loss: 7.776199]\n",
      "epoch:39 step:36869 [D loss: 1.037161, acc.: 57.03%] [G loss: 6.838662]\n",
      "epoch:39 step:36870 [D loss: 0.477456, acc.: 73.44%] [G loss: 5.012717]\n",
      "epoch:39 step:36871 [D loss: 0.036114, acc.: 99.22%] [G loss: 3.377788]\n",
      "epoch:39 step:36872 [D loss: 0.032577, acc.: 100.00%] [G loss: 4.850494]\n",
      "epoch:39 step:36873 [D loss: 0.284236, acc.: 86.72%] [G loss: 2.658403]\n",
      "epoch:39 step:36874 [D loss: 0.025555, acc.: 100.00%] [G loss: 4.098650]\n",
      "epoch:39 step:36875 [D loss: 0.402950, acc.: 77.34%] [G loss: 2.298141]\n",
      "epoch:39 step:36876 [D loss: 0.113200, acc.: 98.44%] [G loss: 5.426197]\n",
      "epoch:39 step:36877 [D loss: 0.143526, acc.: 98.44%] [G loss: 1.440089]\n",
      "epoch:39 step:36878 [D loss: 0.275598, acc.: 90.62%] [G loss: 5.902549]\n",
      "epoch:39 step:36879 [D loss: 0.050465, acc.: 99.22%] [G loss: 4.139825]\n",
      "epoch:39 step:36880 [D loss: 0.169706, acc.: 93.75%] [G loss: 6.061688]\n",
      "epoch:39 step:36881 [D loss: 0.036756, acc.: 98.44%] [G loss: 2.217744]\n",
      "epoch:39 step:36882 [D loss: 0.156837, acc.: 99.22%] [G loss: 4.654779]\n",
      "epoch:39 step:36883 [D loss: 2.346846, acc.: 7.81%] [G loss: 4.276748]\n",
      "epoch:39 step:36884 [D loss: 0.689250, acc.: 69.53%] [G loss: 1.247436]\n",
      "epoch:39 step:36885 [D loss: 0.027203, acc.: 100.00%] [G loss: 6.884910]\n",
      "epoch:39 step:36886 [D loss: 0.157075, acc.: 93.75%] [G loss: 3.548953]\n",
      "epoch:39 step:36887 [D loss: 0.012167, acc.: 100.00%] [G loss: 1.783341]\n",
      "epoch:39 step:36888 [D loss: 0.115791, acc.: 96.88%] [G loss: 2.349832]\n",
      "epoch:39 step:36889 [D loss: 0.069024, acc.: 98.44%] [G loss: 5.138410]\n",
      "epoch:39 step:36890 [D loss: 0.105033, acc.: 100.00%] [G loss: 3.693644]\n",
      "epoch:39 step:36891 [D loss: 0.035825, acc.: 99.22%] [G loss: 0.375875]\n",
      "epoch:39 step:36892 [D loss: 0.420131, acc.: 85.16%] [G loss: 2.129737]\n",
      "epoch:39 step:36893 [D loss: 0.111647, acc.: 94.53%] [G loss: 3.505279]\n",
      "epoch:39 step:36894 [D loss: 0.052903, acc.: 100.00%] [G loss: 1.386063]\n",
      "epoch:39 step:36895 [D loss: 0.038190, acc.: 100.00%] [G loss: 7.770643]\n",
      "epoch:39 step:36896 [D loss: 0.010273, acc.: 100.00%] [G loss: 3.484283]\n",
      "epoch:39 step:36897 [D loss: 0.043561, acc.: 100.00%] [G loss: 5.809741]\n",
      "epoch:39 step:36898 [D loss: 0.014150, acc.: 100.00%] [G loss: 6.330816]\n",
      "epoch:39 step:36899 [D loss: 0.009017, acc.: 100.00%] [G loss: 3.245140]\n",
      "epoch:39 step:36900 [D loss: 0.370255, acc.: 78.91%] [G loss: 4.690896]\n",
      "epoch:39 step:36901 [D loss: 0.085115, acc.: 99.22%] [G loss: 2.258985]\n",
      "epoch:39 step:36902 [D loss: 0.794542, acc.: 56.25%] [G loss: 5.621358]\n",
      "epoch:39 step:36903 [D loss: 0.003234, acc.: 100.00%] [G loss: 5.934607]\n",
      "epoch:39 step:36904 [D loss: 0.029548, acc.: 100.00%] [G loss: 4.399646]\n",
      "epoch:39 step:36905 [D loss: 0.388348, acc.: 82.03%] [G loss: 2.898544]\n",
      "epoch:39 step:36906 [D loss: 0.067526, acc.: 96.88%] [G loss: 6.142933]\n",
      "epoch:39 step:36907 [D loss: 0.078729, acc.: 97.66%] [G loss: 3.472894]\n",
      "epoch:39 step:36908 [D loss: 0.218016, acc.: 95.31%] [G loss: 4.725734]\n",
      "epoch:39 step:36909 [D loss: 0.139407, acc.: 98.44%] [G loss: 5.605433]\n",
      "epoch:39 step:36910 [D loss: 0.211365, acc.: 94.53%] [G loss: 4.024322]\n",
      "epoch:39 step:36911 [D loss: 0.170048, acc.: 96.88%] [G loss: 5.094507]\n",
      "epoch:39 step:36912 [D loss: 0.213457, acc.: 93.75%] [G loss: 3.747608]\n",
      "epoch:39 step:36913 [D loss: 0.015465, acc.: 100.00%] [G loss: 6.604018]\n",
      "epoch:39 step:36914 [D loss: 0.064800, acc.: 98.44%] [G loss: 7.202557]\n",
      "epoch:39 step:36915 [D loss: 0.032546, acc.: 100.00%] [G loss: 7.707902]\n",
      "epoch:39 step:36916 [D loss: 0.134400, acc.: 97.66%] [G loss: 3.219285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:36917 [D loss: 0.244721, acc.: 90.62%] [G loss: 7.080432]\n",
      "epoch:39 step:36918 [D loss: 0.082192, acc.: 97.66%] [G loss: 2.914618]\n",
      "epoch:39 step:36919 [D loss: 0.320487, acc.: 82.81%] [G loss: 4.477646]\n",
      "epoch:39 step:36920 [D loss: 0.064131, acc.: 100.00%] [G loss: 6.139784]\n",
      "epoch:39 step:36921 [D loss: 0.007716, acc.: 100.00%] [G loss: 5.686540]\n",
      "epoch:39 step:36922 [D loss: 0.024302, acc.: 100.00%] [G loss: 1.876240]\n",
      "epoch:39 step:36923 [D loss: 1.784943, acc.: 50.00%] [G loss: 3.795371]\n",
      "epoch:39 step:36924 [D loss: 0.458247, acc.: 74.22%] [G loss: 1.264964]\n",
      "epoch:39 step:36925 [D loss: 0.128614, acc.: 99.22%] [G loss: 1.645014]\n",
      "epoch:39 step:36926 [D loss: 0.309360, acc.: 86.72%] [G loss: 3.305944]\n",
      "epoch:39 step:36927 [D loss: 0.113577, acc.: 98.44%] [G loss: 0.592482]\n",
      "epoch:39 step:36928 [D loss: 0.181247, acc.: 96.88%] [G loss: 0.398572]\n",
      "epoch:39 step:36929 [D loss: 1.753644, acc.: 47.66%] [G loss: 4.949710]\n",
      "epoch:39 step:36930 [D loss: 0.182967, acc.: 94.53%] [G loss: 2.032295]\n",
      "epoch:39 step:36931 [D loss: 0.667914, acc.: 64.06%] [G loss: 2.010174]\n",
      "epoch:39 step:36932 [D loss: 1.181135, acc.: 29.69%] [G loss: 5.094723]\n",
      "epoch:39 step:36933 [D loss: 0.013712, acc.: 100.00%] [G loss: 3.687089]\n",
      "epoch:39 step:36934 [D loss: 0.074048, acc.: 100.00%] [G loss: 5.264675]\n",
      "epoch:39 step:36935 [D loss: 0.284537, acc.: 83.59%] [G loss: 1.578027]\n",
      "epoch:39 step:36936 [D loss: 0.163446, acc.: 96.09%] [G loss: 2.734006]\n",
      "epoch:39 step:36937 [D loss: 0.014636, acc.: 100.00%] [G loss: 3.235459]\n",
      "epoch:39 step:36938 [D loss: 0.107002, acc.: 99.22%] [G loss: 1.418476]\n",
      "epoch:39 step:36939 [D loss: 0.123861, acc.: 100.00%] [G loss: 4.164463]\n",
      "epoch:39 step:36940 [D loss: 0.531067, acc.: 66.41%] [G loss: 9.341573]\n",
      "epoch:39 step:36941 [D loss: 0.032851, acc.: 100.00%] [G loss: 1.648986]\n",
      "epoch:39 step:36942 [D loss: 0.039441, acc.: 100.00%] [G loss: 2.728768]\n",
      "epoch:39 step:36943 [D loss: 1.662960, acc.: 39.06%] [G loss: 6.245945]\n",
      "epoch:39 step:36944 [D loss: 0.358743, acc.: 81.25%] [G loss: 7.783062]\n",
      "epoch:39 step:36945 [D loss: 0.025312, acc.: 100.00%] [G loss: 7.021574]\n",
      "epoch:39 step:36946 [D loss: 0.287695, acc.: 89.84%] [G loss: 0.633689]\n",
      "epoch:39 step:36947 [D loss: 0.122014, acc.: 98.44%] [G loss: 2.700148]\n",
      "epoch:39 step:36948 [D loss: 0.099259, acc.: 97.66%] [G loss: 5.514908]\n",
      "epoch:39 step:36949 [D loss: 0.012183, acc.: 100.00%] [G loss: 7.335443]\n",
      "epoch:39 step:36950 [D loss: 0.066705, acc.: 100.00%] [G loss: 2.954806]\n",
      "epoch:39 step:36951 [D loss: 0.573016, acc.: 68.75%] [G loss: 5.076336]\n",
      "epoch:39 step:36952 [D loss: 0.532120, acc.: 71.09%] [G loss: 5.694359]\n",
      "epoch:39 step:36953 [D loss: 0.240479, acc.: 91.41%] [G loss: 1.499140]\n",
      "epoch:39 step:36954 [D loss: 0.244327, acc.: 94.53%] [G loss: 4.132584]\n",
      "epoch:39 step:36955 [D loss: 0.423816, acc.: 83.59%] [G loss: 2.089898]\n",
      "epoch:39 step:36956 [D loss: 0.190151, acc.: 94.53%] [G loss: 2.396158]\n",
      "epoch:39 step:36957 [D loss: 1.360400, acc.: 53.91%] [G loss: 2.385452]\n",
      "epoch:39 step:36958 [D loss: 0.216123, acc.: 92.97%] [G loss: 3.503527]\n",
      "epoch:39 step:36959 [D loss: 0.792711, acc.: 56.25%] [G loss: 5.707209]\n",
      "epoch:39 step:36960 [D loss: 0.141304, acc.: 97.66%] [G loss: 4.751970]\n",
      "epoch:39 step:36961 [D loss: 0.376527, acc.: 81.25%] [G loss: 2.870155]\n",
      "epoch:39 step:36962 [D loss: 0.032669, acc.: 100.00%] [G loss: 4.453442]\n",
      "epoch:39 step:36963 [D loss: 0.039191, acc.: 99.22%] [G loss: 2.737107]\n",
      "epoch:39 step:36964 [D loss: 0.226666, acc.: 92.19%] [G loss: 3.140570]\n",
      "epoch:39 step:36965 [D loss: 0.111984, acc.: 99.22%] [G loss: 5.031096]\n",
      "epoch:39 step:36966 [D loss: 0.069057, acc.: 98.44%] [G loss: 4.975028]\n",
      "epoch:39 step:36967 [D loss: 0.385495, acc.: 76.56%] [G loss: 4.064478]\n",
      "epoch:39 step:36968 [D loss: 0.107508, acc.: 100.00%] [G loss: 3.652617]\n",
      "epoch:39 step:36969 [D loss: 0.638541, acc.: 63.28%] [G loss: 1.458782]\n",
      "epoch:39 step:36970 [D loss: 0.036584, acc.: 100.00%] [G loss: 2.591405]\n",
      "epoch:39 step:36971 [D loss: 0.199014, acc.: 97.66%] [G loss: 2.500011]\n",
      "epoch:39 step:36972 [D loss: 0.082801, acc.: 99.22%] [G loss: 3.704545]\n",
      "epoch:39 step:36973 [D loss: 0.080380, acc.: 98.44%] [G loss: 2.184621]\n",
      "epoch:39 step:36974 [D loss: 0.348623, acc.: 89.84%] [G loss: 1.230923]\n",
      "epoch:39 step:36975 [D loss: 0.073389, acc.: 99.22%] [G loss: 5.266115]\n",
      "epoch:39 step:36976 [D loss: 0.101851, acc.: 99.22%] [G loss: 1.559664]\n",
      "epoch:39 step:36977 [D loss: 0.138681, acc.: 98.44%] [G loss: 0.753760]\n",
      "epoch:39 step:36978 [D loss: 0.046854, acc.: 100.00%] [G loss: 1.276548]\n",
      "epoch:39 step:36979 [D loss: 0.097375, acc.: 96.09%] [G loss: 1.500946]\n",
      "epoch:39 step:36980 [D loss: 0.300648, acc.: 85.16%] [G loss: 2.496529]\n",
      "epoch:39 step:36981 [D loss: 0.024777, acc.: 100.00%] [G loss: 2.650365]\n",
      "epoch:39 step:36982 [D loss: 0.134757, acc.: 96.09%] [G loss: 5.202844]\n",
      "epoch:39 step:36983 [D loss: 0.444807, acc.: 82.81%] [G loss: 5.868718]\n",
      "epoch:39 step:36984 [D loss: 0.060087, acc.: 100.00%] [G loss: 3.030507]\n",
      "epoch:39 step:36985 [D loss: 0.094203, acc.: 99.22%] [G loss: 2.450428]\n",
      "epoch:39 step:36986 [D loss: 2.336287, acc.: 10.16%] [G loss: 0.968446]\n",
      "epoch:39 step:36987 [D loss: 0.175373, acc.: 93.75%] [G loss: 4.494770]\n",
      "epoch:39 step:36988 [D loss: 0.176847, acc.: 96.09%] [G loss: 3.486813]\n",
      "epoch:39 step:36989 [D loss: 0.033280, acc.: 100.00%] [G loss: 5.269743]\n",
      "epoch:39 step:36990 [D loss: 0.241256, acc.: 89.84%] [G loss: 4.344578]\n",
      "epoch:39 step:36991 [D loss: 0.308719, acc.: 93.75%] [G loss: 1.568169]\n",
      "epoch:39 step:36992 [D loss: 0.219371, acc.: 91.41%] [G loss: 1.907789]\n",
      "epoch:39 step:36993 [D loss: 0.017723, acc.: 100.00%] [G loss: 1.882061]\n",
      "epoch:39 step:36994 [D loss: 0.469776, acc.: 67.97%] [G loss: 4.637171]\n",
      "epoch:39 step:36995 [D loss: 0.151387, acc.: 95.31%] [G loss: 2.774016]\n",
      "epoch:39 step:36996 [D loss: 1.319507, acc.: 45.31%] [G loss: 4.370119]\n",
      "epoch:39 step:36997 [D loss: 1.456129, acc.: 51.56%] [G loss: 3.978368]\n",
      "epoch:39 step:36998 [D loss: 0.071906, acc.: 97.66%] [G loss: 4.453396]\n",
      "epoch:39 step:36999 [D loss: 0.446741, acc.: 77.34%] [G loss: 2.718572]\n",
      "epoch:39 step:37000 [D loss: 0.335727, acc.: 85.94%] [G loss: 3.848667]\n",
      "epoch:39 step:37001 [D loss: 0.230131, acc.: 90.62%] [G loss: 3.735298]\n",
      "epoch:39 step:37002 [D loss: 0.044568, acc.: 99.22%] [G loss: 4.232885]\n",
      "epoch:39 step:37003 [D loss: 0.172250, acc.: 95.31%] [G loss: 1.765996]\n",
      "epoch:39 step:37004 [D loss: 1.475189, acc.: 53.91%] [G loss: 0.751801]\n",
      "epoch:39 step:37005 [D loss: 0.151843, acc.: 96.09%] [G loss: 2.492576]\n",
      "epoch:39 step:37006 [D loss: 0.103856, acc.: 100.00%] [G loss: 1.808675]\n",
      "epoch:39 step:37007 [D loss: 0.369338, acc.: 87.50%] [G loss: 1.595718]\n",
      "epoch:39 step:37008 [D loss: 0.206606, acc.: 94.53%] [G loss: 1.827861]\n",
      "epoch:39 step:37009 [D loss: 0.308214, acc.: 89.06%] [G loss: 3.109303]\n",
      "epoch:39 step:37010 [D loss: 0.015545, acc.: 100.00%] [G loss: 2.842122]\n",
      "epoch:39 step:37011 [D loss: 0.098608, acc.: 99.22%] [G loss: 5.611924]\n",
      "epoch:39 step:37012 [D loss: 0.203183, acc.: 93.75%] [G loss: 4.875603]\n",
      "epoch:39 step:37013 [D loss: 0.244241, acc.: 92.97%] [G loss: 4.269605]\n",
      "epoch:39 step:37014 [D loss: 0.093767, acc.: 100.00%] [G loss: 3.505795]\n",
      "epoch:39 step:37015 [D loss: 0.061039, acc.: 100.00%] [G loss: 1.300396]\n",
      "epoch:39 step:37016 [D loss: 0.152569, acc.: 99.22%] [G loss: 2.619702]\n",
      "epoch:39 step:37017 [D loss: 0.156061, acc.: 98.44%] [G loss: 2.461302]\n",
      "epoch:39 step:37018 [D loss: 0.075605, acc.: 99.22%] [G loss: 5.754469]\n",
      "epoch:39 step:37019 [D loss: 0.298932, acc.: 84.38%] [G loss: 6.125469]\n",
      "epoch:39 step:37020 [D loss: 0.079812, acc.: 100.00%] [G loss: 1.118484]\n",
      "epoch:39 step:37021 [D loss: 0.018600, acc.: 100.00%] [G loss: 4.967228]\n",
      "epoch:39 step:37022 [D loss: 0.277319, acc.: 91.41%] [G loss: 3.033159]\n",
      "epoch:39 step:37023 [D loss: 0.161412, acc.: 96.88%] [G loss: 3.863508]\n",
      "epoch:39 step:37024 [D loss: 0.054077, acc.: 99.22%] [G loss: 3.724100]\n",
      "epoch:39 step:37025 [D loss: 0.031023, acc.: 100.00%] [G loss: 2.988176]\n",
      "epoch:39 step:37026 [D loss: 0.321919, acc.: 86.72%] [G loss: 5.049504]\n",
      "epoch:39 step:37027 [D loss: 0.153542, acc.: 96.88%] [G loss: 3.684129]\n",
      "epoch:39 step:37028 [D loss: 0.038989, acc.: 99.22%] [G loss: 3.889306]\n",
      "epoch:39 step:37029 [D loss: 0.237657, acc.: 94.53%] [G loss: 2.068300]\n",
      "epoch:39 step:37030 [D loss: 0.214250, acc.: 92.19%] [G loss: 3.158830]\n",
      "epoch:39 step:37031 [D loss: 0.108452, acc.: 100.00%] [G loss: 1.014880]\n",
      "epoch:39 step:37032 [D loss: 0.582970, acc.: 61.72%] [G loss: 3.541900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:37033 [D loss: 0.366424, acc.: 78.91%] [G loss: 4.654339]\n",
      "epoch:39 step:37034 [D loss: 1.232648, acc.: 50.78%] [G loss: 3.174733]\n",
      "epoch:39 step:37035 [D loss: 0.049935, acc.: 100.00%] [G loss: 0.983762]\n",
      "epoch:39 step:37036 [D loss: 0.233154, acc.: 88.28%] [G loss: 2.991101]\n",
      "epoch:39 step:37037 [D loss: 0.114723, acc.: 97.66%] [G loss: 1.359714]\n",
      "epoch:39 step:37038 [D loss: 0.045545, acc.: 100.00%] [G loss: 0.218526]\n",
      "epoch:39 step:37039 [D loss: 0.083472, acc.: 98.44%] [G loss: 3.662670]\n",
      "epoch:39 step:37040 [D loss: 0.694690, acc.: 62.50%] [G loss: 0.343251]\n",
      "epoch:39 step:37041 [D loss: 0.346057, acc.: 82.81%] [G loss: 0.133153]\n",
      "epoch:39 step:37042 [D loss: 0.039874, acc.: 99.22%] [G loss: 1.698944]\n",
      "epoch:39 step:37043 [D loss: 0.066032, acc.: 100.00%] [G loss: 3.337168]\n",
      "epoch:39 step:37044 [D loss: 0.651064, acc.: 63.28%] [G loss: 4.737094]\n",
      "epoch:39 step:37045 [D loss: 0.077283, acc.: 98.44%] [G loss: 1.526301]\n",
      "epoch:39 step:37046 [D loss: 0.120718, acc.: 98.44%] [G loss: 0.667136]\n",
      "epoch:39 step:37047 [D loss: 1.374310, acc.: 50.00%] [G loss: 2.681037]\n",
      "epoch:39 step:37048 [D loss: 0.025066, acc.: 100.00%] [G loss: 5.687099]\n",
      "epoch:39 step:37049 [D loss: 1.142971, acc.: 55.47%] [G loss: 5.346429]\n",
      "epoch:39 step:37050 [D loss: 0.275617, acc.: 92.19%] [G loss: 4.439421]\n",
      "epoch:39 step:37051 [D loss: 0.015019, acc.: 100.00%] [G loss: 4.269049]\n",
      "epoch:39 step:37052 [D loss: 0.092129, acc.: 96.09%] [G loss: 5.411896]\n",
      "epoch:39 step:37053 [D loss: 0.087337, acc.: 96.09%] [G loss: 1.975207]\n",
      "epoch:39 step:37054 [D loss: 0.092591, acc.: 100.00%] [G loss: 4.732172]\n",
      "epoch:39 step:37055 [D loss: 0.672386, acc.: 63.28%] [G loss: 1.414840]\n",
      "epoch:39 step:37056 [D loss: 0.081731, acc.: 97.66%] [G loss: 3.467915]\n",
      "epoch:39 step:37057 [D loss: 0.135346, acc.: 96.88%] [G loss: 5.009192]\n",
      "epoch:39 step:37058 [D loss: 0.070355, acc.: 98.44%] [G loss: 1.842676]\n",
      "epoch:39 step:37059 [D loss: 0.469760, acc.: 84.38%] [G loss: 1.429630]\n",
      "epoch:39 step:37060 [D loss: 0.056581, acc.: 99.22%] [G loss: 3.456209]\n",
      "epoch:39 step:37061 [D loss: 0.217980, acc.: 93.75%] [G loss: 1.371657]\n",
      "epoch:39 step:37062 [D loss: 0.139156, acc.: 96.88%] [G loss: 3.014925]\n",
      "epoch:39 step:37063 [D loss: 0.160399, acc.: 95.31%] [G loss: 4.997595]\n",
      "epoch:39 step:37064 [D loss: 0.197839, acc.: 92.97%] [G loss: 3.697992]\n",
      "epoch:39 step:37065 [D loss: 0.148720, acc.: 97.66%] [G loss: 4.438377]\n",
      "epoch:39 step:37066 [D loss: 0.092394, acc.: 99.22%] [G loss: 8.713312]\n",
      "epoch:39 step:37067 [D loss: 0.054483, acc.: 99.22%] [G loss: 1.500319]\n",
      "epoch:39 step:37068 [D loss: 0.199699, acc.: 92.19%] [G loss: 0.884985]\n",
      "epoch:39 step:37069 [D loss: 0.019586, acc.: 100.00%] [G loss: 2.807947]\n",
      "epoch:39 step:37070 [D loss: 0.555020, acc.: 70.31%] [G loss: 3.133038]\n",
      "epoch:39 step:37071 [D loss: 1.889737, acc.: 39.84%] [G loss: 5.646920]\n",
      "epoch:39 step:37072 [D loss: 0.289385, acc.: 85.94%] [G loss: 3.834823]\n",
      "epoch:39 step:37073 [D loss: 0.948257, acc.: 57.03%] [G loss: 2.331514]\n",
      "epoch:39 step:37074 [D loss: 0.186508, acc.: 96.09%] [G loss: 2.962113]\n",
      "epoch:39 step:37075 [D loss: 0.096685, acc.: 98.44%] [G loss: 1.834520]\n",
      "epoch:39 step:37076 [D loss: 0.234160, acc.: 89.06%] [G loss: 2.979654]\n",
      "epoch:39 step:37077 [D loss: 0.209190, acc.: 94.53%] [G loss: 3.633442]\n",
      "epoch:39 step:37078 [D loss: 0.043767, acc.: 100.00%] [G loss: 6.323751]\n",
      "epoch:39 step:37079 [D loss: 0.052122, acc.: 98.44%] [G loss: 1.332308]\n",
      "epoch:39 step:37080 [D loss: 0.223257, acc.: 93.75%] [G loss: 2.172371]\n",
      "epoch:39 step:37081 [D loss: 0.512696, acc.: 67.97%] [G loss: 5.866986]\n",
      "epoch:39 step:37082 [D loss: 1.280738, acc.: 47.66%] [G loss: 0.945599]\n",
      "epoch:39 step:37083 [D loss: 0.574854, acc.: 61.72%] [G loss: 6.127629]\n",
      "epoch:39 step:37084 [D loss: 0.051794, acc.: 100.00%] [G loss: 2.854205]\n",
      "epoch:39 step:37085 [D loss: 0.372559, acc.: 85.16%] [G loss: 2.397082]\n",
      "epoch:39 step:37086 [D loss: 0.014262, acc.: 100.00%] [G loss: 2.709655]\n",
      "epoch:39 step:37087 [D loss: 0.133332, acc.: 96.09%] [G loss: 2.588093]\n",
      "epoch:39 step:37088 [D loss: 0.059822, acc.: 99.22%] [G loss: 4.516381]\n",
      "epoch:39 step:37089 [D loss: 0.279674, acc.: 88.28%] [G loss: 2.598593]\n",
      "epoch:39 step:37090 [D loss: 0.128656, acc.: 97.66%] [G loss: 3.762339]\n",
      "epoch:39 step:37091 [D loss: 0.147332, acc.: 95.31%] [G loss: 3.675796]\n",
      "epoch:39 step:37092 [D loss: 0.566315, acc.: 63.28%] [G loss: 2.154964]\n",
      "epoch:39 step:37093 [D loss: 0.245433, acc.: 86.72%] [G loss: 5.904860]\n",
      "epoch:39 step:37094 [D loss: 0.206395, acc.: 96.09%] [G loss: 4.246408]\n",
      "epoch:39 step:37095 [D loss: 0.443279, acc.: 78.12%] [G loss: 1.163203]\n",
      "epoch:39 step:37096 [D loss: 0.050511, acc.: 99.22%] [G loss: 3.115133]\n",
      "epoch:39 step:37097 [D loss: 0.199583, acc.: 97.66%] [G loss: 0.721369]\n",
      "epoch:39 step:37098 [D loss: 0.053908, acc.: 100.00%] [G loss: 1.760992]\n",
      "epoch:39 step:37099 [D loss: 0.114591, acc.: 98.44%] [G loss: 0.108501]\n",
      "epoch:39 step:37100 [D loss: 0.286402, acc.: 86.72%] [G loss: 3.945629]\n",
      "epoch:39 step:37101 [D loss: 0.058301, acc.: 99.22%] [G loss: 4.422068]\n",
      "epoch:39 step:37102 [D loss: 0.039704, acc.: 100.00%] [G loss: 2.512117]\n",
      "epoch:39 step:37103 [D loss: 0.082868, acc.: 99.22%] [G loss: 2.724357]\n",
      "epoch:39 step:37104 [D loss: 0.057851, acc.: 98.44%] [G loss: 3.082088]\n",
      "epoch:39 step:37105 [D loss: 0.022901, acc.: 99.22%] [G loss: 2.351220]\n",
      "epoch:39 step:37106 [D loss: 0.123987, acc.: 98.44%] [G loss: 1.666204]\n",
      "epoch:39 step:37107 [D loss: 0.285396, acc.: 85.94%] [G loss: 3.474228]\n",
      "epoch:39 step:37108 [D loss: 0.559566, acc.: 70.31%] [G loss: 1.965050]\n",
      "epoch:39 step:37109 [D loss: 0.007334, acc.: 100.00%] [G loss: 2.925617]\n",
      "epoch:39 step:37110 [D loss: 0.091473, acc.: 97.66%] [G loss: 6.713567]\n",
      "epoch:39 step:37111 [D loss: 0.166197, acc.: 96.09%] [G loss: 5.345265]\n",
      "epoch:39 step:37112 [D loss: 0.265480, acc.: 94.53%] [G loss: 5.881995]\n",
      "epoch:39 step:37113 [D loss: 0.866819, acc.: 53.12%] [G loss: 3.324822]\n",
      "epoch:39 step:37114 [D loss: 0.350359, acc.: 85.16%] [G loss: 3.351479]\n",
      "epoch:39 step:37115 [D loss: 0.058315, acc.: 99.22%] [G loss: 1.895112]\n",
      "epoch:39 step:37116 [D loss: 0.083956, acc.: 97.66%] [G loss: 2.624408]\n",
      "epoch:39 step:37117 [D loss: 0.052107, acc.: 99.22%] [G loss: 3.911762]\n",
      "epoch:39 step:37118 [D loss: 0.045391, acc.: 98.44%] [G loss: 2.695730]\n",
      "epoch:39 step:37119 [D loss: 0.016034, acc.: 100.00%] [G loss: 1.349059]\n",
      "epoch:39 step:37120 [D loss: 0.034580, acc.: 99.22%] [G loss: 3.814205]\n",
      "epoch:39 step:37121 [D loss: 0.053459, acc.: 100.00%] [G loss: 4.170004]\n",
      "epoch:39 step:37122 [D loss: 0.076859, acc.: 97.66%] [G loss: 4.222270]\n",
      "epoch:39 step:37123 [D loss: 1.087595, acc.: 34.38%] [G loss: 2.111161]\n",
      "epoch:39 step:37124 [D loss: 0.316703, acc.: 85.94%] [G loss: 5.081268]\n",
      "epoch:39 step:37125 [D loss: 0.306062, acc.: 89.06%] [G loss: 3.999234]\n",
      "epoch:39 step:37126 [D loss: 1.433411, acc.: 16.41%] [G loss: 4.432006]\n",
      "epoch:39 step:37127 [D loss: 0.109473, acc.: 99.22%] [G loss: 4.520040]\n",
      "epoch:39 step:37128 [D loss: 0.231935, acc.: 92.97%] [G loss: 2.579671]\n",
      "epoch:39 step:37129 [D loss: 0.480866, acc.: 71.88%] [G loss: 4.441868]\n",
      "epoch:39 step:37130 [D loss: 0.456429, acc.: 73.44%] [G loss: 3.744974]\n",
      "epoch:39 step:37131 [D loss: 0.314671, acc.: 87.50%] [G loss: 3.134635]\n",
      "epoch:39 step:37132 [D loss: 0.186214, acc.: 96.88%] [G loss: 2.705313]\n",
      "epoch:39 step:37133 [D loss: 0.263254, acc.: 85.94%] [G loss: 5.255460]\n",
      "epoch:39 step:37134 [D loss: 1.435408, acc.: 43.75%] [G loss: 2.432117]\n",
      "epoch:39 step:37135 [D loss: 0.217248, acc.: 95.31%] [G loss: 3.328546]\n",
      "epoch:39 step:37136 [D loss: 0.080274, acc.: 97.66%] [G loss: 3.264614]\n",
      "epoch:39 step:37137 [D loss: 0.304321, acc.: 86.72%] [G loss: 4.984524]\n",
      "epoch:39 step:37138 [D loss: 0.123261, acc.: 98.44%] [G loss: 2.803071]\n",
      "epoch:39 step:37139 [D loss: 0.157858, acc.: 97.66%] [G loss: 2.719013]\n",
      "epoch:39 step:37140 [D loss: 0.159687, acc.: 96.88%] [G loss: 3.633923]\n",
      "epoch:39 step:37141 [D loss: 0.693258, acc.: 61.72%] [G loss: 3.750271]\n",
      "epoch:39 step:37142 [D loss: 0.065036, acc.: 99.22%] [G loss: 4.541679]\n",
      "epoch:39 step:37143 [D loss: 0.189009, acc.: 92.19%] [G loss: 4.092234]\n",
      "epoch:39 step:37144 [D loss: 0.041734, acc.: 100.00%] [G loss: 4.836446]\n",
      "epoch:39 step:37145 [D loss: 0.006726, acc.: 100.00%] [G loss: 4.603956]\n",
      "epoch:39 step:37146 [D loss: 0.696282, acc.: 65.62%] [G loss: 1.329300]\n",
      "epoch:39 step:37147 [D loss: 0.465775, acc.: 75.78%] [G loss: 0.973653]\n",
      "epoch:39 step:37148 [D loss: 0.121683, acc.: 99.22%] [G loss: 2.590238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:37149 [D loss: 0.102006, acc.: 99.22%] [G loss: 3.578142]\n",
      "epoch:39 step:37150 [D loss: 0.068616, acc.: 99.22%] [G loss: 6.647661]\n",
      "epoch:39 step:37151 [D loss: 0.608780, acc.: 62.50%] [G loss: 4.245649]\n",
      "epoch:39 step:37152 [D loss: 0.472535, acc.: 71.88%] [G loss: 0.833166]\n",
      "epoch:39 step:37153 [D loss: 0.271809, acc.: 91.41%] [G loss: 4.575424]\n",
      "epoch:39 step:37154 [D loss: 0.217516, acc.: 90.62%] [G loss: 3.219940]\n",
      "epoch:39 step:37155 [D loss: 0.140514, acc.: 97.66%] [G loss: 3.353244]\n",
      "epoch:39 step:37156 [D loss: 0.090309, acc.: 99.22%] [G loss: 7.114620]\n",
      "epoch:39 step:37157 [D loss: 0.573500, acc.: 71.88%] [G loss: 3.463032]\n",
      "epoch:39 step:37158 [D loss: 0.257732, acc.: 87.50%] [G loss: 7.157255]\n",
      "epoch:39 step:37159 [D loss: 0.345564, acc.: 85.94%] [G loss: 5.657464]\n",
      "epoch:39 step:37160 [D loss: 0.342195, acc.: 89.06%] [G loss: 3.443536]\n",
      "epoch:39 step:37161 [D loss: 0.088814, acc.: 100.00%] [G loss: 5.509246]\n",
      "epoch:39 step:37162 [D loss: 0.090953, acc.: 97.66%] [G loss: 6.851947]\n",
      "epoch:39 step:37163 [D loss: 0.003963, acc.: 100.00%] [G loss: 1.589527]\n",
      "epoch:39 step:37164 [D loss: 0.019739, acc.: 100.00%] [G loss: 2.360114]\n",
      "epoch:39 step:37165 [D loss: 0.026042, acc.: 100.00%] [G loss: 0.762138]\n",
      "epoch:39 step:37166 [D loss: 0.162377, acc.: 96.09%] [G loss: 7.296615]\n",
      "epoch:39 step:37167 [D loss: 0.879874, acc.: 59.38%] [G loss: 5.231906]\n",
      "epoch:39 step:37168 [D loss: 0.519611, acc.: 71.09%] [G loss: 1.329617]\n",
      "epoch:39 step:37169 [D loss: 0.525823, acc.: 71.09%] [G loss: 3.832915]\n",
      "epoch:39 step:37170 [D loss: 0.148871, acc.: 94.53%] [G loss: 2.320740]\n",
      "epoch:39 step:37171 [D loss: 0.117605, acc.: 98.44%] [G loss: 2.332657]\n",
      "epoch:39 step:37172 [D loss: 0.209532, acc.: 96.09%] [G loss: 1.240217]\n",
      "epoch:39 step:37173 [D loss: 0.043825, acc.: 98.44%] [G loss: 4.496027]\n",
      "epoch:39 step:37174 [D loss: 0.023702, acc.: 100.00%] [G loss: 2.994673]\n",
      "epoch:39 step:37175 [D loss: 0.100213, acc.: 99.22%] [G loss: 5.041537]\n",
      "epoch:39 step:37176 [D loss: 0.305085, acc.: 92.97%] [G loss: 3.800462]\n",
      "epoch:39 step:37177 [D loss: 0.100406, acc.: 96.88%] [G loss: 4.022882]\n",
      "epoch:39 step:37178 [D loss: 0.086340, acc.: 99.22%] [G loss: 2.447873]\n",
      "epoch:39 step:37179 [D loss: 0.322945, acc.: 89.84%] [G loss: 5.601632]\n",
      "epoch:39 step:37180 [D loss: 0.230585, acc.: 92.97%] [G loss: 7.468173]\n",
      "epoch:39 step:37181 [D loss: 0.899110, acc.: 64.06%] [G loss: 1.374118]\n",
      "epoch:39 step:37182 [D loss: 0.225638, acc.: 92.19%] [G loss: 4.873810]\n",
      "epoch:39 step:37183 [D loss: 0.069081, acc.: 98.44%] [G loss: 1.696145]\n",
      "epoch:39 step:37184 [D loss: 0.075681, acc.: 98.44%] [G loss: 5.851560]\n",
      "epoch:39 step:37185 [D loss: 0.010171, acc.: 100.00%] [G loss: 5.218552]\n",
      "epoch:39 step:37186 [D loss: 0.245516, acc.: 92.97%] [G loss: 8.106524]\n",
      "epoch:39 step:37187 [D loss: 0.794873, acc.: 59.38%] [G loss: 3.562325]\n",
      "epoch:39 step:37188 [D loss: 0.070282, acc.: 99.22%] [G loss: 0.222159]\n",
      "epoch:39 step:37189 [D loss: 0.243110, acc.: 89.84%] [G loss: 4.319194]\n",
      "epoch:39 step:37190 [D loss: 0.302298, acc.: 85.94%] [G loss: 7.550292]\n",
      "epoch:39 step:37191 [D loss: 0.050490, acc.: 99.22%] [G loss: 8.195456]\n",
      "epoch:39 step:37192 [D loss: 0.528528, acc.: 68.75%] [G loss: 3.404546]\n",
      "epoch:39 step:37193 [D loss: 0.187545, acc.: 96.88%] [G loss: 3.992011]\n",
      "epoch:39 step:37194 [D loss: 0.033006, acc.: 99.22%] [G loss: 0.568423]\n",
      "epoch:39 step:37195 [D loss: 0.377213, acc.: 86.72%] [G loss: 4.372001]\n",
      "epoch:39 step:37196 [D loss: 0.040571, acc.: 99.22%] [G loss: 2.085969]\n",
      "epoch:39 step:37197 [D loss: 0.411659, acc.: 74.22%] [G loss: 4.654308]\n",
      "epoch:39 step:37198 [D loss: 0.310726, acc.: 90.62%] [G loss: 0.874660]\n",
      "epoch:39 step:37199 [D loss: 0.087719, acc.: 98.44%] [G loss: 4.433379]\n",
      "epoch:39 step:37200 [D loss: 0.566233, acc.: 71.88%] [G loss: 3.181274]\n",
      "epoch:39 step:37201 [D loss: 0.248705, acc.: 91.41%] [G loss: 2.998364]\n",
      "epoch:39 step:37202 [D loss: 0.023606, acc.: 99.22%] [G loss: 2.760677]\n",
      "epoch:39 step:37203 [D loss: 0.009406, acc.: 100.00%] [G loss: 3.288088]\n",
      "epoch:39 step:37204 [D loss: 0.939850, acc.: 54.69%] [G loss: 2.803126]\n",
      "epoch:39 step:37205 [D loss: 0.047150, acc.: 99.22%] [G loss: 4.219689]\n",
      "epoch:39 step:37206 [D loss: 0.427015, acc.: 74.22%] [G loss: 3.018183]\n",
      "epoch:39 step:37207 [D loss: 0.045562, acc.: 99.22%] [G loss: 0.642150]\n",
      "epoch:39 step:37208 [D loss: 0.269533, acc.: 86.72%] [G loss: 1.760932]\n",
      "epoch:39 step:37209 [D loss: 0.388454, acc.: 87.50%] [G loss: 4.033170]\n",
      "epoch:39 step:37210 [D loss: 0.075637, acc.: 99.22%] [G loss: 3.906838]\n",
      "epoch:39 step:37211 [D loss: 0.871873, acc.: 52.34%] [G loss: 4.690142]\n",
      "epoch:39 step:37212 [D loss: 0.064649, acc.: 100.00%] [G loss: 3.823588]\n",
      "epoch:39 step:37213 [D loss: 0.254081, acc.: 94.53%] [G loss: 3.724469]\n",
      "epoch:39 step:37214 [D loss: 0.140140, acc.: 96.88%] [G loss: 4.914266]\n",
      "epoch:39 step:37215 [D loss: 0.194389, acc.: 92.97%] [G loss: 2.776234]\n",
      "epoch:39 step:37216 [D loss: 0.370252, acc.: 78.91%] [G loss: 1.979796]\n",
      "epoch:39 step:37217 [D loss: 0.046253, acc.: 99.22%] [G loss: 4.810542]\n",
      "epoch:39 step:37218 [D loss: 0.054232, acc.: 97.66%] [G loss: 7.734335]\n",
      "epoch:39 step:37219 [D loss: 0.146979, acc.: 95.31%] [G loss: 6.923392]\n",
      "epoch:39 step:37220 [D loss: 0.024836, acc.: 100.00%] [G loss: 8.315622]\n",
      "epoch:39 step:37221 [D loss: 0.686433, acc.: 64.06%] [G loss: 0.717943]\n",
      "epoch:39 step:37222 [D loss: 0.017681, acc.: 99.22%] [G loss: 2.225905]\n",
      "epoch:39 step:37223 [D loss: 0.030495, acc.: 100.00%] [G loss: 2.332271]\n",
      "epoch:39 step:37224 [D loss: 0.212723, acc.: 92.97%] [G loss: 5.307529]\n",
      "epoch:39 step:37225 [D loss: 0.006891, acc.: 100.00%] [G loss: 0.579514]\n",
      "epoch:39 step:37226 [D loss: 0.179954, acc.: 96.09%] [G loss: 1.707638]\n",
      "epoch:39 step:37227 [D loss: 0.565770, acc.: 67.19%] [G loss: 1.589856]\n",
      "epoch:39 step:37228 [D loss: 0.061208, acc.: 99.22%] [G loss: 2.513076]\n",
      "epoch:39 step:37229 [D loss: 0.708822, acc.: 59.38%] [G loss: 0.350348]\n",
      "epoch:39 step:37230 [D loss: 2.019861, acc.: 50.78%] [G loss: 6.122807]\n",
      "epoch:39 step:37231 [D loss: 0.049801, acc.: 100.00%] [G loss: 8.985119]\n",
      "epoch:39 step:37232 [D loss: 0.020479, acc.: 100.00%] [G loss: 8.036011]\n",
      "epoch:39 step:37233 [D loss: 0.203781, acc.: 94.53%] [G loss: 5.023134]\n",
      "epoch:39 step:37234 [D loss: 0.201740, acc.: 94.53%] [G loss: 6.292077]\n",
      "epoch:39 step:37235 [D loss: 0.538086, acc.: 67.97%] [G loss: 4.917102]\n",
      "epoch:39 step:37236 [D loss: 0.028889, acc.: 99.22%] [G loss: 5.746426]\n",
      "epoch:39 step:37237 [D loss: 0.123300, acc.: 96.09%] [G loss: 5.352155]\n",
      "epoch:39 step:37238 [D loss: 0.349348, acc.: 79.69%] [G loss: 1.163408]\n",
      "epoch:39 step:37239 [D loss: 0.026524, acc.: 99.22%] [G loss: 4.415594]\n",
      "epoch:39 step:37240 [D loss: 0.190502, acc.: 92.97%] [G loss: 4.980265]\n",
      "epoch:39 step:37241 [D loss: 0.380497, acc.: 82.03%] [G loss: 4.164742]\n",
      "epoch:39 step:37242 [D loss: 0.005483, acc.: 100.00%] [G loss: 7.578823]\n",
      "epoch:39 step:37243 [D loss: 0.223720, acc.: 92.19%] [G loss: 4.600600]\n",
      "epoch:39 step:37244 [D loss: 0.580681, acc.: 66.41%] [G loss: 2.077567]\n",
      "epoch:39 step:37245 [D loss: 0.081308, acc.: 97.66%] [G loss: 2.908032]\n",
      "epoch:39 step:37246 [D loss: 0.306515, acc.: 85.94%] [G loss: 2.916005]\n",
      "epoch:39 step:37247 [D loss: 0.364944, acc.: 82.03%] [G loss: 4.597623]\n",
      "epoch:39 step:37248 [D loss: 0.518994, acc.: 77.34%] [G loss: 3.813966]\n",
      "epoch:39 step:37249 [D loss: 0.105838, acc.: 97.66%] [G loss: 5.631294]\n",
      "epoch:39 step:37250 [D loss: 0.071681, acc.: 98.44%] [G loss: 5.080107]\n",
      "epoch:39 step:37251 [D loss: 0.024223, acc.: 100.00%] [G loss: 4.869393]\n",
      "epoch:39 step:37252 [D loss: 0.335399, acc.: 86.72%] [G loss: 4.378838]\n",
      "epoch:39 step:37253 [D loss: 0.072598, acc.: 100.00%] [G loss: 2.280941]\n",
      "epoch:39 step:37254 [D loss: 0.078409, acc.: 99.22%] [G loss: 3.962344]\n",
      "epoch:39 step:37255 [D loss: 0.120180, acc.: 97.66%] [G loss: 1.170987]\n",
      "epoch:39 step:37256 [D loss: 0.697282, acc.: 59.38%] [G loss: 0.956109]\n",
      "epoch:39 step:37257 [D loss: 0.210018, acc.: 93.75%] [G loss: 3.929285]\n",
      "epoch:39 step:37258 [D loss: 0.006414, acc.: 100.00%] [G loss: 0.966878]\n",
      "epoch:39 step:37259 [D loss: 0.033282, acc.: 100.00%] [G loss: 2.036171]\n",
      "epoch:39 step:37260 [D loss: 0.310112, acc.: 82.03%] [G loss: 3.140701]\n",
      "epoch:39 step:37261 [D loss: 0.228883, acc.: 89.84%] [G loss: 0.826674]\n",
      "epoch:39 step:37262 [D loss: 0.077125, acc.: 99.22%] [G loss: 5.049448]\n",
      "epoch:39 step:37263 [D loss: 0.055903, acc.: 99.22%] [G loss: 7.405149]\n",
      "epoch:39 step:37264 [D loss: 0.152460, acc.: 94.53%] [G loss: 6.648344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:37265 [D loss: 0.027493, acc.: 99.22%] [G loss: 4.659481]\n",
      "epoch:39 step:37266 [D loss: 0.063672, acc.: 100.00%] [G loss: 5.861021]\n",
      "epoch:39 step:37267 [D loss: 0.158432, acc.: 96.88%] [G loss: 4.073735]\n",
      "epoch:39 step:37268 [D loss: 0.099376, acc.: 99.22%] [G loss: 7.198611]\n",
      "epoch:39 step:37269 [D loss: 0.030661, acc.: 100.00%] [G loss: 0.820024]\n",
      "epoch:39 step:37270 [D loss: 0.829763, acc.: 60.16%] [G loss: 4.245721]\n",
      "epoch:39 step:37271 [D loss: 0.077623, acc.: 99.22%] [G loss: 9.657006]\n",
      "epoch:39 step:37272 [D loss: 0.161819, acc.: 95.31%] [G loss: 4.736683]\n",
      "epoch:39 step:37273 [D loss: 0.338648, acc.: 83.59%] [G loss: 0.931454]\n",
      "epoch:39 step:37274 [D loss: 0.108777, acc.: 96.88%] [G loss: 5.132522]\n",
      "epoch:39 step:37275 [D loss: 0.258622, acc.: 88.28%] [G loss: 6.025646]\n",
      "epoch:39 step:37276 [D loss: 0.097250, acc.: 96.88%] [G loss: 1.429300]\n",
      "epoch:39 step:37277 [D loss: 0.224419, acc.: 92.19%] [G loss: 5.661040]\n",
      "epoch:39 step:37278 [D loss: 0.476627, acc.: 80.47%] [G loss: 6.473361]\n",
      "epoch:39 step:37279 [D loss: 0.021985, acc.: 100.00%] [G loss: 4.482682]\n",
      "epoch:39 step:37280 [D loss: 0.034000, acc.: 100.00%] [G loss: 3.248272]\n",
      "epoch:39 step:37281 [D loss: 1.304579, acc.: 26.56%] [G loss: 0.898164]\n",
      "epoch:39 step:37282 [D loss: 0.142750, acc.: 95.31%] [G loss: 5.449693]\n",
      "epoch:39 step:37283 [D loss: 0.046972, acc.: 99.22%] [G loss: 2.342819]\n",
      "epoch:39 step:37284 [D loss: 0.054358, acc.: 98.44%] [G loss: 2.849844]\n",
      "epoch:39 step:37285 [D loss: 0.110238, acc.: 97.66%] [G loss: 2.283269]\n",
      "epoch:39 step:37286 [D loss: 0.207536, acc.: 96.09%] [G loss: 5.656959]\n",
      "epoch:39 step:37287 [D loss: 0.059408, acc.: 99.22%] [G loss: 1.544560]\n",
      "epoch:39 step:37288 [D loss: 1.047572, acc.: 53.91%] [G loss: 4.207254]\n",
      "epoch:39 step:37289 [D loss: 0.241843, acc.: 87.50%] [G loss: 4.519761]\n",
      "epoch:39 step:37290 [D loss: 0.171321, acc.: 97.66%] [G loss: 10.044913]\n",
      "epoch:39 step:37291 [D loss: 0.045407, acc.: 100.00%] [G loss: 9.829153]\n",
      "epoch:39 step:37292 [D loss: 0.123623, acc.: 96.88%] [G loss: 8.303417]\n",
      "epoch:39 step:37293 [D loss: 0.145436, acc.: 99.22%] [G loss: 4.652607]\n",
      "epoch:39 step:37294 [D loss: 0.337913, acc.: 79.69%] [G loss: 5.261673]\n",
      "epoch:39 step:37295 [D loss: 0.636704, acc.: 59.38%] [G loss: 2.548753]\n",
      "epoch:39 step:37296 [D loss: 0.032384, acc.: 100.00%] [G loss: 2.522333]\n",
      "epoch:39 step:37297 [D loss: 0.658116, acc.: 59.38%] [G loss: 1.244435]\n",
      "epoch:39 step:37298 [D loss: 1.036803, acc.: 58.59%] [G loss: 5.995913]\n",
      "epoch:39 step:37299 [D loss: 0.133990, acc.: 95.31%] [G loss: 6.504571]\n",
      "epoch:39 step:37300 [D loss: 0.881671, acc.: 51.56%] [G loss: 7.964396]\n",
      "epoch:39 step:37301 [D loss: 0.014784, acc.: 100.00%] [G loss: 4.545700]\n",
      "epoch:39 step:37302 [D loss: 0.362423, acc.: 78.91%] [G loss: 5.311292]\n",
      "epoch:39 step:37303 [D loss: 0.090533, acc.: 99.22%] [G loss: 4.159130]\n",
      "epoch:39 step:37304 [D loss: 0.151372, acc.: 97.66%] [G loss: 2.414877]\n",
      "epoch:39 step:37305 [D loss: 0.044836, acc.: 100.00%] [G loss: 2.638053]\n",
      "epoch:39 step:37306 [D loss: 0.022111, acc.: 100.00%] [G loss: 2.844200]\n",
      "epoch:39 step:37307 [D loss: 0.030865, acc.: 100.00%] [G loss: 4.436306]\n",
      "epoch:39 step:37308 [D loss: 0.372179, acc.: 89.06%] [G loss: 3.218977]\n",
      "epoch:39 step:37309 [D loss: 0.030791, acc.: 98.44%] [G loss: 2.652469]\n",
      "epoch:39 step:37310 [D loss: 0.098752, acc.: 99.22%] [G loss: 3.124210]\n",
      "epoch:39 step:37311 [D loss: 0.092160, acc.: 98.44%] [G loss: 4.393867]\n",
      "epoch:39 step:37312 [D loss: 0.097341, acc.: 98.44%] [G loss: 4.939602]\n",
      "epoch:39 step:37313 [D loss: 0.184889, acc.: 98.44%] [G loss: 4.898053]\n",
      "epoch:39 step:37314 [D loss: 0.139650, acc.: 98.44%] [G loss: 6.372823]\n",
      "epoch:39 step:37315 [D loss: 0.106737, acc.: 97.66%] [G loss: 3.585097]\n",
      "epoch:39 step:37316 [D loss: 0.464948, acc.: 78.91%] [G loss: 0.800895]\n",
      "epoch:39 step:37317 [D loss: 0.075180, acc.: 98.44%] [G loss: 2.896343]\n",
      "epoch:39 step:37318 [D loss: 0.094833, acc.: 100.00%] [G loss: 5.959825]\n",
      "epoch:39 step:37319 [D loss: 2.106832, acc.: 44.53%] [G loss: 3.670763]\n",
      "epoch:39 step:37320 [D loss: 0.395886, acc.: 78.91%] [G loss: 5.946594]\n",
      "epoch:39 step:37321 [D loss: 0.068620, acc.: 99.22%] [G loss: 2.353515]\n",
      "epoch:39 step:37322 [D loss: 0.194864, acc.: 92.97%] [G loss: 4.634652]\n",
      "epoch:39 step:37323 [D loss: 0.110938, acc.: 99.22%] [G loss: 6.882300]\n",
      "epoch:39 step:37324 [D loss: 0.197236, acc.: 92.97%] [G loss: 6.607374]\n",
      "epoch:39 step:37325 [D loss: 0.483199, acc.: 78.12%] [G loss: 5.344983]\n",
      "epoch:39 step:37326 [D loss: 0.196191, acc.: 95.31%] [G loss: 3.503891]\n",
      "epoch:39 step:37327 [D loss: 0.043737, acc.: 100.00%] [G loss: 3.461710]\n",
      "epoch:39 step:37328 [D loss: 0.292813, acc.: 84.38%] [G loss: 1.542725]\n",
      "epoch:39 step:37329 [D loss: 0.006639, acc.: 100.00%] [G loss: 3.771405]\n",
      "epoch:39 step:37330 [D loss: 0.037310, acc.: 99.22%] [G loss: 1.052854]\n",
      "epoch:39 step:37331 [D loss: 0.061785, acc.: 100.00%] [G loss: 5.035888]\n",
      "epoch:39 step:37332 [D loss: 0.093036, acc.: 97.66%] [G loss: 2.732948]\n",
      "epoch:39 step:37333 [D loss: 0.151942, acc.: 96.09%] [G loss: 1.135604]\n",
      "epoch:39 step:37334 [D loss: 0.228961, acc.: 87.50%] [G loss: 3.149357]\n",
      "epoch:39 step:37335 [D loss: 0.101178, acc.: 98.44%] [G loss: 4.580708]\n",
      "epoch:39 step:37336 [D loss: 0.032081, acc.: 100.00%] [G loss: 3.006844]\n",
      "epoch:39 step:37337 [D loss: 0.134831, acc.: 98.44%] [G loss: 3.602479]\n",
      "epoch:39 step:37338 [D loss: 0.250344, acc.: 87.50%] [G loss: 3.541433]\n",
      "epoch:39 step:37339 [D loss: 0.237329, acc.: 89.84%] [G loss: 1.500574]\n",
      "epoch:39 step:37340 [D loss: 0.032897, acc.: 98.44%] [G loss: 5.857398]\n",
      "epoch:39 step:37341 [D loss: 0.180289, acc.: 93.75%] [G loss: 2.470748]\n",
      "epoch:39 step:37342 [D loss: 0.047732, acc.: 99.22%] [G loss: 4.542439]\n",
      "epoch:39 step:37343 [D loss: 0.566282, acc.: 67.19%] [G loss: 2.642959]\n",
      "epoch:39 step:37344 [D loss: 0.269157, acc.: 82.81%] [G loss: 7.120023]\n",
      "epoch:39 step:37345 [D loss: 0.205191, acc.: 89.84%] [G loss: 6.413975]\n",
      "epoch:39 step:37346 [D loss: 0.346791, acc.: 87.50%] [G loss: 2.743525]\n",
      "epoch:39 step:37347 [D loss: 0.016042, acc.: 100.00%] [G loss: 5.015894]\n",
      "epoch:39 step:37348 [D loss: 0.119489, acc.: 99.22%] [G loss: 2.319769]\n",
      "epoch:39 step:37349 [D loss: 0.118342, acc.: 97.66%] [G loss: 5.139871]\n",
      "epoch:39 step:37350 [D loss: 0.028221, acc.: 100.00%] [G loss: 2.835041]\n",
      "epoch:39 step:37351 [D loss: 0.101613, acc.: 96.88%] [G loss: 4.640889]\n",
      "epoch:39 step:37352 [D loss: 0.297691, acc.: 86.72%] [G loss: 3.411655]\n",
      "epoch:39 step:37353 [D loss: 0.111160, acc.: 99.22%] [G loss: 5.069017]\n",
      "epoch:39 step:37354 [D loss: 0.044668, acc.: 100.00%] [G loss: 3.412032]\n",
      "epoch:39 step:37355 [D loss: 0.043452, acc.: 100.00%] [G loss: 1.285059]\n",
      "epoch:39 step:37356 [D loss: 0.303403, acc.: 81.25%] [G loss: 1.601350]\n",
      "epoch:39 step:37357 [D loss: 0.064328, acc.: 100.00%] [G loss: 4.229927]\n",
      "epoch:39 step:37358 [D loss: 0.055653, acc.: 99.22%] [G loss: 6.419062]\n",
      "epoch:39 step:37359 [D loss: 1.001756, acc.: 49.22%] [G loss: 3.888570]\n",
      "epoch:39 step:37360 [D loss: 0.016359, acc.: 100.00%] [G loss: 0.678174]\n",
      "epoch:39 step:37361 [D loss: 0.160336, acc.: 96.09%] [G loss: 5.039839]\n",
      "epoch:39 step:37362 [D loss: 0.800218, acc.: 57.81%] [G loss: 0.352186]\n",
      "epoch:39 step:37363 [D loss: 0.064112, acc.: 99.22%] [G loss: 3.386984]\n",
      "epoch:39 step:37364 [D loss: 0.806305, acc.: 60.94%] [G loss: 5.800966]\n",
      "epoch:39 step:37365 [D loss: 0.393965, acc.: 80.47%] [G loss: 4.158644]\n",
      "epoch:39 step:37366 [D loss: 0.163492, acc.: 94.53%] [G loss: 3.401566]\n",
      "epoch:39 step:37367 [D loss: 0.161843, acc.: 95.31%] [G loss: 5.362446]\n",
      "epoch:39 step:37368 [D loss: 0.544723, acc.: 73.44%] [G loss: 5.213885]\n",
      "epoch:39 step:37369 [D loss: 0.401464, acc.: 78.12%] [G loss: 4.763512]\n",
      "epoch:39 step:37370 [D loss: 0.126927, acc.: 97.66%] [G loss: 3.989331]\n",
      "epoch:39 step:37371 [D loss: 0.177056, acc.: 95.31%] [G loss: 6.945220]\n",
      "epoch:39 step:37372 [D loss: 0.100509, acc.: 99.22%] [G loss: 1.281739]\n",
      "epoch:39 step:37373 [D loss: 0.488281, acc.: 72.66%] [G loss: 4.993872]\n",
      "epoch:39 step:37374 [D loss: 0.673247, acc.: 60.94%] [G loss: 2.596989]\n",
      "epoch:39 step:37375 [D loss: 0.285678, acc.: 89.84%] [G loss: 3.749655]\n",
      "epoch:39 step:37376 [D loss: 0.039796, acc.: 100.00%] [G loss: 1.194839]\n",
      "epoch:39 step:37377 [D loss: 0.439138, acc.: 75.78%] [G loss: 6.898961]\n",
      "epoch:39 step:37378 [D loss: 0.060190, acc.: 99.22%] [G loss: 2.324588]\n",
      "epoch:39 step:37379 [D loss: 0.197672, acc.: 95.31%] [G loss: 2.814112]\n",
      "epoch:39 step:37380 [D loss: 0.078949, acc.: 97.66%] [G loss: 7.100163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:37381 [D loss: 0.091108, acc.: 100.00%] [G loss: 3.427351]\n",
      "epoch:39 step:37382 [D loss: 0.042836, acc.: 100.00%] [G loss: 4.268675]\n",
      "epoch:39 step:37383 [D loss: 0.115276, acc.: 98.44%] [G loss: 3.889555]\n",
      "epoch:39 step:37384 [D loss: 0.301216, acc.: 92.97%] [G loss: 3.915226]\n",
      "epoch:39 step:37385 [D loss: 0.090448, acc.: 98.44%] [G loss: 5.988023]\n",
      "epoch:39 step:37386 [D loss: 0.037318, acc.: 99.22%] [G loss: 4.528171]\n",
      "epoch:39 step:37387 [D loss: 0.012846, acc.: 100.00%] [G loss: 1.304944]\n",
      "epoch:39 step:37388 [D loss: 0.122961, acc.: 97.66%] [G loss: 2.273865]\n",
      "epoch:39 step:37389 [D loss: 0.049200, acc.: 100.00%] [G loss: 4.077186]\n",
      "epoch:39 step:37390 [D loss: 0.080569, acc.: 99.22%] [G loss: 1.620469]\n",
      "epoch:39 step:37391 [D loss: 1.030072, acc.: 41.41%] [G loss: 1.842632]\n",
      "epoch:39 step:37392 [D loss: 0.301110, acc.: 87.50%] [G loss: 4.568303]\n",
      "epoch:39 step:37393 [D loss: 0.033926, acc.: 100.00%] [G loss: 7.260922]\n",
      "epoch:39 step:37394 [D loss: 0.125074, acc.: 95.31%] [G loss: 2.752417]\n",
      "epoch:39 step:37395 [D loss: 0.183239, acc.: 95.31%] [G loss: 4.968608]\n",
      "epoch:39 step:37396 [D loss: 0.016483, acc.: 100.00%] [G loss: 7.676103]\n",
      "epoch:39 step:37397 [D loss: 0.029544, acc.: 100.00%] [G loss: 6.461400]\n",
      "epoch:39 step:37398 [D loss: 0.020873, acc.: 100.00%] [G loss: 2.877209]\n",
      "epoch:39 step:37399 [D loss: 0.583150, acc.: 70.31%] [G loss: 0.955820]\n",
      "epoch:39 step:37400 [D loss: 0.247883, acc.: 89.06%] [G loss: 0.223101]\n",
      "epoch:39 step:37401 [D loss: 1.129988, acc.: 45.31%] [G loss: 1.977187]\n",
      "epoch:39 step:37402 [D loss: 0.336295, acc.: 82.03%] [G loss: 6.075173]\n",
      "epoch:39 step:37403 [D loss: 0.044345, acc.: 99.22%] [G loss: 2.918080]\n",
      "epoch:39 step:37404 [D loss: 0.079542, acc.: 97.66%] [G loss: 4.531261]\n",
      "epoch:39 step:37405 [D loss: 0.029899, acc.: 100.00%] [G loss: 2.354970]\n",
      "epoch:39 step:37406 [D loss: 0.254329, acc.: 89.84%] [G loss: 6.391108]\n",
      "epoch:39 step:37407 [D loss: 0.197730, acc.: 92.19%] [G loss: 7.855766]\n",
      "epoch:39 step:37408 [D loss: 0.320522, acc.: 83.59%] [G loss: 4.660086]\n",
      "epoch:39 step:37409 [D loss: 0.102654, acc.: 97.66%] [G loss: 4.584619]\n",
      "epoch:39 step:37410 [D loss: 0.094201, acc.: 97.66%] [G loss: 1.705649]\n",
      "epoch:39 step:37411 [D loss: 0.016339, acc.: 100.00%] [G loss: 5.607333]\n",
      "epoch:39 step:37412 [D loss: 0.181536, acc.: 92.97%] [G loss: 1.814597]\n",
      "epoch:39 step:37413 [D loss: 0.013869, acc.: 100.00%] [G loss: 4.073930]\n",
      "epoch:39 step:37414 [D loss: 0.029735, acc.: 100.00%] [G loss: 4.706204]\n",
      "epoch:39 step:37415 [D loss: 0.046777, acc.: 99.22%] [G loss: 2.111318]\n",
      "epoch:39 step:37416 [D loss: 0.067088, acc.: 98.44%] [G loss: 2.603474]\n",
      "epoch:39 step:37417 [D loss: 0.438936, acc.: 82.03%] [G loss: 3.487290]\n",
      "epoch:39 step:37418 [D loss: 2.119015, acc.: 17.19%] [G loss: 2.931703]\n",
      "epoch:39 step:37419 [D loss: 0.008248, acc.: 100.00%] [G loss: 5.352838]\n",
      "epoch:39 step:37420 [D loss: 1.763054, acc.: 18.75%] [G loss: 2.610598]\n",
      "epoch:39 step:37421 [D loss: 0.283622, acc.: 87.50%] [G loss: 4.648624]\n",
      "epoch:39 step:37422 [D loss: 0.028828, acc.: 100.00%] [G loss: 5.153998]\n",
      "epoch:39 step:37423 [D loss: 0.529503, acc.: 71.88%] [G loss: 5.158741]\n",
      "epoch:39 step:37424 [D loss: 0.149538, acc.: 95.31%] [G loss: 1.537655]\n",
      "epoch:39 step:37425 [D loss: 0.276040, acc.: 88.28%] [G loss: 6.842885]\n",
      "epoch:39 step:37426 [D loss: 0.044758, acc.: 99.22%] [G loss: 1.430688]\n",
      "epoch:39 step:37427 [D loss: 0.036228, acc.: 99.22%] [G loss: 3.632030]\n",
      "epoch:39 step:37428 [D loss: 0.620382, acc.: 67.19%] [G loss: 2.882420]\n",
      "epoch:39 step:37429 [D loss: 0.106795, acc.: 96.88%] [G loss: 2.535360]\n",
      "epoch:39 step:37430 [D loss: 0.351318, acc.: 91.41%] [G loss: 4.692154]\n",
      "epoch:39 step:37431 [D loss: 0.035882, acc.: 100.00%] [G loss: 4.890236]\n",
      "epoch:39 step:37432 [D loss: 0.196226, acc.: 93.75%] [G loss: 3.935289]\n",
      "epoch:39 step:37433 [D loss: 0.522870, acc.: 66.41%] [G loss: 1.988793]\n",
      "epoch:39 step:37434 [D loss: 0.028237, acc.: 100.00%] [G loss: 2.267776]\n",
      "epoch:39 step:37435 [D loss: 0.269755, acc.: 89.06%] [G loss: 4.824578]\n",
      "epoch:39 step:37436 [D loss: 0.323994, acc.: 85.94%] [G loss: 5.442654]\n",
      "epoch:39 step:37437 [D loss: 0.175238, acc.: 94.53%] [G loss: 5.324947]\n",
      "epoch:39 step:37438 [D loss: 0.138312, acc.: 94.53%] [G loss: 3.514395]\n",
      "epoch:39 step:37439 [D loss: 0.078144, acc.: 98.44%] [G loss: 6.600933]\n",
      "epoch:39 step:37440 [D loss: 0.091931, acc.: 98.44%] [G loss: 5.142848]\n",
      "epoch:39 step:37441 [D loss: 0.670952, acc.: 60.94%] [G loss: 3.698766]\n",
      "epoch:39 step:37442 [D loss: 0.074911, acc.: 97.66%] [G loss: 5.138350]\n",
      "epoch:39 step:37443 [D loss: 0.313448, acc.: 84.38%] [G loss: 3.156835]\n",
      "epoch:39 step:37444 [D loss: 0.018446, acc.: 99.22%] [G loss: 4.533273]\n",
      "epoch:39 step:37445 [D loss: 0.261658, acc.: 89.84%] [G loss: 4.693804]\n",
      "epoch:39 step:37446 [D loss: 0.103217, acc.: 99.22%] [G loss: 3.663376]\n",
      "epoch:39 step:37447 [D loss: 0.759348, acc.: 59.38%] [G loss: 4.481359]\n",
      "epoch:39 step:37448 [D loss: 0.074831, acc.: 98.44%] [G loss: 4.749408]\n",
      "epoch:39 step:37449 [D loss: 0.079876, acc.: 98.44%] [G loss: 3.391021]\n",
      "epoch:39 step:37450 [D loss: 0.462496, acc.: 76.56%] [G loss: 5.104535]\n",
      "epoch:39 step:37451 [D loss: 1.279195, acc.: 54.69%] [G loss: 0.551487]\n",
      "epoch:39 step:37452 [D loss: 0.439131, acc.: 72.66%] [G loss: 1.837534]\n",
      "epoch:39 step:37453 [D loss: 0.231724, acc.: 90.62%] [G loss: 1.021514]\n",
      "epoch:39 step:37454 [D loss: 0.029118, acc.: 100.00%] [G loss: 7.010773]\n",
      "epoch:39 step:37455 [D loss: 0.643374, acc.: 64.06%] [G loss: 8.518812]\n",
      "epoch:39 step:37456 [D loss: 0.110576, acc.: 97.66%] [G loss: 3.624240]\n",
      "epoch:39 step:37457 [D loss: 0.063500, acc.: 99.22%] [G loss: 6.101684]\n",
      "epoch:39 step:37458 [D loss: 1.039374, acc.: 56.25%] [G loss: 1.820164]\n",
      "epoch:39 step:37459 [D loss: 0.412293, acc.: 75.78%] [G loss: 3.546118]\n",
      "epoch:39 step:37460 [D loss: 0.056302, acc.: 99.22%] [G loss: 5.716064]\n",
      "epoch:39 step:37461 [D loss: 0.041448, acc.: 99.22%] [G loss: 5.138698]\n",
      "epoch:39 step:37462 [D loss: 0.277328, acc.: 87.50%] [G loss: 5.547121]\n",
      "epoch:39 step:37463 [D loss: 0.148422, acc.: 97.66%] [G loss: 3.684242]\n",
      "epoch:39 step:37464 [D loss: 0.569015, acc.: 64.06%] [G loss: 5.366238]\n",
      "epoch:39 step:37465 [D loss: 0.187829, acc.: 95.31%] [G loss: 3.791435]\n",
      "epoch:39 step:37466 [D loss: 0.393533, acc.: 78.12%] [G loss: 4.084925]\n",
      "epoch:39 step:37467 [D loss: 0.036882, acc.: 100.00%] [G loss: 3.334833]\n",
      "epoch:39 step:37468 [D loss: 0.078041, acc.: 100.00%] [G loss: 3.060203]\n",
      "epoch:39 step:37469 [D loss: 0.046984, acc.: 99.22%] [G loss: 5.129351]\n",
      "epoch:39 step:37470 [D loss: 0.193343, acc.: 95.31%] [G loss: 2.598547]\n",
      "epoch:39 step:37471 [D loss: 0.062131, acc.: 99.22%] [G loss: 3.444690]\n",
      "epoch:39 step:37472 [D loss: 0.112384, acc.: 99.22%] [G loss: 2.430996]\n",
      "epoch:39 step:37473 [D loss: 0.131878, acc.: 99.22%] [G loss: 6.669223]\n",
      "epoch:39 step:37474 [D loss: 0.188146, acc.: 92.97%] [G loss: 2.529157]\n",
      "epoch:39 step:37475 [D loss: 0.218593, acc.: 91.41%] [G loss: 2.902552]\n",
      "epoch:39 step:37476 [D loss: 0.339430, acc.: 89.06%] [G loss: 2.485902]\n",
      "epoch:39 step:37477 [D loss: 0.249684, acc.: 89.06%] [G loss: 4.674914]\n",
      "epoch:39 step:37478 [D loss: 1.026180, acc.: 35.94%] [G loss: 4.722391]\n",
      "epoch:39 step:37479 [D loss: 0.542444, acc.: 66.41%] [G loss: 2.257247]\n",
      "epoch:39 step:37480 [D loss: 0.034890, acc.: 99.22%] [G loss: 6.962357]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 10\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([noise, label])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        valid = self.discriminator([img, label])\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512 * 7 * 7, input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7,7, 512)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3,strides=1,padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3,strides=1, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(Conv2D(self.channels, strides=2,kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.img_rows*self.img_cols*self.channels, input_dim=np.prod(self.img_shape)))\n",
    "        model.add(Reshape((self.img_rows,self.img_cols,self.channels)))\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2,padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "        img = Input(shape=self.img_shape)\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
    "        flat_img = Flatten()(img)\n",
    "\n",
    "        model_input = multiply([flat_img, label_embedding])\n",
    "\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model([img, label], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (_, _) = fashion_mnist.load_data()\n",
    "\n",
    "        # Configure input\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "                # Generate a half batch of new images\n",
    "                gen_imgs = self.generator.predict([noise, labels])\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on labels\n",
    "                sampled_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
    "\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,global_step, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if  global_step % sample_interval == 0:\n",
    "                    self.sample_images(epoch,global_step)\n",
    "\n",
    "\n",
    "    def sample_images(self, epoch,global_step):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        if not os.path.isdir('images_cgan_fashion_mnist'):\n",
    "            os.mkdir('images_cgan_fashion_mnist')\n",
    "        fig.savefig(\"images_cgan_fashion_mnist/epoch_%d_step_%d.png\" % (epoch,global_step))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cgan = CGAN()\n",
    "    cgan.train(epochs=40, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
