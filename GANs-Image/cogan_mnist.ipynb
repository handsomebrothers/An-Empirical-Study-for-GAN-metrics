{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "=================================================================\n",
      "Total params: 160,512\n",
      "Trainable params: 158,976\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:1 [D1 loss: 0.569182, acc.: 51.56%] [D2 loss: 0.589382, acc.: 77.34%] [G loss: 1.364645]\n",
      "epoch:0 step:2 [D1 loss: 0.478858, acc.: 61.72%] [D2 loss: 0.381388, acc.: 75.00%] [G loss: 1.643843]\n",
      "epoch:0 step:3 [D1 loss: 0.332318, acc.: 78.91%] [D2 loss: 0.282622, acc.: 88.28%] [G loss: 2.116379]\n",
      "epoch:0 step:4 [D1 loss: 0.264059, acc.: 92.97%] [D2 loss: 0.200256, acc.: 98.44%] [G loss: 2.728363]\n",
      "epoch:0 step:5 [D1 loss: 0.169261, acc.: 98.44%] [D2 loss: 0.161291, acc.: 100.00%] [G loss: 3.325578]\n",
      "epoch:0 step:6 [D1 loss: 0.139687, acc.: 100.00%] [D2 loss: 0.137714, acc.: 100.00%] [G loss: 3.655188]\n",
      "epoch:0 step:7 [D1 loss: 0.123926, acc.: 100.00%] [D2 loss: 0.107207, acc.: 100.00%] [G loss: 4.060299]\n",
      "epoch:0 step:8 [D1 loss: 0.097487, acc.: 100.00%] [D2 loss: 0.081975, acc.: 100.00%] [G loss: 4.439066]\n",
      "epoch:0 step:9 [D1 loss: 0.078853, acc.: 100.00%] [D2 loss: 0.066776, acc.: 100.00%] [G loss: 4.690831]\n",
      "epoch:0 step:10 [D1 loss: 0.070649, acc.: 100.00%] [D2 loss: 0.071406, acc.: 100.00%] [G loss: 5.014606]\n",
      "epoch:0 step:11 [D1 loss: 0.062889, acc.: 100.00%] [D2 loss: 0.058652, acc.: 100.00%] [G loss: 5.327691]\n",
      "epoch:0 step:12 [D1 loss: 0.061015, acc.: 100.00%] [D2 loss: 0.048762, acc.: 100.00%] [G loss: 5.585679]\n",
      "epoch:0 step:13 [D1 loss: 0.047602, acc.: 100.00%] [D2 loss: 0.044861, acc.: 100.00%] [G loss: 5.777767]\n",
      "epoch:0 step:14 [D1 loss: 0.040668, acc.: 100.00%] [D2 loss: 0.039319, acc.: 100.00%] [G loss: 6.001331]\n",
      "epoch:0 step:15 [D1 loss: 0.042504, acc.: 100.00%] [D2 loss: 0.034984, acc.: 100.00%] [G loss: 6.158264]\n",
      "epoch:0 step:16 [D1 loss: 0.031787, acc.: 100.00%] [D2 loss: 0.029740, acc.: 100.00%] [G loss: 6.336268]\n",
      "epoch:0 step:17 [D1 loss: 0.036270, acc.: 100.00%] [D2 loss: 0.035087, acc.: 100.00%] [G loss: 6.509195]\n",
      "epoch:0 step:18 [D1 loss: 0.031196, acc.: 100.00%] [D2 loss: 0.030080, acc.: 100.00%] [G loss: 6.647073]\n",
      "epoch:0 step:19 [D1 loss: 0.025657, acc.: 100.00%] [D2 loss: 0.027579, acc.: 100.00%] [G loss: 6.816799]\n",
      "epoch:0 step:20 [D1 loss: 0.031490, acc.: 100.00%] [D2 loss: 0.022002, acc.: 100.00%] [G loss: 6.924981]\n",
      "epoch:0 step:21 [D1 loss: 0.020059, acc.: 100.00%] [D2 loss: 0.018434, acc.: 100.00%] [G loss: 7.257260]\n",
      "epoch:0 step:22 [D1 loss: 0.027863, acc.: 100.00%] [D2 loss: 0.022532, acc.: 100.00%] [G loss: 7.191414]\n",
      "epoch:0 step:23 [D1 loss: 0.023850, acc.: 100.00%] [D2 loss: 0.025166, acc.: 100.00%] [G loss: 7.413207]\n",
      "epoch:0 step:24 [D1 loss: 0.020031, acc.: 100.00%] [D2 loss: 0.016814, acc.: 100.00%] [G loss: 7.550214]\n",
      "epoch:0 step:25 [D1 loss: 0.017682, acc.: 100.00%] [D2 loss: 0.017409, acc.: 100.00%] [G loss: 7.691715]\n",
      "epoch:0 step:26 [D1 loss: 0.027380, acc.: 100.00%] [D2 loss: 0.018775, acc.: 100.00%] [G loss: 7.799084]\n",
      "epoch:0 step:27 [D1 loss: 0.016232, acc.: 100.00%] [D2 loss: 0.016233, acc.: 100.00%] [G loss: 7.846023]\n",
      "epoch:0 step:28 [D1 loss: 0.015286, acc.: 100.00%] [D2 loss: 0.017086, acc.: 100.00%] [G loss: 8.025417]\n",
      "epoch:0 step:29 [D1 loss: 0.017077, acc.: 100.00%] [D2 loss: 0.016311, acc.: 100.00%] [G loss: 8.049351]\n",
      "epoch:0 step:30 [D1 loss: 0.015671, acc.: 100.00%] [D2 loss: 0.015242, acc.: 100.00%] [G loss: 8.206289]\n",
      "epoch:0 step:31 [D1 loss: 0.018570, acc.: 100.00%] [D2 loss: 0.016883, acc.: 100.00%] [G loss: 8.346106]\n",
      "epoch:0 step:32 [D1 loss: 0.015324, acc.: 100.00%] [D2 loss: 0.013072, acc.: 100.00%] [G loss: 8.526391]\n",
      "epoch:0 step:33 [D1 loss: 0.012303, acc.: 100.00%] [D2 loss: 0.012895, acc.: 100.00%] [G loss: 8.401402]\n",
      "epoch:0 step:34 [D1 loss: 0.014873, acc.: 100.00%] [D2 loss: 0.016970, acc.: 100.00%] [G loss: 8.406788]\n",
      "epoch:0 step:35 [D1 loss: 0.013845, acc.: 100.00%] [D2 loss: 0.014114, acc.: 100.00%] [G loss: 8.570497]\n",
      "epoch:0 step:36 [D1 loss: 0.012590, acc.: 100.00%] [D2 loss: 0.012140, acc.: 100.00%] [G loss: 8.716305]\n",
      "epoch:0 step:37 [D1 loss: 0.013876, acc.: 100.00%] [D2 loss: 0.012311, acc.: 100.00%] [G loss: 8.699435]\n",
      "epoch:0 step:38 [D1 loss: 0.009587, acc.: 100.00%] [D2 loss: 0.010698, acc.: 100.00%] [G loss: 8.661205]\n",
      "epoch:0 step:39 [D1 loss: 0.013182, acc.: 100.00%] [D2 loss: 0.015588, acc.: 100.00%] [G loss: 8.798351]\n",
      "epoch:0 step:40 [D1 loss: 0.012439, acc.: 100.00%] [D2 loss: 0.015225, acc.: 100.00%] [G loss: 8.874273]\n",
      "epoch:0 step:41 [D1 loss: 0.009935, acc.: 100.00%] [D2 loss: 0.010573, acc.: 100.00%] [G loss: 8.910840]\n",
      "epoch:0 step:42 [D1 loss: 0.013061, acc.: 100.00%] [D2 loss: 0.012859, acc.: 100.00%] [G loss: 8.893915]\n",
      "epoch:0 step:43 [D1 loss: 0.013773, acc.: 100.00%] [D2 loss: 0.013079, acc.: 100.00%] [G loss: 9.123263]\n",
      "epoch:0 step:44 [D1 loss: 0.015001, acc.: 100.00%] [D2 loss: 0.012329, acc.: 100.00%] [G loss: 9.219377]\n",
      "epoch:0 step:45 [D1 loss: 0.010946, acc.: 100.00%] [D2 loss: 0.012747, acc.: 100.00%] [G loss: 9.358400]\n",
      "epoch:0 step:46 [D1 loss: 0.010271, acc.: 100.00%] [D2 loss: 0.008405, acc.: 100.00%] [G loss: 9.288632]\n",
      "epoch:0 step:47 [D1 loss: 0.013789, acc.: 100.00%] [D2 loss: 0.015466, acc.: 100.00%] [G loss: 9.242708]\n",
      "epoch:0 step:48 [D1 loss: 0.011393, acc.: 100.00%] [D2 loss: 0.014367, acc.: 100.00%] [G loss: 9.258997]\n",
      "epoch:0 step:49 [D1 loss: 0.012780, acc.: 100.00%] [D2 loss: 0.013671, acc.: 100.00%] [G loss: 9.537643]\n",
      "epoch:0 step:50 [D1 loss: 0.012432, acc.: 100.00%] [D2 loss: 0.014640, acc.: 100.00%] [G loss: 9.480347]\n",
      "epoch:0 step:51 [D1 loss: 0.011890, acc.: 100.00%] [D2 loss: 0.011857, acc.: 100.00%] [G loss: 9.805485]\n",
      "epoch:0 step:52 [D1 loss: 0.014020, acc.: 100.00%] [D2 loss: 0.013661, acc.: 100.00%] [G loss: 9.944301]\n",
      "epoch:0 step:53 [D1 loss: 0.016447, acc.: 100.00%] [D2 loss: 0.017785, acc.: 100.00%] [G loss: 9.998018]\n",
      "epoch:0 step:54 [D1 loss: 0.009102, acc.: 100.00%] [D2 loss: 0.010509, acc.: 100.00%] [G loss: 9.899681]\n",
      "epoch:0 step:55 [D1 loss: 0.013419, acc.: 100.00%] [D2 loss: 0.011279, acc.: 100.00%] [G loss: 10.038484]\n",
      "epoch:0 step:56 [D1 loss: 0.017495, acc.: 100.00%] [D2 loss: 0.009355, acc.: 100.00%] [G loss: 10.114748]\n",
      "epoch:0 step:57 [D1 loss: 0.016930, acc.: 100.00%] [D2 loss: 0.009253, acc.: 100.00%] [G loss: 9.969106]\n",
      "epoch:0 step:58 [D1 loss: 0.013774, acc.: 100.00%] [D2 loss: 0.015843, acc.: 100.00%] [G loss: 10.351334]\n",
      "epoch:0 step:59 [D1 loss: 0.013243, acc.: 100.00%] [D2 loss: 0.015336, acc.: 100.00%] [G loss: 10.237703]\n",
      "epoch:0 step:60 [D1 loss: 0.014102, acc.: 100.00%] [D2 loss: 0.015201, acc.: 100.00%] [G loss: 10.342978]\n",
      "epoch:0 step:61 [D1 loss: 0.013952, acc.: 100.00%] [D2 loss: 0.017139, acc.: 100.00%] [G loss: 10.478370]\n",
      "epoch:0 step:62 [D1 loss: 0.016942, acc.: 100.00%] [D2 loss: 0.017952, acc.: 100.00%] [G loss: 10.886003]\n",
      "epoch:0 step:63 [D1 loss: 0.020615, acc.: 100.00%] [D2 loss: 0.026362, acc.: 99.22%] [G loss: 10.670353]\n",
      "epoch:0 step:64 [D1 loss: 0.027525, acc.: 99.22%] [D2 loss: 0.010837, acc.: 100.00%] [G loss: 11.452710]\n",
      "epoch:0 step:65 [D1 loss: 0.077477, acc.: 96.09%] [D2 loss: 0.041357, acc.: 98.44%] [G loss: 10.771477]\n",
      "epoch:0 step:66 [D1 loss: 0.020513, acc.: 100.00%] [D2 loss: 0.027107, acc.: 100.00%] [G loss: 11.124149]\n",
      "epoch:0 step:67 [D1 loss: 0.023894, acc.: 100.00%] [D2 loss: 0.025730, acc.: 100.00%] [G loss: 11.705362]\n",
      "epoch:0 step:68 [D1 loss: 0.163265, acc.: 96.09%] [D2 loss: 0.190863, acc.: 89.84%] [G loss: 10.741475]\n",
      "epoch:0 step:69 [D1 loss: 0.069875, acc.: 96.88%] [D2 loss: 0.030030, acc.: 99.22%] [G loss: 12.623884]\n",
      "epoch:0 step:70 [D1 loss: 0.360398, acc.: 87.50%] [D2 loss: 0.760415, acc.: 62.50%] [G loss: 9.212974]\n",
      "epoch:0 step:71 [D1 loss: 0.140731, acc.: 91.41%] [D2 loss: 0.249799, acc.: 89.06%] [G loss: 9.551132]\n",
      "epoch:0 step:72 [D1 loss: 0.076375, acc.: 96.88%] [D2 loss: 0.138945, acc.: 91.41%] [G loss: 10.594422]\n",
      "epoch:0 step:73 [D1 loss: 0.025784, acc.: 99.22%] [D2 loss: 0.045300, acc.: 98.44%] [G loss: 11.133862]\n",
      "epoch:0 step:74 [D1 loss: 0.013850, acc.: 100.00%] [D2 loss: 0.029434, acc.: 100.00%] [G loss: 11.375864]\n",
      "epoch:0 step:75 [D1 loss: 0.020817, acc.: 100.00%] [D2 loss: 0.035217, acc.: 98.44%] [G loss: 11.116383]\n",
      "epoch:0 step:76 [D1 loss: 0.024234, acc.: 99.22%] [D2 loss: 0.030111, acc.: 100.00%] [G loss: 10.539994]\n",
      "epoch:0 step:77 [D1 loss: 0.061451, acc.: 96.88%] [D2 loss: 0.083232, acc.: 97.66%] [G loss: 11.106590]\n",
      "epoch:0 step:78 [D1 loss: 0.064547, acc.: 97.66%] [D2 loss: 0.061426, acc.: 97.66%] [G loss: 10.355225]\n",
      "epoch:0 step:79 [D1 loss: 0.037733, acc.: 99.22%] [D2 loss: 0.046030, acc.: 99.22%] [G loss: 10.458817]\n",
      "epoch:0 step:80 [D1 loss: 0.023550, acc.: 100.00%] [D2 loss: 0.030221, acc.: 99.22%] [G loss: 10.379064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:81 [D1 loss: 0.042361, acc.: 98.44%] [D2 loss: 0.031608, acc.: 100.00%] [G loss: 10.590966]\n",
      "epoch:0 step:82 [D1 loss: 0.059716, acc.: 97.66%] [D2 loss: 0.042152, acc.: 100.00%] [G loss: 10.774583]\n",
      "epoch:0 step:83 [D1 loss: 0.144527, acc.: 95.31%] [D2 loss: 0.129568, acc.: 93.75%] [G loss: 10.120764]\n",
      "epoch:0 step:84 [D1 loss: 0.023194, acc.: 100.00%] [D2 loss: 0.035554, acc.: 98.44%] [G loss: 10.665756]\n",
      "epoch:0 step:85 [D1 loss: 0.186609, acc.: 92.97%] [D2 loss: 0.081559, acc.: 96.88%] [G loss: 10.070204]\n",
      "epoch:0 step:86 [D1 loss: 0.032954, acc.: 99.22%] [D2 loss: 0.067124, acc.: 98.44%] [G loss: 10.712704]\n",
      "epoch:0 step:87 [D1 loss: 0.081205, acc.: 96.88%] [D2 loss: 0.043208, acc.: 100.00%] [G loss: 10.925547]\n",
      "epoch:0 step:88 [D1 loss: 0.062684, acc.: 97.66%] [D2 loss: 0.058795, acc.: 99.22%] [G loss: 10.400395]\n",
      "epoch:0 step:89 [D1 loss: 0.057572, acc.: 100.00%] [D2 loss: 0.041386, acc.: 99.22%] [G loss: 10.475477]\n",
      "epoch:0 step:90 [D1 loss: 0.070739, acc.: 98.44%] [D2 loss: 0.036483, acc.: 99.22%] [G loss: 9.840469]\n",
      "epoch:0 step:91 [D1 loss: 0.083260, acc.: 97.66%] [D2 loss: 0.067097, acc.: 96.88%] [G loss: 11.726627]\n",
      "epoch:0 step:92 [D1 loss: 0.499248, acc.: 83.59%] [D2 loss: 0.323607, acc.: 85.94%] [G loss: 7.907608]\n",
      "epoch:0 step:93 [D1 loss: 0.224229, acc.: 89.84%] [D2 loss: 0.042633, acc.: 99.22%] [G loss: 11.977607]\n",
      "epoch:0 step:94 [D1 loss: 0.082185, acc.: 96.88%] [D2 loss: 0.040077, acc.: 99.22%] [G loss: 11.559368]\n",
      "epoch:0 step:95 [D1 loss: 0.043264, acc.: 98.44%] [D2 loss: 0.037627, acc.: 100.00%] [G loss: 10.273451]\n",
      "epoch:0 step:96 [D1 loss: 0.104321, acc.: 95.31%] [D2 loss: 0.115984, acc.: 96.09%] [G loss: 10.334010]\n",
      "epoch:0 step:97 [D1 loss: 0.152736, acc.: 92.97%] [D2 loss: 0.126274, acc.: 96.88%] [G loss: 10.291945]\n",
      "epoch:0 step:98 [D1 loss: 0.054098, acc.: 99.22%] [D2 loss: 0.050967, acc.: 99.22%] [G loss: 12.944270]\n",
      "epoch:0 step:99 [D1 loss: 0.835382, acc.: 64.84%] [D2 loss: 0.688985, acc.: 76.56%] [G loss: 4.910080]\n",
      "epoch:0 step:100 [D1 loss: 0.674261, acc.: 77.34%] [D2 loss: 0.469128, acc.: 75.00%] [G loss: 6.256123]\n",
      "epoch:0 step:101 [D1 loss: 0.198105, acc.: 88.28%] [D2 loss: 0.033660, acc.: 100.00%] [G loss: 9.891366]\n",
      "epoch:0 step:102 [D1 loss: 0.043682, acc.: 98.44%] [D2 loss: 0.115805, acc.: 97.66%] [G loss: 9.997593]\n",
      "epoch:0 step:103 [D1 loss: 0.049754, acc.: 98.44%] [D2 loss: 0.062563, acc.: 98.44%] [G loss: 9.365875]\n",
      "epoch:0 step:104 [D1 loss: 0.092847, acc.: 96.09%] [D2 loss: 0.071107, acc.: 98.44%] [G loss: 10.562198]\n",
      "epoch:0 step:105 [D1 loss: 0.078565, acc.: 98.44%] [D2 loss: 0.078732, acc.: 99.22%] [G loss: 7.858783]\n",
      "epoch:0 step:106 [D1 loss: 0.129111, acc.: 93.75%] [D2 loss: 0.075283, acc.: 99.22%] [G loss: 9.242590]\n",
      "epoch:0 step:107 [D1 loss: 0.249043, acc.: 86.72%] [D2 loss: 0.091491, acc.: 96.88%] [G loss: 8.542362]\n",
      "epoch:0 step:108 [D1 loss: 0.226042, acc.: 89.06%] [D2 loss: 0.060433, acc.: 100.00%] [G loss: 10.584036]\n",
      "epoch:0 step:109 [D1 loss: 0.776118, acc.: 63.28%] [D2 loss: 0.480649, acc.: 75.00%] [G loss: 5.192492]\n",
      "epoch:0 step:110 [D1 loss: 0.194924, acc.: 89.84%] [D2 loss: 0.114997, acc.: 95.31%] [G loss: 9.301667]\n",
      "epoch:0 step:111 [D1 loss: 0.379619, acc.: 79.69%] [D2 loss: 0.286196, acc.: 90.62%] [G loss: 6.111659]\n",
      "epoch:0 step:112 [D1 loss: 0.075477, acc.: 96.09%] [D2 loss: 0.040799, acc.: 100.00%] [G loss: 8.945078]\n",
      "epoch:0 step:113 [D1 loss: 0.084017, acc.: 98.44%] [D2 loss: 0.050468, acc.: 99.22%] [G loss: 9.899763]\n",
      "epoch:0 step:114 [D1 loss: 0.353764, acc.: 82.81%] [D2 loss: 0.153961, acc.: 92.97%] [G loss: 9.320885]\n",
      "epoch:0 step:115 [D1 loss: 0.112709, acc.: 96.88%] [D2 loss: 0.097754, acc.: 98.44%] [G loss: 9.533089]\n",
      "epoch:0 step:116 [D1 loss: 0.150109, acc.: 94.53%] [D2 loss: 0.072351, acc.: 99.22%] [G loss: 7.981225]\n",
      "epoch:0 step:117 [D1 loss: 0.239390, acc.: 89.06%] [D2 loss: 0.099379, acc.: 98.44%] [G loss: 9.049989]\n",
      "epoch:0 step:118 [D1 loss: 0.221199, acc.: 90.62%] [D2 loss: 0.074711, acc.: 99.22%] [G loss: 8.786918]\n",
      "epoch:0 step:119 [D1 loss: 0.201550, acc.: 91.41%] [D2 loss: 0.067660, acc.: 100.00%] [G loss: 9.300634]\n",
      "epoch:0 step:120 [D1 loss: 0.833424, acc.: 64.06%] [D2 loss: 0.616825, acc.: 70.31%] [G loss: 7.364650]\n",
      "epoch:0 step:121 [D1 loss: 0.138201, acc.: 94.53%] [D2 loss: 0.083723, acc.: 99.22%] [G loss: 10.441042]\n",
      "epoch:0 step:122 [D1 loss: 0.447612, acc.: 78.12%] [D2 loss: 0.200046, acc.: 92.19%] [G loss: 7.590299]\n",
      "epoch:0 step:123 [D1 loss: 0.197748, acc.: 89.84%] [D2 loss: 0.119870, acc.: 97.66%] [G loss: 8.365026]\n",
      "epoch:0 step:124 [D1 loss: 0.367551, acc.: 80.47%] [D2 loss: 0.101050, acc.: 98.44%] [G loss: 9.927943]\n",
      "epoch:0 step:125 [D1 loss: 0.597171, acc.: 72.66%] [D2 loss: 0.237475, acc.: 87.50%] [G loss: 9.888439]\n",
      "epoch:0 step:126 [D1 loss: 0.283091, acc.: 86.72%] [D2 loss: 0.077281, acc.: 100.00%] [G loss: 9.645062]\n",
      "epoch:0 step:127 [D1 loss: 0.793131, acc.: 65.62%] [D2 loss: 0.331502, acc.: 81.25%] [G loss: 8.034822]\n",
      "epoch:0 step:128 [D1 loss: 0.403727, acc.: 78.12%] [D2 loss: 0.275877, acc.: 90.62%] [G loss: 7.180363]\n",
      "epoch:0 step:129 [D1 loss: 0.167635, acc.: 96.09%] [D2 loss: 0.066677, acc.: 100.00%] [G loss: 8.972333]\n",
      "epoch:0 step:130 [D1 loss: 0.942364, acc.: 50.78%] [D2 loss: 0.680786, acc.: 67.97%] [G loss: 5.744108]\n",
      "epoch:0 step:131 [D1 loss: 0.178372, acc.: 89.06%] [D2 loss: 0.056235, acc.: 100.00%] [G loss: 11.173299]\n",
      "epoch:0 step:132 [D1 loss: 0.596953, acc.: 66.41%] [D2 loss: 0.315901, acc.: 81.25%] [G loss: 7.625593]\n",
      "epoch:0 step:133 [D1 loss: 0.452941, acc.: 76.56%] [D2 loss: 0.179437, acc.: 95.31%] [G loss: 7.567812]\n",
      "epoch:0 step:134 [D1 loss: 0.445546, acc.: 76.56%] [D2 loss: 0.415959, acc.: 81.25%] [G loss: 5.165792]\n",
      "epoch:0 step:135 [D1 loss: 0.204691, acc.: 91.41%] [D2 loss: 0.109198, acc.: 100.00%] [G loss: 9.152966]\n",
      "epoch:0 step:136 [D1 loss: 0.585529, acc.: 65.62%] [D2 loss: 0.374900, acc.: 75.00%] [G loss: 8.376469]\n",
      "epoch:0 step:137 [D1 loss: 0.356292, acc.: 80.47%] [D2 loss: 0.131310, acc.: 97.66%] [G loss: 9.181150]\n",
      "epoch:0 step:138 [D1 loss: 0.590206, acc.: 71.09%] [D2 loss: 0.284161, acc.: 85.94%] [G loss: 7.301152]\n",
      "epoch:0 step:139 [D1 loss: 0.481405, acc.: 68.75%] [D2 loss: 0.299539, acc.: 89.84%] [G loss: 6.568372]\n",
      "epoch:0 step:140 [D1 loss: 0.211904, acc.: 92.19%] [D2 loss: 0.157454, acc.: 97.66%] [G loss: 8.350173]\n",
      "epoch:0 step:141 [D1 loss: 0.755809, acc.: 53.91%] [D2 loss: 0.494355, acc.: 65.62%] [G loss: 9.494000]\n",
      "epoch:0 step:142 [D1 loss: 0.492130, acc.: 72.66%] [D2 loss: 0.147254, acc.: 98.44%] [G loss: 8.981312]\n",
      "epoch:0 step:143 [D1 loss: 0.854404, acc.: 58.59%] [D2 loss: 1.068127, acc.: 44.53%] [G loss: 3.143847]\n",
      "epoch:0 step:144 [D1 loss: 0.283997, acc.: 83.59%] [D2 loss: 0.289771, acc.: 81.25%] [G loss: 9.043936]\n",
      "epoch:0 step:145 [D1 loss: 0.493129, acc.: 76.56%] [D2 loss: 0.180992, acc.: 97.66%] [G loss: 8.617336]\n",
      "epoch:0 step:146 [D1 loss: 0.372973, acc.: 81.25%] [D2 loss: 0.175702, acc.: 98.44%] [G loss: 7.611207]\n",
      "epoch:0 step:147 [D1 loss: 0.400515, acc.: 77.34%] [D2 loss: 0.265890, acc.: 93.75%] [G loss: 7.732900]\n",
      "epoch:0 step:148 [D1 loss: 0.380185, acc.: 82.81%] [D2 loss: 0.189071, acc.: 94.53%] [G loss: 9.344219]\n",
      "epoch:0 step:149 [D1 loss: 0.909708, acc.: 46.09%] [D2 loss: 0.593247, acc.: 67.19%] [G loss: 3.955455]\n",
      "epoch:0 step:150 [D1 loss: 0.305369, acc.: 78.91%] [D2 loss: 0.136934, acc.: 99.22%] [G loss: 9.393853]\n",
      "epoch:0 step:151 [D1 loss: 0.714993, acc.: 54.69%] [D2 loss: 0.474460, acc.: 71.88%] [G loss: 5.616409]\n",
      "epoch:0 step:152 [D1 loss: 0.401020, acc.: 76.56%] [D2 loss: 0.308046, acc.: 89.06%] [G loss: 6.519067]\n",
      "epoch:0 step:153 [D1 loss: 0.443919, acc.: 73.44%] [D2 loss: 0.454316, acc.: 78.91%] [G loss: 5.870322]\n",
      "epoch:0 step:154 [D1 loss: 0.288853, acc.: 89.84%] [D2 loss: 0.260781, acc.: 90.62%] [G loss: 7.574154]\n",
      "epoch:0 step:155 [D1 loss: 0.480223, acc.: 71.09%] [D2 loss: 0.260813, acc.: 89.06%] [G loss: 8.970793]\n",
      "epoch:0 step:156 [D1 loss: 0.820993, acc.: 55.47%] [D2 loss: 0.498018, acc.: 65.62%] [G loss: 7.324306]\n",
      "epoch:0 step:157 [D1 loss: 0.694596, acc.: 57.81%] [D2 loss: 0.658311, acc.: 53.91%] [G loss: 4.431341]\n",
      "epoch:0 step:158 [D1 loss: 0.309246, acc.: 83.59%] [D2 loss: 0.228204, acc.: 97.66%] [G loss: 7.355411]\n",
      "epoch:0 step:159 [D1 loss: 0.512658, acc.: 67.97%] [D2 loss: 0.384044, acc.: 81.25%] [G loss: 6.488448]\n",
      "epoch:0 step:160 [D1 loss: 0.588292, acc.: 60.94%] [D2 loss: 0.393915, acc.: 78.91%] [G loss: 7.547042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:161 [D1 loss: 0.669311, acc.: 60.16%] [D2 loss: 0.587113, acc.: 65.62%] [G loss: 5.665946]\n",
      "epoch:0 step:162 [D1 loss: 0.467688, acc.: 75.00%] [D2 loss: 0.341282, acc.: 87.50%] [G loss: 7.388217]\n",
      "epoch:0 step:163 [D1 loss: 0.623842, acc.: 62.50%] [D2 loss: 0.432358, acc.: 74.22%] [G loss: 6.129644]\n",
      "epoch:0 step:164 [D1 loss: 0.664074, acc.: 57.81%] [D2 loss: 0.603922, acc.: 64.84%] [G loss: 5.716417]\n",
      "epoch:0 step:165 [D1 loss: 0.557528, acc.: 64.06%] [D2 loss: 0.499284, acc.: 71.09%] [G loss: 4.852267]\n",
      "epoch:0 step:166 [D1 loss: 0.518499, acc.: 69.53%] [D2 loss: 0.446361, acc.: 71.09%] [G loss: 7.125200]\n",
      "epoch:0 step:167 [D1 loss: 0.826796, acc.: 43.75%] [D2 loss: 0.539961, acc.: 60.16%] [G loss: 7.455234]\n",
      "epoch:0 step:168 [D1 loss: 0.730589, acc.: 53.12%] [D2 loss: 0.422031, acc.: 75.78%] [G loss: 6.404889]\n",
      "epoch:0 step:169 [D1 loss: 0.803150, acc.: 50.78%] [D2 loss: 0.908095, acc.: 46.88%] [G loss: 2.372348]\n",
      "epoch:0 step:170 [D1 loss: 0.416418, acc.: 71.88%] [D2 loss: 0.344191, acc.: 84.38%] [G loss: 6.688920]\n",
      "epoch:0 step:171 [D1 loss: 0.731389, acc.: 50.00%] [D2 loss: 0.515936, acc.: 61.72%] [G loss: 4.367722]\n",
      "epoch:0 step:172 [D1 loss: 0.589486, acc.: 59.38%] [D2 loss: 0.490093, acc.: 78.12%] [G loss: 5.699122]\n",
      "epoch:0 step:173 [D1 loss: 0.677660, acc.: 54.69%] [D2 loss: 0.513996, acc.: 69.53%] [G loss: 5.115549]\n",
      "epoch:0 step:174 [D1 loss: 0.669264, acc.: 53.91%] [D2 loss: 0.719545, acc.: 51.56%] [G loss: 3.228473]\n",
      "epoch:0 step:175 [D1 loss: 0.577362, acc.: 58.59%] [D2 loss: 0.692056, acc.: 53.12%] [G loss: 3.894723]\n",
      "epoch:0 step:176 [D1 loss: 0.591334, acc.: 60.16%] [D2 loss: 0.657837, acc.: 56.25%] [G loss: 3.563666]\n",
      "epoch:0 step:177 [D1 loss: 0.567997, acc.: 64.84%] [D2 loss: 0.614764, acc.: 65.62%] [G loss: 3.810280]\n",
      "epoch:0 step:178 [D1 loss: 0.611317, acc.: 60.16%] [D2 loss: 0.572985, acc.: 66.41%] [G loss: 4.343683]\n",
      "epoch:0 step:179 [D1 loss: 0.682297, acc.: 51.56%] [D2 loss: 0.606481, acc.: 56.25%] [G loss: 3.651662]\n",
      "epoch:0 step:180 [D1 loss: 0.549477, acc.: 66.41%] [D2 loss: 0.555454, acc.: 67.97%] [G loss: 4.423966]\n",
      "epoch:0 step:181 [D1 loss: 0.626474, acc.: 56.25%] [D2 loss: 0.620869, acc.: 59.38%] [G loss: 3.674116]\n",
      "epoch:0 step:182 [D1 loss: 0.599708, acc.: 62.50%] [D2 loss: 0.572040, acc.: 63.28%] [G loss: 4.060077]\n",
      "epoch:0 step:183 [D1 loss: 0.628215, acc.: 57.03%] [D2 loss: 0.591277, acc.: 60.94%] [G loss: 3.555624]\n",
      "epoch:0 step:184 [D1 loss: 0.606378, acc.: 58.59%] [D2 loss: 0.631106, acc.: 56.25%] [G loss: 3.793649]\n",
      "epoch:0 step:185 [D1 loss: 0.617600, acc.: 60.16%] [D2 loss: 0.548333, acc.: 63.28%] [G loss: 3.926016]\n",
      "epoch:0 step:186 [D1 loss: 0.741585, acc.: 41.41%] [D2 loss: 0.830218, acc.: 40.62%] [G loss: 2.031666]\n",
      "epoch:0 step:187 [D1 loss: 0.545400, acc.: 59.38%] [D2 loss: 0.494578, acc.: 71.88%] [G loss: 3.536369]\n",
      "epoch:0 step:188 [D1 loss: 0.757185, acc.: 40.62%] [D2 loss: 0.655554, acc.: 52.34%] [G loss: 2.514664]\n",
      "epoch:0 step:189 [D1 loss: 0.616195, acc.: 52.34%] [D2 loss: 0.600638, acc.: 59.38%] [G loss: 3.560988]\n",
      "epoch:0 step:190 [D1 loss: 0.710470, acc.: 45.31%] [D2 loss: 0.583086, acc.: 62.50%] [G loss: 2.861127]\n",
      "epoch:0 step:191 [D1 loss: 0.714552, acc.: 41.41%] [D2 loss: 0.632471, acc.: 53.91%] [G loss: 2.483414]\n",
      "epoch:0 step:192 [D1 loss: 0.709170, acc.: 42.97%] [D2 loss: 0.622072, acc.: 54.69%] [G loss: 2.567055]\n",
      "epoch:0 step:193 [D1 loss: 0.726863, acc.: 43.75%] [D2 loss: 0.770823, acc.: 39.84%] [G loss: 1.835928]\n",
      "epoch:0 step:194 [D1 loss: 0.634822, acc.: 48.44%] [D2 loss: 0.790246, acc.: 35.16%] [G loss: 1.836843]\n",
      "epoch:0 step:195 [D1 loss: 0.639980, acc.: 51.56%] [D2 loss: 0.639703, acc.: 53.12%] [G loss: 2.245364]\n",
      "epoch:0 step:196 [D1 loss: 0.672564, acc.: 46.09%] [D2 loss: 0.608515, acc.: 62.50%] [G loss: 2.551054]\n",
      "epoch:0 step:197 [D1 loss: 0.720610, acc.: 42.97%] [D2 loss: 0.755532, acc.: 41.41%] [G loss: 1.715672]\n",
      "epoch:0 step:198 [D1 loss: 0.644649, acc.: 49.22%] [D2 loss: 0.767465, acc.: 40.62%] [G loss: 1.653322]\n",
      "epoch:0 step:199 [D1 loss: 0.656528, acc.: 48.44%] [D2 loss: 0.669130, acc.: 48.44%] [G loss: 1.770151]\n",
      "epoch:0 step:200 [D1 loss: 0.662132, acc.: 47.66%] [D2 loss: 0.697041, acc.: 47.66%] [G loss: 1.860298]\n",
      "epoch:0 step:201 [D1 loss: 0.650185, acc.: 50.78%] [D2 loss: 0.688410, acc.: 45.31%] [G loss: 1.685397]\n",
      "epoch:0 step:202 [D1 loss: 0.637937, acc.: 56.25%] [D2 loss: 0.695717, acc.: 42.97%] [G loss: 1.775027]\n",
      "epoch:0 step:203 [D1 loss: 0.679927, acc.: 50.00%] [D2 loss: 0.694974, acc.: 47.66%] [G loss: 1.737159]\n",
      "epoch:0 step:204 [D1 loss: 0.664908, acc.: 51.56%] [D2 loss: 0.637105, acc.: 49.22%] [G loss: 1.838512]\n",
      "epoch:0 step:205 [D1 loss: 0.655918, acc.: 50.00%] [D2 loss: 0.623120, acc.: 50.00%] [G loss: 1.909453]\n",
      "epoch:0 step:206 [D1 loss: 0.634824, acc.: 53.12%] [D2 loss: 0.618571, acc.: 52.34%] [G loss: 2.025352]\n",
      "epoch:0 step:207 [D1 loss: 0.696510, acc.: 46.09%] [D2 loss: 0.699904, acc.: 39.06%] [G loss: 1.589340]\n",
      "epoch:0 step:208 [D1 loss: 0.644064, acc.: 56.25%] [D2 loss: 0.632182, acc.: 47.66%] [G loss: 1.641950]\n",
      "epoch:0 step:209 [D1 loss: 0.617230, acc.: 53.91%] [D2 loss: 0.633237, acc.: 48.44%] [G loss: 1.735581]\n",
      "epoch:0 step:210 [D1 loss: 0.672612, acc.: 53.91%] [D2 loss: 0.696619, acc.: 39.06%] [G loss: 1.556627]\n",
      "epoch:0 step:211 [D1 loss: 0.637285, acc.: 53.91%] [D2 loss: 0.644646, acc.: 46.88%] [G loss: 1.748257]\n",
      "epoch:0 step:212 [D1 loss: 0.648880, acc.: 46.88%] [D2 loss: 0.702512, acc.: 42.19%] [G loss: 1.647181]\n",
      "epoch:0 step:213 [D1 loss: 0.646669, acc.: 50.00%] [D2 loss: 0.694818, acc.: 42.19%] [G loss: 1.547447]\n",
      "epoch:0 step:214 [D1 loss: 0.660171, acc.: 48.44%] [D2 loss: 0.622323, acc.: 47.66%] [G loss: 1.645993]\n",
      "epoch:0 step:215 [D1 loss: 0.643756, acc.: 52.34%] [D2 loss: 0.607430, acc.: 47.66%] [G loss: 1.751265]\n",
      "epoch:0 step:216 [D1 loss: 0.646099, acc.: 53.91%] [D2 loss: 0.625299, acc.: 45.31%] [G loss: 1.719843]\n",
      "epoch:0 step:217 [D1 loss: 0.627652, acc.: 56.25%] [D2 loss: 0.677508, acc.: 39.84%] [G loss: 1.631148]\n",
      "epoch:0 step:218 [D1 loss: 0.621975, acc.: 57.03%] [D2 loss: 0.614165, acc.: 49.22%] [G loss: 1.786370]\n",
      "epoch:0 step:219 [D1 loss: 0.643957, acc.: 54.69%] [D2 loss: 0.649121, acc.: 42.97%] [G loss: 1.607970]\n",
      "epoch:0 step:220 [D1 loss: 0.710831, acc.: 46.09%] [D2 loss: 0.722415, acc.: 40.62%] [G loss: 1.434511]\n",
      "epoch:0 step:221 [D1 loss: 0.650005, acc.: 50.00%] [D2 loss: 0.687072, acc.: 42.97%] [G loss: 1.484814]\n",
      "epoch:0 step:222 [D1 loss: 0.641870, acc.: 53.12%] [D2 loss: 0.677748, acc.: 42.97%] [G loss: 1.513452]\n",
      "epoch:0 step:223 [D1 loss: 0.644430, acc.: 53.12%] [D2 loss: 0.664893, acc.: 46.09%] [G loss: 1.547118]\n",
      "epoch:0 step:224 [D1 loss: 0.655704, acc.: 50.00%] [D2 loss: 0.621823, acc.: 49.22%] [G loss: 1.588378]\n",
      "epoch:0 step:225 [D1 loss: 0.681977, acc.: 48.44%] [D2 loss: 0.658310, acc.: 38.28%] [G loss: 1.497440]\n",
      "epoch:0 step:226 [D1 loss: 0.640254, acc.: 52.34%] [D2 loss: 0.654877, acc.: 40.62%] [G loss: 1.521250]\n",
      "epoch:0 step:227 [D1 loss: 0.654210, acc.: 55.47%] [D2 loss: 0.676136, acc.: 41.41%] [G loss: 1.566970]\n",
      "epoch:0 step:228 [D1 loss: 0.634532, acc.: 53.91%] [D2 loss: 0.651707, acc.: 42.97%] [G loss: 1.670981]\n",
      "epoch:0 step:229 [D1 loss: 0.636678, acc.: 51.56%] [D2 loss: 0.645019, acc.: 42.97%] [G loss: 1.591433]\n",
      "epoch:0 step:230 [D1 loss: 0.627941, acc.: 51.56%] [D2 loss: 0.622696, acc.: 51.56%] [G loss: 1.633221]\n",
      "epoch:0 step:231 [D1 loss: 0.618748, acc.: 56.25%] [D2 loss: 0.670165, acc.: 42.97%] [G loss: 1.572069]\n",
      "epoch:0 step:232 [D1 loss: 0.615439, acc.: 50.00%] [D2 loss: 0.629979, acc.: 49.22%] [G loss: 1.606579]\n",
      "epoch:0 step:233 [D1 loss: 0.663269, acc.: 50.00%] [D2 loss: 0.643424, acc.: 46.88%] [G loss: 1.630310]\n",
      "epoch:0 step:234 [D1 loss: 0.657792, acc.: 52.34%] [D2 loss: 0.633735, acc.: 44.53%] [G loss: 1.640978]\n",
      "epoch:0 step:235 [D1 loss: 0.617738, acc.: 55.47%] [D2 loss: 0.634632, acc.: 43.75%] [G loss: 1.689034]\n",
      "epoch:0 step:236 [D1 loss: 0.679188, acc.: 51.56%] [D2 loss: 0.670964, acc.: 46.09%] [G loss: 1.504438]\n",
      "epoch:0 step:237 [D1 loss: 0.630155, acc.: 60.94%] [D2 loss: 0.633623, acc.: 46.88%] [G loss: 1.564655]\n",
      "epoch:0 step:238 [D1 loss: 0.635530, acc.: 55.47%] [D2 loss: 0.622204, acc.: 50.78%] [G loss: 1.637919]\n",
      "epoch:0 step:239 [D1 loss: 0.665008, acc.: 48.44%] [D2 loss: 0.614281, acc.: 49.22%] [G loss: 1.668968]\n",
      "epoch:0 step:240 [D1 loss: 0.687183, acc.: 48.44%] [D2 loss: 0.601164, acc.: 50.00%] [G loss: 1.684009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:241 [D1 loss: 0.663986, acc.: 51.56%] [D2 loss: 0.677443, acc.: 43.75%] [G loss: 1.544929]\n",
      "epoch:0 step:242 [D1 loss: 0.675900, acc.: 50.78%] [D2 loss: 0.652991, acc.: 46.88%] [G loss: 1.570542]\n",
      "epoch:0 step:243 [D1 loss: 0.686960, acc.: 45.31%] [D2 loss: 0.643290, acc.: 51.56%] [G loss: 1.594382]\n",
      "epoch:0 step:244 [D1 loss: 0.670233, acc.: 52.34%] [D2 loss: 0.616593, acc.: 59.38%] [G loss: 1.708507]\n",
      "epoch:0 step:245 [D1 loss: 0.700490, acc.: 46.88%] [D2 loss: 0.666527, acc.: 47.66%] [G loss: 1.578455]\n",
      "epoch:0 step:246 [D1 loss: 0.709802, acc.: 43.75%] [D2 loss: 0.640521, acc.: 53.12%] [G loss: 1.524535]\n",
      "epoch:0 step:247 [D1 loss: 0.674850, acc.: 50.78%] [D2 loss: 0.675551, acc.: 42.97%] [G loss: 1.558774]\n",
      "epoch:0 step:248 [D1 loss: 0.686261, acc.: 46.09%] [D2 loss: 0.649815, acc.: 41.41%] [G loss: 1.608441]\n",
      "epoch:0 step:249 [D1 loss: 0.661369, acc.: 56.25%] [D2 loss: 0.644855, acc.: 53.91%] [G loss: 1.659880]\n",
      "epoch:0 step:250 [D1 loss: 0.662039, acc.: 56.25%] [D2 loss: 0.628561, acc.: 52.34%] [G loss: 1.733094]\n",
      "epoch:0 step:251 [D1 loss: 0.703444, acc.: 48.44%] [D2 loss: 0.671399, acc.: 47.66%] [G loss: 1.563154]\n",
      "epoch:0 step:252 [D1 loss: 0.650391, acc.: 52.34%] [D2 loss: 0.621538, acc.: 59.38%] [G loss: 1.645195]\n",
      "epoch:0 step:253 [D1 loss: 0.689320, acc.: 45.31%] [D2 loss: 0.650999, acc.: 59.38%] [G loss: 1.689498]\n",
      "epoch:0 step:254 [D1 loss: 0.679272, acc.: 53.12%] [D2 loss: 0.675438, acc.: 47.66%] [G loss: 1.615623]\n",
      "epoch:0 step:255 [D1 loss: 0.676618, acc.: 53.91%] [D2 loss: 0.675443, acc.: 46.88%] [G loss: 1.528610]\n",
      "epoch:0 step:256 [D1 loss: 0.673658, acc.: 53.91%] [D2 loss: 0.670835, acc.: 46.09%] [G loss: 1.506971]\n",
      "epoch:0 step:257 [D1 loss: 0.649910, acc.: 52.34%] [D2 loss: 0.637310, acc.: 50.78%] [G loss: 1.643033]\n",
      "epoch:0 step:258 [D1 loss: 0.664489, acc.: 53.12%] [D2 loss: 0.630610, acc.: 61.72%] [G loss: 1.697603]\n",
      "epoch:0 step:259 [D1 loss: 0.716038, acc.: 44.53%] [D2 loss: 0.677085, acc.: 51.56%] [G loss: 1.474563]\n",
      "epoch:0 step:260 [D1 loss: 0.661769, acc.: 50.78%] [D2 loss: 0.636934, acc.: 55.47%] [G loss: 1.420700]\n",
      "epoch:0 step:261 [D1 loss: 0.638758, acc.: 52.34%] [D2 loss: 0.665492, acc.: 50.00%] [G loss: 1.464795]\n",
      "epoch:0 step:262 [D1 loss: 0.674831, acc.: 49.22%] [D2 loss: 0.632855, acc.: 56.25%] [G loss: 1.532017]\n",
      "epoch:0 step:263 [D1 loss: 0.714770, acc.: 43.75%] [D2 loss: 0.635354, acc.: 58.59%] [G loss: 1.515451]\n",
      "epoch:0 step:264 [D1 loss: 0.674126, acc.: 46.88%] [D2 loss: 0.652001, acc.: 48.44%] [G loss: 1.478510]\n",
      "epoch:0 step:265 [D1 loss: 0.668914, acc.: 47.66%] [D2 loss: 0.636206, acc.: 52.34%] [G loss: 1.443396]\n",
      "epoch:0 step:266 [D1 loss: 0.649222, acc.: 50.78%] [D2 loss: 0.650271, acc.: 50.00%] [G loss: 1.451560]\n",
      "epoch:0 step:267 [D1 loss: 0.652616, acc.: 50.78%] [D2 loss: 0.652114, acc.: 60.16%] [G loss: 1.506281]\n",
      "epoch:0 step:268 [D1 loss: 0.683383, acc.: 47.66%] [D2 loss: 0.647787, acc.: 60.94%] [G loss: 1.524735]\n",
      "epoch:0 step:269 [D1 loss: 0.680326, acc.: 42.97%] [D2 loss: 0.640646, acc.: 58.59%] [G loss: 1.549019]\n",
      "epoch:0 step:270 [D1 loss: 0.680409, acc.: 46.09%] [D2 loss: 0.639338, acc.: 52.34%] [G loss: 1.507056]\n",
      "epoch:0 step:271 [D1 loss: 0.687129, acc.: 42.19%] [D2 loss: 0.637699, acc.: 56.25%] [G loss: 1.478838]\n",
      "epoch:0 step:272 [D1 loss: 0.653938, acc.: 51.56%] [D2 loss: 0.665475, acc.: 57.81%] [G loss: 1.485539]\n",
      "epoch:0 step:273 [D1 loss: 0.655739, acc.: 48.44%] [D2 loss: 0.663942, acc.: 50.78%] [G loss: 1.487686]\n",
      "epoch:0 step:274 [D1 loss: 0.653080, acc.: 50.78%] [D2 loss: 0.635319, acc.: 57.03%] [G loss: 1.487141]\n",
      "epoch:0 step:275 [D1 loss: 0.663296, acc.: 47.66%] [D2 loss: 0.640136, acc.: 50.00%] [G loss: 1.510403]\n",
      "epoch:0 step:276 [D1 loss: 0.644847, acc.: 51.56%] [D2 loss: 0.691726, acc.: 41.41%] [G loss: 1.506689]\n",
      "epoch:0 step:277 [D1 loss: 0.655767, acc.: 53.91%] [D2 loss: 0.651709, acc.: 47.66%] [G loss: 1.532224]\n",
      "epoch:0 step:278 [D1 loss: 0.662783, acc.: 51.56%] [D2 loss: 0.639456, acc.: 50.00%] [G loss: 1.526464]\n",
      "epoch:0 step:279 [D1 loss: 0.627307, acc.: 57.81%] [D2 loss: 0.638294, acc.: 51.56%] [G loss: 1.566785]\n",
      "epoch:0 step:280 [D1 loss: 0.648606, acc.: 59.38%] [D2 loss: 0.613417, acc.: 58.59%] [G loss: 1.622375]\n",
      "epoch:0 step:281 [D1 loss: 0.680731, acc.: 53.91%] [D2 loss: 0.653689, acc.: 55.47%] [G loss: 1.527834]\n",
      "epoch:0 step:282 [D1 loss: 0.669691, acc.: 53.12%] [D2 loss: 0.635315, acc.: 53.91%] [G loss: 1.550934]\n",
      "epoch:0 step:283 [D1 loss: 0.654559, acc.: 57.81%] [D2 loss: 0.628487, acc.: 59.38%] [G loss: 1.550468]\n",
      "epoch:0 step:284 [D1 loss: 0.650507, acc.: 54.69%] [D2 loss: 0.637941, acc.: 51.56%] [G loss: 1.545307]\n",
      "epoch:0 step:285 [D1 loss: 0.646303, acc.: 48.44%] [D2 loss: 0.652070, acc.: 55.47%] [G loss: 1.540397]\n",
      "epoch:0 step:286 [D1 loss: 0.656772, acc.: 47.66%] [D2 loss: 0.639385, acc.: 53.91%] [G loss: 1.546644]\n",
      "epoch:0 step:287 [D1 loss: 0.654303, acc.: 53.91%] [D2 loss: 0.651190, acc.: 56.25%] [G loss: 1.529555]\n",
      "epoch:0 step:288 [D1 loss: 0.655482, acc.: 52.34%] [D2 loss: 0.665470, acc.: 53.12%] [G loss: 1.523220]\n",
      "epoch:0 step:289 [D1 loss: 0.661671, acc.: 48.44%] [D2 loss: 0.652157, acc.: 51.56%] [G loss: 1.553539]\n",
      "epoch:0 step:290 [D1 loss: 0.670586, acc.: 48.44%] [D2 loss: 0.675506, acc.: 43.75%] [G loss: 1.497457]\n",
      "epoch:0 step:291 [D1 loss: 0.684512, acc.: 47.66%] [D2 loss: 0.647957, acc.: 54.69%] [G loss: 1.452349]\n",
      "epoch:0 step:292 [D1 loss: 0.669543, acc.: 43.75%] [D2 loss: 0.667060, acc.: 53.12%] [G loss: 1.477185]\n",
      "epoch:0 step:293 [D1 loss: 0.674774, acc.: 57.81%] [D2 loss: 0.627374, acc.: 61.72%] [G loss: 1.546153]\n",
      "epoch:0 step:294 [D1 loss: 0.670418, acc.: 54.69%] [D2 loss: 0.610988, acc.: 70.31%] [G loss: 1.654472]\n",
      "epoch:0 step:295 [D1 loss: 0.680160, acc.: 46.88%] [D2 loss: 0.629492, acc.: 53.91%] [G loss: 1.541773]\n",
      "epoch:0 step:296 [D1 loss: 0.678082, acc.: 47.66%] [D2 loss: 0.693181, acc.: 42.97%] [G loss: 1.465196]\n",
      "epoch:0 step:297 [D1 loss: 0.654631, acc.: 53.12%] [D2 loss: 0.672578, acc.: 46.09%] [G loss: 1.433586]\n",
      "epoch:0 step:298 [D1 loss: 0.646308, acc.: 55.47%] [D2 loss: 0.651119, acc.: 50.78%] [G loss: 1.435696]\n",
      "epoch:0 step:299 [D1 loss: 0.666886, acc.: 54.69%] [D2 loss: 0.684387, acc.: 48.44%] [G loss: 1.514886]\n",
      "epoch:0 step:300 [D1 loss: 0.648640, acc.: 53.91%] [D2 loss: 0.669283, acc.: 51.56%] [G loss: 1.526769]\n",
      "epoch:0 step:301 [D1 loss: 0.682269, acc.: 43.75%] [D2 loss: 0.655879, acc.: 49.22%] [G loss: 1.496687]\n",
      "epoch:0 step:302 [D1 loss: 0.659358, acc.: 50.00%] [D2 loss: 0.653205, acc.: 52.34%] [G loss: 1.567167]\n",
      "epoch:0 step:303 [D1 loss: 0.677366, acc.: 42.97%] [D2 loss: 0.632395, acc.: 55.47%] [G loss: 1.550972]\n",
      "epoch:0 step:304 [D1 loss: 0.659706, acc.: 50.00%] [D2 loss: 0.660668, acc.: 55.47%] [G loss: 1.465281]\n",
      "epoch:0 step:305 [D1 loss: 0.645635, acc.: 54.69%] [D2 loss: 0.655533, acc.: 57.81%] [G loss: 1.442891]\n",
      "epoch:0 step:306 [D1 loss: 0.669678, acc.: 51.56%] [D2 loss: 0.652356, acc.: 60.94%] [G loss: 1.511707]\n",
      "epoch:0 step:307 [D1 loss: 0.624797, acc.: 54.69%] [D2 loss: 0.634773, acc.: 60.16%] [G loss: 1.604617]\n",
      "epoch:0 step:308 [D1 loss: 0.646912, acc.: 53.12%] [D2 loss: 0.657038, acc.: 50.78%] [G loss: 1.550559]\n",
      "epoch:0 step:309 [D1 loss: 0.639842, acc.: 50.78%] [D2 loss: 0.666081, acc.: 47.66%] [G loss: 1.495961]\n",
      "epoch:0 step:310 [D1 loss: 0.636463, acc.: 52.34%] [D2 loss: 0.659328, acc.: 49.22%] [G loss: 1.510867]\n",
      "epoch:0 step:311 [D1 loss: 0.637646, acc.: 51.56%] [D2 loss: 0.647178, acc.: 50.78%] [G loss: 1.522732]\n",
      "epoch:0 step:312 [D1 loss: 0.698424, acc.: 41.41%] [D2 loss: 0.646043, acc.: 53.12%] [G loss: 1.514046]\n",
      "epoch:0 step:313 [D1 loss: 0.656397, acc.: 51.56%] [D2 loss: 0.639722, acc.: 52.34%] [G loss: 1.497249]\n",
      "epoch:0 step:314 [D1 loss: 0.634995, acc.: 53.12%] [D2 loss: 0.638436, acc.: 49.22%] [G loss: 1.517209]\n",
      "epoch:0 step:315 [D1 loss: 0.606612, acc.: 67.19%] [D2 loss: 0.664405, acc.: 56.25%] [G loss: 1.571261]\n",
      "epoch:0 step:316 [D1 loss: 0.698146, acc.: 50.00%] [D2 loss: 0.652566, acc.: 63.28%] [G loss: 1.551415]\n",
      "epoch:0 step:317 [D1 loss: 0.664191, acc.: 57.81%] [D2 loss: 0.648022, acc.: 59.38%] [G loss: 1.583893]\n",
      "epoch:0 step:318 [D1 loss: 0.671112, acc.: 53.12%] [D2 loss: 0.679935, acc.: 52.34%] [G loss: 1.526951]\n",
      "epoch:0 step:319 [D1 loss: 0.658654, acc.: 57.03%] [D2 loss: 0.637447, acc.: 48.44%] [G loss: 1.523496]\n",
      "epoch:0 step:320 [D1 loss: 0.643941, acc.: 52.34%] [D2 loss: 0.653697, acc.: 45.31%] [G loss: 1.454551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:321 [D1 loss: 0.682210, acc.: 51.56%] [D2 loss: 0.652366, acc.: 50.78%] [G loss: 1.508982]\n",
      "epoch:0 step:322 [D1 loss: 0.652270, acc.: 56.25%] [D2 loss: 0.629306, acc.: 55.47%] [G loss: 1.603310]\n",
      "epoch:0 step:323 [D1 loss: 0.661815, acc.: 50.78%] [D2 loss: 0.660115, acc.: 50.00%] [G loss: 1.561544]\n",
      "epoch:0 step:324 [D1 loss: 0.645338, acc.: 49.22%] [D2 loss: 0.637118, acc.: 50.00%] [G loss: 1.521414]\n",
      "epoch:0 step:325 [D1 loss: 0.629386, acc.: 60.94%] [D2 loss: 0.634532, acc.: 64.06%] [G loss: 1.618562]\n",
      "epoch:0 step:326 [D1 loss: 0.671205, acc.: 57.03%] [D2 loss: 0.649983, acc.: 56.25%] [G loss: 1.630757]\n",
      "epoch:0 step:327 [D1 loss: 0.649121, acc.: 59.38%] [D2 loss: 0.593512, acc.: 66.41%] [G loss: 1.771948]\n",
      "epoch:0 step:328 [D1 loss: 0.666776, acc.: 49.22%] [D2 loss: 0.661893, acc.: 47.66%] [G loss: 1.649515]\n",
      "epoch:0 step:329 [D1 loss: 0.665746, acc.: 54.69%] [D2 loss: 0.641587, acc.: 52.34%] [G loss: 1.522134]\n",
      "epoch:0 step:330 [D1 loss: 0.640105, acc.: 60.94%] [D2 loss: 0.626351, acc.: 56.25%] [G loss: 1.485774]\n",
      "epoch:0 step:331 [D1 loss: 0.632186, acc.: 64.06%] [D2 loss: 0.672908, acc.: 54.69%] [G loss: 1.471549]\n",
      "epoch:0 step:332 [D1 loss: 0.658579, acc.: 60.16%] [D2 loss: 0.635791, acc.: 63.28%] [G loss: 1.556875]\n",
      "epoch:0 step:333 [D1 loss: 0.670028, acc.: 57.03%] [D2 loss: 0.633616, acc.: 61.72%] [G loss: 1.544735]\n",
      "epoch:0 step:334 [D1 loss: 0.671758, acc.: 52.34%] [D2 loss: 0.627320, acc.: 66.41%] [G loss: 1.561694]\n",
      "epoch:0 step:335 [D1 loss: 0.660136, acc.: 55.47%] [D2 loss: 0.671486, acc.: 50.78%] [G loss: 1.532949]\n",
      "epoch:0 step:336 [D1 loss: 0.640702, acc.: 57.03%] [D2 loss: 0.660478, acc.: 52.34%] [G loss: 1.579479]\n",
      "epoch:0 step:337 [D1 loss: 0.639096, acc.: 64.06%] [D2 loss: 0.635491, acc.: 57.81%] [G loss: 1.567169]\n",
      "epoch:0 step:338 [D1 loss: 0.630999, acc.: 66.41%] [D2 loss: 0.619420, acc.: 58.59%] [G loss: 1.538134]\n",
      "epoch:0 step:339 [D1 loss: 0.649077, acc.: 59.38%] [D2 loss: 0.636437, acc.: 58.59%] [G loss: 1.510437]\n",
      "epoch:0 step:340 [D1 loss: 0.619068, acc.: 65.62%] [D2 loss: 0.656055, acc.: 55.47%] [G loss: 1.550397]\n",
      "epoch:0 step:341 [D1 loss: 0.649612, acc.: 57.03%] [D2 loss: 0.629700, acc.: 61.72%] [G loss: 1.611412]\n",
      "epoch:0 step:342 [D1 loss: 0.628044, acc.: 64.06%] [D2 loss: 0.631817, acc.: 63.28%] [G loss: 1.583286]\n",
      "epoch:0 step:343 [D1 loss: 0.603235, acc.: 64.84%] [D2 loss: 0.619985, acc.: 64.06%] [G loss: 1.589248]\n",
      "epoch:0 step:344 [D1 loss: 0.602177, acc.: 59.38%] [D2 loss: 0.624725, acc.: 63.28%] [G loss: 1.607397]\n",
      "epoch:0 step:345 [D1 loss: 0.644190, acc.: 58.59%] [D2 loss: 0.621881, acc.: 61.72%] [G loss: 1.592948]\n",
      "epoch:0 step:346 [D1 loss: 0.649820, acc.: 60.16%] [D2 loss: 0.631308, acc.: 66.41%] [G loss: 1.587051]\n",
      "epoch:0 step:347 [D1 loss: 0.624956, acc.: 66.41%] [D2 loss: 0.602486, acc.: 67.19%] [G loss: 1.566349]\n",
      "epoch:0 step:348 [D1 loss: 0.675079, acc.: 55.47%] [D2 loss: 0.594466, acc.: 65.62%] [G loss: 1.692320]\n",
      "epoch:0 step:349 [D1 loss: 0.698732, acc.: 50.78%] [D2 loss: 0.615985, acc.: 66.41%] [G loss: 1.663473]\n",
      "epoch:0 step:350 [D1 loss: 0.658009, acc.: 57.81%] [D2 loss: 0.580081, acc.: 71.09%] [G loss: 1.634362]\n",
      "epoch:0 step:351 [D1 loss: 0.680261, acc.: 51.56%] [D2 loss: 0.625669, acc.: 62.50%] [G loss: 1.583122]\n",
      "epoch:0 step:352 [D1 loss: 0.682250, acc.: 54.69%] [D2 loss: 0.613416, acc.: 60.94%] [G loss: 1.564227]\n",
      "epoch:0 step:353 [D1 loss: 0.693480, acc.: 49.22%] [D2 loss: 0.603894, acc.: 59.38%] [G loss: 1.630795]\n",
      "epoch:0 step:354 [D1 loss: 0.668554, acc.: 53.12%] [D2 loss: 0.615122, acc.: 65.62%] [G loss: 1.621803]\n",
      "epoch:0 step:355 [D1 loss: 0.670772, acc.: 48.44%] [D2 loss: 0.634809, acc.: 51.56%] [G loss: 1.519472]\n",
      "epoch:0 step:356 [D1 loss: 0.666544, acc.: 56.25%] [D2 loss: 0.610740, acc.: 64.84%] [G loss: 1.529255]\n",
      "epoch:0 step:357 [D1 loss: 0.642751, acc.: 57.03%] [D2 loss: 0.609949, acc.: 71.88%] [G loss: 1.620437]\n",
      "epoch:0 step:358 [D1 loss: 0.640535, acc.: 60.16%] [D2 loss: 0.613218, acc.: 73.44%] [G loss: 1.720644]\n",
      "epoch:0 step:359 [D1 loss: 0.677950, acc.: 44.53%] [D2 loss: 0.623618, acc.: 57.03%] [G loss: 1.608166]\n",
      "epoch:0 step:360 [D1 loss: 0.663396, acc.: 49.22%] [D2 loss: 0.588236, acc.: 62.50%] [G loss: 1.629861]\n",
      "epoch:0 step:361 [D1 loss: 0.658858, acc.: 57.03%] [D2 loss: 0.573387, acc.: 83.59%] [G loss: 1.654569]\n",
      "epoch:0 step:362 [D1 loss: 0.663872, acc.: 49.22%] [D2 loss: 0.590682, acc.: 69.53%] [G loss: 1.582799]\n",
      "epoch:0 step:363 [D1 loss: 0.643251, acc.: 55.47%] [D2 loss: 0.594266, acc.: 72.66%] [G loss: 1.614151]\n",
      "epoch:0 step:364 [D1 loss: 0.640978, acc.: 54.69%] [D2 loss: 0.583432, acc.: 73.44%] [G loss: 1.685473]\n",
      "epoch:0 step:365 [D1 loss: 0.655621, acc.: 53.12%] [D2 loss: 0.621255, acc.: 66.41%] [G loss: 1.584359]\n",
      "epoch:0 step:366 [D1 loss: 0.636147, acc.: 51.56%] [D2 loss: 0.617807, acc.: 62.50%] [G loss: 1.571297]\n",
      "epoch:0 step:367 [D1 loss: 0.641542, acc.: 55.47%] [D2 loss: 0.626025, acc.: 64.06%] [G loss: 1.570667]\n",
      "epoch:0 step:368 [D1 loss: 0.616784, acc.: 57.81%] [D2 loss: 0.630471, acc.: 62.50%] [G loss: 1.568980]\n",
      "epoch:0 step:369 [D1 loss: 0.652446, acc.: 50.78%] [D2 loss: 0.638544, acc.: 56.25%] [G loss: 1.580928]\n",
      "epoch:0 step:370 [D1 loss: 0.652903, acc.: 53.12%] [D2 loss: 0.640067, acc.: 56.25%] [G loss: 1.675820]\n",
      "epoch:0 step:371 [D1 loss: 0.666426, acc.: 50.00%] [D2 loss: 0.603105, acc.: 62.50%] [G loss: 1.699145]\n",
      "epoch:0 step:372 [D1 loss: 0.647443, acc.: 52.34%] [D2 loss: 0.605771, acc.: 60.16%] [G loss: 1.693591]\n",
      "epoch:0 step:373 [D1 loss: 0.646862, acc.: 52.34%] [D2 loss: 0.626224, acc.: 63.28%] [G loss: 1.591028]\n",
      "epoch:0 step:374 [D1 loss: 0.656537, acc.: 50.00%] [D2 loss: 0.663074, acc.: 57.03%] [G loss: 1.532616]\n",
      "epoch:0 step:375 [D1 loss: 0.634373, acc.: 51.56%] [D2 loss: 0.644304, acc.: 63.28%] [G loss: 1.549034]\n",
      "epoch:0 step:376 [D1 loss: 0.666059, acc.: 51.56%] [D2 loss: 0.635857, acc.: 64.06%] [G loss: 1.565349]\n",
      "epoch:0 step:377 [D1 loss: 0.650835, acc.: 49.22%] [D2 loss: 0.634427, acc.: 61.72%] [G loss: 1.649678]\n",
      "epoch:0 step:378 [D1 loss: 0.658396, acc.: 48.44%] [D2 loss: 0.635591, acc.: 60.16%] [G loss: 1.656400]\n",
      "epoch:0 step:379 [D1 loss: 0.682796, acc.: 48.44%] [D2 loss: 0.628549, acc.: 59.38%] [G loss: 1.588526]\n",
      "epoch:0 step:380 [D1 loss: 0.672765, acc.: 46.88%] [D2 loss: 0.615669, acc.: 60.94%] [G loss: 1.570404]\n",
      "epoch:0 step:381 [D1 loss: 0.645299, acc.: 55.47%] [D2 loss: 0.624009, acc.: 59.38%] [G loss: 1.641225]\n",
      "epoch:0 step:382 [D1 loss: 0.676720, acc.: 46.09%] [D2 loss: 0.601855, acc.: 70.31%] [G loss: 1.670540]\n",
      "epoch:0 step:383 [D1 loss: 0.686062, acc.: 41.41%] [D2 loss: 0.633077, acc.: 63.28%] [G loss: 1.657035]\n",
      "epoch:0 step:384 [D1 loss: 0.664139, acc.: 56.25%] [D2 loss: 0.609555, acc.: 62.50%] [G loss: 1.690681]\n",
      "epoch:0 step:385 [D1 loss: 0.674152, acc.: 52.34%] [D2 loss: 0.604464, acc.: 64.06%] [G loss: 1.690374]\n",
      "epoch:0 step:386 [D1 loss: 0.636477, acc.: 58.59%] [D2 loss: 0.620418, acc.: 67.97%] [G loss: 1.654494]\n",
      "epoch:0 step:387 [D1 loss: 0.628637, acc.: 59.38%] [D2 loss: 0.628425, acc.: 57.81%] [G loss: 1.628139]\n",
      "epoch:0 step:388 [D1 loss: 0.613052, acc.: 59.38%] [D2 loss: 0.632772, acc.: 62.50%] [G loss: 1.649138]\n",
      "epoch:0 step:389 [D1 loss: 0.640969, acc.: 54.69%] [D2 loss: 0.659178, acc.: 48.44%] [G loss: 1.609306]\n",
      "epoch:0 step:390 [D1 loss: 0.635266, acc.: 57.03%] [D2 loss: 0.645928, acc.: 61.72%] [G loss: 1.608322]\n",
      "epoch:0 step:391 [D1 loss: 0.661397, acc.: 49.22%] [D2 loss: 0.634175, acc.: 59.38%] [G loss: 1.683123]\n",
      "epoch:0 step:392 [D1 loss: 0.628115, acc.: 57.81%] [D2 loss: 0.657869, acc.: 53.12%] [G loss: 1.651537]\n",
      "epoch:0 step:393 [D1 loss: 0.657737, acc.: 48.44%] [D2 loss: 0.632126, acc.: 62.50%] [G loss: 1.603903]\n",
      "epoch:0 step:394 [D1 loss: 0.675138, acc.: 50.78%] [D2 loss: 0.625678, acc.: 67.19%] [G loss: 1.595901]\n",
      "epoch:0 step:395 [D1 loss: 0.636996, acc.: 51.56%] [D2 loss: 0.576706, acc.: 74.22%] [G loss: 1.659950]\n",
      "epoch:0 step:396 [D1 loss: 0.702994, acc.: 42.19%] [D2 loss: 0.612443, acc.: 67.19%] [G loss: 1.687111]\n",
      "epoch:0 step:397 [D1 loss: 0.629909, acc.: 63.28%] [D2 loss: 0.589901, acc.: 67.97%] [G loss: 1.730934]\n",
      "epoch:0 step:398 [D1 loss: 0.699460, acc.: 50.00%] [D2 loss: 0.674536, acc.: 50.78%] [G loss: 1.656999]\n",
      "epoch:0 step:399 [D1 loss: 0.651056, acc.: 50.78%] [D2 loss: 0.628268, acc.: 60.94%] [G loss: 1.689807]\n",
      "epoch:0 step:400 [D1 loss: 0.715592, acc.: 44.53%] [D2 loss: 0.638458, acc.: 59.38%] [G loss: 1.638187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:401 [D1 loss: 0.679543, acc.: 50.00%] [D2 loss: 0.636688, acc.: 64.84%] [G loss: 1.644575]\n",
      "epoch:0 step:402 [D1 loss: 0.652232, acc.: 59.38%] [D2 loss: 0.579638, acc.: 72.66%] [G loss: 1.681696]\n",
      "epoch:0 step:403 [D1 loss: 0.677822, acc.: 48.44%] [D2 loss: 0.602913, acc.: 64.84%] [G loss: 1.742830]\n",
      "epoch:0 step:404 [D1 loss: 0.686832, acc.: 47.66%] [D2 loss: 0.631338, acc.: 64.06%] [G loss: 1.835646]\n",
      "epoch:0 step:405 [D1 loss: 0.675251, acc.: 53.12%] [D2 loss: 0.648254, acc.: 59.38%] [G loss: 1.639475]\n",
      "epoch:0 step:406 [D1 loss: 0.660779, acc.: 57.81%] [D2 loss: 0.636940, acc.: 63.28%] [G loss: 1.597425]\n",
      "epoch:0 step:407 [D1 loss: 0.680084, acc.: 51.56%] [D2 loss: 0.655920, acc.: 59.38%] [G loss: 1.586995]\n",
      "epoch:0 step:408 [D1 loss: 0.681361, acc.: 50.00%] [D2 loss: 0.653595, acc.: 55.47%] [G loss: 1.636846]\n",
      "epoch:0 step:409 [D1 loss: 0.665240, acc.: 58.59%] [D2 loss: 0.629121, acc.: 67.97%] [G loss: 1.676657]\n",
      "epoch:0 step:410 [D1 loss: 0.662249, acc.: 52.34%] [D2 loss: 0.641839, acc.: 70.31%] [G loss: 1.677827]\n",
      "epoch:0 step:411 [D1 loss: 0.692166, acc.: 47.66%] [D2 loss: 0.651017, acc.: 59.38%] [G loss: 1.617950]\n",
      "epoch:0 step:412 [D1 loss: 0.658885, acc.: 50.00%] [D2 loss: 0.628940, acc.: 57.81%] [G loss: 1.680661]\n",
      "epoch:0 step:413 [D1 loss: 0.646079, acc.: 57.81%] [D2 loss: 0.617320, acc.: 64.84%] [G loss: 1.770854]\n",
      "epoch:0 step:414 [D1 loss: 0.671914, acc.: 55.47%] [D2 loss: 0.619782, acc.: 67.97%] [G loss: 1.836475]\n",
      "epoch:0 step:415 [D1 loss: 0.646843, acc.: 54.69%] [D2 loss: 0.647736, acc.: 64.84%] [G loss: 1.719532]\n",
      "epoch:0 step:416 [D1 loss: 0.631640, acc.: 60.16%] [D2 loss: 0.616898, acc.: 65.62%] [G loss: 1.755504]\n",
      "epoch:0 step:417 [D1 loss: 0.699027, acc.: 45.31%] [D2 loss: 0.638404, acc.: 56.25%] [G loss: 1.706084]\n",
      "epoch:0 step:418 [D1 loss: 0.686981, acc.: 40.62%] [D2 loss: 0.627211, acc.: 68.75%] [G loss: 1.699793]\n",
      "epoch:0 step:419 [D1 loss: 0.673002, acc.: 47.66%] [D2 loss: 0.599791, acc.: 67.19%] [G loss: 1.691394]\n",
      "epoch:0 step:420 [D1 loss: 0.670818, acc.: 50.78%] [D2 loss: 0.619229, acc.: 64.84%] [G loss: 1.690825]\n",
      "epoch:0 step:421 [D1 loss: 0.678657, acc.: 50.00%] [D2 loss: 0.624778, acc.: 66.41%] [G loss: 1.684695]\n",
      "epoch:0 step:422 [D1 loss: 0.657670, acc.: 49.22%] [D2 loss: 0.643890, acc.: 56.25%] [G loss: 1.681339]\n",
      "epoch:0 step:423 [D1 loss: 0.657325, acc.: 49.22%] [D2 loss: 0.649377, acc.: 59.38%] [G loss: 1.694537]\n",
      "epoch:0 step:424 [D1 loss: 0.683048, acc.: 46.88%] [D2 loss: 0.641675, acc.: 56.25%] [G loss: 1.661102]\n",
      "epoch:0 step:425 [D1 loss: 0.677360, acc.: 46.09%] [D2 loss: 0.650765, acc.: 57.81%] [G loss: 1.672587]\n",
      "epoch:0 step:426 [D1 loss: 0.657229, acc.: 52.34%] [D2 loss: 0.641767, acc.: 57.03%] [G loss: 1.702842]\n",
      "epoch:0 step:427 [D1 loss: 0.649877, acc.: 56.25%] [D2 loss: 0.636114, acc.: 58.59%] [G loss: 1.732303]\n",
      "epoch:0 step:428 [D1 loss: 0.641424, acc.: 53.91%] [D2 loss: 0.604938, acc.: 67.97%] [G loss: 1.701539]\n",
      "epoch:0 step:429 [D1 loss: 0.634664, acc.: 57.03%] [D2 loss: 0.631788, acc.: 65.62%] [G loss: 1.818978]\n",
      "epoch:0 step:430 [D1 loss: 0.645522, acc.: 58.59%] [D2 loss: 0.609650, acc.: 74.22%] [G loss: 1.749794]\n",
      "epoch:0 step:431 [D1 loss: 0.690056, acc.: 40.62%] [D2 loss: 0.604965, acc.: 71.09%] [G loss: 1.625746]\n",
      "epoch:0 step:432 [D1 loss: 0.677059, acc.: 40.62%] [D2 loss: 0.612335, acc.: 71.09%] [G loss: 1.658282]\n",
      "epoch:0 step:433 [D1 loss: 0.649193, acc.: 50.00%] [D2 loss: 0.618763, acc.: 71.09%] [G loss: 1.667009]\n",
      "epoch:0 step:434 [D1 loss: 0.648909, acc.: 48.44%] [D2 loss: 0.613140, acc.: 75.00%] [G loss: 1.686452]\n",
      "epoch:0 step:435 [D1 loss: 0.645699, acc.: 55.47%] [D2 loss: 0.615023, acc.: 67.19%] [G loss: 1.761361]\n",
      "epoch:0 step:436 [D1 loss: 0.662155, acc.: 55.47%] [D2 loss: 0.584540, acc.: 78.91%] [G loss: 1.820296]\n",
      "epoch:0 step:437 [D1 loss: 0.725187, acc.: 41.41%] [D2 loss: 0.613010, acc.: 68.75%] [G loss: 1.610937]\n",
      "epoch:0 step:438 [D1 loss: 0.671169, acc.: 51.56%] [D2 loss: 0.618970, acc.: 63.28%] [G loss: 1.595903]\n",
      "epoch:0 step:439 [D1 loss: 0.659478, acc.: 55.47%] [D2 loss: 0.600718, acc.: 67.19%] [G loss: 1.607248]\n",
      "epoch:0 step:440 [D1 loss: 0.661391, acc.: 53.91%] [D2 loss: 0.629857, acc.: 60.16%] [G loss: 1.626691]\n",
      "epoch:0 step:441 [D1 loss: 0.658559, acc.: 52.34%] [D2 loss: 0.609801, acc.: 67.19%] [G loss: 1.604424]\n",
      "epoch:0 step:442 [D1 loss: 0.661608, acc.: 54.69%] [D2 loss: 0.648844, acc.: 58.59%] [G loss: 1.626062]\n",
      "epoch:0 step:443 [D1 loss: 0.659541, acc.: 57.03%] [D2 loss: 0.653638, acc.: 67.97%] [G loss: 1.624224]\n",
      "epoch:0 step:444 [D1 loss: 0.643766, acc.: 57.81%] [D2 loss: 0.633530, acc.: 67.19%] [G loss: 1.699830]\n",
      "epoch:0 step:445 [D1 loss: 0.643613, acc.: 61.72%] [D2 loss: 0.603191, acc.: 71.09%] [G loss: 1.809548]\n",
      "epoch:0 step:446 [D1 loss: 0.681366, acc.: 52.34%] [D2 loss: 0.640405, acc.: 64.84%] [G loss: 1.711601]\n",
      "epoch:0 step:447 [D1 loss: 0.645970, acc.: 59.38%] [D2 loss: 0.650558, acc.: 61.72%] [G loss: 1.690462]\n",
      "epoch:0 step:448 [D1 loss: 0.682118, acc.: 42.97%] [D2 loss: 0.630187, acc.: 60.94%] [G loss: 1.675715]\n",
      "epoch:0 step:449 [D1 loss: 0.672804, acc.: 55.47%] [D2 loss: 0.589111, acc.: 72.66%] [G loss: 1.642919]\n",
      "epoch:0 step:450 [D1 loss: 0.652239, acc.: 58.59%] [D2 loss: 0.605024, acc.: 67.97%] [G loss: 1.737538]\n",
      "epoch:0 step:451 [D1 loss: 0.651224, acc.: 60.16%] [D2 loss: 0.688009, acc.: 45.31%] [G loss: 1.812594]\n",
      "epoch:0 step:452 [D1 loss: 0.665270, acc.: 62.50%] [D2 loss: 0.606784, acc.: 70.31%] [G loss: 1.805297]\n",
      "epoch:0 step:453 [D1 loss: 0.697613, acc.: 61.72%] [D2 loss: 0.654838, acc.: 57.03%] [G loss: 1.533423]\n",
      "epoch:0 step:454 [D1 loss: 0.649673, acc.: 61.72%] [D2 loss: 0.608338, acc.: 65.62%] [G loss: 1.580185]\n",
      "epoch:0 step:455 [D1 loss: 0.636090, acc.: 62.50%] [D2 loss: 0.588651, acc.: 75.78%] [G loss: 1.640300]\n",
      "epoch:0 step:456 [D1 loss: 0.640557, acc.: 62.50%] [D2 loss: 0.581424, acc.: 74.22%] [G loss: 1.715962]\n",
      "epoch:0 step:457 [D1 loss: 0.632298, acc.: 65.62%] [D2 loss: 0.560598, acc.: 76.56%] [G loss: 1.768328]\n",
      "epoch:0 step:458 [D1 loss: 0.628856, acc.: 63.28%] [D2 loss: 0.563747, acc.: 75.00%] [G loss: 1.821836]\n",
      "epoch:0 step:459 [D1 loss: 0.613398, acc.: 68.75%] [D2 loss: 0.633716, acc.: 64.84%] [G loss: 1.772358]\n",
      "epoch:0 step:460 [D1 loss: 0.631280, acc.: 56.25%] [D2 loss: 0.697618, acc.: 50.00%] [G loss: 1.725298]\n",
      "epoch:0 step:461 [D1 loss: 0.628877, acc.: 60.16%] [D2 loss: 0.549573, acc.: 79.69%] [G loss: 1.818317]\n",
      "epoch:0 step:462 [D1 loss: 0.666458, acc.: 55.47%] [D2 loss: 0.645237, acc.: 68.75%] [G loss: 1.654516]\n",
      "epoch:0 step:463 [D1 loss: 0.649384, acc.: 55.47%] [D2 loss: 0.605617, acc.: 63.28%] [G loss: 1.602983]\n",
      "epoch:0 step:464 [D1 loss: 0.624428, acc.: 52.34%] [D2 loss: 0.600570, acc.: 65.62%] [G loss: 1.758115]\n",
      "epoch:0 step:465 [D1 loss: 0.624479, acc.: 66.41%] [D2 loss: 0.530538, acc.: 79.69%] [G loss: 1.863362]\n",
      "epoch:0 step:466 [D1 loss: 0.622236, acc.: 71.09%] [D2 loss: 0.542018, acc.: 83.59%] [G loss: 1.885118]\n",
      "epoch:0 step:467 [D1 loss: 0.603662, acc.: 70.31%] [D2 loss: 0.496291, acc.: 82.81%] [G loss: 1.947782]\n",
      "epoch:0 step:468 [D1 loss: 0.639644, acc.: 64.84%] [D2 loss: 0.597493, acc.: 71.88%] [G loss: 1.815736]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 48 input samples and 64 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1b5ecf2e1434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-1b5ecf2e1434>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;31m# Train the discriminators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0md1_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0md2_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0md1_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    235\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 48 input samples and 64 target samples."
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class COGAN():\n",
    "    \"\"\"Reference: https://wiseodd.github.io/techblog/2017/02/18/coupled_gan/\"\"\"\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.d1, self.d2 = self.build_discriminators()\n",
    "        self.d1.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d2.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.g1, self.g2 = self.build_generators()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img1 = self.g1(z)\n",
    "        img2 = self.g2(z)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d1.trainable = False\n",
    "        self.d2.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid1 = self.d1(img1)\n",
    "        valid2 = self.d2(img2)\n",
    "\n",
    "        # The combined model  (stacked generators and discriminators)\n",
    "        # Trains generators to fool discriminators\n",
    "        self.combined = Model(z, [valid1, valid2])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "                                    optimizer=optimizer)\n",
    "\n",
    "    def build_generators(self):\n",
    "\n",
    "        # Shared weights between generators\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        feature_repr = model(noise)\n",
    "\n",
    "        # Generator 1\n",
    "        g1 = Dense(1024)(feature_repr)\n",
    "        g1 = LeakyReLU(alpha=0.2)(g1)\n",
    "        g1 = BatchNormalization(momentum=0.8)(g1)\n",
    "        g1 = Dense(np.prod(self.img_shape), activation='tanh')(g1)\n",
    "        img1 = Reshape(self.img_shape)(g1)\n",
    "\n",
    "        # Generator 2\n",
    "        g2 = Dense(1024)(feature_repr)\n",
    "        g2 = LeakyReLU(alpha=0.2)(g2)\n",
    "        g2 = BatchNormalization(momentum=0.8)(g2)\n",
    "        g2 = Dense(np.prod(self.img_shape), activation='tanh')(g2)\n",
    "        img2 = Reshape(self.img_shape)(g2)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return Model(noise, img1), Model(noise, img2)\n",
    "\n",
    "    def build_discriminators(self):\n",
    "\n",
    "        img1 = Input(shape=self.img_shape)\n",
    "        img2 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Shared discriminator layers\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        img1_embedding = model(img1)\n",
    "        img2_embedding = model(img2)\n",
    "\n",
    "        # Discriminator 1\n",
    "        validity1 = Dense(1, activation='sigmoid')(img1_embedding)\n",
    "        # Discriminator 2\n",
    "        validity2 = Dense(1, activation='sigmoid')(img2_embedding)\n",
    "\n",
    "        return Model(img1, validity1), Model(img2, validity2)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Images in domain A and B (rotated)\n",
    "        X1 = X_train[:int(X_train.shape[0]/2)]\n",
    "        X2 = X_train[int(X_train.shape[0]/2):]\n",
    "        X2 = scipy.ndimage.interpolation.rotate(X2, 90, axes=(1, 2))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                # imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "                imgs1 = X1[index * batch_size:(index + 1) * batch_size]\n",
    "                imgs2 = X2[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs1 = self.g1.predict(noise)\n",
    "                gen_imgs2 = self.g2.predict(noise)\n",
    "\n",
    "                # Train the discriminators\n",
    "                d1_loss_real = self.d1.train_on_batch(imgs1, valid)\n",
    "                d2_loss_real = self.d2.train_on_batch(imgs2, valid)\n",
    "                d1_loss_fake = self.d1.train_on_batch(gen_imgs1, fake)\n",
    "                d2_loss_fake = self.d2.train_on_batch(gen_imgs2, fake)\n",
    "                d1_loss = 0.5 * np.add(d1_loss_real, d1_loss_fake)\n",
    "                d2_loss = 0.5 * np.add(d2_loss_real, d2_loss_fake)\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                g_loss = self.combined.train_on_batch(noise, [valid, valid])\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D1 loss: %f, acc.: %.2f%%] [D2 loss: %f, acc.: %.2f%%] [G loss: %f]\" \\\n",
    "                      % (epoch,global_step, d1_loss[0], 100 * d1_loss[1], d2_loss[0], 100 * d2_loss[1], g_loss[0]))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if  global_step % sample_interval == 0:\n",
    "                    self.sample_images(epoch, global_step)\n",
    "\n",
    "    def sample_images(self, epoch,global_step):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * int(c/2), 100))\n",
    "        gen_imgs1 = self.g1.predict(noise)\n",
    "        gen_imgs2 = self.g2.predict(noise)\n",
    "\n",
    "        gen_imgs = np.concatenate([gen_imgs1, gen_imgs2])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        if not os.path.isdir('images_cogan'):\n",
    "            os.mkdir('images_cogan')\n",
    "        fig.savefig(\"images_cogan/epoch_%d_step_%d.png\" % (epoch,global_step))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = COGAN()\n",
    "    gan.train(epochs=30, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
