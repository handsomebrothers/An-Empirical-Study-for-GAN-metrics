{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               402192    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 720,656\n",
      "Trainable params: 718,608\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               51300     \n",
      "=================================================================\n",
      "Total params: 719,972\n",
      "Trainable params: 717,924\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:1 [D loss: 0.959536, acc: 29.69%] [G loss: 4.626052]\n",
      "epoch:0 step:2 [D loss: 0.595189, acc: 62.50%] [G loss: 5.081646]\n",
      "epoch:0 step:3 [D loss: 0.227627, acc: 93.75%] [G loss: 5.421597]\n",
      "epoch:0 step:4 [D loss: 0.162079, acc: 98.44%] [G loss: 6.341406]\n",
      "epoch:0 step:5 [D loss: 0.088884, acc: 99.22%] [G loss: 7.642174]\n",
      "epoch:0 step:6 [D loss: 0.060217, acc: 100.00%] [G loss: 9.106062]\n",
      "epoch:0 step:7 [D loss: 0.045664, acc: 99.22%] [G loss: 9.720157]\n",
      "epoch:0 step:8 [D loss: 0.031582, acc: 100.00%] [G loss: 10.113420]\n",
      "epoch:0 step:9 [D loss: 0.028420, acc: 100.00%] [G loss: 10.601333]\n",
      "epoch:0 step:10 [D loss: 0.017493, acc: 100.00%] [G loss: 11.563252]\n",
      "epoch:0 step:11 [D loss: 0.016290, acc: 100.00%] [G loss: 11.591646]\n",
      "epoch:0 step:12 [D loss: 0.014904, acc: 100.00%] [G loss: 11.569134]\n",
      "epoch:0 step:13 [D loss: 0.012463, acc: 100.00%] [G loss: 11.921530]\n",
      "epoch:0 step:14 [D loss: 0.012938, acc: 100.00%] [G loss: 12.236393]\n",
      "epoch:0 step:15 [D loss: 0.013923, acc: 100.00%] [G loss: 12.568917]\n",
      "epoch:0 step:16 [D loss: 0.016411, acc: 100.00%] [G loss: 12.909225]\n",
      "epoch:0 step:17 [D loss: 0.008709, acc: 100.00%] [G loss: 13.359159]\n",
      "epoch:0 step:18 [D loss: 0.005801, acc: 100.00%] [G loss: 13.928886]\n",
      "epoch:0 step:19 [D loss: 0.011435, acc: 100.00%] [G loss: 13.765875]\n",
      "epoch:0 step:20 [D loss: 0.009458, acc: 100.00%] [G loss: 14.196501]\n",
      "epoch:0 step:21 [D loss: 0.007578, acc: 100.00%] [G loss: 14.233506]\n",
      "epoch:0 step:22 [D loss: 0.020372, acc: 99.22%] [G loss: 15.386753]\n",
      "epoch:0 step:23 [D loss: 0.020315, acc: 99.22%] [G loss: 16.663414]\n",
      "epoch:0 step:24 [D loss: 0.010201, acc: 100.00%] [G loss: 17.202572]\n",
      "epoch:0 step:25 [D loss: 0.006723, acc: 100.00%] [G loss: 16.887917]\n",
      "epoch:0 step:26 [D loss: 0.009158, acc: 100.00%] [G loss: 17.004959]\n",
      "epoch:0 step:27 [D loss: 0.006922, acc: 100.00%] [G loss: 16.449747]\n",
      "epoch:0 step:28 [D loss: 0.004548, acc: 100.00%] [G loss: 16.790668]\n",
      "epoch:0 step:29 [D loss: 0.016601, acc: 99.22%] [G loss: 17.947363]\n",
      "epoch:0 step:30 [D loss: 0.009042, acc: 100.00%] [G loss: 17.642506]\n",
      "epoch:0 step:31 [D loss: 0.031693, acc: 99.22%] [G loss: 19.104366]\n",
      "epoch:0 step:32 [D loss: 0.009709, acc: 100.00%] [G loss: 19.611748]\n",
      "epoch:0 step:33 [D loss: 0.008552, acc: 100.00%] [G loss: 19.209415]\n",
      "epoch:0 step:34 [D loss: 0.003384, acc: 100.00%] [G loss: 19.119143]\n",
      "epoch:0 step:35 [D loss: 0.006382, acc: 100.00%] [G loss: 18.999266]\n",
      "epoch:0 step:36 [D loss: 0.028566, acc: 99.22%] [G loss: 20.134199]\n",
      "epoch:0 step:37 [D loss: 0.004989, acc: 100.00%] [G loss: 20.324207]\n",
      "epoch:0 step:38 [D loss: 0.010839, acc: 100.00%] [G loss: 20.929396]\n",
      "epoch:0 step:39 [D loss: 0.014051, acc: 100.00%] [G loss: 20.706810]\n",
      "epoch:0 step:40 [D loss: 0.003859, acc: 100.00%] [G loss: 20.146585]\n",
      "epoch:0 step:41 [D loss: 0.006615, acc: 100.00%] [G loss: 20.279778]\n",
      "epoch:0 step:42 [D loss: 0.011245, acc: 99.22%] [G loss: 20.816374]\n",
      "epoch:0 step:43 [D loss: 0.006153, acc: 100.00%] [G loss: 21.327816]\n",
      "epoch:0 step:44 [D loss: 0.025704, acc: 98.44%] [G loss: 20.776209]\n",
      "epoch:0 step:45 [D loss: 0.042205, acc: 98.44%] [G loss: 21.053703]\n",
      "epoch:0 step:46 [D loss: 0.010985, acc: 100.00%] [G loss: 21.638086]\n",
      "epoch:0 step:47 [D loss: 0.017954, acc: 100.00%] [G loss: 21.436396]\n",
      "epoch:0 step:48 [D loss: 0.008563, acc: 100.00%] [G loss: 22.230068]\n",
      "epoch:0 step:49 [D loss: 0.015052, acc: 100.00%] [G loss: 21.540535]\n",
      "epoch:0 step:50 [D loss: 0.027709, acc: 98.44%] [G loss: 22.089930]\n",
      "epoch:0 step:51 [D loss: 0.012691, acc: 100.00%] [G loss: 23.331711]\n",
      "epoch:0 step:52 [D loss: 0.013282, acc: 100.00%] [G loss: 23.123867]\n",
      "epoch:0 step:53 [D loss: 0.004424, acc: 100.00%] [G loss: 23.007616]\n",
      "epoch:0 step:54 [D loss: 0.005844, acc: 100.00%] [G loss: 23.178474]\n",
      "epoch:0 step:55 [D loss: 0.016211, acc: 100.00%] [G loss: 22.965010]\n",
      "epoch:0 step:56 [D loss: 0.058256, acc: 99.22%] [G loss: 22.544847]\n",
      "epoch:0 step:57 [D loss: 0.006127, acc: 100.00%] [G loss: 22.599827]\n",
      "epoch:0 step:58 [D loss: 0.013855, acc: 100.00%] [G loss: 22.709337]\n",
      "epoch:0 step:59 [D loss: 0.009037, acc: 100.00%] [G loss: 22.987503]\n",
      "epoch:0 step:60 [D loss: 0.009786, acc: 100.00%] [G loss: 22.926239]\n",
      "epoch:0 step:61 [D loss: 0.012969, acc: 100.00%] [G loss: 23.030399]\n",
      "epoch:0 step:62 [D loss: 0.014840, acc: 100.00%] [G loss: 23.358416]\n",
      "epoch:0 step:63 [D loss: 0.004040, acc: 100.00%] [G loss: 23.013670]\n",
      "epoch:0 step:64 [D loss: 0.516760, acc: 93.75%] [G loss: 20.982807]\n",
      "epoch:0 step:65 [D loss: 0.396698, acc: 87.50%] [G loss: 21.381413]\n",
      "epoch:0 step:66 [D loss: 0.163189, acc: 93.75%] [G loss: 23.976095]\n",
      "epoch:0 step:67 [D loss: 0.030692, acc: 99.22%] [G loss: 24.071720]\n",
      "epoch:0 step:68 [D loss: 0.407845, acc: 91.41%] [G loss: 23.069263]\n",
      "epoch:0 step:69 [D loss: 0.079968, acc: 96.09%] [G loss: 23.452354]\n",
      "epoch:0 step:70 [D loss: 0.432665, acc: 89.84%] [G loss: 24.502550]\n",
      "epoch:0 step:71 [D loss: 0.204219, acc: 89.84%] [G loss: 24.410942]\n",
      "epoch:0 step:72 [D loss: 0.160530, acc: 92.97%] [G loss: 23.650543]\n",
      "epoch:0 step:73 [D loss: 0.148448, acc: 94.53%] [G loss: 20.057793]\n",
      "epoch:0 step:74 [D loss: 1.252308, acc: 78.12%] [G loss: 23.949232]\n",
      "epoch:0 step:75 [D loss: 0.263952, acc: 92.19%] [G loss: 20.826918]\n",
      "epoch:0 step:76 [D loss: 0.744880, acc: 84.38%] [G loss: 23.170946]\n",
      "epoch:0 step:77 [D loss: 0.487542, acc: 84.38%] [G loss: 21.671471]\n",
      "epoch:0 step:78 [D loss: 0.190034, acc: 91.41%] [G loss: 18.506048]\n",
      "epoch:0 step:79 [D loss: 0.412845, acc: 85.16%] [G loss: 22.538673]\n",
      "epoch:0 step:80 [D loss: 0.167082, acc: 94.53%] [G loss: 17.200893]\n",
      "epoch:0 step:81 [D loss: 0.842391, acc: 78.12%] [G loss: 22.359224]\n",
      "epoch:0 step:82 [D loss: 0.663383, acc: 88.28%] [G loss: 22.225803]\n",
      "epoch:0 step:83 [D loss: 0.325359, acc: 87.50%] [G loss: 16.149710]\n",
      "epoch:0 step:84 [D loss: 0.883465, acc: 79.69%] [G loss: 15.131268]\n",
      "epoch:0 step:85 [D loss: 0.756124, acc: 75.78%] [G loss: 19.482334]\n",
      "epoch:0 step:86 [D loss: 0.503031, acc: 81.25%] [G loss: 14.505774]\n",
      "epoch:0 step:87 [D loss: 0.607072, acc: 75.00%] [G loss: 16.541388]\n",
      "epoch:0 step:88 [D loss: 0.221052, acc: 89.06%] [G loss: 15.444617]\n",
      "epoch:0 step:89 [D loss: 0.465380, acc: 75.78%] [G loss: 18.463320]\n",
      "epoch:0 step:90 [D loss: 0.249809, acc: 87.50%] [G loss: 15.338549]\n",
      "epoch:0 step:91 [D loss: 0.255921, acc: 87.50%] [G loss: 14.104509]\n",
      "epoch:0 step:92 [D loss: 0.168810, acc: 92.19%] [G loss: 15.881643]\n",
      "epoch:0 step:93 [D loss: 0.249751, acc: 87.50%] [G loss: 12.919662]\n",
      "epoch:0 step:94 [D loss: 0.430680, acc: 76.56%] [G loss: 13.904495]\n",
      "epoch:0 step:95 [D loss: 0.278231, acc: 90.62%] [G loss: 14.522070]\n",
      "epoch:0 step:96 [D loss: 0.174184, acc: 90.62%] [G loss: 11.929965]\n",
      "epoch:0 step:97 [D loss: 0.445041, acc: 77.34%] [G loss: 12.539976]\n",
      "epoch:0 step:98 [D loss: 0.252445, acc: 87.50%] [G loss: 13.142628]\n",
      "epoch:0 step:99 [D loss: 0.286490, acc: 88.28%] [G loss: 10.316191]\n",
      "epoch:0 step:100 [D loss: 0.415740, acc: 79.69%] [G loss: 12.687202]\n",
      "epoch:0 step:101 [D loss: 0.383104, acc: 83.59%] [G loss: 9.880590]\n",
      "epoch:0 step:102 [D loss: 0.490767, acc: 78.91%] [G loss: 9.454634]\n",
      "epoch:0 step:103 [D loss: 0.309936, acc: 88.28%] [G loss: 10.880252]\n",
      "epoch:0 step:104 [D loss: 0.360869, acc: 83.59%] [G loss: 11.235097]\n",
      "epoch:0 step:105 [D loss: 0.198018, acc: 91.41%] [G loss: 8.314338]\n",
      "epoch:0 step:106 [D loss: 0.627329, acc: 71.09%] [G loss: 12.167282]\n",
      "epoch:0 step:107 [D loss: 0.281762, acc: 84.38%] [G loss: 10.208928]\n",
      "epoch:0 step:108 [D loss: 0.700145, acc: 71.09%] [G loss: 13.557068]\n",
      "epoch:0 step:109 [D loss: 0.469158, acc: 81.25%] [G loss: 10.929087]\n",
      "epoch:0 step:110 [D loss: 0.313685, acc: 81.25%] [G loss: 8.713357]\n",
      "epoch:0 step:111 [D loss: 0.342173, acc: 83.59%] [G loss: 10.969029]\n",
      "epoch:0 step:112 [D loss: 0.263128, acc: 87.50%] [G loss: 10.309240]\n",
      "epoch:0 step:113 [D loss: 0.254273, acc: 87.50%] [G loss: 9.365286]\n",
      "epoch:0 step:114 [D loss: 0.356626, acc: 82.81%] [G loss: 10.616381]\n",
      "epoch:0 step:115 [D loss: 0.203520, acc: 88.28%] [G loss: 10.043741]\n",
      "epoch:0 step:116 [D loss: 0.313529, acc: 81.25%] [G loss: 9.739859]\n",
      "epoch:0 step:117 [D loss: 0.296555, acc: 88.28%] [G loss: 10.535027]\n",
      "epoch:0 step:118 [D loss: 0.219852, acc: 89.06%] [G loss: 9.947546]\n",
      "epoch:0 step:119 [D loss: 0.305417, acc: 82.81%] [G loss: 10.041264]\n",
      "epoch:0 step:120 [D loss: 0.298783, acc: 83.59%] [G loss: 9.629285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:121 [D loss: 0.149015, acc: 93.75%] [G loss: 9.243011]\n",
      "epoch:0 step:122 [D loss: 0.210772, acc: 92.19%] [G loss: 9.750341]\n",
      "epoch:0 step:123 [D loss: 0.254992, acc: 89.06%] [G loss: 9.830744]\n",
      "epoch:0 step:124 [D loss: 0.218075, acc: 88.28%] [G loss: 9.230008]\n",
      "epoch:0 step:125 [D loss: 0.287851, acc: 84.38%] [G loss: 10.812195]\n",
      "epoch:0 step:126 [D loss: 0.199843, acc: 89.06%] [G loss: 8.976267]\n",
      "epoch:0 step:127 [D loss: 0.245305, acc: 89.84%] [G loss: 10.378321]\n",
      "epoch:0 step:128 [D loss: 0.223550, acc: 90.62%] [G loss: 9.636259]\n",
      "epoch:0 step:129 [D loss: 0.243253, acc: 89.84%] [G loss: 10.219021]\n",
      "epoch:0 step:130 [D loss: 0.331619, acc: 85.16%] [G loss: 9.974600]\n",
      "epoch:0 step:131 [D loss: 0.206074, acc: 87.50%] [G loss: 9.873271]\n",
      "epoch:0 step:132 [D loss: 0.171571, acc: 92.97%] [G loss: 9.953037]\n",
      "epoch:0 step:133 [D loss: 0.232923, acc: 84.38%] [G loss: 9.985651]\n",
      "epoch:0 step:134 [D loss: 0.186726, acc: 92.19%] [G loss: 9.248980]\n",
      "epoch:0 step:135 [D loss: 0.371629, acc: 83.59%] [G loss: 9.356424]\n",
      "epoch:0 step:136 [D loss: 0.233040, acc: 89.06%] [G loss: 9.161750]\n",
      "epoch:0 step:137 [D loss: 0.263399, acc: 85.16%] [G loss: 9.202673]\n",
      "epoch:0 step:138 [D loss: 0.270336, acc: 87.50%] [G loss: 10.331250]\n",
      "epoch:0 step:139 [D loss: 0.190582, acc: 90.62%] [G loss: 9.004980]\n",
      "epoch:0 step:140 [D loss: 0.334961, acc: 82.81%] [G loss: 9.827908]\n",
      "epoch:0 step:141 [D loss: 0.201852, acc: 89.06%] [G loss: 9.269529]\n",
      "epoch:0 step:142 [D loss: 0.327874, acc: 82.81%] [G loss: 9.781851]\n",
      "epoch:0 step:143 [D loss: 0.178026, acc: 90.62%] [G loss: 8.623363]\n",
      "epoch:0 step:144 [D loss: 0.248090, acc: 89.84%] [G loss: 9.638237]\n",
      "epoch:0 step:145 [D loss: 0.179470, acc: 89.84%] [G loss: 9.044417]\n",
      "epoch:0 step:146 [D loss: 0.535039, acc: 79.69%] [G loss: 10.413512]\n",
      "epoch:0 step:147 [D loss: 0.262672, acc: 84.38%] [G loss: 9.412743]\n",
      "epoch:0 step:148 [D loss: 0.205127, acc: 88.28%] [G loss: 8.098774]\n",
      "epoch:0 step:149 [D loss: 0.218867, acc: 88.28%] [G loss: 9.205676]\n",
      "epoch:0 step:150 [D loss: 0.179984, acc: 93.75%] [G loss: 8.820652]\n",
      "epoch:0 step:151 [D loss: 0.128897, acc: 93.75%] [G loss: 8.244816]\n",
      "epoch:0 step:152 [D loss: 0.129189, acc: 96.09%] [G loss: 8.115879]\n",
      "epoch:0 step:153 [D loss: 0.269653, acc: 89.06%] [G loss: 9.991661]\n",
      "epoch:0 step:154 [D loss: 0.154450, acc: 92.97%] [G loss: 8.870124]\n",
      "epoch:0 step:155 [D loss: 0.186138, acc: 92.19%] [G loss: 8.586614]\n",
      "epoch:0 step:156 [D loss: 0.180530, acc: 92.97%] [G loss: 8.356625]\n",
      "epoch:0 step:157 [D loss: 0.199990, acc: 89.84%] [G loss: 7.836921]\n",
      "epoch:0 step:158 [D loss: 0.117433, acc: 96.88%] [G loss: 7.450687]\n",
      "epoch:0 step:159 [D loss: 0.127666, acc: 95.31%] [G loss: 8.247067]\n",
      "epoch:0 step:160 [D loss: 0.593222, acc: 69.53%] [G loss: 11.193317]\n",
      "epoch:0 step:161 [D loss: 0.232235, acc: 85.94%] [G loss: 9.333013]\n",
      "epoch:0 step:162 [D loss: 0.375323, acc: 80.47%] [G loss: 8.724563]\n",
      "epoch:0 step:163 [D loss: 0.148349, acc: 93.75%] [G loss: 6.937594]\n",
      "epoch:0 step:164 [D loss: 0.093294, acc: 96.88%] [G loss: 6.848114]\n",
      "epoch:0 step:165 [D loss: 0.163435, acc: 95.31%] [G loss: 8.071681]\n",
      "epoch:0 step:166 [D loss: 0.155778, acc: 92.97%] [G loss: 7.388255]\n",
      "epoch:0 step:167 [D loss: 0.182153, acc: 94.53%] [G loss: 8.337367]\n",
      "epoch:0 step:168 [D loss: 0.173729, acc: 92.97%] [G loss: 7.352606]\n",
      "epoch:0 step:169 [D loss: 0.164346, acc: 93.75%] [G loss: 6.790750]\n",
      "epoch:0 step:170 [D loss: 0.265564, acc: 88.28%] [G loss: 8.153587]\n",
      "epoch:0 step:171 [D loss: 0.248295, acc: 86.72%] [G loss: 8.072963]\n",
      "epoch:0 step:172 [D loss: 0.113707, acc: 97.66%] [G loss: 7.779911]\n",
      "epoch:0 step:173 [D loss: 0.239053, acc: 88.28%] [G loss: 6.987638]\n",
      "epoch:0 step:174 [D loss: 0.320944, acc: 83.59%] [G loss: 8.066247]\n",
      "epoch:0 step:175 [D loss: 0.132311, acc: 96.09%] [G loss: 6.580427]\n",
      "epoch:0 step:176 [D loss: 0.357897, acc: 85.94%] [G loss: 7.563878]\n",
      "epoch:0 step:177 [D loss: 0.274307, acc: 84.38%] [G loss: 6.444998]\n",
      "epoch:0 step:178 [D loss: 0.194285, acc: 90.62%] [G loss: 6.493208]\n",
      "epoch:0 step:179 [D loss: 0.219600, acc: 92.19%] [G loss: 6.998640]\n",
      "epoch:0 step:180 [D loss: 0.287914, acc: 86.72%] [G loss: 6.974136]\n",
      "epoch:0 step:181 [D loss: 0.197016, acc: 92.19%] [G loss: 6.246480]\n",
      "epoch:0 step:182 [D loss: 0.205142, acc: 91.41%] [G loss: 7.756720]\n",
      "epoch:0 step:183 [D loss: 0.216861, acc: 88.28%] [G loss: 6.003690]\n",
      "epoch:0 step:184 [D loss: 0.320336, acc: 84.38%] [G loss: 7.628675]\n",
      "epoch:0 step:185 [D loss: 0.307912, acc: 85.94%] [G loss: 6.654101]\n",
      "epoch:0 step:186 [D loss: 0.177438, acc: 94.53%] [G loss: 6.399016]\n",
      "epoch:0 step:187 [D loss: 0.310288, acc: 85.94%] [G loss: 6.827825]\n",
      "epoch:0 step:188 [D loss: 0.356655, acc: 82.03%] [G loss: 6.376037]\n",
      "epoch:0 step:189 [D loss: 0.261189, acc: 88.28%] [G loss: 5.851240]\n",
      "epoch:0 step:190 [D loss: 0.403718, acc: 83.59%] [G loss: 6.671441]\n",
      "epoch:0 step:191 [D loss: 0.257314, acc: 86.72%] [G loss: 5.934644]\n",
      "epoch:0 step:192 [D loss: 0.461978, acc: 82.81%] [G loss: 7.018377]\n",
      "epoch:0 step:193 [D loss: 0.237909, acc: 89.84%] [G loss: 5.527761]\n",
      "epoch:0 step:194 [D loss: 0.342555, acc: 83.59%] [G loss: 6.162703]\n",
      "epoch:0 step:195 [D loss: 0.219932, acc: 90.62%] [G loss: 5.737215]\n",
      "epoch:0 step:196 [D loss: 0.394326, acc: 80.47%] [G loss: 5.291095]\n",
      "epoch:0 step:197 [D loss: 0.178549, acc: 93.75%] [G loss: 4.722676]\n",
      "epoch:0 step:198 [D loss: 0.264378, acc: 86.72%] [G loss: 5.257165]\n",
      "epoch:0 step:199 [D loss: 0.245645, acc: 88.28%] [G loss: 4.676891]\n",
      "epoch:0 step:200 [D loss: 0.428501, acc: 80.47%] [G loss: 5.545441]\n",
      "epoch:0 step:201 [D loss: 0.218171, acc: 90.62%] [G loss: 4.931383]\n",
      "epoch:0 step:202 [D loss: 0.472218, acc: 75.78%] [G loss: 6.035040]\n",
      "epoch:0 step:203 [D loss: 0.176326, acc: 91.41%] [G loss: 4.232564]\n",
      "epoch:0 step:204 [D loss: 0.442857, acc: 81.25%] [G loss: 5.302794]\n",
      "epoch:0 step:205 [D loss: 0.343556, acc: 80.47%] [G loss: 4.967786]\n",
      "epoch:0 step:206 [D loss: 0.230566, acc: 89.06%] [G loss: 4.396819]\n",
      "epoch:0 step:207 [D loss: 0.755758, acc: 57.03%] [G loss: 6.880897]\n",
      "epoch:0 step:208 [D loss: 0.229356, acc: 86.72%] [G loss: 4.324033]\n",
      "epoch:0 step:209 [D loss: 0.415048, acc: 79.69%] [G loss: 5.050681]\n",
      "epoch:0 step:210 [D loss: 0.386391, acc: 82.03%] [G loss: 4.891321]\n",
      "epoch:0 step:211 [D loss: 0.335851, acc: 89.06%] [G loss: 4.758058]\n",
      "epoch:0 step:212 [D loss: 0.515956, acc: 67.97%] [G loss: 4.590608]\n",
      "epoch:0 step:213 [D loss: 0.279320, acc: 86.72%] [G loss: 4.680266]\n",
      "epoch:0 step:214 [D loss: 0.408631, acc: 86.72%] [G loss: 4.631324]\n",
      "epoch:0 step:215 [D loss: 0.228929, acc: 89.84%] [G loss: 3.972404]\n",
      "epoch:0 step:216 [D loss: 0.774636, acc: 54.69%] [G loss: 6.038030]\n",
      "epoch:0 step:217 [D loss: 0.212913, acc: 90.62%] [G loss: 4.151852]\n",
      "epoch:0 step:218 [D loss: 0.382824, acc: 82.03%] [G loss: 4.881320]\n",
      "epoch:0 step:219 [D loss: 0.240325, acc: 92.97%] [G loss: 4.616938]\n",
      "epoch:0 step:220 [D loss: 0.892628, acc: 51.56%] [G loss: 5.733539]\n",
      "epoch:0 step:221 [D loss: 0.225470, acc: 91.41%] [G loss: 4.443712]\n",
      "epoch:0 step:222 [D loss: 0.269865, acc: 96.09%] [G loss: 4.169617]\n",
      "epoch:0 step:223 [D loss: 0.236330, acc: 93.75%] [G loss: 4.055054]\n",
      "epoch:0 step:224 [D loss: 0.293457, acc: 87.50%] [G loss: 3.814946]\n",
      "epoch:0 step:225 [D loss: 0.235604, acc: 92.97%] [G loss: 3.980675]\n",
      "epoch:0 step:226 [D loss: 0.216520, acc: 97.66%] [G loss: 3.690521]\n",
      "epoch:0 step:227 [D loss: 0.307737, acc: 88.28%] [G loss: 4.176159]\n",
      "epoch:0 step:228 [D loss: 0.214048, acc: 96.88%] [G loss: 4.296838]\n",
      "epoch:0 step:229 [D loss: 0.336976, acc: 84.38%] [G loss: 4.570233]\n",
      "epoch:0 step:230 [D loss: 0.336639, acc: 84.38%] [G loss: 4.102281]\n",
      "epoch:0 step:231 [D loss: 0.380421, acc: 82.81%] [G loss: 5.271321]\n",
      "epoch:0 step:232 [D loss: 0.301319, acc: 87.50%] [G loss: 4.643295]\n",
      "epoch:0 step:233 [D loss: 0.287410, acc: 91.41%] [G loss: 4.375401]\n",
      "epoch:0 step:234 [D loss: 0.190115, acc: 92.19%] [G loss: 3.684438]\n",
      "epoch:0 step:235 [D loss: 0.313204, acc: 87.50%] [G loss: 4.105884]\n",
      "epoch:0 step:236 [D loss: 0.257110, acc: 90.62%] [G loss: 4.087394]\n",
      "epoch:0 step:237 [D loss: 0.489250, acc: 72.66%] [G loss: 4.600052]\n",
      "epoch:0 step:238 [D loss: 0.262250, acc: 91.41%] [G loss: 3.802838]\n",
      "epoch:0 step:239 [D loss: 0.403352, acc: 85.94%] [G loss: 4.664036]\n",
      "epoch:0 step:240 [D loss: 0.286547, acc: 85.16%] [G loss: 3.967776]\n",
      "epoch:0 step:241 [D loss: 0.499868, acc: 72.66%] [G loss: 4.994780]\n",
      "epoch:0 step:242 [D loss: 0.336789, acc: 83.59%] [G loss: 4.192772]\n",
      "epoch:0 step:243 [D loss: 0.735145, acc: 51.56%] [G loss: 6.007383]\n",
      "epoch:0 step:244 [D loss: 0.284510, acc: 85.94%] [G loss: 4.529418]\n",
      "epoch:0 step:245 [D loss: 0.477154, acc: 77.34%] [G loss: 4.590708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:246 [D loss: 0.274193, acc: 89.06%] [G loss: 4.074724]\n",
      "epoch:0 step:247 [D loss: 0.393648, acc: 83.59%] [G loss: 4.825886]\n",
      "epoch:0 step:248 [D loss: 0.417007, acc: 77.34%] [G loss: 4.817543]\n",
      "epoch:0 step:249 [D loss: 0.419982, acc: 81.25%] [G loss: 5.440853]\n",
      "epoch:0 step:250 [D loss: 0.259158, acc: 89.84%] [G loss: 4.787635]\n",
      "epoch:0 step:251 [D loss: 0.371470, acc: 84.38%] [G loss: 4.873925]\n",
      "epoch:0 step:252 [D loss: 0.391861, acc: 82.81%] [G loss: 4.812848]\n",
      "epoch:0 step:253 [D loss: 0.291242, acc: 87.50%] [G loss: 4.465428]\n",
      "epoch:0 step:254 [D loss: 0.587937, acc: 71.09%] [G loss: 5.215032]\n",
      "epoch:0 step:255 [D loss: 0.187518, acc: 93.75%] [G loss: 4.899704]\n",
      "epoch:0 step:256 [D loss: 0.534052, acc: 70.31%] [G loss: 6.584853]\n",
      "epoch:0 step:257 [D loss: 0.231385, acc: 92.19%] [G loss: 4.816522]\n",
      "epoch:0 step:258 [D loss: 0.322282, acc: 86.72%] [G loss: 5.255944]\n",
      "epoch:0 step:259 [D loss: 0.239902, acc: 89.06%] [G loss: 4.644705]\n",
      "epoch:0 step:260 [D loss: 0.359113, acc: 82.81%] [G loss: 5.218607]\n",
      "epoch:0 step:261 [D loss: 0.229846, acc: 93.75%] [G loss: 5.020847]\n",
      "epoch:0 step:262 [D loss: 0.540010, acc: 70.31%] [G loss: 5.888308]\n",
      "epoch:0 step:263 [D loss: 0.249442, acc: 87.50%] [G loss: 3.995095]\n",
      "epoch:0 step:264 [D loss: 0.391067, acc: 81.25%] [G loss: 4.655566]\n",
      "epoch:0 step:265 [D loss: 0.419558, acc: 76.56%] [G loss: 5.049364]\n",
      "epoch:0 step:266 [D loss: 0.313525, acc: 86.72%] [G loss: 4.377004]\n",
      "epoch:0 step:267 [D loss: 0.364948, acc: 84.38%] [G loss: 4.595725]\n",
      "epoch:0 step:268 [D loss: 0.310889, acc: 87.50%] [G loss: 4.261811]\n",
      "epoch:0 step:269 [D loss: 0.278509, acc: 90.62%] [G loss: 3.488262]\n",
      "epoch:0 step:270 [D loss: 0.311239, acc: 86.72%] [G loss: 3.880880]\n",
      "epoch:0 step:271 [D loss: 0.372661, acc: 82.03%] [G loss: 3.988140]\n",
      "epoch:0 step:272 [D loss: 0.256551, acc: 90.62%] [G loss: 4.500412]\n",
      "epoch:0 step:273 [D loss: 0.520028, acc: 75.78%] [G loss: 5.051451]\n",
      "epoch:0 step:274 [D loss: 0.507229, acc: 74.22%] [G loss: 3.545661]\n",
      "epoch:0 step:275 [D loss: 0.461269, acc: 82.81%] [G loss: 4.050871]\n",
      "epoch:0 step:276 [D loss: 0.273082, acc: 92.19%] [G loss: 4.123738]\n",
      "epoch:0 step:277 [D loss: 0.384318, acc: 84.38%] [G loss: 4.168704]\n",
      "epoch:0 step:278 [D loss: 0.356553, acc: 81.25%] [G loss: 4.128654]\n",
      "epoch:0 step:279 [D loss: 0.291239, acc: 88.28%] [G loss: 4.240904]\n",
      "epoch:0 step:280 [D loss: 0.377614, acc: 85.16%] [G loss: 5.124559]\n",
      "epoch:0 step:281 [D loss: 0.358234, acc: 83.59%] [G loss: 4.324573]\n",
      "epoch:0 step:282 [D loss: 0.427627, acc: 79.69%] [G loss: 4.388689]\n",
      "epoch:0 step:283 [D loss: 0.360082, acc: 85.94%] [G loss: 4.084025]\n",
      "epoch:0 step:284 [D loss: 0.328837, acc: 86.72%] [G loss: 3.949359]\n",
      "epoch:0 step:285 [D loss: 0.465513, acc: 82.03%] [G loss: 4.920861]\n",
      "epoch:0 step:286 [D loss: 0.243881, acc: 96.09%] [G loss: 4.436526]\n",
      "epoch:0 step:287 [D loss: 0.464429, acc: 76.56%] [G loss: 5.193898]\n",
      "epoch:0 step:288 [D loss: 0.320145, acc: 88.28%] [G loss: 4.572433]\n",
      "epoch:0 step:289 [D loss: 0.343315, acc: 86.72%] [G loss: 5.180685]\n",
      "epoch:0 step:290 [D loss: 0.302351, acc: 90.62%] [G loss: 4.169346]\n",
      "epoch:0 step:291 [D loss: 0.433239, acc: 82.81%] [G loss: 3.982539]\n",
      "epoch:0 step:292 [D loss: 0.231827, acc: 94.53%] [G loss: 3.987446]\n",
      "epoch:0 step:293 [D loss: 0.532996, acc: 72.66%] [G loss: 4.114743]\n",
      "epoch:0 step:294 [D loss: 0.387477, acc: 87.50%] [G loss: 4.802292]\n",
      "epoch:0 step:295 [D loss: 0.240595, acc: 91.41%] [G loss: 4.171608]\n",
      "epoch:0 step:296 [D loss: 0.431426, acc: 83.59%] [G loss: 5.214437]\n",
      "epoch:0 step:297 [D loss: 0.274244, acc: 88.28%] [G loss: 4.114705]\n",
      "epoch:0 step:298 [D loss: 0.302856, acc: 90.62%] [G loss: 4.736624]\n",
      "epoch:0 step:299 [D loss: 0.282070, acc: 87.50%] [G loss: 4.802503]\n",
      "epoch:0 step:300 [D loss: 0.339261, acc: 83.59%] [G loss: 4.628882]\n",
      "epoch:0 step:301 [D loss: 0.848750, acc: 53.12%] [G loss: 5.838770]\n",
      "epoch:0 step:302 [D loss: 0.198676, acc: 95.31%] [G loss: 4.885425]\n",
      "epoch:0 step:303 [D loss: 0.252230, acc: 92.97%] [G loss: 5.473168]\n",
      "epoch:0 step:304 [D loss: 0.248903, acc: 91.41%] [G loss: 5.116953]\n",
      "epoch:0 step:305 [D loss: 0.205276, acc: 96.09%] [G loss: 4.873215]\n",
      "epoch:0 step:306 [D loss: 0.217826, acc: 93.75%] [G loss: 5.726211]\n",
      "epoch:0 step:307 [D loss: 0.311143, acc: 87.50%] [G loss: 4.758032]\n",
      "epoch:0 step:308 [D loss: 0.365070, acc: 86.72%] [G loss: 4.586926]\n",
      "epoch:0 step:309 [D loss: 0.283138, acc: 89.06%] [G loss: 4.370275]\n",
      "epoch:0 step:310 [D loss: 0.339443, acc: 84.38%] [G loss: 5.368350]\n",
      "epoch:0 step:311 [D loss: 0.276459, acc: 86.72%] [G loss: 4.444747]\n",
      "epoch:0 step:312 [D loss: 0.641997, acc: 68.75%] [G loss: 4.703336]\n",
      "epoch:0 step:313 [D loss: 0.211550, acc: 93.75%] [G loss: 4.472126]\n",
      "epoch:0 step:314 [D loss: 0.214847, acc: 93.75%] [G loss: 4.025136]\n",
      "epoch:0 step:315 [D loss: 0.245523, acc: 90.62%] [G loss: 4.522970]\n",
      "epoch:0 step:316 [D loss: 0.243349, acc: 93.75%] [G loss: 3.671448]\n",
      "epoch:0 step:317 [D loss: 0.328639, acc: 84.38%] [G loss: 3.770897]\n",
      "epoch:0 step:318 [D loss: 0.498645, acc: 71.88%] [G loss: 4.147490]\n",
      "epoch:0 step:319 [D loss: 0.372943, acc: 80.47%] [G loss: 3.778608]\n",
      "epoch:0 step:320 [D loss: 0.534775, acc: 68.75%] [G loss: 3.567418]\n",
      "epoch:0 step:321 [D loss: 0.330908, acc: 87.50%] [G loss: 3.852903]\n",
      "epoch:0 step:322 [D loss: 0.609477, acc: 65.62%] [G loss: 4.361238]\n",
      "epoch:0 step:323 [D loss: 0.363258, acc: 84.38%] [G loss: 3.815981]\n",
      "epoch:0 step:324 [D loss: 0.591036, acc: 64.84%] [G loss: 4.742049]\n",
      "epoch:0 step:325 [D loss: 0.264450, acc: 90.62%] [G loss: 3.789082]\n",
      "epoch:0 step:326 [D loss: 0.423499, acc: 85.16%] [G loss: 4.309956]\n",
      "epoch:0 step:327 [D loss: 0.295353, acc: 86.72%] [G loss: 4.069412]\n",
      "epoch:0 step:328 [D loss: 0.372721, acc: 85.16%] [G loss: 4.747650]\n",
      "epoch:0 step:329 [D loss: 0.346260, acc: 82.81%] [G loss: 4.281085]\n",
      "epoch:0 step:330 [D loss: 0.486359, acc: 72.66%] [G loss: 4.392938]\n",
      "epoch:0 step:331 [D loss: 0.269638, acc: 93.75%] [G loss: 4.064285]\n",
      "epoch:0 step:332 [D loss: 0.499993, acc: 76.56%] [G loss: 4.222346]\n",
      "epoch:0 step:333 [D loss: 0.360349, acc: 84.38%] [G loss: 4.211651]\n",
      "epoch:0 step:334 [D loss: 0.327446, acc: 88.28%] [G loss: 4.373151]\n",
      "epoch:0 step:335 [D loss: 0.447663, acc: 78.91%] [G loss: 4.373431]\n",
      "epoch:0 step:336 [D loss: 0.295438, acc: 88.28%] [G loss: 5.033168]\n",
      "epoch:0 step:337 [D loss: 0.288091, acc: 89.84%] [G loss: 4.477020]\n",
      "epoch:0 step:338 [D loss: 0.285827, acc: 89.84%] [G loss: 4.411072]\n",
      "epoch:0 step:339 [D loss: 0.419840, acc: 81.25%] [G loss: 4.854173]\n",
      "epoch:0 step:340 [D loss: 0.156755, acc: 100.00%] [G loss: 4.891071]\n",
      "epoch:0 step:341 [D loss: 1.053524, acc: 46.09%] [G loss: 5.742191]\n",
      "epoch:0 step:342 [D loss: 0.187014, acc: 96.09%] [G loss: 4.025871]\n",
      "epoch:0 step:343 [D loss: 0.502275, acc: 76.56%] [G loss: 5.351735]\n",
      "epoch:0 step:344 [D loss: 0.297252, acc: 91.41%] [G loss: 4.536569]\n",
      "epoch:0 step:345 [D loss: 0.502319, acc: 75.00%] [G loss: 4.556415]\n",
      "epoch:0 step:346 [D loss: 0.208935, acc: 93.75%] [G loss: 4.260106]\n",
      "epoch:0 step:347 [D loss: 0.214578, acc: 95.31%] [G loss: 4.483790]\n",
      "epoch:0 step:348 [D loss: 0.634316, acc: 67.97%] [G loss: 4.610329]\n",
      "epoch:0 step:349 [D loss: 0.349088, acc: 85.94%] [G loss: 3.774200]\n",
      "epoch:0 step:350 [D loss: 0.292496, acc: 91.41%] [G loss: 3.973032]\n",
      "epoch:0 step:351 [D loss: 0.447814, acc: 78.91%] [G loss: 4.527925]\n",
      "epoch:0 step:352 [D loss: 0.335840, acc: 85.94%] [G loss: 4.402328]\n",
      "epoch:0 step:353 [D loss: 0.324439, acc: 89.84%] [G loss: 4.606809]\n",
      "epoch:0 step:354 [D loss: 0.416647, acc: 81.25%] [G loss: 4.215262]\n",
      "epoch:0 step:355 [D loss: 0.480669, acc: 80.47%] [G loss: 4.587706]\n",
      "epoch:0 step:356 [D loss: 0.350444, acc: 83.59%] [G loss: 3.952246]\n",
      "epoch:0 step:357 [D loss: 0.415840, acc: 78.91%] [G loss: 4.255477]\n",
      "epoch:0 step:358 [D loss: 0.320298, acc: 86.72%] [G loss: 4.762975]\n",
      "epoch:0 step:359 [D loss: 0.323117, acc: 86.72%] [G loss: 4.692763]\n",
      "epoch:0 step:360 [D loss: 0.450271, acc: 79.69%] [G loss: 3.989161]\n",
      "epoch:0 step:361 [D loss: 0.303601, acc: 91.41%] [G loss: 4.561945]\n",
      "epoch:0 step:362 [D loss: 0.384130, acc: 83.59%] [G loss: 4.382163]\n",
      "epoch:0 step:363 [D loss: 0.354406, acc: 87.50%] [G loss: 4.034311]\n",
      "epoch:0 step:364 [D loss: 0.337772, acc: 88.28%] [G loss: 4.423185]\n",
      "epoch:0 step:365 [D loss: 0.497328, acc: 76.56%] [G loss: 4.804014]\n",
      "epoch:0 step:366 [D loss: 0.329382, acc: 85.94%] [G loss: 3.984336]\n",
      "epoch:0 step:367 [D loss: 0.419299, acc: 78.12%] [G loss: 4.237834]\n",
      "epoch:0 step:368 [D loss: 0.417797, acc: 84.38%] [G loss: 4.271516]\n",
      "epoch:0 step:369 [D loss: 0.443071, acc: 80.47%] [G loss: 4.331520]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:370 [D loss: 0.301771, acc: 89.84%] [G loss: 4.569265]\n",
      "epoch:0 step:371 [D loss: 0.365143, acc: 87.50%] [G loss: 4.343934]\n",
      "epoch:0 step:372 [D loss: 0.353019, acc: 88.28%] [G loss: 4.511176]\n",
      "epoch:0 step:373 [D loss: 0.329260, acc: 91.41%] [G loss: 4.422723]\n",
      "epoch:0 step:374 [D loss: 0.322959, acc: 89.84%] [G loss: 4.468306]\n",
      "epoch:0 step:375 [D loss: 0.291635, acc: 88.28%] [G loss: 4.627486]\n",
      "epoch:0 step:376 [D loss: 0.485768, acc: 74.22%] [G loss: 4.378201]\n",
      "epoch:0 step:377 [D loss: 0.356669, acc: 84.38%] [G loss: 4.259843]\n",
      "epoch:0 step:378 [D loss: 0.312321, acc: 91.41%] [G loss: 4.723150]\n",
      "epoch:0 step:379 [D loss: 0.457054, acc: 77.34%] [G loss: 4.753057]\n",
      "epoch:0 step:380 [D loss: 0.278478, acc: 92.19%] [G loss: 4.174332]\n",
      "epoch:0 step:381 [D loss: 0.359758, acc: 83.59%] [G loss: 4.561756]\n",
      "epoch:0 step:382 [D loss: 0.462320, acc: 78.12%] [G loss: 4.580508]\n",
      "epoch:0 step:383 [D loss: 0.362866, acc: 85.16%] [G loss: 3.972015]\n",
      "epoch:0 step:384 [D loss: 0.518312, acc: 74.22%] [G loss: 4.349738]\n",
      "epoch:0 step:385 [D loss: 0.365562, acc: 82.03%] [G loss: 4.421160]\n",
      "epoch:0 step:386 [D loss: 0.468099, acc: 77.34%] [G loss: 4.559505]\n",
      "epoch:0 step:387 [D loss: 0.236331, acc: 96.09%] [G loss: 4.193725]\n",
      "epoch:0 step:388 [D loss: 0.434788, acc: 78.12%] [G loss: 4.645199]\n",
      "epoch:0 step:389 [D loss: 0.368796, acc: 86.72%] [G loss: 4.148802]\n",
      "epoch:0 step:390 [D loss: 0.267508, acc: 92.19%] [G loss: 4.553846]\n",
      "epoch:0 step:391 [D loss: 0.564360, acc: 75.00%] [G loss: 4.768172]\n",
      "epoch:0 step:392 [D loss: 0.320106, acc: 87.50%] [G loss: 4.066558]\n",
      "epoch:0 step:393 [D loss: 0.483372, acc: 74.22%] [G loss: 4.406112]\n",
      "epoch:0 step:394 [D loss: 0.298474, acc: 89.06%] [G loss: 4.017644]\n",
      "epoch:0 step:395 [D loss: 0.300782, acc: 91.41%] [G loss: 4.199107]\n",
      "epoch:0 step:396 [D loss: 0.480912, acc: 76.56%] [G loss: 4.351182]\n",
      "epoch:0 step:397 [D loss: 0.319069, acc: 87.50%] [G loss: 4.375766]\n",
      "epoch:0 step:398 [D loss: 0.486866, acc: 78.91%] [G loss: 4.864491]\n",
      "epoch:0 step:399 [D loss: 0.255572, acc: 92.19%] [G loss: 4.177363]\n",
      "epoch:0 step:400 [D loss: 0.646845, acc: 68.75%] [G loss: 4.876498]\n",
      "epoch:0 step:401 [D loss: 0.293118, acc: 91.41%] [G loss: 4.261189]\n",
      "epoch:0 step:402 [D loss: 0.363911, acc: 83.59%] [G loss: 4.070208]\n",
      "epoch:0 step:403 [D loss: 0.392699, acc: 82.81%] [G loss: 4.761220]\n",
      "epoch:0 step:404 [D loss: 0.215748, acc: 94.53%] [G loss: 4.548469]\n",
      "epoch:0 step:405 [D loss: 0.331068, acc: 89.06%] [G loss: 4.613602]\n",
      "epoch:0 step:406 [D loss: 0.564569, acc: 71.88%] [G loss: 4.552580]\n",
      "epoch:0 step:407 [D loss: 0.540966, acc: 77.34%] [G loss: 4.464297]\n",
      "epoch:0 step:408 [D loss: 0.402088, acc: 81.25%] [G loss: 3.936925]\n",
      "epoch:0 step:409 [D loss: 0.444512, acc: 81.25%] [G loss: 4.123008]\n",
      "epoch:0 step:410 [D loss: 0.244097, acc: 93.75%] [G loss: 4.009297]\n",
      "epoch:0 step:411 [D loss: 0.341845, acc: 88.28%] [G loss: 3.839088]\n",
      "epoch:0 step:412 [D loss: 0.437179, acc: 83.59%] [G loss: 3.911291]\n",
      "epoch:0 step:413 [D loss: 0.307666, acc: 91.41%] [G loss: 3.816683]\n",
      "epoch:0 step:414 [D loss: 0.400125, acc: 88.28%] [G loss: 4.071560]\n",
      "epoch:0 step:415 [D loss: 0.381779, acc: 82.03%] [G loss: 4.135974]\n",
      "epoch:0 step:416 [D loss: 0.352924, acc: 86.72%] [G loss: 3.862066]\n",
      "epoch:0 step:417 [D loss: 0.634978, acc: 67.97%] [G loss: 4.407180]\n",
      "epoch:0 step:418 [D loss: 0.402553, acc: 84.38%] [G loss: 4.390836]\n",
      "epoch:0 step:419 [D loss: 0.330596, acc: 89.84%] [G loss: 4.163699]\n",
      "epoch:0 step:420 [D loss: 0.393586, acc: 82.81%] [G loss: 5.077199]\n",
      "epoch:0 step:421 [D loss: 0.419287, acc: 78.91%] [G loss: 3.747209]\n",
      "epoch:0 step:422 [D loss: 0.441299, acc: 79.69%] [G loss: 3.441370]\n",
      "epoch:0 step:423 [D loss: 0.420017, acc: 82.03%] [G loss: 3.787493]\n",
      "epoch:0 step:424 [D loss: 0.380556, acc: 83.59%] [G loss: 3.943827]\n",
      "epoch:0 step:425 [D loss: 0.350380, acc: 86.72%] [G loss: 4.341311]\n",
      "epoch:0 step:426 [D loss: 0.407934, acc: 85.16%] [G loss: 4.228261]\n",
      "epoch:0 step:427 [D loss: 0.376588, acc: 83.59%] [G loss: 4.389085]\n",
      "epoch:0 step:428 [D loss: 0.326982, acc: 88.28%] [G loss: 4.601971]\n",
      "epoch:0 step:429 [D loss: 0.378258, acc: 85.16%] [G loss: 4.444778]\n",
      "epoch:0 step:430 [D loss: 0.372571, acc: 84.38%] [G loss: 4.490465]\n",
      "epoch:0 step:431 [D loss: 0.456542, acc: 77.34%] [G loss: 3.980070]\n",
      "epoch:0 step:432 [D loss: 0.464910, acc: 78.91%] [G loss: 4.044985]\n",
      "epoch:0 step:433 [D loss: 0.349323, acc: 84.38%] [G loss: 4.015465]\n",
      "epoch:0 step:434 [D loss: 0.488376, acc: 80.47%] [G loss: 4.294299]\n",
      "epoch:0 step:435 [D loss: 0.328011, acc: 90.62%] [G loss: 3.978220]\n",
      "epoch:0 step:436 [D loss: 0.428030, acc: 80.47%] [G loss: 4.162027]\n",
      "epoch:0 step:437 [D loss: 0.512869, acc: 73.44%] [G loss: 3.700725]\n",
      "epoch:0 step:438 [D loss: 0.383903, acc: 83.59%] [G loss: 3.955376]\n",
      "epoch:0 step:439 [D loss: 0.476174, acc: 77.34%] [G loss: 4.189721]\n",
      "epoch:0 step:440 [D loss: 0.338373, acc: 88.28%] [G loss: 3.878401]\n",
      "epoch:0 step:441 [D loss: 0.501455, acc: 78.12%] [G loss: 4.072155]\n",
      "epoch:0 step:442 [D loss: 0.406470, acc: 81.25%] [G loss: 4.361928]\n",
      "epoch:0 step:443 [D loss: 0.321295, acc: 88.28%] [G loss: 4.117032]\n",
      "epoch:0 step:444 [D loss: 0.352358, acc: 85.94%] [G loss: 4.347526]\n",
      "epoch:0 step:445 [D loss: 0.430493, acc: 78.12%] [G loss: 4.338188]\n",
      "epoch:0 step:446 [D loss: 0.391243, acc: 84.38%] [G loss: 4.527707]\n",
      "epoch:0 step:447 [D loss: 0.307816, acc: 90.62%] [G loss: 4.680471]\n",
      "epoch:0 step:448 [D loss: 0.358949, acc: 85.16%] [G loss: 4.264633]\n",
      "epoch:0 step:449 [D loss: 0.428547, acc: 78.12%] [G loss: 4.610509]\n",
      "epoch:0 step:450 [D loss: 0.386141, acc: 79.69%] [G loss: 4.181079]\n",
      "epoch:0 step:451 [D loss: 0.411590, acc: 80.47%] [G loss: 4.150112]\n",
      "epoch:0 step:452 [D loss: 0.503293, acc: 75.78%] [G loss: 4.197833]\n",
      "epoch:0 step:453 [D loss: 0.371471, acc: 83.59%] [G loss: 4.186202]\n",
      "epoch:0 step:454 [D loss: 0.477338, acc: 75.78%] [G loss: 3.804466]\n",
      "epoch:0 step:455 [D loss: 0.385125, acc: 79.69%] [G loss: 3.882796]\n",
      "epoch:0 step:456 [D loss: 0.462702, acc: 82.81%] [G loss: 3.645370]\n",
      "epoch:0 step:457 [D loss: 0.386282, acc: 84.38%] [G loss: 4.089310]\n",
      "epoch:0 step:458 [D loss: 0.510812, acc: 72.66%] [G loss: 4.557006]\n",
      "epoch:0 step:459 [D loss: 0.371642, acc: 82.81%] [G loss: 4.407864]\n",
      "epoch:0 step:460 [D loss: 0.512664, acc: 78.12%] [G loss: 4.019229]\n",
      "epoch:0 step:461 [D loss: 0.405992, acc: 86.72%] [G loss: 3.948769]\n",
      "epoch:0 step:462 [D loss: 0.657462, acc: 64.06%] [G loss: 3.560239]\n",
      "epoch:0 step:463 [D loss: 0.396882, acc: 83.59%] [G loss: 3.597984]\n",
      "epoch:0 step:464 [D loss: 0.522012, acc: 71.88%] [G loss: 3.684166]\n",
      "epoch:0 step:465 [D loss: 0.383469, acc: 86.72%] [G loss: 4.066648]\n",
      "epoch:0 step:466 [D loss: 0.492913, acc: 75.78%] [G loss: 3.986809]\n",
      "epoch:0 step:467 [D loss: 0.366485, acc: 85.94%] [G loss: 3.831680]\n",
      "epoch:0 step:468 [D loss: 0.460612, acc: 75.00%] [G loss: 3.979633]\n",
      "epoch:0 step:469 [D loss: 0.429376, acc: 80.47%] [G loss: 3.650513]\n",
      "epoch:0 step:470 [D loss: 0.433977, acc: 75.78%] [G loss: 3.728623]\n",
      "epoch:0 step:471 [D loss: 0.465844, acc: 78.12%] [G loss: 4.440139]\n",
      "epoch:0 step:472 [D loss: 0.462798, acc: 76.56%] [G loss: 3.985933]\n",
      "epoch:0 step:473 [D loss: 0.459677, acc: 75.78%] [G loss: 4.055660]\n",
      "epoch:0 step:474 [D loss: 0.378383, acc: 79.69%] [G loss: 3.798322]\n",
      "epoch:0 step:475 [D loss: 0.418692, acc: 84.38%] [G loss: 3.987022]\n",
      "epoch:0 step:476 [D loss: 0.442760, acc: 79.69%] [G loss: 4.285760]\n",
      "epoch:0 step:477 [D loss: 0.514376, acc: 71.88%] [G loss: 3.616411]\n",
      "epoch:0 step:478 [D loss: 0.426276, acc: 82.81%] [G loss: 3.568873]\n",
      "epoch:0 step:479 [D loss: 0.615979, acc: 69.53%] [G loss: 3.570248]\n",
      "epoch:0 step:480 [D loss: 0.345771, acc: 89.06%] [G loss: 3.796980]\n",
      "epoch:0 step:481 [D loss: 0.524913, acc: 72.66%] [G loss: 4.178507]\n",
      "epoch:0 step:482 [D loss: 0.616108, acc: 65.62%] [G loss: 3.481911]\n",
      "epoch:0 step:483 [D loss: 0.483343, acc: 82.81%] [G loss: 3.443751]\n",
      "epoch:0 step:484 [D loss: 0.457489, acc: 81.25%] [G loss: 3.612176]\n",
      "epoch:0 step:485 [D loss: 0.472419, acc: 75.78%] [G loss: 3.670444]\n",
      "epoch:0 step:486 [D loss: 0.578342, acc: 65.62%] [G loss: 3.380194]\n",
      "epoch:0 step:487 [D loss: 0.442919, acc: 79.69%] [G loss: 3.438243]\n",
      "epoch:0 step:488 [D loss: 0.478840, acc: 78.91%] [G loss: 3.530930]\n",
      "epoch:0 step:489 [D loss: 0.522407, acc: 71.88%] [G loss: 3.333243]\n",
      "epoch:0 step:490 [D loss: 0.438849, acc: 82.03%] [G loss: 3.689014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:491 [D loss: 0.414751, acc: 83.59%] [G loss: 3.616984]\n",
      "epoch:0 step:492 [D loss: 0.512287, acc: 75.78%] [G loss: 3.738382]\n",
      "epoch:0 step:493 [D loss: 0.482781, acc: 80.47%] [G loss: 3.613414]\n",
      "epoch:0 step:494 [D loss: 0.349406, acc: 88.28%] [G loss: 4.005427]\n",
      "epoch:0 step:495 [D loss: 0.568747, acc: 70.31%] [G loss: 3.605458]\n",
      "epoch:0 step:496 [D loss: 0.617293, acc: 71.09%] [G loss: 3.584614]\n",
      "epoch:0 step:497 [D loss: 0.424474, acc: 79.69%] [G loss: 3.606283]\n",
      "epoch:0 step:498 [D loss: 0.332892, acc: 88.28%] [G loss: 3.831781]\n",
      "epoch:0 step:499 [D loss: 0.417983, acc: 85.94%] [G loss: 3.956560]\n",
      "epoch:0 step:500 [D loss: 0.613947, acc: 64.84%] [G loss: 3.489562]\n",
      "epoch:0 step:501 [D loss: 0.467496, acc: 74.22%] [G loss: 3.295313]\n",
      "epoch:0 step:502 [D loss: 0.420886, acc: 84.38%] [G loss: 3.506037]\n",
      "epoch:0 step:503 [D loss: 0.540970, acc: 67.19%] [G loss: 3.420406]\n",
      "epoch:0 step:504 [D loss: 0.442748, acc: 79.69%] [G loss: 3.513201]\n",
      "epoch:0 step:505 [D loss: 0.398789, acc: 84.38%] [G loss: 3.502007]\n",
      "epoch:0 step:506 [D loss: 0.413112, acc: 81.25%] [G loss: 3.614124]\n",
      "epoch:0 step:507 [D loss: 0.443565, acc: 82.81%] [G loss: 3.592761]\n",
      "epoch:0 step:508 [D loss: 0.398143, acc: 82.81%] [G loss: 3.797984]\n",
      "epoch:0 step:509 [D loss: 0.564734, acc: 65.62%] [G loss: 3.823349]\n",
      "epoch:0 step:510 [D loss: 0.544137, acc: 70.31%] [G loss: 3.233285]\n",
      "epoch:0 step:511 [D loss: 0.419098, acc: 82.03%] [G loss: 3.406540]\n",
      "epoch:0 step:512 [D loss: 0.494686, acc: 77.34%] [G loss: 3.390968]\n",
      "epoch:0 step:513 [D loss: 0.467618, acc: 78.91%] [G loss: 3.523065]\n",
      "epoch:0 step:514 [D loss: 0.535286, acc: 69.53%] [G loss: 3.444847]\n",
      "epoch:0 step:515 [D loss: 0.435579, acc: 81.25%] [G loss: 3.623829]\n",
      "epoch:0 step:516 [D loss: 0.589956, acc: 73.44%] [G loss: 3.549886]\n",
      "epoch:0 step:517 [D loss: 0.662777, acc: 64.06%] [G loss: 3.541497]\n",
      "epoch:0 step:518 [D loss: 0.437157, acc: 78.91%] [G loss: 3.622813]\n",
      "epoch:0 step:519 [D loss: 0.427598, acc: 80.47%] [G loss: 3.488468]\n",
      "epoch:0 step:520 [D loss: 0.496018, acc: 75.78%] [G loss: 3.382433]\n",
      "epoch:0 step:521 [D loss: 0.474558, acc: 78.91%] [G loss: 3.402416]\n",
      "epoch:0 step:522 [D loss: 0.468585, acc: 79.69%] [G loss: 3.685694]\n",
      "epoch:0 step:523 [D loss: 0.452207, acc: 78.12%] [G loss: 3.557762]\n",
      "epoch:0 step:524 [D loss: 0.439249, acc: 83.59%] [G loss: 3.506448]\n",
      "epoch:0 step:525 [D loss: 0.508449, acc: 77.34%] [G loss: 3.567649]\n",
      "epoch:0 step:526 [D loss: 0.427993, acc: 77.34%] [G loss: 3.335335]\n",
      "epoch:0 step:527 [D loss: 0.575047, acc: 73.44%] [G loss: 3.841251]\n",
      "epoch:0 step:528 [D loss: 0.395055, acc: 82.81%] [G loss: 3.975315]\n",
      "epoch:0 step:529 [D loss: 0.420467, acc: 82.03%] [G loss: 3.736476]\n",
      "epoch:0 step:530 [D loss: 0.369210, acc: 85.94%] [G loss: 4.044747]\n",
      "epoch:0 step:531 [D loss: 0.717140, acc: 58.59%] [G loss: 3.703768]\n",
      "epoch:0 step:532 [D loss: 0.532689, acc: 70.31%] [G loss: 3.298680]\n",
      "epoch:0 step:533 [D loss: 0.657218, acc: 59.38%] [G loss: 3.574402]\n",
      "epoch:0 step:534 [D loss: 0.419851, acc: 81.25%] [G loss: 3.263042]\n",
      "epoch:0 step:535 [D loss: 0.637619, acc: 64.84%] [G loss: 2.922192]\n",
      "epoch:0 step:536 [D loss: 0.442578, acc: 79.69%] [G loss: 3.408637]\n",
      "epoch:0 step:537 [D loss: 0.383333, acc: 86.72%] [G loss: 3.622499]\n",
      "epoch:0 step:538 [D loss: 0.473550, acc: 77.34%] [G loss: 3.512834]\n",
      "epoch:0 step:539 [D loss: 0.515851, acc: 75.78%] [G loss: 3.238596]\n",
      "epoch:0 step:540 [D loss: 0.470930, acc: 76.56%] [G loss: 3.392642]\n",
      "epoch:0 step:541 [D loss: 0.484213, acc: 78.12%] [G loss: 3.423663]\n",
      "epoch:0 step:542 [D loss: 0.390215, acc: 85.94%] [G loss: 3.475213]\n",
      "epoch:0 step:543 [D loss: 0.472477, acc: 82.03%] [G loss: 3.362338]\n",
      "epoch:0 step:544 [D loss: 0.413982, acc: 81.25%] [G loss: 3.436446]\n",
      "epoch:0 step:545 [D loss: 0.453685, acc: 78.12%] [G loss: 3.668733]\n",
      "epoch:0 step:546 [D loss: 0.412265, acc: 78.91%] [G loss: 3.758032]\n",
      "epoch:0 step:547 [D loss: 0.551489, acc: 67.97%] [G loss: 3.954950]\n",
      "epoch:0 step:548 [D loss: 0.486707, acc: 75.00%] [G loss: 3.757256]\n",
      "epoch:0 step:549 [D loss: 0.468662, acc: 77.34%] [G loss: 3.379971]\n",
      "epoch:0 step:550 [D loss: 0.526088, acc: 75.78%] [G loss: 3.403431]\n",
      "epoch:0 step:551 [D loss: 0.382487, acc: 89.06%] [G loss: 3.493836]\n",
      "epoch:0 step:552 [D loss: 0.559589, acc: 71.09%] [G loss: 3.257602]\n",
      "epoch:0 step:553 [D loss: 0.544668, acc: 76.56%] [G loss: 3.237460]\n",
      "epoch:0 step:554 [D loss: 0.465450, acc: 79.69%] [G loss: 3.837905]\n",
      "epoch:0 step:555 [D loss: 0.534622, acc: 71.09%] [G loss: 3.259201]\n",
      "epoch:0 step:556 [D loss: 0.470674, acc: 75.00%] [G loss: 3.511882]\n",
      "epoch:0 step:557 [D loss: 0.553885, acc: 70.31%] [G loss: 3.179214]\n",
      "epoch:0 step:558 [D loss: 0.615877, acc: 69.53%] [G loss: 3.587343]\n",
      "epoch:0 step:559 [D loss: 0.618416, acc: 69.53%] [G loss: 3.070903]\n",
      "epoch:0 step:560 [D loss: 0.491132, acc: 73.44%] [G loss: 2.939636]\n",
      "epoch:0 step:561 [D loss: 0.499830, acc: 75.78%] [G loss: 2.901544]\n",
      "epoch:0 step:562 [D loss: 0.508732, acc: 76.56%] [G loss: 3.378519]\n",
      "epoch:0 step:563 [D loss: 0.501471, acc: 80.47%] [G loss: 3.324793]\n",
      "epoch:0 step:564 [D loss: 0.436568, acc: 80.47%] [G loss: 3.254596]\n",
      "epoch:0 step:565 [D loss: 0.663343, acc: 67.19%] [G loss: 3.131009]\n",
      "epoch:0 step:566 [D loss: 0.646890, acc: 61.72%] [G loss: 3.582271]\n",
      "epoch:0 step:567 [D loss: 0.464590, acc: 77.34%] [G loss: 3.463345]\n",
      "epoch:0 step:568 [D loss: 0.493734, acc: 76.56%] [G loss: 3.186898]\n",
      "epoch:0 step:569 [D loss: 0.621391, acc: 62.50%] [G loss: 2.947475]\n",
      "epoch:0 step:570 [D loss: 0.450543, acc: 78.91%] [G loss: 3.075012]\n",
      "epoch:0 step:571 [D loss: 0.544346, acc: 73.44%] [G loss: 3.255273]\n",
      "epoch:0 step:572 [D loss: 0.557320, acc: 71.09%] [G loss: 3.434443]\n",
      "epoch:0 step:573 [D loss: 0.405228, acc: 82.81%] [G loss: 3.558079]\n",
      "epoch:0 step:574 [D loss: 0.430964, acc: 78.12%] [G loss: 3.712816]\n",
      "epoch:0 step:575 [D loss: 0.396227, acc: 82.81%] [G loss: 3.757434]\n",
      "epoch:0 step:576 [D loss: 0.565521, acc: 69.53%] [G loss: 3.400131]\n",
      "epoch:0 step:577 [D loss: 0.442218, acc: 80.47%] [G loss: 2.892112]\n",
      "epoch:0 step:578 [D loss: 0.452306, acc: 78.12%] [G loss: 3.263550]\n",
      "epoch:0 step:579 [D loss: 0.465324, acc: 73.44%] [G loss: 3.193559]\n",
      "epoch:0 step:580 [D loss: 0.620628, acc: 65.62%] [G loss: 3.729303]\n",
      "epoch:0 step:581 [D loss: 0.377988, acc: 85.94%] [G loss: 3.576941]\n",
      "epoch:0 step:582 [D loss: 0.460106, acc: 73.44%] [G loss: 3.786045]\n",
      "epoch:0 step:583 [D loss: 0.502650, acc: 77.34%] [G loss: 3.162862]\n",
      "epoch:0 step:584 [D loss: 0.556008, acc: 72.66%] [G loss: 3.018774]\n",
      "epoch:0 step:585 [D loss: 0.473253, acc: 76.56%] [G loss: 3.053959]\n",
      "epoch:0 step:586 [D loss: 0.520007, acc: 73.44%] [G loss: 3.323312]\n",
      "epoch:0 step:587 [D loss: 0.625104, acc: 60.94%] [G loss: 2.887778]\n",
      "epoch:0 step:588 [D loss: 0.491588, acc: 75.00%] [G loss: 3.605123]\n",
      "epoch:0 step:589 [D loss: 0.481145, acc: 77.34%] [G loss: 3.235104]\n",
      "epoch:0 step:590 [D loss: 0.811559, acc: 53.91%] [G loss: 3.421023]\n",
      "epoch:0 step:591 [D loss: 0.503850, acc: 76.56%] [G loss: 2.951526]\n",
      "epoch:0 step:592 [D loss: 0.512200, acc: 71.88%] [G loss: 3.284544]\n",
      "epoch:0 step:593 [D loss: 0.691124, acc: 60.94%] [G loss: 3.060727]\n",
      "epoch:0 step:594 [D loss: 0.451932, acc: 80.47%] [G loss: 3.156074]\n",
      "epoch:0 step:595 [D loss: 0.481617, acc: 82.03%] [G loss: 3.163594]\n",
      "epoch:0 step:596 [D loss: 0.547977, acc: 73.44%] [G loss: 3.054505]\n",
      "epoch:0 step:597 [D loss: 0.548432, acc: 70.31%] [G loss: 3.273971]\n",
      "epoch:0 step:598 [D loss: 0.454714, acc: 82.03%] [G loss: 3.338862]\n",
      "epoch:0 step:599 [D loss: 0.472319, acc: 79.69%] [G loss: 3.205841]\n",
      "epoch:0 step:600 [D loss: 0.462676, acc: 76.56%] [G loss: 3.143322]\n",
      "epoch:0 step:601 [D loss: 0.503063, acc: 77.34%] [G loss: 3.211375]\n",
      "epoch:0 step:602 [D loss: 0.524234, acc: 74.22%] [G loss: 2.978672]\n",
      "epoch:0 step:603 [D loss: 0.539371, acc: 71.09%] [G loss: 3.107687]\n",
      "epoch:0 step:604 [D loss: 0.515351, acc: 75.78%] [G loss: 3.200149]\n",
      "epoch:0 step:605 [D loss: 0.514345, acc: 75.78%] [G loss: 3.458333]\n",
      "epoch:0 step:606 [D loss: 0.491799, acc: 74.22%] [G loss: 3.296167]\n",
      "epoch:0 step:607 [D loss: 0.522844, acc: 70.31%] [G loss: 3.112165]\n",
      "epoch:0 step:608 [D loss: 0.512962, acc: 76.56%] [G loss: 3.129013]\n",
      "epoch:0 step:609 [D loss: 0.519491, acc: 71.88%] [G loss: 3.339343]\n",
      "epoch:0 step:610 [D loss: 0.553424, acc: 71.88%] [G loss: 3.248074]\n",
      "epoch:0 step:611 [D loss: 0.487417, acc: 77.34%] [G loss: 3.363306]\n",
      "epoch:0 step:612 [D loss: 0.441997, acc: 80.47%] [G loss: 3.376246]\n",
      "epoch:0 step:613 [D loss: 0.512599, acc: 72.66%] [G loss: 3.273260]\n",
      "epoch:0 step:614 [D loss: 0.430483, acc: 82.03%] [G loss: 3.517663]\n",
      "epoch:0 step:615 [D loss: 0.527619, acc: 75.78%] [G loss: 3.334189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:616 [D loss: 0.438699, acc: 75.78%] [G loss: 3.594081]\n",
      "epoch:0 step:617 [D loss: 0.487908, acc: 74.22%] [G loss: 3.666053]\n",
      "epoch:0 step:618 [D loss: 0.436976, acc: 78.91%] [G loss: 3.904269]\n",
      "epoch:0 step:619 [D loss: 0.508070, acc: 75.00%] [G loss: 3.778494]\n",
      "epoch:0 step:620 [D loss: 0.456011, acc: 76.56%] [G loss: 3.417957]\n",
      "epoch:0 step:621 [D loss: 0.499214, acc: 77.34%] [G loss: 3.339365]\n",
      "epoch:0 step:622 [D loss: 0.510038, acc: 76.56%] [G loss: 3.280131]\n",
      "epoch:0 step:623 [D loss: 0.468642, acc: 78.91%] [G loss: 3.155060]\n",
      "epoch:0 step:624 [D loss: 0.555924, acc: 71.09%] [G loss: 3.409427]\n",
      "epoch:0 step:625 [D loss: 0.593578, acc: 73.44%] [G loss: 3.186448]\n",
      "epoch:0 step:626 [D loss: 0.553217, acc: 71.09%] [G loss: 3.104549]\n",
      "epoch:0 step:627 [D loss: 0.502432, acc: 78.12%] [G loss: 3.131732]\n",
      "epoch:0 step:628 [D loss: 0.452917, acc: 80.47%] [G loss: 3.021069]\n",
      "epoch:0 step:629 [D loss: 0.524461, acc: 70.31%] [G loss: 3.228392]\n",
      "epoch:0 step:630 [D loss: 0.447307, acc: 81.25%] [G loss: 3.051697]\n",
      "epoch:0 step:631 [D loss: 0.562543, acc: 72.66%] [G loss: 3.267590]\n",
      "epoch:0 step:632 [D loss: 0.529209, acc: 74.22%] [G loss: 3.302537]\n",
      "epoch:0 step:633 [D loss: 0.501087, acc: 73.44%] [G loss: 3.186828]\n",
      "epoch:0 step:634 [D loss: 0.448389, acc: 82.81%] [G loss: 3.361916]\n",
      "epoch:0 step:635 [D loss: 0.392409, acc: 81.25%] [G loss: 3.569760]\n",
      "epoch:0 step:636 [D loss: 0.612508, acc: 67.97%] [G loss: 2.992890]\n",
      "epoch:0 step:637 [D loss: 0.415329, acc: 82.03%] [G loss: 3.099536]\n",
      "epoch:0 step:638 [D loss: 0.520833, acc: 76.56%] [G loss: 3.091439]\n",
      "epoch:0 step:639 [D loss: 0.444164, acc: 85.16%] [G loss: 3.271237]\n",
      "epoch:0 step:640 [D loss: 0.480474, acc: 78.91%] [G loss: 3.488096]\n",
      "epoch:0 step:641 [D loss: 0.499059, acc: 78.91%] [G loss: 3.374617]\n",
      "epoch:0 step:642 [D loss: 0.420711, acc: 78.91%] [G loss: 3.344303]\n",
      "epoch:0 step:643 [D loss: 0.669421, acc: 66.41%] [G loss: 3.092623]\n",
      "epoch:0 step:644 [D loss: 0.573727, acc: 69.53%] [G loss: 3.225658]\n",
      "epoch:0 step:645 [D loss: 0.509035, acc: 76.56%] [G loss: 3.166870]\n",
      "epoch:0 step:646 [D loss: 0.587419, acc: 69.53%] [G loss: 3.485781]\n",
      "epoch:0 step:647 [D loss: 0.435721, acc: 82.03%] [G loss: 3.309767]\n",
      "epoch:0 step:648 [D loss: 0.698203, acc: 60.16%] [G loss: 3.403180]\n",
      "epoch:0 step:649 [D loss: 0.407109, acc: 82.81%] [G loss: 3.814366]\n",
      "epoch:0 step:650 [D loss: 0.474753, acc: 76.56%] [G loss: 3.526123]\n",
      "epoch:0 step:651 [D loss: 0.558821, acc: 72.66%] [G loss: 3.301311]\n",
      "epoch:0 step:652 [D loss: 0.679526, acc: 67.97%] [G loss: 2.874420]\n",
      "epoch:0 step:653 [D loss: 0.474561, acc: 79.69%] [G loss: 2.985096]\n",
      "epoch:0 step:654 [D loss: 0.600069, acc: 65.62%] [G loss: 3.299596]\n",
      "epoch:0 step:655 [D loss: 0.538977, acc: 72.66%] [G loss: 3.252210]\n",
      "epoch:0 step:656 [D loss: 0.478901, acc: 78.91%] [G loss: 3.089505]\n",
      "epoch:0 step:657 [D loss: 0.553363, acc: 72.66%] [G loss: 3.152169]\n",
      "epoch:0 step:658 [D loss: 0.514012, acc: 74.22%] [G loss: 3.111616]\n",
      "epoch:0 step:659 [D loss: 0.382344, acc: 85.94%] [G loss: 3.680216]\n",
      "epoch:0 step:660 [D loss: 0.443063, acc: 79.69%] [G loss: 3.448110]\n",
      "epoch:0 step:661 [D loss: 0.478980, acc: 75.78%] [G loss: 3.438986]\n",
      "epoch:0 step:662 [D loss: 0.607261, acc: 67.19%] [G loss: 3.041337]\n",
      "epoch:0 step:663 [D loss: 0.503120, acc: 75.00%] [G loss: 3.002266]\n",
      "epoch:0 step:664 [D loss: 0.541547, acc: 75.78%] [G loss: 3.440886]\n",
      "epoch:0 step:665 [D loss: 0.518598, acc: 74.22%] [G loss: 3.074434]\n",
      "epoch:0 step:666 [D loss: 0.581408, acc: 72.66%] [G loss: 3.115743]\n",
      "epoch:0 step:667 [D loss: 0.579131, acc: 71.88%] [G loss: 3.292955]\n",
      "epoch:0 step:668 [D loss: 0.496531, acc: 79.69%] [G loss: 3.155513]\n",
      "epoch:0 step:669 [D loss: 0.460232, acc: 78.12%] [G loss: 3.028062]\n",
      "epoch:0 step:670 [D loss: 0.532118, acc: 72.66%] [G loss: 3.104046]\n",
      "epoch:0 step:671 [D loss: 0.681107, acc: 61.72%] [G loss: 2.841618]\n",
      "epoch:0 step:672 [D loss: 0.593352, acc: 69.53%] [G loss: 2.814185]\n",
      "epoch:0 step:673 [D loss: 0.535017, acc: 69.53%] [G loss: 2.918501]\n",
      "epoch:0 step:674 [D loss: 0.546210, acc: 70.31%] [G loss: 2.920611]\n",
      "epoch:0 step:675 [D loss: 0.553396, acc: 69.53%] [G loss: 3.344632]\n",
      "epoch:0 step:676 [D loss: 0.688864, acc: 61.72%] [G loss: 2.979598]\n",
      "epoch:0 step:677 [D loss: 0.492317, acc: 77.34%] [G loss: 3.020719]\n",
      "epoch:0 step:678 [D loss: 0.568089, acc: 71.09%] [G loss: 2.898654]\n",
      "epoch:0 step:679 [D loss: 0.487173, acc: 75.78%] [G loss: 3.089751]\n",
      "epoch:0 step:680 [D loss: 0.571646, acc: 69.53%] [G loss: 3.155596]\n",
      "epoch:0 step:681 [D loss: 0.496878, acc: 75.78%] [G loss: 2.778243]\n",
      "epoch:0 step:682 [D loss: 0.611384, acc: 67.97%] [G loss: 2.702950]\n",
      "epoch:0 step:683 [D loss: 0.502145, acc: 77.34%] [G loss: 2.943971]\n",
      "epoch:0 step:684 [D loss: 0.606630, acc: 65.62%] [G loss: 3.107055]\n",
      "epoch:0 step:685 [D loss: 0.561302, acc: 71.88%] [G loss: 2.915635]\n",
      "epoch:0 step:686 [D loss: 0.600659, acc: 63.28%] [G loss: 2.851155]\n",
      "epoch:0 step:687 [D loss: 0.487297, acc: 73.44%] [G loss: 2.972380]\n",
      "epoch:0 step:688 [D loss: 0.518976, acc: 71.88%] [G loss: 3.065917]\n",
      "epoch:0 step:689 [D loss: 0.521406, acc: 70.31%] [G loss: 2.840770]\n",
      "epoch:0 step:690 [D loss: 0.530355, acc: 70.31%] [G loss: 3.056732]\n",
      "epoch:0 step:691 [D loss: 0.511793, acc: 74.22%] [G loss: 2.855548]\n",
      "epoch:0 step:692 [D loss: 0.540184, acc: 73.44%] [G loss: 3.087087]\n",
      "epoch:0 step:693 [D loss: 0.614318, acc: 67.19%] [G loss: 3.310923]\n",
      "epoch:0 step:694 [D loss: 0.453549, acc: 79.69%] [G loss: 3.054987]\n",
      "epoch:0 step:695 [D loss: 0.557953, acc: 71.09%] [G loss: 2.949168]\n",
      "epoch:0 step:696 [D loss: 0.554949, acc: 72.66%] [G loss: 3.285225]\n",
      "epoch:0 step:697 [D loss: 0.522605, acc: 73.44%] [G loss: 3.083670]\n",
      "epoch:0 step:698 [D loss: 0.467051, acc: 79.69%] [G loss: 2.863356]\n",
      "epoch:0 step:699 [D loss: 0.526446, acc: 75.00%] [G loss: 2.938081]\n",
      "epoch:0 step:700 [D loss: 0.492404, acc: 76.56%] [G loss: 3.183900]\n",
      "epoch:0 step:701 [D loss: 0.417735, acc: 84.38%] [G loss: 3.155156]\n",
      "epoch:0 step:702 [D loss: 0.669139, acc: 60.94%] [G loss: 2.943837]\n",
      "epoch:0 step:703 [D loss: 0.497743, acc: 75.78%] [G loss: 2.840817]\n",
      "epoch:0 step:704 [D loss: 0.484654, acc: 78.12%] [G loss: 3.288852]\n",
      "epoch:0 step:705 [D loss: 0.591242, acc: 67.97%] [G loss: 3.214586]\n",
      "epoch:0 step:706 [D loss: 0.497789, acc: 78.91%] [G loss: 3.188680]\n",
      "epoch:0 step:707 [D loss: 0.531021, acc: 71.09%] [G loss: 3.234359]\n",
      "epoch:0 step:708 [D loss: 0.468117, acc: 78.91%] [G loss: 3.314785]\n",
      "epoch:0 step:709 [D loss: 0.525671, acc: 71.88%] [G loss: 3.640872]\n",
      "epoch:0 step:710 [D loss: 0.678964, acc: 65.62%] [G loss: 2.770488]\n",
      "epoch:0 step:711 [D loss: 0.639506, acc: 65.62%] [G loss: 2.833861]\n",
      "epoch:0 step:712 [D loss: 0.534486, acc: 75.00%] [G loss: 2.639500]\n",
      "epoch:0 step:713 [D loss: 0.603613, acc: 66.41%] [G loss: 3.161890]\n",
      "epoch:0 step:714 [D loss: 0.560820, acc: 72.66%] [G loss: 2.955257]\n",
      "epoch:0 step:715 [D loss: 0.561170, acc: 70.31%] [G loss: 2.676564]\n",
      "epoch:0 step:716 [D loss: 0.609371, acc: 68.75%] [G loss: 2.526888]\n",
      "epoch:0 step:717 [D loss: 0.651546, acc: 63.28%] [G loss: 2.619997]\n",
      "epoch:0 step:718 [D loss: 0.562840, acc: 70.31%] [G loss: 2.714895]\n",
      "epoch:0 step:719 [D loss: 0.588220, acc: 71.88%] [G loss: 2.792643]\n",
      "epoch:0 step:720 [D loss: 0.555897, acc: 69.53%] [G loss: 3.037470]\n",
      "epoch:0 step:721 [D loss: 0.598703, acc: 67.19%] [G loss: 2.674385]\n",
      "epoch:0 step:722 [D loss: 0.573060, acc: 72.66%] [G loss: 2.789571]\n",
      "epoch:0 step:723 [D loss: 0.635321, acc: 66.41%] [G loss: 2.894950]\n",
      "epoch:0 step:724 [D loss: 0.546817, acc: 71.88%] [G loss: 2.824255]\n",
      "epoch:0 step:725 [D loss: 0.613973, acc: 64.06%] [G loss: 2.852997]\n",
      "epoch:0 step:726 [D loss: 0.517676, acc: 74.22%] [G loss: 2.714378]\n",
      "epoch:0 step:727 [D loss: 0.688480, acc: 59.38%] [G loss: 2.761834]\n",
      "epoch:0 step:728 [D loss: 0.552726, acc: 72.66%] [G loss: 2.680020]\n",
      "epoch:0 step:729 [D loss: 0.532706, acc: 68.75%] [G loss: 2.617319]\n",
      "epoch:0 step:730 [D loss: 0.565867, acc: 71.09%] [G loss: 2.948616]\n",
      "epoch:0 step:731 [D loss: 0.556304, acc: 68.75%] [G loss: 3.055890]\n",
      "epoch:0 step:732 [D loss: 0.624583, acc: 64.06%] [G loss: 3.019125]\n",
      "epoch:0 step:733 [D loss: 0.483343, acc: 78.91%] [G loss: 3.176621]\n",
      "epoch:0 step:734 [D loss: 0.551264, acc: 75.00%] [G loss: 3.094628]\n",
      "epoch:0 step:735 [D loss: 0.602876, acc: 63.28%] [G loss: 2.930683]\n",
      "epoch:0 step:736 [D loss: 0.514186, acc: 75.00%] [G loss: 2.501120]\n",
      "epoch:0 step:737 [D loss: 0.541074, acc: 76.56%] [G loss: 2.961608]\n",
      "epoch:0 step:738 [D loss: 0.617411, acc: 64.84%] [G loss: 2.642980]\n",
      "epoch:0 step:739 [D loss: 0.714245, acc: 57.81%] [G loss: 2.683713]\n",
      "epoch:0 step:740 [D loss: 0.459162, acc: 79.69%] [G loss: 2.836354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:741 [D loss: 0.626371, acc: 64.06%] [G loss: 2.843239]\n",
      "epoch:0 step:742 [D loss: 0.583381, acc: 69.53%] [G loss: 2.676835]\n",
      "epoch:0 step:743 [D loss: 0.497243, acc: 78.91%] [G loss: 2.732053]\n",
      "epoch:0 step:744 [D loss: 0.629176, acc: 64.84%] [G loss: 2.834566]\n",
      "epoch:0 step:745 [D loss: 0.549559, acc: 70.31%] [G loss: 3.141126]\n",
      "epoch:0 step:746 [D loss: 0.523416, acc: 75.00%] [G loss: 3.070225]\n",
      "epoch:0 step:747 [D loss: 0.462766, acc: 77.34%] [G loss: 3.114693]\n",
      "epoch:0 step:748 [D loss: 0.676273, acc: 64.06%] [G loss: 2.622766]\n",
      "epoch:0 step:749 [D loss: 0.521944, acc: 76.56%] [G loss: 2.931873]\n",
      "epoch:0 step:750 [D loss: 0.610804, acc: 65.62%] [G loss: 2.894654]\n",
      "epoch:0 step:751 [D loss: 0.545506, acc: 70.31%] [G loss: 2.739794]\n",
      "epoch:0 step:752 [D loss: 0.588698, acc: 72.66%] [G loss: 2.920513]\n",
      "epoch:0 step:753 [D loss: 0.603298, acc: 69.53%] [G loss: 2.797722]\n",
      "epoch:0 step:754 [D loss: 0.571820, acc: 73.44%] [G loss: 2.879014]\n",
      "epoch:0 step:755 [D loss: 0.552582, acc: 68.75%] [G loss: 2.936714]\n",
      "epoch:0 step:756 [D loss: 0.608246, acc: 64.84%] [G loss: 2.711892]\n",
      "epoch:0 step:757 [D loss: 0.554593, acc: 74.22%] [G loss: 2.888121]\n",
      "epoch:0 step:758 [D loss: 0.676316, acc: 57.81%] [G loss: 2.636226]\n",
      "epoch:0 step:759 [D loss: 0.575335, acc: 67.19%] [G loss: 2.675655]\n",
      "epoch:0 step:760 [D loss: 0.511108, acc: 75.00%] [G loss: 2.520437]\n",
      "epoch:0 step:761 [D loss: 0.551990, acc: 72.66%] [G loss: 2.639892]\n",
      "epoch:0 step:762 [D loss: 0.500539, acc: 76.56%] [G loss: 2.988760]\n",
      "epoch:0 step:763 [D loss: 0.477432, acc: 75.78%] [G loss: 2.788727]\n",
      "epoch:0 step:764 [D loss: 0.529239, acc: 72.66%] [G loss: 2.725613]\n",
      "epoch:0 step:765 [D loss: 0.677583, acc: 64.06%] [G loss: 2.581003]\n",
      "epoch:0 step:766 [D loss: 0.544884, acc: 71.09%] [G loss: 2.432032]\n",
      "epoch:0 step:767 [D loss: 0.606022, acc: 71.09%] [G loss: 2.584853]\n",
      "epoch:0 step:768 [D loss: 0.659663, acc: 64.84%] [G loss: 3.029658]\n",
      "epoch:0 step:769 [D loss: 0.610987, acc: 64.84%] [G loss: 2.898193]\n",
      "epoch:0 step:770 [D loss: 0.658943, acc: 63.28%] [G loss: 2.461916]\n",
      "epoch:0 step:771 [D loss: 0.635264, acc: 62.50%] [G loss: 2.559973]\n",
      "epoch:0 step:772 [D loss: 0.547849, acc: 71.88%] [G loss: 2.643228]\n",
      "epoch:0 step:773 [D loss: 0.563826, acc: 70.31%] [G loss: 2.808231]\n",
      "epoch:0 step:774 [D loss: 0.550494, acc: 77.34%] [G loss: 2.941696]\n",
      "epoch:0 step:775 [D loss: 0.576969, acc: 71.09%] [G loss: 2.681482]\n",
      "epoch:0 step:776 [D loss: 0.575328, acc: 67.19%] [G loss: 2.532401]\n",
      "epoch:0 step:777 [D loss: 0.541425, acc: 74.22%] [G loss: 2.764518]\n",
      "epoch:0 step:778 [D loss: 0.719179, acc: 57.03%] [G loss: 2.560497]\n",
      "epoch:0 step:779 [D loss: 0.639529, acc: 67.97%] [G loss: 2.758475]\n",
      "epoch:0 step:780 [D loss: 0.577081, acc: 69.53%] [G loss: 2.783812]\n",
      "epoch:0 step:781 [D loss: 0.558187, acc: 71.88%] [G loss: 2.753542]\n",
      "epoch:0 step:782 [D loss: 0.542516, acc: 70.31%] [G loss: 3.123919]\n",
      "epoch:0 step:783 [D loss: 0.558662, acc: 72.66%] [G loss: 2.755666]\n",
      "epoch:0 step:784 [D loss: 0.611943, acc: 64.84%] [G loss: 2.695148]\n",
      "epoch:0 step:785 [D loss: 0.587570, acc: 66.41%] [G loss: 2.603176]\n",
      "epoch:0 step:786 [D loss: 0.548723, acc: 72.66%] [G loss: 2.635050]\n",
      "epoch:0 step:787 [D loss: 0.606683, acc: 70.31%] [G loss: 2.632465]\n",
      "epoch:0 step:788 [D loss: 0.720592, acc: 55.47%] [G loss: 2.485796]\n",
      "epoch:0 step:789 [D loss: 0.472559, acc: 79.69%] [G loss: 2.757623]\n",
      "epoch:0 step:790 [D loss: 0.606948, acc: 65.62%] [G loss: 2.606295]\n",
      "epoch:0 step:791 [D loss: 0.548474, acc: 75.00%] [G loss: 3.124529]\n",
      "epoch:0 step:792 [D loss: 0.512010, acc: 71.88%] [G loss: 3.166365]\n",
      "epoch:0 step:793 [D loss: 0.559323, acc: 69.53%] [G loss: 2.734325]\n",
      "epoch:0 step:794 [D loss: 0.661321, acc: 59.38%] [G loss: 2.672088]\n",
      "epoch:0 step:795 [D loss: 0.544746, acc: 74.22%] [G loss: 3.066822]\n",
      "epoch:0 step:796 [D loss: 0.560752, acc: 72.66%] [G loss: 2.914839]\n",
      "epoch:0 step:797 [D loss: 0.625392, acc: 60.94%] [G loss: 2.756366]\n",
      "epoch:0 step:798 [D loss: 0.493831, acc: 83.59%] [G loss: 2.764690]\n",
      "epoch:0 step:799 [D loss: 0.536680, acc: 71.09%] [G loss: 2.697263]\n",
      "epoch:0 step:800 [D loss: 0.539618, acc: 72.66%] [G loss: 2.686094]\n",
      "epoch:0 step:801 [D loss: 0.534210, acc: 77.34%] [G loss: 2.862081]\n",
      "epoch:0 step:802 [D loss: 0.503451, acc: 73.44%] [G loss: 2.857382]\n",
      "epoch:0 step:803 [D loss: 0.691279, acc: 55.47%] [G loss: 2.895137]\n",
      "epoch:0 step:804 [D loss: 0.539990, acc: 75.00%] [G loss: 2.720642]\n",
      "epoch:0 step:805 [D loss: 0.524338, acc: 76.56%] [G loss: 2.843830]\n",
      "epoch:0 step:806 [D loss: 0.515538, acc: 75.00%] [G loss: 2.997263]\n",
      "epoch:0 step:807 [D loss: 0.560737, acc: 67.19%] [G loss: 3.006528]\n",
      "epoch:0 step:808 [D loss: 0.629332, acc: 64.06%] [G loss: 2.870585]\n",
      "epoch:0 step:809 [D loss: 0.605388, acc: 68.75%] [G loss: 2.705394]\n",
      "epoch:0 step:810 [D loss: 0.560267, acc: 68.75%] [G loss: 2.753637]\n",
      "epoch:0 step:811 [D loss: 0.580283, acc: 69.53%] [G loss: 2.523097]\n",
      "epoch:0 step:812 [D loss: 0.623933, acc: 66.41%] [G loss: 2.702924]\n",
      "epoch:0 step:813 [D loss: 0.668939, acc: 61.72%] [G loss: 2.749778]\n",
      "epoch:0 step:814 [D loss: 0.579207, acc: 72.66%] [G loss: 2.670100]\n",
      "epoch:0 step:815 [D loss: 0.511198, acc: 77.34%] [G loss: 2.966466]\n",
      "epoch:0 step:816 [D loss: 0.600330, acc: 64.06%] [G loss: 2.754654]\n",
      "epoch:0 step:817 [D loss: 0.508260, acc: 75.78%] [G loss: 2.662650]\n",
      "epoch:0 step:818 [D loss: 0.526650, acc: 74.22%] [G loss: 2.727657]\n",
      "epoch:0 step:819 [D loss: 0.638576, acc: 63.28%] [G loss: 2.939004]\n",
      "epoch:0 step:820 [D loss: 0.570714, acc: 71.88%] [G loss: 2.529726]\n",
      "epoch:0 step:821 [D loss: 0.651189, acc: 64.06%] [G loss: 2.768996]\n",
      "epoch:0 step:822 [D loss: 0.459140, acc: 78.91%] [G loss: 2.926174]\n",
      "epoch:0 step:823 [D loss: 0.513040, acc: 73.44%] [G loss: 3.027986]\n",
      "epoch:0 step:824 [D loss: 0.608021, acc: 69.53%] [G loss: 2.788887]\n",
      "epoch:0 step:825 [D loss: 0.480074, acc: 78.12%] [G loss: 2.818810]\n",
      "epoch:0 step:826 [D loss: 0.580196, acc: 63.28%] [G loss: 2.826209]\n",
      "epoch:0 step:827 [D loss: 0.615567, acc: 69.53%] [G loss: 2.526268]\n",
      "epoch:0 step:828 [D loss: 0.578149, acc: 72.66%] [G loss: 2.886419]\n",
      "epoch:0 step:829 [D loss: 0.534836, acc: 70.31%] [G loss: 2.856680]\n",
      "epoch:0 step:830 [D loss: 0.532448, acc: 76.56%] [G loss: 2.827021]\n",
      "epoch:0 step:831 [D loss: 0.585546, acc: 69.53%] [G loss: 2.757062]\n",
      "epoch:0 step:832 [D loss: 0.518893, acc: 77.34%] [G loss: 3.035173]\n",
      "epoch:0 step:833 [D loss: 0.491071, acc: 74.22%] [G loss: 2.871559]\n",
      "epoch:0 step:834 [D loss: 0.526210, acc: 76.56%] [G loss: 2.628098]\n",
      "epoch:0 step:835 [D loss: 0.578708, acc: 69.53%] [G loss: 3.061348]\n",
      "epoch:0 step:836 [D loss: 0.551978, acc: 72.66%] [G loss: 2.805120]\n",
      "epoch:0 step:837 [D loss: 0.611363, acc: 66.41%] [G loss: 2.708395]\n",
      "epoch:0 step:838 [D loss: 0.528474, acc: 75.78%] [G loss: 2.917760]\n",
      "epoch:0 step:839 [D loss: 0.581399, acc: 71.09%] [G loss: 2.758354]\n",
      "epoch:0 step:840 [D loss: 0.562882, acc: 72.66%] [G loss: 2.794195]\n",
      "epoch:0 step:841 [D loss: 0.494787, acc: 76.56%] [G loss: 2.952095]\n",
      "epoch:0 step:842 [D loss: 0.481594, acc: 77.34%] [G loss: 2.857662]\n",
      "epoch:0 step:843 [D loss: 0.591558, acc: 68.75%] [G loss: 3.064733]\n",
      "epoch:0 step:844 [D loss: 0.626560, acc: 65.62%] [G loss: 2.710125]\n",
      "epoch:0 step:845 [D loss: 0.618009, acc: 67.19%] [G loss: 2.681898]\n",
      "epoch:0 step:846 [D loss: 0.520744, acc: 78.12%] [G loss: 2.546206]\n",
      "epoch:0 step:847 [D loss: 0.632939, acc: 67.97%] [G loss: 2.781880]\n",
      "epoch:0 step:848 [D loss: 0.560347, acc: 71.09%] [G loss: 2.933124]\n",
      "epoch:0 step:849 [D loss: 0.615691, acc: 64.06%] [G loss: 2.739928]\n",
      "epoch:0 step:850 [D loss: 0.577812, acc: 67.19%] [G loss: 2.871359]\n",
      "epoch:0 step:851 [D loss: 0.563953, acc: 71.09%] [G loss: 2.872906]\n",
      "epoch:0 step:852 [D loss: 0.471519, acc: 79.69%] [G loss: 2.926386]\n",
      "epoch:0 step:853 [D loss: 0.545019, acc: 71.09%] [G loss: 2.811552]\n",
      "epoch:0 step:854 [D loss: 0.564658, acc: 71.09%] [G loss: 2.696905]\n",
      "epoch:0 step:855 [D loss: 0.523232, acc: 71.88%] [G loss: 3.011005]\n",
      "epoch:0 step:856 [D loss: 0.570311, acc: 68.75%] [G loss: 2.777800]\n",
      "epoch:0 step:857 [D loss: 0.539736, acc: 71.88%] [G loss: 2.978308]\n",
      "epoch:0 step:858 [D loss: 0.637368, acc: 64.84%] [G loss: 2.833524]\n",
      "epoch:0 step:859 [D loss: 0.628448, acc: 64.06%] [G loss: 2.935532]\n",
      "epoch:0 step:860 [D loss: 0.698172, acc: 62.50%] [G loss: 3.004016]\n",
      "epoch:0 step:861 [D loss: 0.617740, acc: 70.31%] [G loss: 2.677439]\n",
      "epoch:0 step:862 [D loss: 0.589157, acc: 67.97%] [G loss: 2.709613]\n",
      "epoch:0 step:863 [D loss: 0.701591, acc: 63.28%] [G loss: 2.806144]\n",
      "epoch:0 step:864 [D loss: 0.537167, acc: 73.44%] [G loss: 2.710277]\n",
      "epoch:0 step:865 [D loss: 0.549927, acc: 70.31%] [G loss: 2.633406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:866 [D loss: 0.509193, acc: 78.91%] [G loss: 2.749659]\n",
      "epoch:0 step:867 [D loss: 0.532241, acc: 76.56%] [G loss: 2.789477]\n",
      "epoch:0 step:868 [D loss: 0.548302, acc: 70.31%] [G loss: 2.748517]\n",
      "epoch:0 step:869 [D loss: 0.618652, acc: 64.84%] [G loss: 3.022816]\n",
      "epoch:0 step:870 [D loss: 0.514632, acc: 78.91%] [G loss: 3.154293]\n",
      "epoch:0 step:871 [D loss: 0.659071, acc: 61.72%] [G loss: 2.934079]\n",
      "epoch:0 step:872 [D loss: 0.512352, acc: 73.44%] [G loss: 2.770502]\n",
      "epoch:0 step:873 [D loss: 0.525407, acc: 74.22%] [G loss: 2.838798]\n",
      "epoch:0 step:874 [D loss: 0.576751, acc: 71.09%] [G loss: 2.633346]\n",
      "epoch:0 step:875 [D loss: 0.584627, acc: 69.53%] [G loss: 2.716398]\n",
      "epoch:0 step:876 [D loss: 0.498998, acc: 75.00%] [G loss: 2.745372]\n",
      "epoch:0 step:877 [D loss: 0.547346, acc: 68.75%] [G loss: 2.607690]\n",
      "epoch:0 step:878 [D loss: 0.655064, acc: 60.16%] [G loss: 2.486267]\n",
      "epoch:0 step:879 [D loss: 0.585784, acc: 69.53%] [G loss: 2.544528]\n",
      "epoch:0 step:880 [D loss: 0.542601, acc: 73.44%] [G loss: 2.699004]\n",
      "epoch:0 step:881 [D loss: 0.540573, acc: 72.66%] [G loss: 2.589433]\n",
      "epoch:0 step:882 [D loss: 0.565350, acc: 73.44%] [G loss: 2.806663]\n",
      "epoch:0 step:883 [D loss: 0.592048, acc: 72.66%] [G loss: 2.567759]\n",
      "epoch:0 step:884 [D loss: 0.573791, acc: 71.09%] [G loss: 2.766226]\n",
      "epoch:0 step:885 [D loss: 0.522050, acc: 71.88%] [G loss: 2.904288]\n",
      "epoch:0 step:886 [D loss: 0.470255, acc: 77.34%] [G loss: 3.166237]\n",
      "epoch:0 step:887 [D loss: 0.571778, acc: 66.41%] [G loss: 3.065129]\n",
      "epoch:0 step:888 [D loss: 0.686656, acc: 67.19%] [G loss: 2.831053]\n",
      "epoch:0 step:889 [D loss: 0.463136, acc: 83.59%] [G loss: 2.973799]\n",
      "epoch:0 step:890 [D loss: 0.532559, acc: 71.88%] [G loss: 2.985827]\n",
      "epoch:0 step:891 [D loss: 0.637738, acc: 64.06%] [G loss: 2.520247]\n",
      "epoch:0 step:892 [D loss: 0.616194, acc: 64.84%] [G loss: 2.751951]\n",
      "epoch:0 step:893 [D loss: 0.577794, acc: 67.97%] [G loss: 2.681408]\n",
      "epoch:0 step:894 [D loss: 0.644218, acc: 69.53%] [G loss: 2.894738]\n",
      "epoch:0 step:895 [D loss: 0.567335, acc: 71.09%] [G loss: 2.660541]\n",
      "epoch:0 step:896 [D loss: 0.662681, acc: 63.28%] [G loss: 2.433582]\n",
      "epoch:0 step:897 [D loss: 0.560575, acc: 69.53%] [G loss: 2.528571]\n",
      "epoch:0 step:898 [D loss: 0.618337, acc: 69.53%] [G loss: 2.787904]\n",
      "epoch:0 step:899 [D loss: 0.593581, acc: 66.41%] [G loss: 2.767540]\n",
      "epoch:0 step:900 [D loss: 0.591811, acc: 69.53%] [G loss: 2.932436]\n",
      "epoch:0 step:901 [D loss: 0.578208, acc: 65.62%] [G loss: 2.679528]\n",
      "epoch:0 step:902 [D loss: 0.598685, acc: 64.06%] [G loss: 2.622070]\n",
      "epoch:0 step:903 [D loss: 0.584275, acc: 66.41%] [G loss: 2.797905]\n",
      "epoch:0 step:904 [D loss: 0.604220, acc: 64.06%] [G loss: 2.766723]\n",
      "epoch:0 step:905 [D loss: 0.599651, acc: 67.97%] [G loss: 2.838132]\n",
      "epoch:0 step:906 [D loss: 0.639958, acc: 64.84%] [G loss: 2.652972]\n",
      "epoch:0 step:907 [D loss: 0.610707, acc: 65.62%] [G loss: 2.624870]\n",
      "epoch:0 step:908 [D loss: 0.481881, acc: 75.78%] [G loss: 2.736094]\n",
      "epoch:0 step:909 [D loss: 0.644776, acc: 60.16%] [G loss: 2.704806]\n",
      "epoch:0 step:910 [D loss: 0.550352, acc: 71.09%] [G loss: 2.862329]\n",
      "epoch:0 step:911 [D loss: 0.558758, acc: 68.75%] [G loss: 2.584399]\n",
      "epoch:0 step:912 [D loss: 0.706312, acc: 58.59%] [G loss: 2.879215]\n",
      "epoch:0 step:913 [D loss: 0.581736, acc: 75.00%] [G loss: 3.122696]\n",
      "epoch:0 step:914 [D loss: 0.403133, acc: 82.03%] [G loss: 3.018178]\n",
      "epoch:0 step:915 [D loss: 0.564594, acc: 68.75%] [G loss: 2.863948]\n",
      "epoch:0 step:916 [D loss: 0.553124, acc: 73.44%] [G loss: 2.861183]\n",
      "epoch:0 step:917 [D loss: 0.542027, acc: 75.00%] [G loss: 2.884227]\n",
      "epoch:0 step:918 [D loss: 0.473090, acc: 77.34%] [G loss: 2.938955]\n",
      "epoch:0 step:919 [D loss: 0.614256, acc: 65.62%] [G loss: 2.737813]\n",
      "epoch:0 step:920 [D loss: 0.851461, acc: 42.97%] [G loss: 2.723922]\n",
      "epoch:0 step:921 [D loss: 0.496271, acc: 75.00%] [G loss: 2.711905]\n",
      "epoch:0 step:922 [D loss: 0.660963, acc: 64.06%] [G loss: 2.951871]\n",
      "epoch:0 step:923 [D loss: 0.470830, acc: 79.69%] [G loss: 2.948445]\n",
      "epoch:0 step:924 [D loss: 0.459216, acc: 81.25%] [G loss: 3.225823]\n",
      "epoch:0 step:925 [D loss: 0.451708, acc: 76.56%] [G loss: 3.125881]\n",
      "epoch:0 step:926 [D loss: 0.442545, acc: 80.47%] [G loss: 3.164025]\n",
      "epoch:0 step:927 [D loss: 0.502229, acc: 79.69%] [G loss: 3.014190]\n",
      "epoch:0 step:928 [D loss: 0.844613, acc: 60.94%] [G loss: 3.385520]\n",
      "epoch:0 step:929 [D loss: 0.417263, acc: 80.47%] [G loss: 4.277880]\n",
      "epoch:0 step:930 [D loss: 0.576555, acc: 71.88%] [G loss: 3.317442]\n",
      "epoch:0 step:931 [D loss: 0.739727, acc: 60.16%] [G loss: 2.767056]\n",
      "epoch:0 step:932 [D loss: 0.683197, acc: 59.38%] [G loss: 2.600632]\n",
      "epoch:0 step:933 [D loss: 0.615978, acc: 65.62%] [G loss: 3.099639]\n",
      "epoch:0 step:934 [D loss: 0.560395, acc: 64.84%] [G loss: 2.844291]\n",
      "epoch:0 step:935 [D loss: 0.451820, acc: 75.78%] [G loss: 3.140623]\n",
      "epoch:0 step:936 [D loss: 0.445182, acc: 78.91%] [G loss: 3.309075]\n",
      "epoch:0 step:937 [D loss: 0.754164, acc: 55.47%] [G loss: 3.269830]\n",
      "epoch:1 step:938 [D loss: 0.655567, acc: 64.06%] [G loss: 2.711483]\n",
      "epoch:1 step:939 [D loss: 0.674352, acc: 60.94%] [G loss: 2.711880]\n",
      "epoch:1 step:940 [D loss: 0.652125, acc: 59.38%] [G loss: 2.498493]\n",
      "epoch:1 step:941 [D loss: 0.630978, acc: 67.19%] [G loss: 2.644848]\n",
      "epoch:1 step:942 [D loss: 0.637722, acc: 64.06%] [G loss: 2.647398]\n",
      "epoch:1 step:943 [D loss: 0.642636, acc: 60.94%] [G loss: 2.664821]\n",
      "epoch:1 step:944 [D loss: 0.568190, acc: 73.44%] [G loss: 2.559331]\n",
      "epoch:1 step:945 [D loss: 0.600632, acc: 71.88%] [G loss: 2.830537]\n",
      "epoch:1 step:946 [D loss: 0.600743, acc: 73.44%] [G loss: 2.487158]\n",
      "epoch:1 step:947 [D loss: 0.549629, acc: 71.88%] [G loss: 2.516977]\n",
      "epoch:1 step:948 [D loss: 0.543361, acc: 72.66%] [G loss: 2.910931]\n",
      "epoch:1 step:949 [D loss: 0.492019, acc: 76.56%] [G loss: 2.877855]\n",
      "epoch:1 step:950 [D loss: 0.608182, acc: 64.06%] [G loss: 2.585978]\n",
      "epoch:1 step:951 [D loss: 0.470570, acc: 76.56%] [G loss: 2.951217]\n",
      "epoch:1 step:952 [D loss: 0.599779, acc: 64.84%] [G loss: 2.895823]\n",
      "epoch:1 step:953 [D loss: 0.596415, acc: 67.97%] [G loss: 2.621053]\n",
      "epoch:1 step:954 [D loss: 0.632628, acc: 62.50%] [G loss: 2.525195]\n",
      "epoch:1 step:955 [D loss: 0.597628, acc: 66.41%] [G loss: 2.498254]\n",
      "epoch:1 step:956 [D loss: 0.620891, acc: 64.84%] [G loss: 2.386538]\n",
      "epoch:1 step:957 [D loss: 0.636912, acc: 62.50%] [G loss: 2.520353]\n",
      "epoch:1 step:958 [D loss: 0.488882, acc: 78.91%] [G loss: 2.706199]\n",
      "epoch:1 step:959 [D loss: 0.551573, acc: 70.31%] [G loss: 2.703483]\n",
      "epoch:1 step:960 [D loss: 0.564885, acc: 70.31%] [G loss: 2.703411]\n",
      "epoch:1 step:961 [D loss: 0.728142, acc: 57.81%] [G loss: 2.501541]\n",
      "epoch:1 step:962 [D loss: 0.533439, acc: 68.75%] [G loss: 2.751087]\n",
      "epoch:1 step:963 [D loss: 0.591094, acc: 71.88%] [G loss: 2.628421]\n",
      "epoch:1 step:964 [D loss: 0.590117, acc: 68.75%] [G loss: 2.490033]\n",
      "epoch:1 step:965 [D loss: 0.545615, acc: 67.97%] [G loss: 2.618754]\n",
      "epoch:1 step:966 [D loss: 0.581455, acc: 69.53%] [G loss: 2.542839]\n",
      "epoch:1 step:967 [D loss: 0.594905, acc: 60.16%] [G loss: 2.550606]\n",
      "epoch:1 step:968 [D loss: 0.554983, acc: 73.44%] [G loss: 2.530899]\n",
      "epoch:1 step:969 [D loss: 0.514548, acc: 73.44%] [G loss: 2.756432]\n",
      "epoch:1 step:970 [D loss: 0.539428, acc: 70.31%] [G loss: 2.693807]\n",
      "epoch:1 step:971 [D loss: 0.517824, acc: 74.22%] [G loss: 2.778491]\n",
      "epoch:1 step:972 [D loss: 0.556330, acc: 68.75%] [G loss: 2.855211]\n",
      "epoch:1 step:973 [D loss: 0.462579, acc: 78.12%] [G loss: 2.785317]\n",
      "epoch:1 step:974 [D loss: 0.664634, acc: 64.06%] [G loss: 2.955821]\n",
      "epoch:1 step:975 [D loss: 0.612304, acc: 65.62%] [G loss: 2.888710]\n",
      "epoch:1 step:976 [D loss: 0.500059, acc: 81.25%] [G loss: 2.623910]\n",
      "epoch:1 step:977 [D loss: 0.592675, acc: 68.75%] [G loss: 2.804570]\n",
      "epoch:1 step:978 [D loss: 0.573388, acc: 72.66%] [G loss: 2.613827]\n",
      "epoch:1 step:979 [D loss: 0.558107, acc: 68.75%] [G loss: 3.042098]\n",
      "epoch:1 step:980 [D loss: 0.549717, acc: 71.88%] [G loss: 2.936368]\n",
      "epoch:1 step:981 [D loss: 0.529591, acc: 72.66%] [G loss: 2.677842]\n",
      "epoch:1 step:982 [D loss: 0.537625, acc: 75.00%] [G loss: 2.832315]\n",
      "epoch:1 step:983 [D loss: 0.506549, acc: 73.44%] [G loss: 2.842339]\n",
      "epoch:1 step:984 [D loss: 0.620087, acc: 66.41%] [G loss: 2.928930]\n",
      "epoch:1 step:985 [D loss: 0.661135, acc: 61.72%] [G loss: 2.821394]\n",
      "epoch:1 step:986 [D loss: 0.603887, acc: 67.97%] [G loss: 2.755282]\n",
      "epoch:1 step:987 [D loss: 0.504183, acc: 78.12%] [G loss: 2.788700]\n",
      "epoch:1 step:988 [D loss: 0.558031, acc: 71.88%] [G loss: 2.724756]\n",
      "epoch:1 step:989 [D loss: 0.607074, acc: 64.06%] [G loss: 2.834245]\n",
      "epoch:1 step:990 [D loss: 0.495573, acc: 76.56%] [G loss: 3.222226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:991 [D loss: 0.507733, acc: 78.12%] [G loss: 2.775427]\n",
      "epoch:1 step:992 [D loss: 0.451725, acc: 83.59%] [G loss: 2.970435]\n",
      "epoch:1 step:993 [D loss: 0.546600, acc: 74.22%] [G loss: 3.043957]\n",
      "epoch:1 step:994 [D loss: 0.504423, acc: 75.78%] [G loss: 2.924237]\n",
      "epoch:1 step:995 [D loss: 0.496709, acc: 76.56%] [G loss: 2.612323]\n",
      "epoch:1 step:996 [D loss: 0.509454, acc: 73.44%] [G loss: 2.874993]\n",
      "epoch:1 step:997 [D loss: 0.486785, acc: 78.91%] [G loss: 3.045206]\n",
      "epoch:1 step:998 [D loss: 0.625998, acc: 67.97%] [G loss: 3.067512]\n",
      "epoch:1 step:999 [D loss: 0.517570, acc: 72.66%] [G loss: 2.839684]\n",
      "epoch:1 step:1000 [D loss: 0.567086, acc: 70.31%] [G loss: 3.048241]\n",
      "epoch:1 step:1001 [D loss: 0.552777, acc: 67.19%] [G loss: 2.701906]\n",
      "epoch:1 step:1002 [D loss: 0.469157, acc: 80.47%] [G loss: 2.924064]\n",
      "epoch:1 step:1003 [D loss: 0.539266, acc: 74.22%] [G loss: 2.699557]\n",
      "epoch:1 step:1004 [D loss: 0.475074, acc: 80.47%] [G loss: 3.078206]\n",
      "epoch:1 step:1005 [D loss: 0.475879, acc: 82.03%] [G loss: 2.867990]\n",
      "epoch:1 step:1006 [D loss: 0.575575, acc: 67.97%] [G loss: 2.781380]\n",
      "epoch:1 step:1007 [D loss: 0.576344, acc: 67.97%] [G loss: 2.735998]\n",
      "epoch:1 step:1008 [D loss: 0.591583, acc: 67.19%] [G loss: 2.884814]\n",
      "epoch:1 step:1009 [D loss: 0.559059, acc: 75.00%] [G loss: 3.125991]\n",
      "epoch:1 step:1010 [D loss: 0.491163, acc: 72.66%] [G loss: 3.153895]\n",
      "epoch:1 step:1011 [D loss: 0.476648, acc: 78.12%] [G loss: 3.132953]\n",
      "epoch:1 step:1012 [D loss: 0.536354, acc: 74.22%] [G loss: 3.262226]\n",
      "epoch:1 step:1013 [D loss: 0.564924, acc: 68.75%] [G loss: 2.824556]\n",
      "epoch:1 step:1014 [D loss: 0.470444, acc: 76.56%] [G loss: 3.358006]\n",
      "epoch:1 step:1015 [D loss: 0.589001, acc: 68.75%] [G loss: 2.814314]\n",
      "epoch:1 step:1016 [D loss: 0.456490, acc: 79.69%] [G loss: 2.773479]\n",
      "epoch:1 step:1017 [D loss: 0.516025, acc: 74.22%] [G loss: 2.948147]\n",
      "epoch:1 step:1018 [D loss: 0.607191, acc: 64.84%] [G loss: 2.795306]\n",
      "epoch:1 step:1019 [D loss: 0.544427, acc: 73.44%] [G loss: 3.015934]\n",
      "epoch:1 step:1020 [D loss: 0.616443, acc: 60.94%] [G loss: 3.043521]\n",
      "epoch:1 step:1021 [D loss: 0.617181, acc: 65.62%] [G loss: 2.839656]\n",
      "epoch:1 step:1022 [D loss: 0.554653, acc: 67.97%] [G loss: 2.878626]\n",
      "epoch:1 step:1023 [D loss: 0.531408, acc: 75.78%] [G loss: 2.872522]\n",
      "epoch:1 step:1024 [D loss: 0.596941, acc: 70.31%] [G loss: 2.646379]\n",
      "epoch:1 step:1025 [D loss: 0.511905, acc: 76.56%] [G loss: 2.904475]\n",
      "epoch:1 step:1026 [D loss: 0.545598, acc: 71.09%] [G loss: 2.827368]\n",
      "epoch:1 step:1027 [D loss: 0.493172, acc: 77.34%] [G loss: 3.019247]\n",
      "epoch:1 step:1028 [D loss: 0.533840, acc: 72.66%] [G loss: 2.652773]\n",
      "epoch:1 step:1029 [D loss: 0.525992, acc: 71.09%] [G loss: 2.927188]\n",
      "epoch:1 step:1030 [D loss: 0.489839, acc: 75.78%] [G loss: 3.157685]\n",
      "epoch:1 step:1031 [D loss: 0.579303, acc: 67.97%] [G loss: 3.265642]\n",
      "epoch:1 step:1032 [D loss: 0.572665, acc: 69.53%] [G loss: 3.103825]\n",
      "epoch:1 step:1033 [D loss: 0.543881, acc: 71.09%] [G loss: 3.409374]\n",
      "epoch:1 step:1034 [D loss: 0.570661, acc: 67.19%] [G loss: 3.344985]\n",
      "epoch:1 step:1035 [D loss: 0.483632, acc: 75.00%] [G loss: 3.399791]\n",
      "epoch:1 step:1036 [D loss: 0.609981, acc: 70.31%] [G loss: 3.342332]\n",
      "epoch:1 step:1037 [D loss: 0.524287, acc: 77.34%] [G loss: 3.225732]\n",
      "epoch:1 step:1038 [D loss: 0.548933, acc: 72.66%] [G loss: 2.752121]\n",
      "epoch:1 step:1039 [D loss: 0.562921, acc: 68.75%] [G loss: 2.841164]\n",
      "epoch:1 step:1040 [D loss: 0.510870, acc: 76.56%] [G loss: 3.380424]\n",
      "epoch:1 step:1041 [D loss: 0.560409, acc: 68.75%] [G loss: 2.813453]\n",
      "epoch:1 step:1042 [D loss: 0.622443, acc: 64.06%] [G loss: 2.806545]\n",
      "epoch:1 step:1043 [D loss: 0.479733, acc: 78.91%] [G loss: 3.000389]\n",
      "epoch:1 step:1044 [D loss: 0.737652, acc: 59.38%] [G loss: 2.616205]\n",
      "epoch:1 step:1045 [D loss: 0.536377, acc: 70.31%] [G loss: 3.192206]\n",
      "epoch:1 step:1046 [D loss: 0.688453, acc: 65.62%] [G loss: 3.089648]\n",
      "epoch:1 step:1047 [D loss: 0.524023, acc: 72.66%] [G loss: 2.891190]\n",
      "epoch:1 step:1048 [D loss: 0.618899, acc: 66.41%] [G loss: 3.083408]\n",
      "epoch:1 step:1049 [D loss: 0.576465, acc: 69.53%] [G loss: 2.834634]\n",
      "epoch:1 step:1050 [D loss: 0.575350, acc: 70.31%] [G loss: 2.677309]\n",
      "epoch:1 step:1051 [D loss: 0.572737, acc: 67.19%] [G loss: 2.810753]\n",
      "epoch:1 step:1052 [D loss: 0.664400, acc: 60.16%] [G loss: 2.503409]\n",
      "epoch:1 step:1053 [D loss: 0.541857, acc: 72.66%] [G loss: 2.948337]\n",
      "epoch:1 step:1054 [D loss: 0.632311, acc: 64.06%] [G loss: 2.972221]\n",
      "epoch:1 step:1055 [D loss: 0.568222, acc: 71.88%] [G loss: 2.779724]\n",
      "epoch:1 step:1056 [D loss: 0.521355, acc: 70.31%] [G loss: 2.911462]\n",
      "epoch:1 step:1057 [D loss: 0.503696, acc: 76.56%] [G loss: 3.094172]\n",
      "epoch:1 step:1058 [D loss: 0.726294, acc: 53.91%] [G loss: 2.994857]\n",
      "epoch:1 step:1059 [D loss: 0.572170, acc: 72.66%] [G loss: 3.096490]\n",
      "epoch:1 step:1060 [D loss: 0.550320, acc: 69.53%] [G loss: 3.000352]\n",
      "epoch:1 step:1061 [D loss: 0.601456, acc: 69.53%] [G loss: 3.347988]\n",
      "epoch:1 step:1062 [D loss: 0.502339, acc: 75.78%] [G loss: 3.177504]\n",
      "epoch:1 step:1063 [D loss: 0.571413, acc: 69.53%] [G loss: 2.825394]\n",
      "epoch:1 step:1064 [D loss: 0.535623, acc: 70.31%] [G loss: 2.961149]\n",
      "epoch:1 step:1065 [D loss: 0.654444, acc: 60.94%] [G loss: 3.055285]\n",
      "epoch:1 step:1066 [D loss: 0.551379, acc: 67.97%] [G loss: 2.783812]\n",
      "epoch:1 step:1067 [D loss: 0.515132, acc: 77.34%] [G loss: 2.926532]\n",
      "epoch:1 step:1068 [D loss: 0.597315, acc: 68.75%] [G loss: 2.933442]\n",
      "epoch:1 step:1069 [D loss: 0.484968, acc: 76.56%] [G loss: 2.911735]\n",
      "epoch:1 step:1070 [D loss: 0.650627, acc: 66.41%] [G loss: 2.972228]\n",
      "epoch:1 step:1071 [D loss: 0.492147, acc: 76.56%] [G loss: 3.193294]\n",
      "epoch:1 step:1072 [D loss: 0.512237, acc: 74.22%] [G loss: 3.142460]\n",
      "epoch:1 step:1073 [D loss: 0.530647, acc: 75.00%] [G loss: 3.085225]\n",
      "epoch:1 step:1074 [D loss: 0.612116, acc: 67.19%] [G loss: 2.741393]\n",
      "epoch:1 step:1075 [D loss: 0.505956, acc: 75.00%] [G loss: 2.989118]\n",
      "epoch:1 step:1076 [D loss: 0.516668, acc: 74.22%] [G loss: 2.943806]\n",
      "epoch:1 step:1077 [D loss: 0.600088, acc: 70.31%] [G loss: 2.880407]\n",
      "epoch:1 step:1078 [D loss: 0.554541, acc: 72.66%] [G loss: 3.063150]\n",
      "epoch:1 step:1079 [D loss: 0.591100, acc: 72.66%] [G loss: 2.938620]\n",
      "epoch:1 step:1080 [D loss: 0.541787, acc: 68.75%] [G loss: 3.011248]\n",
      "epoch:1 step:1081 [D loss: 0.576285, acc: 67.19%] [G loss: 2.891119]\n",
      "epoch:1 step:1082 [D loss: 0.595798, acc: 64.84%] [G loss: 2.855699]\n",
      "epoch:1 step:1083 [D loss: 0.521204, acc: 70.31%] [G loss: 2.885252]\n",
      "epoch:1 step:1084 [D loss: 0.623363, acc: 67.19%] [G loss: 3.273341]\n",
      "epoch:1 step:1085 [D loss: 0.586109, acc: 69.53%] [G loss: 2.795743]\n",
      "epoch:1 step:1086 [D loss: 0.509841, acc: 75.78%] [G loss: 2.859680]\n",
      "epoch:1 step:1087 [D loss: 0.570709, acc: 72.66%] [G loss: 2.934590]\n",
      "epoch:1 step:1088 [D loss: 0.496763, acc: 78.12%] [G loss: 3.102324]\n",
      "epoch:1 step:1089 [D loss: 0.506014, acc: 71.88%] [G loss: 3.136645]\n",
      "epoch:1 step:1090 [D loss: 0.585064, acc: 65.62%] [G loss: 3.050805]\n",
      "epoch:1 step:1091 [D loss: 0.573859, acc: 77.34%] [G loss: 2.728732]\n",
      "epoch:1 step:1092 [D loss: 0.578346, acc: 73.44%] [G loss: 3.040525]\n",
      "epoch:1 step:1093 [D loss: 0.562931, acc: 69.53%] [G loss: 2.839060]\n",
      "epoch:1 step:1094 [D loss: 0.519248, acc: 72.66%] [G loss: 2.950347]\n",
      "epoch:1 step:1095 [D loss: 0.640613, acc: 69.53%] [G loss: 2.802463]\n",
      "epoch:1 step:1096 [D loss: 0.580609, acc: 71.88%] [G loss: 2.604857]\n",
      "epoch:1 step:1097 [D loss: 0.544570, acc: 73.44%] [G loss: 3.039999]\n",
      "epoch:1 step:1098 [D loss: 0.559772, acc: 71.09%] [G loss: 3.193057]\n",
      "epoch:1 step:1099 [D loss: 0.451262, acc: 79.69%] [G loss: 3.104979]\n",
      "epoch:1 step:1100 [D loss: 0.627564, acc: 62.50%] [G loss: 2.875919]\n",
      "epoch:1 step:1101 [D loss: 0.554496, acc: 72.66%] [G loss: 3.112260]\n",
      "epoch:1 step:1102 [D loss: 0.495942, acc: 76.56%] [G loss: 3.034377]\n",
      "epoch:1 step:1103 [D loss: 0.583998, acc: 71.09%] [G loss: 2.954475]\n",
      "epoch:1 step:1104 [D loss: 0.582785, acc: 67.19%] [G loss: 2.988278]\n",
      "epoch:1 step:1105 [D loss: 0.510887, acc: 79.69%] [G loss: 2.741832]\n",
      "epoch:1 step:1106 [D loss: 0.573513, acc: 71.09%] [G loss: 2.727943]\n",
      "epoch:1 step:1107 [D loss: 0.528290, acc: 73.44%] [G loss: 3.041348]\n",
      "epoch:1 step:1108 [D loss: 0.481060, acc: 72.66%] [G loss: 2.889709]\n",
      "epoch:1 step:1109 [D loss: 0.488635, acc: 77.34%] [G loss: 2.874969]\n",
      "epoch:1 step:1110 [D loss: 0.547733, acc: 71.09%] [G loss: 2.826868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1111 [D loss: 0.588000, acc: 66.41%] [G loss: 3.156939]\n",
      "epoch:1 step:1112 [D loss: 0.537126, acc: 73.44%] [G loss: 2.966166]\n",
      "epoch:1 step:1113 [D loss: 0.547248, acc: 67.97%] [G loss: 2.939400]\n",
      "epoch:1 step:1114 [D loss: 0.633045, acc: 70.31%] [G loss: 3.147583]\n",
      "epoch:1 step:1115 [D loss: 0.534826, acc: 75.78%] [G loss: 2.842829]\n",
      "epoch:1 step:1116 [D loss: 0.566017, acc: 72.66%] [G loss: 2.870599]\n",
      "epoch:1 step:1117 [D loss: 0.555404, acc: 73.44%] [G loss: 2.839903]\n",
      "epoch:1 step:1118 [D loss: 0.569865, acc: 67.19%] [G loss: 3.002040]\n",
      "epoch:1 step:1119 [D loss: 0.606150, acc: 61.72%] [G loss: 3.216842]\n",
      "epoch:1 step:1120 [D loss: 0.521941, acc: 73.44%] [G loss: 2.843174]\n",
      "epoch:1 step:1121 [D loss: 0.601811, acc: 67.97%] [G loss: 2.764523]\n",
      "epoch:1 step:1122 [D loss: 0.444664, acc: 80.47%] [G loss: 3.284129]\n",
      "epoch:1 step:1123 [D loss: 0.546210, acc: 75.00%] [G loss: 3.470711]\n",
      "epoch:1 step:1124 [D loss: 0.590524, acc: 67.19%] [G loss: 2.930279]\n",
      "epoch:1 step:1125 [D loss: 0.570680, acc: 68.75%] [G loss: 3.179316]\n",
      "epoch:1 step:1126 [D loss: 0.423420, acc: 83.59%] [G loss: 3.100720]\n",
      "epoch:1 step:1127 [D loss: 0.602477, acc: 67.97%] [G loss: 3.114386]\n",
      "epoch:1 step:1128 [D loss: 0.464196, acc: 80.47%] [G loss: 3.008728]\n",
      "epoch:1 step:1129 [D loss: 0.464229, acc: 81.25%] [G loss: 3.151017]\n",
      "epoch:1 step:1130 [D loss: 0.476279, acc: 79.69%] [G loss: 3.040387]\n",
      "epoch:1 step:1131 [D loss: 0.481236, acc: 80.47%] [G loss: 3.128231]\n",
      "epoch:1 step:1132 [D loss: 0.589622, acc: 66.41%] [G loss: 2.971511]\n",
      "epoch:1 step:1133 [D loss: 0.548930, acc: 71.88%] [G loss: 2.852694]\n",
      "epoch:1 step:1134 [D loss: 0.584561, acc: 74.22%] [G loss: 3.169579]\n",
      "epoch:1 step:1135 [D loss: 0.632123, acc: 66.41%] [G loss: 3.148634]\n",
      "epoch:1 step:1136 [D loss: 0.490095, acc: 78.91%] [G loss: 2.932298]\n",
      "epoch:1 step:1137 [D loss: 0.666120, acc: 60.94%] [G loss: 2.897813]\n",
      "epoch:1 step:1138 [D loss: 0.503344, acc: 73.44%] [G loss: 2.979848]\n",
      "epoch:1 step:1139 [D loss: 0.633654, acc: 65.62%] [G loss: 2.750322]\n",
      "epoch:1 step:1140 [D loss: 0.662633, acc: 63.28%] [G loss: 2.893511]\n",
      "epoch:1 step:1141 [D loss: 0.588394, acc: 73.44%] [G loss: 3.065021]\n",
      "epoch:1 step:1142 [D loss: 0.504085, acc: 76.56%] [G loss: 3.334960]\n",
      "epoch:1 step:1143 [D loss: 0.505461, acc: 76.56%] [G loss: 3.165440]\n",
      "epoch:1 step:1144 [D loss: 0.574531, acc: 68.75%] [G loss: 2.864098]\n",
      "epoch:1 step:1145 [D loss: 0.573205, acc: 70.31%] [G loss: 3.282142]\n",
      "epoch:1 step:1146 [D loss: 0.562205, acc: 75.00%] [G loss: 3.119018]\n",
      "epoch:1 step:1147 [D loss: 0.701332, acc: 58.59%] [G loss: 2.909174]\n",
      "epoch:1 step:1148 [D loss: 0.479396, acc: 75.00%] [G loss: 2.910410]\n",
      "epoch:1 step:1149 [D loss: 0.613599, acc: 65.62%] [G loss: 2.666060]\n",
      "epoch:1 step:1150 [D loss: 0.609554, acc: 69.53%] [G loss: 2.789125]\n",
      "epoch:1 step:1151 [D loss: 0.690699, acc: 60.94%] [G loss: 2.732030]\n",
      "epoch:1 step:1152 [D loss: 0.643598, acc: 64.06%] [G loss: 2.458957]\n",
      "epoch:1 step:1153 [D loss: 0.612019, acc: 67.19%] [G loss: 2.797183]\n",
      "epoch:1 step:1154 [D loss: 0.584065, acc: 69.53%] [G loss: 2.779418]\n",
      "epoch:1 step:1155 [D loss: 0.530261, acc: 71.88%] [G loss: 2.878370]\n",
      "epoch:1 step:1156 [D loss: 0.713011, acc: 57.03%] [G loss: 2.608567]\n",
      "epoch:1 step:1157 [D loss: 0.611667, acc: 66.41%] [G loss: 3.061063]\n",
      "epoch:1 step:1158 [D loss: 0.558937, acc: 68.75%] [G loss: 3.244057]\n",
      "epoch:1 step:1159 [D loss: 0.477587, acc: 80.47%] [G loss: 2.962672]\n",
      "epoch:1 step:1160 [D loss: 0.608751, acc: 72.66%] [G loss: 3.068371]\n",
      "epoch:1 step:1161 [D loss: 0.644290, acc: 65.62%] [G loss: 2.643398]\n",
      "epoch:1 step:1162 [D loss: 0.560668, acc: 67.97%] [G loss: 2.980998]\n",
      "epoch:1 step:1163 [D loss: 0.524101, acc: 71.88%] [G loss: 2.494448]\n",
      "epoch:1 step:1164 [D loss: 0.665334, acc: 59.38%] [G loss: 2.552337]\n",
      "epoch:1 step:1165 [D loss: 0.575385, acc: 72.66%] [G loss: 2.749742]\n",
      "epoch:1 step:1166 [D loss: 0.634713, acc: 64.84%] [G loss: 2.857005]\n",
      "epoch:1 step:1167 [D loss: 0.494753, acc: 77.34%] [G loss: 3.115616]\n",
      "epoch:1 step:1168 [D loss: 0.542673, acc: 69.53%] [G loss: 3.093681]\n",
      "epoch:1 step:1169 [D loss: 0.456078, acc: 83.59%] [G loss: 3.367647]\n",
      "epoch:1 step:1170 [D loss: 0.680071, acc: 60.16%] [G loss: 2.792948]\n",
      "epoch:1 step:1171 [D loss: 0.586042, acc: 67.97%] [G loss: 2.673369]\n",
      "epoch:1 step:1172 [D loss: 0.641678, acc: 63.28%] [G loss: 2.592758]\n",
      "epoch:1 step:1173 [D loss: 0.695084, acc: 60.94%] [G loss: 2.529558]\n",
      "epoch:1 step:1174 [D loss: 0.560566, acc: 75.00%] [G loss: 2.671567]\n",
      "epoch:1 step:1175 [D loss: 0.587497, acc: 66.41%] [G loss: 2.727742]\n",
      "epoch:1 step:1176 [D loss: 0.731227, acc: 58.59%] [G loss: 2.541396]\n",
      "epoch:1 step:1177 [D loss: 0.620208, acc: 64.06%] [G loss: 2.479947]\n",
      "epoch:1 step:1178 [D loss: 0.711975, acc: 57.81%] [G loss: 2.534072]\n",
      "epoch:1 step:1179 [D loss: 0.600695, acc: 66.41%] [G loss: 2.385587]\n",
      "epoch:1 step:1180 [D loss: 0.481778, acc: 76.56%] [G loss: 2.689758]\n",
      "epoch:1 step:1181 [D loss: 0.575405, acc: 71.88%] [G loss: 2.806631]\n",
      "epoch:1 step:1182 [D loss: 0.639147, acc: 70.31%] [G loss: 2.758022]\n",
      "epoch:1 step:1183 [D loss: 0.677325, acc: 61.72%] [G loss: 2.468911]\n",
      "epoch:1 step:1184 [D loss: 0.605027, acc: 67.97%] [G loss: 2.656127]\n",
      "epoch:1 step:1185 [D loss: 0.634668, acc: 67.97%] [G loss: 2.634380]\n",
      "epoch:1 step:1186 [D loss: 0.610407, acc: 67.19%] [G loss: 2.591065]\n",
      "epoch:1 step:1187 [D loss: 0.685479, acc: 55.47%] [G loss: 2.419800]\n",
      "epoch:1 step:1188 [D loss: 0.641369, acc: 61.72%] [G loss: 2.643578]\n",
      "epoch:1 step:1189 [D loss: 0.729600, acc: 59.38%] [G loss: 2.652234]\n",
      "epoch:1 step:1190 [D loss: 0.434796, acc: 82.03%] [G loss: 2.875727]\n",
      "epoch:1 step:1191 [D loss: 0.557410, acc: 73.44%] [G loss: 2.648760]\n",
      "epoch:1 step:1192 [D loss: 0.596967, acc: 69.53%] [G loss: 2.784180]\n",
      "epoch:1 step:1193 [D loss: 0.508032, acc: 76.56%] [G loss: 2.654269]\n",
      "epoch:1 step:1194 [D loss: 0.610535, acc: 63.28%] [G loss: 2.638516]\n",
      "epoch:1 step:1195 [D loss: 0.602362, acc: 66.41%] [G loss: 2.655871]\n",
      "epoch:1 step:1196 [D loss: 0.576391, acc: 71.09%] [G loss: 2.626929]\n",
      "epoch:1 step:1197 [D loss: 0.561521, acc: 76.56%] [G loss: 2.575410]\n",
      "epoch:1 step:1198 [D loss: 0.609421, acc: 68.75%] [G loss: 2.915895]\n",
      "epoch:1 step:1199 [D loss: 0.567983, acc: 67.19%] [G loss: 2.674686]\n",
      "epoch:1 step:1200 [D loss: 0.691859, acc: 67.19%] [G loss: 2.446822]\n",
      "epoch:1 step:1201 [D loss: 0.583043, acc: 68.75%] [G loss: 2.570508]\n",
      "epoch:1 step:1202 [D loss: 0.528355, acc: 71.88%] [G loss: 2.788134]\n",
      "epoch:1 step:1203 [D loss: 0.524165, acc: 72.66%] [G loss: 2.693467]\n",
      "epoch:1 step:1204 [D loss: 0.529156, acc: 75.00%] [G loss: 2.750262]\n",
      "epoch:1 step:1205 [D loss: 0.665663, acc: 57.81%] [G loss: 2.584577]\n",
      "epoch:1 step:1206 [D loss: 0.668224, acc: 64.84%] [G loss: 2.476209]\n",
      "epoch:1 step:1207 [D loss: 0.627566, acc: 62.50%] [G loss: 2.602511]\n",
      "epoch:1 step:1208 [D loss: 0.603978, acc: 64.06%] [G loss: 2.783448]\n",
      "epoch:1 step:1209 [D loss: 0.641080, acc: 64.84%] [G loss: 2.675464]\n",
      "epoch:1 step:1210 [D loss: 0.537178, acc: 73.44%] [G loss: 2.939409]\n",
      "epoch:1 step:1211 [D loss: 0.709360, acc: 55.47%] [G loss: 2.334010]\n",
      "epoch:1 step:1212 [D loss: 0.651916, acc: 66.41%] [G loss: 2.332633]\n",
      "epoch:1 step:1213 [D loss: 0.578974, acc: 70.31%] [G loss: 2.249588]\n",
      "epoch:1 step:1214 [D loss: 0.726280, acc: 58.59%] [G loss: 2.616432]\n",
      "epoch:1 step:1215 [D loss: 0.569613, acc: 75.00%] [G loss: 2.584706]\n",
      "epoch:1 step:1216 [D loss: 0.610121, acc: 73.44%] [G loss: 2.668347]\n",
      "epoch:1 step:1217 [D loss: 0.559588, acc: 70.31%] [G loss: 2.467116]\n",
      "epoch:1 step:1218 [D loss: 0.636027, acc: 64.84%] [G loss: 2.819238]\n",
      "epoch:1 step:1219 [D loss: 0.611652, acc: 66.41%] [G loss: 2.389381]\n",
      "epoch:1 step:1220 [D loss: 0.605041, acc: 66.41%] [G loss: 2.436543]\n",
      "epoch:1 step:1221 [D loss: 0.575177, acc: 74.22%] [G loss: 2.933983]\n",
      "epoch:1 step:1222 [D loss: 0.536654, acc: 71.88%] [G loss: 2.844383]\n",
      "epoch:1 step:1223 [D loss: 0.502377, acc: 77.34%] [G loss: 2.844855]\n",
      "epoch:1 step:1224 [D loss: 0.663093, acc: 57.81%] [G loss: 2.600373]\n",
      "epoch:1 step:1225 [D loss: 0.544733, acc: 75.00%] [G loss: 3.040320]\n",
      "epoch:1 step:1226 [D loss: 0.591362, acc: 68.75%] [G loss: 2.741092]\n",
      "epoch:1 step:1227 [D loss: 0.624694, acc: 69.53%] [G loss: 2.640040]\n",
      "epoch:1 step:1228 [D loss: 0.672882, acc: 61.72%] [G loss: 2.407837]\n",
      "epoch:1 step:1229 [D loss: 0.610312, acc: 66.41%] [G loss: 2.669063]\n",
      "epoch:1 step:1230 [D loss: 0.628808, acc: 64.84%] [G loss: 2.737993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1231 [D loss: 0.619142, acc: 67.19%] [G loss: 2.754562]\n",
      "epoch:1 step:1232 [D loss: 0.581360, acc: 67.19%] [G loss: 2.574248]\n",
      "epoch:1 step:1233 [D loss: 0.612450, acc: 64.84%] [G loss: 2.495500]\n",
      "epoch:1 step:1234 [D loss: 0.607216, acc: 65.62%] [G loss: 2.484441]\n",
      "epoch:1 step:1235 [D loss: 0.565904, acc: 70.31%] [G loss: 2.508570]\n",
      "epoch:1 step:1236 [D loss: 0.564023, acc: 69.53%] [G loss: 2.558175]\n",
      "epoch:1 step:1237 [D loss: 0.511390, acc: 75.78%] [G loss: 2.597706]\n",
      "epoch:1 step:1238 [D loss: 0.690675, acc: 62.50%] [G loss: 2.661841]\n",
      "epoch:1 step:1239 [D loss: 0.539209, acc: 70.31%] [G loss: 2.550972]\n",
      "epoch:1 step:1240 [D loss: 0.580185, acc: 67.97%] [G loss: 2.639135]\n",
      "epoch:1 step:1241 [D loss: 0.636080, acc: 62.50%] [G loss: 2.758985]\n",
      "epoch:1 step:1242 [D loss: 0.578348, acc: 69.53%] [G loss: 2.753357]\n",
      "epoch:1 step:1243 [D loss: 0.521155, acc: 76.56%] [G loss: 2.632318]\n",
      "epoch:1 step:1244 [D loss: 0.520458, acc: 71.09%] [G loss: 2.946266]\n",
      "epoch:1 step:1245 [D loss: 0.600457, acc: 68.75%] [G loss: 2.675882]\n",
      "epoch:1 step:1246 [D loss: 0.506881, acc: 78.12%] [G loss: 2.949320]\n",
      "epoch:1 step:1247 [D loss: 0.501899, acc: 71.88%] [G loss: 2.723833]\n",
      "epoch:1 step:1248 [D loss: 0.516966, acc: 77.34%] [G loss: 2.889950]\n",
      "epoch:1 step:1249 [D loss: 0.432983, acc: 78.91%] [G loss: 3.319979]\n",
      "epoch:1 step:1250 [D loss: 0.505078, acc: 71.09%] [G loss: 3.057094]\n",
      "epoch:1 step:1251 [D loss: 0.494787, acc: 75.00%] [G loss: 3.385131]\n",
      "epoch:1 step:1252 [D loss: 0.532122, acc: 67.97%] [G loss: 3.311451]\n",
      "epoch:1 step:1253 [D loss: 0.750796, acc: 54.69%] [G loss: 2.494115]\n",
      "epoch:1 step:1254 [D loss: 0.579767, acc: 70.31%] [G loss: 2.627473]\n",
      "epoch:1 step:1255 [D loss: 0.595422, acc: 67.97%] [G loss: 2.321611]\n",
      "epoch:1 step:1256 [D loss: 0.588881, acc: 65.62%] [G loss: 2.640564]\n",
      "epoch:1 step:1257 [D loss: 0.557884, acc: 73.44%] [G loss: 2.596259]\n",
      "epoch:1 step:1258 [D loss: 0.556497, acc: 68.75%] [G loss: 2.699663]\n",
      "epoch:1 step:1259 [D loss: 0.609756, acc: 66.41%] [G loss: 2.750240]\n",
      "epoch:1 step:1260 [D loss: 0.602296, acc: 66.41%] [G loss: 2.707867]\n",
      "epoch:1 step:1261 [D loss: 0.534175, acc: 75.00%] [G loss: 2.776934]\n",
      "epoch:1 step:1262 [D loss: 0.742881, acc: 50.00%] [G loss: 2.825424]\n",
      "epoch:1 step:1263 [D loss: 0.524478, acc: 71.09%] [G loss: 2.741004]\n",
      "epoch:1 step:1264 [D loss: 0.561895, acc: 73.44%] [G loss: 2.666825]\n",
      "epoch:1 step:1265 [D loss: 0.595707, acc: 64.84%] [G loss: 2.981589]\n",
      "epoch:1 step:1266 [D loss: 0.619383, acc: 71.09%] [G loss: 2.697609]\n",
      "epoch:1 step:1267 [D loss: 0.731565, acc: 54.69%] [G loss: 2.598757]\n",
      "epoch:1 step:1268 [D loss: 0.652716, acc: 62.50%] [G loss: 2.585664]\n",
      "epoch:1 step:1269 [D loss: 0.593324, acc: 67.19%] [G loss: 2.564909]\n",
      "epoch:1 step:1270 [D loss: 0.599382, acc: 64.06%] [G loss: 2.639318]\n",
      "epoch:1 step:1271 [D loss: 0.574364, acc: 68.75%] [G loss: 2.571428]\n",
      "epoch:1 step:1272 [D loss: 0.548514, acc: 67.19%] [G loss: 3.071147]\n",
      "epoch:1 step:1273 [D loss: 0.528856, acc: 75.78%] [G loss: 2.952545]\n",
      "epoch:1 step:1274 [D loss: 0.575717, acc: 71.09%] [G loss: 2.698842]\n",
      "epoch:1 step:1275 [D loss: 0.633620, acc: 60.16%] [G loss: 2.530850]\n",
      "epoch:1 step:1276 [D loss: 0.620652, acc: 60.94%] [G loss: 2.755727]\n",
      "epoch:1 step:1277 [D loss: 0.552082, acc: 71.09%] [G loss: 2.566801]\n",
      "epoch:1 step:1278 [D loss: 0.685040, acc: 59.38%] [G loss: 2.864510]\n",
      "epoch:1 step:1279 [D loss: 0.571768, acc: 70.31%] [G loss: 3.035740]\n",
      "epoch:1 step:1280 [D loss: 0.493261, acc: 79.69%] [G loss: 2.820706]\n",
      "epoch:1 step:1281 [D loss: 0.541824, acc: 73.44%] [G loss: 3.130667]\n",
      "epoch:1 step:1282 [D loss: 0.579845, acc: 70.31%] [G loss: 2.773168]\n",
      "epoch:1 step:1283 [D loss: 0.594721, acc: 74.22%] [G loss: 3.016236]\n",
      "epoch:1 step:1284 [D loss: 0.497222, acc: 73.44%] [G loss: 2.899604]\n",
      "epoch:1 step:1285 [D loss: 0.602269, acc: 71.09%] [G loss: 2.783271]\n",
      "epoch:1 step:1286 [D loss: 0.676065, acc: 57.81%] [G loss: 2.403599]\n",
      "epoch:1 step:1287 [D loss: 0.494012, acc: 72.66%] [G loss: 2.814460]\n",
      "epoch:1 step:1288 [D loss: 0.559695, acc: 70.31%] [G loss: 2.517557]\n",
      "epoch:1 step:1289 [D loss: 0.628285, acc: 67.97%] [G loss: 2.629689]\n",
      "epoch:1 step:1290 [D loss: 0.591996, acc: 64.84%] [G loss: 2.748242]\n",
      "epoch:1 step:1291 [D loss: 0.508190, acc: 75.00%] [G loss: 2.843962]\n",
      "epoch:1 step:1292 [D loss: 0.545538, acc: 73.44%] [G loss: 2.952591]\n",
      "epoch:1 step:1293 [D loss: 0.523258, acc: 72.66%] [G loss: 2.719837]\n",
      "epoch:1 step:1294 [D loss: 0.565618, acc: 69.53%] [G loss: 2.554483]\n",
      "epoch:1 step:1295 [D loss: 0.475769, acc: 75.78%] [G loss: 2.983277]\n",
      "epoch:1 step:1296 [D loss: 0.409185, acc: 85.16%] [G loss: 3.163450]\n",
      "epoch:1 step:1297 [D loss: 0.603955, acc: 69.53%] [G loss: 2.825386]\n",
      "epoch:1 step:1298 [D loss: 0.540510, acc: 72.66%] [G loss: 2.844696]\n",
      "epoch:1 step:1299 [D loss: 0.526968, acc: 71.09%] [G loss: 2.613379]\n",
      "epoch:1 step:1300 [D loss: 0.495486, acc: 74.22%] [G loss: 2.880329]\n",
      "epoch:1 step:1301 [D loss: 0.493623, acc: 75.78%] [G loss: 2.948836]\n",
      "epoch:1 step:1302 [D loss: 0.522128, acc: 71.88%] [G loss: 2.668088]\n",
      "epoch:1 step:1303 [D loss: 0.487470, acc: 78.91%] [G loss: 2.964694]\n",
      "epoch:1 step:1304 [D loss: 0.546677, acc: 76.56%] [G loss: 2.988828]\n",
      "epoch:1 step:1305 [D loss: 0.574402, acc: 72.66%] [G loss: 3.120516]\n",
      "epoch:1 step:1306 [D loss: 0.515283, acc: 71.09%] [G loss: 2.677834]\n",
      "epoch:1 step:1307 [D loss: 0.546154, acc: 73.44%] [G loss: 2.613621]\n",
      "epoch:1 step:1308 [D loss: 0.568394, acc: 75.00%] [G loss: 2.810003]\n",
      "epoch:1 step:1309 [D loss: 0.585363, acc: 67.19%] [G loss: 2.866054]\n",
      "epoch:1 step:1310 [D loss: 0.641438, acc: 70.31%] [G loss: 2.757978]\n",
      "epoch:1 step:1311 [D loss: 0.547965, acc: 71.09%] [G loss: 2.747719]\n",
      "epoch:1 step:1312 [D loss: 0.607861, acc: 68.75%] [G loss: 2.698475]\n",
      "epoch:1 step:1313 [D loss: 0.605998, acc: 67.19%] [G loss: 2.642085]\n",
      "epoch:1 step:1314 [D loss: 0.568909, acc: 67.19%] [G loss: 2.591467]\n",
      "epoch:1 step:1315 [D loss: 0.497719, acc: 78.12%] [G loss: 2.799342]\n",
      "epoch:1 step:1316 [D loss: 0.506962, acc: 72.66%] [G loss: 2.722020]\n",
      "epoch:1 step:1317 [D loss: 0.540162, acc: 71.88%] [G loss: 2.527466]\n",
      "epoch:1 step:1318 [D loss: 0.620318, acc: 67.19%] [G loss: 2.876287]\n",
      "epoch:1 step:1319 [D loss: 0.537632, acc: 71.09%] [G loss: 3.087461]\n",
      "epoch:1 step:1320 [D loss: 0.566586, acc: 71.88%] [G loss: 2.790625]\n",
      "epoch:1 step:1321 [D loss: 0.543917, acc: 75.78%] [G loss: 2.943980]\n",
      "epoch:1 step:1322 [D loss: 0.618255, acc: 64.84%] [G loss: 2.938866]\n",
      "epoch:1 step:1323 [D loss: 0.594228, acc: 70.31%] [G loss: 2.769435]\n",
      "epoch:1 step:1324 [D loss: 0.676168, acc: 67.19%] [G loss: 2.717954]\n",
      "epoch:1 step:1325 [D loss: 0.526759, acc: 71.88%] [G loss: 2.833391]\n",
      "epoch:1 step:1326 [D loss: 0.600767, acc: 65.62%] [G loss: 2.486608]\n",
      "epoch:1 step:1327 [D loss: 0.601013, acc: 70.31%] [G loss: 2.533316]\n",
      "epoch:1 step:1328 [D loss: 0.586160, acc: 67.97%] [G loss: 2.824568]\n",
      "epoch:1 step:1329 [D loss: 0.546104, acc: 71.09%] [G loss: 2.880363]\n",
      "epoch:1 step:1330 [D loss: 0.553185, acc: 66.41%] [G loss: 2.489078]\n",
      "epoch:1 step:1331 [D loss: 0.593092, acc: 65.62%] [G loss: 2.540918]\n",
      "epoch:1 step:1332 [D loss: 0.508682, acc: 73.44%] [G loss: 2.854810]\n",
      "epoch:1 step:1333 [D loss: 0.605674, acc: 67.19%] [G loss: 2.669035]\n",
      "epoch:1 step:1334 [D loss: 0.519997, acc: 72.66%] [G loss: 3.006479]\n",
      "epoch:1 step:1335 [D loss: 0.457234, acc: 80.47%] [G loss: 3.331542]\n",
      "epoch:1 step:1336 [D loss: 0.498628, acc: 74.22%] [G loss: 3.069426]\n",
      "epoch:1 step:1337 [D loss: 0.508628, acc: 71.88%] [G loss: 2.863792]\n",
      "epoch:1 step:1338 [D loss: 0.529371, acc: 75.00%] [G loss: 2.867130]\n",
      "epoch:1 step:1339 [D loss: 0.477750, acc: 75.78%] [G loss: 3.270661]\n",
      "epoch:1 step:1340 [D loss: 0.540151, acc: 68.75%] [G loss: 3.034812]\n",
      "epoch:1 step:1341 [D loss: 0.546698, acc: 72.66%] [G loss: 3.060752]\n",
      "epoch:1 step:1342 [D loss: 0.528879, acc: 75.00%] [G loss: 3.276544]\n",
      "epoch:1 step:1343 [D loss: 0.512457, acc: 70.31%] [G loss: 3.243698]\n",
      "epoch:1 step:1344 [D loss: 0.601155, acc: 72.66%] [G loss: 3.101671]\n",
      "epoch:1 step:1345 [D loss: 0.479943, acc: 74.22%] [G loss: 3.090617]\n",
      "epoch:1 step:1346 [D loss: 0.493492, acc: 76.56%] [G loss: 3.202677]\n",
      "epoch:1 step:1347 [D loss: 0.624605, acc: 66.41%] [G loss: 3.075031]\n",
      "epoch:1 step:1348 [D loss: 0.506323, acc: 74.22%] [G loss: 2.698116]\n",
      "epoch:1 step:1349 [D loss: 0.537112, acc: 71.09%] [G loss: 2.879524]\n",
      "epoch:1 step:1350 [D loss: 0.516903, acc: 76.56%] [G loss: 2.887649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1351 [D loss: 0.486747, acc: 75.78%] [G loss: 2.957753]\n",
      "epoch:1 step:1352 [D loss: 0.595292, acc: 65.62%] [G loss: 2.635399]\n",
      "epoch:1 step:1353 [D loss: 0.543604, acc: 71.88%] [G loss: 2.830688]\n",
      "epoch:1 step:1354 [D loss: 0.627390, acc: 65.62%] [G loss: 2.772968]\n",
      "epoch:1 step:1355 [D loss: 0.639359, acc: 61.72%] [G loss: 2.954801]\n",
      "epoch:1 step:1356 [D loss: 0.545934, acc: 74.22%] [G loss: 2.861204]\n",
      "epoch:1 step:1357 [D loss: 0.570853, acc: 67.19%] [G loss: 3.104689]\n",
      "epoch:1 step:1358 [D loss: 0.592326, acc: 68.75%] [G loss: 2.846438]\n",
      "epoch:1 step:1359 [D loss: 0.557357, acc: 69.53%] [G loss: 3.031651]\n",
      "epoch:1 step:1360 [D loss: 0.569602, acc: 71.88%] [G loss: 3.025539]\n",
      "epoch:1 step:1361 [D loss: 0.619317, acc: 66.41%] [G loss: 3.005045]\n",
      "epoch:1 step:1362 [D loss: 0.559752, acc: 71.88%] [G loss: 3.021515]\n",
      "epoch:1 step:1363 [D loss: 0.472039, acc: 78.91%] [G loss: 3.345144]\n",
      "epoch:1 step:1364 [D loss: 0.523795, acc: 75.00%] [G loss: 3.093071]\n",
      "epoch:1 step:1365 [D loss: 0.509648, acc: 78.12%] [G loss: 3.179660]\n",
      "epoch:1 step:1366 [D loss: 0.520655, acc: 76.56%] [G loss: 3.043418]\n",
      "epoch:1 step:1367 [D loss: 0.523913, acc: 69.53%] [G loss: 3.266096]\n",
      "epoch:1 step:1368 [D loss: 0.564823, acc: 71.09%] [G loss: 3.100183]\n",
      "epoch:1 step:1369 [D loss: 0.538050, acc: 72.66%] [G loss: 2.916432]\n",
      "epoch:1 step:1370 [D loss: 0.620644, acc: 60.94%] [G loss: 2.719917]\n",
      "epoch:1 step:1371 [D loss: 0.498590, acc: 76.56%] [G loss: 2.804729]\n",
      "epoch:1 step:1372 [D loss: 0.565003, acc: 71.88%] [G loss: 2.779885]\n",
      "epoch:1 step:1373 [D loss: 0.546591, acc: 70.31%] [G loss: 2.844187]\n",
      "epoch:1 step:1374 [D loss: 0.587965, acc: 71.09%] [G loss: 2.854957]\n",
      "epoch:1 step:1375 [D loss: 0.499711, acc: 80.47%] [G loss: 3.140182]\n",
      "epoch:1 step:1376 [D loss: 0.619437, acc: 69.53%] [G loss: 2.851630]\n",
      "epoch:1 step:1377 [D loss: 0.482915, acc: 72.66%] [G loss: 2.948095]\n",
      "epoch:1 step:1378 [D loss: 0.575475, acc: 70.31%] [G loss: 3.018140]\n",
      "epoch:1 step:1379 [D loss: 0.509302, acc: 69.53%] [G loss: 2.970129]\n",
      "epoch:1 step:1380 [D loss: 0.598409, acc: 66.41%] [G loss: 2.647287]\n",
      "epoch:1 step:1381 [D loss: 0.518010, acc: 72.66%] [G loss: 2.795856]\n",
      "epoch:1 step:1382 [D loss: 0.537765, acc: 69.53%] [G loss: 2.766835]\n",
      "epoch:1 step:1383 [D loss: 0.495368, acc: 82.03%] [G loss: 2.883771]\n",
      "epoch:1 step:1384 [D loss: 0.496894, acc: 74.22%] [G loss: 3.336069]\n",
      "epoch:1 step:1385 [D loss: 0.593189, acc: 71.09%] [G loss: 2.698420]\n",
      "epoch:1 step:1386 [D loss: 0.638852, acc: 62.50%] [G loss: 2.805058]\n",
      "epoch:1 step:1387 [D loss: 0.588607, acc: 64.84%] [G loss: 2.785034]\n",
      "epoch:1 step:1388 [D loss: 0.557504, acc: 74.22%] [G loss: 3.082559]\n",
      "epoch:1 step:1389 [D loss: 0.543420, acc: 71.09%] [G loss: 3.157098]\n",
      "epoch:1 step:1390 [D loss: 0.663694, acc: 61.72%] [G loss: 3.006793]\n",
      "epoch:1 step:1391 [D loss: 0.610618, acc: 66.41%] [G loss: 2.909225]\n",
      "epoch:1 step:1392 [D loss: 0.637483, acc: 62.50%] [G loss: 2.671372]\n",
      "epoch:1 step:1393 [D loss: 0.581880, acc: 67.97%] [G loss: 2.589146]\n",
      "epoch:1 step:1394 [D loss: 0.715633, acc: 57.81%] [G loss: 2.562351]\n",
      "epoch:1 step:1395 [D loss: 0.643249, acc: 65.62%] [G loss: 2.652405]\n",
      "epoch:1 step:1396 [D loss: 0.514481, acc: 75.00%] [G loss: 2.862242]\n",
      "epoch:1 step:1397 [D loss: 0.470144, acc: 82.03%] [G loss: 3.163859]\n",
      "epoch:1 step:1398 [D loss: 0.450581, acc: 78.12%] [G loss: 3.043338]\n",
      "epoch:1 step:1399 [D loss: 0.641716, acc: 67.97%] [G loss: 2.803526]\n",
      "epoch:1 step:1400 [D loss: 0.535309, acc: 75.00%] [G loss: 2.752452]\n",
      "epoch:1 step:1401 [D loss: 0.543483, acc: 73.44%] [G loss: 2.761251]\n",
      "epoch:1 step:1402 [D loss: 0.568024, acc: 71.88%] [G loss: 2.606351]\n",
      "epoch:1 step:1403 [D loss: 0.525418, acc: 72.66%] [G loss: 2.934762]\n",
      "epoch:1 step:1404 [D loss: 0.561270, acc: 68.75%] [G loss: 3.069744]\n",
      "epoch:1 step:1405 [D loss: 0.661993, acc: 63.28%] [G loss: 2.568920]\n",
      "epoch:1 step:1406 [D loss: 0.557534, acc: 71.09%] [G loss: 2.451456]\n",
      "epoch:1 step:1407 [D loss: 0.562310, acc: 72.66%] [G loss: 3.028436]\n",
      "epoch:1 step:1408 [D loss: 0.635378, acc: 69.53%] [G loss: 3.087364]\n",
      "epoch:1 step:1409 [D loss: 0.542257, acc: 74.22%] [G loss: 2.840982]\n",
      "epoch:1 step:1410 [D loss: 0.673995, acc: 63.28%] [G loss: 2.928637]\n",
      "epoch:1 step:1411 [D loss: 0.589902, acc: 67.97%] [G loss: 2.863793]\n",
      "epoch:1 step:1412 [D loss: 0.519237, acc: 73.44%] [G loss: 2.961043]\n",
      "epoch:1 step:1413 [D loss: 0.607604, acc: 65.62%] [G loss: 2.749594]\n",
      "epoch:1 step:1414 [D loss: 0.764626, acc: 53.12%] [G loss: 2.499876]\n",
      "epoch:1 step:1415 [D loss: 0.644579, acc: 58.59%] [G loss: 2.613519]\n",
      "epoch:1 step:1416 [D loss: 0.587982, acc: 66.41%] [G loss: 2.712633]\n",
      "epoch:1 step:1417 [D loss: 0.554563, acc: 67.97%] [G loss: 2.464329]\n",
      "epoch:1 step:1418 [D loss: 0.649977, acc: 63.28%] [G loss: 2.720425]\n",
      "epoch:1 step:1419 [D loss: 0.571678, acc: 73.44%] [G loss: 2.780611]\n",
      "epoch:1 step:1420 [D loss: 0.701434, acc: 57.03%] [G loss: 2.587318]\n",
      "epoch:1 step:1421 [D loss: 0.625892, acc: 70.31%] [G loss: 2.820764]\n",
      "epoch:1 step:1422 [D loss: 0.595670, acc: 67.97%] [G loss: 2.734006]\n",
      "epoch:1 step:1423 [D loss: 0.540980, acc: 67.97%] [G loss: 2.696863]\n",
      "epoch:1 step:1424 [D loss: 0.575498, acc: 69.53%] [G loss: 2.598479]\n",
      "epoch:1 step:1425 [D loss: 0.579107, acc: 66.41%] [G loss: 3.033661]\n",
      "epoch:1 step:1426 [D loss: 0.736978, acc: 57.03%] [G loss: 2.529655]\n",
      "epoch:1 step:1427 [D loss: 0.670726, acc: 58.59%] [G loss: 2.587041]\n",
      "epoch:1 step:1428 [D loss: 0.555573, acc: 71.88%] [G loss: 3.193221]\n",
      "epoch:1 step:1429 [D loss: 0.647051, acc: 64.84%] [G loss: 2.861635]\n",
      "epoch:1 step:1430 [D loss: 0.624278, acc: 66.41%] [G loss: 2.447951]\n",
      "epoch:1 step:1431 [D loss: 0.548825, acc: 70.31%] [G loss: 2.740289]\n",
      "epoch:1 step:1432 [D loss: 0.636931, acc: 63.28%] [G loss: 2.942440]\n",
      "epoch:1 step:1433 [D loss: 0.607629, acc: 67.97%] [G loss: 2.728520]\n",
      "epoch:1 step:1434 [D loss: 0.527536, acc: 72.66%] [G loss: 2.911049]\n",
      "epoch:1 step:1435 [D loss: 0.471826, acc: 80.47%] [G loss: 3.102361]\n",
      "epoch:1 step:1436 [D loss: 0.537167, acc: 71.88%] [G loss: 3.370939]\n",
      "epoch:1 step:1437 [D loss: 0.611372, acc: 67.19%] [G loss: 2.592712]\n",
      "epoch:1 step:1438 [D loss: 0.694271, acc: 57.81%] [G loss: 2.774702]\n",
      "epoch:1 step:1439 [D loss: 0.627774, acc: 63.28%] [G loss: 2.542730]\n",
      "epoch:1 step:1440 [D loss: 0.562327, acc: 69.53%] [G loss: 2.817780]\n",
      "epoch:1 step:1441 [D loss: 0.582261, acc: 69.53%] [G loss: 3.143661]\n",
      "epoch:1 step:1442 [D loss: 0.639483, acc: 60.94%] [G loss: 2.567674]\n",
      "epoch:1 step:1443 [D loss: 0.548990, acc: 71.09%] [G loss: 2.932978]\n",
      "epoch:1 step:1444 [D loss: 0.512968, acc: 74.22%] [G loss: 2.845292]\n",
      "epoch:1 step:1445 [D loss: 0.522225, acc: 74.22%] [G loss: 3.244720]\n",
      "epoch:1 step:1446 [D loss: 0.576918, acc: 66.41%] [G loss: 2.729192]\n",
      "epoch:1 step:1447 [D loss: 0.563181, acc: 69.53%] [G loss: 2.471658]\n",
      "epoch:1 step:1448 [D loss: 0.736768, acc: 56.25%] [G loss: 2.598258]\n",
      "epoch:1 step:1449 [D loss: 0.616930, acc: 65.62%] [G loss: 2.737664]\n",
      "epoch:1 step:1450 [D loss: 0.576111, acc: 64.84%] [G loss: 2.732588]\n",
      "epoch:1 step:1451 [D loss: 0.544601, acc: 67.97%] [G loss: 2.989284]\n",
      "epoch:1 step:1452 [D loss: 0.555195, acc: 68.75%] [G loss: 2.791337]\n",
      "epoch:1 step:1453 [D loss: 0.582499, acc: 71.09%] [G loss: 2.387188]\n",
      "epoch:1 step:1454 [D loss: 0.607983, acc: 64.06%] [G loss: 2.593846]\n",
      "epoch:1 step:1455 [D loss: 0.562459, acc: 69.53%] [G loss: 2.804816]\n",
      "epoch:1 step:1456 [D loss: 0.544263, acc: 70.31%] [G loss: 2.876307]\n",
      "epoch:1 step:1457 [D loss: 0.526743, acc: 72.66%] [G loss: 2.832087]\n",
      "epoch:1 step:1458 [D loss: 0.579765, acc: 68.75%] [G loss: 2.915086]\n",
      "epoch:1 step:1459 [D loss: 0.514679, acc: 69.53%] [G loss: 2.781065]\n",
      "epoch:1 step:1460 [D loss: 0.571176, acc: 66.41%] [G loss: 3.305569]\n",
      "epoch:1 step:1461 [D loss: 0.633065, acc: 68.75%] [G loss: 2.493810]\n",
      "epoch:1 step:1462 [D loss: 0.668739, acc: 60.94%] [G loss: 2.420986]\n",
      "epoch:1 step:1463 [D loss: 0.535494, acc: 71.88%] [G loss: 2.825301]\n",
      "epoch:1 step:1464 [D loss: 0.617646, acc: 64.84%] [G loss: 2.513757]\n",
      "epoch:1 step:1465 [D loss: 0.664285, acc: 64.84%] [G loss: 2.617300]\n",
      "epoch:1 step:1466 [D loss: 0.655063, acc: 64.06%] [G loss: 2.557183]\n",
      "epoch:1 step:1467 [D loss: 0.602729, acc: 64.84%] [G loss: 2.848475]\n",
      "epoch:1 step:1468 [D loss: 0.602321, acc: 70.31%] [G loss: 2.472771]\n",
      "epoch:1 step:1469 [D loss: 0.627346, acc: 64.06%] [G loss: 2.713326]\n",
      "epoch:1 step:1470 [D loss: 0.562476, acc: 68.75%] [G loss: 2.792898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1471 [D loss: 0.507337, acc: 69.53%] [G loss: 2.721577]\n",
      "epoch:1 step:1472 [D loss: 0.655716, acc: 55.47%] [G loss: 2.618492]\n",
      "epoch:1 step:1473 [D loss: 0.582604, acc: 71.88%] [G loss: 2.716469]\n",
      "epoch:1 step:1474 [D loss: 0.586263, acc: 63.28%] [G loss: 2.683659]\n",
      "epoch:1 step:1475 [D loss: 0.606571, acc: 66.41%] [G loss: 2.542054]\n",
      "epoch:1 step:1476 [D loss: 0.599286, acc: 71.09%] [G loss: 2.826103]\n",
      "epoch:1 step:1477 [D loss: 0.552299, acc: 71.09%] [G loss: 2.543534]\n",
      "epoch:1 step:1478 [D loss: 0.640668, acc: 66.41%] [G loss: 2.662238]\n",
      "epoch:1 step:1479 [D loss: 0.667211, acc: 64.06%] [G loss: 2.443403]\n",
      "epoch:1 step:1480 [D loss: 0.596599, acc: 69.53%] [G loss: 2.689506]\n",
      "epoch:1 step:1481 [D loss: 0.608794, acc: 66.41%] [G loss: 2.859455]\n",
      "epoch:1 step:1482 [D loss: 0.527786, acc: 71.88%] [G loss: 2.725863]\n",
      "epoch:1 step:1483 [D loss: 0.533796, acc: 71.88%] [G loss: 3.003641]\n",
      "epoch:1 step:1484 [D loss: 0.605352, acc: 70.31%] [G loss: 3.000310]\n",
      "epoch:1 step:1485 [D loss: 0.633589, acc: 67.97%] [G loss: 2.917000]\n",
      "epoch:1 step:1486 [D loss: 0.602835, acc: 68.75%] [G loss: 2.932366]\n",
      "epoch:1 step:1487 [D loss: 0.597899, acc: 71.88%] [G loss: 2.709617]\n",
      "epoch:1 step:1488 [D loss: 0.599292, acc: 67.19%] [G loss: 2.822284]\n",
      "epoch:1 step:1489 [D loss: 0.577152, acc: 70.31%] [G loss: 2.621154]\n",
      "epoch:1 step:1490 [D loss: 0.600370, acc: 69.53%] [G loss: 3.048808]\n",
      "epoch:1 step:1491 [D loss: 0.490258, acc: 79.69%] [G loss: 2.824048]\n",
      "epoch:1 step:1492 [D loss: 0.557086, acc: 67.97%] [G loss: 2.950078]\n",
      "epoch:1 step:1493 [D loss: 0.511567, acc: 76.56%] [G loss: 2.901678]\n",
      "epoch:1 step:1494 [D loss: 0.479119, acc: 77.34%] [G loss: 2.955117]\n",
      "epoch:1 step:1495 [D loss: 0.541032, acc: 75.00%] [G loss: 2.656939]\n",
      "epoch:1 step:1496 [D loss: 0.622948, acc: 63.28%] [G loss: 2.518122]\n",
      "epoch:1 step:1497 [D loss: 0.611509, acc: 64.84%] [G loss: 2.668351]\n",
      "epoch:1 step:1498 [D loss: 0.548096, acc: 71.09%] [G loss: 2.802330]\n",
      "epoch:1 step:1499 [D loss: 0.558353, acc: 74.22%] [G loss: 2.621292]\n",
      "epoch:1 step:1500 [D loss: 0.682101, acc: 60.94%] [G loss: 2.447442]\n",
      "epoch:1 step:1501 [D loss: 0.503176, acc: 75.78%] [G loss: 2.702659]\n",
      "epoch:1 step:1502 [D loss: 0.623324, acc: 65.62%] [G loss: 2.658579]\n",
      "epoch:1 step:1503 [D loss: 0.661642, acc: 62.50%] [G loss: 2.814306]\n",
      "epoch:1 step:1504 [D loss: 0.487770, acc: 78.12%] [G loss: 3.062927]\n",
      "epoch:1 step:1505 [D loss: 0.586264, acc: 67.97%] [G loss: 2.779420]\n",
      "epoch:1 step:1506 [D loss: 0.569989, acc: 64.06%] [G loss: 2.705567]\n",
      "epoch:1 step:1507 [D loss: 0.499181, acc: 74.22%] [G loss: 2.918961]\n",
      "epoch:1 step:1508 [D loss: 0.551462, acc: 69.53%] [G loss: 3.159329]\n",
      "epoch:1 step:1509 [D loss: 0.642879, acc: 61.72%] [G loss: 2.630028]\n",
      "epoch:1 step:1510 [D loss: 0.638207, acc: 68.75%] [G loss: 2.658844]\n",
      "epoch:1 step:1511 [D loss: 0.542498, acc: 73.44%] [G loss: 2.931818]\n",
      "epoch:1 step:1512 [D loss: 0.495083, acc: 76.56%] [G loss: 3.038501]\n",
      "epoch:1 step:1513 [D loss: 0.609873, acc: 70.31%] [G loss: 2.902226]\n",
      "epoch:1 step:1514 [D loss: 0.605463, acc: 64.06%] [G loss: 2.652493]\n",
      "epoch:1 step:1515 [D loss: 0.510168, acc: 78.91%] [G loss: 2.747512]\n",
      "epoch:1 step:1516 [D loss: 0.605908, acc: 66.41%] [G loss: 2.886464]\n",
      "epoch:1 step:1517 [D loss: 0.665570, acc: 71.09%] [G loss: 3.048397]\n",
      "epoch:1 step:1518 [D loss: 0.522860, acc: 70.31%] [G loss: 2.841003]\n",
      "epoch:1 step:1519 [D loss: 0.410740, acc: 85.94%] [G loss: 3.247383]\n",
      "epoch:1 step:1520 [D loss: 0.542525, acc: 72.66%] [G loss: 2.971409]\n",
      "epoch:1 step:1521 [D loss: 0.650816, acc: 64.84%] [G loss: 2.848873]\n",
      "epoch:1 step:1522 [D loss: 0.599448, acc: 70.31%] [G loss: 2.572236]\n",
      "epoch:1 step:1523 [D loss: 0.590534, acc: 67.19%] [G loss: 2.636616]\n",
      "epoch:1 step:1524 [D loss: 0.655245, acc: 62.50%] [G loss: 2.625388]\n",
      "epoch:1 step:1525 [D loss: 0.651915, acc: 61.72%] [G loss: 2.628808]\n",
      "epoch:1 step:1526 [D loss: 0.618578, acc: 65.62%] [G loss: 2.882327]\n",
      "epoch:1 step:1527 [D loss: 0.547004, acc: 76.56%] [G loss: 2.763262]\n",
      "epoch:1 step:1528 [D loss: 0.618163, acc: 67.19%] [G loss: 2.606449]\n",
      "epoch:1 step:1529 [D loss: 0.516067, acc: 78.12%] [G loss: 2.712370]\n",
      "epoch:1 step:1530 [D loss: 0.574392, acc: 71.88%] [G loss: 2.361206]\n",
      "epoch:1 step:1531 [D loss: 0.561535, acc: 71.09%] [G loss: 2.852177]\n",
      "epoch:1 step:1532 [D loss: 0.570625, acc: 74.22%] [G loss: 2.700730]\n",
      "epoch:1 step:1533 [D loss: 0.640290, acc: 67.97%] [G loss: 2.885310]\n",
      "epoch:1 step:1534 [D loss: 0.588465, acc: 69.53%] [G loss: 2.907231]\n",
      "epoch:1 step:1535 [D loss: 0.517834, acc: 78.12%] [G loss: 3.003578]\n",
      "epoch:1 step:1536 [D loss: 0.601919, acc: 67.19%] [G loss: 2.547614]\n",
      "epoch:1 step:1537 [D loss: 0.610152, acc: 68.75%] [G loss: 2.619987]\n",
      "epoch:1 step:1538 [D loss: 0.545462, acc: 70.31%] [G loss: 2.620419]\n",
      "epoch:1 step:1539 [D loss: 0.564880, acc: 69.53%] [G loss: 2.573569]\n",
      "epoch:1 step:1540 [D loss: 0.607041, acc: 64.06%] [G loss: 2.838061]\n",
      "epoch:1 step:1541 [D loss: 0.551128, acc: 72.66%] [G loss: 2.895286]\n",
      "epoch:1 step:1542 [D loss: 0.509126, acc: 74.22%] [G loss: 2.941536]\n",
      "epoch:1 step:1543 [D loss: 0.543586, acc: 67.19%] [G loss: 2.890871]\n",
      "epoch:1 step:1544 [D loss: 0.571787, acc: 69.53%] [G loss: 2.780546]\n",
      "epoch:1 step:1545 [D loss: 0.468188, acc: 79.69%] [G loss: 2.774971]\n",
      "epoch:1 step:1546 [D loss: 0.560312, acc: 71.09%] [G loss: 2.870073]\n",
      "epoch:1 step:1547 [D loss: 0.556184, acc: 75.78%] [G loss: 2.771119]\n",
      "epoch:1 step:1548 [D loss: 0.465196, acc: 79.69%] [G loss: 3.051306]\n",
      "epoch:1 step:1549 [D loss: 0.577383, acc: 67.97%] [G loss: 2.974442]\n",
      "epoch:1 step:1550 [D loss: 0.546361, acc: 71.09%] [G loss: 2.960654]\n",
      "epoch:1 step:1551 [D loss: 0.569813, acc: 66.41%] [G loss: 2.917814]\n",
      "epoch:1 step:1552 [D loss: 0.532871, acc: 71.88%] [G loss: 2.813258]\n",
      "epoch:1 step:1553 [D loss: 0.529789, acc: 75.78%] [G loss: 2.816051]\n",
      "epoch:1 step:1554 [D loss: 0.639132, acc: 67.97%] [G loss: 2.560713]\n",
      "epoch:1 step:1555 [D loss: 0.590923, acc: 67.19%] [G loss: 2.633168]\n",
      "epoch:1 step:1556 [D loss: 0.485539, acc: 77.34%] [G loss: 2.948525]\n",
      "epoch:1 step:1557 [D loss: 0.536349, acc: 71.88%] [G loss: 2.822461]\n",
      "epoch:1 step:1558 [D loss: 0.628602, acc: 66.41%] [G loss: 2.754983]\n",
      "epoch:1 step:1559 [D loss: 0.705400, acc: 57.81%] [G loss: 2.542734]\n",
      "epoch:1 step:1560 [D loss: 0.594888, acc: 69.53%] [G loss: 2.841106]\n",
      "epoch:1 step:1561 [D loss: 0.511507, acc: 74.22%] [G loss: 2.836682]\n",
      "epoch:1 step:1562 [D loss: 0.571335, acc: 68.75%] [G loss: 2.989052]\n",
      "epoch:1 step:1563 [D loss: 0.546708, acc: 75.00%] [G loss: 2.717641]\n",
      "epoch:1 step:1564 [D loss: 0.528346, acc: 75.78%] [G loss: 3.072886]\n",
      "epoch:1 step:1565 [D loss: 0.521255, acc: 76.56%] [G loss: 2.879151]\n",
      "epoch:1 step:1566 [D loss: 0.540212, acc: 70.31%] [G loss: 3.106496]\n",
      "epoch:1 step:1567 [D loss: 0.525924, acc: 73.44%] [G loss: 2.946834]\n",
      "epoch:1 step:1568 [D loss: 0.567589, acc: 67.19%] [G loss: 2.764550]\n",
      "epoch:1 step:1569 [D loss: 0.542953, acc: 69.53%] [G loss: 2.905725]\n",
      "epoch:1 step:1570 [D loss: 0.449613, acc: 82.81%] [G loss: 2.979987]\n",
      "epoch:1 step:1571 [D loss: 0.516946, acc: 75.00%] [G loss: 3.407951]\n",
      "epoch:1 step:1572 [D loss: 0.525124, acc: 74.22%] [G loss: 3.427846]\n",
      "epoch:1 step:1573 [D loss: 0.588283, acc: 64.84%] [G loss: 2.706059]\n",
      "epoch:1 step:1574 [D loss: 0.551401, acc: 69.53%] [G loss: 2.581353]\n",
      "epoch:1 step:1575 [D loss: 0.583940, acc: 67.19%] [G loss: 3.053413]\n",
      "epoch:1 step:1576 [D loss: 0.448828, acc: 78.91%] [G loss: 3.558866]\n",
      "epoch:1 step:1577 [D loss: 0.573778, acc: 72.66%] [G loss: 3.007548]\n",
      "epoch:1 step:1578 [D loss: 0.488557, acc: 77.34%] [G loss: 3.219709]\n",
      "epoch:1 step:1579 [D loss: 0.496092, acc: 74.22%] [G loss: 3.274463]\n",
      "epoch:1 step:1580 [D loss: 0.560223, acc: 71.09%] [G loss: 2.690417]\n",
      "epoch:1 step:1581 [D loss: 0.604147, acc: 67.97%] [G loss: 2.926832]\n",
      "epoch:1 step:1582 [D loss: 0.566636, acc: 71.09%] [G loss: 2.965397]\n",
      "epoch:1 step:1583 [D loss: 0.510076, acc: 75.00%] [G loss: 2.911435]\n",
      "epoch:1 step:1584 [D loss: 0.607614, acc: 67.19%] [G loss: 2.988748]\n",
      "epoch:1 step:1585 [D loss: 0.615865, acc: 68.75%] [G loss: 3.164625]\n",
      "epoch:1 step:1586 [D loss: 0.586817, acc: 67.19%] [G loss: 3.162117]\n",
      "epoch:1 step:1587 [D loss: 0.537549, acc: 73.44%] [G loss: 3.023308]\n",
      "epoch:1 step:1588 [D loss: 0.531742, acc: 69.53%] [G loss: 3.268341]\n",
      "epoch:1 step:1589 [D loss: 0.667457, acc: 60.94%] [G loss: 2.498904]\n",
      "epoch:1 step:1590 [D loss: 0.619189, acc: 64.84%] [G loss: 2.695956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1591 [D loss: 0.654864, acc: 64.06%] [G loss: 2.662707]\n",
      "epoch:1 step:1592 [D loss: 0.597997, acc: 67.19%] [G loss: 2.800140]\n",
      "epoch:1 step:1593 [D loss: 0.651648, acc: 57.03%] [G loss: 2.516435]\n",
      "epoch:1 step:1594 [D loss: 0.579156, acc: 70.31%] [G loss: 2.860085]\n",
      "epoch:1 step:1595 [D loss: 0.552099, acc: 71.09%] [G loss: 2.813078]\n",
      "epoch:1 step:1596 [D loss: 0.583208, acc: 68.75%] [G loss: 2.940467]\n",
      "epoch:1 step:1597 [D loss: 0.507530, acc: 76.56%] [G loss: 2.954439]\n",
      "epoch:1 step:1598 [D loss: 0.524213, acc: 78.12%] [G loss: 3.140579]\n",
      "epoch:1 step:1599 [D loss: 0.633740, acc: 62.50%] [G loss: 2.886006]\n",
      "epoch:1 step:1600 [D loss: 0.586848, acc: 69.53%] [G loss: 3.196862]\n",
      "epoch:1 step:1601 [D loss: 0.511379, acc: 75.00%] [G loss: 2.881218]\n",
      "epoch:1 step:1602 [D loss: 0.507105, acc: 74.22%] [G loss: 2.893025]\n",
      "epoch:1 step:1603 [D loss: 0.604907, acc: 70.31%] [G loss: 2.921827]\n",
      "epoch:1 step:1604 [D loss: 0.478549, acc: 81.25%] [G loss: 2.904320]\n",
      "epoch:1 step:1605 [D loss: 0.538805, acc: 67.97%] [G loss: 3.200235]\n",
      "epoch:1 step:1606 [D loss: 0.527668, acc: 72.66%] [G loss: 2.929368]\n",
      "epoch:1 step:1607 [D loss: 0.496604, acc: 75.78%] [G loss: 2.936787]\n",
      "epoch:1 step:1608 [D loss: 0.530547, acc: 74.22%] [G loss: 3.227075]\n",
      "epoch:1 step:1609 [D loss: 0.543445, acc: 73.44%] [G loss: 2.780039]\n",
      "epoch:1 step:1610 [D loss: 0.609014, acc: 68.75%] [G loss: 2.801291]\n",
      "epoch:1 step:1611 [D loss: 0.694245, acc: 58.59%] [G loss: 2.629564]\n",
      "epoch:1 step:1612 [D loss: 0.523221, acc: 72.66%] [G loss: 2.857393]\n",
      "epoch:1 step:1613 [D loss: 0.605746, acc: 62.50%] [G loss: 2.706869]\n",
      "epoch:1 step:1614 [D loss: 0.504592, acc: 75.78%] [G loss: 2.924169]\n",
      "epoch:1 step:1615 [D loss: 0.547886, acc: 67.19%] [G loss: 2.712155]\n",
      "epoch:1 step:1616 [D loss: 0.574806, acc: 69.53%] [G loss: 3.074228]\n",
      "epoch:1 step:1617 [D loss: 0.588143, acc: 64.84%] [G loss: 3.104624]\n",
      "epoch:1 step:1618 [D loss: 0.448061, acc: 80.47%] [G loss: 2.903012]\n",
      "epoch:1 step:1619 [D loss: 0.633542, acc: 66.41%] [G loss: 2.819958]\n",
      "epoch:1 step:1620 [D loss: 0.625510, acc: 64.84%] [G loss: 2.764471]\n",
      "epoch:1 step:1621 [D loss: 0.495757, acc: 78.12%] [G loss: 2.966026]\n",
      "epoch:1 step:1622 [D loss: 0.575518, acc: 71.09%] [G loss: 2.921286]\n",
      "epoch:1 step:1623 [D loss: 0.609761, acc: 71.88%] [G loss: 3.006522]\n",
      "epoch:1 step:1624 [D loss: 0.592185, acc: 70.31%] [G loss: 2.524641]\n",
      "epoch:1 step:1625 [D loss: 0.680940, acc: 62.50%] [G loss: 2.773898]\n",
      "epoch:1 step:1626 [D loss: 0.525001, acc: 76.56%] [G loss: 2.987308]\n",
      "epoch:1 step:1627 [D loss: 0.589612, acc: 71.88%] [G loss: 3.028810]\n",
      "epoch:1 step:1628 [D loss: 0.590548, acc: 69.53%] [G loss: 2.918323]\n",
      "epoch:1 step:1629 [D loss: 0.622232, acc: 67.19%] [G loss: 2.779792]\n",
      "epoch:1 step:1630 [D loss: 0.535445, acc: 74.22%] [G loss: 2.924627]\n",
      "epoch:1 step:1631 [D loss: 0.561533, acc: 71.88%] [G loss: 2.781227]\n",
      "epoch:1 step:1632 [D loss: 0.559494, acc: 71.88%] [G loss: 2.716336]\n",
      "epoch:1 step:1633 [D loss: 0.605837, acc: 65.62%] [G loss: 2.766494]\n",
      "epoch:1 step:1634 [D loss: 0.534693, acc: 72.66%] [G loss: 3.086768]\n",
      "epoch:1 step:1635 [D loss: 0.537752, acc: 71.88%] [G loss: 2.834817]\n",
      "epoch:1 step:1636 [D loss: 0.554136, acc: 71.88%] [G loss: 2.960395]\n",
      "epoch:1 step:1637 [D loss: 0.502755, acc: 75.00%] [G loss: 2.945397]\n",
      "epoch:1 step:1638 [D loss: 0.614110, acc: 64.84%] [G loss: 2.656684]\n",
      "epoch:1 step:1639 [D loss: 0.562198, acc: 67.97%] [G loss: 2.871589]\n",
      "epoch:1 step:1640 [D loss: 0.579079, acc: 69.53%] [G loss: 2.726858]\n",
      "epoch:1 step:1641 [D loss: 0.553621, acc: 69.53%] [G loss: 2.957783]\n",
      "epoch:1 step:1642 [D loss: 0.554108, acc: 73.44%] [G loss: 3.054758]\n",
      "epoch:1 step:1643 [D loss: 0.506986, acc: 69.53%] [G loss: 2.883791]\n",
      "epoch:1 step:1644 [D loss: 0.490757, acc: 82.81%] [G loss: 3.172225]\n",
      "epoch:1 step:1645 [D loss: 0.473576, acc: 75.78%] [G loss: 3.495297]\n",
      "epoch:1 step:1646 [D loss: 0.569905, acc: 69.53%] [G loss: 3.180449]\n",
      "epoch:1 step:1647 [D loss: 0.623664, acc: 69.53%] [G loss: 2.685141]\n",
      "epoch:1 step:1648 [D loss: 0.572281, acc: 68.75%] [G loss: 2.790971]\n",
      "epoch:1 step:1649 [D loss: 0.594723, acc: 67.97%] [G loss: 2.821616]\n",
      "epoch:1 step:1650 [D loss: 0.569949, acc: 71.09%] [G loss: 3.060764]\n",
      "epoch:1 step:1651 [D loss: 0.490404, acc: 79.69%] [G loss: 3.214216]\n",
      "epoch:1 step:1652 [D loss: 0.598068, acc: 70.31%] [G loss: 2.870742]\n",
      "epoch:1 step:1653 [D loss: 0.598560, acc: 67.19%] [G loss: 2.777020]\n",
      "epoch:1 step:1654 [D loss: 0.551329, acc: 70.31%] [G loss: 2.598171]\n",
      "epoch:1 step:1655 [D loss: 0.605547, acc: 67.19%] [G loss: 2.457466]\n",
      "epoch:1 step:1656 [D loss: 0.608167, acc: 63.28%] [G loss: 2.745087]\n",
      "epoch:1 step:1657 [D loss: 0.635887, acc: 65.62%] [G loss: 2.763718]\n",
      "epoch:1 step:1658 [D loss: 0.598907, acc: 65.62%] [G loss: 2.773947]\n",
      "epoch:1 step:1659 [D loss: 0.580883, acc: 70.31%] [G loss: 3.099008]\n",
      "epoch:1 step:1660 [D loss: 0.583889, acc: 67.97%] [G loss: 2.514709]\n",
      "epoch:1 step:1661 [D loss: 0.458557, acc: 76.56%] [G loss: 3.028341]\n",
      "epoch:1 step:1662 [D loss: 0.582551, acc: 73.44%] [G loss: 2.851040]\n",
      "epoch:1 step:1663 [D loss: 0.530293, acc: 75.00%] [G loss: 2.903363]\n",
      "epoch:1 step:1664 [D loss: 0.647000, acc: 70.31%] [G loss: 3.065850]\n",
      "epoch:1 step:1665 [D loss: 0.549066, acc: 70.31%] [G loss: 2.883828]\n",
      "epoch:1 step:1666 [D loss: 0.619059, acc: 65.62%] [G loss: 2.916308]\n",
      "epoch:1 step:1667 [D loss: 0.497780, acc: 74.22%] [G loss: 2.870151]\n",
      "epoch:1 step:1668 [D loss: 0.527440, acc: 75.78%] [G loss: 2.909348]\n",
      "epoch:1 step:1669 [D loss: 0.446530, acc: 76.56%] [G loss: 3.104764]\n",
      "epoch:1 step:1670 [D loss: 0.479947, acc: 78.91%] [G loss: 3.301636]\n",
      "epoch:1 step:1671 [D loss: 0.542976, acc: 73.44%] [G loss: 3.468506]\n",
      "epoch:1 step:1672 [D loss: 0.531529, acc: 70.31%] [G loss: 2.910153]\n",
      "epoch:1 step:1673 [D loss: 0.563543, acc: 71.88%] [G loss: 3.123240]\n",
      "epoch:1 step:1674 [D loss: 0.540980, acc: 74.22%] [G loss: 2.794086]\n",
      "epoch:1 step:1675 [D loss: 0.594084, acc: 66.41%] [G loss: 2.852919]\n",
      "epoch:1 step:1676 [D loss: 0.549038, acc: 71.88%] [G loss: 3.011082]\n",
      "epoch:1 step:1677 [D loss: 0.649472, acc: 64.06%] [G loss: 2.562779]\n",
      "epoch:1 step:1678 [D loss: 0.574592, acc: 67.97%] [G loss: 3.045409]\n",
      "epoch:1 step:1679 [D loss: 0.565895, acc: 71.09%] [G loss: 2.892844]\n",
      "epoch:1 step:1680 [D loss: 0.459188, acc: 75.78%] [G loss: 3.501748]\n",
      "epoch:1 step:1681 [D loss: 0.663696, acc: 70.31%] [G loss: 2.927409]\n",
      "epoch:1 step:1682 [D loss: 0.583248, acc: 67.97%] [G loss: 3.306023]\n",
      "epoch:1 step:1683 [D loss: 0.449207, acc: 77.34%] [G loss: 3.262611]\n",
      "epoch:1 step:1684 [D loss: 0.563707, acc: 66.41%] [G loss: 3.208996]\n",
      "epoch:1 step:1685 [D loss: 0.585843, acc: 66.41%] [G loss: 2.927951]\n",
      "epoch:1 step:1686 [D loss: 0.609301, acc: 64.06%] [G loss: 2.836754]\n",
      "epoch:1 step:1687 [D loss: 0.561228, acc: 70.31%] [G loss: 3.015463]\n",
      "epoch:1 step:1688 [D loss: 0.525854, acc: 76.56%] [G loss: 3.407606]\n",
      "epoch:1 step:1689 [D loss: 0.618355, acc: 67.97%] [G loss: 3.219547]\n",
      "epoch:1 step:1690 [D loss: 0.520005, acc: 67.97%] [G loss: 3.183755]\n",
      "epoch:1 step:1691 [D loss: 0.474018, acc: 75.78%] [G loss: 3.031736]\n",
      "epoch:1 step:1692 [D loss: 0.449083, acc: 82.81%] [G loss: 3.327046]\n",
      "epoch:1 step:1693 [D loss: 0.550867, acc: 74.22%] [G loss: 2.919556]\n",
      "epoch:1 step:1694 [D loss: 0.571668, acc: 74.22%] [G loss: 2.788255]\n",
      "epoch:1 step:1695 [D loss: 0.535924, acc: 71.09%] [G loss: 2.930031]\n",
      "epoch:1 step:1696 [D loss: 0.481780, acc: 82.03%] [G loss: 2.977400]\n",
      "epoch:1 step:1697 [D loss: 0.596495, acc: 71.09%] [G loss: 3.086076]\n",
      "epoch:1 step:1698 [D loss: 0.500189, acc: 75.00%] [G loss: 3.119693]\n",
      "epoch:1 step:1699 [D loss: 0.518330, acc: 72.66%] [G loss: 3.261650]\n",
      "epoch:1 step:1700 [D loss: 0.455882, acc: 79.69%] [G loss: 3.234360]\n",
      "epoch:1 step:1701 [D loss: 0.481695, acc: 72.66%] [G loss: 3.217141]\n",
      "epoch:1 step:1702 [D loss: 0.725071, acc: 57.03%] [G loss: 2.879158]\n",
      "epoch:1 step:1703 [D loss: 0.664470, acc: 60.94%] [G loss: 2.852612]\n",
      "epoch:1 step:1704 [D loss: 0.623900, acc: 67.97%] [G loss: 2.870986]\n",
      "epoch:1 step:1705 [D loss: 0.585967, acc: 65.62%] [G loss: 3.018889]\n",
      "epoch:1 step:1706 [D loss: 0.595202, acc: 71.09%] [G loss: 2.891167]\n",
      "epoch:1 step:1707 [D loss: 0.637095, acc: 63.28%] [G loss: 2.983529]\n",
      "epoch:1 step:1708 [D loss: 0.555576, acc: 68.75%] [G loss: 2.655498]\n",
      "epoch:1 step:1709 [D loss: 0.605290, acc: 64.06%] [G loss: 2.915966]\n",
      "epoch:1 step:1710 [D loss: 0.515471, acc: 71.88%] [G loss: 2.898002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1711 [D loss: 0.562994, acc: 72.66%] [G loss: 2.709359]\n",
      "epoch:1 step:1712 [D loss: 0.551096, acc: 67.19%] [G loss: 2.767333]\n",
      "epoch:1 step:1713 [D loss: 0.449537, acc: 79.69%] [G loss: 2.834620]\n",
      "epoch:1 step:1714 [D loss: 0.641086, acc: 60.94%] [G loss: 2.721129]\n",
      "epoch:1 step:1715 [D loss: 0.675830, acc: 61.72%] [G loss: 2.707504]\n",
      "epoch:1 step:1716 [D loss: 0.645946, acc: 67.19%] [G loss: 2.662582]\n",
      "epoch:1 step:1717 [D loss: 0.556413, acc: 72.66%] [G loss: 2.869569]\n",
      "epoch:1 step:1718 [D loss: 0.475874, acc: 77.34%] [G loss: 2.748772]\n",
      "epoch:1 step:1719 [D loss: 0.631937, acc: 67.19%] [G loss: 2.960094]\n",
      "epoch:1 step:1720 [D loss: 0.587788, acc: 67.19%] [G loss: 3.070412]\n",
      "epoch:1 step:1721 [D loss: 0.499782, acc: 75.78%] [G loss: 3.007437]\n",
      "epoch:1 step:1722 [D loss: 0.527455, acc: 74.22%] [G loss: 2.987102]\n",
      "epoch:1 step:1723 [D loss: 0.508456, acc: 75.00%] [G loss: 3.044581]\n",
      "epoch:1 step:1724 [D loss: 0.577293, acc: 71.88%] [G loss: 2.716133]\n",
      "epoch:1 step:1725 [D loss: 0.557063, acc: 74.22%] [G loss: 2.511212]\n",
      "epoch:1 step:1726 [D loss: 0.571602, acc: 68.75%] [G loss: 3.289598]\n",
      "epoch:1 step:1727 [D loss: 0.537088, acc: 72.66%] [G loss: 3.188524]\n",
      "epoch:1 step:1728 [D loss: 0.571633, acc: 68.75%] [G loss: 3.261029]\n",
      "epoch:1 step:1729 [D loss: 0.441198, acc: 80.47%] [G loss: 3.425055]\n",
      "epoch:1 step:1730 [D loss: 0.565413, acc: 69.53%] [G loss: 2.978018]\n",
      "epoch:1 step:1731 [D loss: 0.631217, acc: 67.19%] [G loss: 2.931539]\n",
      "epoch:1 step:1732 [D loss: 0.472285, acc: 74.22%] [G loss: 3.187716]\n",
      "epoch:1 step:1733 [D loss: 0.550206, acc: 67.19%] [G loss: 3.213178]\n",
      "epoch:1 step:1734 [D loss: 0.551950, acc: 71.88%] [G loss: 2.874168]\n",
      "epoch:1 step:1735 [D loss: 0.493940, acc: 76.56%] [G loss: 3.349147]\n",
      "epoch:1 step:1736 [D loss: 0.634401, acc: 67.97%] [G loss: 2.764933]\n",
      "epoch:1 step:1737 [D loss: 0.608801, acc: 70.31%] [G loss: 2.688088]\n",
      "epoch:1 step:1738 [D loss: 0.503965, acc: 75.78%] [G loss: 2.939015]\n",
      "epoch:1 step:1739 [D loss: 0.552248, acc: 70.31%] [G loss: 3.174410]\n",
      "epoch:1 step:1740 [D loss: 0.482923, acc: 75.78%] [G loss: 3.162384]\n",
      "epoch:1 step:1741 [D loss: 0.540592, acc: 72.66%] [G loss: 2.814239]\n",
      "epoch:1 step:1742 [D loss: 0.546880, acc: 71.09%] [G loss: 3.039645]\n",
      "epoch:1 step:1743 [D loss: 0.465465, acc: 82.03%] [G loss: 3.408063]\n",
      "epoch:1 step:1744 [D loss: 0.487107, acc: 78.12%] [G loss: 3.322445]\n",
      "epoch:1 step:1745 [D loss: 0.515173, acc: 75.00%] [G loss: 2.800038]\n",
      "epoch:1 step:1746 [D loss: 0.602949, acc: 64.06%] [G loss: 2.717199]\n",
      "epoch:1 step:1747 [D loss: 0.587731, acc: 67.97%] [G loss: 2.729439]\n",
      "epoch:1 step:1748 [D loss: 0.574205, acc: 64.06%] [G loss: 2.977802]\n",
      "epoch:1 step:1749 [D loss: 0.552588, acc: 71.09%] [G loss: 2.704722]\n",
      "epoch:1 step:1750 [D loss: 0.586460, acc: 67.19%] [G loss: 3.238216]\n",
      "epoch:1 step:1751 [D loss: 0.581651, acc: 71.88%] [G loss: 2.977235]\n",
      "epoch:1 step:1752 [D loss: 0.589062, acc: 70.31%] [G loss: 3.056150]\n",
      "epoch:1 step:1753 [D loss: 0.607823, acc: 66.41%] [G loss: 3.035226]\n",
      "epoch:1 step:1754 [D loss: 0.537851, acc: 72.66%] [G loss: 2.861637]\n",
      "epoch:1 step:1755 [D loss: 0.572634, acc: 72.66%] [G loss: 2.893877]\n",
      "epoch:1 step:1756 [D loss: 0.556453, acc: 67.19%] [G loss: 3.357692]\n",
      "epoch:1 step:1757 [D loss: 0.534129, acc: 75.00%] [G loss: 3.098550]\n",
      "epoch:1 step:1758 [D loss: 0.493203, acc: 83.59%] [G loss: 3.173977]\n",
      "epoch:1 step:1759 [D loss: 0.499421, acc: 78.12%] [G loss: 3.193223]\n",
      "epoch:1 step:1760 [D loss: 0.530596, acc: 71.09%] [G loss: 3.289019]\n",
      "epoch:1 step:1761 [D loss: 0.695699, acc: 63.28%] [G loss: 2.770751]\n",
      "epoch:1 step:1762 [D loss: 0.560212, acc: 70.31%] [G loss: 2.879623]\n",
      "epoch:1 step:1763 [D loss: 0.639646, acc: 64.06%] [G loss: 2.913491]\n",
      "epoch:1 step:1764 [D loss: 0.516414, acc: 75.00%] [G loss: 2.658646]\n",
      "epoch:1 step:1765 [D loss: 0.611997, acc: 64.06%] [G loss: 2.385030]\n",
      "epoch:1 step:1766 [D loss: 0.580069, acc: 70.31%] [G loss: 2.979697]\n",
      "epoch:1 step:1767 [D loss: 0.490267, acc: 71.88%] [G loss: 2.901859]\n",
      "epoch:1 step:1768 [D loss: 0.534207, acc: 73.44%] [G loss: 3.042461]\n",
      "epoch:1 step:1769 [D loss: 0.445788, acc: 81.25%] [G loss: 3.125455]\n",
      "epoch:1 step:1770 [D loss: 0.571607, acc: 66.41%] [G loss: 3.214408]\n",
      "epoch:1 step:1771 [D loss: 0.585054, acc: 73.44%] [G loss: 3.265776]\n",
      "epoch:1 step:1772 [D loss: 0.526044, acc: 75.78%] [G loss: 2.991551]\n",
      "epoch:1 step:1773 [D loss: 0.575798, acc: 71.09%] [G loss: 2.844289]\n",
      "epoch:1 step:1774 [D loss: 0.500705, acc: 78.12%] [G loss: 2.795080]\n",
      "epoch:1 step:1775 [D loss: 0.583305, acc: 68.75%] [G loss: 3.261039]\n",
      "epoch:1 step:1776 [D loss: 0.624635, acc: 64.06%] [G loss: 2.477815]\n",
      "epoch:1 step:1777 [D loss: 0.547620, acc: 75.00%] [G loss: 2.874782]\n",
      "epoch:1 step:1778 [D loss: 0.503771, acc: 70.31%] [G loss: 3.380911]\n",
      "epoch:1 step:1779 [D loss: 0.451626, acc: 81.25%] [G loss: 2.962817]\n",
      "epoch:1 step:1780 [D loss: 0.571219, acc: 65.62%] [G loss: 3.125233]\n",
      "epoch:1 step:1781 [D loss: 0.719936, acc: 62.50%] [G loss: 2.680199]\n",
      "epoch:1 step:1782 [D loss: 0.553271, acc: 73.44%] [G loss: 2.734617]\n",
      "epoch:1 step:1783 [D loss: 0.489068, acc: 78.91%] [G loss: 2.828946]\n",
      "epoch:1 step:1784 [D loss: 0.563455, acc: 70.31%] [G loss: 3.298999]\n",
      "epoch:1 step:1785 [D loss: 0.470235, acc: 76.56%] [G loss: 3.133977]\n",
      "epoch:1 step:1786 [D loss: 0.649546, acc: 66.41%] [G loss: 2.743485]\n",
      "epoch:1 step:1787 [D loss: 0.507899, acc: 75.78%] [G loss: 2.808668]\n",
      "epoch:1 step:1788 [D loss: 0.523818, acc: 75.78%] [G loss: 2.947806]\n",
      "epoch:1 step:1789 [D loss: 0.542996, acc: 75.78%] [G loss: 2.926773]\n",
      "epoch:1 step:1790 [D loss: 0.549952, acc: 71.88%] [G loss: 3.373215]\n",
      "epoch:1 step:1791 [D loss: 0.482721, acc: 79.69%] [G loss: 3.385409]\n",
      "epoch:1 step:1792 [D loss: 0.684206, acc: 66.41%] [G loss: 2.984278]\n",
      "epoch:1 step:1793 [D loss: 0.631919, acc: 68.75%] [G loss: 3.154245]\n",
      "epoch:1 step:1794 [D loss: 0.507037, acc: 69.53%] [G loss: 2.638749]\n",
      "epoch:1 step:1795 [D loss: 0.744471, acc: 61.72%] [G loss: 2.700268]\n",
      "epoch:1 step:1796 [D loss: 0.652957, acc: 63.28%] [G loss: 2.692200]\n",
      "epoch:1 step:1797 [D loss: 0.551135, acc: 69.53%] [G loss: 3.065151]\n",
      "epoch:1 step:1798 [D loss: 0.614860, acc: 68.75%] [G loss: 2.750515]\n",
      "epoch:1 step:1799 [D loss: 0.571623, acc: 69.53%] [G loss: 3.157672]\n",
      "epoch:1 step:1800 [D loss: 0.505038, acc: 74.22%] [G loss: 2.860952]\n",
      "epoch:1 step:1801 [D loss: 0.521934, acc: 74.22%] [G loss: 3.109822]\n",
      "epoch:1 step:1802 [D loss: 0.569169, acc: 69.53%] [G loss: 3.103706]\n",
      "epoch:1 step:1803 [D loss: 0.583413, acc: 73.44%] [G loss: 3.268825]\n",
      "epoch:1 step:1804 [D loss: 0.637968, acc: 64.06%] [G loss: 2.967916]\n",
      "epoch:1 step:1805 [D loss: 0.582889, acc: 66.41%] [G loss: 2.715658]\n",
      "epoch:1 step:1806 [D loss: 0.537750, acc: 71.09%] [G loss: 3.378908]\n",
      "epoch:1 step:1807 [D loss: 0.600510, acc: 73.44%] [G loss: 3.382841]\n",
      "epoch:1 step:1808 [D loss: 0.528078, acc: 72.66%] [G loss: 2.947526]\n",
      "epoch:1 step:1809 [D loss: 0.579419, acc: 69.53%] [G loss: 3.449684]\n",
      "epoch:1 step:1810 [D loss: 0.500452, acc: 72.66%] [G loss: 3.260909]\n",
      "epoch:1 step:1811 [D loss: 0.610442, acc: 68.75%] [G loss: 3.147902]\n",
      "epoch:1 step:1812 [D loss: 0.515998, acc: 75.78%] [G loss: 3.473526]\n",
      "epoch:1 step:1813 [D loss: 0.538700, acc: 70.31%] [G loss: 2.711629]\n",
      "epoch:1 step:1814 [D loss: 0.483388, acc: 79.69%] [G loss: 2.950562]\n",
      "epoch:1 step:1815 [D loss: 0.541963, acc: 72.66%] [G loss: 3.031711]\n",
      "epoch:1 step:1816 [D loss: 0.545805, acc: 71.88%] [G loss: 2.929013]\n",
      "epoch:1 step:1817 [D loss: 0.608370, acc: 66.41%] [G loss: 2.838562]\n",
      "epoch:1 step:1818 [D loss: 0.581813, acc: 66.41%] [G loss: 2.712085]\n",
      "epoch:1 step:1819 [D loss: 0.517623, acc: 75.00%] [G loss: 3.006747]\n",
      "epoch:1 step:1820 [D loss: 0.592823, acc: 67.19%] [G loss: 2.608352]\n",
      "epoch:1 step:1821 [D loss: 0.612645, acc: 63.28%] [G loss: 3.041168]\n",
      "epoch:1 step:1822 [D loss: 0.530898, acc: 74.22%] [G loss: 3.283082]\n",
      "epoch:1 step:1823 [D loss: 0.513842, acc: 77.34%] [G loss: 2.915724]\n",
      "epoch:1 step:1824 [D loss: 0.506944, acc: 75.78%] [G loss: 3.009407]\n",
      "epoch:1 step:1825 [D loss: 0.450630, acc: 81.25%] [G loss: 3.413840]\n",
      "epoch:1 step:1826 [D loss: 0.509599, acc: 77.34%] [G loss: 3.078215]\n",
      "epoch:1 step:1827 [D loss: 0.472147, acc: 76.56%] [G loss: 3.292901]\n",
      "epoch:1 step:1828 [D loss: 0.660521, acc: 64.06%] [G loss: 3.073001]\n",
      "epoch:1 step:1829 [D loss: 0.698935, acc: 63.28%] [G loss: 2.684718]\n",
      "epoch:1 step:1830 [D loss: 0.574226, acc: 67.97%] [G loss: 2.876095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1831 [D loss: 0.646458, acc: 60.94%] [G loss: 2.988576]\n",
      "epoch:1 step:1832 [D loss: 0.603069, acc: 67.19%] [G loss: 2.845158]\n",
      "epoch:1 step:1833 [D loss: 0.602475, acc: 67.97%] [G loss: 2.855389]\n",
      "epoch:1 step:1834 [D loss: 0.572934, acc: 73.44%] [G loss: 2.906661]\n",
      "epoch:1 step:1835 [D loss: 0.525582, acc: 78.12%] [G loss: 2.981470]\n",
      "epoch:1 step:1836 [D loss: 0.523447, acc: 77.34%] [G loss: 2.777658]\n",
      "epoch:1 step:1837 [D loss: 0.560646, acc: 71.09%] [G loss: 3.235674]\n",
      "epoch:1 step:1838 [D loss: 0.497235, acc: 77.34%] [G loss: 3.274343]\n",
      "epoch:1 step:1839 [D loss: 0.613136, acc: 64.06%] [G loss: 2.841180]\n",
      "epoch:1 step:1840 [D loss: 0.579210, acc: 70.31%] [G loss: 2.861847]\n",
      "epoch:1 step:1841 [D loss: 0.571871, acc: 70.31%] [G loss: 2.990943]\n",
      "epoch:1 step:1842 [D loss: 0.646074, acc: 68.75%] [G loss: 2.765239]\n",
      "epoch:1 step:1843 [D loss: 0.501807, acc: 75.00%] [G loss: 3.001376]\n",
      "epoch:1 step:1844 [D loss: 0.535047, acc: 71.09%] [G loss: 3.148323]\n",
      "epoch:1 step:1845 [D loss: 0.532568, acc: 73.44%] [G loss: 3.043382]\n",
      "epoch:1 step:1846 [D loss: 0.526522, acc: 75.00%] [G loss: 3.241569]\n",
      "epoch:1 step:1847 [D loss: 0.491670, acc: 77.34%] [G loss: 3.583063]\n",
      "epoch:1 step:1848 [D loss: 0.561452, acc: 71.09%] [G loss: 3.369464]\n",
      "epoch:1 step:1849 [D loss: 0.562085, acc: 71.88%] [G loss: 3.467338]\n",
      "epoch:1 step:1850 [D loss: 0.565168, acc: 67.19%] [G loss: 3.025014]\n",
      "epoch:1 step:1851 [D loss: 0.522436, acc: 70.31%] [G loss: 3.296653]\n",
      "epoch:1 step:1852 [D loss: 0.642276, acc: 67.19%] [G loss: 2.800338]\n",
      "epoch:1 step:1853 [D loss: 0.603651, acc: 68.75%] [G loss: 2.793805]\n",
      "epoch:1 step:1854 [D loss: 0.558090, acc: 78.91%] [G loss: 3.005486]\n",
      "epoch:1 step:1855 [D loss: 0.513062, acc: 74.22%] [G loss: 3.356470]\n",
      "epoch:1 step:1856 [D loss: 0.554752, acc: 71.09%] [G loss: 3.030493]\n",
      "epoch:1 step:1857 [D loss: 0.764509, acc: 57.81%] [G loss: 2.732291]\n",
      "epoch:1 step:1858 [D loss: 0.526189, acc: 78.91%] [G loss: 3.300945]\n",
      "epoch:1 step:1859 [D loss: 0.553411, acc: 69.53%] [G loss: 3.386101]\n",
      "epoch:1 step:1860 [D loss: 0.494208, acc: 78.12%] [G loss: 3.617495]\n",
      "epoch:1 step:1861 [D loss: 0.463834, acc: 75.78%] [G loss: 3.245433]\n",
      "epoch:1 step:1862 [D loss: 0.409952, acc: 85.94%] [G loss: 3.930034]\n",
      "epoch:1 step:1863 [D loss: 0.426274, acc: 82.81%] [G loss: 3.621126]\n",
      "epoch:1 step:1864 [D loss: 0.525769, acc: 67.97%] [G loss: 3.230387]\n",
      "epoch:1 step:1865 [D loss: 0.679243, acc: 64.06%] [G loss: 3.140224]\n",
      "epoch:1 step:1866 [D loss: 0.641562, acc: 66.41%] [G loss: 3.667217]\n",
      "epoch:1 step:1867 [D loss: 0.474669, acc: 74.22%] [G loss: 3.692418]\n",
      "epoch:1 step:1868 [D loss: 0.545995, acc: 69.53%] [G loss: 3.123226]\n",
      "epoch:1 step:1869 [D loss: 0.582027, acc: 70.31%] [G loss: 3.122133]\n",
      "epoch:1 step:1870 [D loss: 0.507024, acc: 72.66%] [G loss: 3.278182]\n",
      "epoch:1 step:1871 [D loss: 0.526532, acc: 74.22%] [G loss: 3.028229]\n",
      "epoch:1 step:1872 [D loss: 0.514449, acc: 72.66%] [G loss: 3.119053]\n",
      "epoch:1 step:1873 [D loss: 0.324628, acc: 87.50%] [G loss: 3.874162]\n",
      "epoch:1 step:1874 [D loss: 0.599269, acc: 68.75%] [G loss: 3.763336]\n",
      "epoch:2 step:1875 [D loss: 0.597056, acc: 72.66%] [G loss: 3.387771]\n",
      "epoch:2 step:1876 [D loss: 0.539653, acc: 74.22%] [G loss: 3.285519]\n",
      "epoch:2 step:1877 [D loss: 0.534639, acc: 69.53%] [G loss: 2.784048]\n",
      "epoch:2 step:1878 [D loss: 0.592367, acc: 69.53%] [G loss: 3.035342]\n",
      "epoch:2 step:1879 [D loss: 0.515213, acc: 75.78%] [G loss: 3.097938]\n",
      "epoch:2 step:1880 [D loss: 0.537730, acc: 68.75%] [G loss: 3.224142]\n",
      "epoch:2 step:1881 [D loss: 0.543714, acc: 71.09%] [G loss: 2.957619]\n",
      "epoch:2 step:1882 [D loss: 0.603023, acc: 68.75%] [G loss: 3.315342]\n",
      "epoch:2 step:1883 [D loss: 0.519338, acc: 74.22%] [G loss: 2.901157]\n",
      "epoch:2 step:1884 [D loss: 0.596940, acc: 70.31%] [G loss: 3.179604]\n",
      "epoch:2 step:1885 [D loss: 0.514801, acc: 72.66%] [G loss: 3.466693]\n",
      "epoch:2 step:1886 [D loss: 0.539181, acc: 67.19%] [G loss: 3.131287]\n",
      "epoch:2 step:1887 [D loss: 0.490328, acc: 80.47%] [G loss: 3.187770]\n",
      "epoch:2 step:1888 [D loss: 0.644432, acc: 71.88%] [G loss: 3.202336]\n",
      "epoch:2 step:1889 [D loss: 0.498063, acc: 72.66%] [G loss: 3.125366]\n",
      "epoch:2 step:1890 [D loss: 0.493603, acc: 75.00%] [G loss: 3.394222]\n",
      "epoch:2 step:1891 [D loss: 0.508654, acc: 71.88%] [G loss: 2.900438]\n",
      "epoch:2 step:1892 [D loss: 0.615854, acc: 60.94%] [G loss: 2.848315]\n",
      "epoch:2 step:1893 [D loss: 0.606373, acc: 68.75%] [G loss: 3.137939]\n",
      "epoch:2 step:1894 [D loss: 0.614943, acc: 68.75%] [G loss: 2.654537]\n",
      "epoch:2 step:1895 [D loss: 0.532208, acc: 76.56%] [G loss: 3.082915]\n",
      "epoch:2 step:1896 [D loss: 0.524891, acc: 75.00%] [G loss: 3.255529]\n",
      "epoch:2 step:1897 [D loss: 0.581726, acc: 69.53%] [G loss: 3.079155]\n",
      "epoch:2 step:1898 [D loss: 0.619222, acc: 68.75%] [G loss: 3.158537]\n",
      "epoch:2 step:1899 [D loss: 0.579669, acc: 71.88%] [G loss: 3.027626]\n",
      "epoch:2 step:1900 [D loss: 0.618402, acc: 64.84%] [G loss: 2.748010]\n",
      "epoch:2 step:1901 [D loss: 0.542267, acc: 70.31%] [G loss: 2.952858]\n",
      "epoch:2 step:1902 [D loss: 0.565427, acc: 71.09%] [G loss: 3.090912]\n",
      "epoch:2 step:1903 [D loss: 0.460729, acc: 82.03%] [G loss: 2.951268]\n",
      "epoch:2 step:1904 [D loss: 0.547787, acc: 74.22%] [G loss: 2.782705]\n",
      "epoch:2 step:1905 [D loss: 0.528799, acc: 71.09%] [G loss: 2.867033]\n",
      "epoch:2 step:1906 [D loss: 0.558090, acc: 70.31%] [G loss: 3.337313]\n",
      "epoch:2 step:1907 [D loss: 0.580518, acc: 67.97%] [G loss: 2.830452]\n",
      "epoch:2 step:1908 [D loss: 0.445658, acc: 82.03%] [G loss: 3.270965]\n",
      "epoch:2 step:1909 [D loss: 0.540018, acc: 74.22%] [G loss: 3.116877]\n",
      "epoch:2 step:1910 [D loss: 0.590988, acc: 67.97%] [G loss: 3.084097]\n",
      "epoch:2 step:1911 [D loss: 0.447362, acc: 78.91%] [G loss: 2.955969]\n",
      "epoch:2 step:1912 [D loss: 0.577269, acc: 74.22%] [G loss: 2.843163]\n",
      "epoch:2 step:1913 [D loss: 0.428606, acc: 81.25%] [G loss: 3.408247]\n",
      "epoch:2 step:1914 [D loss: 0.489695, acc: 77.34%] [G loss: 3.704926]\n",
      "epoch:2 step:1915 [D loss: 0.625283, acc: 63.28%] [G loss: 3.258666]\n",
      "epoch:2 step:1916 [D loss: 0.529659, acc: 77.34%] [G loss: 3.056175]\n",
      "epoch:2 step:1917 [D loss: 0.624772, acc: 65.62%] [G loss: 2.804782]\n",
      "epoch:2 step:1918 [D loss: 0.617998, acc: 68.75%] [G loss: 2.890150]\n",
      "epoch:2 step:1919 [D loss: 0.559657, acc: 75.00%] [G loss: 2.796373]\n",
      "epoch:2 step:1920 [D loss: 0.586296, acc: 70.31%] [G loss: 2.938490]\n",
      "epoch:2 step:1921 [D loss: 0.582570, acc: 67.97%] [G loss: 2.908309]\n",
      "epoch:2 step:1922 [D loss: 0.590662, acc: 62.50%] [G loss: 3.042136]\n",
      "epoch:2 step:1923 [D loss: 0.502015, acc: 77.34%] [G loss: 3.138405]\n",
      "epoch:2 step:1924 [D loss: 0.554270, acc: 69.53%] [G loss: 2.969856]\n",
      "epoch:2 step:1925 [D loss: 0.511780, acc: 73.44%] [G loss: 3.149064]\n",
      "epoch:2 step:1926 [D loss: 0.606096, acc: 70.31%] [G loss: 2.726269]\n",
      "epoch:2 step:1927 [D loss: 0.521537, acc: 72.66%] [G loss: 3.162811]\n",
      "epoch:2 step:1928 [D loss: 0.514877, acc: 70.31%] [G loss: 3.298527]\n",
      "epoch:2 step:1929 [D loss: 0.549003, acc: 72.66%] [G loss: 3.129596]\n",
      "epoch:2 step:1930 [D loss: 0.616574, acc: 65.62%] [G loss: 3.205298]\n",
      "epoch:2 step:1931 [D loss: 0.586351, acc: 69.53%] [G loss: 3.148830]\n",
      "epoch:2 step:1932 [D loss: 0.497280, acc: 76.56%] [G loss: 3.084263]\n",
      "epoch:2 step:1933 [D loss: 0.534661, acc: 77.34%] [G loss: 3.151892]\n",
      "epoch:2 step:1934 [D loss: 0.562422, acc: 70.31%] [G loss: 3.081527]\n",
      "epoch:2 step:1935 [D loss: 0.559619, acc: 72.66%] [G loss: 3.074610]\n",
      "epoch:2 step:1936 [D loss: 0.559559, acc: 71.88%] [G loss: 2.824151]\n",
      "epoch:2 step:1937 [D loss: 0.578166, acc: 69.53%] [G loss: 2.985258]\n",
      "epoch:2 step:1938 [D loss: 0.597496, acc: 70.31%] [G loss: 2.982589]\n",
      "epoch:2 step:1939 [D loss: 0.629969, acc: 69.53%] [G loss: 2.946962]\n",
      "epoch:2 step:1940 [D loss: 0.535411, acc: 71.88%] [G loss: 3.001565]\n",
      "epoch:2 step:1941 [D loss: 0.509468, acc: 73.44%] [G loss: 3.077752]\n",
      "epoch:2 step:1942 [D loss: 0.657211, acc: 62.50%] [G loss: 2.719981]\n",
      "epoch:2 step:1943 [D loss: 0.594091, acc: 68.75%] [G loss: 2.965215]\n",
      "epoch:2 step:1944 [D loss: 0.538023, acc: 69.53%] [G loss: 2.887894]\n",
      "epoch:2 step:1945 [D loss: 0.565052, acc: 68.75%] [G loss: 3.010682]\n",
      "epoch:2 step:1946 [D loss: 0.561569, acc: 72.66%] [G loss: 3.268160]\n",
      "epoch:2 step:1947 [D loss: 0.544355, acc: 71.09%] [G loss: 3.137973]\n",
      "epoch:2 step:1948 [D loss: 0.641915, acc: 71.09%] [G loss: 3.167392]\n",
      "epoch:2 step:1949 [D loss: 0.574318, acc: 71.88%] [G loss: 3.338729]\n",
      "epoch:2 step:1950 [D loss: 0.473622, acc: 78.12%] [G loss: 3.597056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1951 [D loss: 0.473265, acc: 80.47%] [G loss: 4.142004]\n",
      "epoch:2 step:1952 [D loss: 0.620711, acc: 67.97%] [G loss: 3.128745]\n",
      "epoch:2 step:1953 [D loss: 0.557747, acc: 71.88%] [G loss: 2.957850]\n",
      "epoch:2 step:1954 [D loss: 0.512536, acc: 76.56%] [G loss: 2.933316]\n",
      "epoch:2 step:1955 [D loss: 0.535074, acc: 73.44%] [G loss: 3.083510]\n",
      "epoch:2 step:1956 [D loss: 0.487146, acc: 75.78%] [G loss: 3.381968]\n",
      "epoch:2 step:1957 [D loss: 0.525225, acc: 76.56%] [G loss: 2.938536]\n",
      "epoch:2 step:1958 [D loss: 0.654343, acc: 59.38%] [G loss: 3.001598]\n",
      "epoch:2 step:1959 [D loss: 0.606081, acc: 67.19%] [G loss: 2.987396]\n",
      "epoch:2 step:1960 [D loss: 0.491894, acc: 79.69%] [G loss: 2.893879]\n",
      "epoch:2 step:1961 [D loss: 0.543291, acc: 72.66%] [G loss: 3.153928]\n",
      "epoch:2 step:1962 [D loss: 0.488808, acc: 75.00%] [G loss: 3.121346]\n",
      "epoch:2 step:1963 [D loss: 0.463657, acc: 77.34%] [G loss: 3.229422]\n",
      "epoch:2 step:1964 [D loss: 0.638479, acc: 72.66%] [G loss: 2.859421]\n",
      "epoch:2 step:1965 [D loss: 0.560157, acc: 71.09%] [G loss: 3.247550]\n",
      "epoch:2 step:1966 [D loss: 0.528306, acc: 78.12%] [G loss: 3.084672]\n",
      "epoch:2 step:1967 [D loss: 0.668606, acc: 64.06%] [G loss: 3.275547]\n",
      "epoch:2 step:1968 [D loss: 0.604716, acc: 67.97%] [G loss: 2.886966]\n",
      "epoch:2 step:1969 [D loss: 0.582622, acc: 70.31%] [G loss: 3.311388]\n",
      "epoch:2 step:1970 [D loss: 0.471259, acc: 76.56%] [G loss: 3.286930]\n",
      "epoch:2 step:1971 [D loss: 0.501707, acc: 78.91%] [G loss: 2.722494]\n",
      "epoch:2 step:1972 [D loss: 0.509089, acc: 72.66%] [G loss: 3.001489]\n",
      "epoch:2 step:1973 [D loss: 0.635357, acc: 69.53%] [G loss: 3.329772]\n",
      "epoch:2 step:1974 [D loss: 0.616527, acc: 67.19%] [G loss: 3.139974]\n",
      "epoch:2 step:1975 [D loss: 0.543171, acc: 71.09%] [G loss: 2.759578]\n",
      "epoch:2 step:1976 [D loss: 0.663088, acc: 67.97%] [G loss: 2.870761]\n",
      "epoch:2 step:1977 [D loss: 0.535247, acc: 71.88%] [G loss: 3.479542]\n",
      "epoch:2 step:1978 [D loss: 0.599079, acc: 71.09%] [G loss: 2.950523]\n",
      "epoch:2 step:1979 [D loss: 0.563184, acc: 71.09%] [G loss: 2.832779]\n",
      "epoch:2 step:1980 [D loss: 0.596195, acc: 67.97%] [G loss: 2.965273]\n",
      "epoch:2 step:1981 [D loss: 0.498609, acc: 75.78%] [G loss: 2.950959]\n",
      "epoch:2 step:1982 [D loss: 0.568869, acc: 71.09%] [G loss: 3.083863]\n",
      "epoch:2 step:1983 [D loss: 0.615100, acc: 63.28%] [G loss: 3.254179]\n",
      "epoch:2 step:1984 [D loss: 0.529647, acc: 74.22%] [G loss: 3.310992]\n",
      "epoch:2 step:1985 [D loss: 0.617569, acc: 71.09%] [G loss: 2.978203]\n",
      "epoch:2 step:1986 [D loss: 0.604346, acc: 67.97%] [G loss: 2.942070]\n",
      "epoch:2 step:1987 [D loss: 0.613497, acc: 66.41%] [G loss: 2.881914]\n",
      "epoch:2 step:1988 [D loss: 0.554813, acc: 69.53%] [G loss: 2.845675]\n",
      "epoch:2 step:1989 [D loss: 0.570313, acc: 72.66%] [G loss: 2.866416]\n",
      "epoch:2 step:1990 [D loss: 0.670069, acc: 55.47%] [G loss: 2.762094]\n",
      "epoch:2 step:1991 [D loss: 0.531049, acc: 71.09%] [G loss: 3.111294]\n",
      "epoch:2 step:1992 [D loss: 0.441139, acc: 77.34%] [G loss: 3.084316]\n",
      "epoch:2 step:1993 [D loss: 0.496959, acc: 78.91%] [G loss: 3.497170]\n",
      "epoch:2 step:1994 [D loss: 0.608396, acc: 68.75%] [G loss: 2.866808]\n",
      "epoch:2 step:1995 [D loss: 0.527612, acc: 68.75%] [G loss: 3.096044]\n",
      "epoch:2 step:1996 [D loss: 0.590143, acc: 65.62%] [G loss: 2.899669]\n",
      "epoch:2 step:1997 [D loss: 0.609679, acc: 66.41%] [G loss: 3.194828]\n",
      "epoch:2 step:1998 [D loss: 0.613242, acc: 59.38%] [G loss: 2.832577]\n",
      "epoch:2 step:1999 [D loss: 0.572697, acc: 67.97%] [G loss: 3.010114]\n",
      "epoch:2 step:2000 [D loss: 0.541479, acc: 71.88%] [G loss: 2.990476]\n",
      "epoch:2 step:2001 [D loss: 0.505074, acc: 76.56%] [G loss: 3.244238]\n",
      "epoch:2 step:2002 [D loss: 0.578406, acc: 67.19%] [G loss: 3.228307]\n",
      "epoch:2 step:2003 [D loss: 0.594526, acc: 68.75%] [G loss: 2.954975]\n",
      "epoch:2 step:2004 [D loss: 0.617024, acc: 67.97%] [G loss: 3.003690]\n",
      "epoch:2 step:2005 [D loss: 0.542762, acc: 73.44%] [G loss: 3.300386]\n",
      "epoch:2 step:2006 [D loss: 0.628851, acc: 65.62%] [G loss: 2.831695]\n",
      "epoch:2 step:2007 [D loss: 0.588482, acc: 67.19%] [G loss: 2.956048]\n",
      "epoch:2 step:2008 [D loss: 0.567064, acc: 72.66%] [G loss: 3.232020]\n",
      "epoch:2 step:2009 [D loss: 0.525814, acc: 73.44%] [G loss: 3.044010]\n",
      "epoch:2 step:2010 [D loss: 0.529041, acc: 76.56%] [G loss: 2.964575]\n",
      "epoch:2 step:2011 [D loss: 0.530988, acc: 77.34%] [G loss: 2.789960]\n",
      "epoch:2 step:2012 [D loss: 0.534686, acc: 69.53%] [G loss: 3.100998]\n",
      "epoch:2 step:2013 [D loss: 0.618674, acc: 67.97%] [G loss: 2.650198]\n",
      "epoch:2 step:2014 [D loss: 0.583098, acc: 68.75%] [G loss: 3.061328]\n",
      "epoch:2 step:2015 [D loss: 0.504377, acc: 77.34%] [G loss: 3.136225]\n",
      "epoch:2 step:2016 [D loss: 0.607846, acc: 71.88%] [G loss: 2.998276]\n",
      "epoch:2 step:2017 [D loss: 0.622422, acc: 67.19%] [G loss: 2.794757]\n",
      "epoch:2 step:2018 [D loss: 0.531001, acc: 74.22%] [G loss: 2.856433]\n",
      "epoch:2 step:2019 [D loss: 0.631216, acc: 60.94%] [G loss: 3.120246]\n",
      "epoch:2 step:2020 [D loss: 0.646688, acc: 66.41%] [G loss: 2.800323]\n",
      "epoch:2 step:2021 [D loss: 0.633967, acc: 65.62%] [G loss: 2.949702]\n",
      "epoch:2 step:2022 [D loss: 0.674008, acc: 62.50%] [G loss: 2.890777]\n",
      "epoch:2 step:2023 [D loss: 0.518951, acc: 78.12%] [G loss: 3.296742]\n",
      "epoch:2 step:2024 [D loss: 0.542989, acc: 77.34%] [G loss: 3.079574]\n",
      "epoch:2 step:2025 [D loss: 0.507775, acc: 75.78%] [G loss: 3.262451]\n",
      "epoch:2 step:2026 [D loss: 0.536404, acc: 72.66%] [G loss: 3.431708]\n",
      "epoch:2 step:2027 [D loss: 0.497068, acc: 75.78%] [G loss: 3.280464]\n",
      "epoch:2 step:2028 [D loss: 0.545710, acc: 68.75%] [G loss: 2.522649]\n",
      "epoch:2 step:2029 [D loss: 0.532943, acc: 68.75%] [G loss: 3.128237]\n",
      "epoch:2 step:2030 [D loss: 0.582308, acc: 70.31%] [G loss: 3.048311]\n",
      "epoch:2 step:2031 [D loss: 0.510282, acc: 76.56%] [G loss: 2.868012]\n",
      "epoch:2 step:2032 [D loss: 0.577050, acc: 71.09%] [G loss: 2.878807]\n",
      "epoch:2 step:2033 [D loss: 0.547801, acc: 73.44%] [G loss: 3.136570]\n",
      "epoch:2 step:2034 [D loss: 0.578879, acc: 71.09%] [G loss: 2.889061]\n",
      "epoch:2 step:2035 [D loss: 0.662263, acc: 63.28%] [G loss: 2.931276]\n",
      "epoch:2 step:2036 [D loss: 0.620324, acc: 69.53%] [G loss: 3.116464]\n",
      "epoch:2 step:2037 [D loss: 0.438132, acc: 84.38%] [G loss: 2.979281]\n",
      "epoch:2 step:2038 [D loss: 0.591476, acc: 67.97%] [G loss: 3.083543]\n",
      "epoch:2 step:2039 [D loss: 0.539026, acc: 70.31%] [G loss: 3.220381]\n",
      "epoch:2 step:2040 [D loss: 0.525870, acc: 75.78%] [G loss: 3.230466]\n",
      "epoch:2 step:2041 [D loss: 0.738114, acc: 58.59%] [G loss: 2.902101]\n",
      "epoch:2 step:2042 [D loss: 0.637763, acc: 69.53%] [G loss: 2.734985]\n",
      "epoch:2 step:2043 [D loss: 0.582714, acc: 70.31%] [G loss: 2.717484]\n",
      "epoch:2 step:2044 [D loss: 0.599264, acc: 68.75%] [G loss: 2.984496]\n",
      "epoch:2 step:2045 [D loss: 0.541076, acc: 78.12%] [G loss: 3.174071]\n",
      "epoch:2 step:2046 [D loss: 0.571608, acc: 64.84%] [G loss: 3.027742]\n",
      "epoch:2 step:2047 [D loss: 0.580062, acc: 70.31%] [G loss: 2.987369]\n",
      "epoch:2 step:2048 [D loss: 0.510340, acc: 76.56%] [G loss: 3.085793]\n",
      "epoch:2 step:2049 [D loss: 0.556828, acc: 72.66%] [G loss: 2.874289]\n",
      "epoch:2 step:2050 [D loss: 0.505331, acc: 77.34%] [G loss: 3.039886]\n",
      "epoch:2 step:2051 [D loss: 0.486554, acc: 77.34%] [G loss: 2.921716]\n",
      "epoch:2 step:2052 [D loss: 0.491905, acc: 77.34%] [G loss: 3.039583]\n",
      "epoch:2 step:2053 [D loss: 0.543004, acc: 73.44%] [G loss: 2.969638]\n",
      "epoch:2 step:2054 [D loss: 0.666705, acc: 64.84%] [G loss: 2.551764]\n",
      "epoch:2 step:2055 [D loss: 0.530061, acc: 73.44%] [G loss: 2.936797]\n",
      "epoch:2 step:2056 [D loss: 0.676319, acc: 60.16%] [G loss: 2.907817]\n",
      "epoch:2 step:2057 [D loss: 0.540649, acc: 68.75%] [G loss: 3.032549]\n",
      "epoch:2 step:2058 [D loss: 0.558539, acc: 71.09%] [G loss: 3.012002]\n",
      "epoch:2 step:2059 [D loss: 0.627796, acc: 71.09%] [G loss: 2.855778]\n",
      "epoch:2 step:2060 [D loss: 0.564443, acc: 70.31%] [G loss: 3.043850]\n",
      "epoch:2 step:2061 [D loss: 0.593629, acc: 67.97%] [G loss: 2.820971]\n",
      "epoch:2 step:2062 [D loss: 0.523240, acc: 75.78%] [G loss: 3.427306]\n",
      "epoch:2 step:2063 [D loss: 0.618303, acc: 65.62%] [G loss: 2.841173]\n",
      "epoch:2 step:2064 [D loss: 0.505435, acc: 73.44%] [G loss: 2.955691]\n",
      "epoch:2 step:2065 [D loss: 0.472454, acc: 75.78%] [G loss: 3.142035]\n",
      "epoch:2 step:2066 [D loss: 0.419729, acc: 82.03%] [G loss: 3.367602]\n",
      "epoch:2 step:2067 [D loss: 0.455077, acc: 78.12%] [G loss: 3.015557]\n",
      "epoch:2 step:2068 [D loss: 0.562124, acc: 68.75%] [G loss: 3.486225]\n",
      "epoch:2 step:2069 [D loss: 0.531696, acc: 73.44%] [G loss: 3.037879]\n",
      "epoch:2 step:2070 [D loss: 0.598782, acc: 67.97%] [G loss: 3.047169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2071 [D loss: 0.560750, acc: 74.22%] [G loss: 3.024474]\n",
      "epoch:2 step:2072 [D loss: 0.566703, acc: 72.66%] [G loss: 3.152307]\n",
      "epoch:2 step:2073 [D loss: 0.489471, acc: 78.91%] [G loss: 2.978838]\n",
      "epoch:2 step:2074 [D loss: 0.537801, acc: 71.09%] [G loss: 3.148653]\n",
      "epoch:2 step:2075 [D loss: 0.567700, acc: 73.44%] [G loss: 3.084134]\n",
      "epoch:2 step:2076 [D loss: 0.548517, acc: 72.66%] [G loss: 3.145833]\n",
      "epoch:2 step:2077 [D loss: 0.615612, acc: 66.41%] [G loss: 2.890224]\n",
      "epoch:2 step:2078 [D loss: 0.469873, acc: 79.69%] [G loss: 3.072931]\n",
      "epoch:2 step:2079 [D loss: 0.548021, acc: 72.66%] [G loss: 3.070232]\n",
      "epoch:2 step:2080 [D loss: 0.534884, acc: 74.22%] [G loss: 3.357548]\n",
      "epoch:2 step:2081 [D loss: 0.498366, acc: 76.56%] [G loss: 3.499123]\n",
      "epoch:2 step:2082 [D loss: 0.496011, acc: 74.22%] [G loss: 3.425446]\n",
      "epoch:2 step:2083 [D loss: 0.535995, acc: 78.12%] [G loss: 3.124804]\n",
      "epoch:2 step:2084 [D loss: 0.585017, acc: 66.41%] [G loss: 3.139231]\n",
      "epoch:2 step:2085 [D loss: 0.527681, acc: 73.44%] [G loss: 3.046741]\n",
      "epoch:2 step:2086 [D loss: 0.556996, acc: 71.88%] [G loss: 3.181704]\n",
      "epoch:2 step:2087 [D loss: 0.554707, acc: 69.53%] [G loss: 3.309416]\n",
      "epoch:2 step:2088 [D loss: 0.607763, acc: 69.53%] [G loss: 2.778365]\n",
      "epoch:2 step:2089 [D loss: 0.618321, acc: 67.97%] [G loss: 2.728966]\n",
      "epoch:2 step:2090 [D loss: 0.536203, acc: 75.00%] [G loss: 2.872167]\n",
      "epoch:2 step:2091 [D loss: 0.535908, acc: 73.44%] [G loss: 3.461963]\n",
      "epoch:2 step:2092 [D loss: 0.584127, acc: 70.31%] [G loss: 2.796383]\n",
      "epoch:2 step:2093 [D loss: 0.562068, acc: 72.66%] [G loss: 2.922290]\n",
      "epoch:2 step:2094 [D loss: 0.554255, acc: 67.19%] [G loss: 3.059055]\n",
      "epoch:2 step:2095 [D loss: 0.496321, acc: 74.22%] [G loss: 3.064630]\n",
      "epoch:2 step:2096 [D loss: 0.511888, acc: 71.09%] [G loss: 3.208363]\n",
      "epoch:2 step:2097 [D loss: 0.482918, acc: 75.78%] [G loss: 3.683908]\n",
      "epoch:2 step:2098 [D loss: 0.543189, acc: 68.75%] [G loss: 3.217189]\n",
      "epoch:2 step:2099 [D loss: 0.611830, acc: 66.41%] [G loss: 2.830124]\n",
      "epoch:2 step:2100 [D loss: 0.454400, acc: 75.78%] [G loss: 3.200669]\n",
      "epoch:2 step:2101 [D loss: 0.621864, acc: 69.53%] [G loss: 2.665379]\n",
      "epoch:2 step:2102 [D loss: 0.598124, acc: 68.75%] [G loss: 2.858018]\n",
      "epoch:2 step:2103 [D loss: 0.604940, acc: 58.59%] [G loss: 2.783120]\n",
      "epoch:2 step:2104 [D loss: 0.519849, acc: 71.88%] [G loss: 3.237121]\n",
      "epoch:2 step:2105 [D loss: 0.439353, acc: 76.56%] [G loss: 3.325572]\n",
      "epoch:2 step:2106 [D loss: 0.449993, acc: 78.91%] [G loss: 3.571374]\n",
      "epoch:2 step:2107 [D loss: 0.572260, acc: 69.53%] [G loss: 3.412131]\n",
      "epoch:2 step:2108 [D loss: 0.602468, acc: 65.62%] [G loss: 3.210838]\n",
      "epoch:2 step:2109 [D loss: 0.589007, acc: 64.84%] [G loss: 3.294700]\n",
      "epoch:2 step:2110 [D loss: 0.525697, acc: 75.00%] [G loss: 2.937491]\n",
      "epoch:2 step:2111 [D loss: 0.490667, acc: 77.34%] [G loss: 3.365089]\n",
      "epoch:2 step:2112 [D loss: 0.558963, acc: 75.78%] [G loss: 2.872753]\n",
      "epoch:2 step:2113 [D loss: 0.576580, acc: 67.97%] [G loss: 3.028327]\n",
      "epoch:2 step:2114 [D loss: 0.493125, acc: 76.56%] [G loss: 2.832376]\n",
      "epoch:2 step:2115 [D loss: 0.544467, acc: 72.66%] [G loss: 2.963584]\n",
      "epoch:2 step:2116 [D loss: 0.578768, acc: 72.66%] [G loss: 2.716373]\n",
      "epoch:2 step:2117 [D loss: 0.600109, acc: 67.19%] [G loss: 3.051909]\n",
      "epoch:2 step:2118 [D loss: 0.581107, acc: 70.31%] [G loss: 2.996209]\n",
      "epoch:2 step:2119 [D loss: 0.481199, acc: 74.22%] [G loss: 3.029249]\n",
      "epoch:2 step:2120 [D loss: 0.644021, acc: 64.84%] [G loss: 2.802991]\n",
      "epoch:2 step:2121 [D loss: 0.674191, acc: 64.06%] [G loss: 2.657056]\n",
      "epoch:2 step:2122 [D loss: 0.498855, acc: 75.78%] [G loss: 2.562819]\n",
      "epoch:2 step:2123 [D loss: 0.652027, acc: 62.50%] [G loss: 2.715983]\n",
      "epoch:2 step:2124 [D loss: 0.555254, acc: 70.31%] [G loss: 3.000024]\n",
      "epoch:2 step:2125 [D loss: 0.551023, acc: 75.00%] [G loss: 2.767540]\n",
      "epoch:2 step:2126 [D loss: 0.517546, acc: 75.78%] [G loss: 3.279241]\n",
      "epoch:2 step:2127 [D loss: 0.501663, acc: 73.44%] [G loss: 2.917136]\n",
      "epoch:2 step:2128 [D loss: 0.565456, acc: 68.75%] [G loss: 3.239114]\n",
      "epoch:2 step:2129 [D loss: 0.514791, acc: 80.47%] [G loss: 3.257155]\n",
      "epoch:2 step:2130 [D loss: 0.541496, acc: 76.56%] [G loss: 3.160011]\n",
      "epoch:2 step:2131 [D loss: 0.522356, acc: 69.53%] [G loss: 2.863618]\n",
      "epoch:2 step:2132 [D loss: 0.504338, acc: 78.91%] [G loss: 3.354686]\n",
      "epoch:2 step:2133 [D loss: 0.545218, acc: 69.53%] [G loss: 3.196340]\n",
      "epoch:2 step:2134 [D loss: 0.502969, acc: 72.66%] [G loss: 2.956082]\n",
      "epoch:2 step:2135 [D loss: 0.574809, acc: 71.09%] [G loss: 3.265267]\n",
      "epoch:2 step:2136 [D loss: 0.570289, acc: 67.97%] [G loss: 3.073054]\n",
      "epoch:2 step:2137 [D loss: 0.551453, acc: 68.75%] [G loss: 2.715775]\n",
      "epoch:2 step:2138 [D loss: 0.570765, acc: 71.09%] [G loss: 3.232714]\n",
      "epoch:2 step:2139 [D loss: 0.621799, acc: 71.09%] [G loss: 2.839358]\n",
      "epoch:2 step:2140 [D loss: 0.687113, acc: 62.50%] [G loss: 2.804013]\n",
      "epoch:2 step:2141 [D loss: 0.499816, acc: 76.56%] [G loss: 3.109580]\n",
      "epoch:2 step:2142 [D loss: 0.559691, acc: 68.75%] [G loss: 2.951869]\n",
      "epoch:2 step:2143 [D loss: 0.615201, acc: 66.41%] [G loss: 2.852556]\n",
      "epoch:2 step:2144 [D loss: 0.583686, acc: 73.44%] [G loss: 2.848171]\n",
      "epoch:2 step:2145 [D loss: 0.471879, acc: 76.56%] [G loss: 3.211698]\n",
      "epoch:2 step:2146 [D loss: 0.700948, acc: 58.59%] [G loss: 2.927896]\n",
      "epoch:2 step:2147 [D loss: 0.519592, acc: 74.22%] [G loss: 2.992240]\n",
      "epoch:2 step:2148 [D loss: 0.588972, acc: 66.41%] [G loss: 2.864948]\n",
      "epoch:2 step:2149 [D loss: 0.712336, acc: 57.03%] [G loss: 2.762374]\n",
      "epoch:2 step:2150 [D loss: 0.563065, acc: 74.22%] [G loss: 2.794281]\n",
      "epoch:2 step:2151 [D loss: 0.553040, acc: 73.44%] [G loss: 3.045292]\n",
      "epoch:2 step:2152 [D loss: 0.635380, acc: 63.28%] [G loss: 3.161635]\n",
      "epoch:2 step:2153 [D loss: 0.495195, acc: 75.78%] [G loss: 3.082383]\n",
      "epoch:2 step:2154 [D loss: 0.537754, acc: 70.31%] [G loss: 3.055313]\n",
      "epoch:2 step:2155 [D loss: 0.564515, acc: 72.66%] [G loss: 2.908241]\n",
      "epoch:2 step:2156 [D loss: 0.545965, acc: 67.97%] [G loss: 3.024920]\n",
      "epoch:2 step:2157 [D loss: 0.560352, acc: 72.66%] [G loss: 3.188640]\n",
      "epoch:2 step:2158 [D loss: 0.510863, acc: 70.31%] [G loss: 3.084846]\n",
      "epoch:2 step:2159 [D loss: 0.511423, acc: 75.78%] [G loss: 3.453771]\n",
      "epoch:2 step:2160 [D loss: 0.443234, acc: 85.94%] [G loss: 3.232094]\n",
      "epoch:2 step:2161 [D loss: 0.598843, acc: 66.41%] [G loss: 3.195108]\n",
      "epoch:2 step:2162 [D loss: 0.539048, acc: 75.78%] [G loss: 2.802961]\n",
      "epoch:2 step:2163 [D loss: 0.540353, acc: 75.00%] [G loss: 3.133762]\n",
      "epoch:2 step:2164 [D loss: 0.544275, acc: 70.31%] [G loss: 3.027775]\n",
      "epoch:2 step:2165 [D loss: 0.568436, acc: 68.75%] [G loss: 2.854162]\n",
      "epoch:2 step:2166 [D loss: 0.550916, acc: 71.88%] [G loss: 3.079665]\n",
      "epoch:2 step:2167 [D loss: 0.577426, acc: 70.31%] [G loss: 3.352675]\n",
      "epoch:2 step:2168 [D loss: 0.613448, acc: 70.31%] [G loss: 3.072713]\n",
      "epoch:2 step:2169 [D loss: 0.538325, acc: 75.78%] [G loss: 3.041182]\n",
      "epoch:2 step:2170 [D loss: 0.590169, acc: 69.53%] [G loss: 2.821955]\n",
      "epoch:2 step:2171 [D loss: 0.589167, acc: 68.75%] [G loss: 2.851744]\n",
      "epoch:2 step:2172 [D loss: 0.578468, acc: 67.97%] [G loss: 3.076043]\n",
      "epoch:2 step:2173 [D loss: 0.577903, acc: 68.75%] [G loss: 2.799591]\n",
      "epoch:2 step:2174 [D loss: 0.515062, acc: 73.44%] [G loss: 2.866073]\n",
      "epoch:2 step:2175 [D loss: 0.573605, acc: 72.66%] [G loss: 2.824434]\n",
      "epoch:2 step:2176 [D loss: 0.484288, acc: 74.22%] [G loss: 2.957849]\n",
      "epoch:2 step:2177 [D loss: 0.535210, acc: 75.00%] [G loss: 3.179286]\n",
      "epoch:2 step:2178 [D loss: 0.433633, acc: 79.69%] [G loss: 3.545152]\n",
      "epoch:2 step:2179 [D loss: 0.535626, acc: 75.00%] [G loss: 3.153727]\n",
      "epoch:2 step:2180 [D loss: 0.516650, acc: 75.00%] [G loss: 3.063574]\n",
      "epoch:2 step:2181 [D loss: 0.594455, acc: 67.19%] [G loss: 2.952082]\n",
      "epoch:2 step:2182 [D loss: 0.573995, acc: 64.06%] [G loss: 3.359500]\n",
      "epoch:2 step:2183 [D loss: 0.550036, acc: 71.88%] [G loss: 3.332023]\n",
      "epoch:2 step:2184 [D loss: 0.530168, acc: 75.00%] [G loss: 3.250908]\n",
      "epoch:2 step:2185 [D loss: 0.551989, acc: 71.09%] [G loss: 3.128621]\n",
      "epoch:2 step:2186 [D loss: 0.473985, acc: 75.00%] [G loss: 3.206556]\n",
      "epoch:2 step:2187 [D loss: 0.441656, acc: 77.34%] [G loss: 3.163328]\n",
      "epoch:2 step:2188 [D loss: 0.531681, acc: 75.78%] [G loss: 3.740730]\n",
      "epoch:2 step:2189 [D loss: 0.479990, acc: 75.78%] [G loss: 3.681022]\n",
      "epoch:2 step:2190 [D loss: 0.702358, acc: 61.72%] [G loss: 2.757981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2191 [D loss: 0.511763, acc: 75.78%] [G loss: 2.723609]\n",
      "epoch:2 step:2192 [D loss: 0.591295, acc: 65.62%] [G loss: 3.009384]\n",
      "epoch:2 step:2193 [D loss: 0.549811, acc: 69.53%] [G loss: 3.244179]\n",
      "epoch:2 step:2194 [D loss: 0.533444, acc: 74.22%] [G loss: 3.000810]\n",
      "epoch:2 step:2195 [D loss: 0.500047, acc: 75.00%] [G loss: 3.147882]\n",
      "epoch:2 step:2196 [D loss: 0.558889, acc: 67.97%] [G loss: 3.112794]\n",
      "epoch:2 step:2197 [D loss: 0.537396, acc: 71.09%] [G loss: 3.141332]\n",
      "epoch:2 step:2198 [D loss: 0.461340, acc: 75.78%] [G loss: 3.082618]\n",
      "epoch:2 step:2199 [D loss: 0.570032, acc: 72.66%] [G loss: 2.849561]\n",
      "epoch:2 step:2200 [D loss: 0.643192, acc: 64.06%] [G loss: 3.024726]\n",
      "epoch:2 step:2201 [D loss: 0.494166, acc: 76.56%] [G loss: 3.138283]\n",
      "epoch:2 step:2202 [D loss: 0.552985, acc: 70.31%] [G loss: 3.166102]\n",
      "epoch:2 step:2203 [D loss: 0.474301, acc: 75.78%] [G loss: 3.157930]\n",
      "epoch:2 step:2204 [D loss: 0.491661, acc: 77.34%] [G loss: 3.323179]\n",
      "epoch:2 step:2205 [D loss: 0.547702, acc: 73.44%] [G loss: 3.192891]\n",
      "epoch:2 step:2206 [D loss: 0.479220, acc: 76.56%] [G loss: 3.272372]\n",
      "epoch:2 step:2207 [D loss: 0.558728, acc: 66.41%] [G loss: 3.538287]\n",
      "epoch:2 step:2208 [D loss: 0.579902, acc: 71.88%] [G loss: 2.884953]\n",
      "epoch:2 step:2209 [D loss: 0.545278, acc: 75.00%] [G loss: 4.089454]\n",
      "epoch:2 step:2210 [D loss: 0.562142, acc: 70.31%] [G loss: 3.586366]\n",
      "epoch:2 step:2211 [D loss: 0.696010, acc: 61.72%] [G loss: 3.033836]\n",
      "epoch:2 step:2212 [D loss: 0.470075, acc: 78.12%] [G loss: 2.913760]\n",
      "epoch:2 step:2213 [D loss: 0.500452, acc: 78.12%] [G loss: 3.098243]\n",
      "epoch:2 step:2214 [D loss: 0.481781, acc: 75.00%] [G loss: 3.151039]\n",
      "epoch:2 step:2215 [D loss: 0.683439, acc: 62.50%] [G loss: 3.158889]\n",
      "epoch:2 step:2216 [D loss: 0.551466, acc: 70.31%] [G loss: 3.099186]\n",
      "epoch:2 step:2217 [D loss: 0.556070, acc: 72.66%] [G loss: 3.573744]\n",
      "epoch:2 step:2218 [D loss: 0.438319, acc: 82.03%] [G loss: 3.344912]\n",
      "epoch:2 step:2219 [D loss: 0.502942, acc: 79.69%] [G loss: 3.255734]\n",
      "epoch:2 step:2220 [D loss: 0.481495, acc: 76.56%] [G loss: 3.441417]\n",
      "epoch:2 step:2221 [D loss: 0.491510, acc: 75.00%] [G loss: 3.817336]\n",
      "epoch:2 step:2222 [D loss: 0.636287, acc: 69.53%] [G loss: 3.074433]\n",
      "epoch:2 step:2223 [D loss: 0.734793, acc: 57.81%] [G loss: 2.674917]\n",
      "epoch:2 step:2224 [D loss: 0.552980, acc: 71.09%] [G loss: 3.219293]\n",
      "epoch:2 step:2225 [D loss: 0.602540, acc: 64.84%] [G loss: 2.917791]\n",
      "epoch:2 step:2226 [D loss: 0.588620, acc: 67.19%] [G loss: 3.014166]\n",
      "epoch:2 step:2227 [D loss: 0.618168, acc: 67.19%] [G loss: 3.000208]\n",
      "epoch:2 step:2228 [D loss: 0.456394, acc: 81.25%] [G loss: 3.079669]\n",
      "epoch:2 step:2229 [D loss: 0.566735, acc: 73.44%] [G loss: 3.320676]\n",
      "epoch:2 step:2230 [D loss: 0.557726, acc: 71.88%] [G loss: 2.714502]\n",
      "epoch:2 step:2231 [D loss: 0.551046, acc: 67.19%] [G loss: 3.544424]\n",
      "epoch:2 step:2232 [D loss: 0.455311, acc: 78.12%] [G loss: 3.360552]\n",
      "epoch:2 step:2233 [D loss: 0.572333, acc: 73.44%] [G loss: 3.916176]\n",
      "epoch:2 step:2234 [D loss: 0.616338, acc: 67.97%] [G loss: 3.040287]\n",
      "epoch:2 step:2235 [D loss: 0.579954, acc: 67.97%] [G loss: 2.839773]\n",
      "epoch:2 step:2236 [D loss: 0.658063, acc: 64.06%] [G loss: 2.969463]\n",
      "epoch:2 step:2237 [D loss: 0.545644, acc: 73.44%] [G loss: 3.313742]\n",
      "epoch:2 step:2238 [D loss: 0.444091, acc: 79.69%] [G loss: 3.268904]\n",
      "epoch:2 step:2239 [D loss: 0.518195, acc: 74.22%] [G loss: 3.070582]\n",
      "epoch:2 step:2240 [D loss: 0.511779, acc: 71.88%] [G loss: 3.097032]\n",
      "epoch:2 step:2241 [D loss: 0.592701, acc: 67.97%] [G loss: 2.944843]\n",
      "epoch:2 step:2242 [D loss: 0.601075, acc: 72.66%] [G loss: 3.527243]\n",
      "epoch:2 step:2243 [D loss: 0.636655, acc: 67.19%] [G loss: 2.573517]\n",
      "epoch:2 step:2244 [D loss: 0.572706, acc: 65.62%] [G loss: 2.582395]\n",
      "epoch:2 step:2245 [D loss: 0.555509, acc: 75.78%] [G loss: 2.817361]\n",
      "epoch:2 step:2246 [D loss: 0.543908, acc: 73.44%] [G loss: 2.891539]\n",
      "epoch:2 step:2247 [D loss: 0.542399, acc: 73.44%] [G loss: 2.663367]\n",
      "epoch:2 step:2248 [D loss: 0.577412, acc: 69.53%] [G loss: 2.990255]\n",
      "epoch:2 step:2249 [D loss: 0.593976, acc: 71.09%] [G loss: 3.041475]\n",
      "epoch:2 step:2250 [D loss: 0.556839, acc: 70.31%] [G loss: 2.804096]\n",
      "epoch:2 step:2251 [D loss: 0.584964, acc: 68.75%] [G loss: 2.864119]\n",
      "epoch:2 step:2252 [D loss: 0.462730, acc: 76.56%] [G loss: 3.438265]\n",
      "epoch:2 step:2253 [D loss: 0.542526, acc: 68.75%] [G loss: 2.941034]\n",
      "epoch:2 step:2254 [D loss: 0.543534, acc: 71.09%] [G loss: 2.885205]\n",
      "epoch:2 step:2255 [D loss: 0.564665, acc: 67.19%] [G loss: 3.212977]\n",
      "epoch:2 step:2256 [D loss: 0.560183, acc: 68.75%] [G loss: 3.178959]\n",
      "epoch:2 step:2257 [D loss: 0.566504, acc: 67.19%] [G loss: 2.993676]\n",
      "epoch:2 step:2258 [D loss: 0.589347, acc: 67.19%] [G loss: 3.357220]\n",
      "epoch:2 step:2259 [D loss: 0.471816, acc: 76.56%] [G loss: 2.991848]\n",
      "epoch:2 step:2260 [D loss: 0.574467, acc: 69.53%] [G loss: 2.838718]\n",
      "epoch:2 step:2261 [D loss: 0.616772, acc: 67.97%] [G loss: 3.023894]\n",
      "epoch:2 step:2262 [D loss: 0.626032, acc: 65.62%] [G loss: 3.139382]\n",
      "epoch:2 step:2263 [D loss: 0.553405, acc: 71.09%] [G loss: 3.092103]\n",
      "epoch:2 step:2264 [D loss: 0.629126, acc: 62.50%] [G loss: 2.953097]\n",
      "epoch:2 step:2265 [D loss: 0.580855, acc: 70.31%] [G loss: 2.877228]\n",
      "epoch:2 step:2266 [D loss: 0.518022, acc: 71.88%] [G loss: 3.058687]\n",
      "epoch:2 step:2267 [D loss: 0.501559, acc: 75.00%] [G loss: 3.423949]\n",
      "epoch:2 step:2268 [D loss: 0.576207, acc: 71.88%] [G loss: 3.003675]\n",
      "epoch:2 step:2269 [D loss: 0.534999, acc: 68.75%] [G loss: 3.342466]\n",
      "epoch:2 step:2270 [D loss: 0.603487, acc: 65.62%] [G loss: 2.754973]\n",
      "epoch:2 step:2271 [D loss: 0.565398, acc: 71.09%] [G loss: 3.486918]\n",
      "epoch:2 step:2272 [D loss: 0.493053, acc: 75.78%] [G loss: 3.640979]\n",
      "epoch:2 step:2273 [D loss: 0.510373, acc: 74.22%] [G loss: 3.703182]\n",
      "epoch:2 step:2274 [D loss: 0.594586, acc: 72.66%] [G loss: 3.342368]\n",
      "epoch:2 step:2275 [D loss: 0.549301, acc: 69.53%] [G loss: 2.952084]\n",
      "epoch:2 step:2276 [D loss: 0.479117, acc: 75.00%] [G loss: 3.457083]\n",
      "epoch:2 step:2277 [D loss: 0.586810, acc: 68.75%] [G loss: 3.233769]\n",
      "epoch:2 step:2278 [D loss: 0.506221, acc: 75.78%] [G loss: 2.868387]\n",
      "epoch:2 step:2279 [D loss: 0.499198, acc: 78.12%] [G loss: 3.521544]\n",
      "epoch:2 step:2280 [D loss: 0.539861, acc: 69.53%] [G loss: 3.229023]\n",
      "epoch:2 step:2281 [D loss: 0.524778, acc: 69.53%] [G loss: 2.937882]\n",
      "epoch:2 step:2282 [D loss: 0.539066, acc: 74.22%] [G loss: 3.274439]\n",
      "epoch:2 step:2283 [D loss: 0.567083, acc: 64.06%] [G loss: 3.421227]\n",
      "epoch:2 step:2284 [D loss: 0.645277, acc: 65.62%] [G loss: 3.146368]\n",
      "epoch:2 step:2285 [D loss: 0.540848, acc: 72.66%] [G loss: 2.998068]\n",
      "epoch:2 step:2286 [D loss: 0.560386, acc: 73.44%] [G loss: 3.093098]\n",
      "epoch:2 step:2287 [D loss: 0.573981, acc: 71.09%] [G loss: 3.054806]\n",
      "epoch:2 step:2288 [D loss: 0.638976, acc: 68.75%] [G loss: 2.812575]\n",
      "epoch:2 step:2289 [D loss: 0.606281, acc: 66.41%] [G loss: 2.668708]\n",
      "epoch:2 step:2290 [D loss: 0.528842, acc: 75.78%] [G loss: 3.015861]\n",
      "epoch:2 step:2291 [D loss: 0.605265, acc: 67.19%] [G loss: 2.876791]\n",
      "epoch:2 step:2292 [D loss: 0.610930, acc: 69.53%] [G loss: 2.928528]\n",
      "epoch:2 step:2293 [D loss: 0.506639, acc: 74.22%] [G loss: 3.209287]\n",
      "epoch:2 step:2294 [D loss: 0.515218, acc: 73.44%] [G loss: 3.134327]\n",
      "epoch:2 step:2295 [D loss: 0.509057, acc: 71.09%] [G loss: 3.046617]\n",
      "epoch:2 step:2296 [D loss: 0.557906, acc: 68.75%] [G loss: 3.385528]\n",
      "epoch:2 step:2297 [D loss: 0.507236, acc: 75.78%] [G loss: 3.048827]\n",
      "epoch:2 step:2298 [D loss: 0.567283, acc: 66.41%] [G loss: 2.904789]\n",
      "epoch:2 step:2299 [D loss: 0.591920, acc: 68.75%] [G loss: 3.140323]\n",
      "epoch:2 step:2300 [D loss: 0.581126, acc: 70.31%] [G loss: 3.162323]\n",
      "epoch:2 step:2301 [D loss: 0.545014, acc: 72.66%] [G loss: 3.141985]\n",
      "epoch:2 step:2302 [D loss: 0.440645, acc: 80.47%] [G loss: 3.487916]\n",
      "epoch:2 step:2303 [D loss: 0.532623, acc: 76.56%] [G loss: 3.582374]\n",
      "epoch:2 step:2304 [D loss: 0.518921, acc: 77.34%] [G loss: 3.417333]\n",
      "epoch:2 step:2305 [D loss: 0.508447, acc: 76.56%] [G loss: 3.161025]\n",
      "epoch:2 step:2306 [D loss: 0.581367, acc: 66.41%] [G loss: 2.913496]\n",
      "epoch:2 step:2307 [D loss: 0.588088, acc: 64.06%] [G loss: 3.081399]\n",
      "epoch:2 step:2308 [D loss: 0.505759, acc: 74.22%] [G loss: 3.072585]\n",
      "epoch:2 step:2309 [D loss: 0.576849, acc: 69.53%] [G loss: 3.221675]\n",
      "epoch:2 step:2310 [D loss: 0.557854, acc: 71.09%] [G loss: 3.218792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2311 [D loss: 0.703143, acc: 64.06%] [G loss: 2.902344]\n",
      "epoch:2 step:2312 [D loss: 0.555270, acc: 67.97%] [G loss: 2.985132]\n",
      "epoch:2 step:2313 [D loss: 0.535586, acc: 71.88%] [G loss: 3.202199]\n",
      "epoch:2 step:2314 [D loss: 0.543413, acc: 73.44%] [G loss: 3.225127]\n",
      "epoch:2 step:2315 [D loss: 0.561429, acc: 72.66%] [G loss: 2.959705]\n",
      "epoch:2 step:2316 [D loss: 0.553563, acc: 72.66%] [G loss: 2.910834]\n",
      "epoch:2 step:2317 [D loss: 0.566529, acc: 70.31%] [G loss: 2.781505]\n",
      "epoch:2 step:2318 [D loss: 0.582233, acc: 68.75%] [G loss: 3.211591]\n",
      "epoch:2 step:2319 [D loss: 0.585747, acc: 67.19%] [G loss: 2.886499]\n",
      "epoch:2 step:2320 [D loss: 0.533251, acc: 75.78%] [G loss: 3.099840]\n",
      "epoch:2 step:2321 [D loss: 0.463288, acc: 75.78%] [G loss: 3.395241]\n",
      "epoch:2 step:2322 [D loss: 0.610169, acc: 67.19%] [G loss: 2.856601]\n",
      "epoch:2 step:2323 [D loss: 0.661347, acc: 66.41%] [G loss: 2.564410]\n",
      "epoch:2 step:2324 [D loss: 0.524176, acc: 75.78%] [G loss: 3.375810]\n",
      "epoch:2 step:2325 [D loss: 0.440964, acc: 82.81%] [G loss: 3.408360]\n",
      "epoch:2 step:2326 [D loss: 0.590602, acc: 69.53%] [G loss: 3.205989]\n",
      "epoch:2 step:2327 [D loss: 0.547830, acc: 74.22%] [G loss: 3.103563]\n",
      "epoch:2 step:2328 [D loss: 0.583102, acc: 67.19%] [G loss: 2.723271]\n",
      "epoch:2 step:2329 [D loss: 0.529874, acc: 73.44%] [G loss: 3.264431]\n",
      "epoch:2 step:2330 [D loss: 0.638647, acc: 63.28%] [G loss: 2.566638]\n",
      "epoch:2 step:2331 [D loss: 0.646199, acc: 65.62%] [G loss: 3.022138]\n",
      "epoch:2 step:2332 [D loss: 0.710413, acc: 64.06%] [G loss: 2.911242]\n",
      "epoch:2 step:2333 [D loss: 0.543208, acc: 71.09%] [G loss: 2.618202]\n",
      "epoch:2 step:2334 [D loss: 0.525992, acc: 71.09%] [G loss: 2.934427]\n",
      "epoch:2 step:2335 [D loss: 0.527172, acc: 71.09%] [G loss: 3.045133]\n",
      "epoch:2 step:2336 [D loss: 0.526089, acc: 75.00%] [G loss: 2.913318]\n",
      "epoch:2 step:2337 [D loss: 0.582913, acc: 67.97%] [G loss: 2.912831]\n",
      "epoch:2 step:2338 [D loss: 0.605317, acc: 68.75%] [G loss: 3.158424]\n",
      "epoch:2 step:2339 [D loss: 0.776468, acc: 60.16%] [G loss: 2.405029]\n",
      "epoch:2 step:2340 [D loss: 0.483683, acc: 76.56%] [G loss: 3.131307]\n",
      "epoch:2 step:2341 [D loss: 0.646349, acc: 60.94%] [G loss: 2.961009]\n",
      "epoch:2 step:2342 [D loss: 0.616298, acc: 71.09%] [G loss: 2.575885]\n",
      "epoch:2 step:2343 [D loss: 0.482468, acc: 75.00%] [G loss: 2.758252]\n",
      "epoch:2 step:2344 [D loss: 0.458987, acc: 78.12%] [G loss: 2.969873]\n",
      "epoch:2 step:2345 [D loss: 0.572347, acc: 73.44%] [G loss: 3.474727]\n",
      "epoch:2 step:2346 [D loss: 0.596256, acc: 69.53%] [G loss: 3.106351]\n",
      "epoch:2 step:2347 [D loss: 0.570855, acc: 68.75%] [G loss: 2.907870]\n",
      "epoch:2 step:2348 [D loss: 0.505950, acc: 76.56%] [G loss: 3.281158]\n",
      "epoch:2 step:2349 [D loss: 0.502776, acc: 73.44%] [G loss: 3.344896]\n",
      "epoch:2 step:2350 [D loss: 0.606538, acc: 64.84%] [G loss: 2.987852]\n",
      "epoch:2 step:2351 [D loss: 0.709556, acc: 53.91%] [G loss: 2.898715]\n",
      "epoch:2 step:2352 [D loss: 0.555184, acc: 71.09%] [G loss: 2.680827]\n",
      "epoch:2 step:2353 [D loss: 0.571230, acc: 71.88%] [G loss: 2.863304]\n",
      "epoch:2 step:2354 [D loss: 0.638887, acc: 65.62%] [G loss: 2.793749]\n",
      "epoch:2 step:2355 [D loss: 0.543370, acc: 75.78%] [G loss: 3.150647]\n",
      "epoch:2 step:2356 [D loss: 0.589779, acc: 65.62%] [G loss: 3.075567]\n",
      "epoch:2 step:2357 [D loss: 0.601462, acc: 63.28%] [G loss: 3.116046]\n",
      "epoch:2 step:2358 [D loss: 0.529531, acc: 71.09%] [G loss: 2.919761]\n",
      "epoch:2 step:2359 [D loss: 0.481969, acc: 78.12%] [G loss: 3.104618]\n",
      "epoch:2 step:2360 [D loss: 0.564916, acc: 70.31%] [G loss: 2.797899]\n",
      "epoch:2 step:2361 [D loss: 0.523112, acc: 76.56%] [G loss: 3.039255]\n",
      "epoch:2 step:2362 [D loss: 0.480599, acc: 78.12%] [G loss: 3.418622]\n",
      "epoch:2 step:2363 [D loss: 0.655489, acc: 66.41%] [G loss: 3.014930]\n",
      "epoch:2 step:2364 [D loss: 0.551082, acc: 71.88%] [G loss: 3.314642]\n",
      "epoch:2 step:2365 [D loss: 0.572675, acc: 74.22%] [G loss: 3.162852]\n",
      "epoch:2 step:2366 [D loss: 0.574769, acc: 72.66%] [G loss: 3.184575]\n",
      "epoch:2 step:2367 [D loss: 0.555673, acc: 74.22%] [G loss: 2.752728]\n",
      "epoch:2 step:2368 [D loss: 0.569708, acc: 67.97%] [G loss: 2.960389]\n",
      "epoch:2 step:2369 [D loss: 0.538464, acc: 75.78%] [G loss: 3.063102]\n",
      "epoch:2 step:2370 [D loss: 0.497767, acc: 74.22%] [G loss: 2.953143]\n",
      "epoch:2 step:2371 [D loss: 0.505652, acc: 73.44%] [G loss: 3.116216]\n",
      "epoch:2 step:2372 [D loss: 0.512237, acc: 73.44%] [G loss: 3.626958]\n",
      "epoch:2 step:2373 [D loss: 0.492552, acc: 73.44%] [G loss: 3.789604]\n",
      "epoch:2 step:2374 [D loss: 0.635257, acc: 67.97%] [G loss: 3.076266]\n",
      "epoch:2 step:2375 [D loss: 0.770806, acc: 55.47%] [G loss: 2.638575]\n",
      "epoch:2 step:2376 [D loss: 0.554098, acc: 72.66%] [G loss: 2.819589]\n",
      "epoch:2 step:2377 [D loss: 0.486643, acc: 74.22%] [G loss: 3.000747]\n",
      "epoch:2 step:2378 [D loss: 0.587518, acc: 70.31%] [G loss: 2.740777]\n",
      "epoch:2 step:2379 [D loss: 0.583837, acc: 68.75%] [G loss: 3.016679]\n",
      "epoch:2 step:2380 [D loss: 0.520942, acc: 74.22%] [G loss: 3.248619]\n",
      "epoch:2 step:2381 [D loss: 0.469230, acc: 81.25%] [G loss: 3.208173]\n",
      "epoch:2 step:2382 [D loss: 0.463576, acc: 78.91%] [G loss: 3.427969]\n",
      "epoch:2 step:2383 [D loss: 0.568583, acc: 71.88%] [G loss: 3.422822]\n",
      "epoch:2 step:2384 [D loss: 0.562077, acc: 71.09%] [G loss: 2.871221]\n",
      "epoch:2 step:2385 [D loss: 0.551408, acc: 68.75%] [G loss: 3.024222]\n",
      "epoch:2 step:2386 [D loss: 0.565880, acc: 68.75%] [G loss: 3.245751]\n",
      "epoch:2 step:2387 [D loss: 0.519600, acc: 75.00%] [G loss: 3.347643]\n",
      "epoch:2 step:2388 [D loss: 0.438495, acc: 79.69%] [G loss: 3.431592]\n",
      "epoch:2 step:2389 [D loss: 0.450548, acc: 78.12%] [G loss: 2.945526]\n",
      "epoch:2 step:2390 [D loss: 0.548782, acc: 71.88%] [G loss: 2.976202]\n",
      "epoch:2 step:2391 [D loss: 0.574628, acc: 67.97%] [G loss: 2.890607]\n",
      "epoch:2 step:2392 [D loss: 0.434303, acc: 79.69%] [G loss: 3.476169]\n",
      "epoch:2 step:2393 [D loss: 0.507489, acc: 77.34%] [G loss: 2.962345]\n",
      "epoch:2 step:2394 [D loss: 0.512185, acc: 75.78%] [G loss: 3.007173]\n",
      "epoch:2 step:2395 [D loss: 0.516519, acc: 75.00%] [G loss: 3.073214]\n",
      "epoch:2 step:2396 [D loss: 0.615963, acc: 71.88%] [G loss: 2.838733]\n",
      "epoch:2 step:2397 [D loss: 0.515410, acc: 71.88%] [G loss: 3.826118]\n",
      "epoch:2 step:2398 [D loss: 0.555900, acc: 72.66%] [G loss: 2.960451]\n",
      "epoch:2 step:2399 [D loss: 0.548986, acc: 70.31%] [G loss: 3.068688]\n",
      "epoch:2 step:2400 [D loss: 0.575042, acc: 66.41%] [G loss: 3.095102]\n",
      "epoch:2 step:2401 [D loss: 0.628717, acc: 67.19%] [G loss: 3.024956]\n",
      "epoch:2 step:2402 [D loss: 0.625876, acc: 67.19%] [G loss: 3.025615]\n",
      "epoch:2 step:2403 [D loss: 0.614436, acc: 64.06%] [G loss: 3.007711]\n",
      "epoch:2 step:2404 [D loss: 0.556753, acc: 70.31%] [G loss: 2.819946]\n",
      "epoch:2 step:2405 [D loss: 0.698224, acc: 65.62%] [G loss: 3.023619]\n",
      "epoch:2 step:2406 [D loss: 0.594539, acc: 67.97%] [G loss: 3.080342]\n",
      "epoch:2 step:2407 [D loss: 0.561546, acc: 71.09%] [G loss: 3.290888]\n",
      "epoch:2 step:2408 [D loss: 0.505121, acc: 72.66%] [G loss: 3.358548]\n",
      "epoch:2 step:2409 [D loss: 0.544834, acc: 66.41%] [G loss: 2.999315]\n",
      "epoch:2 step:2410 [D loss: 0.520611, acc: 73.44%] [G loss: 3.121197]\n",
      "epoch:2 step:2411 [D loss: 0.567827, acc: 75.78%] [G loss: 3.520022]\n",
      "epoch:2 step:2412 [D loss: 0.565110, acc: 73.44%] [G loss: 2.930800]\n",
      "epoch:2 step:2413 [D loss: 0.539515, acc: 72.66%] [G loss: 2.990676]\n",
      "epoch:2 step:2414 [D loss: 0.545748, acc: 71.09%] [G loss: 3.448815]\n",
      "epoch:2 step:2415 [D loss: 0.425805, acc: 82.03%] [G loss: 3.331891]\n",
      "epoch:2 step:2416 [D loss: 0.606989, acc: 71.09%] [G loss: 3.209855]\n",
      "epoch:2 step:2417 [D loss: 0.657694, acc: 68.75%] [G loss: 2.961662]\n",
      "epoch:2 step:2418 [D loss: 0.540254, acc: 69.53%] [G loss: 3.253504]\n",
      "epoch:2 step:2419 [D loss: 0.462570, acc: 78.91%] [G loss: 3.369509]\n",
      "epoch:2 step:2420 [D loss: 0.483421, acc: 78.12%] [G loss: 3.517373]\n",
      "epoch:2 step:2421 [D loss: 0.473120, acc: 79.69%] [G loss: 3.285518]\n",
      "epoch:2 step:2422 [D loss: 0.552071, acc: 71.09%] [G loss: 3.280102]\n",
      "epoch:2 step:2423 [D loss: 0.538902, acc: 76.56%] [G loss: 2.887007]\n",
      "epoch:2 step:2424 [D loss: 0.559734, acc: 71.88%] [G loss: 3.020348]\n",
      "epoch:2 step:2425 [D loss: 0.511218, acc: 76.56%] [G loss: 3.229147]\n",
      "epoch:2 step:2426 [D loss: 0.512077, acc: 74.22%] [G loss: 3.282169]\n",
      "epoch:2 step:2427 [D loss: 0.511707, acc: 78.91%] [G loss: 3.460840]\n",
      "epoch:2 step:2428 [D loss: 0.470999, acc: 76.56%] [G loss: 3.786702]\n",
      "epoch:2 step:2429 [D loss: 0.500898, acc: 72.66%] [G loss: 3.504094]\n",
      "epoch:2 step:2430 [D loss: 0.495911, acc: 75.00%] [G loss: 3.753938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2431 [D loss: 0.544681, acc: 74.22%] [G loss: 3.341644]\n",
      "epoch:2 step:2432 [D loss: 0.507270, acc: 72.66%] [G loss: 3.083984]\n",
      "epoch:2 step:2433 [D loss: 0.493918, acc: 71.88%] [G loss: 3.288431]\n",
      "epoch:2 step:2434 [D loss: 0.474962, acc: 78.12%] [G loss: 2.823410]\n",
      "epoch:2 step:2435 [D loss: 0.480174, acc: 76.56%] [G loss: 3.412901]\n",
      "epoch:2 step:2436 [D loss: 0.576727, acc: 69.53%] [G loss: 2.818244]\n",
      "epoch:2 step:2437 [D loss: 0.631968, acc: 64.84%] [G loss: 2.850759]\n",
      "epoch:2 step:2438 [D loss: 0.508790, acc: 75.00%] [G loss: 2.913076]\n",
      "epoch:2 step:2439 [D loss: 0.661472, acc: 67.97%] [G loss: 3.359725]\n",
      "epoch:2 step:2440 [D loss: 0.570841, acc: 74.22%] [G loss: 3.175806]\n",
      "epoch:2 step:2441 [D loss: 0.518053, acc: 75.00%] [G loss: 3.566646]\n",
      "epoch:2 step:2442 [D loss: 0.468848, acc: 82.03%] [G loss: 3.309190]\n",
      "epoch:2 step:2443 [D loss: 0.576543, acc: 71.88%] [G loss: 3.336030]\n",
      "epoch:2 step:2444 [D loss: 0.525288, acc: 74.22%] [G loss: 3.080824]\n",
      "epoch:2 step:2445 [D loss: 0.468965, acc: 82.03%] [G loss: 3.575140]\n",
      "epoch:2 step:2446 [D loss: 0.702699, acc: 61.72%] [G loss: 2.747808]\n",
      "epoch:2 step:2447 [D loss: 0.511228, acc: 78.91%] [G loss: 3.078427]\n",
      "epoch:2 step:2448 [D loss: 0.489821, acc: 78.12%] [G loss: 3.200656]\n",
      "epoch:2 step:2449 [D loss: 0.465462, acc: 78.12%] [G loss: 3.241302]\n",
      "epoch:2 step:2450 [D loss: 0.557320, acc: 71.09%] [G loss: 2.645479]\n",
      "epoch:2 step:2451 [D loss: 0.555655, acc: 71.09%] [G loss: 2.895750]\n",
      "epoch:2 step:2452 [D loss: 0.605012, acc: 65.62%] [G loss: 2.911720]\n",
      "epoch:2 step:2453 [D loss: 0.563853, acc: 72.66%] [G loss: 3.179637]\n",
      "epoch:2 step:2454 [D loss: 0.712151, acc: 60.94%] [G loss: 3.285012]\n",
      "epoch:2 step:2455 [D loss: 0.523603, acc: 71.88%] [G loss: 3.450862]\n",
      "epoch:2 step:2456 [D loss: 0.411670, acc: 83.59%] [G loss: 3.304739]\n",
      "epoch:2 step:2457 [D loss: 0.541644, acc: 77.34%] [G loss: 3.239307]\n",
      "epoch:2 step:2458 [D loss: 0.618749, acc: 67.97%] [G loss: 3.098148]\n",
      "epoch:2 step:2459 [D loss: 0.558967, acc: 70.31%] [G loss: 2.881894]\n",
      "epoch:2 step:2460 [D loss: 0.588859, acc: 67.97%] [G loss: 2.779393]\n",
      "epoch:2 step:2461 [D loss: 0.588611, acc: 69.53%] [G loss: 2.933935]\n",
      "epoch:2 step:2462 [D loss: 0.621890, acc: 64.84%] [G loss: 3.115382]\n",
      "epoch:2 step:2463 [D loss: 0.533224, acc: 69.53%] [G loss: 3.223608]\n",
      "epoch:2 step:2464 [D loss: 0.588266, acc: 73.44%] [G loss: 3.102484]\n",
      "epoch:2 step:2465 [D loss: 0.607700, acc: 67.97%] [G loss: 2.809608]\n",
      "epoch:2 step:2466 [D loss: 0.563126, acc: 69.53%] [G loss: 3.188927]\n",
      "epoch:2 step:2467 [D loss: 0.537673, acc: 76.56%] [G loss: 2.966362]\n",
      "epoch:2 step:2468 [D loss: 0.459100, acc: 78.91%] [G loss: 3.040693]\n",
      "epoch:2 step:2469 [D loss: 0.623865, acc: 66.41%] [G loss: 3.077935]\n",
      "epoch:2 step:2470 [D loss: 0.547976, acc: 75.00%] [G loss: 2.788021]\n",
      "epoch:2 step:2471 [D loss: 0.489493, acc: 74.22%] [G loss: 3.442697]\n",
      "epoch:2 step:2472 [D loss: 0.533373, acc: 72.66%] [G loss: 3.699165]\n",
      "epoch:2 step:2473 [D loss: 0.534079, acc: 74.22%] [G loss: 3.105821]\n",
      "epoch:2 step:2474 [D loss: 0.592735, acc: 65.62%] [G loss: 2.825303]\n",
      "epoch:2 step:2475 [D loss: 0.589577, acc: 70.31%] [G loss: 3.073200]\n",
      "epoch:2 step:2476 [D loss: 0.585428, acc: 67.97%] [G loss: 3.108804]\n",
      "epoch:2 step:2477 [D loss: 0.495792, acc: 76.56%] [G loss: 3.503572]\n",
      "epoch:2 step:2478 [D loss: 0.535964, acc: 76.56%] [G loss: 2.989591]\n",
      "epoch:2 step:2479 [D loss: 0.509901, acc: 75.78%] [G loss: 3.110207]\n",
      "epoch:2 step:2480 [D loss: 0.513049, acc: 78.12%] [G loss: 3.284239]\n",
      "epoch:2 step:2481 [D loss: 0.601633, acc: 64.06%] [G loss: 3.133870]\n",
      "epoch:2 step:2482 [D loss: 0.545404, acc: 73.44%] [G loss: 3.048146]\n",
      "epoch:2 step:2483 [D loss: 0.500805, acc: 75.78%] [G loss: 3.337905]\n",
      "epoch:2 step:2484 [D loss: 0.534487, acc: 71.88%] [G loss: 3.332575]\n",
      "epoch:2 step:2485 [D loss: 0.582445, acc: 70.31%] [G loss: 3.325323]\n",
      "epoch:2 step:2486 [D loss: 0.493805, acc: 75.78%] [G loss: 3.642405]\n",
      "epoch:2 step:2487 [D loss: 0.592242, acc: 68.75%] [G loss: 3.176590]\n",
      "epoch:2 step:2488 [D loss: 0.570972, acc: 74.22%] [G loss: 3.274119]\n",
      "epoch:2 step:2489 [D loss: 0.541085, acc: 71.88%] [G loss: 3.113270]\n",
      "epoch:2 step:2490 [D loss: 0.596604, acc: 68.75%] [G loss: 2.974632]\n",
      "epoch:2 step:2491 [D loss: 0.517683, acc: 76.56%] [G loss: 3.025070]\n",
      "epoch:2 step:2492 [D loss: 0.563081, acc: 71.88%] [G loss: 3.351059]\n",
      "epoch:2 step:2493 [D loss: 0.456141, acc: 82.81%] [G loss: 3.401276]\n",
      "epoch:2 step:2494 [D loss: 0.540083, acc: 74.22%] [G loss: 3.115230]\n",
      "epoch:2 step:2495 [D loss: 0.578179, acc: 71.88%] [G loss: 2.932881]\n",
      "epoch:2 step:2496 [D loss: 0.583531, acc: 68.75%] [G loss: 2.750504]\n",
      "epoch:2 step:2497 [D loss: 0.552534, acc: 71.09%] [G loss: 3.122712]\n",
      "epoch:2 step:2498 [D loss: 0.647797, acc: 64.84%] [G loss: 2.910341]\n",
      "epoch:2 step:2499 [D loss: 0.490318, acc: 72.66%] [G loss: 3.120734]\n",
      "epoch:2 step:2500 [D loss: 0.505162, acc: 75.78%] [G loss: 2.968377]\n",
      "epoch:2 step:2501 [D loss: 0.556850, acc: 72.66%] [G loss: 3.161938]\n",
      "epoch:2 step:2502 [D loss: 0.482185, acc: 77.34%] [G loss: 3.248089]\n",
      "epoch:2 step:2503 [D loss: 0.625438, acc: 67.97%] [G loss: 2.970218]\n",
      "epoch:2 step:2504 [D loss: 0.617661, acc: 64.84%] [G loss: 3.141913]\n",
      "epoch:2 step:2505 [D loss: 0.470321, acc: 75.78%] [G loss: 3.262355]\n",
      "epoch:2 step:2506 [D loss: 0.463747, acc: 81.25%] [G loss: 2.958509]\n",
      "epoch:2 step:2507 [D loss: 0.528656, acc: 71.09%] [G loss: 3.684648]\n",
      "epoch:2 step:2508 [D loss: 0.490861, acc: 76.56%] [G loss: 3.314554]\n",
      "epoch:2 step:2509 [D loss: 0.543286, acc: 72.66%] [G loss: 3.346515]\n",
      "epoch:2 step:2510 [D loss: 0.548200, acc: 69.53%] [G loss: 3.322350]\n",
      "epoch:2 step:2511 [D loss: 0.680809, acc: 64.06%] [G loss: 3.204935]\n",
      "epoch:2 step:2512 [D loss: 0.428126, acc: 82.81%] [G loss: 3.047247]\n",
      "epoch:2 step:2513 [D loss: 0.462624, acc: 79.69%] [G loss: 3.780972]\n",
      "epoch:2 step:2514 [D loss: 0.533618, acc: 76.56%] [G loss: 3.435827]\n",
      "epoch:2 step:2515 [D loss: 0.489079, acc: 79.69%] [G loss: 3.093381]\n",
      "epoch:2 step:2516 [D loss: 0.619738, acc: 69.53%] [G loss: 2.914451]\n",
      "epoch:2 step:2517 [D loss: 0.536568, acc: 70.31%] [G loss: 3.569201]\n",
      "epoch:2 step:2518 [D loss: 0.513584, acc: 75.00%] [G loss: 3.254401]\n",
      "epoch:2 step:2519 [D loss: 0.551916, acc: 73.44%] [G loss: 3.176484]\n",
      "epoch:2 step:2520 [D loss: 0.481542, acc: 76.56%] [G loss: 3.276266]\n",
      "epoch:2 step:2521 [D loss: 0.541125, acc: 75.78%] [G loss: 3.558529]\n",
      "epoch:2 step:2522 [D loss: 0.520429, acc: 73.44%] [G loss: 3.702905]\n",
      "epoch:2 step:2523 [D loss: 0.581108, acc: 71.88%] [G loss: 3.107308]\n",
      "epoch:2 step:2524 [D loss: 0.479671, acc: 77.34%] [G loss: 3.521676]\n",
      "epoch:2 step:2525 [D loss: 0.526667, acc: 71.09%] [G loss: 2.792844]\n",
      "epoch:2 step:2526 [D loss: 0.699227, acc: 60.16%] [G loss: 2.738730]\n",
      "epoch:2 step:2527 [D loss: 0.511142, acc: 71.88%] [G loss: 3.044931]\n",
      "epoch:2 step:2528 [D loss: 0.573375, acc: 73.44%] [G loss: 3.052296]\n",
      "epoch:2 step:2529 [D loss: 0.591679, acc: 67.97%] [G loss: 3.007000]\n",
      "epoch:2 step:2530 [D loss: 0.565891, acc: 68.75%] [G loss: 3.176554]\n",
      "epoch:2 step:2531 [D loss: 0.551417, acc: 66.41%] [G loss: 3.051195]\n",
      "epoch:2 step:2532 [D loss: 0.466630, acc: 78.12%] [G loss: 2.937334]\n",
      "epoch:2 step:2533 [D loss: 0.522514, acc: 72.66%] [G loss: 2.991663]\n",
      "epoch:2 step:2534 [D loss: 0.507025, acc: 73.44%] [G loss: 3.098505]\n",
      "epoch:2 step:2535 [D loss: 0.612844, acc: 72.66%] [G loss: 3.303652]\n",
      "epoch:2 step:2536 [D loss: 0.588722, acc: 71.88%] [G loss: 3.196296]\n",
      "epoch:2 step:2537 [D loss: 0.582877, acc: 67.19%] [G loss: 3.300000]\n",
      "epoch:2 step:2538 [D loss: 0.484670, acc: 77.34%] [G loss: 3.250322]\n",
      "epoch:2 step:2539 [D loss: 0.526403, acc: 76.56%] [G loss: 3.198201]\n",
      "epoch:2 step:2540 [D loss: 0.489532, acc: 75.00%] [G loss: 3.104733]\n",
      "epoch:2 step:2541 [D loss: 0.568065, acc: 74.22%] [G loss: 3.142227]\n",
      "epoch:2 step:2542 [D loss: 0.572603, acc: 68.75%] [G loss: 3.042516]\n",
      "epoch:2 step:2543 [D loss: 0.596566, acc: 66.41%] [G loss: 3.113353]\n",
      "epoch:2 step:2544 [D loss: 0.594191, acc: 69.53%] [G loss: 3.426764]\n",
      "epoch:2 step:2545 [D loss: 0.543153, acc: 68.75%] [G loss: 3.309788]\n",
      "epoch:2 step:2546 [D loss: 0.651530, acc: 65.62%] [G loss: 3.061822]\n",
      "epoch:2 step:2547 [D loss: 0.672553, acc: 64.84%] [G loss: 2.836357]\n",
      "epoch:2 step:2548 [D loss: 0.689028, acc: 63.28%] [G loss: 2.700046]\n",
      "epoch:2 step:2549 [D loss: 0.606751, acc: 66.41%] [G loss: 2.647912]\n",
      "epoch:2 step:2550 [D loss: 0.509237, acc: 74.22%] [G loss: 2.789838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2551 [D loss: 0.471398, acc: 78.91%] [G loss: 2.934201]\n",
      "epoch:2 step:2552 [D loss: 0.474514, acc: 78.91%] [G loss: 3.048613]\n",
      "epoch:2 step:2553 [D loss: 0.585963, acc: 74.22%] [G loss: 3.102010]\n",
      "epoch:2 step:2554 [D loss: 0.534135, acc: 78.12%] [G loss: 2.825698]\n",
      "epoch:2 step:2555 [D loss: 0.536478, acc: 73.44%] [G loss: 3.266678]\n",
      "epoch:2 step:2556 [D loss: 0.592817, acc: 67.97%] [G loss: 2.976478]\n",
      "epoch:2 step:2557 [D loss: 0.644133, acc: 60.94%] [G loss: 3.043351]\n",
      "epoch:2 step:2558 [D loss: 0.477980, acc: 77.34%] [G loss: 2.730638]\n",
      "epoch:2 step:2559 [D loss: 0.513669, acc: 78.91%] [G loss: 2.907664]\n",
      "epoch:2 step:2560 [D loss: 0.515939, acc: 71.88%] [G loss: 2.728789]\n",
      "epoch:2 step:2561 [D loss: 0.650175, acc: 64.06%] [G loss: 3.002870]\n",
      "epoch:2 step:2562 [D loss: 0.572209, acc: 69.53%] [G loss: 2.860056]\n",
      "epoch:2 step:2563 [D loss: 0.472919, acc: 76.56%] [G loss: 3.285240]\n",
      "epoch:2 step:2564 [D loss: 0.556264, acc: 71.88%] [G loss: 3.119803]\n",
      "epoch:2 step:2565 [D loss: 0.590156, acc: 70.31%] [G loss: 2.987869]\n",
      "epoch:2 step:2566 [D loss: 0.540840, acc: 68.75%] [G loss: 2.693002]\n",
      "epoch:2 step:2567 [D loss: 0.475407, acc: 78.91%] [G loss: 2.967724]\n",
      "epoch:2 step:2568 [D loss: 0.559524, acc: 67.97%] [G loss: 3.134170]\n",
      "epoch:2 step:2569 [D loss: 0.577362, acc: 75.00%] [G loss: 2.744226]\n",
      "epoch:2 step:2570 [D loss: 0.481253, acc: 79.69%] [G loss: 3.213509]\n",
      "epoch:2 step:2571 [D loss: 0.569890, acc: 71.88%] [G loss: 3.400729]\n",
      "epoch:2 step:2572 [D loss: 0.574243, acc: 71.88%] [G loss: 2.949984]\n",
      "epoch:2 step:2573 [D loss: 0.533453, acc: 71.09%] [G loss: 3.091497]\n",
      "epoch:2 step:2574 [D loss: 0.565755, acc: 70.31%] [G loss: 3.056184]\n",
      "epoch:2 step:2575 [D loss: 0.563469, acc: 67.97%] [G loss: 2.898408]\n",
      "epoch:2 step:2576 [D loss: 0.514796, acc: 73.44%] [G loss: 2.821717]\n",
      "epoch:2 step:2577 [D loss: 0.562680, acc: 71.09%] [G loss: 3.169153]\n",
      "epoch:2 step:2578 [D loss: 0.533054, acc: 74.22%] [G loss: 2.867860]\n",
      "epoch:2 step:2579 [D loss: 0.619049, acc: 65.62%] [G loss: 2.835802]\n",
      "epoch:2 step:2580 [D loss: 0.512636, acc: 75.00%] [G loss: 3.096906]\n",
      "epoch:2 step:2581 [D loss: 0.499223, acc: 76.56%] [G loss: 3.637676]\n",
      "epoch:2 step:2582 [D loss: 0.453704, acc: 78.12%] [G loss: 3.268451]\n",
      "epoch:2 step:2583 [D loss: 0.541001, acc: 70.31%] [G loss: 3.179720]\n",
      "epoch:2 step:2584 [D loss: 0.731614, acc: 61.72%] [G loss: 2.855458]\n",
      "epoch:2 step:2585 [D loss: 0.596792, acc: 71.88%] [G loss: 2.816473]\n",
      "epoch:2 step:2586 [D loss: 0.618234, acc: 60.16%] [G loss: 2.862675]\n",
      "epoch:2 step:2587 [D loss: 0.556948, acc: 71.09%] [G loss: 2.890104]\n",
      "epoch:2 step:2588 [D loss: 0.495002, acc: 76.56%] [G loss: 3.271712]\n",
      "epoch:2 step:2589 [D loss: 0.566816, acc: 66.41%] [G loss: 2.896303]\n",
      "epoch:2 step:2590 [D loss: 0.661057, acc: 63.28%] [G loss: 2.549471]\n",
      "epoch:2 step:2591 [D loss: 0.642005, acc: 66.41%] [G loss: 2.768896]\n",
      "epoch:2 step:2592 [D loss: 0.530901, acc: 72.66%] [G loss: 2.899809]\n",
      "epoch:2 step:2593 [D loss: 0.601881, acc: 70.31%] [G loss: 2.832577]\n",
      "epoch:2 step:2594 [D loss: 0.583186, acc: 67.97%] [G loss: 2.659145]\n",
      "epoch:2 step:2595 [D loss: 0.513977, acc: 72.66%] [G loss: 2.819477]\n",
      "epoch:2 step:2596 [D loss: 0.537988, acc: 69.53%] [G loss: 3.221253]\n",
      "epoch:2 step:2597 [D loss: 0.664139, acc: 60.16%] [G loss: 2.777937]\n",
      "epoch:2 step:2598 [D loss: 0.464826, acc: 78.91%] [G loss: 3.195275]\n",
      "epoch:2 step:2599 [D loss: 0.515894, acc: 71.88%] [G loss: 3.488810]\n",
      "epoch:2 step:2600 [D loss: 0.506949, acc: 76.56%] [G loss: 3.114598]\n",
      "epoch:2 step:2601 [D loss: 0.593939, acc: 72.66%] [G loss: 2.923564]\n",
      "epoch:2 step:2602 [D loss: 0.629048, acc: 67.19%] [G loss: 3.383990]\n",
      "epoch:2 step:2603 [D loss: 0.543555, acc: 75.00%] [G loss: 3.126319]\n",
      "epoch:2 step:2604 [D loss: 0.485152, acc: 77.34%] [G loss: 3.186337]\n",
      "epoch:2 step:2605 [D loss: 0.560863, acc: 69.53%] [G loss: 3.090147]\n",
      "epoch:2 step:2606 [D loss: 0.484606, acc: 75.78%] [G loss: 3.504519]\n",
      "epoch:2 step:2607 [D loss: 0.449297, acc: 81.25%] [G loss: 3.226062]\n",
      "epoch:2 step:2608 [D loss: 0.548063, acc: 73.44%] [G loss: 3.594235]\n",
      "epoch:2 step:2609 [D loss: 0.583759, acc: 75.00%] [G loss: 2.984757]\n",
      "epoch:2 step:2610 [D loss: 0.588510, acc: 68.75%] [G loss: 3.309104]\n",
      "epoch:2 step:2611 [D loss: 0.491192, acc: 73.44%] [G loss: 3.125741]\n",
      "epoch:2 step:2612 [D loss: 0.591518, acc: 67.19%] [G loss: 3.041688]\n",
      "epoch:2 step:2613 [D loss: 0.648688, acc: 64.84%] [G loss: 3.037785]\n",
      "epoch:2 step:2614 [D loss: 0.575870, acc: 69.53%] [G loss: 2.932456]\n",
      "epoch:2 step:2615 [D loss: 0.605934, acc: 64.06%] [G loss: 3.083842]\n",
      "epoch:2 step:2616 [D loss: 0.670872, acc: 57.03%] [G loss: 2.839894]\n",
      "epoch:2 step:2617 [D loss: 0.503762, acc: 75.00%] [G loss: 3.227025]\n",
      "epoch:2 step:2618 [D loss: 0.594699, acc: 67.97%] [G loss: 2.868269]\n",
      "epoch:2 step:2619 [D loss: 0.494469, acc: 76.56%] [G loss: 3.208385]\n",
      "epoch:2 step:2620 [D loss: 0.540115, acc: 66.41%] [G loss: 3.509306]\n",
      "epoch:2 step:2621 [D loss: 0.471255, acc: 77.34%] [G loss: 3.759610]\n",
      "epoch:2 step:2622 [D loss: 0.537573, acc: 73.44%] [G loss: 3.315354]\n",
      "epoch:2 step:2623 [D loss: 0.502946, acc: 71.88%] [G loss: 2.912394]\n",
      "epoch:2 step:2624 [D loss: 0.573050, acc: 70.31%] [G loss: 3.182009]\n",
      "epoch:2 step:2625 [D loss: 0.585531, acc: 70.31%] [G loss: 3.244778]\n",
      "epoch:2 step:2626 [D loss: 0.541251, acc: 69.53%] [G loss: 3.402986]\n",
      "epoch:2 step:2627 [D loss: 0.522706, acc: 71.88%] [G loss: 3.316163]\n",
      "epoch:2 step:2628 [D loss: 0.498293, acc: 75.00%] [G loss: 3.345120]\n",
      "epoch:2 step:2629 [D loss: 0.445620, acc: 75.00%] [G loss: 3.313695]\n",
      "epoch:2 step:2630 [D loss: 0.534202, acc: 73.44%] [G loss: 3.278139]\n",
      "epoch:2 step:2631 [D loss: 0.530282, acc: 74.22%] [G loss: 3.309464]\n",
      "epoch:2 step:2632 [D loss: 0.487700, acc: 74.22%] [G loss: 3.577333]\n",
      "epoch:2 step:2633 [D loss: 0.519641, acc: 72.66%] [G loss: 3.393099]\n",
      "epoch:2 step:2634 [D loss: 0.521365, acc: 71.09%] [G loss: 3.444428]\n",
      "epoch:2 step:2635 [D loss: 0.513092, acc: 76.56%] [G loss: 3.411402]\n",
      "epoch:2 step:2636 [D loss: 0.554766, acc: 71.88%] [G loss: 3.186790]\n",
      "epoch:2 step:2637 [D loss: 0.421791, acc: 81.25%] [G loss: 3.433964]\n",
      "epoch:2 step:2638 [D loss: 0.567717, acc: 70.31%] [G loss: 3.607186]\n",
      "epoch:2 step:2639 [D loss: 0.697555, acc: 62.50%] [G loss: 3.375308]\n",
      "epoch:2 step:2640 [D loss: 0.677998, acc: 65.62%] [G loss: 2.818872]\n",
      "epoch:2 step:2641 [D loss: 0.583565, acc: 71.88%] [G loss: 2.768633]\n",
      "epoch:2 step:2642 [D loss: 0.508292, acc: 75.78%] [G loss: 3.021118]\n",
      "epoch:2 step:2643 [D loss: 0.574529, acc: 73.44%] [G loss: 2.842039]\n",
      "epoch:2 step:2644 [D loss: 0.549652, acc: 69.53%] [G loss: 2.928879]\n",
      "epoch:2 step:2645 [D loss: 0.542694, acc: 71.88%] [G loss: 3.316437]\n",
      "epoch:2 step:2646 [D loss: 0.566048, acc: 70.31%] [G loss: 3.130117]\n",
      "epoch:2 step:2647 [D loss: 0.542293, acc: 71.88%] [G loss: 3.129950]\n",
      "epoch:2 step:2648 [D loss: 0.507765, acc: 78.12%] [G loss: 3.511358]\n",
      "epoch:2 step:2649 [D loss: 0.508626, acc: 77.34%] [G loss: 3.422865]\n",
      "epoch:2 step:2650 [D loss: 0.587578, acc: 67.97%] [G loss: 2.849900]\n",
      "epoch:2 step:2651 [D loss: 0.631118, acc: 66.41%] [G loss: 3.070802]\n",
      "epoch:2 step:2652 [D loss: 0.497784, acc: 75.78%] [G loss: 2.740173]\n",
      "epoch:2 step:2653 [D loss: 0.594246, acc: 67.19%] [G loss: 2.891405]\n",
      "epoch:2 step:2654 [D loss: 0.497568, acc: 79.69%] [G loss: 3.102136]\n",
      "epoch:2 step:2655 [D loss: 0.517260, acc: 71.88%] [G loss: 3.208368]\n",
      "epoch:2 step:2656 [D loss: 0.534681, acc: 73.44%] [G loss: 2.896875]\n",
      "epoch:2 step:2657 [D loss: 0.598960, acc: 70.31%] [G loss: 2.834350]\n",
      "epoch:2 step:2658 [D loss: 0.613422, acc: 62.50%] [G loss: 3.362667]\n",
      "epoch:2 step:2659 [D loss: 0.595535, acc: 68.75%] [G loss: 2.730816]\n",
      "epoch:2 step:2660 [D loss: 0.628568, acc: 68.75%] [G loss: 3.148732]\n",
      "epoch:2 step:2661 [D loss: 0.631997, acc: 64.06%] [G loss: 3.104225]\n",
      "epoch:2 step:2662 [D loss: 0.522116, acc: 77.34%] [G loss: 2.688446]\n",
      "epoch:2 step:2663 [D loss: 0.570598, acc: 67.19%] [G loss: 2.997771]\n",
      "epoch:2 step:2664 [D loss: 0.544823, acc: 71.88%] [G loss: 2.776155]\n",
      "epoch:2 step:2665 [D loss: 0.548223, acc: 75.78%] [G loss: 3.233295]\n",
      "epoch:2 step:2666 [D loss: 0.456414, acc: 77.34%] [G loss: 3.388599]\n",
      "epoch:2 step:2667 [D loss: 0.541404, acc: 71.88%] [G loss: 3.148173]\n",
      "epoch:2 step:2668 [D loss: 0.543641, acc: 74.22%] [G loss: 2.976713]\n",
      "epoch:2 step:2669 [D loss: 0.583969, acc: 67.19%] [G loss: 4.004862]\n",
      "epoch:2 step:2670 [D loss: 0.522502, acc: 71.88%] [G loss: 3.491713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2671 [D loss: 0.474826, acc: 78.12%] [G loss: 3.084477]\n",
      "epoch:2 step:2672 [D loss: 0.629962, acc: 69.53%] [G loss: 3.408674]\n",
      "epoch:2 step:2673 [D loss: 0.531035, acc: 75.00%] [G loss: 3.020542]\n",
      "epoch:2 step:2674 [D loss: 0.742346, acc: 65.62%] [G loss: 3.164425]\n",
      "epoch:2 step:2675 [D loss: 0.534936, acc: 71.09%] [G loss: 2.767053]\n",
      "epoch:2 step:2676 [D loss: 0.643931, acc: 68.75%] [G loss: 3.001184]\n",
      "epoch:2 step:2677 [D loss: 0.436724, acc: 78.91%] [G loss: 3.077649]\n",
      "epoch:2 step:2678 [D loss: 0.662622, acc: 64.84%] [G loss: 2.738859]\n",
      "epoch:2 step:2679 [D loss: 0.507275, acc: 70.31%] [G loss: 3.391566]\n",
      "epoch:2 step:2680 [D loss: 0.478115, acc: 79.69%] [G loss: 3.687375]\n",
      "epoch:2 step:2681 [D loss: 0.522240, acc: 69.53%] [G loss: 3.205063]\n",
      "epoch:2 step:2682 [D loss: 0.595480, acc: 67.97%] [G loss: 2.840311]\n",
      "epoch:2 step:2683 [D loss: 0.626796, acc: 68.75%] [G loss: 3.111025]\n",
      "epoch:2 step:2684 [D loss: 0.549751, acc: 67.97%] [G loss: 2.777428]\n",
      "epoch:2 step:2685 [D loss: 0.574987, acc: 70.31%] [G loss: 3.053662]\n",
      "epoch:2 step:2686 [D loss: 0.625064, acc: 63.28%] [G loss: 2.775556]\n",
      "epoch:2 step:2687 [D loss: 0.572518, acc: 66.41%] [G loss: 2.756511]\n",
      "epoch:2 step:2688 [D loss: 0.553734, acc: 74.22%] [G loss: 3.255037]\n",
      "epoch:2 step:2689 [D loss: 0.541048, acc: 72.66%] [G loss: 3.047828]\n",
      "epoch:2 step:2690 [D loss: 0.604649, acc: 68.75%] [G loss: 3.073606]\n",
      "epoch:2 step:2691 [D loss: 0.594610, acc: 71.09%] [G loss: 3.019063]\n",
      "epoch:2 step:2692 [D loss: 0.604444, acc: 67.97%] [G loss: 3.109302]\n",
      "epoch:2 step:2693 [D loss: 0.531021, acc: 67.97%] [G loss: 2.881465]\n",
      "epoch:2 step:2694 [D loss: 0.579763, acc: 69.53%] [G loss: 2.988038]\n",
      "epoch:2 step:2695 [D loss: 0.548516, acc: 72.66%] [G loss: 2.901026]\n",
      "epoch:2 step:2696 [D loss: 0.466776, acc: 80.47%] [G loss: 3.318208]\n",
      "epoch:2 step:2697 [D loss: 0.487068, acc: 81.25%] [G loss: 3.061361]\n",
      "epoch:2 step:2698 [D loss: 0.663907, acc: 62.50%] [G loss: 2.734699]\n",
      "epoch:2 step:2699 [D loss: 0.600955, acc: 61.72%] [G loss: 2.781785]\n",
      "epoch:2 step:2700 [D loss: 0.510366, acc: 76.56%] [G loss: 2.828053]\n",
      "epoch:2 step:2701 [D loss: 0.668450, acc: 59.38%] [G loss: 2.974177]\n",
      "epoch:2 step:2702 [D loss: 0.627522, acc: 64.84%] [G loss: 2.990006]\n",
      "epoch:2 step:2703 [D loss: 0.584848, acc: 71.88%] [G loss: 2.846310]\n",
      "epoch:2 step:2704 [D loss: 0.511302, acc: 76.56%] [G loss: 2.793166]\n",
      "epoch:2 step:2705 [D loss: 0.542731, acc: 71.88%] [G loss: 3.356671]\n",
      "epoch:2 step:2706 [D loss: 0.574154, acc: 68.75%] [G loss: 2.849028]\n",
      "epoch:2 step:2707 [D loss: 0.520531, acc: 75.00%] [G loss: 3.247603]\n",
      "epoch:2 step:2708 [D loss: 0.609141, acc: 68.75%] [G loss: 2.947002]\n",
      "epoch:2 step:2709 [D loss: 0.570989, acc: 71.88%] [G loss: 2.824428]\n",
      "epoch:2 step:2710 [D loss: 0.638388, acc: 61.72%] [G loss: 2.752535]\n",
      "epoch:2 step:2711 [D loss: 0.499975, acc: 78.12%] [G loss: 2.972667]\n",
      "epoch:2 step:2712 [D loss: 0.554434, acc: 69.53%] [G loss: 3.138154]\n",
      "epoch:2 step:2713 [D loss: 0.612922, acc: 68.75%] [G loss: 2.717705]\n",
      "epoch:2 step:2714 [D loss: 0.552283, acc: 67.19%] [G loss: 2.595202]\n",
      "epoch:2 step:2715 [D loss: 0.459962, acc: 80.47%] [G loss: 3.104204]\n",
      "epoch:2 step:2716 [D loss: 0.580323, acc: 75.78%] [G loss: 3.244040]\n",
      "epoch:2 step:2717 [D loss: 0.573435, acc: 67.19%] [G loss: 3.265593]\n",
      "epoch:2 step:2718 [D loss: 0.570979, acc: 71.09%] [G loss: 2.998005]\n",
      "epoch:2 step:2719 [D loss: 0.526998, acc: 76.56%] [G loss: 2.976165]\n",
      "epoch:2 step:2720 [D loss: 0.561680, acc: 74.22%] [G loss: 2.846505]\n",
      "epoch:2 step:2721 [D loss: 0.517656, acc: 70.31%] [G loss: 3.069762]\n",
      "epoch:2 step:2722 [D loss: 0.468268, acc: 78.91%] [G loss: 2.906148]\n",
      "epoch:2 step:2723 [D loss: 0.579626, acc: 76.56%] [G loss: 2.876645]\n",
      "epoch:2 step:2724 [D loss: 0.554105, acc: 73.44%] [G loss: 3.361989]\n",
      "epoch:2 step:2725 [D loss: 0.521495, acc: 74.22%] [G loss: 3.239150]\n",
      "epoch:2 step:2726 [D loss: 0.523071, acc: 73.44%] [G loss: 3.474298]\n",
      "epoch:2 step:2727 [D loss: 0.538947, acc: 75.00%] [G loss: 3.392934]\n",
      "epoch:2 step:2728 [D loss: 0.478478, acc: 75.78%] [G loss: 3.230017]\n",
      "epoch:2 step:2729 [D loss: 0.539453, acc: 73.44%] [G loss: 3.252391]\n",
      "epoch:2 step:2730 [D loss: 0.612860, acc: 67.97%] [G loss: 2.828520]\n",
      "epoch:2 step:2731 [D loss: 0.515672, acc: 75.78%] [G loss: 3.261229]\n",
      "epoch:2 step:2732 [D loss: 0.665224, acc: 61.72%] [G loss: 2.838370]\n",
      "epoch:2 step:2733 [D loss: 0.658127, acc: 67.97%] [G loss: 2.732652]\n",
      "epoch:2 step:2734 [D loss: 0.490898, acc: 76.56%] [G loss: 3.175851]\n",
      "epoch:2 step:2735 [D loss: 0.559658, acc: 67.19%] [G loss: 3.092792]\n",
      "epoch:2 step:2736 [D loss: 0.593075, acc: 67.19%] [G loss: 3.192085]\n",
      "epoch:2 step:2737 [D loss: 0.534085, acc: 64.84%] [G loss: 3.027865]\n",
      "epoch:2 step:2738 [D loss: 0.631101, acc: 64.84%] [G loss: 3.104179]\n",
      "epoch:2 step:2739 [D loss: 0.576852, acc: 70.31%] [G loss: 3.100041]\n",
      "epoch:2 step:2740 [D loss: 0.513789, acc: 72.66%] [G loss: 3.330256]\n",
      "epoch:2 step:2741 [D loss: 0.616263, acc: 68.75%] [G loss: 2.869216]\n",
      "epoch:2 step:2742 [D loss: 0.561912, acc: 75.00%] [G loss: 2.843446]\n",
      "epoch:2 step:2743 [D loss: 0.567061, acc: 71.09%] [G loss: 3.437809]\n",
      "epoch:2 step:2744 [D loss: 0.541002, acc: 73.44%] [G loss: 2.940794]\n",
      "epoch:2 step:2745 [D loss: 0.561870, acc: 74.22%] [G loss: 3.161564]\n",
      "epoch:2 step:2746 [D loss: 0.507108, acc: 78.12%] [G loss: 3.518281]\n",
      "epoch:2 step:2747 [D loss: 0.530705, acc: 67.97%] [G loss: 3.032401]\n",
      "epoch:2 step:2748 [D loss: 0.593357, acc: 65.62%] [G loss: 2.998333]\n",
      "epoch:2 step:2749 [D loss: 0.558092, acc: 75.78%] [G loss: 2.942311]\n",
      "epoch:2 step:2750 [D loss: 0.548620, acc: 72.66%] [G loss: 2.895148]\n",
      "epoch:2 step:2751 [D loss: 0.531856, acc: 71.88%] [G loss: 2.789037]\n",
      "epoch:2 step:2752 [D loss: 0.650577, acc: 64.06%] [G loss: 2.653304]\n",
      "epoch:2 step:2753 [D loss: 0.432798, acc: 85.16%] [G loss: 3.035766]\n",
      "epoch:2 step:2754 [D loss: 0.575775, acc: 67.97%] [G loss: 3.128686]\n",
      "epoch:2 step:2755 [D loss: 0.542233, acc: 70.31%] [G loss: 3.216259]\n",
      "epoch:2 step:2756 [D loss: 0.540793, acc: 73.44%] [G loss: 2.898651]\n",
      "epoch:2 step:2757 [D loss: 0.565637, acc: 68.75%] [G loss: 3.048410]\n",
      "epoch:2 step:2758 [D loss: 0.547628, acc: 70.31%] [G loss: 2.991946]\n",
      "epoch:2 step:2759 [D loss: 0.523716, acc: 68.75%] [G loss: 3.503656]\n",
      "epoch:2 step:2760 [D loss: 0.463132, acc: 79.69%] [G loss: 3.930980]\n",
      "epoch:2 step:2761 [D loss: 0.487065, acc: 78.12%] [G loss: 3.621147]\n",
      "epoch:2 step:2762 [D loss: 0.609300, acc: 67.97%] [G loss: 3.416445]\n",
      "epoch:2 step:2763 [D loss: 0.488110, acc: 77.34%] [G loss: 3.434961]\n",
      "epoch:2 step:2764 [D loss: 0.456809, acc: 77.34%] [G loss: 3.758084]\n",
      "epoch:2 step:2765 [D loss: 0.658859, acc: 66.41%] [G loss: 3.009730]\n",
      "epoch:2 step:2766 [D loss: 0.624226, acc: 67.97%] [G loss: 3.103360]\n",
      "epoch:2 step:2767 [D loss: 0.468060, acc: 78.12%] [G loss: 2.831059]\n",
      "epoch:2 step:2768 [D loss: 0.516267, acc: 77.34%] [G loss: 2.769167]\n",
      "epoch:2 step:2769 [D loss: 0.578693, acc: 69.53%] [G loss: 3.095901]\n",
      "epoch:2 step:2770 [D loss: 0.541249, acc: 73.44%] [G loss: 2.872413]\n",
      "epoch:2 step:2771 [D loss: 0.541982, acc: 71.88%] [G loss: 2.973769]\n",
      "epoch:2 step:2772 [D loss: 0.436316, acc: 82.03%] [G loss: 3.542600]\n",
      "epoch:2 step:2773 [D loss: 0.530537, acc: 72.66%] [G loss: 2.885108]\n",
      "epoch:2 step:2774 [D loss: 0.536293, acc: 74.22%] [G loss: 2.953745]\n",
      "epoch:2 step:2775 [D loss: 0.572640, acc: 70.31%] [G loss: 3.421524]\n",
      "epoch:2 step:2776 [D loss: 0.563699, acc: 69.53%] [G loss: 2.818882]\n",
      "epoch:2 step:2777 [D loss: 0.624048, acc: 69.53%] [G loss: 3.097425]\n",
      "epoch:2 step:2778 [D loss: 0.550660, acc: 73.44%] [G loss: 3.551267]\n",
      "epoch:2 step:2779 [D loss: 0.518829, acc: 76.56%] [G loss: 3.126984]\n",
      "epoch:2 step:2780 [D loss: 0.463399, acc: 77.34%] [G loss: 3.929106]\n",
      "epoch:2 step:2781 [D loss: 0.538262, acc: 69.53%] [G loss: 3.135232]\n",
      "epoch:2 step:2782 [D loss: 0.496636, acc: 73.44%] [G loss: 3.045782]\n",
      "epoch:2 step:2783 [D loss: 0.492182, acc: 76.56%] [G loss: 3.648239]\n",
      "epoch:2 step:2784 [D loss: 0.509163, acc: 76.56%] [G loss: 3.540257]\n",
      "epoch:2 step:2785 [D loss: 0.470396, acc: 78.12%] [G loss: 3.232828]\n",
      "epoch:2 step:2786 [D loss: 0.450483, acc: 78.12%] [G loss: 3.946733]\n",
      "epoch:2 step:2787 [D loss: 0.568738, acc: 71.88%] [G loss: 3.056408]\n",
      "epoch:2 step:2788 [D loss: 0.500312, acc: 75.78%] [G loss: 3.429345]\n",
      "epoch:2 step:2789 [D loss: 0.597436, acc: 65.62%] [G loss: 3.150250]\n",
      "epoch:2 step:2790 [D loss: 0.513198, acc: 74.22%] [G loss: 3.551209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2791 [D loss: 0.534870, acc: 74.22%] [G loss: 3.601463]\n",
      "epoch:2 step:2792 [D loss: 0.505139, acc: 81.25%] [G loss: 3.965234]\n",
      "epoch:2 step:2793 [D loss: 0.559222, acc: 71.88%] [G loss: 3.287613]\n",
      "epoch:2 step:2794 [D loss: 0.649369, acc: 65.62%] [G loss: 3.365981]\n",
      "epoch:2 step:2795 [D loss: 0.513243, acc: 76.56%] [G loss: 3.421079]\n",
      "epoch:2 step:2796 [D loss: 0.567224, acc: 71.09%] [G loss: 3.415121]\n",
      "epoch:2 step:2797 [D loss: 0.483332, acc: 75.78%] [G loss: 3.522946]\n",
      "epoch:2 step:2798 [D loss: 0.499620, acc: 78.12%] [G loss: 3.594882]\n",
      "epoch:2 step:2799 [D loss: 0.492760, acc: 73.44%] [G loss: 3.330775]\n",
      "epoch:2 step:2800 [D loss: 0.540182, acc: 71.88%] [G loss: 4.085860]\n",
      "epoch:2 step:2801 [D loss: 0.561666, acc: 69.53%] [G loss: 3.157640]\n",
      "epoch:2 step:2802 [D loss: 0.819189, acc: 57.03%] [G loss: 3.224082]\n",
      "epoch:2 step:2803 [D loss: 0.652054, acc: 67.97%] [G loss: 3.509132]\n",
      "epoch:2 step:2804 [D loss: 0.566311, acc: 73.44%] [G loss: 3.639472]\n",
      "epoch:2 step:2805 [D loss: 0.513996, acc: 75.78%] [G loss: 3.259870]\n",
      "epoch:2 step:2806 [D loss: 0.617063, acc: 65.62%] [G loss: 2.949464]\n",
      "epoch:2 step:2807 [D loss: 0.598295, acc: 67.19%] [G loss: 3.160832]\n",
      "epoch:2 step:2808 [D loss: 0.530731, acc: 71.88%] [G loss: 3.662838]\n",
      "epoch:2 step:2809 [D loss: 0.516022, acc: 71.09%] [G loss: 3.316967]\n",
      "epoch:2 step:2810 [D loss: 0.438198, acc: 80.47%] [G loss: 3.604954]\n",
      "epoch:2 step:2811 [D loss: 0.593331, acc: 66.41%] [G loss: 3.522600]\n",
      "epoch:3 step:2812 [D loss: 0.559084, acc: 69.53%] [G loss: 3.180343]\n",
      "epoch:3 step:2813 [D loss: 0.515035, acc: 76.56%] [G loss: 3.645676]\n",
      "epoch:3 step:2814 [D loss: 0.550739, acc: 66.41%] [G loss: 3.146262]\n",
      "epoch:3 step:2815 [D loss: 0.628230, acc: 64.84%] [G loss: 3.107898]\n",
      "epoch:3 step:2816 [D loss: 0.535330, acc: 71.09%] [G loss: 3.289479]\n",
      "epoch:3 step:2817 [D loss: 0.564129, acc: 64.84%] [G loss: 2.942626]\n",
      "epoch:3 step:2818 [D loss: 0.534622, acc: 73.44%] [G loss: 3.238035]\n",
      "epoch:3 step:2819 [D loss: 0.520973, acc: 75.78%] [G loss: 3.645822]\n",
      "epoch:3 step:2820 [D loss: 0.494040, acc: 77.34%] [G loss: 3.332441]\n",
      "epoch:3 step:2821 [D loss: 0.581248, acc: 72.66%] [G loss: 3.018291]\n",
      "epoch:3 step:2822 [D loss: 0.586881, acc: 71.88%] [G loss: 2.934981]\n",
      "epoch:3 step:2823 [D loss: 0.634356, acc: 68.75%] [G loss: 2.924113]\n",
      "epoch:3 step:2824 [D loss: 0.581832, acc: 68.75%] [G loss: 2.692055]\n",
      "epoch:3 step:2825 [D loss: 0.547107, acc: 74.22%] [G loss: 2.900989]\n",
      "epoch:3 step:2826 [D loss: 0.489537, acc: 78.12%] [G loss: 3.251719]\n",
      "epoch:3 step:2827 [D loss: 0.545948, acc: 73.44%] [G loss: 3.096698]\n",
      "epoch:3 step:2828 [D loss: 0.668183, acc: 60.16%] [G loss: 2.862861]\n",
      "epoch:3 step:2829 [D loss: 0.670836, acc: 60.94%] [G loss: 2.345791]\n",
      "epoch:3 step:2830 [D loss: 0.577477, acc: 68.75%] [G loss: 2.800591]\n",
      "epoch:3 step:2831 [D loss: 0.634685, acc: 65.62%] [G loss: 2.566416]\n",
      "epoch:3 step:2832 [D loss: 0.616415, acc: 68.75%] [G loss: 2.988828]\n",
      "epoch:3 step:2833 [D loss: 0.560173, acc: 68.75%] [G loss: 3.092487]\n",
      "epoch:3 step:2834 [D loss: 0.521144, acc: 71.88%] [G loss: 2.700533]\n",
      "epoch:3 step:2835 [D loss: 0.588539, acc: 74.22%] [G loss: 3.031581]\n",
      "epoch:3 step:2836 [D loss: 0.574792, acc: 70.31%] [G loss: 3.094204]\n",
      "epoch:3 step:2837 [D loss: 0.576058, acc: 67.97%] [G loss: 2.998517]\n",
      "epoch:3 step:2838 [D loss: 0.583466, acc: 71.09%] [G loss: 3.208761]\n",
      "epoch:3 step:2839 [D loss: 0.555458, acc: 71.09%] [G loss: 2.852251]\n",
      "epoch:3 step:2840 [D loss: 0.482029, acc: 78.91%] [G loss: 2.981881]\n",
      "epoch:3 step:2841 [D loss: 0.632171, acc: 67.19%] [G loss: 2.770771]\n",
      "epoch:3 step:2842 [D loss: 0.477377, acc: 82.81%] [G loss: 2.981390]\n",
      "epoch:3 step:2843 [D loss: 0.586945, acc: 71.88%] [G loss: 3.134432]\n",
      "epoch:3 step:2844 [D loss: 0.616616, acc: 70.31%] [G loss: 2.803743]\n",
      "epoch:3 step:2845 [D loss: 0.456572, acc: 80.47%] [G loss: 3.476820]\n",
      "epoch:3 step:2846 [D loss: 0.552881, acc: 69.53%] [G loss: 3.296684]\n",
      "epoch:3 step:2847 [D loss: 0.578501, acc: 71.09%] [G loss: 3.309572]\n",
      "epoch:3 step:2848 [D loss: 0.514617, acc: 72.66%] [G loss: 2.991667]\n",
      "epoch:3 step:2849 [D loss: 0.594644, acc: 67.97%] [G loss: 3.047942]\n",
      "epoch:3 step:2850 [D loss: 0.586382, acc: 64.06%] [G loss: 3.146460]\n",
      "epoch:3 step:2851 [D loss: 0.522634, acc: 73.44%] [G loss: 3.078787]\n",
      "epoch:3 step:2852 [D loss: 0.551988, acc: 69.53%] [G loss: 3.091682]\n",
      "epoch:3 step:2853 [D loss: 0.511521, acc: 73.44%] [G loss: 3.175704]\n",
      "epoch:3 step:2854 [D loss: 0.598027, acc: 72.66%] [G loss: 3.249215]\n",
      "epoch:3 step:2855 [D loss: 0.605506, acc: 64.84%] [G loss: 2.930426]\n",
      "epoch:3 step:2856 [D loss: 0.584042, acc: 71.88%] [G loss: 2.932187]\n",
      "epoch:3 step:2857 [D loss: 0.651513, acc: 62.50%] [G loss: 3.006238]\n",
      "epoch:3 step:2858 [D loss: 0.521935, acc: 76.56%] [G loss: 3.333059]\n",
      "epoch:3 step:2859 [D loss: 0.585210, acc: 64.84%] [G loss: 2.738050]\n",
      "epoch:3 step:2860 [D loss: 0.675706, acc: 64.06%] [G loss: 2.882071]\n",
      "epoch:3 step:2861 [D loss: 0.482771, acc: 78.12%] [G loss: 2.949977]\n",
      "epoch:3 step:2862 [D loss: 0.493939, acc: 71.09%] [G loss: 3.285065]\n",
      "epoch:3 step:2863 [D loss: 0.585287, acc: 68.75%] [G loss: 2.659424]\n",
      "epoch:3 step:2864 [D loss: 0.623126, acc: 67.97%] [G loss: 2.981765]\n",
      "epoch:3 step:2865 [D loss: 0.586287, acc: 67.97%] [G loss: 2.924854]\n",
      "epoch:3 step:2866 [D loss: 0.514180, acc: 78.91%] [G loss: 3.376858]\n",
      "epoch:3 step:2867 [D loss: 0.555467, acc: 75.00%] [G loss: 3.261678]\n",
      "epoch:3 step:2868 [D loss: 0.581496, acc: 68.75%] [G loss: 2.935235]\n",
      "epoch:3 step:2869 [D loss: 0.621048, acc: 64.06%] [G loss: 2.990434]\n",
      "epoch:3 step:2870 [D loss: 0.532300, acc: 72.66%] [G loss: 3.288116]\n",
      "epoch:3 step:2871 [D loss: 0.614057, acc: 64.84%] [G loss: 3.080023]\n",
      "epoch:3 step:2872 [D loss: 0.543675, acc: 78.91%] [G loss: 3.300714]\n",
      "epoch:3 step:2873 [D loss: 0.579680, acc: 71.88%] [G loss: 2.898636]\n",
      "epoch:3 step:2874 [D loss: 0.534786, acc: 73.44%] [G loss: 3.095506]\n",
      "epoch:3 step:2875 [D loss: 0.591448, acc: 72.66%] [G loss: 2.867656]\n",
      "epoch:3 step:2876 [D loss: 0.556494, acc: 71.09%] [G loss: 3.205689]\n",
      "epoch:3 step:2877 [D loss: 0.585283, acc: 70.31%] [G loss: 2.786830]\n",
      "epoch:3 step:2878 [D loss: 0.506475, acc: 78.12%] [G loss: 3.109687]\n",
      "epoch:3 step:2879 [D loss: 0.553612, acc: 68.75%] [G loss: 2.923063]\n",
      "epoch:3 step:2880 [D loss: 0.545882, acc: 70.31%] [G loss: 3.104572]\n",
      "epoch:3 step:2881 [D loss: 0.532660, acc: 77.34%] [G loss: 2.995540]\n",
      "epoch:3 step:2882 [D loss: 0.558545, acc: 67.97%] [G loss: 3.219816]\n",
      "epoch:3 step:2883 [D loss: 0.499531, acc: 76.56%] [G loss: 3.259948]\n",
      "epoch:3 step:2884 [D loss: 0.489515, acc: 82.81%] [G loss: 3.017401]\n",
      "epoch:3 step:2885 [D loss: 0.525139, acc: 72.66%] [G loss: 3.188188]\n",
      "epoch:3 step:2886 [D loss: 0.511183, acc: 76.56%] [G loss: 3.664138]\n",
      "epoch:3 step:2887 [D loss: 0.511348, acc: 75.00%] [G loss: 3.374135]\n",
      "epoch:3 step:2888 [D loss: 0.417721, acc: 82.81%] [G loss: 3.729809]\n",
      "epoch:3 step:2889 [D loss: 0.622410, acc: 67.19%] [G loss: 3.452090]\n",
      "epoch:3 step:2890 [D loss: 0.555749, acc: 71.88%] [G loss: 3.277088]\n",
      "epoch:3 step:2891 [D loss: 0.529709, acc: 73.44%] [G loss: 3.036585]\n",
      "epoch:3 step:2892 [D loss: 0.609424, acc: 67.97%] [G loss: 3.008219]\n",
      "epoch:3 step:2893 [D loss: 0.479156, acc: 78.91%] [G loss: 3.125503]\n",
      "epoch:3 step:2894 [D loss: 0.578995, acc: 67.97%] [G loss: 3.191771]\n",
      "epoch:3 step:2895 [D loss: 0.611792, acc: 67.19%] [G loss: 2.898664]\n",
      "epoch:3 step:2896 [D loss: 0.512381, acc: 72.66%] [G loss: 3.129604]\n",
      "epoch:3 step:2897 [D loss: 0.534100, acc: 76.56%] [G loss: 2.983572]\n",
      "epoch:3 step:2898 [D loss: 0.539064, acc: 74.22%] [G loss: 3.287432]\n",
      "epoch:3 step:2899 [D loss: 0.543440, acc: 75.00%] [G loss: 3.305531]\n",
      "epoch:3 step:2900 [D loss: 0.493793, acc: 76.56%] [G loss: 3.294538]\n",
      "epoch:3 step:2901 [D loss: 0.514596, acc: 70.31%] [G loss: 3.195488]\n",
      "epoch:3 step:2902 [D loss: 0.553103, acc: 79.69%] [G loss: 3.126244]\n",
      "epoch:3 step:2903 [D loss: 0.586342, acc: 69.53%] [G loss: 3.089107]\n",
      "epoch:3 step:2904 [D loss: 0.492382, acc: 74.22%] [G loss: 3.388022]\n",
      "epoch:3 step:2905 [D loss: 0.610732, acc: 67.19%] [G loss: 3.025950]\n",
      "epoch:3 step:2906 [D loss: 0.586022, acc: 64.06%] [G loss: 2.964396]\n",
      "epoch:3 step:2907 [D loss: 0.553196, acc: 67.19%] [G loss: 3.148133]\n",
      "epoch:3 step:2908 [D loss: 0.615178, acc: 68.75%] [G loss: 2.945929]\n",
      "epoch:3 step:2909 [D loss: 0.576246, acc: 73.44%] [G loss: 2.929032]\n",
      "epoch:3 step:2910 [D loss: 0.500452, acc: 76.56%] [G loss: 2.898343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2911 [D loss: 0.474291, acc: 78.91%] [G loss: 3.023186]\n",
      "epoch:3 step:2912 [D loss: 0.488796, acc: 76.56%] [G loss: 3.396644]\n",
      "epoch:3 step:2913 [D loss: 0.565870, acc: 67.97%] [G loss: 3.119558]\n",
      "epoch:3 step:2914 [D loss: 0.528432, acc: 67.19%] [G loss: 4.136780]\n",
      "epoch:3 step:2915 [D loss: 0.645116, acc: 69.53%] [G loss: 2.945614]\n",
      "epoch:3 step:2916 [D loss: 0.591733, acc: 71.09%] [G loss: 3.272461]\n",
      "epoch:3 step:2917 [D loss: 0.587257, acc: 68.75%] [G loss: 3.030831]\n",
      "epoch:3 step:2918 [D loss: 0.604017, acc: 64.06%] [G loss: 2.711658]\n",
      "epoch:3 step:2919 [D loss: 0.632576, acc: 67.19%] [G loss: 2.830700]\n",
      "epoch:3 step:2920 [D loss: 0.582196, acc: 71.09%] [G loss: 2.851403]\n",
      "epoch:3 step:2921 [D loss: 0.574851, acc: 69.53%] [G loss: 2.927088]\n",
      "epoch:3 step:2922 [D loss: 0.595848, acc: 67.19%] [G loss: 3.285254]\n",
      "epoch:3 step:2923 [D loss: 0.571472, acc: 75.00%] [G loss: 2.937643]\n",
      "epoch:3 step:2924 [D loss: 0.551722, acc: 72.66%] [G loss: 2.753758]\n",
      "epoch:3 step:2925 [D loss: 0.610936, acc: 65.62%] [G loss: 2.740265]\n",
      "epoch:3 step:2926 [D loss: 0.664567, acc: 64.06%] [G loss: 3.015174]\n",
      "epoch:3 step:2927 [D loss: 0.576144, acc: 69.53%] [G loss: 3.085005]\n",
      "epoch:3 step:2928 [D loss: 0.522096, acc: 73.44%] [G loss: 3.191044]\n",
      "epoch:3 step:2929 [D loss: 0.505530, acc: 76.56%] [G loss: 3.558589]\n",
      "epoch:3 step:2930 [D loss: 0.526544, acc: 73.44%] [G loss: 3.662444]\n",
      "epoch:3 step:2931 [D loss: 0.640916, acc: 65.62%] [G loss: 3.213573]\n",
      "epoch:3 step:2932 [D loss: 0.560567, acc: 67.97%] [G loss: 2.998611]\n",
      "epoch:3 step:2933 [D loss: 0.558671, acc: 74.22%] [G loss: 3.184159]\n",
      "epoch:3 step:2934 [D loss: 0.576743, acc: 66.41%] [G loss: 3.115836]\n",
      "epoch:3 step:2935 [D loss: 0.674194, acc: 60.16%] [G loss: 2.540339]\n",
      "epoch:3 step:2936 [D loss: 0.519951, acc: 71.88%] [G loss: 3.000257]\n",
      "epoch:3 step:2937 [D loss: 0.657291, acc: 67.19%] [G loss: 2.722020]\n",
      "epoch:3 step:2938 [D loss: 0.538451, acc: 69.53%] [G loss: 3.021976]\n",
      "epoch:3 step:2939 [D loss: 0.473678, acc: 78.91%] [G loss: 3.411955]\n",
      "epoch:3 step:2940 [D loss: 0.563744, acc: 69.53%] [G loss: 3.128692]\n",
      "epoch:3 step:2941 [D loss: 0.547817, acc: 73.44%] [G loss: 3.246233]\n",
      "epoch:3 step:2942 [D loss: 0.503248, acc: 74.22%] [G loss: 3.635906]\n",
      "epoch:3 step:2943 [D loss: 0.496211, acc: 75.78%] [G loss: 3.192575]\n",
      "epoch:3 step:2944 [D loss: 0.636219, acc: 65.62%] [G loss: 2.944944]\n",
      "epoch:3 step:2945 [D loss: 0.590004, acc: 71.88%] [G loss: 3.408670]\n",
      "epoch:3 step:2946 [D loss: 0.612881, acc: 64.84%] [G loss: 3.018010]\n",
      "epoch:3 step:2947 [D loss: 0.591625, acc: 68.75%] [G loss: 2.929363]\n",
      "epoch:3 step:2948 [D loss: 0.566565, acc: 73.44%] [G loss: 2.663190]\n",
      "epoch:3 step:2949 [D loss: 0.514072, acc: 75.78%] [G loss: 3.027002]\n",
      "epoch:3 step:2950 [D loss: 0.524945, acc: 72.66%] [G loss: 2.916372]\n",
      "epoch:3 step:2951 [D loss: 0.657276, acc: 61.72%] [G loss: 3.001307]\n",
      "epoch:3 step:2952 [D loss: 0.563967, acc: 72.66%] [G loss: 2.679215]\n",
      "epoch:3 step:2953 [D loss: 0.564321, acc: 71.09%] [G loss: 2.696686]\n",
      "epoch:3 step:2954 [D loss: 0.538595, acc: 68.75%] [G loss: 2.775876]\n",
      "epoch:3 step:2955 [D loss: 0.626328, acc: 67.19%] [G loss: 2.976650]\n",
      "epoch:3 step:2956 [D loss: 0.515545, acc: 74.22%] [G loss: 3.017524]\n",
      "epoch:3 step:2957 [D loss: 0.550670, acc: 74.22%] [G loss: 3.042969]\n",
      "epoch:3 step:2958 [D loss: 0.554739, acc: 72.66%] [G loss: 2.819154]\n",
      "epoch:3 step:2959 [D loss: 0.543703, acc: 78.12%] [G loss: 3.209580]\n",
      "epoch:3 step:2960 [D loss: 0.550431, acc: 72.66%] [G loss: 2.980383]\n",
      "epoch:3 step:2961 [D loss: 0.588421, acc: 67.19%] [G loss: 3.443967]\n",
      "epoch:3 step:2962 [D loss: 0.559120, acc: 75.00%] [G loss: 3.243566]\n",
      "epoch:3 step:2963 [D loss: 0.432766, acc: 79.69%] [G loss: 3.035045]\n",
      "epoch:3 step:2964 [D loss: 0.660034, acc: 63.28%] [G loss: 2.566574]\n",
      "epoch:3 step:2965 [D loss: 0.531127, acc: 75.00%] [G loss: 2.878317]\n",
      "epoch:3 step:2966 [D loss: 0.551079, acc: 72.66%] [G loss: 3.048883]\n",
      "epoch:3 step:2967 [D loss: 0.583295, acc: 67.97%] [G loss: 3.149623]\n",
      "epoch:3 step:2968 [D loss: 0.578194, acc: 71.09%] [G loss: 2.982644]\n",
      "epoch:3 step:2969 [D loss: 0.520779, acc: 74.22%] [G loss: 2.939773]\n",
      "epoch:3 step:2970 [D loss: 0.510442, acc: 80.47%] [G loss: 3.537187]\n",
      "epoch:3 step:2971 [D loss: 0.678657, acc: 65.62%] [G loss: 2.796793]\n",
      "epoch:3 step:2972 [D loss: 0.569327, acc: 75.78%] [G loss: 2.570668]\n",
      "epoch:3 step:2973 [D loss: 0.508689, acc: 72.66%] [G loss: 3.474850]\n",
      "epoch:3 step:2974 [D loss: 0.554360, acc: 76.56%] [G loss: 3.213678]\n",
      "epoch:3 step:2975 [D loss: 0.565432, acc: 67.19%] [G loss: 2.901140]\n",
      "epoch:3 step:2976 [D loss: 0.553334, acc: 75.78%] [G loss: 2.921154]\n",
      "epoch:3 step:2977 [D loss: 0.548565, acc: 75.78%] [G loss: 3.210735]\n",
      "epoch:3 step:2978 [D loss: 0.518642, acc: 73.44%] [G loss: 3.445042]\n",
      "epoch:3 step:2979 [D loss: 0.513651, acc: 77.34%] [G loss: 3.147013]\n",
      "epoch:3 step:2980 [D loss: 0.551455, acc: 71.09%] [G loss: 3.012513]\n",
      "epoch:3 step:2981 [D loss: 0.514752, acc: 72.66%] [G loss: 2.879802]\n",
      "epoch:3 step:2982 [D loss: 0.544342, acc: 75.00%] [G loss: 3.331602]\n",
      "epoch:3 step:2983 [D loss: 0.529407, acc: 74.22%] [G loss: 3.159560]\n",
      "epoch:3 step:2984 [D loss: 0.610035, acc: 62.50%] [G loss: 2.938793]\n",
      "epoch:3 step:2985 [D loss: 0.542339, acc: 73.44%] [G loss: 3.075755]\n",
      "epoch:3 step:2986 [D loss: 0.549604, acc: 76.56%] [G loss: 3.166788]\n",
      "epoch:3 step:2987 [D loss: 0.532555, acc: 74.22%] [G loss: 3.337592]\n",
      "epoch:3 step:2988 [D loss: 0.601811, acc: 70.31%] [G loss: 3.229027]\n",
      "epoch:3 step:2989 [D loss: 0.497970, acc: 74.22%] [G loss: 3.121255]\n",
      "epoch:3 step:2990 [D loss: 0.643836, acc: 64.06%] [G loss: 3.172621]\n",
      "epoch:3 step:2991 [D loss: 0.544996, acc: 72.66%] [G loss: 2.848273]\n",
      "epoch:3 step:2992 [D loss: 0.613742, acc: 68.75%] [G loss: 3.250594]\n",
      "epoch:3 step:2993 [D loss: 0.640324, acc: 63.28%] [G loss: 3.081135]\n",
      "epoch:3 step:2994 [D loss: 0.604499, acc: 67.97%] [G loss: 3.127383]\n",
      "epoch:3 step:2995 [D loss: 0.541463, acc: 71.88%] [G loss: 3.057357]\n",
      "epoch:3 step:2996 [D loss: 0.522545, acc: 76.56%] [G loss: 2.918988]\n",
      "epoch:3 step:2997 [D loss: 0.654535, acc: 66.41%] [G loss: 3.061665]\n",
      "epoch:3 step:2998 [D loss: 0.637449, acc: 62.50%] [G loss: 2.703432]\n",
      "epoch:3 step:2999 [D loss: 0.509729, acc: 74.22%] [G loss: 3.236230]\n",
      "epoch:3 step:3000 [D loss: 0.611289, acc: 64.06%] [G loss: 3.141104]\n",
      "epoch:3 step:3001 [D loss: 0.505453, acc: 78.12%] [G loss: 3.212834]\n",
      "epoch:3 step:3002 [D loss: 0.509993, acc: 74.22%] [G loss: 3.049732]\n",
      "epoch:3 step:3003 [D loss: 0.540861, acc: 71.09%] [G loss: 3.256625]\n",
      "epoch:3 step:3004 [D loss: 0.482866, acc: 74.22%] [G loss: 3.033713]\n",
      "epoch:3 step:3005 [D loss: 0.527761, acc: 75.78%] [G loss: 3.444431]\n",
      "epoch:3 step:3006 [D loss: 0.592768, acc: 72.66%] [G loss: 3.143316]\n",
      "epoch:3 step:3007 [D loss: 0.605984, acc: 68.75%] [G loss: 2.768556]\n",
      "epoch:3 step:3008 [D loss: 0.498604, acc: 79.69%] [G loss: 3.402076]\n",
      "epoch:3 step:3009 [D loss: 0.507639, acc: 78.12%] [G loss: 2.921755]\n",
      "epoch:3 step:3010 [D loss: 0.562961, acc: 70.31%] [G loss: 2.888434]\n",
      "epoch:3 step:3011 [D loss: 0.619472, acc: 67.19%] [G loss: 2.872285]\n",
      "epoch:3 step:3012 [D loss: 0.583094, acc: 71.09%] [G loss: 3.057086]\n",
      "epoch:3 step:3013 [D loss: 0.567960, acc: 75.00%] [G loss: 2.926192]\n",
      "epoch:3 step:3014 [D loss: 0.599870, acc: 64.84%] [G loss: 2.780190]\n",
      "epoch:3 step:3015 [D loss: 0.677516, acc: 62.50%] [G loss: 2.848887]\n",
      "epoch:3 step:3016 [D loss: 0.517343, acc: 76.56%] [G loss: 3.422133]\n",
      "epoch:3 step:3017 [D loss: 0.495185, acc: 75.78%] [G loss: 3.023356]\n",
      "epoch:3 step:3018 [D loss: 0.546061, acc: 78.12%] [G loss: 3.311985]\n",
      "epoch:3 step:3019 [D loss: 0.483402, acc: 78.91%] [G loss: 3.379650]\n",
      "epoch:3 step:3020 [D loss: 0.507231, acc: 77.34%] [G loss: 3.079203]\n",
      "epoch:3 step:3021 [D loss: 0.515979, acc: 72.66%] [G loss: 3.065343]\n",
      "epoch:3 step:3022 [D loss: 0.482398, acc: 75.00%] [G loss: 2.909987]\n",
      "epoch:3 step:3023 [D loss: 0.545433, acc: 75.00%] [G loss: 3.190499]\n",
      "epoch:3 step:3024 [D loss: 0.508830, acc: 75.00%] [G loss: 3.130365]\n",
      "epoch:3 step:3025 [D loss: 0.627038, acc: 65.62%] [G loss: 2.645253]\n",
      "epoch:3 step:3026 [D loss: 0.574807, acc: 71.09%] [G loss: 2.753115]\n",
      "epoch:3 step:3027 [D loss: 0.628573, acc: 69.53%] [G loss: 2.639925]\n",
      "epoch:3 step:3028 [D loss: 0.459991, acc: 77.34%] [G loss: 3.296142]\n",
      "epoch:3 step:3029 [D loss: 0.529145, acc: 75.78%] [G loss: 3.037022]\n",
      "epoch:3 step:3030 [D loss: 0.548012, acc: 75.78%] [G loss: 2.819415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3031 [D loss: 0.541654, acc: 72.66%] [G loss: 3.099984]\n",
      "epoch:3 step:3032 [D loss: 0.576399, acc: 71.09%] [G loss: 3.367295]\n",
      "epoch:3 step:3033 [D loss: 0.640674, acc: 67.19%] [G loss: 3.109716]\n",
      "epoch:3 step:3034 [D loss: 0.546635, acc: 74.22%] [G loss: 3.247093]\n",
      "epoch:3 step:3035 [D loss: 0.699060, acc: 66.41%] [G loss: 3.313499]\n",
      "epoch:3 step:3036 [D loss: 0.626381, acc: 70.31%] [G loss: 2.950942]\n",
      "epoch:3 step:3037 [D loss: 0.540898, acc: 72.66%] [G loss: 2.938257]\n",
      "epoch:3 step:3038 [D loss: 0.598214, acc: 66.41%] [G loss: 2.940567]\n",
      "epoch:3 step:3039 [D loss: 0.601140, acc: 63.28%] [G loss: 2.959123]\n",
      "epoch:3 step:3040 [D loss: 0.543866, acc: 80.47%] [G loss: 2.928683]\n",
      "epoch:3 step:3041 [D loss: 0.518446, acc: 75.00%] [G loss: 3.243468]\n",
      "epoch:3 step:3042 [D loss: 0.556792, acc: 69.53%] [G loss: 3.331018]\n",
      "epoch:3 step:3043 [D loss: 0.462735, acc: 80.47%] [G loss: 3.672044]\n",
      "epoch:3 step:3044 [D loss: 0.776172, acc: 52.34%] [G loss: 3.125572]\n",
      "epoch:3 step:3045 [D loss: 0.534413, acc: 76.56%] [G loss: 2.996150]\n",
      "epoch:3 step:3046 [D loss: 0.535884, acc: 69.53%] [G loss: 3.210230]\n",
      "epoch:3 step:3047 [D loss: 0.550116, acc: 73.44%] [G loss: 2.928114]\n",
      "epoch:3 step:3048 [D loss: 0.572633, acc: 67.19%] [G loss: 2.972190]\n",
      "epoch:3 step:3049 [D loss: 0.544644, acc: 70.31%] [G loss: 3.106726]\n",
      "epoch:3 step:3050 [D loss: 0.561599, acc: 70.31%] [G loss: 2.870185]\n",
      "epoch:3 step:3051 [D loss: 0.527394, acc: 72.66%] [G loss: 3.230865]\n",
      "epoch:3 step:3052 [D loss: 0.654965, acc: 71.09%] [G loss: 2.897312]\n",
      "epoch:3 step:3053 [D loss: 0.580645, acc: 68.75%] [G loss: 2.583050]\n",
      "epoch:3 step:3054 [D loss: 0.567159, acc: 73.44%] [G loss: 2.699660]\n",
      "epoch:3 step:3055 [D loss: 0.556544, acc: 75.00%] [G loss: 3.102801]\n",
      "epoch:3 step:3056 [D loss: 0.503224, acc: 75.00%] [G loss: 3.145723]\n",
      "epoch:3 step:3057 [D loss: 0.640545, acc: 61.72%] [G loss: 2.992063]\n",
      "epoch:3 step:3058 [D loss: 0.625195, acc: 66.41%] [G loss: 2.957693]\n",
      "epoch:3 step:3059 [D loss: 0.600102, acc: 69.53%] [G loss: 2.802533]\n",
      "epoch:3 step:3060 [D loss: 0.604259, acc: 71.88%] [G loss: 3.069472]\n",
      "epoch:3 step:3061 [D loss: 0.666730, acc: 66.41%] [G loss: 2.703302]\n",
      "epoch:3 step:3062 [D loss: 0.586963, acc: 67.19%] [G loss: 2.791780]\n",
      "epoch:3 step:3063 [D loss: 0.530322, acc: 73.44%] [G loss: 2.759397]\n",
      "epoch:3 step:3064 [D loss: 0.492108, acc: 71.88%] [G loss: 2.562840]\n",
      "epoch:3 step:3065 [D loss: 0.485636, acc: 78.91%] [G loss: 3.218102]\n",
      "epoch:3 step:3066 [D loss: 0.545652, acc: 75.00%] [G loss: 3.113977]\n",
      "epoch:3 step:3067 [D loss: 0.541860, acc: 70.31%] [G loss: 3.128889]\n",
      "epoch:3 step:3068 [D loss: 0.510969, acc: 78.91%] [G loss: 3.038761]\n",
      "epoch:3 step:3069 [D loss: 0.503936, acc: 76.56%] [G loss: 3.521604]\n",
      "epoch:3 step:3070 [D loss: 0.497507, acc: 80.47%] [G loss: 3.156440]\n",
      "epoch:3 step:3071 [D loss: 0.484286, acc: 75.00%] [G loss: 3.168190]\n",
      "epoch:3 step:3072 [D loss: 0.559160, acc: 68.75%] [G loss: 3.309011]\n",
      "epoch:3 step:3073 [D loss: 0.485485, acc: 77.34%] [G loss: 3.203221]\n",
      "epoch:3 step:3074 [D loss: 0.550791, acc: 74.22%] [G loss: 2.921596]\n",
      "epoch:3 step:3075 [D loss: 0.513731, acc: 74.22%] [G loss: 3.465959]\n",
      "epoch:3 step:3076 [D loss: 0.546660, acc: 72.66%] [G loss: 3.065319]\n",
      "epoch:3 step:3077 [D loss: 0.492395, acc: 76.56%] [G loss: 3.441310]\n",
      "epoch:3 step:3078 [D loss: 0.557725, acc: 73.44%] [G loss: 3.271371]\n",
      "epoch:3 step:3079 [D loss: 0.538042, acc: 71.88%] [G loss: 3.029983]\n",
      "epoch:3 step:3080 [D loss: 0.582353, acc: 74.22%] [G loss: 3.332340]\n",
      "epoch:3 step:3081 [D loss: 0.532664, acc: 75.00%] [G loss: 3.145319]\n",
      "epoch:3 step:3082 [D loss: 0.470019, acc: 77.34%] [G loss: 3.243573]\n",
      "epoch:3 step:3083 [D loss: 0.503327, acc: 77.34%] [G loss: 3.226386]\n",
      "epoch:3 step:3084 [D loss: 0.499097, acc: 75.00%] [G loss: 3.214738]\n",
      "epoch:3 step:3085 [D loss: 0.586014, acc: 67.19%] [G loss: 2.842775]\n",
      "epoch:3 step:3086 [D loss: 0.583014, acc: 67.97%] [G loss: 3.081378]\n",
      "epoch:3 step:3087 [D loss: 0.573906, acc: 71.88%] [G loss: 2.960607]\n",
      "epoch:3 step:3088 [D loss: 0.612673, acc: 69.53%] [G loss: 3.013037]\n",
      "epoch:3 step:3089 [D loss: 0.556285, acc: 71.09%] [G loss: 2.909411]\n",
      "epoch:3 step:3090 [D loss: 0.536345, acc: 75.00%] [G loss: 3.038350]\n",
      "epoch:3 step:3091 [D loss: 0.541060, acc: 72.66%] [G loss: 3.013122]\n",
      "epoch:3 step:3092 [D loss: 0.620214, acc: 64.84%] [G loss: 3.051777]\n",
      "epoch:3 step:3093 [D loss: 0.565570, acc: 71.09%] [G loss: 2.954454]\n",
      "epoch:3 step:3094 [D loss: 0.496463, acc: 75.00%] [G loss: 3.021152]\n",
      "epoch:3 step:3095 [D loss: 0.454495, acc: 81.25%] [G loss: 3.264435]\n",
      "epoch:3 step:3096 [D loss: 0.535052, acc: 67.97%] [G loss: 2.937065]\n",
      "epoch:3 step:3097 [D loss: 0.522467, acc: 76.56%] [G loss: 3.236312]\n",
      "epoch:3 step:3098 [D loss: 0.600304, acc: 66.41%] [G loss: 2.609467]\n",
      "epoch:3 step:3099 [D loss: 0.556926, acc: 71.88%] [G loss: 2.990237]\n",
      "epoch:3 step:3100 [D loss: 0.516982, acc: 75.00%] [G loss: 3.105622]\n",
      "epoch:3 step:3101 [D loss: 0.555976, acc: 73.44%] [G loss: 3.033806]\n",
      "epoch:3 step:3102 [D loss: 0.559020, acc: 72.66%] [G loss: 2.472842]\n",
      "epoch:3 step:3103 [D loss: 0.532664, acc: 71.88%] [G loss: 2.774522]\n",
      "epoch:3 step:3104 [D loss: 0.606102, acc: 68.75%] [G loss: 2.919827]\n",
      "epoch:3 step:3105 [D loss: 0.464486, acc: 80.47%] [G loss: 3.412647]\n",
      "epoch:3 step:3106 [D loss: 0.491780, acc: 72.66%] [G loss: 3.350590]\n",
      "epoch:3 step:3107 [D loss: 0.552300, acc: 71.88%] [G loss: 3.058366]\n",
      "epoch:3 step:3108 [D loss: 0.546875, acc: 71.09%] [G loss: 3.129247]\n",
      "epoch:3 step:3109 [D loss: 0.527736, acc: 71.88%] [G loss: 3.467041]\n",
      "epoch:3 step:3110 [D loss: 0.538867, acc: 70.31%] [G loss: 3.212456]\n",
      "epoch:3 step:3111 [D loss: 0.598992, acc: 69.53%] [G loss: 3.510156]\n",
      "epoch:3 step:3112 [D loss: 0.576077, acc: 69.53%] [G loss: 2.644756]\n",
      "epoch:3 step:3113 [D loss: 0.647518, acc: 64.84%] [G loss: 2.665198]\n",
      "epoch:3 step:3114 [D loss: 0.542344, acc: 73.44%] [G loss: 2.771750]\n",
      "epoch:3 step:3115 [D loss: 0.542348, acc: 72.66%] [G loss: 3.226449]\n",
      "epoch:3 step:3116 [D loss: 0.480445, acc: 72.66%] [G loss: 2.770926]\n",
      "epoch:3 step:3117 [D loss: 0.584767, acc: 71.09%] [G loss: 2.736474]\n",
      "epoch:3 step:3118 [D loss: 0.473854, acc: 78.12%] [G loss: 3.273441]\n",
      "epoch:3 step:3119 [D loss: 0.570555, acc: 75.00%] [G loss: 2.992604]\n",
      "epoch:3 step:3120 [D loss: 0.462615, acc: 74.22%] [G loss: 3.254938]\n",
      "epoch:3 step:3121 [D loss: 0.502973, acc: 75.78%] [G loss: 3.210935]\n",
      "epoch:3 step:3122 [D loss: 0.534442, acc: 75.00%] [G loss: 3.189706]\n",
      "epoch:3 step:3123 [D loss: 0.395297, acc: 83.59%] [G loss: 3.837857]\n",
      "epoch:3 step:3124 [D loss: 0.484402, acc: 78.91%] [G loss: 3.913773]\n",
      "epoch:3 step:3125 [D loss: 0.430569, acc: 82.03%] [G loss: 4.003709]\n",
      "epoch:3 step:3126 [D loss: 0.448730, acc: 81.25%] [G loss: 3.754119]\n",
      "epoch:3 step:3127 [D loss: 0.517823, acc: 71.09%] [G loss: 2.862288]\n",
      "epoch:3 step:3128 [D loss: 0.560740, acc: 71.88%] [G loss: 3.431538]\n",
      "epoch:3 step:3129 [D loss: 0.575892, acc: 71.88%] [G loss: 3.223257]\n",
      "epoch:3 step:3130 [D loss: 0.545755, acc: 73.44%] [G loss: 3.439926]\n",
      "epoch:3 step:3131 [D loss: 0.483840, acc: 79.69%] [G loss: 3.046042]\n",
      "epoch:3 step:3132 [D loss: 0.451271, acc: 78.91%] [G loss: 3.209022]\n",
      "epoch:3 step:3133 [D loss: 0.523982, acc: 77.34%] [G loss: 3.219071]\n",
      "epoch:3 step:3134 [D loss: 0.571666, acc: 74.22%] [G loss: 2.920116]\n",
      "epoch:3 step:3135 [D loss: 0.585137, acc: 68.75%] [G loss: 3.254767]\n",
      "epoch:3 step:3136 [D loss: 0.556848, acc: 67.97%] [G loss: 3.304286]\n",
      "epoch:3 step:3137 [D loss: 0.483176, acc: 74.22%] [G loss: 3.237917]\n",
      "epoch:3 step:3138 [D loss: 0.576518, acc: 69.53%] [G loss: 3.376087]\n",
      "epoch:3 step:3139 [D loss: 0.568529, acc: 74.22%] [G loss: 3.446318]\n",
      "epoch:3 step:3140 [D loss: 0.621617, acc: 69.53%] [G loss: 2.940217]\n",
      "epoch:3 step:3141 [D loss: 0.514651, acc: 75.78%] [G loss: 3.194700]\n",
      "epoch:3 step:3142 [D loss: 0.568841, acc: 65.62%] [G loss: 2.906961]\n",
      "epoch:3 step:3143 [D loss: 0.504386, acc: 76.56%] [G loss: 3.098530]\n",
      "epoch:3 step:3144 [D loss: 0.549312, acc: 72.66%] [G loss: 3.345033]\n",
      "epoch:3 step:3145 [D loss: 0.586136, acc: 69.53%] [G loss: 3.145006]\n",
      "epoch:3 step:3146 [D loss: 0.530861, acc: 69.53%] [G loss: 3.846007]\n",
      "epoch:3 step:3147 [D loss: 0.475105, acc: 73.44%] [G loss: 3.382302]\n",
      "epoch:3 step:3148 [D loss: 0.471172, acc: 78.91%] [G loss: 3.470220]\n",
      "epoch:3 step:3149 [D loss: 0.590394, acc: 71.88%] [G loss: 3.173649]\n",
      "epoch:3 step:3150 [D loss: 0.563301, acc: 69.53%] [G loss: 3.146281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3151 [D loss: 0.534080, acc: 70.31%] [G loss: 3.303240]\n",
      "epoch:3 step:3152 [D loss: 0.513389, acc: 74.22%] [G loss: 3.146724]\n",
      "epoch:3 step:3153 [D loss: 0.675526, acc: 67.97%] [G loss: 2.840768]\n",
      "epoch:3 step:3154 [D loss: 0.448053, acc: 82.81%] [G loss: 3.081259]\n",
      "epoch:3 step:3155 [D loss: 0.540506, acc: 78.12%] [G loss: 3.207544]\n",
      "epoch:3 step:3156 [D loss: 0.581540, acc: 68.75%] [G loss: 3.055800]\n",
      "epoch:3 step:3157 [D loss: 0.461262, acc: 79.69%] [G loss: 3.219757]\n",
      "epoch:3 step:3158 [D loss: 0.565220, acc: 70.31%] [G loss: 3.351091]\n",
      "epoch:3 step:3159 [D loss: 0.726731, acc: 59.38%] [G loss: 3.153306]\n",
      "epoch:3 step:3160 [D loss: 0.571647, acc: 66.41%] [G loss: 2.729546]\n",
      "epoch:3 step:3161 [D loss: 0.507500, acc: 77.34%] [G loss: 3.027977]\n",
      "epoch:3 step:3162 [D loss: 0.478221, acc: 82.03%] [G loss: 3.224143]\n",
      "epoch:3 step:3163 [D loss: 0.628293, acc: 60.94%] [G loss: 3.317425]\n",
      "epoch:3 step:3164 [D loss: 0.580608, acc: 71.09%] [G loss: 3.132936]\n",
      "epoch:3 step:3165 [D loss: 0.476379, acc: 80.47%] [G loss: 3.316471]\n",
      "epoch:3 step:3166 [D loss: 0.537312, acc: 74.22%] [G loss: 3.175229]\n",
      "epoch:3 step:3167 [D loss: 0.493069, acc: 76.56%] [G loss: 3.154920]\n",
      "epoch:3 step:3168 [D loss: 0.600658, acc: 67.19%] [G loss: 3.570369]\n",
      "epoch:3 step:3169 [D loss: 0.436373, acc: 77.34%] [G loss: 3.624514]\n",
      "epoch:3 step:3170 [D loss: 0.462407, acc: 81.25%] [G loss: 3.818938]\n",
      "epoch:3 step:3171 [D loss: 0.529935, acc: 72.66%] [G loss: 3.095763]\n",
      "epoch:3 step:3172 [D loss: 0.547535, acc: 71.88%] [G loss: 2.999342]\n",
      "epoch:3 step:3173 [D loss: 0.630834, acc: 68.75%] [G loss: 2.841279]\n",
      "epoch:3 step:3174 [D loss: 0.494829, acc: 72.66%] [G loss: 3.275669]\n",
      "epoch:3 step:3175 [D loss: 0.524903, acc: 73.44%] [G loss: 3.338329]\n",
      "epoch:3 step:3176 [D loss: 0.541472, acc: 71.88%] [G loss: 3.283094]\n",
      "epoch:3 step:3177 [D loss: 0.457175, acc: 78.91%] [G loss: 3.194948]\n",
      "epoch:3 step:3178 [D loss: 0.577763, acc: 68.75%] [G loss: 3.410548]\n",
      "epoch:3 step:3179 [D loss: 0.522430, acc: 76.56%] [G loss: 3.532898]\n",
      "epoch:3 step:3180 [D loss: 0.600387, acc: 69.53%] [G loss: 3.369976]\n",
      "epoch:3 step:3181 [D loss: 0.547998, acc: 70.31%] [G loss: 3.223928]\n",
      "epoch:3 step:3182 [D loss: 0.524252, acc: 66.41%] [G loss: 3.123677]\n",
      "epoch:3 step:3183 [D loss: 0.479991, acc: 74.22%] [G loss: 3.366768]\n",
      "epoch:3 step:3184 [D loss: 0.544123, acc: 74.22%] [G loss: 2.738523]\n",
      "epoch:3 step:3185 [D loss: 0.571566, acc: 70.31%] [G loss: 3.599219]\n",
      "epoch:3 step:3186 [D loss: 0.515668, acc: 70.31%] [G loss: 3.281565]\n",
      "epoch:3 step:3187 [D loss: 0.559984, acc: 69.53%] [G loss: 3.112966]\n",
      "epoch:3 step:3188 [D loss: 0.714520, acc: 55.47%] [G loss: 2.686554]\n",
      "epoch:3 step:3189 [D loss: 0.518737, acc: 72.66%] [G loss: 3.392063]\n",
      "epoch:3 step:3190 [D loss: 0.492139, acc: 74.22%] [G loss: 3.307004]\n",
      "epoch:3 step:3191 [D loss: 0.623615, acc: 65.62%] [G loss: 3.419183]\n",
      "epoch:3 step:3192 [D loss: 0.451261, acc: 79.69%] [G loss: 4.221003]\n",
      "epoch:3 step:3193 [D loss: 0.614123, acc: 67.19%] [G loss: 3.396688]\n",
      "epoch:3 step:3194 [D loss: 0.639039, acc: 63.28%] [G loss: 2.926946]\n",
      "epoch:3 step:3195 [D loss: 0.601238, acc: 67.19%] [G loss: 2.858720]\n",
      "epoch:3 step:3196 [D loss: 0.424411, acc: 81.25%] [G loss: 2.855713]\n",
      "epoch:3 step:3197 [D loss: 0.561235, acc: 72.66%] [G loss: 2.909107]\n",
      "epoch:3 step:3198 [D loss: 0.556405, acc: 71.88%] [G loss: 2.969911]\n",
      "epoch:3 step:3199 [D loss: 0.590188, acc: 68.75%] [G loss: 3.043915]\n",
      "epoch:3 step:3200 [D loss: 0.594061, acc: 67.97%] [G loss: 3.130193]\n",
      "epoch:3 step:3201 [D loss: 0.533030, acc: 74.22%] [G loss: 3.144096]\n",
      "epoch:3 step:3202 [D loss: 0.568784, acc: 73.44%] [G loss: 3.290656]\n",
      "epoch:3 step:3203 [D loss: 0.529820, acc: 73.44%] [G loss: 3.378908]\n",
      "epoch:3 step:3204 [D loss: 0.571869, acc: 72.66%] [G loss: 3.311126]\n",
      "epoch:3 step:3205 [D loss: 0.553775, acc: 76.56%] [G loss: 3.118303]\n",
      "epoch:3 step:3206 [D loss: 0.519471, acc: 75.78%] [G loss: 3.283349]\n",
      "epoch:3 step:3207 [D loss: 0.655935, acc: 64.06%] [G loss: 2.637744]\n",
      "epoch:3 step:3208 [D loss: 0.401811, acc: 79.69%] [G loss: 3.331473]\n",
      "epoch:3 step:3209 [D loss: 0.460579, acc: 81.25%] [G loss: 3.807708]\n",
      "epoch:3 step:3210 [D loss: 0.500858, acc: 75.78%] [G loss: 3.356073]\n",
      "epoch:3 step:3211 [D loss: 0.534415, acc: 73.44%] [G loss: 3.199616]\n",
      "epoch:3 step:3212 [D loss: 0.535960, acc: 73.44%] [G loss: 2.814809]\n",
      "epoch:3 step:3213 [D loss: 0.465050, acc: 78.12%] [G loss: 3.541696]\n",
      "epoch:3 step:3214 [D loss: 0.534374, acc: 75.78%] [G loss: 3.388471]\n",
      "epoch:3 step:3215 [D loss: 0.593159, acc: 67.19%] [G loss: 3.150639]\n",
      "epoch:3 step:3216 [D loss: 0.529560, acc: 72.66%] [G loss: 3.590061]\n",
      "epoch:3 step:3217 [D loss: 0.517278, acc: 75.00%] [G loss: 3.435109]\n",
      "epoch:3 step:3218 [D loss: 0.495655, acc: 74.22%] [G loss: 3.446214]\n",
      "epoch:3 step:3219 [D loss: 0.557340, acc: 75.78%] [G loss: 3.608919]\n",
      "epoch:3 step:3220 [D loss: 0.480415, acc: 74.22%] [G loss: 3.216669]\n",
      "epoch:3 step:3221 [D loss: 0.542296, acc: 70.31%] [G loss: 3.019979]\n",
      "epoch:3 step:3222 [D loss: 0.551297, acc: 76.56%] [G loss: 3.032188]\n",
      "epoch:3 step:3223 [D loss: 0.624657, acc: 67.97%] [G loss: 2.905723]\n",
      "epoch:3 step:3224 [D loss: 0.439695, acc: 82.03%] [G loss: 3.322845]\n",
      "epoch:3 step:3225 [D loss: 0.617811, acc: 65.62%] [G loss: 3.508130]\n",
      "epoch:3 step:3226 [D loss: 0.421457, acc: 81.25%] [G loss: 3.167460]\n",
      "epoch:3 step:3227 [D loss: 0.541157, acc: 73.44%] [G loss: 3.369459]\n",
      "epoch:3 step:3228 [D loss: 0.577275, acc: 67.19%] [G loss: 2.902427]\n",
      "epoch:3 step:3229 [D loss: 0.490452, acc: 78.12%] [G loss: 3.582685]\n",
      "epoch:3 step:3230 [D loss: 0.534906, acc: 73.44%] [G loss: 3.401680]\n",
      "epoch:3 step:3231 [D loss: 0.571354, acc: 68.75%] [G loss: 3.135813]\n",
      "epoch:3 step:3232 [D loss: 0.528110, acc: 75.00%] [G loss: 3.067738]\n",
      "epoch:3 step:3233 [D loss: 0.623465, acc: 65.62%] [G loss: 3.563354]\n",
      "epoch:3 step:3234 [D loss: 0.512976, acc: 69.53%] [G loss: 3.321945]\n",
      "epoch:3 step:3235 [D loss: 0.527934, acc: 67.97%] [G loss: 3.107139]\n",
      "epoch:3 step:3236 [D loss: 0.521213, acc: 75.00%] [G loss: 3.054056]\n",
      "epoch:3 step:3237 [D loss: 0.584232, acc: 71.88%] [G loss: 3.145208]\n",
      "epoch:3 step:3238 [D loss: 0.533266, acc: 71.09%] [G loss: 3.403919]\n",
      "epoch:3 step:3239 [D loss: 0.507640, acc: 76.56%] [G loss: 3.501393]\n",
      "epoch:3 step:3240 [D loss: 0.457616, acc: 80.47%] [G loss: 3.828021]\n",
      "epoch:3 step:3241 [D loss: 0.509200, acc: 73.44%] [G loss: 3.639665]\n",
      "epoch:3 step:3242 [D loss: 0.501082, acc: 76.56%] [G loss: 3.813481]\n",
      "epoch:3 step:3243 [D loss: 0.617068, acc: 65.62%] [G loss: 3.201414]\n",
      "epoch:3 step:3244 [D loss: 0.522670, acc: 75.00%] [G loss: 3.108992]\n",
      "epoch:3 step:3245 [D loss: 0.529497, acc: 72.66%] [G loss: 3.136043]\n",
      "epoch:3 step:3246 [D loss: 0.561147, acc: 69.53%] [G loss: 3.203962]\n",
      "epoch:3 step:3247 [D loss: 0.550918, acc: 72.66%] [G loss: 3.089723]\n",
      "epoch:3 step:3248 [D loss: 0.530302, acc: 73.44%] [G loss: 2.924351]\n",
      "epoch:3 step:3249 [D loss: 0.552649, acc: 71.88%] [G loss: 3.091997]\n",
      "epoch:3 step:3250 [D loss: 0.539699, acc: 70.31%] [G loss: 3.216157]\n",
      "epoch:3 step:3251 [D loss: 0.621768, acc: 70.31%] [G loss: 3.170949]\n",
      "epoch:3 step:3252 [D loss: 0.527387, acc: 73.44%] [G loss: 2.678225]\n",
      "epoch:3 step:3253 [D loss: 0.547693, acc: 71.09%] [G loss: 3.372685]\n",
      "epoch:3 step:3254 [D loss: 0.474692, acc: 82.81%] [G loss: 3.051652]\n",
      "epoch:3 step:3255 [D loss: 0.582852, acc: 70.31%] [G loss: 2.948324]\n",
      "epoch:3 step:3256 [D loss: 0.498777, acc: 76.56%] [G loss: 3.329113]\n",
      "epoch:3 step:3257 [D loss: 0.539655, acc: 74.22%] [G loss: 3.006130]\n",
      "epoch:3 step:3258 [D loss: 0.571723, acc: 68.75%] [G loss: 3.172333]\n",
      "epoch:3 step:3259 [D loss: 0.578085, acc: 71.09%] [G loss: 3.443451]\n",
      "epoch:3 step:3260 [D loss: 0.586368, acc: 69.53%] [G loss: 3.378395]\n",
      "epoch:3 step:3261 [D loss: 0.499248, acc: 76.56%] [G loss: 3.034668]\n",
      "epoch:3 step:3262 [D loss: 0.552073, acc: 73.44%] [G loss: 3.355025]\n",
      "epoch:3 step:3263 [D loss: 0.463717, acc: 75.00%] [G loss: 3.078778]\n",
      "epoch:3 step:3264 [D loss: 0.649473, acc: 64.06%] [G loss: 2.775916]\n",
      "epoch:3 step:3265 [D loss: 0.619703, acc: 71.09%] [G loss: 2.808484]\n",
      "epoch:3 step:3266 [D loss: 0.555700, acc: 75.00%] [G loss: 3.169132]\n",
      "epoch:3 step:3267 [D loss: 0.569617, acc: 68.75%] [G loss: 2.898979]\n",
      "epoch:3 step:3268 [D loss: 0.611765, acc: 68.75%] [G loss: 3.478388]\n",
      "epoch:3 step:3269 [D loss: 0.582423, acc: 73.44%] [G loss: 2.688246]\n",
      "epoch:3 step:3270 [D loss: 0.601518, acc: 67.19%] [G loss: 3.256445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3271 [D loss: 0.503534, acc: 76.56%] [G loss: 3.109310]\n",
      "epoch:3 step:3272 [D loss: 0.578647, acc: 67.19%] [G loss: 2.910251]\n",
      "epoch:3 step:3273 [D loss: 0.522712, acc: 77.34%] [G loss: 3.043887]\n",
      "epoch:3 step:3274 [D loss: 0.596461, acc: 64.84%] [G loss: 3.005233]\n",
      "epoch:3 step:3275 [D loss: 0.561308, acc: 71.88%] [G loss: 2.590618]\n",
      "epoch:3 step:3276 [D loss: 0.643467, acc: 64.84%] [G loss: 2.669665]\n",
      "epoch:3 step:3277 [D loss: 0.529192, acc: 75.78%] [G loss: 3.284008]\n",
      "epoch:3 step:3278 [D loss: 0.590597, acc: 67.19%] [G loss: 3.065996]\n",
      "epoch:3 step:3279 [D loss: 0.593155, acc: 67.19%] [G loss: 2.842157]\n",
      "epoch:3 step:3280 [D loss: 0.550335, acc: 67.19%] [G loss: 2.629020]\n",
      "epoch:3 step:3281 [D loss: 0.534611, acc: 75.78%] [G loss: 3.237747]\n",
      "epoch:3 step:3282 [D loss: 0.483266, acc: 75.78%] [G loss: 3.626441]\n",
      "epoch:3 step:3283 [D loss: 0.452400, acc: 80.47%] [G loss: 3.178735]\n",
      "epoch:3 step:3284 [D loss: 0.590287, acc: 64.06%] [G loss: 2.886387]\n",
      "epoch:3 step:3285 [D loss: 0.561043, acc: 67.19%] [G loss: 3.198353]\n",
      "epoch:3 step:3286 [D loss: 0.508923, acc: 73.44%] [G loss: 3.404918]\n",
      "epoch:3 step:3287 [D loss: 0.574765, acc: 68.75%] [G loss: 2.910778]\n",
      "epoch:3 step:3288 [D loss: 0.687747, acc: 61.72%] [G loss: 2.930146]\n",
      "epoch:3 step:3289 [D loss: 0.604226, acc: 69.53%] [G loss: 2.712353]\n",
      "epoch:3 step:3290 [D loss: 0.628376, acc: 62.50%] [G loss: 3.264942]\n",
      "epoch:3 step:3291 [D loss: 0.577973, acc: 69.53%] [G loss: 3.065891]\n",
      "epoch:3 step:3292 [D loss: 0.547406, acc: 68.75%] [G loss: 3.079336]\n",
      "epoch:3 step:3293 [D loss: 0.608700, acc: 67.97%] [G loss: 2.697670]\n",
      "epoch:3 step:3294 [D loss: 0.576833, acc: 69.53%] [G loss: 2.895122]\n",
      "epoch:3 step:3295 [D loss: 0.560601, acc: 70.31%] [G loss: 3.075570]\n",
      "epoch:3 step:3296 [D loss: 0.540971, acc: 72.66%] [G loss: 3.268481]\n",
      "epoch:3 step:3297 [D loss: 0.576312, acc: 71.09%] [G loss: 3.037994]\n",
      "epoch:3 step:3298 [D loss: 0.531689, acc: 72.66%] [G loss: 3.136884]\n",
      "epoch:3 step:3299 [D loss: 0.583211, acc: 74.22%] [G loss: 2.989465]\n",
      "epoch:3 step:3300 [D loss: 0.654135, acc: 65.62%] [G loss: 2.981935]\n",
      "epoch:3 step:3301 [D loss: 0.505822, acc: 75.78%] [G loss: 3.057000]\n",
      "epoch:3 step:3302 [D loss: 0.683016, acc: 65.62%] [G loss: 2.962020]\n",
      "epoch:3 step:3303 [D loss: 0.592871, acc: 68.75%] [G loss: 2.989221]\n",
      "epoch:3 step:3304 [D loss: 0.654015, acc: 60.94%] [G loss: 2.599146]\n",
      "epoch:3 step:3305 [D loss: 0.563139, acc: 68.75%] [G loss: 3.287567]\n",
      "epoch:3 step:3306 [D loss: 0.626588, acc: 62.50%] [G loss: 3.236395]\n",
      "epoch:3 step:3307 [D loss: 0.619269, acc: 67.19%] [G loss: 2.808937]\n",
      "epoch:3 step:3308 [D loss: 0.459343, acc: 76.56%] [G loss: 3.335442]\n",
      "epoch:3 step:3309 [D loss: 0.449381, acc: 75.78%] [G loss: 3.449104]\n",
      "epoch:3 step:3310 [D loss: 0.375616, acc: 85.16%] [G loss: 3.717919]\n",
      "epoch:3 step:3311 [D loss: 0.665104, acc: 63.28%] [G loss: 2.892954]\n",
      "epoch:3 step:3312 [D loss: 0.634083, acc: 66.41%] [G loss: 2.705712]\n",
      "epoch:3 step:3313 [D loss: 0.601483, acc: 70.31%] [G loss: 2.748334]\n",
      "epoch:3 step:3314 [D loss: 0.420302, acc: 82.03%] [G loss: 3.661975]\n",
      "epoch:3 step:3315 [D loss: 0.539875, acc: 76.56%] [G loss: 3.447560]\n",
      "epoch:3 step:3316 [D loss: 0.624202, acc: 64.06%] [G loss: 3.023026]\n",
      "epoch:3 step:3317 [D loss: 0.589415, acc: 67.97%] [G loss: 3.148891]\n",
      "epoch:3 step:3318 [D loss: 0.483637, acc: 78.91%] [G loss: 3.288312]\n",
      "epoch:3 step:3319 [D loss: 0.520314, acc: 75.78%] [G loss: 3.124103]\n",
      "epoch:3 step:3320 [D loss: 0.621834, acc: 68.75%] [G loss: 3.251840]\n",
      "epoch:3 step:3321 [D loss: 0.583319, acc: 74.22%] [G loss: 3.011542]\n",
      "epoch:3 step:3322 [D loss: 0.703842, acc: 64.84%] [G loss: 2.949591]\n",
      "epoch:3 step:3323 [D loss: 0.570647, acc: 75.00%] [G loss: 3.069868]\n",
      "epoch:3 step:3324 [D loss: 0.519522, acc: 74.22%] [G loss: 2.969336]\n",
      "epoch:3 step:3325 [D loss: 0.546668, acc: 71.88%] [G loss: 3.437909]\n",
      "epoch:3 step:3326 [D loss: 0.546244, acc: 71.09%] [G loss: 3.300770]\n",
      "epoch:3 step:3327 [D loss: 0.485856, acc: 75.78%] [G loss: 2.923550]\n",
      "epoch:3 step:3328 [D loss: 0.631306, acc: 66.41%] [G loss: 2.981558]\n",
      "epoch:3 step:3329 [D loss: 0.636414, acc: 64.06%] [G loss: 3.296166]\n",
      "epoch:3 step:3330 [D loss: 0.570499, acc: 71.88%] [G loss: 3.217213]\n",
      "epoch:3 step:3331 [D loss: 0.549123, acc: 72.66%] [G loss: 3.078241]\n",
      "epoch:3 step:3332 [D loss: 0.536210, acc: 73.44%] [G loss: 3.016156]\n",
      "epoch:3 step:3333 [D loss: 0.539792, acc: 73.44%] [G loss: 3.035056]\n",
      "epoch:3 step:3334 [D loss: 0.567574, acc: 67.19%] [G loss: 3.388068]\n",
      "epoch:3 step:3335 [D loss: 0.563031, acc: 71.88%] [G loss: 3.034085]\n",
      "epoch:3 step:3336 [D loss: 0.662677, acc: 64.84%] [G loss: 2.710872]\n",
      "epoch:3 step:3337 [D loss: 0.636335, acc: 64.06%] [G loss: 3.279013]\n",
      "epoch:3 step:3338 [D loss: 0.559205, acc: 71.09%] [G loss: 2.670361]\n",
      "epoch:3 step:3339 [D loss: 0.613173, acc: 68.75%] [G loss: 2.730737]\n",
      "epoch:3 step:3340 [D loss: 0.681617, acc: 66.41%] [G loss: 2.529636]\n",
      "epoch:3 step:3341 [D loss: 0.573495, acc: 66.41%] [G loss: 3.005018]\n",
      "epoch:3 step:3342 [D loss: 0.566746, acc: 71.88%] [G loss: 3.037964]\n",
      "epoch:3 step:3343 [D loss: 0.592459, acc: 70.31%] [G loss: 3.070550]\n",
      "epoch:3 step:3344 [D loss: 0.611271, acc: 65.62%] [G loss: 3.027312]\n",
      "epoch:3 step:3345 [D loss: 0.479511, acc: 82.03%] [G loss: 2.938421]\n",
      "epoch:3 step:3346 [D loss: 0.620972, acc: 66.41%] [G loss: 2.857757]\n",
      "epoch:3 step:3347 [D loss: 0.474727, acc: 74.22%] [G loss: 3.143033]\n",
      "epoch:3 step:3348 [D loss: 0.546894, acc: 75.00%] [G loss: 2.655405]\n",
      "epoch:3 step:3349 [D loss: 0.562976, acc: 71.09%] [G loss: 2.832587]\n",
      "epoch:3 step:3350 [D loss: 0.621397, acc: 64.06%] [G loss: 2.566343]\n",
      "epoch:3 step:3351 [D loss: 0.550070, acc: 71.88%] [G loss: 3.222860]\n",
      "epoch:3 step:3352 [D loss: 0.563180, acc: 69.53%] [G loss: 2.872651]\n",
      "epoch:3 step:3353 [D loss: 0.554875, acc: 71.09%] [G loss: 3.225115]\n",
      "epoch:3 step:3354 [D loss: 0.574542, acc: 71.09%] [G loss: 2.917609]\n",
      "epoch:3 step:3355 [D loss: 0.674481, acc: 67.19%] [G loss: 3.197594]\n",
      "epoch:3 step:3356 [D loss: 0.648034, acc: 64.84%] [G loss: 2.860186]\n",
      "epoch:3 step:3357 [D loss: 0.461714, acc: 79.69%] [G loss: 3.527603]\n",
      "epoch:3 step:3358 [D loss: 0.486782, acc: 78.12%] [G loss: 3.494955]\n",
      "epoch:3 step:3359 [D loss: 0.562328, acc: 70.31%] [G loss: 2.966812]\n",
      "epoch:3 step:3360 [D loss: 0.472790, acc: 75.78%] [G loss: 3.360079]\n",
      "epoch:3 step:3361 [D loss: 0.616809, acc: 66.41%] [G loss: 2.949178]\n",
      "epoch:3 step:3362 [D loss: 0.599386, acc: 67.97%] [G loss: 2.980747]\n",
      "epoch:3 step:3363 [D loss: 0.510596, acc: 70.31%] [G loss: 2.793430]\n",
      "epoch:3 step:3364 [D loss: 0.553783, acc: 72.66%] [G loss: 3.061118]\n",
      "epoch:3 step:3365 [D loss: 0.500875, acc: 75.78%] [G loss: 3.286272]\n",
      "epoch:3 step:3366 [D loss: 0.548059, acc: 73.44%] [G loss: 3.083611]\n",
      "epoch:3 step:3367 [D loss: 0.494453, acc: 78.91%] [G loss: 3.080457]\n",
      "epoch:3 step:3368 [D loss: 0.500491, acc: 75.78%] [G loss: 3.412855]\n",
      "epoch:3 step:3369 [D loss: 0.506256, acc: 75.00%] [G loss: 3.210429]\n",
      "epoch:3 step:3370 [D loss: 0.510745, acc: 71.88%] [G loss: 3.202572]\n",
      "epoch:3 step:3371 [D loss: 0.619402, acc: 63.28%] [G loss: 2.867936]\n",
      "epoch:3 step:3372 [D loss: 0.550812, acc: 69.53%] [G loss: 2.909598]\n",
      "epoch:3 step:3373 [D loss: 0.635960, acc: 64.06%] [G loss: 2.684451]\n",
      "epoch:3 step:3374 [D loss: 0.578169, acc: 70.31%] [G loss: 2.821780]\n",
      "epoch:3 step:3375 [D loss: 0.680827, acc: 64.06%] [G loss: 2.851788]\n",
      "epoch:3 step:3376 [D loss: 0.623903, acc: 66.41%] [G loss: 2.848468]\n",
      "epoch:3 step:3377 [D loss: 0.544171, acc: 69.53%] [G loss: 2.624434]\n",
      "epoch:3 step:3378 [D loss: 0.516918, acc: 75.00%] [G loss: 3.404961]\n",
      "epoch:3 step:3379 [D loss: 0.513761, acc: 72.66%] [G loss: 2.853026]\n",
      "epoch:3 step:3380 [D loss: 0.575321, acc: 71.09%] [G loss: 2.897381]\n",
      "epoch:3 step:3381 [D loss: 0.470812, acc: 76.56%] [G loss: 3.357761]\n",
      "epoch:3 step:3382 [D loss: 0.514674, acc: 71.88%] [G loss: 2.887714]\n",
      "epoch:3 step:3383 [D loss: 0.551978, acc: 70.31%] [G loss: 2.838431]\n",
      "epoch:3 step:3384 [D loss: 0.555845, acc: 76.56%] [G loss: 2.688453]\n",
      "epoch:3 step:3385 [D loss: 0.615660, acc: 72.66%] [G loss: 3.035354]\n",
      "epoch:3 step:3386 [D loss: 0.528218, acc: 75.00%] [G loss: 3.148222]\n",
      "epoch:3 step:3387 [D loss: 0.581890, acc: 67.97%] [G loss: 2.670000]\n",
      "epoch:3 step:3388 [D loss: 0.613191, acc: 67.97%] [G loss: 3.198219]\n",
      "epoch:3 step:3389 [D loss: 0.626768, acc: 64.06%] [G loss: 2.834101]\n",
      "epoch:3 step:3390 [D loss: 0.626533, acc: 67.97%] [G loss: 2.814284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3391 [D loss: 0.641440, acc: 67.97%] [G loss: 2.469271]\n",
      "epoch:3 step:3392 [D loss: 0.593945, acc: 70.31%] [G loss: 2.698418]\n",
      "epoch:3 step:3393 [D loss: 0.468906, acc: 78.91%] [G loss: 3.248136]\n",
      "epoch:3 step:3394 [D loss: 0.562883, acc: 75.78%] [G loss: 2.910369]\n",
      "epoch:3 step:3395 [D loss: 0.617736, acc: 71.88%] [G loss: 2.709372]\n",
      "epoch:3 step:3396 [D loss: 0.620733, acc: 65.62%] [G loss: 2.706522]\n",
      "epoch:3 step:3397 [D loss: 0.561269, acc: 72.66%] [G loss: 2.716866]\n",
      "epoch:3 step:3398 [D loss: 0.576808, acc: 67.97%] [G loss: 2.990227]\n",
      "epoch:3 step:3399 [D loss: 0.607050, acc: 69.53%] [G loss: 3.055031]\n",
      "epoch:3 step:3400 [D loss: 0.532726, acc: 75.78%] [G loss: 3.313775]\n",
      "epoch:3 step:3401 [D loss: 0.663514, acc: 62.50%] [G loss: 2.857693]\n",
      "epoch:3 step:3402 [D loss: 0.665600, acc: 60.16%] [G loss: 2.839776]\n",
      "epoch:3 step:3403 [D loss: 0.536452, acc: 75.00%] [G loss: 3.466398]\n",
      "epoch:3 step:3404 [D loss: 0.633601, acc: 64.84%] [G loss: 2.622146]\n",
      "epoch:3 step:3405 [D loss: 0.525957, acc: 72.66%] [G loss: 2.830558]\n",
      "epoch:3 step:3406 [D loss: 0.566853, acc: 71.09%] [G loss: 3.003080]\n",
      "epoch:3 step:3407 [D loss: 0.527569, acc: 74.22%] [G loss: 2.997590]\n",
      "epoch:3 step:3408 [D loss: 0.571816, acc: 70.31%] [G loss: 3.102567]\n",
      "epoch:3 step:3409 [D loss: 0.449253, acc: 78.12%] [G loss: 3.002387]\n",
      "epoch:3 step:3410 [D loss: 0.643372, acc: 60.94%] [G loss: 2.940674]\n",
      "epoch:3 step:3411 [D loss: 0.651762, acc: 66.41%] [G loss: 2.506264]\n",
      "epoch:3 step:3412 [D loss: 0.576014, acc: 71.09%] [G loss: 3.036583]\n",
      "epoch:3 step:3413 [D loss: 0.599895, acc: 72.66%] [G loss: 3.331114]\n",
      "epoch:3 step:3414 [D loss: 0.466532, acc: 80.47%] [G loss: 3.685546]\n",
      "epoch:3 step:3415 [D loss: 0.557380, acc: 69.53%] [G loss: 2.749682]\n",
      "epoch:3 step:3416 [D loss: 0.541661, acc: 68.75%] [G loss: 2.861965]\n",
      "epoch:3 step:3417 [D loss: 0.587847, acc: 69.53%] [G loss: 2.837970]\n",
      "epoch:3 step:3418 [D loss: 0.605544, acc: 68.75%] [G loss: 2.784277]\n",
      "epoch:3 step:3419 [D loss: 0.470356, acc: 79.69%] [G loss: 3.075229]\n",
      "epoch:3 step:3420 [D loss: 0.511598, acc: 70.31%] [G loss: 3.417341]\n",
      "epoch:3 step:3421 [D loss: 0.513898, acc: 73.44%] [G loss: 3.106210]\n",
      "epoch:3 step:3422 [D loss: 0.472241, acc: 78.91%] [G loss: 3.605434]\n",
      "epoch:3 step:3423 [D loss: 0.482813, acc: 78.91%] [G loss: 3.574450]\n",
      "epoch:3 step:3424 [D loss: 0.547773, acc: 76.56%] [G loss: 3.254966]\n",
      "epoch:3 step:3425 [D loss: 0.476825, acc: 77.34%] [G loss: 3.460234]\n",
      "epoch:3 step:3426 [D loss: 0.651746, acc: 62.50%] [G loss: 2.834812]\n",
      "epoch:3 step:3427 [D loss: 0.545412, acc: 74.22%] [G loss: 3.064711]\n",
      "epoch:3 step:3428 [D loss: 0.633807, acc: 67.19%] [G loss: 3.009067]\n",
      "epoch:3 step:3429 [D loss: 0.570278, acc: 68.75%] [G loss: 3.181961]\n",
      "epoch:3 step:3430 [D loss: 0.534465, acc: 72.66%] [G loss: 3.390140]\n",
      "epoch:3 step:3431 [D loss: 0.646702, acc: 66.41%] [G loss: 3.132761]\n",
      "epoch:3 step:3432 [D loss: 0.485326, acc: 77.34%] [G loss: 3.193407]\n",
      "epoch:3 step:3433 [D loss: 0.639823, acc: 65.62%] [G loss: 2.655197]\n",
      "epoch:3 step:3434 [D loss: 0.602430, acc: 65.62%] [G loss: 2.750434]\n",
      "epoch:3 step:3435 [D loss: 0.518572, acc: 75.00%] [G loss: 3.052061]\n",
      "epoch:3 step:3436 [D loss: 0.521299, acc: 67.97%] [G loss: 3.027875]\n",
      "epoch:3 step:3437 [D loss: 0.570058, acc: 68.75%] [G loss: 2.906837]\n",
      "epoch:3 step:3438 [D loss: 0.612905, acc: 70.31%] [G loss: 3.144354]\n",
      "epoch:3 step:3439 [D loss: 0.567425, acc: 67.97%] [G loss: 2.524797]\n",
      "epoch:3 step:3440 [D loss: 0.545125, acc: 71.88%] [G loss: 2.845686]\n",
      "epoch:3 step:3441 [D loss: 0.491958, acc: 74.22%] [G loss: 3.001258]\n",
      "epoch:3 step:3442 [D loss: 0.620254, acc: 67.97%] [G loss: 2.841655]\n",
      "epoch:3 step:3443 [D loss: 0.525869, acc: 78.91%] [G loss: 3.096407]\n",
      "epoch:3 step:3444 [D loss: 0.512927, acc: 75.00%] [G loss: 3.329035]\n",
      "epoch:3 step:3445 [D loss: 0.537709, acc: 71.09%] [G loss: 3.146426]\n",
      "epoch:3 step:3446 [D loss: 0.474706, acc: 78.12%] [G loss: 3.105245]\n",
      "epoch:3 step:3447 [D loss: 0.534450, acc: 76.56%] [G loss: 3.333001]\n",
      "epoch:3 step:3448 [D loss: 0.554753, acc: 73.44%] [G loss: 3.028098]\n",
      "epoch:3 step:3449 [D loss: 0.478780, acc: 75.00%] [G loss: 3.168293]\n",
      "epoch:3 step:3450 [D loss: 0.497911, acc: 77.34%] [G loss: 3.655902]\n",
      "epoch:3 step:3451 [D loss: 0.489119, acc: 76.56%] [G loss: 3.346022]\n",
      "epoch:3 step:3452 [D loss: 0.554951, acc: 71.88%] [G loss: 3.349203]\n",
      "epoch:3 step:3453 [D loss: 0.475716, acc: 78.12%] [G loss: 3.195223]\n",
      "epoch:3 step:3454 [D loss: 0.612127, acc: 66.41%] [G loss: 3.142491]\n",
      "epoch:3 step:3455 [D loss: 0.518076, acc: 74.22%] [G loss: 3.176887]\n",
      "epoch:3 step:3456 [D loss: 0.604549, acc: 64.84%] [G loss: 2.908501]\n",
      "epoch:3 step:3457 [D loss: 0.553182, acc: 71.88%] [G loss: 3.138124]\n",
      "epoch:3 step:3458 [D loss: 0.514760, acc: 77.34%] [G loss: 3.357052]\n",
      "epoch:3 step:3459 [D loss: 0.482469, acc: 74.22%] [G loss: 3.234984]\n",
      "epoch:3 step:3460 [D loss: 0.554348, acc: 71.88%] [G loss: 3.658842]\n",
      "epoch:3 step:3461 [D loss: 0.522887, acc: 71.09%] [G loss: 3.171355]\n",
      "epoch:3 step:3462 [D loss: 0.539952, acc: 71.09%] [G loss: 2.891982]\n",
      "epoch:3 step:3463 [D loss: 0.650284, acc: 64.84%] [G loss: 2.806739]\n",
      "epoch:3 step:3464 [D loss: 0.503363, acc: 72.66%] [G loss: 3.160631]\n",
      "epoch:3 step:3465 [D loss: 0.533767, acc: 69.53%] [G loss: 3.460329]\n",
      "epoch:3 step:3466 [D loss: 0.478717, acc: 75.00%] [G loss: 2.969328]\n",
      "epoch:3 step:3467 [D loss: 0.603287, acc: 69.53%] [G loss: 3.156300]\n",
      "epoch:3 step:3468 [D loss: 0.699050, acc: 60.94%] [G loss: 2.900659]\n",
      "epoch:3 step:3469 [D loss: 0.603981, acc: 67.97%] [G loss: 2.889383]\n",
      "epoch:3 step:3470 [D loss: 0.631298, acc: 68.75%] [G loss: 3.084267]\n",
      "epoch:3 step:3471 [D loss: 0.557137, acc: 72.66%] [G loss: 3.270221]\n",
      "epoch:3 step:3472 [D loss: 0.515750, acc: 78.91%] [G loss: 2.982985]\n",
      "epoch:3 step:3473 [D loss: 0.635239, acc: 64.06%] [G loss: 2.973032]\n",
      "epoch:3 step:3474 [D loss: 0.576462, acc: 66.41%] [G loss: 3.033028]\n",
      "epoch:3 step:3475 [D loss: 0.599859, acc: 68.75%] [G loss: 3.179363]\n",
      "epoch:3 step:3476 [D loss: 0.556251, acc: 72.66%] [G loss: 3.154330]\n",
      "epoch:3 step:3477 [D loss: 0.535304, acc: 74.22%] [G loss: 3.033230]\n",
      "epoch:3 step:3478 [D loss: 0.574451, acc: 69.53%] [G loss: 2.571103]\n",
      "epoch:3 step:3479 [D loss: 0.619114, acc: 72.66%] [G loss: 2.746713]\n",
      "epoch:3 step:3480 [D loss: 0.488254, acc: 82.03%] [G loss: 2.952642]\n",
      "epoch:3 step:3481 [D loss: 0.599884, acc: 70.31%] [G loss: 3.184175]\n",
      "epoch:3 step:3482 [D loss: 0.605686, acc: 62.50%] [G loss: 2.684772]\n",
      "epoch:3 step:3483 [D loss: 0.695495, acc: 56.25%] [G loss: 2.806215]\n",
      "epoch:3 step:3484 [D loss: 0.589881, acc: 65.62%] [G loss: 2.914443]\n",
      "epoch:3 step:3485 [D loss: 0.641625, acc: 59.38%] [G loss: 2.804942]\n",
      "epoch:3 step:3486 [D loss: 0.522561, acc: 73.44%] [G loss: 2.912144]\n",
      "epoch:3 step:3487 [D loss: 0.537435, acc: 73.44%] [G loss: 3.147933]\n",
      "epoch:3 step:3488 [D loss: 0.522547, acc: 75.78%] [G loss: 3.782385]\n",
      "epoch:3 step:3489 [D loss: 0.524727, acc: 76.56%] [G loss: 3.229225]\n",
      "epoch:3 step:3490 [D loss: 0.501775, acc: 75.00%] [G loss: 3.196692]\n",
      "epoch:3 step:3491 [D loss: 0.508742, acc: 76.56%] [G loss: 3.133549]\n",
      "epoch:3 step:3492 [D loss: 0.530245, acc: 69.53%] [G loss: 3.362103]\n",
      "epoch:3 step:3493 [D loss: 0.571850, acc: 68.75%] [G loss: 2.845635]\n",
      "epoch:3 step:3494 [D loss: 0.614401, acc: 71.09%] [G loss: 2.867449]\n",
      "epoch:3 step:3495 [D loss: 0.515821, acc: 74.22%] [G loss: 3.007693]\n",
      "epoch:3 step:3496 [D loss: 0.560843, acc: 74.22%] [G loss: 3.174353]\n",
      "epoch:3 step:3497 [D loss: 0.512591, acc: 73.44%] [G loss: 2.846672]\n",
      "epoch:3 step:3498 [D loss: 0.476402, acc: 76.56%] [G loss: 3.219541]\n",
      "epoch:3 step:3499 [D loss: 0.708951, acc: 63.28%] [G loss: 3.175170]\n",
      "epoch:3 step:3500 [D loss: 0.485742, acc: 78.12%] [G loss: 2.858764]\n",
      "epoch:3 step:3501 [D loss: 0.542421, acc: 74.22%] [G loss: 3.289245]\n",
      "epoch:3 step:3502 [D loss: 0.577156, acc: 67.19%] [G loss: 2.881854]\n",
      "epoch:3 step:3503 [D loss: 0.474013, acc: 77.34%] [G loss: 3.218426]\n",
      "epoch:3 step:3504 [D loss: 0.641890, acc: 66.41%] [G loss: 2.613146]\n",
      "epoch:3 step:3505 [D loss: 0.496590, acc: 73.44%] [G loss: 3.143918]\n",
      "epoch:3 step:3506 [D loss: 0.589715, acc: 75.78%] [G loss: 3.155123]\n",
      "epoch:3 step:3507 [D loss: 0.565986, acc: 71.09%] [G loss: 2.921576]\n",
      "epoch:3 step:3508 [D loss: 0.570096, acc: 72.66%] [G loss: 3.161079]\n",
      "epoch:3 step:3509 [D loss: 0.616585, acc: 66.41%] [G loss: 2.852960]\n",
      "epoch:3 step:3510 [D loss: 0.543955, acc: 67.97%] [G loss: 3.074046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3511 [D loss: 0.586875, acc: 70.31%] [G loss: 2.946223]\n",
      "epoch:3 step:3512 [D loss: 0.648550, acc: 64.06%] [G loss: 3.047224]\n",
      "epoch:3 step:3513 [D loss: 0.524160, acc: 73.44%] [G loss: 2.896274]\n",
      "epoch:3 step:3514 [D loss: 0.606292, acc: 72.66%] [G loss: 2.797430]\n",
      "epoch:3 step:3515 [D loss: 0.553136, acc: 69.53%] [G loss: 2.919944]\n",
      "epoch:3 step:3516 [D loss: 0.580841, acc: 70.31%] [G loss: 2.773136]\n",
      "epoch:3 step:3517 [D loss: 0.581698, acc: 67.19%] [G loss: 2.950223]\n",
      "epoch:3 step:3518 [D loss: 0.464474, acc: 78.12%] [G loss: 3.464578]\n",
      "epoch:3 step:3519 [D loss: 0.625670, acc: 67.97%] [G loss: 3.040107]\n",
      "epoch:3 step:3520 [D loss: 0.562353, acc: 72.66%] [G loss: 3.198416]\n",
      "epoch:3 step:3521 [D loss: 0.680748, acc: 62.50%] [G loss: 2.940554]\n",
      "epoch:3 step:3522 [D loss: 0.609425, acc: 64.06%] [G loss: 2.578710]\n",
      "epoch:3 step:3523 [D loss: 0.613130, acc: 67.97%] [G loss: 2.809750]\n",
      "epoch:3 step:3524 [D loss: 0.567554, acc: 71.88%] [G loss: 3.087683]\n",
      "epoch:3 step:3525 [D loss: 0.545187, acc: 72.66%] [G loss: 3.235923]\n",
      "epoch:3 step:3526 [D loss: 0.523971, acc: 75.00%] [G loss: 2.874953]\n",
      "epoch:3 step:3527 [D loss: 0.576156, acc: 69.53%] [G loss: 3.058116]\n",
      "epoch:3 step:3528 [D loss: 0.573540, acc: 65.62%] [G loss: 2.435464]\n",
      "epoch:3 step:3529 [D loss: 0.454276, acc: 76.56%] [G loss: 2.907362]\n",
      "epoch:3 step:3530 [D loss: 0.587807, acc: 67.97%] [G loss: 3.220364]\n",
      "epoch:3 step:3531 [D loss: 0.627907, acc: 68.75%] [G loss: 3.210003]\n",
      "epoch:3 step:3532 [D loss: 0.582230, acc: 70.31%] [G loss: 2.892162]\n",
      "epoch:3 step:3533 [D loss: 0.634055, acc: 70.31%] [G loss: 3.485576]\n",
      "epoch:3 step:3534 [D loss: 0.580259, acc: 67.19%] [G loss: 2.977993]\n",
      "epoch:3 step:3535 [D loss: 0.569366, acc: 71.09%] [G loss: 3.343351]\n",
      "epoch:3 step:3536 [D loss: 0.566316, acc: 70.31%] [G loss: 3.100847]\n",
      "epoch:3 step:3537 [D loss: 0.565460, acc: 65.62%] [G loss: 2.799645]\n",
      "epoch:3 step:3538 [D loss: 0.543928, acc: 76.56%] [G loss: 3.019723]\n",
      "epoch:3 step:3539 [D loss: 0.540007, acc: 75.00%] [G loss: 3.047213]\n",
      "epoch:3 step:3540 [D loss: 0.592838, acc: 69.53%] [G loss: 2.808610]\n",
      "epoch:3 step:3541 [D loss: 0.520288, acc: 75.00%] [G loss: 2.926707]\n",
      "epoch:3 step:3542 [D loss: 0.537219, acc: 70.31%] [G loss: 2.791400]\n",
      "epoch:3 step:3543 [D loss: 0.494358, acc: 78.91%] [G loss: 3.554548]\n",
      "epoch:3 step:3544 [D loss: 0.601132, acc: 68.75%] [G loss: 2.971634]\n",
      "epoch:3 step:3545 [D loss: 0.480823, acc: 78.12%] [G loss: 3.052125]\n",
      "epoch:3 step:3546 [D loss: 0.565365, acc: 75.00%] [G loss: 2.444627]\n",
      "epoch:3 step:3547 [D loss: 0.458668, acc: 78.91%] [G loss: 3.365019]\n",
      "epoch:3 step:3548 [D loss: 0.499268, acc: 78.12%] [G loss: 2.933623]\n",
      "epoch:3 step:3549 [D loss: 0.647216, acc: 67.97%] [G loss: 2.786671]\n",
      "epoch:3 step:3550 [D loss: 0.616755, acc: 65.62%] [G loss: 2.803744]\n",
      "epoch:3 step:3551 [D loss: 0.597990, acc: 66.41%] [G loss: 3.011769]\n",
      "epoch:3 step:3552 [D loss: 0.495848, acc: 75.78%] [G loss: 2.796940]\n",
      "epoch:3 step:3553 [D loss: 0.630828, acc: 68.75%] [G loss: 2.746833]\n",
      "epoch:3 step:3554 [D loss: 0.516131, acc: 73.44%] [G loss: 3.453454]\n",
      "epoch:3 step:3555 [D loss: 0.603794, acc: 66.41%] [G loss: 2.793653]\n",
      "epoch:3 step:3556 [D loss: 0.493481, acc: 75.00%] [G loss: 3.186886]\n",
      "epoch:3 step:3557 [D loss: 0.491628, acc: 82.03%] [G loss: 3.576288]\n",
      "epoch:3 step:3558 [D loss: 0.457316, acc: 78.91%] [G loss: 3.594978]\n",
      "epoch:3 step:3559 [D loss: 0.556371, acc: 73.44%] [G loss: 2.721683]\n",
      "epoch:3 step:3560 [D loss: 0.599994, acc: 70.31%] [G loss: 2.711326]\n",
      "epoch:3 step:3561 [D loss: 0.601336, acc: 68.75%] [G loss: 2.864067]\n",
      "epoch:3 step:3562 [D loss: 0.521927, acc: 75.78%] [G loss: 3.149961]\n",
      "epoch:3 step:3563 [D loss: 0.556253, acc: 73.44%] [G loss: 2.889423]\n",
      "epoch:3 step:3564 [D loss: 0.536830, acc: 66.41%] [G loss: 2.818200]\n",
      "epoch:3 step:3565 [D loss: 0.467767, acc: 77.34%] [G loss: 2.892923]\n",
      "epoch:3 step:3566 [D loss: 0.558083, acc: 71.88%] [G loss: 3.198134]\n",
      "epoch:3 step:3567 [D loss: 0.581913, acc: 71.09%] [G loss: 3.230742]\n",
      "epoch:3 step:3568 [D loss: 0.519328, acc: 73.44%] [G loss: 3.169487]\n",
      "epoch:3 step:3569 [D loss: 0.496816, acc: 75.00%] [G loss: 3.207710]\n",
      "epoch:3 step:3570 [D loss: 0.701062, acc: 64.06%] [G loss: 2.734349]\n",
      "epoch:3 step:3571 [D loss: 0.613032, acc: 75.00%] [G loss: 2.636460]\n",
      "epoch:3 step:3572 [D loss: 0.531360, acc: 75.78%] [G loss: 2.992458]\n",
      "epoch:3 step:3573 [D loss: 0.589035, acc: 69.53%] [G loss: 2.877773]\n",
      "epoch:3 step:3574 [D loss: 0.568582, acc: 71.88%] [G loss: 2.940898]\n",
      "epoch:3 step:3575 [D loss: 0.577757, acc: 67.97%] [G loss: 2.931705]\n",
      "epoch:3 step:3576 [D loss: 0.688716, acc: 64.84%] [G loss: 2.495982]\n",
      "epoch:3 step:3577 [D loss: 0.713033, acc: 57.03%] [G loss: 2.315636]\n",
      "epoch:3 step:3578 [D loss: 0.608292, acc: 68.75%] [G loss: 2.841182]\n",
      "epoch:3 step:3579 [D loss: 0.663260, acc: 62.50%] [G loss: 2.687301]\n",
      "epoch:3 step:3580 [D loss: 0.630458, acc: 64.84%] [G loss: 3.169298]\n",
      "epoch:3 step:3581 [D loss: 0.538063, acc: 72.66%] [G loss: 2.726684]\n",
      "epoch:3 step:3582 [D loss: 0.537127, acc: 72.66%] [G loss: 2.921145]\n",
      "epoch:3 step:3583 [D loss: 0.582103, acc: 64.06%] [G loss: 3.055600]\n",
      "epoch:3 step:3584 [D loss: 0.575942, acc: 69.53%] [G loss: 2.474032]\n",
      "epoch:3 step:3585 [D loss: 0.598974, acc: 67.97%] [G loss: 3.022225]\n",
      "epoch:3 step:3586 [D loss: 0.548915, acc: 67.19%] [G loss: 3.202405]\n",
      "epoch:3 step:3587 [D loss: 0.520325, acc: 70.31%] [G loss: 2.705086]\n",
      "epoch:3 step:3588 [D loss: 0.559480, acc: 75.78%] [G loss: 2.774972]\n",
      "epoch:3 step:3589 [D loss: 0.582608, acc: 72.66%] [G loss: 2.951653]\n",
      "epoch:3 step:3590 [D loss: 0.638789, acc: 67.19%] [G loss: 2.821303]\n",
      "epoch:3 step:3591 [D loss: 0.527867, acc: 71.88%] [G loss: 3.091363]\n",
      "epoch:3 step:3592 [D loss: 0.582040, acc: 68.75%] [G loss: 2.800388]\n",
      "epoch:3 step:3593 [D loss: 0.629060, acc: 71.09%] [G loss: 2.719613]\n",
      "epoch:3 step:3594 [D loss: 0.581334, acc: 64.06%] [G loss: 2.847207]\n",
      "epoch:3 step:3595 [D loss: 0.623890, acc: 64.06%] [G loss: 2.943329]\n",
      "epoch:3 step:3596 [D loss: 0.587070, acc: 64.84%] [G loss: 2.545003]\n",
      "epoch:3 step:3597 [D loss: 0.561999, acc: 73.44%] [G loss: 2.664080]\n",
      "epoch:3 step:3598 [D loss: 0.572099, acc: 69.53%] [G loss: 3.003699]\n",
      "epoch:3 step:3599 [D loss: 0.616338, acc: 64.06%] [G loss: 2.274668]\n",
      "epoch:3 step:3600 [D loss: 0.558418, acc: 67.97%] [G loss: 2.729741]\n",
      "epoch:3 step:3601 [D loss: 0.571732, acc: 69.53%] [G loss: 2.307569]\n",
      "epoch:3 step:3602 [D loss: 0.530440, acc: 71.88%] [G loss: 2.971655]\n",
      "epoch:3 step:3603 [D loss: 0.559855, acc: 69.53%] [G loss: 3.378789]\n",
      "epoch:3 step:3604 [D loss: 0.580954, acc: 67.97%] [G loss: 2.829595]\n",
      "epoch:3 step:3605 [D loss: 0.680269, acc: 63.28%] [G loss: 2.791731]\n",
      "epoch:3 step:3606 [D loss: 0.496943, acc: 79.69%] [G loss: 3.116061]\n",
      "epoch:3 step:3607 [D loss: 0.581251, acc: 74.22%] [G loss: 2.991116]\n",
      "epoch:3 step:3608 [D loss: 0.548689, acc: 75.78%] [G loss: 2.846463]\n",
      "epoch:3 step:3609 [D loss: 0.527026, acc: 75.00%] [G loss: 3.008491]\n",
      "epoch:3 step:3610 [D loss: 0.539266, acc: 72.66%] [G loss: 2.851883]\n",
      "epoch:3 step:3611 [D loss: 0.653357, acc: 64.06%] [G loss: 2.590483]\n",
      "epoch:3 step:3612 [D loss: 0.522041, acc: 73.44%] [G loss: 3.303178]\n",
      "epoch:3 step:3613 [D loss: 0.526070, acc: 71.09%] [G loss: 2.823418]\n",
      "epoch:3 step:3614 [D loss: 0.523583, acc: 73.44%] [G loss: 2.999503]\n",
      "epoch:3 step:3615 [D loss: 0.540139, acc: 74.22%] [G loss: 2.998354]\n",
      "epoch:3 step:3616 [D loss: 0.534756, acc: 70.31%] [G loss: 3.190573]\n",
      "epoch:3 step:3617 [D loss: 0.543496, acc: 72.66%] [G loss: 3.469428]\n",
      "epoch:3 step:3618 [D loss: 0.618065, acc: 69.53%] [G loss: 2.929780]\n",
      "epoch:3 step:3619 [D loss: 0.571469, acc: 71.09%] [G loss: 2.813827]\n",
      "epoch:3 step:3620 [D loss: 0.617932, acc: 67.97%] [G loss: 2.863235]\n",
      "epoch:3 step:3621 [D loss: 0.583722, acc: 67.19%] [G loss: 2.758407]\n",
      "epoch:3 step:3622 [D loss: 0.667816, acc: 60.94%] [G loss: 2.788135]\n",
      "epoch:3 step:3623 [D loss: 0.600446, acc: 64.84%] [G loss: 2.837466]\n",
      "epoch:3 step:3624 [D loss: 0.564631, acc: 69.53%] [G loss: 2.895139]\n",
      "epoch:3 step:3625 [D loss: 0.637170, acc: 67.19%] [G loss: 2.924696]\n",
      "epoch:3 step:3626 [D loss: 0.563423, acc: 68.75%] [G loss: 2.897994]\n",
      "epoch:3 step:3627 [D loss: 0.546411, acc: 75.00%] [G loss: 3.123853]\n",
      "epoch:3 step:3628 [D loss: 0.651260, acc: 62.50%] [G loss: 2.816445]\n",
      "epoch:3 step:3629 [D loss: 0.635370, acc: 70.31%] [G loss: 2.806310]\n",
      "epoch:3 step:3630 [D loss: 0.592682, acc: 67.19%] [G loss: 2.846309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3631 [D loss: 0.601310, acc: 67.19%] [G loss: 2.643779]\n",
      "epoch:3 step:3632 [D loss: 0.628688, acc: 71.88%] [G loss: 2.708709]\n",
      "epoch:3 step:3633 [D loss: 0.543151, acc: 75.78%] [G loss: 3.007902]\n",
      "epoch:3 step:3634 [D loss: 0.570111, acc: 71.88%] [G loss: 2.814824]\n",
      "epoch:3 step:3635 [D loss: 0.710232, acc: 60.94%] [G loss: 2.711090]\n",
      "epoch:3 step:3636 [D loss: 0.517009, acc: 79.69%] [G loss: 2.816543]\n",
      "epoch:3 step:3637 [D loss: 0.627227, acc: 63.28%] [G loss: 2.882645]\n",
      "epoch:3 step:3638 [D loss: 0.685333, acc: 56.25%] [G loss: 2.560359]\n",
      "epoch:3 step:3639 [D loss: 0.609038, acc: 64.06%] [G loss: 2.355630]\n",
      "epoch:3 step:3640 [D loss: 0.591583, acc: 67.97%] [G loss: 2.502096]\n",
      "epoch:3 step:3641 [D loss: 0.551639, acc: 71.88%] [G loss: 2.674499]\n",
      "epoch:3 step:3642 [D loss: 0.538718, acc: 72.66%] [G loss: 3.451481]\n",
      "epoch:3 step:3643 [D loss: 0.543647, acc: 71.09%] [G loss: 3.213300]\n",
      "epoch:3 step:3644 [D loss: 0.594965, acc: 68.75%] [G loss: 2.885929]\n",
      "epoch:3 step:3645 [D loss: 0.556713, acc: 70.31%] [G loss: 2.793320]\n",
      "epoch:3 step:3646 [D loss: 0.582687, acc: 71.88%] [G loss: 2.628504]\n",
      "epoch:3 step:3647 [D loss: 0.544577, acc: 73.44%] [G loss: 2.814559]\n",
      "epoch:3 step:3648 [D loss: 0.494295, acc: 75.00%] [G loss: 2.738747]\n",
      "epoch:3 step:3649 [D loss: 0.559263, acc: 71.09%] [G loss: 2.879849]\n",
      "epoch:3 step:3650 [D loss: 0.530662, acc: 74.22%] [G loss: 2.613651]\n",
      "epoch:3 step:3651 [D loss: 0.664433, acc: 59.38%] [G loss: 2.621831]\n",
      "epoch:3 step:3652 [D loss: 0.526700, acc: 72.66%] [G loss: 2.987440]\n",
      "epoch:3 step:3653 [D loss: 0.568375, acc: 71.88%] [G loss: 2.843207]\n",
      "epoch:3 step:3654 [D loss: 0.648440, acc: 65.62%] [G loss: 2.732700]\n",
      "epoch:3 step:3655 [D loss: 0.565338, acc: 69.53%] [G loss: 2.772681]\n",
      "epoch:3 step:3656 [D loss: 0.585143, acc: 71.09%] [G loss: 2.990747]\n",
      "epoch:3 step:3657 [D loss: 0.649068, acc: 67.19%] [G loss: 2.642529]\n",
      "epoch:3 step:3658 [D loss: 0.540716, acc: 71.09%] [G loss: 3.113979]\n",
      "epoch:3 step:3659 [D loss: 0.498827, acc: 75.00%] [G loss: 3.164189]\n",
      "epoch:3 step:3660 [D loss: 0.543442, acc: 71.88%] [G loss: 2.653286]\n",
      "epoch:3 step:3661 [D loss: 0.700731, acc: 63.28%] [G loss: 2.572567]\n",
      "epoch:3 step:3662 [D loss: 0.659982, acc: 62.50%] [G loss: 2.567533]\n",
      "epoch:3 step:3663 [D loss: 0.590194, acc: 71.09%] [G loss: 2.740119]\n",
      "epoch:3 step:3664 [D loss: 0.546712, acc: 71.88%] [G loss: 3.024333]\n",
      "epoch:3 step:3665 [D loss: 0.521420, acc: 71.88%] [G loss: 2.917272]\n",
      "epoch:3 step:3666 [D loss: 0.561506, acc: 70.31%] [G loss: 2.870647]\n",
      "epoch:3 step:3667 [D loss: 0.577345, acc: 73.44%] [G loss: 3.142765]\n",
      "epoch:3 step:3668 [D loss: 0.534803, acc: 72.66%] [G loss: 3.038560]\n",
      "epoch:3 step:3669 [D loss: 0.690807, acc: 65.62%] [G loss: 2.749384]\n",
      "epoch:3 step:3670 [D loss: 0.567209, acc: 71.88%] [G loss: 2.966941]\n",
      "epoch:3 step:3671 [D loss: 0.593523, acc: 69.53%] [G loss: 3.025221]\n",
      "epoch:3 step:3672 [D loss: 0.667832, acc: 64.84%] [G loss: 2.745754]\n",
      "epoch:3 step:3673 [D loss: 0.555708, acc: 71.09%] [G loss: 2.958937]\n",
      "epoch:3 step:3674 [D loss: 0.483997, acc: 78.12%] [G loss: 2.846761]\n",
      "epoch:3 step:3675 [D loss: 0.492009, acc: 76.56%] [G loss: 2.782683]\n",
      "epoch:3 step:3676 [D loss: 0.641416, acc: 64.06%] [G loss: 2.529598]\n",
      "epoch:3 step:3677 [D loss: 0.587363, acc: 66.41%] [G loss: 2.973155]\n",
      "epoch:3 step:3678 [D loss: 0.598231, acc: 67.19%] [G loss: 2.678636]\n",
      "epoch:3 step:3679 [D loss: 0.659183, acc: 64.84%] [G loss: 2.569368]\n",
      "epoch:3 step:3680 [D loss: 0.577356, acc: 65.62%] [G loss: 2.976765]\n",
      "epoch:3 step:3681 [D loss: 0.557235, acc: 69.53%] [G loss: 2.925308]\n",
      "epoch:3 step:3682 [D loss: 0.520117, acc: 74.22%] [G loss: 2.994892]\n",
      "epoch:3 step:3683 [D loss: 0.511968, acc: 75.78%] [G loss: 2.880004]\n",
      "epoch:3 step:3684 [D loss: 0.578818, acc: 64.06%] [G loss: 2.716173]\n",
      "epoch:3 step:3685 [D loss: 0.628715, acc: 67.97%] [G loss: 2.957937]\n",
      "epoch:3 step:3686 [D loss: 0.581427, acc: 68.75%] [G loss: 2.887771]\n",
      "epoch:3 step:3687 [D loss: 0.564295, acc: 69.53%] [G loss: 2.829765]\n",
      "epoch:3 step:3688 [D loss: 0.580993, acc: 69.53%] [G loss: 3.115724]\n",
      "epoch:3 step:3689 [D loss: 0.588676, acc: 69.53%] [G loss: 2.960870]\n",
      "epoch:3 step:3690 [D loss: 0.600723, acc: 67.19%] [G loss: 2.527864]\n",
      "epoch:3 step:3691 [D loss: 0.568285, acc: 73.44%] [G loss: 2.603625]\n",
      "epoch:3 step:3692 [D loss: 0.600817, acc: 69.53%] [G loss: 2.446079]\n",
      "epoch:3 step:3693 [D loss: 0.633594, acc: 66.41%] [G loss: 2.667196]\n",
      "epoch:3 step:3694 [D loss: 0.533103, acc: 74.22%] [G loss: 2.730789]\n",
      "epoch:3 step:3695 [D loss: 0.516145, acc: 71.09%] [G loss: 2.713334]\n",
      "epoch:3 step:3696 [D loss: 0.501516, acc: 78.91%] [G loss: 3.249310]\n",
      "epoch:3 step:3697 [D loss: 0.491217, acc: 75.00%] [G loss: 3.572816]\n",
      "epoch:3 step:3698 [D loss: 0.513492, acc: 75.00%] [G loss: 3.555005]\n",
      "epoch:3 step:3699 [D loss: 0.547342, acc: 68.75%] [G loss: 3.300491]\n",
      "epoch:3 step:3700 [D loss: 0.532329, acc: 75.78%] [G loss: 3.037040]\n",
      "epoch:3 step:3701 [D loss: 0.480678, acc: 77.34%] [G loss: 3.511123]\n",
      "epoch:3 step:3702 [D loss: 0.584179, acc: 70.31%] [G loss: 3.036106]\n",
      "epoch:3 step:3703 [D loss: 0.661679, acc: 66.41%] [G loss: 2.825305]\n",
      "epoch:3 step:3704 [D loss: 0.534798, acc: 72.66%] [G loss: 3.142616]\n",
      "epoch:3 step:3705 [D loss: 0.496325, acc: 74.22%] [G loss: 2.901989]\n",
      "epoch:3 step:3706 [D loss: 0.550213, acc: 70.31%] [G loss: 3.252037]\n",
      "epoch:3 step:3707 [D loss: 0.655952, acc: 68.75%] [G loss: 2.923220]\n",
      "epoch:3 step:3708 [D loss: 0.609726, acc: 65.62%] [G loss: 2.906020]\n",
      "epoch:3 step:3709 [D loss: 0.496104, acc: 74.22%] [G loss: 3.141272]\n",
      "epoch:3 step:3710 [D loss: 0.570764, acc: 71.09%] [G loss: 3.094965]\n",
      "epoch:3 step:3711 [D loss: 0.582908, acc: 70.31%] [G loss: 3.432915]\n",
      "epoch:3 step:3712 [D loss: 0.616192, acc: 64.06%] [G loss: 2.960192]\n",
      "epoch:3 step:3713 [D loss: 0.609822, acc: 71.09%] [G loss: 2.854498]\n",
      "epoch:3 step:3714 [D loss: 0.583716, acc: 69.53%] [G loss: 2.785954]\n",
      "epoch:3 step:3715 [D loss: 0.557252, acc: 74.22%] [G loss: 3.207857]\n",
      "epoch:3 step:3716 [D loss: 0.516209, acc: 70.31%] [G loss: 2.871957]\n",
      "epoch:3 step:3717 [D loss: 0.611509, acc: 67.19%] [G loss: 3.141789]\n",
      "epoch:3 step:3718 [D loss: 0.548048, acc: 71.09%] [G loss: 2.782862]\n",
      "epoch:3 step:3719 [D loss: 0.500301, acc: 75.00%] [G loss: 2.903710]\n",
      "epoch:3 step:3720 [D loss: 0.566370, acc: 72.66%] [G loss: 3.443948]\n",
      "epoch:3 step:3721 [D loss: 0.503532, acc: 77.34%] [G loss: 3.385630]\n",
      "epoch:3 step:3722 [D loss: 0.508017, acc: 75.78%] [G loss: 3.274034]\n",
      "epoch:3 step:3723 [D loss: 0.497599, acc: 75.00%] [G loss: 3.664105]\n",
      "epoch:3 step:3724 [D loss: 0.606258, acc: 70.31%] [G loss: 2.899239]\n",
      "epoch:3 step:3725 [D loss: 0.522926, acc: 71.09%] [G loss: 3.300485]\n",
      "epoch:3 step:3726 [D loss: 0.654978, acc: 63.28%] [G loss: 2.576437]\n",
      "epoch:3 step:3727 [D loss: 0.589269, acc: 66.41%] [G loss: 2.993265]\n",
      "epoch:3 step:3728 [D loss: 0.577834, acc: 68.75%] [G loss: 2.850562]\n",
      "epoch:3 step:3729 [D loss: 0.563364, acc: 74.22%] [G loss: 3.286853]\n",
      "epoch:3 step:3730 [D loss: 0.666682, acc: 67.19%] [G loss: 3.304913]\n",
      "epoch:3 step:3731 [D loss: 0.735337, acc: 57.03%] [G loss: 3.179040]\n",
      "epoch:3 step:3732 [D loss: 0.535431, acc: 72.66%] [G loss: 3.249458]\n",
      "epoch:3 step:3733 [D loss: 0.607553, acc: 67.19%] [G loss: 2.789562]\n",
      "epoch:3 step:3734 [D loss: 0.438166, acc: 82.03%] [G loss: 3.517360]\n",
      "epoch:3 step:3735 [D loss: 0.448708, acc: 75.78%] [G loss: 3.391565]\n",
      "epoch:3 step:3736 [D loss: 0.464337, acc: 80.47%] [G loss: 3.264419]\n",
      "epoch:3 step:3737 [D loss: 0.590143, acc: 69.53%] [G loss: 3.464894]\n",
      "epoch:3 step:3738 [D loss: 0.659361, acc: 60.16%] [G loss: 3.098698]\n",
      "epoch:3 step:3739 [D loss: 0.819200, acc: 62.50%] [G loss: 2.974397]\n",
      "epoch:3 step:3740 [D loss: 0.577486, acc: 68.75%] [G loss: 3.446260]\n",
      "epoch:3 step:3741 [D loss: 0.470227, acc: 75.78%] [G loss: 2.947486]\n",
      "epoch:3 step:3742 [D loss: 0.526085, acc: 78.12%] [G loss: 3.192814]\n",
      "epoch:3 step:3743 [D loss: 0.656097, acc: 64.84%] [G loss: 2.870002]\n",
      "epoch:3 step:3744 [D loss: 0.514487, acc: 73.44%] [G loss: 3.104944]\n",
      "epoch:3 step:3745 [D loss: 0.549583, acc: 68.75%] [G loss: 3.271118]\n",
      "epoch:3 step:3746 [D loss: 0.501721, acc: 75.78%] [G loss: 3.318841]\n",
      "epoch:3 step:3747 [D loss: 0.556652, acc: 75.78%] [G loss: 3.481570]\n",
      "epoch:3 step:3748 [D loss: 0.541652, acc: 71.88%] [G loss: 3.375412]\n",
      "epoch:4 step:3749 [D loss: 0.615675, acc: 64.84%] [G loss: 2.925723]\n",
      "epoch:4 step:3750 [D loss: 0.482491, acc: 77.34%] [G loss: 3.025307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3751 [D loss: 0.544656, acc: 71.09%] [G loss: 2.811903]\n",
      "epoch:4 step:3752 [D loss: 0.644090, acc: 63.28%] [G loss: 3.073509]\n",
      "epoch:4 step:3753 [D loss: 0.533090, acc: 72.66%] [G loss: 3.075891]\n",
      "epoch:4 step:3754 [D loss: 0.524735, acc: 76.56%] [G loss: 2.982336]\n",
      "epoch:4 step:3755 [D loss: 0.548022, acc: 67.97%] [G loss: 2.926296]\n",
      "epoch:4 step:3756 [D loss: 0.530958, acc: 75.78%] [G loss: 3.327396]\n",
      "epoch:4 step:3757 [D loss: 0.572095, acc: 73.44%] [G loss: 3.227604]\n",
      "epoch:4 step:3758 [D loss: 0.544158, acc: 74.22%] [G loss: 3.012748]\n",
      "epoch:4 step:3759 [D loss: 0.599237, acc: 68.75%] [G loss: 2.984890]\n",
      "epoch:4 step:3760 [D loss: 0.573261, acc: 70.31%] [G loss: 2.846053]\n",
      "epoch:4 step:3761 [D loss: 0.624829, acc: 67.19%] [G loss: 2.773755]\n",
      "epoch:4 step:3762 [D loss: 0.628852, acc: 69.53%] [G loss: 2.882079]\n",
      "epoch:4 step:3763 [D loss: 0.464326, acc: 79.69%] [G loss: 2.831439]\n",
      "epoch:4 step:3764 [D loss: 0.431025, acc: 82.03%] [G loss: 3.148040]\n",
      "epoch:4 step:3765 [D loss: 0.630265, acc: 65.62%] [G loss: 2.777609]\n",
      "epoch:4 step:3766 [D loss: 0.606712, acc: 64.84%] [G loss: 2.544612]\n",
      "epoch:4 step:3767 [D loss: 0.576943, acc: 68.75%] [G loss: 2.877698]\n",
      "epoch:4 step:3768 [D loss: 0.835750, acc: 54.69%] [G loss: 2.547842]\n",
      "epoch:4 step:3769 [D loss: 0.607391, acc: 67.19%] [G loss: 2.635408]\n",
      "epoch:4 step:3770 [D loss: 0.674129, acc: 67.97%] [G loss: 2.285255]\n",
      "epoch:4 step:3771 [D loss: 0.608594, acc: 66.41%] [G loss: 2.675024]\n",
      "epoch:4 step:3772 [D loss: 0.579707, acc: 71.09%] [G loss: 2.922481]\n",
      "epoch:4 step:3773 [D loss: 0.488562, acc: 78.91%] [G loss: 2.868167]\n",
      "epoch:4 step:3774 [D loss: 0.562175, acc: 68.75%] [G loss: 2.893153]\n",
      "epoch:4 step:3775 [D loss: 0.571025, acc: 71.09%] [G loss: 2.928852]\n",
      "epoch:4 step:3776 [D loss: 0.581656, acc: 71.09%] [G loss: 2.906910]\n",
      "epoch:4 step:3777 [D loss: 0.498258, acc: 75.78%] [G loss: 3.097420]\n",
      "epoch:4 step:3778 [D loss: 0.571037, acc: 71.09%] [G loss: 2.647728]\n",
      "epoch:4 step:3779 [D loss: 0.667710, acc: 62.50%] [G loss: 2.688188]\n",
      "epoch:4 step:3780 [D loss: 0.610306, acc: 64.06%] [G loss: 2.810832]\n",
      "epoch:4 step:3781 [D loss: 0.570319, acc: 69.53%] [G loss: 2.999241]\n",
      "epoch:4 step:3782 [D loss: 0.563076, acc: 71.09%] [G loss: 3.303955]\n",
      "epoch:4 step:3783 [D loss: 0.507885, acc: 76.56%] [G loss: 2.936864]\n",
      "epoch:4 step:3784 [D loss: 0.553141, acc: 74.22%] [G loss: 3.249083]\n",
      "epoch:4 step:3785 [D loss: 0.549945, acc: 77.34%] [G loss: 3.697669]\n",
      "epoch:4 step:3786 [D loss: 0.611319, acc: 68.75%] [G loss: 2.951929]\n",
      "epoch:4 step:3787 [D loss: 0.507406, acc: 75.78%] [G loss: 3.031737]\n",
      "epoch:4 step:3788 [D loss: 0.559659, acc: 70.31%] [G loss: 2.963695]\n",
      "epoch:4 step:3789 [D loss: 0.544437, acc: 70.31%] [G loss: 2.982675]\n",
      "epoch:4 step:3790 [D loss: 0.521537, acc: 77.34%] [G loss: 2.785402]\n",
      "epoch:4 step:3791 [D loss: 0.458220, acc: 82.81%] [G loss: 3.459984]\n",
      "epoch:4 step:3792 [D loss: 0.626274, acc: 64.84%] [G loss: 2.758236]\n",
      "epoch:4 step:3793 [D loss: 0.656329, acc: 64.06%] [G loss: 2.658331]\n",
      "epoch:4 step:3794 [D loss: 0.525878, acc: 77.34%] [G loss: 3.211476]\n",
      "epoch:4 step:3795 [D loss: 0.659351, acc: 67.97%] [G loss: 3.378241]\n",
      "epoch:4 step:3796 [D loss: 0.505364, acc: 74.22%] [G loss: 2.833003]\n",
      "epoch:4 step:3797 [D loss: 0.612054, acc: 65.62%] [G loss: 2.833489]\n",
      "epoch:4 step:3798 [D loss: 0.531250, acc: 71.09%] [G loss: 2.878492]\n",
      "epoch:4 step:3799 [D loss: 0.552875, acc: 71.09%] [G loss: 2.906318]\n",
      "epoch:4 step:3800 [D loss: 0.589368, acc: 67.97%] [G loss: 2.737717]\n",
      "epoch:4 step:3801 [D loss: 0.569792, acc: 64.84%] [G loss: 3.063829]\n",
      "epoch:4 step:3802 [D loss: 0.542975, acc: 73.44%] [G loss: 2.884156]\n",
      "epoch:4 step:3803 [D loss: 0.494186, acc: 78.12%] [G loss: 3.414406]\n",
      "epoch:4 step:3804 [D loss: 0.560377, acc: 75.00%] [G loss: 2.943188]\n",
      "epoch:4 step:3805 [D loss: 0.585135, acc: 71.09%] [G loss: 2.950529]\n",
      "epoch:4 step:3806 [D loss: 0.615086, acc: 67.97%] [G loss: 2.869292]\n",
      "epoch:4 step:3807 [D loss: 0.573569, acc: 68.75%] [G loss: 2.798178]\n",
      "epoch:4 step:3808 [D loss: 0.499377, acc: 71.88%] [G loss: 3.043846]\n",
      "epoch:4 step:3809 [D loss: 0.590525, acc: 69.53%] [G loss: 3.347463]\n",
      "epoch:4 step:3810 [D loss: 0.614919, acc: 69.53%] [G loss: 2.779115]\n",
      "epoch:4 step:3811 [D loss: 0.624240, acc: 67.19%] [G loss: 2.638501]\n",
      "epoch:4 step:3812 [D loss: 0.568005, acc: 68.75%] [G loss: 2.757667]\n",
      "epoch:4 step:3813 [D loss: 0.554356, acc: 72.66%] [G loss: 3.076133]\n",
      "epoch:4 step:3814 [D loss: 0.562160, acc: 74.22%] [G loss: 2.739952]\n",
      "epoch:4 step:3815 [D loss: 0.603734, acc: 67.19%] [G loss: 2.939731]\n",
      "epoch:4 step:3816 [D loss: 0.540847, acc: 71.88%] [G loss: 3.031068]\n",
      "epoch:4 step:3817 [D loss: 0.550523, acc: 71.88%] [G loss: 2.978513]\n",
      "epoch:4 step:3818 [D loss: 0.540527, acc: 72.66%] [G loss: 2.632509]\n",
      "epoch:4 step:3819 [D loss: 0.576993, acc: 73.44%] [G loss: 2.987695]\n",
      "epoch:4 step:3820 [D loss: 0.546488, acc: 70.31%] [G loss: 3.059807]\n",
      "epoch:4 step:3821 [D loss: 0.565797, acc: 69.53%] [G loss: 2.920386]\n",
      "epoch:4 step:3822 [D loss: 0.566927, acc: 73.44%] [G loss: 3.007276]\n",
      "epoch:4 step:3823 [D loss: 0.474656, acc: 77.34%] [G loss: 3.597943]\n",
      "epoch:4 step:3824 [D loss: 0.461885, acc: 78.91%] [G loss: 3.138304]\n",
      "epoch:4 step:3825 [D loss: 0.502561, acc: 75.78%] [G loss: 3.771065]\n",
      "epoch:4 step:3826 [D loss: 0.655234, acc: 65.62%] [G loss: 2.910596]\n",
      "epoch:4 step:3827 [D loss: 0.596119, acc: 69.53%] [G loss: 2.856147]\n",
      "epoch:4 step:3828 [D loss: 0.620476, acc: 67.97%] [G loss: 2.570316]\n",
      "epoch:4 step:3829 [D loss: 0.548944, acc: 70.31%] [G loss: 2.746034]\n",
      "epoch:4 step:3830 [D loss: 0.553304, acc: 71.09%] [G loss: 3.163177]\n",
      "epoch:4 step:3831 [D loss: 0.573362, acc: 67.97%] [G loss: 3.143882]\n",
      "epoch:4 step:3832 [D loss: 0.544235, acc: 74.22%] [G loss: 2.631775]\n",
      "epoch:4 step:3833 [D loss: 0.594775, acc: 71.09%] [G loss: 2.703045]\n",
      "epoch:4 step:3834 [D loss: 0.525042, acc: 75.78%] [G loss: 2.759904]\n",
      "epoch:4 step:3835 [D loss: 0.529762, acc: 75.00%] [G loss: 3.066909]\n",
      "epoch:4 step:3836 [D loss: 0.634625, acc: 71.09%] [G loss: 2.884546]\n",
      "epoch:4 step:3837 [D loss: 0.601575, acc: 65.62%] [G loss: 3.059464]\n",
      "epoch:4 step:3838 [D loss: 0.601764, acc: 67.97%] [G loss: 2.621742]\n",
      "epoch:4 step:3839 [D loss: 0.576471, acc: 69.53%] [G loss: 2.579626]\n",
      "epoch:4 step:3840 [D loss: 0.594142, acc: 67.97%] [G loss: 3.009667]\n",
      "epoch:4 step:3841 [D loss: 0.526625, acc: 75.00%] [G loss: 3.008514]\n",
      "epoch:4 step:3842 [D loss: 0.629596, acc: 64.84%] [G loss: 2.871588]\n",
      "epoch:4 step:3843 [D loss: 0.568553, acc: 67.19%] [G loss: 2.860949]\n",
      "epoch:4 step:3844 [D loss: 0.494439, acc: 78.12%] [G loss: 3.003605]\n",
      "epoch:4 step:3845 [D loss: 0.563964, acc: 67.97%] [G loss: 2.825533]\n",
      "epoch:4 step:3846 [D loss: 0.497210, acc: 76.56%] [G loss: 2.712426]\n",
      "epoch:4 step:3847 [D loss: 0.535997, acc: 75.00%] [G loss: 2.819156]\n",
      "epoch:4 step:3848 [D loss: 0.511595, acc: 80.47%] [G loss: 3.028940]\n",
      "epoch:4 step:3849 [D loss: 0.584164, acc: 69.53%] [G loss: 2.836426]\n",
      "epoch:4 step:3850 [D loss: 0.575933, acc: 68.75%] [G loss: 2.725458]\n",
      "epoch:4 step:3851 [D loss: 0.427269, acc: 82.81%] [G loss: 3.407713]\n",
      "epoch:4 step:3852 [D loss: 0.477764, acc: 80.47%] [G loss: 3.107676]\n",
      "epoch:4 step:3853 [D loss: 0.528545, acc: 75.00%] [G loss: 3.196777]\n",
      "epoch:4 step:3854 [D loss: 0.540800, acc: 70.31%] [G loss: 3.042442]\n",
      "epoch:4 step:3855 [D loss: 0.692500, acc: 64.84%] [G loss: 2.738549]\n",
      "epoch:4 step:3856 [D loss: 0.565542, acc: 66.41%] [G loss: 2.660227]\n",
      "epoch:4 step:3857 [D loss: 0.591897, acc: 67.19%] [G loss: 2.895910]\n",
      "epoch:4 step:3858 [D loss: 0.493938, acc: 75.78%] [G loss: 2.926320]\n",
      "epoch:4 step:3859 [D loss: 0.595437, acc: 67.97%] [G loss: 3.187133]\n",
      "epoch:4 step:3860 [D loss: 0.501819, acc: 75.78%] [G loss: 2.879511]\n",
      "epoch:4 step:3861 [D loss: 0.639655, acc: 69.53%] [G loss: 2.851560]\n",
      "epoch:4 step:3862 [D loss: 0.548091, acc: 69.53%] [G loss: 2.891044]\n",
      "epoch:4 step:3863 [D loss: 0.558946, acc: 69.53%] [G loss: 3.155089]\n",
      "epoch:4 step:3864 [D loss: 0.551924, acc: 75.78%] [G loss: 2.898021]\n",
      "epoch:4 step:3865 [D loss: 0.612195, acc: 64.84%] [G loss: 2.605316]\n",
      "epoch:4 step:3866 [D loss: 0.502733, acc: 75.00%] [G loss: 3.260475]\n",
      "epoch:4 step:3867 [D loss: 0.540242, acc: 71.88%] [G loss: 3.321329]\n",
      "epoch:4 step:3868 [D loss: 0.637242, acc: 64.84%] [G loss: 3.175207]\n",
      "epoch:4 step:3869 [D loss: 0.641629, acc: 64.84%] [G loss: 2.910261]\n",
      "epoch:4 step:3870 [D loss: 0.491466, acc: 78.12%] [G loss: 2.984995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3871 [D loss: 0.544877, acc: 75.00%] [G loss: 2.870090]\n",
      "epoch:4 step:3872 [D loss: 0.637225, acc: 67.19%] [G loss: 2.844904]\n",
      "epoch:4 step:3873 [D loss: 0.627233, acc: 68.75%] [G loss: 2.629686]\n",
      "epoch:4 step:3874 [D loss: 0.529077, acc: 75.00%] [G loss: 2.710693]\n",
      "epoch:4 step:3875 [D loss: 0.619514, acc: 67.19%] [G loss: 2.769118]\n",
      "epoch:4 step:3876 [D loss: 0.565851, acc: 67.19%] [G loss: 2.986455]\n",
      "epoch:4 step:3877 [D loss: 0.640325, acc: 64.84%] [G loss: 2.721690]\n",
      "epoch:4 step:3878 [D loss: 0.538432, acc: 72.66%] [G loss: 2.695867]\n",
      "epoch:4 step:3879 [D loss: 0.446305, acc: 82.81%] [G loss: 2.882511]\n",
      "epoch:4 step:3880 [D loss: 0.561452, acc: 69.53%] [G loss: 2.853446]\n",
      "epoch:4 step:3881 [D loss: 0.665822, acc: 64.06%] [G loss: 2.645672]\n",
      "epoch:4 step:3882 [D loss: 0.627365, acc: 71.09%] [G loss: 3.095254]\n",
      "epoch:4 step:3883 [D loss: 0.527763, acc: 77.34%] [G loss: 2.768378]\n",
      "epoch:4 step:3884 [D loss: 0.515010, acc: 75.78%] [G loss: 2.654372]\n",
      "epoch:4 step:3885 [D loss: 0.608915, acc: 71.09%] [G loss: 2.534101]\n",
      "epoch:4 step:3886 [D loss: 0.719579, acc: 67.19%] [G loss: 2.833671]\n",
      "epoch:4 step:3887 [D loss: 0.591496, acc: 67.19%] [G loss: 2.833147]\n",
      "epoch:4 step:3888 [D loss: 0.643188, acc: 63.28%] [G loss: 2.454759]\n",
      "epoch:4 step:3889 [D loss: 0.562940, acc: 68.75%] [G loss: 3.250691]\n",
      "epoch:4 step:3890 [D loss: 0.665756, acc: 65.62%] [G loss: 2.949698]\n",
      "epoch:4 step:3891 [D loss: 0.544084, acc: 75.00%] [G loss: 2.944011]\n",
      "epoch:4 step:3892 [D loss: 0.524477, acc: 75.00%] [G loss: 2.967036]\n",
      "epoch:4 step:3893 [D loss: 0.552395, acc: 71.88%] [G loss: 3.151524]\n",
      "epoch:4 step:3894 [D loss: 0.548320, acc: 77.34%] [G loss: 2.992888]\n",
      "epoch:4 step:3895 [D loss: 0.620149, acc: 64.84%] [G loss: 2.824098]\n",
      "epoch:4 step:3896 [D loss: 0.603283, acc: 67.97%] [G loss: 2.664880]\n",
      "epoch:4 step:3897 [D loss: 0.549857, acc: 73.44%] [G loss: 2.845378]\n",
      "epoch:4 step:3898 [D loss: 0.563121, acc: 70.31%] [G loss: 3.289789]\n",
      "epoch:4 step:3899 [D loss: 0.559716, acc: 67.97%] [G loss: 3.129524]\n",
      "epoch:4 step:3900 [D loss: 0.560675, acc: 70.31%] [G loss: 3.410411]\n",
      "epoch:4 step:3901 [D loss: 0.542861, acc: 72.66%] [G loss: 2.816982]\n",
      "epoch:4 step:3902 [D loss: 0.536726, acc: 74.22%] [G loss: 2.846972]\n",
      "epoch:4 step:3903 [D loss: 0.559595, acc: 71.09%] [G loss: 2.884559]\n",
      "epoch:4 step:3904 [D loss: 0.555156, acc: 70.31%] [G loss: 2.939379]\n",
      "epoch:4 step:3905 [D loss: 0.558944, acc: 68.75%] [G loss: 2.615789]\n",
      "epoch:4 step:3906 [D loss: 0.558566, acc: 72.66%] [G loss: 3.006654]\n",
      "epoch:4 step:3907 [D loss: 0.562915, acc: 68.75%] [G loss: 3.134491]\n",
      "epoch:4 step:3908 [D loss: 0.684673, acc: 58.59%] [G loss: 2.229437]\n",
      "epoch:4 step:3909 [D loss: 0.616582, acc: 67.19%] [G loss: 2.642925]\n",
      "epoch:4 step:3910 [D loss: 0.515858, acc: 76.56%] [G loss: 2.711244]\n",
      "epoch:4 step:3911 [D loss: 0.486140, acc: 75.00%] [G loss: 2.698699]\n",
      "epoch:4 step:3912 [D loss: 0.519342, acc: 75.78%] [G loss: 2.624380]\n",
      "epoch:4 step:3913 [D loss: 0.528450, acc: 73.44%] [G loss: 2.933330]\n",
      "epoch:4 step:3914 [D loss: 0.488269, acc: 78.91%] [G loss: 2.893678]\n",
      "epoch:4 step:3915 [D loss: 0.542704, acc: 75.78%] [G loss: 3.023308]\n",
      "epoch:4 step:3916 [D loss: 0.670899, acc: 64.84%] [G loss: 2.721437]\n",
      "epoch:4 step:3917 [D loss: 0.624109, acc: 65.62%] [G loss: 2.565912]\n",
      "epoch:4 step:3918 [D loss: 0.565370, acc: 71.88%] [G loss: 2.701527]\n",
      "epoch:4 step:3919 [D loss: 0.492164, acc: 75.78%] [G loss: 2.890720]\n",
      "epoch:4 step:3920 [D loss: 0.508388, acc: 69.53%] [G loss: 3.092902]\n",
      "epoch:4 step:3921 [D loss: 0.575994, acc: 67.19%] [G loss: 2.899868]\n",
      "epoch:4 step:3922 [D loss: 0.566159, acc: 67.97%] [G loss: 3.172114]\n",
      "epoch:4 step:3923 [D loss: 0.506655, acc: 73.44%] [G loss: 2.916465]\n",
      "epoch:4 step:3924 [D loss: 0.514489, acc: 75.00%] [G loss: 3.435193]\n",
      "epoch:4 step:3925 [D loss: 0.500919, acc: 77.34%] [G loss: 3.439203]\n",
      "epoch:4 step:3926 [D loss: 0.545004, acc: 75.78%] [G loss: 3.215029]\n",
      "epoch:4 step:3927 [D loss: 0.579239, acc: 68.75%] [G loss: 2.836398]\n",
      "epoch:4 step:3928 [D loss: 0.569652, acc: 73.44%] [G loss: 2.787950]\n",
      "epoch:4 step:3929 [D loss: 0.494179, acc: 74.22%] [G loss: 3.182158]\n",
      "epoch:4 step:3930 [D loss: 0.601559, acc: 63.28%] [G loss: 3.210701]\n",
      "epoch:4 step:3931 [D loss: 0.577285, acc: 73.44%] [G loss: 2.999943]\n",
      "epoch:4 step:3932 [D loss: 0.579898, acc: 68.75%] [G loss: 2.898315]\n",
      "epoch:4 step:3933 [D loss: 0.520046, acc: 71.88%] [G loss: 2.740963]\n",
      "epoch:4 step:3934 [D loss: 0.523839, acc: 74.22%] [G loss: 2.776488]\n",
      "epoch:4 step:3935 [D loss: 0.643180, acc: 64.84%] [G loss: 2.671727]\n",
      "epoch:4 step:3936 [D loss: 0.543838, acc: 69.53%] [G loss: 2.735512]\n",
      "epoch:4 step:3937 [D loss: 0.499712, acc: 74.22%] [G loss: 2.964625]\n",
      "epoch:4 step:3938 [D loss: 0.536624, acc: 71.09%] [G loss: 3.121242]\n",
      "epoch:4 step:3939 [D loss: 0.506313, acc: 72.66%] [G loss: 3.290008]\n",
      "epoch:4 step:3940 [D loss: 0.493898, acc: 81.25%] [G loss: 2.940192]\n",
      "epoch:4 step:3941 [D loss: 0.609112, acc: 67.97%] [G loss: 3.264607]\n",
      "epoch:4 step:3942 [D loss: 0.522861, acc: 72.66%] [G loss: 3.214393]\n",
      "epoch:4 step:3943 [D loss: 0.543712, acc: 75.00%] [G loss: 2.858607]\n",
      "epoch:4 step:3944 [D loss: 0.543889, acc: 75.00%] [G loss: 2.894153]\n",
      "epoch:4 step:3945 [D loss: 0.556949, acc: 71.09%] [G loss: 3.292166]\n",
      "epoch:4 step:3946 [D loss: 0.518663, acc: 78.12%] [G loss: 3.253033]\n",
      "epoch:4 step:3947 [D loss: 0.541474, acc: 73.44%] [G loss: 3.044697]\n",
      "epoch:4 step:3948 [D loss: 0.612878, acc: 70.31%] [G loss: 2.827380]\n",
      "epoch:4 step:3949 [D loss: 0.626598, acc: 68.75%] [G loss: 2.711129]\n",
      "epoch:4 step:3950 [D loss: 0.527178, acc: 71.88%] [G loss: 3.228248]\n",
      "epoch:4 step:3951 [D loss: 0.627966, acc: 64.84%] [G loss: 3.316647]\n",
      "epoch:4 step:3952 [D loss: 0.567749, acc: 75.78%] [G loss: 2.951443]\n",
      "epoch:4 step:3953 [D loss: 0.503719, acc: 75.00%] [G loss: 3.110949]\n",
      "epoch:4 step:3954 [D loss: 0.559574, acc: 68.75%] [G loss: 3.278818]\n",
      "epoch:4 step:3955 [D loss: 0.423736, acc: 81.25%] [G loss: 3.649832]\n",
      "epoch:4 step:3956 [D loss: 0.473859, acc: 77.34%] [G loss: 3.603122]\n",
      "epoch:4 step:3957 [D loss: 0.515180, acc: 79.69%] [G loss: 3.425027]\n",
      "epoch:4 step:3958 [D loss: 0.550898, acc: 75.00%] [G loss: 3.338983]\n",
      "epoch:4 step:3959 [D loss: 0.536128, acc: 72.66%] [G loss: 2.962282]\n",
      "epoch:4 step:3960 [D loss: 0.591012, acc: 71.09%] [G loss: 3.128291]\n",
      "epoch:4 step:3961 [D loss: 0.565418, acc: 71.88%] [G loss: 3.074147]\n",
      "epoch:4 step:3962 [D loss: 0.636262, acc: 64.06%] [G loss: 2.830675]\n",
      "epoch:4 step:3963 [D loss: 0.663548, acc: 65.62%] [G loss: 2.816019]\n",
      "epoch:4 step:3964 [D loss: 0.665698, acc: 64.06%] [G loss: 2.675687]\n",
      "epoch:4 step:3965 [D loss: 0.541401, acc: 73.44%] [G loss: 3.423126]\n",
      "epoch:4 step:3966 [D loss: 0.518128, acc: 75.00%] [G loss: 3.015464]\n",
      "epoch:4 step:3967 [D loss: 0.656506, acc: 65.62%] [G loss: 2.935632]\n",
      "epoch:4 step:3968 [D loss: 0.594622, acc: 71.09%] [G loss: 2.969926]\n",
      "epoch:4 step:3969 [D loss: 0.509872, acc: 73.44%] [G loss: 2.891563]\n",
      "epoch:4 step:3970 [D loss: 0.614137, acc: 69.53%] [G loss: 2.863343]\n",
      "epoch:4 step:3971 [D loss: 0.540334, acc: 72.66%] [G loss: 2.942136]\n",
      "epoch:4 step:3972 [D loss: 0.535092, acc: 70.31%] [G loss: 2.635145]\n",
      "epoch:4 step:3973 [D loss: 0.607850, acc: 69.53%] [G loss: 2.832420]\n",
      "epoch:4 step:3974 [D loss: 0.566271, acc: 67.97%] [G loss: 2.940467]\n",
      "epoch:4 step:3975 [D loss: 0.583794, acc: 70.31%] [G loss: 2.825637]\n",
      "epoch:4 step:3976 [D loss: 0.680461, acc: 60.16%] [G loss: 2.721766]\n",
      "epoch:4 step:3977 [D loss: 0.573487, acc: 66.41%] [G loss: 2.846996]\n",
      "epoch:4 step:3978 [D loss: 0.546526, acc: 72.66%] [G loss: 3.183995]\n",
      "epoch:4 step:3979 [D loss: 0.489550, acc: 79.69%] [G loss: 3.246200]\n",
      "epoch:4 step:3980 [D loss: 0.491724, acc: 78.12%] [G loss: 3.673266]\n",
      "epoch:4 step:3981 [D loss: 0.592264, acc: 67.97%] [G loss: 2.851724]\n",
      "epoch:4 step:3982 [D loss: 0.574676, acc: 73.44%] [G loss: 2.757691]\n",
      "epoch:4 step:3983 [D loss: 0.519501, acc: 73.44%] [G loss: 2.944792]\n",
      "epoch:4 step:3984 [D loss: 0.567276, acc: 69.53%] [G loss: 2.912228]\n",
      "epoch:4 step:3985 [D loss: 0.573991, acc: 71.88%] [G loss: 2.847459]\n",
      "epoch:4 step:3986 [D loss: 0.570571, acc: 69.53%] [G loss: 3.127947]\n",
      "epoch:4 step:3987 [D loss: 0.605997, acc: 68.75%] [G loss: 3.081586]\n",
      "epoch:4 step:3988 [D loss: 0.608430, acc: 67.19%] [G loss: 2.839331]\n",
      "epoch:4 step:3989 [D loss: 0.576213, acc: 67.19%] [G loss: 2.876545]\n",
      "epoch:4 step:3990 [D loss: 0.544066, acc: 75.00%] [G loss: 3.301562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3991 [D loss: 0.559105, acc: 73.44%] [G loss: 3.134804]\n",
      "epoch:4 step:3992 [D loss: 0.560100, acc: 71.09%] [G loss: 2.765427]\n",
      "epoch:4 step:3993 [D loss: 0.642705, acc: 63.28%] [G loss: 2.866589]\n",
      "epoch:4 step:3994 [D loss: 0.580206, acc: 70.31%] [G loss: 2.410320]\n",
      "epoch:4 step:3995 [D loss: 0.633675, acc: 67.97%] [G loss: 2.490764]\n",
      "epoch:4 step:3996 [D loss: 0.620322, acc: 66.41%] [G loss: 2.658095]\n",
      "epoch:4 step:3997 [D loss: 0.589318, acc: 67.97%] [G loss: 2.839803]\n",
      "epoch:4 step:3998 [D loss: 0.602810, acc: 66.41%] [G loss: 2.737871]\n",
      "epoch:4 step:3999 [D loss: 0.639666, acc: 62.50%] [G loss: 2.705004]\n",
      "epoch:4 step:4000 [D loss: 0.582125, acc: 63.28%] [G loss: 2.581166]\n",
      "epoch:4 step:4001 [D loss: 0.561100, acc: 71.09%] [G loss: 2.301707]\n",
      "epoch:4 step:4002 [D loss: 0.571517, acc: 67.97%] [G loss: 2.792246]\n",
      "epoch:4 step:4003 [D loss: 0.583734, acc: 69.53%] [G loss: 2.648177]\n",
      "epoch:4 step:4004 [D loss: 0.605301, acc: 70.31%] [G loss: 2.864728]\n",
      "epoch:4 step:4005 [D loss: 0.564049, acc: 69.53%] [G loss: 2.880516]\n",
      "epoch:4 step:4006 [D loss: 0.654374, acc: 67.19%] [G loss: 3.084504]\n",
      "epoch:4 step:4007 [D loss: 0.493193, acc: 78.12%] [G loss: 2.700607]\n",
      "epoch:4 step:4008 [D loss: 0.486264, acc: 78.91%] [G loss: 2.686826]\n",
      "epoch:4 step:4009 [D loss: 0.594812, acc: 67.97%] [G loss: 3.189328]\n",
      "epoch:4 step:4010 [D loss: 0.566568, acc: 71.09%] [G loss: 3.113861]\n",
      "epoch:4 step:4011 [D loss: 0.593935, acc: 67.97%] [G loss: 2.649839]\n",
      "epoch:4 step:4012 [D loss: 0.551794, acc: 75.00%] [G loss: 2.987753]\n",
      "epoch:4 step:4013 [D loss: 0.560241, acc: 71.09%] [G loss: 3.013079]\n",
      "epoch:4 step:4014 [D loss: 0.567919, acc: 74.22%] [G loss: 3.036277]\n",
      "epoch:4 step:4015 [D loss: 0.530789, acc: 72.66%] [G loss: 2.937879]\n",
      "epoch:4 step:4016 [D loss: 0.585061, acc: 70.31%] [G loss: 2.414109]\n",
      "epoch:4 step:4017 [D loss: 0.605257, acc: 66.41%] [G loss: 3.052687]\n",
      "epoch:4 step:4018 [D loss: 0.577655, acc: 70.31%] [G loss: 2.908498]\n",
      "epoch:4 step:4019 [D loss: 0.527250, acc: 70.31%] [G loss: 3.146811]\n",
      "epoch:4 step:4020 [D loss: 0.531600, acc: 76.56%] [G loss: 2.776712]\n",
      "epoch:4 step:4021 [D loss: 0.551653, acc: 70.31%] [G loss: 3.132134]\n",
      "epoch:4 step:4022 [D loss: 0.530861, acc: 71.88%] [G loss: 2.842250]\n",
      "epoch:4 step:4023 [D loss: 0.644918, acc: 64.84%] [G loss: 2.659316]\n",
      "epoch:4 step:4024 [D loss: 0.595284, acc: 68.75%] [G loss: 3.027146]\n",
      "epoch:4 step:4025 [D loss: 0.569281, acc: 72.66%] [G loss: 2.828732]\n",
      "epoch:4 step:4026 [D loss: 0.535255, acc: 71.09%] [G loss: 2.795469]\n",
      "epoch:4 step:4027 [D loss: 0.483815, acc: 75.00%] [G loss: 3.048801]\n",
      "epoch:4 step:4028 [D loss: 0.495265, acc: 75.00%] [G loss: 2.980663]\n",
      "epoch:4 step:4029 [D loss: 0.654832, acc: 69.53%] [G loss: 2.943764]\n",
      "epoch:4 step:4030 [D loss: 0.548785, acc: 73.44%] [G loss: 2.836526]\n",
      "epoch:4 step:4031 [D loss: 0.504819, acc: 73.44%] [G loss: 2.916015]\n",
      "epoch:4 step:4032 [D loss: 0.508263, acc: 78.12%] [G loss: 3.012933]\n",
      "epoch:4 step:4033 [D loss: 0.486747, acc: 78.12%] [G loss: 3.106950]\n",
      "epoch:4 step:4034 [D loss: 0.510183, acc: 77.34%] [G loss: 3.430530]\n",
      "epoch:4 step:4035 [D loss: 0.579995, acc: 66.41%] [G loss: 2.987788]\n",
      "epoch:4 step:4036 [D loss: 0.575284, acc: 69.53%] [G loss: 2.854172]\n",
      "epoch:4 step:4037 [D loss: 0.553490, acc: 71.09%] [G loss: 3.277043]\n",
      "epoch:4 step:4038 [D loss: 0.483752, acc: 75.78%] [G loss: 3.097041]\n",
      "epoch:4 step:4039 [D loss: 0.556773, acc: 67.19%] [G loss: 2.893289]\n",
      "epoch:4 step:4040 [D loss: 0.561241, acc: 72.66%] [G loss: 2.881144]\n",
      "epoch:4 step:4041 [D loss: 0.517503, acc: 75.00%] [G loss: 3.262113]\n",
      "epoch:4 step:4042 [D loss: 0.565942, acc: 75.00%] [G loss: 3.361266]\n",
      "epoch:4 step:4043 [D loss: 0.492081, acc: 76.56%] [G loss: 3.480108]\n",
      "epoch:4 step:4044 [D loss: 0.514561, acc: 74.22%] [G loss: 3.469421]\n",
      "epoch:4 step:4045 [D loss: 0.512560, acc: 75.78%] [G loss: 2.942869]\n",
      "epoch:4 step:4046 [D loss: 0.514914, acc: 73.44%] [G loss: 3.267328]\n",
      "epoch:4 step:4047 [D loss: 0.553066, acc: 72.66%] [G loss: 3.084829]\n",
      "epoch:4 step:4048 [D loss: 0.519786, acc: 73.44%] [G loss: 3.164922]\n",
      "epoch:4 step:4049 [D loss: 0.671420, acc: 64.06%] [G loss: 2.817672]\n",
      "epoch:4 step:4050 [D loss: 0.619499, acc: 65.62%] [G loss: 2.869744]\n",
      "epoch:4 step:4051 [D loss: 0.559616, acc: 70.31%] [G loss: 3.267973]\n",
      "epoch:4 step:4052 [D loss: 0.516722, acc: 76.56%] [G loss: 3.376688]\n",
      "epoch:4 step:4053 [D loss: 0.512718, acc: 77.34%] [G loss: 2.967028]\n",
      "epoch:4 step:4054 [D loss: 0.637033, acc: 61.72%] [G loss: 2.595572]\n",
      "epoch:4 step:4055 [D loss: 0.501435, acc: 74.22%] [G loss: 2.976880]\n",
      "epoch:4 step:4056 [D loss: 0.521068, acc: 75.00%] [G loss: 2.892415]\n",
      "epoch:4 step:4057 [D loss: 0.531406, acc: 75.78%] [G loss: 3.317749]\n",
      "epoch:4 step:4058 [D loss: 0.595746, acc: 70.31%] [G loss: 3.077330]\n",
      "epoch:4 step:4059 [D loss: 0.526926, acc: 75.00%] [G loss: 3.291502]\n",
      "epoch:4 step:4060 [D loss: 0.541447, acc: 71.88%] [G loss: 3.583351]\n",
      "epoch:4 step:4061 [D loss: 0.487317, acc: 78.12%] [G loss: 3.623864]\n",
      "epoch:4 step:4062 [D loss: 0.373950, acc: 85.16%] [G loss: 3.946860]\n",
      "epoch:4 step:4063 [D loss: 0.500509, acc: 75.00%] [G loss: 3.725466]\n",
      "epoch:4 step:4064 [D loss: 0.727677, acc: 63.28%] [G loss: 2.716084]\n",
      "epoch:4 step:4065 [D loss: 0.622692, acc: 64.06%] [G loss: 3.099840]\n",
      "epoch:4 step:4066 [D loss: 0.586165, acc: 67.19%] [G loss: 2.799851]\n",
      "epoch:4 step:4067 [D loss: 0.495415, acc: 75.78%] [G loss: 2.574168]\n",
      "epoch:4 step:4068 [D loss: 0.509229, acc: 78.12%] [G loss: 2.853146]\n",
      "epoch:4 step:4069 [D loss: 0.583214, acc: 72.66%] [G loss: 3.002311]\n",
      "epoch:4 step:4070 [D loss: 0.473920, acc: 76.56%] [G loss: 3.274886]\n",
      "epoch:4 step:4071 [D loss: 0.585102, acc: 67.19%] [G loss: 3.066958]\n",
      "epoch:4 step:4072 [D loss: 0.617731, acc: 68.75%] [G loss: 3.051007]\n",
      "epoch:4 step:4073 [D loss: 0.591892, acc: 71.88%] [G loss: 2.778959]\n",
      "epoch:4 step:4074 [D loss: 0.665275, acc: 63.28%] [G loss: 2.846811]\n",
      "epoch:4 step:4075 [D loss: 0.552928, acc: 66.41%] [G loss: 3.315867]\n",
      "epoch:4 step:4076 [D loss: 0.607601, acc: 73.44%] [G loss: 2.849306]\n",
      "epoch:4 step:4077 [D loss: 0.623460, acc: 61.72%] [G loss: 2.516768]\n",
      "epoch:4 step:4078 [D loss: 0.531506, acc: 72.66%] [G loss: 2.964296]\n",
      "epoch:4 step:4079 [D loss: 0.580635, acc: 67.97%] [G loss: 2.860957]\n",
      "epoch:4 step:4080 [D loss: 0.574449, acc: 72.66%] [G loss: 2.922625]\n",
      "epoch:4 step:4081 [D loss: 0.504494, acc: 71.09%] [G loss: 3.341170]\n",
      "epoch:4 step:4082 [D loss: 0.692616, acc: 59.38%] [G loss: 2.836251]\n",
      "epoch:4 step:4083 [D loss: 0.455354, acc: 80.47%] [G loss: 3.474613]\n",
      "epoch:4 step:4084 [D loss: 0.560017, acc: 73.44%] [G loss: 2.867442]\n",
      "epoch:4 step:4085 [D loss: 0.594197, acc: 72.66%] [G loss: 2.812155]\n",
      "epoch:4 step:4086 [D loss: 0.609266, acc: 70.31%] [G loss: 2.670930]\n",
      "epoch:4 step:4087 [D loss: 0.641081, acc: 61.72%] [G loss: 2.421086]\n",
      "epoch:4 step:4088 [D loss: 0.555744, acc: 71.09%] [G loss: 2.825763]\n",
      "epoch:4 step:4089 [D loss: 0.639285, acc: 64.84%] [G loss: 2.539192]\n",
      "epoch:4 step:4090 [D loss: 0.653406, acc: 62.50%] [G loss: 2.719519]\n",
      "epoch:4 step:4091 [D loss: 0.576476, acc: 67.19%] [G loss: 3.070518]\n",
      "epoch:4 step:4092 [D loss: 0.567908, acc: 71.88%] [G loss: 2.785793]\n",
      "epoch:4 step:4093 [D loss: 0.578884, acc: 68.75%] [G loss: 2.957763]\n",
      "epoch:4 step:4094 [D loss: 0.539013, acc: 76.56%] [G loss: 3.256979]\n",
      "epoch:4 step:4095 [D loss: 0.630709, acc: 66.41%] [G loss: 3.033858]\n",
      "epoch:4 step:4096 [D loss: 0.635595, acc: 63.28%] [G loss: 2.529621]\n",
      "epoch:4 step:4097 [D loss: 0.713086, acc: 57.03%] [G loss: 2.375132]\n",
      "epoch:4 step:4098 [D loss: 0.624773, acc: 67.19%] [G loss: 2.900917]\n",
      "epoch:4 step:4099 [D loss: 0.580947, acc: 67.19%] [G loss: 2.772389]\n",
      "epoch:4 step:4100 [D loss: 0.605859, acc: 68.75%] [G loss: 2.768991]\n",
      "epoch:4 step:4101 [D loss: 0.689935, acc: 68.75%] [G loss: 2.799529]\n",
      "epoch:4 step:4102 [D loss: 0.643328, acc: 61.72%] [G loss: 2.643199]\n",
      "epoch:4 step:4103 [D loss: 0.567864, acc: 71.09%] [G loss: 2.570163]\n",
      "epoch:4 step:4104 [D loss: 0.608656, acc: 65.62%] [G loss: 2.351891]\n",
      "epoch:4 step:4105 [D loss: 0.562448, acc: 70.31%] [G loss: 2.647608]\n",
      "epoch:4 step:4106 [D loss: 0.544353, acc: 69.53%] [G loss: 2.794616]\n",
      "epoch:4 step:4107 [D loss: 0.462383, acc: 80.47%] [G loss: 2.670711]\n",
      "epoch:4 step:4108 [D loss: 0.582358, acc: 67.19%] [G loss: 2.896575]\n",
      "epoch:4 step:4109 [D loss: 0.561692, acc: 72.66%] [G loss: 2.586477]\n",
      "epoch:4 step:4110 [D loss: 0.612178, acc: 60.16%] [G loss: 2.696227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4111 [D loss: 0.520057, acc: 73.44%] [G loss: 3.053130]\n",
      "epoch:4 step:4112 [D loss: 0.506193, acc: 74.22%] [G loss: 2.837152]\n",
      "epoch:4 step:4113 [D loss: 0.494138, acc: 74.22%] [G loss: 3.121286]\n",
      "epoch:4 step:4114 [D loss: 0.537573, acc: 73.44%] [G loss: 3.014359]\n",
      "epoch:4 step:4115 [D loss: 0.529084, acc: 71.88%] [G loss: 2.896342]\n",
      "epoch:4 step:4116 [D loss: 0.520603, acc: 75.78%] [G loss: 2.901172]\n",
      "epoch:4 step:4117 [D loss: 0.558113, acc: 71.09%] [G loss: 2.748603]\n",
      "epoch:4 step:4118 [D loss: 0.552527, acc: 71.88%] [G loss: 3.092956]\n",
      "epoch:4 step:4119 [D loss: 0.605779, acc: 69.53%] [G loss: 3.077812]\n",
      "epoch:4 step:4120 [D loss: 0.513393, acc: 74.22%] [G loss: 3.223315]\n",
      "epoch:4 step:4121 [D loss: 0.652029, acc: 62.50%] [G loss: 2.772036]\n",
      "epoch:4 step:4122 [D loss: 0.467061, acc: 80.47%] [G loss: 2.961689]\n",
      "epoch:4 step:4123 [D loss: 0.589051, acc: 68.75%] [G loss: 2.815454]\n",
      "epoch:4 step:4124 [D loss: 0.670715, acc: 64.06%] [G loss: 2.606074]\n",
      "epoch:4 step:4125 [D loss: 0.664521, acc: 64.06%] [G loss: 2.335926]\n",
      "epoch:4 step:4126 [D loss: 0.611757, acc: 70.31%] [G loss: 2.993029]\n",
      "epoch:4 step:4127 [D loss: 0.583450, acc: 68.75%] [G loss: 2.734831]\n",
      "epoch:4 step:4128 [D loss: 0.501575, acc: 77.34%] [G loss: 3.837528]\n",
      "epoch:4 step:4129 [D loss: 0.506573, acc: 78.12%] [G loss: 3.571142]\n",
      "epoch:4 step:4130 [D loss: 0.550407, acc: 70.31%] [G loss: 2.694825]\n",
      "epoch:4 step:4131 [D loss: 0.647580, acc: 64.84%] [G loss: 2.664463]\n",
      "epoch:4 step:4132 [D loss: 0.551264, acc: 76.56%] [G loss: 2.889329]\n",
      "epoch:4 step:4133 [D loss: 0.583521, acc: 68.75%] [G loss: 2.858327]\n",
      "epoch:4 step:4134 [D loss: 0.532916, acc: 74.22%] [G loss: 2.865155]\n",
      "epoch:4 step:4135 [D loss: 0.562583, acc: 67.97%] [G loss: 2.990191]\n",
      "epoch:4 step:4136 [D loss: 0.591896, acc: 67.19%] [G loss: 2.940731]\n",
      "epoch:4 step:4137 [D loss: 0.567125, acc: 71.09%] [G loss: 3.003723]\n",
      "epoch:4 step:4138 [D loss: 0.536073, acc: 73.44%] [G loss: 3.282602]\n",
      "epoch:4 step:4139 [D loss: 0.592135, acc: 74.22%] [G loss: 2.881833]\n",
      "epoch:4 step:4140 [D loss: 0.539908, acc: 74.22%] [G loss: 2.973647]\n",
      "epoch:4 step:4141 [D loss: 0.582315, acc: 71.88%] [G loss: 2.824245]\n",
      "epoch:4 step:4142 [D loss: 0.601285, acc: 69.53%] [G loss: 2.595454]\n",
      "epoch:4 step:4143 [D loss: 0.520135, acc: 78.12%] [G loss: 3.301120]\n",
      "epoch:4 step:4144 [D loss: 0.629135, acc: 64.84%] [G loss: 2.620205]\n",
      "epoch:4 step:4145 [D loss: 0.447160, acc: 78.91%] [G loss: 2.987476]\n",
      "epoch:4 step:4146 [D loss: 0.507719, acc: 77.34%] [G loss: 3.039122]\n",
      "epoch:4 step:4147 [D loss: 0.526158, acc: 75.00%] [G loss: 3.629015]\n",
      "epoch:4 step:4148 [D loss: 0.595461, acc: 67.97%] [G loss: 2.542868]\n",
      "epoch:4 step:4149 [D loss: 0.606278, acc: 61.72%] [G loss: 3.108708]\n",
      "epoch:4 step:4150 [D loss: 0.489628, acc: 76.56%] [G loss: 3.072481]\n",
      "epoch:4 step:4151 [D loss: 0.723232, acc: 63.28%] [G loss: 3.235409]\n",
      "epoch:4 step:4152 [D loss: 0.599287, acc: 68.75%] [G loss: 3.111754]\n",
      "epoch:4 step:4153 [D loss: 0.545680, acc: 75.78%] [G loss: 3.247414]\n",
      "epoch:4 step:4154 [D loss: 0.533365, acc: 72.66%] [G loss: 3.160176]\n",
      "epoch:4 step:4155 [D loss: 0.567406, acc: 70.31%] [G loss: 2.815601]\n",
      "epoch:4 step:4156 [D loss: 0.599057, acc: 64.84%] [G loss: 3.108635]\n",
      "epoch:4 step:4157 [D loss: 0.618441, acc: 61.72%] [G loss: 2.889017]\n",
      "epoch:4 step:4158 [D loss: 0.632641, acc: 66.41%] [G loss: 2.723267]\n",
      "epoch:4 step:4159 [D loss: 0.614154, acc: 67.19%] [G loss: 2.789365]\n",
      "epoch:4 step:4160 [D loss: 0.611857, acc: 65.62%] [G loss: 2.772585]\n",
      "epoch:4 step:4161 [D loss: 0.521158, acc: 72.66%] [G loss: 2.888805]\n",
      "epoch:4 step:4162 [D loss: 0.746516, acc: 58.59%] [G loss: 2.727395]\n",
      "epoch:4 step:4163 [D loss: 0.558629, acc: 69.53%] [G loss: 2.586401]\n",
      "epoch:4 step:4164 [D loss: 0.643866, acc: 64.06%] [G loss: 2.533255]\n",
      "epoch:4 step:4165 [D loss: 0.617575, acc: 69.53%] [G loss: 2.215819]\n",
      "epoch:4 step:4166 [D loss: 0.616704, acc: 64.06%] [G loss: 2.607781]\n",
      "epoch:4 step:4167 [D loss: 0.554587, acc: 75.78%] [G loss: 2.918895]\n",
      "epoch:4 step:4168 [D loss: 0.642277, acc: 61.72%] [G loss: 2.440098]\n",
      "epoch:4 step:4169 [D loss: 0.567540, acc: 68.75%] [G loss: 2.723516]\n",
      "epoch:4 step:4170 [D loss: 0.415870, acc: 82.81%] [G loss: 3.327029]\n",
      "epoch:4 step:4171 [D loss: 0.485574, acc: 73.44%] [G loss: 3.174241]\n",
      "epoch:4 step:4172 [D loss: 0.525766, acc: 75.00%] [G loss: 2.798402]\n",
      "epoch:4 step:4173 [D loss: 0.636532, acc: 65.62%] [G loss: 2.884670]\n",
      "epoch:4 step:4174 [D loss: 0.521476, acc: 75.00%] [G loss: 2.927371]\n",
      "epoch:4 step:4175 [D loss: 0.572479, acc: 67.97%] [G loss: 3.335772]\n",
      "epoch:4 step:4176 [D loss: 0.486133, acc: 74.22%] [G loss: 3.143323]\n",
      "epoch:4 step:4177 [D loss: 0.512718, acc: 76.56%] [G loss: 2.930752]\n",
      "epoch:4 step:4178 [D loss: 0.574593, acc: 68.75%] [G loss: 3.185277]\n",
      "epoch:4 step:4179 [D loss: 0.543551, acc: 72.66%] [G loss: 3.030786]\n",
      "epoch:4 step:4180 [D loss: 0.584492, acc: 68.75%] [G loss: 2.995339]\n",
      "epoch:4 step:4181 [D loss: 0.605228, acc: 68.75%] [G loss: 2.782824]\n",
      "epoch:4 step:4182 [D loss: 0.486670, acc: 76.56%] [G loss: 2.741262]\n",
      "epoch:4 step:4183 [D loss: 0.636896, acc: 67.19%] [G loss: 2.778726]\n",
      "epoch:4 step:4184 [D loss: 0.516093, acc: 72.66%] [G loss: 2.998723]\n",
      "epoch:4 step:4185 [D loss: 0.691114, acc: 60.16%] [G loss: 2.399613]\n",
      "epoch:4 step:4186 [D loss: 0.614227, acc: 63.28%] [G loss: 2.706199]\n",
      "epoch:4 step:4187 [D loss: 0.600489, acc: 67.19%] [G loss: 2.514326]\n",
      "epoch:4 step:4188 [D loss: 0.558975, acc: 74.22%] [G loss: 2.843907]\n",
      "epoch:4 step:4189 [D loss: 0.653565, acc: 60.94%] [G loss: 2.517903]\n",
      "epoch:4 step:4190 [D loss: 0.545842, acc: 76.56%] [G loss: 2.667053]\n",
      "epoch:4 step:4191 [D loss: 0.635959, acc: 64.84%] [G loss: 2.838490]\n",
      "epoch:4 step:4192 [D loss: 0.576093, acc: 67.97%] [G loss: 2.639915]\n",
      "epoch:4 step:4193 [D loss: 0.547885, acc: 71.09%] [G loss: 2.521950]\n",
      "epoch:4 step:4194 [D loss: 0.568199, acc: 71.88%] [G loss: 2.747976]\n",
      "epoch:4 step:4195 [D loss: 0.543524, acc: 72.66%] [G loss: 2.766294]\n",
      "epoch:4 step:4196 [D loss: 0.644095, acc: 67.19%] [G loss: 2.580372]\n",
      "epoch:4 step:4197 [D loss: 0.584007, acc: 72.66%] [G loss: 2.882556]\n",
      "epoch:4 step:4198 [D loss: 0.505736, acc: 75.78%] [G loss: 2.949048]\n",
      "epoch:4 step:4199 [D loss: 0.576437, acc: 71.09%] [G loss: 2.987595]\n",
      "epoch:4 step:4200 [D loss: 0.507644, acc: 74.22%] [G loss: 2.866654]\n",
      "epoch:4 step:4201 [D loss: 0.605717, acc: 67.97%] [G loss: 2.733098]\n",
      "epoch:4 step:4202 [D loss: 0.535178, acc: 70.31%] [G loss: 2.747225]\n",
      "epoch:4 step:4203 [D loss: 0.592959, acc: 71.09%] [G loss: 2.764463]\n",
      "epoch:4 step:4204 [D loss: 0.589681, acc: 71.88%] [G loss: 2.894064]\n",
      "epoch:4 step:4205 [D loss: 0.524471, acc: 74.22%] [G loss: 2.723901]\n",
      "epoch:4 step:4206 [D loss: 0.543820, acc: 75.00%] [G loss: 2.816217]\n",
      "epoch:4 step:4207 [D loss: 0.600377, acc: 67.97%] [G loss: 2.629659]\n",
      "epoch:4 step:4208 [D loss: 0.588263, acc: 73.44%] [G loss: 2.882484]\n",
      "epoch:4 step:4209 [D loss: 0.622994, acc: 69.53%] [G loss: 2.653125]\n",
      "epoch:4 step:4210 [D loss: 0.505122, acc: 76.56%] [G loss: 2.820155]\n",
      "epoch:4 step:4211 [D loss: 0.587301, acc: 67.97%] [G loss: 3.040962]\n",
      "epoch:4 step:4212 [D loss: 0.534930, acc: 75.78%] [G loss: 2.852423]\n",
      "epoch:4 step:4213 [D loss: 0.654557, acc: 66.41%] [G loss: 2.646908]\n",
      "epoch:4 step:4214 [D loss: 0.625772, acc: 66.41%] [G loss: 2.740195]\n",
      "epoch:4 step:4215 [D loss: 0.535600, acc: 69.53%] [G loss: 2.700934]\n",
      "epoch:4 step:4216 [D loss: 0.609741, acc: 64.84%] [G loss: 2.696580]\n",
      "epoch:4 step:4217 [D loss: 0.620872, acc: 68.75%] [G loss: 2.792386]\n",
      "epoch:4 step:4218 [D loss: 0.513355, acc: 72.66%] [G loss: 2.832911]\n",
      "epoch:4 step:4219 [D loss: 0.601891, acc: 70.31%] [G loss: 3.368319]\n",
      "epoch:4 step:4220 [D loss: 0.476230, acc: 74.22%] [G loss: 3.338891]\n",
      "epoch:4 step:4221 [D loss: 0.584978, acc: 70.31%] [G loss: 2.895940]\n",
      "epoch:4 step:4222 [D loss: 0.479659, acc: 78.12%] [G loss: 3.220647]\n",
      "epoch:4 step:4223 [D loss: 0.503349, acc: 77.34%] [G loss: 3.383897]\n",
      "epoch:4 step:4224 [D loss: 0.593099, acc: 67.19%] [G loss: 2.980336]\n",
      "epoch:4 step:4225 [D loss: 0.707096, acc: 62.50%] [G loss: 2.478400]\n",
      "epoch:4 step:4226 [D loss: 0.643202, acc: 63.28%] [G loss: 2.572217]\n",
      "epoch:4 step:4227 [D loss: 0.607819, acc: 68.75%] [G loss: 2.657771]\n",
      "epoch:4 step:4228 [D loss: 0.665335, acc: 61.72%] [G loss: 2.482322]\n",
      "epoch:4 step:4229 [D loss: 0.601714, acc: 64.06%] [G loss: 2.595311]\n",
      "epoch:4 step:4230 [D loss: 0.619970, acc: 67.97%] [G loss: 2.459304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4231 [D loss: 0.632500, acc: 60.94%] [G loss: 2.600186]\n",
      "epoch:4 step:4232 [D loss: 0.563188, acc: 67.19%] [G loss: 2.916481]\n",
      "epoch:4 step:4233 [D loss: 0.537143, acc: 78.91%] [G loss: 3.075502]\n",
      "epoch:4 step:4234 [D loss: 0.591625, acc: 68.75%] [G loss: 2.668334]\n",
      "epoch:4 step:4235 [D loss: 0.564896, acc: 72.66%] [G loss: 2.515498]\n",
      "epoch:4 step:4236 [D loss: 0.534536, acc: 71.88%] [G loss: 2.903693]\n",
      "epoch:4 step:4237 [D loss: 0.610844, acc: 68.75%] [G loss: 2.916999]\n",
      "epoch:4 step:4238 [D loss: 0.534776, acc: 67.19%] [G loss: 2.877936]\n",
      "epoch:4 step:4239 [D loss: 0.581914, acc: 67.19%] [G loss: 2.979496]\n",
      "epoch:4 step:4240 [D loss: 0.529232, acc: 71.88%] [G loss: 2.902047]\n",
      "epoch:4 step:4241 [D loss: 0.593817, acc: 65.62%] [G loss: 2.657706]\n",
      "epoch:4 step:4242 [D loss: 0.590284, acc: 73.44%] [G loss: 2.865904]\n",
      "epoch:4 step:4243 [D loss: 0.516034, acc: 72.66%] [G loss: 2.875112]\n",
      "epoch:4 step:4244 [D loss: 0.515887, acc: 73.44%] [G loss: 2.997465]\n",
      "epoch:4 step:4245 [D loss: 0.545184, acc: 68.75%] [G loss: 3.153238]\n",
      "epoch:4 step:4246 [D loss: 0.512722, acc: 77.34%] [G loss: 3.354316]\n",
      "epoch:4 step:4247 [D loss: 0.511683, acc: 76.56%] [G loss: 3.605856]\n",
      "epoch:4 step:4248 [D loss: 0.739780, acc: 60.16%] [G loss: 2.512698]\n",
      "epoch:4 step:4249 [D loss: 0.652316, acc: 65.62%] [G loss: 2.598711]\n",
      "epoch:4 step:4250 [D loss: 0.576176, acc: 67.97%] [G loss: 2.411010]\n",
      "epoch:4 step:4251 [D loss: 0.511217, acc: 76.56%] [G loss: 2.886662]\n",
      "epoch:4 step:4252 [D loss: 0.586538, acc: 72.66%] [G loss: 3.118834]\n",
      "epoch:4 step:4253 [D loss: 0.595536, acc: 64.84%] [G loss: 2.475405]\n",
      "epoch:4 step:4254 [D loss: 0.560635, acc: 69.53%] [G loss: 2.789801]\n",
      "epoch:4 step:4255 [D loss: 0.622667, acc: 64.84%] [G loss: 2.630743]\n",
      "epoch:4 step:4256 [D loss: 0.504423, acc: 73.44%] [G loss: 3.185298]\n",
      "epoch:4 step:4257 [D loss: 0.655599, acc: 65.62%] [G loss: 2.549331]\n",
      "epoch:4 step:4258 [D loss: 0.641509, acc: 71.09%] [G loss: 2.647753]\n",
      "epoch:4 step:4259 [D loss: 0.578213, acc: 66.41%] [G loss: 2.850326]\n",
      "epoch:4 step:4260 [D loss: 0.586145, acc: 71.88%] [G loss: 2.832909]\n",
      "epoch:4 step:4261 [D loss: 0.625030, acc: 69.53%] [G loss: 2.605126]\n",
      "epoch:4 step:4262 [D loss: 0.508565, acc: 71.88%] [G loss: 2.982875]\n",
      "epoch:4 step:4263 [D loss: 0.533836, acc: 74.22%] [G loss: 3.153479]\n",
      "epoch:4 step:4264 [D loss: 0.525617, acc: 70.31%] [G loss: 2.542287]\n",
      "epoch:4 step:4265 [D loss: 0.573516, acc: 75.78%] [G loss: 2.764040]\n",
      "epoch:4 step:4266 [D loss: 0.575654, acc: 71.09%] [G loss: 2.951237]\n",
      "epoch:4 step:4267 [D loss: 0.551500, acc: 71.88%] [G loss: 2.749357]\n",
      "epoch:4 step:4268 [D loss: 0.550260, acc: 73.44%] [G loss: 3.410242]\n",
      "epoch:4 step:4269 [D loss: 0.544015, acc: 77.34%] [G loss: 3.103484]\n",
      "epoch:4 step:4270 [D loss: 0.479542, acc: 78.12%] [G loss: 2.861387]\n",
      "epoch:4 step:4271 [D loss: 0.539111, acc: 72.66%] [G loss: 3.593168]\n",
      "epoch:4 step:4272 [D loss: 0.610232, acc: 69.53%] [G loss: 3.014777]\n",
      "epoch:4 step:4273 [D loss: 0.613001, acc: 68.75%] [G loss: 2.607430]\n",
      "epoch:4 step:4274 [D loss: 0.561273, acc: 69.53%] [G loss: 3.016974]\n",
      "epoch:4 step:4275 [D loss: 0.540188, acc: 76.56%] [G loss: 3.032154]\n",
      "epoch:4 step:4276 [D loss: 0.634809, acc: 60.16%] [G loss: 2.542639]\n",
      "epoch:4 step:4277 [D loss: 0.605452, acc: 74.22%] [G loss: 2.636310]\n",
      "epoch:4 step:4278 [D loss: 0.570358, acc: 67.19%] [G loss: 2.871656]\n",
      "epoch:4 step:4279 [D loss: 0.662183, acc: 60.94%] [G loss: 2.744797]\n",
      "epoch:4 step:4280 [D loss: 0.563513, acc: 75.78%] [G loss: 2.733194]\n",
      "epoch:4 step:4281 [D loss: 0.653923, acc: 62.50%] [G loss: 2.896734]\n",
      "epoch:4 step:4282 [D loss: 0.576380, acc: 67.97%] [G loss: 3.077381]\n",
      "epoch:4 step:4283 [D loss: 0.594745, acc: 68.75%] [G loss: 2.394946]\n",
      "epoch:4 step:4284 [D loss: 0.511868, acc: 76.56%] [G loss: 2.931150]\n",
      "epoch:4 step:4285 [D loss: 0.624034, acc: 67.19%] [G loss: 2.805990]\n",
      "epoch:4 step:4286 [D loss: 0.627401, acc: 66.41%] [G loss: 2.685076]\n",
      "epoch:4 step:4287 [D loss: 0.544446, acc: 72.66%] [G loss: 2.457859]\n",
      "epoch:4 step:4288 [D loss: 0.550235, acc: 69.53%] [G loss: 2.734271]\n",
      "epoch:4 step:4289 [D loss: 0.457625, acc: 78.12%] [G loss: 3.032769]\n",
      "epoch:4 step:4290 [D loss: 0.587609, acc: 67.97%] [G loss: 2.873746]\n",
      "epoch:4 step:4291 [D loss: 0.608720, acc: 69.53%] [G loss: 2.897432]\n",
      "epoch:4 step:4292 [D loss: 0.564817, acc: 73.44%] [G loss: 3.191908]\n",
      "epoch:4 step:4293 [D loss: 0.585006, acc: 69.53%] [G loss: 2.876775]\n",
      "epoch:4 step:4294 [D loss: 0.541038, acc: 75.78%] [G loss: 3.004425]\n",
      "epoch:4 step:4295 [D loss: 0.537793, acc: 70.31%] [G loss: 2.936167]\n",
      "epoch:4 step:4296 [D loss: 0.615860, acc: 65.62%] [G loss: 3.020792]\n",
      "epoch:4 step:4297 [D loss: 0.612605, acc: 70.31%] [G loss: 2.605323]\n",
      "epoch:4 step:4298 [D loss: 0.586919, acc: 70.31%] [G loss: 2.807790]\n",
      "epoch:4 step:4299 [D loss: 0.512335, acc: 79.69%] [G loss: 2.709563]\n",
      "epoch:4 step:4300 [D loss: 0.492033, acc: 81.25%] [G loss: 2.929812]\n",
      "epoch:4 step:4301 [D loss: 0.528617, acc: 75.00%] [G loss: 2.958578]\n",
      "epoch:4 step:4302 [D loss: 0.583239, acc: 75.00%] [G loss: 3.126616]\n",
      "epoch:4 step:4303 [D loss: 0.560695, acc: 73.44%] [G loss: 2.797352]\n",
      "epoch:4 step:4304 [D loss: 0.501929, acc: 75.00%] [G loss: 3.273014]\n",
      "epoch:4 step:4305 [D loss: 0.513328, acc: 76.56%] [G loss: 3.066391]\n",
      "epoch:4 step:4306 [D loss: 0.555795, acc: 71.09%] [G loss: 2.895010]\n",
      "epoch:4 step:4307 [D loss: 0.672026, acc: 62.50%] [G loss: 2.746951]\n",
      "epoch:4 step:4308 [D loss: 0.675265, acc: 60.16%] [G loss: 2.417722]\n",
      "epoch:4 step:4309 [D loss: 0.552802, acc: 78.91%] [G loss: 2.870296]\n",
      "epoch:4 step:4310 [D loss: 0.561044, acc: 67.97%] [G loss: 2.696100]\n",
      "epoch:4 step:4311 [D loss: 0.546494, acc: 70.31%] [G loss: 2.660754]\n",
      "epoch:4 step:4312 [D loss: 0.477158, acc: 78.12%] [G loss: 3.026748]\n",
      "epoch:4 step:4313 [D loss: 0.581843, acc: 66.41%] [G loss: 2.976495]\n",
      "epoch:4 step:4314 [D loss: 0.723405, acc: 64.84%] [G loss: 2.576633]\n",
      "epoch:4 step:4315 [D loss: 0.546312, acc: 65.62%] [G loss: 2.542157]\n",
      "epoch:4 step:4316 [D loss: 0.642857, acc: 63.28%] [G loss: 2.678887]\n",
      "epoch:4 step:4317 [D loss: 0.550594, acc: 72.66%] [G loss: 2.698422]\n",
      "epoch:4 step:4318 [D loss: 0.637550, acc: 65.62%] [G loss: 2.949076]\n",
      "epoch:4 step:4319 [D loss: 0.529823, acc: 75.00%] [G loss: 2.603076]\n",
      "epoch:4 step:4320 [D loss: 0.630353, acc: 62.50%] [G loss: 2.659163]\n",
      "epoch:4 step:4321 [D loss: 0.618017, acc: 68.75%] [G loss: 2.492889]\n",
      "epoch:4 step:4322 [D loss: 0.548047, acc: 69.53%] [G loss: 2.990234]\n",
      "epoch:4 step:4323 [D loss: 0.529630, acc: 74.22%] [G loss: 2.666277]\n",
      "epoch:4 step:4324 [D loss: 0.645702, acc: 67.19%] [G loss: 2.602112]\n",
      "epoch:4 step:4325 [D loss: 0.653650, acc: 67.97%] [G loss: 2.815715]\n",
      "epoch:4 step:4326 [D loss: 0.592231, acc: 69.53%] [G loss: 3.057021]\n",
      "epoch:4 step:4327 [D loss: 0.585088, acc: 69.53%] [G loss: 2.797016]\n",
      "epoch:4 step:4328 [D loss: 0.528781, acc: 75.00%] [G loss: 2.748107]\n",
      "epoch:4 step:4329 [D loss: 0.574917, acc: 67.19%] [G loss: 2.761835]\n",
      "epoch:4 step:4330 [D loss: 0.615287, acc: 67.19%] [G loss: 3.008720]\n",
      "epoch:4 step:4331 [D loss: 0.605199, acc: 64.84%] [G loss: 3.015922]\n",
      "epoch:4 step:4332 [D loss: 0.591127, acc: 64.84%] [G loss: 2.632506]\n",
      "epoch:4 step:4333 [D loss: 0.654318, acc: 63.28%] [G loss: 2.888537]\n",
      "epoch:4 step:4334 [D loss: 0.527853, acc: 74.22%] [G loss: 2.878994]\n",
      "epoch:4 step:4335 [D loss: 0.549705, acc: 70.31%] [G loss: 3.282472]\n",
      "epoch:4 step:4336 [D loss: 0.681865, acc: 66.41%] [G loss: 2.642379]\n",
      "epoch:4 step:4337 [D loss: 0.589145, acc: 75.00%] [G loss: 2.855582]\n",
      "epoch:4 step:4338 [D loss: 0.717621, acc: 64.84%] [G loss: 2.567788]\n",
      "epoch:4 step:4339 [D loss: 0.638686, acc: 66.41%] [G loss: 2.586784]\n",
      "epoch:4 step:4340 [D loss: 0.539742, acc: 70.31%] [G loss: 3.081139]\n",
      "epoch:4 step:4341 [D loss: 0.600621, acc: 71.88%] [G loss: 2.486351]\n",
      "epoch:4 step:4342 [D loss: 0.597068, acc: 68.75%] [G loss: 2.622021]\n",
      "epoch:4 step:4343 [D loss: 0.635874, acc: 67.19%] [G loss: 2.581526]\n",
      "epoch:4 step:4344 [D loss: 0.569014, acc: 74.22%] [G loss: 2.630424]\n",
      "epoch:4 step:4345 [D loss: 0.538885, acc: 75.00%] [G loss: 2.606574]\n",
      "epoch:4 step:4346 [D loss: 0.628538, acc: 65.62%] [G loss: 2.957037]\n",
      "epoch:4 step:4347 [D loss: 0.634053, acc: 65.62%] [G loss: 2.607857]\n",
      "epoch:4 step:4348 [D loss: 0.524047, acc: 69.53%] [G loss: 2.461285]\n",
      "epoch:4 step:4349 [D loss: 0.672461, acc: 61.72%] [G loss: 2.618343]\n",
      "epoch:4 step:4350 [D loss: 0.556531, acc: 71.88%] [G loss: 2.730181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4351 [D loss: 0.572161, acc: 72.66%] [G loss: 3.022972]\n",
      "epoch:4 step:4352 [D loss: 0.552371, acc: 71.88%] [G loss: 2.712080]\n",
      "epoch:4 step:4353 [D loss: 0.634558, acc: 67.97%] [G loss: 2.907254]\n",
      "epoch:4 step:4354 [D loss: 0.545912, acc: 75.00%] [G loss: 2.797167]\n",
      "epoch:4 step:4355 [D loss: 0.692341, acc: 60.16%] [G loss: 2.631264]\n",
      "epoch:4 step:4356 [D loss: 0.672972, acc: 60.94%] [G loss: 2.474082]\n",
      "epoch:4 step:4357 [D loss: 0.562815, acc: 74.22%] [G loss: 2.795441]\n",
      "epoch:4 step:4358 [D loss: 0.535673, acc: 76.56%] [G loss: 2.828802]\n",
      "epoch:4 step:4359 [D loss: 0.569732, acc: 75.78%] [G loss: 2.647166]\n",
      "epoch:4 step:4360 [D loss: 0.629636, acc: 64.84%] [G loss: 2.778952]\n",
      "epoch:4 step:4361 [D loss: 0.557828, acc: 73.44%] [G loss: 2.936013]\n",
      "epoch:4 step:4362 [D loss: 0.470464, acc: 76.56%] [G loss: 2.946533]\n",
      "epoch:4 step:4363 [D loss: 0.582560, acc: 70.31%] [G loss: 2.691544]\n",
      "epoch:4 step:4364 [D loss: 0.639291, acc: 67.97%] [G loss: 2.483233]\n",
      "epoch:4 step:4365 [D loss: 0.546754, acc: 75.00%] [G loss: 2.714347]\n",
      "epoch:4 step:4366 [D loss: 0.565757, acc: 71.88%] [G loss: 2.373133]\n",
      "epoch:4 step:4367 [D loss: 0.565709, acc: 67.19%] [G loss: 2.717957]\n",
      "epoch:4 step:4368 [D loss: 0.536011, acc: 75.00%] [G loss: 2.483067]\n",
      "epoch:4 step:4369 [D loss: 0.598700, acc: 68.75%] [G loss: 2.674091]\n",
      "epoch:4 step:4370 [D loss: 0.689519, acc: 63.28%] [G loss: 2.721310]\n",
      "epoch:4 step:4371 [D loss: 0.587668, acc: 66.41%] [G loss: 2.677360]\n",
      "epoch:4 step:4372 [D loss: 0.539492, acc: 71.88%] [G loss: 2.825726]\n",
      "epoch:4 step:4373 [D loss: 0.616113, acc: 70.31%] [G loss: 2.475115]\n",
      "epoch:4 step:4374 [D loss: 0.609701, acc: 71.88%] [G loss: 2.770532]\n",
      "epoch:4 step:4375 [D loss: 0.599255, acc: 67.19%] [G loss: 2.519931]\n",
      "epoch:4 step:4376 [D loss: 0.491785, acc: 80.47%] [G loss: 2.608191]\n",
      "epoch:4 step:4377 [D loss: 0.577772, acc: 75.78%] [G loss: 2.779116]\n",
      "epoch:4 step:4378 [D loss: 0.605289, acc: 67.97%] [G loss: 2.747468]\n",
      "epoch:4 step:4379 [D loss: 0.499037, acc: 75.78%] [G loss: 2.983978]\n",
      "epoch:4 step:4380 [D loss: 0.546744, acc: 73.44%] [G loss: 2.961176]\n",
      "epoch:4 step:4381 [D loss: 0.505459, acc: 71.88%] [G loss: 3.091035]\n",
      "epoch:4 step:4382 [D loss: 0.620556, acc: 65.62%] [G loss: 2.931144]\n",
      "epoch:4 step:4383 [D loss: 0.593672, acc: 71.88%] [G loss: 2.845951]\n",
      "epoch:4 step:4384 [D loss: 0.642343, acc: 67.97%] [G loss: 2.560266]\n",
      "epoch:4 step:4385 [D loss: 0.567558, acc: 71.88%] [G loss: 2.484367]\n",
      "epoch:4 step:4386 [D loss: 0.504518, acc: 75.78%] [G loss: 2.693146]\n",
      "epoch:4 step:4387 [D loss: 0.544992, acc: 74.22%] [G loss: 3.090033]\n",
      "epoch:4 step:4388 [D loss: 0.466631, acc: 79.69%] [G loss: 3.266768]\n",
      "epoch:4 step:4389 [D loss: 0.517499, acc: 78.12%] [G loss: 3.036347]\n",
      "epoch:4 step:4390 [D loss: 0.470664, acc: 75.78%] [G loss: 3.146054]\n",
      "epoch:4 step:4391 [D loss: 0.642854, acc: 60.16%] [G loss: 2.966233]\n",
      "epoch:4 step:4392 [D loss: 0.503608, acc: 76.56%] [G loss: 3.026125]\n",
      "epoch:4 step:4393 [D loss: 0.543893, acc: 70.31%] [G loss: 2.945048]\n",
      "epoch:4 step:4394 [D loss: 0.556195, acc: 69.53%] [G loss: 2.670398]\n",
      "epoch:4 step:4395 [D loss: 0.497958, acc: 76.56%] [G loss: 3.380164]\n",
      "epoch:4 step:4396 [D loss: 0.489961, acc: 78.91%] [G loss: 3.209423]\n",
      "epoch:4 step:4397 [D loss: 0.434034, acc: 83.59%] [G loss: 3.156146]\n",
      "epoch:4 step:4398 [D loss: 0.522513, acc: 69.53%] [G loss: 3.237325]\n",
      "epoch:4 step:4399 [D loss: 0.599068, acc: 70.31%] [G loss: 2.603506]\n",
      "epoch:4 step:4400 [D loss: 0.621929, acc: 64.06%] [G loss: 2.606015]\n",
      "epoch:4 step:4401 [D loss: 0.585720, acc: 64.84%] [G loss: 2.924566]\n",
      "epoch:4 step:4402 [D loss: 0.525945, acc: 71.09%] [G loss: 3.211818]\n",
      "epoch:4 step:4403 [D loss: 0.551733, acc: 75.78%] [G loss: 2.755571]\n",
      "epoch:4 step:4404 [D loss: 0.600208, acc: 65.62%] [G loss: 3.011565]\n",
      "epoch:4 step:4405 [D loss: 0.576334, acc: 71.88%] [G loss: 2.713913]\n",
      "epoch:4 step:4406 [D loss: 0.646098, acc: 66.41%] [G loss: 2.565850]\n",
      "epoch:4 step:4407 [D loss: 0.580008, acc: 70.31%] [G loss: 2.880354]\n",
      "epoch:4 step:4408 [D loss: 0.542876, acc: 70.31%] [G loss: 3.052451]\n",
      "epoch:4 step:4409 [D loss: 0.652544, acc: 66.41%] [G loss: 2.727918]\n",
      "epoch:4 step:4410 [D loss: 0.554426, acc: 73.44%] [G loss: 2.742772]\n",
      "epoch:4 step:4411 [D loss: 0.544898, acc: 71.09%] [G loss: 3.043216]\n",
      "epoch:4 step:4412 [D loss: 0.528673, acc: 73.44%] [G loss: 3.119079]\n",
      "epoch:4 step:4413 [D loss: 0.638016, acc: 65.62%] [G loss: 2.922971]\n",
      "epoch:4 step:4414 [D loss: 0.630629, acc: 63.28%] [G loss: 2.801461]\n",
      "epoch:4 step:4415 [D loss: 0.552881, acc: 71.88%] [G loss: 2.534827]\n",
      "epoch:4 step:4416 [D loss: 0.604310, acc: 66.41%] [G loss: 2.917588]\n",
      "epoch:4 step:4417 [D loss: 0.451463, acc: 83.59%] [G loss: 2.600342]\n",
      "epoch:4 step:4418 [D loss: 0.619953, acc: 66.41%] [G loss: 2.529963]\n",
      "epoch:4 step:4419 [D loss: 0.537968, acc: 74.22%] [G loss: 2.642219]\n",
      "epoch:4 step:4420 [D loss: 0.700166, acc: 62.50%] [G loss: 2.735351]\n",
      "epoch:4 step:4421 [D loss: 0.624246, acc: 65.62%] [G loss: 2.822777]\n",
      "epoch:4 step:4422 [D loss: 0.689397, acc: 62.50%] [G loss: 2.556214]\n",
      "epoch:4 step:4423 [D loss: 0.590358, acc: 71.09%] [G loss: 2.808299]\n",
      "epoch:4 step:4424 [D loss: 0.590498, acc: 71.09%] [G loss: 2.632729]\n",
      "epoch:4 step:4425 [D loss: 0.500036, acc: 76.56%] [G loss: 3.006872]\n",
      "epoch:4 step:4426 [D loss: 0.509701, acc: 73.44%] [G loss: 3.144804]\n",
      "epoch:4 step:4427 [D loss: 0.608922, acc: 63.28%] [G loss: 2.616298]\n",
      "epoch:4 step:4428 [D loss: 0.537906, acc: 71.88%] [G loss: 2.793432]\n",
      "epoch:4 step:4429 [D loss: 0.525513, acc: 75.00%] [G loss: 3.100539]\n",
      "epoch:4 step:4430 [D loss: 0.627264, acc: 67.19%] [G loss: 2.487269]\n",
      "epoch:4 step:4431 [D loss: 0.628413, acc: 63.28%] [G loss: 2.652225]\n",
      "epoch:4 step:4432 [D loss: 0.601280, acc: 69.53%] [G loss: 2.610010]\n",
      "epoch:4 step:4433 [D loss: 0.598822, acc: 71.88%] [G loss: 2.636915]\n",
      "epoch:4 step:4434 [D loss: 0.587024, acc: 69.53%] [G loss: 2.966083]\n",
      "epoch:4 step:4435 [D loss: 0.582793, acc: 67.19%] [G loss: 2.710232]\n",
      "epoch:4 step:4436 [D loss: 0.631903, acc: 66.41%] [G loss: 2.598280]\n",
      "epoch:4 step:4437 [D loss: 0.593658, acc: 68.75%] [G loss: 2.610827]\n",
      "epoch:4 step:4438 [D loss: 0.545533, acc: 72.66%] [G loss: 2.973554]\n",
      "epoch:4 step:4439 [D loss: 0.580551, acc: 67.97%] [G loss: 2.682171]\n",
      "epoch:4 step:4440 [D loss: 0.540176, acc: 70.31%] [G loss: 2.855361]\n",
      "epoch:4 step:4441 [D loss: 0.610355, acc: 71.09%] [G loss: 2.701498]\n",
      "epoch:4 step:4442 [D loss: 0.509495, acc: 75.00%] [G loss: 2.790153]\n",
      "epoch:4 step:4443 [D loss: 0.511554, acc: 75.00%] [G loss: 3.028738]\n",
      "epoch:4 step:4444 [D loss: 0.597723, acc: 64.06%] [G loss: 2.429763]\n",
      "epoch:4 step:4445 [D loss: 0.524962, acc: 73.44%] [G loss: 2.656577]\n",
      "epoch:4 step:4446 [D loss: 0.711171, acc: 60.16%] [G loss: 2.624976]\n",
      "epoch:4 step:4447 [D loss: 0.634962, acc: 67.97%] [G loss: 2.802106]\n",
      "epoch:4 step:4448 [D loss: 0.676553, acc: 60.16%] [G loss: 2.589156]\n",
      "epoch:4 step:4449 [D loss: 0.583295, acc: 64.06%] [G loss: 2.987420]\n",
      "epoch:4 step:4450 [D loss: 0.612700, acc: 66.41%] [G loss: 2.310573]\n",
      "epoch:4 step:4451 [D loss: 0.640640, acc: 59.38%] [G loss: 2.453419]\n",
      "epoch:4 step:4452 [D loss: 0.531145, acc: 78.91%] [G loss: 2.597167]\n",
      "epoch:4 step:4453 [D loss: 0.599759, acc: 66.41%] [G loss: 2.569414]\n",
      "epoch:4 step:4454 [D loss: 0.555717, acc: 75.78%] [G loss: 2.774855]\n",
      "epoch:4 step:4455 [D loss: 0.565547, acc: 71.88%] [G loss: 2.877926]\n",
      "epoch:4 step:4456 [D loss: 0.487123, acc: 78.12%] [G loss: 2.827385]\n",
      "epoch:4 step:4457 [D loss: 0.574955, acc: 67.97%] [G loss: 3.270106]\n",
      "epoch:4 step:4458 [D loss: 0.606109, acc: 66.41%] [G loss: 2.581858]\n",
      "epoch:4 step:4459 [D loss: 0.613705, acc: 67.19%] [G loss: 2.537126]\n",
      "epoch:4 step:4460 [D loss: 0.566445, acc: 68.75%] [G loss: 2.803849]\n",
      "epoch:4 step:4461 [D loss: 0.548399, acc: 71.88%] [G loss: 2.683179]\n",
      "epoch:4 step:4462 [D loss: 0.487436, acc: 80.47%] [G loss: 3.037441]\n",
      "epoch:4 step:4463 [D loss: 0.609071, acc: 69.53%] [G loss: 2.569551]\n",
      "epoch:4 step:4464 [D loss: 0.617699, acc: 70.31%] [G loss: 2.542408]\n",
      "epoch:4 step:4465 [D loss: 0.586245, acc: 69.53%] [G loss: 2.674547]\n",
      "epoch:4 step:4466 [D loss: 0.617311, acc: 64.84%] [G loss: 2.697535]\n",
      "epoch:4 step:4467 [D loss: 0.655048, acc: 60.16%] [G loss: 2.761418]\n",
      "epoch:4 step:4468 [D loss: 0.626037, acc: 64.06%] [G loss: 2.922338]\n",
      "epoch:4 step:4469 [D loss: 0.525570, acc: 75.78%] [G loss: 2.670818]\n",
      "epoch:4 step:4470 [D loss: 0.573511, acc: 65.62%] [G loss: 2.900490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4471 [D loss: 0.610522, acc: 67.19%] [G loss: 2.637326]\n",
      "epoch:4 step:4472 [D loss: 0.526971, acc: 72.66%] [G loss: 2.926981]\n",
      "epoch:4 step:4473 [D loss: 0.606083, acc: 70.31%] [G loss: 3.050681]\n",
      "epoch:4 step:4474 [D loss: 0.620260, acc: 63.28%] [G loss: 2.606334]\n",
      "epoch:4 step:4475 [D loss: 0.609306, acc: 66.41%] [G loss: 2.705059]\n",
      "epoch:4 step:4476 [D loss: 0.620399, acc: 67.19%] [G loss: 2.863345]\n",
      "epoch:4 step:4477 [D loss: 0.533958, acc: 74.22%] [G loss: 2.760650]\n",
      "epoch:4 step:4478 [D loss: 0.551335, acc: 71.09%] [G loss: 2.495773]\n",
      "epoch:4 step:4479 [D loss: 0.524881, acc: 74.22%] [G loss: 2.753060]\n",
      "epoch:4 step:4480 [D loss: 0.602573, acc: 68.75%] [G loss: 2.931626]\n",
      "epoch:4 step:4481 [D loss: 0.469383, acc: 78.91%] [G loss: 2.808149]\n",
      "epoch:4 step:4482 [D loss: 0.657308, acc: 68.75%] [G loss: 2.652347]\n",
      "epoch:4 step:4483 [D loss: 0.588854, acc: 71.88%] [G loss: 2.309827]\n",
      "epoch:4 step:4484 [D loss: 0.503677, acc: 78.91%] [G loss: 2.846845]\n",
      "epoch:4 step:4485 [D loss: 0.571795, acc: 71.09%] [G loss: 2.521222]\n",
      "epoch:4 step:4486 [D loss: 0.602145, acc: 70.31%] [G loss: 2.692090]\n",
      "epoch:4 step:4487 [D loss: 0.660242, acc: 64.84%] [G loss: 2.345934]\n",
      "epoch:4 step:4488 [D loss: 0.592825, acc: 70.31%] [G loss: 2.469756]\n",
      "epoch:4 step:4489 [D loss: 0.610448, acc: 67.97%] [G loss: 2.519140]\n",
      "epoch:4 step:4490 [D loss: 0.581268, acc: 69.53%] [G loss: 2.420216]\n",
      "epoch:4 step:4491 [D loss: 0.612149, acc: 65.62%] [G loss: 2.778746]\n",
      "epoch:4 step:4492 [D loss: 0.626755, acc: 66.41%] [G loss: 2.763759]\n",
      "epoch:4 step:4493 [D loss: 0.590432, acc: 71.09%] [G loss: 2.527219]\n",
      "epoch:4 step:4494 [D loss: 0.556271, acc: 76.56%] [G loss: 2.876176]\n",
      "epoch:4 step:4495 [D loss: 0.558154, acc: 69.53%] [G loss: 3.031173]\n",
      "epoch:4 step:4496 [D loss: 0.558026, acc: 69.53%] [G loss: 2.626390]\n",
      "epoch:4 step:4497 [D loss: 0.583230, acc: 71.09%] [G loss: 2.575488]\n",
      "epoch:4 step:4498 [D loss: 0.526289, acc: 78.91%] [G loss: 2.681597]\n",
      "epoch:4 step:4499 [D loss: 0.578219, acc: 71.88%] [G loss: 2.546284]\n",
      "epoch:4 step:4500 [D loss: 0.605254, acc: 71.88%] [G loss: 2.895032]\n",
      "epoch:4 step:4501 [D loss: 0.568220, acc: 71.09%] [G loss: 2.790887]\n",
      "epoch:4 step:4502 [D loss: 0.586220, acc: 66.41%] [G loss: 2.473078]\n",
      "epoch:4 step:4503 [D loss: 0.580764, acc: 69.53%] [G loss: 2.705193]\n",
      "epoch:4 step:4504 [D loss: 0.690987, acc: 60.16%] [G loss: 2.569273]\n",
      "epoch:4 step:4505 [D loss: 0.560143, acc: 69.53%] [G loss: 2.535658]\n",
      "epoch:4 step:4506 [D loss: 0.653036, acc: 63.28%] [G loss: 2.535246]\n",
      "epoch:4 step:4507 [D loss: 0.588804, acc: 62.50%] [G loss: 2.706360]\n",
      "epoch:4 step:4508 [D loss: 0.600096, acc: 63.28%] [G loss: 2.641828]\n",
      "epoch:4 step:4509 [D loss: 0.550725, acc: 66.41%] [G loss: 2.587806]\n",
      "epoch:4 step:4510 [D loss: 0.582397, acc: 71.09%] [G loss: 2.477535]\n",
      "epoch:4 step:4511 [D loss: 0.578395, acc: 67.19%] [G loss: 2.944289]\n",
      "epoch:4 step:4512 [D loss: 0.739596, acc: 57.03%] [G loss: 2.588203]\n",
      "epoch:4 step:4513 [D loss: 0.578964, acc: 69.53%] [G loss: 2.542704]\n",
      "epoch:4 step:4514 [D loss: 0.686300, acc: 67.97%] [G loss: 2.555740]\n",
      "epoch:4 step:4515 [D loss: 0.592107, acc: 66.41%] [G loss: 2.572009]\n",
      "epoch:4 step:4516 [D loss: 0.619935, acc: 67.97%] [G loss: 2.418212]\n",
      "epoch:4 step:4517 [D loss: 0.554132, acc: 71.09%] [G loss: 2.810780]\n",
      "epoch:4 step:4518 [D loss: 0.662343, acc: 59.38%] [G loss: 2.331581]\n",
      "epoch:4 step:4519 [D loss: 0.635198, acc: 63.28%] [G loss: 2.530300]\n",
      "epoch:4 step:4520 [D loss: 0.552743, acc: 72.66%] [G loss: 2.679615]\n",
      "epoch:4 step:4521 [D loss: 0.591170, acc: 68.75%] [G loss: 2.743654]\n",
      "epoch:4 step:4522 [D loss: 0.580633, acc: 71.09%] [G loss: 2.734443]\n",
      "epoch:4 step:4523 [D loss: 0.563576, acc: 71.09%] [G loss: 2.848074]\n",
      "epoch:4 step:4524 [D loss: 0.626724, acc: 64.84%] [G loss: 2.706603]\n",
      "epoch:4 step:4525 [D loss: 0.543319, acc: 75.00%] [G loss: 2.624153]\n",
      "epoch:4 step:4526 [D loss: 0.559905, acc: 69.53%] [G loss: 2.688229]\n",
      "epoch:4 step:4527 [D loss: 0.500614, acc: 76.56%] [G loss: 2.923426]\n",
      "epoch:4 step:4528 [D loss: 0.578826, acc: 70.31%] [G loss: 2.908900]\n",
      "epoch:4 step:4529 [D loss: 0.458773, acc: 79.69%] [G loss: 3.223116]\n",
      "epoch:4 step:4530 [D loss: 0.557089, acc: 68.75%] [G loss: 2.788095]\n",
      "epoch:4 step:4531 [D loss: 0.562708, acc: 71.88%] [G loss: 2.808781]\n",
      "epoch:4 step:4532 [D loss: 0.593860, acc: 71.09%] [G loss: 2.538124]\n",
      "epoch:4 step:4533 [D loss: 0.631458, acc: 69.53%] [G loss: 2.668307]\n",
      "epoch:4 step:4534 [D loss: 0.567491, acc: 71.09%] [G loss: 2.675130]\n",
      "epoch:4 step:4535 [D loss: 0.670981, acc: 61.72%] [G loss: 2.593801]\n",
      "epoch:4 step:4536 [D loss: 0.673803, acc: 70.31%] [G loss: 2.176313]\n",
      "epoch:4 step:4537 [D loss: 0.587248, acc: 67.97%] [G loss: 2.614560]\n",
      "epoch:4 step:4538 [D loss: 0.601978, acc: 67.19%] [G loss: 2.735500]\n",
      "epoch:4 step:4539 [D loss: 0.520345, acc: 76.56%] [G loss: 2.921386]\n",
      "epoch:4 step:4540 [D loss: 0.499709, acc: 74.22%] [G loss: 2.993409]\n",
      "epoch:4 step:4541 [D loss: 0.589245, acc: 66.41%] [G loss: 2.459511]\n",
      "epoch:4 step:4542 [D loss: 0.606341, acc: 69.53%] [G loss: 2.407921]\n",
      "epoch:4 step:4543 [D loss: 0.546636, acc: 80.47%] [G loss: 2.732509]\n",
      "epoch:4 step:4544 [D loss: 0.588217, acc: 74.22%] [G loss: 2.504852]\n",
      "epoch:4 step:4545 [D loss: 0.719403, acc: 59.38%] [G loss: 2.564596]\n",
      "epoch:4 step:4546 [D loss: 0.525271, acc: 75.00%] [G loss: 2.654450]\n",
      "epoch:4 step:4547 [D loss: 0.596002, acc: 70.31%] [G loss: 2.375734]\n",
      "epoch:4 step:4548 [D loss: 0.594236, acc: 67.19%] [G loss: 2.599877]\n",
      "epoch:4 step:4549 [D loss: 0.585866, acc: 68.75%] [G loss: 2.591290]\n",
      "epoch:4 step:4550 [D loss: 0.572264, acc: 71.88%] [G loss: 2.943601]\n",
      "epoch:4 step:4551 [D loss: 0.538086, acc: 71.09%] [G loss: 2.689854]\n",
      "epoch:4 step:4552 [D loss: 0.498305, acc: 71.88%] [G loss: 3.069187]\n",
      "epoch:4 step:4553 [D loss: 0.580093, acc: 69.53%] [G loss: 3.266428]\n",
      "epoch:4 step:4554 [D loss: 0.513028, acc: 74.22%] [G loss: 2.890246]\n",
      "epoch:4 step:4555 [D loss: 0.484098, acc: 75.78%] [G loss: 2.662975]\n",
      "epoch:4 step:4556 [D loss: 0.552001, acc: 71.88%] [G loss: 2.821972]\n",
      "epoch:4 step:4557 [D loss: 0.532817, acc: 71.88%] [G loss: 2.932430]\n",
      "epoch:4 step:4558 [D loss: 0.622561, acc: 65.62%] [G loss: 3.027366]\n",
      "epoch:4 step:4559 [D loss: 0.575035, acc: 75.00%] [G loss: 2.719684]\n",
      "epoch:4 step:4560 [D loss: 0.653026, acc: 63.28%] [G loss: 2.426827]\n",
      "epoch:4 step:4561 [D loss: 0.637871, acc: 65.62%] [G loss: 2.797897]\n",
      "epoch:4 step:4562 [D loss: 0.584921, acc: 66.41%] [G loss: 2.797154]\n",
      "epoch:4 step:4563 [D loss: 0.592101, acc: 70.31%] [G loss: 2.995447]\n",
      "epoch:4 step:4564 [D loss: 0.520872, acc: 79.69%] [G loss: 3.082004]\n",
      "epoch:4 step:4565 [D loss: 0.591912, acc: 64.06%] [G loss: 2.967957]\n",
      "epoch:4 step:4566 [D loss: 0.714549, acc: 59.38%] [G loss: 2.619174]\n",
      "epoch:4 step:4567 [D loss: 0.621631, acc: 68.75%] [G loss: 2.862056]\n",
      "epoch:4 step:4568 [D loss: 0.625664, acc: 67.19%] [G loss: 2.616598]\n",
      "epoch:4 step:4569 [D loss: 0.488041, acc: 78.12%] [G loss: 3.083012]\n",
      "epoch:4 step:4570 [D loss: 0.477768, acc: 81.25%] [G loss: 2.880754]\n",
      "epoch:4 step:4571 [D loss: 0.531147, acc: 69.53%] [G loss: 2.815651]\n",
      "epoch:4 step:4572 [D loss: 0.578750, acc: 68.75%] [G loss: 2.765311]\n",
      "epoch:4 step:4573 [D loss: 0.510738, acc: 71.88%] [G loss: 3.022155]\n",
      "epoch:4 step:4574 [D loss: 0.603227, acc: 65.62%] [G loss: 2.441094]\n",
      "epoch:4 step:4575 [D loss: 0.687499, acc: 58.59%] [G loss: 2.454766]\n",
      "epoch:4 step:4576 [D loss: 0.637767, acc: 63.28%] [G loss: 2.369610]\n",
      "epoch:4 step:4577 [D loss: 0.598890, acc: 66.41%] [G loss: 2.480881]\n",
      "epoch:4 step:4578 [D loss: 0.635119, acc: 64.84%] [G loss: 2.675187]\n",
      "epoch:4 step:4579 [D loss: 0.545755, acc: 71.88%] [G loss: 2.789730]\n",
      "epoch:4 step:4580 [D loss: 0.581124, acc: 72.66%] [G loss: 2.571328]\n",
      "epoch:4 step:4581 [D loss: 0.697000, acc: 66.41%] [G loss: 2.726568]\n",
      "epoch:4 step:4582 [D loss: 0.514540, acc: 74.22%] [G loss: 2.733541]\n",
      "epoch:4 step:4583 [D loss: 0.535153, acc: 71.88%] [G loss: 2.630928]\n",
      "epoch:4 step:4584 [D loss: 0.633142, acc: 67.97%] [G loss: 2.650060]\n",
      "epoch:4 step:4585 [D loss: 0.555083, acc: 66.41%] [G loss: 2.870236]\n",
      "epoch:4 step:4586 [D loss: 0.628041, acc: 60.94%] [G loss: 2.691840]\n",
      "epoch:4 step:4587 [D loss: 0.602339, acc: 69.53%] [G loss: 3.005227]\n",
      "epoch:4 step:4588 [D loss: 0.575249, acc: 70.31%] [G loss: 2.644976]\n",
      "epoch:4 step:4589 [D loss: 0.528624, acc: 75.78%] [G loss: 2.882586]\n",
      "epoch:4 step:4590 [D loss: 0.565521, acc: 71.09%] [G loss: 2.765814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:4591 [D loss: 0.632856, acc: 63.28%] [G loss: 2.716605]\n",
      "epoch:4 step:4592 [D loss: 0.633632, acc: 62.50%] [G loss: 2.481390]\n",
      "epoch:4 step:4593 [D loss: 0.573785, acc: 66.41%] [G loss: 2.395370]\n",
      "epoch:4 step:4594 [D loss: 0.588694, acc: 70.31%] [G loss: 2.539923]\n",
      "epoch:4 step:4595 [D loss: 0.592709, acc: 69.53%] [G loss: 2.618668]\n",
      "epoch:4 step:4596 [D loss: 0.540897, acc: 73.44%] [G loss: 2.690026]\n",
      "epoch:4 step:4597 [D loss: 0.571842, acc: 65.62%] [G loss: 2.630443]\n",
      "epoch:4 step:4598 [D loss: 0.631083, acc: 65.62%] [G loss: 2.584378]\n",
      "epoch:4 step:4599 [D loss: 0.616239, acc: 66.41%] [G loss: 2.415517]\n",
      "epoch:4 step:4600 [D loss: 0.691376, acc: 60.94%] [G loss: 2.691694]\n",
      "epoch:4 step:4601 [D loss: 0.556042, acc: 74.22%] [G loss: 2.672535]\n",
      "epoch:4 step:4602 [D loss: 0.497233, acc: 78.12%] [G loss: 2.822400]\n",
      "epoch:4 step:4603 [D loss: 0.598023, acc: 65.62%] [G loss: 2.882752]\n",
      "epoch:4 step:4604 [D loss: 0.568819, acc: 74.22%] [G loss: 2.575072]\n",
      "epoch:4 step:4605 [D loss: 0.596124, acc: 67.97%] [G loss: 2.634424]\n",
      "epoch:4 step:4606 [D loss: 0.715891, acc: 61.72%] [G loss: 2.571821]\n",
      "epoch:4 step:4607 [D loss: 0.716602, acc: 58.59%] [G loss: 2.509730]\n",
      "epoch:4 step:4608 [D loss: 0.583093, acc: 69.53%] [G loss: 2.721080]\n",
      "epoch:4 step:4609 [D loss: 0.655964, acc: 64.84%] [G loss: 2.444850]\n",
      "epoch:4 step:4610 [D loss: 0.585796, acc: 68.75%] [G loss: 2.399747]\n",
      "epoch:4 step:4611 [D loss: 0.589607, acc: 72.66%] [G loss: 2.841937]\n",
      "epoch:4 step:4612 [D loss: 0.585598, acc: 64.84%] [G loss: 2.501055]\n",
      "epoch:4 step:4613 [D loss: 0.559108, acc: 69.53%] [G loss: 2.638688]\n",
      "epoch:4 step:4614 [D loss: 0.622598, acc: 65.62%] [G loss: 2.725209]\n",
      "epoch:4 step:4615 [D loss: 0.644847, acc: 60.94%] [G loss: 2.228567]\n",
      "epoch:4 step:4616 [D loss: 0.596679, acc: 65.62%] [G loss: 2.487251]\n",
      "epoch:4 step:4617 [D loss: 0.570220, acc: 70.31%] [G loss: 2.469665]\n",
      "epoch:4 step:4618 [D loss: 0.671659, acc: 64.06%] [G loss: 2.397171]\n",
      "epoch:4 step:4619 [D loss: 0.519299, acc: 78.12%] [G loss: 2.718298]\n",
      "epoch:4 step:4620 [D loss: 0.603078, acc: 65.62%] [G loss: 2.440124]\n",
      "epoch:4 step:4621 [D loss: 0.682965, acc: 63.28%] [G loss: 2.181056]\n",
      "epoch:4 step:4622 [D loss: 0.578003, acc: 72.66%] [G loss: 2.516785]\n",
      "epoch:4 step:4623 [D loss: 0.520473, acc: 74.22%] [G loss: 2.752858]\n",
      "epoch:4 step:4624 [D loss: 0.570431, acc: 71.09%] [G loss: 2.765390]\n",
      "epoch:4 step:4625 [D loss: 0.554093, acc: 72.66%] [G loss: 2.604129]\n",
      "epoch:4 step:4626 [D loss: 0.550652, acc: 70.31%] [G loss: 2.576240]\n",
      "epoch:4 step:4627 [D loss: 0.618428, acc: 64.84%] [G loss: 2.488971]\n",
      "epoch:4 step:4628 [D loss: 0.599746, acc: 71.09%] [G loss: 2.582144]\n",
      "epoch:4 step:4629 [D loss: 0.595905, acc: 69.53%] [G loss: 2.430090]\n",
      "epoch:4 step:4630 [D loss: 0.612777, acc: 62.50%] [G loss: 2.675375]\n",
      "epoch:4 step:4631 [D loss: 0.631889, acc: 67.19%] [G loss: 2.455044]\n",
      "epoch:4 step:4632 [D loss: 0.516035, acc: 75.78%] [G loss: 2.965272]\n",
      "epoch:4 step:4633 [D loss: 0.583167, acc: 67.97%] [G loss: 2.768488]\n",
      "epoch:4 step:4634 [D loss: 0.529077, acc: 73.44%] [G loss: 3.285081]\n",
      "epoch:4 step:4635 [D loss: 0.541893, acc: 75.78%] [G loss: 2.735683]\n",
      "epoch:4 step:4636 [D loss: 0.577768, acc: 67.19%] [G loss: 2.700152]\n",
      "epoch:4 step:4637 [D loss: 0.550777, acc: 71.88%] [G loss: 2.767834]\n",
      "epoch:4 step:4638 [D loss: 0.508945, acc: 71.09%] [G loss: 2.789726]\n",
      "epoch:4 step:4639 [D loss: 0.653808, acc: 69.53%] [G loss: 2.807500]\n",
      "epoch:4 step:4640 [D loss: 0.585543, acc: 71.09%] [G loss: 2.663551]\n",
      "epoch:4 step:4641 [D loss: 0.598641, acc: 67.97%] [G loss: 2.452020]\n",
      "epoch:4 step:4642 [D loss: 0.551260, acc: 71.88%] [G loss: 2.867278]\n",
      "epoch:4 step:4643 [D loss: 0.592642, acc: 67.97%] [G loss: 2.722468]\n",
      "epoch:4 step:4644 [D loss: 0.661278, acc: 68.75%] [G loss: 2.399133]\n",
      "epoch:4 step:4645 [D loss: 0.562969, acc: 69.53%] [G loss: 2.562019]\n",
      "epoch:4 step:4646 [D loss: 0.494356, acc: 79.69%] [G loss: 2.925788]\n",
      "epoch:4 step:4647 [D loss: 0.563110, acc: 66.41%] [G loss: 2.970212]\n",
      "epoch:4 step:4648 [D loss: 0.559933, acc: 80.47%] [G loss: 2.897904]\n",
      "epoch:4 step:4649 [D loss: 0.564405, acc: 70.31%] [G loss: 3.211787]\n",
      "epoch:4 step:4650 [D loss: 0.563096, acc: 67.97%] [G loss: 2.696562]\n",
      "epoch:4 step:4651 [D loss: 0.572967, acc: 71.09%] [G loss: 2.554419]\n",
      "epoch:4 step:4652 [D loss: 0.601439, acc: 66.41%] [G loss: 2.993409]\n",
      "epoch:4 step:4653 [D loss: 0.522938, acc: 75.78%] [G loss: 2.801820]\n",
      "epoch:4 step:4654 [D loss: 0.608771, acc: 66.41%] [G loss: 2.699362]\n",
      "epoch:4 step:4655 [D loss: 0.544637, acc: 71.88%] [G loss: 2.898652]\n",
      "epoch:4 step:4656 [D loss: 0.631900, acc: 67.97%] [G loss: 2.771911]\n",
      "epoch:4 step:4657 [D loss: 0.503561, acc: 75.78%] [G loss: 2.554343]\n",
      "epoch:4 step:4658 [D loss: 0.570122, acc: 68.75%] [G loss: 2.779361]\n",
      "epoch:4 step:4659 [D loss: 0.489963, acc: 73.44%] [G loss: 2.780275]\n",
      "epoch:4 step:4660 [D loss: 0.483880, acc: 78.12%] [G loss: 3.044510]\n",
      "epoch:4 step:4661 [D loss: 0.636000, acc: 64.84%] [G loss: 2.533596]\n",
      "epoch:4 step:4662 [D loss: 0.556564, acc: 67.19%] [G loss: 2.802686]\n",
      "epoch:4 step:4663 [D loss: 0.599328, acc: 68.75%] [G loss: 2.749465]\n",
      "epoch:4 step:4664 [D loss: 0.549258, acc: 72.66%] [G loss: 2.761551]\n",
      "epoch:4 step:4665 [D loss: 0.573116, acc: 69.53%] [G loss: 2.680379]\n",
      "epoch:4 step:4666 [D loss: 0.503818, acc: 79.69%] [G loss: 3.019172]\n",
      "epoch:4 step:4667 [D loss: 0.517655, acc: 71.88%] [G loss: 2.968212]\n",
      "epoch:4 step:4668 [D loss: 0.589165, acc: 70.31%] [G loss: 2.780667]\n",
      "epoch:4 step:4669 [D loss: 0.519561, acc: 75.00%] [G loss: 3.236804]\n",
      "epoch:4 step:4670 [D loss: 0.705736, acc: 55.47%] [G loss: 2.705866]\n",
      "epoch:4 step:4671 [D loss: 0.550236, acc: 71.88%] [G loss: 3.391190]\n",
      "epoch:4 step:4672 [D loss: 0.514403, acc: 77.34%] [G loss: 3.174117]\n",
      "epoch:4 step:4673 [D loss: 0.529890, acc: 74.22%] [G loss: 3.742064]\n",
      "epoch:4 step:4674 [D loss: 0.501817, acc: 76.56%] [G loss: 3.094905]\n",
      "epoch:4 step:4675 [D loss: 0.547515, acc: 67.19%] [G loss: 3.063788]\n",
      "epoch:4 step:4676 [D loss: 0.814469, acc: 57.81%] [G loss: 2.482425]\n",
      "epoch:4 step:4677 [D loss: 0.684891, acc: 63.28%] [G loss: 2.835288]\n",
      "epoch:4 step:4678 [D loss: 0.569058, acc: 72.66%] [G loss: 3.034580]\n",
      "epoch:4 step:4679 [D loss: 0.532156, acc: 75.78%] [G loss: 2.709758]\n",
      "epoch:4 step:4680 [D loss: 0.589922, acc: 67.19%] [G loss: 2.467604]\n",
      "epoch:4 step:4681 [D loss: 0.510718, acc: 75.00%] [G loss: 2.987165]\n",
      "epoch:4 step:4682 [D loss: 0.515850, acc: 75.00%] [G loss: 2.923912]\n",
      "epoch:4 step:4683 [D loss: 0.545558, acc: 72.66%] [G loss: 2.951135]\n",
      "epoch:4 step:4684 [D loss: 0.562597, acc: 73.44%] [G loss: 2.856737]\n",
      "epoch:4 step:4685 [D loss: 0.473969, acc: 75.00%] [G loss: 3.733873]\n",
      "epoch:5 step:4686 [D loss: 0.542186, acc: 77.34%] [G loss: 2.944928]\n",
      "epoch:5 step:4687 [D loss: 0.611929, acc: 69.53%] [G loss: 2.875057]\n",
      "epoch:5 step:4688 [D loss: 0.583710, acc: 67.19%] [G loss: 2.629395]\n",
      "epoch:5 step:4689 [D loss: 0.520440, acc: 72.66%] [G loss: 2.620979]\n",
      "epoch:5 step:4690 [D loss: 0.597134, acc: 71.09%] [G loss: 2.618614]\n",
      "epoch:5 step:4691 [D loss: 0.550430, acc: 74.22%] [G loss: 2.952312]\n",
      "epoch:5 step:4692 [D loss: 0.509719, acc: 75.00%] [G loss: 3.243926]\n",
      "epoch:5 step:4693 [D loss: 0.555617, acc: 70.31%] [G loss: 3.402397]\n",
      "epoch:5 step:4694 [D loss: 0.618255, acc: 64.84%] [G loss: 2.668365]\n",
      "epoch:5 step:4695 [D loss: 0.534292, acc: 73.44%] [G loss: 3.021947]\n",
      "epoch:5 step:4696 [D loss: 0.705665, acc: 59.38%] [G loss: 2.775877]\n",
      "epoch:5 step:4697 [D loss: 0.571457, acc: 71.09%] [G loss: 2.751087]\n",
      "epoch:5 step:4698 [D loss: 0.587287, acc: 71.88%] [G loss: 3.060453]\n",
      "epoch:5 step:4699 [D loss: 0.696765, acc: 67.19%] [G loss: 2.874698]\n",
      "epoch:5 step:4700 [D loss: 0.540639, acc: 75.00%] [G loss: 2.762171]\n",
      "epoch:5 step:4701 [D loss: 0.518992, acc: 75.00%] [G loss: 3.042321]\n",
      "epoch:5 step:4702 [D loss: 0.537321, acc: 74.22%] [G loss: 2.728431]\n",
      "epoch:5 step:4703 [D loss: 0.643780, acc: 67.19%] [G loss: 2.482407]\n",
      "epoch:5 step:4704 [D loss: 0.596466, acc: 67.97%] [G loss: 2.487998]\n",
      "epoch:5 step:4705 [D loss: 0.746955, acc: 57.81%] [G loss: 2.088723]\n",
      "epoch:5 step:4706 [D loss: 0.590850, acc: 67.97%] [G loss: 2.309469]\n",
      "epoch:5 step:4707 [D loss: 0.568246, acc: 71.88%] [G loss: 2.498446]\n",
      "epoch:5 step:4708 [D loss: 0.452279, acc: 76.56%] [G loss: 2.660107]\n",
      "epoch:5 step:4709 [D loss: 0.553604, acc: 70.31%] [G loss: 3.128062]\n",
      "epoch:5 step:4710 [D loss: 0.558399, acc: 69.53%] [G loss: 2.938985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4711 [D loss: 0.566184, acc: 68.75%] [G loss: 2.513268]\n",
      "epoch:5 step:4712 [D loss: 0.605677, acc: 64.84%] [G loss: 2.745872]\n",
      "epoch:5 step:4713 [D loss: 0.569225, acc: 68.75%] [G loss: 2.645252]\n",
      "epoch:5 step:4714 [D loss: 0.477972, acc: 79.69%] [G loss: 2.887701]\n",
      "epoch:5 step:4715 [D loss: 0.548306, acc: 73.44%] [G loss: 2.392587]\n",
      "epoch:5 step:4716 [D loss: 0.600964, acc: 65.62%] [G loss: 2.769810]\n",
      "epoch:5 step:4717 [D loss: 0.662798, acc: 64.84%] [G loss: 2.379864]\n",
      "epoch:5 step:4718 [D loss: 0.606597, acc: 68.75%] [G loss: 2.523983]\n",
      "epoch:5 step:4719 [D loss: 0.564913, acc: 67.19%] [G loss: 2.951079]\n",
      "epoch:5 step:4720 [D loss: 0.573638, acc: 67.97%] [G loss: 2.564912]\n",
      "epoch:5 step:4721 [D loss: 0.552143, acc: 69.53%] [G loss: 2.732462]\n",
      "epoch:5 step:4722 [D loss: 0.536042, acc: 72.66%] [G loss: 2.666157]\n",
      "epoch:5 step:4723 [D loss: 0.576584, acc: 69.53%] [G loss: 2.843845]\n",
      "epoch:5 step:4724 [D loss: 0.531606, acc: 71.88%] [G loss: 3.069591]\n",
      "epoch:5 step:4725 [D loss: 0.575384, acc: 73.44%] [G loss: 3.490822]\n",
      "epoch:5 step:4726 [D loss: 0.596242, acc: 71.88%] [G loss: 2.917860]\n",
      "epoch:5 step:4727 [D loss: 0.534141, acc: 70.31%] [G loss: 3.231047]\n",
      "epoch:5 step:4728 [D loss: 0.492847, acc: 75.00%] [G loss: 3.098334]\n",
      "epoch:5 step:4729 [D loss: 0.579193, acc: 67.97%] [G loss: 2.559300]\n",
      "epoch:5 step:4730 [D loss: 0.682840, acc: 59.38%] [G loss: 2.759109]\n",
      "epoch:5 step:4731 [D loss: 0.603700, acc: 67.97%] [G loss: 2.689070]\n",
      "epoch:5 step:4732 [D loss: 0.541502, acc: 71.88%] [G loss: 2.978106]\n",
      "epoch:5 step:4733 [D loss: 0.625716, acc: 63.28%] [G loss: 2.748589]\n",
      "epoch:5 step:4734 [D loss: 0.554627, acc: 72.66%] [G loss: 2.683844]\n",
      "epoch:5 step:4735 [D loss: 0.476532, acc: 78.12%] [G loss: 3.006307]\n",
      "epoch:5 step:4736 [D loss: 0.535237, acc: 74.22%] [G loss: 2.783856]\n",
      "epoch:5 step:4737 [D loss: 0.588058, acc: 65.62%] [G loss: 2.612772]\n",
      "epoch:5 step:4738 [D loss: 0.579638, acc: 72.66%] [G loss: 2.907346]\n",
      "epoch:5 step:4739 [D loss: 0.573676, acc: 70.31%] [G loss: 3.229942]\n",
      "epoch:5 step:4740 [D loss: 0.568710, acc: 68.75%] [G loss: 3.044899]\n",
      "epoch:5 step:4741 [D loss: 0.632466, acc: 65.62%] [G loss: 2.607493]\n",
      "epoch:5 step:4742 [D loss: 0.552985, acc: 73.44%] [G loss: 2.642949]\n",
      "epoch:5 step:4743 [D loss: 0.549783, acc: 74.22%] [G loss: 2.777267]\n",
      "epoch:5 step:4744 [D loss: 0.565320, acc: 68.75%] [G loss: 3.020219]\n",
      "epoch:5 step:4745 [D loss: 0.546658, acc: 72.66%] [G loss: 2.877574]\n",
      "epoch:5 step:4746 [D loss: 0.544189, acc: 71.09%] [G loss: 2.977044]\n",
      "epoch:5 step:4747 [D loss: 0.571081, acc: 72.66%] [G loss: 2.774109]\n",
      "epoch:5 step:4748 [D loss: 0.571294, acc: 69.53%] [G loss: 2.832882]\n",
      "epoch:5 step:4749 [D loss: 0.559605, acc: 70.31%] [G loss: 2.842972]\n",
      "epoch:5 step:4750 [D loss: 0.540560, acc: 75.00%] [G loss: 2.885134]\n",
      "epoch:5 step:4751 [D loss: 0.547792, acc: 70.31%] [G loss: 2.937239]\n",
      "epoch:5 step:4752 [D loss: 0.613801, acc: 67.19%] [G loss: 2.817551]\n",
      "epoch:5 step:4753 [D loss: 0.603070, acc: 69.53%] [G loss: 2.401135]\n",
      "epoch:5 step:4754 [D loss: 0.560011, acc: 71.88%] [G loss: 2.962377]\n",
      "epoch:5 step:4755 [D loss: 0.569911, acc: 72.66%] [G loss: 2.747106]\n",
      "epoch:5 step:4756 [D loss: 0.558425, acc: 67.19%] [G loss: 2.859380]\n",
      "epoch:5 step:4757 [D loss: 0.515067, acc: 77.34%] [G loss: 3.161315]\n",
      "epoch:5 step:4758 [D loss: 0.623625, acc: 67.19%] [G loss: 2.700438]\n",
      "epoch:5 step:4759 [D loss: 0.596611, acc: 67.97%] [G loss: 2.922391]\n",
      "epoch:5 step:4760 [D loss: 0.575952, acc: 69.53%] [G loss: 2.613357]\n",
      "epoch:5 step:4761 [D loss: 0.549492, acc: 71.09%] [G loss: 3.085590]\n",
      "epoch:5 step:4762 [D loss: 0.443979, acc: 82.03%] [G loss: 3.287428]\n",
      "epoch:5 step:4763 [D loss: 0.637055, acc: 69.53%] [G loss: 2.681696]\n",
      "epoch:5 step:4764 [D loss: 0.610717, acc: 71.88%] [G loss: 2.595190]\n",
      "epoch:5 step:4765 [D loss: 0.638727, acc: 64.84%] [G loss: 2.779634]\n",
      "epoch:5 step:4766 [D loss: 0.604822, acc: 69.53%] [G loss: 2.693188]\n",
      "epoch:5 step:4767 [D loss: 0.572219, acc: 71.09%] [G loss: 2.783843]\n",
      "epoch:5 step:4768 [D loss: 0.513653, acc: 75.78%] [G loss: 2.914180]\n",
      "epoch:5 step:4769 [D loss: 0.634524, acc: 68.75%] [G loss: 2.611551]\n",
      "epoch:5 step:4770 [D loss: 0.709224, acc: 63.28%] [G loss: 2.521444]\n",
      "epoch:5 step:4771 [D loss: 0.601549, acc: 71.09%] [G loss: 2.396849]\n",
      "epoch:5 step:4772 [D loss: 0.558601, acc: 72.66%] [G loss: 2.727496]\n",
      "epoch:5 step:4773 [D loss: 0.631363, acc: 67.19%] [G loss: 2.308101]\n",
      "epoch:5 step:4774 [D loss: 0.569872, acc: 71.88%] [G loss: 3.018474]\n",
      "epoch:5 step:4775 [D loss: 0.515776, acc: 66.41%] [G loss: 2.351614]\n",
      "epoch:5 step:4776 [D loss: 0.636111, acc: 59.38%] [G loss: 2.844641]\n",
      "epoch:5 step:4777 [D loss: 0.621393, acc: 67.97%] [G loss: 2.642433]\n",
      "epoch:5 step:4778 [D loss: 0.553481, acc: 72.66%] [G loss: 2.712093]\n",
      "epoch:5 step:4779 [D loss: 0.608774, acc: 65.62%] [G loss: 2.534836]\n",
      "epoch:5 step:4780 [D loss: 0.637564, acc: 67.19%] [G loss: 2.665204]\n",
      "epoch:5 step:4781 [D loss: 0.520881, acc: 78.91%] [G loss: 2.882618]\n",
      "epoch:5 step:4782 [D loss: 0.648229, acc: 64.84%] [G loss: 2.602551]\n",
      "epoch:5 step:4783 [D loss: 0.591973, acc: 70.31%] [G loss: 2.586905]\n",
      "epoch:5 step:4784 [D loss: 0.583673, acc: 71.88%] [G loss: 2.498153]\n",
      "epoch:5 step:4785 [D loss: 0.600850, acc: 67.97%] [G loss: 2.690195]\n",
      "epoch:5 step:4786 [D loss: 0.646835, acc: 69.53%] [G loss: 2.633259]\n",
      "epoch:5 step:4787 [D loss: 0.714898, acc: 60.94%] [G loss: 2.537672]\n",
      "epoch:5 step:4788 [D loss: 0.522243, acc: 78.12%] [G loss: 3.285155]\n",
      "epoch:5 step:4789 [D loss: 0.530201, acc: 74.22%] [G loss: 2.464647]\n",
      "epoch:5 step:4790 [D loss: 0.644120, acc: 64.84%] [G loss: 2.532948]\n",
      "epoch:5 step:4791 [D loss: 0.638234, acc: 62.50%] [G loss: 2.492728]\n",
      "epoch:5 step:4792 [D loss: 0.598036, acc: 64.06%] [G loss: 2.713293]\n",
      "epoch:5 step:4793 [D loss: 0.681546, acc: 59.38%] [G loss: 2.434591]\n",
      "epoch:5 step:4794 [D loss: 0.704400, acc: 63.28%] [G loss: 2.199949]\n",
      "epoch:5 step:4795 [D loss: 0.602364, acc: 62.50%] [G loss: 2.341087]\n",
      "epoch:5 step:4796 [D loss: 0.539233, acc: 72.66%] [G loss: 2.717353]\n",
      "epoch:5 step:4797 [D loss: 0.635818, acc: 66.41%] [G loss: 2.636016]\n",
      "epoch:5 step:4798 [D loss: 0.567327, acc: 71.88%] [G loss: 2.469590]\n",
      "epoch:5 step:4799 [D loss: 0.590170, acc: 71.09%] [G loss: 2.523072]\n",
      "epoch:5 step:4800 [D loss: 0.631732, acc: 64.06%] [G loss: 2.598118]\n",
      "epoch:5 step:4801 [D loss: 0.583288, acc: 68.75%] [G loss: 2.377004]\n",
      "epoch:5 step:4802 [D loss: 0.585810, acc: 69.53%] [G loss: 2.836777]\n",
      "epoch:5 step:4803 [D loss: 0.540770, acc: 76.56%] [G loss: 2.969677]\n",
      "epoch:5 step:4804 [D loss: 0.501847, acc: 74.22%] [G loss: 2.889245]\n",
      "epoch:5 step:4805 [D loss: 0.604173, acc: 71.09%] [G loss: 2.691902]\n",
      "epoch:5 step:4806 [D loss: 0.662291, acc: 66.41%] [G loss: 2.661061]\n",
      "epoch:5 step:4807 [D loss: 0.556010, acc: 72.66%] [G loss: 2.938583]\n",
      "epoch:5 step:4808 [D loss: 0.616980, acc: 67.97%] [G loss: 2.726822]\n",
      "epoch:5 step:4809 [D loss: 0.600464, acc: 70.31%] [G loss: 2.508421]\n",
      "epoch:5 step:4810 [D loss: 0.658799, acc: 63.28%] [G loss: 2.435514]\n",
      "epoch:5 step:4811 [D loss: 0.637629, acc: 62.50%] [G loss: 2.685037]\n",
      "epoch:5 step:4812 [D loss: 0.608559, acc: 71.09%] [G loss: 2.338162]\n",
      "epoch:5 step:4813 [D loss: 0.623345, acc: 64.84%] [G loss: 2.653795]\n",
      "epoch:5 step:4814 [D loss: 0.637709, acc: 66.41%] [G loss: 2.476081]\n",
      "epoch:5 step:4815 [D loss: 0.477818, acc: 78.12%] [G loss: 3.063076]\n",
      "epoch:5 step:4816 [D loss: 0.568318, acc: 67.97%] [G loss: 2.972921]\n",
      "epoch:5 step:4817 [D loss: 0.572640, acc: 71.88%] [G loss: 2.820653]\n",
      "epoch:5 step:4818 [D loss: 0.597947, acc: 68.75%] [G loss: 2.346537]\n",
      "epoch:5 step:4819 [D loss: 0.614872, acc: 70.31%] [G loss: 2.403672]\n",
      "epoch:5 step:4820 [D loss: 0.554695, acc: 71.09%] [G loss: 2.432039]\n",
      "epoch:5 step:4821 [D loss: 0.619106, acc: 65.62%] [G loss: 2.564026]\n",
      "epoch:5 step:4822 [D loss: 0.633575, acc: 70.31%] [G loss: 2.411941]\n",
      "epoch:5 step:4823 [D loss: 0.573885, acc: 67.19%] [G loss: 2.615999]\n",
      "epoch:5 step:4824 [D loss: 0.597557, acc: 64.84%] [G loss: 2.539999]\n",
      "epoch:5 step:4825 [D loss: 0.597429, acc: 68.75%] [G loss: 2.443931]\n",
      "epoch:5 step:4826 [D loss: 0.565664, acc: 71.88%] [G loss: 2.797310]\n",
      "epoch:5 step:4827 [D loss: 0.530657, acc: 73.44%] [G loss: 2.444910]\n",
      "epoch:5 step:4828 [D loss: 0.649728, acc: 67.19%] [G loss: 2.480414]\n",
      "epoch:5 step:4829 [D loss: 0.507001, acc: 75.00%] [G loss: 2.607075]\n",
      "epoch:5 step:4830 [D loss: 0.558182, acc: 71.09%] [G loss: 2.878677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4831 [D loss: 0.626246, acc: 64.84%] [G loss: 2.442459]\n",
      "epoch:5 step:4832 [D loss: 0.658584, acc: 66.41%] [G loss: 2.462972]\n",
      "epoch:5 step:4833 [D loss: 0.643122, acc: 68.75%] [G loss: 2.426534]\n",
      "epoch:5 step:4834 [D loss: 0.590400, acc: 72.66%] [G loss: 2.970063]\n",
      "epoch:5 step:4835 [D loss: 0.695150, acc: 61.72%] [G loss: 2.590400]\n",
      "epoch:5 step:4836 [D loss: 0.547435, acc: 70.31%] [G loss: 2.860296]\n",
      "epoch:5 step:4837 [D loss: 0.632100, acc: 62.50%] [G loss: 2.885682]\n",
      "epoch:5 step:4838 [D loss: 0.550421, acc: 75.00%] [G loss: 2.439227]\n",
      "epoch:5 step:4839 [D loss: 0.536166, acc: 72.66%] [G loss: 2.927869]\n",
      "epoch:5 step:4840 [D loss: 0.551682, acc: 71.09%] [G loss: 2.907860]\n",
      "epoch:5 step:4841 [D loss: 0.499992, acc: 72.66%] [G loss: 2.823318]\n",
      "epoch:5 step:4842 [D loss: 0.601697, acc: 67.97%] [G loss: 2.595892]\n",
      "epoch:5 step:4843 [D loss: 0.592172, acc: 71.09%] [G loss: 2.457777]\n",
      "epoch:5 step:4844 [D loss: 0.553066, acc: 68.75%] [G loss: 2.752070]\n",
      "epoch:5 step:4845 [D loss: 0.671585, acc: 60.94%] [G loss: 2.338813]\n",
      "epoch:5 step:4846 [D loss: 0.574192, acc: 71.09%] [G loss: 2.181293]\n",
      "epoch:5 step:4847 [D loss: 0.603831, acc: 70.31%] [G loss: 2.700817]\n",
      "epoch:5 step:4848 [D loss: 0.563760, acc: 71.88%] [G loss: 2.560032]\n",
      "epoch:5 step:4849 [D loss: 0.532598, acc: 75.78%] [G loss: 2.477974]\n",
      "epoch:5 step:4850 [D loss: 0.511684, acc: 75.00%] [G loss: 2.768000]\n",
      "epoch:5 step:4851 [D loss: 0.594166, acc: 65.62%] [G loss: 2.684193]\n",
      "epoch:5 step:4852 [D loss: 0.644075, acc: 64.06%] [G loss: 2.726336]\n",
      "epoch:5 step:4853 [D loss: 0.552084, acc: 71.09%] [G loss: 2.715160]\n",
      "epoch:5 step:4854 [D loss: 0.588870, acc: 69.53%] [G loss: 2.614098]\n",
      "epoch:5 step:4855 [D loss: 0.539814, acc: 69.53%] [G loss: 2.479879]\n",
      "epoch:5 step:4856 [D loss: 0.511965, acc: 75.00%] [G loss: 2.865776]\n",
      "epoch:5 step:4857 [D loss: 0.570096, acc: 72.66%] [G loss: 2.608604]\n",
      "epoch:5 step:4858 [D loss: 0.645669, acc: 67.19%] [G loss: 2.398356]\n",
      "epoch:5 step:4859 [D loss: 0.564118, acc: 69.53%] [G loss: 2.535418]\n",
      "epoch:5 step:4860 [D loss: 0.554977, acc: 71.09%] [G loss: 2.370100]\n",
      "epoch:5 step:4861 [D loss: 0.571395, acc: 64.06%] [G loss: 2.506303]\n",
      "epoch:5 step:4862 [D loss: 0.580298, acc: 74.22%] [G loss: 2.333169]\n",
      "epoch:5 step:4863 [D loss: 0.611933, acc: 64.84%] [G loss: 2.494289]\n",
      "epoch:5 step:4864 [D loss: 0.601754, acc: 67.97%] [G loss: 2.378876]\n",
      "epoch:5 step:4865 [D loss: 0.516850, acc: 75.00%] [G loss: 2.629438]\n",
      "epoch:5 step:4866 [D loss: 0.617006, acc: 67.19%] [G loss: 2.597144]\n",
      "epoch:5 step:4867 [D loss: 0.558703, acc: 69.53%] [G loss: 2.798361]\n",
      "epoch:5 step:4868 [D loss: 0.565389, acc: 74.22%] [G loss: 2.580869]\n",
      "epoch:5 step:4869 [D loss: 0.595060, acc: 69.53%] [G loss: 2.834101]\n",
      "epoch:5 step:4870 [D loss: 0.594424, acc: 69.53%] [G loss: 2.547146]\n",
      "epoch:5 step:4871 [D loss: 0.644456, acc: 60.94%] [G loss: 2.505302]\n",
      "epoch:5 step:4872 [D loss: 0.534348, acc: 72.66%] [G loss: 2.508057]\n",
      "epoch:5 step:4873 [D loss: 0.631857, acc: 67.19%] [G loss: 2.528595]\n",
      "epoch:5 step:4874 [D loss: 0.600191, acc: 67.19%] [G loss: 2.569977]\n",
      "epoch:5 step:4875 [D loss: 0.598725, acc: 74.22%] [G loss: 2.786082]\n",
      "epoch:5 step:4876 [D loss: 0.519275, acc: 71.88%] [G loss: 2.667075]\n",
      "epoch:5 step:4877 [D loss: 0.558752, acc: 72.66%] [G loss: 3.136097]\n",
      "epoch:5 step:4878 [D loss: 0.532030, acc: 72.66%] [G loss: 3.152648]\n",
      "epoch:5 step:4879 [D loss: 0.503543, acc: 78.12%] [G loss: 3.021471]\n",
      "epoch:5 step:4880 [D loss: 0.563014, acc: 77.34%] [G loss: 2.819909]\n",
      "epoch:5 step:4881 [D loss: 0.646298, acc: 70.31%] [G loss: 2.799580]\n",
      "epoch:5 step:4882 [D loss: 0.586294, acc: 67.19%] [G loss: 2.826137]\n",
      "epoch:5 step:4883 [D loss: 0.502732, acc: 74.22%] [G loss: 3.137581]\n",
      "epoch:5 step:4884 [D loss: 0.611968, acc: 67.97%] [G loss: 2.803524]\n",
      "epoch:5 step:4885 [D loss: 0.650905, acc: 63.28%] [G loss: 2.430542]\n",
      "epoch:5 step:4886 [D loss: 0.529002, acc: 70.31%] [G loss: 2.652484]\n",
      "epoch:5 step:4887 [D loss: 0.570752, acc: 71.09%] [G loss: 2.594866]\n",
      "epoch:5 step:4888 [D loss: 0.626495, acc: 64.06%] [G loss: 2.539246]\n",
      "epoch:5 step:4889 [D loss: 0.628954, acc: 66.41%] [G loss: 2.700163]\n",
      "epoch:5 step:4890 [D loss: 0.654022, acc: 59.38%] [G loss: 2.458638]\n",
      "epoch:5 step:4891 [D loss: 0.633448, acc: 65.62%] [G loss: 2.903776]\n",
      "epoch:5 step:4892 [D loss: 0.463951, acc: 78.91%] [G loss: 2.671407]\n",
      "epoch:5 step:4893 [D loss: 0.503492, acc: 75.78%] [G loss: 2.991282]\n",
      "epoch:5 step:4894 [D loss: 0.574825, acc: 71.09%] [G loss: 2.900614]\n",
      "epoch:5 step:4895 [D loss: 0.607198, acc: 67.97%] [G loss: 2.629331]\n",
      "epoch:5 step:4896 [D loss: 0.633750, acc: 68.75%] [G loss: 2.424104]\n",
      "epoch:5 step:4897 [D loss: 0.620254, acc: 67.97%] [G loss: 2.552517]\n",
      "epoch:5 step:4898 [D loss: 0.554317, acc: 73.44%] [G loss: 2.519994]\n",
      "epoch:5 step:4899 [D loss: 0.640259, acc: 58.59%] [G loss: 2.697014]\n",
      "epoch:5 step:4900 [D loss: 0.717187, acc: 53.91%] [G loss: 2.503370]\n",
      "epoch:5 step:4901 [D loss: 0.612438, acc: 66.41%] [G loss: 2.408558]\n",
      "epoch:5 step:4902 [D loss: 0.570163, acc: 70.31%] [G loss: 2.687487]\n",
      "epoch:5 step:4903 [D loss: 0.509053, acc: 77.34%] [G loss: 2.830065]\n",
      "epoch:5 step:4904 [D loss: 0.589666, acc: 65.62%] [G loss: 2.774344]\n",
      "epoch:5 step:4905 [D loss: 0.730186, acc: 58.59%] [G loss: 2.323901]\n",
      "epoch:5 step:4906 [D loss: 0.573131, acc: 67.19%] [G loss: 2.532799]\n",
      "epoch:5 step:4907 [D loss: 0.598202, acc: 67.97%] [G loss: 2.521334]\n",
      "epoch:5 step:4908 [D loss: 0.543965, acc: 72.66%] [G loss: 2.649723]\n",
      "epoch:5 step:4909 [D loss: 0.598634, acc: 67.19%] [G loss: 2.682471]\n",
      "epoch:5 step:4910 [D loss: 0.674693, acc: 57.03%] [G loss: 2.402667]\n",
      "epoch:5 step:4911 [D loss: 0.555998, acc: 66.41%] [G loss: 2.460376]\n",
      "epoch:5 step:4912 [D loss: 0.596161, acc: 66.41%] [G loss: 2.441599]\n",
      "epoch:5 step:4913 [D loss: 0.624950, acc: 70.31%] [G loss: 2.218444]\n",
      "epoch:5 step:4914 [D loss: 0.522635, acc: 72.66%] [G loss: 2.462889]\n",
      "epoch:5 step:4915 [D loss: 0.505192, acc: 76.56%] [G loss: 3.374068]\n",
      "epoch:5 step:4916 [D loss: 0.613049, acc: 71.88%] [G loss: 3.028631]\n",
      "epoch:5 step:4917 [D loss: 0.600117, acc: 68.75%] [G loss: 2.789554]\n",
      "epoch:5 step:4918 [D loss: 0.579173, acc: 71.88%] [G loss: 2.689422]\n",
      "epoch:5 step:4919 [D loss: 0.641516, acc: 64.06%] [G loss: 2.571076]\n",
      "epoch:5 step:4920 [D loss: 0.584844, acc: 70.31%] [G loss: 2.630072]\n",
      "epoch:5 step:4921 [D loss: 0.558524, acc: 75.00%] [G loss: 2.700889]\n",
      "epoch:5 step:4922 [D loss: 0.601035, acc: 65.62%] [G loss: 2.674154]\n",
      "epoch:5 step:4923 [D loss: 0.554329, acc: 71.88%] [G loss: 2.675674]\n",
      "epoch:5 step:4924 [D loss: 0.560746, acc: 66.41%] [G loss: 2.497541]\n",
      "epoch:5 step:4925 [D loss: 0.598255, acc: 73.44%] [G loss: 2.578051]\n",
      "epoch:5 step:4926 [D loss: 0.591691, acc: 67.97%] [G loss: 2.583221]\n",
      "epoch:5 step:4927 [D loss: 0.547427, acc: 71.88%] [G loss: 2.894677]\n",
      "epoch:5 step:4928 [D loss: 0.525989, acc: 75.00%] [G loss: 2.939485]\n",
      "epoch:5 step:4929 [D loss: 0.626918, acc: 62.50%] [G loss: 2.675770]\n",
      "epoch:5 step:4930 [D loss: 0.608485, acc: 70.31%] [G loss: 2.848788]\n",
      "epoch:5 step:4931 [D loss: 0.570532, acc: 74.22%] [G loss: 2.475709]\n",
      "epoch:5 step:4932 [D loss: 0.530643, acc: 73.44%] [G loss: 2.654284]\n",
      "epoch:5 step:4933 [D loss: 0.620729, acc: 70.31%] [G loss: 2.613634]\n",
      "epoch:5 step:4934 [D loss: 0.620125, acc: 67.97%] [G loss: 2.359191]\n",
      "epoch:5 step:4935 [D loss: 0.691889, acc: 60.16%] [G loss: 2.624119]\n",
      "epoch:5 step:4936 [D loss: 0.662704, acc: 59.38%] [G loss: 2.375392]\n",
      "epoch:5 step:4937 [D loss: 0.638697, acc: 65.62%] [G loss: 2.478573]\n",
      "epoch:5 step:4938 [D loss: 0.525461, acc: 76.56%] [G loss: 2.448589]\n",
      "epoch:5 step:4939 [D loss: 0.529583, acc: 71.09%] [G loss: 2.937939]\n",
      "epoch:5 step:4940 [D loss: 0.559090, acc: 71.09%] [G loss: 2.820074]\n",
      "epoch:5 step:4941 [D loss: 0.592634, acc: 73.44%] [G loss: 2.522236]\n",
      "epoch:5 step:4942 [D loss: 0.583654, acc: 67.97%] [G loss: 2.430984]\n",
      "epoch:5 step:4943 [D loss: 0.584903, acc: 67.97%] [G loss: 2.633081]\n",
      "epoch:5 step:4944 [D loss: 0.540835, acc: 74.22%] [G loss: 2.684416]\n",
      "epoch:5 step:4945 [D loss: 0.608120, acc: 64.06%] [G loss: 2.765966]\n",
      "epoch:5 step:4946 [D loss: 0.579122, acc: 71.09%] [G loss: 2.894278]\n",
      "epoch:5 step:4947 [D loss: 0.655568, acc: 67.19%] [G loss: 2.663298]\n",
      "epoch:5 step:4948 [D loss: 0.672181, acc: 63.28%] [G loss: 2.650067]\n",
      "epoch:5 step:4949 [D loss: 0.516736, acc: 74.22%] [G loss: 2.903033]\n",
      "epoch:5 step:4950 [D loss: 0.748600, acc: 60.94%] [G loss: 2.416972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4951 [D loss: 0.548651, acc: 73.44%] [G loss: 2.547564]\n",
      "epoch:5 step:4952 [D loss: 0.581027, acc: 68.75%] [G loss: 2.573756]\n",
      "epoch:5 step:4953 [D loss: 0.620080, acc: 67.97%] [G loss: 2.572171]\n",
      "epoch:5 step:4954 [D loss: 0.549191, acc: 75.00%] [G loss: 2.754858]\n",
      "epoch:5 step:4955 [D loss: 0.609700, acc: 72.66%] [G loss: 2.851860]\n",
      "epoch:5 step:4956 [D loss: 0.531460, acc: 72.66%] [G loss: 2.748487]\n",
      "epoch:5 step:4957 [D loss: 0.566929, acc: 69.53%] [G loss: 3.138063]\n",
      "epoch:5 step:4958 [D loss: 0.663108, acc: 62.50%] [G loss: 3.158252]\n",
      "epoch:5 step:4959 [D loss: 0.594951, acc: 68.75%] [G loss: 2.992927]\n",
      "epoch:5 step:4960 [D loss: 0.679088, acc: 64.06%] [G loss: 2.581275]\n",
      "epoch:5 step:4961 [D loss: 0.532645, acc: 69.53%] [G loss: 2.609375]\n",
      "epoch:5 step:4962 [D loss: 0.573753, acc: 69.53%] [G loss: 2.407017]\n",
      "epoch:5 step:4963 [D loss: 0.733302, acc: 54.69%] [G loss: 2.507058]\n",
      "epoch:5 step:4964 [D loss: 0.511380, acc: 75.00%] [G loss: 2.717006]\n",
      "epoch:5 step:4965 [D loss: 0.546705, acc: 73.44%] [G loss: 2.446538]\n",
      "epoch:5 step:4966 [D loss: 0.654960, acc: 67.19%] [G loss: 2.375756]\n",
      "epoch:5 step:4967 [D loss: 0.602141, acc: 67.19%] [G loss: 2.589959]\n",
      "epoch:5 step:4968 [D loss: 0.510864, acc: 77.34%] [G loss: 2.736748]\n",
      "epoch:5 step:4969 [D loss: 0.602010, acc: 67.19%] [G loss: 2.682210]\n",
      "epoch:5 step:4970 [D loss: 0.602927, acc: 69.53%] [G loss: 2.498425]\n",
      "epoch:5 step:4971 [D loss: 0.488130, acc: 78.91%] [G loss: 2.724004]\n",
      "epoch:5 step:4972 [D loss: 0.613468, acc: 67.19%] [G loss: 2.716227]\n",
      "epoch:5 step:4973 [D loss: 0.589685, acc: 68.75%] [G loss: 2.479404]\n",
      "epoch:5 step:4974 [D loss: 0.580343, acc: 68.75%] [G loss: 2.657747]\n",
      "epoch:5 step:4975 [D loss: 0.545615, acc: 75.00%] [G loss: 2.774489]\n",
      "epoch:5 step:4976 [D loss: 0.514523, acc: 74.22%] [G loss: 2.723979]\n",
      "epoch:5 step:4977 [D loss: 0.615162, acc: 64.84%] [G loss: 2.525153]\n",
      "epoch:5 step:4978 [D loss: 0.609112, acc: 69.53%] [G loss: 2.739541]\n",
      "epoch:5 step:4979 [D loss: 0.602195, acc: 64.06%] [G loss: 2.498088]\n",
      "epoch:5 step:4980 [D loss: 0.498630, acc: 72.66%] [G loss: 2.675511]\n",
      "epoch:5 step:4981 [D loss: 0.504511, acc: 75.78%] [G loss: 2.943479]\n",
      "epoch:5 step:4982 [D loss: 0.545080, acc: 71.09%] [G loss: 2.792417]\n",
      "epoch:5 step:4983 [D loss: 0.609353, acc: 67.19%] [G loss: 3.063227]\n",
      "epoch:5 step:4984 [D loss: 0.561523, acc: 73.44%] [G loss: 2.822091]\n",
      "epoch:5 step:4985 [D loss: 0.562332, acc: 74.22%] [G loss: 2.838202]\n",
      "epoch:5 step:4986 [D loss: 0.652620, acc: 57.03%] [G loss: 2.438197]\n",
      "epoch:5 step:4987 [D loss: 0.609523, acc: 66.41%] [G loss: 2.594399]\n",
      "epoch:5 step:4988 [D loss: 0.580447, acc: 69.53%] [G loss: 2.619606]\n",
      "epoch:5 step:4989 [D loss: 0.544080, acc: 74.22%] [G loss: 2.690675]\n",
      "epoch:5 step:4990 [D loss: 0.553331, acc: 72.66%] [G loss: 2.503914]\n",
      "epoch:5 step:4991 [D loss: 0.621517, acc: 64.06%] [G loss: 2.498029]\n",
      "epoch:5 step:4992 [D loss: 0.634796, acc: 67.19%] [G loss: 2.704933]\n",
      "epoch:5 step:4993 [D loss: 0.602802, acc: 70.31%] [G loss: 2.592032]\n",
      "epoch:5 step:4994 [D loss: 0.568050, acc: 67.97%] [G loss: 2.450902]\n",
      "epoch:5 step:4995 [D loss: 0.556691, acc: 71.09%] [G loss: 2.505663]\n",
      "epoch:5 step:4996 [D loss: 0.626830, acc: 62.50%] [G loss: 2.492446]\n",
      "epoch:5 step:4997 [D loss: 0.478877, acc: 76.56%] [G loss: 3.334808]\n",
      "epoch:5 step:4998 [D loss: 0.596791, acc: 64.84%] [G loss: 3.100504]\n",
      "epoch:5 step:4999 [D loss: 0.536839, acc: 75.00%] [G loss: 3.512335]\n",
      "epoch:5 step:5000 [D loss: 0.535052, acc: 75.78%] [G loss: 3.264170]\n",
      "epoch:5 step:5001 [D loss: 0.638302, acc: 67.97%] [G loss: 2.511318]\n",
      "epoch:5 step:5002 [D loss: 0.729205, acc: 58.59%] [G loss: 2.561771]\n",
      "epoch:5 step:5003 [D loss: 0.582089, acc: 69.53%] [G loss: 2.686295]\n",
      "epoch:5 step:5004 [D loss: 0.578262, acc: 69.53%] [G loss: 2.656255]\n",
      "epoch:5 step:5005 [D loss: 0.596814, acc: 66.41%] [G loss: 2.530185]\n",
      "epoch:5 step:5006 [D loss: 0.529592, acc: 76.56%] [G loss: 2.942675]\n",
      "epoch:5 step:5007 [D loss: 0.550001, acc: 71.88%] [G loss: 2.651114]\n",
      "epoch:5 step:5008 [D loss: 0.610364, acc: 64.84%] [G loss: 2.339903]\n",
      "epoch:5 step:5009 [D loss: 0.629420, acc: 65.62%] [G loss: 2.601789]\n",
      "epoch:5 step:5010 [D loss: 0.720800, acc: 57.81%] [G loss: 2.726976]\n",
      "epoch:5 step:5011 [D loss: 0.601240, acc: 62.50%] [G loss: 2.327412]\n",
      "epoch:5 step:5012 [D loss: 0.602844, acc: 62.50%] [G loss: 2.662015]\n",
      "epoch:5 step:5013 [D loss: 0.676025, acc: 62.50%] [G loss: 2.338710]\n",
      "epoch:5 step:5014 [D loss: 0.567200, acc: 69.53%] [G loss: 2.633460]\n",
      "epoch:5 step:5015 [D loss: 0.561269, acc: 71.09%] [G loss: 2.737999]\n",
      "epoch:5 step:5016 [D loss: 0.545850, acc: 74.22%] [G loss: 2.497902]\n",
      "epoch:5 step:5017 [D loss: 0.508707, acc: 75.78%] [G loss: 2.599122]\n",
      "epoch:5 step:5018 [D loss: 0.639470, acc: 67.19%] [G loss: 2.869719]\n",
      "epoch:5 step:5019 [D loss: 0.648818, acc: 65.62%] [G loss: 2.700619]\n",
      "epoch:5 step:5020 [D loss: 0.552196, acc: 71.09%] [G loss: 2.600887]\n",
      "epoch:5 step:5021 [D loss: 0.606564, acc: 70.31%] [G loss: 2.664534]\n",
      "epoch:5 step:5022 [D loss: 0.608135, acc: 68.75%] [G loss: 2.482339]\n",
      "epoch:5 step:5023 [D loss: 0.561605, acc: 71.09%] [G loss: 2.469046]\n",
      "epoch:5 step:5024 [D loss: 0.535015, acc: 74.22%] [G loss: 2.639478]\n",
      "epoch:5 step:5025 [D loss: 0.581082, acc: 71.09%] [G loss: 2.832714]\n",
      "epoch:5 step:5026 [D loss: 0.664790, acc: 62.50%] [G loss: 2.488836]\n",
      "epoch:5 step:5027 [D loss: 0.668075, acc: 62.50%] [G loss: 2.577393]\n",
      "epoch:5 step:5028 [D loss: 0.624950, acc: 63.28%] [G loss: 2.627053]\n",
      "epoch:5 step:5029 [D loss: 0.518086, acc: 75.00%] [G loss: 2.615694]\n",
      "epoch:5 step:5030 [D loss: 0.473755, acc: 78.12%] [G loss: 2.880712]\n",
      "epoch:5 step:5031 [D loss: 0.533203, acc: 72.66%] [G loss: 3.136253]\n",
      "epoch:5 step:5032 [D loss: 0.534924, acc: 73.44%] [G loss: 2.889376]\n",
      "epoch:5 step:5033 [D loss: 0.662856, acc: 60.94%] [G loss: 2.461879]\n",
      "epoch:5 step:5034 [D loss: 0.652458, acc: 61.72%] [G loss: 2.304220]\n",
      "epoch:5 step:5035 [D loss: 0.568087, acc: 75.00%] [G loss: 2.555675]\n",
      "epoch:5 step:5036 [D loss: 0.569053, acc: 71.09%] [G loss: 2.705656]\n",
      "epoch:5 step:5037 [D loss: 0.637772, acc: 64.06%] [G loss: 2.353093]\n",
      "epoch:5 step:5038 [D loss: 0.585674, acc: 68.75%] [G loss: 2.916694]\n",
      "epoch:5 step:5039 [D loss: 0.556148, acc: 72.66%] [G loss: 2.850906]\n",
      "epoch:5 step:5040 [D loss: 0.557629, acc: 74.22%] [G loss: 2.487073]\n",
      "epoch:5 step:5041 [D loss: 0.645573, acc: 63.28%] [G loss: 2.463922]\n",
      "epoch:5 step:5042 [D loss: 0.603838, acc: 64.84%] [G loss: 2.716977]\n",
      "epoch:5 step:5043 [D loss: 0.514478, acc: 69.53%] [G loss: 2.882044]\n",
      "epoch:5 step:5044 [D loss: 0.542583, acc: 75.00%] [G loss: 2.724210]\n",
      "epoch:5 step:5045 [D loss: 0.501953, acc: 79.69%] [G loss: 2.829460]\n",
      "epoch:5 step:5046 [D loss: 0.582189, acc: 72.66%] [G loss: 2.467678]\n",
      "epoch:5 step:5047 [D loss: 0.576644, acc: 71.09%] [G loss: 2.572025]\n",
      "epoch:5 step:5048 [D loss: 0.529079, acc: 71.88%] [G loss: 2.771297]\n",
      "epoch:5 step:5049 [D loss: 0.541850, acc: 74.22%] [G loss: 2.691650]\n",
      "epoch:5 step:5050 [D loss: 0.523416, acc: 71.88%] [G loss: 2.896557]\n",
      "epoch:5 step:5051 [D loss: 0.563475, acc: 66.41%] [G loss: 2.728553]\n",
      "epoch:5 step:5052 [D loss: 0.551923, acc: 71.88%] [G loss: 2.755092]\n",
      "epoch:5 step:5053 [D loss: 0.614246, acc: 63.28%] [G loss: 2.382320]\n",
      "epoch:5 step:5054 [D loss: 0.494767, acc: 78.91%] [G loss: 3.033518]\n",
      "epoch:5 step:5055 [D loss: 0.682256, acc: 60.16%] [G loss: 2.779073]\n",
      "epoch:5 step:5056 [D loss: 0.527656, acc: 74.22%] [G loss: 2.890326]\n",
      "epoch:5 step:5057 [D loss: 0.554972, acc: 70.31%] [G loss: 2.880938]\n",
      "epoch:5 step:5058 [D loss: 0.707903, acc: 53.12%] [G loss: 2.287877]\n",
      "epoch:5 step:5059 [D loss: 0.542882, acc: 68.75%] [G loss: 2.947692]\n",
      "epoch:5 step:5060 [D loss: 0.621059, acc: 63.28%] [G loss: 2.499309]\n",
      "epoch:5 step:5061 [D loss: 0.641283, acc: 65.62%] [G loss: 2.452956]\n",
      "epoch:5 step:5062 [D loss: 0.615744, acc: 63.28%] [G loss: 2.233398]\n",
      "epoch:5 step:5063 [D loss: 0.617217, acc: 67.97%] [G loss: 3.190951]\n",
      "epoch:5 step:5064 [D loss: 0.558646, acc: 68.75%] [G loss: 2.601224]\n",
      "epoch:5 step:5065 [D loss: 0.581849, acc: 67.19%] [G loss: 3.024813]\n",
      "epoch:5 step:5066 [D loss: 0.585799, acc: 67.19%] [G loss: 3.035800]\n",
      "epoch:5 step:5067 [D loss: 0.691241, acc: 56.25%] [G loss: 2.598215]\n",
      "epoch:5 step:5068 [D loss: 0.570255, acc: 70.31%] [G loss: 2.835952]\n",
      "epoch:5 step:5069 [D loss: 0.589668, acc: 69.53%] [G loss: 2.664876]\n",
      "epoch:5 step:5070 [D loss: 0.565581, acc: 74.22%] [G loss: 2.697486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5071 [D loss: 0.641376, acc: 62.50%] [G loss: 2.529751]\n",
      "epoch:5 step:5072 [D loss: 0.718726, acc: 55.47%] [G loss: 2.286777]\n",
      "epoch:5 step:5073 [D loss: 0.640633, acc: 64.84%] [G loss: 2.221587]\n",
      "epoch:5 step:5074 [D loss: 0.583264, acc: 69.53%] [G loss: 2.466338]\n",
      "epoch:5 step:5075 [D loss: 0.554253, acc: 75.78%] [G loss: 2.476815]\n",
      "epoch:5 step:5076 [D loss: 0.651751, acc: 64.84%] [G loss: 2.427249]\n",
      "epoch:5 step:5077 [D loss: 0.536509, acc: 76.56%] [G loss: 2.764225]\n",
      "epoch:5 step:5078 [D loss: 0.561319, acc: 74.22%] [G loss: 2.387516]\n",
      "epoch:5 step:5079 [D loss: 0.647734, acc: 60.94%] [G loss: 2.713568]\n",
      "epoch:5 step:5080 [D loss: 0.524537, acc: 78.91%] [G loss: 2.965404]\n",
      "epoch:5 step:5081 [D loss: 0.645562, acc: 59.38%] [G loss: 2.319088]\n",
      "epoch:5 step:5082 [D loss: 0.587852, acc: 67.97%] [G loss: 2.482626]\n",
      "epoch:5 step:5083 [D loss: 0.534538, acc: 72.66%] [G loss: 2.725204]\n",
      "epoch:5 step:5084 [D loss: 0.483035, acc: 80.47%] [G loss: 2.603501]\n",
      "epoch:5 step:5085 [D loss: 0.573042, acc: 73.44%] [G loss: 2.490236]\n",
      "epoch:5 step:5086 [D loss: 0.527781, acc: 74.22%] [G loss: 2.438255]\n",
      "epoch:5 step:5087 [D loss: 0.480896, acc: 76.56%] [G loss: 2.959696]\n",
      "epoch:5 step:5088 [D loss: 0.611348, acc: 66.41%] [G loss: 2.646676]\n",
      "epoch:5 step:5089 [D loss: 0.594620, acc: 68.75%] [G loss: 2.558348]\n",
      "epoch:5 step:5090 [D loss: 0.537393, acc: 72.66%] [G loss: 2.940806]\n",
      "epoch:5 step:5091 [D loss: 0.595277, acc: 70.31%] [G loss: 2.811052]\n",
      "epoch:5 step:5092 [D loss: 0.653856, acc: 62.50%] [G loss: 2.251404]\n",
      "epoch:5 step:5093 [D loss: 0.630109, acc: 68.75%] [G loss: 2.814818]\n",
      "epoch:5 step:5094 [D loss: 0.575267, acc: 69.53%] [G loss: 2.521527]\n",
      "epoch:5 step:5095 [D loss: 0.542873, acc: 68.75%] [G loss: 2.667844]\n",
      "epoch:5 step:5096 [D loss: 0.543466, acc: 70.31%] [G loss: 2.830366]\n",
      "epoch:5 step:5097 [D loss: 0.605763, acc: 70.31%] [G loss: 2.562354]\n",
      "epoch:5 step:5098 [D loss: 0.620790, acc: 66.41%] [G loss: 2.630292]\n",
      "epoch:5 step:5099 [D loss: 0.573642, acc: 68.75%] [G loss: 2.635893]\n",
      "epoch:5 step:5100 [D loss: 0.558708, acc: 69.53%] [G loss: 2.822765]\n",
      "epoch:5 step:5101 [D loss: 0.593041, acc: 67.19%] [G loss: 2.514242]\n",
      "epoch:5 step:5102 [D loss: 0.705230, acc: 61.72%] [G loss: 2.383211]\n",
      "epoch:5 step:5103 [D loss: 0.610083, acc: 66.41%] [G loss: 2.320182]\n",
      "epoch:5 step:5104 [D loss: 0.578019, acc: 66.41%] [G loss: 2.604110]\n",
      "epoch:5 step:5105 [D loss: 0.626597, acc: 64.84%] [G loss: 2.607362]\n",
      "epoch:5 step:5106 [D loss: 0.595526, acc: 68.75%] [G loss: 2.528442]\n",
      "epoch:5 step:5107 [D loss: 0.524622, acc: 75.00%] [G loss: 2.714339]\n",
      "epoch:5 step:5108 [D loss: 0.533806, acc: 71.09%] [G loss: 2.625273]\n",
      "epoch:5 step:5109 [D loss: 0.635025, acc: 62.50%] [G loss: 2.492093]\n",
      "epoch:5 step:5110 [D loss: 0.641399, acc: 64.06%] [G loss: 2.382916]\n",
      "epoch:5 step:5111 [D loss: 0.577864, acc: 66.41%] [G loss: 2.892313]\n",
      "epoch:5 step:5112 [D loss: 0.591438, acc: 67.19%] [G loss: 2.986743]\n",
      "epoch:5 step:5113 [D loss: 0.570066, acc: 71.88%] [G loss: 3.134015]\n",
      "epoch:5 step:5114 [D loss: 0.609052, acc: 67.19%] [G loss: 2.799246]\n",
      "epoch:5 step:5115 [D loss: 0.550013, acc: 71.88%] [G loss: 2.780384]\n",
      "epoch:5 step:5116 [D loss: 0.598148, acc: 70.31%] [G loss: 2.427116]\n",
      "epoch:5 step:5117 [D loss: 0.644634, acc: 65.62%] [G loss: 2.288886]\n",
      "epoch:5 step:5118 [D loss: 0.643794, acc: 64.06%] [G loss: 2.651382]\n",
      "epoch:5 step:5119 [D loss: 0.559080, acc: 73.44%] [G loss: 2.544218]\n",
      "epoch:5 step:5120 [D loss: 0.569678, acc: 71.09%] [G loss: 2.526116]\n",
      "epoch:5 step:5121 [D loss: 0.607714, acc: 68.75%] [G loss: 2.637459]\n",
      "epoch:5 step:5122 [D loss: 0.668166, acc: 60.16%] [G loss: 2.385171]\n",
      "epoch:5 step:5123 [D loss: 0.648975, acc: 64.84%] [G loss: 2.644109]\n",
      "epoch:5 step:5124 [D loss: 0.593091, acc: 69.53%] [G loss: 2.705835]\n",
      "epoch:5 step:5125 [D loss: 0.606609, acc: 69.53%] [G loss: 2.624837]\n",
      "epoch:5 step:5126 [D loss: 0.629158, acc: 62.50%] [G loss: 2.412320]\n",
      "epoch:5 step:5127 [D loss: 0.626392, acc: 67.19%] [G loss: 2.389565]\n",
      "epoch:5 step:5128 [D loss: 0.701970, acc: 58.59%] [G loss: 2.375942]\n",
      "epoch:5 step:5129 [D loss: 0.594090, acc: 64.84%] [G loss: 2.403474]\n",
      "epoch:5 step:5130 [D loss: 0.613584, acc: 67.97%] [G loss: 2.447723]\n",
      "epoch:5 step:5131 [D loss: 0.548378, acc: 76.56%] [G loss: 2.358242]\n",
      "epoch:5 step:5132 [D loss: 0.619489, acc: 67.19%] [G loss: 2.605314]\n",
      "epoch:5 step:5133 [D loss: 0.666842, acc: 62.50%] [G loss: 2.524774]\n",
      "epoch:5 step:5134 [D loss: 0.569267, acc: 71.88%] [G loss: 2.698900]\n",
      "epoch:5 step:5135 [D loss: 0.563581, acc: 72.66%] [G loss: 2.530717]\n",
      "epoch:5 step:5136 [D loss: 0.526855, acc: 71.09%] [G loss: 3.094146]\n",
      "epoch:5 step:5137 [D loss: 0.611632, acc: 60.94%] [G loss: 2.921203]\n",
      "epoch:5 step:5138 [D loss: 0.548088, acc: 72.66%] [G loss: 2.628787]\n",
      "epoch:5 step:5139 [D loss: 0.500567, acc: 75.78%] [G loss: 2.411444]\n",
      "epoch:5 step:5140 [D loss: 0.505325, acc: 78.12%] [G loss: 2.869776]\n",
      "epoch:5 step:5141 [D loss: 0.615740, acc: 64.06%] [G loss: 2.901624]\n",
      "epoch:5 step:5142 [D loss: 0.557318, acc: 70.31%] [G loss: 2.806525]\n",
      "epoch:5 step:5143 [D loss: 0.629158, acc: 63.28%] [G loss: 2.443439]\n",
      "epoch:5 step:5144 [D loss: 0.641033, acc: 60.16%] [G loss: 2.504842]\n",
      "epoch:5 step:5145 [D loss: 0.570876, acc: 68.75%] [G loss: 2.683394]\n",
      "epoch:5 step:5146 [D loss: 0.610719, acc: 71.09%] [G loss: 2.246188]\n",
      "epoch:5 step:5147 [D loss: 0.615423, acc: 68.75%] [G loss: 2.507949]\n",
      "epoch:5 step:5148 [D loss: 0.550368, acc: 71.09%] [G loss: 2.549728]\n",
      "epoch:5 step:5149 [D loss: 0.525758, acc: 75.00%] [G loss: 2.716547]\n",
      "epoch:5 step:5150 [D loss: 0.660974, acc: 59.38%] [G loss: 2.468615]\n",
      "epoch:5 step:5151 [D loss: 0.539516, acc: 77.34%] [G loss: 2.860383]\n",
      "epoch:5 step:5152 [D loss: 0.637108, acc: 64.06%] [G loss: 2.615367]\n",
      "epoch:5 step:5153 [D loss: 0.582208, acc: 64.84%] [G loss: 2.896885]\n",
      "epoch:5 step:5154 [D loss: 0.561623, acc: 69.53%] [G loss: 2.803469]\n",
      "epoch:5 step:5155 [D loss: 0.509634, acc: 74.22%] [G loss: 3.087703]\n",
      "epoch:5 step:5156 [D loss: 0.546837, acc: 70.31%] [G loss: 3.372847]\n",
      "epoch:5 step:5157 [D loss: 0.508837, acc: 75.00%] [G loss: 3.345437]\n",
      "epoch:5 step:5158 [D loss: 0.697613, acc: 60.94%] [G loss: 2.465607]\n",
      "epoch:5 step:5159 [D loss: 0.592481, acc: 68.75%] [G loss: 2.878887]\n",
      "epoch:5 step:5160 [D loss: 0.575155, acc: 67.97%] [G loss: 3.106824]\n",
      "epoch:5 step:5161 [D loss: 0.644865, acc: 60.94%] [G loss: 2.864231]\n",
      "epoch:5 step:5162 [D loss: 0.752047, acc: 61.72%] [G loss: 2.075177]\n",
      "epoch:5 step:5163 [D loss: 0.586998, acc: 70.31%] [G loss: 2.627358]\n",
      "epoch:5 step:5164 [D loss: 0.550009, acc: 71.09%] [G loss: 2.632070]\n",
      "epoch:5 step:5165 [D loss: 0.603782, acc: 64.84%] [G loss: 2.517793]\n",
      "epoch:5 step:5166 [D loss: 0.533696, acc: 76.56%] [G loss: 2.777478]\n",
      "epoch:5 step:5167 [D loss: 0.662769, acc: 60.16%] [G loss: 2.275108]\n",
      "epoch:5 step:5168 [D loss: 0.622688, acc: 69.53%] [G loss: 2.286835]\n",
      "epoch:5 step:5169 [D loss: 0.576641, acc: 67.97%] [G loss: 2.612943]\n",
      "epoch:5 step:5170 [D loss: 0.575755, acc: 71.09%] [G loss: 2.349221]\n",
      "epoch:5 step:5171 [D loss: 0.631603, acc: 62.50%] [G loss: 2.505040]\n",
      "epoch:5 step:5172 [D loss: 0.563063, acc: 69.53%] [G loss: 2.756246]\n",
      "epoch:5 step:5173 [D loss: 0.528509, acc: 78.12%] [G loss: 2.663027]\n",
      "epoch:5 step:5174 [D loss: 0.573797, acc: 71.09%] [G loss: 2.484269]\n",
      "epoch:5 step:5175 [D loss: 0.512745, acc: 75.00%] [G loss: 2.894765]\n",
      "epoch:5 step:5176 [D loss: 0.609488, acc: 64.06%] [G loss: 2.755692]\n",
      "epoch:5 step:5177 [D loss: 0.627730, acc: 65.62%] [G loss: 2.680058]\n",
      "epoch:5 step:5178 [D loss: 0.601008, acc: 67.97%] [G loss: 2.664324]\n",
      "epoch:5 step:5179 [D loss: 0.555296, acc: 74.22%] [G loss: 2.732763]\n",
      "epoch:5 step:5180 [D loss: 0.508516, acc: 75.00%] [G loss: 2.629451]\n",
      "epoch:5 step:5181 [D loss: 0.639059, acc: 64.84%] [G loss: 2.573017]\n",
      "epoch:5 step:5182 [D loss: 0.548832, acc: 77.34%] [G loss: 2.749147]\n",
      "epoch:5 step:5183 [D loss: 0.466483, acc: 75.00%] [G loss: 2.863011]\n",
      "epoch:5 step:5184 [D loss: 0.538338, acc: 72.66%] [G loss: 3.025731]\n",
      "epoch:5 step:5185 [D loss: 0.635868, acc: 71.88%] [G loss: 2.308007]\n",
      "epoch:5 step:5186 [D loss: 0.738376, acc: 57.81%] [G loss: 2.117648]\n",
      "epoch:5 step:5187 [D loss: 0.601885, acc: 67.19%] [G loss: 2.180367]\n",
      "epoch:5 step:5188 [D loss: 0.541854, acc: 71.88%] [G loss: 2.724919]\n",
      "epoch:5 step:5189 [D loss: 0.533297, acc: 75.78%] [G loss: 2.842513]\n",
      "epoch:5 step:5190 [D loss: 0.563635, acc: 69.53%] [G loss: 2.245931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5191 [D loss: 0.602871, acc: 66.41%] [G loss: 2.405507]\n",
      "epoch:5 step:5192 [D loss: 0.613502, acc: 66.41%] [G loss: 2.591049]\n",
      "epoch:5 step:5193 [D loss: 0.597876, acc: 65.62%] [G loss: 2.827646]\n",
      "epoch:5 step:5194 [D loss: 0.603980, acc: 68.75%] [G loss: 2.466034]\n",
      "epoch:5 step:5195 [D loss: 0.623447, acc: 65.62%] [G loss: 2.384467]\n",
      "epoch:5 step:5196 [D loss: 0.673550, acc: 61.72%] [G loss: 2.405972]\n",
      "epoch:5 step:5197 [D loss: 0.636873, acc: 65.62%] [G loss: 2.172791]\n",
      "epoch:5 step:5198 [D loss: 0.613088, acc: 68.75%] [G loss: 2.421038]\n",
      "epoch:5 step:5199 [D loss: 0.649636, acc: 69.53%] [G loss: 2.593813]\n",
      "epoch:5 step:5200 [D loss: 0.564712, acc: 72.66%] [G loss: 2.522120]\n",
      "epoch:5 step:5201 [D loss: 0.611601, acc: 71.09%] [G loss: 2.819041]\n",
      "epoch:5 step:5202 [D loss: 0.596827, acc: 66.41%] [G loss: 2.741945]\n",
      "epoch:5 step:5203 [D loss: 0.544526, acc: 71.88%] [G loss: 2.444246]\n",
      "epoch:5 step:5204 [D loss: 0.655531, acc: 63.28%] [G loss: 2.373770]\n",
      "epoch:5 step:5205 [D loss: 0.558428, acc: 73.44%] [G loss: 2.408468]\n",
      "epoch:5 step:5206 [D loss: 0.505809, acc: 77.34%] [G loss: 2.509296]\n",
      "epoch:5 step:5207 [D loss: 0.645215, acc: 63.28%] [G loss: 2.590284]\n",
      "epoch:5 step:5208 [D loss: 0.640239, acc: 62.50%] [G loss: 2.753633]\n",
      "epoch:5 step:5209 [D loss: 0.528395, acc: 78.91%] [G loss: 2.901592]\n",
      "epoch:5 step:5210 [D loss: 0.597188, acc: 66.41%] [G loss: 2.392176]\n",
      "epoch:5 step:5211 [D loss: 0.652813, acc: 64.84%] [G loss: 2.765932]\n",
      "epoch:5 step:5212 [D loss: 0.584945, acc: 67.19%] [G loss: 2.454093]\n",
      "epoch:5 step:5213 [D loss: 0.656611, acc: 60.16%] [G loss: 2.155861]\n",
      "epoch:5 step:5214 [D loss: 0.621335, acc: 67.19%] [G loss: 2.474451]\n",
      "epoch:5 step:5215 [D loss: 0.639501, acc: 63.28%] [G loss: 2.418261]\n",
      "epoch:5 step:5216 [D loss: 0.659140, acc: 66.41%] [G loss: 2.350187]\n",
      "epoch:5 step:5217 [D loss: 0.593793, acc: 67.97%] [G loss: 2.551682]\n",
      "epoch:5 step:5218 [D loss: 0.638418, acc: 64.84%] [G loss: 2.742186]\n",
      "epoch:5 step:5219 [D loss: 0.542692, acc: 74.22%] [G loss: 2.646540]\n",
      "epoch:5 step:5220 [D loss: 0.622244, acc: 66.41%] [G loss: 2.455858]\n",
      "epoch:5 step:5221 [D loss: 0.582743, acc: 75.78%] [G loss: 2.785450]\n",
      "epoch:5 step:5222 [D loss: 0.585072, acc: 72.66%] [G loss: 2.704872]\n",
      "epoch:5 step:5223 [D loss: 0.676059, acc: 67.19%] [G loss: 1.921338]\n",
      "epoch:5 step:5224 [D loss: 0.624824, acc: 70.31%] [G loss: 2.312037]\n",
      "epoch:5 step:5225 [D loss: 0.716436, acc: 55.47%] [G loss: 2.381557]\n",
      "epoch:5 step:5226 [D loss: 0.663638, acc: 57.03%] [G loss: 2.129790]\n",
      "epoch:5 step:5227 [D loss: 0.613074, acc: 62.50%] [G loss: 2.489501]\n",
      "epoch:5 step:5228 [D loss: 0.600795, acc: 66.41%] [G loss: 2.520502]\n",
      "epoch:5 step:5229 [D loss: 0.596087, acc: 68.75%] [G loss: 2.373405]\n",
      "epoch:5 step:5230 [D loss: 0.590401, acc: 67.97%] [G loss: 2.618018]\n",
      "epoch:5 step:5231 [D loss: 0.521957, acc: 76.56%] [G loss: 2.879556]\n",
      "epoch:5 step:5232 [D loss: 0.535998, acc: 73.44%] [G loss: 2.603624]\n",
      "epoch:5 step:5233 [D loss: 0.581123, acc: 67.97%] [G loss: 2.857896]\n",
      "epoch:5 step:5234 [D loss: 0.575338, acc: 71.09%] [G loss: 2.661117]\n",
      "epoch:5 step:5235 [D loss: 0.559766, acc: 71.09%] [G loss: 2.452037]\n",
      "epoch:5 step:5236 [D loss: 0.570713, acc: 68.75%] [G loss: 2.508386]\n",
      "epoch:5 step:5237 [D loss: 0.601958, acc: 67.97%] [G loss: 2.806811]\n",
      "epoch:5 step:5238 [D loss: 0.565515, acc: 66.41%] [G loss: 2.824036]\n",
      "epoch:5 step:5239 [D loss: 0.511383, acc: 80.47%] [G loss: 3.058542]\n",
      "epoch:5 step:5240 [D loss: 0.564029, acc: 75.78%] [G loss: 2.888093]\n",
      "epoch:5 step:5241 [D loss: 0.468261, acc: 79.69%] [G loss: 3.156518]\n",
      "epoch:5 step:5242 [D loss: 0.559209, acc: 69.53%] [G loss: 2.652356]\n",
      "epoch:5 step:5243 [D loss: 0.559697, acc: 71.09%] [G loss: 2.726961]\n",
      "epoch:5 step:5244 [D loss: 0.654416, acc: 64.06%] [G loss: 2.452971]\n",
      "epoch:5 step:5245 [D loss: 0.670666, acc: 60.94%] [G loss: 2.436372]\n",
      "epoch:5 step:5246 [D loss: 0.533685, acc: 71.09%] [G loss: 2.473225]\n",
      "epoch:5 step:5247 [D loss: 0.563058, acc: 70.31%] [G loss: 2.570908]\n",
      "epoch:5 step:5248 [D loss: 0.590861, acc: 70.31%] [G loss: 2.470416]\n",
      "epoch:5 step:5249 [D loss: 0.505832, acc: 78.91%] [G loss: 2.693070]\n",
      "epoch:5 step:5250 [D loss: 0.507488, acc: 76.56%] [G loss: 2.575590]\n",
      "epoch:5 step:5251 [D loss: 0.778224, acc: 54.69%] [G loss: 2.424145]\n",
      "epoch:5 step:5252 [D loss: 0.635279, acc: 67.19%] [G loss: 2.506391]\n",
      "epoch:5 step:5253 [D loss: 0.675054, acc: 64.84%] [G loss: 2.430290]\n",
      "epoch:5 step:5254 [D loss: 0.563853, acc: 71.09%] [G loss: 2.411862]\n",
      "epoch:5 step:5255 [D loss: 0.568643, acc: 64.06%] [G loss: 2.631166]\n",
      "epoch:5 step:5256 [D loss: 0.528581, acc: 71.88%] [G loss: 2.519088]\n",
      "epoch:5 step:5257 [D loss: 0.597706, acc: 66.41%] [G loss: 2.423151]\n",
      "epoch:5 step:5258 [D loss: 0.645935, acc: 60.94%] [G loss: 2.308221]\n",
      "epoch:5 step:5259 [D loss: 0.610823, acc: 67.19%] [G loss: 2.490899]\n",
      "epoch:5 step:5260 [D loss: 0.531016, acc: 78.12%] [G loss: 2.690074]\n",
      "epoch:5 step:5261 [D loss: 0.658780, acc: 59.38%] [G loss: 2.311442]\n",
      "epoch:5 step:5262 [D loss: 0.605105, acc: 64.84%] [G loss: 2.522171]\n",
      "epoch:5 step:5263 [D loss: 0.606671, acc: 63.28%] [G loss: 2.416987]\n",
      "epoch:5 step:5264 [D loss: 0.597971, acc: 64.84%] [G loss: 2.469182]\n",
      "epoch:5 step:5265 [D loss: 0.660051, acc: 60.16%] [G loss: 2.447767]\n",
      "epoch:5 step:5266 [D loss: 0.620167, acc: 65.62%] [G loss: 2.212626]\n",
      "epoch:5 step:5267 [D loss: 0.607741, acc: 71.88%] [G loss: 2.823070]\n",
      "epoch:5 step:5268 [D loss: 0.588260, acc: 67.97%] [G loss: 2.538319]\n",
      "epoch:5 step:5269 [D loss: 0.697361, acc: 65.62%] [G loss: 2.899628]\n",
      "epoch:5 step:5270 [D loss: 0.601160, acc: 69.53%] [G loss: 2.507121]\n",
      "epoch:5 step:5271 [D loss: 0.566347, acc: 70.31%] [G loss: 2.652906]\n",
      "epoch:5 step:5272 [D loss: 0.592421, acc: 69.53%] [G loss: 2.532341]\n",
      "epoch:5 step:5273 [D loss: 0.559408, acc: 70.31%] [G loss: 2.710104]\n",
      "epoch:5 step:5274 [D loss: 0.513768, acc: 82.03%] [G loss: 2.736920]\n",
      "epoch:5 step:5275 [D loss: 0.637872, acc: 66.41%] [G loss: 2.548746]\n",
      "epoch:5 step:5276 [D loss: 0.554183, acc: 74.22%] [G loss: 2.629234]\n",
      "epoch:5 step:5277 [D loss: 0.555439, acc: 71.09%] [G loss: 3.015685]\n",
      "epoch:5 step:5278 [D loss: 0.691230, acc: 56.25%] [G loss: 2.632528]\n",
      "epoch:5 step:5279 [D loss: 0.553954, acc: 67.19%] [G loss: 2.523235]\n",
      "epoch:5 step:5280 [D loss: 0.628343, acc: 66.41%] [G loss: 2.609756]\n",
      "epoch:5 step:5281 [D loss: 0.584825, acc: 71.09%] [G loss: 2.589579]\n",
      "epoch:5 step:5282 [D loss: 0.615752, acc: 65.62%] [G loss: 2.469353]\n",
      "epoch:5 step:5283 [D loss: 0.563658, acc: 75.00%] [G loss: 2.644684]\n",
      "epoch:5 step:5284 [D loss: 0.593934, acc: 69.53%] [G loss: 2.328060]\n",
      "epoch:5 step:5285 [D loss: 0.698545, acc: 63.28%] [G loss: 2.554215]\n",
      "epoch:5 step:5286 [D loss: 0.582939, acc: 72.66%] [G loss: 2.450484]\n",
      "epoch:5 step:5287 [D loss: 0.506362, acc: 76.56%] [G loss: 2.703226]\n",
      "epoch:5 step:5288 [D loss: 0.633413, acc: 64.06%] [G loss: 2.644676]\n",
      "epoch:5 step:5289 [D loss: 0.650055, acc: 60.94%] [G loss: 2.329084]\n",
      "epoch:5 step:5290 [D loss: 0.609685, acc: 64.84%] [G loss: 2.444551]\n",
      "epoch:5 step:5291 [D loss: 0.581302, acc: 70.31%] [G loss: 2.601656]\n",
      "epoch:5 step:5292 [D loss: 0.670129, acc: 61.72%] [G loss: 2.476309]\n",
      "epoch:5 step:5293 [D loss: 0.538371, acc: 73.44%] [G loss: 2.463409]\n",
      "epoch:5 step:5294 [D loss: 0.553713, acc: 71.09%] [G loss: 2.642723]\n",
      "epoch:5 step:5295 [D loss: 0.592870, acc: 66.41%] [G loss: 2.334148]\n",
      "epoch:5 step:5296 [D loss: 0.624648, acc: 66.41%] [G loss: 2.565126]\n",
      "epoch:5 step:5297 [D loss: 0.572123, acc: 75.00%] [G loss: 2.783260]\n",
      "epoch:5 step:5298 [D loss: 0.617654, acc: 59.38%] [G loss: 2.797294]\n",
      "epoch:5 step:5299 [D loss: 0.620094, acc: 69.53%] [G loss: 2.402595]\n",
      "epoch:5 step:5300 [D loss: 0.669311, acc: 60.16%] [G loss: 2.207749]\n",
      "epoch:5 step:5301 [D loss: 0.577134, acc: 71.09%] [G loss: 2.426469]\n",
      "epoch:5 step:5302 [D loss: 0.662272, acc: 61.72%] [G loss: 2.315224]\n",
      "epoch:5 step:5303 [D loss: 0.666738, acc: 68.75%] [G loss: 2.454955]\n",
      "epoch:5 step:5304 [D loss: 0.637806, acc: 68.75%] [G loss: 2.451820]\n",
      "epoch:5 step:5305 [D loss: 0.597868, acc: 62.50%] [G loss: 2.479468]\n",
      "epoch:5 step:5306 [D loss: 0.636555, acc: 66.41%] [G loss: 2.311929]\n",
      "epoch:5 step:5307 [D loss: 0.612582, acc: 63.28%] [G loss: 2.261310]\n",
      "epoch:5 step:5308 [D loss: 0.581506, acc: 67.19%] [G loss: 2.580755]\n",
      "epoch:5 step:5309 [D loss: 0.568844, acc: 71.88%] [G loss: 2.636952]\n",
      "epoch:5 step:5310 [D loss: 0.590176, acc: 69.53%] [G loss: 2.332562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5311 [D loss: 0.622895, acc: 68.75%] [G loss: 2.671627]\n",
      "epoch:5 step:5312 [D loss: 0.513641, acc: 78.91%] [G loss: 2.542041]\n",
      "epoch:5 step:5313 [D loss: 0.567106, acc: 67.19%] [G loss: 2.523983]\n",
      "epoch:5 step:5314 [D loss: 0.587246, acc: 68.75%] [G loss: 2.665351]\n",
      "epoch:5 step:5315 [D loss: 0.580747, acc: 67.97%] [G loss: 2.728868]\n",
      "epoch:5 step:5316 [D loss: 0.659609, acc: 64.06%] [G loss: 2.668295]\n",
      "epoch:5 step:5317 [D loss: 0.563724, acc: 71.09%] [G loss: 2.640227]\n",
      "epoch:5 step:5318 [D loss: 0.504249, acc: 78.91%] [G loss: 2.719584]\n",
      "epoch:5 step:5319 [D loss: 0.515897, acc: 72.66%] [G loss: 2.902889]\n",
      "epoch:5 step:5320 [D loss: 0.508580, acc: 78.12%] [G loss: 2.765781]\n",
      "epoch:5 step:5321 [D loss: 0.642598, acc: 64.06%] [G loss: 2.568418]\n",
      "epoch:5 step:5322 [D loss: 0.615190, acc: 67.19%] [G loss: 2.520018]\n",
      "epoch:5 step:5323 [D loss: 0.632357, acc: 67.97%] [G loss: 2.631829]\n",
      "epoch:5 step:5324 [D loss: 0.490172, acc: 75.78%] [G loss: 2.671500]\n",
      "epoch:5 step:5325 [D loss: 0.606384, acc: 68.75%] [G loss: 2.612648]\n",
      "epoch:5 step:5326 [D loss: 0.602845, acc: 67.19%] [G loss: 2.847606]\n",
      "epoch:5 step:5327 [D loss: 0.529934, acc: 75.00%] [G loss: 2.984690]\n",
      "epoch:5 step:5328 [D loss: 0.521483, acc: 67.97%] [G loss: 2.674838]\n",
      "epoch:5 step:5329 [D loss: 0.593103, acc: 67.97%] [G loss: 2.693465]\n",
      "epoch:5 step:5330 [D loss: 0.533240, acc: 75.00%] [G loss: 2.685493]\n",
      "epoch:5 step:5331 [D loss: 0.602800, acc: 67.19%] [G loss: 2.900802]\n",
      "epoch:5 step:5332 [D loss: 0.548812, acc: 72.66%] [G loss: 2.724730]\n",
      "epoch:5 step:5333 [D loss: 0.489552, acc: 76.56%] [G loss: 3.243570]\n",
      "epoch:5 step:5334 [D loss: 0.549748, acc: 67.19%] [G loss: 3.007637]\n",
      "epoch:5 step:5335 [D loss: 0.568335, acc: 70.31%] [G loss: 3.502325]\n",
      "epoch:5 step:5336 [D loss: 0.592682, acc: 67.97%] [G loss: 2.773723]\n",
      "epoch:5 step:5337 [D loss: 0.561552, acc: 71.88%] [G loss: 2.517911]\n",
      "epoch:5 step:5338 [D loss: 0.549417, acc: 70.31%] [G loss: 2.774761]\n",
      "epoch:5 step:5339 [D loss: 0.585503, acc: 67.97%] [G loss: 2.952070]\n",
      "epoch:5 step:5340 [D loss: 0.587940, acc: 67.19%] [G loss: 2.695811]\n",
      "epoch:5 step:5341 [D loss: 0.555267, acc: 71.88%] [G loss: 2.569256]\n",
      "epoch:5 step:5342 [D loss: 0.659069, acc: 62.50%] [G loss: 2.403625]\n",
      "epoch:5 step:5343 [D loss: 0.625486, acc: 69.53%] [G loss: 2.332154]\n",
      "epoch:5 step:5344 [D loss: 0.602980, acc: 67.97%] [G loss: 2.504766]\n",
      "epoch:5 step:5345 [D loss: 0.541680, acc: 71.88%] [G loss: 2.813269]\n",
      "epoch:5 step:5346 [D loss: 0.571578, acc: 71.09%] [G loss: 2.405668]\n",
      "epoch:5 step:5347 [D loss: 0.564706, acc: 71.88%] [G loss: 2.721333]\n",
      "epoch:5 step:5348 [D loss: 0.605301, acc: 70.31%] [G loss: 2.992332]\n",
      "epoch:5 step:5349 [D loss: 0.609073, acc: 70.31%] [G loss: 2.708507]\n",
      "epoch:5 step:5350 [D loss: 0.584452, acc: 68.75%] [G loss: 2.678561]\n",
      "epoch:5 step:5351 [D loss: 0.622127, acc: 69.53%] [G loss: 2.609307]\n",
      "epoch:5 step:5352 [D loss: 0.635735, acc: 64.84%] [G loss: 2.370805]\n",
      "epoch:5 step:5353 [D loss: 0.610196, acc: 62.50%] [G loss: 2.351034]\n",
      "epoch:5 step:5354 [D loss: 0.669010, acc: 63.28%] [G loss: 2.285786]\n",
      "epoch:5 step:5355 [D loss: 0.534946, acc: 71.09%] [G loss: 2.583951]\n",
      "epoch:5 step:5356 [D loss: 0.573573, acc: 73.44%] [G loss: 2.421351]\n",
      "epoch:5 step:5357 [D loss: 0.550303, acc: 71.09%] [G loss: 2.743213]\n",
      "epoch:5 step:5358 [D loss: 0.589988, acc: 67.19%] [G loss: 2.863677]\n",
      "epoch:5 step:5359 [D loss: 0.618090, acc: 67.19%] [G loss: 2.300383]\n",
      "epoch:5 step:5360 [D loss: 0.592781, acc: 70.31%] [G loss: 2.337306]\n",
      "epoch:5 step:5361 [D loss: 0.590854, acc: 71.09%] [G loss: 2.480274]\n",
      "epoch:5 step:5362 [D loss: 0.521008, acc: 74.22%] [G loss: 2.878231]\n",
      "epoch:5 step:5363 [D loss: 0.600963, acc: 64.84%] [G loss: 2.879214]\n",
      "epoch:5 step:5364 [D loss: 0.525168, acc: 75.78%] [G loss: 2.655720]\n",
      "epoch:5 step:5365 [D loss: 0.532628, acc: 75.78%] [G loss: 2.890879]\n",
      "epoch:5 step:5366 [D loss: 0.620431, acc: 72.66%] [G loss: 2.655514]\n",
      "epoch:5 step:5367 [D loss: 0.563702, acc: 69.53%] [G loss: 2.303349]\n",
      "epoch:5 step:5368 [D loss: 0.631868, acc: 64.84%] [G loss: 2.332640]\n",
      "epoch:5 step:5369 [D loss: 0.602663, acc: 64.84%] [G loss: 2.418453]\n",
      "epoch:5 step:5370 [D loss: 0.627320, acc: 63.28%] [G loss: 2.248574]\n",
      "epoch:5 step:5371 [D loss: 0.518136, acc: 78.12%] [G loss: 2.548829]\n",
      "epoch:5 step:5372 [D loss: 0.644300, acc: 64.84%] [G loss: 2.176967]\n",
      "epoch:5 step:5373 [D loss: 0.580340, acc: 70.31%] [G loss: 2.592614]\n",
      "epoch:5 step:5374 [D loss: 0.557457, acc: 71.88%] [G loss: 2.748153]\n",
      "epoch:5 step:5375 [D loss: 0.599406, acc: 65.62%] [G loss: 2.656481]\n",
      "epoch:5 step:5376 [D loss: 0.567669, acc: 70.31%] [G loss: 2.840478]\n",
      "epoch:5 step:5377 [D loss: 0.552496, acc: 70.31%] [G loss: 2.824460]\n",
      "epoch:5 step:5378 [D loss: 0.539829, acc: 72.66%] [G loss: 2.534069]\n",
      "epoch:5 step:5379 [D loss: 0.579543, acc: 79.69%] [G loss: 2.991165]\n",
      "epoch:5 step:5380 [D loss: 0.580419, acc: 75.00%] [G loss: 2.503449]\n",
      "epoch:5 step:5381 [D loss: 0.632469, acc: 64.06%] [G loss: 2.515480]\n",
      "epoch:5 step:5382 [D loss: 0.601152, acc: 63.28%] [G loss: 2.710955]\n",
      "epoch:5 step:5383 [D loss: 0.587927, acc: 71.88%] [G loss: 2.674923]\n",
      "epoch:5 step:5384 [D loss: 0.475004, acc: 77.34%] [G loss: 2.759990]\n",
      "epoch:5 step:5385 [D loss: 0.611902, acc: 67.19%] [G loss: 2.536252]\n",
      "epoch:5 step:5386 [D loss: 0.558332, acc: 71.09%] [G loss: 2.738027]\n",
      "epoch:5 step:5387 [D loss: 0.675038, acc: 60.94%] [G loss: 2.469919]\n",
      "epoch:5 step:5388 [D loss: 0.640085, acc: 64.84%] [G loss: 2.331568]\n",
      "epoch:5 step:5389 [D loss: 0.671970, acc: 57.03%] [G loss: 2.337504]\n",
      "epoch:5 step:5390 [D loss: 0.619064, acc: 63.28%] [G loss: 2.470903]\n",
      "epoch:5 step:5391 [D loss: 0.619259, acc: 62.50%] [G loss: 2.445016]\n",
      "epoch:5 step:5392 [D loss: 0.528384, acc: 75.78%] [G loss: 2.964071]\n",
      "epoch:5 step:5393 [D loss: 0.621284, acc: 71.88%] [G loss: 2.751124]\n",
      "epoch:5 step:5394 [D loss: 0.578524, acc: 69.53%] [G loss: 2.738922]\n",
      "epoch:5 step:5395 [D loss: 0.679760, acc: 62.50%] [G loss: 2.373399]\n",
      "epoch:5 step:5396 [D loss: 0.593532, acc: 66.41%] [G loss: 2.421725]\n",
      "epoch:5 step:5397 [D loss: 0.636670, acc: 67.19%] [G loss: 2.235019]\n",
      "epoch:5 step:5398 [D loss: 0.561579, acc: 72.66%] [G loss: 2.402221]\n",
      "epoch:5 step:5399 [D loss: 0.580308, acc: 64.84%] [G loss: 2.608149]\n",
      "epoch:5 step:5400 [D loss: 0.569905, acc: 68.75%] [G loss: 2.288962]\n",
      "epoch:5 step:5401 [D loss: 0.578093, acc: 69.53%] [G loss: 2.459247]\n",
      "epoch:5 step:5402 [D loss: 0.632131, acc: 65.62%] [G loss: 2.382563]\n",
      "epoch:5 step:5403 [D loss: 0.611485, acc: 66.41%] [G loss: 2.450906]\n",
      "epoch:5 step:5404 [D loss: 0.586392, acc: 70.31%] [G loss: 2.589729]\n",
      "epoch:5 step:5405 [D loss: 0.607138, acc: 60.94%] [G loss: 2.601928]\n",
      "epoch:5 step:5406 [D loss: 0.552669, acc: 73.44%] [G loss: 3.012708]\n",
      "epoch:5 step:5407 [D loss: 0.619670, acc: 66.41%] [G loss: 2.745843]\n",
      "epoch:5 step:5408 [D loss: 0.570180, acc: 69.53%] [G loss: 2.634335]\n",
      "epoch:5 step:5409 [D loss: 0.658649, acc: 66.41%] [G loss: 2.467902]\n",
      "epoch:5 step:5410 [D loss: 0.580808, acc: 68.75%] [G loss: 2.677872]\n",
      "epoch:5 step:5411 [D loss: 0.608324, acc: 66.41%] [G loss: 2.651403]\n",
      "epoch:5 step:5412 [D loss: 0.586799, acc: 67.19%] [G loss: 2.664224]\n",
      "epoch:5 step:5413 [D loss: 0.612946, acc: 70.31%] [G loss: 2.502662]\n",
      "epoch:5 step:5414 [D loss: 0.598079, acc: 69.53%] [G loss: 2.442054]\n",
      "epoch:5 step:5415 [D loss: 0.623379, acc: 65.62%] [G loss: 2.419086]\n",
      "epoch:5 step:5416 [D loss: 0.598839, acc: 67.97%] [G loss: 2.204250]\n",
      "epoch:5 step:5417 [D loss: 0.487336, acc: 77.34%] [G loss: 2.669297]\n",
      "epoch:5 step:5418 [D loss: 0.617213, acc: 67.19%] [G loss: 2.532791]\n",
      "epoch:5 step:5419 [D loss: 0.485336, acc: 78.12%] [G loss: 2.698450]\n",
      "epoch:5 step:5420 [D loss: 0.632571, acc: 64.84%] [G loss: 2.635992]\n",
      "epoch:5 step:5421 [D loss: 0.538320, acc: 72.66%] [G loss: 2.698490]\n",
      "epoch:5 step:5422 [D loss: 0.560610, acc: 68.75%] [G loss: 2.538512]\n",
      "epoch:5 step:5423 [D loss: 0.670353, acc: 64.06%] [G loss: 2.340537]\n",
      "epoch:5 step:5424 [D loss: 0.594815, acc: 68.75%] [G loss: 2.285992]\n",
      "epoch:5 step:5425 [D loss: 0.540509, acc: 71.88%] [G loss: 2.366963]\n",
      "epoch:5 step:5426 [D loss: 0.644998, acc: 67.97%] [G loss: 2.390854]\n",
      "epoch:5 step:5427 [D loss: 0.720738, acc: 54.69%] [G loss: 2.398398]\n",
      "epoch:5 step:5428 [D loss: 0.584799, acc: 67.97%] [G loss: 2.568655]\n",
      "epoch:5 step:5429 [D loss: 0.661030, acc: 63.28%] [G loss: 2.359405]\n",
      "epoch:5 step:5430 [D loss: 0.652439, acc: 60.94%] [G loss: 2.292336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5431 [D loss: 0.568508, acc: 67.97%] [G loss: 2.409056]\n",
      "epoch:5 step:5432 [D loss: 0.614041, acc: 64.06%] [G loss: 2.600356]\n",
      "epoch:5 step:5433 [D loss: 0.611503, acc: 68.75%] [G loss: 2.584758]\n",
      "epoch:5 step:5434 [D loss: 0.609266, acc: 71.88%] [G loss: 2.213509]\n",
      "epoch:5 step:5435 [D loss: 0.595913, acc: 73.44%] [G loss: 2.570278]\n",
      "epoch:5 step:5436 [D loss: 0.638528, acc: 66.41%] [G loss: 2.306271]\n",
      "epoch:5 step:5437 [D loss: 0.636431, acc: 61.72%] [G loss: 2.241013]\n",
      "epoch:5 step:5438 [D loss: 0.562174, acc: 66.41%] [G loss: 2.405139]\n",
      "epoch:5 step:5439 [D loss: 0.577123, acc: 67.19%] [G loss: 2.390005]\n",
      "epoch:5 step:5440 [D loss: 0.578831, acc: 65.62%] [G loss: 2.298041]\n",
      "epoch:5 step:5441 [D loss: 0.627257, acc: 65.62%] [G loss: 2.717219]\n",
      "epoch:5 step:5442 [D loss: 0.619414, acc: 60.94%] [G loss: 2.474805]\n",
      "epoch:5 step:5443 [D loss: 0.605158, acc: 67.19%] [G loss: 2.524103]\n",
      "epoch:5 step:5444 [D loss: 0.670003, acc: 65.62%] [G loss: 2.339629]\n",
      "epoch:5 step:5445 [D loss: 0.590843, acc: 68.75%] [G loss: 2.251486]\n",
      "epoch:5 step:5446 [D loss: 0.606735, acc: 73.44%] [G loss: 2.482481]\n",
      "epoch:5 step:5447 [D loss: 0.660872, acc: 58.59%] [G loss: 2.181042]\n",
      "epoch:5 step:5448 [D loss: 0.619608, acc: 68.75%] [G loss: 2.379361]\n",
      "epoch:5 step:5449 [D loss: 0.558017, acc: 65.62%] [G loss: 2.346104]\n",
      "epoch:5 step:5450 [D loss: 0.624667, acc: 67.19%] [G loss: 2.356341]\n",
      "epoch:5 step:5451 [D loss: 0.700899, acc: 57.03%] [G loss: 2.244429]\n",
      "epoch:5 step:5452 [D loss: 0.697670, acc: 64.06%] [G loss: 2.388803]\n",
      "epoch:5 step:5453 [D loss: 0.584736, acc: 69.53%] [G loss: 2.212288]\n",
      "epoch:5 step:5454 [D loss: 0.573893, acc: 66.41%] [G loss: 2.686552]\n",
      "epoch:5 step:5455 [D loss: 0.620103, acc: 67.19%] [G loss: 2.240307]\n",
      "epoch:5 step:5456 [D loss: 0.595914, acc: 67.19%] [G loss: 2.416676]\n",
      "epoch:5 step:5457 [D loss: 0.601615, acc: 72.66%] [G loss: 2.716299]\n",
      "epoch:5 step:5458 [D loss: 0.600322, acc: 64.84%] [G loss: 2.250492]\n",
      "epoch:5 step:5459 [D loss: 0.575117, acc: 70.31%] [G loss: 2.863250]\n",
      "epoch:5 step:5460 [D loss: 0.555501, acc: 76.56%] [G loss: 2.856781]\n",
      "epoch:5 step:5461 [D loss: 0.574256, acc: 71.09%] [G loss: 2.520255]\n",
      "epoch:5 step:5462 [D loss: 0.571520, acc: 74.22%] [G loss: 2.449514]\n",
      "epoch:5 step:5463 [D loss: 0.679596, acc: 60.16%] [G loss: 2.445979]\n",
      "epoch:5 step:5464 [D loss: 0.510705, acc: 75.00%] [G loss: 2.711872]\n",
      "epoch:5 step:5465 [D loss: 0.643537, acc: 66.41%] [G loss: 2.707052]\n",
      "epoch:5 step:5466 [D loss: 0.527617, acc: 73.44%] [G loss: 3.037744]\n",
      "epoch:5 step:5467 [D loss: 0.642670, acc: 60.16%] [G loss: 2.668801]\n",
      "epoch:5 step:5468 [D loss: 0.559876, acc: 67.97%] [G loss: 2.466426]\n",
      "epoch:5 step:5469 [D loss: 0.669715, acc: 59.38%] [G loss: 2.234010]\n",
      "epoch:5 step:5470 [D loss: 0.615396, acc: 69.53%] [G loss: 2.456470]\n",
      "epoch:5 step:5471 [D loss: 0.535632, acc: 72.66%] [G loss: 2.586951]\n",
      "epoch:5 step:5472 [D loss: 0.677924, acc: 64.06%] [G loss: 2.343373]\n",
      "epoch:5 step:5473 [D loss: 0.629553, acc: 67.19%] [G loss: 2.258336]\n",
      "epoch:5 step:5474 [D loss: 0.602266, acc: 67.19%] [G loss: 2.343077]\n",
      "epoch:5 step:5475 [D loss: 0.581038, acc: 73.44%] [G loss: 2.555169]\n",
      "epoch:5 step:5476 [D loss: 0.564646, acc: 71.09%] [G loss: 2.757861]\n",
      "epoch:5 step:5477 [D loss: 0.598147, acc: 69.53%] [G loss: 2.964362]\n",
      "epoch:5 step:5478 [D loss: 0.618094, acc: 61.72%] [G loss: 2.488826]\n",
      "epoch:5 step:5479 [D loss: 0.693024, acc: 62.50%] [G loss: 2.038141]\n",
      "epoch:5 step:5480 [D loss: 0.597161, acc: 67.97%] [G loss: 2.686745]\n",
      "epoch:5 step:5481 [D loss: 0.581007, acc: 76.56%] [G loss: 2.455238]\n",
      "epoch:5 step:5482 [D loss: 0.557177, acc: 67.97%] [G loss: 2.555830]\n",
      "epoch:5 step:5483 [D loss: 0.618520, acc: 69.53%] [G loss: 2.464440]\n",
      "epoch:5 step:5484 [D loss: 0.550298, acc: 75.78%] [G loss: 2.474621]\n",
      "epoch:5 step:5485 [D loss: 0.666655, acc: 64.84%] [G loss: 2.450870]\n",
      "epoch:5 step:5486 [D loss: 0.545559, acc: 74.22%] [G loss: 2.666481]\n",
      "epoch:5 step:5487 [D loss: 0.616819, acc: 71.88%] [G loss: 2.582967]\n",
      "epoch:5 step:5488 [D loss: 0.509516, acc: 78.91%] [G loss: 2.720987]\n",
      "epoch:5 step:5489 [D loss: 0.524259, acc: 73.44%] [G loss: 2.698887]\n",
      "epoch:5 step:5490 [D loss: 0.566577, acc: 69.53%] [G loss: 2.932531]\n",
      "epoch:5 step:5491 [D loss: 0.470198, acc: 81.25%] [G loss: 2.872030]\n",
      "epoch:5 step:5492 [D loss: 0.541813, acc: 71.09%] [G loss: 2.883988]\n",
      "epoch:5 step:5493 [D loss: 0.575231, acc: 71.09%] [G loss: 2.807073]\n",
      "epoch:5 step:5494 [D loss: 0.584947, acc: 67.97%] [G loss: 2.564108]\n",
      "epoch:5 step:5495 [D loss: 0.549730, acc: 74.22%] [G loss: 2.217058]\n",
      "epoch:5 step:5496 [D loss: 0.526189, acc: 74.22%] [G loss: 2.564163]\n",
      "epoch:5 step:5497 [D loss: 0.593321, acc: 70.31%] [G loss: 2.521801]\n",
      "epoch:5 step:5498 [D loss: 0.551687, acc: 74.22%] [G loss: 2.938520]\n",
      "epoch:5 step:5499 [D loss: 0.565986, acc: 75.78%] [G loss: 2.567916]\n",
      "epoch:5 step:5500 [D loss: 0.550244, acc: 72.66%] [G loss: 2.933574]\n",
      "epoch:5 step:5501 [D loss: 0.512884, acc: 71.09%] [G loss: 3.113352]\n",
      "epoch:5 step:5502 [D loss: 0.604088, acc: 65.62%] [G loss: 2.789338]\n",
      "epoch:5 step:5503 [D loss: 0.686218, acc: 63.28%] [G loss: 2.361850]\n",
      "epoch:5 step:5504 [D loss: 0.536433, acc: 73.44%] [G loss: 2.434272]\n",
      "epoch:5 step:5505 [D loss: 0.630710, acc: 68.75%] [G loss: 2.319134]\n",
      "epoch:5 step:5506 [D loss: 0.569311, acc: 71.09%] [G loss: 2.802368]\n",
      "epoch:5 step:5507 [D loss: 0.555923, acc: 67.19%] [G loss: 2.674233]\n",
      "epoch:5 step:5508 [D loss: 0.502607, acc: 75.00%] [G loss: 2.955210]\n",
      "epoch:5 step:5509 [D loss: 0.646048, acc: 60.16%] [G loss: 2.298077]\n",
      "epoch:5 step:5510 [D loss: 0.511681, acc: 72.66%] [G loss: 3.036384]\n",
      "epoch:5 step:5511 [D loss: 0.577978, acc: 70.31%] [G loss: 2.554555]\n",
      "epoch:5 step:5512 [D loss: 0.612292, acc: 65.62%] [G loss: 2.357117]\n",
      "epoch:5 step:5513 [D loss: 0.641554, acc: 60.94%] [G loss: 2.241099]\n",
      "epoch:5 step:5514 [D loss: 0.682645, acc: 57.81%] [G loss: 2.363629]\n",
      "epoch:5 step:5515 [D loss: 0.681516, acc: 60.94%] [G loss: 2.461177]\n",
      "epoch:5 step:5516 [D loss: 0.589754, acc: 67.97%] [G loss: 2.705195]\n",
      "epoch:5 step:5517 [D loss: 0.586257, acc: 71.09%] [G loss: 2.444503]\n",
      "epoch:5 step:5518 [D loss: 0.657151, acc: 60.16%] [G loss: 2.572432]\n",
      "epoch:5 step:5519 [D loss: 0.547818, acc: 69.53%] [G loss: 2.621241]\n",
      "epoch:5 step:5520 [D loss: 0.613285, acc: 67.97%] [G loss: 2.587226]\n",
      "epoch:5 step:5521 [D loss: 0.679984, acc: 60.94%] [G loss: 2.362495]\n",
      "epoch:5 step:5522 [D loss: 0.544882, acc: 75.00%] [G loss: 2.559260]\n",
      "epoch:5 step:5523 [D loss: 0.594975, acc: 70.31%] [G loss: 2.705892]\n",
      "epoch:5 step:5524 [D loss: 0.576246, acc: 67.19%] [G loss: 2.529688]\n",
      "epoch:5 step:5525 [D loss: 0.553416, acc: 71.88%] [G loss: 2.648986]\n",
      "epoch:5 step:5526 [D loss: 0.514160, acc: 71.88%] [G loss: 2.738008]\n",
      "epoch:5 step:5527 [D loss: 0.577826, acc: 70.31%] [G loss: 2.807557]\n",
      "epoch:5 step:5528 [D loss: 0.646631, acc: 67.19%] [G loss: 2.350623]\n",
      "epoch:5 step:5529 [D loss: 0.632116, acc: 67.97%] [G loss: 2.302002]\n",
      "epoch:5 step:5530 [D loss: 0.645576, acc: 64.84%] [G loss: 2.348358]\n",
      "epoch:5 step:5531 [D loss: 0.576996, acc: 74.22%] [G loss: 2.341483]\n",
      "epoch:5 step:5532 [D loss: 0.596648, acc: 67.97%] [G loss: 2.462184]\n",
      "epoch:5 step:5533 [D loss: 0.567128, acc: 72.66%] [G loss: 2.628231]\n",
      "epoch:5 step:5534 [D loss: 0.653895, acc: 67.97%] [G loss: 2.314711]\n",
      "epoch:5 step:5535 [D loss: 0.599456, acc: 68.75%] [G loss: 2.402717]\n",
      "epoch:5 step:5536 [D loss: 0.732528, acc: 55.47%] [G loss: 2.330206]\n",
      "epoch:5 step:5537 [D loss: 0.648691, acc: 57.81%] [G loss: 2.307348]\n",
      "epoch:5 step:5538 [D loss: 0.591155, acc: 70.31%] [G loss: 2.459381]\n",
      "epoch:5 step:5539 [D loss: 0.620336, acc: 69.53%] [G loss: 2.568175]\n",
      "epoch:5 step:5540 [D loss: 0.579098, acc: 65.62%] [G loss: 2.185697]\n",
      "epoch:5 step:5541 [D loss: 0.618097, acc: 67.97%] [G loss: 2.270001]\n",
      "epoch:5 step:5542 [D loss: 0.629178, acc: 66.41%] [G loss: 2.675075]\n",
      "epoch:5 step:5543 [D loss: 0.711023, acc: 55.47%] [G loss: 2.407821]\n",
      "epoch:5 step:5544 [D loss: 0.681678, acc: 60.16%] [G loss: 2.481466]\n",
      "epoch:5 step:5545 [D loss: 0.569810, acc: 71.88%] [G loss: 2.470984]\n",
      "epoch:5 step:5546 [D loss: 0.601934, acc: 67.97%] [G loss: 2.436844]\n",
      "epoch:5 step:5547 [D loss: 0.589919, acc: 65.62%] [G loss: 2.277792]\n",
      "epoch:5 step:5548 [D loss: 0.593471, acc: 69.53%] [G loss: 2.574424]\n",
      "epoch:5 step:5549 [D loss: 0.568255, acc: 71.88%] [G loss: 2.558990]\n",
      "epoch:5 step:5550 [D loss: 0.625877, acc: 66.41%] [G loss: 2.400337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:5551 [D loss: 0.589967, acc: 71.09%] [G loss: 2.589202]\n",
      "epoch:5 step:5552 [D loss: 0.674220, acc: 67.19%] [G loss: 2.295560]\n",
      "epoch:5 step:5553 [D loss: 0.637705, acc: 63.28%] [G loss: 2.372835]\n",
      "epoch:5 step:5554 [D loss: 0.591233, acc: 64.06%] [G loss: 2.504883]\n",
      "epoch:5 step:5555 [D loss: 0.676711, acc: 60.16%] [G loss: 2.288508]\n",
      "epoch:5 step:5556 [D loss: 0.517267, acc: 79.69%] [G loss: 2.465161]\n",
      "epoch:5 step:5557 [D loss: 0.552376, acc: 71.09%] [G loss: 2.538963]\n",
      "epoch:5 step:5558 [D loss: 0.690277, acc: 54.69%] [G loss: 2.287632]\n",
      "epoch:5 step:5559 [D loss: 0.626068, acc: 67.97%] [G loss: 2.306555]\n",
      "epoch:5 step:5560 [D loss: 0.540701, acc: 68.75%] [G loss: 2.683746]\n",
      "epoch:5 step:5561 [D loss: 0.533910, acc: 72.66%] [G loss: 2.743765]\n",
      "epoch:5 step:5562 [D loss: 0.688963, acc: 67.19%] [G loss: 2.560224]\n",
      "epoch:5 step:5563 [D loss: 0.668150, acc: 64.84%] [G loss: 2.347414]\n",
      "epoch:5 step:5564 [D loss: 0.605925, acc: 67.19%] [G loss: 2.386582]\n",
      "epoch:5 step:5565 [D loss: 0.594792, acc: 69.53%] [G loss: 2.221065]\n",
      "epoch:5 step:5566 [D loss: 0.648907, acc: 63.28%] [G loss: 2.363307]\n",
      "epoch:5 step:5567 [D loss: 0.687680, acc: 58.59%] [G loss: 2.269780]\n",
      "epoch:5 step:5568 [D loss: 0.610852, acc: 64.06%] [G loss: 2.666122]\n",
      "epoch:5 step:5569 [D loss: 0.546014, acc: 68.75%] [G loss: 2.672136]\n",
      "epoch:5 step:5570 [D loss: 0.602015, acc: 65.62%] [G loss: 2.530530]\n",
      "epoch:5 step:5571 [D loss: 0.510036, acc: 73.44%] [G loss: 3.145275]\n",
      "epoch:5 step:5572 [D loss: 0.564810, acc: 68.75%] [G loss: 2.686222]\n",
      "epoch:5 step:5573 [D loss: 0.575369, acc: 67.19%] [G loss: 2.434123]\n",
      "epoch:5 step:5574 [D loss: 0.582440, acc: 71.88%] [G loss: 2.359812]\n",
      "epoch:5 step:5575 [D loss: 0.547223, acc: 75.00%] [G loss: 2.834307]\n",
      "epoch:5 step:5576 [D loss: 0.630777, acc: 64.06%] [G loss: 2.382369]\n",
      "epoch:5 step:5577 [D loss: 0.626938, acc: 68.75%] [G loss: 2.532416]\n",
      "epoch:5 step:5578 [D loss: 0.536674, acc: 69.53%] [G loss: 2.504081]\n",
      "epoch:5 step:5579 [D loss: 0.466549, acc: 79.69%] [G loss: 2.723860]\n",
      "epoch:5 step:5580 [D loss: 0.634641, acc: 60.16%] [G loss: 2.389856]\n",
      "epoch:5 step:5581 [D loss: 0.619554, acc: 72.66%] [G loss: 2.550672]\n",
      "epoch:5 step:5582 [D loss: 0.585084, acc: 65.62%] [G loss: 2.513393]\n",
      "epoch:5 step:5583 [D loss: 0.659322, acc: 71.88%] [G loss: 2.468183]\n",
      "epoch:5 step:5584 [D loss: 0.585542, acc: 67.97%] [G loss: 2.456744]\n",
      "epoch:5 step:5585 [D loss: 0.602295, acc: 67.19%] [G loss: 2.829134]\n",
      "epoch:5 step:5586 [D loss: 0.645294, acc: 62.50%] [G loss: 2.712132]\n",
      "epoch:5 step:5587 [D loss: 0.626436, acc: 65.62%] [G loss: 2.364246]\n",
      "epoch:5 step:5588 [D loss: 0.637405, acc: 68.75%] [G loss: 2.508882]\n",
      "epoch:5 step:5589 [D loss: 0.582813, acc: 72.66%] [G loss: 2.687947]\n",
      "epoch:5 step:5590 [D loss: 0.557721, acc: 71.09%] [G loss: 2.616579]\n",
      "epoch:5 step:5591 [D loss: 0.610685, acc: 62.50%] [G loss: 2.817629]\n",
      "epoch:5 step:5592 [D loss: 0.609869, acc: 67.19%] [G loss: 2.637838]\n",
      "epoch:5 step:5593 [D loss: 0.611803, acc: 64.84%] [G loss: 2.658361]\n",
      "epoch:5 step:5594 [D loss: 0.508755, acc: 75.78%] [G loss: 2.550457]\n",
      "epoch:5 step:5595 [D loss: 0.550136, acc: 71.09%] [G loss: 2.597923]\n",
      "epoch:5 step:5596 [D loss: 0.549726, acc: 67.97%] [G loss: 2.842360]\n",
      "epoch:5 step:5597 [D loss: 0.569858, acc: 73.44%] [G loss: 3.268286]\n",
      "epoch:5 step:5598 [D loss: 0.642806, acc: 62.50%] [G loss: 2.597384]\n",
      "epoch:5 step:5599 [D loss: 0.603660, acc: 70.31%] [G loss: 2.573053]\n",
      "epoch:5 step:5600 [D loss: 0.613526, acc: 64.06%] [G loss: 2.443867]\n",
      "epoch:5 step:5601 [D loss: 0.602976, acc: 72.66%] [G loss: 2.732322]\n",
      "epoch:5 step:5602 [D loss: 0.664517, acc: 66.41%] [G loss: 2.643879]\n",
      "epoch:5 step:5603 [D loss: 0.559434, acc: 71.88%] [G loss: 2.557749]\n",
      "epoch:5 step:5604 [D loss: 0.588723, acc: 71.09%] [G loss: 2.943773]\n",
      "epoch:5 step:5605 [D loss: 0.674746, acc: 63.28%] [G loss: 2.645772]\n",
      "epoch:5 step:5606 [D loss: 0.638874, acc: 64.06%] [G loss: 2.912441]\n",
      "epoch:5 step:5607 [D loss: 0.612182, acc: 63.28%] [G loss: 2.312577]\n",
      "epoch:5 step:5608 [D loss: 0.540919, acc: 75.78%] [G loss: 2.603386]\n",
      "epoch:5 step:5609 [D loss: 0.511094, acc: 78.91%] [G loss: 2.717613]\n",
      "epoch:5 step:5610 [D loss: 0.572795, acc: 68.75%] [G loss: 2.937480]\n",
      "epoch:5 step:5611 [D loss: 0.608520, acc: 66.41%] [G loss: 3.059917]\n",
      "epoch:5 step:5612 [D loss: 0.604689, acc: 66.41%] [G loss: 2.820815]\n",
      "epoch:5 step:5613 [D loss: 0.805706, acc: 54.69%] [G loss: 2.214054]\n",
      "epoch:5 step:5614 [D loss: 0.635054, acc: 60.94%] [G loss: 2.469748]\n",
      "epoch:5 step:5615 [D loss: 0.614991, acc: 66.41%] [G loss: 2.736643]\n",
      "epoch:5 step:5616 [D loss: 0.501893, acc: 77.34%] [G loss: 2.592425]\n",
      "epoch:5 step:5617 [D loss: 0.623955, acc: 64.84%] [G loss: 2.512002]\n",
      "epoch:5 step:5618 [D loss: 0.606483, acc: 71.88%] [G loss: 2.901330]\n",
      "epoch:5 step:5619 [D loss: 0.574477, acc: 75.78%] [G loss: 2.868154]\n",
      "epoch:5 step:5620 [D loss: 0.546016, acc: 78.91%] [G loss: 2.761419]\n",
      "epoch:5 step:5621 [D loss: 0.603296, acc: 67.97%] [G loss: 3.226270]\n",
      "epoch:5 step:5622 [D loss: 0.536300, acc: 72.66%] [G loss: 3.249988]\n",
      "epoch:6 step:5623 [D loss: 0.605198, acc: 66.41%] [G loss: 2.335417]\n",
      "epoch:6 step:5624 [D loss: 0.587165, acc: 71.88%] [G loss: 2.604743]\n",
      "epoch:6 step:5625 [D loss: 0.591027, acc: 67.97%] [G loss: 2.435429]\n",
      "epoch:6 step:5626 [D loss: 0.588686, acc: 68.75%] [G loss: 2.880654]\n",
      "epoch:6 step:5627 [D loss: 0.545854, acc: 75.78%] [G loss: 2.397619]\n",
      "epoch:6 step:5628 [D loss: 0.579265, acc: 74.22%] [G loss: 2.636723]\n",
      "epoch:6 step:5629 [D loss: 0.586618, acc: 69.53%] [G loss: 2.521794]\n",
      "epoch:6 step:5630 [D loss: 0.518034, acc: 73.44%] [G loss: 2.590976]\n",
      "epoch:6 step:5631 [D loss: 0.597544, acc: 69.53%] [G loss: 2.875953]\n",
      "epoch:6 step:5632 [D loss: 0.517657, acc: 71.88%] [G loss: 2.791975]\n",
      "epoch:6 step:5633 [D loss: 0.569652, acc: 71.88%] [G loss: 2.453171]\n",
      "epoch:6 step:5634 [D loss: 0.586651, acc: 67.97%] [G loss: 2.751764]\n",
      "epoch:6 step:5635 [D loss: 0.522424, acc: 67.19%] [G loss: 2.967238]\n",
      "epoch:6 step:5636 [D loss: 0.534747, acc: 70.31%] [G loss: 2.627644]\n",
      "epoch:6 step:5637 [D loss: 0.480273, acc: 75.78%] [G loss: 2.948090]\n",
      "epoch:6 step:5638 [D loss: 0.542436, acc: 70.31%] [G loss: 3.040696]\n",
      "epoch:6 step:5639 [D loss: 0.662969, acc: 60.16%] [G loss: 2.425807]\n",
      "epoch:6 step:5640 [D loss: 0.668291, acc: 61.72%] [G loss: 2.233579]\n",
      "epoch:6 step:5641 [D loss: 0.613380, acc: 69.53%] [G loss: 2.266361]\n",
      "epoch:6 step:5642 [D loss: 0.741153, acc: 55.47%] [G loss: 2.169826]\n",
      "epoch:6 step:5643 [D loss: 0.688432, acc: 59.38%] [G loss: 2.204780]\n",
      "epoch:6 step:5644 [D loss: 0.573734, acc: 72.66%] [G loss: 2.319485]\n",
      "epoch:6 step:5645 [D loss: 0.622706, acc: 61.72%] [G loss: 2.580519]\n",
      "epoch:6 step:5646 [D loss: 0.538382, acc: 78.12%] [G loss: 2.511374]\n",
      "epoch:6 step:5647 [D loss: 0.534911, acc: 75.00%] [G loss: 2.690065]\n",
      "epoch:6 step:5648 [D loss: 0.657657, acc: 67.19%] [G loss: 2.447768]\n",
      "epoch:6 step:5649 [D loss: 0.535458, acc: 71.88%] [G loss: 2.480029]\n",
      "epoch:6 step:5650 [D loss: 0.588511, acc: 65.62%] [G loss: 2.674625]\n",
      "epoch:6 step:5651 [D loss: 0.542996, acc: 71.88%] [G loss: 2.432819]\n",
      "epoch:6 step:5652 [D loss: 0.627047, acc: 64.06%] [G loss: 2.284793]\n",
      "epoch:6 step:5653 [D loss: 0.632434, acc: 63.28%] [G loss: 2.068791]\n",
      "epoch:6 step:5654 [D loss: 0.586264, acc: 66.41%] [G loss: 2.280029]\n",
      "epoch:6 step:5655 [D loss: 0.588810, acc: 67.19%] [G loss: 2.201632]\n",
      "epoch:6 step:5656 [D loss: 0.639068, acc: 64.06%] [G loss: 2.353048]\n",
      "epoch:6 step:5657 [D loss: 0.603975, acc: 67.97%] [G loss: 2.301967]\n",
      "epoch:6 step:5658 [D loss: 0.613537, acc: 69.53%] [G loss: 2.730393]\n",
      "epoch:6 step:5659 [D loss: 0.539479, acc: 76.56%] [G loss: 2.417949]\n",
      "epoch:6 step:5660 [D loss: 0.593747, acc: 62.50%] [G loss: 2.547703]\n",
      "epoch:6 step:5661 [D loss: 0.589446, acc: 67.97%] [G loss: 2.457402]\n",
      "epoch:6 step:5662 [D loss: 0.544289, acc: 71.09%] [G loss: 2.792369]\n",
      "epoch:6 step:5663 [D loss: 0.612166, acc: 67.19%] [G loss: 2.334711]\n",
      "epoch:6 step:5664 [D loss: 0.530594, acc: 72.66%] [G loss: 2.889760]\n",
      "epoch:6 step:5665 [D loss: 0.591162, acc: 67.19%] [G loss: 2.892631]\n",
      "epoch:6 step:5666 [D loss: 0.617431, acc: 62.50%] [G loss: 2.313599]\n",
      "epoch:6 step:5667 [D loss: 0.551410, acc: 69.53%] [G loss: 2.566321]\n",
      "epoch:6 step:5668 [D loss: 0.557405, acc: 66.41%] [G loss: 2.688059]\n",
      "epoch:6 step:5669 [D loss: 0.619678, acc: 71.88%] [G loss: 2.551993]\n",
      "epoch:6 step:5670 [D loss: 0.586281, acc: 72.66%] [G loss: 2.356701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5671 [D loss: 0.665091, acc: 66.41%] [G loss: 2.449533]\n",
      "epoch:6 step:5672 [D loss: 0.592708, acc: 63.28%] [G loss: 2.202475]\n",
      "epoch:6 step:5673 [D loss: 0.594453, acc: 64.84%] [G loss: 2.709545]\n",
      "epoch:6 step:5674 [D loss: 0.549529, acc: 70.31%] [G loss: 2.511909]\n",
      "epoch:6 step:5675 [D loss: 0.626629, acc: 67.97%] [G loss: 2.687252]\n",
      "epoch:6 step:5676 [D loss: 0.565882, acc: 67.97%] [G loss: 2.590346]\n",
      "epoch:6 step:5677 [D loss: 0.496212, acc: 78.12%] [G loss: 2.808300]\n",
      "epoch:6 step:5678 [D loss: 0.619516, acc: 64.84%] [G loss: 2.544073]\n",
      "epoch:6 step:5679 [D loss: 0.599033, acc: 71.09%] [G loss: 2.379977]\n",
      "epoch:6 step:5680 [D loss: 0.619732, acc: 67.97%] [G loss: 2.309755]\n",
      "epoch:6 step:5681 [D loss: 0.566310, acc: 71.88%] [G loss: 2.785832]\n",
      "epoch:6 step:5682 [D loss: 0.564421, acc: 71.88%] [G loss: 2.346565]\n",
      "epoch:6 step:5683 [D loss: 0.618963, acc: 67.19%] [G loss: 2.804704]\n",
      "epoch:6 step:5684 [D loss: 0.531127, acc: 75.78%] [G loss: 2.716326]\n",
      "epoch:6 step:5685 [D loss: 0.581598, acc: 71.09%] [G loss: 2.449480]\n",
      "epoch:6 step:5686 [D loss: 0.649625, acc: 62.50%] [G loss: 2.445380]\n",
      "epoch:6 step:5687 [D loss: 0.543965, acc: 74.22%] [G loss: 2.603908]\n",
      "epoch:6 step:5688 [D loss: 0.572155, acc: 66.41%] [G loss: 2.529014]\n",
      "epoch:6 step:5689 [D loss: 0.615719, acc: 68.75%] [G loss: 2.539123]\n",
      "epoch:6 step:5690 [D loss: 0.616679, acc: 65.62%] [G loss: 2.471539]\n",
      "epoch:6 step:5691 [D loss: 0.594420, acc: 68.75%] [G loss: 2.802106]\n",
      "epoch:6 step:5692 [D loss: 0.556185, acc: 63.28%] [G loss: 2.623814]\n",
      "epoch:6 step:5693 [D loss: 0.672801, acc: 61.72%] [G loss: 2.575137]\n",
      "epoch:6 step:5694 [D loss: 0.576527, acc: 66.41%] [G loss: 2.362447]\n",
      "epoch:6 step:5695 [D loss: 0.611675, acc: 66.41%] [G loss: 2.468606]\n",
      "epoch:6 step:5696 [D loss: 0.542303, acc: 71.88%] [G loss: 2.452420]\n",
      "epoch:6 step:5697 [D loss: 0.569724, acc: 71.88%] [G loss: 2.908580]\n",
      "epoch:6 step:5698 [D loss: 0.525062, acc: 70.31%] [G loss: 2.835588]\n",
      "epoch:6 step:5699 [D loss: 0.500807, acc: 77.34%] [G loss: 2.944516]\n",
      "epoch:6 step:5700 [D loss: 0.648923, acc: 62.50%] [G loss: 2.845778]\n",
      "epoch:6 step:5701 [D loss: 0.573945, acc: 72.66%] [G loss: 2.372834]\n",
      "epoch:6 step:5702 [D loss: 0.644036, acc: 64.06%] [G loss: 2.417574]\n",
      "epoch:6 step:5703 [D loss: 0.626650, acc: 68.75%] [G loss: 2.486614]\n",
      "epoch:6 step:5704 [D loss: 0.560771, acc: 73.44%] [G loss: 2.941035]\n",
      "epoch:6 step:5705 [D loss: 0.556971, acc: 77.34%] [G loss: 2.942317]\n",
      "epoch:6 step:5706 [D loss: 0.611837, acc: 64.84%] [G loss: 2.507303]\n",
      "epoch:6 step:5707 [D loss: 0.611967, acc: 68.75%] [G loss: 2.395314]\n",
      "epoch:6 step:5708 [D loss: 0.559962, acc: 74.22%] [G loss: 2.513433]\n",
      "epoch:6 step:5709 [D loss: 0.663598, acc: 66.41%] [G loss: 2.327981]\n",
      "epoch:6 step:5710 [D loss: 0.611568, acc: 68.75%] [G loss: 2.577319]\n",
      "epoch:6 step:5711 [D loss: 0.673154, acc: 66.41%] [G loss: 2.688355]\n",
      "epoch:6 step:5712 [D loss: 0.559205, acc: 68.75%] [G loss: 2.405787]\n",
      "epoch:6 step:5713 [D loss: 0.530628, acc: 71.88%] [G loss: 2.780107]\n",
      "epoch:6 step:5714 [D loss: 0.569446, acc: 70.31%] [G loss: 2.660464]\n",
      "epoch:6 step:5715 [D loss: 0.506919, acc: 75.00%] [G loss: 2.489686]\n",
      "epoch:6 step:5716 [D loss: 0.619106, acc: 65.62%] [G loss: 2.420407]\n",
      "epoch:6 step:5717 [D loss: 0.636076, acc: 62.50%] [G loss: 2.236558]\n",
      "epoch:6 step:5718 [D loss: 0.580247, acc: 67.19%] [G loss: 2.762760]\n",
      "epoch:6 step:5719 [D loss: 0.566056, acc: 70.31%] [G loss: 2.497763]\n",
      "epoch:6 step:5720 [D loss: 0.588599, acc: 67.97%] [G loss: 2.337183]\n",
      "epoch:6 step:5721 [D loss: 0.657985, acc: 60.16%] [G loss: 2.208293]\n",
      "epoch:6 step:5722 [D loss: 0.588993, acc: 66.41%] [G loss: 2.528645]\n",
      "epoch:6 step:5723 [D loss: 0.579809, acc: 70.31%] [G loss: 2.611224]\n",
      "epoch:6 step:5724 [D loss: 0.587571, acc: 64.84%] [G loss: 2.407991]\n",
      "epoch:6 step:5725 [D loss: 0.500614, acc: 77.34%] [G loss: 2.922398]\n",
      "epoch:6 step:5726 [D loss: 0.577866, acc: 65.62%] [G loss: 2.392258]\n",
      "epoch:6 step:5727 [D loss: 0.613947, acc: 65.62%] [G loss: 2.503940]\n",
      "epoch:6 step:5728 [D loss: 0.601254, acc: 68.75%] [G loss: 2.657161]\n",
      "epoch:6 step:5729 [D loss: 0.683396, acc: 66.41%] [G loss: 2.672863]\n",
      "epoch:6 step:5730 [D loss: 0.634049, acc: 64.06%] [G loss: 2.335169]\n",
      "epoch:6 step:5731 [D loss: 0.695604, acc: 62.50%] [G loss: 2.277046]\n",
      "epoch:6 step:5732 [D loss: 0.619412, acc: 62.50%] [G loss: 2.347845]\n",
      "epoch:6 step:5733 [D loss: 0.604442, acc: 65.62%] [G loss: 2.511313]\n",
      "epoch:6 step:5734 [D loss: 0.540652, acc: 70.31%] [G loss: 2.756501]\n",
      "epoch:6 step:5735 [D loss: 0.618467, acc: 65.62%] [G loss: 2.675148]\n",
      "epoch:6 step:5736 [D loss: 0.590512, acc: 70.31%] [G loss: 2.533902]\n",
      "epoch:6 step:5737 [D loss: 0.544182, acc: 71.09%] [G loss: 2.451651]\n",
      "epoch:6 step:5738 [D loss: 0.581543, acc: 66.41%] [G loss: 2.767678]\n",
      "epoch:6 step:5739 [D loss: 0.662450, acc: 61.72%] [G loss: 2.715755]\n",
      "epoch:6 step:5740 [D loss: 0.634548, acc: 66.41%] [G loss: 2.346413]\n",
      "epoch:6 step:5741 [D loss: 0.525144, acc: 76.56%] [G loss: 2.698772]\n",
      "epoch:6 step:5742 [D loss: 0.592089, acc: 71.09%] [G loss: 2.564021]\n",
      "epoch:6 step:5743 [D loss: 0.605591, acc: 67.19%] [G loss: 2.366994]\n",
      "epoch:6 step:5744 [D loss: 0.543042, acc: 73.44%] [G loss: 2.758860]\n",
      "epoch:6 step:5745 [D loss: 0.696440, acc: 58.59%] [G loss: 2.688944]\n",
      "epoch:6 step:5746 [D loss: 0.629239, acc: 62.50%] [G loss: 2.296079]\n",
      "epoch:6 step:5747 [D loss: 0.583637, acc: 71.88%] [G loss: 2.476766]\n",
      "epoch:6 step:5748 [D loss: 0.613529, acc: 63.28%] [G loss: 2.587969]\n",
      "epoch:6 step:5749 [D loss: 0.601812, acc: 71.09%] [G loss: 2.321771]\n",
      "epoch:6 step:5750 [D loss: 0.597351, acc: 65.62%] [G loss: 2.322952]\n",
      "epoch:6 step:5751 [D loss: 0.615606, acc: 63.28%] [G loss: 2.292744]\n",
      "epoch:6 step:5752 [D loss: 0.603812, acc: 67.19%] [G loss: 2.494507]\n",
      "epoch:6 step:5753 [D loss: 0.570429, acc: 71.88%] [G loss: 2.692034]\n",
      "epoch:6 step:5754 [D loss: 0.621800, acc: 64.84%] [G loss: 2.743969]\n",
      "epoch:6 step:5755 [D loss: 0.676591, acc: 62.50%] [G loss: 2.220561]\n",
      "epoch:6 step:5756 [D loss: 0.634503, acc: 65.62%] [G loss: 2.199008]\n",
      "epoch:6 step:5757 [D loss: 0.575649, acc: 71.09%] [G loss: 2.474280]\n",
      "epoch:6 step:5758 [D loss: 0.625639, acc: 64.84%] [G loss: 2.339243]\n",
      "epoch:6 step:5759 [D loss: 0.693679, acc: 57.81%] [G loss: 2.285514]\n",
      "epoch:6 step:5760 [D loss: 0.618929, acc: 67.19%] [G loss: 2.237304]\n",
      "epoch:6 step:5761 [D loss: 0.549996, acc: 71.09%] [G loss: 2.228711]\n",
      "epoch:6 step:5762 [D loss: 0.600676, acc: 66.41%] [G loss: 2.403268]\n",
      "epoch:6 step:5763 [D loss: 0.601170, acc: 71.09%] [G loss: 2.299791]\n",
      "epoch:6 step:5764 [D loss: 0.604136, acc: 64.84%] [G loss: 2.548314]\n",
      "epoch:6 step:5765 [D loss: 0.607421, acc: 65.62%] [G loss: 2.435427]\n",
      "epoch:6 step:5766 [D loss: 0.572503, acc: 75.78%] [G loss: 2.821482]\n",
      "epoch:6 step:5767 [D loss: 0.550057, acc: 75.78%] [G loss: 2.423665]\n",
      "epoch:6 step:5768 [D loss: 0.611424, acc: 68.75%] [G loss: 2.426060]\n",
      "epoch:6 step:5769 [D loss: 0.618895, acc: 65.62%] [G loss: 2.545793]\n",
      "epoch:6 step:5770 [D loss: 0.599123, acc: 68.75%] [G loss: 2.258292]\n",
      "epoch:6 step:5771 [D loss: 0.572152, acc: 66.41%] [G loss: 2.881341]\n",
      "epoch:6 step:5772 [D loss: 0.566097, acc: 67.97%] [G loss: 2.545344]\n",
      "epoch:6 step:5773 [D loss: 0.518793, acc: 73.44%] [G loss: 3.113294]\n",
      "epoch:6 step:5774 [D loss: 0.578279, acc: 67.97%] [G loss: 2.871661]\n",
      "epoch:6 step:5775 [D loss: 0.610390, acc: 63.28%] [G loss: 2.477976]\n",
      "epoch:6 step:5776 [D loss: 0.589243, acc: 67.19%] [G loss: 2.769790]\n",
      "epoch:6 step:5777 [D loss: 0.503428, acc: 76.56%] [G loss: 2.834533]\n",
      "epoch:6 step:5778 [D loss: 0.639051, acc: 63.28%] [G loss: 2.528919]\n",
      "epoch:6 step:5779 [D loss: 0.582945, acc: 70.31%] [G loss: 2.621063]\n",
      "epoch:6 step:5780 [D loss: 0.580174, acc: 68.75%] [G loss: 2.563709]\n",
      "epoch:6 step:5781 [D loss: 0.605775, acc: 70.31%] [G loss: 2.691617]\n",
      "epoch:6 step:5782 [D loss: 0.645363, acc: 60.16%] [G loss: 2.248722]\n",
      "epoch:6 step:5783 [D loss: 0.636838, acc: 61.72%] [G loss: 2.364291]\n",
      "epoch:6 step:5784 [D loss: 0.631190, acc: 67.97%] [G loss: 2.816328]\n",
      "epoch:6 step:5785 [D loss: 0.590584, acc: 70.31%] [G loss: 2.576525]\n",
      "epoch:6 step:5786 [D loss: 0.635807, acc: 64.06%] [G loss: 2.517793]\n",
      "epoch:6 step:5787 [D loss: 0.549073, acc: 74.22%] [G loss: 2.487727]\n",
      "epoch:6 step:5788 [D loss: 0.532055, acc: 78.91%] [G loss: 2.869637]\n",
      "epoch:6 step:5789 [D loss: 0.626580, acc: 64.84%] [G loss: 2.645216]\n",
      "epoch:6 step:5790 [D loss: 0.567882, acc: 75.00%] [G loss: 2.859359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5791 [D loss: 0.567192, acc: 69.53%] [G loss: 2.391638]\n",
      "epoch:6 step:5792 [D loss: 0.604802, acc: 67.19%] [G loss: 2.494487]\n",
      "epoch:6 step:5793 [D loss: 0.608604, acc: 66.41%] [G loss: 2.685503]\n",
      "epoch:6 step:5794 [D loss: 0.579290, acc: 67.97%] [G loss: 2.520151]\n",
      "epoch:6 step:5795 [D loss: 0.574646, acc: 71.09%] [G loss: 2.656663]\n",
      "epoch:6 step:5796 [D loss: 0.614146, acc: 67.97%] [G loss: 2.553263]\n",
      "epoch:6 step:5797 [D loss: 0.679081, acc: 64.84%] [G loss: 2.614733]\n",
      "epoch:6 step:5798 [D loss: 0.545670, acc: 75.78%] [G loss: 2.581253]\n",
      "epoch:6 step:5799 [D loss: 0.563371, acc: 69.53%] [G loss: 2.615359]\n",
      "epoch:6 step:5800 [D loss: 0.613867, acc: 62.50%] [G loss: 2.488624]\n",
      "epoch:6 step:5801 [D loss: 0.635952, acc: 67.97%] [G loss: 2.323731]\n",
      "epoch:6 step:5802 [D loss: 0.534462, acc: 71.88%] [G loss: 2.420993]\n",
      "epoch:6 step:5803 [D loss: 0.692612, acc: 58.59%] [G loss: 2.523357]\n",
      "epoch:6 step:5804 [D loss: 0.676625, acc: 63.28%] [G loss: 2.263067]\n",
      "epoch:6 step:5805 [D loss: 0.582195, acc: 68.75%] [G loss: 2.281030]\n",
      "epoch:6 step:5806 [D loss: 0.620469, acc: 70.31%] [G loss: 2.236004]\n",
      "epoch:6 step:5807 [D loss: 0.601886, acc: 69.53%] [G loss: 2.287175]\n",
      "epoch:6 step:5808 [D loss: 0.591139, acc: 67.97%] [G loss: 2.394330]\n",
      "epoch:6 step:5809 [D loss: 0.678653, acc: 59.38%] [G loss: 2.219462]\n",
      "epoch:6 step:5810 [D loss: 0.653672, acc: 60.94%] [G loss: 2.424682]\n",
      "epoch:6 step:5811 [D loss: 0.606450, acc: 63.28%] [G loss: 2.365133]\n",
      "epoch:6 step:5812 [D loss: 0.560303, acc: 73.44%] [G loss: 2.450157]\n",
      "epoch:6 step:5813 [D loss: 0.611474, acc: 67.97%] [G loss: 2.610348]\n",
      "epoch:6 step:5814 [D loss: 0.598736, acc: 67.97%] [G loss: 2.263743]\n",
      "epoch:6 step:5815 [D loss: 0.618151, acc: 66.41%] [G loss: 2.522038]\n",
      "epoch:6 step:5816 [D loss: 0.581950, acc: 70.31%] [G loss: 2.659232]\n",
      "epoch:6 step:5817 [D loss: 0.558142, acc: 73.44%] [G loss: 2.676586]\n",
      "epoch:6 step:5818 [D loss: 0.635752, acc: 71.09%] [G loss: 2.275748]\n",
      "epoch:6 step:5819 [D loss: 0.583048, acc: 71.88%] [G loss: 2.426990]\n",
      "epoch:6 step:5820 [D loss: 0.549092, acc: 72.66%] [G loss: 2.641726]\n",
      "epoch:6 step:5821 [D loss: 0.590487, acc: 65.62%] [G loss: 2.452834]\n",
      "epoch:6 step:5822 [D loss: 0.649630, acc: 64.06%] [G loss: 2.371998]\n",
      "epoch:6 step:5823 [D loss: 0.605956, acc: 68.75%] [G loss: 2.305930]\n",
      "epoch:6 step:5824 [D loss: 0.604019, acc: 68.75%] [G loss: 2.293423]\n",
      "epoch:6 step:5825 [D loss: 0.615157, acc: 64.84%] [G loss: 2.351888]\n",
      "epoch:6 step:5826 [D loss: 0.647947, acc: 60.94%] [G loss: 2.132307]\n",
      "epoch:6 step:5827 [D loss: 0.569338, acc: 74.22%] [G loss: 2.295001]\n",
      "epoch:6 step:5828 [D loss: 0.552540, acc: 74.22%] [G loss: 2.654199]\n",
      "epoch:6 step:5829 [D loss: 0.559049, acc: 75.78%] [G loss: 2.813860]\n",
      "epoch:6 step:5830 [D loss: 0.546738, acc: 75.78%] [G loss: 3.021694]\n",
      "epoch:6 step:5831 [D loss: 0.611328, acc: 65.62%] [G loss: 2.611500]\n",
      "epoch:6 step:5832 [D loss: 0.541406, acc: 75.00%] [G loss: 2.325028]\n",
      "epoch:6 step:5833 [D loss: 0.732639, acc: 57.03%] [G loss: 2.328872]\n",
      "epoch:6 step:5834 [D loss: 0.605262, acc: 68.75%] [G loss: 2.415998]\n",
      "epoch:6 step:5835 [D loss: 0.640206, acc: 61.72%] [G loss: 2.562075]\n",
      "epoch:6 step:5836 [D loss: 0.606517, acc: 71.09%] [G loss: 2.368204]\n",
      "epoch:6 step:5837 [D loss: 0.621776, acc: 70.31%] [G loss: 2.543274]\n",
      "epoch:6 step:5838 [D loss: 0.623483, acc: 62.50%] [G loss: 2.584455]\n",
      "epoch:6 step:5839 [D loss: 0.548477, acc: 71.88%] [G loss: 2.799370]\n",
      "epoch:6 step:5840 [D loss: 0.552707, acc: 78.12%] [G loss: 2.612889]\n",
      "epoch:6 step:5841 [D loss: 0.538448, acc: 75.78%] [G loss: 2.700475]\n",
      "epoch:6 step:5842 [D loss: 0.655437, acc: 65.62%] [G loss: 2.337009]\n",
      "epoch:6 step:5843 [D loss: 0.634710, acc: 64.06%] [G loss: 2.518145]\n",
      "epoch:6 step:5844 [D loss: 0.613930, acc: 66.41%] [G loss: 2.153009]\n",
      "epoch:6 step:5845 [D loss: 0.585365, acc: 70.31%] [G loss: 2.525661]\n",
      "epoch:6 step:5846 [D loss: 0.583208, acc: 66.41%] [G loss: 2.408828]\n",
      "epoch:6 step:5847 [D loss: 0.685700, acc: 56.25%] [G loss: 2.470426]\n",
      "epoch:6 step:5848 [D loss: 0.553964, acc: 69.53%] [G loss: 2.353535]\n",
      "epoch:6 step:5849 [D loss: 0.551056, acc: 69.53%] [G loss: 2.419017]\n",
      "epoch:6 step:5850 [D loss: 0.594346, acc: 69.53%] [G loss: 2.349921]\n",
      "epoch:6 step:5851 [D loss: 0.637004, acc: 67.19%] [G loss: 2.450786]\n",
      "epoch:6 step:5852 [D loss: 0.632785, acc: 68.75%] [G loss: 2.494807]\n",
      "epoch:6 step:5853 [D loss: 0.536593, acc: 75.78%] [G loss: 2.756793]\n",
      "epoch:6 step:5854 [D loss: 0.524231, acc: 77.34%] [G loss: 3.136790]\n",
      "epoch:6 step:5855 [D loss: 0.668749, acc: 71.09%] [G loss: 2.421459]\n",
      "epoch:6 step:5856 [D loss: 0.637731, acc: 64.06%] [G loss: 2.248379]\n",
      "epoch:6 step:5857 [D loss: 0.625685, acc: 63.28%] [G loss: 2.318033]\n",
      "epoch:6 step:5858 [D loss: 0.563711, acc: 67.19%] [G loss: 2.540045]\n",
      "epoch:6 step:5859 [D loss: 0.595095, acc: 71.88%] [G loss: 2.681171]\n",
      "epoch:6 step:5860 [D loss: 0.623027, acc: 71.09%] [G loss: 2.327859]\n",
      "epoch:6 step:5861 [D loss: 0.595996, acc: 70.31%] [G loss: 2.510842]\n",
      "epoch:6 step:5862 [D loss: 0.607171, acc: 67.19%] [G loss: 2.770362]\n",
      "epoch:6 step:5863 [D loss: 0.567898, acc: 65.62%] [G loss: 2.585065]\n",
      "epoch:6 step:5864 [D loss: 0.536571, acc: 75.00%] [G loss: 2.505685]\n",
      "epoch:6 step:5865 [D loss: 0.588881, acc: 67.19%] [G loss: 2.527284]\n",
      "epoch:6 step:5866 [D loss: 0.540231, acc: 71.88%] [G loss: 2.437750]\n",
      "epoch:6 step:5867 [D loss: 0.540087, acc: 71.09%] [G loss: 2.432721]\n",
      "epoch:6 step:5868 [D loss: 0.585142, acc: 71.09%] [G loss: 2.497469]\n",
      "epoch:6 step:5869 [D loss: 0.562782, acc: 71.09%] [G loss: 2.570820]\n",
      "epoch:6 step:5870 [D loss: 0.713401, acc: 56.25%] [G loss: 2.530088]\n",
      "epoch:6 step:5871 [D loss: 0.682784, acc: 60.94%] [G loss: 2.092169]\n",
      "epoch:6 step:5872 [D loss: 0.703424, acc: 57.03%] [G loss: 2.258522]\n",
      "epoch:6 step:5873 [D loss: 0.646521, acc: 62.50%] [G loss: 2.124658]\n",
      "epoch:6 step:5874 [D loss: 0.593426, acc: 67.97%] [G loss: 2.225090]\n",
      "epoch:6 step:5875 [D loss: 0.614486, acc: 63.28%] [G loss: 2.272185]\n",
      "epoch:6 step:5876 [D loss: 0.604878, acc: 67.97%] [G loss: 2.152055]\n",
      "epoch:6 step:5877 [D loss: 0.588752, acc: 67.97%] [G loss: 2.495816]\n",
      "epoch:6 step:5878 [D loss: 0.636125, acc: 59.38%] [G loss: 2.309540]\n",
      "epoch:6 step:5879 [D loss: 0.571592, acc: 68.75%] [G loss: 2.501487]\n",
      "epoch:6 step:5880 [D loss: 0.582818, acc: 69.53%] [G loss: 2.481984]\n",
      "epoch:6 step:5881 [D loss: 0.602575, acc: 65.62%] [G loss: 2.543908]\n",
      "epoch:6 step:5882 [D loss: 0.586303, acc: 65.62%] [G loss: 2.656142]\n",
      "epoch:6 step:5883 [D loss: 0.669128, acc: 60.94%] [G loss: 2.688931]\n",
      "epoch:6 step:5884 [D loss: 0.570985, acc: 72.66%] [G loss: 2.551234]\n",
      "epoch:6 step:5885 [D loss: 0.636057, acc: 62.50%] [G loss: 2.220890]\n",
      "epoch:6 step:5886 [D loss: 0.542012, acc: 75.00%] [G loss: 2.693593]\n",
      "epoch:6 step:5887 [D loss: 0.640257, acc: 62.50%] [G loss: 2.273643]\n",
      "epoch:6 step:5888 [D loss: 0.568900, acc: 64.06%] [G loss: 2.307527]\n",
      "epoch:6 step:5889 [D loss: 0.622086, acc: 64.06%] [G loss: 2.428025]\n",
      "epoch:6 step:5890 [D loss: 0.636386, acc: 62.50%] [G loss: 2.380350]\n",
      "epoch:6 step:5891 [D loss: 0.612258, acc: 64.84%] [G loss: 2.182607]\n",
      "epoch:6 step:5892 [D loss: 0.575727, acc: 71.88%] [G loss: 2.604749]\n",
      "epoch:6 step:5893 [D loss: 0.603125, acc: 70.31%] [G loss: 2.297573]\n",
      "epoch:6 step:5894 [D loss: 0.650648, acc: 64.84%] [G loss: 2.650191]\n",
      "epoch:6 step:5895 [D loss: 0.577412, acc: 72.66%] [G loss: 2.342706]\n",
      "epoch:6 step:5896 [D loss: 0.553515, acc: 70.31%] [G loss: 2.571414]\n",
      "epoch:6 step:5897 [D loss: 0.583896, acc: 68.75%] [G loss: 2.387179]\n",
      "epoch:6 step:5898 [D loss: 0.568813, acc: 72.66%] [G loss: 2.496968]\n",
      "epoch:6 step:5899 [D loss: 0.578756, acc: 67.19%] [G loss: 2.212450]\n",
      "epoch:6 step:5900 [D loss: 0.679333, acc: 60.94%] [G loss: 2.281792]\n",
      "epoch:6 step:5901 [D loss: 0.567303, acc: 68.75%] [G loss: 2.577500]\n",
      "epoch:6 step:5902 [D loss: 0.515157, acc: 74.22%] [G loss: 2.465734]\n",
      "epoch:6 step:5903 [D loss: 0.548480, acc: 73.44%] [G loss: 2.506765]\n",
      "epoch:6 step:5904 [D loss: 0.562423, acc: 72.66%] [G loss: 2.554684]\n",
      "epoch:6 step:5905 [D loss: 0.575352, acc: 71.88%] [G loss: 2.474737]\n",
      "epoch:6 step:5906 [D loss: 0.596359, acc: 70.31%] [G loss: 2.430515]\n",
      "epoch:6 step:5907 [D loss: 0.543634, acc: 71.09%] [G loss: 2.310641]\n",
      "epoch:6 step:5908 [D loss: 0.603117, acc: 71.09%] [G loss: 2.754323]\n",
      "epoch:6 step:5909 [D loss: 0.613230, acc: 67.19%] [G loss: 2.397543]\n",
      "epoch:6 step:5910 [D loss: 0.612017, acc: 64.84%] [G loss: 2.284702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5911 [D loss: 0.564875, acc: 66.41%] [G loss: 2.461236]\n",
      "epoch:6 step:5912 [D loss: 0.605711, acc: 63.28%] [G loss: 2.295568]\n",
      "epoch:6 step:5913 [D loss: 0.517376, acc: 77.34%] [G loss: 2.646811]\n",
      "epoch:6 step:5914 [D loss: 0.618844, acc: 66.41%] [G loss: 2.533323]\n",
      "epoch:6 step:5915 [D loss: 0.608665, acc: 67.97%] [G loss: 2.578847]\n",
      "epoch:6 step:5916 [D loss: 0.574015, acc: 70.31%] [G loss: 2.552750]\n",
      "epoch:6 step:5917 [D loss: 0.518758, acc: 75.78%] [G loss: 3.111746]\n",
      "epoch:6 step:5918 [D loss: 0.591989, acc: 71.09%] [G loss: 2.494967]\n",
      "epoch:6 step:5919 [D loss: 0.532652, acc: 69.53%] [G loss: 2.760506]\n",
      "epoch:6 step:5920 [D loss: 0.530010, acc: 77.34%] [G loss: 2.652008]\n",
      "epoch:6 step:5921 [D loss: 0.605237, acc: 66.41%] [G loss: 2.246471]\n",
      "epoch:6 step:5922 [D loss: 0.575547, acc: 72.66%] [G loss: 2.614511]\n",
      "epoch:6 step:5923 [D loss: 0.730753, acc: 53.12%] [G loss: 2.329153]\n",
      "epoch:6 step:5924 [D loss: 0.587487, acc: 67.97%] [G loss: 2.461070]\n",
      "epoch:6 step:5925 [D loss: 0.620423, acc: 67.19%] [G loss: 2.291352]\n",
      "epoch:6 step:5926 [D loss: 0.528329, acc: 73.44%] [G loss: 2.255714]\n",
      "epoch:6 step:5927 [D loss: 0.609732, acc: 66.41%] [G loss: 2.419866]\n",
      "epoch:6 step:5928 [D loss: 0.608721, acc: 67.19%] [G loss: 2.245556]\n",
      "epoch:6 step:5929 [D loss: 0.564533, acc: 68.75%] [G loss: 2.472034]\n",
      "epoch:6 step:5930 [D loss: 0.603223, acc: 68.75%] [G loss: 2.477722]\n",
      "epoch:6 step:5931 [D loss: 0.564305, acc: 71.09%] [G loss: 2.595529]\n",
      "epoch:6 step:5932 [D loss: 0.642133, acc: 62.50%] [G loss: 2.510380]\n",
      "epoch:6 step:5933 [D loss: 0.539105, acc: 72.66%] [G loss: 2.658389]\n",
      "epoch:6 step:5934 [D loss: 0.564527, acc: 68.75%] [G loss: 3.147008]\n",
      "epoch:6 step:5935 [D loss: 0.573674, acc: 71.88%] [G loss: 2.870779]\n",
      "epoch:6 step:5936 [D loss: 0.467156, acc: 78.12%] [G loss: 2.979195]\n",
      "epoch:6 step:5937 [D loss: 0.492360, acc: 76.56%] [G loss: 3.068979]\n",
      "epoch:6 step:5938 [D loss: 0.619619, acc: 64.06%] [G loss: 2.384677]\n",
      "epoch:6 step:5939 [D loss: 0.611841, acc: 71.88%] [G loss: 2.680284]\n",
      "epoch:6 step:5940 [D loss: 0.565708, acc: 71.09%] [G loss: 2.628047]\n",
      "epoch:6 step:5941 [D loss: 0.540760, acc: 72.66%] [G loss: 2.654447]\n",
      "epoch:6 step:5942 [D loss: 0.702829, acc: 59.38%] [G loss: 2.653572]\n",
      "epoch:6 step:5943 [D loss: 0.524151, acc: 75.00%] [G loss: 3.008256]\n",
      "epoch:6 step:5944 [D loss: 0.647796, acc: 59.38%] [G loss: 2.502356]\n",
      "epoch:6 step:5945 [D loss: 0.642410, acc: 64.06%] [G loss: 2.144508]\n",
      "epoch:6 step:5946 [D loss: 0.621150, acc: 61.72%] [G loss: 2.455287]\n",
      "epoch:6 step:5947 [D loss: 0.611719, acc: 68.75%] [G loss: 2.620720]\n",
      "epoch:6 step:5948 [D loss: 0.621925, acc: 66.41%] [G loss: 2.603737]\n",
      "epoch:6 step:5949 [D loss: 0.613098, acc: 68.75%] [G loss: 2.608113]\n",
      "epoch:6 step:5950 [D loss: 0.607481, acc: 64.84%] [G loss: 2.207163]\n",
      "epoch:6 step:5951 [D loss: 0.574868, acc: 69.53%] [G loss: 2.574750]\n",
      "epoch:6 step:5952 [D loss: 0.600825, acc: 68.75%] [G loss: 2.532887]\n",
      "epoch:6 step:5953 [D loss: 0.564165, acc: 71.88%] [G loss: 2.496475]\n",
      "epoch:6 step:5954 [D loss: 0.579384, acc: 71.88%] [G loss: 2.417019]\n",
      "epoch:6 step:5955 [D loss: 0.520198, acc: 75.00%] [G loss: 2.754623]\n",
      "epoch:6 step:5956 [D loss: 0.609018, acc: 71.09%] [G loss: 2.373632]\n",
      "epoch:6 step:5957 [D loss: 0.595847, acc: 69.53%] [G loss: 2.580284]\n",
      "epoch:6 step:5958 [D loss: 0.543711, acc: 71.09%] [G loss: 2.679803]\n",
      "epoch:6 step:5959 [D loss: 0.549657, acc: 72.66%] [G loss: 2.735201]\n",
      "epoch:6 step:5960 [D loss: 0.599541, acc: 67.97%] [G loss: 2.815029]\n",
      "epoch:6 step:5961 [D loss: 0.554570, acc: 68.75%] [G loss: 2.833159]\n",
      "epoch:6 step:5962 [D loss: 0.581336, acc: 72.66%] [G loss: 2.648295]\n",
      "epoch:6 step:5963 [D loss: 0.622748, acc: 66.41%] [G loss: 2.157080]\n",
      "epoch:6 step:5964 [D loss: 0.712330, acc: 55.47%] [G loss: 2.269748]\n",
      "epoch:6 step:5965 [D loss: 0.608345, acc: 69.53%] [G loss: 2.598177]\n",
      "epoch:6 step:5966 [D loss: 0.561498, acc: 71.09%] [G loss: 2.862792]\n",
      "epoch:6 step:5967 [D loss: 0.542320, acc: 76.56%] [G loss: 2.598020]\n",
      "epoch:6 step:5968 [D loss: 0.541777, acc: 74.22%] [G loss: 3.124045]\n",
      "epoch:6 step:5969 [D loss: 0.610424, acc: 65.62%] [G loss: 2.913620]\n",
      "epoch:6 step:5970 [D loss: 0.662687, acc: 67.19%] [G loss: 2.378227]\n",
      "epoch:6 step:5971 [D loss: 0.590557, acc: 67.19%] [G loss: 2.378680]\n",
      "epoch:6 step:5972 [D loss: 0.630711, acc: 69.53%] [G loss: 2.656115]\n",
      "epoch:6 step:5973 [D loss: 0.649000, acc: 68.75%] [G loss: 2.597789]\n",
      "epoch:6 step:5974 [D loss: 0.661390, acc: 62.50%] [G loss: 2.214642]\n",
      "epoch:6 step:5975 [D loss: 0.630184, acc: 63.28%] [G loss: 2.619646]\n",
      "epoch:6 step:5976 [D loss: 0.529636, acc: 76.56%] [G loss: 2.944384]\n",
      "epoch:6 step:5977 [D loss: 0.612145, acc: 65.62%] [G loss: 2.542471]\n",
      "epoch:6 step:5978 [D loss: 0.684462, acc: 57.81%] [G loss: 2.213017]\n",
      "epoch:6 step:5979 [D loss: 0.539941, acc: 71.09%] [G loss: 2.515379]\n",
      "epoch:6 step:5980 [D loss: 0.589262, acc: 71.88%] [G loss: 2.785120]\n",
      "epoch:6 step:5981 [D loss: 0.572040, acc: 71.88%] [G loss: 2.469238]\n",
      "epoch:6 step:5982 [D loss: 0.531864, acc: 74.22%] [G loss: 2.735134]\n",
      "epoch:6 step:5983 [D loss: 0.588187, acc: 67.97%] [G loss: 2.458926]\n",
      "epoch:6 step:5984 [D loss: 0.646672, acc: 60.16%] [G loss: 2.562839]\n",
      "epoch:6 step:5985 [D loss: 0.608050, acc: 67.19%] [G loss: 2.565024]\n",
      "epoch:6 step:5986 [D loss: 0.547683, acc: 75.78%] [G loss: 2.507438]\n",
      "epoch:6 step:5987 [D loss: 0.561618, acc: 70.31%] [G loss: 2.491256]\n",
      "epoch:6 step:5988 [D loss: 0.630359, acc: 61.72%] [G loss: 2.680783]\n",
      "epoch:6 step:5989 [D loss: 0.527483, acc: 75.00%] [G loss: 2.552684]\n",
      "epoch:6 step:5990 [D loss: 0.525986, acc: 74.22%] [G loss: 2.587981]\n",
      "epoch:6 step:5991 [D loss: 0.511517, acc: 75.00%] [G loss: 2.684624]\n",
      "epoch:6 step:5992 [D loss: 0.483003, acc: 75.78%] [G loss: 3.088077]\n",
      "epoch:6 step:5993 [D loss: 0.584167, acc: 67.19%] [G loss: 3.109507]\n",
      "epoch:6 step:5994 [D loss: 0.608400, acc: 65.62%] [G loss: 2.829816]\n",
      "epoch:6 step:5995 [D loss: 0.636580, acc: 64.84%] [G loss: 2.425806]\n",
      "epoch:6 step:5996 [D loss: 0.567305, acc: 75.00%] [G loss: 2.958889]\n",
      "epoch:6 step:5997 [D loss: 0.642549, acc: 65.62%] [G loss: 2.350857]\n",
      "epoch:6 step:5998 [D loss: 0.669947, acc: 60.94%] [G loss: 2.275663]\n",
      "epoch:6 step:5999 [D loss: 0.702787, acc: 59.38%] [G loss: 2.239395]\n",
      "epoch:6 step:6000 [D loss: 0.653262, acc: 64.06%] [G loss: 2.451729]\n",
      "epoch:6 step:6001 [D loss: 0.530929, acc: 71.88%] [G loss: 2.438788]\n",
      "epoch:6 step:6002 [D loss: 0.660436, acc: 62.50%] [G loss: 2.732147]\n",
      "epoch:6 step:6003 [D loss: 0.548230, acc: 69.53%] [G loss: 2.881233]\n",
      "epoch:6 step:6004 [D loss: 0.571264, acc: 72.66%] [G loss: 2.343678]\n",
      "epoch:6 step:6005 [D loss: 0.644227, acc: 64.06%] [G loss: 2.514735]\n",
      "epoch:6 step:6006 [D loss: 0.507269, acc: 74.22%] [G loss: 2.679721]\n",
      "epoch:6 step:6007 [D loss: 0.508644, acc: 78.91%] [G loss: 2.790141]\n",
      "epoch:6 step:6008 [D loss: 0.634589, acc: 61.72%] [G loss: 2.326284]\n",
      "epoch:6 step:6009 [D loss: 0.630918, acc: 66.41%] [G loss: 2.463111]\n",
      "epoch:6 step:6010 [D loss: 0.630176, acc: 67.19%] [G loss: 2.331288]\n",
      "epoch:6 step:6011 [D loss: 0.653622, acc: 59.38%] [G loss: 2.309908]\n",
      "epoch:6 step:6012 [D loss: 0.603302, acc: 61.72%] [G loss: 2.350150]\n",
      "epoch:6 step:6013 [D loss: 0.645070, acc: 67.97%] [G loss: 2.307026]\n",
      "epoch:6 step:6014 [D loss: 0.530683, acc: 74.22%] [G loss: 2.743301]\n",
      "epoch:6 step:6015 [D loss: 0.606007, acc: 66.41%] [G loss: 2.431348]\n",
      "epoch:6 step:6016 [D loss: 0.651909, acc: 60.94%] [G loss: 2.472516]\n",
      "epoch:6 step:6017 [D loss: 0.616665, acc: 70.31%] [G loss: 2.805768]\n",
      "epoch:6 step:6018 [D loss: 0.712899, acc: 61.72%] [G loss: 2.206244]\n",
      "epoch:6 step:6019 [D loss: 0.577776, acc: 69.53%] [G loss: 2.435117]\n",
      "epoch:6 step:6020 [D loss: 0.615241, acc: 64.84%] [G loss: 2.520844]\n",
      "epoch:6 step:6021 [D loss: 0.655302, acc: 67.19%] [G loss: 2.551414]\n",
      "epoch:6 step:6022 [D loss: 0.630170, acc: 60.94%] [G loss: 2.394685]\n",
      "epoch:6 step:6023 [D loss: 0.598867, acc: 65.62%] [G loss: 2.329755]\n",
      "epoch:6 step:6024 [D loss: 0.590051, acc: 67.19%] [G loss: 2.568312]\n",
      "epoch:6 step:6025 [D loss: 0.589177, acc: 71.88%] [G loss: 2.347751]\n",
      "epoch:6 step:6026 [D loss: 0.648548, acc: 60.16%] [G loss: 2.568644]\n",
      "epoch:6 step:6027 [D loss: 0.660459, acc: 67.19%] [G loss: 2.624506]\n",
      "epoch:6 step:6028 [D loss: 0.551560, acc: 71.88%] [G loss: 2.542379]\n",
      "epoch:6 step:6029 [D loss: 0.650281, acc: 63.28%] [G loss: 2.500434]\n",
      "epoch:6 step:6030 [D loss: 0.596606, acc: 64.84%] [G loss: 2.450330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6031 [D loss: 0.545799, acc: 71.88%] [G loss: 2.505725]\n",
      "epoch:6 step:6032 [D loss: 0.557290, acc: 70.31%] [G loss: 2.469813]\n",
      "epoch:6 step:6033 [D loss: 0.558137, acc: 72.66%] [G loss: 2.440609]\n",
      "epoch:6 step:6034 [D loss: 0.621516, acc: 63.28%] [G loss: 2.415948]\n",
      "epoch:6 step:6035 [D loss: 0.610607, acc: 67.97%] [G loss: 2.482719]\n",
      "epoch:6 step:6036 [D loss: 0.632391, acc: 64.84%] [G loss: 2.497294]\n",
      "epoch:6 step:6037 [D loss: 0.566544, acc: 75.78%] [G loss: 2.529469]\n",
      "epoch:6 step:6038 [D loss: 0.556505, acc: 73.44%] [G loss: 2.454540]\n",
      "epoch:6 step:6039 [D loss: 0.761740, acc: 61.72%] [G loss: 2.315382]\n",
      "epoch:6 step:6040 [D loss: 0.602975, acc: 64.84%] [G loss: 2.649949]\n",
      "epoch:6 step:6041 [D loss: 0.639307, acc: 68.75%] [G loss: 2.587337]\n",
      "epoch:6 step:6042 [D loss: 0.668300, acc: 61.72%] [G loss: 2.259595]\n",
      "epoch:6 step:6043 [D loss: 0.658301, acc: 60.16%] [G loss: 2.376457]\n",
      "epoch:6 step:6044 [D loss: 0.734945, acc: 59.38%] [G loss: 2.233065]\n",
      "epoch:6 step:6045 [D loss: 0.717174, acc: 60.94%] [G loss: 2.195925]\n",
      "epoch:6 step:6046 [D loss: 0.628674, acc: 66.41%] [G loss: 2.289644]\n",
      "epoch:6 step:6047 [D loss: 0.728515, acc: 54.69%] [G loss: 2.188519]\n",
      "epoch:6 step:6048 [D loss: 0.576056, acc: 74.22%] [G loss: 2.379501]\n",
      "epoch:6 step:6049 [D loss: 0.619117, acc: 67.19%] [G loss: 2.358284]\n",
      "epoch:6 step:6050 [D loss: 0.592602, acc: 66.41%] [G loss: 2.603749]\n",
      "epoch:6 step:6051 [D loss: 0.515568, acc: 72.66%] [G loss: 2.843478]\n",
      "epoch:6 step:6052 [D loss: 0.551113, acc: 73.44%] [G loss: 2.820916]\n",
      "epoch:6 step:6053 [D loss: 0.582485, acc: 66.41%] [G loss: 2.567551]\n",
      "epoch:6 step:6054 [D loss: 0.570351, acc: 68.75%] [G loss: 2.403817]\n",
      "epoch:6 step:6055 [D loss: 0.627606, acc: 67.97%] [G loss: 2.169571]\n",
      "epoch:6 step:6056 [D loss: 0.573277, acc: 70.31%] [G loss: 2.352887]\n",
      "epoch:6 step:6057 [D loss: 0.582606, acc: 68.75%] [G loss: 2.716851]\n",
      "epoch:6 step:6058 [D loss: 0.642756, acc: 65.62%] [G loss: 2.399569]\n",
      "epoch:6 step:6059 [D loss: 0.677543, acc: 60.16%] [G loss: 2.283460]\n",
      "epoch:6 step:6060 [D loss: 0.611301, acc: 65.62%] [G loss: 2.076914]\n",
      "epoch:6 step:6061 [D loss: 0.573516, acc: 68.75%] [G loss: 2.268265]\n",
      "epoch:6 step:6062 [D loss: 0.635064, acc: 61.72%] [G loss: 2.426389]\n",
      "epoch:6 step:6063 [D loss: 0.603756, acc: 64.84%] [G loss: 2.173372]\n",
      "epoch:6 step:6064 [D loss: 0.596380, acc: 70.31%] [G loss: 2.182734]\n",
      "epoch:6 step:6065 [D loss: 0.632127, acc: 63.28%] [G loss: 2.370471]\n",
      "epoch:6 step:6066 [D loss: 0.618377, acc: 66.41%] [G loss: 2.403425]\n",
      "epoch:6 step:6067 [D loss: 0.595240, acc: 68.75%] [G loss: 2.364690]\n",
      "epoch:6 step:6068 [D loss: 0.615099, acc: 66.41%] [G loss: 2.328491]\n",
      "epoch:6 step:6069 [D loss: 0.586651, acc: 66.41%] [G loss: 2.366877]\n",
      "epoch:6 step:6070 [D loss: 0.633447, acc: 61.72%] [G loss: 2.112206]\n",
      "epoch:6 step:6071 [D loss: 0.610420, acc: 64.84%] [G loss: 2.444744]\n",
      "epoch:6 step:6072 [D loss: 0.586432, acc: 71.09%] [G loss: 2.659312]\n",
      "epoch:6 step:6073 [D loss: 0.591446, acc: 67.19%] [G loss: 2.761189]\n",
      "epoch:6 step:6074 [D loss: 0.567023, acc: 69.53%] [G loss: 2.498981]\n",
      "epoch:6 step:6075 [D loss: 0.489865, acc: 79.69%] [G loss: 2.724741]\n",
      "epoch:6 step:6076 [D loss: 0.542731, acc: 72.66%] [G loss: 2.527285]\n",
      "epoch:6 step:6077 [D loss: 0.543507, acc: 71.88%] [G loss: 2.535476]\n",
      "epoch:6 step:6078 [D loss: 0.622463, acc: 67.19%] [G loss: 2.538482]\n",
      "epoch:6 step:6079 [D loss: 0.586094, acc: 73.44%] [G loss: 2.799049]\n",
      "epoch:6 step:6080 [D loss: 0.661548, acc: 60.16%] [G loss: 2.379298]\n",
      "epoch:6 step:6081 [D loss: 0.655902, acc: 62.50%] [G loss: 2.304705]\n",
      "epoch:6 step:6082 [D loss: 0.599176, acc: 67.97%] [G loss: 2.411704]\n",
      "epoch:6 step:6083 [D loss: 0.682334, acc: 58.59%] [G loss: 2.251512]\n",
      "epoch:6 step:6084 [D loss: 0.537570, acc: 71.88%] [G loss: 2.383760]\n",
      "epoch:6 step:6085 [D loss: 0.581769, acc: 66.41%] [G loss: 2.192072]\n",
      "epoch:6 step:6086 [D loss: 0.562090, acc: 71.88%] [G loss: 2.581843]\n",
      "epoch:6 step:6087 [D loss: 0.688484, acc: 57.03%] [G loss: 2.589578]\n",
      "epoch:6 step:6088 [D loss: 0.557504, acc: 71.09%] [G loss: 2.595269]\n",
      "epoch:6 step:6089 [D loss: 0.511706, acc: 78.91%] [G loss: 2.744903]\n",
      "epoch:6 step:6090 [D loss: 0.592505, acc: 68.75%] [G loss: 2.707231]\n",
      "epoch:6 step:6091 [D loss: 0.640091, acc: 62.50%] [G loss: 2.606038]\n",
      "epoch:6 step:6092 [D loss: 0.592101, acc: 62.50%] [G loss: 2.396202]\n",
      "epoch:6 step:6093 [D loss: 0.531563, acc: 72.66%] [G loss: 3.048062]\n",
      "epoch:6 step:6094 [D loss: 0.555297, acc: 71.09%] [G loss: 3.259570]\n",
      "epoch:6 step:6095 [D loss: 0.620106, acc: 67.97%] [G loss: 2.452063]\n",
      "epoch:6 step:6096 [D loss: 0.544704, acc: 71.88%] [G loss: 2.566044]\n",
      "epoch:6 step:6097 [D loss: 0.594829, acc: 65.62%] [G loss: 2.664116]\n",
      "epoch:6 step:6098 [D loss: 0.570594, acc: 67.19%] [G loss: 2.622905]\n",
      "epoch:6 step:6099 [D loss: 0.732738, acc: 61.72%] [G loss: 2.409076]\n",
      "epoch:6 step:6100 [D loss: 0.664358, acc: 64.84%] [G loss: 2.433618]\n",
      "epoch:6 step:6101 [D loss: 0.628219, acc: 68.75%] [G loss: 2.857827]\n",
      "epoch:6 step:6102 [D loss: 0.567244, acc: 72.66%] [G loss: 2.271705]\n",
      "epoch:6 step:6103 [D loss: 0.627118, acc: 67.97%] [G loss: 2.496852]\n",
      "epoch:6 step:6104 [D loss: 0.603402, acc: 62.50%] [G loss: 2.398591]\n",
      "epoch:6 step:6105 [D loss: 0.644050, acc: 60.94%] [G loss: 2.526114]\n",
      "epoch:6 step:6106 [D loss: 0.569157, acc: 65.62%] [G loss: 2.587769]\n",
      "epoch:6 step:6107 [D loss: 0.649258, acc: 60.16%] [G loss: 2.360867]\n",
      "epoch:6 step:6108 [D loss: 0.625291, acc: 61.72%] [G loss: 2.245976]\n",
      "epoch:6 step:6109 [D loss: 0.653579, acc: 64.06%] [G loss: 2.266260]\n",
      "epoch:6 step:6110 [D loss: 0.489923, acc: 78.12%] [G loss: 2.480294]\n",
      "epoch:6 step:6111 [D loss: 0.591195, acc: 68.75%] [G loss: 2.560651]\n",
      "epoch:6 step:6112 [D loss: 0.602250, acc: 64.06%] [G loss: 2.619929]\n",
      "epoch:6 step:6113 [D loss: 0.605024, acc: 66.41%] [G loss: 2.361102]\n",
      "epoch:6 step:6114 [D loss: 0.567869, acc: 74.22%] [G loss: 2.274760]\n",
      "epoch:6 step:6115 [D loss: 0.576094, acc: 66.41%] [G loss: 2.537092]\n",
      "epoch:6 step:6116 [D loss: 0.556455, acc: 73.44%] [G loss: 2.677848]\n",
      "epoch:6 step:6117 [D loss: 0.493584, acc: 76.56%] [G loss: 2.465720]\n",
      "epoch:6 step:6118 [D loss: 0.632672, acc: 63.28%] [G loss: 2.572047]\n",
      "epoch:6 step:6119 [D loss: 0.567216, acc: 71.88%] [G loss: 2.585655]\n",
      "epoch:6 step:6120 [D loss: 0.543868, acc: 76.56%] [G loss: 2.712068]\n",
      "epoch:6 step:6121 [D loss: 0.527769, acc: 72.66%] [G loss: 2.931014]\n",
      "epoch:6 step:6122 [D loss: 0.665184, acc: 64.84%] [G loss: 2.097211]\n",
      "epoch:6 step:6123 [D loss: 0.657671, acc: 63.28%] [G loss: 2.435355]\n",
      "epoch:6 step:6124 [D loss: 0.666999, acc: 66.41%] [G loss: 2.046083]\n",
      "epoch:6 step:6125 [D loss: 0.624282, acc: 63.28%] [G loss: 2.450320]\n",
      "epoch:6 step:6126 [D loss: 0.568785, acc: 64.84%] [G loss: 2.330212]\n",
      "epoch:6 step:6127 [D loss: 0.607126, acc: 63.28%] [G loss: 2.332371]\n",
      "epoch:6 step:6128 [D loss: 0.636901, acc: 62.50%] [G loss: 2.522748]\n",
      "epoch:6 step:6129 [D loss: 0.636244, acc: 62.50%] [G loss: 2.382147]\n",
      "epoch:6 step:6130 [D loss: 0.533963, acc: 73.44%] [G loss: 2.786381]\n",
      "epoch:6 step:6131 [D loss: 0.669234, acc: 64.84%] [G loss: 2.342066]\n",
      "epoch:6 step:6132 [D loss: 0.564968, acc: 75.78%] [G loss: 2.331282]\n",
      "epoch:6 step:6133 [D loss: 0.607920, acc: 68.75%] [G loss: 2.187536]\n",
      "epoch:6 step:6134 [D loss: 0.682182, acc: 64.06%] [G loss: 2.412721]\n",
      "epoch:6 step:6135 [D loss: 0.701268, acc: 59.38%] [G loss: 2.291940]\n",
      "epoch:6 step:6136 [D loss: 0.627904, acc: 63.28%] [G loss: 2.273237]\n",
      "epoch:6 step:6137 [D loss: 0.557934, acc: 71.09%] [G loss: 2.430969]\n",
      "epoch:6 step:6138 [D loss: 0.584663, acc: 69.53%] [G loss: 2.345880]\n",
      "epoch:6 step:6139 [D loss: 0.653255, acc: 61.72%] [G loss: 2.216748]\n",
      "epoch:6 step:6140 [D loss: 0.608522, acc: 71.88%] [G loss: 2.301674]\n",
      "epoch:6 step:6141 [D loss: 0.581201, acc: 68.75%] [G loss: 2.324146]\n",
      "epoch:6 step:6142 [D loss: 0.555015, acc: 72.66%] [G loss: 2.509827]\n",
      "epoch:6 step:6143 [D loss: 0.550144, acc: 70.31%] [G loss: 2.503855]\n",
      "epoch:6 step:6144 [D loss: 0.533100, acc: 73.44%] [G loss: 2.468299]\n",
      "epoch:6 step:6145 [D loss: 0.618954, acc: 61.72%] [G loss: 2.698887]\n",
      "epoch:6 step:6146 [D loss: 0.573685, acc: 67.19%] [G loss: 2.677568]\n",
      "epoch:6 step:6147 [D loss: 0.573929, acc: 69.53%] [G loss: 2.530046]\n",
      "epoch:6 step:6148 [D loss: 0.571971, acc: 71.88%] [G loss: 2.664493]\n",
      "epoch:6 step:6149 [D loss: 0.726743, acc: 60.16%] [G loss: 2.327985]\n",
      "epoch:6 step:6150 [D loss: 0.770181, acc: 54.69%] [G loss: 2.414547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6151 [D loss: 0.666985, acc: 57.81%] [G loss: 1.997940]\n",
      "epoch:6 step:6152 [D loss: 0.617800, acc: 67.97%] [G loss: 2.440979]\n",
      "epoch:6 step:6153 [D loss: 0.646766, acc: 67.97%] [G loss: 2.179947]\n",
      "epoch:6 step:6154 [D loss: 0.535341, acc: 75.00%] [G loss: 2.490314]\n",
      "epoch:6 step:6155 [D loss: 0.659427, acc: 57.81%] [G loss: 2.788674]\n",
      "epoch:6 step:6156 [D loss: 0.570131, acc: 71.09%] [G loss: 2.628026]\n",
      "epoch:6 step:6157 [D loss: 0.693540, acc: 62.50%] [G loss: 2.128379]\n",
      "epoch:6 step:6158 [D loss: 0.616783, acc: 67.19%] [G loss: 2.468615]\n",
      "epoch:6 step:6159 [D loss: 0.729720, acc: 59.38%] [G loss: 2.172213]\n",
      "epoch:6 step:6160 [D loss: 0.640828, acc: 62.50%] [G loss: 2.054660]\n",
      "epoch:6 step:6161 [D loss: 0.633835, acc: 65.62%] [G loss: 2.141854]\n",
      "epoch:6 step:6162 [D loss: 0.670448, acc: 57.81%] [G loss: 2.525366]\n",
      "epoch:6 step:6163 [D loss: 0.602676, acc: 64.06%] [G loss: 2.207134]\n",
      "epoch:6 step:6164 [D loss: 0.623971, acc: 71.09%] [G loss: 2.477534]\n",
      "epoch:6 step:6165 [D loss: 0.636647, acc: 60.94%] [G loss: 2.564176]\n",
      "epoch:6 step:6166 [D loss: 0.643358, acc: 67.97%] [G loss: 2.288706]\n",
      "epoch:6 step:6167 [D loss: 0.560007, acc: 70.31%] [G loss: 2.477228]\n",
      "epoch:6 step:6168 [D loss: 0.609200, acc: 64.84%] [G loss: 2.369554]\n",
      "epoch:6 step:6169 [D loss: 0.529502, acc: 71.09%] [G loss: 2.634894]\n",
      "epoch:6 step:6170 [D loss: 0.587074, acc: 67.19%] [G loss: 2.379188]\n",
      "epoch:6 step:6171 [D loss: 0.566758, acc: 64.84%] [G loss: 2.613811]\n",
      "epoch:6 step:6172 [D loss: 0.576550, acc: 71.88%] [G loss: 2.626348]\n",
      "epoch:6 step:6173 [D loss: 0.538546, acc: 75.00%] [G loss: 2.833227]\n",
      "epoch:6 step:6174 [D loss: 0.590746, acc: 69.53%] [G loss: 2.707877]\n",
      "epoch:6 step:6175 [D loss: 0.591929, acc: 67.19%] [G loss: 2.205614]\n",
      "epoch:6 step:6176 [D loss: 0.624935, acc: 61.72%] [G loss: 2.856217]\n",
      "epoch:6 step:6177 [D loss: 0.609290, acc: 66.41%] [G loss: 2.672918]\n",
      "epoch:6 step:6178 [D loss: 0.544595, acc: 74.22%] [G loss: 2.576175]\n",
      "epoch:6 step:6179 [D loss: 0.563827, acc: 68.75%] [G loss: 2.657329]\n",
      "epoch:6 step:6180 [D loss: 0.602318, acc: 71.09%] [G loss: 2.636539]\n",
      "epoch:6 step:6181 [D loss: 0.619857, acc: 60.94%] [G loss: 2.366459]\n",
      "epoch:6 step:6182 [D loss: 0.571514, acc: 69.53%] [G loss: 2.300897]\n",
      "epoch:6 step:6183 [D loss: 0.579176, acc: 65.62%] [G loss: 2.560181]\n",
      "epoch:6 step:6184 [D loss: 0.619636, acc: 59.38%] [G loss: 2.238776]\n",
      "epoch:6 step:6185 [D loss: 0.696090, acc: 60.94%] [G loss: 2.041380]\n",
      "epoch:6 step:6186 [D loss: 0.564777, acc: 70.31%] [G loss: 2.765712]\n",
      "epoch:6 step:6187 [D loss: 0.645544, acc: 60.16%] [G loss: 2.468714]\n",
      "epoch:6 step:6188 [D loss: 0.684321, acc: 60.94%] [G loss: 2.109599]\n",
      "epoch:6 step:6189 [D loss: 0.609335, acc: 62.50%] [G loss: 2.237084]\n",
      "epoch:6 step:6190 [D loss: 0.611044, acc: 67.97%] [G loss: 2.418777]\n",
      "epoch:6 step:6191 [D loss: 0.601919, acc: 69.53%] [G loss: 2.276556]\n",
      "epoch:6 step:6192 [D loss: 0.589691, acc: 69.53%] [G loss: 2.414676]\n",
      "epoch:6 step:6193 [D loss: 0.606743, acc: 63.28%] [G loss: 2.486659]\n",
      "epoch:6 step:6194 [D loss: 0.666332, acc: 57.03%] [G loss: 2.082836]\n",
      "epoch:6 step:6195 [D loss: 0.577595, acc: 71.09%] [G loss: 2.195994]\n",
      "epoch:6 step:6196 [D loss: 0.567906, acc: 67.97%] [G loss: 2.255747]\n",
      "epoch:6 step:6197 [D loss: 0.601884, acc: 65.62%] [G loss: 2.233824]\n",
      "epoch:6 step:6198 [D loss: 0.669558, acc: 60.94%] [G loss: 2.005425]\n",
      "epoch:6 step:6199 [D loss: 0.564470, acc: 71.09%] [G loss: 2.277648]\n",
      "epoch:6 step:6200 [D loss: 0.589933, acc: 67.19%] [G loss: 2.203458]\n",
      "epoch:6 step:6201 [D loss: 0.549289, acc: 71.88%] [G loss: 2.250471]\n",
      "epoch:6 step:6202 [D loss: 0.649504, acc: 56.25%] [G loss: 2.133103]\n",
      "epoch:6 step:6203 [D loss: 0.570466, acc: 69.53%] [G loss: 2.354243]\n",
      "epoch:6 step:6204 [D loss: 0.601700, acc: 69.53%] [G loss: 2.246413]\n",
      "epoch:6 step:6205 [D loss: 0.628148, acc: 59.38%] [G loss: 2.305897]\n",
      "epoch:6 step:6206 [D loss: 0.624670, acc: 67.97%] [G loss: 2.252175]\n",
      "epoch:6 step:6207 [D loss: 0.602950, acc: 65.62%] [G loss: 2.388741]\n",
      "epoch:6 step:6208 [D loss: 0.648552, acc: 62.50%] [G loss: 2.527536]\n",
      "epoch:6 step:6209 [D loss: 0.648244, acc: 62.50%] [G loss: 2.391500]\n",
      "epoch:6 step:6210 [D loss: 0.608657, acc: 64.06%] [G loss: 2.439646]\n",
      "epoch:6 step:6211 [D loss: 0.557611, acc: 74.22%] [G loss: 2.448024]\n",
      "epoch:6 step:6212 [D loss: 0.647591, acc: 62.50%] [G loss: 2.396726]\n",
      "epoch:6 step:6213 [D loss: 0.623971, acc: 71.09%] [G loss: 2.451863]\n",
      "epoch:6 step:6214 [D loss: 0.511630, acc: 78.91%] [G loss: 2.678106]\n",
      "epoch:6 step:6215 [D loss: 0.623253, acc: 67.97%] [G loss: 2.366887]\n",
      "epoch:6 step:6216 [D loss: 0.582656, acc: 72.66%] [G loss: 2.317440]\n",
      "epoch:6 step:6217 [D loss: 0.578631, acc: 71.88%] [G loss: 2.415976]\n",
      "epoch:6 step:6218 [D loss: 0.717274, acc: 58.59%] [G loss: 2.171089]\n",
      "epoch:6 step:6219 [D loss: 0.617873, acc: 67.97%] [G loss: 2.465646]\n",
      "epoch:6 step:6220 [D loss: 0.579484, acc: 70.31%] [G loss: 2.649904]\n",
      "epoch:6 step:6221 [D loss: 0.623532, acc: 65.62%] [G loss: 2.462342]\n",
      "epoch:6 step:6222 [D loss: 0.581128, acc: 70.31%] [G loss: 2.307960]\n",
      "epoch:6 step:6223 [D loss: 0.613300, acc: 62.50%] [G loss: 2.539227]\n",
      "epoch:6 step:6224 [D loss: 0.591686, acc: 64.06%] [G loss: 2.510170]\n",
      "epoch:6 step:6225 [D loss: 0.564778, acc: 71.88%] [G loss: 2.687753]\n",
      "epoch:6 step:6226 [D loss: 0.579884, acc: 66.41%] [G loss: 2.077916]\n",
      "epoch:6 step:6227 [D loss: 0.562814, acc: 67.97%] [G loss: 2.547547]\n",
      "epoch:6 step:6228 [D loss: 0.616971, acc: 62.50%] [G loss: 2.179209]\n",
      "epoch:6 step:6229 [D loss: 0.660713, acc: 64.06%] [G loss: 2.296617]\n",
      "epoch:6 step:6230 [D loss: 0.521408, acc: 78.91%] [G loss: 2.526792]\n",
      "epoch:6 step:6231 [D loss: 0.547042, acc: 70.31%] [G loss: 2.536951]\n",
      "epoch:6 step:6232 [D loss: 0.583383, acc: 70.31%] [G loss: 2.572179]\n",
      "epoch:6 step:6233 [D loss: 0.656855, acc: 60.16%] [G loss: 2.267094]\n",
      "epoch:6 step:6234 [D loss: 0.647218, acc: 63.28%] [G loss: 2.129131]\n",
      "epoch:6 step:6235 [D loss: 0.555554, acc: 67.19%] [G loss: 2.477705]\n",
      "epoch:6 step:6236 [D loss: 0.596613, acc: 66.41%] [G loss: 2.367299]\n",
      "epoch:6 step:6237 [D loss: 0.614628, acc: 62.50%] [G loss: 2.008964]\n",
      "epoch:6 step:6238 [D loss: 0.680890, acc: 59.38%] [G loss: 2.308120]\n",
      "epoch:6 step:6239 [D loss: 0.611948, acc: 66.41%] [G loss: 2.470314]\n",
      "epoch:6 step:6240 [D loss: 0.626820, acc: 62.50%] [G loss: 2.269936]\n",
      "epoch:6 step:6241 [D loss: 0.615978, acc: 65.62%] [G loss: 2.374132]\n",
      "epoch:6 step:6242 [D loss: 0.661970, acc: 65.62%] [G loss: 2.428657]\n",
      "epoch:6 step:6243 [D loss: 0.628521, acc: 64.84%] [G loss: 2.195381]\n",
      "epoch:6 step:6244 [D loss: 0.584459, acc: 69.53%] [G loss: 2.512357]\n",
      "epoch:6 step:6245 [D loss: 0.643366, acc: 64.84%] [G loss: 2.476270]\n",
      "epoch:6 step:6246 [D loss: 0.571377, acc: 65.62%] [G loss: 2.584167]\n",
      "epoch:6 step:6247 [D loss: 0.573101, acc: 73.44%] [G loss: 2.411055]\n",
      "epoch:6 step:6248 [D loss: 0.734948, acc: 58.59%] [G loss: 2.249828]\n",
      "epoch:6 step:6249 [D loss: 0.545935, acc: 74.22%] [G loss: 2.388036]\n",
      "epoch:6 step:6250 [D loss: 0.591280, acc: 67.19%] [G loss: 2.187779]\n",
      "epoch:6 step:6251 [D loss: 0.604025, acc: 73.44%] [G loss: 2.313049]\n",
      "epoch:6 step:6252 [D loss: 0.541551, acc: 74.22%] [G loss: 2.590677]\n",
      "epoch:6 step:6253 [D loss: 0.521749, acc: 75.00%] [G loss: 2.680426]\n",
      "epoch:6 step:6254 [D loss: 0.552817, acc: 70.31%] [G loss: 2.500917]\n",
      "epoch:6 step:6255 [D loss: 0.658532, acc: 60.94%] [G loss: 2.712652]\n",
      "epoch:6 step:6256 [D loss: 0.570299, acc: 70.31%] [G loss: 2.530711]\n",
      "epoch:6 step:6257 [D loss: 0.626085, acc: 67.97%] [G loss: 2.351603]\n",
      "epoch:6 step:6258 [D loss: 0.635942, acc: 64.06%] [G loss: 2.382341]\n",
      "epoch:6 step:6259 [D loss: 0.535529, acc: 72.66%] [G loss: 2.840591]\n",
      "epoch:6 step:6260 [D loss: 0.605103, acc: 65.62%] [G loss: 2.446730]\n",
      "epoch:6 step:6261 [D loss: 0.620442, acc: 69.53%] [G loss: 2.290921]\n",
      "epoch:6 step:6262 [D loss: 0.640298, acc: 71.09%] [G loss: 2.291968]\n",
      "epoch:6 step:6263 [D loss: 0.545963, acc: 73.44%] [G loss: 2.582007]\n",
      "epoch:6 step:6264 [D loss: 0.640768, acc: 67.19%] [G loss: 2.577405]\n",
      "epoch:6 step:6265 [D loss: 0.570889, acc: 71.88%] [G loss: 2.354685]\n",
      "epoch:6 step:6266 [D loss: 0.575796, acc: 64.06%] [G loss: 2.639314]\n",
      "epoch:6 step:6267 [D loss: 0.624757, acc: 64.06%] [G loss: 2.235146]\n",
      "epoch:6 step:6268 [D loss: 0.615514, acc: 64.84%] [G loss: 2.361033]\n",
      "epoch:6 step:6269 [D loss: 0.596248, acc: 68.75%] [G loss: 2.592909]\n",
      "epoch:6 step:6270 [D loss: 0.530198, acc: 75.78%] [G loss: 2.908747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6271 [D loss: 0.514565, acc: 78.12%] [G loss: 2.594963]\n",
      "epoch:6 step:6272 [D loss: 0.624047, acc: 67.19%] [G loss: 2.679533]\n",
      "epoch:6 step:6273 [D loss: 0.630937, acc: 70.31%] [G loss: 2.489933]\n",
      "epoch:6 step:6274 [D loss: 0.650214, acc: 67.97%] [G loss: 2.401378]\n",
      "epoch:6 step:6275 [D loss: 0.693248, acc: 60.94%] [G loss: 2.470948]\n",
      "epoch:6 step:6276 [D loss: 0.605201, acc: 65.62%] [G loss: 2.776583]\n",
      "epoch:6 step:6277 [D loss: 0.594892, acc: 70.31%] [G loss: 2.556410]\n",
      "epoch:6 step:6278 [D loss: 0.594581, acc: 68.75%] [G loss: 2.251699]\n",
      "epoch:6 step:6279 [D loss: 0.608610, acc: 64.06%] [G loss: 2.364945]\n",
      "epoch:6 step:6280 [D loss: 0.712285, acc: 57.81%] [G loss: 2.061047]\n",
      "epoch:6 step:6281 [D loss: 0.740351, acc: 53.12%] [G loss: 2.289720]\n",
      "epoch:6 step:6282 [D loss: 0.631351, acc: 64.84%] [G loss: 2.427688]\n",
      "epoch:6 step:6283 [D loss: 0.602607, acc: 63.28%] [G loss: 2.193340]\n",
      "epoch:6 step:6284 [D loss: 0.633301, acc: 66.41%] [G loss: 2.312996]\n",
      "epoch:6 step:6285 [D loss: 0.575303, acc: 72.66%] [G loss: 2.392203]\n",
      "epoch:6 step:6286 [D loss: 0.558142, acc: 70.31%] [G loss: 2.384494]\n",
      "epoch:6 step:6287 [D loss: 0.604055, acc: 68.75%] [G loss: 2.295843]\n",
      "epoch:6 step:6288 [D loss: 0.563812, acc: 70.31%] [G loss: 2.296145]\n",
      "epoch:6 step:6289 [D loss: 0.714847, acc: 58.59%] [G loss: 2.202808]\n",
      "epoch:6 step:6290 [D loss: 0.577684, acc: 70.31%] [G loss: 2.300959]\n",
      "epoch:6 step:6291 [D loss: 0.630222, acc: 67.19%] [G loss: 2.201546]\n",
      "epoch:6 step:6292 [D loss: 0.636047, acc: 60.94%] [G loss: 2.341945]\n",
      "epoch:6 step:6293 [D loss: 0.696103, acc: 60.16%] [G loss: 2.185696]\n",
      "epoch:6 step:6294 [D loss: 0.565631, acc: 72.66%] [G loss: 2.419986]\n",
      "epoch:6 step:6295 [D loss: 0.571652, acc: 69.53%] [G loss: 2.339132]\n",
      "epoch:6 step:6296 [D loss: 0.575991, acc: 69.53%] [G loss: 2.156888]\n",
      "epoch:6 step:6297 [D loss: 0.601110, acc: 71.88%] [G loss: 2.345017]\n",
      "epoch:6 step:6298 [D loss: 0.622317, acc: 62.50%] [G loss: 2.212883]\n",
      "epoch:6 step:6299 [D loss: 0.599678, acc: 67.19%] [G loss: 2.625267]\n",
      "epoch:6 step:6300 [D loss: 0.629632, acc: 66.41%] [G loss: 2.521176]\n",
      "epoch:6 step:6301 [D loss: 0.524999, acc: 76.56%] [G loss: 2.584501]\n",
      "epoch:6 step:6302 [D loss: 0.536032, acc: 75.00%] [G loss: 2.364691]\n",
      "epoch:6 step:6303 [D loss: 0.525396, acc: 74.22%] [G loss: 2.706415]\n",
      "epoch:6 step:6304 [D loss: 0.581874, acc: 70.31%] [G loss: 2.495709]\n",
      "epoch:6 step:6305 [D loss: 0.533090, acc: 73.44%] [G loss: 2.273564]\n",
      "epoch:6 step:6306 [D loss: 0.609609, acc: 64.84%] [G loss: 2.368683]\n",
      "epoch:6 step:6307 [D loss: 0.591189, acc: 71.88%] [G loss: 2.436245]\n",
      "epoch:6 step:6308 [D loss: 0.624291, acc: 67.97%] [G loss: 2.356214]\n",
      "epoch:6 step:6309 [D loss: 0.566547, acc: 73.44%] [G loss: 2.385184]\n",
      "epoch:6 step:6310 [D loss: 0.643466, acc: 66.41%] [G loss: 2.483120]\n",
      "epoch:6 step:6311 [D loss: 0.584423, acc: 69.53%] [G loss: 2.483407]\n",
      "epoch:6 step:6312 [D loss: 0.622821, acc: 67.19%] [G loss: 2.737119]\n",
      "epoch:6 step:6313 [D loss: 0.536565, acc: 73.44%] [G loss: 2.800020]\n",
      "epoch:6 step:6314 [D loss: 0.565313, acc: 71.88%] [G loss: 2.687281]\n",
      "epoch:6 step:6315 [D loss: 0.666672, acc: 64.84%] [G loss: 2.772776]\n",
      "epoch:6 step:6316 [D loss: 0.655138, acc: 67.97%] [G loss: 2.693082]\n",
      "epoch:6 step:6317 [D loss: 0.639388, acc: 63.28%] [G loss: 2.388913]\n",
      "epoch:6 step:6318 [D loss: 0.628491, acc: 67.97%] [G loss: 2.314424]\n",
      "epoch:6 step:6319 [D loss: 0.651343, acc: 64.84%] [G loss: 2.396723]\n",
      "epoch:6 step:6320 [D loss: 0.588960, acc: 69.53%] [G loss: 2.294142]\n",
      "epoch:6 step:6321 [D loss: 0.603497, acc: 68.75%] [G loss: 2.444279]\n",
      "epoch:6 step:6322 [D loss: 0.633709, acc: 59.38%] [G loss: 2.484466]\n",
      "epoch:6 step:6323 [D loss: 0.581767, acc: 67.19%] [G loss: 2.444638]\n",
      "epoch:6 step:6324 [D loss: 0.697846, acc: 57.81%] [G loss: 2.047333]\n",
      "epoch:6 step:6325 [D loss: 0.658294, acc: 63.28%] [G loss: 2.302157]\n",
      "epoch:6 step:6326 [D loss: 0.656323, acc: 64.84%] [G loss: 2.158758]\n",
      "epoch:6 step:6327 [D loss: 0.527563, acc: 71.88%] [G loss: 2.466080]\n",
      "epoch:6 step:6328 [D loss: 0.563439, acc: 74.22%] [G loss: 2.700480]\n",
      "epoch:6 step:6329 [D loss: 0.522362, acc: 78.91%] [G loss: 2.897804]\n",
      "epoch:6 step:6330 [D loss: 0.583241, acc: 68.75%] [G loss: 2.561149]\n",
      "epoch:6 step:6331 [D loss: 0.534771, acc: 73.44%] [G loss: 2.472980]\n",
      "epoch:6 step:6332 [D loss: 0.708169, acc: 60.94%] [G loss: 2.217372]\n",
      "epoch:6 step:6333 [D loss: 0.639646, acc: 64.84%] [G loss: 2.352961]\n",
      "epoch:6 step:6334 [D loss: 0.653167, acc: 57.03%] [G loss: 2.390858]\n",
      "epoch:6 step:6335 [D loss: 0.638696, acc: 62.50%] [G loss: 2.137077]\n",
      "epoch:6 step:6336 [D loss: 0.642523, acc: 65.62%] [G loss: 2.515973]\n",
      "epoch:6 step:6337 [D loss: 0.606855, acc: 67.19%] [G loss: 2.312817]\n",
      "epoch:6 step:6338 [D loss: 0.620638, acc: 64.06%] [G loss: 2.208164]\n",
      "epoch:6 step:6339 [D loss: 0.682511, acc: 59.38%] [G loss: 2.167840]\n",
      "epoch:6 step:6340 [D loss: 0.587074, acc: 69.53%] [G loss: 2.402701]\n",
      "epoch:6 step:6341 [D loss: 0.626373, acc: 63.28%] [G loss: 2.697379]\n",
      "epoch:6 step:6342 [D loss: 0.617676, acc: 65.62%] [G loss: 2.421884]\n",
      "epoch:6 step:6343 [D loss: 0.510326, acc: 75.78%] [G loss: 2.450628]\n",
      "epoch:6 step:6344 [D loss: 0.641237, acc: 58.59%] [G loss: 2.411234]\n",
      "epoch:6 step:6345 [D loss: 0.632801, acc: 65.62%] [G loss: 2.326350]\n",
      "epoch:6 step:6346 [D loss: 0.578430, acc: 64.84%] [G loss: 2.416704]\n",
      "epoch:6 step:6347 [D loss: 0.613308, acc: 70.31%] [G loss: 2.697151]\n",
      "epoch:6 step:6348 [D loss: 0.570295, acc: 71.09%] [G loss: 2.377345]\n",
      "epoch:6 step:6349 [D loss: 0.609651, acc: 68.75%] [G loss: 2.214780]\n",
      "epoch:6 step:6350 [D loss: 0.605531, acc: 66.41%] [G loss: 2.359579]\n",
      "epoch:6 step:6351 [D loss: 0.608349, acc: 64.06%] [G loss: 2.217222]\n",
      "epoch:6 step:6352 [D loss: 0.614469, acc: 62.50%] [G loss: 2.190938]\n",
      "epoch:6 step:6353 [D loss: 0.621365, acc: 64.84%] [G loss: 2.182106]\n",
      "epoch:6 step:6354 [D loss: 0.588337, acc: 70.31%] [G loss: 2.198865]\n",
      "epoch:6 step:6355 [D loss: 0.553733, acc: 74.22%] [G loss: 2.431622]\n",
      "epoch:6 step:6356 [D loss: 0.671915, acc: 61.72%] [G loss: 1.918713]\n",
      "epoch:6 step:6357 [D loss: 0.607297, acc: 67.97%] [G loss: 2.412290]\n",
      "epoch:6 step:6358 [D loss: 0.516964, acc: 74.22%] [G loss: 2.495835]\n",
      "epoch:6 step:6359 [D loss: 0.639582, acc: 62.50%] [G loss: 2.152818]\n",
      "epoch:6 step:6360 [D loss: 0.625833, acc: 64.06%] [G loss: 2.190022]\n",
      "epoch:6 step:6361 [D loss: 0.595321, acc: 66.41%] [G loss: 2.270804]\n",
      "epoch:6 step:6362 [D loss: 0.582058, acc: 68.75%] [G loss: 2.390258]\n",
      "epoch:6 step:6363 [D loss: 0.612611, acc: 65.62%] [G loss: 2.239504]\n",
      "epoch:6 step:6364 [D loss: 0.663049, acc: 58.59%] [G loss: 2.239704]\n",
      "epoch:6 step:6365 [D loss: 0.579716, acc: 66.41%] [G loss: 2.389915]\n",
      "epoch:6 step:6366 [D loss: 0.656364, acc: 66.41%] [G loss: 2.297514]\n",
      "epoch:6 step:6367 [D loss: 0.659011, acc: 59.38%] [G loss: 2.128324]\n",
      "epoch:6 step:6368 [D loss: 0.677582, acc: 61.72%] [G loss: 2.114671]\n",
      "epoch:6 step:6369 [D loss: 0.569665, acc: 71.88%] [G loss: 2.328099]\n",
      "epoch:6 step:6370 [D loss: 0.558249, acc: 67.19%] [G loss: 2.303853]\n",
      "epoch:6 step:6371 [D loss: 0.670083, acc: 61.72%] [G loss: 2.121954]\n",
      "epoch:6 step:6372 [D loss: 0.657533, acc: 64.06%] [G loss: 2.155704]\n",
      "epoch:6 step:6373 [D loss: 0.560693, acc: 72.66%] [G loss: 2.341151]\n",
      "epoch:6 step:6374 [D loss: 0.646197, acc: 60.94%] [G loss: 2.164850]\n",
      "epoch:6 step:6375 [D loss: 0.627015, acc: 62.50%] [G loss: 2.381077]\n",
      "epoch:6 step:6376 [D loss: 0.525284, acc: 75.78%] [G loss: 2.459840]\n",
      "epoch:6 step:6377 [D loss: 0.630831, acc: 60.16%] [G loss: 2.263203]\n",
      "epoch:6 step:6378 [D loss: 0.580444, acc: 67.97%] [G loss: 2.301868]\n",
      "epoch:6 step:6379 [D loss: 0.591863, acc: 68.75%] [G loss: 2.630912]\n",
      "epoch:6 step:6380 [D loss: 0.664265, acc: 63.28%] [G loss: 2.214027]\n",
      "epoch:6 step:6381 [D loss: 0.662267, acc: 65.62%] [G loss: 2.111595]\n",
      "epoch:6 step:6382 [D loss: 0.673597, acc: 60.16%] [G loss: 2.247997]\n",
      "epoch:6 step:6383 [D loss: 0.570391, acc: 67.97%] [G loss: 2.361124]\n",
      "epoch:6 step:6384 [D loss: 0.653977, acc: 59.38%] [G loss: 2.146287]\n",
      "epoch:6 step:6385 [D loss: 0.603989, acc: 64.84%] [G loss: 2.204156]\n",
      "epoch:6 step:6386 [D loss: 0.647385, acc: 63.28%] [G loss: 2.417263]\n",
      "epoch:6 step:6387 [D loss: 0.660741, acc: 63.28%] [G loss: 2.165852]\n",
      "epoch:6 step:6388 [D loss: 0.724176, acc: 55.47%] [G loss: 2.040478]\n",
      "epoch:6 step:6389 [D loss: 0.606423, acc: 67.97%] [G loss: 2.374872]\n",
      "epoch:6 step:6390 [D loss: 0.635812, acc: 60.94%] [G loss: 2.200644]\n",
      "epoch:6 step:6391 [D loss: 0.549835, acc: 71.88%] [G loss: 2.562935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6392 [D loss: 0.582005, acc: 71.88%] [G loss: 2.283442]\n",
      "epoch:6 step:6393 [D loss: 0.578187, acc: 69.53%] [G loss: 2.441411]\n",
      "epoch:6 step:6394 [D loss: 0.561435, acc: 71.09%] [G loss: 2.402672]\n",
      "epoch:6 step:6395 [D loss: 0.678872, acc: 61.72%] [G loss: 2.191123]\n",
      "epoch:6 step:6396 [D loss: 0.635863, acc: 60.16%] [G loss: 2.536999]\n",
      "epoch:6 step:6397 [D loss: 0.595272, acc: 64.84%] [G loss: 2.695207]\n",
      "epoch:6 step:6398 [D loss: 0.567259, acc: 68.75%] [G loss: 2.388007]\n",
      "epoch:6 step:6399 [D loss: 0.561941, acc: 71.09%] [G loss: 2.408846]\n",
      "epoch:6 step:6400 [D loss: 0.617007, acc: 65.62%] [G loss: 2.205349]\n",
      "epoch:6 step:6401 [D loss: 0.604203, acc: 68.75%] [G loss: 2.446414]\n",
      "epoch:6 step:6402 [D loss: 0.560737, acc: 70.31%] [G loss: 2.327857]\n",
      "epoch:6 step:6403 [D loss: 0.599442, acc: 66.41%] [G loss: 2.512586]\n",
      "epoch:6 step:6404 [D loss: 0.569732, acc: 69.53%] [G loss: 2.451678]\n",
      "epoch:6 step:6405 [D loss: 0.632562, acc: 60.16%] [G loss: 2.438896]\n",
      "epoch:6 step:6406 [D loss: 0.596371, acc: 64.06%] [G loss: 2.272789]\n",
      "epoch:6 step:6407 [D loss: 0.712381, acc: 64.06%] [G loss: 2.463565]\n",
      "epoch:6 step:6408 [D loss: 0.680480, acc: 66.41%] [G loss: 2.492801]\n",
      "epoch:6 step:6409 [D loss: 0.738885, acc: 58.59%] [G loss: 2.054907]\n",
      "epoch:6 step:6410 [D loss: 0.618667, acc: 71.09%] [G loss: 1.923242]\n",
      "epoch:6 step:6411 [D loss: 0.570042, acc: 73.44%] [G loss: 2.422660]\n",
      "epoch:6 step:6412 [D loss: 0.644781, acc: 64.84%] [G loss: 2.444877]\n",
      "epoch:6 step:6413 [D loss: 0.567426, acc: 71.09%] [G loss: 2.437892]\n",
      "epoch:6 step:6414 [D loss: 0.568344, acc: 73.44%] [G loss: 2.711201]\n",
      "epoch:6 step:6415 [D loss: 0.640805, acc: 63.28%] [G loss: 2.378287]\n",
      "epoch:6 step:6416 [D loss: 0.714579, acc: 54.69%] [G loss: 2.246891]\n",
      "epoch:6 step:6417 [D loss: 0.627443, acc: 60.94%] [G loss: 2.363119]\n",
      "epoch:6 step:6418 [D loss: 0.636024, acc: 64.84%] [G loss: 2.398980]\n",
      "epoch:6 step:6419 [D loss: 0.637740, acc: 58.59%] [G loss: 2.212821]\n",
      "epoch:6 step:6420 [D loss: 0.582568, acc: 67.19%] [G loss: 2.272809]\n",
      "epoch:6 step:6421 [D loss: 0.664166, acc: 64.84%] [G loss: 2.143793]\n",
      "epoch:6 step:6422 [D loss: 0.651365, acc: 64.84%] [G loss: 2.275877]\n",
      "epoch:6 step:6423 [D loss: 0.677505, acc: 60.94%] [G loss: 2.267880]\n",
      "epoch:6 step:6424 [D loss: 0.566991, acc: 69.53%] [G loss: 2.308181]\n",
      "epoch:6 step:6425 [D loss: 0.592790, acc: 68.75%] [G loss: 2.328172]\n",
      "epoch:6 step:6426 [D loss: 0.612342, acc: 67.97%] [G loss: 2.326281]\n",
      "epoch:6 step:6427 [D loss: 0.548756, acc: 71.88%] [G loss: 2.373516]\n",
      "epoch:6 step:6428 [D loss: 0.552439, acc: 71.88%] [G loss: 2.487358]\n",
      "epoch:6 step:6429 [D loss: 0.557493, acc: 72.66%] [G loss: 2.556824]\n",
      "epoch:6 step:6430 [D loss: 0.565797, acc: 71.88%] [G loss: 2.356725]\n",
      "epoch:6 step:6431 [D loss: 0.570945, acc: 71.88%] [G loss: 2.718350]\n",
      "epoch:6 step:6432 [D loss: 0.603872, acc: 64.84%] [G loss: 2.447124]\n",
      "epoch:6 step:6433 [D loss: 0.649629, acc: 63.28%] [G loss: 2.189843]\n",
      "epoch:6 step:6434 [D loss: 0.679750, acc: 53.91%] [G loss: 2.326239]\n",
      "epoch:6 step:6435 [D loss: 0.624814, acc: 64.84%] [G loss: 2.295361]\n",
      "epoch:6 step:6436 [D loss: 0.603435, acc: 69.53%] [G loss: 2.401012]\n",
      "epoch:6 step:6437 [D loss: 0.576988, acc: 75.78%] [G loss: 2.938884]\n",
      "epoch:6 step:6438 [D loss: 0.655139, acc: 65.62%] [G loss: 2.476202]\n",
      "epoch:6 step:6439 [D loss: 0.627513, acc: 65.62%] [G loss: 2.372066]\n",
      "epoch:6 step:6440 [D loss: 0.736267, acc: 57.81%] [G loss: 2.264026]\n",
      "epoch:6 step:6441 [D loss: 0.623521, acc: 67.97%] [G loss: 2.671409]\n",
      "epoch:6 step:6442 [D loss: 0.645621, acc: 64.06%] [G loss: 2.264427]\n",
      "epoch:6 step:6443 [D loss: 0.602202, acc: 66.41%] [G loss: 2.379206]\n",
      "epoch:6 step:6444 [D loss: 0.547424, acc: 75.78%] [G loss: 2.369538]\n",
      "epoch:6 step:6445 [D loss: 0.683741, acc: 60.16%] [G loss: 2.512774]\n",
      "epoch:6 step:6446 [D loss: 0.647700, acc: 61.72%] [G loss: 2.343001]\n",
      "epoch:6 step:6447 [D loss: 0.567160, acc: 71.88%] [G loss: 2.762757]\n",
      "epoch:6 step:6448 [D loss: 0.561110, acc: 69.53%] [G loss: 2.321185]\n",
      "epoch:6 step:6449 [D loss: 0.740542, acc: 52.34%] [G loss: 2.082530]\n",
      "epoch:6 step:6450 [D loss: 0.654358, acc: 60.94%] [G loss: 2.194786]\n",
      "epoch:6 step:6451 [D loss: 0.603928, acc: 70.31%] [G loss: 2.122976]\n",
      "epoch:6 step:6452 [D loss: 0.579605, acc: 67.97%] [G loss: 2.216519]\n",
      "epoch:6 step:6453 [D loss: 0.620928, acc: 65.62%] [G loss: 2.315839]\n",
      "epoch:6 step:6454 [D loss: 0.613388, acc: 65.62%] [G loss: 2.231230]\n",
      "epoch:6 step:6455 [D loss: 0.557298, acc: 74.22%] [G loss: 2.387686]\n",
      "epoch:6 step:6456 [D loss: 0.578169, acc: 70.31%] [G loss: 2.248394]\n",
      "epoch:6 step:6457 [D loss: 0.519756, acc: 76.56%] [G loss: 2.449330]\n",
      "epoch:6 step:6458 [D loss: 0.562572, acc: 69.53%] [G loss: 2.234656]\n",
      "epoch:6 step:6459 [D loss: 0.600376, acc: 66.41%] [G loss: 2.494183]\n",
      "epoch:6 step:6460 [D loss: 0.571533, acc: 69.53%] [G loss: 2.547402]\n",
      "epoch:6 step:6461 [D loss: 0.565833, acc: 68.75%] [G loss: 2.557050]\n",
      "epoch:6 step:6462 [D loss: 0.497824, acc: 74.22%] [G loss: 2.717731]\n",
      "epoch:6 step:6463 [D loss: 0.617847, acc: 67.97%] [G loss: 2.679000]\n",
      "epoch:6 step:6464 [D loss: 0.624817, acc: 71.09%] [G loss: 2.577604]\n",
      "epoch:6 step:6465 [D loss: 0.672914, acc: 64.06%] [G loss: 2.314569]\n",
      "epoch:6 step:6466 [D loss: 0.589772, acc: 72.66%] [G loss: 2.431840]\n",
      "epoch:6 step:6467 [D loss: 0.623059, acc: 64.84%] [G loss: 2.424952]\n",
      "epoch:6 step:6468 [D loss: 0.681732, acc: 60.94%] [G loss: 2.285270]\n",
      "epoch:6 step:6469 [D loss: 0.574725, acc: 67.97%] [G loss: 2.482759]\n",
      "epoch:6 step:6470 [D loss: 0.586996, acc: 69.53%] [G loss: 2.544618]\n",
      "epoch:6 step:6471 [D loss: 0.599735, acc: 69.53%] [G loss: 2.444952]\n",
      "epoch:6 step:6472 [D loss: 0.617253, acc: 67.97%] [G loss: 2.386290]\n",
      "epoch:6 step:6473 [D loss: 0.608431, acc: 67.19%] [G loss: 2.212653]\n",
      "epoch:6 step:6474 [D loss: 0.639797, acc: 67.97%] [G loss: 2.390981]\n",
      "epoch:6 step:6475 [D loss: 0.615950, acc: 62.50%] [G loss: 2.503334]\n",
      "epoch:6 step:6476 [D loss: 0.537685, acc: 78.91%] [G loss: 2.203974]\n",
      "epoch:6 step:6477 [D loss: 0.611044, acc: 69.53%] [G loss: 2.136127]\n",
      "epoch:6 step:6478 [D loss: 0.618724, acc: 67.97%] [G loss: 2.184670]\n",
      "epoch:6 step:6479 [D loss: 0.568315, acc: 67.97%] [G loss: 2.175517]\n",
      "epoch:6 step:6480 [D loss: 0.677121, acc: 55.47%] [G loss: 2.168136]\n",
      "epoch:6 step:6481 [D loss: 0.614666, acc: 68.75%] [G loss: 2.260956]\n",
      "epoch:6 step:6482 [D loss: 0.601692, acc: 61.72%] [G loss: 2.546189]\n",
      "epoch:6 step:6483 [D loss: 0.606718, acc: 69.53%] [G loss: 2.366093]\n",
      "epoch:6 step:6484 [D loss: 0.607921, acc: 71.09%] [G loss: 2.386674]\n",
      "epoch:6 step:6485 [D loss: 0.600731, acc: 70.31%] [G loss: 2.191287]\n",
      "epoch:6 step:6486 [D loss: 0.558212, acc: 74.22%] [G loss: 2.175721]\n",
      "epoch:6 step:6487 [D loss: 0.625429, acc: 65.62%] [G loss: 2.271312]\n",
      "epoch:6 step:6488 [D loss: 0.672776, acc: 64.84%] [G loss: 2.369261]\n",
      "epoch:6 step:6489 [D loss: 0.615568, acc: 65.62%] [G loss: 2.138606]\n",
      "epoch:6 step:6490 [D loss: 0.587350, acc: 66.41%] [G loss: 2.246913]\n",
      "epoch:6 step:6491 [D loss: 0.706765, acc: 53.91%] [G loss: 1.995702]\n",
      "epoch:6 step:6492 [D loss: 0.619401, acc: 63.28%] [G loss: 2.372706]\n",
      "epoch:6 step:6493 [D loss: 0.604040, acc: 65.62%] [G loss: 2.232069]\n",
      "epoch:6 step:6494 [D loss: 0.646010, acc: 64.84%] [G loss: 2.280497]\n",
      "epoch:6 step:6495 [D loss: 0.677283, acc: 60.94%] [G loss: 2.122678]\n",
      "epoch:6 step:6496 [D loss: 0.574922, acc: 72.66%] [G loss: 2.290841]\n",
      "epoch:6 step:6497 [D loss: 0.593640, acc: 69.53%] [G loss: 2.603496]\n",
      "epoch:6 step:6498 [D loss: 0.614032, acc: 70.31%] [G loss: 2.418114]\n",
      "epoch:6 step:6499 [D loss: 0.591894, acc: 63.28%] [G loss: 2.184135]\n",
      "epoch:6 step:6500 [D loss: 0.649872, acc: 63.28%] [G loss: 2.287571]\n",
      "epoch:6 step:6501 [D loss: 0.724886, acc: 56.25%] [G loss: 2.234410]\n",
      "epoch:6 step:6502 [D loss: 0.597292, acc: 69.53%] [G loss: 2.316010]\n",
      "epoch:6 step:6503 [D loss: 0.643291, acc: 63.28%] [G loss: 2.173940]\n",
      "epoch:6 step:6504 [D loss: 0.662665, acc: 61.72%] [G loss: 2.347020]\n",
      "epoch:6 step:6505 [D loss: 0.656193, acc: 65.62%] [G loss: 2.369720]\n",
      "epoch:6 step:6506 [D loss: 0.520967, acc: 75.00%] [G loss: 2.777248]\n",
      "epoch:6 step:6507 [D loss: 0.568492, acc: 69.53%] [G loss: 2.462173]\n",
      "epoch:6 step:6508 [D loss: 0.543357, acc: 69.53%] [G loss: 2.929327]\n",
      "epoch:6 step:6509 [D loss: 0.573572, acc: 67.97%] [G loss: 2.527516]\n",
      "epoch:6 step:6510 [D loss: 0.659813, acc: 57.03%] [G loss: 2.391755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:6511 [D loss: 0.585466, acc: 67.97%] [G loss: 2.418883]\n",
      "epoch:6 step:6512 [D loss: 0.609255, acc: 68.75%] [G loss: 2.542530]\n",
      "epoch:6 step:6513 [D loss: 0.629725, acc: 68.75%] [G loss: 2.386235]\n",
      "epoch:6 step:6514 [D loss: 0.602262, acc: 65.62%] [G loss: 2.400337]\n",
      "epoch:6 step:6515 [D loss: 0.572504, acc: 72.66%] [G loss: 2.654781]\n",
      "epoch:6 step:6516 [D loss: 0.600797, acc: 67.19%] [G loss: 2.619223]\n",
      "epoch:6 step:6517 [D loss: 0.581888, acc: 72.66%] [G loss: 2.333088]\n",
      "epoch:6 step:6518 [D loss: 0.617142, acc: 68.75%] [G loss: 2.452612]\n",
      "epoch:6 step:6519 [D loss: 0.630154, acc: 65.62%] [G loss: 2.332428]\n",
      "epoch:6 step:6520 [D loss: 0.566358, acc: 67.97%] [G loss: 2.367911]\n",
      "epoch:6 step:6521 [D loss: 0.633193, acc: 64.06%] [G loss: 2.529787]\n",
      "epoch:6 step:6522 [D loss: 0.642336, acc: 63.28%] [G loss: 2.462920]\n",
      "epoch:6 step:6523 [D loss: 0.606780, acc: 64.84%] [G loss: 2.265250]\n",
      "epoch:6 step:6524 [D loss: 0.642678, acc: 64.06%] [G loss: 2.278292]\n",
      "epoch:6 step:6525 [D loss: 0.601759, acc: 67.19%] [G loss: 2.391056]\n",
      "epoch:6 step:6526 [D loss: 0.636315, acc: 64.06%] [G loss: 2.420424]\n",
      "epoch:6 step:6527 [D loss: 0.646503, acc: 64.84%] [G loss: 2.176227]\n",
      "epoch:6 step:6528 [D loss: 0.607440, acc: 65.62%] [G loss: 2.413519]\n",
      "epoch:6 step:6529 [D loss: 0.594299, acc: 66.41%] [G loss: 2.401790]\n",
      "epoch:6 step:6530 [D loss: 0.631779, acc: 62.50%] [G loss: 2.256061]\n",
      "epoch:6 step:6531 [D loss: 0.539060, acc: 71.88%] [G loss: 2.491833]\n",
      "epoch:6 step:6532 [D loss: 0.607792, acc: 66.41%] [G loss: 2.303377]\n",
      "epoch:6 step:6533 [D loss: 0.535330, acc: 72.66%] [G loss: 2.715539]\n",
      "epoch:6 step:6534 [D loss: 0.478912, acc: 76.56%] [G loss: 3.083099]\n",
      "epoch:6 step:6535 [D loss: 0.612196, acc: 68.75%] [G loss: 2.718404]\n",
      "epoch:6 step:6536 [D loss: 0.609786, acc: 64.06%] [G loss: 2.557096]\n",
      "epoch:6 step:6537 [D loss: 0.536282, acc: 71.88%] [G loss: 2.544598]\n",
      "epoch:6 step:6538 [D loss: 0.591277, acc: 71.09%] [G loss: 2.393646]\n",
      "epoch:6 step:6539 [D loss: 0.597169, acc: 65.62%] [G loss: 2.517848]\n",
      "epoch:6 step:6540 [D loss: 0.569396, acc: 67.19%] [G loss: 2.488965]\n",
      "epoch:6 step:6541 [D loss: 0.550916, acc: 75.78%] [G loss: 2.819506]\n",
      "epoch:6 step:6542 [D loss: 0.701680, acc: 57.03%] [G loss: 2.390893]\n",
      "epoch:6 step:6543 [D loss: 0.601460, acc: 67.97%] [G loss: 2.437029]\n",
      "epoch:6 step:6544 [D loss: 0.565176, acc: 71.88%] [G loss: 2.307572]\n",
      "epoch:6 step:6545 [D loss: 0.528060, acc: 75.00%] [G loss: 2.544727]\n",
      "epoch:6 step:6546 [D loss: 0.539679, acc: 72.66%] [G loss: 3.033254]\n",
      "epoch:6 step:6547 [D loss: 0.551730, acc: 67.19%] [G loss: 2.917269]\n",
      "epoch:6 step:6548 [D loss: 0.564416, acc: 69.53%] [G loss: 2.658493]\n",
      "epoch:6 step:6549 [D loss: 0.656319, acc: 63.28%] [G loss: 2.865175]\n",
      "epoch:6 step:6550 [D loss: 0.740608, acc: 60.94%] [G loss: 2.245735]\n",
      "epoch:6 step:6551 [D loss: 0.635564, acc: 62.50%] [G loss: 2.504194]\n",
      "epoch:6 step:6552 [D loss: 0.599527, acc: 67.97%] [G loss: 2.452995]\n",
      "epoch:6 step:6553 [D loss: 0.602687, acc: 64.84%] [G loss: 2.566474]\n",
      "epoch:6 step:6554 [D loss: 0.600002, acc: 67.97%] [G loss: 2.425848]\n",
      "epoch:6 step:6555 [D loss: 0.563763, acc: 71.09%] [G loss: 2.462138]\n",
      "epoch:6 step:6556 [D loss: 0.588950, acc: 71.09%] [G loss: 2.769235]\n",
      "epoch:6 step:6557 [D loss: 0.626793, acc: 61.72%] [G loss: 2.371441]\n",
      "epoch:6 step:6558 [D loss: 0.516774, acc: 71.88%] [G loss: 2.636456]\n",
      "epoch:6 step:6559 [D loss: 0.542257, acc: 73.44%] [G loss: 3.155180]\n",
      "epoch:7 step:6560 [D loss: 0.603327, acc: 64.06%] [G loss: 2.521791]\n",
      "epoch:7 step:6561 [D loss: 0.658263, acc: 65.62%] [G loss: 2.566710]\n",
      "epoch:7 step:6562 [D loss: 0.614959, acc: 65.62%] [G loss: 2.654042]\n",
      "epoch:7 step:6563 [D loss: 0.557808, acc: 71.09%] [G loss: 2.497343]\n",
      "epoch:7 step:6564 [D loss: 0.630959, acc: 68.75%] [G loss: 2.386128]\n",
      "epoch:7 step:6565 [D loss: 0.620241, acc: 67.19%] [G loss: 2.485644]\n",
      "epoch:7 step:6566 [D loss: 0.613551, acc: 64.84%] [G loss: 2.743548]\n",
      "epoch:7 step:6567 [D loss: 0.551665, acc: 73.44%] [G loss: 2.662171]\n",
      "epoch:7 step:6568 [D loss: 0.605270, acc: 67.97%] [G loss: 2.596754]\n",
      "epoch:7 step:6569 [D loss: 0.618306, acc: 66.41%] [G loss: 2.594943]\n",
      "epoch:7 step:6570 [D loss: 0.577541, acc: 66.41%] [G loss: 2.467794]\n",
      "epoch:7 step:6571 [D loss: 0.581641, acc: 71.88%] [G loss: 2.563412]\n",
      "epoch:7 step:6572 [D loss: 0.567797, acc: 74.22%] [G loss: 2.510900]\n",
      "epoch:7 step:6573 [D loss: 0.585219, acc: 68.75%] [G loss: 2.614038]\n",
      "epoch:7 step:6574 [D loss: 0.509855, acc: 76.56%] [G loss: 2.958032]\n",
      "epoch:7 step:6575 [D loss: 0.530041, acc: 77.34%] [G loss: 2.870246]\n",
      "epoch:7 step:6576 [D loss: 0.643837, acc: 64.84%] [G loss: 2.403305]\n",
      "epoch:7 step:6577 [D loss: 0.681509, acc: 59.38%] [G loss: 2.448151]\n",
      "epoch:7 step:6578 [D loss: 0.632836, acc: 66.41%] [G loss: 2.269098]\n",
      "epoch:7 step:6579 [D loss: 0.660104, acc: 64.84%] [G loss: 2.155305]\n",
      "epoch:7 step:6580 [D loss: 0.678930, acc: 54.69%] [G loss: 2.233481]\n",
      "epoch:7 step:6581 [D loss: 0.609997, acc: 67.97%] [G loss: 2.250903]\n",
      "epoch:7 step:6582 [D loss: 0.572693, acc: 70.31%] [G loss: 2.588149]\n",
      "epoch:7 step:6583 [D loss: 0.529708, acc: 71.09%] [G loss: 2.644705]\n",
      "epoch:7 step:6584 [D loss: 0.568050, acc: 71.88%] [G loss: 2.647400]\n",
      "epoch:7 step:6585 [D loss: 0.610349, acc: 64.84%] [G loss: 2.350037]\n",
      "epoch:7 step:6586 [D loss: 0.609371, acc: 70.31%] [G loss: 2.329218]\n",
      "epoch:7 step:6587 [D loss: 0.625760, acc: 64.84%] [G loss: 2.270210]\n",
      "epoch:7 step:6588 [D loss: 0.624162, acc: 69.53%] [G loss: 2.321927]\n",
      "epoch:7 step:6589 [D loss: 0.600473, acc: 67.19%] [G loss: 2.079901]\n",
      "epoch:7 step:6590 [D loss: 0.748699, acc: 54.69%] [G loss: 2.116010]\n",
      "epoch:7 step:6591 [D loss: 0.656207, acc: 63.28%] [G loss: 1.986973]\n",
      "epoch:7 step:6592 [D loss: 0.638185, acc: 57.81%] [G loss: 2.114249]\n",
      "epoch:7 step:6593 [D loss: 0.536684, acc: 70.31%] [G loss: 2.317956]\n",
      "epoch:7 step:6594 [D loss: 0.656573, acc: 60.94%] [G loss: 2.079514]\n",
      "epoch:7 step:6595 [D loss: 0.576457, acc: 69.53%] [G loss: 2.453921]\n",
      "epoch:7 step:6596 [D loss: 0.620661, acc: 62.50%] [G loss: 2.218202]\n",
      "epoch:7 step:6597 [D loss: 0.610529, acc: 65.62%] [G loss: 2.433540]\n",
      "epoch:7 step:6598 [D loss: 0.648068, acc: 66.41%] [G loss: 2.207428]\n",
      "epoch:7 step:6599 [D loss: 0.485339, acc: 74.22%] [G loss: 2.733850]\n",
      "epoch:7 step:6600 [D loss: 0.599712, acc: 65.62%] [G loss: 2.466094]\n",
      "epoch:7 step:6601 [D loss: 0.581734, acc: 67.97%] [G loss: 2.586588]\n",
      "epoch:7 step:6602 [D loss: 0.591304, acc: 71.88%] [G loss: 2.548398]\n",
      "epoch:7 step:6603 [D loss: 0.683372, acc: 61.72%] [G loss: 2.292696]\n",
      "epoch:7 step:6604 [D loss: 0.605249, acc: 67.97%] [G loss: 2.254036]\n",
      "epoch:7 step:6605 [D loss: 0.646737, acc: 62.50%] [G loss: 2.209658]\n",
      "epoch:7 step:6606 [D loss: 0.564224, acc: 71.88%] [G loss: 2.346580]\n",
      "epoch:7 step:6607 [D loss: 0.554170, acc: 72.66%] [G loss: 2.347775]\n",
      "epoch:7 step:6608 [D loss: 0.530177, acc: 74.22%] [G loss: 2.461197]\n",
      "epoch:7 step:6609 [D loss: 0.589299, acc: 67.97%] [G loss: 2.324929]\n",
      "epoch:7 step:6610 [D loss: 0.593188, acc: 67.97%] [G loss: 2.380985]\n",
      "epoch:7 step:6611 [D loss: 0.624700, acc: 63.28%] [G loss: 2.478382]\n",
      "epoch:7 step:6612 [D loss: 0.598977, acc: 67.97%] [G loss: 2.424221]\n",
      "epoch:7 step:6613 [D loss: 0.557228, acc: 67.19%] [G loss: 2.518292]\n",
      "epoch:7 step:6614 [D loss: 0.618648, acc: 67.19%] [G loss: 2.806994]\n",
      "epoch:7 step:6615 [D loss: 0.607727, acc: 69.53%] [G loss: 2.406327]\n",
      "epoch:7 step:6616 [D loss: 0.612317, acc: 64.06%] [G loss: 2.279273]\n",
      "epoch:7 step:6617 [D loss: 0.562659, acc: 75.78%] [G loss: 2.431177]\n",
      "epoch:7 step:6618 [D loss: 0.594333, acc: 65.62%] [G loss: 2.262935]\n",
      "epoch:7 step:6619 [D loss: 0.567211, acc: 68.75%] [G loss: 2.414529]\n",
      "epoch:7 step:6620 [D loss: 0.545019, acc: 76.56%] [G loss: 2.439720]\n",
      "epoch:7 step:6621 [D loss: 0.697634, acc: 54.69%] [G loss: 2.406454]\n",
      "epoch:7 step:6622 [D loss: 0.611757, acc: 67.97%] [G loss: 2.428972]\n",
      "epoch:7 step:6623 [D loss: 0.641600, acc: 64.06%] [G loss: 2.729472]\n",
      "epoch:7 step:6624 [D loss: 0.528461, acc: 75.78%] [G loss: 2.578095]\n",
      "epoch:7 step:6625 [D loss: 0.563888, acc: 70.31%] [G loss: 2.327482]\n",
      "epoch:7 step:6626 [D loss: 0.669532, acc: 65.62%] [G loss: 2.379880]\n",
      "epoch:7 step:6627 [D loss: 0.668574, acc: 58.59%] [G loss: 2.275084]\n",
      "epoch:7 step:6628 [D loss: 0.544729, acc: 74.22%] [G loss: 2.442802]\n",
      "epoch:7 step:6629 [D loss: 0.641996, acc: 64.84%] [G loss: 2.173162]\n",
      "epoch:7 step:6630 [D loss: 0.619871, acc: 67.97%] [G loss: 2.243643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6631 [D loss: 0.556975, acc: 73.44%] [G loss: 2.386261]\n",
      "epoch:7 step:6632 [D loss: 0.664379, acc: 66.41%] [G loss: 2.171228]\n",
      "epoch:7 step:6633 [D loss: 0.483530, acc: 81.25%] [G loss: 2.527357]\n",
      "epoch:7 step:6634 [D loss: 0.549994, acc: 71.09%] [G loss: 2.510704]\n",
      "epoch:7 step:6635 [D loss: 0.619564, acc: 63.28%] [G loss: 2.627953]\n",
      "epoch:7 step:6636 [D loss: 0.530501, acc: 71.09%] [G loss: 2.841592]\n",
      "epoch:7 step:6637 [D loss: 0.590595, acc: 68.75%] [G loss: 2.447823]\n",
      "epoch:7 step:6638 [D loss: 0.649280, acc: 64.06%] [G loss: 2.351391]\n",
      "epoch:7 step:6639 [D loss: 0.618430, acc: 62.50%] [G loss: 1.981815]\n",
      "epoch:7 step:6640 [D loss: 0.605239, acc: 64.84%] [G loss: 2.251831]\n",
      "epoch:7 step:6641 [D loss: 0.648597, acc: 67.97%] [G loss: 2.521247]\n",
      "epoch:7 step:6642 [D loss: 0.570058, acc: 68.75%] [G loss: 2.540774]\n",
      "epoch:7 step:6643 [D loss: 0.631076, acc: 66.41%] [G loss: 2.277253]\n",
      "epoch:7 step:6644 [D loss: 0.606493, acc: 67.19%] [G loss: 2.199317]\n",
      "epoch:7 step:6645 [D loss: 0.639562, acc: 61.72%] [G loss: 2.187969]\n",
      "epoch:7 step:6646 [D loss: 0.579101, acc: 66.41%] [G loss: 2.513987]\n",
      "epoch:7 step:6647 [D loss: 0.562225, acc: 71.09%] [G loss: 2.114142]\n",
      "epoch:7 step:6648 [D loss: 0.601781, acc: 62.50%] [G loss: 2.404049]\n",
      "epoch:7 step:6649 [D loss: 0.589518, acc: 69.53%] [G loss: 2.443214]\n",
      "epoch:7 step:6650 [D loss: 0.567148, acc: 74.22%] [G loss: 2.480453]\n",
      "epoch:7 step:6651 [D loss: 0.567142, acc: 64.84%] [G loss: 2.528634]\n",
      "epoch:7 step:6652 [D loss: 0.523464, acc: 75.00%] [G loss: 2.456065]\n",
      "epoch:7 step:6653 [D loss: 0.569421, acc: 67.97%] [G loss: 2.593332]\n",
      "epoch:7 step:6654 [D loss: 0.681497, acc: 62.50%] [G loss: 2.222527]\n",
      "epoch:7 step:6655 [D loss: 0.569998, acc: 67.97%] [G loss: 2.368551]\n",
      "epoch:7 step:6656 [D loss: 0.522117, acc: 71.88%] [G loss: 2.562248]\n",
      "epoch:7 step:6657 [D loss: 0.598254, acc: 68.75%] [G loss: 2.274096]\n",
      "epoch:7 step:6658 [D loss: 0.625248, acc: 64.84%] [G loss: 2.258073]\n",
      "epoch:7 step:6659 [D loss: 0.536080, acc: 73.44%] [G loss: 2.717011]\n",
      "epoch:7 step:6660 [D loss: 0.545234, acc: 66.41%] [G loss: 2.676997]\n",
      "epoch:7 step:6661 [D loss: 0.620667, acc: 66.41%] [G loss: 2.440543]\n",
      "epoch:7 step:6662 [D loss: 0.501736, acc: 76.56%] [G loss: 2.853085]\n",
      "epoch:7 step:6663 [D loss: 0.510525, acc: 74.22%] [G loss: 2.493960]\n",
      "epoch:7 step:6664 [D loss: 0.653495, acc: 66.41%] [G loss: 2.307124]\n",
      "epoch:7 step:6665 [D loss: 0.514723, acc: 75.78%] [G loss: 2.586557]\n",
      "epoch:7 step:6666 [D loss: 0.619828, acc: 64.84%] [G loss: 2.377196]\n",
      "epoch:7 step:6667 [D loss: 0.607533, acc: 67.97%] [G loss: 2.460387]\n",
      "epoch:7 step:6668 [D loss: 0.623687, acc: 64.06%] [G loss: 2.548518]\n",
      "epoch:7 step:6669 [D loss: 0.615599, acc: 67.19%] [G loss: 2.493022]\n",
      "epoch:7 step:6670 [D loss: 0.553782, acc: 71.09%] [G loss: 2.592532]\n",
      "epoch:7 step:6671 [D loss: 0.523605, acc: 71.88%] [G loss: 2.863158]\n",
      "epoch:7 step:6672 [D loss: 0.596888, acc: 66.41%] [G loss: 2.581865]\n",
      "epoch:7 step:6673 [D loss: 0.562824, acc: 71.88%] [G loss: 2.765141]\n",
      "epoch:7 step:6674 [D loss: 0.577607, acc: 70.31%] [G loss: 2.537163]\n",
      "epoch:7 step:6675 [D loss: 0.619875, acc: 72.66%] [G loss: 2.507427]\n",
      "epoch:7 step:6676 [D loss: 0.571656, acc: 70.31%] [G loss: 2.511343]\n",
      "epoch:7 step:6677 [D loss: 0.609220, acc: 70.31%] [G loss: 2.506294]\n",
      "epoch:7 step:6678 [D loss: 0.531170, acc: 71.09%] [G loss: 2.757046]\n",
      "epoch:7 step:6679 [D loss: 0.550800, acc: 71.09%] [G loss: 2.516000]\n",
      "epoch:7 step:6680 [D loss: 0.553807, acc: 67.97%] [G loss: 2.509866]\n",
      "epoch:7 step:6681 [D loss: 0.664753, acc: 60.94%] [G loss: 2.612829]\n",
      "epoch:7 step:6682 [D loss: 0.685487, acc: 58.59%] [G loss: 2.384929]\n",
      "epoch:7 step:6683 [D loss: 0.650017, acc: 62.50%] [G loss: 2.285888]\n",
      "epoch:7 step:6684 [D loss: 0.610362, acc: 60.94%] [G loss: 2.182599]\n",
      "epoch:7 step:6685 [D loss: 0.666407, acc: 60.94%] [G loss: 2.436677]\n",
      "epoch:7 step:6686 [D loss: 0.635293, acc: 66.41%] [G loss: 2.260467]\n",
      "epoch:7 step:6687 [D loss: 0.550976, acc: 71.88%] [G loss: 2.407640]\n",
      "epoch:7 step:6688 [D loss: 0.585457, acc: 67.19%] [G loss: 2.315347]\n",
      "epoch:7 step:6689 [D loss: 0.610240, acc: 68.75%] [G loss: 2.495309]\n",
      "epoch:7 step:6690 [D loss: 0.550988, acc: 71.09%] [G loss: 2.670129]\n",
      "epoch:7 step:6691 [D loss: 0.646114, acc: 65.62%] [G loss: 2.562356]\n",
      "epoch:7 step:6692 [D loss: 0.658383, acc: 61.72%] [G loss: 2.223638]\n",
      "epoch:7 step:6693 [D loss: 0.715304, acc: 58.59%] [G loss: 2.111260]\n",
      "epoch:7 step:6694 [D loss: 0.662480, acc: 64.84%] [G loss: 2.348526]\n",
      "epoch:7 step:6695 [D loss: 0.583438, acc: 67.19%] [G loss: 2.125035]\n",
      "epoch:7 step:6696 [D loss: 0.605572, acc: 67.97%] [G loss: 2.390986]\n",
      "epoch:7 step:6697 [D loss: 0.697305, acc: 58.59%] [G loss: 2.235015]\n",
      "epoch:7 step:6698 [D loss: 0.620132, acc: 64.06%] [G loss: 2.491246]\n",
      "epoch:7 step:6699 [D loss: 0.570826, acc: 68.75%] [G loss: 2.285602]\n",
      "epoch:7 step:6700 [D loss: 0.598764, acc: 70.31%] [G loss: 2.320097]\n",
      "epoch:7 step:6701 [D loss: 0.713885, acc: 53.12%] [G loss: 2.180895]\n",
      "epoch:7 step:6702 [D loss: 0.650445, acc: 61.72%] [G loss: 2.266721]\n",
      "epoch:7 step:6703 [D loss: 0.584609, acc: 62.50%] [G loss: 2.495841]\n",
      "epoch:7 step:6704 [D loss: 0.707629, acc: 57.03%] [G loss: 2.354599]\n",
      "epoch:7 step:6705 [D loss: 0.535273, acc: 80.47%] [G loss: 2.174358]\n",
      "epoch:7 step:6706 [D loss: 0.598363, acc: 67.19%] [G loss: 2.355118]\n",
      "epoch:7 step:6707 [D loss: 0.609497, acc: 67.97%] [G loss: 2.296718]\n",
      "epoch:7 step:6708 [D loss: 0.607077, acc: 68.75%] [G loss: 2.449949]\n",
      "epoch:7 step:6709 [D loss: 0.605530, acc: 67.19%] [G loss: 2.365835]\n",
      "epoch:7 step:6710 [D loss: 0.567074, acc: 74.22%] [G loss: 2.947218]\n",
      "epoch:7 step:6711 [D loss: 0.536772, acc: 71.09%] [G loss: 2.733317]\n",
      "epoch:7 step:6712 [D loss: 0.684384, acc: 62.50%] [G loss: 2.223707]\n",
      "epoch:7 step:6713 [D loss: 0.626322, acc: 64.06%] [G loss: 2.235819]\n",
      "epoch:7 step:6714 [D loss: 0.558801, acc: 71.88%] [G loss: 2.462487]\n",
      "epoch:7 step:6715 [D loss: 0.657152, acc: 65.62%] [G loss: 2.244576]\n",
      "epoch:7 step:6716 [D loss: 0.627879, acc: 67.19%] [G loss: 2.309110]\n",
      "epoch:7 step:6717 [D loss: 0.613501, acc: 68.75%] [G loss: 2.121257]\n",
      "epoch:7 step:6718 [D loss: 0.612686, acc: 71.09%] [G loss: 2.500059]\n",
      "epoch:7 step:6719 [D loss: 0.670659, acc: 59.38%] [G loss: 2.187239]\n",
      "epoch:7 step:6720 [D loss: 0.607176, acc: 66.41%] [G loss: 2.421453]\n",
      "epoch:7 step:6721 [D loss: 0.655914, acc: 63.28%] [G loss: 2.335970]\n",
      "epoch:7 step:6722 [D loss: 0.664224, acc: 66.41%] [G loss: 2.150204]\n",
      "epoch:7 step:6723 [D loss: 0.638396, acc: 61.72%] [G loss: 2.103415]\n",
      "epoch:7 step:6724 [D loss: 0.593501, acc: 67.19%] [G loss: 2.306124]\n",
      "epoch:7 step:6725 [D loss: 0.552891, acc: 71.88%] [G loss: 2.356443]\n",
      "epoch:7 step:6726 [D loss: 0.577183, acc: 69.53%] [G loss: 2.299128]\n",
      "epoch:7 step:6727 [D loss: 0.508946, acc: 77.34%] [G loss: 2.801903]\n",
      "epoch:7 step:6728 [D loss: 0.727601, acc: 53.91%] [G loss: 2.188701]\n",
      "epoch:7 step:6729 [D loss: 0.599728, acc: 67.19%] [G loss: 2.337473]\n",
      "epoch:7 step:6730 [D loss: 0.607971, acc: 63.28%] [G loss: 2.390462]\n",
      "epoch:7 step:6731 [D loss: 0.622308, acc: 63.28%] [G loss: 2.227003]\n",
      "epoch:7 step:6732 [D loss: 0.593765, acc: 70.31%] [G loss: 2.345135]\n",
      "epoch:7 step:6733 [D loss: 0.620065, acc: 67.97%] [G loss: 2.222560]\n",
      "epoch:7 step:6734 [D loss: 0.600934, acc: 64.84%] [G loss: 2.389194]\n",
      "epoch:7 step:6735 [D loss: 0.630535, acc: 64.06%] [G loss: 2.343339]\n",
      "epoch:7 step:6736 [D loss: 0.652104, acc: 62.50%] [G loss: 2.093659]\n",
      "epoch:7 step:6737 [D loss: 0.587588, acc: 67.19%] [G loss: 2.014975]\n",
      "epoch:7 step:6738 [D loss: 0.624212, acc: 61.72%] [G loss: 2.135765]\n",
      "epoch:7 step:6739 [D loss: 0.564298, acc: 70.31%] [G loss: 2.319057]\n",
      "epoch:7 step:6740 [D loss: 0.601155, acc: 68.75%] [G loss: 2.210032]\n",
      "epoch:7 step:6741 [D loss: 0.602645, acc: 69.53%] [G loss: 2.365461]\n",
      "epoch:7 step:6742 [D loss: 0.651324, acc: 61.72%] [G loss: 2.265297]\n",
      "epoch:7 step:6743 [D loss: 0.631172, acc: 63.28%] [G loss: 2.247330]\n",
      "epoch:7 step:6744 [D loss: 0.652206, acc: 60.16%] [G loss: 2.240256]\n",
      "epoch:7 step:6745 [D loss: 0.579039, acc: 67.19%] [G loss: 2.328947]\n",
      "epoch:7 step:6746 [D loss: 0.607504, acc: 64.06%] [G loss: 1.996158]\n",
      "epoch:7 step:6747 [D loss: 0.646953, acc: 65.62%] [G loss: 2.354057]\n",
      "epoch:7 step:6748 [D loss: 0.633273, acc: 67.19%] [G loss: 2.207404]\n",
      "epoch:7 step:6749 [D loss: 0.561614, acc: 71.09%] [G loss: 2.526132]\n",
      "epoch:7 step:6750 [D loss: 0.564511, acc: 70.31%] [G loss: 2.350124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6751 [D loss: 0.591867, acc: 68.75%] [G loss: 2.354904]\n",
      "epoch:7 step:6752 [D loss: 0.654422, acc: 58.59%] [G loss: 2.417919]\n",
      "epoch:7 step:6753 [D loss: 0.566681, acc: 74.22%] [G loss: 2.713826]\n",
      "epoch:7 step:6754 [D loss: 0.580847, acc: 70.31%] [G loss: 2.298458]\n",
      "epoch:7 step:6755 [D loss: 0.615910, acc: 67.19%] [G loss: 2.189391]\n",
      "epoch:7 step:6756 [D loss: 0.577468, acc: 67.97%] [G loss: 2.478142]\n",
      "epoch:7 step:6757 [D loss: 0.591790, acc: 69.53%] [G loss: 2.624165]\n",
      "epoch:7 step:6758 [D loss: 0.623330, acc: 67.19%] [G loss: 2.505607]\n",
      "epoch:7 step:6759 [D loss: 0.653819, acc: 60.94%] [G loss: 2.221243]\n",
      "epoch:7 step:6760 [D loss: 0.636362, acc: 67.97%] [G loss: 2.271283]\n",
      "epoch:7 step:6761 [D loss: 0.636055, acc: 59.38%] [G loss: 2.388556]\n",
      "epoch:7 step:6762 [D loss: 0.668867, acc: 59.38%] [G loss: 2.206924]\n",
      "epoch:7 step:6763 [D loss: 0.626540, acc: 63.28%] [G loss: 2.252760]\n",
      "epoch:7 step:6764 [D loss: 0.614251, acc: 67.97%] [G loss: 2.310535]\n",
      "epoch:7 step:6765 [D loss: 0.621232, acc: 64.06%] [G loss: 2.436833]\n",
      "epoch:7 step:6766 [D loss: 0.586538, acc: 64.84%] [G loss: 2.628347]\n",
      "epoch:7 step:6767 [D loss: 0.579411, acc: 67.97%] [G loss: 2.718636]\n",
      "epoch:7 step:6768 [D loss: 0.510214, acc: 73.44%] [G loss: 2.636832]\n",
      "epoch:7 step:6769 [D loss: 0.684525, acc: 58.59%] [G loss: 2.287362]\n",
      "epoch:7 step:6770 [D loss: 0.709277, acc: 56.25%] [G loss: 1.973436]\n",
      "epoch:7 step:6771 [D loss: 0.579570, acc: 71.88%] [G loss: 2.345293]\n",
      "epoch:7 step:6772 [D loss: 0.626636, acc: 67.97%] [G loss: 2.121337]\n",
      "epoch:7 step:6773 [D loss: 0.655939, acc: 61.72%] [G loss: 2.270377]\n",
      "epoch:7 step:6774 [D loss: 0.659275, acc: 62.50%] [G loss: 2.257689]\n",
      "epoch:7 step:6775 [D loss: 0.685328, acc: 59.38%] [G loss: 2.471473]\n",
      "epoch:7 step:6776 [D loss: 0.577442, acc: 64.84%] [G loss: 2.470468]\n",
      "epoch:7 step:6777 [D loss: 0.510496, acc: 78.91%] [G loss: 2.435639]\n",
      "epoch:7 step:6778 [D loss: 0.608459, acc: 67.19%] [G loss: 2.622228]\n",
      "epoch:7 step:6779 [D loss: 0.636769, acc: 63.28%] [G loss: 2.448227]\n",
      "epoch:7 step:6780 [D loss: 0.608674, acc: 66.41%] [G loss: 2.308532]\n",
      "epoch:7 step:6781 [D loss: 0.663030, acc: 58.59%] [G loss: 2.416541]\n",
      "epoch:7 step:6782 [D loss: 0.607392, acc: 63.28%] [G loss: 2.293142]\n",
      "epoch:7 step:6783 [D loss: 0.594337, acc: 71.09%] [G loss: 2.365182]\n",
      "epoch:7 step:6784 [D loss: 0.665733, acc: 62.50%] [G loss: 2.305373]\n",
      "epoch:7 step:6785 [D loss: 0.595486, acc: 65.62%] [G loss: 2.212286]\n",
      "epoch:7 step:6786 [D loss: 0.629467, acc: 64.06%] [G loss: 1.988917]\n",
      "epoch:7 step:6787 [D loss: 0.651422, acc: 62.50%] [G loss: 2.081770]\n",
      "epoch:7 step:6788 [D loss: 0.601232, acc: 72.66%] [G loss: 2.525108]\n",
      "epoch:7 step:6789 [D loss: 0.537631, acc: 72.66%] [G loss: 2.696921]\n",
      "epoch:7 step:6790 [D loss: 0.619444, acc: 72.66%] [G loss: 2.849422]\n",
      "epoch:7 step:6791 [D loss: 0.539613, acc: 68.75%] [G loss: 2.687328]\n",
      "epoch:7 step:6792 [D loss: 0.699224, acc: 61.72%] [G loss: 2.231617]\n",
      "epoch:7 step:6793 [D loss: 0.630317, acc: 62.50%] [G loss: 2.362786]\n",
      "epoch:7 step:6794 [D loss: 0.606511, acc: 65.62%] [G loss: 2.339052]\n",
      "epoch:7 step:6795 [D loss: 0.607933, acc: 69.53%] [G loss: 2.400641]\n",
      "epoch:7 step:6796 [D loss: 0.625524, acc: 69.53%] [G loss: 2.152295]\n",
      "epoch:7 step:6797 [D loss: 0.620630, acc: 67.19%] [G loss: 2.576727]\n",
      "epoch:7 step:6798 [D loss: 0.545421, acc: 74.22%] [G loss: 2.228981]\n",
      "epoch:7 step:6799 [D loss: 0.560889, acc: 72.66%] [G loss: 2.308419]\n",
      "epoch:7 step:6800 [D loss: 0.627576, acc: 67.19%] [G loss: 2.493024]\n",
      "epoch:7 step:6801 [D loss: 0.622906, acc: 58.59%] [G loss: 2.604021]\n",
      "epoch:7 step:6802 [D loss: 0.608707, acc: 67.19%] [G loss: 2.463762]\n",
      "epoch:7 step:6803 [D loss: 0.715959, acc: 58.59%] [G loss: 2.409447]\n",
      "epoch:7 step:6804 [D loss: 0.594946, acc: 67.19%] [G loss: 2.368555]\n",
      "epoch:7 step:6805 [D loss: 0.634404, acc: 66.41%] [G loss: 2.359800]\n",
      "epoch:7 step:6806 [D loss: 0.593021, acc: 66.41%] [G loss: 2.411002]\n",
      "epoch:7 step:6807 [D loss: 0.558467, acc: 73.44%] [G loss: 2.371959]\n",
      "epoch:7 step:6808 [D loss: 0.712448, acc: 55.47%] [G loss: 2.214926]\n",
      "epoch:7 step:6809 [D loss: 0.713686, acc: 60.94%] [G loss: 2.037047]\n",
      "epoch:7 step:6810 [D loss: 0.631510, acc: 55.47%] [G loss: 2.046818]\n",
      "epoch:7 step:6811 [D loss: 0.632061, acc: 64.06%] [G loss: 2.085849]\n",
      "epoch:7 step:6812 [D loss: 0.626030, acc: 66.41%] [G loss: 2.068256]\n",
      "epoch:7 step:6813 [D loss: 0.646788, acc: 67.19%] [G loss: 2.239176]\n",
      "epoch:7 step:6814 [D loss: 0.612489, acc: 67.19%] [G loss: 2.295401]\n",
      "epoch:7 step:6815 [D loss: 0.603935, acc: 69.53%] [G loss: 2.353822]\n",
      "epoch:7 step:6816 [D loss: 0.591283, acc: 69.53%] [G loss: 2.255580]\n",
      "epoch:7 step:6817 [D loss: 0.615588, acc: 67.19%] [G loss: 2.131529]\n",
      "epoch:7 step:6818 [D loss: 0.536529, acc: 77.34%] [G loss: 2.235942]\n",
      "epoch:7 step:6819 [D loss: 0.597206, acc: 67.97%] [G loss: 2.189717]\n",
      "epoch:7 step:6820 [D loss: 0.647483, acc: 62.50%] [G loss: 2.351952]\n",
      "epoch:7 step:6821 [D loss: 0.601316, acc: 67.97%] [G loss: 2.465057]\n",
      "epoch:7 step:6822 [D loss: 0.619250, acc: 64.06%] [G loss: 2.231615]\n",
      "epoch:7 step:6823 [D loss: 0.619978, acc: 67.19%] [G loss: 2.615001]\n",
      "epoch:7 step:6824 [D loss: 0.668102, acc: 60.16%] [G loss: 2.101172]\n",
      "epoch:7 step:6825 [D loss: 0.668472, acc: 54.69%] [G loss: 2.302973]\n",
      "epoch:7 step:6826 [D loss: 0.618096, acc: 64.06%] [G loss: 2.197207]\n",
      "epoch:7 step:6827 [D loss: 0.686687, acc: 61.72%] [G loss: 2.158064]\n",
      "epoch:7 step:6828 [D loss: 0.607738, acc: 67.19%] [G loss: 2.498590]\n",
      "epoch:7 step:6829 [D loss: 0.604144, acc: 60.16%] [G loss: 2.400665]\n",
      "epoch:7 step:6830 [D loss: 0.525878, acc: 75.00%] [G loss: 2.482295]\n",
      "epoch:7 step:6831 [D loss: 0.615977, acc: 66.41%] [G loss: 2.195892]\n",
      "epoch:7 step:6832 [D loss: 0.548113, acc: 74.22%] [G loss: 2.373259]\n",
      "epoch:7 step:6833 [D loss: 0.571874, acc: 67.97%] [G loss: 2.199011]\n",
      "epoch:7 step:6834 [D loss: 0.613310, acc: 66.41%] [G loss: 2.259746]\n",
      "epoch:7 step:6835 [D loss: 0.653977, acc: 64.84%] [G loss: 2.380066]\n",
      "epoch:7 step:6836 [D loss: 0.635474, acc: 64.06%] [G loss: 2.394357]\n",
      "epoch:7 step:6837 [D loss: 0.621171, acc: 64.06%] [G loss: 2.230050]\n",
      "epoch:7 step:6838 [D loss: 0.582027, acc: 70.31%] [G loss: 2.279660]\n",
      "epoch:7 step:6839 [D loss: 0.681784, acc: 63.28%] [G loss: 2.580768]\n",
      "epoch:7 step:6840 [D loss: 0.712417, acc: 59.38%] [G loss: 2.164996]\n",
      "epoch:7 step:6841 [D loss: 0.604314, acc: 69.53%] [G loss: 2.330600]\n",
      "epoch:7 step:6842 [D loss: 0.575146, acc: 69.53%] [G loss: 2.265516]\n",
      "epoch:7 step:6843 [D loss: 0.579449, acc: 67.97%] [G loss: 2.249208]\n",
      "epoch:7 step:6844 [D loss: 0.632668, acc: 64.84%] [G loss: 2.187527]\n",
      "epoch:7 step:6845 [D loss: 0.538673, acc: 74.22%] [G loss: 2.373140]\n",
      "epoch:7 step:6846 [D loss: 0.690001, acc: 61.72%] [G loss: 2.207401]\n",
      "epoch:7 step:6847 [D loss: 0.679321, acc: 60.16%] [G loss: 1.993664]\n",
      "epoch:7 step:6848 [D loss: 0.623348, acc: 63.28%] [G loss: 2.180509]\n",
      "epoch:7 step:6849 [D loss: 0.601357, acc: 71.09%] [G loss: 2.275739]\n",
      "epoch:7 step:6850 [D loss: 0.574988, acc: 70.31%] [G loss: 2.376558]\n",
      "epoch:7 step:6851 [D loss: 0.638754, acc: 65.62%] [G loss: 2.259192]\n",
      "epoch:7 step:6852 [D loss: 0.570550, acc: 70.31%] [G loss: 2.264981]\n",
      "epoch:7 step:6853 [D loss: 0.593859, acc: 67.97%] [G loss: 2.304653]\n",
      "epoch:7 step:6854 [D loss: 0.652328, acc: 62.50%] [G loss: 2.340406]\n",
      "epoch:7 step:6855 [D loss: 0.606224, acc: 65.62%] [G loss: 2.310243]\n",
      "epoch:7 step:6856 [D loss: 0.574995, acc: 72.66%] [G loss: 2.232976]\n",
      "epoch:7 step:6857 [D loss: 0.573273, acc: 71.88%] [G loss: 2.532976]\n",
      "epoch:7 step:6858 [D loss: 0.592484, acc: 70.31%] [G loss: 2.098861]\n",
      "epoch:7 step:6859 [D loss: 0.552447, acc: 72.66%] [G loss: 2.716662]\n",
      "epoch:7 step:6860 [D loss: 0.653544, acc: 59.38%] [G loss: 2.087218]\n",
      "epoch:7 step:6861 [D loss: 0.618554, acc: 64.84%] [G loss: 2.419006]\n",
      "epoch:7 step:6862 [D loss: 0.648969, acc: 60.16%] [G loss: 2.268554]\n",
      "epoch:7 step:6863 [D loss: 0.567592, acc: 74.22%] [G loss: 2.185656]\n",
      "epoch:7 step:6864 [D loss: 0.597575, acc: 66.41%] [G loss: 2.183128]\n",
      "epoch:7 step:6865 [D loss: 0.611068, acc: 66.41%] [G loss: 2.330569]\n",
      "epoch:7 step:6866 [D loss: 0.597745, acc: 63.28%] [G loss: 2.363503]\n",
      "epoch:7 step:6867 [D loss: 0.614546, acc: 64.84%] [G loss: 2.574467]\n",
      "epoch:7 step:6868 [D loss: 0.578391, acc: 66.41%] [G loss: 2.507221]\n",
      "epoch:7 step:6869 [D loss: 0.607195, acc: 66.41%] [G loss: 2.220551]\n",
      "epoch:7 step:6870 [D loss: 0.729156, acc: 60.16%] [G loss: 2.202941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6871 [D loss: 0.517290, acc: 78.91%] [G loss: 2.884865]\n",
      "epoch:7 step:6872 [D loss: 0.507883, acc: 78.12%] [G loss: 2.546304]\n",
      "epoch:7 step:6873 [D loss: 0.497479, acc: 82.03%] [G loss: 2.768993]\n",
      "epoch:7 step:6874 [D loss: 0.586701, acc: 68.75%] [G loss: 2.910422]\n",
      "epoch:7 step:6875 [D loss: 0.659128, acc: 67.97%] [G loss: 2.253150]\n",
      "epoch:7 step:6876 [D loss: 0.598129, acc: 65.62%] [G loss: 2.240514]\n",
      "epoch:7 step:6877 [D loss: 0.612116, acc: 65.62%] [G loss: 2.382239]\n",
      "epoch:7 step:6878 [D loss: 0.599716, acc: 69.53%] [G loss: 2.203466]\n",
      "epoch:7 step:6879 [D loss: 0.611324, acc: 67.19%] [G loss: 2.556304]\n",
      "epoch:7 step:6880 [D loss: 0.648165, acc: 67.19%] [G loss: 2.479332]\n",
      "epoch:7 step:6881 [D loss: 0.562901, acc: 71.09%] [G loss: 2.586362]\n",
      "epoch:7 step:6882 [D loss: 0.732702, acc: 57.03%] [G loss: 2.105819]\n",
      "epoch:7 step:6883 [D loss: 0.720557, acc: 57.81%] [G loss: 2.088869]\n",
      "epoch:7 step:6884 [D loss: 0.581602, acc: 67.97%] [G loss: 2.404123]\n",
      "epoch:7 step:6885 [D loss: 0.579919, acc: 73.44%] [G loss: 2.344203]\n",
      "epoch:7 step:6886 [D loss: 0.571804, acc: 69.53%] [G loss: 2.267278]\n",
      "epoch:7 step:6887 [D loss: 0.644826, acc: 63.28%] [G loss: 2.403474]\n",
      "epoch:7 step:6888 [D loss: 0.567041, acc: 69.53%] [G loss: 2.569600]\n",
      "epoch:7 step:6889 [D loss: 0.544127, acc: 77.34%] [G loss: 2.308085]\n",
      "epoch:7 step:6890 [D loss: 0.568835, acc: 75.00%] [G loss: 2.488467]\n",
      "epoch:7 step:6891 [D loss: 0.560044, acc: 72.66%] [G loss: 2.480604]\n",
      "epoch:7 step:6892 [D loss: 0.644068, acc: 66.41%] [G loss: 2.622296]\n",
      "epoch:7 step:6893 [D loss: 0.689428, acc: 63.28%] [G loss: 2.338661]\n",
      "epoch:7 step:6894 [D loss: 0.597745, acc: 68.75%] [G loss: 2.625160]\n",
      "epoch:7 step:6895 [D loss: 0.591953, acc: 71.09%] [G loss: 2.362863]\n",
      "epoch:7 step:6896 [D loss: 0.612711, acc: 70.31%] [G loss: 2.655944]\n",
      "epoch:7 step:6897 [D loss: 0.601583, acc: 67.97%] [G loss: 2.530225]\n",
      "epoch:7 step:6898 [D loss: 0.658105, acc: 64.06%] [G loss: 2.380440]\n",
      "epoch:7 step:6899 [D loss: 0.619346, acc: 63.28%] [G loss: 2.357960]\n",
      "epoch:7 step:6900 [D loss: 0.649725, acc: 62.50%] [G loss: 2.050682]\n",
      "epoch:7 step:6901 [D loss: 0.678587, acc: 60.16%] [G loss: 1.990512]\n",
      "epoch:7 step:6902 [D loss: 0.581518, acc: 71.09%] [G loss: 2.515049]\n",
      "epoch:7 step:6903 [D loss: 0.573124, acc: 69.53%] [G loss: 2.592419]\n",
      "epoch:7 step:6904 [D loss: 0.666952, acc: 63.28%] [G loss: 2.373341]\n",
      "epoch:7 step:6905 [D loss: 0.485614, acc: 81.25%] [G loss: 2.584151]\n",
      "epoch:7 step:6906 [D loss: 0.581119, acc: 75.78%] [G loss: 2.777093]\n",
      "epoch:7 step:6907 [D loss: 0.716296, acc: 58.59%] [G loss: 2.029784]\n",
      "epoch:7 step:6908 [D loss: 0.665877, acc: 60.16%] [G loss: 2.268865]\n",
      "epoch:7 step:6909 [D loss: 0.590705, acc: 67.19%] [G loss: 2.420748]\n",
      "epoch:7 step:6910 [D loss: 0.595242, acc: 68.75%] [G loss: 2.264395]\n",
      "epoch:7 step:6911 [D loss: 0.636526, acc: 66.41%] [G loss: 2.526182]\n",
      "epoch:7 step:6912 [D loss: 0.595109, acc: 64.84%] [G loss: 2.569082]\n",
      "epoch:7 step:6913 [D loss: 0.571114, acc: 68.75%] [G loss: 2.361952]\n",
      "epoch:7 step:6914 [D loss: 0.636287, acc: 60.94%] [G loss: 2.152235]\n",
      "epoch:7 step:6915 [D loss: 0.621694, acc: 63.28%] [G loss: 2.147372]\n",
      "epoch:7 step:6916 [D loss: 0.605064, acc: 67.19%] [G loss: 2.403291]\n",
      "epoch:7 step:6917 [D loss: 0.526451, acc: 72.66%] [G loss: 2.463294]\n",
      "epoch:7 step:6918 [D loss: 0.630622, acc: 63.28%] [G loss: 2.388756]\n",
      "epoch:7 step:6919 [D loss: 0.562015, acc: 68.75%] [G loss: 2.226841]\n",
      "epoch:7 step:6920 [D loss: 0.658394, acc: 60.16%] [G loss: 2.223317]\n",
      "epoch:7 step:6921 [D loss: 0.618341, acc: 66.41%] [G loss: 2.235012]\n",
      "epoch:7 step:6922 [D loss: 0.595180, acc: 64.06%] [G loss: 2.263231]\n",
      "epoch:7 step:6923 [D loss: 0.569585, acc: 71.88%] [G loss: 2.368409]\n",
      "epoch:7 step:6924 [D loss: 0.615011, acc: 67.97%] [G loss: 2.430110]\n",
      "epoch:7 step:6925 [D loss: 0.631352, acc: 63.28%] [G loss: 2.601360]\n",
      "epoch:7 step:6926 [D loss: 0.572238, acc: 75.78%] [G loss: 2.322131]\n",
      "epoch:7 step:6927 [D loss: 0.608188, acc: 69.53%] [G loss: 2.152771]\n",
      "epoch:7 step:6928 [D loss: 0.628069, acc: 68.75%] [G loss: 2.675545]\n",
      "epoch:7 step:6929 [D loss: 0.666793, acc: 60.94%] [G loss: 2.487922]\n",
      "epoch:7 step:6930 [D loss: 0.520630, acc: 75.78%] [G loss: 2.416593]\n",
      "epoch:7 step:6931 [D loss: 0.613775, acc: 64.84%] [G loss: 2.271150]\n",
      "epoch:7 step:6932 [D loss: 0.617175, acc: 66.41%] [G loss: 2.264545]\n",
      "epoch:7 step:6933 [D loss: 0.585744, acc: 67.97%] [G loss: 2.747192]\n",
      "epoch:7 step:6934 [D loss: 0.678310, acc: 64.06%] [G loss: 2.131562]\n",
      "epoch:7 step:6935 [D loss: 0.674750, acc: 55.47%] [G loss: 2.186849]\n",
      "epoch:7 step:6936 [D loss: 0.682602, acc: 56.25%] [G loss: 2.089724]\n",
      "epoch:7 step:6937 [D loss: 0.638093, acc: 60.94%] [G loss: 2.325571]\n",
      "epoch:7 step:6938 [D loss: 0.596620, acc: 65.62%] [G loss: 2.281162]\n",
      "epoch:7 step:6939 [D loss: 0.601936, acc: 65.62%] [G loss: 2.158328]\n",
      "epoch:7 step:6940 [D loss: 0.578336, acc: 72.66%] [G loss: 2.478882]\n",
      "epoch:7 step:6941 [D loss: 0.669533, acc: 64.06%] [G loss: 2.210404]\n",
      "epoch:7 step:6942 [D loss: 0.601210, acc: 64.84%] [G loss: 2.188495]\n",
      "epoch:7 step:6943 [D loss: 0.656790, acc: 64.84%] [G loss: 2.149102]\n",
      "epoch:7 step:6944 [D loss: 0.647950, acc: 66.41%] [G loss: 2.281549]\n",
      "epoch:7 step:6945 [D loss: 0.676880, acc: 61.72%] [G loss: 2.158155]\n",
      "epoch:7 step:6946 [D loss: 0.650336, acc: 65.62%] [G loss: 2.309089]\n",
      "epoch:7 step:6947 [D loss: 0.680873, acc: 57.03%] [G loss: 2.198515]\n",
      "epoch:7 step:6948 [D loss: 0.597066, acc: 64.84%] [G loss: 2.280245]\n",
      "epoch:7 step:6949 [D loss: 0.695921, acc: 56.25%] [G loss: 2.048438]\n",
      "epoch:7 step:6950 [D loss: 0.651183, acc: 62.50%] [G loss: 2.065226]\n",
      "epoch:7 step:6951 [D loss: 0.640610, acc: 65.62%] [G loss: 2.312875]\n",
      "epoch:7 step:6952 [D loss: 0.665862, acc: 62.50%] [G loss: 2.093835]\n",
      "epoch:7 step:6953 [D loss: 0.629183, acc: 65.62%] [G loss: 2.226653]\n",
      "epoch:7 step:6954 [D loss: 0.627020, acc: 66.41%] [G loss: 2.220831]\n",
      "epoch:7 step:6955 [D loss: 0.692188, acc: 57.81%] [G loss: 2.062722]\n",
      "epoch:7 step:6956 [D loss: 0.625310, acc: 61.72%] [G loss: 2.138839]\n",
      "epoch:7 step:6957 [D loss: 0.589716, acc: 64.06%] [G loss: 2.281855]\n",
      "epoch:7 step:6958 [D loss: 0.591505, acc: 68.75%] [G loss: 2.330888]\n",
      "epoch:7 step:6959 [D loss: 0.661370, acc: 63.28%] [G loss: 2.140889]\n",
      "epoch:7 step:6960 [D loss: 0.614059, acc: 67.19%] [G loss: 2.170459]\n",
      "epoch:7 step:6961 [D loss: 0.584665, acc: 66.41%] [G loss: 2.228914]\n",
      "epoch:7 step:6962 [D loss: 0.618560, acc: 63.28%] [G loss: 2.266396]\n",
      "epoch:7 step:6963 [D loss: 0.602820, acc: 70.31%] [G loss: 2.389959]\n",
      "epoch:7 step:6964 [D loss: 0.575765, acc: 73.44%] [G loss: 2.590036]\n",
      "epoch:7 step:6965 [D loss: 0.609995, acc: 67.19%] [G loss: 2.357142]\n",
      "epoch:7 step:6966 [D loss: 0.584462, acc: 67.19%] [G loss: 2.129706]\n",
      "epoch:7 step:6967 [D loss: 0.644770, acc: 61.72%] [G loss: 2.251454]\n",
      "epoch:7 step:6968 [D loss: 0.513882, acc: 80.47%] [G loss: 2.535260]\n",
      "epoch:7 step:6969 [D loss: 0.647907, acc: 62.50%] [G loss: 2.316328]\n",
      "epoch:7 step:6970 [D loss: 0.626010, acc: 64.06%] [G loss: 2.224204]\n",
      "epoch:7 step:6971 [D loss: 0.625951, acc: 62.50%] [G loss: 2.235517]\n",
      "epoch:7 step:6972 [D loss: 0.578932, acc: 70.31%] [G loss: 2.529455]\n",
      "epoch:7 step:6973 [D loss: 0.728914, acc: 64.06%] [G loss: 2.192766]\n",
      "epoch:7 step:6974 [D loss: 0.536716, acc: 72.66%] [G loss: 2.374684]\n",
      "epoch:7 step:6975 [D loss: 0.642416, acc: 62.50%] [G loss: 2.264365]\n",
      "epoch:7 step:6976 [D loss: 0.669611, acc: 66.41%] [G loss: 2.136307]\n",
      "epoch:7 step:6977 [D loss: 0.678523, acc: 63.28%] [G loss: 2.116490]\n",
      "epoch:7 step:6978 [D loss: 0.601382, acc: 67.97%] [G loss: 2.425050]\n",
      "epoch:7 step:6979 [D loss: 0.649366, acc: 58.59%] [G loss: 2.102547]\n",
      "epoch:7 step:6980 [D loss: 0.672885, acc: 60.16%] [G loss: 2.147271]\n",
      "epoch:7 step:6981 [D loss: 0.624274, acc: 66.41%] [G loss: 1.978622]\n",
      "epoch:7 step:6982 [D loss: 0.621113, acc: 63.28%] [G loss: 2.143821]\n",
      "epoch:7 step:6983 [D loss: 0.644485, acc: 61.72%] [G loss: 2.286672]\n",
      "epoch:7 step:6984 [D loss: 0.638166, acc: 61.72%] [G loss: 2.379116]\n",
      "epoch:7 step:6985 [D loss: 0.575863, acc: 71.88%] [G loss: 2.304281]\n",
      "epoch:7 step:6986 [D loss: 0.566408, acc: 76.56%] [G loss: 2.334251]\n",
      "epoch:7 step:6987 [D loss: 0.571857, acc: 72.66%] [G loss: 2.610062]\n",
      "epoch:7 step:6988 [D loss: 0.579572, acc: 67.97%] [G loss: 2.723033]\n",
      "epoch:7 step:6989 [D loss: 0.503767, acc: 75.00%] [G loss: 2.716254]\n",
      "epoch:7 step:6990 [D loss: 0.608963, acc: 67.19%] [G loss: 2.305184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6991 [D loss: 0.668415, acc: 62.50%] [G loss: 2.233769]\n",
      "epoch:7 step:6992 [D loss: 0.583040, acc: 67.19%] [G loss: 2.384056]\n",
      "epoch:7 step:6993 [D loss: 0.552746, acc: 75.00%] [G loss: 2.330215]\n",
      "epoch:7 step:6994 [D loss: 0.636635, acc: 67.19%] [G loss: 2.372138]\n",
      "epoch:7 step:6995 [D loss: 0.694897, acc: 64.84%] [G loss: 2.074118]\n",
      "epoch:7 step:6996 [D loss: 0.624587, acc: 62.50%] [G loss: 2.146137]\n",
      "epoch:7 step:6997 [D loss: 0.674686, acc: 60.16%] [G loss: 2.087668]\n",
      "epoch:7 step:6998 [D loss: 0.646420, acc: 65.62%] [G loss: 2.154690]\n",
      "epoch:7 step:6999 [D loss: 0.687099, acc: 56.25%] [G loss: 2.263449]\n",
      "epoch:7 step:7000 [D loss: 0.678972, acc: 54.69%] [G loss: 1.979419]\n",
      "epoch:7 step:7001 [D loss: 0.658686, acc: 66.41%] [G loss: 2.132475]\n",
      "epoch:7 step:7002 [D loss: 0.700918, acc: 64.06%] [G loss: 2.081590]\n",
      "epoch:7 step:7003 [D loss: 0.585305, acc: 67.97%] [G loss: 2.141741]\n",
      "epoch:7 step:7004 [D loss: 0.614262, acc: 64.84%] [G loss: 2.179286]\n",
      "epoch:7 step:7005 [D loss: 0.594884, acc: 67.19%] [G loss: 2.370896]\n",
      "epoch:7 step:7006 [D loss: 0.622012, acc: 67.19%] [G loss: 2.301266]\n",
      "epoch:7 step:7007 [D loss: 0.651380, acc: 64.06%] [G loss: 2.087847]\n",
      "epoch:7 step:7008 [D loss: 0.622322, acc: 60.16%] [G loss: 2.219290]\n",
      "epoch:7 step:7009 [D loss: 0.651027, acc: 60.94%] [G loss: 2.237131]\n",
      "epoch:7 step:7010 [D loss: 0.604554, acc: 64.84%] [G loss: 2.590361]\n",
      "epoch:7 step:7011 [D loss: 0.589348, acc: 68.75%] [G loss: 2.387724]\n",
      "epoch:7 step:7012 [D loss: 0.553947, acc: 75.00%] [G loss: 2.431908]\n",
      "epoch:7 step:7013 [D loss: 0.582910, acc: 66.41%] [G loss: 2.277909]\n",
      "epoch:7 step:7014 [D loss: 0.679376, acc: 59.38%] [G loss: 2.310159]\n",
      "epoch:7 step:7015 [D loss: 0.637748, acc: 61.72%] [G loss: 2.418389]\n",
      "epoch:7 step:7016 [D loss: 0.567387, acc: 71.88%] [G loss: 2.420908]\n",
      "epoch:7 step:7017 [D loss: 0.685736, acc: 60.16%] [G loss: 2.251553]\n",
      "epoch:7 step:7018 [D loss: 0.638589, acc: 62.50%] [G loss: 2.096129]\n",
      "epoch:7 step:7019 [D loss: 0.673864, acc: 53.91%] [G loss: 2.105019]\n",
      "epoch:7 step:7020 [D loss: 0.688168, acc: 56.25%] [G loss: 2.067331]\n",
      "epoch:7 step:7021 [D loss: 0.610682, acc: 62.50%] [G loss: 2.053013]\n",
      "epoch:7 step:7022 [D loss: 0.543441, acc: 75.00%] [G loss: 2.178035]\n",
      "epoch:7 step:7023 [D loss: 0.577496, acc: 68.75%] [G loss: 2.203313]\n",
      "epoch:7 step:7024 [D loss: 0.690062, acc: 56.25%] [G loss: 2.244946]\n",
      "epoch:7 step:7025 [D loss: 0.618223, acc: 64.84%] [G loss: 2.319038]\n",
      "epoch:7 step:7026 [D loss: 0.567952, acc: 70.31%] [G loss: 2.371617]\n",
      "epoch:7 step:7027 [D loss: 0.611091, acc: 69.53%] [G loss: 2.464898]\n",
      "epoch:7 step:7028 [D loss: 0.629345, acc: 67.19%] [G loss: 2.358242]\n",
      "epoch:7 step:7029 [D loss: 0.621831, acc: 64.84%] [G loss: 2.490189]\n",
      "epoch:7 step:7030 [D loss: 0.567317, acc: 75.00%] [G loss: 2.715970]\n",
      "epoch:7 step:7031 [D loss: 0.522701, acc: 75.00%] [G loss: 2.663475]\n",
      "epoch:7 step:7032 [D loss: 0.681036, acc: 66.41%] [G loss: 2.215237]\n",
      "epoch:7 step:7033 [D loss: 0.604443, acc: 70.31%] [G loss: 2.438828]\n",
      "epoch:7 step:7034 [D loss: 0.595466, acc: 67.97%] [G loss: 2.494506]\n",
      "epoch:7 step:7035 [D loss: 0.659154, acc: 61.72%] [G loss: 2.399321]\n",
      "epoch:7 step:7036 [D loss: 0.710865, acc: 57.03%] [G loss: 2.048012]\n",
      "epoch:7 step:7037 [D loss: 0.563179, acc: 74.22%] [G loss: 2.071087]\n",
      "epoch:7 step:7038 [D loss: 0.572714, acc: 70.31%] [G loss: 2.341124]\n",
      "epoch:7 step:7039 [D loss: 0.642238, acc: 64.06%] [G loss: 2.293346]\n",
      "epoch:7 step:7040 [D loss: 0.598398, acc: 72.66%] [G loss: 2.417267]\n",
      "epoch:7 step:7041 [D loss: 0.663277, acc: 63.28%] [G loss: 1.977211]\n",
      "epoch:7 step:7042 [D loss: 0.672174, acc: 57.81%] [G loss: 1.944867]\n",
      "epoch:7 step:7043 [D loss: 0.618764, acc: 72.66%] [G loss: 2.271763]\n",
      "epoch:7 step:7044 [D loss: 0.596237, acc: 67.19%] [G loss: 2.308948]\n",
      "epoch:7 step:7045 [D loss: 0.593264, acc: 70.31%] [G loss: 1.990762]\n",
      "epoch:7 step:7046 [D loss: 0.627507, acc: 62.50%] [G loss: 2.096425]\n",
      "epoch:7 step:7047 [D loss: 0.519082, acc: 76.56%] [G loss: 2.478197]\n",
      "epoch:7 step:7048 [D loss: 0.683792, acc: 61.72%] [G loss: 2.215430]\n",
      "epoch:7 step:7049 [D loss: 0.590997, acc: 70.31%] [G loss: 2.503952]\n",
      "epoch:7 step:7050 [D loss: 0.631616, acc: 64.84%] [G loss: 2.382797]\n",
      "epoch:7 step:7051 [D loss: 0.656858, acc: 62.50%] [G loss: 2.238575]\n",
      "epoch:7 step:7052 [D loss: 0.625460, acc: 63.28%] [G loss: 2.085337]\n",
      "epoch:7 step:7053 [D loss: 0.594556, acc: 68.75%] [G loss: 2.378143]\n",
      "epoch:7 step:7054 [D loss: 0.536460, acc: 72.66%] [G loss: 2.386587]\n",
      "epoch:7 step:7055 [D loss: 0.597956, acc: 68.75%] [G loss: 2.322853]\n",
      "epoch:7 step:7056 [D loss: 0.604639, acc: 69.53%] [G loss: 2.583649]\n",
      "epoch:7 step:7057 [D loss: 0.566882, acc: 72.66%] [G loss: 2.436985]\n",
      "epoch:7 step:7058 [D loss: 0.553810, acc: 71.09%] [G loss: 2.522928]\n",
      "epoch:7 step:7059 [D loss: 0.719990, acc: 54.69%] [G loss: 1.847055]\n",
      "epoch:7 step:7060 [D loss: 0.693366, acc: 60.94%] [G loss: 1.941864]\n",
      "epoch:7 step:7061 [D loss: 0.720539, acc: 52.34%] [G loss: 1.938199]\n",
      "epoch:7 step:7062 [D loss: 0.559890, acc: 75.00%] [G loss: 2.183112]\n",
      "epoch:7 step:7063 [D loss: 0.633463, acc: 64.06%] [G loss: 2.292205]\n",
      "epoch:7 step:7064 [D loss: 0.651040, acc: 55.47%] [G loss: 2.159773]\n",
      "epoch:7 step:7065 [D loss: 0.633531, acc: 67.97%] [G loss: 2.077630]\n",
      "epoch:7 step:7066 [D loss: 0.675804, acc: 61.72%] [G loss: 2.189726]\n",
      "epoch:7 step:7067 [D loss: 0.572032, acc: 74.22%] [G loss: 2.272540]\n",
      "epoch:7 step:7068 [D loss: 0.633395, acc: 66.41%] [G loss: 2.293253]\n",
      "epoch:7 step:7069 [D loss: 0.656774, acc: 60.16%] [G loss: 2.238053]\n",
      "epoch:7 step:7070 [D loss: 0.665363, acc: 60.16%] [G loss: 2.135270]\n",
      "epoch:7 step:7071 [D loss: 0.632950, acc: 62.50%] [G loss: 2.131582]\n",
      "epoch:7 step:7072 [D loss: 0.638685, acc: 58.59%] [G loss: 2.340029]\n",
      "epoch:7 step:7073 [D loss: 0.625455, acc: 68.75%] [G loss: 2.168039]\n",
      "epoch:7 step:7074 [D loss: 0.598259, acc: 71.09%] [G loss: 2.258395]\n",
      "epoch:7 step:7075 [D loss: 0.541698, acc: 71.88%] [G loss: 2.306184]\n",
      "epoch:7 step:7076 [D loss: 0.628147, acc: 66.41%] [G loss: 2.160951]\n",
      "epoch:7 step:7077 [D loss: 0.603863, acc: 67.19%] [G loss: 2.242079]\n",
      "epoch:7 step:7078 [D loss: 0.553596, acc: 71.88%] [G loss: 2.378772]\n",
      "epoch:7 step:7079 [D loss: 0.591603, acc: 68.75%] [G loss: 2.421845]\n",
      "epoch:7 step:7080 [D loss: 0.607611, acc: 67.19%] [G loss: 2.581241]\n",
      "epoch:7 step:7081 [D loss: 0.589830, acc: 68.75%] [G loss: 2.662694]\n",
      "epoch:7 step:7082 [D loss: 0.622683, acc: 63.28%] [G loss: 2.394854]\n",
      "epoch:7 step:7083 [D loss: 0.615847, acc: 66.41%] [G loss: 2.054410]\n",
      "epoch:7 step:7084 [D loss: 0.619625, acc: 66.41%] [G loss: 2.159630]\n",
      "epoch:7 step:7085 [D loss: 0.642949, acc: 64.84%] [G loss: 2.203264]\n",
      "epoch:7 step:7086 [D loss: 0.674990, acc: 60.16%] [G loss: 2.366883]\n",
      "epoch:7 step:7087 [D loss: 0.612701, acc: 71.09%] [G loss: 2.094250]\n",
      "epoch:7 step:7088 [D loss: 0.629650, acc: 63.28%] [G loss: 2.076355]\n",
      "epoch:7 step:7089 [D loss: 0.609762, acc: 65.62%] [G loss: 2.175886]\n",
      "epoch:7 step:7090 [D loss: 0.613382, acc: 67.97%] [G loss: 2.150726]\n",
      "epoch:7 step:7091 [D loss: 0.542587, acc: 75.78%] [G loss: 2.182391]\n",
      "epoch:7 step:7092 [D loss: 0.597335, acc: 68.75%] [G loss: 2.401725]\n",
      "epoch:7 step:7093 [D loss: 0.614162, acc: 68.75%] [G loss: 2.406764]\n",
      "epoch:7 step:7094 [D loss: 0.596309, acc: 68.75%] [G loss: 2.411970]\n",
      "epoch:7 step:7095 [D loss: 0.609606, acc: 72.66%] [G loss: 2.512682]\n",
      "epoch:7 step:7096 [D loss: 0.633811, acc: 63.28%] [G loss: 2.401359]\n",
      "epoch:7 step:7097 [D loss: 0.696626, acc: 59.38%] [G loss: 2.082112]\n",
      "epoch:7 step:7098 [D loss: 0.622462, acc: 64.06%] [G loss: 2.315185]\n",
      "epoch:7 step:7099 [D loss: 0.622395, acc: 64.06%] [G loss: 2.308074]\n",
      "epoch:7 step:7100 [D loss: 0.622613, acc: 64.06%] [G loss: 2.307905]\n",
      "epoch:7 step:7101 [D loss: 0.671851, acc: 60.94%] [G loss: 2.202046]\n",
      "epoch:7 step:7102 [D loss: 0.622953, acc: 66.41%] [G loss: 2.156225]\n",
      "epoch:7 step:7103 [D loss: 0.608299, acc: 67.97%] [G loss: 2.185782]\n",
      "epoch:7 step:7104 [D loss: 0.592782, acc: 67.97%] [G loss: 2.136505]\n",
      "epoch:7 step:7105 [D loss: 0.560054, acc: 71.88%] [G loss: 2.283712]\n",
      "epoch:7 step:7106 [D loss: 0.594576, acc: 68.75%] [G loss: 2.533343]\n",
      "epoch:7 step:7107 [D loss: 0.644861, acc: 64.06%] [G loss: 2.209775]\n",
      "epoch:7 step:7108 [D loss: 0.562150, acc: 74.22%] [G loss: 2.368036]\n",
      "epoch:7 step:7109 [D loss: 0.668423, acc: 63.28%] [G loss: 2.341526]\n",
      "epoch:7 step:7110 [D loss: 0.528195, acc: 72.66%] [G loss: 2.397174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7111 [D loss: 0.606198, acc: 66.41%] [G loss: 2.559379]\n",
      "epoch:7 step:7112 [D loss: 0.698237, acc: 59.38%] [G loss: 2.322196]\n",
      "epoch:7 step:7113 [D loss: 0.516645, acc: 75.78%] [G loss: 2.596097]\n",
      "epoch:7 step:7114 [D loss: 0.558527, acc: 71.09%] [G loss: 2.407385]\n",
      "epoch:7 step:7115 [D loss: 0.537154, acc: 72.66%] [G loss: 2.482830]\n",
      "epoch:7 step:7116 [D loss: 0.580343, acc: 70.31%] [G loss: 2.442887]\n",
      "epoch:7 step:7117 [D loss: 0.656840, acc: 62.50%] [G loss: 2.589933]\n",
      "epoch:7 step:7118 [D loss: 0.634973, acc: 62.50%] [G loss: 2.168255]\n",
      "epoch:7 step:7119 [D loss: 0.681831, acc: 57.03%] [G loss: 2.173119]\n",
      "epoch:7 step:7120 [D loss: 0.600138, acc: 70.31%] [G loss: 2.279333]\n",
      "epoch:7 step:7121 [D loss: 0.654161, acc: 60.94%] [G loss: 2.269547]\n",
      "epoch:7 step:7122 [D loss: 0.604136, acc: 71.88%] [G loss: 2.331977]\n",
      "epoch:7 step:7123 [D loss: 0.595189, acc: 67.19%] [G loss: 2.515626]\n",
      "epoch:7 step:7124 [D loss: 0.749772, acc: 56.25%] [G loss: 2.111257]\n",
      "epoch:7 step:7125 [D loss: 0.628552, acc: 60.16%] [G loss: 2.156536]\n",
      "epoch:7 step:7126 [D loss: 0.658224, acc: 64.84%] [G loss: 2.075075]\n",
      "epoch:7 step:7127 [D loss: 0.627819, acc: 68.75%] [G loss: 2.355544]\n",
      "epoch:7 step:7128 [D loss: 0.651721, acc: 65.62%] [G loss: 1.945870]\n",
      "epoch:7 step:7129 [D loss: 0.637611, acc: 66.41%] [G loss: 2.191686]\n",
      "epoch:7 step:7130 [D loss: 0.604036, acc: 69.53%] [G loss: 2.239385]\n",
      "epoch:7 step:7131 [D loss: 0.568099, acc: 67.19%] [G loss: 2.102875]\n",
      "epoch:7 step:7132 [D loss: 0.588229, acc: 67.19%] [G loss: 2.046503]\n",
      "epoch:7 step:7133 [D loss: 0.605335, acc: 68.75%] [G loss: 2.295021]\n",
      "epoch:7 step:7134 [D loss: 0.660367, acc: 61.72%] [G loss: 2.191743]\n",
      "epoch:7 step:7135 [D loss: 0.641716, acc: 63.28%] [G loss: 2.082887]\n",
      "epoch:7 step:7136 [D loss: 0.580131, acc: 71.88%] [G loss: 2.233536]\n",
      "epoch:7 step:7137 [D loss: 0.653467, acc: 66.41%] [G loss: 2.212826]\n",
      "epoch:7 step:7138 [D loss: 0.559883, acc: 71.09%] [G loss: 2.273766]\n",
      "epoch:7 step:7139 [D loss: 0.693656, acc: 60.16%] [G loss: 1.985388]\n",
      "epoch:7 step:7140 [D loss: 0.645474, acc: 61.72%] [G loss: 2.198859]\n",
      "epoch:7 step:7141 [D loss: 0.582778, acc: 68.75%] [G loss: 2.246385]\n",
      "epoch:7 step:7142 [D loss: 0.669607, acc: 64.84%] [G loss: 2.107986]\n",
      "epoch:7 step:7143 [D loss: 0.611234, acc: 64.84%] [G loss: 2.060179]\n",
      "epoch:7 step:7144 [D loss: 0.615306, acc: 67.97%] [G loss: 2.196881]\n",
      "epoch:7 step:7145 [D loss: 0.629778, acc: 62.50%] [G loss: 2.291585]\n",
      "epoch:7 step:7146 [D loss: 0.599569, acc: 63.28%] [G loss: 2.299247]\n",
      "epoch:7 step:7147 [D loss: 0.623386, acc: 64.84%] [G loss: 2.076399]\n",
      "epoch:7 step:7148 [D loss: 0.577865, acc: 67.97%] [G loss: 2.305112]\n",
      "epoch:7 step:7149 [D loss: 0.624606, acc: 61.72%] [G loss: 2.123828]\n",
      "epoch:7 step:7150 [D loss: 0.637650, acc: 63.28%] [G loss: 2.161979]\n",
      "epoch:7 step:7151 [D loss: 0.615230, acc: 67.97%] [G loss: 2.156284]\n",
      "epoch:7 step:7152 [D loss: 0.607380, acc: 64.06%] [G loss: 2.058941]\n",
      "epoch:7 step:7153 [D loss: 0.641443, acc: 64.84%] [G loss: 2.084108]\n",
      "epoch:7 step:7154 [D loss: 0.633095, acc: 65.62%] [G loss: 2.098820]\n",
      "epoch:7 step:7155 [D loss: 0.604657, acc: 68.75%] [G loss: 2.112284]\n",
      "epoch:7 step:7156 [D loss: 0.576400, acc: 72.66%] [G loss: 2.416884]\n",
      "epoch:7 step:7157 [D loss: 0.563526, acc: 69.53%] [G loss: 2.483955]\n",
      "epoch:7 step:7158 [D loss: 0.668181, acc: 64.06%] [G loss: 2.109607]\n",
      "epoch:7 step:7159 [D loss: 0.699717, acc: 51.56%] [G loss: 2.253251]\n",
      "epoch:7 step:7160 [D loss: 0.711421, acc: 60.94%] [G loss: 2.147127]\n",
      "epoch:7 step:7161 [D loss: 0.657786, acc: 57.81%] [G loss: 2.302062]\n",
      "epoch:7 step:7162 [D loss: 0.600924, acc: 69.53%] [G loss: 2.223507]\n",
      "epoch:7 step:7163 [D loss: 0.633881, acc: 64.84%] [G loss: 2.287406]\n",
      "epoch:7 step:7164 [D loss: 0.539199, acc: 78.12%] [G loss: 2.365068]\n",
      "epoch:7 step:7165 [D loss: 0.647421, acc: 62.50%] [G loss: 2.261378]\n",
      "epoch:7 step:7166 [D loss: 0.624329, acc: 64.06%] [G loss: 2.210340]\n",
      "epoch:7 step:7167 [D loss: 0.572960, acc: 68.75%] [G loss: 2.453041]\n",
      "epoch:7 step:7168 [D loss: 0.609553, acc: 67.19%] [G loss: 2.249767]\n",
      "epoch:7 step:7169 [D loss: 0.526591, acc: 74.22%] [G loss: 2.386075]\n",
      "epoch:7 step:7170 [D loss: 0.620625, acc: 65.62%] [G loss: 2.175899]\n",
      "epoch:7 step:7171 [D loss: 0.630226, acc: 64.06%] [G loss: 2.299648]\n",
      "epoch:7 step:7172 [D loss: 0.542429, acc: 71.88%] [G loss: 2.489505]\n",
      "epoch:7 step:7173 [D loss: 0.615814, acc: 66.41%] [G loss: 2.177609]\n",
      "epoch:7 step:7174 [D loss: 0.679521, acc: 61.72%] [G loss: 2.011766]\n",
      "epoch:7 step:7175 [D loss: 0.646293, acc: 64.06%] [G loss: 2.082169]\n",
      "epoch:7 step:7176 [D loss: 0.634774, acc: 61.72%] [G loss: 2.337502]\n",
      "epoch:7 step:7177 [D loss: 0.579850, acc: 74.22%] [G loss: 2.292324]\n",
      "epoch:7 step:7178 [D loss: 0.689206, acc: 60.94%] [G loss: 2.006625]\n",
      "epoch:7 step:7179 [D loss: 0.581143, acc: 67.19%] [G loss: 2.233359]\n",
      "epoch:7 step:7180 [D loss: 0.646334, acc: 59.38%] [G loss: 2.226309]\n",
      "epoch:7 step:7181 [D loss: 0.592760, acc: 71.88%] [G loss: 2.179237]\n",
      "epoch:7 step:7182 [D loss: 0.615154, acc: 65.62%] [G loss: 2.310061]\n",
      "epoch:7 step:7183 [D loss: 0.631883, acc: 64.84%] [G loss: 2.375758]\n",
      "epoch:7 step:7184 [D loss: 0.582552, acc: 72.66%] [G loss: 2.178953]\n",
      "epoch:7 step:7185 [D loss: 0.650432, acc: 64.06%] [G loss: 2.346553]\n",
      "epoch:7 step:7186 [D loss: 0.668129, acc: 61.72%] [G loss: 2.266343]\n",
      "epoch:7 step:7187 [D loss: 0.652569, acc: 59.38%] [G loss: 2.008286]\n",
      "epoch:7 step:7188 [D loss: 0.626929, acc: 68.75%] [G loss: 2.342683]\n",
      "epoch:7 step:7189 [D loss: 0.606961, acc: 71.09%] [G loss: 2.377865]\n",
      "epoch:7 step:7190 [D loss: 0.580384, acc: 72.66%] [G loss: 2.457788]\n",
      "epoch:7 step:7191 [D loss: 0.545585, acc: 71.88%] [G loss: 2.413041]\n",
      "epoch:7 step:7192 [D loss: 0.588139, acc: 66.41%] [G loss: 2.256758]\n",
      "epoch:7 step:7193 [D loss: 0.551953, acc: 75.00%] [G loss: 2.318874]\n",
      "epoch:7 step:7194 [D loss: 0.571513, acc: 73.44%] [G loss: 2.378276]\n",
      "epoch:7 step:7195 [D loss: 0.637623, acc: 60.16%] [G loss: 2.511307]\n",
      "epoch:7 step:7196 [D loss: 0.592533, acc: 68.75%] [G loss: 2.480802]\n",
      "epoch:7 step:7197 [D loss: 0.599340, acc: 68.75%] [G loss: 2.273598]\n",
      "epoch:7 step:7198 [D loss: 0.558869, acc: 67.97%] [G loss: 2.240693]\n",
      "epoch:7 step:7199 [D loss: 0.558281, acc: 73.44%] [G loss: 2.624233]\n",
      "epoch:7 step:7200 [D loss: 0.541541, acc: 71.09%] [G loss: 2.496578]\n",
      "epoch:7 step:7201 [D loss: 0.516290, acc: 74.22%] [G loss: 2.780821]\n",
      "epoch:7 step:7202 [D loss: 0.667902, acc: 61.72%] [G loss: 2.350506]\n",
      "epoch:7 step:7203 [D loss: 0.626422, acc: 63.28%] [G loss: 2.228986]\n",
      "epoch:7 step:7204 [D loss: 0.583514, acc: 66.41%] [G loss: 2.222041]\n",
      "epoch:7 step:7205 [D loss: 0.569995, acc: 64.06%] [G loss: 2.365618]\n",
      "epoch:7 step:7206 [D loss: 0.579596, acc: 67.97%] [G loss: 2.737060]\n",
      "epoch:7 step:7207 [D loss: 0.526845, acc: 73.44%] [G loss: 2.954158]\n",
      "epoch:7 step:7208 [D loss: 0.648627, acc: 65.62%] [G loss: 2.594056]\n",
      "epoch:7 step:7209 [D loss: 0.593954, acc: 67.19%] [G loss: 2.438867]\n",
      "epoch:7 step:7210 [D loss: 0.676386, acc: 61.72%] [G loss: 2.419404]\n",
      "epoch:7 step:7211 [D loss: 0.528067, acc: 71.09%] [G loss: 2.387495]\n",
      "epoch:7 step:7212 [D loss: 0.579914, acc: 67.97%] [G loss: 2.369407]\n",
      "epoch:7 step:7213 [D loss: 0.561059, acc: 71.88%] [G loss: 2.539484]\n",
      "epoch:7 step:7214 [D loss: 0.632906, acc: 60.16%] [G loss: 2.430354]\n",
      "epoch:7 step:7215 [D loss: 0.633921, acc: 64.84%] [G loss: 2.218351]\n",
      "epoch:7 step:7216 [D loss: 0.687601, acc: 64.84%] [G loss: 2.073123]\n",
      "epoch:7 step:7217 [D loss: 0.675463, acc: 64.84%] [G loss: 2.031151]\n",
      "epoch:7 step:7218 [D loss: 0.574132, acc: 71.88%] [G loss: 2.181637]\n",
      "epoch:7 step:7219 [D loss: 0.582302, acc: 69.53%] [G loss: 2.172841]\n",
      "epoch:7 step:7220 [D loss: 0.629944, acc: 63.28%] [G loss: 2.132399]\n",
      "epoch:7 step:7221 [D loss: 0.625901, acc: 68.75%] [G loss: 2.204890]\n",
      "epoch:7 step:7222 [D loss: 0.581569, acc: 68.75%] [G loss: 2.405930]\n",
      "epoch:7 step:7223 [D loss: 0.714608, acc: 51.56%] [G loss: 2.175383]\n",
      "epoch:7 step:7224 [D loss: 0.629073, acc: 67.97%] [G loss: 2.269633]\n",
      "epoch:7 step:7225 [D loss: 0.691109, acc: 64.84%] [G loss: 2.155656]\n",
      "epoch:7 step:7226 [D loss: 0.620721, acc: 64.06%] [G loss: 1.855855]\n",
      "epoch:7 step:7227 [D loss: 0.597053, acc: 67.97%] [G loss: 2.322424]\n",
      "epoch:7 step:7228 [D loss: 0.657528, acc: 66.41%] [G loss: 2.077891]\n",
      "epoch:7 step:7229 [D loss: 0.562490, acc: 71.09%] [G loss: 2.256002]\n",
      "epoch:7 step:7230 [D loss: 0.590487, acc: 73.44%] [G loss: 2.120147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7231 [D loss: 0.617171, acc: 67.19%] [G loss: 2.148916]\n",
      "epoch:7 step:7232 [D loss: 0.664960, acc: 60.16%] [G loss: 2.299742]\n",
      "epoch:7 step:7233 [D loss: 0.684321, acc: 60.16%] [G loss: 1.988097]\n",
      "epoch:7 step:7234 [D loss: 0.604492, acc: 70.31%] [G loss: 2.260438]\n",
      "epoch:7 step:7235 [D loss: 0.590918, acc: 67.97%] [G loss: 2.123859]\n",
      "epoch:7 step:7236 [D loss: 0.524329, acc: 71.88%] [G loss: 2.504067]\n",
      "epoch:7 step:7237 [D loss: 0.626781, acc: 65.62%] [G loss: 2.372556]\n",
      "epoch:7 step:7238 [D loss: 0.631332, acc: 62.50%] [G loss: 2.328088]\n",
      "epoch:7 step:7239 [D loss: 0.610343, acc: 67.19%] [G loss: 2.294684]\n",
      "epoch:7 step:7240 [D loss: 0.547592, acc: 71.88%] [G loss: 2.290074]\n",
      "epoch:7 step:7241 [D loss: 0.678957, acc: 60.16%] [G loss: 2.289866]\n",
      "epoch:7 step:7242 [D loss: 0.568282, acc: 70.31%] [G loss: 2.320080]\n",
      "epoch:7 step:7243 [D loss: 0.680079, acc: 58.59%] [G loss: 2.028431]\n",
      "epoch:7 step:7244 [D loss: 0.641858, acc: 65.62%] [G loss: 2.181608]\n",
      "epoch:7 step:7245 [D loss: 0.569273, acc: 69.53%] [G loss: 2.089694]\n",
      "epoch:7 step:7246 [D loss: 0.609557, acc: 63.28%] [G loss: 2.048818]\n",
      "epoch:7 step:7247 [D loss: 0.659445, acc: 60.16%] [G loss: 2.456460]\n",
      "epoch:7 step:7248 [D loss: 0.664702, acc: 66.41%] [G loss: 2.246457]\n",
      "epoch:7 step:7249 [D loss: 0.554072, acc: 73.44%] [G loss: 2.393914]\n",
      "epoch:7 step:7250 [D loss: 0.613484, acc: 64.06%] [G loss: 2.614698]\n",
      "epoch:7 step:7251 [D loss: 0.595078, acc: 71.09%] [G loss: 2.375536]\n",
      "epoch:7 step:7252 [D loss: 0.577378, acc: 65.62%] [G loss: 2.381754]\n",
      "epoch:7 step:7253 [D loss: 0.563362, acc: 71.09%] [G loss: 2.457149]\n",
      "epoch:7 step:7254 [D loss: 0.638431, acc: 62.50%] [G loss: 2.391726]\n",
      "epoch:7 step:7255 [D loss: 0.616464, acc: 66.41%] [G loss: 2.219574]\n",
      "epoch:7 step:7256 [D loss: 0.606341, acc: 63.28%] [G loss: 2.091125]\n",
      "epoch:7 step:7257 [D loss: 0.500567, acc: 78.91%] [G loss: 2.153943]\n",
      "epoch:7 step:7258 [D loss: 0.570997, acc: 72.66%] [G loss: 2.369869]\n",
      "epoch:7 step:7259 [D loss: 0.616054, acc: 67.19%] [G loss: 2.310060]\n",
      "epoch:7 step:7260 [D loss: 0.598667, acc: 68.75%] [G loss: 2.396479]\n",
      "epoch:7 step:7261 [D loss: 0.637352, acc: 60.94%] [G loss: 2.189726]\n",
      "epoch:7 step:7262 [D loss: 0.542243, acc: 75.78%] [G loss: 2.148061]\n",
      "epoch:7 step:7263 [D loss: 0.638888, acc: 64.06%] [G loss: 2.170885]\n",
      "epoch:7 step:7264 [D loss: 0.536591, acc: 74.22%] [G loss: 2.483006]\n",
      "epoch:7 step:7265 [D loss: 0.619549, acc: 62.50%] [G loss: 2.547949]\n",
      "epoch:7 step:7266 [D loss: 0.607558, acc: 68.75%] [G loss: 2.389083]\n",
      "epoch:7 step:7267 [D loss: 0.615115, acc: 63.28%] [G loss: 2.454855]\n",
      "epoch:7 step:7268 [D loss: 0.588986, acc: 66.41%] [G loss: 2.641654]\n",
      "epoch:7 step:7269 [D loss: 0.685628, acc: 59.38%] [G loss: 2.129068]\n",
      "epoch:7 step:7270 [D loss: 0.656022, acc: 59.38%] [G loss: 2.167034]\n",
      "epoch:7 step:7271 [D loss: 0.635367, acc: 64.84%] [G loss: 2.126693]\n",
      "epoch:7 step:7272 [D loss: 0.600703, acc: 62.50%] [G loss: 2.280916]\n",
      "epoch:7 step:7273 [D loss: 0.643254, acc: 70.31%] [G loss: 2.149192]\n",
      "epoch:7 step:7274 [D loss: 0.622759, acc: 66.41%] [G loss: 2.096769]\n",
      "epoch:7 step:7275 [D loss: 0.673487, acc: 65.62%] [G loss: 1.982214]\n",
      "epoch:7 step:7276 [D loss: 0.598642, acc: 68.75%] [G loss: 2.118414]\n",
      "epoch:7 step:7277 [D loss: 0.598218, acc: 72.66%] [G loss: 2.385407]\n",
      "epoch:7 step:7278 [D loss: 0.624566, acc: 64.06%] [G loss: 2.396873]\n",
      "epoch:7 step:7279 [D loss: 0.580474, acc: 68.75%] [G loss: 2.244534]\n",
      "epoch:7 step:7280 [D loss: 0.582211, acc: 68.75%] [G loss: 2.481713]\n",
      "epoch:7 step:7281 [D loss: 0.688002, acc: 54.69%] [G loss: 2.194704]\n",
      "epoch:7 step:7282 [D loss: 0.669247, acc: 57.03%] [G loss: 2.194403]\n",
      "epoch:7 step:7283 [D loss: 0.611515, acc: 64.84%] [G loss: 2.425530]\n",
      "epoch:7 step:7284 [D loss: 0.623547, acc: 65.62%] [G loss: 2.364987]\n",
      "epoch:7 step:7285 [D loss: 0.642690, acc: 62.50%] [G loss: 2.207004]\n",
      "epoch:7 step:7286 [D loss: 0.627435, acc: 65.62%] [G loss: 2.162449]\n",
      "epoch:7 step:7287 [D loss: 0.564433, acc: 73.44%] [G loss: 2.238982]\n",
      "epoch:7 step:7288 [D loss: 0.592167, acc: 71.09%] [G loss: 2.200258]\n",
      "epoch:7 step:7289 [D loss: 0.605416, acc: 70.31%] [G loss: 2.127854]\n",
      "epoch:7 step:7290 [D loss: 0.655666, acc: 61.72%] [G loss: 2.198554]\n",
      "epoch:7 step:7291 [D loss: 0.643003, acc: 58.59%] [G loss: 2.328827]\n",
      "epoch:7 step:7292 [D loss: 0.683252, acc: 57.03%] [G loss: 2.216559]\n",
      "epoch:7 step:7293 [D loss: 0.683470, acc: 67.19%] [G loss: 2.400328]\n",
      "epoch:7 step:7294 [D loss: 0.639783, acc: 67.19%] [G loss: 2.196844]\n",
      "epoch:7 step:7295 [D loss: 0.547018, acc: 78.91%] [G loss: 2.350574]\n",
      "epoch:7 step:7296 [D loss: 0.624331, acc: 64.06%] [G loss: 2.171137]\n",
      "epoch:7 step:7297 [D loss: 0.570192, acc: 73.44%] [G loss: 2.129467]\n",
      "epoch:7 step:7298 [D loss: 0.658847, acc: 61.72%] [G loss: 1.950067]\n",
      "epoch:7 step:7299 [D loss: 0.641494, acc: 58.59%] [G loss: 2.255089]\n",
      "epoch:7 step:7300 [D loss: 0.614037, acc: 64.84%] [G loss: 2.074875]\n",
      "epoch:7 step:7301 [D loss: 0.647766, acc: 64.84%] [G loss: 2.084609]\n",
      "epoch:7 step:7302 [D loss: 0.574645, acc: 66.41%] [G loss: 2.461612]\n",
      "epoch:7 step:7303 [D loss: 0.630261, acc: 64.84%] [G loss: 2.136683]\n",
      "epoch:7 step:7304 [D loss: 0.729140, acc: 53.91%] [G loss: 2.218384]\n",
      "epoch:7 step:7305 [D loss: 0.560136, acc: 71.88%] [G loss: 2.160776]\n",
      "epoch:7 step:7306 [D loss: 0.617216, acc: 71.09%] [G loss: 2.347110]\n",
      "epoch:7 step:7307 [D loss: 0.652125, acc: 66.41%] [G loss: 2.223058]\n",
      "epoch:7 step:7308 [D loss: 0.597835, acc: 66.41%] [G loss: 2.254295]\n",
      "epoch:7 step:7309 [D loss: 0.635137, acc: 61.72%] [G loss: 2.152285]\n",
      "epoch:7 step:7310 [D loss: 0.622797, acc: 61.72%] [G loss: 2.316906]\n",
      "epoch:7 step:7311 [D loss: 0.671947, acc: 59.38%] [G loss: 2.169610]\n",
      "epoch:7 step:7312 [D loss: 0.586981, acc: 64.84%] [G loss: 1.988765]\n",
      "epoch:7 step:7313 [D loss: 0.558154, acc: 71.88%] [G loss: 2.226729]\n",
      "epoch:7 step:7314 [D loss: 0.634294, acc: 63.28%] [G loss: 2.167728]\n",
      "epoch:7 step:7315 [D loss: 0.622012, acc: 65.62%] [G loss: 2.357684]\n",
      "epoch:7 step:7316 [D loss: 0.564556, acc: 67.97%] [G loss: 2.362459]\n",
      "epoch:7 step:7317 [D loss: 0.648288, acc: 64.84%] [G loss: 2.216981]\n",
      "epoch:7 step:7318 [D loss: 0.612974, acc: 65.62%] [G loss: 2.030913]\n",
      "epoch:7 step:7319 [D loss: 0.682861, acc: 57.03%] [G loss: 2.012460]\n",
      "epoch:7 step:7320 [D loss: 0.664196, acc: 56.25%] [G loss: 2.113608]\n",
      "epoch:7 step:7321 [D loss: 0.672508, acc: 61.72%] [G loss: 2.058965]\n",
      "epoch:7 step:7322 [D loss: 0.655373, acc: 59.38%] [G loss: 2.228045]\n",
      "epoch:7 step:7323 [D loss: 0.580988, acc: 69.53%] [G loss: 2.140727]\n",
      "epoch:7 step:7324 [D loss: 0.727643, acc: 55.47%] [G loss: 2.044411]\n",
      "epoch:7 step:7325 [D loss: 0.654073, acc: 67.19%] [G loss: 2.032191]\n",
      "epoch:7 step:7326 [D loss: 0.623109, acc: 67.97%] [G loss: 2.355988]\n",
      "epoch:7 step:7327 [D loss: 0.689333, acc: 60.16%] [G loss: 2.105839]\n",
      "epoch:7 step:7328 [D loss: 0.552654, acc: 75.78%] [G loss: 2.282683]\n",
      "epoch:7 step:7329 [D loss: 0.629597, acc: 62.50%] [G loss: 2.050509]\n",
      "epoch:7 step:7330 [D loss: 0.602107, acc: 70.31%] [G loss: 2.174481]\n",
      "epoch:7 step:7331 [D loss: 0.607118, acc: 64.84%] [G loss: 2.220654]\n",
      "epoch:7 step:7332 [D loss: 0.613538, acc: 71.09%] [G loss: 2.205851]\n",
      "epoch:7 step:7333 [D loss: 0.583147, acc: 66.41%] [G loss: 2.561088]\n",
      "epoch:7 step:7334 [D loss: 0.529187, acc: 72.66%] [G loss: 2.784523]\n",
      "epoch:7 step:7335 [D loss: 0.639804, acc: 64.06%] [G loss: 2.417599]\n",
      "epoch:7 step:7336 [D loss: 0.580898, acc: 67.19%] [G loss: 2.439006]\n",
      "epoch:7 step:7337 [D loss: 0.645918, acc: 65.62%] [G loss: 2.179572]\n",
      "epoch:7 step:7338 [D loss: 0.645426, acc: 62.50%] [G loss: 2.274963]\n",
      "epoch:7 step:7339 [D loss: 0.605063, acc: 63.28%] [G loss: 2.430509]\n",
      "epoch:7 step:7340 [D loss: 0.674930, acc: 62.50%] [G loss: 2.345654]\n",
      "epoch:7 step:7341 [D loss: 0.629017, acc: 67.19%] [G loss: 2.368916]\n",
      "epoch:7 step:7342 [D loss: 0.635094, acc: 67.97%] [G loss: 2.178342]\n",
      "epoch:7 step:7343 [D loss: 0.653334, acc: 62.50%] [G loss: 2.272120]\n",
      "epoch:7 step:7344 [D loss: 0.618890, acc: 62.50%] [G loss: 2.290407]\n",
      "epoch:7 step:7345 [D loss: 0.570023, acc: 71.09%] [G loss: 2.366685]\n",
      "epoch:7 step:7346 [D loss: 0.689449, acc: 59.38%] [G loss: 2.194935]\n",
      "epoch:7 step:7347 [D loss: 0.652779, acc: 64.84%] [G loss: 2.244148]\n",
      "epoch:7 step:7348 [D loss: 0.608977, acc: 67.19%] [G loss: 2.139481]\n",
      "epoch:7 step:7349 [D loss: 0.621855, acc: 67.97%] [G loss: 2.170802]\n",
      "epoch:7 step:7350 [D loss: 0.644115, acc: 62.50%] [G loss: 2.292489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7351 [D loss: 0.557741, acc: 67.97%] [G loss: 2.610215]\n",
      "epoch:7 step:7352 [D loss: 0.645020, acc: 66.41%] [G loss: 2.249205]\n",
      "epoch:7 step:7353 [D loss: 0.681424, acc: 60.94%] [G loss: 1.925466]\n",
      "epoch:7 step:7354 [D loss: 0.612764, acc: 68.75%] [G loss: 2.229920]\n",
      "epoch:7 step:7355 [D loss: 0.569102, acc: 69.53%] [G loss: 2.291247]\n",
      "epoch:7 step:7356 [D loss: 0.628123, acc: 64.84%] [G loss: 1.994434]\n",
      "epoch:7 step:7357 [D loss: 0.634718, acc: 57.03%] [G loss: 2.176996]\n",
      "epoch:7 step:7358 [D loss: 0.663576, acc: 57.81%] [G loss: 2.087095]\n",
      "epoch:7 step:7359 [D loss: 0.674098, acc: 58.59%] [G loss: 1.881282]\n",
      "epoch:7 step:7360 [D loss: 0.655268, acc: 63.28%] [G loss: 2.207801]\n",
      "epoch:7 step:7361 [D loss: 0.637204, acc: 66.41%] [G loss: 2.273433]\n",
      "epoch:7 step:7362 [D loss: 0.574893, acc: 71.09%] [G loss: 2.420548]\n",
      "epoch:7 step:7363 [D loss: 0.653339, acc: 61.72%] [G loss: 1.985447]\n",
      "epoch:7 step:7364 [D loss: 0.609745, acc: 67.97%] [G loss: 2.344837]\n",
      "epoch:7 step:7365 [D loss: 0.627261, acc: 64.06%] [G loss: 2.114108]\n",
      "epoch:7 step:7366 [D loss: 0.541675, acc: 73.44%] [G loss: 2.319905]\n",
      "epoch:7 step:7367 [D loss: 0.558927, acc: 71.88%] [G loss: 2.309063]\n",
      "epoch:7 step:7368 [D loss: 0.639901, acc: 65.62%] [G loss: 2.314027]\n",
      "epoch:7 step:7369 [D loss: 0.601678, acc: 66.41%] [G loss: 2.198576]\n",
      "epoch:7 step:7370 [D loss: 0.647928, acc: 63.28%] [G loss: 2.135504]\n",
      "epoch:7 step:7371 [D loss: 0.711033, acc: 61.72%] [G loss: 2.015069]\n",
      "epoch:7 step:7372 [D loss: 0.573255, acc: 68.75%] [G loss: 2.228028]\n",
      "epoch:7 step:7373 [D loss: 0.660334, acc: 64.06%] [G loss: 2.219100]\n",
      "epoch:7 step:7374 [D loss: 0.594929, acc: 64.84%] [G loss: 2.492592]\n",
      "epoch:7 step:7375 [D loss: 0.628056, acc: 68.75%] [G loss: 2.602053]\n",
      "epoch:7 step:7376 [D loss: 0.651829, acc: 58.59%] [G loss: 2.093804]\n",
      "epoch:7 step:7377 [D loss: 0.701655, acc: 58.59%] [G loss: 2.243707]\n",
      "epoch:7 step:7378 [D loss: 0.599799, acc: 72.66%] [G loss: 2.404009]\n",
      "epoch:7 step:7379 [D loss: 0.736889, acc: 60.16%] [G loss: 2.032811]\n",
      "epoch:7 step:7380 [D loss: 0.568611, acc: 70.31%] [G loss: 2.361079]\n",
      "epoch:7 step:7381 [D loss: 0.571310, acc: 74.22%] [G loss: 2.370478]\n",
      "epoch:7 step:7382 [D loss: 0.565696, acc: 75.78%] [G loss: 2.450317]\n",
      "epoch:7 step:7383 [D loss: 0.640444, acc: 60.94%] [G loss: 2.323770]\n",
      "epoch:7 step:7384 [D loss: 0.599102, acc: 67.97%] [G loss: 2.348254]\n",
      "epoch:7 step:7385 [D loss: 0.633143, acc: 67.97%] [G loss: 2.430502]\n",
      "epoch:7 step:7386 [D loss: 0.682902, acc: 59.38%] [G loss: 2.107524]\n",
      "epoch:7 step:7387 [D loss: 0.638780, acc: 60.16%] [G loss: 1.929817]\n",
      "epoch:7 step:7388 [D loss: 0.700643, acc: 53.12%] [G loss: 2.055843]\n",
      "epoch:7 step:7389 [D loss: 0.608793, acc: 64.84%] [G loss: 2.073055]\n",
      "epoch:7 step:7390 [D loss: 0.601178, acc: 66.41%] [G loss: 2.140676]\n",
      "epoch:7 step:7391 [D loss: 0.582078, acc: 71.09%] [G loss: 2.230346]\n",
      "epoch:7 step:7392 [D loss: 0.620559, acc: 67.19%] [G loss: 2.386023]\n",
      "epoch:7 step:7393 [D loss: 0.636379, acc: 67.97%] [G loss: 2.387455]\n",
      "epoch:7 step:7394 [D loss: 0.569602, acc: 71.09%] [G loss: 2.213903]\n",
      "epoch:7 step:7395 [D loss: 0.675221, acc: 64.06%] [G loss: 1.982664]\n",
      "epoch:7 step:7396 [D loss: 0.560026, acc: 75.00%] [G loss: 2.377582]\n",
      "epoch:7 step:7397 [D loss: 0.595187, acc: 67.19%] [G loss: 2.283833]\n",
      "epoch:7 step:7398 [D loss: 0.625861, acc: 67.97%] [G loss: 2.297754]\n",
      "epoch:7 step:7399 [D loss: 0.614780, acc: 64.84%] [G loss: 2.227440]\n",
      "epoch:7 step:7400 [D loss: 0.629232, acc: 67.97%] [G loss: 2.338543]\n",
      "epoch:7 step:7401 [D loss: 0.532161, acc: 78.91%] [G loss: 2.183645]\n",
      "epoch:7 step:7402 [D loss: 0.561239, acc: 75.78%] [G loss: 2.164990]\n",
      "epoch:7 step:7403 [D loss: 0.626231, acc: 65.62%] [G loss: 2.192416]\n",
      "epoch:7 step:7404 [D loss: 0.597267, acc: 71.09%] [G loss: 2.156452]\n",
      "epoch:7 step:7405 [D loss: 0.620334, acc: 65.62%] [G loss: 2.300895]\n",
      "epoch:7 step:7406 [D loss: 0.583815, acc: 65.62%] [G loss: 2.470009]\n",
      "epoch:7 step:7407 [D loss: 0.662527, acc: 66.41%] [G loss: 2.300629]\n",
      "epoch:7 step:7408 [D loss: 0.538689, acc: 73.44%] [G loss: 2.482890]\n",
      "epoch:7 step:7409 [D loss: 0.628220, acc: 64.84%] [G loss: 2.135887]\n",
      "epoch:7 step:7410 [D loss: 0.675115, acc: 60.94%] [G loss: 2.202759]\n",
      "epoch:7 step:7411 [D loss: 0.604079, acc: 66.41%] [G loss: 2.321582]\n",
      "epoch:7 step:7412 [D loss: 0.582342, acc: 66.41%] [G loss: 2.230289]\n",
      "epoch:7 step:7413 [D loss: 0.587325, acc: 71.09%] [G loss: 2.562934]\n",
      "epoch:7 step:7414 [D loss: 0.681875, acc: 62.50%] [G loss: 2.149972]\n",
      "epoch:7 step:7415 [D loss: 0.624509, acc: 70.31%] [G loss: 2.194879]\n",
      "epoch:7 step:7416 [D loss: 0.628969, acc: 67.19%] [G loss: 2.414941]\n",
      "epoch:7 step:7417 [D loss: 0.753080, acc: 52.34%] [G loss: 1.873039]\n",
      "epoch:7 step:7418 [D loss: 0.640888, acc: 63.28%] [G loss: 2.148039]\n",
      "epoch:7 step:7419 [D loss: 0.567904, acc: 75.78%] [G loss: 2.312041]\n",
      "epoch:7 step:7420 [D loss: 0.627672, acc: 67.19%] [G loss: 2.110767]\n",
      "epoch:7 step:7421 [D loss: 0.612329, acc: 67.19%] [G loss: 2.198177]\n",
      "epoch:7 step:7422 [D loss: 0.636907, acc: 65.62%] [G loss: 2.197927]\n",
      "epoch:7 step:7423 [D loss: 0.644148, acc: 64.06%] [G loss: 2.135645]\n",
      "epoch:7 step:7424 [D loss: 0.604712, acc: 64.06%] [G loss: 2.218174]\n",
      "epoch:7 step:7425 [D loss: 0.654613, acc: 60.16%] [G loss: 2.272859]\n",
      "epoch:7 step:7426 [D loss: 0.617663, acc: 67.19%] [G loss: 2.190892]\n",
      "epoch:7 step:7427 [D loss: 0.576413, acc: 74.22%] [G loss: 2.108704]\n",
      "epoch:7 step:7428 [D loss: 0.614451, acc: 67.19%] [G loss: 2.262454]\n",
      "epoch:7 step:7429 [D loss: 0.604808, acc: 72.66%] [G loss: 2.177100]\n",
      "epoch:7 step:7430 [D loss: 0.620808, acc: 61.72%] [G loss: 2.153340]\n",
      "epoch:7 step:7431 [D loss: 0.606237, acc: 69.53%] [G loss: 2.318993]\n",
      "epoch:7 step:7432 [D loss: 0.650892, acc: 60.94%] [G loss: 2.014294]\n",
      "epoch:7 step:7433 [D loss: 0.668589, acc: 63.28%] [G loss: 2.116760]\n",
      "epoch:7 step:7434 [D loss: 0.627901, acc: 65.62%] [G loss: 2.367421]\n",
      "epoch:7 step:7435 [D loss: 0.614983, acc: 67.19%] [G loss: 2.127867]\n",
      "epoch:7 step:7436 [D loss: 0.591074, acc: 67.97%] [G loss: 2.295014]\n",
      "epoch:7 step:7437 [D loss: 0.641017, acc: 64.84%] [G loss: 2.194817]\n",
      "epoch:7 step:7438 [D loss: 0.642885, acc: 64.06%] [G loss: 2.289406]\n",
      "epoch:7 step:7439 [D loss: 0.684068, acc: 60.16%] [G loss: 2.173695]\n",
      "epoch:7 step:7440 [D loss: 0.612713, acc: 64.84%] [G loss: 2.221891]\n",
      "epoch:7 step:7441 [D loss: 0.614514, acc: 66.41%] [G loss: 2.114855]\n",
      "epoch:7 step:7442 [D loss: 0.664267, acc: 63.28%] [G loss: 2.115992]\n",
      "epoch:7 step:7443 [D loss: 0.526917, acc: 78.91%] [G loss: 2.552151]\n",
      "epoch:7 step:7444 [D loss: 0.626555, acc: 67.19%] [G loss: 2.422174]\n",
      "epoch:7 step:7445 [D loss: 0.626386, acc: 64.06%] [G loss: 2.412622]\n",
      "epoch:7 step:7446 [D loss: 0.536301, acc: 70.31%] [G loss: 2.162558]\n",
      "epoch:7 step:7447 [D loss: 0.615655, acc: 66.41%] [G loss: 2.391270]\n",
      "epoch:7 step:7448 [D loss: 0.621801, acc: 63.28%] [G loss: 2.297360]\n",
      "epoch:7 step:7449 [D loss: 0.624107, acc: 66.41%] [G loss: 2.369626]\n",
      "epoch:7 step:7450 [D loss: 0.683516, acc: 54.69%] [G loss: 2.068302]\n",
      "epoch:7 step:7451 [D loss: 0.680061, acc: 57.03%] [G loss: 2.061551]\n",
      "epoch:7 step:7452 [D loss: 0.556699, acc: 70.31%] [G loss: 2.111916]\n",
      "epoch:7 step:7453 [D loss: 0.585270, acc: 71.88%] [G loss: 2.432449]\n",
      "epoch:7 step:7454 [D loss: 0.585200, acc: 72.66%] [G loss: 2.243549]\n",
      "epoch:7 step:7455 [D loss: 0.620636, acc: 67.19%] [G loss: 2.237021]\n",
      "epoch:7 step:7456 [D loss: 0.561480, acc: 74.22%] [G loss: 2.285485]\n",
      "epoch:7 step:7457 [D loss: 0.655799, acc: 64.06%] [G loss: 2.301200]\n",
      "epoch:7 step:7458 [D loss: 0.572089, acc: 69.53%] [G loss: 2.492282]\n",
      "epoch:7 step:7459 [D loss: 0.688290, acc: 61.72%] [G loss: 2.181919]\n",
      "epoch:7 step:7460 [D loss: 0.563737, acc: 68.75%] [G loss: 2.335061]\n",
      "epoch:7 step:7461 [D loss: 0.623857, acc: 65.62%] [G loss: 2.133871]\n",
      "epoch:7 step:7462 [D loss: 0.653404, acc: 61.72%] [G loss: 2.386565]\n",
      "epoch:7 step:7463 [D loss: 0.616410, acc: 66.41%] [G loss: 2.402171]\n",
      "epoch:7 step:7464 [D loss: 0.590986, acc: 67.97%] [G loss: 2.246306]\n",
      "epoch:7 step:7465 [D loss: 0.665009, acc: 58.59%] [G loss: 2.235641]\n",
      "epoch:7 step:7466 [D loss: 0.642985, acc: 67.19%] [G loss: 2.491922]\n",
      "epoch:7 step:7467 [D loss: 0.656177, acc: 62.50%] [G loss: 2.064122]\n",
      "epoch:7 step:7468 [D loss: 0.572589, acc: 67.19%] [G loss: 2.631340]\n",
      "epoch:7 step:7469 [D loss: 0.560260, acc: 72.66%] [G loss: 2.409984]\n",
      "epoch:7 step:7470 [D loss: 0.570387, acc: 65.62%] [G loss: 2.408319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:7471 [D loss: 0.598003, acc: 65.62%] [G loss: 2.604241]\n",
      "epoch:7 step:7472 [D loss: 0.653476, acc: 60.94%] [G loss: 2.336559]\n",
      "epoch:7 step:7473 [D loss: 0.659057, acc: 64.84%] [G loss: 2.158160]\n",
      "epoch:7 step:7474 [D loss: 0.686801, acc: 56.25%] [G loss: 2.265501]\n",
      "epoch:7 step:7475 [D loss: 0.597497, acc: 68.75%] [G loss: 2.342314]\n",
      "epoch:7 step:7476 [D loss: 0.637601, acc: 63.28%] [G loss: 2.333680]\n",
      "epoch:7 step:7477 [D loss: 0.570111, acc: 74.22%] [G loss: 2.320899]\n",
      "epoch:7 step:7478 [D loss: 0.548177, acc: 71.09%] [G loss: 2.254657]\n",
      "epoch:7 step:7479 [D loss: 0.683342, acc: 61.72%] [G loss: 2.025248]\n",
      "epoch:7 step:7480 [D loss: 0.603057, acc: 64.06%] [G loss: 2.325035]\n",
      "epoch:7 step:7481 [D loss: 0.628070, acc: 67.97%] [G loss: 2.240035]\n",
      "epoch:7 step:7482 [D loss: 0.559044, acc: 67.19%] [G loss: 2.175347]\n",
      "epoch:7 step:7483 [D loss: 0.500536, acc: 79.69%] [G loss: 2.790088]\n",
      "epoch:7 step:7484 [D loss: 0.514583, acc: 74.22%] [G loss: 2.678519]\n",
      "epoch:7 step:7485 [D loss: 0.645117, acc: 68.75%] [G loss: 2.612241]\n",
      "epoch:7 step:7486 [D loss: 0.631954, acc: 62.50%] [G loss: 2.524307]\n",
      "epoch:7 step:7487 [D loss: 0.733173, acc: 60.16%] [G loss: 2.218342]\n",
      "epoch:7 step:7488 [D loss: 0.720182, acc: 57.81%] [G loss: 2.408720]\n",
      "epoch:7 step:7489 [D loss: 0.551286, acc: 68.75%] [G loss: 2.424321]\n",
      "epoch:7 step:7490 [D loss: 0.622265, acc: 67.97%] [G loss: 2.350552]\n",
      "epoch:7 step:7491 [D loss: 0.611580, acc: 65.62%] [G loss: 2.336776]\n",
      "epoch:7 step:7492 [D loss: 0.596520, acc: 66.41%] [G loss: 2.500580]\n",
      "epoch:7 step:7493 [D loss: 0.543636, acc: 74.22%] [G loss: 2.501772]\n",
      "epoch:7 step:7494 [D loss: 0.569189, acc: 71.09%] [G loss: 2.162212]\n",
      "epoch:7 step:7495 [D loss: 0.572434, acc: 71.88%] [G loss: 2.487588]\n",
      "epoch:7 step:7496 [D loss: 0.551829, acc: 75.78%] [G loss: 3.010971]\n",
      "epoch:8 step:7497 [D loss: 0.620078, acc: 64.84%] [G loss: 2.442804]\n",
      "epoch:8 step:7498 [D loss: 0.613399, acc: 68.75%] [G loss: 2.468509]\n",
      "epoch:8 step:7499 [D loss: 0.609590, acc: 64.84%] [G loss: 2.399928]\n",
      "epoch:8 step:7500 [D loss: 0.585926, acc: 67.97%] [G loss: 2.377919]\n",
      "epoch:8 step:7501 [D loss: 0.583863, acc: 66.41%] [G loss: 2.380727]\n",
      "epoch:8 step:7502 [D loss: 0.648950, acc: 64.06%] [G loss: 2.422363]\n",
      "epoch:8 step:7503 [D loss: 0.569802, acc: 69.53%] [G loss: 2.466751]\n",
      "epoch:8 step:7504 [D loss: 0.686728, acc: 64.06%] [G loss: 2.441156]\n",
      "epoch:8 step:7505 [D loss: 0.579459, acc: 69.53%] [G loss: 2.375804]\n",
      "epoch:8 step:7506 [D loss: 0.538640, acc: 78.91%] [G loss: 2.513485]\n",
      "epoch:8 step:7507 [D loss: 0.556667, acc: 74.22%] [G loss: 2.335177]\n",
      "epoch:8 step:7508 [D loss: 0.636186, acc: 65.62%] [G loss: 2.584159]\n",
      "epoch:8 step:7509 [D loss: 0.616496, acc: 64.84%] [G loss: 2.329147]\n",
      "epoch:8 step:7510 [D loss: 0.636686, acc: 62.50%] [G loss: 2.380248]\n",
      "epoch:8 step:7511 [D loss: 0.550422, acc: 73.44%] [G loss: 2.411191]\n",
      "epoch:8 step:7512 [D loss: 0.621074, acc: 70.31%] [G loss: 2.611034]\n",
      "epoch:8 step:7513 [D loss: 0.647133, acc: 67.19%] [G loss: 2.186968]\n",
      "epoch:8 step:7514 [D loss: 0.696645, acc: 58.59%] [G loss: 2.144255]\n",
      "epoch:8 step:7515 [D loss: 0.700804, acc: 58.59%] [G loss: 2.068539]\n",
      "epoch:8 step:7516 [D loss: 0.684802, acc: 57.81%] [G loss: 1.840733]\n",
      "epoch:8 step:7517 [D loss: 0.665952, acc: 65.62%] [G loss: 2.085746]\n",
      "epoch:8 step:7518 [D loss: 0.606776, acc: 67.97%] [G loss: 2.226006]\n",
      "epoch:8 step:7519 [D loss: 0.638720, acc: 67.19%] [G loss: 2.185874]\n",
      "epoch:8 step:7520 [D loss: 0.622557, acc: 69.53%] [G loss: 2.396018]\n",
      "epoch:8 step:7521 [D loss: 0.607090, acc: 70.31%] [G loss: 2.390765]\n",
      "epoch:8 step:7522 [D loss: 0.618515, acc: 64.84%] [G loss: 2.276619]\n",
      "epoch:8 step:7523 [D loss: 0.644974, acc: 61.72%] [G loss: 2.106634]\n",
      "epoch:8 step:7524 [D loss: 0.616890, acc: 70.31%] [G loss: 2.519040]\n",
      "epoch:8 step:7525 [D loss: 0.606650, acc: 66.41%] [G loss: 2.388594]\n",
      "epoch:8 step:7526 [D loss: 0.631447, acc: 62.50%] [G loss: 1.996809]\n",
      "epoch:8 step:7527 [D loss: 0.665131, acc: 60.94%] [G loss: 2.159813]\n",
      "epoch:8 step:7528 [D loss: 0.609624, acc: 64.06%] [G loss: 2.204777]\n",
      "epoch:8 step:7529 [D loss: 0.578926, acc: 76.56%] [G loss: 2.253476]\n",
      "epoch:8 step:7530 [D loss: 0.603953, acc: 66.41%] [G loss: 2.386972]\n",
      "epoch:8 step:7531 [D loss: 0.613463, acc: 66.41%] [G loss: 2.316334]\n",
      "epoch:8 step:7532 [D loss: 0.537921, acc: 78.12%] [G loss: 2.645450]\n",
      "epoch:8 step:7533 [D loss: 0.579691, acc: 73.44%] [G loss: 2.483326]\n",
      "epoch:8 step:7534 [D loss: 0.652183, acc: 64.84%] [G loss: 2.371679]\n",
      "epoch:8 step:7535 [D loss: 0.595158, acc: 67.97%] [G loss: 2.254685]\n",
      "epoch:8 step:7536 [D loss: 0.553005, acc: 71.09%] [G loss: 2.690153]\n",
      "epoch:8 step:7537 [D loss: 0.596358, acc: 67.19%] [G loss: 2.297152]\n",
      "epoch:8 step:7538 [D loss: 0.622052, acc: 64.06%] [G loss: 2.366388]\n",
      "epoch:8 step:7539 [D loss: 0.589052, acc: 67.19%] [G loss: 2.172834]\n",
      "epoch:8 step:7540 [D loss: 0.677272, acc: 60.94%] [G loss: 2.193320]\n",
      "epoch:8 step:7541 [D loss: 0.612615, acc: 67.97%] [G loss: 2.259130]\n",
      "epoch:8 step:7542 [D loss: 0.655859, acc: 66.41%] [G loss: 2.232238]\n",
      "epoch:8 step:7543 [D loss: 0.552580, acc: 74.22%] [G loss: 2.201035]\n",
      "epoch:8 step:7544 [D loss: 0.609351, acc: 67.19%] [G loss: 2.332131]\n",
      "epoch:8 step:7545 [D loss: 0.595946, acc: 67.97%] [G loss: 2.178376]\n",
      "epoch:8 step:7546 [D loss: 0.630558, acc: 64.06%] [G loss: 2.229023]\n",
      "epoch:8 step:7547 [D loss: 0.723989, acc: 61.72%] [G loss: 2.200003]\n",
      "epoch:8 step:7548 [D loss: 0.605663, acc: 67.19%] [G loss: 2.340478]\n",
      "epoch:8 step:7549 [D loss: 0.574873, acc: 71.09%] [G loss: 2.335153]\n",
      "epoch:8 step:7550 [D loss: 0.554265, acc: 75.00%] [G loss: 2.468985]\n",
      "epoch:8 step:7551 [D loss: 0.644173, acc: 64.84%] [G loss: 2.529003]\n",
      "epoch:8 step:7552 [D loss: 0.576164, acc: 70.31%] [G loss: 2.401082]\n",
      "epoch:8 step:7553 [D loss: 0.673034, acc: 61.72%] [G loss: 2.259723]\n",
      "epoch:8 step:7554 [D loss: 0.587239, acc: 70.31%] [G loss: 2.322800]\n",
      "epoch:8 step:7555 [D loss: 0.613824, acc: 64.06%] [G loss: 2.129256]\n",
      "epoch:8 step:7556 [D loss: 0.617361, acc: 67.19%] [G loss: 2.277040]\n",
      "epoch:8 step:7557 [D loss: 0.632354, acc: 63.28%] [G loss: 2.267993]\n",
      "epoch:8 step:7558 [D loss: 0.619790, acc: 69.53%] [G loss: 2.102823]\n",
      "epoch:8 step:7559 [D loss: 0.611863, acc: 67.97%] [G loss: 2.052464]\n",
      "epoch:8 step:7560 [D loss: 0.676923, acc: 59.38%] [G loss: 2.289703]\n",
      "epoch:8 step:7561 [D loss: 0.551401, acc: 72.66%] [G loss: 2.281833]\n",
      "epoch:8 step:7562 [D loss: 0.590711, acc: 73.44%] [G loss: 2.190569]\n",
      "epoch:8 step:7563 [D loss: 0.625647, acc: 62.50%] [G loss: 2.124779]\n",
      "epoch:8 step:7564 [D loss: 0.562487, acc: 67.97%] [G loss: 2.404272]\n",
      "epoch:8 step:7565 [D loss: 0.526530, acc: 78.12%] [G loss: 2.454404]\n",
      "epoch:8 step:7566 [D loss: 0.619198, acc: 65.62%] [G loss: 2.459935]\n",
      "epoch:8 step:7567 [D loss: 0.581008, acc: 67.97%] [G loss: 2.312785]\n",
      "epoch:8 step:7568 [D loss: 0.577949, acc: 65.62%] [G loss: 2.228852]\n",
      "epoch:8 step:7569 [D loss: 0.592420, acc: 71.88%] [G loss: 2.243666]\n",
      "epoch:8 step:7570 [D loss: 0.556858, acc: 67.97%] [G loss: 2.425070]\n",
      "epoch:8 step:7571 [D loss: 0.503746, acc: 76.56%] [G loss: 2.720823]\n",
      "epoch:8 step:7572 [D loss: 0.606090, acc: 64.06%] [G loss: 2.796475]\n",
      "epoch:8 step:7573 [D loss: 0.565401, acc: 71.88%] [G loss: 2.834606]\n",
      "epoch:8 step:7574 [D loss: 0.625386, acc: 62.50%] [G loss: 2.180022]\n",
      "epoch:8 step:7575 [D loss: 0.629295, acc: 66.41%] [G loss: 2.081965]\n",
      "epoch:8 step:7576 [D loss: 0.663794, acc: 62.50%] [G loss: 2.252480]\n",
      "epoch:8 step:7577 [D loss: 0.697163, acc: 57.81%] [G loss: 2.083220]\n",
      "epoch:8 step:7578 [D loss: 0.621855, acc: 69.53%] [G loss: 2.253849]\n",
      "epoch:8 step:7579 [D loss: 0.575538, acc: 71.09%] [G loss: 2.527397]\n",
      "epoch:8 step:7580 [D loss: 0.589021, acc: 70.31%] [G loss: 2.376880]\n",
      "epoch:8 step:7581 [D loss: 0.637752, acc: 64.06%] [G loss: 2.068822]\n",
      "epoch:8 step:7582 [D loss: 0.611063, acc: 64.84%] [G loss: 2.078691]\n",
      "epoch:8 step:7583 [D loss: 0.599122, acc: 70.31%] [G loss: 2.301722]\n",
      "epoch:8 step:7584 [D loss: 0.603638, acc: 66.41%] [G loss: 2.387694]\n",
      "epoch:8 step:7585 [D loss: 0.647891, acc: 66.41%] [G loss: 2.216776]\n",
      "epoch:8 step:7586 [D loss: 0.596481, acc: 69.53%] [G loss: 2.351549]\n",
      "epoch:8 step:7587 [D loss: 0.641703, acc: 59.38%] [G loss: 2.230795]\n",
      "epoch:8 step:7588 [D loss: 0.570942, acc: 72.66%] [G loss: 2.330516]\n",
      "epoch:8 step:7589 [D loss: 0.571829, acc: 67.19%] [G loss: 2.322977]\n",
      "epoch:8 step:7590 [D loss: 0.644290, acc: 64.84%] [G loss: 2.218168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7591 [D loss: 0.627777, acc: 65.62%] [G loss: 2.221663]\n",
      "epoch:8 step:7592 [D loss: 0.564810, acc: 70.31%] [G loss: 2.347858]\n",
      "epoch:8 step:7593 [D loss: 0.565213, acc: 71.09%] [G loss: 2.412660]\n",
      "epoch:8 step:7594 [D loss: 0.676124, acc: 60.94%] [G loss: 2.201568]\n",
      "epoch:8 step:7595 [D loss: 0.658144, acc: 66.41%] [G loss: 2.157014]\n",
      "epoch:8 step:7596 [D loss: 0.625055, acc: 65.62%] [G loss: 2.197234]\n",
      "epoch:8 step:7597 [D loss: 0.610309, acc: 65.62%] [G loss: 2.261683]\n",
      "epoch:8 step:7598 [D loss: 0.636810, acc: 61.72%] [G loss: 2.177290]\n",
      "epoch:8 step:7599 [D loss: 0.622677, acc: 64.06%] [G loss: 2.450911]\n",
      "epoch:8 step:7600 [D loss: 0.590530, acc: 63.28%] [G loss: 2.251364]\n",
      "epoch:8 step:7601 [D loss: 0.541096, acc: 76.56%] [G loss: 2.138582]\n",
      "epoch:8 step:7602 [D loss: 0.572629, acc: 74.22%] [G loss: 2.451902]\n",
      "epoch:8 step:7603 [D loss: 0.592325, acc: 66.41%] [G loss: 2.444538]\n",
      "epoch:8 step:7604 [D loss: 0.733964, acc: 57.81%] [G loss: 2.091424]\n",
      "epoch:8 step:7605 [D loss: 0.756797, acc: 52.34%] [G loss: 2.085312]\n",
      "epoch:8 step:7606 [D loss: 0.593138, acc: 71.09%] [G loss: 2.168143]\n",
      "epoch:8 step:7607 [D loss: 0.630805, acc: 67.19%] [G loss: 2.176112]\n",
      "epoch:8 step:7608 [D loss: 0.634985, acc: 64.06%] [G loss: 2.264977]\n",
      "epoch:8 step:7609 [D loss: 0.663440, acc: 63.28%] [G loss: 2.225149]\n",
      "epoch:8 step:7610 [D loss: 0.676533, acc: 58.59%] [G loss: 2.138088]\n",
      "epoch:8 step:7611 [D loss: 0.631462, acc: 66.41%] [G loss: 2.218117]\n",
      "epoch:8 step:7612 [D loss: 0.524971, acc: 74.22%] [G loss: 2.460863]\n",
      "epoch:8 step:7613 [D loss: 0.556484, acc: 73.44%] [G loss: 2.505225]\n",
      "epoch:8 step:7614 [D loss: 0.661440, acc: 61.72%] [G loss: 2.387309]\n",
      "epoch:8 step:7615 [D loss: 0.547851, acc: 70.31%] [G loss: 2.521833]\n",
      "epoch:8 step:7616 [D loss: 0.655627, acc: 60.16%] [G loss: 2.316875]\n",
      "epoch:8 step:7617 [D loss: 0.615024, acc: 67.19%] [G loss: 2.405736]\n",
      "epoch:8 step:7618 [D loss: 0.602121, acc: 63.28%] [G loss: 2.452683]\n",
      "epoch:8 step:7619 [D loss: 0.665646, acc: 59.38%] [G loss: 2.246426]\n",
      "epoch:8 step:7620 [D loss: 0.663871, acc: 62.50%] [G loss: 2.316485]\n",
      "epoch:8 step:7621 [D loss: 0.659342, acc: 59.38%] [G loss: 2.050374]\n",
      "epoch:8 step:7622 [D loss: 0.598682, acc: 65.62%] [G loss: 2.249530]\n",
      "epoch:8 step:7623 [D loss: 0.671089, acc: 56.25%] [G loss: 2.115740]\n",
      "epoch:8 step:7624 [D loss: 0.666031, acc: 60.16%] [G loss: 2.086740]\n",
      "epoch:8 step:7625 [D loss: 0.660936, acc: 62.50%] [G loss: 2.085085]\n",
      "epoch:8 step:7626 [D loss: 0.681612, acc: 60.16%] [G loss: 2.168854]\n",
      "epoch:8 step:7627 [D loss: 0.659507, acc: 64.06%] [G loss: 2.088478]\n",
      "epoch:8 step:7628 [D loss: 0.579594, acc: 73.44%] [G loss: 2.292596]\n",
      "epoch:8 step:7629 [D loss: 0.652353, acc: 60.94%] [G loss: 2.203871]\n",
      "epoch:8 step:7630 [D loss: 0.605962, acc: 64.84%] [G loss: 2.155234]\n",
      "epoch:8 step:7631 [D loss: 0.617381, acc: 62.50%] [G loss: 2.139012]\n",
      "epoch:8 step:7632 [D loss: 0.657055, acc: 58.59%] [G loss: 2.004797]\n",
      "epoch:8 step:7633 [D loss: 0.632459, acc: 66.41%] [G loss: 2.145946]\n",
      "epoch:8 step:7634 [D loss: 0.615436, acc: 68.75%] [G loss: 2.307268]\n",
      "epoch:8 step:7635 [D loss: 0.562072, acc: 67.97%] [G loss: 2.276508]\n",
      "epoch:8 step:7636 [D loss: 0.613390, acc: 64.06%] [G loss: 2.171355]\n",
      "epoch:8 step:7637 [D loss: 0.601167, acc: 71.88%] [G loss: 2.157875]\n",
      "epoch:8 step:7638 [D loss: 0.635818, acc: 61.72%] [G loss: 1.998891]\n",
      "epoch:8 step:7639 [D loss: 0.682841, acc: 55.47%] [G loss: 2.079207]\n",
      "epoch:8 step:7640 [D loss: 0.595150, acc: 67.97%] [G loss: 2.266129]\n",
      "epoch:8 step:7641 [D loss: 0.627967, acc: 65.62%] [G loss: 2.176230]\n",
      "epoch:8 step:7642 [D loss: 0.628390, acc: 64.06%] [G loss: 2.217369]\n",
      "epoch:8 step:7643 [D loss: 0.627225, acc: 58.59%] [G loss: 2.004609]\n",
      "epoch:8 step:7644 [D loss: 0.648442, acc: 58.59%] [G loss: 1.982594]\n",
      "epoch:8 step:7645 [D loss: 0.585572, acc: 69.53%] [G loss: 2.514923]\n",
      "epoch:8 step:7646 [D loss: 0.609405, acc: 71.09%] [G loss: 2.243612]\n",
      "epoch:8 step:7647 [D loss: 0.531136, acc: 71.88%] [G loss: 2.708662]\n",
      "epoch:8 step:7648 [D loss: 0.647277, acc: 63.28%] [G loss: 2.439391]\n",
      "epoch:8 step:7649 [D loss: 0.585040, acc: 69.53%] [G loss: 2.102801]\n",
      "epoch:8 step:7650 [D loss: 0.589988, acc: 66.41%] [G loss: 2.450550]\n",
      "epoch:8 step:7651 [D loss: 0.625201, acc: 61.72%] [G loss: 2.190475]\n",
      "epoch:8 step:7652 [D loss: 0.573296, acc: 70.31%] [G loss: 2.189137]\n",
      "epoch:8 step:7653 [D loss: 0.682389, acc: 52.34%] [G loss: 2.106723]\n",
      "epoch:8 step:7654 [D loss: 0.626579, acc: 63.28%] [G loss: 2.169956]\n",
      "epoch:8 step:7655 [D loss: 0.619037, acc: 66.41%] [G loss: 2.284443]\n",
      "epoch:8 step:7656 [D loss: 0.622840, acc: 62.50%] [G loss: 2.116288]\n",
      "epoch:8 step:7657 [D loss: 0.603216, acc: 65.62%] [G loss: 2.188690]\n",
      "epoch:8 step:7658 [D loss: 0.621628, acc: 64.06%] [G loss: 2.113142]\n",
      "epoch:8 step:7659 [D loss: 0.567931, acc: 74.22%] [G loss: 2.192561]\n",
      "epoch:8 step:7660 [D loss: 0.580841, acc: 71.88%] [G loss: 2.145982]\n",
      "epoch:8 step:7661 [D loss: 0.619275, acc: 66.41%] [G loss: 2.318835]\n",
      "epoch:8 step:7662 [D loss: 0.598719, acc: 65.62%] [G loss: 2.205421]\n",
      "epoch:8 step:7663 [D loss: 0.610479, acc: 63.28%] [G loss: 2.196442]\n",
      "epoch:8 step:7664 [D loss: 0.606583, acc: 65.62%] [G loss: 2.242004]\n",
      "epoch:8 step:7665 [D loss: 0.632317, acc: 65.62%] [G loss: 2.224413]\n",
      "epoch:8 step:7666 [D loss: 0.664155, acc: 60.16%] [G loss: 2.060040]\n",
      "epoch:8 step:7667 [D loss: 0.587977, acc: 73.44%] [G loss: 2.113323]\n",
      "epoch:8 step:7668 [D loss: 0.684673, acc: 58.59%] [G loss: 2.070808]\n",
      "epoch:8 step:7669 [D loss: 0.608656, acc: 68.75%] [G loss: 2.098570]\n",
      "epoch:8 step:7670 [D loss: 0.640148, acc: 64.84%] [G loss: 2.126484]\n",
      "epoch:8 step:7671 [D loss: 0.619359, acc: 64.84%] [G loss: 2.123086]\n",
      "epoch:8 step:7672 [D loss: 0.620099, acc: 60.94%] [G loss: 2.235767]\n",
      "epoch:8 step:7673 [D loss: 0.576946, acc: 70.31%] [G loss: 2.127416]\n",
      "epoch:8 step:7674 [D loss: 0.591734, acc: 71.88%] [G loss: 2.185481]\n",
      "epoch:8 step:7675 [D loss: 0.638516, acc: 66.41%] [G loss: 2.128992]\n",
      "epoch:8 step:7676 [D loss: 0.668907, acc: 62.50%] [G loss: 2.206385]\n",
      "epoch:8 step:7677 [D loss: 0.632723, acc: 64.84%] [G loss: 2.194407]\n",
      "epoch:8 step:7678 [D loss: 0.679805, acc: 62.50%] [G loss: 2.061229]\n",
      "epoch:8 step:7679 [D loss: 0.610479, acc: 65.62%] [G loss: 2.271685]\n",
      "epoch:8 step:7680 [D loss: 0.638289, acc: 64.84%] [G loss: 2.246252]\n",
      "epoch:8 step:7681 [D loss: 0.655985, acc: 64.84%] [G loss: 2.149641]\n",
      "epoch:8 step:7682 [D loss: 0.645810, acc: 64.84%] [G loss: 2.095738]\n",
      "epoch:8 step:7683 [D loss: 0.676735, acc: 52.34%] [G loss: 2.321305]\n",
      "epoch:8 step:7684 [D loss: 0.632042, acc: 60.94%] [G loss: 2.063532]\n",
      "epoch:8 step:7685 [D loss: 0.662152, acc: 68.75%] [G loss: 2.050280]\n",
      "epoch:8 step:7686 [D loss: 0.619145, acc: 60.94%] [G loss: 2.247781]\n",
      "epoch:8 step:7687 [D loss: 0.594841, acc: 65.62%] [G loss: 2.372488]\n",
      "epoch:8 step:7688 [D loss: 0.580870, acc: 67.97%] [G loss: 2.450294]\n",
      "epoch:8 step:7689 [D loss: 0.633322, acc: 65.62%] [G loss: 2.281145]\n",
      "epoch:8 step:7690 [D loss: 0.564991, acc: 71.88%] [G loss: 2.340530]\n",
      "epoch:8 step:7691 [D loss: 0.596429, acc: 66.41%] [G loss: 2.250609]\n",
      "epoch:8 step:7692 [D loss: 0.644088, acc: 59.38%] [G loss: 2.253657]\n",
      "epoch:8 step:7693 [D loss: 0.616220, acc: 70.31%] [G loss: 2.486438]\n",
      "epoch:8 step:7694 [D loss: 0.574824, acc: 67.97%] [G loss: 2.446323]\n",
      "epoch:8 step:7695 [D loss: 0.650339, acc: 64.84%] [G loss: 2.257142]\n",
      "epoch:8 step:7696 [D loss: 0.709597, acc: 56.25%] [G loss: 2.063079]\n",
      "epoch:8 step:7697 [D loss: 0.612094, acc: 64.84%] [G loss: 2.300349]\n",
      "epoch:8 step:7698 [D loss: 0.641357, acc: 67.19%] [G loss: 2.090077]\n",
      "epoch:8 step:7699 [D loss: 0.640410, acc: 65.62%] [G loss: 1.964180]\n",
      "epoch:8 step:7700 [D loss: 0.620895, acc: 66.41%] [G loss: 2.226017]\n",
      "epoch:8 step:7701 [D loss: 0.644134, acc: 64.84%] [G loss: 2.071978]\n",
      "epoch:8 step:7702 [D loss: 0.590513, acc: 72.66%] [G loss: 2.385553]\n",
      "epoch:8 step:7703 [D loss: 0.491351, acc: 79.69%] [G loss: 2.494402]\n",
      "epoch:8 step:7704 [D loss: 0.633812, acc: 64.84%] [G loss: 2.371378]\n",
      "epoch:8 step:7705 [D loss: 0.506851, acc: 82.03%] [G loss: 2.533558]\n",
      "epoch:8 step:7706 [D loss: 0.774605, acc: 55.47%] [G loss: 2.099568]\n",
      "epoch:8 step:7707 [D loss: 0.681304, acc: 63.28%] [G loss: 2.048840]\n",
      "epoch:8 step:7708 [D loss: 0.684057, acc: 54.69%] [G loss: 2.213476]\n",
      "epoch:8 step:7709 [D loss: 0.618004, acc: 64.06%] [G loss: 2.145716]\n",
      "epoch:8 step:7710 [D loss: 0.566393, acc: 71.88%] [G loss: 2.246747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7711 [D loss: 0.599304, acc: 67.97%] [G loss: 2.126938]\n",
      "epoch:8 step:7712 [D loss: 0.603701, acc: 67.19%] [G loss: 2.136962]\n",
      "epoch:8 step:7713 [D loss: 0.570209, acc: 71.09%] [G loss: 2.317080]\n",
      "epoch:8 step:7714 [D loss: 0.592805, acc: 70.31%] [G loss: 2.280534]\n",
      "epoch:8 step:7715 [D loss: 0.675511, acc: 61.72%] [G loss: 2.342808]\n",
      "epoch:8 step:7716 [D loss: 0.637216, acc: 62.50%] [G loss: 2.078725]\n",
      "epoch:8 step:7717 [D loss: 0.640227, acc: 62.50%] [G loss: 2.201923]\n",
      "epoch:8 step:7718 [D loss: 0.554746, acc: 71.88%] [G loss: 2.245179]\n",
      "epoch:8 step:7719 [D loss: 0.670979, acc: 62.50%] [G loss: 2.213814]\n",
      "epoch:8 step:7720 [D loss: 0.622311, acc: 66.41%] [G loss: 2.291546]\n",
      "epoch:8 step:7721 [D loss: 0.679928, acc: 60.94%] [G loss: 2.168158]\n",
      "epoch:8 step:7722 [D loss: 0.606981, acc: 65.62%] [G loss: 2.127790]\n",
      "epoch:8 step:7723 [D loss: 0.678636, acc: 58.59%] [G loss: 2.160271]\n",
      "epoch:8 step:7724 [D loss: 0.696439, acc: 59.38%] [G loss: 1.979292]\n",
      "epoch:8 step:7725 [D loss: 0.527434, acc: 75.00%] [G loss: 2.188284]\n",
      "epoch:8 step:7726 [D loss: 0.598526, acc: 71.09%] [G loss: 2.734602]\n",
      "epoch:8 step:7727 [D loss: 0.498539, acc: 75.78%] [G loss: 2.558707]\n",
      "epoch:8 step:7728 [D loss: 0.508709, acc: 75.00%] [G loss: 2.858878]\n",
      "epoch:8 step:7729 [D loss: 0.684766, acc: 64.06%] [G loss: 2.130196]\n",
      "epoch:8 step:7730 [D loss: 0.679457, acc: 62.50%] [G loss: 2.195577]\n",
      "epoch:8 step:7731 [D loss: 0.672632, acc: 57.81%] [G loss: 2.067108]\n",
      "epoch:8 step:7732 [D loss: 0.602167, acc: 68.75%] [G loss: 2.277141]\n",
      "epoch:8 step:7733 [D loss: 0.685839, acc: 57.03%] [G loss: 2.103934]\n",
      "epoch:8 step:7734 [D loss: 0.604651, acc: 67.97%] [G loss: 2.128683]\n",
      "epoch:8 step:7735 [D loss: 0.588239, acc: 66.41%] [G loss: 2.416188]\n",
      "epoch:8 step:7736 [D loss: 0.630479, acc: 66.41%] [G loss: 2.197141]\n",
      "epoch:8 step:7737 [D loss: 0.638134, acc: 66.41%] [G loss: 2.124170]\n",
      "epoch:8 step:7738 [D loss: 0.594684, acc: 67.19%] [G loss: 2.295639]\n",
      "epoch:8 step:7739 [D loss: 0.642877, acc: 62.50%] [G loss: 2.128524]\n",
      "epoch:8 step:7740 [D loss: 0.611082, acc: 64.06%] [G loss: 2.166506]\n",
      "epoch:8 step:7741 [D loss: 0.573255, acc: 67.19%] [G loss: 2.264193]\n",
      "epoch:8 step:7742 [D loss: 0.700558, acc: 58.59%] [G loss: 2.158962]\n",
      "epoch:8 step:7743 [D loss: 0.658955, acc: 66.41%] [G loss: 2.355599]\n",
      "epoch:8 step:7744 [D loss: 0.584933, acc: 69.53%] [G loss: 2.227062]\n",
      "epoch:8 step:7745 [D loss: 0.704175, acc: 57.81%] [G loss: 2.086589]\n",
      "epoch:8 step:7746 [D loss: 0.651301, acc: 57.81%] [G loss: 2.064895]\n",
      "epoch:8 step:7747 [D loss: 0.644103, acc: 63.28%] [G loss: 2.050458]\n",
      "epoch:8 step:7748 [D loss: 0.626831, acc: 65.62%] [G loss: 2.142130]\n",
      "epoch:8 step:7749 [D loss: 0.630593, acc: 65.62%] [G loss: 2.189955]\n",
      "epoch:8 step:7750 [D loss: 0.624226, acc: 63.28%] [G loss: 2.048986]\n",
      "epoch:8 step:7751 [D loss: 0.644124, acc: 63.28%] [G loss: 1.996485]\n",
      "epoch:8 step:7752 [D loss: 0.657408, acc: 62.50%] [G loss: 1.934627]\n",
      "epoch:8 step:7753 [D loss: 0.671851, acc: 60.16%] [G loss: 2.088253]\n",
      "epoch:8 step:7754 [D loss: 0.558412, acc: 69.53%] [G loss: 2.044763]\n",
      "epoch:8 step:7755 [D loss: 0.617870, acc: 65.62%] [G loss: 2.343904]\n",
      "epoch:8 step:7756 [D loss: 0.635420, acc: 66.41%] [G loss: 2.069860]\n",
      "epoch:8 step:7757 [D loss: 0.632978, acc: 68.75%] [G loss: 2.119795]\n",
      "epoch:8 step:7758 [D loss: 0.609239, acc: 68.75%] [G loss: 2.319230]\n",
      "epoch:8 step:7759 [D loss: 0.691702, acc: 62.50%] [G loss: 2.152779]\n",
      "epoch:8 step:7760 [D loss: 0.588450, acc: 74.22%] [G loss: 2.264825]\n",
      "epoch:8 step:7761 [D loss: 0.704708, acc: 60.94%] [G loss: 1.962066]\n",
      "epoch:8 step:7762 [D loss: 0.621497, acc: 66.41%] [G loss: 2.116073]\n",
      "epoch:8 step:7763 [D loss: 0.612224, acc: 63.28%] [G loss: 2.041246]\n",
      "epoch:8 step:7764 [D loss: 0.596332, acc: 68.75%] [G loss: 2.109823]\n",
      "epoch:8 step:7765 [D loss: 0.629499, acc: 66.41%] [G loss: 2.088897]\n",
      "epoch:8 step:7766 [D loss: 0.612006, acc: 67.97%] [G loss: 2.280550]\n",
      "epoch:8 step:7767 [D loss: 0.611114, acc: 62.50%] [G loss: 2.108833]\n",
      "epoch:8 step:7768 [D loss: 0.623126, acc: 68.75%] [G loss: 2.270657]\n",
      "epoch:8 step:7769 [D loss: 0.676480, acc: 60.16%] [G loss: 2.156764]\n",
      "epoch:8 step:7770 [D loss: 0.580586, acc: 69.53%] [G loss: 2.225937]\n",
      "epoch:8 step:7771 [D loss: 0.605079, acc: 64.84%] [G loss: 2.177309]\n",
      "epoch:8 step:7772 [D loss: 0.590554, acc: 67.97%] [G loss: 2.355185]\n",
      "epoch:8 step:7773 [D loss: 0.671584, acc: 65.62%] [G loss: 1.993316]\n",
      "epoch:8 step:7774 [D loss: 0.626347, acc: 68.75%] [G loss: 1.942528]\n",
      "epoch:8 step:7775 [D loss: 0.620887, acc: 63.28%] [G loss: 2.163417]\n",
      "epoch:8 step:7776 [D loss: 0.573963, acc: 66.41%] [G loss: 2.207392]\n",
      "epoch:8 step:7777 [D loss: 0.668517, acc: 56.25%] [G loss: 2.173775]\n",
      "epoch:8 step:7778 [D loss: 0.620491, acc: 66.41%] [G loss: 2.177229]\n",
      "epoch:8 step:7779 [D loss: 0.597710, acc: 62.50%] [G loss: 2.169115]\n",
      "epoch:8 step:7780 [D loss: 0.622915, acc: 66.41%] [G loss: 2.172534]\n",
      "epoch:8 step:7781 [D loss: 0.643481, acc: 67.19%] [G loss: 2.280452]\n",
      "epoch:8 step:7782 [D loss: 0.582730, acc: 65.62%] [G loss: 2.423257]\n",
      "epoch:8 step:7783 [D loss: 0.642388, acc: 62.50%] [G loss: 2.146779]\n",
      "epoch:8 step:7784 [D loss: 0.650622, acc: 62.50%] [G loss: 1.940238]\n",
      "epoch:8 step:7785 [D loss: 0.644652, acc: 67.19%] [G loss: 2.165834]\n",
      "epoch:8 step:7786 [D loss: 0.676349, acc: 55.47%] [G loss: 2.131082]\n",
      "epoch:8 step:7787 [D loss: 0.543055, acc: 74.22%] [G loss: 2.174280]\n",
      "epoch:8 step:7788 [D loss: 0.644229, acc: 64.84%] [G loss: 2.154562]\n",
      "epoch:8 step:7789 [D loss: 0.595397, acc: 69.53%] [G loss: 2.267747]\n",
      "epoch:8 step:7790 [D loss: 0.561857, acc: 73.44%] [G loss: 2.316300]\n",
      "epoch:8 step:7791 [D loss: 0.624276, acc: 58.59%] [G loss: 2.115704]\n",
      "epoch:8 step:7792 [D loss: 0.569460, acc: 68.75%] [G loss: 2.374204]\n",
      "epoch:8 step:7793 [D loss: 0.610636, acc: 64.06%] [G loss: 2.223147]\n",
      "epoch:8 step:7794 [D loss: 0.500293, acc: 75.78%] [G loss: 2.454103]\n",
      "epoch:8 step:7795 [D loss: 0.604470, acc: 68.75%] [G loss: 2.320780]\n",
      "epoch:8 step:7796 [D loss: 0.621863, acc: 68.75%] [G loss: 2.577654]\n",
      "epoch:8 step:7797 [D loss: 0.739177, acc: 51.56%] [G loss: 2.068478]\n",
      "epoch:8 step:7798 [D loss: 0.663768, acc: 64.84%] [G loss: 2.146688]\n",
      "epoch:8 step:7799 [D loss: 0.649020, acc: 66.41%] [G loss: 2.263178]\n",
      "epoch:8 step:7800 [D loss: 0.613092, acc: 60.16%] [G loss: 2.180101]\n",
      "epoch:8 step:7801 [D loss: 0.584507, acc: 71.09%] [G loss: 2.309885]\n",
      "epoch:8 step:7802 [D loss: 0.579586, acc: 67.97%] [G loss: 2.191804]\n",
      "epoch:8 step:7803 [D loss: 0.642332, acc: 67.19%] [G loss: 2.291670]\n",
      "epoch:8 step:7804 [D loss: 0.644824, acc: 61.72%] [G loss: 2.007370]\n",
      "epoch:8 step:7805 [D loss: 0.555110, acc: 75.78%] [G loss: 2.283258]\n",
      "epoch:8 step:7806 [D loss: 0.651676, acc: 63.28%] [G loss: 2.205494]\n",
      "epoch:8 step:7807 [D loss: 0.596803, acc: 71.09%] [G loss: 2.062416]\n",
      "epoch:8 step:7808 [D loss: 0.573745, acc: 70.31%] [G loss: 3.049884]\n",
      "epoch:8 step:7809 [D loss: 0.687124, acc: 57.81%] [G loss: 2.544754]\n",
      "epoch:8 step:7810 [D loss: 0.502571, acc: 76.56%] [G loss: 2.538254]\n",
      "epoch:8 step:7811 [D loss: 0.578125, acc: 72.66%] [G loss: 2.486073]\n",
      "epoch:8 step:7812 [D loss: 0.730922, acc: 57.03%] [G loss: 1.998838]\n",
      "epoch:8 step:7813 [D loss: 0.565888, acc: 67.97%] [G loss: 2.148643]\n",
      "epoch:8 step:7814 [D loss: 0.594525, acc: 67.97%] [G loss: 2.484043]\n",
      "epoch:8 step:7815 [D loss: 0.572001, acc: 67.97%] [G loss: 2.365221]\n",
      "epoch:8 step:7816 [D loss: 0.654811, acc: 67.19%] [G loss: 2.204628]\n",
      "epoch:8 step:7817 [D loss: 0.607286, acc: 68.75%] [G loss: 2.638192]\n",
      "epoch:8 step:7818 [D loss: 0.647630, acc: 63.28%] [G loss: 2.284829]\n",
      "epoch:8 step:7819 [D loss: 0.678406, acc: 61.72%] [G loss: 2.086385]\n",
      "epoch:8 step:7820 [D loss: 0.584463, acc: 71.09%] [G loss: 2.158031]\n",
      "epoch:8 step:7821 [D loss: 0.589847, acc: 69.53%] [G loss: 2.268637]\n",
      "epoch:8 step:7822 [D loss: 0.678123, acc: 64.84%] [G loss: 2.077288]\n",
      "epoch:8 step:7823 [D loss: 0.597098, acc: 67.19%] [G loss: 2.119528]\n",
      "epoch:8 step:7824 [D loss: 0.664540, acc: 65.62%] [G loss: 2.039736]\n",
      "epoch:8 step:7825 [D loss: 0.639532, acc: 64.84%] [G loss: 2.187598]\n",
      "epoch:8 step:7826 [D loss: 0.642187, acc: 58.59%] [G loss: 2.256129]\n",
      "epoch:8 step:7827 [D loss: 0.534463, acc: 75.00%] [G loss: 2.265921]\n",
      "epoch:8 step:7828 [D loss: 0.575119, acc: 70.31%] [G loss: 2.402287]\n",
      "epoch:8 step:7829 [D loss: 0.562696, acc: 71.88%] [G loss: 2.086195]\n",
      "epoch:8 step:7830 [D loss: 0.615340, acc: 64.84%] [G loss: 2.341413]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7831 [D loss: 0.601824, acc: 68.75%] [G loss: 2.514011]\n",
      "epoch:8 step:7832 [D loss: 0.665004, acc: 60.94%] [G loss: 2.281433]\n",
      "epoch:8 step:7833 [D loss: 0.607666, acc: 67.97%] [G loss: 2.267597]\n",
      "epoch:8 step:7834 [D loss: 0.555990, acc: 75.78%] [G loss: 2.106979]\n",
      "epoch:8 step:7835 [D loss: 0.603740, acc: 64.06%] [G loss: 2.258534]\n",
      "epoch:8 step:7836 [D loss: 0.537853, acc: 75.00%] [G loss: 2.303016]\n",
      "epoch:8 step:7837 [D loss: 0.739339, acc: 53.91%] [G loss: 2.188721]\n",
      "epoch:8 step:7838 [D loss: 0.659063, acc: 63.28%] [G loss: 1.938473]\n",
      "epoch:8 step:7839 [D loss: 0.649010, acc: 64.06%] [G loss: 2.199785]\n",
      "epoch:8 step:7840 [D loss: 0.606428, acc: 64.06%] [G loss: 2.298946]\n",
      "epoch:8 step:7841 [D loss: 0.668316, acc: 63.28%] [G loss: 2.496822]\n",
      "epoch:8 step:7842 [D loss: 0.595251, acc: 67.19%] [G loss: 2.366316]\n",
      "epoch:8 step:7843 [D loss: 0.539019, acc: 77.34%] [G loss: 2.494749]\n",
      "epoch:8 step:7844 [D loss: 0.682459, acc: 60.94%] [G loss: 2.056812]\n",
      "epoch:8 step:7845 [D loss: 0.648915, acc: 65.62%] [G loss: 2.038166]\n",
      "epoch:8 step:7846 [D loss: 0.620543, acc: 64.06%] [G loss: 2.287863]\n",
      "epoch:8 step:7847 [D loss: 0.629624, acc: 67.97%] [G loss: 2.233563]\n",
      "epoch:8 step:7848 [D loss: 0.579840, acc: 64.84%] [G loss: 2.121325]\n",
      "epoch:8 step:7849 [D loss: 0.623915, acc: 64.06%] [G loss: 2.347260]\n",
      "epoch:8 step:7850 [D loss: 0.588964, acc: 71.09%] [G loss: 2.427978]\n",
      "epoch:8 step:7851 [D loss: 0.629193, acc: 67.19%] [G loss: 1.984836]\n",
      "epoch:8 step:7852 [D loss: 0.686704, acc: 64.84%] [G loss: 2.070641]\n",
      "epoch:8 step:7853 [D loss: 0.583055, acc: 65.62%] [G loss: 2.507060]\n",
      "epoch:8 step:7854 [D loss: 0.568809, acc: 67.97%] [G loss: 2.470098]\n",
      "epoch:8 step:7855 [D loss: 0.580148, acc: 71.88%] [G loss: 2.334085]\n",
      "epoch:8 step:7856 [D loss: 0.574498, acc: 68.75%] [G loss: 2.357202]\n",
      "epoch:8 step:7857 [D loss: 0.552179, acc: 75.78%] [G loss: 2.166236]\n",
      "epoch:8 step:7858 [D loss: 0.710989, acc: 52.34%] [G loss: 2.115460]\n",
      "epoch:8 step:7859 [D loss: 0.568997, acc: 68.75%] [G loss: 2.308101]\n",
      "epoch:8 step:7860 [D loss: 0.663019, acc: 57.81%] [G loss: 2.144763]\n",
      "epoch:8 step:7861 [D loss: 0.680170, acc: 60.94%] [G loss: 2.114567]\n",
      "epoch:8 step:7862 [D loss: 0.567979, acc: 67.19%] [G loss: 2.265021]\n",
      "epoch:8 step:7863 [D loss: 0.654616, acc: 67.19%] [G loss: 2.337669]\n",
      "epoch:8 step:7864 [D loss: 0.622995, acc: 63.28%] [G loss: 2.160995]\n",
      "epoch:8 step:7865 [D loss: 0.674746, acc: 59.38%] [G loss: 2.193735]\n",
      "epoch:8 step:7866 [D loss: 0.593880, acc: 67.97%] [G loss: 2.386717]\n",
      "epoch:8 step:7867 [D loss: 0.557394, acc: 67.97%] [G loss: 2.289778]\n",
      "epoch:8 step:7868 [D loss: 0.631783, acc: 62.50%] [G loss: 2.171545]\n",
      "epoch:8 step:7869 [D loss: 0.703061, acc: 51.56%] [G loss: 2.040798]\n",
      "epoch:8 step:7870 [D loss: 0.574158, acc: 71.09%] [G loss: 2.261312]\n",
      "epoch:8 step:7871 [D loss: 0.681921, acc: 60.16%] [G loss: 2.121292]\n",
      "epoch:8 step:7872 [D loss: 0.747501, acc: 49.22%] [G loss: 1.921066]\n",
      "epoch:8 step:7873 [D loss: 0.653665, acc: 67.97%] [G loss: 1.837309]\n",
      "epoch:8 step:7874 [D loss: 0.650492, acc: 58.59%] [G loss: 2.084743]\n",
      "epoch:8 step:7875 [D loss: 0.605163, acc: 67.19%] [G loss: 2.241254]\n",
      "epoch:8 step:7876 [D loss: 0.626324, acc: 64.84%] [G loss: 2.106340]\n",
      "epoch:8 step:7877 [D loss: 0.605893, acc: 67.19%] [G loss: 2.152699]\n",
      "epoch:8 step:7878 [D loss: 0.678045, acc: 54.69%] [G loss: 2.103609]\n",
      "epoch:8 step:7879 [D loss: 0.708614, acc: 53.12%] [G loss: 2.172629]\n",
      "epoch:8 step:7880 [D loss: 0.657468, acc: 63.28%] [G loss: 2.098120]\n",
      "epoch:8 step:7881 [D loss: 0.668248, acc: 61.72%] [G loss: 2.135720]\n",
      "epoch:8 step:7882 [D loss: 0.615580, acc: 65.62%] [G loss: 2.060645]\n",
      "epoch:8 step:7883 [D loss: 0.664863, acc: 60.94%] [G loss: 1.978273]\n",
      "epoch:8 step:7884 [D loss: 0.624119, acc: 67.19%] [G loss: 2.109826]\n",
      "epoch:8 step:7885 [D loss: 0.618286, acc: 63.28%] [G loss: 2.256175]\n",
      "epoch:8 step:7886 [D loss: 0.645745, acc: 66.41%] [G loss: 2.222335]\n",
      "epoch:8 step:7887 [D loss: 0.592521, acc: 74.22%] [G loss: 2.208436]\n",
      "epoch:8 step:7888 [D loss: 0.600566, acc: 65.62%] [G loss: 2.109007]\n",
      "epoch:8 step:7889 [D loss: 0.589821, acc: 67.19%] [G loss: 2.332858]\n",
      "epoch:8 step:7890 [D loss: 0.603232, acc: 63.28%] [G loss: 2.150196]\n",
      "epoch:8 step:7891 [D loss: 0.583977, acc: 64.84%] [G loss: 2.146771]\n",
      "epoch:8 step:7892 [D loss: 0.685994, acc: 58.59%] [G loss: 2.029377]\n",
      "epoch:8 step:7893 [D loss: 0.619347, acc: 66.41%] [G loss: 2.177241]\n",
      "epoch:8 step:7894 [D loss: 0.610594, acc: 62.50%] [G loss: 2.250082]\n",
      "epoch:8 step:7895 [D loss: 0.627971, acc: 60.94%] [G loss: 2.303328]\n",
      "epoch:8 step:7896 [D loss: 0.678356, acc: 60.94%] [G loss: 2.156478]\n",
      "epoch:8 step:7897 [D loss: 0.605803, acc: 69.53%] [G loss: 2.224888]\n",
      "epoch:8 step:7898 [D loss: 0.612189, acc: 64.84%] [G loss: 2.262493]\n",
      "epoch:8 step:7899 [D loss: 0.584195, acc: 73.44%] [G loss: 2.431879]\n",
      "epoch:8 step:7900 [D loss: 0.597248, acc: 67.19%] [G loss: 2.308518]\n",
      "epoch:8 step:7901 [D loss: 0.563446, acc: 71.88%] [G loss: 2.466483]\n",
      "epoch:8 step:7902 [D loss: 0.572642, acc: 70.31%] [G loss: 2.516237]\n",
      "epoch:8 step:7903 [D loss: 0.640790, acc: 66.41%] [G loss: 2.360121]\n",
      "epoch:8 step:7904 [D loss: 0.653062, acc: 61.72%] [G loss: 2.066720]\n",
      "epoch:8 step:7905 [D loss: 0.664649, acc: 63.28%] [G loss: 2.080105]\n",
      "epoch:8 step:7906 [D loss: 0.600275, acc: 65.62%] [G loss: 2.037712]\n",
      "epoch:8 step:7907 [D loss: 0.583632, acc: 70.31%] [G loss: 2.311285]\n",
      "epoch:8 step:7908 [D loss: 0.568761, acc: 70.31%] [G loss: 2.296818]\n",
      "epoch:8 step:7909 [D loss: 0.625455, acc: 63.28%] [G loss: 2.305238]\n",
      "epoch:8 step:7910 [D loss: 0.644991, acc: 62.50%] [G loss: 2.457358]\n",
      "epoch:8 step:7911 [D loss: 0.635057, acc: 68.75%] [G loss: 2.150419]\n",
      "epoch:8 step:7912 [D loss: 0.595307, acc: 67.19%] [G loss: 2.210601]\n",
      "epoch:8 step:7913 [D loss: 0.606933, acc: 67.97%] [G loss: 2.042046]\n",
      "epoch:8 step:7914 [D loss: 0.715417, acc: 52.34%] [G loss: 2.167449]\n",
      "epoch:8 step:7915 [D loss: 0.675453, acc: 61.72%] [G loss: 2.248635]\n",
      "epoch:8 step:7916 [D loss: 0.644154, acc: 63.28%] [G loss: 1.926100]\n",
      "epoch:8 step:7917 [D loss: 0.607004, acc: 67.97%] [G loss: 1.989089]\n",
      "epoch:8 step:7918 [D loss: 0.673789, acc: 59.38%] [G loss: 2.114280]\n",
      "epoch:8 step:7919 [D loss: 0.600513, acc: 71.09%] [G loss: 2.010669]\n",
      "epoch:8 step:7920 [D loss: 0.677007, acc: 61.72%] [G loss: 2.091706]\n",
      "epoch:8 step:7921 [D loss: 0.669414, acc: 60.16%] [G loss: 2.009214]\n",
      "epoch:8 step:7922 [D loss: 0.603092, acc: 70.31%] [G loss: 2.344453]\n",
      "epoch:8 step:7923 [D loss: 0.619203, acc: 67.19%] [G loss: 2.210466]\n",
      "epoch:8 step:7924 [D loss: 0.572679, acc: 66.41%] [G loss: 2.470470]\n",
      "epoch:8 step:7925 [D loss: 0.548006, acc: 71.09%] [G loss: 2.550699]\n",
      "epoch:8 step:7926 [D loss: 0.557032, acc: 74.22%] [G loss: 2.444064]\n",
      "epoch:8 step:7927 [D loss: 0.606263, acc: 65.62%] [G loss: 2.051609]\n",
      "epoch:8 step:7928 [D loss: 0.658766, acc: 63.28%] [G loss: 2.038188]\n",
      "epoch:8 step:7929 [D loss: 0.620925, acc: 59.38%] [G loss: 2.117177]\n",
      "epoch:8 step:7930 [D loss: 0.594938, acc: 69.53%] [G loss: 2.354759]\n",
      "epoch:8 step:7931 [D loss: 0.609985, acc: 66.41%] [G loss: 2.117302]\n",
      "epoch:8 step:7932 [D loss: 0.686615, acc: 60.94%] [G loss: 2.250185]\n",
      "epoch:8 step:7933 [D loss: 0.722587, acc: 55.47%] [G loss: 1.863556]\n",
      "epoch:8 step:7934 [D loss: 0.655271, acc: 61.72%] [G loss: 1.867348]\n",
      "epoch:8 step:7935 [D loss: 0.612279, acc: 66.41%] [G loss: 1.948842]\n",
      "epoch:8 step:7936 [D loss: 0.660777, acc: 63.28%] [G loss: 2.069512]\n",
      "epoch:8 step:7937 [D loss: 0.669142, acc: 53.91%] [G loss: 2.077588]\n",
      "epoch:8 step:7938 [D loss: 0.589660, acc: 70.31%] [G loss: 2.129987]\n",
      "epoch:8 step:7939 [D loss: 0.560035, acc: 75.00%] [G loss: 2.296880]\n",
      "epoch:8 step:7940 [D loss: 0.668632, acc: 62.50%] [G loss: 2.017312]\n",
      "epoch:8 step:7941 [D loss: 0.688052, acc: 54.69%] [G loss: 2.118403]\n",
      "epoch:8 step:7942 [D loss: 0.630237, acc: 62.50%] [G loss: 1.951083]\n",
      "epoch:8 step:7943 [D loss: 0.597604, acc: 71.09%] [G loss: 2.068824]\n",
      "epoch:8 step:7944 [D loss: 0.587800, acc: 67.97%] [G loss: 2.016256]\n",
      "epoch:8 step:7945 [D loss: 0.567545, acc: 67.97%] [G loss: 2.255012]\n",
      "epoch:8 step:7946 [D loss: 0.558737, acc: 73.44%] [G loss: 2.347171]\n",
      "epoch:8 step:7947 [D loss: 0.657530, acc: 60.16%] [G loss: 2.260861]\n",
      "epoch:8 step:7948 [D loss: 0.613345, acc: 70.31%] [G loss: 2.228734]\n",
      "epoch:8 step:7949 [D loss: 0.595956, acc: 71.09%] [G loss: 2.312730]\n",
      "epoch:8 step:7950 [D loss: 0.657179, acc: 67.19%] [G loss: 2.120924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:7951 [D loss: 0.648959, acc: 62.50%] [G loss: 2.336343]\n",
      "epoch:8 step:7952 [D loss: 0.590898, acc: 67.97%] [G loss: 2.244541]\n",
      "epoch:8 step:7953 [D loss: 0.535991, acc: 78.91%] [G loss: 2.211959]\n",
      "epoch:8 step:7954 [D loss: 0.628563, acc: 64.84%] [G loss: 2.057739]\n",
      "epoch:8 step:7955 [D loss: 0.698464, acc: 62.50%] [G loss: 2.076242]\n",
      "epoch:8 step:7956 [D loss: 0.637569, acc: 65.62%] [G loss: 2.101017]\n",
      "epoch:8 step:7957 [D loss: 0.625972, acc: 67.19%] [G loss: 2.080505]\n",
      "epoch:8 step:7958 [D loss: 0.655974, acc: 68.75%] [G loss: 2.196025]\n",
      "epoch:8 step:7959 [D loss: 0.689329, acc: 58.59%] [G loss: 2.056082]\n",
      "epoch:8 step:7960 [D loss: 0.598489, acc: 67.19%] [G loss: 2.162480]\n",
      "epoch:8 step:7961 [D loss: 0.637353, acc: 63.28%] [G loss: 1.963095]\n",
      "epoch:8 step:7962 [D loss: 0.652143, acc: 62.50%] [G loss: 2.345676]\n",
      "epoch:8 step:7963 [D loss: 0.581076, acc: 71.88%] [G loss: 2.140668]\n",
      "epoch:8 step:7964 [D loss: 0.639875, acc: 64.06%] [G loss: 2.294830]\n",
      "epoch:8 step:7965 [D loss: 0.567647, acc: 74.22%] [G loss: 2.299377]\n",
      "epoch:8 step:7966 [D loss: 0.545157, acc: 71.09%] [G loss: 2.451195]\n",
      "epoch:8 step:7967 [D loss: 0.591923, acc: 66.41%] [G loss: 2.697364]\n",
      "epoch:8 step:7968 [D loss: 0.580907, acc: 69.53%] [G loss: 2.491417]\n",
      "epoch:8 step:7969 [D loss: 0.738742, acc: 59.38%] [G loss: 2.099249]\n",
      "epoch:8 step:7970 [D loss: 0.601466, acc: 67.19%] [G loss: 2.418780]\n",
      "epoch:8 step:7971 [D loss: 0.646893, acc: 62.50%] [G loss: 2.218944]\n",
      "epoch:8 step:7972 [D loss: 0.591152, acc: 67.97%] [G loss: 2.208450]\n",
      "epoch:8 step:7973 [D loss: 0.704174, acc: 60.16%] [G loss: 1.989982]\n",
      "epoch:8 step:7974 [D loss: 0.591921, acc: 69.53%] [G loss: 2.020279]\n",
      "epoch:8 step:7975 [D loss: 0.570468, acc: 69.53%] [G loss: 2.163216]\n",
      "epoch:8 step:7976 [D loss: 0.617906, acc: 67.19%] [G loss: 2.210917]\n",
      "epoch:8 step:7977 [D loss: 0.558677, acc: 71.88%] [G loss: 2.393930]\n",
      "epoch:8 step:7978 [D loss: 0.684139, acc: 60.94%] [G loss: 2.047367]\n",
      "epoch:8 step:7979 [D loss: 0.656650, acc: 58.59%] [G loss: 2.229691]\n",
      "epoch:8 step:7980 [D loss: 0.596482, acc: 67.97%] [G loss: 2.285486]\n",
      "epoch:8 step:7981 [D loss: 0.672109, acc: 57.03%] [G loss: 2.170396]\n",
      "epoch:8 step:7982 [D loss: 0.643021, acc: 64.84%] [G loss: 2.265747]\n",
      "epoch:8 step:7983 [D loss: 0.561968, acc: 67.19%] [G loss: 2.230141]\n",
      "epoch:8 step:7984 [D loss: 0.561016, acc: 64.84%] [G loss: 2.328776]\n",
      "epoch:8 step:7985 [D loss: 0.643538, acc: 65.62%] [G loss: 2.293417]\n",
      "epoch:8 step:7986 [D loss: 0.610869, acc: 64.84%] [G loss: 2.178949]\n",
      "epoch:8 step:7987 [D loss: 0.655631, acc: 60.16%] [G loss: 2.166158]\n",
      "epoch:8 step:7988 [D loss: 0.577946, acc: 65.62%] [G loss: 2.121724]\n",
      "epoch:8 step:7989 [D loss: 0.659411, acc: 62.50%] [G loss: 1.976088]\n",
      "epoch:8 step:7990 [D loss: 0.684417, acc: 56.25%] [G loss: 2.339813]\n",
      "epoch:8 step:7991 [D loss: 0.566084, acc: 73.44%] [G loss: 2.217138]\n",
      "epoch:8 step:7992 [D loss: 0.609154, acc: 64.84%] [G loss: 2.083526]\n",
      "epoch:8 step:7993 [D loss: 0.571077, acc: 67.97%] [G loss: 2.487022]\n",
      "epoch:8 step:7994 [D loss: 0.617167, acc: 64.06%] [G loss: 2.617083]\n",
      "epoch:8 step:7995 [D loss: 0.549254, acc: 72.66%] [G loss: 2.332310]\n",
      "epoch:8 step:7996 [D loss: 0.665731, acc: 57.81%] [G loss: 2.085856]\n",
      "epoch:8 step:7997 [D loss: 0.747229, acc: 52.34%] [G loss: 1.961312]\n",
      "epoch:8 step:7998 [D loss: 0.760707, acc: 56.25%] [G loss: 1.958542]\n",
      "epoch:8 step:7999 [D loss: 0.657671, acc: 57.03%] [G loss: 2.081783]\n",
      "epoch:8 step:8000 [D loss: 0.658795, acc: 63.28%] [G loss: 2.178872]\n",
      "epoch:8 step:8001 [D loss: 0.590725, acc: 69.53%] [G loss: 2.079464]\n",
      "epoch:8 step:8002 [D loss: 0.698777, acc: 56.25%] [G loss: 1.896820]\n",
      "epoch:8 step:8003 [D loss: 0.606369, acc: 64.84%] [G loss: 2.085595]\n",
      "epoch:8 step:8004 [D loss: 0.600767, acc: 68.75%] [G loss: 2.137632]\n",
      "epoch:8 step:8005 [D loss: 0.660478, acc: 60.16%] [G loss: 2.191570]\n",
      "epoch:8 step:8006 [D loss: 0.634598, acc: 64.84%] [G loss: 2.004876]\n",
      "epoch:8 step:8007 [D loss: 0.616926, acc: 64.06%] [G loss: 1.941580]\n",
      "epoch:8 step:8008 [D loss: 0.670233, acc: 65.62%] [G loss: 2.008194]\n",
      "epoch:8 step:8009 [D loss: 0.604388, acc: 66.41%] [G loss: 2.227149]\n",
      "epoch:8 step:8010 [D loss: 0.640321, acc: 65.62%] [G loss: 2.138275]\n",
      "epoch:8 step:8011 [D loss: 0.627170, acc: 64.06%] [G loss: 2.224988]\n",
      "epoch:8 step:8012 [D loss: 0.600724, acc: 68.75%] [G loss: 2.346693]\n",
      "epoch:8 step:8013 [D loss: 0.657537, acc: 61.72%] [G loss: 2.210492]\n",
      "epoch:8 step:8014 [D loss: 0.558607, acc: 73.44%] [G loss: 2.555558]\n",
      "epoch:8 step:8015 [D loss: 0.570722, acc: 71.88%] [G loss: 2.250531]\n",
      "epoch:8 step:8016 [D loss: 0.596211, acc: 66.41%] [G loss: 2.246562]\n",
      "epoch:8 step:8017 [D loss: 0.609557, acc: 67.97%] [G loss: 2.152241]\n",
      "epoch:8 step:8018 [D loss: 0.608759, acc: 66.41%] [G loss: 2.345442]\n",
      "epoch:8 step:8019 [D loss: 0.571743, acc: 67.19%] [G loss: 2.346980]\n",
      "epoch:8 step:8020 [D loss: 0.635175, acc: 66.41%] [G loss: 2.253574]\n",
      "epoch:8 step:8021 [D loss: 0.640694, acc: 66.41%] [G loss: 2.058819]\n",
      "epoch:8 step:8022 [D loss: 0.589594, acc: 71.88%] [G loss: 2.074821]\n",
      "epoch:8 step:8023 [D loss: 0.594045, acc: 69.53%] [G loss: 2.097547]\n",
      "epoch:8 step:8024 [D loss: 0.645628, acc: 63.28%] [G loss: 2.170624]\n",
      "epoch:8 step:8025 [D loss: 0.668487, acc: 57.03%] [G loss: 1.973971]\n",
      "epoch:8 step:8026 [D loss: 0.663379, acc: 61.72%] [G loss: 2.048487]\n",
      "epoch:8 step:8027 [D loss: 0.645837, acc: 61.72%] [G loss: 1.929425]\n",
      "epoch:8 step:8028 [D loss: 0.596460, acc: 68.75%] [G loss: 2.218890]\n",
      "epoch:8 step:8029 [D loss: 0.670628, acc: 60.94%] [G loss: 1.954430]\n",
      "epoch:8 step:8030 [D loss: 0.591416, acc: 67.19%] [G loss: 2.368174]\n",
      "epoch:8 step:8031 [D loss: 0.637561, acc: 61.72%] [G loss: 2.216726]\n",
      "epoch:8 step:8032 [D loss: 0.655594, acc: 62.50%] [G loss: 2.260141]\n",
      "epoch:8 step:8033 [D loss: 0.627624, acc: 61.72%] [G loss: 2.282984]\n",
      "epoch:8 step:8034 [D loss: 0.700328, acc: 60.94%] [G loss: 2.016529]\n",
      "epoch:8 step:8035 [D loss: 0.653063, acc: 61.72%] [G loss: 1.998549]\n",
      "epoch:8 step:8036 [D loss: 0.650755, acc: 63.28%] [G loss: 2.041861]\n",
      "epoch:8 step:8037 [D loss: 0.595118, acc: 74.22%] [G loss: 2.235519]\n",
      "epoch:8 step:8038 [D loss: 0.637722, acc: 58.59%] [G loss: 1.957165]\n",
      "epoch:8 step:8039 [D loss: 0.624911, acc: 63.28%] [G loss: 2.036240]\n",
      "epoch:8 step:8040 [D loss: 0.611685, acc: 69.53%] [G loss: 2.061593]\n",
      "epoch:8 step:8041 [D loss: 0.586741, acc: 67.19%] [G loss: 2.125015]\n",
      "epoch:8 step:8042 [D loss: 0.586231, acc: 72.66%] [G loss: 2.122982]\n",
      "epoch:8 step:8043 [D loss: 0.632201, acc: 63.28%] [G loss: 2.193742]\n",
      "epoch:8 step:8044 [D loss: 0.659397, acc: 61.72%] [G loss: 2.043007]\n",
      "epoch:8 step:8045 [D loss: 0.645875, acc: 67.97%] [G loss: 2.134800]\n",
      "epoch:8 step:8046 [D loss: 0.585345, acc: 68.75%] [G loss: 2.206544]\n",
      "epoch:8 step:8047 [D loss: 0.572538, acc: 71.88%] [G loss: 2.394803]\n",
      "epoch:8 step:8048 [D loss: 0.561265, acc: 70.31%] [G loss: 2.406314]\n",
      "epoch:8 step:8049 [D loss: 0.704354, acc: 58.59%] [G loss: 2.041338]\n",
      "epoch:8 step:8050 [D loss: 0.608590, acc: 64.84%] [G loss: 2.405417]\n",
      "epoch:8 step:8051 [D loss: 0.585023, acc: 71.88%] [G loss: 2.179143]\n",
      "epoch:8 step:8052 [D loss: 0.608793, acc: 64.84%] [G loss: 2.362138]\n",
      "epoch:8 step:8053 [D loss: 0.610492, acc: 64.06%] [G loss: 2.250507]\n",
      "epoch:8 step:8054 [D loss: 0.578264, acc: 68.75%] [G loss: 2.476850]\n",
      "epoch:8 step:8055 [D loss: 0.706571, acc: 61.72%] [G loss: 1.968667]\n",
      "epoch:8 step:8056 [D loss: 0.694702, acc: 57.81%] [G loss: 2.109599]\n",
      "epoch:8 step:8057 [D loss: 0.624730, acc: 63.28%] [G loss: 2.286383]\n",
      "epoch:8 step:8058 [D loss: 0.670504, acc: 60.94%] [G loss: 2.111605]\n",
      "epoch:8 step:8059 [D loss: 0.629786, acc: 64.84%] [G loss: 2.034527]\n",
      "epoch:8 step:8060 [D loss: 0.548035, acc: 72.66%] [G loss: 2.389283]\n",
      "epoch:8 step:8061 [D loss: 0.610050, acc: 67.19%] [G loss: 2.128493]\n",
      "epoch:8 step:8062 [D loss: 0.681655, acc: 60.94%] [G loss: 1.936377]\n",
      "epoch:8 step:8063 [D loss: 0.655547, acc: 66.41%] [G loss: 2.070383]\n",
      "epoch:8 step:8064 [D loss: 0.591419, acc: 67.19%] [G loss: 2.300327]\n",
      "epoch:8 step:8065 [D loss: 0.658632, acc: 65.62%] [G loss: 2.042413]\n",
      "epoch:8 step:8066 [D loss: 0.554837, acc: 71.88%] [G loss: 2.194481]\n",
      "epoch:8 step:8067 [D loss: 0.593851, acc: 66.41%] [G loss: 2.272242]\n",
      "epoch:8 step:8068 [D loss: 0.657362, acc: 62.50%] [G loss: 2.157100]\n",
      "epoch:8 step:8069 [D loss: 0.669154, acc: 60.94%] [G loss: 2.197355]\n",
      "epoch:8 step:8070 [D loss: 0.651776, acc: 63.28%] [G loss: 2.316292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8071 [D loss: 0.603276, acc: 65.62%] [G loss: 2.175004]\n",
      "epoch:8 step:8072 [D loss: 0.727027, acc: 52.34%] [G loss: 1.968055]\n",
      "epoch:8 step:8073 [D loss: 0.616038, acc: 64.84%] [G loss: 2.129508]\n",
      "epoch:8 step:8074 [D loss: 0.619053, acc: 54.69%] [G loss: 2.344877]\n",
      "epoch:8 step:8075 [D loss: 0.642287, acc: 59.38%] [G loss: 2.107878]\n",
      "epoch:8 step:8076 [D loss: 0.601965, acc: 68.75%] [G loss: 2.127269]\n",
      "epoch:8 step:8077 [D loss: 0.645141, acc: 64.06%] [G loss: 2.236169]\n",
      "epoch:8 step:8078 [D loss: 0.621002, acc: 61.72%] [G loss: 2.173860]\n",
      "epoch:8 step:8079 [D loss: 0.644636, acc: 64.06%] [G loss: 2.114018]\n",
      "epoch:8 step:8080 [D loss: 0.602094, acc: 67.97%] [G loss: 2.134631]\n",
      "epoch:8 step:8081 [D loss: 0.657357, acc: 60.94%] [G loss: 2.089210]\n",
      "epoch:8 step:8082 [D loss: 0.652287, acc: 63.28%] [G loss: 2.242934]\n",
      "epoch:8 step:8083 [D loss: 0.583751, acc: 69.53%] [G loss: 2.146514]\n",
      "epoch:8 step:8084 [D loss: 0.596257, acc: 68.75%] [G loss: 2.254331]\n",
      "epoch:8 step:8085 [D loss: 0.584513, acc: 70.31%] [G loss: 2.544404]\n",
      "epoch:8 step:8086 [D loss: 0.659408, acc: 64.06%] [G loss: 1.967777]\n",
      "epoch:8 step:8087 [D loss: 0.674443, acc: 63.28%] [G loss: 2.117577]\n",
      "epoch:8 step:8088 [D loss: 0.647927, acc: 62.50%] [G loss: 2.250310]\n",
      "epoch:8 step:8089 [D loss: 0.629422, acc: 64.84%] [G loss: 1.989554]\n",
      "epoch:8 step:8090 [D loss: 0.630621, acc: 61.72%] [G loss: 2.159365]\n",
      "epoch:8 step:8091 [D loss: 0.580788, acc: 71.88%] [G loss: 2.222996]\n",
      "epoch:8 step:8092 [D loss: 0.657452, acc: 61.72%] [G loss: 2.074685]\n",
      "epoch:8 step:8093 [D loss: 0.633944, acc: 64.06%] [G loss: 2.210772]\n",
      "epoch:8 step:8094 [D loss: 0.651399, acc: 60.16%] [G loss: 2.177162]\n",
      "epoch:8 step:8095 [D loss: 0.611365, acc: 65.62%] [G loss: 2.127419]\n",
      "epoch:8 step:8096 [D loss: 0.672553, acc: 60.16%] [G loss: 2.095672]\n",
      "epoch:8 step:8097 [D loss: 0.669093, acc: 60.16%] [G loss: 2.052075]\n",
      "epoch:8 step:8098 [D loss: 0.620448, acc: 64.06%] [G loss: 2.129327]\n",
      "epoch:8 step:8099 [D loss: 0.585397, acc: 69.53%] [G loss: 2.252368]\n",
      "epoch:8 step:8100 [D loss: 0.627196, acc: 72.66%] [G loss: 2.176462]\n",
      "epoch:8 step:8101 [D loss: 0.590007, acc: 67.97%] [G loss: 2.266764]\n",
      "epoch:8 step:8102 [D loss: 0.662748, acc: 60.16%] [G loss: 2.196671]\n",
      "epoch:8 step:8103 [D loss: 0.614412, acc: 68.75%] [G loss: 2.143699]\n",
      "epoch:8 step:8104 [D loss: 0.573374, acc: 67.19%] [G loss: 2.252375]\n",
      "epoch:8 step:8105 [D loss: 0.596155, acc: 70.31%] [G loss: 2.240590]\n",
      "epoch:8 step:8106 [D loss: 0.627658, acc: 68.75%] [G loss: 2.134872]\n",
      "epoch:8 step:8107 [D loss: 0.669352, acc: 60.94%] [G loss: 1.994785]\n",
      "epoch:8 step:8108 [D loss: 0.653517, acc: 59.38%] [G loss: 2.027068]\n",
      "epoch:8 step:8109 [D loss: 0.629033, acc: 64.84%] [G loss: 2.288226]\n",
      "epoch:8 step:8110 [D loss: 0.648847, acc: 54.69%] [G loss: 2.032691]\n",
      "epoch:8 step:8111 [D loss: 0.654681, acc: 65.62%] [G loss: 2.052710]\n",
      "epoch:8 step:8112 [D loss: 0.654301, acc: 65.62%] [G loss: 2.181332]\n",
      "epoch:8 step:8113 [D loss: 0.621313, acc: 64.06%] [G loss: 2.112275]\n",
      "epoch:8 step:8114 [D loss: 0.636130, acc: 69.53%] [G loss: 2.115158]\n",
      "epoch:8 step:8115 [D loss: 0.717864, acc: 54.69%] [G loss: 1.972625]\n",
      "epoch:8 step:8116 [D loss: 0.585081, acc: 69.53%] [G loss: 2.198144]\n",
      "epoch:8 step:8117 [D loss: 0.640015, acc: 64.06%] [G loss: 1.927974]\n",
      "epoch:8 step:8118 [D loss: 0.611802, acc: 66.41%] [G loss: 2.085272]\n",
      "epoch:8 step:8119 [D loss: 0.573476, acc: 69.53%] [G loss: 2.217683]\n",
      "epoch:8 step:8120 [D loss: 0.607285, acc: 64.84%] [G loss: 2.118540]\n",
      "epoch:8 step:8121 [D loss: 0.709640, acc: 58.59%] [G loss: 1.999436]\n",
      "epoch:8 step:8122 [D loss: 0.617458, acc: 65.62%] [G loss: 2.286473]\n",
      "epoch:8 step:8123 [D loss: 0.625297, acc: 65.62%] [G loss: 2.181263]\n",
      "epoch:8 step:8124 [D loss: 0.655915, acc: 59.38%] [G loss: 2.024850]\n",
      "epoch:8 step:8125 [D loss: 0.632490, acc: 60.94%] [G loss: 1.966162]\n",
      "epoch:8 step:8126 [D loss: 0.577089, acc: 71.88%] [G loss: 2.380450]\n",
      "epoch:8 step:8127 [D loss: 0.620615, acc: 69.53%] [G loss: 2.193089]\n",
      "epoch:8 step:8128 [D loss: 0.540494, acc: 69.53%] [G loss: 2.264245]\n",
      "epoch:8 step:8129 [D loss: 0.708120, acc: 59.38%] [G loss: 2.219756]\n",
      "epoch:8 step:8130 [D loss: 0.613225, acc: 74.22%] [G loss: 2.261970]\n",
      "epoch:8 step:8131 [D loss: 0.613170, acc: 64.84%] [G loss: 2.059470]\n",
      "epoch:8 step:8132 [D loss: 0.603905, acc: 64.06%] [G loss: 2.199866]\n",
      "epoch:8 step:8133 [D loss: 0.561484, acc: 71.88%] [G loss: 2.368062]\n",
      "epoch:8 step:8134 [D loss: 0.589249, acc: 69.53%] [G loss: 2.058672]\n",
      "epoch:8 step:8135 [D loss: 0.620870, acc: 64.06%] [G loss: 2.284452]\n",
      "epoch:8 step:8136 [D loss: 0.591581, acc: 69.53%] [G loss: 2.326808]\n",
      "epoch:8 step:8137 [D loss: 0.549056, acc: 75.00%] [G loss: 2.392421]\n",
      "epoch:8 step:8138 [D loss: 0.631464, acc: 60.16%] [G loss: 2.516145]\n",
      "epoch:8 step:8139 [D loss: 0.613772, acc: 64.06%] [G loss: 2.060632]\n",
      "epoch:8 step:8140 [D loss: 0.603011, acc: 65.62%] [G loss: 2.394259]\n",
      "epoch:8 step:8141 [D loss: 0.621879, acc: 67.97%] [G loss: 2.317361]\n",
      "epoch:8 step:8142 [D loss: 0.639416, acc: 64.84%] [G loss: 2.446960]\n",
      "epoch:8 step:8143 [D loss: 0.569884, acc: 70.31%] [G loss: 2.450493]\n",
      "epoch:8 step:8144 [D loss: 0.578070, acc: 71.88%] [G loss: 2.536699]\n",
      "epoch:8 step:8145 [D loss: 0.533079, acc: 75.00%] [G loss: 2.595596]\n",
      "epoch:8 step:8146 [D loss: 0.590629, acc: 66.41%] [G loss: 2.514537]\n",
      "epoch:8 step:8147 [D loss: 0.608298, acc: 68.75%] [G loss: 2.145968]\n",
      "epoch:8 step:8148 [D loss: 0.717782, acc: 58.59%] [G loss: 2.114221]\n",
      "epoch:8 step:8149 [D loss: 0.619878, acc: 67.19%] [G loss: 2.130397]\n",
      "epoch:8 step:8150 [D loss: 0.608679, acc: 63.28%] [G loss: 2.291494]\n",
      "epoch:8 step:8151 [D loss: 0.648289, acc: 63.28%] [G loss: 2.217200]\n",
      "epoch:8 step:8152 [D loss: 0.620122, acc: 67.19%] [G loss: 2.310695]\n",
      "epoch:8 step:8153 [D loss: 0.614753, acc: 67.97%] [G loss: 2.057137]\n",
      "epoch:8 step:8154 [D loss: 0.709670, acc: 57.03%] [G loss: 1.967668]\n",
      "epoch:8 step:8155 [D loss: 0.601628, acc: 72.66%] [G loss: 2.048865]\n",
      "epoch:8 step:8156 [D loss: 0.565250, acc: 69.53%] [G loss: 2.191016]\n",
      "epoch:8 step:8157 [D loss: 0.630335, acc: 64.06%] [G loss: 2.164402]\n",
      "epoch:8 step:8158 [D loss: 0.606106, acc: 64.84%] [G loss: 2.331659]\n",
      "epoch:8 step:8159 [D loss: 0.632587, acc: 65.62%] [G loss: 2.392543]\n",
      "epoch:8 step:8160 [D loss: 0.662701, acc: 64.06%] [G loss: 2.043256]\n",
      "epoch:8 step:8161 [D loss: 0.666383, acc: 61.72%] [G loss: 2.108727]\n",
      "epoch:8 step:8162 [D loss: 0.659193, acc: 64.84%] [G loss: 2.048517]\n",
      "epoch:8 step:8163 [D loss: 0.618235, acc: 66.41%] [G loss: 2.094839]\n",
      "epoch:8 step:8164 [D loss: 0.585662, acc: 69.53%] [G loss: 2.171289]\n",
      "epoch:8 step:8165 [D loss: 0.621361, acc: 62.50%] [G loss: 2.185968]\n",
      "epoch:8 step:8166 [D loss: 0.627239, acc: 64.84%] [G loss: 2.175409]\n",
      "epoch:8 step:8167 [D loss: 0.607071, acc: 73.44%] [G loss: 2.133736]\n",
      "epoch:8 step:8168 [D loss: 0.652330, acc: 61.72%] [G loss: 2.210888]\n",
      "epoch:8 step:8169 [D loss: 0.627154, acc: 63.28%] [G loss: 2.033966]\n",
      "epoch:8 step:8170 [D loss: 0.613965, acc: 68.75%] [G loss: 2.054856]\n",
      "epoch:8 step:8171 [D loss: 0.551832, acc: 70.31%] [G loss: 1.994584]\n",
      "epoch:8 step:8172 [D loss: 0.732559, acc: 53.12%] [G loss: 2.111311]\n",
      "epoch:8 step:8173 [D loss: 0.572494, acc: 68.75%] [G loss: 2.279545]\n",
      "epoch:8 step:8174 [D loss: 0.585354, acc: 71.88%] [G loss: 2.243425]\n",
      "epoch:8 step:8175 [D loss: 0.610831, acc: 69.53%] [G loss: 2.126816]\n",
      "epoch:8 step:8176 [D loss: 0.598806, acc: 68.75%] [G loss: 2.242070]\n",
      "epoch:8 step:8177 [D loss: 0.525047, acc: 75.00%] [G loss: 2.368330]\n",
      "epoch:8 step:8178 [D loss: 0.630654, acc: 69.53%] [G loss: 2.354114]\n",
      "epoch:8 step:8179 [D loss: 0.594721, acc: 67.97%] [G loss: 2.285804]\n",
      "epoch:8 step:8180 [D loss: 0.644629, acc: 62.50%] [G loss: 2.065201]\n",
      "epoch:8 step:8181 [D loss: 0.612151, acc: 66.41%] [G loss: 2.095477]\n",
      "epoch:8 step:8182 [D loss: 0.574088, acc: 66.41%] [G loss: 2.250317]\n",
      "epoch:8 step:8183 [D loss: 0.594525, acc: 71.88%] [G loss: 2.201744]\n",
      "epoch:8 step:8184 [D loss: 0.599560, acc: 64.06%] [G loss: 2.156604]\n",
      "epoch:8 step:8185 [D loss: 0.619123, acc: 63.28%] [G loss: 2.064634]\n",
      "epoch:8 step:8186 [D loss: 0.614813, acc: 64.06%] [G loss: 2.242877]\n",
      "epoch:8 step:8187 [D loss: 0.586442, acc: 67.97%] [G loss: 2.512247]\n",
      "epoch:8 step:8188 [D loss: 0.691784, acc: 63.28%] [G loss: 2.320941]\n",
      "epoch:8 step:8189 [D loss: 0.647112, acc: 59.38%] [G loss: 2.351045]\n",
      "epoch:8 step:8190 [D loss: 0.575350, acc: 73.44%] [G loss: 2.415687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8191 [D loss: 0.581899, acc: 69.53%] [G loss: 2.227092]\n",
      "epoch:8 step:8192 [D loss: 0.694664, acc: 57.03%] [G loss: 2.017550]\n",
      "epoch:8 step:8193 [D loss: 0.691114, acc: 57.03%] [G loss: 2.169697]\n",
      "epoch:8 step:8194 [D loss: 0.597469, acc: 67.19%] [G loss: 2.106483]\n",
      "epoch:8 step:8195 [D loss: 0.592541, acc: 70.31%] [G loss: 2.161780]\n",
      "epoch:8 step:8196 [D loss: 0.646919, acc: 64.06%] [G loss: 2.147465]\n",
      "epoch:8 step:8197 [D loss: 0.626159, acc: 62.50%] [G loss: 2.316333]\n",
      "epoch:8 step:8198 [D loss: 0.639461, acc: 60.94%] [G loss: 2.143357]\n",
      "epoch:8 step:8199 [D loss: 0.623016, acc: 64.06%] [G loss: 2.073337]\n",
      "epoch:8 step:8200 [D loss: 0.719223, acc: 58.59%] [G loss: 1.805503]\n",
      "epoch:8 step:8201 [D loss: 0.577372, acc: 71.88%] [G loss: 2.019895]\n",
      "epoch:8 step:8202 [D loss: 0.611443, acc: 65.62%] [G loss: 2.222416]\n",
      "epoch:8 step:8203 [D loss: 0.639046, acc: 65.62%] [G loss: 2.321191]\n",
      "epoch:8 step:8204 [D loss: 0.558298, acc: 69.53%] [G loss: 2.151031]\n",
      "epoch:8 step:8205 [D loss: 0.603263, acc: 71.88%] [G loss: 2.256993]\n",
      "epoch:8 step:8206 [D loss: 0.691404, acc: 57.81%] [G loss: 2.071648]\n",
      "epoch:8 step:8207 [D loss: 0.608771, acc: 65.62%] [G loss: 2.010405]\n",
      "epoch:8 step:8208 [D loss: 0.583412, acc: 71.88%] [G loss: 2.078487]\n",
      "epoch:8 step:8209 [D loss: 0.682479, acc: 54.69%] [G loss: 1.965948]\n",
      "epoch:8 step:8210 [D loss: 0.676500, acc: 64.06%] [G loss: 2.140024]\n",
      "epoch:8 step:8211 [D loss: 0.573899, acc: 72.66%] [G loss: 2.064927]\n",
      "epoch:8 step:8212 [D loss: 0.649164, acc: 64.06%] [G loss: 1.883425]\n",
      "epoch:8 step:8213 [D loss: 0.692592, acc: 58.59%] [G loss: 1.948640]\n",
      "epoch:8 step:8214 [D loss: 0.596357, acc: 67.97%] [G loss: 2.107041]\n",
      "epoch:8 step:8215 [D loss: 0.590693, acc: 69.53%] [G loss: 2.070227]\n",
      "epoch:8 step:8216 [D loss: 0.715137, acc: 52.34%] [G loss: 2.057450]\n",
      "epoch:8 step:8217 [D loss: 0.595634, acc: 67.97%] [G loss: 2.241932]\n",
      "epoch:8 step:8218 [D loss: 0.608330, acc: 70.31%] [G loss: 2.096405]\n",
      "epoch:8 step:8219 [D loss: 0.602872, acc: 68.75%] [G loss: 2.207837]\n",
      "epoch:8 step:8220 [D loss: 0.619699, acc: 67.19%] [G loss: 2.213437]\n",
      "epoch:8 step:8221 [D loss: 0.661115, acc: 63.28%] [G loss: 2.416692]\n",
      "epoch:8 step:8222 [D loss: 0.616441, acc: 67.19%] [G loss: 2.107538]\n",
      "epoch:8 step:8223 [D loss: 0.677302, acc: 64.06%] [G loss: 2.109617]\n",
      "epoch:8 step:8224 [D loss: 0.682988, acc: 60.16%] [G loss: 2.126719]\n",
      "epoch:8 step:8225 [D loss: 0.634496, acc: 63.28%] [G loss: 2.079841]\n",
      "epoch:8 step:8226 [D loss: 0.597963, acc: 67.97%] [G loss: 2.172559]\n",
      "epoch:8 step:8227 [D loss: 0.629448, acc: 66.41%] [G loss: 2.140823]\n",
      "epoch:8 step:8228 [D loss: 0.586152, acc: 67.97%] [G loss: 2.325361]\n",
      "epoch:8 step:8229 [D loss: 0.641781, acc: 63.28%] [G loss: 2.189521]\n",
      "epoch:8 step:8230 [D loss: 0.645227, acc: 64.84%] [G loss: 2.023781]\n",
      "epoch:8 step:8231 [D loss: 0.619353, acc: 67.19%] [G loss: 2.195920]\n",
      "epoch:8 step:8232 [D loss: 0.580675, acc: 75.78%] [G loss: 2.217742]\n",
      "epoch:8 step:8233 [D loss: 0.637158, acc: 67.19%] [G loss: 1.990045]\n",
      "epoch:8 step:8234 [D loss: 0.651584, acc: 61.72%] [G loss: 2.040999]\n",
      "epoch:8 step:8235 [D loss: 0.600785, acc: 71.09%] [G loss: 2.038692]\n",
      "epoch:8 step:8236 [D loss: 0.615487, acc: 64.84%] [G loss: 2.145044]\n",
      "epoch:8 step:8237 [D loss: 0.632018, acc: 63.28%] [G loss: 1.922291]\n",
      "epoch:8 step:8238 [D loss: 0.655085, acc: 57.03%] [G loss: 2.306695]\n",
      "epoch:8 step:8239 [D loss: 0.637184, acc: 63.28%] [G loss: 2.019939]\n",
      "epoch:8 step:8240 [D loss: 0.649292, acc: 61.72%] [G loss: 2.035689]\n",
      "epoch:8 step:8241 [D loss: 0.668926, acc: 58.59%] [G loss: 2.026874]\n",
      "epoch:8 step:8242 [D loss: 0.592900, acc: 69.53%] [G loss: 2.412010]\n",
      "epoch:8 step:8243 [D loss: 0.571836, acc: 72.66%] [G loss: 2.154893]\n",
      "epoch:8 step:8244 [D loss: 0.644466, acc: 63.28%] [G loss: 2.131108]\n",
      "epoch:8 step:8245 [D loss: 0.597502, acc: 72.66%] [G loss: 2.040087]\n",
      "epoch:8 step:8246 [D loss: 0.648856, acc: 62.50%] [G loss: 2.065669]\n",
      "epoch:8 step:8247 [D loss: 0.641901, acc: 64.06%] [G loss: 1.964522]\n",
      "epoch:8 step:8248 [D loss: 0.623695, acc: 68.75%] [G loss: 1.942454]\n",
      "epoch:8 step:8249 [D loss: 0.568868, acc: 71.88%] [G loss: 2.109768]\n",
      "epoch:8 step:8250 [D loss: 0.552877, acc: 71.09%] [G loss: 2.274190]\n",
      "epoch:8 step:8251 [D loss: 0.613353, acc: 67.19%] [G loss: 2.193499]\n",
      "epoch:8 step:8252 [D loss: 0.599814, acc: 68.75%] [G loss: 2.080649]\n",
      "epoch:8 step:8253 [D loss: 0.565229, acc: 74.22%] [G loss: 2.224212]\n",
      "epoch:8 step:8254 [D loss: 0.670036, acc: 65.62%] [G loss: 2.060426]\n",
      "epoch:8 step:8255 [D loss: 0.621934, acc: 64.84%] [G loss: 2.002874]\n",
      "epoch:8 step:8256 [D loss: 0.606245, acc: 70.31%] [G loss: 2.191752]\n",
      "epoch:8 step:8257 [D loss: 0.632034, acc: 66.41%] [G loss: 2.312726]\n",
      "epoch:8 step:8258 [D loss: 0.631917, acc: 68.75%] [G loss: 2.095134]\n",
      "epoch:8 step:8259 [D loss: 0.677014, acc: 63.28%] [G loss: 2.139594]\n",
      "epoch:8 step:8260 [D loss: 0.616695, acc: 64.06%] [G loss: 2.101615]\n",
      "epoch:8 step:8261 [D loss: 0.647532, acc: 61.72%] [G loss: 2.058237]\n",
      "epoch:8 step:8262 [D loss: 0.678647, acc: 61.72%] [G loss: 1.943693]\n",
      "epoch:8 step:8263 [D loss: 0.624491, acc: 64.06%] [G loss: 2.261291]\n",
      "epoch:8 step:8264 [D loss: 0.599644, acc: 63.28%] [G loss: 2.112967]\n",
      "epoch:8 step:8265 [D loss: 0.612340, acc: 65.62%] [G loss: 2.292201]\n",
      "epoch:8 step:8266 [D loss: 0.627683, acc: 65.62%] [G loss: 2.194246]\n",
      "epoch:8 step:8267 [D loss: 0.653965, acc: 59.38%] [G loss: 2.373461]\n",
      "epoch:8 step:8268 [D loss: 0.580149, acc: 69.53%] [G loss: 2.385207]\n",
      "epoch:8 step:8269 [D loss: 0.646909, acc: 64.84%] [G loss: 2.014863]\n",
      "epoch:8 step:8270 [D loss: 0.656940, acc: 60.94%] [G loss: 2.272436]\n",
      "epoch:8 step:8271 [D loss: 0.594697, acc: 69.53%] [G loss: 2.339144]\n",
      "epoch:8 step:8272 [D loss: 0.573493, acc: 68.75%] [G loss: 2.268339]\n",
      "epoch:8 step:8273 [D loss: 0.605707, acc: 66.41%] [G loss: 2.218662]\n",
      "epoch:8 step:8274 [D loss: 0.557099, acc: 70.31%] [G loss: 2.074132]\n",
      "epoch:8 step:8275 [D loss: 0.634913, acc: 62.50%] [G loss: 2.231695]\n",
      "epoch:8 step:8276 [D loss: 0.646910, acc: 66.41%] [G loss: 2.207939]\n",
      "epoch:8 step:8277 [D loss: 0.569954, acc: 74.22%] [G loss: 2.244293]\n",
      "epoch:8 step:8278 [D loss: 0.601269, acc: 70.31%] [G loss: 2.305665]\n",
      "epoch:8 step:8279 [D loss: 0.589284, acc: 70.31%] [G loss: 2.157263]\n",
      "epoch:8 step:8280 [D loss: 0.657694, acc: 64.06%] [G loss: 1.999947]\n",
      "epoch:8 step:8281 [D loss: 0.667293, acc: 66.41%] [G loss: 2.115388]\n",
      "epoch:8 step:8282 [D loss: 0.652828, acc: 62.50%] [G loss: 2.234005]\n",
      "epoch:8 step:8283 [D loss: 0.670054, acc: 61.72%] [G loss: 1.968983]\n",
      "epoch:8 step:8284 [D loss: 0.684553, acc: 60.94%] [G loss: 1.986534]\n",
      "epoch:8 step:8285 [D loss: 0.633099, acc: 62.50%] [G loss: 1.987847]\n",
      "epoch:8 step:8286 [D loss: 0.577688, acc: 68.75%] [G loss: 2.365508]\n",
      "epoch:8 step:8287 [D loss: 0.612053, acc: 61.72%] [G loss: 2.114459]\n",
      "epoch:8 step:8288 [D loss: 0.591285, acc: 72.66%] [G loss: 2.283976]\n",
      "epoch:8 step:8289 [D loss: 0.602258, acc: 69.53%] [G loss: 2.082859]\n",
      "epoch:8 step:8290 [D loss: 0.639428, acc: 59.38%] [G loss: 2.063348]\n",
      "epoch:8 step:8291 [D loss: 0.618707, acc: 65.62%] [G loss: 2.184988]\n",
      "epoch:8 step:8292 [D loss: 0.631735, acc: 60.94%] [G loss: 2.289085]\n",
      "epoch:8 step:8293 [D loss: 0.597061, acc: 68.75%] [G loss: 2.177563]\n",
      "epoch:8 step:8294 [D loss: 0.579342, acc: 68.75%] [G loss: 2.203264]\n",
      "epoch:8 step:8295 [D loss: 0.661713, acc: 67.19%] [G loss: 2.102813]\n",
      "epoch:8 step:8296 [D loss: 0.570785, acc: 70.31%] [G loss: 2.005155]\n",
      "epoch:8 step:8297 [D loss: 0.690119, acc: 57.81%] [G loss: 2.010161]\n",
      "epoch:8 step:8298 [D loss: 0.600931, acc: 63.28%] [G loss: 2.141142]\n",
      "epoch:8 step:8299 [D loss: 0.630608, acc: 64.84%] [G loss: 2.378086]\n",
      "epoch:8 step:8300 [D loss: 0.617366, acc: 66.41%] [G loss: 2.084697]\n",
      "epoch:8 step:8301 [D loss: 0.585901, acc: 68.75%] [G loss: 2.267491]\n",
      "epoch:8 step:8302 [D loss: 0.570416, acc: 71.88%] [G loss: 2.363085]\n",
      "epoch:8 step:8303 [D loss: 0.561962, acc: 70.31%] [G loss: 2.268829]\n",
      "epoch:8 step:8304 [D loss: 0.578626, acc: 71.09%] [G loss: 2.134475]\n",
      "epoch:8 step:8305 [D loss: 0.609877, acc: 71.88%] [G loss: 2.156561]\n",
      "epoch:8 step:8306 [D loss: 0.571389, acc: 69.53%] [G loss: 2.124281]\n",
      "epoch:8 step:8307 [D loss: 0.647146, acc: 65.62%] [G loss: 2.226031]\n",
      "epoch:8 step:8308 [D loss: 0.666041, acc: 58.59%] [G loss: 2.114902]\n",
      "epoch:8 step:8309 [D loss: 0.599216, acc: 71.09%] [G loss: 2.199968]\n",
      "epoch:8 step:8310 [D loss: 0.653385, acc: 59.38%] [G loss: 2.279152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8311 [D loss: 0.627168, acc: 67.19%] [G loss: 2.326522]\n",
      "epoch:8 step:8312 [D loss: 0.584368, acc: 63.28%] [G loss: 2.367121]\n",
      "epoch:8 step:8313 [D loss: 0.660642, acc: 55.47%] [G loss: 2.265926]\n",
      "epoch:8 step:8314 [D loss: 0.618272, acc: 67.97%] [G loss: 2.171852]\n",
      "epoch:8 step:8315 [D loss: 0.634982, acc: 66.41%] [G loss: 2.242917]\n",
      "epoch:8 step:8316 [D loss: 0.678668, acc: 63.28%] [G loss: 1.863393]\n",
      "epoch:8 step:8317 [D loss: 0.624196, acc: 67.19%] [G loss: 2.080267]\n",
      "epoch:8 step:8318 [D loss: 0.585798, acc: 71.09%] [G loss: 2.339477]\n",
      "epoch:8 step:8319 [D loss: 0.605247, acc: 67.97%] [G loss: 2.342270]\n",
      "epoch:8 step:8320 [D loss: 0.666375, acc: 59.38%] [G loss: 2.194607]\n",
      "epoch:8 step:8321 [D loss: 0.644451, acc: 62.50%] [G loss: 2.167991]\n",
      "epoch:8 step:8322 [D loss: 0.671123, acc: 59.38%] [G loss: 2.312646]\n",
      "epoch:8 step:8323 [D loss: 0.688802, acc: 58.59%] [G loss: 1.942527]\n",
      "epoch:8 step:8324 [D loss: 0.666949, acc: 57.81%] [G loss: 1.965432]\n",
      "epoch:8 step:8325 [D loss: 0.593076, acc: 67.97%] [G loss: 2.104392]\n",
      "epoch:8 step:8326 [D loss: 0.678477, acc: 58.59%] [G loss: 2.128749]\n",
      "epoch:8 step:8327 [D loss: 0.608613, acc: 66.41%] [G loss: 1.999291]\n",
      "epoch:8 step:8328 [D loss: 0.643465, acc: 63.28%] [G loss: 2.017452]\n",
      "epoch:8 step:8329 [D loss: 0.607479, acc: 66.41%] [G loss: 2.001297]\n",
      "epoch:8 step:8330 [D loss: 0.590974, acc: 65.62%] [G loss: 2.267748]\n",
      "epoch:8 step:8331 [D loss: 0.604942, acc: 66.41%] [G loss: 2.009513]\n",
      "epoch:8 step:8332 [D loss: 0.620542, acc: 64.06%] [G loss: 2.132485]\n",
      "epoch:8 step:8333 [D loss: 0.623629, acc: 64.84%] [G loss: 2.082396]\n",
      "epoch:8 step:8334 [D loss: 0.591769, acc: 70.31%] [G loss: 2.294853]\n",
      "epoch:8 step:8335 [D loss: 0.656715, acc: 58.59%] [G loss: 2.509835]\n",
      "epoch:8 step:8336 [D loss: 0.593784, acc: 69.53%] [G loss: 2.338028]\n",
      "epoch:8 step:8337 [D loss: 0.599272, acc: 64.06%] [G loss: 2.143553]\n",
      "epoch:8 step:8338 [D loss: 0.578161, acc: 67.19%] [G loss: 2.204634]\n",
      "epoch:8 step:8339 [D loss: 0.636351, acc: 69.53%] [G loss: 2.196440]\n",
      "epoch:8 step:8340 [D loss: 0.616027, acc: 66.41%] [G loss: 2.250907]\n",
      "epoch:8 step:8341 [D loss: 0.510703, acc: 81.25%] [G loss: 2.395392]\n",
      "epoch:8 step:8342 [D loss: 0.618065, acc: 67.97%] [G loss: 2.134436]\n",
      "epoch:8 step:8343 [D loss: 0.668856, acc: 65.62%] [G loss: 2.170755]\n",
      "epoch:8 step:8344 [D loss: 0.634215, acc: 62.50%] [G loss: 2.168549]\n",
      "epoch:8 step:8345 [D loss: 0.582807, acc: 68.75%] [G loss: 2.455549]\n",
      "epoch:8 step:8346 [D loss: 0.656295, acc: 61.72%] [G loss: 2.203315]\n",
      "epoch:8 step:8347 [D loss: 0.606260, acc: 62.50%] [G loss: 2.210071]\n",
      "epoch:8 step:8348 [D loss: 0.639888, acc: 66.41%] [G loss: 2.051941]\n",
      "epoch:8 step:8349 [D loss: 0.703388, acc: 58.59%] [G loss: 2.252155]\n",
      "epoch:8 step:8350 [D loss: 0.748083, acc: 59.38%] [G loss: 1.901626]\n",
      "epoch:8 step:8351 [D loss: 0.691642, acc: 58.59%] [G loss: 1.869802]\n",
      "epoch:8 step:8352 [D loss: 0.648947, acc: 60.94%] [G loss: 2.013182]\n",
      "epoch:8 step:8353 [D loss: 0.643237, acc: 67.97%] [G loss: 2.131971]\n",
      "epoch:8 step:8354 [D loss: 0.730367, acc: 55.47%] [G loss: 1.997615]\n",
      "epoch:8 step:8355 [D loss: 0.681907, acc: 57.03%] [G loss: 1.982155]\n",
      "epoch:8 step:8356 [D loss: 0.655639, acc: 59.38%] [G loss: 2.156018]\n",
      "epoch:8 step:8357 [D loss: 0.606510, acc: 63.28%] [G loss: 2.068045]\n",
      "epoch:8 step:8358 [D loss: 0.635274, acc: 65.62%] [G loss: 1.917473]\n",
      "epoch:8 step:8359 [D loss: 0.586562, acc: 71.09%] [G loss: 2.050381]\n",
      "epoch:8 step:8360 [D loss: 0.654382, acc: 60.16%] [G loss: 2.112006]\n",
      "epoch:8 step:8361 [D loss: 0.586387, acc: 70.31%] [G loss: 1.914807]\n",
      "epoch:8 step:8362 [D loss: 0.675386, acc: 60.16%] [G loss: 2.066773]\n",
      "epoch:8 step:8363 [D loss: 0.602956, acc: 66.41%] [G loss: 2.178802]\n",
      "epoch:8 step:8364 [D loss: 0.599250, acc: 72.66%] [G loss: 2.078418]\n",
      "epoch:8 step:8365 [D loss: 0.648026, acc: 67.97%] [G loss: 2.168623]\n",
      "epoch:8 step:8366 [D loss: 0.627608, acc: 64.84%] [G loss: 1.890160]\n",
      "epoch:8 step:8367 [D loss: 0.567231, acc: 73.44%] [G loss: 2.021122]\n",
      "epoch:8 step:8368 [D loss: 0.693780, acc: 60.94%] [G loss: 2.204296]\n",
      "epoch:8 step:8369 [D loss: 0.569060, acc: 73.44%] [G loss: 1.978782]\n",
      "epoch:8 step:8370 [D loss: 0.646883, acc: 67.19%] [G loss: 2.127033]\n",
      "epoch:8 step:8371 [D loss: 0.619884, acc: 64.84%] [G loss: 2.214166]\n",
      "epoch:8 step:8372 [D loss: 0.571540, acc: 75.00%] [G loss: 2.164420]\n",
      "epoch:8 step:8373 [D loss: 0.613548, acc: 73.44%] [G loss: 2.052697]\n",
      "epoch:8 step:8374 [D loss: 0.649936, acc: 64.06%] [G loss: 2.143620]\n",
      "epoch:8 step:8375 [D loss: 0.653759, acc: 62.50%] [G loss: 1.953165]\n",
      "epoch:8 step:8376 [D loss: 0.635084, acc: 65.62%] [G loss: 2.047518]\n",
      "epoch:8 step:8377 [D loss: 0.659072, acc: 62.50%] [G loss: 2.135954]\n",
      "epoch:8 step:8378 [D loss: 0.637943, acc: 61.72%] [G loss: 2.104596]\n",
      "epoch:8 step:8379 [D loss: 0.620843, acc: 67.19%] [G loss: 2.225842]\n",
      "epoch:8 step:8380 [D loss: 0.611920, acc: 73.44%] [G loss: 2.277864]\n",
      "epoch:8 step:8381 [D loss: 0.630931, acc: 67.97%] [G loss: 2.504340]\n",
      "epoch:8 step:8382 [D loss: 0.597429, acc: 65.62%] [G loss: 2.391519]\n",
      "epoch:8 step:8383 [D loss: 0.609731, acc: 71.88%] [G loss: 2.354693]\n",
      "epoch:8 step:8384 [D loss: 0.633339, acc: 61.72%] [G loss: 2.275198]\n",
      "epoch:8 step:8385 [D loss: 0.590718, acc: 68.75%] [G loss: 2.275323]\n",
      "epoch:8 step:8386 [D loss: 0.655488, acc: 64.84%] [G loss: 2.068672]\n",
      "epoch:8 step:8387 [D loss: 0.664577, acc: 60.16%] [G loss: 2.136452]\n",
      "epoch:8 step:8388 [D loss: 0.712421, acc: 52.34%] [G loss: 2.171406]\n",
      "epoch:8 step:8389 [D loss: 0.600915, acc: 64.06%] [G loss: 2.179844]\n",
      "epoch:8 step:8390 [D loss: 0.531935, acc: 78.12%] [G loss: 2.443677]\n",
      "epoch:8 step:8391 [D loss: 0.624068, acc: 62.50%] [G loss: 2.105014]\n",
      "epoch:8 step:8392 [D loss: 0.664361, acc: 62.50%] [G loss: 2.148621]\n",
      "epoch:8 step:8393 [D loss: 0.639651, acc: 71.09%] [G loss: 2.267061]\n",
      "epoch:8 step:8394 [D loss: 0.569928, acc: 73.44%] [G loss: 2.168876]\n",
      "epoch:8 step:8395 [D loss: 0.590859, acc: 61.72%] [G loss: 2.291254]\n",
      "epoch:8 step:8396 [D loss: 0.562534, acc: 72.66%] [G loss: 2.161376]\n",
      "epoch:8 step:8397 [D loss: 0.625426, acc: 63.28%] [G loss: 2.059541]\n",
      "epoch:8 step:8398 [D loss: 0.662343, acc: 63.28%] [G loss: 2.289512]\n",
      "epoch:8 step:8399 [D loss: 0.597311, acc: 69.53%] [G loss: 2.161461]\n",
      "epoch:8 step:8400 [D loss: 0.633321, acc: 67.19%] [G loss: 2.355235]\n",
      "epoch:8 step:8401 [D loss: 0.606651, acc: 66.41%] [G loss: 2.474184]\n",
      "epoch:8 step:8402 [D loss: 0.554202, acc: 68.75%] [G loss: 2.198248]\n",
      "epoch:8 step:8403 [D loss: 0.597591, acc: 65.62%] [G loss: 2.212725]\n",
      "epoch:8 step:8404 [D loss: 0.740546, acc: 57.81%] [G loss: 1.993294]\n",
      "epoch:8 step:8405 [D loss: 0.585128, acc: 68.75%] [G loss: 2.376750]\n",
      "epoch:8 step:8406 [D loss: 0.585584, acc: 67.97%] [G loss: 2.269189]\n",
      "epoch:8 step:8407 [D loss: 0.584957, acc: 68.75%] [G loss: 2.256283]\n",
      "epoch:8 step:8408 [D loss: 0.597493, acc: 65.62%] [G loss: 2.518241]\n",
      "epoch:8 step:8409 [D loss: 0.612218, acc: 62.50%] [G loss: 2.129718]\n",
      "epoch:8 step:8410 [D loss: 0.628889, acc: 64.06%] [G loss: 2.183762]\n",
      "epoch:8 step:8411 [D loss: 0.588787, acc: 72.66%] [G loss: 2.248298]\n",
      "epoch:8 step:8412 [D loss: 0.679957, acc: 60.94%] [G loss: 2.155882]\n",
      "epoch:8 step:8413 [D loss: 0.650754, acc: 61.72%] [G loss: 2.129081]\n",
      "epoch:8 step:8414 [D loss: 0.609861, acc: 71.09%] [G loss: 2.461982]\n",
      "epoch:8 step:8415 [D loss: 0.585430, acc: 67.97%] [G loss: 2.497927]\n",
      "epoch:8 step:8416 [D loss: 0.759085, acc: 52.34%] [G loss: 1.963739]\n",
      "epoch:8 step:8417 [D loss: 0.690605, acc: 57.03%] [G loss: 2.209768]\n",
      "epoch:8 step:8418 [D loss: 0.628253, acc: 65.62%] [G loss: 1.960629]\n",
      "epoch:8 step:8419 [D loss: 0.576962, acc: 68.75%] [G loss: 2.156631]\n",
      "epoch:8 step:8420 [D loss: 0.561884, acc: 69.53%] [G loss: 2.322423]\n",
      "epoch:8 step:8421 [D loss: 0.589568, acc: 70.31%] [G loss: 2.467658]\n",
      "epoch:8 step:8422 [D loss: 0.549916, acc: 67.19%] [G loss: 2.286086]\n",
      "epoch:8 step:8423 [D loss: 0.560399, acc: 68.75%] [G loss: 2.370943]\n",
      "epoch:8 step:8424 [D loss: 0.750847, acc: 46.88%] [G loss: 2.220176]\n",
      "epoch:8 step:8425 [D loss: 0.712228, acc: 50.78%] [G loss: 2.289639]\n",
      "epoch:8 step:8426 [D loss: 0.583688, acc: 67.97%] [G loss: 2.173021]\n",
      "epoch:8 step:8427 [D loss: 0.627738, acc: 67.19%] [G loss: 2.235726]\n",
      "epoch:8 step:8428 [D loss: 0.595396, acc: 71.88%] [G loss: 2.187267]\n",
      "epoch:8 step:8429 [D loss: 0.600969, acc: 67.19%] [G loss: 2.220324]\n",
      "epoch:8 step:8430 [D loss: 0.606974, acc: 69.53%] [G loss: 2.268448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:8431 [D loss: 0.582793, acc: 68.75%] [G loss: 2.100060]\n",
      "epoch:8 step:8432 [D loss: 0.684430, acc: 61.72%] [G loss: 2.320531]\n",
      "epoch:8 step:8433 [D loss: 0.513727, acc: 73.44%] [G loss: 2.659734]\n",
      "epoch:9 step:8434 [D loss: 0.681782, acc: 65.62%] [G loss: 2.210400]\n",
      "epoch:9 step:8435 [D loss: 0.633545, acc: 64.06%] [G loss: 2.311945]\n",
      "epoch:9 step:8436 [D loss: 0.646756, acc: 58.59%] [G loss: 2.257218]\n",
      "epoch:9 step:8437 [D loss: 0.628866, acc: 66.41%] [G loss: 2.200993]\n",
      "epoch:9 step:8438 [D loss: 0.600268, acc: 66.41%] [G loss: 2.058933]\n",
      "epoch:9 step:8439 [D loss: 0.631066, acc: 64.06%] [G loss: 2.177730]\n",
      "epoch:9 step:8440 [D loss: 0.611363, acc: 65.62%] [G loss: 2.198579]\n",
      "epoch:9 step:8441 [D loss: 0.642742, acc: 64.06%] [G loss: 1.985920]\n",
      "epoch:9 step:8442 [D loss: 0.595738, acc: 67.97%] [G loss: 2.392100]\n",
      "epoch:9 step:8443 [D loss: 0.653893, acc: 64.84%] [G loss: 2.360699]\n",
      "epoch:9 step:8444 [D loss: 0.589138, acc: 70.31%] [G loss: 2.150285]\n",
      "epoch:9 step:8445 [D loss: 0.634118, acc: 60.94%] [G loss: 2.296950]\n",
      "epoch:9 step:8446 [D loss: 0.541915, acc: 75.00%] [G loss: 2.270952]\n",
      "epoch:9 step:8447 [D loss: 0.592775, acc: 66.41%] [G loss: 2.344436]\n",
      "epoch:9 step:8448 [D loss: 0.604391, acc: 64.84%] [G loss: 2.440578]\n",
      "epoch:9 step:8449 [D loss: 0.510106, acc: 73.44%] [G loss: 2.514817]\n",
      "epoch:9 step:8450 [D loss: 0.623354, acc: 66.41%] [G loss: 2.333563]\n",
      "epoch:9 step:8451 [D loss: 0.620536, acc: 66.41%] [G loss: 1.854892]\n",
      "epoch:9 step:8452 [D loss: 0.644974, acc: 66.41%] [G loss: 2.064143]\n",
      "epoch:9 step:8453 [D loss: 0.711525, acc: 56.25%] [G loss: 1.799780]\n",
      "epoch:9 step:8454 [D loss: 0.663206, acc: 58.59%] [G loss: 2.198281]\n",
      "epoch:9 step:8455 [D loss: 0.660852, acc: 61.72%] [G loss: 2.259905]\n",
      "epoch:9 step:8456 [D loss: 0.558706, acc: 71.88%] [G loss: 2.386887]\n",
      "epoch:9 step:8457 [D loss: 0.571817, acc: 73.44%] [G loss: 2.283165]\n",
      "epoch:9 step:8458 [D loss: 0.549224, acc: 69.53%] [G loss: 2.442904]\n",
      "epoch:9 step:8459 [D loss: 0.612583, acc: 65.62%] [G loss: 2.130786]\n",
      "epoch:9 step:8460 [D loss: 0.709038, acc: 58.59%] [G loss: 1.969376]\n",
      "epoch:9 step:8461 [D loss: 0.615553, acc: 65.62%] [G loss: 2.151775]\n",
      "epoch:9 step:8462 [D loss: 0.606882, acc: 67.19%] [G loss: 2.211416]\n",
      "epoch:9 step:8463 [D loss: 0.678070, acc: 60.94%] [G loss: 2.084927]\n",
      "epoch:9 step:8464 [D loss: 0.704661, acc: 52.34%] [G loss: 1.942140]\n",
      "epoch:9 step:8465 [D loss: 0.650753, acc: 59.38%] [G loss: 2.101846]\n",
      "epoch:9 step:8466 [D loss: 0.596853, acc: 68.75%] [G loss: 2.069320]\n",
      "epoch:9 step:8467 [D loss: 0.643345, acc: 62.50%] [G loss: 2.046279]\n",
      "epoch:9 step:8468 [D loss: 0.592409, acc: 69.53%] [G loss: 2.107155]\n",
      "epoch:9 step:8469 [D loss: 0.629576, acc: 61.72%] [G loss: 2.389457]\n",
      "epoch:9 step:8470 [D loss: 0.612613, acc: 71.09%] [G loss: 2.220951]\n",
      "epoch:9 step:8471 [D loss: 0.633445, acc: 60.94%] [G loss: 2.290085]\n",
      "epoch:9 step:8472 [D loss: 0.591083, acc: 71.09%] [G loss: 2.400372]\n",
      "epoch:9 step:8473 [D loss: 0.604658, acc: 62.50%] [G loss: 2.435284]\n",
      "epoch:9 step:8474 [D loss: 0.605537, acc: 69.53%] [G loss: 2.004970]\n",
      "epoch:9 step:8475 [D loss: 0.582746, acc: 71.88%] [G loss: 2.119585]\n",
      "epoch:9 step:8476 [D loss: 0.599094, acc: 64.06%] [G loss: 2.192637]\n",
      "epoch:9 step:8477 [D loss: 0.617971, acc: 67.19%] [G loss: 2.142826]\n",
      "epoch:9 step:8478 [D loss: 0.698603, acc: 63.28%] [G loss: 2.129621]\n",
      "epoch:9 step:8479 [D loss: 0.759934, acc: 55.47%] [G loss: 1.768362]\n",
      "epoch:9 step:8480 [D loss: 0.714299, acc: 55.47%] [G loss: 2.015051]\n",
      "epoch:9 step:8481 [D loss: 0.557090, acc: 69.53%] [G loss: 1.978832]\n",
      "epoch:9 step:8482 [D loss: 0.639543, acc: 70.31%] [G loss: 1.980180]\n",
      "epoch:9 step:8483 [D loss: 0.595916, acc: 70.31%] [G loss: 2.122291]\n",
      "epoch:9 step:8484 [D loss: 0.647131, acc: 64.06%] [G loss: 2.000785]\n",
      "epoch:9 step:8485 [D loss: 0.678316, acc: 61.72%] [G loss: 2.156690]\n",
      "epoch:9 step:8486 [D loss: 0.547906, acc: 71.09%] [G loss: 2.249322]\n",
      "epoch:9 step:8487 [D loss: 0.567224, acc: 69.53%] [G loss: 2.237819]\n",
      "epoch:9 step:8488 [D loss: 0.566060, acc: 73.44%] [G loss: 2.257423]\n",
      "epoch:9 step:8489 [D loss: 0.608584, acc: 68.75%] [G loss: 2.136965]\n",
      "epoch:9 step:8490 [D loss: 0.624432, acc: 62.50%] [G loss: 2.309704]\n",
      "epoch:9 step:8491 [D loss: 0.614560, acc: 67.97%] [G loss: 2.261005]\n",
      "epoch:9 step:8492 [D loss: 0.648099, acc: 66.41%] [G loss: 2.164275]\n",
      "epoch:9 step:8493 [D loss: 0.651171, acc: 59.38%] [G loss: 2.141524]\n",
      "epoch:9 step:8494 [D loss: 0.664540, acc: 60.94%] [G loss: 2.270557]\n",
      "epoch:9 step:8495 [D loss: 0.626241, acc: 65.62%] [G loss: 2.179274]\n",
      "epoch:9 step:8496 [D loss: 0.708341, acc: 60.16%] [G loss: 2.094775]\n",
      "epoch:9 step:8497 [D loss: 0.561798, acc: 66.41%] [G loss: 2.258736]\n",
      "epoch:9 step:8498 [D loss: 0.671679, acc: 61.72%] [G loss: 2.149379]\n",
      "epoch:9 step:8499 [D loss: 0.573970, acc: 70.31%] [G loss: 2.203863]\n",
      "epoch:9 step:8500 [D loss: 0.608812, acc: 64.06%] [G loss: 2.125443]\n",
      "epoch:9 step:8501 [D loss: 0.629087, acc: 64.06%] [G loss: 2.056103]\n",
      "epoch:9 step:8502 [D loss: 0.597286, acc: 65.62%] [G loss: 2.304274]\n",
      "epoch:9 step:8503 [D loss: 0.531969, acc: 77.34%] [G loss: 2.225783]\n",
      "epoch:9 step:8504 [D loss: 0.609424, acc: 61.72%] [G loss: 2.076749]\n",
      "epoch:9 step:8505 [D loss: 0.602383, acc: 67.97%] [G loss: 2.397038]\n",
      "epoch:9 step:8506 [D loss: 0.646699, acc: 60.94%] [G loss: 2.081260]\n",
      "epoch:9 step:8507 [D loss: 0.612236, acc: 70.31%] [G loss: 2.257980]\n",
      "epoch:9 step:8508 [D loss: 0.599662, acc: 65.62%] [G loss: 2.629328]\n",
      "epoch:9 step:8509 [D loss: 0.591477, acc: 63.28%] [G loss: 2.705048]\n",
      "epoch:9 step:8510 [D loss: 0.535560, acc: 73.44%] [G loss: 2.608342]\n",
      "epoch:9 step:8511 [D loss: 0.709706, acc: 54.69%] [G loss: 2.326239]\n",
      "epoch:9 step:8512 [D loss: 0.597986, acc: 72.66%] [G loss: 2.142233]\n",
      "epoch:9 step:8513 [D loss: 0.603020, acc: 64.84%] [G loss: 2.099587]\n",
      "epoch:9 step:8514 [D loss: 0.679586, acc: 59.38%] [G loss: 2.016778]\n",
      "epoch:9 step:8515 [D loss: 0.689347, acc: 62.50%] [G loss: 2.124542]\n",
      "epoch:9 step:8516 [D loss: 0.615292, acc: 67.97%] [G loss: 2.332815]\n",
      "epoch:9 step:8517 [D loss: 0.606869, acc: 69.53%] [G loss: 2.036159]\n",
      "epoch:9 step:8518 [D loss: 0.709813, acc: 53.91%] [G loss: 2.093293]\n",
      "epoch:9 step:8519 [D loss: 0.671295, acc: 61.72%] [G loss: 1.897581]\n",
      "epoch:9 step:8520 [D loss: 0.587573, acc: 67.19%] [G loss: 2.089281]\n",
      "epoch:9 step:8521 [D loss: 0.657527, acc: 60.94%] [G loss: 1.967455]\n",
      "epoch:9 step:8522 [D loss: 0.596972, acc: 71.09%] [G loss: 2.361582]\n",
      "epoch:9 step:8523 [D loss: 0.588263, acc: 69.53%] [G loss: 2.229915]\n",
      "epoch:9 step:8524 [D loss: 0.647596, acc: 63.28%] [G loss: 2.071470]\n",
      "epoch:9 step:8525 [D loss: 0.609839, acc: 71.09%] [G loss: 2.158348]\n",
      "epoch:9 step:8526 [D loss: 0.579957, acc: 72.66%] [G loss: 2.310588]\n",
      "epoch:9 step:8527 [D loss: 0.672596, acc: 57.03%] [G loss: 2.091737]\n",
      "epoch:9 step:8528 [D loss: 0.629725, acc: 65.62%] [G loss: 2.015271]\n",
      "epoch:9 step:8529 [D loss: 0.596690, acc: 69.53%] [G loss: 2.302042]\n",
      "epoch:9 step:8530 [D loss: 0.628834, acc: 63.28%] [G loss: 2.107831]\n",
      "epoch:9 step:8531 [D loss: 0.646099, acc: 63.28%] [G loss: 1.895108]\n",
      "epoch:9 step:8532 [D loss: 0.597116, acc: 69.53%] [G loss: 2.048497]\n",
      "epoch:9 step:8533 [D loss: 0.592903, acc: 66.41%] [G loss: 2.097495]\n",
      "epoch:9 step:8534 [D loss: 0.617757, acc: 66.41%] [G loss: 2.050739]\n",
      "epoch:9 step:8535 [D loss: 0.661293, acc: 61.72%] [G loss: 2.035983]\n",
      "epoch:9 step:8536 [D loss: 0.589441, acc: 71.09%] [G loss: 2.293799]\n",
      "epoch:9 step:8537 [D loss: 0.576019, acc: 67.97%] [G loss: 2.160680]\n",
      "epoch:9 step:8538 [D loss: 0.621029, acc: 64.84%] [G loss: 2.087118]\n",
      "epoch:9 step:8539 [D loss: 0.587446, acc: 70.31%] [G loss: 2.149300]\n",
      "epoch:9 step:8540 [D loss: 0.605181, acc: 65.62%] [G loss: 2.052371]\n",
      "epoch:9 step:8541 [D loss: 0.574637, acc: 71.09%] [G loss: 2.036191]\n",
      "epoch:9 step:8542 [D loss: 0.736437, acc: 55.47%] [G loss: 2.018030]\n",
      "epoch:9 step:8543 [D loss: 0.645721, acc: 67.19%] [G loss: 1.946995]\n",
      "epoch:9 step:8544 [D loss: 0.567474, acc: 73.44%] [G loss: 2.193423]\n",
      "epoch:9 step:8545 [D loss: 0.676444, acc: 60.94%] [G loss: 2.062711]\n",
      "epoch:9 step:8546 [D loss: 0.650189, acc: 64.06%] [G loss: 2.208872]\n",
      "epoch:9 step:8547 [D loss: 0.596369, acc: 67.97%] [G loss: 2.295198]\n",
      "epoch:9 step:8548 [D loss: 0.637936, acc: 61.72%] [G loss: 2.465762]\n",
      "epoch:9 step:8549 [D loss: 0.566921, acc: 68.75%] [G loss: 2.354465]\n",
      "epoch:9 step:8550 [D loss: 0.583011, acc: 68.75%] [G loss: 2.454124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8551 [D loss: 0.602552, acc: 67.19%] [G loss: 2.304432]\n",
      "epoch:9 step:8552 [D loss: 0.539029, acc: 74.22%] [G loss: 2.386220]\n",
      "epoch:9 step:8553 [D loss: 0.666116, acc: 59.38%] [G loss: 2.091558]\n",
      "epoch:9 step:8554 [D loss: 0.618854, acc: 64.84%] [G loss: 2.357948]\n",
      "epoch:9 step:8555 [D loss: 0.605924, acc: 70.31%] [G loss: 2.375130]\n",
      "epoch:9 step:8556 [D loss: 0.671586, acc: 57.03%] [G loss: 2.032913]\n",
      "epoch:9 step:8557 [D loss: 0.742493, acc: 51.56%] [G loss: 2.010308]\n",
      "epoch:9 step:8558 [D loss: 0.641063, acc: 60.16%] [G loss: 2.044822]\n",
      "epoch:9 step:8559 [D loss: 0.636915, acc: 64.84%] [G loss: 2.088376]\n",
      "epoch:9 step:8560 [D loss: 0.674352, acc: 57.81%] [G loss: 2.160160]\n",
      "epoch:9 step:8561 [D loss: 0.605412, acc: 68.75%] [G loss: 2.273391]\n",
      "epoch:9 step:8562 [D loss: 0.580450, acc: 67.19%] [G loss: 2.109371]\n",
      "epoch:9 step:8563 [D loss: 0.641697, acc: 64.84%] [G loss: 2.040361]\n",
      "epoch:9 step:8564 [D loss: 0.662791, acc: 60.16%] [G loss: 2.342622]\n",
      "epoch:9 step:8565 [D loss: 0.630633, acc: 62.50%] [G loss: 2.085149]\n",
      "epoch:9 step:8566 [D loss: 0.744653, acc: 51.56%] [G loss: 2.104005]\n",
      "epoch:9 step:8567 [D loss: 0.652953, acc: 59.38%] [G loss: 1.951893]\n",
      "epoch:9 step:8568 [D loss: 0.652947, acc: 62.50%] [G loss: 2.008786]\n",
      "epoch:9 step:8569 [D loss: 0.635052, acc: 62.50%] [G loss: 1.958561]\n",
      "epoch:9 step:8570 [D loss: 0.624383, acc: 67.19%] [G loss: 2.001315]\n",
      "epoch:9 step:8571 [D loss: 0.619508, acc: 65.62%] [G loss: 1.950461]\n",
      "epoch:9 step:8572 [D loss: 0.591981, acc: 72.66%] [G loss: 2.100989]\n",
      "epoch:9 step:8573 [D loss: 0.623329, acc: 62.50%] [G loss: 2.088755]\n",
      "epoch:9 step:8574 [D loss: 0.627427, acc: 62.50%] [G loss: 1.977382]\n",
      "epoch:9 step:8575 [D loss: 0.630509, acc: 62.50%] [G loss: 1.917371]\n",
      "epoch:9 step:8576 [D loss: 0.637874, acc: 63.28%] [G loss: 2.109367]\n",
      "epoch:9 step:8577 [D loss: 0.616779, acc: 64.84%] [G loss: 2.334942]\n",
      "epoch:9 step:8578 [D loss: 0.639125, acc: 60.94%] [G loss: 1.964883]\n",
      "epoch:9 step:8579 [D loss: 0.573109, acc: 75.00%] [G loss: 2.210543]\n",
      "epoch:9 step:8580 [D loss: 0.643171, acc: 60.16%] [G loss: 2.189882]\n",
      "epoch:9 step:8581 [D loss: 0.595805, acc: 69.53%] [G loss: 2.094070]\n",
      "epoch:9 step:8582 [D loss: 0.588751, acc: 67.19%] [G loss: 2.362697]\n",
      "epoch:9 step:8583 [D loss: 0.693446, acc: 57.81%] [G loss: 2.212328]\n",
      "epoch:9 step:8584 [D loss: 0.612345, acc: 67.19%] [G loss: 2.204192]\n",
      "epoch:9 step:8585 [D loss: 0.610319, acc: 67.97%] [G loss: 2.169368]\n",
      "epoch:9 step:8586 [D loss: 0.652351, acc: 60.94%] [G loss: 2.254333]\n",
      "epoch:9 step:8587 [D loss: 0.592076, acc: 68.75%] [G loss: 2.417936]\n",
      "epoch:9 step:8588 [D loss: 0.649655, acc: 64.84%] [G loss: 2.020640]\n",
      "epoch:9 step:8589 [D loss: 0.638115, acc: 60.94%] [G loss: 2.127731]\n",
      "epoch:9 step:8590 [D loss: 0.598215, acc: 69.53%] [G loss: 2.153518]\n",
      "epoch:9 step:8591 [D loss: 0.646733, acc: 61.72%] [G loss: 2.149909]\n",
      "epoch:9 step:8592 [D loss: 0.610527, acc: 67.97%] [G loss: 2.272779]\n",
      "epoch:9 step:8593 [D loss: 0.694711, acc: 60.94%] [G loss: 2.133302]\n",
      "epoch:9 step:8594 [D loss: 0.676770, acc: 57.81%] [G loss: 2.193940]\n",
      "epoch:9 step:8595 [D loss: 0.557371, acc: 75.78%] [G loss: 2.093403]\n",
      "epoch:9 step:8596 [D loss: 0.639362, acc: 63.28%] [G loss: 2.073127]\n",
      "epoch:9 step:8597 [D loss: 0.670333, acc: 63.28%] [G loss: 1.961433]\n",
      "epoch:9 step:8598 [D loss: 0.614920, acc: 71.09%] [G loss: 1.968595]\n",
      "epoch:9 step:8599 [D loss: 0.613877, acc: 64.06%] [G loss: 2.091986]\n",
      "epoch:9 step:8600 [D loss: 0.633477, acc: 65.62%] [G loss: 2.002986]\n",
      "epoch:9 step:8601 [D loss: 0.641629, acc: 68.75%] [G loss: 2.018330]\n",
      "epoch:9 step:8602 [D loss: 0.653292, acc: 60.16%] [G loss: 2.121891]\n",
      "epoch:9 step:8603 [D loss: 0.718425, acc: 60.16%] [G loss: 1.929791]\n",
      "epoch:9 step:8604 [D loss: 0.582564, acc: 72.66%] [G loss: 2.040421]\n",
      "epoch:9 step:8605 [D loss: 0.641102, acc: 68.75%] [G loss: 2.109621]\n",
      "epoch:9 step:8606 [D loss: 0.591174, acc: 68.75%] [G loss: 2.144969]\n",
      "epoch:9 step:8607 [D loss: 0.688012, acc: 57.81%] [G loss: 2.067468]\n",
      "epoch:9 step:8608 [D loss: 0.665811, acc: 60.16%] [G loss: 1.942000]\n",
      "epoch:9 step:8609 [D loss: 0.656889, acc: 64.06%] [G loss: 2.188663]\n",
      "epoch:9 step:8610 [D loss: 0.645829, acc: 65.62%] [G loss: 2.030491]\n",
      "epoch:9 step:8611 [D loss: 0.655382, acc: 60.16%] [G loss: 2.030178]\n",
      "epoch:9 step:8612 [D loss: 0.637139, acc: 71.88%] [G loss: 1.982339]\n",
      "epoch:9 step:8613 [D loss: 0.735157, acc: 52.34%] [G loss: 2.046660]\n",
      "epoch:9 step:8614 [D loss: 0.644900, acc: 63.28%] [G loss: 2.055882]\n",
      "epoch:9 step:8615 [D loss: 0.631040, acc: 64.84%] [G loss: 2.086717]\n",
      "epoch:9 step:8616 [D loss: 0.705733, acc: 61.72%] [G loss: 1.952829]\n",
      "epoch:9 step:8617 [D loss: 0.659894, acc: 63.28%] [G loss: 2.090656]\n",
      "epoch:9 step:8618 [D loss: 0.628813, acc: 68.75%] [G loss: 2.206932]\n",
      "epoch:9 step:8619 [D loss: 0.680113, acc: 55.47%] [G loss: 2.006197]\n",
      "epoch:9 step:8620 [D loss: 0.642258, acc: 65.62%] [G loss: 1.863504]\n",
      "epoch:9 step:8621 [D loss: 0.628319, acc: 65.62%] [G loss: 2.030142]\n",
      "epoch:9 step:8622 [D loss: 0.629688, acc: 60.16%] [G loss: 2.057162]\n",
      "epoch:9 step:8623 [D loss: 0.678244, acc: 60.16%] [G loss: 2.162516]\n",
      "epoch:9 step:8624 [D loss: 0.625846, acc: 65.62%] [G loss: 1.864035]\n",
      "epoch:9 step:8625 [D loss: 0.571893, acc: 70.31%] [G loss: 2.177547]\n",
      "epoch:9 step:8626 [D loss: 0.694516, acc: 58.59%] [G loss: 2.075698]\n",
      "epoch:9 step:8627 [D loss: 0.546101, acc: 71.88%] [G loss: 2.215517]\n",
      "epoch:9 step:8628 [D loss: 0.645767, acc: 57.81%] [G loss: 2.125898]\n",
      "epoch:9 step:8629 [D loss: 0.628928, acc: 61.72%] [G loss: 2.064393]\n",
      "epoch:9 step:8630 [D loss: 0.684356, acc: 59.38%] [G loss: 2.136743]\n",
      "epoch:9 step:8631 [D loss: 0.601790, acc: 70.31%] [G loss: 2.084256]\n",
      "epoch:9 step:8632 [D loss: 0.633400, acc: 62.50%] [G loss: 1.986887]\n",
      "epoch:9 step:8633 [D loss: 0.680444, acc: 55.47%] [G loss: 1.980209]\n",
      "epoch:9 step:8634 [D loss: 0.632556, acc: 59.38%] [G loss: 1.949357]\n",
      "epoch:9 step:8635 [D loss: 0.653447, acc: 57.03%] [G loss: 1.959962]\n",
      "epoch:9 step:8636 [D loss: 0.631171, acc: 67.19%] [G loss: 1.951813]\n",
      "epoch:9 step:8637 [D loss: 0.622188, acc: 68.75%] [G loss: 2.009521]\n",
      "epoch:9 step:8638 [D loss: 0.643183, acc: 60.16%] [G loss: 2.124533]\n",
      "epoch:9 step:8639 [D loss: 0.668475, acc: 61.72%] [G loss: 2.141720]\n",
      "epoch:9 step:8640 [D loss: 0.576844, acc: 67.19%] [G loss: 2.209566]\n",
      "epoch:9 step:8641 [D loss: 0.570660, acc: 68.75%] [G loss: 2.569019]\n",
      "epoch:9 step:8642 [D loss: 0.604934, acc: 67.19%] [G loss: 2.313319]\n",
      "epoch:9 step:8643 [D loss: 0.678073, acc: 58.59%] [G loss: 2.041864]\n",
      "epoch:9 step:8644 [D loss: 0.633650, acc: 65.62%] [G loss: 2.069860]\n",
      "epoch:9 step:8645 [D loss: 0.676169, acc: 64.06%] [G loss: 1.924860]\n",
      "epoch:9 step:8646 [D loss: 0.707914, acc: 53.91%] [G loss: 1.719972]\n",
      "epoch:9 step:8647 [D loss: 0.630927, acc: 62.50%] [G loss: 1.846309]\n",
      "epoch:9 step:8648 [D loss: 0.664049, acc: 59.38%] [G loss: 1.920531]\n",
      "epoch:9 step:8649 [D loss: 0.643797, acc: 60.16%] [G loss: 2.120724]\n",
      "epoch:9 step:8650 [D loss: 0.613009, acc: 63.28%] [G loss: 2.336317]\n",
      "epoch:9 step:8651 [D loss: 0.568141, acc: 71.88%] [G loss: 2.331654]\n",
      "epoch:9 step:8652 [D loss: 0.529258, acc: 75.00%] [G loss: 2.213965]\n",
      "epoch:9 step:8653 [D loss: 0.736218, acc: 59.38%] [G loss: 1.981427]\n",
      "epoch:9 step:8654 [D loss: 0.670251, acc: 58.59%] [G loss: 2.032103]\n",
      "epoch:9 step:8655 [D loss: 0.605474, acc: 62.50%] [G loss: 2.063218]\n",
      "epoch:9 step:8656 [D loss: 0.623749, acc: 67.19%] [G loss: 2.048831]\n",
      "epoch:9 step:8657 [D loss: 0.630523, acc: 66.41%] [G loss: 2.133176]\n",
      "epoch:9 step:8658 [D loss: 0.629656, acc: 64.84%] [G loss: 1.989218]\n",
      "epoch:9 step:8659 [D loss: 0.668937, acc: 60.94%] [G loss: 2.020467]\n",
      "epoch:9 step:8660 [D loss: 0.591689, acc: 68.75%] [G loss: 2.139825]\n",
      "epoch:9 step:8661 [D loss: 0.732333, acc: 59.38%] [G loss: 1.868100]\n",
      "epoch:9 step:8662 [D loss: 0.619598, acc: 64.06%] [G loss: 2.083333]\n",
      "epoch:9 step:8663 [D loss: 0.520357, acc: 78.12%] [G loss: 2.503026]\n",
      "epoch:9 step:8664 [D loss: 0.573657, acc: 72.66%] [G loss: 2.653610]\n",
      "epoch:9 step:8665 [D loss: 0.563977, acc: 69.53%] [G loss: 2.521080]\n",
      "epoch:9 step:8666 [D loss: 0.664522, acc: 60.16%] [G loss: 2.141376]\n",
      "epoch:9 step:8667 [D loss: 0.629080, acc: 62.50%] [G loss: 2.134662]\n",
      "epoch:9 step:8668 [D loss: 0.664288, acc: 61.72%] [G loss: 2.035893]\n",
      "epoch:9 step:8669 [D loss: 0.577891, acc: 71.88%] [G loss: 2.264458]\n",
      "epoch:9 step:8670 [D loss: 0.667973, acc: 61.72%] [G loss: 2.192879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8671 [D loss: 0.621097, acc: 67.97%] [G loss: 2.178110]\n",
      "epoch:9 step:8672 [D loss: 0.584092, acc: 68.75%] [G loss: 2.116753]\n",
      "epoch:9 step:8673 [D loss: 0.624123, acc: 63.28%] [G loss: 2.057203]\n",
      "epoch:9 step:8674 [D loss: 0.611406, acc: 67.97%] [G loss: 2.051567]\n",
      "epoch:9 step:8675 [D loss: 0.605953, acc: 67.19%] [G loss: 2.346867]\n",
      "epoch:9 step:8676 [D loss: 0.587171, acc: 71.88%] [G loss: 2.053759]\n",
      "epoch:9 step:8677 [D loss: 0.688434, acc: 64.84%] [G loss: 2.091897]\n",
      "epoch:9 step:8678 [D loss: 0.581077, acc: 67.97%] [G loss: 2.290133]\n",
      "epoch:9 step:8679 [D loss: 0.651076, acc: 62.50%] [G loss: 2.060897]\n",
      "epoch:9 step:8680 [D loss: 0.643423, acc: 61.72%] [G loss: 2.230849]\n",
      "epoch:9 step:8681 [D loss: 0.639114, acc: 62.50%] [G loss: 2.196189]\n",
      "epoch:9 step:8682 [D loss: 0.673677, acc: 57.81%] [G loss: 2.024279]\n",
      "epoch:9 step:8683 [D loss: 0.664641, acc: 61.72%] [G loss: 1.926276]\n",
      "epoch:9 step:8684 [D loss: 0.668529, acc: 60.16%] [G loss: 1.916861]\n",
      "epoch:9 step:8685 [D loss: 0.660470, acc: 64.06%] [G loss: 1.825856]\n",
      "epoch:9 step:8686 [D loss: 0.594294, acc: 64.84%] [G loss: 2.071142]\n",
      "epoch:9 step:8687 [D loss: 0.647005, acc: 64.84%] [G loss: 1.942219]\n",
      "epoch:9 step:8688 [D loss: 0.557470, acc: 71.09%] [G loss: 1.961469]\n",
      "epoch:9 step:8689 [D loss: 0.667091, acc: 61.72%] [G loss: 2.039834]\n",
      "epoch:9 step:8690 [D loss: 0.673149, acc: 61.72%] [G loss: 2.101906]\n",
      "epoch:9 step:8691 [D loss: 0.616337, acc: 67.97%] [G loss: 2.083583]\n",
      "epoch:9 step:8692 [D loss: 0.638467, acc: 66.41%] [G loss: 2.010064]\n",
      "epoch:9 step:8693 [D loss: 0.624284, acc: 68.75%] [G loss: 2.093392]\n",
      "epoch:9 step:8694 [D loss: 0.667296, acc: 53.12%] [G loss: 2.066050]\n",
      "epoch:9 step:8695 [D loss: 0.605933, acc: 66.41%] [G loss: 2.014640]\n",
      "epoch:9 step:8696 [D loss: 0.690132, acc: 60.16%] [G loss: 2.025801]\n",
      "epoch:9 step:8697 [D loss: 0.605941, acc: 64.84%] [G loss: 2.297383]\n",
      "epoch:9 step:8698 [D loss: 0.703323, acc: 57.03%] [G loss: 1.836507]\n",
      "epoch:9 step:8699 [D loss: 0.670763, acc: 58.59%] [G loss: 2.063544]\n",
      "epoch:9 step:8700 [D loss: 0.603510, acc: 68.75%] [G loss: 1.966880]\n",
      "epoch:9 step:8701 [D loss: 0.602981, acc: 66.41%] [G loss: 2.182704]\n",
      "epoch:9 step:8702 [D loss: 0.591767, acc: 71.09%] [G loss: 2.046197]\n",
      "epoch:9 step:8703 [D loss: 0.605102, acc: 64.06%] [G loss: 2.216807]\n",
      "epoch:9 step:8704 [D loss: 0.586903, acc: 70.31%] [G loss: 2.147449]\n",
      "epoch:9 step:8705 [D loss: 0.595092, acc: 71.88%] [G loss: 2.213432]\n",
      "epoch:9 step:8706 [D loss: 0.621817, acc: 66.41%] [G loss: 2.122273]\n",
      "epoch:9 step:8707 [D loss: 0.547993, acc: 72.66%] [G loss: 2.244658]\n",
      "epoch:9 step:8708 [D loss: 0.601574, acc: 66.41%] [G loss: 2.181673]\n",
      "epoch:9 step:8709 [D loss: 0.606096, acc: 70.31%] [G loss: 2.286601]\n",
      "epoch:9 step:8710 [D loss: 0.595411, acc: 71.88%] [G loss: 1.998908]\n",
      "epoch:9 step:8711 [D loss: 0.654908, acc: 63.28%] [G loss: 2.340944]\n",
      "epoch:9 step:8712 [D loss: 0.629483, acc: 60.94%] [G loss: 2.058800]\n",
      "epoch:9 step:8713 [D loss: 0.615837, acc: 64.84%] [G loss: 2.246179]\n",
      "epoch:9 step:8714 [D loss: 0.681164, acc: 60.94%] [G loss: 1.861237]\n",
      "epoch:9 step:8715 [D loss: 0.638155, acc: 65.62%] [G loss: 2.063414]\n",
      "epoch:9 step:8716 [D loss: 0.584949, acc: 71.09%] [G loss: 2.243266]\n",
      "epoch:9 step:8717 [D loss: 0.650435, acc: 59.38%] [G loss: 2.172759]\n",
      "epoch:9 step:8718 [D loss: 0.624305, acc: 66.41%] [G loss: 2.164144]\n",
      "epoch:9 step:8719 [D loss: 0.545462, acc: 74.22%] [G loss: 2.085800]\n",
      "epoch:9 step:8720 [D loss: 0.701727, acc: 55.47%] [G loss: 2.000525]\n",
      "epoch:9 step:8721 [D loss: 0.645475, acc: 65.62%] [G loss: 1.907956]\n",
      "epoch:9 step:8722 [D loss: 0.593637, acc: 67.97%] [G loss: 2.069423]\n",
      "epoch:9 step:8723 [D loss: 0.589027, acc: 74.22%] [G loss: 2.031866]\n",
      "epoch:9 step:8724 [D loss: 0.635826, acc: 62.50%] [G loss: 2.258565]\n",
      "epoch:9 step:8725 [D loss: 0.566046, acc: 70.31%] [G loss: 2.130139]\n",
      "epoch:9 step:8726 [D loss: 0.620540, acc: 67.97%] [G loss: 2.121598]\n",
      "epoch:9 step:8727 [D loss: 0.659149, acc: 59.38%] [G loss: 2.095740]\n",
      "epoch:9 step:8728 [D loss: 0.577394, acc: 66.41%] [G loss: 2.221584]\n",
      "epoch:9 step:8729 [D loss: 0.568425, acc: 69.53%] [G loss: 2.289015]\n",
      "epoch:9 step:8730 [D loss: 0.594962, acc: 65.62%] [G loss: 2.334496]\n",
      "epoch:9 step:8731 [D loss: 0.630158, acc: 64.06%] [G loss: 2.254901]\n",
      "epoch:9 step:8732 [D loss: 0.646967, acc: 62.50%] [G loss: 2.105625]\n",
      "epoch:9 step:8733 [D loss: 0.679483, acc: 62.50%] [G loss: 2.123249]\n",
      "epoch:9 step:8734 [D loss: 0.636569, acc: 62.50%] [G loss: 2.144146]\n",
      "epoch:9 step:8735 [D loss: 0.628848, acc: 63.28%] [G loss: 2.229326]\n",
      "epoch:9 step:8736 [D loss: 0.641569, acc: 63.28%] [G loss: 2.100003]\n",
      "epoch:9 step:8737 [D loss: 0.586825, acc: 69.53%] [G loss: 2.156978]\n",
      "epoch:9 step:8738 [D loss: 0.581983, acc: 71.09%] [G loss: 2.243907]\n",
      "epoch:9 step:8739 [D loss: 0.630074, acc: 65.62%] [G loss: 2.216556]\n",
      "epoch:9 step:8740 [D loss: 0.616035, acc: 67.19%] [G loss: 2.040053]\n",
      "epoch:9 step:8741 [D loss: 0.647462, acc: 62.50%] [G loss: 2.063704]\n",
      "epoch:9 step:8742 [D loss: 0.598619, acc: 69.53%] [G loss: 2.256309]\n",
      "epoch:9 step:8743 [D loss: 0.632632, acc: 60.16%] [G loss: 1.951123]\n",
      "epoch:9 step:8744 [D loss: 0.637942, acc: 61.72%] [G loss: 2.089024]\n",
      "epoch:9 step:8745 [D loss: 0.557459, acc: 71.88%] [G loss: 2.658035]\n",
      "epoch:9 step:8746 [D loss: 0.593083, acc: 69.53%] [G loss: 2.275548]\n",
      "epoch:9 step:8747 [D loss: 0.538673, acc: 73.44%] [G loss: 2.357851]\n",
      "epoch:9 step:8748 [D loss: 0.570930, acc: 70.31%] [G loss: 2.518466]\n",
      "epoch:9 step:8749 [D loss: 0.637611, acc: 57.03%] [G loss: 2.150514]\n",
      "epoch:9 step:8750 [D loss: 0.759470, acc: 56.25%] [G loss: 2.181419]\n",
      "epoch:9 step:8751 [D loss: 0.589688, acc: 63.28%] [G loss: 2.256191]\n",
      "epoch:9 step:8752 [D loss: 0.653053, acc: 60.16%] [G loss: 2.027336]\n",
      "epoch:9 step:8753 [D loss: 0.645440, acc: 65.62%] [G loss: 2.237905]\n",
      "epoch:9 step:8754 [D loss: 0.543481, acc: 71.88%] [G loss: 2.403469]\n",
      "epoch:9 step:8755 [D loss: 0.611874, acc: 65.62%] [G loss: 2.182021]\n",
      "epoch:9 step:8756 [D loss: 0.632652, acc: 65.62%] [G loss: 1.933691]\n",
      "epoch:9 step:8757 [D loss: 0.629189, acc: 62.50%] [G loss: 2.100570]\n",
      "epoch:9 step:8758 [D loss: 0.647722, acc: 63.28%] [G loss: 2.033062]\n",
      "epoch:9 step:8759 [D loss: 0.639494, acc: 60.94%] [G loss: 2.011200]\n",
      "epoch:9 step:8760 [D loss: 0.614322, acc: 64.06%] [G loss: 2.035408]\n",
      "epoch:9 step:8761 [D loss: 0.679673, acc: 55.47%] [G loss: 2.180896]\n",
      "epoch:9 step:8762 [D loss: 0.588234, acc: 67.19%] [G loss: 2.028930]\n",
      "epoch:9 step:8763 [D loss: 0.627638, acc: 64.84%] [G loss: 2.259694]\n",
      "epoch:9 step:8764 [D loss: 0.595709, acc: 69.53%] [G loss: 2.389744]\n",
      "epoch:9 step:8765 [D loss: 0.619054, acc: 69.53%] [G loss: 2.120328]\n",
      "epoch:9 step:8766 [D loss: 0.607344, acc: 67.19%] [G loss: 2.186288]\n",
      "epoch:9 step:8767 [D loss: 0.597123, acc: 66.41%] [G loss: 2.196470]\n",
      "epoch:9 step:8768 [D loss: 0.588232, acc: 63.28%] [G loss: 2.298621]\n",
      "epoch:9 step:8769 [D loss: 0.613482, acc: 70.31%] [G loss: 2.252240]\n",
      "epoch:9 step:8770 [D loss: 0.617254, acc: 65.62%] [G loss: 2.112431]\n",
      "epoch:9 step:8771 [D loss: 0.586812, acc: 70.31%] [G loss: 2.238713]\n",
      "epoch:9 step:8772 [D loss: 0.720554, acc: 57.03%] [G loss: 2.091854]\n",
      "epoch:9 step:8773 [D loss: 0.643965, acc: 61.72%] [G loss: 2.049534]\n",
      "epoch:9 step:8774 [D loss: 0.665758, acc: 57.81%] [G loss: 1.898013]\n",
      "epoch:9 step:8775 [D loss: 0.708656, acc: 55.47%] [G loss: 1.982308]\n",
      "epoch:9 step:8776 [D loss: 0.687306, acc: 59.38%] [G loss: 2.065731]\n",
      "epoch:9 step:8777 [D loss: 0.634318, acc: 65.62%] [G loss: 2.190371]\n",
      "epoch:9 step:8778 [D loss: 0.551350, acc: 70.31%] [G loss: 2.427574]\n",
      "epoch:9 step:8779 [D loss: 0.574608, acc: 67.97%] [G loss: 2.739385]\n",
      "epoch:9 step:8780 [D loss: 0.611703, acc: 64.06%] [G loss: 2.322519]\n",
      "epoch:9 step:8781 [D loss: 0.611005, acc: 65.62%] [G loss: 2.181289]\n",
      "epoch:9 step:8782 [D loss: 0.695137, acc: 57.81%] [G loss: 1.839213]\n",
      "epoch:9 step:8783 [D loss: 0.611610, acc: 69.53%] [G loss: 2.087370]\n",
      "epoch:9 step:8784 [D loss: 0.618145, acc: 67.97%] [G loss: 2.028053]\n",
      "epoch:9 step:8785 [D loss: 0.628075, acc: 67.19%] [G loss: 2.296890]\n",
      "epoch:9 step:8786 [D loss: 0.539904, acc: 70.31%] [G loss: 2.304384]\n",
      "epoch:9 step:8787 [D loss: 0.605806, acc: 67.19%] [G loss: 2.198315]\n",
      "epoch:9 step:8788 [D loss: 0.631810, acc: 64.84%] [G loss: 1.903914]\n",
      "epoch:9 step:8789 [D loss: 0.644470, acc: 63.28%] [G loss: 1.853092]\n",
      "epoch:9 step:8790 [D loss: 0.642485, acc: 70.31%] [G loss: 2.431816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8791 [D loss: 0.620048, acc: 67.97%] [G loss: 2.255546]\n",
      "epoch:9 step:8792 [D loss: 0.619369, acc: 70.31%] [G loss: 2.353157]\n",
      "epoch:9 step:8793 [D loss: 0.622558, acc: 67.97%] [G loss: 2.146913]\n",
      "epoch:9 step:8794 [D loss: 0.667956, acc: 64.06%] [G loss: 2.138941]\n",
      "epoch:9 step:8795 [D loss: 0.642173, acc: 63.28%] [G loss: 2.034424]\n",
      "epoch:9 step:8796 [D loss: 0.586877, acc: 68.75%] [G loss: 2.259780]\n",
      "epoch:9 step:8797 [D loss: 0.585691, acc: 70.31%] [G loss: 2.056518]\n",
      "epoch:9 step:8798 [D loss: 0.624417, acc: 65.62%] [G loss: 2.197272]\n",
      "epoch:9 step:8799 [D loss: 0.624923, acc: 65.62%] [G loss: 2.231856]\n",
      "epoch:9 step:8800 [D loss: 0.591679, acc: 67.97%] [G loss: 2.497633]\n",
      "epoch:9 step:8801 [D loss: 0.650163, acc: 60.16%] [G loss: 2.049093]\n",
      "epoch:9 step:8802 [D loss: 0.643817, acc: 60.16%] [G loss: 2.236539]\n",
      "epoch:9 step:8803 [D loss: 0.634631, acc: 65.62%] [G loss: 2.166550]\n",
      "epoch:9 step:8804 [D loss: 0.559065, acc: 71.88%] [G loss: 2.472922]\n",
      "epoch:9 step:8805 [D loss: 0.598055, acc: 68.75%] [G loss: 2.143726]\n",
      "epoch:9 step:8806 [D loss: 0.688357, acc: 56.25%] [G loss: 1.966874]\n",
      "epoch:9 step:8807 [D loss: 0.560911, acc: 71.88%] [G loss: 2.203625]\n",
      "epoch:9 step:8808 [D loss: 0.679073, acc: 58.59%] [G loss: 2.097894]\n",
      "epoch:9 step:8809 [D loss: 0.673207, acc: 57.03%] [G loss: 1.988012]\n",
      "epoch:9 step:8810 [D loss: 0.700873, acc: 53.12%] [G loss: 1.951567]\n",
      "epoch:9 step:8811 [D loss: 0.641053, acc: 61.72%] [G loss: 2.181807]\n",
      "epoch:9 step:8812 [D loss: 0.608132, acc: 67.19%] [G loss: 2.247903]\n",
      "epoch:9 step:8813 [D loss: 0.655923, acc: 62.50%] [G loss: 2.227875]\n",
      "epoch:9 step:8814 [D loss: 0.569011, acc: 70.31%] [G loss: 2.105262]\n",
      "epoch:9 step:8815 [D loss: 0.634499, acc: 64.84%] [G loss: 2.124641]\n",
      "epoch:9 step:8816 [D loss: 0.714581, acc: 56.25%] [G loss: 1.988934]\n",
      "epoch:9 step:8817 [D loss: 0.589024, acc: 62.50%] [G loss: 1.947060]\n",
      "epoch:9 step:8818 [D loss: 0.656600, acc: 56.25%] [G loss: 1.900760]\n",
      "epoch:9 step:8819 [D loss: 0.642059, acc: 61.72%] [G loss: 2.046243]\n",
      "epoch:9 step:8820 [D loss: 0.667655, acc: 64.06%] [G loss: 1.972029]\n",
      "epoch:9 step:8821 [D loss: 0.633274, acc: 62.50%] [G loss: 1.979057]\n",
      "epoch:9 step:8822 [D loss: 0.613660, acc: 68.75%] [G loss: 1.964283]\n",
      "epoch:9 step:8823 [D loss: 0.643419, acc: 65.62%] [G loss: 2.015658]\n",
      "epoch:9 step:8824 [D loss: 0.640334, acc: 65.62%] [G loss: 1.830543]\n",
      "epoch:9 step:8825 [D loss: 0.581475, acc: 68.75%] [G loss: 2.114718]\n",
      "epoch:9 step:8826 [D loss: 0.673697, acc: 57.81%] [G loss: 2.100103]\n",
      "epoch:9 step:8827 [D loss: 0.603095, acc: 68.75%] [G loss: 2.147594]\n",
      "epoch:9 step:8828 [D loss: 0.630849, acc: 65.62%] [G loss: 2.433550]\n",
      "epoch:9 step:8829 [D loss: 0.639042, acc: 63.28%] [G loss: 1.800301]\n",
      "epoch:9 step:8830 [D loss: 0.613839, acc: 64.06%] [G loss: 2.256296]\n",
      "epoch:9 step:8831 [D loss: 0.602322, acc: 67.19%] [G loss: 2.180721]\n",
      "epoch:9 step:8832 [D loss: 0.632816, acc: 63.28%] [G loss: 2.235435]\n",
      "epoch:9 step:8833 [D loss: 0.668520, acc: 61.72%] [G loss: 2.331614]\n",
      "epoch:9 step:8834 [D loss: 0.616070, acc: 65.62%] [G loss: 2.189768]\n",
      "epoch:9 step:8835 [D loss: 0.621293, acc: 67.97%] [G loss: 2.084420]\n",
      "epoch:9 step:8836 [D loss: 0.565371, acc: 74.22%] [G loss: 2.045129]\n",
      "epoch:9 step:8837 [D loss: 0.596993, acc: 72.66%] [G loss: 2.201301]\n",
      "epoch:9 step:8838 [D loss: 0.618822, acc: 67.19%] [G loss: 2.188251]\n",
      "epoch:9 step:8839 [D loss: 0.598378, acc: 67.19%] [G loss: 2.366526]\n",
      "epoch:9 step:8840 [D loss: 0.667301, acc: 60.94%] [G loss: 2.236377]\n",
      "epoch:9 step:8841 [D loss: 0.648003, acc: 61.72%] [G loss: 2.195925]\n",
      "epoch:9 step:8842 [D loss: 0.597797, acc: 70.31%] [G loss: 2.025771]\n",
      "epoch:9 step:8843 [D loss: 0.654978, acc: 62.50%] [G loss: 2.072170]\n",
      "epoch:9 step:8844 [D loss: 0.632913, acc: 61.72%] [G loss: 2.080499]\n",
      "epoch:9 step:8845 [D loss: 0.682202, acc: 60.94%] [G loss: 2.025535]\n",
      "epoch:9 step:8846 [D loss: 0.697331, acc: 62.50%] [G loss: 2.088609]\n",
      "epoch:9 step:8847 [D loss: 0.621150, acc: 71.09%] [G loss: 2.355452]\n",
      "epoch:9 step:8848 [D loss: 0.594743, acc: 71.09%] [G loss: 2.265266]\n",
      "epoch:9 step:8849 [D loss: 0.640759, acc: 67.97%] [G loss: 2.220999]\n",
      "epoch:9 step:8850 [D loss: 0.705456, acc: 60.94%] [G loss: 1.947765]\n",
      "epoch:9 step:8851 [D loss: 0.640863, acc: 60.94%] [G loss: 2.024297]\n",
      "epoch:9 step:8852 [D loss: 0.689065, acc: 61.72%] [G loss: 2.033044]\n",
      "epoch:9 step:8853 [D loss: 0.685818, acc: 57.81%] [G loss: 2.062719]\n",
      "epoch:9 step:8854 [D loss: 0.671202, acc: 58.59%] [G loss: 2.080245]\n",
      "epoch:9 step:8855 [D loss: 0.679172, acc: 59.38%] [G loss: 1.990839]\n",
      "epoch:9 step:8856 [D loss: 0.691156, acc: 52.34%] [G loss: 1.928891]\n",
      "epoch:9 step:8857 [D loss: 0.648423, acc: 66.41%] [G loss: 2.003300]\n",
      "epoch:9 step:8858 [D loss: 0.629022, acc: 67.97%] [G loss: 2.102686]\n",
      "epoch:9 step:8859 [D loss: 0.673843, acc: 66.41%] [G loss: 1.922884]\n",
      "epoch:9 step:8860 [D loss: 0.541674, acc: 76.56%] [G loss: 2.138621]\n",
      "epoch:9 step:8861 [D loss: 0.562248, acc: 72.66%] [G loss: 2.388974]\n",
      "epoch:9 step:8862 [D loss: 0.549302, acc: 69.53%] [G loss: 2.384110]\n",
      "epoch:9 step:8863 [D loss: 0.563947, acc: 67.97%] [G loss: 2.518174]\n",
      "epoch:9 step:8864 [D loss: 0.682667, acc: 60.16%] [G loss: 2.047104]\n",
      "epoch:9 step:8865 [D loss: 0.685944, acc: 57.03%] [G loss: 1.991131]\n",
      "epoch:9 step:8866 [D loss: 0.581219, acc: 73.44%] [G loss: 2.211439]\n",
      "epoch:9 step:8867 [D loss: 0.600182, acc: 65.62%] [G loss: 2.067137]\n",
      "epoch:9 step:8868 [D loss: 0.651971, acc: 61.72%] [G loss: 2.138007]\n",
      "epoch:9 step:8869 [D loss: 0.642348, acc: 60.94%] [G loss: 2.026828]\n",
      "epoch:9 step:8870 [D loss: 0.662476, acc: 57.81%] [G loss: 2.003800]\n",
      "epoch:9 step:8871 [D loss: 0.643433, acc: 60.94%] [G loss: 2.036361]\n",
      "epoch:9 step:8872 [D loss: 0.704521, acc: 58.59%] [G loss: 1.968145]\n",
      "epoch:9 step:8873 [D loss: 0.694612, acc: 56.25%] [G loss: 1.962700]\n",
      "epoch:9 step:8874 [D loss: 0.630358, acc: 68.75%] [G loss: 1.911838]\n",
      "epoch:9 step:8875 [D loss: 0.659422, acc: 60.16%] [G loss: 2.033805]\n",
      "epoch:9 step:8876 [D loss: 0.673403, acc: 59.38%] [G loss: 2.048176]\n",
      "epoch:9 step:8877 [D loss: 0.680313, acc: 61.72%] [G loss: 1.875734]\n",
      "epoch:9 step:8878 [D loss: 0.642596, acc: 66.41%] [G loss: 1.908991]\n",
      "epoch:9 step:8879 [D loss: 0.630706, acc: 66.41%] [G loss: 2.031256]\n",
      "epoch:9 step:8880 [D loss: 0.577597, acc: 72.66%] [G loss: 2.092032]\n",
      "epoch:9 step:8881 [D loss: 0.701488, acc: 55.47%] [G loss: 1.944262]\n",
      "epoch:9 step:8882 [D loss: 0.634203, acc: 60.94%] [G loss: 2.210806]\n",
      "epoch:9 step:8883 [D loss: 0.637950, acc: 61.72%] [G loss: 1.952967]\n",
      "epoch:9 step:8884 [D loss: 0.595890, acc: 67.97%] [G loss: 2.107103]\n",
      "epoch:9 step:8885 [D loss: 0.621525, acc: 67.19%] [G loss: 2.035784]\n",
      "epoch:9 step:8886 [D loss: 0.619753, acc: 61.72%] [G loss: 2.033177]\n",
      "epoch:9 step:8887 [D loss: 0.638813, acc: 62.50%] [G loss: 2.055327]\n",
      "epoch:9 step:8888 [D loss: 0.673191, acc: 59.38%] [G loss: 1.947965]\n",
      "epoch:9 step:8889 [D loss: 0.605662, acc: 64.06%] [G loss: 2.091986]\n",
      "epoch:9 step:8890 [D loss: 0.669194, acc: 60.16%] [G loss: 1.986983]\n",
      "epoch:9 step:8891 [D loss: 0.644487, acc: 58.59%] [G loss: 1.901037]\n",
      "epoch:9 step:8892 [D loss: 0.601493, acc: 65.62%] [G loss: 1.997213]\n",
      "epoch:9 step:8893 [D loss: 0.757227, acc: 57.81%] [G loss: 1.872342]\n",
      "epoch:9 step:8894 [D loss: 0.624368, acc: 64.06%] [G loss: 2.004583]\n",
      "epoch:9 step:8895 [D loss: 0.597459, acc: 70.31%] [G loss: 1.970253]\n",
      "epoch:9 step:8896 [D loss: 0.625456, acc: 66.41%] [G loss: 1.908766]\n",
      "epoch:9 step:8897 [D loss: 0.615046, acc: 65.62%] [G loss: 1.878577]\n",
      "epoch:9 step:8898 [D loss: 0.612594, acc: 67.97%] [G loss: 2.036475]\n",
      "epoch:9 step:8899 [D loss: 0.655220, acc: 63.28%] [G loss: 1.913848]\n",
      "epoch:9 step:8900 [D loss: 0.614261, acc: 69.53%] [G loss: 1.909445]\n",
      "epoch:9 step:8901 [D loss: 0.654706, acc: 57.03%] [G loss: 2.331693]\n",
      "epoch:9 step:8902 [D loss: 0.597901, acc: 67.97%] [G loss: 2.204423]\n",
      "epoch:9 step:8903 [D loss: 0.623462, acc: 61.72%] [G loss: 2.337329]\n",
      "epoch:9 step:8904 [D loss: 0.597160, acc: 70.31%] [G loss: 2.675822]\n",
      "epoch:9 step:8905 [D loss: 0.604033, acc: 67.19%] [G loss: 2.571251]\n",
      "epoch:9 step:8906 [D loss: 0.648210, acc: 58.59%] [G loss: 2.117252]\n",
      "epoch:9 step:8907 [D loss: 0.578124, acc: 69.53%] [G loss: 2.143106]\n",
      "epoch:9 step:8908 [D loss: 0.615848, acc: 64.06%] [G loss: 2.214483]\n",
      "epoch:9 step:8909 [D loss: 0.532843, acc: 71.09%] [G loss: 2.039876]\n",
      "epoch:9 step:8910 [D loss: 0.685158, acc: 56.25%] [G loss: 1.816223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:8911 [D loss: 0.664882, acc: 55.47%] [G loss: 1.916105]\n",
      "epoch:9 step:8912 [D loss: 0.580654, acc: 69.53%] [G loss: 2.314203]\n",
      "epoch:9 step:8913 [D loss: 0.637800, acc: 61.72%] [G loss: 2.128737]\n",
      "epoch:9 step:8914 [D loss: 0.529916, acc: 75.78%] [G loss: 2.476359]\n",
      "epoch:9 step:8915 [D loss: 0.702885, acc: 55.47%] [G loss: 1.933362]\n",
      "epoch:9 step:8916 [D loss: 0.645682, acc: 64.06%] [G loss: 2.154209]\n",
      "epoch:9 step:8917 [D loss: 0.603848, acc: 67.97%] [G loss: 2.180057]\n",
      "epoch:9 step:8918 [D loss: 0.615524, acc: 64.84%] [G loss: 2.018552]\n",
      "epoch:9 step:8919 [D loss: 0.637968, acc: 66.41%] [G loss: 2.177442]\n",
      "epoch:9 step:8920 [D loss: 0.662546, acc: 64.06%] [G loss: 2.193068]\n",
      "epoch:9 step:8921 [D loss: 0.586073, acc: 69.53%] [G loss: 2.494189]\n",
      "epoch:9 step:8922 [D loss: 0.635781, acc: 64.84%] [G loss: 2.199058]\n",
      "epoch:9 step:8923 [D loss: 0.676993, acc: 60.94%] [G loss: 2.104843]\n",
      "epoch:9 step:8924 [D loss: 0.646823, acc: 63.28%] [G loss: 2.191187]\n",
      "epoch:9 step:8925 [D loss: 0.628839, acc: 64.84%] [G loss: 2.104528]\n",
      "epoch:9 step:8926 [D loss: 0.639304, acc: 62.50%] [G loss: 2.091336]\n",
      "epoch:9 step:8927 [D loss: 0.588356, acc: 69.53%] [G loss: 2.189851]\n",
      "epoch:9 step:8928 [D loss: 0.625362, acc: 64.06%] [G loss: 2.255182]\n",
      "epoch:9 step:8929 [D loss: 0.637149, acc: 69.53%] [G loss: 2.082050]\n",
      "epoch:9 step:8930 [D loss: 0.572195, acc: 69.53%] [G loss: 2.283087]\n",
      "epoch:9 step:8931 [D loss: 0.588112, acc: 68.75%] [G loss: 2.526623]\n",
      "epoch:9 step:8932 [D loss: 0.553023, acc: 75.00%] [G loss: 2.468178]\n",
      "epoch:9 step:8933 [D loss: 0.696600, acc: 58.59%] [G loss: 1.927827]\n",
      "epoch:9 step:8934 [D loss: 0.680859, acc: 57.81%] [G loss: 2.070377]\n",
      "epoch:9 step:8935 [D loss: 0.637488, acc: 62.50%] [G loss: 1.967275]\n",
      "epoch:9 step:8936 [D loss: 0.667504, acc: 67.19%] [G loss: 2.185706]\n",
      "epoch:9 step:8937 [D loss: 0.622694, acc: 64.84%] [G loss: 2.456545]\n",
      "epoch:9 step:8938 [D loss: 0.637354, acc: 64.84%] [G loss: 2.255962]\n",
      "epoch:9 step:8939 [D loss: 0.693538, acc: 57.03%] [G loss: 2.099022]\n",
      "epoch:9 step:8940 [D loss: 0.630319, acc: 60.16%] [G loss: 1.936751]\n",
      "epoch:9 step:8941 [D loss: 0.600010, acc: 66.41%] [G loss: 2.115670]\n",
      "epoch:9 step:8942 [D loss: 0.603022, acc: 70.31%] [G loss: 2.123805]\n",
      "epoch:9 step:8943 [D loss: 0.601635, acc: 66.41%] [G loss: 1.976844]\n",
      "epoch:9 step:8944 [D loss: 0.639452, acc: 60.94%] [G loss: 2.044425]\n",
      "epoch:9 step:8945 [D loss: 0.637235, acc: 66.41%] [G loss: 2.087661]\n",
      "epoch:9 step:8946 [D loss: 0.646189, acc: 61.72%] [G loss: 1.928819]\n",
      "epoch:9 step:8947 [D loss: 0.664773, acc: 57.81%] [G loss: 1.915959]\n",
      "epoch:9 step:8948 [D loss: 0.588473, acc: 68.75%] [G loss: 2.220562]\n",
      "epoch:9 step:8949 [D loss: 0.547251, acc: 71.88%] [G loss: 2.217586]\n",
      "epoch:9 step:8950 [D loss: 0.633409, acc: 64.84%] [G loss: 2.277850]\n",
      "epoch:9 step:8951 [D loss: 0.662339, acc: 57.81%] [G loss: 2.151198]\n",
      "epoch:9 step:8952 [D loss: 0.593336, acc: 69.53%] [G loss: 2.267964]\n",
      "epoch:9 step:8953 [D loss: 0.608657, acc: 64.06%] [G loss: 2.238170]\n",
      "epoch:9 step:8954 [D loss: 0.551411, acc: 72.66%] [G loss: 2.243716]\n",
      "epoch:9 step:8955 [D loss: 0.548309, acc: 74.22%] [G loss: 2.378722]\n",
      "epoch:9 step:8956 [D loss: 0.578741, acc: 72.66%] [G loss: 2.433619]\n",
      "epoch:9 step:8957 [D loss: 0.647502, acc: 64.84%] [G loss: 2.256658]\n",
      "epoch:9 step:8958 [D loss: 0.677886, acc: 64.06%] [G loss: 2.030324]\n",
      "epoch:9 step:8959 [D loss: 0.624329, acc: 70.31%] [G loss: 2.000994]\n",
      "epoch:9 step:8960 [D loss: 0.688919, acc: 57.81%] [G loss: 2.225641]\n",
      "epoch:9 step:8961 [D loss: 0.651647, acc: 60.16%] [G loss: 1.866622]\n",
      "epoch:9 step:8962 [D loss: 0.662501, acc: 62.50%] [G loss: 1.993419]\n",
      "epoch:9 step:8963 [D loss: 0.608265, acc: 66.41%] [G loss: 2.135476]\n",
      "epoch:9 step:8964 [D loss: 0.637405, acc: 62.50%] [G loss: 2.142605]\n",
      "epoch:9 step:8965 [D loss: 0.645253, acc: 62.50%] [G loss: 2.331268]\n",
      "epoch:9 step:8966 [D loss: 0.669227, acc: 57.81%] [G loss: 2.116981]\n",
      "epoch:9 step:8967 [D loss: 0.610811, acc: 66.41%] [G loss: 2.105935]\n",
      "epoch:9 step:8968 [D loss: 0.688810, acc: 55.47%] [G loss: 2.010172]\n",
      "epoch:9 step:8969 [D loss: 0.654214, acc: 62.50%] [G loss: 2.223517]\n",
      "epoch:9 step:8970 [D loss: 0.641349, acc: 65.62%] [G loss: 2.133715]\n",
      "epoch:9 step:8971 [D loss: 0.652128, acc: 64.84%] [G loss: 1.945190]\n",
      "epoch:9 step:8972 [D loss: 0.657209, acc: 57.81%] [G loss: 1.985678]\n",
      "epoch:9 step:8973 [D loss: 0.587419, acc: 70.31%] [G loss: 2.115176]\n",
      "epoch:9 step:8974 [D loss: 0.606416, acc: 68.75%] [G loss: 2.028186]\n",
      "epoch:9 step:8975 [D loss: 0.685693, acc: 65.62%] [G loss: 1.958165]\n",
      "epoch:9 step:8976 [D loss: 0.641872, acc: 60.94%] [G loss: 1.855543]\n",
      "epoch:9 step:8977 [D loss: 0.613014, acc: 67.19%] [G loss: 1.957895]\n",
      "epoch:9 step:8978 [D loss: 0.654786, acc: 63.28%] [G loss: 2.175096]\n",
      "epoch:9 step:8979 [D loss: 0.575393, acc: 73.44%] [G loss: 2.084509]\n",
      "epoch:9 step:8980 [D loss: 0.641441, acc: 64.84%] [G loss: 2.050048]\n",
      "epoch:9 step:8981 [D loss: 0.576552, acc: 71.88%] [G loss: 2.400040]\n",
      "epoch:9 step:8982 [D loss: 0.564337, acc: 70.31%] [G loss: 2.258461]\n",
      "epoch:9 step:8983 [D loss: 0.628526, acc: 64.06%] [G loss: 2.135209]\n",
      "epoch:9 step:8984 [D loss: 0.586101, acc: 69.53%] [G loss: 2.222571]\n",
      "epoch:9 step:8985 [D loss: 0.648467, acc: 67.19%] [G loss: 2.164962]\n",
      "epoch:9 step:8986 [D loss: 0.643643, acc: 61.72%] [G loss: 2.089231]\n",
      "epoch:9 step:8987 [D loss: 0.578803, acc: 66.41%] [G loss: 2.405123]\n",
      "epoch:9 step:8988 [D loss: 0.552756, acc: 71.09%] [G loss: 2.041364]\n",
      "epoch:9 step:8989 [D loss: 0.591366, acc: 66.41%] [G loss: 2.116519]\n",
      "epoch:9 step:8990 [D loss: 0.581781, acc: 71.88%] [G loss: 2.383053]\n",
      "epoch:9 step:8991 [D loss: 0.593504, acc: 70.31%] [G loss: 2.289629]\n",
      "epoch:9 step:8992 [D loss: 0.706349, acc: 57.81%] [G loss: 2.010929]\n",
      "epoch:9 step:8993 [D loss: 0.604081, acc: 64.06%] [G loss: 1.910536]\n",
      "epoch:9 step:8994 [D loss: 0.619029, acc: 67.19%] [G loss: 2.244449]\n",
      "epoch:9 step:8995 [D loss: 0.634250, acc: 62.50%] [G loss: 2.034081]\n",
      "epoch:9 step:8996 [D loss: 0.675907, acc: 63.28%] [G loss: 2.014879]\n",
      "epoch:9 step:8997 [D loss: 0.608736, acc: 68.75%] [G loss: 2.256750]\n",
      "epoch:9 step:8998 [D loss: 0.674424, acc: 61.72%] [G loss: 2.025698]\n",
      "epoch:9 step:8999 [D loss: 0.671746, acc: 58.59%] [G loss: 1.915691]\n",
      "epoch:9 step:9000 [D loss: 0.674536, acc: 60.94%] [G loss: 1.972201]\n",
      "epoch:9 step:9001 [D loss: 0.587585, acc: 67.97%] [G loss: 1.919404]\n",
      "epoch:9 step:9002 [D loss: 0.565534, acc: 69.53%] [G loss: 2.143493]\n",
      "epoch:9 step:9003 [D loss: 0.598226, acc: 61.72%] [G loss: 2.131715]\n",
      "epoch:9 step:9004 [D loss: 0.619169, acc: 66.41%] [G loss: 2.144871]\n",
      "epoch:9 step:9005 [D loss: 0.727272, acc: 52.34%] [G loss: 1.953503]\n",
      "epoch:9 step:9006 [D loss: 0.653814, acc: 62.50%] [G loss: 1.994917]\n",
      "epoch:9 step:9007 [D loss: 0.628563, acc: 66.41%] [G loss: 2.017829]\n",
      "epoch:9 step:9008 [D loss: 0.596942, acc: 74.22%] [G loss: 2.152838]\n",
      "epoch:9 step:9009 [D loss: 0.602235, acc: 68.75%] [G loss: 1.890507]\n",
      "epoch:9 step:9010 [D loss: 0.688935, acc: 54.69%] [G loss: 1.869925]\n",
      "epoch:9 step:9011 [D loss: 0.586657, acc: 64.84%] [G loss: 1.942346]\n",
      "epoch:9 step:9012 [D loss: 0.644095, acc: 63.28%] [G loss: 2.071123]\n",
      "epoch:9 step:9013 [D loss: 0.637158, acc: 64.84%] [G loss: 1.985225]\n",
      "epoch:9 step:9014 [D loss: 0.632066, acc: 64.84%] [G loss: 1.977747]\n",
      "epoch:9 step:9015 [D loss: 0.656322, acc: 57.81%] [G loss: 2.112588]\n",
      "epoch:9 step:9016 [D loss: 0.698518, acc: 53.91%] [G loss: 2.026116]\n",
      "epoch:9 step:9017 [D loss: 0.637007, acc: 60.94%] [G loss: 1.947690]\n",
      "epoch:9 step:9018 [D loss: 0.631977, acc: 60.16%] [G loss: 1.948001]\n",
      "epoch:9 step:9019 [D loss: 0.623997, acc: 65.62%] [G loss: 2.091743]\n",
      "epoch:9 step:9020 [D loss: 0.652651, acc: 64.06%] [G loss: 1.902529]\n",
      "epoch:9 step:9021 [D loss: 0.567310, acc: 71.88%] [G loss: 2.130996]\n",
      "epoch:9 step:9022 [D loss: 0.703497, acc: 50.78%] [G loss: 1.935339]\n",
      "epoch:9 step:9023 [D loss: 0.640502, acc: 63.28%] [G loss: 2.085014]\n",
      "epoch:9 step:9024 [D loss: 0.596468, acc: 69.53%] [G loss: 2.161989]\n",
      "epoch:9 step:9025 [D loss: 0.656463, acc: 61.72%] [G loss: 1.984667]\n",
      "epoch:9 step:9026 [D loss: 0.689020, acc: 57.81%] [G loss: 1.951328]\n",
      "epoch:9 step:9027 [D loss: 0.624135, acc: 64.84%] [G loss: 1.887226]\n",
      "epoch:9 step:9028 [D loss: 0.674849, acc: 57.03%] [G loss: 2.061275]\n",
      "epoch:9 step:9029 [D loss: 0.642771, acc: 64.06%] [G loss: 1.902016]\n",
      "epoch:9 step:9030 [D loss: 0.643611, acc: 64.06%] [G loss: 2.052627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9031 [D loss: 0.613857, acc: 71.09%] [G loss: 1.974603]\n",
      "epoch:9 step:9032 [D loss: 0.643778, acc: 62.50%] [G loss: 1.863464]\n",
      "epoch:9 step:9033 [D loss: 0.628886, acc: 64.84%] [G loss: 2.083700]\n",
      "epoch:9 step:9034 [D loss: 0.689481, acc: 67.19%] [G loss: 2.136581]\n",
      "epoch:9 step:9035 [D loss: 0.650245, acc: 58.59%] [G loss: 2.252594]\n",
      "epoch:9 step:9036 [D loss: 0.568087, acc: 70.31%] [G loss: 2.237025]\n",
      "epoch:9 step:9037 [D loss: 0.621140, acc: 67.19%] [G loss: 2.113374]\n",
      "epoch:9 step:9038 [D loss: 0.604372, acc: 68.75%] [G loss: 2.118348]\n",
      "epoch:9 step:9039 [D loss: 0.662745, acc: 63.28%] [G loss: 1.980097]\n",
      "epoch:9 step:9040 [D loss: 0.576163, acc: 68.75%] [G loss: 2.043087]\n",
      "epoch:9 step:9041 [D loss: 0.610776, acc: 67.19%] [G loss: 2.039303]\n",
      "epoch:9 step:9042 [D loss: 0.605551, acc: 68.75%] [G loss: 2.137156]\n",
      "epoch:9 step:9043 [D loss: 0.628135, acc: 64.06%] [G loss: 2.016200]\n",
      "epoch:9 step:9044 [D loss: 0.680228, acc: 60.16%] [G loss: 1.826701]\n",
      "epoch:9 step:9045 [D loss: 0.675373, acc: 59.38%] [G loss: 1.997338]\n",
      "epoch:9 step:9046 [D loss: 0.562355, acc: 73.44%] [G loss: 2.158887]\n",
      "epoch:9 step:9047 [D loss: 0.643436, acc: 61.72%] [G loss: 1.980805]\n",
      "epoch:9 step:9048 [D loss: 0.647606, acc: 60.94%] [G loss: 1.903693]\n",
      "epoch:9 step:9049 [D loss: 0.650437, acc: 58.59%] [G loss: 1.993218]\n",
      "epoch:9 step:9050 [D loss: 0.661421, acc: 61.72%] [G loss: 1.947592]\n",
      "epoch:9 step:9051 [D loss: 0.627574, acc: 67.97%] [G loss: 1.926119]\n",
      "epoch:9 step:9052 [D loss: 0.672421, acc: 61.72%] [G loss: 1.954575]\n",
      "epoch:9 step:9053 [D loss: 0.628171, acc: 61.72%] [G loss: 1.946273]\n",
      "epoch:9 step:9054 [D loss: 0.656930, acc: 64.06%] [G loss: 2.005528]\n",
      "epoch:9 step:9055 [D loss: 0.702284, acc: 57.03%] [G loss: 1.967141]\n",
      "epoch:9 step:9056 [D loss: 0.663863, acc: 63.28%] [G loss: 2.096854]\n",
      "epoch:9 step:9057 [D loss: 0.654496, acc: 58.59%] [G loss: 2.040069]\n",
      "epoch:9 step:9058 [D loss: 0.694082, acc: 62.50%] [G loss: 1.926862]\n",
      "epoch:9 step:9059 [D loss: 0.649667, acc: 68.75%] [G loss: 2.032247]\n",
      "epoch:9 step:9060 [D loss: 0.720014, acc: 53.12%] [G loss: 1.897491]\n",
      "epoch:9 step:9061 [D loss: 0.673026, acc: 65.62%] [G loss: 1.698857]\n",
      "epoch:9 step:9062 [D loss: 0.596947, acc: 65.62%] [G loss: 1.947545]\n",
      "epoch:9 step:9063 [D loss: 0.548005, acc: 69.53%] [G loss: 2.073769]\n",
      "epoch:9 step:9064 [D loss: 0.625359, acc: 65.62%] [G loss: 2.033181]\n",
      "epoch:9 step:9065 [D loss: 0.597182, acc: 66.41%] [G loss: 2.093929]\n",
      "epoch:9 step:9066 [D loss: 0.638390, acc: 64.06%] [G loss: 2.109847]\n",
      "epoch:9 step:9067 [D loss: 0.573478, acc: 68.75%] [G loss: 2.109411]\n",
      "epoch:9 step:9068 [D loss: 0.609536, acc: 71.09%] [G loss: 2.083730]\n",
      "epoch:9 step:9069 [D loss: 0.681275, acc: 59.38%] [G loss: 2.083535]\n",
      "epoch:9 step:9070 [D loss: 0.602966, acc: 67.19%] [G loss: 2.304733]\n",
      "epoch:9 step:9071 [D loss: 0.616582, acc: 64.06%] [G loss: 2.112956]\n",
      "epoch:9 step:9072 [D loss: 0.618412, acc: 66.41%] [G loss: 2.333005]\n",
      "epoch:9 step:9073 [D loss: 0.618799, acc: 62.50%] [G loss: 2.331105]\n",
      "epoch:9 step:9074 [D loss: 0.572268, acc: 68.75%] [G loss: 2.054123]\n",
      "epoch:9 step:9075 [D loss: 0.538169, acc: 75.00%] [G loss: 2.325885]\n",
      "epoch:9 step:9076 [D loss: 0.639027, acc: 60.16%] [G loss: 2.033817]\n",
      "epoch:9 step:9077 [D loss: 0.624863, acc: 64.06%] [G loss: 2.353810]\n",
      "epoch:9 step:9078 [D loss: 0.609685, acc: 64.84%] [G loss: 2.195134]\n",
      "epoch:9 step:9079 [D loss: 0.641243, acc: 64.84%] [G loss: 2.260074]\n",
      "epoch:9 step:9080 [D loss: 0.585766, acc: 68.75%] [G loss: 2.553617]\n",
      "epoch:9 step:9081 [D loss: 0.585487, acc: 69.53%] [G loss: 2.440797]\n",
      "epoch:9 step:9082 [D loss: 0.591779, acc: 64.06%] [G loss: 2.305346]\n",
      "epoch:9 step:9083 [D loss: 0.600932, acc: 67.19%] [G loss: 2.240985]\n",
      "epoch:9 step:9084 [D loss: 0.595395, acc: 70.31%] [G loss: 2.108886]\n",
      "epoch:9 step:9085 [D loss: 0.637386, acc: 68.75%] [G loss: 2.096344]\n",
      "epoch:9 step:9086 [D loss: 0.604151, acc: 66.41%] [G loss: 2.205548]\n",
      "epoch:9 step:9087 [D loss: 0.625516, acc: 63.28%] [G loss: 2.178546]\n",
      "epoch:9 step:9088 [D loss: 0.622566, acc: 63.28%] [G loss: 2.088660]\n",
      "epoch:9 step:9089 [D loss: 0.624813, acc: 63.28%] [G loss: 2.172441]\n",
      "epoch:9 step:9090 [D loss: 0.705769, acc: 56.25%] [G loss: 1.925430]\n",
      "epoch:9 step:9091 [D loss: 0.715721, acc: 56.25%] [G loss: 1.928861]\n",
      "epoch:9 step:9092 [D loss: 0.683034, acc: 65.62%] [G loss: 1.898471]\n",
      "epoch:9 step:9093 [D loss: 0.654001, acc: 59.38%] [G loss: 2.066016]\n",
      "epoch:9 step:9094 [D loss: 0.678889, acc: 61.72%] [G loss: 2.021669]\n",
      "epoch:9 step:9095 [D loss: 0.599566, acc: 72.66%] [G loss: 2.268616]\n",
      "epoch:9 step:9096 [D loss: 0.671372, acc: 62.50%] [G loss: 2.119915]\n",
      "epoch:9 step:9097 [D loss: 0.629881, acc: 63.28%] [G loss: 2.006298]\n",
      "epoch:9 step:9098 [D loss: 0.679436, acc: 56.25%] [G loss: 1.975227]\n",
      "epoch:9 step:9099 [D loss: 0.710558, acc: 57.03%] [G loss: 1.905546]\n",
      "epoch:9 step:9100 [D loss: 0.689292, acc: 56.25%] [G loss: 1.928824]\n",
      "epoch:9 step:9101 [D loss: 0.631872, acc: 61.72%] [G loss: 2.025750]\n",
      "epoch:9 step:9102 [D loss: 0.683176, acc: 57.03%] [G loss: 1.864918]\n",
      "epoch:9 step:9103 [D loss: 0.687465, acc: 65.62%] [G loss: 1.911354]\n",
      "epoch:9 step:9104 [D loss: 0.623827, acc: 64.06%] [G loss: 1.930836]\n",
      "epoch:9 step:9105 [D loss: 0.615128, acc: 67.19%] [G loss: 2.051753]\n",
      "epoch:9 step:9106 [D loss: 0.595719, acc: 66.41%] [G loss: 2.019322]\n",
      "epoch:9 step:9107 [D loss: 0.666379, acc: 61.72%] [G loss: 1.874190]\n",
      "epoch:9 step:9108 [D loss: 0.632798, acc: 64.06%] [G loss: 1.960101]\n",
      "epoch:9 step:9109 [D loss: 0.580361, acc: 71.09%] [G loss: 1.902905]\n",
      "epoch:9 step:9110 [D loss: 0.598042, acc: 70.31%] [G loss: 2.169125]\n",
      "epoch:9 step:9111 [D loss: 0.570993, acc: 72.66%] [G loss: 2.079645]\n",
      "epoch:9 step:9112 [D loss: 0.678134, acc: 61.72%] [G loss: 2.075563]\n",
      "epoch:9 step:9113 [D loss: 0.573683, acc: 69.53%] [G loss: 2.086835]\n",
      "epoch:9 step:9114 [D loss: 0.659763, acc: 64.06%] [G loss: 2.226676]\n",
      "epoch:9 step:9115 [D loss: 0.592830, acc: 67.19%] [G loss: 2.009644]\n",
      "epoch:9 step:9116 [D loss: 0.663506, acc: 63.28%] [G loss: 2.045537]\n",
      "epoch:9 step:9117 [D loss: 0.572191, acc: 72.66%] [G loss: 2.065042]\n",
      "epoch:9 step:9118 [D loss: 0.684568, acc: 60.94%] [G loss: 1.974228]\n",
      "epoch:9 step:9119 [D loss: 0.553145, acc: 74.22%] [G loss: 2.039047]\n",
      "epoch:9 step:9120 [D loss: 0.668704, acc: 62.50%] [G loss: 2.144676]\n",
      "epoch:9 step:9121 [D loss: 0.636458, acc: 61.72%] [G loss: 2.083062]\n",
      "epoch:9 step:9122 [D loss: 0.614969, acc: 64.84%] [G loss: 2.205244]\n",
      "epoch:9 step:9123 [D loss: 0.618936, acc: 64.84%] [G loss: 2.184744]\n",
      "epoch:9 step:9124 [D loss: 0.615339, acc: 67.19%] [G loss: 2.207530]\n",
      "epoch:9 step:9125 [D loss: 0.588599, acc: 72.66%] [G loss: 2.357797]\n",
      "epoch:9 step:9126 [D loss: 0.634893, acc: 64.06%] [G loss: 2.133597]\n",
      "epoch:9 step:9127 [D loss: 0.590616, acc: 71.09%] [G loss: 2.509651]\n",
      "epoch:9 step:9128 [D loss: 0.604708, acc: 65.62%] [G loss: 2.281738]\n",
      "epoch:9 step:9129 [D loss: 0.713856, acc: 53.91%] [G loss: 1.880044]\n",
      "epoch:9 step:9130 [D loss: 0.621150, acc: 66.41%] [G loss: 2.083516]\n",
      "epoch:9 step:9131 [D loss: 0.625013, acc: 64.06%] [G loss: 2.029232]\n",
      "epoch:9 step:9132 [D loss: 0.659932, acc: 57.81%] [G loss: 2.227535]\n",
      "epoch:9 step:9133 [D loss: 0.651901, acc: 62.50%] [G loss: 2.119360]\n",
      "epoch:9 step:9134 [D loss: 0.639230, acc: 62.50%] [G loss: 2.118043]\n",
      "epoch:9 step:9135 [D loss: 0.660605, acc: 60.16%] [G loss: 1.793261]\n",
      "epoch:9 step:9136 [D loss: 0.662223, acc: 59.38%] [G loss: 2.075122]\n",
      "epoch:9 step:9137 [D loss: 0.641351, acc: 60.94%] [G loss: 1.822493]\n",
      "epoch:9 step:9138 [D loss: 0.627906, acc: 65.62%] [G loss: 2.024770]\n",
      "epoch:9 step:9139 [D loss: 0.595235, acc: 68.75%] [G loss: 2.139000]\n",
      "epoch:9 step:9140 [D loss: 0.592130, acc: 68.75%] [G loss: 2.388782]\n",
      "epoch:9 step:9141 [D loss: 0.600811, acc: 67.19%] [G loss: 2.237885]\n",
      "epoch:9 step:9142 [D loss: 0.546191, acc: 74.22%] [G loss: 2.289307]\n",
      "epoch:9 step:9143 [D loss: 0.720431, acc: 57.03%] [G loss: 1.856322]\n",
      "epoch:9 step:9144 [D loss: 0.617550, acc: 60.94%] [G loss: 2.075993]\n",
      "epoch:9 step:9145 [D loss: 0.616517, acc: 67.97%] [G loss: 2.175257]\n",
      "epoch:9 step:9146 [D loss: 0.596613, acc: 68.75%] [G loss: 1.928009]\n",
      "epoch:9 step:9147 [D loss: 0.588632, acc: 67.19%] [G loss: 1.902340]\n",
      "epoch:9 step:9148 [D loss: 0.667469, acc: 60.94%] [G loss: 2.238889]\n",
      "epoch:9 step:9149 [D loss: 0.616875, acc: 65.62%] [G loss: 1.986958]\n",
      "epoch:9 step:9150 [D loss: 0.635602, acc: 62.50%] [G loss: 1.940357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9151 [D loss: 0.599866, acc: 65.62%] [G loss: 2.174239]\n",
      "epoch:9 step:9152 [D loss: 0.558011, acc: 75.78%] [G loss: 2.118203]\n",
      "epoch:9 step:9153 [D loss: 0.589641, acc: 71.09%] [G loss: 2.089098]\n",
      "epoch:9 step:9154 [D loss: 0.661037, acc: 62.50%] [G loss: 2.289694]\n",
      "epoch:9 step:9155 [D loss: 0.654766, acc: 60.94%] [G loss: 2.201382]\n",
      "epoch:9 step:9156 [D loss: 0.662605, acc: 65.62%] [G loss: 2.083740]\n",
      "epoch:9 step:9157 [D loss: 0.636237, acc: 64.84%] [G loss: 2.097854]\n",
      "epoch:9 step:9158 [D loss: 0.634639, acc: 65.62%] [G loss: 2.284799]\n",
      "epoch:9 step:9159 [D loss: 0.687666, acc: 57.81%] [G loss: 2.162566]\n",
      "epoch:9 step:9160 [D loss: 0.731573, acc: 57.03%] [G loss: 1.963542]\n",
      "epoch:9 step:9161 [D loss: 0.631950, acc: 63.28%] [G loss: 2.238155]\n",
      "epoch:9 step:9162 [D loss: 0.620746, acc: 64.84%] [G loss: 2.020884]\n",
      "epoch:9 step:9163 [D loss: 0.691792, acc: 58.59%] [G loss: 1.953122]\n",
      "epoch:9 step:9164 [D loss: 0.646090, acc: 60.16%] [G loss: 1.976249]\n",
      "epoch:9 step:9165 [D loss: 0.659420, acc: 67.19%] [G loss: 1.972727]\n",
      "epoch:9 step:9166 [D loss: 0.678315, acc: 58.59%] [G loss: 2.068531]\n",
      "epoch:9 step:9167 [D loss: 0.631868, acc: 62.50%] [G loss: 2.006207]\n",
      "epoch:9 step:9168 [D loss: 0.612889, acc: 63.28%] [G loss: 2.069240]\n",
      "epoch:9 step:9169 [D loss: 0.575804, acc: 73.44%] [G loss: 2.020341]\n",
      "epoch:9 step:9170 [D loss: 0.619610, acc: 62.50%] [G loss: 2.158553]\n",
      "epoch:9 step:9171 [D loss: 0.596924, acc: 64.84%] [G loss: 2.087106]\n",
      "epoch:9 step:9172 [D loss: 0.709330, acc: 60.16%] [G loss: 1.852959]\n",
      "epoch:9 step:9173 [D loss: 0.610776, acc: 69.53%] [G loss: 1.934210]\n",
      "epoch:9 step:9174 [D loss: 0.637195, acc: 64.06%] [G loss: 1.937496]\n",
      "epoch:9 step:9175 [D loss: 0.677463, acc: 57.81%] [G loss: 1.989392]\n",
      "epoch:9 step:9176 [D loss: 0.614850, acc: 65.62%] [G loss: 2.076040]\n",
      "epoch:9 step:9177 [D loss: 0.712610, acc: 57.81%] [G loss: 2.047294]\n",
      "epoch:9 step:9178 [D loss: 0.681598, acc: 55.47%] [G loss: 1.923738]\n",
      "epoch:9 step:9179 [D loss: 0.646230, acc: 62.50%] [G loss: 2.055047]\n",
      "epoch:9 step:9180 [D loss: 0.618500, acc: 61.72%] [G loss: 1.999968]\n",
      "epoch:9 step:9181 [D loss: 0.617727, acc: 67.19%] [G loss: 1.950614]\n",
      "epoch:9 step:9182 [D loss: 0.652754, acc: 59.38%] [G loss: 1.912078]\n",
      "epoch:9 step:9183 [D loss: 0.632058, acc: 60.94%] [G loss: 1.923970]\n",
      "epoch:9 step:9184 [D loss: 0.628491, acc: 67.19%] [G loss: 1.973223]\n",
      "epoch:9 step:9185 [D loss: 0.666613, acc: 60.94%] [G loss: 1.908031]\n",
      "epoch:9 step:9186 [D loss: 0.622606, acc: 65.62%] [G loss: 1.920267]\n",
      "epoch:9 step:9187 [D loss: 0.744599, acc: 58.59%] [G loss: 1.921288]\n",
      "epoch:9 step:9188 [D loss: 0.609376, acc: 69.53%] [G loss: 2.127142]\n",
      "epoch:9 step:9189 [D loss: 0.633884, acc: 62.50%] [G loss: 2.066844]\n",
      "epoch:9 step:9190 [D loss: 0.581330, acc: 71.09%] [G loss: 2.115246]\n",
      "epoch:9 step:9191 [D loss: 0.610352, acc: 67.97%] [G loss: 1.957658]\n",
      "epoch:9 step:9192 [D loss: 0.696247, acc: 60.16%] [G loss: 1.847252]\n",
      "epoch:9 step:9193 [D loss: 0.611378, acc: 67.19%] [G loss: 2.005183]\n",
      "epoch:9 step:9194 [D loss: 0.641440, acc: 64.84%] [G loss: 2.073548]\n",
      "epoch:9 step:9195 [D loss: 0.663603, acc: 59.38%] [G loss: 1.932278]\n",
      "epoch:9 step:9196 [D loss: 0.596806, acc: 70.31%] [G loss: 2.013907]\n",
      "epoch:9 step:9197 [D loss: 0.567450, acc: 75.78%] [G loss: 2.065232]\n",
      "epoch:9 step:9198 [D loss: 0.717063, acc: 57.81%] [G loss: 1.877564]\n",
      "epoch:9 step:9199 [D loss: 0.703437, acc: 53.91%] [G loss: 1.765629]\n",
      "epoch:9 step:9200 [D loss: 0.630770, acc: 65.62%] [G loss: 2.026581]\n",
      "epoch:9 step:9201 [D loss: 0.651319, acc: 64.06%] [G loss: 1.856652]\n",
      "epoch:9 step:9202 [D loss: 0.594878, acc: 67.97%] [G loss: 1.980931]\n",
      "epoch:9 step:9203 [D loss: 0.627250, acc: 70.31%] [G loss: 1.977121]\n",
      "epoch:9 step:9204 [D loss: 0.643607, acc: 61.72%] [G loss: 1.934720]\n",
      "epoch:9 step:9205 [D loss: 0.632620, acc: 65.62%] [G loss: 2.017244]\n",
      "epoch:9 step:9206 [D loss: 0.641262, acc: 67.19%] [G loss: 1.978096]\n",
      "epoch:9 step:9207 [D loss: 0.627741, acc: 64.84%] [G loss: 2.266387]\n",
      "epoch:9 step:9208 [D loss: 0.575043, acc: 69.53%] [G loss: 2.385296]\n",
      "epoch:9 step:9209 [D loss: 0.669956, acc: 58.59%] [G loss: 1.992669]\n",
      "epoch:9 step:9210 [D loss: 0.645892, acc: 66.41%] [G loss: 2.029978]\n",
      "epoch:9 step:9211 [D loss: 0.571944, acc: 74.22%] [G loss: 2.044548]\n",
      "epoch:9 step:9212 [D loss: 0.673273, acc: 57.03%] [G loss: 2.016933]\n",
      "epoch:9 step:9213 [D loss: 0.601939, acc: 70.31%] [G loss: 2.081093]\n",
      "epoch:9 step:9214 [D loss: 0.617811, acc: 66.41%] [G loss: 2.361388]\n",
      "epoch:9 step:9215 [D loss: 0.553706, acc: 69.53%] [G loss: 2.353128]\n",
      "epoch:9 step:9216 [D loss: 0.607465, acc: 66.41%] [G loss: 2.160472]\n",
      "epoch:9 step:9217 [D loss: 0.714919, acc: 56.25%] [G loss: 1.887072]\n",
      "epoch:9 step:9218 [D loss: 0.614690, acc: 68.75%] [G loss: 1.992422]\n",
      "epoch:9 step:9219 [D loss: 0.636964, acc: 68.75%] [G loss: 2.171589]\n",
      "epoch:9 step:9220 [D loss: 0.624609, acc: 67.97%] [G loss: 2.019897]\n",
      "epoch:9 step:9221 [D loss: 0.692344, acc: 58.59%] [G loss: 1.793135]\n",
      "epoch:9 step:9222 [D loss: 0.576555, acc: 67.19%] [G loss: 2.090068]\n",
      "epoch:9 step:9223 [D loss: 0.593121, acc: 71.88%] [G loss: 2.035063]\n",
      "epoch:9 step:9224 [D loss: 0.640247, acc: 63.28%] [G loss: 2.200679]\n",
      "epoch:9 step:9225 [D loss: 0.599698, acc: 69.53%] [G loss: 2.468255]\n",
      "epoch:9 step:9226 [D loss: 0.597348, acc: 67.19%] [G loss: 2.302665]\n",
      "epoch:9 step:9227 [D loss: 0.676241, acc: 55.47%] [G loss: 2.030531]\n",
      "epoch:9 step:9228 [D loss: 0.612355, acc: 67.97%] [G loss: 2.096566]\n",
      "epoch:9 step:9229 [D loss: 0.598495, acc: 69.53%] [G loss: 2.239198]\n",
      "epoch:9 step:9230 [D loss: 0.626079, acc: 64.84%] [G loss: 1.961088]\n",
      "epoch:9 step:9231 [D loss: 0.617292, acc: 68.75%] [G loss: 2.053790]\n",
      "epoch:9 step:9232 [D loss: 0.631096, acc: 64.84%] [G loss: 2.137648]\n",
      "epoch:9 step:9233 [D loss: 0.633196, acc: 63.28%] [G loss: 1.975813]\n",
      "epoch:9 step:9234 [D loss: 0.690656, acc: 51.56%] [G loss: 1.987260]\n",
      "epoch:9 step:9235 [D loss: 0.587312, acc: 67.97%] [G loss: 2.129179]\n",
      "epoch:9 step:9236 [D loss: 0.537278, acc: 76.56%] [G loss: 2.149511]\n",
      "epoch:9 step:9237 [D loss: 0.575226, acc: 67.97%] [G loss: 2.060671]\n",
      "epoch:9 step:9238 [D loss: 0.623502, acc: 60.94%] [G loss: 2.115854]\n",
      "epoch:9 step:9239 [D loss: 0.650409, acc: 64.06%] [G loss: 2.091357]\n",
      "epoch:9 step:9240 [D loss: 0.588763, acc: 67.19%] [G loss: 2.108545]\n",
      "epoch:9 step:9241 [D loss: 0.612451, acc: 64.06%] [G loss: 2.138572]\n",
      "epoch:9 step:9242 [D loss: 0.632724, acc: 65.62%] [G loss: 1.958793]\n",
      "epoch:9 step:9243 [D loss: 0.599955, acc: 67.97%] [G loss: 2.244785]\n",
      "epoch:9 step:9244 [D loss: 0.663888, acc: 63.28%] [G loss: 2.052074]\n",
      "epoch:9 step:9245 [D loss: 0.630557, acc: 61.72%] [G loss: 1.969389]\n",
      "epoch:9 step:9246 [D loss: 0.644436, acc: 61.72%] [G loss: 2.172173]\n",
      "epoch:9 step:9247 [D loss: 0.641049, acc: 63.28%] [G loss: 2.086571]\n",
      "epoch:9 step:9248 [D loss: 0.658003, acc: 62.50%] [G loss: 2.312318]\n",
      "epoch:9 step:9249 [D loss: 0.601503, acc: 68.75%] [G loss: 2.227214]\n",
      "epoch:9 step:9250 [D loss: 0.651081, acc: 64.84%] [G loss: 2.146090]\n",
      "epoch:9 step:9251 [D loss: 0.660641, acc: 61.72%] [G loss: 1.974416]\n",
      "epoch:9 step:9252 [D loss: 0.617692, acc: 67.97%] [G loss: 2.074306]\n",
      "epoch:9 step:9253 [D loss: 0.688651, acc: 54.69%] [G loss: 1.832570]\n",
      "epoch:9 step:9254 [D loss: 0.664168, acc: 57.03%] [G loss: 2.109045]\n",
      "epoch:9 step:9255 [D loss: 0.589168, acc: 71.88%] [G loss: 2.218140]\n",
      "epoch:9 step:9256 [D loss: 0.593310, acc: 63.28%] [G loss: 2.110649]\n",
      "epoch:9 step:9257 [D loss: 0.615338, acc: 69.53%] [G loss: 1.980892]\n",
      "epoch:9 step:9258 [D loss: 0.611835, acc: 65.62%] [G loss: 2.358259]\n",
      "epoch:9 step:9259 [D loss: 0.675813, acc: 57.81%] [G loss: 2.005237]\n",
      "epoch:9 step:9260 [D loss: 0.690372, acc: 58.59%] [G loss: 1.989617]\n",
      "epoch:9 step:9261 [D loss: 0.648533, acc: 62.50%] [G loss: 2.069045]\n",
      "epoch:9 step:9262 [D loss: 0.716777, acc: 55.47%] [G loss: 2.121559]\n",
      "epoch:9 step:9263 [D loss: 0.604354, acc: 62.50%] [G loss: 2.036095]\n",
      "epoch:9 step:9264 [D loss: 0.669532, acc: 55.47%] [G loss: 2.147537]\n",
      "epoch:9 step:9265 [D loss: 0.628886, acc: 66.41%] [G loss: 2.150492]\n",
      "epoch:9 step:9266 [D loss: 0.591976, acc: 71.09%] [G loss: 2.252406]\n",
      "epoch:9 step:9267 [D loss: 0.620492, acc: 71.09%] [G loss: 2.071456]\n",
      "epoch:9 step:9268 [D loss: 0.670515, acc: 60.94%] [G loss: 2.050769]\n",
      "epoch:9 step:9269 [D loss: 0.618112, acc: 63.28%] [G loss: 2.077404]\n",
      "epoch:9 step:9270 [D loss: 0.612740, acc: 64.06%] [G loss: 2.101810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:9271 [D loss: 0.592129, acc: 72.66%] [G loss: 2.178366]\n",
      "epoch:9 step:9272 [D loss: 0.624113, acc: 71.88%] [G loss: 2.182024]\n",
      "epoch:9 step:9273 [D loss: 0.623694, acc: 62.50%] [G loss: 2.130200]\n",
      "epoch:9 step:9274 [D loss: 0.587308, acc: 68.75%] [G loss: 2.116835]\n",
      "epoch:9 step:9275 [D loss: 0.645292, acc: 66.41%] [G loss: 2.226566]\n",
      "epoch:9 step:9276 [D loss: 0.621707, acc: 62.50%] [G loss: 2.123258]\n",
      "epoch:9 step:9277 [D loss: 0.607066, acc: 63.28%] [G loss: 2.121261]\n",
      "epoch:9 step:9278 [D loss: 0.653105, acc: 61.72%] [G loss: 2.170750]\n",
      "epoch:9 step:9279 [D loss: 0.668515, acc: 57.81%] [G loss: 1.979787]\n",
      "epoch:9 step:9280 [D loss: 0.596769, acc: 67.19%] [G loss: 2.093779]\n",
      "epoch:9 step:9281 [D loss: 0.608467, acc: 63.28%] [G loss: 2.206145]\n",
      "epoch:9 step:9282 [D loss: 0.594823, acc: 64.84%] [G loss: 2.138410]\n",
      "epoch:9 step:9283 [D loss: 0.594865, acc: 67.19%] [G loss: 2.221598]\n",
      "epoch:9 step:9284 [D loss: 0.636243, acc: 67.19%] [G loss: 1.961013]\n",
      "epoch:9 step:9285 [D loss: 0.623497, acc: 64.84%] [G loss: 2.140567]\n",
      "epoch:9 step:9286 [D loss: 0.640481, acc: 60.16%] [G loss: 2.222725]\n",
      "epoch:9 step:9287 [D loss: 0.610359, acc: 67.97%] [G loss: 1.990933]\n",
      "epoch:9 step:9288 [D loss: 0.630590, acc: 65.62%] [G loss: 2.115171]\n",
      "epoch:9 step:9289 [D loss: 0.661299, acc: 57.03%] [G loss: 2.014066]\n",
      "epoch:9 step:9290 [D loss: 0.605050, acc: 66.41%] [G loss: 2.186546]\n",
      "epoch:9 step:9291 [D loss: 0.726700, acc: 56.25%] [G loss: 1.812277]\n",
      "epoch:9 step:9292 [D loss: 0.680939, acc: 57.81%] [G loss: 2.126120]\n",
      "epoch:9 step:9293 [D loss: 0.567804, acc: 77.34%] [G loss: 2.085154]\n",
      "epoch:9 step:9294 [D loss: 0.636752, acc: 61.72%] [G loss: 2.046624]\n",
      "epoch:9 step:9295 [D loss: 0.602895, acc: 67.19%] [G loss: 2.036530]\n",
      "epoch:9 step:9296 [D loss: 0.547636, acc: 72.66%] [G loss: 2.019171]\n",
      "epoch:9 step:9297 [D loss: 0.623906, acc: 65.62%] [G loss: 2.129684]\n",
      "epoch:9 step:9298 [D loss: 0.623355, acc: 64.06%] [G loss: 1.984098]\n",
      "epoch:9 step:9299 [D loss: 0.684297, acc: 58.59%] [G loss: 2.003050]\n",
      "epoch:9 step:9300 [D loss: 0.677180, acc: 55.47%] [G loss: 1.898760]\n",
      "epoch:9 step:9301 [D loss: 0.612795, acc: 69.53%] [G loss: 2.114760]\n",
      "epoch:9 step:9302 [D loss: 0.622443, acc: 71.09%] [G loss: 2.013111]\n",
      "epoch:9 step:9303 [D loss: 0.648113, acc: 65.62%] [G loss: 2.116190]\n",
      "epoch:9 step:9304 [D loss: 0.647138, acc: 67.19%] [G loss: 1.962402]\n",
      "epoch:9 step:9305 [D loss: 0.598551, acc: 67.97%] [G loss: 2.075710]\n",
      "epoch:9 step:9306 [D loss: 0.678814, acc: 57.03%] [G loss: 1.928182]\n",
      "epoch:9 step:9307 [D loss: 0.670700, acc: 57.81%] [G loss: 2.065749]\n",
      "epoch:9 step:9308 [D loss: 0.599882, acc: 69.53%] [G loss: 2.131403]\n",
      "epoch:9 step:9309 [D loss: 0.610632, acc: 66.41%] [G loss: 2.091439]\n",
      "epoch:9 step:9310 [D loss: 0.624108, acc: 64.06%] [G loss: 2.262121]\n",
      "epoch:9 step:9311 [D loss: 0.687443, acc: 60.16%] [G loss: 1.971695]\n",
      "epoch:9 step:9312 [D loss: 0.687048, acc: 64.06%] [G loss: 1.862955]\n",
      "epoch:9 step:9313 [D loss: 0.716008, acc: 59.38%] [G loss: 1.899595]\n",
      "epoch:9 step:9314 [D loss: 0.694139, acc: 60.94%] [G loss: 1.869158]\n",
      "epoch:9 step:9315 [D loss: 0.618666, acc: 62.50%] [G loss: 1.899931]\n",
      "epoch:9 step:9316 [D loss: 0.637550, acc: 63.28%] [G loss: 1.865495]\n",
      "epoch:9 step:9317 [D loss: 0.574797, acc: 69.53%] [G loss: 2.156987]\n",
      "epoch:9 step:9318 [D loss: 0.645590, acc: 63.28%] [G loss: 2.025546]\n",
      "epoch:9 step:9319 [D loss: 0.553982, acc: 71.09%] [G loss: 2.129123]\n",
      "epoch:9 step:9320 [D loss: 0.624285, acc: 64.06%] [G loss: 2.011572]\n",
      "epoch:9 step:9321 [D loss: 0.610236, acc: 63.28%] [G loss: 2.255949]\n",
      "epoch:9 step:9322 [D loss: 0.643414, acc: 58.59%] [G loss: 2.167270]\n",
      "epoch:9 step:9323 [D loss: 0.608958, acc: 68.75%] [G loss: 2.188941]\n",
      "epoch:9 step:9324 [D loss: 0.680176, acc: 63.28%] [G loss: 1.951701]\n",
      "epoch:9 step:9325 [D loss: 0.698196, acc: 58.59%] [G loss: 1.976127]\n",
      "epoch:9 step:9326 [D loss: 0.606191, acc: 65.62%] [G loss: 2.035339]\n",
      "epoch:9 step:9327 [D loss: 0.558302, acc: 72.66%] [G loss: 2.179937]\n",
      "epoch:9 step:9328 [D loss: 0.600112, acc: 66.41%] [G loss: 2.202707]\n",
      "epoch:9 step:9329 [D loss: 0.637041, acc: 62.50%] [G loss: 1.981331]\n",
      "epoch:9 step:9330 [D loss: 0.614214, acc: 63.28%] [G loss: 1.917381]\n",
      "epoch:9 step:9331 [D loss: 0.590992, acc: 67.19%] [G loss: 2.009072]\n",
      "epoch:9 step:9332 [D loss: 0.577257, acc: 68.75%] [G loss: 2.312007]\n",
      "epoch:9 step:9333 [D loss: 0.644246, acc: 67.19%] [G loss: 2.036435]\n",
      "epoch:9 step:9334 [D loss: 0.604765, acc: 69.53%] [G loss: 1.933208]\n",
      "epoch:9 step:9335 [D loss: 0.656175, acc: 57.81%] [G loss: 2.052847]\n",
      "epoch:9 step:9336 [D loss: 0.691630, acc: 63.28%] [G loss: 2.077667]\n",
      "epoch:9 step:9337 [D loss: 0.742977, acc: 50.78%] [G loss: 2.144814]\n",
      "epoch:9 step:9338 [D loss: 0.640457, acc: 63.28%] [G loss: 1.959620]\n",
      "epoch:9 step:9339 [D loss: 0.605292, acc: 66.41%] [G loss: 2.106140]\n",
      "epoch:9 step:9340 [D loss: 0.592464, acc: 66.41%] [G loss: 2.101574]\n",
      "epoch:9 step:9341 [D loss: 0.578595, acc: 67.97%] [G loss: 2.192283]\n",
      "epoch:9 step:9342 [D loss: 0.566559, acc: 71.09%] [G loss: 2.153743]\n",
      "epoch:9 step:9343 [D loss: 0.628034, acc: 64.84%] [G loss: 2.107578]\n",
      "epoch:9 step:9344 [D loss: 0.602214, acc: 64.84%] [G loss: 2.203453]\n",
      "epoch:9 step:9345 [D loss: 0.621653, acc: 61.72%] [G loss: 2.450033]\n",
      "epoch:9 step:9346 [D loss: 0.646769, acc: 61.72%] [G loss: 2.027632]\n",
      "epoch:9 step:9347 [D loss: 0.694851, acc: 57.81%] [G loss: 2.062017]\n",
      "epoch:9 step:9348 [D loss: 0.608774, acc: 69.53%] [G loss: 2.094081]\n",
      "epoch:9 step:9349 [D loss: 0.615692, acc: 65.62%] [G loss: 2.105952]\n",
      "epoch:9 step:9350 [D loss: 0.669100, acc: 62.50%] [G loss: 2.096696]\n",
      "epoch:9 step:9351 [D loss: 0.575595, acc: 67.19%] [G loss: 2.332830]\n",
      "epoch:9 step:9352 [D loss: 0.639958, acc: 68.75%] [G loss: 2.251948]\n",
      "epoch:9 step:9353 [D loss: 0.710777, acc: 55.47%] [G loss: 2.109594]\n",
      "epoch:9 step:9354 [D loss: 0.579118, acc: 71.88%] [G loss: 2.393749]\n",
      "epoch:9 step:9355 [D loss: 0.597166, acc: 69.53%] [G loss: 2.177826]\n",
      "epoch:9 step:9356 [D loss: 0.576658, acc: 73.44%] [G loss: 2.239269]\n",
      "epoch:9 step:9357 [D loss: 0.528793, acc: 77.34%] [G loss: 2.570937]\n",
      "epoch:9 step:9358 [D loss: 0.526863, acc: 73.44%] [G loss: 2.650794]\n",
      "epoch:9 step:9359 [D loss: 0.618450, acc: 64.84%] [G loss: 2.423126]\n",
      "epoch:9 step:9360 [D loss: 0.682928, acc: 55.47%] [G loss: 2.246359]\n",
      "epoch:9 step:9361 [D loss: 0.760087, acc: 56.25%] [G loss: 2.011899]\n",
      "epoch:9 step:9362 [D loss: 0.701699, acc: 50.00%] [G loss: 2.000217]\n",
      "epoch:9 step:9363 [D loss: 0.593404, acc: 64.06%] [G loss: 2.156136]\n",
      "epoch:9 step:9364 [D loss: 0.660551, acc: 62.50%] [G loss: 2.196593]\n",
      "epoch:9 step:9365 [D loss: 0.568307, acc: 70.31%] [G loss: 2.139832]\n",
      "epoch:9 step:9366 [D loss: 0.565495, acc: 71.09%] [G loss: 2.185821]\n",
      "epoch:9 step:9367 [D loss: 0.556892, acc: 69.53%] [G loss: 2.232004]\n",
      "epoch:9 step:9368 [D loss: 0.697152, acc: 56.25%] [G loss: 2.057244]\n",
      "epoch:9 step:9369 [D loss: 0.590574, acc: 67.19%] [G loss: 2.193302]\n",
      "epoch:9 step:9370 [D loss: 0.560739, acc: 74.22%] [G loss: 2.913126]\n",
      "epoch:10 step:9371 [D loss: 0.626755, acc: 64.84%] [G loss: 2.279384]\n",
      "epoch:10 step:9372 [D loss: 0.670687, acc: 65.62%] [G loss: 2.087868]\n",
      "epoch:10 step:9373 [D loss: 0.628591, acc: 64.06%] [G loss: 2.141798]\n",
      "epoch:10 step:9374 [D loss: 0.596510, acc: 64.06%] [G loss: 2.106658]\n",
      "epoch:10 step:9375 [D loss: 0.581844, acc: 70.31%] [G loss: 2.175057]\n",
      "epoch:10 step:9376 [D loss: 0.665402, acc: 61.72%] [G loss: 2.171249]\n",
      "epoch:10 step:9377 [D loss: 0.575217, acc: 67.19%] [G loss: 2.073827]\n",
      "epoch:10 step:9378 [D loss: 0.631367, acc: 63.28%] [G loss: 2.017033]\n",
      "epoch:10 step:9379 [D loss: 0.604837, acc: 69.53%] [G loss: 2.123664]\n",
      "epoch:10 step:9380 [D loss: 0.592869, acc: 65.62%] [G loss: 2.202439]\n",
      "epoch:10 step:9381 [D loss: 0.679533, acc: 60.16%] [G loss: 2.126765]\n",
      "epoch:10 step:9382 [D loss: 0.657396, acc: 58.59%] [G loss: 1.988083]\n",
      "epoch:10 step:9383 [D loss: 0.642086, acc: 62.50%] [G loss: 2.275545]\n",
      "epoch:10 step:9384 [D loss: 0.584371, acc: 70.31%] [G loss: 2.253678]\n",
      "epoch:10 step:9385 [D loss: 0.576942, acc: 68.75%] [G loss: 2.504850]\n",
      "epoch:10 step:9386 [D loss: 0.582283, acc: 67.19%] [G loss: 2.288460]\n",
      "epoch:10 step:9387 [D loss: 0.653882, acc: 57.03%] [G loss: 2.104054]\n",
      "epoch:10 step:9388 [D loss: 0.692063, acc: 60.94%] [G loss: 1.988743]\n",
      "epoch:10 step:9389 [D loss: 0.655217, acc: 63.28%] [G loss: 2.041518]\n",
      "epoch:10 step:9390 [D loss: 0.633876, acc: 62.50%] [G loss: 2.004331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9391 [D loss: 0.717500, acc: 56.25%] [G loss: 1.982148]\n",
      "epoch:10 step:9392 [D loss: 0.665368, acc: 58.59%] [G loss: 2.093375]\n",
      "epoch:10 step:9393 [D loss: 0.568573, acc: 70.31%] [G loss: 2.229014]\n",
      "epoch:10 step:9394 [D loss: 0.621157, acc: 67.97%] [G loss: 2.303567]\n",
      "epoch:10 step:9395 [D loss: 0.587743, acc: 67.97%] [G loss: 2.157832]\n",
      "epoch:10 step:9396 [D loss: 0.576377, acc: 67.97%] [G loss: 1.956739]\n",
      "epoch:10 step:9397 [D loss: 0.707566, acc: 56.25%] [G loss: 1.945674]\n",
      "epoch:10 step:9398 [D loss: 0.608726, acc: 64.06%] [G loss: 1.981053]\n",
      "epoch:10 step:9399 [D loss: 0.610323, acc: 71.09%] [G loss: 2.070675]\n",
      "epoch:10 step:9400 [D loss: 0.600182, acc: 67.19%] [G loss: 2.179193]\n",
      "epoch:10 step:9401 [D loss: 0.672413, acc: 65.62%] [G loss: 1.904194]\n",
      "epoch:10 step:9402 [D loss: 0.610915, acc: 69.53%] [G loss: 1.896745]\n",
      "epoch:10 step:9403 [D loss: 0.630574, acc: 67.97%] [G loss: 1.957643]\n",
      "epoch:10 step:9404 [D loss: 0.612397, acc: 64.06%] [G loss: 2.039847]\n",
      "epoch:10 step:9405 [D loss: 0.584268, acc: 66.41%] [G loss: 2.119078]\n",
      "epoch:10 step:9406 [D loss: 0.613518, acc: 67.19%] [G loss: 2.264189]\n",
      "epoch:10 step:9407 [D loss: 0.651038, acc: 60.16%] [G loss: 2.111931]\n",
      "epoch:10 step:9408 [D loss: 0.694773, acc: 63.28%] [G loss: 2.097505]\n",
      "epoch:10 step:9409 [D loss: 0.614905, acc: 65.62%] [G loss: 2.085871]\n",
      "epoch:10 step:9410 [D loss: 0.575930, acc: 71.09%] [G loss: 2.488445]\n",
      "epoch:10 step:9411 [D loss: 0.617850, acc: 66.41%] [G loss: 1.977235]\n",
      "epoch:10 step:9412 [D loss: 0.630038, acc: 67.19%] [G loss: 2.072330]\n",
      "epoch:10 step:9413 [D loss: 0.598310, acc: 67.97%] [G loss: 2.096644]\n",
      "epoch:10 step:9414 [D loss: 0.645788, acc: 60.94%] [G loss: 2.029774]\n",
      "epoch:10 step:9415 [D loss: 0.722554, acc: 52.34%] [G loss: 2.067395]\n",
      "epoch:10 step:9416 [D loss: 0.660392, acc: 60.16%] [G loss: 1.951714]\n",
      "epoch:10 step:9417 [D loss: 0.604144, acc: 64.84%] [G loss: 1.880310]\n",
      "epoch:10 step:9418 [D loss: 0.630371, acc: 64.06%] [G loss: 2.203407]\n",
      "epoch:10 step:9419 [D loss: 0.615718, acc: 64.06%] [G loss: 2.140854]\n",
      "epoch:10 step:9420 [D loss: 0.631295, acc: 62.50%] [G loss: 2.196681]\n",
      "epoch:10 step:9421 [D loss: 0.619104, acc: 68.75%] [G loss: 2.088888]\n",
      "epoch:10 step:9422 [D loss: 0.646760, acc: 62.50%] [G loss: 2.167483]\n",
      "epoch:10 step:9423 [D loss: 0.577777, acc: 70.31%] [G loss: 2.109396]\n",
      "epoch:10 step:9424 [D loss: 0.601160, acc: 67.97%] [G loss: 2.091910]\n",
      "epoch:10 step:9425 [D loss: 0.584820, acc: 68.75%] [G loss: 2.194215]\n",
      "epoch:10 step:9426 [D loss: 0.641510, acc: 61.72%] [G loss: 2.168396]\n",
      "epoch:10 step:9427 [D loss: 0.647180, acc: 58.59%] [G loss: 2.139847]\n",
      "epoch:10 step:9428 [D loss: 0.629668, acc: 66.41%] [G loss: 2.010036]\n",
      "epoch:10 step:9429 [D loss: 0.659430, acc: 59.38%] [G loss: 2.008712]\n",
      "epoch:10 step:9430 [D loss: 0.696682, acc: 61.72%] [G loss: 2.002060]\n",
      "epoch:10 step:9431 [D loss: 0.640293, acc: 64.06%] [G loss: 1.960309]\n",
      "epoch:10 step:9432 [D loss: 0.603177, acc: 68.75%] [G loss: 2.108906]\n",
      "epoch:10 step:9433 [D loss: 0.638674, acc: 69.53%] [G loss: 1.890716]\n",
      "epoch:10 step:9434 [D loss: 0.594729, acc: 67.19%] [G loss: 2.113476]\n",
      "epoch:10 step:9435 [D loss: 0.643577, acc: 66.41%] [G loss: 1.952079]\n",
      "epoch:10 step:9436 [D loss: 0.596250, acc: 67.97%] [G loss: 2.181729]\n",
      "epoch:10 step:9437 [D loss: 0.595098, acc: 67.19%] [G loss: 1.983750]\n",
      "epoch:10 step:9438 [D loss: 0.628955, acc: 66.41%] [G loss: 2.033256]\n",
      "epoch:10 step:9439 [D loss: 0.598527, acc: 67.97%] [G loss: 2.327579]\n",
      "epoch:10 step:9440 [D loss: 0.582880, acc: 68.75%] [G loss: 2.268538]\n",
      "epoch:10 step:9441 [D loss: 0.665107, acc: 58.59%] [G loss: 1.976890]\n",
      "epoch:10 step:9442 [D loss: 0.627585, acc: 60.16%] [G loss: 2.122288]\n",
      "epoch:10 step:9443 [D loss: 0.602042, acc: 70.31%] [G loss: 2.104086]\n",
      "epoch:10 step:9444 [D loss: 0.611743, acc: 69.53%] [G loss: 2.189060]\n",
      "epoch:10 step:9445 [D loss: 0.596485, acc: 65.62%] [G loss: 2.284575]\n",
      "epoch:10 step:9446 [D loss: 0.615778, acc: 66.41%] [G loss: 2.315570]\n",
      "epoch:10 step:9447 [D loss: 0.577241, acc: 73.44%] [G loss: 2.323123]\n",
      "epoch:10 step:9448 [D loss: 0.637698, acc: 64.06%] [G loss: 1.813787]\n",
      "epoch:10 step:9449 [D loss: 0.658309, acc: 59.38%] [G loss: 1.853644]\n",
      "epoch:10 step:9450 [D loss: 0.679985, acc: 55.47%] [G loss: 1.895735]\n",
      "epoch:10 step:9451 [D loss: 0.679419, acc: 60.94%] [G loss: 1.973824]\n",
      "epoch:10 step:9452 [D loss: 0.607356, acc: 67.97%] [G loss: 2.133828]\n",
      "epoch:10 step:9453 [D loss: 0.651565, acc: 60.94%] [G loss: 2.116554]\n",
      "epoch:10 step:9454 [D loss: 0.668976, acc: 65.62%] [G loss: 2.023414]\n",
      "epoch:10 step:9455 [D loss: 0.657345, acc: 60.94%] [G loss: 1.927712]\n",
      "epoch:10 step:9456 [D loss: 0.609057, acc: 67.97%] [G loss: 1.969999]\n",
      "epoch:10 step:9457 [D loss: 0.629227, acc: 65.62%] [G loss: 1.952108]\n",
      "epoch:10 step:9458 [D loss: 0.630201, acc: 64.84%] [G loss: 2.050032]\n",
      "epoch:10 step:9459 [D loss: 0.589386, acc: 70.31%] [G loss: 2.168408]\n",
      "epoch:10 step:9460 [D loss: 0.595711, acc: 67.97%] [G loss: 2.069562]\n",
      "epoch:10 step:9461 [D loss: 0.625972, acc: 64.84%] [G loss: 1.996362]\n",
      "epoch:10 step:9462 [D loss: 0.649064, acc: 64.06%] [G loss: 2.173614]\n",
      "epoch:10 step:9463 [D loss: 0.588409, acc: 71.88%] [G loss: 2.045941]\n",
      "epoch:10 step:9464 [D loss: 0.682281, acc: 62.50%] [G loss: 2.054628]\n",
      "epoch:10 step:9465 [D loss: 0.629082, acc: 65.62%] [G loss: 1.895459]\n",
      "epoch:10 step:9466 [D loss: 0.587161, acc: 68.75%] [G loss: 2.001167]\n",
      "epoch:10 step:9467 [D loss: 0.597473, acc: 67.19%] [G loss: 2.163001]\n",
      "epoch:10 step:9468 [D loss: 0.700980, acc: 57.81%] [G loss: 2.087477]\n",
      "epoch:10 step:9469 [D loss: 0.667929, acc: 65.62%] [G loss: 1.984830]\n",
      "epoch:10 step:9470 [D loss: 0.578473, acc: 74.22%] [G loss: 2.082372]\n",
      "epoch:10 step:9471 [D loss: 0.560514, acc: 71.88%] [G loss: 2.230455]\n",
      "epoch:10 step:9472 [D loss: 0.669828, acc: 59.38%] [G loss: 2.011166]\n",
      "epoch:10 step:9473 [D loss: 0.563502, acc: 75.78%] [G loss: 2.227895]\n",
      "epoch:10 step:9474 [D loss: 0.677706, acc: 60.94%] [G loss: 1.988550]\n",
      "epoch:10 step:9475 [D loss: 0.656347, acc: 61.72%] [G loss: 2.096577]\n",
      "epoch:10 step:9476 [D loss: 0.614818, acc: 69.53%] [G loss: 2.185534]\n",
      "epoch:10 step:9477 [D loss: 0.662574, acc: 60.16%] [G loss: 2.076430]\n",
      "epoch:10 step:9478 [D loss: 0.684165, acc: 53.12%] [G loss: 1.883265]\n",
      "epoch:10 step:9479 [D loss: 0.649325, acc: 63.28%] [G loss: 2.017527]\n",
      "epoch:10 step:9480 [D loss: 0.664257, acc: 57.03%] [G loss: 1.933593]\n",
      "epoch:10 step:9481 [D loss: 0.628363, acc: 68.75%] [G loss: 2.120478]\n",
      "epoch:10 step:9482 [D loss: 0.636606, acc: 62.50%] [G loss: 1.871626]\n",
      "epoch:10 step:9483 [D loss: 0.591299, acc: 65.62%] [G loss: 2.139016]\n",
      "epoch:10 step:9484 [D loss: 0.628801, acc: 67.19%] [G loss: 2.341489]\n",
      "epoch:10 step:9485 [D loss: 0.631675, acc: 64.06%] [G loss: 2.244721]\n",
      "epoch:10 step:9486 [D loss: 0.678941, acc: 62.50%] [G loss: 1.993278]\n",
      "epoch:10 step:9487 [D loss: 0.588441, acc: 67.97%] [G loss: 2.277266]\n",
      "epoch:10 step:9488 [D loss: 0.672072, acc: 55.47%] [G loss: 2.147184]\n",
      "epoch:10 step:9489 [D loss: 0.568683, acc: 73.44%] [G loss: 2.463302]\n",
      "epoch:10 step:9490 [D loss: 0.670713, acc: 62.50%] [G loss: 2.043510]\n",
      "epoch:10 step:9491 [D loss: 0.627867, acc: 64.06%] [G loss: 2.139194]\n",
      "epoch:10 step:9492 [D loss: 0.649968, acc: 61.72%] [G loss: 2.215840]\n",
      "epoch:10 step:9493 [D loss: 0.629328, acc: 64.84%] [G loss: 2.252115]\n",
      "epoch:10 step:9494 [D loss: 0.658302, acc: 57.81%] [G loss: 1.955299]\n",
      "epoch:10 step:9495 [D loss: 0.651786, acc: 52.34%] [G loss: 1.881110]\n",
      "epoch:10 step:9496 [D loss: 0.628248, acc: 64.06%] [G loss: 2.093915]\n",
      "epoch:10 step:9497 [D loss: 0.587548, acc: 69.53%] [G loss: 2.071229]\n",
      "epoch:10 step:9498 [D loss: 0.631273, acc: 63.28%] [G loss: 1.949307]\n",
      "epoch:10 step:9499 [D loss: 0.635873, acc: 60.16%] [G loss: 2.104340]\n",
      "epoch:10 step:9500 [D loss: 0.611383, acc: 70.31%] [G loss: 1.966744]\n",
      "epoch:10 step:9501 [D loss: 0.627859, acc: 63.28%] [G loss: 2.310075]\n",
      "epoch:10 step:9502 [D loss: 0.597726, acc: 64.84%] [G loss: 1.970105]\n",
      "epoch:10 step:9503 [D loss: 0.683242, acc: 55.47%] [G loss: 1.973872]\n",
      "epoch:10 step:9504 [D loss: 0.640013, acc: 61.72%] [G loss: 1.879826]\n",
      "epoch:10 step:9505 [D loss: 0.689315, acc: 59.38%] [G loss: 1.892212]\n",
      "epoch:10 step:9506 [D loss: 0.671257, acc: 64.84%] [G loss: 1.826440]\n",
      "epoch:10 step:9507 [D loss: 0.618904, acc: 67.97%] [G loss: 2.016161]\n",
      "epoch:10 step:9508 [D loss: 0.641662, acc: 64.84%] [G loss: 1.939579]\n",
      "epoch:10 step:9509 [D loss: 0.657608, acc: 61.72%] [G loss: 2.037597]\n",
      "epoch:10 step:9510 [D loss: 0.584632, acc: 75.00%] [G loss: 2.154980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9511 [D loss: 0.620444, acc: 66.41%] [G loss: 2.008208]\n",
      "epoch:10 step:9512 [D loss: 0.647969, acc: 58.59%] [G loss: 1.948135]\n",
      "epoch:10 step:9513 [D loss: 0.661718, acc: 60.94%] [G loss: 1.875154]\n",
      "epoch:10 step:9514 [D loss: 0.683971, acc: 60.16%] [G loss: 2.005014]\n",
      "epoch:10 step:9515 [D loss: 0.619405, acc: 62.50%] [G loss: 1.838661]\n",
      "epoch:10 step:9516 [D loss: 0.595424, acc: 71.09%] [G loss: 2.112078]\n",
      "epoch:10 step:9517 [D loss: 0.674852, acc: 64.84%] [G loss: 1.873806]\n",
      "epoch:10 step:9518 [D loss: 0.627605, acc: 62.50%] [G loss: 2.044600]\n",
      "epoch:10 step:9519 [D loss: 0.629617, acc: 63.28%] [G loss: 2.143634]\n",
      "epoch:10 step:9520 [D loss: 0.670232, acc: 64.84%] [G loss: 2.074885]\n",
      "epoch:10 step:9521 [D loss: 0.590065, acc: 71.09%] [G loss: 2.301885]\n",
      "epoch:10 step:9522 [D loss: 0.635222, acc: 66.41%] [G loss: 2.107187]\n",
      "epoch:10 step:9523 [D loss: 0.635751, acc: 65.62%] [G loss: 2.169589]\n",
      "epoch:10 step:9524 [D loss: 0.622256, acc: 65.62%] [G loss: 2.232315]\n",
      "epoch:10 step:9525 [D loss: 0.691632, acc: 53.91%] [G loss: 2.103417]\n",
      "epoch:10 step:9526 [D loss: 0.650215, acc: 64.06%] [G loss: 1.932241]\n",
      "epoch:10 step:9527 [D loss: 0.635476, acc: 64.06%] [G loss: 2.011737]\n",
      "epoch:10 step:9528 [D loss: 0.647336, acc: 57.81%] [G loss: 2.023524]\n",
      "epoch:10 step:9529 [D loss: 0.631886, acc: 60.16%] [G loss: 2.118195]\n",
      "epoch:10 step:9530 [D loss: 0.645024, acc: 62.50%] [G loss: 1.857347]\n",
      "epoch:10 step:9531 [D loss: 0.591520, acc: 72.66%] [G loss: 2.109328]\n",
      "epoch:10 step:9532 [D loss: 0.643992, acc: 67.19%] [G loss: 1.963988]\n",
      "epoch:10 step:9533 [D loss: 0.588104, acc: 72.66%] [G loss: 1.933223]\n",
      "epoch:10 step:9534 [D loss: 0.641616, acc: 57.81%] [G loss: 1.793287]\n",
      "epoch:10 step:9535 [D loss: 0.565756, acc: 71.09%] [G loss: 2.096124]\n",
      "epoch:10 step:9536 [D loss: 0.591571, acc: 71.88%] [G loss: 1.878406]\n",
      "epoch:10 step:9537 [D loss: 0.594666, acc: 64.84%] [G loss: 2.182731]\n",
      "epoch:10 step:9538 [D loss: 0.585643, acc: 68.75%] [G loss: 2.244564]\n",
      "epoch:10 step:9539 [D loss: 0.639117, acc: 63.28%] [G loss: 2.016078]\n",
      "epoch:10 step:9540 [D loss: 0.642734, acc: 65.62%] [G loss: 2.146643]\n",
      "epoch:10 step:9541 [D loss: 0.633589, acc: 64.06%] [G loss: 2.157869]\n",
      "epoch:10 step:9542 [D loss: 0.573033, acc: 71.09%] [G loss: 2.186735]\n",
      "epoch:10 step:9543 [D loss: 0.633684, acc: 67.19%] [G loss: 2.166140]\n",
      "epoch:10 step:9544 [D loss: 0.662623, acc: 66.41%] [G loss: 1.975612]\n",
      "epoch:10 step:9545 [D loss: 0.640829, acc: 67.19%] [G loss: 1.984256]\n",
      "epoch:10 step:9546 [D loss: 0.686237, acc: 60.16%] [G loss: 2.006216]\n",
      "epoch:10 step:9547 [D loss: 0.668437, acc: 63.28%] [G loss: 1.876440]\n",
      "epoch:10 step:9548 [D loss: 0.666803, acc: 60.16%] [G loss: 1.990905]\n",
      "epoch:10 step:9549 [D loss: 0.632622, acc: 66.41%] [G loss: 1.911382]\n",
      "epoch:10 step:9550 [D loss: 0.658670, acc: 63.28%] [G loss: 2.117006]\n",
      "epoch:10 step:9551 [D loss: 0.634513, acc: 67.19%] [G loss: 2.032708]\n",
      "epoch:10 step:9552 [D loss: 0.653464, acc: 64.84%] [G loss: 1.924259]\n",
      "epoch:10 step:9553 [D loss: 0.653594, acc: 59.38%] [G loss: 2.033579]\n",
      "epoch:10 step:9554 [D loss: 0.591385, acc: 65.62%] [G loss: 2.024686]\n",
      "epoch:10 step:9555 [D loss: 0.700752, acc: 57.81%] [G loss: 2.158351]\n",
      "epoch:10 step:9556 [D loss: 0.625780, acc: 66.41%] [G loss: 2.026731]\n",
      "epoch:10 step:9557 [D loss: 0.665269, acc: 57.81%] [G loss: 2.062366]\n",
      "epoch:10 step:9558 [D loss: 0.660884, acc: 55.47%] [G loss: 2.063373]\n",
      "epoch:10 step:9559 [D loss: 0.699633, acc: 57.03%] [G loss: 1.888107]\n",
      "epoch:10 step:9560 [D loss: 0.624965, acc: 64.84%] [G loss: 2.078539]\n",
      "epoch:10 step:9561 [D loss: 0.631444, acc: 62.50%] [G loss: 2.145111]\n",
      "epoch:10 step:9562 [D loss: 0.643126, acc: 60.94%] [G loss: 2.150174]\n",
      "epoch:10 step:9563 [D loss: 0.622069, acc: 69.53%] [G loss: 2.031846]\n",
      "epoch:10 step:9564 [D loss: 0.651899, acc: 61.72%] [G loss: 2.150615]\n",
      "epoch:10 step:9565 [D loss: 0.668253, acc: 57.03%] [G loss: 2.086488]\n",
      "epoch:10 step:9566 [D loss: 0.686908, acc: 57.81%] [G loss: 2.013005]\n",
      "epoch:10 step:9567 [D loss: 0.658607, acc: 60.16%] [G loss: 2.008184]\n",
      "epoch:10 step:9568 [D loss: 0.627986, acc: 64.06%] [G loss: 2.021613]\n",
      "epoch:10 step:9569 [D loss: 0.648103, acc: 67.19%] [G loss: 2.108000]\n",
      "epoch:10 step:9570 [D loss: 0.664151, acc: 57.03%] [G loss: 1.902772]\n",
      "epoch:10 step:9571 [D loss: 0.626385, acc: 71.88%] [G loss: 2.045873]\n",
      "epoch:10 step:9572 [D loss: 0.688841, acc: 62.50%] [G loss: 1.998380]\n",
      "epoch:10 step:9573 [D loss: 0.677539, acc: 59.38%] [G loss: 1.949709]\n",
      "epoch:10 step:9574 [D loss: 0.670319, acc: 64.06%] [G loss: 2.035318]\n",
      "epoch:10 step:9575 [D loss: 0.608975, acc: 69.53%] [G loss: 1.967051]\n",
      "epoch:10 step:9576 [D loss: 0.560616, acc: 75.00%] [G loss: 2.284038]\n",
      "epoch:10 step:9577 [D loss: 0.545926, acc: 73.44%] [G loss: 2.440048]\n",
      "epoch:10 step:9578 [D loss: 0.618297, acc: 68.75%] [G loss: 2.277626]\n",
      "epoch:10 step:9579 [D loss: 0.617276, acc: 67.97%] [G loss: 2.256288]\n",
      "epoch:10 step:9580 [D loss: 0.659118, acc: 64.84%] [G loss: 1.950999]\n",
      "epoch:10 step:9581 [D loss: 0.645103, acc: 65.62%] [G loss: 1.960729]\n",
      "epoch:10 step:9582 [D loss: 0.662541, acc: 57.81%] [G loss: 1.977969]\n",
      "epoch:10 step:9583 [D loss: 0.640562, acc: 64.06%] [G loss: 1.804877]\n",
      "epoch:10 step:9584 [D loss: 0.654195, acc: 65.62%] [G loss: 1.850561]\n",
      "epoch:10 step:9585 [D loss: 0.677890, acc: 59.38%] [G loss: 1.962269]\n",
      "epoch:10 step:9586 [D loss: 0.662685, acc: 58.59%] [G loss: 1.824840]\n",
      "epoch:10 step:9587 [D loss: 0.586785, acc: 67.97%] [G loss: 2.278867]\n",
      "epoch:10 step:9588 [D loss: 0.572644, acc: 74.22%] [G loss: 2.275460]\n",
      "epoch:10 step:9589 [D loss: 0.583736, acc: 69.53%] [G loss: 2.540343]\n",
      "epoch:10 step:9590 [D loss: 0.685886, acc: 55.47%] [G loss: 1.735010]\n",
      "epoch:10 step:9591 [D loss: 0.681619, acc: 60.16%] [G loss: 2.080957]\n",
      "epoch:10 step:9592 [D loss: 0.662091, acc: 66.41%] [G loss: 2.224172]\n",
      "epoch:10 step:9593 [D loss: 0.667628, acc: 60.94%] [G loss: 1.947773]\n",
      "epoch:10 step:9594 [D loss: 0.655380, acc: 55.47%] [G loss: 1.981838]\n",
      "epoch:10 step:9595 [D loss: 0.672135, acc: 62.50%] [G loss: 1.894328]\n",
      "epoch:10 step:9596 [D loss: 0.652327, acc: 61.72%] [G loss: 1.915095]\n",
      "epoch:10 step:9597 [D loss: 0.667039, acc: 66.41%] [G loss: 1.973761]\n",
      "epoch:10 step:9598 [D loss: 0.657925, acc: 60.94%] [G loss: 1.882632]\n",
      "epoch:10 step:9599 [D loss: 0.579799, acc: 70.31%] [G loss: 2.202720]\n",
      "epoch:10 step:9600 [D loss: 0.570111, acc: 67.97%] [G loss: 2.293859]\n",
      "epoch:10 step:9601 [D loss: 0.576990, acc: 68.75%] [G loss: 2.402279]\n",
      "epoch:10 step:9602 [D loss: 0.574129, acc: 68.75%] [G loss: 2.326423]\n",
      "epoch:10 step:9603 [D loss: 0.618450, acc: 65.62%] [G loss: 1.911381]\n",
      "epoch:10 step:9604 [D loss: 0.704229, acc: 56.25%] [G loss: 1.999297]\n",
      "epoch:10 step:9605 [D loss: 0.649847, acc: 61.72%] [G loss: 2.148429]\n",
      "epoch:10 step:9606 [D loss: 0.645128, acc: 60.16%] [G loss: 1.964138]\n",
      "epoch:10 step:9607 [D loss: 0.620930, acc: 65.62%] [G loss: 1.931884]\n",
      "epoch:10 step:9608 [D loss: 0.595066, acc: 73.44%] [G loss: 2.004505]\n",
      "epoch:10 step:9609 [D loss: 0.592472, acc: 63.28%] [G loss: 2.024230]\n",
      "epoch:10 step:9610 [D loss: 0.664172, acc: 57.81%] [G loss: 2.037483]\n",
      "epoch:10 step:9611 [D loss: 0.661071, acc: 60.94%] [G loss: 2.084929]\n",
      "epoch:10 step:9612 [D loss: 0.659433, acc: 60.16%] [G loss: 2.068666]\n",
      "epoch:10 step:9613 [D loss: 0.625932, acc: 68.75%] [G loss: 2.050270]\n",
      "epoch:10 step:9614 [D loss: 0.654374, acc: 62.50%] [G loss: 2.008691]\n",
      "epoch:10 step:9615 [D loss: 0.620863, acc: 64.06%] [G loss: 2.120049]\n",
      "epoch:10 step:9616 [D loss: 0.592398, acc: 71.09%] [G loss: 2.106171]\n",
      "epoch:10 step:9617 [D loss: 0.648144, acc: 65.62%] [G loss: 2.114213]\n",
      "epoch:10 step:9618 [D loss: 0.543286, acc: 72.66%] [G loss: 2.136104]\n",
      "epoch:10 step:9619 [D loss: 0.652548, acc: 58.59%] [G loss: 1.933458]\n",
      "epoch:10 step:9620 [D loss: 0.739245, acc: 57.03%] [G loss: 1.941213]\n",
      "epoch:10 step:9621 [D loss: 0.675245, acc: 57.81%] [G loss: 1.909665]\n",
      "epoch:10 step:9622 [D loss: 0.648323, acc: 57.03%] [G loss: 1.958750]\n",
      "epoch:10 step:9623 [D loss: 0.654308, acc: 66.41%] [G loss: 1.847377]\n",
      "epoch:10 step:9624 [D loss: 0.613767, acc: 65.62%] [G loss: 1.903679]\n",
      "epoch:10 step:9625 [D loss: 0.610207, acc: 61.72%] [G loss: 1.945569]\n",
      "epoch:10 step:9626 [D loss: 0.611319, acc: 68.75%] [G loss: 1.904303]\n",
      "epoch:10 step:9627 [D loss: 0.651057, acc: 61.72%] [G loss: 2.004369]\n",
      "epoch:10 step:9628 [D loss: 0.663427, acc: 63.28%] [G loss: 1.992861]\n",
      "epoch:10 step:9629 [D loss: 0.636170, acc: 59.38%] [G loss: 2.003256]\n",
      "epoch:10 step:9630 [D loss: 0.659555, acc: 58.59%] [G loss: 1.986104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9631 [D loss: 0.615334, acc: 69.53%] [G loss: 1.947840]\n",
      "epoch:10 step:9632 [D loss: 0.598958, acc: 65.62%] [G loss: 2.280964]\n",
      "epoch:10 step:9633 [D loss: 0.680983, acc: 55.47%] [G loss: 2.129141]\n",
      "epoch:10 step:9634 [D loss: 0.612574, acc: 68.75%] [G loss: 2.433479]\n",
      "epoch:10 step:9635 [D loss: 0.669281, acc: 59.38%] [G loss: 1.907688]\n",
      "epoch:10 step:9636 [D loss: 0.658907, acc: 57.81%] [G loss: 1.961721]\n",
      "epoch:10 step:9637 [D loss: 0.597117, acc: 64.84%] [G loss: 2.093945]\n",
      "epoch:10 step:9638 [D loss: 0.680003, acc: 59.38%] [G loss: 1.971440]\n",
      "epoch:10 step:9639 [D loss: 0.665928, acc: 60.94%] [G loss: 2.064445]\n",
      "epoch:10 step:9640 [D loss: 0.653034, acc: 60.16%] [G loss: 1.988103]\n",
      "epoch:10 step:9641 [D loss: 0.630157, acc: 62.50%] [G loss: 2.053602]\n",
      "epoch:10 step:9642 [D loss: 0.618293, acc: 66.41%] [G loss: 2.011142]\n",
      "epoch:10 step:9643 [D loss: 0.651289, acc: 60.16%] [G loss: 2.020549]\n",
      "epoch:10 step:9644 [D loss: 0.574240, acc: 72.66%] [G loss: 2.082069]\n",
      "epoch:10 step:9645 [D loss: 0.624079, acc: 64.06%] [G loss: 2.181525]\n",
      "epoch:10 step:9646 [D loss: 0.570867, acc: 68.75%] [G loss: 2.249564]\n",
      "epoch:10 step:9647 [D loss: 0.642941, acc: 61.72%] [G loss: 2.043985]\n",
      "epoch:10 step:9648 [D loss: 0.649431, acc: 60.16%] [G loss: 2.029922]\n",
      "epoch:10 step:9649 [D loss: 0.672940, acc: 60.94%] [G loss: 1.976474]\n",
      "epoch:10 step:9650 [D loss: 0.638610, acc: 67.19%] [G loss: 2.064718]\n",
      "epoch:10 step:9651 [D loss: 0.662359, acc: 56.25%] [G loss: 1.981763]\n",
      "epoch:10 step:9652 [D loss: 0.655644, acc: 62.50%] [G loss: 2.084880]\n",
      "epoch:10 step:9653 [D loss: 0.603105, acc: 69.53%] [G loss: 2.104876]\n",
      "epoch:10 step:9654 [D loss: 0.628887, acc: 63.28%] [G loss: 2.180152]\n",
      "epoch:10 step:9655 [D loss: 0.617628, acc: 68.75%] [G loss: 2.277387]\n",
      "epoch:10 step:9656 [D loss: 0.559021, acc: 74.22%] [G loss: 2.153661]\n",
      "epoch:10 step:9657 [D loss: 0.593515, acc: 65.62%] [G loss: 2.087707]\n",
      "epoch:10 step:9658 [D loss: 0.661209, acc: 59.38%] [G loss: 2.202889]\n",
      "epoch:10 step:9659 [D loss: 0.604559, acc: 74.22%] [G loss: 2.097201]\n",
      "epoch:10 step:9660 [D loss: 0.627403, acc: 65.62%] [G loss: 1.991419]\n",
      "epoch:10 step:9661 [D loss: 0.676994, acc: 63.28%] [G loss: 2.183469]\n",
      "epoch:10 step:9662 [D loss: 0.645436, acc: 64.84%] [G loss: 2.023567]\n",
      "epoch:10 step:9663 [D loss: 0.657910, acc: 62.50%] [G loss: 2.273036]\n",
      "epoch:10 step:9664 [D loss: 0.641605, acc: 63.28%] [G loss: 2.161011]\n",
      "epoch:10 step:9665 [D loss: 0.673122, acc: 61.72%] [G loss: 1.973282]\n",
      "epoch:10 step:9666 [D loss: 0.588567, acc: 71.09%] [G loss: 2.214737]\n",
      "epoch:10 step:9667 [D loss: 0.611501, acc: 66.41%] [G loss: 2.169503]\n",
      "epoch:10 step:9668 [D loss: 0.607676, acc: 66.41%] [G loss: 2.085620]\n",
      "epoch:10 step:9669 [D loss: 0.618909, acc: 66.41%] [G loss: 2.101166]\n",
      "epoch:10 step:9670 [D loss: 0.623661, acc: 65.62%] [G loss: 1.986718]\n",
      "epoch:10 step:9671 [D loss: 0.614907, acc: 60.94%] [G loss: 1.990614]\n",
      "epoch:10 step:9672 [D loss: 0.654425, acc: 61.72%] [G loss: 1.969521]\n",
      "epoch:10 step:9673 [D loss: 0.633010, acc: 62.50%] [G loss: 2.068054]\n",
      "epoch:10 step:9674 [D loss: 0.650432, acc: 62.50%] [G loss: 1.898739]\n",
      "epoch:10 step:9675 [D loss: 0.622421, acc: 64.84%] [G loss: 2.092182]\n",
      "epoch:10 step:9676 [D loss: 0.632936, acc: 59.38%] [G loss: 2.122420]\n",
      "epoch:10 step:9677 [D loss: 0.620785, acc: 70.31%] [G loss: 2.011731]\n",
      "epoch:10 step:9678 [D loss: 0.662207, acc: 64.06%] [G loss: 1.898117]\n",
      "epoch:10 step:9679 [D loss: 0.565357, acc: 71.09%] [G loss: 2.181649]\n",
      "epoch:10 step:9680 [D loss: 0.633934, acc: 62.50%] [G loss: 2.061808]\n",
      "epoch:10 step:9681 [D loss: 0.697324, acc: 62.50%] [G loss: 1.979989]\n",
      "epoch:10 step:9682 [D loss: 0.541015, acc: 75.00%] [G loss: 2.335487]\n",
      "epoch:10 step:9683 [D loss: 0.570859, acc: 68.75%] [G loss: 2.596207]\n",
      "epoch:10 step:9684 [D loss: 0.564379, acc: 71.09%] [G loss: 2.530475]\n",
      "epoch:10 step:9685 [D loss: 0.580847, acc: 66.41%] [G loss: 2.399407]\n",
      "epoch:10 step:9686 [D loss: 0.687330, acc: 60.16%] [G loss: 1.886205]\n",
      "epoch:10 step:9687 [D loss: 0.682970, acc: 59.38%] [G loss: 1.993675]\n",
      "epoch:10 step:9688 [D loss: 0.625165, acc: 61.72%] [G loss: 2.279467]\n",
      "epoch:10 step:9689 [D loss: 0.696921, acc: 56.25%] [G loss: 1.900241]\n",
      "epoch:10 step:9690 [D loss: 0.648446, acc: 65.62%] [G loss: 2.213989]\n",
      "epoch:10 step:9691 [D loss: 0.575011, acc: 69.53%] [G loss: 2.201901]\n",
      "epoch:10 step:9692 [D loss: 0.669236, acc: 63.28%] [G loss: 2.043689]\n",
      "epoch:10 step:9693 [D loss: 0.678373, acc: 56.25%] [G loss: 1.847554]\n",
      "epoch:10 step:9694 [D loss: 0.639500, acc: 61.72%] [G loss: 2.004842]\n",
      "epoch:10 step:9695 [D loss: 0.555413, acc: 76.56%] [G loss: 2.222135]\n",
      "epoch:10 step:9696 [D loss: 0.631566, acc: 64.06%] [G loss: 2.115077]\n",
      "epoch:10 step:9697 [D loss: 0.648124, acc: 62.50%] [G loss: 1.872903]\n",
      "epoch:10 step:9698 [D loss: 0.587155, acc: 71.09%] [G loss: 2.132307]\n",
      "epoch:10 step:9699 [D loss: 0.665777, acc: 64.06%] [G loss: 2.081133]\n",
      "epoch:10 step:9700 [D loss: 0.607004, acc: 64.84%] [G loss: 2.106161]\n",
      "epoch:10 step:9701 [D loss: 0.578304, acc: 70.31%] [G loss: 2.181479]\n",
      "epoch:10 step:9702 [D loss: 0.612007, acc: 67.19%] [G loss: 1.993338]\n",
      "epoch:10 step:9703 [D loss: 0.646587, acc: 60.94%] [G loss: 2.162515]\n",
      "epoch:10 step:9704 [D loss: 0.665638, acc: 60.94%] [G loss: 2.134339]\n",
      "epoch:10 step:9705 [D loss: 0.641676, acc: 61.72%] [G loss: 2.158777]\n",
      "epoch:10 step:9706 [D loss: 0.581854, acc: 71.09%] [G loss: 2.109955]\n",
      "epoch:10 step:9707 [D loss: 0.667564, acc: 60.16%] [G loss: 2.055929]\n",
      "epoch:10 step:9708 [D loss: 0.607284, acc: 68.75%] [G loss: 2.077881]\n",
      "epoch:10 step:9709 [D loss: 0.582492, acc: 67.97%] [G loss: 2.035879]\n",
      "epoch:10 step:9710 [D loss: 0.631206, acc: 66.41%] [G loss: 2.037252]\n",
      "epoch:10 step:9711 [D loss: 0.642498, acc: 60.94%] [G loss: 2.112224]\n",
      "epoch:10 step:9712 [D loss: 0.644450, acc: 61.72%] [G loss: 2.067563]\n",
      "epoch:10 step:9713 [D loss: 0.594390, acc: 72.66%] [G loss: 2.270051]\n",
      "epoch:10 step:9714 [D loss: 0.656310, acc: 60.94%] [G loss: 2.127055]\n",
      "epoch:10 step:9715 [D loss: 0.556931, acc: 71.09%] [G loss: 2.434848]\n",
      "epoch:10 step:9716 [D loss: 0.622211, acc: 71.09%] [G loss: 2.257509]\n",
      "epoch:10 step:9717 [D loss: 0.467266, acc: 82.81%] [G loss: 2.472779]\n",
      "epoch:10 step:9718 [D loss: 0.622168, acc: 63.28%] [G loss: 2.153542]\n",
      "epoch:10 step:9719 [D loss: 0.677752, acc: 53.91%] [G loss: 1.944718]\n",
      "epoch:10 step:9720 [D loss: 0.603385, acc: 66.41%] [G loss: 2.077419]\n",
      "epoch:10 step:9721 [D loss: 0.629282, acc: 65.62%] [G loss: 2.206969]\n",
      "epoch:10 step:9722 [D loss: 0.631398, acc: 67.19%] [G loss: 2.087019]\n",
      "epoch:10 step:9723 [D loss: 0.654952, acc: 57.81%] [G loss: 2.289623]\n",
      "epoch:10 step:9724 [D loss: 0.611262, acc: 68.75%] [G loss: 2.379843]\n",
      "epoch:10 step:9725 [D loss: 0.674798, acc: 61.72%] [G loss: 1.922025]\n",
      "epoch:10 step:9726 [D loss: 0.667552, acc: 62.50%] [G loss: 1.981666]\n",
      "epoch:10 step:9727 [D loss: 0.590475, acc: 71.09%] [G loss: 2.142227]\n",
      "epoch:10 step:9728 [D loss: 0.572872, acc: 68.75%] [G loss: 2.367517]\n",
      "epoch:10 step:9729 [D loss: 0.618031, acc: 64.84%] [G loss: 2.328015]\n",
      "epoch:10 step:9730 [D loss: 0.588074, acc: 71.88%] [G loss: 2.084908]\n",
      "epoch:10 step:9731 [D loss: 0.620594, acc: 63.28%] [G loss: 2.011540]\n",
      "epoch:10 step:9732 [D loss: 0.702411, acc: 53.91%] [G loss: 2.041891]\n",
      "epoch:10 step:9733 [D loss: 0.601521, acc: 63.28%] [G loss: 2.266380]\n",
      "epoch:10 step:9734 [D loss: 0.605767, acc: 67.19%] [G loss: 2.176088]\n",
      "epoch:10 step:9735 [D loss: 0.628041, acc: 64.84%] [G loss: 2.206738]\n",
      "epoch:10 step:9736 [D loss: 0.638903, acc: 59.38%] [G loss: 2.044330]\n",
      "epoch:10 step:9737 [D loss: 0.650617, acc: 61.72%] [G loss: 2.151694]\n",
      "epoch:10 step:9738 [D loss: 0.594173, acc: 64.84%] [G loss: 2.017259]\n",
      "epoch:10 step:9739 [D loss: 0.671127, acc: 56.25%] [G loss: 1.998593]\n",
      "epoch:10 step:9740 [D loss: 0.598593, acc: 71.09%] [G loss: 2.401082]\n",
      "epoch:10 step:9741 [D loss: 0.606643, acc: 67.19%] [G loss: 2.354665]\n",
      "epoch:10 step:9742 [D loss: 0.696784, acc: 60.16%] [G loss: 2.017141]\n",
      "epoch:10 step:9743 [D loss: 0.602109, acc: 69.53%] [G loss: 2.044017]\n",
      "epoch:10 step:9744 [D loss: 0.671136, acc: 63.28%] [G loss: 2.084331]\n",
      "epoch:10 step:9745 [D loss: 0.604494, acc: 67.97%] [G loss: 2.029390]\n",
      "epoch:10 step:9746 [D loss: 0.629950, acc: 65.62%] [G loss: 1.887771]\n",
      "epoch:10 step:9747 [D loss: 0.721243, acc: 47.66%] [G loss: 1.946712]\n",
      "epoch:10 step:9748 [D loss: 0.674062, acc: 56.25%] [G loss: 2.075896]\n",
      "epoch:10 step:9749 [D loss: 0.575191, acc: 72.66%] [G loss: 2.148056]\n",
      "epoch:10 step:9750 [D loss: 0.599273, acc: 64.06%] [G loss: 2.058553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9751 [D loss: 0.599064, acc: 71.09%] [G loss: 2.174625]\n",
      "epoch:10 step:9752 [D loss: 0.674810, acc: 64.06%] [G loss: 1.925545]\n",
      "epoch:10 step:9753 [D loss: 0.678065, acc: 63.28%] [G loss: 1.939932]\n",
      "epoch:10 step:9754 [D loss: 0.658338, acc: 60.94%] [G loss: 1.933367]\n",
      "epoch:10 step:9755 [D loss: 0.629017, acc: 62.50%] [G loss: 2.113611]\n",
      "epoch:10 step:9756 [D loss: 0.667512, acc: 60.16%] [G loss: 2.067436]\n",
      "epoch:10 step:9757 [D loss: 0.637638, acc: 67.19%] [G loss: 1.916708]\n",
      "epoch:10 step:9758 [D loss: 0.740372, acc: 55.47%] [G loss: 2.011289]\n",
      "epoch:10 step:9759 [D loss: 0.652231, acc: 59.38%] [G loss: 1.952222]\n",
      "epoch:10 step:9760 [D loss: 0.636699, acc: 63.28%] [G loss: 2.000320]\n",
      "epoch:10 step:9761 [D loss: 0.614085, acc: 67.97%] [G loss: 2.029099]\n",
      "epoch:10 step:9762 [D loss: 0.591180, acc: 64.06%] [G loss: 2.162636]\n",
      "epoch:10 step:9763 [D loss: 0.619338, acc: 68.75%] [G loss: 2.230354]\n",
      "epoch:10 step:9764 [D loss: 0.621907, acc: 64.06%] [G loss: 1.971733]\n",
      "epoch:10 step:9765 [D loss: 0.570559, acc: 75.78%] [G loss: 2.012861]\n",
      "epoch:10 step:9766 [D loss: 0.653997, acc: 60.94%] [G loss: 2.024264]\n",
      "epoch:10 step:9767 [D loss: 0.719249, acc: 53.12%] [G loss: 1.894297]\n",
      "epoch:10 step:9768 [D loss: 0.599310, acc: 62.50%] [G loss: 2.116234]\n",
      "epoch:10 step:9769 [D loss: 0.600740, acc: 70.31%] [G loss: 2.112736]\n",
      "epoch:10 step:9770 [D loss: 0.632307, acc: 64.06%] [G loss: 1.926798]\n",
      "epoch:10 step:9771 [D loss: 0.646948, acc: 63.28%] [G loss: 2.065015]\n",
      "epoch:10 step:9772 [D loss: 0.597421, acc: 72.66%] [G loss: 2.237185]\n",
      "epoch:10 step:9773 [D loss: 0.656913, acc: 64.84%] [G loss: 2.060497]\n",
      "epoch:10 step:9774 [D loss: 0.655287, acc: 59.38%] [G loss: 2.054405]\n",
      "epoch:10 step:9775 [D loss: 0.688409, acc: 59.38%] [G loss: 2.138991]\n",
      "epoch:10 step:9776 [D loss: 0.604870, acc: 68.75%] [G loss: 2.219614]\n",
      "epoch:10 step:9777 [D loss: 0.672696, acc: 58.59%] [G loss: 1.983459]\n",
      "epoch:10 step:9778 [D loss: 0.633300, acc: 60.94%] [G loss: 2.038413]\n",
      "epoch:10 step:9779 [D loss: 0.637123, acc: 61.72%] [G loss: 2.087200]\n",
      "epoch:10 step:9780 [D loss: 0.634443, acc: 59.38%] [G loss: 2.243089]\n",
      "epoch:10 step:9781 [D loss: 0.626013, acc: 60.94%] [G loss: 2.071490]\n",
      "epoch:10 step:9782 [D loss: 0.589128, acc: 67.97%] [G loss: 2.150731]\n",
      "epoch:10 step:9783 [D loss: 0.668173, acc: 60.16%] [G loss: 2.057851]\n",
      "epoch:10 step:9784 [D loss: 0.648375, acc: 63.28%] [G loss: 2.095241]\n",
      "epoch:10 step:9785 [D loss: 0.580901, acc: 70.31%] [G loss: 2.062013]\n",
      "epoch:10 step:9786 [D loss: 0.644181, acc: 64.06%] [G loss: 1.920874]\n",
      "epoch:10 step:9787 [D loss: 0.626132, acc: 63.28%] [G loss: 2.054657]\n",
      "epoch:10 step:9788 [D loss: 0.679399, acc: 59.38%] [G loss: 1.994677]\n",
      "epoch:10 step:9789 [D loss: 0.646547, acc: 64.06%] [G loss: 2.140264]\n",
      "epoch:10 step:9790 [D loss: 0.701290, acc: 54.69%] [G loss: 1.887560]\n",
      "epoch:10 step:9791 [D loss: 0.641822, acc: 60.16%] [G loss: 1.991579]\n",
      "epoch:10 step:9792 [D loss: 0.635209, acc: 64.06%] [G loss: 1.963279]\n",
      "epoch:10 step:9793 [D loss: 0.659780, acc: 59.38%] [G loss: 2.000979]\n",
      "epoch:10 step:9794 [D loss: 0.635173, acc: 63.28%] [G loss: 2.056153]\n",
      "epoch:10 step:9795 [D loss: 0.590070, acc: 71.88%] [G loss: 2.118388]\n",
      "epoch:10 step:9796 [D loss: 0.618838, acc: 67.19%] [G loss: 2.286887]\n",
      "epoch:10 step:9797 [D loss: 0.550222, acc: 74.22%] [G loss: 2.500949]\n",
      "epoch:10 step:9798 [D loss: 0.538734, acc: 75.78%] [G loss: 2.349626]\n",
      "epoch:10 step:9799 [D loss: 0.573958, acc: 67.19%] [G loss: 2.657754]\n",
      "epoch:10 step:9800 [D loss: 0.549618, acc: 69.53%] [G loss: 2.455028]\n",
      "epoch:10 step:9801 [D loss: 0.572220, acc: 70.31%] [G loss: 2.269902]\n",
      "epoch:10 step:9802 [D loss: 0.674199, acc: 57.81%] [G loss: 1.931783]\n",
      "epoch:10 step:9803 [D loss: 0.654230, acc: 60.94%] [G loss: 2.061080]\n",
      "epoch:10 step:9804 [D loss: 0.613389, acc: 62.50%] [G loss: 2.086447]\n",
      "epoch:10 step:9805 [D loss: 0.665570, acc: 64.06%] [G loss: 2.208364]\n",
      "epoch:10 step:9806 [D loss: 0.645443, acc: 62.50%] [G loss: 2.194301]\n",
      "epoch:10 step:9807 [D loss: 0.711884, acc: 55.47%] [G loss: 1.824508]\n",
      "epoch:10 step:9808 [D loss: 0.678076, acc: 59.38%] [G loss: 1.911071]\n",
      "epoch:10 step:9809 [D loss: 0.674151, acc: 62.50%] [G loss: 1.941957]\n",
      "epoch:10 step:9810 [D loss: 0.650067, acc: 59.38%] [G loss: 2.105772]\n",
      "epoch:10 step:9811 [D loss: 0.670969, acc: 62.50%] [G loss: 2.049303]\n",
      "epoch:10 step:9812 [D loss: 0.615130, acc: 64.06%] [G loss: 1.883218]\n",
      "epoch:10 step:9813 [D loss: 0.618163, acc: 64.06%] [G loss: 1.995112]\n",
      "epoch:10 step:9814 [D loss: 0.598654, acc: 70.31%] [G loss: 2.087183]\n",
      "epoch:10 step:9815 [D loss: 0.616761, acc: 62.50%] [G loss: 2.054584]\n",
      "epoch:10 step:9816 [D loss: 0.637858, acc: 65.62%] [G loss: 1.952010]\n",
      "epoch:10 step:9817 [D loss: 0.612076, acc: 67.97%] [G loss: 2.078435]\n",
      "epoch:10 step:9818 [D loss: 0.752710, acc: 49.22%] [G loss: 1.786891]\n",
      "epoch:10 step:9819 [D loss: 0.648046, acc: 60.94%] [G loss: 1.954917]\n",
      "epoch:10 step:9820 [D loss: 0.666205, acc: 65.62%] [G loss: 1.932673]\n",
      "epoch:10 step:9821 [D loss: 0.575676, acc: 69.53%] [G loss: 2.342008]\n",
      "epoch:10 step:9822 [D loss: 0.596736, acc: 70.31%] [G loss: 2.147520]\n",
      "epoch:10 step:9823 [D loss: 0.615137, acc: 65.62%] [G loss: 2.076470]\n",
      "epoch:10 step:9824 [D loss: 0.596921, acc: 64.06%] [G loss: 2.212256]\n",
      "epoch:10 step:9825 [D loss: 0.627346, acc: 64.84%] [G loss: 2.024630]\n",
      "epoch:10 step:9826 [D loss: 0.606264, acc: 69.53%] [G loss: 2.132709]\n",
      "epoch:10 step:9827 [D loss: 0.574503, acc: 69.53%] [G loss: 2.238171]\n",
      "epoch:10 step:9828 [D loss: 0.702458, acc: 61.72%] [G loss: 1.994059]\n",
      "epoch:10 step:9829 [D loss: 0.637010, acc: 64.84%] [G loss: 1.820197]\n",
      "epoch:10 step:9830 [D loss: 0.756833, acc: 49.22%] [G loss: 1.993170]\n",
      "epoch:10 step:9831 [D loss: 0.645322, acc: 63.28%] [G loss: 1.996369]\n",
      "epoch:10 step:9832 [D loss: 0.623665, acc: 66.41%] [G loss: 1.995104]\n",
      "epoch:10 step:9833 [D loss: 0.658222, acc: 60.94%] [G loss: 1.868227]\n",
      "epoch:10 step:9834 [D loss: 0.663095, acc: 57.81%] [G loss: 2.042764]\n",
      "epoch:10 step:9835 [D loss: 0.669278, acc: 62.50%] [G loss: 1.964038]\n",
      "epoch:10 step:9836 [D loss: 0.627907, acc: 65.62%] [G loss: 1.985731]\n",
      "epoch:10 step:9837 [D loss: 0.638487, acc: 63.28%] [G loss: 2.043962]\n",
      "epoch:10 step:9838 [D loss: 0.585434, acc: 69.53%] [G loss: 2.193400]\n",
      "epoch:10 step:9839 [D loss: 0.628988, acc: 69.53%] [G loss: 2.192444]\n",
      "epoch:10 step:9840 [D loss: 0.619059, acc: 70.31%] [G loss: 2.333860]\n",
      "epoch:10 step:9841 [D loss: 0.537028, acc: 75.78%] [G loss: 2.838790]\n",
      "epoch:10 step:9842 [D loss: 0.651148, acc: 67.19%] [G loss: 2.257629]\n",
      "epoch:10 step:9843 [D loss: 0.659799, acc: 62.50%] [G loss: 2.005100]\n",
      "epoch:10 step:9844 [D loss: 0.611078, acc: 64.84%] [G loss: 2.092335]\n",
      "epoch:10 step:9845 [D loss: 0.635065, acc: 61.72%] [G loss: 1.994802]\n",
      "epoch:10 step:9846 [D loss: 0.672749, acc: 69.53%] [G loss: 2.148483]\n",
      "epoch:10 step:9847 [D loss: 0.670960, acc: 61.72%] [G loss: 1.865676]\n",
      "epoch:10 step:9848 [D loss: 0.673213, acc: 59.38%] [G loss: 2.130349]\n",
      "epoch:10 step:9849 [D loss: 0.592963, acc: 69.53%] [G loss: 2.209978]\n",
      "epoch:10 step:9850 [D loss: 0.644448, acc: 60.16%] [G loss: 2.186790]\n",
      "epoch:10 step:9851 [D loss: 0.565296, acc: 72.66%] [G loss: 2.362075]\n",
      "epoch:10 step:9852 [D loss: 0.703723, acc: 56.25%] [G loss: 1.919115]\n",
      "epoch:10 step:9853 [D loss: 0.654904, acc: 60.94%] [G loss: 1.752033]\n",
      "epoch:10 step:9854 [D loss: 0.613593, acc: 67.19%] [G loss: 2.053238]\n",
      "epoch:10 step:9855 [D loss: 0.653780, acc: 61.72%] [G loss: 1.992026]\n",
      "epoch:10 step:9856 [D loss: 0.640984, acc: 63.28%] [G loss: 1.879296]\n",
      "epoch:10 step:9857 [D loss: 0.601129, acc: 71.88%] [G loss: 2.032192]\n",
      "epoch:10 step:9858 [D loss: 0.627245, acc: 68.75%] [G loss: 2.062487]\n",
      "epoch:10 step:9859 [D loss: 0.697802, acc: 54.69%] [G loss: 2.096474]\n",
      "epoch:10 step:9860 [D loss: 0.651692, acc: 60.94%] [G loss: 1.922964]\n",
      "epoch:10 step:9861 [D loss: 0.622533, acc: 61.72%] [G loss: 1.903996]\n",
      "epoch:10 step:9862 [D loss: 0.669496, acc: 57.81%] [G loss: 2.008561]\n",
      "epoch:10 step:9863 [D loss: 0.630613, acc: 62.50%] [G loss: 1.923412]\n",
      "epoch:10 step:9864 [D loss: 0.654639, acc: 63.28%] [G loss: 2.115558]\n",
      "epoch:10 step:9865 [D loss: 0.626585, acc: 67.19%] [G loss: 2.175503]\n",
      "epoch:10 step:9866 [D loss: 0.653913, acc: 57.81%] [G loss: 2.004496]\n",
      "epoch:10 step:9867 [D loss: 0.622859, acc: 64.84%] [G loss: 2.198157]\n",
      "epoch:10 step:9868 [D loss: 0.588899, acc: 68.75%] [G loss: 2.259003]\n",
      "epoch:10 step:9869 [D loss: 0.616897, acc: 61.72%] [G loss: 2.292933]\n",
      "epoch:10 step:9870 [D loss: 0.691390, acc: 58.59%] [G loss: 1.941563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9871 [D loss: 0.666630, acc: 54.69%] [G loss: 1.799381]\n",
      "epoch:10 step:9872 [D loss: 0.685245, acc: 59.38%] [G loss: 1.865868]\n",
      "epoch:10 step:9873 [D loss: 0.554669, acc: 75.78%] [G loss: 2.182811]\n",
      "epoch:10 step:9874 [D loss: 0.608561, acc: 64.84%] [G loss: 2.084379]\n",
      "epoch:10 step:9875 [D loss: 0.648185, acc: 63.28%] [G loss: 1.929213]\n",
      "epoch:10 step:9876 [D loss: 0.703588, acc: 56.25%] [G loss: 1.869532]\n",
      "epoch:10 step:9877 [D loss: 0.656829, acc: 62.50%] [G loss: 1.899436]\n",
      "epoch:10 step:9878 [D loss: 0.631830, acc: 67.97%] [G loss: 2.384185]\n",
      "epoch:10 step:9879 [D loss: 0.665300, acc: 60.94%] [G loss: 2.022338]\n",
      "epoch:10 step:9880 [D loss: 0.653369, acc: 64.06%] [G loss: 2.067503]\n",
      "epoch:10 step:9881 [D loss: 0.675333, acc: 60.94%] [G loss: 2.035183]\n",
      "epoch:10 step:9882 [D loss: 0.664866, acc: 60.94%] [G loss: 1.981749]\n",
      "epoch:10 step:9883 [D loss: 0.618249, acc: 62.50%] [G loss: 1.934818]\n",
      "epoch:10 step:9884 [D loss: 0.652180, acc: 58.59%] [G loss: 2.085895]\n",
      "epoch:10 step:9885 [D loss: 0.663071, acc: 59.38%] [G loss: 1.879454]\n",
      "epoch:10 step:9886 [D loss: 0.589234, acc: 69.53%] [G loss: 2.093125]\n",
      "epoch:10 step:9887 [D loss: 0.601003, acc: 66.41%] [G loss: 2.265912]\n",
      "epoch:10 step:9888 [D loss: 0.644647, acc: 60.94%] [G loss: 1.992391]\n",
      "epoch:10 step:9889 [D loss: 0.645108, acc: 64.84%] [G loss: 2.047184]\n",
      "epoch:10 step:9890 [D loss: 0.585431, acc: 71.09%] [G loss: 2.171043]\n",
      "epoch:10 step:9891 [D loss: 0.577969, acc: 70.31%] [G loss: 2.073403]\n",
      "epoch:10 step:9892 [D loss: 0.588147, acc: 66.41%] [G loss: 2.287429]\n",
      "epoch:10 step:9893 [D loss: 0.564667, acc: 71.88%] [G loss: 2.195906]\n",
      "epoch:10 step:9894 [D loss: 0.620965, acc: 64.06%] [G loss: 2.079381]\n",
      "epoch:10 step:9895 [D loss: 0.691359, acc: 54.69%] [G loss: 1.949462]\n",
      "epoch:10 step:9896 [D loss: 0.692680, acc: 60.16%] [G loss: 1.959580]\n",
      "epoch:10 step:9897 [D loss: 0.648036, acc: 63.28%] [G loss: 1.982773]\n",
      "epoch:10 step:9898 [D loss: 0.712572, acc: 57.81%] [G loss: 1.805844]\n",
      "epoch:10 step:9899 [D loss: 0.651837, acc: 63.28%] [G loss: 1.985127]\n",
      "epoch:10 step:9900 [D loss: 0.648082, acc: 63.28%] [G loss: 1.881765]\n",
      "epoch:10 step:9901 [D loss: 0.629493, acc: 60.94%] [G loss: 1.975666]\n",
      "epoch:10 step:9902 [D loss: 0.539578, acc: 74.22%] [G loss: 2.045784]\n",
      "epoch:10 step:9903 [D loss: 0.639722, acc: 64.84%] [G loss: 1.986068]\n",
      "epoch:10 step:9904 [D loss: 0.579143, acc: 71.09%] [G loss: 2.089698]\n",
      "epoch:10 step:9905 [D loss: 0.592865, acc: 66.41%] [G loss: 1.934384]\n",
      "epoch:10 step:9906 [D loss: 0.633447, acc: 63.28%] [G loss: 2.234862]\n",
      "epoch:10 step:9907 [D loss: 0.695991, acc: 58.59%] [G loss: 1.971355]\n",
      "epoch:10 step:9908 [D loss: 0.669526, acc: 60.94%] [G loss: 1.866602]\n",
      "epoch:10 step:9909 [D loss: 0.677236, acc: 58.59%] [G loss: 1.941655]\n",
      "epoch:10 step:9910 [D loss: 0.652566, acc: 62.50%] [G loss: 1.804563]\n",
      "epoch:10 step:9911 [D loss: 0.621006, acc: 62.50%] [G loss: 2.032334]\n",
      "epoch:10 step:9912 [D loss: 0.647851, acc: 61.72%] [G loss: 2.107298]\n",
      "epoch:10 step:9913 [D loss: 0.640162, acc: 60.16%] [G loss: 2.001268]\n",
      "epoch:10 step:9914 [D loss: 0.611214, acc: 64.06%] [G loss: 1.940770]\n",
      "epoch:10 step:9915 [D loss: 0.611360, acc: 66.41%] [G loss: 2.120459]\n",
      "epoch:10 step:9916 [D loss: 0.634747, acc: 63.28%] [G loss: 2.202201]\n",
      "epoch:10 step:9917 [D loss: 0.652084, acc: 59.38%] [G loss: 1.939320]\n",
      "epoch:10 step:9918 [D loss: 0.638436, acc: 63.28%] [G loss: 2.112214]\n",
      "epoch:10 step:9919 [D loss: 0.608418, acc: 70.31%] [G loss: 2.275393]\n",
      "epoch:10 step:9920 [D loss: 0.563948, acc: 70.31%] [G loss: 2.282977]\n",
      "epoch:10 step:9921 [D loss: 0.621004, acc: 66.41%] [G loss: 2.369833]\n",
      "epoch:10 step:9922 [D loss: 0.593797, acc: 69.53%] [G loss: 2.225837]\n",
      "epoch:10 step:9923 [D loss: 0.649698, acc: 63.28%] [G loss: 1.950008]\n",
      "epoch:10 step:9924 [D loss: 0.616655, acc: 66.41%] [G loss: 2.203127]\n",
      "epoch:10 step:9925 [D loss: 0.594618, acc: 67.19%] [G loss: 2.240917]\n",
      "epoch:10 step:9926 [D loss: 0.612275, acc: 67.97%] [G loss: 2.394073]\n",
      "epoch:10 step:9927 [D loss: 0.599467, acc: 64.84%] [G loss: 2.056861]\n",
      "epoch:10 step:9928 [D loss: 0.586127, acc: 70.31%] [G loss: 2.356354]\n",
      "epoch:10 step:9929 [D loss: 0.682568, acc: 63.28%] [G loss: 1.988624]\n",
      "epoch:10 step:9930 [D loss: 0.679699, acc: 57.03%] [G loss: 2.072580]\n",
      "epoch:10 step:9931 [D loss: 0.693945, acc: 56.25%] [G loss: 2.157178]\n",
      "epoch:10 step:9932 [D loss: 0.610972, acc: 65.62%] [G loss: 1.979110]\n",
      "epoch:10 step:9933 [D loss: 0.665459, acc: 64.06%] [G loss: 2.121818]\n",
      "epoch:10 step:9934 [D loss: 0.666226, acc: 67.19%] [G loss: 2.059094]\n",
      "epoch:10 step:9935 [D loss: 0.656295, acc: 60.94%] [G loss: 1.873201]\n",
      "epoch:10 step:9936 [D loss: 0.727811, acc: 46.88%] [G loss: 1.743603]\n",
      "epoch:10 step:9937 [D loss: 0.611009, acc: 65.62%] [G loss: 1.916481]\n",
      "epoch:10 step:9938 [D loss: 0.614251, acc: 62.50%] [G loss: 2.018122]\n",
      "epoch:10 step:9939 [D loss: 0.635560, acc: 63.28%] [G loss: 2.020481]\n",
      "epoch:10 step:9940 [D loss: 0.510764, acc: 75.78%] [G loss: 2.061804]\n",
      "epoch:10 step:9941 [D loss: 0.657679, acc: 64.06%] [G loss: 2.181612]\n",
      "epoch:10 step:9942 [D loss: 0.682230, acc: 60.16%] [G loss: 1.848309]\n",
      "epoch:10 step:9943 [D loss: 0.623786, acc: 70.31%] [G loss: 1.961781]\n",
      "epoch:10 step:9944 [D loss: 0.592472, acc: 67.19%] [G loss: 2.072367]\n",
      "epoch:10 step:9945 [D loss: 0.624645, acc: 65.62%] [G loss: 2.065409]\n",
      "epoch:10 step:9946 [D loss: 0.650484, acc: 66.41%] [G loss: 2.021895]\n",
      "epoch:10 step:9947 [D loss: 0.634285, acc: 62.50%] [G loss: 1.978162]\n",
      "epoch:10 step:9948 [D loss: 0.667075, acc: 58.59%] [G loss: 1.763233]\n",
      "epoch:10 step:9949 [D loss: 0.639892, acc: 63.28%] [G loss: 1.951791]\n",
      "epoch:10 step:9950 [D loss: 0.679189, acc: 61.72%] [G loss: 2.042253]\n",
      "epoch:10 step:9951 [D loss: 0.646881, acc: 56.25%] [G loss: 2.022214]\n",
      "epoch:10 step:9952 [D loss: 0.609169, acc: 69.53%] [G loss: 1.984751]\n",
      "epoch:10 step:9953 [D loss: 0.686296, acc: 61.72%] [G loss: 1.863312]\n",
      "epoch:10 step:9954 [D loss: 0.637136, acc: 60.16%] [G loss: 2.133733]\n",
      "epoch:10 step:9955 [D loss: 0.614691, acc: 63.28%] [G loss: 2.030533]\n",
      "epoch:10 step:9956 [D loss: 0.613289, acc: 68.75%] [G loss: 2.007483]\n",
      "epoch:10 step:9957 [D loss: 0.653347, acc: 59.38%] [G loss: 2.031186]\n",
      "epoch:10 step:9958 [D loss: 0.619225, acc: 64.84%] [G loss: 2.100866]\n",
      "epoch:10 step:9959 [D loss: 0.601574, acc: 63.28%] [G loss: 1.951466]\n",
      "epoch:10 step:9960 [D loss: 0.608980, acc: 64.06%] [G loss: 1.987309]\n",
      "epoch:10 step:9961 [D loss: 0.684222, acc: 57.03%] [G loss: 1.948357]\n",
      "epoch:10 step:9962 [D loss: 0.639867, acc: 65.62%] [G loss: 2.062570]\n",
      "epoch:10 step:9963 [D loss: 0.624655, acc: 62.50%] [G loss: 1.902997]\n",
      "epoch:10 step:9964 [D loss: 0.648696, acc: 64.06%] [G loss: 1.948101]\n",
      "epoch:10 step:9965 [D loss: 0.670853, acc: 57.03%] [G loss: 1.982010]\n",
      "epoch:10 step:9966 [D loss: 0.652623, acc: 60.94%] [G loss: 1.880601]\n",
      "epoch:10 step:9967 [D loss: 0.608467, acc: 67.97%] [G loss: 2.047709]\n",
      "epoch:10 step:9968 [D loss: 0.634218, acc: 64.06%] [G loss: 2.012068]\n",
      "epoch:10 step:9969 [D loss: 0.668709, acc: 64.84%] [G loss: 1.909114]\n",
      "epoch:10 step:9970 [D loss: 0.671969, acc: 65.62%] [G loss: 2.095109]\n",
      "epoch:10 step:9971 [D loss: 0.703878, acc: 61.72%] [G loss: 2.162368]\n",
      "epoch:10 step:9972 [D loss: 0.661720, acc: 60.94%] [G loss: 2.116516]\n",
      "epoch:10 step:9973 [D loss: 0.585524, acc: 69.53%] [G loss: 2.165685]\n",
      "epoch:10 step:9974 [D loss: 0.659187, acc: 62.50%] [G loss: 1.891483]\n",
      "epoch:10 step:9975 [D loss: 0.624957, acc: 61.72%] [G loss: 2.036893]\n",
      "epoch:10 step:9976 [D loss: 0.668483, acc: 58.59%] [G loss: 2.014268]\n",
      "epoch:10 step:9977 [D loss: 0.613543, acc: 69.53%] [G loss: 1.890331]\n",
      "epoch:10 step:9978 [D loss: 0.681351, acc: 59.38%] [G loss: 2.075491]\n",
      "epoch:10 step:9979 [D loss: 0.604617, acc: 65.62%] [G loss: 2.048826]\n",
      "epoch:10 step:9980 [D loss: 0.621447, acc: 66.41%] [G loss: 2.005732]\n",
      "epoch:10 step:9981 [D loss: 0.650411, acc: 62.50%] [G loss: 1.811604]\n",
      "epoch:10 step:9982 [D loss: 0.687531, acc: 57.81%] [G loss: 1.947119]\n",
      "epoch:10 step:9983 [D loss: 0.544763, acc: 72.66%] [G loss: 1.963527]\n",
      "epoch:10 step:9984 [D loss: 0.656032, acc: 61.72%] [G loss: 1.994687]\n",
      "epoch:10 step:9985 [D loss: 0.666365, acc: 64.06%] [G loss: 1.881561]\n",
      "epoch:10 step:9986 [D loss: 0.659516, acc: 65.62%] [G loss: 2.008520]\n",
      "epoch:10 step:9987 [D loss: 0.625449, acc: 61.72%] [G loss: 1.958154]\n",
      "epoch:10 step:9988 [D loss: 0.614956, acc: 65.62%] [G loss: 1.931933]\n",
      "epoch:10 step:9989 [D loss: 0.686706, acc: 59.38%] [G loss: 1.874432]\n",
      "epoch:10 step:9990 [D loss: 0.701972, acc: 55.47%] [G loss: 2.023486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:9991 [D loss: 0.610152, acc: 66.41%] [G loss: 1.866624]\n",
      "epoch:10 step:9992 [D loss: 0.608606, acc: 64.06%] [G loss: 1.944933]\n",
      "epoch:10 step:9993 [D loss: 0.625824, acc: 66.41%] [G loss: 2.124745]\n",
      "epoch:10 step:9994 [D loss: 0.596458, acc: 67.97%] [G loss: 1.996335]\n",
      "epoch:10 step:9995 [D loss: 0.669594, acc: 60.94%] [G loss: 1.836349]\n",
      "epoch:10 step:9996 [D loss: 0.603217, acc: 66.41%] [G loss: 2.000001]\n",
      "epoch:10 step:9997 [D loss: 0.708272, acc: 61.72%] [G loss: 1.976814]\n",
      "epoch:10 step:9998 [D loss: 0.654025, acc: 62.50%] [G loss: 1.867141]\n",
      "epoch:10 step:9999 [D loss: 0.632043, acc: 68.75%] [G loss: 1.973393]\n",
      "epoch:10 step:10000 [D loss: 0.611164, acc: 64.06%] [G loss: 2.184248]\n",
      "epoch:10 step:10001 [D loss: 0.620560, acc: 63.28%] [G loss: 2.062913]\n",
      "epoch:10 step:10002 [D loss: 0.634386, acc: 63.28%] [G loss: 2.107745]\n",
      "epoch:10 step:10003 [D loss: 0.581323, acc: 68.75%] [G loss: 2.069172]\n",
      "epoch:10 step:10004 [D loss: 0.622244, acc: 66.41%] [G loss: 2.131629]\n",
      "epoch:10 step:10005 [D loss: 0.590115, acc: 66.41%] [G loss: 2.182806]\n",
      "epoch:10 step:10006 [D loss: 0.679413, acc: 59.38%] [G loss: 2.002856]\n",
      "epoch:10 step:10007 [D loss: 0.611481, acc: 69.53%] [G loss: 1.985725]\n",
      "epoch:10 step:10008 [D loss: 0.585630, acc: 72.66%] [G loss: 2.006349]\n",
      "epoch:10 step:10009 [D loss: 0.601878, acc: 71.09%] [G loss: 2.133414]\n",
      "epoch:10 step:10010 [D loss: 0.583517, acc: 69.53%] [G loss: 2.133398]\n",
      "epoch:10 step:10011 [D loss: 0.609785, acc: 68.75%] [G loss: 2.110461]\n",
      "epoch:10 step:10012 [D loss: 0.633464, acc: 61.72%] [G loss: 2.318765]\n",
      "epoch:10 step:10013 [D loss: 0.551142, acc: 73.44%] [G loss: 2.166632]\n",
      "epoch:10 step:10014 [D loss: 0.643721, acc: 67.19%] [G loss: 2.037905]\n",
      "epoch:10 step:10015 [D loss: 0.625452, acc: 60.16%] [G loss: 2.097020]\n",
      "epoch:10 step:10016 [D loss: 0.582927, acc: 69.53%] [G loss: 2.294718]\n",
      "epoch:10 step:10017 [D loss: 0.594194, acc: 64.06%] [G loss: 2.545462]\n",
      "epoch:10 step:10018 [D loss: 0.506573, acc: 75.78%] [G loss: 2.604352]\n",
      "epoch:10 step:10019 [D loss: 0.545102, acc: 71.88%] [G loss: 2.319922]\n",
      "epoch:10 step:10020 [D loss: 0.580657, acc: 69.53%] [G loss: 2.380447]\n",
      "epoch:10 step:10021 [D loss: 0.633836, acc: 62.50%] [G loss: 2.139740]\n",
      "epoch:10 step:10022 [D loss: 0.612453, acc: 67.19%] [G loss: 2.173822]\n",
      "epoch:10 step:10023 [D loss: 0.607784, acc: 65.62%] [G loss: 2.250097]\n",
      "epoch:10 step:10024 [D loss: 0.636295, acc: 66.41%] [G loss: 2.121422]\n",
      "epoch:10 step:10025 [D loss: 0.701397, acc: 61.72%] [G loss: 2.106241]\n",
      "epoch:10 step:10026 [D loss: 0.670850, acc: 58.59%] [G loss: 2.087173]\n",
      "epoch:10 step:10027 [D loss: 0.661182, acc: 58.59%] [G loss: 1.909846]\n",
      "epoch:10 step:10028 [D loss: 0.608653, acc: 68.75%] [G loss: 1.962973]\n",
      "epoch:10 step:10029 [D loss: 0.728951, acc: 54.69%] [G loss: 1.963460]\n",
      "epoch:10 step:10030 [D loss: 0.639835, acc: 64.84%] [G loss: 2.109939]\n",
      "epoch:10 step:10031 [D loss: 0.654492, acc: 64.06%] [G loss: 1.974126]\n",
      "epoch:10 step:10032 [D loss: 0.648413, acc: 63.28%] [G loss: 2.080785]\n",
      "epoch:10 step:10033 [D loss: 0.627826, acc: 67.19%] [G loss: 2.165206]\n",
      "epoch:10 step:10034 [D loss: 0.629755, acc: 70.31%] [G loss: 2.070035]\n",
      "epoch:10 step:10035 [D loss: 0.586111, acc: 69.53%] [G loss: 2.153592]\n",
      "epoch:10 step:10036 [D loss: 0.688112, acc: 57.81%] [G loss: 1.880582]\n",
      "epoch:10 step:10037 [D loss: 0.673137, acc: 60.16%] [G loss: 1.829256]\n",
      "epoch:10 step:10038 [D loss: 0.659058, acc: 59.38%] [G loss: 2.036185]\n",
      "epoch:10 step:10039 [D loss: 0.612580, acc: 64.84%] [G loss: 1.963246]\n",
      "epoch:10 step:10040 [D loss: 0.659474, acc: 58.59%] [G loss: 1.907197]\n",
      "epoch:10 step:10041 [D loss: 0.678125, acc: 60.16%] [G loss: 1.826417]\n",
      "epoch:10 step:10042 [D loss: 0.628325, acc: 65.62%] [G loss: 1.990802]\n",
      "epoch:10 step:10043 [D loss: 0.644647, acc: 64.84%] [G loss: 1.904829]\n",
      "epoch:10 step:10044 [D loss: 0.611981, acc: 66.41%] [G loss: 2.206123]\n",
      "epoch:10 step:10045 [D loss: 0.579653, acc: 71.88%] [G loss: 1.949581]\n",
      "epoch:10 step:10046 [D loss: 0.656547, acc: 58.59%] [G loss: 1.988582]\n",
      "epoch:10 step:10047 [D loss: 0.619848, acc: 64.06%] [G loss: 2.134380]\n",
      "epoch:10 step:10048 [D loss: 0.632235, acc: 57.81%] [G loss: 2.080513]\n",
      "epoch:10 step:10049 [D loss: 0.605407, acc: 67.19%] [G loss: 2.099833]\n",
      "epoch:10 step:10050 [D loss: 0.640162, acc: 62.50%] [G loss: 1.987060]\n",
      "epoch:10 step:10051 [D loss: 0.588294, acc: 71.09%] [G loss: 2.155563]\n",
      "epoch:10 step:10052 [D loss: 0.638097, acc: 59.38%] [G loss: 1.991947]\n",
      "epoch:10 step:10053 [D loss: 0.597219, acc: 63.28%] [G loss: 1.763791]\n",
      "epoch:10 step:10054 [D loss: 0.572364, acc: 68.75%] [G loss: 2.026696]\n",
      "epoch:10 step:10055 [D loss: 0.626658, acc: 68.75%] [G loss: 2.025778]\n",
      "epoch:10 step:10056 [D loss: 0.579649, acc: 71.88%] [G loss: 2.153665]\n",
      "epoch:10 step:10057 [D loss: 0.588744, acc: 66.41%] [G loss: 1.943471]\n",
      "epoch:10 step:10058 [D loss: 0.630090, acc: 66.41%] [G loss: 1.970917]\n",
      "epoch:10 step:10059 [D loss: 0.607479, acc: 66.41%] [G loss: 2.136940]\n",
      "epoch:10 step:10060 [D loss: 0.691820, acc: 55.47%] [G loss: 1.901263]\n",
      "epoch:10 step:10061 [D loss: 0.545736, acc: 71.88%] [G loss: 2.128257]\n",
      "epoch:10 step:10062 [D loss: 0.551028, acc: 71.88%] [G loss: 2.258305]\n",
      "epoch:10 step:10063 [D loss: 0.616441, acc: 69.53%] [G loss: 2.259483]\n",
      "epoch:10 step:10064 [D loss: 0.610067, acc: 64.84%] [G loss: 2.467033]\n",
      "epoch:10 step:10065 [D loss: 0.655259, acc: 58.59%] [G loss: 2.049688]\n",
      "epoch:10 step:10066 [D loss: 0.675010, acc: 64.84%] [G loss: 2.040945]\n",
      "epoch:10 step:10067 [D loss: 0.609358, acc: 67.97%] [G loss: 2.094704]\n",
      "epoch:10 step:10068 [D loss: 0.675869, acc: 58.59%] [G loss: 2.043888]\n",
      "epoch:10 step:10069 [D loss: 0.666947, acc: 61.72%] [G loss: 2.078630]\n",
      "epoch:10 step:10070 [D loss: 0.590461, acc: 67.97%] [G loss: 1.908469]\n",
      "epoch:10 step:10071 [D loss: 0.605475, acc: 68.75%] [G loss: 2.060509]\n",
      "epoch:10 step:10072 [D loss: 0.643257, acc: 57.03%] [G loss: 1.786925]\n",
      "epoch:10 step:10073 [D loss: 0.716618, acc: 51.56%] [G loss: 1.929486]\n",
      "epoch:10 step:10074 [D loss: 0.620292, acc: 62.50%] [G loss: 1.903824]\n",
      "epoch:10 step:10075 [D loss: 0.692639, acc: 58.59%] [G loss: 1.954167]\n",
      "epoch:10 step:10076 [D loss: 0.667040, acc: 64.06%] [G loss: 2.042772]\n",
      "epoch:10 step:10077 [D loss: 0.574173, acc: 68.75%] [G loss: 2.307250]\n",
      "epoch:10 step:10078 [D loss: 0.616947, acc: 70.31%] [G loss: 2.258301]\n",
      "epoch:10 step:10079 [D loss: 0.667404, acc: 64.06%] [G loss: 2.121765]\n",
      "epoch:10 step:10080 [D loss: 0.668575, acc: 57.03%] [G loss: 1.847166]\n",
      "epoch:10 step:10081 [D loss: 0.667710, acc: 60.94%] [G loss: 1.978893]\n",
      "epoch:10 step:10082 [D loss: 0.607529, acc: 60.16%] [G loss: 1.996043]\n",
      "epoch:10 step:10083 [D loss: 0.645406, acc: 60.94%] [G loss: 2.081316]\n",
      "epoch:10 step:10084 [D loss: 0.595879, acc: 69.53%] [G loss: 2.030348]\n",
      "epoch:10 step:10085 [D loss: 0.632715, acc: 67.97%] [G loss: 2.315465]\n",
      "epoch:10 step:10086 [D loss: 0.697075, acc: 64.06%] [G loss: 1.893845]\n",
      "epoch:10 step:10087 [D loss: 0.667415, acc: 59.38%] [G loss: 1.888545]\n",
      "epoch:10 step:10088 [D loss: 0.689341, acc: 57.03%] [G loss: 2.099435]\n",
      "epoch:10 step:10089 [D loss: 0.622229, acc: 65.62%] [G loss: 2.209818]\n",
      "epoch:10 step:10090 [D loss: 0.644753, acc: 64.84%] [G loss: 2.047394]\n",
      "epoch:10 step:10091 [D loss: 0.648891, acc: 60.94%] [G loss: 1.906989]\n",
      "epoch:10 step:10092 [D loss: 0.643771, acc: 66.41%] [G loss: 2.100038]\n",
      "epoch:10 step:10093 [D loss: 0.613671, acc: 67.97%] [G loss: 1.928879]\n",
      "epoch:10 step:10094 [D loss: 0.664758, acc: 61.72%] [G loss: 2.034821]\n",
      "epoch:10 step:10095 [D loss: 0.579314, acc: 71.09%] [G loss: 2.143409]\n",
      "epoch:10 step:10096 [D loss: 0.681792, acc: 64.84%] [G loss: 1.974779]\n",
      "epoch:10 step:10097 [D loss: 0.609100, acc: 68.75%] [G loss: 1.928129]\n",
      "epoch:10 step:10098 [D loss: 0.642977, acc: 66.41%] [G loss: 2.208951]\n",
      "epoch:10 step:10099 [D loss: 0.616393, acc: 67.19%] [G loss: 1.912400]\n",
      "epoch:10 step:10100 [D loss: 0.638818, acc: 60.16%] [G loss: 1.956886]\n",
      "epoch:10 step:10101 [D loss: 0.642182, acc: 57.81%] [G loss: 2.020313]\n",
      "epoch:10 step:10102 [D loss: 0.587532, acc: 70.31%] [G loss: 1.891815]\n",
      "epoch:10 step:10103 [D loss: 0.635749, acc: 62.50%] [G loss: 2.054691]\n",
      "epoch:10 step:10104 [D loss: 0.628551, acc: 66.41%] [G loss: 1.984460]\n",
      "epoch:10 step:10105 [D loss: 0.618414, acc: 62.50%] [G loss: 2.119478]\n",
      "epoch:10 step:10106 [D loss: 0.629562, acc: 70.31%] [G loss: 1.988645]\n",
      "epoch:10 step:10107 [D loss: 0.574067, acc: 70.31%] [G loss: 1.966052]\n",
      "epoch:10 step:10108 [D loss: 0.628484, acc: 59.38%] [G loss: 2.039431]\n",
      "epoch:10 step:10109 [D loss: 0.624458, acc: 66.41%] [G loss: 1.948813]\n",
      "epoch:10 step:10110 [D loss: 0.592826, acc: 70.31%] [G loss: 2.017871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10111 [D loss: 0.709563, acc: 57.03%] [G loss: 1.881682]\n",
      "epoch:10 step:10112 [D loss: 0.704178, acc: 61.72%] [G loss: 1.947013]\n",
      "epoch:10 step:10113 [D loss: 0.649158, acc: 59.38%] [G loss: 1.900100]\n",
      "epoch:10 step:10114 [D loss: 0.644333, acc: 63.28%] [G loss: 2.054749]\n",
      "epoch:10 step:10115 [D loss: 0.653782, acc: 64.84%] [G loss: 2.043859]\n",
      "epoch:10 step:10116 [D loss: 0.633837, acc: 64.84%] [G loss: 2.226399]\n",
      "epoch:10 step:10117 [D loss: 0.550018, acc: 74.22%] [G loss: 1.956174]\n",
      "epoch:10 step:10118 [D loss: 0.643466, acc: 63.28%] [G loss: 1.900554]\n",
      "epoch:10 step:10119 [D loss: 0.634552, acc: 67.97%] [G loss: 1.944937]\n",
      "epoch:10 step:10120 [D loss: 0.639319, acc: 64.06%] [G loss: 1.969907]\n",
      "epoch:10 step:10121 [D loss: 0.634436, acc: 69.53%] [G loss: 2.143154]\n",
      "epoch:10 step:10122 [D loss: 0.629613, acc: 64.84%] [G loss: 1.911219]\n",
      "epoch:10 step:10123 [D loss: 0.595991, acc: 67.97%] [G loss: 1.915141]\n",
      "epoch:10 step:10124 [D loss: 0.592862, acc: 70.31%] [G loss: 2.115768]\n",
      "epoch:10 step:10125 [D loss: 0.661533, acc: 57.03%] [G loss: 1.915965]\n",
      "epoch:10 step:10126 [D loss: 0.616237, acc: 67.19%] [G loss: 1.901278]\n",
      "epoch:10 step:10127 [D loss: 0.628652, acc: 64.84%] [G loss: 2.020881]\n",
      "epoch:10 step:10128 [D loss: 0.691479, acc: 60.94%] [G loss: 1.924220]\n",
      "epoch:10 step:10129 [D loss: 0.687092, acc: 56.25%] [G loss: 1.978338]\n",
      "epoch:10 step:10130 [D loss: 0.582157, acc: 68.75%] [G loss: 1.948531]\n",
      "epoch:10 step:10131 [D loss: 0.604684, acc: 67.19%] [G loss: 2.036016]\n",
      "epoch:10 step:10132 [D loss: 0.619937, acc: 63.28%] [G loss: 2.021461]\n",
      "epoch:10 step:10133 [D loss: 0.664659, acc: 60.94%] [G loss: 1.909786]\n",
      "epoch:10 step:10134 [D loss: 0.691401, acc: 53.91%] [G loss: 1.908848]\n",
      "epoch:10 step:10135 [D loss: 0.717014, acc: 53.91%] [G loss: 1.863817]\n",
      "epoch:10 step:10136 [D loss: 0.662724, acc: 60.94%] [G loss: 1.898710]\n",
      "epoch:10 step:10137 [D loss: 0.630681, acc: 60.94%] [G loss: 2.139470]\n",
      "epoch:10 step:10138 [D loss: 0.630555, acc: 63.28%] [G loss: 1.884793]\n",
      "epoch:10 step:10139 [D loss: 0.645046, acc: 64.06%] [G loss: 1.969547]\n",
      "epoch:10 step:10140 [D loss: 0.687736, acc: 63.28%] [G loss: 1.914787]\n",
      "epoch:10 step:10141 [D loss: 0.643646, acc: 57.03%] [G loss: 1.985657]\n",
      "epoch:10 step:10142 [D loss: 0.595405, acc: 67.19%] [G loss: 1.983433]\n",
      "epoch:10 step:10143 [D loss: 0.663806, acc: 61.72%] [G loss: 1.983387]\n",
      "epoch:10 step:10144 [D loss: 0.661347, acc: 57.03%] [G loss: 2.066952]\n",
      "epoch:10 step:10145 [D loss: 0.579552, acc: 69.53%] [G loss: 2.008726]\n",
      "epoch:10 step:10146 [D loss: 0.645657, acc: 63.28%] [G loss: 2.033522]\n",
      "epoch:10 step:10147 [D loss: 0.617442, acc: 64.84%] [G loss: 1.982123]\n",
      "epoch:10 step:10148 [D loss: 0.650740, acc: 60.94%] [G loss: 2.001976]\n",
      "epoch:10 step:10149 [D loss: 0.618430, acc: 69.53%] [G loss: 2.037137]\n",
      "epoch:10 step:10150 [D loss: 0.639452, acc: 65.62%] [G loss: 2.034503]\n",
      "epoch:10 step:10151 [D loss: 0.615973, acc: 60.94%] [G loss: 2.244766]\n",
      "epoch:10 step:10152 [D loss: 0.596312, acc: 67.97%] [G loss: 2.292152]\n",
      "epoch:10 step:10153 [D loss: 0.657175, acc: 60.94%] [G loss: 1.962124]\n",
      "epoch:10 step:10154 [D loss: 0.712731, acc: 57.03%] [G loss: 1.907273]\n",
      "epoch:10 step:10155 [D loss: 0.647877, acc: 61.72%] [G loss: 2.023832]\n",
      "epoch:10 step:10156 [D loss: 0.601358, acc: 69.53%] [G loss: 2.023281]\n",
      "epoch:10 step:10157 [D loss: 0.660959, acc: 62.50%] [G loss: 2.104808]\n",
      "epoch:10 step:10158 [D loss: 0.622924, acc: 66.41%] [G loss: 1.954378]\n",
      "epoch:10 step:10159 [D loss: 0.662745, acc: 60.16%] [G loss: 2.045009]\n",
      "epoch:10 step:10160 [D loss: 0.591462, acc: 68.75%] [G loss: 2.089822]\n",
      "epoch:10 step:10161 [D loss: 0.655877, acc: 59.38%] [G loss: 2.011739]\n",
      "epoch:10 step:10162 [D loss: 0.644623, acc: 63.28%] [G loss: 2.158286]\n",
      "epoch:10 step:10163 [D loss: 0.594309, acc: 66.41%] [G loss: 2.145231]\n",
      "epoch:10 step:10164 [D loss: 0.669886, acc: 59.38%] [G loss: 1.834750]\n",
      "epoch:10 step:10165 [D loss: 0.681606, acc: 57.03%] [G loss: 1.986140]\n",
      "epoch:10 step:10166 [D loss: 0.639096, acc: 59.38%] [G loss: 1.894379]\n",
      "epoch:10 step:10167 [D loss: 0.679061, acc: 59.38%] [G loss: 1.873681]\n",
      "epoch:10 step:10168 [D loss: 0.586182, acc: 73.44%] [G loss: 2.238397]\n",
      "epoch:10 step:10169 [D loss: 0.713485, acc: 55.47%] [G loss: 1.979903]\n",
      "epoch:10 step:10170 [D loss: 0.748238, acc: 59.38%] [G loss: 1.879022]\n",
      "epoch:10 step:10171 [D loss: 0.721975, acc: 56.25%] [G loss: 1.771190]\n",
      "epoch:10 step:10172 [D loss: 0.616320, acc: 67.19%] [G loss: 1.958778]\n",
      "epoch:10 step:10173 [D loss: 0.629144, acc: 67.97%] [G loss: 1.934470]\n",
      "epoch:10 step:10174 [D loss: 0.620251, acc: 66.41%] [G loss: 1.952617]\n",
      "epoch:10 step:10175 [D loss: 0.638072, acc: 67.19%] [G loss: 2.112476]\n",
      "epoch:10 step:10176 [D loss: 0.630330, acc: 68.75%] [G loss: 2.093443]\n",
      "epoch:10 step:10177 [D loss: 0.664029, acc: 59.38%] [G loss: 2.106258]\n",
      "epoch:10 step:10178 [D loss: 0.653198, acc: 65.62%] [G loss: 1.966804]\n",
      "epoch:10 step:10179 [D loss: 0.667912, acc: 62.50%] [G loss: 2.031773]\n",
      "epoch:10 step:10180 [D loss: 0.612278, acc: 68.75%] [G loss: 1.948161]\n",
      "epoch:10 step:10181 [D loss: 0.627733, acc: 61.72%] [G loss: 2.086699]\n",
      "epoch:10 step:10182 [D loss: 0.645813, acc: 64.84%] [G loss: 1.801973]\n",
      "epoch:10 step:10183 [D loss: 0.602538, acc: 67.19%] [G loss: 1.923405]\n",
      "epoch:10 step:10184 [D loss: 0.655622, acc: 61.72%] [G loss: 2.047152]\n",
      "epoch:10 step:10185 [D loss: 0.579939, acc: 70.31%] [G loss: 2.393366]\n",
      "epoch:10 step:10186 [D loss: 0.574337, acc: 68.75%] [G loss: 2.285016]\n",
      "epoch:10 step:10187 [D loss: 0.695888, acc: 56.25%] [G loss: 2.048504]\n",
      "epoch:10 step:10188 [D loss: 0.629546, acc: 70.31%] [G loss: 1.971359]\n",
      "epoch:10 step:10189 [D loss: 0.604270, acc: 67.97%] [G loss: 2.081101]\n",
      "epoch:10 step:10190 [D loss: 0.756657, acc: 50.00%] [G loss: 1.800690]\n",
      "epoch:10 step:10191 [D loss: 0.639621, acc: 66.41%] [G loss: 1.868773]\n",
      "epoch:10 step:10192 [D loss: 0.598612, acc: 67.97%] [G loss: 1.967335]\n",
      "epoch:10 step:10193 [D loss: 0.655221, acc: 67.97%] [G loss: 2.101952]\n",
      "epoch:10 step:10194 [D loss: 0.660749, acc: 59.38%] [G loss: 1.860152]\n",
      "epoch:10 step:10195 [D loss: 0.656619, acc: 67.97%] [G loss: 2.123981]\n",
      "epoch:10 step:10196 [D loss: 0.680890, acc: 58.59%] [G loss: 1.889979]\n",
      "epoch:10 step:10197 [D loss: 0.688840, acc: 59.38%] [G loss: 1.926996]\n",
      "epoch:10 step:10198 [D loss: 0.660620, acc: 59.38%] [G loss: 1.872460]\n",
      "epoch:10 step:10199 [D loss: 0.709589, acc: 52.34%] [G loss: 1.925155]\n",
      "epoch:10 step:10200 [D loss: 0.632604, acc: 67.19%] [G loss: 1.910647]\n",
      "epoch:10 step:10201 [D loss: 0.614237, acc: 66.41%] [G loss: 1.975340]\n",
      "epoch:10 step:10202 [D loss: 0.606604, acc: 65.62%] [G loss: 1.890384]\n",
      "epoch:10 step:10203 [D loss: 0.607161, acc: 66.41%] [G loss: 2.102976]\n",
      "epoch:10 step:10204 [D loss: 0.656674, acc: 61.72%] [G loss: 1.982503]\n",
      "epoch:10 step:10205 [D loss: 0.643449, acc: 63.28%] [G loss: 1.933699]\n",
      "epoch:10 step:10206 [D loss: 0.651624, acc: 60.16%] [G loss: 1.973621]\n",
      "epoch:10 step:10207 [D loss: 0.599625, acc: 67.97%] [G loss: 1.962068]\n",
      "epoch:10 step:10208 [D loss: 0.624767, acc: 62.50%] [G loss: 1.906823]\n",
      "epoch:10 step:10209 [D loss: 0.662773, acc: 60.16%] [G loss: 2.046575]\n",
      "epoch:10 step:10210 [D loss: 0.619730, acc: 64.84%] [G loss: 1.973658]\n",
      "epoch:10 step:10211 [D loss: 0.646217, acc: 63.28%] [G loss: 2.087801]\n",
      "epoch:10 step:10212 [D loss: 0.562952, acc: 71.88%] [G loss: 2.017369]\n",
      "epoch:10 step:10213 [D loss: 0.646103, acc: 61.72%] [G loss: 2.038290]\n",
      "epoch:10 step:10214 [D loss: 0.736629, acc: 52.34%] [G loss: 1.919514]\n",
      "epoch:10 step:10215 [D loss: 0.599913, acc: 70.31%] [G loss: 2.046689]\n",
      "epoch:10 step:10216 [D loss: 0.621483, acc: 67.97%] [G loss: 1.827963]\n",
      "epoch:10 step:10217 [D loss: 0.658619, acc: 59.38%] [G loss: 1.999518]\n",
      "epoch:10 step:10218 [D loss: 0.610480, acc: 70.31%] [G loss: 2.171456]\n",
      "epoch:10 step:10219 [D loss: 0.573159, acc: 69.53%] [G loss: 2.089013]\n",
      "epoch:10 step:10220 [D loss: 0.669789, acc: 60.94%] [G loss: 2.014016]\n",
      "epoch:10 step:10221 [D loss: 0.621477, acc: 67.19%] [G loss: 2.050699]\n",
      "epoch:10 step:10222 [D loss: 0.580521, acc: 66.41%] [G loss: 2.249322]\n",
      "epoch:10 step:10223 [D loss: 0.626810, acc: 60.94%] [G loss: 2.120043]\n",
      "epoch:10 step:10224 [D loss: 0.611591, acc: 64.06%] [G loss: 1.957924]\n",
      "epoch:10 step:10225 [D loss: 0.681736, acc: 61.72%] [G loss: 1.942908]\n",
      "epoch:10 step:10226 [D loss: 0.629788, acc: 61.72%] [G loss: 1.949113]\n",
      "epoch:10 step:10227 [D loss: 0.614874, acc: 67.19%] [G loss: 2.145073]\n",
      "epoch:10 step:10228 [D loss: 0.709341, acc: 55.47%] [G loss: 1.994237]\n",
      "epoch:10 step:10229 [D loss: 0.698645, acc: 57.81%] [G loss: 2.019583]\n",
      "epoch:10 step:10230 [D loss: 0.616660, acc: 67.19%] [G loss: 2.044800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:10231 [D loss: 0.695971, acc: 57.81%] [G loss: 1.986092]\n",
      "epoch:10 step:10232 [D loss: 0.662099, acc: 62.50%] [G loss: 1.749393]\n",
      "epoch:10 step:10233 [D loss: 0.617026, acc: 59.38%] [G loss: 1.986838]\n",
      "epoch:10 step:10234 [D loss: 0.672413, acc: 53.91%] [G loss: 1.755937]\n",
      "epoch:10 step:10235 [D loss: 0.692400, acc: 57.03%] [G loss: 1.814585]\n",
      "epoch:10 step:10236 [D loss: 0.666017, acc: 55.47%] [G loss: 1.839722]\n",
      "epoch:10 step:10237 [D loss: 0.655717, acc: 61.72%] [G loss: 1.891764]\n",
      "epoch:10 step:10238 [D loss: 0.602158, acc: 67.97%] [G loss: 1.881847]\n",
      "epoch:10 step:10239 [D loss: 0.642159, acc: 60.94%] [G loss: 2.036289]\n",
      "epoch:10 step:10240 [D loss: 0.626758, acc: 64.84%] [G loss: 1.870155]\n",
      "epoch:10 step:10241 [D loss: 0.605955, acc: 66.41%] [G loss: 2.112059]\n",
      "epoch:10 step:10242 [D loss: 0.649293, acc: 62.50%] [G loss: 1.941101]\n",
      "epoch:10 step:10243 [D loss: 0.665886, acc: 60.94%] [G loss: 1.981348]\n",
      "epoch:10 step:10244 [D loss: 0.630747, acc: 65.62%] [G loss: 1.938996]\n",
      "epoch:10 step:10245 [D loss: 0.580415, acc: 71.09%] [G loss: 1.990102]\n",
      "epoch:10 step:10246 [D loss: 0.581182, acc: 72.66%] [G loss: 2.078138]\n",
      "epoch:10 step:10247 [D loss: 0.658099, acc: 65.62%] [G loss: 1.890220]\n",
      "epoch:10 step:10248 [D loss: 0.677389, acc: 57.03%] [G loss: 1.993904]\n",
      "epoch:10 step:10249 [D loss: 0.672761, acc: 57.03%] [G loss: 1.743510]\n",
      "epoch:10 step:10250 [D loss: 0.653479, acc: 55.47%] [G loss: 1.876987]\n",
      "epoch:10 step:10251 [D loss: 0.651409, acc: 63.28%] [G loss: 1.973106]\n",
      "epoch:10 step:10252 [D loss: 0.652355, acc: 57.81%] [G loss: 1.899678]\n",
      "epoch:10 step:10253 [D loss: 0.596247, acc: 70.31%] [G loss: 2.054698]\n",
      "epoch:10 step:10254 [D loss: 0.588564, acc: 68.75%] [G loss: 2.314242]\n",
      "epoch:10 step:10255 [D loss: 0.584720, acc: 67.19%] [G loss: 2.127758]\n",
      "epoch:10 step:10256 [D loss: 0.584044, acc: 64.06%] [G loss: 2.398156]\n",
      "epoch:10 step:10257 [D loss: 0.588650, acc: 70.31%] [G loss: 2.139122]\n",
      "epoch:10 step:10258 [D loss: 0.686575, acc: 58.59%] [G loss: 2.037273]\n",
      "epoch:10 step:10259 [D loss: 0.665193, acc: 59.38%] [G loss: 2.046671]\n",
      "epoch:10 step:10260 [D loss: 0.618234, acc: 64.84%] [G loss: 2.260921]\n",
      "epoch:10 step:10261 [D loss: 0.619976, acc: 71.09%] [G loss: 2.036879]\n",
      "epoch:10 step:10262 [D loss: 0.653636, acc: 57.81%] [G loss: 1.854258]\n",
      "epoch:10 step:10263 [D loss: 0.680182, acc: 60.94%] [G loss: 2.003592]\n",
      "epoch:10 step:10264 [D loss: 0.611589, acc: 67.97%] [G loss: 2.147669]\n",
      "epoch:10 step:10265 [D loss: 0.610890, acc: 68.75%] [G loss: 1.964301]\n",
      "epoch:10 step:10266 [D loss: 0.689715, acc: 54.69%] [G loss: 1.893169]\n",
      "epoch:10 step:10267 [D loss: 0.665960, acc: 62.50%] [G loss: 1.962231]\n",
      "epoch:10 step:10268 [D loss: 0.626347, acc: 64.06%] [G loss: 2.013139]\n",
      "epoch:10 step:10269 [D loss: 0.632541, acc: 61.72%] [G loss: 2.315474]\n",
      "epoch:10 step:10270 [D loss: 0.606375, acc: 69.53%] [G loss: 2.132920]\n",
      "epoch:10 step:10271 [D loss: 0.631210, acc: 59.38%] [G loss: 2.025043]\n",
      "epoch:10 step:10272 [D loss: 0.679162, acc: 58.59%] [G loss: 2.005270]\n",
      "epoch:10 step:10273 [D loss: 0.641295, acc: 57.81%] [G loss: 2.036520]\n",
      "epoch:10 step:10274 [D loss: 0.693041, acc: 61.72%] [G loss: 1.958719]\n",
      "epoch:10 step:10275 [D loss: 0.594997, acc: 69.53%] [G loss: 2.044035]\n",
      "epoch:10 step:10276 [D loss: 0.583594, acc: 68.75%] [G loss: 2.312789]\n",
      "epoch:10 step:10277 [D loss: 0.650943, acc: 68.75%] [G loss: 2.141122]\n",
      "epoch:10 step:10278 [D loss: 0.635012, acc: 64.84%] [G loss: 1.951657]\n",
      "epoch:10 step:10279 [D loss: 0.650045, acc: 65.62%] [G loss: 2.185001]\n",
      "epoch:10 step:10280 [D loss: 0.650394, acc: 62.50%] [G loss: 2.124728]\n",
      "epoch:10 step:10281 [D loss: 0.601200, acc: 68.75%] [G loss: 2.081351]\n",
      "epoch:10 step:10282 [D loss: 0.557743, acc: 74.22%] [G loss: 2.548640]\n",
      "epoch:10 step:10283 [D loss: 0.658130, acc: 60.16%] [G loss: 2.285967]\n",
      "epoch:10 step:10284 [D loss: 0.677658, acc: 64.84%] [G loss: 1.903151]\n",
      "epoch:10 step:10285 [D loss: 0.639702, acc: 63.28%] [G loss: 1.830502]\n",
      "epoch:10 step:10286 [D loss: 0.645067, acc: 63.28%] [G loss: 2.141095]\n",
      "epoch:10 step:10287 [D loss: 0.571844, acc: 68.75%] [G loss: 2.213153]\n",
      "epoch:10 step:10288 [D loss: 0.540965, acc: 71.09%] [G loss: 2.305654]\n",
      "epoch:10 step:10289 [D loss: 0.599090, acc: 68.75%] [G loss: 2.232157]\n",
      "epoch:10 step:10290 [D loss: 0.805543, acc: 47.66%] [G loss: 2.088203]\n",
      "epoch:10 step:10291 [D loss: 0.632249, acc: 60.16%] [G loss: 2.171412]\n",
      "epoch:10 step:10292 [D loss: 0.657951, acc: 59.38%] [G loss: 2.071280]\n",
      "epoch:10 step:10293 [D loss: 0.633984, acc: 59.38%] [G loss: 2.183680]\n",
      "epoch:10 step:10294 [D loss: 0.611962, acc: 68.75%] [G loss: 2.297665]\n",
      "epoch:10 step:10295 [D loss: 0.575303, acc: 76.56%] [G loss: 2.383738]\n",
      "epoch:10 step:10296 [D loss: 0.573778, acc: 69.53%] [G loss: 2.287138]\n",
      "epoch:10 step:10297 [D loss: 0.657740, acc: 64.06%] [G loss: 2.164290]\n",
      "epoch:10 step:10298 [D loss: 0.749603, acc: 53.12%] [G loss: 1.775370]\n",
      "epoch:10 step:10299 [D loss: 0.712033, acc: 49.22%] [G loss: 2.107349]\n",
      "epoch:10 step:10300 [D loss: 0.617779, acc: 66.41%] [G loss: 2.130631]\n",
      "epoch:10 step:10301 [D loss: 0.607785, acc: 67.97%] [G loss: 2.187118]\n",
      "epoch:10 step:10302 [D loss: 0.657383, acc: 60.94%] [G loss: 1.936505]\n",
      "epoch:10 step:10303 [D loss: 0.559879, acc: 75.78%] [G loss: 2.094498]\n",
      "epoch:10 step:10304 [D loss: 0.591542, acc: 65.62%] [G loss: 2.152868]\n",
      "epoch:10 step:10305 [D loss: 0.602004, acc: 67.19%] [G loss: 2.188736]\n",
      "epoch:10 step:10306 [D loss: 0.583888, acc: 66.41%] [G loss: 2.303733]\n",
      "epoch:10 step:10307 [D loss: 0.522184, acc: 77.34%] [G loss: 2.696739]\n",
      "epoch:11 step:10308 [D loss: 0.656035, acc: 61.72%] [G loss: 2.089118]\n",
      "epoch:11 step:10309 [D loss: 0.623009, acc: 67.19%] [G loss: 2.197301]\n",
      "epoch:11 step:10310 [D loss: 0.648167, acc: 63.28%] [G loss: 2.116755]\n",
      "epoch:11 step:10311 [D loss: 0.653573, acc: 62.50%] [G loss: 2.092405]\n",
      "epoch:11 step:10312 [D loss: 0.644232, acc: 63.28%] [G loss: 2.049853]\n",
      "epoch:11 step:10313 [D loss: 0.631332, acc: 62.50%] [G loss: 2.018204]\n",
      "epoch:11 step:10314 [D loss: 0.625098, acc: 62.50%] [G loss: 1.916703]\n",
      "epoch:11 step:10315 [D loss: 0.631911, acc: 69.53%] [G loss: 2.034492]\n",
      "epoch:11 step:10316 [D loss: 0.606464, acc: 65.62%] [G loss: 2.205514]\n",
      "epoch:11 step:10317 [D loss: 0.626072, acc: 64.84%] [G loss: 2.150591]\n",
      "epoch:11 step:10318 [D loss: 0.629421, acc: 65.62%] [G loss: 2.096363]\n",
      "epoch:11 step:10319 [D loss: 0.625716, acc: 62.50%] [G loss: 1.847355]\n",
      "epoch:11 step:10320 [D loss: 0.605734, acc: 68.75%] [G loss: 1.948923]\n",
      "epoch:11 step:10321 [D loss: 0.617417, acc: 67.19%] [G loss: 1.984332]\n",
      "epoch:11 step:10322 [D loss: 0.546865, acc: 68.75%] [G loss: 2.465022]\n",
      "epoch:11 step:10323 [D loss: 0.536944, acc: 75.00%] [G loss: 2.046162]\n",
      "epoch:11 step:10324 [D loss: 0.683391, acc: 59.38%] [G loss: 1.966083]\n",
      "epoch:11 step:10325 [D loss: 0.671322, acc: 57.03%] [G loss: 2.005545]\n",
      "epoch:11 step:10326 [D loss: 0.617172, acc: 66.41%] [G loss: 2.026643]\n",
      "epoch:11 step:10327 [D loss: 0.682289, acc: 60.16%] [G loss: 1.780953]\n",
      "epoch:11 step:10328 [D loss: 0.682836, acc: 60.94%] [G loss: 2.018588]\n",
      "epoch:11 step:10329 [D loss: 0.611844, acc: 65.62%] [G loss: 2.002626]\n",
      "epoch:11 step:10330 [D loss: 0.584644, acc: 71.88%] [G loss: 1.977021]\n",
      "epoch:11 step:10331 [D loss: 0.541051, acc: 74.22%] [G loss: 2.221307]\n",
      "epoch:11 step:10332 [D loss: 0.536778, acc: 74.22%] [G loss: 2.424864]\n",
      "epoch:11 step:10333 [D loss: 0.699599, acc: 58.59%] [G loss: 1.956675]\n",
      "epoch:11 step:10334 [D loss: 0.677723, acc: 59.38%] [G loss: 1.942971]\n",
      "epoch:11 step:10335 [D loss: 0.659326, acc: 60.94%] [G loss: 1.907583]\n",
      "epoch:11 step:10336 [D loss: 0.577729, acc: 71.09%] [G loss: 1.991266]\n",
      "epoch:11 step:10337 [D loss: 0.663461, acc: 52.34%] [G loss: 1.853058]\n",
      "epoch:11 step:10338 [D loss: 0.643605, acc: 60.16%] [G loss: 1.906193]\n",
      "epoch:11 step:10339 [D loss: 0.640815, acc: 67.19%] [G loss: 2.023946]\n",
      "epoch:11 step:10340 [D loss: 0.647206, acc: 67.19%] [G loss: 1.982256]\n",
      "epoch:11 step:10341 [D loss: 0.610162, acc: 67.97%] [G loss: 1.918753]\n",
      "epoch:11 step:10342 [D loss: 0.620592, acc: 61.72%] [G loss: 2.104346]\n",
      "epoch:11 step:10343 [D loss: 0.607026, acc: 69.53%] [G loss: 1.996938]\n",
      "epoch:11 step:10344 [D loss: 0.686126, acc: 58.59%] [G loss: 1.898821]\n",
      "epoch:11 step:10345 [D loss: 0.663161, acc: 59.38%] [G loss: 2.105544]\n",
      "epoch:11 step:10346 [D loss: 0.588527, acc: 65.62%] [G loss: 1.910501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10347 [D loss: 0.614523, acc: 66.41%] [G loss: 2.232151]\n",
      "epoch:11 step:10348 [D loss: 0.659894, acc: 64.06%] [G loss: 1.923248]\n",
      "epoch:11 step:10349 [D loss: 0.617845, acc: 65.62%] [G loss: 2.061111]\n",
      "epoch:11 step:10350 [D loss: 0.630141, acc: 65.62%] [G loss: 2.060994]\n",
      "epoch:11 step:10351 [D loss: 0.644337, acc: 63.28%] [G loss: 2.056154]\n",
      "epoch:11 step:10352 [D loss: 0.619721, acc: 59.38%] [G loss: 2.071449]\n",
      "epoch:11 step:10353 [D loss: 0.677494, acc: 60.94%] [G loss: 1.977328]\n",
      "epoch:11 step:10354 [D loss: 0.633850, acc: 61.72%] [G loss: 2.171429]\n",
      "epoch:11 step:10355 [D loss: 0.675466, acc: 57.03%] [G loss: 1.921215]\n",
      "epoch:11 step:10356 [D loss: 0.617640, acc: 66.41%] [G loss: 2.092484]\n",
      "epoch:11 step:10357 [D loss: 0.606208, acc: 67.97%] [G loss: 2.108253]\n",
      "epoch:11 step:10358 [D loss: 0.622799, acc: 67.19%] [G loss: 1.995329]\n",
      "epoch:11 step:10359 [D loss: 0.694269, acc: 59.38%] [G loss: 1.922506]\n",
      "epoch:11 step:10360 [D loss: 0.595987, acc: 72.66%] [G loss: 2.142052]\n",
      "epoch:11 step:10361 [D loss: 0.647512, acc: 61.72%] [G loss: 2.242979]\n",
      "epoch:11 step:10362 [D loss: 0.621593, acc: 72.66%] [G loss: 2.044095]\n",
      "epoch:11 step:10363 [D loss: 0.601631, acc: 67.97%] [G loss: 2.050776]\n",
      "epoch:11 step:10364 [D loss: 0.638144, acc: 60.94%] [G loss: 2.149635]\n",
      "epoch:11 step:10365 [D loss: 0.618091, acc: 67.19%] [G loss: 2.153341]\n",
      "epoch:11 step:10366 [D loss: 0.684438, acc: 59.38%] [G loss: 1.962137]\n",
      "epoch:11 step:10367 [D loss: 0.605141, acc: 68.75%] [G loss: 1.927248]\n",
      "epoch:11 step:10368 [D loss: 0.591592, acc: 70.31%] [G loss: 2.053788]\n",
      "epoch:11 step:10369 [D loss: 0.623067, acc: 64.84%] [G loss: 2.092765]\n",
      "epoch:11 step:10370 [D loss: 0.614215, acc: 64.84%] [G loss: 1.991448]\n",
      "epoch:11 step:10371 [D loss: 0.595346, acc: 75.00%] [G loss: 2.045965]\n",
      "epoch:11 step:10372 [D loss: 0.607427, acc: 67.19%] [G loss: 2.051378]\n",
      "epoch:11 step:10373 [D loss: 0.617125, acc: 67.19%] [G loss: 2.006810]\n",
      "epoch:11 step:10374 [D loss: 0.671245, acc: 60.16%] [G loss: 2.241785]\n",
      "epoch:11 step:10375 [D loss: 0.648069, acc: 64.06%] [G loss: 1.969006]\n",
      "epoch:11 step:10376 [D loss: 0.592479, acc: 70.31%] [G loss: 2.176074]\n",
      "epoch:11 step:10377 [D loss: 0.593881, acc: 68.75%] [G loss: 2.094626]\n",
      "epoch:11 step:10378 [D loss: 0.614057, acc: 67.19%] [G loss: 2.035625]\n",
      "epoch:11 step:10379 [D loss: 0.656728, acc: 63.28%] [G loss: 2.032128]\n",
      "epoch:11 step:10380 [D loss: 0.631426, acc: 64.84%] [G loss: 1.931784]\n",
      "epoch:11 step:10381 [D loss: 0.591484, acc: 68.75%] [G loss: 2.284046]\n",
      "epoch:11 step:10382 [D loss: 0.571397, acc: 68.75%] [G loss: 2.216897]\n",
      "epoch:11 step:10383 [D loss: 0.619895, acc: 60.94%] [G loss: 2.141495]\n",
      "epoch:11 step:10384 [D loss: 0.548814, acc: 73.44%] [G loss: 2.366175]\n",
      "epoch:11 step:10385 [D loss: 0.678695, acc: 60.16%] [G loss: 1.896701]\n",
      "epoch:11 step:10386 [D loss: 0.581234, acc: 67.97%] [G loss: 1.978967]\n",
      "epoch:11 step:10387 [D loss: 0.660451, acc: 64.06%] [G loss: 2.022376]\n",
      "epoch:11 step:10388 [D loss: 0.670599, acc: 57.03%] [G loss: 1.980748]\n",
      "epoch:11 step:10389 [D loss: 0.619830, acc: 67.97%] [G loss: 2.016905]\n",
      "epoch:11 step:10390 [D loss: 0.645070, acc: 66.41%] [G loss: 2.091605]\n",
      "epoch:11 step:10391 [D loss: 0.619841, acc: 64.06%] [G loss: 1.934619]\n",
      "epoch:11 step:10392 [D loss: 0.593474, acc: 69.53%] [G loss: 1.877684]\n",
      "epoch:11 step:10393 [D loss: 0.608659, acc: 69.53%] [G loss: 2.026762]\n",
      "epoch:11 step:10394 [D loss: 0.607784, acc: 66.41%] [G loss: 2.022019]\n",
      "epoch:11 step:10395 [D loss: 0.642640, acc: 66.41%] [G loss: 2.162690]\n",
      "epoch:11 step:10396 [D loss: 0.551398, acc: 70.31%] [G loss: 2.214198]\n",
      "epoch:11 step:10397 [D loss: 0.662853, acc: 59.38%] [G loss: 2.047059]\n",
      "epoch:11 step:10398 [D loss: 0.606818, acc: 64.84%] [G loss: 2.010329]\n",
      "epoch:11 step:10399 [D loss: 0.653347, acc: 63.28%] [G loss: 2.248847]\n",
      "epoch:11 step:10400 [D loss: 0.578117, acc: 71.09%] [G loss: 2.138347]\n",
      "epoch:11 step:10401 [D loss: 0.656636, acc: 58.59%] [G loss: 2.253167]\n",
      "epoch:11 step:10402 [D loss: 0.615664, acc: 64.84%] [G loss: 1.883048]\n",
      "epoch:11 step:10403 [D loss: 0.597185, acc: 67.97%] [G loss: 2.058185]\n",
      "epoch:11 step:10404 [D loss: 0.606994, acc: 65.62%] [G loss: 2.172104]\n",
      "epoch:11 step:10405 [D loss: 0.630028, acc: 66.41%] [G loss: 1.989125]\n",
      "epoch:11 step:10406 [D loss: 0.634443, acc: 64.06%] [G loss: 2.127654]\n",
      "epoch:11 step:10407 [D loss: 0.642133, acc: 63.28%] [G loss: 2.004850]\n",
      "epoch:11 step:10408 [D loss: 0.603138, acc: 67.19%] [G loss: 2.139397]\n",
      "epoch:11 step:10409 [D loss: 0.617896, acc: 58.59%] [G loss: 1.985475]\n",
      "epoch:11 step:10410 [D loss: 0.573794, acc: 70.31%] [G loss: 2.041481]\n",
      "epoch:11 step:10411 [D loss: 0.655176, acc: 62.50%] [G loss: 1.956184]\n",
      "epoch:11 step:10412 [D loss: 0.678504, acc: 65.62%] [G loss: 2.057582]\n",
      "epoch:11 step:10413 [D loss: 0.599868, acc: 66.41%] [G loss: 1.954644]\n",
      "epoch:11 step:10414 [D loss: 0.598698, acc: 69.53%] [G loss: 2.160499]\n",
      "epoch:11 step:10415 [D loss: 0.742835, acc: 55.47%] [G loss: 1.819146]\n",
      "epoch:11 step:10416 [D loss: 0.671492, acc: 64.06%] [G loss: 1.982882]\n",
      "epoch:11 step:10417 [D loss: 0.644529, acc: 62.50%] [G loss: 1.956427]\n",
      "epoch:11 step:10418 [D loss: 0.599930, acc: 66.41%] [G loss: 2.190080]\n",
      "epoch:11 step:10419 [D loss: 0.602871, acc: 68.75%] [G loss: 2.142144]\n",
      "epoch:11 step:10420 [D loss: 0.659791, acc: 62.50%] [G loss: 2.036037]\n",
      "epoch:11 step:10421 [D loss: 0.642328, acc: 63.28%] [G loss: 1.990534]\n",
      "epoch:11 step:10422 [D loss: 0.692482, acc: 60.94%] [G loss: 2.338675]\n",
      "epoch:11 step:10423 [D loss: 0.576044, acc: 74.22%] [G loss: 2.142526]\n",
      "epoch:11 step:10424 [D loss: 0.589505, acc: 67.19%] [G loss: 2.243262]\n",
      "epoch:11 step:10425 [D loss: 0.611785, acc: 68.75%] [G loss: 2.247048]\n",
      "epoch:11 step:10426 [D loss: 0.561805, acc: 67.97%] [G loss: 2.504965]\n",
      "epoch:11 step:10427 [D loss: 0.656654, acc: 56.25%] [G loss: 2.077535]\n",
      "epoch:11 step:10428 [D loss: 0.588673, acc: 66.41%] [G loss: 2.244708]\n",
      "epoch:11 step:10429 [D loss: 0.580573, acc: 67.97%] [G loss: 2.335320]\n",
      "epoch:11 step:10430 [D loss: 0.696262, acc: 57.81%] [G loss: 2.059925]\n",
      "epoch:11 step:10431 [D loss: 0.656881, acc: 64.84%] [G loss: 2.266490]\n",
      "epoch:11 step:10432 [D loss: 0.664840, acc: 61.72%] [G loss: 2.000558]\n",
      "epoch:11 step:10433 [D loss: 0.638369, acc: 61.72%] [G loss: 2.025430]\n",
      "epoch:11 step:10434 [D loss: 0.643260, acc: 63.28%] [G loss: 1.899488]\n",
      "epoch:11 step:10435 [D loss: 0.657233, acc: 64.06%] [G loss: 2.023953]\n",
      "epoch:11 step:10436 [D loss: 0.678102, acc: 60.94%] [G loss: 1.900811]\n",
      "epoch:11 step:10437 [D loss: 0.634934, acc: 65.62%] [G loss: 2.026483]\n",
      "epoch:11 step:10438 [D loss: 0.622365, acc: 70.31%] [G loss: 2.007904]\n",
      "epoch:11 step:10439 [D loss: 0.599850, acc: 66.41%] [G loss: 2.014053]\n",
      "epoch:11 step:10440 [D loss: 0.638273, acc: 60.94%] [G loss: 2.003795]\n",
      "epoch:11 step:10441 [D loss: 0.684802, acc: 61.72%] [G loss: 2.134806]\n",
      "epoch:11 step:10442 [D loss: 0.637331, acc: 62.50%] [G loss: 2.026598]\n",
      "epoch:11 step:10443 [D loss: 0.625824, acc: 64.84%] [G loss: 1.927611]\n",
      "epoch:11 step:10444 [D loss: 0.655528, acc: 64.84%] [G loss: 1.983037]\n",
      "epoch:11 step:10445 [D loss: 0.665787, acc: 58.59%] [G loss: 1.784948]\n",
      "epoch:11 step:10446 [D loss: 0.650685, acc: 60.16%] [G loss: 1.829411]\n",
      "epoch:11 step:10447 [D loss: 0.664260, acc: 65.62%] [G loss: 1.901567]\n",
      "epoch:11 step:10448 [D loss: 0.607007, acc: 67.19%] [G loss: 2.061499]\n",
      "epoch:11 step:10449 [D loss: 0.648890, acc: 62.50%] [G loss: 1.911306]\n",
      "epoch:11 step:10450 [D loss: 0.690991, acc: 60.16%] [G loss: 1.863440]\n",
      "epoch:11 step:10451 [D loss: 0.675652, acc: 67.19%] [G loss: 2.001579]\n",
      "epoch:11 step:10452 [D loss: 0.629042, acc: 63.28%] [G loss: 1.997247]\n",
      "epoch:11 step:10453 [D loss: 0.683723, acc: 58.59%] [G loss: 2.064565]\n",
      "epoch:11 step:10454 [D loss: 0.687726, acc: 55.47%] [G loss: 2.151115]\n",
      "epoch:11 step:10455 [D loss: 0.691882, acc: 58.59%] [G loss: 1.932249]\n",
      "epoch:11 step:10456 [D loss: 0.618255, acc: 62.50%] [G loss: 2.000427]\n",
      "epoch:11 step:10457 [D loss: 0.591429, acc: 68.75%] [G loss: 2.011249]\n",
      "epoch:11 step:10458 [D loss: 0.640836, acc: 68.75%] [G loss: 2.050961]\n",
      "epoch:11 step:10459 [D loss: 0.595450, acc: 72.66%] [G loss: 2.020144]\n",
      "epoch:11 step:10460 [D loss: 0.686508, acc: 51.56%] [G loss: 1.899818]\n",
      "epoch:11 step:10461 [D loss: 0.577443, acc: 69.53%] [G loss: 1.967535]\n",
      "epoch:11 step:10462 [D loss: 0.607374, acc: 63.28%] [G loss: 1.982967]\n",
      "epoch:11 step:10463 [D loss: 0.612023, acc: 63.28%] [G loss: 2.113425]\n",
      "epoch:11 step:10464 [D loss: 0.650201, acc: 65.62%] [G loss: 2.011196]\n",
      "epoch:11 step:10465 [D loss: 0.626048, acc: 64.84%] [G loss: 1.963507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10466 [D loss: 0.622786, acc: 61.72%] [G loss: 2.097845]\n",
      "epoch:11 step:10467 [D loss: 0.662653, acc: 57.03%] [G loss: 1.934861]\n",
      "epoch:11 step:10468 [D loss: 0.660401, acc: 60.16%] [G loss: 2.001766]\n",
      "epoch:11 step:10469 [D loss: 0.584569, acc: 70.31%] [G loss: 2.137816]\n",
      "epoch:11 step:10470 [D loss: 0.593008, acc: 66.41%] [G loss: 2.012301]\n",
      "epoch:11 step:10471 [D loss: 0.714904, acc: 51.56%] [G loss: 1.827241]\n",
      "epoch:11 step:10472 [D loss: 0.616262, acc: 64.84%] [G loss: 1.989993]\n",
      "epoch:11 step:10473 [D loss: 0.617187, acc: 63.28%] [G loss: 1.875918]\n",
      "epoch:11 step:10474 [D loss: 0.601863, acc: 68.75%] [G loss: 1.990423]\n",
      "epoch:11 step:10475 [D loss: 0.603630, acc: 70.31%] [G loss: 1.981877]\n",
      "epoch:11 step:10476 [D loss: 0.700927, acc: 55.47%] [G loss: 1.945192]\n",
      "epoch:11 step:10477 [D loss: 0.672162, acc: 55.47%] [G loss: 1.968055]\n",
      "epoch:11 step:10478 [D loss: 0.636828, acc: 64.06%] [G loss: 2.044846]\n",
      "epoch:11 step:10479 [D loss: 0.662944, acc: 63.28%] [G loss: 1.896663]\n",
      "epoch:11 step:10480 [D loss: 0.700103, acc: 54.69%] [G loss: 1.916697]\n",
      "epoch:11 step:10481 [D loss: 0.655166, acc: 64.84%] [G loss: 1.954343]\n",
      "epoch:11 step:10482 [D loss: 0.638444, acc: 61.72%] [G loss: 1.948041]\n",
      "epoch:11 step:10483 [D loss: 0.636849, acc: 63.28%] [G loss: 1.881282]\n",
      "epoch:11 step:10484 [D loss: 0.690605, acc: 52.34%] [G loss: 1.798385]\n",
      "epoch:11 step:10485 [D loss: 0.660744, acc: 60.94%] [G loss: 1.868637]\n",
      "epoch:11 step:10486 [D loss: 0.673990, acc: 59.38%] [G loss: 1.951532]\n",
      "epoch:11 step:10487 [D loss: 0.640652, acc: 62.50%] [G loss: 2.059878]\n",
      "epoch:11 step:10488 [D loss: 0.646116, acc: 59.38%] [G loss: 1.825549]\n",
      "epoch:11 step:10489 [D loss: 0.606631, acc: 67.19%] [G loss: 1.851969]\n",
      "epoch:11 step:10490 [D loss: 0.645036, acc: 66.41%] [G loss: 1.824329]\n",
      "epoch:11 step:10491 [D loss: 0.637233, acc: 61.72%] [G loss: 1.909240]\n",
      "epoch:11 step:10492 [D loss: 0.604319, acc: 68.75%] [G loss: 2.128462]\n",
      "epoch:11 step:10493 [D loss: 0.719379, acc: 60.94%] [G loss: 1.848693]\n",
      "epoch:11 step:10494 [D loss: 0.673373, acc: 60.94%] [G loss: 1.842355]\n",
      "epoch:11 step:10495 [D loss: 0.656715, acc: 60.16%] [G loss: 1.889490]\n",
      "epoch:11 step:10496 [D loss: 0.659382, acc: 64.06%] [G loss: 1.759855]\n",
      "epoch:11 step:10497 [D loss: 0.639765, acc: 61.72%] [G loss: 2.006939]\n",
      "epoch:11 step:10498 [D loss: 0.608679, acc: 64.84%] [G loss: 1.984513]\n",
      "epoch:11 step:10499 [D loss: 0.656065, acc: 65.62%] [G loss: 1.936333]\n",
      "epoch:11 step:10500 [D loss: 0.535008, acc: 78.12%] [G loss: 1.961218]\n",
      "epoch:11 step:10501 [D loss: 0.596488, acc: 70.31%] [G loss: 2.224038]\n",
      "epoch:11 step:10502 [D loss: 0.606903, acc: 63.28%] [G loss: 2.174267]\n",
      "epoch:11 step:10503 [D loss: 0.677253, acc: 56.25%] [G loss: 1.898043]\n",
      "epoch:11 step:10504 [D loss: 0.651645, acc: 62.50%] [G loss: 1.930910]\n",
      "epoch:11 step:10505 [D loss: 0.600669, acc: 68.75%] [G loss: 2.149779]\n",
      "epoch:11 step:10506 [D loss: 0.687937, acc: 60.16%] [G loss: 1.905442]\n",
      "epoch:11 step:10507 [D loss: 0.685067, acc: 59.38%] [G loss: 1.911672]\n",
      "epoch:11 step:10508 [D loss: 0.626420, acc: 64.06%] [G loss: 1.973533]\n",
      "epoch:11 step:10509 [D loss: 0.624877, acc: 58.59%] [G loss: 1.995971]\n",
      "epoch:11 step:10510 [D loss: 0.676452, acc: 58.59%] [G loss: 1.850285]\n",
      "epoch:11 step:10511 [D loss: 0.655162, acc: 60.94%] [G loss: 1.957351]\n",
      "epoch:11 step:10512 [D loss: 0.673803, acc: 56.25%] [G loss: 1.998980]\n",
      "epoch:11 step:10513 [D loss: 0.638264, acc: 64.84%] [G loss: 2.017693]\n",
      "epoch:11 step:10514 [D loss: 0.548673, acc: 71.09%] [G loss: 2.438830]\n",
      "epoch:11 step:10515 [D loss: 0.555917, acc: 71.88%] [G loss: 2.382231]\n",
      "epoch:11 step:10516 [D loss: 0.564571, acc: 73.44%] [G loss: 2.501028]\n",
      "epoch:11 step:10517 [D loss: 0.693445, acc: 58.59%] [G loss: 1.975997]\n",
      "epoch:11 step:10518 [D loss: 0.655093, acc: 60.94%] [G loss: 1.968254]\n",
      "epoch:11 step:10519 [D loss: 0.655498, acc: 66.41%] [G loss: 1.930023]\n",
      "epoch:11 step:10520 [D loss: 0.667401, acc: 63.28%] [G loss: 1.810019]\n",
      "epoch:11 step:10521 [D loss: 0.647263, acc: 64.06%] [G loss: 1.900944]\n",
      "epoch:11 step:10522 [D loss: 0.647080, acc: 59.38%] [G loss: 1.994819]\n",
      "epoch:11 step:10523 [D loss: 0.592063, acc: 68.75%] [G loss: 1.937993]\n",
      "epoch:11 step:10524 [D loss: 0.702663, acc: 56.25%] [G loss: 2.113402]\n",
      "epoch:11 step:10525 [D loss: 0.586357, acc: 69.53%] [G loss: 2.262914]\n",
      "epoch:11 step:10526 [D loss: 0.617480, acc: 62.50%] [G loss: 2.164335]\n",
      "epoch:11 step:10527 [D loss: 0.728616, acc: 60.94%] [G loss: 1.974862]\n",
      "epoch:11 step:10528 [D loss: 0.632228, acc: 60.16%] [G loss: 2.079826]\n",
      "epoch:11 step:10529 [D loss: 0.578560, acc: 70.31%] [G loss: 2.020504]\n",
      "epoch:11 step:10530 [D loss: 0.643359, acc: 60.94%] [G loss: 1.932081]\n",
      "epoch:11 step:10531 [D loss: 0.631045, acc: 62.50%] [G loss: 2.000557]\n",
      "epoch:11 step:10532 [D loss: 0.636038, acc: 61.72%] [G loss: 2.033955]\n",
      "epoch:11 step:10533 [D loss: 0.720380, acc: 53.91%] [G loss: 1.881387]\n",
      "epoch:11 step:10534 [D loss: 0.655739, acc: 64.06%] [G loss: 1.954599]\n",
      "epoch:11 step:10535 [D loss: 0.692437, acc: 56.25%] [G loss: 1.885717]\n",
      "epoch:11 step:10536 [D loss: 0.624377, acc: 67.19%] [G loss: 2.117622]\n",
      "epoch:11 step:10537 [D loss: 0.635391, acc: 63.28%] [G loss: 2.192067]\n",
      "epoch:11 step:10538 [D loss: 0.572535, acc: 75.00%] [G loss: 2.243234]\n",
      "epoch:11 step:10539 [D loss: 0.512532, acc: 83.59%] [G loss: 2.631419]\n",
      "epoch:11 step:10540 [D loss: 0.634944, acc: 65.62%] [G loss: 2.025162]\n",
      "epoch:11 step:10541 [D loss: 0.677510, acc: 61.72%] [G loss: 2.069649]\n",
      "epoch:11 step:10542 [D loss: 0.669571, acc: 61.72%] [G loss: 2.145013]\n",
      "epoch:11 step:10543 [D loss: 0.675485, acc: 55.47%] [G loss: 2.021640]\n",
      "epoch:11 step:10544 [D loss: 0.620441, acc: 65.62%] [G loss: 1.923327]\n",
      "epoch:11 step:10545 [D loss: 0.646934, acc: 61.72%] [G loss: 1.917367]\n",
      "epoch:11 step:10546 [D loss: 0.662020, acc: 61.72%] [G loss: 2.076543]\n",
      "epoch:11 step:10547 [D loss: 0.687007, acc: 58.59%] [G loss: 2.074560]\n",
      "epoch:11 step:10548 [D loss: 0.648765, acc: 64.06%] [G loss: 2.026944]\n",
      "epoch:11 step:10549 [D loss: 0.612942, acc: 68.75%] [G loss: 2.037829]\n",
      "epoch:11 step:10550 [D loss: 0.670748, acc: 56.25%] [G loss: 1.956367]\n",
      "epoch:11 step:10551 [D loss: 0.606540, acc: 69.53%] [G loss: 1.945598]\n",
      "epoch:11 step:10552 [D loss: 0.652821, acc: 59.38%] [G loss: 2.043734]\n",
      "epoch:11 step:10553 [D loss: 0.631482, acc: 64.84%] [G loss: 1.959235]\n",
      "epoch:11 step:10554 [D loss: 0.606421, acc: 71.09%] [G loss: 1.983772]\n",
      "epoch:11 step:10555 [D loss: 0.632718, acc: 62.50%] [G loss: 1.987499]\n",
      "epoch:11 step:10556 [D loss: 0.688878, acc: 54.69%] [G loss: 1.872830]\n",
      "epoch:11 step:10557 [D loss: 0.681206, acc: 57.81%] [G loss: 1.874406]\n",
      "epoch:11 step:10558 [D loss: 0.638524, acc: 63.28%] [G loss: 1.723048]\n",
      "epoch:11 step:10559 [D loss: 0.694626, acc: 55.47%] [G loss: 1.947167]\n",
      "epoch:11 step:10560 [D loss: 0.707800, acc: 57.81%] [G loss: 2.006310]\n",
      "epoch:11 step:10561 [D loss: 0.635021, acc: 64.06%] [G loss: 1.842614]\n",
      "epoch:11 step:10562 [D loss: 0.580047, acc: 68.75%] [G loss: 1.912752]\n",
      "epoch:11 step:10563 [D loss: 0.693493, acc: 60.94%] [G loss: 1.941677]\n",
      "epoch:11 step:10564 [D loss: 0.620363, acc: 67.97%] [G loss: 1.927016]\n",
      "epoch:11 step:10565 [D loss: 0.639799, acc: 61.72%] [G loss: 2.005580]\n",
      "epoch:11 step:10566 [D loss: 0.572637, acc: 69.53%] [G loss: 2.025098]\n",
      "epoch:11 step:10567 [D loss: 0.567480, acc: 75.78%] [G loss: 2.052406]\n",
      "epoch:11 step:10568 [D loss: 0.599991, acc: 68.75%] [G loss: 2.119864]\n",
      "epoch:11 step:10569 [D loss: 0.616511, acc: 71.88%] [G loss: 1.979212]\n",
      "epoch:11 step:10570 [D loss: 0.674117, acc: 63.28%] [G loss: 1.855512]\n",
      "epoch:11 step:10571 [D loss: 0.609496, acc: 67.19%] [G loss: 2.151442]\n",
      "epoch:11 step:10572 [D loss: 0.665621, acc: 64.06%] [G loss: 1.828515]\n",
      "epoch:11 step:10573 [D loss: 0.619654, acc: 66.41%] [G loss: 1.981256]\n",
      "epoch:11 step:10574 [D loss: 0.683243, acc: 62.50%] [G loss: 2.136112]\n",
      "epoch:11 step:10575 [D loss: 0.637484, acc: 67.19%] [G loss: 2.131078]\n",
      "epoch:11 step:10576 [D loss: 0.659258, acc: 58.59%] [G loss: 2.046020]\n",
      "epoch:11 step:10577 [D loss: 0.591390, acc: 68.75%] [G loss: 2.134003]\n",
      "epoch:11 step:10578 [D loss: 0.671943, acc: 62.50%] [G loss: 2.048992]\n",
      "epoch:11 step:10579 [D loss: 0.627341, acc: 63.28%] [G loss: 2.002751]\n",
      "epoch:11 step:10580 [D loss: 0.653799, acc: 60.94%] [G loss: 2.007041]\n",
      "epoch:11 step:10581 [D loss: 0.603873, acc: 67.19%] [G loss: 2.035841]\n",
      "epoch:11 step:10582 [D loss: 0.590954, acc: 72.66%] [G loss: 1.963118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10583 [D loss: 0.607150, acc: 66.41%] [G loss: 2.031860]\n",
      "epoch:11 step:10584 [D loss: 0.592443, acc: 70.31%] [G loss: 2.014273]\n",
      "epoch:11 step:10585 [D loss: 0.653432, acc: 58.59%] [G loss: 1.983440]\n",
      "epoch:11 step:10586 [D loss: 0.630179, acc: 68.75%] [G loss: 2.059417]\n",
      "epoch:11 step:10587 [D loss: 0.557599, acc: 73.44%] [G loss: 2.154320]\n",
      "epoch:11 step:10588 [D loss: 0.700000, acc: 59.38%] [G loss: 1.955335]\n",
      "epoch:11 step:10589 [D loss: 0.580563, acc: 68.75%] [G loss: 2.060861]\n",
      "epoch:11 step:10590 [D loss: 0.664863, acc: 60.16%] [G loss: 1.965700]\n",
      "epoch:11 step:10591 [D loss: 0.637511, acc: 63.28%] [G loss: 2.147942]\n",
      "epoch:11 step:10592 [D loss: 0.629243, acc: 64.06%] [G loss: 2.081666]\n",
      "epoch:11 step:10593 [D loss: 0.615690, acc: 60.94%] [G loss: 2.136575]\n",
      "epoch:11 step:10594 [D loss: 0.687646, acc: 59.38%] [G loss: 2.033846]\n",
      "epoch:11 step:10595 [D loss: 0.622962, acc: 65.62%] [G loss: 1.928500]\n",
      "epoch:11 step:10596 [D loss: 0.634447, acc: 66.41%] [G loss: 1.951561]\n",
      "epoch:11 step:10597 [D loss: 0.699375, acc: 53.12%] [G loss: 2.043437]\n",
      "epoch:11 step:10598 [D loss: 0.736634, acc: 58.59%] [G loss: 1.965086]\n",
      "epoch:11 step:10599 [D loss: 0.630684, acc: 63.28%] [G loss: 2.054922]\n",
      "epoch:11 step:10600 [D loss: 0.619414, acc: 67.97%] [G loss: 2.074066]\n",
      "epoch:11 step:10601 [D loss: 0.661674, acc: 60.16%] [G loss: 1.862486]\n",
      "epoch:11 step:10602 [D loss: 0.644970, acc: 63.28%] [G loss: 1.933179]\n",
      "epoch:11 step:10603 [D loss: 0.622683, acc: 61.72%] [G loss: 2.064619]\n",
      "epoch:11 step:10604 [D loss: 0.652709, acc: 61.72%] [G loss: 2.000079]\n",
      "epoch:11 step:10605 [D loss: 0.649059, acc: 62.50%] [G loss: 1.979147]\n",
      "epoch:11 step:10606 [D loss: 0.578684, acc: 77.34%] [G loss: 1.982342]\n",
      "epoch:11 step:10607 [D loss: 0.586659, acc: 67.19%] [G loss: 2.000759]\n",
      "epoch:11 step:10608 [D loss: 0.633220, acc: 64.06%] [G loss: 1.909450]\n",
      "epoch:11 step:10609 [D loss: 0.669746, acc: 56.25%] [G loss: 2.015769]\n",
      "epoch:11 step:10610 [D loss: 0.609214, acc: 66.41%] [G loss: 1.973551]\n",
      "epoch:11 step:10611 [D loss: 0.659994, acc: 64.84%] [G loss: 1.966571]\n",
      "epoch:11 step:10612 [D loss: 0.650740, acc: 62.50%] [G loss: 1.846987]\n",
      "epoch:11 step:10613 [D loss: 0.674774, acc: 53.91%] [G loss: 1.778487]\n",
      "epoch:11 step:10614 [D loss: 0.613572, acc: 71.09%] [G loss: 1.989796]\n",
      "epoch:11 step:10615 [D loss: 0.654587, acc: 60.94%] [G loss: 1.961699]\n",
      "epoch:11 step:10616 [D loss: 0.590201, acc: 60.94%] [G loss: 1.952568]\n",
      "epoch:11 step:10617 [D loss: 0.596988, acc: 67.19%] [G loss: 2.078151]\n",
      "epoch:11 step:10618 [D loss: 0.567777, acc: 69.53%] [G loss: 2.039373]\n",
      "epoch:11 step:10619 [D loss: 0.586868, acc: 68.75%] [G loss: 2.616101]\n",
      "epoch:11 step:10620 [D loss: 0.589878, acc: 63.28%] [G loss: 2.206028]\n",
      "epoch:11 step:10621 [D loss: 0.525232, acc: 76.56%] [G loss: 2.468347]\n",
      "epoch:11 step:10622 [D loss: 0.564392, acc: 68.75%] [G loss: 2.487037]\n",
      "epoch:11 step:10623 [D loss: 0.688211, acc: 58.59%] [G loss: 2.105848]\n",
      "epoch:11 step:10624 [D loss: 0.681763, acc: 63.28%] [G loss: 2.044790]\n",
      "epoch:11 step:10625 [D loss: 0.657195, acc: 61.72%] [G loss: 2.078582]\n",
      "epoch:11 step:10626 [D loss: 0.649607, acc: 57.03%] [G loss: 1.809927]\n",
      "epoch:11 step:10627 [D loss: 0.670287, acc: 62.50%] [G loss: 1.929897]\n",
      "epoch:11 step:10628 [D loss: 0.643011, acc: 61.72%] [G loss: 2.160665]\n",
      "epoch:11 step:10629 [D loss: 0.661110, acc: 62.50%] [G loss: 1.879272]\n",
      "epoch:11 step:10630 [D loss: 0.644094, acc: 63.28%] [G loss: 1.751065]\n",
      "epoch:11 step:10631 [D loss: 0.599724, acc: 67.97%] [G loss: 2.028714]\n",
      "epoch:11 step:10632 [D loss: 0.625152, acc: 67.19%] [G loss: 1.972976]\n",
      "epoch:11 step:10633 [D loss: 0.677102, acc: 58.59%] [G loss: 1.952770]\n",
      "epoch:11 step:10634 [D loss: 0.620656, acc: 66.41%] [G loss: 1.921323]\n",
      "epoch:11 step:10635 [D loss: 0.650135, acc: 61.72%] [G loss: 2.048788]\n",
      "epoch:11 step:10636 [D loss: 0.623790, acc: 67.97%] [G loss: 1.884697]\n",
      "epoch:11 step:10637 [D loss: 0.650632, acc: 64.06%] [G loss: 1.986721]\n",
      "epoch:11 step:10638 [D loss: 0.604771, acc: 67.97%] [G loss: 2.008126]\n",
      "epoch:11 step:10639 [D loss: 0.586224, acc: 72.66%] [G loss: 2.285744]\n",
      "epoch:11 step:10640 [D loss: 0.592798, acc: 69.53%] [G loss: 2.189348]\n",
      "epoch:11 step:10641 [D loss: 0.661829, acc: 60.94%] [G loss: 2.014158]\n",
      "epoch:11 step:10642 [D loss: 0.549057, acc: 72.66%] [G loss: 2.173903]\n",
      "epoch:11 step:10643 [D loss: 0.689284, acc: 59.38%] [G loss: 1.940064]\n",
      "epoch:11 step:10644 [D loss: 0.598114, acc: 70.31%] [G loss: 2.071257]\n",
      "epoch:11 step:10645 [D loss: 0.641039, acc: 58.59%] [G loss: 2.068180]\n",
      "epoch:11 step:10646 [D loss: 0.585105, acc: 71.09%] [G loss: 2.121732]\n",
      "epoch:11 step:10647 [D loss: 0.600268, acc: 70.31%] [G loss: 2.080831]\n",
      "epoch:11 step:10648 [D loss: 0.616590, acc: 67.97%] [G loss: 2.050071]\n",
      "epoch:11 step:10649 [D loss: 0.721915, acc: 56.25%] [G loss: 1.932969]\n",
      "epoch:11 step:10650 [D loss: 0.611401, acc: 64.84%] [G loss: 2.057664]\n",
      "epoch:11 step:10651 [D loss: 0.637394, acc: 64.84%] [G loss: 2.001317]\n",
      "epoch:11 step:10652 [D loss: 0.622537, acc: 68.75%] [G loss: 2.438379]\n",
      "epoch:11 step:10653 [D loss: 0.616455, acc: 64.06%] [G loss: 2.268344]\n",
      "epoch:11 step:10654 [D loss: 0.546542, acc: 73.44%] [G loss: 2.390076]\n",
      "epoch:11 step:10655 [D loss: 0.694446, acc: 61.72%] [G loss: 1.979482]\n",
      "epoch:11 step:10656 [D loss: 0.688483, acc: 55.47%] [G loss: 1.741261]\n",
      "epoch:11 step:10657 [D loss: 0.658882, acc: 60.94%] [G loss: 1.971434]\n",
      "epoch:11 step:10658 [D loss: 0.608022, acc: 67.19%] [G loss: 2.089247]\n",
      "epoch:11 step:10659 [D loss: 0.642928, acc: 59.38%] [G loss: 1.789173]\n",
      "epoch:11 step:10660 [D loss: 0.647520, acc: 64.06%] [G loss: 2.137804]\n",
      "epoch:11 step:10661 [D loss: 0.635684, acc: 66.41%] [G loss: 1.996744]\n",
      "epoch:11 step:10662 [D loss: 0.768934, acc: 50.00%] [G loss: 1.913927]\n",
      "epoch:11 step:10663 [D loss: 0.617868, acc: 62.50%] [G loss: 1.944420]\n",
      "epoch:11 step:10664 [D loss: 0.593042, acc: 70.31%] [G loss: 1.990175]\n",
      "epoch:11 step:10665 [D loss: 0.661569, acc: 57.03%] [G loss: 2.004112]\n",
      "epoch:11 step:10666 [D loss: 0.551553, acc: 73.44%] [G loss: 2.113990]\n",
      "epoch:11 step:10667 [D loss: 0.599678, acc: 67.97%] [G loss: 2.059878]\n",
      "epoch:11 step:10668 [D loss: 0.671758, acc: 60.94%] [G loss: 2.057657]\n",
      "epoch:11 step:10669 [D loss: 0.661578, acc: 60.16%] [G loss: 1.826132]\n",
      "epoch:11 step:10670 [D loss: 0.597420, acc: 71.09%] [G loss: 2.038899]\n",
      "epoch:11 step:10671 [D loss: 0.619685, acc: 60.94%] [G loss: 2.106433]\n",
      "epoch:11 step:10672 [D loss: 0.626675, acc: 64.06%] [G loss: 1.983345]\n",
      "epoch:11 step:10673 [D loss: 0.647427, acc: 63.28%] [G loss: 2.163423]\n",
      "epoch:11 step:10674 [D loss: 0.612870, acc: 64.06%] [G loss: 2.059948]\n",
      "epoch:11 step:10675 [D loss: 0.631677, acc: 61.72%] [G loss: 1.934484]\n",
      "epoch:11 step:10676 [D loss: 0.648296, acc: 65.62%] [G loss: 2.009865]\n",
      "epoch:11 step:10677 [D loss: 0.585627, acc: 71.09%] [G loss: 2.107812]\n",
      "epoch:11 step:10678 [D loss: 0.641148, acc: 60.16%] [G loss: 2.272922]\n",
      "epoch:11 step:10679 [D loss: 0.656858, acc: 60.16%] [G loss: 1.997738]\n",
      "epoch:11 step:10680 [D loss: 0.636629, acc: 62.50%] [G loss: 1.926480]\n",
      "epoch:11 step:10681 [D loss: 0.578645, acc: 70.31%] [G loss: 2.252005]\n",
      "epoch:11 step:10682 [D loss: 0.614935, acc: 65.62%] [G loss: 1.947541]\n",
      "epoch:11 step:10683 [D loss: 0.659576, acc: 61.72%] [G loss: 1.889353]\n",
      "epoch:11 step:10684 [D loss: 0.701090, acc: 57.81%] [G loss: 1.825509]\n",
      "epoch:11 step:10685 [D loss: 0.649217, acc: 61.72%] [G loss: 1.799829]\n",
      "epoch:11 step:10686 [D loss: 0.660319, acc: 58.59%] [G loss: 2.024806]\n",
      "epoch:11 step:10687 [D loss: 0.646468, acc: 61.72%] [G loss: 1.988266]\n",
      "epoch:11 step:10688 [D loss: 0.585314, acc: 68.75%] [G loss: 2.144537]\n",
      "epoch:11 step:10689 [D loss: 0.655878, acc: 58.59%] [G loss: 2.067438]\n",
      "epoch:11 step:10690 [D loss: 0.674727, acc: 64.06%] [G loss: 1.846195]\n",
      "epoch:11 step:10691 [D loss: 0.626946, acc: 63.28%] [G loss: 1.936692]\n",
      "epoch:11 step:10692 [D loss: 0.607112, acc: 63.28%] [G loss: 2.088969]\n",
      "epoch:11 step:10693 [D loss: 0.627198, acc: 60.16%] [G loss: 1.996543]\n",
      "epoch:11 step:10694 [D loss: 0.696277, acc: 53.12%] [G loss: 2.001256]\n",
      "epoch:11 step:10695 [D loss: 0.659054, acc: 56.25%] [G loss: 1.927055]\n",
      "epoch:11 step:10696 [D loss: 0.692041, acc: 58.59%] [G loss: 1.936802]\n",
      "epoch:11 step:10697 [D loss: 0.653878, acc: 64.06%] [G loss: 2.031847]\n",
      "epoch:11 step:10698 [D loss: 0.704500, acc: 58.59%] [G loss: 1.897933]\n",
      "epoch:11 step:10699 [D loss: 0.596418, acc: 68.75%] [G loss: 1.950209]\n",
      "epoch:11 step:10700 [D loss: 0.674484, acc: 58.59%] [G loss: 2.050122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10701 [D loss: 0.654448, acc: 60.16%] [G loss: 1.823395]\n",
      "epoch:11 step:10702 [D loss: 0.684378, acc: 57.81%] [G loss: 1.890713]\n",
      "epoch:11 step:10703 [D loss: 0.644270, acc: 57.81%] [G loss: 1.868322]\n",
      "epoch:11 step:10704 [D loss: 0.680841, acc: 58.59%] [G loss: 1.993432]\n",
      "epoch:11 step:10705 [D loss: 0.660744, acc: 59.38%] [G loss: 1.925217]\n",
      "epoch:11 step:10706 [D loss: 0.662083, acc: 60.94%] [G loss: 1.988280]\n",
      "epoch:11 step:10707 [D loss: 0.596142, acc: 68.75%] [G loss: 1.974298]\n",
      "epoch:11 step:10708 [D loss: 0.651578, acc: 66.41%] [G loss: 2.030209]\n",
      "epoch:11 step:10709 [D loss: 0.571517, acc: 69.53%] [G loss: 2.181706]\n",
      "epoch:11 step:10710 [D loss: 0.593271, acc: 67.19%] [G loss: 1.999285]\n",
      "epoch:11 step:10711 [D loss: 0.581641, acc: 71.09%] [G loss: 2.106968]\n",
      "epoch:11 step:10712 [D loss: 0.641404, acc: 64.06%] [G loss: 2.250809]\n",
      "epoch:11 step:10713 [D loss: 0.627393, acc: 63.28%] [G loss: 2.228216]\n",
      "epoch:11 step:10714 [D loss: 0.558889, acc: 75.78%] [G loss: 2.142198]\n",
      "epoch:11 step:10715 [D loss: 0.689505, acc: 64.06%] [G loss: 1.926692]\n",
      "epoch:11 step:10716 [D loss: 0.662970, acc: 64.84%] [G loss: 2.032540]\n",
      "epoch:11 step:10717 [D loss: 0.618268, acc: 64.06%] [G loss: 2.137056]\n",
      "epoch:11 step:10718 [D loss: 0.647226, acc: 59.38%] [G loss: 2.013305]\n",
      "epoch:11 step:10719 [D loss: 0.685125, acc: 58.59%] [G loss: 1.903275]\n",
      "epoch:11 step:10720 [D loss: 0.641640, acc: 65.62%] [G loss: 2.070344]\n",
      "epoch:11 step:10721 [D loss: 0.604098, acc: 63.28%] [G loss: 2.167013]\n",
      "epoch:11 step:10722 [D loss: 0.634809, acc: 70.31%] [G loss: 2.205255]\n",
      "epoch:11 step:10723 [D loss: 0.666952, acc: 64.06%] [G loss: 2.017275]\n",
      "epoch:11 step:10724 [D loss: 0.689014, acc: 57.03%] [G loss: 1.931877]\n",
      "epoch:11 step:10725 [D loss: 0.770481, acc: 51.56%] [G loss: 2.039714]\n",
      "epoch:11 step:10726 [D loss: 0.661526, acc: 59.38%] [G loss: 1.981212]\n",
      "epoch:11 step:10727 [D loss: 0.602059, acc: 70.31%] [G loss: 2.039263]\n",
      "epoch:11 step:10728 [D loss: 0.684073, acc: 57.03%] [G loss: 1.884772]\n",
      "epoch:11 step:10729 [D loss: 0.662374, acc: 60.16%] [G loss: 1.763049]\n",
      "epoch:11 step:10730 [D loss: 0.613668, acc: 65.62%] [G loss: 1.887689]\n",
      "epoch:11 step:10731 [D loss: 0.690271, acc: 58.59%] [G loss: 2.056853]\n",
      "epoch:11 step:10732 [D loss: 0.677703, acc: 60.94%] [G loss: 1.921424]\n",
      "epoch:11 step:10733 [D loss: 0.627515, acc: 61.72%] [G loss: 2.055194]\n",
      "epoch:11 step:10734 [D loss: 0.610736, acc: 64.06%] [G loss: 2.065661]\n",
      "epoch:11 step:10735 [D loss: 0.550440, acc: 75.00%] [G loss: 2.203725]\n",
      "epoch:11 step:10736 [D loss: 0.573243, acc: 69.53%] [G loss: 2.305433]\n",
      "epoch:11 step:10737 [D loss: 0.643042, acc: 62.50%] [G loss: 2.381670]\n",
      "epoch:11 step:10738 [D loss: 0.631637, acc: 65.62%] [G loss: 2.205486]\n",
      "epoch:11 step:10739 [D loss: 0.652085, acc: 63.28%] [G loss: 1.852096]\n",
      "epoch:11 step:10740 [D loss: 0.660770, acc: 62.50%] [G loss: 1.972890]\n",
      "epoch:11 step:10741 [D loss: 0.639060, acc: 64.84%] [G loss: 2.032972]\n",
      "epoch:11 step:10742 [D loss: 0.629534, acc: 64.06%] [G loss: 2.126102]\n",
      "epoch:11 step:10743 [D loss: 0.637346, acc: 62.50%] [G loss: 1.992910]\n",
      "epoch:11 step:10744 [D loss: 0.700772, acc: 60.16%] [G loss: 1.814078]\n",
      "epoch:11 step:10745 [D loss: 0.664690, acc: 65.62%] [G loss: 1.883488]\n",
      "epoch:11 step:10746 [D loss: 0.712806, acc: 52.34%] [G loss: 1.862718]\n",
      "epoch:11 step:10747 [D loss: 0.604650, acc: 71.09%] [G loss: 1.957249]\n",
      "epoch:11 step:10748 [D loss: 0.669672, acc: 58.59%] [G loss: 1.871878]\n",
      "epoch:11 step:10749 [D loss: 0.670196, acc: 57.81%] [G loss: 1.722556]\n",
      "epoch:11 step:10750 [D loss: 0.648042, acc: 64.84%] [G loss: 2.006627]\n",
      "epoch:11 step:10751 [D loss: 0.640112, acc: 60.94%] [G loss: 1.844071]\n",
      "epoch:11 step:10752 [D loss: 0.600883, acc: 70.31%] [G loss: 1.869924]\n",
      "epoch:11 step:10753 [D loss: 0.616745, acc: 66.41%] [G loss: 1.943558]\n",
      "epoch:11 step:10754 [D loss: 0.642053, acc: 60.16%] [G loss: 2.036485]\n",
      "epoch:11 step:10755 [D loss: 0.659116, acc: 57.81%] [G loss: 1.928542]\n",
      "epoch:11 step:10756 [D loss: 0.647542, acc: 61.72%] [G loss: 1.933569]\n",
      "epoch:11 step:10757 [D loss: 0.640394, acc: 70.31%] [G loss: 2.067452]\n",
      "epoch:11 step:10758 [D loss: 0.580789, acc: 73.44%] [G loss: 2.171486]\n",
      "epoch:11 step:10759 [D loss: 0.610992, acc: 70.31%] [G loss: 1.925287]\n",
      "epoch:11 step:10760 [D loss: 0.573570, acc: 68.75%] [G loss: 2.058084]\n",
      "epoch:11 step:10761 [D loss: 0.646319, acc: 67.19%] [G loss: 1.994734]\n",
      "epoch:11 step:10762 [D loss: 0.663355, acc: 60.94%] [G loss: 2.079462]\n",
      "epoch:11 step:10763 [D loss: 0.662469, acc: 60.94%] [G loss: 1.961020]\n",
      "epoch:11 step:10764 [D loss: 0.612450, acc: 71.88%] [G loss: 2.036768]\n",
      "epoch:11 step:10765 [D loss: 0.619505, acc: 66.41%] [G loss: 1.885001]\n",
      "epoch:11 step:10766 [D loss: 0.721489, acc: 51.56%] [G loss: 1.965287]\n",
      "epoch:11 step:10767 [D loss: 0.617511, acc: 64.06%] [G loss: 1.990095]\n",
      "epoch:11 step:10768 [D loss: 0.617113, acc: 64.06%] [G loss: 2.045793]\n",
      "epoch:11 step:10769 [D loss: 0.606388, acc: 67.19%] [G loss: 2.123395]\n",
      "epoch:11 step:10770 [D loss: 0.626215, acc: 68.75%] [G loss: 2.109771]\n",
      "epoch:11 step:10771 [D loss: 0.581246, acc: 66.41%] [G loss: 1.963169]\n",
      "epoch:11 step:10772 [D loss: 0.684386, acc: 61.72%] [G loss: 2.044104]\n",
      "epoch:11 step:10773 [D loss: 0.627237, acc: 61.72%] [G loss: 1.854460]\n",
      "epoch:11 step:10774 [D loss: 0.649317, acc: 66.41%] [G loss: 2.002178]\n",
      "epoch:11 step:10775 [D loss: 0.678495, acc: 56.25%] [G loss: 1.937057]\n",
      "epoch:11 step:10776 [D loss: 0.636524, acc: 65.62%] [G loss: 2.070701]\n",
      "epoch:11 step:10777 [D loss: 0.531020, acc: 73.44%] [G loss: 2.201853]\n",
      "epoch:11 step:10778 [D loss: 0.618728, acc: 67.19%] [G loss: 2.430293]\n",
      "epoch:11 step:10779 [D loss: 0.592992, acc: 64.84%] [G loss: 2.314411]\n",
      "epoch:11 step:10780 [D loss: 0.646925, acc: 62.50%] [G loss: 2.011146]\n",
      "epoch:11 step:10781 [D loss: 0.622862, acc: 62.50%] [G loss: 2.058629]\n",
      "epoch:11 step:10782 [D loss: 0.652823, acc: 55.47%] [G loss: 2.119410]\n",
      "epoch:11 step:10783 [D loss: 0.619358, acc: 67.19%] [G loss: 2.033626]\n",
      "epoch:11 step:10784 [D loss: 0.693698, acc: 57.03%] [G loss: 1.816944]\n",
      "epoch:11 step:10785 [D loss: 0.653691, acc: 62.50%] [G loss: 2.018780]\n",
      "epoch:11 step:10786 [D loss: 0.645698, acc: 60.94%] [G loss: 2.102197]\n",
      "epoch:11 step:10787 [D loss: 0.631152, acc: 64.84%] [G loss: 2.041394]\n",
      "epoch:11 step:10788 [D loss: 0.586937, acc: 70.31%] [G loss: 2.308811]\n",
      "epoch:11 step:10789 [D loss: 0.721226, acc: 53.12%] [G loss: 1.909395]\n",
      "epoch:11 step:10790 [D loss: 0.670342, acc: 57.81%] [G loss: 2.001787]\n",
      "epoch:11 step:10791 [D loss: 0.641048, acc: 63.28%] [G loss: 2.060637]\n",
      "epoch:11 step:10792 [D loss: 0.618001, acc: 71.09%] [G loss: 2.118826]\n",
      "epoch:11 step:10793 [D loss: 0.696720, acc: 53.12%] [G loss: 1.880465]\n",
      "epoch:11 step:10794 [D loss: 0.597879, acc: 66.41%] [G loss: 1.923340]\n",
      "epoch:11 step:10795 [D loss: 0.668384, acc: 61.72%] [G loss: 1.946823]\n",
      "epoch:11 step:10796 [D loss: 0.651396, acc: 63.28%] [G loss: 2.048637]\n",
      "epoch:11 step:10797 [D loss: 0.634849, acc: 62.50%] [G loss: 1.978122]\n",
      "epoch:11 step:10798 [D loss: 0.628035, acc: 64.84%] [G loss: 2.031293]\n",
      "epoch:11 step:10799 [D loss: 0.604563, acc: 72.66%] [G loss: 1.907282]\n",
      "epoch:11 step:10800 [D loss: 0.690498, acc: 60.16%] [G loss: 1.869231]\n",
      "epoch:11 step:10801 [D loss: 0.616809, acc: 64.84%] [G loss: 1.966613]\n",
      "epoch:11 step:10802 [D loss: 0.588435, acc: 71.88%] [G loss: 2.157527]\n",
      "epoch:11 step:10803 [D loss: 0.621519, acc: 64.84%] [G loss: 1.978839]\n",
      "epoch:11 step:10804 [D loss: 0.595784, acc: 67.19%] [G loss: 2.221323]\n",
      "epoch:11 step:10805 [D loss: 0.575109, acc: 71.09%] [G loss: 2.152876]\n",
      "epoch:11 step:10806 [D loss: 0.517766, acc: 78.12%] [G loss: 2.252555]\n",
      "epoch:11 step:10807 [D loss: 0.680869, acc: 60.16%] [G loss: 1.991281]\n",
      "epoch:11 step:10808 [D loss: 0.794281, acc: 50.78%] [G loss: 1.796931]\n",
      "epoch:11 step:10809 [D loss: 0.694962, acc: 56.25%] [G loss: 1.762322]\n",
      "epoch:11 step:10810 [D loss: 0.653842, acc: 64.84%] [G loss: 2.032662]\n",
      "epoch:11 step:10811 [D loss: 0.589638, acc: 70.31%] [G loss: 2.260008]\n",
      "epoch:11 step:10812 [D loss: 0.670972, acc: 59.38%] [G loss: 2.011361]\n",
      "epoch:11 step:10813 [D loss: 0.634973, acc: 64.84%] [G loss: 1.867243]\n",
      "epoch:11 step:10814 [D loss: 0.643580, acc: 64.84%] [G loss: 1.876960]\n",
      "epoch:11 step:10815 [D loss: 0.617146, acc: 62.50%] [G loss: 2.043532]\n",
      "epoch:11 step:10816 [D loss: 0.678914, acc: 60.16%] [G loss: 1.947766]\n",
      "epoch:11 step:10817 [D loss: 0.689182, acc: 62.50%] [G loss: 1.876173]\n",
      "epoch:11 step:10818 [D loss: 0.671332, acc: 57.03%] [G loss: 1.938163]\n",
      "epoch:11 step:10819 [D loss: 0.648148, acc: 57.81%] [G loss: 1.926116]\n",
      "epoch:11 step:10820 [D loss: 0.644025, acc: 62.50%] [G loss: 2.037379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10821 [D loss: 0.628071, acc: 64.06%] [G loss: 2.076885]\n",
      "epoch:11 step:10822 [D loss: 0.596216, acc: 68.75%] [G loss: 1.926034]\n",
      "epoch:11 step:10823 [D loss: 0.613918, acc: 71.88%] [G loss: 1.901610]\n",
      "epoch:11 step:10824 [D loss: 0.594721, acc: 68.75%] [G loss: 2.153545]\n",
      "epoch:11 step:10825 [D loss: 0.604240, acc: 67.97%] [G loss: 1.896313]\n",
      "epoch:11 step:10826 [D loss: 0.620107, acc: 64.06%] [G loss: 2.033207]\n",
      "epoch:11 step:10827 [D loss: 0.658466, acc: 57.03%] [G loss: 2.137592]\n",
      "epoch:11 step:10828 [D loss: 0.637582, acc: 66.41%] [G loss: 2.073305]\n",
      "epoch:11 step:10829 [D loss: 0.613818, acc: 65.62%] [G loss: 2.142226]\n",
      "epoch:11 step:10830 [D loss: 0.613907, acc: 66.41%] [G loss: 2.024088]\n",
      "epoch:11 step:10831 [D loss: 0.640209, acc: 61.72%] [G loss: 2.109972]\n",
      "epoch:11 step:10832 [D loss: 0.640690, acc: 62.50%] [G loss: 1.986295]\n",
      "epoch:11 step:10833 [D loss: 0.626154, acc: 65.62%] [G loss: 1.986042]\n",
      "epoch:11 step:10834 [D loss: 0.627848, acc: 60.94%] [G loss: 2.099328]\n",
      "epoch:11 step:10835 [D loss: 0.667023, acc: 59.38%] [G loss: 1.969986]\n",
      "epoch:11 step:10836 [D loss: 0.677369, acc: 62.50%] [G loss: 1.956177]\n",
      "epoch:11 step:10837 [D loss: 0.643740, acc: 60.94%] [G loss: 2.090744]\n",
      "epoch:11 step:10838 [D loss: 0.698995, acc: 61.72%] [G loss: 1.873290]\n",
      "epoch:11 step:10839 [D loss: 0.636821, acc: 68.75%] [G loss: 2.142719]\n",
      "epoch:11 step:10840 [D loss: 0.601656, acc: 68.75%] [G loss: 1.998035]\n",
      "epoch:11 step:10841 [D loss: 0.566433, acc: 71.09%] [G loss: 2.145779]\n",
      "epoch:11 step:10842 [D loss: 0.672152, acc: 66.41%] [G loss: 1.962850]\n",
      "epoch:11 step:10843 [D loss: 0.620519, acc: 64.84%] [G loss: 2.069568]\n",
      "epoch:11 step:10844 [D loss: 0.633541, acc: 67.97%] [G loss: 1.913066]\n",
      "epoch:11 step:10845 [D loss: 0.650260, acc: 64.06%] [G loss: 1.965910]\n",
      "epoch:11 step:10846 [D loss: 0.617647, acc: 66.41%] [G loss: 2.045446]\n",
      "epoch:11 step:10847 [D loss: 0.685609, acc: 57.81%] [G loss: 2.037146]\n",
      "epoch:11 step:10848 [D loss: 0.630469, acc: 64.84%] [G loss: 1.944186]\n",
      "epoch:11 step:10849 [D loss: 0.701360, acc: 58.59%] [G loss: 1.882192]\n",
      "epoch:11 step:10850 [D loss: 0.629569, acc: 63.28%] [G loss: 1.883100]\n",
      "epoch:11 step:10851 [D loss: 0.635840, acc: 61.72%] [G loss: 1.997339]\n",
      "epoch:11 step:10852 [D loss: 0.622162, acc: 65.62%] [G loss: 2.070047]\n",
      "epoch:11 step:10853 [D loss: 0.649626, acc: 67.97%] [G loss: 1.923463]\n",
      "epoch:11 step:10854 [D loss: 0.665056, acc: 62.50%] [G loss: 1.973111]\n",
      "epoch:11 step:10855 [D loss: 0.589633, acc: 60.16%] [G loss: 2.208070]\n",
      "epoch:11 step:10856 [D loss: 0.588696, acc: 69.53%] [G loss: 2.103889]\n",
      "epoch:11 step:10857 [D loss: 0.591829, acc: 68.75%] [G loss: 2.077059]\n",
      "epoch:11 step:10858 [D loss: 0.584555, acc: 69.53%] [G loss: 2.312771]\n",
      "epoch:11 step:10859 [D loss: 0.608570, acc: 67.19%] [G loss: 2.302764]\n",
      "epoch:11 step:10860 [D loss: 0.630118, acc: 64.84%] [G loss: 2.005017]\n",
      "epoch:11 step:10861 [D loss: 0.637725, acc: 64.84%] [G loss: 2.228377]\n",
      "epoch:11 step:10862 [D loss: 0.626679, acc: 65.62%] [G loss: 2.117545]\n",
      "epoch:11 step:10863 [D loss: 0.633561, acc: 61.72%] [G loss: 2.171663]\n",
      "epoch:11 step:10864 [D loss: 0.633992, acc: 67.97%] [G loss: 2.190828]\n",
      "epoch:11 step:10865 [D loss: 0.611800, acc: 58.59%] [G loss: 2.181908]\n",
      "epoch:11 step:10866 [D loss: 0.679121, acc: 57.81%] [G loss: 1.966602]\n",
      "epoch:11 step:10867 [D loss: 0.624035, acc: 61.72%] [G loss: 1.978817]\n",
      "epoch:11 step:10868 [D loss: 0.651675, acc: 62.50%] [G loss: 1.955000]\n",
      "epoch:11 step:10869 [D loss: 0.626338, acc: 63.28%] [G loss: 1.994603]\n",
      "epoch:11 step:10870 [D loss: 0.631345, acc: 67.19%] [G loss: 2.082333]\n",
      "epoch:11 step:10871 [D loss: 0.572742, acc: 71.09%] [G loss: 2.240084]\n",
      "epoch:11 step:10872 [D loss: 0.685232, acc: 60.94%] [G loss: 2.013312]\n",
      "epoch:11 step:10873 [D loss: 0.708711, acc: 57.81%] [G loss: 1.864458]\n",
      "epoch:11 step:10874 [D loss: 0.655041, acc: 60.94%] [G loss: 1.884502]\n",
      "epoch:11 step:10875 [D loss: 0.628352, acc: 64.84%] [G loss: 2.247917]\n",
      "epoch:11 step:10876 [D loss: 0.647014, acc: 60.16%] [G loss: 1.893427]\n",
      "epoch:11 step:10877 [D loss: 0.569852, acc: 73.44%] [G loss: 2.004995]\n",
      "epoch:11 step:10878 [D loss: 0.622814, acc: 66.41%] [G loss: 2.051859]\n",
      "epoch:11 step:10879 [D loss: 0.680836, acc: 58.59%] [G loss: 1.974224]\n",
      "epoch:11 step:10880 [D loss: 0.646929, acc: 61.72%] [G loss: 1.974985]\n",
      "epoch:11 step:10881 [D loss: 0.670352, acc: 59.38%] [G loss: 1.985135]\n",
      "epoch:11 step:10882 [D loss: 0.619591, acc: 67.97%] [G loss: 1.915686]\n",
      "epoch:11 step:10883 [D loss: 0.697436, acc: 59.38%] [G loss: 1.842547]\n",
      "epoch:11 step:10884 [D loss: 0.650349, acc: 63.28%] [G loss: 1.831896]\n",
      "epoch:11 step:10885 [D loss: 0.619604, acc: 65.62%] [G loss: 1.967006]\n",
      "epoch:11 step:10886 [D loss: 0.711538, acc: 54.69%] [G loss: 1.880585]\n",
      "epoch:11 step:10887 [D loss: 0.668530, acc: 59.38%] [G loss: 1.917307]\n",
      "epoch:11 step:10888 [D loss: 0.638034, acc: 57.03%] [G loss: 1.944268]\n",
      "epoch:11 step:10889 [D loss: 0.613464, acc: 67.19%] [G loss: 2.039920]\n",
      "epoch:11 step:10890 [D loss: 0.652561, acc: 60.16%] [G loss: 2.010141]\n",
      "epoch:11 step:10891 [D loss: 0.642605, acc: 64.84%] [G loss: 1.942790]\n",
      "epoch:11 step:10892 [D loss: 0.671223, acc: 60.94%] [G loss: 1.914079]\n",
      "epoch:11 step:10893 [D loss: 0.607938, acc: 71.09%] [G loss: 1.872979]\n",
      "epoch:11 step:10894 [D loss: 0.660780, acc: 61.72%] [G loss: 1.861772]\n",
      "epoch:11 step:10895 [D loss: 0.641803, acc: 63.28%] [G loss: 2.005545]\n",
      "epoch:11 step:10896 [D loss: 0.614692, acc: 67.97%] [G loss: 2.026168]\n",
      "epoch:11 step:10897 [D loss: 0.674413, acc: 56.25%] [G loss: 2.121024]\n",
      "epoch:11 step:10898 [D loss: 0.645741, acc: 62.50%] [G loss: 2.028767]\n",
      "epoch:11 step:10899 [D loss: 0.573389, acc: 77.34%] [G loss: 1.893695]\n",
      "epoch:11 step:10900 [D loss: 0.651329, acc: 65.62%] [G loss: 2.002901]\n",
      "epoch:11 step:10901 [D loss: 0.648688, acc: 63.28%] [G loss: 2.048490]\n",
      "epoch:11 step:10902 [D loss: 0.640046, acc: 62.50%] [G loss: 1.814486]\n",
      "epoch:11 step:10903 [D loss: 0.644339, acc: 63.28%] [G loss: 1.914463]\n",
      "epoch:11 step:10904 [D loss: 0.658820, acc: 67.19%] [G loss: 2.024857]\n",
      "epoch:11 step:10905 [D loss: 0.672431, acc: 64.06%] [G loss: 1.894994]\n",
      "epoch:11 step:10906 [D loss: 0.660486, acc: 63.28%] [G loss: 1.847358]\n",
      "epoch:11 step:10907 [D loss: 0.631768, acc: 62.50%] [G loss: 2.043030]\n",
      "epoch:11 step:10908 [D loss: 0.719695, acc: 60.94%] [G loss: 1.797760]\n",
      "epoch:11 step:10909 [D loss: 0.594502, acc: 66.41%] [G loss: 1.908704]\n",
      "epoch:11 step:10910 [D loss: 0.668652, acc: 62.50%] [G loss: 1.926833]\n",
      "epoch:11 step:10911 [D loss: 0.630422, acc: 70.31%] [G loss: 1.949809]\n",
      "epoch:11 step:10912 [D loss: 0.621267, acc: 70.31%] [G loss: 1.950852]\n",
      "epoch:11 step:10913 [D loss: 0.664904, acc: 58.59%] [G loss: 1.955643]\n",
      "epoch:11 step:10914 [D loss: 0.667275, acc: 57.81%] [G loss: 1.946749]\n",
      "epoch:11 step:10915 [D loss: 0.618038, acc: 68.75%] [G loss: 1.960932]\n",
      "epoch:11 step:10916 [D loss: 0.632112, acc: 60.16%] [G loss: 1.902529]\n",
      "epoch:11 step:10917 [D loss: 0.674744, acc: 60.16%] [G loss: 1.907176]\n",
      "epoch:11 step:10918 [D loss: 0.713716, acc: 53.12%] [G loss: 1.845201]\n",
      "epoch:11 step:10919 [D loss: 0.657438, acc: 64.84%] [G loss: 1.867870]\n",
      "epoch:11 step:10920 [D loss: 0.608702, acc: 66.41%] [G loss: 2.189418]\n",
      "epoch:11 step:10921 [D loss: 0.665668, acc: 60.94%] [G loss: 1.813564]\n",
      "epoch:11 step:10922 [D loss: 0.644601, acc: 62.50%] [G loss: 1.852339]\n",
      "epoch:11 step:10923 [D loss: 0.637187, acc: 71.09%] [G loss: 2.011469]\n",
      "epoch:11 step:10924 [D loss: 0.642855, acc: 58.59%] [G loss: 1.945070]\n",
      "epoch:11 step:10925 [D loss: 0.649747, acc: 67.19%] [G loss: 2.121746]\n",
      "epoch:11 step:10926 [D loss: 0.631631, acc: 58.59%] [G loss: 1.835083]\n",
      "epoch:11 step:10927 [D loss: 0.670889, acc: 60.16%] [G loss: 2.009376]\n",
      "epoch:11 step:10928 [D loss: 0.672805, acc: 61.72%] [G loss: 1.752488]\n",
      "epoch:11 step:10929 [D loss: 0.591558, acc: 71.09%] [G loss: 1.947744]\n",
      "epoch:11 step:10930 [D loss: 0.643153, acc: 66.41%] [G loss: 2.065748]\n",
      "epoch:11 step:10931 [D loss: 0.598510, acc: 74.22%] [G loss: 2.062196]\n",
      "epoch:11 step:10932 [D loss: 0.616330, acc: 68.75%] [G loss: 1.850512]\n",
      "epoch:11 step:10933 [D loss: 0.621937, acc: 66.41%] [G loss: 2.000315]\n",
      "epoch:11 step:10934 [D loss: 0.622179, acc: 61.72%] [G loss: 2.090157]\n",
      "epoch:11 step:10935 [D loss: 0.630688, acc: 66.41%] [G loss: 2.088201]\n",
      "epoch:11 step:10936 [D loss: 0.644053, acc: 67.97%] [G loss: 2.150155]\n",
      "epoch:11 step:10937 [D loss: 0.601967, acc: 68.75%] [G loss: 2.024240]\n",
      "epoch:11 step:10938 [D loss: 0.592676, acc: 71.09%] [G loss: 2.133402]\n",
      "epoch:11 step:10939 [D loss: 0.635070, acc: 64.06%] [G loss: 2.044746]\n",
      "epoch:11 step:10940 [D loss: 0.611120, acc: 63.28%] [G loss: 2.036242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:10941 [D loss: 0.602971, acc: 67.19%] [G loss: 2.058272]\n",
      "epoch:11 step:10942 [D loss: 0.657015, acc: 63.28%] [G loss: 2.128738]\n",
      "epoch:11 step:10943 [D loss: 0.611716, acc: 61.72%] [G loss: 1.982896]\n",
      "epoch:11 step:10944 [D loss: 0.634537, acc: 67.97%] [G loss: 2.135679]\n",
      "epoch:11 step:10945 [D loss: 0.577972, acc: 70.31%] [G loss: 2.012182]\n",
      "epoch:11 step:10946 [D loss: 0.572046, acc: 73.44%] [G loss: 2.194705]\n",
      "epoch:11 step:10947 [D loss: 0.601973, acc: 66.41%] [G loss: 2.010970]\n",
      "epoch:11 step:10948 [D loss: 0.625089, acc: 61.72%] [G loss: 2.154461]\n",
      "epoch:11 step:10949 [D loss: 0.546517, acc: 71.09%] [G loss: 2.239257]\n",
      "epoch:11 step:10950 [D loss: 0.677937, acc: 61.72%] [G loss: 1.996189]\n",
      "epoch:11 step:10951 [D loss: 0.664525, acc: 61.72%] [G loss: 2.005743]\n",
      "epoch:11 step:10952 [D loss: 0.656986, acc: 64.84%] [G loss: 2.115501]\n",
      "epoch:11 step:10953 [D loss: 0.632496, acc: 71.09%] [G loss: 1.989617]\n",
      "epoch:11 step:10954 [D loss: 0.619246, acc: 65.62%] [G loss: 2.206110]\n",
      "epoch:11 step:10955 [D loss: 0.548193, acc: 70.31%] [G loss: 2.353745]\n",
      "epoch:11 step:10956 [D loss: 0.618129, acc: 69.53%] [G loss: 2.320728]\n",
      "epoch:11 step:10957 [D loss: 0.611423, acc: 64.84%] [G loss: 2.096778]\n",
      "epoch:11 step:10958 [D loss: 0.645027, acc: 64.84%] [G loss: 2.086020]\n",
      "epoch:11 step:10959 [D loss: 0.661866, acc: 59.38%] [G loss: 1.895591]\n",
      "epoch:11 step:10960 [D loss: 0.610671, acc: 65.62%] [G loss: 2.128926]\n",
      "epoch:11 step:10961 [D loss: 0.587815, acc: 67.97%] [G loss: 2.145586]\n",
      "epoch:11 step:10962 [D loss: 0.647035, acc: 62.50%] [G loss: 1.994520]\n",
      "epoch:11 step:10963 [D loss: 0.680705, acc: 63.28%] [G loss: 1.911999]\n",
      "epoch:11 step:10964 [D loss: 0.672188, acc: 56.25%] [G loss: 1.804195]\n",
      "epoch:11 step:10965 [D loss: 0.682819, acc: 59.38%] [G loss: 1.960678]\n",
      "epoch:11 step:10966 [D loss: 0.612701, acc: 66.41%] [G loss: 1.918744]\n",
      "epoch:11 step:10967 [D loss: 0.611861, acc: 67.19%] [G loss: 1.881686]\n",
      "epoch:11 step:10968 [D loss: 0.621108, acc: 63.28%] [G loss: 1.935241]\n",
      "epoch:11 step:10969 [D loss: 0.617142, acc: 65.62%] [G loss: 1.886708]\n",
      "epoch:11 step:10970 [D loss: 0.668881, acc: 64.84%] [G loss: 2.019350]\n",
      "epoch:11 step:10971 [D loss: 0.702327, acc: 57.81%] [G loss: 1.786193]\n",
      "epoch:11 step:10972 [D loss: 0.636948, acc: 64.84%] [G loss: 1.984691]\n",
      "epoch:11 step:10973 [D loss: 0.606623, acc: 70.31%] [G loss: 2.022264]\n",
      "epoch:11 step:10974 [D loss: 0.654914, acc: 57.81%] [G loss: 1.939977]\n",
      "epoch:11 step:10975 [D loss: 0.610852, acc: 66.41%] [G loss: 1.962116]\n",
      "epoch:11 step:10976 [D loss: 0.716080, acc: 54.69%] [G loss: 1.902047]\n",
      "epoch:11 step:10977 [D loss: 0.650966, acc: 66.41%] [G loss: 1.885085]\n",
      "epoch:11 step:10978 [D loss: 0.642166, acc: 64.84%] [G loss: 2.098177]\n",
      "epoch:11 step:10979 [D loss: 0.695076, acc: 60.16%] [G loss: 1.924352]\n",
      "epoch:11 step:10980 [D loss: 0.665911, acc: 60.16%] [G loss: 2.024331]\n",
      "epoch:11 step:10981 [D loss: 0.619604, acc: 68.75%] [G loss: 2.028214]\n",
      "epoch:11 step:10982 [D loss: 0.643577, acc: 63.28%] [G loss: 2.007977]\n",
      "epoch:11 step:10983 [D loss: 0.681150, acc: 57.81%] [G loss: 1.895957]\n",
      "epoch:11 step:10984 [D loss: 0.577796, acc: 67.19%] [G loss: 2.055423]\n",
      "epoch:11 step:10985 [D loss: 0.625245, acc: 63.28%] [G loss: 2.051254]\n",
      "epoch:11 step:10986 [D loss: 0.609635, acc: 66.41%] [G loss: 2.239925]\n",
      "epoch:11 step:10987 [D loss: 0.621070, acc: 67.19%] [G loss: 2.028468]\n",
      "epoch:11 step:10988 [D loss: 0.641094, acc: 57.03%] [G loss: 2.048248]\n",
      "epoch:11 step:10989 [D loss: 0.592490, acc: 67.97%] [G loss: 2.067295]\n",
      "epoch:11 step:10990 [D loss: 0.632899, acc: 59.38%] [G loss: 2.065896]\n",
      "epoch:11 step:10991 [D loss: 0.666584, acc: 56.25%] [G loss: 1.983702]\n",
      "epoch:11 step:10992 [D loss: 0.642680, acc: 64.06%] [G loss: 1.975141]\n",
      "epoch:11 step:10993 [D loss: 0.648293, acc: 64.84%] [G loss: 1.947807]\n",
      "epoch:11 step:10994 [D loss: 0.609792, acc: 69.53%] [G loss: 2.160844]\n",
      "epoch:11 step:10995 [D loss: 0.592439, acc: 66.41%] [G loss: 2.076231]\n",
      "epoch:11 step:10996 [D loss: 0.650806, acc: 63.28%] [G loss: 2.065371]\n",
      "epoch:11 step:10997 [D loss: 0.669341, acc: 58.59%] [G loss: 2.081080]\n",
      "epoch:11 step:10998 [D loss: 0.581208, acc: 71.09%] [G loss: 2.284449]\n",
      "epoch:11 step:10999 [D loss: 0.656704, acc: 64.06%] [G loss: 2.153717]\n",
      "epoch:11 step:11000 [D loss: 0.620115, acc: 66.41%] [G loss: 2.237221]\n",
      "epoch:11 step:11001 [D loss: 0.619026, acc: 64.06%] [G loss: 2.163118]\n",
      "epoch:11 step:11002 [D loss: 0.618476, acc: 67.97%] [G loss: 2.194684]\n",
      "epoch:11 step:11003 [D loss: 0.608719, acc: 61.72%] [G loss: 2.015865]\n",
      "epoch:11 step:11004 [D loss: 0.620158, acc: 67.19%] [G loss: 2.135509]\n",
      "epoch:11 step:11005 [D loss: 0.618883, acc: 67.19%] [G loss: 2.073353]\n",
      "epoch:11 step:11006 [D loss: 0.644601, acc: 67.19%] [G loss: 2.107677]\n",
      "epoch:11 step:11007 [D loss: 0.708542, acc: 55.47%] [G loss: 2.011125]\n",
      "epoch:11 step:11008 [D loss: 0.608564, acc: 65.62%] [G loss: 1.990428]\n",
      "epoch:11 step:11009 [D loss: 0.655667, acc: 58.59%] [G loss: 1.857697]\n",
      "epoch:11 step:11010 [D loss: 0.649227, acc: 64.84%] [G loss: 1.946152]\n",
      "epoch:11 step:11011 [D loss: 0.687053, acc: 60.16%] [G loss: 1.822313]\n",
      "epoch:11 step:11012 [D loss: 0.690912, acc: 52.34%] [G loss: 1.889248]\n",
      "epoch:11 step:11013 [D loss: 0.649439, acc: 64.06%] [G loss: 2.173575]\n",
      "epoch:11 step:11014 [D loss: 0.578089, acc: 71.88%] [G loss: 2.285798]\n",
      "epoch:11 step:11015 [D loss: 0.560926, acc: 72.66%] [G loss: 2.196814]\n",
      "epoch:11 step:11016 [D loss: 0.649054, acc: 64.06%] [G loss: 2.379350]\n",
      "epoch:11 step:11017 [D loss: 0.697481, acc: 57.81%] [G loss: 1.788978]\n",
      "epoch:11 step:11018 [D loss: 0.659800, acc: 57.81%] [G loss: 2.008519]\n",
      "epoch:11 step:11019 [D loss: 0.636026, acc: 63.28%] [G loss: 2.154484]\n",
      "epoch:11 step:11020 [D loss: 0.653072, acc: 64.84%] [G loss: 1.959525]\n",
      "epoch:11 step:11021 [D loss: 0.580181, acc: 68.75%] [G loss: 2.029568]\n",
      "epoch:11 step:11022 [D loss: 0.590722, acc: 67.97%] [G loss: 1.992211]\n",
      "epoch:11 step:11023 [D loss: 0.718703, acc: 51.56%] [G loss: 1.785282]\n",
      "epoch:11 step:11024 [D loss: 0.664516, acc: 55.47%] [G loss: 1.880824]\n",
      "epoch:11 step:11025 [D loss: 0.615646, acc: 64.06%] [G loss: 2.158482]\n",
      "epoch:11 step:11026 [D loss: 0.604924, acc: 69.53%] [G loss: 1.966290]\n",
      "epoch:11 step:11027 [D loss: 0.637759, acc: 60.16%] [G loss: 1.939282]\n",
      "epoch:11 step:11028 [D loss: 0.716352, acc: 57.03%] [G loss: 2.019621]\n",
      "epoch:11 step:11029 [D loss: 0.632811, acc: 64.06%] [G loss: 1.990990]\n",
      "epoch:11 step:11030 [D loss: 0.622254, acc: 66.41%] [G loss: 2.132188]\n",
      "epoch:11 step:11031 [D loss: 0.649715, acc: 64.84%] [G loss: 1.964135]\n",
      "epoch:11 step:11032 [D loss: 0.686690, acc: 60.16%] [G loss: 1.956282]\n",
      "epoch:11 step:11033 [D loss: 0.665178, acc: 54.69%] [G loss: 1.933185]\n",
      "epoch:11 step:11034 [D loss: 0.651722, acc: 63.28%] [G loss: 1.865963]\n",
      "epoch:11 step:11035 [D loss: 0.614420, acc: 66.41%] [G loss: 2.143012]\n",
      "epoch:11 step:11036 [D loss: 0.629269, acc: 63.28%] [G loss: 1.907273]\n",
      "epoch:11 step:11037 [D loss: 0.664428, acc: 53.12%] [G loss: 1.721019]\n",
      "epoch:11 step:11038 [D loss: 0.633008, acc: 64.84%] [G loss: 1.998900]\n",
      "epoch:11 step:11039 [D loss: 0.640339, acc: 64.84%] [G loss: 2.105043]\n",
      "epoch:11 step:11040 [D loss: 0.624728, acc: 65.62%] [G loss: 2.000481]\n",
      "epoch:11 step:11041 [D loss: 0.660008, acc: 61.72%] [G loss: 1.835680]\n",
      "epoch:11 step:11042 [D loss: 0.644425, acc: 60.94%] [G loss: 1.926417]\n",
      "epoch:11 step:11043 [D loss: 0.602860, acc: 65.62%] [G loss: 1.890415]\n",
      "epoch:11 step:11044 [D loss: 0.624978, acc: 66.41%] [G loss: 1.971412]\n",
      "epoch:11 step:11045 [D loss: 0.671846, acc: 60.94%] [G loss: 1.901726]\n",
      "epoch:11 step:11046 [D loss: 0.633604, acc: 62.50%] [G loss: 1.945507]\n",
      "epoch:11 step:11047 [D loss: 0.605087, acc: 69.53%] [G loss: 2.084713]\n",
      "epoch:11 step:11048 [D loss: 0.704602, acc: 60.16%] [G loss: 1.714523]\n",
      "epoch:11 step:11049 [D loss: 0.642203, acc: 60.94%] [G loss: 1.843760]\n",
      "epoch:11 step:11050 [D loss: 0.618583, acc: 62.50%] [G loss: 2.127741]\n",
      "epoch:11 step:11051 [D loss: 0.632912, acc: 67.19%] [G loss: 2.019899]\n",
      "epoch:11 step:11052 [D loss: 0.657282, acc: 60.16%] [G loss: 1.922308]\n",
      "epoch:11 step:11053 [D loss: 0.626563, acc: 61.72%] [G loss: 1.943723]\n",
      "epoch:11 step:11054 [D loss: 0.608871, acc: 67.97%] [G loss: 1.892106]\n",
      "epoch:11 step:11055 [D loss: 0.668698, acc: 58.59%] [G loss: 1.982066]\n",
      "epoch:11 step:11056 [D loss: 0.661061, acc: 57.81%] [G loss: 1.896381]\n",
      "epoch:11 step:11057 [D loss: 0.647694, acc: 60.94%] [G loss: 1.885149]\n",
      "epoch:11 step:11058 [D loss: 0.680993, acc: 54.69%] [G loss: 1.789102]\n",
      "epoch:11 step:11059 [D loss: 0.652874, acc: 60.94%] [G loss: 2.011181]\n",
      "epoch:11 step:11060 [D loss: 0.625001, acc: 63.28%] [G loss: 1.904428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11061 [D loss: 0.613222, acc: 69.53%] [G loss: 1.969312]\n",
      "epoch:11 step:11062 [D loss: 0.643442, acc: 61.72%] [G loss: 2.020910]\n",
      "epoch:11 step:11063 [D loss: 0.649593, acc: 66.41%] [G loss: 1.876367]\n",
      "epoch:11 step:11064 [D loss: 0.615491, acc: 66.41%] [G loss: 2.050711]\n",
      "epoch:11 step:11065 [D loss: 0.634944, acc: 61.72%] [G loss: 2.013623]\n",
      "epoch:11 step:11066 [D loss: 0.603038, acc: 67.19%] [G loss: 1.910270]\n",
      "epoch:11 step:11067 [D loss: 0.611408, acc: 69.53%] [G loss: 1.872546]\n",
      "epoch:11 step:11068 [D loss: 0.641758, acc: 68.75%] [G loss: 1.907719]\n",
      "epoch:11 step:11069 [D loss: 0.640897, acc: 57.03%] [G loss: 1.903428]\n",
      "epoch:11 step:11070 [D loss: 0.618896, acc: 64.06%] [G loss: 1.941655]\n",
      "epoch:11 step:11071 [D loss: 0.632128, acc: 66.41%] [G loss: 1.993277]\n",
      "epoch:11 step:11072 [D loss: 0.703331, acc: 57.03%] [G loss: 1.842007]\n",
      "epoch:11 step:11073 [D loss: 0.657484, acc: 58.59%] [G loss: 1.743115]\n",
      "epoch:11 step:11074 [D loss: 0.656528, acc: 64.84%] [G loss: 1.926360]\n",
      "epoch:11 step:11075 [D loss: 0.676030, acc: 53.12%] [G loss: 1.830040]\n",
      "epoch:11 step:11076 [D loss: 0.670460, acc: 60.94%] [G loss: 1.846973]\n",
      "epoch:11 step:11077 [D loss: 0.630815, acc: 65.62%] [G loss: 1.956641]\n",
      "epoch:11 step:11078 [D loss: 0.627982, acc: 65.62%] [G loss: 1.880671]\n",
      "epoch:11 step:11079 [D loss: 0.608325, acc: 65.62%] [G loss: 1.902558]\n",
      "epoch:11 step:11080 [D loss: 0.654207, acc: 64.06%] [G loss: 1.904902]\n",
      "epoch:11 step:11081 [D loss: 0.642733, acc: 66.41%] [G loss: 2.015907]\n",
      "epoch:11 step:11082 [D loss: 0.620753, acc: 63.28%] [G loss: 2.085197]\n",
      "epoch:11 step:11083 [D loss: 0.628742, acc: 62.50%] [G loss: 1.981606]\n",
      "epoch:11 step:11084 [D loss: 0.626690, acc: 68.75%] [G loss: 2.086585]\n",
      "epoch:11 step:11085 [D loss: 0.606907, acc: 67.19%] [G loss: 1.933140]\n",
      "epoch:11 step:11086 [D loss: 0.587468, acc: 74.22%] [G loss: 2.058177]\n",
      "epoch:11 step:11087 [D loss: 0.658530, acc: 58.59%] [G loss: 2.032599]\n",
      "epoch:11 step:11088 [D loss: 0.568618, acc: 75.00%] [G loss: 2.250893]\n",
      "epoch:11 step:11089 [D loss: 0.584633, acc: 67.19%] [G loss: 2.244926]\n",
      "epoch:11 step:11090 [D loss: 0.553496, acc: 72.66%] [G loss: 2.009721]\n",
      "epoch:11 step:11091 [D loss: 0.677448, acc: 57.03%] [G loss: 1.838663]\n",
      "epoch:11 step:11092 [D loss: 0.644452, acc: 62.50%] [G loss: 2.020898]\n",
      "epoch:11 step:11093 [D loss: 0.680864, acc: 59.38%] [G loss: 2.052305]\n",
      "epoch:11 step:11094 [D loss: 0.725913, acc: 56.25%] [G loss: 1.900600]\n",
      "epoch:11 step:11095 [D loss: 0.651659, acc: 62.50%] [G loss: 1.900339]\n",
      "epoch:11 step:11096 [D loss: 0.643261, acc: 64.06%] [G loss: 1.969512]\n",
      "epoch:11 step:11097 [D loss: 0.659894, acc: 55.47%] [G loss: 2.153597]\n",
      "epoch:11 step:11098 [D loss: 0.572945, acc: 70.31%] [G loss: 2.183976]\n",
      "epoch:11 step:11099 [D loss: 0.604810, acc: 68.75%] [G loss: 2.394896]\n",
      "epoch:11 step:11100 [D loss: 0.636700, acc: 60.94%] [G loss: 2.146257]\n",
      "epoch:11 step:11101 [D loss: 0.688503, acc: 58.59%] [G loss: 1.930007]\n",
      "epoch:11 step:11102 [D loss: 0.674543, acc: 60.16%] [G loss: 2.010267]\n",
      "epoch:11 step:11103 [D loss: 0.610662, acc: 61.72%] [G loss: 2.072892]\n",
      "epoch:11 step:11104 [D loss: 0.624090, acc: 63.28%] [G loss: 1.880003]\n",
      "epoch:11 step:11105 [D loss: 0.620393, acc: 60.94%] [G loss: 2.230149]\n",
      "epoch:11 step:11106 [D loss: 0.643875, acc: 61.72%] [G loss: 1.906178]\n",
      "epoch:11 step:11107 [D loss: 0.691336, acc: 57.03%] [G loss: 1.808627]\n",
      "epoch:11 step:11108 [D loss: 0.688883, acc: 61.72%] [G loss: 1.899112]\n",
      "epoch:11 step:11109 [D loss: 0.632455, acc: 60.94%] [G loss: 1.997770]\n",
      "epoch:11 step:11110 [D loss: 0.657779, acc: 62.50%] [G loss: 1.941558]\n",
      "epoch:11 step:11111 [D loss: 0.632959, acc: 64.06%] [G loss: 2.044832]\n",
      "epoch:11 step:11112 [D loss: 0.586963, acc: 66.41%] [G loss: 2.251279]\n",
      "epoch:11 step:11113 [D loss: 0.578004, acc: 68.75%] [G loss: 2.117456]\n",
      "epoch:11 step:11114 [D loss: 0.589680, acc: 68.75%] [G loss: 2.315001]\n",
      "epoch:11 step:11115 [D loss: 0.598979, acc: 67.19%] [G loss: 2.181276]\n",
      "epoch:11 step:11116 [D loss: 0.553738, acc: 72.66%] [G loss: 2.091546]\n",
      "epoch:11 step:11117 [D loss: 0.673857, acc: 67.97%] [G loss: 1.957366]\n",
      "epoch:11 step:11118 [D loss: 0.628953, acc: 62.50%] [G loss: 2.015499]\n",
      "epoch:11 step:11119 [D loss: 0.633281, acc: 57.81%] [G loss: 1.968631]\n",
      "epoch:11 step:11120 [D loss: 0.650222, acc: 57.81%] [G loss: 2.115374]\n",
      "epoch:11 step:11121 [D loss: 0.607371, acc: 67.97%] [G loss: 2.042203]\n",
      "epoch:11 step:11122 [D loss: 0.667771, acc: 60.16%] [G loss: 2.374264]\n",
      "epoch:11 step:11123 [D loss: 0.550323, acc: 68.75%] [G loss: 2.322651]\n",
      "epoch:11 step:11124 [D loss: 0.660867, acc: 67.19%] [G loss: 2.108266]\n",
      "epoch:11 step:11125 [D loss: 0.645182, acc: 58.59%] [G loss: 1.914586]\n",
      "epoch:11 step:11126 [D loss: 0.640210, acc: 64.06%] [G loss: 2.054317]\n",
      "epoch:11 step:11127 [D loss: 0.687862, acc: 54.69%] [G loss: 1.823286]\n",
      "epoch:11 step:11128 [D loss: 0.668144, acc: 57.03%] [G loss: 1.907676]\n",
      "epoch:11 step:11129 [D loss: 0.592545, acc: 70.31%] [G loss: 1.970867]\n",
      "epoch:11 step:11130 [D loss: 0.603943, acc: 66.41%] [G loss: 2.015247]\n",
      "epoch:11 step:11131 [D loss: 0.676172, acc: 64.84%] [G loss: 2.007665]\n",
      "epoch:11 step:11132 [D loss: 0.629092, acc: 67.19%] [G loss: 2.205210]\n",
      "epoch:11 step:11133 [D loss: 0.663435, acc: 59.38%] [G loss: 2.039077]\n",
      "epoch:11 step:11134 [D loss: 0.685505, acc: 56.25%] [G loss: 1.840729]\n",
      "epoch:11 step:11135 [D loss: 0.679037, acc: 54.69%] [G loss: 1.867116]\n",
      "epoch:11 step:11136 [D loss: 0.702802, acc: 56.25%] [G loss: 1.907555]\n",
      "epoch:11 step:11137 [D loss: 0.664717, acc: 59.38%] [G loss: 1.901990]\n",
      "epoch:11 step:11138 [D loss: 0.637883, acc: 66.41%] [G loss: 1.921946]\n",
      "epoch:11 step:11139 [D loss: 0.671191, acc: 60.94%] [G loss: 1.844665]\n",
      "epoch:11 step:11140 [D loss: 0.591798, acc: 70.31%] [G loss: 2.067373]\n",
      "epoch:11 step:11141 [D loss: 0.605982, acc: 70.31%] [G loss: 1.991461]\n",
      "epoch:11 step:11142 [D loss: 0.624346, acc: 64.84%] [G loss: 2.062246]\n",
      "epoch:11 step:11143 [D loss: 0.624718, acc: 65.62%] [G loss: 2.026650]\n",
      "epoch:11 step:11144 [D loss: 0.606636, acc: 64.84%] [G loss: 1.963515]\n",
      "epoch:11 step:11145 [D loss: 0.608761, acc: 64.06%] [G loss: 2.017230]\n",
      "epoch:11 step:11146 [D loss: 0.654756, acc: 60.16%] [G loss: 2.205502]\n",
      "epoch:11 step:11147 [D loss: 0.618359, acc: 63.28%] [G loss: 1.960492]\n",
      "epoch:11 step:11148 [D loss: 0.627713, acc: 64.84%] [G loss: 2.131073]\n",
      "epoch:11 step:11149 [D loss: 0.581631, acc: 65.62%] [G loss: 2.133448]\n",
      "epoch:11 step:11150 [D loss: 0.641112, acc: 64.84%] [G loss: 2.001178]\n",
      "epoch:11 step:11151 [D loss: 0.616071, acc: 69.53%] [G loss: 2.012699]\n",
      "epoch:11 step:11152 [D loss: 0.624894, acc: 59.38%] [G loss: 2.030868]\n",
      "epoch:11 step:11153 [D loss: 0.681400, acc: 64.06%] [G loss: 2.037174]\n",
      "epoch:11 step:11154 [D loss: 0.621681, acc: 62.50%] [G loss: 2.018626]\n",
      "epoch:11 step:11155 [D loss: 0.598924, acc: 69.53%] [G loss: 2.199021]\n",
      "epoch:11 step:11156 [D loss: 0.614217, acc: 67.97%] [G loss: 2.138371]\n",
      "epoch:11 step:11157 [D loss: 0.679884, acc: 60.94%] [G loss: 2.018790]\n",
      "epoch:11 step:11158 [D loss: 0.656059, acc: 56.25%] [G loss: 2.150812]\n",
      "epoch:11 step:11159 [D loss: 0.608970, acc: 64.06%] [G loss: 1.993104]\n",
      "epoch:11 step:11160 [D loss: 0.645557, acc: 60.16%] [G loss: 1.970377]\n",
      "epoch:11 step:11161 [D loss: 0.597870, acc: 67.19%] [G loss: 2.066769]\n",
      "epoch:11 step:11162 [D loss: 0.605369, acc: 66.41%] [G loss: 1.974005]\n",
      "epoch:11 step:11163 [D loss: 0.644421, acc: 64.06%] [G loss: 2.035735]\n",
      "epoch:11 step:11164 [D loss: 0.604775, acc: 65.62%] [G loss: 2.039692]\n",
      "epoch:11 step:11165 [D loss: 0.713325, acc: 53.91%] [G loss: 1.936880]\n",
      "epoch:11 step:11166 [D loss: 0.628856, acc: 64.84%] [G loss: 1.909539]\n",
      "epoch:11 step:11167 [D loss: 0.630218, acc: 67.19%] [G loss: 2.020781]\n",
      "epoch:11 step:11168 [D loss: 0.618808, acc: 67.97%] [G loss: 2.060111]\n",
      "epoch:11 step:11169 [D loss: 0.605906, acc: 66.41%] [G loss: 1.966254]\n",
      "epoch:11 step:11170 [D loss: 0.657101, acc: 64.84%] [G loss: 1.976562]\n",
      "epoch:11 step:11171 [D loss: 0.630080, acc: 67.97%] [G loss: 1.958905]\n",
      "epoch:11 step:11172 [D loss: 0.650818, acc: 59.38%] [G loss: 1.953588]\n",
      "epoch:11 step:11173 [D loss: 0.631327, acc: 61.72%] [G loss: 1.925255]\n",
      "epoch:11 step:11174 [D loss: 0.658666, acc: 64.06%] [G loss: 1.942147]\n",
      "epoch:11 step:11175 [D loss: 0.635794, acc: 69.53%] [G loss: 1.805448]\n",
      "epoch:11 step:11176 [D loss: 0.660077, acc: 59.38%] [G loss: 1.929616]\n",
      "epoch:11 step:11177 [D loss: 0.656660, acc: 66.41%] [G loss: 1.840453]\n",
      "epoch:11 step:11178 [D loss: 0.633196, acc: 59.38%] [G loss: 1.978009]\n",
      "epoch:11 step:11179 [D loss: 0.613699, acc: 67.97%] [G loss: 1.978604]\n",
      "epoch:11 step:11180 [D loss: 0.679700, acc: 63.28%] [G loss: 1.798625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:11181 [D loss: 0.656643, acc: 67.19%] [G loss: 1.911695]\n",
      "epoch:11 step:11182 [D loss: 0.617308, acc: 67.97%] [G loss: 2.117275]\n",
      "epoch:11 step:11183 [D loss: 0.651104, acc: 58.59%] [G loss: 2.066711]\n",
      "epoch:11 step:11184 [D loss: 0.635411, acc: 67.19%] [G loss: 2.016910]\n",
      "epoch:11 step:11185 [D loss: 0.629676, acc: 63.28%] [G loss: 1.967838]\n",
      "epoch:11 step:11186 [D loss: 0.657688, acc: 64.06%] [G loss: 1.910315]\n",
      "epoch:11 step:11187 [D loss: 0.676129, acc: 50.78%] [G loss: 1.963444]\n",
      "epoch:11 step:11188 [D loss: 0.631513, acc: 66.41%] [G loss: 2.035074]\n",
      "epoch:11 step:11189 [D loss: 0.660397, acc: 64.84%] [G loss: 1.942406]\n",
      "epoch:11 step:11190 [D loss: 0.631622, acc: 63.28%] [G loss: 2.098530]\n",
      "epoch:11 step:11191 [D loss: 0.578772, acc: 67.97%] [G loss: 2.171990]\n",
      "epoch:11 step:11192 [D loss: 0.646840, acc: 64.84%] [G loss: 2.131922]\n",
      "epoch:11 step:11193 [D loss: 0.601792, acc: 73.44%] [G loss: 2.218164]\n",
      "epoch:11 step:11194 [D loss: 0.629771, acc: 67.19%] [G loss: 1.977557]\n",
      "epoch:11 step:11195 [D loss: 0.680107, acc: 62.50%] [G loss: 2.028733]\n",
      "epoch:11 step:11196 [D loss: 0.667876, acc: 60.16%] [G loss: 2.124771]\n",
      "epoch:11 step:11197 [D loss: 0.639996, acc: 66.41%] [G loss: 1.937597]\n",
      "epoch:11 step:11198 [D loss: 0.640990, acc: 61.72%] [G loss: 1.778998]\n",
      "epoch:11 step:11199 [D loss: 0.662317, acc: 64.06%] [G loss: 1.940410]\n",
      "epoch:11 step:11200 [D loss: 0.613546, acc: 66.41%] [G loss: 1.862437]\n",
      "epoch:11 step:11201 [D loss: 0.595790, acc: 69.53%] [G loss: 2.100839]\n",
      "epoch:11 step:11202 [D loss: 0.619164, acc: 68.75%] [G loss: 1.981565]\n",
      "epoch:11 step:11203 [D loss: 0.663090, acc: 63.28%] [G loss: 1.740118]\n",
      "epoch:11 step:11204 [D loss: 0.662777, acc: 64.06%] [G loss: 1.966507]\n",
      "epoch:11 step:11205 [D loss: 0.562720, acc: 75.00%] [G loss: 1.876912]\n",
      "epoch:11 step:11206 [D loss: 0.607335, acc: 67.19%] [G loss: 2.228582]\n",
      "epoch:11 step:11207 [D loss: 0.630878, acc: 63.28%] [G loss: 2.007190]\n",
      "epoch:11 step:11208 [D loss: 0.596974, acc: 65.62%] [G loss: 2.105770]\n",
      "epoch:11 step:11209 [D loss: 0.638336, acc: 69.53%] [G loss: 2.179623]\n",
      "epoch:11 step:11210 [D loss: 0.608219, acc: 65.62%] [G loss: 2.076109]\n",
      "epoch:11 step:11211 [D loss: 0.642477, acc: 62.50%] [G loss: 2.067898]\n",
      "epoch:11 step:11212 [D loss: 0.626767, acc: 67.19%] [G loss: 2.053003]\n",
      "epoch:11 step:11213 [D loss: 0.706068, acc: 57.81%] [G loss: 2.008445]\n",
      "epoch:11 step:11214 [D loss: 0.584057, acc: 64.06%] [G loss: 2.272025]\n",
      "epoch:11 step:11215 [D loss: 0.597603, acc: 71.88%] [G loss: 1.973778]\n",
      "epoch:11 step:11216 [D loss: 0.630535, acc: 60.94%] [G loss: 2.033577]\n",
      "epoch:11 step:11217 [D loss: 0.646920, acc: 61.72%] [G loss: 1.995573]\n",
      "epoch:11 step:11218 [D loss: 0.614183, acc: 68.75%] [G loss: 2.165825]\n",
      "epoch:11 step:11219 [D loss: 0.578732, acc: 67.19%] [G loss: 2.389849]\n",
      "epoch:11 step:11220 [D loss: 0.630198, acc: 65.62%] [G loss: 2.159392]\n",
      "epoch:11 step:11221 [D loss: 0.656547, acc: 57.03%] [G loss: 2.103910]\n",
      "epoch:11 step:11222 [D loss: 0.674820, acc: 59.38%] [G loss: 1.964926]\n",
      "epoch:11 step:11223 [D loss: 0.633803, acc: 60.16%] [G loss: 1.896549]\n",
      "epoch:11 step:11224 [D loss: 0.645866, acc: 63.28%] [G loss: 1.933037]\n",
      "epoch:11 step:11225 [D loss: 0.551877, acc: 73.44%] [G loss: 2.148185]\n",
      "epoch:11 step:11226 [D loss: 0.600150, acc: 64.84%] [G loss: 2.297539]\n",
      "epoch:11 step:11227 [D loss: 0.657963, acc: 66.41%] [G loss: 1.983756]\n",
      "epoch:11 step:11228 [D loss: 0.652752, acc: 63.28%] [G loss: 2.054728]\n",
      "epoch:11 step:11229 [D loss: 0.613356, acc: 67.19%] [G loss: 2.197141]\n",
      "epoch:11 step:11230 [D loss: 0.553941, acc: 75.00%] [G loss: 2.177470]\n",
      "epoch:11 step:11231 [D loss: 0.588839, acc: 72.66%] [G loss: 2.277926]\n",
      "epoch:11 step:11232 [D loss: 0.536682, acc: 75.78%] [G loss: 2.271018]\n",
      "epoch:11 step:11233 [D loss: 0.626720, acc: 62.50%] [G loss: 2.391581]\n",
      "epoch:11 step:11234 [D loss: 0.622866, acc: 64.84%] [G loss: 2.203301]\n",
      "epoch:11 step:11235 [D loss: 0.782703, acc: 46.09%] [G loss: 1.815808]\n",
      "epoch:11 step:11236 [D loss: 0.791450, acc: 44.53%] [G loss: 2.018360]\n",
      "epoch:11 step:11237 [D loss: 0.586950, acc: 71.09%] [G loss: 2.135757]\n",
      "epoch:11 step:11238 [D loss: 0.652283, acc: 64.06%] [G loss: 2.090692]\n",
      "epoch:11 step:11239 [D loss: 0.600429, acc: 68.75%] [G loss: 1.980112]\n",
      "epoch:11 step:11240 [D loss: 0.628045, acc: 60.94%] [G loss: 2.112800]\n",
      "epoch:11 step:11241 [D loss: 0.614076, acc: 69.53%] [G loss: 2.156793]\n",
      "epoch:11 step:11242 [D loss: 0.646821, acc: 60.94%] [G loss: 1.912422]\n",
      "epoch:11 step:11243 [D loss: 0.595063, acc: 69.53%] [G loss: 2.105333]\n",
      "epoch:11 step:11244 [D loss: 0.539821, acc: 74.22%] [G loss: 2.374748]\n",
      "epoch:12 step:11245 [D loss: 0.655284, acc: 58.59%] [G loss: 1.910068]\n",
      "epoch:12 step:11246 [D loss: 0.648709, acc: 65.62%] [G loss: 2.150211]\n",
      "epoch:12 step:11247 [D loss: 0.605357, acc: 67.97%] [G loss: 1.946217]\n",
      "epoch:12 step:11248 [D loss: 0.688760, acc: 57.03%] [G loss: 1.930319]\n",
      "epoch:12 step:11249 [D loss: 0.649278, acc: 61.72%] [G loss: 2.077042]\n",
      "epoch:12 step:11250 [D loss: 0.646517, acc: 60.16%] [G loss: 1.864733]\n",
      "epoch:12 step:11251 [D loss: 0.622250, acc: 67.19%] [G loss: 1.964331]\n",
      "epoch:12 step:11252 [D loss: 0.646288, acc: 57.03%] [G loss: 2.186373]\n",
      "epoch:12 step:11253 [D loss: 0.620364, acc: 64.84%] [G loss: 1.972903]\n",
      "epoch:12 step:11254 [D loss: 0.635566, acc: 62.50%] [G loss: 2.000811]\n",
      "epoch:12 step:11255 [D loss: 0.643887, acc: 68.75%] [G loss: 1.991423]\n",
      "epoch:12 step:11256 [D loss: 0.623854, acc: 64.06%] [G loss: 1.987696]\n",
      "epoch:12 step:11257 [D loss: 0.601717, acc: 67.19%] [G loss: 1.962918]\n",
      "epoch:12 step:11258 [D loss: 0.633133, acc: 63.28%] [G loss: 1.981989]\n",
      "epoch:12 step:11259 [D loss: 0.588062, acc: 71.09%] [G loss: 2.224803]\n",
      "epoch:12 step:11260 [D loss: 0.576901, acc: 66.41%] [G loss: 1.985826]\n",
      "epoch:12 step:11261 [D loss: 0.652534, acc: 57.81%] [G loss: 1.798384]\n",
      "epoch:12 step:11262 [D loss: 0.614429, acc: 64.84%] [G loss: 1.993380]\n",
      "epoch:12 step:11263 [D loss: 0.628549, acc: 66.41%] [G loss: 1.883691]\n",
      "epoch:12 step:11264 [D loss: 0.678241, acc: 57.03%] [G loss: 1.892373]\n",
      "epoch:12 step:11265 [D loss: 0.619240, acc: 65.62%] [G loss: 1.979986]\n",
      "epoch:12 step:11266 [D loss: 0.656183, acc: 60.94%] [G loss: 2.003025]\n",
      "epoch:12 step:11267 [D loss: 0.609851, acc: 71.09%] [G loss: 2.163991]\n",
      "epoch:12 step:11268 [D loss: 0.573876, acc: 66.41%] [G loss: 2.136506]\n",
      "epoch:12 step:11269 [D loss: 0.538754, acc: 72.66%] [G loss: 2.204088]\n",
      "epoch:12 step:11270 [D loss: 0.707902, acc: 60.16%] [G loss: 1.923901]\n",
      "epoch:12 step:11271 [D loss: 0.635813, acc: 66.41%] [G loss: 1.924039]\n",
      "epoch:12 step:11272 [D loss: 0.633682, acc: 64.06%] [G loss: 1.849005]\n",
      "epoch:12 step:11273 [D loss: 0.605675, acc: 67.97%] [G loss: 2.187680]\n",
      "epoch:12 step:11274 [D loss: 0.706935, acc: 56.25%] [G loss: 1.987933]\n",
      "epoch:12 step:11275 [D loss: 0.651661, acc: 61.72%] [G loss: 1.722332]\n",
      "epoch:12 step:11276 [D loss: 0.624921, acc: 64.06%] [G loss: 2.031137]\n",
      "epoch:12 step:11277 [D loss: 0.640840, acc: 64.84%] [G loss: 1.872010]\n",
      "epoch:12 step:11278 [D loss: 0.583908, acc: 74.22%] [G loss: 2.067037]\n",
      "epoch:12 step:11279 [D loss: 0.601418, acc: 67.19%] [G loss: 2.070855]\n",
      "epoch:12 step:11280 [D loss: 0.538346, acc: 75.00%] [G loss: 2.209779]\n",
      "epoch:12 step:11281 [D loss: 0.628694, acc: 60.16%] [G loss: 1.990066]\n",
      "epoch:12 step:11282 [D loss: 0.674943, acc: 61.72%] [G loss: 2.024969]\n",
      "epoch:12 step:11283 [D loss: 0.600099, acc: 67.19%] [G loss: 2.022776]\n",
      "epoch:12 step:11284 [D loss: 0.607410, acc: 65.62%] [G loss: 2.271829]\n",
      "epoch:12 step:11285 [D loss: 0.592354, acc: 68.75%] [G loss: 1.995890]\n",
      "epoch:12 step:11286 [D loss: 0.579021, acc: 70.31%] [G loss: 2.234274]\n",
      "epoch:12 step:11287 [D loss: 0.676961, acc: 60.94%] [G loss: 1.960127]\n",
      "epoch:12 step:11288 [D loss: 0.649392, acc: 62.50%] [G loss: 1.921393]\n",
      "epoch:12 step:11289 [D loss: 0.631305, acc: 60.16%] [G loss: 2.191990]\n",
      "epoch:12 step:11290 [D loss: 0.636842, acc: 65.62%] [G loss: 1.979021]\n",
      "epoch:12 step:11291 [D loss: 0.601282, acc: 67.19%] [G loss: 2.028544]\n",
      "epoch:12 step:11292 [D loss: 0.613346, acc: 70.31%] [G loss: 2.096284]\n",
      "epoch:12 step:11293 [D loss: 0.596404, acc: 67.19%] [G loss: 2.023182]\n",
      "epoch:12 step:11294 [D loss: 0.570540, acc: 71.88%] [G loss: 2.064703]\n",
      "epoch:12 step:11295 [D loss: 0.696728, acc: 54.69%] [G loss: 1.848981]\n",
      "epoch:12 step:11296 [D loss: 0.633485, acc: 69.53%] [G loss: 1.920661]\n",
      "epoch:12 step:11297 [D loss: 0.591796, acc: 69.53%] [G loss: 2.027692]\n",
      "epoch:12 step:11298 [D loss: 0.645976, acc: 61.72%] [G loss: 2.056429]\n",
      "epoch:12 step:11299 [D loss: 0.590497, acc: 69.53%] [G loss: 2.159444]\n",
      "epoch:12 step:11300 [D loss: 0.628026, acc: 67.97%] [G loss: 2.058895]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11301 [D loss: 0.630707, acc: 64.06%] [G loss: 2.025081]\n",
      "epoch:12 step:11302 [D loss: 0.670084, acc: 58.59%] [G loss: 1.987878]\n",
      "epoch:12 step:11303 [D loss: 0.620395, acc: 67.19%] [G loss: 2.092846]\n",
      "epoch:12 step:11304 [D loss: 0.665165, acc: 58.59%] [G loss: 1.960534]\n",
      "epoch:12 step:11305 [D loss: 0.625168, acc: 67.19%] [G loss: 1.852377]\n",
      "epoch:12 step:11306 [D loss: 0.679818, acc: 60.16%] [G loss: 2.068217]\n",
      "epoch:12 step:11307 [D loss: 0.610702, acc: 64.84%] [G loss: 2.049586]\n",
      "epoch:12 step:11308 [D loss: 0.671756, acc: 59.38%] [G loss: 1.950106]\n",
      "epoch:12 step:11309 [D loss: 0.595274, acc: 67.19%] [G loss: 2.019238]\n",
      "epoch:12 step:11310 [D loss: 0.608536, acc: 67.97%] [G loss: 1.959272]\n",
      "epoch:12 step:11311 [D loss: 0.680927, acc: 57.81%] [G loss: 2.021471]\n",
      "epoch:12 step:11312 [D loss: 0.603015, acc: 64.06%] [G loss: 1.979800]\n",
      "epoch:12 step:11313 [D loss: 0.634933, acc: 61.72%] [G loss: 2.143629]\n",
      "epoch:12 step:11314 [D loss: 0.616671, acc: 67.19%] [G loss: 2.124728]\n",
      "epoch:12 step:11315 [D loss: 0.662088, acc: 60.16%] [G loss: 1.979231]\n",
      "epoch:12 step:11316 [D loss: 0.654319, acc: 62.50%] [G loss: 2.064699]\n",
      "epoch:12 step:11317 [D loss: 0.664775, acc: 62.50%] [G loss: 2.014541]\n",
      "epoch:12 step:11318 [D loss: 0.638974, acc: 60.16%] [G loss: 2.096425]\n",
      "epoch:12 step:11319 [D loss: 0.623388, acc: 67.97%] [G loss: 2.199268]\n",
      "epoch:12 step:11320 [D loss: 0.617073, acc: 62.50%] [G loss: 2.121110]\n",
      "epoch:12 step:11321 [D loss: 0.568357, acc: 75.78%] [G loss: 2.248796]\n",
      "epoch:12 step:11322 [D loss: 0.666232, acc: 60.94%] [G loss: 1.929616]\n",
      "epoch:12 step:11323 [D loss: 0.632505, acc: 60.16%] [G loss: 1.885149]\n",
      "epoch:12 step:11324 [D loss: 0.695316, acc: 50.00%] [G loss: 2.009390]\n",
      "epoch:12 step:11325 [D loss: 0.674517, acc: 56.25%] [G loss: 1.844478]\n",
      "epoch:12 step:11326 [D loss: 0.655115, acc: 64.84%] [G loss: 1.955345]\n",
      "epoch:12 step:11327 [D loss: 0.647417, acc: 63.28%] [G loss: 2.067564]\n",
      "epoch:12 step:11328 [D loss: 0.616127, acc: 67.97%] [G loss: 2.018049]\n",
      "epoch:12 step:11329 [D loss: 0.638278, acc: 67.19%] [G loss: 2.021023]\n",
      "epoch:12 step:11330 [D loss: 0.660743, acc: 58.59%] [G loss: 1.924136]\n",
      "epoch:12 step:11331 [D loss: 0.641049, acc: 57.03%] [G loss: 1.837570]\n",
      "epoch:12 step:11332 [D loss: 0.711486, acc: 53.91%] [G loss: 1.889311]\n",
      "epoch:12 step:11333 [D loss: 0.615657, acc: 65.62%] [G loss: 1.996375]\n",
      "epoch:12 step:11334 [D loss: 0.624281, acc: 71.09%] [G loss: 1.999992]\n",
      "epoch:12 step:11335 [D loss: 0.620498, acc: 67.97%] [G loss: 1.857750]\n",
      "epoch:12 step:11336 [D loss: 0.609243, acc: 68.75%] [G loss: 2.238075]\n",
      "epoch:12 step:11337 [D loss: 0.671668, acc: 58.59%] [G loss: 2.168246]\n",
      "epoch:12 step:11338 [D loss: 0.622133, acc: 64.84%] [G loss: 1.894415]\n",
      "epoch:12 step:11339 [D loss: 0.681149, acc: 61.72%] [G loss: 1.815660]\n",
      "epoch:12 step:11340 [D loss: 0.661474, acc: 62.50%] [G loss: 1.896193]\n",
      "epoch:12 step:11341 [D loss: 0.682122, acc: 49.22%] [G loss: 2.002522]\n",
      "epoch:12 step:11342 [D loss: 0.620016, acc: 67.97%] [G loss: 1.876367]\n",
      "epoch:12 step:11343 [D loss: 0.644024, acc: 64.84%] [G loss: 1.812518]\n",
      "epoch:12 step:11344 [D loss: 0.566643, acc: 72.66%] [G loss: 2.049203]\n",
      "epoch:12 step:11345 [D loss: 0.632851, acc: 64.84%] [G loss: 2.070795]\n",
      "epoch:12 step:11346 [D loss: 0.642951, acc: 64.84%] [G loss: 1.799907]\n",
      "epoch:12 step:11347 [D loss: 0.613947, acc: 65.62%] [G loss: 2.013654]\n",
      "epoch:12 step:11348 [D loss: 0.622027, acc: 63.28%] [G loss: 2.073884]\n",
      "epoch:12 step:11349 [D loss: 0.627277, acc: 67.19%] [G loss: 1.988130]\n",
      "epoch:12 step:11350 [D loss: 0.639111, acc: 64.84%] [G loss: 2.117661]\n",
      "epoch:12 step:11351 [D loss: 0.638141, acc: 67.97%] [G loss: 1.984239]\n",
      "epoch:12 step:11352 [D loss: 0.722161, acc: 55.47%] [G loss: 1.825298]\n",
      "epoch:12 step:11353 [D loss: 0.713397, acc: 60.16%] [G loss: 1.926644]\n",
      "epoch:12 step:11354 [D loss: 0.629889, acc: 63.28%] [G loss: 1.880285]\n",
      "epoch:12 step:11355 [D loss: 0.638077, acc: 62.50%] [G loss: 1.965020]\n",
      "epoch:12 step:11356 [D loss: 0.572112, acc: 71.09%] [G loss: 2.046809]\n",
      "epoch:12 step:11357 [D loss: 0.617618, acc: 62.50%] [G loss: 2.102713]\n",
      "epoch:12 step:11358 [D loss: 0.672470, acc: 59.38%] [G loss: 1.894803]\n",
      "epoch:12 step:11359 [D loss: 0.621806, acc: 67.19%] [G loss: 2.202915]\n",
      "epoch:12 step:11360 [D loss: 0.687605, acc: 50.00%] [G loss: 2.016406]\n",
      "epoch:12 step:11361 [D loss: 0.639499, acc: 67.19%] [G loss: 2.052317]\n",
      "epoch:12 step:11362 [D loss: 0.646999, acc: 61.72%] [G loss: 2.057065]\n",
      "epoch:12 step:11363 [D loss: 0.596411, acc: 63.28%] [G loss: 2.134374]\n",
      "epoch:12 step:11364 [D loss: 0.763582, acc: 53.12%] [G loss: 1.936575]\n",
      "epoch:12 step:11365 [D loss: 0.631977, acc: 65.62%] [G loss: 2.045240]\n",
      "epoch:12 step:11366 [D loss: 0.603747, acc: 67.19%] [G loss: 2.161105]\n",
      "epoch:12 step:11367 [D loss: 0.638375, acc: 65.62%] [G loss: 1.991246]\n",
      "epoch:12 step:11368 [D loss: 0.663694, acc: 64.06%] [G loss: 1.972681]\n",
      "epoch:12 step:11369 [D loss: 0.663852, acc: 57.03%] [G loss: 1.837825]\n",
      "epoch:12 step:11370 [D loss: 0.627807, acc: 65.62%] [G loss: 2.199110]\n",
      "epoch:12 step:11371 [D loss: 0.680213, acc: 61.72%] [G loss: 1.882967]\n",
      "epoch:12 step:11372 [D loss: 0.637371, acc: 65.62%] [G loss: 1.952527]\n",
      "epoch:12 step:11373 [D loss: 0.664072, acc: 57.03%] [G loss: 1.826328]\n",
      "epoch:12 step:11374 [D loss: 0.626959, acc: 63.28%] [G loss: 2.239697]\n",
      "epoch:12 step:11375 [D loss: 0.614816, acc: 71.88%] [G loss: 2.243529]\n",
      "epoch:12 step:11376 [D loss: 0.656372, acc: 60.16%] [G loss: 1.925132]\n",
      "epoch:12 step:11377 [D loss: 0.687386, acc: 56.25%] [G loss: 1.776793]\n",
      "epoch:12 step:11378 [D loss: 0.665691, acc: 60.94%] [G loss: 1.893847]\n",
      "epoch:12 step:11379 [D loss: 0.628075, acc: 68.75%] [G loss: 1.794699]\n",
      "epoch:12 step:11380 [D loss: 0.663408, acc: 57.03%] [G loss: 1.799480]\n",
      "epoch:12 step:11381 [D loss: 0.684394, acc: 64.84%] [G loss: 1.925621]\n",
      "epoch:12 step:11382 [D loss: 0.658216, acc: 58.59%] [G loss: 1.787017]\n",
      "epoch:12 step:11383 [D loss: 0.624639, acc: 65.62%] [G loss: 1.963658]\n",
      "epoch:12 step:11384 [D loss: 0.597681, acc: 64.84%] [G loss: 1.884284]\n",
      "epoch:12 step:11385 [D loss: 0.600063, acc: 67.97%] [G loss: 1.966779]\n",
      "epoch:12 step:11386 [D loss: 0.631832, acc: 63.28%] [G loss: 1.954557]\n",
      "epoch:12 step:11387 [D loss: 0.620320, acc: 63.28%] [G loss: 1.929854]\n",
      "epoch:12 step:11388 [D loss: 0.634564, acc: 63.28%] [G loss: 1.998311]\n",
      "epoch:12 step:11389 [D loss: 0.655426, acc: 64.84%] [G loss: 1.976208]\n",
      "epoch:12 step:11390 [D loss: 0.651590, acc: 60.94%] [G loss: 1.986492]\n",
      "epoch:12 step:11391 [D loss: 0.622977, acc: 64.06%] [G loss: 1.984926]\n",
      "epoch:12 step:11392 [D loss: 0.653587, acc: 60.16%] [G loss: 1.919499]\n",
      "epoch:12 step:11393 [D loss: 0.590992, acc: 73.44%] [G loss: 2.113935]\n",
      "epoch:12 step:11394 [D loss: 0.615406, acc: 61.72%] [G loss: 2.102457]\n",
      "epoch:12 step:11395 [D loss: 0.554501, acc: 71.09%] [G loss: 2.498848]\n",
      "epoch:12 step:11396 [D loss: 0.661732, acc: 57.81%] [G loss: 2.119398]\n",
      "epoch:12 step:11397 [D loss: 0.650899, acc: 64.84%] [G loss: 1.844254]\n",
      "epoch:12 step:11398 [D loss: 0.639158, acc: 62.50%] [G loss: 2.259905]\n",
      "epoch:12 step:11399 [D loss: 0.600885, acc: 71.09%] [G loss: 2.018010]\n",
      "epoch:12 step:11400 [D loss: 0.601930, acc: 68.75%] [G loss: 2.235483]\n",
      "epoch:12 step:11401 [D loss: 0.684782, acc: 57.81%] [G loss: 1.786320]\n",
      "epoch:12 step:11402 [D loss: 0.625621, acc: 62.50%] [G loss: 1.993654]\n",
      "epoch:12 step:11403 [D loss: 0.627498, acc: 61.72%] [G loss: 1.896580]\n",
      "epoch:12 step:11404 [D loss: 0.661291, acc: 61.72%] [G loss: 1.812570]\n",
      "epoch:12 step:11405 [D loss: 0.649488, acc: 61.72%] [G loss: 1.845999]\n",
      "epoch:12 step:11406 [D loss: 0.622061, acc: 64.06%] [G loss: 1.887630]\n",
      "epoch:12 step:11407 [D loss: 0.635285, acc: 57.03%] [G loss: 2.024605]\n",
      "epoch:12 step:11408 [D loss: 0.671717, acc: 64.06%] [G loss: 1.910086]\n",
      "epoch:12 step:11409 [D loss: 0.624818, acc: 66.41%] [G loss: 1.997283]\n",
      "epoch:12 step:11410 [D loss: 0.624321, acc: 67.19%] [G loss: 2.104321]\n",
      "epoch:12 step:11411 [D loss: 0.606101, acc: 64.06%] [G loss: 2.046661]\n",
      "epoch:12 step:11412 [D loss: 0.711815, acc: 56.25%] [G loss: 2.067966]\n",
      "epoch:12 step:11413 [D loss: 0.664683, acc: 60.16%] [G loss: 1.948766]\n",
      "epoch:12 step:11414 [D loss: 0.644219, acc: 60.94%] [G loss: 1.945791]\n",
      "epoch:12 step:11415 [D loss: 0.627977, acc: 65.62%] [G loss: 1.998439]\n",
      "epoch:12 step:11416 [D loss: 0.593945, acc: 71.88%] [G loss: 1.940473]\n",
      "epoch:12 step:11417 [D loss: 0.635444, acc: 64.06%] [G loss: 1.876365]\n",
      "epoch:12 step:11418 [D loss: 0.650817, acc: 62.50%] [G loss: 1.960154]\n",
      "epoch:12 step:11419 [D loss: 0.653521, acc: 64.06%] [G loss: 1.893952]\n",
      "epoch:12 step:11420 [D loss: 0.659532, acc: 57.81%] [G loss: 2.031387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11421 [D loss: 0.632062, acc: 66.41%] [G loss: 1.971965]\n",
      "epoch:12 step:11422 [D loss: 0.660625, acc: 58.59%] [G loss: 1.997889]\n",
      "epoch:12 step:11423 [D loss: 0.625455, acc: 64.84%] [G loss: 1.950847]\n",
      "epoch:12 step:11424 [D loss: 0.625473, acc: 67.19%] [G loss: 1.980769]\n",
      "epoch:12 step:11425 [D loss: 0.679474, acc: 60.16%] [G loss: 1.957888]\n",
      "epoch:12 step:11426 [D loss: 0.715316, acc: 57.03%] [G loss: 1.935960]\n",
      "epoch:12 step:11427 [D loss: 0.644265, acc: 66.41%] [G loss: 1.982692]\n",
      "epoch:12 step:11428 [D loss: 0.691256, acc: 57.81%] [G loss: 1.894309]\n",
      "epoch:12 step:11429 [D loss: 0.653294, acc: 60.94%] [G loss: 1.923085]\n",
      "epoch:12 step:11430 [D loss: 0.656545, acc: 60.16%] [G loss: 1.854512]\n",
      "epoch:12 step:11431 [D loss: 0.688431, acc: 52.34%] [G loss: 2.062571]\n",
      "epoch:12 step:11432 [D loss: 0.683533, acc: 55.47%] [G loss: 1.899235]\n",
      "epoch:12 step:11433 [D loss: 0.655007, acc: 60.94%] [G loss: 1.827751]\n",
      "epoch:12 step:11434 [D loss: 0.640209, acc: 65.62%] [G loss: 2.034530]\n",
      "epoch:12 step:11435 [D loss: 0.609605, acc: 64.06%] [G loss: 2.090198]\n",
      "epoch:12 step:11436 [D loss: 0.597626, acc: 71.88%] [G loss: 2.014510]\n",
      "epoch:12 step:11437 [D loss: 0.631238, acc: 63.28%] [G loss: 1.975359]\n",
      "epoch:12 step:11438 [D loss: 0.601382, acc: 68.75%] [G loss: 2.144379]\n",
      "epoch:12 step:11439 [D loss: 0.615493, acc: 64.06%] [G loss: 2.070251]\n",
      "epoch:12 step:11440 [D loss: 0.642209, acc: 68.75%] [G loss: 1.994156]\n",
      "epoch:12 step:11441 [D loss: 0.599245, acc: 66.41%] [G loss: 2.169585]\n",
      "epoch:12 step:11442 [D loss: 0.615472, acc: 67.97%] [G loss: 2.212051]\n",
      "epoch:12 step:11443 [D loss: 0.644947, acc: 67.97%] [G loss: 2.159728]\n",
      "epoch:12 step:11444 [D loss: 0.646702, acc: 63.28%] [G loss: 2.027759]\n",
      "epoch:12 step:11445 [D loss: 0.641421, acc: 60.94%] [G loss: 2.027253]\n",
      "epoch:12 step:11446 [D loss: 0.684462, acc: 59.38%] [G loss: 1.916749]\n",
      "epoch:12 step:11447 [D loss: 0.670348, acc: 58.59%] [G loss: 1.943017]\n",
      "epoch:12 step:11448 [D loss: 0.605367, acc: 66.41%] [G loss: 2.057034]\n",
      "epoch:12 step:11449 [D loss: 0.653140, acc: 58.59%] [G loss: 2.030447]\n",
      "epoch:12 step:11450 [D loss: 0.637176, acc: 64.84%] [G loss: 2.061003]\n",
      "epoch:12 step:11451 [D loss: 0.592597, acc: 68.75%] [G loss: 2.218724]\n",
      "epoch:12 step:11452 [D loss: 0.647954, acc: 59.38%] [G loss: 2.478557]\n",
      "epoch:12 step:11453 [D loss: 0.596805, acc: 66.41%] [G loss: 2.356278]\n",
      "epoch:12 step:11454 [D loss: 0.647306, acc: 64.06%] [G loss: 1.911753]\n",
      "epoch:12 step:11455 [D loss: 0.722325, acc: 56.25%] [G loss: 1.937575]\n",
      "epoch:12 step:11456 [D loss: 0.705230, acc: 55.47%] [G loss: 1.926837]\n",
      "epoch:12 step:11457 [D loss: 0.705729, acc: 55.47%] [G loss: 1.855350]\n",
      "epoch:12 step:11458 [D loss: 0.692911, acc: 61.72%] [G loss: 2.056018]\n",
      "epoch:12 step:11459 [D loss: 0.705979, acc: 53.91%] [G loss: 1.981257]\n",
      "epoch:12 step:11460 [D loss: 0.643921, acc: 60.94%] [G loss: 1.863213]\n",
      "epoch:12 step:11461 [D loss: 0.665089, acc: 62.50%] [G loss: 2.053911]\n",
      "epoch:12 step:11462 [D loss: 0.606341, acc: 65.62%] [G loss: 2.196490]\n",
      "epoch:12 step:11463 [D loss: 0.571734, acc: 70.31%] [G loss: 2.218367]\n",
      "epoch:12 step:11464 [D loss: 0.718743, acc: 60.94%] [G loss: 1.780875]\n",
      "epoch:12 step:11465 [D loss: 0.618412, acc: 62.50%] [G loss: 2.157470]\n",
      "epoch:12 step:11466 [D loss: 0.647604, acc: 67.19%] [G loss: 1.979191]\n",
      "epoch:12 step:11467 [D loss: 0.593940, acc: 70.31%] [G loss: 2.092902]\n",
      "epoch:12 step:11468 [D loss: 0.659827, acc: 55.47%] [G loss: 1.883284]\n",
      "epoch:12 step:11469 [D loss: 0.706836, acc: 60.94%] [G loss: 1.871960]\n",
      "epoch:12 step:11470 [D loss: 0.654001, acc: 59.38%] [G loss: 1.851609]\n",
      "epoch:12 step:11471 [D loss: 0.609405, acc: 65.62%] [G loss: 1.914029]\n",
      "epoch:12 step:11472 [D loss: 0.694652, acc: 57.81%] [G loss: 1.850991]\n",
      "epoch:12 step:11473 [D loss: 0.594954, acc: 67.19%] [G loss: 2.152818]\n",
      "epoch:12 step:11474 [D loss: 0.514800, acc: 77.34%] [G loss: 2.284484]\n",
      "epoch:12 step:11475 [D loss: 0.610968, acc: 63.28%] [G loss: 2.375367]\n",
      "epoch:12 step:11476 [D loss: 0.544278, acc: 71.09%] [G loss: 2.539104]\n",
      "epoch:12 step:11477 [D loss: 0.652101, acc: 64.84%] [G loss: 1.968390]\n",
      "epoch:12 step:11478 [D loss: 0.682590, acc: 62.50%] [G loss: 2.144279]\n",
      "epoch:12 step:11479 [D loss: 0.640691, acc: 60.16%] [G loss: 1.960402]\n",
      "epoch:12 step:11480 [D loss: 0.626011, acc: 64.84%] [G loss: 2.093706]\n",
      "epoch:12 step:11481 [D loss: 0.659875, acc: 59.38%] [G loss: 2.016320]\n",
      "epoch:12 step:11482 [D loss: 0.628082, acc: 61.72%] [G loss: 2.014400]\n",
      "epoch:12 step:11483 [D loss: 0.637145, acc: 59.38%] [G loss: 2.114721]\n",
      "epoch:12 step:11484 [D loss: 0.636265, acc: 68.75%] [G loss: 1.907471]\n",
      "epoch:12 step:11485 [D loss: 0.616124, acc: 67.19%] [G loss: 2.032496]\n",
      "epoch:12 step:11486 [D loss: 0.585920, acc: 70.31%] [G loss: 2.020745]\n",
      "epoch:12 step:11487 [D loss: 0.637209, acc: 66.41%] [G loss: 2.127341]\n",
      "epoch:12 step:11488 [D loss: 0.597320, acc: 71.09%] [G loss: 2.095274]\n",
      "epoch:12 step:11489 [D loss: 0.644175, acc: 67.97%] [G loss: 2.048412]\n",
      "epoch:12 step:11490 [D loss: 0.669147, acc: 57.81%] [G loss: 1.983132]\n",
      "epoch:12 step:11491 [D loss: 0.722522, acc: 51.56%] [G loss: 1.869106]\n",
      "epoch:12 step:11492 [D loss: 0.637853, acc: 57.81%] [G loss: 2.061488]\n",
      "epoch:12 step:11493 [D loss: 0.627100, acc: 67.97%] [G loss: 1.906295]\n",
      "epoch:12 step:11494 [D loss: 0.717885, acc: 51.56%] [G loss: 1.787177]\n",
      "epoch:12 step:11495 [D loss: 0.652815, acc: 61.72%] [G loss: 1.843474]\n",
      "epoch:12 step:11496 [D loss: 0.650867, acc: 61.72%] [G loss: 1.833384]\n",
      "epoch:12 step:11497 [D loss: 0.649517, acc: 64.06%] [G loss: 1.970537]\n",
      "epoch:12 step:11498 [D loss: 0.636682, acc: 60.94%] [G loss: 1.999865]\n",
      "epoch:12 step:11499 [D loss: 0.653628, acc: 62.50%] [G loss: 1.904311]\n",
      "epoch:12 step:11500 [D loss: 0.663664, acc: 64.84%] [G loss: 1.753775]\n",
      "epoch:12 step:11501 [D loss: 0.625012, acc: 64.84%] [G loss: 1.859297]\n",
      "epoch:12 step:11502 [D loss: 0.640381, acc: 68.75%] [G loss: 1.961608]\n",
      "epoch:12 step:11503 [D loss: 0.665352, acc: 56.25%] [G loss: 1.954961]\n",
      "epoch:12 step:11504 [D loss: 0.604067, acc: 64.06%] [G loss: 1.993037]\n",
      "epoch:12 step:11505 [D loss: 0.674075, acc: 59.38%] [G loss: 2.011589]\n",
      "epoch:12 step:11506 [D loss: 0.596740, acc: 67.19%] [G loss: 2.077367]\n",
      "epoch:12 step:11507 [D loss: 0.627330, acc: 67.19%] [G loss: 1.948665]\n",
      "epoch:12 step:11508 [D loss: 0.600102, acc: 68.75%] [G loss: 2.345803]\n",
      "epoch:12 step:11509 [D loss: 0.670354, acc: 57.03%] [G loss: 1.889447]\n",
      "epoch:12 step:11510 [D loss: 0.611676, acc: 67.97%] [G loss: 1.934783]\n",
      "epoch:12 step:11511 [D loss: 0.668442, acc: 62.50%] [G loss: 1.894910]\n",
      "epoch:12 step:11512 [D loss: 0.657905, acc: 60.16%] [G loss: 1.963462]\n",
      "epoch:12 step:11513 [D loss: 0.679140, acc: 56.25%] [G loss: 1.965626]\n",
      "epoch:12 step:11514 [D loss: 0.652793, acc: 61.72%] [G loss: 2.042398]\n",
      "epoch:12 step:11515 [D loss: 0.623104, acc: 64.84%] [G loss: 2.016335]\n",
      "epoch:12 step:11516 [D loss: 0.682583, acc: 59.38%] [G loss: 1.962037]\n",
      "epoch:12 step:11517 [D loss: 0.603027, acc: 63.28%] [G loss: 1.848537]\n",
      "epoch:12 step:11518 [D loss: 0.602124, acc: 64.84%] [G loss: 2.016718]\n",
      "epoch:12 step:11519 [D loss: 0.590988, acc: 67.19%] [G loss: 2.284838]\n",
      "epoch:12 step:11520 [D loss: 0.572154, acc: 70.31%] [G loss: 2.049652]\n",
      "epoch:12 step:11521 [D loss: 0.656838, acc: 61.72%] [G loss: 1.803468]\n",
      "epoch:12 step:11522 [D loss: 0.633577, acc: 70.31%] [G loss: 1.993146]\n",
      "epoch:12 step:11523 [D loss: 0.613807, acc: 64.84%] [G loss: 1.855793]\n",
      "epoch:12 step:11524 [D loss: 0.592144, acc: 67.19%] [G loss: 2.164731]\n",
      "epoch:12 step:11525 [D loss: 0.696175, acc: 60.16%] [G loss: 2.122434]\n",
      "epoch:12 step:11526 [D loss: 0.626273, acc: 67.97%] [G loss: 1.904090]\n",
      "epoch:12 step:11527 [D loss: 0.602661, acc: 64.06%] [G loss: 2.075591]\n",
      "epoch:12 step:11528 [D loss: 0.696164, acc: 64.06%] [G loss: 2.003782]\n",
      "epoch:12 step:11529 [D loss: 0.648778, acc: 62.50%] [G loss: 1.987909]\n",
      "epoch:12 step:11530 [D loss: 0.658317, acc: 62.50%] [G loss: 1.955525]\n",
      "epoch:12 step:11531 [D loss: 0.628066, acc: 64.06%] [G loss: 2.068493]\n",
      "epoch:12 step:11532 [D loss: 0.678116, acc: 58.59%] [G loss: 1.873963]\n",
      "epoch:12 step:11533 [D loss: 0.657596, acc: 61.72%] [G loss: 1.955123]\n",
      "epoch:12 step:11534 [D loss: 0.701523, acc: 57.03%] [G loss: 1.870621]\n",
      "epoch:12 step:11535 [D loss: 0.625354, acc: 71.88%] [G loss: 1.913480]\n",
      "epoch:12 step:11536 [D loss: 0.622550, acc: 61.72%] [G loss: 1.917379]\n",
      "epoch:12 step:11537 [D loss: 0.596498, acc: 69.53%] [G loss: 2.055301]\n",
      "epoch:12 step:11538 [D loss: 0.683164, acc: 60.94%] [G loss: 1.966259]\n",
      "epoch:12 step:11539 [D loss: 0.661048, acc: 57.81%] [G loss: 1.917487]\n",
      "epoch:12 step:11540 [D loss: 0.563448, acc: 68.75%] [G loss: 2.127303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11541 [D loss: 0.641351, acc: 60.16%] [G loss: 1.953429]\n",
      "epoch:12 step:11542 [D loss: 0.598154, acc: 69.53%] [G loss: 2.130026]\n",
      "epoch:12 step:11543 [D loss: 0.664978, acc: 64.06%] [G loss: 2.098838]\n",
      "epoch:12 step:11544 [D loss: 0.586790, acc: 75.00%] [G loss: 2.082166]\n",
      "epoch:12 step:11545 [D loss: 0.673153, acc: 60.94%] [G loss: 1.886254]\n",
      "epoch:12 step:11546 [D loss: 0.599127, acc: 68.75%] [G loss: 1.892491]\n",
      "epoch:12 step:11547 [D loss: 0.659194, acc: 61.72%] [G loss: 1.932142]\n",
      "epoch:12 step:11548 [D loss: 0.631427, acc: 63.28%] [G loss: 1.866890]\n",
      "epoch:12 step:11549 [D loss: 0.585268, acc: 66.41%] [G loss: 1.928342]\n",
      "epoch:12 step:11550 [D loss: 0.606312, acc: 67.97%] [G loss: 1.978422]\n",
      "epoch:12 step:11551 [D loss: 0.691336, acc: 55.47%] [G loss: 1.978425]\n",
      "epoch:12 step:11552 [D loss: 0.661799, acc: 61.72%] [G loss: 1.951319]\n",
      "epoch:12 step:11553 [D loss: 0.603361, acc: 64.06%] [G loss: 2.169462]\n",
      "epoch:12 step:11554 [D loss: 0.671194, acc: 60.94%] [G loss: 1.885609]\n",
      "epoch:12 step:11555 [D loss: 0.581648, acc: 67.97%] [G loss: 2.074296]\n",
      "epoch:12 step:11556 [D loss: 0.567364, acc: 67.97%] [G loss: 2.292326]\n",
      "epoch:12 step:11557 [D loss: 0.594874, acc: 67.97%] [G loss: 2.159894]\n",
      "epoch:12 step:11558 [D loss: 0.596604, acc: 75.78%] [G loss: 2.214827]\n",
      "epoch:12 step:11559 [D loss: 0.526856, acc: 78.91%] [G loss: 2.482177]\n",
      "epoch:12 step:11560 [D loss: 0.691133, acc: 57.81%] [G loss: 1.805762]\n",
      "epoch:12 step:11561 [D loss: 0.711468, acc: 57.03%] [G loss: 1.877558]\n",
      "epoch:12 step:11562 [D loss: 0.615294, acc: 67.97%] [G loss: 1.960505]\n",
      "epoch:12 step:11563 [D loss: 0.684900, acc: 56.25%] [G loss: 1.828346]\n",
      "epoch:12 step:11564 [D loss: 0.667389, acc: 57.81%] [G loss: 1.974726]\n",
      "epoch:12 step:11565 [D loss: 0.636974, acc: 65.62%] [G loss: 2.220431]\n",
      "epoch:12 step:11566 [D loss: 0.581838, acc: 66.41%] [G loss: 1.937396]\n",
      "epoch:12 step:11567 [D loss: 0.677770, acc: 62.50%] [G loss: 1.868777]\n",
      "epoch:12 step:11568 [D loss: 0.626122, acc: 61.72%] [G loss: 1.917744]\n",
      "epoch:12 step:11569 [D loss: 0.589665, acc: 68.75%] [G loss: 2.045692]\n",
      "epoch:12 step:11570 [D loss: 0.626061, acc: 64.84%] [G loss: 2.068335]\n",
      "epoch:12 step:11571 [D loss: 0.613841, acc: 67.19%] [G loss: 2.026384]\n",
      "epoch:12 step:11572 [D loss: 0.704796, acc: 60.16%] [G loss: 1.980425]\n",
      "epoch:12 step:11573 [D loss: 0.678858, acc: 64.06%] [G loss: 1.950052]\n",
      "epoch:12 step:11574 [D loss: 0.608333, acc: 67.19%] [G loss: 2.113307]\n",
      "epoch:12 step:11575 [D loss: 0.598130, acc: 75.00%] [G loss: 2.022617]\n",
      "epoch:12 step:11576 [D loss: 0.638467, acc: 65.62%] [G loss: 2.008610]\n",
      "epoch:12 step:11577 [D loss: 0.639925, acc: 62.50%] [G loss: 1.852714]\n",
      "epoch:12 step:11578 [D loss: 0.639329, acc: 60.94%] [G loss: 1.957946]\n",
      "epoch:12 step:11579 [D loss: 0.615203, acc: 58.59%] [G loss: 2.107372]\n",
      "epoch:12 step:11580 [D loss: 0.655820, acc: 63.28%] [G loss: 2.128851]\n",
      "epoch:12 step:11581 [D loss: 0.639041, acc: 64.06%] [G loss: 2.184148]\n",
      "epoch:12 step:11582 [D loss: 0.675185, acc: 64.06%] [G loss: 2.049887]\n",
      "epoch:12 step:11583 [D loss: 0.623486, acc: 60.94%] [G loss: 1.942249]\n",
      "epoch:12 step:11584 [D loss: 0.627205, acc: 57.81%] [G loss: 2.176908]\n",
      "epoch:12 step:11585 [D loss: 0.655416, acc: 60.16%] [G loss: 1.977094]\n",
      "epoch:12 step:11586 [D loss: 0.682581, acc: 57.03%] [G loss: 1.968370]\n",
      "epoch:12 step:11587 [D loss: 0.640473, acc: 61.72%] [G loss: 2.068256]\n",
      "epoch:12 step:11588 [D loss: 0.643734, acc: 57.81%] [G loss: 2.128582]\n",
      "epoch:12 step:11589 [D loss: 0.606011, acc: 61.72%] [G loss: 2.290001]\n",
      "epoch:12 step:11590 [D loss: 0.570121, acc: 69.53%] [G loss: 2.369486]\n",
      "epoch:12 step:11591 [D loss: 0.592331, acc: 67.97%] [G loss: 2.393162]\n",
      "epoch:12 step:11592 [D loss: 0.713787, acc: 57.81%] [G loss: 1.970515]\n",
      "epoch:12 step:11593 [D loss: 0.739950, acc: 53.91%] [G loss: 1.788268]\n",
      "epoch:12 step:11594 [D loss: 0.621590, acc: 68.75%] [G loss: 2.082696]\n",
      "epoch:12 step:11595 [D loss: 0.658354, acc: 62.50%] [G loss: 1.937931]\n",
      "epoch:12 step:11596 [D loss: 0.695435, acc: 59.38%] [G loss: 1.828852]\n",
      "epoch:12 step:11597 [D loss: 0.635273, acc: 64.06%] [G loss: 2.031945]\n",
      "epoch:12 step:11598 [D loss: 0.579878, acc: 69.53%] [G loss: 2.066546]\n",
      "epoch:12 step:11599 [D loss: 0.693740, acc: 57.81%] [G loss: 1.842790]\n",
      "epoch:12 step:11600 [D loss: 0.641820, acc: 63.28%] [G loss: 1.888150]\n",
      "epoch:12 step:11601 [D loss: 0.614536, acc: 62.50%] [G loss: 1.922323]\n",
      "epoch:12 step:11602 [D loss: 0.573985, acc: 75.78%] [G loss: 2.113621]\n",
      "epoch:12 step:11603 [D loss: 0.586932, acc: 69.53%] [G loss: 2.017347]\n",
      "epoch:12 step:11604 [D loss: 0.686683, acc: 59.38%] [G loss: 2.053671]\n",
      "epoch:12 step:11605 [D loss: 0.630095, acc: 65.62%] [G loss: 1.942960]\n",
      "epoch:12 step:11606 [D loss: 0.667534, acc: 60.94%] [G loss: 1.852648]\n",
      "epoch:12 step:11607 [D loss: 0.617922, acc: 61.72%] [G loss: 2.027452]\n",
      "epoch:12 step:11608 [D loss: 0.531387, acc: 78.91%] [G loss: 2.098233]\n",
      "epoch:12 step:11609 [D loss: 0.655592, acc: 60.94%] [G loss: 1.828111]\n",
      "epoch:12 step:11610 [D loss: 0.588150, acc: 69.53%] [G loss: 1.980524]\n",
      "epoch:12 step:11611 [D loss: 0.647765, acc: 59.38%] [G loss: 1.911182]\n",
      "epoch:12 step:11612 [D loss: 0.638617, acc: 66.41%] [G loss: 1.908465]\n",
      "epoch:12 step:11613 [D loss: 0.678489, acc: 62.50%] [G loss: 1.885722]\n",
      "epoch:12 step:11614 [D loss: 0.649806, acc: 60.94%] [G loss: 2.031420]\n",
      "epoch:12 step:11615 [D loss: 0.608058, acc: 64.84%] [G loss: 2.189380]\n",
      "epoch:12 step:11616 [D loss: 0.650027, acc: 61.72%] [G loss: 1.878154]\n",
      "epoch:12 step:11617 [D loss: 0.656817, acc: 64.06%] [G loss: 1.897238]\n",
      "epoch:12 step:11618 [D loss: 0.637890, acc: 64.84%] [G loss: 2.057208]\n",
      "epoch:12 step:11619 [D loss: 0.639952, acc: 63.28%] [G loss: 1.852338]\n",
      "epoch:12 step:11620 [D loss: 0.641841, acc: 64.06%] [G loss: 1.753182]\n",
      "epoch:12 step:11621 [D loss: 0.727811, acc: 48.44%] [G loss: 1.765313]\n",
      "epoch:12 step:11622 [D loss: 0.639282, acc: 60.16%] [G loss: 1.767070]\n",
      "epoch:12 step:11623 [D loss: 0.675581, acc: 60.16%] [G loss: 1.904359]\n",
      "epoch:12 step:11624 [D loss: 0.638689, acc: 63.28%] [G loss: 1.880536]\n",
      "epoch:12 step:11625 [D loss: 0.619089, acc: 67.19%] [G loss: 2.105805]\n",
      "epoch:12 step:11626 [D loss: 0.620734, acc: 65.62%] [G loss: 1.857763]\n",
      "epoch:12 step:11627 [D loss: 0.582237, acc: 73.44%] [G loss: 2.030602]\n",
      "epoch:12 step:11628 [D loss: 0.669737, acc: 60.94%] [G loss: 1.920800]\n",
      "epoch:12 step:11629 [D loss: 0.635274, acc: 65.62%] [G loss: 2.048125]\n",
      "epoch:12 step:11630 [D loss: 0.630054, acc: 61.72%] [G loss: 1.955369]\n",
      "epoch:12 step:11631 [D loss: 0.693312, acc: 52.34%] [G loss: 1.856913]\n",
      "epoch:12 step:11632 [D loss: 0.640303, acc: 61.72%] [G loss: 1.948566]\n",
      "epoch:12 step:11633 [D loss: 0.676109, acc: 60.94%] [G loss: 1.839650]\n",
      "epoch:12 step:11634 [D loss: 0.630147, acc: 60.94%] [G loss: 2.002404]\n",
      "epoch:12 step:11635 [D loss: 0.739603, acc: 58.59%] [G loss: 1.845122]\n",
      "epoch:12 step:11636 [D loss: 0.701146, acc: 58.59%] [G loss: 1.936412]\n",
      "epoch:12 step:11637 [D loss: 0.624166, acc: 65.62%] [G loss: 1.825861]\n",
      "epoch:12 step:11638 [D loss: 0.635848, acc: 68.75%] [G loss: 1.891441]\n",
      "epoch:12 step:11639 [D loss: 0.624711, acc: 64.06%] [G loss: 1.984478]\n",
      "epoch:12 step:11640 [D loss: 0.676354, acc: 56.25%] [G loss: 1.948099]\n",
      "epoch:12 step:11641 [D loss: 0.638121, acc: 64.06%] [G loss: 1.962367]\n",
      "epoch:12 step:11642 [D loss: 0.654930, acc: 62.50%] [G loss: 1.930576]\n",
      "epoch:12 step:11643 [D loss: 0.640578, acc: 64.06%] [G loss: 2.036166]\n",
      "epoch:12 step:11644 [D loss: 0.655411, acc: 61.72%] [G loss: 1.858552]\n",
      "epoch:12 step:11645 [D loss: 0.578088, acc: 74.22%] [G loss: 2.062034]\n",
      "epoch:12 step:11646 [D loss: 0.611107, acc: 69.53%] [G loss: 2.115984]\n",
      "epoch:12 step:11647 [D loss: 0.667575, acc: 58.59%] [G loss: 1.958238]\n",
      "epoch:12 step:11648 [D loss: 0.635753, acc: 61.72%] [G loss: 2.051614]\n",
      "epoch:12 step:11649 [D loss: 0.642573, acc: 62.50%] [G loss: 2.124413]\n",
      "epoch:12 step:11650 [D loss: 0.607048, acc: 67.19%] [G loss: 2.105543]\n",
      "epoch:12 step:11651 [D loss: 0.635130, acc: 62.50%] [G loss: 1.919702]\n",
      "epoch:12 step:11652 [D loss: 0.725294, acc: 61.72%] [G loss: 1.914837]\n",
      "epoch:12 step:11653 [D loss: 0.635362, acc: 71.09%] [G loss: 2.082125]\n",
      "epoch:12 step:11654 [D loss: 0.587631, acc: 67.19%] [G loss: 1.750818]\n",
      "epoch:12 step:11655 [D loss: 0.659631, acc: 53.12%] [G loss: 1.898137]\n",
      "epoch:12 step:11656 [D loss: 0.580443, acc: 72.66%] [G loss: 1.943093]\n",
      "epoch:12 step:11657 [D loss: 0.610381, acc: 72.66%] [G loss: 1.978103]\n",
      "epoch:12 step:11658 [D loss: 0.647483, acc: 62.50%] [G loss: 2.046243]\n",
      "epoch:12 step:11659 [D loss: 0.639690, acc: 64.06%] [G loss: 2.097589]\n",
      "epoch:12 step:11660 [D loss: 0.603351, acc: 65.62%] [G loss: 2.065401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11661 [D loss: 0.640338, acc: 64.84%] [G loss: 2.134782]\n",
      "epoch:12 step:11662 [D loss: 0.681409, acc: 58.59%] [G loss: 1.963364]\n",
      "epoch:12 step:11663 [D loss: 0.714199, acc: 59.38%] [G loss: 2.096354]\n",
      "epoch:12 step:11664 [D loss: 0.597135, acc: 64.06%] [G loss: 2.101912]\n",
      "epoch:12 step:11665 [D loss: 0.679160, acc: 55.47%] [G loss: 1.935104]\n",
      "epoch:12 step:11666 [D loss: 0.612596, acc: 60.94%] [G loss: 1.940517]\n",
      "epoch:12 step:11667 [D loss: 0.665685, acc: 60.16%] [G loss: 1.844472]\n",
      "epoch:12 step:11668 [D loss: 0.645994, acc: 65.62%] [G loss: 1.985811]\n",
      "epoch:12 step:11669 [D loss: 0.703110, acc: 54.69%] [G loss: 1.955946]\n",
      "epoch:12 step:11670 [D loss: 0.677823, acc: 57.81%] [G loss: 1.932577]\n",
      "epoch:12 step:11671 [D loss: 0.662326, acc: 60.16%] [G loss: 2.022363]\n",
      "epoch:12 step:11672 [D loss: 0.545358, acc: 76.56%] [G loss: 2.126910]\n",
      "epoch:12 step:11673 [D loss: 0.626442, acc: 68.75%] [G loss: 2.303737]\n",
      "epoch:12 step:11674 [D loss: 0.627930, acc: 61.72%] [G loss: 2.143318]\n",
      "epoch:12 step:11675 [D loss: 0.628935, acc: 64.06%] [G loss: 2.010922]\n",
      "epoch:12 step:11676 [D loss: 0.611714, acc: 67.19%] [G loss: 1.870777]\n",
      "epoch:12 step:11677 [D loss: 0.617826, acc: 67.97%] [G loss: 1.959157]\n",
      "epoch:12 step:11678 [D loss: 0.640293, acc: 66.41%] [G loss: 2.039942]\n",
      "epoch:12 step:11679 [D loss: 0.650146, acc: 64.06%] [G loss: 1.909565]\n",
      "epoch:12 step:11680 [D loss: 0.667516, acc: 62.50%] [G loss: 1.971074]\n",
      "epoch:12 step:11681 [D loss: 0.683210, acc: 58.59%] [G loss: 1.819666]\n",
      "epoch:12 step:11682 [D loss: 0.695262, acc: 50.78%] [G loss: 1.837503]\n",
      "epoch:12 step:11683 [D loss: 0.646712, acc: 56.25%] [G loss: 1.795844]\n",
      "epoch:12 step:11684 [D loss: 0.640349, acc: 62.50%] [G loss: 1.796373]\n",
      "epoch:12 step:11685 [D loss: 0.700750, acc: 57.81%] [G loss: 1.906875]\n",
      "epoch:12 step:11686 [D loss: 0.681689, acc: 60.94%] [G loss: 1.936335]\n",
      "epoch:12 step:11687 [D loss: 0.619054, acc: 61.72%] [G loss: 1.851713]\n",
      "epoch:12 step:11688 [D loss: 0.638653, acc: 67.97%] [G loss: 1.742229]\n",
      "epoch:12 step:11689 [D loss: 0.650312, acc: 69.53%] [G loss: 1.975274]\n",
      "epoch:12 step:11690 [D loss: 0.613964, acc: 64.06%] [G loss: 1.940768]\n",
      "epoch:12 step:11691 [D loss: 0.606968, acc: 64.84%] [G loss: 1.976662]\n",
      "epoch:12 step:11692 [D loss: 0.628859, acc: 66.41%] [G loss: 1.860639]\n",
      "epoch:12 step:11693 [D loss: 0.597459, acc: 61.72%] [G loss: 1.950450]\n",
      "epoch:12 step:11694 [D loss: 0.614666, acc: 65.62%] [G loss: 1.974496]\n",
      "epoch:12 step:11695 [D loss: 0.593044, acc: 71.09%] [G loss: 2.051331]\n",
      "epoch:12 step:11696 [D loss: 0.686048, acc: 55.47%] [G loss: 1.819667]\n",
      "epoch:12 step:11697 [D loss: 0.578697, acc: 74.22%] [G loss: 2.104236]\n",
      "epoch:12 step:11698 [D loss: 0.612120, acc: 67.97%] [G loss: 1.972960]\n",
      "epoch:12 step:11699 [D loss: 0.625982, acc: 67.97%] [G loss: 1.978960]\n",
      "epoch:12 step:11700 [D loss: 0.625723, acc: 63.28%] [G loss: 2.022914]\n",
      "epoch:12 step:11701 [D loss: 0.648921, acc: 64.84%] [G loss: 2.078043]\n",
      "epoch:12 step:11702 [D loss: 0.652826, acc: 61.72%] [G loss: 1.915013]\n",
      "epoch:12 step:11703 [D loss: 0.673825, acc: 58.59%] [G loss: 1.993359]\n",
      "epoch:12 step:11704 [D loss: 0.642213, acc: 60.16%] [G loss: 1.932775]\n",
      "epoch:12 step:11705 [D loss: 0.629560, acc: 67.97%] [G loss: 1.870010]\n",
      "epoch:12 step:11706 [D loss: 0.659272, acc: 56.25%] [G loss: 1.830151]\n",
      "epoch:12 step:11707 [D loss: 0.626578, acc: 62.50%] [G loss: 2.001511]\n",
      "epoch:12 step:11708 [D loss: 0.638643, acc: 65.62%] [G loss: 2.055478]\n",
      "epoch:12 step:11709 [D loss: 0.673088, acc: 60.16%] [G loss: 1.781742]\n",
      "epoch:12 step:11710 [D loss: 0.632767, acc: 66.41%] [G loss: 2.002189]\n",
      "epoch:12 step:11711 [D loss: 0.620246, acc: 62.50%] [G loss: 1.801755]\n",
      "epoch:12 step:11712 [D loss: 0.669180, acc: 60.16%] [G loss: 2.071217]\n",
      "epoch:12 step:11713 [D loss: 0.622526, acc: 65.62%] [G loss: 2.028739]\n",
      "epoch:12 step:11714 [D loss: 0.629291, acc: 62.50%] [G loss: 2.039592]\n",
      "epoch:12 step:11715 [D loss: 0.587337, acc: 67.97%] [G loss: 2.145710]\n",
      "epoch:12 step:11716 [D loss: 0.623328, acc: 64.06%] [G loss: 2.140476]\n",
      "epoch:12 step:11717 [D loss: 0.710752, acc: 53.91%] [G loss: 1.925368]\n",
      "epoch:12 step:11718 [D loss: 0.630246, acc: 61.72%] [G loss: 1.900758]\n",
      "epoch:12 step:11719 [D loss: 0.603622, acc: 63.28%] [G loss: 2.003247]\n",
      "epoch:12 step:11720 [D loss: 0.622050, acc: 61.72%] [G loss: 2.120508]\n",
      "epoch:12 step:11721 [D loss: 0.626224, acc: 64.06%] [G loss: 1.750304]\n",
      "epoch:12 step:11722 [D loss: 0.628124, acc: 67.19%] [G loss: 1.827119]\n",
      "epoch:12 step:11723 [D loss: 0.600345, acc: 68.75%] [G loss: 2.004597]\n",
      "epoch:12 step:11724 [D loss: 0.617524, acc: 63.28%] [G loss: 2.124607]\n",
      "epoch:12 step:11725 [D loss: 0.610869, acc: 64.06%] [G loss: 2.138931]\n",
      "epoch:12 step:11726 [D loss: 0.722132, acc: 55.47%] [G loss: 1.813279]\n",
      "epoch:12 step:11727 [D loss: 0.669285, acc: 58.59%] [G loss: 1.860006]\n",
      "epoch:12 step:11728 [D loss: 0.600538, acc: 60.94%] [G loss: 2.149600]\n",
      "epoch:12 step:11729 [D loss: 0.640436, acc: 64.06%] [G loss: 1.908895]\n",
      "epoch:12 step:11730 [D loss: 0.645585, acc: 64.06%] [G loss: 1.975653]\n",
      "epoch:12 step:11731 [D loss: 0.642165, acc: 58.59%] [G loss: 1.975575]\n",
      "epoch:12 step:11732 [D loss: 0.589300, acc: 67.97%] [G loss: 2.169958]\n",
      "epoch:12 step:11733 [D loss: 0.656094, acc: 63.28%] [G loss: 1.948802]\n",
      "epoch:12 step:11734 [D loss: 0.683611, acc: 61.72%] [G loss: 2.032125]\n",
      "epoch:12 step:11735 [D loss: 0.643436, acc: 60.16%] [G loss: 2.017891]\n",
      "epoch:12 step:11736 [D loss: 0.622645, acc: 63.28%] [G loss: 1.970622]\n",
      "epoch:12 step:11737 [D loss: 0.667620, acc: 58.59%] [G loss: 2.030425]\n",
      "epoch:12 step:11738 [D loss: 0.568700, acc: 71.09%] [G loss: 2.119122]\n",
      "epoch:12 step:11739 [D loss: 0.568188, acc: 71.88%] [G loss: 2.257896]\n",
      "epoch:12 step:11740 [D loss: 0.631077, acc: 64.84%] [G loss: 2.035209]\n",
      "epoch:12 step:11741 [D loss: 0.584094, acc: 71.09%] [G loss: 2.250214]\n",
      "epoch:12 step:11742 [D loss: 0.624745, acc: 63.28%] [G loss: 2.295665]\n",
      "epoch:12 step:11743 [D loss: 0.615363, acc: 73.44%] [G loss: 2.369339]\n",
      "epoch:12 step:11744 [D loss: 0.720854, acc: 50.00%] [G loss: 1.840683]\n",
      "epoch:12 step:11745 [D loss: 0.664932, acc: 62.50%] [G loss: 1.822716]\n",
      "epoch:12 step:11746 [D loss: 0.716045, acc: 56.25%] [G loss: 1.718009]\n",
      "epoch:12 step:11747 [D loss: 0.605282, acc: 66.41%] [G loss: 1.775551]\n",
      "epoch:12 step:11748 [D loss: 0.630736, acc: 61.72%] [G loss: 2.084361]\n",
      "epoch:12 step:11749 [D loss: 0.659384, acc: 58.59%] [G loss: 1.950755]\n",
      "epoch:12 step:11750 [D loss: 0.664223, acc: 61.72%] [G loss: 1.869086]\n",
      "epoch:12 step:11751 [D loss: 0.721156, acc: 57.81%] [G loss: 1.866003]\n",
      "epoch:12 step:11752 [D loss: 0.624012, acc: 69.53%] [G loss: 2.076677]\n",
      "epoch:12 step:11753 [D loss: 0.626443, acc: 63.28%] [G loss: 2.003261]\n",
      "epoch:12 step:11754 [D loss: 0.662964, acc: 57.03%] [G loss: 1.934450]\n",
      "epoch:12 step:11755 [D loss: 0.692417, acc: 56.25%] [G loss: 1.950763]\n",
      "epoch:12 step:11756 [D loss: 0.639643, acc: 64.06%] [G loss: 2.077733]\n",
      "epoch:12 step:11757 [D loss: 0.680949, acc: 54.69%] [G loss: 1.883136]\n",
      "epoch:12 step:11758 [D loss: 0.592602, acc: 67.97%] [G loss: 1.870466]\n",
      "epoch:12 step:11759 [D loss: 0.660944, acc: 59.38%] [G loss: 1.955892]\n",
      "epoch:12 step:11760 [D loss: 0.637505, acc: 64.06%] [G loss: 1.889770]\n",
      "epoch:12 step:11761 [D loss: 0.615720, acc: 67.19%] [G loss: 2.019101]\n",
      "epoch:12 step:11762 [D loss: 0.663795, acc: 65.62%] [G loss: 1.996908]\n",
      "epoch:12 step:11763 [D loss: 0.612791, acc: 66.41%] [G loss: 1.927091]\n",
      "epoch:12 step:11764 [D loss: 0.662805, acc: 59.38%] [G loss: 1.928278]\n",
      "epoch:12 step:11765 [D loss: 0.617455, acc: 62.50%] [G loss: 1.977625]\n",
      "epoch:12 step:11766 [D loss: 0.549487, acc: 77.34%] [G loss: 2.107077]\n",
      "epoch:12 step:11767 [D loss: 0.617306, acc: 67.97%] [G loss: 2.144065]\n",
      "epoch:12 step:11768 [D loss: 0.672019, acc: 61.72%] [G loss: 2.146307]\n",
      "epoch:12 step:11769 [D loss: 0.614290, acc: 67.19%] [G loss: 1.820790]\n",
      "epoch:12 step:11770 [D loss: 0.632304, acc: 63.28%] [G loss: 1.857201]\n",
      "epoch:12 step:11771 [D loss: 0.678102, acc: 58.59%] [G loss: 1.943085]\n",
      "epoch:12 step:11772 [D loss: 0.662942, acc: 57.81%] [G loss: 1.913526]\n",
      "epoch:12 step:11773 [D loss: 0.671602, acc: 57.03%] [G loss: 1.864704]\n",
      "epoch:12 step:11774 [D loss: 0.649078, acc: 60.94%] [G loss: 1.913076]\n",
      "epoch:12 step:11775 [D loss: 0.662818, acc: 61.72%] [G loss: 1.778712]\n",
      "epoch:12 step:11776 [D loss: 0.634609, acc: 63.28%] [G loss: 2.024663]\n",
      "epoch:12 step:11777 [D loss: 0.671518, acc: 57.81%] [G loss: 1.883663]\n",
      "epoch:12 step:11778 [D loss: 0.603896, acc: 66.41%] [G loss: 2.072222]\n",
      "epoch:12 step:11779 [D loss: 0.614562, acc: 66.41%] [G loss: 1.944547]\n",
      "epoch:12 step:11780 [D loss: 0.602706, acc: 69.53%] [G loss: 2.150290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11781 [D loss: 0.612513, acc: 63.28%] [G loss: 2.108698]\n",
      "epoch:12 step:11782 [D loss: 0.632492, acc: 62.50%] [G loss: 1.988578]\n",
      "epoch:12 step:11783 [D loss: 0.704856, acc: 55.47%] [G loss: 1.951581]\n",
      "epoch:12 step:11784 [D loss: 0.613367, acc: 62.50%] [G loss: 1.911748]\n",
      "epoch:12 step:11785 [D loss: 0.671641, acc: 61.72%] [G loss: 1.967799]\n",
      "epoch:12 step:11786 [D loss: 0.674722, acc: 58.59%] [G loss: 1.941203]\n",
      "epoch:12 step:11787 [D loss: 0.631657, acc: 63.28%] [G loss: 1.891695]\n",
      "epoch:12 step:11788 [D loss: 0.647037, acc: 64.06%] [G loss: 1.876574]\n",
      "epoch:12 step:11789 [D loss: 0.613081, acc: 68.75%] [G loss: 2.029123]\n",
      "epoch:12 step:11790 [D loss: 0.631467, acc: 60.94%] [G loss: 2.000324]\n",
      "epoch:12 step:11791 [D loss: 0.608206, acc: 67.97%] [G loss: 2.095682]\n",
      "epoch:12 step:11792 [D loss: 0.614484, acc: 61.72%] [G loss: 1.896733]\n",
      "epoch:12 step:11793 [D loss: 0.582395, acc: 68.75%] [G loss: 2.311315]\n",
      "epoch:12 step:11794 [D loss: 0.590703, acc: 70.31%] [G loss: 2.322320]\n",
      "epoch:12 step:11795 [D loss: 0.605368, acc: 63.28%] [G loss: 2.153159]\n",
      "epoch:12 step:11796 [D loss: 0.635948, acc: 60.94%] [G loss: 2.064341]\n",
      "epoch:12 step:11797 [D loss: 0.598555, acc: 67.19%] [G loss: 1.927187]\n",
      "epoch:12 step:11798 [D loss: 0.591212, acc: 66.41%] [G loss: 2.070227]\n",
      "epoch:12 step:11799 [D loss: 0.687140, acc: 62.50%] [G loss: 2.106838]\n",
      "epoch:12 step:11800 [D loss: 0.597657, acc: 71.09%] [G loss: 2.179133]\n",
      "epoch:12 step:11801 [D loss: 0.630379, acc: 65.62%] [G loss: 2.045732]\n",
      "epoch:12 step:11802 [D loss: 0.628885, acc: 63.28%] [G loss: 2.131172]\n",
      "epoch:12 step:11803 [D loss: 0.671440, acc: 55.47%] [G loss: 2.021662]\n",
      "epoch:12 step:11804 [D loss: 0.642630, acc: 60.16%] [G loss: 1.857554]\n",
      "epoch:12 step:11805 [D loss: 0.653487, acc: 64.84%] [G loss: 2.019115]\n",
      "epoch:12 step:11806 [D loss: 0.622826, acc: 64.84%] [G loss: 1.967431]\n",
      "epoch:12 step:11807 [D loss: 0.643716, acc: 62.50%] [G loss: 2.070496]\n",
      "epoch:12 step:11808 [D loss: 0.607273, acc: 61.72%] [G loss: 2.110084]\n",
      "epoch:12 step:11809 [D loss: 0.686425, acc: 57.03%] [G loss: 1.906710]\n",
      "epoch:12 step:11810 [D loss: 0.692380, acc: 52.34%] [G loss: 1.795879]\n",
      "epoch:12 step:11811 [D loss: 0.621370, acc: 63.28%] [G loss: 2.032973]\n",
      "epoch:12 step:11812 [D loss: 0.626174, acc: 70.31%] [G loss: 1.862898]\n",
      "epoch:12 step:11813 [D loss: 0.646041, acc: 62.50%] [G loss: 2.080050]\n",
      "epoch:12 step:11814 [D loss: 0.647168, acc: 60.16%] [G loss: 1.809567]\n",
      "epoch:12 step:11815 [D loss: 0.657438, acc: 61.72%] [G loss: 2.014528]\n",
      "epoch:12 step:11816 [D loss: 0.633041, acc: 67.19%] [G loss: 1.797006]\n",
      "epoch:12 step:11817 [D loss: 0.643492, acc: 64.06%] [G loss: 1.865825]\n",
      "epoch:12 step:11818 [D loss: 0.597989, acc: 66.41%] [G loss: 1.985601]\n",
      "epoch:12 step:11819 [D loss: 0.617617, acc: 68.75%] [G loss: 1.910801]\n",
      "epoch:12 step:11820 [D loss: 0.625832, acc: 63.28%] [G loss: 1.853306]\n",
      "epoch:12 step:11821 [D loss: 0.661174, acc: 61.72%] [G loss: 1.771691]\n",
      "epoch:12 step:11822 [D loss: 0.615805, acc: 65.62%] [G loss: 2.085068]\n",
      "epoch:12 step:11823 [D loss: 0.662521, acc: 62.50%] [G loss: 1.974807]\n",
      "epoch:12 step:11824 [D loss: 0.684335, acc: 58.59%] [G loss: 1.744535]\n",
      "epoch:12 step:11825 [D loss: 0.639287, acc: 62.50%] [G loss: 1.954482]\n",
      "epoch:12 step:11826 [D loss: 0.593458, acc: 67.97%] [G loss: 1.989001]\n",
      "epoch:12 step:11827 [D loss: 0.649335, acc: 66.41%] [G loss: 1.917391]\n",
      "epoch:12 step:11828 [D loss: 0.667145, acc: 63.28%] [G loss: 1.857886]\n",
      "epoch:12 step:11829 [D loss: 0.620495, acc: 63.28%] [G loss: 1.997887]\n",
      "epoch:12 step:11830 [D loss: 0.631944, acc: 64.84%] [G loss: 1.959367]\n",
      "epoch:12 step:11831 [D loss: 0.627790, acc: 60.94%] [G loss: 2.003599]\n",
      "epoch:12 step:11832 [D loss: 0.630840, acc: 65.62%] [G loss: 2.255744]\n",
      "epoch:12 step:11833 [D loss: 0.633764, acc: 64.06%] [G loss: 2.185974]\n",
      "epoch:12 step:11834 [D loss: 0.648423, acc: 60.94%] [G loss: 1.824798]\n",
      "epoch:12 step:11835 [D loss: 0.646331, acc: 63.28%] [G loss: 2.051259]\n",
      "epoch:12 step:11836 [D loss: 0.618820, acc: 65.62%] [G loss: 1.955552]\n",
      "epoch:12 step:11837 [D loss: 0.654385, acc: 62.50%] [G loss: 1.958166]\n",
      "epoch:12 step:11838 [D loss: 0.623068, acc: 71.09%] [G loss: 1.952898]\n",
      "epoch:12 step:11839 [D loss: 0.655751, acc: 67.19%] [G loss: 2.000249]\n",
      "epoch:12 step:11840 [D loss: 0.652081, acc: 65.62%] [G loss: 1.847521]\n",
      "epoch:12 step:11841 [D loss: 0.691132, acc: 53.91%] [G loss: 1.924167]\n",
      "epoch:12 step:11842 [D loss: 0.570212, acc: 77.34%] [G loss: 1.949638]\n",
      "epoch:12 step:11843 [D loss: 0.688749, acc: 63.28%] [G loss: 1.961861]\n",
      "epoch:12 step:11844 [D loss: 0.670918, acc: 60.94%] [G loss: 1.920548]\n",
      "epoch:12 step:11845 [D loss: 0.632607, acc: 66.41%] [G loss: 2.001231]\n",
      "epoch:12 step:11846 [D loss: 0.627544, acc: 64.06%] [G loss: 2.046936]\n",
      "epoch:12 step:11847 [D loss: 0.597431, acc: 68.75%] [G loss: 2.105952]\n",
      "epoch:12 step:11848 [D loss: 0.701230, acc: 53.12%] [G loss: 1.928988]\n",
      "epoch:12 step:11849 [D loss: 0.642833, acc: 64.06%] [G loss: 2.074377]\n",
      "epoch:12 step:11850 [D loss: 0.666961, acc: 63.28%] [G loss: 1.885428]\n",
      "epoch:12 step:11851 [D loss: 0.636711, acc: 63.28%] [G loss: 1.894480]\n",
      "epoch:12 step:11852 [D loss: 0.708086, acc: 61.72%] [G loss: 2.064840]\n",
      "epoch:12 step:11853 [D loss: 0.558240, acc: 69.53%] [G loss: 2.023143]\n",
      "epoch:12 step:11854 [D loss: 0.690686, acc: 63.28%] [G loss: 1.859528]\n",
      "epoch:12 step:11855 [D loss: 0.601928, acc: 71.09%] [G loss: 1.924613]\n",
      "epoch:12 step:11856 [D loss: 0.651987, acc: 60.94%] [G loss: 1.889000]\n",
      "epoch:12 step:11857 [D loss: 0.616369, acc: 65.62%] [G loss: 1.934503]\n",
      "epoch:12 step:11858 [D loss: 0.629178, acc: 63.28%] [G loss: 1.880178]\n",
      "epoch:12 step:11859 [D loss: 0.674475, acc: 60.94%] [G loss: 1.857304]\n",
      "epoch:12 step:11860 [D loss: 0.662303, acc: 61.72%] [G loss: 2.010438]\n",
      "epoch:12 step:11861 [D loss: 0.676730, acc: 55.47%] [G loss: 1.998526]\n",
      "epoch:12 step:11862 [D loss: 0.662081, acc: 63.28%] [G loss: 1.898241]\n",
      "epoch:12 step:11863 [D loss: 0.580142, acc: 70.31%] [G loss: 1.894155]\n",
      "epoch:12 step:11864 [D loss: 0.660224, acc: 64.84%] [G loss: 1.947098]\n",
      "epoch:12 step:11865 [D loss: 0.639943, acc: 66.41%] [G loss: 1.947312]\n",
      "epoch:12 step:11866 [D loss: 0.686668, acc: 60.94%] [G loss: 1.836858]\n",
      "epoch:12 step:11867 [D loss: 0.621048, acc: 67.97%] [G loss: 1.903888]\n",
      "epoch:12 step:11868 [D loss: 0.638975, acc: 62.50%] [G loss: 1.951516]\n",
      "epoch:12 step:11869 [D loss: 0.683139, acc: 59.38%] [G loss: 1.902754]\n",
      "epoch:12 step:11870 [D loss: 0.680343, acc: 56.25%] [G loss: 1.883715]\n",
      "epoch:12 step:11871 [D loss: 0.643076, acc: 60.16%] [G loss: 1.955347]\n",
      "epoch:12 step:11872 [D loss: 0.697295, acc: 54.69%] [G loss: 1.857970]\n",
      "epoch:12 step:11873 [D loss: 0.673511, acc: 59.38%] [G loss: 1.946173]\n",
      "epoch:12 step:11874 [D loss: 0.623082, acc: 69.53%] [G loss: 2.020483]\n",
      "epoch:12 step:11875 [D loss: 0.643603, acc: 65.62%] [G loss: 1.903560]\n",
      "epoch:12 step:11876 [D loss: 0.631359, acc: 67.19%] [G loss: 1.923669]\n",
      "epoch:12 step:11877 [D loss: 0.636524, acc: 62.50%] [G loss: 1.878748]\n",
      "epoch:12 step:11878 [D loss: 0.623859, acc: 65.62%] [G loss: 2.154150]\n",
      "epoch:12 step:11879 [D loss: 0.648662, acc: 61.72%] [G loss: 2.009235]\n",
      "epoch:12 step:11880 [D loss: 0.633516, acc: 67.19%] [G loss: 2.004343]\n",
      "epoch:12 step:11881 [D loss: 0.607463, acc: 70.31%] [G loss: 1.990648]\n",
      "epoch:12 step:11882 [D loss: 0.627979, acc: 66.41%] [G loss: 2.130456]\n",
      "epoch:12 step:11883 [D loss: 0.654836, acc: 63.28%] [G loss: 2.067108]\n",
      "epoch:12 step:11884 [D loss: 0.639107, acc: 64.06%] [G loss: 2.111569]\n",
      "epoch:12 step:11885 [D loss: 0.652097, acc: 66.41%] [G loss: 2.183333]\n",
      "epoch:12 step:11886 [D loss: 0.551805, acc: 75.00%] [G loss: 2.234547]\n",
      "epoch:12 step:11887 [D loss: 0.638871, acc: 64.06%] [G loss: 1.965701]\n",
      "epoch:12 step:11888 [D loss: 0.639936, acc: 66.41%] [G loss: 1.953862]\n",
      "epoch:12 step:11889 [D loss: 0.662993, acc: 62.50%] [G loss: 1.877294]\n",
      "epoch:12 step:11890 [D loss: 0.607379, acc: 73.44%] [G loss: 2.127886]\n",
      "epoch:12 step:11891 [D loss: 0.668438, acc: 55.47%] [G loss: 2.167605]\n",
      "epoch:12 step:11892 [D loss: 0.573499, acc: 67.97%] [G loss: 2.387685]\n",
      "epoch:12 step:11893 [D loss: 0.606696, acc: 68.75%] [G loss: 2.313149]\n",
      "epoch:12 step:11894 [D loss: 0.620941, acc: 66.41%] [G loss: 2.169141]\n",
      "epoch:12 step:11895 [D loss: 0.616918, acc: 64.06%] [G loss: 2.030929]\n",
      "epoch:12 step:11896 [D loss: 0.603512, acc: 68.75%] [G loss: 1.900055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:11897 [D loss: 0.622273, acc: 64.06%] [G loss: 1.980466]\n",
      "epoch:12 step:11898 [D loss: 0.601100, acc: 69.53%] [G loss: 2.078799]\n",
      "epoch:12 step:11899 [D loss: 0.632857, acc: 64.84%] [G loss: 1.990510]\n",
      "epoch:12 step:11900 [D loss: 0.686543, acc: 57.81%] [G loss: 1.855400]\n",
      "epoch:12 step:11901 [D loss: 0.704506, acc: 56.25%] [G loss: 1.900079]\n",
      "epoch:12 step:11902 [D loss: 0.689785, acc: 52.34%] [G loss: 1.827425]\n",
      "epoch:12 step:11903 [D loss: 0.695212, acc: 57.81%] [G loss: 1.853897]\n",
      "epoch:12 step:11904 [D loss: 0.630029, acc: 65.62%] [G loss: 1.777315]\n",
      "epoch:12 step:11905 [D loss: 0.649595, acc: 63.28%] [G loss: 1.990538]\n",
      "epoch:12 step:11906 [D loss: 0.639576, acc: 61.72%] [G loss: 2.010068]\n",
      "epoch:12 step:11907 [D loss: 0.594827, acc: 69.53%] [G loss: 1.863780]\n",
      "epoch:12 step:11908 [D loss: 0.675245, acc: 56.25%] [G loss: 1.882676]\n",
      "epoch:12 step:11909 [D loss: 0.636917, acc: 65.62%] [G loss: 1.947981]\n",
      "epoch:12 step:11910 [D loss: 0.633515, acc: 65.62%] [G loss: 1.915132]\n",
      "epoch:12 step:11911 [D loss: 0.634009, acc: 64.84%] [G loss: 1.780857]\n",
      "epoch:12 step:11912 [D loss: 0.651995, acc: 57.81%] [G loss: 1.893449]\n",
      "epoch:12 step:11913 [D loss: 0.647430, acc: 61.72%] [G loss: 1.705917]\n",
      "epoch:12 step:11914 [D loss: 0.630732, acc: 64.84%] [G loss: 1.965188]\n",
      "epoch:12 step:11915 [D loss: 0.652099, acc: 64.06%] [G loss: 1.983385]\n",
      "epoch:12 step:11916 [D loss: 0.639605, acc: 62.50%] [G loss: 2.000114]\n",
      "epoch:12 step:11917 [D loss: 0.651930, acc: 63.28%] [G loss: 1.872469]\n",
      "epoch:12 step:11918 [D loss: 0.662171, acc: 57.81%] [G loss: 1.982463]\n",
      "epoch:12 step:11919 [D loss: 0.735471, acc: 54.69%] [G loss: 1.910733]\n",
      "epoch:12 step:11920 [D loss: 0.649938, acc: 66.41%] [G loss: 1.968616]\n",
      "epoch:12 step:11921 [D loss: 0.580138, acc: 74.22%] [G loss: 2.004922]\n",
      "epoch:12 step:11922 [D loss: 0.639683, acc: 64.84%] [G loss: 2.087048]\n",
      "epoch:12 step:11923 [D loss: 0.623024, acc: 70.31%] [G loss: 2.015525]\n",
      "epoch:12 step:11924 [D loss: 0.647005, acc: 64.06%] [G loss: 1.998045]\n",
      "epoch:12 step:11925 [D loss: 0.642269, acc: 61.72%] [G loss: 2.023185]\n",
      "epoch:12 step:11926 [D loss: 0.611357, acc: 63.28%] [G loss: 1.943082]\n",
      "epoch:12 step:11927 [D loss: 0.662692, acc: 64.06%] [G loss: 1.993002]\n",
      "epoch:12 step:11928 [D loss: 0.636610, acc: 60.94%] [G loss: 1.947281]\n",
      "epoch:12 step:11929 [D loss: 0.651250, acc: 64.06%] [G loss: 1.996939]\n",
      "epoch:12 step:11930 [D loss: 0.626873, acc: 64.06%] [G loss: 1.972164]\n",
      "epoch:12 step:11931 [D loss: 0.663612, acc: 57.81%] [G loss: 1.874341]\n",
      "epoch:12 step:11932 [D loss: 0.659434, acc: 60.16%] [G loss: 1.885032]\n",
      "epoch:12 step:11933 [D loss: 0.630307, acc: 62.50%] [G loss: 1.938154]\n",
      "epoch:12 step:11934 [D loss: 0.656182, acc: 58.59%] [G loss: 2.006066]\n",
      "epoch:12 step:11935 [D loss: 0.647297, acc: 57.03%] [G loss: 2.031985]\n",
      "epoch:12 step:11936 [D loss: 0.606141, acc: 67.97%] [G loss: 2.090413]\n",
      "epoch:12 step:11937 [D loss: 0.622761, acc: 66.41%] [G loss: 2.106856]\n",
      "epoch:12 step:11938 [D loss: 0.621907, acc: 67.19%] [G loss: 2.110700]\n",
      "epoch:12 step:11939 [D loss: 0.633831, acc: 65.62%] [G loss: 2.051424]\n",
      "epoch:12 step:11940 [D loss: 0.658168, acc: 57.81%] [G loss: 1.867458]\n",
      "epoch:12 step:11941 [D loss: 0.625822, acc: 64.06%] [G loss: 1.944389]\n",
      "epoch:12 step:11942 [D loss: 0.675483, acc: 59.38%] [G loss: 1.889525]\n",
      "epoch:12 step:11943 [D loss: 0.579633, acc: 76.56%] [G loss: 1.976863]\n",
      "epoch:12 step:11944 [D loss: 0.651222, acc: 57.81%] [G loss: 1.949486]\n",
      "epoch:12 step:11945 [D loss: 0.647006, acc: 66.41%] [G loss: 2.074556]\n",
      "epoch:12 step:11946 [D loss: 0.623108, acc: 63.28%] [G loss: 1.991546]\n",
      "epoch:12 step:11947 [D loss: 0.684652, acc: 60.16%] [G loss: 1.874346]\n",
      "epoch:12 step:11948 [D loss: 0.620969, acc: 65.62%] [G loss: 1.877033]\n",
      "epoch:12 step:11949 [D loss: 0.678215, acc: 58.59%] [G loss: 2.085174]\n",
      "epoch:12 step:11950 [D loss: 0.603532, acc: 69.53%] [G loss: 1.962507]\n",
      "epoch:12 step:11951 [D loss: 0.620988, acc: 66.41%] [G loss: 2.162830]\n",
      "epoch:12 step:11952 [D loss: 0.572629, acc: 74.22%] [G loss: 2.270488]\n",
      "epoch:12 step:11953 [D loss: 0.703207, acc: 59.38%] [G loss: 2.046279]\n",
      "epoch:12 step:11954 [D loss: 0.680217, acc: 61.72%] [G loss: 1.968632]\n",
      "epoch:12 step:11955 [D loss: 0.649605, acc: 64.84%] [G loss: 1.998103]\n",
      "epoch:12 step:11956 [D loss: 0.578293, acc: 66.41%] [G loss: 1.942743]\n",
      "epoch:12 step:11957 [D loss: 0.603770, acc: 64.84%] [G loss: 2.044571]\n",
      "epoch:12 step:11958 [D loss: 0.671784, acc: 61.72%] [G loss: 2.072357]\n",
      "epoch:12 step:11959 [D loss: 0.654394, acc: 62.50%] [G loss: 1.888482]\n",
      "epoch:12 step:11960 [D loss: 0.711438, acc: 57.81%] [G loss: 1.939390]\n",
      "epoch:12 step:11961 [D loss: 0.662948, acc: 61.72%] [G loss: 1.866635]\n",
      "epoch:12 step:11962 [D loss: 0.664419, acc: 62.50%] [G loss: 1.849527]\n",
      "epoch:12 step:11963 [D loss: 0.622036, acc: 63.28%] [G loss: 1.901085]\n",
      "epoch:12 step:11964 [D loss: 0.616650, acc: 65.62%] [G loss: 2.088323]\n",
      "epoch:12 step:11965 [D loss: 0.689710, acc: 57.03%] [G loss: 1.966657]\n",
      "epoch:12 step:11966 [D loss: 0.653499, acc: 61.72%] [G loss: 1.973266]\n",
      "epoch:12 step:11967 [D loss: 0.698219, acc: 63.28%] [G loss: 1.891198]\n",
      "epoch:12 step:11968 [D loss: 0.605582, acc: 62.50%] [G loss: 1.897435]\n",
      "epoch:12 step:11969 [D loss: 0.674397, acc: 60.16%] [G loss: 2.006715]\n",
      "epoch:12 step:11970 [D loss: 0.634234, acc: 65.62%] [G loss: 1.943338]\n",
      "epoch:12 step:11971 [D loss: 0.606374, acc: 67.19%] [G loss: 1.931516]\n",
      "epoch:12 step:11972 [D loss: 0.615847, acc: 66.41%] [G loss: 1.952607]\n",
      "epoch:12 step:11973 [D loss: 0.636853, acc: 63.28%] [G loss: 1.881162]\n",
      "epoch:12 step:11974 [D loss: 0.712949, acc: 55.47%] [G loss: 1.778090]\n",
      "epoch:12 step:11975 [D loss: 0.691642, acc: 57.81%] [G loss: 1.857112]\n",
      "epoch:12 step:11976 [D loss: 0.680979, acc: 56.25%] [G loss: 1.868251]\n",
      "epoch:12 step:11977 [D loss: 0.659689, acc: 60.16%] [G loss: 1.903041]\n",
      "epoch:12 step:11978 [D loss: 0.658919, acc: 61.72%] [G loss: 1.767329]\n",
      "epoch:12 step:11979 [D loss: 0.651703, acc: 60.16%] [G loss: 1.890826]\n",
      "epoch:12 step:11980 [D loss: 0.683297, acc: 53.91%] [G loss: 2.013726]\n",
      "epoch:12 step:11981 [D loss: 0.617027, acc: 61.72%] [G loss: 1.865116]\n",
      "epoch:12 step:11982 [D loss: 0.711730, acc: 56.25%] [G loss: 1.901201]\n",
      "epoch:12 step:11983 [D loss: 0.651554, acc: 59.38%] [G loss: 1.983484]\n",
      "epoch:12 step:11984 [D loss: 0.625035, acc: 64.06%] [G loss: 1.942256]\n",
      "epoch:12 step:11985 [D loss: 0.672651, acc: 57.03%] [G loss: 1.833438]\n",
      "epoch:12 step:11986 [D loss: 0.652110, acc: 65.62%] [G loss: 1.903596]\n",
      "epoch:12 step:11987 [D loss: 0.674487, acc: 60.94%] [G loss: 1.808146]\n",
      "epoch:12 step:11988 [D loss: 0.638422, acc: 65.62%] [G loss: 1.911006]\n",
      "epoch:12 step:11989 [D loss: 0.675891, acc: 60.94%] [G loss: 1.997900]\n",
      "epoch:12 step:11990 [D loss: 0.583385, acc: 75.00%] [G loss: 2.001227]\n",
      "epoch:12 step:11991 [D loss: 0.597551, acc: 69.53%] [G loss: 1.955809]\n",
      "epoch:12 step:11992 [D loss: 0.632024, acc: 61.72%] [G loss: 1.888394]\n",
      "epoch:12 step:11993 [D loss: 0.645989, acc: 57.03%] [G loss: 1.821249]\n",
      "epoch:12 step:11994 [D loss: 0.660395, acc: 63.28%] [G loss: 1.815041]\n",
      "epoch:12 step:11995 [D loss: 0.628413, acc: 69.53%] [G loss: 1.842972]\n",
      "epoch:12 step:11996 [D loss: 0.625379, acc: 65.62%] [G loss: 1.928245]\n",
      "epoch:12 step:11997 [D loss: 0.683328, acc: 53.91%] [G loss: 2.008811]\n",
      "epoch:12 step:11998 [D loss: 0.660994, acc: 63.28%] [G loss: 1.992521]\n",
      "epoch:12 step:11999 [D loss: 0.635993, acc: 69.53%] [G loss: 1.951270]\n",
      "epoch:12 step:12000 [D loss: 0.641569, acc: 67.97%] [G loss: 1.921214]\n",
      "epoch:12 step:12001 [D loss: 0.609812, acc: 66.41%] [G loss: 1.883627]\n",
      "epoch:12 step:12002 [D loss: 0.657141, acc: 64.06%] [G loss: 1.924938]\n",
      "epoch:12 step:12003 [D loss: 0.665725, acc: 57.03%] [G loss: 2.009892]\n",
      "epoch:12 step:12004 [D loss: 0.632017, acc: 60.94%] [G loss: 1.908580]\n",
      "epoch:12 step:12005 [D loss: 0.640418, acc: 64.06%] [G loss: 1.890003]\n",
      "epoch:12 step:12006 [D loss: 0.628395, acc: 64.06%] [G loss: 1.881299]\n",
      "epoch:12 step:12007 [D loss: 0.600395, acc: 72.66%] [G loss: 1.943333]\n",
      "epoch:12 step:12008 [D loss: 0.634187, acc: 64.06%] [G loss: 1.901596]\n",
      "epoch:12 step:12009 [D loss: 0.726985, acc: 50.00%] [G loss: 1.937546]\n",
      "epoch:12 step:12010 [D loss: 0.667748, acc: 59.38%] [G loss: 1.924150]\n",
      "epoch:12 step:12011 [D loss: 0.625546, acc: 66.41%] [G loss: 2.052738]\n",
      "epoch:12 step:12012 [D loss: 0.620501, acc: 70.31%] [G loss: 1.808479]\n",
      "epoch:12 step:12013 [D loss: 0.644755, acc: 61.72%] [G loss: 1.944812]\n",
      "epoch:12 step:12014 [D loss: 0.643643, acc: 64.84%] [G loss: 1.993621]\n",
      "epoch:12 step:12015 [D loss: 0.636073, acc: 60.16%] [G loss: 2.025644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12016 [D loss: 0.635304, acc: 65.62%] [G loss: 2.002333]\n",
      "epoch:12 step:12017 [D loss: 0.628005, acc: 61.72%] [G loss: 1.950643]\n",
      "epoch:12 step:12018 [D loss: 0.621033, acc: 65.62%] [G loss: 2.265472]\n",
      "epoch:12 step:12019 [D loss: 0.595317, acc: 65.62%] [G loss: 2.119557]\n",
      "epoch:12 step:12020 [D loss: 0.634274, acc: 60.94%] [G loss: 1.926074]\n",
      "epoch:12 step:12021 [D loss: 0.697562, acc: 55.47%] [G loss: 1.867105]\n",
      "epoch:12 step:12022 [D loss: 0.596364, acc: 68.75%] [G loss: 2.027495]\n",
      "epoch:12 step:12023 [D loss: 0.654766, acc: 60.94%] [G loss: 1.931796]\n",
      "epoch:12 step:12024 [D loss: 0.615533, acc: 65.62%] [G loss: 2.193502]\n",
      "epoch:12 step:12025 [D loss: 0.597871, acc: 72.66%] [G loss: 2.026001]\n",
      "epoch:12 step:12026 [D loss: 0.590294, acc: 71.88%] [G loss: 2.193998]\n",
      "epoch:12 step:12027 [D loss: 0.611734, acc: 67.19%] [G loss: 2.103027]\n",
      "epoch:12 step:12028 [D loss: 0.666517, acc: 60.16%] [G loss: 1.776878]\n",
      "epoch:12 step:12029 [D loss: 0.610695, acc: 67.97%] [G loss: 2.006424]\n",
      "epoch:12 step:12030 [D loss: 0.620887, acc: 63.28%] [G loss: 1.975190]\n",
      "epoch:12 step:12031 [D loss: 0.694746, acc: 53.12%] [G loss: 1.789915]\n",
      "epoch:12 step:12032 [D loss: 0.634612, acc: 66.41%] [G loss: 1.864748]\n",
      "epoch:12 step:12033 [D loss: 0.620488, acc: 64.06%] [G loss: 1.970837]\n",
      "epoch:12 step:12034 [D loss: 0.602171, acc: 67.97%] [G loss: 2.019777]\n",
      "epoch:12 step:12035 [D loss: 0.623075, acc: 67.19%] [G loss: 2.014808]\n",
      "epoch:12 step:12036 [D loss: 0.520499, acc: 78.91%] [G loss: 2.147992]\n",
      "epoch:12 step:12037 [D loss: 0.611568, acc: 62.50%] [G loss: 1.925442]\n",
      "epoch:12 step:12038 [D loss: 0.751758, acc: 58.59%] [G loss: 1.854606]\n",
      "epoch:12 step:12039 [D loss: 0.638774, acc: 65.62%] [G loss: 1.907066]\n",
      "epoch:12 step:12040 [D loss: 0.668657, acc: 56.25%] [G loss: 2.030913]\n",
      "epoch:12 step:12041 [D loss: 0.691402, acc: 53.91%] [G loss: 1.951881]\n",
      "epoch:12 step:12042 [D loss: 0.642420, acc: 65.62%] [G loss: 1.879329]\n",
      "epoch:12 step:12043 [D loss: 0.617757, acc: 67.97%] [G loss: 1.918529]\n",
      "epoch:12 step:12044 [D loss: 0.714337, acc: 53.12%] [G loss: 1.897051]\n",
      "epoch:12 step:12045 [D loss: 0.719923, acc: 52.34%] [G loss: 1.760905]\n",
      "epoch:12 step:12046 [D loss: 0.658765, acc: 63.28%] [G loss: 1.895815]\n",
      "epoch:12 step:12047 [D loss: 0.672459, acc: 57.81%] [G loss: 1.946077]\n",
      "epoch:12 step:12048 [D loss: 0.669060, acc: 58.59%] [G loss: 1.985712]\n",
      "epoch:12 step:12049 [D loss: 0.635133, acc: 60.16%] [G loss: 1.857286]\n",
      "epoch:12 step:12050 [D loss: 0.632566, acc: 64.06%] [G loss: 1.880495]\n",
      "epoch:12 step:12051 [D loss: 0.627105, acc: 65.62%] [G loss: 1.873689]\n",
      "epoch:12 step:12052 [D loss: 0.622423, acc: 69.53%] [G loss: 1.842399]\n",
      "epoch:12 step:12053 [D loss: 0.634487, acc: 60.94%] [G loss: 1.939179]\n",
      "epoch:12 step:12054 [D loss: 0.643754, acc: 66.41%] [G loss: 1.962029]\n",
      "epoch:12 step:12055 [D loss: 0.628990, acc: 66.41%] [G loss: 1.905200]\n",
      "epoch:12 step:12056 [D loss: 0.648853, acc: 61.72%] [G loss: 1.881150]\n",
      "epoch:12 step:12057 [D loss: 0.673408, acc: 65.62%] [G loss: 1.890665]\n",
      "epoch:12 step:12058 [D loss: 0.672068, acc: 64.06%] [G loss: 1.854939]\n",
      "epoch:12 step:12059 [D loss: 0.567405, acc: 71.88%] [G loss: 2.131901]\n",
      "epoch:12 step:12060 [D loss: 0.600584, acc: 71.09%] [G loss: 2.129220]\n",
      "epoch:12 step:12061 [D loss: 0.698626, acc: 53.12%] [G loss: 1.947431]\n",
      "epoch:12 step:12062 [D loss: 0.684952, acc: 57.03%] [G loss: 1.832557]\n",
      "epoch:12 step:12063 [D loss: 0.625834, acc: 67.19%] [G loss: 1.892856]\n",
      "epoch:12 step:12064 [D loss: 0.670386, acc: 63.28%] [G loss: 1.879142]\n",
      "epoch:12 step:12065 [D loss: 0.613863, acc: 67.97%] [G loss: 1.900701]\n",
      "epoch:12 step:12066 [D loss: 0.604325, acc: 71.09%] [G loss: 1.898607]\n",
      "epoch:12 step:12067 [D loss: 0.609965, acc: 70.31%] [G loss: 2.131776]\n",
      "epoch:12 step:12068 [D loss: 0.663527, acc: 62.50%] [G loss: 2.027668]\n",
      "epoch:12 step:12069 [D loss: 0.605375, acc: 65.62%] [G loss: 2.068793]\n",
      "epoch:12 step:12070 [D loss: 0.671159, acc: 60.94%] [G loss: 2.013788]\n",
      "epoch:12 step:12071 [D loss: 0.641990, acc: 59.38%] [G loss: 1.829648]\n",
      "epoch:12 step:12072 [D loss: 0.662214, acc: 57.03%] [G loss: 1.877002]\n",
      "epoch:12 step:12073 [D loss: 0.672634, acc: 59.38%] [G loss: 1.860675]\n",
      "epoch:12 step:12074 [D loss: 0.667036, acc: 60.16%] [G loss: 1.784167]\n",
      "epoch:12 step:12075 [D loss: 0.732937, acc: 52.34%] [G loss: 1.971005]\n",
      "epoch:12 step:12076 [D loss: 0.617634, acc: 67.19%] [G loss: 1.925590]\n",
      "epoch:12 step:12077 [D loss: 0.584981, acc: 67.19%] [G loss: 2.103051]\n",
      "epoch:12 step:12078 [D loss: 0.638341, acc: 65.62%] [G loss: 1.981128]\n",
      "epoch:12 step:12079 [D loss: 0.592995, acc: 69.53%] [G loss: 1.982075]\n",
      "epoch:12 step:12080 [D loss: 0.645360, acc: 60.16%] [G loss: 1.879026]\n",
      "epoch:12 step:12081 [D loss: 0.635701, acc: 65.62%] [G loss: 2.060985]\n",
      "epoch:12 step:12082 [D loss: 0.638050, acc: 64.06%] [G loss: 2.195895]\n",
      "epoch:12 step:12083 [D loss: 0.576797, acc: 69.53%] [G loss: 2.075019]\n",
      "epoch:12 step:12084 [D loss: 0.595600, acc: 69.53%] [G loss: 2.062157]\n",
      "epoch:12 step:12085 [D loss: 0.643219, acc: 64.06%] [G loss: 2.047575]\n",
      "epoch:12 step:12086 [D loss: 0.579140, acc: 73.44%] [G loss: 2.088161]\n",
      "epoch:12 step:12087 [D loss: 0.619423, acc: 64.06%] [G loss: 1.961454]\n",
      "epoch:12 step:12088 [D loss: 0.657123, acc: 62.50%] [G loss: 2.151159]\n",
      "epoch:12 step:12089 [D loss: 0.633413, acc: 63.28%] [G loss: 2.036352]\n",
      "epoch:12 step:12090 [D loss: 0.610715, acc: 64.84%] [G loss: 1.976490]\n",
      "epoch:12 step:12091 [D loss: 0.676100, acc: 58.59%] [G loss: 1.925457]\n",
      "epoch:12 step:12092 [D loss: 0.650998, acc: 60.16%] [G loss: 1.972454]\n",
      "epoch:12 step:12093 [D loss: 0.622374, acc: 67.19%] [G loss: 2.112472]\n",
      "epoch:12 step:12094 [D loss: 0.684140, acc: 59.38%] [G loss: 2.033989]\n",
      "epoch:12 step:12095 [D loss: 0.659046, acc: 60.94%] [G loss: 1.916474]\n",
      "epoch:12 step:12096 [D loss: 0.681397, acc: 59.38%] [G loss: 2.027771]\n",
      "epoch:12 step:12097 [D loss: 0.642054, acc: 64.06%] [G loss: 2.165768]\n",
      "epoch:12 step:12098 [D loss: 0.644227, acc: 61.72%] [G loss: 1.980054]\n",
      "epoch:12 step:12099 [D loss: 0.662127, acc: 53.91%] [G loss: 1.850806]\n",
      "epoch:12 step:12100 [D loss: 0.622768, acc: 68.75%] [G loss: 1.944608]\n",
      "epoch:12 step:12101 [D loss: 0.684975, acc: 58.59%] [G loss: 2.000398]\n",
      "epoch:12 step:12102 [D loss: 0.775310, acc: 47.66%] [G loss: 1.735360]\n",
      "epoch:12 step:12103 [D loss: 0.697585, acc: 53.91%] [G loss: 1.800454]\n",
      "epoch:12 step:12104 [D loss: 0.657856, acc: 60.94%] [G loss: 1.922455]\n",
      "epoch:12 step:12105 [D loss: 0.643698, acc: 62.50%] [G loss: 1.829728]\n",
      "epoch:12 step:12106 [D loss: 0.622757, acc: 67.97%] [G loss: 1.932210]\n",
      "epoch:12 step:12107 [D loss: 0.656544, acc: 59.38%] [G loss: 1.735267]\n",
      "epoch:12 step:12108 [D loss: 0.641410, acc: 56.25%] [G loss: 1.827183]\n",
      "epoch:12 step:12109 [D loss: 0.597391, acc: 69.53%] [G loss: 1.880714]\n",
      "epoch:12 step:12110 [D loss: 0.594884, acc: 69.53%] [G loss: 1.876467]\n",
      "epoch:12 step:12111 [D loss: 0.664515, acc: 56.25%] [G loss: 1.954682]\n",
      "epoch:12 step:12112 [D loss: 0.614951, acc: 72.66%] [G loss: 1.900588]\n",
      "epoch:12 step:12113 [D loss: 0.629363, acc: 64.06%] [G loss: 1.988478]\n",
      "epoch:12 step:12114 [D loss: 0.674938, acc: 59.38%] [G loss: 1.932773]\n",
      "epoch:12 step:12115 [D loss: 0.624205, acc: 64.84%] [G loss: 1.962870]\n",
      "epoch:12 step:12116 [D loss: 0.610928, acc: 70.31%] [G loss: 2.002323]\n",
      "epoch:12 step:12117 [D loss: 0.695959, acc: 53.12%] [G loss: 1.764776]\n",
      "epoch:12 step:12118 [D loss: 0.679522, acc: 60.94%] [G loss: 1.914621]\n",
      "epoch:12 step:12119 [D loss: 0.652466, acc: 65.62%] [G loss: 2.001899]\n",
      "epoch:12 step:12120 [D loss: 0.614399, acc: 61.72%] [G loss: 2.057578]\n",
      "epoch:12 step:12121 [D loss: 0.642456, acc: 63.28%] [G loss: 1.921618]\n",
      "epoch:12 step:12122 [D loss: 0.645070, acc: 59.38%] [G loss: 1.944237]\n",
      "epoch:12 step:12123 [D loss: 0.660568, acc: 62.50%] [G loss: 1.977058]\n",
      "epoch:12 step:12124 [D loss: 0.658185, acc: 59.38%] [G loss: 1.842000]\n",
      "epoch:12 step:12125 [D loss: 0.667288, acc: 64.06%] [G loss: 1.907835]\n",
      "epoch:12 step:12126 [D loss: 0.670094, acc: 61.72%] [G loss: 1.840687]\n",
      "epoch:12 step:12127 [D loss: 0.627787, acc: 71.09%] [G loss: 1.943458]\n",
      "epoch:12 step:12128 [D loss: 0.575935, acc: 68.75%] [G loss: 2.013458]\n",
      "epoch:12 step:12129 [D loss: 0.607889, acc: 65.62%] [G loss: 2.011837]\n",
      "epoch:12 step:12130 [D loss: 0.570275, acc: 75.00%] [G loss: 2.309212]\n",
      "epoch:12 step:12131 [D loss: 0.634482, acc: 64.06%] [G loss: 2.012367]\n",
      "epoch:12 step:12132 [D loss: 0.599820, acc: 66.41%] [G loss: 1.916378]\n",
      "epoch:12 step:12133 [D loss: 0.648837, acc: 62.50%] [G loss: 1.919956]\n",
      "epoch:12 step:12134 [D loss: 0.649068, acc: 60.16%] [G loss: 2.125872]\n",
      "epoch:12 step:12135 [D loss: 0.684154, acc: 57.81%] [G loss: 1.924647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:12136 [D loss: 0.704314, acc: 52.34%] [G loss: 1.840800]\n",
      "epoch:12 step:12137 [D loss: 0.697801, acc: 55.47%] [G loss: 1.980765]\n",
      "epoch:12 step:12138 [D loss: 0.614138, acc: 68.75%] [G loss: 1.972752]\n",
      "epoch:12 step:12139 [D loss: 0.634204, acc: 63.28%] [G loss: 1.930642]\n",
      "epoch:12 step:12140 [D loss: 0.714209, acc: 57.03%] [G loss: 1.867903]\n",
      "epoch:12 step:12141 [D loss: 0.646048, acc: 61.72%] [G loss: 1.820562]\n",
      "epoch:12 step:12142 [D loss: 0.674574, acc: 57.03%] [G loss: 1.877828]\n",
      "epoch:12 step:12143 [D loss: 0.599401, acc: 71.09%] [G loss: 2.094611]\n",
      "epoch:12 step:12144 [D loss: 0.625230, acc: 61.72%] [G loss: 1.969825]\n",
      "epoch:12 step:12145 [D loss: 0.620297, acc: 62.50%] [G loss: 1.981855]\n",
      "epoch:12 step:12146 [D loss: 0.648172, acc: 57.81%] [G loss: 1.939944]\n",
      "epoch:12 step:12147 [D loss: 0.687210, acc: 60.94%] [G loss: 2.058385]\n",
      "epoch:12 step:12148 [D loss: 0.674208, acc: 60.16%] [G loss: 1.826705]\n",
      "epoch:12 step:12149 [D loss: 0.593100, acc: 68.75%] [G loss: 2.065141]\n",
      "epoch:12 step:12150 [D loss: 0.646289, acc: 61.72%] [G loss: 2.139317]\n",
      "epoch:12 step:12151 [D loss: 0.614532, acc: 70.31%] [G loss: 2.222292]\n",
      "epoch:12 step:12152 [D loss: 0.647294, acc: 63.28%] [G loss: 1.962974]\n",
      "epoch:12 step:12153 [D loss: 0.632135, acc: 64.84%] [G loss: 2.015859]\n",
      "epoch:12 step:12154 [D loss: 0.671981, acc: 57.81%] [G loss: 1.998967]\n",
      "epoch:12 step:12155 [D loss: 0.624973, acc: 64.84%] [G loss: 2.204561]\n",
      "epoch:12 step:12156 [D loss: 0.596248, acc: 67.19%] [G loss: 2.430978]\n",
      "epoch:12 step:12157 [D loss: 0.631368, acc: 67.97%] [G loss: 2.047802]\n",
      "epoch:12 step:12158 [D loss: 0.618838, acc: 64.84%] [G loss: 2.061624]\n",
      "epoch:12 step:12159 [D loss: 0.671981, acc: 62.50%] [G loss: 1.989589]\n",
      "epoch:12 step:12160 [D loss: 0.677900, acc: 57.81%] [G loss: 2.142116]\n",
      "epoch:12 step:12161 [D loss: 0.644814, acc: 60.94%] [G loss: 2.069449]\n",
      "epoch:12 step:12162 [D loss: 0.625357, acc: 71.09%] [G loss: 1.956325]\n",
      "epoch:12 step:12163 [D loss: 0.601647, acc: 67.19%] [G loss: 2.319641]\n",
      "epoch:12 step:12164 [D loss: 0.629495, acc: 63.28%] [G loss: 1.886435]\n",
      "epoch:12 step:12165 [D loss: 0.638546, acc: 64.84%] [G loss: 2.073948]\n",
      "epoch:12 step:12166 [D loss: 0.607187, acc: 71.09%] [G loss: 2.231583]\n",
      "epoch:12 step:12167 [D loss: 0.569935, acc: 75.00%] [G loss: 2.061007]\n",
      "epoch:12 step:12168 [D loss: 0.553335, acc: 71.09%] [G loss: 2.241976]\n",
      "epoch:12 step:12169 [D loss: 0.538847, acc: 75.78%] [G loss: 2.308619]\n",
      "epoch:12 step:12170 [D loss: 0.609592, acc: 68.75%] [G loss: 2.372743]\n",
      "epoch:12 step:12171 [D loss: 0.661602, acc: 60.94%] [G loss: 2.271323]\n",
      "epoch:12 step:12172 [D loss: 0.795392, acc: 52.34%] [G loss: 1.875104]\n",
      "epoch:12 step:12173 [D loss: 0.712656, acc: 53.91%] [G loss: 1.888617]\n",
      "epoch:12 step:12174 [D loss: 0.632927, acc: 58.59%] [G loss: 2.058837]\n",
      "epoch:12 step:12175 [D loss: 0.614988, acc: 69.53%] [G loss: 2.071019]\n",
      "epoch:12 step:12176 [D loss: 0.647937, acc: 64.84%] [G loss: 1.904029]\n",
      "epoch:12 step:12177 [D loss: 0.628262, acc: 67.19%] [G loss: 1.976036]\n",
      "epoch:12 step:12178 [D loss: 0.589391, acc: 67.19%] [G loss: 2.377800]\n",
      "epoch:12 step:12179 [D loss: 0.669742, acc: 60.94%] [G loss: 1.922299]\n",
      "epoch:12 step:12180 [D loss: 0.594004, acc: 69.53%] [G loss: 2.210343]\n",
      "epoch:12 step:12181 [D loss: 0.576069, acc: 71.88%] [G loss: 2.189912]\n",
      "epoch:13 step:12182 [D loss: 0.663671, acc: 63.28%] [G loss: 1.936783]\n",
      "epoch:13 step:12183 [D loss: 0.699669, acc: 53.12%] [G loss: 2.071871]\n",
      "epoch:13 step:12184 [D loss: 0.622717, acc: 64.84%] [G loss: 1.943362]\n",
      "epoch:13 step:12185 [D loss: 0.652589, acc: 67.19%] [G loss: 1.886599]\n",
      "epoch:13 step:12186 [D loss: 0.633997, acc: 61.72%] [G loss: 2.016282]\n",
      "epoch:13 step:12187 [D loss: 0.683104, acc: 60.94%] [G loss: 1.858566]\n",
      "epoch:13 step:12188 [D loss: 0.636047, acc: 61.72%] [G loss: 2.008643]\n",
      "epoch:13 step:12189 [D loss: 0.689429, acc: 62.50%] [G loss: 1.969684]\n",
      "epoch:13 step:12190 [D loss: 0.611972, acc: 68.75%] [G loss: 1.923928]\n",
      "epoch:13 step:12191 [D loss: 0.614113, acc: 64.06%] [G loss: 1.937274]\n",
      "epoch:13 step:12192 [D loss: 0.624422, acc: 65.62%] [G loss: 1.985345]\n",
      "epoch:13 step:12193 [D loss: 0.622474, acc: 68.75%] [G loss: 1.984699]\n",
      "epoch:13 step:12194 [D loss: 0.652888, acc: 64.84%] [G loss: 1.963876]\n",
      "epoch:13 step:12195 [D loss: 0.703937, acc: 57.81%] [G loss: 1.954415]\n",
      "epoch:13 step:12196 [D loss: 0.620777, acc: 72.66%] [G loss: 2.019959]\n",
      "epoch:13 step:12197 [D loss: 0.623377, acc: 67.97%] [G loss: 1.896616]\n",
      "epoch:13 step:12198 [D loss: 0.629607, acc: 66.41%] [G loss: 2.021628]\n",
      "epoch:13 step:12199 [D loss: 0.622693, acc: 71.88%] [G loss: 1.929367]\n",
      "epoch:13 step:12200 [D loss: 0.655412, acc: 57.81%] [G loss: 1.844186]\n",
      "epoch:13 step:12201 [D loss: 0.702965, acc: 54.69%] [G loss: 1.727846]\n",
      "epoch:13 step:12202 [D loss: 0.657200, acc: 64.06%] [G loss: 1.965112]\n",
      "epoch:13 step:12203 [D loss: 0.633696, acc: 64.06%] [G loss: 1.823213]\n",
      "epoch:13 step:12204 [D loss: 0.620186, acc: 67.19%] [G loss: 2.104434]\n",
      "epoch:13 step:12205 [D loss: 0.609151, acc: 67.19%] [G loss: 2.054405]\n",
      "epoch:13 step:12206 [D loss: 0.558012, acc: 77.34%] [G loss: 2.137600]\n",
      "epoch:13 step:12207 [D loss: 0.623329, acc: 68.75%] [G loss: 1.937394]\n",
      "epoch:13 step:12208 [D loss: 0.649507, acc: 67.19%] [G loss: 1.825390]\n",
      "epoch:13 step:12209 [D loss: 0.622121, acc: 67.19%] [G loss: 2.035145]\n",
      "epoch:13 step:12210 [D loss: 0.638260, acc: 67.19%] [G loss: 1.906111]\n",
      "epoch:13 step:12211 [D loss: 0.675108, acc: 58.59%] [G loss: 1.806454]\n",
      "epoch:13 step:12212 [D loss: 0.698771, acc: 56.25%] [G loss: 1.676943]\n",
      "epoch:13 step:12213 [D loss: 0.680728, acc: 60.94%] [G loss: 1.828631]\n",
      "epoch:13 step:12214 [D loss: 0.642427, acc: 64.06%] [G loss: 1.786188]\n",
      "epoch:13 step:12215 [D loss: 0.640799, acc: 60.16%] [G loss: 1.876750]\n",
      "epoch:13 step:12216 [D loss: 0.601151, acc: 67.97%] [G loss: 1.857192]\n",
      "epoch:13 step:12217 [D loss: 0.629596, acc: 59.38%] [G loss: 2.009895]\n",
      "epoch:13 step:12218 [D loss: 0.639727, acc: 63.28%] [G loss: 2.076365]\n",
      "epoch:13 step:12219 [D loss: 0.664808, acc: 57.81%] [G loss: 1.893770]\n",
      "epoch:13 step:12220 [D loss: 0.669316, acc: 64.84%] [G loss: 1.961523]\n",
      "epoch:13 step:12221 [D loss: 0.599073, acc: 65.62%] [G loss: 2.184368]\n",
      "epoch:13 step:12222 [D loss: 0.646207, acc: 60.16%] [G loss: 1.936892]\n",
      "epoch:13 step:12223 [D loss: 0.646889, acc: 60.94%] [G loss: 2.089118]\n",
      "epoch:13 step:12224 [D loss: 0.588312, acc: 70.31%] [G loss: 1.811175]\n",
      "epoch:13 step:12225 [D loss: 0.681514, acc: 53.12%] [G loss: 1.895423]\n",
      "epoch:13 step:12226 [D loss: 0.631781, acc: 65.62%] [G loss: 2.101039]\n",
      "epoch:13 step:12227 [D loss: 0.648584, acc: 63.28%] [G loss: 1.872685]\n",
      "epoch:13 step:12228 [D loss: 0.639994, acc: 69.53%] [G loss: 2.030085]\n",
      "epoch:13 step:12229 [D loss: 0.631813, acc: 63.28%] [G loss: 1.936125]\n",
      "epoch:13 step:12230 [D loss: 0.654980, acc: 61.72%] [G loss: 2.019081]\n",
      "epoch:13 step:12231 [D loss: 0.606867, acc: 71.09%] [G loss: 1.964310]\n",
      "epoch:13 step:12232 [D loss: 0.651722, acc: 57.81%] [G loss: 2.056374]\n",
      "epoch:13 step:12233 [D loss: 0.686435, acc: 57.03%] [G loss: 1.833850]\n",
      "epoch:13 step:12234 [D loss: 0.652429, acc: 63.28%] [G loss: 1.894785]\n",
      "epoch:13 step:12235 [D loss: 0.606897, acc: 67.97%] [G loss: 2.174175]\n",
      "epoch:13 step:12236 [D loss: 0.586040, acc: 71.09%] [G loss: 2.043904]\n",
      "epoch:13 step:12237 [D loss: 0.599012, acc: 67.97%] [G loss: 1.968457]\n",
      "epoch:13 step:12238 [D loss: 0.667211, acc: 60.16%] [G loss: 1.979550]\n",
      "epoch:13 step:12239 [D loss: 0.710670, acc: 54.69%] [G loss: 2.082130]\n",
      "epoch:13 step:12240 [D loss: 0.665822, acc: 64.06%] [G loss: 1.867244]\n",
      "epoch:13 step:12241 [D loss: 0.607789, acc: 67.97%] [G loss: 1.891660]\n",
      "epoch:13 step:12242 [D loss: 0.641011, acc: 61.72%] [G loss: 1.821707]\n",
      "epoch:13 step:12243 [D loss: 0.650355, acc: 60.16%] [G loss: 2.002844]\n",
      "epoch:13 step:12244 [D loss: 0.617762, acc: 64.06%] [G loss: 2.114241]\n",
      "epoch:13 step:12245 [D loss: 0.665913, acc: 61.72%] [G loss: 1.887254]\n",
      "epoch:13 step:12246 [D loss: 0.667161, acc: 59.38%] [G loss: 2.015815]\n",
      "epoch:13 step:12247 [D loss: 0.670138, acc: 58.59%] [G loss: 1.847870]\n",
      "epoch:13 step:12248 [D loss: 0.671693, acc: 58.59%] [G loss: 1.886276]\n",
      "epoch:13 step:12249 [D loss: 0.578178, acc: 70.31%] [G loss: 2.097586]\n",
      "epoch:13 step:12250 [D loss: 0.638779, acc: 66.41%] [G loss: 2.141760]\n",
      "epoch:13 step:12251 [D loss: 0.597613, acc: 66.41%] [G loss: 2.150162]\n",
      "epoch:13 step:12252 [D loss: 0.696620, acc: 54.69%] [G loss: 1.913208]\n",
      "epoch:13 step:12253 [D loss: 0.624087, acc: 68.75%] [G loss: 1.818781]\n",
      "epoch:13 step:12254 [D loss: 0.651842, acc: 60.16%] [G loss: 1.805038]\n",
      "epoch:13 step:12255 [D loss: 0.622791, acc: 69.53%] [G loss: 2.159628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12256 [D loss: 0.632800, acc: 66.41%] [G loss: 2.187094]\n",
      "epoch:13 step:12257 [D loss: 0.666960, acc: 60.16%] [G loss: 2.043011]\n",
      "epoch:13 step:12258 [D loss: 0.594978, acc: 73.44%] [G loss: 2.095380]\n",
      "epoch:13 step:12259 [D loss: 0.669227, acc: 60.16%] [G loss: 1.845049]\n",
      "epoch:13 step:12260 [D loss: 0.675189, acc: 64.06%] [G loss: 2.058799]\n",
      "epoch:13 step:12261 [D loss: 0.645000, acc: 63.28%] [G loss: 1.724499]\n",
      "epoch:13 step:12262 [D loss: 0.692018, acc: 59.38%] [G loss: 1.828922]\n",
      "epoch:13 step:12263 [D loss: 0.613512, acc: 67.97%] [G loss: 1.851476]\n",
      "epoch:13 step:12264 [D loss: 0.584225, acc: 72.66%] [G loss: 1.980104]\n",
      "epoch:13 step:12265 [D loss: 0.638991, acc: 69.53%] [G loss: 2.044566]\n",
      "epoch:13 step:12266 [D loss: 0.677150, acc: 54.69%] [G loss: 1.681231]\n",
      "epoch:13 step:12267 [D loss: 0.634370, acc: 63.28%] [G loss: 1.858803]\n",
      "epoch:13 step:12268 [D loss: 0.702266, acc: 58.59%] [G loss: 1.936345]\n",
      "epoch:13 step:12269 [D loss: 0.603646, acc: 64.84%] [G loss: 1.987736]\n",
      "epoch:13 step:12270 [D loss: 0.619134, acc: 67.97%] [G loss: 1.961872]\n",
      "epoch:13 step:12271 [D loss: 0.614477, acc: 63.28%] [G loss: 1.992587]\n",
      "epoch:13 step:12272 [D loss: 0.613919, acc: 68.75%] [G loss: 1.970155]\n",
      "epoch:13 step:12273 [D loss: 0.633106, acc: 65.62%] [G loss: 1.974329]\n",
      "epoch:13 step:12274 [D loss: 0.591160, acc: 67.19%] [G loss: 1.964402]\n",
      "epoch:13 step:12275 [D loss: 0.630365, acc: 62.50%] [G loss: 1.945984]\n",
      "epoch:13 step:12276 [D loss: 0.669434, acc: 57.03%] [G loss: 1.858222]\n",
      "epoch:13 step:12277 [D loss: 0.617746, acc: 66.41%] [G loss: 1.985206]\n",
      "epoch:13 step:12278 [D loss: 0.590861, acc: 69.53%] [G loss: 1.958482]\n",
      "epoch:13 step:12279 [D loss: 0.674072, acc: 62.50%] [G loss: 1.892588]\n",
      "epoch:13 step:12280 [D loss: 0.688651, acc: 60.16%] [G loss: 1.821210]\n",
      "epoch:13 step:12281 [D loss: 0.598199, acc: 66.41%] [G loss: 2.001123]\n",
      "epoch:13 step:12282 [D loss: 0.647914, acc: 65.62%] [G loss: 1.881652]\n",
      "epoch:13 step:12283 [D loss: 0.613094, acc: 60.94%] [G loss: 1.893921]\n",
      "epoch:13 step:12284 [D loss: 0.639311, acc: 63.28%] [G loss: 2.068377]\n",
      "epoch:13 step:12285 [D loss: 0.617652, acc: 64.06%] [G loss: 1.955018]\n",
      "epoch:13 step:12286 [D loss: 0.648996, acc: 57.81%] [G loss: 1.991342]\n",
      "epoch:13 step:12287 [D loss: 0.662054, acc: 58.59%] [G loss: 1.919164]\n",
      "epoch:13 step:12288 [D loss: 0.622951, acc: 68.75%] [G loss: 1.922453]\n",
      "epoch:13 step:12289 [D loss: 0.690138, acc: 64.06%] [G loss: 1.745441]\n",
      "epoch:13 step:12290 [D loss: 0.652585, acc: 61.72%] [G loss: 1.956754]\n",
      "epoch:13 step:12291 [D loss: 0.667623, acc: 61.72%] [G loss: 1.898889]\n",
      "epoch:13 step:12292 [D loss: 0.721184, acc: 58.59%] [G loss: 1.930067]\n",
      "epoch:13 step:12293 [D loss: 0.610763, acc: 65.62%] [G loss: 2.033386]\n",
      "epoch:13 step:12294 [D loss: 0.577147, acc: 71.88%] [G loss: 2.068527]\n",
      "epoch:13 step:12295 [D loss: 0.682986, acc: 58.59%] [G loss: 1.858994]\n",
      "epoch:13 step:12296 [D loss: 0.636696, acc: 65.62%] [G loss: 2.047348]\n",
      "epoch:13 step:12297 [D loss: 0.589037, acc: 70.31%] [G loss: 2.211841]\n",
      "epoch:13 step:12298 [D loss: 0.614677, acc: 69.53%] [G loss: 2.356138]\n",
      "epoch:13 step:12299 [D loss: 0.641118, acc: 64.84%] [G loss: 2.058744]\n",
      "epoch:13 step:12300 [D loss: 0.540627, acc: 74.22%] [G loss: 2.230679]\n",
      "epoch:13 step:12301 [D loss: 0.631163, acc: 65.62%] [G loss: 2.136710]\n",
      "epoch:13 step:12302 [D loss: 0.668453, acc: 60.16%] [G loss: 2.049860]\n",
      "epoch:13 step:12303 [D loss: 0.670183, acc: 62.50%] [G loss: 2.175787]\n",
      "epoch:13 step:12304 [D loss: 0.649204, acc: 57.81%] [G loss: 1.995933]\n",
      "epoch:13 step:12305 [D loss: 0.666231, acc: 61.72%] [G loss: 1.932546]\n",
      "epoch:13 step:12306 [D loss: 0.667123, acc: 61.72%] [G loss: 1.868615]\n",
      "epoch:13 step:12307 [D loss: 0.629762, acc: 63.28%] [G loss: 2.021539]\n",
      "epoch:13 step:12308 [D loss: 0.623382, acc: 62.50%] [G loss: 1.843503]\n",
      "epoch:13 step:12309 [D loss: 0.658304, acc: 63.28%] [G loss: 1.971906]\n",
      "epoch:13 step:12310 [D loss: 0.640299, acc: 63.28%] [G loss: 1.873100]\n",
      "epoch:13 step:12311 [D loss: 0.628940, acc: 65.62%] [G loss: 2.051010]\n",
      "epoch:13 step:12312 [D loss: 0.598235, acc: 71.09%] [G loss: 2.116313]\n",
      "epoch:13 step:12313 [D loss: 0.638660, acc: 61.72%] [G loss: 2.112052]\n",
      "epoch:13 step:12314 [D loss: 0.682177, acc: 53.12%] [G loss: 1.988743]\n",
      "epoch:13 step:12315 [D loss: 0.675472, acc: 56.25%] [G loss: 1.783870]\n",
      "epoch:13 step:12316 [D loss: 0.692127, acc: 61.72%] [G loss: 1.913927]\n",
      "epoch:13 step:12317 [D loss: 0.690531, acc: 54.69%] [G loss: 1.770962]\n",
      "epoch:13 step:12318 [D loss: 0.646148, acc: 64.06%] [G loss: 1.838916]\n",
      "epoch:13 step:12319 [D loss: 0.662493, acc: 62.50%] [G loss: 1.859726]\n",
      "epoch:13 step:12320 [D loss: 0.666816, acc: 57.81%] [G loss: 1.936968]\n",
      "epoch:13 step:12321 [D loss: 0.687351, acc: 57.81%] [G loss: 1.852899]\n",
      "epoch:13 step:12322 [D loss: 0.696140, acc: 54.69%] [G loss: 1.895895]\n",
      "epoch:13 step:12323 [D loss: 0.614092, acc: 66.41%] [G loss: 1.979934]\n",
      "epoch:13 step:12324 [D loss: 0.683292, acc: 58.59%] [G loss: 1.938688]\n",
      "epoch:13 step:12325 [D loss: 0.638404, acc: 64.84%] [G loss: 1.970476]\n",
      "epoch:13 step:12326 [D loss: 0.592684, acc: 70.31%] [G loss: 2.006199]\n",
      "epoch:13 step:12327 [D loss: 0.639067, acc: 60.94%] [G loss: 1.964743]\n",
      "epoch:13 step:12328 [D loss: 0.639286, acc: 66.41%] [G loss: 1.856398]\n",
      "epoch:13 step:12329 [D loss: 0.655760, acc: 60.16%] [G loss: 1.855990]\n",
      "epoch:13 step:12330 [D loss: 0.676120, acc: 53.91%] [G loss: 1.885602]\n",
      "epoch:13 step:12331 [D loss: 0.698903, acc: 53.91%] [G loss: 1.838313]\n",
      "epoch:13 step:12332 [D loss: 0.618649, acc: 69.53%] [G loss: 2.160269]\n",
      "epoch:13 step:12333 [D loss: 0.622676, acc: 64.84%] [G loss: 2.028975]\n",
      "epoch:13 step:12334 [D loss: 0.674860, acc: 57.81%] [G loss: 1.863849]\n",
      "epoch:13 step:12335 [D loss: 0.615358, acc: 64.84%] [G loss: 2.215201]\n",
      "epoch:13 step:12336 [D loss: 0.601254, acc: 69.53%] [G loss: 2.113992]\n",
      "epoch:13 step:12337 [D loss: 0.555469, acc: 75.00%] [G loss: 2.032171]\n",
      "epoch:13 step:12338 [D loss: 0.618870, acc: 63.28%] [G loss: 1.946986]\n",
      "epoch:13 step:12339 [D loss: 0.650205, acc: 60.94%] [G loss: 1.952070]\n",
      "epoch:13 step:12340 [D loss: 0.623525, acc: 67.97%] [G loss: 2.112215]\n",
      "epoch:13 step:12341 [D loss: 0.721052, acc: 54.69%] [G loss: 1.775739]\n",
      "epoch:13 step:12342 [D loss: 0.645340, acc: 62.50%] [G loss: 1.934303]\n",
      "epoch:13 step:12343 [D loss: 0.621759, acc: 64.06%] [G loss: 1.895515]\n",
      "epoch:13 step:12344 [D loss: 0.619375, acc: 66.41%] [G loss: 2.034747]\n",
      "epoch:13 step:12345 [D loss: 0.627814, acc: 63.28%] [G loss: 1.754663]\n",
      "epoch:13 step:12346 [D loss: 0.672140, acc: 61.72%] [G loss: 1.901613]\n",
      "epoch:13 step:12347 [D loss: 0.611155, acc: 68.75%] [G loss: 2.040011]\n",
      "epoch:13 step:12348 [D loss: 0.672800, acc: 61.72%] [G loss: 1.928540]\n",
      "epoch:13 step:12349 [D loss: 0.620849, acc: 69.53%] [G loss: 1.980677]\n",
      "epoch:13 step:12350 [D loss: 0.672799, acc: 55.47%] [G loss: 1.766239]\n",
      "epoch:13 step:12351 [D loss: 0.637864, acc: 63.28%] [G loss: 1.911148]\n",
      "epoch:13 step:12352 [D loss: 0.601629, acc: 67.97%] [G loss: 1.971911]\n",
      "epoch:13 step:12353 [D loss: 0.646237, acc: 60.16%] [G loss: 1.917125]\n",
      "epoch:13 step:12354 [D loss: 0.643880, acc: 68.75%] [G loss: 2.027410]\n",
      "epoch:13 step:12355 [D loss: 0.688406, acc: 58.59%] [G loss: 1.912458]\n",
      "epoch:13 step:12356 [D loss: 0.645282, acc: 58.59%] [G loss: 1.790841]\n",
      "epoch:13 step:12357 [D loss: 0.672773, acc: 58.59%] [G loss: 1.878242]\n",
      "epoch:13 step:12358 [D loss: 0.667525, acc: 61.72%] [G loss: 1.882396]\n",
      "epoch:13 step:12359 [D loss: 0.637722, acc: 60.16%] [G loss: 1.970488]\n",
      "epoch:13 step:12360 [D loss: 0.646439, acc: 63.28%] [G loss: 1.810915]\n",
      "epoch:13 step:12361 [D loss: 0.641384, acc: 62.50%] [G loss: 1.947633]\n",
      "epoch:13 step:12362 [D loss: 0.645184, acc: 64.84%] [G loss: 1.919908]\n",
      "epoch:13 step:12363 [D loss: 0.664835, acc: 57.81%] [G loss: 1.815327]\n",
      "epoch:13 step:12364 [D loss: 0.659982, acc: 59.38%] [G loss: 1.832930]\n",
      "epoch:13 step:12365 [D loss: 0.593214, acc: 68.75%] [G loss: 1.858740]\n",
      "epoch:13 step:12366 [D loss: 0.603524, acc: 67.19%] [G loss: 2.119017]\n",
      "epoch:13 step:12367 [D loss: 0.744996, acc: 51.56%] [G loss: 1.825311]\n",
      "epoch:13 step:12368 [D loss: 0.649343, acc: 60.94%] [G loss: 1.897059]\n",
      "epoch:13 step:12369 [D loss: 0.671955, acc: 56.25%] [G loss: 1.920677]\n",
      "epoch:13 step:12370 [D loss: 0.675133, acc: 63.28%] [G loss: 1.874779]\n",
      "epoch:13 step:12371 [D loss: 0.599719, acc: 70.31%] [G loss: 1.919985]\n",
      "epoch:13 step:12372 [D loss: 0.710584, acc: 58.59%] [G loss: 1.888975]\n",
      "epoch:13 step:12373 [D loss: 0.635609, acc: 65.62%] [G loss: 1.963061]\n",
      "epoch:13 step:12374 [D loss: 0.638421, acc: 62.50%] [G loss: 1.872695]\n",
      "epoch:13 step:12375 [D loss: 0.607930, acc: 68.75%] [G loss: 2.199733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12376 [D loss: 0.617335, acc: 69.53%] [G loss: 1.898717]\n",
      "epoch:13 step:12377 [D loss: 0.700176, acc: 57.81%] [G loss: 1.857634]\n",
      "epoch:13 step:12378 [D loss: 0.588701, acc: 68.75%] [G loss: 1.974003]\n",
      "epoch:13 step:12379 [D loss: 0.626184, acc: 65.62%] [G loss: 1.942302]\n",
      "epoch:13 step:12380 [D loss: 0.590663, acc: 69.53%] [G loss: 2.027105]\n",
      "epoch:13 step:12381 [D loss: 0.662526, acc: 59.38%] [G loss: 1.933615]\n",
      "epoch:13 step:12382 [D loss: 0.612137, acc: 62.50%] [G loss: 1.982198]\n",
      "epoch:13 step:12383 [D loss: 0.605702, acc: 67.97%] [G loss: 1.982628]\n",
      "epoch:13 step:12384 [D loss: 0.617948, acc: 67.19%] [G loss: 1.980519]\n",
      "epoch:13 step:12385 [D loss: 0.636325, acc: 65.62%] [G loss: 1.977210]\n",
      "epoch:13 step:12386 [D loss: 0.652213, acc: 59.38%] [G loss: 1.798751]\n",
      "epoch:13 step:12387 [D loss: 0.625668, acc: 64.06%] [G loss: 2.240308]\n",
      "epoch:13 step:12388 [D loss: 0.541915, acc: 78.12%] [G loss: 2.266308]\n",
      "epoch:13 step:12389 [D loss: 0.543058, acc: 69.53%] [G loss: 2.401121]\n",
      "epoch:13 step:12390 [D loss: 0.615796, acc: 71.09%] [G loss: 2.130279]\n",
      "epoch:13 step:12391 [D loss: 0.662444, acc: 63.28%] [G loss: 2.019083]\n",
      "epoch:13 step:12392 [D loss: 0.720500, acc: 57.03%] [G loss: 1.741555]\n",
      "epoch:13 step:12393 [D loss: 0.694537, acc: 57.03%] [G loss: 1.827620]\n",
      "epoch:13 step:12394 [D loss: 0.647542, acc: 67.97%] [G loss: 1.903414]\n",
      "epoch:13 step:12395 [D loss: 0.633724, acc: 62.50%] [G loss: 1.793208]\n",
      "epoch:13 step:12396 [D loss: 0.612378, acc: 70.31%] [G loss: 1.971853]\n",
      "epoch:13 step:12397 [D loss: 0.651937, acc: 55.47%] [G loss: 1.931469]\n",
      "epoch:13 step:12398 [D loss: 0.612367, acc: 66.41%] [G loss: 1.975053]\n",
      "epoch:13 step:12399 [D loss: 0.556333, acc: 73.44%] [G loss: 2.258469]\n",
      "epoch:13 step:12400 [D loss: 0.569664, acc: 70.31%] [G loss: 2.334572]\n",
      "epoch:13 step:12401 [D loss: 0.688332, acc: 53.12%] [G loss: 1.936306]\n",
      "epoch:13 step:12402 [D loss: 0.666777, acc: 64.84%] [G loss: 2.083437]\n",
      "epoch:13 step:12403 [D loss: 0.615029, acc: 65.62%] [G loss: 1.800002]\n",
      "epoch:13 step:12404 [D loss: 0.647393, acc: 60.16%] [G loss: 2.016742]\n",
      "epoch:13 step:12405 [D loss: 0.687899, acc: 55.47%] [G loss: 2.014974]\n",
      "epoch:13 step:12406 [D loss: 0.666708, acc: 60.16%] [G loss: 1.928336]\n",
      "epoch:13 step:12407 [D loss: 0.596911, acc: 70.31%] [G loss: 1.840527]\n",
      "epoch:13 step:12408 [D loss: 0.670051, acc: 57.81%] [G loss: 1.829884]\n",
      "epoch:13 step:12409 [D loss: 0.691485, acc: 58.59%] [G loss: 1.897090]\n",
      "epoch:13 step:12410 [D loss: 0.617257, acc: 69.53%] [G loss: 2.034777]\n",
      "epoch:13 step:12411 [D loss: 0.603495, acc: 64.06%] [G loss: 2.260671]\n",
      "epoch:13 step:12412 [D loss: 0.566890, acc: 75.00%] [G loss: 2.443842]\n",
      "epoch:13 step:12413 [D loss: 0.599737, acc: 71.09%] [G loss: 2.484486]\n",
      "epoch:13 step:12414 [D loss: 0.680520, acc: 64.06%] [G loss: 1.888604]\n",
      "epoch:13 step:12415 [D loss: 0.684133, acc: 56.25%] [G loss: 1.992350]\n",
      "epoch:13 step:12416 [D loss: 0.713318, acc: 55.47%] [G loss: 1.981590]\n",
      "epoch:13 step:12417 [D loss: 0.631024, acc: 60.16%] [G loss: 1.917753]\n",
      "epoch:13 step:12418 [D loss: 0.627357, acc: 63.28%] [G loss: 1.877711]\n",
      "epoch:13 step:12419 [D loss: 0.634969, acc: 62.50%] [G loss: 1.975005]\n",
      "epoch:13 step:12420 [D loss: 0.613099, acc: 62.50%] [G loss: 2.127119]\n",
      "epoch:13 step:12421 [D loss: 0.669372, acc: 59.38%] [G loss: 1.947513]\n",
      "epoch:13 step:12422 [D loss: 0.602213, acc: 67.19%] [G loss: 2.051865]\n",
      "epoch:13 step:12423 [D loss: 0.620509, acc: 65.62%] [G loss: 1.936457]\n",
      "epoch:13 step:12424 [D loss: 0.632749, acc: 67.19%] [G loss: 1.868166]\n",
      "epoch:13 step:12425 [D loss: 0.661595, acc: 61.72%] [G loss: 1.904088]\n",
      "epoch:13 step:12426 [D loss: 0.629787, acc: 62.50%] [G loss: 1.971472]\n",
      "epoch:13 step:12427 [D loss: 0.667940, acc: 60.94%] [G loss: 1.994834]\n",
      "epoch:13 step:12428 [D loss: 0.696449, acc: 54.69%] [G loss: 1.997358]\n",
      "epoch:13 step:12429 [D loss: 0.654800, acc: 66.41%] [G loss: 1.903465]\n",
      "epoch:13 step:12430 [D loss: 0.678590, acc: 60.94%] [G loss: 1.935869]\n",
      "epoch:13 step:12431 [D loss: 0.737716, acc: 49.22%] [G loss: 1.744011]\n",
      "epoch:13 step:12432 [D loss: 0.667403, acc: 59.38%] [G loss: 1.831794]\n",
      "epoch:13 step:12433 [D loss: 0.650219, acc: 60.94%] [G loss: 1.845204]\n",
      "epoch:13 step:12434 [D loss: 0.640121, acc: 60.94%] [G loss: 1.825137]\n",
      "epoch:13 step:12435 [D loss: 0.607704, acc: 72.66%] [G loss: 1.886368]\n",
      "epoch:13 step:12436 [D loss: 0.660554, acc: 64.84%] [G loss: 1.754833]\n",
      "epoch:13 step:12437 [D loss: 0.639246, acc: 65.62%] [G loss: 1.889737]\n",
      "epoch:13 step:12438 [D loss: 0.634054, acc: 64.84%] [G loss: 1.846781]\n",
      "epoch:13 step:12439 [D loss: 0.610739, acc: 69.53%] [G loss: 1.914038]\n",
      "epoch:13 step:12440 [D loss: 0.597236, acc: 64.84%] [G loss: 1.965314]\n",
      "epoch:13 step:12441 [D loss: 0.670494, acc: 62.50%] [G loss: 1.925330]\n",
      "epoch:13 step:12442 [D loss: 0.669535, acc: 58.59%] [G loss: 1.862427]\n",
      "epoch:13 step:12443 [D loss: 0.598156, acc: 68.75%] [G loss: 1.945725]\n",
      "epoch:13 step:12444 [D loss: 0.664085, acc: 57.03%] [G loss: 1.846352]\n",
      "epoch:13 step:12445 [D loss: 0.679971, acc: 59.38%] [G loss: 2.134742]\n",
      "epoch:13 step:12446 [D loss: 0.650393, acc: 65.62%] [G loss: 1.911451]\n",
      "epoch:13 step:12447 [D loss: 0.642197, acc: 55.47%] [G loss: 1.964172]\n",
      "epoch:13 step:12448 [D loss: 0.641190, acc: 63.28%] [G loss: 1.783298]\n",
      "epoch:13 step:12449 [D loss: 0.661245, acc: 60.94%] [G loss: 1.982971]\n",
      "epoch:13 step:12450 [D loss: 0.680815, acc: 58.59%] [G loss: 1.886598]\n",
      "epoch:13 step:12451 [D loss: 0.651199, acc: 64.06%] [G loss: 1.938690]\n",
      "epoch:13 step:12452 [D loss: 0.654979, acc: 60.94%] [G loss: 1.808978]\n",
      "epoch:13 step:12453 [D loss: 0.661119, acc: 60.94%] [G loss: 2.083588]\n",
      "epoch:13 step:12454 [D loss: 0.640456, acc: 63.28%] [G loss: 1.973807]\n",
      "epoch:13 step:12455 [D loss: 0.575950, acc: 69.53%] [G loss: 2.075448]\n",
      "epoch:13 step:12456 [D loss: 0.651435, acc: 66.41%] [G loss: 1.964805]\n",
      "epoch:13 step:12457 [D loss: 0.580814, acc: 72.66%] [G loss: 2.195454]\n",
      "epoch:13 step:12458 [D loss: 0.687161, acc: 57.03%] [G loss: 1.937803]\n",
      "epoch:13 step:12459 [D loss: 0.658815, acc: 53.91%] [G loss: 1.928954]\n",
      "epoch:13 step:12460 [D loss: 0.630849, acc: 63.28%] [G loss: 1.941737]\n",
      "epoch:13 step:12461 [D loss: 0.642316, acc: 64.06%] [G loss: 2.140746]\n",
      "epoch:13 step:12462 [D loss: 0.659811, acc: 60.94%] [G loss: 1.845574]\n",
      "epoch:13 step:12463 [D loss: 0.689435, acc: 63.28%] [G loss: 1.904803]\n",
      "epoch:13 step:12464 [D loss: 0.599805, acc: 69.53%] [G loss: 1.892405]\n",
      "epoch:13 step:12465 [D loss: 0.632900, acc: 62.50%] [G loss: 1.959147]\n",
      "epoch:13 step:12466 [D loss: 0.634740, acc: 60.94%] [G loss: 1.766896]\n",
      "epoch:13 step:12467 [D loss: 0.589492, acc: 74.22%] [G loss: 2.041098]\n",
      "epoch:13 step:12468 [D loss: 0.583452, acc: 64.06%] [G loss: 1.853152]\n",
      "epoch:13 step:12469 [D loss: 0.657127, acc: 61.72%] [G loss: 1.906952]\n",
      "epoch:13 step:12470 [D loss: 0.624397, acc: 63.28%] [G loss: 1.901888]\n",
      "epoch:13 step:12471 [D loss: 0.615102, acc: 70.31%] [G loss: 1.891592]\n",
      "epoch:13 step:12472 [D loss: 0.631011, acc: 64.84%] [G loss: 1.923146]\n",
      "epoch:13 step:12473 [D loss: 0.591302, acc: 67.97%] [G loss: 1.999467]\n",
      "epoch:13 step:12474 [D loss: 0.573147, acc: 72.66%] [G loss: 2.141688]\n",
      "epoch:13 step:12475 [D loss: 0.638327, acc: 68.75%] [G loss: 2.047354]\n",
      "epoch:13 step:12476 [D loss: 0.639515, acc: 66.41%] [G loss: 1.815560]\n",
      "epoch:13 step:12477 [D loss: 0.611642, acc: 68.75%] [G loss: 2.043333]\n",
      "epoch:13 step:12478 [D loss: 0.685124, acc: 59.38%] [G loss: 1.806170]\n",
      "epoch:13 step:12479 [D loss: 0.626422, acc: 67.19%] [G loss: 2.004541]\n",
      "epoch:13 step:12480 [D loss: 0.652462, acc: 63.28%] [G loss: 1.921564]\n",
      "epoch:13 step:12481 [D loss: 0.602575, acc: 67.97%] [G loss: 2.199399]\n",
      "epoch:13 step:12482 [D loss: 0.647899, acc: 65.62%] [G loss: 1.952492]\n",
      "epoch:13 step:12483 [D loss: 0.636570, acc: 62.50%] [G loss: 2.067934]\n",
      "epoch:13 step:12484 [D loss: 0.621874, acc: 66.41%] [G loss: 2.008505]\n",
      "epoch:13 step:12485 [D loss: 0.620984, acc: 60.94%] [G loss: 1.936408]\n",
      "epoch:13 step:12486 [D loss: 0.703537, acc: 58.59%] [G loss: 2.013258]\n",
      "epoch:13 step:12487 [D loss: 0.663354, acc: 59.38%] [G loss: 1.890712]\n",
      "epoch:13 step:12488 [D loss: 0.654195, acc: 60.16%] [G loss: 1.802548]\n",
      "epoch:13 step:12489 [D loss: 0.710917, acc: 57.81%] [G loss: 1.784840]\n",
      "epoch:13 step:12490 [D loss: 0.630031, acc: 62.50%] [G loss: 1.907192]\n",
      "epoch:13 step:12491 [D loss: 0.636991, acc: 65.62%] [G loss: 1.776364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12492 [D loss: 0.621046, acc: 61.72%] [G loss: 1.985970]\n",
      "epoch:13 step:12493 [D loss: 0.642746, acc: 65.62%] [G loss: 2.202931]\n",
      "epoch:13 step:12494 [D loss: 0.567658, acc: 68.75%] [G loss: 2.232950]\n",
      "epoch:13 step:12495 [D loss: 0.508180, acc: 78.12%] [G loss: 2.174184]\n",
      "epoch:13 step:12496 [D loss: 0.608912, acc: 65.62%] [G loss: 2.301462]\n",
      "epoch:13 step:12497 [D loss: 0.676377, acc: 54.69%] [G loss: 1.910273]\n",
      "epoch:13 step:12498 [D loss: 0.672681, acc: 57.81%] [G loss: 1.917200]\n",
      "epoch:13 step:12499 [D loss: 0.650436, acc: 62.50%] [G loss: 2.212291]\n",
      "epoch:13 step:12500 [D loss: 0.611957, acc: 67.19%] [G loss: 1.999135]\n",
      "epoch:13 step:12501 [D loss: 0.622169, acc: 64.06%] [G loss: 1.929883]\n",
      "epoch:13 step:12502 [D loss: 0.672008, acc: 61.72%] [G loss: 2.089348]\n",
      "epoch:13 step:12503 [D loss: 0.631189, acc: 59.38%] [G loss: 1.912315]\n",
      "epoch:13 step:12504 [D loss: 0.670782, acc: 58.59%] [G loss: 1.887530]\n",
      "epoch:13 step:12505 [D loss: 0.693796, acc: 61.72%] [G loss: 1.943053]\n",
      "epoch:13 step:12506 [D loss: 0.654444, acc: 59.38%] [G loss: 1.878686]\n",
      "epoch:13 step:12507 [D loss: 0.637112, acc: 61.72%] [G loss: 1.849217]\n",
      "epoch:13 step:12508 [D loss: 0.661008, acc: 53.91%] [G loss: 1.912044]\n",
      "epoch:13 step:12509 [D loss: 0.688533, acc: 56.25%] [G loss: 1.860653]\n",
      "epoch:13 step:12510 [D loss: 0.614441, acc: 68.75%] [G loss: 2.021121]\n",
      "epoch:13 step:12511 [D loss: 0.620966, acc: 74.22%] [G loss: 2.094564]\n",
      "epoch:13 step:12512 [D loss: 0.638478, acc: 67.19%] [G loss: 2.048769]\n",
      "epoch:13 step:12513 [D loss: 0.585691, acc: 66.41%] [G loss: 2.115269]\n",
      "epoch:13 step:12514 [D loss: 0.606261, acc: 71.88%] [G loss: 2.012114]\n",
      "epoch:13 step:12515 [D loss: 0.622925, acc: 63.28%] [G loss: 1.933969]\n",
      "epoch:13 step:12516 [D loss: 0.607291, acc: 62.50%] [G loss: 2.108702]\n",
      "epoch:13 step:12517 [D loss: 0.619697, acc: 63.28%] [G loss: 1.988091]\n",
      "epoch:13 step:12518 [D loss: 0.642145, acc: 61.72%] [G loss: 2.017193]\n",
      "epoch:13 step:12519 [D loss: 0.646835, acc: 63.28%] [G loss: 2.035190]\n",
      "epoch:13 step:12520 [D loss: 0.601631, acc: 69.53%] [G loss: 1.933453]\n",
      "epoch:13 step:12521 [D loss: 0.605858, acc: 70.31%] [G loss: 2.006523]\n",
      "epoch:13 step:12522 [D loss: 0.732851, acc: 57.03%] [G loss: 1.884930]\n",
      "epoch:13 step:12523 [D loss: 0.681536, acc: 55.47%] [G loss: 1.853985]\n",
      "epoch:13 step:12524 [D loss: 0.622830, acc: 65.62%] [G loss: 2.040133]\n",
      "epoch:13 step:12525 [D loss: 0.614430, acc: 62.50%] [G loss: 1.823248]\n",
      "epoch:13 step:12526 [D loss: 0.580183, acc: 67.19%] [G loss: 2.260485]\n",
      "epoch:13 step:12527 [D loss: 0.602622, acc: 67.97%] [G loss: 2.309338]\n",
      "epoch:13 step:12528 [D loss: 0.609093, acc: 67.19%] [G loss: 2.190618]\n",
      "epoch:13 step:12529 [D loss: 0.672291, acc: 64.84%] [G loss: 1.830387]\n",
      "epoch:13 step:12530 [D loss: 0.673643, acc: 58.59%] [G loss: 1.697182]\n",
      "epoch:13 step:12531 [D loss: 0.589227, acc: 69.53%] [G loss: 2.086436]\n",
      "epoch:13 step:12532 [D loss: 0.667143, acc: 64.84%] [G loss: 1.968641]\n",
      "epoch:13 step:12533 [D loss: 0.650797, acc: 60.94%] [G loss: 2.028043]\n",
      "epoch:13 step:12534 [D loss: 0.645240, acc: 62.50%] [G loss: 2.047885]\n",
      "epoch:13 step:12535 [D loss: 0.630097, acc: 67.97%] [G loss: 2.077287]\n",
      "epoch:13 step:12536 [D loss: 0.720180, acc: 53.91%] [G loss: 1.979544]\n",
      "epoch:13 step:12537 [D loss: 0.692425, acc: 60.16%] [G loss: 1.817120]\n",
      "epoch:13 step:12538 [D loss: 0.645220, acc: 63.28%] [G loss: 2.030868]\n",
      "epoch:13 step:12539 [D loss: 0.597845, acc: 68.75%] [G loss: 2.151486]\n",
      "epoch:13 step:12540 [D loss: 0.621699, acc: 64.84%] [G loss: 2.074859]\n",
      "epoch:13 step:12541 [D loss: 0.664648, acc: 59.38%] [G loss: 1.931224]\n",
      "epoch:13 step:12542 [D loss: 0.673590, acc: 57.81%] [G loss: 1.918929]\n",
      "epoch:13 step:12543 [D loss: 0.652252, acc: 60.16%] [G loss: 1.903647]\n",
      "epoch:13 step:12544 [D loss: 0.660589, acc: 62.50%] [G loss: 1.857559]\n",
      "epoch:13 step:12545 [D loss: 0.615832, acc: 70.31%] [G loss: 2.005184]\n",
      "epoch:13 step:12546 [D loss: 0.633028, acc: 65.62%] [G loss: 1.931021]\n",
      "epoch:13 step:12547 [D loss: 0.628835, acc: 65.62%] [G loss: 2.123797]\n",
      "epoch:13 step:12548 [D loss: 0.645095, acc: 59.38%] [G loss: 2.001247]\n",
      "epoch:13 step:12549 [D loss: 0.665398, acc: 61.72%] [G loss: 2.036214]\n",
      "epoch:13 step:12550 [D loss: 0.605433, acc: 67.97%] [G loss: 2.018992]\n",
      "epoch:13 step:12551 [D loss: 0.618395, acc: 67.19%] [G loss: 2.263216]\n",
      "epoch:13 step:12552 [D loss: 0.592720, acc: 67.19%] [G loss: 2.187021]\n",
      "epoch:13 step:12553 [D loss: 0.568259, acc: 71.09%] [G loss: 2.040727]\n",
      "epoch:13 step:12554 [D loss: 0.704631, acc: 57.81%] [G loss: 1.821131]\n",
      "epoch:13 step:12555 [D loss: 0.645335, acc: 65.62%] [G loss: 1.999004]\n",
      "epoch:13 step:12556 [D loss: 0.673445, acc: 60.16%] [G loss: 1.890086]\n",
      "epoch:13 step:12557 [D loss: 0.656067, acc: 64.06%] [G loss: 1.904089]\n",
      "epoch:13 step:12558 [D loss: 0.697062, acc: 51.56%] [G loss: 1.745104]\n",
      "epoch:13 step:12559 [D loss: 0.708596, acc: 57.03%] [G loss: 1.972493]\n",
      "epoch:13 step:12560 [D loss: 0.602133, acc: 68.75%] [G loss: 1.869687]\n",
      "epoch:13 step:12561 [D loss: 0.634036, acc: 62.50%] [G loss: 1.963451]\n",
      "epoch:13 step:12562 [D loss: 0.591098, acc: 71.88%] [G loss: 2.064321]\n",
      "epoch:13 step:12563 [D loss: 0.679469, acc: 60.16%] [G loss: 1.915538]\n",
      "epoch:13 step:12564 [D loss: 0.657673, acc: 59.38%] [G loss: 1.923279]\n",
      "epoch:13 step:12565 [D loss: 0.639009, acc: 62.50%] [G loss: 2.203457]\n",
      "epoch:13 step:12566 [D loss: 0.603315, acc: 65.62%] [G loss: 2.148882]\n",
      "epoch:13 step:12567 [D loss: 0.652409, acc: 60.94%] [G loss: 1.919995]\n",
      "epoch:13 step:12568 [D loss: 0.690982, acc: 55.47%] [G loss: 1.623657]\n",
      "epoch:13 step:12569 [D loss: 0.661646, acc: 58.59%] [G loss: 1.881779]\n",
      "epoch:13 step:12570 [D loss: 0.617903, acc: 65.62%] [G loss: 1.988925]\n",
      "epoch:13 step:12571 [D loss: 0.621177, acc: 68.75%] [G loss: 1.840559]\n",
      "epoch:13 step:12572 [D loss: 0.683593, acc: 60.94%] [G loss: 1.746693]\n",
      "epoch:13 step:12573 [D loss: 0.649833, acc: 63.28%] [G loss: 2.017354]\n",
      "epoch:13 step:12574 [D loss: 0.648198, acc: 60.16%] [G loss: 1.948525]\n",
      "epoch:13 step:12575 [D loss: 0.613248, acc: 65.62%] [G loss: 1.831424]\n",
      "epoch:13 step:12576 [D loss: 0.620163, acc: 65.62%] [G loss: 1.879634]\n",
      "epoch:13 step:12577 [D loss: 0.692137, acc: 53.91%] [G loss: 1.826059]\n",
      "epoch:13 step:12578 [D loss: 0.679931, acc: 59.38%] [G loss: 1.859513]\n",
      "epoch:13 step:12579 [D loss: 0.612699, acc: 69.53%] [G loss: 1.955780]\n",
      "epoch:13 step:12580 [D loss: 0.619301, acc: 64.84%] [G loss: 2.031612]\n",
      "epoch:13 step:12581 [D loss: 0.619715, acc: 64.06%] [G loss: 1.981982]\n",
      "epoch:13 step:12582 [D loss: 0.660966, acc: 58.59%] [G loss: 1.901739]\n",
      "epoch:13 step:12583 [D loss: 0.660807, acc: 64.84%] [G loss: 1.986600]\n",
      "epoch:13 step:12584 [D loss: 0.624661, acc: 62.50%] [G loss: 2.045184]\n",
      "epoch:13 step:12585 [D loss: 0.717005, acc: 56.25%] [G loss: 1.965857]\n",
      "epoch:13 step:12586 [D loss: 0.609946, acc: 64.06%] [G loss: 2.124632]\n",
      "epoch:13 step:12587 [D loss: 0.601791, acc: 75.00%] [G loss: 2.158436]\n",
      "epoch:13 step:12588 [D loss: 0.635751, acc: 60.16%] [G loss: 2.038367]\n",
      "epoch:13 step:12589 [D loss: 0.696353, acc: 57.81%] [G loss: 1.789057]\n",
      "epoch:13 step:12590 [D loss: 0.601772, acc: 68.75%] [G loss: 2.089227]\n",
      "epoch:13 step:12591 [D loss: 0.626981, acc: 62.50%] [G loss: 1.877015]\n",
      "epoch:13 step:12592 [D loss: 0.644779, acc: 65.62%] [G loss: 1.819846]\n",
      "epoch:13 step:12593 [D loss: 0.672687, acc: 63.28%] [G loss: 1.866658]\n",
      "epoch:13 step:12594 [D loss: 0.584278, acc: 72.66%] [G loss: 2.091393]\n",
      "epoch:13 step:12595 [D loss: 0.697625, acc: 56.25%] [G loss: 2.004721]\n",
      "epoch:13 step:12596 [D loss: 0.595248, acc: 70.31%] [G loss: 2.037757]\n",
      "epoch:13 step:12597 [D loss: 0.681083, acc: 58.59%] [G loss: 1.977542]\n",
      "epoch:13 step:12598 [D loss: 0.623631, acc: 67.97%] [G loss: 1.870430]\n",
      "epoch:13 step:12599 [D loss: 0.720955, acc: 53.91%] [G loss: 1.788765]\n",
      "epoch:13 step:12600 [D loss: 0.668491, acc: 60.94%] [G loss: 1.965895]\n",
      "epoch:13 step:12601 [D loss: 0.613223, acc: 68.75%] [G loss: 1.928434]\n",
      "epoch:13 step:12602 [D loss: 0.665945, acc: 56.25%] [G loss: 1.920222]\n",
      "epoch:13 step:12603 [D loss: 0.607875, acc: 65.62%] [G loss: 2.003252]\n",
      "epoch:13 step:12604 [D loss: 0.674746, acc: 61.72%] [G loss: 1.939084]\n",
      "epoch:13 step:12605 [D loss: 0.625783, acc: 70.31%] [G loss: 1.953203]\n",
      "epoch:13 step:12606 [D loss: 0.643272, acc: 59.38%] [G loss: 2.019893]\n",
      "epoch:13 step:12607 [D loss: 0.704026, acc: 57.03%] [G loss: 1.979479]\n",
      "epoch:13 step:12608 [D loss: 0.596517, acc: 69.53%] [G loss: 1.987192]\n",
      "epoch:13 step:12609 [D loss: 0.610647, acc: 71.88%] [G loss: 2.177107]\n",
      "epoch:13 step:12610 [D loss: 0.592367, acc: 69.53%] [G loss: 2.275734]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12611 [D loss: 0.609661, acc: 68.75%] [G loss: 2.290652]\n",
      "epoch:13 step:12612 [D loss: 0.625329, acc: 68.75%] [G loss: 2.048732]\n",
      "epoch:13 step:12613 [D loss: 0.670443, acc: 59.38%] [G loss: 2.013296]\n",
      "epoch:13 step:12614 [D loss: 0.671657, acc: 59.38%] [G loss: 1.866058]\n",
      "epoch:13 step:12615 [D loss: 0.594905, acc: 64.06%] [G loss: 1.924670]\n",
      "epoch:13 step:12616 [D loss: 0.617985, acc: 62.50%] [G loss: 2.011996]\n",
      "epoch:13 step:12617 [D loss: 0.614688, acc: 65.62%] [G loss: 2.063544]\n",
      "epoch:13 step:12618 [D loss: 0.754904, acc: 49.22%] [G loss: 1.704770]\n",
      "epoch:13 step:12619 [D loss: 0.736909, acc: 53.12%] [G loss: 1.711072]\n",
      "epoch:13 step:12620 [D loss: 0.702848, acc: 57.03%] [G loss: 1.800983]\n",
      "epoch:13 step:12621 [D loss: 0.675963, acc: 53.91%] [G loss: 1.816249]\n",
      "epoch:13 step:12622 [D loss: 0.657894, acc: 60.94%] [G loss: 1.743605]\n",
      "epoch:13 step:12623 [D loss: 0.653981, acc: 64.06%] [G loss: 1.823536]\n",
      "epoch:13 step:12624 [D loss: 0.702338, acc: 54.69%] [G loss: 1.986635]\n",
      "epoch:13 step:12625 [D loss: 0.596303, acc: 69.53%] [G loss: 1.917572]\n",
      "epoch:13 step:12626 [D loss: 0.656846, acc: 63.28%] [G loss: 1.926495]\n",
      "epoch:13 step:12627 [D loss: 0.659843, acc: 60.16%] [G loss: 1.777522]\n",
      "epoch:13 step:12628 [D loss: 0.687739, acc: 61.72%] [G loss: 1.947010]\n",
      "epoch:13 step:12629 [D loss: 0.698111, acc: 59.38%] [G loss: 1.912391]\n",
      "epoch:13 step:12630 [D loss: 0.654167, acc: 64.84%] [G loss: 1.851161]\n",
      "epoch:13 step:12631 [D loss: 0.614779, acc: 71.88%] [G loss: 1.879220]\n",
      "epoch:13 step:12632 [D loss: 0.645567, acc: 62.50%] [G loss: 2.000032]\n",
      "epoch:13 step:12633 [D loss: 0.660758, acc: 64.84%] [G loss: 1.719924]\n",
      "epoch:13 step:12634 [D loss: 0.617329, acc: 67.19%] [G loss: 1.761625]\n",
      "epoch:13 step:12635 [D loss: 0.679607, acc: 61.72%] [G loss: 1.994765]\n",
      "epoch:13 step:12636 [D loss: 0.682628, acc: 60.16%] [G loss: 1.952578]\n",
      "epoch:13 step:12637 [D loss: 0.618167, acc: 57.81%] [G loss: 1.892078]\n",
      "epoch:13 step:12638 [D loss: 0.615010, acc: 64.06%] [G loss: 1.912071]\n",
      "epoch:13 step:12639 [D loss: 0.620352, acc: 60.94%] [G loss: 1.919238]\n",
      "epoch:13 step:12640 [D loss: 0.663936, acc: 57.81%] [G loss: 1.689354]\n",
      "epoch:13 step:12641 [D loss: 0.695172, acc: 56.25%] [G loss: 1.833287]\n",
      "epoch:13 step:12642 [D loss: 0.630267, acc: 65.62%] [G loss: 1.972972]\n",
      "epoch:13 step:12643 [D loss: 0.662377, acc: 55.47%] [G loss: 1.855158]\n",
      "epoch:13 step:12644 [D loss: 0.597533, acc: 68.75%] [G loss: 2.049291]\n",
      "epoch:13 step:12645 [D loss: 0.688780, acc: 50.78%] [G loss: 1.757898]\n",
      "epoch:13 step:12646 [D loss: 0.686028, acc: 61.72%] [G loss: 1.835647]\n",
      "epoch:13 step:12647 [D loss: 0.626662, acc: 66.41%] [G loss: 1.947528]\n",
      "epoch:13 step:12648 [D loss: 0.650134, acc: 57.81%] [G loss: 1.960382]\n",
      "epoch:13 step:12649 [D loss: 0.640970, acc: 63.28%] [G loss: 1.938356]\n",
      "epoch:13 step:12650 [D loss: 0.596650, acc: 70.31%] [G loss: 2.127925]\n",
      "epoch:13 step:12651 [D loss: 0.655518, acc: 64.84%] [G loss: 2.227795]\n",
      "epoch:13 step:12652 [D loss: 0.557086, acc: 70.31%] [G loss: 2.080431]\n",
      "epoch:13 step:12653 [D loss: 0.605213, acc: 63.28%] [G loss: 2.253986]\n",
      "epoch:13 step:12654 [D loss: 0.687812, acc: 57.81%] [G loss: 1.999003]\n",
      "epoch:13 step:12655 [D loss: 0.629737, acc: 61.72%] [G loss: 2.025666]\n",
      "epoch:13 step:12656 [D loss: 0.668567, acc: 57.81%] [G loss: 1.965351]\n",
      "epoch:13 step:12657 [D loss: 0.687113, acc: 59.38%] [G loss: 1.979202]\n",
      "epoch:13 step:12658 [D loss: 0.694663, acc: 50.78%] [G loss: 1.824509]\n",
      "epoch:13 step:12659 [D loss: 0.691348, acc: 55.47%] [G loss: 1.863647]\n",
      "epoch:13 step:12660 [D loss: 0.580811, acc: 71.88%] [G loss: 1.981571]\n",
      "epoch:13 step:12661 [D loss: 0.619045, acc: 65.62%] [G loss: 2.011481]\n",
      "epoch:13 step:12662 [D loss: 0.602348, acc: 67.97%] [G loss: 2.184452]\n",
      "epoch:13 step:12663 [D loss: 0.686662, acc: 57.03%] [G loss: 1.752830]\n",
      "epoch:13 step:12664 [D loss: 0.624026, acc: 65.62%] [G loss: 1.959091]\n",
      "epoch:13 step:12665 [D loss: 0.622635, acc: 62.50%] [G loss: 2.175109]\n",
      "epoch:13 step:12666 [D loss: 0.703543, acc: 53.91%] [G loss: 1.920763]\n",
      "epoch:13 step:12667 [D loss: 0.633098, acc: 64.84%] [G loss: 1.929759]\n",
      "epoch:13 step:12668 [D loss: 0.643550, acc: 67.19%] [G loss: 1.853829]\n",
      "epoch:13 step:12669 [D loss: 0.603621, acc: 67.19%] [G loss: 2.124247]\n",
      "epoch:13 step:12670 [D loss: 0.641041, acc: 62.50%] [G loss: 1.856699]\n",
      "epoch:13 step:12671 [D loss: 0.618907, acc: 71.09%] [G loss: 1.985719]\n",
      "epoch:13 step:12672 [D loss: 0.627420, acc: 63.28%] [G loss: 2.019998]\n",
      "epoch:13 step:12673 [D loss: 0.574206, acc: 74.22%] [G loss: 1.862238]\n",
      "epoch:13 step:12674 [D loss: 0.626788, acc: 67.97%] [G loss: 1.972020]\n",
      "epoch:13 step:12675 [D loss: 0.603327, acc: 68.75%] [G loss: 2.024909]\n",
      "epoch:13 step:12676 [D loss: 0.612688, acc: 66.41%] [G loss: 2.105232]\n",
      "epoch:13 step:12677 [D loss: 0.661606, acc: 61.72%] [G loss: 2.024956]\n",
      "epoch:13 step:12678 [D loss: 0.652455, acc: 64.06%] [G loss: 2.040762]\n",
      "epoch:13 step:12679 [D loss: 0.604971, acc: 64.84%] [G loss: 2.030782]\n",
      "epoch:13 step:12680 [D loss: 0.540632, acc: 75.78%] [G loss: 2.026062]\n",
      "epoch:13 step:12681 [D loss: 0.694136, acc: 53.91%] [G loss: 1.763598]\n",
      "epoch:13 step:12682 [D loss: 0.694899, acc: 54.69%] [G loss: 1.853649]\n",
      "epoch:13 step:12683 [D loss: 0.696319, acc: 52.34%] [G loss: 1.824918]\n",
      "epoch:13 step:12684 [D loss: 0.583082, acc: 70.31%] [G loss: 1.958724]\n",
      "epoch:13 step:12685 [D loss: 0.644117, acc: 66.41%] [G loss: 2.282471]\n",
      "epoch:13 step:12686 [D loss: 0.624217, acc: 63.28%] [G loss: 2.109380]\n",
      "epoch:13 step:12687 [D loss: 0.668158, acc: 61.72%] [G loss: 1.976357]\n",
      "epoch:13 step:12688 [D loss: 0.654589, acc: 63.28%] [G loss: 2.015676]\n",
      "epoch:13 step:12689 [D loss: 0.660338, acc: 63.28%] [G loss: 2.083899]\n",
      "epoch:13 step:12690 [D loss: 0.735026, acc: 51.56%] [G loss: 1.948532]\n",
      "epoch:13 step:12691 [D loss: 0.682269, acc: 57.81%] [G loss: 1.800660]\n",
      "epoch:13 step:12692 [D loss: 0.700624, acc: 54.69%] [G loss: 1.804511]\n",
      "epoch:13 step:12693 [D loss: 0.669319, acc: 64.06%] [G loss: 1.837866]\n",
      "epoch:13 step:12694 [D loss: 0.646303, acc: 67.19%] [G loss: 1.951195]\n",
      "epoch:13 step:12695 [D loss: 0.654070, acc: 59.38%] [G loss: 1.905025]\n",
      "epoch:13 step:12696 [D loss: 0.626192, acc: 64.84%] [G loss: 1.820095]\n",
      "epoch:13 step:12697 [D loss: 0.607385, acc: 65.62%] [G loss: 2.012619]\n",
      "epoch:13 step:12698 [D loss: 0.642038, acc: 65.62%] [G loss: 2.091064]\n",
      "epoch:13 step:12699 [D loss: 0.588493, acc: 71.88%] [G loss: 1.821286]\n",
      "epoch:13 step:12700 [D loss: 0.660577, acc: 58.59%] [G loss: 1.948371]\n",
      "epoch:13 step:12701 [D loss: 0.641614, acc: 64.06%] [G loss: 1.948943]\n",
      "epoch:13 step:12702 [D loss: 0.643804, acc: 64.84%] [G loss: 1.936929]\n",
      "epoch:13 step:12703 [D loss: 0.592441, acc: 70.31%] [G loss: 2.015516]\n",
      "epoch:13 step:12704 [D loss: 0.539943, acc: 78.12%] [G loss: 2.349500]\n",
      "epoch:13 step:12705 [D loss: 0.602731, acc: 71.88%] [G loss: 1.861394]\n",
      "epoch:13 step:12706 [D loss: 0.656354, acc: 63.28%] [G loss: 1.959611]\n",
      "epoch:13 step:12707 [D loss: 0.662735, acc: 61.72%] [G loss: 1.831195]\n",
      "epoch:13 step:12708 [D loss: 0.639944, acc: 63.28%] [G loss: 1.974025]\n",
      "epoch:13 step:12709 [D loss: 0.705054, acc: 58.59%] [G loss: 1.793615]\n",
      "epoch:13 step:12710 [D loss: 0.639386, acc: 60.16%] [G loss: 1.895910]\n",
      "epoch:13 step:12711 [D loss: 0.641758, acc: 60.94%] [G loss: 1.833386]\n",
      "epoch:13 step:12712 [D loss: 0.634917, acc: 63.28%] [G loss: 1.833668]\n",
      "epoch:13 step:12713 [D loss: 0.631091, acc: 62.50%] [G loss: 2.059738]\n",
      "epoch:13 step:12714 [D loss: 0.604429, acc: 66.41%] [G loss: 1.882553]\n",
      "epoch:13 step:12715 [D loss: 0.577117, acc: 71.88%] [G loss: 2.063719]\n",
      "epoch:13 step:12716 [D loss: 0.675992, acc: 63.28%] [G loss: 1.997411]\n",
      "epoch:13 step:12717 [D loss: 0.632999, acc: 67.97%] [G loss: 2.035423]\n",
      "epoch:13 step:12718 [D loss: 0.644232, acc: 59.38%] [G loss: 1.953280]\n",
      "epoch:13 step:12719 [D loss: 0.674071, acc: 53.91%] [G loss: 1.853896]\n",
      "epoch:13 step:12720 [D loss: 0.596966, acc: 70.31%] [G loss: 1.980750]\n",
      "epoch:13 step:12721 [D loss: 0.697539, acc: 51.56%] [G loss: 1.948789]\n",
      "epoch:13 step:12722 [D loss: 0.599873, acc: 69.53%] [G loss: 2.000058]\n",
      "epoch:13 step:12723 [D loss: 0.679044, acc: 58.59%] [G loss: 1.920411]\n",
      "epoch:13 step:12724 [D loss: 0.682733, acc: 60.94%] [G loss: 1.942752]\n",
      "epoch:13 step:12725 [D loss: 0.636041, acc: 58.59%] [G loss: 1.967682]\n",
      "epoch:13 step:12726 [D loss: 0.616434, acc: 62.50%] [G loss: 2.014680]\n",
      "epoch:13 step:12727 [D loss: 0.634971, acc: 60.94%] [G loss: 1.915784]\n",
      "epoch:13 step:12728 [D loss: 0.583318, acc: 68.75%] [G loss: 2.105772]\n",
      "epoch:13 step:12729 [D loss: 0.648172, acc: 65.62%] [G loss: 1.938830]\n",
      "epoch:13 step:12730 [D loss: 0.614116, acc: 67.97%] [G loss: 2.120809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12731 [D loss: 0.651519, acc: 60.16%] [G loss: 2.174923]\n",
      "epoch:13 step:12732 [D loss: 0.603333, acc: 64.84%] [G loss: 2.070332]\n",
      "epoch:13 step:12733 [D loss: 0.592046, acc: 67.97%] [G loss: 2.101145]\n",
      "epoch:13 step:12734 [D loss: 0.674311, acc: 66.41%] [G loss: 1.915763]\n",
      "epoch:13 step:12735 [D loss: 0.664342, acc: 61.72%] [G loss: 2.128804]\n",
      "epoch:13 step:12736 [D loss: 0.631605, acc: 64.84%] [G loss: 1.847646]\n",
      "epoch:13 step:12737 [D loss: 0.596886, acc: 67.97%] [G loss: 2.149031]\n",
      "epoch:13 step:12738 [D loss: 0.617013, acc: 71.09%] [G loss: 2.099745]\n",
      "epoch:13 step:12739 [D loss: 0.604638, acc: 68.75%] [G loss: 2.155468]\n",
      "epoch:13 step:12740 [D loss: 0.652777, acc: 63.28%] [G loss: 1.940499]\n",
      "epoch:13 step:12741 [D loss: 0.655096, acc: 65.62%] [G loss: 1.935242]\n",
      "epoch:13 step:12742 [D loss: 0.613436, acc: 64.06%] [G loss: 2.060556]\n",
      "epoch:13 step:12743 [D loss: 0.677041, acc: 61.72%] [G loss: 1.984319]\n",
      "epoch:13 step:12744 [D loss: 0.629655, acc: 58.59%] [G loss: 1.973731]\n",
      "epoch:13 step:12745 [D loss: 0.581947, acc: 68.75%] [G loss: 2.246058]\n",
      "epoch:13 step:12746 [D loss: 0.676468, acc: 60.16%] [G loss: 1.806089]\n",
      "epoch:13 step:12747 [D loss: 0.727364, acc: 54.69%] [G loss: 1.797104]\n",
      "epoch:13 step:12748 [D loss: 0.611497, acc: 65.62%] [G loss: 1.985018]\n",
      "epoch:13 step:12749 [D loss: 0.640480, acc: 64.84%] [G loss: 1.889635]\n",
      "epoch:13 step:12750 [D loss: 0.621063, acc: 67.19%] [G loss: 1.881764]\n",
      "epoch:13 step:12751 [D loss: 0.668104, acc: 60.94%] [G loss: 1.972183]\n",
      "epoch:13 step:12752 [D loss: 0.602895, acc: 75.78%] [G loss: 2.063332]\n",
      "epoch:13 step:12753 [D loss: 0.689122, acc: 59.38%] [G loss: 1.864517]\n",
      "epoch:13 step:12754 [D loss: 0.651732, acc: 60.94%] [G loss: 2.036014]\n",
      "epoch:13 step:12755 [D loss: 0.600447, acc: 72.66%] [G loss: 2.095500]\n",
      "epoch:13 step:12756 [D loss: 0.652214, acc: 62.50%] [G loss: 1.905792]\n",
      "epoch:13 step:12757 [D loss: 0.637119, acc: 64.06%] [G loss: 1.895771]\n",
      "epoch:13 step:12758 [D loss: 0.690360, acc: 57.81%] [G loss: 1.896458]\n",
      "epoch:13 step:12759 [D loss: 0.664212, acc: 60.16%] [G loss: 1.962965]\n",
      "epoch:13 step:12760 [D loss: 0.684188, acc: 52.34%] [G loss: 1.836000]\n",
      "epoch:13 step:12761 [D loss: 0.632059, acc: 67.97%] [G loss: 1.954620]\n",
      "epoch:13 step:12762 [D loss: 0.608401, acc: 64.84%] [G loss: 1.932731]\n",
      "epoch:13 step:12763 [D loss: 0.623653, acc: 59.38%] [G loss: 1.937036]\n",
      "epoch:13 step:12764 [D loss: 0.693582, acc: 57.81%] [G loss: 2.010077]\n",
      "epoch:13 step:12765 [D loss: 0.702144, acc: 55.47%] [G loss: 1.763645]\n",
      "epoch:13 step:12766 [D loss: 0.663989, acc: 64.06%] [G loss: 1.923709]\n",
      "epoch:13 step:12767 [D loss: 0.660670, acc: 58.59%] [G loss: 1.793850]\n",
      "epoch:13 step:12768 [D loss: 0.665167, acc: 64.84%] [G loss: 1.928337]\n",
      "epoch:13 step:12769 [D loss: 0.627835, acc: 67.19%] [G loss: 1.970518]\n",
      "epoch:13 step:12770 [D loss: 0.658319, acc: 62.50%] [G loss: 1.965822]\n",
      "epoch:13 step:12771 [D loss: 0.676748, acc: 54.69%] [G loss: 1.848776]\n",
      "epoch:13 step:12772 [D loss: 0.614924, acc: 62.50%] [G loss: 1.918951]\n",
      "epoch:13 step:12773 [D loss: 0.679194, acc: 57.81%] [G loss: 1.997838]\n",
      "epoch:13 step:12774 [D loss: 0.604903, acc: 67.97%] [G loss: 1.778928]\n",
      "epoch:13 step:12775 [D loss: 0.640218, acc: 66.41%] [G loss: 1.979476]\n",
      "epoch:13 step:12776 [D loss: 0.602249, acc: 67.19%] [G loss: 1.976071]\n",
      "epoch:13 step:12777 [D loss: 0.712970, acc: 53.91%] [G loss: 1.965849]\n",
      "epoch:13 step:12778 [D loss: 0.613260, acc: 60.94%] [G loss: 1.789639]\n",
      "epoch:13 step:12779 [D loss: 0.645165, acc: 61.72%] [G loss: 1.995050]\n",
      "epoch:13 step:12780 [D loss: 0.624305, acc: 64.84%] [G loss: 1.730229]\n",
      "epoch:13 step:12781 [D loss: 0.648582, acc: 64.84%] [G loss: 2.131062]\n",
      "epoch:13 step:12782 [D loss: 0.706591, acc: 62.50%] [G loss: 1.936749]\n",
      "epoch:13 step:12783 [D loss: 0.653593, acc: 57.03%] [G loss: 2.096986]\n",
      "epoch:13 step:12784 [D loss: 0.604458, acc: 69.53%] [G loss: 1.950485]\n",
      "epoch:13 step:12785 [D loss: 0.651512, acc: 60.16%] [G loss: 2.001925]\n",
      "epoch:13 step:12786 [D loss: 0.644017, acc: 60.16%] [G loss: 1.975155]\n",
      "epoch:13 step:12787 [D loss: 0.653912, acc: 63.28%] [G loss: 1.883814]\n",
      "epoch:13 step:12788 [D loss: 0.668892, acc: 62.50%] [G loss: 1.908763]\n",
      "epoch:13 step:12789 [D loss: 0.650888, acc: 59.38%] [G loss: 1.944814]\n",
      "epoch:13 step:12790 [D loss: 0.663801, acc: 56.25%] [G loss: 2.043260]\n",
      "epoch:13 step:12791 [D loss: 0.640327, acc: 61.72%] [G loss: 1.845976]\n",
      "epoch:13 step:12792 [D loss: 0.649891, acc: 61.72%] [G loss: 1.785955]\n",
      "epoch:13 step:12793 [D loss: 0.665700, acc: 59.38%] [G loss: 1.750405]\n",
      "epoch:13 step:12794 [D loss: 0.678541, acc: 57.03%] [G loss: 1.855948]\n",
      "epoch:13 step:12795 [D loss: 0.679118, acc: 60.16%] [G loss: 1.711732]\n",
      "epoch:13 step:12796 [D loss: 0.661480, acc: 57.03%] [G loss: 1.829040]\n",
      "epoch:13 step:12797 [D loss: 0.670064, acc: 58.59%] [G loss: 1.974955]\n",
      "epoch:13 step:12798 [D loss: 0.638605, acc: 62.50%] [G loss: 1.866714]\n",
      "epoch:13 step:12799 [D loss: 0.618031, acc: 71.09%] [G loss: 1.963557]\n",
      "epoch:13 step:12800 [D loss: 0.686883, acc: 64.06%] [G loss: 1.686625]\n",
      "epoch:13 step:12801 [D loss: 0.614356, acc: 66.41%] [G loss: 1.856473]\n",
      "epoch:13 step:12802 [D loss: 0.638621, acc: 62.50%] [G loss: 1.921067]\n",
      "epoch:13 step:12803 [D loss: 0.638836, acc: 56.25%] [G loss: 1.964696]\n",
      "epoch:13 step:12804 [D loss: 0.633063, acc: 67.97%] [G loss: 1.974059]\n",
      "epoch:13 step:12805 [D loss: 0.639338, acc: 67.97%] [G loss: 1.921686]\n",
      "epoch:13 step:12806 [D loss: 0.650218, acc: 60.16%] [G loss: 1.911025]\n",
      "epoch:13 step:12807 [D loss: 0.670666, acc: 57.81%] [G loss: 1.890083]\n",
      "epoch:13 step:12808 [D loss: 0.653258, acc: 64.84%] [G loss: 1.880593]\n",
      "epoch:13 step:12809 [D loss: 0.611344, acc: 64.84%] [G loss: 1.920886]\n",
      "epoch:13 step:12810 [D loss: 0.635514, acc: 61.72%] [G loss: 2.000673]\n",
      "epoch:13 step:12811 [D loss: 0.598294, acc: 65.62%] [G loss: 1.957977]\n",
      "epoch:13 step:12812 [D loss: 0.606823, acc: 71.09%] [G loss: 1.903744]\n",
      "epoch:13 step:12813 [D loss: 0.591919, acc: 67.97%] [G loss: 2.116090]\n",
      "epoch:13 step:12814 [D loss: 0.592094, acc: 67.19%] [G loss: 1.966625]\n",
      "epoch:13 step:12815 [D loss: 0.599731, acc: 67.19%] [G loss: 2.146263]\n",
      "epoch:13 step:12816 [D loss: 0.606155, acc: 64.84%] [G loss: 1.955110]\n",
      "epoch:13 step:12817 [D loss: 0.591847, acc: 67.19%] [G loss: 2.058510]\n",
      "epoch:13 step:12818 [D loss: 0.649075, acc: 61.72%] [G loss: 2.176009]\n",
      "epoch:13 step:12819 [D loss: 0.615496, acc: 65.62%] [G loss: 2.092198]\n",
      "epoch:13 step:12820 [D loss: 0.662887, acc: 59.38%] [G loss: 2.193753]\n",
      "epoch:13 step:12821 [D loss: 0.678363, acc: 56.25%] [G loss: 2.030795]\n",
      "epoch:13 step:12822 [D loss: 0.625741, acc: 68.75%] [G loss: 1.769668]\n",
      "epoch:13 step:12823 [D loss: 0.572552, acc: 74.22%] [G loss: 2.154911]\n",
      "epoch:13 step:12824 [D loss: 0.675039, acc: 58.59%] [G loss: 1.960563]\n",
      "epoch:13 step:12825 [D loss: 0.587416, acc: 66.41%] [G loss: 2.223354]\n",
      "epoch:13 step:12826 [D loss: 0.608119, acc: 67.19%] [G loss: 2.177839]\n",
      "epoch:13 step:12827 [D loss: 0.637379, acc: 60.94%] [G loss: 2.143037]\n",
      "epoch:13 step:12828 [D loss: 0.596679, acc: 70.31%] [G loss: 2.310766]\n",
      "epoch:13 step:12829 [D loss: 0.593636, acc: 67.19%] [G loss: 2.461781]\n",
      "epoch:13 step:12830 [D loss: 0.600184, acc: 70.31%] [G loss: 2.123105]\n",
      "epoch:13 step:12831 [D loss: 0.678073, acc: 62.50%] [G loss: 2.239860]\n",
      "epoch:13 step:12832 [D loss: 0.610089, acc: 69.53%] [G loss: 2.004293]\n",
      "epoch:13 step:12833 [D loss: 0.667139, acc: 58.59%] [G loss: 1.837234]\n",
      "epoch:13 step:12834 [D loss: 0.622515, acc: 68.75%] [G loss: 1.990197]\n",
      "epoch:13 step:12835 [D loss: 0.599677, acc: 69.53%] [G loss: 1.998167]\n",
      "epoch:13 step:12836 [D loss: 0.660689, acc: 57.81%] [G loss: 1.937524]\n",
      "epoch:13 step:12837 [D loss: 0.578132, acc: 71.88%] [G loss: 1.907231]\n",
      "epoch:13 step:12838 [D loss: 0.654063, acc: 61.72%] [G loss: 2.024192]\n",
      "epoch:13 step:12839 [D loss: 0.651802, acc: 60.94%] [G loss: 1.758560]\n",
      "epoch:13 step:12840 [D loss: 0.681029, acc: 62.50%] [G loss: 1.935158]\n",
      "epoch:13 step:12841 [D loss: 0.592205, acc: 65.62%] [G loss: 2.092219]\n",
      "epoch:13 step:12842 [D loss: 0.604780, acc: 68.75%] [G loss: 1.869720]\n",
      "epoch:13 step:12843 [D loss: 0.648488, acc: 67.19%] [G loss: 1.970569]\n",
      "epoch:13 step:12844 [D loss: 0.608042, acc: 65.62%] [G loss: 1.875834]\n",
      "epoch:13 step:12845 [D loss: 0.644452, acc: 62.50%] [G loss: 1.987919]\n",
      "epoch:13 step:12846 [D loss: 0.669775, acc: 57.81%] [G loss: 1.906039]\n",
      "epoch:13 step:12847 [D loss: 0.647489, acc: 60.94%] [G loss: 1.868815]\n",
      "epoch:13 step:12848 [D loss: 0.676862, acc: 56.25%] [G loss: 1.903211]\n",
      "epoch:13 step:12849 [D loss: 0.710893, acc: 53.91%] [G loss: 1.832738]\n",
      "epoch:13 step:12850 [D loss: 0.677521, acc: 53.91%] [G loss: 1.869874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12851 [D loss: 0.640133, acc: 62.50%] [G loss: 1.979146]\n",
      "epoch:13 step:12852 [D loss: 0.618178, acc: 67.19%] [G loss: 1.824774]\n",
      "epoch:13 step:12853 [D loss: 0.638838, acc: 63.28%] [G loss: 1.939132]\n",
      "epoch:13 step:12854 [D loss: 0.663967, acc: 62.50%] [G loss: 1.820187]\n",
      "epoch:13 step:12855 [D loss: 0.682369, acc: 53.91%] [G loss: 1.928069]\n",
      "epoch:13 step:12856 [D loss: 0.608774, acc: 69.53%] [G loss: 1.850087]\n",
      "epoch:13 step:12857 [D loss: 0.669260, acc: 61.72%] [G loss: 1.895620]\n",
      "epoch:13 step:12858 [D loss: 0.634183, acc: 69.53%] [G loss: 1.961343]\n",
      "epoch:13 step:12859 [D loss: 0.631253, acc: 64.06%] [G loss: 2.001413]\n",
      "epoch:13 step:12860 [D loss: 0.631885, acc: 63.28%] [G loss: 1.965771]\n",
      "epoch:13 step:12861 [D loss: 0.645736, acc: 64.84%] [G loss: 2.015266]\n",
      "epoch:13 step:12862 [D loss: 0.609882, acc: 70.31%] [G loss: 1.995267]\n",
      "epoch:13 step:12863 [D loss: 0.645817, acc: 64.06%] [G loss: 1.994966]\n",
      "epoch:13 step:12864 [D loss: 0.608706, acc: 67.19%] [G loss: 1.952033]\n",
      "epoch:13 step:12865 [D loss: 0.616718, acc: 63.28%] [G loss: 1.969911]\n",
      "epoch:13 step:12866 [D loss: 0.639184, acc: 63.28%] [G loss: 1.957477]\n",
      "epoch:13 step:12867 [D loss: 0.632206, acc: 69.53%] [G loss: 1.970831]\n",
      "epoch:13 step:12868 [D loss: 0.628300, acc: 67.97%] [G loss: 1.901222]\n",
      "epoch:13 step:12869 [D loss: 0.657007, acc: 62.50%] [G loss: 2.045319]\n",
      "epoch:13 step:12870 [D loss: 0.638468, acc: 64.06%] [G loss: 1.946018]\n",
      "epoch:13 step:12871 [D loss: 0.629861, acc: 67.97%] [G loss: 2.006540]\n",
      "epoch:13 step:12872 [D loss: 0.624428, acc: 64.06%] [G loss: 2.106139]\n",
      "epoch:13 step:12873 [D loss: 0.684940, acc: 58.59%] [G loss: 2.086063]\n",
      "epoch:13 step:12874 [D loss: 0.596685, acc: 65.62%] [G loss: 2.148692]\n",
      "epoch:13 step:12875 [D loss: 0.586331, acc: 68.75%] [G loss: 2.169665]\n",
      "epoch:13 step:12876 [D loss: 0.575544, acc: 68.75%] [G loss: 2.095980]\n",
      "epoch:13 step:12877 [D loss: 0.714766, acc: 62.50%] [G loss: 1.875854]\n",
      "epoch:13 step:12878 [D loss: 0.627391, acc: 65.62%] [G loss: 1.984888]\n",
      "epoch:13 step:12879 [D loss: 0.642876, acc: 64.06%] [G loss: 1.991189]\n",
      "epoch:13 step:12880 [D loss: 0.616653, acc: 67.97%] [G loss: 1.940309]\n",
      "epoch:13 step:12881 [D loss: 0.642334, acc: 63.28%] [G loss: 1.988313]\n",
      "epoch:13 step:12882 [D loss: 0.674670, acc: 56.25%] [G loss: 1.933536]\n",
      "epoch:13 step:12883 [D loss: 0.649461, acc: 59.38%] [G loss: 2.027862]\n",
      "epoch:13 step:12884 [D loss: 0.673466, acc: 64.84%] [G loss: 1.955364]\n",
      "epoch:13 step:12885 [D loss: 0.694766, acc: 57.81%] [G loss: 1.836806]\n",
      "epoch:13 step:12886 [D loss: 0.672938, acc: 60.16%] [G loss: 1.988849]\n",
      "epoch:13 step:12887 [D loss: 0.657532, acc: 63.28%] [G loss: 2.069469]\n",
      "epoch:13 step:12888 [D loss: 0.633173, acc: 63.28%] [G loss: 2.026652]\n",
      "epoch:13 step:12889 [D loss: 0.622997, acc: 68.75%] [G loss: 2.149702]\n",
      "epoch:13 step:12890 [D loss: 0.622408, acc: 71.88%] [G loss: 2.099912]\n",
      "epoch:13 step:12891 [D loss: 0.676287, acc: 62.50%] [G loss: 1.914487]\n",
      "epoch:13 step:12892 [D loss: 0.618030, acc: 60.16%] [G loss: 1.888010]\n",
      "epoch:13 step:12893 [D loss: 0.580127, acc: 72.66%] [G loss: 1.952109]\n",
      "epoch:13 step:12894 [D loss: 0.676364, acc: 64.84%] [G loss: 1.784761]\n",
      "epoch:13 step:12895 [D loss: 0.625267, acc: 59.38%] [G loss: 1.955848]\n",
      "epoch:13 step:12896 [D loss: 0.640667, acc: 64.06%] [G loss: 1.908904]\n",
      "epoch:13 step:12897 [D loss: 0.725308, acc: 53.12%] [G loss: 1.860547]\n",
      "epoch:13 step:12898 [D loss: 0.658152, acc: 57.81%] [G loss: 1.875834]\n",
      "epoch:13 step:12899 [D loss: 0.660601, acc: 59.38%] [G loss: 1.824483]\n",
      "epoch:13 step:12900 [D loss: 0.589258, acc: 69.53%] [G loss: 2.193507]\n",
      "epoch:13 step:12901 [D loss: 0.633540, acc: 61.72%] [G loss: 1.941165]\n",
      "epoch:13 step:12902 [D loss: 0.659020, acc: 62.50%] [G loss: 1.976471]\n",
      "epoch:13 step:12903 [D loss: 0.663039, acc: 65.62%] [G loss: 1.729181]\n",
      "epoch:13 step:12904 [D loss: 0.657284, acc: 60.16%] [G loss: 1.868033]\n",
      "epoch:13 step:12905 [D loss: 0.658490, acc: 58.59%] [G loss: 1.968170]\n",
      "epoch:13 step:12906 [D loss: 0.662954, acc: 57.81%] [G loss: 1.876179]\n",
      "epoch:13 step:12907 [D loss: 0.649502, acc: 61.72%] [G loss: 1.962356]\n",
      "epoch:13 step:12908 [D loss: 0.679762, acc: 58.59%] [G loss: 1.859519]\n",
      "epoch:13 step:12909 [D loss: 0.631220, acc: 66.41%] [G loss: 1.882052]\n",
      "epoch:13 step:12910 [D loss: 0.659010, acc: 56.25%] [G loss: 1.943884]\n",
      "epoch:13 step:12911 [D loss: 0.587519, acc: 72.66%] [G loss: 1.958134]\n",
      "epoch:13 step:12912 [D loss: 0.609854, acc: 62.50%] [G loss: 2.050470]\n",
      "epoch:13 step:12913 [D loss: 0.638284, acc: 60.94%] [G loss: 2.022743]\n",
      "epoch:13 step:12914 [D loss: 0.564011, acc: 75.00%] [G loss: 2.095764]\n",
      "epoch:13 step:12915 [D loss: 0.645395, acc: 64.06%] [G loss: 1.886343]\n",
      "epoch:13 step:12916 [D loss: 0.651986, acc: 64.84%] [G loss: 1.900729]\n",
      "epoch:13 step:12917 [D loss: 0.657429, acc: 59.38%] [G loss: 2.052822]\n",
      "epoch:13 step:12918 [D loss: 0.647925, acc: 62.50%] [G loss: 1.918698]\n",
      "epoch:13 step:12919 [D loss: 0.670858, acc: 58.59%] [G loss: 2.001855]\n",
      "epoch:13 step:12920 [D loss: 0.688699, acc: 57.81%] [G loss: 1.887492]\n",
      "epoch:13 step:12921 [D loss: 0.647900, acc: 58.59%] [G loss: 1.918308]\n",
      "epoch:13 step:12922 [D loss: 0.631974, acc: 66.41%] [G loss: 1.785455]\n",
      "epoch:13 step:12923 [D loss: 0.689562, acc: 53.12%] [G loss: 1.843998]\n",
      "epoch:13 step:12924 [D loss: 0.682640, acc: 61.72%] [G loss: 1.980848]\n",
      "epoch:13 step:12925 [D loss: 0.623383, acc: 66.41%] [G loss: 1.883551]\n",
      "epoch:13 step:12926 [D loss: 0.634922, acc: 64.84%] [G loss: 1.791185]\n",
      "epoch:13 step:12927 [D loss: 0.654917, acc: 57.81%] [G loss: 1.962828]\n",
      "epoch:13 step:12928 [D loss: 0.641952, acc: 59.38%] [G loss: 1.959411]\n",
      "epoch:13 step:12929 [D loss: 0.705445, acc: 62.50%] [G loss: 1.814943]\n",
      "epoch:13 step:12930 [D loss: 0.664653, acc: 55.47%] [G loss: 1.855770]\n",
      "epoch:13 step:12931 [D loss: 0.650662, acc: 62.50%] [G loss: 1.868625]\n",
      "epoch:13 step:12932 [D loss: 0.661342, acc: 57.81%] [G loss: 1.829303]\n",
      "epoch:13 step:12933 [D loss: 0.678713, acc: 58.59%] [G loss: 1.809508]\n",
      "epoch:13 step:12934 [D loss: 0.628666, acc: 60.94%] [G loss: 1.894703]\n",
      "epoch:13 step:12935 [D loss: 0.626317, acc: 64.06%] [G loss: 1.904621]\n",
      "epoch:13 step:12936 [D loss: 0.642729, acc: 65.62%] [G loss: 1.881032]\n",
      "epoch:13 step:12937 [D loss: 0.662392, acc: 61.72%] [G loss: 1.959599]\n",
      "epoch:13 step:12938 [D loss: 0.631610, acc: 64.84%] [G loss: 1.909838]\n",
      "epoch:13 step:12939 [D loss: 0.652069, acc: 60.16%] [G loss: 1.819460]\n",
      "epoch:13 step:12940 [D loss: 0.620294, acc: 67.19%] [G loss: 1.819865]\n",
      "epoch:13 step:12941 [D loss: 0.617979, acc: 65.62%] [G loss: 1.872878]\n",
      "epoch:13 step:12942 [D loss: 0.605120, acc: 60.94%] [G loss: 1.867952]\n",
      "epoch:13 step:12943 [D loss: 0.598270, acc: 67.19%] [G loss: 2.052766]\n",
      "epoch:13 step:12944 [D loss: 0.674914, acc: 60.16%] [G loss: 2.001731]\n",
      "epoch:13 step:12945 [D loss: 0.620662, acc: 64.06%] [G loss: 2.069554]\n",
      "epoch:13 step:12946 [D loss: 0.679597, acc: 62.50%] [G loss: 1.862420]\n",
      "epoch:13 step:12947 [D loss: 0.680638, acc: 62.50%] [G loss: 1.805015]\n",
      "epoch:13 step:12948 [D loss: 0.600302, acc: 69.53%] [G loss: 2.115476]\n",
      "epoch:13 step:12949 [D loss: 0.686299, acc: 55.47%] [G loss: 1.937207]\n",
      "epoch:13 step:12950 [D loss: 0.623262, acc: 65.62%] [G loss: 1.951758]\n",
      "epoch:13 step:12951 [D loss: 0.617422, acc: 64.84%] [G loss: 2.120230]\n",
      "epoch:13 step:12952 [D loss: 0.668363, acc: 57.03%] [G loss: 1.976513]\n",
      "epoch:13 step:12953 [D loss: 0.622342, acc: 68.75%] [G loss: 1.899253]\n",
      "epoch:13 step:12954 [D loss: 0.584692, acc: 67.19%] [G loss: 2.047967]\n",
      "epoch:13 step:12955 [D loss: 0.588215, acc: 71.88%] [G loss: 2.199174]\n",
      "epoch:13 step:12956 [D loss: 0.615629, acc: 69.53%] [G loss: 2.202853]\n",
      "epoch:13 step:12957 [D loss: 0.621987, acc: 67.19%] [G loss: 2.103250]\n",
      "epoch:13 step:12958 [D loss: 0.583633, acc: 65.62%] [G loss: 2.166221]\n",
      "epoch:13 step:12959 [D loss: 0.687201, acc: 57.03%] [G loss: 1.878912]\n",
      "epoch:13 step:12960 [D loss: 0.628806, acc: 66.41%] [G loss: 1.844048]\n",
      "epoch:13 step:12961 [D loss: 0.603162, acc: 62.50%] [G loss: 2.091469]\n",
      "epoch:13 step:12962 [D loss: 0.593825, acc: 66.41%] [G loss: 2.302316]\n",
      "epoch:13 step:12963 [D loss: 0.586879, acc: 68.75%] [G loss: 2.248769]\n",
      "epoch:13 step:12964 [D loss: 0.653177, acc: 57.81%] [G loss: 2.012686]\n",
      "epoch:13 step:12965 [D loss: 0.677052, acc: 60.16%] [G loss: 1.959079]\n",
      "epoch:13 step:12966 [D loss: 0.629202, acc: 66.41%] [G loss: 2.026986]\n",
      "epoch:13 step:12967 [D loss: 0.604286, acc: 71.88%] [G loss: 1.998419]\n",
      "epoch:13 step:12968 [D loss: 0.687586, acc: 57.03%] [G loss: 1.902061]\n",
      "epoch:13 step:12969 [D loss: 0.638103, acc: 59.38%] [G loss: 1.930303]\n",
      "epoch:13 step:12970 [D loss: 0.661021, acc: 62.50%] [G loss: 2.078123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:12971 [D loss: 0.613516, acc: 69.53%] [G loss: 1.909432]\n",
      "epoch:13 step:12972 [D loss: 0.629430, acc: 65.62%] [G loss: 1.989680]\n",
      "epoch:13 step:12973 [D loss: 0.636895, acc: 66.41%] [G loss: 2.179439]\n",
      "epoch:13 step:12974 [D loss: 0.620923, acc: 67.97%] [G loss: 1.981051]\n",
      "epoch:13 step:12975 [D loss: 0.687018, acc: 54.69%] [G loss: 1.848067]\n",
      "epoch:13 step:12976 [D loss: 0.655326, acc: 60.94%] [G loss: 1.805304]\n",
      "epoch:13 step:12977 [D loss: 0.615498, acc: 65.62%] [G loss: 1.877539]\n",
      "epoch:13 step:12978 [D loss: 0.628994, acc: 60.94%] [G loss: 1.946660]\n",
      "epoch:13 step:12979 [D loss: 0.623042, acc: 62.50%] [G loss: 1.950183]\n",
      "epoch:13 step:12980 [D loss: 0.618002, acc: 64.84%] [G loss: 2.061637]\n",
      "epoch:13 step:12981 [D loss: 0.696222, acc: 56.25%] [G loss: 1.894854]\n",
      "epoch:13 step:12982 [D loss: 0.645143, acc: 62.50%] [G loss: 1.726623]\n",
      "epoch:13 step:12983 [D loss: 0.636331, acc: 62.50%] [G loss: 1.952443]\n",
      "epoch:13 step:12984 [D loss: 0.595260, acc: 65.62%] [G loss: 2.019879]\n",
      "epoch:13 step:12985 [D loss: 0.658024, acc: 57.03%] [G loss: 1.971544]\n",
      "epoch:13 step:12986 [D loss: 0.590572, acc: 70.31%] [G loss: 1.936325]\n",
      "epoch:13 step:12987 [D loss: 0.643476, acc: 64.84%] [G loss: 2.117396]\n",
      "epoch:13 step:12988 [D loss: 0.613956, acc: 64.06%] [G loss: 1.908329]\n",
      "epoch:13 step:12989 [D loss: 0.668255, acc: 57.03%] [G loss: 1.967732]\n",
      "epoch:13 step:12990 [D loss: 0.613361, acc: 66.41%] [G loss: 1.868495]\n",
      "epoch:13 step:12991 [D loss: 0.660452, acc: 64.84%] [G loss: 1.927384]\n",
      "epoch:13 step:12992 [D loss: 0.622283, acc: 64.84%] [G loss: 1.915136]\n",
      "epoch:13 step:12993 [D loss: 0.688066, acc: 53.12%] [G loss: 1.880091]\n",
      "epoch:13 step:12994 [D loss: 0.635203, acc: 69.53%] [G loss: 2.059966]\n",
      "epoch:13 step:12995 [D loss: 0.649423, acc: 65.62%] [G loss: 1.935901]\n",
      "epoch:13 step:12996 [D loss: 0.637946, acc: 66.41%] [G loss: 2.212097]\n",
      "epoch:13 step:12997 [D loss: 0.630381, acc: 68.75%] [G loss: 1.985390]\n",
      "epoch:13 step:12998 [D loss: 0.655821, acc: 59.38%] [G loss: 1.751095]\n",
      "epoch:13 step:12999 [D loss: 0.652173, acc: 62.50%] [G loss: 1.835675]\n",
      "epoch:13 step:13000 [D loss: 0.628213, acc: 63.28%] [G loss: 1.936849]\n",
      "epoch:13 step:13001 [D loss: 0.683661, acc: 51.56%] [G loss: 1.800678]\n",
      "epoch:13 step:13002 [D loss: 0.627690, acc: 66.41%] [G loss: 1.965022]\n",
      "epoch:13 step:13003 [D loss: 0.661861, acc: 60.94%] [G loss: 2.027959]\n",
      "epoch:13 step:13004 [D loss: 0.579790, acc: 71.09%] [G loss: 2.068454]\n",
      "epoch:13 step:13005 [D loss: 0.716116, acc: 54.69%] [G loss: 1.778014]\n",
      "epoch:13 step:13006 [D loss: 0.611408, acc: 66.41%] [G loss: 2.059641]\n",
      "epoch:13 step:13007 [D loss: 0.651593, acc: 62.50%] [G loss: 1.761089]\n",
      "epoch:13 step:13008 [D loss: 0.668124, acc: 63.28%] [G loss: 1.913896]\n",
      "epoch:13 step:13009 [D loss: 0.703629, acc: 57.81%] [G loss: 1.791278]\n",
      "epoch:13 step:13010 [D loss: 0.675642, acc: 59.38%] [G loss: 1.816036]\n",
      "epoch:13 step:13011 [D loss: 0.622317, acc: 67.97%] [G loss: 1.901499]\n",
      "epoch:13 step:13012 [D loss: 0.664536, acc: 60.94%] [G loss: 1.884825]\n",
      "epoch:13 step:13013 [D loss: 0.623015, acc: 69.53%] [G loss: 1.879597]\n",
      "epoch:13 step:13014 [D loss: 0.557225, acc: 76.56%] [G loss: 2.019838]\n",
      "epoch:13 step:13015 [D loss: 0.627971, acc: 66.41%] [G loss: 1.918272]\n",
      "epoch:13 step:13016 [D loss: 0.606059, acc: 67.97%] [G loss: 1.960246]\n",
      "epoch:13 step:13017 [D loss: 0.683233, acc: 57.03%] [G loss: 1.960360]\n",
      "epoch:13 step:13018 [D loss: 0.616775, acc: 65.62%] [G loss: 2.056201]\n",
      "epoch:13 step:13019 [D loss: 0.631288, acc: 65.62%] [G loss: 1.911286]\n",
      "epoch:13 step:13020 [D loss: 0.627192, acc: 65.62%] [G loss: 1.945931]\n",
      "epoch:13 step:13021 [D loss: 0.676796, acc: 60.16%] [G loss: 1.899313]\n",
      "epoch:13 step:13022 [D loss: 0.552961, acc: 74.22%] [G loss: 2.066639]\n",
      "epoch:13 step:13023 [D loss: 0.654724, acc: 60.94%] [G loss: 2.075899]\n",
      "epoch:13 step:13024 [D loss: 0.692068, acc: 62.50%] [G loss: 2.072339]\n",
      "epoch:13 step:13025 [D loss: 0.655240, acc: 57.81%] [G loss: 1.953319]\n",
      "epoch:13 step:13026 [D loss: 0.629444, acc: 67.19%] [G loss: 2.119252]\n",
      "epoch:13 step:13027 [D loss: 0.601817, acc: 68.75%] [G loss: 1.918979]\n",
      "epoch:13 step:13028 [D loss: 0.604540, acc: 70.31%] [G loss: 2.074540]\n",
      "epoch:13 step:13029 [D loss: 0.638613, acc: 66.41%] [G loss: 2.004248]\n",
      "epoch:13 step:13030 [D loss: 0.676334, acc: 65.62%] [G loss: 2.017181]\n",
      "epoch:13 step:13031 [D loss: 0.597286, acc: 63.28%] [G loss: 2.014158]\n",
      "epoch:13 step:13032 [D loss: 0.629068, acc: 66.41%] [G loss: 2.184109]\n",
      "epoch:13 step:13033 [D loss: 0.708810, acc: 47.66%] [G loss: 1.933902]\n",
      "epoch:13 step:13034 [D loss: 0.628769, acc: 60.94%] [G loss: 1.941785]\n",
      "epoch:13 step:13035 [D loss: 0.653510, acc: 64.06%] [G loss: 1.798609]\n",
      "epoch:13 step:13036 [D loss: 0.696121, acc: 53.91%] [G loss: 1.773168]\n",
      "epoch:13 step:13037 [D loss: 0.658027, acc: 62.50%] [G loss: 1.789599]\n",
      "epoch:13 step:13038 [D loss: 0.677899, acc: 57.81%] [G loss: 1.917645]\n",
      "epoch:13 step:13039 [D loss: 0.706123, acc: 57.81%] [G loss: 1.731732]\n",
      "epoch:13 step:13040 [D loss: 0.630426, acc: 64.06%] [G loss: 2.015211]\n",
      "epoch:13 step:13041 [D loss: 0.611667, acc: 68.75%] [G loss: 1.823317]\n",
      "epoch:13 step:13042 [D loss: 0.648210, acc: 59.38%] [G loss: 1.922588]\n",
      "epoch:13 step:13043 [D loss: 0.686648, acc: 58.59%] [G loss: 1.806267]\n",
      "epoch:13 step:13044 [D loss: 0.681174, acc: 58.59%] [G loss: 1.866698]\n",
      "epoch:13 step:13045 [D loss: 0.632165, acc: 64.84%] [G loss: 2.040468]\n",
      "epoch:13 step:13046 [D loss: 0.665723, acc: 57.03%] [G loss: 1.796036]\n",
      "epoch:13 step:13047 [D loss: 0.647938, acc: 64.06%] [G loss: 1.909098]\n",
      "epoch:13 step:13048 [D loss: 0.666542, acc: 56.25%] [G loss: 1.877950]\n",
      "epoch:13 step:13049 [D loss: 0.666298, acc: 60.94%] [G loss: 1.804735]\n",
      "epoch:13 step:13050 [D loss: 0.674910, acc: 56.25%] [G loss: 1.790092]\n",
      "epoch:13 step:13051 [D loss: 0.659048, acc: 58.59%] [G loss: 1.683179]\n",
      "epoch:13 step:13052 [D loss: 0.605887, acc: 67.19%] [G loss: 1.905554]\n",
      "epoch:13 step:13053 [D loss: 0.604869, acc: 67.97%] [G loss: 1.794081]\n",
      "epoch:13 step:13054 [D loss: 0.669242, acc: 60.94%] [G loss: 1.727630]\n",
      "epoch:13 step:13055 [D loss: 0.684286, acc: 57.03%] [G loss: 1.809232]\n",
      "epoch:13 step:13056 [D loss: 0.621352, acc: 67.97%] [G loss: 1.906942]\n",
      "epoch:13 step:13057 [D loss: 0.646425, acc: 63.28%] [G loss: 1.918243]\n",
      "epoch:13 step:13058 [D loss: 0.614357, acc: 61.72%] [G loss: 1.944360]\n",
      "epoch:13 step:13059 [D loss: 0.678493, acc: 64.06%] [G loss: 1.845402]\n",
      "epoch:13 step:13060 [D loss: 0.654554, acc: 59.38%] [G loss: 1.818183]\n",
      "epoch:13 step:13061 [D loss: 0.687230, acc: 60.16%] [G loss: 1.824371]\n",
      "epoch:13 step:13062 [D loss: 0.620395, acc: 66.41%] [G loss: 1.927513]\n",
      "epoch:13 step:13063 [D loss: 0.651722, acc: 57.81%] [G loss: 1.873025]\n",
      "epoch:13 step:13064 [D loss: 0.620536, acc: 64.84%] [G loss: 1.936643]\n",
      "epoch:13 step:13065 [D loss: 0.619864, acc: 65.62%] [G loss: 2.169438]\n",
      "epoch:13 step:13066 [D loss: 0.596737, acc: 68.75%] [G loss: 1.878891]\n",
      "epoch:13 step:13067 [D loss: 0.624080, acc: 60.16%] [G loss: 2.204455]\n",
      "epoch:13 step:13068 [D loss: 0.627761, acc: 64.84%] [G loss: 1.895692]\n",
      "epoch:13 step:13069 [D loss: 0.649743, acc: 60.94%] [G loss: 1.974219]\n",
      "epoch:13 step:13070 [D loss: 0.669268, acc: 64.06%] [G loss: 1.902477]\n",
      "epoch:13 step:13071 [D loss: 0.645972, acc: 64.84%] [G loss: 1.893534]\n",
      "epoch:13 step:13072 [D loss: 0.690779, acc: 54.69%] [G loss: 1.868598]\n",
      "epoch:13 step:13073 [D loss: 0.734176, acc: 51.56%] [G loss: 1.858077]\n",
      "epoch:13 step:13074 [D loss: 0.572047, acc: 71.09%] [G loss: 1.875313]\n",
      "epoch:13 step:13075 [D loss: 0.584259, acc: 70.31%] [G loss: 2.061674]\n",
      "epoch:13 step:13076 [D loss: 0.679153, acc: 60.16%] [G loss: 1.914778]\n",
      "epoch:13 step:13077 [D loss: 0.680123, acc: 58.59%] [G loss: 1.933799]\n",
      "epoch:13 step:13078 [D loss: 0.583654, acc: 72.66%] [G loss: 1.962529]\n",
      "epoch:13 step:13079 [D loss: 0.627255, acc: 70.31%] [G loss: 1.962125]\n",
      "epoch:13 step:13080 [D loss: 0.616125, acc: 67.19%] [G loss: 2.009332]\n",
      "epoch:13 step:13081 [D loss: 0.587686, acc: 67.97%] [G loss: 2.142511]\n",
      "epoch:13 step:13082 [D loss: 0.660625, acc: 60.16%] [G loss: 1.939668]\n",
      "epoch:13 step:13083 [D loss: 0.656582, acc: 65.62%] [G loss: 1.872545]\n",
      "epoch:13 step:13084 [D loss: 0.611452, acc: 67.97%] [G loss: 1.944249]\n",
      "epoch:13 step:13085 [D loss: 0.661307, acc: 54.69%] [G loss: 2.049861]\n",
      "epoch:13 step:13086 [D loss: 0.618351, acc: 63.28%] [G loss: 2.068296]\n",
      "epoch:13 step:13087 [D loss: 0.681542, acc: 58.59%] [G loss: 2.067816]\n",
      "epoch:13 step:13088 [D loss: 0.669060, acc: 59.38%] [G loss: 1.904229]\n",
      "epoch:13 step:13089 [D loss: 0.629237, acc: 65.62%] [G loss: 1.870807]\n",
      "epoch:13 step:13090 [D loss: 0.592920, acc: 67.97%] [G loss: 2.029006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:13091 [D loss: 0.605348, acc: 73.44%] [G loss: 1.995886]\n",
      "epoch:13 step:13092 [D loss: 0.622073, acc: 67.19%] [G loss: 2.095137]\n",
      "epoch:13 step:13093 [D loss: 0.594297, acc: 63.28%] [G loss: 2.270394]\n",
      "epoch:13 step:13094 [D loss: 0.674755, acc: 59.38%] [G loss: 2.023406]\n",
      "epoch:13 step:13095 [D loss: 0.645139, acc: 62.50%] [G loss: 1.944727]\n",
      "epoch:13 step:13096 [D loss: 0.667258, acc: 63.28%] [G loss: 1.886400]\n",
      "epoch:13 step:13097 [D loss: 0.567171, acc: 73.44%] [G loss: 2.041694]\n",
      "epoch:13 step:13098 [D loss: 0.637433, acc: 66.41%] [G loss: 1.954283]\n",
      "epoch:13 step:13099 [D loss: 0.616815, acc: 69.53%] [G loss: 2.288943]\n",
      "epoch:13 step:13100 [D loss: 0.581446, acc: 71.09%] [G loss: 2.189581]\n",
      "epoch:13 step:13101 [D loss: 0.714106, acc: 55.47%] [G loss: 1.877301]\n",
      "epoch:13 step:13102 [D loss: 0.662422, acc: 60.94%] [G loss: 2.010730]\n",
      "epoch:13 step:13103 [D loss: 0.656691, acc: 63.28%] [G loss: 1.957504]\n",
      "epoch:13 step:13104 [D loss: 0.568228, acc: 78.12%] [G loss: 2.230335]\n",
      "epoch:13 step:13105 [D loss: 0.535250, acc: 75.00%] [G loss: 2.202260]\n",
      "epoch:13 step:13106 [D loss: 0.561933, acc: 72.66%] [G loss: 2.167153]\n",
      "epoch:13 step:13107 [D loss: 0.555143, acc: 71.09%] [G loss: 2.343220]\n",
      "epoch:13 step:13108 [D loss: 0.598487, acc: 68.75%] [G loss: 2.183931]\n",
      "epoch:13 step:13109 [D loss: 0.819291, acc: 49.22%] [G loss: 1.942011]\n",
      "epoch:13 step:13110 [D loss: 0.728820, acc: 52.34%] [G loss: 1.947477]\n",
      "epoch:13 step:13111 [D loss: 0.623853, acc: 66.41%] [G loss: 2.218436]\n",
      "epoch:13 step:13112 [D loss: 0.625937, acc: 67.19%] [G loss: 1.993015]\n",
      "epoch:13 step:13113 [D loss: 0.658801, acc: 60.16%] [G loss: 2.117731]\n",
      "epoch:13 step:13114 [D loss: 0.632276, acc: 62.50%] [G loss: 2.111121]\n",
      "epoch:13 step:13115 [D loss: 0.603266, acc: 64.06%] [G loss: 2.073808]\n",
      "epoch:13 step:13116 [D loss: 0.606948, acc: 68.75%] [G loss: 2.049354]\n",
      "epoch:13 step:13117 [D loss: 0.561699, acc: 74.22%] [G loss: 2.336428]\n",
      "epoch:13 step:13118 [D loss: 0.563544, acc: 73.44%] [G loss: 2.479032]\n",
      "epoch:14 step:13119 [D loss: 0.687450, acc: 62.50%] [G loss: 1.919940]\n",
      "epoch:14 step:13120 [D loss: 0.655374, acc: 60.94%] [G loss: 1.959404]\n",
      "epoch:14 step:13121 [D loss: 0.631260, acc: 64.84%] [G loss: 1.958470]\n",
      "epoch:14 step:13122 [D loss: 0.655390, acc: 61.72%] [G loss: 1.868318]\n",
      "epoch:14 step:13123 [D loss: 0.647769, acc: 61.72%] [G loss: 1.913731]\n",
      "epoch:14 step:13124 [D loss: 0.649497, acc: 60.94%] [G loss: 2.021391]\n",
      "epoch:14 step:13125 [D loss: 0.656528, acc: 67.19%] [G loss: 2.024375]\n",
      "epoch:14 step:13126 [D loss: 0.618602, acc: 64.84%] [G loss: 2.052594]\n",
      "epoch:14 step:13127 [D loss: 0.618145, acc: 63.28%] [G loss: 2.034286]\n",
      "epoch:14 step:13128 [D loss: 0.596526, acc: 71.88%] [G loss: 2.038047]\n",
      "epoch:14 step:13129 [D loss: 0.634165, acc: 65.62%] [G loss: 1.904819]\n",
      "epoch:14 step:13130 [D loss: 0.683274, acc: 57.03%] [G loss: 1.860376]\n",
      "epoch:14 step:13131 [D loss: 0.634094, acc: 64.06%] [G loss: 1.871500]\n",
      "epoch:14 step:13132 [D loss: 0.635228, acc: 64.06%] [G loss: 1.951675]\n",
      "epoch:14 step:13133 [D loss: 0.567501, acc: 72.66%] [G loss: 2.252323]\n",
      "epoch:14 step:13134 [D loss: 0.598469, acc: 69.53%] [G loss: 2.064171]\n",
      "epoch:14 step:13135 [D loss: 0.623911, acc: 64.06%] [G loss: 2.058746]\n",
      "epoch:14 step:13136 [D loss: 0.689398, acc: 57.81%] [G loss: 2.000699]\n",
      "epoch:14 step:13137 [D loss: 0.651881, acc: 59.38%] [G loss: 1.721344]\n",
      "epoch:14 step:13138 [D loss: 0.672724, acc: 57.03%] [G loss: 1.728675]\n",
      "epoch:14 step:13139 [D loss: 0.703444, acc: 60.16%] [G loss: 1.774352]\n",
      "epoch:14 step:13140 [D loss: 0.670880, acc: 57.03%] [G loss: 1.970641]\n",
      "epoch:14 step:13141 [D loss: 0.618165, acc: 63.28%] [G loss: 2.062488]\n",
      "epoch:14 step:13142 [D loss: 0.697491, acc: 55.47%] [G loss: 2.004770]\n",
      "epoch:14 step:13143 [D loss: 0.582171, acc: 67.97%] [G loss: 2.102032]\n",
      "epoch:14 step:13144 [D loss: 0.637707, acc: 60.94%] [G loss: 1.828639]\n",
      "epoch:14 step:13145 [D loss: 0.629745, acc: 67.19%] [G loss: 1.787508]\n",
      "epoch:14 step:13146 [D loss: 0.629805, acc: 64.06%] [G loss: 1.899778]\n",
      "epoch:14 step:13147 [D loss: 0.639297, acc: 64.06%] [G loss: 1.836713]\n",
      "epoch:14 step:13148 [D loss: 0.696568, acc: 59.38%] [G loss: 1.868322]\n",
      "epoch:14 step:13149 [D loss: 0.720963, acc: 53.91%] [G loss: 1.724888]\n",
      "epoch:14 step:13150 [D loss: 0.620410, acc: 67.19%] [G loss: 1.920994]\n",
      "epoch:14 step:13151 [D loss: 0.591829, acc: 71.88%] [G loss: 1.862689]\n",
      "epoch:14 step:13152 [D loss: 0.641134, acc: 63.28%] [G loss: 1.841964]\n",
      "epoch:14 step:13153 [D loss: 0.613709, acc: 71.09%] [G loss: 1.798408]\n",
      "epoch:14 step:13154 [D loss: 0.633155, acc: 64.84%] [G loss: 1.994405]\n",
      "epoch:14 step:13155 [D loss: 0.631776, acc: 64.84%] [G loss: 1.894876]\n",
      "epoch:14 step:13156 [D loss: 0.649533, acc: 60.94%] [G loss: 1.961328]\n",
      "epoch:14 step:13157 [D loss: 0.627512, acc: 60.16%] [G loss: 1.987151]\n",
      "epoch:14 step:13158 [D loss: 0.562423, acc: 72.66%] [G loss: 2.177839]\n",
      "epoch:14 step:13159 [D loss: 0.700637, acc: 58.59%] [G loss: 1.889023]\n",
      "epoch:14 step:13160 [D loss: 0.585822, acc: 71.09%] [G loss: 1.997370]\n",
      "epoch:14 step:13161 [D loss: 0.649065, acc: 61.72%] [G loss: 2.024869]\n",
      "epoch:14 step:13162 [D loss: 0.709136, acc: 57.81%] [G loss: 1.832066]\n",
      "epoch:14 step:13163 [D loss: 0.644037, acc: 63.28%] [G loss: 1.855668]\n",
      "epoch:14 step:13164 [D loss: 0.656730, acc: 68.75%] [G loss: 1.755450]\n",
      "epoch:14 step:13165 [D loss: 0.610712, acc: 69.53%] [G loss: 1.961349]\n",
      "epoch:14 step:13166 [D loss: 0.619424, acc: 66.41%] [G loss: 1.938531]\n",
      "epoch:14 step:13167 [D loss: 0.596035, acc: 68.75%] [G loss: 2.022809]\n",
      "epoch:14 step:13168 [D loss: 0.629816, acc: 63.28%] [G loss: 2.036500]\n",
      "epoch:14 step:13169 [D loss: 0.646303, acc: 67.97%] [G loss: 1.898170]\n",
      "epoch:14 step:13170 [D loss: 0.648330, acc: 64.84%] [G loss: 1.956898]\n",
      "epoch:14 step:13171 [D loss: 0.599268, acc: 67.19%] [G loss: 2.032285]\n",
      "epoch:14 step:13172 [D loss: 0.599748, acc: 70.31%] [G loss: 2.187202]\n",
      "epoch:14 step:13173 [D loss: 0.599343, acc: 64.06%] [G loss: 2.179427]\n",
      "epoch:14 step:13174 [D loss: 0.659354, acc: 60.94%] [G loss: 2.048065]\n",
      "epoch:14 step:13175 [D loss: 0.623954, acc: 62.50%] [G loss: 1.994730]\n",
      "epoch:14 step:13176 [D loss: 0.677501, acc: 57.03%] [G loss: 1.892313]\n",
      "epoch:14 step:13177 [D loss: 0.644913, acc: 67.19%] [G loss: 2.092372]\n",
      "epoch:14 step:13178 [D loss: 0.668660, acc: 60.94%] [G loss: 1.899727]\n",
      "epoch:14 step:13179 [D loss: 0.653742, acc: 63.28%] [G loss: 1.879938]\n",
      "epoch:14 step:13180 [D loss: 0.638975, acc: 61.72%] [G loss: 1.981378]\n",
      "epoch:14 step:13181 [D loss: 0.625199, acc: 64.06%] [G loss: 1.944120]\n",
      "epoch:14 step:13182 [D loss: 0.678937, acc: 60.94%] [G loss: 1.920850]\n",
      "epoch:14 step:13183 [D loss: 0.682719, acc: 63.28%] [G loss: 1.868944]\n",
      "epoch:14 step:13184 [D loss: 0.664923, acc: 59.38%] [G loss: 1.888946]\n",
      "epoch:14 step:13185 [D loss: 0.684213, acc: 54.69%] [G loss: 1.844840]\n",
      "epoch:14 step:13186 [D loss: 0.633285, acc: 64.84%] [G loss: 1.805619]\n",
      "epoch:14 step:13187 [D loss: 0.615167, acc: 64.84%] [G loss: 2.160978]\n",
      "epoch:14 step:13188 [D loss: 0.612816, acc: 65.62%] [G loss: 1.923281]\n",
      "epoch:14 step:13189 [D loss: 0.688590, acc: 60.94%] [G loss: 1.920219]\n",
      "epoch:14 step:13190 [D loss: 0.609868, acc: 64.06%] [G loss: 1.896100]\n",
      "epoch:14 step:13191 [D loss: 0.679168, acc: 61.72%] [G loss: 1.753889]\n",
      "epoch:14 step:13192 [D loss: 0.647530, acc: 61.72%] [G loss: 1.901246]\n",
      "epoch:14 step:13193 [D loss: 0.626803, acc: 66.41%] [G loss: 2.116103]\n",
      "epoch:14 step:13194 [D loss: 0.617209, acc: 65.62%] [G loss: 1.960912]\n",
      "epoch:14 step:13195 [D loss: 0.565461, acc: 75.00%] [G loss: 2.385192]\n",
      "epoch:14 step:13196 [D loss: 0.679849, acc: 59.38%] [G loss: 1.755833]\n",
      "epoch:14 step:13197 [D loss: 0.716427, acc: 49.22%] [G loss: 1.885854]\n",
      "epoch:14 step:13198 [D loss: 0.693749, acc: 54.69%] [G loss: 1.783873]\n",
      "epoch:14 step:13199 [D loss: 0.749429, acc: 45.31%] [G loss: 1.638028]\n",
      "epoch:14 step:13200 [D loss: 0.629634, acc: 59.38%] [G loss: 1.988363]\n",
      "epoch:14 step:13201 [D loss: 0.645947, acc: 66.41%] [G loss: 1.831215]\n",
      "epoch:14 step:13202 [D loss: 0.586465, acc: 67.97%] [G loss: 1.950287]\n",
      "epoch:14 step:13203 [D loss: 0.708313, acc: 53.12%] [G loss: 1.804424]\n",
      "epoch:14 step:13204 [D loss: 0.681510, acc: 58.59%] [G loss: 1.795573]\n",
      "epoch:14 step:13205 [D loss: 0.650514, acc: 59.38%] [G loss: 1.815371]\n",
      "epoch:14 step:13206 [D loss: 0.598666, acc: 70.31%] [G loss: 1.882380]\n",
      "epoch:14 step:13207 [D loss: 0.617850, acc: 67.97%] [G loss: 1.886717]\n",
      "epoch:14 step:13208 [D loss: 0.653482, acc: 60.94%] [G loss: 1.831032]\n",
      "epoch:14 step:13209 [D loss: 0.651210, acc: 64.84%] [G loss: 2.003996]\n",
      "epoch:14 step:13210 [D loss: 0.581298, acc: 70.31%] [G loss: 1.940142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13211 [D loss: 0.582863, acc: 71.88%] [G loss: 2.074030]\n",
      "epoch:14 step:13212 [D loss: 0.656050, acc: 55.47%] [G loss: 1.966730]\n",
      "epoch:14 step:13213 [D loss: 0.665086, acc: 58.59%] [G loss: 1.857523]\n",
      "epoch:14 step:13214 [D loss: 0.654734, acc: 60.94%] [G loss: 1.944368]\n",
      "epoch:14 step:13215 [D loss: 0.612582, acc: 68.75%] [G loss: 1.967737]\n",
      "epoch:14 step:13216 [D loss: 0.669190, acc: 60.94%] [G loss: 1.803908]\n",
      "epoch:14 step:13217 [D loss: 0.633396, acc: 62.50%] [G loss: 1.927110]\n",
      "epoch:14 step:13218 [D loss: 0.635818, acc: 64.06%] [G loss: 1.944629]\n",
      "epoch:14 step:13219 [D loss: 0.654418, acc: 58.59%] [G loss: 1.997298]\n",
      "epoch:14 step:13220 [D loss: 0.621316, acc: 64.84%] [G loss: 1.914483]\n",
      "epoch:14 step:13221 [D loss: 0.610901, acc: 66.41%] [G loss: 1.849385]\n",
      "epoch:14 step:13222 [D loss: 0.641660, acc: 67.19%] [G loss: 1.914069]\n",
      "epoch:14 step:13223 [D loss: 0.642959, acc: 65.62%] [G loss: 2.060958]\n",
      "epoch:14 step:13224 [D loss: 0.645988, acc: 65.62%] [G loss: 2.248975]\n",
      "epoch:14 step:13225 [D loss: 0.601335, acc: 64.84%] [G loss: 2.045058]\n",
      "epoch:14 step:13226 [D loss: 0.704821, acc: 53.12%] [G loss: 1.760775]\n",
      "epoch:14 step:13227 [D loss: 0.645454, acc: 61.72%] [G loss: 1.835715]\n",
      "epoch:14 step:13228 [D loss: 0.613438, acc: 63.28%] [G loss: 1.889232]\n",
      "epoch:14 step:13229 [D loss: 0.661836, acc: 59.38%] [G loss: 1.927480]\n",
      "epoch:14 step:13230 [D loss: 0.624833, acc: 65.62%] [G loss: 1.981634]\n",
      "epoch:14 step:13231 [D loss: 0.653555, acc: 66.41%] [G loss: 1.973065]\n",
      "epoch:14 step:13232 [D loss: 0.638685, acc: 68.75%] [G loss: 1.970813]\n",
      "epoch:14 step:13233 [D loss: 0.640292, acc: 62.50%] [G loss: 2.093989]\n",
      "epoch:14 step:13234 [D loss: 0.659845, acc: 65.62%] [G loss: 1.999946]\n",
      "epoch:14 step:13235 [D loss: 0.610454, acc: 67.19%] [G loss: 2.086046]\n",
      "epoch:14 step:13236 [D loss: 0.631133, acc: 67.97%] [G loss: 2.045363]\n",
      "epoch:14 step:13237 [D loss: 0.568691, acc: 71.88%] [G loss: 2.340176]\n",
      "epoch:14 step:13238 [D loss: 0.695519, acc: 58.59%] [G loss: 1.941604]\n",
      "epoch:14 step:13239 [D loss: 0.730375, acc: 55.47%] [G loss: 2.008941]\n",
      "epoch:14 step:13240 [D loss: 0.604872, acc: 62.50%] [G loss: 2.138945]\n",
      "epoch:14 step:13241 [D loss: 0.633582, acc: 65.62%] [G loss: 2.031276]\n",
      "epoch:14 step:13242 [D loss: 0.719608, acc: 56.25%] [G loss: 1.916045]\n",
      "epoch:14 step:13243 [D loss: 0.701426, acc: 53.91%] [G loss: 1.749147]\n",
      "epoch:14 step:13244 [D loss: 0.653209, acc: 60.16%] [G loss: 1.851443]\n",
      "epoch:14 step:13245 [D loss: 0.626152, acc: 61.72%] [G loss: 1.911896]\n",
      "epoch:14 step:13246 [D loss: 0.688094, acc: 53.12%] [G loss: 1.877269]\n",
      "epoch:14 step:13247 [D loss: 0.679920, acc: 57.03%] [G loss: 1.750199]\n",
      "epoch:14 step:13248 [D loss: 0.680948, acc: 53.12%] [G loss: 1.803261]\n",
      "epoch:14 step:13249 [D loss: 0.616835, acc: 63.28%] [G loss: 1.930989]\n",
      "epoch:14 step:13250 [D loss: 0.641788, acc: 60.94%] [G loss: 1.826945]\n",
      "epoch:14 step:13251 [D loss: 0.637443, acc: 64.06%] [G loss: 1.882452]\n",
      "epoch:14 step:13252 [D loss: 0.649990, acc: 61.72%] [G loss: 1.883339]\n",
      "epoch:14 step:13253 [D loss: 0.640571, acc: 64.84%] [G loss: 1.905218]\n",
      "epoch:14 step:13254 [D loss: 0.641439, acc: 63.28%] [G loss: 1.879417]\n",
      "epoch:14 step:13255 [D loss: 0.669144, acc: 60.94%] [G loss: 1.792907]\n",
      "epoch:14 step:13256 [D loss: 0.625694, acc: 64.06%] [G loss: 1.857588]\n",
      "epoch:14 step:13257 [D loss: 0.659773, acc: 60.94%] [G loss: 1.994193]\n",
      "epoch:14 step:13258 [D loss: 0.600877, acc: 64.06%] [G loss: 1.822844]\n",
      "epoch:14 step:13259 [D loss: 0.660992, acc: 63.28%] [G loss: 1.833426]\n",
      "epoch:14 step:13260 [D loss: 0.647685, acc: 59.38%] [G loss: 1.831422]\n",
      "epoch:14 step:13261 [D loss: 0.629826, acc: 63.28%] [G loss: 1.872192]\n",
      "epoch:14 step:13262 [D loss: 0.604138, acc: 66.41%] [G loss: 1.985659]\n",
      "epoch:14 step:13263 [D loss: 0.631212, acc: 64.06%] [G loss: 1.833136]\n",
      "epoch:14 step:13264 [D loss: 0.601850, acc: 68.75%] [G loss: 2.013047]\n",
      "epoch:14 step:13265 [D loss: 0.636590, acc: 59.38%] [G loss: 2.004437]\n",
      "epoch:14 step:13266 [D loss: 0.686652, acc: 57.03%] [G loss: 1.818110]\n",
      "epoch:14 step:13267 [D loss: 0.640415, acc: 58.59%] [G loss: 1.906884]\n",
      "epoch:14 step:13268 [D loss: 0.634000, acc: 64.06%] [G loss: 1.999640]\n",
      "epoch:14 step:13269 [D loss: 0.618124, acc: 64.06%] [G loss: 2.143423]\n",
      "epoch:14 step:13270 [D loss: 0.622746, acc: 64.84%] [G loss: 2.047755]\n",
      "epoch:14 step:13271 [D loss: 0.642105, acc: 65.62%] [G loss: 1.829962]\n",
      "epoch:14 step:13272 [D loss: 0.621581, acc: 64.06%] [G loss: 1.838226]\n",
      "epoch:14 step:13273 [D loss: 0.645842, acc: 60.16%] [G loss: 2.013068]\n",
      "epoch:14 step:13274 [D loss: 0.614470, acc: 63.28%] [G loss: 2.006640]\n",
      "epoch:14 step:13275 [D loss: 0.621368, acc: 69.53%] [G loss: 1.955665]\n",
      "epoch:14 step:13276 [D loss: 0.649594, acc: 63.28%] [G loss: 1.788746]\n",
      "epoch:14 step:13277 [D loss: 0.648354, acc: 64.06%] [G loss: 1.878573]\n",
      "epoch:14 step:13278 [D loss: 0.728377, acc: 55.47%] [G loss: 1.789190]\n",
      "epoch:14 step:13279 [D loss: 0.632249, acc: 67.19%] [G loss: 1.881000]\n",
      "epoch:14 step:13280 [D loss: 0.635728, acc: 59.38%] [G loss: 1.936625]\n",
      "epoch:14 step:13281 [D loss: 0.659084, acc: 59.38%] [G loss: 1.963949]\n",
      "epoch:14 step:13282 [D loss: 0.635255, acc: 64.06%] [G loss: 1.922014]\n",
      "epoch:14 step:13283 [D loss: 0.666036, acc: 55.47%] [G loss: 1.868037]\n",
      "epoch:14 step:13284 [D loss: 0.641377, acc: 64.84%] [G loss: 1.892904]\n",
      "epoch:14 step:13285 [D loss: 0.648684, acc: 62.50%] [G loss: 1.943429]\n",
      "epoch:14 step:13286 [D loss: 0.595419, acc: 66.41%] [G loss: 1.961251]\n",
      "epoch:14 step:13287 [D loss: 0.638193, acc: 71.88%] [G loss: 1.859441]\n",
      "epoch:14 step:13288 [D loss: 0.713653, acc: 55.47%] [G loss: 1.861945]\n",
      "epoch:14 step:13289 [D loss: 0.604571, acc: 68.75%] [G loss: 2.030371]\n",
      "epoch:14 step:13290 [D loss: 0.649795, acc: 64.06%] [G loss: 1.883558]\n",
      "epoch:14 step:13291 [D loss: 0.614439, acc: 66.41%] [G loss: 1.980486]\n",
      "epoch:14 step:13292 [D loss: 0.703187, acc: 52.34%] [G loss: 1.762908]\n",
      "epoch:14 step:13293 [D loss: 0.648897, acc: 64.06%] [G loss: 1.859934]\n",
      "epoch:14 step:13294 [D loss: 0.700599, acc: 53.91%] [G loss: 1.847121]\n",
      "epoch:14 step:13295 [D loss: 0.641221, acc: 58.59%] [G loss: 1.810198]\n",
      "epoch:14 step:13296 [D loss: 0.584975, acc: 69.53%] [G loss: 2.084937]\n",
      "epoch:14 step:13297 [D loss: 0.700091, acc: 56.25%] [G loss: 1.876279]\n",
      "epoch:14 step:13298 [D loss: 0.664479, acc: 60.94%] [G loss: 1.897523]\n",
      "epoch:14 step:13299 [D loss: 0.654665, acc: 62.50%] [G loss: 1.936079]\n",
      "epoch:14 step:13300 [D loss: 0.650549, acc: 62.50%] [G loss: 1.834228]\n",
      "epoch:14 step:13301 [D loss: 0.653810, acc: 61.72%] [G loss: 1.921099]\n",
      "epoch:14 step:13302 [D loss: 0.624631, acc: 67.97%] [G loss: 1.853478]\n",
      "epoch:14 step:13303 [D loss: 0.662670, acc: 57.81%] [G loss: 2.021257]\n",
      "epoch:14 step:13304 [D loss: 0.638822, acc: 64.06%] [G loss: 2.004944]\n",
      "epoch:14 step:13305 [D loss: 0.646956, acc: 61.72%] [G loss: 1.983314]\n",
      "epoch:14 step:13306 [D loss: 0.644911, acc: 62.50%] [G loss: 1.796373]\n",
      "epoch:14 step:13307 [D loss: 0.662120, acc: 58.59%] [G loss: 1.836090]\n",
      "epoch:14 step:13308 [D loss: 0.623276, acc: 64.84%] [G loss: 1.979804]\n",
      "epoch:14 step:13309 [D loss: 0.623296, acc: 64.06%] [G loss: 1.904532]\n",
      "epoch:14 step:13310 [D loss: 0.604520, acc: 67.97%] [G loss: 1.987503]\n",
      "epoch:14 step:13311 [D loss: 0.656036, acc: 60.94%] [G loss: 1.963751]\n",
      "epoch:14 step:13312 [D loss: 0.560083, acc: 70.31%] [G loss: 2.086419]\n",
      "epoch:14 step:13313 [D loss: 0.662551, acc: 59.38%] [G loss: 1.901206]\n",
      "epoch:14 step:13314 [D loss: 0.634589, acc: 64.06%] [G loss: 1.988927]\n",
      "epoch:14 step:13315 [D loss: 0.592319, acc: 68.75%] [G loss: 2.103133]\n",
      "epoch:14 step:13316 [D loss: 0.646062, acc: 63.28%] [G loss: 2.059516]\n",
      "epoch:14 step:13317 [D loss: 0.608543, acc: 66.41%] [G loss: 1.956204]\n",
      "epoch:14 step:13318 [D loss: 0.686909, acc: 61.72%] [G loss: 1.815131]\n",
      "epoch:14 step:13319 [D loss: 0.670808, acc: 61.72%] [G loss: 1.948653]\n",
      "epoch:14 step:13320 [D loss: 0.665337, acc: 57.81%] [G loss: 1.923800]\n",
      "epoch:14 step:13321 [D loss: 0.680887, acc: 60.94%] [G loss: 1.930976]\n",
      "epoch:14 step:13322 [D loss: 0.644789, acc: 60.16%] [G loss: 1.969027]\n",
      "epoch:14 step:13323 [D loss: 0.693107, acc: 56.25%] [G loss: 1.865019]\n",
      "epoch:14 step:13324 [D loss: 0.638082, acc: 60.16%] [G loss: 1.967337]\n",
      "epoch:14 step:13325 [D loss: 0.570484, acc: 68.75%] [G loss: 2.340595]\n",
      "epoch:14 step:13326 [D loss: 0.559720, acc: 75.00%] [G loss: 2.202662]\n",
      "epoch:14 step:13327 [D loss: 0.633311, acc: 64.06%] [G loss: 2.288349]\n",
      "epoch:14 step:13328 [D loss: 0.687048, acc: 57.03%] [G loss: 1.871487]\n",
      "epoch:14 step:13329 [D loss: 0.710423, acc: 53.12%] [G loss: 1.741528]\n",
      "epoch:14 step:13330 [D loss: 0.638545, acc: 62.50%] [G loss: 1.824350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13331 [D loss: 0.691084, acc: 57.03%] [G loss: 1.828141]\n",
      "epoch:14 step:13332 [D loss: 0.638862, acc: 63.28%] [G loss: 1.853451]\n",
      "epoch:14 step:13333 [D loss: 0.643822, acc: 60.94%] [G loss: 1.934460]\n",
      "epoch:14 step:13334 [D loss: 0.615154, acc: 70.31%] [G loss: 1.853427]\n",
      "epoch:14 step:13335 [D loss: 0.703476, acc: 57.81%] [G loss: 1.953555]\n",
      "epoch:14 step:13336 [D loss: 0.604786, acc: 66.41%] [G loss: 2.067319]\n",
      "epoch:14 step:13337 [D loss: 0.583038, acc: 72.66%] [G loss: 2.248227]\n",
      "epoch:14 step:13338 [D loss: 0.662645, acc: 63.28%] [G loss: 1.859290]\n",
      "epoch:14 step:13339 [D loss: 0.683008, acc: 54.69%] [G loss: 1.860507]\n",
      "epoch:14 step:13340 [D loss: 0.642410, acc: 62.50%] [G loss: 1.963850]\n",
      "epoch:14 step:13341 [D loss: 0.621331, acc: 61.72%] [G loss: 1.975199]\n",
      "epoch:14 step:13342 [D loss: 0.630443, acc: 66.41%] [G loss: 1.865670]\n",
      "epoch:14 step:13343 [D loss: 0.662911, acc: 58.59%] [G loss: 1.895668]\n",
      "epoch:14 step:13344 [D loss: 0.627915, acc: 65.62%] [G loss: 1.989097]\n",
      "epoch:14 step:13345 [D loss: 0.665376, acc: 61.72%] [G loss: 1.846049]\n",
      "epoch:14 step:13346 [D loss: 0.674585, acc: 63.28%] [G loss: 1.807860]\n",
      "epoch:14 step:13347 [D loss: 0.585402, acc: 66.41%] [G loss: 2.150553]\n",
      "epoch:14 step:13348 [D loss: 0.560067, acc: 67.19%] [G loss: 2.352039]\n",
      "epoch:14 step:13349 [D loss: 0.555668, acc: 71.09%] [G loss: 2.515869]\n",
      "epoch:14 step:13350 [D loss: 0.697670, acc: 58.59%] [G loss: 2.356482]\n",
      "epoch:14 step:13351 [D loss: 0.630673, acc: 64.06%] [G loss: 1.819788]\n",
      "epoch:14 step:13352 [D loss: 0.708162, acc: 55.47%] [G loss: 1.882426]\n",
      "epoch:14 step:13353 [D loss: 0.674765, acc: 61.72%] [G loss: 1.909488]\n",
      "epoch:14 step:13354 [D loss: 0.659517, acc: 61.72%] [G loss: 1.808074]\n",
      "epoch:14 step:13355 [D loss: 0.650934, acc: 61.72%] [G loss: 1.885365]\n",
      "epoch:14 step:13356 [D loss: 0.644904, acc: 63.28%] [G loss: 1.982259]\n",
      "epoch:14 step:13357 [D loss: 0.626280, acc: 61.72%] [G loss: 1.889840]\n",
      "epoch:14 step:13358 [D loss: 0.664761, acc: 58.59%] [G loss: 1.879855]\n",
      "epoch:14 step:13359 [D loss: 0.637539, acc: 66.41%] [G loss: 1.914987]\n",
      "epoch:14 step:13360 [D loss: 0.622037, acc: 66.41%] [G loss: 1.974734]\n",
      "epoch:14 step:13361 [D loss: 0.688309, acc: 51.56%] [G loss: 1.985675]\n",
      "epoch:14 step:13362 [D loss: 0.597958, acc: 68.75%] [G loss: 1.921038]\n",
      "epoch:14 step:13363 [D loss: 0.607136, acc: 64.06%] [G loss: 1.917187]\n",
      "epoch:14 step:13364 [D loss: 0.617450, acc: 64.84%] [G loss: 2.059819]\n",
      "epoch:14 step:13365 [D loss: 0.654907, acc: 61.72%] [G loss: 2.086723]\n",
      "epoch:14 step:13366 [D loss: 0.647771, acc: 60.94%] [G loss: 2.114250]\n",
      "epoch:14 step:13367 [D loss: 0.701164, acc: 58.59%] [G loss: 1.769237]\n",
      "epoch:14 step:13368 [D loss: 0.655248, acc: 59.38%] [G loss: 1.904613]\n",
      "epoch:14 step:13369 [D loss: 0.679912, acc: 57.81%] [G loss: 1.813402]\n",
      "epoch:14 step:13370 [D loss: 0.650976, acc: 62.50%] [G loss: 1.829014]\n",
      "epoch:14 step:13371 [D loss: 0.653778, acc: 64.06%] [G loss: 1.881037]\n",
      "epoch:14 step:13372 [D loss: 0.604596, acc: 71.09%] [G loss: 1.841715]\n",
      "epoch:14 step:13373 [D loss: 0.668228, acc: 58.59%] [G loss: 1.835382]\n",
      "epoch:14 step:13374 [D loss: 0.646165, acc: 57.81%] [G loss: 1.964127]\n",
      "epoch:14 step:13375 [D loss: 0.657387, acc: 63.28%] [G loss: 1.724201]\n",
      "epoch:14 step:13376 [D loss: 0.603762, acc: 70.31%] [G loss: 1.861992]\n",
      "epoch:14 step:13377 [D loss: 0.644402, acc: 63.28%] [G loss: 1.937499]\n",
      "epoch:14 step:13378 [D loss: 0.653239, acc: 64.06%] [G loss: 1.972835]\n",
      "epoch:14 step:13379 [D loss: 0.601862, acc: 63.28%] [G loss: 1.990872]\n",
      "epoch:14 step:13380 [D loss: 0.639494, acc: 57.81%] [G loss: 2.051026]\n",
      "epoch:14 step:13381 [D loss: 0.680149, acc: 60.94%] [G loss: 1.937293]\n",
      "epoch:14 step:13382 [D loss: 0.626104, acc: 62.50%] [G loss: 2.057935]\n",
      "epoch:14 step:13383 [D loss: 0.611879, acc: 68.75%] [G loss: 1.859015]\n",
      "epoch:14 step:13384 [D loss: 0.656962, acc: 67.19%] [G loss: 2.026318]\n",
      "epoch:14 step:13385 [D loss: 0.633684, acc: 66.41%] [G loss: 1.893226]\n",
      "epoch:14 step:13386 [D loss: 0.680662, acc: 53.91%] [G loss: 1.774253]\n",
      "epoch:14 step:13387 [D loss: 0.622468, acc: 60.16%] [G loss: 2.156981]\n",
      "epoch:14 step:13388 [D loss: 0.664141, acc: 62.50%] [G loss: 1.960011]\n",
      "epoch:14 step:13389 [D loss: 0.655944, acc: 57.81%] [G loss: 1.934100]\n",
      "epoch:14 step:13390 [D loss: 0.619401, acc: 67.97%] [G loss: 1.830425]\n",
      "epoch:14 step:13391 [D loss: 0.637541, acc: 67.19%] [G loss: 2.051811]\n",
      "epoch:14 step:13392 [D loss: 0.621643, acc: 65.62%] [G loss: 2.194094]\n",
      "epoch:14 step:13393 [D loss: 0.647690, acc: 58.59%] [G loss: 1.982112]\n",
      "epoch:14 step:13394 [D loss: 0.594544, acc: 68.75%] [G loss: 2.133383]\n",
      "epoch:14 step:13395 [D loss: 0.638076, acc: 64.06%] [G loss: 1.881068]\n",
      "epoch:14 step:13396 [D loss: 0.666614, acc: 57.03%] [G loss: 1.749856]\n",
      "epoch:14 step:13397 [D loss: 0.669229, acc: 58.59%] [G loss: 1.931911]\n",
      "epoch:14 step:13398 [D loss: 0.614996, acc: 61.72%] [G loss: 2.050451]\n",
      "epoch:14 step:13399 [D loss: 0.654509, acc: 60.94%] [G loss: 1.828598]\n",
      "epoch:14 step:13400 [D loss: 0.693152, acc: 57.03%] [G loss: 1.774319]\n",
      "epoch:14 step:13401 [D loss: 0.646301, acc: 61.72%] [G loss: 2.004528]\n",
      "epoch:14 step:13402 [D loss: 0.590421, acc: 65.62%] [G loss: 1.968477]\n",
      "epoch:14 step:13403 [D loss: 0.674496, acc: 59.38%] [G loss: 2.052482]\n",
      "epoch:14 step:13404 [D loss: 0.605520, acc: 67.97%] [G loss: 2.101162]\n",
      "epoch:14 step:13405 [D loss: 0.666596, acc: 60.94%] [G loss: 1.854812]\n",
      "epoch:14 step:13406 [D loss: 0.668069, acc: 56.25%] [G loss: 1.899227]\n",
      "epoch:14 step:13407 [D loss: 0.634469, acc: 64.84%] [G loss: 1.818370]\n",
      "epoch:14 step:13408 [D loss: 0.638974, acc: 67.19%] [G loss: 1.927745]\n",
      "epoch:14 step:13409 [D loss: 0.673221, acc: 56.25%] [G loss: 1.923769]\n",
      "epoch:14 step:13410 [D loss: 0.635163, acc: 67.19%] [G loss: 1.913991]\n",
      "epoch:14 step:13411 [D loss: 0.655204, acc: 64.06%] [G loss: 1.968196]\n",
      "epoch:14 step:13412 [D loss: 0.593510, acc: 65.62%] [G loss: 1.979036]\n",
      "epoch:14 step:13413 [D loss: 0.662932, acc: 60.16%] [G loss: 1.872495]\n",
      "epoch:14 step:13414 [D loss: 0.660852, acc: 65.62%] [G loss: 2.022499]\n",
      "epoch:14 step:13415 [D loss: 0.568123, acc: 73.44%] [G loss: 1.788555]\n",
      "epoch:14 step:13416 [D loss: 0.600566, acc: 73.44%] [G loss: 2.051771]\n",
      "epoch:14 step:13417 [D loss: 0.672723, acc: 60.16%] [G loss: 2.004534]\n",
      "epoch:14 step:13418 [D loss: 0.625912, acc: 62.50%] [G loss: 2.081962]\n",
      "epoch:14 step:13419 [D loss: 0.614354, acc: 67.97%] [G loss: 1.823305]\n",
      "epoch:14 step:13420 [D loss: 0.664455, acc: 60.94%] [G loss: 1.918105]\n",
      "epoch:14 step:13421 [D loss: 0.622996, acc: 65.62%] [G loss: 1.961459]\n",
      "epoch:14 step:13422 [D loss: 0.643814, acc: 62.50%] [G loss: 1.866037]\n",
      "epoch:14 step:13423 [D loss: 0.663053, acc: 60.16%] [G loss: 2.073058]\n",
      "epoch:14 step:13424 [D loss: 0.620225, acc: 64.06%] [G loss: 2.015665]\n",
      "epoch:14 step:13425 [D loss: 0.624415, acc: 64.06%] [G loss: 1.857468]\n",
      "epoch:14 step:13426 [D loss: 0.592888, acc: 69.53%] [G loss: 1.967612]\n",
      "epoch:14 step:13427 [D loss: 0.643274, acc: 62.50%] [G loss: 1.977367]\n",
      "epoch:14 step:13428 [D loss: 0.682735, acc: 58.59%] [G loss: 1.765416]\n",
      "epoch:14 step:13429 [D loss: 0.608960, acc: 67.97%] [G loss: 2.010909]\n",
      "epoch:14 step:13430 [D loss: 0.589460, acc: 71.09%] [G loss: 2.320566]\n",
      "epoch:14 step:13431 [D loss: 0.561976, acc: 67.97%] [G loss: 2.371540]\n",
      "epoch:14 step:13432 [D loss: 0.643877, acc: 64.84%] [G loss: 2.145213]\n",
      "epoch:14 step:13433 [D loss: 0.548848, acc: 69.53%] [G loss: 2.430693]\n",
      "epoch:14 step:13434 [D loss: 0.680706, acc: 57.81%] [G loss: 1.869731]\n",
      "epoch:14 step:13435 [D loss: 0.672602, acc: 62.50%] [G loss: 1.894906]\n",
      "epoch:14 step:13436 [D loss: 0.697919, acc: 64.06%] [G loss: 2.056489]\n",
      "epoch:14 step:13437 [D loss: 0.640921, acc: 71.88%] [G loss: 1.795915]\n",
      "epoch:14 step:13438 [D loss: 0.662156, acc: 61.72%] [G loss: 2.001112]\n",
      "epoch:14 step:13439 [D loss: 0.612563, acc: 64.84%] [G loss: 1.983288]\n",
      "epoch:14 step:13440 [D loss: 0.696067, acc: 55.47%] [G loss: 1.947857]\n",
      "epoch:14 step:13441 [D loss: 0.700354, acc: 56.25%] [G loss: 1.716300]\n",
      "epoch:14 step:13442 [D loss: 0.641112, acc: 65.62%] [G loss: 1.877507]\n",
      "epoch:14 step:13443 [D loss: 0.630353, acc: 59.38%] [G loss: 1.879947]\n",
      "epoch:14 step:13444 [D loss: 0.678346, acc: 60.16%] [G loss: 1.867430]\n",
      "epoch:14 step:13445 [D loss: 0.625370, acc: 65.62%] [G loss: 1.918241]\n",
      "epoch:14 step:13446 [D loss: 0.658557, acc: 61.72%] [G loss: 2.004410]\n",
      "epoch:14 step:13447 [D loss: 0.643328, acc: 63.28%] [G loss: 1.944873]\n",
      "epoch:14 step:13448 [D loss: 0.643707, acc: 63.28%] [G loss: 1.886222]\n",
      "epoch:14 step:13449 [D loss: 0.621267, acc: 66.41%] [G loss: 2.018231]\n",
      "epoch:14 step:13450 [D loss: 0.612086, acc: 67.19%] [G loss: 2.051232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13451 [D loss: 0.616156, acc: 67.19%] [G loss: 2.013989]\n",
      "epoch:14 step:13452 [D loss: 0.660603, acc: 62.50%] [G loss: 2.051908]\n",
      "epoch:14 step:13453 [D loss: 0.594055, acc: 70.31%] [G loss: 2.043899]\n",
      "epoch:14 step:13454 [D loss: 0.684780, acc: 60.94%] [G loss: 1.929006]\n",
      "epoch:14 step:13455 [D loss: 0.633643, acc: 68.75%] [G loss: 2.103064]\n",
      "epoch:14 step:13456 [D loss: 0.615266, acc: 63.28%] [G loss: 2.099602]\n",
      "epoch:14 step:13457 [D loss: 0.644960, acc: 68.75%] [G loss: 2.092118]\n",
      "epoch:14 step:13458 [D loss: 0.637313, acc: 62.50%] [G loss: 2.137929]\n",
      "epoch:14 step:13459 [D loss: 0.677862, acc: 56.25%] [G loss: 1.821422]\n",
      "epoch:14 step:13460 [D loss: 0.677958, acc: 60.94%] [G loss: 1.803032]\n",
      "epoch:14 step:13461 [D loss: 0.643888, acc: 64.06%] [G loss: 2.063189]\n",
      "epoch:14 step:13462 [D loss: 0.590617, acc: 69.53%] [G loss: 1.975160]\n",
      "epoch:14 step:13463 [D loss: 0.643628, acc: 61.72%] [G loss: 2.089777]\n",
      "epoch:14 step:13464 [D loss: 0.546171, acc: 73.44%] [G loss: 2.496401]\n",
      "epoch:14 step:13465 [D loss: 0.610027, acc: 64.84%] [G loss: 2.377123]\n",
      "epoch:14 step:13466 [D loss: 0.644125, acc: 60.94%] [G loss: 1.823475]\n",
      "epoch:14 step:13467 [D loss: 0.644606, acc: 63.28%] [G loss: 1.815566]\n",
      "epoch:14 step:13468 [D loss: 0.663370, acc: 67.19%] [G loss: 1.898674]\n",
      "epoch:14 step:13469 [D loss: 0.594859, acc: 63.28%] [G loss: 1.939035]\n",
      "epoch:14 step:13470 [D loss: 0.678196, acc: 57.03%] [G loss: 1.961207]\n",
      "epoch:14 step:13471 [D loss: 0.705851, acc: 57.03%] [G loss: 1.934766]\n",
      "epoch:14 step:13472 [D loss: 0.600989, acc: 65.62%] [G loss: 1.941421]\n",
      "epoch:14 step:13473 [D loss: 0.645663, acc: 64.06%] [G loss: 1.955097]\n",
      "epoch:14 step:13474 [D loss: 0.676467, acc: 58.59%] [G loss: 1.838548]\n",
      "epoch:14 step:13475 [D loss: 0.610020, acc: 64.84%] [G loss: 2.160081]\n",
      "epoch:14 step:13476 [D loss: 0.652021, acc: 63.28%] [G loss: 1.911986]\n",
      "epoch:14 step:13477 [D loss: 0.553398, acc: 73.44%] [G loss: 2.050138]\n",
      "epoch:14 step:13478 [D loss: 0.614973, acc: 67.19%] [G loss: 2.002746]\n",
      "epoch:14 step:13479 [D loss: 0.635829, acc: 67.19%] [G loss: 1.851027]\n",
      "epoch:14 step:13480 [D loss: 0.685310, acc: 60.16%] [G loss: 1.821417]\n",
      "epoch:14 step:13481 [D loss: 0.677054, acc: 56.25%] [G loss: 2.039137]\n",
      "epoch:14 step:13482 [D loss: 0.648966, acc: 67.97%] [G loss: 1.881751]\n",
      "epoch:14 step:13483 [D loss: 0.669372, acc: 60.16%] [G loss: 2.104319]\n",
      "epoch:14 step:13484 [D loss: 0.611934, acc: 64.06%] [G loss: 2.129934]\n",
      "epoch:14 step:13485 [D loss: 0.627663, acc: 65.62%] [G loss: 2.238055]\n",
      "epoch:14 step:13486 [D loss: 0.686024, acc: 60.94%] [G loss: 2.009218]\n",
      "epoch:14 step:13487 [D loss: 0.669678, acc: 60.16%] [G loss: 1.878697]\n",
      "epoch:14 step:13488 [D loss: 0.645952, acc: 63.28%] [G loss: 1.897966]\n",
      "epoch:14 step:13489 [D loss: 0.613200, acc: 66.41%] [G loss: 2.151580]\n",
      "epoch:14 step:13490 [D loss: 0.661970, acc: 55.47%] [G loss: 1.948445]\n",
      "epoch:14 step:13491 [D loss: 0.645862, acc: 62.50%] [G loss: 1.924625]\n",
      "epoch:14 step:13492 [D loss: 0.598288, acc: 67.19%] [G loss: 2.162854]\n",
      "epoch:14 step:13493 [D loss: 0.664603, acc: 57.03%] [G loss: 1.942609]\n",
      "epoch:14 step:13494 [D loss: 0.679592, acc: 57.81%] [G loss: 1.954238]\n",
      "epoch:14 step:13495 [D loss: 0.658883, acc: 61.72%] [G loss: 1.911710]\n",
      "epoch:14 step:13496 [D loss: 0.612023, acc: 67.19%] [G loss: 1.902828]\n",
      "epoch:14 step:13497 [D loss: 0.685492, acc: 58.59%] [G loss: 2.047881]\n",
      "epoch:14 step:13498 [D loss: 0.638995, acc: 60.94%] [G loss: 1.910580]\n",
      "epoch:14 step:13499 [D loss: 0.604666, acc: 64.84%] [G loss: 2.082417]\n",
      "epoch:14 step:13500 [D loss: 0.614631, acc: 69.53%] [G loss: 1.907428]\n",
      "epoch:14 step:13501 [D loss: 0.696383, acc: 60.94%] [G loss: 1.918412]\n",
      "epoch:14 step:13502 [D loss: 0.686878, acc: 61.72%] [G loss: 1.831446]\n",
      "epoch:14 step:13503 [D loss: 0.651995, acc: 62.50%] [G loss: 1.958215]\n",
      "epoch:14 step:13504 [D loss: 0.623445, acc: 63.28%] [G loss: 1.836728]\n",
      "epoch:14 step:13505 [D loss: 0.675528, acc: 60.94%] [G loss: 1.749985]\n",
      "epoch:14 step:13506 [D loss: 0.709159, acc: 54.69%] [G loss: 1.983545]\n",
      "epoch:14 step:13507 [D loss: 0.601640, acc: 70.31%] [G loss: 1.912224]\n",
      "epoch:14 step:13508 [D loss: 0.696663, acc: 52.34%] [G loss: 1.773862]\n",
      "epoch:14 step:13509 [D loss: 0.696396, acc: 57.03%] [G loss: 1.717491]\n",
      "epoch:14 step:13510 [D loss: 0.689082, acc: 58.59%] [G loss: 1.888410]\n",
      "epoch:14 step:13511 [D loss: 0.646355, acc: 63.28%] [G loss: 1.972394]\n",
      "epoch:14 step:13512 [D loss: 0.664030, acc: 64.06%] [G loss: 1.926522]\n",
      "epoch:14 step:13513 [D loss: 0.611491, acc: 66.41%] [G loss: 1.962366]\n",
      "epoch:14 step:13514 [D loss: 0.718647, acc: 52.34%] [G loss: 1.764545]\n",
      "epoch:14 step:13515 [D loss: 0.676195, acc: 60.16%] [G loss: 1.916815]\n",
      "epoch:14 step:13516 [D loss: 0.634704, acc: 71.09%] [G loss: 1.926180]\n",
      "epoch:14 step:13517 [D loss: 0.668008, acc: 57.81%] [G loss: 1.884715]\n",
      "epoch:14 step:13518 [D loss: 0.625023, acc: 63.28%] [G loss: 1.872386]\n",
      "epoch:14 step:13519 [D loss: 0.712315, acc: 53.91%] [G loss: 1.832613]\n",
      "epoch:14 step:13520 [D loss: 0.614892, acc: 68.75%] [G loss: 1.935716]\n",
      "epoch:14 step:13521 [D loss: 0.626818, acc: 62.50%] [G loss: 1.894469]\n",
      "epoch:14 step:13522 [D loss: 0.613653, acc: 67.19%] [G loss: 2.044096]\n",
      "epoch:14 step:13523 [D loss: 0.591636, acc: 65.62%] [G loss: 2.201222]\n",
      "epoch:14 step:13524 [D loss: 0.607215, acc: 66.41%] [G loss: 2.120272]\n",
      "epoch:14 step:13525 [D loss: 0.600474, acc: 71.09%] [G loss: 1.898309]\n",
      "epoch:14 step:13526 [D loss: 0.605725, acc: 66.41%] [G loss: 1.952902]\n",
      "epoch:14 step:13527 [D loss: 0.651881, acc: 60.16%] [G loss: 1.849601]\n",
      "epoch:14 step:13528 [D loss: 0.554871, acc: 71.09%] [G loss: 1.996151]\n",
      "epoch:14 step:13529 [D loss: 0.677421, acc: 60.16%] [G loss: 1.854237]\n",
      "epoch:14 step:13530 [D loss: 0.690436, acc: 58.59%] [G loss: 1.789736]\n",
      "epoch:14 step:13531 [D loss: 0.641187, acc: 65.62%] [G loss: 2.026743]\n",
      "epoch:14 step:13532 [D loss: 0.629720, acc: 68.75%] [G loss: 2.005816]\n",
      "epoch:14 step:13533 [D loss: 0.652235, acc: 58.59%] [G loss: 2.102165]\n",
      "epoch:14 step:13534 [D loss: 0.629607, acc: 67.19%] [G loss: 2.141324]\n",
      "epoch:14 step:13535 [D loss: 0.628316, acc: 64.06%] [G loss: 1.953105]\n",
      "epoch:14 step:13536 [D loss: 0.674271, acc: 60.16%] [G loss: 1.893054]\n",
      "epoch:14 step:13537 [D loss: 0.734744, acc: 57.81%] [G loss: 1.868218]\n",
      "epoch:14 step:13538 [D loss: 0.600904, acc: 71.09%] [G loss: 1.805318]\n",
      "epoch:14 step:13539 [D loss: 0.692824, acc: 60.94%] [G loss: 1.787429]\n",
      "epoch:14 step:13540 [D loss: 0.648517, acc: 61.72%] [G loss: 1.930371]\n",
      "epoch:14 step:13541 [D loss: 0.676492, acc: 63.28%] [G loss: 1.940300]\n",
      "epoch:14 step:13542 [D loss: 0.703174, acc: 51.56%] [G loss: 1.760609]\n",
      "epoch:14 step:13543 [D loss: 0.617066, acc: 67.97%] [G loss: 1.916413]\n",
      "epoch:14 step:13544 [D loss: 0.600503, acc: 67.97%] [G loss: 1.877619]\n",
      "epoch:14 step:13545 [D loss: 0.663133, acc: 64.06%] [G loss: 2.012985]\n",
      "epoch:14 step:13546 [D loss: 0.601392, acc: 67.19%] [G loss: 1.982793]\n",
      "epoch:14 step:13547 [D loss: 0.637094, acc: 64.84%] [G loss: 1.933704]\n",
      "epoch:14 step:13548 [D loss: 0.603350, acc: 69.53%] [G loss: 2.184312]\n",
      "epoch:14 step:13549 [D loss: 0.671808, acc: 60.16%] [G loss: 1.964535]\n",
      "epoch:14 step:13550 [D loss: 0.672453, acc: 57.81%] [G loss: 1.862092]\n",
      "epoch:14 step:13551 [D loss: 0.650641, acc: 57.81%] [G loss: 1.999928]\n",
      "epoch:14 step:13552 [D loss: 0.641436, acc: 64.84%] [G loss: 1.871289]\n",
      "epoch:14 step:13553 [D loss: 0.577904, acc: 68.75%] [G loss: 1.946132]\n",
      "epoch:14 step:13554 [D loss: 0.639846, acc: 67.19%] [G loss: 2.016559]\n",
      "epoch:14 step:13555 [D loss: 0.759530, acc: 54.69%] [G loss: 1.926454]\n",
      "epoch:14 step:13556 [D loss: 0.713940, acc: 57.81%] [G loss: 1.676917]\n",
      "epoch:14 step:13557 [D loss: 0.645776, acc: 61.72%] [G loss: 1.868996]\n",
      "epoch:14 step:13558 [D loss: 0.715406, acc: 53.91%] [G loss: 1.892814]\n",
      "epoch:14 step:13559 [D loss: 0.682420, acc: 57.03%] [G loss: 1.782267]\n",
      "epoch:14 step:13560 [D loss: 0.600283, acc: 70.31%] [G loss: 1.830120]\n",
      "epoch:14 step:13561 [D loss: 0.728919, acc: 53.12%] [G loss: 1.766073]\n",
      "epoch:14 step:13562 [D loss: 0.626386, acc: 70.31%] [G loss: 1.860792]\n",
      "epoch:14 step:13563 [D loss: 0.623214, acc: 66.41%] [G loss: 1.930485]\n",
      "epoch:14 step:13564 [D loss: 0.643144, acc: 66.41%] [G loss: 1.901854]\n",
      "epoch:14 step:13565 [D loss: 0.618136, acc: 64.84%] [G loss: 1.913261]\n",
      "epoch:14 step:13566 [D loss: 0.652852, acc: 60.16%] [G loss: 1.740714]\n",
      "epoch:14 step:13567 [D loss: 0.635586, acc: 59.38%] [G loss: 1.862985]\n",
      "epoch:14 step:13568 [D loss: 0.625997, acc: 68.75%] [G loss: 1.865592]\n",
      "epoch:14 step:13569 [D loss: 0.600703, acc: 65.62%] [G loss: 1.889323]\n",
      "epoch:14 step:13570 [D loss: 0.615105, acc: 71.09%] [G loss: 1.994999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13571 [D loss: 0.583714, acc: 68.75%] [G loss: 1.876115]\n",
      "epoch:14 step:13572 [D loss: 0.638077, acc: 60.94%] [G loss: 1.877021]\n",
      "epoch:14 step:13573 [D loss: 0.634602, acc: 65.62%] [G loss: 2.037115]\n",
      "epoch:14 step:13574 [D loss: 0.636829, acc: 65.62%] [G loss: 1.813549]\n",
      "epoch:14 step:13575 [D loss: 0.596108, acc: 71.88%] [G loss: 2.132852]\n",
      "epoch:14 step:13576 [D loss: 0.677915, acc: 58.59%] [G loss: 1.752630]\n",
      "epoch:14 step:13577 [D loss: 0.716533, acc: 52.34%] [G loss: 1.711970]\n",
      "epoch:14 step:13578 [D loss: 0.695137, acc: 57.81%] [G loss: 1.905448]\n",
      "epoch:14 step:13579 [D loss: 0.644605, acc: 61.72%] [G loss: 1.865600]\n",
      "epoch:14 step:13580 [D loss: 0.641406, acc: 64.84%] [G loss: 1.934363]\n",
      "epoch:14 step:13581 [D loss: 0.613692, acc: 65.62%] [G loss: 1.909688]\n",
      "epoch:14 step:13582 [D loss: 0.611711, acc: 64.06%] [G loss: 1.945010]\n",
      "epoch:14 step:13583 [D loss: 0.676342, acc: 56.25%] [G loss: 1.898267]\n",
      "epoch:14 step:13584 [D loss: 0.639646, acc: 66.41%] [G loss: 1.850194]\n",
      "epoch:14 step:13585 [D loss: 0.672558, acc: 60.16%] [G loss: 1.885302]\n",
      "epoch:14 step:13586 [D loss: 0.597823, acc: 67.97%] [G loss: 2.141080]\n",
      "epoch:14 step:13587 [D loss: 0.599395, acc: 64.84%] [G loss: 2.055651]\n",
      "epoch:14 step:13588 [D loss: 0.618428, acc: 66.41%] [G loss: 2.216854]\n",
      "epoch:14 step:13589 [D loss: 0.653056, acc: 65.62%] [G loss: 2.294197]\n",
      "epoch:14 step:13590 [D loss: 0.615887, acc: 67.97%] [G loss: 2.138217]\n",
      "epoch:14 step:13591 [D loss: 0.701955, acc: 55.47%] [G loss: 1.848216]\n",
      "epoch:14 step:13592 [D loss: 0.636250, acc: 66.41%] [G loss: 1.867854]\n",
      "epoch:14 step:13593 [D loss: 0.673288, acc: 62.50%] [G loss: 2.017301]\n",
      "epoch:14 step:13594 [D loss: 0.655338, acc: 62.50%] [G loss: 1.919810]\n",
      "epoch:14 step:13595 [D loss: 0.691001, acc: 53.91%] [G loss: 1.828988]\n",
      "epoch:14 step:13596 [D loss: 0.623179, acc: 64.84%] [G loss: 1.881891]\n",
      "epoch:14 step:13597 [D loss: 0.584292, acc: 68.75%] [G loss: 2.063354]\n",
      "epoch:14 step:13598 [D loss: 0.625746, acc: 64.06%] [G loss: 2.003137]\n",
      "epoch:14 step:13599 [D loss: 0.553451, acc: 73.44%] [G loss: 2.168288]\n",
      "epoch:14 step:13600 [D loss: 0.732762, acc: 50.78%] [G loss: 1.783229]\n",
      "epoch:14 step:13601 [D loss: 0.633745, acc: 63.28%] [G loss: 1.900586]\n",
      "epoch:14 step:13602 [D loss: 0.596293, acc: 68.75%] [G loss: 1.962177]\n",
      "epoch:14 step:13603 [D loss: 0.706146, acc: 57.03%] [G loss: 1.845985]\n",
      "epoch:14 step:13604 [D loss: 0.651075, acc: 62.50%] [G loss: 1.970568]\n",
      "epoch:14 step:13605 [D loss: 0.686252, acc: 50.00%] [G loss: 1.841183]\n",
      "epoch:14 step:13606 [D loss: 0.619896, acc: 60.16%] [G loss: 2.056159]\n",
      "epoch:14 step:13607 [D loss: 0.633148, acc: 64.06%] [G loss: 1.850042]\n",
      "epoch:14 step:13608 [D loss: 0.649479, acc: 63.28%] [G loss: 1.904443]\n",
      "epoch:14 step:13609 [D loss: 0.631190, acc: 63.28%] [G loss: 1.921410]\n",
      "epoch:14 step:13610 [D loss: 0.641715, acc: 64.84%] [G loss: 1.829296]\n",
      "epoch:14 step:13611 [D loss: 0.609490, acc: 70.31%] [G loss: 1.912454]\n",
      "epoch:14 step:13612 [D loss: 0.599219, acc: 67.97%] [G loss: 2.036258]\n",
      "epoch:14 step:13613 [D loss: 0.575416, acc: 70.31%] [G loss: 2.072603]\n",
      "epoch:14 step:13614 [D loss: 0.619326, acc: 66.41%] [G loss: 1.978555]\n",
      "epoch:14 step:13615 [D loss: 0.644726, acc: 63.28%] [G loss: 2.066695]\n",
      "epoch:14 step:13616 [D loss: 0.636488, acc: 62.50%] [G loss: 2.028509]\n",
      "epoch:14 step:13617 [D loss: 0.614018, acc: 67.97%] [G loss: 2.084622]\n",
      "epoch:14 step:13618 [D loss: 0.717101, acc: 56.25%] [G loss: 1.684775]\n",
      "epoch:14 step:13619 [D loss: 0.696583, acc: 60.94%] [G loss: 1.830341]\n",
      "epoch:14 step:13620 [D loss: 0.705294, acc: 55.47%] [G loss: 1.640872]\n",
      "epoch:14 step:13621 [D loss: 0.667934, acc: 61.72%] [G loss: 1.907288]\n",
      "epoch:14 step:13622 [D loss: 0.551020, acc: 71.88%] [G loss: 2.159041]\n",
      "epoch:14 step:13623 [D loss: 0.692318, acc: 57.03%] [G loss: 1.833638]\n",
      "epoch:14 step:13624 [D loss: 0.630514, acc: 66.41%] [G loss: 1.833462]\n",
      "epoch:14 step:13625 [D loss: 0.695098, acc: 57.03%] [G loss: 1.953723]\n",
      "epoch:14 step:13626 [D loss: 0.636636, acc: 62.50%] [G loss: 2.006393]\n",
      "epoch:14 step:13627 [D loss: 0.630674, acc: 61.72%] [G loss: 1.859819]\n",
      "epoch:14 step:13628 [D loss: 0.661071, acc: 58.59%] [G loss: 1.869581]\n",
      "epoch:14 step:13629 [D loss: 0.685268, acc: 60.94%] [G loss: 1.914966]\n",
      "epoch:14 step:13630 [D loss: 0.655943, acc: 61.72%] [G loss: 1.751661]\n",
      "epoch:14 step:13631 [D loss: 0.611635, acc: 62.50%] [G loss: 1.916684]\n",
      "epoch:14 step:13632 [D loss: 0.630539, acc: 64.84%] [G loss: 2.005195]\n",
      "epoch:14 step:13633 [D loss: 0.617810, acc: 66.41%] [G loss: 2.040230]\n",
      "epoch:14 step:13634 [D loss: 0.602061, acc: 68.75%] [G loss: 1.942529]\n",
      "epoch:14 step:13635 [D loss: 0.671077, acc: 61.72%] [G loss: 1.955210]\n",
      "epoch:14 step:13636 [D loss: 0.646279, acc: 64.06%] [G loss: 1.969917]\n",
      "epoch:14 step:13637 [D loss: 0.606929, acc: 67.19%] [G loss: 2.027925]\n",
      "epoch:14 step:13638 [D loss: 0.627474, acc: 65.62%] [G loss: 1.837415]\n",
      "epoch:14 step:13639 [D loss: 0.699677, acc: 63.28%] [G loss: 1.881759]\n",
      "epoch:14 step:13640 [D loss: 0.607434, acc: 67.97%] [G loss: 1.961897]\n",
      "epoch:14 step:13641 [D loss: 0.607280, acc: 67.97%] [G loss: 2.004020]\n",
      "epoch:14 step:13642 [D loss: 0.622998, acc: 60.94%] [G loss: 1.866405]\n",
      "epoch:14 step:13643 [D loss: 0.627569, acc: 62.50%] [G loss: 1.859841]\n",
      "epoch:14 step:13644 [D loss: 0.644566, acc: 64.06%] [G loss: 1.869079]\n",
      "epoch:14 step:13645 [D loss: 0.633448, acc: 67.19%] [G loss: 1.816818]\n",
      "epoch:14 step:13646 [D loss: 0.669197, acc: 55.47%] [G loss: 1.767266]\n",
      "epoch:14 step:13647 [D loss: 0.695381, acc: 58.59%] [G loss: 1.770239]\n",
      "epoch:14 step:13648 [D loss: 0.666488, acc: 59.38%] [G loss: 1.801873]\n",
      "epoch:14 step:13649 [D loss: 0.699511, acc: 55.47%] [G loss: 1.834047]\n",
      "epoch:14 step:13650 [D loss: 0.625205, acc: 64.84%] [G loss: 1.895076]\n",
      "epoch:14 step:13651 [D loss: 0.665053, acc: 62.50%] [G loss: 1.812760]\n",
      "epoch:14 step:13652 [D loss: 0.590208, acc: 68.75%] [G loss: 2.069788]\n",
      "epoch:14 step:13653 [D loss: 0.655777, acc: 66.41%] [G loss: 1.938700]\n",
      "epoch:14 step:13654 [D loss: 0.617339, acc: 67.19%] [G loss: 1.916169]\n",
      "epoch:14 step:13655 [D loss: 0.656736, acc: 63.28%] [G loss: 1.952938]\n",
      "epoch:14 step:13656 [D loss: 0.648477, acc: 59.38%] [G loss: 1.876713]\n",
      "epoch:14 step:13657 [D loss: 0.631925, acc: 64.84%] [G loss: 1.943907]\n",
      "epoch:14 step:13658 [D loss: 0.649632, acc: 60.94%] [G loss: 1.900642]\n",
      "epoch:14 step:13659 [D loss: 0.673432, acc: 51.56%] [G loss: 1.961801]\n",
      "epoch:14 step:13660 [D loss: 0.623297, acc: 67.19%] [G loss: 1.966474]\n",
      "epoch:14 step:13661 [D loss: 0.650965, acc: 60.16%] [G loss: 1.943914]\n",
      "epoch:14 step:13662 [D loss: 0.610972, acc: 68.75%] [G loss: 1.910501]\n",
      "epoch:14 step:13663 [D loss: 0.628683, acc: 65.62%] [G loss: 1.838956]\n",
      "epoch:14 step:13664 [D loss: 0.631194, acc: 57.81%] [G loss: 1.913662]\n",
      "epoch:14 step:13665 [D loss: 0.609827, acc: 67.19%] [G loss: 2.065575]\n",
      "epoch:14 step:13666 [D loss: 0.629899, acc: 62.50%] [G loss: 1.994602]\n",
      "epoch:14 step:13667 [D loss: 0.627204, acc: 65.62%] [G loss: 1.969147]\n",
      "epoch:14 step:13668 [D loss: 0.584866, acc: 67.19%] [G loss: 2.052877]\n",
      "epoch:14 step:13669 [D loss: 0.625937, acc: 62.50%] [G loss: 2.119209]\n",
      "epoch:14 step:13670 [D loss: 0.592897, acc: 68.75%] [G loss: 2.049995]\n",
      "epoch:14 step:13671 [D loss: 0.727293, acc: 55.47%] [G loss: 1.745076]\n",
      "epoch:14 step:13672 [D loss: 0.592030, acc: 72.66%] [G loss: 2.051759]\n",
      "epoch:14 step:13673 [D loss: 0.620344, acc: 61.72%] [G loss: 1.946095]\n",
      "epoch:14 step:13674 [D loss: 0.592253, acc: 66.41%] [G loss: 2.022092]\n",
      "epoch:14 step:13675 [D loss: 0.631730, acc: 65.62%] [G loss: 2.114905]\n",
      "epoch:14 step:13676 [D loss: 0.627173, acc: 64.06%] [G loss: 2.146985]\n",
      "epoch:14 step:13677 [D loss: 0.659971, acc: 62.50%] [G loss: 1.947182]\n",
      "epoch:14 step:13678 [D loss: 0.688576, acc: 60.94%] [G loss: 1.872331]\n",
      "epoch:14 step:13679 [D loss: 0.619419, acc: 67.19%] [G loss: 2.020030]\n",
      "epoch:14 step:13680 [D loss: 0.682678, acc: 55.47%] [G loss: 1.865473]\n",
      "epoch:14 step:13681 [D loss: 0.616961, acc: 65.62%] [G loss: 2.106922]\n",
      "epoch:14 step:13682 [D loss: 0.559463, acc: 73.44%] [G loss: 2.267416]\n",
      "epoch:14 step:13683 [D loss: 0.645015, acc: 62.50%] [G loss: 1.983021]\n",
      "epoch:14 step:13684 [D loss: 0.642449, acc: 65.62%] [G loss: 1.989094]\n",
      "epoch:14 step:13685 [D loss: 0.618067, acc: 65.62%] [G loss: 1.979654]\n",
      "epoch:14 step:13686 [D loss: 0.623155, acc: 66.41%] [G loss: 2.043337]\n",
      "epoch:14 step:13687 [D loss: 0.653765, acc: 64.06%] [G loss: 1.890166]\n",
      "epoch:14 step:13688 [D loss: 0.645094, acc: 64.84%] [G loss: 1.957604]\n",
      "epoch:14 step:13689 [D loss: 0.667899, acc: 64.06%] [G loss: 1.875965]\n",
      "epoch:14 step:13690 [D loss: 0.643154, acc: 61.72%] [G loss: 2.020917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13691 [D loss: 0.623489, acc: 64.84%] [G loss: 1.840160]\n",
      "epoch:14 step:13692 [D loss: 0.586706, acc: 71.88%] [G loss: 1.970496]\n",
      "epoch:14 step:13693 [D loss: 0.606207, acc: 68.75%] [G loss: 1.894336]\n",
      "epoch:14 step:13694 [D loss: 0.724096, acc: 50.78%] [G loss: 1.900673]\n",
      "epoch:14 step:13695 [D loss: 0.658287, acc: 57.03%] [G loss: 1.821198]\n",
      "epoch:14 step:13696 [D loss: 0.619712, acc: 65.62%] [G loss: 1.901482]\n",
      "epoch:14 step:13697 [D loss: 0.665169, acc: 63.28%] [G loss: 1.873018]\n",
      "epoch:14 step:13698 [D loss: 0.681338, acc: 57.81%] [G loss: 1.790659]\n",
      "epoch:14 step:13699 [D loss: 0.646879, acc: 56.25%] [G loss: 1.830125]\n",
      "epoch:14 step:13700 [D loss: 0.656786, acc: 64.84%] [G loss: 1.839790]\n",
      "epoch:14 step:13701 [D loss: 0.657060, acc: 61.72%] [G loss: 1.895835]\n",
      "epoch:14 step:13702 [D loss: 0.654481, acc: 57.81%] [G loss: 1.848437]\n",
      "epoch:14 step:13703 [D loss: 0.605602, acc: 67.19%] [G loss: 1.957264]\n",
      "epoch:14 step:13704 [D loss: 0.658955, acc: 65.62%] [G loss: 2.072118]\n",
      "epoch:14 step:13705 [D loss: 0.688693, acc: 57.03%] [G loss: 2.039083]\n",
      "epoch:14 step:13706 [D loss: 0.582257, acc: 72.66%] [G loss: 1.872746]\n",
      "epoch:14 step:13707 [D loss: 0.619200, acc: 66.41%] [G loss: 1.942181]\n",
      "epoch:14 step:13708 [D loss: 0.640267, acc: 64.84%] [G loss: 2.049395]\n",
      "epoch:14 step:13709 [D loss: 0.637428, acc: 67.19%] [G loss: 1.989617]\n",
      "epoch:14 step:13710 [D loss: 0.662834, acc: 58.59%] [G loss: 1.871747]\n",
      "epoch:14 step:13711 [D loss: 0.635988, acc: 65.62%] [G loss: 1.874758]\n",
      "epoch:14 step:13712 [D loss: 0.656197, acc: 59.38%] [G loss: 2.006388]\n",
      "epoch:14 step:13713 [D loss: 0.638586, acc: 67.19%] [G loss: 1.988800]\n",
      "epoch:14 step:13714 [D loss: 0.629406, acc: 62.50%] [G loss: 1.837095]\n",
      "epoch:14 step:13715 [D loss: 0.682471, acc: 57.03%] [G loss: 1.929531]\n",
      "epoch:14 step:13716 [D loss: 0.648883, acc: 67.19%] [G loss: 2.010833]\n",
      "epoch:14 step:13717 [D loss: 0.668687, acc: 60.16%] [G loss: 1.810909]\n",
      "epoch:14 step:13718 [D loss: 0.590591, acc: 71.09%] [G loss: 1.820515]\n",
      "epoch:14 step:13719 [D loss: 0.655331, acc: 62.50%] [G loss: 1.836201]\n",
      "epoch:14 step:13720 [D loss: 0.678128, acc: 57.03%] [G loss: 2.000903]\n",
      "epoch:14 step:13721 [D loss: 0.588605, acc: 70.31%] [G loss: 2.030841]\n",
      "epoch:14 step:13722 [D loss: 0.627918, acc: 57.81%] [G loss: 1.917649]\n",
      "epoch:14 step:13723 [D loss: 0.660363, acc: 61.72%] [G loss: 1.962473]\n",
      "epoch:14 step:13724 [D loss: 0.633554, acc: 64.06%] [G loss: 1.915552]\n",
      "epoch:14 step:13725 [D loss: 0.651288, acc: 60.94%] [G loss: 1.964247]\n",
      "epoch:14 step:13726 [D loss: 0.619155, acc: 63.28%] [G loss: 1.809993]\n",
      "epoch:14 step:13727 [D loss: 0.642971, acc: 61.72%] [G loss: 2.002572]\n",
      "epoch:14 step:13728 [D loss: 0.660001, acc: 60.94%] [G loss: 2.007672]\n",
      "epoch:14 step:13729 [D loss: 0.613914, acc: 65.62%] [G loss: 1.970615]\n",
      "epoch:14 step:13730 [D loss: 0.638548, acc: 64.06%] [G loss: 1.802606]\n",
      "epoch:14 step:13731 [D loss: 0.684388, acc: 55.47%] [G loss: 1.956461]\n",
      "epoch:14 step:13732 [D loss: 0.685942, acc: 57.81%] [G loss: 1.811983]\n",
      "epoch:14 step:13733 [D loss: 0.658463, acc: 63.28%] [G loss: 1.739864]\n",
      "epoch:14 step:13734 [D loss: 0.654596, acc: 59.38%] [G loss: 1.939310]\n",
      "epoch:14 step:13735 [D loss: 0.649933, acc: 60.94%] [G loss: 1.868047]\n",
      "epoch:14 step:13736 [D loss: 0.660370, acc: 61.72%] [G loss: 1.850115]\n",
      "epoch:14 step:13737 [D loss: 0.707437, acc: 56.25%] [G loss: 1.773301]\n",
      "epoch:14 step:13738 [D loss: 0.643254, acc: 64.84%] [G loss: 1.852932]\n",
      "epoch:14 step:13739 [D loss: 0.603258, acc: 67.97%] [G loss: 1.827041]\n",
      "epoch:14 step:13740 [D loss: 0.643073, acc: 57.81%] [G loss: 1.831650]\n",
      "epoch:14 step:13741 [D loss: 0.652161, acc: 59.38%] [G loss: 1.962143]\n",
      "epoch:14 step:13742 [D loss: 0.679792, acc: 57.03%] [G loss: 2.073922]\n",
      "epoch:14 step:13743 [D loss: 0.691355, acc: 57.81%] [G loss: 1.775106]\n",
      "epoch:14 step:13744 [D loss: 0.657846, acc: 59.38%] [G loss: 1.765655]\n",
      "epoch:14 step:13745 [D loss: 0.635307, acc: 61.72%] [G loss: 1.909785]\n",
      "epoch:14 step:13746 [D loss: 0.646827, acc: 65.62%] [G loss: 1.812662]\n",
      "epoch:14 step:13747 [D loss: 0.658955, acc: 60.94%] [G loss: 1.940980]\n",
      "epoch:14 step:13748 [D loss: 0.646520, acc: 63.28%] [G loss: 2.015853]\n",
      "epoch:14 step:13749 [D loss: 0.583280, acc: 73.44%] [G loss: 2.142076]\n",
      "epoch:14 step:13750 [D loss: 0.623112, acc: 63.28%] [G loss: 2.101857]\n",
      "epoch:14 step:13751 [D loss: 0.665944, acc: 66.41%] [G loss: 1.926763]\n",
      "epoch:14 step:13752 [D loss: 0.613258, acc: 67.97%] [G loss: 2.175457]\n",
      "epoch:14 step:13753 [D loss: 0.598598, acc: 69.53%] [G loss: 1.975642]\n",
      "epoch:14 step:13754 [D loss: 0.694194, acc: 59.38%] [G loss: 1.893086]\n",
      "epoch:14 step:13755 [D loss: 0.666261, acc: 60.16%] [G loss: 1.931958]\n",
      "epoch:14 step:13756 [D loss: 0.579030, acc: 68.75%] [G loss: 2.001097]\n",
      "epoch:14 step:13757 [D loss: 0.673528, acc: 58.59%] [G loss: 1.884781]\n",
      "epoch:14 step:13758 [D loss: 0.684234, acc: 55.47%] [G loss: 1.897279]\n",
      "epoch:14 step:13759 [D loss: 0.627715, acc: 60.94%] [G loss: 1.999513]\n",
      "epoch:14 step:13760 [D loss: 0.639175, acc: 61.72%] [G loss: 2.018751]\n",
      "epoch:14 step:13761 [D loss: 0.641156, acc: 64.84%] [G loss: 2.125490]\n",
      "epoch:14 step:13762 [D loss: 0.641761, acc: 63.28%] [G loss: 1.966675]\n",
      "epoch:14 step:13763 [D loss: 0.606850, acc: 64.06%] [G loss: 2.024076]\n",
      "epoch:14 step:13764 [D loss: 0.674240, acc: 62.50%] [G loss: 2.029914]\n",
      "epoch:14 step:13765 [D loss: 0.575962, acc: 75.00%] [G loss: 2.086467]\n",
      "epoch:14 step:13766 [D loss: 0.573606, acc: 70.31%] [G loss: 2.456030]\n",
      "epoch:14 step:13767 [D loss: 0.578799, acc: 68.75%] [G loss: 2.060519]\n",
      "epoch:14 step:13768 [D loss: 0.607509, acc: 66.41%] [G loss: 2.055409]\n",
      "epoch:14 step:13769 [D loss: 0.659236, acc: 57.81%] [G loss: 2.029758]\n",
      "epoch:14 step:13770 [D loss: 0.653765, acc: 59.38%] [G loss: 1.918867]\n",
      "epoch:14 step:13771 [D loss: 0.649976, acc: 64.84%] [G loss: 1.975745]\n",
      "epoch:14 step:13772 [D loss: 0.603441, acc: 67.97%] [G loss: 2.135128]\n",
      "epoch:14 step:13773 [D loss: 0.660222, acc: 61.72%] [G loss: 1.831000]\n",
      "epoch:14 step:13774 [D loss: 0.637706, acc: 65.62%] [G loss: 1.917121]\n",
      "epoch:14 step:13775 [D loss: 0.698249, acc: 55.47%] [G loss: 1.815518]\n",
      "epoch:14 step:13776 [D loss: 0.688450, acc: 56.25%] [G loss: 1.856970]\n",
      "epoch:14 step:13777 [D loss: 0.652340, acc: 60.94%] [G loss: 1.948932]\n",
      "epoch:14 step:13778 [D loss: 0.639197, acc: 64.84%] [G loss: 2.040377]\n",
      "epoch:14 step:13779 [D loss: 0.645690, acc: 59.38%] [G loss: 1.926893]\n",
      "epoch:14 step:13780 [D loss: 0.633917, acc: 63.28%] [G loss: 2.062765]\n",
      "epoch:14 step:13781 [D loss: 0.674993, acc: 59.38%] [G loss: 1.815013]\n",
      "epoch:14 step:13782 [D loss: 0.618452, acc: 66.41%] [G loss: 1.926286]\n",
      "epoch:14 step:13783 [D loss: 0.672491, acc: 60.16%] [G loss: 1.943063]\n",
      "epoch:14 step:13784 [D loss: 0.645314, acc: 61.72%] [G loss: 1.912406]\n",
      "epoch:14 step:13785 [D loss: 0.685166, acc: 55.47%] [G loss: 1.805043]\n",
      "epoch:14 step:13786 [D loss: 0.642542, acc: 63.28%] [G loss: 1.871014]\n",
      "epoch:14 step:13787 [D loss: 0.694603, acc: 59.38%] [G loss: 1.901849]\n",
      "epoch:14 step:13788 [D loss: 0.650772, acc: 60.94%] [G loss: 1.892141]\n",
      "epoch:14 step:13789 [D loss: 0.652722, acc: 61.72%] [G loss: 1.790361]\n",
      "epoch:14 step:13790 [D loss: 0.659706, acc: 57.03%] [G loss: 1.946934]\n",
      "epoch:14 step:13791 [D loss: 0.624200, acc: 65.62%] [G loss: 1.937452]\n",
      "epoch:14 step:13792 [D loss: 0.635276, acc: 61.72%] [G loss: 1.878297]\n",
      "epoch:14 step:13793 [D loss: 0.680612, acc: 59.38%] [G loss: 1.756707]\n",
      "epoch:14 step:13794 [D loss: 0.646855, acc: 60.94%] [G loss: 1.861522]\n",
      "epoch:14 step:13795 [D loss: 0.600230, acc: 71.88%] [G loss: 1.970648]\n",
      "epoch:14 step:13796 [D loss: 0.659589, acc: 57.81%] [G loss: 1.943074]\n",
      "epoch:14 step:13797 [D loss: 0.608694, acc: 69.53%] [G loss: 1.993434]\n",
      "epoch:14 step:13798 [D loss: 0.663866, acc: 61.72%] [G loss: 1.961856]\n",
      "epoch:14 step:13799 [D loss: 0.571466, acc: 74.22%] [G loss: 1.934772]\n",
      "epoch:14 step:13800 [D loss: 0.697139, acc: 59.38%] [G loss: 1.860030]\n",
      "epoch:14 step:13801 [D loss: 0.693109, acc: 58.59%] [G loss: 1.867409]\n",
      "epoch:14 step:13802 [D loss: 0.598671, acc: 68.75%] [G loss: 1.807265]\n",
      "epoch:14 step:13803 [D loss: 0.653564, acc: 62.50%] [G loss: 1.968537]\n",
      "epoch:14 step:13804 [D loss: 0.632447, acc: 61.72%] [G loss: 1.962487]\n",
      "epoch:14 step:13805 [D loss: 0.611949, acc: 64.84%] [G loss: 1.819610]\n",
      "epoch:14 step:13806 [D loss: 0.629960, acc: 70.31%] [G loss: 1.894211]\n",
      "epoch:14 step:13807 [D loss: 0.639946, acc: 61.72%] [G loss: 2.054753]\n",
      "epoch:14 step:13808 [D loss: 0.619227, acc: 65.62%] [G loss: 2.010451]\n",
      "epoch:14 step:13809 [D loss: 0.671431, acc: 57.03%] [G loss: 2.228699]\n",
      "epoch:14 step:13810 [D loss: 0.614452, acc: 67.97%] [G loss: 1.963332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13811 [D loss: 0.599065, acc: 73.44%] [G loss: 1.992921]\n",
      "epoch:14 step:13812 [D loss: 0.587777, acc: 73.44%] [G loss: 2.269116]\n",
      "epoch:14 step:13813 [D loss: 0.604526, acc: 70.31%] [G loss: 2.019739]\n",
      "epoch:14 step:13814 [D loss: 0.693234, acc: 56.25%] [G loss: 1.882626]\n",
      "epoch:14 step:13815 [D loss: 0.646022, acc: 63.28%] [G loss: 1.890547]\n",
      "epoch:14 step:13816 [D loss: 0.628049, acc: 64.06%] [G loss: 1.885811]\n",
      "epoch:14 step:13817 [D loss: 0.646568, acc: 65.62%] [G loss: 2.003063]\n",
      "epoch:14 step:13818 [D loss: 0.670671, acc: 57.81%] [G loss: 1.904322]\n",
      "epoch:14 step:13819 [D loss: 0.617714, acc: 70.31%] [G loss: 1.869438]\n",
      "epoch:14 step:13820 [D loss: 0.719703, acc: 56.25%] [G loss: 1.887693]\n",
      "epoch:14 step:13821 [D loss: 0.656009, acc: 61.72%] [G loss: 1.813558]\n",
      "epoch:14 step:13822 [D loss: 0.659914, acc: 58.59%] [G loss: 1.882473]\n",
      "epoch:14 step:13823 [D loss: 0.665979, acc: 60.16%] [G loss: 1.810406]\n",
      "epoch:14 step:13824 [D loss: 0.606355, acc: 66.41%] [G loss: 2.020429]\n",
      "epoch:14 step:13825 [D loss: 0.623648, acc: 66.41%] [G loss: 2.107479]\n",
      "epoch:14 step:13826 [D loss: 0.590979, acc: 64.84%] [G loss: 2.089419]\n",
      "epoch:14 step:13827 [D loss: 0.599954, acc: 68.75%] [G loss: 2.050659]\n",
      "epoch:14 step:13828 [D loss: 0.626779, acc: 69.53%] [G loss: 1.941853]\n",
      "epoch:14 step:13829 [D loss: 0.634779, acc: 61.72%] [G loss: 1.914187]\n",
      "epoch:14 step:13830 [D loss: 0.627208, acc: 67.97%] [G loss: 2.113273]\n",
      "epoch:14 step:13831 [D loss: 0.635054, acc: 60.94%] [G loss: 1.957260]\n",
      "epoch:14 step:13832 [D loss: 0.652839, acc: 61.72%] [G loss: 2.081845]\n",
      "epoch:14 step:13833 [D loss: 0.591091, acc: 71.88%] [G loss: 1.936209]\n",
      "epoch:14 step:13834 [D loss: 0.599200, acc: 66.41%] [G loss: 1.923177]\n",
      "epoch:14 step:13835 [D loss: 0.601665, acc: 65.62%] [G loss: 1.947168]\n",
      "epoch:14 step:13836 [D loss: 0.673179, acc: 59.38%] [G loss: 1.963488]\n",
      "epoch:14 step:13837 [D loss: 0.622436, acc: 64.84%] [G loss: 2.071205]\n",
      "epoch:14 step:13838 [D loss: 0.639822, acc: 63.28%] [G loss: 1.798398]\n",
      "epoch:14 step:13839 [D loss: 0.653046, acc: 59.38%] [G loss: 2.003894]\n",
      "epoch:14 step:13840 [D loss: 0.661345, acc: 61.72%] [G loss: 1.989351]\n",
      "epoch:14 step:13841 [D loss: 0.638353, acc: 60.16%] [G loss: 1.815533]\n",
      "epoch:14 step:13842 [D loss: 0.631973, acc: 68.75%] [G loss: 1.925470]\n",
      "epoch:14 step:13843 [D loss: 0.640079, acc: 64.84%] [G loss: 1.970884]\n",
      "epoch:14 step:13844 [D loss: 0.632269, acc: 60.94%] [G loss: 2.004105]\n",
      "epoch:14 step:13845 [D loss: 0.666350, acc: 60.16%] [G loss: 1.941054]\n",
      "epoch:14 step:13846 [D loss: 0.625340, acc: 62.50%] [G loss: 2.011621]\n",
      "epoch:14 step:13847 [D loss: 0.644362, acc: 63.28%] [G loss: 2.074804]\n",
      "epoch:14 step:13848 [D loss: 0.647877, acc: 63.28%] [G loss: 1.878495]\n",
      "epoch:14 step:13849 [D loss: 0.613469, acc: 68.75%] [G loss: 2.086190]\n",
      "epoch:14 step:13850 [D loss: 0.608278, acc: 65.62%] [G loss: 1.834034]\n",
      "epoch:14 step:13851 [D loss: 0.604999, acc: 64.06%] [G loss: 1.949842]\n",
      "epoch:14 step:13852 [D loss: 0.692814, acc: 51.56%] [G loss: 1.940947]\n",
      "epoch:14 step:13853 [D loss: 0.648981, acc: 66.41%] [G loss: 2.039443]\n",
      "epoch:14 step:13854 [D loss: 0.631498, acc: 65.62%] [G loss: 1.966656]\n",
      "epoch:14 step:13855 [D loss: 0.648643, acc: 64.06%] [G loss: 1.923996]\n",
      "epoch:14 step:13856 [D loss: 0.680641, acc: 60.16%] [G loss: 1.896409]\n",
      "epoch:14 step:13857 [D loss: 0.664241, acc: 60.16%] [G loss: 1.813690]\n",
      "epoch:14 step:13858 [D loss: 0.627223, acc: 64.06%] [G loss: 2.047472]\n",
      "epoch:14 step:13859 [D loss: 0.665014, acc: 57.81%] [G loss: 1.776068]\n",
      "epoch:14 step:13860 [D loss: 0.614147, acc: 68.75%] [G loss: 1.995172]\n",
      "epoch:14 step:13861 [D loss: 0.626591, acc: 60.16%] [G loss: 1.794444]\n",
      "epoch:14 step:13862 [D loss: 0.687129, acc: 65.62%] [G loss: 1.822397]\n",
      "epoch:14 step:13863 [D loss: 0.647228, acc: 64.84%] [G loss: 1.934196]\n",
      "epoch:14 step:13864 [D loss: 0.599881, acc: 69.53%] [G loss: 2.084216]\n",
      "epoch:14 step:13865 [D loss: 0.632002, acc: 64.84%] [G loss: 2.036709]\n",
      "epoch:14 step:13866 [D loss: 0.628990, acc: 64.84%] [G loss: 1.796398]\n",
      "epoch:14 step:13867 [D loss: 0.653883, acc: 61.72%] [G loss: 1.763928]\n",
      "epoch:14 step:13868 [D loss: 0.647395, acc: 59.38%] [G loss: 1.895568]\n",
      "epoch:14 step:13869 [D loss: 0.672737, acc: 57.81%] [G loss: 1.925644]\n",
      "epoch:14 step:13870 [D loss: 0.643968, acc: 71.09%] [G loss: 1.793911]\n",
      "epoch:14 step:13871 [D loss: 0.621842, acc: 61.72%] [G loss: 1.939589]\n",
      "epoch:14 step:13872 [D loss: 0.641605, acc: 64.84%] [G loss: 1.981678]\n",
      "epoch:14 step:13873 [D loss: 0.656633, acc: 59.38%] [G loss: 1.885057]\n",
      "epoch:14 step:13874 [D loss: 0.595767, acc: 69.53%] [G loss: 1.895470]\n",
      "epoch:14 step:13875 [D loss: 0.601474, acc: 68.75%] [G loss: 1.952325]\n",
      "epoch:14 step:13876 [D loss: 0.665689, acc: 64.06%] [G loss: 1.958076]\n",
      "epoch:14 step:13877 [D loss: 0.638839, acc: 64.06%] [G loss: 1.859014]\n",
      "epoch:14 step:13878 [D loss: 0.594498, acc: 67.97%] [G loss: 1.956412]\n",
      "epoch:14 step:13879 [D loss: 0.665597, acc: 66.41%] [G loss: 1.924110]\n",
      "epoch:14 step:13880 [D loss: 0.709544, acc: 52.34%] [G loss: 1.757397]\n",
      "epoch:14 step:13881 [D loss: 0.656242, acc: 59.38%] [G loss: 1.995716]\n",
      "epoch:14 step:13882 [D loss: 0.610684, acc: 67.97%] [G loss: 1.943777]\n",
      "epoch:14 step:13883 [D loss: 0.677803, acc: 59.38%] [G loss: 1.797974]\n",
      "epoch:14 step:13884 [D loss: 0.721059, acc: 53.12%] [G loss: 1.854617]\n",
      "epoch:14 step:13885 [D loss: 0.603392, acc: 72.66%] [G loss: 1.895460]\n",
      "epoch:14 step:13886 [D loss: 0.660296, acc: 59.38%] [G loss: 1.914752]\n",
      "epoch:14 step:13887 [D loss: 0.649899, acc: 63.28%] [G loss: 1.862580]\n",
      "epoch:14 step:13888 [D loss: 0.601925, acc: 67.97%] [G loss: 2.008639]\n",
      "epoch:14 step:13889 [D loss: 0.628370, acc: 59.38%] [G loss: 1.936243]\n",
      "epoch:14 step:13890 [D loss: 0.602409, acc: 65.62%] [G loss: 1.855811]\n",
      "epoch:14 step:13891 [D loss: 0.641892, acc: 60.94%] [G loss: 2.058342]\n",
      "epoch:14 step:13892 [D loss: 0.637303, acc: 59.38%] [G loss: 2.128707]\n",
      "epoch:14 step:13893 [D loss: 0.660584, acc: 62.50%] [G loss: 2.119260]\n",
      "epoch:14 step:13894 [D loss: 0.647009, acc: 56.25%] [G loss: 1.937302]\n",
      "epoch:14 step:13895 [D loss: 0.628131, acc: 67.19%] [G loss: 2.034488]\n",
      "epoch:14 step:13896 [D loss: 0.626281, acc: 63.28%] [G loss: 1.822360]\n",
      "epoch:14 step:13897 [D loss: 0.635662, acc: 63.28%] [G loss: 1.939387]\n",
      "epoch:14 step:13898 [D loss: 0.624954, acc: 64.84%] [G loss: 1.982485]\n",
      "epoch:14 step:13899 [D loss: 0.580502, acc: 71.09%] [G loss: 2.069553]\n",
      "epoch:14 step:13900 [D loss: 0.607049, acc: 64.84%] [G loss: 2.220059]\n",
      "epoch:14 step:13901 [D loss: 0.650631, acc: 59.38%] [G loss: 2.090491]\n",
      "epoch:14 step:13902 [D loss: 0.695297, acc: 60.16%] [G loss: 1.868048]\n",
      "epoch:14 step:13903 [D loss: 0.670270, acc: 61.72%] [G loss: 1.834701]\n",
      "epoch:14 step:13904 [D loss: 0.686723, acc: 58.59%] [G loss: 1.929190]\n",
      "epoch:14 step:13905 [D loss: 0.649949, acc: 60.94%] [G loss: 1.864958]\n",
      "epoch:14 step:13906 [D loss: 0.703278, acc: 57.81%] [G loss: 1.805551]\n",
      "epoch:14 step:13907 [D loss: 0.677122, acc: 58.59%] [G loss: 1.950528]\n",
      "epoch:14 step:13908 [D loss: 0.627729, acc: 66.41%] [G loss: 1.780780]\n",
      "epoch:14 step:13909 [D loss: 0.623524, acc: 64.84%] [G loss: 2.015683]\n",
      "epoch:14 step:13910 [D loss: 0.577883, acc: 71.88%] [G loss: 2.166257]\n",
      "epoch:14 step:13911 [D loss: 0.632316, acc: 72.66%] [G loss: 2.051032]\n",
      "epoch:14 step:13912 [D loss: 0.703156, acc: 53.91%] [G loss: 1.861978]\n",
      "epoch:14 step:13913 [D loss: 0.655358, acc: 63.28%] [G loss: 1.896939]\n",
      "epoch:14 step:13914 [D loss: 0.628668, acc: 63.28%] [G loss: 1.978680]\n",
      "epoch:14 step:13915 [D loss: 0.685184, acc: 60.16%] [G loss: 1.829665]\n",
      "epoch:14 step:13916 [D loss: 0.609895, acc: 67.97%] [G loss: 1.917705]\n",
      "epoch:14 step:13917 [D loss: 0.597822, acc: 74.22%] [G loss: 1.937160]\n",
      "epoch:14 step:13918 [D loss: 0.651138, acc: 62.50%] [G loss: 1.739490]\n",
      "epoch:14 step:13919 [D loss: 0.669395, acc: 61.72%] [G loss: 1.989142]\n",
      "epoch:14 step:13920 [D loss: 0.623236, acc: 66.41%] [G loss: 2.117273]\n",
      "epoch:14 step:13921 [D loss: 0.650306, acc: 61.72%] [G loss: 1.829587]\n",
      "epoch:14 step:13922 [D loss: 0.624413, acc: 61.72%] [G loss: 1.934585]\n",
      "epoch:14 step:13923 [D loss: 0.632878, acc: 65.62%] [G loss: 2.042415]\n",
      "epoch:14 step:13924 [D loss: 0.634683, acc: 63.28%] [G loss: 1.991395]\n",
      "epoch:14 step:13925 [D loss: 0.627693, acc: 63.28%] [G loss: 2.048643]\n",
      "epoch:14 step:13926 [D loss: 0.612942, acc: 69.53%] [G loss: 2.261382]\n",
      "epoch:14 step:13927 [D loss: 0.653985, acc: 68.75%] [G loss: 2.098687]\n",
      "epoch:14 step:13928 [D loss: 0.589156, acc: 70.31%] [G loss: 1.870832]\n",
      "epoch:14 step:13929 [D loss: 0.648633, acc: 60.94%] [G loss: 1.929501]\n",
      "epoch:14 step:13930 [D loss: 0.672556, acc: 63.28%] [G loss: 1.853328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:13931 [D loss: 0.687196, acc: 59.38%] [G loss: 1.759153]\n",
      "epoch:14 step:13932 [D loss: 0.554647, acc: 75.78%] [G loss: 2.007545]\n",
      "epoch:14 step:13933 [D loss: 0.678687, acc: 61.72%] [G loss: 2.222137]\n",
      "epoch:14 step:13934 [D loss: 0.640338, acc: 58.59%] [G loss: 2.065477]\n",
      "epoch:14 step:13935 [D loss: 0.679285, acc: 60.94%] [G loss: 1.919670]\n",
      "epoch:14 step:13936 [D loss: 0.623107, acc: 65.62%] [G loss: 1.913218]\n",
      "epoch:14 step:13937 [D loss: 0.612651, acc: 72.66%] [G loss: 1.845169]\n",
      "epoch:14 step:13938 [D loss: 0.739115, acc: 50.00%] [G loss: 1.616572]\n",
      "epoch:14 step:13939 [D loss: 0.704476, acc: 57.81%] [G loss: 1.901954]\n",
      "epoch:14 step:13940 [D loss: 0.592781, acc: 71.09%] [G loss: 2.042372]\n",
      "epoch:14 step:13941 [D loss: 0.642823, acc: 60.16%] [G loss: 1.959821]\n",
      "epoch:14 step:13942 [D loss: 0.673960, acc: 57.81%] [G loss: 2.018880]\n",
      "epoch:14 step:13943 [D loss: 0.617342, acc: 68.75%] [G loss: 1.913157]\n",
      "epoch:14 step:13944 [D loss: 0.661508, acc: 57.81%] [G loss: 1.789829]\n",
      "epoch:14 step:13945 [D loss: 0.651106, acc: 60.94%] [G loss: 1.841564]\n",
      "epoch:14 step:13946 [D loss: 0.668243, acc: 57.03%] [G loss: 1.762308]\n",
      "epoch:14 step:13947 [D loss: 0.722358, acc: 52.34%] [G loss: 1.808734]\n",
      "epoch:14 step:13948 [D loss: 0.687410, acc: 58.59%] [G loss: 1.890055]\n",
      "epoch:14 step:13949 [D loss: 0.630518, acc: 62.50%] [G loss: 1.877767]\n",
      "epoch:14 step:13950 [D loss: 0.671612, acc: 57.81%] [G loss: 1.787314]\n",
      "epoch:14 step:13951 [D loss: 0.626390, acc: 64.84%] [G loss: 1.960415]\n",
      "epoch:14 step:13952 [D loss: 0.663912, acc: 60.94%] [G loss: 1.984876]\n",
      "epoch:14 step:13953 [D loss: 0.650355, acc: 59.38%] [G loss: 1.838209]\n",
      "epoch:14 step:13954 [D loss: 0.607481, acc: 63.28%] [G loss: 2.053662]\n",
      "epoch:14 step:13955 [D loss: 0.693558, acc: 64.84%] [G loss: 1.980621]\n",
      "epoch:14 step:13956 [D loss: 0.653246, acc: 60.16%] [G loss: 1.840970]\n",
      "epoch:14 step:13957 [D loss: 0.594512, acc: 71.09%] [G loss: 1.942265]\n",
      "epoch:14 step:13958 [D loss: 0.613052, acc: 67.97%] [G loss: 1.963019]\n",
      "epoch:14 step:13959 [D loss: 0.592790, acc: 65.62%] [G loss: 1.931467]\n",
      "epoch:14 step:13960 [D loss: 0.609456, acc: 64.84%] [G loss: 1.867003]\n",
      "epoch:14 step:13961 [D loss: 0.636131, acc: 66.41%] [G loss: 2.116647]\n",
      "epoch:14 step:13962 [D loss: 0.642151, acc: 63.28%] [G loss: 1.971591]\n",
      "epoch:14 step:13963 [D loss: 0.629521, acc: 64.06%] [G loss: 1.930027]\n",
      "epoch:14 step:13964 [D loss: 0.683665, acc: 59.38%] [G loss: 1.887405]\n",
      "epoch:14 step:13965 [D loss: 0.661089, acc: 61.72%] [G loss: 2.018747]\n",
      "epoch:14 step:13966 [D loss: 0.618361, acc: 67.19%] [G loss: 1.958701]\n",
      "epoch:14 step:13967 [D loss: 0.623225, acc: 67.97%] [G loss: 1.990274]\n",
      "epoch:14 step:13968 [D loss: 0.649987, acc: 63.28%] [G loss: 1.943135]\n",
      "epoch:14 step:13969 [D loss: 0.631611, acc: 65.62%] [G loss: 2.024766]\n",
      "epoch:14 step:13970 [D loss: 0.647644, acc: 62.50%] [G loss: 1.856396]\n",
      "epoch:14 step:13971 [D loss: 0.666063, acc: 56.25%] [G loss: 1.990214]\n",
      "epoch:14 step:13972 [D loss: 0.663621, acc: 60.16%] [G loss: 1.847717]\n",
      "epoch:14 step:13973 [D loss: 0.667107, acc: 57.81%] [G loss: 1.824596]\n",
      "epoch:14 step:13974 [D loss: 0.692447, acc: 53.91%] [G loss: 1.779678]\n",
      "epoch:14 step:13975 [D loss: 0.616812, acc: 64.84%] [G loss: 1.846869]\n",
      "epoch:14 step:13976 [D loss: 0.700078, acc: 52.34%] [G loss: 1.816129]\n",
      "epoch:14 step:13977 [D loss: 0.713097, acc: 51.56%] [G loss: 1.889596]\n",
      "epoch:14 step:13978 [D loss: 0.620370, acc: 66.41%] [G loss: 1.899622]\n",
      "epoch:14 step:13979 [D loss: 0.632507, acc: 67.97%] [G loss: 1.831537]\n",
      "epoch:14 step:13980 [D loss: 0.632535, acc: 64.84%] [G loss: 1.804642]\n",
      "epoch:14 step:13981 [D loss: 0.629433, acc: 62.50%] [G loss: 1.823417]\n",
      "epoch:14 step:13982 [D loss: 0.629073, acc: 61.72%] [G loss: 1.934512]\n",
      "epoch:14 step:13983 [D loss: 0.673658, acc: 57.03%] [G loss: 1.967656]\n",
      "epoch:14 step:13984 [D loss: 0.672816, acc: 56.25%] [G loss: 1.898287]\n",
      "epoch:14 step:13985 [D loss: 0.647269, acc: 62.50%] [G loss: 1.843551]\n",
      "epoch:14 step:13986 [D loss: 0.629793, acc: 64.06%] [G loss: 1.969527]\n",
      "epoch:14 step:13987 [D loss: 0.688761, acc: 62.50%] [G loss: 1.833110]\n",
      "epoch:14 step:13988 [D loss: 0.678272, acc: 60.94%] [G loss: 1.782535]\n",
      "epoch:14 step:13989 [D loss: 0.610331, acc: 64.06%] [G loss: 1.972272]\n",
      "epoch:14 step:13990 [D loss: 0.630330, acc: 64.06%] [G loss: 1.816493]\n",
      "epoch:14 step:13991 [D loss: 0.693127, acc: 56.25%] [G loss: 1.717503]\n",
      "epoch:14 step:13992 [D loss: 0.618208, acc: 63.28%] [G loss: 1.932169]\n",
      "epoch:14 step:13993 [D loss: 0.615660, acc: 70.31%] [G loss: 2.027398]\n",
      "epoch:14 step:13994 [D loss: 0.665390, acc: 62.50%] [G loss: 2.054676]\n",
      "epoch:14 step:13995 [D loss: 0.641156, acc: 60.94%] [G loss: 1.884659]\n",
      "epoch:14 step:13996 [D loss: 0.640754, acc: 62.50%] [G loss: 1.923788]\n",
      "epoch:14 step:13997 [D loss: 0.673086, acc: 54.69%] [G loss: 1.984510]\n",
      "epoch:14 step:13998 [D loss: 0.647731, acc: 64.84%] [G loss: 1.873901]\n",
      "epoch:14 step:13999 [D loss: 0.643037, acc: 61.72%] [G loss: 1.922783]\n",
      "epoch:14 step:14000 [D loss: 0.686205, acc: 62.50%] [G loss: 1.971377]\n",
      "epoch:14 step:14001 [D loss: 0.635054, acc: 68.75%] [G loss: 2.068989]\n",
      "epoch:14 step:14002 [D loss: 0.570536, acc: 69.53%] [G loss: 2.120047]\n",
      "epoch:14 step:14003 [D loss: 0.640011, acc: 63.28%] [G loss: 1.964296]\n",
      "epoch:14 step:14004 [D loss: 0.608764, acc: 64.06%] [G loss: 2.104552]\n",
      "epoch:14 step:14005 [D loss: 0.678583, acc: 61.72%] [G loss: 1.898701]\n",
      "epoch:14 step:14006 [D loss: 0.655160, acc: 60.94%] [G loss: 1.947204]\n",
      "epoch:14 step:14007 [D loss: 0.576461, acc: 71.09%] [G loss: 1.906322]\n",
      "epoch:14 step:14008 [D loss: 0.560506, acc: 75.00%] [G loss: 2.064505]\n",
      "epoch:14 step:14009 [D loss: 0.689614, acc: 54.69%] [G loss: 1.902026]\n",
      "epoch:14 step:14010 [D loss: 0.669788, acc: 57.03%] [G loss: 1.917284]\n",
      "epoch:14 step:14011 [D loss: 0.643079, acc: 61.72%] [G loss: 2.017625]\n",
      "epoch:14 step:14012 [D loss: 0.656277, acc: 63.28%] [G loss: 1.922274]\n",
      "epoch:14 step:14013 [D loss: 0.648987, acc: 66.41%] [G loss: 1.929574]\n",
      "epoch:14 step:14014 [D loss: 0.671138, acc: 64.84%] [G loss: 1.959829]\n",
      "epoch:14 step:14015 [D loss: 0.650280, acc: 63.28%] [G loss: 1.942095]\n",
      "epoch:14 step:14016 [D loss: 0.669788, acc: 64.06%] [G loss: 2.039793]\n",
      "epoch:14 step:14017 [D loss: 0.613432, acc: 65.62%] [G loss: 2.051212]\n",
      "epoch:14 step:14018 [D loss: 0.645914, acc: 66.41%] [G loss: 1.921680]\n",
      "epoch:14 step:14019 [D loss: 0.647794, acc: 62.50%] [G loss: 1.924233]\n",
      "epoch:14 step:14020 [D loss: 0.657130, acc: 64.06%] [G loss: 1.951979]\n",
      "epoch:14 step:14021 [D loss: 0.639724, acc: 59.38%] [G loss: 1.818928]\n",
      "epoch:14 step:14022 [D loss: 0.686796, acc: 57.03%] [G loss: 1.981574]\n",
      "epoch:14 step:14023 [D loss: 0.606009, acc: 69.53%] [G loss: 1.824152]\n",
      "epoch:14 step:14024 [D loss: 0.651536, acc: 64.06%] [G loss: 2.097449]\n",
      "epoch:14 step:14025 [D loss: 0.662196, acc: 61.72%] [G loss: 2.042419]\n",
      "epoch:14 step:14026 [D loss: 0.590071, acc: 70.31%] [G loss: 1.921194]\n",
      "epoch:14 step:14027 [D loss: 0.610449, acc: 68.75%] [G loss: 2.101439]\n",
      "epoch:14 step:14028 [D loss: 0.613130, acc: 70.31%] [G loss: 2.069222]\n",
      "epoch:14 step:14029 [D loss: 0.637511, acc: 60.94%] [G loss: 1.876397]\n",
      "epoch:14 step:14030 [D loss: 0.610906, acc: 64.06%] [G loss: 2.167537]\n",
      "epoch:14 step:14031 [D loss: 0.616188, acc: 64.06%] [G loss: 2.110069]\n",
      "epoch:14 step:14032 [D loss: 0.704957, acc: 57.03%] [G loss: 1.874000]\n",
      "epoch:14 step:14033 [D loss: 0.681646, acc: 58.59%] [G loss: 1.914283]\n",
      "epoch:14 step:14034 [D loss: 0.635819, acc: 67.19%] [G loss: 2.092879]\n",
      "epoch:14 step:14035 [D loss: 0.642788, acc: 64.84%] [G loss: 2.031288]\n",
      "epoch:14 step:14036 [D loss: 0.573165, acc: 75.00%] [G loss: 2.170026]\n",
      "epoch:14 step:14037 [D loss: 0.544483, acc: 74.22%] [G loss: 2.196436]\n",
      "epoch:14 step:14038 [D loss: 0.723180, acc: 57.03%] [G loss: 1.798083]\n",
      "epoch:14 step:14039 [D loss: 0.657096, acc: 61.72%] [G loss: 1.859815]\n",
      "epoch:14 step:14040 [D loss: 0.653049, acc: 60.94%] [G loss: 1.908085]\n",
      "epoch:14 step:14041 [D loss: 0.544554, acc: 77.34%] [G loss: 2.158974]\n",
      "epoch:14 step:14042 [D loss: 0.592177, acc: 76.56%] [G loss: 2.346778]\n",
      "epoch:14 step:14043 [D loss: 0.589661, acc: 70.31%] [G loss: 2.258065]\n",
      "epoch:14 step:14044 [D loss: 0.560380, acc: 71.09%] [G loss: 2.146480]\n",
      "epoch:14 step:14045 [D loss: 0.569306, acc: 68.75%] [G loss: 2.204350]\n",
      "epoch:14 step:14046 [D loss: 0.790456, acc: 51.56%] [G loss: 1.868822]\n",
      "epoch:14 step:14047 [D loss: 0.706694, acc: 53.12%] [G loss: 2.005880]\n",
      "epoch:14 step:14048 [D loss: 0.639562, acc: 63.28%] [G loss: 2.073623]\n",
      "epoch:14 step:14049 [D loss: 0.563572, acc: 71.09%] [G loss: 2.117362]\n",
      "epoch:14 step:14050 [D loss: 0.575314, acc: 73.44%] [G loss: 1.998307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:14051 [D loss: 0.582359, acc: 72.66%] [G loss: 1.990541]\n",
      "epoch:14 step:14052 [D loss: 0.675109, acc: 57.81%] [G loss: 2.038674]\n",
      "epoch:14 step:14053 [D loss: 0.610028, acc: 63.28%] [G loss: 2.089776]\n",
      "epoch:14 step:14054 [D loss: 0.561899, acc: 75.00%] [G loss: 2.351943]\n",
      "epoch:14 step:14055 [D loss: 0.555364, acc: 73.44%] [G loss: 2.580332]\n",
      "epoch:15 step:14056 [D loss: 0.657348, acc: 62.50%] [G loss: 1.931244]\n",
      "epoch:15 step:14057 [D loss: 0.619555, acc: 67.97%] [G loss: 2.070883]\n",
      "epoch:15 step:14058 [D loss: 0.652102, acc: 62.50%] [G loss: 2.070833]\n",
      "epoch:15 step:14059 [D loss: 0.639125, acc: 66.41%] [G loss: 1.992893]\n",
      "epoch:15 step:14060 [D loss: 0.675974, acc: 60.94%] [G loss: 2.030786]\n",
      "epoch:15 step:14061 [D loss: 0.647347, acc: 60.94%] [G loss: 2.125695]\n",
      "epoch:15 step:14062 [D loss: 0.629587, acc: 64.06%] [G loss: 1.999094]\n",
      "epoch:15 step:14063 [D loss: 0.643667, acc: 65.62%] [G loss: 2.052997]\n",
      "epoch:15 step:14064 [D loss: 0.627634, acc: 67.19%] [G loss: 2.150411]\n",
      "epoch:15 step:14065 [D loss: 0.601523, acc: 71.09%] [G loss: 2.138220]\n",
      "epoch:15 step:14066 [D loss: 0.637772, acc: 66.41%] [G loss: 1.955902]\n",
      "epoch:15 step:14067 [D loss: 0.652317, acc: 62.50%] [G loss: 2.013725]\n",
      "epoch:15 step:14068 [D loss: 0.678077, acc: 64.06%] [G loss: 2.108393]\n",
      "epoch:15 step:14069 [D loss: 0.575594, acc: 67.19%] [G loss: 1.993784]\n",
      "epoch:15 step:14070 [D loss: 0.586185, acc: 64.84%] [G loss: 2.189814]\n",
      "epoch:15 step:14071 [D loss: 0.607259, acc: 63.28%] [G loss: 2.126741]\n",
      "epoch:15 step:14072 [D loss: 0.648090, acc: 60.94%] [G loss: 2.238526]\n",
      "epoch:15 step:14073 [D loss: 0.705404, acc: 60.94%] [G loss: 2.094074]\n",
      "epoch:15 step:14074 [D loss: 0.624799, acc: 65.62%] [G loss: 2.036319]\n",
      "epoch:15 step:14075 [D loss: 0.720814, acc: 59.38%] [G loss: 1.807905]\n",
      "epoch:15 step:14076 [D loss: 0.659021, acc: 57.81%] [G loss: 1.928319]\n",
      "epoch:15 step:14077 [D loss: 0.645745, acc: 59.38%] [G loss: 1.909760]\n",
      "epoch:15 step:14078 [D loss: 0.633214, acc: 64.84%] [G loss: 2.128601]\n",
      "epoch:15 step:14079 [D loss: 0.562179, acc: 76.56%] [G loss: 1.973412]\n",
      "epoch:15 step:14080 [D loss: 0.614342, acc: 65.62%] [G loss: 2.010242]\n",
      "epoch:15 step:14081 [D loss: 0.609736, acc: 73.44%] [G loss: 1.824810]\n",
      "epoch:15 step:14082 [D loss: 0.668869, acc: 57.03%] [G loss: 1.853781]\n",
      "epoch:15 step:14083 [D loss: 0.632513, acc: 66.41%] [G loss: 1.793031]\n",
      "epoch:15 step:14084 [D loss: 0.589043, acc: 69.53%] [G loss: 1.992794]\n",
      "epoch:15 step:14085 [D loss: 0.619177, acc: 71.09%] [G loss: 1.927001]\n",
      "epoch:15 step:14086 [D loss: 0.692107, acc: 60.16%] [G loss: 1.891319]\n",
      "epoch:15 step:14087 [D loss: 0.599352, acc: 69.53%] [G loss: 1.825754]\n",
      "epoch:15 step:14088 [D loss: 0.633872, acc: 69.53%] [G loss: 1.898644]\n",
      "epoch:15 step:14089 [D loss: 0.686573, acc: 60.94%] [G loss: 2.059921]\n",
      "epoch:15 step:14090 [D loss: 0.657462, acc: 59.38%] [G loss: 1.992405]\n",
      "epoch:15 step:14091 [D loss: 0.609799, acc: 71.09%] [G loss: 2.050674]\n",
      "epoch:15 step:14092 [D loss: 0.735092, acc: 55.47%] [G loss: 1.995806]\n",
      "epoch:15 step:14093 [D loss: 0.691164, acc: 58.59%] [G loss: 1.827256]\n",
      "epoch:15 step:14094 [D loss: 0.614454, acc: 72.66%] [G loss: 2.125835]\n",
      "epoch:15 step:14095 [D loss: 0.560928, acc: 74.22%] [G loss: 2.091955]\n",
      "epoch:15 step:14096 [D loss: 0.636317, acc: 62.50%] [G loss: 2.017340]\n",
      "epoch:15 step:14097 [D loss: 0.627311, acc: 65.62%] [G loss: 1.996170]\n",
      "epoch:15 step:14098 [D loss: 0.674524, acc: 57.81%] [G loss: 1.893022]\n",
      "epoch:15 step:14099 [D loss: 0.629657, acc: 61.72%] [G loss: 1.930303]\n",
      "epoch:15 step:14100 [D loss: 0.639120, acc: 62.50%] [G loss: 2.071856]\n",
      "epoch:15 step:14101 [D loss: 0.690788, acc: 57.03%] [G loss: 1.841193]\n",
      "epoch:15 step:14102 [D loss: 0.639985, acc: 60.94%] [G loss: 1.905123]\n",
      "epoch:15 step:14103 [D loss: 0.600157, acc: 65.62%] [G loss: 2.076379]\n",
      "epoch:15 step:14104 [D loss: 0.570616, acc: 71.88%] [G loss: 2.037220]\n",
      "epoch:15 step:14105 [D loss: 0.565763, acc: 72.66%] [G loss: 2.095475]\n",
      "epoch:15 step:14106 [D loss: 0.674859, acc: 65.62%] [G loss: 1.819906]\n",
      "epoch:15 step:14107 [D loss: 0.651280, acc: 61.72%] [G loss: 2.050500]\n",
      "epoch:15 step:14108 [D loss: 0.593386, acc: 65.62%] [G loss: 1.930070]\n",
      "epoch:15 step:14109 [D loss: 0.666143, acc: 64.06%] [G loss: 2.123051]\n",
      "epoch:15 step:14110 [D loss: 0.666234, acc: 64.06%] [G loss: 1.931298]\n",
      "epoch:15 step:14111 [D loss: 0.625405, acc: 63.28%] [G loss: 1.930271]\n",
      "epoch:15 step:14112 [D loss: 0.652063, acc: 67.19%] [G loss: 1.959037]\n",
      "epoch:15 step:14113 [D loss: 0.613922, acc: 65.62%] [G loss: 2.044585]\n",
      "epoch:15 step:14114 [D loss: 0.617465, acc: 63.28%] [G loss: 1.891147]\n",
      "epoch:15 step:14115 [D loss: 0.650697, acc: 61.72%] [G loss: 1.974118]\n",
      "epoch:15 step:14116 [D loss: 0.684485, acc: 53.91%] [G loss: 1.860523]\n",
      "epoch:15 step:14117 [D loss: 0.619873, acc: 65.62%] [G loss: 2.013540]\n",
      "epoch:15 step:14118 [D loss: 0.651460, acc: 68.75%] [G loss: 2.149759]\n",
      "epoch:15 step:14119 [D loss: 0.664295, acc: 64.06%] [G loss: 1.955186]\n",
      "epoch:15 step:14120 [D loss: 0.623823, acc: 62.50%] [G loss: 1.946876]\n",
      "epoch:15 step:14121 [D loss: 0.593268, acc: 68.75%] [G loss: 2.079782]\n",
      "epoch:15 step:14122 [D loss: 0.608654, acc: 67.19%] [G loss: 1.952670]\n",
      "epoch:15 step:14123 [D loss: 0.612772, acc: 68.75%] [G loss: 1.965254]\n",
      "epoch:15 step:14124 [D loss: 0.630700, acc: 64.84%] [G loss: 2.035776]\n",
      "epoch:15 step:14125 [D loss: 0.660955, acc: 59.38%] [G loss: 2.126829]\n",
      "epoch:15 step:14126 [D loss: 0.631876, acc: 67.19%] [G loss: 1.903042]\n",
      "epoch:15 step:14127 [D loss: 0.630881, acc: 67.19%] [G loss: 2.010471]\n",
      "epoch:15 step:14128 [D loss: 0.655111, acc: 62.50%] [G loss: 1.912008]\n",
      "epoch:15 step:14129 [D loss: 0.619723, acc: 62.50%] [G loss: 2.022765]\n",
      "epoch:15 step:14130 [D loss: 0.630781, acc: 57.81%] [G loss: 2.155007]\n",
      "epoch:15 step:14131 [D loss: 0.641448, acc: 64.84%] [G loss: 2.006407]\n",
      "epoch:15 step:14132 [D loss: 0.559037, acc: 73.44%] [G loss: 2.208174]\n",
      "epoch:15 step:14133 [D loss: 0.680874, acc: 55.47%] [G loss: 1.981003]\n",
      "epoch:15 step:14134 [D loss: 0.622878, acc: 65.62%] [G loss: 1.874626]\n",
      "epoch:15 step:14135 [D loss: 0.673687, acc: 57.81%] [G loss: 1.938218]\n",
      "epoch:15 step:14136 [D loss: 0.708951, acc: 56.25%] [G loss: 1.721815]\n",
      "epoch:15 step:14137 [D loss: 0.660516, acc: 60.16%] [G loss: 2.026593]\n",
      "epoch:15 step:14138 [D loss: 0.640635, acc: 64.84%] [G loss: 1.968214]\n",
      "epoch:15 step:14139 [D loss: 0.640146, acc: 59.38%] [G loss: 1.801998]\n",
      "epoch:15 step:14140 [D loss: 0.638021, acc: 57.81%] [G loss: 1.920429]\n",
      "epoch:15 step:14141 [D loss: 0.701276, acc: 63.28%] [G loss: 1.855028]\n",
      "epoch:15 step:14142 [D loss: 0.598267, acc: 66.41%] [G loss: 1.931438]\n",
      "epoch:15 step:14143 [D loss: 0.651601, acc: 60.94%] [G loss: 1.896059]\n",
      "epoch:15 step:14144 [D loss: 0.665160, acc: 64.06%] [G loss: 2.113132]\n",
      "epoch:15 step:14145 [D loss: 0.659835, acc: 61.72%] [G loss: 1.920273]\n",
      "epoch:15 step:14146 [D loss: 0.665116, acc: 62.50%] [G loss: 1.950754]\n",
      "epoch:15 step:14147 [D loss: 0.641648, acc: 57.03%] [G loss: 2.040497]\n",
      "epoch:15 step:14148 [D loss: 0.669356, acc: 55.47%] [G loss: 2.043748]\n",
      "epoch:15 step:14149 [D loss: 0.634626, acc: 65.62%] [G loss: 1.852164]\n",
      "epoch:15 step:14150 [D loss: 0.630698, acc: 62.50%] [G loss: 1.844749]\n",
      "epoch:15 step:14151 [D loss: 0.707494, acc: 53.91%] [G loss: 1.893391]\n",
      "epoch:15 step:14152 [D loss: 0.649229, acc: 62.50%] [G loss: 1.842567]\n",
      "epoch:15 step:14153 [D loss: 0.677967, acc: 56.25%] [G loss: 1.758177]\n",
      "epoch:15 step:14154 [D loss: 0.706214, acc: 56.25%] [G loss: 1.876988]\n",
      "epoch:15 step:14155 [D loss: 0.650122, acc: 64.06%] [G loss: 1.904806]\n",
      "epoch:15 step:14156 [D loss: 0.606068, acc: 64.84%] [G loss: 1.840030]\n",
      "epoch:15 step:14157 [D loss: 0.650805, acc: 61.72%] [G loss: 1.918226]\n",
      "epoch:15 step:14158 [D loss: 0.644033, acc: 57.81%] [G loss: 1.799695]\n",
      "epoch:15 step:14159 [D loss: 0.625969, acc: 64.06%] [G loss: 1.866902]\n",
      "epoch:15 step:14160 [D loss: 0.636623, acc: 67.97%] [G loss: 1.939839]\n",
      "epoch:15 step:14161 [D loss: 0.649264, acc: 59.38%] [G loss: 2.103014]\n",
      "epoch:15 step:14162 [D loss: 0.617175, acc: 65.62%] [G loss: 2.172139]\n",
      "epoch:15 step:14163 [D loss: 0.716939, acc: 56.25%] [G loss: 1.836359]\n",
      "epoch:15 step:14164 [D loss: 0.682771, acc: 52.34%] [G loss: 1.952285]\n",
      "epoch:15 step:14165 [D loss: 0.665188, acc: 63.28%] [G loss: 1.827683]\n",
      "epoch:15 step:14166 [D loss: 0.565223, acc: 73.44%] [G loss: 2.070312]\n",
      "epoch:15 step:14167 [D loss: 0.619040, acc: 62.50%] [G loss: 1.840425]\n",
      "epoch:15 step:14168 [D loss: 0.664438, acc: 57.03%] [G loss: 2.048160]\n",
      "epoch:15 step:14169 [D loss: 0.667307, acc: 57.03%] [G loss: 2.136180]\n",
      "epoch:15 step:14170 [D loss: 0.624069, acc: 62.50%] [G loss: 2.137532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14171 [D loss: 0.617447, acc: 64.06%] [G loss: 2.149564]\n",
      "epoch:15 step:14172 [D loss: 0.615297, acc: 64.84%] [G loss: 2.075564]\n",
      "epoch:15 step:14173 [D loss: 0.727826, acc: 51.56%] [G loss: 1.946695]\n",
      "epoch:15 step:14174 [D loss: 0.572633, acc: 69.53%] [G loss: 2.100149]\n",
      "epoch:15 step:14175 [D loss: 0.698284, acc: 55.47%] [G loss: 1.858668]\n",
      "epoch:15 step:14176 [D loss: 0.737443, acc: 57.03%] [G loss: 1.904994]\n",
      "epoch:15 step:14177 [D loss: 0.622845, acc: 66.41%] [G loss: 2.077298]\n",
      "epoch:15 step:14178 [D loss: 0.672535, acc: 58.59%] [G loss: 2.031449]\n",
      "epoch:15 step:14179 [D loss: 0.656855, acc: 60.16%] [G loss: 1.804216]\n",
      "epoch:15 step:14180 [D loss: 0.669180, acc: 59.38%] [G loss: 1.743714]\n",
      "epoch:15 step:14181 [D loss: 0.630713, acc: 65.62%] [G loss: 1.934521]\n",
      "epoch:15 step:14182 [D loss: 0.620823, acc: 65.62%] [G loss: 1.834075]\n",
      "epoch:15 step:14183 [D loss: 0.655664, acc: 61.72%] [G loss: 1.871172]\n",
      "epoch:15 step:14184 [D loss: 0.629769, acc: 64.06%] [G loss: 1.636998]\n",
      "epoch:15 step:14185 [D loss: 0.636843, acc: 60.94%] [G loss: 1.899821]\n",
      "epoch:15 step:14186 [D loss: 0.657532, acc: 58.59%] [G loss: 1.977026]\n",
      "epoch:15 step:14187 [D loss: 0.634835, acc: 63.28%] [G loss: 1.966391]\n",
      "epoch:15 step:14188 [D loss: 0.702061, acc: 56.25%] [G loss: 1.843533]\n",
      "epoch:15 step:14189 [D loss: 0.669354, acc: 55.47%] [G loss: 1.976671]\n",
      "epoch:15 step:14190 [D loss: 0.653046, acc: 60.94%] [G loss: 1.900408]\n",
      "epoch:15 step:14191 [D loss: 0.675519, acc: 54.69%] [G loss: 1.832038]\n",
      "epoch:15 step:14192 [D loss: 0.677574, acc: 61.72%] [G loss: 1.817249]\n",
      "epoch:15 step:14193 [D loss: 0.649518, acc: 59.38%] [G loss: 1.818854]\n",
      "epoch:15 step:14194 [D loss: 0.642801, acc: 65.62%] [G loss: 1.919605]\n",
      "epoch:15 step:14195 [D loss: 0.656976, acc: 60.16%] [G loss: 1.778355]\n",
      "epoch:15 step:14196 [D loss: 0.683134, acc: 60.16%] [G loss: 1.773177]\n",
      "epoch:15 step:14197 [D loss: 0.683998, acc: 57.81%] [G loss: 1.771565]\n",
      "epoch:15 step:14198 [D loss: 0.645192, acc: 60.94%] [G loss: 1.951306]\n",
      "epoch:15 step:14199 [D loss: 0.657879, acc: 60.16%] [G loss: 1.793044]\n",
      "epoch:15 step:14200 [D loss: 0.673919, acc: 59.38%] [G loss: 1.823668]\n",
      "epoch:15 step:14201 [D loss: 0.643459, acc: 60.94%] [G loss: 1.887850]\n",
      "epoch:15 step:14202 [D loss: 0.658374, acc: 59.38%] [G loss: 1.720343]\n",
      "epoch:15 step:14203 [D loss: 0.654294, acc: 61.72%] [G loss: 1.778156]\n",
      "epoch:15 step:14204 [D loss: 0.636785, acc: 63.28%] [G loss: 1.866863]\n",
      "epoch:15 step:14205 [D loss: 0.633735, acc: 60.16%] [G loss: 1.783676]\n",
      "epoch:15 step:14206 [D loss: 0.693595, acc: 63.28%] [G loss: 1.938398]\n",
      "epoch:15 step:14207 [D loss: 0.690531, acc: 57.81%] [G loss: 1.927409]\n",
      "epoch:15 step:14208 [D loss: 0.641916, acc: 62.50%] [G loss: 1.865893]\n",
      "epoch:15 step:14209 [D loss: 0.594603, acc: 69.53%] [G loss: 1.972426]\n",
      "epoch:15 step:14210 [D loss: 0.635591, acc: 67.97%] [G loss: 1.886047]\n",
      "epoch:15 step:14211 [D loss: 0.622704, acc: 63.28%] [G loss: 1.987175]\n",
      "epoch:15 step:14212 [D loss: 0.663391, acc: 58.59%] [G loss: 1.827032]\n",
      "epoch:15 step:14213 [D loss: 0.602001, acc: 71.88%] [G loss: 1.851619]\n",
      "epoch:15 step:14214 [D loss: 0.598568, acc: 65.62%] [G loss: 1.948110]\n",
      "epoch:15 step:14215 [D loss: 0.699096, acc: 53.91%] [G loss: 1.751256]\n",
      "epoch:15 step:14216 [D loss: 0.688659, acc: 59.38%] [G loss: 1.889977]\n",
      "epoch:15 step:14217 [D loss: 0.624345, acc: 65.62%] [G loss: 1.963697]\n",
      "epoch:15 step:14218 [D loss: 0.592068, acc: 68.75%] [G loss: 2.004376]\n",
      "epoch:15 step:14219 [D loss: 0.682591, acc: 55.47%] [G loss: 1.784533]\n",
      "epoch:15 step:14220 [D loss: 0.692969, acc: 55.47%] [G loss: 1.937387]\n",
      "epoch:15 step:14221 [D loss: 0.635613, acc: 67.97%] [G loss: 1.890736]\n",
      "epoch:15 step:14222 [D loss: 0.605241, acc: 69.53%] [G loss: 1.902967]\n",
      "epoch:15 step:14223 [D loss: 0.603253, acc: 65.62%] [G loss: 1.812165]\n",
      "epoch:15 step:14224 [D loss: 0.644997, acc: 59.38%] [G loss: 1.906783]\n",
      "epoch:15 step:14225 [D loss: 0.626043, acc: 63.28%] [G loss: 2.066992]\n",
      "epoch:15 step:14226 [D loss: 0.683598, acc: 58.59%] [G loss: 1.962471]\n",
      "epoch:15 step:14227 [D loss: 0.616220, acc: 62.50%] [G loss: 1.853670]\n",
      "epoch:15 step:14228 [D loss: 0.627878, acc: 62.50%] [G loss: 1.890437]\n",
      "epoch:15 step:14229 [D loss: 0.639177, acc: 65.62%] [G loss: 1.823231]\n",
      "epoch:15 step:14230 [D loss: 0.669238, acc: 54.69%] [G loss: 1.820202]\n",
      "epoch:15 step:14231 [D loss: 0.677902, acc: 60.16%] [G loss: 1.799961]\n",
      "epoch:15 step:14232 [D loss: 0.659365, acc: 63.28%] [G loss: 1.797630]\n",
      "epoch:15 step:14233 [D loss: 0.648017, acc: 62.50%] [G loss: 1.836543]\n",
      "epoch:15 step:14234 [D loss: 0.631858, acc: 59.38%] [G loss: 1.746665]\n",
      "epoch:15 step:14235 [D loss: 0.588647, acc: 68.75%] [G loss: 1.933426]\n",
      "epoch:15 step:14236 [D loss: 0.631954, acc: 65.62%] [G loss: 1.921200]\n",
      "epoch:15 step:14237 [D loss: 0.643294, acc: 60.94%] [G loss: 1.893417]\n",
      "epoch:15 step:14238 [D loss: 0.603407, acc: 67.97%] [G loss: 1.961220]\n",
      "epoch:15 step:14239 [D loss: 0.693215, acc: 58.59%] [G loss: 1.946906]\n",
      "epoch:15 step:14240 [D loss: 0.576056, acc: 73.44%] [G loss: 1.989474]\n",
      "epoch:15 step:14241 [D loss: 0.624654, acc: 61.72%] [G loss: 1.934938]\n",
      "epoch:15 step:14242 [D loss: 0.607234, acc: 66.41%] [G loss: 2.012123]\n",
      "epoch:15 step:14243 [D loss: 0.606718, acc: 67.97%] [G loss: 2.010123]\n",
      "epoch:15 step:14244 [D loss: 0.632612, acc: 67.97%] [G loss: 1.939456]\n",
      "epoch:15 step:14245 [D loss: 0.715136, acc: 54.69%] [G loss: 2.042633]\n",
      "epoch:15 step:14246 [D loss: 0.657732, acc: 65.62%] [G loss: 1.973868]\n",
      "epoch:15 step:14247 [D loss: 0.650114, acc: 64.06%] [G loss: 2.055751]\n",
      "epoch:15 step:14248 [D loss: 0.629599, acc: 69.53%] [G loss: 1.916170]\n",
      "epoch:15 step:14249 [D loss: 0.616336, acc: 65.62%] [G loss: 2.004030]\n",
      "epoch:15 step:14250 [D loss: 0.681355, acc: 60.16%] [G loss: 1.742471]\n",
      "epoch:15 step:14251 [D loss: 0.631664, acc: 60.16%] [G loss: 1.876625]\n",
      "epoch:15 step:14252 [D loss: 0.614699, acc: 70.31%] [G loss: 1.962212]\n",
      "epoch:15 step:14253 [D loss: 0.649944, acc: 64.06%] [G loss: 1.832170]\n",
      "epoch:15 step:14254 [D loss: 0.635281, acc: 59.38%] [G loss: 1.916299]\n",
      "epoch:15 step:14255 [D loss: 0.646462, acc: 65.62%] [G loss: 1.882595]\n",
      "epoch:15 step:14256 [D loss: 0.665094, acc: 55.47%] [G loss: 1.886212]\n",
      "epoch:15 step:14257 [D loss: 0.659967, acc: 62.50%] [G loss: 1.934564]\n",
      "epoch:15 step:14258 [D loss: 0.630399, acc: 65.62%] [G loss: 1.871448]\n",
      "epoch:15 step:14259 [D loss: 0.659405, acc: 57.81%] [G loss: 1.946172]\n",
      "epoch:15 step:14260 [D loss: 0.634731, acc: 60.16%] [G loss: 1.818898]\n",
      "epoch:15 step:14261 [D loss: 0.601950, acc: 64.84%] [G loss: 2.165396]\n",
      "epoch:15 step:14262 [D loss: 0.620243, acc: 65.62%] [G loss: 2.061122]\n",
      "epoch:15 step:14263 [D loss: 0.591697, acc: 67.97%] [G loss: 2.342460]\n",
      "epoch:15 step:14264 [D loss: 0.538073, acc: 78.12%] [G loss: 2.178751]\n",
      "epoch:15 step:14265 [D loss: 0.636824, acc: 64.84%] [G loss: 1.772885]\n",
      "epoch:15 step:14266 [D loss: 0.672004, acc: 57.03%] [G loss: 1.773041]\n",
      "epoch:15 step:14267 [D loss: 0.635152, acc: 62.50%] [G loss: 2.016755]\n",
      "epoch:15 step:14268 [D loss: 0.669260, acc: 57.81%] [G loss: 1.881881]\n",
      "epoch:15 step:14269 [D loss: 0.691137, acc: 61.72%] [G loss: 1.770097]\n",
      "epoch:15 step:14270 [D loss: 0.675790, acc: 57.03%] [G loss: 1.877030]\n",
      "epoch:15 step:14271 [D loss: 0.590727, acc: 71.09%] [G loss: 2.031391]\n",
      "epoch:15 step:14272 [D loss: 0.663839, acc: 60.16%] [G loss: 2.095186]\n",
      "epoch:15 step:14273 [D loss: 0.580870, acc: 68.75%] [G loss: 2.009251]\n",
      "epoch:15 step:14274 [D loss: 0.552185, acc: 72.66%] [G loss: 2.337728]\n",
      "epoch:15 step:14275 [D loss: 0.650322, acc: 60.16%] [G loss: 1.887573]\n",
      "epoch:15 step:14276 [D loss: 0.592786, acc: 67.97%] [G loss: 1.943308]\n",
      "epoch:15 step:14277 [D loss: 0.641629, acc: 59.38%] [G loss: 1.943132]\n",
      "epoch:15 step:14278 [D loss: 0.665272, acc: 57.81%] [G loss: 1.926982]\n",
      "epoch:15 step:14279 [D loss: 0.683662, acc: 53.12%] [G loss: 2.011619]\n",
      "epoch:15 step:14280 [D loss: 0.675672, acc: 58.59%] [G loss: 1.769380]\n",
      "epoch:15 step:14281 [D loss: 0.620762, acc: 64.84%] [G loss: 1.876633]\n",
      "epoch:15 step:14282 [D loss: 0.695227, acc: 57.81%] [G loss: 1.823257]\n",
      "epoch:15 step:14283 [D loss: 0.634479, acc: 66.41%] [G loss: 1.786462]\n",
      "epoch:15 step:14284 [D loss: 0.641811, acc: 66.41%] [G loss: 2.110009]\n",
      "epoch:15 step:14285 [D loss: 0.595125, acc: 71.09%] [G loss: 2.081076]\n",
      "epoch:15 step:14286 [D loss: 0.532745, acc: 75.00%] [G loss: 2.386883]\n",
      "epoch:15 step:14287 [D loss: 0.523290, acc: 75.78%] [G loss: 2.461639]\n",
      "epoch:15 step:14288 [D loss: 0.704616, acc: 58.59%] [G loss: 1.864770]\n",
      "epoch:15 step:14289 [D loss: 0.641095, acc: 61.72%] [G loss: 1.749214]\n",
      "epoch:15 step:14290 [D loss: 0.704720, acc: 53.91%] [G loss: 1.875036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14291 [D loss: 0.615666, acc: 61.72%] [G loss: 1.951787]\n",
      "epoch:15 step:14292 [D loss: 0.640980, acc: 64.06%] [G loss: 1.950837]\n",
      "epoch:15 step:14293 [D loss: 0.668742, acc: 58.59%] [G loss: 2.018585]\n",
      "epoch:15 step:14294 [D loss: 0.646641, acc: 63.28%] [G loss: 1.867739]\n",
      "epoch:15 step:14295 [D loss: 0.637693, acc: 60.94%] [G loss: 1.845418]\n",
      "epoch:15 step:14296 [D loss: 0.660243, acc: 60.94%] [G loss: 2.095297]\n",
      "epoch:15 step:14297 [D loss: 0.643976, acc: 64.84%] [G loss: 2.306879]\n",
      "epoch:15 step:14298 [D loss: 0.693651, acc: 57.03%] [G loss: 1.961089]\n",
      "epoch:15 step:14299 [D loss: 0.614836, acc: 65.62%] [G loss: 2.029535]\n",
      "epoch:15 step:14300 [D loss: 0.607259, acc: 70.31%] [G loss: 2.040629]\n",
      "epoch:15 step:14301 [D loss: 0.650239, acc: 61.72%] [G loss: 2.114261]\n",
      "epoch:15 step:14302 [D loss: 0.662396, acc: 62.50%] [G loss: 2.008990]\n",
      "epoch:15 step:14303 [D loss: 0.656265, acc: 66.41%] [G loss: 2.148083]\n",
      "epoch:15 step:14304 [D loss: 0.682263, acc: 58.59%] [G loss: 1.785508]\n",
      "epoch:15 step:14305 [D loss: 0.656695, acc: 60.94%] [G loss: 1.848659]\n",
      "epoch:15 step:14306 [D loss: 0.707506, acc: 56.25%] [G loss: 1.752213]\n",
      "epoch:15 step:14307 [D loss: 0.680603, acc: 58.59%] [G loss: 1.754424]\n",
      "epoch:15 step:14308 [D loss: 0.656473, acc: 60.16%] [G loss: 1.832993]\n",
      "epoch:15 step:14309 [D loss: 0.645366, acc: 67.19%] [G loss: 1.901330]\n",
      "epoch:15 step:14310 [D loss: 0.680561, acc: 55.47%] [G loss: 1.907138]\n",
      "epoch:15 step:14311 [D loss: 0.691690, acc: 53.91%] [G loss: 1.871678]\n",
      "epoch:15 step:14312 [D loss: 0.670541, acc: 58.59%] [G loss: 1.837704]\n",
      "epoch:15 step:14313 [D loss: 0.615112, acc: 65.62%] [G loss: 1.918211]\n",
      "epoch:15 step:14314 [D loss: 0.626435, acc: 64.06%] [G loss: 1.846024]\n",
      "epoch:15 step:14315 [D loss: 0.649900, acc: 67.19%] [G loss: 1.986822]\n",
      "epoch:15 step:14316 [D loss: 0.645151, acc: 62.50%] [G loss: 1.895191]\n",
      "epoch:15 step:14317 [D loss: 0.617764, acc: 66.41%] [G loss: 1.988320]\n",
      "epoch:15 step:14318 [D loss: 0.647017, acc: 62.50%] [G loss: 1.902748]\n",
      "epoch:15 step:14319 [D loss: 0.608902, acc: 67.19%] [G loss: 1.995222]\n",
      "epoch:15 step:14320 [D loss: 0.714897, acc: 55.47%] [G loss: 1.854445]\n",
      "epoch:15 step:14321 [D loss: 0.661766, acc: 60.16%] [G loss: 1.852206]\n",
      "epoch:15 step:14322 [D loss: 0.647042, acc: 60.16%] [G loss: 1.899614]\n",
      "epoch:15 step:14323 [D loss: 0.628247, acc: 62.50%] [G loss: 1.856377]\n",
      "epoch:15 step:14324 [D loss: 0.632778, acc: 59.38%] [G loss: 1.853629]\n",
      "epoch:15 step:14325 [D loss: 0.650160, acc: 58.59%] [G loss: 1.880867]\n",
      "epoch:15 step:14326 [D loss: 0.661857, acc: 58.59%] [G loss: 1.982790]\n",
      "epoch:15 step:14327 [D loss: 0.600671, acc: 66.41%] [G loss: 1.979351]\n",
      "epoch:15 step:14328 [D loss: 0.592973, acc: 69.53%] [G loss: 1.971432]\n",
      "epoch:15 step:14329 [D loss: 0.582482, acc: 67.97%] [G loss: 2.131655]\n",
      "epoch:15 step:14330 [D loss: 0.670925, acc: 57.03%] [G loss: 2.130461]\n",
      "epoch:15 step:14331 [D loss: 0.597061, acc: 70.31%] [G loss: 2.240915]\n",
      "epoch:15 step:14332 [D loss: 0.662131, acc: 60.16%] [G loss: 1.826892]\n",
      "epoch:15 step:14333 [D loss: 0.704594, acc: 57.03%] [G loss: 1.860123]\n",
      "epoch:15 step:14334 [D loss: 0.620831, acc: 65.62%] [G loss: 1.959266]\n",
      "epoch:15 step:14335 [D loss: 0.665427, acc: 63.28%] [G loss: 1.861121]\n",
      "epoch:15 step:14336 [D loss: 0.630244, acc: 62.50%] [G loss: 1.991918]\n",
      "epoch:15 step:14337 [D loss: 0.657102, acc: 60.16%] [G loss: 1.926770]\n",
      "epoch:15 step:14338 [D loss: 0.598596, acc: 74.22%] [G loss: 2.008716]\n",
      "epoch:15 step:14339 [D loss: 0.630160, acc: 65.62%] [G loss: 1.798269]\n",
      "epoch:15 step:14340 [D loss: 0.618889, acc: 67.19%] [G loss: 1.930002]\n",
      "epoch:15 step:14341 [D loss: 0.671924, acc: 57.81%] [G loss: 1.956826]\n",
      "epoch:15 step:14342 [D loss: 0.644785, acc: 64.84%] [G loss: 1.917196]\n",
      "epoch:15 step:14343 [D loss: 0.664753, acc: 60.16%] [G loss: 1.873340]\n",
      "epoch:15 step:14344 [D loss: 0.709414, acc: 54.69%] [G loss: 2.086387]\n",
      "epoch:15 step:14345 [D loss: 0.683518, acc: 59.38%] [G loss: 1.847678]\n",
      "epoch:15 step:14346 [D loss: 0.650119, acc: 61.72%] [G loss: 1.828859]\n",
      "epoch:15 step:14347 [D loss: 0.646769, acc: 62.50%] [G loss: 1.914914]\n",
      "epoch:15 step:14348 [D loss: 0.649041, acc: 55.47%] [G loss: 1.961869]\n",
      "epoch:15 step:14349 [D loss: 0.650478, acc: 60.94%] [G loss: 1.962936]\n",
      "epoch:15 step:14350 [D loss: 0.679023, acc: 57.81%] [G loss: 1.791971]\n",
      "epoch:15 step:14351 [D loss: 0.599200, acc: 72.66%] [G loss: 1.960743]\n",
      "epoch:15 step:14352 [D loss: 0.622941, acc: 62.50%] [G loss: 1.952146]\n",
      "epoch:15 step:14353 [D loss: 0.682752, acc: 60.94%] [G loss: 2.056168]\n",
      "epoch:15 step:14354 [D loss: 0.633948, acc: 61.72%] [G loss: 2.050722]\n",
      "epoch:15 step:14355 [D loss: 0.605182, acc: 69.53%] [G loss: 1.904303]\n",
      "epoch:15 step:14356 [D loss: 0.627563, acc: 64.06%] [G loss: 1.797172]\n",
      "epoch:15 step:14357 [D loss: 0.610381, acc: 71.09%] [G loss: 2.036956]\n",
      "epoch:15 step:14358 [D loss: 0.623701, acc: 62.50%] [G loss: 2.099741]\n",
      "epoch:15 step:14359 [D loss: 0.647908, acc: 69.53%] [G loss: 1.861173]\n",
      "epoch:15 step:14360 [D loss: 0.689845, acc: 56.25%] [G loss: 1.748492]\n",
      "epoch:15 step:14361 [D loss: 0.678240, acc: 55.47%] [G loss: 1.798737]\n",
      "epoch:15 step:14362 [D loss: 0.605291, acc: 65.62%] [G loss: 1.863601]\n",
      "epoch:15 step:14363 [D loss: 0.656126, acc: 62.50%] [G loss: 1.859620]\n",
      "epoch:15 step:14364 [D loss: 0.701914, acc: 58.59%] [G loss: 1.830879]\n",
      "epoch:15 step:14365 [D loss: 0.650364, acc: 60.16%] [G loss: 1.841520]\n",
      "epoch:15 step:14366 [D loss: 0.606664, acc: 67.19%] [G loss: 2.048547]\n",
      "epoch:15 step:14367 [D loss: 0.615999, acc: 70.31%] [G loss: 2.196977]\n",
      "epoch:15 step:14368 [D loss: 0.616096, acc: 67.97%] [G loss: 2.190041]\n",
      "epoch:15 step:14369 [D loss: 0.585618, acc: 67.97%] [G loss: 2.109781]\n",
      "epoch:15 step:14370 [D loss: 0.573512, acc: 71.88%] [G loss: 2.168783]\n",
      "epoch:15 step:14371 [D loss: 0.710011, acc: 56.25%] [G loss: 1.821971]\n",
      "epoch:15 step:14372 [D loss: 0.698543, acc: 56.25%] [G loss: 1.852413]\n",
      "epoch:15 step:14373 [D loss: 0.600519, acc: 67.19%] [G loss: 2.005507]\n",
      "epoch:15 step:14374 [D loss: 0.629819, acc: 65.62%] [G loss: 1.850984]\n",
      "epoch:15 step:14375 [D loss: 0.605700, acc: 64.06%] [G loss: 1.888207]\n",
      "epoch:15 step:14376 [D loss: 0.673344, acc: 60.94%] [G loss: 2.123984]\n",
      "epoch:15 step:14377 [D loss: 0.660198, acc: 65.62%] [G loss: 1.894599]\n",
      "epoch:15 step:14378 [D loss: 0.664770, acc: 60.94%] [G loss: 1.730626]\n",
      "epoch:15 step:14379 [D loss: 0.642373, acc: 61.72%] [G loss: 1.865641]\n",
      "epoch:15 step:14380 [D loss: 0.659024, acc: 58.59%] [G loss: 2.035769]\n",
      "epoch:15 step:14381 [D loss: 0.614981, acc: 65.62%] [G loss: 1.936883]\n",
      "epoch:15 step:14382 [D loss: 0.649553, acc: 60.16%] [G loss: 1.868082]\n",
      "epoch:15 step:14383 [D loss: 0.641815, acc: 62.50%] [G loss: 1.977680]\n",
      "epoch:15 step:14384 [D loss: 0.683897, acc: 56.25%] [G loss: 1.940194]\n",
      "epoch:15 step:14385 [D loss: 0.621411, acc: 64.06%] [G loss: 2.027743]\n",
      "epoch:15 step:14386 [D loss: 0.604295, acc: 66.41%] [G loss: 2.039340]\n",
      "epoch:15 step:14387 [D loss: 0.615796, acc: 67.97%] [G loss: 1.982911]\n",
      "epoch:15 step:14388 [D loss: 0.633568, acc: 62.50%] [G loss: 2.059673]\n",
      "epoch:15 step:14389 [D loss: 0.650687, acc: 57.81%] [G loss: 2.036408]\n",
      "epoch:15 step:14390 [D loss: 0.642580, acc: 60.16%] [G loss: 1.936328]\n",
      "epoch:15 step:14391 [D loss: 0.604372, acc: 63.28%] [G loss: 2.077199]\n",
      "epoch:15 step:14392 [D loss: 0.635355, acc: 65.62%] [G loss: 1.898487]\n",
      "epoch:15 step:14393 [D loss: 0.660712, acc: 59.38%] [G loss: 2.009684]\n",
      "epoch:15 step:14394 [D loss: 0.595084, acc: 71.09%] [G loss: 1.945014]\n",
      "epoch:15 step:14395 [D loss: 0.629458, acc: 68.75%] [G loss: 2.013947]\n",
      "epoch:15 step:14396 [D loss: 0.729577, acc: 54.69%] [G loss: 1.844303]\n",
      "epoch:15 step:14397 [D loss: 0.740532, acc: 50.00%] [G loss: 1.829593]\n",
      "epoch:15 step:14398 [D loss: 0.657129, acc: 60.94%] [G loss: 1.889550]\n",
      "epoch:15 step:14399 [D loss: 0.624867, acc: 66.41%] [G loss: 2.005394]\n",
      "epoch:15 step:14400 [D loss: 0.568682, acc: 68.75%] [G loss: 2.220100]\n",
      "epoch:15 step:14401 [D loss: 0.590171, acc: 72.66%] [G loss: 2.287559]\n",
      "epoch:15 step:14402 [D loss: 0.564816, acc: 73.44%] [G loss: 2.121166]\n",
      "epoch:15 step:14403 [D loss: 0.664356, acc: 64.06%] [G loss: 1.843715]\n",
      "epoch:15 step:14404 [D loss: 0.709592, acc: 53.12%] [G loss: 1.727828]\n",
      "epoch:15 step:14405 [D loss: 0.642388, acc: 62.50%] [G loss: 2.056641]\n",
      "epoch:15 step:14406 [D loss: 0.638602, acc: 64.06%] [G loss: 1.762125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14407 [D loss: 0.683519, acc: 55.47%] [G loss: 1.854063]\n",
      "epoch:15 step:14408 [D loss: 0.687312, acc: 57.81%] [G loss: 1.801120]\n",
      "epoch:15 step:14409 [D loss: 0.660169, acc: 52.34%] [G loss: 2.081358]\n",
      "epoch:15 step:14410 [D loss: 0.663391, acc: 60.94%] [G loss: 1.829842]\n",
      "epoch:15 step:14411 [D loss: 0.657030, acc: 61.72%] [G loss: 1.854919]\n",
      "epoch:15 step:14412 [D loss: 0.647099, acc: 59.38%] [G loss: 1.913875]\n",
      "epoch:15 step:14413 [D loss: 0.671184, acc: 61.72%] [G loss: 2.025022]\n",
      "epoch:15 step:14414 [D loss: 0.598298, acc: 67.19%] [G loss: 2.091759]\n",
      "epoch:15 step:14415 [D loss: 0.623043, acc: 60.94%] [G loss: 1.906971]\n",
      "epoch:15 step:14416 [D loss: 0.630159, acc: 67.97%] [G loss: 1.892810]\n",
      "epoch:15 step:14417 [D loss: 0.631457, acc: 65.62%] [G loss: 1.939987]\n",
      "epoch:15 step:14418 [D loss: 0.645082, acc: 64.84%] [G loss: 1.873639]\n",
      "epoch:15 step:14419 [D loss: 0.666455, acc: 63.28%] [G loss: 1.914917]\n",
      "epoch:15 step:14420 [D loss: 0.629315, acc: 63.28%] [G loss: 1.788891]\n",
      "epoch:15 step:14421 [D loss: 0.661940, acc: 59.38%] [G loss: 1.883197]\n",
      "epoch:15 step:14422 [D loss: 0.617672, acc: 67.97%] [G loss: 1.830807]\n",
      "epoch:15 step:14423 [D loss: 0.605210, acc: 70.31%] [G loss: 1.907660]\n",
      "epoch:15 step:14424 [D loss: 0.608874, acc: 71.09%] [G loss: 2.034723]\n",
      "epoch:15 step:14425 [D loss: 0.627031, acc: 64.06%] [G loss: 1.904032]\n",
      "epoch:15 step:14426 [D loss: 0.600486, acc: 64.84%] [G loss: 2.075652]\n",
      "epoch:15 step:14427 [D loss: 0.617454, acc: 66.41%] [G loss: 1.889421]\n",
      "epoch:15 step:14428 [D loss: 0.651588, acc: 60.94%] [G loss: 1.907988]\n",
      "epoch:15 step:14429 [D loss: 0.565401, acc: 75.00%] [G loss: 2.029499]\n",
      "epoch:15 step:14430 [D loss: 0.670562, acc: 59.38%] [G loss: 1.944384]\n",
      "epoch:15 step:14431 [D loss: 0.660290, acc: 61.72%] [G loss: 1.887233]\n",
      "epoch:15 step:14432 [D loss: 0.682559, acc: 55.47%] [G loss: 1.911208]\n",
      "epoch:15 step:14433 [D loss: 0.658372, acc: 58.59%] [G loss: 1.992897]\n",
      "epoch:15 step:14434 [D loss: 0.633165, acc: 65.62%] [G loss: 1.913958]\n",
      "epoch:15 step:14435 [D loss: 0.626318, acc: 66.41%] [G loss: 1.988006]\n",
      "epoch:15 step:14436 [D loss: 0.650341, acc: 57.81%] [G loss: 2.153516]\n",
      "epoch:15 step:14437 [D loss: 0.650750, acc: 57.81%] [G loss: 1.910610]\n",
      "epoch:15 step:14438 [D loss: 0.605898, acc: 67.97%] [G loss: 2.025915]\n",
      "epoch:15 step:14439 [D loss: 0.641859, acc: 60.94%] [G loss: 1.978287]\n",
      "epoch:15 step:14440 [D loss: 0.611643, acc: 68.75%] [G loss: 1.927900]\n",
      "epoch:15 step:14441 [D loss: 0.681062, acc: 55.47%] [G loss: 1.829836]\n",
      "epoch:15 step:14442 [D loss: 0.636811, acc: 64.06%] [G loss: 1.784323]\n",
      "epoch:15 step:14443 [D loss: 0.641353, acc: 60.94%] [G loss: 1.894687]\n",
      "epoch:15 step:14444 [D loss: 0.640140, acc: 64.84%] [G loss: 1.897997]\n",
      "epoch:15 step:14445 [D loss: 0.678290, acc: 59.38%] [G loss: 1.824281]\n",
      "epoch:15 step:14446 [D loss: 0.634182, acc: 61.72%] [G loss: 1.858308]\n",
      "epoch:15 step:14447 [D loss: 0.609735, acc: 60.94%] [G loss: 1.879978]\n",
      "epoch:15 step:14448 [D loss: 0.654285, acc: 62.50%] [G loss: 1.822976]\n",
      "epoch:15 step:14449 [D loss: 0.624207, acc: 67.19%] [G loss: 1.847357]\n",
      "epoch:15 step:14450 [D loss: 0.661755, acc: 63.28%] [G loss: 1.952828]\n",
      "epoch:15 step:14451 [D loss: 0.699548, acc: 52.34%] [G loss: 1.741839]\n",
      "epoch:15 step:14452 [D loss: 0.674544, acc: 55.47%] [G loss: 1.861913]\n",
      "epoch:15 step:14453 [D loss: 0.635387, acc: 67.19%] [G loss: 1.839009]\n",
      "epoch:15 step:14454 [D loss: 0.591981, acc: 67.19%] [G loss: 1.846204]\n",
      "epoch:15 step:14455 [D loss: 0.635949, acc: 64.84%] [G loss: 2.005457]\n",
      "epoch:15 step:14456 [D loss: 0.606378, acc: 68.75%] [G loss: 1.878744]\n",
      "epoch:15 step:14457 [D loss: 0.646315, acc: 57.03%] [G loss: 1.948148]\n",
      "epoch:15 step:14458 [D loss: 0.641563, acc: 61.72%] [G loss: 1.935691]\n",
      "epoch:15 step:14459 [D loss: 0.651386, acc: 60.94%] [G loss: 1.912902]\n",
      "epoch:15 step:14460 [D loss: 0.668971, acc: 60.16%] [G loss: 2.087014]\n",
      "epoch:15 step:14461 [D loss: 0.596386, acc: 70.31%] [G loss: 2.030451]\n",
      "epoch:15 step:14462 [D loss: 0.593764, acc: 69.53%] [G loss: 2.017224]\n",
      "epoch:15 step:14463 [D loss: 0.655337, acc: 57.81%] [G loss: 1.995816]\n",
      "epoch:15 step:14464 [D loss: 0.590654, acc: 69.53%] [G loss: 1.959916]\n",
      "epoch:15 step:14465 [D loss: 0.697877, acc: 58.59%] [G loss: 2.024014]\n",
      "epoch:15 step:14466 [D loss: 0.696134, acc: 58.59%] [G loss: 1.756258]\n",
      "epoch:15 step:14467 [D loss: 0.644303, acc: 61.72%] [G loss: 1.969787]\n",
      "epoch:15 step:14468 [D loss: 0.651821, acc: 61.72%] [G loss: 1.932989]\n",
      "epoch:15 step:14469 [D loss: 0.605156, acc: 70.31%] [G loss: 2.021319]\n",
      "epoch:15 step:14470 [D loss: 0.664206, acc: 59.38%] [G loss: 2.084166]\n",
      "epoch:15 step:14471 [D loss: 0.629830, acc: 65.62%] [G loss: 1.993893]\n",
      "epoch:15 step:14472 [D loss: 0.654798, acc: 57.03%] [G loss: 1.826524]\n",
      "epoch:15 step:14473 [D loss: 0.650270, acc: 62.50%] [G loss: 1.865999]\n",
      "epoch:15 step:14474 [D loss: 0.644084, acc: 63.28%] [G loss: 1.825015]\n",
      "epoch:15 step:14475 [D loss: 0.669303, acc: 57.03%] [G loss: 1.829341]\n",
      "epoch:15 step:14476 [D loss: 0.684148, acc: 61.72%] [G loss: 1.807909]\n",
      "epoch:15 step:14477 [D loss: 0.720529, acc: 54.69%] [G loss: 1.864852]\n",
      "epoch:15 step:14478 [D loss: 0.678331, acc: 57.81%] [G loss: 1.861843]\n",
      "epoch:15 step:14479 [D loss: 0.664234, acc: 58.59%] [G loss: 1.831186]\n",
      "epoch:15 step:14480 [D loss: 0.668818, acc: 60.94%] [G loss: 1.898120]\n",
      "epoch:15 step:14481 [D loss: 0.635008, acc: 63.28%] [G loss: 1.882359]\n",
      "epoch:15 step:14482 [D loss: 0.609368, acc: 73.44%] [G loss: 1.918317]\n",
      "epoch:15 step:14483 [D loss: 0.584695, acc: 75.78%] [G loss: 2.301337]\n",
      "epoch:15 step:14484 [D loss: 0.575756, acc: 68.75%] [G loss: 2.229783]\n",
      "epoch:15 step:14485 [D loss: 0.536294, acc: 71.88%] [G loss: 2.355167]\n",
      "epoch:15 step:14486 [D loss: 0.646803, acc: 64.06%] [G loss: 1.972928]\n",
      "epoch:15 step:14487 [D loss: 0.685680, acc: 53.91%] [G loss: 1.826104]\n",
      "epoch:15 step:14488 [D loss: 0.665496, acc: 61.72%] [G loss: 1.950382]\n",
      "epoch:15 step:14489 [D loss: 0.656613, acc: 64.06%] [G loss: 2.050184]\n",
      "epoch:15 step:14490 [D loss: 0.642424, acc: 66.41%] [G loss: 1.970638]\n",
      "epoch:15 step:14491 [D loss: 0.622850, acc: 63.28%] [G loss: 2.053328]\n",
      "epoch:15 step:14492 [D loss: 0.666755, acc: 57.81%] [G loss: 1.831372]\n",
      "epoch:15 step:14493 [D loss: 0.689219, acc: 56.25%] [G loss: 1.791385]\n",
      "epoch:15 step:14494 [D loss: 0.723020, acc: 53.91%] [G loss: 1.919074]\n",
      "epoch:15 step:14495 [D loss: 0.654556, acc: 57.03%] [G loss: 1.877916]\n",
      "epoch:15 step:14496 [D loss: 0.630536, acc: 67.19%] [G loss: 1.981941]\n",
      "epoch:15 step:14497 [D loss: 0.728246, acc: 53.12%] [G loss: 1.874265]\n",
      "epoch:15 step:14498 [D loss: 0.667027, acc: 62.50%] [G loss: 1.834827]\n",
      "epoch:15 step:14499 [D loss: 0.615566, acc: 66.41%] [G loss: 1.835793]\n",
      "epoch:15 step:14500 [D loss: 0.650152, acc: 62.50%] [G loss: 1.854048]\n",
      "epoch:15 step:14501 [D loss: 0.675964, acc: 60.16%] [G loss: 1.790787]\n",
      "epoch:15 step:14502 [D loss: 0.631391, acc: 64.84%] [G loss: 1.753691]\n",
      "epoch:15 step:14503 [D loss: 0.658562, acc: 57.03%] [G loss: 1.815124]\n",
      "epoch:15 step:14504 [D loss: 0.640953, acc: 62.50%] [G loss: 1.813980]\n",
      "epoch:15 step:14505 [D loss: 0.614556, acc: 65.62%] [G loss: 1.898723]\n",
      "epoch:15 step:14506 [D loss: 0.576512, acc: 70.31%] [G loss: 1.919739]\n",
      "epoch:15 step:14507 [D loss: 0.646556, acc: 62.50%] [G loss: 1.897541]\n",
      "epoch:15 step:14508 [D loss: 0.587176, acc: 65.62%] [G loss: 1.870173]\n",
      "epoch:15 step:14509 [D loss: 0.636832, acc: 65.62%] [G loss: 1.947283]\n",
      "epoch:15 step:14510 [D loss: 0.627724, acc: 62.50%] [G loss: 1.983534]\n",
      "epoch:15 step:14511 [D loss: 0.628043, acc: 68.75%] [G loss: 1.952152]\n",
      "epoch:15 step:14512 [D loss: 0.666083, acc: 58.59%] [G loss: 2.058377]\n",
      "epoch:15 step:14513 [D loss: 0.689028, acc: 56.25%] [G loss: 1.821559]\n",
      "epoch:15 step:14514 [D loss: 0.689570, acc: 51.56%] [G loss: 1.795737]\n",
      "epoch:15 step:14515 [D loss: 0.669506, acc: 57.03%] [G loss: 1.817875]\n",
      "epoch:15 step:14516 [D loss: 0.646370, acc: 62.50%] [G loss: 1.944784]\n",
      "epoch:15 step:14517 [D loss: 0.631756, acc: 66.41%] [G loss: 1.950610]\n",
      "epoch:15 step:14518 [D loss: 0.638909, acc: 64.84%] [G loss: 1.816890]\n",
      "epoch:15 step:14519 [D loss: 0.590695, acc: 71.09%] [G loss: 1.833585]\n",
      "epoch:15 step:14520 [D loss: 0.628935, acc: 62.50%] [G loss: 1.986091]\n",
      "epoch:15 step:14521 [D loss: 0.704006, acc: 59.38%] [G loss: 1.803257]\n",
      "epoch:15 step:14522 [D loss: 0.666891, acc: 60.94%] [G loss: 1.871047]\n",
      "epoch:15 step:14523 [D loss: 0.670315, acc: 59.38%] [G loss: 2.005463]\n",
      "epoch:15 step:14524 [D loss: 0.603586, acc: 68.75%] [G loss: 2.041570]\n",
      "epoch:15 step:14525 [D loss: 0.575603, acc: 71.09%] [G loss: 2.171295]\n",
      "epoch:15 step:14526 [D loss: 0.573696, acc: 71.88%] [G loss: 2.198743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14527 [D loss: 0.624229, acc: 62.50%] [G loss: 2.046313]\n",
      "epoch:15 step:14528 [D loss: 0.690944, acc: 57.03%] [G loss: 1.774392]\n",
      "epoch:15 step:14529 [D loss: 0.597402, acc: 66.41%] [G loss: 1.815474]\n",
      "epoch:15 step:14530 [D loss: 0.633179, acc: 63.28%] [G loss: 2.005860]\n",
      "epoch:15 step:14531 [D loss: 0.638688, acc: 63.28%] [G loss: 1.954855]\n",
      "epoch:15 step:14532 [D loss: 0.691354, acc: 53.12%] [G loss: 1.820721]\n",
      "epoch:15 step:14533 [D loss: 0.682421, acc: 55.47%] [G loss: 1.792100]\n",
      "epoch:15 step:14534 [D loss: 0.611119, acc: 65.62%] [G loss: 1.989910]\n",
      "epoch:15 step:14535 [D loss: 0.635312, acc: 59.38%] [G loss: 1.837378]\n",
      "epoch:15 step:14536 [D loss: 0.589777, acc: 75.00%] [G loss: 2.096955]\n",
      "epoch:15 step:14537 [D loss: 0.696994, acc: 50.78%] [G loss: 1.796248]\n",
      "epoch:15 step:14538 [D loss: 0.681183, acc: 60.94%] [G loss: 1.884138]\n",
      "epoch:15 step:14539 [D loss: 0.603570, acc: 67.97%] [G loss: 2.083722]\n",
      "epoch:15 step:14540 [D loss: 0.637606, acc: 65.62%] [G loss: 1.861883]\n",
      "epoch:15 step:14541 [D loss: 0.670357, acc: 58.59%] [G loss: 1.841400]\n",
      "epoch:15 step:14542 [D loss: 0.620237, acc: 64.84%] [G loss: 1.902456]\n",
      "epoch:15 step:14543 [D loss: 0.590581, acc: 68.75%] [G loss: 2.088283]\n",
      "epoch:15 step:14544 [D loss: 0.721069, acc: 51.56%] [G loss: 1.800054]\n",
      "epoch:15 step:14545 [D loss: 0.628094, acc: 67.19%] [G loss: 1.988172]\n",
      "epoch:15 step:14546 [D loss: 0.651385, acc: 57.03%] [G loss: 1.975289]\n",
      "epoch:15 step:14547 [D loss: 0.662500, acc: 61.72%] [G loss: 1.881039]\n",
      "epoch:15 step:14548 [D loss: 0.637850, acc: 60.94%] [G loss: 1.982102]\n",
      "epoch:15 step:14549 [D loss: 0.611951, acc: 68.75%] [G loss: 2.065100]\n",
      "epoch:15 step:14550 [D loss: 0.547389, acc: 70.31%] [G loss: 2.110459]\n",
      "epoch:15 step:14551 [D loss: 0.661445, acc: 59.38%] [G loss: 1.930631]\n",
      "epoch:15 step:14552 [D loss: 0.642273, acc: 64.06%] [G loss: 2.012758]\n",
      "epoch:15 step:14553 [D loss: 0.611232, acc: 67.97%] [G loss: 2.383641]\n",
      "epoch:15 step:14554 [D loss: 0.610473, acc: 72.66%] [G loss: 2.088714]\n",
      "epoch:15 step:14555 [D loss: 0.706571, acc: 51.56%] [G loss: 1.854651]\n",
      "epoch:15 step:14556 [D loss: 0.716051, acc: 57.81%] [G loss: 1.802916]\n",
      "epoch:15 step:14557 [D loss: 0.663535, acc: 62.50%] [G loss: 1.761271]\n",
      "epoch:15 step:14558 [D loss: 0.686958, acc: 57.03%] [G loss: 1.971607]\n",
      "epoch:15 step:14559 [D loss: 0.572248, acc: 74.22%] [G loss: 2.084878]\n",
      "epoch:15 step:14560 [D loss: 0.628935, acc: 60.16%] [G loss: 2.033217]\n",
      "epoch:15 step:14561 [D loss: 0.627509, acc: 64.06%] [G loss: 1.807266]\n",
      "epoch:15 step:14562 [D loss: 0.730934, acc: 50.78%] [G loss: 1.787629]\n",
      "epoch:15 step:14563 [D loss: 0.631217, acc: 66.41%] [G loss: 2.150913]\n",
      "epoch:15 step:14564 [D loss: 0.653087, acc: 64.84%] [G loss: 1.845991]\n",
      "epoch:15 step:14565 [D loss: 0.596980, acc: 67.97%] [G loss: 1.999424]\n",
      "epoch:15 step:14566 [D loss: 0.627679, acc: 68.75%] [G loss: 1.861645]\n",
      "epoch:15 step:14567 [D loss: 0.667462, acc: 60.16%] [G loss: 1.990606]\n",
      "epoch:15 step:14568 [D loss: 0.618095, acc: 64.06%] [G loss: 1.756775]\n",
      "epoch:15 step:14569 [D loss: 0.609280, acc: 65.62%] [G loss: 1.868036]\n",
      "epoch:15 step:14570 [D loss: 0.617543, acc: 67.19%] [G loss: 1.903863]\n",
      "epoch:15 step:14571 [D loss: 0.587278, acc: 66.41%] [G loss: 1.983867]\n",
      "epoch:15 step:14572 [D loss: 0.603162, acc: 67.19%] [G loss: 2.032254]\n",
      "epoch:15 step:14573 [D loss: 0.676368, acc: 60.94%] [G loss: 1.904880]\n",
      "epoch:15 step:14574 [D loss: 0.619526, acc: 64.84%] [G loss: 1.957923]\n",
      "epoch:15 step:14575 [D loss: 0.675518, acc: 60.16%] [G loss: 2.017860]\n",
      "epoch:15 step:14576 [D loss: 0.595986, acc: 71.88%] [G loss: 2.042762]\n",
      "epoch:15 step:14577 [D loss: 0.629893, acc: 63.28%] [G loss: 2.022453]\n",
      "epoch:15 step:14578 [D loss: 0.636072, acc: 62.50%] [G loss: 1.963575]\n",
      "epoch:15 step:14579 [D loss: 0.632544, acc: 64.84%] [G loss: 1.972958]\n",
      "epoch:15 step:14580 [D loss: 0.638597, acc: 63.28%] [G loss: 1.949444]\n",
      "epoch:15 step:14581 [D loss: 0.623736, acc: 67.97%] [G loss: 1.844857]\n",
      "epoch:15 step:14582 [D loss: 0.667656, acc: 60.94%] [G loss: 1.889927]\n",
      "epoch:15 step:14583 [D loss: 0.695630, acc: 57.03%] [G loss: 1.815887]\n",
      "epoch:15 step:14584 [D loss: 0.720424, acc: 51.56%] [G loss: 1.750286]\n",
      "epoch:15 step:14585 [D loss: 0.658889, acc: 55.47%] [G loss: 1.858393]\n",
      "epoch:15 step:14586 [D loss: 0.655167, acc: 64.06%] [G loss: 1.734069]\n",
      "epoch:15 step:14587 [D loss: 0.634470, acc: 64.06%] [G loss: 2.080176]\n",
      "epoch:15 step:14588 [D loss: 0.645830, acc: 57.81%] [G loss: 1.998514]\n",
      "epoch:15 step:14589 [D loss: 0.676702, acc: 60.94%] [G loss: 1.963794]\n",
      "epoch:15 step:14590 [D loss: 0.625888, acc: 65.62%] [G loss: 1.902958]\n",
      "epoch:15 step:14591 [D loss: 0.608127, acc: 74.22%] [G loss: 2.052956]\n",
      "epoch:15 step:14592 [D loss: 0.635636, acc: 66.41%] [G loss: 1.930894]\n",
      "epoch:15 step:14593 [D loss: 0.672988, acc: 54.69%] [G loss: 1.740042]\n",
      "epoch:15 step:14594 [D loss: 0.652455, acc: 64.06%] [G loss: 1.917302]\n",
      "epoch:15 step:14595 [D loss: 0.663640, acc: 57.03%] [G loss: 2.014045]\n",
      "epoch:15 step:14596 [D loss: 0.660187, acc: 62.50%] [G loss: 1.866009]\n",
      "epoch:15 step:14597 [D loss: 0.658152, acc: 61.72%] [G loss: 1.905102]\n",
      "epoch:15 step:14598 [D loss: 0.693044, acc: 53.12%] [G loss: 1.845428]\n",
      "epoch:15 step:14599 [D loss: 0.638397, acc: 66.41%] [G loss: 1.906052]\n",
      "epoch:15 step:14600 [D loss: 0.633726, acc: 60.94%] [G loss: 2.001036]\n",
      "epoch:15 step:14601 [D loss: 0.654450, acc: 62.50%] [G loss: 1.934336]\n",
      "epoch:15 step:14602 [D loss: 0.688700, acc: 54.69%] [G loss: 1.962247]\n",
      "epoch:15 step:14603 [D loss: 0.601838, acc: 68.75%] [G loss: 1.925675]\n",
      "epoch:15 step:14604 [D loss: 0.573142, acc: 72.66%] [G loss: 2.148285]\n",
      "epoch:15 step:14605 [D loss: 0.625799, acc: 66.41%] [G loss: 1.954407]\n",
      "epoch:15 step:14606 [D loss: 0.625255, acc: 64.84%] [G loss: 2.141078]\n",
      "epoch:15 step:14607 [D loss: 0.585825, acc: 68.75%] [G loss: 2.188619]\n",
      "epoch:15 step:14608 [D loss: 0.728148, acc: 49.22%] [G loss: 1.790725]\n",
      "epoch:15 step:14609 [D loss: 0.517039, acc: 76.56%] [G loss: 2.093988]\n",
      "epoch:15 step:14610 [D loss: 0.657643, acc: 60.94%] [G loss: 2.020774]\n",
      "epoch:15 step:14611 [D loss: 0.619596, acc: 68.75%] [G loss: 2.212691]\n",
      "epoch:15 step:14612 [D loss: 0.682614, acc: 61.72%] [G loss: 1.947271]\n",
      "epoch:15 step:14613 [D loss: 0.619293, acc: 63.28%] [G loss: 2.056603]\n",
      "epoch:15 step:14614 [D loss: 0.687572, acc: 57.81%] [G loss: 1.892084]\n",
      "epoch:15 step:14615 [D loss: 0.655757, acc: 62.50%] [G loss: 1.771372]\n",
      "epoch:15 step:14616 [D loss: 0.605292, acc: 63.28%] [G loss: 2.003962]\n",
      "epoch:15 step:14617 [D loss: 0.598744, acc: 64.84%] [G loss: 1.941588]\n",
      "epoch:15 step:14618 [D loss: 0.659350, acc: 60.16%] [G loss: 2.049637]\n",
      "epoch:15 step:14619 [D loss: 0.591603, acc: 67.97%] [G loss: 1.959310]\n",
      "epoch:15 step:14620 [D loss: 0.663229, acc: 61.72%] [G loss: 1.889293]\n",
      "epoch:15 step:14621 [D loss: 0.697492, acc: 58.59%] [G loss: 1.780941]\n",
      "epoch:15 step:14622 [D loss: 0.645778, acc: 65.62%] [G loss: 1.832360]\n",
      "epoch:15 step:14623 [D loss: 0.636937, acc: 59.38%] [G loss: 1.838348]\n",
      "epoch:15 step:14624 [D loss: 0.661679, acc: 61.72%] [G loss: 1.994267]\n",
      "epoch:15 step:14625 [D loss: 0.652128, acc: 62.50%] [G loss: 1.995065]\n",
      "epoch:15 step:14626 [D loss: 0.658639, acc: 62.50%] [G loss: 1.962403]\n",
      "epoch:15 step:14627 [D loss: 0.618188, acc: 67.97%] [G loss: 1.835065]\n",
      "epoch:15 step:14628 [D loss: 0.628968, acc: 62.50%] [G loss: 1.705036]\n",
      "epoch:15 step:14629 [D loss: 0.599645, acc: 71.88%] [G loss: 1.974677]\n",
      "epoch:15 step:14630 [D loss: 0.662062, acc: 60.94%] [G loss: 2.019377]\n",
      "epoch:15 step:14631 [D loss: 0.665011, acc: 57.81%] [G loss: 1.800974]\n",
      "epoch:15 step:14632 [D loss: 0.663856, acc: 61.72%] [G loss: 1.784635]\n",
      "epoch:15 step:14633 [D loss: 0.642233, acc: 64.06%] [G loss: 1.844147]\n",
      "epoch:15 step:14634 [D loss: 0.674090, acc: 60.16%] [G loss: 1.793858]\n",
      "epoch:15 step:14635 [D loss: 0.645371, acc: 62.50%] [G loss: 1.797872]\n",
      "epoch:15 step:14636 [D loss: 0.673202, acc: 59.38%] [G loss: 1.875579]\n",
      "epoch:15 step:14637 [D loss: 0.690025, acc: 57.81%] [G loss: 1.857301]\n",
      "epoch:15 step:14638 [D loss: 0.626821, acc: 67.97%] [G loss: 1.871927]\n",
      "epoch:15 step:14639 [D loss: 0.621856, acc: 64.84%] [G loss: 1.888019]\n",
      "epoch:15 step:14640 [D loss: 0.622909, acc: 63.28%] [G loss: 1.929698]\n",
      "epoch:15 step:14641 [D loss: 0.674433, acc: 60.16%] [G loss: 1.896510]\n",
      "epoch:15 step:14642 [D loss: 0.627604, acc: 60.16%] [G loss: 1.924768]\n",
      "epoch:15 step:14643 [D loss: 0.614017, acc: 67.19%] [G loss: 2.022322]\n",
      "epoch:15 step:14644 [D loss: 0.583667, acc: 73.44%] [G loss: 1.829767]\n",
      "epoch:15 step:14645 [D loss: 0.649696, acc: 59.38%] [G loss: 1.842271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14646 [D loss: 0.622948, acc: 64.84%] [G loss: 1.938936]\n",
      "epoch:15 step:14647 [D loss: 0.627302, acc: 70.31%] [G loss: 2.000895]\n",
      "epoch:15 step:14648 [D loss: 0.627075, acc: 59.38%] [G loss: 1.944303]\n",
      "epoch:15 step:14649 [D loss: 0.580817, acc: 68.75%] [G loss: 1.938078]\n",
      "epoch:15 step:14650 [D loss: 0.639899, acc: 67.97%] [G loss: 1.883855]\n",
      "epoch:15 step:14651 [D loss: 0.704807, acc: 61.72%] [G loss: 1.772429]\n",
      "epoch:15 step:14652 [D loss: 0.629081, acc: 63.28%] [G loss: 1.909979]\n",
      "epoch:15 step:14653 [D loss: 0.670049, acc: 61.72%] [G loss: 1.888512]\n",
      "epoch:15 step:14654 [D loss: 0.680638, acc: 62.50%] [G loss: 1.795212]\n",
      "epoch:15 step:14655 [D loss: 0.684247, acc: 57.81%] [G loss: 1.860774]\n",
      "epoch:15 step:14656 [D loss: 0.682400, acc: 57.81%] [G loss: 1.770021]\n",
      "epoch:15 step:14657 [D loss: 0.652795, acc: 57.81%] [G loss: 1.936425]\n",
      "epoch:15 step:14658 [D loss: 0.577160, acc: 75.78%] [G loss: 1.999418]\n",
      "epoch:15 step:14659 [D loss: 0.676912, acc: 60.16%] [G loss: 1.756599]\n",
      "epoch:15 step:14660 [D loss: 0.589997, acc: 72.66%] [G loss: 1.883172]\n",
      "epoch:15 step:14661 [D loss: 0.723096, acc: 53.12%] [G loss: 1.814566]\n",
      "epoch:15 step:14662 [D loss: 0.647381, acc: 64.84%] [G loss: 1.870252]\n",
      "epoch:15 step:14663 [D loss: 0.635980, acc: 64.84%] [G loss: 1.766141]\n",
      "epoch:15 step:14664 [D loss: 0.691860, acc: 63.28%] [G loss: 1.956556]\n",
      "epoch:15 step:14665 [D loss: 0.630085, acc: 65.62%] [G loss: 1.838049]\n",
      "epoch:15 step:14666 [D loss: 0.658466, acc: 60.94%] [G loss: 1.767547]\n",
      "epoch:15 step:14667 [D loss: 0.623736, acc: 66.41%] [G loss: 1.864549]\n",
      "epoch:15 step:14668 [D loss: 0.601022, acc: 69.53%] [G loss: 1.873266]\n",
      "epoch:15 step:14669 [D loss: 0.689752, acc: 58.59%] [G loss: 1.771684]\n",
      "epoch:15 step:14670 [D loss: 0.708295, acc: 55.47%] [G loss: 1.767951]\n",
      "epoch:15 step:14671 [D loss: 0.631250, acc: 65.62%] [G loss: 1.844130]\n",
      "epoch:15 step:14672 [D loss: 0.651223, acc: 60.94%] [G loss: 1.879410]\n",
      "epoch:15 step:14673 [D loss: 0.614950, acc: 64.06%] [G loss: 1.926705]\n",
      "epoch:15 step:14674 [D loss: 0.623127, acc: 64.84%] [G loss: 1.849287]\n",
      "epoch:15 step:14675 [D loss: 0.596120, acc: 67.97%] [G loss: 1.980536]\n",
      "epoch:15 step:14676 [D loss: 0.615387, acc: 67.19%] [G loss: 1.897263]\n",
      "epoch:15 step:14677 [D loss: 0.688507, acc: 55.47%] [G loss: 1.923096]\n",
      "epoch:15 step:14678 [D loss: 0.676085, acc: 60.16%] [G loss: 1.886430]\n",
      "epoch:15 step:14679 [D loss: 0.619371, acc: 64.84%] [G loss: 1.958068]\n",
      "epoch:15 step:14680 [D loss: 0.641228, acc: 64.84%] [G loss: 1.824050]\n",
      "epoch:15 step:14681 [D loss: 0.618088, acc: 67.19%] [G loss: 1.964459]\n",
      "epoch:15 step:14682 [D loss: 0.618741, acc: 57.81%] [G loss: 1.960960]\n",
      "epoch:15 step:14683 [D loss: 0.658542, acc: 55.47%] [G loss: 1.823174]\n",
      "epoch:15 step:14684 [D loss: 0.655425, acc: 64.06%] [G loss: 1.880787]\n",
      "epoch:15 step:14685 [D loss: 0.573281, acc: 71.88%] [G loss: 1.966224]\n",
      "epoch:15 step:14686 [D loss: 0.596918, acc: 66.41%] [G loss: 1.904308]\n",
      "epoch:15 step:14687 [D loss: 0.559791, acc: 71.88%] [G loss: 2.144726]\n",
      "epoch:15 step:14688 [D loss: 0.641620, acc: 64.84%] [G loss: 1.962488]\n",
      "epoch:15 step:14689 [D loss: 0.575615, acc: 70.31%] [G loss: 2.130977]\n",
      "epoch:15 step:14690 [D loss: 0.606828, acc: 67.97%] [G loss: 1.925467]\n",
      "epoch:15 step:14691 [D loss: 0.677929, acc: 55.47%] [G loss: 1.864149]\n",
      "epoch:15 step:14692 [D loss: 0.632378, acc: 64.06%] [G loss: 1.981681]\n",
      "epoch:15 step:14693 [D loss: 0.644834, acc: 61.72%] [G loss: 1.953606]\n",
      "epoch:15 step:14694 [D loss: 0.661569, acc: 60.94%] [G loss: 1.902423]\n",
      "epoch:15 step:14695 [D loss: 0.704174, acc: 53.12%] [G loss: 1.970247]\n",
      "epoch:15 step:14696 [D loss: 0.655286, acc: 65.62%] [G loss: 1.958831]\n",
      "epoch:15 step:14697 [D loss: 0.654388, acc: 58.59%] [G loss: 1.936887]\n",
      "epoch:15 step:14698 [D loss: 0.683251, acc: 57.81%] [G loss: 1.948095]\n",
      "epoch:15 step:14699 [D loss: 0.629818, acc: 62.50%] [G loss: 1.969659]\n",
      "epoch:15 step:14700 [D loss: 0.648890, acc: 61.72%] [G loss: 1.902042]\n",
      "epoch:15 step:14701 [D loss: 0.630859, acc: 65.62%] [G loss: 1.999543]\n",
      "epoch:15 step:14702 [D loss: 0.676360, acc: 62.50%] [G loss: 2.074017]\n",
      "epoch:15 step:14703 [D loss: 0.596591, acc: 67.97%] [G loss: 2.265985]\n",
      "epoch:15 step:14704 [D loss: 0.585676, acc: 68.75%] [G loss: 2.098459]\n",
      "epoch:15 step:14705 [D loss: 0.618414, acc: 61.72%] [G loss: 2.305787]\n",
      "epoch:15 step:14706 [D loss: 0.652855, acc: 60.94%] [G loss: 1.988920]\n",
      "epoch:15 step:14707 [D loss: 0.616241, acc: 65.62%] [G loss: 2.017548]\n",
      "epoch:15 step:14708 [D loss: 0.670912, acc: 62.50%] [G loss: 1.948081]\n",
      "epoch:15 step:14709 [D loss: 0.611938, acc: 67.19%] [G loss: 2.088218]\n",
      "epoch:15 step:14710 [D loss: 0.706190, acc: 55.47%] [G loss: 1.678934]\n",
      "epoch:15 step:14711 [D loss: 0.673347, acc: 59.38%] [G loss: 1.837705]\n",
      "epoch:15 step:14712 [D loss: 0.652512, acc: 60.94%] [G loss: 1.847373]\n",
      "epoch:15 step:14713 [D loss: 0.708684, acc: 49.22%] [G loss: 1.669382]\n",
      "epoch:15 step:14714 [D loss: 0.715221, acc: 53.12%] [G loss: 1.834643]\n",
      "epoch:15 step:14715 [D loss: 0.586704, acc: 67.19%] [G loss: 1.929651]\n",
      "epoch:15 step:14716 [D loss: 0.641044, acc: 65.62%] [G loss: 1.861347]\n",
      "epoch:15 step:14717 [D loss: 0.713178, acc: 53.91%] [G loss: 1.888902]\n",
      "epoch:15 step:14718 [D loss: 0.614091, acc: 65.62%] [G loss: 1.973561]\n",
      "epoch:15 step:14719 [D loss: 0.691662, acc: 56.25%] [G loss: 1.763292]\n",
      "epoch:15 step:14720 [D loss: 0.619657, acc: 63.28%] [G loss: 1.774480]\n",
      "epoch:15 step:14721 [D loss: 0.647316, acc: 58.59%] [G loss: 1.884237]\n",
      "epoch:15 step:14722 [D loss: 0.633058, acc: 64.84%] [G loss: 1.941855]\n",
      "epoch:15 step:14723 [D loss: 0.664686, acc: 61.72%] [G loss: 1.915879]\n",
      "epoch:15 step:14724 [D loss: 0.577234, acc: 70.31%] [G loss: 1.843133]\n",
      "epoch:15 step:14725 [D loss: 0.671495, acc: 63.28%] [G loss: 1.803085]\n",
      "epoch:15 step:14726 [D loss: 0.651986, acc: 61.72%] [G loss: 1.895398]\n",
      "epoch:15 step:14727 [D loss: 0.622510, acc: 63.28%] [G loss: 1.972522]\n",
      "epoch:15 step:14728 [D loss: 0.642171, acc: 64.06%] [G loss: 1.945600]\n",
      "epoch:15 step:14729 [D loss: 0.695377, acc: 60.16%] [G loss: 1.886328]\n",
      "epoch:15 step:14730 [D loss: 0.664744, acc: 63.28%] [G loss: 1.961499]\n",
      "epoch:15 step:14731 [D loss: 0.675181, acc: 63.28%] [G loss: 1.836642]\n",
      "epoch:15 step:14732 [D loss: 0.630276, acc: 59.38%] [G loss: 1.871382]\n",
      "epoch:15 step:14733 [D loss: 0.644084, acc: 62.50%] [G loss: 1.871884]\n",
      "epoch:15 step:14734 [D loss: 0.638684, acc: 64.84%] [G loss: 1.888698]\n",
      "epoch:15 step:14735 [D loss: 0.628593, acc: 65.62%] [G loss: 1.853593]\n",
      "epoch:15 step:14736 [D loss: 0.602011, acc: 65.62%] [G loss: 2.054899]\n",
      "epoch:15 step:14737 [D loss: 0.674747, acc: 60.16%] [G loss: 1.739835]\n",
      "epoch:15 step:14738 [D loss: 0.675679, acc: 56.25%] [G loss: 1.907401]\n",
      "epoch:15 step:14739 [D loss: 0.628513, acc: 62.50%] [G loss: 1.832725]\n",
      "epoch:15 step:14740 [D loss: 0.674545, acc: 57.03%] [G loss: 1.956379]\n",
      "epoch:15 step:14741 [D loss: 0.631127, acc: 62.50%] [G loss: 2.023756]\n",
      "epoch:15 step:14742 [D loss: 0.633091, acc: 63.28%] [G loss: 1.980881]\n",
      "epoch:15 step:14743 [D loss: 0.610039, acc: 68.75%] [G loss: 1.961291]\n",
      "epoch:15 step:14744 [D loss: 0.669466, acc: 57.81%] [G loss: 1.940875]\n",
      "epoch:15 step:14745 [D loss: 0.604328, acc: 64.84%] [G loss: 1.866742]\n",
      "epoch:15 step:14746 [D loss: 0.648617, acc: 64.84%] [G loss: 2.040856]\n",
      "epoch:15 step:14747 [D loss: 0.635776, acc: 65.62%] [G loss: 1.943509]\n",
      "epoch:15 step:14748 [D loss: 0.606096, acc: 71.09%] [G loss: 2.061956]\n",
      "epoch:15 step:14749 [D loss: 0.623952, acc: 67.19%] [G loss: 2.049788]\n",
      "epoch:15 step:14750 [D loss: 0.652097, acc: 64.84%] [G loss: 2.136521]\n",
      "epoch:15 step:14751 [D loss: 0.624702, acc: 62.50%] [G loss: 1.862349]\n",
      "epoch:15 step:14752 [D loss: 0.671942, acc: 57.81%] [G loss: 1.948447]\n",
      "epoch:15 step:14753 [D loss: 0.676424, acc: 60.16%] [G loss: 2.055652]\n",
      "epoch:15 step:14754 [D loss: 0.616307, acc: 67.97%] [G loss: 1.963385]\n",
      "epoch:15 step:14755 [D loss: 0.700266, acc: 54.69%] [G loss: 1.863166]\n",
      "epoch:15 step:14756 [D loss: 0.640803, acc: 67.97%] [G loss: 1.927037]\n",
      "epoch:15 step:14757 [D loss: 0.689277, acc: 54.69%] [G loss: 1.811149]\n",
      "epoch:15 step:14758 [D loss: 0.664628, acc: 60.16%] [G loss: 1.891436]\n",
      "epoch:15 step:14759 [D loss: 0.665594, acc: 57.81%] [G loss: 1.809080]\n",
      "epoch:15 step:14760 [D loss: 0.618959, acc: 64.06%] [G loss: 1.813915]\n",
      "epoch:15 step:14761 [D loss: 0.615099, acc: 69.53%] [G loss: 1.871104]\n",
      "epoch:15 step:14762 [D loss: 0.617665, acc: 65.62%] [G loss: 2.003276]\n",
      "epoch:15 step:14763 [D loss: 0.617050, acc: 62.50%] [G loss: 1.947530]\n",
      "epoch:15 step:14764 [D loss: 0.571288, acc: 74.22%] [G loss: 2.044705]\n",
      "epoch:15 step:14765 [D loss: 0.644371, acc: 63.28%] [G loss: 1.963965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14766 [D loss: 0.613880, acc: 59.38%] [G loss: 2.108027]\n",
      "epoch:15 step:14767 [D loss: 0.610705, acc: 62.50%] [G loss: 2.083241]\n",
      "epoch:15 step:14768 [D loss: 0.597431, acc: 69.53%] [G loss: 1.961221]\n",
      "epoch:15 step:14769 [D loss: 0.627286, acc: 66.41%] [G loss: 2.055948]\n",
      "epoch:15 step:14770 [D loss: 0.658054, acc: 63.28%] [G loss: 1.986830]\n",
      "epoch:15 step:14771 [D loss: 0.628263, acc: 71.88%] [G loss: 1.903995]\n",
      "epoch:15 step:14772 [D loss: 0.681005, acc: 64.06%] [G loss: 2.144118]\n",
      "epoch:15 step:14773 [D loss: 0.690562, acc: 56.25%] [G loss: 1.894143]\n",
      "epoch:15 step:14774 [D loss: 0.646529, acc: 60.94%] [G loss: 2.046788]\n",
      "epoch:15 step:14775 [D loss: 0.634300, acc: 62.50%] [G loss: 1.981883]\n",
      "epoch:15 step:14776 [D loss: 0.621110, acc: 62.50%] [G loss: 1.998262]\n",
      "epoch:15 step:14777 [D loss: 0.711762, acc: 57.03%] [G loss: 1.893147]\n",
      "epoch:15 step:14778 [D loss: 0.626676, acc: 68.75%] [G loss: 1.870199]\n",
      "epoch:15 step:14779 [D loss: 0.659427, acc: 56.25%] [G loss: 1.917837]\n",
      "epoch:15 step:14780 [D loss: 0.616625, acc: 65.62%] [G loss: 2.000035]\n",
      "epoch:15 step:14781 [D loss: 0.657557, acc: 63.28%] [G loss: 1.976732]\n",
      "epoch:15 step:14782 [D loss: 0.688585, acc: 57.03%] [G loss: 1.738302]\n",
      "epoch:15 step:14783 [D loss: 0.608473, acc: 71.09%] [G loss: 1.944281]\n",
      "epoch:15 step:14784 [D loss: 0.637841, acc: 58.59%] [G loss: 1.809453]\n",
      "epoch:15 step:14785 [D loss: 0.609892, acc: 65.62%] [G loss: 1.820931]\n",
      "epoch:15 step:14786 [D loss: 0.625803, acc: 67.19%] [G loss: 2.097893]\n",
      "epoch:15 step:14787 [D loss: 0.650042, acc: 65.62%] [G loss: 1.950454]\n",
      "epoch:15 step:14788 [D loss: 0.622845, acc: 66.41%] [G loss: 2.032931]\n",
      "epoch:15 step:14789 [D loss: 0.681990, acc: 57.81%] [G loss: 1.855720]\n",
      "epoch:15 step:14790 [D loss: 0.612326, acc: 65.62%] [G loss: 2.083495]\n",
      "epoch:15 step:14791 [D loss: 0.657383, acc: 62.50%] [G loss: 1.868039]\n",
      "epoch:15 step:14792 [D loss: 0.597993, acc: 67.19%] [G loss: 1.894046]\n",
      "epoch:15 step:14793 [D loss: 0.625708, acc: 64.84%] [G loss: 1.798063]\n",
      "epoch:15 step:14794 [D loss: 0.643123, acc: 62.50%] [G loss: 1.831440]\n",
      "epoch:15 step:14795 [D loss: 0.611252, acc: 61.72%] [G loss: 1.989193]\n",
      "epoch:15 step:14796 [D loss: 0.653286, acc: 58.59%] [G loss: 1.796725]\n",
      "epoch:15 step:14797 [D loss: 0.642901, acc: 64.06%] [G loss: 1.884730]\n",
      "epoch:15 step:14798 [D loss: 0.626021, acc: 62.50%] [G loss: 1.918525]\n",
      "epoch:15 step:14799 [D loss: 0.683552, acc: 62.50%] [G loss: 1.862789]\n",
      "epoch:15 step:14800 [D loss: 0.679308, acc: 63.28%] [G loss: 1.899787]\n",
      "epoch:15 step:14801 [D loss: 0.606574, acc: 70.31%] [G loss: 2.085916]\n",
      "epoch:15 step:14802 [D loss: 0.577080, acc: 67.19%] [G loss: 2.147498]\n",
      "epoch:15 step:14803 [D loss: 0.626285, acc: 62.50%] [G loss: 1.900432]\n",
      "epoch:15 step:14804 [D loss: 0.695160, acc: 59.38%] [G loss: 1.862630]\n",
      "epoch:15 step:14805 [D loss: 0.623266, acc: 68.75%] [G loss: 1.946089]\n",
      "epoch:15 step:14806 [D loss: 0.699323, acc: 52.34%] [G loss: 1.949906]\n",
      "epoch:15 step:14807 [D loss: 0.688599, acc: 57.03%] [G loss: 1.808384]\n",
      "epoch:15 step:14808 [D loss: 0.614393, acc: 69.53%] [G loss: 1.886388]\n",
      "epoch:15 step:14809 [D loss: 0.667907, acc: 57.81%] [G loss: 1.925393]\n",
      "epoch:15 step:14810 [D loss: 0.663249, acc: 58.59%] [G loss: 1.884341]\n",
      "epoch:15 step:14811 [D loss: 0.633978, acc: 67.97%] [G loss: 1.882327]\n",
      "epoch:15 step:14812 [D loss: 0.635280, acc: 61.72%] [G loss: 1.972626]\n",
      "epoch:15 step:14813 [D loss: 0.591806, acc: 73.44%] [G loss: 1.852210]\n",
      "epoch:15 step:14814 [D loss: 0.678438, acc: 57.81%] [G loss: 1.868777]\n",
      "epoch:15 step:14815 [D loss: 0.636147, acc: 65.62%] [G loss: 1.823491]\n",
      "epoch:15 step:14816 [D loss: 0.603517, acc: 70.31%] [G loss: 1.943805]\n",
      "epoch:15 step:14817 [D loss: 0.680984, acc: 60.94%] [G loss: 1.777349]\n",
      "epoch:15 step:14818 [D loss: 0.658361, acc: 58.59%] [G loss: 2.048933]\n",
      "epoch:15 step:14819 [D loss: 0.643248, acc: 64.84%] [G loss: 1.917987]\n",
      "epoch:15 step:14820 [D loss: 0.673376, acc: 60.16%] [G loss: 1.645195]\n",
      "epoch:15 step:14821 [D loss: 0.682028, acc: 57.03%] [G loss: 1.763293]\n",
      "epoch:15 step:14822 [D loss: 0.722151, acc: 56.25%] [G loss: 1.855847]\n",
      "epoch:15 step:14823 [D loss: 0.624059, acc: 66.41%] [G loss: 1.974033]\n",
      "epoch:15 step:14824 [D loss: 0.647814, acc: 63.28%] [G loss: 1.867536]\n",
      "epoch:15 step:14825 [D loss: 0.579660, acc: 77.34%] [G loss: 1.921181]\n",
      "epoch:15 step:14826 [D loss: 0.630362, acc: 64.84%] [G loss: 1.875922]\n",
      "epoch:15 step:14827 [D loss: 0.677616, acc: 58.59%] [G loss: 1.893603]\n",
      "epoch:15 step:14828 [D loss: 0.676981, acc: 54.69%] [G loss: 1.786073]\n",
      "epoch:15 step:14829 [D loss: 0.606684, acc: 69.53%] [G loss: 2.239617]\n",
      "epoch:15 step:14830 [D loss: 0.648057, acc: 63.28%] [G loss: 2.141148]\n",
      "epoch:15 step:14831 [D loss: 0.699683, acc: 55.47%] [G loss: 1.759102]\n",
      "epoch:15 step:14832 [D loss: 0.679736, acc: 60.16%] [G loss: 1.755846]\n",
      "epoch:15 step:14833 [D loss: 0.643744, acc: 58.59%] [G loss: 2.002475]\n",
      "epoch:15 step:14834 [D loss: 0.606339, acc: 71.88%] [G loss: 1.984841]\n",
      "epoch:15 step:14835 [D loss: 0.674068, acc: 58.59%] [G loss: 1.971557]\n",
      "epoch:15 step:14836 [D loss: 0.529000, acc: 79.69%] [G loss: 2.142428]\n",
      "epoch:15 step:14837 [D loss: 0.554931, acc: 71.09%] [G loss: 2.154389]\n",
      "epoch:15 step:14838 [D loss: 0.647306, acc: 64.06%] [G loss: 2.048422]\n",
      "epoch:15 step:14839 [D loss: 0.631480, acc: 60.94%] [G loss: 1.784328]\n",
      "epoch:15 step:14840 [D loss: 0.669558, acc: 58.59%] [G loss: 1.956778]\n",
      "epoch:15 step:14841 [D loss: 0.634382, acc: 67.19%] [G loss: 1.881912]\n",
      "epoch:15 step:14842 [D loss: 0.676627, acc: 63.28%] [G loss: 1.863405]\n",
      "epoch:15 step:14843 [D loss: 0.716154, acc: 51.56%] [G loss: 1.764667]\n",
      "epoch:15 step:14844 [D loss: 0.605232, acc: 67.97%] [G loss: 1.915400]\n",
      "epoch:15 step:14845 [D loss: 0.593448, acc: 71.88%] [G loss: 2.123275]\n",
      "epoch:15 step:14846 [D loss: 0.659407, acc: 57.81%] [G loss: 1.882906]\n",
      "epoch:15 step:14847 [D loss: 0.634323, acc: 64.06%] [G loss: 1.979222]\n",
      "epoch:15 step:14848 [D loss: 0.642776, acc: 61.72%] [G loss: 1.912794]\n",
      "epoch:15 step:14849 [D loss: 0.675926, acc: 61.72%] [G loss: 1.759370]\n",
      "epoch:15 step:14850 [D loss: 0.660877, acc: 58.59%] [G loss: 1.892810]\n",
      "epoch:15 step:14851 [D loss: 0.686898, acc: 53.91%] [G loss: 1.828754]\n",
      "epoch:15 step:14852 [D loss: 0.648633, acc: 62.50%] [G loss: 1.870740]\n",
      "epoch:15 step:14853 [D loss: 0.623167, acc: 64.84%] [G loss: 1.887639]\n",
      "epoch:15 step:14854 [D loss: 0.683597, acc: 58.59%] [G loss: 1.811332]\n",
      "epoch:15 step:14855 [D loss: 0.688591, acc: 55.47%] [G loss: 1.825014]\n",
      "epoch:15 step:14856 [D loss: 0.735926, acc: 51.56%] [G loss: 1.731324]\n",
      "epoch:15 step:14857 [D loss: 0.697426, acc: 54.69%] [G loss: 1.834044]\n",
      "epoch:15 step:14858 [D loss: 0.689030, acc: 60.16%] [G loss: 1.836018]\n",
      "epoch:15 step:14859 [D loss: 0.627349, acc: 65.62%] [G loss: 1.844327]\n",
      "epoch:15 step:14860 [D loss: 0.683955, acc: 54.69%] [G loss: 1.798728]\n",
      "epoch:15 step:14861 [D loss: 0.634707, acc: 64.84%] [G loss: 1.977223]\n",
      "epoch:15 step:14862 [D loss: 0.668052, acc: 60.94%] [G loss: 1.843300]\n",
      "epoch:15 step:14863 [D loss: 0.653255, acc: 66.41%] [G loss: 1.828405]\n",
      "epoch:15 step:14864 [D loss: 0.681983, acc: 60.16%] [G loss: 1.852788]\n",
      "epoch:15 step:14865 [D loss: 0.600437, acc: 70.31%] [G loss: 1.839441]\n",
      "epoch:15 step:14866 [D loss: 0.634414, acc: 61.72%] [G loss: 1.940636]\n",
      "epoch:15 step:14867 [D loss: 0.674144, acc: 60.16%] [G loss: 1.826854]\n",
      "epoch:15 step:14868 [D loss: 0.671584, acc: 61.72%] [G loss: 1.902158]\n",
      "epoch:15 step:14869 [D loss: 0.676185, acc: 65.62%] [G loss: 1.870273]\n",
      "epoch:15 step:14870 [D loss: 0.617943, acc: 66.41%] [G loss: 2.279889]\n",
      "epoch:15 step:14871 [D loss: 0.659902, acc: 60.94%] [G loss: 1.971009]\n",
      "epoch:15 step:14872 [D loss: 0.645580, acc: 62.50%] [G loss: 1.807314]\n",
      "epoch:15 step:14873 [D loss: 0.659369, acc: 60.16%] [G loss: 1.764705]\n",
      "epoch:15 step:14874 [D loss: 0.639184, acc: 68.75%] [G loss: 1.928560]\n",
      "epoch:15 step:14875 [D loss: 0.637482, acc: 65.62%] [G loss: 1.747692]\n",
      "epoch:15 step:14876 [D loss: 0.609618, acc: 69.53%] [G loss: 1.862014]\n",
      "epoch:15 step:14877 [D loss: 0.609560, acc: 65.62%] [G loss: 2.013046]\n",
      "epoch:15 step:14878 [D loss: 0.608720, acc: 69.53%] [G loss: 2.089873]\n",
      "epoch:15 step:14879 [D loss: 0.668602, acc: 58.59%] [G loss: 1.845172]\n",
      "epoch:15 step:14880 [D loss: 0.595196, acc: 71.09%] [G loss: 2.178648]\n",
      "epoch:15 step:14881 [D loss: 0.649957, acc: 64.06%] [G loss: 1.936710]\n",
      "epoch:15 step:14882 [D loss: 0.661812, acc: 61.72%] [G loss: 1.698939]\n",
      "epoch:15 step:14883 [D loss: 0.694879, acc: 54.69%] [G loss: 1.752678]\n",
      "epoch:15 step:14884 [D loss: 0.702953, acc: 51.56%] [G loss: 1.763500]\n",
      "epoch:15 step:14885 [D loss: 0.649125, acc: 64.84%] [G loss: 1.861295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:14886 [D loss: 0.666307, acc: 63.28%] [G loss: 1.843793]\n",
      "epoch:15 step:14887 [D loss: 0.610460, acc: 67.19%] [G loss: 1.896312]\n",
      "epoch:15 step:14888 [D loss: 0.640684, acc: 62.50%] [G loss: 1.937479]\n",
      "epoch:15 step:14889 [D loss: 0.664360, acc: 59.38%] [G loss: 1.963171]\n",
      "epoch:15 step:14890 [D loss: 0.625183, acc: 65.62%] [G loss: 1.889196]\n",
      "epoch:15 step:14891 [D loss: 0.612685, acc: 69.53%] [G loss: 1.909508]\n",
      "epoch:15 step:14892 [D loss: 0.594600, acc: 66.41%] [G loss: 1.979871]\n",
      "epoch:15 step:14893 [D loss: 0.644416, acc: 64.06%] [G loss: 2.085772]\n",
      "epoch:15 step:14894 [D loss: 0.629654, acc: 64.06%] [G loss: 1.942967]\n",
      "epoch:15 step:14895 [D loss: 0.645024, acc: 60.16%] [G loss: 1.826277]\n",
      "epoch:15 step:14896 [D loss: 0.634668, acc: 63.28%] [G loss: 2.005047]\n",
      "epoch:15 step:14897 [D loss: 0.644764, acc: 61.72%] [G loss: 2.025813]\n",
      "epoch:15 step:14898 [D loss: 0.620877, acc: 65.62%] [G loss: 1.894072]\n",
      "epoch:15 step:14899 [D loss: 0.690562, acc: 59.38%] [G loss: 1.898427]\n",
      "epoch:15 step:14900 [D loss: 0.651661, acc: 59.38%] [G loss: 2.052887]\n",
      "epoch:15 step:14901 [D loss: 0.703570, acc: 52.34%] [G loss: 1.752804]\n",
      "epoch:15 step:14902 [D loss: 0.665164, acc: 54.69%] [G loss: 1.998833]\n",
      "epoch:15 step:14903 [D loss: 0.605766, acc: 70.31%] [G loss: 1.931962]\n",
      "epoch:15 step:14904 [D loss: 0.641966, acc: 66.41%] [G loss: 2.093080]\n",
      "epoch:15 step:14905 [D loss: 0.677481, acc: 64.06%] [G loss: 1.988283]\n",
      "epoch:15 step:14906 [D loss: 0.626665, acc: 65.62%] [G loss: 2.095661]\n",
      "epoch:15 step:14907 [D loss: 0.697157, acc: 53.12%] [G loss: 1.891070]\n",
      "epoch:15 step:14908 [D loss: 0.618680, acc: 63.28%] [G loss: 1.877967]\n",
      "epoch:15 step:14909 [D loss: 0.650191, acc: 64.06%] [G loss: 1.890918]\n",
      "epoch:15 step:14910 [D loss: 0.661470, acc: 61.72%] [G loss: 1.982707]\n",
      "epoch:15 step:14911 [D loss: 0.700585, acc: 57.03%] [G loss: 1.846949]\n",
      "epoch:15 step:14912 [D loss: 0.667015, acc: 64.84%] [G loss: 1.816253]\n",
      "epoch:15 step:14913 [D loss: 0.664965, acc: 60.16%] [G loss: 1.743450]\n",
      "epoch:15 step:14914 [D loss: 0.643953, acc: 64.06%] [G loss: 1.888555]\n",
      "epoch:15 step:14915 [D loss: 0.638503, acc: 63.28%] [G loss: 1.807978]\n",
      "epoch:15 step:14916 [D loss: 0.666038, acc: 58.59%] [G loss: 1.721374]\n",
      "epoch:15 step:14917 [D loss: 0.696176, acc: 55.47%] [G loss: 1.834703]\n",
      "epoch:15 step:14918 [D loss: 0.645129, acc: 63.28%] [G loss: 1.913794]\n",
      "epoch:15 step:14919 [D loss: 0.646286, acc: 54.69%] [G loss: 1.884251]\n",
      "epoch:15 step:14920 [D loss: 0.702515, acc: 57.03%] [G loss: 1.819218]\n",
      "epoch:15 step:14921 [D loss: 0.633792, acc: 70.31%] [G loss: 1.765821]\n",
      "epoch:15 step:14922 [D loss: 0.674437, acc: 64.06%] [G loss: 1.779249]\n",
      "epoch:15 step:14923 [D loss: 0.650235, acc: 63.28%] [G loss: 1.785053]\n",
      "epoch:15 step:14924 [D loss: 0.674235, acc: 59.38%] [G loss: 1.776189]\n",
      "epoch:15 step:14925 [D loss: 0.649181, acc: 64.06%] [G loss: 1.965150]\n",
      "epoch:15 step:14926 [D loss: 0.632837, acc: 60.16%] [G loss: 1.740223]\n",
      "epoch:15 step:14927 [D loss: 0.638691, acc: 64.06%] [G loss: 1.869555]\n",
      "epoch:15 step:14928 [D loss: 0.661294, acc: 62.50%] [G loss: 1.725764]\n",
      "epoch:15 step:14929 [D loss: 0.682811, acc: 53.91%] [G loss: 1.831586]\n",
      "epoch:15 step:14930 [D loss: 0.628767, acc: 65.62%] [G loss: 1.895207]\n",
      "epoch:15 step:14931 [D loss: 0.655861, acc: 59.38%] [G loss: 1.808818]\n",
      "epoch:15 step:14932 [D loss: 0.669986, acc: 58.59%] [G loss: 1.902766]\n",
      "epoch:15 step:14933 [D loss: 0.648512, acc: 62.50%] [G loss: 1.897049]\n",
      "epoch:15 step:14934 [D loss: 0.611258, acc: 69.53%] [G loss: 1.892768]\n",
      "epoch:15 step:14935 [D loss: 0.652871, acc: 58.59%] [G loss: 2.033608]\n",
      "epoch:15 step:14936 [D loss: 0.634292, acc: 57.81%] [G loss: 1.932226]\n",
      "epoch:15 step:14937 [D loss: 0.646748, acc: 65.62%] [G loss: 1.952950]\n",
      "epoch:15 step:14938 [D loss: 0.636505, acc: 53.91%] [G loss: 1.925155]\n",
      "epoch:15 step:14939 [D loss: 0.673359, acc: 59.38%] [G loss: 1.993029]\n",
      "epoch:15 step:14940 [D loss: 0.622391, acc: 71.09%] [G loss: 1.920861]\n",
      "epoch:15 step:14941 [D loss: 0.615644, acc: 64.06%] [G loss: 2.000747]\n",
      "epoch:15 step:14942 [D loss: 0.624594, acc: 68.75%] [G loss: 1.946989]\n",
      "epoch:15 step:14943 [D loss: 0.685069, acc: 59.38%] [G loss: 2.045526]\n",
      "epoch:15 step:14944 [D loss: 0.645078, acc: 63.28%] [G loss: 1.854293]\n",
      "epoch:15 step:14945 [D loss: 0.560135, acc: 68.75%] [G loss: 1.974952]\n",
      "epoch:15 step:14946 [D loss: 0.679990, acc: 57.03%] [G loss: 1.839893]\n",
      "epoch:15 step:14947 [D loss: 0.695548, acc: 45.31%] [G loss: 1.809855]\n",
      "epoch:15 step:14948 [D loss: 0.731731, acc: 52.34%] [G loss: 1.910984]\n",
      "epoch:15 step:14949 [D loss: 0.606838, acc: 67.97%] [G loss: 1.924046]\n",
      "epoch:15 step:14950 [D loss: 0.600116, acc: 71.88%] [G loss: 1.965762]\n",
      "epoch:15 step:14951 [D loss: 0.642699, acc: 58.59%] [G loss: 1.879279]\n",
      "epoch:15 step:14952 [D loss: 0.664961, acc: 53.12%] [G loss: 1.851321]\n",
      "epoch:15 step:14953 [D loss: 0.689552, acc: 60.16%] [G loss: 1.972233]\n",
      "epoch:15 step:14954 [D loss: 0.626374, acc: 64.84%] [G loss: 2.139571]\n",
      "epoch:15 step:14955 [D loss: 0.621954, acc: 65.62%] [G loss: 1.931478]\n",
      "epoch:15 step:14956 [D loss: 0.664489, acc: 60.94%] [G loss: 1.929058]\n",
      "epoch:15 step:14957 [D loss: 0.669917, acc: 63.28%] [G loss: 1.891546]\n",
      "epoch:15 step:14958 [D loss: 0.622284, acc: 64.84%] [G loss: 2.037183]\n",
      "epoch:15 step:14959 [D loss: 0.663160, acc: 62.50%] [G loss: 1.907205]\n",
      "epoch:15 step:14960 [D loss: 0.645350, acc: 64.84%] [G loss: 1.898258]\n",
      "epoch:15 step:14961 [D loss: 0.645235, acc: 62.50%] [G loss: 2.016597]\n",
      "epoch:15 step:14962 [D loss: 0.603267, acc: 75.00%] [G loss: 1.956137]\n",
      "epoch:15 step:14963 [D loss: 0.637779, acc: 60.94%] [G loss: 1.887993]\n",
      "epoch:15 step:14964 [D loss: 0.588859, acc: 70.31%] [G loss: 2.074805]\n",
      "epoch:15 step:14965 [D loss: 0.641187, acc: 59.38%] [G loss: 1.998484]\n",
      "epoch:15 step:14966 [D loss: 0.677454, acc: 56.25%] [G loss: 1.919434]\n",
      "epoch:15 step:14967 [D loss: 0.611671, acc: 67.19%] [G loss: 2.180408]\n",
      "epoch:15 step:14968 [D loss: 0.657702, acc: 60.94%] [G loss: 1.972072]\n",
      "epoch:15 step:14969 [D loss: 0.647454, acc: 63.28%] [G loss: 1.980626]\n",
      "epoch:15 step:14970 [D loss: 0.675760, acc: 59.38%] [G loss: 1.887628]\n",
      "epoch:15 step:14971 [D loss: 0.630652, acc: 65.62%] [G loss: 2.035983]\n",
      "epoch:15 step:14972 [D loss: 0.645580, acc: 64.84%] [G loss: 2.051506]\n",
      "epoch:15 step:14973 [D loss: 0.614438, acc: 64.84%] [G loss: 2.059120]\n",
      "epoch:15 step:14974 [D loss: 0.589421, acc: 71.09%] [G loss: 2.194916]\n",
      "epoch:15 step:14975 [D loss: 0.717432, acc: 54.69%] [G loss: 1.757284]\n",
      "epoch:15 step:14976 [D loss: 0.650757, acc: 64.06%] [G loss: 1.968752]\n",
      "epoch:15 step:14977 [D loss: 0.649957, acc: 57.03%] [G loss: 1.982698]\n",
      "epoch:15 step:14978 [D loss: 0.605699, acc: 68.75%] [G loss: 2.118586]\n",
      "epoch:15 step:14979 [D loss: 0.583152, acc: 72.66%] [G loss: 2.130416]\n",
      "epoch:15 step:14980 [D loss: 0.547385, acc: 74.22%] [G loss: 2.202783]\n",
      "epoch:15 step:14981 [D loss: 0.590355, acc: 69.53%] [G loss: 2.323376]\n",
      "epoch:15 step:14982 [D loss: 0.643566, acc: 64.06%] [G loss: 2.241206]\n",
      "epoch:15 step:14983 [D loss: 0.804298, acc: 46.88%] [G loss: 1.833585]\n",
      "epoch:15 step:14984 [D loss: 0.819405, acc: 38.28%] [G loss: 1.831105]\n",
      "epoch:15 step:14985 [D loss: 0.577182, acc: 72.66%] [G loss: 2.060736]\n",
      "epoch:15 step:14986 [D loss: 0.602805, acc: 67.19%] [G loss: 1.978419]\n",
      "epoch:15 step:14987 [D loss: 0.667255, acc: 67.19%] [G loss: 1.945235]\n",
      "epoch:15 step:14988 [D loss: 0.662077, acc: 63.28%] [G loss: 1.954621]\n",
      "epoch:15 step:14989 [D loss: 0.651549, acc: 65.62%] [G loss: 1.972586]\n",
      "epoch:15 step:14990 [D loss: 0.596939, acc: 67.19%] [G loss: 2.141899]\n",
      "epoch:15 step:14991 [D loss: 0.556281, acc: 71.09%] [G loss: 2.199062]\n",
      "epoch:15 step:14992 [D loss: 0.587243, acc: 69.53%] [G loss: 2.423774]\n",
      "epoch:16 step:14993 [D loss: 0.714290, acc: 54.69%] [G loss: 1.922944]\n",
      "epoch:16 step:14994 [D loss: 0.665543, acc: 60.94%] [G loss: 2.047193]\n",
      "epoch:16 step:14995 [D loss: 0.596826, acc: 67.97%] [G loss: 1.997542]\n",
      "epoch:16 step:14996 [D loss: 0.620509, acc: 69.53%] [G loss: 1.789734]\n",
      "epoch:16 step:14997 [D loss: 0.626690, acc: 67.19%] [G loss: 1.864028]\n",
      "epoch:16 step:14998 [D loss: 0.631568, acc: 67.19%] [G loss: 1.903847]\n",
      "epoch:16 step:14999 [D loss: 0.662303, acc: 66.41%] [G loss: 1.984437]\n",
      "epoch:16 step:15000 [D loss: 0.601348, acc: 62.50%] [G loss: 2.045507]\n",
      "epoch:16 step:15001 [D loss: 0.635264, acc: 63.28%] [G loss: 1.960288]\n",
      "epoch:16 step:15002 [D loss: 0.576231, acc: 71.88%] [G loss: 2.015454]\n",
      "epoch:16 step:15003 [D loss: 0.606973, acc: 68.75%] [G loss: 1.903762]\n",
      "epoch:16 step:15004 [D loss: 0.652163, acc: 64.06%] [G loss: 1.918749]\n",
      "epoch:16 step:15005 [D loss: 0.619835, acc: 67.97%] [G loss: 1.993649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15006 [D loss: 0.655572, acc: 64.84%] [G loss: 1.885999]\n",
      "epoch:16 step:15007 [D loss: 0.608027, acc: 64.06%] [G loss: 2.219026]\n",
      "epoch:16 step:15008 [D loss: 0.565618, acc: 75.78%] [G loss: 2.137197]\n",
      "epoch:16 step:15009 [D loss: 0.615887, acc: 68.75%] [G loss: 1.949651]\n",
      "epoch:16 step:15010 [D loss: 0.685805, acc: 60.16%] [G loss: 2.039504]\n",
      "epoch:16 step:15011 [D loss: 0.659741, acc: 62.50%] [G loss: 1.886402]\n",
      "epoch:16 step:15012 [D loss: 0.695560, acc: 53.91%] [G loss: 1.605221]\n",
      "epoch:16 step:15013 [D loss: 0.644449, acc: 61.72%] [G loss: 1.995612]\n",
      "epoch:16 step:15014 [D loss: 0.672210, acc: 53.91%] [G loss: 1.869966]\n",
      "epoch:16 step:15015 [D loss: 0.604783, acc: 65.62%] [G loss: 2.092170]\n",
      "epoch:16 step:15016 [D loss: 0.567530, acc: 71.88%] [G loss: 2.103386]\n",
      "epoch:16 step:15017 [D loss: 0.612030, acc: 67.19%] [G loss: 2.008985]\n",
      "epoch:16 step:15018 [D loss: 0.685078, acc: 57.03%] [G loss: 1.854846]\n",
      "epoch:16 step:15019 [D loss: 0.661775, acc: 57.03%] [G loss: 1.812720]\n",
      "epoch:16 step:15020 [D loss: 0.581859, acc: 68.75%] [G loss: 1.945890]\n",
      "epoch:16 step:15021 [D loss: 0.643300, acc: 57.03%] [G loss: 1.996532]\n",
      "epoch:16 step:15022 [D loss: 0.649754, acc: 59.38%] [G loss: 1.934209]\n",
      "epoch:16 step:15023 [D loss: 0.757259, acc: 49.22%] [G loss: 1.664189]\n",
      "epoch:16 step:15024 [D loss: 0.678315, acc: 57.81%] [G loss: 1.910869]\n",
      "epoch:16 step:15025 [D loss: 0.658974, acc: 60.94%] [G loss: 1.707036]\n",
      "epoch:16 step:15026 [D loss: 0.624460, acc: 63.28%] [G loss: 1.900352]\n",
      "epoch:16 step:15027 [D loss: 0.629314, acc: 66.41%] [G loss: 1.831024]\n",
      "epoch:16 step:15028 [D loss: 0.645291, acc: 61.72%] [G loss: 1.932243]\n",
      "epoch:16 step:15029 [D loss: 0.621536, acc: 64.06%] [G loss: 2.046703]\n",
      "epoch:16 step:15030 [D loss: 0.677366, acc: 64.06%] [G loss: 1.914288]\n",
      "epoch:16 step:15031 [D loss: 0.627782, acc: 64.06%] [G loss: 1.905606]\n",
      "epoch:16 step:15032 [D loss: 0.625646, acc: 63.28%] [G loss: 2.168430]\n",
      "epoch:16 step:15033 [D loss: 0.650561, acc: 62.50%] [G loss: 1.889498]\n",
      "epoch:16 step:15034 [D loss: 0.670263, acc: 60.16%] [G loss: 2.022012]\n",
      "epoch:16 step:15035 [D loss: 0.629475, acc: 64.06%] [G loss: 1.922777]\n",
      "epoch:16 step:15036 [D loss: 0.640011, acc: 63.28%] [G loss: 1.840114]\n",
      "epoch:16 step:15037 [D loss: 0.676094, acc: 57.81%] [G loss: 1.862323]\n",
      "epoch:16 step:15038 [D loss: 0.604540, acc: 68.75%] [G loss: 1.852963]\n",
      "epoch:16 step:15039 [D loss: 0.688151, acc: 53.91%] [G loss: 2.092507]\n",
      "epoch:16 step:15040 [D loss: 0.538794, acc: 72.66%] [G loss: 2.115556]\n",
      "epoch:16 step:15041 [D loss: 0.618668, acc: 66.41%] [G loss: 1.980458]\n",
      "epoch:16 step:15042 [D loss: 0.638691, acc: 55.47%] [G loss: 2.049557]\n",
      "epoch:16 step:15043 [D loss: 0.697903, acc: 55.47%] [G loss: 1.849985]\n",
      "epoch:16 step:15044 [D loss: 0.700405, acc: 54.69%] [G loss: 1.837450]\n",
      "epoch:16 step:15045 [D loss: 0.605150, acc: 67.97%] [G loss: 1.745498]\n",
      "epoch:16 step:15046 [D loss: 0.553791, acc: 77.34%] [G loss: 2.057267]\n",
      "epoch:16 step:15047 [D loss: 0.587533, acc: 69.53%] [G loss: 2.014504]\n",
      "epoch:16 step:15048 [D loss: 0.719756, acc: 55.47%] [G loss: 2.034899]\n",
      "epoch:16 step:15049 [D loss: 0.713976, acc: 53.91%] [G loss: 1.947999]\n",
      "epoch:16 step:15050 [D loss: 0.625746, acc: 67.97%] [G loss: 1.774026]\n",
      "epoch:16 step:15051 [D loss: 0.600641, acc: 70.31%] [G loss: 1.819808]\n",
      "epoch:16 step:15052 [D loss: 0.647316, acc: 59.38%] [G loss: 1.727991]\n",
      "epoch:16 step:15053 [D loss: 0.678368, acc: 57.81%] [G loss: 1.877677]\n",
      "epoch:16 step:15054 [D loss: 0.609796, acc: 65.62%] [G loss: 1.845460]\n",
      "epoch:16 step:15055 [D loss: 0.618302, acc: 66.41%] [G loss: 2.019022]\n",
      "epoch:16 step:15056 [D loss: 0.651163, acc: 63.28%] [G loss: 1.860007]\n",
      "epoch:16 step:15057 [D loss: 0.602911, acc: 71.88%] [G loss: 1.905843]\n",
      "epoch:16 step:15058 [D loss: 0.620499, acc: 64.84%] [G loss: 1.930830]\n",
      "epoch:16 step:15059 [D loss: 0.637481, acc: 60.16%] [G loss: 1.930524]\n",
      "epoch:16 step:15060 [D loss: 0.637451, acc: 62.50%] [G loss: 1.911567]\n",
      "epoch:16 step:15061 [D loss: 0.637751, acc: 67.19%] [G loss: 1.979185]\n",
      "epoch:16 step:15062 [D loss: 0.643498, acc: 63.28%] [G loss: 1.985383]\n",
      "epoch:16 step:15063 [D loss: 0.685729, acc: 60.16%] [G loss: 1.891818]\n",
      "epoch:16 step:15064 [D loss: 0.665960, acc: 59.38%] [G loss: 2.037735]\n",
      "epoch:16 step:15065 [D loss: 0.657949, acc: 54.69%] [G loss: 1.874291]\n",
      "epoch:16 step:15066 [D loss: 0.631380, acc: 63.28%] [G loss: 1.967487]\n",
      "epoch:16 step:15067 [D loss: 0.610855, acc: 68.75%] [G loss: 2.255237]\n",
      "epoch:16 step:15068 [D loss: 0.644370, acc: 62.50%] [G loss: 2.041502]\n",
      "epoch:16 step:15069 [D loss: 0.588394, acc: 70.31%] [G loss: 2.163253]\n",
      "epoch:16 step:15070 [D loss: 0.696232, acc: 57.81%] [G loss: 1.854841]\n",
      "epoch:16 step:15071 [D loss: 0.691676, acc: 60.16%] [G loss: 1.876312]\n",
      "epoch:16 step:15072 [D loss: 0.677971, acc: 57.03%] [G loss: 1.867518]\n",
      "epoch:16 step:15073 [D loss: 0.741312, acc: 50.78%] [G loss: 1.757485]\n",
      "epoch:16 step:15074 [D loss: 0.638695, acc: 60.94%] [G loss: 1.906742]\n",
      "epoch:16 step:15075 [D loss: 0.602780, acc: 69.53%] [G loss: 2.002092]\n",
      "epoch:16 step:15076 [D loss: 0.651997, acc: 55.47%] [G loss: 1.954194]\n",
      "epoch:16 step:15077 [D loss: 0.682867, acc: 61.72%] [G loss: 1.860566]\n",
      "epoch:16 step:15078 [D loss: 0.691596, acc: 54.69%] [G loss: 1.788010]\n",
      "epoch:16 step:15079 [D loss: 0.650604, acc: 64.06%] [G loss: 1.869580]\n",
      "epoch:16 step:15080 [D loss: 0.601305, acc: 70.31%] [G loss: 1.886917]\n",
      "epoch:16 step:15081 [D loss: 0.592983, acc: 67.97%] [G loss: 1.864549]\n",
      "epoch:16 step:15082 [D loss: 0.664950, acc: 65.62%] [G loss: 2.009928]\n",
      "epoch:16 step:15083 [D loss: 0.717853, acc: 49.22%] [G loss: 1.897906]\n",
      "epoch:16 step:15084 [D loss: 0.612392, acc: 67.19%] [G loss: 1.931169]\n",
      "epoch:16 step:15085 [D loss: 0.554996, acc: 78.12%] [G loss: 2.054168]\n",
      "epoch:16 step:15086 [D loss: 0.656434, acc: 58.59%] [G loss: 1.920594]\n",
      "epoch:16 step:15087 [D loss: 0.641187, acc: 64.06%] [G loss: 1.803138]\n",
      "epoch:16 step:15088 [D loss: 0.629734, acc: 67.97%] [G loss: 1.994347]\n",
      "epoch:16 step:15089 [D loss: 0.587862, acc: 67.19%] [G loss: 1.868719]\n",
      "epoch:16 step:15090 [D loss: 0.670907, acc: 58.59%] [G loss: 1.784326]\n",
      "epoch:16 step:15091 [D loss: 0.641052, acc: 64.06%] [G loss: 1.940076]\n",
      "epoch:16 step:15092 [D loss: 0.638958, acc: 68.75%] [G loss: 1.885479]\n",
      "epoch:16 step:15093 [D loss: 0.657113, acc: 59.38%] [G loss: 2.018709]\n",
      "epoch:16 step:15094 [D loss: 0.664569, acc: 58.59%] [G loss: 1.870946]\n",
      "epoch:16 step:15095 [D loss: 0.637281, acc: 63.28%] [G loss: 1.787518]\n",
      "epoch:16 step:15096 [D loss: 0.614517, acc: 69.53%] [G loss: 1.878701]\n",
      "epoch:16 step:15097 [D loss: 0.645648, acc: 64.06%] [G loss: 1.798988]\n",
      "epoch:16 step:15098 [D loss: 0.604813, acc: 64.84%] [G loss: 2.001539]\n",
      "epoch:16 step:15099 [D loss: 0.611109, acc: 68.75%] [G loss: 2.179016]\n",
      "epoch:16 step:15100 [D loss: 0.706513, acc: 59.38%] [G loss: 1.867246]\n",
      "epoch:16 step:15101 [D loss: 0.668182, acc: 64.84%] [G loss: 1.883694]\n",
      "epoch:16 step:15102 [D loss: 0.703935, acc: 58.59%] [G loss: 1.852629]\n",
      "epoch:16 step:15103 [D loss: 0.633655, acc: 63.28%] [G loss: 1.895581]\n",
      "epoch:16 step:15104 [D loss: 0.630695, acc: 59.38%] [G loss: 1.917093]\n",
      "epoch:16 step:15105 [D loss: 0.637539, acc: 60.94%] [G loss: 2.003312]\n",
      "epoch:16 step:15106 [D loss: 0.634309, acc: 64.84%] [G loss: 2.006999]\n",
      "epoch:16 step:15107 [D loss: 0.630533, acc: 63.28%] [G loss: 2.071882]\n",
      "epoch:16 step:15108 [D loss: 0.633394, acc: 60.94%] [G loss: 2.136797]\n",
      "epoch:16 step:15109 [D loss: 0.628318, acc: 61.72%] [G loss: 2.047724]\n",
      "epoch:16 step:15110 [D loss: 0.637994, acc: 62.50%] [G loss: 2.159902]\n",
      "epoch:16 step:15111 [D loss: 0.692443, acc: 59.38%] [G loss: 2.151357]\n",
      "epoch:16 step:15112 [D loss: 0.661292, acc: 59.38%] [G loss: 1.908359]\n",
      "epoch:16 step:15113 [D loss: 0.649809, acc: 61.72%] [G loss: 1.939605]\n",
      "epoch:16 step:15114 [D loss: 0.630489, acc: 64.84%] [G loss: 2.041502]\n",
      "epoch:16 step:15115 [D loss: 0.652775, acc: 62.50%] [G loss: 2.020612]\n",
      "epoch:16 step:15116 [D loss: 0.623977, acc: 63.28%] [G loss: 1.952407]\n",
      "epoch:16 step:15117 [D loss: 0.685133, acc: 53.91%] [G loss: 1.727869]\n",
      "epoch:16 step:15118 [D loss: 0.575076, acc: 71.09%] [G loss: 2.015317]\n",
      "epoch:16 step:15119 [D loss: 0.684576, acc: 53.91%] [G loss: 1.840649]\n",
      "epoch:16 step:15120 [D loss: 0.592097, acc: 67.97%] [G loss: 2.081678]\n",
      "epoch:16 step:15121 [D loss: 0.667994, acc: 58.59%] [G loss: 1.799126]\n",
      "epoch:16 step:15122 [D loss: 0.661920, acc: 61.72%] [G loss: 1.907744]\n",
      "epoch:16 step:15123 [D loss: 0.626650, acc: 60.94%] [G loss: 2.016121]\n",
      "epoch:16 step:15124 [D loss: 0.654129, acc: 57.81%] [G loss: 1.972509]\n",
      "epoch:16 step:15125 [D loss: 0.659908, acc: 58.59%] [G loss: 1.882461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15126 [D loss: 0.694432, acc: 56.25%] [G loss: 1.946785]\n",
      "epoch:16 step:15127 [D loss: 0.694282, acc: 55.47%] [G loss: 1.790417]\n",
      "epoch:16 step:15128 [D loss: 0.659417, acc: 60.16%] [G loss: 1.736204]\n",
      "epoch:16 step:15129 [D loss: 0.658576, acc: 59.38%] [G loss: 1.793327]\n",
      "epoch:16 step:15130 [D loss: 0.668614, acc: 60.16%] [G loss: 1.829006]\n",
      "epoch:16 step:15131 [D loss: 0.699248, acc: 57.81%] [G loss: 1.888854]\n",
      "epoch:16 step:15132 [D loss: 0.628589, acc: 61.72%] [G loss: 1.856330]\n",
      "epoch:16 step:15133 [D loss: 0.671312, acc: 58.59%] [G loss: 1.779223]\n",
      "epoch:16 step:15134 [D loss: 0.667839, acc: 57.03%] [G loss: 1.858051]\n",
      "epoch:16 step:15135 [D loss: 0.675696, acc: 58.59%] [G loss: 1.728093]\n",
      "epoch:16 step:15136 [D loss: 0.698629, acc: 53.12%] [G loss: 1.852499]\n",
      "epoch:16 step:15137 [D loss: 0.662753, acc: 61.72%] [G loss: 1.860093]\n",
      "epoch:16 step:15138 [D loss: 0.620597, acc: 65.62%] [G loss: 2.095816]\n",
      "epoch:16 step:15139 [D loss: 0.669321, acc: 64.06%] [G loss: 1.749356]\n",
      "epoch:16 step:15140 [D loss: 0.655534, acc: 61.72%] [G loss: 1.815705]\n",
      "epoch:16 step:15141 [D loss: 0.669675, acc: 59.38%] [G loss: 1.743581]\n",
      "epoch:16 step:15142 [D loss: 0.621946, acc: 67.97%] [G loss: 1.841722]\n",
      "epoch:16 step:15143 [D loss: 0.615736, acc: 64.84%] [G loss: 1.832170]\n",
      "epoch:16 step:15144 [D loss: 0.668890, acc: 62.50%] [G loss: 1.884065]\n",
      "epoch:16 step:15145 [D loss: 0.644283, acc: 60.94%] [G loss: 1.871531]\n",
      "epoch:16 step:15146 [D loss: 0.643764, acc: 57.81%] [G loss: 2.038677]\n",
      "epoch:16 step:15147 [D loss: 0.615300, acc: 69.53%] [G loss: 1.930845]\n",
      "epoch:16 step:15148 [D loss: 0.626799, acc: 63.28%] [G loss: 2.037388]\n",
      "epoch:16 step:15149 [D loss: 0.625627, acc: 64.06%] [G loss: 1.830997]\n",
      "epoch:16 step:15150 [D loss: 0.625608, acc: 62.50%] [G loss: 1.923957]\n",
      "epoch:16 step:15151 [D loss: 0.601694, acc: 65.62%] [G loss: 2.118466]\n",
      "epoch:16 step:15152 [D loss: 0.652303, acc: 57.81%] [G loss: 1.733381]\n",
      "epoch:16 step:15153 [D loss: 0.638167, acc: 62.50%] [G loss: 1.801629]\n",
      "epoch:16 step:15154 [D loss: 0.650992, acc: 56.25%] [G loss: 1.886982]\n",
      "epoch:16 step:15155 [D loss: 0.682058, acc: 59.38%] [G loss: 1.908947]\n",
      "epoch:16 step:15156 [D loss: 0.604977, acc: 64.84%] [G loss: 1.804514]\n",
      "epoch:16 step:15157 [D loss: 0.636935, acc: 60.16%] [G loss: 1.878642]\n",
      "epoch:16 step:15158 [D loss: 0.617279, acc: 63.28%] [G loss: 1.803169]\n",
      "epoch:16 step:15159 [D loss: 0.573012, acc: 75.00%] [G loss: 1.907329]\n",
      "epoch:16 step:15160 [D loss: 0.669597, acc: 57.03%] [G loss: 2.003919]\n",
      "epoch:16 step:15161 [D loss: 0.679184, acc: 55.47%] [G loss: 1.796319]\n",
      "epoch:16 step:15162 [D loss: 0.617098, acc: 62.50%] [G loss: 1.902246]\n",
      "epoch:16 step:15163 [D loss: 0.593193, acc: 68.75%] [G loss: 1.908295]\n",
      "epoch:16 step:15164 [D loss: 0.644617, acc: 56.25%] [G loss: 1.926552]\n",
      "epoch:16 step:15165 [D loss: 0.655241, acc: 55.47%] [G loss: 1.880040]\n",
      "epoch:16 step:15166 [D loss: 0.679743, acc: 63.28%] [G loss: 1.798991]\n",
      "epoch:16 step:15167 [D loss: 0.646749, acc: 58.59%] [G loss: 1.871018]\n",
      "epoch:16 step:15168 [D loss: 0.688965, acc: 56.25%] [G loss: 1.919286]\n",
      "epoch:16 step:15169 [D loss: 0.689580, acc: 53.91%] [G loss: 1.921293]\n",
      "epoch:16 step:15170 [D loss: 0.668667, acc: 56.25%] [G loss: 1.803548]\n",
      "epoch:16 step:15171 [D loss: 0.648502, acc: 64.06%] [G loss: 1.895460]\n",
      "epoch:16 step:15172 [D loss: 0.607517, acc: 65.62%] [G loss: 2.021286]\n",
      "epoch:16 step:15173 [D loss: 0.645166, acc: 68.75%] [G loss: 1.848115]\n",
      "epoch:16 step:15174 [D loss: 0.655015, acc: 60.94%] [G loss: 1.837446]\n",
      "epoch:16 step:15175 [D loss: 0.648575, acc: 57.03%] [G loss: 1.999357]\n",
      "epoch:16 step:15176 [D loss: 0.688621, acc: 62.50%] [G loss: 1.880593]\n",
      "epoch:16 step:15177 [D loss: 0.642903, acc: 53.91%] [G loss: 1.881828]\n",
      "epoch:16 step:15178 [D loss: 0.659917, acc: 60.94%] [G loss: 1.966730]\n",
      "epoch:16 step:15179 [D loss: 0.633416, acc: 64.06%] [G loss: 1.832889]\n",
      "epoch:16 step:15180 [D loss: 0.674041, acc: 57.81%] [G loss: 1.847059]\n",
      "epoch:16 step:15181 [D loss: 0.603604, acc: 65.62%] [G loss: 1.965683]\n",
      "epoch:16 step:15182 [D loss: 0.625186, acc: 67.19%] [G loss: 2.105279]\n",
      "epoch:16 step:15183 [D loss: 0.592525, acc: 67.97%] [G loss: 1.967761]\n",
      "epoch:16 step:15184 [D loss: 0.614972, acc: 66.41%] [G loss: 2.100904]\n",
      "epoch:16 step:15185 [D loss: 0.606351, acc: 68.75%] [G loss: 2.114094]\n",
      "epoch:16 step:15186 [D loss: 0.650903, acc: 64.06%] [G loss: 2.148348]\n",
      "epoch:16 step:15187 [D loss: 0.639717, acc: 71.88%] [G loss: 1.917279]\n",
      "epoch:16 step:15188 [D loss: 0.624879, acc: 62.50%] [G loss: 1.796250]\n",
      "epoch:16 step:15189 [D loss: 0.635219, acc: 64.06%] [G loss: 1.990972]\n",
      "epoch:16 step:15190 [D loss: 0.611701, acc: 67.97%] [G loss: 1.845367]\n",
      "epoch:16 step:15191 [D loss: 0.644575, acc: 67.97%] [G loss: 1.913809]\n",
      "epoch:16 step:15192 [D loss: 0.660203, acc: 59.38%] [G loss: 1.787748]\n",
      "epoch:16 step:15193 [D loss: 0.676848, acc: 59.38%] [G loss: 2.153660]\n",
      "epoch:16 step:15194 [D loss: 0.636056, acc: 65.62%] [G loss: 1.889445]\n",
      "epoch:16 step:15195 [D loss: 0.639364, acc: 64.06%] [G loss: 1.909279]\n",
      "epoch:16 step:15196 [D loss: 0.638036, acc: 61.72%] [G loss: 1.891975]\n",
      "epoch:16 step:15197 [D loss: 0.665149, acc: 62.50%] [G loss: 1.794653]\n",
      "epoch:16 step:15198 [D loss: 0.677712, acc: 58.59%] [G loss: 1.998319]\n",
      "epoch:16 step:15199 [D loss: 0.627351, acc: 67.97%] [G loss: 2.046541]\n",
      "epoch:16 step:15200 [D loss: 0.578264, acc: 67.97%] [G loss: 2.235717]\n",
      "epoch:16 step:15201 [D loss: 0.593675, acc: 67.97%] [G loss: 2.299660]\n",
      "epoch:16 step:15202 [D loss: 0.673127, acc: 60.94%] [G loss: 1.869008]\n",
      "epoch:16 step:15203 [D loss: 0.681070, acc: 55.47%] [G loss: 1.846807]\n",
      "epoch:16 step:15204 [D loss: 0.674381, acc: 57.03%] [G loss: 1.742857]\n",
      "epoch:16 step:15205 [D loss: 0.669985, acc: 62.50%] [G loss: 1.877316]\n",
      "epoch:16 step:15206 [D loss: 0.669438, acc: 59.38%] [G loss: 1.827651]\n",
      "epoch:16 step:15207 [D loss: 0.647179, acc: 57.81%] [G loss: 1.801201]\n",
      "epoch:16 step:15208 [D loss: 0.669184, acc: 63.28%] [G loss: 1.830292]\n",
      "epoch:16 step:15209 [D loss: 0.602033, acc: 67.19%] [G loss: 2.073828]\n",
      "epoch:16 step:15210 [D loss: 0.576995, acc: 73.44%] [G loss: 1.971831]\n",
      "epoch:16 step:15211 [D loss: 0.582630, acc: 71.88%] [G loss: 2.046717]\n",
      "epoch:16 step:15212 [D loss: 0.685913, acc: 57.81%] [G loss: 1.916559]\n",
      "epoch:16 step:15213 [D loss: 0.666987, acc: 63.28%] [G loss: 1.934760]\n",
      "epoch:16 step:15214 [D loss: 0.624104, acc: 64.06%] [G loss: 1.936392]\n",
      "epoch:16 step:15215 [D loss: 0.610184, acc: 70.31%] [G loss: 1.982156]\n",
      "epoch:16 step:15216 [D loss: 0.624380, acc: 65.62%] [G loss: 1.932009]\n",
      "epoch:16 step:15217 [D loss: 0.679503, acc: 58.59%] [G loss: 1.911434]\n",
      "epoch:16 step:15218 [D loss: 0.648874, acc: 64.06%] [G loss: 1.908948]\n",
      "epoch:16 step:15219 [D loss: 0.677707, acc: 62.50%] [G loss: 1.775670]\n",
      "epoch:16 step:15220 [D loss: 0.683138, acc: 57.03%] [G loss: 1.794946]\n",
      "epoch:16 step:15221 [D loss: 0.624127, acc: 67.19%] [G loss: 2.134202]\n",
      "epoch:16 step:15222 [D loss: 0.596760, acc: 70.31%] [G loss: 2.090765]\n",
      "epoch:16 step:15223 [D loss: 0.607455, acc: 65.62%] [G loss: 2.375240]\n",
      "epoch:16 step:15224 [D loss: 0.550571, acc: 71.88%] [G loss: 2.380311]\n",
      "epoch:16 step:15225 [D loss: 0.691227, acc: 60.16%] [G loss: 1.936198]\n",
      "epoch:16 step:15226 [D loss: 0.647774, acc: 60.16%] [G loss: 1.924501]\n",
      "epoch:16 step:15227 [D loss: 0.651018, acc: 59.38%] [G loss: 1.894475]\n",
      "epoch:16 step:15228 [D loss: 0.586651, acc: 71.09%] [G loss: 1.960905]\n",
      "epoch:16 step:15229 [D loss: 0.647275, acc: 60.94%] [G loss: 2.021337]\n",
      "epoch:16 step:15230 [D loss: 0.654566, acc: 58.59%] [G loss: 1.954043]\n",
      "epoch:16 step:15231 [D loss: 0.661623, acc: 57.03%] [G loss: 1.774831]\n",
      "epoch:16 step:15232 [D loss: 0.628817, acc: 63.28%] [G loss: 1.947671]\n",
      "epoch:16 step:15233 [D loss: 0.634357, acc: 67.19%] [G loss: 1.991039]\n",
      "epoch:16 step:15234 [D loss: 0.666573, acc: 58.59%] [G loss: 1.906447]\n",
      "epoch:16 step:15235 [D loss: 0.572180, acc: 71.09%] [G loss: 1.987813]\n",
      "epoch:16 step:15236 [D loss: 0.645373, acc: 64.84%] [G loss: 1.896325]\n",
      "epoch:16 step:15237 [D loss: 0.690768, acc: 61.72%] [G loss: 1.882058]\n",
      "epoch:16 step:15238 [D loss: 0.637463, acc: 60.94%] [G loss: 1.950628]\n",
      "epoch:16 step:15239 [D loss: 0.617051, acc: 69.53%] [G loss: 2.060431]\n",
      "epoch:16 step:15240 [D loss: 0.643841, acc: 64.06%] [G loss: 2.194934]\n",
      "epoch:16 step:15241 [D loss: 0.699058, acc: 57.81%] [G loss: 1.815218]\n",
      "epoch:16 step:15242 [D loss: 0.666201, acc: 57.03%] [G loss: 1.886069]\n",
      "epoch:16 step:15243 [D loss: 0.707480, acc: 48.44%] [G loss: 1.738720]\n",
      "epoch:16 step:15244 [D loss: 0.675045, acc: 55.47%] [G loss: 1.672614]\n",
      "epoch:16 step:15245 [D loss: 0.656012, acc: 61.72%] [G loss: 1.906451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15246 [D loss: 0.668289, acc: 58.59%] [G loss: 1.878418]\n",
      "epoch:16 step:15247 [D loss: 0.609294, acc: 68.75%] [G loss: 1.882054]\n",
      "epoch:16 step:15248 [D loss: 0.662066, acc: 60.16%] [G loss: 1.757325]\n",
      "epoch:16 step:15249 [D loss: 0.635014, acc: 63.28%] [G loss: 1.737491]\n",
      "epoch:16 step:15250 [D loss: 0.647423, acc: 61.72%] [G loss: 1.843749]\n",
      "epoch:16 step:15251 [D loss: 0.648595, acc: 62.50%] [G loss: 1.892519]\n",
      "epoch:16 step:15252 [D loss: 0.644475, acc: 61.72%] [G loss: 1.977826]\n",
      "epoch:16 step:15253 [D loss: 0.655242, acc: 59.38%] [G loss: 1.954709]\n",
      "epoch:16 step:15254 [D loss: 0.621598, acc: 65.62%] [G loss: 1.846647]\n",
      "epoch:16 step:15255 [D loss: 0.716664, acc: 57.81%] [G loss: 1.912047]\n",
      "epoch:16 step:15256 [D loss: 0.603958, acc: 65.62%] [G loss: 1.970026]\n",
      "epoch:16 step:15257 [D loss: 0.653384, acc: 57.81%] [G loss: 1.747973]\n",
      "epoch:16 step:15258 [D loss: 0.664256, acc: 65.62%] [G loss: 1.906147]\n",
      "epoch:16 step:15259 [D loss: 0.683737, acc: 54.69%] [G loss: 1.814998]\n",
      "epoch:16 step:15260 [D loss: 0.678313, acc: 59.38%] [G loss: 1.696239]\n",
      "epoch:16 step:15261 [D loss: 0.628060, acc: 62.50%] [G loss: 1.964057]\n",
      "epoch:16 step:15262 [D loss: 0.656947, acc: 64.84%] [G loss: 1.902067]\n",
      "epoch:16 step:15263 [D loss: 0.660650, acc: 60.94%] [G loss: 1.880114]\n",
      "epoch:16 step:15264 [D loss: 0.661145, acc: 67.19%] [G loss: 1.922494]\n",
      "epoch:16 step:15265 [D loss: 0.646955, acc: 60.16%] [G loss: 1.951184]\n",
      "epoch:16 step:15266 [D loss: 0.590526, acc: 69.53%] [G loss: 2.034835]\n",
      "epoch:16 step:15267 [D loss: 0.661105, acc: 60.16%] [G loss: 1.868270]\n",
      "epoch:16 step:15268 [D loss: 0.591973, acc: 69.53%] [G loss: 2.000279]\n",
      "epoch:16 step:15269 [D loss: 0.626503, acc: 62.50%] [G loss: 1.828491]\n",
      "epoch:16 step:15270 [D loss: 0.638042, acc: 63.28%] [G loss: 1.768657]\n",
      "epoch:16 step:15271 [D loss: 0.698165, acc: 54.69%] [G loss: 1.813607]\n",
      "epoch:16 step:15272 [D loss: 0.603308, acc: 67.97%] [G loss: 1.980527]\n",
      "epoch:16 step:15273 [D loss: 0.593304, acc: 72.66%] [G loss: 1.863502]\n",
      "epoch:16 step:15274 [D loss: 0.581247, acc: 68.75%] [G loss: 1.952129]\n",
      "epoch:16 step:15275 [D loss: 0.640697, acc: 62.50%] [G loss: 2.003893]\n",
      "epoch:16 step:15276 [D loss: 0.631593, acc: 63.28%] [G loss: 1.923872]\n",
      "epoch:16 step:15277 [D loss: 0.675958, acc: 60.16%] [G loss: 1.829225]\n",
      "epoch:16 step:15278 [D loss: 0.621970, acc: 60.94%] [G loss: 2.045060]\n",
      "epoch:16 step:15279 [D loss: 0.631797, acc: 65.62%] [G loss: 1.929325]\n",
      "epoch:16 step:15280 [D loss: 0.627594, acc: 63.28%] [G loss: 1.941353]\n",
      "epoch:16 step:15281 [D loss: 0.675633, acc: 57.81%] [G loss: 1.995680]\n",
      "epoch:16 step:15282 [D loss: 0.610083, acc: 61.72%] [G loss: 1.865987]\n",
      "epoch:16 step:15283 [D loss: 0.732355, acc: 51.56%] [G loss: 1.808531]\n",
      "epoch:16 step:15284 [D loss: 0.654274, acc: 65.62%] [G loss: 1.838277]\n",
      "epoch:16 step:15285 [D loss: 0.645886, acc: 59.38%] [G loss: 1.956137]\n",
      "epoch:16 step:15286 [D loss: 0.616397, acc: 64.84%] [G loss: 1.865575]\n",
      "epoch:16 step:15287 [D loss: 0.607934, acc: 61.72%] [G loss: 1.824396]\n",
      "epoch:16 step:15288 [D loss: 0.591073, acc: 67.19%] [G loss: 1.993784]\n",
      "epoch:16 step:15289 [D loss: 0.630359, acc: 64.06%] [G loss: 1.853367]\n",
      "epoch:16 step:15290 [D loss: 0.602497, acc: 65.62%] [G loss: 1.983899]\n",
      "epoch:16 step:15291 [D loss: 0.609885, acc: 65.62%] [G loss: 2.279108]\n",
      "epoch:16 step:15292 [D loss: 0.588078, acc: 71.88%] [G loss: 2.098295]\n",
      "epoch:16 step:15293 [D loss: 0.669362, acc: 57.03%] [G loss: 1.969723]\n",
      "epoch:16 step:15294 [D loss: 0.706559, acc: 53.91%] [G loss: 1.918172]\n",
      "epoch:16 step:15295 [D loss: 0.652254, acc: 58.59%] [G loss: 2.019226]\n",
      "epoch:16 step:15296 [D loss: 0.636908, acc: 62.50%] [G loss: 1.977983]\n",
      "epoch:16 step:15297 [D loss: 0.621791, acc: 64.06%] [G loss: 1.938496]\n",
      "epoch:16 step:15298 [D loss: 0.630248, acc: 64.84%] [G loss: 1.972130]\n",
      "epoch:16 step:15299 [D loss: 0.644614, acc: 64.06%] [G loss: 1.942063]\n",
      "epoch:16 step:15300 [D loss: 0.640896, acc: 61.72%] [G loss: 1.822080]\n",
      "epoch:16 step:15301 [D loss: 0.590592, acc: 67.97%] [G loss: 1.917519]\n",
      "epoch:16 step:15302 [D loss: 0.630062, acc: 62.50%] [G loss: 1.951618]\n",
      "epoch:16 step:15303 [D loss: 0.642092, acc: 64.06%] [G loss: 1.971698]\n",
      "epoch:16 step:15304 [D loss: 0.622224, acc: 64.84%] [G loss: 2.073357]\n",
      "epoch:16 step:15305 [D loss: 0.543676, acc: 75.00%] [G loss: 2.234088]\n",
      "epoch:16 step:15306 [D loss: 0.555883, acc: 76.56%] [G loss: 2.430003]\n",
      "epoch:16 step:15307 [D loss: 0.535278, acc: 76.56%] [G loss: 2.327853]\n",
      "epoch:16 step:15308 [D loss: 0.634399, acc: 64.06%] [G loss: 1.762842]\n",
      "epoch:16 step:15309 [D loss: 0.660590, acc: 61.72%] [G loss: 2.012828]\n",
      "epoch:16 step:15310 [D loss: 0.665785, acc: 60.94%] [G loss: 2.106841]\n",
      "epoch:16 step:15311 [D loss: 0.697250, acc: 59.38%] [G loss: 1.764567]\n",
      "epoch:16 step:15312 [D loss: 0.625412, acc: 64.06%] [G loss: 1.889132]\n",
      "epoch:16 step:15313 [D loss: 0.629065, acc: 64.84%] [G loss: 2.083779]\n",
      "epoch:16 step:15314 [D loss: 0.653108, acc: 63.28%] [G loss: 1.905607]\n",
      "epoch:16 step:15315 [D loss: 0.742558, acc: 53.12%] [G loss: 1.726970]\n",
      "epoch:16 step:15316 [D loss: 0.692847, acc: 54.69%] [G loss: 1.880540]\n",
      "epoch:16 step:15317 [D loss: 0.631082, acc: 69.53%] [G loss: 2.001196]\n",
      "epoch:16 step:15318 [D loss: 0.633963, acc: 64.84%] [G loss: 1.939785]\n",
      "epoch:16 step:15319 [D loss: 0.648059, acc: 63.28%] [G loss: 1.984216]\n",
      "epoch:16 step:15320 [D loss: 0.631919, acc: 64.06%] [G loss: 1.871233]\n",
      "epoch:16 step:15321 [D loss: 0.620298, acc: 63.28%] [G loss: 1.934813]\n",
      "epoch:16 step:15322 [D loss: 0.669797, acc: 63.28%] [G loss: 2.148278]\n",
      "epoch:16 step:15323 [D loss: 0.614082, acc: 64.84%] [G loss: 1.936021]\n",
      "epoch:16 step:15324 [D loss: 0.624852, acc: 66.41%] [G loss: 1.957944]\n",
      "epoch:16 step:15325 [D loss: 0.635133, acc: 64.06%] [G loss: 2.053697]\n",
      "epoch:16 step:15326 [D loss: 0.662915, acc: 64.06%] [G loss: 2.055178]\n",
      "epoch:16 step:15327 [D loss: 0.611061, acc: 60.94%] [G loss: 1.881441]\n",
      "epoch:16 step:15328 [D loss: 0.601900, acc: 65.62%] [G loss: 1.956845]\n",
      "epoch:16 step:15329 [D loss: 0.689390, acc: 57.03%] [G loss: 1.997193]\n",
      "epoch:16 step:15330 [D loss: 0.649581, acc: 63.28%] [G loss: 2.189081]\n",
      "epoch:16 step:15331 [D loss: 0.623503, acc: 65.62%] [G loss: 2.041606]\n",
      "epoch:16 step:15332 [D loss: 0.613076, acc: 69.53%] [G loss: 1.971017]\n",
      "epoch:16 step:15333 [D loss: 0.667162, acc: 60.16%] [G loss: 1.799351]\n",
      "epoch:16 step:15334 [D loss: 0.748188, acc: 48.44%] [G loss: 1.880184]\n",
      "epoch:16 step:15335 [D loss: 0.632799, acc: 60.94%] [G loss: 1.814241]\n",
      "epoch:16 step:15336 [D loss: 0.699477, acc: 57.03%] [G loss: 2.048331]\n",
      "epoch:16 step:15337 [D loss: 0.602504, acc: 68.75%] [G loss: 2.147678]\n",
      "epoch:16 step:15338 [D loss: 0.630002, acc: 61.72%] [G loss: 2.187847]\n",
      "epoch:16 step:15339 [D loss: 0.571807, acc: 72.66%] [G loss: 2.213303]\n",
      "epoch:16 step:15340 [D loss: 0.691017, acc: 61.72%] [G loss: 1.858041]\n",
      "epoch:16 step:15341 [D loss: 0.731499, acc: 51.56%] [G loss: 1.707422]\n",
      "epoch:16 step:15342 [D loss: 0.612867, acc: 70.31%] [G loss: 1.760045]\n",
      "epoch:16 step:15343 [D loss: 0.673726, acc: 57.03%] [G loss: 1.894417]\n",
      "epoch:16 step:15344 [D loss: 0.729909, acc: 52.34%] [G loss: 1.806962]\n",
      "epoch:16 step:15345 [D loss: 0.665036, acc: 55.47%] [G loss: 2.024844]\n",
      "epoch:16 step:15346 [D loss: 0.618364, acc: 64.84%] [G loss: 2.069029]\n",
      "epoch:16 step:15347 [D loss: 0.672716, acc: 57.03%] [G loss: 1.789625]\n",
      "epoch:16 step:15348 [D loss: 0.651561, acc: 64.06%] [G loss: 1.844862]\n",
      "epoch:16 step:15349 [D loss: 0.665019, acc: 56.25%] [G loss: 2.055859]\n",
      "epoch:16 step:15350 [D loss: 0.585097, acc: 67.97%] [G loss: 2.001444]\n",
      "epoch:16 step:15351 [D loss: 0.650544, acc: 62.50%] [G loss: 1.980806]\n",
      "epoch:16 step:15352 [D loss: 0.613756, acc: 66.41%] [G loss: 1.913741]\n",
      "epoch:16 step:15353 [D loss: 0.630988, acc: 65.62%] [G loss: 1.933165]\n",
      "epoch:16 step:15354 [D loss: 0.654565, acc: 65.62%] [G loss: 1.819868]\n",
      "epoch:16 step:15355 [D loss: 0.677778, acc: 60.94%] [G loss: 1.861161]\n",
      "epoch:16 step:15356 [D loss: 0.584764, acc: 70.31%] [G loss: 2.031634]\n",
      "epoch:16 step:15357 [D loss: 0.741697, acc: 51.56%] [G loss: 1.880385]\n",
      "epoch:16 step:15358 [D loss: 0.628601, acc: 64.84%] [G loss: 1.894189]\n",
      "epoch:16 step:15359 [D loss: 0.651704, acc: 62.50%] [G loss: 2.057346]\n",
      "epoch:16 step:15360 [D loss: 0.615620, acc: 65.62%] [G loss: 1.830619]\n",
      "epoch:16 step:15361 [D loss: 0.608598, acc: 70.31%] [G loss: 2.113522]\n",
      "epoch:16 step:15362 [D loss: 0.600702, acc: 71.09%] [G loss: 1.904597]\n",
      "epoch:16 step:15363 [D loss: 0.614243, acc: 64.06%] [G loss: 2.121438]\n",
      "epoch:16 step:15364 [D loss: 0.619924, acc: 64.84%] [G loss: 1.902762]\n",
      "epoch:16 step:15365 [D loss: 0.746043, acc: 50.78%] [G loss: 1.818868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15366 [D loss: 0.590875, acc: 68.75%] [G loss: 2.098468]\n",
      "epoch:16 step:15367 [D loss: 0.713907, acc: 57.81%] [G loss: 1.877609]\n",
      "epoch:16 step:15368 [D loss: 0.627791, acc: 63.28%] [G loss: 1.837456]\n",
      "epoch:16 step:15369 [D loss: 0.657175, acc: 58.59%] [G loss: 1.816160]\n",
      "epoch:16 step:15370 [D loss: 0.712505, acc: 52.34%] [G loss: 1.727404]\n",
      "epoch:16 step:15371 [D loss: 0.641267, acc: 65.62%] [G loss: 1.964070]\n",
      "epoch:16 step:15372 [D loss: 0.660248, acc: 62.50%] [G loss: 1.973884]\n",
      "epoch:16 step:15373 [D loss: 0.639491, acc: 64.84%] [G loss: 2.055695]\n",
      "epoch:16 step:15374 [D loss: 0.604028, acc: 64.06%] [G loss: 1.851320]\n",
      "epoch:16 step:15375 [D loss: 0.660720, acc: 62.50%] [G loss: 1.913925]\n",
      "epoch:16 step:15376 [D loss: 0.694847, acc: 54.69%] [G loss: 1.945293]\n",
      "epoch:16 step:15377 [D loss: 0.596817, acc: 67.97%] [G loss: 1.946887]\n",
      "epoch:16 step:15378 [D loss: 0.720313, acc: 52.34%] [G loss: 1.775674]\n",
      "epoch:16 step:15379 [D loss: 0.665428, acc: 59.38%] [G loss: 1.798500]\n",
      "epoch:16 step:15380 [D loss: 0.701294, acc: 56.25%] [G loss: 1.735250]\n",
      "epoch:16 step:15381 [D loss: 0.633820, acc: 66.41%] [G loss: 1.807281]\n",
      "epoch:16 step:15382 [D loss: 0.640270, acc: 62.50%] [G loss: 1.808840]\n",
      "epoch:16 step:15383 [D loss: 0.648930, acc: 60.94%] [G loss: 1.734552]\n",
      "epoch:16 step:15384 [D loss: 0.615803, acc: 67.97%] [G loss: 1.804329]\n",
      "epoch:16 step:15385 [D loss: 0.659758, acc: 64.06%] [G loss: 1.870268]\n",
      "epoch:16 step:15386 [D loss: 0.688394, acc: 56.25%] [G loss: 1.820900]\n",
      "epoch:16 step:15387 [D loss: 0.617311, acc: 65.62%] [G loss: 1.936691]\n",
      "epoch:16 step:15388 [D loss: 0.704023, acc: 54.69%] [G loss: 1.860762]\n",
      "epoch:16 step:15389 [D loss: 0.672420, acc: 56.25%] [G loss: 1.709936]\n",
      "epoch:16 step:15390 [D loss: 0.633349, acc: 62.50%] [G loss: 1.967716]\n",
      "epoch:16 step:15391 [D loss: 0.641341, acc: 60.16%] [G loss: 1.820541]\n",
      "epoch:16 step:15392 [D loss: 0.674097, acc: 55.47%] [G loss: 1.794121]\n",
      "epoch:16 step:15393 [D loss: 0.675482, acc: 55.47%] [G loss: 1.748611]\n",
      "epoch:16 step:15394 [D loss: 0.611769, acc: 68.75%] [G loss: 1.834456]\n",
      "epoch:16 step:15395 [D loss: 0.655285, acc: 57.03%] [G loss: 2.014992]\n",
      "epoch:16 step:15396 [D loss: 0.653229, acc: 64.84%] [G loss: 1.889453]\n",
      "epoch:16 step:15397 [D loss: 0.594237, acc: 69.53%] [G loss: 2.140485]\n",
      "epoch:16 step:15398 [D loss: 0.640821, acc: 64.84%] [G loss: 1.932745]\n",
      "epoch:16 step:15399 [D loss: 0.591389, acc: 67.97%] [G loss: 1.803342]\n",
      "epoch:16 step:15400 [D loss: 0.637306, acc: 63.28%] [G loss: 1.836087]\n",
      "epoch:16 step:15401 [D loss: 0.646768, acc: 63.28%] [G loss: 2.003232]\n",
      "epoch:16 step:15402 [D loss: 0.626152, acc: 66.41%] [G loss: 1.830976]\n",
      "epoch:16 step:15403 [D loss: 0.666609, acc: 58.59%] [G loss: 1.801815]\n",
      "epoch:16 step:15404 [D loss: 0.663298, acc: 61.72%] [G loss: 1.925600]\n",
      "epoch:16 step:15405 [D loss: 0.715586, acc: 51.56%] [G loss: 1.962920]\n",
      "epoch:16 step:15406 [D loss: 0.603158, acc: 66.41%] [G loss: 2.079144]\n",
      "epoch:16 step:15407 [D loss: 0.635500, acc: 67.19%] [G loss: 2.072307]\n",
      "epoch:16 step:15408 [D loss: 0.649176, acc: 65.62%] [G loss: 2.051490]\n",
      "epoch:16 step:15409 [D loss: 0.706999, acc: 56.25%] [G loss: 1.851920]\n",
      "epoch:16 step:15410 [D loss: 0.676345, acc: 57.03%] [G loss: 1.827907]\n",
      "epoch:16 step:15411 [D loss: 0.663728, acc: 60.16%] [G loss: 1.807845]\n",
      "epoch:16 step:15412 [D loss: 0.619731, acc: 66.41%] [G loss: 1.821212]\n",
      "epoch:16 step:15413 [D loss: 0.591244, acc: 72.66%] [G loss: 1.945089]\n",
      "epoch:16 step:15414 [D loss: 0.695910, acc: 56.25%] [G loss: 1.895008]\n",
      "epoch:16 step:15415 [D loss: 0.602022, acc: 68.75%] [G loss: 1.803748]\n",
      "epoch:16 step:15416 [D loss: 0.678222, acc: 56.25%] [G loss: 1.799292]\n",
      "epoch:16 step:15417 [D loss: 0.673309, acc: 59.38%] [G loss: 1.838006]\n",
      "epoch:16 step:15418 [D loss: 0.619596, acc: 66.41%] [G loss: 1.890220]\n",
      "epoch:16 step:15419 [D loss: 0.616347, acc: 66.41%] [G loss: 2.030916]\n",
      "epoch:16 step:15420 [D loss: 0.585391, acc: 66.41%] [G loss: 2.174184]\n",
      "epoch:16 step:15421 [D loss: 0.621801, acc: 67.19%] [G loss: 2.149951]\n",
      "epoch:16 step:15422 [D loss: 0.617588, acc: 62.50%] [G loss: 2.010160]\n",
      "epoch:16 step:15423 [D loss: 0.670903, acc: 62.50%] [G loss: 1.956644]\n",
      "epoch:16 step:15424 [D loss: 0.726494, acc: 55.47%] [G loss: 1.795075]\n",
      "epoch:16 step:15425 [D loss: 0.653797, acc: 60.16%] [G loss: 1.849277]\n",
      "epoch:16 step:15426 [D loss: 0.654435, acc: 62.50%] [G loss: 1.905748]\n",
      "epoch:16 step:15427 [D loss: 0.631941, acc: 62.50%] [G loss: 1.823372]\n",
      "epoch:16 step:15428 [D loss: 0.648245, acc: 60.94%] [G loss: 1.980069]\n",
      "epoch:16 step:15429 [D loss: 0.714233, acc: 52.34%] [G loss: 1.824228]\n",
      "epoch:16 step:15430 [D loss: 0.671281, acc: 56.25%] [G loss: 1.775063]\n",
      "epoch:16 step:15431 [D loss: 0.682013, acc: 60.94%] [G loss: 1.846077]\n",
      "epoch:16 step:15432 [D loss: 0.645285, acc: 61.72%] [G loss: 1.916287]\n",
      "epoch:16 step:15433 [D loss: 0.676359, acc: 54.69%] [G loss: 1.803074]\n",
      "epoch:16 step:15434 [D loss: 0.688718, acc: 59.38%] [G loss: 1.828424]\n",
      "epoch:16 step:15435 [D loss: 0.631228, acc: 64.84%] [G loss: 1.716575]\n",
      "epoch:16 step:15436 [D loss: 0.659812, acc: 60.94%] [G loss: 1.904909]\n",
      "epoch:16 step:15437 [D loss: 0.636540, acc: 63.28%] [G loss: 1.827987]\n",
      "epoch:16 step:15438 [D loss: 0.648924, acc: 67.19%] [G loss: 1.831012]\n",
      "epoch:16 step:15439 [D loss: 0.591477, acc: 71.09%] [G loss: 1.801231]\n",
      "epoch:16 step:15440 [D loss: 0.683922, acc: 53.91%] [G loss: 1.784626]\n",
      "epoch:16 step:15441 [D loss: 0.641900, acc: 62.50%] [G loss: 1.753537]\n",
      "epoch:16 step:15442 [D loss: 0.662233, acc: 64.84%] [G loss: 1.814767]\n",
      "epoch:16 step:15443 [D loss: 0.634641, acc: 59.38%] [G loss: 1.963742]\n",
      "epoch:16 step:15444 [D loss: 0.678378, acc: 60.16%] [G loss: 1.828821]\n",
      "epoch:16 step:15445 [D loss: 0.623441, acc: 65.62%] [G loss: 1.830923]\n",
      "epoch:16 step:15446 [D loss: 0.661911, acc: 60.94%] [G loss: 1.817690]\n",
      "epoch:16 step:15447 [D loss: 0.666336, acc: 59.38%] [G loss: 1.848323]\n",
      "epoch:16 step:15448 [D loss: 0.601133, acc: 66.41%] [G loss: 1.925481]\n",
      "epoch:16 step:15449 [D loss: 0.633626, acc: 64.84%] [G loss: 2.052264]\n",
      "epoch:16 step:15450 [D loss: 0.661984, acc: 62.50%] [G loss: 1.823546]\n",
      "epoch:16 step:15451 [D loss: 0.641631, acc: 62.50%] [G loss: 1.843406]\n",
      "epoch:16 step:15452 [D loss: 0.643165, acc: 59.38%] [G loss: 1.836147]\n",
      "epoch:16 step:15453 [D loss: 0.595688, acc: 66.41%] [G loss: 1.873031]\n",
      "epoch:16 step:15454 [D loss: 0.669151, acc: 59.38%] [G loss: 1.845067]\n",
      "epoch:16 step:15455 [D loss: 0.649180, acc: 58.59%] [G loss: 1.891821]\n",
      "epoch:16 step:15456 [D loss: 0.611407, acc: 69.53%] [G loss: 1.943117]\n",
      "epoch:16 step:15457 [D loss: 0.663758, acc: 58.59%] [G loss: 1.889548]\n",
      "epoch:16 step:15458 [D loss: 0.658736, acc: 61.72%] [G loss: 1.860015]\n",
      "epoch:16 step:15459 [D loss: 0.649507, acc: 66.41%] [G loss: 1.893172]\n",
      "epoch:16 step:15460 [D loss: 0.635960, acc: 63.28%] [G loss: 2.000750]\n",
      "epoch:16 step:15461 [D loss: 0.634251, acc: 64.06%] [G loss: 2.105952]\n",
      "epoch:16 step:15462 [D loss: 0.629641, acc: 64.06%] [G loss: 2.175143]\n",
      "epoch:16 step:15463 [D loss: 0.614837, acc: 66.41%] [G loss: 2.321714]\n",
      "epoch:16 step:15464 [D loss: 0.597768, acc: 67.97%] [G loss: 2.286365]\n",
      "epoch:16 step:15465 [D loss: 0.678670, acc: 59.38%] [G loss: 1.893495]\n",
      "epoch:16 step:15466 [D loss: 0.581658, acc: 69.53%] [G loss: 1.937068]\n",
      "epoch:16 step:15467 [D loss: 0.729785, acc: 53.91%] [G loss: 1.915258]\n",
      "epoch:16 step:15468 [D loss: 0.672808, acc: 60.94%] [G loss: 1.960497]\n",
      "epoch:16 step:15469 [D loss: 0.670259, acc: 60.16%] [G loss: 1.772658]\n",
      "epoch:16 step:15470 [D loss: 0.725338, acc: 54.69%] [G loss: 1.798412]\n",
      "epoch:16 step:15471 [D loss: 0.634549, acc: 64.84%] [G loss: 1.970511]\n",
      "epoch:16 step:15472 [D loss: 0.591260, acc: 66.41%] [G loss: 1.935506]\n",
      "epoch:16 step:15473 [D loss: 0.610404, acc: 71.09%] [G loss: 2.019568]\n",
      "epoch:16 step:15474 [D loss: 0.681878, acc: 63.28%] [G loss: 1.708491]\n",
      "epoch:16 step:15475 [D loss: 0.706895, acc: 56.25%] [G loss: 1.800363]\n",
      "epoch:16 step:15476 [D loss: 0.605624, acc: 63.28%] [G loss: 2.045328]\n",
      "epoch:16 step:15477 [D loss: 0.661985, acc: 54.69%] [G loss: 1.888220]\n",
      "epoch:16 step:15478 [D loss: 0.668270, acc: 56.25%] [G loss: 1.950829]\n",
      "epoch:16 step:15479 [D loss: 0.662669, acc: 65.62%] [G loss: 1.742519]\n",
      "epoch:16 step:15480 [D loss: 0.599890, acc: 67.97%] [G loss: 2.063496]\n",
      "epoch:16 step:15481 [D loss: 0.669646, acc: 59.38%] [G loss: 1.883815]\n",
      "epoch:16 step:15482 [D loss: 0.675962, acc: 60.16%] [G loss: 1.867284]\n",
      "epoch:16 step:15483 [D loss: 0.632035, acc: 60.94%] [G loss: 1.926623]\n",
      "epoch:16 step:15484 [D loss: 0.660229, acc: 57.81%] [G loss: 1.751025]\n",
      "epoch:16 step:15485 [D loss: 0.620943, acc: 66.41%] [G loss: 1.871398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15486 [D loss: 0.635572, acc: 63.28%] [G loss: 1.950905]\n",
      "epoch:16 step:15487 [D loss: 0.594006, acc: 64.84%] [G loss: 2.052301]\n",
      "epoch:16 step:15488 [D loss: 0.688908, acc: 57.03%] [G loss: 1.850059]\n",
      "epoch:16 step:15489 [D loss: 0.616620, acc: 63.28%] [G loss: 1.964381]\n",
      "epoch:16 step:15490 [D loss: 0.571808, acc: 75.78%] [G loss: 2.124935]\n",
      "epoch:16 step:15491 [D loss: 0.595763, acc: 72.66%] [G loss: 2.127774]\n",
      "epoch:16 step:15492 [D loss: 0.738965, acc: 54.69%] [G loss: 1.676068]\n",
      "epoch:16 step:15493 [D loss: 0.714699, acc: 60.16%] [G loss: 1.697145]\n",
      "epoch:16 step:15494 [D loss: 0.682604, acc: 54.69%] [G loss: 1.748906]\n",
      "epoch:16 step:15495 [D loss: 0.678922, acc: 55.47%] [G loss: 1.860554]\n",
      "epoch:16 step:15496 [D loss: 0.616023, acc: 64.84%] [G loss: 2.023077]\n",
      "epoch:16 step:15497 [D loss: 0.672812, acc: 54.69%] [G loss: 1.940552]\n",
      "epoch:16 step:15498 [D loss: 0.652230, acc: 63.28%] [G loss: 1.779603]\n",
      "epoch:16 step:15499 [D loss: 0.687008, acc: 60.16%] [G loss: 1.875239]\n",
      "epoch:16 step:15500 [D loss: 0.600848, acc: 64.06%] [G loss: 2.202847]\n",
      "epoch:16 step:15501 [D loss: 0.730923, acc: 57.03%] [G loss: 1.747055]\n",
      "epoch:16 step:15502 [D loss: 0.662444, acc: 57.81%] [G loss: 1.706138]\n",
      "epoch:16 step:15503 [D loss: 0.680361, acc: 54.69%] [G loss: 1.839005]\n",
      "epoch:16 step:15504 [D loss: 0.660947, acc: 53.12%] [G loss: 1.878104]\n",
      "epoch:16 step:15505 [D loss: 0.672441, acc: 60.94%] [G loss: 1.869052]\n",
      "epoch:16 step:15506 [D loss: 0.670656, acc: 57.03%] [G loss: 1.918249]\n",
      "epoch:16 step:15507 [D loss: 0.618572, acc: 63.28%] [G loss: 1.862460]\n",
      "epoch:16 step:15508 [D loss: 0.648839, acc: 63.28%] [G loss: 1.914075]\n",
      "epoch:16 step:15509 [D loss: 0.628110, acc: 64.06%] [G loss: 1.777656]\n",
      "epoch:16 step:15510 [D loss: 0.676722, acc: 60.16%] [G loss: 1.833866]\n",
      "epoch:16 step:15511 [D loss: 0.647482, acc: 60.16%] [G loss: 1.852284]\n",
      "epoch:16 step:15512 [D loss: 0.621235, acc: 64.84%] [G loss: 2.006800]\n",
      "epoch:16 step:15513 [D loss: 0.637607, acc: 60.94%] [G loss: 1.927467]\n",
      "epoch:16 step:15514 [D loss: 0.609319, acc: 72.66%] [G loss: 1.959467]\n",
      "epoch:16 step:15515 [D loss: 0.644196, acc: 63.28%] [G loss: 1.951474]\n",
      "epoch:16 step:15516 [D loss: 0.668690, acc: 64.06%] [G loss: 1.935413]\n",
      "epoch:16 step:15517 [D loss: 0.660397, acc: 60.94%] [G loss: 1.884736]\n",
      "epoch:16 step:15518 [D loss: 0.632094, acc: 61.72%] [G loss: 1.868975]\n",
      "epoch:16 step:15519 [D loss: 0.702648, acc: 53.91%] [G loss: 1.853578]\n",
      "epoch:16 step:15520 [D loss: 0.673959, acc: 56.25%] [G loss: 1.748002]\n",
      "epoch:16 step:15521 [D loss: 0.663142, acc: 57.03%] [G loss: 1.673705]\n",
      "epoch:16 step:15522 [D loss: 0.697874, acc: 57.81%] [G loss: 1.712930]\n",
      "epoch:16 step:15523 [D loss: 0.741232, acc: 55.47%] [G loss: 1.760203]\n",
      "epoch:16 step:15524 [D loss: 0.642759, acc: 64.06%] [G loss: 1.954804]\n",
      "epoch:16 step:15525 [D loss: 0.627233, acc: 66.41%] [G loss: 1.942730]\n",
      "epoch:16 step:15526 [D loss: 0.606062, acc: 68.75%] [G loss: 1.895609]\n",
      "epoch:16 step:15527 [D loss: 0.648253, acc: 60.16%] [G loss: 1.814205]\n",
      "epoch:16 step:15528 [D loss: 0.647541, acc: 67.19%] [G loss: 1.903398]\n",
      "epoch:16 step:15529 [D loss: 0.668435, acc: 60.16%] [G loss: 1.824161]\n",
      "epoch:16 step:15530 [D loss: 0.647679, acc: 61.72%] [G loss: 1.768271]\n",
      "epoch:16 step:15531 [D loss: 0.650071, acc: 65.62%] [G loss: 1.758789]\n",
      "epoch:16 step:15532 [D loss: 0.609253, acc: 68.75%] [G loss: 1.834224]\n",
      "epoch:16 step:15533 [D loss: 0.640108, acc: 63.28%] [G loss: 1.810065]\n",
      "epoch:16 step:15534 [D loss: 0.629112, acc: 63.28%] [G loss: 1.870380]\n",
      "epoch:16 step:15535 [D loss: 0.642616, acc: 60.94%] [G loss: 1.952238]\n",
      "epoch:16 step:15536 [D loss: 0.624078, acc: 67.19%] [G loss: 1.860154]\n",
      "epoch:16 step:15537 [D loss: 0.626686, acc: 60.94%] [G loss: 1.954689]\n",
      "epoch:16 step:15538 [D loss: 0.650182, acc: 63.28%] [G loss: 1.770479]\n",
      "epoch:16 step:15539 [D loss: 0.621501, acc: 66.41%] [G loss: 1.868145]\n",
      "epoch:16 step:15540 [D loss: 0.578036, acc: 67.19%] [G loss: 1.905879]\n",
      "epoch:16 step:15541 [D loss: 0.601292, acc: 74.22%] [G loss: 1.905362]\n",
      "epoch:16 step:15542 [D loss: 0.633671, acc: 67.97%] [G loss: 1.961093]\n",
      "epoch:16 step:15543 [D loss: 0.624781, acc: 63.28%] [G loss: 2.034939]\n",
      "epoch:16 step:15544 [D loss: 0.624762, acc: 71.88%] [G loss: 1.928669]\n",
      "epoch:16 step:15545 [D loss: 0.640436, acc: 55.47%] [G loss: 1.951284]\n",
      "epoch:16 step:15546 [D loss: 0.600493, acc: 69.53%] [G loss: 2.092835]\n",
      "epoch:16 step:15547 [D loss: 0.578569, acc: 74.22%] [G loss: 2.030097]\n",
      "epoch:16 step:15548 [D loss: 0.574639, acc: 68.75%] [G loss: 2.083483]\n",
      "epoch:16 step:15549 [D loss: 0.611643, acc: 66.41%] [G loss: 2.096131]\n",
      "epoch:16 step:15550 [D loss: 0.632778, acc: 62.50%] [G loss: 1.964282]\n",
      "epoch:16 step:15551 [D loss: 0.698352, acc: 53.91%] [G loss: 1.783098]\n",
      "epoch:16 step:15552 [D loss: 0.664515, acc: 57.81%] [G loss: 1.775256]\n",
      "epoch:16 step:15553 [D loss: 0.634893, acc: 64.06%] [G loss: 1.885566]\n",
      "epoch:16 step:15554 [D loss: 0.673885, acc: 58.59%] [G loss: 1.837732]\n",
      "epoch:16 step:15555 [D loss: 0.615059, acc: 60.16%] [G loss: 1.971447]\n",
      "epoch:16 step:15556 [D loss: 0.620445, acc: 64.84%] [G loss: 2.046338]\n",
      "epoch:16 step:15557 [D loss: 0.682571, acc: 57.03%] [G loss: 1.820384]\n",
      "epoch:16 step:15558 [D loss: 0.717029, acc: 48.44%] [G loss: 1.672241]\n",
      "epoch:16 step:15559 [D loss: 0.639149, acc: 64.06%] [G loss: 1.850463]\n",
      "epoch:16 step:15560 [D loss: 0.656590, acc: 61.72%] [G loss: 1.899828]\n",
      "epoch:16 step:15561 [D loss: 0.657965, acc: 63.28%] [G loss: 1.839055]\n",
      "epoch:16 step:15562 [D loss: 0.607615, acc: 61.72%] [G loss: 1.911893]\n",
      "epoch:16 step:15563 [D loss: 0.631809, acc: 61.72%] [G loss: 1.982943]\n",
      "epoch:16 step:15564 [D loss: 0.612452, acc: 67.97%] [G loss: 1.887581]\n",
      "epoch:16 step:15565 [D loss: 0.648315, acc: 57.03%] [G loss: 1.892542]\n",
      "epoch:16 step:15566 [D loss: 0.618385, acc: 68.75%] [G loss: 1.931889]\n",
      "epoch:16 step:15567 [D loss: 0.634315, acc: 66.41%] [G loss: 1.910841]\n",
      "epoch:16 step:15568 [D loss: 0.655168, acc: 62.50%] [G loss: 1.825900]\n",
      "epoch:16 step:15569 [D loss: 0.665380, acc: 64.06%] [G loss: 1.765168]\n",
      "epoch:16 step:15570 [D loss: 0.665229, acc: 60.16%] [G loss: 1.831975]\n",
      "epoch:16 step:15571 [D loss: 0.652796, acc: 61.72%] [G loss: 1.819715]\n",
      "epoch:16 step:15572 [D loss: 0.653587, acc: 63.28%] [G loss: 1.806021]\n",
      "epoch:16 step:15573 [D loss: 0.658110, acc: 59.38%] [G loss: 1.893851]\n",
      "epoch:16 step:15574 [D loss: 0.625670, acc: 66.41%] [G loss: 1.841422]\n",
      "epoch:16 step:15575 [D loss: 0.669783, acc: 62.50%] [G loss: 1.729459]\n",
      "epoch:16 step:15576 [D loss: 0.681487, acc: 57.81%] [G loss: 1.773763]\n",
      "epoch:16 step:15577 [D loss: 0.669502, acc: 59.38%] [G loss: 1.911984]\n",
      "epoch:16 step:15578 [D loss: 0.662410, acc: 60.94%] [G loss: 1.928368]\n",
      "epoch:16 step:15579 [D loss: 0.643775, acc: 61.72%] [G loss: 1.964839]\n",
      "epoch:16 step:15580 [D loss: 0.638696, acc: 67.97%] [G loss: 2.014080]\n",
      "epoch:16 step:15581 [D loss: 0.663792, acc: 57.03%] [G loss: 1.895339]\n",
      "epoch:16 step:15582 [D loss: 0.729182, acc: 53.91%] [G loss: 1.865376]\n",
      "epoch:16 step:15583 [D loss: 0.643012, acc: 57.81%] [G loss: 1.889410]\n",
      "epoch:16 step:15584 [D loss: 0.659888, acc: 64.84%] [G loss: 1.918815]\n",
      "epoch:16 step:15585 [D loss: 0.621300, acc: 64.84%] [G loss: 1.903558]\n",
      "epoch:16 step:15586 [D loss: 0.673508, acc: 59.38%] [G loss: 1.733632]\n",
      "epoch:16 step:15587 [D loss: 0.644988, acc: 60.16%] [G loss: 1.800767]\n",
      "epoch:16 step:15588 [D loss: 0.660221, acc: 61.72%] [G loss: 1.831053]\n",
      "epoch:16 step:15589 [D loss: 0.666302, acc: 60.16%] [G loss: 1.661721]\n",
      "epoch:16 step:15590 [D loss: 0.650117, acc: 61.72%] [G loss: 1.872936]\n",
      "epoch:16 step:15591 [D loss: 0.661583, acc: 64.84%] [G loss: 1.750549]\n",
      "epoch:16 step:15592 [D loss: 0.633536, acc: 63.28%] [G loss: 1.884357]\n",
      "epoch:16 step:15593 [D loss: 0.723086, acc: 53.12%] [G loss: 2.004451]\n",
      "epoch:16 step:15594 [D loss: 0.632350, acc: 58.59%] [G loss: 1.946891]\n",
      "epoch:16 step:15595 [D loss: 0.623040, acc: 62.50%] [G loss: 1.894717]\n",
      "epoch:16 step:15596 [D loss: 0.654900, acc: 63.28%] [G loss: 1.827801]\n",
      "epoch:16 step:15597 [D loss: 0.648509, acc: 65.62%] [G loss: 1.932246]\n",
      "epoch:16 step:15598 [D loss: 0.677828, acc: 52.34%] [G loss: 1.689301]\n",
      "epoch:16 step:15599 [D loss: 0.609387, acc: 65.62%] [G loss: 1.801377]\n",
      "epoch:16 step:15600 [D loss: 0.673301, acc: 62.50%] [G loss: 1.972490]\n",
      "epoch:16 step:15601 [D loss: 0.641833, acc: 64.84%] [G loss: 1.851216]\n",
      "epoch:16 step:15602 [D loss: 0.655772, acc: 58.59%] [G loss: 1.863331]\n",
      "epoch:16 step:15603 [D loss: 0.623355, acc: 64.06%] [G loss: 1.844402]\n",
      "epoch:16 step:15604 [D loss: 0.633441, acc: 65.62%] [G loss: 1.838757]\n",
      "epoch:16 step:15605 [D loss: 0.624436, acc: 67.97%] [G loss: 1.925713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15606 [D loss: 0.652973, acc: 61.72%] [G loss: 1.900485]\n",
      "epoch:16 step:15607 [D loss: 0.687127, acc: 59.38%] [G loss: 1.827986]\n",
      "epoch:16 step:15608 [D loss: 0.635307, acc: 65.62%] [G loss: 1.819905]\n",
      "epoch:16 step:15609 [D loss: 0.623407, acc: 67.97%] [G loss: 1.875285]\n",
      "epoch:16 step:15610 [D loss: 0.611950, acc: 66.41%] [G loss: 1.935299]\n",
      "epoch:16 step:15611 [D loss: 0.702154, acc: 57.81%] [G loss: 1.844506]\n",
      "epoch:16 step:15612 [D loss: 0.629882, acc: 64.06%] [G loss: 1.849770]\n",
      "epoch:16 step:15613 [D loss: 0.699838, acc: 52.34%] [G loss: 1.736230]\n",
      "epoch:16 step:15614 [D loss: 0.675259, acc: 59.38%] [G loss: 1.912907]\n",
      "epoch:16 step:15615 [D loss: 0.631676, acc: 67.97%] [G loss: 1.829315]\n",
      "epoch:16 step:15616 [D loss: 0.607176, acc: 72.66%] [G loss: 2.104890]\n",
      "epoch:16 step:15617 [D loss: 0.662404, acc: 61.72%] [G loss: 1.806252]\n",
      "epoch:16 step:15618 [D loss: 0.701195, acc: 57.81%] [G loss: 1.886656]\n",
      "epoch:16 step:15619 [D loss: 0.616594, acc: 64.84%] [G loss: 1.862194]\n",
      "epoch:16 step:15620 [D loss: 0.688766, acc: 59.38%] [G loss: 1.859902]\n",
      "epoch:16 step:15621 [D loss: 0.653780, acc: 65.62%] [G loss: 1.919371]\n",
      "epoch:16 step:15622 [D loss: 0.588438, acc: 71.88%] [G loss: 1.956059]\n",
      "epoch:16 step:15623 [D loss: 0.603841, acc: 64.84%] [G loss: 2.050072]\n",
      "epoch:16 step:15624 [D loss: 0.646047, acc: 57.81%] [G loss: 2.086911]\n",
      "epoch:16 step:15625 [D loss: 0.617742, acc: 59.38%] [G loss: 1.976477]\n",
      "epoch:16 step:15626 [D loss: 0.547570, acc: 74.22%] [G loss: 2.095159]\n",
      "epoch:16 step:15627 [D loss: 0.634886, acc: 69.53%] [G loss: 2.080381]\n",
      "epoch:16 step:15628 [D loss: 0.656851, acc: 60.16%] [G loss: 1.753324]\n",
      "epoch:16 step:15629 [D loss: 0.662733, acc: 60.94%] [G loss: 2.079303]\n",
      "epoch:16 step:15630 [D loss: 0.618340, acc: 63.28%] [G loss: 1.970205]\n",
      "epoch:16 step:15631 [D loss: 0.647066, acc: 61.72%] [G loss: 1.893543]\n",
      "epoch:16 step:15632 [D loss: 0.621869, acc: 64.84%] [G loss: 1.874232]\n",
      "epoch:16 step:15633 [D loss: 0.693934, acc: 57.03%] [G loss: 1.978697]\n",
      "epoch:16 step:15634 [D loss: 0.589610, acc: 71.88%] [G loss: 1.944602]\n",
      "epoch:16 step:15635 [D loss: 0.668539, acc: 56.25%] [G loss: 1.966493]\n",
      "epoch:16 step:15636 [D loss: 0.655625, acc: 59.38%] [G loss: 1.933200]\n",
      "epoch:16 step:15637 [D loss: 0.618292, acc: 65.62%] [G loss: 1.864091]\n",
      "epoch:16 step:15638 [D loss: 0.619820, acc: 64.06%] [G loss: 2.005221]\n",
      "epoch:16 step:15639 [D loss: 0.551837, acc: 72.66%] [G loss: 2.116078]\n",
      "epoch:16 step:15640 [D loss: 0.617689, acc: 67.19%] [G loss: 2.356171]\n",
      "epoch:16 step:15641 [D loss: 0.586931, acc: 71.88%] [G loss: 2.246218]\n",
      "epoch:16 step:15642 [D loss: 0.613409, acc: 67.19%] [G loss: 2.043623]\n",
      "epoch:16 step:15643 [D loss: 0.672246, acc: 58.59%] [G loss: 1.999114]\n",
      "epoch:16 step:15644 [D loss: 0.626329, acc: 65.62%] [G loss: 1.760039]\n",
      "epoch:16 step:15645 [D loss: 0.666034, acc: 64.06%] [G loss: 1.979739]\n",
      "epoch:16 step:15646 [D loss: 0.646029, acc: 60.16%] [G loss: 2.057437]\n",
      "epoch:16 step:15647 [D loss: 0.691998, acc: 62.50%] [G loss: 1.856476]\n",
      "epoch:16 step:15648 [D loss: 0.692114, acc: 57.81%] [G loss: 1.751446]\n",
      "epoch:16 step:15649 [D loss: 0.656085, acc: 64.84%] [G loss: 1.922309]\n",
      "epoch:16 step:15650 [D loss: 0.691220, acc: 57.81%] [G loss: 1.763631]\n",
      "epoch:16 step:15651 [D loss: 0.616428, acc: 69.53%] [G loss: 1.831125]\n",
      "epoch:16 step:15652 [D loss: 0.593396, acc: 67.19%] [G loss: 1.933705]\n",
      "epoch:16 step:15653 [D loss: 0.668213, acc: 67.19%] [G loss: 1.987219]\n",
      "epoch:16 step:15654 [D loss: 0.690563, acc: 59.38%] [G loss: 1.904732]\n",
      "epoch:16 step:15655 [D loss: 0.629292, acc: 66.41%] [G loss: 2.021026]\n",
      "epoch:16 step:15656 [D loss: 0.609378, acc: 64.06%] [G loss: 1.965109]\n",
      "epoch:16 step:15657 [D loss: 0.659724, acc: 62.50%] [G loss: 1.930481]\n",
      "epoch:16 step:15658 [D loss: 0.732252, acc: 53.12%] [G loss: 1.853095]\n",
      "epoch:16 step:15659 [D loss: 0.663189, acc: 56.25%] [G loss: 1.710373]\n",
      "epoch:16 step:15660 [D loss: 0.627343, acc: 67.97%] [G loss: 1.905045]\n",
      "epoch:16 step:15661 [D loss: 0.684117, acc: 57.81%] [G loss: 1.790378]\n",
      "epoch:16 step:15662 [D loss: 0.677951, acc: 58.59%] [G loss: 1.767728]\n",
      "epoch:16 step:15663 [D loss: 0.657530, acc: 60.16%] [G loss: 1.881129]\n",
      "epoch:16 step:15664 [D loss: 0.642647, acc: 62.50%] [G loss: 1.894808]\n",
      "epoch:16 step:15665 [D loss: 0.688488, acc: 61.72%] [G loss: 1.814413]\n",
      "epoch:16 step:15666 [D loss: 0.648309, acc: 63.28%] [G loss: 1.899073]\n",
      "epoch:16 step:15667 [D loss: 0.657427, acc: 56.25%] [G loss: 1.833055]\n",
      "epoch:16 step:15668 [D loss: 0.606860, acc: 67.97%] [G loss: 1.786761]\n",
      "epoch:16 step:15669 [D loss: 0.640434, acc: 67.97%] [G loss: 1.825373]\n",
      "epoch:16 step:15670 [D loss: 0.680808, acc: 57.81%] [G loss: 1.747372]\n",
      "epoch:16 step:15671 [D loss: 0.607011, acc: 68.75%] [G loss: 1.976450]\n",
      "epoch:16 step:15672 [D loss: 0.669438, acc: 60.16%] [G loss: 1.973055]\n",
      "epoch:16 step:15673 [D loss: 0.647853, acc: 63.28%] [G loss: 1.968798]\n",
      "epoch:16 step:15674 [D loss: 0.674093, acc: 55.47%] [G loss: 1.879486]\n",
      "epoch:16 step:15675 [D loss: 0.627310, acc: 64.84%] [G loss: 2.014031]\n",
      "epoch:16 step:15676 [D loss: 0.616795, acc: 64.84%] [G loss: 1.857868]\n",
      "epoch:16 step:15677 [D loss: 0.658491, acc: 60.16%] [G loss: 1.833393]\n",
      "epoch:16 step:15678 [D loss: 0.650760, acc: 65.62%] [G loss: 1.988173]\n",
      "epoch:16 step:15679 [D loss: 0.631338, acc: 65.62%] [G loss: 1.901430]\n",
      "epoch:16 step:15680 [D loss: 0.638062, acc: 67.19%] [G loss: 1.889219]\n",
      "epoch:16 step:15681 [D loss: 0.704982, acc: 59.38%] [G loss: 1.875240]\n",
      "epoch:16 step:15682 [D loss: 0.674129, acc: 53.91%] [G loss: 1.892623]\n",
      "epoch:16 step:15683 [D loss: 0.659854, acc: 59.38%] [G loss: 1.953124]\n",
      "epoch:16 step:15684 [D loss: 0.611628, acc: 70.31%] [G loss: 1.905531]\n",
      "epoch:16 step:15685 [D loss: 0.638194, acc: 63.28%] [G loss: 2.076770]\n",
      "epoch:16 step:15686 [D loss: 0.586267, acc: 70.31%] [G loss: 2.150161]\n",
      "epoch:16 step:15687 [D loss: 0.642853, acc: 62.50%] [G loss: 1.978107]\n",
      "epoch:16 step:15688 [D loss: 0.642837, acc: 65.62%] [G loss: 1.820540]\n",
      "epoch:16 step:15689 [D loss: 0.679337, acc: 56.25%] [G loss: 1.859451]\n",
      "epoch:16 step:15690 [D loss: 0.627684, acc: 67.97%] [G loss: 1.877868]\n",
      "epoch:16 step:15691 [D loss: 0.596762, acc: 67.97%] [G loss: 1.883647]\n",
      "epoch:16 step:15692 [D loss: 0.672969, acc: 57.03%] [G loss: 1.825723]\n",
      "epoch:16 step:15693 [D loss: 0.610921, acc: 67.19%] [G loss: 1.928387]\n",
      "epoch:16 step:15694 [D loss: 0.666619, acc: 57.81%] [G loss: 1.764683]\n",
      "epoch:16 step:15695 [D loss: 0.686931, acc: 53.12%] [G loss: 1.852783]\n",
      "epoch:16 step:15696 [D loss: 0.661189, acc: 60.16%] [G loss: 1.722240]\n",
      "epoch:16 step:15697 [D loss: 0.662038, acc: 60.16%] [G loss: 1.799783]\n",
      "epoch:16 step:15698 [D loss: 0.656824, acc: 64.06%] [G loss: 1.873745]\n",
      "epoch:16 step:15699 [D loss: 0.621349, acc: 68.75%] [G loss: 1.963444]\n",
      "epoch:16 step:15700 [D loss: 0.635408, acc: 70.31%] [G loss: 1.997568]\n",
      "epoch:16 step:15701 [D loss: 0.606462, acc: 67.97%] [G loss: 2.006351]\n",
      "epoch:16 step:15702 [D loss: 0.683987, acc: 55.47%] [G loss: 1.918483]\n",
      "epoch:16 step:15703 [D loss: 0.627713, acc: 62.50%] [G loss: 1.929431]\n",
      "epoch:16 step:15704 [D loss: 0.625007, acc: 61.72%] [G loss: 1.917922]\n",
      "epoch:16 step:15705 [D loss: 0.621273, acc: 67.97%] [G loss: 1.929781]\n",
      "epoch:16 step:15706 [D loss: 0.573526, acc: 68.75%] [G loss: 1.949546]\n",
      "epoch:16 step:15707 [D loss: 0.671903, acc: 57.81%] [G loss: 1.876211]\n",
      "epoch:16 step:15708 [D loss: 0.674641, acc: 56.25%] [G loss: 1.869965]\n",
      "epoch:16 step:15709 [D loss: 0.592974, acc: 69.53%] [G loss: 1.958749]\n",
      "epoch:16 step:15710 [D loss: 0.632455, acc: 64.06%] [G loss: 1.883984]\n",
      "epoch:16 step:15711 [D loss: 0.612825, acc: 61.72%] [G loss: 2.065636]\n",
      "epoch:16 step:15712 [D loss: 0.602236, acc: 69.53%] [G loss: 1.926663]\n",
      "epoch:16 step:15713 [D loss: 0.626934, acc: 67.97%] [G loss: 2.031635]\n",
      "epoch:16 step:15714 [D loss: 0.684984, acc: 57.03%] [G loss: 1.828635]\n",
      "epoch:16 step:15715 [D loss: 0.663983, acc: 58.59%] [G loss: 1.920569]\n",
      "epoch:16 step:15716 [D loss: 0.626450, acc: 67.19%] [G loss: 1.931501]\n",
      "epoch:16 step:15717 [D loss: 0.645558, acc: 64.06%] [G loss: 1.883086]\n",
      "epoch:16 step:15718 [D loss: 0.702704, acc: 63.28%] [G loss: 1.952386]\n",
      "epoch:16 step:15719 [D loss: 0.646574, acc: 66.41%] [G loss: 1.944371]\n",
      "epoch:16 step:15720 [D loss: 0.647992, acc: 65.62%] [G loss: 1.936528]\n",
      "epoch:16 step:15721 [D loss: 0.648849, acc: 62.50%] [G loss: 1.865212]\n",
      "epoch:16 step:15722 [D loss: 0.652037, acc: 55.47%] [G loss: 1.793556]\n",
      "epoch:16 step:15723 [D loss: 0.628766, acc: 63.28%] [G loss: 2.097000]\n",
      "epoch:16 step:15724 [D loss: 0.639161, acc: 57.03%] [G loss: 1.894853]\n",
      "epoch:16 step:15725 [D loss: 0.701845, acc: 57.81%] [G loss: 2.010134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15726 [D loss: 0.647299, acc: 61.72%] [G loss: 1.818291]\n",
      "epoch:16 step:15727 [D loss: 0.665816, acc: 59.38%] [G loss: 1.923315]\n",
      "epoch:16 step:15728 [D loss: 0.632809, acc: 65.62%] [G loss: 1.838279]\n",
      "epoch:16 step:15729 [D loss: 0.632155, acc: 63.28%] [G loss: 1.860275]\n",
      "epoch:16 step:15730 [D loss: 0.643124, acc: 60.16%] [G loss: 1.947767]\n",
      "epoch:16 step:15731 [D loss: 0.660198, acc: 54.69%] [G loss: 1.874861]\n",
      "epoch:16 step:15732 [D loss: 0.639373, acc: 64.84%] [G loss: 1.893980]\n",
      "epoch:16 step:15733 [D loss: 0.638460, acc: 64.06%] [G loss: 1.887652]\n",
      "epoch:16 step:15734 [D loss: 0.672386, acc: 57.03%] [G loss: 1.756401]\n",
      "epoch:16 step:15735 [D loss: 0.649668, acc: 64.06%] [G loss: 1.789190]\n",
      "epoch:16 step:15736 [D loss: 0.684254, acc: 60.94%] [G loss: 1.705529]\n",
      "epoch:16 step:15737 [D loss: 0.651494, acc: 60.16%] [G loss: 1.840519]\n",
      "epoch:16 step:15738 [D loss: 0.658104, acc: 62.50%] [G loss: 1.855037]\n",
      "epoch:16 step:15739 [D loss: 0.619810, acc: 67.97%] [G loss: 1.957810]\n",
      "epoch:16 step:15740 [D loss: 0.593709, acc: 64.84%] [G loss: 1.810185]\n",
      "epoch:16 step:15741 [D loss: 0.698098, acc: 57.81%] [G loss: 1.916893]\n",
      "epoch:16 step:15742 [D loss: 0.644475, acc: 62.50%] [G loss: 1.803598]\n",
      "epoch:16 step:15743 [D loss: 0.697482, acc: 54.69%] [G loss: 1.777253]\n",
      "epoch:16 step:15744 [D loss: 0.648933, acc: 60.16%] [G loss: 1.737728]\n",
      "epoch:16 step:15745 [D loss: 0.622481, acc: 67.97%] [G loss: 1.824848]\n",
      "epoch:16 step:15746 [D loss: 0.667496, acc: 60.94%] [G loss: 1.925932]\n",
      "epoch:16 step:15747 [D loss: 0.640688, acc: 64.06%] [G loss: 1.931412]\n",
      "epoch:16 step:15748 [D loss: 0.613185, acc: 66.41%] [G loss: 1.997127]\n",
      "epoch:16 step:15749 [D loss: 0.645915, acc: 65.62%] [G loss: 1.957912]\n",
      "epoch:16 step:15750 [D loss: 0.672401, acc: 62.50%] [G loss: 1.806452]\n",
      "epoch:16 step:15751 [D loss: 0.670752, acc: 63.28%] [G loss: 1.755386]\n",
      "epoch:16 step:15752 [D loss: 0.666067, acc: 59.38%] [G loss: 1.775223]\n",
      "epoch:16 step:15753 [D loss: 0.666708, acc: 60.16%] [G loss: 1.903382]\n",
      "epoch:16 step:15754 [D loss: 0.694869, acc: 58.59%] [G loss: 1.836325]\n",
      "epoch:16 step:15755 [D loss: 0.642104, acc: 67.19%] [G loss: 1.718832]\n",
      "epoch:16 step:15756 [D loss: 0.649760, acc: 60.16%] [G loss: 1.860172]\n",
      "epoch:16 step:15757 [D loss: 0.668183, acc: 57.03%] [G loss: 1.877659]\n",
      "epoch:16 step:15758 [D loss: 0.656026, acc: 60.94%] [G loss: 1.787936]\n",
      "epoch:16 step:15759 [D loss: 0.656277, acc: 64.06%] [G loss: 1.861339]\n",
      "epoch:16 step:15760 [D loss: 0.642721, acc: 66.41%] [G loss: 1.932812]\n",
      "epoch:16 step:15761 [D loss: 0.615780, acc: 68.75%] [G loss: 1.944095]\n",
      "epoch:16 step:15762 [D loss: 0.644366, acc: 64.06%] [G loss: 1.821978]\n",
      "epoch:16 step:15763 [D loss: 0.659734, acc: 65.62%] [G loss: 1.941764]\n",
      "epoch:16 step:15764 [D loss: 0.681505, acc: 59.38%] [G loss: 1.870856]\n",
      "epoch:16 step:15765 [D loss: 0.602873, acc: 63.28%] [G loss: 2.007407]\n",
      "epoch:16 step:15766 [D loss: 0.641667, acc: 65.62%] [G loss: 2.229239]\n",
      "epoch:16 step:15767 [D loss: 0.614867, acc: 68.75%] [G loss: 2.196456]\n",
      "epoch:16 step:15768 [D loss: 0.661668, acc: 58.59%] [G loss: 1.843699]\n",
      "epoch:16 step:15769 [D loss: 0.664457, acc: 58.59%] [G loss: 2.086893]\n",
      "epoch:16 step:15770 [D loss: 0.656926, acc: 63.28%] [G loss: 1.943171]\n",
      "epoch:16 step:15771 [D loss: 0.641087, acc: 64.84%] [G loss: 1.812511]\n",
      "epoch:16 step:15772 [D loss: 0.617889, acc: 64.84%] [G loss: 1.979861]\n",
      "epoch:16 step:15773 [D loss: 0.615790, acc: 67.19%] [G loss: 2.042928]\n",
      "epoch:16 step:15774 [D loss: 0.598222, acc: 67.97%] [G loss: 2.139873]\n",
      "epoch:16 step:15775 [D loss: 0.695250, acc: 54.69%] [G loss: 1.910997]\n",
      "epoch:16 step:15776 [D loss: 0.694318, acc: 58.59%] [G loss: 1.791190]\n",
      "epoch:16 step:15777 [D loss: 0.612217, acc: 67.19%] [G loss: 1.905473]\n",
      "epoch:16 step:15778 [D loss: 0.621683, acc: 64.06%] [G loss: 1.926055]\n",
      "epoch:16 step:15779 [D loss: 0.682896, acc: 58.59%] [G loss: 1.834032]\n",
      "epoch:16 step:15780 [D loss: 0.649913, acc: 63.28%] [G loss: 1.788491]\n",
      "epoch:16 step:15781 [D loss: 0.620693, acc: 67.97%] [G loss: 1.958674]\n",
      "epoch:16 step:15782 [D loss: 0.696238, acc: 59.38%] [G loss: 1.825604]\n",
      "epoch:16 step:15783 [D loss: 0.693253, acc: 53.91%] [G loss: 1.979952]\n",
      "epoch:16 step:15784 [D loss: 0.589610, acc: 68.75%] [G loss: 2.152746]\n",
      "epoch:16 step:15785 [D loss: 0.675996, acc: 57.03%] [G loss: 1.865112]\n",
      "epoch:16 step:15786 [D loss: 0.669149, acc: 60.16%] [G loss: 1.745251]\n",
      "epoch:16 step:15787 [D loss: 0.700430, acc: 51.56%] [G loss: 1.804537]\n",
      "epoch:16 step:15788 [D loss: 0.648457, acc: 63.28%] [G loss: 1.764404]\n",
      "epoch:16 step:15789 [D loss: 0.627634, acc: 64.84%] [G loss: 1.861134]\n",
      "epoch:16 step:15790 [D loss: 0.689207, acc: 57.81%] [G loss: 1.858934]\n",
      "epoch:16 step:15791 [D loss: 0.685357, acc: 56.25%] [G loss: 1.699150]\n",
      "epoch:16 step:15792 [D loss: 0.680107, acc: 57.03%] [G loss: 1.672996]\n",
      "epoch:16 step:15793 [D loss: 0.681001, acc: 57.03%] [G loss: 1.779095]\n",
      "epoch:16 step:15794 [D loss: 0.668993, acc: 59.38%] [G loss: 1.799264]\n",
      "epoch:16 step:15795 [D loss: 0.645824, acc: 62.50%] [G loss: 1.940985]\n",
      "epoch:16 step:15796 [D loss: 0.612529, acc: 66.41%] [G loss: 1.920410]\n",
      "epoch:16 step:15797 [D loss: 0.619720, acc: 64.06%] [G loss: 2.065603]\n",
      "epoch:16 step:15798 [D loss: 0.609150, acc: 67.97%] [G loss: 2.017395]\n",
      "epoch:16 step:15799 [D loss: 0.620530, acc: 64.06%] [G loss: 2.055016]\n",
      "epoch:16 step:15800 [D loss: 0.613483, acc: 64.06%] [G loss: 1.898687]\n",
      "epoch:16 step:15801 [D loss: 0.695486, acc: 63.28%] [G loss: 2.011505]\n",
      "epoch:16 step:15802 [D loss: 0.602092, acc: 73.44%] [G loss: 1.956667]\n",
      "epoch:16 step:15803 [D loss: 0.693858, acc: 53.12%] [G loss: 1.885646]\n",
      "epoch:16 step:15804 [D loss: 0.682009, acc: 50.00%] [G loss: 1.685669]\n",
      "epoch:16 step:15805 [D loss: 0.647811, acc: 60.94%] [G loss: 1.814126]\n",
      "epoch:16 step:15806 [D loss: 0.675556, acc: 57.81%] [G loss: 1.796543]\n",
      "epoch:16 step:15807 [D loss: 0.608015, acc: 70.31%] [G loss: 2.249371]\n",
      "epoch:16 step:15808 [D loss: 0.591068, acc: 68.75%] [G loss: 1.941484]\n",
      "epoch:16 step:15809 [D loss: 0.636377, acc: 65.62%] [G loss: 1.843878]\n",
      "epoch:16 step:15810 [D loss: 0.679417, acc: 59.38%] [G loss: 1.838504]\n",
      "epoch:16 step:15811 [D loss: 0.625376, acc: 68.75%] [G loss: 1.921326]\n",
      "epoch:16 step:15812 [D loss: 0.697965, acc: 53.91%] [G loss: 1.721617]\n",
      "epoch:16 step:15813 [D loss: 0.653816, acc: 60.16%] [G loss: 1.960754]\n",
      "epoch:16 step:15814 [D loss: 0.618012, acc: 69.53%] [G loss: 1.936393]\n",
      "epoch:16 step:15815 [D loss: 0.601332, acc: 64.84%] [G loss: 1.839080]\n",
      "epoch:16 step:15816 [D loss: 0.656423, acc: 59.38%] [G loss: 1.952172]\n",
      "epoch:16 step:15817 [D loss: 0.641176, acc: 60.94%] [G loss: 2.015316]\n",
      "epoch:16 step:15818 [D loss: 0.732726, acc: 54.69%] [G loss: 1.882928]\n",
      "epoch:16 step:15819 [D loss: 0.652318, acc: 60.94%] [G loss: 1.827812]\n",
      "epoch:16 step:15820 [D loss: 0.671093, acc: 57.03%] [G loss: 1.836514]\n",
      "epoch:16 step:15821 [D loss: 0.676516, acc: 56.25%] [G loss: 1.748621]\n",
      "epoch:16 step:15822 [D loss: 0.657266, acc: 62.50%] [G loss: 1.778142]\n",
      "epoch:16 step:15823 [D loss: 0.627567, acc: 66.41%] [G loss: 1.714811]\n",
      "epoch:16 step:15824 [D loss: 0.642516, acc: 65.62%] [G loss: 1.840932]\n",
      "epoch:16 step:15825 [D loss: 0.676107, acc: 64.84%] [G loss: 1.842854]\n",
      "epoch:16 step:15826 [D loss: 0.644633, acc: 60.94%] [G loss: 1.895930]\n",
      "epoch:16 step:15827 [D loss: 0.605904, acc: 69.53%] [G loss: 1.876498]\n",
      "epoch:16 step:15828 [D loss: 0.654707, acc: 62.50%] [G loss: 1.806933]\n",
      "epoch:16 step:15829 [D loss: 0.589444, acc: 71.09%] [G loss: 1.897929]\n",
      "epoch:16 step:15830 [D loss: 0.605324, acc: 72.66%] [G loss: 1.903476]\n",
      "epoch:16 step:15831 [D loss: 0.644970, acc: 62.50%] [G loss: 2.009634]\n",
      "epoch:16 step:15832 [D loss: 0.666044, acc: 58.59%] [G loss: 1.870307]\n",
      "epoch:16 step:15833 [D loss: 0.594446, acc: 67.97%] [G loss: 1.997677]\n",
      "epoch:16 step:15834 [D loss: 0.589405, acc: 71.09%] [G loss: 1.879155]\n",
      "epoch:16 step:15835 [D loss: 0.629387, acc: 67.19%] [G loss: 1.954851]\n",
      "epoch:16 step:15836 [D loss: 0.666348, acc: 57.03%] [G loss: 1.885422]\n",
      "epoch:16 step:15837 [D loss: 0.684421, acc: 53.91%] [G loss: 2.046473]\n",
      "epoch:16 step:15838 [D loss: 0.607919, acc: 64.06%] [G loss: 1.835703]\n",
      "epoch:16 step:15839 [D loss: 0.619543, acc: 67.97%] [G loss: 1.990360]\n",
      "epoch:16 step:15840 [D loss: 0.687589, acc: 61.72%] [G loss: 1.787920]\n",
      "epoch:16 step:15841 [D loss: 0.662969, acc: 57.03%] [G loss: 1.990349]\n",
      "epoch:16 step:15842 [D loss: 0.683433, acc: 57.81%] [G loss: 1.833246]\n",
      "epoch:16 step:15843 [D loss: 0.650270, acc: 64.06%] [G loss: 1.874177]\n",
      "epoch:16 step:15844 [D loss: 0.635816, acc: 66.41%] [G loss: 1.912597]\n",
      "epoch:16 step:15845 [D loss: 0.647625, acc: 60.16%] [G loss: 1.895535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:15846 [D loss: 0.620645, acc: 71.88%] [G loss: 1.922207]\n",
      "epoch:16 step:15847 [D loss: 0.661856, acc: 58.59%] [G loss: 1.847924]\n",
      "epoch:16 step:15848 [D loss: 0.660081, acc: 57.81%] [G loss: 1.756851]\n",
      "epoch:16 step:15849 [D loss: 0.650041, acc: 61.72%] [G loss: 1.975204]\n",
      "epoch:16 step:15850 [D loss: 0.718276, acc: 53.12%] [G loss: 1.718803]\n",
      "epoch:16 step:15851 [D loss: 0.683825, acc: 57.03%] [G loss: 1.747969]\n",
      "epoch:16 step:15852 [D loss: 0.609144, acc: 64.84%] [G loss: 2.006992]\n",
      "epoch:16 step:15853 [D loss: 0.656815, acc: 57.81%] [G loss: 1.794438]\n",
      "epoch:16 step:15854 [D loss: 0.633509, acc: 64.06%] [G loss: 1.819027]\n",
      "epoch:16 step:15855 [D loss: 0.652487, acc: 56.25%] [G loss: 1.763664]\n",
      "epoch:16 step:15856 [D loss: 0.640833, acc: 64.06%] [G loss: 1.863893]\n",
      "epoch:16 step:15857 [D loss: 0.629025, acc: 66.41%] [G loss: 1.769225]\n",
      "epoch:16 step:15858 [D loss: 0.600172, acc: 66.41%] [G loss: 1.893227]\n",
      "epoch:16 step:15859 [D loss: 0.682488, acc: 59.38%] [G loss: 1.785326]\n",
      "epoch:16 step:15860 [D loss: 0.628049, acc: 65.62%] [G loss: 1.772917]\n",
      "epoch:16 step:15861 [D loss: 0.607892, acc: 67.97%] [G loss: 1.894066]\n",
      "epoch:16 step:15862 [D loss: 0.634179, acc: 61.72%] [G loss: 1.766405]\n",
      "epoch:16 step:15863 [D loss: 0.587702, acc: 65.62%] [G loss: 1.930079]\n",
      "epoch:16 step:15864 [D loss: 0.704482, acc: 53.91%] [G loss: 1.772835]\n",
      "epoch:16 step:15865 [D loss: 0.679498, acc: 58.59%] [G loss: 1.818920]\n",
      "epoch:16 step:15866 [D loss: 0.625635, acc: 67.19%] [G loss: 1.786187]\n",
      "epoch:16 step:15867 [D loss: 0.632564, acc: 66.41%] [G loss: 2.005394]\n",
      "epoch:16 step:15868 [D loss: 0.638838, acc: 64.84%] [G loss: 1.884236]\n",
      "epoch:16 step:15869 [D loss: 0.735066, acc: 51.56%] [G loss: 1.756554]\n",
      "epoch:16 step:15870 [D loss: 0.648175, acc: 62.50%] [G loss: 1.814901]\n",
      "epoch:16 step:15871 [D loss: 0.661676, acc: 60.94%] [G loss: 1.715249]\n",
      "epoch:16 step:15872 [D loss: 0.614553, acc: 69.53%] [G loss: 1.872356]\n",
      "epoch:16 step:15873 [D loss: 0.653068, acc: 64.84%] [G loss: 1.789327]\n",
      "epoch:16 step:15874 [D loss: 0.591912, acc: 69.53%] [G loss: 1.951518]\n",
      "epoch:16 step:15875 [D loss: 0.668805, acc: 60.94%] [G loss: 1.962744]\n",
      "epoch:16 step:15876 [D loss: 0.636375, acc: 61.72%] [G loss: 2.150303]\n",
      "epoch:16 step:15877 [D loss: 0.671842, acc: 60.94%] [G loss: 1.879595]\n",
      "epoch:16 step:15878 [D loss: 0.678984, acc: 60.94%] [G loss: 1.861994]\n",
      "epoch:16 step:15879 [D loss: 0.637963, acc: 63.28%] [G loss: 1.865235]\n",
      "epoch:16 step:15880 [D loss: 0.601657, acc: 67.97%] [G loss: 1.953142]\n",
      "epoch:16 step:15881 [D loss: 0.654054, acc: 62.50%] [G loss: 1.980246]\n",
      "epoch:16 step:15882 [D loss: 0.622320, acc: 60.16%] [G loss: 2.000622]\n",
      "epoch:16 step:15883 [D loss: 0.671484, acc: 59.38%] [G loss: 1.869192]\n",
      "epoch:16 step:15884 [D loss: 0.721551, acc: 53.12%] [G loss: 1.861884]\n",
      "epoch:16 step:15885 [D loss: 0.645550, acc: 62.50%] [G loss: 1.872926]\n",
      "epoch:16 step:15886 [D loss: 0.598048, acc: 67.97%] [G loss: 1.988794]\n",
      "epoch:16 step:15887 [D loss: 0.655427, acc: 64.84%] [G loss: 1.818082]\n",
      "epoch:16 step:15888 [D loss: 0.665590, acc: 60.94%] [G loss: 1.865733]\n",
      "epoch:16 step:15889 [D loss: 0.650330, acc: 60.16%] [G loss: 2.015125]\n",
      "epoch:16 step:15890 [D loss: 0.604792, acc: 68.75%] [G loss: 1.971421]\n",
      "epoch:16 step:15891 [D loss: 0.601663, acc: 67.97%] [G loss: 2.363353]\n",
      "epoch:16 step:15892 [D loss: 0.598006, acc: 67.97%] [G loss: 1.957106]\n",
      "epoch:16 step:15893 [D loss: 0.621746, acc: 66.41%] [G loss: 1.971219]\n",
      "epoch:16 step:15894 [D loss: 0.669803, acc: 62.50%] [G loss: 1.852195]\n",
      "epoch:16 step:15895 [D loss: 0.680779, acc: 56.25%] [G loss: 1.801894]\n",
      "epoch:16 step:15896 [D loss: 0.683570, acc: 60.94%] [G loss: 1.977296]\n",
      "epoch:16 step:15897 [D loss: 0.689844, acc: 60.94%] [G loss: 1.866758]\n",
      "epoch:16 step:15898 [D loss: 0.638166, acc: 60.94%] [G loss: 1.924590]\n",
      "epoch:16 step:15899 [D loss: 0.620122, acc: 67.19%] [G loss: 1.928235]\n",
      "epoch:16 step:15900 [D loss: 0.618700, acc: 67.19%] [G loss: 1.905125]\n",
      "epoch:16 step:15901 [D loss: 0.630500, acc: 66.41%] [G loss: 1.944858]\n",
      "epoch:16 step:15902 [D loss: 0.585386, acc: 67.97%] [G loss: 1.985016]\n",
      "epoch:16 step:15903 [D loss: 0.623430, acc: 66.41%] [G loss: 2.030658]\n",
      "epoch:16 step:15904 [D loss: 0.623858, acc: 62.50%] [G loss: 2.160241]\n",
      "epoch:16 step:15905 [D loss: 0.704838, acc: 57.03%] [G loss: 2.027400]\n",
      "epoch:16 step:15906 [D loss: 0.667765, acc: 60.16%] [G loss: 2.136995]\n",
      "epoch:16 step:15907 [D loss: 0.645269, acc: 64.84%] [G loss: 2.019931]\n",
      "epoch:16 step:15908 [D loss: 0.666973, acc: 61.72%] [G loss: 1.968741]\n",
      "epoch:16 step:15909 [D loss: 0.649414, acc: 61.72%] [G loss: 1.840905]\n",
      "epoch:16 step:15910 [D loss: 0.573989, acc: 74.22%] [G loss: 2.123925]\n",
      "epoch:16 step:15911 [D loss: 0.582982, acc: 71.09%] [G loss: 2.303308]\n",
      "epoch:16 step:15912 [D loss: 0.727109, acc: 51.56%] [G loss: 1.727618]\n",
      "epoch:16 step:15913 [D loss: 0.607189, acc: 65.62%] [G loss: 2.026727]\n",
      "epoch:16 step:15914 [D loss: 0.629299, acc: 70.31%] [G loss: 1.986227]\n",
      "epoch:16 step:15915 [D loss: 0.587215, acc: 69.53%] [G loss: 2.033795]\n",
      "epoch:16 step:15916 [D loss: 0.625864, acc: 67.97%] [G loss: 1.947970]\n",
      "epoch:16 step:15917 [D loss: 0.577330, acc: 71.88%] [G loss: 2.198034]\n",
      "epoch:16 step:15918 [D loss: 0.597847, acc: 64.06%] [G loss: 2.178760]\n",
      "epoch:16 step:15919 [D loss: 0.588303, acc: 70.31%] [G loss: 2.095394]\n",
      "epoch:16 step:15920 [D loss: 0.775583, acc: 55.47%] [G loss: 1.664570]\n",
      "epoch:16 step:15921 [D loss: 0.759432, acc: 43.75%] [G loss: 1.828281]\n",
      "epoch:16 step:15922 [D loss: 0.618395, acc: 66.41%] [G loss: 2.071011]\n",
      "epoch:16 step:15923 [D loss: 0.637529, acc: 68.75%] [G loss: 2.128518]\n",
      "epoch:16 step:15924 [D loss: 0.627151, acc: 67.19%] [G loss: 1.937710]\n",
      "epoch:16 step:15925 [D loss: 0.661448, acc: 62.50%] [G loss: 1.889010]\n",
      "epoch:16 step:15926 [D loss: 0.600495, acc: 67.97%] [G loss: 2.056801]\n",
      "epoch:16 step:15927 [D loss: 0.596313, acc: 73.44%] [G loss: 2.077978]\n",
      "epoch:16 step:15928 [D loss: 0.551416, acc: 71.09%] [G loss: 2.277750]\n",
      "epoch:16 step:15929 [D loss: 0.576288, acc: 68.75%] [G loss: 2.758556]\n",
      "epoch:17 step:15930 [D loss: 0.638899, acc: 60.16%] [G loss: 1.988396]\n",
      "epoch:17 step:15931 [D loss: 0.736040, acc: 57.03%] [G loss: 2.025345]\n",
      "epoch:17 step:15932 [D loss: 0.661125, acc: 61.72%] [G loss: 1.899052]\n",
      "epoch:17 step:15933 [D loss: 0.651380, acc: 64.06%] [G loss: 1.803033]\n",
      "epoch:17 step:15934 [D loss: 0.650536, acc: 68.75%] [G loss: 1.856901]\n",
      "epoch:17 step:15935 [D loss: 0.660450, acc: 59.38%] [G loss: 1.882800]\n",
      "epoch:17 step:15936 [D loss: 0.638623, acc: 64.84%] [G loss: 1.848257]\n",
      "epoch:17 step:15937 [D loss: 0.643611, acc: 66.41%] [G loss: 2.008130]\n",
      "epoch:17 step:15938 [D loss: 0.605778, acc: 62.50%] [G loss: 1.915531]\n",
      "epoch:17 step:15939 [D loss: 0.619808, acc: 65.62%] [G loss: 1.987388]\n",
      "epoch:17 step:15940 [D loss: 0.617309, acc: 65.62%] [G loss: 2.005715]\n",
      "epoch:17 step:15941 [D loss: 0.647029, acc: 64.06%] [G loss: 1.857782]\n",
      "epoch:17 step:15942 [D loss: 0.637327, acc: 64.06%] [G loss: 2.042305]\n",
      "epoch:17 step:15943 [D loss: 0.635777, acc: 62.50%] [G loss: 1.897331]\n",
      "epoch:17 step:15944 [D loss: 0.636923, acc: 62.50%] [G loss: 2.168783]\n",
      "epoch:17 step:15945 [D loss: 0.600811, acc: 69.53%] [G loss: 2.134294]\n",
      "epoch:17 step:15946 [D loss: 0.621490, acc: 64.06%] [G loss: 1.823068]\n",
      "epoch:17 step:15947 [D loss: 0.646724, acc: 67.19%] [G loss: 1.914981]\n",
      "epoch:17 step:15948 [D loss: 0.717569, acc: 55.47%] [G loss: 1.812707]\n",
      "epoch:17 step:15949 [D loss: 0.740657, acc: 49.22%] [G loss: 1.543283]\n",
      "epoch:17 step:15950 [D loss: 0.663341, acc: 60.94%] [G loss: 1.932717]\n",
      "epoch:17 step:15951 [D loss: 0.703235, acc: 53.12%] [G loss: 1.800449]\n",
      "epoch:17 step:15952 [D loss: 0.626091, acc: 67.97%] [G loss: 2.059075]\n",
      "epoch:17 step:15953 [D loss: 0.635522, acc: 66.41%] [G loss: 1.911688]\n",
      "epoch:17 step:15954 [D loss: 0.617832, acc: 66.41%] [G loss: 1.918747]\n",
      "epoch:17 step:15955 [D loss: 0.638910, acc: 62.50%] [G loss: 1.741911]\n",
      "epoch:17 step:15956 [D loss: 0.682844, acc: 60.16%] [G loss: 1.656393]\n",
      "epoch:17 step:15957 [D loss: 0.652341, acc: 66.41%] [G loss: 1.842113]\n",
      "epoch:17 step:15958 [D loss: 0.641299, acc: 62.50%] [G loss: 1.984700]\n",
      "epoch:17 step:15959 [D loss: 0.648807, acc: 59.38%] [G loss: 1.855680]\n",
      "epoch:17 step:15960 [D loss: 0.660590, acc: 57.81%] [G loss: 1.758746]\n",
      "epoch:17 step:15961 [D loss: 0.632614, acc: 67.97%] [G loss: 1.791618]\n",
      "epoch:17 step:15962 [D loss: 0.601587, acc: 67.19%] [G loss: 1.839238]\n",
      "epoch:17 step:15963 [D loss: 0.705643, acc: 53.91%] [G loss: 1.756172]\n",
      "epoch:17 step:15964 [D loss: 0.645496, acc: 60.16%] [G loss: 1.781032]\n",
      "epoch:17 step:15965 [D loss: 0.612496, acc: 63.28%] [G loss: 1.889102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:15966 [D loss: 0.662485, acc: 60.16%] [G loss: 1.897552]\n",
      "epoch:17 step:15967 [D loss: 0.625529, acc: 68.75%] [G loss: 1.907761]\n",
      "epoch:17 step:15968 [D loss: 0.635593, acc: 66.41%] [G loss: 1.927560]\n",
      "epoch:17 step:15969 [D loss: 0.597640, acc: 74.22%] [G loss: 1.998512]\n",
      "epoch:17 step:15970 [D loss: 0.601087, acc: 72.66%] [G loss: 2.029231]\n",
      "epoch:17 step:15971 [D loss: 0.582400, acc: 67.19%] [G loss: 1.989121]\n",
      "epoch:17 step:15972 [D loss: 0.613695, acc: 64.06%] [G loss: 1.861572]\n",
      "epoch:17 step:15973 [D loss: 0.664647, acc: 60.16%] [G loss: 1.945479]\n",
      "epoch:17 step:15974 [D loss: 0.680549, acc: 54.69%] [G loss: 1.931416]\n",
      "epoch:17 step:15975 [D loss: 0.613779, acc: 63.28%] [G loss: 1.973096]\n",
      "epoch:17 step:15976 [D loss: 0.643939, acc: 63.28%] [G loss: 2.136649]\n",
      "epoch:17 step:15977 [D loss: 0.647544, acc: 64.84%] [G loss: 1.992074]\n",
      "epoch:17 step:15978 [D loss: 0.603472, acc: 70.31%] [G loss: 2.100062]\n",
      "epoch:17 step:15979 [D loss: 0.657431, acc: 58.59%] [G loss: 1.968191]\n",
      "epoch:17 step:15980 [D loss: 0.704486, acc: 53.12%] [G loss: 1.994554]\n",
      "epoch:17 step:15981 [D loss: 0.650214, acc: 64.84%] [G loss: 1.799497]\n",
      "epoch:17 step:15982 [D loss: 0.623548, acc: 64.84%] [G loss: 1.886885]\n",
      "epoch:17 step:15983 [D loss: 0.636322, acc: 67.97%] [G loss: 2.075369]\n",
      "epoch:17 step:15984 [D loss: 0.670860, acc: 63.28%] [G loss: 1.949741]\n",
      "epoch:17 step:15985 [D loss: 0.616524, acc: 66.41%] [G loss: 2.039307]\n",
      "epoch:17 step:15986 [D loss: 0.584955, acc: 71.88%] [G loss: 2.072482]\n",
      "epoch:17 step:15987 [D loss: 0.620903, acc: 69.53%] [G loss: 2.028651]\n",
      "epoch:17 step:15988 [D loss: 0.638880, acc: 61.72%] [G loss: 1.963475]\n",
      "epoch:17 step:15989 [D loss: 0.648563, acc: 62.50%] [G loss: 1.943821]\n",
      "epoch:17 step:15990 [D loss: 0.642511, acc: 64.06%] [G loss: 1.973713]\n",
      "epoch:17 step:15991 [D loss: 0.647027, acc: 64.06%] [G loss: 1.888830]\n",
      "epoch:17 step:15992 [D loss: 0.649578, acc: 59.38%] [G loss: 1.924523]\n",
      "epoch:17 step:15993 [D loss: 0.668655, acc: 57.81%] [G loss: 1.930859]\n",
      "epoch:17 step:15994 [D loss: 0.675502, acc: 57.81%] [G loss: 1.920126]\n",
      "epoch:17 step:15995 [D loss: 0.663030, acc: 59.38%] [G loss: 1.895079]\n",
      "epoch:17 step:15996 [D loss: 0.645277, acc: 62.50%] [G loss: 1.843987]\n",
      "epoch:17 step:15997 [D loss: 0.620266, acc: 67.97%] [G loss: 1.897693]\n",
      "epoch:17 step:15998 [D loss: 0.659715, acc: 59.38%] [G loss: 1.982183]\n",
      "epoch:17 step:15999 [D loss: 0.620766, acc: 65.62%] [G loss: 1.903390]\n",
      "epoch:17 step:16000 [D loss: 0.646183, acc: 59.38%] [G loss: 1.853902]\n",
      "epoch:17 step:16001 [D loss: 0.619550, acc: 67.97%] [G loss: 1.906577]\n",
      "epoch:17 step:16002 [D loss: 0.633956, acc: 64.06%] [G loss: 1.756584]\n",
      "epoch:17 step:16003 [D loss: 0.608089, acc: 60.94%] [G loss: 2.066352]\n",
      "epoch:17 step:16004 [D loss: 0.623865, acc: 60.94%] [G loss: 2.030763]\n",
      "epoch:17 step:16005 [D loss: 0.625277, acc: 63.28%] [G loss: 2.043111]\n",
      "epoch:17 step:16006 [D loss: 0.587559, acc: 67.19%] [G loss: 2.208820]\n",
      "epoch:17 step:16007 [D loss: 0.634749, acc: 63.28%] [G loss: 1.893778]\n",
      "epoch:17 step:16008 [D loss: 0.701597, acc: 54.69%] [G loss: 2.010921]\n",
      "epoch:17 step:16009 [D loss: 0.680875, acc: 56.25%] [G loss: 1.929008]\n",
      "epoch:17 step:16010 [D loss: 0.702058, acc: 51.56%] [G loss: 1.780321]\n",
      "epoch:17 step:16011 [D loss: 0.644012, acc: 66.41%] [G loss: 1.937515]\n",
      "epoch:17 step:16012 [D loss: 0.665741, acc: 58.59%] [G loss: 1.978850]\n",
      "epoch:17 step:16013 [D loss: 0.649210, acc: 68.75%] [G loss: 1.874624]\n",
      "epoch:17 step:16014 [D loss: 0.622727, acc: 71.09%] [G loss: 1.807602]\n",
      "epoch:17 step:16015 [D loss: 0.642746, acc: 60.94%] [G loss: 1.830241]\n",
      "epoch:17 step:16016 [D loss: 0.658143, acc: 63.28%] [G loss: 1.868052]\n",
      "epoch:17 step:16017 [D loss: 0.668522, acc: 65.62%] [G loss: 1.868371]\n",
      "epoch:17 step:16018 [D loss: 0.642338, acc: 61.72%] [G loss: 1.938621]\n",
      "epoch:17 step:16019 [D loss: 0.626102, acc: 65.62%] [G loss: 1.804597]\n",
      "epoch:17 step:16020 [D loss: 0.638650, acc: 67.19%] [G loss: 1.833152]\n",
      "epoch:17 step:16021 [D loss: 0.611064, acc: 68.75%] [G loss: 1.976272]\n",
      "epoch:17 step:16022 [D loss: 0.613248, acc: 67.19%] [G loss: 2.041264]\n",
      "epoch:17 step:16023 [D loss: 0.640792, acc: 66.41%] [G loss: 1.982660]\n",
      "epoch:17 step:16024 [D loss: 0.631023, acc: 66.41%] [G loss: 1.880253]\n",
      "epoch:17 step:16025 [D loss: 0.653561, acc: 60.16%] [G loss: 1.989782]\n",
      "epoch:17 step:16026 [D loss: 0.660104, acc: 53.91%] [G loss: 1.932172]\n",
      "epoch:17 step:16027 [D loss: 0.709140, acc: 55.47%] [G loss: 1.777084]\n",
      "epoch:17 step:16028 [D loss: 0.636065, acc: 61.72%] [G loss: 1.801263]\n",
      "epoch:17 step:16029 [D loss: 0.640463, acc: 61.72%] [G loss: 1.921033]\n",
      "epoch:17 step:16030 [D loss: 0.673061, acc: 62.50%] [G loss: 1.807542]\n",
      "epoch:17 step:16031 [D loss: 0.659453, acc: 58.59%] [G loss: 1.917365]\n",
      "epoch:17 step:16032 [D loss: 0.644476, acc: 61.72%] [G loss: 1.884217]\n",
      "epoch:17 step:16033 [D loss: 0.604396, acc: 65.62%] [G loss: 1.913055]\n",
      "epoch:17 step:16034 [D loss: 0.672484, acc: 54.69%] [G loss: 1.844641]\n",
      "epoch:17 step:16035 [D loss: 0.631125, acc: 65.62%] [G loss: 2.245262]\n",
      "epoch:17 step:16036 [D loss: 0.603245, acc: 69.53%] [G loss: 2.075730]\n",
      "epoch:17 step:16037 [D loss: 0.632049, acc: 67.97%] [G loss: 1.833189]\n",
      "epoch:17 step:16038 [D loss: 0.687787, acc: 60.16%] [G loss: 1.813558]\n",
      "epoch:17 step:16039 [D loss: 0.650809, acc: 59.38%] [G loss: 1.840079]\n",
      "epoch:17 step:16040 [D loss: 0.644532, acc: 64.06%] [G loss: 1.952600]\n",
      "epoch:17 step:16041 [D loss: 0.600210, acc: 66.41%] [G loss: 1.891252]\n",
      "epoch:17 step:16042 [D loss: 0.588913, acc: 72.66%] [G loss: 1.980968]\n",
      "epoch:17 step:16043 [D loss: 0.704414, acc: 52.34%] [G loss: 2.058923]\n",
      "epoch:17 step:16044 [D loss: 0.663676, acc: 61.72%] [G loss: 1.984295]\n",
      "epoch:17 step:16045 [D loss: 0.588205, acc: 71.09%] [G loss: 1.982885]\n",
      "epoch:17 step:16046 [D loss: 0.639688, acc: 67.19%] [G loss: 2.058991]\n",
      "epoch:17 step:16047 [D loss: 0.586828, acc: 72.66%] [G loss: 2.067322]\n",
      "epoch:17 step:16048 [D loss: 0.605454, acc: 68.75%] [G loss: 2.338290]\n",
      "epoch:17 step:16049 [D loss: 0.686307, acc: 62.50%] [G loss: 2.212011]\n",
      "epoch:17 step:16050 [D loss: 0.669975, acc: 57.03%] [G loss: 2.158623]\n",
      "epoch:17 step:16051 [D loss: 0.661185, acc: 60.16%] [G loss: 2.127985]\n",
      "epoch:17 step:16052 [D loss: 0.728893, acc: 50.78%] [G loss: 1.923747]\n",
      "epoch:17 step:16053 [D loss: 0.699422, acc: 57.81%] [G loss: 1.989581]\n",
      "epoch:17 step:16054 [D loss: 0.722107, acc: 51.56%] [G loss: 1.850048]\n",
      "epoch:17 step:16055 [D loss: 0.620780, acc: 64.84%] [G loss: 1.848837]\n",
      "epoch:17 step:16056 [D loss: 0.666883, acc: 61.72%] [G loss: 1.894995]\n",
      "epoch:17 step:16057 [D loss: 0.669949, acc: 54.69%] [G loss: 2.052977]\n",
      "epoch:17 step:16058 [D loss: 0.708491, acc: 50.78%] [G loss: 1.788244]\n",
      "epoch:17 step:16059 [D loss: 0.617117, acc: 66.41%] [G loss: 1.867417]\n",
      "epoch:17 step:16060 [D loss: 0.616569, acc: 65.62%] [G loss: 1.849018]\n",
      "epoch:17 step:16061 [D loss: 0.666748, acc: 57.03%] [G loss: 1.902533]\n",
      "epoch:17 step:16062 [D loss: 0.699772, acc: 53.12%] [G loss: 1.771659]\n",
      "epoch:17 step:16063 [D loss: 0.646205, acc: 61.72%] [G loss: 1.819560]\n",
      "epoch:17 step:16064 [D loss: 0.653157, acc: 64.84%] [G loss: 1.806445]\n",
      "epoch:17 step:16065 [D loss: 0.651343, acc: 62.50%] [G loss: 1.838533]\n",
      "epoch:17 step:16066 [D loss: 0.694096, acc: 55.47%] [G loss: 1.935780]\n",
      "epoch:17 step:16067 [D loss: 0.650846, acc: 57.81%] [G loss: 1.725658]\n",
      "epoch:17 step:16068 [D loss: 0.654427, acc: 63.28%] [G loss: 1.851958]\n",
      "epoch:17 step:16069 [D loss: 0.666808, acc: 57.03%] [G loss: 1.772462]\n",
      "epoch:17 step:16070 [D loss: 0.666941, acc: 60.94%] [G loss: 1.903280]\n",
      "epoch:17 step:16071 [D loss: 0.649837, acc: 61.72%] [G loss: 1.769108]\n",
      "epoch:17 step:16072 [D loss: 0.662106, acc: 59.38%] [G loss: 1.819768]\n",
      "epoch:17 step:16073 [D loss: 0.661882, acc: 61.72%] [G loss: 1.802467]\n",
      "epoch:17 step:16074 [D loss: 0.694259, acc: 57.03%] [G loss: 1.948735]\n",
      "epoch:17 step:16075 [D loss: 0.591425, acc: 71.88%] [G loss: 1.954279]\n",
      "epoch:17 step:16076 [D loss: 0.680271, acc: 60.16%] [G loss: 1.791953]\n",
      "epoch:17 step:16077 [D loss: 0.699296, acc: 55.47%] [G loss: 1.719088]\n",
      "epoch:17 step:16078 [D loss: 0.625357, acc: 63.28%] [G loss: 1.929518]\n",
      "epoch:17 step:16079 [D loss: 0.627199, acc: 68.75%] [G loss: 1.863676]\n",
      "epoch:17 step:16080 [D loss: 0.626622, acc: 61.72%] [G loss: 1.997258]\n",
      "epoch:17 step:16081 [D loss: 0.692694, acc: 60.16%] [G loss: 1.872402]\n",
      "epoch:17 step:16082 [D loss: 0.637727, acc: 62.50%] [G loss: 1.726658]\n",
      "epoch:17 step:16083 [D loss: 0.660287, acc: 66.41%] [G loss: 1.973137]\n",
      "epoch:17 step:16084 [D loss: 0.680544, acc: 57.81%] [G loss: 1.823144]\n",
      "epoch:17 step:16085 [D loss: 0.591394, acc: 71.88%] [G loss: 1.855523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16086 [D loss: 0.624291, acc: 64.84%] [G loss: 1.840473]\n",
      "epoch:17 step:16087 [D loss: 0.617495, acc: 65.62%] [G loss: 1.900745]\n",
      "epoch:17 step:16088 [D loss: 0.660336, acc: 63.28%] [G loss: 1.847955]\n",
      "epoch:17 step:16089 [D loss: 0.689201, acc: 60.16%] [G loss: 1.858075]\n",
      "epoch:17 step:16090 [D loss: 0.626282, acc: 66.41%] [G loss: 1.819408]\n",
      "epoch:17 step:16091 [D loss: 0.664086, acc: 57.81%] [G loss: 1.799928]\n",
      "epoch:17 step:16092 [D loss: 0.698132, acc: 53.12%] [G loss: 1.738877]\n",
      "epoch:17 step:16093 [D loss: 0.622084, acc: 67.97%] [G loss: 1.693222]\n",
      "epoch:17 step:16094 [D loss: 0.635133, acc: 62.50%] [G loss: 1.879473]\n",
      "epoch:17 step:16095 [D loss: 0.646064, acc: 62.50%] [G loss: 1.856007]\n",
      "epoch:17 step:16096 [D loss: 0.668454, acc: 59.38%] [G loss: 1.812501]\n",
      "epoch:17 step:16097 [D loss: 0.602581, acc: 70.31%] [G loss: 1.947413]\n",
      "epoch:17 step:16098 [D loss: 0.648076, acc: 63.28%] [G loss: 1.823014]\n",
      "epoch:17 step:16099 [D loss: 0.625717, acc: 64.84%] [G loss: 1.831936]\n",
      "epoch:17 step:16100 [D loss: 0.642063, acc: 61.72%] [G loss: 1.976408]\n",
      "epoch:17 step:16101 [D loss: 0.688162, acc: 58.59%] [G loss: 1.820793]\n",
      "epoch:17 step:16102 [D loss: 0.679643, acc: 60.16%] [G loss: 1.871945]\n",
      "epoch:17 step:16103 [D loss: 0.648053, acc: 58.59%] [G loss: 1.699754]\n",
      "epoch:17 step:16104 [D loss: 0.682083, acc: 57.81%] [G loss: 1.811475]\n",
      "epoch:17 step:16105 [D loss: 0.643507, acc: 62.50%] [G loss: 1.846500]\n",
      "epoch:17 step:16106 [D loss: 0.625080, acc: 64.06%] [G loss: 1.771072]\n",
      "epoch:17 step:16107 [D loss: 0.650938, acc: 60.16%] [G loss: 1.835863]\n",
      "epoch:17 step:16108 [D loss: 0.676932, acc: 57.81%] [G loss: 1.850555]\n",
      "epoch:17 step:16109 [D loss: 0.593413, acc: 69.53%] [G loss: 1.936714]\n",
      "epoch:17 step:16110 [D loss: 0.647424, acc: 59.38%] [G loss: 1.732959]\n",
      "epoch:17 step:16111 [D loss: 0.659403, acc: 64.06%] [G loss: 1.855090]\n",
      "epoch:17 step:16112 [D loss: 0.646262, acc: 66.41%] [G loss: 1.913391]\n",
      "epoch:17 step:16113 [D loss: 0.646515, acc: 64.06%] [G loss: 1.857613]\n",
      "epoch:17 step:16114 [D loss: 0.648400, acc: 60.94%] [G loss: 1.887138]\n",
      "epoch:17 step:16115 [D loss: 0.655469, acc: 55.47%] [G loss: 1.914619]\n",
      "epoch:17 step:16116 [D loss: 0.640480, acc: 63.28%] [G loss: 1.930229]\n",
      "epoch:17 step:16117 [D loss: 0.700272, acc: 56.25%] [G loss: 1.867630]\n",
      "epoch:17 step:16118 [D loss: 0.677566, acc: 62.50%] [G loss: 1.949363]\n",
      "epoch:17 step:16119 [D loss: 0.654356, acc: 62.50%] [G loss: 1.849589]\n",
      "epoch:17 step:16120 [D loss: 0.609647, acc: 68.75%] [G loss: 1.891922]\n",
      "epoch:17 step:16121 [D loss: 0.666049, acc: 64.06%] [G loss: 1.962044]\n",
      "epoch:17 step:16122 [D loss: 0.658405, acc: 68.75%] [G loss: 1.919881]\n",
      "epoch:17 step:16123 [D loss: 0.628291, acc: 60.94%] [G loss: 2.225982]\n",
      "epoch:17 step:16124 [D loss: 0.661768, acc: 60.16%] [G loss: 1.852998]\n",
      "epoch:17 step:16125 [D loss: 0.662387, acc: 57.03%] [G loss: 1.833620]\n",
      "epoch:17 step:16126 [D loss: 0.665271, acc: 63.28%] [G loss: 2.050173]\n",
      "epoch:17 step:16127 [D loss: 0.612514, acc: 65.62%] [G loss: 1.914543]\n",
      "epoch:17 step:16128 [D loss: 0.675909, acc: 63.28%] [G loss: 1.943104]\n",
      "epoch:17 step:16129 [D loss: 0.736526, acc: 53.91%] [G loss: 1.821895]\n",
      "epoch:17 step:16130 [D loss: 0.584563, acc: 68.75%] [G loss: 1.915653]\n",
      "epoch:17 step:16131 [D loss: 0.652288, acc: 64.06%] [G loss: 2.032257]\n",
      "epoch:17 step:16132 [D loss: 0.668812, acc: 57.03%] [G loss: 1.919446]\n",
      "epoch:17 step:16133 [D loss: 0.677665, acc: 59.38%] [G loss: 1.899535]\n",
      "epoch:17 step:16134 [D loss: 0.659272, acc: 59.38%] [G loss: 1.804734]\n",
      "epoch:17 step:16135 [D loss: 0.675689, acc: 60.16%] [G loss: 1.867678]\n",
      "epoch:17 step:16136 [D loss: 0.575840, acc: 71.88%] [G loss: 2.024549]\n",
      "epoch:17 step:16137 [D loss: 0.587527, acc: 67.97%] [G loss: 1.971228]\n",
      "epoch:17 step:16138 [D loss: 0.649574, acc: 67.97%] [G loss: 1.958714]\n",
      "epoch:17 step:16139 [D loss: 0.663497, acc: 59.38%] [G loss: 1.917980]\n",
      "epoch:17 step:16140 [D loss: 0.686642, acc: 56.25%] [G loss: 1.768686]\n",
      "epoch:17 step:16141 [D loss: 0.621865, acc: 64.06%] [G loss: 1.809582]\n",
      "epoch:17 step:16142 [D loss: 0.693425, acc: 58.59%] [G loss: 1.730410]\n",
      "epoch:17 step:16143 [D loss: 0.639046, acc: 69.53%] [G loss: 1.925227]\n",
      "epoch:17 step:16144 [D loss: 0.672923, acc: 56.25%] [G loss: 1.902107]\n",
      "epoch:17 step:16145 [D loss: 0.628237, acc: 66.41%] [G loss: 1.889684]\n",
      "epoch:17 step:16146 [D loss: 0.610046, acc: 67.19%] [G loss: 1.887173]\n",
      "epoch:17 step:16147 [D loss: 0.614426, acc: 64.84%] [G loss: 2.082242]\n",
      "epoch:17 step:16148 [D loss: 0.601439, acc: 73.44%] [G loss: 2.175003]\n",
      "epoch:17 step:16149 [D loss: 0.691122, acc: 55.47%] [G loss: 1.668077]\n",
      "epoch:17 step:16150 [D loss: 0.683671, acc: 53.12%] [G loss: 1.918809]\n",
      "epoch:17 step:16151 [D loss: 0.637938, acc: 63.28%] [G loss: 2.000009]\n",
      "epoch:17 step:16152 [D loss: 0.676073, acc: 60.16%] [G loss: 1.934121]\n",
      "epoch:17 step:16153 [D loss: 0.626332, acc: 63.28%] [G loss: 1.969383]\n",
      "epoch:17 step:16154 [D loss: 0.640557, acc: 66.41%] [G loss: 1.831675]\n",
      "epoch:17 step:16155 [D loss: 0.647938, acc: 58.59%] [G loss: 1.791505]\n",
      "epoch:17 step:16156 [D loss: 0.639342, acc: 64.06%] [G loss: 1.788567]\n",
      "epoch:17 step:16157 [D loss: 0.663533, acc: 60.94%] [G loss: 1.842778]\n",
      "epoch:17 step:16158 [D loss: 0.615820, acc: 64.84%] [G loss: 2.008172]\n",
      "epoch:17 step:16159 [D loss: 0.629766, acc: 60.16%] [G loss: 2.060141]\n",
      "epoch:17 step:16160 [D loss: 0.545819, acc: 70.31%] [G loss: 2.319593]\n",
      "epoch:17 step:16161 [D loss: 0.571687, acc: 71.88%] [G loss: 2.068936]\n",
      "epoch:17 step:16162 [D loss: 0.661227, acc: 62.50%] [G loss: 1.863356]\n",
      "epoch:17 step:16163 [D loss: 0.660762, acc: 63.28%] [G loss: 1.783254]\n",
      "epoch:17 step:16164 [D loss: 0.668755, acc: 53.91%] [G loss: 1.874964]\n",
      "epoch:17 step:16165 [D loss: 0.604790, acc: 63.28%] [G loss: 1.904049]\n",
      "epoch:17 step:16166 [D loss: 0.566081, acc: 72.66%] [G loss: 1.884567]\n",
      "epoch:17 step:16167 [D loss: 0.617405, acc: 67.97%] [G loss: 1.942588]\n",
      "epoch:17 step:16168 [D loss: 0.644171, acc: 66.41%] [G loss: 1.773070]\n",
      "epoch:17 step:16169 [D loss: 0.585406, acc: 66.41%] [G loss: 1.982168]\n",
      "epoch:17 step:16170 [D loss: 0.639331, acc: 67.19%] [G loss: 1.966828]\n",
      "epoch:17 step:16171 [D loss: 0.615409, acc: 69.53%] [G loss: 1.953733]\n",
      "epoch:17 step:16172 [D loss: 0.653305, acc: 61.72%] [G loss: 1.922400]\n",
      "epoch:17 step:16173 [D loss: 0.622090, acc: 64.84%] [G loss: 1.905531]\n",
      "epoch:17 step:16174 [D loss: 0.587832, acc: 71.09%] [G loss: 2.116851]\n",
      "epoch:17 step:16175 [D loss: 0.665569, acc: 60.94%] [G loss: 2.030477]\n",
      "epoch:17 step:16176 [D loss: 0.611969, acc: 61.72%] [G loss: 2.004657]\n",
      "epoch:17 step:16177 [D loss: 0.636780, acc: 60.94%] [G loss: 2.051369]\n",
      "epoch:17 step:16178 [D loss: 0.739478, acc: 53.12%] [G loss: 1.799821]\n",
      "epoch:17 step:16179 [D loss: 0.640898, acc: 61.72%] [G loss: 1.717727]\n",
      "epoch:17 step:16180 [D loss: 0.742741, acc: 53.91%] [G loss: 1.775872]\n",
      "epoch:17 step:16181 [D loss: 0.653216, acc: 57.03%] [G loss: 1.808507]\n",
      "epoch:17 step:16182 [D loss: 0.627239, acc: 67.97%] [G loss: 1.835754]\n",
      "epoch:17 step:16183 [D loss: 0.592783, acc: 68.75%] [G loss: 1.778573]\n",
      "epoch:17 step:16184 [D loss: 0.664655, acc: 61.72%] [G loss: 1.761024]\n",
      "epoch:17 step:16185 [D loss: 0.603245, acc: 66.41%] [G loss: 1.864066]\n",
      "epoch:17 step:16186 [D loss: 0.620729, acc: 64.84%] [G loss: 1.777368]\n",
      "epoch:17 step:16187 [D loss: 0.653472, acc: 62.50%] [G loss: 1.892324]\n",
      "epoch:17 step:16188 [D loss: 0.619573, acc: 66.41%] [G loss: 1.793479]\n",
      "epoch:17 step:16189 [D loss: 0.651819, acc: 57.03%] [G loss: 1.936107]\n",
      "epoch:17 step:16190 [D loss: 0.659511, acc: 61.72%] [G loss: 1.799625]\n",
      "epoch:17 step:16191 [D loss: 0.605333, acc: 69.53%] [G loss: 2.067324]\n",
      "epoch:17 step:16192 [D loss: 0.654101, acc: 57.81%] [G loss: 1.915686]\n",
      "epoch:17 step:16193 [D loss: 0.622528, acc: 60.16%] [G loss: 2.156208]\n",
      "epoch:17 step:16194 [D loss: 0.669379, acc: 58.59%] [G loss: 1.723601]\n",
      "epoch:17 step:16195 [D loss: 0.723367, acc: 57.03%] [G loss: 1.966624]\n",
      "epoch:17 step:16196 [D loss: 0.646965, acc: 56.25%] [G loss: 1.917032]\n",
      "epoch:17 step:16197 [D loss: 0.712408, acc: 61.72%] [G loss: 1.803295]\n",
      "epoch:17 step:16198 [D loss: 0.634976, acc: 63.28%] [G loss: 1.895051]\n",
      "epoch:17 step:16199 [D loss: 0.656360, acc: 62.50%] [G loss: 1.991504]\n",
      "epoch:17 step:16200 [D loss: 0.617510, acc: 66.41%] [G loss: 1.913970]\n",
      "epoch:17 step:16201 [D loss: 0.608207, acc: 64.06%] [G loss: 1.975485]\n",
      "epoch:17 step:16202 [D loss: 0.636708, acc: 70.31%] [G loss: 2.080539]\n",
      "epoch:17 step:16203 [D loss: 0.636336, acc: 66.41%] [G loss: 2.015962]\n",
      "epoch:17 step:16204 [D loss: 0.640391, acc: 68.75%] [G loss: 2.019085]\n",
      "epoch:17 step:16205 [D loss: 0.590273, acc: 71.09%] [G loss: 2.095389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16206 [D loss: 0.655996, acc: 60.16%] [G loss: 1.893985]\n",
      "epoch:17 step:16207 [D loss: 0.672582, acc: 57.81%] [G loss: 1.962296]\n",
      "epoch:17 step:16208 [D loss: 0.631037, acc: 61.72%] [G loss: 1.950049]\n",
      "epoch:17 step:16209 [D loss: 0.639283, acc: 64.06%] [G loss: 1.980545]\n",
      "epoch:17 step:16210 [D loss: 0.642919, acc: 59.38%] [G loss: 1.842825]\n",
      "epoch:17 step:16211 [D loss: 0.638188, acc: 61.72%] [G loss: 1.864224]\n",
      "epoch:17 step:16212 [D loss: 0.622246, acc: 67.97%] [G loss: 1.891165]\n",
      "epoch:17 step:16213 [D loss: 0.644577, acc: 61.72%] [G loss: 1.847380]\n",
      "epoch:17 step:16214 [D loss: 0.666162, acc: 63.28%] [G loss: 1.984770]\n",
      "epoch:17 step:16215 [D loss: 0.576256, acc: 71.09%] [G loss: 2.026244]\n",
      "epoch:17 step:16216 [D loss: 0.656695, acc: 58.59%] [G loss: 1.746903]\n",
      "epoch:17 step:16217 [D loss: 0.598018, acc: 71.09%] [G loss: 1.860995]\n",
      "epoch:17 step:16218 [D loss: 0.649296, acc: 59.38%] [G loss: 1.879438]\n",
      "epoch:17 step:16219 [D loss: 0.680842, acc: 60.94%] [G loss: 1.903322]\n",
      "epoch:17 step:16220 [D loss: 0.662572, acc: 57.03%] [G loss: 1.876132]\n",
      "epoch:17 step:16221 [D loss: 0.640684, acc: 64.06%] [G loss: 1.864069]\n",
      "epoch:17 step:16222 [D loss: 0.646828, acc: 60.94%] [G loss: 1.965858]\n",
      "epoch:17 step:16223 [D loss: 0.616877, acc: 65.62%] [G loss: 1.888337]\n",
      "epoch:17 step:16224 [D loss: 0.648882, acc: 61.72%] [G loss: 1.867238]\n",
      "epoch:17 step:16225 [D loss: 0.598157, acc: 67.97%] [G loss: 1.997788]\n",
      "epoch:17 step:16226 [D loss: 0.674122, acc: 56.25%] [G loss: 1.894406]\n",
      "epoch:17 step:16227 [D loss: 0.616779, acc: 65.62%] [G loss: 2.142894]\n",
      "epoch:17 step:16228 [D loss: 0.626047, acc: 57.81%] [G loss: 2.009952]\n",
      "epoch:17 step:16229 [D loss: 0.661502, acc: 60.94%] [G loss: 2.076231]\n",
      "epoch:17 step:16230 [D loss: 0.670418, acc: 59.38%] [G loss: 1.868662]\n",
      "epoch:17 step:16231 [D loss: 0.627761, acc: 62.50%] [G loss: 1.902614]\n",
      "epoch:17 step:16232 [D loss: 0.652561, acc: 58.59%] [G loss: 1.847054]\n",
      "epoch:17 step:16233 [D loss: 0.656037, acc: 59.38%] [G loss: 1.741972]\n",
      "epoch:17 step:16234 [D loss: 0.669529, acc: 57.81%] [G loss: 1.907259]\n",
      "epoch:17 step:16235 [D loss: 0.638938, acc: 63.28%] [G loss: 1.783272]\n",
      "epoch:17 step:16236 [D loss: 0.624013, acc: 64.84%] [G loss: 1.840373]\n",
      "epoch:17 step:16237 [D loss: 0.667384, acc: 63.28%] [G loss: 1.798932]\n",
      "epoch:17 step:16238 [D loss: 0.644590, acc: 61.72%] [G loss: 1.887227]\n",
      "epoch:17 step:16239 [D loss: 0.648665, acc: 61.72%] [G loss: 1.761040]\n",
      "epoch:17 step:16240 [D loss: 0.621744, acc: 64.84%] [G loss: 1.890446]\n",
      "epoch:17 step:16241 [D loss: 0.590380, acc: 66.41%] [G loss: 2.197650]\n",
      "epoch:17 step:16242 [D loss: 0.629221, acc: 62.50%] [G loss: 2.047974]\n",
      "epoch:17 step:16243 [D loss: 0.606968, acc: 69.53%] [G loss: 2.210130]\n",
      "epoch:17 step:16244 [D loss: 0.591724, acc: 67.19%] [G loss: 2.174905]\n",
      "epoch:17 step:16245 [D loss: 0.669011, acc: 60.94%] [G loss: 1.791687]\n",
      "epoch:17 step:16246 [D loss: 0.614982, acc: 64.06%] [G loss: 1.859587]\n",
      "epoch:17 step:16247 [D loss: 0.661365, acc: 61.72%] [G loss: 1.997648]\n",
      "epoch:17 step:16248 [D loss: 0.653446, acc: 64.06%] [G loss: 1.803029]\n",
      "epoch:17 step:16249 [D loss: 0.640376, acc: 58.59%] [G loss: 1.780117]\n",
      "epoch:17 step:16250 [D loss: 0.655376, acc: 60.94%] [G loss: 2.012431]\n",
      "epoch:17 step:16251 [D loss: 0.622538, acc: 67.19%] [G loss: 1.784986]\n",
      "epoch:17 step:16252 [D loss: 0.706859, acc: 60.16%] [G loss: 1.797674]\n",
      "epoch:17 step:16253 [D loss: 0.664269, acc: 61.72%] [G loss: 1.757787]\n",
      "epoch:17 step:16254 [D loss: 0.613720, acc: 69.53%] [G loss: 1.987750]\n",
      "epoch:17 step:16255 [D loss: 0.695479, acc: 56.25%] [G loss: 1.798495]\n",
      "epoch:17 step:16256 [D loss: 0.651391, acc: 62.50%] [G loss: 1.864323]\n",
      "epoch:17 step:16257 [D loss: 0.652091, acc: 59.38%] [G loss: 1.888130]\n",
      "epoch:17 step:16258 [D loss: 0.679861, acc: 60.16%] [G loss: 1.832282]\n",
      "epoch:17 step:16259 [D loss: 0.639324, acc: 59.38%] [G loss: 1.849455]\n",
      "epoch:17 step:16260 [D loss: 0.625115, acc: 64.84%] [G loss: 1.957843]\n",
      "epoch:17 step:16261 [D loss: 0.658310, acc: 57.81%] [G loss: 2.054625]\n",
      "epoch:17 step:16262 [D loss: 0.662692, acc: 58.59%] [G loss: 1.838809]\n",
      "epoch:17 step:16263 [D loss: 0.658860, acc: 59.38%] [G loss: 1.993446]\n",
      "epoch:17 step:16264 [D loss: 0.631530, acc: 64.06%] [G loss: 1.962877]\n",
      "epoch:17 step:16265 [D loss: 0.637068, acc: 62.50%] [G loss: 1.906605]\n",
      "epoch:17 step:16266 [D loss: 0.660821, acc: 60.16%] [G loss: 1.959614]\n",
      "epoch:17 step:16267 [D loss: 0.626592, acc: 64.84%] [G loss: 1.908473]\n",
      "epoch:17 step:16268 [D loss: 0.667557, acc: 62.50%] [G loss: 1.890757]\n",
      "epoch:17 step:16269 [D loss: 0.633767, acc: 69.53%] [G loss: 1.853935]\n",
      "epoch:17 step:16270 [D loss: 0.660860, acc: 59.38%] [G loss: 1.776694]\n",
      "epoch:17 step:16271 [D loss: 0.671931, acc: 57.81%] [G loss: 1.831545]\n",
      "epoch:17 step:16272 [D loss: 0.647505, acc: 64.06%] [G loss: 1.835747]\n",
      "epoch:17 step:16273 [D loss: 0.677721, acc: 56.25%] [G loss: 1.850042]\n",
      "epoch:17 step:16274 [D loss: 0.593033, acc: 66.41%] [G loss: 2.180175]\n",
      "epoch:17 step:16275 [D loss: 0.604149, acc: 68.75%] [G loss: 2.074993]\n",
      "epoch:17 step:16276 [D loss: 0.563386, acc: 71.09%] [G loss: 2.367888]\n",
      "epoch:17 step:16277 [D loss: 0.698007, acc: 53.91%] [G loss: 1.837682]\n",
      "epoch:17 step:16278 [D loss: 0.726215, acc: 50.00%] [G loss: 1.665591]\n",
      "epoch:17 step:16279 [D loss: 0.633435, acc: 65.62%] [G loss: 1.883368]\n",
      "epoch:17 step:16280 [D loss: 0.653423, acc: 61.72%] [G loss: 1.785658]\n",
      "epoch:17 step:16281 [D loss: 0.692930, acc: 53.12%] [G loss: 1.800789]\n",
      "epoch:17 step:16282 [D loss: 0.614315, acc: 69.53%] [G loss: 1.794773]\n",
      "epoch:17 step:16283 [D loss: 0.606996, acc: 66.41%] [G loss: 2.129220]\n",
      "epoch:17 step:16284 [D loss: 0.646424, acc: 60.16%] [G loss: 1.822404]\n",
      "epoch:17 step:16285 [D loss: 0.686868, acc: 55.47%] [G loss: 1.845434]\n",
      "epoch:17 step:16286 [D loss: 0.672129, acc: 62.50%] [G loss: 1.895653]\n",
      "epoch:17 step:16287 [D loss: 0.664817, acc: 60.94%] [G loss: 1.818656]\n",
      "epoch:17 step:16288 [D loss: 0.610261, acc: 70.31%] [G loss: 2.042767]\n",
      "epoch:17 step:16289 [D loss: 0.605362, acc: 67.19%] [G loss: 2.030473]\n",
      "epoch:17 step:16290 [D loss: 0.631829, acc: 65.62%] [G loss: 1.932848]\n",
      "epoch:17 step:16291 [D loss: 0.629921, acc: 66.41%] [G loss: 1.928183]\n",
      "epoch:17 step:16292 [D loss: 0.641970, acc: 64.06%] [G loss: 2.006447]\n",
      "epoch:17 step:16293 [D loss: 0.642033, acc: 64.06%] [G loss: 2.049546]\n",
      "epoch:17 step:16294 [D loss: 0.647636, acc: 61.72%] [G loss: 2.013849]\n",
      "epoch:17 step:16295 [D loss: 0.624209, acc: 65.62%] [G loss: 2.129877]\n",
      "epoch:17 step:16296 [D loss: 0.611313, acc: 70.31%] [G loss: 1.988596]\n",
      "epoch:17 step:16297 [D loss: 0.630725, acc: 64.06%] [G loss: 1.881788]\n",
      "epoch:17 step:16298 [D loss: 0.655553, acc: 62.50%] [G loss: 1.913809]\n",
      "epoch:17 step:16299 [D loss: 0.641190, acc: 68.75%] [G loss: 1.978738]\n",
      "epoch:17 step:16300 [D loss: 0.678945, acc: 60.94%] [G loss: 2.083965]\n",
      "epoch:17 step:16301 [D loss: 0.614609, acc: 61.72%] [G loss: 1.985029]\n",
      "epoch:17 step:16302 [D loss: 0.679070, acc: 58.59%] [G loss: 1.791545]\n",
      "epoch:17 step:16303 [D loss: 0.617236, acc: 65.62%] [G loss: 2.059134]\n",
      "epoch:17 step:16304 [D loss: 0.675372, acc: 56.25%] [G loss: 1.804740]\n",
      "epoch:17 step:16305 [D loss: 0.645892, acc: 59.38%] [G loss: 1.770778]\n",
      "epoch:17 step:16306 [D loss: 0.708691, acc: 54.69%] [G loss: 1.761080]\n",
      "epoch:17 step:16307 [D loss: 0.644175, acc: 62.50%] [G loss: 1.930296]\n",
      "epoch:17 step:16308 [D loss: 0.724021, acc: 54.69%] [G loss: 1.891546]\n",
      "epoch:17 step:16309 [D loss: 0.686438, acc: 58.59%] [G loss: 1.859170]\n",
      "epoch:17 step:16310 [D loss: 0.618652, acc: 69.53%] [G loss: 1.972567]\n",
      "epoch:17 step:16311 [D loss: 0.629272, acc: 70.31%] [G loss: 1.902384]\n",
      "epoch:17 step:16312 [D loss: 0.647735, acc: 60.94%] [G loss: 1.842031]\n",
      "epoch:17 step:16313 [D loss: 0.601971, acc: 64.06%] [G loss: 1.920189]\n",
      "epoch:17 step:16314 [D loss: 0.567728, acc: 70.31%] [G loss: 1.933066]\n",
      "epoch:17 step:16315 [D loss: 0.655025, acc: 63.28%] [G loss: 1.755088]\n",
      "epoch:17 step:16316 [D loss: 0.668651, acc: 54.69%] [G loss: 1.768605]\n",
      "epoch:17 step:16317 [D loss: 0.623432, acc: 65.62%] [G loss: 1.824394]\n",
      "epoch:17 step:16318 [D loss: 0.648320, acc: 63.28%] [G loss: 1.840890]\n",
      "epoch:17 step:16319 [D loss: 0.719248, acc: 55.47%] [G loss: 1.749946]\n",
      "epoch:17 step:16320 [D loss: 0.674844, acc: 58.59%] [G loss: 1.750215]\n",
      "epoch:17 step:16321 [D loss: 0.670865, acc: 65.62%] [G loss: 1.928648]\n",
      "epoch:17 step:16322 [D loss: 0.656819, acc: 56.25%] [G loss: 1.792399]\n",
      "epoch:17 step:16323 [D loss: 0.645424, acc: 62.50%] [G loss: 1.871046]\n",
      "epoch:17 step:16324 [D loss: 0.656871, acc: 60.16%] [G loss: 1.875203]\n",
      "epoch:17 step:16325 [D loss: 0.674794, acc: 51.56%] [G loss: 1.689350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16326 [D loss: 0.660325, acc: 59.38%] [G loss: 1.730770]\n",
      "epoch:17 step:16327 [D loss: 0.629600, acc: 67.19%] [G loss: 1.868621]\n",
      "epoch:17 step:16328 [D loss: 0.625088, acc: 64.84%] [G loss: 1.986304]\n",
      "epoch:17 step:16329 [D loss: 0.677736, acc: 56.25%] [G loss: 1.772321]\n",
      "epoch:17 step:16330 [D loss: 0.655830, acc: 59.38%] [G loss: 1.799604]\n",
      "epoch:17 step:16331 [D loss: 0.635517, acc: 62.50%] [G loss: 1.895272]\n",
      "epoch:17 step:16332 [D loss: 0.660731, acc: 60.94%] [G loss: 1.955839]\n",
      "epoch:17 step:16333 [D loss: 0.626702, acc: 64.06%] [G loss: 1.918347]\n",
      "epoch:17 step:16334 [D loss: 0.666572, acc: 58.59%] [G loss: 1.987342]\n",
      "epoch:17 step:16335 [D loss: 0.605218, acc: 65.62%] [G loss: 2.026136]\n",
      "epoch:17 step:16336 [D loss: 0.670827, acc: 61.72%] [G loss: 2.012805]\n",
      "epoch:17 step:16337 [D loss: 0.641807, acc: 62.50%] [G loss: 1.890775]\n",
      "epoch:17 step:16338 [D loss: 0.617317, acc: 65.62%] [G loss: 1.955274]\n",
      "epoch:17 step:16339 [D loss: 0.691294, acc: 55.47%] [G loss: 1.779867]\n",
      "epoch:17 step:16340 [D loss: 0.644456, acc: 63.28%] [G loss: 1.825397]\n",
      "epoch:17 step:16341 [D loss: 0.672423, acc: 64.84%] [G loss: 1.886110]\n",
      "epoch:17 step:16342 [D loss: 0.702060, acc: 56.25%] [G loss: 2.011731]\n",
      "epoch:17 step:16343 [D loss: 0.670427, acc: 54.69%] [G loss: 1.966881]\n",
      "epoch:17 step:16344 [D loss: 0.634830, acc: 61.72%] [G loss: 1.916159]\n",
      "epoch:17 step:16345 [D loss: 0.670634, acc: 60.16%] [G loss: 1.979542]\n",
      "epoch:17 step:16346 [D loss: 0.680113, acc: 63.28%] [G loss: 2.016960]\n",
      "epoch:17 step:16347 [D loss: 0.710200, acc: 54.69%] [G loss: 1.818435]\n",
      "epoch:17 step:16348 [D loss: 0.616758, acc: 66.41%] [G loss: 1.876752]\n",
      "epoch:17 step:16349 [D loss: 0.646152, acc: 62.50%] [G loss: 1.850238]\n",
      "epoch:17 step:16350 [D loss: 0.659520, acc: 61.72%] [G loss: 1.793408]\n",
      "epoch:17 step:16351 [D loss: 0.706281, acc: 53.91%] [G loss: 1.850990]\n",
      "epoch:17 step:16352 [D loss: 0.691799, acc: 56.25%] [G loss: 1.850784]\n",
      "epoch:17 step:16353 [D loss: 0.650219, acc: 60.94%] [G loss: 1.912072]\n",
      "epoch:17 step:16354 [D loss: 0.655462, acc: 66.41%] [G loss: 1.871613]\n",
      "epoch:17 step:16355 [D loss: 0.624051, acc: 69.53%] [G loss: 1.817083]\n",
      "epoch:17 step:16356 [D loss: 0.636890, acc: 66.41%] [G loss: 2.078350]\n",
      "epoch:17 step:16357 [D loss: 0.594634, acc: 71.09%] [G loss: 1.972097]\n",
      "epoch:17 step:16358 [D loss: 0.588698, acc: 64.84%] [G loss: 2.074616]\n",
      "epoch:17 step:16359 [D loss: 0.611283, acc: 67.19%] [G loss: 2.116289]\n",
      "epoch:17 step:16360 [D loss: 0.688431, acc: 60.16%] [G loss: 2.135718]\n",
      "epoch:17 step:16361 [D loss: 0.696495, acc: 57.81%] [G loss: 1.829682]\n",
      "epoch:17 step:16362 [D loss: 0.617006, acc: 64.84%] [G loss: 1.792231]\n",
      "epoch:17 step:16363 [D loss: 0.633477, acc: 62.50%] [G loss: 1.818119]\n",
      "epoch:17 step:16364 [D loss: 0.622924, acc: 66.41%] [G loss: 1.896536]\n",
      "epoch:17 step:16365 [D loss: 0.607957, acc: 65.62%] [G loss: 2.057351]\n",
      "epoch:17 step:16366 [D loss: 0.689272, acc: 57.03%] [G loss: 1.642174]\n",
      "epoch:17 step:16367 [D loss: 0.647654, acc: 64.06%] [G loss: 1.723545]\n",
      "epoch:17 step:16368 [D loss: 0.667194, acc: 58.59%] [G loss: 1.789171]\n",
      "epoch:17 step:16369 [D loss: 0.684740, acc: 53.12%] [G loss: 1.796365]\n",
      "epoch:17 step:16370 [D loss: 0.656478, acc: 59.38%] [G loss: 1.916352]\n",
      "epoch:17 step:16371 [D loss: 0.674202, acc: 60.94%] [G loss: 1.721979]\n",
      "epoch:17 step:16372 [D loss: 0.664476, acc: 60.94%] [G loss: 1.856551]\n",
      "epoch:17 step:16373 [D loss: 0.671147, acc: 63.28%] [G loss: 1.833863]\n",
      "epoch:17 step:16374 [D loss: 0.619641, acc: 63.28%] [G loss: 1.755897]\n",
      "epoch:17 step:16375 [D loss: 0.676509, acc: 65.62%] [G loss: 1.932206]\n",
      "epoch:17 step:16376 [D loss: 0.657674, acc: 59.38%] [G loss: 1.884479]\n",
      "epoch:17 step:16377 [D loss: 0.682738, acc: 54.69%] [G loss: 1.725308]\n",
      "epoch:17 step:16378 [D loss: 0.687024, acc: 55.47%] [G loss: 1.725440]\n",
      "epoch:17 step:16379 [D loss: 0.634872, acc: 60.16%] [G loss: 1.779833]\n",
      "epoch:17 step:16380 [D loss: 0.626590, acc: 67.19%] [G loss: 1.850254]\n",
      "epoch:17 step:16381 [D loss: 0.653005, acc: 60.16%] [G loss: 1.873235]\n",
      "epoch:17 step:16382 [D loss: 0.676315, acc: 64.06%] [G loss: 1.862288]\n",
      "epoch:17 step:16383 [D loss: 0.626586, acc: 66.41%] [G loss: 1.784380]\n",
      "epoch:17 step:16384 [D loss: 0.652418, acc: 63.28%] [G loss: 1.849430]\n",
      "epoch:17 step:16385 [D loss: 0.643158, acc: 63.28%] [G loss: 1.942126]\n",
      "epoch:17 step:16386 [D loss: 0.621155, acc: 59.38%] [G loss: 2.037503]\n",
      "epoch:17 step:16387 [D loss: 0.644620, acc: 57.81%] [G loss: 1.845189]\n",
      "epoch:17 step:16388 [D loss: 0.651247, acc: 62.50%] [G loss: 1.794688]\n",
      "epoch:17 step:16389 [D loss: 0.636057, acc: 63.28%] [G loss: 1.958899]\n",
      "epoch:17 step:16390 [D loss: 0.689840, acc: 60.16%] [G loss: 1.901804]\n",
      "epoch:17 step:16391 [D loss: 0.630482, acc: 63.28%] [G loss: 1.939270]\n",
      "epoch:17 step:16392 [D loss: 0.626832, acc: 65.62%] [G loss: 1.822398]\n",
      "epoch:17 step:16393 [D loss: 0.635905, acc: 64.06%] [G loss: 1.927392]\n",
      "epoch:17 step:16394 [D loss: 0.671667, acc: 63.28%] [G loss: 2.004645]\n",
      "epoch:17 step:16395 [D loss: 0.667264, acc: 60.94%] [G loss: 1.830769]\n",
      "epoch:17 step:16396 [D loss: 0.672511, acc: 59.38%] [G loss: 2.023544]\n",
      "epoch:17 step:16397 [D loss: 0.676303, acc: 57.03%] [G loss: 2.011279]\n",
      "epoch:17 step:16398 [D loss: 0.610556, acc: 73.44%] [G loss: 2.131244]\n",
      "epoch:17 step:16399 [D loss: 0.595883, acc: 65.62%] [G loss: 2.243357]\n",
      "epoch:17 step:16400 [D loss: 0.569869, acc: 68.75%] [G loss: 2.302452]\n",
      "epoch:17 step:16401 [D loss: 0.679456, acc: 59.38%] [G loss: 2.169785]\n",
      "epoch:17 step:16402 [D loss: 0.699877, acc: 53.12%] [G loss: 1.766449]\n",
      "epoch:17 step:16403 [D loss: 0.631931, acc: 64.06%] [G loss: 2.030755]\n",
      "epoch:17 step:16404 [D loss: 0.645885, acc: 64.06%] [G loss: 1.973382]\n",
      "epoch:17 step:16405 [D loss: 0.641921, acc: 59.38%] [G loss: 1.904535]\n",
      "epoch:17 step:16406 [D loss: 0.650282, acc: 61.72%] [G loss: 1.780157]\n",
      "epoch:17 step:16407 [D loss: 0.673197, acc: 62.50%] [G loss: 1.897095]\n",
      "epoch:17 step:16408 [D loss: 0.639122, acc: 60.16%] [G loss: 2.017031]\n",
      "epoch:17 step:16409 [D loss: 0.650689, acc: 60.16%] [G loss: 1.935838]\n",
      "epoch:17 step:16410 [D loss: 0.559954, acc: 74.22%] [G loss: 2.079392]\n",
      "epoch:17 step:16411 [D loss: 0.695425, acc: 54.69%] [G loss: 1.664289]\n",
      "epoch:17 step:16412 [D loss: 0.612401, acc: 67.97%] [G loss: 1.693089]\n",
      "epoch:17 step:16413 [D loss: 0.676888, acc: 56.25%] [G loss: 1.902084]\n",
      "epoch:17 step:16414 [D loss: 0.696307, acc: 55.47%] [G loss: 1.789225]\n",
      "epoch:17 step:16415 [D loss: 0.656336, acc: 58.59%] [G loss: 1.796128]\n",
      "epoch:17 step:16416 [D loss: 0.645501, acc: 60.94%] [G loss: 1.788701]\n",
      "epoch:17 step:16417 [D loss: 0.633474, acc: 62.50%] [G loss: 2.057291]\n",
      "epoch:17 step:16418 [D loss: 0.707270, acc: 56.25%] [G loss: 1.922691]\n",
      "epoch:17 step:16419 [D loss: 0.613630, acc: 67.19%] [G loss: 1.929627]\n",
      "epoch:17 step:16420 [D loss: 0.633909, acc: 64.06%] [G loss: 2.101772]\n",
      "epoch:17 step:16421 [D loss: 0.693893, acc: 58.59%] [G loss: 1.788765]\n",
      "epoch:17 step:16422 [D loss: 0.665749, acc: 62.50%] [G loss: 1.849176]\n",
      "epoch:17 step:16423 [D loss: 0.588265, acc: 71.88%] [G loss: 1.944462]\n",
      "epoch:17 step:16424 [D loss: 0.583267, acc: 67.97%] [G loss: 1.990908]\n",
      "epoch:17 step:16425 [D loss: 0.689062, acc: 57.81%] [G loss: 1.936064]\n",
      "epoch:17 step:16426 [D loss: 0.612217, acc: 65.62%] [G loss: 2.054806]\n",
      "epoch:17 step:16427 [D loss: 0.655575, acc: 65.62%] [G loss: 2.006837]\n",
      "epoch:17 step:16428 [D loss: 0.587851, acc: 70.31%] [G loss: 2.191022]\n",
      "epoch:17 step:16429 [D loss: 0.685515, acc: 56.25%] [G loss: 1.827848]\n",
      "epoch:17 step:16430 [D loss: 0.700884, acc: 55.47%] [G loss: 1.800721]\n",
      "epoch:17 step:16431 [D loss: 0.665929, acc: 64.84%] [G loss: 1.755832]\n",
      "epoch:17 step:16432 [D loss: 0.645757, acc: 60.94%] [G loss: 1.992859]\n",
      "epoch:17 step:16433 [D loss: 0.604346, acc: 62.50%] [G loss: 2.178365]\n",
      "epoch:17 step:16434 [D loss: 0.663208, acc: 60.94%] [G loss: 1.942702]\n",
      "epoch:17 step:16435 [D loss: 0.680580, acc: 57.03%] [G loss: 1.767740]\n",
      "epoch:17 step:16436 [D loss: 0.608049, acc: 66.41%] [G loss: 1.745389]\n",
      "epoch:17 step:16437 [D loss: 0.579485, acc: 71.88%] [G loss: 2.023007]\n",
      "epoch:17 step:16438 [D loss: 0.660358, acc: 60.16%] [G loss: 1.926444]\n",
      "epoch:17 step:16439 [D loss: 0.767097, acc: 51.56%] [G loss: 1.868861]\n",
      "epoch:17 step:16440 [D loss: 0.686004, acc: 54.69%] [G loss: 1.851503]\n",
      "epoch:17 step:16441 [D loss: 0.680002, acc: 56.25%] [G loss: 1.782174]\n",
      "epoch:17 step:16442 [D loss: 0.653400, acc: 64.06%] [G loss: 1.841505]\n",
      "epoch:17 step:16443 [D loss: 0.675310, acc: 57.03%] [G loss: 1.905129]\n",
      "epoch:17 step:16444 [D loss: 0.632580, acc: 64.84%] [G loss: 1.773958]\n",
      "epoch:17 step:16445 [D loss: 0.594453, acc: 72.66%] [G loss: 2.172773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16446 [D loss: 0.621696, acc: 64.06%] [G loss: 1.976416]\n",
      "epoch:17 step:16447 [D loss: 0.665023, acc: 57.81%] [G loss: 1.992200]\n",
      "epoch:17 step:16448 [D loss: 0.611261, acc: 70.31%] [G loss: 1.782321]\n",
      "epoch:17 step:16449 [D loss: 0.648971, acc: 59.38%] [G loss: 1.960137]\n",
      "epoch:17 step:16450 [D loss: 0.630051, acc: 64.06%] [G loss: 1.902626]\n",
      "epoch:17 step:16451 [D loss: 0.613904, acc: 62.50%] [G loss: 1.858830]\n",
      "epoch:17 step:16452 [D loss: 0.612597, acc: 66.41%] [G loss: 2.103912]\n",
      "epoch:17 step:16453 [D loss: 0.666644, acc: 60.16%] [G loss: 1.982406]\n",
      "epoch:17 step:16454 [D loss: 0.678582, acc: 60.16%] [G loss: 1.899441]\n",
      "epoch:17 step:16455 [D loss: 0.648577, acc: 59.38%] [G loss: 1.888162]\n",
      "epoch:17 step:16456 [D loss: 0.667260, acc: 57.81%] [G loss: 1.760486]\n",
      "epoch:17 step:16457 [D loss: 0.688611, acc: 54.69%] [G loss: 1.712596]\n",
      "epoch:17 step:16458 [D loss: 0.750268, acc: 47.66%] [G loss: 1.712073]\n",
      "epoch:17 step:16459 [D loss: 0.707931, acc: 46.88%] [G loss: 1.883155]\n",
      "epoch:17 step:16460 [D loss: 0.669364, acc: 60.94%] [G loss: 1.768793]\n",
      "epoch:17 step:16461 [D loss: 0.624218, acc: 67.19%] [G loss: 1.900502]\n",
      "epoch:17 step:16462 [D loss: 0.639134, acc: 63.28%] [G loss: 1.833186]\n",
      "epoch:17 step:16463 [D loss: 0.600046, acc: 70.31%] [G loss: 1.982927]\n",
      "epoch:17 step:16464 [D loss: 0.635960, acc: 67.97%] [G loss: 1.834599]\n",
      "epoch:17 step:16465 [D loss: 0.596255, acc: 70.31%] [G loss: 1.940756]\n",
      "epoch:17 step:16466 [D loss: 0.666493, acc: 60.16%] [G loss: 1.799712]\n",
      "epoch:17 step:16467 [D loss: 0.664418, acc: 57.81%] [G loss: 1.825016]\n",
      "epoch:17 step:16468 [D loss: 0.633928, acc: 66.41%] [G loss: 1.920762]\n",
      "epoch:17 step:16469 [D loss: 0.664091, acc: 58.59%] [G loss: 1.820165]\n",
      "epoch:17 step:16470 [D loss: 0.622083, acc: 68.75%] [G loss: 1.789475]\n",
      "epoch:17 step:16471 [D loss: 0.654082, acc: 59.38%] [G loss: 1.777236]\n",
      "epoch:17 step:16472 [D loss: 0.688462, acc: 54.69%] [G loss: 1.904963]\n",
      "epoch:17 step:16473 [D loss: 0.648990, acc: 65.62%] [G loss: 1.868179]\n",
      "epoch:17 step:16474 [D loss: 0.654648, acc: 60.94%] [G loss: 1.893149]\n",
      "epoch:17 step:16475 [D loss: 0.679377, acc: 59.38%] [G loss: 1.851467]\n",
      "epoch:17 step:16476 [D loss: 0.610864, acc: 61.72%] [G loss: 1.856625]\n",
      "epoch:17 step:16477 [D loss: 0.602659, acc: 67.97%] [G loss: 1.965255]\n",
      "epoch:17 step:16478 [D loss: 0.642357, acc: 62.50%] [G loss: 2.005502]\n",
      "epoch:17 step:16479 [D loss: 0.586116, acc: 71.09%] [G loss: 2.067220]\n",
      "epoch:17 step:16480 [D loss: 0.570053, acc: 71.88%] [G loss: 2.172646]\n",
      "epoch:17 step:16481 [D loss: 0.640421, acc: 67.97%] [G loss: 2.091662]\n",
      "epoch:17 step:16482 [D loss: 0.672189, acc: 57.81%] [G loss: 1.662638]\n",
      "epoch:17 step:16483 [D loss: 0.621203, acc: 68.75%] [G loss: 2.181255]\n",
      "epoch:17 step:16484 [D loss: 0.656786, acc: 64.06%] [G loss: 1.994208]\n",
      "epoch:17 step:16485 [D loss: 0.589194, acc: 75.00%] [G loss: 2.047412]\n",
      "epoch:17 step:16486 [D loss: 0.569623, acc: 71.09%] [G loss: 2.046964]\n",
      "epoch:17 step:16487 [D loss: 0.598337, acc: 66.41%] [G loss: 2.057385]\n",
      "epoch:17 step:16488 [D loss: 0.667235, acc: 60.94%] [G loss: 1.824916]\n",
      "epoch:17 step:16489 [D loss: 0.683323, acc: 53.12%] [G loss: 1.806357]\n",
      "epoch:17 step:16490 [D loss: 0.636024, acc: 67.97%] [G loss: 2.032115]\n",
      "epoch:17 step:16491 [D loss: 0.677329, acc: 60.94%] [G loss: 1.974563]\n",
      "epoch:17 step:16492 [D loss: 0.635964, acc: 64.06%] [G loss: 1.892728]\n",
      "epoch:17 step:16493 [D loss: 0.605845, acc: 61.72%] [G loss: 2.019675]\n",
      "epoch:17 step:16494 [D loss: 0.700264, acc: 48.44%] [G loss: 1.758474]\n",
      "epoch:17 step:16495 [D loss: 0.740382, acc: 45.31%] [G loss: 1.697634]\n",
      "epoch:17 step:16496 [D loss: 0.691209, acc: 57.81%] [G loss: 1.695616]\n",
      "epoch:17 step:16497 [D loss: 0.647774, acc: 58.59%] [G loss: 1.722459]\n",
      "epoch:17 step:16498 [D loss: 0.687460, acc: 56.25%] [G loss: 1.695986]\n",
      "epoch:17 step:16499 [D loss: 0.661999, acc: 55.47%] [G loss: 1.802017]\n",
      "epoch:17 step:16500 [D loss: 0.626857, acc: 59.38%] [G loss: 1.803426]\n",
      "epoch:17 step:16501 [D loss: 0.660403, acc: 54.69%] [G loss: 1.868653]\n",
      "epoch:17 step:16502 [D loss: 0.682545, acc: 60.94%] [G loss: 1.751545]\n",
      "epoch:17 step:16503 [D loss: 0.635113, acc: 63.28%] [G loss: 1.949709]\n",
      "epoch:17 step:16504 [D loss: 0.695569, acc: 55.47%] [G loss: 1.752455]\n",
      "epoch:17 step:16505 [D loss: 0.672510, acc: 57.03%] [G loss: 1.854971]\n",
      "epoch:17 step:16506 [D loss: 0.664311, acc: 59.38%] [G loss: 1.608194]\n",
      "epoch:17 step:16507 [D loss: 0.625750, acc: 69.53%] [G loss: 1.847942]\n",
      "epoch:17 step:16508 [D loss: 0.639295, acc: 64.06%] [G loss: 1.781626]\n",
      "epoch:17 step:16509 [D loss: 0.685343, acc: 60.16%] [G loss: 1.752635]\n",
      "epoch:17 step:16510 [D loss: 0.632568, acc: 64.84%] [G loss: 1.799371]\n",
      "epoch:17 step:16511 [D loss: 0.624062, acc: 67.97%] [G loss: 1.759992]\n",
      "epoch:17 step:16512 [D loss: 0.637504, acc: 62.50%] [G loss: 1.843336]\n",
      "epoch:17 step:16513 [D loss: 0.682609, acc: 57.03%] [G loss: 1.871840]\n",
      "epoch:17 step:16514 [D loss: 0.637943, acc: 61.72%] [G loss: 1.855553]\n",
      "epoch:17 step:16515 [D loss: 0.636932, acc: 63.28%] [G loss: 1.752425]\n",
      "epoch:17 step:16516 [D loss: 0.664644, acc: 57.81%] [G loss: 1.872356]\n",
      "epoch:17 step:16517 [D loss: 0.595333, acc: 67.19%] [G loss: 1.914936]\n",
      "epoch:17 step:16518 [D loss: 0.623076, acc: 64.84%] [G loss: 1.899514]\n",
      "epoch:17 step:16519 [D loss: 0.697568, acc: 59.38%] [G loss: 1.765108]\n",
      "epoch:17 step:16520 [D loss: 0.702200, acc: 55.47%] [G loss: 1.783609]\n",
      "epoch:17 step:16521 [D loss: 0.691012, acc: 59.38%] [G loss: 1.804597]\n",
      "epoch:17 step:16522 [D loss: 0.676596, acc: 59.38%] [G loss: 1.817273]\n",
      "epoch:17 step:16523 [D loss: 0.631172, acc: 60.94%] [G loss: 1.809536]\n",
      "epoch:17 step:16524 [D loss: 0.611546, acc: 67.97%] [G loss: 1.735295]\n",
      "epoch:17 step:16525 [D loss: 0.669446, acc: 62.50%] [G loss: 1.835716]\n",
      "epoch:17 step:16526 [D loss: 0.656049, acc: 58.59%] [G loss: 1.816854]\n",
      "epoch:17 step:16527 [D loss: 0.589069, acc: 69.53%] [G loss: 1.904621]\n",
      "epoch:17 step:16528 [D loss: 0.699768, acc: 56.25%] [G loss: 1.836659]\n",
      "epoch:17 step:16529 [D loss: 0.675884, acc: 60.94%] [G loss: 1.787341]\n",
      "epoch:17 step:16530 [D loss: 0.696928, acc: 58.59%] [G loss: 1.751850]\n",
      "epoch:17 step:16531 [D loss: 0.651655, acc: 62.50%] [G loss: 1.919776]\n",
      "epoch:17 step:16532 [D loss: 0.637591, acc: 67.19%] [G loss: 1.930794]\n",
      "epoch:17 step:16533 [D loss: 0.672379, acc: 56.25%] [G loss: 1.801531]\n",
      "epoch:17 step:16534 [D loss: 0.621005, acc: 66.41%] [G loss: 1.891989]\n",
      "epoch:17 step:16535 [D loss: 0.672446, acc: 58.59%] [G loss: 1.772600]\n",
      "epoch:17 step:16536 [D loss: 0.681079, acc: 54.69%] [G loss: 1.878029]\n",
      "epoch:17 step:16537 [D loss: 0.641782, acc: 65.62%] [G loss: 1.931034]\n",
      "epoch:17 step:16538 [D loss: 0.643086, acc: 61.72%] [G loss: 1.872873]\n",
      "epoch:17 step:16539 [D loss: 0.653180, acc: 63.28%] [G loss: 1.763421]\n",
      "epoch:17 step:16540 [D loss: 0.652188, acc: 64.06%] [G loss: 1.815827]\n",
      "epoch:17 step:16541 [D loss: 0.609533, acc: 67.19%] [G loss: 1.824281]\n",
      "epoch:17 step:16542 [D loss: 0.690829, acc: 64.06%] [G loss: 1.802809]\n",
      "epoch:17 step:16543 [D loss: 0.683473, acc: 53.91%] [G loss: 1.770998]\n",
      "epoch:17 step:16544 [D loss: 0.620426, acc: 65.62%] [G loss: 1.773263]\n",
      "epoch:17 step:16545 [D loss: 0.645146, acc: 66.41%] [G loss: 1.866998]\n",
      "epoch:17 step:16546 [D loss: 0.671363, acc: 62.50%] [G loss: 1.853617]\n",
      "epoch:17 step:16547 [D loss: 0.653513, acc: 60.16%] [G loss: 1.758059]\n",
      "epoch:17 step:16548 [D loss: 0.658679, acc: 63.28%] [G loss: 1.791430]\n",
      "epoch:17 step:16549 [D loss: 0.656389, acc: 58.59%] [G loss: 1.901234]\n",
      "epoch:17 step:16550 [D loss: 0.607280, acc: 66.41%] [G loss: 1.786321]\n",
      "epoch:17 step:16551 [D loss: 0.666669, acc: 70.31%] [G loss: 1.928248]\n",
      "epoch:17 step:16552 [D loss: 0.654472, acc: 55.47%] [G loss: 1.861365]\n",
      "epoch:17 step:16553 [D loss: 0.602754, acc: 67.97%] [G loss: 2.123082]\n",
      "epoch:17 step:16554 [D loss: 0.667070, acc: 60.16%] [G loss: 1.797552]\n",
      "epoch:17 step:16555 [D loss: 0.679493, acc: 53.12%] [G loss: 1.877926]\n",
      "epoch:17 step:16556 [D loss: 0.585815, acc: 68.75%] [G loss: 1.843253]\n",
      "epoch:17 step:16557 [D loss: 0.613230, acc: 64.84%] [G loss: 1.858909]\n",
      "epoch:17 step:16558 [D loss: 0.658177, acc: 63.28%] [G loss: 1.946716]\n",
      "epoch:17 step:16559 [D loss: 0.633667, acc: 66.41%] [G loss: 1.944362]\n",
      "epoch:17 step:16560 [D loss: 0.663148, acc: 61.72%] [G loss: 1.907445]\n",
      "epoch:17 step:16561 [D loss: 0.640323, acc: 61.72%] [G loss: 1.921203]\n",
      "epoch:17 step:16562 [D loss: 0.666553, acc: 64.06%] [G loss: 1.911880]\n",
      "epoch:17 step:16563 [D loss: 0.567591, acc: 71.88%] [G loss: 1.946601]\n",
      "epoch:17 step:16564 [D loss: 0.645912, acc: 60.94%] [G loss: 1.898545]\n",
      "epoch:17 step:16565 [D loss: 0.620990, acc: 64.84%] [G loss: 1.863157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16566 [D loss: 0.607655, acc: 67.19%] [G loss: 1.810420]\n",
      "epoch:17 step:16567 [D loss: 0.615903, acc: 64.84%] [G loss: 1.960321]\n",
      "epoch:17 step:16568 [D loss: 0.605830, acc: 65.62%] [G loss: 1.814515]\n",
      "epoch:17 step:16569 [D loss: 0.650900, acc: 63.28%] [G loss: 1.881295]\n",
      "epoch:17 step:16570 [D loss: 0.600507, acc: 69.53%] [G loss: 2.004719]\n",
      "epoch:17 step:16571 [D loss: 0.623728, acc: 67.97%] [G loss: 2.034178]\n",
      "epoch:17 step:16572 [D loss: 0.667103, acc: 60.16%] [G loss: 1.920743]\n",
      "epoch:17 step:16573 [D loss: 0.615561, acc: 63.28%] [G loss: 2.002066]\n",
      "epoch:17 step:16574 [D loss: 0.621261, acc: 66.41%] [G loss: 1.995995]\n",
      "epoch:17 step:16575 [D loss: 0.622750, acc: 65.62%] [G loss: 2.081719]\n",
      "epoch:17 step:16576 [D loss: 0.653403, acc: 67.19%] [G loss: 2.058013]\n",
      "epoch:17 step:16577 [D loss: 0.564070, acc: 75.78%] [G loss: 2.278206]\n",
      "epoch:17 step:16578 [D loss: 0.598005, acc: 74.22%] [G loss: 2.059928]\n",
      "epoch:17 step:16579 [D loss: 0.638660, acc: 64.06%] [G loss: 2.154323]\n",
      "epoch:17 step:16580 [D loss: 0.653192, acc: 64.84%] [G loss: 1.994539]\n",
      "epoch:17 step:16581 [D loss: 0.668562, acc: 54.69%] [G loss: 1.889793]\n",
      "epoch:17 step:16582 [D loss: 0.647742, acc: 62.50%] [G loss: 1.871565]\n",
      "epoch:17 step:16583 [D loss: 0.630118, acc: 66.41%] [G loss: 2.003248]\n",
      "epoch:17 step:16584 [D loss: 0.665779, acc: 60.16%] [G loss: 1.973442]\n",
      "epoch:17 step:16585 [D loss: 0.676589, acc: 58.59%] [G loss: 1.875743]\n",
      "epoch:17 step:16586 [D loss: 0.725945, acc: 52.34%] [G loss: 1.889009]\n",
      "epoch:17 step:16587 [D loss: 0.713404, acc: 56.25%] [G loss: 1.587671]\n",
      "epoch:17 step:16588 [D loss: 0.638244, acc: 64.84%] [G loss: 1.738057]\n",
      "epoch:17 step:16589 [D loss: 0.633849, acc: 67.19%] [G loss: 1.939309]\n",
      "epoch:17 step:16590 [D loss: 0.652380, acc: 63.28%] [G loss: 1.762443]\n",
      "epoch:17 step:16591 [D loss: 0.615404, acc: 67.19%] [G loss: 2.019793]\n",
      "epoch:17 step:16592 [D loss: 0.632958, acc: 60.94%] [G loss: 1.865031]\n",
      "epoch:17 step:16593 [D loss: 0.655555, acc: 62.50%] [G loss: 1.818786]\n",
      "epoch:17 step:16594 [D loss: 0.652633, acc: 60.94%] [G loss: 1.870011]\n",
      "epoch:17 step:16595 [D loss: 0.711559, acc: 55.47%] [G loss: 1.755653]\n",
      "epoch:17 step:16596 [D loss: 0.680240, acc: 51.56%] [G loss: 1.718225]\n",
      "epoch:17 step:16597 [D loss: 0.591554, acc: 70.31%] [G loss: 1.876166]\n",
      "epoch:17 step:16598 [D loss: 0.656049, acc: 58.59%] [G loss: 1.824533]\n",
      "epoch:17 step:16599 [D loss: 0.664562, acc: 60.94%] [G loss: 1.800657]\n",
      "epoch:17 step:16600 [D loss: 0.703946, acc: 57.81%] [G loss: 1.788972]\n",
      "epoch:17 step:16601 [D loss: 0.640690, acc: 60.16%] [G loss: 1.818679]\n",
      "epoch:17 step:16602 [D loss: 0.666115, acc: 56.25%] [G loss: 1.854051]\n",
      "epoch:17 step:16603 [D loss: 0.612333, acc: 63.28%] [G loss: 1.841836]\n",
      "epoch:17 step:16604 [D loss: 0.643720, acc: 64.06%] [G loss: 1.796479]\n",
      "epoch:17 step:16605 [D loss: 0.593752, acc: 71.09%] [G loss: 1.831171]\n",
      "epoch:17 step:16606 [D loss: 0.616401, acc: 67.97%] [G loss: 1.943838]\n",
      "epoch:17 step:16607 [D loss: 0.665311, acc: 61.72%] [G loss: 1.844649]\n",
      "epoch:17 step:16608 [D loss: 0.604245, acc: 66.41%] [G loss: 1.946300]\n",
      "epoch:17 step:16609 [D loss: 0.645413, acc: 64.84%] [G loss: 1.994493]\n",
      "epoch:17 step:16610 [D loss: 0.609548, acc: 64.06%] [G loss: 1.934855]\n",
      "epoch:17 step:16611 [D loss: 0.656442, acc: 60.94%] [G loss: 1.848665]\n",
      "epoch:17 step:16612 [D loss: 0.715058, acc: 52.34%] [G loss: 1.892770]\n",
      "epoch:17 step:16613 [D loss: 0.707566, acc: 56.25%] [G loss: 1.731945]\n",
      "epoch:17 step:16614 [D loss: 0.685268, acc: 60.16%] [G loss: 1.861314]\n",
      "epoch:17 step:16615 [D loss: 0.618496, acc: 64.06%] [G loss: 1.833289]\n",
      "epoch:17 step:16616 [D loss: 0.662149, acc: 64.84%] [G loss: 1.889337]\n",
      "epoch:17 step:16617 [D loss: 0.600582, acc: 67.19%] [G loss: 1.900759]\n",
      "epoch:17 step:16618 [D loss: 0.655049, acc: 60.94%] [G loss: 1.911483]\n",
      "epoch:17 step:16619 [D loss: 0.580966, acc: 69.53%] [G loss: 2.199048]\n",
      "epoch:17 step:16620 [D loss: 0.633549, acc: 66.41%] [G loss: 1.994875]\n",
      "epoch:17 step:16621 [D loss: 0.641018, acc: 61.72%] [G loss: 2.018727]\n",
      "epoch:17 step:16622 [D loss: 0.595828, acc: 68.75%] [G loss: 2.009140]\n",
      "epoch:17 step:16623 [D loss: 0.625054, acc: 65.62%] [G loss: 2.110770]\n",
      "epoch:17 step:16624 [D loss: 0.617990, acc: 66.41%] [G loss: 2.074773]\n",
      "epoch:17 step:16625 [D loss: 0.631439, acc: 62.50%] [G loss: 1.844633]\n",
      "epoch:17 step:16626 [D loss: 0.647589, acc: 65.62%] [G loss: 1.913595]\n",
      "epoch:17 step:16627 [D loss: 0.732357, acc: 57.81%] [G loss: 1.803905]\n",
      "epoch:17 step:16628 [D loss: 0.616664, acc: 62.50%] [G loss: 2.083150]\n",
      "epoch:17 step:16629 [D loss: 0.672200, acc: 62.50%] [G loss: 1.900579]\n",
      "epoch:17 step:16630 [D loss: 0.636464, acc: 56.25%] [G loss: 1.926652]\n",
      "epoch:17 step:16631 [D loss: 0.699096, acc: 54.69%] [G loss: 1.745182]\n",
      "epoch:17 step:16632 [D loss: 0.627226, acc: 69.53%] [G loss: 1.793202]\n",
      "epoch:17 step:16633 [D loss: 0.706582, acc: 51.56%] [G loss: 1.774698]\n",
      "epoch:17 step:16634 [D loss: 0.642158, acc: 60.94%] [G loss: 1.944967]\n",
      "epoch:17 step:16635 [D loss: 0.663316, acc: 62.50%] [G loss: 1.841749]\n",
      "epoch:17 step:16636 [D loss: 0.642219, acc: 64.06%] [G loss: 1.930285]\n",
      "epoch:17 step:16637 [D loss: 0.641353, acc: 65.62%] [G loss: 1.925391]\n",
      "epoch:17 step:16638 [D loss: 0.607690, acc: 67.19%] [G loss: 1.939436]\n",
      "epoch:17 step:16639 [D loss: 0.683056, acc: 58.59%] [G loss: 1.921193]\n",
      "epoch:17 step:16640 [D loss: 0.624321, acc: 68.75%] [G loss: 2.061275]\n",
      "epoch:17 step:16641 [D loss: 0.646927, acc: 55.47%] [G loss: 1.863515]\n",
      "epoch:17 step:16642 [D loss: 0.636142, acc: 65.62%] [G loss: 1.891878]\n",
      "epoch:17 step:16643 [D loss: 0.648561, acc: 59.38%] [G loss: 1.993039]\n",
      "epoch:17 step:16644 [D loss: 0.652015, acc: 67.19%] [G loss: 1.893553]\n",
      "epoch:17 step:16645 [D loss: 0.716017, acc: 50.78%] [G loss: 1.757120]\n",
      "epoch:17 step:16646 [D loss: 0.619415, acc: 67.19%] [G loss: 1.838377]\n",
      "epoch:17 step:16647 [D loss: 0.674102, acc: 62.50%] [G loss: 1.931155]\n",
      "epoch:17 step:16648 [D loss: 0.651226, acc: 63.28%] [G loss: 1.902443]\n",
      "epoch:17 step:16649 [D loss: 0.632815, acc: 66.41%] [G loss: 1.981012]\n",
      "epoch:17 step:16650 [D loss: 0.629131, acc: 66.41%] [G loss: 1.922886]\n",
      "epoch:17 step:16651 [D loss: 0.627461, acc: 61.72%] [G loss: 1.725755]\n",
      "epoch:17 step:16652 [D loss: 0.653070, acc: 60.94%] [G loss: 1.910290]\n",
      "epoch:17 step:16653 [D loss: 0.677848, acc: 62.50%] [G loss: 1.793165]\n",
      "epoch:17 step:16654 [D loss: 0.656030, acc: 60.16%] [G loss: 1.919605]\n",
      "epoch:17 step:16655 [D loss: 0.617746, acc: 67.19%] [G loss: 1.945400]\n",
      "epoch:17 step:16656 [D loss: 0.666507, acc: 64.06%] [G loss: 1.833348]\n",
      "epoch:17 step:16657 [D loss: 0.662709, acc: 61.72%] [G loss: 1.866638]\n",
      "epoch:17 step:16658 [D loss: 0.643258, acc: 63.28%] [G loss: 1.855770]\n",
      "epoch:17 step:16659 [D loss: 0.637368, acc: 64.06%] [G loss: 1.869853]\n",
      "epoch:17 step:16660 [D loss: 0.644591, acc: 63.28%] [G loss: 1.760891]\n",
      "epoch:17 step:16661 [D loss: 0.628813, acc: 62.50%] [G loss: 1.871794]\n",
      "epoch:17 step:16662 [D loss: 0.640786, acc: 67.19%] [G loss: 2.027799]\n",
      "epoch:17 step:16663 [D loss: 0.648580, acc: 61.72%] [G loss: 1.846505]\n",
      "epoch:17 step:16664 [D loss: 0.652822, acc: 58.59%] [G loss: 1.837147]\n",
      "epoch:17 step:16665 [D loss: 0.615772, acc: 66.41%] [G loss: 1.851713]\n",
      "epoch:17 step:16666 [D loss: 0.628134, acc: 67.97%] [G loss: 1.931728]\n",
      "epoch:17 step:16667 [D loss: 0.643529, acc: 61.72%] [G loss: 1.767290]\n",
      "epoch:17 step:16668 [D loss: 0.657646, acc: 65.62%] [G loss: 1.862051]\n",
      "epoch:17 step:16669 [D loss: 0.657602, acc: 60.94%] [G loss: 1.959356]\n",
      "epoch:17 step:16670 [D loss: 0.734223, acc: 54.69%] [G loss: 1.858619]\n",
      "epoch:17 step:16671 [D loss: 0.643915, acc: 64.06%] [G loss: 1.847838]\n",
      "epoch:17 step:16672 [D loss: 0.711362, acc: 56.25%] [G loss: 1.830009]\n",
      "epoch:17 step:16673 [D loss: 0.668656, acc: 59.38%] [G loss: 1.870763]\n",
      "epoch:17 step:16674 [D loss: 0.668935, acc: 60.16%] [G loss: 1.675816]\n",
      "epoch:17 step:16675 [D loss: 0.670662, acc: 58.59%] [G loss: 1.841653]\n",
      "epoch:17 step:16676 [D loss: 0.636154, acc: 60.94%] [G loss: 1.978911]\n",
      "epoch:17 step:16677 [D loss: 0.638821, acc: 67.19%] [G loss: 1.842162]\n",
      "epoch:17 step:16678 [D loss: 0.685093, acc: 60.16%] [G loss: 1.674272]\n",
      "epoch:17 step:16679 [D loss: 0.645565, acc: 61.72%] [G loss: 1.887518]\n",
      "epoch:17 step:16680 [D loss: 0.623531, acc: 59.38%] [G loss: 1.717886]\n",
      "epoch:17 step:16681 [D loss: 0.670962, acc: 61.72%] [G loss: 1.909209]\n",
      "epoch:17 step:16682 [D loss: 0.667287, acc: 58.59%] [G loss: 1.838432]\n",
      "epoch:17 step:16683 [D loss: 0.630208, acc: 66.41%] [G loss: 1.831012]\n",
      "epoch:17 step:16684 [D loss: 0.679676, acc: 53.91%] [G loss: 1.862642]\n",
      "epoch:17 step:16685 [D loss: 0.592955, acc: 69.53%] [G loss: 1.745303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16686 [D loss: 0.599675, acc: 64.84%] [G loss: 2.004187]\n",
      "epoch:17 step:16687 [D loss: 0.685531, acc: 60.94%] [G loss: 1.791561]\n",
      "epoch:17 step:16688 [D loss: 0.659312, acc: 55.47%] [G loss: 1.795701]\n",
      "epoch:17 step:16689 [D loss: 0.659993, acc: 62.50%] [G loss: 1.797873]\n",
      "epoch:17 step:16690 [D loss: 0.655103, acc: 60.94%] [G loss: 1.810843]\n",
      "epoch:17 step:16691 [D loss: 0.625253, acc: 62.50%] [G loss: 1.843486]\n",
      "epoch:17 step:16692 [D loss: 0.667136, acc: 57.03%] [G loss: 1.780470]\n",
      "epoch:17 step:16693 [D loss: 0.625061, acc: 62.50%] [G loss: 1.787484]\n",
      "epoch:17 step:16694 [D loss: 0.626216, acc: 67.19%] [G loss: 1.819536]\n",
      "epoch:17 step:16695 [D loss: 0.691206, acc: 60.16%] [G loss: 1.788484]\n",
      "epoch:17 step:16696 [D loss: 0.585335, acc: 70.31%] [G loss: 1.958598]\n",
      "epoch:17 step:16697 [D loss: 0.663124, acc: 60.16%] [G loss: 1.840623]\n",
      "epoch:17 step:16698 [D loss: 0.650047, acc: 60.16%] [G loss: 1.896681]\n",
      "epoch:17 step:16699 [D loss: 0.656877, acc: 59.38%] [G loss: 1.932429]\n",
      "epoch:17 step:16700 [D loss: 0.658085, acc: 61.72%] [G loss: 1.799501]\n",
      "epoch:17 step:16701 [D loss: 0.614886, acc: 61.72%] [G loss: 1.856754]\n",
      "epoch:17 step:16702 [D loss: 0.638018, acc: 62.50%] [G loss: 1.842794]\n",
      "epoch:17 step:16703 [D loss: 0.645451, acc: 63.28%] [G loss: 2.085066]\n",
      "epoch:17 step:16704 [D loss: 0.569910, acc: 67.19%] [G loss: 2.149727]\n",
      "epoch:17 step:16705 [D loss: 0.605904, acc: 67.19%] [G loss: 1.951616]\n",
      "epoch:17 step:16706 [D loss: 0.597912, acc: 71.09%] [G loss: 1.961158]\n",
      "epoch:17 step:16707 [D loss: 0.622562, acc: 64.06%] [G loss: 2.164330]\n",
      "epoch:17 step:16708 [D loss: 0.667948, acc: 60.94%] [G loss: 1.981950]\n",
      "epoch:17 step:16709 [D loss: 0.615970, acc: 64.84%] [G loss: 1.928642]\n",
      "epoch:17 step:16710 [D loss: 0.619306, acc: 69.53%] [G loss: 2.371142]\n",
      "epoch:17 step:16711 [D loss: 0.610061, acc: 64.06%] [G loss: 2.048233]\n",
      "epoch:17 step:16712 [D loss: 0.645942, acc: 61.72%] [G loss: 1.916890]\n",
      "epoch:17 step:16713 [D loss: 0.715823, acc: 64.06%] [G loss: 1.848450]\n",
      "epoch:17 step:16714 [D loss: 0.667204, acc: 60.16%] [G loss: 2.035654]\n",
      "epoch:17 step:16715 [D loss: 0.658917, acc: 57.81%] [G loss: 1.808856]\n",
      "epoch:17 step:16716 [D loss: 0.662798, acc: 57.81%] [G loss: 1.969872]\n",
      "epoch:17 step:16717 [D loss: 0.655901, acc: 58.59%] [G loss: 1.731040]\n",
      "epoch:17 step:16718 [D loss: 0.624406, acc: 61.72%] [G loss: 1.926984]\n",
      "epoch:17 step:16719 [D loss: 0.631536, acc: 64.84%] [G loss: 1.927316]\n",
      "epoch:17 step:16720 [D loss: 0.635861, acc: 65.62%] [G loss: 1.892739]\n",
      "epoch:17 step:16721 [D loss: 0.608882, acc: 64.84%] [G loss: 1.868879]\n",
      "epoch:17 step:16722 [D loss: 0.570337, acc: 67.97%] [G loss: 2.207110]\n",
      "epoch:17 step:16723 [D loss: 0.715652, acc: 53.91%] [G loss: 1.838035]\n",
      "epoch:17 step:16724 [D loss: 0.706636, acc: 56.25%] [G loss: 1.817587]\n",
      "epoch:17 step:16725 [D loss: 0.710157, acc: 53.12%] [G loss: 1.908318]\n",
      "epoch:17 step:16726 [D loss: 0.608069, acc: 67.97%] [G loss: 1.825337]\n",
      "epoch:17 step:16727 [D loss: 0.677891, acc: 58.59%] [G loss: 1.972951]\n",
      "epoch:17 step:16728 [D loss: 0.653544, acc: 61.72%] [G loss: 1.836571]\n",
      "epoch:17 step:16729 [D loss: 0.655687, acc: 62.50%] [G loss: 1.802256]\n",
      "epoch:17 step:16730 [D loss: 0.659671, acc: 60.16%] [G loss: 2.041653]\n",
      "epoch:17 step:16731 [D loss: 0.663422, acc: 61.72%] [G loss: 1.956196]\n",
      "epoch:17 step:16732 [D loss: 0.675072, acc: 63.28%] [G loss: 1.739678]\n",
      "epoch:17 step:16733 [D loss: 0.640453, acc: 63.28%] [G loss: 2.096731]\n",
      "epoch:17 step:16734 [D loss: 0.626871, acc: 64.06%] [G loss: 1.854235]\n",
      "epoch:17 step:16735 [D loss: 0.669208, acc: 59.38%] [G loss: 1.887020]\n",
      "epoch:17 step:16736 [D loss: 0.614527, acc: 65.62%] [G loss: 1.895470]\n",
      "epoch:17 step:16737 [D loss: 0.655892, acc: 61.72%] [G loss: 1.878772]\n",
      "epoch:17 step:16738 [D loss: 0.690269, acc: 57.03%] [G loss: 1.818429]\n",
      "epoch:17 step:16739 [D loss: 0.634560, acc: 62.50%] [G loss: 2.020481]\n",
      "epoch:17 step:16740 [D loss: 0.660399, acc: 61.72%] [G loss: 1.893377]\n",
      "epoch:17 step:16741 [D loss: 0.625316, acc: 65.62%] [G loss: 1.978853]\n",
      "epoch:17 step:16742 [D loss: 0.608173, acc: 64.06%] [G loss: 1.917951]\n",
      "epoch:17 step:16743 [D loss: 0.604375, acc: 68.75%] [G loss: 1.976500]\n",
      "epoch:17 step:16744 [D loss: 0.649861, acc: 62.50%] [G loss: 2.174911]\n",
      "epoch:17 step:16745 [D loss: 0.606005, acc: 67.19%] [G loss: 2.140713]\n",
      "epoch:17 step:16746 [D loss: 0.647200, acc: 62.50%] [G loss: 1.999011]\n",
      "epoch:17 step:16747 [D loss: 0.672832, acc: 60.94%] [G loss: 1.797574]\n",
      "epoch:17 step:16748 [D loss: 0.646397, acc: 63.28%] [G loss: 2.007469]\n",
      "epoch:17 step:16749 [D loss: 0.728997, acc: 48.44%] [G loss: 1.793969]\n",
      "epoch:17 step:16750 [D loss: 0.646921, acc: 58.59%] [G loss: 1.849681]\n",
      "epoch:17 step:16751 [D loss: 0.666900, acc: 62.50%] [G loss: 1.931995]\n",
      "epoch:17 step:16752 [D loss: 0.656650, acc: 57.03%] [G loss: 1.971885]\n",
      "epoch:17 step:16753 [D loss: 0.656394, acc: 60.16%] [G loss: 1.826034]\n",
      "epoch:17 step:16754 [D loss: 0.650543, acc: 60.16%] [G loss: 1.917316]\n",
      "epoch:17 step:16755 [D loss: 0.680345, acc: 58.59%] [G loss: 1.880532]\n",
      "epoch:17 step:16756 [D loss: 0.677701, acc: 59.38%] [G loss: 1.736707]\n",
      "epoch:17 step:16757 [D loss: 0.672841, acc: 57.81%] [G loss: 1.593722]\n",
      "epoch:17 step:16758 [D loss: 0.657052, acc: 59.38%] [G loss: 1.818329]\n",
      "epoch:17 step:16759 [D loss: 0.711883, acc: 53.12%] [G loss: 1.724789]\n",
      "epoch:17 step:16760 [D loss: 0.635214, acc: 64.84%] [G loss: 1.862068]\n",
      "epoch:17 step:16761 [D loss: 0.608230, acc: 65.62%] [G loss: 1.918060]\n",
      "epoch:17 step:16762 [D loss: 0.650848, acc: 60.94%] [G loss: 1.981061]\n",
      "epoch:17 step:16763 [D loss: 0.624272, acc: 63.28%] [G loss: 1.994964]\n",
      "epoch:17 step:16764 [D loss: 0.671099, acc: 59.38%] [G loss: 1.919619]\n",
      "epoch:17 step:16765 [D loss: 0.621220, acc: 67.97%] [G loss: 1.790645]\n",
      "epoch:17 step:16766 [D loss: 0.612029, acc: 62.50%] [G loss: 1.848495]\n",
      "epoch:17 step:16767 [D loss: 0.664109, acc: 64.06%] [G loss: 1.848324]\n",
      "epoch:17 step:16768 [D loss: 0.626713, acc: 66.41%] [G loss: 1.883370]\n",
      "epoch:17 step:16769 [D loss: 0.691044, acc: 52.34%] [G loss: 1.864143]\n",
      "epoch:17 step:16770 [D loss: 0.618218, acc: 65.62%] [G loss: 1.910849]\n",
      "epoch:17 step:16771 [D loss: 0.646630, acc: 64.06%] [G loss: 1.997232]\n",
      "epoch:17 step:16772 [D loss: 0.660249, acc: 60.16%] [G loss: 1.828426]\n",
      "epoch:17 step:16773 [D loss: 0.641981, acc: 57.81%] [G loss: 1.821771]\n",
      "epoch:17 step:16774 [D loss: 0.634771, acc: 62.50%] [G loss: 1.813632]\n",
      "epoch:17 step:16775 [D loss: 0.641400, acc: 67.97%] [G loss: 1.844539]\n",
      "epoch:17 step:16776 [D loss: 0.623547, acc: 59.38%] [G loss: 2.057209]\n",
      "epoch:17 step:16777 [D loss: 0.631534, acc: 64.06%] [G loss: 1.836008]\n",
      "epoch:17 step:16778 [D loss: 0.654732, acc: 55.47%] [G loss: 2.008589]\n",
      "epoch:17 step:16779 [D loss: 0.690776, acc: 56.25%] [G loss: 1.767842]\n",
      "epoch:17 step:16780 [D loss: 0.616788, acc: 67.97%] [G loss: 1.777939]\n",
      "epoch:17 step:16781 [D loss: 0.625509, acc: 65.62%] [G loss: 1.852420]\n",
      "epoch:17 step:16782 [D loss: 0.623679, acc: 69.53%] [G loss: 1.892689]\n",
      "epoch:17 step:16783 [D loss: 0.675042, acc: 60.16%] [G loss: 1.859012]\n",
      "epoch:17 step:16784 [D loss: 0.653847, acc: 57.81%] [G loss: 1.803899]\n",
      "epoch:17 step:16785 [D loss: 0.676523, acc: 61.72%] [G loss: 1.797843]\n",
      "epoch:17 step:16786 [D loss: 0.671694, acc: 58.59%] [G loss: 1.996029]\n",
      "epoch:17 step:16787 [D loss: 0.702662, acc: 59.38%] [G loss: 1.744413]\n",
      "epoch:17 step:16788 [D loss: 0.683483, acc: 57.81%] [G loss: 1.810869]\n",
      "epoch:17 step:16789 [D loss: 0.637031, acc: 65.62%] [G loss: 1.926404]\n",
      "epoch:17 step:16790 [D loss: 0.691958, acc: 50.78%] [G loss: 1.783351]\n",
      "epoch:17 step:16791 [D loss: 0.663440, acc: 64.06%] [G loss: 1.705458]\n",
      "epoch:17 step:16792 [D loss: 0.658849, acc: 63.28%] [G loss: 1.770305]\n",
      "epoch:17 step:16793 [D loss: 0.624172, acc: 66.41%] [G loss: 1.898960]\n",
      "epoch:17 step:16794 [D loss: 0.628608, acc: 64.06%] [G loss: 1.748105]\n",
      "epoch:17 step:16795 [D loss: 0.627143, acc: 71.09%] [G loss: 1.885112]\n",
      "epoch:17 step:16796 [D loss: 0.653254, acc: 60.16%] [G loss: 1.755246]\n",
      "epoch:17 step:16797 [D loss: 0.642120, acc: 63.28%] [G loss: 1.820658]\n",
      "epoch:17 step:16798 [D loss: 0.673531, acc: 64.06%] [G loss: 1.748777]\n",
      "epoch:17 step:16799 [D loss: 0.646712, acc: 62.50%] [G loss: 1.920071]\n",
      "epoch:17 step:16800 [D loss: 0.598522, acc: 68.75%] [G loss: 1.779092]\n",
      "epoch:17 step:16801 [D loss: 0.621114, acc: 61.72%] [G loss: 1.674931]\n",
      "epoch:17 step:16802 [D loss: 0.646637, acc: 64.06%] [G loss: 1.697133]\n",
      "epoch:17 step:16803 [D loss: 0.659890, acc: 58.59%] [G loss: 1.708759]\n",
      "epoch:17 step:16804 [D loss: 0.642150, acc: 64.84%] [G loss: 1.788780]\n",
      "epoch:17 step:16805 [D loss: 0.655224, acc: 60.94%] [G loss: 1.885684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:16806 [D loss: 0.633891, acc: 67.19%] [G loss: 1.813297]\n",
      "epoch:17 step:16807 [D loss: 0.623847, acc: 63.28%] [G loss: 1.811596]\n",
      "epoch:17 step:16808 [D loss: 0.656641, acc: 56.25%] [G loss: 1.756434]\n",
      "epoch:17 step:16809 [D loss: 0.661922, acc: 61.72%] [G loss: 1.818506]\n",
      "epoch:17 step:16810 [D loss: 0.629866, acc: 60.16%] [G loss: 1.885423]\n",
      "epoch:17 step:16811 [D loss: 0.640589, acc: 67.19%] [G loss: 1.892675]\n",
      "epoch:17 step:16812 [D loss: 0.633790, acc: 58.59%] [G loss: 1.893962]\n",
      "epoch:17 step:16813 [D loss: 0.620304, acc: 62.50%] [G loss: 1.904855]\n",
      "epoch:17 step:16814 [D loss: 0.653651, acc: 60.94%] [G loss: 1.953570]\n",
      "epoch:17 step:16815 [D loss: 0.607615, acc: 64.84%] [G loss: 2.002786]\n",
      "epoch:17 step:16816 [D loss: 0.667004, acc: 59.38%] [G loss: 1.908277]\n",
      "epoch:17 step:16817 [D loss: 0.658070, acc: 57.81%] [G loss: 1.835543]\n",
      "epoch:17 step:16818 [D loss: 0.662007, acc: 53.91%] [G loss: 1.785497]\n",
      "epoch:17 step:16819 [D loss: 0.645257, acc: 65.62%] [G loss: 2.001341]\n",
      "epoch:17 step:16820 [D loss: 0.632558, acc: 63.28%] [G loss: 1.809966]\n",
      "epoch:17 step:16821 [D loss: 0.658606, acc: 60.16%] [G loss: 1.691095]\n",
      "epoch:17 step:16822 [D loss: 0.674418, acc: 60.94%] [G loss: 1.791639]\n",
      "epoch:17 step:16823 [D loss: 0.618524, acc: 67.97%] [G loss: 1.902673]\n",
      "epoch:17 step:16824 [D loss: 0.666096, acc: 59.38%] [G loss: 1.984823]\n",
      "epoch:17 step:16825 [D loss: 0.609596, acc: 64.84%] [G loss: 1.922032]\n",
      "epoch:17 step:16826 [D loss: 0.652749, acc: 60.94%] [G loss: 1.964094]\n",
      "epoch:17 step:16827 [D loss: 0.606109, acc: 63.28%] [G loss: 1.977023]\n",
      "epoch:17 step:16828 [D loss: 0.612270, acc: 64.84%] [G loss: 2.029577]\n",
      "epoch:17 step:16829 [D loss: 0.609271, acc: 64.06%] [G loss: 1.949867]\n",
      "epoch:17 step:16830 [D loss: 0.587002, acc: 70.31%] [G loss: 2.152196]\n",
      "epoch:17 step:16831 [D loss: 0.648416, acc: 64.84%] [G loss: 1.957004]\n",
      "epoch:17 step:16832 [D loss: 0.612217, acc: 66.41%] [G loss: 1.886555]\n",
      "epoch:17 step:16833 [D loss: 0.667610, acc: 62.50%] [G loss: 2.004530]\n",
      "epoch:17 step:16834 [D loss: 0.667284, acc: 53.91%] [G loss: 1.999929]\n",
      "epoch:17 step:16835 [D loss: 0.663398, acc: 62.50%] [G loss: 1.997262]\n",
      "epoch:17 step:16836 [D loss: 0.631073, acc: 64.84%] [G loss: 2.029036]\n",
      "epoch:17 step:16837 [D loss: 0.627146, acc: 69.53%] [G loss: 1.867034]\n",
      "epoch:17 step:16838 [D loss: 0.614908, acc: 65.62%] [G loss: 2.051122]\n",
      "epoch:17 step:16839 [D loss: 0.611350, acc: 71.09%] [G loss: 1.960692]\n",
      "epoch:17 step:16840 [D loss: 0.718377, acc: 59.38%] [G loss: 1.962242]\n",
      "epoch:17 step:16841 [D loss: 0.656616, acc: 64.06%] [G loss: 2.172428]\n",
      "epoch:17 step:16842 [D loss: 0.669321, acc: 61.72%] [G loss: 1.913918]\n",
      "epoch:17 step:16843 [D loss: 0.632975, acc: 62.50%] [G loss: 1.882100]\n",
      "epoch:17 step:16844 [D loss: 0.664614, acc: 58.59%] [G loss: 2.018554]\n",
      "epoch:17 step:16845 [D loss: 0.639126, acc: 62.50%] [G loss: 1.941542]\n",
      "epoch:17 step:16846 [D loss: 0.625724, acc: 62.50%] [G loss: 1.998077]\n",
      "epoch:17 step:16847 [D loss: 0.620476, acc: 67.19%] [G loss: 2.095362]\n",
      "epoch:17 step:16848 [D loss: 0.617519, acc: 69.53%] [G loss: 2.059268]\n",
      "epoch:17 step:16849 [D loss: 0.728833, acc: 49.22%] [G loss: 1.953531]\n",
      "epoch:17 step:16850 [D loss: 0.650277, acc: 64.06%] [G loss: 1.827123]\n",
      "epoch:17 step:16851 [D loss: 0.642927, acc: 58.59%] [G loss: 1.942210]\n",
      "epoch:17 step:16852 [D loss: 0.559977, acc: 74.22%] [G loss: 2.053328]\n",
      "epoch:17 step:16853 [D loss: 0.590777, acc: 69.53%] [G loss: 2.183177]\n",
      "epoch:17 step:16854 [D loss: 0.568379, acc: 72.66%] [G loss: 2.201365]\n",
      "epoch:17 step:16855 [D loss: 0.586829, acc: 71.09%] [G loss: 2.275491]\n",
      "epoch:17 step:16856 [D loss: 0.640317, acc: 62.50%] [G loss: 2.201823]\n",
      "epoch:17 step:16857 [D loss: 0.770777, acc: 51.56%] [G loss: 1.893846]\n",
      "epoch:17 step:16858 [D loss: 0.740834, acc: 53.91%] [G loss: 2.194409]\n",
      "epoch:17 step:16859 [D loss: 0.578895, acc: 71.09%] [G loss: 2.048828]\n",
      "epoch:17 step:16860 [D loss: 0.608516, acc: 64.84%] [G loss: 1.916913]\n",
      "epoch:17 step:16861 [D loss: 0.726713, acc: 57.81%] [G loss: 1.899760]\n",
      "epoch:17 step:16862 [D loss: 0.630731, acc: 65.62%] [G loss: 1.932528]\n",
      "epoch:17 step:16863 [D loss: 0.638526, acc: 64.84%] [G loss: 1.916965]\n",
      "epoch:17 step:16864 [D loss: 0.664701, acc: 62.50%] [G loss: 1.831228]\n",
      "epoch:17 step:16865 [D loss: 0.602752, acc: 69.53%] [G loss: 2.075493]\n",
      "epoch:17 step:16866 [D loss: 0.569170, acc: 71.88%] [G loss: 2.482748]\n",
      "epoch:18 step:16867 [D loss: 0.671526, acc: 58.59%] [G loss: 1.959704]\n",
      "epoch:18 step:16868 [D loss: 0.690055, acc: 60.16%] [G loss: 1.944668]\n",
      "epoch:18 step:16869 [D loss: 0.635308, acc: 60.94%] [G loss: 1.950763]\n",
      "epoch:18 step:16870 [D loss: 0.642620, acc: 64.84%] [G loss: 1.807732]\n",
      "epoch:18 step:16871 [D loss: 0.624294, acc: 63.28%] [G loss: 1.937090]\n",
      "epoch:18 step:16872 [D loss: 0.684666, acc: 56.25%] [G loss: 1.851835]\n",
      "epoch:18 step:16873 [D loss: 0.620528, acc: 64.84%] [G loss: 1.944950]\n",
      "epoch:18 step:16874 [D loss: 0.627828, acc: 67.19%] [G loss: 1.887421]\n",
      "epoch:18 step:16875 [D loss: 0.628251, acc: 66.41%] [G loss: 2.186882]\n",
      "epoch:18 step:16876 [D loss: 0.638820, acc: 64.06%] [G loss: 2.001481]\n",
      "epoch:18 step:16877 [D loss: 0.640436, acc: 64.06%] [G loss: 1.926090]\n",
      "epoch:18 step:16878 [D loss: 0.640583, acc: 60.16%] [G loss: 1.841905]\n",
      "epoch:18 step:16879 [D loss: 0.635867, acc: 64.06%] [G loss: 1.947561]\n",
      "epoch:18 step:16880 [D loss: 0.660728, acc: 61.72%] [G loss: 1.958814]\n",
      "epoch:18 step:16881 [D loss: 0.539691, acc: 78.12%] [G loss: 2.254361]\n",
      "epoch:18 step:16882 [D loss: 0.649160, acc: 64.84%] [G loss: 2.161486]\n",
      "epoch:18 step:16883 [D loss: 0.635117, acc: 64.06%] [G loss: 1.830584]\n",
      "epoch:18 step:16884 [D loss: 0.619612, acc: 68.75%] [G loss: 1.951325]\n",
      "epoch:18 step:16885 [D loss: 0.720511, acc: 53.12%] [G loss: 1.978675]\n",
      "epoch:18 step:16886 [D loss: 0.704737, acc: 54.69%] [G loss: 1.766027]\n",
      "epoch:18 step:16887 [D loss: 0.701194, acc: 55.47%] [G loss: 1.798766]\n",
      "epoch:18 step:16888 [D loss: 0.702773, acc: 57.03%] [G loss: 1.760270]\n",
      "epoch:18 step:16889 [D loss: 0.637668, acc: 61.72%] [G loss: 1.955715]\n",
      "epoch:18 step:16890 [D loss: 0.658563, acc: 60.94%] [G loss: 1.989986]\n",
      "epoch:18 step:16891 [D loss: 0.625133, acc: 65.62%] [G loss: 2.019166]\n",
      "epoch:18 step:16892 [D loss: 0.641071, acc: 61.72%] [G loss: 1.871660]\n",
      "epoch:18 step:16893 [D loss: 0.675828, acc: 55.47%] [G loss: 1.789165]\n",
      "epoch:18 step:16894 [D loss: 0.675236, acc: 50.78%] [G loss: 1.836950]\n",
      "epoch:18 step:16895 [D loss: 0.611775, acc: 66.41%] [G loss: 1.893491]\n",
      "epoch:18 step:16896 [D loss: 0.684604, acc: 56.25%] [G loss: 1.831109]\n",
      "epoch:18 step:16897 [D loss: 0.698963, acc: 54.69%] [G loss: 1.729204]\n",
      "epoch:18 step:16898 [D loss: 0.646776, acc: 63.28%] [G loss: 1.811323]\n",
      "epoch:18 step:16899 [D loss: 0.672313, acc: 58.59%] [G loss: 1.796834]\n",
      "epoch:18 step:16900 [D loss: 0.658009, acc: 61.72%] [G loss: 1.697768]\n",
      "epoch:18 step:16901 [D loss: 0.648975, acc: 60.94%] [G loss: 1.910033]\n",
      "epoch:18 step:16902 [D loss: 0.612885, acc: 64.06%] [G loss: 1.923320]\n",
      "epoch:18 step:16903 [D loss: 0.634860, acc: 64.06%] [G loss: 1.994582]\n",
      "epoch:18 step:16904 [D loss: 0.633492, acc: 59.38%] [G loss: 1.896440]\n",
      "epoch:18 step:16905 [D loss: 0.657775, acc: 60.16%] [G loss: 1.832691]\n",
      "epoch:18 step:16906 [D loss: 0.609951, acc: 68.75%] [G loss: 2.031766]\n",
      "epoch:18 step:16907 [D loss: 0.641550, acc: 62.50%] [G loss: 1.997459]\n",
      "epoch:18 step:16908 [D loss: 0.633994, acc: 66.41%] [G loss: 2.068913]\n",
      "epoch:18 step:16909 [D loss: 0.617553, acc: 62.50%] [G loss: 2.010639]\n",
      "epoch:18 step:16910 [D loss: 0.697636, acc: 57.81%] [G loss: 1.845952]\n",
      "epoch:18 step:16911 [D loss: 0.687270, acc: 64.84%] [G loss: 1.780391]\n",
      "epoch:18 step:16912 [D loss: 0.643719, acc: 64.06%] [G loss: 1.828200]\n",
      "epoch:18 step:16913 [D loss: 0.600989, acc: 65.62%] [G loss: 1.975693]\n",
      "epoch:18 step:16914 [D loss: 0.642868, acc: 69.53%] [G loss: 1.939668]\n",
      "epoch:18 step:16915 [D loss: 0.641225, acc: 64.84%] [G loss: 1.825610]\n",
      "epoch:18 step:16916 [D loss: 0.629461, acc: 65.62%] [G loss: 2.098183]\n",
      "epoch:18 step:16917 [D loss: 0.616705, acc: 66.41%] [G loss: 1.885885]\n",
      "epoch:18 step:16918 [D loss: 0.649629, acc: 63.28%] [G loss: 2.057891]\n",
      "epoch:18 step:16919 [D loss: 0.628955, acc: 61.72%] [G loss: 1.813056]\n",
      "epoch:18 step:16920 [D loss: 0.635018, acc: 67.19%] [G loss: 1.912205]\n",
      "epoch:18 step:16921 [D loss: 0.577437, acc: 70.31%] [G loss: 1.966609]\n",
      "epoch:18 step:16922 [D loss: 0.637538, acc: 68.75%] [G loss: 2.012515]\n",
      "epoch:18 step:16923 [D loss: 0.674921, acc: 59.38%] [G loss: 2.086726]\n",
      "epoch:18 step:16924 [D loss: 0.663330, acc: 64.06%] [G loss: 2.032675]\n",
      "epoch:18 step:16925 [D loss: 0.640970, acc: 62.50%] [G loss: 1.935580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:16926 [D loss: 0.643754, acc: 62.50%] [G loss: 1.715630]\n",
      "epoch:18 step:16927 [D loss: 0.661429, acc: 60.16%] [G loss: 1.836046]\n",
      "epoch:18 step:16928 [D loss: 0.666481, acc: 52.34%] [G loss: 1.893069]\n",
      "epoch:18 step:16929 [D loss: 0.709321, acc: 51.56%] [G loss: 1.994450]\n",
      "epoch:18 step:16930 [D loss: 0.589907, acc: 64.84%] [G loss: 1.869163]\n",
      "epoch:18 step:16931 [D loss: 0.658344, acc: 58.59%] [G loss: 1.945944]\n",
      "epoch:18 step:16932 [D loss: 0.641242, acc: 64.84%] [G loss: 1.811826]\n",
      "epoch:18 step:16933 [D loss: 0.656954, acc: 53.91%] [G loss: 1.838200]\n",
      "epoch:18 step:16934 [D loss: 0.608102, acc: 67.97%] [G loss: 1.885052]\n",
      "epoch:18 step:16935 [D loss: 0.617259, acc: 71.09%] [G loss: 1.997093]\n",
      "epoch:18 step:16936 [D loss: 0.631794, acc: 64.06%] [G loss: 1.853695]\n",
      "epoch:18 step:16937 [D loss: 0.658241, acc: 64.06%] [G loss: 1.861547]\n",
      "epoch:18 step:16938 [D loss: 0.649504, acc: 61.72%] [G loss: 1.905951]\n",
      "epoch:18 step:16939 [D loss: 0.683169, acc: 51.56%] [G loss: 1.772622]\n",
      "epoch:18 step:16940 [D loss: 0.606128, acc: 69.53%] [G loss: 1.857641]\n",
      "epoch:18 step:16941 [D loss: 0.725329, acc: 53.12%] [G loss: 2.013307]\n",
      "epoch:18 step:16942 [D loss: 0.598323, acc: 68.75%] [G loss: 2.008885]\n",
      "epoch:18 step:16943 [D loss: 0.624212, acc: 60.94%] [G loss: 1.997205]\n",
      "epoch:18 step:16944 [D loss: 0.684441, acc: 60.16%] [G loss: 1.812302]\n",
      "epoch:18 step:16945 [D loss: 0.682064, acc: 57.03%] [G loss: 1.767415]\n",
      "epoch:18 step:16946 [D loss: 0.664483, acc: 59.38%] [G loss: 1.780921]\n",
      "epoch:18 step:16947 [D loss: 0.643492, acc: 57.81%] [G loss: 1.763043]\n",
      "epoch:18 step:16948 [D loss: 0.679633, acc: 57.03%] [G loss: 1.783356]\n",
      "epoch:18 step:16949 [D loss: 0.668007, acc: 66.41%] [G loss: 1.849324]\n",
      "epoch:18 step:16950 [D loss: 0.639991, acc: 60.94%] [G loss: 1.911457]\n",
      "epoch:18 step:16951 [D loss: 0.685962, acc: 56.25%] [G loss: 1.677905]\n",
      "epoch:18 step:16952 [D loss: 0.665525, acc: 62.50%] [G loss: 1.803967]\n",
      "epoch:18 step:16953 [D loss: 0.631049, acc: 64.84%] [G loss: 1.788099]\n",
      "epoch:18 step:16954 [D loss: 0.680625, acc: 58.59%] [G loss: 1.959394]\n",
      "epoch:18 step:16955 [D loss: 0.637205, acc: 63.28%] [G loss: 1.816122]\n",
      "epoch:18 step:16956 [D loss: 0.647826, acc: 62.50%] [G loss: 1.760280]\n",
      "epoch:18 step:16957 [D loss: 0.663867, acc: 56.25%] [G loss: 1.788798]\n",
      "epoch:18 step:16958 [D loss: 0.590830, acc: 69.53%] [G loss: 1.847707]\n",
      "epoch:18 step:16959 [D loss: 0.661555, acc: 59.38%] [G loss: 1.935424]\n",
      "epoch:18 step:16960 [D loss: 0.609308, acc: 67.97%] [G loss: 1.843748]\n",
      "epoch:18 step:16961 [D loss: 0.630156, acc: 68.75%] [G loss: 1.937928]\n",
      "epoch:18 step:16962 [D loss: 0.677701, acc: 64.84%] [G loss: 1.913141]\n",
      "epoch:18 step:16963 [D loss: 0.646162, acc: 63.28%] [G loss: 2.015833]\n",
      "epoch:18 step:16964 [D loss: 0.683288, acc: 62.50%] [G loss: 1.835136]\n",
      "epoch:18 step:16965 [D loss: 0.684707, acc: 58.59%] [G loss: 1.824552]\n",
      "epoch:18 step:16966 [D loss: 0.644911, acc: 61.72%] [G loss: 1.888528]\n",
      "epoch:18 step:16967 [D loss: 0.685891, acc: 57.03%] [G loss: 1.916740]\n",
      "epoch:18 step:16968 [D loss: 0.674920, acc: 53.91%] [G loss: 1.886810]\n",
      "epoch:18 step:16969 [D loss: 0.604279, acc: 71.09%] [G loss: 1.887310]\n",
      "epoch:18 step:16970 [D loss: 0.667055, acc: 61.72%] [G loss: 1.971122]\n",
      "epoch:18 step:16971 [D loss: 0.672486, acc: 58.59%] [G loss: 1.883153]\n",
      "epoch:18 step:16972 [D loss: 0.611056, acc: 72.66%] [G loss: 1.881908]\n",
      "epoch:18 step:16973 [D loss: 0.632292, acc: 62.50%] [G loss: 1.981167]\n",
      "epoch:18 step:16974 [D loss: 0.613525, acc: 66.41%] [G loss: 1.726310]\n",
      "epoch:18 step:16975 [D loss: 0.654195, acc: 63.28%] [G loss: 1.920781]\n",
      "epoch:18 step:16976 [D loss: 0.645852, acc: 63.28%] [G loss: 1.828658]\n",
      "epoch:18 step:16977 [D loss: 0.626705, acc: 65.62%] [G loss: 1.917803]\n",
      "epoch:18 step:16978 [D loss: 0.606386, acc: 72.66%] [G loss: 1.988967]\n",
      "epoch:18 step:16979 [D loss: 0.633225, acc: 63.28%] [G loss: 2.017777]\n",
      "epoch:18 step:16980 [D loss: 0.635306, acc: 67.19%] [G loss: 1.902009]\n",
      "epoch:18 step:16981 [D loss: 0.649763, acc: 61.72%] [G loss: 2.133328]\n",
      "epoch:18 step:16982 [D loss: 0.602053, acc: 70.31%] [G loss: 2.005070]\n",
      "epoch:18 step:16983 [D loss: 0.584858, acc: 66.41%] [G loss: 2.081838]\n",
      "epoch:18 step:16984 [D loss: 0.658362, acc: 59.38%] [G loss: 1.957079]\n",
      "epoch:18 step:16985 [D loss: 0.605893, acc: 64.84%] [G loss: 2.006154]\n",
      "epoch:18 step:16986 [D loss: 0.663874, acc: 61.72%] [G loss: 1.811827]\n",
      "epoch:18 step:16987 [D loss: 0.602866, acc: 65.62%] [G loss: 2.069040]\n",
      "epoch:18 step:16988 [D loss: 0.629957, acc: 63.28%] [G loss: 2.019820]\n",
      "epoch:18 step:16989 [D loss: 0.622394, acc: 67.19%] [G loss: 1.919203]\n",
      "epoch:18 step:16990 [D loss: 0.703465, acc: 58.59%] [G loss: 1.793025]\n",
      "epoch:18 step:16991 [D loss: 0.688238, acc: 56.25%] [G loss: 1.684453]\n",
      "epoch:18 step:16992 [D loss: 0.618968, acc: 64.84%] [G loss: 2.009490]\n",
      "epoch:18 step:16993 [D loss: 0.651146, acc: 62.50%] [G loss: 1.900466]\n",
      "epoch:18 step:16994 [D loss: 0.648978, acc: 65.62%] [G loss: 1.819178]\n",
      "epoch:18 step:16995 [D loss: 0.686273, acc: 57.81%] [G loss: 1.957842]\n",
      "epoch:18 step:16996 [D loss: 0.616065, acc: 69.53%] [G loss: 1.960184]\n",
      "epoch:18 step:16997 [D loss: 0.603898, acc: 64.84%] [G loss: 1.949024]\n",
      "epoch:18 step:16998 [D loss: 0.658567, acc: 57.81%] [G loss: 1.891979]\n",
      "epoch:18 step:16999 [D loss: 0.670765, acc: 59.38%] [G loss: 1.877750]\n",
      "epoch:18 step:17000 [D loss: 0.676744, acc: 61.72%] [G loss: 1.770962]\n",
      "epoch:18 step:17001 [D loss: 0.633772, acc: 66.41%] [G loss: 1.846093]\n",
      "epoch:18 step:17002 [D loss: 0.682838, acc: 56.25%] [G loss: 1.814454]\n",
      "epoch:18 step:17003 [D loss: 0.680523, acc: 54.69%] [G loss: 1.836062]\n",
      "epoch:18 step:17004 [D loss: 0.617406, acc: 67.19%] [G loss: 1.918697]\n",
      "epoch:18 step:17005 [D loss: 0.670453, acc: 60.16%] [G loss: 1.845155]\n",
      "epoch:18 step:17006 [D loss: 0.644015, acc: 60.16%] [G loss: 1.807853]\n",
      "epoch:18 step:17007 [D loss: 0.698991, acc: 58.59%] [G loss: 1.789944]\n",
      "epoch:18 step:17008 [D loss: 0.635752, acc: 63.28%] [G loss: 1.936401]\n",
      "epoch:18 step:17009 [D loss: 0.660252, acc: 57.03%] [G loss: 1.785977]\n",
      "epoch:18 step:17010 [D loss: 0.679952, acc: 60.16%] [G loss: 1.944567]\n",
      "epoch:18 step:17011 [D loss: 0.602136, acc: 64.84%] [G loss: 1.837791]\n",
      "epoch:18 step:17012 [D loss: 0.665985, acc: 60.94%] [G loss: 1.971895]\n",
      "epoch:18 step:17013 [D loss: 0.678208, acc: 56.25%] [G loss: 1.879668]\n",
      "epoch:18 step:17014 [D loss: 0.689028, acc: 54.69%] [G loss: 1.691555]\n",
      "epoch:18 step:17015 [D loss: 0.604519, acc: 64.84%] [G loss: 1.893848]\n",
      "epoch:18 step:17016 [D loss: 0.573076, acc: 74.22%] [G loss: 1.922546]\n",
      "epoch:18 step:17017 [D loss: 0.660882, acc: 55.47%] [G loss: 2.113724]\n",
      "epoch:18 step:17018 [D loss: 0.648007, acc: 67.97%] [G loss: 2.003080]\n",
      "epoch:18 step:17019 [D loss: 0.677090, acc: 60.94%] [G loss: 1.859338]\n",
      "epoch:18 step:17020 [D loss: 0.655391, acc: 60.94%] [G loss: 1.912121]\n",
      "epoch:18 step:17021 [D loss: 0.667753, acc: 63.28%] [G loss: 1.849465]\n",
      "epoch:18 step:17022 [D loss: 0.609477, acc: 65.62%] [G loss: 1.892535]\n",
      "epoch:18 step:17023 [D loss: 0.609225, acc: 70.31%] [G loss: 1.736002]\n",
      "epoch:18 step:17024 [D loss: 0.650148, acc: 60.94%] [G loss: 1.885324]\n",
      "epoch:18 step:17025 [D loss: 0.654004, acc: 62.50%] [G loss: 1.731618]\n",
      "epoch:18 step:17026 [D loss: 0.700707, acc: 52.34%] [G loss: 1.815497]\n",
      "epoch:18 step:17027 [D loss: 0.650351, acc: 63.28%] [G loss: 1.831849]\n",
      "epoch:18 step:17028 [D loss: 0.617348, acc: 62.50%] [G loss: 1.800277]\n",
      "epoch:18 step:17029 [D loss: 0.628354, acc: 64.84%] [G loss: 1.911825]\n",
      "epoch:18 step:17030 [D loss: 0.673110, acc: 64.84%] [G loss: 1.779231]\n",
      "epoch:18 step:17031 [D loss: 0.602718, acc: 62.50%] [G loss: 1.817744]\n",
      "epoch:18 step:17032 [D loss: 0.655321, acc: 60.16%] [G loss: 1.870585]\n",
      "epoch:18 step:17033 [D loss: 0.617828, acc: 64.84%] [G loss: 1.880278]\n",
      "epoch:18 step:17034 [D loss: 0.664623, acc: 64.06%] [G loss: 1.968609]\n",
      "epoch:18 step:17035 [D loss: 0.665196, acc: 59.38%] [G loss: 1.890415]\n",
      "epoch:18 step:17036 [D loss: 0.656678, acc: 64.06%] [G loss: 1.843238]\n",
      "epoch:18 step:17037 [D loss: 0.684154, acc: 57.81%] [G loss: 1.864512]\n",
      "epoch:18 step:17038 [D loss: 0.701895, acc: 51.56%] [G loss: 1.785925]\n",
      "epoch:18 step:17039 [D loss: 0.635688, acc: 67.97%] [G loss: 1.814993]\n",
      "epoch:18 step:17040 [D loss: 0.691112, acc: 57.81%] [G loss: 1.735893]\n",
      "epoch:18 step:17041 [D loss: 0.648978, acc: 60.94%] [G loss: 1.787794]\n",
      "epoch:18 step:17042 [D loss: 0.643527, acc: 60.94%] [G loss: 1.711575]\n",
      "epoch:18 step:17043 [D loss: 0.661757, acc: 61.72%] [G loss: 1.814491]\n",
      "epoch:18 step:17044 [D loss: 0.601325, acc: 66.41%] [G loss: 1.893650]\n",
      "epoch:18 step:17045 [D loss: 0.654833, acc: 64.84%] [G loss: 1.777754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17046 [D loss: 0.644230, acc: 64.06%] [G loss: 1.909787]\n",
      "epoch:18 step:17047 [D loss: 0.667766, acc: 59.38%] [G loss: 1.744921]\n",
      "epoch:18 step:17048 [D loss: 0.661192, acc: 69.53%] [G loss: 1.786590]\n",
      "epoch:18 step:17049 [D loss: 0.667374, acc: 61.72%] [G loss: 1.843116]\n",
      "epoch:18 step:17050 [D loss: 0.666279, acc: 63.28%] [G loss: 1.915494]\n",
      "epoch:18 step:17051 [D loss: 0.647220, acc: 60.94%] [G loss: 1.881769]\n",
      "epoch:18 step:17052 [D loss: 0.645683, acc: 60.16%] [G loss: 1.937551]\n",
      "epoch:18 step:17053 [D loss: 0.695234, acc: 55.47%] [G loss: 1.865489]\n",
      "epoch:18 step:17054 [D loss: 0.651343, acc: 62.50%] [G loss: 1.887726]\n",
      "epoch:18 step:17055 [D loss: 0.695700, acc: 60.16%] [G loss: 1.734328]\n",
      "epoch:18 step:17056 [D loss: 0.635970, acc: 65.62%] [G loss: 1.897964]\n",
      "epoch:18 step:17057 [D loss: 0.667203, acc: 58.59%] [G loss: 1.798228]\n",
      "epoch:18 step:17058 [D loss: 0.633345, acc: 65.62%] [G loss: 1.898566]\n",
      "epoch:18 step:17059 [D loss: 0.603109, acc: 67.97%] [G loss: 1.857887]\n",
      "epoch:18 step:17060 [D loss: 0.605374, acc: 66.41%] [G loss: 1.931811]\n",
      "epoch:18 step:17061 [D loss: 0.599859, acc: 66.41%] [G loss: 1.916753]\n",
      "epoch:18 step:17062 [D loss: 0.677731, acc: 57.81%] [G loss: 1.846286]\n",
      "epoch:18 step:17063 [D loss: 0.636786, acc: 60.16%] [G loss: 1.942420]\n",
      "epoch:18 step:17064 [D loss: 0.606460, acc: 70.31%] [G loss: 1.892896]\n",
      "epoch:18 step:17065 [D loss: 0.659607, acc: 60.16%] [G loss: 1.911893]\n",
      "epoch:18 step:17066 [D loss: 0.668552, acc: 61.72%] [G loss: 1.829692]\n",
      "epoch:18 step:17067 [D loss: 0.661407, acc: 60.94%] [G loss: 1.899253]\n",
      "epoch:18 step:17068 [D loss: 0.656764, acc: 59.38%] [G loss: 2.011590]\n",
      "epoch:18 step:17069 [D loss: 0.670360, acc: 57.03%] [G loss: 1.930607]\n",
      "epoch:18 step:17070 [D loss: 0.673598, acc: 59.38%] [G loss: 1.887641]\n",
      "epoch:18 step:17071 [D loss: 0.639272, acc: 67.97%] [G loss: 1.716764]\n",
      "epoch:18 step:17072 [D loss: 0.569508, acc: 70.31%] [G loss: 1.933511]\n",
      "epoch:18 step:17073 [D loss: 0.600051, acc: 67.19%] [G loss: 2.008458]\n",
      "epoch:18 step:17074 [D loss: 0.624493, acc: 64.06%] [G loss: 2.248186]\n",
      "epoch:18 step:17075 [D loss: 0.593893, acc: 68.75%] [G loss: 2.105083]\n",
      "epoch:18 step:17076 [D loss: 0.642590, acc: 60.94%] [G loss: 1.821981]\n",
      "epoch:18 step:17077 [D loss: 0.658568, acc: 57.03%] [G loss: 1.774101]\n",
      "epoch:18 step:17078 [D loss: 0.710482, acc: 52.34%] [G loss: 1.847007]\n",
      "epoch:18 step:17079 [D loss: 0.633919, acc: 61.72%] [G loss: 1.890473]\n",
      "epoch:18 step:17080 [D loss: 0.656520, acc: 60.16%] [G loss: 1.730766]\n",
      "epoch:18 step:17081 [D loss: 0.638463, acc: 63.28%] [G loss: 2.058505]\n",
      "epoch:18 step:17082 [D loss: 0.609345, acc: 62.50%] [G loss: 1.916024]\n",
      "epoch:18 step:17083 [D loss: 0.605199, acc: 66.41%] [G loss: 2.115932]\n",
      "epoch:18 step:17084 [D loss: 0.582363, acc: 77.34%] [G loss: 2.093444]\n",
      "epoch:18 step:17085 [D loss: 0.585689, acc: 71.09%] [G loss: 2.225603]\n",
      "epoch:18 step:17086 [D loss: 0.787294, acc: 50.78%] [G loss: 1.726934]\n",
      "epoch:18 step:17087 [D loss: 0.654684, acc: 59.38%] [G loss: 1.954330]\n",
      "epoch:18 step:17088 [D loss: 0.620878, acc: 60.16%] [G loss: 1.933607]\n",
      "epoch:18 step:17089 [D loss: 0.667243, acc: 57.81%] [G loss: 1.830174]\n",
      "epoch:18 step:17090 [D loss: 0.739827, acc: 52.34%] [G loss: 1.870595]\n",
      "epoch:18 step:17091 [D loss: 0.696055, acc: 58.59%] [G loss: 1.928951]\n",
      "epoch:18 step:17092 [D loss: 0.634685, acc: 62.50%] [G loss: 1.774345]\n",
      "epoch:18 step:17093 [D loss: 0.665104, acc: 59.38%] [G loss: 1.792695]\n",
      "epoch:18 step:17094 [D loss: 0.636541, acc: 64.84%] [G loss: 1.892208]\n",
      "epoch:18 step:17095 [D loss: 0.567640, acc: 71.88%] [G loss: 2.091990]\n",
      "epoch:18 step:17096 [D loss: 0.597801, acc: 67.97%] [G loss: 2.110984]\n",
      "epoch:18 step:17097 [D loss: 0.572319, acc: 73.44%] [G loss: 2.165574]\n",
      "epoch:18 step:17098 [D loss: 0.592921, acc: 67.97%] [G loss: 2.243692]\n",
      "epoch:18 step:17099 [D loss: 0.672600, acc: 64.06%] [G loss: 1.986141]\n",
      "epoch:18 step:17100 [D loss: 0.651576, acc: 61.72%] [G loss: 1.836428]\n",
      "epoch:18 step:17101 [D loss: 0.676502, acc: 54.69%] [G loss: 1.861772]\n",
      "epoch:18 step:17102 [D loss: 0.609021, acc: 67.97%] [G loss: 1.953730]\n",
      "epoch:18 step:17103 [D loss: 0.613708, acc: 65.62%] [G loss: 1.926858]\n",
      "epoch:18 step:17104 [D loss: 0.632254, acc: 62.50%] [G loss: 2.026809]\n",
      "epoch:18 step:17105 [D loss: 0.697469, acc: 57.03%] [G loss: 1.951406]\n",
      "epoch:18 step:17106 [D loss: 0.688156, acc: 64.06%] [G loss: 1.857026]\n",
      "epoch:18 step:17107 [D loss: 0.660615, acc: 58.59%] [G loss: 1.868389]\n",
      "epoch:18 step:17108 [D loss: 0.650646, acc: 64.06%] [G loss: 1.993941]\n",
      "epoch:18 step:17109 [D loss: 0.668008, acc: 57.81%] [G loss: 1.956574]\n",
      "epoch:18 step:17110 [D loss: 0.618011, acc: 64.06%] [G loss: 1.857929]\n",
      "epoch:18 step:17111 [D loss: 0.649470, acc: 66.41%] [G loss: 1.982136]\n",
      "epoch:18 step:17112 [D loss: 0.627858, acc: 64.84%] [G loss: 1.944007]\n",
      "epoch:18 step:17113 [D loss: 0.671427, acc: 61.72%] [G loss: 2.051515]\n",
      "epoch:18 step:17114 [D loss: 0.621557, acc: 64.84%] [G loss: 2.097604]\n",
      "epoch:18 step:17115 [D loss: 0.733068, acc: 52.34%] [G loss: 1.895842]\n",
      "epoch:18 step:17116 [D loss: 0.704552, acc: 50.78%] [G loss: 1.729920]\n",
      "epoch:18 step:17117 [D loss: 0.666192, acc: 61.72%] [G loss: 1.666699]\n",
      "epoch:18 step:17118 [D loss: 0.651619, acc: 62.50%] [G loss: 1.827464]\n",
      "epoch:18 step:17119 [D loss: 0.671809, acc: 60.16%] [G loss: 1.923514]\n",
      "epoch:18 step:17120 [D loss: 0.641543, acc: 64.06%] [G loss: 1.945291]\n",
      "epoch:18 step:17121 [D loss: 0.659774, acc: 64.84%] [G loss: 1.908838]\n",
      "epoch:18 step:17122 [D loss: 0.645506, acc: 62.50%] [G loss: 1.826071]\n",
      "epoch:18 step:17123 [D loss: 0.657622, acc: 60.16%] [G loss: 2.006732]\n",
      "epoch:18 step:17124 [D loss: 0.683644, acc: 57.03%] [G loss: 1.723710]\n",
      "epoch:18 step:17125 [D loss: 0.685577, acc: 53.12%] [G loss: 1.958535]\n",
      "epoch:18 step:17126 [D loss: 0.654464, acc: 59.38%] [G loss: 1.963987]\n",
      "epoch:18 step:17127 [D loss: 0.671074, acc: 56.25%] [G loss: 2.047741]\n",
      "epoch:18 step:17128 [D loss: 0.644890, acc: 64.06%] [G loss: 1.796303]\n",
      "epoch:18 step:17129 [D loss: 0.676676, acc: 55.47%] [G loss: 1.863173]\n",
      "epoch:18 step:17130 [D loss: 0.604576, acc: 67.19%] [G loss: 1.983349]\n",
      "epoch:18 step:17131 [D loss: 0.675431, acc: 62.50%] [G loss: 1.774579]\n",
      "epoch:18 step:17132 [D loss: 0.647424, acc: 60.16%] [G loss: 1.948162]\n",
      "epoch:18 step:17133 [D loss: 0.665791, acc: 60.16%] [G loss: 1.888554]\n",
      "epoch:18 step:17134 [D loss: 0.720054, acc: 48.44%] [G loss: 1.862007]\n",
      "epoch:18 step:17135 [D loss: 0.666099, acc: 57.03%] [G loss: 1.955264]\n",
      "epoch:18 step:17136 [D loss: 0.604783, acc: 68.75%] [G loss: 1.946602]\n",
      "epoch:18 step:17137 [D loss: 0.625703, acc: 63.28%] [G loss: 1.767443]\n",
      "epoch:18 step:17138 [D loss: 0.651195, acc: 61.72%] [G loss: 1.822681]\n",
      "epoch:18 step:17139 [D loss: 0.639632, acc: 68.75%] [G loss: 1.807157]\n",
      "epoch:18 step:17140 [D loss: 0.579022, acc: 73.44%] [G loss: 2.069720]\n",
      "epoch:18 step:17141 [D loss: 0.644921, acc: 67.19%] [G loss: 2.036060]\n",
      "epoch:18 step:17142 [D loss: 0.553884, acc: 71.88%] [G loss: 2.070185]\n",
      "epoch:18 step:17143 [D loss: 0.636992, acc: 63.28%] [G loss: 1.916361]\n",
      "epoch:18 step:17144 [D loss: 0.698177, acc: 56.25%] [G loss: 1.891337]\n",
      "epoch:18 step:17145 [D loss: 0.631480, acc: 61.72%] [G loss: 1.994608]\n",
      "epoch:18 step:17146 [D loss: 0.609800, acc: 62.50%] [G loss: 1.879848]\n",
      "epoch:18 step:17147 [D loss: 0.679307, acc: 55.47%] [G loss: 1.899909]\n",
      "epoch:18 step:17148 [D loss: 0.669582, acc: 57.81%] [G loss: 1.862573]\n",
      "epoch:18 step:17149 [D loss: 0.615637, acc: 66.41%] [G loss: 1.857922]\n",
      "epoch:18 step:17150 [D loss: 0.605891, acc: 65.62%] [G loss: 1.936720]\n",
      "epoch:18 step:17151 [D loss: 0.629746, acc: 64.84%] [G loss: 1.974427]\n",
      "epoch:18 step:17152 [D loss: 0.627624, acc: 67.19%] [G loss: 1.818385]\n",
      "epoch:18 step:17153 [D loss: 0.622682, acc: 64.06%] [G loss: 1.973668]\n",
      "epoch:18 step:17154 [D loss: 0.660619, acc: 56.25%] [G loss: 2.021894]\n",
      "epoch:18 step:17155 [D loss: 0.666607, acc: 63.28%] [G loss: 1.836260]\n",
      "epoch:18 step:17156 [D loss: 0.675276, acc: 57.81%] [G loss: 1.870398]\n",
      "epoch:18 step:17157 [D loss: 0.642708, acc: 66.41%] [G loss: 1.854037]\n",
      "epoch:18 step:17158 [D loss: 0.639268, acc: 63.28%] [G loss: 1.893280]\n",
      "epoch:18 step:17159 [D loss: 0.647185, acc: 61.72%] [G loss: 2.013331]\n",
      "epoch:18 step:17160 [D loss: 0.665945, acc: 60.94%] [G loss: 1.875099]\n",
      "epoch:18 step:17161 [D loss: 0.672558, acc: 57.03%] [G loss: 1.775978]\n",
      "epoch:18 step:17162 [D loss: 0.619475, acc: 64.06%] [G loss: 2.116565]\n",
      "epoch:18 step:17163 [D loss: 0.636247, acc: 66.41%] [G loss: 1.888765]\n",
      "epoch:18 step:17164 [D loss: 0.648409, acc: 56.25%] [G loss: 1.988379]\n",
      "epoch:18 step:17165 [D loss: 0.620409, acc: 64.84%] [G loss: 2.058822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17166 [D loss: 0.601426, acc: 72.66%] [G loss: 1.972344]\n",
      "epoch:18 step:17167 [D loss: 0.661498, acc: 60.16%] [G loss: 1.746376]\n",
      "epoch:18 step:17168 [D loss: 0.636629, acc: 64.84%] [G loss: 1.819098]\n",
      "epoch:18 step:17169 [D loss: 0.648194, acc: 58.59%] [G loss: 1.781354]\n",
      "epoch:18 step:17170 [D loss: 0.632339, acc: 60.94%] [G loss: 1.797884]\n",
      "epoch:18 step:17171 [D loss: 0.636434, acc: 64.06%] [G loss: 1.853066]\n",
      "epoch:18 step:17172 [D loss: 0.667201, acc: 64.84%] [G loss: 1.757408]\n",
      "epoch:18 step:17173 [D loss: 0.679835, acc: 59.38%] [G loss: 1.853948]\n",
      "epoch:18 step:17174 [D loss: 0.679190, acc: 59.38%] [G loss: 1.771286]\n",
      "epoch:18 step:17175 [D loss: 0.608188, acc: 67.97%] [G loss: 1.939235]\n",
      "epoch:18 step:17176 [D loss: 0.624751, acc: 66.41%] [G loss: 1.943247]\n",
      "epoch:18 step:17177 [D loss: 0.613219, acc: 67.97%] [G loss: 1.912392]\n",
      "epoch:18 step:17178 [D loss: 0.626169, acc: 64.84%] [G loss: 2.363148]\n",
      "epoch:18 step:17179 [D loss: 0.633166, acc: 64.84%] [G loss: 2.062385]\n",
      "epoch:18 step:17180 [D loss: 0.548959, acc: 73.44%] [G loss: 2.167066]\n",
      "epoch:18 step:17181 [D loss: 0.592622, acc: 75.00%] [G loss: 2.213392]\n",
      "epoch:18 step:17182 [D loss: 0.688519, acc: 57.81%] [G loss: 1.786015]\n",
      "epoch:18 step:17183 [D loss: 0.675467, acc: 62.50%] [G loss: 1.828323]\n",
      "epoch:18 step:17184 [D loss: 0.648160, acc: 62.50%] [G loss: 2.052655]\n",
      "epoch:18 step:17185 [D loss: 0.613443, acc: 71.88%] [G loss: 1.798092]\n",
      "epoch:18 step:17186 [D loss: 0.677078, acc: 60.16%] [G loss: 1.884043]\n",
      "epoch:18 step:17187 [D loss: 0.606683, acc: 64.84%] [G loss: 2.201190]\n",
      "epoch:18 step:17188 [D loss: 0.655572, acc: 56.25%] [G loss: 1.841804]\n",
      "epoch:18 step:17189 [D loss: 0.681733, acc: 61.72%] [G loss: 1.850391]\n",
      "epoch:18 step:17190 [D loss: 0.618215, acc: 62.50%] [G loss: 1.836658]\n",
      "epoch:18 step:17191 [D loss: 0.658147, acc: 60.94%] [G loss: 1.795122]\n",
      "epoch:18 step:17192 [D loss: 0.594606, acc: 69.53%] [G loss: 1.955791]\n",
      "epoch:18 step:17193 [D loss: 0.670364, acc: 57.81%] [G loss: 1.772730]\n",
      "epoch:18 step:17194 [D loss: 0.623955, acc: 64.06%] [G loss: 1.950882]\n",
      "epoch:18 step:17195 [D loss: 0.688191, acc: 56.25%] [G loss: 1.973691]\n",
      "epoch:18 step:17196 [D loss: 0.621407, acc: 71.09%] [G loss: 1.920871]\n",
      "epoch:18 step:17197 [D loss: 0.601376, acc: 67.19%] [G loss: 1.989459]\n",
      "epoch:18 step:17198 [D loss: 0.631937, acc: 66.41%] [G loss: 1.919426]\n",
      "epoch:18 step:17199 [D loss: 0.611651, acc: 68.75%] [G loss: 1.908065]\n",
      "epoch:18 step:17200 [D loss: 0.654512, acc: 62.50%] [G loss: 1.915924]\n",
      "epoch:18 step:17201 [D loss: 0.650740, acc: 57.81%] [G loss: 1.915337]\n",
      "epoch:18 step:17202 [D loss: 0.623091, acc: 60.94%] [G loss: 2.041520]\n",
      "epoch:18 step:17203 [D loss: 0.619833, acc: 66.41%] [G loss: 2.052865]\n",
      "epoch:18 step:17204 [D loss: 0.605532, acc: 68.75%] [G loss: 1.916360]\n",
      "epoch:18 step:17205 [D loss: 0.699232, acc: 58.59%] [G loss: 1.931869]\n",
      "epoch:18 step:17206 [D loss: 0.626026, acc: 64.06%] [G loss: 2.216273]\n",
      "epoch:18 step:17207 [D loss: 0.660519, acc: 60.16%] [G loss: 1.833506]\n",
      "epoch:18 step:17208 [D loss: 0.715326, acc: 49.22%] [G loss: 1.830570]\n",
      "epoch:18 step:17209 [D loss: 0.649350, acc: 59.38%] [G loss: 1.864301]\n",
      "epoch:18 step:17210 [D loss: 0.646188, acc: 64.06%] [G loss: 1.814649]\n",
      "epoch:18 step:17211 [D loss: 0.619454, acc: 61.72%] [G loss: 2.174064]\n",
      "epoch:18 step:17212 [D loss: 0.593743, acc: 71.09%] [G loss: 2.144052]\n",
      "epoch:18 step:17213 [D loss: 0.565175, acc: 75.78%] [G loss: 2.288149]\n",
      "epoch:18 step:17214 [D loss: 0.649899, acc: 56.25%] [G loss: 1.958734]\n",
      "epoch:18 step:17215 [D loss: 0.722873, acc: 55.47%] [G loss: 1.586327]\n",
      "epoch:18 step:17216 [D loss: 0.643496, acc: 63.28%] [G loss: 1.762487]\n",
      "epoch:18 step:17217 [D loss: 0.658102, acc: 58.59%] [G loss: 1.771228]\n",
      "epoch:18 step:17218 [D loss: 0.671470, acc: 60.94%] [G loss: 1.795276]\n",
      "epoch:18 step:17219 [D loss: 0.749679, acc: 58.59%] [G loss: 1.875287]\n",
      "epoch:18 step:17220 [D loss: 0.587637, acc: 68.75%] [G loss: 1.919329]\n",
      "epoch:18 step:17221 [D loss: 0.708288, acc: 55.47%] [G loss: 1.720612]\n",
      "epoch:18 step:17222 [D loss: 0.629595, acc: 63.28%] [G loss: 1.735843]\n",
      "epoch:18 step:17223 [D loss: 0.659001, acc: 57.81%] [G loss: 1.906599]\n",
      "epoch:18 step:17224 [D loss: 0.718954, acc: 60.16%] [G loss: 1.857099]\n",
      "epoch:18 step:17225 [D loss: 0.619487, acc: 69.53%] [G loss: 1.946270]\n",
      "epoch:18 step:17226 [D loss: 0.590675, acc: 67.97%] [G loss: 1.917020]\n",
      "epoch:18 step:17227 [D loss: 0.687199, acc: 60.94%] [G loss: 1.798650]\n",
      "epoch:18 step:17228 [D loss: 0.640608, acc: 65.62%] [G loss: 1.874776]\n",
      "epoch:18 step:17229 [D loss: 0.666582, acc: 58.59%] [G loss: 1.865324]\n",
      "epoch:18 step:17230 [D loss: 0.650357, acc: 60.94%] [G loss: 1.871268]\n",
      "epoch:18 step:17231 [D loss: 0.660257, acc: 63.28%] [G loss: 1.789540]\n",
      "epoch:18 step:17232 [D loss: 0.653483, acc: 59.38%] [G loss: 1.875607]\n",
      "epoch:18 step:17233 [D loss: 0.661120, acc: 56.25%] [G loss: 1.973567]\n",
      "epoch:18 step:17234 [D loss: 0.620599, acc: 67.19%] [G loss: 1.862041]\n",
      "epoch:18 step:17235 [D loss: 0.628741, acc: 67.19%] [G loss: 1.945235]\n",
      "epoch:18 step:17236 [D loss: 0.613899, acc: 64.06%] [G loss: 2.064626]\n",
      "epoch:18 step:17237 [D loss: 0.655003, acc: 67.19%] [G loss: 2.136056]\n",
      "epoch:18 step:17238 [D loss: 0.676515, acc: 57.81%] [G loss: 1.875302]\n",
      "epoch:18 step:17239 [D loss: 0.689980, acc: 52.34%] [G loss: 1.714135]\n",
      "epoch:18 step:17240 [D loss: 0.620224, acc: 69.53%] [G loss: 2.083245]\n",
      "epoch:18 step:17241 [D loss: 0.693288, acc: 48.44%] [G loss: 1.769860]\n",
      "epoch:18 step:17242 [D loss: 0.687305, acc: 50.78%] [G loss: 1.685205]\n",
      "epoch:18 step:17243 [D loss: 0.670688, acc: 57.81%] [G loss: 1.720993]\n",
      "epoch:18 step:17244 [D loss: 0.670699, acc: 55.47%] [G loss: 1.821099]\n",
      "epoch:18 step:17245 [D loss: 0.661356, acc: 62.50%] [G loss: 1.810765]\n",
      "epoch:18 step:17246 [D loss: 0.628191, acc: 64.84%] [G loss: 1.891804]\n",
      "epoch:18 step:17247 [D loss: 0.629537, acc: 65.62%] [G loss: 2.024395]\n",
      "epoch:18 step:17248 [D loss: 0.660913, acc: 60.16%] [G loss: 1.793755]\n",
      "epoch:18 step:17249 [D loss: 0.719700, acc: 54.69%] [G loss: 1.914933]\n",
      "epoch:18 step:17250 [D loss: 0.616768, acc: 64.84%] [G loss: 1.776092]\n",
      "epoch:18 step:17251 [D loss: 0.630924, acc: 67.19%] [G loss: 1.847777]\n",
      "epoch:18 step:17252 [D loss: 0.635187, acc: 64.06%] [G loss: 1.867756]\n",
      "epoch:18 step:17253 [D loss: 0.706584, acc: 57.03%] [G loss: 1.706028]\n",
      "epoch:18 step:17254 [D loss: 0.650499, acc: 55.47%] [G loss: 1.813570]\n",
      "epoch:18 step:17255 [D loss: 0.640043, acc: 69.53%] [G loss: 1.905878]\n",
      "epoch:18 step:17256 [D loss: 0.644128, acc: 63.28%] [G loss: 1.869932]\n",
      "epoch:18 step:17257 [D loss: 0.657086, acc: 56.25%] [G loss: 1.785699]\n",
      "epoch:18 step:17258 [D loss: 0.632253, acc: 65.62%] [G loss: 1.851515]\n",
      "epoch:18 step:17259 [D loss: 0.618321, acc: 67.19%] [G loss: 1.864867]\n",
      "epoch:18 step:17260 [D loss: 0.645531, acc: 63.28%] [G loss: 1.792755]\n",
      "epoch:18 step:17261 [D loss: 0.645596, acc: 57.81%] [G loss: 1.846766]\n",
      "epoch:18 step:17262 [D loss: 0.692738, acc: 53.12%] [G loss: 1.733783]\n",
      "epoch:18 step:17263 [D loss: 0.647612, acc: 58.59%] [G loss: 1.720623]\n",
      "epoch:18 step:17264 [D loss: 0.629158, acc: 62.50%] [G loss: 1.857434]\n",
      "epoch:18 step:17265 [D loss: 0.630345, acc: 64.84%] [G loss: 1.849517]\n",
      "epoch:18 step:17266 [D loss: 0.665311, acc: 57.03%] [G loss: 1.929978]\n",
      "epoch:18 step:17267 [D loss: 0.624400, acc: 69.53%] [G loss: 1.871926]\n",
      "epoch:18 step:17268 [D loss: 0.651280, acc: 64.06%] [G loss: 1.900231]\n",
      "epoch:18 step:17269 [D loss: 0.614848, acc: 67.97%] [G loss: 1.981763]\n",
      "epoch:18 step:17270 [D loss: 0.650702, acc: 59.38%] [G loss: 2.061563]\n",
      "epoch:18 step:17271 [D loss: 0.619514, acc: 67.97%] [G loss: 2.004577]\n",
      "epoch:18 step:17272 [D loss: 0.570857, acc: 71.09%] [G loss: 2.228111]\n",
      "epoch:18 step:17273 [D loss: 0.696325, acc: 62.50%] [G loss: 1.938569]\n",
      "epoch:18 step:17274 [D loss: 0.649567, acc: 68.75%] [G loss: 1.938361]\n",
      "epoch:18 step:17275 [D loss: 0.696220, acc: 57.81%] [G loss: 1.911453]\n",
      "epoch:18 step:17276 [D loss: 0.639243, acc: 63.28%] [G loss: 1.873120]\n",
      "epoch:18 step:17277 [D loss: 0.676269, acc: 55.47%] [G loss: 1.818070]\n",
      "epoch:18 step:17278 [D loss: 0.616241, acc: 64.84%] [G loss: 1.877233]\n",
      "epoch:18 step:17279 [D loss: 0.675096, acc: 60.94%] [G loss: 2.015765]\n",
      "epoch:18 step:17280 [D loss: 0.638342, acc: 67.19%] [G loss: 1.914526]\n",
      "epoch:18 step:17281 [D loss: 0.669348, acc: 54.69%] [G loss: 2.102869]\n",
      "epoch:18 step:17282 [D loss: 0.609832, acc: 67.97%] [G loss: 2.138826]\n",
      "epoch:18 step:17283 [D loss: 0.697778, acc: 58.59%] [G loss: 1.883381]\n",
      "epoch:18 step:17284 [D loss: 0.657204, acc: 63.28%] [G loss: 1.784844]\n",
      "epoch:18 step:17285 [D loss: 0.640728, acc: 64.84%] [G loss: 1.939788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17286 [D loss: 0.664085, acc: 56.25%] [G loss: 1.789404]\n",
      "epoch:18 step:17287 [D loss: 0.629618, acc: 64.84%] [G loss: 1.746116]\n",
      "epoch:18 step:17288 [D loss: 0.658387, acc: 61.72%] [G loss: 1.818050]\n",
      "epoch:18 step:17289 [D loss: 0.680417, acc: 58.59%] [G loss: 1.831154]\n",
      "epoch:18 step:17290 [D loss: 0.692788, acc: 57.03%] [G loss: 1.709018]\n",
      "epoch:18 step:17291 [D loss: 0.684119, acc: 57.81%] [G loss: 1.808783]\n",
      "epoch:18 step:17292 [D loss: 0.638322, acc: 67.97%] [G loss: 2.003928]\n",
      "epoch:18 step:17293 [D loss: 0.634357, acc: 60.94%] [G loss: 1.970675]\n",
      "epoch:18 step:17294 [D loss: 0.646168, acc: 67.19%] [G loss: 1.935367]\n",
      "epoch:18 step:17295 [D loss: 0.598482, acc: 67.19%] [G loss: 1.975157]\n",
      "epoch:18 step:17296 [D loss: 0.640799, acc: 56.25%] [G loss: 1.971585]\n",
      "epoch:18 step:17297 [D loss: 0.642075, acc: 66.41%] [G loss: 2.036838]\n",
      "epoch:18 step:17298 [D loss: 0.701545, acc: 62.50%] [G loss: 1.818130]\n",
      "epoch:18 step:17299 [D loss: 0.693912, acc: 56.25%] [G loss: 1.721693]\n",
      "epoch:18 step:17300 [D loss: 0.615087, acc: 65.62%] [G loss: 1.824193]\n",
      "epoch:18 step:17301 [D loss: 0.634006, acc: 61.72%] [G loss: 1.882472]\n",
      "epoch:18 step:17302 [D loss: 0.630875, acc: 64.84%] [G loss: 1.922066]\n",
      "epoch:18 step:17303 [D loss: 0.702204, acc: 53.12%] [G loss: 1.731323]\n",
      "epoch:18 step:17304 [D loss: 0.688414, acc: 52.34%] [G loss: 1.770697]\n",
      "epoch:18 step:17305 [D loss: 0.682661, acc: 56.25%] [G loss: 1.838330]\n",
      "epoch:18 step:17306 [D loss: 0.674764, acc: 58.59%] [G loss: 1.710948]\n",
      "epoch:18 step:17307 [D loss: 0.669912, acc: 61.72%] [G loss: 1.859687]\n",
      "epoch:18 step:17308 [D loss: 0.653519, acc: 64.06%] [G loss: 1.735010]\n",
      "epoch:18 step:17309 [D loss: 0.670438, acc: 60.16%] [G loss: 1.852919]\n",
      "epoch:18 step:17310 [D loss: 0.617160, acc: 65.62%] [G loss: 1.770286]\n",
      "epoch:18 step:17311 [D loss: 0.659110, acc: 61.72%] [G loss: 1.838120]\n",
      "epoch:18 step:17312 [D loss: 0.620939, acc: 65.62%] [G loss: 1.769325]\n",
      "epoch:18 step:17313 [D loss: 0.645633, acc: 60.94%] [G loss: 1.765629]\n",
      "epoch:18 step:17314 [D loss: 0.634269, acc: 61.72%] [G loss: 1.850256]\n",
      "epoch:18 step:17315 [D loss: 0.653349, acc: 61.72%] [G loss: 1.746042]\n",
      "epoch:18 step:17316 [D loss: 0.628637, acc: 65.62%] [G loss: 1.731902]\n",
      "epoch:18 step:17317 [D loss: 0.640508, acc: 60.16%] [G loss: 1.864302]\n",
      "epoch:18 step:17318 [D loss: 0.618975, acc: 66.41%] [G loss: 1.859960]\n",
      "epoch:18 step:17319 [D loss: 0.635336, acc: 61.72%] [G loss: 1.854437]\n",
      "epoch:18 step:17320 [D loss: 0.630303, acc: 64.06%] [G loss: 1.880917]\n",
      "epoch:18 step:17321 [D loss: 0.646326, acc: 62.50%] [G loss: 1.907842]\n",
      "epoch:18 step:17322 [D loss: 0.671496, acc: 61.72%] [G loss: 1.837613]\n",
      "epoch:18 step:17323 [D loss: 0.593602, acc: 69.53%] [G loss: 2.000355]\n",
      "epoch:18 step:17324 [D loss: 0.691591, acc: 59.38%] [G loss: 1.821536]\n",
      "epoch:18 step:17325 [D loss: 0.712533, acc: 49.22%] [G loss: 1.745527]\n",
      "epoch:18 step:17326 [D loss: 0.649767, acc: 59.38%] [G loss: 1.792177]\n",
      "epoch:18 step:17327 [D loss: 0.685947, acc: 58.59%] [G loss: 1.827555]\n",
      "epoch:18 step:17328 [D loss: 0.631774, acc: 65.62%] [G loss: 1.921152]\n",
      "epoch:18 step:17329 [D loss: 0.634383, acc: 63.28%] [G loss: 1.796577]\n",
      "epoch:18 step:17330 [D loss: 0.679523, acc: 58.59%] [G loss: 1.685327]\n",
      "epoch:18 step:17331 [D loss: 0.634153, acc: 64.06%] [G loss: 1.893975]\n",
      "epoch:18 step:17332 [D loss: 0.672137, acc: 50.00%] [G loss: 1.715186]\n",
      "epoch:18 step:17333 [D loss: 0.703883, acc: 58.59%] [G loss: 1.804317]\n",
      "epoch:18 step:17334 [D loss: 0.632362, acc: 58.59%] [G loss: 1.921850]\n",
      "epoch:18 step:17335 [D loss: 0.603090, acc: 71.88%] [G loss: 2.075402]\n",
      "epoch:18 step:17336 [D loss: 0.610419, acc: 66.41%] [G loss: 2.112598]\n",
      "epoch:18 step:17337 [D loss: 0.608554, acc: 66.41%] [G loss: 2.205947]\n",
      "epoch:18 step:17338 [D loss: 0.593105, acc: 68.75%] [G loss: 2.182359]\n",
      "epoch:18 step:17339 [D loss: 0.649302, acc: 60.94%] [G loss: 1.792100]\n",
      "epoch:18 step:17340 [D loss: 0.736000, acc: 55.47%] [G loss: 1.818196]\n",
      "epoch:18 step:17341 [D loss: 0.685277, acc: 58.59%] [G loss: 1.853883]\n",
      "epoch:18 step:17342 [D loss: 0.659245, acc: 62.50%] [G loss: 1.811088]\n",
      "epoch:18 step:17343 [D loss: 0.709801, acc: 57.03%] [G loss: 1.824759]\n",
      "epoch:18 step:17344 [D loss: 0.684512, acc: 57.03%] [G loss: 1.760235]\n",
      "epoch:18 step:17345 [D loss: 0.651171, acc: 64.84%] [G loss: 2.024522]\n",
      "epoch:18 step:17346 [D loss: 0.662496, acc: 53.91%] [G loss: 1.843587]\n",
      "epoch:18 step:17347 [D loss: 0.620366, acc: 67.19%] [G loss: 1.910623]\n",
      "epoch:18 step:17348 [D loss: 0.657143, acc: 59.38%] [G loss: 1.773945]\n",
      "epoch:18 step:17349 [D loss: 0.695794, acc: 53.12%] [G loss: 1.742586]\n",
      "epoch:18 step:17350 [D loss: 0.613030, acc: 67.19%] [G loss: 1.968994]\n",
      "epoch:18 step:17351 [D loss: 0.649376, acc: 57.81%] [G loss: 1.749703]\n",
      "epoch:18 step:17352 [D loss: 0.640461, acc: 67.19%] [G loss: 1.840853]\n",
      "epoch:18 step:17353 [D loss: 0.632571, acc: 63.28%] [G loss: 1.953416]\n",
      "epoch:18 step:17354 [D loss: 0.572733, acc: 71.88%] [G loss: 1.879831]\n",
      "epoch:18 step:17355 [D loss: 0.630236, acc: 65.62%] [G loss: 1.958112]\n",
      "epoch:18 step:17356 [D loss: 0.657922, acc: 59.38%] [G loss: 1.907870]\n",
      "epoch:18 step:17357 [D loss: 0.648018, acc: 66.41%] [G loss: 2.003167]\n",
      "epoch:18 step:17358 [D loss: 0.635922, acc: 67.19%] [G loss: 1.732727]\n",
      "epoch:18 step:17359 [D loss: 0.637560, acc: 58.59%] [G loss: 1.821162]\n",
      "epoch:18 step:17360 [D loss: 0.652159, acc: 62.50%] [G loss: 1.948129]\n",
      "epoch:18 step:17361 [D loss: 0.639484, acc: 64.84%] [G loss: 1.973987]\n",
      "epoch:18 step:17362 [D loss: 0.632287, acc: 64.06%] [G loss: 1.874639]\n",
      "epoch:18 step:17363 [D loss: 0.620375, acc: 64.06%] [G loss: 1.812714]\n",
      "epoch:18 step:17364 [D loss: 0.617687, acc: 60.94%] [G loss: 1.991615]\n",
      "epoch:18 step:17365 [D loss: 0.601282, acc: 70.31%] [G loss: 2.130712]\n",
      "epoch:18 step:17366 [D loss: 0.712943, acc: 54.69%] [G loss: 1.802464]\n",
      "epoch:18 step:17367 [D loss: 0.702640, acc: 50.78%] [G loss: 1.804343]\n",
      "epoch:18 step:17368 [D loss: 0.690808, acc: 56.25%] [G loss: 1.747034]\n",
      "epoch:18 step:17369 [D loss: 0.615256, acc: 70.31%] [G loss: 1.912584]\n",
      "epoch:18 step:17370 [D loss: 0.640053, acc: 64.84%] [G loss: 2.004047]\n",
      "epoch:18 step:17371 [D loss: 0.631738, acc: 60.16%] [G loss: 2.016308]\n",
      "epoch:18 step:17372 [D loss: 0.637328, acc: 60.16%] [G loss: 1.745754]\n",
      "epoch:18 step:17373 [D loss: 0.667441, acc: 62.50%] [G loss: 1.985568]\n",
      "epoch:18 step:17374 [D loss: 0.591383, acc: 64.84%] [G loss: 2.131469]\n",
      "epoch:18 step:17375 [D loss: 0.625564, acc: 66.41%] [G loss: 1.971305]\n",
      "epoch:18 step:17376 [D loss: 0.693515, acc: 54.69%] [G loss: 1.712879]\n",
      "epoch:18 step:17377 [D loss: 0.652968, acc: 60.16%] [G loss: 1.765156]\n",
      "epoch:18 step:17378 [D loss: 0.638226, acc: 62.50%] [G loss: 1.835902]\n",
      "epoch:18 step:17379 [D loss: 0.606631, acc: 69.53%] [G loss: 1.771866]\n",
      "epoch:18 step:17380 [D loss: 0.642396, acc: 64.06%] [G loss: 1.907067]\n",
      "epoch:18 step:17381 [D loss: 0.631990, acc: 60.94%] [G loss: 1.835575]\n",
      "epoch:18 step:17382 [D loss: 0.632716, acc: 60.94%] [G loss: 1.852111]\n",
      "epoch:18 step:17383 [D loss: 0.644881, acc: 63.28%] [G loss: 1.904892]\n",
      "epoch:18 step:17384 [D loss: 0.632073, acc: 63.28%] [G loss: 1.788926]\n",
      "epoch:18 step:17385 [D loss: 0.672402, acc: 58.59%] [G loss: 1.965088]\n",
      "epoch:18 step:17386 [D loss: 0.612143, acc: 70.31%] [G loss: 1.997831]\n",
      "epoch:18 step:17387 [D loss: 0.680585, acc: 55.47%] [G loss: 1.874819]\n",
      "epoch:18 step:17388 [D loss: 0.631807, acc: 64.06%] [G loss: 1.894628]\n",
      "epoch:18 step:17389 [D loss: 0.603220, acc: 66.41%] [G loss: 1.922264]\n",
      "epoch:18 step:17390 [D loss: 0.671058, acc: 61.72%] [G loss: 1.833218]\n",
      "epoch:18 step:17391 [D loss: 0.667595, acc: 61.72%] [G loss: 1.761630]\n",
      "epoch:18 step:17392 [D loss: 0.625170, acc: 62.50%] [G loss: 1.880308]\n",
      "epoch:18 step:17393 [D loss: 0.599711, acc: 67.19%] [G loss: 1.893311]\n",
      "epoch:18 step:17394 [D loss: 0.706065, acc: 50.00%] [G loss: 1.782539]\n",
      "epoch:18 step:17395 [D loss: 0.666480, acc: 57.81%] [G loss: 1.818619]\n",
      "epoch:18 step:17396 [D loss: 0.714719, acc: 55.47%] [G loss: 1.811665]\n",
      "epoch:18 step:17397 [D loss: 0.674643, acc: 60.94%] [G loss: 1.698553]\n",
      "epoch:18 step:17398 [D loss: 0.642091, acc: 66.41%] [G loss: 2.035109]\n",
      "epoch:18 step:17399 [D loss: 0.652798, acc: 63.28%] [G loss: 1.937291]\n",
      "epoch:18 step:17400 [D loss: 0.641237, acc: 63.28%] [G loss: 1.970708]\n",
      "epoch:18 step:17401 [D loss: 0.649088, acc: 59.38%] [G loss: 1.908772]\n",
      "epoch:18 step:17402 [D loss: 0.627044, acc: 66.41%] [G loss: 1.706214]\n",
      "epoch:18 step:17403 [D loss: 0.629113, acc: 67.19%] [G loss: 1.883992]\n",
      "epoch:18 step:17404 [D loss: 0.677699, acc: 58.59%] [G loss: 1.761012]\n",
      "epoch:18 step:17405 [D loss: 0.637637, acc: 63.28%] [G loss: 1.879419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17406 [D loss: 0.627392, acc: 62.50%] [G loss: 1.984281]\n",
      "epoch:18 step:17407 [D loss: 0.620360, acc: 64.84%] [G loss: 1.804894]\n",
      "epoch:18 step:17408 [D loss: 0.709915, acc: 50.00%] [G loss: 1.784418]\n",
      "epoch:18 step:17409 [D loss: 0.656186, acc: 60.94%] [G loss: 1.845923]\n",
      "epoch:18 step:17410 [D loss: 0.613438, acc: 68.75%] [G loss: 1.948023]\n",
      "epoch:18 step:17411 [D loss: 0.636703, acc: 57.81%] [G loss: 1.931850]\n",
      "epoch:18 step:17412 [D loss: 0.640353, acc: 64.84%] [G loss: 1.941538]\n",
      "epoch:18 step:17413 [D loss: 0.573998, acc: 70.31%] [G loss: 1.970359]\n",
      "epoch:18 step:17414 [D loss: 0.631678, acc: 68.75%] [G loss: 1.911932]\n",
      "epoch:18 step:17415 [D loss: 0.585331, acc: 70.31%] [G loss: 2.099869]\n",
      "epoch:18 step:17416 [D loss: 0.634876, acc: 61.72%] [G loss: 2.077061]\n",
      "epoch:18 step:17417 [D loss: 0.585088, acc: 68.75%] [G loss: 2.077590]\n",
      "epoch:18 step:17418 [D loss: 0.617670, acc: 64.84%] [G loss: 2.105422]\n",
      "epoch:18 step:17419 [D loss: 0.635192, acc: 64.06%] [G loss: 1.878681]\n",
      "epoch:18 step:17420 [D loss: 0.618047, acc: 67.97%] [G loss: 2.062915]\n",
      "epoch:18 step:17421 [D loss: 0.603029, acc: 67.97%] [G loss: 2.159228]\n",
      "epoch:18 step:17422 [D loss: 0.567949, acc: 70.31%] [G loss: 2.210329]\n",
      "epoch:18 step:17423 [D loss: 0.628896, acc: 66.41%] [G loss: 2.147400]\n",
      "epoch:18 step:17424 [D loss: 0.585410, acc: 66.41%] [G loss: 2.105034]\n",
      "epoch:18 step:17425 [D loss: 0.662913, acc: 60.94%] [G loss: 1.712184]\n",
      "epoch:18 step:17426 [D loss: 0.659003, acc: 59.38%] [G loss: 1.910375]\n",
      "epoch:18 step:17427 [D loss: 0.656212, acc: 65.62%] [G loss: 1.932567]\n",
      "epoch:18 step:17428 [D loss: 0.642941, acc: 61.72%] [G loss: 1.812894]\n",
      "epoch:18 step:17429 [D loss: 0.643140, acc: 60.94%] [G loss: 2.060222]\n",
      "epoch:18 step:17430 [D loss: 0.663412, acc: 60.94%] [G loss: 2.016722]\n",
      "epoch:18 step:17431 [D loss: 0.713214, acc: 54.69%] [G loss: 1.942533]\n",
      "epoch:18 step:17432 [D loss: 0.690469, acc: 56.25%] [G loss: 1.791737]\n",
      "epoch:18 step:17433 [D loss: 0.664260, acc: 57.81%] [G loss: 1.839475]\n",
      "epoch:18 step:17434 [D loss: 0.655933, acc: 63.28%] [G loss: 1.762738]\n",
      "epoch:18 step:17435 [D loss: 0.685056, acc: 54.69%] [G loss: 1.819284]\n",
      "epoch:18 step:17436 [D loss: 0.610724, acc: 71.09%] [G loss: 1.894329]\n",
      "epoch:18 step:17437 [D loss: 0.635930, acc: 61.72%] [G loss: 1.866142]\n",
      "epoch:18 step:17438 [D loss: 0.656714, acc: 61.72%] [G loss: 1.797816]\n",
      "epoch:18 step:17439 [D loss: 0.630927, acc: 62.50%] [G loss: 1.849681]\n",
      "epoch:18 step:17440 [D loss: 0.640153, acc: 67.19%] [G loss: 1.874526]\n",
      "epoch:18 step:17441 [D loss: 0.630069, acc: 61.72%] [G loss: 1.780813]\n",
      "epoch:18 step:17442 [D loss: 0.639005, acc: 65.62%] [G loss: 1.684173]\n",
      "epoch:18 step:17443 [D loss: 0.665626, acc: 60.16%] [G loss: 1.882801]\n",
      "epoch:18 step:17444 [D loss: 0.682388, acc: 58.59%] [G loss: 1.799143]\n",
      "epoch:18 step:17445 [D loss: 0.726067, acc: 46.88%] [G loss: 1.743613]\n",
      "epoch:18 step:17446 [D loss: 0.667353, acc: 60.94%] [G loss: 1.745642]\n",
      "epoch:18 step:17447 [D loss: 0.634363, acc: 64.84%] [G loss: 1.896701]\n",
      "epoch:18 step:17448 [D loss: 0.664019, acc: 57.03%] [G loss: 1.809108]\n",
      "epoch:18 step:17449 [D loss: 0.624927, acc: 62.50%] [G loss: 1.830663]\n",
      "epoch:18 step:17450 [D loss: 0.680933, acc: 60.16%] [G loss: 1.840132]\n",
      "epoch:18 step:17451 [D loss: 0.630119, acc: 67.97%] [G loss: 2.051750]\n",
      "epoch:18 step:17452 [D loss: 0.671197, acc: 59.38%] [G loss: 1.871598]\n",
      "epoch:18 step:17453 [D loss: 0.658261, acc: 63.28%] [G loss: 1.862875]\n",
      "epoch:18 step:17454 [D loss: 0.611082, acc: 64.84%] [G loss: 1.950177]\n",
      "epoch:18 step:17455 [D loss: 0.623131, acc: 71.09%] [G loss: 1.916764]\n",
      "epoch:18 step:17456 [D loss: 0.679036, acc: 53.91%] [G loss: 1.921211]\n",
      "epoch:18 step:17457 [D loss: 0.669607, acc: 56.25%] [G loss: 2.026207]\n",
      "epoch:18 step:17458 [D loss: 0.625986, acc: 65.62%] [G loss: 2.004688]\n",
      "epoch:18 step:17459 [D loss: 0.620586, acc: 66.41%] [G loss: 1.907951]\n",
      "epoch:18 step:17460 [D loss: 0.665344, acc: 60.16%] [G loss: 1.909411]\n",
      "epoch:18 step:17461 [D loss: 0.637976, acc: 61.72%] [G loss: 1.877423]\n",
      "epoch:18 step:17462 [D loss: 0.699995, acc: 57.03%] [G loss: 1.798444]\n",
      "epoch:18 step:17463 [D loss: 0.615809, acc: 67.97%] [G loss: 1.735341]\n",
      "epoch:18 step:17464 [D loss: 0.667719, acc: 60.94%] [G loss: 1.904648]\n",
      "epoch:18 step:17465 [D loss: 0.627495, acc: 67.19%] [G loss: 1.939606]\n",
      "epoch:18 step:17466 [D loss: 0.638112, acc: 59.38%] [G loss: 1.893280]\n",
      "epoch:18 step:17467 [D loss: 0.664460, acc: 59.38%] [G loss: 1.815829]\n",
      "epoch:18 step:17468 [D loss: 0.648178, acc: 62.50%] [G loss: 1.943320]\n",
      "epoch:18 step:17469 [D loss: 0.708516, acc: 55.47%] [G loss: 1.832385]\n",
      "epoch:18 step:17470 [D loss: 0.628969, acc: 63.28%] [G loss: 1.853915]\n",
      "epoch:18 step:17471 [D loss: 0.627007, acc: 66.41%] [G loss: 2.041430]\n",
      "epoch:18 step:17472 [D loss: 0.675000, acc: 57.81%] [G loss: 1.880543]\n",
      "epoch:18 step:17473 [D loss: 0.684084, acc: 55.47%] [G loss: 1.872275]\n",
      "epoch:18 step:17474 [D loss: 0.573576, acc: 73.44%] [G loss: 1.937369]\n",
      "epoch:18 step:17475 [D loss: 0.613266, acc: 64.84%] [G loss: 1.955036]\n",
      "epoch:18 step:17476 [D loss: 0.636669, acc: 67.97%] [G loss: 1.751369]\n",
      "epoch:18 step:17477 [D loss: 0.671587, acc: 61.72%] [G loss: 1.687132]\n",
      "epoch:18 step:17478 [D loss: 0.688072, acc: 59.38%] [G loss: 1.743932]\n",
      "epoch:18 step:17479 [D loss: 0.660139, acc: 60.16%] [G loss: 1.787544]\n",
      "epoch:18 step:17480 [D loss: 0.674160, acc: 51.56%] [G loss: 1.740080]\n",
      "epoch:18 step:17481 [D loss: 0.618355, acc: 67.97%] [G loss: 1.777133]\n",
      "epoch:18 step:17482 [D loss: 0.673022, acc: 61.72%] [G loss: 1.942393]\n",
      "epoch:18 step:17483 [D loss: 0.649027, acc: 63.28%] [G loss: 1.837129]\n",
      "epoch:18 step:17484 [D loss: 0.597859, acc: 67.97%] [G loss: 1.773825]\n",
      "epoch:18 step:17485 [D loss: 0.682743, acc: 58.59%] [G loss: 1.920098]\n",
      "epoch:18 step:17486 [D loss: 0.661688, acc: 57.81%] [G loss: 1.761177]\n",
      "epoch:18 step:17487 [D loss: 0.656887, acc: 66.41%] [G loss: 1.895744]\n",
      "epoch:18 step:17488 [D loss: 0.633560, acc: 62.50%] [G loss: 1.872027]\n",
      "epoch:18 step:17489 [D loss: 0.666380, acc: 58.59%] [G loss: 1.864402]\n",
      "epoch:18 step:17490 [D loss: 0.650910, acc: 63.28%] [G loss: 2.018994]\n",
      "epoch:18 step:17491 [D loss: 0.721387, acc: 56.25%] [G loss: 1.864383]\n",
      "epoch:18 step:17492 [D loss: 0.657773, acc: 54.69%] [G loss: 1.942221]\n",
      "epoch:18 step:17493 [D loss: 0.641551, acc: 67.19%] [G loss: 1.831630]\n",
      "epoch:18 step:17494 [D loss: 0.662355, acc: 56.25%] [G loss: 1.733927]\n",
      "epoch:18 step:17495 [D loss: 0.624358, acc: 65.62%] [G loss: 2.057864]\n",
      "epoch:18 step:17496 [D loss: 0.681058, acc: 57.03%] [G loss: 1.868431]\n",
      "epoch:18 step:17497 [D loss: 0.649261, acc: 57.81%] [G loss: 1.861462]\n",
      "epoch:18 step:17498 [D loss: 0.619790, acc: 66.41%] [G loss: 1.877996]\n",
      "epoch:18 step:17499 [D loss: 0.628605, acc: 65.62%] [G loss: 1.891338]\n",
      "epoch:18 step:17500 [D loss: 0.594489, acc: 67.19%] [G loss: 1.920726]\n",
      "epoch:18 step:17501 [D loss: 0.649958, acc: 61.72%] [G loss: 1.905970]\n",
      "epoch:18 step:17502 [D loss: 0.634238, acc: 64.06%] [G loss: 1.878312]\n",
      "epoch:18 step:17503 [D loss: 0.589618, acc: 71.88%] [G loss: 2.057252]\n",
      "epoch:18 step:17504 [D loss: 0.649507, acc: 64.06%] [G loss: 1.899899]\n",
      "epoch:18 step:17505 [D loss: 0.634687, acc: 60.94%] [G loss: 1.861065]\n",
      "epoch:18 step:17506 [D loss: 0.688381, acc: 57.81%] [G loss: 1.832778]\n",
      "epoch:18 step:17507 [D loss: 0.656796, acc: 63.28%] [G loss: 2.009849]\n",
      "epoch:18 step:17508 [D loss: 0.605586, acc: 67.19%] [G loss: 2.036596]\n",
      "epoch:18 step:17509 [D loss: 0.640013, acc: 61.72%] [G loss: 1.950775]\n",
      "epoch:18 step:17510 [D loss: 0.661384, acc: 57.81%] [G loss: 1.880067]\n",
      "epoch:18 step:17511 [D loss: 0.660134, acc: 54.69%] [G loss: 1.901862]\n",
      "epoch:18 step:17512 [D loss: 0.628419, acc: 64.06%] [G loss: 1.997076]\n",
      "epoch:18 step:17513 [D loss: 0.681312, acc: 60.16%] [G loss: 2.025373]\n",
      "epoch:18 step:17514 [D loss: 0.610195, acc: 66.41%] [G loss: 2.167909]\n",
      "epoch:18 step:17515 [D loss: 0.577990, acc: 68.75%] [G loss: 2.160594]\n",
      "epoch:18 step:17516 [D loss: 0.648426, acc: 57.81%] [G loss: 1.987451]\n",
      "epoch:18 step:17517 [D loss: 0.647702, acc: 60.16%] [G loss: 1.989264]\n",
      "epoch:18 step:17518 [D loss: 0.605010, acc: 67.97%] [G loss: 1.811863]\n",
      "epoch:18 step:17519 [D loss: 0.638097, acc: 60.94%] [G loss: 1.819239]\n",
      "epoch:18 step:17520 [D loss: 0.643528, acc: 64.06%] [G loss: 1.990237]\n",
      "epoch:18 step:17521 [D loss: 0.652337, acc: 61.72%] [G loss: 1.884941]\n",
      "epoch:18 step:17522 [D loss: 0.667912, acc: 58.59%] [G loss: 1.829097]\n",
      "epoch:18 step:17523 [D loss: 0.706844, acc: 55.47%] [G loss: 1.804074]\n",
      "epoch:18 step:17524 [D loss: 0.704079, acc: 55.47%] [G loss: 1.720843]\n",
      "epoch:18 step:17525 [D loss: 0.655423, acc: 55.47%] [G loss: 1.724334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17526 [D loss: 0.682319, acc: 55.47%] [G loss: 1.781335]\n",
      "epoch:18 step:17527 [D loss: 0.606387, acc: 65.62%] [G loss: 1.848381]\n",
      "epoch:18 step:17528 [D loss: 0.655793, acc: 63.28%] [G loss: 1.864568]\n",
      "epoch:18 step:17529 [D loss: 0.688546, acc: 59.38%] [G loss: 1.868405]\n",
      "epoch:18 step:17530 [D loss: 0.685749, acc: 55.47%] [G loss: 1.872825]\n",
      "epoch:18 step:17531 [D loss: 0.652304, acc: 64.84%] [G loss: 1.887065]\n",
      "epoch:18 step:17532 [D loss: 0.706128, acc: 51.56%] [G loss: 1.690428]\n",
      "epoch:18 step:17533 [D loss: 0.653142, acc: 64.06%] [G loss: 1.787303]\n",
      "epoch:18 step:17534 [D loss: 0.665965, acc: 59.38%] [G loss: 1.718547]\n",
      "epoch:18 step:17535 [D loss: 0.675648, acc: 58.59%] [G loss: 1.811081]\n",
      "epoch:18 step:17536 [D loss: 0.644624, acc: 62.50%] [G loss: 1.760877]\n",
      "epoch:18 step:17537 [D loss: 0.658941, acc: 61.72%] [G loss: 1.777171]\n",
      "epoch:18 step:17538 [D loss: 0.670699, acc: 61.72%] [G loss: 1.814832]\n",
      "epoch:18 step:17539 [D loss: 0.660284, acc: 59.38%] [G loss: 1.720241]\n",
      "epoch:18 step:17540 [D loss: 0.645575, acc: 64.06%] [G loss: 1.757424]\n",
      "epoch:18 step:17541 [D loss: 0.691791, acc: 56.25%] [G loss: 1.788486]\n",
      "epoch:18 step:17542 [D loss: 0.644835, acc: 59.38%] [G loss: 1.859552]\n",
      "epoch:18 step:17543 [D loss: 0.627795, acc: 64.06%] [G loss: 2.004493]\n",
      "epoch:18 step:17544 [D loss: 0.657666, acc: 60.94%] [G loss: 1.905315]\n",
      "epoch:18 step:17545 [D loss: 0.627831, acc: 61.72%] [G loss: 1.896787]\n",
      "epoch:18 step:17546 [D loss: 0.597044, acc: 68.75%] [G loss: 1.907179]\n",
      "epoch:18 step:17547 [D loss: 0.637135, acc: 64.06%] [G loss: 1.913613]\n",
      "epoch:18 step:17548 [D loss: 0.656006, acc: 60.94%] [G loss: 1.956748]\n",
      "epoch:18 step:17549 [D loss: 0.653235, acc: 60.94%] [G loss: 1.970788]\n",
      "epoch:18 step:17550 [D loss: 0.680194, acc: 61.72%] [G loss: 1.920585]\n",
      "epoch:18 step:17551 [D loss: 0.624529, acc: 62.50%] [G loss: 1.813620]\n",
      "epoch:18 step:17552 [D loss: 0.654898, acc: 59.38%] [G loss: 1.837881]\n",
      "epoch:18 step:17553 [D loss: 0.651437, acc: 61.72%] [G loss: 1.826486]\n",
      "epoch:18 step:17554 [D loss: 0.649702, acc: 64.06%] [G loss: 1.778493]\n",
      "epoch:18 step:17555 [D loss: 0.654885, acc: 60.16%] [G loss: 1.926792]\n",
      "epoch:18 step:17556 [D loss: 0.650996, acc: 64.84%] [G loss: 1.949613]\n",
      "epoch:18 step:17557 [D loss: 0.628062, acc: 69.53%] [G loss: 1.878129]\n",
      "epoch:18 step:17558 [D loss: 0.614118, acc: 66.41%] [G loss: 1.850613]\n",
      "epoch:18 step:17559 [D loss: 0.600554, acc: 66.41%] [G loss: 2.053703]\n",
      "epoch:18 step:17560 [D loss: 0.610287, acc: 67.19%] [G loss: 2.134670]\n",
      "epoch:18 step:17561 [D loss: 0.637535, acc: 57.03%] [G loss: 2.072118]\n",
      "epoch:18 step:17562 [D loss: 0.620029, acc: 68.75%] [G loss: 1.958827]\n",
      "epoch:18 step:17563 [D loss: 0.620697, acc: 61.72%] [G loss: 1.897975]\n",
      "epoch:18 step:17564 [D loss: 0.656170, acc: 64.84%] [G loss: 1.731276]\n",
      "epoch:18 step:17565 [D loss: 0.612354, acc: 67.19%] [G loss: 1.905226]\n",
      "epoch:18 step:17566 [D loss: 0.661624, acc: 62.50%] [G loss: 1.824765]\n",
      "epoch:18 step:17567 [D loss: 0.630422, acc: 64.06%] [G loss: 1.992070]\n",
      "epoch:18 step:17568 [D loss: 0.667904, acc: 61.72%] [G loss: 1.862782]\n",
      "epoch:18 step:17569 [D loss: 0.666305, acc: 57.81%] [G loss: 1.774057]\n",
      "epoch:18 step:17570 [D loss: 0.686030, acc: 56.25%] [G loss: 1.854097]\n",
      "epoch:18 step:17571 [D loss: 0.672165, acc: 59.38%] [G loss: 1.885571]\n",
      "epoch:18 step:17572 [D loss: 0.579881, acc: 71.88%] [G loss: 1.954530]\n",
      "epoch:18 step:17573 [D loss: 0.605762, acc: 60.16%] [G loss: 1.945595]\n",
      "epoch:18 step:17574 [D loss: 0.633228, acc: 62.50%] [G loss: 2.127629]\n",
      "epoch:18 step:17575 [D loss: 0.613800, acc: 63.28%] [G loss: 1.918121]\n",
      "epoch:18 step:17576 [D loss: 0.647621, acc: 63.28%] [G loss: 1.826023]\n",
      "epoch:18 step:17577 [D loss: 0.659354, acc: 62.50%] [G loss: 2.001472]\n",
      "epoch:18 step:17578 [D loss: 0.583681, acc: 64.06%] [G loss: 1.922209]\n",
      "epoch:18 step:17579 [D loss: 0.680919, acc: 51.56%] [G loss: 1.829524]\n",
      "epoch:18 step:17580 [D loss: 0.675867, acc: 60.16%] [G loss: 1.835862]\n",
      "epoch:18 step:17581 [D loss: 0.649434, acc: 59.38%] [G loss: 1.997221]\n",
      "epoch:18 step:17582 [D loss: 0.663498, acc: 53.91%] [G loss: 1.766292]\n",
      "epoch:18 step:17583 [D loss: 0.628666, acc: 62.50%] [G loss: 1.761255]\n",
      "epoch:18 step:17584 [D loss: 0.676208, acc: 61.72%] [G loss: 1.792388]\n",
      "epoch:18 step:17585 [D loss: 0.619542, acc: 64.84%] [G loss: 2.162612]\n",
      "epoch:18 step:17586 [D loss: 0.665817, acc: 50.78%] [G loss: 1.893812]\n",
      "epoch:18 step:17587 [D loss: 0.662557, acc: 53.12%] [G loss: 1.942742]\n",
      "epoch:18 step:17588 [D loss: 0.671628, acc: 57.81%] [G loss: 1.830308]\n",
      "epoch:18 step:17589 [D loss: 0.723829, acc: 50.78%] [G loss: 1.794600]\n",
      "epoch:18 step:17590 [D loss: 0.608982, acc: 69.53%] [G loss: 1.857172]\n",
      "epoch:18 step:17591 [D loss: 0.662198, acc: 63.28%] [G loss: 1.887834]\n",
      "epoch:18 step:17592 [D loss: 0.624706, acc: 63.28%] [G loss: 2.006185]\n",
      "epoch:18 step:17593 [D loss: 0.669834, acc: 58.59%] [G loss: 1.925885]\n",
      "epoch:18 step:17594 [D loss: 0.604512, acc: 68.75%] [G loss: 1.943280]\n",
      "epoch:18 step:17595 [D loss: 0.650862, acc: 60.94%] [G loss: 1.857455]\n",
      "epoch:18 step:17596 [D loss: 0.649642, acc: 60.94%] [G loss: 1.868580]\n",
      "epoch:18 step:17597 [D loss: 0.619897, acc: 67.19%] [G loss: 1.903430]\n",
      "epoch:18 step:17598 [D loss: 0.600842, acc: 70.31%] [G loss: 1.883340]\n",
      "epoch:18 step:17599 [D loss: 0.633218, acc: 59.38%] [G loss: 1.970970]\n",
      "epoch:18 step:17600 [D loss: 0.675014, acc: 59.38%] [G loss: 1.831511]\n",
      "epoch:18 step:17601 [D loss: 0.679129, acc: 56.25%] [G loss: 1.865680]\n",
      "epoch:18 step:17602 [D loss: 0.670802, acc: 59.38%] [G loss: 1.951067]\n",
      "epoch:18 step:17603 [D loss: 0.649546, acc: 63.28%] [G loss: 1.809430]\n",
      "epoch:18 step:17604 [D loss: 0.632861, acc: 60.16%] [G loss: 1.882176]\n",
      "epoch:18 step:17605 [D loss: 0.658629, acc: 60.94%] [G loss: 1.857796]\n",
      "epoch:18 step:17606 [D loss: 0.678102, acc: 60.94%] [G loss: 1.714344]\n",
      "epoch:18 step:17607 [D loss: 0.651911, acc: 62.50%] [G loss: 1.859883]\n",
      "epoch:18 step:17608 [D loss: 0.662200, acc: 57.03%] [G loss: 1.890544]\n",
      "epoch:18 step:17609 [D loss: 0.638864, acc: 63.28%] [G loss: 1.908814]\n",
      "epoch:18 step:17610 [D loss: 0.685803, acc: 59.38%] [G loss: 1.789082]\n",
      "epoch:18 step:17611 [D loss: 0.666475, acc: 57.03%] [G loss: 1.867008]\n",
      "epoch:18 step:17612 [D loss: 0.602369, acc: 65.62%] [G loss: 1.946295]\n",
      "epoch:18 step:17613 [D loss: 0.617081, acc: 65.62%] [G loss: 1.950919]\n",
      "epoch:18 step:17614 [D loss: 0.684152, acc: 60.94%] [G loss: 1.825461]\n",
      "epoch:18 step:17615 [D loss: 0.611299, acc: 65.62%] [G loss: 1.753182]\n",
      "epoch:18 step:17616 [D loss: 0.694066, acc: 55.47%] [G loss: 1.779452]\n",
      "epoch:18 step:17617 [D loss: 0.654683, acc: 64.84%] [G loss: 1.747967]\n",
      "epoch:18 step:17618 [D loss: 0.666679, acc: 57.03%] [G loss: 1.732840]\n",
      "epoch:18 step:17619 [D loss: 0.630744, acc: 61.72%] [G loss: 1.888797]\n",
      "epoch:18 step:17620 [D loss: 0.635554, acc: 63.28%] [G loss: 1.748941]\n",
      "epoch:18 step:17621 [D loss: 0.675410, acc: 55.47%] [G loss: 1.838212]\n",
      "epoch:18 step:17622 [D loss: 0.638860, acc: 64.84%] [G loss: 1.916961]\n",
      "epoch:18 step:17623 [D loss: 0.595646, acc: 69.53%] [G loss: 1.816011]\n",
      "epoch:18 step:17624 [D loss: 0.688565, acc: 52.34%] [G loss: 1.813004]\n",
      "epoch:18 step:17625 [D loss: 0.655828, acc: 63.28%] [G loss: 1.882101]\n",
      "epoch:18 step:17626 [D loss: 0.646387, acc: 60.16%] [G loss: 1.804093]\n",
      "epoch:18 step:17627 [D loss: 0.651932, acc: 57.03%] [G loss: 1.781354]\n",
      "epoch:18 step:17628 [D loss: 0.650198, acc: 60.16%] [G loss: 1.709472]\n",
      "epoch:18 step:17629 [D loss: 0.642746, acc: 63.28%] [G loss: 1.843097]\n",
      "epoch:18 step:17630 [D loss: 0.703249, acc: 55.47%] [G loss: 1.799530]\n",
      "epoch:18 step:17631 [D loss: 0.663661, acc: 61.72%] [G loss: 1.712312]\n",
      "epoch:18 step:17632 [D loss: 0.706040, acc: 53.91%] [G loss: 1.722771]\n",
      "epoch:18 step:17633 [D loss: 0.707841, acc: 56.25%] [G loss: 1.732475]\n",
      "epoch:18 step:17634 [D loss: 0.693980, acc: 53.12%] [G loss: 1.756329]\n",
      "epoch:18 step:17635 [D loss: 0.627116, acc: 71.09%] [G loss: 1.808559]\n",
      "epoch:18 step:17636 [D loss: 0.667938, acc: 60.94%] [G loss: 1.838162]\n",
      "epoch:18 step:17637 [D loss: 0.639638, acc: 65.62%] [G loss: 1.820377]\n",
      "epoch:18 step:17638 [D loss: 0.658086, acc: 59.38%] [G loss: 1.866320]\n",
      "epoch:18 step:17639 [D loss: 0.629552, acc: 64.84%] [G loss: 1.967339]\n",
      "epoch:18 step:17640 [D loss: 0.657850, acc: 59.38%] [G loss: 1.971362]\n",
      "epoch:18 step:17641 [D loss: 0.583798, acc: 68.75%] [G loss: 2.010834]\n",
      "epoch:18 step:17642 [D loss: 0.627427, acc: 64.06%] [G loss: 1.906476]\n",
      "epoch:18 step:17643 [D loss: 0.632919, acc: 67.97%] [G loss: 1.996862]\n",
      "epoch:18 step:17644 [D loss: 0.656156, acc: 60.16%] [G loss: 1.760688]\n",
      "epoch:18 step:17645 [D loss: 0.621520, acc: 71.09%] [G loss: 1.776544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17646 [D loss: 0.581116, acc: 69.53%] [G loss: 2.131738]\n",
      "epoch:18 step:17647 [D loss: 0.572031, acc: 69.53%] [G loss: 2.286149]\n",
      "epoch:18 step:17648 [D loss: 0.627953, acc: 62.50%] [G loss: 2.101636]\n",
      "epoch:18 step:17649 [D loss: 0.625533, acc: 65.62%] [G loss: 2.104833]\n",
      "epoch:18 step:17650 [D loss: 0.736069, acc: 50.78%] [G loss: 1.925504]\n",
      "epoch:18 step:17651 [D loss: 0.631253, acc: 64.84%] [G loss: 1.813256]\n",
      "epoch:18 step:17652 [D loss: 0.644675, acc: 59.38%] [G loss: 1.994509]\n",
      "epoch:18 step:17653 [D loss: 0.683344, acc: 53.12%] [G loss: 1.919351]\n",
      "epoch:18 step:17654 [D loss: 0.694892, acc: 53.91%] [G loss: 1.813075]\n",
      "epoch:18 step:17655 [D loss: 0.609189, acc: 67.97%] [G loss: 1.893953]\n",
      "epoch:18 step:17656 [D loss: 0.643353, acc: 61.72%] [G loss: 1.981327]\n",
      "epoch:18 step:17657 [D loss: 0.592687, acc: 69.53%] [G loss: 2.015818]\n",
      "epoch:18 step:17658 [D loss: 0.597975, acc: 70.31%] [G loss: 2.062979]\n",
      "epoch:18 step:17659 [D loss: 0.666593, acc: 58.59%] [G loss: 2.046432]\n",
      "epoch:18 step:17660 [D loss: 0.746534, acc: 49.22%] [G loss: 1.640736]\n",
      "epoch:18 step:17661 [D loss: 0.707986, acc: 51.56%] [G loss: 1.769394]\n",
      "epoch:18 step:17662 [D loss: 0.693603, acc: 54.69%] [G loss: 1.848789]\n",
      "epoch:18 step:17663 [D loss: 0.701826, acc: 53.91%] [G loss: 1.841393]\n",
      "epoch:18 step:17664 [D loss: 0.677898, acc: 59.38%] [G loss: 1.772818]\n",
      "epoch:18 step:17665 [D loss: 0.607840, acc: 64.84%] [G loss: 1.751445]\n",
      "epoch:18 step:17666 [D loss: 0.646749, acc: 57.81%] [G loss: 1.685634]\n",
      "epoch:18 step:17667 [D loss: 0.683301, acc: 60.94%] [G loss: 1.758380]\n",
      "epoch:18 step:17668 [D loss: 0.651785, acc: 60.16%] [G loss: 1.983010]\n",
      "epoch:18 step:17669 [D loss: 0.653861, acc: 60.94%] [G loss: 1.851803]\n",
      "epoch:18 step:17670 [D loss: 0.640362, acc: 64.06%] [G loss: 1.944223]\n",
      "epoch:18 step:17671 [D loss: 0.628427, acc: 63.28%] [G loss: 1.928378]\n",
      "epoch:18 step:17672 [D loss: 0.612137, acc: 69.53%] [G loss: 1.941830]\n",
      "epoch:18 step:17673 [D loss: 0.612531, acc: 64.84%] [G loss: 1.872367]\n",
      "epoch:18 step:17674 [D loss: 0.619230, acc: 66.41%] [G loss: 1.804723]\n",
      "epoch:18 step:17675 [D loss: 0.611605, acc: 71.09%] [G loss: 1.869521]\n",
      "epoch:18 step:17676 [D loss: 0.625138, acc: 67.97%] [G loss: 1.860393]\n",
      "epoch:18 step:17677 [D loss: 0.649935, acc: 60.16%] [G loss: 1.743462]\n",
      "epoch:18 step:17678 [D loss: 0.662187, acc: 59.38%] [G loss: 1.760375]\n",
      "epoch:18 step:17679 [D loss: 0.670762, acc: 57.03%] [G loss: 1.809530]\n",
      "epoch:18 step:17680 [D loss: 0.683494, acc: 60.94%] [G loss: 1.866744]\n",
      "epoch:18 step:17681 [D loss: 0.670848, acc: 64.06%] [G loss: 1.979846]\n",
      "epoch:18 step:17682 [D loss: 0.568951, acc: 70.31%] [G loss: 2.027016]\n",
      "epoch:18 step:17683 [D loss: 0.665718, acc: 61.72%] [G loss: 1.835925]\n",
      "epoch:18 step:17684 [D loss: 0.715111, acc: 55.47%] [G loss: 1.872331]\n",
      "epoch:18 step:17685 [D loss: 0.664625, acc: 61.72%] [G loss: 1.915110]\n",
      "epoch:18 step:17686 [D loss: 0.686088, acc: 57.81%] [G loss: 1.738214]\n",
      "epoch:18 step:17687 [D loss: 0.632020, acc: 64.06%] [G loss: 1.901254]\n",
      "epoch:18 step:17688 [D loss: 0.623809, acc: 63.28%] [G loss: 1.885353]\n",
      "epoch:18 step:17689 [D loss: 0.621707, acc: 67.97%] [G loss: 2.053357]\n",
      "epoch:18 step:17690 [D loss: 0.642729, acc: 61.72%] [G loss: 1.902797]\n",
      "epoch:18 step:17691 [D loss: 0.627188, acc: 65.62%] [G loss: 2.015167]\n",
      "epoch:18 step:17692 [D loss: 0.621669, acc: 63.28%] [G loss: 1.816354]\n",
      "epoch:18 step:17693 [D loss: 0.651308, acc: 60.16%] [G loss: 1.859829]\n",
      "epoch:18 step:17694 [D loss: 0.667506, acc: 61.72%] [G loss: 1.659184]\n",
      "epoch:18 step:17695 [D loss: 0.634704, acc: 65.62%] [G loss: 1.729879]\n",
      "epoch:18 step:17696 [D loss: 0.675509, acc: 57.03%] [G loss: 1.721757]\n",
      "epoch:18 step:17697 [D loss: 0.654351, acc: 55.47%] [G loss: 1.855447]\n",
      "epoch:18 step:17698 [D loss: 0.664791, acc: 59.38%] [G loss: 1.894582]\n",
      "epoch:18 step:17699 [D loss: 0.615371, acc: 72.66%] [G loss: 1.985757]\n",
      "epoch:18 step:17700 [D loss: 0.688328, acc: 58.59%] [G loss: 1.943473]\n",
      "epoch:18 step:17701 [D loss: 0.629156, acc: 59.38%] [G loss: 1.854879]\n",
      "epoch:18 step:17702 [D loss: 0.619974, acc: 67.97%] [G loss: 1.927683]\n",
      "epoch:18 step:17703 [D loss: 0.637081, acc: 59.38%] [G loss: 1.871713]\n",
      "epoch:18 step:17704 [D loss: 0.630393, acc: 68.75%] [G loss: 2.054239]\n",
      "epoch:18 step:17705 [D loss: 0.594322, acc: 75.00%] [G loss: 1.897199]\n",
      "epoch:18 step:17706 [D loss: 0.622731, acc: 68.75%] [G loss: 1.957979]\n",
      "epoch:18 step:17707 [D loss: 0.608762, acc: 67.19%] [G loss: 1.955927]\n",
      "epoch:18 step:17708 [D loss: 0.574686, acc: 65.62%] [G loss: 2.047850]\n",
      "epoch:18 step:17709 [D loss: 0.593085, acc: 68.75%] [G loss: 1.840154]\n",
      "epoch:18 step:17710 [D loss: 0.705318, acc: 58.59%] [G loss: 1.869559]\n",
      "epoch:18 step:17711 [D loss: 0.654059, acc: 65.62%] [G loss: 2.069640]\n",
      "epoch:18 step:17712 [D loss: 0.641323, acc: 67.97%] [G loss: 1.856305]\n",
      "epoch:18 step:17713 [D loss: 0.643075, acc: 62.50%] [G loss: 2.040850]\n",
      "epoch:18 step:17714 [D loss: 0.658338, acc: 57.81%] [G loss: 2.002379]\n",
      "epoch:18 step:17715 [D loss: 0.617506, acc: 65.62%] [G loss: 1.901812]\n",
      "epoch:18 step:17716 [D loss: 0.648616, acc: 60.16%] [G loss: 1.911588]\n",
      "epoch:18 step:17717 [D loss: 0.609260, acc: 65.62%] [G loss: 1.922662]\n",
      "epoch:18 step:17718 [D loss: 0.606691, acc: 68.75%] [G loss: 1.849043]\n",
      "epoch:18 step:17719 [D loss: 0.663024, acc: 60.16%] [G loss: 1.977273]\n",
      "epoch:18 step:17720 [D loss: 0.643396, acc: 67.19%] [G loss: 2.022467]\n",
      "epoch:18 step:17721 [D loss: 0.632145, acc: 64.06%] [G loss: 1.826364]\n",
      "epoch:18 step:17722 [D loss: 0.685332, acc: 56.25%] [G loss: 1.800764]\n",
      "epoch:18 step:17723 [D loss: 0.659476, acc: 64.06%] [G loss: 1.817554]\n",
      "epoch:18 step:17724 [D loss: 0.733121, acc: 51.56%] [G loss: 1.700728]\n",
      "epoch:18 step:17725 [D loss: 0.669615, acc: 62.50%] [G loss: 1.862875]\n",
      "epoch:18 step:17726 [D loss: 0.656276, acc: 61.72%] [G loss: 1.964812]\n",
      "epoch:18 step:17727 [D loss: 0.677450, acc: 60.16%] [G loss: 1.846551]\n",
      "epoch:18 step:17728 [D loss: 0.651172, acc: 62.50%] [G loss: 1.809884]\n",
      "epoch:18 step:17729 [D loss: 0.703446, acc: 50.78%] [G loss: 1.794306]\n",
      "epoch:18 step:17730 [D loss: 0.656571, acc: 64.06%] [G loss: 1.794555]\n",
      "epoch:18 step:17731 [D loss: 0.678380, acc: 55.47%] [G loss: 1.776497]\n",
      "epoch:18 step:17732 [D loss: 0.661681, acc: 58.59%] [G loss: 1.803863]\n",
      "epoch:18 step:17733 [D loss: 0.692272, acc: 53.91%] [G loss: 1.704305]\n",
      "epoch:18 step:17734 [D loss: 0.699570, acc: 60.94%] [G loss: 1.832803]\n",
      "epoch:18 step:17735 [D loss: 0.631698, acc: 65.62%] [G loss: 1.713767]\n",
      "epoch:18 step:17736 [D loss: 0.666041, acc: 60.94%] [G loss: 1.811030]\n",
      "epoch:18 step:17737 [D loss: 0.657354, acc: 65.62%] [G loss: 1.886945]\n",
      "epoch:18 step:17738 [D loss: 0.665615, acc: 57.81%] [G loss: 1.772121]\n",
      "epoch:18 step:17739 [D loss: 0.676294, acc: 60.16%] [G loss: 1.831517]\n",
      "epoch:18 step:17740 [D loss: 0.663483, acc: 64.06%] [G loss: 1.832116]\n",
      "epoch:18 step:17741 [D loss: 0.635314, acc: 60.16%] [G loss: 1.938322]\n",
      "epoch:18 step:17742 [D loss: 0.728067, acc: 52.34%] [G loss: 1.861452]\n",
      "epoch:18 step:17743 [D loss: 0.664178, acc: 65.62%] [G loss: 1.812829]\n",
      "epoch:18 step:17744 [D loss: 0.624458, acc: 64.84%] [G loss: 1.765525]\n",
      "epoch:18 step:17745 [D loss: 0.645424, acc: 60.94%] [G loss: 1.735487]\n",
      "epoch:18 step:17746 [D loss: 0.641941, acc: 60.16%] [G loss: 1.794741]\n",
      "epoch:18 step:17747 [D loss: 0.637198, acc: 63.28%] [G loss: 1.964171]\n",
      "epoch:18 step:17748 [D loss: 0.641735, acc: 63.28%] [G loss: 1.891111]\n",
      "epoch:18 step:17749 [D loss: 0.668241, acc: 60.16%] [G loss: 1.948616]\n",
      "epoch:18 step:17750 [D loss: 0.637791, acc: 61.72%] [G loss: 1.911098]\n",
      "epoch:18 step:17751 [D loss: 0.649569, acc: 64.84%] [G loss: 1.905806]\n",
      "epoch:18 step:17752 [D loss: 0.664432, acc: 58.59%] [G loss: 1.862307]\n",
      "epoch:18 step:17753 [D loss: 0.627333, acc: 60.16%] [G loss: 1.851291]\n",
      "epoch:18 step:17754 [D loss: 0.650324, acc: 57.81%] [G loss: 1.899128]\n",
      "epoch:18 step:17755 [D loss: 0.628301, acc: 66.41%] [G loss: 1.986154]\n",
      "epoch:18 step:17756 [D loss: 0.616138, acc: 65.62%] [G loss: 1.979680]\n",
      "epoch:18 step:17757 [D loss: 0.708397, acc: 55.47%] [G loss: 1.767555]\n",
      "epoch:18 step:17758 [D loss: 0.699845, acc: 53.12%] [G loss: 1.851300]\n",
      "epoch:18 step:17759 [D loss: 0.642713, acc: 65.62%] [G loss: 1.931820]\n",
      "epoch:18 step:17760 [D loss: 0.651290, acc: 55.47%] [G loss: 1.778071]\n",
      "epoch:18 step:17761 [D loss: 0.650836, acc: 60.16%] [G loss: 1.799245]\n",
      "epoch:18 step:17762 [D loss: 0.653657, acc: 61.72%] [G loss: 1.784903]\n",
      "epoch:18 step:17763 [D loss: 0.654355, acc: 64.84%] [G loss: 1.910753]\n",
      "epoch:18 step:17764 [D loss: 0.650922, acc: 58.59%] [G loss: 1.939497]\n",
      "epoch:18 step:17765 [D loss: 0.614007, acc: 63.28%] [G loss: 2.033374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:17766 [D loss: 0.608831, acc: 66.41%] [G loss: 1.998477]\n",
      "epoch:18 step:17767 [D loss: 0.635731, acc: 62.50%] [G loss: 2.011788]\n",
      "epoch:18 step:17768 [D loss: 0.617157, acc: 61.72%] [G loss: 1.753321]\n",
      "epoch:18 step:17769 [D loss: 0.694029, acc: 58.59%] [G loss: 1.861603]\n",
      "epoch:18 step:17770 [D loss: 0.655275, acc: 62.50%] [G loss: 1.913195]\n",
      "epoch:18 step:17771 [D loss: 0.631796, acc: 63.28%] [G loss: 1.884729]\n",
      "epoch:18 step:17772 [D loss: 0.635407, acc: 70.31%] [G loss: 2.014868]\n",
      "epoch:18 step:17773 [D loss: 0.586761, acc: 70.31%] [G loss: 2.005187]\n",
      "epoch:18 step:17774 [D loss: 0.625757, acc: 66.41%] [G loss: 1.866779]\n",
      "epoch:18 step:17775 [D loss: 0.599798, acc: 68.75%] [G loss: 1.976914]\n",
      "epoch:18 step:17776 [D loss: 0.600931, acc: 73.44%] [G loss: 1.911881]\n",
      "epoch:18 step:17777 [D loss: 0.683444, acc: 59.38%] [G loss: 1.938678]\n",
      "epoch:18 step:17778 [D loss: 0.647186, acc: 63.28%] [G loss: 1.960831]\n",
      "epoch:18 step:17779 [D loss: 0.716613, acc: 53.12%] [G loss: 2.010917]\n",
      "epoch:18 step:17780 [D loss: 0.652485, acc: 62.50%] [G loss: 1.938365]\n",
      "epoch:18 step:17781 [D loss: 0.657607, acc: 55.47%] [G loss: 1.793699]\n",
      "epoch:18 step:17782 [D loss: 0.674171, acc: 60.94%] [G loss: 1.873757]\n",
      "epoch:18 step:17783 [D loss: 0.621393, acc: 67.97%] [G loss: 2.003632]\n",
      "epoch:18 step:17784 [D loss: 0.605321, acc: 67.97%] [G loss: 2.145138]\n",
      "epoch:18 step:17785 [D loss: 0.596460, acc: 64.84%] [G loss: 2.129226]\n",
      "epoch:18 step:17786 [D loss: 0.668136, acc: 56.25%] [G loss: 1.842057]\n",
      "epoch:18 step:17787 [D loss: 0.644002, acc: 62.50%] [G loss: 1.910130]\n",
      "epoch:18 step:17788 [D loss: 0.634149, acc: 62.50%] [G loss: 2.032808]\n",
      "epoch:18 step:17789 [D loss: 0.595583, acc: 67.97%] [G loss: 2.271342]\n",
      "epoch:18 step:17790 [D loss: 0.584422, acc: 69.53%] [G loss: 2.212934]\n",
      "epoch:18 step:17791 [D loss: 0.616004, acc: 64.06%] [G loss: 2.278605]\n",
      "epoch:18 step:17792 [D loss: 0.635953, acc: 64.84%] [G loss: 2.076859]\n",
      "epoch:18 step:17793 [D loss: 0.642465, acc: 67.19%] [G loss: 2.179721]\n",
      "epoch:18 step:17794 [D loss: 0.766284, acc: 46.88%] [G loss: 1.894540]\n",
      "epoch:18 step:17795 [D loss: 0.792079, acc: 42.97%] [G loss: 1.825763]\n",
      "epoch:18 step:17796 [D loss: 0.602826, acc: 65.62%] [G loss: 1.985825]\n",
      "epoch:18 step:17797 [D loss: 0.666862, acc: 63.28%] [G loss: 1.950372]\n",
      "epoch:18 step:17798 [D loss: 0.571331, acc: 72.66%] [G loss: 1.921487]\n",
      "epoch:18 step:17799 [D loss: 0.656979, acc: 65.62%] [G loss: 1.841832]\n",
      "epoch:18 step:17800 [D loss: 0.620279, acc: 66.41%] [G loss: 2.008444]\n",
      "epoch:18 step:17801 [D loss: 0.595448, acc: 68.75%] [G loss: 2.047086]\n",
      "epoch:18 step:17802 [D loss: 0.611612, acc: 64.84%] [G loss: 2.073774]\n",
      "epoch:18 step:17803 [D loss: 0.520510, acc: 76.56%] [G loss: 2.539935]\n",
      "epoch:19 step:17804 [D loss: 0.645837, acc: 63.28%] [G loss: 2.014156]\n",
      "epoch:19 step:17805 [D loss: 0.648620, acc: 61.72%] [G loss: 2.043103]\n",
      "epoch:19 step:17806 [D loss: 0.718208, acc: 58.59%] [G loss: 1.798297]\n",
      "epoch:19 step:17807 [D loss: 0.606136, acc: 62.50%] [G loss: 1.911381]\n",
      "epoch:19 step:17808 [D loss: 0.685135, acc: 55.47%] [G loss: 1.889741]\n",
      "epoch:19 step:17809 [D loss: 0.671951, acc: 58.59%] [G loss: 1.891422]\n",
      "epoch:19 step:17810 [D loss: 0.615230, acc: 69.53%] [G loss: 2.025843]\n",
      "epoch:19 step:17811 [D loss: 0.637648, acc: 65.62%] [G loss: 1.892915]\n",
      "epoch:19 step:17812 [D loss: 0.608207, acc: 69.53%] [G loss: 1.946409]\n",
      "epoch:19 step:17813 [D loss: 0.595036, acc: 65.62%] [G loss: 2.063616]\n",
      "epoch:19 step:17814 [D loss: 0.569275, acc: 71.09%] [G loss: 1.915525]\n",
      "epoch:19 step:17815 [D loss: 0.651312, acc: 63.28%] [G loss: 1.954331]\n",
      "epoch:19 step:17816 [D loss: 0.608292, acc: 62.50%] [G loss: 1.949520]\n",
      "epoch:19 step:17817 [D loss: 0.633110, acc: 68.75%] [G loss: 1.981980]\n",
      "epoch:19 step:17818 [D loss: 0.618113, acc: 64.84%] [G loss: 2.136506]\n",
      "epoch:19 step:17819 [D loss: 0.619627, acc: 63.28%] [G loss: 2.196723]\n",
      "epoch:19 step:17820 [D loss: 0.639373, acc: 60.16%] [G loss: 1.864931]\n",
      "epoch:19 step:17821 [D loss: 0.662736, acc: 61.72%] [G loss: 2.092798]\n",
      "epoch:19 step:17822 [D loss: 0.669531, acc: 59.38%] [G loss: 1.843161]\n",
      "epoch:19 step:17823 [D loss: 0.682720, acc: 53.91%] [G loss: 1.724754]\n",
      "epoch:19 step:17824 [D loss: 0.667598, acc: 60.94%] [G loss: 1.953293]\n",
      "epoch:19 step:17825 [D loss: 0.636670, acc: 60.94%] [G loss: 1.864980]\n",
      "epoch:19 step:17826 [D loss: 0.645279, acc: 60.16%] [G loss: 2.027357]\n",
      "epoch:19 step:17827 [D loss: 0.635384, acc: 60.94%] [G loss: 2.062810]\n",
      "epoch:19 step:17828 [D loss: 0.601880, acc: 62.50%] [G loss: 1.987358]\n",
      "epoch:19 step:17829 [D loss: 0.599766, acc: 69.53%] [G loss: 1.868658]\n",
      "epoch:19 step:17830 [D loss: 0.675930, acc: 58.59%] [G loss: 1.804881]\n",
      "epoch:19 step:17831 [D loss: 0.654425, acc: 65.62%] [G loss: 1.780841]\n",
      "epoch:19 step:17832 [D loss: 0.655957, acc: 54.69%] [G loss: 2.005199]\n",
      "epoch:19 step:17833 [D loss: 0.666541, acc: 58.59%] [G loss: 1.878514]\n",
      "epoch:19 step:17834 [D loss: 0.694821, acc: 51.56%] [G loss: 1.726640]\n",
      "epoch:19 step:17835 [D loss: 0.687293, acc: 58.59%] [G loss: 1.774476]\n",
      "epoch:19 step:17836 [D loss: 0.632825, acc: 64.84%] [G loss: 1.845313]\n",
      "epoch:19 step:17837 [D loss: 0.668873, acc: 63.28%] [G loss: 1.796906]\n",
      "epoch:19 step:17838 [D loss: 0.642688, acc: 62.50%] [G loss: 1.826043]\n",
      "epoch:19 step:17839 [D loss: 0.641100, acc: 68.75%] [G loss: 1.823454]\n",
      "epoch:19 step:17840 [D loss: 0.639658, acc: 57.81%] [G loss: 1.925414]\n",
      "epoch:19 step:17841 [D loss: 0.633505, acc: 64.84%] [G loss: 1.930141]\n",
      "epoch:19 step:17842 [D loss: 0.609056, acc: 65.62%] [G loss: 2.020880]\n",
      "epoch:19 step:17843 [D loss: 0.648743, acc: 67.19%] [G loss: 2.246792]\n",
      "epoch:19 step:17844 [D loss: 0.641786, acc: 67.97%] [G loss: 1.810894]\n",
      "epoch:19 step:17845 [D loss: 0.652950, acc: 58.59%] [G loss: 2.042438]\n",
      "epoch:19 step:17846 [D loss: 0.687221, acc: 63.28%] [G loss: 1.976835]\n",
      "epoch:19 step:17847 [D loss: 0.656051, acc: 60.16%] [G loss: 1.789656]\n",
      "epoch:19 step:17848 [D loss: 0.608173, acc: 67.19%] [G loss: 2.062355]\n",
      "epoch:19 step:17849 [D loss: 0.678359, acc: 60.94%] [G loss: 1.887089]\n",
      "epoch:19 step:17850 [D loss: 0.598074, acc: 70.31%] [G loss: 1.999751]\n",
      "epoch:19 step:17851 [D loss: 0.630557, acc: 62.50%] [G loss: 1.837797]\n",
      "epoch:19 step:17852 [D loss: 0.628735, acc: 60.94%] [G loss: 2.018040]\n",
      "epoch:19 step:17853 [D loss: 0.633564, acc: 64.84%] [G loss: 1.779977]\n",
      "epoch:19 step:17854 [D loss: 0.680830, acc: 56.25%] [G loss: 1.821331]\n",
      "epoch:19 step:17855 [D loss: 0.635624, acc: 59.38%] [G loss: 1.958895]\n",
      "epoch:19 step:17856 [D loss: 0.608481, acc: 71.09%] [G loss: 2.050884]\n",
      "epoch:19 step:17857 [D loss: 0.641943, acc: 67.97%] [G loss: 2.073625]\n",
      "epoch:19 step:17858 [D loss: 0.608081, acc: 64.06%] [G loss: 2.021379]\n",
      "epoch:19 step:17859 [D loss: 0.622239, acc: 64.84%] [G loss: 2.055810]\n",
      "epoch:19 step:17860 [D loss: 0.690969, acc: 61.72%] [G loss: 1.935948]\n",
      "epoch:19 step:17861 [D loss: 0.631477, acc: 66.41%] [G loss: 1.928845]\n",
      "epoch:19 step:17862 [D loss: 0.706078, acc: 57.03%] [G loss: 1.868457]\n",
      "epoch:19 step:17863 [D loss: 0.639794, acc: 63.28%] [G loss: 1.925567]\n",
      "epoch:19 step:17864 [D loss: 0.659237, acc: 61.72%] [G loss: 1.789261]\n",
      "epoch:19 step:17865 [D loss: 0.637367, acc: 62.50%] [G loss: 1.755449]\n",
      "epoch:19 step:17866 [D loss: 0.669644, acc: 62.50%] [G loss: 1.895666]\n",
      "epoch:19 step:17867 [D loss: 0.676698, acc: 57.03%] [G loss: 1.824969]\n",
      "epoch:19 step:17868 [D loss: 0.646121, acc: 61.72%] [G loss: 1.856282]\n",
      "epoch:19 step:17869 [D loss: 0.685670, acc: 52.34%] [G loss: 1.814111]\n",
      "epoch:19 step:17870 [D loss: 0.620910, acc: 66.41%] [G loss: 1.906916]\n",
      "epoch:19 step:17871 [D loss: 0.652925, acc: 62.50%] [G loss: 1.881478]\n",
      "epoch:19 step:17872 [D loss: 0.630772, acc: 67.97%] [G loss: 1.999982]\n",
      "epoch:19 step:17873 [D loss: 0.580934, acc: 73.44%] [G loss: 1.852263]\n",
      "epoch:19 step:17874 [D loss: 0.656391, acc: 57.03%] [G loss: 1.873756]\n",
      "epoch:19 step:17875 [D loss: 0.669937, acc: 57.03%] [G loss: 1.806451]\n",
      "epoch:19 step:17876 [D loss: 0.673321, acc: 56.25%] [G loss: 1.723999]\n",
      "epoch:19 step:17877 [D loss: 0.624873, acc: 64.84%] [G loss: 1.947460]\n",
      "epoch:19 step:17878 [D loss: 0.648410, acc: 59.38%] [G loss: 2.034910]\n",
      "epoch:19 step:17879 [D loss: 0.588659, acc: 69.53%] [G loss: 1.900822]\n",
      "epoch:19 step:17880 [D loss: 0.611726, acc: 64.84%] [G loss: 2.112323]\n",
      "epoch:19 step:17881 [D loss: 0.704490, acc: 53.91%] [G loss: 1.800258]\n",
      "epoch:19 step:17882 [D loss: 0.676284, acc: 59.38%] [G loss: 1.746467]\n",
      "epoch:19 step:17883 [D loss: 0.663521, acc: 62.50%] [G loss: 1.774351]\n",
      "epoch:19 step:17884 [D loss: 0.674812, acc: 57.81%] [G loss: 1.737978]\n",
      "epoch:19 step:17885 [D loss: 0.702278, acc: 57.81%] [G loss: 1.907669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:17886 [D loss: 0.612247, acc: 64.06%] [G loss: 1.830983]\n",
      "epoch:19 step:17887 [D loss: 0.595884, acc: 66.41%] [G loss: 1.898713]\n",
      "epoch:19 step:17888 [D loss: 0.682004, acc: 60.16%] [G loss: 1.878488]\n",
      "epoch:19 step:17889 [D loss: 0.693795, acc: 60.94%] [G loss: 1.781768]\n",
      "epoch:19 step:17890 [D loss: 0.672249, acc: 60.94%] [G loss: 1.774825]\n",
      "epoch:19 step:17891 [D loss: 0.619749, acc: 64.84%] [G loss: 1.887374]\n",
      "epoch:19 step:17892 [D loss: 0.706915, acc: 55.47%] [G loss: 1.851142]\n",
      "epoch:19 step:17893 [D loss: 0.677071, acc: 56.25%] [G loss: 1.890943]\n",
      "epoch:19 step:17894 [D loss: 0.646706, acc: 67.97%] [G loss: 1.783895]\n",
      "epoch:19 step:17895 [D loss: 0.634646, acc: 68.75%] [G loss: 1.801850]\n",
      "epoch:19 step:17896 [D loss: 0.645509, acc: 67.97%] [G loss: 1.930263]\n",
      "epoch:19 step:17897 [D loss: 0.648581, acc: 60.94%] [G loss: 1.941414]\n",
      "epoch:19 step:17898 [D loss: 0.701312, acc: 57.81%] [G loss: 1.799509]\n",
      "epoch:19 step:17899 [D loss: 0.636910, acc: 60.94%] [G loss: 1.837521]\n",
      "epoch:19 step:17900 [D loss: 0.683806, acc: 55.47%] [G loss: 1.976316]\n",
      "epoch:19 step:17901 [D loss: 0.665181, acc: 59.38%] [G loss: 1.874206]\n",
      "epoch:19 step:17902 [D loss: 0.660071, acc: 61.72%] [G loss: 1.746592]\n",
      "epoch:19 step:17903 [D loss: 0.650962, acc: 55.47%] [G loss: 1.849413]\n",
      "epoch:19 step:17904 [D loss: 0.627392, acc: 64.06%] [G loss: 1.951394]\n",
      "epoch:19 step:17905 [D loss: 0.642894, acc: 62.50%] [G loss: 1.933401]\n",
      "epoch:19 step:17906 [D loss: 0.643275, acc: 67.19%] [G loss: 1.807794]\n",
      "epoch:19 step:17907 [D loss: 0.638507, acc: 64.84%] [G loss: 1.731538]\n",
      "epoch:19 step:17908 [D loss: 0.681064, acc: 55.47%] [G loss: 1.906933]\n",
      "epoch:19 step:17909 [D loss: 0.615124, acc: 65.62%] [G loss: 2.049962]\n",
      "epoch:19 step:17910 [D loss: 0.632050, acc: 60.16%] [G loss: 1.966914]\n",
      "epoch:19 step:17911 [D loss: 0.663035, acc: 60.16%] [G loss: 1.788710]\n",
      "epoch:19 step:17912 [D loss: 0.672899, acc: 57.03%] [G loss: 1.848879]\n",
      "epoch:19 step:17913 [D loss: 0.661181, acc: 57.03%] [G loss: 1.739908]\n",
      "epoch:19 step:17914 [D loss: 0.579839, acc: 73.44%] [G loss: 2.084723]\n",
      "epoch:19 step:17915 [D loss: 0.597452, acc: 67.97%] [G loss: 2.187840]\n",
      "epoch:19 step:17916 [D loss: 0.670025, acc: 61.72%] [G loss: 1.997312]\n",
      "epoch:19 step:17917 [D loss: 0.659993, acc: 62.50%] [G loss: 1.967361]\n",
      "epoch:19 step:17918 [D loss: 0.635019, acc: 63.28%] [G loss: 2.151877]\n",
      "epoch:19 step:17919 [D loss: 0.616240, acc: 70.31%] [G loss: 1.974944]\n",
      "epoch:19 step:17920 [D loss: 0.636878, acc: 62.50%] [G loss: 2.007126]\n",
      "epoch:19 step:17921 [D loss: 0.617781, acc: 67.97%] [G loss: 2.024804]\n",
      "epoch:19 step:17922 [D loss: 0.605323, acc: 67.97%] [G loss: 2.370018]\n",
      "epoch:19 step:17923 [D loss: 0.649295, acc: 65.62%] [G loss: 1.910529]\n",
      "epoch:19 step:17924 [D loss: 0.661091, acc: 59.38%] [G loss: 1.927566]\n",
      "epoch:19 step:17925 [D loss: 0.661132, acc: 60.16%] [G loss: 1.887422]\n",
      "epoch:19 step:17926 [D loss: 0.651130, acc: 65.62%] [G loss: 1.909443]\n",
      "epoch:19 step:17927 [D loss: 0.686764, acc: 55.47%] [G loss: 1.914922]\n",
      "epoch:19 step:17928 [D loss: 0.717530, acc: 51.56%] [G loss: 1.834506]\n",
      "epoch:19 step:17929 [D loss: 0.662063, acc: 60.16%] [G loss: 2.018218]\n",
      "epoch:19 step:17930 [D loss: 0.649069, acc: 65.62%] [G loss: 1.739615]\n",
      "epoch:19 step:17931 [D loss: 0.649506, acc: 62.50%] [G loss: 1.979568]\n",
      "epoch:19 step:17932 [D loss: 0.667448, acc: 63.28%] [G loss: 1.718714]\n",
      "epoch:19 step:17933 [D loss: 0.670614, acc: 61.72%] [G loss: 1.801182]\n",
      "epoch:19 step:17934 [D loss: 0.645555, acc: 64.06%] [G loss: 1.879920]\n",
      "epoch:19 step:17935 [D loss: 0.618621, acc: 60.94%] [G loss: 1.940826]\n",
      "epoch:19 step:17936 [D loss: 0.617964, acc: 64.06%] [G loss: 1.709921]\n",
      "epoch:19 step:17937 [D loss: 0.632223, acc: 62.50%] [G loss: 1.818604]\n",
      "epoch:19 step:17938 [D loss: 0.697800, acc: 53.91%] [G loss: 1.784994]\n",
      "epoch:19 step:17939 [D loss: 0.669039, acc: 60.16%] [G loss: 1.830022]\n",
      "epoch:19 step:17940 [D loss: 0.628140, acc: 69.53%] [G loss: 1.838773]\n",
      "epoch:19 step:17941 [D loss: 0.685918, acc: 58.59%] [G loss: 1.918222]\n",
      "epoch:19 step:17942 [D loss: 0.631597, acc: 64.84%] [G loss: 1.816537]\n",
      "epoch:19 step:17943 [D loss: 0.649502, acc: 64.06%] [G loss: 1.722262]\n",
      "epoch:19 step:17944 [D loss: 0.692076, acc: 53.91%] [G loss: 1.765233]\n",
      "epoch:19 step:17945 [D loss: 0.659081, acc: 55.47%] [G loss: 1.744973]\n",
      "epoch:19 step:17946 [D loss: 0.682901, acc: 57.03%] [G loss: 1.764487]\n",
      "epoch:19 step:17947 [D loss: 0.662363, acc: 57.03%] [G loss: 1.788379]\n",
      "epoch:19 step:17948 [D loss: 0.611647, acc: 62.50%] [G loss: 1.972615]\n",
      "epoch:19 step:17949 [D loss: 0.627161, acc: 68.75%] [G loss: 1.900407]\n",
      "epoch:19 step:17950 [D loss: 0.682934, acc: 55.47%] [G loss: 1.746866]\n",
      "epoch:19 step:17951 [D loss: 0.716463, acc: 50.78%] [G loss: 1.654423]\n",
      "epoch:19 step:17952 [D loss: 0.657928, acc: 64.06%] [G loss: 1.872115]\n",
      "epoch:19 step:17953 [D loss: 0.631739, acc: 66.41%] [G loss: 1.909791]\n",
      "epoch:19 step:17954 [D loss: 0.670635, acc: 58.59%] [G loss: 1.915669]\n",
      "epoch:19 step:17955 [D loss: 0.635215, acc: 65.62%] [G loss: 1.995890]\n",
      "epoch:19 step:17956 [D loss: 0.711018, acc: 55.47%] [G loss: 1.808151]\n",
      "epoch:19 step:17957 [D loss: 0.674040, acc: 60.94%] [G loss: 1.850977]\n",
      "epoch:19 step:17958 [D loss: 0.651130, acc: 64.84%] [G loss: 1.847860]\n",
      "epoch:19 step:17959 [D loss: 0.664192, acc: 60.94%] [G loss: 1.884002]\n",
      "epoch:19 step:17960 [D loss: 0.636911, acc: 62.50%] [G loss: 1.665644]\n",
      "epoch:19 step:17961 [D loss: 0.670516, acc: 54.69%] [G loss: 1.886945]\n",
      "epoch:19 step:17962 [D loss: 0.607773, acc: 72.66%] [G loss: 1.810802]\n",
      "epoch:19 step:17963 [D loss: 0.686314, acc: 57.03%] [G loss: 1.675289]\n",
      "epoch:19 step:17964 [D loss: 0.676470, acc: 58.59%] [G loss: 1.869325]\n",
      "epoch:19 step:17965 [D loss: 0.616644, acc: 66.41%] [G loss: 1.864792]\n",
      "epoch:19 step:17966 [D loss: 0.631130, acc: 62.50%] [G loss: 1.848097]\n",
      "epoch:19 step:17967 [D loss: 0.642767, acc: 59.38%] [G loss: 1.765728]\n",
      "epoch:19 step:17968 [D loss: 0.637837, acc: 62.50%] [G loss: 1.892785]\n",
      "epoch:19 step:17969 [D loss: 0.630490, acc: 63.28%] [G loss: 1.844312]\n",
      "epoch:19 step:17970 [D loss: 0.591334, acc: 74.22%] [G loss: 1.943218]\n",
      "epoch:19 step:17971 [D loss: 0.682269, acc: 57.81%] [G loss: 1.853163]\n",
      "epoch:19 step:17972 [D loss: 0.651826, acc: 58.59%] [G loss: 1.860902]\n",
      "epoch:19 step:17973 [D loss: 0.662660, acc: 55.47%] [G loss: 1.847141]\n",
      "epoch:19 step:17974 [D loss: 0.687642, acc: 57.81%] [G loss: 1.875171]\n",
      "epoch:19 step:17975 [D loss: 0.692661, acc: 57.81%] [G loss: 1.714885]\n",
      "epoch:19 step:17976 [D loss: 0.644505, acc: 64.84%] [G loss: 1.820647]\n",
      "epoch:19 step:17977 [D loss: 0.698673, acc: 52.34%] [G loss: 1.659626]\n",
      "epoch:19 step:17978 [D loss: 0.691461, acc: 58.59%] [G loss: 1.679631]\n",
      "epoch:19 step:17979 [D loss: 0.626728, acc: 64.84%] [G loss: 1.793724]\n",
      "epoch:19 step:17980 [D loss: 0.672604, acc: 57.03%] [G loss: 1.807617]\n",
      "epoch:19 step:17981 [D loss: 0.642638, acc: 64.06%] [G loss: 1.837731]\n",
      "epoch:19 step:17982 [D loss: 0.698101, acc: 57.03%] [G loss: 1.750873]\n",
      "epoch:19 step:17983 [D loss: 0.670357, acc: 60.94%] [G loss: 1.749492]\n",
      "epoch:19 step:17984 [D loss: 0.685693, acc: 60.16%] [G loss: 1.823591]\n",
      "epoch:19 step:17985 [D loss: 0.659514, acc: 65.62%] [G loss: 1.790170]\n",
      "epoch:19 step:17986 [D loss: 0.630604, acc: 66.41%] [G loss: 1.723029]\n",
      "epoch:19 step:17987 [D loss: 0.584423, acc: 73.44%] [G loss: 1.911551]\n",
      "epoch:19 step:17988 [D loss: 0.684234, acc: 56.25%] [G loss: 1.826389]\n",
      "epoch:19 step:17989 [D loss: 0.697630, acc: 57.81%] [G loss: 1.844345]\n",
      "epoch:19 step:17990 [D loss: 0.669442, acc: 57.03%] [G loss: 1.783106]\n",
      "epoch:19 step:17991 [D loss: 0.659654, acc: 57.81%] [G loss: 1.834876]\n",
      "epoch:19 step:17992 [D loss: 0.675545, acc: 59.38%] [G loss: 1.794880]\n",
      "epoch:19 step:17993 [D loss: 0.701617, acc: 53.12%] [G loss: 1.838563]\n",
      "epoch:19 step:17994 [D loss: 0.631819, acc: 64.06%] [G loss: 1.871554]\n",
      "epoch:19 step:17995 [D loss: 0.617192, acc: 64.06%] [G loss: 1.983343]\n",
      "epoch:19 step:17996 [D loss: 0.629177, acc: 64.06%] [G loss: 2.103362]\n",
      "epoch:19 step:17997 [D loss: 0.623726, acc: 66.41%] [G loss: 1.900225]\n",
      "epoch:19 step:17998 [D loss: 0.647887, acc: 57.03%] [G loss: 1.930213]\n",
      "epoch:19 step:17999 [D loss: 0.650270, acc: 62.50%] [G loss: 1.822554]\n",
      "epoch:19 step:18000 [D loss: 0.654203, acc: 64.06%] [G loss: 1.925967]\n",
      "epoch:19 step:18001 [D loss: 0.572280, acc: 71.88%] [G loss: 1.993964]\n",
      "epoch:19 step:18002 [D loss: 0.678132, acc: 57.81%] [G loss: 1.885212]\n",
      "epoch:19 step:18003 [D loss: 0.669052, acc: 65.62%] [G loss: 1.603684]\n",
      "epoch:19 step:18004 [D loss: 0.649334, acc: 62.50%] [G loss: 1.990598]\n",
      "epoch:19 step:18005 [D loss: 0.658757, acc: 60.16%] [G loss: 2.032514]\n",
      "epoch:19 step:18006 [D loss: 0.620878, acc: 69.53%] [G loss: 1.815507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18007 [D loss: 0.717306, acc: 56.25%] [G loss: 1.850073]\n",
      "epoch:19 step:18008 [D loss: 0.683605, acc: 57.81%] [G loss: 1.830558]\n",
      "epoch:19 step:18009 [D loss: 0.583395, acc: 68.75%] [G loss: 1.995308]\n",
      "epoch:19 step:18010 [D loss: 0.651771, acc: 64.06%] [G loss: 1.993082]\n",
      "epoch:19 step:18011 [D loss: 0.597998, acc: 68.75%] [G loss: 1.992797]\n",
      "epoch:19 step:18012 [D loss: 0.632947, acc: 64.84%] [G loss: 2.113579]\n",
      "epoch:19 step:18013 [D loss: 0.609840, acc: 67.97%] [G loss: 1.795276]\n",
      "epoch:19 step:18014 [D loss: 0.667782, acc: 57.03%] [G loss: 1.710126]\n",
      "epoch:19 step:18015 [D loss: 0.650089, acc: 61.72%] [G loss: 1.858534]\n",
      "epoch:19 step:18016 [D loss: 0.636999, acc: 65.62%] [G loss: 1.700534]\n",
      "epoch:19 step:18017 [D loss: 0.653875, acc: 62.50%] [G loss: 1.928987]\n",
      "epoch:19 step:18018 [D loss: 0.666941, acc: 59.38%] [G loss: 1.888990]\n",
      "epoch:19 step:18019 [D loss: 0.642033, acc: 60.94%] [G loss: 2.021726]\n",
      "epoch:19 step:18020 [D loss: 0.657913, acc: 61.72%] [G loss: 1.881743]\n",
      "epoch:19 step:18021 [D loss: 0.680480, acc: 65.62%] [G loss: 1.909481]\n",
      "epoch:19 step:18022 [D loss: 0.617986, acc: 70.31%] [G loss: 1.982357]\n",
      "epoch:19 step:18023 [D loss: 0.704448, acc: 54.69%] [G loss: 1.761994]\n",
      "epoch:19 step:18024 [D loss: 0.724956, acc: 51.56%] [G loss: 2.016484]\n",
      "epoch:19 step:18025 [D loss: 0.620579, acc: 64.06%] [G loss: 1.982021]\n",
      "epoch:19 step:18026 [D loss: 0.611431, acc: 66.41%] [G loss: 1.969214]\n",
      "epoch:19 step:18027 [D loss: 0.633787, acc: 63.28%] [G loss: 1.788358]\n",
      "epoch:19 step:18028 [D loss: 0.605781, acc: 63.28%] [G loss: 1.812926]\n",
      "epoch:19 step:18029 [D loss: 0.672830, acc: 57.03%] [G loss: 1.852463]\n",
      "epoch:19 step:18030 [D loss: 0.593065, acc: 68.75%] [G loss: 1.751566]\n",
      "epoch:19 step:18031 [D loss: 0.679239, acc: 56.25%] [G loss: 1.786505]\n",
      "epoch:19 step:18032 [D loss: 0.649346, acc: 60.16%] [G loss: 2.057052]\n",
      "epoch:19 step:18033 [D loss: 0.613403, acc: 64.84%] [G loss: 2.119528]\n",
      "epoch:19 step:18034 [D loss: 0.583266, acc: 67.97%] [G loss: 2.325903]\n",
      "epoch:19 step:18035 [D loss: 0.597460, acc: 66.41%] [G loss: 2.177848]\n",
      "epoch:19 step:18036 [D loss: 0.727382, acc: 55.47%] [G loss: 1.932628]\n",
      "epoch:19 step:18037 [D loss: 0.647258, acc: 60.16%] [G loss: 1.894665]\n",
      "epoch:19 step:18038 [D loss: 0.669661, acc: 57.81%] [G loss: 1.896578]\n",
      "epoch:19 step:18039 [D loss: 0.649799, acc: 64.06%] [G loss: 1.913273]\n",
      "epoch:19 step:18040 [D loss: 0.653078, acc: 65.62%] [G loss: 1.980785]\n",
      "epoch:19 step:18041 [D loss: 0.678875, acc: 54.69%] [G loss: 1.998605]\n",
      "epoch:19 step:18042 [D loss: 0.614273, acc: 64.84%] [G loss: 1.920537]\n",
      "epoch:19 step:18043 [D loss: 0.639412, acc: 66.41%] [G loss: 1.978506]\n",
      "epoch:19 step:18044 [D loss: 0.565866, acc: 72.66%] [G loss: 1.902652]\n",
      "epoch:19 step:18045 [D loss: 0.638380, acc: 62.50%] [G loss: 2.066443]\n",
      "epoch:19 step:18046 [D loss: 0.721895, acc: 50.78%] [G loss: 1.864506]\n",
      "epoch:19 step:18047 [D loss: 0.637320, acc: 60.94%] [G loss: 1.792468]\n",
      "epoch:19 step:18048 [D loss: 0.678911, acc: 57.81%] [G loss: 1.851018]\n",
      "epoch:19 step:18049 [D loss: 0.597390, acc: 69.53%] [G loss: 1.861929]\n",
      "epoch:19 step:18050 [D loss: 0.649413, acc: 62.50%] [G loss: 1.745385]\n",
      "epoch:19 step:18051 [D loss: 0.639488, acc: 60.16%] [G loss: 1.954167]\n",
      "epoch:19 step:18052 [D loss: 0.676818, acc: 60.16%] [G loss: 1.880952]\n",
      "epoch:19 step:18053 [D loss: 0.662924, acc: 57.81%] [G loss: 1.828566]\n",
      "epoch:19 step:18054 [D loss: 0.672999, acc: 57.03%] [G loss: 1.777919]\n",
      "epoch:19 step:18055 [D loss: 0.651326, acc: 63.28%] [G loss: 1.764780]\n",
      "epoch:19 step:18056 [D loss: 0.710195, acc: 53.91%] [G loss: 1.725514]\n",
      "epoch:19 step:18057 [D loss: 0.602567, acc: 67.19%] [G loss: 1.725356]\n",
      "epoch:19 step:18058 [D loss: 0.636711, acc: 62.50%] [G loss: 1.857175]\n",
      "epoch:19 step:18059 [D loss: 0.648509, acc: 62.50%] [G loss: 1.984673]\n",
      "epoch:19 step:18060 [D loss: 0.667703, acc: 61.72%] [G loss: 1.837400]\n",
      "epoch:19 step:18061 [D loss: 0.633689, acc: 65.62%] [G loss: 1.874261]\n",
      "epoch:19 step:18062 [D loss: 0.626367, acc: 69.53%] [G loss: 1.840055]\n",
      "epoch:19 step:18063 [D loss: 0.662876, acc: 59.38%] [G loss: 1.845275]\n",
      "epoch:19 step:18064 [D loss: 0.661605, acc: 62.50%] [G loss: 1.928464]\n",
      "epoch:19 step:18065 [D loss: 0.612909, acc: 67.97%] [G loss: 1.836820]\n",
      "epoch:19 step:18066 [D loss: 0.669429, acc: 62.50%] [G loss: 1.877508]\n",
      "epoch:19 step:18067 [D loss: 0.620848, acc: 67.19%] [G loss: 1.903035]\n",
      "epoch:19 step:18068 [D loss: 0.658069, acc: 59.38%] [G loss: 1.961852]\n",
      "epoch:19 step:18069 [D loss: 0.639726, acc: 60.16%] [G loss: 1.868072]\n",
      "epoch:19 step:18070 [D loss: 0.667576, acc: 59.38%] [G loss: 1.810315]\n",
      "epoch:19 step:18071 [D loss: 0.631692, acc: 60.94%] [G loss: 1.806904]\n",
      "epoch:19 step:18072 [D loss: 0.654040, acc: 59.38%] [G loss: 1.855477]\n",
      "epoch:19 step:18073 [D loss: 0.653305, acc: 62.50%] [G loss: 1.939976]\n",
      "epoch:19 step:18074 [D loss: 0.608411, acc: 65.62%] [G loss: 1.943370]\n",
      "epoch:19 step:18075 [D loss: 0.656284, acc: 62.50%] [G loss: 2.046913]\n",
      "epoch:19 step:18076 [D loss: 0.644265, acc: 66.41%] [G loss: 1.976961]\n",
      "epoch:19 step:18077 [D loss: 0.652933, acc: 60.16%] [G loss: 1.958716]\n",
      "epoch:19 step:18078 [D loss: 0.606978, acc: 67.19%] [G loss: 1.866579]\n",
      "epoch:19 step:18079 [D loss: 0.609129, acc: 64.84%] [G loss: 2.254782]\n",
      "epoch:19 step:18080 [D loss: 0.638888, acc: 63.28%] [G loss: 1.779374]\n",
      "epoch:19 step:18081 [D loss: 0.667942, acc: 62.50%] [G loss: 1.810480]\n",
      "epoch:19 step:18082 [D loss: 0.668246, acc: 60.16%] [G loss: 1.757926]\n",
      "epoch:19 step:18083 [D loss: 0.614703, acc: 69.53%] [G loss: 2.199929]\n",
      "epoch:19 step:18084 [D loss: 0.694831, acc: 60.16%] [G loss: 1.863714]\n",
      "epoch:19 step:18085 [D loss: 0.631184, acc: 57.81%] [G loss: 1.977839]\n",
      "epoch:19 step:18086 [D loss: 0.656882, acc: 62.50%] [G loss: 1.872072]\n",
      "epoch:19 step:18087 [D loss: 0.619505, acc: 60.94%] [G loss: 1.808785]\n",
      "epoch:19 step:18088 [D loss: 0.724408, acc: 50.00%] [G loss: 1.814655]\n",
      "epoch:19 step:18089 [D loss: 0.604382, acc: 68.75%] [G loss: 1.929744]\n",
      "epoch:19 step:18090 [D loss: 0.672275, acc: 59.38%] [G loss: 1.893826]\n",
      "epoch:19 step:18091 [D loss: 0.698204, acc: 57.03%] [G loss: 1.884226]\n",
      "epoch:19 step:18092 [D loss: 0.672850, acc: 60.16%] [G loss: 1.921576]\n",
      "epoch:19 step:18093 [D loss: 0.632792, acc: 65.62%] [G loss: 1.817730]\n",
      "epoch:19 step:18094 [D loss: 0.709177, acc: 58.59%] [G loss: 1.899059]\n",
      "epoch:19 step:18095 [D loss: 0.664228, acc: 61.72%] [G loss: 1.888074]\n",
      "epoch:19 step:18096 [D loss: 0.660998, acc: 60.94%] [G loss: 1.885318]\n",
      "epoch:19 step:18097 [D loss: 0.617139, acc: 63.28%] [G loss: 1.773752]\n",
      "epoch:19 step:18098 [D loss: 0.687658, acc: 54.69%] [G loss: 1.836833]\n",
      "epoch:19 step:18099 [D loss: 0.682142, acc: 60.94%] [G loss: 1.974649]\n",
      "epoch:19 step:18100 [D loss: 0.624102, acc: 62.50%] [G loss: 1.938191]\n",
      "epoch:19 step:18101 [D loss: 0.668779, acc: 65.62%] [G loss: 1.918559]\n",
      "epoch:19 step:18102 [D loss: 0.659525, acc: 64.06%] [G loss: 1.912934]\n",
      "epoch:19 step:18103 [D loss: 0.630459, acc: 66.41%] [G loss: 1.857670]\n",
      "epoch:19 step:18104 [D loss: 0.656606, acc: 60.16%] [G loss: 1.796445]\n",
      "epoch:19 step:18105 [D loss: 0.635939, acc: 58.59%] [G loss: 1.914587]\n",
      "epoch:19 step:18106 [D loss: 0.647449, acc: 62.50%] [G loss: 1.761031]\n",
      "epoch:19 step:18107 [D loss: 0.657601, acc: 57.81%] [G loss: 1.804829]\n",
      "epoch:19 step:18108 [D loss: 0.646404, acc: 63.28%] [G loss: 1.771335]\n",
      "epoch:19 step:18109 [D loss: 0.653236, acc: 59.38%] [G loss: 1.751638]\n",
      "epoch:19 step:18110 [D loss: 0.607519, acc: 66.41%] [G loss: 1.858191]\n",
      "epoch:19 step:18111 [D loss: 0.626562, acc: 65.62%] [G loss: 1.897485]\n",
      "epoch:19 step:18112 [D loss: 0.628479, acc: 63.28%] [G loss: 1.837593]\n",
      "epoch:19 step:18113 [D loss: 0.618568, acc: 67.19%] [G loss: 1.899933]\n",
      "epoch:19 step:18114 [D loss: 0.630311, acc: 64.84%] [G loss: 1.906507]\n",
      "epoch:19 step:18115 [D loss: 0.610875, acc: 63.28%] [G loss: 2.134050]\n",
      "epoch:19 step:18116 [D loss: 0.618003, acc: 69.53%] [G loss: 2.120261]\n",
      "epoch:19 step:18117 [D loss: 0.615148, acc: 67.19%] [G loss: 2.005365]\n",
      "epoch:19 step:18118 [D loss: 0.648114, acc: 61.72%] [G loss: 2.132019]\n",
      "epoch:19 step:18119 [D loss: 0.732061, acc: 55.47%] [G loss: 1.751017]\n",
      "epoch:19 step:18120 [D loss: 0.668368, acc: 61.72%] [G loss: 1.790462]\n",
      "epoch:19 step:18121 [D loss: 0.608303, acc: 64.06%] [G loss: 1.972271]\n",
      "epoch:19 step:18122 [D loss: 0.643328, acc: 61.72%] [G loss: 1.756960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18123 [D loss: 0.579857, acc: 68.75%] [G loss: 1.911165]\n",
      "epoch:19 step:18124 [D loss: 0.686084, acc: 57.81%] [G loss: 1.905786]\n",
      "epoch:19 step:18125 [D loss: 0.651488, acc: 62.50%] [G loss: 1.792118]\n",
      "epoch:19 step:18126 [D loss: 0.655082, acc: 58.59%] [G loss: 1.835717]\n",
      "epoch:19 step:18127 [D loss: 0.622887, acc: 67.19%] [G loss: 1.772149]\n",
      "epoch:19 step:18128 [D loss: 0.652313, acc: 62.50%] [G loss: 1.917406]\n",
      "epoch:19 step:18129 [D loss: 0.666367, acc: 60.94%] [G loss: 1.780516]\n",
      "epoch:19 step:18130 [D loss: 0.640248, acc: 64.84%] [G loss: 1.841214]\n",
      "epoch:19 step:18131 [D loss: 0.650420, acc: 60.16%] [G loss: 1.930586]\n",
      "epoch:19 step:18132 [D loss: 0.612246, acc: 60.94%] [G loss: 1.958313]\n",
      "epoch:19 step:18133 [D loss: 0.662291, acc: 61.72%] [G loss: 1.927474]\n",
      "epoch:19 step:18134 [D loss: 0.658056, acc: 57.81%] [G loss: 1.835884]\n",
      "epoch:19 step:18135 [D loss: 0.614892, acc: 64.06%] [G loss: 1.962383]\n",
      "epoch:19 step:18136 [D loss: 0.661955, acc: 66.41%] [G loss: 1.939414]\n",
      "epoch:19 step:18137 [D loss: 0.666469, acc: 63.28%] [G loss: 1.900494]\n",
      "epoch:19 step:18138 [D loss: 0.628498, acc: 70.31%] [G loss: 1.896117]\n",
      "epoch:19 step:18139 [D loss: 0.665863, acc: 62.50%] [G loss: 1.928888]\n",
      "epoch:19 step:18140 [D loss: 0.599753, acc: 71.09%] [G loss: 2.044333]\n",
      "epoch:19 step:18141 [D loss: 0.584606, acc: 71.09%] [G loss: 2.005802]\n",
      "epoch:19 step:18142 [D loss: 0.623702, acc: 68.75%] [G loss: 1.918439]\n",
      "epoch:19 step:18143 [D loss: 0.674791, acc: 60.16%] [G loss: 1.963186]\n",
      "epoch:19 step:18144 [D loss: 0.709026, acc: 55.47%] [G loss: 1.718978]\n",
      "epoch:19 step:18145 [D loss: 0.708357, acc: 53.91%] [G loss: 1.778644]\n",
      "epoch:19 step:18146 [D loss: 0.650705, acc: 57.81%] [G loss: 1.789526]\n",
      "epoch:19 step:18147 [D loss: 0.640942, acc: 60.16%] [G loss: 1.913236]\n",
      "epoch:19 step:18148 [D loss: 0.603002, acc: 66.41%] [G loss: 2.073447]\n",
      "epoch:19 step:18149 [D loss: 0.611899, acc: 67.19%] [G loss: 1.965910]\n",
      "epoch:19 step:18150 [D loss: 0.572638, acc: 68.75%] [G loss: 2.048583]\n",
      "epoch:19 step:18151 [D loss: 0.669181, acc: 62.50%] [G loss: 1.762002]\n",
      "epoch:19 step:18152 [D loss: 0.696254, acc: 55.47%] [G loss: 1.765296]\n",
      "epoch:19 step:18153 [D loss: 0.671930, acc: 58.59%] [G loss: 1.932102]\n",
      "epoch:19 step:18154 [D loss: 0.676966, acc: 54.69%] [G loss: 1.714778]\n",
      "epoch:19 step:18155 [D loss: 0.643419, acc: 57.81%] [G loss: 1.851178]\n",
      "epoch:19 step:18156 [D loss: 0.632786, acc: 63.28%] [G loss: 1.890668]\n",
      "epoch:19 step:18157 [D loss: 0.589624, acc: 69.53%] [G loss: 2.083961]\n",
      "epoch:19 step:18158 [D loss: 0.706625, acc: 55.47%] [G loss: 1.761355]\n",
      "epoch:19 step:18159 [D loss: 0.671093, acc: 60.94%] [G loss: 1.844568]\n",
      "epoch:19 step:18160 [D loss: 0.623135, acc: 66.41%] [G loss: 1.909097]\n",
      "epoch:19 step:18161 [D loss: 0.683238, acc: 61.72%] [G loss: 1.888969]\n",
      "epoch:19 step:18162 [D loss: 0.631156, acc: 62.50%] [G loss: 1.930186]\n",
      "epoch:19 step:18163 [D loss: 0.633353, acc: 62.50%] [G loss: 1.851097]\n",
      "epoch:19 step:18164 [D loss: 0.617751, acc: 69.53%] [G loss: 1.836654]\n",
      "epoch:19 step:18165 [D loss: 0.646087, acc: 58.59%] [G loss: 1.789603]\n",
      "epoch:19 step:18166 [D loss: 0.642678, acc: 60.94%] [G loss: 1.908960]\n",
      "epoch:19 step:18167 [D loss: 0.629958, acc: 60.94%] [G loss: 1.869045]\n",
      "epoch:19 step:18168 [D loss: 0.654138, acc: 63.28%] [G loss: 1.893145]\n",
      "epoch:19 step:18169 [D loss: 0.619860, acc: 64.06%] [G loss: 1.865762]\n",
      "epoch:19 step:18170 [D loss: 0.583686, acc: 74.22%] [G loss: 1.989028]\n",
      "epoch:19 step:18171 [D loss: 0.650163, acc: 62.50%] [G loss: 1.926089]\n",
      "epoch:19 step:18172 [D loss: 0.605134, acc: 64.84%] [G loss: 1.974280]\n",
      "epoch:19 step:18173 [D loss: 0.644551, acc: 62.50%] [G loss: 1.992036]\n",
      "epoch:19 step:18174 [D loss: 0.655763, acc: 61.72%] [G loss: 1.900651]\n",
      "epoch:19 step:18175 [D loss: 0.672920, acc: 60.94%] [G loss: 1.943618]\n",
      "epoch:19 step:18176 [D loss: 0.642370, acc: 64.84%] [G loss: 1.738836]\n",
      "epoch:19 step:18177 [D loss: 0.630152, acc: 63.28%] [G loss: 2.141907]\n",
      "epoch:19 step:18178 [D loss: 0.643356, acc: 62.50%] [G loss: 1.877479]\n",
      "epoch:19 step:18179 [D loss: 0.715120, acc: 54.69%] [G loss: 1.824096]\n",
      "epoch:19 step:18180 [D loss: 0.694422, acc: 50.00%] [G loss: 1.736457]\n",
      "epoch:19 step:18181 [D loss: 0.681578, acc: 60.94%] [G loss: 1.793248]\n",
      "epoch:19 step:18182 [D loss: 0.606112, acc: 69.53%] [G loss: 1.897953]\n",
      "epoch:19 step:18183 [D loss: 0.598297, acc: 67.97%] [G loss: 1.988497]\n",
      "epoch:19 step:18184 [D loss: 0.616967, acc: 62.50%] [G loss: 1.959297]\n",
      "epoch:19 step:18185 [D loss: 0.603204, acc: 67.97%] [G loss: 1.881121]\n",
      "epoch:19 step:18186 [D loss: 0.612984, acc: 64.84%] [G loss: 1.964512]\n",
      "epoch:19 step:18187 [D loss: 0.646527, acc: 64.84%] [G loss: 1.835610]\n",
      "epoch:19 step:18188 [D loss: 0.559755, acc: 76.56%] [G loss: 2.083896]\n",
      "epoch:19 step:18189 [D loss: 0.653473, acc: 61.72%] [G loss: 1.761279]\n",
      "epoch:19 step:18190 [D loss: 0.694706, acc: 56.25%] [G loss: 1.854263]\n",
      "epoch:19 step:18191 [D loss: 0.656300, acc: 57.81%] [G loss: 1.972125]\n",
      "epoch:19 step:18192 [D loss: 0.631027, acc: 68.75%] [G loss: 1.924049]\n",
      "epoch:19 step:18193 [D loss: 0.589539, acc: 69.53%] [G loss: 2.024877]\n",
      "epoch:19 step:18194 [D loss: 0.674605, acc: 57.03%] [G loss: 1.809147]\n",
      "epoch:19 step:18195 [D loss: 0.660553, acc: 62.50%] [G loss: 1.908880]\n",
      "epoch:19 step:18196 [D loss: 0.649416, acc: 64.06%] [G loss: 1.785829]\n",
      "epoch:19 step:18197 [D loss: 0.623537, acc: 64.84%] [G loss: 1.743289]\n",
      "epoch:19 step:18198 [D loss: 0.644681, acc: 61.72%] [G loss: 1.962944]\n",
      "epoch:19 step:18199 [D loss: 0.669236, acc: 57.81%] [G loss: 1.941637]\n",
      "epoch:19 step:18200 [D loss: 0.665531, acc: 59.38%] [G loss: 1.800349]\n",
      "epoch:19 step:18201 [D loss: 0.660136, acc: 65.62%] [G loss: 1.926470]\n",
      "epoch:19 step:18202 [D loss: 0.631980, acc: 67.97%] [G loss: 1.737433]\n",
      "epoch:19 step:18203 [D loss: 0.684831, acc: 56.25%] [G loss: 1.858211]\n",
      "epoch:19 step:18204 [D loss: 0.632509, acc: 67.97%] [G loss: 1.889494]\n",
      "epoch:19 step:18205 [D loss: 0.618299, acc: 64.84%] [G loss: 1.943392]\n",
      "epoch:19 step:18206 [D loss: 0.666998, acc: 58.59%] [G loss: 2.041182]\n",
      "epoch:19 step:18207 [D loss: 0.658733, acc: 58.59%] [G loss: 1.876836]\n",
      "epoch:19 step:18208 [D loss: 0.627746, acc: 65.62%] [G loss: 2.265458]\n",
      "epoch:19 step:18209 [D loss: 0.625802, acc: 64.84%] [G loss: 2.123143]\n",
      "epoch:19 step:18210 [D loss: 0.595562, acc: 68.75%] [G loss: 1.984984]\n",
      "epoch:19 step:18211 [D loss: 0.666423, acc: 60.94%] [G loss: 1.932487]\n",
      "epoch:19 step:18212 [D loss: 0.667068, acc: 59.38%] [G loss: 1.905415]\n",
      "epoch:19 step:18213 [D loss: 0.648007, acc: 60.94%] [G loss: 1.951441]\n",
      "epoch:19 step:18214 [D loss: 0.660327, acc: 62.50%] [G loss: 1.835385]\n",
      "epoch:19 step:18215 [D loss: 0.634312, acc: 62.50%] [G loss: 1.891589]\n",
      "epoch:19 step:18216 [D loss: 0.649581, acc: 60.16%] [G loss: 1.934173]\n",
      "epoch:19 step:18217 [D loss: 0.585231, acc: 69.53%] [G loss: 1.987212]\n",
      "epoch:19 step:18218 [D loss: 0.612741, acc: 74.22%] [G loss: 2.027881]\n",
      "epoch:19 step:18219 [D loss: 0.640207, acc: 61.72%] [G loss: 2.057021]\n",
      "epoch:19 step:18220 [D loss: 0.705251, acc: 59.38%] [G loss: 1.957383]\n",
      "epoch:19 step:18221 [D loss: 0.667405, acc: 58.59%] [G loss: 1.780806]\n",
      "epoch:19 step:18222 [D loss: 0.712156, acc: 50.00%] [G loss: 1.982449]\n",
      "epoch:19 step:18223 [D loss: 0.651871, acc: 61.72%] [G loss: 1.844437]\n",
      "epoch:19 step:18224 [D loss: 0.701015, acc: 53.91%] [G loss: 1.762212]\n",
      "epoch:19 step:18225 [D loss: 0.705490, acc: 60.94%] [G loss: 1.841609]\n",
      "epoch:19 step:18226 [D loss: 0.698406, acc: 55.47%] [G loss: 1.818151]\n",
      "epoch:19 step:18227 [D loss: 0.718475, acc: 48.44%] [G loss: 1.780116]\n",
      "epoch:19 step:18228 [D loss: 0.656266, acc: 60.94%] [G loss: 1.776298]\n",
      "epoch:19 step:18229 [D loss: 0.661603, acc: 66.41%] [G loss: 1.916078]\n",
      "epoch:19 step:18230 [D loss: 0.583687, acc: 72.66%] [G loss: 2.045466]\n",
      "epoch:19 step:18231 [D loss: 0.625303, acc: 64.84%] [G loss: 1.932244]\n",
      "epoch:19 step:18232 [D loss: 0.567064, acc: 71.88%] [G loss: 1.932828]\n",
      "epoch:19 step:18233 [D loss: 0.590732, acc: 65.62%] [G loss: 2.013433]\n",
      "epoch:19 step:18234 [D loss: 0.626720, acc: 66.41%] [G loss: 1.935880]\n",
      "epoch:19 step:18235 [D loss: 0.622151, acc: 63.28%] [G loss: 1.893173]\n",
      "epoch:19 step:18236 [D loss: 0.674581, acc: 60.16%] [G loss: 1.824799]\n",
      "epoch:19 step:18237 [D loss: 0.668631, acc: 63.28%] [G loss: 1.846006]\n",
      "epoch:19 step:18238 [D loss: 0.623322, acc: 68.75%] [G loss: 2.051284]\n",
      "epoch:19 step:18239 [D loss: 0.598096, acc: 64.84%] [G loss: 1.915319]\n",
      "epoch:19 step:18240 [D loss: 0.696568, acc: 52.34%] [G loss: 1.781352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18241 [D loss: 0.712700, acc: 53.12%] [G loss: 1.779829]\n",
      "epoch:19 step:18242 [D loss: 0.674174, acc: 56.25%] [G loss: 1.787883]\n",
      "epoch:19 step:18243 [D loss: 0.674072, acc: 59.38%] [G loss: 1.694784]\n",
      "epoch:19 step:18244 [D loss: 0.683965, acc: 58.59%] [G loss: 1.751021]\n",
      "epoch:19 step:18245 [D loss: 0.659636, acc: 58.59%] [G loss: 1.803220]\n",
      "epoch:19 step:18246 [D loss: 0.706535, acc: 52.34%] [G loss: 1.802269]\n",
      "epoch:19 step:18247 [D loss: 0.650562, acc: 60.16%] [G loss: 1.929573]\n",
      "epoch:19 step:18248 [D loss: 0.660610, acc: 65.62%] [G loss: 1.814461]\n",
      "epoch:19 step:18249 [D loss: 0.652149, acc: 60.16%] [G loss: 1.722351]\n",
      "epoch:19 step:18250 [D loss: 0.667784, acc: 57.03%] [G loss: 1.730732]\n",
      "epoch:19 step:18251 [D loss: 0.687152, acc: 56.25%] [G loss: 1.731963]\n",
      "epoch:19 step:18252 [D loss: 0.639493, acc: 60.16%] [G loss: 1.853130]\n",
      "epoch:19 step:18253 [D loss: 0.711076, acc: 52.34%] [G loss: 1.718783]\n",
      "epoch:19 step:18254 [D loss: 0.657607, acc: 56.25%] [G loss: 1.757124]\n",
      "epoch:19 step:18255 [D loss: 0.657658, acc: 60.16%] [G loss: 1.868711]\n",
      "epoch:19 step:18256 [D loss: 0.661369, acc: 61.72%] [G loss: 1.922599]\n",
      "epoch:19 step:18257 [D loss: 0.612022, acc: 67.19%] [G loss: 1.837652]\n",
      "epoch:19 step:18258 [D loss: 0.641706, acc: 59.38%] [G loss: 1.844928]\n",
      "epoch:19 step:18259 [D loss: 0.643615, acc: 63.28%] [G loss: 1.853644]\n",
      "epoch:19 step:18260 [D loss: 0.601814, acc: 66.41%] [G loss: 1.962920]\n",
      "epoch:19 step:18261 [D loss: 0.669655, acc: 57.81%] [G loss: 1.710749]\n",
      "epoch:19 step:18262 [D loss: 0.668368, acc: 57.03%] [G loss: 1.773831]\n",
      "epoch:19 step:18263 [D loss: 0.684670, acc: 57.03%] [G loss: 1.782237]\n",
      "epoch:19 step:18264 [D loss: 0.639722, acc: 63.28%] [G loss: 1.869811]\n",
      "epoch:19 step:18265 [D loss: 0.654244, acc: 59.38%] [G loss: 1.890008]\n",
      "epoch:19 step:18266 [D loss: 0.672484, acc: 56.25%] [G loss: 1.850893]\n",
      "epoch:19 step:18267 [D loss: 0.653446, acc: 59.38%] [G loss: 1.877727]\n",
      "epoch:19 step:18268 [D loss: 0.646480, acc: 63.28%] [G loss: 1.810527]\n",
      "epoch:19 step:18269 [D loss: 0.664237, acc: 60.16%] [G loss: 1.723548]\n",
      "epoch:19 step:18270 [D loss: 0.611112, acc: 65.62%] [G loss: 1.862304]\n",
      "epoch:19 step:18271 [D loss: 0.673348, acc: 62.50%] [G loss: 2.003516]\n",
      "epoch:19 step:18272 [D loss: 0.640346, acc: 62.50%] [G loss: 2.165259]\n",
      "epoch:19 step:18273 [D loss: 0.632497, acc: 58.59%] [G loss: 1.966788]\n",
      "epoch:19 step:18274 [D loss: 0.563960, acc: 78.12%] [G loss: 2.189133]\n",
      "epoch:19 step:18275 [D loss: 0.559533, acc: 75.00%] [G loss: 1.979814]\n",
      "epoch:19 step:18276 [D loss: 0.683178, acc: 60.16%] [G loss: 1.796172]\n",
      "epoch:19 step:18277 [D loss: 0.616797, acc: 67.97%] [G loss: 2.080973]\n",
      "epoch:19 step:18278 [D loss: 0.652098, acc: 60.16%] [G loss: 1.829195]\n",
      "epoch:19 step:18279 [D loss: 0.688555, acc: 57.81%] [G loss: 2.116271]\n",
      "epoch:19 step:18280 [D loss: 0.652914, acc: 60.94%] [G loss: 1.799788]\n",
      "epoch:19 step:18281 [D loss: 0.675831, acc: 57.03%] [G loss: 1.781035]\n",
      "epoch:19 step:18282 [D loss: 0.651025, acc: 64.06%] [G loss: 1.919580]\n",
      "epoch:19 step:18283 [D loss: 0.677257, acc: 52.34%] [G loss: 1.916314]\n",
      "epoch:19 step:18284 [D loss: 0.602008, acc: 69.53%] [G loss: 1.986556]\n",
      "epoch:19 step:18285 [D loss: 0.643253, acc: 55.47%] [G loss: 1.781293]\n",
      "epoch:19 step:18286 [D loss: 0.632585, acc: 67.19%] [G loss: 1.891056]\n",
      "epoch:19 step:18287 [D loss: 0.649112, acc: 65.62%] [G loss: 2.026933]\n",
      "epoch:19 step:18288 [D loss: 0.628625, acc: 64.06%] [G loss: 1.873892]\n",
      "epoch:19 step:18289 [D loss: 0.664125, acc: 63.28%] [G loss: 1.836974]\n",
      "epoch:19 step:18290 [D loss: 0.598245, acc: 68.75%] [G loss: 2.054872]\n",
      "epoch:19 step:18291 [D loss: 0.617671, acc: 70.31%] [G loss: 2.069032]\n",
      "epoch:19 step:18292 [D loss: 0.638038, acc: 64.06%] [G loss: 1.969282]\n",
      "epoch:19 step:18293 [D loss: 0.667160, acc: 63.28%] [G loss: 1.884048]\n",
      "epoch:19 step:18294 [D loss: 0.565164, acc: 74.22%] [G loss: 2.105963]\n",
      "epoch:19 step:18295 [D loss: 0.710340, acc: 55.47%] [G loss: 1.862202]\n",
      "epoch:19 step:18296 [D loss: 0.645077, acc: 61.72%] [G loss: 1.995013]\n",
      "epoch:19 step:18297 [D loss: 0.637287, acc: 58.59%] [G loss: 1.905047]\n",
      "epoch:19 step:18298 [D loss: 0.579425, acc: 70.31%] [G loss: 2.078021]\n",
      "epoch:19 step:18299 [D loss: 0.659984, acc: 63.28%] [G loss: 1.909096]\n",
      "epoch:19 step:18300 [D loss: 0.631928, acc: 65.62%] [G loss: 2.017145]\n",
      "epoch:19 step:18301 [D loss: 0.620302, acc: 69.53%] [G loss: 1.920119]\n",
      "epoch:19 step:18302 [D loss: 0.628096, acc: 64.06%] [G loss: 1.992073]\n",
      "epoch:19 step:18303 [D loss: 0.712651, acc: 56.25%] [G loss: 1.689834]\n",
      "epoch:19 step:18304 [D loss: 0.703802, acc: 51.56%] [G loss: 1.724062]\n",
      "epoch:19 step:18305 [D loss: 0.673107, acc: 56.25%] [G loss: 1.646453]\n",
      "epoch:19 step:18306 [D loss: 0.712131, acc: 60.94%] [G loss: 1.917434]\n",
      "epoch:19 step:18307 [D loss: 0.652112, acc: 64.06%] [G loss: 1.830947]\n",
      "epoch:19 step:18308 [D loss: 0.653836, acc: 61.72%] [G loss: 1.724862]\n",
      "epoch:19 step:18309 [D loss: 0.674237, acc: 55.47%] [G loss: 1.744987]\n",
      "epoch:19 step:18310 [D loss: 0.700213, acc: 54.69%] [G loss: 1.843252]\n",
      "epoch:19 step:18311 [D loss: 0.611515, acc: 69.53%] [G loss: 2.018747]\n",
      "epoch:19 step:18312 [D loss: 0.585251, acc: 69.53%] [G loss: 1.874029]\n",
      "epoch:19 step:18313 [D loss: 0.713353, acc: 58.59%] [G loss: 1.779486]\n",
      "epoch:19 step:18314 [D loss: 0.673020, acc: 60.94%] [G loss: 1.686335]\n",
      "epoch:19 step:18315 [D loss: 0.649236, acc: 61.72%] [G loss: 1.773639]\n",
      "epoch:19 step:18316 [D loss: 0.650500, acc: 57.81%] [G loss: 1.839881]\n",
      "epoch:19 step:18317 [D loss: 0.638766, acc: 64.84%] [G loss: 1.848799]\n",
      "epoch:19 step:18318 [D loss: 0.587946, acc: 71.88%] [G loss: 1.886100]\n",
      "epoch:19 step:18319 [D loss: 0.645631, acc: 63.28%] [G loss: 1.860875]\n",
      "epoch:19 step:18320 [D loss: 0.641517, acc: 61.72%] [G loss: 2.077747]\n",
      "epoch:19 step:18321 [D loss: 0.679927, acc: 64.06%] [G loss: 1.778044]\n",
      "epoch:19 step:18322 [D loss: 0.633280, acc: 62.50%] [G loss: 1.763804]\n",
      "epoch:19 step:18323 [D loss: 0.609717, acc: 63.28%] [G loss: 2.041285]\n",
      "epoch:19 step:18324 [D loss: 0.621401, acc: 64.84%] [G loss: 1.919665]\n",
      "epoch:19 step:18325 [D loss: 0.613483, acc: 67.19%] [G loss: 2.065903]\n",
      "epoch:19 step:18326 [D loss: 0.604026, acc: 65.62%] [G loss: 2.048367]\n",
      "epoch:19 step:18327 [D loss: 0.653737, acc: 60.16%] [G loss: 1.875458]\n",
      "epoch:19 step:18328 [D loss: 0.706807, acc: 50.78%] [G loss: 1.982483]\n",
      "epoch:19 step:18329 [D loss: 0.635375, acc: 62.50%] [G loss: 1.897221]\n",
      "epoch:19 step:18330 [D loss: 0.675086, acc: 53.12%] [G loss: 1.748709]\n",
      "epoch:19 step:18331 [D loss: 0.600500, acc: 73.44%] [G loss: 1.770678]\n",
      "epoch:19 step:18332 [D loss: 0.627687, acc: 60.94%] [G loss: 1.739530]\n",
      "epoch:19 step:18333 [D loss: 0.623950, acc: 69.53%] [G loss: 1.707315]\n",
      "epoch:19 step:18334 [D loss: 0.645604, acc: 60.16%] [G loss: 1.829741]\n",
      "epoch:19 step:18335 [D loss: 0.642210, acc: 64.84%] [G loss: 2.015956]\n",
      "epoch:19 step:18336 [D loss: 0.648838, acc: 61.72%] [G loss: 1.920524]\n",
      "epoch:19 step:18337 [D loss: 0.585572, acc: 71.09%] [G loss: 2.040918]\n",
      "epoch:19 step:18338 [D loss: 0.684309, acc: 54.69%] [G loss: 1.821643]\n",
      "epoch:19 step:18339 [D loss: 0.651597, acc: 60.94%] [G loss: 1.955151]\n",
      "epoch:19 step:18340 [D loss: 0.614148, acc: 67.97%] [G loss: 1.783898]\n",
      "epoch:19 step:18341 [D loss: 0.667815, acc: 59.38%] [G loss: 1.801738]\n",
      "epoch:19 step:18342 [D loss: 0.672410, acc: 61.72%] [G loss: 1.905800]\n",
      "epoch:19 step:18343 [D loss: 0.662232, acc: 53.91%] [G loss: 1.969817]\n",
      "epoch:19 step:18344 [D loss: 0.692677, acc: 52.34%] [G loss: 1.834761]\n",
      "epoch:19 step:18345 [D loss: 0.649476, acc: 61.72%] [G loss: 1.851607]\n",
      "epoch:19 step:18346 [D loss: 0.681990, acc: 58.59%] [G loss: 1.866773]\n",
      "epoch:19 step:18347 [D loss: 0.700140, acc: 56.25%] [G loss: 1.925584]\n",
      "epoch:19 step:18348 [D loss: 0.652577, acc: 59.38%] [G loss: 1.849657]\n",
      "epoch:19 step:18349 [D loss: 0.694847, acc: 51.56%] [G loss: 1.764169]\n",
      "epoch:19 step:18350 [D loss: 0.614838, acc: 67.97%] [G loss: 1.900102]\n",
      "epoch:19 step:18351 [D loss: 0.636644, acc: 58.59%] [G loss: 1.885437]\n",
      "epoch:19 step:18352 [D loss: 0.615329, acc: 73.44%] [G loss: 1.862539]\n",
      "epoch:19 step:18353 [D loss: 0.652533, acc: 60.94%] [G loss: 1.872488]\n",
      "epoch:19 step:18354 [D loss: 0.587308, acc: 73.44%] [G loss: 1.961887]\n",
      "epoch:19 step:18355 [D loss: 0.626853, acc: 63.28%] [G loss: 1.966894]\n",
      "epoch:19 step:18356 [D loss: 0.664465, acc: 62.50%] [G loss: 1.831658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18357 [D loss: 0.573736, acc: 71.88%] [G loss: 2.087268]\n",
      "epoch:19 step:18358 [D loss: 0.680035, acc: 56.25%] [G loss: 1.791597]\n",
      "epoch:19 step:18359 [D loss: 0.635904, acc: 60.94%] [G loss: 1.999804]\n",
      "epoch:19 step:18360 [D loss: 0.631879, acc: 64.84%] [G loss: 2.085323]\n",
      "epoch:19 step:18361 [D loss: 0.643103, acc: 63.28%] [G loss: 1.872303]\n",
      "epoch:19 step:18362 [D loss: 0.667964, acc: 54.69%] [G loss: 1.936848]\n",
      "epoch:19 step:18363 [D loss: 0.634101, acc: 60.94%] [G loss: 1.850701]\n",
      "epoch:19 step:18364 [D loss: 0.599142, acc: 70.31%] [G loss: 1.945194]\n",
      "epoch:19 step:18365 [D loss: 0.633346, acc: 65.62%] [G loss: 1.895872]\n",
      "epoch:19 step:18366 [D loss: 0.625275, acc: 62.50%] [G loss: 1.922811]\n",
      "epoch:19 step:18367 [D loss: 0.592391, acc: 69.53%] [G loss: 2.050007]\n",
      "epoch:19 step:18368 [D loss: 0.679232, acc: 58.59%] [G loss: 1.782359]\n",
      "epoch:19 step:18369 [D loss: 0.719962, acc: 58.59%] [G loss: 1.618522]\n",
      "epoch:19 step:18370 [D loss: 0.649854, acc: 65.62%] [G loss: 1.823188]\n",
      "epoch:19 step:18371 [D loss: 0.636204, acc: 62.50%] [G loss: 1.778830]\n",
      "epoch:19 step:18372 [D loss: 0.659458, acc: 57.81%] [G loss: 1.882266]\n",
      "epoch:19 step:18373 [D loss: 0.605291, acc: 70.31%] [G loss: 1.876889]\n",
      "epoch:19 step:18374 [D loss: 0.605856, acc: 68.75%] [G loss: 1.843934]\n",
      "epoch:19 step:18375 [D loss: 0.658016, acc: 57.03%] [G loss: 1.709036]\n",
      "epoch:19 step:18376 [D loss: 0.692631, acc: 56.25%] [G loss: 1.749742]\n",
      "epoch:19 step:18377 [D loss: 0.643094, acc: 64.06%] [G loss: 1.970117]\n",
      "epoch:19 step:18378 [D loss: 0.665430, acc: 64.84%] [G loss: 1.818534]\n",
      "epoch:19 step:18379 [D loss: 0.665489, acc: 60.94%] [G loss: 1.797134]\n",
      "epoch:19 step:18380 [D loss: 0.673544, acc: 57.81%] [G loss: 1.773861]\n",
      "epoch:19 step:18381 [D loss: 0.602025, acc: 72.66%] [G loss: 1.792762]\n",
      "epoch:19 step:18382 [D loss: 0.660336, acc: 60.16%] [G loss: 1.740633]\n",
      "epoch:19 step:18383 [D loss: 0.670773, acc: 62.50%] [G loss: 1.872899]\n",
      "epoch:19 step:18384 [D loss: 0.628872, acc: 64.84%] [G loss: 1.975706]\n",
      "epoch:19 step:18385 [D loss: 0.602936, acc: 74.22%] [G loss: 1.969457]\n",
      "epoch:19 step:18386 [D loss: 0.637652, acc: 64.84%] [G loss: 1.899493]\n",
      "epoch:19 step:18387 [D loss: 0.611452, acc: 65.62%] [G loss: 1.876196]\n",
      "epoch:19 step:18388 [D loss: 0.648965, acc: 58.59%] [G loss: 1.874077]\n",
      "epoch:19 step:18389 [D loss: 0.656947, acc: 57.81%] [G loss: 1.968861]\n",
      "epoch:19 step:18390 [D loss: 0.650770, acc: 63.28%] [G loss: 1.903395]\n",
      "epoch:19 step:18391 [D loss: 0.633191, acc: 54.69%] [G loss: 2.048073]\n",
      "epoch:19 step:18392 [D loss: 0.617921, acc: 65.62%] [G loss: 2.014075]\n",
      "epoch:19 step:18393 [D loss: 0.623930, acc: 63.28%] [G loss: 2.039330]\n",
      "epoch:19 step:18394 [D loss: 0.652587, acc: 60.94%] [G loss: 1.780981]\n",
      "epoch:19 step:18395 [D loss: 0.641785, acc: 66.41%] [G loss: 1.818805]\n",
      "epoch:19 step:18396 [D loss: 0.648128, acc: 64.06%] [G loss: 1.866217]\n",
      "epoch:19 step:18397 [D loss: 0.622668, acc: 66.41%] [G loss: 1.892470]\n",
      "epoch:19 step:18398 [D loss: 0.668219, acc: 60.94%] [G loss: 1.893962]\n",
      "epoch:19 step:18399 [D loss: 0.632022, acc: 64.84%] [G loss: 1.913578]\n",
      "epoch:19 step:18400 [D loss: 0.711788, acc: 54.69%] [G loss: 1.884214]\n",
      "epoch:19 step:18401 [D loss: 0.626460, acc: 66.41%] [G loss: 1.885275]\n",
      "epoch:19 step:18402 [D loss: 0.651048, acc: 59.38%] [G loss: 1.851518]\n",
      "epoch:19 step:18403 [D loss: 0.664229, acc: 52.34%] [G loss: 1.847789]\n",
      "epoch:19 step:18404 [D loss: 0.647110, acc: 63.28%] [G loss: 1.868007]\n",
      "epoch:19 step:18405 [D loss: 0.668613, acc: 58.59%] [G loss: 1.908303]\n",
      "epoch:19 step:18406 [D loss: 0.599753, acc: 65.62%] [G loss: 2.082738]\n",
      "epoch:19 step:18407 [D loss: 0.654414, acc: 60.94%] [G loss: 1.843602]\n",
      "epoch:19 step:18408 [D loss: 0.585144, acc: 73.44%] [G loss: 1.866052]\n",
      "epoch:19 step:18409 [D loss: 0.676780, acc: 63.28%] [G loss: 1.839895]\n",
      "epoch:19 step:18410 [D loss: 0.640447, acc: 59.38%] [G loss: 1.822245]\n",
      "epoch:19 step:18411 [D loss: 0.641892, acc: 58.59%] [G loss: 1.823503]\n",
      "epoch:19 step:18412 [D loss: 0.590717, acc: 66.41%] [G loss: 1.997364]\n",
      "epoch:19 step:18413 [D loss: 0.622201, acc: 68.75%] [G loss: 1.771288]\n",
      "epoch:19 step:18414 [D loss: 0.726707, acc: 52.34%] [G loss: 1.738569]\n",
      "epoch:19 step:18415 [D loss: 0.660610, acc: 64.06%] [G loss: 1.814416]\n",
      "epoch:19 step:18416 [D loss: 0.678600, acc: 60.16%] [G loss: 1.776193]\n",
      "epoch:19 step:18417 [D loss: 0.668564, acc: 62.50%] [G loss: 1.704612]\n",
      "epoch:19 step:18418 [D loss: 0.665812, acc: 59.38%] [G loss: 1.764382]\n",
      "epoch:19 step:18419 [D loss: 0.681659, acc: 60.94%] [G loss: 1.778012]\n",
      "epoch:19 step:18420 [D loss: 0.668439, acc: 61.72%] [G loss: 1.789433]\n",
      "epoch:19 step:18421 [D loss: 0.599808, acc: 69.53%] [G loss: 1.962183]\n",
      "epoch:19 step:18422 [D loss: 0.632178, acc: 65.62%] [G loss: 1.869355]\n",
      "epoch:19 step:18423 [D loss: 0.648451, acc: 61.72%] [G loss: 1.884749]\n",
      "epoch:19 step:18424 [D loss: 0.708895, acc: 55.47%] [G loss: 1.844239]\n",
      "epoch:19 step:18425 [D loss: 0.627331, acc: 71.09%] [G loss: 1.824532]\n",
      "epoch:19 step:18426 [D loss: 0.643105, acc: 60.94%] [G loss: 1.863157]\n",
      "epoch:19 step:18427 [D loss: 0.589397, acc: 70.31%] [G loss: 1.988927]\n",
      "epoch:19 step:18428 [D loss: 0.639901, acc: 68.75%] [G loss: 1.878809]\n",
      "epoch:19 step:18429 [D loss: 0.656033, acc: 60.16%] [G loss: 1.773580]\n",
      "epoch:19 step:18430 [D loss: 0.639972, acc: 65.62%] [G loss: 1.881579]\n",
      "epoch:19 step:18431 [D loss: 0.667736, acc: 57.03%] [G loss: 1.837263]\n",
      "epoch:19 step:18432 [D loss: 0.678235, acc: 53.12%] [G loss: 1.933158]\n",
      "epoch:19 step:18433 [D loss: 0.658148, acc: 59.38%] [G loss: 1.830067]\n",
      "epoch:19 step:18434 [D loss: 0.647378, acc: 60.94%] [G loss: 1.919874]\n",
      "epoch:19 step:18435 [D loss: 0.574427, acc: 72.66%] [G loss: 1.997566]\n",
      "epoch:19 step:18436 [D loss: 0.669210, acc: 57.81%] [G loss: 1.795025]\n",
      "epoch:19 step:18437 [D loss: 0.584628, acc: 69.53%] [G loss: 2.000986]\n",
      "epoch:19 step:18438 [D loss: 0.696732, acc: 59.38%] [G loss: 1.888611]\n",
      "epoch:19 step:18439 [D loss: 0.634702, acc: 63.28%] [G loss: 1.878789]\n",
      "epoch:19 step:18440 [D loss: 0.625991, acc: 67.97%] [G loss: 1.874892]\n",
      "epoch:19 step:18441 [D loss: 0.606097, acc: 70.31%] [G loss: 1.935386]\n",
      "epoch:19 step:18442 [D loss: 0.616421, acc: 67.97%] [G loss: 1.990257]\n",
      "epoch:19 step:18443 [D loss: 0.628789, acc: 64.84%] [G loss: 1.939395]\n",
      "epoch:19 step:18444 [D loss: 0.668240, acc: 60.94%] [G loss: 1.942106]\n",
      "epoch:19 step:18445 [D loss: 0.658185, acc: 58.59%] [G loss: 1.941582]\n",
      "epoch:19 step:18446 [D loss: 0.645773, acc: 63.28%] [G loss: 2.021657]\n",
      "epoch:19 step:18447 [D loss: 0.682009, acc: 56.25%] [G loss: 1.984708]\n",
      "epoch:19 step:18448 [D loss: 0.672672, acc: 60.16%] [G loss: 1.972406]\n",
      "epoch:19 step:18449 [D loss: 0.618314, acc: 64.84%] [G loss: 1.941532]\n",
      "epoch:19 step:18450 [D loss: 0.587441, acc: 69.53%] [G loss: 2.247060]\n",
      "epoch:19 step:18451 [D loss: 0.567039, acc: 71.88%] [G loss: 2.340701]\n",
      "epoch:19 step:18452 [D loss: 0.627244, acc: 64.84%] [G loss: 2.197036]\n",
      "epoch:19 step:18453 [D loss: 0.616238, acc: 68.75%] [G loss: 2.194312]\n",
      "epoch:19 step:18454 [D loss: 0.603157, acc: 65.62%] [G loss: 2.018786]\n",
      "epoch:19 step:18455 [D loss: 0.637577, acc: 63.28%] [G loss: 2.053371]\n",
      "epoch:19 step:18456 [D loss: 0.632643, acc: 68.75%] [G loss: 1.917296]\n",
      "epoch:19 step:18457 [D loss: 0.641728, acc: 63.28%] [G loss: 2.028259]\n",
      "epoch:19 step:18458 [D loss: 0.659341, acc: 58.59%] [G loss: 1.987633]\n",
      "epoch:19 step:18459 [D loss: 0.688720, acc: 56.25%] [G loss: 1.867998]\n",
      "epoch:19 step:18460 [D loss: 0.669825, acc: 60.16%] [G loss: 1.857562]\n",
      "epoch:19 step:18461 [D loss: 0.686683, acc: 56.25%] [G loss: 1.832890]\n",
      "epoch:19 step:18462 [D loss: 0.671109, acc: 58.59%] [G loss: 1.816724]\n",
      "epoch:19 step:18463 [D loss: 0.678133, acc: 52.34%] [G loss: 1.777756]\n",
      "epoch:19 step:18464 [D loss: 0.621624, acc: 68.75%] [G loss: 1.924283]\n",
      "epoch:19 step:18465 [D loss: 0.641979, acc: 58.59%] [G loss: 1.851041]\n",
      "epoch:19 step:18466 [D loss: 0.624378, acc: 64.84%] [G loss: 1.885686]\n",
      "epoch:19 step:18467 [D loss: 0.687042, acc: 58.59%] [G loss: 1.849992]\n",
      "epoch:19 step:18468 [D loss: 0.633933, acc: 62.50%] [G loss: 1.868376]\n",
      "epoch:19 step:18469 [D loss: 0.673476, acc: 56.25%] [G loss: 1.821246]\n",
      "epoch:19 step:18470 [D loss: 0.641777, acc: 60.16%] [G loss: 1.739817]\n",
      "epoch:19 step:18471 [D loss: 0.672536, acc: 57.81%] [G loss: 1.798366]\n",
      "epoch:19 step:18472 [D loss: 0.608903, acc: 68.75%] [G loss: 1.874026]\n",
      "epoch:19 step:18473 [D loss: 0.615517, acc: 67.97%] [G loss: 1.875914]\n",
      "epoch:19 step:18474 [D loss: 0.610469, acc: 65.62%] [G loss: 1.800656]\n",
      "epoch:19 step:18475 [D loss: 0.641596, acc: 64.84%] [G loss: 1.895959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18476 [D loss: 0.672843, acc: 60.94%] [G loss: 1.891460]\n",
      "epoch:19 step:18477 [D loss: 0.676182, acc: 57.81%] [G loss: 1.941707]\n",
      "epoch:19 step:18478 [D loss: 0.678581, acc: 60.16%] [G loss: 1.804063]\n",
      "epoch:19 step:18479 [D loss: 0.608570, acc: 64.84%] [G loss: 1.964046]\n",
      "epoch:19 step:18480 [D loss: 0.635969, acc: 64.84%] [G loss: 1.826567]\n",
      "epoch:19 step:18481 [D loss: 0.641623, acc: 66.41%] [G loss: 1.925103]\n",
      "epoch:19 step:18482 [D loss: 0.635824, acc: 64.06%] [G loss: 2.003586]\n",
      "epoch:19 step:18483 [D loss: 0.609794, acc: 66.41%] [G loss: 1.882099]\n",
      "epoch:19 step:18484 [D loss: 0.626799, acc: 67.97%] [G loss: 2.012608]\n",
      "epoch:19 step:18485 [D loss: 0.631727, acc: 65.62%] [G loss: 1.837481]\n",
      "epoch:19 step:18486 [D loss: 0.638015, acc: 63.28%] [G loss: 1.779364]\n",
      "epoch:19 step:18487 [D loss: 0.616814, acc: 66.41%] [G loss: 1.850055]\n",
      "epoch:19 step:18488 [D loss: 0.616862, acc: 65.62%] [G loss: 1.781239]\n",
      "epoch:19 step:18489 [D loss: 0.683432, acc: 62.50%] [G loss: 1.952782]\n",
      "epoch:19 step:18490 [D loss: 0.620266, acc: 63.28%] [G loss: 2.032842]\n",
      "epoch:19 step:18491 [D loss: 0.681789, acc: 60.94%] [G loss: 1.951054]\n",
      "epoch:19 step:18492 [D loss: 0.674445, acc: 62.50%] [G loss: 2.129169]\n",
      "epoch:19 step:18493 [D loss: 0.579491, acc: 70.31%] [G loss: 1.934812]\n",
      "epoch:19 step:18494 [D loss: 0.646162, acc: 61.72%] [G loss: 1.978664]\n",
      "epoch:19 step:18495 [D loss: 0.593957, acc: 65.62%] [G loss: 1.983127]\n",
      "epoch:19 step:18496 [D loss: 0.597202, acc: 66.41%] [G loss: 2.037595]\n",
      "epoch:19 step:18497 [D loss: 0.596349, acc: 67.97%] [G loss: 2.227412]\n",
      "epoch:19 step:18498 [D loss: 0.620460, acc: 66.41%] [G loss: 1.988133]\n",
      "epoch:19 step:18499 [D loss: 0.655913, acc: 58.59%] [G loss: 1.801225]\n",
      "epoch:19 step:18500 [D loss: 0.646293, acc: 64.06%] [G loss: 1.882357]\n",
      "epoch:19 step:18501 [D loss: 0.623920, acc: 67.19%] [G loss: 1.961265]\n",
      "epoch:19 step:18502 [D loss: 0.669282, acc: 57.81%] [G loss: 1.961395]\n",
      "epoch:19 step:18503 [D loss: 0.658156, acc: 60.94%] [G loss: 1.886834]\n",
      "epoch:19 step:18504 [D loss: 0.641421, acc: 60.94%] [G loss: 2.007985]\n",
      "epoch:19 step:18505 [D loss: 0.734184, acc: 50.00%] [G loss: 1.792335]\n",
      "epoch:19 step:18506 [D loss: 0.703997, acc: 48.44%] [G loss: 1.715122]\n",
      "epoch:19 step:18507 [D loss: 0.680872, acc: 60.94%] [G loss: 1.734248]\n",
      "epoch:19 step:18508 [D loss: 0.701244, acc: 55.47%] [G loss: 1.758058]\n",
      "epoch:19 step:18509 [D loss: 0.655938, acc: 62.50%] [G loss: 1.839384]\n",
      "epoch:19 step:18510 [D loss: 0.644049, acc: 61.72%] [G loss: 1.966909]\n",
      "epoch:19 step:18511 [D loss: 0.578416, acc: 74.22%] [G loss: 1.999803]\n",
      "epoch:19 step:18512 [D loss: 0.633668, acc: 61.72%] [G loss: 1.976262]\n",
      "epoch:19 step:18513 [D loss: 0.681213, acc: 60.94%] [G loss: 1.832609]\n",
      "epoch:19 step:18514 [D loss: 0.658262, acc: 66.41%] [G loss: 1.913867]\n",
      "epoch:19 step:18515 [D loss: 0.657175, acc: 60.94%] [G loss: 1.810782]\n",
      "epoch:19 step:18516 [D loss: 0.660484, acc: 60.16%] [G loss: 1.942302]\n",
      "epoch:19 step:18517 [D loss: 0.619966, acc: 68.75%] [G loss: 1.914452]\n",
      "epoch:19 step:18518 [D loss: 0.713515, acc: 57.03%] [G loss: 1.993758]\n",
      "epoch:19 step:18519 [D loss: 0.665977, acc: 67.19%] [G loss: 1.869651]\n",
      "epoch:19 step:18520 [D loss: 0.649784, acc: 60.16%] [G loss: 1.888112]\n",
      "epoch:19 step:18521 [D loss: 0.628609, acc: 62.50%] [G loss: 1.993347]\n",
      "epoch:19 step:18522 [D loss: 0.568556, acc: 72.66%] [G loss: 2.121222]\n",
      "epoch:19 step:18523 [D loss: 0.628678, acc: 65.62%] [G loss: 2.098270]\n",
      "epoch:19 step:18524 [D loss: 0.577012, acc: 74.22%] [G loss: 2.011055]\n",
      "epoch:19 step:18525 [D loss: 0.712776, acc: 53.12%] [G loss: 1.837427]\n",
      "epoch:19 step:18526 [D loss: 0.662017, acc: 61.72%] [G loss: 1.970309]\n",
      "epoch:19 step:18527 [D loss: 0.611969, acc: 66.41%] [G loss: 1.903549]\n",
      "epoch:19 step:18528 [D loss: 0.697893, acc: 57.03%] [G loss: 1.967133]\n",
      "epoch:19 step:18529 [D loss: 0.645581, acc: 60.94%] [G loss: 2.044881]\n",
      "epoch:19 step:18530 [D loss: 0.682188, acc: 63.28%] [G loss: 1.857273]\n",
      "epoch:19 step:18531 [D loss: 0.642010, acc: 66.41%] [G loss: 1.927331]\n",
      "epoch:19 step:18532 [D loss: 0.648065, acc: 63.28%] [G loss: 2.017240]\n",
      "epoch:19 step:18533 [D loss: 0.666130, acc: 56.25%] [G loss: 1.755489]\n",
      "epoch:19 step:18534 [D loss: 0.614746, acc: 63.28%] [G loss: 1.881332]\n",
      "epoch:19 step:18535 [D loss: 0.630188, acc: 65.62%] [G loss: 1.890399]\n",
      "epoch:19 step:18536 [D loss: 0.673346, acc: 53.91%] [G loss: 1.876938]\n",
      "epoch:19 step:18537 [D loss: 0.645851, acc: 64.06%] [G loss: 2.013160]\n",
      "epoch:19 step:18538 [D loss: 0.672230, acc: 55.47%] [G loss: 1.814344]\n",
      "epoch:19 step:18539 [D loss: 0.658522, acc: 64.84%] [G loss: 1.800160]\n",
      "epoch:19 step:18540 [D loss: 0.657728, acc: 62.50%] [G loss: 1.758814]\n",
      "epoch:19 step:18541 [D loss: 0.637963, acc: 64.06%] [G loss: 1.754691]\n",
      "epoch:19 step:18542 [D loss: 0.719742, acc: 56.25%] [G loss: 1.804011]\n",
      "epoch:19 step:18543 [D loss: 0.639257, acc: 63.28%] [G loss: 1.978963]\n",
      "epoch:19 step:18544 [D loss: 0.674062, acc: 55.47%] [G loss: 1.848563]\n",
      "epoch:19 step:18545 [D loss: 0.657530, acc: 59.38%] [G loss: 1.876733]\n",
      "epoch:19 step:18546 [D loss: 0.619413, acc: 64.84%] [G loss: 1.778814]\n",
      "epoch:19 step:18547 [D loss: 0.646695, acc: 60.94%] [G loss: 1.809582]\n",
      "epoch:19 step:18548 [D loss: 0.675296, acc: 58.59%] [G loss: 1.852972]\n",
      "epoch:19 step:18549 [D loss: 0.642720, acc: 61.72%] [G loss: 1.852750]\n",
      "epoch:19 step:18550 [D loss: 0.641293, acc: 64.84%] [G loss: 1.867012]\n",
      "epoch:19 step:18551 [D loss: 0.657318, acc: 60.16%] [G loss: 1.821767]\n",
      "epoch:19 step:18552 [D loss: 0.657773, acc: 64.84%] [G loss: 1.734704]\n",
      "epoch:19 step:18553 [D loss: 0.668095, acc: 57.03%] [G loss: 1.738865]\n",
      "epoch:19 step:18554 [D loss: 0.655100, acc: 57.03%] [G loss: 1.898695]\n",
      "epoch:19 step:18555 [D loss: 0.668862, acc: 60.16%] [G loss: 1.778910]\n",
      "epoch:19 step:18556 [D loss: 0.656188, acc: 57.81%] [G loss: 1.820659]\n",
      "epoch:19 step:18557 [D loss: 0.640808, acc: 66.41%] [G loss: 1.988847]\n",
      "epoch:19 step:18558 [D loss: 0.602859, acc: 64.06%] [G loss: 1.851113]\n",
      "epoch:19 step:18559 [D loss: 0.652077, acc: 62.50%] [G loss: 1.850288]\n",
      "epoch:19 step:18560 [D loss: 0.655310, acc: 60.16%] [G loss: 1.917381]\n",
      "epoch:19 step:18561 [D loss: 0.657669, acc: 62.50%] [G loss: 1.731822]\n",
      "epoch:19 step:18562 [D loss: 0.678654, acc: 60.16%] [G loss: 1.803036]\n",
      "epoch:19 step:18563 [D loss: 0.638803, acc: 64.06%] [G loss: 1.738072]\n",
      "epoch:19 step:18564 [D loss: 0.659435, acc: 62.50%] [G loss: 1.785813]\n",
      "epoch:19 step:18565 [D loss: 0.641284, acc: 62.50%] [G loss: 1.773618]\n",
      "epoch:19 step:18566 [D loss: 0.597519, acc: 65.62%] [G loss: 1.887041]\n",
      "epoch:19 step:18567 [D loss: 0.629787, acc: 61.72%] [G loss: 1.896050]\n",
      "epoch:19 step:18568 [D loss: 0.656373, acc: 55.47%] [G loss: 1.734296]\n",
      "epoch:19 step:18569 [D loss: 0.663123, acc: 63.28%] [G loss: 1.841970]\n",
      "epoch:19 step:18570 [D loss: 0.646829, acc: 64.84%] [G loss: 1.927692]\n",
      "epoch:19 step:18571 [D loss: 0.659281, acc: 60.94%] [G loss: 1.869153]\n",
      "epoch:19 step:18572 [D loss: 0.620146, acc: 69.53%] [G loss: 1.921084]\n",
      "epoch:19 step:18573 [D loss: 0.630502, acc: 57.03%] [G loss: 1.845369]\n",
      "epoch:19 step:18574 [D loss: 0.666786, acc: 57.03%] [G loss: 1.855839]\n",
      "epoch:19 step:18575 [D loss: 0.623239, acc: 67.19%] [G loss: 1.855537]\n",
      "epoch:19 step:18576 [D loss: 0.620659, acc: 64.84%] [G loss: 1.881027]\n",
      "epoch:19 step:18577 [D loss: 0.688145, acc: 58.59%] [G loss: 1.993852]\n",
      "epoch:19 step:18578 [D loss: 0.611722, acc: 72.66%] [G loss: 2.037890]\n",
      "epoch:19 step:18579 [D loss: 0.672210, acc: 64.84%] [G loss: 1.870126]\n",
      "epoch:19 step:18580 [D loss: 0.632679, acc: 61.72%] [G loss: 1.915700]\n",
      "epoch:19 step:18581 [D loss: 0.658208, acc: 59.38%] [G loss: 1.950684]\n",
      "epoch:19 step:18582 [D loss: 0.638183, acc: 62.50%] [G loss: 1.958061]\n",
      "epoch:19 step:18583 [D loss: 0.607279, acc: 67.19%] [G loss: 1.966708]\n",
      "epoch:19 step:18584 [D loss: 0.583532, acc: 68.75%] [G loss: 2.148974]\n",
      "epoch:19 step:18585 [D loss: 0.646719, acc: 67.19%] [G loss: 2.021599]\n",
      "epoch:19 step:18586 [D loss: 0.689201, acc: 57.03%] [G loss: 1.954929]\n",
      "epoch:19 step:18587 [D loss: 0.626969, acc: 62.50%] [G loss: 1.897778]\n",
      "epoch:19 step:18588 [D loss: 0.667115, acc: 58.59%] [G loss: 2.095585]\n",
      "epoch:19 step:18589 [D loss: 0.579830, acc: 66.41%] [G loss: 2.188319]\n",
      "epoch:19 step:18590 [D loss: 0.615342, acc: 67.97%] [G loss: 1.861612]\n",
      "epoch:19 step:18591 [D loss: 0.730431, acc: 51.56%] [G loss: 1.755724]\n",
      "epoch:19 step:18592 [D loss: 0.634496, acc: 61.72%] [G loss: 2.033765]\n",
      "epoch:19 step:18593 [D loss: 0.655194, acc: 61.72%] [G loss: 1.902693]\n",
      "epoch:19 step:18594 [D loss: 0.650780, acc: 60.94%] [G loss: 1.895595]\n",
      "epoch:19 step:18595 [D loss: 0.645086, acc: 63.28%] [G loss: 2.050084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18596 [D loss: 0.654249, acc: 60.94%] [G loss: 1.988915]\n",
      "epoch:19 step:18597 [D loss: 0.719609, acc: 59.38%] [G loss: 1.803994]\n",
      "epoch:19 step:18598 [D loss: 0.680831, acc: 58.59%] [G loss: 1.785785]\n",
      "epoch:19 step:18599 [D loss: 0.694497, acc: 59.38%] [G loss: 1.923294]\n",
      "epoch:19 step:18600 [D loss: 0.680635, acc: 51.56%] [G loss: 1.707403]\n",
      "epoch:19 step:18601 [D loss: 0.704865, acc: 50.78%] [G loss: 1.757326]\n",
      "epoch:19 step:18602 [D loss: 0.654337, acc: 61.72%] [G loss: 1.844455]\n",
      "epoch:19 step:18603 [D loss: 0.698702, acc: 57.81%] [G loss: 1.701985]\n",
      "epoch:19 step:18604 [D loss: 0.655798, acc: 60.94%] [G loss: 1.845460]\n",
      "epoch:19 step:18605 [D loss: 0.661823, acc: 61.72%] [G loss: 1.739749]\n",
      "epoch:19 step:18606 [D loss: 0.606916, acc: 62.50%] [G loss: 1.861162]\n",
      "epoch:19 step:18607 [D loss: 0.641148, acc: 57.81%] [G loss: 1.913834]\n",
      "epoch:19 step:18608 [D loss: 0.628659, acc: 65.62%] [G loss: 1.817986]\n",
      "epoch:19 step:18609 [D loss: 0.690863, acc: 55.47%] [G loss: 1.814899]\n",
      "epoch:19 step:18610 [D loss: 0.666535, acc: 60.16%] [G loss: 1.961245]\n",
      "epoch:19 step:18611 [D loss: 0.637920, acc: 65.62%] [G loss: 1.874000]\n",
      "epoch:19 step:18612 [D loss: 0.647659, acc: 60.16%] [G loss: 1.912722]\n",
      "epoch:19 step:18613 [D loss: 0.667491, acc: 58.59%] [G loss: 1.791132]\n",
      "epoch:19 step:18614 [D loss: 0.644686, acc: 66.41%] [G loss: 1.903649]\n",
      "epoch:19 step:18615 [D loss: 0.677617, acc: 53.12%] [G loss: 1.814052]\n",
      "epoch:19 step:18616 [D loss: 0.613198, acc: 65.62%] [G loss: 1.779812]\n",
      "epoch:19 step:18617 [D loss: 0.681183, acc: 58.59%] [G loss: 1.911855]\n",
      "epoch:19 step:18618 [D loss: 0.602376, acc: 69.53%] [G loss: 2.175875]\n",
      "epoch:19 step:18619 [D loss: 0.653443, acc: 57.03%] [G loss: 2.026761]\n",
      "epoch:19 step:18620 [D loss: 0.632257, acc: 65.62%] [G loss: 2.006334]\n",
      "epoch:19 step:18621 [D loss: 0.677039, acc: 57.81%] [G loss: 1.738714]\n",
      "epoch:19 step:18622 [D loss: 0.632815, acc: 62.50%] [G loss: 1.946794]\n",
      "epoch:19 step:18623 [D loss: 0.717060, acc: 52.34%] [G loss: 1.676677]\n",
      "epoch:19 step:18624 [D loss: 0.660648, acc: 57.81%] [G loss: 1.856614]\n",
      "epoch:19 step:18625 [D loss: 0.633145, acc: 60.16%] [G loss: 1.995708]\n",
      "epoch:19 step:18626 [D loss: 0.642414, acc: 63.28%] [G loss: 2.006836]\n",
      "epoch:19 step:18627 [D loss: 0.656528, acc: 59.38%] [G loss: 1.862017]\n",
      "epoch:19 step:18628 [D loss: 0.598204, acc: 65.62%] [G loss: 2.011195]\n",
      "epoch:19 step:18629 [D loss: 0.691568, acc: 56.25%] [G loss: 1.762593]\n",
      "epoch:19 step:18630 [D loss: 0.637990, acc: 68.75%] [G loss: 1.890576]\n",
      "epoch:19 step:18631 [D loss: 0.690290, acc: 58.59%] [G loss: 1.765652]\n",
      "epoch:19 step:18632 [D loss: 0.683014, acc: 55.47%] [G loss: 1.748070]\n",
      "epoch:19 step:18633 [D loss: 0.671831, acc: 63.28%] [G loss: 1.733627]\n",
      "epoch:19 step:18634 [D loss: 0.669004, acc: 59.38%] [G loss: 1.866811]\n",
      "epoch:19 step:18635 [D loss: 0.674557, acc: 57.03%] [G loss: 1.749036]\n",
      "epoch:19 step:18636 [D loss: 0.629691, acc: 67.19%] [G loss: 1.992234]\n",
      "epoch:19 step:18637 [D loss: 0.606935, acc: 69.53%] [G loss: 1.819020]\n",
      "epoch:19 step:18638 [D loss: 0.602663, acc: 65.62%] [G loss: 1.900697]\n",
      "epoch:19 step:18639 [D loss: 0.625518, acc: 67.19%] [G loss: 1.904232]\n",
      "epoch:19 step:18640 [D loss: 0.651199, acc: 60.16%] [G loss: 1.949347]\n",
      "epoch:19 step:18641 [D loss: 0.610561, acc: 70.31%] [G loss: 1.985697]\n",
      "epoch:19 step:18642 [D loss: 0.606247, acc: 66.41%] [G loss: 1.995699]\n",
      "epoch:19 step:18643 [D loss: 0.628275, acc: 68.75%] [G loss: 1.970863]\n",
      "epoch:19 step:18644 [D loss: 0.541236, acc: 76.56%] [G loss: 2.044027]\n",
      "epoch:19 step:18645 [D loss: 0.645151, acc: 63.28%] [G loss: 2.012809]\n",
      "epoch:19 step:18646 [D loss: 0.736048, acc: 55.47%] [G loss: 1.908694]\n",
      "epoch:19 step:18647 [D loss: 0.583683, acc: 69.53%] [G loss: 1.859822]\n",
      "epoch:19 step:18648 [D loss: 0.667730, acc: 60.16%] [G loss: 1.918833]\n",
      "epoch:19 step:18649 [D loss: 0.678897, acc: 59.38%] [G loss: 1.882491]\n",
      "epoch:19 step:18650 [D loss: 0.642291, acc: 66.41%] [G loss: 1.988792]\n",
      "epoch:19 step:18651 [D loss: 0.669917, acc: 63.28%] [G loss: 1.960153]\n",
      "epoch:19 step:18652 [D loss: 0.600338, acc: 65.62%] [G loss: 1.935093]\n",
      "epoch:19 step:18653 [D loss: 0.687891, acc: 54.69%] [G loss: 1.866612]\n",
      "epoch:19 step:18654 [D loss: 0.593334, acc: 67.97%] [G loss: 1.741587]\n",
      "epoch:19 step:18655 [D loss: 0.653879, acc: 58.59%] [G loss: 1.807833]\n",
      "epoch:19 step:18656 [D loss: 0.630757, acc: 64.06%] [G loss: 1.905490]\n",
      "epoch:19 step:18657 [D loss: 0.697775, acc: 60.16%] [G loss: 1.835745]\n",
      "epoch:19 step:18658 [D loss: 0.657592, acc: 59.38%] [G loss: 1.791913]\n",
      "epoch:19 step:18659 [D loss: 0.699720, acc: 55.47%] [G loss: 1.783363]\n",
      "epoch:19 step:18660 [D loss: 0.630103, acc: 63.28%] [G loss: 1.741498]\n",
      "epoch:19 step:18661 [D loss: 0.695920, acc: 53.91%] [G loss: 1.764636]\n",
      "epoch:19 step:18662 [D loss: 0.768168, acc: 49.22%] [G loss: 1.765795]\n",
      "epoch:19 step:18663 [D loss: 0.641559, acc: 67.19%] [G loss: 1.829357]\n",
      "epoch:19 step:18664 [D loss: 0.652699, acc: 59.38%] [G loss: 1.854830]\n",
      "epoch:19 step:18665 [D loss: 0.681859, acc: 57.03%] [G loss: 1.740210]\n",
      "epoch:19 step:18666 [D loss: 0.649663, acc: 59.38%] [G loss: 1.792220]\n",
      "epoch:19 step:18667 [D loss: 0.610180, acc: 71.88%] [G loss: 1.756115]\n",
      "epoch:19 step:18668 [D loss: 0.659532, acc: 57.81%] [G loss: 1.808315]\n",
      "epoch:19 step:18669 [D loss: 0.656352, acc: 60.94%] [G loss: 1.766209]\n",
      "epoch:19 step:18670 [D loss: 0.716949, acc: 47.66%] [G loss: 1.789042]\n",
      "epoch:19 step:18671 [D loss: 0.661964, acc: 60.16%] [G loss: 1.741588]\n",
      "epoch:19 step:18672 [D loss: 0.685261, acc: 58.59%] [G loss: 1.714433]\n",
      "epoch:19 step:18673 [D loss: 0.667572, acc: 56.25%] [G loss: 1.698208]\n",
      "epoch:19 step:18674 [D loss: 0.656326, acc: 60.94%] [G loss: 1.749682]\n",
      "epoch:19 step:18675 [D loss: 0.659233, acc: 66.41%] [G loss: 1.692266]\n",
      "epoch:19 step:18676 [D loss: 0.682298, acc: 57.81%] [G loss: 1.673524]\n",
      "epoch:19 step:18677 [D loss: 0.641087, acc: 65.62%] [G loss: 1.776234]\n",
      "epoch:19 step:18678 [D loss: 0.642208, acc: 66.41%] [G loss: 1.794444]\n",
      "epoch:19 step:18679 [D loss: 0.629516, acc: 64.06%] [G loss: 1.818985]\n",
      "epoch:19 step:18680 [D loss: 0.663150, acc: 60.94%] [G loss: 1.778766]\n",
      "epoch:19 step:18681 [D loss: 0.612979, acc: 62.50%] [G loss: 1.871046]\n",
      "epoch:19 step:18682 [D loss: 0.604246, acc: 64.84%] [G loss: 1.807146]\n",
      "epoch:19 step:18683 [D loss: 0.647961, acc: 59.38%] [G loss: 1.890135]\n",
      "epoch:19 step:18684 [D loss: 0.641943, acc: 60.16%] [G loss: 1.862130]\n",
      "epoch:19 step:18685 [D loss: 0.707723, acc: 52.34%] [G loss: 1.770118]\n",
      "epoch:19 step:18686 [D loss: 0.683597, acc: 59.38%] [G loss: 1.854669]\n",
      "epoch:19 step:18687 [D loss: 0.606084, acc: 63.28%] [G loss: 1.980375]\n",
      "epoch:19 step:18688 [D loss: 0.632285, acc: 60.94%] [G loss: 1.960433]\n",
      "epoch:19 step:18689 [D loss: 0.666866, acc: 62.50%] [G loss: 1.986424]\n",
      "epoch:19 step:18690 [D loss: 0.653002, acc: 64.84%] [G loss: 1.832624]\n",
      "epoch:19 step:18691 [D loss: 0.644859, acc: 60.94%] [G loss: 1.860575]\n",
      "epoch:19 step:18692 [D loss: 0.628436, acc: 65.62%] [G loss: 1.967325]\n",
      "epoch:19 step:18693 [D loss: 0.626609, acc: 62.50%] [G loss: 2.006142]\n",
      "epoch:19 step:18694 [D loss: 0.637591, acc: 67.19%] [G loss: 1.796051]\n",
      "epoch:19 step:18695 [D loss: 0.678123, acc: 55.47%] [G loss: 1.928193]\n",
      "epoch:19 step:18696 [D loss: 0.701923, acc: 52.34%] [G loss: 1.977025]\n",
      "epoch:19 step:18697 [D loss: 0.631228, acc: 62.50%] [G loss: 1.944959]\n",
      "epoch:19 step:18698 [D loss: 0.685394, acc: 57.03%] [G loss: 1.766445]\n",
      "epoch:19 step:18699 [D loss: 0.675091, acc: 57.81%] [G loss: 1.813639]\n",
      "epoch:19 step:18700 [D loss: 0.610438, acc: 64.84%] [G loss: 1.856893]\n",
      "epoch:19 step:18701 [D loss: 0.614709, acc: 66.41%] [G loss: 1.899541]\n",
      "epoch:19 step:18702 [D loss: 0.580395, acc: 71.09%] [G loss: 2.083849]\n",
      "epoch:19 step:18703 [D loss: 0.616346, acc: 64.06%] [G loss: 2.006352]\n",
      "epoch:19 step:18704 [D loss: 0.646592, acc: 57.81%] [G loss: 2.027282]\n",
      "epoch:19 step:18705 [D loss: 0.689454, acc: 53.12%] [G loss: 1.915602]\n",
      "epoch:19 step:18706 [D loss: 0.634044, acc: 60.94%] [G loss: 2.003901]\n",
      "epoch:19 step:18707 [D loss: 0.633052, acc: 64.84%] [G loss: 1.957546]\n",
      "epoch:19 step:18708 [D loss: 0.663614, acc: 60.94%] [G loss: 1.892041]\n",
      "epoch:19 step:18709 [D loss: 0.609802, acc: 67.19%] [G loss: 2.018335]\n",
      "epoch:19 step:18710 [D loss: 0.585047, acc: 71.09%] [G loss: 2.165708]\n",
      "epoch:19 step:18711 [D loss: 0.670721, acc: 60.16%] [G loss: 1.913177]\n",
      "epoch:19 step:18712 [D loss: 0.620360, acc: 65.62%] [G loss: 2.068792]\n",
      "epoch:19 step:18713 [D loss: 0.629626, acc: 65.62%] [G loss: 2.080970]\n",
      "epoch:19 step:18714 [D loss: 0.644910, acc: 66.41%] [G loss: 1.986173]\n",
      "epoch:19 step:18715 [D loss: 0.681416, acc: 57.81%] [G loss: 2.057189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:18716 [D loss: 0.660485, acc: 61.72%] [G loss: 1.997522]\n",
      "epoch:19 step:18717 [D loss: 0.640302, acc: 67.97%] [G loss: 1.997748]\n",
      "epoch:19 step:18718 [D loss: 0.690607, acc: 52.34%] [G loss: 1.821552]\n",
      "epoch:19 step:18719 [D loss: 0.634469, acc: 60.16%] [G loss: 1.924038]\n",
      "epoch:19 step:18720 [D loss: 0.623735, acc: 65.62%] [G loss: 1.842523]\n",
      "epoch:19 step:18721 [D loss: 0.617471, acc: 64.06%] [G loss: 2.037338]\n",
      "epoch:19 step:18722 [D loss: 0.590423, acc: 68.75%] [G loss: 2.164396]\n",
      "epoch:19 step:18723 [D loss: 0.695632, acc: 57.81%] [G loss: 1.883923]\n",
      "epoch:19 step:18724 [D loss: 0.682378, acc: 57.03%] [G loss: 1.888302]\n",
      "epoch:19 step:18725 [D loss: 0.605411, acc: 71.09%] [G loss: 2.015541]\n",
      "epoch:19 step:18726 [D loss: 0.536485, acc: 78.91%] [G loss: 2.173645]\n",
      "epoch:19 step:18727 [D loss: 0.580337, acc: 71.09%] [G loss: 2.292660]\n",
      "epoch:19 step:18728 [D loss: 0.594364, acc: 71.88%] [G loss: 2.084998]\n",
      "epoch:19 step:18729 [D loss: 0.630705, acc: 63.28%] [G loss: 2.233614]\n",
      "epoch:19 step:18730 [D loss: 0.656726, acc: 64.06%] [G loss: 2.045902]\n",
      "epoch:19 step:18731 [D loss: 0.786177, acc: 47.66%] [G loss: 1.732844]\n",
      "epoch:19 step:18732 [D loss: 0.712883, acc: 53.12%] [G loss: 1.844376]\n",
      "epoch:19 step:18733 [D loss: 0.640137, acc: 66.41%] [G loss: 1.976223]\n",
      "epoch:19 step:18734 [D loss: 0.680944, acc: 57.03%] [G loss: 2.003932]\n",
      "epoch:19 step:18735 [D loss: 0.673954, acc: 50.00%] [G loss: 1.831887]\n",
      "epoch:19 step:18736 [D loss: 0.641991, acc: 66.41%] [G loss: 1.868729]\n",
      "epoch:19 step:18737 [D loss: 0.634018, acc: 64.84%] [G loss: 1.956998]\n",
      "epoch:19 step:18738 [D loss: 0.643412, acc: 65.62%] [G loss: 2.032207]\n",
      "epoch:19 step:18739 [D loss: 0.550108, acc: 72.66%] [G loss: 2.154002]\n",
      "epoch:19 step:18740 [D loss: 0.623236, acc: 64.06%] [G loss: 2.350648]\n",
      "epoch:20 step:18741 [D loss: 0.668037, acc: 62.50%] [G loss: 1.944282]\n",
      "epoch:20 step:18742 [D loss: 0.658660, acc: 64.84%] [G loss: 2.003335]\n",
      "epoch:20 step:18743 [D loss: 0.668044, acc: 54.69%] [G loss: 1.977960]\n",
      "epoch:20 step:18744 [D loss: 0.661931, acc: 56.25%] [G loss: 1.909512]\n",
      "epoch:20 step:18745 [D loss: 0.608213, acc: 66.41%] [G loss: 1.986905]\n",
      "epoch:20 step:18746 [D loss: 0.607155, acc: 67.19%] [G loss: 2.093565]\n",
      "epoch:20 step:18747 [D loss: 0.651446, acc: 61.72%] [G loss: 1.997122]\n",
      "epoch:20 step:18748 [D loss: 0.658790, acc: 60.16%] [G loss: 2.096200]\n",
      "epoch:20 step:18749 [D loss: 0.618875, acc: 63.28%] [G loss: 1.983649]\n",
      "epoch:20 step:18750 [D loss: 0.610874, acc: 64.06%] [G loss: 2.054000]\n",
      "epoch:20 step:18751 [D loss: 0.608990, acc: 66.41%] [G loss: 2.007949]\n",
      "epoch:20 step:18752 [D loss: 0.585607, acc: 70.31%] [G loss: 1.846284]\n",
      "epoch:20 step:18753 [D loss: 0.644948, acc: 62.50%] [G loss: 1.905507]\n",
      "epoch:20 step:18754 [D loss: 0.622482, acc: 63.28%] [G loss: 1.982533]\n",
      "epoch:20 step:18755 [D loss: 0.637477, acc: 62.50%] [G loss: 2.121464]\n",
      "epoch:20 step:18756 [D loss: 0.626486, acc: 64.06%] [G loss: 2.116554]\n",
      "epoch:20 step:18757 [D loss: 0.642119, acc: 64.84%] [G loss: 1.875453]\n",
      "epoch:20 step:18758 [D loss: 0.592518, acc: 64.84%] [G loss: 1.883799]\n",
      "epoch:20 step:18759 [D loss: 0.681354, acc: 59.38%] [G loss: 1.867791]\n",
      "epoch:20 step:18760 [D loss: 0.707845, acc: 56.25%] [G loss: 1.616175]\n",
      "epoch:20 step:18761 [D loss: 0.672856, acc: 58.59%] [G loss: 1.834679]\n",
      "epoch:20 step:18762 [D loss: 0.652942, acc: 57.81%] [G loss: 1.913296]\n",
      "epoch:20 step:18763 [D loss: 0.650817, acc: 62.50%] [G loss: 1.977922]\n",
      "epoch:20 step:18764 [D loss: 0.563283, acc: 75.00%] [G loss: 1.910227]\n",
      "epoch:20 step:18765 [D loss: 0.608023, acc: 67.97%] [G loss: 2.031565]\n",
      "epoch:20 step:18766 [D loss: 0.666269, acc: 58.59%] [G loss: 1.766982]\n",
      "epoch:20 step:18767 [D loss: 0.633682, acc: 63.28%] [G loss: 1.874133]\n",
      "epoch:20 step:18768 [D loss: 0.637545, acc: 59.38%] [G loss: 1.977141]\n",
      "epoch:20 step:18769 [D loss: 0.636994, acc: 64.06%] [G loss: 1.906772]\n",
      "epoch:20 step:18770 [D loss: 0.662504, acc: 62.50%] [G loss: 1.832622]\n",
      "epoch:20 step:18771 [D loss: 0.657113, acc: 59.38%] [G loss: 1.579808]\n",
      "epoch:20 step:18772 [D loss: 0.730654, acc: 53.12%] [G loss: 1.854009]\n",
      "epoch:20 step:18773 [D loss: 0.622155, acc: 70.31%] [G loss: 1.917227]\n",
      "epoch:20 step:18774 [D loss: 0.692052, acc: 53.12%] [G loss: 1.907561]\n",
      "epoch:20 step:18775 [D loss: 0.628871, acc: 62.50%] [G loss: 1.915952]\n",
      "epoch:20 step:18776 [D loss: 0.602769, acc: 68.75%] [G loss: 1.782082]\n",
      "epoch:20 step:18777 [D loss: 0.662690, acc: 58.59%] [G loss: 1.930350]\n",
      "epoch:20 step:18778 [D loss: 0.632791, acc: 63.28%] [G loss: 1.876014]\n",
      "epoch:20 step:18779 [D loss: 0.643033, acc: 64.84%] [G loss: 1.891617]\n",
      "epoch:20 step:18780 [D loss: 0.638479, acc: 67.19%] [G loss: 1.992042]\n",
      "epoch:20 step:18781 [D loss: 0.634953, acc: 62.50%] [G loss: 1.787154]\n",
      "epoch:20 step:18782 [D loss: 0.588105, acc: 69.53%] [G loss: 1.990963]\n",
      "epoch:20 step:18783 [D loss: 0.632127, acc: 63.28%] [G loss: 1.747354]\n",
      "epoch:20 step:18784 [D loss: 0.676647, acc: 60.94%] [G loss: 1.863723]\n",
      "epoch:20 step:18785 [D loss: 0.635017, acc: 67.97%] [G loss: 1.843037]\n",
      "epoch:20 step:18786 [D loss: 0.658330, acc: 60.94%] [G loss: 1.917124]\n",
      "epoch:20 step:18787 [D loss: 0.659124, acc: 63.28%] [G loss: 1.874525]\n",
      "epoch:20 step:18788 [D loss: 0.607578, acc: 70.31%] [G loss: 1.968025]\n",
      "epoch:20 step:18789 [D loss: 0.612244, acc: 66.41%] [G loss: 1.976747]\n",
      "epoch:20 step:18790 [D loss: 0.641146, acc: 65.62%] [G loss: 1.987492]\n",
      "epoch:20 step:18791 [D loss: 0.700536, acc: 60.16%] [G loss: 1.870944]\n",
      "epoch:20 step:18792 [D loss: 0.644184, acc: 62.50%] [G loss: 1.801029]\n",
      "epoch:20 step:18793 [D loss: 0.601757, acc: 69.53%] [G loss: 1.889891]\n",
      "epoch:20 step:18794 [D loss: 0.685524, acc: 56.25%] [G loss: 1.965957]\n",
      "epoch:20 step:18795 [D loss: 0.640746, acc: 71.09%] [G loss: 1.936564]\n",
      "epoch:20 step:18796 [D loss: 0.597540, acc: 64.06%] [G loss: 2.025226]\n",
      "epoch:20 step:18797 [D loss: 0.678873, acc: 64.84%] [G loss: 1.892692]\n",
      "epoch:20 step:18798 [D loss: 0.619843, acc: 67.97%] [G loss: 1.919721]\n",
      "epoch:20 step:18799 [D loss: 0.664396, acc: 65.62%] [G loss: 1.869647]\n",
      "epoch:20 step:18800 [D loss: 0.639469, acc: 64.06%] [G loss: 1.849197]\n",
      "epoch:20 step:18801 [D loss: 0.635757, acc: 63.28%] [G loss: 1.797127]\n",
      "epoch:20 step:18802 [D loss: 0.631528, acc: 65.62%] [G loss: 2.013363]\n",
      "epoch:20 step:18803 [D loss: 0.631741, acc: 67.19%] [G loss: 1.893826]\n",
      "epoch:20 step:18804 [D loss: 0.624933, acc: 65.62%] [G loss: 1.861394]\n",
      "epoch:20 step:18805 [D loss: 0.691616, acc: 60.16%] [G loss: 1.822369]\n",
      "epoch:20 step:18806 [D loss: 0.666710, acc: 59.38%] [G loss: 1.979192]\n",
      "epoch:20 step:18807 [D loss: 0.628062, acc: 71.09%] [G loss: 1.874148]\n",
      "epoch:20 step:18808 [D loss: 0.632249, acc: 63.28%] [G loss: 1.850231]\n",
      "epoch:20 step:18809 [D loss: 0.637026, acc: 59.38%] [G loss: 1.961668]\n",
      "epoch:20 step:18810 [D loss: 0.624732, acc: 63.28%] [G loss: 1.950657]\n",
      "epoch:20 step:18811 [D loss: 0.648095, acc: 63.28%] [G loss: 1.717776]\n",
      "epoch:20 step:18812 [D loss: 0.611216, acc: 64.84%] [G loss: 1.772244]\n",
      "epoch:20 step:18813 [D loss: 0.659397, acc: 61.72%] [G loss: 1.835666]\n",
      "epoch:20 step:18814 [D loss: 0.637691, acc: 63.28%] [G loss: 1.909817]\n",
      "epoch:20 step:18815 [D loss: 0.683980, acc: 60.94%] [G loss: 2.154755]\n",
      "epoch:20 step:18816 [D loss: 0.639715, acc: 62.50%] [G loss: 1.917772]\n",
      "epoch:20 step:18817 [D loss: 0.596396, acc: 68.75%] [G loss: 1.971348]\n",
      "epoch:20 step:18818 [D loss: 0.684566, acc: 57.81%] [G loss: 1.768966]\n",
      "epoch:20 step:18819 [D loss: 0.646441, acc: 60.94%] [G loss: 1.858708]\n",
      "epoch:20 step:18820 [D loss: 0.698142, acc: 57.03%] [G loss: 1.713851]\n",
      "epoch:20 step:18821 [D loss: 0.684484, acc: 57.81%] [G loss: 1.814702]\n",
      "epoch:20 step:18822 [D loss: 0.705523, acc: 58.59%] [G loss: 1.809657]\n",
      "epoch:20 step:18823 [D loss: 0.652322, acc: 60.94%] [G loss: 1.806139]\n",
      "epoch:20 step:18824 [D loss: 0.663747, acc: 60.94%] [G loss: 1.854653]\n",
      "epoch:20 step:18825 [D loss: 0.653968, acc: 58.59%] [G loss: 1.718240]\n",
      "epoch:20 step:18826 [D loss: 0.661190, acc: 64.06%] [G loss: 1.805952]\n",
      "epoch:20 step:18827 [D loss: 0.628796, acc: 64.84%] [G loss: 1.868519]\n",
      "epoch:20 step:18828 [D loss: 0.658221, acc: 59.38%] [G loss: 1.804077]\n",
      "epoch:20 step:18829 [D loss: 0.643567, acc: 68.75%] [G loss: 1.899211]\n",
      "epoch:20 step:18830 [D loss: 0.662103, acc: 59.38%] [G loss: 1.854396]\n",
      "epoch:20 step:18831 [D loss: 0.667435, acc: 59.38%] [G loss: 1.883937]\n",
      "epoch:20 step:18832 [D loss: 0.651764, acc: 60.94%] [G loss: 1.733306]\n",
      "epoch:20 step:18833 [D loss: 0.614263, acc: 65.62%] [G loss: 2.023211]\n",
      "epoch:20 step:18834 [D loss: 0.644578, acc: 64.06%] [G loss: 1.995653]\n",
      "epoch:20 step:18835 [D loss: 0.667531, acc: 59.38%] [G loss: 1.781457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18836 [D loss: 0.633158, acc: 65.62%] [G loss: 1.821139]\n",
      "epoch:20 step:18837 [D loss: 0.634051, acc: 64.84%] [G loss: 1.866556]\n",
      "epoch:20 step:18838 [D loss: 0.668482, acc: 55.47%] [G loss: 1.753928]\n",
      "epoch:20 step:18839 [D loss: 0.619975, acc: 64.06%] [G loss: 1.862339]\n",
      "epoch:20 step:18840 [D loss: 0.678143, acc: 61.72%] [G loss: 1.802554]\n",
      "epoch:20 step:18841 [D loss: 0.656533, acc: 59.38%] [G loss: 1.932494]\n",
      "epoch:20 step:18842 [D loss: 0.664831, acc: 59.38%] [G loss: 1.899955]\n",
      "epoch:20 step:18843 [D loss: 0.653524, acc: 63.28%] [G loss: 1.870228]\n",
      "epoch:20 step:18844 [D loss: 0.629846, acc: 65.62%] [G loss: 1.845375]\n",
      "epoch:20 step:18845 [D loss: 0.677878, acc: 62.50%] [G loss: 1.935023]\n",
      "epoch:20 step:18846 [D loss: 0.649199, acc: 62.50%] [G loss: 2.086823]\n",
      "epoch:20 step:18847 [D loss: 0.630204, acc: 67.19%] [G loss: 2.043054]\n",
      "epoch:20 step:18848 [D loss: 0.656464, acc: 59.38%] [G loss: 1.799177]\n",
      "epoch:20 step:18849 [D loss: 0.622279, acc: 68.75%] [G loss: 1.893585]\n",
      "epoch:20 step:18850 [D loss: 0.735681, acc: 53.12%] [G loss: 1.713590]\n",
      "epoch:20 step:18851 [D loss: 0.617991, acc: 70.31%] [G loss: 2.076390]\n",
      "epoch:20 step:18852 [D loss: 0.650393, acc: 60.94%] [G loss: 1.914015]\n",
      "epoch:20 step:18853 [D loss: 0.627105, acc: 68.75%] [G loss: 2.084850]\n",
      "epoch:20 step:18854 [D loss: 0.626790, acc: 69.53%] [G loss: 2.087398]\n",
      "epoch:20 step:18855 [D loss: 0.609429, acc: 60.16%] [G loss: 2.183984]\n",
      "epoch:20 step:18856 [D loss: 0.592736, acc: 71.09%] [G loss: 2.246007]\n",
      "epoch:20 step:18857 [D loss: 0.621418, acc: 60.94%] [G loss: 2.225780]\n",
      "epoch:20 step:18858 [D loss: 0.667798, acc: 59.38%] [G loss: 2.067756]\n",
      "epoch:20 step:18859 [D loss: 0.557219, acc: 71.09%] [G loss: 2.401971]\n",
      "epoch:20 step:18860 [D loss: 0.664578, acc: 60.94%] [G loss: 2.039094]\n",
      "epoch:20 step:18861 [D loss: 0.654120, acc: 61.72%] [G loss: 2.059279]\n",
      "epoch:20 step:18862 [D loss: 0.650353, acc: 60.94%] [G loss: 2.185056]\n",
      "epoch:20 step:18863 [D loss: 0.638015, acc: 65.62%] [G loss: 1.964956]\n",
      "epoch:20 step:18864 [D loss: 0.731361, acc: 50.78%] [G loss: 1.866145]\n",
      "epoch:20 step:18865 [D loss: 0.704200, acc: 50.78%] [G loss: 1.831724]\n",
      "epoch:20 step:18866 [D loss: 0.644058, acc: 66.41%] [G loss: 1.936617]\n",
      "epoch:20 step:18867 [D loss: 0.686347, acc: 58.59%] [G loss: 1.850752]\n",
      "epoch:20 step:18868 [D loss: 0.659983, acc: 60.94%] [G loss: 1.850262]\n",
      "epoch:20 step:18869 [D loss: 0.677693, acc: 57.81%] [G loss: 1.914587]\n",
      "epoch:20 step:18870 [D loss: 0.600551, acc: 66.41%] [G loss: 1.936969]\n",
      "epoch:20 step:18871 [D loss: 0.593851, acc: 68.75%] [G loss: 1.909779]\n",
      "epoch:20 step:18872 [D loss: 0.668297, acc: 64.06%] [G loss: 1.929032]\n",
      "epoch:20 step:18873 [D loss: 0.698156, acc: 55.47%] [G loss: 1.751891]\n",
      "epoch:20 step:18874 [D loss: 0.631539, acc: 62.50%] [G loss: 1.714010]\n",
      "epoch:20 step:18875 [D loss: 0.597945, acc: 70.31%] [G loss: 1.816946]\n",
      "epoch:20 step:18876 [D loss: 0.662843, acc: 61.72%] [G loss: 1.776439]\n",
      "epoch:20 step:18877 [D loss: 0.635161, acc: 67.19%] [G loss: 1.753985]\n",
      "epoch:20 step:18878 [D loss: 0.661959, acc: 57.81%] [G loss: 1.858338]\n",
      "epoch:20 step:18879 [D loss: 0.714053, acc: 56.25%] [G loss: 1.877237]\n",
      "epoch:20 step:18880 [D loss: 0.656161, acc: 62.50%] [G loss: 1.814765]\n",
      "epoch:20 step:18881 [D loss: 0.621666, acc: 61.72%] [G loss: 1.832426]\n",
      "epoch:20 step:18882 [D loss: 0.699780, acc: 56.25%] [G loss: 1.767396]\n",
      "epoch:20 step:18883 [D loss: 0.701616, acc: 54.69%] [G loss: 1.796071]\n",
      "epoch:20 step:18884 [D loss: 0.667517, acc: 57.81%] [G loss: 1.842725]\n",
      "epoch:20 step:18885 [D loss: 0.649101, acc: 59.38%] [G loss: 1.926983]\n",
      "epoch:20 step:18886 [D loss: 0.671522, acc: 58.59%] [G loss: 1.857525]\n",
      "epoch:20 step:18887 [D loss: 0.662287, acc: 58.59%] [G loss: 1.738232]\n",
      "epoch:20 step:18888 [D loss: 0.655980, acc: 62.50%] [G loss: 1.720819]\n",
      "epoch:20 step:18889 [D loss: 0.626491, acc: 68.75%] [G loss: 1.847872]\n",
      "epoch:20 step:18890 [D loss: 0.624398, acc: 70.31%] [G loss: 1.923809]\n",
      "epoch:20 step:18891 [D loss: 0.654231, acc: 63.28%] [G loss: 2.019882]\n",
      "epoch:20 step:18892 [D loss: 0.637674, acc: 62.50%] [G loss: 2.056053]\n",
      "epoch:20 step:18893 [D loss: 0.666849, acc: 66.41%] [G loss: 1.827073]\n",
      "epoch:20 step:18894 [D loss: 0.624445, acc: 60.94%] [G loss: 1.900640]\n",
      "epoch:20 step:18895 [D loss: 0.682150, acc: 54.69%] [G loss: 1.770869]\n",
      "epoch:20 step:18896 [D loss: 0.654210, acc: 62.50%] [G loss: 1.803456]\n",
      "epoch:20 step:18897 [D loss: 0.653843, acc: 60.16%] [G loss: 1.671301]\n",
      "epoch:20 step:18898 [D loss: 0.683599, acc: 60.16%] [G loss: 1.674312]\n",
      "epoch:20 step:18899 [D loss: 0.667634, acc: 60.16%] [G loss: 1.991070]\n",
      "epoch:20 step:18900 [D loss: 0.705315, acc: 53.91%] [G loss: 1.724939]\n",
      "epoch:20 step:18901 [D loss: 0.673985, acc: 60.16%] [G loss: 1.758629]\n",
      "epoch:20 step:18902 [D loss: 0.651496, acc: 66.41%] [G loss: 1.795705]\n",
      "epoch:20 step:18903 [D loss: 0.618461, acc: 68.75%] [G loss: 1.770490]\n",
      "epoch:20 step:18904 [D loss: 0.633181, acc: 62.50%] [G loss: 1.733780]\n",
      "epoch:20 step:18905 [D loss: 0.704701, acc: 55.47%] [G loss: 1.693369]\n",
      "epoch:20 step:18906 [D loss: 0.667733, acc: 61.72%] [G loss: 1.777269]\n",
      "epoch:20 step:18907 [D loss: 0.608489, acc: 68.75%] [G loss: 1.862875]\n",
      "epoch:20 step:18908 [D loss: 0.629731, acc: 61.72%] [G loss: 1.823771]\n",
      "epoch:20 step:18909 [D loss: 0.651408, acc: 63.28%] [G loss: 1.765108]\n",
      "epoch:20 step:18910 [D loss: 0.672915, acc: 60.94%] [G loss: 1.727314]\n",
      "epoch:20 step:18911 [D loss: 0.600615, acc: 61.72%] [G loss: 1.924453]\n",
      "epoch:20 step:18912 [D loss: 0.642530, acc: 65.62%] [G loss: 1.746033]\n",
      "epoch:20 step:18913 [D loss: 0.658099, acc: 60.94%] [G loss: 1.841901]\n",
      "epoch:20 step:18914 [D loss: 0.678961, acc: 60.16%] [G loss: 1.699519]\n",
      "epoch:20 step:18915 [D loss: 0.653747, acc: 59.38%] [G loss: 1.819111]\n",
      "epoch:20 step:18916 [D loss: 0.633995, acc: 62.50%] [G loss: 1.912721]\n",
      "epoch:20 step:18917 [D loss: 0.664211, acc: 58.59%] [G loss: 1.729262]\n",
      "epoch:20 step:18918 [D loss: 0.732931, acc: 52.34%] [G loss: 1.853299]\n",
      "epoch:20 step:18919 [D loss: 0.643601, acc: 67.19%] [G loss: 1.704689]\n",
      "epoch:20 step:18920 [D loss: 0.690078, acc: 53.91%] [G loss: 1.753187]\n",
      "epoch:20 step:18921 [D loss: 0.621446, acc: 67.97%] [G loss: 1.890592]\n",
      "epoch:20 step:18922 [D loss: 0.645816, acc: 64.84%] [G loss: 1.784183]\n",
      "epoch:20 step:18923 [D loss: 0.699246, acc: 54.69%] [G loss: 1.704505]\n",
      "epoch:20 step:18924 [D loss: 0.631183, acc: 64.84%] [G loss: 1.897193]\n",
      "epoch:20 step:18925 [D loss: 0.688302, acc: 58.59%] [G loss: 1.863852]\n",
      "epoch:20 step:18926 [D loss: 0.651782, acc: 60.16%] [G loss: 1.826784]\n",
      "epoch:20 step:18927 [D loss: 0.653890, acc: 59.38%] [G loss: 1.875658]\n",
      "epoch:20 step:18928 [D loss: 0.704260, acc: 56.25%] [G loss: 1.733302]\n",
      "epoch:20 step:18929 [D loss: 0.648401, acc: 63.28%] [G loss: 1.741137]\n",
      "epoch:20 step:18930 [D loss: 0.637573, acc: 64.84%] [G loss: 1.852013]\n",
      "epoch:20 step:18931 [D loss: 0.691905, acc: 52.34%] [G loss: 1.826883]\n",
      "epoch:20 step:18932 [D loss: 0.665978, acc: 64.06%] [G loss: 1.977817]\n",
      "epoch:20 step:18933 [D loss: 0.663879, acc: 57.81%] [G loss: 1.841389]\n",
      "epoch:20 step:18934 [D loss: 0.622338, acc: 64.06%] [G loss: 1.975251]\n",
      "epoch:20 step:18935 [D loss: 0.674737, acc: 59.38%] [G loss: 1.884787]\n",
      "epoch:20 step:18936 [D loss: 0.663840, acc: 58.59%] [G loss: 1.830830]\n",
      "epoch:20 step:18937 [D loss: 0.684566, acc: 57.03%] [G loss: 2.021990]\n",
      "epoch:20 step:18938 [D loss: 0.597919, acc: 63.28%] [G loss: 1.887742]\n",
      "epoch:20 step:18939 [D loss: 0.700090, acc: 56.25%] [G loss: 1.895992]\n",
      "epoch:20 step:18940 [D loss: 0.742790, acc: 53.91%] [G loss: 1.766214]\n",
      "epoch:20 step:18941 [D loss: 0.603198, acc: 71.09%] [G loss: 1.805210]\n",
      "epoch:20 step:18942 [D loss: 0.601466, acc: 71.09%] [G loss: 1.956292]\n",
      "epoch:20 step:18943 [D loss: 0.674841, acc: 57.81%] [G loss: 1.787989]\n",
      "epoch:20 step:18944 [D loss: 0.609404, acc: 71.88%] [G loss: 1.866152]\n",
      "epoch:20 step:18945 [D loss: 0.702954, acc: 55.47%] [G loss: 1.766613]\n",
      "epoch:20 step:18946 [D loss: 0.629629, acc: 61.72%] [G loss: 1.898314]\n",
      "epoch:20 step:18947 [D loss: 0.624156, acc: 67.19%] [G loss: 1.979042]\n",
      "epoch:20 step:18948 [D loss: 0.610163, acc: 64.84%] [G loss: 2.008171]\n",
      "epoch:20 step:18949 [D loss: 0.620375, acc: 64.06%] [G loss: 1.971134]\n",
      "epoch:20 step:18950 [D loss: 0.670447, acc: 58.59%] [G loss: 1.672598]\n",
      "epoch:20 step:18951 [D loss: 0.644884, acc: 60.94%] [G loss: 1.815140]\n",
      "epoch:20 step:18952 [D loss: 0.614597, acc: 67.19%] [G loss: 1.836024]\n",
      "epoch:20 step:18953 [D loss: 0.652872, acc: 65.62%] [G loss: 1.901450]\n",
      "epoch:20 step:18954 [D loss: 0.654171, acc: 63.28%] [G loss: 1.797230]\n",
      "epoch:20 step:18955 [D loss: 0.645541, acc: 66.41%] [G loss: 1.888716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:18956 [D loss: 0.629819, acc: 66.41%] [G loss: 1.848456]\n",
      "epoch:20 step:18957 [D loss: 0.665298, acc: 59.38%] [G loss: 1.880249]\n",
      "epoch:20 step:18958 [D loss: 0.641170, acc: 65.62%] [G loss: 1.975328]\n",
      "epoch:20 step:18959 [D loss: 0.612308, acc: 67.19%] [G loss: 2.116491]\n",
      "epoch:20 step:18960 [D loss: 0.743542, acc: 53.91%] [G loss: 1.858878]\n",
      "epoch:20 step:18961 [D loss: 0.638851, acc: 58.59%] [G loss: 1.966357]\n",
      "epoch:20 step:18962 [D loss: 0.649027, acc: 61.72%] [G loss: 2.011057]\n",
      "epoch:20 step:18963 [D loss: 0.631803, acc: 65.62%] [G loss: 1.820155]\n",
      "epoch:20 step:18964 [D loss: 0.635956, acc: 66.41%] [G loss: 1.794463]\n",
      "epoch:20 step:18965 [D loss: 0.672714, acc: 57.81%] [G loss: 1.868437]\n",
      "epoch:20 step:18966 [D loss: 0.603765, acc: 67.19%] [G loss: 1.917465]\n",
      "epoch:20 step:18967 [D loss: 0.665451, acc: 60.94%] [G loss: 1.819306]\n",
      "epoch:20 step:18968 [D loss: 0.651457, acc: 67.97%] [G loss: 1.750275]\n",
      "epoch:20 step:18969 [D loss: 0.610227, acc: 64.06%] [G loss: 1.910565]\n",
      "epoch:20 step:18970 [D loss: 0.626255, acc: 66.41%] [G loss: 2.067727]\n",
      "epoch:20 step:18971 [D loss: 0.571864, acc: 68.75%] [G loss: 2.346502]\n",
      "epoch:20 step:18972 [D loss: 0.585483, acc: 72.66%] [G loss: 2.301649]\n",
      "epoch:20 step:18973 [D loss: 0.704496, acc: 55.47%] [G loss: 1.798822]\n",
      "epoch:20 step:18974 [D loss: 0.700004, acc: 57.03%] [G loss: 1.824005]\n",
      "epoch:20 step:18975 [D loss: 0.701449, acc: 56.25%] [G loss: 1.738962]\n",
      "epoch:20 step:18976 [D loss: 0.663836, acc: 64.06%] [G loss: 2.001689]\n",
      "epoch:20 step:18977 [D loss: 0.665737, acc: 60.16%] [G loss: 1.823003]\n",
      "epoch:20 step:18978 [D loss: 0.672945, acc: 64.06%] [G loss: 1.984987]\n",
      "epoch:20 step:18979 [D loss: 0.645671, acc: 60.94%] [G loss: 1.873777]\n",
      "epoch:20 step:18980 [D loss: 0.658193, acc: 59.38%] [G loss: 1.744932]\n",
      "epoch:20 step:18981 [D loss: 0.650193, acc: 64.06%] [G loss: 1.979993]\n",
      "epoch:20 step:18982 [D loss: 0.617110, acc: 67.97%] [G loss: 2.006469]\n",
      "epoch:20 step:18983 [D loss: 0.604561, acc: 64.84%] [G loss: 1.930527]\n",
      "epoch:20 step:18984 [D loss: 0.667412, acc: 59.38%] [G loss: 1.911262]\n",
      "epoch:20 step:18985 [D loss: 0.600355, acc: 64.06%] [G loss: 1.872293]\n",
      "epoch:20 step:18986 [D loss: 0.592926, acc: 71.09%] [G loss: 1.953816]\n",
      "epoch:20 step:18987 [D loss: 0.660649, acc: 60.16%] [G loss: 1.872383]\n",
      "epoch:20 step:18988 [D loss: 0.608295, acc: 64.84%] [G loss: 1.873621]\n",
      "epoch:20 step:18989 [D loss: 0.689764, acc: 57.81%] [G loss: 1.777929]\n",
      "epoch:20 step:18990 [D loss: 0.715120, acc: 50.78%] [G loss: 1.731534]\n",
      "epoch:20 step:18991 [D loss: 0.698558, acc: 50.78%] [G loss: 1.785446]\n",
      "epoch:20 step:18992 [D loss: 0.678128, acc: 58.59%] [G loss: 1.739833]\n",
      "epoch:20 step:18993 [D loss: 0.663673, acc: 60.94%] [G loss: 1.621441]\n",
      "epoch:20 step:18994 [D loss: 0.673478, acc: 58.59%] [G loss: 1.757742]\n",
      "epoch:20 step:18995 [D loss: 0.617461, acc: 68.75%] [G loss: 1.815227]\n",
      "epoch:20 step:18996 [D loss: 0.637733, acc: 63.28%] [G loss: 1.793758]\n",
      "epoch:20 step:18997 [D loss: 0.666881, acc: 56.25%] [G loss: 1.736952]\n",
      "epoch:20 step:18998 [D loss: 0.637054, acc: 60.16%] [G loss: 1.925580]\n",
      "epoch:20 step:18999 [D loss: 0.651898, acc: 57.81%] [G loss: 1.826825]\n",
      "epoch:20 step:19000 [D loss: 0.599421, acc: 69.53%] [G loss: 1.794110]\n",
      "epoch:20 step:19001 [D loss: 0.656408, acc: 55.47%] [G loss: 1.785061]\n",
      "epoch:20 step:19002 [D loss: 0.605325, acc: 64.84%] [G loss: 2.007442]\n",
      "epoch:20 step:19003 [D loss: 0.677918, acc: 57.81%] [G loss: 1.940394]\n",
      "epoch:20 step:19004 [D loss: 0.606803, acc: 67.97%] [G loss: 1.949380]\n",
      "epoch:20 step:19005 [D loss: 0.674715, acc: 59.38%] [G loss: 1.799348]\n",
      "epoch:20 step:19006 [D loss: 0.652730, acc: 64.06%] [G loss: 1.879910]\n",
      "epoch:20 step:19007 [D loss: 0.695942, acc: 60.94%] [G loss: 1.890143]\n",
      "epoch:20 step:19008 [D loss: 0.629034, acc: 61.72%] [G loss: 1.849302]\n",
      "epoch:20 step:19009 [D loss: 0.653373, acc: 57.03%] [G loss: 1.809757]\n",
      "epoch:20 step:19010 [D loss: 0.666397, acc: 59.38%] [G loss: 1.769928]\n",
      "epoch:20 step:19011 [D loss: 0.679659, acc: 55.47%] [G loss: 1.999509]\n",
      "epoch:20 step:19012 [D loss: 0.682263, acc: 59.38%] [G loss: 1.874308]\n",
      "epoch:20 step:19013 [D loss: 0.651973, acc: 64.06%] [G loss: 1.834344]\n",
      "epoch:20 step:19014 [D loss: 0.615807, acc: 67.19%] [G loss: 2.047740]\n",
      "epoch:20 step:19015 [D loss: 0.639753, acc: 59.38%] [G loss: 1.969330]\n",
      "epoch:20 step:19016 [D loss: 0.620207, acc: 70.31%] [G loss: 1.995836]\n",
      "epoch:20 step:19017 [D loss: 0.682597, acc: 56.25%] [G loss: 1.798561]\n",
      "epoch:20 step:19018 [D loss: 0.632315, acc: 63.28%] [G loss: 1.780702]\n",
      "epoch:20 step:19019 [D loss: 0.688905, acc: 53.12%] [G loss: 1.698995]\n",
      "epoch:20 step:19020 [D loss: 0.649713, acc: 66.41%] [G loss: 1.982481]\n",
      "epoch:20 step:19021 [D loss: 0.665911, acc: 64.84%] [G loss: 1.745091]\n",
      "epoch:20 step:19022 [D loss: 0.625612, acc: 66.41%] [G loss: 1.822147]\n",
      "epoch:20 step:19023 [D loss: 0.629383, acc: 64.06%] [G loss: 1.861823]\n",
      "epoch:20 step:19024 [D loss: 0.636635, acc: 68.75%] [G loss: 1.919736]\n",
      "epoch:20 step:19025 [D loss: 0.628565, acc: 67.97%] [G loss: 1.889585]\n",
      "epoch:20 step:19026 [D loss: 0.584954, acc: 71.88%] [G loss: 2.025087]\n",
      "epoch:20 step:19027 [D loss: 0.621516, acc: 64.06%] [G loss: 1.868610]\n",
      "epoch:20 step:19028 [D loss: 0.669858, acc: 60.16%] [G loss: 1.794299]\n",
      "epoch:20 step:19029 [D loss: 0.664446, acc: 65.62%] [G loss: 1.890017]\n",
      "epoch:20 step:19030 [D loss: 0.699186, acc: 57.81%] [G loss: 1.840539]\n",
      "epoch:20 step:19031 [D loss: 0.623393, acc: 62.50%] [G loss: 1.859846]\n",
      "epoch:20 step:19032 [D loss: 0.641199, acc: 59.38%] [G loss: 1.844826]\n",
      "epoch:20 step:19033 [D loss: 0.652865, acc: 63.28%] [G loss: 1.924416]\n",
      "epoch:20 step:19034 [D loss: 0.669108, acc: 58.59%] [G loss: 1.737012]\n",
      "epoch:20 step:19035 [D loss: 0.620434, acc: 64.84%] [G loss: 1.848236]\n",
      "epoch:20 step:19036 [D loss: 0.637806, acc: 62.50%] [G loss: 1.862435]\n",
      "epoch:20 step:19037 [D loss: 0.619975, acc: 64.84%] [G loss: 1.819335]\n",
      "epoch:20 step:19038 [D loss: 0.672610, acc: 57.81%] [G loss: 1.958405]\n",
      "epoch:20 step:19039 [D loss: 0.605085, acc: 67.19%] [G loss: 1.894766]\n",
      "epoch:20 step:19040 [D loss: 0.677173, acc: 65.62%] [G loss: 1.924990]\n",
      "epoch:20 step:19041 [D loss: 0.614238, acc: 70.31%] [G loss: 1.894286]\n",
      "epoch:20 step:19042 [D loss: 0.646241, acc: 64.84%] [G loss: 1.798008]\n",
      "epoch:20 step:19043 [D loss: 0.645705, acc: 64.84%] [G loss: 2.030167]\n",
      "epoch:20 step:19044 [D loss: 0.676536, acc: 64.84%] [G loss: 1.761414]\n",
      "epoch:20 step:19045 [D loss: 0.657824, acc: 61.72%] [G loss: 1.882160]\n",
      "epoch:20 step:19046 [D loss: 0.627111, acc: 58.59%] [G loss: 1.865790]\n",
      "epoch:20 step:19047 [D loss: 0.664839, acc: 60.16%] [G loss: 1.817988]\n",
      "epoch:20 step:19048 [D loss: 0.633759, acc: 64.06%] [G loss: 1.822229]\n",
      "epoch:20 step:19049 [D loss: 0.645907, acc: 62.50%] [G loss: 1.870849]\n",
      "epoch:20 step:19050 [D loss: 0.596312, acc: 67.19%] [G loss: 1.821261]\n",
      "epoch:20 step:19051 [D loss: 0.636126, acc: 66.41%] [G loss: 1.871103]\n",
      "epoch:20 step:19052 [D loss: 0.664586, acc: 64.84%] [G loss: 2.055684]\n",
      "epoch:20 step:19053 [D loss: 0.605280, acc: 68.75%] [G loss: 1.984340]\n",
      "epoch:20 step:19054 [D loss: 0.588150, acc: 73.44%] [G loss: 2.051632]\n",
      "epoch:20 step:19055 [D loss: 0.586686, acc: 67.97%] [G loss: 2.278830]\n",
      "epoch:20 step:19056 [D loss: 0.708153, acc: 55.47%] [G loss: 1.735624]\n",
      "epoch:20 step:19057 [D loss: 0.691888, acc: 55.47%] [G loss: 1.804616]\n",
      "epoch:20 step:19058 [D loss: 0.685248, acc: 53.12%] [G loss: 1.896793]\n",
      "epoch:20 step:19059 [D loss: 0.614026, acc: 64.06%] [G loss: 1.777773]\n",
      "epoch:20 step:19060 [D loss: 0.624786, acc: 64.84%] [G loss: 1.931395]\n",
      "epoch:20 step:19061 [D loss: 0.639614, acc: 64.06%] [G loss: 1.944034]\n",
      "epoch:20 step:19062 [D loss: 0.687302, acc: 60.16%] [G loss: 1.741080]\n",
      "epoch:20 step:19063 [D loss: 0.670184, acc: 60.94%] [G loss: 1.827263]\n",
      "epoch:20 step:19064 [D loss: 0.633701, acc: 62.50%] [G loss: 1.651477]\n",
      "epoch:20 step:19065 [D loss: 0.621454, acc: 65.62%] [G loss: 1.982166]\n",
      "epoch:20 step:19066 [D loss: 0.652830, acc: 60.16%] [G loss: 1.882053]\n",
      "epoch:20 step:19067 [D loss: 0.681284, acc: 53.12%] [G loss: 1.938404]\n",
      "epoch:20 step:19068 [D loss: 0.622747, acc: 65.62%] [G loss: 1.892792]\n",
      "epoch:20 step:19069 [D loss: 0.685304, acc: 57.81%] [G loss: 1.964921]\n",
      "epoch:20 step:19070 [D loss: 0.607508, acc: 67.19%] [G loss: 1.950918]\n",
      "epoch:20 step:19071 [D loss: 0.594668, acc: 64.84%] [G loss: 2.062643]\n",
      "epoch:20 step:19072 [D loss: 0.612154, acc: 65.62%] [G loss: 1.919528]\n",
      "epoch:20 step:19073 [D loss: 0.646914, acc: 60.16%] [G loss: 1.957942]\n",
      "epoch:20 step:19074 [D loss: 0.661955, acc: 57.81%] [G loss: 1.968391]\n",
      "epoch:20 step:19075 [D loss: 0.623091, acc: 64.84%] [G loss: 2.086672]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19076 [D loss: 0.625541, acc: 67.97%] [G loss: 1.963360]\n",
      "epoch:20 step:19077 [D loss: 0.671018, acc: 60.94%] [G loss: 2.006174]\n",
      "epoch:20 step:19078 [D loss: 0.620027, acc: 69.53%] [G loss: 2.023994]\n",
      "epoch:20 step:19079 [D loss: 0.613793, acc: 69.53%] [G loss: 1.936821]\n",
      "epoch:20 step:19080 [D loss: 0.674725, acc: 57.81%] [G loss: 1.962594]\n",
      "epoch:20 step:19081 [D loss: 0.707170, acc: 50.78%] [G loss: 1.785712]\n",
      "epoch:20 step:19082 [D loss: 0.715050, acc: 50.00%] [G loss: 1.705686]\n",
      "epoch:20 step:19083 [D loss: 0.661552, acc: 64.84%] [G loss: 1.979930]\n",
      "epoch:20 step:19084 [D loss: 0.644633, acc: 65.62%] [G loss: 1.895922]\n",
      "epoch:20 step:19085 [D loss: 0.608837, acc: 64.84%] [G loss: 2.043232]\n",
      "epoch:20 step:19086 [D loss: 0.553639, acc: 71.09%] [G loss: 2.091857]\n",
      "epoch:20 step:19087 [D loss: 0.575578, acc: 71.88%] [G loss: 2.203894]\n",
      "epoch:20 step:19088 [D loss: 0.649899, acc: 59.38%] [G loss: 1.817276]\n",
      "epoch:20 step:19089 [D loss: 0.696272, acc: 57.81%] [G loss: 1.775675]\n",
      "epoch:20 step:19090 [D loss: 0.673975, acc: 58.59%] [G loss: 1.838384]\n",
      "epoch:20 step:19091 [D loss: 0.669753, acc: 60.94%] [G loss: 1.947670]\n",
      "epoch:20 step:19092 [D loss: 0.746840, acc: 49.22%] [G loss: 1.632238]\n",
      "epoch:20 step:19093 [D loss: 0.647843, acc: 64.06%] [G loss: 1.828425]\n",
      "epoch:20 step:19094 [D loss: 0.567736, acc: 74.22%] [G loss: 1.990136]\n",
      "epoch:20 step:19095 [D loss: 0.687745, acc: 53.91%] [G loss: 1.744087]\n",
      "epoch:20 step:19096 [D loss: 0.683863, acc: 57.81%] [G loss: 1.761359]\n",
      "epoch:20 step:19097 [D loss: 0.699913, acc: 57.03%] [G loss: 1.923036]\n",
      "epoch:20 step:19098 [D loss: 0.641321, acc: 64.84%] [G loss: 1.932848]\n",
      "epoch:20 step:19099 [D loss: 0.634179, acc: 60.16%] [G loss: 1.904017]\n",
      "epoch:20 step:19100 [D loss: 0.651959, acc: 64.06%] [G loss: 1.926943]\n",
      "epoch:20 step:19101 [D loss: 0.622793, acc: 65.62%] [G loss: 1.887082]\n",
      "epoch:20 step:19102 [D loss: 0.661677, acc: 64.06%] [G loss: 1.892439]\n",
      "epoch:20 step:19103 [D loss: 0.624982, acc: 65.62%] [G loss: 1.841312]\n",
      "epoch:20 step:19104 [D loss: 0.637543, acc: 65.62%] [G loss: 1.814332]\n",
      "epoch:20 step:19105 [D loss: 0.650931, acc: 56.25%] [G loss: 1.963894]\n",
      "epoch:20 step:19106 [D loss: 0.652171, acc: 57.03%] [G loss: 1.811625]\n",
      "epoch:20 step:19107 [D loss: 0.629757, acc: 65.62%] [G loss: 2.031551]\n",
      "epoch:20 step:19108 [D loss: 0.626539, acc: 60.94%] [G loss: 1.803566]\n",
      "epoch:20 step:19109 [D loss: 0.637159, acc: 68.75%] [G loss: 1.948758]\n",
      "epoch:20 step:19110 [D loss: 0.572753, acc: 71.09%] [G loss: 2.009927]\n",
      "epoch:20 step:19111 [D loss: 0.613806, acc: 67.19%] [G loss: 2.013331]\n",
      "epoch:20 step:19112 [D loss: 0.631797, acc: 64.84%] [G loss: 1.853998]\n",
      "epoch:20 step:19113 [D loss: 0.702740, acc: 57.03%] [G loss: 1.722070]\n",
      "epoch:20 step:19114 [D loss: 0.594947, acc: 65.62%] [G loss: 1.890437]\n",
      "epoch:20 step:19115 [D loss: 0.614666, acc: 67.19%] [G loss: 1.886373]\n",
      "epoch:20 step:19116 [D loss: 0.684118, acc: 54.69%] [G loss: 1.782448]\n",
      "epoch:20 step:19117 [D loss: 0.690527, acc: 55.47%] [G loss: 1.677819]\n",
      "epoch:20 step:19118 [D loss: 0.655658, acc: 56.25%] [G loss: 1.828569]\n",
      "epoch:20 step:19119 [D loss: 0.666292, acc: 62.50%] [G loss: 1.894955]\n",
      "epoch:20 step:19120 [D loss: 0.668526, acc: 57.03%] [G loss: 1.930795]\n",
      "epoch:20 step:19121 [D loss: 0.657585, acc: 64.06%] [G loss: 1.962311]\n",
      "epoch:20 step:19122 [D loss: 0.616132, acc: 60.16%] [G loss: 1.939948]\n",
      "epoch:20 step:19123 [D loss: 0.628856, acc: 62.50%] [G loss: 1.924371]\n",
      "epoch:20 step:19124 [D loss: 0.630452, acc: 60.94%] [G loss: 1.884458]\n",
      "epoch:20 step:19125 [D loss: 0.620973, acc: 67.19%] [G loss: 2.140406]\n",
      "epoch:20 step:19126 [D loss: 0.721488, acc: 60.16%] [G loss: 1.650590]\n",
      "epoch:20 step:19127 [D loss: 0.654111, acc: 58.59%] [G loss: 1.810838]\n",
      "epoch:20 step:19128 [D loss: 0.635130, acc: 64.06%] [G loss: 1.856733]\n",
      "epoch:20 step:19129 [D loss: 0.620947, acc: 65.62%] [G loss: 1.772642]\n",
      "epoch:20 step:19130 [D loss: 0.634996, acc: 65.62%] [G loss: 1.867739]\n",
      "epoch:20 step:19131 [D loss: 0.646381, acc: 64.84%] [G loss: 1.702399]\n",
      "epoch:20 step:19132 [D loss: 0.595587, acc: 71.09%] [G loss: 1.908614]\n",
      "epoch:20 step:19133 [D loss: 0.626497, acc: 65.62%] [G loss: 1.794126]\n",
      "epoch:20 step:19134 [D loss: 0.640777, acc: 64.06%] [G loss: 1.777897]\n",
      "epoch:20 step:19135 [D loss: 0.618179, acc: 70.31%] [G loss: 2.014583]\n",
      "epoch:20 step:19136 [D loss: 0.673715, acc: 59.38%] [G loss: 1.750826]\n",
      "epoch:20 step:19137 [D loss: 0.675554, acc: 61.72%] [G loss: 1.762582]\n",
      "epoch:20 step:19138 [D loss: 0.637496, acc: 68.75%] [G loss: 1.792137]\n",
      "epoch:20 step:19139 [D loss: 0.675557, acc: 55.47%] [G loss: 1.964320]\n",
      "epoch:20 step:19140 [D loss: 0.677403, acc: 57.81%] [G loss: 1.826326]\n",
      "epoch:20 step:19141 [D loss: 0.664896, acc: 60.94%] [G loss: 1.997176]\n",
      "epoch:20 step:19142 [D loss: 0.631734, acc: 67.97%] [G loss: 1.891512]\n",
      "epoch:20 step:19143 [D loss: 0.652890, acc: 59.38%] [G loss: 1.867527]\n",
      "epoch:20 step:19144 [D loss: 0.682875, acc: 60.16%] [G loss: 2.059944]\n",
      "epoch:20 step:19145 [D loss: 0.572043, acc: 67.97%] [G loss: 2.070787]\n",
      "epoch:20 step:19146 [D loss: 0.616915, acc: 65.62%] [G loss: 2.144107]\n",
      "epoch:20 step:19147 [D loss: 0.626366, acc: 70.31%] [G loss: 1.868327]\n",
      "epoch:20 step:19148 [D loss: 0.653264, acc: 60.94%] [G loss: 1.870661]\n",
      "epoch:20 step:19149 [D loss: 0.651081, acc: 59.38%] [G loss: 1.996388]\n",
      "epoch:20 step:19150 [D loss: 0.654394, acc: 56.25%] [G loss: 1.820026]\n",
      "epoch:20 step:19151 [D loss: 0.631948, acc: 63.28%] [G loss: 1.957782]\n",
      "epoch:20 step:19152 [D loss: 0.681484, acc: 56.25%] [G loss: 1.873878]\n",
      "epoch:20 step:19153 [D loss: 0.649365, acc: 63.28%] [G loss: 2.037640]\n",
      "epoch:20 step:19154 [D loss: 0.612242, acc: 64.84%] [G loss: 1.823143]\n",
      "epoch:20 step:19155 [D loss: 0.650206, acc: 57.81%] [G loss: 1.855419]\n",
      "epoch:20 step:19156 [D loss: 0.622172, acc: 67.19%] [G loss: 2.006123]\n",
      "epoch:20 step:19157 [D loss: 0.708095, acc: 58.59%] [G loss: 1.917315]\n",
      "epoch:20 step:19158 [D loss: 0.628768, acc: 66.41%] [G loss: 1.905817]\n",
      "epoch:20 step:19159 [D loss: 0.686754, acc: 57.03%] [G loss: 1.818442]\n",
      "epoch:20 step:19160 [D loss: 0.716009, acc: 51.56%] [G loss: 1.855214]\n",
      "epoch:20 step:19161 [D loss: 0.674531, acc: 52.34%] [G loss: 1.852655]\n",
      "epoch:20 step:19162 [D loss: 0.676778, acc: 62.50%] [G loss: 1.794939]\n",
      "epoch:20 step:19163 [D loss: 0.666454, acc: 60.16%] [G loss: 1.776614]\n",
      "epoch:20 step:19164 [D loss: 0.684600, acc: 54.69%] [G loss: 1.859001]\n",
      "epoch:20 step:19165 [D loss: 0.660437, acc: 57.03%] [G loss: 1.871317]\n",
      "epoch:20 step:19166 [D loss: 0.631864, acc: 60.16%] [G loss: 1.787520]\n",
      "epoch:20 step:19167 [D loss: 0.687138, acc: 53.12%] [G loss: 1.880223]\n",
      "epoch:20 step:19168 [D loss: 0.589403, acc: 67.19%] [G loss: 2.215132]\n",
      "epoch:20 step:19169 [D loss: 0.603000, acc: 65.62%] [G loss: 2.014370]\n",
      "epoch:20 step:19170 [D loss: 0.574049, acc: 73.44%] [G loss: 1.994921]\n",
      "epoch:20 step:19171 [D loss: 0.680934, acc: 64.06%] [G loss: 1.969988]\n",
      "epoch:20 step:19172 [D loss: 0.678888, acc: 55.47%] [G loss: 1.930274]\n",
      "epoch:20 step:19173 [D loss: 0.681109, acc: 60.16%] [G loss: 1.823326]\n",
      "epoch:20 step:19174 [D loss: 0.648642, acc: 62.50%] [G loss: 1.886575]\n",
      "epoch:20 step:19175 [D loss: 0.710620, acc: 53.12%] [G loss: 1.837903]\n",
      "epoch:20 step:19176 [D loss: 0.570715, acc: 74.22%] [G loss: 1.971374]\n",
      "epoch:20 step:19177 [D loss: 0.736742, acc: 52.34%] [G loss: 1.762748]\n",
      "epoch:20 step:19178 [D loss: 0.670070, acc: 57.81%] [G loss: 1.740388]\n",
      "epoch:20 step:19179 [D loss: 0.671749, acc: 58.59%] [G loss: 1.653879]\n",
      "epoch:20 step:19180 [D loss: 0.637190, acc: 66.41%] [G loss: 1.809204]\n",
      "epoch:20 step:19181 [D loss: 0.652806, acc: 59.38%] [G loss: 1.775202]\n",
      "epoch:20 step:19182 [D loss: 0.686305, acc: 57.03%] [G loss: 1.778216]\n",
      "epoch:20 step:19183 [D loss: 0.666979, acc: 60.94%] [G loss: 1.707488]\n",
      "epoch:20 step:19184 [D loss: 0.683725, acc: 56.25%] [G loss: 1.739940]\n",
      "epoch:20 step:19185 [D loss: 0.681980, acc: 54.69%] [G loss: 1.848660]\n",
      "epoch:20 step:19186 [D loss: 0.634194, acc: 64.84%] [G loss: 1.732667]\n",
      "epoch:20 step:19187 [D loss: 0.590574, acc: 68.75%] [G loss: 1.823564]\n",
      "epoch:20 step:19188 [D loss: 0.663183, acc: 57.81%] [G loss: 1.759016]\n",
      "epoch:20 step:19189 [D loss: 0.674448, acc: 57.81%] [G loss: 1.803455]\n",
      "epoch:20 step:19190 [D loss: 0.661261, acc: 64.06%] [G loss: 1.889822]\n",
      "epoch:20 step:19191 [D loss: 0.668007, acc: 55.47%] [G loss: 1.899366]\n",
      "epoch:20 step:19192 [D loss: 0.693459, acc: 51.56%] [G loss: 1.797234]\n",
      "epoch:20 step:19193 [D loss: 0.623290, acc: 64.84%] [G loss: 1.891712]\n",
      "epoch:20 step:19194 [D loss: 0.645026, acc: 58.59%] [G loss: 1.868212]\n",
      "epoch:20 step:19195 [D loss: 0.675906, acc: 58.59%] [G loss: 1.793285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19196 [D loss: 0.685815, acc: 57.81%] [G loss: 1.759708]\n",
      "epoch:20 step:19197 [D loss: 0.614525, acc: 70.31%] [G loss: 1.868879]\n",
      "epoch:20 step:19198 [D loss: 0.644105, acc: 64.06%] [G loss: 1.786210]\n",
      "epoch:20 step:19199 [D loss: 0.651151, acc: 60.94%] [G loss: 1.710370]\n",
      "epoch:20 step:19200 [D loss: 0.678346, acc: 57.03%] [G loss: 1.773238]\n",
      "epoch:20 step:19201 [D loss: 0.672019, acc: 59.38%] [G loss: 1.913032]\n",
      "epoch:20 step:19202 [D loss: 0.655423, acc: 59.38%] [G loss: 1.926499]\n",
      "epoch:20 step:19203 [D loss: 0.673732, acc: 55.47%] [G loss: 1.848218]\n",
      "epoch:20 step:19204 [D loss: 0.653472, acc: 65.62%] [G loss: 1.901612]\n",
      "epoch:20 step:19205 [D loss: 0.651350, acc: 67.19%] [G loss: 1.928182]\n",
      "epoch:20 step:19206 [D loss: 0.614303, acc: 64.84%] [G loss: 1.959528]\n",
      "epoch:20 step:19207 [D loss: 0.662657, acc: 56.25%] [G loss: 1.827014]\n",
      "epoch:20 step:19208 [D loss: 0.609174, acc: 66.41%] [G loss: 2.016886]\n",
      "epoch:20 step:19209 [D loss: 0.640434, acc: 63.28%] [G loss: 2.070227]\n",
      "epoch:20 step:19210 [D loss: 0.568141, acc: 71.09%] [G loss: 2.022534]\n",
      "epoch:20 step:19211 [D loss: 0.553242, acc: 68.75%] [G loss: 2.366279]\n",
      "epoch:20 step:19212 [D loss: 0.638869, acc: 64.06%] [G loss: 2.230870]\n",
      "epoch:20 step:19213 [D loss: 0.728020, acc: 50.00%] [G loss: 1.708305]\n",
      "epoch:20 step:19214 [D loss: 0.648675, acc: 57.81%] [G loss: 1.993268]\n",
      "epoch:20 step:19215 [D loss: 0.698421, acc: 60.94%] [G loss: 1.906876]\n",
      "epoch:20 step:19216 [D loss: 0.614693, acc: 63.28%] [G loss: 1.966065]\n",
      "epoch:20 step:19217 [D loss: 0.710044, acc: 54.69%] [G loss: 1.690101]\n",
      "epoch:20 step:19218 [D loss: 0.629415, acc: 64.06%] [G loss: 1.815651]\n",
      "epoch:20 step:19219 [D loss: 0.633099, acc: 65.62%] [G loss: 1.871199]\n",
      "epoch:20 step:19220 [D loss: 0.597574, acc: 69.53%] [G loss: 1.953037]\n",
      "epoch:20 step:19221 [D loss: 0.605754, acc: 70.31%] [G loss: 2.144653]\n",
      "epoch:20 step:19222 [D loss: 0.668257, acc: 59.38%] [G loss: 1.716959]\n",
      "epoch:20 step:19223 [D loss: 0.661692, acc: 57.81%] [G loss: 1.810345]\n",
      "epoch:20 step:19224 [D loss: 0.663793, acc: 60.94%] [G loss: 1.921397]\n",
      "epoch:20 step:19225 [D loss: 0.651004, acc: 61.72%] [G loss: 1.907779]\n",
      "epoch:20 step:19226 [D loss: 0.627169, acc: 65.62%] [G loss: 1.839502]\n",
      "epoch:20 step:19227 [D loss: 0.660299, acc: 60.94%] [G loss: 1.878541]\n",
      "epoch:20 step:19228 [D loss: 0.659576, acc: 64.06%] [G loss: 2.115114]\n",
      "epoch:20 step:19229 [D loss: 0.621577, acc: 66.41%] [G loss: 1.838716]\n",
      "epoch:20 step:19230 [D loss: 0.619766, acc: 66.41%] [G loss: 1.912564]\n",
      "epoch:20 step:19231 [D loss: 0.622878, acc: 64.06%] [G loss: 1.967722]\n",
      "epoch:20 step:19232 [D loss: 0.686043, acc: 57.81%] [G loss: 1.806250]\n",
      "epoch:20 step:19233 [D loss: 0.635308, acc: 62.50%] [G loss: 1.834774]\n",
      "epoch:20 step:19234 [D loss: 0.627499, acc: 63.28%] [G loss: 2.127739]\n",
      "epoch:20 step:19235 [D loss: 0.600590, acc: 68.75%] [G loss: 2.122996]\n",
      "epoch:20 step:19236 [D loss: 0.617178, acc: 62.50%] [G loss: 2.014852]\n",
      "epoch:20 step:19237 [D loss: 0.643787, acc: 58.59%] [G loss: 1.975611]\n",
      "epoch:20 step:19238 [D loss: 0.618498, acc: 67.97%] [G loss: 2.077519]\n",
      "epoch:20 step:19239 [D loss: 0.565358, acc: 72.66%] [G loss: 2.150023]\n",
      "epoch:20 step:19240 [D loss: 0.765337, acc: 50.78%] [G loss: 1.791948]\n",
      "epoch:20 step:19241 [D loss: 0.742264, acc: 49.22%] [G loss: 1.727860]\n",
      "epoch:20 step:19242 [D loss: 0.738327, acc: 47.66%] [G loss: 1.619901]\n",
      "epoch:20 step:19243 [D loss: 0.653152, acc: 60.16%] [G loss: 1.859255]\n",
      "epoch:20 step:19244 [D loss: 0.614159, acc: 64.84%] [G loss: 2.086697]\n",
      "epoch:20 step:19245 [D loss: 0.649643, acc: 63.28%] [G loss: 1.853441]\n",
      "epoch:20 step:19246 [D loss: 0.729002, acc: 53.91%] [G loss: 1.783377]\n",
      "epoch:20 step:19247 [D loss: 0.658759, acc: 58.59%] [G loss: 1.732266]\n",
      "epoch:20 step:19248 [D loss: 0.576391, acc: 72.66%] [G loss: 2.077423]\n",
      "epoch:20 step:19249 [D loss: 0.633961, acc: 58.59%] [G loss: 1.789180]\n",
      "epoch:20 step:19250 [D loss: 0.675465, acc: 57.81%] [G loss: 1.869851]\n",
      "epoch:20 step:19251 [D loss: 0.705277, acc: 48.44%] [G loss: 1.770185]\n",
      "epoch:20 step:19252 [D loss: 0.655005, acc: 63.28%] [G loss: 1.822224]\n",
      "epoch:20 step:19253 [D loss: 0.641932, acc: 65.62%] [G loss: 1.848486]\n",
      "epoch:20 step:19254 [D loss: 0.644717, acc: 60.16%] [G loss: 1.907069]\n",
      "epoch:20 step:19255 [D loss: 0.657916, acc: 57.81%] [G loss: 1.831733]\n",
      "epoch:20 step:19256 [D loss: 0.631912, acc: 65.62%] [G loss: 1.928580]\n",
      "epoch:20 step:19257 [D loss: 0.623820, acc: 66.41%] [G loss: 1.913432]\n",
      "epoch:20 step:19258 [D loss: 0.640860, acc: 64.06%] [G loss: 1.875355]\n",
      "epoch:20 step:19259 [D loss: 0.684854, acc: 56.25%] [G loss: 1.828377]\n",
      "epoch:20 step:19260 [D loss: 0.615591, acc: 63.28%] [G loss: 1.917124]\n",
      "epoch:20 step:19261 [D loss: 0.607451, acc: 65.62%] [G loss: 1.881232]\n",
      "epoch:20 step:19262 [D loss: 0.634208, acc: 63.28%] [G loss: 2.112805]\n",
      "epoch:20 step:19263 [D loss: 0.616777, acc: 60.16%] [G loss: 1.891283]\n",
      "epoch:20 step:19264 [D loss: 0.649174, acc: 64.84%] [G loss: 1.986821]\n",
      "epoch:20 step:19265 [D loss: 0.659337, acc: 57.03%] [G loss: 1.774228]\n",
      "epoch:20 step:19266 [D loss: 0.653422, acc: 62.50%] [G loss: 1.965150]\n",
      "epoch:20 step:19267 [D loss: 0.631160, acc: 61.72%] [G loss: 1.867463]\n",
      "epoch:20 step:19268 [D loss: 0.665426, acc: 61.72%] [G loss: 1.767777]\n",
      "epoch:20 step:19269 [D loss: 0.728053, acc: 51.56%] [G loss: 1.754135]\n",
      "epoch:20 step:19270 [D loss: 0.623947, acc: 64.06%] [G loss: 1.715654]\n",
      "epoch:20 step:19271 [D loss: 0.647386, acc: 63.28%] [G loss: 1.758588]\n",
      "epoch:20 step:19272 [D loss: 0.661221, acc: 57.81%] [G loss: 1.939045]\n",
      "epoch:20 step:19273 [D loss: 0.605706, acc: 66.41%] [G loss: 1.957484]\n",
      "epoch:20 step:19274 [D loss: 0.595258, acc: 71.88%] [G loss: 2.006303]\n",
      "epoch:20 step:19275 [D loss: 0.646237, acc: 66.41%] [G loss: 1.863226]\n",
      "epoch:20 step:19276 [D loss: 0.631773, acc: 67.97%] [G loss: 2.004055]\n",
      "epoch:20 step:19277 [D loss: 0.661905, acc: 59.38%] [G loss: 1.849730]\n",
      "epoch:20 step:19278 [D loss: 0.645940, acc: 56.25%] [G loss: 1.760122]\n",
      "epoch:20 step:19279 [D loss: 0.667574, acc: 57.03%] [G loss: 1.932576]\n",
      "epoch:20 step:19280 [D loss: 0.628821, acc: 60.94%] [G loss: 1.962790]\n",
      "epoch:20 step:19281 [D loss: 0.635010, acc: 60.16%] [G loss: 1.872662]\n",
      "epoch:20 step:19282 [D loss: 0.675947, acc: 60.94%] [G loss: 1.970966]\n",
      "epoch:20 step:19283 [D loss: 0.641411, acc: 61.72%] [G loss: 1.828979]\n",
      "epoch:20 step:19284 [D loss: 0.621097, acc: 60.16%] [G loss: 1.839064]\n",
      "epoch:20 step:19285 [D loss: 0.657432, acc: 62.50%] [G loss: 1.758294]\n",
      "epoch:20 step:19286 [D loss: 0.685626, acc: 57.03%] [G loss: 1.838558]\n",
      "epoch:20 step:19287 [D loss: 0.668799, acc: 58.59%] [G loss: 1.970193]\n",
      "epoch:20 step:19288 [D loss: 0.669771, acc: 59.38%] [G loss: 1.851248]\n",
      "epoch:20 step:19289 [D loss: 0.612888, acc: 67.19%] [G loss: 1.941180]\n",
      "epoch:20 step:19290 [D loss: 0.629766, acc: 69.53%] [G loss: 1.846208]\n",
      "epoch:20 step:19291 [D loss: 0.660944, acc: 64.84%] [G loss: 1.940828]\n",
      "epoch:20 step:19292 [D loss: 0.640502, acc: 65.62%] [G loss: 1.845617]\n",
      "epoch:20 step:19293 [D loss: 0.705212, acc: 53.91%] [G loss: 1.753535]\n",
      "epoch:20 step:19294 [D loss: 0.579327, acc: 72.66%] [G loss: 1.982810]\n",
      "epoch:20 step:19295 [D loss: 0.698598, acc: 57.03%] [G loss: 1.848991]\n",
      "epoch:20 step:19296 [D loss: 0.625492, acc: 67.19%] [G loss: 2.024074]\n",
      "epoch:20 step:19297 [D loss: 0.574448, acc: 73.44%] [G loss: 2.105672]\n",
      "epoch:20 step:19298 [D loss: 0.623219, acc: 64.84%] [G loss: 2.187490]\n",
      "epoch:20 step:19299 [D loss: 0.686869, acc: 48.44%] [G loss: 1.899317]\n",
      "epoch:20 step:19300 [D loss: 0.614282, acc: 64.06%] [G loss: 1.882406]\n",
      "epoch:20 step:19301 [D loss: 0.616144, acc: 66.41%] [G loss: 1.937635]\n",
      "epoch:20 step:19302 [D loss: 0.614860, acc: 72.66%] [G loss: 1.904849]\n",
      "epoch:20 step:19303 [D loss: 0.607604, acc: 66.41%] [G loss: 1.906760]\n",
      "epoch:20 step:19304 [D loss: 0.601710, acc: 70.31%] [G loss: 2.137578]\n",
      "epoch:20 step:19305 [D loss: 0.727363, acc: 55.47%] [G loss: 1.947569]\n",
      "epoch:20 step:19306 [D loss: 0.710344, acc: 54.69%] [G loss: 1.781615]\n",
      "epoch:20 step:19307 [D loss: 0.661487, acc: 61.72%] [G loss: 1.915615]\n",
      "epoch:20 step:19308 [D loss: 0.619010, acc: 65.62%] [G loss: 1.798942]\n",
      "epoch:20 step:19309 [D loss: 0.611976, acc: 68.75%] [G loss: 1.779788]\n",
      "epoch:20 step:19310 [D loss: 0.702642, acc: 61.72%] [G loss: 1.987602]\n",
      "epoch:20 step:19311 [D loss: 0.631434, acc: 64.84%] [G loss: 1.906143]\n",
      "epoch:20 step:19312 [D loss: 0.655601, acc: 60.16%] [G loss: 1.943284]\n",
      "epoch:20 step:19313 [D loss: 0.645575, acc: 61.72%] [G loss: 1.806449]\n",
      "epoch:20 step:19314 [D loss: 0.661834, acc: 57.81%] [G loss: 1.878353]\n",
      "epoch:20 step:19315 [D loss: 0.612271, acc: 69.53%] [G loss: 1.909639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19316 [D loss: 0.654839, acc: 54.69%] [G loss: 1.787566]\n",
      "epoch:20 step:19317 [D loss: 0.671642, acc: 60.94%] [G loss: 1.871112]\n",
      "epoch:20 step:19318 [D loss: 0.634109, acc: 67.19%] [G loss: 1.938048]\n",
      "epoch:20 step:19319 [D loss: 0.685745, acc: 60.16%] [G loss: 1.748646]\n",
      "epoch:20 step:19320 [D loss: 0.634467, acc: 67.19%] [G loss: 1.807163]\n",
      "epoch:20 step:19321 [D loss: 0.627933, acc: 65.62%] [G loss: 1.898162]\n",
      "epoch:20 step:19322 [D loss: 0.631566, acc: 62.50%] [G loss: 1.824765]\n",
      "epoch:20 step:19323 [D loss: 0.670364, acc: 64.84%] [G loss: 1.836985]\n",
      "epoch:20 step:19324 [D loss: 0.676697, acc: 59.38%] [G loss: 1.840653]\n",
      "epoch:20 step:19325 [D loss: 0.648916, acc: 57.81%] [G loss: 1.898981]\n",
      "epoch:20 step:19326 [D loss: 0.657342, acc: 62.50%] [G loss: 1.756269]\n",
      "epoch:20 step:19327 [D loss: 0.661438, acc: 54.69%] [G loss: 1.807493]\n",
      "epoch:20 step:19328 [D loss: 0.611350, acc: 64.06%] [G loss: 1.895603]\n",
      "epoch:20 step:19329 [D loss: 0.601989, acc: 66.41%] [G loss: 1.841799]\n",
      "epoch:20 step:19330 [D loss: 0.646036, acc: 60.94%] [G loss: 1.754927]\n",
      "epoch:20 step:19331 [D loss: 0.638912, acc: 62.50%] [G loss: 1.873135]\n",
      "epoch:20 step:19332 [D loss: 0.624061, acc: 67.97%] [G loss: 2.001822]\n",
      "epoch:20 step:19333 [D loss: 0.651302, acc: 64.84%] [G loss: 1.922530]\n",
      "epoch:20 step:19334 [D loss: 0.659580, acc: 61.72%] [G loss: 1.849861]\n",
      "epoch:20 step:19335 [D loss: 0.678965, acc: 57.81%] [G loss: 1.825413]\n",
      "epoch:20 step:19336 [D loss: 0.645054, acc: 64.06%] [G loss: 1.932850]\n",
      "epoch:20 step:19337 [D loss: 0.662348, acc: 60.16%] [G loss: 1.833007]\n",
      "epoch:20 step:19338 [D loss: 0.624989, acc: 67.97%] [G loss: 1.885434]\n",
      "epoch:20 step:19339 [D loss: 0.643841, acc: 64.06%] [G loss: 1.776341]\n",
      "epoch:20 step:19340 [D loss: 0.666664, acc: 60.94%] [G loss: 1.740955]\n",
      "epoch:20 step:19341 [D loss: 0.629386, acc: 64.06%] [G loss: 1.907853]\n",
      "epoch:20 step:19342 [D loss: 0.648111, acc: 60.16%] [G loss: 2.087067]\n",
      "epoch:20 step:19343 [D loss: 0.643177, acc: 59.38%] [G loss: 1.952003]\n",
      "epoch:20 step:19344 [D loss: 0.642626, acc: 64.84%] [G loss: 1.841603]\n",
      "epoch:20 step:19345 [D loss: 0.627986, acc: 62.50%] [G loss: 2.092664]\n",
      "epoch:20 step:19346 [D loss: 0.677183, acc: 57.03%] [G loss: 1.818220]\n",
      "epoch:20 step:19347 [D loss: 0.672340, acc: 60.16%] [G loss: 1.825076]\n",
      "epoch:20 step:19348 [D loss: 0.667078, acc: 61.72%] [G loss: 1.886955]\n",
      "epoch:20 step:19349 [D loss: 0.674310, acc: 57.81%] [G loss: 1.782485]\n",
      "epoch:20 step:19350 [D loss: 0.610028, acc: 68.75%] [G loss: 1.885837]\n",
      "epoch:20 step:19351 [D loss: 0.709992, acc: 51.56%] [G loss: 1.801835]\n",
      "epoch:20 step:19352 [D loss: 0.704198, acc: 51.56%] [G loss: 1.774046]\n",
      "epoch:20 step:19353 [D loss: 0.624706, acc: 67.19%] [G loss: 1.819237]\n",
      "epoch:20 step:19354 [D loss: 0.609700, acc: 67.19%] [G loss: 1.731728]\n",
      "epoch:20 step:19355 [D loss: 0.691911, acc: 59.38%] [G loss: 1.810674]\n",
      "epoch:20 step:19356 [D loss: 0.646615, acc: 66.41%] [G loss: 1.869421]\n",
      "epoch:20 step:19357 [D loss: 0.665609, acc: 54.69%] [G loss: 1.730791]\n",
      "epoch:20 step:19358 [D loss: 0.632589, acc: 69.53%] [G loss: 1.982433]\n",
      "epoch:20 step:19359 [D loss: 0.621575, acc: 71.09%] [G loss: 1.751506]\n",
      "epoch:20 step:19360 [D loss: 0.660635, acc: 54.69%] [G loss: 2.020228]\n",
      "epoch:20 step:19361 [D loss: 0.633062, acc: 64.06%] [G loss: 1.800098]\n",
      "epoch:20 step:19362 [D loss: 0.666245, acc: 53.12%] [G loss: 2.036180]\n",
      "epoch:20 step:19363 [D loss: 0.664755, acc: 63.28%] [G loss: 2.028494]\n",
      "epoch:20 step:19364 [D loss: 0.586376, acc: 64.06%] [G loss: 1.980726]\n",
      "epoch:20 step:19365 [D loss: 0.692806, acc: 54.69%] [G loss: 1.787723]\n",
      "epoch:20 step:19366 [D loss: 0.610826, acc: 65.62%] [G loss: 1.809996]\n",
      "epoch:20 step:19367 [D loss: 0.655718, acc: 62.50%] [G loss: 1.921232]\n",
      "epoch:20 step:19368 [D loss: 0.658276, acc: 55.47%] [G loss: 1.764687]\n",
      "epoch:20 step:19369 [D loss: 0.601200, acc: 68.75%] [G loss: 2.006026]\n",
      "epoch:20 step:19370 [D loss: 0.647708, acc: 61.72%] [G loss: 1.830343]\n",
      "epoch:20 step:19371 [D loss: 0.648697, acc: 55.47%] [G loss: 1.938823]\n",
      "epoch:20 step:19372 [D loss: 0.653850, acc: 61.72%] [G loss: 1.864883]\n",
      "epoch:20 step:19373 [D loss: 0.639348, acc: 63.28%] [G loss: 2.091307]\n",
      "epoch:20 step:19374 [D loss: 0.622371, acc: 69.53%] [G loss: 1.869367]\n",
      "epoch:20 step:19375 [D loss: 0.656251, acc: 66.41%] [G loss: 1.997974]\n",
      "epoch:20 step:19376 [D loss: 0.645050, acc: 60.16%] [G loss: 1.943133]\n",
      "epoch:20 step:19377 [D loss: 0.651496, acc: 59.38%] [G loss: 2.019476]\n",
      "epoch:20 step:19378 [D loss: 0.595007, acc: 70.31%] [G loss: 1.899075]\n",
      "epoch:20 step:19379 [D loss: 0.712416, acc: 54.69%] [G loss: 1.888349]\n",
      "epoch:20 step:19380 [D loss: 0.698784, acc: 53.91%] [G loss: 1.896502]\n",
      "epoch:20 step:19381 [D loss: 0.637578, acc: 67.97%] [G loss: 1.987412]\n",
      "epoch:20 step:19382 [D loss: 0.653378, acc: 62.50%] [G loss: 1.842369]\n",
      "epoch:20 step:19383 [D loss: 0.653743, acc: 64.06%] [G loss: 1.935446]\n",
      "epoch:20 step:19384 [D loss: 0.658533, acc: 57.03%] [G loss: 1.916832]\n",
      "epoch:20 step:19385 [D loss: 0.651070, acc: 57.81%] [G loss: 1.772877]\n",
      "epoch:20 step:19386 [D loss: 0.599423, acc: 67.19%] [G loss: 2.008545]\n",
      "epoch:20 step:19387 [D loss: 0.663871, acc: 64.06%] [G loss: 2.002275]\n",
      "epoch:20 step:19388 [D loss: 0.565708, acc: 69.53%] [G loss: 2.149875]\n",
      "epoch:20 step:19389 [D loss: 0.575379, acc: 70.31%] [G loss: 1.969747]\n",
      "epoch:20 step:19390 [D loss: 0.625111, acc: 65.62%] [G loss: 1.958942]\n",
      "epoch:20 step:19391 [D loss: 0.673928, acc: 64.84%] [G loss: 1.846945]\n",
      "epoch:20 step:19392 [D loss: 0.606915, acc: 67.19%] [G loss: 1.868866]\n",
      "epoch:20 step:19393 [D loss: 0.646413, acc: 67.19%] [G loss: 1.882743]\n",
      "epoch:20 step:19394 [D loss: 0.630890, acc: 62.50%] [G loss: 2.052605]\n",
      "epoch:20 step:19395 [D loss: 0.656586, acc: 64.06%] [G loss: 1.836056]\n",
      "epoch:20 step:19396 [D loss: 0.663501, acc: 62.50%] [G loss: 1.817678]\n",
      "epoch:20 step:19397 [D loss: 0.635620, acc: 61.72%] [G loss: 1.740267]\n",
      "epoch:20 step:19398 [D loss: 0.685274, acc: 55.47%] [G loss: 1.687828]\n",
      "epoch:20 step:19399 [D loss: 0.663378, acc: 63.28%] [G loss: 1.867671]\n",
      "epoch:20 step:19400 [D loss: 0.649720, acc: 64.06%] [G loss: 1.967500]\n",
      "epoch:20 step:19401 [D loss: 0.640999, acc: 64.84%] [G loss: 1.749399]\n",
      "epoch:20 step:19402 [D loss: 0.649152, acc: 64.06%] [G loss: 1.784733]\n",
      "epoch:20 step:19403 [D loss: 0.650263, acc: 56.25%] [G loss: 1.888413]\n",
      "epoch:20 step:19404 [D loss: 0.675376, acc: 56.25%] [G loss: 1.770762]\n",
      "epoch:20 step:19405 [D loss: 0.636842, acc: 61.72%] [G loss: 1.818880]\n",
      "epoch:20 step:19406 [D loss: 0.698159, acc: 53.12%] [G loss: 1.651375]\n",
      "epoch:20 step:19407 [D loss: 0.670704, acc: 64.06%] [G loss: 1.814059]\n",
      "epoch:20 step:19408 [D loss: 0.632416, acc: 66.41%] [G loss: 1.866794]\n",
      "epoch:20 step:19409 [D loss: 0.661742, acc: 60.16%] [G loss: 1.794027]\n",
      "epoch:20 step:19410 [D loss: 0.708006, acc: 56.25%] [G loss: 1.775280]\n",
      "epoch:20 step:19411 [D loss: 0.610011, acc: 65.62%] [G loss: 1.770314]\n",
      "epoch:20 step:19412 [D loss: 0.691798, acc: 57.81%] [G loss: 1.857942]\n",
      "epoch:20 step:19413 [D loss: 0.658524, acc: 60.94%] [G loss: 1.847295]\n",
      "epoch:20 step:19414 [D loss: 0.644395, acc: 60.16%] [G loss: 1.846770]\n",
      "epoch:20 step:19415 [D loss: 0.722804, acc: 54.69%] [G loss: 1.701534]\n",
      "epoch:20 step:19416 [D loss: 0.678146, acc: 61.72%] [G loss: 1.774153]\n",
      "epoch:20 step:19417 [D loss: 0.654627, acc: 62.50%] [G loss: 1.880232]\n",
      "epoch:20 step:19418 [D loss: 0.646433, acc: 60.94%] [G loss: 1.736963]\n",
      "epoch:20 step:19419 [D loss: 0.672587, acc: 58.59%] [G loss: 1.818137]\n",
      "epoch:20 step:19420 [D loss: 0.649280, acc: 64.06%] [G loss: 1.801946]\n",
      "epoch:20 step:19421 [D loss: 0.630075, acc: 67.19%] [G loss: 1.833031]\n",
      "epoch:20 step:19422 [D loss: 0.647767, acc: 59.38%] [G loss: 1.776103]\n",
      "epoch:20 step:19423 [D loss: 0.671771, acc: 60.16%] [G loss: 1.813689]\n",
      "epoch:20 step:19424 [D loss: 0.682343, acc: 59.38%] [G loss: 1.750273]\n",
      "epoch:20 step:19425 [D loss: 0.623245, acc: 65.62%] [G loss: 1.850961]\n",
      "epoch:20 step:19426 [D loss: 0.673195, acc: 63.28%] [G loss: 1.786358]\n",
      "epoch:20 step:19427 [D loss: 0.649382, acc: 62.50%] [G loss: 1.932688]\n",
      "epoch:20 step:19428 [D loss: 0.636753, acc: 64.06%] [G loss: 1.993443]\n",
      "epoch:20 step:19429 [D loss: 0.640042, acc: 58.59%] [G loss: 1.890022]\n",
      "epoch:20 step:19430 [D loss: 0.628495, acc: 67.19%] [G loss: 1.954565]\n",
      "epoch:20 step:19431 [D loss: 0.621091, acc: 64.84%] [G loss: 1.964475]\n",
      "epoch:20 step:19432 [D loss: 0.608862, acc: 64.06%] [G loss: 2.000076]\n",
      "epoch:20 step:19433 [D loss: 0.621958, acc: 67.97%] [G loss: 2.034774]\n",
      "epoch:20 step:19434 [D loss: 0.600783, acc: 71.09%] [G loss: 1.945601]\n",
      "epoch:20 step:19435 [D loss: 0.616232, acc: 67.97%] [G loss: 1.901863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19436 [D loss: 0.698173, acc: 53.91%] [G loss: 1.936383]\n",
      "epoch:20 step:19437 [D loss: 0.668546, acc: 58.59%] [G loss: 1.940821]\n",
      "epoch:20 step:19438 [D loss: 0.660863, acc: 57.03%] [G loss: 1.765210]\n",
      "epoch:20 step:19439 [D loss: 0.649074, acc: 61.72%] [G loss: 1.833370]\n",
      "epoch:20 step:19440 [D loss: 0.622734, acc: 66.41%] [G loss: 1.969765]\n",
      "epoch:20 step:19441 [D loss: 0.625295, acc: 64.06%] [G loss: 1.954021]\n",
      "epoch:20 step:19442 [D loss: 0.680296, acc: 53.91%] [G loss: 1.760997]\n",
      "epoch:20 step:19443 [D loss: 0.689512, acc: 60.16%] [G loss: 1.724353]\n",
      "epoch:20 step:19444 [D loss: 0.665378, acc: 59.38%] [G loss: 1.684408]\n",
      "epoch:20 step:19445 [D loss: 0.666093, acc: 63.28%] [G loss: 1.812826]\n",
      "epoch:20 step:19446 [D loss: 0.643996, acc: 62.50%] [G loss: 1.908670]\n",
      "epoch:20 step:19447 [D loss: 0.652823, acc: 58.59%] [G loss: 1.974987]\n",
      "epoch:20 step:19448 [D loss: 0.623927, acc: 64.84%] [G loss: 1.870644]\n",
      "epoch:20 step:19449 [D loss: 0.580484, acc: 70.31%] [G loss: 1.865303]\n",
      "epoch:20 step:19450 [D loss: 0.645125, acc: 61.72%] [G loss: 1.773648]\n",
      "epoch:20 step:19451 [D loss: 0.606863, acc: 68.75%] [G loss: 1.859869]\n",
      "epoch:20 step:19452 [D loss: 0.617865, acc: 64.06%] [G loss: 1.950914]\n",
      "epoch:20 step:19453 [D loss: 0.673706, acc: 64.06%] [G loss: 1.857920]\n",
      "epoch:20 step:19454 [D loss: 0.674503, acc: 61.72%] [G loss: 1.939252]\n",
      "epoch:20 step:19455 [D loss: 0.668767, acc: 59.38%] [G loss: 1.975845]\n",
      "epoch:20 step:19456 [D loss: 0.671285, acc: 61.72%] [G loss: 1.732231]\n",
      "epoch:20 step:19457 [D loss: 0.716912, acc: 54.69%] [G loss: 1.770933]\n",
      "epoch:20 step:19458 [D loss: 0.626245, acc: 60.16%] [G loss: 1.897050]\n",
      "epoch:20 step:19459 [D loss: 0.638472, acc: 65.62%] [G loss: 2.125474]\n",
      "epoch:20 step:19460 [D loss: 0.667950, acc: 59.38%] [G loss: 1.818494]\n",
      "epoch:20 step:19461 [D loss: 0.604241, acc: 71.88%] [G loss: 2.081146]\n",
      "epoch:20 step:19462 [D loss: 0.720042, acc: 55.47%] [G loss: 1.763919]\n",
      "epoch:20 step:19463 [D loss: 0.646877, acc: 64.06%] [G loss: 1.878390]\n",
      "epoch:20 step:19464 [D loss: 0.652517, acc: 60.94%] [G loss: 1.832908]\n",
      "epoch:20 step:19465 [D loss: 0.639966, acc: 64.84%] [G loss: 1.919110]\n",
      "epoch:20 step:19466 [D loss: 0.622187, acc: 65.62%] [G loss: 2.009604]\n",
      "epoch:20 step:19467 [D loss: 0.670610, acc: 59.38%] [G loss: 1.917702]\n",
      "epoch:20 step:19468 [D loss: 0.623217, acc: 66.41%] [G loss: 1.946027]\n",
      "epoch:20 step:19469 [D loss: 0.676825, acc: 54.69%] [G loss: 1.808951]\n",
      "epoch:20 step:19470 [D loss: 0.684229, acc: 58.59%] [G loss: 1.858212]\n",
      "epoch:20 step:19471 [D loss: 0.660885, acc: 57.81%] [G loss: 1.905522]\n",
      "epoch:20 step:19472 [D loss: 0.643607, acc: 62.50%] [G loss: 1.743085]\n",
      "epoch:20 step:19473 [D loss: 0.670828, acc: 64.06%] [G loss: 1.914154]\n",
      "epoch:20 step:19474 [D loss: 0.636958, acc: 66.41%] [G loss: 1.767771]\n",
      "epoch:20 step:19475 [D loss: 0.689815, acc: 56.25%] [G loss: 1.782949]\n",
      "epoch:20 step:19476 [D loss: 0.632772, acc: 63.28%] [G loss: 1.885129]\n",
      "epoch:20 step:19477 [D loss: 0.633108, acc: 60.94%] [G loss: 1.773805]\n",
      "epoch:20 step:19478 [D loss: 0.663496, acc: 59.38%] [G loss: 1.722641]\n",
      "epoch:20 step:19479 [D loss: 0.723089, acc: 57.03%] [G loss: 1.768666]\n",
      "epoch:20 step:19480 [D loss: 0.657026, acc: 66.41%] [G loss: 1.811284]\n",
      "epoch:20 step:19481 [D loss: 0.654143, acc: 65.62%] [G loss: 1.810932]\n",
      "epoch:20 step:19482 [D loss: 0.584968, acc: 68.75%] [G loss: 1.905491]\n",
      "epoch:20 step:19483 [D loss: 0.622281, acc: 65.62%] [G loss: 1.785574]\n",
      "epoch:20 step:19484 [D loss: 0.677124, acc: 59.38%] [G loss: 1.727379]\n",
      "epoch:20 step:19485 [D loss: 0.672142, acc: 58.59%] [G loss: 1.968123]\n",
      "epoch:20 step:19486 [D loss: 0.663146, acc: 57.81%] [G loss: 1.910786]\n",
      "epoch:20 step:19487 [D loss: 0.627898, acc: 68.75%] [G loss: 1.790015]\n",
      "epoch:20 step:19488 [D loss: 0.622376, acc: 63.28%] [G loss: 1.832744]\n",
      "epoch:20 step:19489 [D loss: 0.667823, acc: 57.81%] [G loss: 1.808192]\n",
      "epoch:20 step:19490 [D loss: 0.626725, acc: 61.72%] [G loss: 1.814843]\n",
      "epoch:20 step:19491 [D loss: 0.693641, acc: 54.69%] [G loss: 1.712928]\n",
      "epoch:20 step:19492 [D loss: 0.665243, acc: 61.72%] [G loss: 1.752126]\n",
      "epoch:20 step:19493 [D loss: 0.630673, acc: 65.62%] [G loss: 1.772047]\n",
      "epoch:20 step:19494 [D loss: 0.620755, acc: 63.28%] [G loss: 1.954110]\n",
      "epoch:20 step:19495 [D loss: 0.664446, acc: 58.59%] [G loss: 1.844435]\n",
      "epoch:20 step:19496 [D loss: 0.646253, acc: 64.84%] [G loss: 1.882331]\n",
      "epoch:20 step:19497 [D loss: 0.613413, acc: 64.84%] [G loss: 1.990519]\n",
      "epoch:20 step:19498 [D loss: 0.649170, acc: 61.72%] [G loss: 1.829284]\n",
      "epoch:20 step:19499 [D loss: 0.620868, acc: 64.84%] [G loss: 1.914185]\n",
      "epoch:20 step:19500 [D loss: 0.622763, acc: 63.28%] [G loss: 1.888524]\n",
      "epoch:20 step:19501 [D loss: 0.632680, acc: 57.03%] [G loss: 1.792537]\n",
      "epoch:20 step:19502 [D loss: 0.679196, acc: 61.72%] [G loss: 1.737140]\n",
      "epoch:20 step:19503 [D loss: 0.628312, acc: 66.41%] [G loss: 1.932154]\n",
      "epoch:20 step:19504 [D loss: 0.690241, acc: 55.47%] [G loss: 1.859402]\n",
      "epoch:20 step:19505 [D loss: 0.670975, acc: 63.28%] [G loss: 1.842641]\n",
      "epoch:20 step:19506 [D loss: 0.675204, acc: 60.94%] [G loss: 1.808698]\n",
      "epoch:20 step:19507 [D loss: 0.613236, acc: 64.84%] [G loss: 1.864678]\n",
      "epoch:20 step:19508 [D loss: 0.681993, acc: 54.69%] [G loss: 1.927150]\n",
      "epoch:20 step:19509 [D loss: 0.655796, acc: 59.38%] [G loss: 1.858524]\n",
      "epoch:20 step:19510 [D loss: 0.632991, acc: 67.19%] [G loss: 1.943719]\n",
      "epoch:20 step:19511 [D loss: 0.649258, acc: 64.06%] [G loss: 1.669527]\n",
      "epoch:20 step:19512 [D loss: 0.636839, acc: 61.72%] [G loss: 1.889352]\n",
      "epoch:20 step:19513 [D loss: 0.681050, acc: 58.59%] [G loss: 1.838923]\n",
      "epoch:20 step:19514 [D loss: 0.626524, acc: 65.62%] [G loss: 2.094308]\n",
      "epoch:20 step:19515 [D loss: 0.575205, acc: 71.09%] [G loss: 2.076781]\n",
      "epoch:20 step:19516 [D loss: 0.662824, acc: 62.50%] [G loss: 1.931864]\n",
      "epoch:20 step:19517 [D loss: 0.630299, acc: 67.97%] [G loss: 1.853280]\n",
      "epoch:20 step:19518 [D loss: 0.644779, acc: 58.59%] [G loss: 1.898074]\n",
      "epoch:20 step:19519 [D loss: 0.664002, acc: 55.47%] [G loss: 1.789138]\n",
      "epoch:20 step:19520 [D loss: 0.642930, acc: 63.28%] [G loss: 1.863902]\n",
      "epoch:20 step:19521 [D loss: 0.575926, acc: 67.97%] [G loss: 1.923543]\n",
      "epoch:20 step:19522 [D loss: 0.671369, acc: 58.59%] [G loss: 2.021626]\n",
      "epoch:20 step:19523 [D loss: 0.660646, acc: 60.94%] [G loss: 1.919969]\n",
      "epoch:20 step:19524 [D loss: 0.693059, acc: 54.69%] [G loss: 1.747656]\n",
      "epoch:20 step:19525 [D loss: 0.636020, acc: 62.50%] [G loss: 1.729383]\n",
      "epoch:20 step:19526 [D loss: 0.576240, acc: 67.19%] [G loss: 1.906365]\n",
      "epoch:20 step:19527 [D loss: 0.631986, acc: 65.62%] [G loss: 1.818764]\n",
      "epoch:20 step:19528 [D loss: 0.695882, acc: 57.81%] [G loss: 1.815296]\n",
      "epoch:20 step:19529 [D loss: 0.660075, acc: 58.59%] [G loss: 1.919733]\n",
      "epoch:20 step:19530 [D loss: 0.616683, acc: 68.75%] [G loss: 2.123115]\n",
      "epoch:20 step:19531 [D loss: 0.655375, acc: 57.03%] [G loss: 2.034534]\n",
      "epoch:20 step:19532 [D loss: 0.609526, acc: 66.41%] [G loss: 1.958550]\n",
      "epoch:20 step:19533 [D loss: 0.654449, acc: 64.84%] [G loss: 1.850281]\n",
      "epoch:20 step:19534 [D loss: 0.681157, acc: 57.03%] [G loss: 1.768206]\n",
      "epoch:20 step:19535 [D loss: 0.752263, acc: 48.44%] [G loss: 1.774701]\n",
      "epoch:20 step:19536 [D loss: 0.653239, acc: 59.38%] [G loss: 1.801714]\n",
      "epoch:20 step:19537 [D loss: 0.693852, acc: 53.91%] [G loss: 1.863122]\n",
      "epoch:20 step:19538 [D loss: 0.627631, acc: 63.28%] [G loss: 1.838363]\n",
      "epoch:20 step:19539 [D loss: 0.630706, acc: 60.16%] [G loss: 1.847990]\n",
      "epoch:20 step:19540 [D loss: 0.676820, acc: 60.16%] [G loss: 1.766307]\n",
      "epoch:20 step:19541 [D loss: 0.639432, acc: 64.84%] [G loss: 1.841240]\n",
      "epoch:20 step:19542 [D loss: 0.581033, acc: 75.78%] [G loss: 1.914918]\n",
      "epoch:20 step:19543 [D loss: 0.628113, acc: 62.50%] [G loss: 1.876046]\n",
      "epoch:20 step:19544 [D loss: 0.635562, acc: 64.06%] [G loss: 1.924261]\n",
      "epoch:20 step:19545 [D loss: 0.646933, acc: 59.38%] [G loss: 1.837216]\n",
      "epoch:20 step:19546 [D loss: 0.701357, acc: 53.91%] [G loss: 2.028732]\n",
      "epoch:20 step:19547 [D loss: 0.675204, acc: 55.47%] [G loss: 1.895975]\n",
      "epoch:20 step:19548 [D loss: 0.648703, acc: 60.94%] [G loss: 1.875215]\n",
      "epoch:20 step:19549 [D loss: 0.609747, acc: 73.44%] [G loss: 1.911264]\n",
      "epoch:20 step:19550 [D loss: 0.650098, acc: 63.28%] [G loss: 1.921435]\n",
      "epoch:20 step:19551 [D loss: 0.631453, acc: 62.50%] [G loss: 1.904400]\n",
      "epoch:20 step:19552 [D loss: 0.656055, acc: 62.50%] [G loss: 1.817743]\n",
      "epoch:20 step:19553 [D loss: 0.643612, acc: 59.38%] [G loss: 1.846761]\n",
      "epoch:20 step:19554 [D loss: 0.626957, acc: 62.50%] [G loss: 1.984594]\n",
      "epoch:20 step:19555 [D loss: 0.659010, acc: 55.47%] [G loss: 2.137598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19556 [D loss: 0.607553, acc: 67.97%] [G loss: 2.239217]\n",
      "epoch:20 step:19557 [D loss: 0.721977, acc: 54.69%] [G loss: 1.837308]\n",
      "epoch:20 step:19558 [D loss: 0.681118, acc: 63.28%] [G loss: 1.816623]\n",
      "epoch:20 step:19559 [D loss: 0.628170, acc: 64.06%] [G loss: 1.845959]\n",
      "epoch:20 step:19560 [D loss: 0.689118, acc: 57.03%] [G loss: 1.705422]\n",
      "epoch:20 step:19561 [D loss: 0.714809, acc: 55.47%] [G loss: 1.727583]\n",
      "epoch:20 step:19562 [D loss: 0.638523, acc: 67.97%] [G loss: 1.745958]\n",
      "epoch:20 step:19563 [D loss: 0.658520, acc: 62.50%] [G loss: 1.914908]\n",
      "epoch:20 step:19564 [D loss: 0.638428, acc: 62.50%] [G loss: 1.728199]\n",
      "epoch:20 step:19565 [D loss: 0.598238, acc: 71.88%] [G loss: 2.086869]\n",
      "epoch:20 step:19566 [D loss: 0.645348, acc: 65.62%] [G loss: 1.953371]\n",
      "epoch:20 step:19567 [D loss: 0.675502, acc: 54.69%] [G loss: 1.845369]\n",
      "epoch:20 step:19568 [D loss: 0.711256, acc: 56.25%] [G loss: 1.808373]\n",
      "epoch:20 step:19569 [D loss: 0.660620, acc: 60.16%] [G loss: 1.691531]\n",
      "epoch:20 step:19570 [D loss: 0.707472, acc: 58.59%] [G loss: 1.812144]\n",
      "epoch:20 step:19571 [D loss: 0.616356, acc: 60.94%] [G loss: 1.951938]\n",
      "epoch:20 step:19572 [D loss: 0.609669, acc: 65.62%] [G loss: 1.884834]\n",
      "epoch:20 step:19573 [D loss: 0.625357, acc: 59.38%] [G loss: 1.967395]\n",
      "epoch:20 step:19574 [D loss: 0.661904, acc: 60.94%] [G loss: 1.888826]\n",
      "epoch:20 step:19575 [D loss: 0.661566, acc: 59.38%] [G loss: 1.814523]\n",
      "epoch:20 step:19576 [D loss: 0.645153, acc: 68.75%] [G loss: 1.969761]\n",
      "epoch:20 step:19577 [D loss: 0.584896, acc: 70.31%] [G loss: 1.958662]\n",
      "epoch:20 step:19578 [D loss: 0.623042, acc: 66.41%] [G loss: 2.083394]\n",
      "epoch:20 step:19579 [D loss: 0.652917, acc: 57.81%] [G loss: 1.889260]\n",
      "epoch:20 step:19580 [D loss: 0.692445, acc: 61.72%] [G loss: 1.917175]\n",
      "epoch:20 step:19581 [D loss: 0.652822, acc: 63.28%] [G loss: 1.826382]\n",
      "epoch:20 step:19582 [D loss: 0.633441, acc: 66.41%] [G loss: 1.955431]\n",
      "epoch:20 step:19583 [D loss: 0.614255, acc: 65.62%] [G loss: 1.970938]\n",
      "epoch:20 step:19584 [D loss: 0.693235, acc: 55.47%] [G loss: 1.849743]\n",
      "epoch:20 step:19585 [D loss: 0.611196, acc: 68.75%] [G loss: 2.164802]\n",
      "epoch:20 step:19586 [D loss: 0.642629, acc: 60.16%] [G loss: 1.991685]\n",
      "epoch:20 step:19587 [D loss: 0.638687, acc: 67.97%] [G loss: 2.009617]\n",
      "epoch:20 step:19588 [D loss: 0.655344, acc: 65.62%] [G loss: 2.015462]\n",
      "epoch:20 step:19589 [D loss: 0.568360, acc: 72.66%] [G loss: 2.062236]\n",
      "epoch:20 step:19590 [D loss: 0.673113, acc: 60.94%] [G loss: 1.929656]\n",
      "epoch:20 step:19591 [D loss: 0.652443, acc: 58.59%] [G loss: 1.911377]\n",
      "epoch:20 step:19592 [D loss: 0.644878, acc: 59.38%] [G loss: 2.019962]\n",
      "epoch:20 step:19593 [D loss: 0.682665, acc: 58.59%] [G loss: 1.872790]\n",
      "epoch:20 step:19594 [D loss: 0.659985, acc: 61.72%] [G loss: 1.896679]\n",
      "epoch:20 step:19595 [D loss: 0.676593, acc: 55.47%] [G loss: 1.775698]\n",
      "epoch:20 step:19596 [D loss: 0.631277, acc: 62.50%] [G loss: 1.751322]\n",
      "epoch:20 step:19597 [D loss: 0.593531, acc: 62.50%] [G loss: 1.992427]\n",
      "epoch:20 step:19598 [D loss: 0.775452, acc: 52.34%] [G loss: 1.809152]\n",
      "epoch:20 step:19599 [D loss: 0.655827, acc: 56.25%] [G loss: 1.939286]\n",
      "epoch:20 step:19600 [D loss: 0.626454, acc: 66.41%] [G loss: 2.099190]\n",
      "epoch:20 step:19601 [D loss: 0.668760, acc: 60.16%] [G loss: 1.730232]\n",
      "epoch:20 step:19602 [D loss: 0.614588, acc: 64.84%] [G loss: 1.849733]\n",
      "epoch:20 step:19603 [D loss: 0.655810, acc: 61.72%] [G loss: 1.963470]\n",
      "epoch:20 step:19604 [D loss: 0.606581, acc: 64.06%] [G loss: 1.970673]\n",
      "epoch:20 step:19605 [D loss: 0.666539, acc: 55.47%] [G loss: 1.859547]\n",
      "epoch:20 step:19606 [D loss: 0.648502, acc: 62.50%] [G loss: 1.931249]\n",
      "epoch:20 step:19607 [D loss: 0.704637, acc: 55.47%] [G loss: 1.774319]\n",
      "epoch:20 step:19608 [D loss: 0.660909, acc: 59.38%] [G loss: 1.778526]\n",
      "epoch:20 step:19609 [D loss: 0.642935, acc: 64.84%] [G loss: 1.842845]\n",
      "epoch:20 step:19610 [D loss: 0.632232, acc: 63.28%] [G loss: 1.815710]\n",
      "epoch:20 step:19611 [D loss: 0.642605, acc: 63.28%] [G loss: 1.883328]\n",
      "epoch:20 step:19612 [D loss: 0.641983, acc: 61.72%] [G loss: 1.772912]\n",
      "epoch:20 step:19613 [D loss: 0.654027, acc: 60.94%] [G loss: 1.797909]\n",
      "epoch:20 step:19614 [D loss: 0.637240, acc: 61.72%] [G loss: 1.801280]\n",
      "epoch:20 step:19615 [D loss: 0.675049, acc: 57.81%] [G loss: 1.909818]\n",
      "epoch:20 step:19616 [D loss: 0.639271, acc: 63.28%] [G loss: 1.856229]\n",
      "epoch:20 step:19617 [D loss: 0.664497, acc: 59.38%] [G loss: 1.940627]\n",
      "epoch:20 step:19618 [D loss: 0.673015, acc: 57.81%] [G loss: 1.983065]\n",
      "epoch:20 step:19619 [D loss: 0.625377, acc: 69.53%] [G loss: 1.880920]\n",
      "epoch:20 step:19620 [D loss: 0.662097, acc: 59.38%] [G loss: 1.703015]\n",
      "epoch:20 step:19621 [D loss: 0.650208, acc: 59.38%] [G loss: 1.905104]\n",
      "epoch:20 step:19622 [D loss: 0.598061, acc: 69.53%] [G loss: 1.853169]\n",
      "epoch:20 step:19623 [D loss: 0.622367, acc: 67.97%] [G loss: 1.873827]\n",
      "epoch:20 step:19624 [D loss: 0.638521, acc: 67.97%] [G loss: 2.057523]\n",
      "epoch:20 step:19625 [D loss: 0.637545, acc: 67.97%] [G loss: 1.892763]\n",
      "epoch:20 step:19626 [D loss: 0.629828, acc: 60.16%] [G loss: 2.040070]\n",
      "epoch:20 step:19627 [D loss: 0.671699, acc: 56.25%] [G loss: 1.710217]\n",
      "epoch:20 step:19628 [D loss: 0.625324, acc: 64.84%] [G loss: 1.877596]\n",
      "epoch:20 step:19629 [D loss: 0.622586, acc: 68.75%] [G loss: 1.967859]\n",
      "epoch:20 step:19630 [D loss: 0.641119, acc: 62.50%] [G loss: 1.843723]\n",
      "epoch:20 step:19631 [D loss: 0.700170, acc: 61.72%] [G loss: 1.740609]\n",
      "epoch:20 step:19632 [D loss: 0.672461, acc: 56.25%] [G loss: 1.845733]\n",
      "epoch:20 step:19633 [D loss: 0.633357, acc: 67.19%] [G loss: 1.894686]\n",
      "epoch:20 step:19634 [D loss: 0.641082, acc: 66.41%] [G loss: 1.950278]\n",
      "epoch:20 step:19635 [D loss: 0.668811, acc: 61.72%] [G loss: 1.868610]\n",
      "epoch:20 step:19636 [D loss: 0.674258, acc: 53.91%] [G loss: 1.749584]\n",
      "epoch:20 step:19637 [D loss: 0.623305, acc: 63.28%] [G loss: 1.914930]\n",
      "epoch:20 step:19638 [D loss: 0.631854, acc: 65.62%] [G loss: 1.907410]\n",
      "epoch:20 step:19639 [D loss: 0.603824, acc: 60.94%] [G loss: 2.057240]\n",
      "epoch:20 step:19640 [D loss: 0.622589, acc: 64.06%] [G loss: 2.025182]\n",
      "epoch:20 step:19641 [D loss: 0.627835, acc: 66.41%] [G loss: 2.002991]\n",
      "epoch:20 step:19642 [D loss: 0.692511, acc: 57.03%] [G loss: 1.878355]\n",
      "epoch:20 step:19643 [D loss: 0.640116, acc: 64.06%] [G loss: 1.819014]\n",
      "epoch:20 step:19644 [D loss: 0.670499, acc: 57.03%] [G loss: 1.987832]\n",
      "epoch:20 step:19645 [D loss: 0.615931, acc: 64.84%] [G loss: 1.965406]\n",
      "epoch:20 step:19646 [D loss: 0.629231, acc: 62.50%] [G loss: 1.925056]\n",
      "epoch:20 step:19647 [D loss: 0.588124, acc: 71.09%] [G loss: 2.028151]\n",
      "epoch:20 step:19648 [D loss: 0.693178, acc: 59.38%] [G loss: 2.025033]\n",
      "epoch:20 step:19649 [D loss: 0.619768, acc: 65.62%] [G loss: 2.000998]\n",
      "epoch:20 step:19650 [D loss: 0.628207, acc: 65.62%] [G loss: 1.923528]\n",
      "epoch:20 step:19651 [D loss: 0.690130, acc: 56.25%] [G loss: 1.889418]\n",
      "epoch:20 step:19652 [D loss: 0.629151, acc: 65.62%] [G loss: 2.175443]\n",
      "epoch:20 step:19653 [D loss: 0.733791, acc: 50.00%] [G loss: 1.830594]\n",
      "epoch:20 step:19654 [D loss: 0.618754, acc: 63.28%] [G loss: 1.955759]\n",
      "epoch:20 step:19655 [D loss: 0.639304, acc: 61.72%] [G loss: 1.814672]\n",
      "epoch:20 step:19656 [D loss: 0.631782, acc: 64.06%] [G loss: 1.979002]\n",
      "epoch:20 step:19657 [D loss: 0.645224, acc: 63.28%] [G loss: 1.972545]\n",
      "epoch:20 step:19658 [D loss: 0.582355, acc: 64.84%] [G loss: 2.093222]\n",
      "epoch:20 step:19659 [D loss: 0.637041, acc: 66.41%] [G loss: 2.128626]\n",
      "epoch:20 step:19660 [D loss: 0.631459, acc: 64.06%] [G loss: 1.726973]\n",
      "epoch:20 step:19661 [D loss: 0.612754, acc: 65.62%] [G loss: 2.003832]\n",
      "epoch:20 step:19662 [D loss: 0.648358, acc: 70.31%] [G loss: 2.005218]\n",
      "epoch:20 step:19663 [D loss: 0.622149, acc: 67.19%] [G loss: 2.076736]\n",
      "epoch:20 step:19664 [D loss: 0.560537, acc: 74.22%] [G loss: 2.246951]\n",
      "epoch:20 step:19665 [D loss: 0.637322, acc: 67.97%] [G loss: 2.046421]\n",
      "epoch:20 step:19666 [D loss: 0.626767, acc: 68.75%] [G loss: 2.147705]\n",
      "epoch:20 step:19667 [D loss: 0.633830, acc: 60.94%] [G loss: 1.988598]\n",
      "epoch:20 step:19668 [D loss: 0.763600, acc: 49.22%] [G loss: 1.685858]\n",
      "epoch:20 step:19669 [D loss: 0.769617, acc: 48.44%] [G loss: 1.692904]\n",
      "epoch:20 step:19670 [D loss: 0.654358, acc: 62.50%] [G loss: 1.877861]\n",
      "epoch:20 step:19671 [D loss: 0.619218, acc: 67.97%] [G loss: 1.952010]\n",
      "epoch:20 step:19672 [D loss: 0.635700, acc: 67.97%] [G loss: 1.940050]\n",
      "epoch:20 step:19673 [D loss: 0.580446, acc: 70.31%] [G loss: 2.045856]\n",
      "epoch:20 step:19674 [D loss: 0.642738, acc: 69.53%] [G loss: 1.951434]\n",
      "epoch:20 step:19675 [D loss: 0.602731, acc: 67.97%] [G loss: 2.058385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:19676 [D loss: 0.633671, acc: 63.28%] [G loss: 2.232008]\n",
      "epoch:20 step:19677 [D loss: 0.631729, acc: 65.62%] [G loss: 2.295346]\n",
      "epoch:21 step:19678 [D loss: 0.692782, acc: 60.16%] [G loss: 1.904570]\n",
      "epoch:21 step:19679 [D loss: 0.648858, acc: 63.28%] [G loss: 1.844043]\n",
      "epoch:21 step:19680 [D loss: 0.645785, acc: 63.28%] [G loss: 1.910759]\n",
      "epoch:21 step:19681 [D loss: 0.644260, acc: 60.16%] [G loss: 1.852554]\n",
      "epoch:21 step:19682 [D loss: 0.661780, acc: 60.94%] [G loss: 1.909977]\n",
      "epoch:21 step:19683 [D loss: 0.684092, acc: 60.16%] [G loss: 2.009316]\n",
      "epoch:21 step:19684 [D loss: 0.622713, acc: 61.72%] [G loss: 1.945535]\n",
      "epoch:21 step:19685 [D loss: 0.648622, acc: 60.94%] [G loss: 1.856895]\n",
      "epoch:21 step:19686 [D loss: 0.618390, acc: 63.28%] [G loss: 2.012442]\n",
      "epoch:21 step:19687 [D loss: 0.551321, acc: 74.22%] [G loss: 1.965677]\n",
      "epoch:21 step:19688 [D loss: 0.645540, acc: 64.06%] [G loss: 1.876890]\n",
      "epoch:21 step:19689 [D loss: 0.679204, acc: 57.81%] [G loss: 1.899987]\n",
      "epoch:21 step:19690 [D loss: 0.682952, acc: 57.81%] [G loss: 1.864740]\n",
      "epoch:21 step:19691 [D loss: 0.641603, acc: 60.94%] [G loss: 1.915350]\n",
      "epoch:21 step:19692 [D loss: 0.636504, acc: 66.41%] [G loss: 1.949337]\n",
      "epoch:21 step:19693 [D loss: 0.648820, acc: 64.06%] [G loss: 1.999788]\n",
      "epoch:21 step:19694 [D loss: 0.599821, acc: 71.09%] [G loss: 2.049280]\n",
      "epoch:21 step:19695 [D loss: 0.619733, acc: 64.06%] [G loss: 1.826180]\n",
      "epoch:21 step:19696 [D loss: 0.681290, acc: 54.69%] [G loss: 1.840176]\n",
      "epoch:21 step:19697 [D loss: 0.703849, acc: 56.25%] [G loss: 1.702360]\n",
      "epoch:21 step:19698 [D loss: 0.690266, acc: 48.44%] [G loss: 1.793292]\n",
      "epoch:21 step:19699 [D loss: 0.656124, acc: 59.38%] [G loss: 1.809036]\n",
      "epoch:21 step:19700 [D loss: 0.615926, acc: 65.62%] [G loss: 1.907460]\n",
      "epoch:21 step:19701 [D loss: 0.618798, acc: 65.62%] [G loss: 1.880976]\n",
      "epoch:21 step:19702 [D loss: 0.621182, acc: 67.19%] [G loss: 2.017985]\n",
      "epoch:21 step:19703 [D loss: 0.609783, acc: 67.19%] [G loss: 1.871115]\n",
      "epoch:21 step:19704 [D loss: 0.685320, acc: 57.81%] [G loss: 1.873495]\n",
      "epoch:21 step:19705 [D loss: 0.702365, acc: 53.91%] [G loss: 1.864959]\n",
      "epoch:21 step:19706 [D loss: 0.660009, acc: 59.38%] [G loss: 1.780747]\n",
      "epoch:21 step:19707 [D loss: 0.661700, acc: 61.72%] [G loss: 1.749341]\n",
      "epoch:21 step:19708 [D loss: 0.706405, acc: 54.69%] [G loss: 1.663199]\n",
      "epoch:21 step:19709 [D loss: 0.684364, acc: 53.12%] [G loss: 1.762691]\n",
      "epoch:21 step:19710 [D loss: 0.640756, acc: 63.28%] [G loss: 1.707625]\n",
      "epoch:21 step:19711 [D loss: 0.698104, acc: 61.72%] [G loss: 1.779082]\n",
      "epoch:21 step:19712 [D loss: 0.644478, acc: 62.50%] [G loss: 1.921196]\n",
      "epoch:21 step:19713 [D loss: 0.594431, acc: 67.19%] [G loss: 1.875019]\n",
      "epoch:21 step:19714 [D loss: 0.649168, acc: 60.16%] [G loss: 1.864910]\n",
      "epoch:21 step:19715 [D loss: 0.647863, acc: 60.16%] [G loss: 1.891372]\n",
      "epoch:21 step:19716 [D loss: 0.668844, acc: 61.72%] [G loss: 1.861942]\n",
      "epoch:21 step:19717 [D loss: 0.584053, acc: 71.09%] [G loss: 2.023483]\n",
      "epoch:21 step:19718 [D loss: 0.651571, acc: 67.97%] [G loss: 1.871203]\n",
      "epoch:21 step:19719 [D loss: 0.629405, acc: 69.53%] [G loss: 2.083762]\n",
      "epoch:21 step:19720 [D loss: 0.678404, acc: 58.59%] [G loss: 1.924452]\n",
      "epoch:21 step:19721 [D loss: 0.652414, acc: 66.41%] [G loss: 1.890643]\n",
      "epoch:21 step:19722 [D loss: 0.700657, acc: 55.47%] [G loss: 1.888098]\n",
      "epoch:21 step:19723 [D loss: 0.669487, acc: 65.62%] [G loss: 1.755067]\n",
      "epoch:21 step:19724 [D loss: 0.665142, acc: 63.28%] [G loss: 1.871442]\n",
      "epoch:21 step:19725 [D loss: 0.663602, acc: 60.94%] [G loss: 1.958263]\n",
      "epoch:21 step:19726 [D loss: 0.600165, acc: 65.62%] [G loss: 2.069161]\n",
      "epoch:21 step:19727 [D loss: 0.634154, acc: 65.62%] [G loss: 1.883617]\n",
      "epoch:21 step:19728 [D loss: 0.632876, acc: 67.97%] [G loss: 1.761727]\n",
      "epoch:21 step:19729 [D loss: 0.619696, acc: 68.75%] [G loss: 1.761751]\n",
      "epoch:21 step:19730 [D loss: 0.612141, acc: 70.31%] [G loss: 1.906605]\n",
      "epoch:21 step:19731 [D loss: 0.643420, acc: 63.28%] [G loss: 1.892444]\n",
      "epoch:21 step:19732 [D loss: 0.663527, acc: 67.97%] [G loss: 2.051972]\n",
      "epoch:21 step:19733 [D loss: 0.620546, acc: 65.62%] [G loss: 1.914936]\n",
      "epoch:21 step:19734 [D loss: 0.651344, acc: 61.72%] [G loss: 2.009128]\n",
      "epoch:21 step:19735 [D loss: 0.664749, acc: 58.59%] [G loss: 1.901612]\n",
      "epoch:21 step:19736 [D loss: 0.612239, acc: 64.84%] [G loss: 1.893243]\n",
      "epoch:21 step:19737 [D loss: 0.657421, acc: 62.50%] [G loss: 1.864591]\n",
      "epoch:21 step:19738 [D loss: 0.627789, acc: 64.84%] [G loss: 1.873510]\n",
      "epoch:21 step:19739 [D loss: 0.674075, acc: 59.38%] [G loss: 1.971348]\n",
      "epoch:21 step:19740 [D loss: 0.648141, acc: 56.25%] [G loss: 1.860804]\n",
      "epoch:21 step:19741 [D loss: 0.653725, acc: 63.28%] [G loss: 1.810175]\n",
      "epoch:21 step:19742 [D loss: 0.648711, acc: 60.16%] [G loss: 1.872014]\n",
      "epoch:21 step:19743 [D loss: 0.646063, acc: 60.94%] [G loss: 1.892273]\n",
      "epoch:21 step:19744 [D loss: 0.595210, acc: 64.06%] [G loss: 1.829271]\n",
      "epoch:21 step:19745 [D loss: 0.612657, acc: 70.31%] [G loss: 1.898379]\n",
      "epoch:21 step:19746 [D loss: 0.611848, acc: 67.97%] [G loss: 2.026650]\n",
      "epoch:21 step:19747 [D loss: 0.619775, acc: 64.06%] [G loss: 1.895772]\n",
      "epoch:21 step:19748 [D loss: 0.663932, acc: 60.94%] [G loss: 1.731957]\n",
      "epoch:21 step:19749 [D loss: 0.629870, acc: 65.62%] [G loss: 1.852840]\n",
      "epoch:21 step:19750 [D loss: 0.659970, acc: 62.50%] [G loss: 1.758379]\n",
      "epoch:21 step:19751 [D loss: 0.655252, acc: 64.84%] [G loss: 1.926626]\n",
      "epoch:21 step:19752 [D loss: 0.651239, acc: 60.94%] [G loss: 2.067941]\n",
      "epoch:21 step:19753 [D loss: 0.594227, acc: 69.53%] [G loss: 1.912271]\n",
      "epoch:21 step:19754 [D loss: 0.611964, acc: 70.31%] [G loss: 2.013669]\n",
      "epoch:21 step:19755 [D loss: 0.752421, acc: 54.69%] [G loss: 1.837331]\n",
      "epoch:21 step:19756 [D loss: 0.643531, acc: 60.94%] [G loss: 1.864419]\n",
      "epoch:21 step:19757 [D loss: 0.687307, acc: 58.59%] [G loss: 1.688336]\n",
      "epoch:21 step:19758 [D loss: 0.654197, acc: 56.25%] [G loss: 1.649578]\n",
      "epoch:21 step:19759 [D loss: 0.684129, acc: 60.16%] [G loss: 1.808084]\n",
      "epoch:21 step:19760 [D loss: 0.645003, acc: 63.28%] [G loss: 1.765497]\n",
      "epoch:21 step:19761 [D loss: 0.637361, acc: 64.06%] [G loss: 1.953724]\n",
      "epoch:21 step:19762 [D loss: 0.619503, acc: 64.84%] [G loss: 1.785173]\n",
      "epoch:21 step:19763 [D loss: 0.661708, acc: 61.72%] [G loss: 1.839749]\n",
      "epoch:21 step:19764 [D loss: 0.627526, acc: 67.97%] [G loss: 1.812376]\n",
      "epoch:21 step:19765 [D loss: 0.696268, acc: 58.59%] [G loss: 2.030874]\n",
      "epoch:21 step:19766 [D loss: 0.583070, acc: 74.22%] [G loss: 1.845340]\n",
      "epoch:21 step:19767 [D loss: 0.676989, acc: 58.59%] [G loss: 1.804132]\n",
      "epoch:21 step:19768 [D loss: 0.658938, acc: 63.28%] [G loss: 1.866893]\n",
      "epoch:21 step:19769 [D loss: 0.690145, acc: 61.72%] [G loss: 1.790907]\n",
      "epoch:21 step:19770 [D loss: 0.588686, acc: 68.75%] [G loss: 2.010820]\n",
      "epoch:21 step:19771 [D loss: 0.609218, acc: 63.28%] [G loss: 1.995529]\n",
      "epoch:21 step:19772 [D loss: 0.637352, acc: 60.94%] [G loss: 1.877977]\n",
      "epoch:21 step:19773 [D loss: 0.645619, acc: 65.62%] [G loss: 1.906262]\n",
      "epoch:21 step:19774 [D loss: 0.652590, acc: 57.03%] [G loss: 1.877775]\n",
      "epoch:21 step:19775 [D loss: 0.651247, acc: 62.50%] [G loss: 1.769798]\n",
      "epoch:21 step:19776 [D loss: 0.659289, acc: 59.38%] [G loss: 1.889930]\n",
      "epoch:21 step:19777 [D loss: 0.624919, acc: 69.53%] [G loss: 1.945483]\n",
      "epoch:21 step:19778 [D loss: 0.592348, acc: 67.97%] [G loss: 1.953335]\n",
      "epoch:21 step:19779 [D loss: 0.671873, acc: 52.34%] [G loss: 1.870270]\n",
      "epoch:21 step:19780 [D loss: 0.689720, acc: 55.47%] [G loss: 1.895518]\n",
      "epoch:21 step:19781 [D loss: 0.650216, acc: 64.84%] [G loss: 1.851759]\n",
      "epoch:21 step:19782 [D loss: 0.661180, acc: 59.38%] [G loss: 1.843212]\n",
      "epoch:21 step:19783 [D loss: 0.598740, acc: 70.31%] [G loss: 2.095415]\n",
      "epoch:21 step:19784 [D loss: 0.606839, acc: 68.75%] [G loss: 1.971532]\n",
      "epoch:21 step:19785 [D loss: 0.669105, acc: 59.38%] [G loss: 1.787731]\n",
      "epoch:21 step:19786 [D loss: 0.620421, acc: 71.88%] [G loss: 1.940974]\n",
      "epoch:21 step:19787 [D loss: 0.646681, acc: 62.50%] [G loss: 1.813823]\n",
      "epoch:21 step:19788 [D loss: 0.583357, acc: 71.88%] [G loss: 2.109528]\n",
      "epoch:21 step:19789 [D loss: 0.575552, acc: 73.44%] [G loss: 2.202532]\n",
      "epoch:21 step:19790 [D loss: 0.642056, acc: 62.50%] [G loss: 2.035637]\n",
      "epoch:21 step:19791 [D loss: 0.654650, acc: 60.94%] [G loss: 1.996762]\n",
      "epoch:21 step:19792 [D loss: 0.633095, acc: 65.62%] [G loss: 2.051434]\n",
      "epoch:21 step:19793 [D loss: 0.597618, acc: 67.19%] [G loss: 2.094060]\n",
      "epoch:21 step:19794 [D loss: 0.611768, acc: 64.84%] [G loss: 2.083206]\n",
      "epoch:21 step:19795 [D loss: 0.595020, acc: 65.62%] [G loss: 2.245401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19796 [D loss: 0.656119, acc: 57.81%] [G loss: 2.235963]\n",
      "epoch:21 step:19797 [D loss: 0.658255, acc: 65.62%] [G loss: 1.990301]\n",
      "epoch:21 step:19798 [D loss: 0.692902, acc: 59.38%] [G loss: 1.953238]\n",
      "epoch:21 step:19799 [D loss: 0.595412, acc: 65.62%] [G loss: 2.112612]\n",
      "epoch:21 step:19800 [D loss: 0.616214, acc: 66.41%] [G loss: 1.966789]\n",
      "epoch:21 step:19801 [D loss: 0.693318, acc: 51.56%] [G loss: 1.867809]\n",
      "epoch:21 step:19802 [D loss: 0.733190, acc: 50.00%] [G loss: 1.751823]\n",
      "epoch:21 step:19803 [D loss: 0.622222, acc: 64.06%] [G loss: 1.891697]\n",
      "epoch:21 step:19804 [D loss: 0.651697, acc: 55.47%] [G loss: 1.831205]\n",
      "epoch:21 step:19805 [D loss: 0.606531, acc: 66.41%] [G loss: 1.967890]\n",
      "epoch:21 step:19806 [D loss: 0.654237, acc: 64.06%] [G loss: 1.862246]\n",
      "epoch:21 step:19807 [D loss: 0.642723, acc: 58.59%] [G loss: 2.022117]\n",
      "epoch:21 step:19808 [D loss: 0.599016, acc: 69.53%] [G loss: 2.104599]\n",
      "epoch:21 step:19809 [D loss: 0.654241, acc: 62.50%] [G loss: 1.941670]\n",
      "epoch:21 step:19810 [D loss: 0.770142, acc: 47.66%] [G loss: 1.853931]\n",
      "epoch:21 step:19811 [D loss: 0.631306, acc: 62.50%] [G loss: 1.723781]\n",
      "epoch:21 step:19812 [D loss: 0.623197, acc: 64.06%] [G loss: 1.947295]\n",
      "epoch:21 step:19813 [D loss: 0.661223, acc: 59.38%] [G loss: 1.836969]\n",
      "epoch:21 step:19814 [D loss: 0.681799, acc: 58.59%] [G loss: 1.860655]\n",
      "epoch:21 step:19815 [D loss: 0.676774, acc: 56.25%] [G loss: 1.991333]\n",
      "epoch:21 step:19816 [D loss: 0.646971, acc: 63.28%] [G loss: 1.917682]\n",
      "epoch:21 step:19817 [D loss: 0.693451, acc: 55.47%] [G loss: 1.756881]\n",
      "epoch:21 step:19818 [D loss: 0.645778, acc: 59.38%] [G loss: 1.792053]\n",
      "epoch:21 step:19819 [D loss: 0.652533, acc: 58.59%] [G loss: 1.822817]\n",
      "epoch:21 step:19820 [D loss: 0.631977, acc: 64.06%] [G loss: 1.823912]\n",
      "epoch:21 step:19821 [D loss: 0.672285, acc: 57.03%] [G loss: 1.984289]\n",
      "epoch:21 step:19822 [D loss: 0.639158, acc: 64.06%] [G loss: 1.884888]\n",
      "epoch:21 step:19823 [D loss: 0.637450, acc: 68.75%] [G loss: 2.011844]\n",
      "epoch:21 step:19824 [D loss: 0.719823, acc: 54.69%] [G loss: 1.916107]\n",
      "epoch:21 step:19825 [D loss: 0.644697, acc: 62.50%] [G loss: 1.826782]\n",
      "epoch:21 step:19826 [D loss: 0.671609, acc: 60.16%] [G loss: 1.915057]\n",
      "epoch:21 step:19827 [D loss: 0.615695, acc: 70.31%] [G loss: 2.004887]\n",
      "epoch:21 step:19828 [D loss: 0.620057, acc: 67.97%] [G loss: 1.834136]\n",
      "epoch:21 step:19829 [D loss: 0.629522, acc: 64.06%] [G loss: 2.000021]\n",
      "epoch:21 step:19830 [D loss: 0.758899, acc: 53.12%] [G loss: 1.818236]\n",
      "epoch:21 step:19831 [D loss: 0.659244, acc: 57.03%] [G loss: 1.900719]\n",
      "epoch:21 step:19832 [D loss: 0.660679, acc: 63.28%] [G loss: 1.871706]\n",
      "epoch:21 step:19833 [D loss: 0.634015, acc: 65.62%] [G loss: 1.891534]\n",
      "epoch:21 step:19834 [D loss: 0.631829, acc: 65.62%] [G loss: 1.804731]\n",
      "epoch:21 step:19835 [D loss: 0.677789, acc: 60.16%] [G loss: 1.768417]\n",
      "epoch:21 step:19836 [D loss: 0.619147, acc: 67.19%] [G loss: 1.972438]\n",
      "epoch:21 step:19837 [D loss: 0.665599, acc: 60.94%] [G loss: 1.796320]\n",
      "epoch:21 step:19838 [D loss: 0.650302, acc: 61.72%] [G loss: 1.755741]\n",
      "epoch:21 step:19839 [D loss: 0.659692, acc: 60.94%] [G loss: 1.931738]\n",
      "epoch:21 step:19840 [D loss: 0.668658, acc: 57.81%] [G loss: 1.866384]\n",
      "epoch:21 step:19841 [D loss: 0.614970, acc: 64.06%] [G loss: 1.800657]\n",
      "epoch:21 step:19842 [D loss: 0.642936, acc: 62.50%] [G loss: 1.897462]\n",
      "epoch:21 step:19843 [D loss: 0.642100, acc: 63.28%] [G loss: 1.827524]\n",
      "epoch:21 step:19844 [D loss: 0.671439, acc: 57.81%] [G loss: 1.866507]\n",
      "epoch:21 step:19845 [D loss: 0.608770, acc: 64.84%] [G loss: 1.890131]\n",
      "epoch:21 step:19846 [D loss: 0.703665, acc: 53.91%] [G loss: 1.870415]\n",
      "epoch:21 step:19847 [D loss: 0.643479, acc: 64.06%] [G loss: 1.806806]\n",
      "epoch:21 step:19848 [D loss: 0.626315, acc: 60.94%] [G loss: 1.811260]\n",
      "epoch:21 step:19849 [D loss: 0.629697, acc: 60.94%] [G loss: 1.907535]\n",
      "epoch:21 step:19850 [D loss: 0.651822, acc: 64.06%] [G loss: 1.751433]\n",
      "epoch:21 step:19851 [D loss: 0.637266, acc: 64.84%] [G loss: 1.745840]\n",
      "epoch:21 step:19852 [D loss: 0.629376, acc: 63.28%] [G loss: 1.940007]\n",
      "epoch:21 step:19853 [D loss: 0.658326, acc: 63.28%] [G loss: 1.845434]\n",
      "epoch:21 step:19854 [D loss: 0.674068, acc: 57.03%] [G loss: 1.728614]\n",
      "epoch:21 step:19855 [D loss: 0.702488, acc: 55.47%] [G loss: 1.985969]\n",
      "epoch:21 step:19856 [D loss: 0.662861, acc: 60.16%] [G loss: 1.838437]\n",
      "epoch:21 step:19857 [D loss: 0.615368, acc: 69.53%] [G loss: 1.754648]\n",
      "epoch:21 step:19858 [D loss: 0.653814, acc: 57.81%] [G loss: 1.799222]\n",
      "epoch:21 step:19859 [D loss: 0.717674, acc: 54.69%] [G loss: 1.862838]\n",
      "epoch:21 step:19860 [D loss: 0.649166, acc: 60.16%] [G loss: 1.821877]\n",
      "epoch:21 step:19861 [D loss: 0.612360, acc: 65.62%] [G loss: 1.915082]\n",
      "epoch:21 step:19862 [D loss: 0.659469, acc: 60.94%] [G loss: 1.775189]\n",
      "epoch:21 step:19863 [D loss: 0.638852, acc: 65.62%] [G loss: 1.903273]\n",
      "epoch:21 step:19864 [D loss: 0.653442, acc: 60.94%] [G loss: 1.839007]\n",
      "epoch:21 step:19865 [D loss: 0.677734, acc: 56.25%] [G loss: 1.743314]\n",
      "epoch:21 step:19866 [D loss: 0.663798, acc: 63.28%] [G loss: 1.753285]\n",
      "epoch:21 step:19867 [D loss: 0.636685, acc: 59.38%] [G loss: 1.841639]\n",
      "epoch:21 step:19868 [D loss: 0.634648, acc: 64.06%] [G loss: 1.950927]\n",
      "epoch:21 step:19869 [D loss: 0.603385, acc: 64.84%] [G loss: 1.975263]\n",
      "epoch:21 step:19870 [D loss: 0.598695, acc: 70.31%] [G loss: 1.895263]\n",
      "epoch:21 step:19871 [D loss: 0.611511, acc: 66.41%] [G loss: 2.109186]\n",
      "epoch:21 step:19872 [D loss: 0.706213, acc: 59.38%] [G loss: 1.894216]\n",
      "epoch:21 step:19873 [D loss: 0.673476, acc: 51.56%] [G loss: 1.942290]\n",
      "epoch:21 step:19874 [D loss: 0.630745, acc: 65.62%] [G loss: 1.913760]\n",
      "epoch:21 step:19875 [D loss: 0.591308, acc: 69.53%] [G loss: 1.933810]\n",
      "epoch:21 step:19876 [D loss: 0.687030, acc: 61.72%] [G loss: 1.940237]\n",
      "epoch:21 step:19877 [D loss: 0.657400, acc: 62.50%] [G loss: 1.886261]\n",
      "epoch:21 step:19878 [D loss: 0.706155, acc: 55.47%] [G loss: 1.919658]\n",
      "epoch:21 step:19879 [D loss: 0.664827, acc: 60.16%] [G loss: 1.930940]\n",
      "epoch:21 step:19880 [D loss: 0.645933, acc: 60.16%] [G loss: 1.922772]\n",
      "epoch:21 step:19881 [D loss: 0.659241, acc: 60.94%] [G loss: 1.843424]\n",
      "epoch:21 step:19882 [D loss: 0.659482, acc: 60.94%] [G loss: 1.851030]\n",
      "epoch:21 step:19883 [D loss: 0.625630, acc: 61.72%] [G loss: 2.017084]\n",
      "epoch:21 step:19884 [D loss: 0.644751, acc: 67.19%] [G loss: 2.091649]\n",
      "epoch:21 step:19885 [D loss: 0.610963, acc: 65.62%] [G loss: 2.067214]\n",
      "epoch:21 step:19886 [D loss: 0.602016, acc: 64.06%] [G loss: 2.139947]\n",
      "epoch:21 step:19887 [D loss: 0.683338, acc: 59.38%] [G loss: 1.953057]\n",
      "epoch:21 step:19888 [D loss: 0.703211, acc: 51.56%] [G loss: 1.731268]\n",
      "epoch:21 step:19889 [D loss: 0.666770, acc: 58.59%] [G loss: 1.761419]\n",
      "epoch:21 step:19890 [D loss: 0.713175, acc: 52.34%] [G loss: 1.841811]\n",
      "epoch:21 step:19891 [D loss: 0.693465, acc: 57.81%] [G loss: 1.778961]\n",
      "epoch:21 step:19892 [D loss: 0.642052, acc: 64.06%] [G loss: 1.836981]\n",
      "epoch:21 step:19893 [D loss: 0.616136, acc: 66.41%] [G loss: 1.882196]\n",
      "epoch:21 step:19894 [D loss: 0.651847, acc: 64.84%] [G loss: 1.970127]\n",
      "epoch:21 step:19895 [D loss: 0.638460, acc: 59.38%] [G loss: 2.079239]\n",
      "epoch:21 step:19896 [D loss: 0.568257, acc: 71.88%] [G loss: 2.129270]\n",
      "epoch:21 step:19897 [D loss: 0.724370, acc: 52.34%] [G loss: 1.811957]\n",
      "epoch:21 step:19898 [D loss: 0.670027, acc: 57.03%] [G loss: 1.807978]\n",
      "epoch:21 step:19899 [D loss: 0.626732, acc: 66.41%] [G loss: 1.913169]\n",
      "epoch:21 step:19900 [D loss: 0.696168, acc: 54.69%] [G loss: 1.894128]\n",
      "epoch:21 step:19901 [D loss: 0.632768, acc: 63.28%] [G loss: 1.892675]\n",
      "epoch:21 step:19902 [D loss: 0.640763, acc: 60.94%] [G loss: 1.830962]\n",
      "epoch:21 step:19903 [D loss: 0.662942, acc: 59.38%] [G loss: 1.778774]\n",
      "epoch:21 step:19904 [D loss: 0.683572, acc: 56.25%] [G loss: 1.786881]\n",
      "epoch:21 step:19905 [D loss: 0.642395, acc: 63.28%] [G loss: 1.796552]\n",
      "epoch:21 step:19906 [D loss: 0.597916, acc: 70.31%] [G loss: 2.177540]\n",
      "epoch:21 step:19907 [D loss: 0.639171, acc: 65.62%] [G loss: 1.973944]\n",
      "epoch:21 step:19908 [D loss: 0.588729, acc: 69.53%] [G loss: 2.167157]\n",
      "epoch:21 step:19909 [D loss: 0.587669, acc: 67.19%] [G loss: 2.168155]\n",
      "epoch:21 step:19910 [D loss: 0.711328, acc: 57.81%] [G loss: 1.918741]\n",
      "epoch:21 step:19911 [D loss: 0.667211, acc: 65.62%] [G loss: 1.820876]\n",
      "epoch:21 step:19912 [D loss: 0.667606, acc: 58.59%] [G loss: 1.800565]\n",
      "epoch:21 step:19913 [D loss: 0.629872, acc: 64.06%] [G loss: 1.915661]\n",
      "epoch:21 step:19914 [D loss: 0.651895, acc: 57.81%] [G loss: 1.803889]\n",
      "epoch:21 step:19915 [D loss: 0.633002, acc: 64.84%] [G loss: 1.848048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:19916 [D loss: 0.653445, acc: 63.28%] [G loss: 1.962463]\n",
      "epoch:21 step:19917 [D loss: 0.683831, acc: 54.69%] [G loss: 1.844191]\n",
      "epoch:21 step:19918 [D loss: 0.640834, acc: 62.50%] [G loss: 2.050401]\n",
      "epoch:21 step:19919 [D loss: 0.633317, acc: 62.50%] [G loss: 1.864109]\n",
      "epoch:21 step:19920 [D loss: 0.639706, acc: 63.28%] [G loss: 1.971428]\n",
      "epoch:21 step:19921 [D loss: 0.657375, acc: 60.94%] [G loss: 1.788493]\n",
      "epoch:21 step:19922 [D loss: 0.619491, acc: 66.41%] [G loss: 1.932709]\n",
      "epoch:21 step:19923 [D loss: 0.677107, acc: 54.69%] [G loss: 1.792781]\n",
      "epoch:21 step:19924 [D loss: 0.643082, acc: 62.50%] [G loss: 1.829751]\n",
      "epoch:21 step:19925 [D loss: 0.608717, acc: 67.97%] [G loss: 1.922103]\n",
      "epoch:21 step:19926 [D loss: 0.649267, acc: 56.25%] [G loss: 1.798021]\n",
      "epoch:21 step:19927 [D loss: 0.759665, acc: 42.97%] [G loss: 1.714962]\n",
      "epoch:21 step:19928 [D loss: 0.709054, acc: 51.56%] [G loss: 1.773334]\n",
      "epoch:21 step:19929 [D loss: 0.666642, acc: 57.81%] [G loss: 1.790756]\n",
      "epoch:21 step:19930 [D loss: 0.648062, acc: 61.72%] [G loss: 1.929459]\n",
      "epoch:21 step:19931 [D loss: 0.685619, acc: 57.03%] [G loss: 1.746802]\n",
      "epoch:21 step:19932 [D loss: 0.660578, acc: 60.94%] [G loss: 1.680829]\n",
      "epoch:21 step:19933 [D loss: 0.678301, acc: 58.59%] [G loss: 1.861625]\n",
      "epoch:21 step:19934 [D loss: 0.669211, acc: 64.84%] [G loss: 1.862756]\n",
      "epoch:21 step:19935 [D loss: 0.632226, acc: 62.50%] [G loss: 1.749759]\n",
      "epoch:21 step:19936 [D loss: 0.662566, acc: 58.59%] [G loss: 1.798814]\n",
      "epoch:21 step:19937 [D loss: 0.657811, acc: 60.94%] [G loss: 1.731991]\n",
      "epoch:21 step:19938 [D loss: 0.667106, acc: 59.38%] [G loss: 1.914506]\n",
      "epoch:21 step:19939 [D loss: 0.660262, acc: 64.06%] [G loss: 1.999087]\n",
      "epoch:21 step:19940 [D loss: 0.680947, acc: 58.59%] [G loss: 1.927646]\n",
      "epoch:21 step:19941 [D loss: 0.651768, acc: 64.06%] [G loss: 2.162524]\n",
      "epoch:21 step:19942 [D loss: 0.673264, acc: 57.81%] [G loss: 1.668307]\n",
      "epoch:21 step:19943 [D loss: 0.674003, acc: 60.94%] [G loss: 1.846260]\n",
      "epoch:21 step:19944 [D loss: 0.610660, acc: 67.97%] [G loss: 1.825040]\n",
      "epoch:21 step:19945 [D loss: 0.657475, acc: 61.72%] [G loss: 1.832501]\n",
      "epoch:21 step:19946 [D loss: 0.644816, acc: 60.16%] [G loss: 1.780379]\n",
      "epoch:21 step:19947 [D loss: 0.645224, acc: 57.81%] [G loss: 2.030069]\n",
      "epoch:21 step:19948 [D loss: 0.656171, acc: 62.50%] [G loss: 1.819551]\n",
      "epoch:21 step:19949 [D loss: 0.629065, acc: 63.28%] [G loss: 1.908228]\n",
      "epoch:21 step:19950 [D loss: 0.624466, acc: 62.50%] [G loss: 1.906108]\n",
      "epoch:21 step:19951 [D loss: 0.603547, acc: 69.53%] [G loss: 1.886795]\n",
      "epoch:21 step:19952 [D loss: 0.629312, acc: 64.84%] [G loss: 2.003548]\n",
      "epoch:21 step:19953 [D loss: 0.617047, acc: 65.62%] [G loss: 2.127591]\n",
      "epoch:21 step:19954 [D loss: 0.654678, acc: 64.84%] [G loss: 1.820352]\n",
      "epoch:21 step:19955 [D loss: 0.604576, acc: 66.41%] [G loss: 1.870567]\n",
      "epoch:21 step:19956 [D loss: 0.716089, acc: 56.25%] [G loss: 1.852013]\n",
      "epoch:21 step:19957 [D loss: 0.692802, acc: 57.03%] [G loss: 1.934999]\n",
      "epoch:21 step:19958 [D loss: 0.654436, acc: 57.81%] [G loss: 1.688555]\n",
      "epoch:21 step:19959 [D loss: 0.632485, acc: 64.84%] [G loss: 1.782523]\n",
      "epoch:21 step:19960 [D loss: 0.638868, acc: 64.06%] [G loss: 1.831846]\n",
      "epoch:21 step:19961 [D loss: 0.650278, acc: 63.28%] [G loss: 1.687338]\n",
      "epoch:21 step:19962 [D loss: 0.631899, acc: 62.50%] [G loss: 1.719284]\n",
      "epoch:21 step:19963 [D loss: 0.603140, acc: 64.06%] [G loss: 1.828969]\n",
      "epoch:21 step:19964 [D loss: 0.679610, acc: 59.38%] [G loss: 1.789563]\n",
      "epoch:21 step:19965 [D loss: 0.627594, acc: 64.06%] [G loss: 1.811068]\n",
      "epoch:21 step:19966 [D loss: 0.685244, acc: 56.25%] [G loss: 1.935604]\n",
      "epoch:21 step:19967 [D loss: 0.614273, acc: 66.41%] [G loss: 1.918698]\n",
      "epoch:21 step:19968 [D loss: 0.664634, acc: 62.50%] [G loss: 1.908725]\n",
      "epoch:21 step:19969 [D loss: 0.661363, acc: 63.28%] [G loss: 1.765722]\n",
      "epoch:21 step:19970 [D loss: 0.624486, acc: 65.62%] [G loss: 1.839139]\n",
      "epoch:21 step:19971 [D loss: 0.667139, acc: 60.16%] [G loss: 1.837454]\n",
      "epoch:21 step:19972 [D loss: 0.660215, acc: 62.50%] [G loss: 1.822146]\n",
      "epoch:21 step:19973 [D loss: 0.627347, acc: 64.84%] [G loss: 1.881932]\n",
      "epoch:21 step:19974 [D loss: 0.635081, acc: 61.72%] [G loss: 1.785802]\n",
      "epoch:21 step:19975 [D loss: 0.661265, acc: 61.72%] [G loss: 1.998720]\n",
      "epoch:21 step:19976 [D loss: 0.642061, acc: 63.28%] [G loss: 1.889271]\n",
      "epoch:21 step:19977 [D loss: 0.635385, acc: 67.19%] [G loss: 1.958851]\n",
      "epoch:21 step:19978 [D loss: 0.659314, acc: 53.91%] [G loss: 1.662067]\n",
      "epoch:21 step:19979 [D loss: 0.691514, acc: 57.03%] [G loss: 1.814258]\n",
      "epoch:21 step:19980 [D loss: 0.610678, acc: 64.06%] [G loss: 1.895120]\n",
      "epoch:21 step:19981 [D loss: 0.658917, acc: 61.72%] [G loss: 1.819672]\n",
      "epoch:21 step:19982 [D loss: 0.652858, acc: 57.03%] [G loss: 1.926244]\n",
      "epoch:21 step:19983 [D loss: 0.648099, acc: 63.28%] [G loss: 1.805437]\n",
      "epoch:21 step:19984 [D loss: 0.624160, acc: 67.97%] [G loss: 1.897028]\n",
      "epoch:21 step:19985 [D loss: 0.676783, acc: 61.72%] [G loss: 1.924247]\n",
      "epoch:21 step:19986 [D loss: 0.661213, acc: 61.72%] [G loss: 1.831661]\n",
      "epoch:21 step:19987 [D loss: 0.680689, acc: 57.03%] [G loss: 1.855694]\n",
      "epoch:21 step:19988 [D loss: 0.641010, acc: 63.28%] [G loss: 1.805738]\n",
      "epoch:21 step:19989 [D loss: 0.600281, acc: 70.31%] [G loss: 2.052741]\n",
      "epoch:21 step:19990 [D loss: 0.620067, acc: 64.84%] [G loss: 2.094036]\n",
      "epoch:21 step:19991 [D loss: 0.584816, acc: 61.72%] [G loss: 2.178928]\n",
      "epoch:21 step:19992 [D loss: 0.554376, acc: 75.78%] [G loss: 2.235712]\n",
      "epoch:21 step:19993 [D loss: 0.714906, acc: 54.69%] [G loss: 1.697515]\n",
      "epoch:21 step:19994 [D loss: 0.668264, acc: 58.59%] [G loss: 1.920450]\n",
      "epoch:21 step:19995 [D loss: 0.632344, acc: 59.38%] [G loss: 1.929778]\n",
      "epoch:21 step:19996 [D loss: 0.747364, acc: 52.34%] [G loss: 1.695209]\n",
      "epoch:21 step:19997 [D loss: 0.670060, acc: 59.38%] [G loss: 1.752340]\n",
      "epoch:21 step:19998 [D loss: 0.628751, acc: 66.41%] [G loss: 1.920441]\n",
      "epoch:21 step:19999 [D loss: 0.649207, acc: 64.06%] [G loss: 1.690545]\n",
      "epoch:21 step:20000 [D loss: 0.628589, acc: 62.50%] [G loss: 1.720464]\n",
      "epoch:21 step:20001 [D loss: 0.659161, acc: 62.50%] [G loss: 1.793078]\n",
      "epoch:21 step:20002 [D loss: 0.630596, acc: 64.06%] [G loss: 1.796011]\n",
      "epoch:21 step:20003 [D loss: 0.629698, acc: 64.84%] [G loss: 1.757996]\n",
      "epoch:21 step:20004 [D loss: 0.633320, acc: 66.41%] [G loss: 1.803141]\n",
      "epoch:21 step:20005 [D loss: 0.624293, acc: 64.06%] [G loss: 1.764639]\n",
      "epoch:21 step:20006 [D loss: 0.618157, acc: 61.72%] [G loss: 1.917097]\n",
      "epoch:21 step:20007 [D loss: 0.614069, acc: 67.97%] [G loss: 1.861222]\n",
      "epoch:21 step:20008 [D loss: 0.592987, acc: 69.53%] [G loss: 1.942722]\n",
      "epoch:21 step:20009 [D loss: 0.658145, acc: 61.72%] [G loss: 1.910853]\n",
      "epoch:21 step:20010 [D loss: 0.628993, acc: 60.94%] [G loss: 1.849925]\n",
      "epoch:21 step:20011 [D loss: 0.641791, acc: 59.38%] [G loss: 1.914294]\n",
      "epoch:21 step:20012 [D loss: 0.655225, acc: 59.38%] [G loss: 1.947341]\n",
      "epoch:21 step:20013 [D loss: 0.627185, acc: 62.50%] [G loss: 1.950380]\n",
      "epoch:21 step:20014 [D loss: 0.724100, acc: 52.34%] [G loss: 1.897832]\n",
      "epoch:21 step:20015 [D loss: 0.650598, acc: 65.62%] [G loss: 1.905991]\n",
      "epoch:21 step:20016 [D loss: 0.649152, acc: 62.50%] [G loss: 1.884830]\n",
      "epoch:21 step:20017 [D loss: 0.646226, acc: 64.06%] [G loss: 1.900092]\n",
      "epoch:21 step:20018 [D loss: 0.638743, acc: 64.84%] [G loss: 1.775531]\n",
      "epoch:21 step:20019 [D loss: 0.652770, acc: 60.16%] [G loss: 1.754578]\n",
      "epoch:21 step:20020 [D loss: 0.666287, acc: 57.03%] [G loss: 1.816825]\n",
      "epoch:21 step:20021 [D loss: 0.672722, acc: 61.72%] [G loss: 1.766443]\n",
      "epoch:21 step:20022 [D loss: 0.628701, acc: 57.81%] [G loss: 2.014581]\n",
      "epoch:21 step:20023 [D loss: 0.597638, acc: 71.09%] [G loss: 2.121702]\n",
      "epoch:21 step:20024 [D loss: 0.545295, acc: 74.22%] [G loss: 2.268430]\n",
      "epoch:21 step:20025 [D loss: 0.671361, acc: 60.16%] [G loss: 1.901404]\n",
      "epoch:21 step:20026 [D loss: 0.693829, acc: 53.12%] [G loss: 1.679037]\n",
      "epoch:21 step:20027 [D loss: 0.654015, acc: 60.16%] [G loss: 1.847350]\n",
      "epoch:21 step:20028 [D loss: 0.662884, acc: 61.72%] [G loss: 1.702090]\n",
      "epoch:21 step:20029 [D loss: 0.639059, acc: 62.50%] [G loss: 1.831538]\n",
      "epoch:21 step:20030 [D loss: 0.691953, acc: 57.03%] [G loss: 1.773611]\n",
      "epoch:21 step:20031 [D loss: 0.592248, acc: 71.88%] [G loss: 1.903189]\n",
      "epoch:21 step:20032 [D loss: 0.707003, acc: 54.69%] [G loss: 1.781370]\n",
      "epoch:21 step:20033 [D loss: 0.651756, acc: 60.94%] [G loss: 1.805185]\n",
      "epoch:21 step:20034 [D loss: 0.636715, acc: 62.50%] [G loss: 1.788930]\n",
      "epoch:21 step:20035 [D loss: 0.644829, acc: 61.72%] [G loss: 1.836063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20036 [D loss: 0.615871, acc: 65.62%] [G loss: 1.992408]\n",
      "epoch:21 step:20037 [D loss: 0.588955, acc: 68.75%] [G loss: 1.932573]\n",
      "epoch:21 step:20038 [D loss: 0.662070, acc: 62.50%] [G loss: 1.795475]\n",
      "epoch:21 step:20039 [D loss: 0.624537, acc: 71.09%] [G loss: 1.834811]\n",
      "epoch:21 step:20040 [D loss: 0.661534, acc: 58.59%] [G loss: 1.824566]\n",
      "epoch:21 step:20041 [D loss: 0.627873, acc: 60.94%] [G loss: 2.052655]\n",
      "epoch:21 step:20042 [D loss: 0.628121, acc: 63.28%] [G loss: 1.847651]\n",
      "epoch:21 step:20043 [D loss: 0.636757, acc: 64.06%] [G loss: 1.951427]\n",
      "epoch:21 step:20044 [D loss: 0.621176, acc: 60.94%] [G loss: 1.890133]\n",
      "epoch:21 step:20045 [D loss: 0.686814, acc: 58.59%] [G loss: 1.960304]\n",
      "epoch:21 step:20046 [D loss: 0.613980, acc: 67.97%] [G loss: 2.067005]\n",
      "epoch:21 step:20047 [D loss: 0.685580, acc: 57.03%] [G loss: 1.944871]\n",
      "epoch:21 step:20048 [D loss: 0.604746, acc: 67.19%] [G loss: 2.066281]\n",
      "epoch:21 step:20049 [D loss: 0.636890, acc: 64.84%] [G loss: 1.976831]\n",
      "epoch:21 step:20050 [D loss: 0.705357, acc: 52.34%] [G loss: 1.832933]\n",
      "epoch:21 step:20051 [D loss: 0.624490, acc: 65.62%] [G loss: 2.017708]\n",
      "epoch:21 step:20052 [D loss: 0.669764, acc: 63.28%] [G loss: 1.782146]\n",
      "epoch:21 step:20053 [D loss: 0.658196, acc: 62.50%] [G loss: 1.798946]\n",
      "epoch:21 step:20054 [D loss: 0.688089, acc: 54.69%] [G loss: 1.665852]\n",
      "epoch:21 step:20055 [D loss: 0.635781, acc: 65.62%] [G loss: 1.829001]\n",
      "epoch:21 step:20056 [D loss: 0.601957, acc: 64.06%] [G loss: 1.927343]\n",
      "epoch:21 step:20057 [D loss: 0.607436, acc: 65.62%] [G loss: 2.008304]\n",
      "epoch:21 step:20058 [D loss: 0.675016, acc: 57.81%] [G loss: 2.016726]\n",
      "epoch:21 step:20059 [D loss: 0.689052, acc: 56.25%] [G loss: 1.797472]\n",
      "epoch:21 step:20060 [D loss: 0.655368, acc: 64.06%] [G loss: 1.869301]\n",
      "epoch:21 step:20061 [D loss: 0.662074, acc: 60.16%] [G loss: 1.988141]\n",
      "epoch:21 step:20062 [D loss: 0.630719, acc: 64.84%] [G loss: 2.018924]\n",
      "epoch:21 step:20063 [D loss: 0.726076, acc: 50.78%] [G loss: 1.784330]\n",
      "epoch:21 step:20064 [D loss: 0.642839, acc: 63.28%] [G loss: 1.811571]\n",
      "epoch:21 step:20065 [D loss: 0.620422, acc: 63.28%] [G loss: 1.789178]\n",
      "epoch:21 step:20066 [D loss: 0.688656, acc: 50.78%] [G loss: 1.672194]\n",
      "epoch:21 step:20067 [D loss: 0.652452, acc: 62.50%] [G loss: 1.780348]\n",
      "epoch:21 step:20068 [D loss: 0.669324, acc: 59.38%] [G loss: 1.801852]\n",
      "epoch:21 step:20069 [D loss: 0.614630, acc: 62.50%] [G loss: 1.828764]\n",
      "epoch:21 step:20070 [D loss: 0.685022, acc: 61.72%] [G loss: 1.804979]\n",
      "epoch:21 step:20071 [D loss: 0.656846, acc: 61.72%] [G loss: 1.852897]\n",
      "epoch:21 step:20072 [D loss: 0.579727, acc: 71.09%] [G loss: 2.025440]\n",
      "epoch:21 step:20073 [D loss: 0.687237, acc: 57.03%] [G loss: 1.772362]\n",
      "epoch:21 step:20074 [D loss: 0.690945, acc: 56.25%] [G loss: 1.754989]\n",
      "epoch:21 step:20075 [D loss: 0.600675, acc: 70.31%] [G loss: 1.983767]\n",
      "epoch:21 step:20076 [D loss: 0.634904, acc: 64.84%] [G loss: 1.852531]\n",
      "epoch:21 step:20077 [D loss: 0.700830, acc: 54.69%] [G loss: 1.752404]\n",
      "epoch:21 step:20078 [D loss: 0.658416, acc: 59.38%] [G loss: 1.939477]\n",
      "epoch:21 step:20079 [D loss: 0.658378, acc: 63.28%] [G loss: 1.911721]\n",
      "epoch:21 step:20080 [D loss: 0.638495, acc: 62.50%] [G loss: 1.866159]\n",
      "epoch:21 step:20081 [D loss: 0.649064, acc: 62.50%] [G loss: 1.978341]\n",
      "epoch:21 step:20082 [D loss: 0.603705, acc: 68.75%] [G loss: 1.919375]\n",
      "epoch:21 step:20083 [D loss: 0.621535, acc: 65.62%] [G loss: 1.811716]\n",
      "epoch:21 step:20084 [D loss: 0.635430, acc: 58.59%] [G loss: 2.004768]\n",
      "epoch:21 step:20085 [D loss: 0.659858, acc: 54.69%] [G loss: 1.800412]\n",
      "epoch:21 step:20086 [D loss: 0.620893, acc: 65.62%] [G loss: 1.762850]\n",
      "epoch:21 step:20087 [D loss: 0.630218, acc: 66.41%] [G loss: 1.966364]\n",
      "epoch:21 step:20088 [D loss: 0.655742, acc: 63.28%] [G loss: 1.810209]\n",
      "epoch:21 step:20089 [D loss: 0.639924, acc: 60.16%] [G loss: 1.941504]\n",
      "epoch:21 step:20090 [D loss: 0.640478, acc: 61.72%] [G loss: 1.939216]\n",
      "epoch:21 step:20091 [D loss: 0.624102, acc: 67.97%] [G loss: 2.053595]\n",
      "epoch:21 step:20092 [D loss: 0.668499, acc: 63.28%] [G loss: 2.109685]\n",
      "epoch:21 step:20093 [D loss: 0.609868, acc: 67.19%] [G loss: 1.991757]\n",
      "epoch:21 step:20094 [D loss: 0.662817, acc: 60.16%] [G loss: 1.949848]\n",
      "epoch:21 step:20095 [D loss: 0.669363, acc: 55.47%] [G loss: 1.922575]\n",
      "epoch:21 step:20096 [D loss: 0.683973, acc: 54.69%] [G loss: 1.830063]\n",
      "epoch:21 step:20097 [D loss: 0.674324, acc: 60.16%] [G loss: 1.891613]\n",
      "epoch:21 step:20098 [D loss: 0.728041, acc: 53.12%] [G loss: 1.894450]\n",
      "epoch:21 step:20099 [D loss: 0.628332, acc: 62.50%] [G loss: 1.929809]\n",
      "epoch:21 step:20100 [D loss: 0.670860, acc: 60.94%] [G loss: 1.873589]\n",
      "epoch:21 step:20101 [D loss: 0.702081, acc: 56.25%] [G loss: 1.739015]\n",
      "epoch:21 step:20102 [D loss: 0.658578, acc: 60.94%] [G loss: 1.735972]\n",
      "epoch:21 step:20103 [D loss: 0.647915, acc: 60.94%] [G loss: 1.803439]\n",
      "epoch:21 step:20104 [D loss: 0.633004, acc: 70.31%] [G loss: 1.891436]\n",
      "epoch:21 step:20105 [D loss: 0.626197, acc: 67.97%] [G loss: 2.059331]\n",
      "epoch:21 step:20106 [D loss: 0.620564, acc: 71.09%] [G loss: 2.092919]\n",
      "epoch:21 step:20107 [D loss: 0.549468, acc: 78.91%] [G loss: 2.176396]\n",
      "epoch:21 step:20108 [D loss: 0.628891, acc: 64.84%] [G loss: 1.922011]\n",
      "epoch:21 step:20109 [D loss: 0.699592, acc: 57.03%] [G loss: 1.891278]\n",
      "epoch:21 step:20110 [D loss: 0.640546, acc: 64.84%] [G loss: 1.795369]\n",
      "epoch:21 step:20111 [D loss: 0.636214, acc: 60.94%] [G loss: 1.941123]\n",
      "epoch:21 step:20112 [D loss: 0.620505, acc: 64.06%] [G loss: 1.915567]\n",
      "epoch:21 step:20113 [D loss: 0.580020, acc: 69.53%] [G loss: 1.953287]\n",
      "epoch:21 step:20114 [D loss: 0.735955, acc: 52.34%] [G loss: 1.641732]\n",
      "epoch:21 step:20115 [D loss: 0.667629, acc: 53.91%] [G loss: 1.872887]\n",
      "epoch:21 step:20116 [D loss: 0.708255, acc: 55.47%] [G loss: 1.800966]\n",
      "epoch:21 step:20117 [D loss: 0.677846, acc: 64.06%] [G loss: 1.798361]\n",
      "epoch:21 step:20118 [D loss: 0.642605, acc: 62.50%] [G loss: 1.745935]\n",
      "epoch:21 step:20119 [D loss: 0.690670, acc: 51.56%] [G loss: 1.747375]\n",
      "epoch:21 step:20120 [D loss: 0.641821, acc: 64.06%] [G loss: 1.906770]\n",
      "epoch:21 step:20121 [D loss: 0.672448, acc: 57.81%] [G loss: 1.740009]\n",
      "epoch:21 step:20122 [D loss: 0.632813, acc: 71.88%] [G loss: 1.880699]\n",
      "epoch:21 step:20123 [D loss: 0.615257, acc: 65.62%] [G loss: 1.740331]\n",
      "epoch:21 step:20124 [D loss: 0.642815, acc: 60.94%] [G loss: 1.866084]\n",
      "epoch:21 step:20125 [D loss: 0.662611, acc: 60.16%] [G loss: 1.720309]\n",
      "epoch:21 step:20126 [D loss: 0.625333, acc: 64.84%] [G loss: 1.858380]\n",
      "epoch:21 step:20127 [D loss: 0.640375, acc: 65.62%] [G loss: 1.957498]\n",
      "epoch:21 step:20128 [D loss: 0.604605, acc: 68.75%] [G loss: 1.970605]\n",
      "epoch:21 step:20129 [D loss: 0.648016, acc: 57.81%] [G loss: 1.972058]\n",
      "epoch:21 step:20130 [D loss: 0.626610, acc: 60.16%] [G loss: 2.055666]\n",
      "epoch:21 step:20131 [D loss: 0.603299, acc: 67.97%] [G loss: 1.886805]\n",
      "epoch:21 step:20132 [D loss: 0.633686, acc: 62.50%] [G loss: 1.882825]\n",
      "epoch:21 step:20133 [D loss: 0.604811, acc: 65.62%] [G loss: 2.021951]\n",
      "epoch:21 step:20134 [D loss: 0.666552, acc: 60.94%] [G loss: 2.070057]\n",
      "epoch:21 step:20135 [D loss: 0.704634, acc: 52.34%] [G loss: 1.824004]\n",
      "epoch:21 step:20136 [D loss: 0.681804, acc: 60.16%] [G loss: 1.793229]\n",
      "epoch:21 step:20137 [D loss: 0.659466, acc: 59.38%] [G loss: 1.640917]\n",
      "epoch:21 step:20138 [D loss: 0.628979, acc: 66.41%] [G loss: 1.804010]\n",
      "epoch:21 step:20139 [D loss: 0.647645, acc: 63.28%] [G loss: 1.720585]\n",
      "epoch:21 step:20140 [D loss: 0.667460, acc: 60.94%] [G loss: 1.853349]\n",
      "epoch:21 step:20141 [D loss: 0.635324, acc: 68.75%] [G loss: 1.801358]\n",
      "epoch:21 step:20142 [D loss: 0.616934, acc: 66.41%] [G loss: 2.029902]\n",
      "epoch:21 step:20143 [D loss: 0.660465, acc: 67.19%] [G loss: 1.994646]\n",
      "epoch:21 step:20144 [D loss: 0.657819, acc: 61.72%] [G loss: 1.906764]\n",
      "epoch:21 step:20145 [D loss: 0.585698, acc: 65.62%] [G loss: 2.115236]\n",
      "epoch:21 step:20146 [D loss: 0.636468, acc: 58.59%] [G loss: 2.225855]\n",
      "epoch:21 step:20147 [D loss: 0.552953, acc: 74.22%] [G loss: 2.140382]\n",
      "epoch:21 step:20148 [D loss: 0.588406, acc: 69.53%] [G loss: 2.354940]\n",
      "epoch:21 step:20149 [D loss: 0.657131, acc: 60.94%] [G loss: 2.052041]\n",
      "epoch:21 step:20150 [D loss: 0.738106, acc: 55.47%] [G loss: 1.740004]\n",
      "epoch:21 step:20151 [D loss: 0.730172, acc: 59.38%] [G loss: 1.836732]\n",
      "epoch:21 step:20152 [D loss: 0.702518, acc: 57.03%] [G loss: 1.841058]\n",
      "epoch:21 step:20153 [D loss: 0.688734, acc: 54.69%] [G loss: 1.909147]\n",
      "epoch:21 step:20154 [D loss: 0.723220, acc: 53.91%] [G loss: 1.708596]\n",
      "epoch:21 step:20155 [D loss: 0.712364, acc: 54.69%] [G loss: 1.814533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20156 [D loss: 0.600191, acc: 70.31%] [G loss: 1.997273]\n",
      "epoch:21 step:20157 [D loss: 0.605536, acc: 65.62%] [G loss: 1.944497]\n",
      "epoch:21 step:20158 [D loss: 0.624762, acc: 64.84%] [G loss: 2.009130]\n",
      "epoch:21 step:20159 [D loss: 0.698442, acc: 54.69%] [G loss: 1.756033]\n",
      "epoch:21 step:20160 [D loss: 0.716910, acc: 56.25%] [G loss: 1.769249]\n",
      "epoch:21 step:20161 [D loss: 0.635425, acc: 61.72%] [G loss: 1.859985]\n",
      "epoch:21 step:20162 [D loss: 0.654886, acc: 62.50%] [G loss: 1.829574]\n",
      "epoch:21 step:20163 [D loss: 0.628719, acc: 66.41%] [G loss: 1.885819]\n",
      "epoch:21 step:20164 [D loss: 0.624162, acc: 67.19%] [G loss: 1.863446]\n",
      "epoch:21 step:20165 [D loss: 0.642902, acc: 63.28%] [G loss: 1.981740]\n",
      "epoch:21 step:20166 [D loss: 0.649457, acc: 63.28%] [G loss: 1.831523]\n",
      "epoch:21 step:20167 [D loss: 0.661156, acc: 60.94%] [G loss: 1.936054]\n",
      "epoch:21 step:20168 [D loss: 0.641689, acc: 64.06%] [G loss: 2.013646]\n",
      "epoch:21 step:20169 [D loss: 0.659784, acc: 63.28%] [G loss: 1.743838]\n",
      "epoch:21 step:20170 [D loss: 0.602448, acc: 67.97%] [G loss: 1.852230]\n",
      "epoch:21 step:20171 [D loss: 0.669723, acc: 59.38%] [G loss: 1.852857]\n",
      "epoch:21 step:20172 [D loss: 0.576796, acc: 73.44%] [G loss: 2.131804]\n",
      "epoch:21 step:20173 [D loss: 0.614372, acc: 66.41%] [G loss: 1.970882]\n",
      "epoch:21 step:20174 [D loss: 0.650882, acc: 59.38%] [G loss: 2.033761]\n",
      "epoch:21 step:20175 [D loss: 0.641361, acc: 57.81%] [G loss: 2.093009]\n",
      "epoch:21 step:20176 [D loss: 0.581707, acc: 76.56%] [G loss: 2.209874]\n",
      "epoch:21 step:20177 [D loss: 0.693029, acc: 58.59%] [G loss: 1.832323]\n",
      "epoch:21 step:20178 [D loss: 0.697425, acc: 55.47%] [G loss: 1.692856]\n",
      "epoch:21 step:20179 [D loss: 0.710207, acc: 53.91%] [G loss: 1.666762]\n",
      "epoch:21 step:20180 [D loss: 0.674716, acc: 59.38%] [G loss: 1.781851]\n",
      "epoch:21 step:20181 [D loss: 0.624773, acc: 65.62%] [G loss: 1.930698]\n",
      "epoch:21 step:20182 [D loss: 0.665367, acc: 61.72%] [G loss: 1.824772]\n",
      "epoch:21 step:20183 [D loss: 0.633932, acc: 65.62%] [G loss: 1.800654]\n",
      "epoch:21 step:20184 [D loss: 0.685503, acc: 52.34%] [G loss: 1.830067]\n",
      "epoch:21 step:20185 [D loss: 0.638481, acc: 60.16%] [G loss: 1.961233]\n",
      "epoch:21 step:20186 [D loss: 0.671752, acc: 62.50%] [G loss: 1.917116]\n",
      "epoch:21 step:20187 [D loss: 0.656226, acc: 65.62%] [G loss: 1.878943]\n",
      "epoch:21 step:20188 [D loss: 0.748781, acc: 48.44%] [G loss: 1.731840]\n",
      "epoch:21 step:20189 [D loss: 0.620126, acc: 65.62%] [G loss: 1.831786]\n",
      "epoch:21 step:20190 [D loss: 0.628120, acc: 65.62%] [G loss: 1.775994]\n",
      "epoch:21 step:20191 [D loss: 0.649472, acc: 65.62%] [G loss: 1.806356]\n",
      "epoch:21 step:20192 [D loss: 0.636879, acc: 58.59%] [G loss: 1.894295]\n",
      "epoch:21 step:20193 [D loss: 0.621894, acc: 59.38%] [G loss: 1.848347]\n",
      "epoch:21 step:20194 [D loss: 0.665501, acc: 64.84%] [G loss: 1.861469]\n",
      "epoch:21 step:20195 [D loss: 0.684333, acc: 56.25%] [G loss: 1.777867]\n",
      "epoch:21 step:20196 [D loss: 0.688065, acc: 50.00%] [G loss: 1.705797]\n",
      "epoch:21 step:20197 [D loss: 0.643627, acc: 64.06%] [G loss: 1.854207]\n",
      "epoch:21 step:20198 [D loss: 0.639848, acc: 61.72%] [G loss: 1.933218]\n",
      "epoch:21 step:20199 [D loss: 0.624623, acc: 70.31%] [G loss: 1.985425]\n",
      "epoch:21 step:20200 [D loss: 0.594318, acc: 65.62%] [G loss: 2.074730]\n",
      "epoch:21 step:20201 [D loss: 0.664481, acc: 56.25%] [G loss: 1.945287]\n",
      "epoch:21 step:20202 [D loss: 0.625666, acc: 67.19%] [G loss: 1.945568]\n",
      "epoch:21 step:20203 [D loss: 0.643929, acc: 64.84%] [G loss: 1.948186]\n",
      "epoch:21 step:20204 [D loss: 0.673646, acc: 56.25%] [G loss: 1.857281]\n",
      "epoch:21 step:20205 [D loss: 0.720927, acc: 53.12%] [G loss: 1.691615]\n",
      "epoch:21 step:20206 [D loss: 0.698257, acc: 53.12%] [G loss: 1.701185]\n",
      "epoch:21 step:20207 [D loss: 0.646082, acc: 60.16%] [G loss: 1.815593]\n",
      "epoch:21 step:20208 [D loss: 0.672666, acc: 60.94%] [G loss: 1.867065]\n",
      "epoch:21 step:20209 [D loss: 0.624414, acc: 64.84%] [G loss: 2.001417]\n",
      "epoch:21 step:20210 [D loss: 0.675379, acc: 52.34%] [G loss: 1.912508]\n",
      "epoch:21 step:20211 [D loss: 0.664222, acc: 60.16%] [G loss: 2.006378]\n",
      "epoch:21 step:20212 [D loss: 0.615967, acc: 66.41%] [G loss: 1.918593]\n",
      "epoch:21 step:20213 [D loss: 0.645355, acc: 59.38%] [G loss: 1.935216]\n",
      "epoch:21 step:20214 [D loss: 0.662161, acc: 57.81%] [G loss: 1.851290]\n",
      "epoch:21 step:20215 [D loss: 0.650931, acc: 59.38%] [G loss: 1.758374]\n",
      "epoch:21 step:20216 [D loss: 0.685218, acc: 55.47%] [G loss: 1.958343]\n",
      "epoch:21 step:20217 [D loss: 0.640010, acc: 57.81%] [G loss: 1.670787]\n",
      "epoch:21 step:20218 [D loss: 0.642658, acc: 67.19%] [G loss: 1.746911]\n",
      "epoch:21 step:20219 [D loss: 0.647643, acc: 61.72%] [G loss: 1.791630]\n",
      "epoch:21 step:20220 [D loss: 0.683165, acc: 57.03%] [G loss: 1.865738]\n",
      "epoch:21 step:20221 [D loss: 0.659585, acc: 53.91%] [G loss: 1.890639]\n",
      "epoch:21 step:20222 [D loss: 0.643116, acc: 64.84%] [G loss: 1.862152]\n",
      "epoch:21 step:20223 [D loss: 0.677021, acc: 53.91%] [G loss: 1.809970]\n",
      "epoch:21 step:20224 [D loss: 0.634672, acc: 61.72%] [G loss: 1.811466]\n",
      "epoch:21 step:20225 [D loss: 0.609584, acc: 64.84%] [G loss: 2.028415]\n",
      "epoch:21 step:20226 [D loss: 0.660710, acc: 61.72%] [G loss: 1.872599]\n",
      "epoch:21 step:20227 [D loss: 0.622058, acc: 64.84%] [G loss: 2.074884]\n",
      "epoch:21 step:20228 [D loss: 0.597998, acc: 70.31%] [G loss: 2.374329]\n",
      "epoch:21 step:20229 [D loss: 0.600810, acc: 66.41%] [G loss: 2.018491]\n",
      "epoch:21 step:20230 [D loss: 0.563985, acc: 71.88%] [G loss: 1.875268]\n",
      "epoch:21 step:20231 [D loss: 0.588408, acc: 68.75%] [G loss: 2.207416]\n",
      "epoch:21 step:20232 [D loss: 0.660866, acc: 60.16%] [G loss: 1.943299]\n",
      "epoch:21 step:20233 [D loss: 0.634697, acc: 62.50%] [G loss: 2.005574]\n",
      "epoch:21 step:20234 [D loss: 0.599247, acc: 66.41%] [G loss: 2.100263]\n",
      "epoch:21 step:20235 [D loss: 0.619910, acc: 64.06%] [G loss: 2.043377]\n",
      "epoch:21 step:20236 [D loss: 0.674336, acc: 54.69%] [G loss: 1.804837]\n",
      "epoch:21 step:20237 [D loss: 0.665051, acc: 64.06%] [G loss: 1.852165]\n",
      "epoch:21 step:20238 [D loss: 0.659738, acc: 58.59%] [G loss: 1.968713]\n",
      "epoch:21 step:20239 [D loss: 0.662936, acc: 61.72%] [G loss: 1.819096]\n",
      "epoch:21 step:20240 [D loss: 0.640967, acc: 62.50%] [G loss: 1.893598]\n",
      "epoch:21 step:20241 [D loss: 0.603681, acc: 71.88%] [G loss: 2.084194]\n",
      "epoch:21 step:20242 [D loss: 0.680344, acc: 53.91%] [G loss: 1.839721]\n",
      "epoch:21 step:20243 [D loss: 0.729482, acc: 52.34%] [G loss: 1.695734]\n",
      "epoch:21 step:20244 [D loss: 0.655000, acc: 61.72%] [G loss: 1.916190]\n",
      "epoch:21 step:20245 [D loss: 0.715025, acc: 52.34%] [G loss: 1.801175]\n",
      "epoch:21 step:20246 [D loss: 0.672588, acc: 63.28%] [G loss: 1.833268]\n",
      "epoch:21 step:20247 [D loss: 0.664209, acc: 67.19%] [G loss: 1.822085]\n",
      "epoch:21 step:20248 [D loss: 0.637677, acc: 60.94%] [G loss: 1.800458]\n",
      "epoch:21 step:20249 [D loss: 0.671318, acc: 56.25%] [G loss: 1.845664]\n",
      "epoch:21 step:20250 [D loss: 0.696780, acc: 60.16%] [G loss: 1.758648]\n",
      "epoch:21 step:20251 [D loss: 0.609041, acc: 67.19%] [G loss: 1.856449]\n",
      "epoch:21 step:20252 [D loss: 0.692188, acc: 50.78%] [G loss: 1.816652]\n",
      "epoch:21 step:20253 [D loss: 0.687698, acc: 53.12%] [G loss: 1.770124]\n",
      "epoch:21 step:20254 [D loss: 0.705875, acc: 54.69%] [G loss: 1.802324]\n",
      "epoch:21 step:20255 [D loss: 0.678725, acc: 60.16%] [G loss: 1.734226]\n",
      "epoch:21 step:20256 [D loss: 0.657194, acc: 61.72%] [G loss: 1.666159]\n",
      "epoch:21 step:20257 [D loss: 0.676715, acc: 56.25%] [G loss: 1.715462]\n",
      "epoch:21 step:20258 [D loss: 0.644659, acc: 62.50%] [G loss: 1.633355]\n",
      "epoch:21 step:20259 [D loss: 0.661996, acc: 62.50%] [G loss: 1.880312]\n",
      "epoch:21 step:20260 [D loss: 0.671834, acc: 62.50%] [G loss: 1.864017]\n",
      "epoch:21 step:20261 [D loss: 0.640601, acc: 58.59%] [G loss: 1.823932]\n",
      "epoch:21 step:20262 [D loss: 0.659376, acc: 55.47%] [G loss: 1.928539]\n",
      "epoch:21 step:20263 [D loss: 0.589251, acc: 67.19%] [G loss: 1.914631]\n",
      "epoch:21 step:20264 [D loss: 0.671134, acc: 61.72%] [G loss: 1.831879]\n",
      "epoch:21 step:20265 [D loss: 0.664198, acc: 53.91%] [G loss: 1.885355]\n",
      "epoch:21 step:20266 [D loss: 0.615771, acc: 68.75%] [G loss: 1.929970]\n",
      "epoch:21 step:20267 [D loss: 0.612346, acc: 65.62%] [G loss: 2.050354]\n",
      "epoch:21 step:20268 [D loss: 0.701156, acc: 54.69%] [G loss: 1.955927]\n",
      "epoch:21 step:20269 [D loss: 0.606128, acc: 70.31%] [G loss: 1.924604]\n",
      "epoch:21 step:20270 [D loss: 0.620230, acc: 62.50%] [G loss: 1.844738]\n",
      "epoch:21 step:20271 [D loss: 0.669881, acc: 59.38%] [G loss: 1.809706]\n",
      "epoch:21 step:20272 [D loss: 0.608433, acc: 66.41%] [G loss: 1.875399]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20273 [D loss: 0.660501, acc: 59.38%] [G loss: 1.831596]\n",
      "epoch:21 step:20274 [D loss: 0.656008, acc: 62.50%] [G loss: 1.808567]\n",
      "epoch:21 step:20275 [D loss: 0.598104, acc: 70.31%] [G loss: 1.986580]\n",
      "epoch:21 step:20276 [D loss: 0.656102, acc: 63.28%] [G loss: 1.828315]\n",
      "epoch:21 step:20277 [D loss: 0.642750, acc: 61.72%] [G loss: 1.803455]\n",
      "epoch:21 step:20278 [D loss: 0.705373, acc: 56.25%] [G loss: 1.941686]\n",
      "epoch:21 step:20279 [D loss: 0.618952, acc: 65.62%] [G loss: 1.947714]\n",
      "epoch:21 step:20280 [D loss: 0.624066, acc: 64.84%] [G loss: 1.891079]\n",
      "epoch:21 step:20281 [D loss: 0.621541, acc: 62.50%] [G loss: 1.992177]\n",
      "epoch:21 step:20282 [D loss: 0.632166, acc: 61.72%] [G loss: 2.149891]\n",
      "epoch:21 step:20283 [D loss: 0.708218, acc: 53.12%] [G loss: 1.719779]\n",
      "epoch:21 step:20284 [D loss: 0.651381, acc: 60.16%] [G loss: 1.811418]\n",
      "epoch:21 step:20285 [D loss: 0.626437, acc: 59.38%] [G loss: 1.838865]\n",
      "epoch:21 step:20286 [D loss: 0.665865, acc: 57.81%] [G loss: 1.779392]\n",
      "epoch:21 step:20287 [D loss: 0.648908, acc: 57.03%] [G loss: 1.731364]\n",
      "epoch:21 step:20288 [D loss: 0.692288, acc: 60.94%] [G loss: 1.773281]\n",
      "epoch:21 step:20289 [D loss: 0.623307, acc: 64.84%] [G loss: 1.813007]\n",
      "epoch:21 step:20290 [D loss: 0.623928, acc: 66.41%] [G loss: 1.810791]\n",
      "epoch:21 step:20291 [D loss: 0.647045, acc: 60.16%] [G loss: 1.818803]\n",
      "epoch:21 step:20292 [D loss: 0.705347, acc: 54.69%] [G loss: 1.689365]\n",
      "epoch:21 step:20293 [D loss: 0.667511, acc: 61.72%] [G loss: 1.925631]\n",
      "epoch:21 step:20294 [D loss: 0.714932, acc: 57.81%] [G loss: 1.790284]\n",
      "epoch:21 step:20295 [D loss: 0.691144, acc: 59.38%] [G loss: 1.835274]\n",
      "epoch:21 step:20296 [D loss: 0.643464, acc: 67.19%] [G loss: 1.789385]\n",
      "epoch:21 step:20297 [D loss: 0.678662, acc: 55.47%] [G loss: 1.765939]\n",
      "epoch:21 step:20298 [D loss: 0.623973, acc: 65.62%] [G loss: 1.770453]\n",
      "epoch:21 step:20299 [D loss: 0.621063, acc: 63.28%] [G loss: 1.937239]\n",
      "epoch:21 step:20300 [D loss: 0.595986, acc: 67.19%] [G loss: 1.971611]\n",
      "epoch:21 step:20301 [D loss: 0.606492, acc: 66.41%] [G loss: 2.079489]\n",
      "epoch:21 step:20302 [D loss: 0.628023, acc: 64.84%] [G loss: 1.737931]\n",
      "epoch:21 step:20303 [D loss: 0.656793, acc: 57.81%] [G loss: 1.844024]\n",
      "epoch:21 step:20304 [D loss: 0.661818, acc: 59.38%] [G loss: 1.851225]\n",
      "epoch:21 step:20305 [D loss: 0.610609, acc: 64.84%] [G loss: 1.821212]\n",
      "epoch:21 step:20306 [D loss: 0.661202, acc: 61.72%] [G loss: 1.872741]\n",
      "epoch:21 step:20307 [D loss: 0.646629, acc: 64.06%] [G loss: 1.854743]\n",
      "epoch:21 step:20308 [D loss: 0.657729, acc: 61.72%] [G loss: 1.960123]\n",
      "epoch:21 step:20309 [D loss: 0.643963, acc: 63.28%] [G loss: 2.020413]\n",
      "epoch:21 step:20310 [D loss: 0.636468, acc: 62.50%] [G loss: 2.066122]\n",
      "epoch:21 step:20311 [D loss: 0.649032, acc: 63.28%] [G loss: 1.958488]\n",
      "epoch:21 step:20312 [D loss: 0.617385, acc: 64.06%] [G loss: 1.929973]\n",
      "epoch:21 step:20313 [D loss: 0.700454, acc: 56.25%] [G loss: 1.887533]\n",
      "epoch:21 step:20314 [D loss: 0.663590, acc: 59.38%] [G loss: 2.070197]\n",
      "epoch:21 step:20315 [D loss: 0.650527, acc: 64.06%] [G loss: 1.852274]\n",
      "epoch:21 step:20316 [D loss: 0.644704, acc: 63.28%] [G loss: 1.821913]\n",
      "epoch:21 step:20317 [D loss: 0.664072, acc: 58.59%] [G loss: 1.900543]\n",
      "epoch:21 step:20318 [D loss: 0.670640, acc: 58.59%] [G loss: 1.845389]\n",
      "epoch:21 step:20319 [D loss: 0.640699, acc: 62.50%] [G loss: 1.974054]\n",
      "epoch:21 step:20320 [D loss: 0.611701, acc: 71.09%] [G loss: 1.937089]\n",
      "epoch:21 step:20321 [D loss: 0.696183, acc: 54.69%] [G loss: 2.149696]\n",
      "epoch:21 step:20322 [D loss: 0.620135, acc: 65.62%] [G loss: 1.941855]\n",
      "epoch:21 step:20323 [D loss: 0.630921, acc: 60.16%] [G loss: 2.157115]\n",
      "epoch:21 step:20324 [D loss: 0.592606, acc: 70.31%] [G loss: 2.104436]\n",
      "epoch:21 step:20325 [D loss: 0.596504, acc: 70.31%] [G loss: 2.300920]\n",
      "epoch:21 step:20326 [D loss: 0.576719, acc: 64.06%] [G loss: 2.176058]\n",
      "epoch:21 step:20327 [D loss: 0.673324, acc: 55.47%] [G loss: 2.051898]\n",
      "epoch:21 step:20328 [D loss: 0.687717, acc: 59.38%] [G loss: 2.109722]\n",
      "epoch:21 step:20329 [D loss: 0.648527, acc: 67.19%] [G loss: 1.819637]\n",
      "epoch:21 step:20330 [D loss: 0.623111, acc: 66.41%] [G loss: 1.898665]\n",
      "epoch:21 step:20331 [D loss: 0.635503, acc: 60.16%] [G loss: 2.130176]\n",
      "epoch:21 step:20332 [D loss: 0.635355, acc: 62.50%] [G loss: 1.839001]\n",
      "epoch:21 step:20333 [D loss: 0.627007, acc: 61.72%] [G loss: 1.813436]\n",
      "epoch:21 step:20334 [D loss: 0.705502, acc: 56.25%] [G loss: 1.738808]\n",
      "epoch:21 step:20335 [D loss: 0.699835, acc: 54.69%] [G loss: 1.760917]\n",
      "epoch:21 step:20336 [D loss: 0.653704, acc: 54.69%] [G loss: 1.846068]\n",
      "epoch:21 step:20337 [D loss: 0.673334, acc: 56.25%] [G loss: 1.803649]\n",
      "epoch:21 step:20338 [D loss: 0.630616, acc: 63.28%] [G loss: 1.882720]\n",
      "epoch:21 step:20339 [D loss: 0.663226, acc: 60.94%] [G loss: 1.813473]\n",
      "epoch:21 step:20340 [D loss: 0.699279, acc: 53.91%] [G loss: 1.765188]\n",
      "epoch:21 step:20341 [D loss: 0.680429, acc: 57.03%] [G loss: 1.803166]\n",
      "epoch:21 step:20342 [D loss: 0.672789, acc: 61.72%] [G loss: 1.767908]\n",
      "epoch:21 step:20343 [D loss: 0.731504, acc: 46.09%] [G loss: 1.753012]\n",
      "epoch:21 step:20344 [D loss: 0.642412, acc: 63.28%] [G loss: 1.654204]\n",
      "epoch:21 step:20345 [D loss: 0.682841, acc: 58.59%] [G loss: 1.718862]\n",
      "epoch:21 step:20346 [D loss: 0.682017, acc: 58.59%] [G loss: 1.668478]\n",
      "epoch:21 step:20347 [D loss: 0.681876, acc: 57.03%] [G loss: 1.707541]\n",
      "epoch:21 step:20348 [D loss: 0.612123, acc: 67.97%] [G loss: 1.765119]\n",
      "epoch:21 step:20349 [D loss: 0.677542, acc: 57.81%] [G loss: 1.824808]\n",
      "epoch:21 step:20350 [D loss: 0.670094, acc: 60.16%] [G loss: 1.809633]\n",
      "epoch:21 step:20351 [D loss: 0.678328, acc: 58.59%] [G loss: 1.775809]\n",
      "epoch:21 step:20352 [D loss: 0.646642, acc: 60.94%] [G loss: 1.806208]\n",
      "epoch:21 step:20353 [D loss: 0.615570, acc: 70.31%] [G loss: 1.809096]\n",
      "epoch:21 step:20354 [D loss: 0.607328, acc: 69.53%] [G loss: 1.887147]\n",
      "epoch:21 step:20355 [D loss: 0.640361, acc: 58.59%] [G loss: 1.882084]\n",
      "epoch:21 step:20356 [D loss: 0.608279, acc: 65.62%] [G loss: 1.965581]\n",
      "epoch:21 step:20357 [D loss: 0.636212, acc: 65.62%] [G loss: 1.787062]\n",
      "epoch:21 step:20358 [D loss: 0.601466, acc: 72.66%] [G loss: 1.921867]\n",
      "epoch:21 step:20359 [D loss: 0.590137, acc: 71.09%] [G loss: 1.884974]\n",
      "epoch:21 step:20360 [D loss: 0.706463, acc: 55.47%] [G loss: 1.801527]\n",
      "epoch:21 step:20361 [D loss: 0.674844, acc: 59.38%] [G loss: 1.654205]\n",
      "epoch:21 step:20362 [D loss: 0.654114, acc: 63.28%] [G loss: 1.831118]\n",
      "epoch:21 step:20363 [D loss: 0.607384, acc: 67.97%] [G loss: 1.964071]\n",
      "epoch:21 step:20364 [D loss: 0.678984, acc: 57.03%] [G loss: 1.910948]\n",
      "epoch:21 step:20365 [D loss: 0.646787, acc: 63.28%] [G loss: 1.906746]\n",
      "epoch:21 step:20366 [D loss: 0.615883, acc: 63.28%] [G loss: 1.961133]\n",
      "epoch:21 step:20367 [D loss: 0.588021, acc: 70.31%] [G loss: 1.959725]\n",
      "epoch:21 step:20368 [D loss: 0.575709, acc: 71.09%] [G loss: 1.974287]\n",
      "epoch:21 step:20369 [D loss: 0.618607, acc: 62.50%] [G loss: 2.079674]\n",
      "epoch:21 step:20370 [D loss: 0.657769, acc: 57.81%] [G loss: 1.955996]\n",
      "epoch:21 step:20371 [D loss: 0.598188, acc: 67.19%] [G loss: 2.179212]\n",
      "epoch:21 step:20372 [D loss: 0.603045, acc: 66.41%] [G loss: 1.908382]\n",
      "epoch:21 step:20373 [D loss: 0.674162, acc: 58.59%] [G loss: 2.023440]\n",
      "epoch:21 step:20374 [D loss: 0.669057, acc: 64.06%] [G loss: 1.798374]\n",
      "epoch:21 step:20375 [D loss: 0.677168, acc: 57.81%] [G loss: 2.028256]\n",
      "epoch:21 step:20376 [D loss: 0.625803, acc: 61.72%] [G loss: 1.911183]\n",
      "epoch:21 step:20377 [D loss: 0.655168, acc: 60.16%] [G loss: 1.819217]\n",
      "epoch:21 step:20378 [D loss: 0.679445, acc: 64.06%] [G loss: 1.810971]\n",
      "epoch:21 step:20379 [D loss: 0.693002, acc: 58.59%] [G loss: 1.779050]\n",
      "epoch:21 step:20380 [D loss: 0.697895, acc: 57.81%] [G loss: 1.702722]\n",
      "epoch:21 step:20381 [D loss: 0.692253, acc: 58.59%] [G loss: 1.797947]\n",
      "epoch:21 step:20382 [D loss: 0.640882, acc: 62.50%] [G loss: 1.692046]\n",
      "epoch:21 step:20383 [D loss: 0.671677, acc: 58.59%] [G loss: 1.941491]\n",
      "epoch:21 step:20384 [D loss: 0.589487, acc: 71.09%] [G loss: 1.951770]\n",
      "epoch:21 step:20385 [D loss: 0.627643, acc: 65.62%] [G loss: 2.040380]\n",
      "epoch:21 step:20386 [D loss: 0.646973, acc: 66.41%] [G loss: 1.724989]\n",
      "epoch:21 step:20387 [D loss: 0.625235, acc: 65.62%] [G loss: 1.913873]\n",
      "epoch:21 step:20388 [D loss: 0.627007, acc: 67.97%] [G loss: 2.092640]\n",
      "epoch:21 step:20389 [D loss: 0.663060, acc: 59.38%] [G loss: 1.835828]\n",
      "epoch:21 step:20390 [D loss: 0.682592, acc: 60.94%] [G loss: 1.841850]\n",
      "epoch:21 step:20391 [D loss: 0.653218, acc: 64.84%] [G loss: 1.915917]\n",
      "epoch:21 step:20392 [D loss: 0.668673, acc: 63.28%] [G loss: 1.818841]\n",
      "epoch:21 step:20393 [D loss: 0.723413, acc: 54.69%] [G loss: 1.791377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20394 [D loss: 0.631899, acc: 64.84%] [G loss: 1.906159]\n",
      "epoch:21 step:20395 [D loss: 0.641850, acc: 64.84%] [G loss: 1.933533]\n",
      "epoch:21 step:20396 [D loss: 0.600534, acc: 71.09%] [G loss: 2.037156]\n",
      "epoch:21 step:20397 [D loss: 0.652085, acc: 62.50%] [G loss: 2.015333]\n",
      "epoch:21 step:20398 [D loss: 0.644724, acc: 61.72%] [G loss: 2.007366]\n",
      "epoch:21 step:20399 [D loss: 0.622216, acc: 65.62%] [G loss: 1.839309]\n",
      "epoch:21 step:20400 [D loss: 0.681552, acc: 53.91%] [G loss: 1.937027]\n",
      "epoch:21 step:20401 [D loss: 0.654404, acc: 60.16%] [G loss: 2.036604]\n",
      "epoch:21 step:20402 [D loss: 0.698601, acc: 55.47%] [G loss: 1.926648]\n",
      "epoch:21 step:20403 [D loss: 0.661290, acc: 63.28%] [G loss: 1.961671]\n",
      "epoch:21 step:20404 [D loss: 0.693655, acc: 60.16%] [G loss: 1.774210]\n",
      "epoch:21 step:20405 [D loss: 0.685664, acc: 59.38%] [G loss: 1.796630]\n",
      "epoch:21 step:20406 [D loss: 0.670313, acc: 64.06%] [G loss: 1.886903]\n",
      "epoch:21 step:20407 [D loss: 0.636145, acc: 63.28%] [G loss: 1.868387]\n",
      "epoch:21 step:20408 [D loss: 0.659115, acc: 60.94%] [G loss: 1.778369]\n",
      "epoch:21 step:20409 [D loss: 0.674776, acc: 60.94%] [G loss: 1.725434]\n",
      "epoch:21 step:20410 [D loss: 0.597953, acc: 65.62%] [G loss: 1.977332]\n",
      "epoch:21 step:20411 [D loss: 0.613199, acc: 66.41%] [G loss: 1.705130]\n",
      "epoch:21 step:20412 [D loss: 0.685526, acc: 59.38%] [G loss: 1.779379]\n",
      "epoch:21 step:20413 [D loss: 0.630586, acc: 60.16%] [G loss: 1.826045]\n",
      "epoch:21 step:20414 [D loss: 0.614321, acc: 65.62%] [G loss: 1.887711]\n",
      "epoch:21 step:20415 [D loss: 0.645066, acc: 66.41%] [G loss: 1.827491]\n",
      "epoch:21 step:20416 [D loss: 0.651617, acc: 64.06%] [G loss: 1.724380]\n",
      "epoch:21 step:20417 [D loss: 0.610789, acc: 65.62%] [G loss: 1.984493]\n",
      "epoch:21 step:20418 [D loss: 0.608477, acc: 66.41%] [G loss: 1.807114]\n",
      "epoch:21 step:20419 [D loss: 0.648019, acc: 59.38%] [G loss: 1.871396]\n",
      "epoch:21 step:20420 [D loss: 0.636112, acc: 60.94%] [G loss: 2.024049]\n",
      "epoch:21 step:20421 [D loss: 0.675107, acc: 60.94%] [G loss: 1.870627]\n",
      "epoch:21 step:20422 [D loss: 0.683623, acc: 54.69%] [G loss: 1.906568]\n",
      "epoch:21 step:20423 [D loss: 0.583152, acc: 74.22%] [G loss: 1.993027]\n",
      "epoch:21 step:20424 [D loss: 0.674813, acc: 55.47%] [G loss: 2.063452]\n",
      "epoch:21 step:20425 [D loss: 0.665606, acc: 65.62%] [G loss: 1.863276]\n",
      "epoch:21 step:20426 [D loss: 0.662473, acc: 57.03%] [G loss: 1.764159]\n",
      "epoch:21 step:20427 [D loss: 0.652351, acc: 60.94%] [G loss: 1.910973]\n",
      "epoch:21 step:20428 [D loss: 0.652649, acc: 60.16%] [G loss: 1.893960]\n",
      "epoch:21 step:20429 [D loss: 0.702576, acc: 57.81%] [G loss: 1.769933]\n",
      "epoch:21 step:20430 [D loss: 0.607618, acc: 67.97%] [G loss: 1.784034]\n",
      "epoch:21 step:20431 [D loss: 0.612371, acc: 66.41%] [G loss: 1.940200]\n",
      "epoch:21 step:20432 [D loss: 0.653935, acc: 59.38%] [G loss: 2.010033]\n",
      "epoch:21 step:20433 [D loss: 0.648758, acc: 60.16%] [G loss: 1.906272]\n",
      "epoch:21 step:20434 [D loss: 0.653240, acc: 57.81%] [G loss: 1.839031]\n",
      "epoch:21 step:20435 [D loss: 0.649902, acc: 58.59%] [G loss: 1.826475]\n",
      "epoch:21 step:20436 [D loss: 0.642360, acc: 64.84%] [G loss: 1.901416]\n",
      "epoch:21 step:20437 [D loss: 0.660327, acc: 61.72%] [G loss: 1.725591]\n",
      "epoch:21 step:20438 [D loss: 0.685294, acc: 60.94%] [G loss: 1.741472]\n",
      "epoch:21 step:20439 [D loss: 0.637636, acc: 66.41%] [G loss: 1.843398]\n",
      "epoch:21 step:20440 [D loss: 0.603391, acc: 68.75%] [G loss: 1.740827]\n",
      "epoch:21 step:20441 [D loss: 0.662898, acc: 61.72%] [G loss: 1.962639]\n",
      "epoch:21 step:20442 [D loss: 0.771872, acc: 43.75%] [G loss: 1.655246]\n",
      "epoch:21 step:20443 [D loss: 0.725035, acc: 49.22%] [G loss: 1.738259]\n",
      "epoch:21 step:20444 [D loss: 0.651771, acc: 62.50%] [G loss: 1.824778]\n",
      "epoch:21 step:20445 [D loss: 0.618666, acc: 65.62%] [G loss: 1.854594]\n",
      "epoch:21 step:20446 [D loss: 0.643840, acc: 62.50%] [G loss: 1.962721]\n",
      "epoch:21 step:20447 [D loss: 0.644299, acc: 64.84%] [G loss: 1.752311]\n",
      "epoch:21 step:20448 [D loss: 0.644648, acc: 59.38%] [G loss: 1.717786]\n",
      "epoch:21 step:20449 [D loss: 0.601820, acc: 72.66%] [G loss: 1.874407]\n",
      "epoch:21 step:20450 [D loss: 0.636726, acc: 55.47%] [G loss: 1.765822]\n",
      "epoch:21 step:20451 [D loss: 0.575068, acc: 70.31%] [G loss: 2.146817]\n",
      "epoch:21 step:20452 [D loss: 0.602713, acc: 65.62%] [G loss: 2.299117]\n",
      "epoch:21 step:20453 [D loss: 0.627550, acc: 63.28%] [G loss: 2.030365]\n",
      "epoch:21 step:20454 [D loss: 0.723204, acc: 54.69%] [G loss: 1.904241]\n",
      "epoch:21 step:20455 [D loss: 0.652352, acc: 60.94%] [G loss: 1.828460]\n",
      "epoch:21 step:20456 [D loss: 0.634582, acc: 62.50%] [G loss: 1.862944]\n",
      "epoch:21 step:20457 [D loss: 0.669216, acc: 60.16%] [G loss: 1.885374]\n",
      "epoch:21 step:20458 [D loss: 0.629206, acc: 62.50%] [G loss: 1.928403]\n",
      "epoch:21 step:20459 [D loss: 0.582568, acc: 70.31%] [G loss: 1.907251]\n",
      "epoch:21 step:20460 [D loss: 0.668878, acc: 61.72%] [G loss: 1.875882]\n",
      "epoch:21 step:20461 [D loss: 0.699171, acc: 56.25%] [G loss: 1.860072]\n",
      "epoch:21 step:20462 [D loss: 0.668799, acc: 62.50%] [G loss: 1.901008]\n",
      "epoch:21 step:20463 [D loss: 0.627905, acc: 63.28%] [G loss: 2.061851]\n",
      "epoch:21 step:20464 [D loss: 0.674569, acc: 60.94%] [G loss: 1.868321]\n",
      "epoch:21 step:20465 [D loss: 0.673653, acc: 57.03%] [G loss: 1.886446]\n",
      "epoch:21 step:20466 [D loss: 0.633889, acc: 64.84%] [G loss: 2.011065]\n",
      "epoch:21 step:20467 [D loss: 0.627507, acc: 60.94%] [G loss: 1.790272]\n",
      "epoch:21 step:20468 [D loss: 0.628324, acc: 67.97%] [G loss: 1.869857]\n",
      "epoch:21 step:20469 [D loss: 0.585844, acc: 66.41%] [G loss: 1.951599]\n",
      "epoch:21 step:20470 [D loss: 0.611939, acc: 67.97%] [G loss: 2.075486]\n",
      "epoch:21 step:20471 [D loss: 0.721230, acc: 49.22%] [G loss: 1.836243]\n",
      "epoch:21 step:20472 [D loss: 0.712039, acc: 53.12%] [G loss: 1.839001]\n",
      "epoch:21 step:20473 [D loss: 0.623694, acc: 62.50%] [G loss: 1.897047]\n",
      "epoch:21 step:20474 [D loss: 0.665890, acc: 60.94%] [G loss: 1.828587]\n",
      "epoch:21 step:20475 [D loss: 0.661682, acc: 59.38%] [G loss: 1.860312]\n",
      "epoch:21 step:20476 [D loss: 0.609762, acc: 67.97%] [G loss: 1.797793]\n",
      "epoch:21 step:20477 [D loss: 0.741040, acc: 49.22%] [G loss: 1.841073]\n",
      "epoch:21 step:20478 [D loss: 0.688885, acc: 57.81%] [G loss: 1.631927]\n",
      "epoch:21 step:20479 [D loss: 0.662774, acc: 60.94%] [G loss: 1.813892]\n",
      "epoch:21 step:20480 [D loss: 0.640909, acc: 63.28%] [G loss: 1.853599]\n",
      "epoch:21 step:20481 [D loss: 0.675086, acc: 60.94%] [G loss: 1.951925]\n",
      "epoch:21 step:20482 [D loss: 0.638122, acc: 60.94%] [G loss: 2.036642]\n",
      "epoch:21 step:20483 [D loss: 0.657029, acc: 58.59%] [G loss: 1.728971]\n",
      "epoch:21 step:20484 [D loss: 0.633445, acc: 60.94%] [G loss: 1.963550]\n",
      "epoch:21 step:20485 [D loss: 0.717060, acc: 53.12%] [G loss: 1.841236]\n",
      "epoch:21 step:20486 [D loss: 0.638247, acc: 65.62%] [G loss: 1.844792]\n",
      "epoch:21 step:20487 [D loss: 0.615585, acc: 67.19%] [G loss: 1.958953]\n",
      "epoch:21 step:20488 [D loss: 0.675650, acc: 60.16%] [G loss: 1.731285]\n",
      "epoch:21 step:20489 [D loss: 0.658768, acc: 59.38%] [G loss: 1.837275]\n",
      "epoch:21 step:20490 [D loss: 0.698367, acc: 55.47%] [G loss: 1.763443]\n",
      "epoch:21 step:20491 [D loss: 0.624098, acc: 67.19%] [G loss: 1.827110]\n",
      "epoch:21 step:20492 [D loss: 0.672283, acc: 60.16%] [G loss: 1.966861]\n",
      "epoch:21 step:20493 [D loss: 0.637707, acc: 64.06%] [G loss: 2.055307]\n",
      "epoch:21 step:20494 [D loss: 0.669743, acc: 62.50%] [G loss: 1.851393]\n",
      "epoch:21 step:20495 [D loss: 0.654298, acc: 61.72%] [G loss: 1.777811]\n",
      "epoch:21 step:20496 [D loss: 0.666051, acc: 61.72%] [G loss: 1.916206]\n",
      "epoch:21 step:20497 [D loss: 0.718030, acc: 46.09%] [G loss: 1.692014]\n",
      "epoch:21 step:20498 [D loss: 0.684014, acc: 53.12%] [G loss: 1.721483]\n",
      "epoch:21 step:20499 [D loss: 0.678186, acc: 60.94%] [G loss: 1.726015]\n",
      "epoch:21 step:20500 [D loss: 0.629618, acc: 64.06%] [G loss: 1.880869]\n",
      "epoch:21 step:20501 [D loss: 0.685192, acc: 56.25%] [G loss: 1.794229]\n",
      "epoch:21 step:20502 [D loss: 0.624179, acc: 65.62%] [G loss: 1.850099]\n",
      "epoch:21 step:20503 [D loss: 0.642056, acc: 59.38%] [G loss: 1.882501]\n",
      "epoch:21 step:20504 [D loss: 0.675732, acc: 60.16%] [G loss: 1.763781]\n",
      "epoch:21 step:20505 [D loss: 0.701141, acc: 54.69%] [G loss: 1.681989]\n",
      "epoch:21 step:20506 [D loss: 0.715798, acc: 50.78%] [G loss: 1.678835]\n",
      "epoch:21 step:20507 [D loss: 0.653760, acc: 58.59%] [G loss: 1.836105]\n",
      "epoch:21 step:20508 [D loss: 0.698028, acc: 54.69%] [G loss: 1.673389]\n",
      "epoch:21 step:20509 [D loss: 0.630617, acc: 65.62%] [G loss: 1.896021]\n",
      "epoch:21 step:20510 [D loss: 0.592965, acc: 67.97%] [G loss: 1.862000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:20511 [D loss: 0.658504, acc: 67.19%] [G loss: 1.756418]\n",
      "epoch:21 step:20512 [D loss: 0.700745, acc: 59.38%] [G loss: 1.888566]\n",
      "epoch:21 step:20513 [D loss: 0.627161, acc: 64.84%] [G loss: 1.850965]\n",
      "epoch:21 step:20514 [D loss: 0.606075, acc: 64.06%] [G loss: 1.811693]\n",
      "epoch:21 step:20515 [D loss: 0.657899, acc: 57.81%] [G loss: 1.881842]\n",
      "epoch:21 step:20516 [D loss: 0.643771, acc: 64.06%] [G loss: 1.813619]\n",
      "epoch:21 step:20517 [D loss: 0.578337, acc: 71.09%] [G loss: 1.919682]\n",
      "epoch:21 step:20518 [D loss: 0.592110, acc: 66.41%] [G loss: 1.871789]\n",
      "epoch:21 step:20519 [D loss: 0.635828, acc: 66.41%] [G loss: 1.977884]\n",
      "epoch:21 step:20520 [D loss: 0.638112, acc: 59.38%] [G loss: 1.691105]\n",
      "epoch:21 step:20521 [D loss: 0.698644, acc: 57.03%] [G loss: 1.703765]\n",
      "epoch:21 step:20522 [D loss: 0.620282, acc: 62.50%] [G loss: 1.972653]\n",
      "epoch:21 step:20523 [D loss: 0.608449, acc: 69.53%] [G loss: 1.892982]\n",
      "epoch:21 step:20524 [D loss: 0.581898, acc: 71.09%] [G loss: 1.976774]\n",
      "epoch:21 step:20525 [D loss: 0.664432, acc: 60.16%] [G loss: 1.884124]\n",
      "epoch:21 step:20526 [D loss: 0.632128, acc: 64.84%] [G loss: 1.928007]\n",
      "epoch:21 step:20527 [D loss: 0.679883, acc: 61.72%] [G loss: 1.819617]\n",
      "epoch:21 step:20528 [D loss: 0.677720, acc: 54.69%] [G loss: 1.948538]\n",
      "epoch:21 step:20529 [D loss: 0.655929, acc: 60.16%] [G loss: 1.761202]\n",
      "epoch:21 step:20530 [D loss: 0.608783, acc: 62.50%] [G loss: 1.801252]\n",
      "epoch:21 step:20531 [D loss: 0.708385, acc: 53.91%] [G loss: 1.842180]\n",
      "epoch:21 step:20532 [D loss: 0.695175, acc: 56.25%] [G loss: 1.820070]\n",
      "epoch:21 step:20533 [D loss: 0.679108, acc: 59.38%] [G loss: 1.764410]\n",
      "epoch:21 step:20534 [D loss: 0.700676, acc: 55.47%] [G loss: 1.791511]\n",
      "epoch:21 step:20535 [D loss: 0.691450, acc: 57.03%] [G loss: 1.690361]\n",
      "epoch:21 step:20536 [D loss: 0.669899, acc: 59.38%] [G loss: 1.749613]\n",
      "epoch:21 step:20537 [D loss: 0.645210, acc: 67.97%] [G loss: 1.744572]\n",
      "epoch:21 step:20538 [D loss: 0.615865, acc: 67.97%] [G loss: 1.866693]\n",
      "epoch:21 step:20539 [D loss: 0.641138, acc: 60.16%] [G loss: 1.715328]\n",
      "epoch:21 step:20540 [D loss: 0.661507, acc: 53.91%] [G loss: 1.747085]\n",
      "epoch:21 step:20541 [D loss: 0.619052, acc: 68.75%] [G loss: 1.809878]\n",
      "epoch:21 step:20542 [D loss: 0.645585, acc: 61.72%] [G loss: 1.804523]\n",
      "epoch:21 step:20543 [D loss: 0.644645, acc: 63.28%] [G loss: 1.827719]\n",
      "epoch:21 step:20544 [D loss: 0.666976, acc: 57.03%] [G loss: 1.784292]\n",
      "epoch:21 step:20545 [D loss: 0.664491, acc: 57.81%] [G loss: 1.836172]\n",
      "epoch:21 step:20546 [D loss: 0.715574, acc: 59.38%] [G loss: 1.629693]\n",
      "epoch:21 step:20547 [D loss: 0.671037, acc: 58.59%] [G loss: 1.815776]\n",
      "epoch:21 step:20548 [D loss: 0.640348, acc: 64.06%] [G loss: 1.868300]\n",
      "epoch:21 step:20549 [D loss: 0.661599, acc: 60.94%] [G loss: 1.720702]\n",
      "epoch:21 step:20550 [D loss: 0.655745, acc: 64.84%] [G loss: 1.842081]\n",
      "epoch:21 step:20551 [D loss: 0.632426, acc: 66.41%] [G loss: 1.735870]\n",
      "epoch:21 step:20552 [D loss: 0.625416, acc: 63.28%] [G loss: 1.896664]\n",
      "epoch:21 step:20553 [D loss: 0.623545, acc: 63.28%] [G loss: 1.880706]\n",
      "epoch:21 step:20554 [D loss: 0.634609, acc: 66.41%] [G loss: 1.825798]\n",
      "epoch:21 step:20555 [D loss: 0.620223, acc: 64.06%] [G loss: 1.828227]\n",
      "epoch:21 step:20556 [D loss: 0.640436, acc: 67.19%] [G loss: 1.710273]\n",
      "epoch:21 step:20557 [D loss: 0.650529, acc: 64.06%] [G loss: 1.828266]\n",
      "epoch:21 step:20558 [D loss: 0.625524, acc: 71.88%] [G loss: 1.855555]\n",
      "epoch:21 step:20559 [D loss: 0.670990, acc: 54.69%] [G loss: 1.917175]\n",
      "epoch:21 step:20560 [D loss: 0.681320, acc: 56.25%] [G loss: 1.857837]\n",
      "epoch:21 step:20561 [D loss: 0.596446, acc: 67.97%] [G loss: 2.118279]\n",
      "epoch:21 step:20562 [D loss: 0.629660, acc: 62.50%] [G loss: 1.880123]\n",
      "epoch:21 step:20563 [D loss: 0.623068, acc: 66.41%] [G loss: 2.027709]\n",
      "epoch:21 step:20564 [D loss: 0.638336, acc: 65.62%] [G loss: 1.881788]\n",
      "epoch:21 step:20565 [D loss: 0.665588, acc: 61.72%] [G loss: 1.868884]\n",
      "epoch:21 step:20566 [D loss: 0.653141, acc: 60.16%] [G loss: 1.859390]\n",
      "epoch:21 step:20567 [D loss: 0.622800, acc: 67.97%] [G loss: 1.903019]\n",
      "epoch:21 step:20568 [D loss: 0.640568, acc: 63.28%] [G loss: 1.914520]\n",
      "epoch:21 step:20569 [D loss: 0.624735, acc: 68.75%] [G loss: 1.872811]\n",
      "epoch:21 step:20570 [D loss: 0.652318, acc: 60.16%] [G loss: 1.853119]\n",
      "epoch:21 step:20571 [D loss: 0.625952, acc: 62.50%] [G loss: 1.862398]\n",
      "epoch:21 step:20572 [D loss: 0.666399, acc: 57.03%] [G loss: 1.918432]\n",
      "epoch:21 step:20573 [D loss: 0.640739, acc: 58.59%] [G loss: 1.823577]\n",
      "epoch:21 step:20574 [D loss: 0.581665, acc: 70.31%] [G loss: 1.855860]\n",
      "epoch:21 step:20575 [D loss: 0.670433, acc: 60.94%] [G loss: 1.774008]\n",
      "epoch:21 step:20576 [D loss: 0.618350, acc: 66.41%] [G loss: 2.023193]\n",
      "epoch:21 step:20577 [D loss: 0.684162, acc: 57.03%] [G loss: 1.881974]\n",
      "epoch:21 step:20578 [D loss: 0.629111, acc: 66.41%] [G loss: 1.987130]\n",
      "epoch:21 step:20579 [D loss: 0.684286, acc: 58.59%] [G loss: 1.851884]\n",
      "epoch:21 step:20580 [D loss: 0.629775, acc: 60.94%] [G loss: 1.806099]\n",
      "epoch:21 step:20581 [D loss: 0.594814, acc: 72.66%] [G loss: 1.982306]\n",
      "epoch:21 step:20582 [D loss: 0.663367, acc: 56.25%] [G loss: 1.990488]\n",
      "epoch:21 step:20583 [D loss: 0.630144, acc: 60.94%] [G loss: 2.033596]\n",
      "epoch:21 step:20584 [D loss: 0.698939, acc: 59.38%] [G loss: 2.060400]\n",
      "epoch:21 step:20585 [D loss: 0.628385, acc: 61.72%] [G loss: 1.919854]\n",
      "epoch:21 step:20586 [D loss: 0.673629, acc: 53.91%] [G loss: 1.916310]\n",
      "epoch:21 step:20587 [D loss: 0.658031, acc: 66.41%] [G loss: 1.873247]\n",
      "epoch:21 step:20588 [D loss: 0.626300, acc: 63.28%] [G loss: 1.961674]\n",
      "epoch:21 step:20589 [D loss: 0.607632, acc: 66.41%] [G loss: 1.977823]\n",
      "epoch:21 step:20590 [D loss: 0.672358, acc: 58.59%] [G loss: 1.791772]\n",
      "epoch:21 step:20591 [D loss: 0.664660, acc: 57.81%] [G loss: 1.821208]\n",
      "epoch:21 step:20592 [D loss: 0.615229, acc: 64.06%] [G loss: 1.980413]\n",
      "epoch:21 step:20593 [D loss: 0.634755, acc: 67.19%] [G loss: 2.000134]\n",
      "epoch:21 step:20594 [D loss: 0.591385, acc: 70.31%] [G loss: 1.953239]\n",
      "epoch:21 step:20595 [D loss: 0.617838, acc: 66.41%] [G loss: 2.160818]\n",
      "epoch:21 step:20596 [D loss: 0.559226, acc: 71.88%] [G loss: 2.142603]\n",
      "epoch:21 step:20597 [D loss: 0.727947, acc: 53.91%] [G loss: 1.725549]\n",
      "epoch:21 step:20598 [D loss: 0.672549, acc: 60.16%] [G loss: 2.068146]\n",
      "epoch:21 step:20599 [D loss: 0.649682, acc: 57.81%] [G loss: 1.853168]\n",
      "epoch:21 step:20600 [D loss: 0.633429, acc: 64.06%] [G loss: 2.009812]\n",
      "epoch:21 step:20601 [D loss: 0.617195, acc: 66.41%] [G loss: 2.058551]\n",
      "epoch:21 step:20602 [D loss: 0.615619, acc: 67.97%] [G loss: 2.045729]\n",
      "epoch:21 step:20603 [D loss: 0.633929, acc: 60.16%] [G loss: 2.115479]\n",
      "epoch:21 step:20604 [D loss: 0.629507, acc: 67.97%] [G loss: 2.013907]\n",
      "epoch:21 step:20605 [D loss: 0.738811, acc: 53.12%] [G loss: 1.797001]\n",
      "epoch:21 step:20606 [D loss: 0.679539, acc: 54.69%] [G loss: 1.873223]\n",
      "epoch:21 step:20607 [D loss: 0.587482, acc: 68.75%] [G loss: 2.033571]\n",
      "epoch:21 step:20608 [D loss: 0.586673, acc: 72.66%] [G loss: 1.976607]\n",
      "epoch:21 step:20609 [D loss: 0.649520, acc: 60.94%] [G loss: 1.832498]\n",
      "epoch:21 step:20610 [D loss: 0.703115, acc: 57.81%] [G loss: 1.914147]\n",
      "epoch:21 step:20611 [D loss: 0.610591, acc: 65.62%] [G loss: 1.897662]\n",
      "epoch:21 step:20612 [D loss: 0.610443, acc: 67.97%] [G loss: 1.987835]\n",
      "epoch:21 step:20613 [D loss: 0.638464, acc: 60.94%] [G loss: 2.051863]\n",
      "epoch:21 step:20614 [D loss: 0.556458, acc: 79.69%] [G loss: 2.429597]\n",
      "epoch:22 step:20615 [D loss: 0.672530, acc: 63.28%] [G loss: 1.994251]\n",
      "epoch:22 step:20616 [D loss: 0.690276, acc: 54.69%] [G loss: 1.897441]\n",
      "epoch:22 step:20617 [D loss: 0.673573, acc: 60.94%] [G loss: 1.822047]\n",
      "epoch:22 step:20618 [D loss: 0.594309, acc: 67.19%] [G loss: 1.877501]\n",
      "epoch:22 step:20619 [D loss: 0.636874, acc: 62.50%] [G loss: 1.849286]\n",
      "epoch:22 step:20620 [D loss: 0.588738, acc: 67.97%] [G loss: 2.040545]\n",
      "epoch:22 step:20621 [D loss: 0.609710, acc: 65.62%] [G loss: 1.893217]\n",
      "epoch:22 step:20622 [D loss: 0.654713, acc: 64.84%] [G loss: 1.982324]\n",
      "epoch:22 step:20623 [D loss: 0.677947, acc: 64.06%] [G loss: 1.898075]\n",
      "epoch:22 step:20624 [D loss: 0.637702, acc: 62.50%] [G loss: 1.925265]\n",
      "epoch:22 step:20625 [D loss: 0.622994, acc: 63.28%] [G loss: 2.045457]\n",
      "epoch:22 step:20626 [D loss: 0.625353, acc: 66.41%] [G loss: 2.013861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20627 [D loss: 0.623567, acc: 63.28%] [G loss: 2.024605]\n",
      "epoch:22 step:20628 [D loss: 0.632743, acc: 67.19%] [G loss: 2.048326]\n",
      "epoch:22 step:20629 [D loss: 0.663934, acc: 57.81%] [G loss: 2.054547]\n",
      "epoch:22 step:20630 [D loss: 0.614690, acc: 67.97%] [G loss: 2.118950]\n",
      "epoch:22 step:20631 [D loss: 0.672537, acc: 63.28%] [G loss: 1.864911]\n",
      "epoch:22 step:20632 [D loss: 0.652229, acc: 60.16%] [G loss: 2.012637]\n",
      "epoch:22 step:20633 [D loss: 0.687279, acc: 55.47%] [G loss: 1.951667]\n",
      "epoch:22 step:20634 [D loss: 0.733556, acc: 53.91%] [G loss: 1.717210]\n",
      "epoch:22 step:20635 [D loss: 0.637317, acc: 65.62%] [G loss: 1.827499]\n",
      "epoch:22 step:20636 [D loss: 0.692798, acc: 57.81%] [G loss: 1.882134]\n",
      "epoch:22 step:20637 [D loss: 0.588250, acc: 67.97%] [G loss: 1.992705]\n",
      "epoch:22 step:20638 [D loss: 0.617354, acc: 67.97%] [G loss: 1.906996]\n",
      "epoch:22 step:20639 [D loss: 0.599574, acc: 72.66%] [G loss: 2.071974]\n",
      "epoch:22 step:20640 [D loss: 0.704500, acc: 62.50%] [G loss: 1.788875]\n",
      "epoch:22 step:20641 [D loss: 0.656542, acc: 62.50%] [G loss: 1.728063]\n",
      "epoch:22 step:20642 [D loss: 0.643862, acc: 63.28%] [G loss: 1.824752]\n",
      "epoch:22 step:20643 [D loss: 0.649883, acc: 67.19%] [G loss: 1.748899]\n",
      "epoch:22 step:20644 [D loss: 0.637619, acc: 61.72%] [G loss: 1.920466]\n",
      "epoch:22 step:20645 [D loss: 0.682110, acc: 56.25%] [G loss: 1.647709]\n",
      "epoch:22 step:20646 [D loss: 0.677653, acc: 57.81%] [G loss: 1.756468]\n",
      "epoch:22 step:20647 [D loss: 0.616467, acc: 70.31%] [G loss: 1.883065]\n",
      "epoch:22 step:20648 [D loss: 0.653556, acc: 64.06%] [G loss: 1.871311]\n",
      "epoch:22 step:20649 [D loss: 0.660347, acc: 62.50%] [G loss: 1.829113]\n",
      "epoch:22 step:20650 [D loss: 0.623013, acc: 68.75%] [G loss: 1.911415]\n",
      "epoch:22 step:20651 [D loss: 0.602345, acc: 73.44%] [G loss: 1.935082]\n",
      "epoch:22 step:20652 [D loss: 0.630419, acc: 64.84%] [G loss: 1.920109]\n",
      "epoch:22 step:20653 [D loss: 0.665097, acc: 57.81%] [G loss: 1.917201]\n",
      "epoch:22 step:20654 [D loss: 0.614855, acc: 64.84%] [G loss: 2.163777]\n",
      "epoch:22 step:20655 [D loss: 0.684852, acc: 59.38%] [G loss: 1.917347]\n",
      "epoch:22 step:20656 [D loss: 0.647193, acc: 57.03%] [G loss: 1.964725]\n",
      "epoch:22 step:20657 [D loss: 0.652262, acc: 62.50%] [G loss: 1.870611]\n",
      "epoch:22 step:20658 [D loss: 0.666635, acc: 55.47%] [G loss: 1.863150]\n",
      "epoch:22 step:20659 [D loss: 0.714411, acc: 56.25%] [G loss: 1.895701]\n",
      "epoch:22 step:20660 [D loss: 0.658536, acc: 53.12%] [G loss: 1.813456]\n",
      "epoch:22 step:20661 [D loss: 0.614775, acc: 59.38%] [G loss: 1.856608]\n",
      "epoch:22 step:20662 [D loss: 0.665366, acc: 57.03%] [G loss: 2.035182]\n",
      "epoch:22 step:20663 [D loss: 0.587261, acc: 72.66%] [G loss: 1.944158]\n",
      "epoch:22 step:20664 [D loss: 0.630647, acc: 67.19%] [G loss: 2.065543]\n",
      "epoch:22 step:20665 [D loss: 0.669888, acc: 62.50%] [G loss: 1.835580]\n",
      "epoch:22 step:20666 [D loss: 0.668389, acc: 60.94%] [G loss: 1.804030]\n",
      "epoch:22 step:20667 [D loss: 0.643683, acc: 67.97%] [G loss: 1.860793]\n",
      "epoch:22 step:20668 [D loss: 0.633776, acc: 60.16%] [G loss: 1.914187]\n",
      "epoch:22 step:20669 [D loss: 0.552604, acc: 75.00%] [G loss: 1.967386]\n",
      "epoch:22 step:20670 [D loss: 0.620108, acc: 68.75%] [G loss: 2.072868]\n",
      "epoch:22 step:20671 [D loss: 0.641266, acc: 65.62%] [G loss: 1.843966]\n",
      "epoch:22 step:20672 [D loss: 0.688059, acc: 59.38%] [G loss: 1.931119]\n",
      "epoch:22 step:20673 [D loss: 0.640245, acc: 64.84%] [G loss: 1.870685]\n",
      "epoch:22 step:20674 [D loss: 0.692505, acc: 56.25%] [G loss: 1.833352]\n",
      "epoch:22 step:20675 [D loss: 0.653082, acc: 58.59%] [G loss: 1.913028]\n",
      "epoch:22 step:20676 [D loss: 0.687217, acc: 55.47%] [G loss: 1.959245]\n",
      "epoch:22 step:20677 [D loss: 0.666092, acc: 58.59%] [G loss: 1.881987]\n",
      "epoch:22 step:20678 [D loss: 0.646354, acc: 64.06%] [G loss: 1.812312]\n",
      "epoch:22 step:20679 [D loss: 0.643719, acc: 66.41%] [G loss: 1.871465]\n",
      "epoch:22 step:20680 [D loss: 0.650782, acc: 64.84%] [G loss: 1.897745]\n",
      "epoch:22 step:20681 [D loss: 0.665119, acc: 60.94%] [G loss: 1.886245]\n",
      "epoch:22 step:20682 [D loss: 0.643720, acc: 61.72%] [G loss: 1.921494]\n",
      "epoch:22 step:20683 [D loss: 0.626849, acc: 62.50%] [G loss: 1.995963]\n",
      "epoch:22 step:20684 [D loss: 0.604453, acc: 67.19%] [G loss: 1.887379]\n",
      "epoch:22 step:20685 [D loss: 0.611083, acc: 66.41%] [G loss: 1.842672]\n",
      "epoch:22 step:20686 [D loss: 0.689173, acc: 60.16%] [G loss: 1.872601]\n",
      "epoch:22 step:20687 [D loss: 0.655418, acc: 56.25%] [G loss: 1.858749]\n",
      "epoch:22 step:20688 [D loss: 0.632604, acc: 64.06%] [G loss: 1.951174]\n",
      "epoch:22 step:20689 [D loss: 0.628727, acc: 64.06%] [G loss: 2.143598]\n",
      "epoch:22 step:20690 [D loss: 0.657330, acc: 64.06%] [G loss: 1.922443]\n",
      "epoch:22 step:20691 [D loss: 0.596733, acc: 67.19%] [G loss: 1.993822]\n",
      "epoch:22 step:20692 [D loss: 0.677905, acc: 60.16%] [G loss: 1.726780]\n",
      "epoch:22 step:20693 [D loss: 0.635354, acc: 64.06%] [G loss: 1.861928]\n",
      "epoch:22 step:20694 [D loss: 0.632972, acc: 67.97%] [G loss: 1.882561]\n",
      "epoch:22 step:20695 [D loss: 0.699134, acc: 59.38%] [G loss: 1.828827]\n",
      "epoch:22 step:20696 [D loss: 0.624916, acc: 59.38%] [G loss: 1.895372]\n",
      "epoch:22 step:20697 [D loss: 0.640370, acc: 64.06%] [G loss: 2.082422]\n",
      "epoch:22 step:20698 [D loss: 0.706974, acc: 58.59%] [G loss: 1.776369]\n",
      "epoch:22 step:20699 [D loss: 0.650353, acc: 57.03%] [G loss: 1.790540]\n",
      "epoch:22 step:20700 [D loss: 0.656437, acc: 60.16%] [G loss: 1.846734]\n",
      "epoch:22 step:20701 [D loss: 0.627389, acc: 66.41%] [G loss: 1.814820]\n",
      "epoch:22 step:20702 [D loss: 0.630969, acc: 60.94%] [G loss: 1.992308]\n",
      "epoch:22 step:20703 [D loss: 0.636150, acc: 61.72%] [G loss: 1.848414]\n",
      "epoch:22 step:20704 [D loss: 0.639325, acc: 62.50%] [G loss: 1.726468]\n",
      "epoch:22 step:20705 [D loss: 0.688561, acc: 60.16%] [G loss: 1.797127]\n",
      "epoch:22 step:20706 [D loss: 0.628604, acc: 66.41%] [G loss: 1.944331]\n",
      "epoch:22 step:20707 [D loss: 0.657726, acc: 62.50%] [G loss: 1.963470]\n",
      "epoch:22 step:20708 [D loss: 0.671321, acc: 57.81%] [G loss: 1.870934]\n",
      "epoch:22 step:20709 [D loss: 0.686820, acc: 59.38%] [G loss: 1.842762]\n",
      "epoch:22 step:20710 [D loss: 0.655823, acc: 60.16%] [G loss: 1.817448]\n",
      "epoch:22 step:20711 [D loss: 0.637153, acc: 64.84%] [G loss: 1.920519]\n",
      "epoch:22 step:20712 [D loss: 0.680074, acc: 53.12%] [G loss: 1.771564]\n",
      "epoch:22 step:20713 [D loss: 0.643235, acc: 68.75%] [G loss: 1.906764]\n",
      "epoch:22 step:20714 [D loss: 0.698031, acc: 61.72%] [G loss: 1.902118]\n",
      "epoch:22 step:20715 [D loss: 0.624464, acc: 64.84%] [G loss: 1.888694]\n",
      "epoch:22 step:20716 [D loss: 0.668816, acc: 59.38%] [G loss: 1.792396]\n",
      "epoch:22 step:20717 [D loss: 0.600611, acc: 71.88%] [G loss: 2.016765]\n",
      "epoch:22 step:20718 [D loss: 0.626419, acc: 66.41%] [G loss: 1.952667]\n",
      "epoch:22 step:20719 [D loss: 0.626500, acc: 68.75%] [G loss: 1.864257]\n",
      "epoch:22 step:20720 [D loss: 0.644838, acc: 66.41%] [G loss: 2.049069]\n",
      "epoch:22 step:20721 [D loss: 0.635314, acc: 65.62%] [G loss: 2.088230]\n",
      "epoch:22 step:20722 [D loss: 0.708096, acc: 52.34%] [G loss: 1.724294]\n",
      "epoch:22 step:20723 [D loss: 0.673309, acc: 59.38%] [G loss: 1.848539]\n",
      "epoch:22 step:20724 [D loss: 0.700925, acc: 55.47%] [G loss: 1.815415]\n",
      "epoch:22 step:20725 [D loss: 0.654556, acc: 62.50%] [G loss: 1.866307]\n",
      "epoch:22 step:20726 [D loss: 0.625092, acc: 62.50%] [G loss: 1.977500]\n",
      "epoch:22 step:20727 [D loss: 0.680847, acc: 58.59%] [G loss: 2.043716]\n",
      "epoch:22 step:20728 [D loss: 0.616422, acc: 60.16%] [G loss: 2.101836]\n",
      "epoch:22 step:20729 [D loss: 0.637588, acc: 60.16%] [G loss: 2.087883]\n",
      "epoch:22 step:20730 [D loss: 0.601028, acc: 66.41%] [G loss: 2.036410]\n",
      "epoch:22 step:20731 [D loss: 0.632800, acc: 67.19%] [G loss: 2.208027]\n",
      "epoch:22 step:20732 [D loss: 0.612785, acc: 64.84%] [G loss: 2.115460]\n",
      "epoch:22 step:20733 [D loss: 0.633462, acc: 67.97%] [G loss: 1.999386]\n",
      "epoch:22 step:20734 [D loss: 0.623737, acc: 66.41%] [G loss: 2.026835]\n",
      "epoch:22 step:20735 [D loss: 0.631676, acc: 60.94%] [G loss: 2.059129]\n",
      "epoch:22 step:20736 [D loss: 0.631236, acc: 62.50%] [G loss: 2.256826]\n",
      "epoch:22 step:20737 [D loss: 0.602136, acc: 70.31%] [G loss: 1.978313]\n",
      "epoch:22 step:20738 [D loss: 0.717846, acc: 55.47%] [G loss: 1.956562]\n",
      "epoch:22 step:20739 [D loss: 0.689397, acc: 53.12%] [G loss: 1.743758]\n",
      "epoch:22 step:20740 [D loss: 0.618173, acc: 66.41%] [G loss: 2.042873]\n",
      "epoch:22 step:20741 [D loss: 0.682750, acc: 57.03%] [G loss: 1.884967]\n",
      "epoch:22 step:20742 [D loss: 0.656224, acc: 63.28%] [G loss: 1.920926]\n",
      "epoch:22 step:20743 [D loss: 0.629266, acc: 65.62%] [G loss: 1.970327]\n",
      "epoch:22 step:20744 [D loss: 0.626066, acc: 67.19%] [G loss: 1.993734]\n",
      "epoch:22 step:20745 [D loss: 0.635630, acc: 63.28%] [G loss: 2.041167]\n",
      "epoch:22 step:20746 [D loss: 0.731183, acc: 50.00%] [G loss: 1.840165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20747 [D loss: 0.697729, acc: 51.56%] [G loss: 1.753208]\n",
      "epoch:22 step:20748 [D loss: 0.674467, acc: 57.81%] [G loss: 1.781649]\n",
      "epoch:22 step:20749 [D loss: 0.723345, acc: 56.25%] [G loss: 1.895131]\n",
      "epoch:22 step:20750 [D loss: 0.690919, acc: 55.47%] [G loss: 1.732091]\n",
      "epoch:22 step:20751 [D loss: 0.614523, acc: 66.41%] [G loss: 1.782495]\n",
      "epoch:22 step:20752 [D loss: 0.639459, acc: 61.72%] [G loss: 1.809944]\n",
      "epoch:22 step:20753 [D loss: 0.635636, acc: 64.06%] [G loss: 1.957312]\n",
      "epoch:22 step:20754 [D loss: 0.650918, acc: 60.94%] [G loss: 1.829464]\n",
      "epoch:22 step:20755 [D loss: 0.708333, acc: 53.91%] [G loss: 1.685997]\n",
      "epoch:22 step:20756 [D loss: 0.648932, acc: 59.38%] [G loss: 1.715970]\n",
      "epoch:22 step:20757 [D loss: 0.654882, acc: 57.81%] [G loss: 1.824504]\n",
      "epoch:22 step:20758 [D loss: 0.607373, acc: 67.19%] [G loss: 1.883268]\n",
      "epoch:22 step:20759 [D loss: 0.623933, acc: 67.19%] [G loss: 1.808551]\n",
      "epoch:22 step:20760 [D loss: 0.677983, acc: 53.91%] [G loss: 2.028103]\n",
      "epoch:22 step:20761 [D loss: 0.651301, acc: 60.16%] [G loss: 1.831778]\n",
      "epoch:22 step:20762 [D loss: 0.692654, acc: 55.47%] [G loss: 1.881976]\n",
      "epoch:22 step:20763 [D loss: 0.693261, acc: 60.16%] [G loss: 1.975808]\n",
      "epoch:22 step:20764 [D loss: 0.651169, acc: 60.16%] [G loss: 2.039049]\n",
      "epoch:22 step:20765 [D loss: 0.654146, acc: 64.06%] [G loss: 1.894460]\n",
      "epoch:22 step:20766 [D loss: 0.610441, acc: 70.31%] [G loss: 1.940768]\n",
      "epoch:22 step:20767 [D loss: 0.652394, acc: 61.72%] [G loss: 1.927866]\n",
      "epoch:22 step:20768 [D loss: 0.609731, acc: 64.84%] [G loss: 1.899008]\n",
      "epoch:22 step:20769 [D loss: 0.635444, acc: 62.50%] [G loss: 1.875612]\n",
      "epoch:22 step:20770 [D loss: 0.596543, acc: 67.97%] [G loss: 1.884436]\n",
      "epoch:22 step:20771 [D loss: 0.630602, acc: 64.84%] [G loss: 1.804069]\n",
      "epoch:22 step:20772 [D loss: 0.644653, acc: 58.59%] [G loss: 1.901343]\n",
      "epoch:22 step:20773 [D loss: 0.642677, acc: 60.94%] [G loss: 1.810536]\n",
      "epoch:22 step:20774 [D loss: 0.745425, acc: 51.56%] [G loss: 1.611295]\n",
      "epoch:22 step:20775 [D loss: 0.656377, acc: 60.94%] [G loss: 1.782438]\n",
      "epoch:22 step:20776 [D loss: 0.625556, acc: 66.41%] [G loss: 1.878018]\n",
      "epoch:22 step:20777 [D loss: 0.669937, acc: 64.06%] [G loss: 1.879994]\n",
      "epoch:22 step:20778 [D loss: 0.670186, acc: 60.94%] [G loss: 1.814058]\n",
      "epoch:22 step:20779 [D loss: 0.644976, acc: 65.62%] [G loss: 1.851138]\n",
      "epoch:22 step:20780 [D loss: 0.646625, acc: 60.16%] [G loss: 1.855899]\n",
      "epoch:22 step:20781 [D loss: 0.619114, acc: 64.06%] [G loss: 2.000212]\n",
      "epoch:22 step:20782 [D loss: 0.632079, acc: 65.62%] [G loss: 1.939789]\n",
      "epoch:22 step:20783 [D loss: 0.657914, acc: 66.41%] [G loss: 1.843992]\n",
      "epoch:22 step:20784 [D loss: 0.658982, acc: 61.72%] [G loss: 1.807922]\n",
      "epoch:22 step:20785 [D loss: 0.681521, acc: 55.47%] [G loss: 1.808052]\n",
      "epoch:22 step:20786 [D loss: 0.643930, acc: 57.81%] [G loss: 1.777336]\n",
      "epoch:22 step:20787 [D loss: 0.708161, acc: 50.78%] [G loss: 1.858638]\n",
      "epoch:22 step:20788 [D loss: 0.669277, acc: 61.72%] [G loss: 1.782314]\n",
      "epoch:22 step:20789 [D loss: 0.655969, acc: 58.59%] [G loss: 1.835565]\n",
      "epoch:22 step:20790 [D loss: 0.714287, acc: 50.78%] [G loss: 1.743297]\n",
      "epoch:22 step:20791 [D loss: 0.649632, acc: 61.72%] [G loss: 1.725758]\n",
      "epoch:22 step:20792 [D loss: 0.650196, acc: 60.94%] [G loss: 1.762977]\n",
      "epoch:22 step:20793 [D loss: 0.640752, acc: 64.84%] [G loss: 1.819327]\n",
      "epoch:22 step:20794 [D loss: 0.672920, acc: 53.91%] [G loss: 1.754837]\n",
      "epoch:22 step:20795 [D loss: 0.661211, acc: 56.25%] [G loss: 1.846707]\n",
      "epoch:22 step:20796 [D loss: 0.655152, acc: 60.94%] [G loss: 1.843365]\n",
      "epoch:22 step:20797 [D loss: 0.662213, acc: 59.38%] [G loss: 1.668002]\n",
      "epoch:22 step:20798 [D loss: 0.616049, acc: 64.84%] [G loss: 1.900348]\n",
      "epoch:22 step:20799 [D loss: 0.703054, acc: 58.59%] [G loss: 1.818986]\n",
      "epoch:22 step:20800 [D loss: 0.669648, acc: 62.50%] [G loss: 1.760471]\n",
      "epoch:22 step:20801 [D loss: 0.676964, acc: 60.16%] [G loss: 1.733264]\n",
      "epoch:22 step:20802 [D loss: 0.658383, acc: 55.47%] [G loss: 1.982638]\n",
      "epoch:22 step:20803 [D loss: 0.664764, acc: 60.16%] [G loss: 1.812026]\n",
      "epoch:22 step:20804 [D loss: 0.651405, acc: 59.38%] [G loss: 1.822896]\n",
      "epoch:22 step:20805 [D loss: 0.616366, acc: 66.41%] [G loss: 1.878430]\n",
      "epoch:22 step:20806 [D loss: 0.651419, acc: 63.28%] [G loss: 1.780462]\n",
      "epoch:22 step:20807 [D loss: 0.630758, acc: 67.97%] [G loss: 1.909931]\n",
      "epoch:22 step:20808 [D loss: 0.627692, acc: 61.72%] [G loss: 1.852316]\n",
      "epoch:22 step:20809 [D loss: 0.657253, acc: 65.62%] [G loss: 1.882617]\n",
      "epoch:22 step:20810 [D loss: 0.663511, acc: 55.47%] [G loss: 1.824948]\n",
      "epoch:22 step:20811 [D loss: 0.601786, acc: 68.75%] [G loss: 1.946291]\n",
      "epoch:22 step:20812 [D loss: 0.606052, acc: 70.31%] [G loss: 1.907996]\n",
      "epoch:22 step:20813 [D loss: 0.641326, acc: 61.72%] [G loss: 1.841501]\n",
      "epoch:22 step:20814 [D loss: 0.690791, acc: 54.69%] [G loss: 1.756727]\n",
      "epoch:22 step:20815 [D loss: 0.637448, acc: 65.62%] [G loss: 1.950010]\n",
      "epoch:22 step:20816 [D loss: 0.679231, acc: 50.00%] [G loss: 1.808825]\n",
      "epoch:22 step:20817 [D loss: 0.643274, acc: 57.03%] [G loss: 1.710684]\n",
      "epoch:22 step:20818 [D loss: 0.633387, acc: 65.62%] [G loss: 1.978235]\n",
      "epoch:22 step:20819 [D loss: 0.674936, acc: 56.25%] [G loss: 1.775069]\n",
      "epoch:22 step:20820 [D loss: 0.605473, acc: 64.06%] [G loss: 1.961266]\n",
      "epoch:22 step:20821 [D loss: 0.614049, acc: 61.72%] [G loss: 2.121238]\n",
      "epoch:22 step:20822 [D loss: 0.549492, acc: 71.88%] [G loss: 2.214802]\n",
      "epoch:22 step:20823 [D loss: 0.587333, acc: 68.75%] [G loss: 2.143477]\n",
      "epoch:22 step:20824 [D loss: 0.739194, acc: 52.34%] [G loss: 1.774927]\n",
      "epoch:22 step:20825 [D loss: 0.653182, acc: 58.59%] [G loss: 1.802202]\n",
      "epoch:22 step:20826 [D loss: 0.710008, acc: 53.91%] [G loss: 1.752569]\n",
      "epoch:22 step:20827 [D loss: 0.673278, acc: 60.16%] [G loss: 1.701878]\n",
      "epoch:22 step:20828 [D loss: 0.626363, acc: 59.38%] [G loss: 1.843367]\n",
      "epoch:22 step:20829 [D loss: 0.703971, acc: 50.00%] [G loss: 1.885807]\n",
      "epoch:22 step:20830 [D loss: 0.686392, acc: 55.47%] [G loss: 1.826889]\n",
      "epoch:22 step:20831 [D loss: 0.659791, acc: 60.16%] [G loss: 1.829948]\n",
      "epoch:22 step:20832 [D loss: 0.644334, acc: 65.62%] [G loss: 1.933246]\n",
      "epoch:22 step:20833 [D loss: 0.584652, acc: 70.31%] [G loss: 2.092328]\n",
      "epoch:22 step:20834 [D loss: 0.751465, acc: 45.31%] [G loss: 1.745197]\n",
      "epoch:22 step:20835 [D loss: 0.669498, acc: 60.94%] [G loss: 1.896110]\n",
      "epoch:22 step:20836 [D loss: 0.621824, acc: 66.41%] [G loss: 1.811904]\n",
      "epoch:22 step:20837 [D loss: 0.682517, acc: 60.16%] [G loss: 1.868126]\n",
      "epoch:22 step:20838 [D loss: 0.676794, acc: 53.91%] [G loss: 1.841829]\n",
      "epoch:22 step:20839 [D loss: 0.620861, acc: 64.84%] [G loss: 1.788980]\n",
      "epoch:22 step:20840 [D loss: 0.693647, acc: 54.69%] [G loss: 1.718309]\n",
      "epoch:22 step:20841 [D loss: 0.673349, acc: 63.28%] [G loss: 1.804547]\n",
      "epoch:22 step:20842 [D loss: 0.660595, acc: 61.72%] [G loss: 1.682972]\n",
      "epoch:22 step:20843 [D loss: 0.609122, acc: 67.97%] [G loss: 1.925564]\n",
      "epoch:22 step:20844 [D loss: 0.617336, acc: 64.84%] [G loss: 2.073071]\n",
      "epoch:22 step:20845 [D loss: 0.619025, acc: 67.19%] [G loss: 2.310605]\n",
      "epoch:22 step:20846 [D loss: 0.601016, acc: 66.41%] [G loss: 2.285093]\n",
      "epoch:22 step:20847 [D loss: 0.708077, acc: 57.81%] [G loss: 1.865116]\n",
      "epoch:22 step:20848 [D loss: 0.705493, acc: 56.25%] [G loss: 1.825719]\n",
      "epoch:22 step:20849 [D loss: 0.642003, acc: 60.94%] [G loss: 1.861942]\n",
      "epoch:22 step:20850 [D loss: 0.668030, acc: 63.28%] [G loss: 1.782378]\n",
      "epoch:22 step:20851 [D loss: 0.653790, acc: 60.94%] [G loss: 1.805597]\n",
      "epoch:22 step:20852 [D loss: 0.622850, acc: 70.31%] [G loss: 1.825883]\n",
      "epoch:22 step:20853 [D loss: 0.650660, acc: 58.59%] [G loss: 1.807328]\n",
      "epoch:22 step:20854 [D loss: 0.692507, acc: 57.81%] [G loss: 1.856619]\n",
      "epoch:22 step:20855 [D loss: 0.614453, acc: 64.84%] [G loss: 1.988929]\n",
      "epoch:22 step:20856 [D loss: 0.583960, acc: 71.88%] [G loss: 2.033873]\n",
      "epoch:22 step:20857 [D loss: 0.631102, acc: 63.28%] [G loss: 1.842635]\n",
      "epoch:22 step:20858 [D loss: 0.664312, acc: 55.47%] [G loss: 1.879043]\n",
      "epoch:22 step:20859 [D loss: 0.657416, acc: 62.50%] [G loss: 1.845827]\n",
      "epoch:22 step:20860 [D loss: 0.616747, acc: 70.31%] [G loss: 1.970079]\n",
      "epoch:22 step:20861 [D loss: 0.639176, acc: 67.19%] [G loss: 1.849061]\n",
      "epoch:22 step:20862 [D loss: 0.636806, acc: 64.06%] [G loss: 1.957021]\n",
      "epoch:22 step:20863 [D loss: 0.664979, acc: 59.38%] [G loss: 1.679225]\n",
      "epoch:22 step:20864 [D loss: 0.705201, acc: 51.56%] [G loss: 1.644843]\n",
      "epoch:22 step:20865 [D loss: 0.674576, acc: 54.69%] [G loss: 1.646496]\n",
      "epoch:22 step:20866 [D loss: 0.660346, acc: 56.25%] [G loss: 1.599627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20867 [D loss: 0.638718, acc: 64.84%] [G loss: 1.804157]\n",
      "epoch:22 step:20868 [D loss: 0.647664, acc: 58.59%] [G loss: 1.644784]\n",
      "epoch:22 step:20869 [D loss: 0.610480, acc: 67.97%] [G loss: 1.806065]\n",
      "epoch:22 step:20870 [D loss: 0.592737, acc: 70.31%] [G loss: 1.834695]\n",
      "epoch:22 step:20871 [D loss: 0.664492, acc: 53.91%] [G loss: 1.880778]\n",
      "epoch:22 step:20872 [D loss: 0.625919, acc: 64.06%] [G loss: 1.807243]\n",
      "epoch:22 step:20873 [D loss: 0.703126, acc: 57.81%] [G loss: 1.772862]\n",
      "epoch:22 step:20874 [D loss: 0.661962, acc: 60.16%] [G loss: 1.751569]\n",
      "epoch:22 step:20875 [D loss: 0.616220, acc: 64.06%] [G loss: 1.923270]\n",
      "epoch:22 step:20876 [D loss: 0.628757, acc: 60.16%] [G loss: 1.915443]\n",
      "epoch:22 step:20877 [D loss: 0.688527, acc: 58.59%] [G loss: 1.791481]\n",
      "epoch:22 step:20878 [D loss: 0.689691, acc: 55.47%] [G loss: 1.922012]\n",
      "epoch:22 step:20879 [D loss: 0.652086, acc: 60.94%] [G loss: 1.840758]\n",
      "epoch:22 step:20880 [D loss: 0.643465, acc: 61.72%] [G loss: 1.835672]\n",
      "epoch:22 step:20881 [D loss: 0.670207, acc: 57.03%] [G loss: 1.767102]\n",
      "epoch:22 step:20882 [D loss: 0.660795, acc: 60.94%] [G loss: 1.891859]\n",
      "epoch:22 step:20883 [D loss: 0.693693, acc: 59.38%] [G loss: 1.872154]\n",
      "epoch:22 step:20884 [D loss: 0.630992, acc: 65.62%] [G loss: 1.910122]\n",
      "epoch:22 step:20885 [D loss: 0.638407, acc: 65.62%] [G loss: 1.864683]\n",
      "epoch:22 step:20886 [D loss: 0.621825, acc: 64.84%] [G loss: 1.704205]\n",
      "epoch:22 step:20887 [D loss: 0.626006, acc: 67.97%] [G loss: 1.799180]\n",
      "epoch:22 step:20888 [D loss: 0.637477, acc: 62.50%] [G loss: 1.998969]\n",
      "epoch:22 step:20889 [D loss: 0.614291, acc: 71.09%] [G loss: 1.847637]\n",
      "epoch:22 step:20890 [D loss: 0.589174, acc: 70.31%] [G loss: 2.136916]\n",
      "epoch:22 step:20891 [D loss: 0.647097, acc: 65.62%] [G loss: 1.920584]\n",
      "epoch:22 step:20892 [D loss: 0.637506, acc: 67.19%] [G loss: 1.830673]\n",
      "epoch:22 step:20893 [D loss: 0.643151, acc: 64.84%] [G loss: 1.871212]\n",
      "epoch:22 step:20894 [D loss: 0.615608, acc: 64.06%] [G loss: 1.919637]\n",
      "epoch:22 step:20895 [D loss: 0.675176, acc: 57.81%] [G loss: 1.797109]\n",
      "epoch:22 step:20896 [D loss: 0.601777, acc: 63.28%] [G loss: 1.906228]\n",
      "epoch:22 step:20897 [D loss: 0.680346, acc: 64.84%] [G loss: 1.991354]\n",
      "epoch:22 step:20898 [D loss: 0.658542, acc: 61.72%] [G loss: 1.778273]\n",
      "epoch:22 step:20899 [D loss: 0.659641, acc: 67.19%] [G loss: 1.854187]\n",
      "epoch:22 step:20900 [D loss: 0.586165, acc: 70.31%] [G loss: 2.044736]\n",
      "epoch:22 step:20901 [D loss: 0.683199, acc: 61.72%] [G loss: 1.880969]\n",
      "epoch:22 step:20902 [D loss: 0.638657, acc: 65.62%] [G loss: 1.838597]\n",
      "epoch:22 step:20903 [D loss: 0.607372, acc: 67.97%] [G loss: 1.998637]\n",
      "epoch:22 step:20904 [D loss: 0.684705, acc: 58.59%] [G loss: 1.836288]\n",
      "epoch:22 step:20905 [D loss: 0.702956, acc: 57.81%] [G loss: 1.731262]\n",
      "epoch:22 step:20906 [D loss: 0.641803, acc: 62.50%] [G loss: 1.810076]\n",
      "epoch:22 step:20907 [D loss: 0.646171, acc: 60.94%] [G loss: 1.958939]\n",
      "epoch:22 step:20908 [D loss: 0.648258, acc: 57.81%] [G loss: 1.855337]\n",
      "epoch:22 step:20909 [D loss: 0.656598, acc: 64.06%] [G loss: 1.748261]\n",
      "epoch:22 step:20910 [D loss: 0.603318, acc: 64.84%] [G loss: 1.967521]\n",
      "epoch:22 step:20911 [D loss: 0.629348, acc: 70.31%] [G loss: 1.776959]\n",
      "epoch:22 step:20912 [D loss: 0.608032, acc: 70.31%] [G loss: 2.006174]\n",
      "epoch:22 step:20913 [D loss: 0.688847, acc: 60.16%] [G loss: 1.931997]\n",
      "epoch:22 step:20914 [D loss: 0.665301, acc: 58.59%] [G loss: 1.861667]\n",
      "epoch:22 step:20915 [D loss: 0.712998, acc: 54.69%] [G loss: 1.687445]\n",
      "epoch:22 step:20916 [D loss: 0.675723, acc: 60.94%] [G loss: 1.763476]\n",
      "epoch:22 step:20917 [D loss: 0.648054, acc: 65.62%] [G loss: 1.820224]\n",
      "epoch:22 step:20918 [D loss: 0.651987, acc: 61.72%] [G loss: 1.902313]\n",
      "epoch:22 step:20919 [D loss: 0.625456, acc: 64.84%] [G loss: 1.799899]\n",
      "epoch:22 step:20920 [D loss: 0.655908, acc: 59.38%] [G loss: 1.855460]\n",
      "epoch:22 step:20921 [D loss: 0.662478, acc: 64.06%] [G loss: 1.818970]\n",
      "epoch:22 step:20922 [D loss: 0.655195, acc: 63.28%] [G loss: 1.879173]\n",
      "epoch:22 step:20923 [D loss: 0.603502, acc: 64.06%] [G loss: 1.873510]\n",
      "epoch:22 step:20924 [D loss: 0.651850, acc: 64.84%] [G loss: 1.864365]\n",
      "epoch:22 step:20925 [D loss: 0.695485, acc: 58.59%] [G loss: 1.834602]\n",
      "epoch:22 step:20926 [D loss: 0.581355, acc: 66.41%] [G loss: 2.129562]\n",
      "epoch:22 step:20927 [D loss: 0.641085, acc: 66.41%] [G loss: 1.992990]\n",
      "epoch:22 step:20928 [D loss: 0.591778, acc: 68.75%] [G loss: 2.197264]\n",
      "epoch:22 step:20929 [D loss: 0.554328, acc: 75.00%] [G loss: 2.162503]\n",
      "epoch:22 step:20930 [D loss: 0.786666, acc: 46.88%] [G loss: 1.722405]\n",
      "epoch:22 step:20931 [D loss: 0.645852, acc: 58.59%] [G loss: 1.679622]\n",
      "epoch:22 step:20932 [D loss: 0.710818, acc: 53.91%] [G loss: 1.840602]\n",
      "epoch:22 step:20933 [D loss: 0.664717, acc: 61.72%] [G loss: 1.808254]\n",
      "epoch:22 step:20934 [D loss: 0.670094, acc: 60.16%] [G loss: 1.820032]\n",
      "epoch:22 step:20935 [D loss: 0.633459, acc: 60.94%] [G loss: 1.881322]\n",
      "epoch:22 step:20936 [D loss: 0.656191, acc: 64.84%] [G loss: 1.831191]\n",
      "epoch:22 step:20937 [D loss: 0.673967, acc: 60.16%] [G loss: 1.822261]\n",
      "epoch:22 step:20938 [D loss: 0.674673, acc: 62.50%] [G loss: 1.818146]\n",
      "epoch:22 step:20939 [D loss: 0.619334, acc: 67.19%] [G loss: 1.808580]\n",
      "epoch:22 step:20940 [D loss: 0.620701, acc: 69.53%] [G loss: 1.841383]\n",
      "epoch:22 step:20941 [D loss: 0.710006, acc: 53.91%] [G loss: 1.722622]\n",
      "epoch:22 step:20942 [D loss: 0.668664, acc: 58.59%] [G loss: 1.733801]\n",
      "epoch:22 step:20943 [D loss: 0.663194, acc: 53.91%] [G loss: 1.796590]\n",
      "epoch:22 step:20944 [D loss: 0.662711, acc: 64.06%] [G loss: 1.803914]\n",
      "epoch:22 step:20945 [D loss: 0.674155, acc: 59.38%] [G loss: 1.739913]\n",
      "epoch:22 step:20946 [D loss: 0.638812, acc: 64.06%] [G loss: 1.777242]\n",
      "epoch:22 step:20947 [D loss: 0.616596, acc: 70.31%] [G loss: 1.851444]\n",
      "epoch:22 step:20948 [D loss: 0.675814, acc: 62.50%] [G loss: 1.834134]\n",
      "epoch:22 step:20949 [D loss: 0.660803, acc: 62.50%] [G loss: 1.798299]\n",
      "epoch:22 step:20950 [D loss: 0.652825, acc: 64.84%] [G loss: 1.833421]\n",
      "epoch:22 step:20951 [D loss: 0.688569, acc: 58.59%] [G loss: 1.920079]\n",
      "epoch:22 step:20952 [D loss: 0.642404, acc: 67.19%] [G loss: 1.924100]\n",
      "epoch:22 step:20953 [D loss: 0.650342, acc: 62.50%] [G loss: 1.902582]\n",
      "epoch:22 step:20954 [D loss: 0.645507, acc: 64.84%] [G loss: 1.879050]\n",
      "epoch:22 step:20955 [D loss: 0.642439, acc: 57.03%] [G loss: 1.802210]\n",
      "epoch:22 step:20956 [D loss: 0.673148, acc: 53.12%] [G loss: 1.774122]\n",
      "epoch:22 step:20957 [D loss: 0.660516, acc: 66.41%] [G loss: 1.837491]\n",
      "epoch:22 step:20958 [D loss: 0.642117, acc: 62.50%] [G loss: 1.695797]\n",
      "epoch:22 step:20959 [D loss: 0.629831, acc: 65.62%] [G loss: 2.038819]\n",
      "epoch:22 step:20960 [D loss: 0.630459, acc: 63.28%] [G loss: 2.103832]\n",
      "epoch:22 step:20961 [D loss: 0.575321, acc: 70.31%] [G loss: 2.174625]\n",
      "epoch:22 step:20962 [D loss: 0.632315, acc: 60.94%] [G loss: 1.787781]\n",
      "epoch:22 step:20963 [D loss: 0.683292, acc: 53.12%] [G loss: 1.756747]\n",
      "epoch:22 step:20964 [D loss: 0.666339, acc: 59.38%] [G loss: 1.959492]\n",
      "epoch:22 step:20965 [D loss: 0.646310, acc: 63.28%] [G loss: 1.805001]\n",
      "epoch:22 step:20966 [D loss: 0.679016, acc: 58.59%] [G loss: 1.810917]\n",
      "epoch:22 step:20967 [D loss: 0.646390, acc: 65.62%] [G loss: 1.862406]\n",
      "epoch:22 step:20968 [D loss: 0.627121, acc: 63.28%] [G loss: 1.955266]\n",
      "epoch:22 step:20969 [D loss: 0.719646, acc: 55.47%] [G loss: 1.691921]\n",
      "epoch:22 step:20970 [D loss: 0.688165, acc: 56.25%] [G loss: 1.717203]\n",
      "epoch:22 step:20971 [D loss: 0.711910, acc: 55.47%] [G loss: 1.916616]\n",
      "epoch:22 step:20972 [D loss: 0.581779, acc: 72.66%] [G loss: 1.940729]\n",
      "epoch:22 step:20973 [D loss: 0.637431, acc: 64.84%] [G loss: 1.874365]\n",
      "epoch:22 step:20974 [D loss: 0.620536, acc: 65.62%] [G loss: 1.924410]\n",
      "epoch:22 step:20975 [D loss: 0.640757, acc: 65.62%] [G loss: 1.880096]\n",
      "epoch:22 step:20976 [D loss: 0.684469, acc: 58.59%] [G loss: 1.811002]\n",
      "epoch:22 step:20977 [D loss: 0.607712, acc: 68.75%] [G loss: 1.904540]\n",
      "epoch:22 step:20978 [D loss: 0.592329, acc: 70.31%] [G loss: 1.859871]\n",
      "epoch:22 step:20979 [D loss: 0.686697, acc: 57.81%] [G loss: 1.946546]\n",
      "epoch:22 step:20980 [D loss: 0.644440, acc: 66.41%] [G loss: 1.923405]\n",
      "epoch:22 step:20981 [D loss: 0.699535, acc: 55.47%] [G loss: 1.980778]\n",
      "epoch:22 step:20982 [D loss: 0.644530, acc: 62.50%] [G loss: 1.912926]\n",
      "epoch:22 step:20983 [D loss: 0.652032, acc: 64.84%] [G loss: 1.834846]\n",
      "epoch:22 step:20984 [D loss: 0.633997, acc: 67.19%] [G loss: 1.966285]\n",
      "epoch:22 step:20985 [D loss: 0.632308, acc: 68.75%] [G loss: 1.896517]\n",
      "epoch:22 step:20986 [D loss: 0.645390, acc: 67.19%] [G loss: 1.866704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:20987 [D loss: 0.736731, acc: 53.91%] [G loss: 1.762007]\n",
      "epoch:22 step:20988 [D loss: 0.650358, acc: 69.53%] [G loss: 1.920954]\n",
      "epoch:22 step:20989 [D loss: 0.672669, acc: 60.16%] [G loss: 1.714249]\n",
      "epoch:22 step:20990 [D loss: 0.653925, acc: 59.38%] [G loss: 1.780763]\n",
      "epoch:22 step:20991 [D loss: 0.662087, acc: 58.59%] [G loss: 1.921161]\n",
      "epoch:22 step:20992 [D loss: 0.667990, acc: 62.50%] [G loss: 1.794189]\n",
      "epoch:22 step:20993 [D loss: 0.697936, acc: 55.47%] [G loss: 1.905537]\n",
      "epoch:22 step:20994 [D loss: 0.680262, acc: 64.84%] [G loss: 1.845154]\n",
      "epoch:22 step:20995 [D loss: 0.617487, acc: 61.72%] [G loss: 1.930567]\n",
      "epoch:22 step:20996 [D loss: 0.634968, acc: 67.97%] [G loss: 1.828413]\n",
      "epoch:22 step:20997 [D loss: 0.636883, acc: 64.84%] [G loss: 1.965151]\n",
      "epoch:22 step:20998 [D loss: 0.651525, acc: 60.16%] [G loss: 2.000440]\n",
      "epoch:22 step:20999 [D loss: 0.674204, acc: 57.03%] [G loss: 1.955134]\n",
      "epoch:22 step:21000 [D loss: 0.661464, acc: 57.81%] [G loss: 1.763496]\n",
      "epoch:22 step:21001 [D loss: 0.644307, acc: 61.72%] [G loss: 1.833107]\n",
      "epoch:22 step:21002 [D loss: 0.626082, acc: 62.50%] [G loss: 1.940292]\n",
      "epoch:22 step:21003 [D loss: 0.635461, acc: 67.97%] [G loss: 1.908377]\n",
      "epoch:22 step:21004 [D loss: 0.695219, acc: 55.47%] [G loss: 1.885977]\n",
      "epoch:22 step:21005 [D loss: 0.694698, acc: 54.69%] [G loss: 1.794073]\n",
      "epoch:22 step:21006 [D loss: 0.673249, acc: 58.59%] [G loss: 1.866129]\n",
      "epoch:22 step:21007 [D loss: 0.640706, acc: 68.75%] [G loss: 1.857600]\n",
      "epoch:22 step:21008 [D loss: 0.679857, acc: 57.81%] [G loss: 1.826665]\n",
      "epoch:22 step:21009 [D loss: 0.636950, acc: 63.28%] [G loss: 1.959538]\n",
      "epoch:22 step:21010 [D loss: 0.655153, acc: 60.16%] [G loss: 1.817713]\n",
      "epoch:22 step:21011 [D loss: 0.657705, acc: 56.25%] [G loss: 1.811074]\n",
      "epoch:22 step:21012 [D loss: 0.674723, acc: 59.38%] [G loss: 1.779864]\n",
      "epoch:22 step:21013 [D loss: 0.651842, acc: 64.06%] [G loss: 1.837654]\n",
      "epoch:22 step:21014 [D loss: 0.618000, acc: 62.50%] [G loss: 1.926146]\n",
      "epoch:22 step:21015 [D loss: 0.629411, acc: 63.28%] [G loss: 1.914952]\n",
      "epoch:22 step:21016 [D loss: 0.649542, acc: 64.06%] [G loss: 1.948704]\n",
      "epoch:22 step:21017 [D loss: 0.693879, acc: 55.47%] [G loss: 1.886826]\n",
      "epoch:22 step:21018 [D loss: 0.620068, acc: 62.50%] [G loss: 1.878973]\n",
      "epoch:22 step:21019 [D loss: 0.661252, acc: 59.38%] [G loss: 1.907424]\n",
      "epoch:22 step:21020 [D loss: 0.594570, acc: 66.41%] [G loss: 2.101004]\n",
      "epoch:22 step:21021 [D loss: 0.694188, acc: 59.38%] [G loss: 1.807910]\n",
      "epoch:22 step:21022 [D loss: 0.677801, acc: 55.47%] [G loss: 1.846372]\n",
      "epoch:22 step:21023 [D loss: 0.638288, acc: 62.50%] [G loss: 1.882248]\n",
      "epoch:22 step:21024 [D loss: 0.628057, acc: 68.75%] [G loss: 1.837678]\n",
      "epoch:22 step:21025 [D loss: 0.649693, acc: 66.41%] [G loss: 1.754618]\n",
      "epoch:22 step:21026 [D loss: 0.629792, acc: 63.28%] [G loss: 1.815376]\n",
      "epoch:22 step:21027 [D loss: 0.600752, acc: 66.41%] [G loss: 1.938937]\n",
      "epoch:22 step:21028 [D loss: 0.614596, acc: 68.75%] [G loss: 1.960509]\n",
      "epoch:22 step:21029 [D loss: 0.591536, acc: 69.53%] [G loss: 2.079233]\n",
      "epoch:22 step:21030 [D loss: 0.572092, acc: 71.09%] [G loss: 2.155103]\n",
      "epoch:22 step:21031 [D loss: 0.631107, acc: 72.66%] [G loss: 1.873348]\n",
      "epoch:22 step:21032 [D loss: 0.688761, acc: 57.81%] [G loss: 1.854901]\n",
      "epoch:22 step:21033 [D loss: 0.691394, acc: 61.72%] [G loss: 1.971535]\n",
      "epoch:22 step:21034 [D loss: 0.688711, acc: 56.25%] [G loss: 1.817602]\n",
      "epoch:22 step:21035 [D loss: 0.664347, acc: 57.81%] [G loss: 1.933787]\n",
      "epoch:22 step:21036 [D loss: 0.687123, acc: 61.72%] [G loss: 1.816493]\n",
      "epoch:22 step:21037 [D loss: 0.671399, acc: 56.25%] [G loss: 1.752871]\n",
      "epoch:22 step:21038 [D loss: 0.671553, acc: 56.25%] [G loss: 1.754747]\n",
      "epoch:22 step:21039 [D loss: 0.689298, acc: 53.91%] [G loss: 1.721158]\n",
      "epoch:22 step:21040 [D loss: 0.615835, acc: 67.97%] [G loss: 1.778246]\n",
      "epoch:22 step:21041 [D loss: 0.615428, acc: 69.53%] [G loss: 2.069881]\n",
      "epoch:22 step:21042 [D loss: 0.568692, acc: 68.75%] [G loss: 2.207404]\n",
      "epoch:22 step:21043 [D loss: 0.605268, acc: 67.19%] [G loss: 2.061957]\n",
      "epoch:22 step:21044 [D loss: 0.558027, acc: 73.44%] [G loss: 2.135879]\n",
      "epoch:22 step:21045 [D loss: 0.579918, acc: 67.19%] [G loss: 2.089527]\n",
      "epoch:22 step:21046 [D loss: 0.664271, acc: 60.16%] [G loss: 1.819753]\n",
      "epoch:22 step:21047 [D loss: 0.675407, acc: 60.16%] [G loss: 1.984367]\n",
      "epoch:22 step:21048 [D loss: 0.604261, acc: 68.75%] [G loss: 1.954550]\n",
      "epoch:22 step:21049 [D loss: 0.636891, acc: 62.50%] [G loss: 1.956460]\n",
      "epoch:22 step:21050 [D loss: 0.665264, acc: 60.16%] [G loss: 2.018235]\n",
      "epoch:22 step:21051 [D loss: 0.724613, acc: 53.91%] [G loss: 1.687072]\n",
      "epoch:22 step:21052 [D loss: 0.622971, acc: 65.62%] [G loss: 1.773890]\n",
      "epoch:22 step:21053 [D loss: 0.661842, acc: 59.38%] [G loss: 1.788618]\n",
      "epoch:22 step:21054 [D loss: 0.647166, acc: 63.28%] [G loss: 1.753178]\n",
      "epoch:22 step:21055 [D loss: 0.636622, acc: 60.94%] [G loss: 2.001627]\n",
      "epoch:22 step:21056 [D loss: 0.702242, acc: 61.72%] [G loss: 1.834023]\n",
      "epoch:22 step:21057 [D loss: 0.805819, acc: 54.69%] [G loss: 1.917424]\n",
      "epoch:22 step:21058 [D loss: 0.687283, acc: 53.12%] [G loss: 1.734105]\n",
      "epoch:22 step:21059 [D loss: 0.642116, acc: 65.62%] [G loss: 1.750404]\n",
      "epoch:22 step:21060 [D loss: 0.666247, acc: 57.81%] [G loss: 1.706115]\n",
      "epoch:22 step:21061 [D loss: 0.623307, acc: 65.62%] [G loss: 1.778214]\n",
      "epoch:22 step:21062 [D loss: 0.647863, acc: 60.94%] [G loss: 1.625782]\n",
      "epoch:22 step:21063 [D loss: 0.622216, acc: 68.75%] [G loss: 1.867336]\n",
      "epoch:22 step:21064 [D loss: 0.658505, acc: 60.16%] [G loss: 1.818495]\n",
      "epoch:22 step:21065 [D loss: 0.638551, acc: 60.16%] [G loss: 1.809850]\n",
      "epoch:22 step:21066 [D loss: 0.679639, acc: 61.72%] [G loss: 1.853728]\n",
      "epoch:22 step:21067 [D loss: 0.672978, acc: 61.72%] [G loss: 1.796696]\n",
      "epoch:22 step:21068 [D loss: 0.661329, acc: 57.03%] [G loss: 1.833924]\n",
      "epoch:22 step:21069 [D loss: 0.617700, acc: 66.41%] [G loss: 1.969884]\n",
      "epoch:22 step:21070 [D loss: 0.655809, acc: 63.28%] [G loss: 1.901192]\n",
      "epoch:22 step:21071 [D loss: 0.645452, acc: 64.84%] [G loss: 1.971946]\n",
      "epoch:22 step:21072 [D loss: 0.708177, acc: 51.56%] [G loss: 1.776882]\n",
      "epoch:22 step:21073 [D loss: 0.689585, acc: 58.59%] [G loss: 1.751532]\n",
      "epoch:22 step:21074 [D loss: 0.698904, acc: 55.47%] [G loss: 1.745245]\n",
      "epoch:22 step:21075 [D loss: 0.652901, acc: 60.16%] [G loss: 1.752178]\n",
      "epoch:22 step:21076 [D loss: 0.657671, acc: 60.16%] [G loss: 1.849827]\n",
      "epoch:22 step:21077 [D loss: 0.667137, acc: 62.50%] [G loss: 1.886883]\n",
      "epoch:22 step:21078 [D loss: 0.604801, acc: 65.62%] [G loss: 1.755611]\n",
      "epoch:22 step:21079 [D loss: 0.664653, acc: 63.28%] [G loss: 1.808752]\n",
      "epoch:22 step:21080 [D loss: 0.694456, acc: 52.34%] [G loss: 1.791410]\n",
      "epoch:22 step:21081 [D loss: 0.646447, acc: 65.62%] [G loss: 1.871989]\n",
      "epoch:22 step:21082 [D loss: 0.631288, acc: 64.84%] [G loss: 1.960990]\n",
      "epoch:22 step:21083 [D loss: 0.637543, acc: 67.97%] [G loss: 1.932056]\n",
      "epoch:22 step:21084 [D loss: 0.636213, acc: 62.50%] [G loss: 2.002561]\n",
      "epoch:22 step:21085 [D loss: 0.572390, acc: 77.34%] [G loss: 2.087564]\n",
      "epoch:22 step:21086 [D loss: 0.631053, acc: 64.84%] [G loss: 2.161205]\n",
      "epoch:22 step:21087 [D loss: 0.714459, acc: 55.47%] [G loss: 1.964003]\n",
      "epoch:22 step:21088 [D loss: 0.631576, acc: 62.50%] [G loss: 1.876420]\n",
      "epoch:22 step:21089 [D loss: 0.678077, acc: 60.94%] [G loss: 1.789396]\n",
      "epoch:22 step:21090 [D loss: 0.673716, acc: 60.94%] [G loss: 1.801095]\n",
      "epoch:22 step:21091 [D loss: 0.703692, acc: 57.03%] [G loss: 1.716714]\n",
      "epoch:22 step:21092 [D loss: 0.722315, acc: 48.44%] [G loss: 1.783030]\n",
      "epoch:22 step:21093 [D loss: 0.636614, acc: 66.41%] [G loss: 1.909729]\n",
      "epoch:22 step:21094 [D loss: 0.652769, acc: 61.72%] [G loss: 1.967276]\n",
      "epoch:22 step:21095 [D loss: 0.586664, acc: 70.31%] [G loss: 1.928765]\n",
      "epoch:22 step:21096 [D loss: 0.643786, acc: 61.72%] [G loss: 1.707578]\n",
      "epoch:22 step:21097 [D loss: 0.634354, acc: 62.50%] [G loss: 1.860664]\n",
      "epoch:22 step:21098 [D loss: 0.615609, acc: 62.50%] [G loss: 1.992956]\n",
      "epoch:22 step:21099 [D loss: 0.636679, acc: 63.28%] [G loss: 1.867649]\n",
      "epoch:22 step:21100 [D loss: 0.708989, acc: 55.47%] [G loss: 1.839910]\n",
      "epoch:22 step:21101 [D loss: 0.606116, acc: 66.41%] [G loss: 1.944453]\n",
      "epoch:22 step:21102 [D loss: 0.607136, acc: 69.53%] [G loss: 2.085891]\n",
      "epoch:22 step:21103 [D loss: 0.676398, acc: 56.25%] [G loss: 1.887883]\n",
      "epoch:22 step:21104 [D loss: 0.667383, acc: 57.81%] [G loss: 1.875845]\n",
      "epoch:22 step:21105 [D loss: 0.637703, acc: 60.16%] [G loss: 1.849058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21106 [D loss: 0.683493, acc: 59.38%] [G loss: 1.743520]\n",
      "epoch:22 step:21107 [D loss: 0.641868, acc: 69.53%] [G loss: 1.827925]\n",
      "epoch:22 step:21108 [D loss: 0.614511, acc: 70.31%] [G loss: 1.988332]\n",
      "epoch:22 step:21109 [D loss: 0.635228, acc: 63.28%] [G loss: 1.922562]\n",
      "epoch:22 step:21110 [D loss: 0.655909, acc: 61.72%] [G loss: 1.883253]\n",
      "epoch:22 step:21111 [D loss: 0.640915, acc: 60.16%] [G loss: 1.788486]\n",
      "epoch:22 step:21112 [D loss: 0.630251, acc: 64.84%] [G loss: 1.950805]\n",
      "epoch:22 step:21113 [D loss: 0.580796, acc: 73.44%] [G loss: 2.063261]\n",
      "epoch:22 step:21114 [D loss: 0.700014, acc: 50.78%] [G loss: 1.724207]\n",
      "epoch:22 step:21115 [D loss: 0.688649, acc: 56.25%] [G loss: 1.658322]\n",
      "epoch:22 step:21116 [D loss: 0.695719, acc: 55.47%] [G loss: 1.749416]\n",
      "epoch:22 step:21117 [D loss: 0.643011, acc: 61.72%] [G loss: 1.914630]\n",
      "epoch:22 step:21118 [D loss: 0.599991, acc: 74.22%] [G loss: 1.950124]\n",
      "epoch:22 step:21119 [D loss: 0.623981, acc: 66.41%] [G loss: 1.990912]\n",
      "epoch:22 step:21120 [D loss: 0.628735, acc: 64.84%] [G loss: 1.788817]\n",
      "epoch:22 step:21121 [D loss: 0.685707, acc: 59.38%] [G loss: 1.784003]\n",
      "epoch:22 step:21122 [D loss: 0.548986, acc: 73.44%] [G loss: 2.043816]\n",
      "epoch:22 step:21123 [D loss: 0.627584, acc: 66.41%] [G loss: 2.029740]\n",
      "epoch:22 step:21124 [D loss: 0.658976, acc: 60.94%] [G loss: 1.767027]\n",
      "epoch:22 step:21125 [D loss: 0.697808, acc: 52.34%] [G loss: 1.753256]\n",
      "epoch:22 step:21126 [D loss: 0.714018, acc: 54.69%] [G loss: 1.818900]\n",
      "epoch:22 step:21127 [D loss: 0.681663, acc: 56.25%] [G loss: 1.875594]\n",
      "epoch:22 step:21128 [D loss: 0.611117, acc: 65.62%] [G loss: 1.916163]\n",
      "epoch:22 step:21129 [D loss: 0.619135, acc: 63.28%] [G loss: 1.906075]\n",
      "epoch:22 step:21130 [D loss: 0.694687, acc: 59.38%] [G loss: 1.960797]\n",
      "epoch:22 step:21131 [D loss: 0.635827, acc: 64.06%] [G loss: 1.938545]\n",
      "epoch:22 step:21132 [D loss: 0.694313, acc: 57.81%] [G loss: 1.784976]\n",
      "epoch:22 step:21133 [D loss: 0.632736, acc: 63.28%] [G loss: 1.821041]\n",
      "epoch:22 step:21134 [D loss: 0.631016, acc: 62.50%] [G loss: 1.942585]\n",
      "epoch:22 step:21135 [D loss: 0.603295, acc: 64.06%] [G loss: 1.983586]\n",
      "epoch:22 step:21136 [D loss: 0.646363, acc: 64.84%] [G loss: 1.980725]\n",
      "epoch:22 step:21137 [D loss: 0.597189, acc: 62.50%] [G loss: 2.019822]\n",
      "epoch:22 step:21138 [D loss: 0.633643, acc: 58.59%] [G loss: 2.020092]\n",
      "epoch:22 step:21139 [D loss: 0.668311, acc: 60.94%] [G loss: 1.945031]\n",
      "epoch:22 step:21140 [D loss: 0.664340, acc: 64.06%] [G loss: 1.868640]\n",
      "epoch:22 step:21141 [D loss: 0.701830, acc: 54.69%] [G loss: 1.965604]\n",
      "epoch:22 step:21142 [D loss: 0.671313, acc: 58.59%] [G loss: 1.795185]\n",
      "epoch:22 step:21143 [D loss: 0.713885, acc: 53.12%] [G loss: 1.666198]\n",
      "epoch:22 step:21144 [D loss: 0.701562, acc: 50.78%] [G loss: 1.703430]\n",
      "epoch:22 step:21145 [D loss: 0.599557, acc: 67.19%] [G loss: 1.823146]\n",
      "epoch:22 step:21146 [D loss: 0.637015, acc: 64.06%] [G loss: 1.860689]\n",
      "epoch:22 step:21147 [D loss: 0.629745, acc: 63.28%] [G loss: 1.967194]\n",
      "epoch:22 step:21148 [D loss: 0.609141, acc: 69.53%] [G loss: 2.002135]\n",
      "epoch:22 step:21149 [D loss: 0.615766, acc: 67.19%] [G loss: 1.695857]\n",
      "epoch:22 step:21150 [D loss: 0.624829, acc: 63.28%] [G loss: 1.887374]\n",
      "epoch:22 step:21151 [D loss: 0.694260, acc: 60.94%] [G loss: 1.782718]\n",
      "epoch:22 step:21152 [D loss: 0.662998, acc: 57.03%] [G loss: 1.779115]\n",
      "epoch:22 step:21153 [D loss: 0.643815, acc: 61.72%] [G loss: 1.873918]\n",
      "epoch:22 step:21154 [D loss: 0.664245, acc: 61.72%] [G loss: 1.819493]\n",
      "epoch:22 step:21155 [D loss: 0.646020, acc: 65.62%] [G loss: 1.827726]\n",
      "epoch:22 step:21156 [D loss: 0.674127, acc: 60.16%] [G loss: 1.866478]\n",
      "epoch:22 step:21157 [D loss: 0.701076, acc: 57.03%] [G loss: 1.745914]\n",
      "epoch:22 step:21158 [D loss: 0.600414, acc: 66.41%] [G loss: 1.956445]\n",
      "epoch:22 step:21159 [D loss: 0.646402, acc: 66.41%] [G loss: 1.837164]\n",
      "epoch:22 step:21160 [D loss: 0.697947, acc: 59.38%] [G loss: 1.942304]\n",
      "epoch:22 step:21161 [D loss: 0.663725, acc: 56.25%] [G loss: 1.833914]\n",
      "epoch:22 step:21162 [D loss: 0.627022, acc: 64.84%] [G loss: 1.894579]\n",
      "epoch:22 step:21163 [D loss: 0.653338, acc: 64.06%] [G loss: 1.885046]\n",
      "epoch:22 step:21164 [D loss: 0.594812, acc: 70.31%] [G loss: 2.061158]\n",
      "epoch:22 step:21165 [D loss: 0.637153, acc: 60.16%] [G loss: 1.909970]\n",
      "epoch:22 step:21166 [D loss: 0.610005, acc: 65.62%] [G loss: 1.837981]\n",
      "epoch:22 step:21167 [D loss: 0.620241, acc: 62.50%] [G loss: 1.894579]\n",
      "epoch:22 step:21168 [D loss: 0.599279, acc: 71.09%] [G loss: 2.225050]\n",
      "epoch:22 step:21169 [D loss: 0.639767, acc: 63.28%] [G loss: 1.836429]\n",
      "epoch:22 step:21170 [D loss: 0.606967, acc: 69.53%] [G loss: 2.112396]\n",
      "epoch:22 step:21171 [D loss: 0.623010, acc: 65.62%] [G loss: 2.036572]\n",
      "epoch:22 step:21172 [D loss: 0.642331, acc: 60.16%] [G loss: 2.066517]\n",
      "epoch:22 step:21173 [D loss: 0.709914, acc: 56.25%] [G loss: 1.667804]\n",
      "epoch:22 step:21174 [D loss: 0.659200, acc: 59.38%] [G loss: 1.861675]\n",
      "epoch:22 step:21175 [D loss: 0.671451, acc: 53.91%] [G loss: 1.817842]\n",
      "epoch:22 step:21176 [D loss: 0.632851, acc: 63.28%] [G loss: 1.960797]\n",
      "epoch:22 step:21177 [D loss: 0.593632, acc: 67.19%] [G loss: 1.993155]\n",
      "epoch:22 step:21178 [D loss: 0.663579, acc: 60.16%] [G loss: 2.054698]\n",
      "epoch:22 step:21179 [D loss: 0.629144, acc: 67.97%] [G loss: 2.003154]\n",
      "epoch:22 step:21180 [D loss: 0.745146, acc: 46.09%] [G loss: 1.683577]\n",
      "epoch:22 step:21181 [D loss: 0.667003, acc: 58.59%] [G loss: 1.844068]\n",
      "epoch:22 step:21182 [D loss: 0.665548, acc: 61.72%] [G loss: 1.815379]\n",
      "epoch:22 step:21183 [D loss: 0.690749, acc: 54.69%] [G loss: 1.793652]\n",
      "epoch:22 step:21184 [D loss: 0.640432, acc: 60.94%] [G loss: 1.727429]\n",
      "epoch:22 step:21185 [D loss: 0.612952, acc: 68.75%] [G loss: 1.807855]\n",
      "epoch:22 step:21186 [D loss: 0.668390, acc: 60.16%] [G loss: 1.857493]\n",
      "epoch:22 step:21187 [D loss: 0.668418, acc: 57.03%] [G loss: 1.869317]\n",
      "epoch:22 step:21188 [D loss: 0.648269, acc: 64.06%] [G loss: 1.776045]\n",
      "epoch:22 step:21189 [D loss: 0.641046, acc: 60.16%] [G loss: 1.843262]\n",
      "epoch:22 step:21190 [D loss: 0.667238, acc: 64.84%] [G loss: 1.707361]\n",
      "epoch:22 step:21191 [D loss: 0.629209, acc: 64.06%] [G loss: 1.800771]\n",
      "epoch:22 step:21192 [D loss: 0.635755, acc: 68.75%] [G loss: 1.750234]\n",
      "epoch:22 step:21193 [D loss: 0.643145, acc: 59.38%] [G loss: 1.821841]\n",
      "epoch:22 step:21194 [D loss: 0.622406, acc: 65.62%] [G loss: 1.899119]\n",
      "epoch:22 step:21195 [D loss: 0.639503, acc: 62.50%] [G loss: 1.833988]\n",
      "epoch:22 step:21196 [D loss: 0.586146, acc: 74.22%] [G loss: 1.871898]\n",
      "epoch:22 step:21197 [D loss: 0.670251, acc: 64.84%] [G loss: 1.881922]\n",
      "epoch:22 step:21198 [D loss: 0.651627, acc: 57.81%] [G loss: 1.830315]\n",
      "epoch:22 step:21199 [D loss: 0.610949, acc: 69.53%] [G loss: 1.811414]\n",
      "epoch:22 step:21200 [D loss: 0.644044, acc: 61.72%] [G loss: 1.788728]\n",
      "epoch:22 step:21201 [D loss: 0.650126, acc: 60.94%] [G loss: 1.852715]\n",
      "epoch:22 step:21202 [D loss: 0.609667, acc: 68.75%] [G loss: 1.986180]\n",
      "epoch:22 step:21203 [D loss: 0.686399, acc: 55.47%] [G loss: 1.953242]\n",
      "epoch:22 step:21204 [D loss: 0.604736, acc: 64.06%] [G loss: 1.842903]\n",
      "epoch:22 step:21205 [D loss: 0.652339, acc: 57.81%] [G loss: 1.749513]\n",
      "epoch:22 step:21206 [D loss: 0.632258, acc: 61.72%] [G loss: 1.911250]\n",
      "epoch:22 step:21207 [D loss: 0.624219, acc: 67.97%] [G loss: 1.839405]\n",
      "epoch:22 step:21208 [D loss: 0.667529, acc: 61.72%] [G loss: 1.956112]\n",
      "epoch:22 step:21209 [D loss: 0.658863, acc: 61.72%] [G loss: 1.768205]\n",
      "epoch:22 step:21210 [D loss: 0.660473, acc: 60.16%] [G loss: 1.818320]\n",
      "epoch:22 step:21211 [D loss: 0.715317, acc: 53.91%] [G loss: 1.839252]\n",
      "epoch:22 step:21212 [D loss: 0.622656, acc: 71.09%] [G loss: 1.869490]\n",
      "epoch:22 step:21213 [D loss: 0.684938, acc: 57.81%] [G loss: 1.838683]\n",
      "epoch:22 step:21214 [D loss: 0.621764, acc: 67.19%] [G loss: 1.918292]\n",
      "epoch:22 step:21215 [D loss: 0.674866, acc: 59.38%] [G loss: 1.930924]\n",
      "epoch:22 step:21216 [D loss: 0.699392, acc: 60.16%] [G loss: 1.917883]\n",
      "epoch:22 step:21217 [D loss: 0.677689, acc: 57.03%] [G loss: 1.776339]\n",
      "epoch:22 step:21218 [D loss: 0.690464, acc: 58.59%] [G loss: 1.798845]\n",
      "epoch:22 step:21219 [D loss: 0.678265, acc: 57.03%] [G loss: 1.926407]\n",
      "epoch:22 step:21220 [D loss: 0.643258, acc: 63.28%] [G loss: 1.813689]\n",
      "epoch:22 step:21221 [D loss: 0.610740, acc: 66.41%] [G loss: 1.860459]\n",
      "epoch:22 step:21222 [D loss: 0.661608, acc: 58.59%] [G loss: 1.857824]\n",
      "epoch:22 step:21223 [D loss: 0.668905, acc: 60.94%] [G loss: 2.026832]\n",
      "epoch:22 step:21224 [D loss: 0.577078, acc: 73.44%] [G loss: 1.917424]\n",
      "epoch:22 step:21225 [D loss: 0.657082, acc: 57.81%] [G loss: 1.823168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21226 [D loss: 0.644079, acc: 57.03%] [G loss: 1.805863]\n",
      "epoch:22 step:21227 [D loss: 0.599647, acc: 71.88%] [G loss: 1.827334]\n",
      "epoch:22 step:21228 [D loss: 0.694072, acc: 58.59%] [G loss: 1.797341]\n",
      "epoch:22 step:21229 [D loss: 0.672452, acc: 56.25%] [G loss: 1.731738]\n",
      "epoch:22 step:21230 [D loss: 0.639520, acc: 66.41%] [G loss: 1.822675]\n",
      "epoch:22 step:21231 [D loss: 0.689153, acc: 53.91%] [G loss: 1.772494]\n",
      "epoch:22 step:21232 [D loss: 0.646410, acc: 57.03%] [G loss: 1.900524]\n",
      "epoch:22 step:21233 [D loss: 0.623664, acc: 64.84%] [G loss: 1.796837]\n",
      "epoch:22 step:21234 [D loss: 0.662046, acc: 61.72%] [G loss: 1.794954]\n",
      "epoch:22 step:21235 [D loss: 0.640696, acc: 67.97%] [G loss: 1.864221]\n",
      "epoch:22 step:21236 [D loss: 0.644817, acc: 64.84%] [G loss: 1.996687]\n",
      "epoch:22 step:21237 [D loss: 0.691194, acc: 62.50%] [G loss: 1.924666]\n",
      "epoch:22 step:21238 [D loss: 0.638823, acc: 60.94%] [G loss: 2.081948]\n",
      "epoch:22 step:21239 [D loss: 0.669237, acc: 57.03%] [G loss: 1.787917]\n",
      "epoch:22 step:21240 [D loss: 0.661595, acc: 59.38%] [G loss: 1.845225]\n",
      "epoch:22 step:21241 [D loss: 0.674538, acc: 57.81%] [G loss: 1.809794]\n",
      "epoch:22 step:21242 [D loss: 0.673877, acc: 57.81%] [G loss: 1.775893]\n",
      "epoch:22 step:21243 [D loss: 0.662037, acc: 60.16%] [G loss: 1.867436]\n",
      "epoch:22 step:21244 [D loss: 0.623045, acc: 64.06%] [G loss: 1.805842]\n",
      "epoch:22 step:21245 [D loss: 0.696962, acc: 59.38%] [G loss: 1.841754]\n",
      "epoch:22 step:21246 [D loss: 0.616873, acc: 66.41%] [G loss: 1.954061]\n",
      "epoch:22 step:21247 [D loss: 0.630099, acc: 63.28%] [G loss: 1.774134]\n",
      "epoch:22 step:21248 [D loss: 0.613550, acc: 67.19%] [G loss: 1.978490]\n",
      "epoch:22 step:21249 [D loss: 0.632109, acc: 65.62%] [G loss: 1.897769]\n",
      "epoch:22 step:21250 [D loss: 0.647772, acc: 61.72%] [G loss: 2.016011]\n",
      "epoch:22 step:21251 [D loss: 0.620952, acc: 67.19%] [G loss: 2.018992]\n",
      "epoch:22 step:21252 [D loss: 0.610705, acc: 69.53%] [G loss: 1.865541]\n",
      "epoch:22 step:21253 [D loss: 0.630248, acc: 62.50%] [G loss: 1.856637]\n",
      "epoch:22 step:21254 [D loss: 0.675065, acc: 51.56%] [G loss: 1.850796]\n",
      "epoch:22 step:21255 [D loss: 0.622903, acc: 67.97%] [G loss: 1.969393]\n",
      "epoch:22 step:21256 [D loss: 0.632574, acc: 64.06%] [G loss: 2.036227]\n",
      "epoch:22 step:21257 [D loss: 0.622171, acc: 66.41%] [G loss: 1.894004]\n",
      "epoch:22 step:21258 [D loss: 0.639048, acc: 64.84%] [G loss: 1.923269]\n",
      "epoch:22 step:21259 [D loss: 0.607528, acc: 57.81%] [G loss: 1.949611]\n",
      "epoch:22 step:21260 [D loss: 0.596930, acc: 71.09%] [G loss: 2.105298]\n",
      "epoch:22 step:21261 [D loss: 0.676114, acc: 58.59%] [G loss: 2.023790]\n",
      "epoch:22 step:21262 [D loss: 0.572163, acc: 70.31%] [G loss: 2.294584]\n",
      "epoch:22 step:21263 [D loss: 0.618324, acc: 64.06%] [G loss: 2.288948]\n",
      "epoch:22 step:21264 [D loss: 0.698432, acc: 59.38%] [G loss: 1.965778]\n",
      "epoch:22 step:21265 [D loss: 0.639266, acc: 61.72%] [G loss: 1.819051]\n",
      "epoch:22 step:21266 [D loss: 0.621068, acc: 64.06%] [G loss: 1.977671]\n",
      "epoch:22 step:21267 [D loss: 0.629687, acc: 66.41%] [G loss: 2.141156]\n",
      "epoch:22 step:21268 [D loss: 0.578964, acc: 69.53%] [G loss: 2.021224]\n",
      "epoch:22 step:21269 [D loss: 0.652262, acc: 58.59%] [G loss: 1.885838]\n",
      "epoch:22 step:21270 [D loss: 0.631252, acc: 62.50%] [G loss: 1.923022]\n",
      "epoch:22 step:21271 [D loss: 0.680170, acc: 64.06%] [G loss: 1.816201]\n",
      "epoch:22 step:21272 [D loss: 0.666679, acc: 58.59%] [G loss: 1.798736]\n",
      "epoch:22 step:21273 [D loss: 0.652241, acc: 56.25%] [G loss: 1.909469]\n",
      "epoch:22 step:21274 [D loss: 0.630749, acc: 67.19%] [G loss: 1.787296]\n",
      "epoch:22 step:21275 [D loss: 0.630402, acc: 67.19%] [G loss: 1.879894]\n",
      "epoch:22 step:21276 [D loss: 0.648778, acc: 66.41%] [G loss: 1.998155]\n",
      "epoch:22 step:21277 [D loss: 0.653036, acc: 63.28%] [G loss: 1.832096]\n",
      "epoch:22 step:21278 [D loss: 0.649435, acc: 65.62%] [G loss: 1.767046]\n",
      "epoch:22 step:21279 [D loss: 0.672291, acc: 53.91%] [G loss: 1.875085]\n",
      "epoch:22 step:21280 [D loss: 0.649728, acc: 61.72%] [G loss: 1.861531]\n",
      "epoch:22 step:21281 [D loss: 0.650463, acc: 62.50%] [G loss: 1.826983]\n",
      "epoch:22 step:21282 [D loss: 0.691217, acc: 59.38%] [G loss: 1.875586]\n",
      "epoch:22 step:21283 [D loss: 0.637566, acc: 61.72%] [G loss: 1.801540]\n",
      "epoch:22 step:21284 [D loss: 0.623521, acc: 57.81%] [G loss: 1.885327]\n",
      "epoch:22 step:21285 [D loss: 0.680199, acc: 58.59%] [G loss: 1.969928]\n",
      "epoch:22 step:21286 [D loss: 0.627264, acc: 65.62%] [G loss: 1.768831]\n",
      "epoch:22 step:21287 [D loss: 0.672448, acc: 57.03%] [G loss: 1.811218]\n",
      "epoch:22 step:21288 [D loss: 0.637826, acc: 66.41%] [G loss: 1.816861]\n",
      "epoch:22 step:21289 [D loss: 0.667906, acc: 59.38%] [G loss: 1.726664]\n",
      "epoch:22 step:21290 [D loss: 0.668386, acc: 58.59%] [G loss: 1.821147]\n",
      "epoch:22 step:21291 [D loss: 0.658469, acc: 61.72%] [G loss: 1.972885]\n",
      "epoch:22 step:21292 [D loss: 0.615174, acc: 67.19%] [G loss: 2.014600]\n",
      "epoch:22 step:21293 [D loss: 0.619499, acc: 67.19%] [G loss: 1.875074]\n",
      "epoch:22 step:21294 [D loss: 0.633973, acc: 63.28%] [G loss: 1.853172]\n",
      "epoch:22 step:21295 [D loss: 0.628292, acc: 64.06%] [G loss: 1.853861]\n",
      "epoch:22 step:21296 [D loss: 0.689446, acc: 57.81%] [G loss: 1.904929]\n",
      "epoch:22 step:21297 [D loss: 0.622771, acc: 65.62%] [G loss: 1.932280]\n",
      "epoch:22 step:21298 [D loss: 0.663077, acc: 65.62%] [G loss: 1.808325]\n",
      "epoch:22 step:21299 [D loss: 0.636685, acc: 67.19%] [G loss: 1.831306]\n",
      "epoch:22 step:21300 [D loss: 0.663735, acc: 58.59%] [G loss: 1.746749]\n",
      "epoch:22 step:21301 [D loss: 0.701231, acc: 55.47%] [G loss: 1.808137]\n",
      "epoch:22 step:21302 [D loss: 0.634345, acc: 63.28%] [G loss: 1.928811]\n",
      "epoch:22 step:21303 [D loss: 0.632985, acc: 62.50%] [G loss: 2.015092]\n",
      "epoch:22 step:21304 [D loss: 0.672823, acc: 60.94%] [G loss: 1.990370]\n",
      "epoch:22 step:21305 [D loss: 0.657210, acc: 64.84%] [G loss: 2.034203]\n",
      "epoch:22 step:21306 [D loss: 0.622982, acc: 67.19%] [G loss: 1.848397]\n",
      "epoch:22 step:21307 [D loss: 0.625605, acc: 62.50%] [G loss: 1.981448]\n",
      "epoch:22 step:21308 [D loss: 0.631846, acc: 60.94%] [G loss: 2.101881]\n",
      "epoch:22 step:21309 [D loss: 0.681415, acc: 58.59%] [G loss: 1.892694]\n",
      "epoch:22 step:21310 [D loss: 0.641769, acc: 60.94%] [G loss: 1.790343]\n",
      "epoch:22 step:21311 [D loss: 0.631067, acc: 64.84%] [G loss: 1.893335]\n",
      "epoch:22 step:21312 [D loss: 0.651048, acc: 59.38%] [G loss: 1.836333]\n",
      "epoch:22 step:21313 [D loss: 0.637415, acc: 62.50%] [G loss: 1.979847]\n",
      "epoch:22 step:21314 [D loss: 0.652780, acc: 59.38%] [G loss: 1.927515]\n",
      "epoch:22 step:21315 [D loss: 0.617122, acc: 67.19%] [G loss: 1.894664]\n",
      "epoch:22 step:21316 [D loss: 0.614763, acc: 67.19%] [G loss: 1.869780]\n",
      "epoch:22 step:21317 [D loss: 0.694207, acc: 59.38%] [G loss: 1.679862]\n",
      "epoch:22 step:21318 [D loss: 0.705115, acc: 52.34%] [G loss: 1.762911]\n",
      "epoch:22 step:21319 [D loss: 0.662112, acc: 60.16%] [G loss: 1.777202]\n",
      "epoch:22 step:21320 [D loss: 0.616688, acc: 64.84%] [G loss: 1.919424]\n",
      "epoch:22 step:21321 [D loss: 0.681936, acc: 58.59%] [G loss: 1.854046]\n",
      "epoch:22 step:21322 [D loss: 0.650988, acc: 62.50%] [G loss: 1.917942]\n",
      "epoch:22 step:21323 [D loss: 0.562106, acc: 74.22%] [G loss: 2.015541]\n",
      "epoch:22 step:21324 [D loss: 0.674300, acc: 57.03%] [G loss: 1.823244]\n",
      "epoch:22 step:21325 [D loss: 0.601403, acc: 67.19%] [G loss: 2.132654]\n",
      "epoch:22 step:21326 [D loss: 0.653470, acc: 63.28%] [G loss: 1.953637]\n",
      "epoch:22 step:21327 [D loss: 0.625613, acc: 70.31%] [G loss: 1.903421]\n",
      "epoch:22 step:21328 [D loss: 0.632672, acc: 65.62%] [G loss: 1.935266]\n",
      "epoch:22 step:21329 [D loss: 0.670054, acc: 58.59%] [G loss: 1.905451]\n",
      "epoch:22 step:21330 [D loss: 0.720000, acc: 52.34%] [G loss: 1.765789]\n",
      "epoch:22 step:21331 [D loss: 0.680920, acc: 58.59%] [G loss: 1.811490]\n",
      "epoch:22 step:21332 [D loss: 0.650335, acc: 63.28%] [G loss: 1.901745]\n",
      "epoch:22 step:21333 [D loss: 0.582233, acc: 68.75%] [G loss: 2.027149]\n",
      "epoch:22 step:21334 [D loss: 0.673469, acc: 55.47%] [G loss: 1.915033]\n",
      "epoch:22 step:21335 [D loss: 0.657501, acc: 62.50%] [G loss: 1.928926]\n",
      "epoch:22 step:21336 [D loss: 0.651268, acc: 60.16%] [G loss: 1.904644]\n",
      "epoch:22 step:21337 [D loss: 0.635547, acc: 64.84%] [G loss: 1.879429]\n",
      "epoch:22 step:21338 [D loss: 0.625364, acc: 63.28%] [G loss: 2.026773]\n",
      "epoch:22 step:21339 [D loss: 0.664841, acc: 62.50%] [G loss: 1.870814]\n",
      "epoch:22 step:21340 [D loss: 0.576892, acc: 72.66%] [G loss: 1.912933]\n",
      "epoch:22 step:21341 [D loss: 0.640254, acc: 63.28%] [G loss: 1.928503]\n",
      "epoch:22 step:21342 [D loss: 0.589994, acc: 69.53%] [G loss: 1.952162]\n",
      "epoch:22 step:21343 [D loss: 0.699923, acc: 61.72%] [G loss: 1.743093]\n",
      "epoch:22 step:21344 [D loss: 0.643588, acc: 60.94%] [G loss: 1.869956]\n",
      "epoch:22 step:21345 [D loss: 0.661728, acc: 58.59%] [G loss: 1.888665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21346 [D loss: 0.640750, acc: 61.72%] [G loss: 1.937708]\n",
      "epoch:22 step:21347 [D loss: 0.656136, acc: 64.84%] [G loss: 1.894230]\n",
      "epoch:22 step:21348 [D loss: 0.673344, acc: 53.12%] [G loss: 1.872914]\n",
      "epoch:22 step:21349 [D loss: 0.641475, acc: 63.28%] [G loss: 1.742274]\n",
      "epoch:22 step:21350 [D loss: 0.616743, acc: 66.41%] [G loss: 1.897147]\n",
      "epoch:22 step:21351 [D loss: 0.616318, acc: 61.72%] [G loss: 1.887830]\n",
      "epoch:22 step:21352 [D loss: 0.656337, acc: 62.50%] [G loss: 1.795006]\n",
      "epoch:22 step:21353 [D loss: 0.665063, acc: 61.72%] [G loss: 1.831911]\n",
      "epoch:22 step:21354 [D loss: 0.625184, acc: 67.19%] [G loss: 1.836039]\n",
      "epoch:22 step:21355 [D loss: 0.724234, acc: 50.78%] [G loss: 1.821263]\n",
      "epoch:22 step:21356 [D loss: 0.680276, acc: 57.81%] [G loss: 1.870874]\n",
      "epoch:22 step:21357 [D loss: 0.633000, acc: 66.41%] [G loss: 1.843703]\n",
      "epoch:22 step:21358 [D loss: 0.653843, acc: 64.06%] [G loss: 1.778940]\n",
      "epoch:22 step:21359 [D loss: 0.622144, acc: 66.41%] [G loss: 1.875115]\n",
      "epoch:22 step:21360 [D loss: 0.661274, acc: 58.59%] [G loss: 1.870066]\n",
      "epoch:22 step:21361 [D loss: 0.589428, acc: 71.09%] [G loss: 1.942372]\n",
      "epoch:22 step:21362 [D loss: 0.631434, acc: 67.97%] [G loss: 1.780204]\n",
      "epoch:22 step:21363 [D loss: 0.625823, acc: 57.81%] [G loss: 1.720510]\n",
      "epoch:22 step:21364 [D loss: 0.629993, acc: 64.84%] [G loss: 1.905271]\n",
      "epoch:22 step:21365 [D loss: 0.654202, acc: 60.16%] [G loss: 1.734366]\n",
      "epoch:22 step:21366 [D loss: 0.594666, acc: 67.97%] [G loss: 1.811858]\n",
      "epoch:22 step:21367 [D loss: 0.640304, acc: 63.28%] [G loss: 1.899148]\n",
      "epoch:22 step:21368 [D loss: 0.637928, acc: 67.97%] [G loss: 1.847182]\n",
      "epoch:22 step:21369 [D loss: 0.647680, acc: 62.50%] [G loss: 1.870286]\n",
      "epoch:22 step:21370 [D loss: 0.633286, acc: 67.19%] [G loss: 1.892309]\n",
      "epoch:22 step:21371 [D loss: 0.639260, acc: 68.75%] [G loss: 1.782464]\n",
      "epoch:22 step:21372 [D loss: 0.670660, acc: 64.06%] [G loss: 1.903904]\n",
      "epoch:22 step:21373 [D loss: 0.674055, acc: 60.94%] [G loss: 1.789300]\n",
      "epoch:22 step:21374 [D loss: 0.676482, acc: 59.38%] [G loss: 1.876419]\n",
      "epoch:22 step:21375 [D loss: 0.663579, acc: 61.72%] [G loss: 1.934794]\n",
      "epoch:22 step:21376 [D loss: 0.661921, acc: 57.03%] [G loss: 1.768656]\n",
      "epoch:22 step:21377 [D loss: 0.658596, acc: 61.72%] [G loss: 1.910202]\n",
      "epoch:22 step:21378 [D loss: 0.667628, acc: 58.59%] [G loss: 1.845413]\n",
      "epoch:22 step:21379 [D loss: 0.671531, acc: 58.59%] [G loss: 1.809240]\n",
      "epoch:22 step:21380 [D loss: 0.619010, acc: 66.41%] [G loss: 1.857997]\n",
      "epoch:22 step:21381 [D loss: 0.651063, acc: 64.06%] [G loss: 1.809834]\n",
      "epoch:22 step:21382 [D loss: 0.669421, acc: 60.16%] [G loss: 1.944112]\n",
      "epoch:22 step:21383 [D loss: 0.620748, acc: 66.41%] [G loss: 1.954373]\n",
      "epoch:22 step:21384 [D loss: 0.683183, acc: 59.38%] [G loss: 1.850539]\n",
      "epoch:22 step:21385 [D loss: 0.643485, acc: 62.50%] [G loss: 1.852241]\n",
      "epoch:22 step:21386 [D loss: 0.626576, acc: 64.84%] [G loss: 1.835843]\n",
      "epoch:22 step:21387 [D loss: 0.664529, acc: 61.72%] [G loss: 1.909771]\n",
      "epoch:22 step:21388 [D loss: 0.604300, acc: 65.62%] [G loss: 2.067383]\n",
      "epoch:22 step:21389 [D loss: 0.638598, acc: 64.06%] [G loss: 1.963299]\n",
      "epoch:22 step:21390 [D loss: 0.619814, acc: 61.72%] [G loss: 1.885045]\n",
      "epoch:22 step:21391 [D loss: 0.648772, acc: 62.50%] [G loss: 1.852353]\n",
      "epoch:22 step:21392 [D loss: 0.624534, acc: 64.06%] [G loss: 2.024852]\n",
      "epoch:22 step:21393 [D loss: 0.697867, acc: 53.91%] [G loss: 1.907545]\n",
      "epoch:22 step:21394 [D loss: 0.643912, acc: 62.50%] [G loss: 1.912405]\n",
      "epoch:22 step:21395 [D loss: 0.588191, acc: 70.31%] [G loss: 2.033036]\n",
      "epoch:22 step:21396 [D loss: 0.617373, acc: 64.84%] [G loss: 2.068570]\n",
      "epoch:22 step:21397 [D loss: 0.648704, acc: 66.41%] [G loss: 1.952051]\n",
      "epoch:22 step:21398 [D loss: 0.712578, acc: 53.12%] [G loss: 1.794039]\n",
      "epoch:22 step:21399 [D loss: 0.658899, acc: 65.62%] [G loss: 1.913501]\n",
      "epoch:22 step:21400 [D loss: 0.618681, acc: 65.62%] [G loss: 2.019363]\n",
      "epoch:22 step:21401 [D loss: 0.751416, acc: 53.12%] [G loss: 1.813579]\n",
      "epoch:22 step:21402 [D loss: 0.645123, acc: 65.62%] [G loss: 1.929790]\n",
      "epoch:22 step:21403 [D loss: 0.619993, acc: 70.31%] [G loss: 1.937445]\n",
      "epoch:22 step:21404 [D loss: 0.652444, acc: 63.28%] [G loss: 1.870932]\n",
      "epoch:22 step:21405 [D loss: 0.694897, acc: 57.03%] [G loss: 2.133612]\n",
      "epoch:22 step:21406 [D loss: 0.617000, acc: 65.62%] [G loss: 1.961879]\n",
      "epoch:22 step:21407 [D loss: 0.584463, acc: 70.31%] [G loss: 2.064163]\n",
      "epoch:22 step:21408 [D loss: 0.722748, acc: 50.00%] [G loss: 1.754398]\n",
      "epoch:22 step:21409 [D loss: 0.691544, acc: 57.81%] [G loss: 1.829120]\n",
      "epoch:22 step:21410 [D loss: 0.639157, acc: 60.94%] [G loss: 1.732982]\n",
      "epoch:22 step:21411 [D loss: 0.682269, acc: 53.91%] [G loss: 1.776658]\n",
      "epoch:22 step:21412 [D loss: 0.710972, acc: 57.03%] [G loss: 1.816394]\n",
      "epoch:22 step:21413 [D loss: 0.638626, acc: 61.72%] [G loss: 1.755944]\n",
      "epoch:22 step:21414 [D loss: 0.709976, acc: 55.47%] [G loss: 1.655032]\n",
      "epoch:22 step:21415 [D loss: 0.665481, acc: 60.16%] [G loss: 1.801382]\n",
      "epoch:22 step:21416 [D loss: 0.630098, acc: 67.19%] [G loss: 1.871951]\n",
      "epoch:22 step:21417 [D loss: 0.665800, acc: 57.03%] [G loss: 1.784980]\n",
      "epoch:22 step:21418 [D loss: 0.623403, acc: 65.62%] [G loss: 1.862639]\n",
      "epoch:22 step:21419 [D loss: 0.620500, acc: 68.75%] [G loss: 2.032260]\n",
      "epoch:22 step:21420 [D loss: 0.653970, acc: 66.41%] [G loss: 1.874940]\n",
      "epoch:22 step:21421 [D loss: 0.629794, acc: 59.38%] [G loss: 1.959541]\n",
      "epoch:22 step:21422 [D loss: 0.629891, acc: 64.06%] [G loss: 2.022501]\n",
      "epoch:22 step:21423 [D loss: 0.643560, acc: 63.28%] [G loss: 1.946485]\n",
      "epoch:22 step:21424 [D loss: 0.677884, acc: 60.94%] [G loss: 1.889187]\n",
      "epoch:22 step:21425 [D loss: 0.649947, acc: 64.84%] [G loss: 1.891389]\n",
      "epoch:22 step:21426 [D loss: 0.627044, acc: 63.28%] [G loss: 1.878348]\n",
      "epoch:22 step:21427 [D loss: 0.648284, acc: 60.16%] [G loss: 2.088982]\n",
      "epoch:22 step:21428 [D loss: 0.654731, acc: 64.06%] [G loss: 1.920936]\n",
      "epoch:22 step:21429 [D loss: 0.647084, acc: 65.62%] [G loss: 2.137441]\n",
      "epoch:22 step:21430 [D loss: 0.632255, acc: 64.84%] [G loss: 2.056514]\n",
      "epoch:22 step:21431 [D loss: 0.634029, acc: 65.62%] [G loss: 1.878332]\n",
      "epoch:22 step:21432 [D loss: 0.684455, acc: 55.47%] [G loss: 1.849911]\n",
      "epoch:22 step:21433 [D loss: 0.582000, acc: 71.09%] [G loss: 2.007059]\n",
      "epoch:22 step:21434 [D loss: 0.726773, acc: 55.47%] [G loss: 1.680763]\n",
      "epoch:22 step:21435 [D loss: 0.679780, acc: 57.03%] [G loss: 1.792278]\n",
      "epoch:22 step:21436 [D loss: 0.603081, acc: 65.62%] [G loss: 1.963143]\n",
      "epoch:22 step:21437 [D loss: 0.633031, acc: 64.06%] [G loss: 1.983047]\n",
      "epoch:22 step:21438 [D loss: 0.658357, acc: 64.06%] [G loss: 1.823175]\n",
      "epoch:22 step:21439 [D loss: 0.638682, acc: 59.38%] [G loss: 1.911373]\n",
      "epoch:22 step:21440 [D loss: 0.691789, acc: 54.69%] [G loss: 1.810070]\n",
      "epoch:22 step:21441 [D loss: 0.674062, acc: 56.25%] [G loss: 1.724768]\n",
      "epoch:22 step:21442 [D loss: 0.702628, acc: 54.69%] [G loss: 1.742924]\n",
      "epoch:22 step:21443 [D loss: 0.684490, acc: 55.47%] [G loss: 1.752062]\n",
      "epoch:22 step:21444 [D loss: 0.670590, acc: 61.72%] [G loss: 1.761654]\n",
      "epoch:22 step:21445 [D loss: 0.677946, acc: 57.03%] [G loss: 1.820509]\n",
      "epoch:22 step:21446 [D loss: 0.692925, acc: 58.59%] [G loss: 1.882135]\n",
      "epoch:22 step:21447 [D loss: 0.619168, acc: 71.88%] [G loss: 1.955544]\n",
      "epoch:22 step:21448 [D loss: 0.699003, acc: 59.38%] [G loss: 1.710914]\n",
      "epoch:22 step:21449 [D loss: 0.634383, acc: 64.06%] [G loss: 1.719020]\n",
      "epoch:22 step:21450 [D loss: 0.665849, acc: 61.72%] [G loss: 1.943299]\n",
      "epoch:22 step:21451 [D loss: 0.645357, acc: 61.72%] [G loss: 1.852831]\n",
      "epoch:22 step:21452 [D loss: 0.637392, acc: 65.62%] [G loss: 1.855592]\n",
      "epoch:22 step:21453 [D loss: 0.622547, acc: 63.28%] [G loss: 1.925282]\n",
      "epoch:22 step:21454 [D loss: 0.669593, acc: 59.38%] [G loss: 1.942833]\n",
      "epoch:22 step:21455 [D loss: 0.706361, acc: 58.59%] [G loss: 1.873004]\n",
      "epoch:22 step:21456 [D loss: 0.627149, acc: 70.31%] [G loss: 2.024074]\n",
      "epoch:22 step:21457 [D loss: 0.720520, acc: 51.56%] [G loss: 1.740789]\n",
      "epoch:22 step:21458 [D loss: 0.712618, acc: 53.12%] [G loss: 1.821893]\n",
      "epoch:22 step:21459 [D loss: 0.615394, acc: 68.75%] [G loss: 1.838796]\n",
      "epoch:22 step:21460 [D loss: 0.673991, acc: 55.47%] [G loss: 1.863322]\n",
      "epoch:22 step:21461 [D loss: 0.660310, acc: 62.50%] [G loss: 1.869781]\n",
      "epoch:22 step:21462 [D loss: 0.635753, acc: 67.19%] [G loss: 1.839635]\n",
      "epoch:22 step:21463 [D loss: 0.665750, acc: 58.59%] [G loss: 1.979281]\n",
      "epoch:22 step:21464 [D loss: 0.629646, acc: 60.16%] [G loss: 1.785170]\n",
      "epoch:22 step:21465 [D loss: 0.661688, acc: 60.94%] [G loss: 1.802449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:21466 [D loss: 0.600539, acc: 69.53%] [G loss: 1.875937]\n",
      "epoch:22 step:21467 [D loss: 0.623201, acc: 65.62%] [G loss: 1.919933]\n",
      "epoch:22 step:21468 [D loss: 0.654239, acc: 61.72%] [G loss: 1.853305]\n",
      "epoch:22 step:21469 [D loss: 0.641894, acc: 57.03%] [G loss: 1.800901]\n",
      "epoch:22 step:21470 [D loss: 0.666551, acc: 59.38%] [G loss: 2.079970]\n",
      "epoch:22 step:21471 [D loss: 0.658729, acc: 59.38%] [G loss: 1.859213]\n",
      "epoch:22 step:21472 [D loss: 0.720581, acc: 52.34%] [G loss: 1.778020]\n",
      "epoch:22 step:21473 [D loss: 0.677053, acc: 60.94%] [G loss: 1.723589]\n",
      "epoch:22 step:21474 [D loss: 0.655613, acc: 60.94%] [G loss: 1.861049]\n",
      "epoch:22 step:21475 [D loss: 0.642383, acc: 64.06%] [G loss: 1.825343]\n",
      "epoch:22 step:21476 [D loss: 0.652344, acc: 65.62%] [G loss: 1.697732]\n",
      "epoch:22 step:21477 [D loss: 0.665311, acc: 59.38%] [G loss: 1.840404]\n",
      "epoch:22 step:21478 [D loss: 0.655328, acc: 61.72%] [G loss: 1.859262]\n",
      "epoch:22 step:21479 [D loss: 0.669820, acc: 57.03%] [G loss: 1.675531]\n",
      "epoch:22 step:21480 [D loss: 0.645358, acc: 60.94%] [G loss: 1.767112]\n",
      "epoch:22 step:21481 [D loss: 0.675329, acc: 60.16%] [G loss: 1.795110]\n",
      "epoch:22 step:21482 [D loss: 0.624793, acc: 67.97%] [G loss: 1.789736]\n",
      "epoch:22 step:21483 [D loss: 0.676667, acc: 60.16%] [G loss: 1.816198]\n",
      "epoch:22 step:21484 [D loss: 0.652887, acc: 61.72%] [G loss: 1.754482]\n",
      "epoch:22 step:21485 [D loss: 0.666635, acc: 57.03%] [G loss: 1.885828]\n",
      "epoch:22 step:21486 [D loss: 0.650172, acc: 59.38%] [G loss: 1.712526]\n",
      "epoch:22 step:21487 [D loss: 0.654516, acc: 57.03%] [G loss: 1.714241]\n",
      "epoch:22 step:21488 [D loss: 0.653544, acc: 60.16%] [G loss: 1.787149]\n",
      "epoch:22 step:21489 [D loss: 0.660469, acc: 59.38%] [G loss: 1.856948]\n",
      "epoch:22 step:21490 [D loss: 0.651084, acc: 62.50%] [G loss: 1.998011]\n",
      "epoch:22 step:21491 [D loss: 0.712339, acc: 49.22%] [G loss: 1.769039]\n",
      "epoch:22 step:21492 [D loss: 0.633933, acc: 66.41%] [G loss: 1.874093]\n",
      "epoch:22 step:21493 [D loss: 0.663720, acc: 57.81%] [G loss: 1.901497]\n",
      "epoch:22 step:21494 [D loss: 0.633774, acc: 63.28%] [G loss: 1.834679]\n",
      "epoch:22 step:21495 [D loss: 0.605571, acc: 69.53%] [G loss: 1.852041]\n",
      "epoch:22 step:21496 [D loss: 0.637934, acc: 69.53%] [G loss: 1.896353]\n",
      "epoch:22 step:21497 [D loss: 0.645726, acc: 62.50%] [G loss: 1.765630]\n",
      "epoch:22 step:21498 [D loss: 0.637078, acc: 59.38%] [G loss: 1.906579]\n",
      "epoch:22 step:21499 [D loss: 0.678435, acc: 52.34%] [G loss: 1.905293]\n",
      "epoch:22 step:21500 [D loss: 0.639825, acc: 58.59%] [G loss: 1.867901]\n",
      "epoch:22 step:21501 [D loss: 0.603941, acc: 64.06%] [G loss: 1.923897]\n",
      "epoch:22 step:21502 [D loss: 0.638167, acc: 64.84%] [G loss: 1.725631]\n",
      "epoch:22 step:21503 [D loss: 0.622453, acc: 67.97%] [G loss: 1.912660]\n",
      "epoch:22 step:21504 [D loss: 0.635381, acc: 66.41%] [G loss: 1.855083]\n",
      "epoch:22 step:21505 [D loss: 0.650690, acc: 61.72%] [G loss: 1.788630]\n",
      "epoch:22 step:21506 [D loss: 0.647624, acc: 63.28%] [G loss: 1.922397]\n",
      "epoch:22 step:21507 [D loss: 0.664012, acc: 61.72%] [G loss: 1.791802]\n",
      "epoch:22 step:21508 [D loss: 0.634787, acc: 67.19%] [G loss: 1.998473]\n",
      "epoch:22 step:21509 [D loss: 0.626778, acc: 62.50%] [G loss: 1.821943]\n",
      "epoch:22 step:21510 [D loss: 0.651865, acc: 62.50%] [G loss: 1.866839]\n",
      "epoch:22 step:21511 [D loss: 0.644174, acc: 63.28%] [G loss: 1.895020]\n",
      "epoch:22 step:21512 [D loss: 0.687874, acc: 55.47%] [G loss: 2.032700]\n",
      "epoch:22 step:21513 [D loss: 0.602824, acc: 65.62%] [G loss: 1.967382]\n",
      "epoch:22 step:21514 [D loss: 0.584732, acc: 71.88%] [G loss: 1.900452]\n",
      "epoch:22 step:21515 [D loss: 0.662573, acc: 60.94%] [G loss: 1.930120]\n",
      "epoch:22 step:21516 [D loss: 0.604489, acc: 66.41%] [G loss: 2.021924]\n",
      "epoch:22 step:21517 [D loss: 0.667109, acc: 59.38%] [G loss: 1.834353]\n",
      "epoch:22 step:21518 [D loss: 0.640506, acc: 62.50%] [G loss: 2.009356]\n",
      "epoch:22 step:21519 [D loss: 0.710193, acc: 53.12%] [G loss: 1.918257]\n",
      "epoch:22 step:21520 [D loss: 0.609256, acc: 67.19%] [G loss: 1.995773]\n",
      "epoch:22 step:21521 [D loss: 0.600869, acc: 66.41%] [G loss: 1.975327]\n",
      "epoch:22 step:21522 [D loss: 0.650298, acc: 62.50%] [G loss: 1.832919]\n",
      "epoch:22 step:21523 [D loss: 0.636287, acc: 62.50%] [G loss: 2.065788]\n",
      "epoch:22 step:21524 [D loss: 0.665306, acc: 59.38%] [G loss: 1.800938]\n",
      "epoch:22 step:21525 [D loss: 0.674990, acc: 56.25%] [G loss: 2.026228]\n",
      "epoch:22 step:21526 [D loss: 0.652889, acc: 60.94%] [G loss: 2.047360]\n",
      "epoch:22 step:21527 [D loss: 0.708673, acc: 61.72%] [G loss: 1.853956]\n",
      "epoch:22 step:21528 [D loss: 0.652004, acc: 57.03%] [G loss: 1.903115]\n",
      "epoch:22 step:21529 [D loss: 0.640417, acc: 65.62%] [G loss: 1.925673]\n",
      "epoch:22 step:21530 [D loss: 0.668745, acc: 61.72%] [G loss: 1.866957]\n",
      "epoch:22 step:21531 [D loss: 0.663128, acc: 61.72%] [G loss: 1.911998]\n",
      "epoch:22 step:21532 [D loss: 0.604980, acc: 71.88%] [G loss: 2.101073]\n",
      "epoch:22 step:21533 [D loss: 0.621735, acc: 64.06%] [G loss: 2.180969]\n",
      "epoch:22 step:21534 [D loss: 0.701754, acc: 56.25%] [G loss: 1.766221]\n",
      "epoch:22 step:21535 [D loss: 0.648875, acc: 60.16%] [G loss: 1.837404]\n",
      "epoch:22 step:21536 [D loss: 0.629234, acc: 64.84%] [G loss: 1.784956]\n",
      "epoch:22 step:21537 [D loss: 0.568475, acc: 75.00%] [G loss: 2.237080]\n",
      "epoch:22 step:21538 [D loss: 0.594013, acc: 68.75%] [G loss: 2.133799]\n",
      "epoch:22 step:21539 [D loss: 0.614341, acc: 66.41%] [G loss: 2.067400]\n",
      "epoch:22 step:21540 [D loss: 0.622114, acc: 61.72%] [G loss: 1.959551]\n",
      "epoch:22 step:21541 [D loss: 0.653617, acc: 64.06%] [G loss: 1.980079]\n",
      "epoch:22 step:21542 [D loss: 0.715034, acc: 57.03%] [G loss: 1.805105]\n",
      "epoch:22 step:21543 [D loss: 0.739797, acc: 50.78%] [G loss: 1.868524]\n",
      "epoch:22 step:21544 [D loss: 0.627403, acc: 66.41%] [G loss: 2.044778]\n",
      "epoch:22 step:21545 [D loss: 0.609318, acc: 64.84%] [G loss: 2.047945]\n",
      "epoch:22 step:21546 [D loss: 0.684722, acc: 52.34%] [G loss: 1.901262]\n",
      "epoch:22 step:21547 [D loss: 0.673464, acc: 59.38%] [G loss: 1.893441]\n",
      "epoch:22 step:21548 [D loss: 0.638407, acc: 64.84%] [G loss: 1.956112]\n",
      "epoch:22 step:21549 [D loss: 0.613242, acc: 68.75%] [G loss: 1.944436]\n",
      "epoch:22 step:21550 [D loss: 0.574297, acc: 75.00%] [G loss: 2.012835]\n",
      "epoch:22 step:21551 [D loss: 0.567686, acc: 70.31%] [G loss: 2.337648]\n",
      "epoch:23 step:21552 [D loss: 0.634391, acc: 67.97%] [G loss: 1.951248]\n",
      "epoch:23 step:21553 [D loss: 0.606198, acc: 67.19%] [G loss: 2.069720]\n",
      "epoch:23 step:21554 [D loss: 0.697242, acc: 57.03%] [G loss: 1.952268]\n",
      "epoch:23 step:21555 [D loss: 0.659143, acc: 58.59%] [G loss: 1.899478]\n",
      "epoch:23 step:21556 [D loss: 0.672848, acc: 63.28%] [G loss: 1.870686]\n",
      "epoch:23 step:21557 [D loss: 0.623233, acc: 62.50%] [G loss: 1.948331]\n",
      "epoch:23 step:21558 [D loss: 0.646694, acc: 59.38%] [G loss: 2.000777]\n",
      "epoch:23 step:21559 [D loss: 0.619908, acc: 60.94%] [G loss: 1.912871]\n",
      "epoch:23 step:21560 [D loss: 0.583061, acc: 67.19%] [G loss: 2.075294]\n",
      "epoch:23 step:21561 [D loss: 0.639397, acc: 62.50%] [G loss: 2.005301]\n",
      "epoch:23 step:21562 [D loss: 0.653923, acc: 64.84%] [G loss: 2.062814]\n",
      "epoch:23 step:21563 [D loss: 0.695827, acc: 58.59%] [G loss: 1.863346]\n",
      "epoch:23 step:21564 [D loss: 0.606646, acc: 67.19%] [G loss: 1.966598]\n",
      "epoch:23 step:21565 [D loss: 0.641420, acc: 58.59%] [G loss: 1.966257]\n",
      "epoch:23 step:21566 [D loss: 0.634823, acc: 64.06%] [G loss: 2.208687]\n",
      "epoch:23 step:21567 [D loss: 0.639567, acc: 62.50%] [G loss: 2.094459]\n",
      "epoch:23 step:21568 [D loss: 0.628611, acc: 65.62%] [G loss: 1.926729]\n",
      "epoch:23 step:21569 [D loss: 0.645779, acc: 62.50%] [G loss: 1.997557]\n",
      "epoch:23 step:21570 [D loss: 0.661313, acc: 54.69%] [G loss: 1.884153]\n",
      "epoch:23 step:21571 [D loss: 0.739891, acc: 53.12%] [G loss: 1.625942]\n",
      "epoch:23 step:21572 [D loss: 0.624485, acc: 61.72%] [G loss: 1.745006]\n",
      "epoch:23 step:21573 [D loss: 0.642620, acc: 62.50%] [G loss: 1.826334]\n",
      "epoch:23 step:21574 [D loss: 0.670210, acc: 59.38%] [G loss: 1.810051]\n",
      "epoch:23 step:21575 [D loss: 0.585881, acc: 68.75%] [G loss: 1.976485]\n",
      "epoch:23 step:21576 [D loss: 0.608894, acc: 64.06%] [G loss: 1.864027]\n",
      "epoch:23 step:21577 [D loss: 0.644450, acc: 61.72%] [G loss: 1.810929]\n",
      "epoch:23 step:21578 [D loss: 0.644256, acc: 63.28%] [G loss: 1.842931]\n",
      "epoch:23 step:21579 [D loss: 0.651460, acc: 61.72%] [G loss: 1.963264]\n",
      "epoch:23 step:21580 [D loss: 0.630505, acc: 66.41%] [G loss: 1.889021]\n",
      "epoch:23 step:21581 [D loss: 0.603360, acc: 71.09%] [G loss: 1.852309]\n",
      "epoch:23 step:21582 [D loss: 0.692716, acc: 56.25%] [G loss: 1.827281]\n",
      "epoch:23 step:21583 [D loss: 0.640656, acc: 60.16%] [G loss: 1.834038]\n",
      "epoch:23 step:21584 [D loss: 0.648773, acc: 64.84%] [G loss: 1.891939]\n",
      "epoch:23 step:21585 [D loss: 0.627677, acc: 67.19%] [G loss: 1.942740]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21586 [D loss: 0.637042, acc: 63.28%] [G loss: 1.963061]\n",
      "epoch:23 step:21587 [D loss: 0.618832, acc: 70.31%] [G loss: 1.954694]\n",
      "epoch:23 step:21588 [D loss: 0.658745, acc: 63.28%] [G loss: 1.810680]\n",
      "epoch:23 step:21589 [D loss: 0.674906, acc: 63.28%] [G loss: 1.976696]\n",
      "epoch:23 step:21590 [D loss: 0.649667, acc: 59.38%] [G loss: 1.847565]\n",
      "epoch:23 step:21591 [D loss: 0.617771, acc: 71.09%] [G loss: 2.018659]\n",
      "epoch:23 step:21592 [D loss: 0.667953, acc: 59.38%] [G loss: 1.895835]\n",
      "epoch:23 step:21593 [D loss: 0.659003, acc: 60.16%] [G loss: 1.883321]\n",
      "epoch:23 step:21594 [D loss: 0.655464, acc: 60.94%] [G loss: 1.760668]\n",
      "epoch:23 step:21595 [D loss: 0.629701, acc: 60.94%] [G loss: 1.795594]\n",
      "epoch:23 step:21596 [D loss: 0.656843, acc: 58.59%] [G loss: 1.805408]\n",
      "epoch:23 step:21597 [D loss: 0.691338, acc: 57.81%] [G loss: 1.889329]\n",
      "epoch:23 step:21598 [D loss: 0.644593, acc: 67.19%] [G loss: 1.979864]\n",
      "epoch:23 step:21599 [D loss: 0.605810, acc: 65.62%] [G loss: 1.894908]\n",
      "epoch:23 step:21600 [D loss: 0.601002, acc: 66.41%] [G loss: 2.125941]\n",
      "epoch:23 step:21601 [D loss: 0.575308, acc: 70.31%] [G loss: 1.991882]\n",
      "epoch:23 step:21602 [D loss: 0.657115, acc: 59.38%] [G loss: 1.889830]\n",
      "epoch:23 step:21603 [D loss: 0.642549, acc: 61.72%] [G loss: 1.934642]\n",
      "epoch:23 step:21604 [D loss: 0.633055, acc: 68.75%] [G loss: 1.984911]\n",
      "epoch:23 step:21605 [D loss: 0.660903, acc: 63.28%] [G loss: 2.078931]\n",
      "epoch:23 step:21606 [D loss: 0.622503, acc: 66.41%] [G loss: 2.103402]\n",
      "epoch:23 step:21607 [D loss: 0.660045, acc: 56.25%] [G loss: 2.090752]\n",
      "epoch:23 step:21608 [D loss: 0.653322, acc: 57.81%] [G loss: 1.778321]\n",
      "epoch:23 step:21609 [D loss: 0.646731, acc: 60.16%] [G loss: 1.921947]\n",
      "epoch:23 step:21610 [D loss: 0.663906, acc: 57.81%] [G loss: 2.034323]\n",
      "epoch:23 step:21611 [D loss: 0.648938, acc: 57.81%] [G loss: 1.771930]\n",
      "epoch:23 step:21612 [D loss: 0.673690, acc: 57.81%] [G loss: 1.864955]\n",
      "epoch:23 step:21613 [D loss: 0.672068, acc: 56.25%] [G loss: 1.907178]\n",
      "epoch:23 step:21614 [D loss: 0.659782, acc: 54.69%] [G loss: 1.825507]\n",
      "epoch:23 step:21615 [D loss: 0.617212, acc: 65.62%] [G loss: 1.965055]\n",
      "epoch:23 step:21616 [D loss: 0.618746, acc: 64.84%] [G loss: 1.910408]\n",
      "epoch:23 step:21617 [D loss: 0.634625, acc: 64.84%] [G loss: 1.786280]\n",
      "epoch:23 step:21618 [D loss: 0.667530, acc: 58.59%] [G loss: 1.785389]\n",
      "epoch:23 step:21619 [D loss: 0.642470, acc: 59.38%] [G loss: 1.713503]\n",
      "epoch:23 step:21620 [D loss: 0.657503, acc: 60.94%] [G loss: 2.049891]\n",
      "epoch:23 step:21621 [D loss: 0.685366, acc: 63.28%] [G loss: 2.002211]\n",
      "epoch:23 step:21622 [D loss: 0.675327, acc: 58.59%] [G loss: 1.791514]\n",
      "epoch:23 step:21623 [D loss: 0.650645, acc: 59.38%] [G loss: 1.792153]\n",
      "epoch:23 step:21624 [D loss: 0.618290, acc: 65.62%] [G loss: 1.847646]\n",
      "epoch:23 step:21625 [D loss: 0.610652, acc: 65.62%] [G loss: 1.945463]\n",
      "epoch:23 step:21626 [D loss: 0.702139, acc: 59.38%] [G loss: 2.131295]\n",
      "epoch:23 step:21627 [D loss: 0.604594, acc: 64.84%] [G loss: 1.865260]\n",
      "epoch:23 step:21628 [D loss: 0.589943, acc: 69.53%] [G loss: 2.048625]\n",
      "epoch:23 step:21629 [D loss: 0.684669, acc: 57.03%] [G loss: 1.807955]\n",
      "epoch:23 step:21630 [D loss: 0.637610, acc: 62.50%] [G loss: 1.792741]\n",
      "epoch:23 step:21631 [D loss: 0.666243, acc: 56.25%] [G loss: 1.779410]\n",
      "epoch:23 step:21632 [D loss: 0.655493, acc: 65.62%] [G loss: 1.784926]\n",
      "epoch:23 step:21633 [D loss: 0.653771, acc: 59.38%] [G loss: 1.960984]\n",
      "epoch:23 step:21634 [D loss: 0.635116, acc: 64.84%] [G loss: 1.887724]\n",
      "epoch:23 step:21635 [D loss: 0.632502, acc: 65.62%] [G loss: 1.812314]\n",
      "epoch:23 step:21636 [D loss: 0.692281, acc: 56.25%] [G loss: 1.808270]\n",
      "epoch:23 step:21637 [D loss: 0.660836, acc: 58.59%] [G loss: 1.856163]\n",
      "epoch:23 step:21638 [D loss: 0.643026, acc: 60.16%] [G loss: 1.831329]\n",
      "epoch:23 step:21639 [D loss: 0.658941, acc: 58.59%] [G loss: 1.789980]\n",
      "epoch:23 step:21640 [D loss: 0.598927, acc: 70.31%] [G loss: 1.967660]\n",
      "epoch:23 step:21641 [D loss: 0.671155, acc: 64.84%] [G loss: 1.815828]\n",
      "epoch:23 step:21642 [D loss: 0.618274, acc: 66.41%] [G loss: 1.881114]\n",
      "epoch:23 step:21643 [D loss: 0.646823, acc: 64.84%] [G loss: 1.876394]\n",
      "epoch:23 step:21644 [D loss: 0.621952, acc: 64.06%] [G loss: 2.013847]\n",
      "epoch:23 step:21645 [D loss: 0.642174, acc: 63.28%] [G loss: 1.891582]\n",
      "epoch:23 step:21646 [D loss: 0.634867, acc: 64.84%] [G loss: 1.856462]\n",
      "epoch:23 step:21647 [D loss: 0.636255, acc: 61.72%] [G loss: 1.952214]\n",
      "epoch:23 step:21648 [D loss: 0.663529, acc: 57.03%] [G loss: 2.033031]\n",
      "epoch:23 step:21649 [D loss: 0.724557, acc: 50.00%] [G loss: 1.807230]\n",
      "epoch:23 step:21650 [D loss: 0.664578, acc: 57.81%] [G loss: 1.827759]\n",
      "epoch:23 step:21651 [D loss: 0.652908, acc: 54.69%] [G loss: 1.898380]\n",
      "epoch:23 step:21652 [D loss: 0.615565, acc: 64.06%] [G loss: 1.752159]\n",
      "epoch:23 step:21653 [D loss: 0.676944, acc: 58.59%] [G loss: 1.796953]\n",
      "epoch:23 step:21654 [D loss: 0.610892, acc: 70.31%] [G loss: 1.836068]\n",
      "epoch:23 step:21655 [D loss: 0.668602, acc: 62.50%] [G loss: 1.674239]\n",
      "epoch:23 step:21656 [D loss: 0.662389, acc: 57.81%] [G loss: 1.939766]\n",
      "epoch:23 step:21657 [D loss: 0.630678, acc: 60.16%] [G loss: 2.018169]\n",
      "epoch:23 step:21658 [D loss: 0.639296, acc: 63.28%] [G loss: 2.038560]\n",
      "epoch:23 step:21659 [D loss: 0.673957, acc: 58.59%] [G loss: 1.704068]\n",
      "epoch:23 step:21660 [D loss: 0.705949, acc: 50.00%] [G loss: 1.812472]\n",
      "epoch:23 step:21661 [D loss: 0.690150, acc: 60.94%] [G loss: 1.802490]\n",
      "epoch:23 step:21662 [D loss: 0.620322, acc: 64.06%] [G loss: 1.844988]\n",
      "epoch:23 step:21663 [D loss: 0.594475, acc: 71.88%] [G loss: 2.025163]\n",
      "epoch:23 step:21664 [D loss: 0.612271, acc: 68.75%] [G loss: 2.035292]\n",
      "epoch:23 step:21665 [D loss: 0.621634, acc: 65.62%] [G loss: 1.934650]\n",
      "epoch:23 step:21666 [D loss: 0.591662, acc: 70.31%] [G loss: 2.114623]\n",
      "epoch:23 step:21667 [D loss: 0.608192, acc: 67.97%] [G loss: 2.031837]\n",
      "epoch:23 step:21668 [D loss: 0.671674, acc: 59.38%] [G loss: 1.989365]\n",
      "epoch:23 step:21669 [D loss: 0.621152, acc: 65.62%] [G loss: 1.990360]\n",
      "epoch:23 step:21670 [D loss: 0.612586, acc: 62.50%] [G loss: 2.174976]\n",
      "epoch:23 step:21671 [D loss: 0.744690, acc: 51.56%] [G loss: 1.884034]\n",
      "epoch:23 step:21672 [D loss: 0.641119, acc: 59.38%] [G loss: 1.942127]\n",
      "epoch:23 step:21673 [D loss: 0.685632, acc: 60.16%] [G loss: 2.032864]\n",
      "epoch:23 step:21674 [D loss: 0.657050, acc: 62.50%] [G loss: 1.812483]\n",
      "epoch:23 step:21675 [D loss: 0.638537, acc: 59.38%] [G loss: 1.864235]\n",
      "epoch:23 step:21676 [D loss: 0.701308, acc: 55.47%] [G loss: 1.660686]\n",
      "epoch:23 step:21677 [D loss: 0.613720, acc: 63.28%] [G loss: 1.997863]\n",
      "epoch:23 step:21678 [D loss: 0.634579, acc: 62.50%] [G loss: 1.881822]\n",
      "epoch:23 step:21679 [D loss: 0.675706, acc: 60.16%] [G loss: 1.908078]\n",
      "epoch:23 step:21680 [D loss: 0.623901, acc: 65.62%] [G loss: 1.928313]\n",
      "epoch:23 step:21681 [D loss: 0.680772, acc: 56.25%] [G loss: 1.943113]\n",
      "epoch:23 step:21682 [D loss: 0.602974, acc: 68.75%] [G loss: 1.922345]\n",
      "epoch:23 step:21683 [D loss: 0.692086, acc: 57.81%] [G loss: 1.822307]\n",
      "epoch:23 step:21684 [D loss: 0.606407, acc: 72.66%] [G loss: 1.729536]\n",
      "epoch:23 step:21685 [D loss: 0.661715, acc: 63.28%] [G loss: 1.725161]\n",
      "epoch:23 step:21686 [D loss: 0.668514, acc: 62.50%] [G loss: 1.852117]\n",
      "epoch:23 step:21687 [D loss: 0.650207, acc: 58.59%] [G loss: 1.853207]\n",
      "epoch:23 step:21688 [D loss: 0.675848, acc: 54.69%] [G loss: 1.730840]\n",
      "epoch:23 step:21689 [D loss: 0.688916, acc: 55.47%] [G loss: 1.812986]\n",
      "epoch:23 step:21690 [D loss: 0.650814, acc: 59.38%] [G loss: 1.886639]\n",
      "epoch:23 step:21691 [D loss: 0.650711, acc: 57.81%] [G loss: 1.620825]\n",
      "epoch:23 step:21692 [D loss: 0.640976, acc: 61.72%] [G loss: 1.749740]\n",
      "epoch:23 step:21693 [D loss: 0.656617, acc: 60.94%] [G loss: 1.908708]\n",
      "epoch:23 step:21694 [D loss: 0.644958, acc: 66.41%] [G loss: 1.758931]\n",
      "epoch:23 step:21695 [D loss: 0.672549, acc: 62.50%] [G loss: 1.710821]\n",
      "epoch:23 step:21696 [D loss: 0.664109, acc: 58.59%] [G loss: 1.661597]\n",
      "epoch:23 step:21697 [D loss: 0.637299, acc: 65.62%] [G loss: 1.942043]\n",
      "epoch:23 step:21698 [D loss: 0.687817, acc: 58.59%] [G loss: 1.763495]\n",
      "epoch:23 step:21699 [D loss: 0.703493, acc: 51.56%] [G loss: 1.713580]\n",
      "epoch:23 step:21700 [D loss: 0.623373, acc: 67.19%] [G loss: 1.802366]\n",
      "epoch:23 step:21701 [D loss: 0.645824, acc: 59.38%] [G loss: 1.878325]\n",
      "epoch:23 step:21702 [D loss: 0.669939, acc: 55.47%] [G loss: 2.024537]\n",
      "epoch:23 step:21703 [D loss: 0.670137, acc: 64.84%] [G loss: 1.939202]\n",
      "epoch:23 step:21704 [D loss: 0.679347, acc: 55.47%] [G loss: 1.814703]\n",
      "epoch:23 step:21705 [D loss: 0.664482, acc: 55.47%] [G loss: 1.851380]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21706 [D loss: 0.647754, acc: 67.97%] [G loss: 1.837777]\n",
      "epoch:23 step:21707 [D loss: 0.694996, acc: 57.81%] [G loss: 1.815761]\n",
      "epoch:23 step:21708 [D loss: 0.645443, acc: 64.06%] [G loss: 1.871308]\n",
      "epoch:23 step:21709 [D loss: 0.611539, acc: 65.62%] [G loss: 1.862281]\n",
      "epoch:23 step:21710 [D loss: 0.652074, acc: 61.72%] [G loss: 1.882234]\n",
      "epoch:23 step:21711 [D loss: 0.733389, acc: 53.12%] [G loss: 1.771525]\n",
      "epoch:23 step:21712 [D loss: 0.640893, acc: 56.25%] [G loss: 1.856456]\n",
      "epoch:23 step:21713 [D loss: 0.655839, acc: 61.72%] [G loss: 1.806026]\n",
      "epoch:23 step:21714 [D loss: 0.634707, acc: 60.94%] [G loss: 1.909065]\n",
      "epoch:23 step:21715 [D loss: 0.667714, acc: 55.47%] [G loss: 1.909297]\n",
      "epoch:23 step:21716 [D loss: 0.658314, acc: 62.50%] [G loss: 1.876827]\n",
      "epoch:23 step:21717 [D loss: 0.616898, acc: 66.41%] [G loss: 1.815896]\n",
      "epoch:23 step:21718 [D loss: 0.668577, acc: 58.59%] [G loss: 1.823030]\n",
      "epoch:23 step:21719 [D loss: 0.651651, acc: 66.41%] [G loss: 1.808737]\n",
      "epoch:23 step:21720 [D loss: 0.636425, acc: 65.62%] [G loss: 1.916807]\n",
      "epoch:23 step:21721 [D loss: 0.664423, acc: 58.59%] [G loss: 1.863891]\n",
      "epoch:23 step:21722 [D loss: 0.679362, acc: 60.16%] [G loss: 1.755476]\n",
      "epoch:23 step:21723 [D loss: 0.641308, acc: 64.06%] [G loss: 1.730398]\n",
      "epoch:23 step:21724 [D loss: 0.668754, acc: 57.81%] [G loss: 1.723189]\n",
      "epoch:23 step:21725 [D loss: 0.715867, acc: 47.66%] [G loss: 1.604417]\n",
      "epoch:23 step:21726 [D loss: 0.630129, acc: 60.94%] [G loss: 1.839310]\n",
      "epoch:23 step:21727 [D loss: 0.657265, acc: 61.72%] [G loss: 1.743944]\n",
      "epoch:23 step:21728 [D loss: 0.650782, acc: 62.50%] [G loss: 1.711576]\n",
      "epoch:23 step:21729 [D loss: 0.656454, acc: 61.72%] [G loss: 1.622096]\n",
      "epoch:23 step:21730 [D loss: 0.683693, acc: 53.91%] [G loss: 1.768701]\n",
      "epoch:23 step:21731 [D loss: 0.646978, acc: 61.72%] [G loss: 1.757499]\n",
      "epoch:23 step:21732 [D loss: 0.675290, acc: 55.47%] [G loss: 1.704728]\n",
      "epoch:23 step:21733 [D loss: 0.676973, acc: 58.59%] [G loss: 1.795108]\n",
      "epoch:23 step:21734 [D loss: 0.673001, acc: 57.03%] [G loss: 1.684139]\n",
      "epoch:23 step:21735 [D loss: 0.667562, acc: 57.03%] [G loss: 1.712288]\n",
      "epoch:23 step:21736 [D loss: 0.655491, acc: 67.19%] [G loss: 1.843377]\n",
      "epoch:23 step:21737 [D loss: 0.635719, acc: 68.75%] [G loss: 1.879027]\n",
      "epoch:23 step:21738 [D loss: 0.671891, acc: 60.16%] [G loss: 1.811005]\n",
      "epoch:23 step:21739 [D loss: 0.680390, acc: 59.38%] [G loss: 1.881612]\n",
      "epoch:23 step:21740 [D loss: 0.698796, acc: 59.38%] [G loss: 1.633770]\n",
      "epoch:23 step:21741 [D loss: 0.649098, acc: 62.50%] [G loss: 1.740408]\n",
      "epoch:23 step:21742 [D loss: 0.671009, acc: 57.81%] [G loss: 1.808656]\n",
      "epoch:23 step:21743 [D loss: 0.637933, acc: 61.72%] [G loss: 1.912550]\n",
      "epoch:23 step:21744 [D loss: 0.636320, acc: 64.06%] [G loss: 1.988353]\n",
      "epoch:23 step:21745 [D loss: 0.619222, acc: 63.28%] [G loss: 1.907182]\n",
      "epoch:23 step:21746 [D loss: 0.634097, acc: 63.28%] [G loss: 1.816955]\n",
      "epoch:23 step:21747 [D loss: 0.659267, acc: 62.50%] [G loss: 1.896112]\n",
      "epoch:23 step:21748 [D loss: 0.658913, acc: 59.38%] [G loss: 1.970521]\n",
      "epoch:23 step:21749 [D loss: 0.577700, acc: 66.41%] [G loss: 1.908054]\n",
      "epoch:23 step:21750 [D loss: 0.655472, acc: 57.03%] [G loss: 1.945449]\n",
      "epoch:23 step:21751 [D loss: 0.655121, acc: 60.16%] [G loss: 1.684097]\n",
      "epoch:23 step:21752 [D loss: 0.627040, acc: 60.16%] [G loss: 1.911654]\n",
      "epoch:23 step:21753 [D loss: 0.660544, acc: 62.50%] [G loss: 1.806400]\n",
      "epoch:23 step:21754 [D loss: 0.657852, acc: 60.94%] [G loss: 1.890919]\n",
      "epoch:23 step:21755 [D loss: 0.661619, acc: 63.28%] [G loss: 1.892976]\n",
      "epoch:23 step:21756 [D loss: 0.641695, acc: 62.50%] [G loss: 1.869612]\n",
      "epoch:23 step:21757 [D loss: 0.593216, acc: 67.97%] [G loss: 2.000915]\n",
      "epoch:23 step:21758 [D loss: 0.593696, acc: 71.09%] [G loss: 1.958042]\n",
      "epoch:23 step:21759 [D loss: 0.650389, acc: 62.50%] [G loss: 2.232838]\n",
      "epoch:23 step:21760 [D loss: 0.624296, acc: 64.06%] [G loss: 1.991624]\n",
      "epoch:23 step:21761 [D loss: 0.672992, acc: 55.47%] [G loss: 1.755121]\n",
      "epoch:23 step:21762 [D loss: 0.672263, acc: 62.50%] [G loss: 1.759518]\n",
      "epoch:23 step:21763 [D loss: 0.671768, acc: 59.38%] [G loss: 1.793094]\n",
      "epoch:23 step:21764 [D loss: 0.684351, acc: 57.03%] [G loss: 1.840729]\n",
      "epoch:23 step:21765 [D loss: 0.691046, acc: 56.25%] [G loss: 1.768546]\n",
      "epoch:23 step:21766 [D loss: 0.605445, acc: 67.19%] [G loss: 1.913796]\n",
      "epoch:23 step:21767 [D loss: 0.649196, acc: 60.94%] [G loss: 1.894280]\n",
      "epoch:23 step:21768 [D loss: 0.597540, acc: 68.75%] [G loss: 1.940489]\n",
      "epoch:23 step:21769 [D loss: 0.596141, acc: 65.62%] [G loss: 1.841324]\n",
      "epoch:23 step:21770 [D loss: 0.607800, acc: 62.50%] [G loss: 2.162076]\n",
      "epoch:23 step:21771 [D loss: 0.674080, acc: 56.25%] [G loss: 1.756069]\n",
      "epoch:23 step:21772 [D loss: 0.677572, acc: 62.50%] [G loss: 1.928214]\n",
      "epoch:23 step:21773 [D loss: 0.636851, acc: 61.72%] [G loss: 1.892037]\n",
      "epoch:23 step:21774 [D loss: 0.662185, acc: 58.59%] [G loss: 1.864130]\n",
      "epoch:23 step:21775 [D loss: 0.664241, acc: 58.59%] [G loss: 1.837048]\n",
      "epoch:23 step:21776 [D loss: 0.653735, acc: 56.25%] [G loss: 1.814757]\n",
      "epoch:23 step:21777 [D loss: 0.650527, acc: 61.72%] [G loss: 1.868459]\n",
      "epoch:23 step:21778 [D loss: 0.658924, acc: 60.16%] [G loss: 1.940078]\n",
      "epoch:23 step:21779 [D loss: 0.700811, acc: 54.69%] [G loss: 1.832648]\n",
      "epoch:23 step:21780 [D loss: 0.641794, acc: 65.62%] [G loss: 2.021102]\n",
      "epoch:23 step:21781 [D loss: 0.579142, acc: 71.88%] [G loss: 1.877019]\n",
      "epoch:23 step:21782 [D loss: 0.555211, acc: 74.22%] [G loss: 2.183829]\n",
      "epoch:23 step:21783 [D loss: 0.600412, acc: 71.09%] [G loss: 2.192514]\n",
      "epoch:23 step:21784 [D loss: 0.648546, acc: 60.94%] [G loss: 1.918832]\n",
      "epoch:23 step:21785 [D loss: 0.673465, acc: 61.72%] [G loss: 1.912825]\n",
      "epoch:23 step:21786 [D loss: 0.668740, acc: 62.50%] [G loss: 1.887166]\n",
      "epoch:23 step:21787 [D loss: 0.626730, acc: 65.62%] [G loss: 1.843725]\n",
      "epoch:23 step:21788 [D loss: 0.689584, acc: 55.47%] [G loss: 1.984654]\n",
      "epoch:23 step:21789 [D loss: 0.691067, acc: 58.59%] [G loss: 1.959200]\n",
      "epoch:23 step:21790 [D loss: 0.651912, acc: 60.16%] [G loss: 1.981415]\n",
      "epoch:23 step:21791 [D loss: 0.736880, acc: 48.44%] [G loss: 1.860230]\n",
      "epoch:23 step:21792 [D loss: 0.622224, acc: 68.75%] [G loss: 1.988089]\n",
      "epoch:23 step:21793 [D loss: 0.672400, acc: 58.59%] [G loss: 2.014547]\n",
      "epoch:23 step:21794 [D loss: 0.609411, acc: 67.19%] [G loss: 1.936102]\n",
      "epoch:23 step:21795 [D loss: 0.668068, acc: 63.28%] [G loss: 1.942813]\n",
      "epoch:23 step:21796 [D loss: 0.620833, acc: 64.06%] [G loss: 1.878944]\n",
      "epoch:23 step:21797 [D loss: 0.697501, acc: 57.03%] [G loss: 1.929216]\n",
      "epoch:23 step:21798 [D loss: 0.651464, acc: 61.72%] [G loss: 1.913631]\n",
      "epoch:23 step:21799 [D loss: 0.627965, acc: 63.28%] [G loss: 2.075266]\n",
      "epoch:23 step:21800 [D loss: 0.662139, acc: 59.38%] [G loss: 1.745314]\n",
      "epoch:23 step:21801 [D loss: 0.715461, acc: 57.03%] [G loss: 1.625367]\n",
      "epoch:23 step:21802 [D loss: 0.684619, acc: 59.38%] [G loss: 1.725795]\n",
      "epoch:23 step:21803 [D loss: 0.648951, acc: 66.41%] [G loss: 1.662285]\n",
      "epoch:23 step:21804 [D loss: 0.628182, acc: 70.31%] [G loss: 1.852010]\n",
      "epoch:23 step:21805 [D loss: 0.632270, acc: 63.28%] [G loss: 1.816272]\n",
      "epoch:23 step:21806 [D loss: 0.661008, acc: 58.59%] [G loss: 1.720713]\n",
      "epoch:23 step:21807 [D loss: 0.693169, acc: 57.81%] [G loss: 1.895992]\n",
      "epoch:23 step:21808 [D loss: 0.662462, acc: 57.03%] [G loss: 1.780571]\n",
      "epoch:23 step:21809 [D loss: 0.688964, acc: 53.12%] [G loss: 1.720872]\n",
      "epoch:23 step:21810 [D loss: 0.653291, acc: 59.38%] [G loss: 1.799119]\n",
      "epoch:23 step:21811 [D loss: 0.710875, acc: 54.69%] [G loss: 1.755411]\n",
      "epoch:23 step:21812 [D loss: 0.604649, acc: 68.75%] [G loss: 1.964389]\n",
      "epoch:23 step:21813 [D loss: 0.608018, acc: 67.97%] [G loss: 1.873884]\n",
      "epoch:23 step:21814 [D loss: 0.658342, acc: 61.72%] [G loss: 1.767464]\n",
      "epoch:23 step:21815 [D loss: 0.643449, acc: 62.50%] [G loss: 1.906810]\n",
      "epoch:23 step:21816 [D loss: 0.653798, acc: 64.84%] [G loss: 1.790699]\n",
      "epoch:23 step:21817 [D loss: 0.636436, acc: 63.28%] [G loss: 1.805082]\n",
      "epoch:23 step:21818 [D loss: 0.653322, acc: 60.16%] [G loss: 1.903846]\n",
      "epoch:23 step:21819 [D loss: 0.672127, acc: 60.16%] [G loss: 1.812695]\n",
      "epoch:23 step:21820 [D loss: 0.627958, acc: 64.06%] [G loss: 1.710152]\n",
      "epoch:23 step:21821 [D loss: 0.638421, acc: 60.94%] [G loss: 1.876648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21822 [D loss: 0.617631, acc: 67.19%] [G loss: 1.965725]\n",
      "epoch:23 step:21823 [D loss: 0.573513, acc: 74.22%] [G loss: 1.951568]\n",
      "epoch:23 step:21824 [D loss: 0.658039, acc: 60.16%] [G loss: 1.806798]\n",
      "epoch:23 step:21825 [D loss: 0.582146, acc: 67.97%] [G loss: 2.050861]\n",
      "epoch:23 step:21826 [D loss: 0.624685, acc: 61.72%] [G loss: 2.130633]\n",
      "epoch:23 step:21827 [D loss: 0.556318, acc: 68.75%] [G loss: 2.217103]\n",
      "epoch:23 step:21828 [D loss: 0.676738, acc: 56.25%] [G loss: 1.865161]\n",
      "epoch:23 step:21829 [D loss: 0.664645, acc: 57.03%] [G loss: 1.927124]\n",
      "epoch:23 step:21830 [D loss: 0.673887, acc: 58.59%] [G loss: 1.837829]\n",
      "epoch:23 step:21831 [D loss: 0.633955, acc: 64.84%] [G loss: 1.806903]\n",
      "epoch:23 step:21832 [D loss: 0.703889, acc: 55.47%] [G loss: 1.756398]\n",
      "epoch:23 step:21833 [D loss: 0.662148, acc: 61.72%] [G loss: 1.854259]\n",
      "epoch:23 step:21834 [D loss: 0.640997, acc: 61.72%] [G loss: 1.813986]\n",
      "epoch:23 step:21835 [D loss: 0.633027, acc: 64.06%] [G loss: 1.843018]\n",
      "epoch:23 step:21836 [D loss: 0.617774, acc: 65.62%] [G loss: 1.820700]\n",
      "epoch:23 step:21837 [D loss: 0.596868, acc: 67.97%] [G loss: 1.979813]\n",
      "epoch:23 step:21838 [D loss: 0.660912, acc: 57.03%] [G loss: 1.851682]\n",
      "epoch:23 step:21839 [D loss: 0.632123, acc: 64.84%] [G loss: 1.853205]\n",
      "epoch:23 step:21840 [D loss: 0.645984, acc: 61.72%] [G loss: 1.922750]\n",
      "epoch:23 step:21841 [D loss: 0.693903, acc: 54.69%] [G loss: 1.822213]\n",
      "epoch:23 step:21842 [D loss: 0.641003, acc: 60.94%] [G loss: 1.805515]\n",
      "epoch:23 step:21843 [D loss: 0.638817, acc: 64.06%] [G loss: 1.833890]\n",
      "epoch:23 step:21844 [D loss: 0.636561, acc: 66.41%] [G loss: 1.938629]\n",
      "epoch:23 step:21845 [D loss: 0.616715, acc: 70.31%] [G loss: 1.794020]\n",
      "epoch:23 step:21846 [D loss: 0.641997, acc: 63.28%] [G loss: 1.937345]\n",
      "epoch:23 step:21847 [D loss: 0.628986, acc: 66.41%] [G loss: 1.911941]\n",
      "epoch:23 step:21848 [D loss: 0.655359, acc: 66.41%] [G loss: 1.989842]\n",
      "epoch:23 step:21849 [D loss: 0.660448, acc: 58.59%] [G loss: 1.989593]\n",
      "epoch:23 step:21850 [D loss: 0.595930, acc: 68.75%] [G loss: 1.955971]\n",
      "epoch:23 step:21851 [D loss: 0.675898, acc: 53.91%] [G loss: 1.841028]\n",
      "epoch:23 step:21852 [D loss: 0.720340, acc: 57.81%] [G loss: 1.796503]\n",
      "epoch:23 step:21853 [D loss: 0.645225, acc: 64.84%] [G loss: 1.927060]\n",
      "epoch:23 step:21854 [D loss: 0.640553, acc: 57.03%] [G loss: 1.777964]\n",
      "epoch:23 step:21855 [D loss: 0.650916, acc: 57.81%] [G loss: 1.737727]\n",
      "epoch:23 step:21856 [D loss: 0.702882, acc: 50.78%] [G loss: 1.684912]\n",
      "epoch:23 step:21857 [D loss: 0.718146, acc: 54.69%] [G loss: 1.769236]\n",
      "epoch:23 step:21858 [D loss: 0.647441, acc: 60.94%] [G loss: 1.927180]\n",
      "epoch:23 step:21859 [D loss: 0.683599, acc: 66.41%] [G loss: 1.727344]\n",
      "epoch:23 step:21860 [D loss: 0.624206, acc: 64.06%] [G loss: 1.792793]\n",
      "epoch:23 step:21861 [D loss: 0.617176, acc: 66.41%] [G loss: 1.856624]\n",
      "epoch:23 step:21862 [D loss: 0.654409, acc: 60.16%] [G loss: 1.919770]\n",
      "epoch:23 step:21863 [D loss: 0.562109, acc: 76.56%] [G loss: 1.995121]\n",
      "epoch:23 step:21864 [D loss: 0.616461, acc: 75.00%] [G loss: 2.148499]\n",
      "epoch:23 step:21865 [D loss: 0.627531, acc: 69.53%] [G loss: 2.066259]\n",
      "epoch:23 step:21866 [D loss: 0.616223, acc: 65.62%] [G loss: 2.154399]\n",
      "epoch:23 step:21867 [D loss: 0.701350, acc: 57.81%] [G loss: 1.718971]\n",
      "epoch:23 step:21868 [D loss: 0.631748, acc: 66.41%] [G loss: 1.772547]\n",
      "epoch:23 step:21869 [D loss: 0.656139, acc: 70.31%] [G loss: 2.063711]\n",
      "epoch:23 step:21870 [D loss: 0.645823, acc: 71.09%] [G loss: 1.756483]\n",
      "epoch:23 step:21871 [D loss: 0.672865, acc: 58.59%] [G loss: 1.868467]\n",
      "epoch:23 step:21872 [D loss: 0.670796, acc: 61.72%] [G loss: 2.080958]\n",
      "epoch:23 step:21873 [D loss: 0.690747, acc: 57.81%] [G loss: 1.761985]\n",
      "epoch:23 step:21874 [D loss: 0.710148, acc: 57.81%] [G loss: 1.710605]\n",
      "epoch:23 step:21875 [D loss: 0.658868, acc: 58.59%] [G loss: 1.877465]\n",
      "epoch:23 step:21876 [D loss: 0.611116, acc: 69.53%] [G loss: 1.856521]\n",
      "epoch:23 step:21877 [D loss: 0.641894, acc: 64.84%] [G loss: 1.814379]\n",
      "epoch:23 step:21878 [D loss: 0.648230, acc: 64.84%] [G loss: 1.766226]\n",
      "epoch:23 step:21879 [D loss: 0.642645, acc: 62.50%] [G loss: 1.781285]\n",
      "epoch:23 step:21880 [D loss: 0.643707, acc: 61.72%] [G loss: 1.888851]\n",
      "epoch:23 step:21881 [D loss: 0.606761, acc: 66.41%] [G loss: 1.926963]\n",
      "epoch:23 step:21882 [D loss: 0.598366, acc: 70.31%] [G loss: 1.943785]\n",
      "epoch:23 step:21883 [D loss: 0.632669, acc: 64.06%] [G loss: 1.936910]\n",
      "epoch:23 step:21884 [D loss: 0.646156, acc: 58.59%] [G loss: 1.961673]\n",
      "epoch:23 step:21885 [D loss: 0.630613, acc: 63.28%] [G loss: 2.012891]\n",
      "epoch:23 step:21886 [D loss: 0.624789, acc: 60.16%] [G loss: 1.922890]\n",
      "epoch:23 step:21887 [D loss: 0.652413, acc: 64.84%] [G loss: 1.868634]\n",
      "epoch:23 step:21888 [D loss: 0.596077, acc: 66.41%] [G loss: 1.900200]\n",
      "epoch:23 step:21889 [D loss: 0.636111, acc: 64.84%] [G loss: 2.070923]\n",
      "epoch:23 step:21890 [D loss: 0.621709, acc: 64.84%] [G loss: 2.009832]\n",
      "epoch:23 step:21891 [D loss: 0.629368, acc: 64.84%] [G loss: 1.879299]\n",
      "epoch:23 step:21892 [D loss: 0.716637, acc: 55.47%] [G loss: 1.768465]\n",
      "epoch:23 step:21893 [D loss: 0.662937, acc: 57.81%] [G loss: 1.737660]\n",
      "epoch:23 step:21894 [D loss: 0.713485, acc: 51.56%] [G loss: 1.862332]\n",
      "epoch:23 step:21895 [D loss: 0.656618, acc: 59.38%] [G loss: 1.831342]\n",
      "epoch:23 step:21896 [D loss: 0.656210, acc: 57.81%] [G loss: 2.052455]\n",
      "epoch:23 step:21897 [D loss: 0.630805, acc: 66.41%] [G loss: 2.081697]\n",
      "epoch:23 step:21898 [D loss: 0.573137, acc: 72.66%] [G loss: 2.164112]\n",
      "epoch:23 step:21899 [D loss: 0.733491, acc: 54.69%] [G loss: 1.831535]\n",
      "epoch:23 step:21900 [D loss: 0.647188, acc: 60.16%] [G loss: 1.852212]\n",
      "epoch:23 step:21901 [D loss: 0.644689, acc: 60.94%] [G loss: 1.855410]\n",
      "epoch:23 step:21902 [D loss: 0.615712, acc: 68.75%] [G loss: 1.919805]\n",
      "epoch:23 step:21903 [D loss: 0.625872, acc: 65.62%] [G loss: 1.826163]\n",
      "epoch:23 step:21904 [D loss: 0.646456, acc: 61.72%] [G loss: 1.856100]\n",
      "epoch:23 step:21905 [D loss: 0.638952, acc: 65.62%] [G loss: 2.048342]\n",
      "epoch:23 step:21906 [D loss: 0.680351, acc: 57.03%] [G loss: 1.720226]\n",
      "epoch:23 step:21907 [D loss: 0.650531, acc: 57.81%] [G loss: 1.766842]\n",
      "epoch:23 step:21908 [D loss: 0.626572, acc: 62.50%] [G loss: 1.978017]\n",
      "epoch:23 step:21909 [D loss: 0.595949, acc: 69.53%] [G loss: 2.124579]\n",
      "epoch:23 step:21910 [D loss: 0.558079, acc: 72.66%] [G loss: 1.937252]\n",
      "epoch:23 step:21911 [D loss: 0.578514, acc: 67.19%] [G loss: 2.028272]\n",
      "epoch:23 step:21912 [D loss: 0.658932, acc: 60.94%] [G loss: 1.899674]\n",
      "epoch:23 step:21913 [D loss: 0.680291, acc: 60.94%] [G loss: 1.997329]\n",
      "epoch:23 step:21914 [D loss: 0.644742, acc: 61.72%] [G loss: 1.891647]\n",
      "epoch:23 step:21915 [D loss: 0.609101, acc: 67.97%] [G loss: 2.021607]\n",
      "epoch:23 step:21916 [D loss: 0.619626, acc: 66.41%] [G loss: 1.989158]\n",
      "epoch:23 step:21917 [D loss: 0.630525, acc: 63.28%] [G loss: 1.840099]\n",
      "epoch:23 step:21918 [D loss: 0.630185, acc: 64.06%] [G loss: 1.925003]\n",
      "epoch:23 step:21919 [D loss: 0.632534, acc: 63.28%] [G loss: 1.875373]\n",
      "epoch:23 step:21920 [D loss: 0.669044, acc: 57.81%] [G loss: 1.817371]\n",
      "epoch:23 step:21921 [D loss: 0.675437, acc: 55.47%] [G loss: 1.968144]\n",
      "epoch:23 step:21922 [D loss: 0.655405, acc: 60.16%] [G loss: 1.958489]\n",
      "epoch:23 step:21923 [D loss: 0.593968, acc: 73.44%] [G loss: 1.991953]\n",
      "epoch:23 step:21924 [D loss: 0.730568, acc: 57.81%] [G loss: 1.829224]\n",
      "epoch:23 step:21925 [D loss: 0.622529, acc: 67.19%] [G loss: 2.087574]\n",
      "epoch:23 step:21926 [D loss: 0.662901, acc: 59.38%] [G loss: 1.878083]\n",
      "epoch:23 step:21927 [D loss: 0.682423, acc: 57.81%] [G loss: 1.810441]\n",
      "epoch:23 step:21928 [D loss: 0.685569, acc: 56.25%] [G loss: 1.775425]\n",
      "epoch:23 step:21929 [D loss: 0.628666, acc: 64.84%] [G loss: 1.870146]\n",
      "epoch:23 step:21930 [D loss: 0.619367, acc: 63.28%] [G loss: 1.960828]\n",
      "epoch:23 step:21931 [D loss: 0.629239, acc: 67.97%] [G loss: 1.924518]\n",
      "epoch:23 step:21932 [D loss: 0.560637, acc: 71.88%] [G loss: 2.177083]\n",
      "epoch:23 step:21933 [D loss: 0.595177, acc: 65.62%] [G loss: 2.023448]\n",
      "epoch:23 step:21934 [D loss: 0.661638, acc: 60.94%] [G loss: 1.999704]\n",
      "epoch:23 step:21935 [D loss: 0.668861, acc: 62.50%] [G loss: 2.008172]\n",
      "epoch:23 step:21936 [D loss: 0.580612, acc: 71.88%] [G loss: 1.928270]\n",
      "epoch:23 step:21937 [D loss: 0.655221, acc: 59.38%] [G loss: 1.758778]\n",
      "epoch:23 step:21938 [D loss: 0.650831, acc: 61.72%] [G loss: 1.801966]\n",
      "epoch:23 step:21939 [D loss: 0.666793, acc: 61.72%] [G loss: 1.872590]\n",
      "epoch:23 step:21940 [D loss: 0.643693, acc: 62.50%] [G loss: 1.886502]\n",
      "epoch:23 step:21941 [D loss: 0.633058, acc: 64.06%] [G loss: 1.895068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:21942 [D loss: 0.648335, acc: 57.81%] [G loss: 1.851890]\n",
      "epoch:23 step:21943 [D loss: 0.650838, acc: 65.62%] [G loss: 1.900009]\n",
      "epoch:23 step:21944 [D loss: 0.636243, acc: 61.72%] [G loss: 1.840106]\n",
      "epoch:23 step:21945 [D loss: 0.628859, acc: 64.84%] [G loss: 1.851805]\n",
      "epoch:23 step:21946 [D loss: 0.601088, acc: 65.62%] [G loss: 1.923907]\n",
      "epoch:23 step:21947 [D loss: 0.660840, acc: 59.38%] [G loss: 1.928880]\n",
      "epoch:23 step:21948 [D loss: 0.734020, acc: 52.34%] [G loss: 1.840935]\n",
      "epoch:23 step:21949 [D loss: 0.655245, acc: 59.38%] [G loss: 1.969439]\n",
      "epoch:23 step:21950 [D loss: 0.659492, acc: 67.19%] [G loss: 1.889611]\n",
      "epoch:23 step:21951 [D loss: 0.636631, acc: 60.94%] [G loss: 1.853490]\n",
      "epoch:23 step:21952 [D loss: 0.691657, acc: 54.69%] [G loss: 1.829672]\n",
      "epoch:23 step:21953 [D loss: 0.657787, acc: 58.59%] [G loss: 1.830653]\n",
      "epoch:23 step:21954 [D loss: 0.614116, acc: 67.19%] [G loss: 1.836591]\n",
      "epoch:23 step:21955 [D loss: 0.616971, acc: 68.75%] [G loss: 1.893891]\n",
      "epoch:23 step:21956 [D loss: 0.636262, acc: 62.50%] [G loss: 2.004080]\n",
      "epoch:23 step:21957 [D loss: 0.603326, acc: 66.41%] [G loss: 1.959734]\n",
      "epoch:23 step:21958 [D loss: 0.656564, acc: 60.94%] [G loss: 2.106507]\n",
      "epoch:23 step:21959 [D loss: 0.743691, acc: 51.56%] [G loss: 1.789702]\n",
      "epoch:23 step:21960 [D loss: 0.665707, acc: 57.03%] [G loss: 1.827670]\n",
      "epoch:23 step:21961 [D loss: 0.673699, acc: 55.47%] [G loss: 1.898925]\n",
      "epoch:23 step:21962 [D loss: 0.656791, acc: 57.81%] [G loss: 1.755001]\n",
      "epoch:23 step:21963 [D loss: 0.637400, acc: 64.06%] [G loss: 1.835399]\n",
      "epoch:23 step:21964 [D loss: 0.638290, acc: 65.62%] [G loss: 1.871190]\n",
      "epoch:23 step:21965 [D loss: 0.653769, acc: 60.16%] [G loss: 1.884196]\n",
      "epoch:23 step:21966 [D loss: 0.627190, acc: 64.84%] [G loss: 1.905362]\n",
      "epoch:23 step:21967 [D loss: 0.596145, acc: 69.53%] [G loss: 1.997676]\n",
      "epoch:23 step:21968 [D loss: 0.699425, acc: 52.34%] [G loss: 1.881311]\n",
      "epoch:23 step:21969 [D loss: 0.691975, acc: 55.47%] [G loss: 1.876015]\n",
      "epoch:23 step:21970 [D loss: 0.679549, acc: 56.25%] [G loss: 1.821424]\n",
      "epoch:23 step:21971 [D loss: 0.637392, acc: 64.84%] [G loss: 1.866402]\n",
      "epoch:23 step:21972 [D loss: 0.697465, acc: 57.03%] [G loss: 1.803035]\n",
      "epoch:23 step:21973 [D loss: 0.689427, acc: 55.47%] [G loss: 1.772787]\n",
      "epoch:23 step:21974 [D loss: 0.637362, acc: 63.28%] [G loss: 1.777833]\n",
      "epoch:23 step:21975 [D loss: 0.634268, acc: 63.28%] [G loss: 2.007293]\n",
      "epoch:23 step:21976 [D loss: 0.594218, acc: 73.44%] [G loss: 1.936460]\n",
      "epoch:23 step:21977 [D loss: 0.601195, acc: 66.41%] [G loss: 2.080358]\n",
      "epoch:23 step:21978 [D loss: 0.681628, acc: 59.38%] [G loss: 1.917689]\n",
      "epoch:23 step:21979 [D loss: 0.647274, acc: 66.41%] [G loss: 2.074115]\n",
      "epoch:23 step:21980 [D loss: 0.574172, acc: 75.78%] [G loss: 2.060550]\n",
      "epoch:23 step:21981 [D loss: 0.640452, acc: 62.50%] [G loss: 1.974444]\n",
      "epoch:23 step:21982 [D loss: 0.609038, acc: 67.97%] [G loss: 1.961672]\n",
      "epoch:23 step:21983 [D loss: 0.684970, acc: 57.03%] [G loss: 1.807965]\n",
      "epoch:23 step:21984 [D loss: 0.639769, acc: 57.81%] [G loss: 1.946735]\n",
      "epoch:23 step:21985 [D loss: 0.618774, acc: 64.84%] [G loss: 1.936290]\n",
      "epoch:23 step:21986 [D loss: 0.587403, acc: 68.75%] [G loss: 2.040861]\n",
      "epoch:23 step:21987 [D loss: 0.642061, acc: 67.97%] [G loss: 2.142763]\n",
      "epoch:23 step:21988 [D loss: 0.713558, acc: 55.47%] [G loss: 1.737046]\n",
      "epoch:23 step:21989 [D loss: 0.664545, acc: 61.72%] [G loss: 1.805048]\n",
      "epoch:23 step:21990 [D loss: 0.694484, acc: 53.91%] [G loss: 1.640763]\n",
      "epoch:23 step:21991 [D loss: 0.691820, acc: 59.38%] [G loss: 1.857983]\n",
      "epoch:23 step:21992 [D loss: 0.670298, acc: 53.91%] [G loss: 1.808238]\n",
      "epoch:23 step:21993 [D loss: 0.695281, acc: 54.69%] [G loss: 1.771776]\n",
      "epoch:23 step:21994 [D loss: 0.733784, acc: 52.34%] [G loss: 1.757040]\n",
      "epoch:23 step:21995 [D loss: 0.716418, acc: 47.66%] [G loss: 1.644418]\n",
      "epoch:23 step:21996 [D loss: 0.686024, acc: 50.78%] [G loss: 1.694315]\n",
      "epoch:23 step:21997 [D loss: 0.657614, acc: 61.72%] [G loss: 1.742510]\n",
      "epoch:23 step:21998 [D loss: 0.660782, acc: 58.59%] [G loss: 1.737747]\n",
      "epoch:23 step:21999 [D loss: 0.668363, acc: 59.38%] [G loss: 1.820813]\n",
      "epoch:23 step:22000 [D loss: 0.646686, acc: 68.75%] [G loss: 1.832877]\n",
      "epoch:23 step:22001 [D loss: 0.681939, acc: 62.50%] [G loss: 1.700088]\n",
      "epoch:23 step:22002 [D loss: 0.619759, acc: 63.28%] [G loss: 1.890001]\n",
      "epoch:23 step:22003 [D loss: 0.679797, acc: 67.19%] [G loss: 1.799365]\n",
      "epoch:23 step:22004 [D loss: 0.620917, acc: 67.97%] [G loss: 1.893850]\n",
      "epoch:23 step:22005 [D loss: 0.637701, acc: 64.84%] [G loss: 1.805730]\n",
      "epoch:23 step:22006 [D loss: 0.678398, acc: 53.91%] [G loss: 1.944322]\n",
      "epoch:23 step:22007 [D loss: 0.654907, acc: 61.72%] [G loss: 1.888759]\n",
      "epoch:23 step:22008 [D loss: 0.633025, acc: 62.50%] [G loss: 2.030473]\n",
      "epoch:23 step:22009 [D loss: 0.705752, acc: 53.91%] [G loss: 1.746065]\n",
      "epoch:23 step:22010 [D loss: 0.643143, acc: 61.72%] [G loss: 1.821588]\n",
      "epoch:23 step:22011 [D loss: 0.640798, acc: 60.16%] [G loss: 1.783616]\n",
      "epoch:23 step:22012 [D loss: 0.667423, acc: 55.47%] [G loss: 1.808388]\n",
      "epoch:23 step:22013 [D loss: 0.682492, acc: 58.59%] [G loss: 1.705479]\n",
      "epoch:23 step:22014 [D loss: 0.637158, acc: 62.50%] [G loss: 1.861288]\n",
      "epoch:23 step:22015 [D loss: 0.666052, acc: 60.94%] [G loss: 1.853518]\n",
      "epoch:23 step:22016 [D loss: 0.645447, acc: 65.62%] [G loss: 1.638802]\n",
      "epoch:23 step:22017 [D loss: 0.671648, acc: 58.59%] [G loss: 1.721491]\n",
      "epoch:23 step:22018 [D loss: 0.682855, acc: 60.94%] [G loss: 1.698110]\n",
      "epoch:23 step:22019 [D loss: 0.612581, acc: 71.09%] [G loss: 2.123493]\n",
      "epoch:23 step:22020 [D loss: 0.602283, acc: 68.75%] [G loss: 1.974839]\n",
      "epoch:23 step:22021 [D loss: 0.551418, acc: 77.34%] [G loss: 1.986397]\n",
      "epoch:23 step:22022 [D loss: 0.580675, acc: 67.97%] [G loss: 2.310031]\n",
      "epoch:23 step:22023 [D loss: 0.633863, acc: 62.50%] [G loss: 2.160101]\n",
      "epoch:23 step:22024 [D loss: 0.728549, acc: 53.91%] [G loss: 1.945028]\n",
      "epoch:23 step:22025 [D loss: 0.697872, acc: 57.03%] [G loss: 1.851531]\n",
      "epoch:23 step:22026 [D loss: 0.680671, acc: 60.94%] [G loss: 1.894669]\n",
      "epoch:23 step:22027 [D loss: 0.647789, acc: 59.38%] [G loss: 2.023890]\n",
      "epoch:23 step:22028 [D loss: 0.689568, acc: 54.69%] [G loss: 1.662167]\n",
      "epoch:23 step:22029 [D loss: 0.679382, acc: 59.38%] [G loss: 1.818079]\n",
      "epoch:23 step:22030 [D loss: 0.569950, acc: 71.09%] [G loss: 1.885429]\n",
      "epoch:23 step:22031 [D loss: 0.628981, acc: 64.06%] [G loss: 1.978083]\n",
      "epoch:23 step:22032 [D loss: 0.584757, acc: 71.09%] [G loss: 2.107900]\n",
      "epoch:23 step:22033 [D loss: 0.668975, acc: 54.69%] [G loss: 1.772454]\n",
      "epoch:23 step:22034 [D loss: 0.687494, acc: 61.72%] [G loss: 1.732555]\n",
      "epoch:23 step:22035 [D loss: 0.640938, acc: 58.59%] [G loss: 1.810687]\n",
      "epoch:23 step:22036 [D loss: 0.677829, acc: 59.38%] [G loss: 1.674213]\n",
      "epoch:23 step:22037 [D loss: 0.654970, acc: 64.84%] [G loss: 1.890673]\n",
      "epoch:23 step:22038 [D loss: 0.604380, acc: 65.62%] [G loss: 1.833198]\n",
      "epoch:23 step:22039 [D loss: 0.577050, acc: 70.31%] [G loss: 2.032183]\n",
      "epoch:23 step:22040 [D loss: 0.654025, acc: 64.84%] [G loss: 1.845397]\n",
      "epoch:23 step:22041 [D loss: 0.679307, acc: 57.03%] [G loss: 1.824633]\n",
      "epoch:23 step:22042 [D loss: 0.630807, acc: 67.97%] [G loss: 1.962678]\n",
      "epoch:23 step:22043 [D loss: 0.656720, acc: 64.84%] [G loss: 1.917945]\n",
      "epoch:23 step:22044 [D loss: 0.629541, acc: 64.06%] [G loss: 1.862895]\n",
      "epoch:23 step:22045 [D loss: 0.592878, acc: 72.66%] [G loss: 2.017280]\n",
      "epoch:23 step:22046 [D loss: 0.628584, acc: 60.16%] [G loss: 1.959899]\n",
      "epoch:23 step:22047 [D loss: 0.654909, acc: 65.62%] [G loss: 1.908031]\n",
      "epoch:23 step:22048 [D loss: 0.602563, acc: 70.31%] [G loss: 2.011323]\n",
      "epoch:23 step:22049 [D loss: 0.671785, acc: 60.94%] [G loss: 2.087765]\n",
      "epoch:23 step:22050 [D loss: 0.601740, acc: 68.75%] [G loss: 2.075735]\n",
      "epoch:23 step:22051 [D loss: 0.702988, acc: 50.78%] [G loss: 1.825054]\n",
      "epoch:23 step:22052 [D loss: 0.649080, acc: 60.94%] [G loss: 1.793316]\n",
      "epoch:23 step:22053 [D loss: 0.725708, acc: 50.78%] [G loss: 1.722676]\n",
      "epoch:23 step:22054 [D loss: 0.704820, acc: 59.38%] [G loss: 1.852509]\n",
      "epoch:23 step:22055 [D loss: 0.682689, acc: 59.38%] [G loss: 1.828185]\n",
      "epoch:23 step:22056 [D loss: 0.645568, acc: 61.72%] [G loss: 1.713719]\n",
      "epoch:23 step:22057 [D loss: 0.657149, acc: 59.38%] [G loss: 1.776582]\n",
      "epoch:23 step:22058 [D loss: 0.646766, acc: 58.59%] [G loss: 1.818114]\n",
      "epoch:23 step:22059 [D loss: 0.616846, acc: 71.09%] [G loss: 2.033974]\n",
      "epoch:23 step:22060 [D loss: 0.646337, acc: 61.72%] [G loss: 1.954241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22061 [D loss: 0.725223, acc: 45.31%] [G loss: 1.769438]\n",
      "epoch:23 step:22062 [D loss: 0.647176, acc: 58.59%] [G loss: 1.767306]\n",
      "epoch:23 step:22063 [D loss: 0.609845, acc: 64.84%] [G loss: 1.766717]\n",
      "epoch:23 step:22064 [D loss: 0.627559, acc: 69.53%] [G loss: 1.845038]\n",
      "epoch:23 step:22065 [D loss: 0.689541, acc: 60.94%] [G loss: 1.802683]\n",
      "epoch:23 step:22066 [D loss: 0.691803, acc: 58.59%] [G loss: 1.806546]\n",
      "epoch:23 step:22067 [D loss: 0.658456, acc: 58.59%] [G loss: 1.907856]\n",
      "epoch:23 step:22068 [D loss: 0.627376, acc: 61.72%] [G loss: 1.739611]\n",
      "epoch:23 step:22069 [D loss: 0.607605, acc: 67.97%] [G loss: 1.766562]\n",
      "epoch:23 step:22070 [D loss: 0.603473, acc: 64.06%] [G loss: 1.757425]\n",
      "epoch:23 step:22071 [D loss: 0.635212, acc: 64.84%] [G loss: 1.797701]\n",
      "epoch:23 step:22072 [D loss: 0.608832, acc: 67.19%] [G loss: 1.897484]\n",
      "epoch:23 step:22073 [D loss: 0.612739, acc: 64.06%] [G loss: 1.952790]\n",
      "epoch:23 step:22074 [D loss: 0.640237, acc: 59.38%] [G loss: 1.941777]\n",
      "epoch:23 step:22075 [D loss: 0.648994, acc: 60.94%] [G loss: 2.025723]\n",
      "epoch:23 step:22076 [D loss: 0.635375, acc: 63.28%] [G loss: 1.919730]\n",
      "epoch:23 step:22077 [D loss: 0.697780, acc: 54.69%] [G loss: 1.899405]\n",
      "epoch:23 step:22078 [D loss: 0.659015, acc: 59.38%] [G loss: 1.988972]\n",
      "epoch:23 step:22079 [D loss: 0.653043, acc: 63.28%] [G loss: 1.789915]\n",
      "epoch:23 step:22080 [D loss: 0.644856, acc: 64.06%] [G loss: 1.647518]\n",
      "epoch:23 step:22081 [D loss: 0.688330, acc: 52.34%] [G loss: 1.723618]\n",
      "epoch:23 step:22082 [D loss: 0.650113, acc: 62.50%] [G loss: 1.779310]\n",
      "epoch:23 step:22083 [D loss: 0.621990, acc: 69.53%] [G loss: 1.916939]\n",
      "epoch:23 step:22084 [D loss: 0.662728, acc: 64.84%] [G loss: 1.949586]\n",
      "epoch:23 step:22085 [D loss: 0.624229, acc: 64.06%] [G loss: 1.910406]\n",
      "epoch:23 step:22086 [D loss: 0.661268, acc: 63.28%] [G loss: 1.796058]\n",
      "epoch:23 step:22087 [D loss: 0.592096, acc: 64.84%] [G loss: 1.877552]\n",
      "epoch:23 step:22088 [D loss: 0.646598, acc: 64.06%] [G loss: 1.748998]\n",
      "epoch:23 step:22089 [D loss: 0.682602, acc: 56.25%] [G loss: 1.823080]\n",
      "epoch:23 step:22090 [D loss: 0.660759, acc: 59.38%] [G loss: 1.918429]\n",
      "epoch:23 step:22091 [D loss: 0.652363, acc: 64.06%] [G loss: 1.800790]\n",
      "epoch:23 step:22092 [D loss: 0.701575, acc: 55.47%] [G loss: 1.836592]\n",
      "epoch:23 step:22093 [D loss: 0.624374, acc: 65.62%] [G loss: 1.843483]\n",
      "epoch:23 step:22094 [D loss: 0.652159, acc: 60.16%] [G loss: 1.936262]\n",
      "epoch:23 step:22095 [D loss: 0.632879, acc: 64.84%] [G loss: 1.771081]\n",
      "epoch:23 step:22096 [D loss: 0.633705, acc: 63.28%] [G loss: 1.826247]\n",
      "epoch:23 step:22097 [D loss: 0.656964, acc: 63.28%] [G loss: 1.717210]\n",
      "epoch:23 step:22098 [D loss: 0.586827, acc: 72.66%] [G loss: 1.917816]\n",
      "epoch:23 step:22099 [D loss: 0.639898, acc: 66.41%] [G loss: 2.002023]\n",
      "epoch:23 step:22100 [D loss: 0.633653, acc: 57.03%] [G loss: 1.912540]\n",
      "epoch:23 step:22101 [D loss: 0.619514, acc: 64.84%] [G loss: 1.985399]\n",
      "epoch:23 step:22102 [D loss: 0.602528, acc: 65.62%] [G loss: 1.963988]\n",
      "epoch:23 step:22103 [D loss: 0.564024, acc: 73.44%] [G loss: 1.949506]\n",
      "epoch:23 step:22104 [D loss: 0.647396, acc: 65.62%] [G loss: 1.907711]\n",
      "epoch:23 step:22105 [D loss: 0.586222, acc: 67.19%] [G loss: 2.142591]\n",
      "epoch:23 step:22106 [D loss: 0.666663, acc: 61.72%] [G loss: 1.902639]\n",
      "epoch:23 step:22107 [D loss: 0.610670, acc: 64.84%] [G loss: 1.985190]\n",
      "epoch:23 step:22108 [D loss: 0.654093, acc: 68.75%] [G loss: 2.049265]\n",
      "epoch:23 step:22109 [D loss: 0.641848, acc: 65.62%] [G loss: 2.046602]\n",
      "epoch:23 step:22110 [D loss: 0.671880, acc: 57.03%] [G loss: 1.795107]\n",
      "epoch:23 step:22111 [D loss: 0.664291, acc: 56.25%] [G loss: 1.753380]\n",
      "epoch:23 step:22112 [D loss: 0.661789, acc: 60.94%] [G loss: 1.977601]\n",
      "epoch:23 step:22113 [D loss: 0.668664, acc: 57.81%] [G loss: 1.961539]\n",
      "epoch:23 step:22114 [D loss: 0.605467, acc: 68.75%] [G loss: 1.875036]\n",
      "epoch:23 step:22115 [D loss: 0.621907, acc: 63.28%] [G loss: 1.938125]\n",
      "epoch:23 step:22116 [D loss: 0.684699, acc: 60.94%] [G loss: 1.827186]\n",
      "epoch:23 step:22117 [D loss: 0.677763, acc: 60.16%] [G loss: 1.794296]\n",
      "epoch:23 step:22118 [D loss: 0.656462, acc: 57.03%] [G loss: 1.845714]\n",
      "epoch:23 step:22119 [D loss: 0.683491, acc: 53.91%] [G loss: 1.880352]\n",
      "epoch:23 step:22120 [D loss: 0.656590, acc: 57.81%] [G loss: 1.789668]\n",
      "epoch:23 step:22121 [D loss: 0.639950, acc: 65.62%] [G loss: 1.882306]\n",
      "epoch:23 step:22122 [D loss: 0.652207, acc: 57.03%] [G loss: 1.905853]\n",
      "epoch:23 step:22123 [D loss: 0.623629, acc: 67.19%] [G loss: 1.802437]\n",
      "epoch:23 step:22124 [D loss: 0.654147, acc: 60.16%] [G loss: 1.755194]\n",
      "epoch:23 step:22125 [D loss: 0.618422, acc: 64.84%] [G loss: 1.843116]\n",
      "epoch:23 step:22126 [D loss: 0.681229, acc: 57.03%] [G loss: 1.871102]\n",
      "epoch:23 step:22127 [D loss: 0.685756, acc: 59.38%] [G loss: 1.756710]\n",
      "epoch:23 step:22128 [D loss: 0.689889, acc: 56.25%] [G loss: 1.779702]\n",
      "epoch:23 step:22129 [D loss: 0.651844, acc: 69.53%] [G loss: 1.734053]\n",
      "epoch:23 step:22130 [D loss: 0.644736, acc: 64.84%] [G loss: 1.760218]\n",
      "epoch:23 step:22131 [D loss: 0.628670, acc: 61.72%] [G loss: 1.742138]\n",
      "epoch:23 step:22132 [D loss: 0.687697, acc: 52.34%] [G loss: 1.837229]\n",
      "epoch:23 step:22133 [D loss: 0.611623, acc: 66.41%] [G loss: 1.956430]\n",
      "epoch:23 step:22134 [D loss: 0.690287, acc: 56.25%] [G loss: 1.957006]\n",
      "epoch:23 step:22135 [D loss: 0.641690, acc: 60.16%] [G loss: 1.761442]\n",
      "epoch:23 step:22136 [D loss: 0.671335, acc: 56.25%] [G loss: 1.777419]\n",
      "epoch:23 step:22137 [D loss: 0.653393, acc: 62.50%] [G loss: 1.831295]\n",
      "epoch:23 step:22138 [D loss: 0.677951, acc: 62.50%] [G loss: 1.883803]\n",
      "epoch:23 step:22139 [D loss: 0.642050, acc: 60.16%] [G loss: 1.882860]\n",
      "epoch:23 step:22140 [D loss: 0.601440, acc: 68.75%] [G loss: 1.955602]\n",
      "epoch:23 step:22141 [D loss: 0.649750, acc: 63.28%] [G loss: 1.872705]\n",
      "epoch:23 step:22142 [D loss: 0.639678, acc: 60.94%] [G loss: 1.913837]\n",
      "epoch:23 step:22143 [D loss: 0.619648, acc: 62.50%] [G loss: 1.985246]\n",
      "epoch:23 step:22144 [D loss: 0.637626, acc: 62.50%] [G loss: 1.894839]\n",
      "epoch:23 step:22145 [D loss: 0.645293, acc: 61.72%] [G loss: 1.812051]\n",
      "epoch:23 step:22146 [D loss: 0.652696, acc: 64.84%] [G loss: 1.883415]\n",
      "epoch:23 step:22147 [D loss: 0.663193, acc: 61.72%] [G loss: 1.877811]\n",
      "epoch:23 step:22148 [D loss: 0.682811, acc: 60.94%] [G loss: 1.729636]\n",
      "epoch:23 step:22149 [D loss: 0.646426, acc: 66.41%] [G loss: 1.986217]\n",
      "epoch:23 step:22150 [D loss: 0.668719, acc: 57.81%] [G loss: 1.792763]\n",
      "epoch:23 step:22151 [D loss: 0.653833, acc: 67.19%] [G loss: 1.842437]\n",
      "epoch:23 step:22152 [D loss: 0.597082, acc: 64.84%] [G loss: 1.910411]\n",
      "epoch:23 step:22153 [D loss: 0.724924, acc: 50.00%] [G loss: 1.881236]\n",
      "epoch:23 step:22154 [D loss: 0.653326, acc: 63.28%] [G loss: 1.952048]\n",
      "epoch:23 step:22155 [D loss: 0.636082, acc: 61.72%] [G loss: 1.835794]\n",
      "epoch:23 step:22156 [D loss: 0.642768, acc: 58.59%] [G loss: 1.986503]\n",
      "epoch:23 step:22157 [D loss: 0.683071, acc: 46.09%] [G loss: 1.921798]\n",
      "epoch:23 step:22158 [D loss: 0.651922, acc: 64.06%] [G loss: 1.930855]\n",
      "epoch:23 step:22159 [D loss: 0.645742, acc: 61.72%] [G loss: 1.942088]\n",
      "epoch:23 step:22160 [D loss: 0.613137, acc: 68.75%] [G loss: 1.859700]\n",
      "epoch:23 step:22161 [D loss: 0.597075, acc: 71.09%] [G loss: 1.888086]\n",
      "epoch:23 step:22162 [D loss: 0.647806, acc: 65.62%] [G loss: 1.788666]\n",
      "epoch:23 step:22163 [D loss: 0.646925, acc: 62.50%] [G loss: 1.916376]\n",
      "epoch:23 step:22164 [D loss: 0.663707, acc: 57.03%] [G loss: 1.801886]\n",
      "epoch:23 step:22165 [D loss: 0.694993, acc: 51.56%] [G loss: 1.732804]\n",
      "epoch:23 step:22166 [D loss: 0.678508, acc: 60.94%] [G loss: 1.725619]\n",
      "epoch:23 step:22167 [D loss: 0.632133, acc: 63.28%] [G loss: 1.872275]\n",
      "epoch:23 step:22168 [D loss: 0.699590, acc: 53.91%] [G loss: 1.911548]\n",
      "epoch:23 step:22169 [D loss: 0.661576, acc: 59.38%] [G loss: 1.854317]\n",
      "epoch:23 step:22170 [D loss: 0.619838, acc: 68.75%] [G loss: 1.858984]\n",
      "epoch:23 step:22171 [D loss: 0.643582, acc: 64.06%] [G loss: 1.763981]\n",
      "epoch:23 step:22172 [D loss: 0.673015, acc: 64.06%] [G loss: 1.844071]\n",
      "epoch:23 step:22173 [D loss: 0.675960, acc: 63.28%] [G loss: 1.820978]\n",
      "epoch:23 step:22174 [D loss: 0.619386, acc: 69.53%] [G loss: 1.827730]\n",
      "epoch:23 step:22175 [D loss: 0.615650, acc: 63.28%] [G loss: 2.209279]\n",
      "epoch:23 step:22176 [D loss: 0.643578, acc: 63.28%] [G loss: 1.786894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22177 [D loss: 0.705242, acc: 54.69%] [G loss: 1.849235]\n",
      "epoch:23 step:22178 [D loss: 0.657171, acc: 60.16%] [G loss: 1.874338]\n",
      "epoch:23 step:22179 [D loss: 0.648791, acc: 64.06%] [G loss: 1.746343]\n",
      "epoch:23 step:22180 [D loss: 0.618917, acc: 60.94%] [G loss: 1.836032]\n",
      "epoch:23 step:22181 [D loss: 0.592720, acc: 71.88%] [G loss: 1.860838]\n",
      "epoch:23 step:22182 [D loss: 0.620350, acc: 65.62%] [G loss: 2.044271]\n",
      "epoch:23 step:22183 [D loss: 0.576894, acc: 71.09%] [G loss: 1.902152]\n",
      "epoch:23 step:22184 [D loss: 0.631642, acc: 62.50%] [G loss: 1.942093]\n",
      "epoch:23 step:22185 [D loss: 0.643932, acc: 62.50%] [G loss: 2.068278]\n",
      "epoch:23 step:22186 [D loss: 0.613942, acc: 65.62%] [G loss: 2.002397]\n",
      "epoch:23 step:22187 [D loss: 0.665084, acc: 64.84%] [G loss: 1.961532]\n",
      "epoch:23 step:22188 [D loss: 0.634118, acc: 61.72%] [G loss: 2.142914]\n",
      "epoch:23 step:22189 [D loss: 0.615563, acc: 62.50%] [G loss: 1.965592]\n",
      "epoch:23 step:22190 [D loss: 0.631628, acc: 62.50%] [G loss: 1.902757]\n",
      "epoch:23 step:22191 [D loss: 0.638528, acc: 61.72%] [G loss: 1.796427]\n",
      "epoch:23 step:22192 [D loss: 0.596941, acc: 68.75%] [G loss: 2.007803]\n",
      "epoch:23 step:22193 [D loss: 0.661076, acc: 66.41%] [G loss: 2.027709]\n",
      "epoch:23 step:22194 [D loss: 0.598907, acc: 67.97%] [G loss: 2.092239]\n",
      "epoch:23 step:22195 [D loss: 0.636227, acc: 61.72%] [G loss: 1.878902]\n",
      "epoch:23 step:22196 [D loss: 0.641169, acc: 62.50%] [G loss: 1.848529]\n",
      "epoch:23 step:22197 [D loss: 0.638903, acc: 66.41%] [G loss: 1.941065]\n",
      "epoch:23 step:22198 [D loss: 0.617308, acc: 66.41%] [G loss: 1.988148]\n",
      "epoch:23 step:22199 [D loss: 0.582206, acc: 69.53%] [G loss: 2.443323]\n",
      "epoch:23 step:22200 [D loss: 0.638103, acc: 57.81%] [G loss: 2.122695]\n",
      "epoch:23 step:22201 [D loss: 0.636135, acc: 63.28%] [G loss: 2.039312]\n",
      "epoch:23 step:22202 [D loss: 0.667665, acc: 58.59%] [G loss: 2.037825]\n",
      "epoch:23 step:22203 [D loss: 0.662608, acc: 64.84%] [G loss: 2.008042]\n",
      "epoch:23 step:22204 [D loss: 0.667009, acc: 64.06%] [G loss: 1.972254]\n",
      "epoch:23 step:22205 [D loss: 0.593909, acc: 71.88%] [G loss: 2.098690]\n",
      "epoch:23 step:22206 [D loss: 0.643503, acc: 62.50%] [G loss: 1.838576]\n",
      "epoch:23 step:22207 [D loss: 0.662142, acc: 60.94%] [G loss: 1.878216]\n",
      "epoch:23 step:22208 [D loss: 0.687327, acc: 54.69%] [G loss: 1.848230]\n",
      "epoch:23 step:22209 [D loss: 0.661393, acc: 56.25%] [G loss: 1.808751]\n",
      "epoch:23 step:22210 [D loss: 0.690100, acc: 55.47%] [G loss: 1.811639]\n",
      "epoch:23 step:22211 [D loss: 0.608999, acc: 68.75%] [G loss: 1.856029]\n",
      "epoch:23 step:22212 [D loss: 0.598629, acc: 71.09%] [G loss: 1.910224]\n",
      "epoch:23 step:22213 [D loss: 0.673678, acc: 60.94%] [G loss: 1.872327]\n",
      "epoch:23 step:22214 [D loss: 0.670369, acc: 61.72%] [G loss: 1.905437]\n",
      "epoch:23 step:22215 [D loss: 0.675248, acc: 57.03%] [G loss: 1.734299]\n",
      "epoch:23 step:22216 [D loss: 0.661841, acc: 60.16%] [G loss: 1.876638]\n",
      "epoch:23 step:22217 [D loss: 0.646685, acc: 60.16%] [G loss: 1.854870]\n",
      "epoch:23 step:22218 [D loss: 0.701579, acc: 59.38%] [G loss: 1.758877]\n",
      "epoch:23 step:22219 [D loss: 0.639028, acc: 58.59%] [G loss: 1.873872]\n",
      "epoch:23 step:22220 [D loss: 0.649588, acc: 56.25%] [G loss: 1.805826]\n",
      "epoch:23 step:22221 [D loss: 0.664025, acc: 58.59%] [G loss: 1.753015]\n",
      "epoch:23 step:22222 [D loss: 0.603969, acc: 68.75%] [G loss: 1.925109]\n",
      "epoch:23 step:22223 [D loss: 0.686709, acc: 54.69%] [G loss: 1.904383]\n",
      "epoch:23 step:22224 [D loss: 0.657138, acc: 61.72%] [G loss: 1.783018]\n",
      "epoch:23 step:22225 [D loss: 0.626860, acc: 60.94%] [G loss: 1.857016]\n",
      "epoch:23 step:22226 [D loss: 0.668080, acc: 56.25%] [G loss: 1.769049]\n",
      "epoch:23 step:22227 [D loss: 0.633832, acc: 64.84%] [G loss: 1.882690]\n",
      "epoch:23 step:22228 [D loss: 0.674345, acc: 60.16%] [G loss: 1.879078]\n",
      "epoch:23 step:22229 [D loss: 0.677905, acc: 55.47%] [G loss: 1.840766]\n",
      "epoch:23 step:22230 [D loss: 0.660700, acc: 58.59%] [G loss: 1.850894]\n",
      "epoch:23 step:22231 [D loss: 0.594704, acc: 63.28%] [G loss: 1.806825]\n",
      "epoch:23 step:22232 [D loss: 0.608622, acc: 67.19%] [G loss: 1.967533]\n",
      "epoch:23 step:22233 [D loss: 0.633140, acc: 61.72%] [G loss: 1.794280]\n",
      "epoch:23 step:22234 [D loss: 0.646709, acc: 60.94%] [G loss: 1.822340]\n",
      "epoch:23 step:22235 [D loss: 0.688731, acc: 56.25%] [G loss: 1.844522]\n",
      "epoch:23 step:22236 [D loss: 0.609415, acc: 67.19%] [G loss: 1.954172]\n",
      "epoch:23 step:22237 [D loss: 0.676563, acc: 60.94%] [G loss: 1.885251]\n",
      "epoch:23 step:22238 [D loss: 0.607471, acc: 66.41%] [G loss: 2.022941]\n",
      "epoch:23 step:22239 [D loss: 0.637195, acc: 64.06%] [G loss: 1.984436]\n",
      "epoch:23 step:22240 [D loss: 0.640777, acc: 67.19%] [G loss: 1.947556]\n",
      "epoch:23 step:22241 [D loss: 0.649314, acc: 60.16%] [G loss: 1.982442]\n",
      "epoch:23 step:22242 [D loss: 0.592166, acc: 69.53%] [G loss: 2.049498]\n",
      "epoch:23 step:22243 [D loss: 0.618044, acc: 66.41%] [G loss: 2.096144]\n",
      "epoch:23 step:22244 [D loss: 0.618714, acc: 67.97%] [G loss: 1.988930]\n",
      "epoch:23 step:22245 [D loss: 0.580576, acc: 65.62%] [G loss: 2.103272]\n",
      "epoch:23 step:22246 [D loss: 0.645332, acc: 58.59%] [G loss: 1.919893]\n",
      "epoch:23 step:22247 [D loss: 0.640761, acc: 62.50%] [G loss: 2.055602]\n",
      "epoch:23 step:22248 [D loss: 0.631140, acc: 65.62%] [G loss: 1.949650]\n",
      "epoch:23 step:22249 [D loss: 0.643006, acc: 64.06%] [G loss: 1.973814]\n",
      "epoch:23 step:22250 [D loss: 0.595211, acc: 69.53%] [G loss: 2.163564]\n",
      "epoch:23 step:22251 [D loss: 0.636280, acc: 64.06%] [G loss: 2.033916]\n",
      "epoch:23 step:22252 [D loss: 0.665062, acc: 61.72%] [G loss: 1.973283]\n",
      "epoch:23 step:22253 [D loss: 0.649981, acc: 65.62%] [G loss: 1.753966]\n",
      "epoch:23 step:22254 [D loss: 0.636482, acc: 64.84%] [G loss: 1.918477]\n",
      "epoch:23 step:22255 [D loss: 0.692796, acc: 53.12%] [G loss: 1.798469]\n",
      "epoch:23 step:22256 [D loss: 0.718519, acc: 53.12%] [G loss: 1.847331]\n",
      "epoch:23 step:22257 [D loss: 0.602060, acc: 68.75%] [G loss: 1.931767]\n",
      "epoch:23 step:22258 [D loss: 0.688004, acc: 53.12%] [G loss: 1.867038]\n",
      "epoch:23 step:22259 [D loss: 0.583954, acc: 73.44%] [G loss: 2.032994]\n",
      "epoch:23 step:22260 [D loss: 0.646172, acc: 60.94%] [G loss: 1.908948]\n",
      "epoch:23 step:22261 [D loss: 0.700386, acc: 57.81%] [G loss: 1.883981]\n",
      "epoch:23 step:22262 [D loss: 0.655163, acc: 57.03%] [G loss: 1.916308]\n",
      "epoch:23 step:22263 [D loss: 0.667553, acc: 62.50%] [G loss: 2.013680]\n",
      "epoch:23 step:22264 [D loss: 0.679850, acc: 56.25%] [G loss: 1.777334]\n",
      "epoch:23 step:22265 [D loss: 0.654383, acc: 59.38%] [G loss: 1.800743]\n",
      "epoch:23 step:22266 [D loss: 0.621457, acc: 65.62%] [G loss: 1.755007]\n",
      "epoch:23 step:22267 [D loss: 0.643603, acc: 62.50%] [G loss: 1.770858]\n",
      "epoch:23 step:22268 [D loss: 0.668628, acc: 64.06%] [G loss: 1.787062]\n",
      "epoch:23 step:22269 [D loss: 0.689466, acc: 57.03%] [G loss: 1.839111]\n",
      "epoch:23 step:22270 [D loss: 0.618184, acc: 68.75%] [G loss: 1.968577]\n",
      "epoch:23 step:22271 [D loss: 0.643301, acc: 64.06%] [G loss: 1.857077]\n",
      "epoch:23 step:22272 [D loss: 0.646320, acc: 63.28%] [G loss: 1.853618]\n",
      "epoch:23 step:22273 [D loss: 0.676483, acc: 53.12%] [G loss: 1.879310]\n",
      "epoch:23 step:22274 [D loss: 0.691676, acc: 55.47%] [G loss: 1.851303]\n",
      "epoch:23 step:22275 [D loss: 0.670552, acc: 60.16%] [G loss: 1.789610]\n",
      "epoch:23 step:22276 [D loss: 0.652476, acc: 62.50%] [G loss: 2.095700]\n",
      "epoch:23 step:22277 [D loss: 0.604748, acc: 71.88%] [G loss: 1.857078]\n",
      "epoch:23 step:22278 [D loss: 0.692426, acc: 57.81%] [G loss: 1.803253]\n",
      "epoch:23 step:22279 [D loss: 0.624177, acc: 64.06%] [G loss: 1.937474]\n",
      "epoch:23 step:22280 [D loss: 0.661007, acc: 57.81%] [G loss: 1.872136]\n",
      "epoch:23 step:22281 [D loss: 0.635221, acc: 64.84%] [G loss: 1.774357]\n",
      "epoch:23 step:22282 [D loss: 0.660637, acc: 55.47%] [G loss: 1.851979]\n",
      "epoch:23 step:22283 [D loss: 0.641116, acc: 61.72%] [G loss: 1.858008]\n",
      "epoch:23 step:22284 [D loss: 0.643817, acc: 61.72%] [G loss: 2.129666]\n",
      "epoch:23 step:22285 [D loss: 0.669864, acc: 59.38%] [G loss: 1.776197]\n",
      "epoch:23 step:22286 [D loss: 0.639424, acc: 64.06%] [G loss: 1.890080]\n",
      "epoch:23 step:22287 [D loss: 0.684029, acc: 60.16%] [G loss: 1.869897]\n",
      "epoch:23 step:22288 [D loss: 0.632910, acc: 64.06%] [G loss: 1.744263]\n",
      "epoch:23 step:22289 [D loss: 0.642596, acc: 61.72%] [G loss: 1.830148]\n",
      "epoch:23 step:22290 [D loss: 0.671858, acc: 58.59%] [G loss: 1.699852]\n",
      "epoch:23 step:22291 [D loss: 0.635019, acc: 62.50%] [G loss: 1.865533]\n",
      "epoch:23 step:22292 [D loss: 0.678625, acc: 53.91%] [G loss: 1.804014]\n",
      "epoch:23 step:22293 [D loss: 0.640526, acc: 64.06%] [G loss: 1.969172]\n",
      "epoch:23 step:22294 [D loss: 0.621994, acc: 64.84%] [G loss: 1.904719]\n",
      "epoch:23 step:22295 [D loss: 0.615297, acc: 67.19%] [G loss: 1.881371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22296 [D loss: 0.659187, acc: 62.50%] [G loss: 1.882096]\n",
      "epoch:23 step:22297 [D loss: 0.631392, acc: 71.09%] [G loss: 2.024873]\n",
      "epoch:23 step:22298 [D loss: 0.623416, acc: 67.19%] [G loss: 1.889255]\n",
      "epoch:23 step:22299 [D loss: 0.649191, acc: 61.72%] [G loss: 1.864185]\n",
      "epoch:23 step:22300 [D loss: 0.676474, acc: 58.59%] [G loss: 1.862728]\n",
      "epoch:23 step:22301 [D loss: 0.608222, acc: 68.75%] [G loss: 1.783895]\n",
      "epoch:23 step:22302 [D loss: 0.674603, acc: 60.16%] [G loss: 1.813965]\n",
      "epoch:23 step:22303 [D loss: 0.695752, acc: 54.69%] [G loss: 1.777403]\n",
      "epoch:23 step:22304 [D loss: 0.631586, acc: 65.62%] [G loss: 1.867840]\n",
      "epoch:23 step:22305 [D loss: 0.680161, acc: 59.38%] [G loss: 1.831140]\n",
      "epoch:23 step:22306 [D loss: 0.660832, acc: 61.72%] [G loss: 1.877486]\n",
      "epoch:23 step:22307 [D loss: 0.660272, acc: 54.69%] [G loss: 1.860987]\n",
      "epoch:23 step:22308 [D loss: 0.634557, acc: 63.28%] [G loss: 1.773655]\n",
      "epoch:23 step:22309 [D loss: 0.680383, acc: 53.91%] [G loss: 1.829564]\n",
      "epoch:23 step:22310 [D loss: 0.659510, acc: 60.94%] [G loss: 1.851002]\n",
      "epoch:23 step:22311 [D loss: 0.672227, acc: 60.94%] [G loss: 1.748088]\n",
      "epoch:23 step:22312 [D loss: 0.624709, acc: 67.19%] [G loss: 1.768410]\n",
      "epoch:23 step:22313 [D loss: 0.650802, acc: 59.38%] [G loss: 1.749439]\n",
      "epoch:23 step:22314 [D loss: 0.611213, acc: 65.62%] [G loss: 1.898080]\n",
      "epoch:23 step:22315 [D loss: 0.660012, acc: 63.28%] [G loss: 1.809425]\n",
      "epoch:23 step:22316 [D loss: 0.689574, acc: 55.47%] [G loss: 1.779204]\n",
      "epoch:23 step:22317 [D loss: 0.628028, acc: 64.06%] [G loss: 1.784567]\n",
      "epoch:23 step:22318 [D loss: 0.664590, acc: 57.03%] [G loss: 1.916253]\n",
      "epoch:23 step:22319 [D loss: 0.599006, acc: 68.75%] [G loss: 1.858554]\n",
      "epoch:23 step:22320 [D loss: 0.658428, acc: 60.94%] [G loss: 1.863611]\n",
      "epoch:23 step:22321 [D loss: 0.642747, acc: 57.81%] [G loss: 1.934815]\n",
      "epoch:23 step:22322 [D loss: 0.631168, acc: 63.28%] [G loss: 1.837785]\n",
      "epoch:23 step:22323 [D loss: 0.685875, acc: 55.47%] [G loss: 2.038317]\n",
      "epoch:23 step:22324 [D loss: 0.671766, acc: 59.38%] [G loss: 1.771194]\n",
      "epoch:23 step:22325 [D loss: 0.611640, acc: 66.41%] [G loss: 2.062514]\n",
      "epoch:23 step:22326 [D loss: 0.618237, acc: 67.97%] [G loss: 2.178668]\n",
      "epoch:23 step:22327 [D loss: 0.633284, acc: 66.41%] [G loss: 1.914267]\n",
      "epoch:23 step:22328 [D loss: 0.652909, acc: 63.28%] [G loss: 1.824035]\n",
      "epoch:23 step:22329 [D loss: 0.618197, acc: 62.50%] [G loss: 1.864525]\n",
      "epoch:23 step:22330 [D loss: 0.661621, acc: 63.28%] [G loss: 1.935202]\n",
      "epoch:23 step:22331 [D loss: 0.646145, acc: 58.59%] [G loss: 1.898728]\n",
      "epoch:23 step:22332 [D loss: 0.648740, acc: 64.06%] [G loss: 2.017866]\n",
      "epoch:23 step:22333 [D loss: 0.654840, acc: 63.28%] [G loss: 2.005907]\n",
      "epoch:23 step:22334 [D loss: 0.644991, acc: 61.72%] [G loss: 1.912210]\n",
      "epoch:23 step:22335 [D loss: 0.641978, acc: 64.06%] [G loss: 1.818838]\n",
      "epoch:23 step:22336 [D loss: 0.646249, acc: 62.50%] [G loss: 1.886167]\n",
      "epoch:23 step:22337 [D loss: 0.635901, acc: 68.75%] [G loss: 2.020259]\n",
      "epoch:23 step:22338 [D loss: 0.724322, acc: 57.03%] [G loss: 1.889002]\n",
      "epoch:23 step:22339 [D loss: 0.662006, acc: 61.72%] [G loss: 1.887485]\n",
      "epoch:23 step:22340 [D loss: 0.646482, acc: 63.28%] [G loss: 1.896987]\n",
      "epoch:23 step:22341 [D loss: 0.577734, acc: 69.53%] [G loss: 1.937122]\n",
      "epoch:23 step:22342 [D loss: 0.678767, acc: 55.47%] [G loss: 1.802819]\n",
      "epoch:23 step:22343 [D loss: 0.584478, acc: 72.66%] [G loss: 1.925317]\n",
      "epoch:23 step:22344 [D loss: 0.615367, acc: 74.22%] [G loss: 1.991268]\n",
      "epoch:23 step:22345 [D loss: 0.771816, acc: 51.56%] [G loss: 1.691325]\n",
      "epoch:23 step:22346 [D loss: 0.698123, acc: 52.34%] [G loss: 1.754595]\n",
      "epoch:23 step:22347 [D loss: 0.643331, acc: 61.72%] [G loss: 1.819446]\n",
      "epoch:23 step:22348 [D loss: 0.634175, acc: 60.94%] [G loss: 1.792552]\n",
      "epoch:23 step:22349 [D loss: 0.706065, acc: 56.25%] [G loss: 1.718609]\n",
      "epoch:23 step:22350 [D loss: 0.674994, acc: 60.94%] [G loss: 1.714002]\n",
      "epoch:23 step:22351 [D loss: 0.680073, acc: 60.94%] [G loss: 1.728256]\n",
      "epoch:23 step:22352 [D loss: 0.692057, acc: 58.59%] [G loss: 1.629089]\n",
      "epoch:23 step:22353 [D loss: 0.645834, acc: 59.38%] [G loss: 1.708145]\n",
      "epoch:23 step:22354 [D loss: 0.629550, acc: 66.41%] [G loss: 1.786801]\n",
      "epoch:23 step:22355 [D loss: 0.600450, acc: 64.84%] [G loss: 1.946695]\n",
      "epoch:23 step:22356 [D loss: 0.631792, acc: 66.41%] [G loss: 1.729104]\n",
      "epoch:23 step:22357 [D loss: 0.608581, acc: 65.62%] [G loss: 1.770858]\n",
      "epoch:23 step:22358 [D loss: 0.642847, acc: 65.62%] [G loss: 1.786555]\n",
      "epoch:23 step:22359 [D loss: 0.619978, acc: 65.62%] [G loss: 1.852993]\n",
      "epoch:23 step:22360 [D loss: 0.637324, acc: 60.16%] [G loss: 1.774527]\n",
      "epoch:23 step:22361 [D loss: 0.667767, acc: 58.59%] [G loss: 1.967574]\n",
      "epoch:23 step:22362 [D loss: 0.648436, acc: 63.28%] [G loss: 1.787247]\n",
      "epoch:23 step:22363 [D loss: 0.602258, acc: 67.19%] [G loss: 1.781006]\n",
      "epoch:23 step:22364 [D loss: 0.665494, acc: 57.03%] [G loss: 1.768761]\n",
      "epoch:23 step:22365 [D loss: 0.650390, acc: 59.38%] [G loss: 1.785796]\n",
      "epoch:23 step:22366 [D loss: 0.617926, acc: 64.84%] [G loss: 2.062461]\n",
      "epoch:23 step:22367 [D loss: 0.626816, acc: 62.50%] [G loss: 1.845681]\n",
      "epoch:23 step:22368 [D loss: 0.682517, acc: 60.16%] [G loss: 1.840288]\n",
      "epoch:23 step:22369 [D loss: 0.680322, acc: 56.25%] [G loss: 1.846911]\n",
      "epoch:23 step:22370 [D loss: 0.625763, acc: 64.84%] [G loss: 1.876850]\n",
      "epoch:23 step:22371 [D loss: 0.701949, acc: 55.47%] [G loss: 1.683764]\n",
      "epoch:23 step:22372 [D loss: 0.647404, acc: 64.84%] [G loss: 1.766011]\n",
      "epoch:23 step:22373 [D loss: 0.650840, acc: 63.28%] [G loss: 1.833196]\n",
      "epoch:23 step:22374 [D loss: 0.668046, acc: 55.47%] [G loss: 1.887011]\n",
      "epoch:23 step:22375 [D loss: 0.690089, acc: 58.59%] [G loss: 1.750268]\n",
      "epoch:23 step:22376 [D loss: 0.623993, acc: 64.06%] [G loss: 1.963946]\n",
      "epoch:23 step:22377 [D loss: 0.682109, acc: 58.59%] [G loss: 1.846585]\n",
      "epoch:23 step:22378 [D loss: 0.610631, acc: 67.97%] [G loss: 1.829291]\n",
      "epoch:23 step:22379 [D loss: 0.629536, acc: 64.06%] [G loss: 1.885802]\n",
      "epoch:23 step:22380 [D loss: 0.677446, acc: 55.47%] [G loss: 1.800266]\n",
      "epoch:23 step:22381 [D loss: 0.680352, acc: 57.81%] [G loss: 1.835953]\n",
      "epoch:23 step:22382 [D loss: 0.619653, acc: 63.28%] [G loss: 1.789057]\n",
      "epoch:23 step:22383 [D loss: 0.670340, acc: 58.59%] [G loss: 1.907213]\n",
      "epoch:23 step:22384 [D loss: 0.673691, acc: 62.50%] [G loss: 1.918512]\n",
      "epoch:23 step:22385 [D loss: 0.635279, acc: 61.72%] [G loss: 1.900270]\n",
      "epoch:23 step:22386 [D loss: 0.646522, acc: 64.84%] [G loss: 1.927966]\n",
      "epoch:23 step:22387 [D loss: 0.592075, acc: 68.75%] [G loss: 1.861665]\n",
      "epoch:23 step:22388 [D loss: 0.624334, acc: 65.62%] [G loss: 1.849908]\n",
      "epoch:23 step:22389 [D loss: 0.650050, acc: 64.06%] [G loss: 1.916272]\n",
      "epoch:23 step:22390 [D loss: 0.663270, acc: 62.50%] [G loss: 1.825942]\n",
      "epoch:23 step:22391 [D loss: 0.651647, acc: 57.81%] [G loss: 1.786501]\n",
      "epoch:23 step:22392 [D loss: 0.632120, acc: 64.84%] [G loss: 1.821279]\n",
      "epoch:23 step:22393 [D loss: 0.607551, acc: 63.28%] [G loss: 1.976784]\n",
      "epoch:23 step:22394 [D loss: 0.635331, acc: 64.06%] [G loss: 1.821169]\n",
      "epoch:23 step:22395 [D loss: 0.618469, acc: 59.38%] [G loss: 2.014297]\n",
      "epoch:23 step:22396 [D loss: 0.617954, acc: 69.53%] [G loss: 2.070888]\n",
      "epoch:23 step:22397 [D loss: 0.660935, acc: 56.25%] [G loss: 1.900657]\n",
      "epoch:23 step:22398 [D loss: 0.619004, acc: 66.41%] [G loss: 1.904543]\n",
      "epoch:23 step:22399 [D loss: 0.682332, acc: 60.16%] [G loss: 1.975049]\n",
      "epoch:23 step:22400 [D loss: 0.670204, acc: 58.59%] [G loss: 1.987314]\n",
      "epoch:23 step:22401 [D loss: 0.634379, acc: 65.62%] [G loss: 1.932265]\n",
      "epoch:23 step:22402 [D loss: 0.675648, acc: 59.38%] [G loss: 1.845849]\n",
      "epoch:23 step:22403 [D loss: 0.657199, acc: 56.25%] [G loss: 1.947640]\n",
      "epoch:23 step:22404 [D loss: 0.678815, acc: 57.03%] [G loss: 1.914628]\n",
      "epoch:23 step:22405 [D loss: 0.610779, acc: 68.75%] [G loss: 1.782063]\n",
      "epoch:23 step:22406 [D loss: 0.677435, acc: 57.81%] [G loss: 1.666278]\n",
      "epoch:23 step:22407 [D loss: 0.687499, acc: 55.47%] [G loss: 1.793108]\n",
      "epoch:23 step:22408 [D loss: 0.608400, acc: 66.41%] [G loss: 1.968817]\n",
      "epoch:23 step:22409 [D loss: 0.669924, acc: 61.72%] [G loss: 1.886350]\n",
      "epoch:23 step:22410 [D loss: 0.675505, acc: 52.34%] [G loss: 1.776917]\n",
      "epoch:23 step:22411 [D loss: 0.606791, acc: 67.19%] [G loss: 1.806057]\n",
      "epoch:23 step:22412 [D loss: 0.653110, acc: 61.72%] [G loss: 1.850596]\n",
      "epoch:23 step:22413 [D loss: 0.660910, acc: 56.25%] [G loss: 1.769700]\n",
      "epoch:23 step:22414 [D loss: 0.630928, acc: 67.97%] [G loss: 1.880762]\n",
      "epoch:23 step:22415 [D loss: 0.630729, acc: 60.94%] [G loss: 1.807595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:22416 [D loss: 0.713419, acc: 58.59%] [G loss: 1.689496]\n",
      "epoch:23 step:22417 [D loss: 0.604622, acc: 70.31%] [G loss: 1.912887]\n",
      "epoch:23 step:22418 [D loss: 0.695380, acc: 56.25%] [G loss: 1.735033]\n",
      "epoch:23 step:22419 [D loss: 0.647804, acc: 60.16%] [G loss: 1.908617]\n",
      "epoch:23 step:22420 [D loss: 0.634481, acc: 62.50%] [G loss: 1.817372]\n",
      "epoch:23 step:22421 [D loss: 0.665065, acc: 60.16%] [G loss: 1.710373]\n",
      "epoch:23 step:22422 [D loss: 0.681867, acc: 54.69%] [G loss: 1.751068]\n",
      "epoch:23 step:22423 [D loss: 0.695798, acc: 57.03%] [G loss: 1.839839]\n",
      "epoch:23 step:22424 [D loss: 0.701411, acc: 57.03%] [G loss: 1.834408]\n",
      "epoch:23 step:22425 [D loss: 0.643417, acc: 66.41%] [G loss: 1.705657]\n",
      "epoch:23 step:22426 [D loss: 0.638775, acc: 64.84%] [G loss: 1.847412]\n",
      "epoch:23 step:22427 [D loss: 0.604503, acc: 64.06%] [G loss: 1.902966]\n",
      "epoch:23 step:22428 [D loss: 0.665992, acc: 58.59%] [G loss: 1.898970]\n",
      "epoch:23 step:22429 [D loss: 0.642995, acc: 62.50%] [G loss: 1.890411]\n",
      "epoch:23 step:22430 [D loss: 0.666938, acc: 57.03%] [G loss: 1.872189]\n",
      "epoch:23 step:22431 [D loss: 0.707536, acc: 53.91%] [G loss: 1.857830]\n",
      "epoch:23 step:22432 [D loss: 0.643265, acc: 59.38%] [G loss: 1.789464]\n",
      "epoch:23 step:22433 [D loss: 0.655608, acc: 60.94%] [G loss: 1.858652]\n",
      "epoch:23 step:22434 [D loss: 0.665079, acc: 60.94%] [G loss: 1.843145]\n",
      "epoch:23 step:22435 [D loss: 0.593932, acc: 68.75%] [G loss: 1.965882]\n",
      "epoch:23 step:22436 [D loss: 0.608581, acc: 69.53%] [G loss: 1.881235]\n",
      "epoch:23 step:22437 [D loss: 0.630689, acc: 65.62%] [G loss: 1.966370]\n",
      "epoch:23 step:22438 [D loss: 0.657544, acc: 53.12%] [G loss: 1.732605]\n",
      "epoch:23 step:22439 [D loss: 0.634028, acc: 57.81%] [G loss: 1.887437]\n",
      "epoch:23 step:22440 [D loss: 0.664518, acc: 60.16%] [G loss: 1.877114]\n",
      "epoch:23 step:22441 [D loss: 0.600880, acc: 72.66%] [G loss: 2.127905]\n",
      "epoch:23 step:22442 [D loss: 0.686986, acc: 52.34%] [G loss: 1.932547]\n",
      "epoch:23 step:22443 [D loss: 0.628425, acc: 64.84%] [G loss: 1.948459]\n",
      "epoch:23 step:22444 [D loss: 0.652171, acc: 60.16%] [G loss: 1.835757]\n",
      "epoch:23 step:22445 [D loss: 0.621138, acc: 66.41%] [G loss: 1.906487]\n",
      "epoch:23 step:22446 [D loss: 0.641362, acc: 62.50%] [G loss: 1.908498]\n",
      "epoch:23 step:22447 [D loss: 0.633456, acc: 58.59%] [G loss: 1.900860]\n",
      "epoch:23 step:22448 [D loss: 0.619393, acc: 65.62%] [G loss: 1.985654]\n",
      "epoch:23 step:22449 [D loss: 0.687346, acc: 60.16%] [G loss: 2.029850]\n",
      "epoch:23 step:22450 [D loss: 0.597741, acc: 70.31%] [G loss: 2.078099]\n",
      "epoch:23 step:22451 [D loss: 0.610065, acc: 68.75%] [G loss: 1.889631]\n",
      "epoch:23 step:22452 [D loss: 0.604769, acc: 65.62%] [G loss: 1.961658]\n",
      "epoch:23 step:22453 [D loss: 0.639475, acc: 66.41%] [G loss: 1.889695]\n",
      "epoch:23 step:22454 [D loss: 0.654576, acc: 66.41%] [G loss: 2.025246]\n",
      "epoch:23 step:22455 [D loss: 0.629783, acc: 60.94%] [G loss: 2.011758]\n",
      "epoch:23 step:22456 [D loss: 0.708147, acc: 55.47%] [G loss: 1.802505]\n",
      "epoch:23 step:22457 [D loss: 0.654390, acc: 62.50%] [G loss: 1.810389]\n",
      "epoch:23 step:22458 [D loss: 0.606940, acc: 67.19%] [G loss: 1.898701]\n",
      "epoch:23 step:22459 [D loss: 0.619646, acc: 68.75%] [G loss: 1.912079]\n",
      "epoch:23 step:22460 [D loss: 0.679085, acc: 59.38%] [G loss: 2.008736]\n",
      "epoch:23 step:22461 [D loss: 0.621142, acc: 72.66%] [G loss: 1.908270]\n",
      "epoch:23 step:22462 [D loss: 0.621350, acc: 67.19%] [G loss: 1.972036]\n",
      "epoch:23 step:22463 [D loss: 0.557427, acc: 71.88%] [G loss: 2.077415]\n",
      "epoch:23 step:22464 [D loss: 0.671043, acc: 57.03%] [G loss: 1.909917]\n",
      "epoch:23 step:22465 [D loss: 0.665556, acc: 59.38%] [G loss: 1.906223]\n",
      "epoch:23 step:22466 [D loss: 0.649465, acc: 63.28%] [G loss: 2.017746]\n",
      "epoch:23 step:22467 [D loss: 0.629180, acc: 58.59%] [G loss: 1.947915]\n",
      "epoch:23 step:22468 [D loss: 0.658323, acc: 59.38%] [G loss: 2.055715]\n",
      "epoch:23 step:22469 [D loss: 0.585339, acc: 71.88%] [G loss: 2.177281]\n",
      "epoch:23 step:22470 [D loss: 0.593032, acc: 69.53%] [G loss: 2.093732]\n",
      "epoch:23 step:22471 [D loss: 0.765583, acc: 47.66%] [G loss: 1.743302]\n",
      "epoch:23 step:22472 [D loss: 0.643143, acc: 62.50%] [G loss: 1.946328]\n",
      "epoch:23 step:22473 [D loss: 0.646474, acc: 62.50%] [G loss: 1.972001]\n",
      "epoch:23 step:22474 [D loss: 0.585149, acc: 71.09%] [G loss: 2.197957]\n",
      "epoch:23 step:22475 [D loss: 0.622848, acc: 65.62%] [G loss: 2.082553]\n",
      "epoch:23 step:22476 [D loss: 0.644281, acc: 71.88%] [G loss: 2.127043]\n",
      "epoch:23 step:22477 [D loss: 0.609266, acc: 66.41%] [G loss: 2.222236]\n",
      "epoch:23 step:22478 [D loss: 0.656814, acc: 59.38%] [G loss: 2.111502]\n",
      "epoch:23 step:22479 [D loss: 0.753177, acc: 45.31%] [G loss: 1.866248]\n",
      "epoch:23 step:22480 [D loss: 0.740178, acc: 52.34%] [G loss: 1.848287]\n",
      "epoch:23 step:22481 [D loss: 0.654098, acc: 66.41%] [G loss: 1.920985]\n",
      "epoch:23 step:22482 [D loss: 0.654609, acc: 64.06%] [G loss: 1.973832]\n",
      "epoch:23 step:22483 [D loss: 0.623912, acc: 64.06%] [G loss: 1.895473]\n",
      "epoch:23 step:22484 [D loss: 0.616217, acc: 70.31%] [G loss: 1.860580]\n",
      "epoch:23 step:22485 [D loss: 0.708728, acc: 60.94%] [G loss: 1.784459]\n",
      "epoch:23 step:22486 [D loss: 0.589857, acc: 68.75%] [G loss: 2.067802]\n",
      "epoch:23 step:22487 [D loss: 0.553547, acc: 73.44%] [G loss: 2.126836]\n",
      "epoch:23 step:22488 [D loss: 0.623172, acc: 69.53%] [G loss: 2.566539]\n",
      "epoch:24 step:22489 [D loss: 0.670060, acc: 57.81%] [G loss: 1.863015]\n",
      "epoch:24 step:22490 [D loss: 0.634294, acc: 63.28%] [G loss: 1.945945]\n",
      "epoch:24 step:22491 [D loss: 0.678493, acc: 61.72%] [G loss: 1.795403]\n",
      "epoch:24 step:22492 [D loss: 0.675167, acc: 60.94%] [G loss: 1.854882]\n",
      "epoch:24 step:22493 [D loss: 0.675109, acc: 55.47%] [G loss: 1.864076]\n",
      "epoch:24 step:22494 [D loss: 0.643032, acc: 63.28%] [G loss: 1.958741]\n",
      "epoch:24 step:22495 [D loss: 0.608787, acc: 65.62%] [G loss: 1.961657]\n",
      "epoch:24 step:22496 [D loss: 0.642180, acc: 62.50%] [G loss: 2.021345]\n",
      "epoch:24 step:22497 [D loss: 0.619423, acc: 64.84%] [G loss: 2.128473]\n",
      "epoch:24 step:22498 [D loss: 0.627580, acc: 67.97%] [G loss: 2.011488]\n",
      "epoch:24 step:22499 [D loss: 0.679900, acc: 60.16%] [G loss: 1.929644]\n",
      "epoch:24 step:22500 [D loss: 0.651163, acc: 62.50%] [G loss: 1.938280]\n",
      "epoch:24 step:22501 [D loss: 0.661425, acc: 61.72%] [G loss: 1.806447]\n",
      "epoch:24 step:22502 [D loss: 0.662748, acc: 61.72%] [G loss: 1.976069]\n",
      "epoch:24 step:22503 [D loss: 0.589288, acc: 71.09%] [G loss: 2.027467]\n",
      "epoch:24 step:22504 [D loss: 0.579588, acc: 67.19%] [G loss: 2.045106]\n",
      "epoch:24 step:22505 [D loss: 0.604530, acc: 70.31%] [G loss: 1.933417]\n",
      "epoch:24 step:22506 [D loss: 0.596037, acc: 68.75%] [G loss: 1.984379]\n",
      "epoch:24 step:22507 [D loss: 0.698219, acc: 57.03%] [G loss: 2.030032]\n",
      "epoch:24 step:22508 [D loss: 0.750421, acc: 50.00%] [G loss: 1.638996]\n",
      "epoch:24 step:22509 [D loss: 0.702902, acc: 57.03%] [G loss: 1.765270]\n",
      "epoch:24 step:22510 [D loss: 0.659596, acc: 58.59%] [G loss: 1.691558]\n",
      "epoch:24 step:22511 [D loss: 0.627946, acc: 66.41%] [G loss: 2.134956]\n",
      "epoch:24 step:22512 [D loss: 0.597951, acc: 67.97%] [G loss: 2.040823]\n",
      "epoch:24 step:22513 [D loss: 0.565383, acc: 72.66%] [G loss: 1.963618]\n",
      "epoch:24 step:22514 [D loss: 0.648069, acc: 63.28%] [G loss: 1.774792]\n",
      "epoch:24 step:22515 [D loss: 0.650192, acc: 55.47%] [G loss: 1.973915]\n",
      "epoch:24 step:22516 [D loss: 0.660882, acc: 57.03%] [G loss: 1.818812]\n",
      "epoch:24 step:22517 [D loss: 0.692089, acc: 57.81%] [G loss: 1.738391]\n",
      "epoch:24 step:22518 [D loss: 0.593908, acc: 71.88%] [G loss: 1.698175]\n",
      "epoch:24 step:22519 [D loss: 0.702069, acc: 60.94%] [G loss: 1.711206]\n",
      "epoch:24 step:22520 [D loss: 0.659568, acc: 57.03%] [G loss: 1.860431]\n",
      "epoch:24 step:22521 [D loss: 0.619711, acc: 62.50%] [G loss: 1.816818]\n",
      "epoch:24 step:22522 [D loss: 0.685080, acc: 60.94%] [G loss: 1.909136]\n",
      "epoch:24 step:22523 [D loss: 0.642742, acc: 61.72%] [G loss: 1.842550]\n",
      "epoch:24 step:22524 [D loss: 0.637440, acc: 60.16%] [G loss: 1.888612]\n",
      "epoch:24 step:22525 [D loss: 0.649633, acc: 63.28%] [G loss: 1.839882]\n",
      "epoch:24 step:22526 [D loss: 0.615773, acc: 68.75%] [G loss: 1.819248]\n",
      "epoch:24 step:22527 [D loss: 0.647805, acc: 61.72%] [G loss: 1.938236]\n",
      "epoch:24 step:22528 [D loss: 0.590012, acc: 70.31%] [G loss: 2.216862]\n",
      "epoch:24 step:22529 [D loss: 0.641003, acc: 63.28%] [G loss: 1.871816]\n",
      "epoch:24 step:22530 [D loss: 0.595485, acc: 70.31%] [G loss: 2.154441]\n",
      "epoch:24 step:22531 [D loss: 0.612409, acc: 65.62%] [G loss: 2.023577]\n",
      "epoch:24 step:22532 [D loss: 0.687483, acc: 57.03%] [G loss: 1.995609]\n",
      "epoch:24 step:22533 [D loss: 0.688618, acc: 54.69%] [G loss: 1.889731]\n",
      "epoch:24 step:22534 [D loss: 0.622511, acc: 61.72%] [G loss: 1.941306]\n",
      "epoch:24 step:22535 [D loss: 0.653916, acc: 62.50%] [G loss: 1.910632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22536 [D loss: 0.619787, acc: 67.19%] [G loss: 2.074187]\n",
      "epoch:24 step:22537 [D loss: 0.603695, acc: 68.75%] [G loss: 1.952084]\n",
      "epoch:24 step:22538 [D loss: 0.629399, acc: 68.75%] [G loss: 2.078393]\n",
      "epoch:24 step:22539 [D loss: 0.617462, acc: 66.41%] [G loss: 2.009770]\n",
      "epoch:24 step:22540 [D loss: 0.637218, acc: 67.19%] [G loss: 1.922249]\n",
      "epoch:24 step:22541 [D loss: 0.634312, acc: 62.50%] [G loss: 2.182159]\n",
      "epoch:24 step:22542 [D loss: 0.596455, acc: 70.31%] [G loss: 1.962867]\n",
      "epoch:24 step:22543 [D loss: 0.650210, acc: 65.62%] [G loss: 2.091632]\n",
      "epoch:24 step:22544 [D loss: 0.720737, acc: 56.25%] [G loss: 2.015991]\n",
      "epoch:24 step:22545 [D loss: 0.684835, acc: 60.16%] [G loss: 1.923198]\n",
      "epoch:24 step:22546 [D loss: 0.667246, acc: 58.59%] [G loss: 1.898162]\n",
      "epoch:24 step:22547 [D loss: 0.639959, acc: 61.72%] [G loss: 1.851526]\n",
      "epoch:24 step:22548 [D loss: 0.683149, acc: 58.59%] [G loss: 1.931213]\n",
      "epoch:24 step:22549 [D loss: 0.693138, acc: 55.47%] [G loss: 1.834146]\n",
      "epoch:24 step:22550 [D loss: 0.655900, acc: 67.97%] [G loss: 1.768134]\n",
      "epoch:24 step:22551 [D loss: 0.640890, acc: 66.41%] [G loss: 1.801966]\n",
      "epoch:24 step:22552 [D loss: 0.661748, acc: 61.72%] [G loss: 2.050638]\n",
      "epoch:24 step:22553 [D loss: 0.681542, acc: 61.72%] [G loss: 2.039289]\n",
      "epoch:24 step:22554 [D loss: 0.641122, acc: 67.97%] [G loss: 1.774184]\n",
      "epoch:24 step:22555 [D loss: 0.698548, acc: 58.59%] [G loss: 1.814139]\n",
      "epoch:24 step:22556 [D loss: 0.636919, acc: 60.94%] [G loss: 1.883174]\n",
      "epoch:24 step:22557 [D loss: 0.638600, acc: 60.94%] [G loss: 2.003746]\n",
      "epoch:24 step:22558 [D loss: 0.624198, acc: 64.06%] [G loss: 1.826459]\n",
      "epoch:24 step:22559 [D loss: 0.616103, acc: 66.41%] [G loss: 1.787815]\n",
      "epoch:24 step:22560 [D loss: 0.618352, acc: 63.28%] [G loss: 1.935973]\n",
      "epoch:24 step:22561 [D loss: 0.666938, acc: 57.03%] [G loss: 1.636938]\n",
      "epoch:24 step:22562 [D loss: 0.648315, acc: 56.25%] [G loss: 1.813374]\n",
      "epoch:24 step:22563 [D loss: 0.618640, acc: 65.62%] [G loss: 2.039182]\n",
      "epoch:24 step:22564 [D loss: 0.612915, acc: 67.97%] [G loss: 1.979668]\n",
      "epoch:24 step:22565 [D loss: 0.648283, acc: 61.72%] [G loss: 2.145214]\n",
      "epoch:24 step:22566 [D loss: 0.665539, acc: 63.28%] [G loss: 1.751856]\n",
      "epoch:24 step:22567 [D loss: 0.653817, acc: 57.81%] [G loss: 1.837032]\n",
      "epoch:24 step:22568 [D loss: 0.684549, acc: 58.59%] [G loss: 1.898733]\n",
      "epoch:24 step:22569 [D loss: 0.690212, acc: 60.16%] [G loss: 1.773250]\n",
      "epoch:24 step:22570 [D loss: 0.654796, acc: 61.72%] [G loss: 1.784048]\n",
      "epoch:24 step:22571 [D loss: 0.683306, acc: 59.38%] [G loss: 1.914270]\n",
      "epoch:24 step:22572 [D loss: 0.614228, acc: 67.97%] [G loss: 1.845078]\n",
      "epoch:24 step:22573 [D loss: 0.645844, acc: 63.28%] [G loss: 1.906386]\n",
      "epoch:24 step:22574 [D loss: 0.682674, acc: 59.38%] [G loss: 1.789314]\n",
      "epoch:24 step:22575 [D loss: 0.636756, acc: 68.75%] [G loss: 1.903903]\n",
      "epoch:24 step:22576 [D loss: 0.627583, acc: 64.06%] [G loss: 2.078597]\n",
      "epoch:24 step:22577 [D loss: 0.661825, acc: 65.62%] [G loss: 1.921075]\n",
      "epoch:24 step:22578 [D loss: 0.647511, acc: 61.72%] [G loss: 1.901055]\n",
      "epoch:24 step:22579 [D loss: 0.655755, acc: 57.81%] [G loss: 1.885738]\n",
      "epoch:24 step:22580 [D loss: 0.666452, acc: 58.59%] [G loss: 1.823132]\n",
      "epoch:24 step:22581 [D loss: 0.682790, acc: 60.94%] [G loss: 1.839542]\n",
      "epoch:24 step:22582 [D loss: 0.656134, acc: 64.06%] [G loss: 1.812953]\n",
      "epoch:24 step:22583 [D loss: 0.622406, acc: 67.19%] [G loss: 1.842868]\n",
      "epoch:24 step:22584 [D loss: 0.644596, acc: 62.50%] [G loss: 1.875661]\n",
      "epoch:24 step:22585 [D loss: 0.654129, acc: 57.81%] [G loss: 1.898751]\n",
      "epoch:24 step:22586 [D loss: 0.692029, acc: 50.78%] [G loss: 1.816946]\n",
      "epoch:24 step:22587 [D loss: 0.665748, acc: 60.16%] [G loss: 1.872466]\n",
      "epoch:24 step:22588 [D loss: 0.639491, acc: 64.06%] [G loss: 1.700425]\n",
      "epoch:24 step:22589 [D loss: 0.636249, acc: 66.41%] [G loss: 1.875316]\n",
      "epoch:24 step:22590 [D loss: 0.669272, acc: 54.69%] [G loss: 1.776951]\n",
      "epoch:24 step:22591 [D loss: 0.598997, acc: 75.00%] [G loss: 1.882686]\n",
      "epoch:24 step:22592 [D loss: 0.656863, acc: 60.16%] [G loss: 1.848136]\n",
      "epoch:24 step:22593 [D loss: 0.677088, acc: 58.59%] [G loss: 1.817502]\n",
      "epoch:24 step:22594 [D loss: 0.602250, acc: 68.75%] [G loss: 1.997338]\n",
      "epoch:24 step:22595 [D loss: 0.630346, acc: 67.97%] [G loss: 2.030790]\n",
      "epoch:24 step:22596 [D loss: 0.739034, acc: 50.78%] [G loss: 1.689936]\n",
      "epoch:24 step:22597 [D loss: 0.648833, acc: 65.62%] [G loss: 1.838466]\n",
      "epoch:24 step:22598 [D loss: 0.663773, acc: 59.38%] [G loss: 1.831914]\n",
      "epoch:24 step:22599 [D loss: 0.589012, acc: 68.75%] [G loss: 1.843005]\n",
      "epoch:24 step:22600 [D loss: 0.665786, acc: 57.81%] [G loss: 1.913964]\n",
      "epoch:24 step:22601 [D loss: 0.633536, acc: 67.97%] [G loss: 1.960603]\n",
      "epoch:24 step:22602 [D loss: 0.639160, acc: 66.41%] [G loss: 2.059799]\n",
      "epoch:24 step:22603 [D loss: 0.598997, acc: 64.06%] [G loss: 2.101188]\n",
      "epoch:24 step:22604 [D loss: 0.603868, acc: 65.62%] [G loss: 2.022292]\n",
      "epoch:24 step:22605 [D loss: 0.581265, acc: 69.53%] [G loss: 2.112021]\n",
      "epoch:24 step:22606 [D loss: 0.667476, acc: 60.16%] [G loss: 2.016050]\n",
      "epoch:24 step:22607 [D loss: 0.651488, acc: 62.50%] [G loss: 1.993080]\n",
      "epoch:24 step:22608 [D loss: 0.669729, acc: 64.84%] [G loss: 1.991486]\n",
      "epoch:24 step:22609 [D loss: 0.716570, acc: 52.34%] [G loss: 2.158788]\n",
      "epoch:24 step:22610 [D loss: 0.654778, acc: 58.59%] [G loss: 2.204316]\n",
      "epoch:24 step:22611 [D loss: 0.708693, acc: 60.16%] [G loss: 2.062823]\n",
      "epoch:24 step:22612 [D loss: 0.693591, acc: 52.34%] [G loss: 1.743584]\n",
      "epoch:24 step:22613 [D loss: 0.682576, acc: 56.25%] [G loss: 1.764499]\n",
      "epoch:24 step:22614 [D loss: 0.671793, acc: 57.03%] [G loss: 1.805610]\n",
      "epoch:24 step:22615 [D loss: 0.654684, acc: 60.94%] [G loss: 1.743284]\n",
      "epoch:24 step:22616 [D loss: 0.660525, acc: 57.03%] [G loss: 1.849120]\n",
      "epoch:24 step:22617 [D loss: 0.696407, acc: 55.47%] [G loss: 1.792518]\n",
      "epoch:24 step:22618 [D loss: 0.612962, acc: 70.31%] [G loss: 1.920087]\n",
      "epoch:24 step:22619 [D loss: 0.612849, acc: 71.88%] [G loss: 1.864021]\n",
      "epoch:24 step:22620 [D loss: 0.645206, acc: 58.59%] [G loss: 1.851013]\n",
      "epoch:24 step:22621 [D loss: 0.700528, acc: 54.69%] [G loss: 1.758263]\n",
      "epoch:24 step:22622 [D loss: 0.651750, acc: 63.28%] [G loss: 1.862128]\n",
      "epoch:24 step:22623 [D loss: 0.681583, acc: 57.81%] [G loss: 1.758948]\n",
      "epoch:24 step:22624 [D loss: 0.651476, acc: 62.50%] [G loss: 1.841351]\n",
      "epoch:24 step:22625 [D loss: 0.712692, acc: 59.38%] [G loss: 1.814658]\n",
      "epoch:24 step:22626 [D loss: 0.692871, acc: 57.81%] [G loss: 1.792308]\n",
      "epoch:24 step:22627 [D loss: 0.641549, acc: 57.81%] [G loss: 1.835184]\n",
      "epoch:24 step:22628 [D loss: 0.668173, acc: 60.94%] [G loss: 1.766912]\n",
      "epoch:24 step:22629 [D loss: 0.695976, acc: 56.25%] [G loss: 1.778482]\n",
      "epoch:24 step:22630 [D loss: 0.670526, acc: 62.50%] [G loss: 1.644739]\n",
      "epoch:24 step:22631 [D loss: 0.724688, acc: 52.34%] [G loss: 1.712660]\n",
      "epoch:24 step:22632 [D loss: 0.661927, acc: 58.59%] [G loss: 1.648414]\n",
      "epoch:24 step:22633 [D loss: 0.671530, acc: 57.03%] [G loss: 1.800109]\n",
      "epoch:24 step:22634 [D loss: 0.624659, acc: 65.62%] [G loss: 1.864617]\n",
      "epoch:24 step:22635 [D loss: 0.641949, acc: 62.50%] [G loss: 1.700228]\n",
      "epoch:24 step:22636 [D loss: 0.651544, acc: 60.16%] [G loss: 1.772240]\n",
      "epoch:24 step:22637 [D loss: 0.635677, acc: 64.84%] [G loss: 1.824981]\n",
      "epoch:24 step:22638 [D loss: 0.650871, acc: 64.06%] [G loss: 1.852326]\n",
      "epoch:24 step:22639 [D loss: 0.656117, acc: 59.38%] [G loss: 1.893608]\n",
      "epoch:24 step:22640 [D loss: 0.700956, acc: 60.16%] [G loss: 1.892923]\n",
      "epoch:24 step:22641 [D loss: 0.717444, acc: 52.34%] [G loss: 1.687482]\n",
      "epoch:24 step:22642 [D loss: 0.670259, acc: 64.06%] [G loss: 1.759599]\n",
      "epoch:24 step:22643 [D loss: 0.620049, acc: 67.97%] [G loss: 1.886225]\n",
      "epoch:24 step:22644 [D loss: 0.608584, acc: 71.09%] [G loss: 1.799064]\n",
      "epoch:24 step:22645 [D loss: 0.666781, acc: 61.72%] [G loss: 1.802030]\n",
      "epoch:24 step:22646 [D loss: 0.626135, acc: 64.84%] [G loss: 1.732391]\n",
      "epoch:24 step:22647 [D loss: 0.666592, acc: 57.03%] [G loss: 1.748330]\n",
      "epoch:24 step:22648 [D loss: 0.683509, acc: 57.81%] [G loss: 1.748046]\n",
      "epoch:24 step:22649 [D loss: 0.669227, acc: 58.59%] [G loss: 1.925344]\n",
      "epoch:24 step:22650 [D loss: 0.670863, acc: 59.38%] [G loss: 1.889236]\n",
      "epoch:24 step:22651 [D loss: 0.733948, acc: 57.03%] [G loss: 1.793890]\n",
      "epoch:24 step:22652 [D loss: 0.650284, acc: 61.72%] [G loss: 1.811076]\n",
      "epoch:24 step:22653 [D loss: 0.705602, acc: 54.69%] [G loss: 1.751038]\n",
      "epoch:24 step:22654 [D loss: 0.663892, acc: 59.38%] [G loss: 1.761358]\n",
      "epoch:24 step:22655 [D loss: 0.627718, acc: 61.72%] [G loss: 1.799572]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22656 [D loss: 0.633185, acc: 57.81%] [G loss: 1.720234]\n",
      "epoch:24 step:22657 [D loss: 0.671150, acc: 62.50%] [G loss: 1.836714]\n",
      "epoch:24 step:22658 [D loss: 0.675640, acc: 55.47%] [G loss: 1.803777]\n",
      "epoch:24 step:22659 [D loss: 0.649130, acc: 63.28%] [G loss: 1.701794]\n",
      "epoch:24 step:22660 [D loss: 0.617004, acc: 65.62%] [G loss: 1.772707]\n",
      "epoch:24 step:22661 [D loss: 0.657737, acc: 64.06%] [G loss: 1.803430]\n",
      "epoch:24 step:22662 [D loss: 0.661223, acc: 60.16%] [G loss: 1.772915]\n",
      "epoch:24 step:22663 [D loss: 0.673211, acc: 55.47%] [G loss: 1.748356]\n",
      "epoch:24 step:22664 [D loss: 0.658148, acc: 52.34%] [G loss: 1.784796]\n",
      "epoch:24 step:22665 [D loss: 0.640254, acc: 60.16%] [G loss: 1.816464]\n",
      "epoch:24 step:22666 [D loss: 0.645000, acc: 61.72%] [G loss: 1.813711]\n",
      "epoch:24 step:22667 [D loss: 0.658412, acc: 57.81%] [G loss: 1.728034]\n",
      "epoch:24 step:22668 [D loss: 0.678500, acc: 59.38%] [G loss: 1.805519]\n",
      "epoch:24 step:22669 [D loss: 0.677603, acc: 53.91%] [G loss: 1.735799]\n",
      "epoch:24 step:22670 [D loss: 0.645395, acc: 62.50%] [G loss: 1.818046]\n",
      "epoch:24 step:22671 [D loss: 0.722799, acc: 50.78%] [G loss: 1.722457]\n",
      "epoch:24 step:22672 [D loss: 0.660594, acc: 66.41%] [G loss: 1.858244]\n",
      "epoch:24 step:22673 [D loss: 0.633550, acc: 60.16%] [G loss: 1.889588]\n",
      "epoch:24 step:22674 [D loss: 0.675518, acc: 57.03%] [G loss: 1.793669]\n",
      "epoch:24 step:22675 [D loss: 0.681841, acc: 57.81%] [G loss: 1.860432]\n",
      "epoch:24 step:22676 [D loss: 0.656829, acc: 54.69%] [G loss: 1.802328]\n",
      "epoch:24 step:22677 [D loss: 0.650595, acc: 61.72%] [G loss: 1.796315]\n",
      "epoch:24 step:22678 [D loss: 0.633391, acc: 64.84%] [G loss: 1.748061]\n",
      "epoch:24 step:22679 [D loss: 0.653314, acc: 57.81%] [G loss: 1.794971]\n",
      "epoch:24 step:22680 [D loss: 0.652582, acc: 63.28%] [G loss: 1.950774]\n",
      "epoch:24 step:22681 [D loss: 0.628893, acc: 64.84%] [G loss: 1.883565]\n",
      "epoch:24 step:22682 [D loss: 0.584288, acc: 66.41%] [G loss: 2.002750]\n",
      "epoch:24 step:22683 [D loss: 0.642625, acc: 63.28%] [G loss: 1.879673]\n",
      "epoch:24 step:22684 [D loss: 0.686772, acc: 62.50%] [G loss: 1.775391]\n",
      "epoch:24 step:22685 [D loss: 0.649396, acc: 63.28%] [G loss: 1.846467]\n",
      "epoch:24 step:22686 [D loss: 0.573128, acc: 69.53%] [G loss: 1.913222]\n",
      "epoch:24 step:22687 [D loss: 0.687655, acc: 60.16%] [G loss: 1.770867]\n",
      "epoch:24 step:22688 [D loss: 0.680879, acc: 56.25%] [G loss: 1.612746]\n",
      "epoch:24 step:22689 [D loss: 0.634761, acc: 62.50%] [G loss: 1.807107]\n",
      "epoch:24 step:22690 [D loss: 0.651501, acc: 58.59%] [G loss: 1.844663]\n",
      "epoch:24 step:22691 [D loss: 0.632591, acc: 62.50%] [G loss: 1.769341]\n",
      "epoch:24 step:22692 [D loss: 0.636980, acc: 54.69%] [G loss: 1.777727]\n",
      "epoch:24 step:22693 [D loss: 0.659599, acc: 62.50%] [G loss: 1.728631]\n",
      "epoch:24 step:22694 [D loss: 0.634847, acc: 67.19%] [G loss: 1.927926]\n",
      "epoch:24 step:22695 [D loss: 0.629659, acc: 64.84%] [G loss: 2.023846]\n",
      "epoch:24 step:22696 [D loss: 0.623100, acc: 68.75%] [G loss: 2.089155]\n",
      "epoch:24 step:22697 [D loss: 0.647845, acc: 68.75%] [G loss: 2.058044]\n",
      "epoch:24 step:22698 [D loss: 0.704559, acc: 53.12%] [G loss: 1.831498]\n",
      "epoch:24 step:22699 [D loss: 0.626955, acc: 67.19%] [G loss: 1.762022]\n",
      "epoch:24 step:22700 [D loss: 0.720619, acc: 46.88%] [G loss: 1.881061]\n",
      "epoch:24 step:22701 [D loss: 0.690110, acc: 55.47%] [G loss: 1.641657]\n",
      "epoch:24 step:22702 [D loss: 0.653931, acc: 64.06%] [G loss: 1.786477]\n",
      "epoch:24 step:22703 [D loss: 0.619787, acc: 63.28%] [G loss: 1.875538]\n",
      "epoch:24 step:22704 [D loss: 0.636429, acc: 64.84%] [G loss: 1.883282]\n",
      "epoch:24 step:22705 [D loss: 0.690626, acc: 54.69%] [G loss: 1.946076]\n",
      "epoch:24 step:22706 [D loss: 0.607048, acc: 66.41%] [G loss: 1.899163]\n",
      "epoch:24 step:22707 [D loss: 0.630978, acc: 64.06%] [G loss: 1.970346]\n",
      "epoch:24 step:22708 [D loss: 0.717633, acc: 55.47%] [G loss: 1.712819]\n",
      "epoch:24 step:22709 [D loss: 0.644537, acc: 57.81%] [G loss: 1.784981]\n",
      "epoch:24 step:22710 [D loss: 0.645478, acc: 61.72%] [G loss: 1.891434]\n",
      "epoch:24 step:22711 [D loss: 0.672065, acc: 61.72%] [G loss: 1.780200]\n",
      "epoch:24 step:22712 [D loss: 0.649017, acc: 61.72%] [G loss: 1.842377]\n",
      "epoch:24 step:22713 [D loss: 0.669386, acc: 57.81%] [G loss: 1.945671]\n",
      "epoch:24 step:22714 [D loss: 0.649592, acc: 56.25%] [G loss: 1.844709]\n",
      "epoch:24 step:22715 [D loss: 0.626744, acc: 66.41%] [G loss: 1.804269]\n",
      "epoch:24 step:22716 [D loss: 0.694022, acc: 55.47%] [G loss: 1.703475]\n",
      "epoch:24 step:22717 [D loss: 0.629870, acc: 65.62%] [G loss: 2.002714]\n",
      "epoch:24 step:22718 [D loss: 0.608667, acc: 67.97%] [G loss: 1.977082]\n",
      "epoch:24 step:22719 [D loss: 0.568482, acc: 71.88%] [G loss: 2.318315]\n",
      "epoch:24 step:22720 [D loss: 0.580690, acc: 67.97%] [G loss: 2.097810]\n",
      "epoch:24 step:22721 [D loss: 0.705099, acc: 60.94%] [G loss: 1.829704]\n",
      "epoch:24 step:22722 [D loss: 0.721463, acc: 53.91%] [G loss: 1.823553]\n",
      "epoch:24 step:22723 [D loss: 0.652625, acc: 59.38%] [G loss: 1.873869]\n",
      "epoch:24 step:22724 [D loss: 0.648324, acc: 64.06%] [G loss: 1.892617]\n",
      "epoch:24 step:22725 [D loss: 0.657082, acc: 60.94%] [G loss: 1.968779]\n",
      "epoch:24 step:22726 [D loss: 0.649427, acc: 64.84%] [G loss: 1.891609]\n",
      "epoch:24 step:22727 [D loss: 0.605968, acc: 69.53%] [G loss: 1.897527]\n",
      "epoch:24 step:22728 [D loss: 0.674919, acc: 57.81%] [G loss: 1.786049]\n",
      "epoch:24 step:22729 [D loss: 0.602573, acc: 65.62%] [G loss: 1.957745]\n",
      "epoch:24 step:22730 [D loss: 0.628138, acc: 62.50%] [G loss: 1.993421]\n",
      "epoch:24 step:22731 [D loss: 0.613618, acc: 66.41%] [G loss: 1.965928]\n",
      "epoch:24 step:22732 [D loss: 0.706318, acc: 51.56%] [G loss: 1.832007]\n",
      "epoch:24 step:22733 [D loss: 0.679269, acc: 62.50%] [G loss: 1.874735]\n",
      "epoch:24 step:22734 [D loss: 0.654771, acc: 64.84%] [G loss: 1.973335]\n",
      "epoch:24 step:22735 [D loss: 0.619270, acc: 66.41%] [G loss: 1.766279]\n",
      "epoch:24 step:22736 [D loss: 0.614928, acc: 68.75%] [G loss: 1.977910]\n",
      "epoch:24 step:22737 [D loss: 0.649892, acc: 65.62%] [G loss: 1.876323]\n",
      "epoch:24 step:22738 [D loss: 0.739586, acc: 55.47%] [G loss: 1.678163]\n",
      "epoch:24 step:22739 [D loss: 0.690741, acc: 57.81%] [G loss: 1.757341]\n",
      "epoch:24 step:22740 [D loss: 0.660341, acc: 56.25%] [G loss: 1.704381]\n",
      "epoch:24 step:22741 [D loss: 0.671421, acc: 59.38%] [G loss: 1.840065]\n",
      "epoch:24 step:22742 [D loss: 0.683674, acc: 53.91%] [G loss: 1.723852]\n",
      "epoch:24 step:22743 [D loss: 0.686246, acc: 54.69%] [G loss: 1.782007]\n",
      "epoch:24 step:22744 [D loss: 0.649612, acc: 66.41%] [G loss: 1.869966]\n",
      "epoch:24 step:22745 [D loss: 0.669298, acc: 60.16%] [G loss: 1.642314]\n",
      "epoch:24 step:22746 [D loss: 0.643465, acc: 66.41%] [G loss: 1.830021]\n",
      "epoch:24 step:22747 [D loss: 0.646577, acc: 64.06%] [G loss: 1.846679]\n",
      "epoch:24 step:22748 [D loss: 0.627609, acc: 64.06%] [G loss: 1.820907]\n",
      "epoch:24 step:22749 [D loss: 0.664272, acc: 65.62%] [G loss: 1.939843]\n",
      "epoch:24 step:22750 [D loss: 0.639219, acc: 58.59%] [G loss: 1.927042]\n",
      "epoch:24 step:22751 [D loss: 0.680910, acc: 57.81%] [G loss: 2.000865]\n",
      "epoch:24 step:22752 [D loss: 0.637900, acc: 66.41%] [G loss: 1.882776]\n",
      "epoch:24 step:22753 [D loss: 0.667115, acc: 57.81%] [G loss: 1.809894]\n",
      "epoch:24 step:22754 [D loss: 0.654221, acc: 60.16%] [G loss: 1.787410]\n",
      "epoch:24 step:22755 [D loss: 0.651330, acc: 60.16%] [G loss: 1.792767]\n",
      "epoch:24 step:22756 [D loss: 0.634745, acc: 60.16%] [G loss: 1.759773]\n",
      "epoch:24 step:22757 [D loss: 0.660016, acc: 58.59%] [G loss: 1.858808]\n",
      "epoch:24 step:22758 [D loss: 0.706674, acc: 56.25%] [G loss: 1.875416]\n",
      "epoch:24 step:22759 [D loss: 0.637640, acc: 63.28%] [G loss: 1.835881]\n",
      "epoch:24 step:22760 [D loss: 0.646582, acc: 61.72%] [G loss: 1.868188]\n",
      "epoch:24 step:22761 [D loss: 0.624781, acc: 64.84%] [G loss: 1.862915]\n",
      "epoch:24 step:22762 [D loss: 0.644249, acc: 64.84%] [G loss: 2.004097]\n",
      "epoch:24 step:22763 [D loss: 0.602222, acc: 69.53%] [G loss: 1.953846]\n",
      "epoch:24 step:22764 [D loss: 0.642573, acc: 64.84%] [G loss: 2.172333]\n",
      "epoch:24 step:22765 [D loss: 0.680627, acc: 54.69%] [G loss: 1.746669]\n",
      "epoch:24 step:22766 [D loss: 0.629697, acc: 61.72%] [G loss: 1.852431]\n",
      "epoch:24 step:22767 [D loss: 0.671080, acc: 55.47%] [G loss: 1.834237]\n",
      "epoch:24 step:22768 [D loss: 0.680095, acc: 53.91%] [G loss: 1.812148]\n",
      "epoch:24 step:22769 [D loss: 0.656803, acc: 60.94%] [G loss: 1.846910]\n",
      "epoch:24 step:22770 [D loss: 0.624929, acc: 71.88%] [G loss: 1.838695]\n",
      "epoch:24 step:22771 [D loss: 0.609200, acc: 69.53%] [G loss: 1.886842]\n",
      "epoch:24 step:22772 [D loss: 0.689938, acc: 55.47%] [G loss: 1.736317]\n",
      "epoch:24 step:22773 [D loss: 0.634825, acc: 62.50%] [G loss: 1.824514]\n",
      "epoch:24 step:22774 [D loss: 0.600790, acc: 69.53%] [G loss: 1.893768]\n",
      "epoch:24 step:22775 [D loss: 0.656241, acc: 60.16%] [G loss: 1.863105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22776 [D loss: 0.627938, acc: 66.41%] [G loss: 1.876307]\n",
      "epoch:24 step:22777 [D loss: 0.605924, acc: 64.06%] [G loss: 1.820061]\n",
      "epoch:24 step:22778 [D loss: 0.685863, acc: 56.25%] [G loss: 1.804160]\n",
      "epoch:24 step:22779 [D loss: 0.695492, acc: 58.59%] [G loss: 1.678066]\n",
      "epoch:24 step:22780 [D loss: 0.654827, acc: 63.28%] [G loss: 1.699465]\n",
      "epoch:24 step:22781 [D loss: 0.631504, acc: 67.97%] [G loss: 1.914425]\n",
      "epoch:24 step:22782 [D loss: 0.598745, acc: 66.41%] [G loss: 1.740222]\n",
      "epoch:24 step:22783 [D loss: 0.612312, acc: 67.19%] [G loss: 1.810718]\n",
      "epoch:24 step:22784 [D loss: 0.595546, acc: 71.88%] [G loss: 1.938507]\n",
      "epoch:24 step:22785 [D loss: 0.607467, acc: 70.31%] [G loss: 1.816308]\n",
      "epoch:24 step:22786 [D loss: 0.627147, acc: 64.84%] [G loss: 1.996465]\n",
      "epoch:24 step:22787 [D loss: 0.593593, acc: 64.84%] [G loss: 2.095639]\n",
      "epoch:24 step:22788 [D loss: 0.587716, acc: 67.19%] [G loss: 1.903750]\n",
      "epoch:24 step:22789 [D loss: 0.672111, acc: 60.16%] [G loss: 1.932317]\n",
      "epoch:24 step:22790 [D loss: 0.673512, acc: 57.81%] [G loss: 1.952605]\n",
      "epoch:24 step:22791 [D loss: 0.687712, acc: 57.03%] [G loss: 1.777449]\n",
      "epoch:24 step:22792 [D loss: 0.662904, acc: 60.94%] [G loss: 1.832863]\n",
      "epoch:24 step:22793 [D loss: 0.680036, acc: 60.94%] [G loss: 1.893413]\n",
      "epoch:24 step:22794 [D loss: 0.630524, acc: 60.16%] [G loss: 1.849515]\n",
      "epoch:24 step:22795 [D loss: 0.636846, acc: 64.06%] [G loss: 1.870951]\n",
      "epoch:24 step:22796 [D loss: 0.639775, acc: 65.62%] [G loss: 1.759757]\n",
      "epoch:24 step:22797 [D loss: 0.652107, acc: 60.16%] [G loss: 1.869284]\n",
      "epoch:24 step:22798 [D loss: 0.637929, acc: 64.06%] [G loss: 1.846841]\n",
      "epoch:24 step:22799 [D loss: 0.650420, acc: 64.84%] [G loss: 1.877168]\n",
      "epoch:24 step:22800 [D loss: 0.614666, acc: 67.97%] [G loss: 2.231119]\n",
      "epoch:24 step:22801 [D loss: 0.615762, acc: 66.41%] [G loss: 2.000732]\n",
      "epoch:24 step:22802 [D loss: 0.662938, acc: 63.28%] [G loss: 2.034905]\n",
      "epoch:24 step:22803 [D loss: 0.564680, acc: 74.22%] [G loss: 2.183057]\n",
      "epoch:24 step:22804 [D loss: 0.664715, acc: 58.59%] [G loss: 1.884484]\n",
      "epoch:24 step:22805 [D loss: 0.696507, acc: 52.34%] [G loss: 1.793509]\n",
      "epoch:24 step:22806 [D loss: 0.712209, acc: 52.34%] [G loss: 1.814627]\n",
      "epoch:24 step:22807 [D loss: 0.659093, acc: 64.84%] [G loss: 1.736505]\n",
      "epoch:24 step:22808 [D loss: 0.670998, acc: 60.94%] [G loss: 1.789894]\n",
      "epoch:24 step:22809 [D loss: 0.621401, acc: 66.41%] [G loss: 1.939851]\n",
      "epoch:24 step:22810 [D loss: 0.632164, acc: 64.84%] [G loss: 1.805420]\n",
      "epoch:24 step:22811 [D loss: 0.670310, acc: 59.38%] [G loss: 1.694708]\n",
      "epoch:24 step:22812 [D loss: 0.675551, acc: 64.06%] [G loss: 1.882135]\n",
      "epoch:24 step:22813 [D loss: 0.662081, acc: 58.59%] [G loss: 1.707911]\n",
      "epoch:24 step:22814 [D loss: 0.661924, acc: 60.94%] [G loss: 1.783243]\n",
      "epoch:24 step:22815 [D loss: 0.689235, acc: 57.03%] [G loss: 1.780369]\n",
      "epoch:24 step:22816 [D loss: 0.649004, acc: 60.94%] [G loss: 1.939066]\n",
      "epoch:24 step:22817 [D loss: 0.634172, acc: 58.59%] [G loss: 2.005872]\n",
      "epoch:24 step:22818 [D loss: 0.646524, acc: 62.50%] [G loss: 2.044769]\n",
      "epoch:24 step:22819 [D loss: 0.606595, acc: 67.97%] [G loss: 1.999645]\n",
      "epoch:24 step:22820 [D loss: 0.622472, acc: 65.62%] [G loss: 2.010573]\n",
      "epoch:24 step:22821 [D loss: 0.606794, acc: 67.19%] [G loss: 1.889450]\n",
      "epoch:24 step:22822 [D loss: 0.647602, acc: 64.06%] [G loss: 1.982352]\n",
      "epoch:24 step:22823 [D loss: 0.576924, acc: 75.78%] [G loss: 1.908372]\n",
      "epoch:24 step:22824 [D loss: 0.632046, acc: 62.50%] [G loss: 1.882586]\n",
      "epoch:24 step:22825 [D loss: 0.690436, acc: 62.50%] [G loss: 2.007185]\n",
      "epoch:24 step:22826 [D loss: 0.664715, acc: 62.50%] [G loss: 1.983203]\n",
      "epoch:24 step:22827 [D loss: 0.601422, acc: 71.09%] [G loss: 2.045200]\n",
      "epoch:24 step:22828 [D loss: 0.654399, acc: 60.94%] [G loss: 1.856345]\n",
      "epoch:24 step:22829 [D loss: 0.661576, acc: 57.81%] [G loss: 1.709533]\n",
      "epoch:24 step:22830 [D loss: 0.685124, acc: 57.81%] [G loss: 1.666923]\n",
      "epoch:24 step:22831 [D loss: 0.667535, acc: 57.81%] [G loss: 1.864549]\n",
      "epoch:24 step:22832 [D loss: 0.681937, acc: 55.47%] [G loss: 1.723467]\n",
      "epoch:24 step:22833 [D loss: 0.603696, acc: 65.62%] [G loss: 2.089044]\n",
      "epoch:24 step:22834 [D loss: 0.572042, acc: 66.41%] [G loss: 2.114929]\n",
      "epoch:24 step:22835 [D loss: 0.591089, acc: 65.62%] [G loss: 2.207217]\n",
      "epoch:24 step:22836 [D loss: 0.653473, acc: 57.03%] [G loss: 1.987262]\n",
      "epoch:24 step:22837 [D loss: 0.701810, acc: 57.81%] [G loss: 1.738275]\n",
      "epoch:24 step:22838 [D loss: 0.640018, acc: 67.97%] [G loss: 1.773839]\n",
      "epoch:24 step:22839 [D loss: 0.607693, acc: 70.31%] [G loss: 1.714029]\n",
      "epoch:24 step:22840 [D loss: 0.668422, acc: 59.38%] [G loss: 1.873754]\n",
      "epoch:24 step:22841 [D loss: 0.685191, acc: 59.38%] [G loss: 1.893837]\n",
      "epoch:24 step:22842 [D loss: 0.639553, acc: 66.41%] [G loss: 2.128067]\n",
      "epoch:24 step:22843 [D loss: 0.673692, acc: 58.59%] [G loss: 1.739827]\n",
      "epoch:24 step:22844 [D loss: 0.620407, acc: 65.62%] [G loss: 1.848611]\n",
      "epoch:24 step:22845 [D loss: 0.665807, acc: 64.84%] [G loss: 1.996827]\n",
      "epoch:24 step:22846 [D loss: 0.610502, acc: 70.31%] [G loss: 2.001501]\n",
      "epoch:24 step:22847 [D loss: 0.633275, acc: 60.94%] [G loss: 2.099116]\n",
      "epoch:24 step:22848 [D loss: 0.609253, acc: 69.53%] [G loss: 2.150316]\n",
      "epoch:24 step:22849 [D loss: 0.595655, acc: 67.97%] [G loss: 1.944899]\n",
      "epoch:24 step:22850 [D loss: 0.687453, acc: 55.47%] [G loss: 1.928396]\n",
      "epoch:24 step:22851 [D loss: 0.605097, acc: 68.75%] [G loss: 1.973103]\n",
      "epoch:24 step:22852 [D loss: 0.600815, acc: 67.19%] [G loss: 1.974840]\n",
      "epoch:24 step:22853 [D loss: 0.630273, acc: 64.84%] [G loss: 1.875377]\n",
      "epoch:24 step:22854 [D loss: 0.617167, acc: 64.06%] [G loss: 2.001146]\n",
      "epoch:24 step:22855 [D loss: 0.566162, acc: 75.00%] [G loss: 2.091828]\n",
      "epoch:24 step:22856 [D loss: 0.691734, acc: 57.03%] [G loss: 1.698499]\n",
      "epoch:24 step:22857 [D loss: 0.632546, acc: 66.41%] [G loss: 1.975565]\n",
      "epoch:24 step:22858 [D loss: 0.674262, acc: 62.50%] [G loss: 1.922582]\n",
      "epoch:24 step:22859 [D loss: 0.585032, acc: 64.84%] [G loss: 2.045305]\n",
      "epoch:24 step:22860 [D loss: 0.667779, acc: 57.03%] [G loss: 1.908681]\n",
      "epoch:24 step:22861 [D loss: 0.675612, acc: 55.47%] [G loss: 1.814690]\n",
      "epoch:24 step:22862 [D loss: 0.610284, acc: 69.53%] [G loss: 2.042808]\n",
      "epoch:24 step:22863 [D loss: 0.677748, acc: 53.12%] [G loss: 1.823292]\n",
      "epoch:24 step:22864 [D loss: 0.636623, acc: 59.38%] [G loss: 1.795356]\n",
      "epoch:24 step:22865 [D loss: 0.668947, acc: 58.59%] [G loss: 1.790421]\n",
      "epoch:24 step:22866 [D loss: 0.644381, acc: 59.38%] [G loss: 1.830085]\n",
      "epoch:24 step:22867 [D loss: 0.617564, acc: 67.19%] [G loss: 1.923711]\n",
      "epoch:24 step:22868 [D loss: 0.611680, acc: 66.41%] [G loss: 1.803566]\n",
      "epoch:24 step:22869 [D loss: 0.655439, acc: 62.50%] [G loss: 1.991696]\n",
      "epoch:24 step:22870 [D loss: 0.650811, acc: 64.84%] [G loss: 1.899767]\n",
      "epoch:24 step:22871 [D loss: 0.610816, acc: 67.97%] [G loss: 1.897835]\n",
      "epoch:24 step:22872 [D loss: 0.643898, acc: 65.62%] [G loss: 1.888351]\n",
      "epoch:24 step:22873 [D loss: 0.639730, acc: 62.50%] [G loss: 2.027511]\n",
      "epoch:24 step:22874 [D loss: 0.647636, acc: 60.94%] [G loss: 1.819835]\n",
      "epoch:24 step:22875 [D loss: 0.678274, acc: 59.38%] [G loss: 1.764683]\n",
      "epoch:24 step:22876 [D loss: 0.687835, acc: 60.94%] [G loss: 1.870162]\n",
      "epoch:24 step:22877 [D loss: 0.649484, acc: 60.16%] [G loss: 1.789569]\n",
      "epoch:24 step:22878 [D loss: 0.664199, acc: 57.03%] [G loss: 1.827540]\n",
      "epoch:24 step:22879 [D loss: 0.626962, acc: 63.28%] [G loss: 1.754680]\n",
      "epoch:24 step:22880 [D loss: 0.670203, acc: 59.38%] [G loss: 1.811876]\n",
      "epoch:24 step:22881 [D loss: 0.696306, acc: 62.50%] [G loss: 1.939150]\n",
      "epoch:24 step:22882 [D loss: 0.614273, acc: 67.19%] [G loss: 1.872569]\n",
      "epoch:24 step:22883 [D loss: 0.636126, acc: 64.06%] [G loss: 1.990459]\n",
      "epoch:24 step:22884 [D loss: 0.690316, acc: 55.47%] [G loss: 1.902933]\n",
      "epoch:24 step:22885 [D loss: 0.645888, acc: 55.47%] [G loss: 1.859269]\n",
      "epoch:24 step:22886 [D loss: 0.599807, acc: 67.97%] [G loss: 1.925588]\n",
      "epoch:24 step:22887 [D loss: 0.667457, acc: 65.62%] [G loss: 1.858751]\n",
      "epoch:24 step:22888 [D loss: 0.633298, acc: 67.97%] [G loss: 1.847309]\n",
      "epoch:24 step:22889 [D loss: 0.643108, acc: 66.41%] [G loss: 2.003338]\n",
      "epoch:24 step:22890 [D loss: 0.674951, acc: 53.12%] [G loss: 1.822257]\n",
      "epoch:24 step:22891 [D loss: 0.628496, acc: 67.19%] [G loss: 1.889065]\n",
      "epoch:24 step:22892 [D loss: 0.638058, acc: 68.75%] [G loss: 1.966219]\n",
      "epoch:24 step:22893 [D loss: 0.606087, acc: 70.31%] [G loss: 1.908144]\n",
      "epoch:24 step:22894 [D loss: 0.638226, acc: 64.84%] [G loss: 2.080518]\n",
      "epoch:24 step:22895 [D loss: 0.650410, acc: 61.72%] [G loss: 1.750037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:22896 [D loss: 0.668193, acc: 56.25%] [G loss: 1.770566]\n",
      "epoch:24 step:22897 [D loss: 0.662012, acc: 57.03%] [G loss: 1.862708]\n",
      "epoch:24 step:22898 [D loss: 0.681087, acc: 60.16%] [G loss: 1.788225]\n",
      "epoch:24 step:22899 [D loss: 0.696141, acc: 57.81%] [G loss: 1.944176]\n",
      "epoch:24 step:22900 [D loss: 0.652692, acc: 60.94%] [G loss: 1.928763]\n",
      "epoch:24 step:22901 [D loss: 0.663309, acc: 63.28%] [G loss: 1.850456]\n",
      "epoch:24 step:22902 [D loss: 0.665377, acc: 57.03%] [G loss: 2.035290]\n",
      "epoch:24 step:22903 [D loss: 0.633441, acc: 64.06%] [G loss: 1.990018]\n",
      "epoch:24 step:22904 [D loss: 0.644651, acc: 63.28%] [G loss: 2.122494]\n",
      "epoch:24 step:22905 [D loss: 0.665677, acc: 64.06%] [G loss: 2.017247]\n",
      "epoch:24 step:22906 [D loss: 0.664808, acc: 56.25%] [G loss: 1.814374]\n",
      "epoch:24 step:22907 [D loss: 0.654956, acc: 64.06%] [G loss: 1.841369]\n",
      "epoch:24 step:22908 [D loss: 0.638203, acc: 65.62%] [G loss: 1.838514]\n",
      "epoch:24 step:22909 [D loss: 0.664661, acc: 58.59%] [G loss: 1.906122]\n",
      "epoch:24 step:22910 [D loss: 0.724438, acc: 53.91%] [G loss: 1.664232]\n",
      "epoch:24 step:22911 [D loss: 0.679825, acc: 57.81%] [G loss: 1.677971]\n",
      "epoch:24 step:22912 [D loss: 0.665775, acc: 61.72%] [G loss: 1.800481]\n",
      "epoch:24 step:22913 [D loss: 0.593944, acc: 69.53%] [G loss: 1.856765]\n",
      "epoch:24 step:22914 [D loss: 0.696362, acc: 56.25%] [G loss: 1.952371]\n",
      "epoch:24 step:22915 [D loss: 0.605482, acc: 71.09%] [G loss: 2.005487]\n",
      "epoch:24 step:22916 [D loss: 0.648193, acc: 60.94%] [G loss: 2.054303]\n",
      "epoch:24 step:22917 [D loss: 0.626728, acc: 57.03%] [G loss: 1.971294]\n",
      "epoch:24 step:22918 [D loss: 0.633117, acc: 67.97%] [G loss: 2.009848]\n",
      "epoch:24 step:22919 [D loss: 0.634510, acc: 64.06%] [G loss: 1.938826]\n",
      "epoch:24 step:22920 [D loss: 0.672505, acc: 53.12%] [G loss: 1.805543]\n",
      "epoch:24 step:22921 [D loss: 0.670514, acc: 57.03%] [G loss: 1.821053]\n",
      "epoch:24 step:22922 [D loss: 0.654532, acc: 64.06%] [G loss: 1.947877]\n",
      "epoch:24 step:22923 [D loss: 0.620936, acc: 60.94%] [G loss: 1.927617]\n",
      "epoch:24 step:22924 [D loss: 0.646781, acc: 58.59%] [G loss: 1.792523]\n",
      "epoch:24 step:22925 [D loss: 0.714880, acc: 53.12%] [G loss: 1.701389]\n",
      "epoch:24 step:22926 [D loss: 0.713159, acc: 57.81%] [G loss: 1.715321]\n",
      "epoch:24 step:22927 [D loss: 0.674846, acc: 58.59%] [G loss: 1.713194]\n",
      "epoch:24 step:22928 [D loss: 0.684948, acc: 60.94%] [G loss: 1.784695]\n",
      "epoch:24 step:22929 [D loss: 0.661875, acc: 59.38%] [G loss: 1.718400]\n",
      "epoch:24 step:22930 [D loss: 0.637284, acc: 63.28%] [G loss: 1.780585]\n",
      "epoch:24 step:22931 [D loss: 0.679692, acc: 48.44%] [G loss: 1.731173]\n",
      "epoch:24 step:22932 [D loss: 0.655651, acc: 63.28%] [G loss: 1.722965]\n",
      "epoch:24 step:22933 [D loss: 0.628246, acc: 58.59%] [G loss: 1.899580]\n",
      "epoch:24 step:22934 [D loss: 0.626217, acc: 63.28%] [G loss: 1.881408]\n",
      "epoch:24 step:22935 [D loss: 0.642650, acc: 68.75%] [G loss: 1.879077]\n",
      "epoch:24 step:22936 [D loss: 0.672230, acc: 57.03%] [G loss: 1.824147]\n",
      "epoch:24 step:22937 [D loss: 0.662638, acc: 58.59%] [G loss: 1.864364]\n",
      "epoch:24 step:22938 [D loss: 0.650205, acc: 61.72%] [G loss: 1.749968]\n",
      "epoch:24 step:22939 [D loss: 0.629239, acc: 62.50%] [G loss: 1.829163]\n",
      "epoch:24 step:22940 [D loss: 0.657970, acc: 62.50%] [G loss: 1.929938]\n",
      "epoch:24 step:22941 [D loss: 0.610502, acc: 64.06%] [G loss: 1.843201]\n",
      "epoch:24 step:22942 [D loss: 0.668995, acc: 53.91%] [G loss: 1.844531]\n",
      "epoch:24 step:22943 [D loss: 0.693731, acc: 62.50%] [G loss: 1.823420]\n",
      "epoch:24 step:22944 [D loss: 0.664898, acc: 60.94%] [G loss: 1.836153]\n",
      "epoch:24 step:22945 [D loss: 0.607991, acc: 69.53%] [G loss: 2.100048]\n",
      "epoch:24 step:22946 [D loss: 0.689262, acc: 54.69%] [G loss: 1.728471]\n",
      "epoch:24 step:22947 [D loss: 0.668018, acc: 58.59%] [G loss: 1.756143]\n",
      "epoch:24 step:22948 [D loss: 0.706549, acc: 50.78%] [G loss: 1.732844]\n",
      "epoch:24 step:22949 [D loss: 0.639357, acc: 64.84%] [G loss: 1.699887]\n",
      "epoch:24 step:22950 [D loss: 0.659312, acc: 62.50%] [G loss: 1.676720]\n",
      "epoch:24 step:22951 [D loss: 0.627762, acc: 63.28%] [G loss: 1.790361]\n",
      "epoch:24 step:22952 [D loss: 0.681231, acc: 55.47%] [G loss: 1.778890]\n",
      "epoch:24 step:22953 [D loss: 0.710948, acc: 54.69%] [G loss: 1.746630]\n",
      "epoch:24 step:22954 [D loss: 0.698262, acc: 60.94%] [G loss: 1.735362]\n",
      "epoch:24 step:22955 [D loss: 0.644547, acc: 63.28%] [G loss: 1.731176]\n",
      "epoch:24 step:22956 [D loss: 0.618059, acc: 64.06%] [G loss: 1.885090]\n",
      "epoch:24 step:22957 [D loss: 0.630945, acc: 66.41%] [G loss: 2.046109]\n",
      "epoch:24 step:22958 [D loss: 0.647243, acc: 61.72%] [G loss: 1.874854]\n",
      "epoch:24 step:22959 [D loss: 0.568351, acc: 72.66%] [G loss: 2.256286]\n",
      "epoch:24 step:22960 [D loss: 0.562759, acc: 75.00%] [G loss: 2.144229]\n",
      "epoch:24 step:22961 [D loss: 0.706088, acc: 53.91%] [G loss: 1.699102]\n",
      "epoch:24 step:22962 [D loss: 0.728748, acc: 50.78%] [G loss: 1.721254]\n",
      "epoch:24 step:22963 [D loss: 0.638942, acc: 63.28%] [G loss: 1.775736]\n",
      "epoch:24 step:22964 [D loss: 0.610401, acc: 71.09%] [G loss: 1.862192]\n",
      "epoch:24 step:22965 [D loss: 0.710005, acc: 57.81%] [G loss: 1.779060]\n",
      "epoch:24 step:22966 [D loss: 0.641827, acc: 60.16%] [G loss: 1.864426]\n",
      "epoch:24 step:22967 [D loss: 0.583054, acc: 71.09%] [G loss: 1.900977]\n",
      "epoch:24 step:22968 [D loss: 0.658053, acc: 60.16%] [G loss: 1.983600]\n",
      "epoch:24 step:22969 [D loss: 0.639301, acc: 60.16%] [G loss: 2.009411]\n",
      "epoch:24 step:22970 [D loss: 0.665471, acc: 59.38%] [G loss: 1.730806]\n",
      "epoch:24 step:22971 [D loss: 0.710623, acc: 52.34%] [G loss: 1.738165]\n",
      "epoch:24 step:22972 [D loss: 0.638463, acc: 63.28%] [G loss: 1.735214]\n",
      "epoch:24 step:22973 [D loss: 0.690739, acc: 57.81%] [G loss: 1.811559]\n",
      "epoch:24 step:22974 [D loss: 0.627968, acc: 67.97%] [G loss: 1.814081]\n",
      "epoch:24 step:22975 [D loss: 0.611427, acc: 67.97%] [G loss: 1.935702]\n",
      "epoch:24 step:22976 [D loss: 0.594422, acc: 71.88%] [G loss: 1.910530]\n",
      "epoch:24 step:22977 [D loss: 0.678408, acc: 61.72%] [G loss: 1.905849]\n",
      "epoch:24 step:22978 [D loss: 0.669020, acc: 64.84%] [G loss: 1.842498]\n",
      "epoch:24 step:22979 [D loss: 0.641779, acc: 64.84%] [G loss: 1.882803]\n",
      "epoch:24 step:22980 [D loss: 0.650325, acc: 64.06%] [G loss: 1.730449]\n",
      "epoch:24 step:22981 [D loss: 0.661349, acc: 58.59%] [G loss: 1.839351]\n",
      "epoch:24 step:22982 [D loss: 0.628716, acc: 66.41%] [G loss: 2.041754]\n",
      "epoch:24 step:22983 [D loss: 0.626954, acc: 64.84%] [G loss: 1.963969]\n",
      "epoch:24 step:22984 [D loss: 0.609084, acc: 71.09%] [G loss: 1.977401]\n",
      "epoch:24 step:22985 [D loss: 0.691964, acc: 56.25%] [G loss: 1.901220]\n",
      "epoch:24 step:22986 [D loss: 0.593534, acc: 71.09%] [G loss: 1.958381]\n",
      "epoch:24 step:22987 [D loss: 0.678159, acc: 56.25%] [G loss: 2.048207]\n",
      "epoch:24 step:22988 [D loss: 0.667232, acc: 60.94%] [G loss: 1.782520]\n",
      "epoch:24 step:22989 [D loss: 0.703404, acc: 52.34%] [G loss: 1.758859]\n",
      "epoch:24 step:22990 [D loss: 0.736780, acc: 49.22%] [G loss: 1.764582]\n",
      "epoch:24 step:22991 [D loss: 0.634355, acc: 60.16%] [G loss: 1.797767]\n",
      "epoch:24 step:22992 [D loss: 0.649133, acc: 64.06%] [G loss: 2.060578]\n",
      "epoch:24 step:22993 [D loss: 0.584407, acc: 67.19%] [G loss: 1.837496]\n",
      "epoch:24 step:22994 [D loss: 0.719756, acc: 56.25%] [G loss: 1.761092]\n",
      "epoch:24 step:22995 [D loss: 0.673412, acc: 59.38%] [G loss: 1.817494]\n",
      "epoch:24 step:22996 [D loss: 0.634395, acc: 64.84%] [G loss: 1.972704]\n",
      "epoch:24 step:22997 [D loss: 0.651863, acc: 57.03%] [G loss: 1.827073]\n",
      "epoch:24 step:22998 [D loss: 0.716577, acc: 53.12%] [G loss: 1.689687]\n",
      "epoch:24 step:22999 [D loss: 0.731801, acc: 55.47%] [G loss: 1.724832]\n",
      "epoch:24 step:23000 [D loss: 0.659746, acc: 57.03%] [G loss: 1.810539]\n",
      "epoch:24 step:23001 [D loss: 0.641318, acc: 62.50%] [G loss: 1.868384]\n",
      "epoch:24 step:23002 [D loss: 0.624694, acc: 66.41%] [G loss: 1.803829]\n",
      "epoch:24 step:23003 [D loss: 0.643126, acc: 63.28%] [G loss: 1.959627]\n",
      "epoch:24 step:23004 [D loss: 0.640673, acc: 60.16%] [G loss: 1.793513]\n",
      "epoch:24 step:23005 [D loss: 0.636364, acc: 64.84%] [G loss: 1.816361]\n",
      "epoch:24 step:23006 [D loss: 0.620583, acc: 65.62%] [G loss: 1.811367]\n",
      "epoch:24 step:23007 [D loss: 0.650832, acc: 63.28%] [G loss: 1.818217]\n",
      "epoch:24 step:23008 [D loss: 0.632983, acc: 69.53%] [G loss: 1.874818]\n",
      "epoch:24 step:23009 [D loss: 0.633868, acc: 64.84%] [G loss: 1.871894]\n",
      "epoch:24 step:23010 [D loss: 0.607326, acc: 66.41%] [G loss: 1.881677]\n",
      "epoch:24 step:23011 [D loss: 0.589597, acc: 66.41%] [G loss: 1.890701]\n",
      "epoch:24 step:23012 [D loss: 0.652091, acc: 58.59%] [G loss: 1.888184]\n",
      "epoch:24 step:23013 [D loss: 0.656859, acc: 64.06%] [G loss: 1.812458]\n",
      "epoch:24 step:23014 [D loss: 0.619649, acc: 66.41%] [G loss: 1.842314]\n",
      "epoch:24 step:23015 [D loss: 0.658628, acc: 61.72%] [G loss: 1.807548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23016 [D loss: 0.721164, acc: 53.91%] [G loss: 1.731805]\n",
      "epoch:24 step:23017 [D loss: 0.701113, acc: 55.47%] [G loss: 1.635166]\n",
      "epoch:24 step:23018 [D loss: 0.682860, acc: 57.81%] [G loss: 1.758261]\n",
      "epoch:24 step:23019 [D loss: 0.661377, acc: 59.38%] [G loss: 1.836198]\n",
      "epoch:24 step:23020 [D loss: 0.634232, acc: 64.84%] [G loss: 1.992393]\n",
      "epoch:24 step:23021 [D loss: 0.641670, acc: 61.72%] [G loss: 1.978717]\n",
      "epoch:24 step:23022 [D loss: 0.627885, acc: 65.62%] [G loss: 1.946495]\n",
      "epoch:24 step:23023 [D loss: 0.665270, acc: 57.81%] [G loss: 1.842034]\n",
      "epoch:24 step:23024 [D loss: 0.661301, acc: 61.72%] [G loss: 1.836106]\n",
      "epoch:24 step:23025 [D loss: 0.657919, acc: 61.72%] [G loss: 1.772215]\n",
      "epoch:24 step:23026 [D loss: 0.655135, acc: 58.59%] [G loss: 1.827159]\n",
      "epoch:24 step:23027 [D loss: 0.602575, acc: 64.84%] [G loss: 1.811827]\n",
      "epoch:24 step:23028 [D loss: 0.688618, acc: 56.25%] [G loss: 1.840662]\n",
      "epoch:24 step:23029 [D loss: 0.677660, acc: 60.16%] [G loss: 1.808929]\n",
      "epoch:24 step:23030 [D loss: 0.683571, acc: 54.69%] [G loss: 1.726206]\n",
      "epoch:24 step:23031 [D loss: 0.642870, acc: 66.41%] [G loss: 1.880317]\n",
      "epoch:24 step:23032 [D loss: 0.668718, acc: 60.16%] [G loss: 1.820866]\n",
      "epoch:24 step:23033 [D loss: 0.662414, acc: 60.94%] [G loss: 1.885111]\n",
      "epoch:24 step:23034 [D loss: 0.632990, acc: 64.06%] [G loss: 1.858213]\n",
      "epoch:24 step:23035 [D loss: 0.659454, acc: 63.28%] [G loss: 1.899284]\n",
      "epoch:24 step:23036 [D loss: 0.664950, acc: 57.03%] [G loss: 1.886881]\n",
      "epoch:24 step:23037 [D loss: 0.627311, acc: 67.19%] [G loss: 1.936056]\n",
      "epoch:24 step:23038 [D loss: 0.569295, acc: 73.44%] [G loss: 2.011351]\n",
      "epoch:24 step:23039 [D loss: 0.615391, acc: 67.97%] [G loss: 2.124622]\n",
      "epoch:24 step:23040 [D loss: 0.575499, acc: 74.22%] [G loss: 1.971080]\n",
      "epoch:24 step:23041 [D loss: 0.649364, acc: 63.28%] [G loss: 1.824271]\n",
      "epoch:24 step:23042 [D loss: 0.584173, acc: 70.31%] [G loss: 2.120147]\n",
      "epoch:24 step:23043 [D loss: 0.567179, acc: 70.31%] [G loss: 1.983347]\n",
      "epoch:24 step:23044 [D loss: 0.606609, acc: 68.75%] [G loss: 2.029683]\n",
      "epoch:24 step:23045 [D loss: 0.633576, acc: 66.41%] [G loss: 2.083993]\n",
      "epoch:24 step:23046 [D loss: 0.649715, acc: 60.94%] [G loss: 2.012061]\n",
      "epoch:24 step:23047 [D loss: 0.685872, acc: 54.69%] [G loss: 1.941765]\n",
      "epoch:24 step:23048 [D loss: 0.646152, acc: 66.41%] [G loss: 1.790047]\n",
      "epoch:24 step:23049 [D loss: 0.663704, acc: 62.50%] [G loss: 1.895659]\n",
      "epoch:24 step:23050 [D loss: 0.661873, acc: 63.28%] [G loss: 1.913106]\n",
      "epoch:24 step:23051 [D loss: 0.658681, acc: 57.03%] [G loss: 1.930270]\n",
      "epoch:24 step:23052 [D loss: 0.592517, acc: 69.53%] [G loss: 2.214807]\n",
      "epoch:24 step:23053 [D loss: 0.657174, acc: 57.81%] [G loss: 1.855246]\n",
      "epoch:24 step:23054 [D loss: 0.806049, acc: 40.62%] [G loss: 1.675914]\n",
      "epoch:24 step:23055 [D loss: 0.662367, acc: 56.25%] [G loss: 1.868229]\n",
      "epoch:24 step:23056 [D loss: 0.625238, acc: 68.75%] [G loss: 1.785381]\n",
      "epoch:24 step:23057 [D loss: 0.608952, acc: 64.06%] [G loss: 1.862584]\n",
      "epoch:24 step:23058 [D loss: 0.638359, acc: 64.06%] [G loss: 1.803603]\n",
      "epoch:24 step:23059 [D loss: 0.637857, acc: 63.28%] [G loss: 1.897527]\n",
      "epoch:24 step:23060 [D loss: 0.656889, acc: 65.62%] [G loss: 1.802508]\n",
      "epoch:24 step:23061 [D loss: 0.696760, acc: 61.72%] [G loss: 1.788376]\n",
      "epoch:24 step:23062 [D loss: 0.642405, acc: 61.72%] [G loss: 1.822745]\n",
      "epoch:24 step:23063 [D loss: 0.607325, acc: 67.19%] [G loss: 1.756186]\n",
      "epoch:24 step:23064 [D loss: 0.659430, acc: 62.50%] [G loss: 1.756687]\n",
      "epoch:24 step:23065 [D loss: 0.687016, acc: 58.59%] [G loss: 1.630226]\n",
      "epoch:24 step:23066 [D loss: 0.647698, acc: 58.59%] [G loss: 1.858330]\n",
      "epoch:24 step:23067 [D loss: 0.685112, acc: 53.91%] [G loss: 1.722955]\n",
      "epoch:24 step:23068 [D loss: 0.673051, acc: 58.59%] [G loss: 1.859780]\n",
      "epoch:24 step:23069 [D loss: 0.650550, acc: 58.59%] [G loss: 1.793693]\n",
      "epoch:24 step:23070 [D loss: 0.635841, acc: 60.94%] [G loss: 1.832342]\n",
      "epoch:24 step:23071 [D loss: 0.612425, acc: 70.31%] [G loss: 1.785363]\n",
      "epoch:24 step:23072 [D loss: 0.701165, acc: 55.47%] [G loss: 1.803479]\n",
      "epoch:24 step:23073 [D loss: 0.660462, acc: 57.81%] [G loss: 1.806729]\n",
      "epoch:24 step:23074 [D loss: 0.623199, acc: 64.06%] [G loss: 1.776074]\n",
      "epoch:24 step:23075 [D loss: 0.658814, acc: 57.03%] [G loss: 1.854223]\n",
      "epoch:24 step:23076 [D loss: 0.674431, acc: 60.94%] [G loss: 1.939046]\n",
      "epoch:24 step:23077 [D loss: 0.610614, acc: 69.53%] [G loss: 1.950804]\n",
      "epoch:24 step:23078 [D loss: 0.649922, acc: 62.50%] [G loss: 1.794673]\n",
      "epoch:24 step:23079 [D loss: 0.663676, acc: 61.72%] [G loss: 1.970944]\n",
      "epoch:24 step:23080 [D loss: 0.685888, acc: 59.38%] [G loss: 1.888494]\n",
      "epoch:24 step:23081 [D loss: 0.658153, acc: 62.50%] [G loss: 1.801368]\n",
      "epoch:24 step:23082 [D loss: 0.678526, acc: 54.69%] [G loss: 1.756269]\n",
      "epoch:24 step:23083 [D loss: 0.654542, acc: 67.97%] [G loss: 1.878810]\n",
      "epoch:24 step:23084 [D loss: 0.627199, acc: 64.06%] [G loss: 1.717269]\n",
      "epoch:24 step:23085 [D loss: 0.686230, acc: 57.03%] [G loss: 1.844381]\n",
      "epoch:24 step:23086 [D loss: 0.666090, acc: 61.72%] [G loss: 1.942644]\n",
      "epoch:24 step:23087 [D loss: 0.677138, acc: 62.50%] [G loss: 1.723772]\n",
      "epoch:24 step:23088 [D loss: 0.689857, acc: 60.16%] [G loss: 1.873967]\n",
      "epoch:24 step:23089 [D loss: 0.615153, acc: 66.41%] [G loss: 1.871982]\n",
      "epoch:24 step:23090 [D loss: 0.690970, acc: 57.81%] [G loss: 1.908650]\n",
      "epoch:24 step:23091 [D loss: 0.632945, acc: 69.53%] [G loss: 1.799381]\n",
      "epoch:24 step:23092 [D loss: 0.656775, acc: 64.06%] [G loss: 1.878083]\n",
      "epoch:24 step:23093 [D loss: 0.556723, acc: 78.12%] [G loss: 1.906283]\n",
      "epoch:24 step:23094 [D loss: 0.670640, acc: 60.16%] [G loss: 1.777118]\n",
      "epoch:24 step:23095 [D loss: 0.643587, acc: 60.94%] [G loss: 1.843582]\n",
      "epoch:24 step:23096 [D loss: 0.660050, acc: 59.38%] [G loss: 1.778599]\n",
      "epoch:24 step:23097 [D loss: 0.662292, acc: 64.06%] [G loss: 1.884710]\n",
      "epoch:24 step:23098 [D loss: 0.662567, acc: 61.72%] [G loss: 1.995913]\n",
      "epoch:24 step:23099 [D loss: 0.639951, acc: 59.38%] [G loss: 1.799964]\n",
      "epoch:24 step:23100 [D loss: 0.657290, acc: 56.25%] [G loss: 1.743658]\n",
      "epoch:24 step:23101 [D loss: 0.613222, acc: 65.62%] [G loss: 1.816520]\n",
      "epoch:24 step:23102 [D loss: 0.713170, acc: 54.69%] [G loss: 1.705944]\n",
      "epoch:24 step:23103 [D loss: 0.697953, acc: 59.38%] [G loss: 1.738684]\n",
      "epoch:24 step:23104 [D loss: 0.687585, acc: 58.59%] [G loss: 1.855021]\n",
      "epoch:24 step:23105 [D loss: 0.663647, acc: 63.28%] [G loss: 1.723894]\n",
      "epoch:24 step:23106 [D loss: 0.630656, acc: 68.75%] [G loss: 1.840775]\n",
      "epoch:24 step:23107 [D loss: 0.665866, acc: 60.16%] [G loss: 1.750398]\n",
      "epoch:24 step:23108 [D loss: 0.657293, acc: 61.72%] [G loss: 1.840490]\n",
      "epoch:24 step:23109 [D loss: 0.610078, acc: 62.50%] [G loss: 1.832949]\n",
      "epoch:24 step:23110 [D loss: 0.656878, acc: 59.38%] [G loss: 1.948812]\n",
      "epoch:24 step:23111 [D loss: 0.643373, acc: 61.72%] [G loss: 1.868538]\n",
      "epoch:24 step:23112 [D loss: 0.609408, acc: 67.97%] [G loss: 2.010293]\n",
      "epoch:24 step:23113 [D loss: 0.663307, acc: 60.16%] [G loss: 1.777399]\n",
      "epoch:24 step:23114 [D loss: 0.685441, acc: 58.59%] [G loss: 1.805240]\n",
      "epoch:24 step:23115 [D loss: 0.644273, acc: 58.59%] [G loss: 1.891681]\n",
      "epoch:24 step:23116 [D loss: 0.651162, acc: 62.50%] [G loss: 1.770716]\n",
      "epoch:24 step:23117 [D loss: 0.641565, acc: 59.38%] [G loss: 1.796486]\n",
      "epoch:24 step:23118 [D loss: 0.668831, acc: 57.81%] [G loss: 1.884430]\n",
      "epoch:24 step:23119 [D loss: 0.693232, acc: 59.38%] [G loss: 1.841118]\n",
      "epoch:24 step:23120 [D loss: 0.618150, acc: 67.19%] [G loss: 1.954742]\n",
      "epoch:24 step:23121 [D loss: 0.601357, acc: 66.41%] [G loss: 1.855968]\n",
      "epoch:24 step:23122 [D loss: 0.610240, acc: 66.41%] [G loss: 2.096703]\n",
      "epoch:24 step:23123 [D loss: 0.617820, acc: 67.19%] [G loss: 1.942735]\n",
      "epoch:24 step:23124 [D loss: 0.677153, acc: 54.69%] [G loss: 1.906129]\n",
      "epoch:24 step:23125 [D loss: 0.650822, acc: 64.06%] [G loss: 1.889343]\n",
      "epoch:24 step:23126 [D loss: 0.668413, acc: 56.25%] [G loss: 1.864562]\n",
      "epoch:24 step:23127 [D loss: 0.650801, acc: 62.50%] [G loss: 1.810388]\n",
      "epoch:24 step:23128 [D loss: 0.709929, acc: 61.72%] [G loss: 1.977042]\n",
      "epoch:24 step:23129 [D loss: 0.638965, acc: 60.94%] [G loss: 1.912228]\n",
      "epoch:24 step:23130 [D loss: 0.645574, acc: 60.94%] [G loss: 1.887374]\n",
      "epoch:24 step:23131 [D loss: 0.644584, acc: 63.28%] [G loss: 1.824554]\n",
      "epoch:24 step:23132 [D loss: 0.630628, acc: 60.94%] [G loss: 1.881807]\n",
      "epoch:24 step:23133 [D loss: 0.668371, acc: 62.50%] [G loss: 1.953682]\n",
      "epoch:24 step:23134 [D loss: 0.642433, acc: 63.28%] [G loss: 1.924113]\n",
      "epoch:24 step:23135 [D loss: 0.634469, acc: 63.28%] [G loss: 1.923335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23136 [D loss: 0.569261, acc: 64.84%] [G loss: 2.312732]\n",
      "epoch:24 step:23137 [D loss: 0.590277, acc: 68.75%] [G loss: 2.083992]\n",
      "epoch:24 step:23138 [D loss: 0.620731, acc: 64.84%] [G loss: 2.087733]\n",
      "epoch:24 step:23139 [D loss: 0.661478, acc: 55.47%] [G loss: 1.810182]\n",
      "epoch:24 step:23140 [D loss: 0.676166, acc: 60.16%] [G loss: 1.867830]\n",
      "epoch:24 step:23141 [D loss: 0.664173, acc: 61.72%] [G loss: 2.020040]\n",
      "epoch:24 step:23142 [D loss: 0.672691, acc: 59.38%] [G loss: 1.934198]\n",
      "epoch:24 step:23143 [D loss: 0.681695, acc: 56.25%] [G loss: 1.815055]\n",
      "epoch:24 step:23144 [D loss: 0.734164, acc: 53.12%] [G loss: 1.752954]\n",
      "epoch:24 step:23145 [D loss: 0.673863, acc: 59.38%] [G loss: 1.788597]\n",
      "epoch:24 step:23146 [D loss: 0.678235, acc: 53.12%] [G loss: 1.735054]\n",
      "epoch:24 step:23147 [D loss: 0.661059, acc: 59.38%] [G loss: 1.746108]\n",
      "epoch:24 step:23148 [D loss: 0.670840, acc: 60.16%] [G loss: 1.876570]\n",
      "epoch:24 step:23149 [D loss: 0.638276, acc: 60.16%] [G loss: 1.831649]\n",
      "epoch:24 step:23150 [D loss: 0.659107, acc: 57.03%] [G loss: 1.890846]\n",
      "epoch:24 step:23151 [D loss: 0.662538, acc: 57.81%] [G loss: 1.857782]\n",
      "epoch:24 step:23152 [D loss: 0.620884, acc: 63.28%] [G loss: 1.846809]\n",
      "epoch:24 step:23153 [D loss: 0.667205, acc: 57.81%] [G loss: 1.789601]\n",
      "epoch:24 step:23154 [D loss: 0.616128, acc: 66.41%] [G loss: 1.835888]\n",
      "epoch:24 step:23155 [D loss: 0.637267, acc: 58.59%] [G loss: 1.728496]\n",
      "epoch:24 step:23156 [D loss: 0.648885, acc: 56.25%] [G loss: 1.802577]\n",
      "epoch:24 step:23157 [D loss: 0.616752, acc: 67.19%] [G loss: 1.735963]\n",
      "epoch:24 step:23158 [D loss: 0.668366, acc: 61.72%] [G loss: 1.856605]\n",
      "epoch:24 step:23159 [D loss: 0.604165, acc: 66.41%] [G loss: 1.850108]\n",
      "epoch:24 step:23160 [D loss: 0.688785, acc: 59.38%] [G loss: 1.847867]\n",
      "epoch:24 step:23161 [D loss: 0.657949, acc: 62.50%] [G loss: 1.836944]\n",
      "epoch:24 step:23162 [D loss: 0.603431, acc: 67.97%] [G loss: 1.848387]\n",
      "epoch:24 step:23163 [D loss: 0.647798, acc: 61.72%] [G loss: 1.841980]\n",
      "epoch:24 step:23164 [D loss: 0.619109, acc: 63.28%] [G loss: 1.841882]\n",
      "epoch:24 step:23165 [D loss: 0.635879, acc: 64.06%] [G loss: 1.862795]\n",
      "epoch:24 step:23166 [D loss: 0.603600, acc: 71.09%] [G loss: 1.836157]\n",
      "epoch:24 step:23167 [D loss: 0.626712, acc: 63.28%] [G loss: 1.806908]\n",
      "epoch:24 step:23168 [D loss: 0.712992, acc: 55.47%] [G loss: 1.937460]\n",
      "epoch:24 step:23169 [D loss: 0.611815, acc: 67.97%] [G loss: 1.899506]\n",
      "epoch:24 step:23170 [D loss: 0.666163, acc: 61.72%] [G loss: 1.841352]\n",
      "epoch:24 step:23171 [D loss: 0.626436, acc: 60.94%] [G loss: 1.940384]\n",
      "epoch:24 step:23172 [D loss: 0.633212, acc: 62.50%] [G loss: 1.788486]\n",
      "epoch:24 step:23173 [D loss: 0.698682, acc: 54.69%] [G loss: 1.854194]\n",
      "epoch:24 step:23174 [D loss: 0.643630, acc: 65.62%] [G loss: 1.950330]\n",
      "epoch:24 step:23175 [D loss: 0.624898, acc: 67.97%] [G loss: 1.978607]\n",
      "epoch:24 step:23176 [D loss: 0.649804, acc: 61.72%] [G loss: 1.955106]\n",
      "epoch:24 step:23177 [D loss: 0.664836, acc: 60.94%] [G loss: 2.127624]\n",
      "epoch:24 step:23178 [D loss: 0.607936, acc: 60.94%] [G loss: 1.909212]\n",
      "epoch:24 step:23179 [D loss: 0.668567, acc: 56.25%] [G loss: 1.934518]\n",
      "epoch:24 step:23180 [D loss: 0.649555, acc: 60.94%] [G loss: 1.954383]\n",
      "epoch:24 step:23181 [D loss: 0.591351, acc: 72.66%] [G loss: 2.126448]\n",
      "epoch:24 step:23182 [D loss: 0.591532, acc: 69.53%] [G loss: 2.322883]\n",
      "epoch:24 step:23183 [D loss: 0.663238, acc: 62.50%] [G loss: 1.962124]\n",
      "epoch:24 step:23184 [D loss: 0.615027, acc: 64.84%] [G loss: 1.950425]\n",
      "epoch:24 step:23185 [D loss: 0.675468, acc: 60.94%] [G loss: 1.882800]\n",
      "epoch:24 step:23186 [D loss: 0.662360, acc: 62.50%] [G loss: 1.888457]\n",
      "epoch:24 step:23187 [D loss: 0.644775, acc: 60.94%] [G loss: 1.914263]\n",
      "epoch:24 step:23188 [D loss: 0.625787, acc: 64.84%] [G loss: 1.843817]\n",
      "epoch:24 step:23189 [D loss: 0.639736, acc: 65.62%] [G loss: 1.993286]\n",
      "epoch:24 step:23190 [D loss: 0.758139, acc: 54.69%] [G loss: 1.792261]\n",
      "epoch:24 step:23191 [D loss: 0.693678, acc: 60.94%] [G loss: 1.875150]\n",
      "epoch:24 step:23192 [D loss: 0.658904, acc: 60.94%] [G loss: 1.762371]\n",
      "epoch:24 step:23193 [D loss: 0.703441, acc: 52.34%] [G loss: 1.828198]\n",
      "epoch:24 step:23194 [D loss: 0.622697, acc: 62.50%] [G loss: 1.912335]\n",
      "epoch:24 step:23195 [D loss: 0.613264, acc: 69.53%] [G loss: 1.922113]\n",
      "epoch:24 step:23196 [D loss: 0.611646, acc: 71.88%] [G loss: 1.914229]\n",
      "epoch:24 step:23197 [D loss: 0.685707, acc: 60.94%] [G loss: 1.969296]\n",
      "epoch:24 step:23198 [D loss: 0.626028, acc: 62.50%] [G loss: 1.721002]\n",
      "epoch:24 step:23199 [D loss: 0.650916, acc: 60.94%] [G loss: 2.067851]\n",
      "epoch:24 step:23200 [D loss: 0.590770, acc: 71.09%] [G loss: 1.977030]\n",
      "epoch:24 step:23201 [D loss: 0.649690, acc: 61.72%] [G loss: 1.896137]\n",
      "epoch:24 step:23202 [D loss: 0.636034, acc: 55.47%] [G loss: 1.902825]\n",
      "epoch:24 step:23203 [D loss: 0.640034, acc: 60.94%] [G loss: 1.917668]\n",
      "epoch:24 step:23204 [D loss: 0.697607, acc: 56.25%] [G loss: 1.753834]\n",
      "epoch:24 step:23205 [D loss: 0.661930, acc: 60.94%] [G loss: 1.652878]\n",
      "epoch:24 step:23206 [D loss: 0.629494, acc: 64.84%] [G loss: 1.945654]\n",
      "epoch:24 step:23207 [D loss: 0.596749, acc: 67.97%] [G loss: 1.905222]\n",
      "epoch:24 step:23208 [D loss: 0.653179, acc: 64.84%] [G loss: 1.892377]\n",
      "epoch:24 step:23209 [D loss: 0.605895, acc: 66.41%] [G loss: 2.014064]\n",
      "epoch:24 step:23210 [D loss: 0.691341, acc: 59.38%] [G loss: 1.933143]\n",
      "epoch:24 step:23211 [D loss: 0.624480, acc: 67.97%] [G loss: 1.813100]\n",
      "epoch:24 step:23212 [D loss: 0.588708, acc: 66.41%] [G loss: 2.019720]\n",
      "epoch:24 step:23213 [D loss: 0.672732, acc: 60.16%] [G loss: 1.910424]\n",
      "epoch:24 step:23214 [D loss: 0.608502, acc: 63.28%] [G loss: 1.986602]\n",
      "epoch:24 step:23215 [D loss: 0.657311, acc: 65.62%] [G loss: 1.839373]\n",
      "epoch:24 step:23216 [D loss: 0.624817, acc: 63.28%] [G loss: 1.797027]\n",
      "epoch:24 step:23217 [D loss: 0.689461, acc: 53.12%] [G loss: 1.790133]\n",
      "epoch:24 step:23218 [D loss: 0.663319, acc: 58.59%] [G loss: 1.986186]\n",
      "epoch:24 step:23219 [D loss: 0.619174, acc: 60.94%] [G loss: 1.898457]\n",
      "epoch:24 step:23220 [D loss: 0.687746, acc: 58.59%] [G loss: 1.940489]\n",
      "epoch:24 step:23221 [D loss: 0.679590, acc: 56.25%] [G loss: 1.878839]\n",
      "epoch:24 step:23222 [D loss: 0.596282, acc: 69.53%] [G loss: 1.855909]\n",
      "epoch:24 step:23223 [D loss: 0.622231, acc: 63.28%] [G loss: 1.835491]\n",
      "epoch:24 step:23224 [D loss: 0.630193, acc: 62.50%] [G loss: 1.919766]\n",
      "epoch:24 step:23225 [D loss: 0.664277, acc: 61.72%] [G loss: 1.906677]\n",
      "epoch:24 step:23226 [D loss: 0.664101, acc: 58.59%] [G loss: 1.823532]\n",
      "epoch:24 step:23227 [D loss: 0.747635, acc: 50.78%] [G loss: 1.678384]\n",
      "epoch:24 step:23228 [D loss: 0.637406, acc: 67.97%] [G loss: 1.919950]\n",
      "epoch:24 step:23229 [D loss: 0.639830, acc: 67.19%] [G loss: 1.654034]\n",
      "epoch:24 step:23230 [D loss: 0.647143, acc: 62.50%] [G loss: 1.853719]\n",
      "epoch:24 step:23231 [D loss: 0.650333, acc: 60.16%] [G loss: 2.071419]\n",
      "epoch:24 step:23232 [D loss: 0.690844, acc: 57.03%] [G loss: 1.861264]\n",
      "epoch:24 step:23233 [D loss: 0.666159, acc: 62.50%] [G loss: 1.735464]\n",
      "epoch:24 step:23234 [D loss: 0.666729, acc: 60.94%] [G loss: 1.916680]\n",
      "epoch:24 step:23235 [D loss: 0.567567, acc: 71.88%] [G loss: 1.895272]\n",
      "epoch:24 step:23236 [D loss: 0.636882, acc: 57.81%] [G loss: 1.710458]\n",
      "epoch:24 step:23237 [D loss: 0.682327, acc: 56.25%] [G loss: 1.708058]\n",
      "epoch:24 step:23238 [D loss: 0.664064, acc: 61.72%] [G loss: 1.918816]\n",
      "epoch:24 step:23239 [D loss: 0.650381, acc: 60.16%] [G loss: 1.703566]\n",
      "epoch:24 step:23240 [D loss: 0.663184, acc: 57.81%] [G loss: 1.737229]\n",
      "epoch:24 step:23241 [D loss: 0.688821, acc: 57.03%] [G loss: 1.835703]\n",
      "epoch:24 step:23242 [D loss: 0.623798, acc: 67.97%] [G loss: 1.751778]\n",
      "epoch:24 step:23243 [D loss: 0.588248, acc: 71.88%] [G loss: 1.996184]\n",
      "epoch:24 step:23244 [D loss: 0.682330, acc: 61.72%] [G loss: 1.901413]\n",
      "epoch:24 step:23245 [D loss: 0.717631, acc: 57.81%] [G loss: 1.798812]\n",
      "epoch:24 step:23246 [D loss: 0.701314, acc: 53.12%] [G loss: 1.977022]\n",
      "epoch:24 step:23247 [D loss: 0.681988, acc: 59.38%] [G loss: 1.823393]\n",
      "epoch:24 step:23248 [D loss: 0.642142, acc: 60.16%] [G loss: 1.814658]\n",
      "epoch:24 step:23249 [D loss: 0.638768, acc: 60.94%] [G loss: 1.840921]\n",
      "epoch:24 step:23250 [D loss: 0.665193, acc: 58.59%] [G loss: 1.833978]\n",
      "epoch:24 step:23251 [D loss: 0.618258, acc: 66.41%] [G loss: 1.739072]\n",
      "epoch:24 step:23252 [D loss: 0.630828, acc: 62.50%] [G loss: 1.741989]\n",
      "epoch:24 step:23253 [D loss: 0.704050, acc: 54.69%] [G loss: 1.722643]\n",
      "epoch:24 step:23254 [D loss: 0.684434, acc: 57.03%] [G loss: 1.823941]\n",
      "epoch:24 step:23255 [D loss: 0.691810, acc: 53.91%] [G loss: 1.824414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23256 [D loss: 0.656844, acc: 61.72%] [G loss: 1.815071]\n",
      "epoch:24 step:23257 [D loss: 0.608800, acc: 63.28%] [G loss: 1.926927]\n",
      "epoch:24 step:23258 [D loss: 0.608608, acc: 71.09%] [G loss: 1.697177]\n",
      "epoch:24 step:23259 [D loss: 0.670954, acc: 57.81%] [G loss: 1.662448]\n",
      "epoch:24 step:23260 [D loss: 0.660607, acc: 62.50%] [G loss: 1.801348]\n",
      "epoch:24 step:23261 [D loss: 0.651303, acc: 63.28%] [G loss: 2.011214]\n",
      "epoch:24 step:23262 [D loss: 0.632319, acc: 67.19%] [G loss: 1.976530]\n",
      "epoch:24 step:23263 [D loss: 0.596544, acc: 65.62%] [G loss: 2.075316]\n",
      "epoch:24 step:23264 [D loss: 0.649275, acc: 61.72%] [G loss: 1.882985]\n",
      "epoch:24 step:23265 [D loss: 0.641838, acc: 58.59%] [G loss: 1.861641]\n",
      "epoch:24 step:23266 [D loss: 0.607900, acc: 67.97%] [G loss: 1.996459]\n",
      "epoch:24 step:23267 [D loss: 0.705112, acc: 55.47%] [G loss: 1.855799]\n",
      "epoch:24 step:23268 [D loss: 0.649863, acc: 62.50%] [G loss: 1.918248]\n",
      "epoch:24 step:23269 [D loss: 0.593276, acc: 66.41%] [G loss: 1.952708]\n",
      "epoch:24 step:23270 [D loss: 0.596094, acc: 71.88%] [G loss: 2.110532]\n",
      "epoch:24 step:23271 [D loss: 0.662789, acc: 62.50%] [G loss: 1.820398]\n",
      "epoch:24 step:23272 [D loss: 0.704488, acc: 53.12%] [G loss: 1.856969]\n",
      "epoch:24 step:23273 [D loss: 0.674075, acc: 60.94%] [G loss: 1.815205]\n",
      "epoch:24 step:23274 [D loss: 0.605722, acc: 70.31%] [G loss: 1.872507]\n",
      "epoch:24 step:23275 [D loss: 0.596564, acc: 66.41%] [G loss: 1.807189]\n",
      "epoch:24 step:23276 [D loss: 0.710019, acc: 54.69%] [G loss: 1.814230]\n",
      "epoch:24 step:23277 [D loss: 0.630025, acc: 64.84%] [G loss: 1.913550]\n",
      "epoch:24 step:23278 [D loss: 0.660919, acc: 55.47%] [G loss: 1.915571]\n",
      "epoch:24 step:23279 [D loss: 0.661869, acc: 62.50%] [G loss: 1.898747]\n",
      "epoch:24 step:23280 [D loss: 0.614367, acc: 73.44%] [G loss: 2.036202]\n",
      "epoch:24 step:23281 [D loss: 0.644474, acc: 61.72%] [G loss: 1.951158]\n",
      "epoch:24 step:23282 [D loss: 0.769420, acc: 42.97%] [G loss: 1.768414]\n",
      "epoch:24 step:23283 [D loss: 0.717235, acc: 56.25%] [G loss: 1.795693]\n",
      "epoch:24 step:23284 [D loss: 0.671976, acc: 60.16%] [G loss: 1.807559]\n",
      "epoch:24 step:23285 [D loss: 0.648158, acc: 60.16%] [G loss: 1.917449]\n",
      "epoch:24 step:23286 [D loss: 0.645578, acc: 60.16%] [G loss: 1.685107]\n",
      "epoch:24 step:23287 [D loss: 0.716115, acc: 55.47%] [G loss: 1.856904]\n",
      "epoch:24 step:23288 [D loss: 0.687048, acc: 56.25%] [G loss: 1.721733]\n",
      "epoch:24 step:23289 [D loss: 0.707670, acc: 54.69%] [G loss: 1.720969]\n",
      "epoch:24 step:23290 [D loss: 0.667344, acc: 60.94%] [G loss: 1.724822]\n",
      "epoch:24 step:23291 [D loss: 0.664013, acc: 64.06%] [G loss: 1.737553]\n",
      "epoch:24 step:23292 [D loss: 0.654485, acc: 57.03%] [G loss: 1.854175]\n",
      "epoch:24 step:23293 [D loss: 0.631734, acc: 67.97%] [G loss: 1.801010]\n",
      "epoch:24 step:23294 [D loss: 0.665330, acc: 63.28%] [G loss: 1.825640]\n",
      "epoch:24 step:23295 [D loss: 0.639619, acc: 59.38%] [G loss: 1.856116]\n",
      "epoch:24 step:23296 [D loss: 0.638790, acc: 64.84%] [G loss: 1.724665]\n",
      "epoch:24 step:23297 [D loss: 0.675121, acc: 57.03%] [G loss: 1.805003]\n",
      "epoch:24 step:23298 [D loss: 0.675661, acc: 57.81%] [G loss: 1.623682]\n",
      "epoch:24 step:23299 [D loss: 0.675238, acc: 59.38%] [G loss: 1.752746]\n",
      "epoch:24 step:23300 [D loss: 0.675231, acc: 52.34%] [G loss: 1.761830]\n",
      "epoch:24 step:23301 [D loss: 0.688641, acc: 55.47%] [G loss: 1.781486]\n",
      "epoch:24 step:23302 [D loss: 0.622794, acc: 64.06%] [G loss: 1.745627]\n",
      "epoch:24 step:23303 [D loss: 0.609978, acc: 73.44%] [G loss: 1.935544]\n",
      "epoch:24 step:23304 [D loss: 0.692965, acc: 57.03%] [G loss: 1.802711]\n",
      "epoch:24 step:23305 [D loss: 0.688669, acc: 57.81%] [G loss: 1.762056]\n",
      "epoch:24 step:23306 [D loss: 0.693380, acc: 57.81%] [G loss: 1.654766]\n",
      "epoch:24 step:23307 [D loss: 0.672331, acc: 59.38%] [G loss: 1.759873]\n",
      "epoch:24 step:23308 [D loss: 0.687817, acc: 57.03%] [G loss: 1.660619]\n",
      "epoch:24 step:23309 [D loss: 0.628942, acc: 64.84%] [G loss: 1.821028]\n",
      "epoch:24 step:23310 [D loss: 0.648823, acc: 60.94%] [G loss: 1.871857]\n",
      "epoch:24 step:23311 [D loss: 0.597988, acc: 65.62%] [G loss: 1.887741]\n",
      "epoch:24 step:23312 [D loss: 0.655378, acc: 59.38%] [G loss: 1.722084]\n",
      "epoch:24 step:23313 [D loss: 0.615821, acc: 66.41%] [G loss: 1.909434]\n",
      "epoch:24 step:23314 [D loss: 0.684235, acc: 55.47%] [G loss: 1.834805]\n",
      "epoch:24 step:23315 [D loss: 0.658490, acc: 57.81%] [G loss: 1.669517]\n",
      "epoch:24 step:23316 [D loss: 0.649003, acc: 57.81%] [G loss: 1.688295]\n",
      "epoch:24 step:23317 [D loss: 0.633825, acc: 62.50%] [G loss: 1.867280]\n",
      "epoch:24 step:23318 [D loss: 0.662023, acc: 61.72%] [G loss: 1.755233]\n",
      "epoch:24 step:23319 [D loss: 0.645676, acc: 68.75%] [G loss: 1.901417]\n",
      "epoch:24 step:23320 [D loss: 0.655476, acc: 61.72%] [G loss: 1.853753]\n",
      "epoch:24 step:23321 [D loss: 0.638354, acc: 63.28%] [G loss: 1.987354]\n",
      "epoch:24 step:23322 [D loss: 0.637337, acc: 64.06%] [G loss: 1.825322]\n",
      "epoch:24 step:23323 [D loss: 0.736694, acc: 58.59%] [G loss: 1.751766]\n",
      "epoch:24 step:23324 [D loss: 0.666577, acc: 59.38%] [G loss: 1.933183]\n",
      "epoch:24 step:23325 [D loss: 0.621033, acc: 61.72%] [G loss: 1.811317]\n",
      "epoch:24 step:23326 [D loss: 0.667606, acc: 64.84%] [G loss: 1.906631]\n",
      "epoch:24 step:23327 [D loss: 0.634613, acc: 64.06%] [G loss: 1.958429]\n",
      "epoch:24 step:23328 [D loss: 0.589884, acc: 73.44%] [G loss: 1.879015]\n",
      "epoch:24 step:23329 [D loss: 0.661788, acc: 57.81%] [G loss: 2.113979]\n",
      "epoch:24 step:23330 [D loss: 0.632479, acc: 60.94%] [G loss: 2.003454]\n",
      "epoch:24 step:23331 [D loss: 0.619740, acc: 66.41%] [G loss: 1.959933]\n",
      "epoch:24 step:23332 [D loss: 0.679247, acc: 57.81%] [G loss: 1.772550]\n",
      "epoch:24 step:23333 [D loss: 0.630389, acc: 67.19%] [G loss: 2.037815]\n",
      "epoch:24 step:23334 [D loss: 0.618426, acc: 62.50%] [G loss: 1.766803]\n",
      "epoch:24 step:23335 [D loss: 0.675199, acc: 60.94%] [G loss: 1.886054]\n",
      "epoch:24 step:23336 [D loss: 0.693714, acc: 58.59%] [G loss: 1.830838]\n",
      "epoch:24 step:23337 [D loss: 0.644556, acc: 64.84%] [G loss: 1.913958]\n",
      "epoch:24 step:23338 [D loss: 0.706767, acc: 52.34%] [G loss: 1.689174]\n",
      "epoch:24 step:23339 [D loss: 0.653444, acc: 65.62%] [G loss: 1.823279]\n",
      "epoch:24 step:23340 [D loss: 0.694338, acc: 59.38%] [G loss: 1.832636]\n",
      "epoch:24 step:23341 [D loss: 0.655233, acc: 61.72%] [G loss: 1.686654]\n",
      "epoch:24 step:23342 [D loss: 0.604353, acc: 71.09%] [G loss: 1.723560]\n",
      "epoch:24 step:23343 [D loss: 0.647973, acc: 60.16%] [G loss: 1.753972]\n",
      "epoch:24 step:23344 [D loss: 0.639000, acc: 64.06%] [G loss: 1.717813]\n",
      "epoch:24 step:23345 [D loss: 0.633285, acc: 71.88%] [G loss: 1.879681]\n",
      "epoch:24 step:23346 [D loss: 0.690199, acc: 59.38%] [G loss: 1.787712]\n",
      "epoch:24 step:23347 [D loss: 0.671523, acc: 58.59%] [G loss: 1.901883]\n",
      "epoch:24 step:23348 [D loss: 0.662914, acc: 59.38%] [G loss: 1.915955]\n",
      "epoch:24 step:23349 [D loss: 0.652150, acc: 63.28%] [G loss: 1.839744]\n",
      "epoch:24 step:23350 [D loss: 0.650061, acc: 64.06%] [G loss: 1.719170]\n",
      "epoch:24 step:23351 [D loss: 0.614358, acc: 64.06%] [G loss: 1.836897]\n",
      "epoch:24 step:23352 [D loss: 0.648652, acc: 63.28%] [G loss: 1.807482]\n",
      "epoch:24 step:23353 [D loss: 0.673715, acc: 62.50%] [G loss: 1.757637]\n",
      "epoch:24 step:23354 [D loss: 0.665375, acc: 62.50%] [G loss: 1.886901]\n",
      "epoch:24 step:23355 [D loss: 0.670185, acc: 60.16%] [G loss: 1.745328]\n",
      "epoch:24 step:23356 [D loss: 0.603368, acc: 70.31%] [G loss: 1.849352]\n",
      "epoch:24 step:23357 [D loss: 0.729272, acc: 55.47%] [G loss: 1.756465]\n",
      "epoch:24 step:23358 [D loss: 0.627050, acc: 62.50%] [G loss: 1.767899]\n",
      "epoch:24 step:23359 [D loss: 0.645985, acc: 61.72%] [G loss: 1.723493]\n",
      "epoch:24 step:23360 [D loss: 0.680139, acc: 55.47%] [G loss: 1.740430]\n",
      "epoch:24 step:23361 [D loss: 0.702569, acc: 53.91%] [G loss: 1.658069]\n",
      "epoch:24 step:23362 [D loss: 0.640344, acc: 60.94%] [G loss: 1.829874]\n",
      "epoch:24 step:23363 [D loss: 0.622521, acc: 64.06%] [G loss: 1.895999]\n",
      "epoch:24 step:23364 [D loss: 0.682004, acc: 54.69%] [G loss: 1.830716]\n",
      "epoch:24 step:23365 [D loss: 0.665217, acc: 63.28%] [G loss: 1.753460]\n",
      "epoch:24 step:23366 [D loss: 0.622708, acc: 66.41%] [G loss: 1.909180]\n",
      "epoch:24 step:23367 [D loss: 0.603214, acc: 69.53%] [G loss: 1.752368]\n",
      "epoch:24 step:23368 [D loss: 0.674312, acc: 64.06%] [G loss: 1.852563]\n",
      "epoch:24 step:23369 [D loss: 0.621200, acc: 64.06%] [G loss: 1.820131]\n",
      "epoch:24 step:23370 [D loss: 0.606637, acc: 62.50%] [G loss: 1.843742]\n",
      "epoch:24 step:23371 [D loss: 0.640436, acc: 64.06%] [G loss: 1.775932]\n",
      "epoch:24 step:23372 [D loss: 0.620588, acc: 64.06%] [G loss: 2.045573]\n",
      "epoch:24 step:23373 [D loss: 0.619685, acc: 67.97%] [G loss: 1.922217]\n",
      "epoch:24 step:23374 [D loss: 0.663618, acc: 52.34%] [G loss: 1.959799]\n",
      "epoch:24 step:23375 [D loss: 0.607945, acc: 64.84%] [G loss: 1.843565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:23376 [D loss: 0.668004, acc: 63.28%] [G loss: 1.923922]\n",
      "epoch:24 step:23377 [D loss: 0.625198, acc: 59.38%] [G loss: 1.867401]\n",
      "epoch:24 step:23378 [D loss: 0.589023, acc: 66.41%] [G loss: 2.008963]\n",
      "epoch:24 step:23379 [D loss: 0.652358, acc: 58.59%] [G loss: 1.785895]\n",
      "epoch:24 step:23380 [D loss: 0.703361, acc: 59.38%] [G loss: 1.816716]\n",
      "epoch:24 step:23381 [D loss: 0.631877, acc: 63.28%] [G loss: 1.902218]\n",
      "epoch:24 step:23382 [D loss: 0.602467, acc: 72.66%] [G loss: 1.876987]\n",
      "epoch:24 step:23383 [D loss: 0.644750, acc: 65.62%] [G loss: 1.865111]\n",
      "epoch:24 step:23384 [D loss: 0.634856, acc: 64.84%] [G loss: 1.829524]\n",
      "epoch:24 step:23385 [D loss: 0.664471, acc: 63.28%] [G loss: 1.930320]\n",
      "epoch:24 step:23386 [D loss: 0.578970, acc: 67.19%] [G loss: 1.932434]\n",
      "epoch:24 step:23387 [D loss: 0.582113, acc: 68.75%] [G loss: 2.064576]\n",
      "epoch:24 step:23388 [D loss: 0.556909, acc: 74.22%] [G loss: 2.012867]\n",
      "epoch:24 step:23389 [D loss: 0.622949, acc: 62.50%] [G loss: 2.062710]\n",
      "epoch:24 step:23390 [D loss: 0.636620, acc: 61.72%] [G loss: 1.901361]\n",
      "epoch:24 step:23391 [D loss: 0.643084, acc: 65.62%] [G loss: 2.086452]\n",
      "epoch:24 step:23392 [D loss: 0.632589, acc: 62.50%] [G loss: 1.941558]\n",
      "epoch:24 step:23393 [D loss: 0.663017, acc: 61.72%] [G loss: 2.179476]\n",
      "epoch:24 step:23394 [D loss: 0.635981, acc: 64.06%] [G loss: 2.001523]\n",
      "epoch:24 step:23395 [D loss: 0.612261, acc: 64.06%] [G loss: 1.902853]\n",
      "epoch:24 step:23396 [D loss: 0.654907, acc: 64.84%] [G loss: 1.930908]\n",
      "epoch:24 step:23397 [D loss: 0.595287, acc: 70.31%] [G loss: 2.084972]\n",
      "epoch:24 step:23398 [D loss: 0.677245, acc: 57.81%] [G loss: 1.889955]\n",
      "epoch:24 step:23399 [D loss: 0.683418, acc: 64.84%] [G loss: 2.031808]\n",
      "epoch:24 step:23400 [D loss: 0.576998, acc: 70.31%] [G loss: 2.221470]\n",
      "epoch:24 step:23401 [D loss: 0.748878, acc: 50.78%] [G loss: 1.950688]\n",
      "epoch:24 step:23402 [D loss: 0.670221, acc: 64.84%] [G loss: 1.886715]\n",
      "epoch:24 step:23403 [D loss: 0.665190, acc: 59.38%] [G loss: 1.903100]\n",
      "epoch:24 step:23404 [D loss: 0.650287, acc: 60.94%] [G loss: 1.995694]\n",
      "epoch:24 step:23405 [D loss: 0.676975, acc: 62.50%] [G loss: 1.939484]\n",
      "epoch:24 step:23406 [D loss: 0.561888, acc: 75.78%] [G loss: 2.049619]\n",
      "epoch:24 step:23407 [D loss: 0.561714, acc: 71.88%] [G loss: 2.058125]\n",
      "epoch:24 step:23408 [D loss: 0.693120, acc: 56.25%] [G loss: 1.867041]\n",
      "epoch:24 step:23409 [D loss: 0.598831, acc: 73.44%] [G loss: 1.898862]\n",
      "epoch:24 step:23410 [D loss: 0.613742, acc: 64.06%] [G loss: 1.943506]\n",
      "epoch:24 step:23411 [D loss: 0.581896, acc: 72.66%] [G loss: 2.174254]\n",
      "epoch:24 step:23412 [D loss: 0.586812, acc: 73.44%] [G loss: 2.335248]\n",
      "epoch:24 step:23413 [D loss: 0.610155, acc: 67.97%] [G loss: 1.976160]\n",
      "epoch:24 step:23414 [D loss: 0.596195, acc: 69.53%] [G loss: 2.114200]\n",
      "epoch:24 step:23415 [D loss: 0.657224, acc: 67.19%] [G loss: 2.178250]\n",
      "epoch:24 step:23416 [D loss: 0.697724, acc: 57.81%] [G loss: 1.819234]\n",
      "epoch:24 step:23417 [D loss: 0.770638, acc: 53.91%] [G loss: 1.891665]\n",
      "epoch:24 step:23418 [D loss: 0.606235, acc: 59.38%] [G loss: 2.026158]\n",
      "epoch:24 step:23419 [D loss: 0.609870, acc: 61.72%] [G loss: 2.145104]\n",
      "epoch:24 step:23420 [D loss: 0.669042, acc: 60.94%] [G loss: 1.972375]\n",
      "epoch:24 step:23421 [D loss: 0.638938, acc: 65.62%] [G loss: 1.848842]\n",
      "epoch:24 step:23422 [D loss: 0.645313, acc: 60.94%] [G loss: 2.055690]\n",
      "epoch:24 step:23423 [D loss: 0.599766, acc: 65.62%] [G loss: 1.917161]\n",
      "epoch:24 step:23424 [D loss: 0.624621, acc: 64.06%] [G loss: 2.141959]\n",
      "epoch:24 step:23425 [D loss: 0.549434, acc: 74.22%] [G loss: 2.550656]\n",
      "epoch:25 step:23426 [D loss: 0.728167, acc: 53.91%] [G loss: 1.801604]\n",
      "epoch:25 step:23427 [D loss: 0.700509, acc: 59.38%] [G loss: 1.844563]\n",
      "epoch:25 step:23428 [D loss: 0.697158, acc: 57.81%] [G loss: 1.776218]\n",
      "epoch:25 step:23429 [D loss: 0.638863, acc: 56.25%] [G loss: 1.847414]\n",
      "epoch:25 step:23430 [D loss: 0.666979, acc: 62.50%] [G loss: 1.828292]\n",
      "epoch:25 step:23431 [D loss: 0.645464, acc: 62.50%] [G loss: 1.969589]\n",
      "epoch:25 step:23432 [D loss: 0.655507, acc: 63.28%] [G loss: 1.953221]\n",
      "epoch:25 step:23433 [D loss: 0.617516, acc: 66.41%] [G loss: 2.096338]\n",
      "epoch:25 step:23434 [D loss: 0.623780, acc: 67.19%] [G loss: 1.938787]\n",
      "epoch:25 step:23435 [D loss: 0.582435, acc: 69.53%] [G loss: 2.044597]\n",
      "epoch:25 step:23436 [D loss: 0.638346, acc: 64.84%] [G loss: 2.015483]\n",
      "epoch:25 step:23437 [D loss: 0.633240, acc: 58.59%] [G loss: 2.022781]\n",
      "epoch:25 step:23438 [D loss: 0.597207, acc: 64.84%] [G loss: 1.924293]\n",
      "epoch:25 step:23439 [D loss: 0.633404, acc: 65.62%] [G loss: 1.954006]\n",
      "epoch:25 step:23440 [D loss: 0.638577, acc: 64.06%] [G loss: 2.041191]\n",
      "epoch:25 step:23441 [D loss: 0.612306, acc: 64.84%] [G loss: 2.293084]\n",
      "epoch:25 step:23442 [D loss: 0.611519, acc: 68.75%] [G loss: 1.978545]\n",
      "epoch:25 step:23443 [D loss: 0.633995, acc: 63.28%] [G loss: 2.082994]\n",
      "epoch:25 step:23444 [D loss: 0.648952, acc: 62.50%] [G loss: 1.904783]\n",
      "epoch:25 step:23445 [D loss: 0.751394, acc: 50.78%] [G loss: 1.681707]\n",
      "epoch:25 step:23446 [D loss: 0.655567, acc: 60.16%] [G loss: 1.853686]\n",
      "epoch:25 step:23447 [D loss: 0.680886, acc: 57.81%] [G loss: 1.875821]\n",
      "epoch:25 step:23448 [D loss: 0.587921, acc: 73.44%] [G loss: 1.993587]\n",
      "epoch:25 step:23449 [D loss: 0.595732, acc: 67.97%] [G loss: 1.999910]\n",
      "epoch:25 step:23450 [D loss: 0.664992, acc: 64.84%] [G loss: 2.079465]\n",
      "epoch:25 step:23451 [D loss: 0.694460, acc: 53.12%] [G loss: 1.778051]\n",
      "epoch:25 step:23452 [D loss: 0.707307, acc: 54.69%] [G loss: 1.798921]\n",
      "epoch:25 step:23453 [D loss: 0.642249, acc: 58.59%] [G loss: 1.763863]\n",
      "epoch:25 step:23454 [D loss: 0.602431, acc: 70.31%] [G loss: 2.009849]\n",
      "epoch:25 step:23455 [D loss: 0.650877, acc: 64.06%] [G loss: 1.789966]\n",
      "epoch:25 step:23456 [D loss: 0.669640, acc: 61.72%] [G loss: 1.822848]\n",
      "epoch:25 step:23457 [D loss: 0.671849, acc: 60.94%] [G loss: 1.860689]\n",
      "epoch:25 step:23458 [D loss: 0.638219, acc: 60.94%] [G loss: 1.765391]\n",
      "epoch:25 step:23459 [D loss: 0.638772, acc: 63.28%] [G loss: 1.894562]\n",
      "epoch:25 step:23460 [D loss: 0.661761, acc: 63.28%] [G loss: 1.858126]\n",
      "epoch:25 step:23461 [D loss: 0.636914, acc: 62.50%] [G loss: 1.983461]\n",
      "epoch:25 step:23462 [D loss: 0.665812, acc: 59.38%] [G loss: 1.913428]\n",
      "epoch:25 step:23463 [D loss: 0.616045, acc: 67.97%] [G loss: 2.030401]\n",
      "epoch:25 step:23464 [D loss: 0.670179, acc: 57.03%] [G loss: 1.815101]\n",
      "epoch:25 step:23465 [D loss: 0.637596, acc: 64.06%] [G loss: 2.039580]\n",
      "epoch:25 step:23466 [D loss: 0.653493, acc: 60.94%] [G loss: 1.801091]\n",
      "epoch:25 step:23467 [D loss: 0.634194, acc: 68.75%] [G loss: 1.838539]\n",
      "epoch:25 step:23468 [D loss: 0.611972, acc: 67.19%] [G loss: 1.939705]\n",
      "epoch:25 step:23469 [D loss: 0.645960, acc: 67.19%] [G loss: 1.847495]\n",
      "epoch:25 step:23470 [D loss: 0.713738, acc: 52.34%] [G loss: 1.922687]\n",
      "epoch:25 step:23471 [D loss: 0.658581, acc: 60.94%] [G loss: 1.926613]\n",
      "epoch:25 step:23472 [D loss: 0.646952, acc: 64.84%] [G loss: 1.869006]\n",
      "epoch:25 step:23473 [D loss: 0.630459, acc: 62.50%] [G loss: 1.970388]\n",
      "epoch:25 step:23474 [D loss: 0.654927, acc: 63.28%] [G loss: 1.896042]\n",
      "epoch:25 step:23475 [D loss: 0.600882, acc: 63.28%] [G loss: 1.809892]\n",
      "epoch:25 step:23476 [D loss: 0.703031, acc: 55.47%] [G loss: 1.776118]\n",
      "epoch:25 step:23477 [D loss: 0.633231, acc: 64.06%] [G loss: 1.884800]\n",
      "epoch:25 step:23478 [D loss: 0.633558, acc: 60.16%] [G loss: 1.899865]\n",
      "epoch:25 step:23479 [D loss: 0.677561, acc: 58.59%] [G loss: 1.930388]\n",
      "epoch:25 step:23480 [D loss: 0.635005, acc: 64.84%] [G loss: 2.039232]\n",
      "epoch:25 step:23481 [D loss: 0.608671, acc: 65.62%] [G loss: 1.946277]\n",
      "epoch:25 step:23482 [D loss: 0.701006, acc: 55.47%] [G loss: 1.924990]\n",
      "epoch:25 step:23483 [D loss: 0.624240, acc: 64.06%] [G loss: 1.874608]\n",
      "epoch:25 step:23484 [D loss: 0.626002, acc: 64.84%] [G loss: 1.845005]\n",
      "epoch:25 step:23485 [D loss: 0.668343, acc: 57.81%] [G loss: 1.809397]\n",
      "epoch:25 step:23486 [D loss: 0.629234, acc: 62.50%] [G loss: 1.789077]\n",
      "epoch:25 step:23487 [D loss: 0.669511, acc: 59.38%] [G loss: 1.829794]\n",
      "epoch:25 step:23488 [D loss: 0.640899, acc: 62.50%] [G loss: 1.870024]\n",
      "epoch:25 step:23489 [D loss: 0.656416, acc: 57.81%] [G loss: 1.952770]\n",
      "epoch:25 step:23490 [D loss: 0.661530, acc: 60.94%] [G loss: 1.818081]\n",
      "epoch:25 step:23491 [D loss: 0.662986, acc: 59.38%] [G loss: 1.788848]\n",
      "epoch:25 step:23492 [D loss: 0.687116, acc: 56.25%] [G loss: 1.882473]\n",
      "epoch:25 step:23493 [D loss: 0.609477, acc: 64.06%] [G loss: 1.828875]\n",
      "epoch:25 step:23494 [D loss: 0.693103, acc: 55.47%] [G loss: 2.049112]\n",
      "epoch:25 step:23495 [D loss: 0.686449, acc: 58.59%] [G loss: 1.892955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23496 [D loss: 0.661412, acc: 59.38%] [G loss: 1.723951]\n",
      "epoch:25 step:23497 [D loss: 0.667074, acc: 57.03%] [G loss: 1.839326]\n",
      "epoch:25 step:23498 [D loss: 0.633574, acc: 62.50%] [G loss: 1.668019]\n",
      "epoch:25 step:23499 [D loss: 0.632352, acc: 65.62%] [G loss: 1.853505]\n",
      "epoch:25 step:23500 [D loss: 0.619926, acc: 60.16%] [G loss: 1.978383]\n",
      "epoch:25 step:23501 [D loss: 0.620691, acc: 67.19%] [G loss: 1.928370]\n",
      "epoch:25 step:23502 [D loss: 0.608059, acc: 64.84%] [G loss: 2.093628]\n",
      "epoch:25 step:23503 [D loss: 0.674284, acc: 60.94%] [G loss: 1.768776]\n",
      "epoch:25 step:23504 [D loss: 0.640558, acc: 61.72%] [G loss: 1.780397]\n",
      "epoch:25 step:23505 [D loss: 0.656498, acc: 60.94%] [G loss: 1.778911]\n",
      "epoch:25 step:23506 [D loss: 0.655146, acc: 59.38%] [G loss: 1.881891]\n",
      "epoch:25 step:23507 [D loss: 0.643095, acc: 60.16%] [G loss: 1.685600]\n",
      "epoch:25 step:23508 [D loss: 0.661054, acc: 55.47%] [G loss: 1.826263]\n",
      "epoch:25 step:23509 [D loss: 0.651246, acc: 62.50%] [G loss: 1.814819]\n",
      "epoch:25 step:23510 [D loss: 0.663783, acc: 59.38%] [G loss: 1.805435]\n",
      "epoch:25 step:23511 [D loss: 0.639465, acc: 66.41%] [G loss: 1.834496]\n",
      "epoch:25 step:23512 [D loss: 0.672032, acc: 56.25%] [G loss: 1.778303]\n",
      "epoch:25 step:23513 [D loss: 0.605858, acc: 70.31%] [G loss: 1.845697]\n",
      "epoch:25 step:23514 [D loss: 0.622798, acc: 67.19%] [G loss: 1.938366]\n",
      "epoch:25 step:23515 [D loss: 0.658055, acc: 62.50%] [G loss: 1.834946]\n",
      "epoch:25 step:23516 [D loss: 0.713384, acc: 52.34%] [G loss: 1.726943]\n",
      "epoch:25 step:23517 [D loss: 0.625617, acc: 59.38%] [G loss: 1.909590]\n",
      "epoch:25 step:23518 [D loss: 0.650649, acc: 60.16%] [G loss: 1.877849]\n",
      "epoch:25 step:23519 [D loss: 0.628857, acc: 64.84%] [G loss: 1.868255]\n",
      "epoch:25 step:23520 [D loss: 0.637857, acc: 64.06%] [G loss: 1.769288]\n",
      "epoch:25 step:23521 [D loss: 0.703903, acc: 60.94%] [G loss: 1.826518]\n",
      "epoch:25 step:23522 [D loss: 0.634618, acc: 64.84%] [G loss: 1.832518]\n",
      "epoch:25 step:23523 [D loss: 0.692120, acc: 56.25%] [G loss: 1.781343]\n",
      "epoch:25 step:23524 [D loss: 0.638367, acc: 65.62%] [G loss: 1.734533]\n",
      "epoch:25 step:23525 [D loss: 0.641464, acc: 66.41%] [G loss: 1.686948]\n",
      "epoch:25 step:23526 [D loss: 0.659629, acc: 59.38%] [G loss: 1.842835]\n",
      "epoch:25 step:23527 [D loss: 0.676028, acc: 60.16%] [G loss: 1.701635]\n",
      "epoch:25 step:23528 [D loss: 0.658568, acc: 57.81%] [G loss: 1.647002]\n",
      "epoch:25 step:23529 [D loss: 0.655300, acc: 58.59%] [G loss: 1.780506]\n",
      "epoch:25 step:23530 [D loss: 0.658900, acc: 63.28%] [G loss: 1.909322]\n",
      "epoch:25 step:23531 [D loss: 0.621450, acc: 65.62%] [G loss: 2.000630]\n",
      "epoch:25 step:23532 [D loss: 0.617698, acc: 62.50%] [G loss: 2.151690]\n",
      "epoch:25 step:23533 [D loss: 0.743637, acc: 48.44%] [G loss: 1.668205]\n",
      "epoch:25 step:23534 [D loss: 0.729708, acc: 53.12%] [G loss: 1.677301]\n",
      "epoch:25 step:23535 [D loss: 0.666205, acc: 63.28%] [G loss: 1.669144]\n",
      "epoch:25 step:23536 [D loss: 0.622394, acc: 62.50%] [G loss: 1.852228]\n",
      "epoch:25 step:23537 [D loss: 0.608062, acc: 67.97%] [G loss: 1.974620]\n",
      "epoch:25 step:23538 [D loss: 0.624127, acc: 70.31%] [G loss: 2.033935]\n",
      "epoch:25 step:23539 [D loss: 0.651623, acc: 60.16%] [G loss: 1.925820]\n",
      "epoch:25 step:23540 [D loss: 0.632783, acc: 60.16%] [G loss: 2.160323]\n",
      "epoch:25 step:23541 [D loss: 0.582187, acc: 72.66%] [G loss: 2.104459]\n",
      "epoch:25 step:23542 [D loss: 0.615848, acc: 64.06%] [G loss: 2.054925]\n",
      "epoch:25 step:23543 [D loss: 0.697494, acc: 50.78%] [G loss: 1.941121]\n",
      "epoch:25 step:23544 [D loss: 0.604754, acc: 69.53%] [G loss: 2.091254]\n",
      "epoch:25 step:23545 [D loss: 0.698474, acc: 56.25%] [G loss: 1.890108]\n",
      "epoch:25 step:23546 [D loss: 0.598129, acc: 66.41%] [G loss: 2.011665]\n",
      "epoch:25 step:23547 [D loss: 0.598565, acc: 68.75%] [G loss: 2.267133]\n",
      "epoch:25 step:23548 [D loss: 0.662890, acc: 59.38%] [G loss: 1.991919]\n",
      "epoch:25 step:23549 [D loss: 0.654300, acc: 58.59%] [G loss: 1.888608]\n",
      "epoch:25 step:23550 [D loss: 0.716562, acc: 53.12%] [G loss: 1.687947]\n",
      "epoch:25 step:23551 [D loss: 0.656196, acc: 60.16%] [G loss: 2.009664]\n",
      "epoch:25 step:23552 [D loss: 0.662653, acc: 57.03%] [G loss: 1.852172]\n",
      "epoch:25 step:23553 [D loss: 0.631021, acc: 69.53%] [G loss: 1.750901]\n",
      "epoch:25 step:23554 [D loss: 0.684049, acc: 56.25%] [G loss: 1.824353]\n",
      "epoch:25 step:23555 [D loss: 0.627081, acc: 64.06%] [G loss: 1.798290]\n",
      "epoch:25 step:23556 [D loss: 0.613502, acc: 65.62%] [G loss: 1.838875]\n",
      "epoch:25 step:23557 [D loss: 0.666282, acc: 55.47%] [G loss: 1.864399]\n",
      "epoch:25 step:23558 [D loss: 0.715317, acc: 52.34%] [G loss: 1.821117]\n",
      "epoch:25 step:23559 [D loss: 0.652788, acc: 58.59%] [G loss: 1.777633]\n",
      "epoch:25 step:23560 [D loss: 0.647723, acc: 58.59%] [G loss: 1.740786]\n",
      "epoch:25 step:23561 [D loss: 0.669015, acc: 60.94%] [G loss: 1.656077]\n",
      "epoch:25 step:23562 [D loss: 0.671356, acc: 60.16%] [G loss: 1.742478]\n",
      "epoch:25 step:23563 [D loss: 0.621371, acc: 68.75%] [G loss: 1.825340]\n",
      "epoch:25 step:23564 [D loss: 0.643529, acc: 60.94%] [G loss: 1.848409]\n",
      "epoch:25 step:23565 [D loss: 0.637724, acc: 64.84%] [G loss: 1.839484]\n",
      "epoch:25 step:23566 [D loss: 0.659274, acc: 63.28%] [G loss: 1.822743]\n",
      "epoch:25 step:23567 [D loss: 0.666828, acc: 64.84%] [G loss: 1.901591]\n",
      "epoch:25 step:23568 [D loss: 0.655830, acc: 63.28%] [G loss: 1.789632]\n",
      "epoch:25 step:23569 [D loss: 0.685529, acc: 54.69%] [G loss: 1.833174]\n",
      "epoch:25 step:23570 [D loss: 0.612328, acc: 64.06%] [G loss: 1.929918]\n",
      "epoch:25 step:23571 [D loss: 0.627076, acc: 60.94%] [G loss: 2.056996]\n",
      "epoch:25 step:23572 [D loss: 0.669032, acc: 55.47%] [G loss: 1.875802]\n",
      "epoch:25 step:23573 [D loss: 0.677002, acc: 59.38%] [G loss: 1.827894]\n",
      "epoch:25 step:23574 [D loss: 0.675184, acc: 60.16%] [G loss: 1.795364]\n",
      "epoch:25 step:23575 [D loss: 0.652683, acc: 63.28%] [G loss: 1.877210]\n",
      "epoch:25 step:23576 [D loss: 0.653643, acc: 62.50%] [G loss: 1.985245]\n",
      "epoch:25 step:23577 [D loss: 0.664674, acc: 54.69%] [G loss: 1.839482]\n",
      "epoch:25 step:23578 [D loss: 0.676885, acc: 55.47%] [G loss: 1.737807]\n",
      "epoch:25 step:23579 [D loss: 0.621362, acc: 63.28%] [G loss: 1.974645]\n",
      "epoch:25 step:23580 [D loss: 0.683766, acc: 54.69%] [G loss: 1.723613]\n",
      "epoch:25 step:23581 [D loss: 0.660182, acc: 62.50%] [G loss: 1.887420]\n",
      "epoch:25 step:23582 [D loss: 0.662366, acc: 57.81%] [G loss: 1.809295]\n",
      "epoch:25 step:23583 [D loss: 0.620741, acc: 60.94%] [G loss: 1.737018]\n",
      "epoch:25 step:23584 [D loss: 0.616292, acc: 69.53%] [G loss: 1.902522]\n",
      "epoch:25 step:23585 [D loss: 0.684231, acc: 60.16%] [G loss: 1.797525]\n",
      "epoch:25 step:23586 [D loss: 0.651569, acc: 64.84%] [G loss: 1.850656]\n",
      "epoch:25 step:23587 [D loss: 0.655841, acc: 59.38%] [G loss: 1.805271]\n",
      "epoch:25 step:23588 [D loss: 0.664985, acc: 57.81%] [G loss: 1.826209]\n",
      "epoch:25 step:23589 [D loss: 0.688363, acc: 60.16%] [G loss: 1.757552]\n",
      "epoch:25 step:23590 [D loss: 0.620524, acc: 65.62%] [G loss: 1.857074]\n",
      "epoch:25 step:23591 [D loss: 0.610421, acc: 65.62%] [G loss: 1.856108]\n",
      "epoch:25 step:23592 [D loss: 0.654421, acc: 63.28%] [G loss: 1.934326]\n",
      "epoch:25 step:23593 [D loss: 0.647676, acc: 65.62%] [G loss: 1.980569]\n",
      "epoch:25 step:23594 [D loss: 0.656529, acc: 65.62%] [G loss: 1.820901]\n",
      "epoch:25 step:23595 [D loss: 0.636188, acc: 64.06%] [G loss: 1.818035]\n",
      "epoch:25 step:23596 [D loss: 0.621192, acc: 64.84%] [G loss: 1.957394]\n",
      "epoch:25 step:23597 [D loss: 0.638482, acc: 60.94%] [G loss: 1.781792]\n",
      "epoch:25 step:23598 [D loss: 0.659831, acc: 60.94%] [G loss: 1.821728]\n",
      "epoch:25 step:23599 [D loss: 0.677636, acc: 59.38%] [G loss: 1.693676]\n",
      "epoch:25 step:23600 [D loss: 0.680662, acc: 61.72%] [G loss: 1.760562]\n",
      "epoch:25 step:23601 [D loss: 0.709794, acc: 51.56%] [G loss: 1.787887]\n",
      "epoch:25 step:23602 [D loss: 0.660546, acc: 60.94%] [G loss: 1.788263]\n",
      "epoch:25 step:23603 [D loss: 0.592678, acc: 64.84%] [G loss: 1.753946]\n",
      "epoch:25 step:23604 [D loss: 0.669961, acc: 57.03%] [G loss: 1.792911]\n",
      "epoch:25 step:23605 [D loss: 0.634116, acc: 60.16%] [G loss: 1.911606]\n",
      "epoch:25 step:23606 [D loss: 0.675469, acc: 54.69%] [G loss: 1.811009]\n",
      "epoch:25 step:23607 [D loss: 0.675478, acc: 58.59%] [G loss: 1.724642]\n",
      "epoch:25 step:23608 [D loss: 0.655229, acc: 64.84%] [G loss: 1.703329]\n",
      "epoch:25 step:23609 [D loss: 0.617055, acc: 64.06%] [G loss: 1.811146]\n",
      "epoch:25 step:23610 [D loss: 0.653305, acc: 60.16%] [G loss: 1.769443]\n",
      "epoch:25 step:23611 [D loss: 0.690981, acc: 57.03%] [G loss: 1.745284]\n",
      "epoch:25 step:23612 [D loss: 0.686426, acc: 55.47%] [G loss: 1.869362]\n",
      "epoch:25 step:23613 [D loss: 0.614232, acc: 62.50%] [G loss: 1.892974]\n",
      "epoch:25 step:23614 [D loss: 0.669232, acc: 60.16%] [G loss: 1.751423]\n",
      "epoch:25 step:23615 [D loss: 0.609937, acc: 67.97%] [G loss: 1.870532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23616 [D loss: 0.619125, acc: 65.62%] [G loss: 1.855878]\n",
      "epoch:25 step:23617 [D loss: 0.578807, acc: 67.97%] [G loss: 2.027914]\n",
      "epoch:25 step:23618 [D loss: 0.642562, acc: 60.94%] [G loss: 2.037576]\n",
      "epoch:25 step:23619 [D loss: 0.649960, acc: 67.19%] [G loss: 2.119464]\n",
      "epoch:25 step:23620 [D loss: 0.714797, acc: 60.16%] [G loss: 1.887327]\n",
      "epoch:25 step:23621 [D loss: 0.648922, acc: 62.50%] [G loss: 1.935201]\n",
      "epoch:25 step:23622 [D loss: 0.659890, acc: 60.94%] [G loss: 1.890055]\n",
      "epoch:25 step:23623 [D loss: 0.600036, acc: 71.09%] [G loss: 1.951494]\n",
      "epoch:25 step:23624 [D loss: 0.643025, acc: 59.38%] [G loss: 2.009099]\n",
      "epoch:25 step:23625 [D loss: 0.688450, acc: 60.16%] [G loss: 1.901456]\n",
      "epoch:25 step:23626 [D loss: 0.642931, acc: 64.06%] [G loss: 1.989486]\n",
      "epoch:25 step:23627 [D loss: 0.699816, acc: 56.25%] [G loss: 1.842769]\n",
      "epoch:25 step:23628 [D loss: 0.630618, acc: 65.62%] [G loss: 1.836495]\n",
      "epoch:25 step:23629 [D loss: 0.664637, acc: 60.16%] [G loss: 1.865582]\n",
      "epoch:25 step:23630 [D loss: 0.654420, acc: 60.16%] [G loss: 1.782995]\n",
      "epoch:25 step:23631 [D loss: 0.620160, acc: 67.19%] [G loss: 2.015010]\n",
      "epoch:25 step:23632 [D loss: 0.624525, acc: 64.84%] [G loss: 2.025922]\n",
      "epoch:25 step:23633 [D loss: 0.585298, acc: 71.09%] [G loss: 2.022801]\n",
      "epoch:25 step:23634 [D loss: 0.562642, acc: 64.84%] [G loss: 2.257027]\n",
      "epoch:25 step:23635 [D loss: 0.675290, acc: 59.38%] [G loss: 1.842783]\n",
      "epoch:25 step:23636 [D loss: 0.652816, acc: 58.59%] [G loss: 1.782689]\n",
      "epoch:25 step:23637 [D loss: 0.640153, acc: 68.75%] [G loss: 1.806487]\n",
      "epoch:25 step:23638 [D loss: 0.665735, acc: 60.16%] [G loss: 1.705681]\n",
      "epoch:25 step:23639 [D loss: 0.635477, acc: 61.72%] [G loss: 1.900697]\n",
      "epoch:25 step:23640 [D loss: 0.663701, acc: 59.38%] [G loss: 1.809388]\n",
      "epoch:25 step:23641 [D loss: 0.613827, acc: 67.19%] [G loss: 1.956463]\n",
      "epoch:25 step:23642 [D loss: 0.594033, acc: 70.31%] [G loss: 2.023572]\n",
      "epoch:25 step:23643 [D loss: 0.608758, acc: 65.62%] [G loss: 2.047759]\n",
      "epoch:25 step:23644 [D loss: 0.625938, acc: 67.19%] [G loss: 2.106051]\n",
      "epoch:25 step:23645 [D loss: 0.705679, acc: 47.66%] [G loss: 1.870012]\n",
      "epoch:25 step:23646 [D loss: 0.620431, acc: 64.84%] [G loss: 1.784160]\n",
      "epoch:25 step:23647 [D loss: 0.733238, acc: 54.69%] [G loss: 2.027501]\n",
      "epoch:25 step:23648 [D loss: 0.618796, acc: 67.19%] [G loss: 1.841946]\n",
      "epoch:25 step:23649 [D loss: 0.649043, acc: 57.81%] [G loss: 1.894794]\n",
      "epoch:25 step:23650 [D loss: 0.634328, acc: 62.50%] [G loss: 1.839118]\n",
      "epoch:25 step:23651 [D loss: 0.655735, acc: 59.38%] [G loss: 1.918184]\n",
      "epoch:25 step:23652 [D loss: 0.668452, acc: 60.16%] [G loss: 1.827416]\n",
      "epoch:25 step:23653 [D loss: 0.643839, acc: 67.19%] [G loss: 1.964564]\n",
      "epoch:25 step:23654 [D loss: 0.621892, acc: 69.53%] [G loss: 1.899731]\n",
      "epoch:25 step:23655 [D loss: 0.606547, acc: 67.97%] [G loss: 2.032785]\n",
      "epoch:25 step:23656 [D loss: 0.545207, acc: 74.22%] [G loss: 2.244462]\n",
      "epoch:25 step:23657 [D loss: 0.582913, acc: 72.66%] [G loss: 2.440015]\n",
      "epoch:25 step:23658 [D loss: 0.625325, acc: 69.53%] [G loss: 1.964229]\n",
      "epoch:25 step:23659 [D loss: 0.713337, acc: 54.69%] [G loss: 1.806512]\n",
      "epoch:25 step:23660 [D loss: 0.694817, acc: 58.59%] [G loss: 1.964692]\n",
      "epoch:25 step:23661 [D loss: 0.627209, acc: 61.72%] [G loss: 1.839638]\n",
      "epoch:25 step:23662 [D loss: 0.662149, acc: 59.38%] [G loss: 1.949147]\n",
      "epoch:25 step:23663 [D loss: 0.685372, acc: 58.59%] [G loss: 2.033086]\n",
      "epoch:25 step:23664 [D loss: 0.576432, acc: 70.31%] [G loss: 2.004745]\n",
      "epoch:25 step:23665 [D loss: 0.623261, acc: 64.84%] [G loss: 1.858764]\n",
      "epoch:25 step:23666 [D loss: 0.671047, acc: 57.03%] [G loss: 1.763201]\n",
      "epoch:25 step:23667 [D loss: 0.642807, acc: 63.28%] [G loss: 2.079952]\n",
      "epoch:25 step:23668 [D loss: 0.655954, acc: 59.38%] [G loss: 1.955651]\n",
      "epoch:25 step:23669 [D loss: 0.631836, acc: 60.16%] [G loss: 1.910758]\n",
      "epoch:25 step:23670 [D loss: 0.632212, acc: 60.94%] [G loss: 1.809148]\n",
      "epoch:25 step:23671 [D loss: 0.660185, acc: 60.94%] [G loss: 1.937505]\n",
      "epoch:25 step:23672 [D loss: 0.723706, acc: 50.78%] [G loss: 1.922709]\n",
      "epoch:25 step:23673 [D loss: 0.628665, acc: 61.72%] [G loss: 2.054360]\n",
      "epoch:25 step:23674 [D loss: 0.755387, acc: 47.66%] [G loss: 1.726273]\n",
      "epoch:25 step:23675 [D loss: 0.711271, acc: 49.22%] [G loss: 1.748853]\n",
      "epoch:25 step:23676 [D loss: 0.667886, acc: 57.81%] [G loss: 1.742659]\n",
      "epoch:25 step:23677 [D loss: 0.664376, acc: 64.06%] [G loss: 1.812403]\n",
      "epoch:25 step:23678 [D loss: 0.668221, acc: 60.94%] [G loss: 1.803159]\n",
      "epoch:25 step:23679 [D loss: 0.644507, acc: 59.38%] [G loss: 1.799382]\n",
      "epoch:25 step:23680 [D loss: 0.690967, acc: 56.25%] [G loss: 1.807588]\n",
      "epoch:25 step:23681 [D loss: 0.654580, acc: 61.72%] [G loss: 1.748309]\n",
      "epoch:25 step:23682 [D loss: 0.635653, acc: 63.28%] [G loss: 1.700649]\n",
      "epoch:25 step:23683 [D loss: 0.677154, acc: 59.38%] [G loss: 1.740487]\n",
      "epoch:25 step:23684 [D loss: 0.667360, acc: 63.28%] [G loss: 1.734079]\n",
      "epoch:25 step:23685 [D loss: 0.624852, acc: 65.62%] [G loss: 1.932743]\n",
      "epoch:25 step:23686 [D loss: 0.619217, acc: 67.19%] [G loss: 1.911199]\n",
      "epoch:25 step:23687 [D loss: 0.596127, acc: 71.09%] [G loss: 1.859495]\n",
      "epoch:25 step:23688 [D loss: 0.680885, acc: 55.47%] [G loss: 1.903180]\n",
      "epoch:25 step:23689 [D loss: 0.672467, acc: 66.41%] [G loss: 1.898427]\n",
      "epoch:25 step:23690 [D loss: 0.650502, acc: 60.16%] [G loss: 1.743851]\n",
      "epoch:25 step:23691 [D loss: 0.639914, acc: 61.72%] [G loss: 1.747716]\n",
      "epoch:25 step:23692 [D loss: 0.663127, acc: 63.28%] [G loss: 1.845688]\n",
      "epoch:25 step:23693 [D loss: 0.669981, acc: 60.16%] [G loss: 1.860397]\n",
      "epoch:25 step:23694 [D loss: 0.641963, acc: 65.62%] [G loss: 1.972191]\n",
      "epoch:25 step:23695 [D loss: 0.662046, acc: 61.72%] [G loss: 1.824120]\n",
      "epoch:25 step:23696 [D loss: 0.638574, acc: 65.62%] [G loss: 1.950388]\n",
      "epoch:25 step:23697 [D loss: 0.609220, acc: 66.41%] [G loss: 1.824673]\n",
      "epoch:25 step:23698 [D loss: 0.646935, acc: 66.41%] [G loss: 1.973982]\n",
      "epoch:25 step:23699 [D loss: 0.632677, acc: 71.09%] [G loss: 2.095026]\n",
      "epoch:25 step:23700 [D loss: 0.641391, acc: 61.72%] [G loss: 2.060148]\n",
      "epoch:25 step:23701 [D loss: 0.551908, acc: 73.44%] [G loss: 2.314404]\n",
      "epoch:25 step:23702 [D loss: 0.648565, acc: 57.81%] [G loss: 1.778075]\n",
      "epoch:25 step:23703 [D loss: 0.679492, acc: 62.50%] [G loss: 1.782362]\n",
      "epoch:25 step:23704 [D loss: 0.645687, acc: 60.94%] [G loss: 1.934966]\n",
      "epoch:25 step:23705 [D loss: 0.642666, acc: 63.28%] [G loss: 1.781882]\n",
      "epoch:25 step:23706 [D loss: 0.662107, acc: 57.81%] [G loss: 1.857796]\n",
      "epoch:25 step:23707 [D loss: 0.693515, acc: 62.50%] [G loss: 1.787405]\n",
      "epoch:25 step:23708 [D loss: 0.694235, acc: 51.56%] [G loss: 1.967017]\n",
      "epoch:25 step:23709 [D loss: 0.630194, acc: 66.41%] [G loss: 1.735135]\n",
      "epoch:25 step:23710 [D loss: 0.638920, acc: 64.84%] [G loss: 1.700287]\n",
      "epoch:25 step:23711 [D loss: 0.665573, acc: 60.16%] [G loss: 1.960928]\n",
      "epoch:25 step:23712 [D loss: 0.655423, acc: 57.81%] [G loss: 1.832188]\n",
      "epoch:25 step:23713 [D loss: 0.673944, acc: 58.59%] [G loss: 1.725030]\n",
      "epoch:25 step:23714 [D loss: 0.628666, acc: 65.62%] [G loss: 1.857407]\n",
      "epoch:25 step:23715 [D loss: 0.689194, acc: 59.38%] [G loss: 1.829169]\n",
      "epoch:25 step:23716 [D loss: 0.690855, acc: 54.69%] [G loss: 1.824911]\n",
      "epoch:25 step:23717 [D loss: 0.641024, acc: 64.06%] [G loss: 1.848619]\n",
      "epoch:25 step:23718 [D loss: 0.637012, acc: 65.62%] [G loss: 1.937866]\n",
      "epoch:25 step:23719 [D loss: 0.628168, acc: 60.94%] [G loss: 1.810395]\n",
      "epoch:25 step:23720 [D loss: 0.638580, acc: 59.38%] [G loss: 1.836711]\n",
      "epoch:25 step:23721 [D loss: 0.656110, acc: 58.59%] [G loss: 1.876155]\n",
      "epoch:25 step:23722 [D loss: 0.619187, acc: 64.06%] [G loss: 1.969014]\n",
      "epoch:25 step:23723 [D loss: 0.608096, acc: 67.19%] [G loss: 2.076563]\n",
      "epoch:25 step:23724 [D loss: 0.632184, acc: 62.50%] [G loss: 1.986400]\n",
      "epoch:25 step:23725 [D loss: 0.641712, acc: 60.16%] [G loss: 1.764599]\n",
      "epoch:25 step:23726 [D loss: 0.688114, acc: 52.34%] [G loss: 1.751718]\n",
      "epoch:25 step:23727 [D loss: 0.638485, acc: 57.03%] [G loss: 1.940151]\n",
      "epoch:25 step:23728 [D loss: 0.655810, acc: 53.12%] [G loss: 1.784209]\n",
      "epoch:25 step:23729 [D loss: 0.664524, acc: 59.38%] [G loss: 1.857568]\n",
      "epoch:25 step:23730 [D loss: 0.675695, acc: 53.91%] [G loss: 1.753205]\n",
      "epoch:25 step:23731 [D loss: 0.641376, acc: 59.38%] [G loss: 1.853435]\n",
      "epoch:25 step:23732 [D loss: 0.649625, acc: 63.28%] [G loss: 1.834428]\n",
      "epoch:25 step:23733 [D loss: 0.722258, acc: 52.34%] [G loss: 1.813970]\n",
      "epoch:25 step:23734 [D loss: 0.707475, acc: 48.44%] [G loss: 1.836360]\n",
      "epoch:25 step:23735 [D loss: 0.612370, acc: 71.88%] [G loss: 1.831254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23736 [D loss: 0.672324, acc: 56.25%] [G loss: 1.910381]\n",
      "epoch:25 step:23737 [D loss: 0.624560, acc: 67.97%] [G loss: 1.964062]\n",
      "epoch:25 step:23738 [D loss: 0.603844, acc: 66.41%] [G loss: 2.051455]\n",
      "epoch:25 step:23739 [D loss: 0.641810, acc: 60.16%] [G loss: 1.958939]\n",
      "epoch:25 step:23740 [D loss: 0.631140, acc: 65.62%] [G loss: 1.997706]\n",
      "epoch:25 step:23741 [D loss: 0.678618, acc: 53.91%] [G loss: 1.764571]\n",
      "epoch:25 step:23742 [D loss: 0.685679, acc: 60.16%] [G loss: 1.788155]\n",
      "epoch:25 step:23743 [D loss: 0.673875, acc: 60.16%] [G loss: 1.870394]\n",
      "epoch:25 step:23744 [D loss: 0.633209, acc: 64.84%] [G loss: 1.856628]\n",
      "epoch:25 step:23745 [D loss: 0.640536, acc: 64.84%] [G loss: 1.847990]\n",
      "epoch:25 step:23746 [D loss: 0.663364, acc: 64.06%] [G loss: 1.953432]\n",
      "epoch:25 step:23747 [D loss: 0.672207, acc: 56.25%] [G loss: 1.766129]\n",
      "epoch:25 step:23748 [D loss: 0.667661, acc: 59.38%] [G loss: 1.737886]\n",
      "epoch:25 step:23749 [D loss: 0.671160, acc: 58.59%] [G loss: 1.796542]\n",
      "epoch:25 step:23750 [D loss: 0.617263, acc: 62.50%] [G loss: 1.829719]\n",
      "epoch:25 step:23751 [D loss: 0.687911, acc: 47.66%] [G loss: 1.819071]\n",
      "epoch:25 step:23752 [D loss: 0.644957, acc: 62.50%] [G loss: 1.798658]\n",
      "epoch:25 step:23753 [D loss: 0.683474, acc: 54.69%] [G loss: 1.775954]\n",
      "epoch:25 step:23754 [D loss: 0.596008, acc: 70.31%] [G loss: 1.974368]\n",
      "epoch:25 step:23755 [D loss: 0.624990, acc: 67.19%] [G loss: 1.958310]\n",
      "epoch:25 step:23756 [D loss: 0.648208, acc: 61.72%] [G loss: 2.018316]\n",
      "epoch:25 step:23757 [D loss: 0.607503, acc: 67.97%] [G loss: 1.898189]\n",
      "epoch:25 step:23758 [D loss: 0.554333, acc: 78.12%] [G loss: 1.942903]\n",
      "epoch:25 step:23759 [D loss: 0.720535, acc: 55.47%] [G loss: 1.986943]\n",
      "epoch:25 step:23760 [D loss: 0.635636, acc: 57.03%] [G loss: 1.912227]\n",
      "epoch:25 step:23761 [D loss: 0.609397, acc: 62.50%] [G loss: 1.908548]\n",
      "epoch:25 step:23762 [D loss: 0.586366, acc: 70.31%] [G loss: 1.945716]\n",
      "epoch:25 step:23763 [D loss: 0.631833, acc: 66.41%] [G loss: 1.961719]\n",
      "epoch:25 step:23764 [D loss: 0.641690, acc: 66.41%] [G loss: 1.967112]\n",
      "epoch:25 step:23765 [D loss: 0.653709, acc: 63.28%] [G loss: 1.942107]\n",
      "epoch:25 step:23766 [D loss: 0.681494, acc: 53.91%] [G loss: 1.789412]\n",
      "epoch:25 step:23767 [D loss: 0.705235, acc: 50.00%] [G loss: 1.760727]\n",
      "epoch:25 step:23768 [D loss: 0.681175, acc: 61.72%] [G loss: 1.879536]\n",
      "epoch:25 step:23769 [D loss: 0.668824, acc: 64.84%] [G loss: 1.842234]\n",
      "epoch:25 step:23770 [D loss: 0.604147, acc: 71.09%] [G loss: 1.975130]\n",
      "epoch:25 step:23771 [D loss: 0.612879, acc: 65.62%] [G loss: 2.029125]\n",
      "epoch:25 step:23772 [D loss: 0.586397, acc: 72.66%] [G loss: 1.952708]\n",
      "epoch:25 step:23773 [D loss: 0.676167, acc: 57.03%] [G loss: 1.786400]\n",
      "epoch:25 step:23774 [D loss: 0.726431, acc: 47.66%] [G loss: 1.696993]\n",
      "epoch:25 step:23775 [D loss: 0.612483, acc: 64.84%] [G loss: 1.890588]\n",
      "epoch:25 step:23776 [D loss: 0.669732, acc: 58.59%] [G loss: 1.921295]\n",
      "epoch:25 step:23777 [D loss: 0.671663, acc: 57.81%] [G loss: 1.837228]\n",
      "epoch:25 step:23778 [D loss: 0.632248, acc: 61.72%] [G loss: 1.844762]\n",
      "epoch:25 step:23779 [D loss: 0.632908, acc: 63.28%] [G loss: 1.880905]\n",
      "epoch:25 step:23780 [D loss: 0.671917, acc: 59.38%] [G loss: 1.696205]\n",
      "epoch:25 step:23781 [D loss: 0.665713, acc: 60.16%] [G loss: 1.767068]\n",
      "epoch:25 step:23782 [D loss: 0.647912, acc: 59.38%] [G loss: 1.834768]\n",
      "epoch:25 step:23783 [D loss: 0.614854, acc: 63.28%] [G loss: 1.934705]\n",
      "epoch:25 step:23784 [D loss: 0.629505, acc: 67.19%] [G loss: 1.974920]\n",
      "epoch:25 step:23785 [D loss: 0.627083, acc: 63.28%] [G loss: 1.949293]\n",
      "epoch:25 step:23786 [D loss: 0.673525, acc: 53.12%] [G loss: 1.821897]\n",
      "epoch:25 step:23787 [D loss: 0.665727, acc: 60.16%] [G loss: 1.762749]\n",
      "epoch:25 step:23788 [D loss: 0.669262, acc: 57.81%] [G loss: 1.730876]\n",
      "epoch:25 step:23789 [D loss: 0.681835, acc: 59.38%] [G loss: 1.815671]\n",
      "epoch:25 step:23790 [D loss: 0.611322, acc: 70.31%] [G loss: 1.875016]\n",
      "epoch:25 step:23791 [D loss: 0.619139, acc: 65.62%] [G loss: 1.834021]\n",
      "epoch:25 step:23792 [D loss: 0.643729, acc: 57.81%] [G loss: 1.844273]\n",
      "epoch:25 step:23793 [D loss: 0.632164, acc: 66.41%] [G loss: 1.827142]\n",
      "epoch:25 step:23794 [D loss: 0.667181, acc: 61.72%] [G loss: 1.819879]\n",
      "epoch:25 step:23795 [D loss: 0.612205, acc: 64.84%] [G loss: 1.892105]\n",
      "epoch:25 step:23796 [D loss: 0.668944, acc: 60.16%] [G loss: 1.946487]\n",
      "epoch:25 step:23797 [D loss: 0.649175, acc: 62.50%] [G loss: 1.823245]\n",
      "epoch:25 step:23798 [D loss: 0.673594, acc: 62.50%] [G loss: 1.731584]\n",
      "epoch:25 step:23799 [D loss: 0.648703, acc: 60.94%] [G loss: 1.844844]\n",
      "epoch:25 step:23800 [D loss: 0.648210, acc: 60.16%] [G loss: 1.828960]\n",
      "epoch:25 step:23801 [D loss: 0.688190, acc: 54.69%] [G loss: 1.839794]\n",
      "epoch:25 step:23802 [D loss: 0.650772, acc: 58.59%] [G loss: 1.723223]\n",
      "epoch:25 step:23803 [D loss: 0.667455, acc: 61.72%] [G loss: 1.874308]\n",
      "epoch:25 step:23804 [D loss: 0.652886, acc: 60.94%] [G loss: 1.811891]\n",
      "epoch:25 step:23805 [D loss: 0.608544, acc: 70.31%] [G loss: 1.916714]\n",
      "epoch:25 step:23806 [D loss: 0.568453, acc: 73.44%] [G loss: 1.979850]\n",
      "epoch:25 step:23807 [D loss: 0.685051, acc: 60.94%] [G loss: 1.756307]\n",
      "epoch:25 step:23808 [D loss: 0.693912, acc: 60.16%] [G loss: 1.780079]\n",
      "epoch:25 step:23809 [D loss: 0.600935, acc: 67.19%] [G loss: 1.888582]\n",
      "epoch:25 step:23810 [D loss: 0.609464, acc: 62.50%] [G loss: 1.988539]\n",
      "epoch:25 step:23811 [D loss: 0.675084, acc: 60.16%] [G loss: 1.800714]\n",
      "epoch:25 step:23812 [D loss: 0.631411, acc: 63.28%] [G loss: 1.812215]\n",
      "epoch:25 step:23813 [D loss: 0.664397, acc: 59.38%] [G loss: 1.877181]\n",
      "epoch:25 step:23814 [D loss: 0.647001, acc: 60.94%] [G loss: 1.920045]\n",
      "epoch:25 step:23815 [D loss: 0.673047, acc: 53.91%] [G loss: 1.901804]\n",
      "epoch:25 step:23816 [D loss: 0.655321, acc: 60.16%] [G loss: 1.738850]\n",
      "epoch:25 step:23817 [D loss: 0.637148, acc: 64.06%] [G loss: 1.829579]\n",
      "epoch:25 step:23818 [D loss: 0.642429, acc: 68.75%] [G loss: 1.748290]\n",
      "epoch:25 step:23819 [D loss: 0.667529, acc: 59.38%] [G loss: 1.769650]\n",
      "epoch:25 step:23820 [D loss: 0.599032, acc: 68.75%] [G loss: 2.042879]\n",
      "epoch:25 step:23821 [D loss: 0.698291, acc: 52.34%] [G loss: 1.871436]\n",
      "epoch:25 step:23822 [D loss: 0.645427, acc: 60.94%] [G loss: 1.691320]\n",
      "epoch:25 step:23823 [D loss: 0.637971, acc: 64.06%] [G loss: 1.963026]\n",
      "epoch:25 step:23824 [D loss: 0.690485, acc: 57.81%] [G loss: 1.792311]\n",
      "epoch:25 step:23825 [D loss: 0.654430, acc: 60.16%] [G loss: 1.754959]\n",
      "epoch:25 step:23826 [D loss: 0.674748, acc: 60.16%] [G loss: 1.964904]\n",
      "epoch:25 step:23827 [D loss: 0.663497, acc: 60.16%] [G loss: 1.840251]\n",
      "epoch:25 step:23828 [D loss: 0.687240, acc: 53.91%] [G loss: 1.829479]\n",
      "epoch:25 step:23829 [D loss: 0.630060, acc: 64.06%] [G loss: 1.839059]\n",
      "epoch:25 step:23830 [D loss: 0.629573, acc: 67.19%] [G loss: 2.068029]\n",
      "epoch:25 step:23831 [D loss: 0.668855, acc: 64.06%] [G loss: 2.092125]\n",
      "epoch:25 step:23832 [D loss: 0.692928, acc: 58.59%] [G loss: 1.859009]\n",
      "epoch:25 step:23833 [D loss: 0.681407, acc: 57.81%] [G loss: 1.780705]\n",
      "epoch:25 step:23834 [D loss: 0.635746, acc: 66.41%] [G loss: 1.851644]\n",
      "epoch:25 step:23835 [D loss: 0.683330, acc: 58.59%] [G loss: 1.870250]\n",
      "epoch:25 step:23836 [D loss: 0.654041, acc: 63.28%] [G loss: 1.941256]\n",
      "epoch:25 step:23837 [D loss: 0.649056, acc: 64.84%] [G loss: 1.810543]\n",
      "epoch:25 step:23838 [D loss: 0.610252, acc: 66.41%] [G loss: 2.116905]\n",
      "epoch:25 step:23839 [D loss: 0.705293, acc: 60.16%] [G loss: 1.944928]\n",
      "epoch:25 step:23840 [D loss: 0.574481, acc: 70.31%] [G loss: 1.993110]\n",
      "epoch:25 step:23841 [D loss: 0.646587, acc: 61.72%] [G loss: 2.045756]\n",
      "epoch:25 step:23842 [D loss: 0.637466, acc: 59.38%] [G loss: 1.943812]\n",
      "epoch:25 step:23843 [D loss: 0.657360, acc: 60.16%] [G loss: 1.795825]\n",
      "epoch:25 step:23844 [D loss: 0.600485, acc: 68.75%] [G loss: 1.985336]\n",
      "epoch:25 step:23845 [D loss: 0.734136, acc: 49.22%] [G loss: 1.771352]\n",
      "epoch:25 step:23846 [D loss: 0.650000, acc: 59.38%] [G loss: 1.827496]\n",
      "epoch:25 step:23847 [D loss: 0.665452, acc: 57.81%] [G loss: 1.788423]\n",
      "epoch:25 step:23848 [D loss: 0.693679, acc: 55.47%] [G loss: 1.831933]\n",
      "epoch:25 step:23849 [D loss: 0.648250, acc: 60.16%] [G loss: 1.813862]\n",
      "epoch:25 step:23850 [D loss: 0.707521, acc: 50.78%] [G loss: 1.859860]\n",
      "epoch:25 step:23851 [D loss: 0.676232, acc: 62.50%] [G loss: 1.748686]\n",
      "epoch:25 step:23852 [D loss: 0.660064, acc: 61.72%] [G loss: 2.048312]\n",
      "epoch:25 step:23853 [D loss: 0.593759, acc: 72.66%] [G loss: 2.085763]\n",
      "epoch:25 step:23854 [D loss: 0.630459, acc: 65.62%] [G loss: 2.104674]\n",
      "epoch:25 step:23855 [D loss: 0.591632, acc: 71.09%] [G loss: 2.015037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23856 [D loss: 0.628211, acc: 67.19%] [G loss: 1.948907]\n",
      "epoch:25 step:23857 [D loss: 0.729449, acc: 52.34%] [G loss: 1.732990]\n",
      "epoch:25 step:23858 [D loss: 0.679628, acc: 58.59%] [G loss: 1.747451]\n",
      "epoch:25 step:23859 [D loss: 0.621385, acc: 64.06%] [G loss: 1.979373]\n",
      "epoch:25 step:23860 [D loss: 0.619415, acc: 67.97%] [G loss: 1.920017]\n",
      "epoch:25 step:23861 [D loss: 0.695419, acc: 56.25%] [G loss: 1.794378]\n",
      "epoch:25 step:23862 [D loss: 0.752151, acc: 47.66%] [G loss: 1.628451]\n",
      "epoch:25 step:23863 [D loss: 0.645713, acc: 58.59%] [G loss: 1.787598]\n",
      "epoch:25 step:23864 [D loss: 0.646879, acc: 64.84%] [G loss: 1.728432]\n",
      "epoch:25 step:23865 [D loss: 0.653494, acc: 62.50%] [G loss: 1.828118]\n",
      "epoch:25 step:23866 [D loss: 0.629619, acc: 63.28%] [G loss: 1.770373]\n",
      "epoch:25 step:23867 [D loss: 0.720382, acc: 50.78%] [G loss: 1.739375]\n",
      "epoch:25 step:23868 [D loss: 0.657982, acc: 54.69%] [G loss: 1.823038]\n",
      "epoch:25 step:23869 [D loss: 0.648577, acc: 69.53%] [G loss: 1.681220]\n",
      "epoch:25 step:23870 [D loss: 0.620726, acc: 60.94%] [G loss: 1.812195]\n",
      "epoch:25 step:23871 [D loss: 0.663424, acc: 61.72%] [G loss: 2.031038]\n",
      "epoch:25 step:23872 [D loss: 0.666014, acc: 60.94%] [G loss: 1.781128]\n",
      "epoch:25 step:23873 [D loss: 0.733171, acc: 53.12%] [G loss: 1.853603]\n",
      "epoch:25 step:23874 [D loss: 0.666873, acc: 61.72%] [G loss: 1.943899]\n",
      "epoch:25 step:23875 [D loss: 0.667185, acc: 61.72%] [G loss: 1.858233]\n",
      "epoch:25 step:23876 [D loss: 0.617278, acc: 63.28%] [G loss: 1.884968]\n",
      "epoch:25 step:23877 [D loss: 0.645827, acc: 64.06%] [G loss: 1.769212]\n",
      "epoch:25 step:23878 [D loss: 0.644322, acc: 59.38%] [G loss: 1.867985]\n",
      "epoch:25 step:23879 [D loss: 0.604698, acc: 69.53%] [G loss: 1.906113]\n",
      "epoch:25 step:23880 [D loss: 0.607318, acc: 67.19%] [G loss: 1.853468]\n",
      "epoch:25 step:23881 [D loss: 0.617351, acc: 62.50%] [G loss: 2.036818]\n",
      "epoch:25 step:23882 [D loss: 0.631539, acc: 65.62%] [G loss: 1.993409]\n",
      "epoch:25 step:23883 [D loss: 0.680892, acc: 57.81%] [G loss: 1.754694]\n",
      "epoch:25 step:23884 [D loss: 0.706381, acc: 54.69%] [G loss: 1.819349]\n",
      "epoch:25 step:23885 [D loss: 0.701285, acc: 55.47%] [G loss: 1.788951]\n",
      "epoch:25 step:23886 [D loss: 0.657752, acc: 59.38%] [G loss: 1.750469]\n",
      "epoch:25 step:23887 [D loss: 0.729068, acc: 53.12%] [G loss: 1.836265]\n",
      "epoch:25 step:23888 [D loss: 0.658907, acc: 65.62%] [G loss: 1.736697]\n",
      "epoch:25 step:23889 [D loss: 0.660144, acc: 58.59%] [G loss: 1.759237]\n",
      "epoch:25 step:23890 [D loss: 0.635618, acc: 61.72%] [G loss: 1.783822]\n",
      "epoch:25 step:23891 [D loss: 0.621468, acc: 66.41%] [G loss: 1.924897]\n",
      "epoch:25 step:23892 [D loss: 0.648449, acc: 59.38%] [G loss: 1.889693]\n",
      "epoch:25 step:23893 [D loss: 0.611074, acc: 67.19%] [G loss: 1.960090]\n",
      "epoch:25 step:23894 [D loss: 0.607288, acc: 71.09%] [G loss: 2.016403]\n",
      "epoch:25 step:23895 [D loss: 0.643331, acc: 64.06%] [G loss: 1.965442]\n",
      "epoch:25 step:23896 [D loss: 0.593603, acc: 63.28%] [G loss: 2.132364]\n",
      "epoch:25 step:23897 [D loss: 0.633880, acc: 63.28%] [G loss: 1.898342]\n",
      "epoch:25 step:23898 [D loss: 0.681659, acc: 57.81%] [G loss: 1.849292]\n",
      "epoch:25 step:23899 [D loss: 0.708659, acc: 50.78%] [G loss: 1.766636]\n",
      "epoch:25 step:23900 [D loss: 0.683021, acc: 57.03%] [G loss: 1.820364]\n",
      "epoch:25 step:23901 [D loss: 0.618864, acc: 67.19%] [G loss: 1.956772]\n",
      "epoch:25 step:23902 [D loss: 0.710672, acc: 53.12%] [G loss: 1.828314]\n",
      "epoch:25 step:23903 [D loss: 0.652619, acc: 61.72%] [G loss: 1.701868]\n",
      "epoch:25 step:23904 [D loss: 0.569124, acc: 72.66%] [G loss: 2.066772]\n",
      "epoch:25 step:23905 [D loss: 0.643824, acc: 71.09%] [G loss: 1.983936]\n",
      "epoch:25 step:23906 [D loss: 0.600996, acc: 70.31%] [G loss: 2.028687]\n",
      "epoch:25 step:23907 [D loss: 0.685672, acc: 57.81%] [G loss: 1.688505]\n",
      "epoch:25 step:23908 [D loss: 0.642226, acc: 64.06%] [G loss: 1.748672]\n",
      "epoch:25 step:23909 [D loss: 0.605813, acc: 69.53%] [G loss: 1.815065]\n",
      "epoch:25 step:23910 [D loss: 0.720785, acc: 52.34%] [G loss: 1.778709]\n",
      "epoch:25 step:23911 [D loss: 0.642126, acc: 60.94%] [G loss: 1.793509]\n",
      "epoch:25 step:23912 [D loss: 0.683798, acc: 57.81%] [G loss: 1.832565]\n",
      "epoch:25 step:23913 [D loss: 0.601087, acc: 65.62%] [G loss: 1.967323]\n",
      "epoch:25 step:23914 [D loss: 0.629008, acc: 60.94%] [G loss: 1.815408]\n",
      "epoch:25 step:23915 [D loss: 0.640931, acc: 64.06%] [G loss: 1.876395]\n",
      "epoch:25 step:23916 [D loss: 0.616136, acc: 60.94%] [G loss: 1.891748]\n",
      "epoch:25 step:23917 [D loss: 0.662514, acc: 61.72%] [G loss: 1.741155]\n",
      "epoch:25 step:23918 [D loss: 0.625644, acc: 67.97%] [G loss: 1.983503]\n",
      "epoch:25 step:23919 [D loss: 0.628881, acc: 63.28%] [G loss: 1.999935]\n",
      "epoch:25 step:23920 [D loss: 0.583700, acc: 71.88%] [G loss: 1.925156]\n",
      "epoch:25 step:23921 [D loss: 0.597261, acc: 68.75%] [G loss: 2.047592]\n",
      "epoch:25 step:23922 [D loss: 0.610715, acc: 67.19%] [G loss: 2.134761]\n",
      "epoch:25 step:23923 [D loss: 0.623040, acc: 63.28%] [G loss: 2.128780]\n",
      "epoch:25 step:23924 [D loss: 0.594992, acc: 74.22%] [G loss: 2.086000]\n",
      "epoch:25 step:23925 [D loss: 0.707477, acc: 60.16%] [G loss: 1.813985]\n",
      "epoch:25 step:23926 [D loss: 0.714528, acc: 57.81%] [G loss: 1.664083]\n",
      "epoch:25 step:23927 [D loss: 0.680355, acc: 59.38%] [G loss: 1.697064]\n",
      "epoch:25 step:23928 [D loss: 0.620163, acc: 64.84%] [G loss: 1.877758]\n",
      "epoch:25 step:23929 [D loss: 0.717768, acc: 52.34%] [G loss: 1.910653]\n",
      "epoch:25 step:23930 [D loss: 0.621014, acc: 63.28%] [G loss: 1.986496]\n",
      "epoch:25 step:23931 [D loss: 0.683735, acc: 56.25%] [G loss: 1.815281]\n",
      "epoch:25 step:23932 [D loss: 0.673186, acc: 57.81%] [G loss: 1.779775]\n",
      "epoch:25 step:23933 [D loss: 0.588286, acc: 71.09%] [G loss: 1.972112]\n",
      "epoch:25 step:23934 [D loss: 0.651705, acc: 60.16%] [G loss: 1.867381]\n",
      "epoch:25 step:23935 [D loss: 0.689010, acc: 57.03%] [G loss: 1.720674]\n",
      "epoch:25 step:23936 [D loss: 0.626844, acc: 63.28%] [G loss: 1.808292]\n",
      "epoch:25 step:23937 [D loss: 0.641562, acc: 64.84%] [G loss: 1.798790]\n",
      "epoch:25 step:23938 [D loss: 0.617561, acc: 64.84%] [G loss: 1.792130]\n",
      "epoch:25 step:23939 [D loss: 0.648227, acc: 59.38%] [G loss: 1.809742]\n",
      "epoch:25 step:23940 [D loss: 0.664781, acc: 60.16%] [G loss: 1.859343]\n",
      "epoch:25 step:23941 [D loss: 0.632971, acc: 60.16%] [G loss: 1.928823]\n",
      "epoch:25 step:23942 [D loss: 0.641421, acc: 64.06%] [G loss: 1.880981]\n",
      "epoch:25 step:23943 [D loss: 0.671226, acc: 60.16%] [G loss: 1.826922]\n",
      "epoch:25 step:23944 [D loss: 0.604453, acc: 66.41%] [G loss: 1.980497]\n",
      "epoch:25 step:23945 [D loss: 0.576020, acc: 69.53%] [G loss: 1.981908]\n",
      "epoch:25 step:23946 [D loss: 0.663612, acc: 56.25%] [G loss: 1.846660]\n",
      "epoch:25 step:23947 [D loss: 0.614054, acc: 62.50%] [G loss: 2.204118]\n",
      "epoch:25 step:23948 [D loss: 0.609753, acc: 69.53%] [G loss: 1.919549]\n",
      "epoch:25 step:23949 [D loss: 0.605608, acc: 67.19%] [G loss: 1.911586]\n",
      "epoch:25 step:23950 [D loss: 0.626604, acc: 66.41%] [G loss: 1.846324]\n",
      "epoch:25 step:23951 [D loss: 0.687397, acc: 59.38%] [G loss: 1.822129]\n",
      "epoch:25 step:23952 [D loss: 0.640711, acc: 65.62%] [G loss: 1.927702]\n",
      "epoch:25 step:23953 [D loss: 0.719504, acc: 49.22%] [G loss: 1.694301]\n",
      "epoch:25 step:23954 [D loss: 0.722584, acc: 51.56%] [G loss: 1.709474]\n",
      "epoch:25 step:23955 [D loss: 0.657632, acc: 66.41%] [G loss: 1.884938]\n",
      "epoch:25 step:23956 [D loss: 0.663352, acc: 61.72%] [G loss: 1.852502]\n",
      "epoch:25 step:23957 [D loss: 0.612350, acc: 67.97%] [G loss: 1.994876]\n",
      "epoch:25 step:23958 [D loss: 0.604324, acc: 67.97%] [G loss: 2.047336]\n",
      "epoch:25 step:23959 [D loss: 0.669235, acc: 53.91%] [G loss: 2.017000]\n",
      "epoch:25 step:23960 [D loss: 0.704936, acc: 51.56%] [G loss: 1.855412]\n",
      "epoch:25 step:23961 [D loss: 0.646784, acc: 62.50%] [G loss: 1.877845]\n",
      "epoch:25 step:23962 [D loss: 0.665752, acc: 59.38%] [G loss: 1.769418]\n",
      "epoch:25 step:23963 [D loss: 0.705188, acc: 55.47%] [G loss: 1.747439]\n",
      "epoch:25 step:23964 [D loss: 0.642292, acc: 66.41%] [G loss: 1.946785]\n",
      "epoch:25 step:23965 [D loss: 0.692544, acc: 53.91%] [G loss: 1.860620]\n",
      "epoch:25 step:23966 [D loss: 0.623585, acc: 60.94%] [G loss: 1.748758]\n",
      "epoch:25 step:23967 [D loss: 0.645627, acc: 62.50%] [G loss: 1.755705]\n",
      "epoch:25 step:23968 [D loss: 0.667969, acc: 58.59%] [G loss: 1.856848]\n",
      "epoch:25 step:23969 [D loss: 0.700956, acc: 56.25%] [G loss: 1.748414]\n",
      "epoch:25 step:23970 [D loss: 0.631623, acc: 62.50%] [G loss: 1.969617]\n",
      "epoch:25 step:23971 [D loss: 0.659678, acc: 57.03%] [G loss: 1.821493]\n",
      "epoch:25 step:23972 [D loss: 0.628847, acc: 59.38%] [G loss: 1.935695]\n",
      "epoch:25 step:23973 [D loss: 0.637172, acc: 58.59%] [G loss: 1.825791]\n",
      "epoch:25 step:23974 [D loss: 0.613104, acc: 67.97%] [G loss: 1.990418]\n",
      "epoch:25 step:23975 [D loss: 0.611548, acc: 67.19%] [G loss: 1.906591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:23976 [D loss: 0.639180, acc: 67.97%] [G loss: 2.045395]\n",
      "epoch:25 step:23977 [D loss: 0.616021, acc: 65.62%] [G loss: 2.165518]\n",
      "epoch:25 step:23978 [D loss: 0.654350, acc: 64.06%] [G loss: 1.758261]\n",
      "epoch:25 step:23979 [D loss: 0.633171, acc: 62.50%] [G loss: 2.212985]\n",
      "epoch:25 step:23980 [D loss: 0.669572, acc: 66.41%] [G loss: 1.916136]\n",
      "epoch:25 step:23981 [D loss: 0.624849, acc: 67.97%] [G loss: 1.940741]\n",
      "epoch:25 step:23982 [D loss: 0.652245, acc: 58.59%] [G loss: 1.856389]\n",
      "epoch:25 step:23983 [D loss: 0.617627, acc: 67.19%] [G loss: 1.891574]\n",
      "epoch:25 step:23984 [D loss: 0.726125, acc: 50.78%] [G loss: 1.697170]\n",
      "epoch:25 step:23985 [D loss: 0.684626, acc: 58.59%] [G loss: 1.754389]\n",
      "epoch:25 step:23986 [D loss: 0.652332, acc: 61.72%] [G loss: 1.780615]\n",
      "epoch:25 step:23987 [D loss: 0.634125, acc: 61.72%] [G loss: 1.833071]\n",
      "epoch:25 step:23988 [D loss: 0.560434, acc: 71.88%] [G loss: 1.975545]\n",
      "epoch:25 step:23989 [D loss: 0.625834, acc: 65.62%] [G loss: 2.070842]\n",
      "epoch:25 step:23990 [D loss: 0.665488, acc: 60.16%] [G loss: 1.903385]\n",
      "epoch:25 step:23991 [D loss: 0.697834, acc: 57.81%] [G loss: 1.665586]\n",
      "epoch:25 step:23992 [D loss: 0.657239, acc: 63.28%] [G loss: 1.879165]\n",
      "epoch:25 step:23993 [D loss: 0.641556, acc: 57.81%] [G loss: 1.831205]\n",
      "epoch:25 step:23994 [D loss: 0.660443, acc: 57.81%] [G loss: 1.857298]\n",
      "epoch:25 step:23995 [D loss: 0.609656, acc: 68.75%] [G loss: 1.891252]\n",
      "epoch:25 step:23996 [D loss: 0.625875, acc: 57.81%] [G loss: 1.940593]\n",
      "epoch:25 step:23997 [D loss: 0.674979, acc: 60.16%] [G loss: 1.822877]\n",
      "epoch:25 step:23998 [D loss: 0.678482, acc: 53.91%] [G loss: 1.708406]\n",
      "epoch:25 step:23999 [D loss: 0.656030, acc: 59.38%] [G loss: 1.867736]\n",
      "epoch:25 step:24000 [D loss: 0.654717, acc: 61.72%] [G loss: 1.818148]\n",
      "epoch:25 step:24001 [D loss: 0.635707, acc: 64.06%] [G loss: 1.730326]\n",
      "epoch:25 step:24002 [D loss: 0.651628, acc: 63.28%] [G loss: 1.821766]\n",
      "epoch:25 step:24003 [D loss: 0.626707, acc: 64.06%] [G loss: 1.951889]\n",
      "epoch:25 step:24004 [D loss: 0.687625, acc: 59.38%] [G loss: 1.774545]\n",
      "epoch:25 step:24005 [D loss: 0.675510, acc: 60.94%] [G loss: 1.766053]\n",
      "epoch:25 step:24006 [D loss: 0.650289, acc: 60.94%] [G loss: 1.798697]\n",
      "epoch:25 step:24007 [D loss: 0.661836, acc: 63.28%] [G loss: 1.967521]\n",
      "epoch:25 step:24008 [D loss: 0.685498, acc: 58.59%] [G loss: 1.914151]\n",
      "epoch:25 step:24009 [D loss: 0.668399, acc: 54.69%] [G loss: 1.833208]\n",
      "epoch:25 step:24010 [D loss: 0.637479, acc: 62.50%] [G loss: 1.879058]\n",
      "epoch:25 step:24011 [D loss: 0.651197, acc: 60.16%] [G loss: 1.800727]\n",
      "epoch:25 step:24012 [D loss: 0.673045, acc: 57.81%] [G loss: 1.957936]\n",
      "epoch:25 step:24013 [D loss: 0.672642, acc: 61.72%] [G loss: 1.912451]\n",
      "epoch:25 step:24014 [D loss: 0.657345, acc: 61.72%] [G loss: 1.805376]\n",
      "epoch:25 step:24015 [D loss: 0.624920, acc: 66.41%] [G loss: 1.869992]\n",
      "epoch:25 step:24016 [D loss: 0.629767, acc: 69.53%] [G loss: 1.957939]\n",
      "epoch:25 step:24017 [D loss: 0.618524, acc: 66.41%] [G loss: 1.888226]\n",
      "epoch:25 step:24018 [D loss: 0.638471, acc: 59.38%] [G loss: 1.912289]\n",
      "epoch:25 step:24019 [D loss: 0.629452, acc: 58.59%] [G loss: 1.913023]\n",
      "epoch:25 step:24020 [D loss: 0.693612, acc: 53.12%] [G loss: 1.840137]\n",
      "epoch:25 step:24021 [D loss: 0.653420, acc: 56.25%] [G loss: 1.803724]\n",
      "epoch:25 step:24022 [D loss: 0.650785, acc: 61.72%] [G loss: 1.871376]\n",
      "epoch:25 step:24023 [D loss: 0.676759, acc: 56.25%] [G loss: 1.875524]\n",
      "epoch:25 step:24024 [D loss: 0.681604, acc: 61.72%] [G loss: 1.879194]\n",
      "epoch:25 step:24025 [D loss: 0.631171, acc: 68.75%] [G loss: 1.783901]\n",
      "epoch:25 step:24026 [D loss: 0.655932, acc: 63.28%] [G loss: 1.798383]\n",
      "epoch:25 step:24027 [D loss: 0.600160, acc: 67.97%] [G loss: 2.078841]\n",
      "epoch:25 step:24028 [D loss: 0.642576, acc: 68.75%] [G loss: 1.988934]\n",
      "epoch:25 step:24029 [D loss: 0.666196, acc: 59.38%] [G loss: 1.990348]\n",
      "epoch:25 step:24030 [D loss: 0.609688, acc: 67.19%] [G loss: 1.960212]\n",
      "epoch:25 step:24031 [D loss: 0.652459, acc: 64.06%] [G loss: 1.849219]\n",
      "epoch:25 step:24032 [D loss: 0.718147, acc: 57.03%] [G loss: 1.798131]\n",
      "epoch:25 step:24033 [D loss: 0.693068, acc: 57.03%] [G loss: 1.772928]\n",
      "epoch:25 step:24034 [D loss: 0.697569, acc: 57.81%] [G loss: 1.864636]\n",
      "epoch:25 step:24035 [D loss: 0.624671, acc: 67.19%] [G loss: 1.907568]\n",
      "epoch:25 step:24036 [D loss: 0.658927, acc: 59.38%] [G loss: 1.912426]\n",
      "epoch:25 step:24037 [D loss: 0.695776, acc: 56.25%] [G loss: 1.877331]\n",
      "epoch:25 step:24038 [D loss: 0.592434, acc: 71.88%] [G loss: 1.989965]\n",
      "epoch:25 step:24039 [D loss: 0.738503, acc: 44.53%] [G loss: 1.769475]\n",
      "epoch:25 step:24040 [D loss: 0.651491, acc: 58.59%] [G loss: 1.847560]\n",
      "epoch:25 step:24041 [D loss: 0.674807, acc: 57.03%] [G loss: 1.868867]\n",
      "epoch:25 step:24042 [D loss: 0.721445, acc: 53.12%] [G loss: 1.768981]\n",
      "epoch:25 step:24043 [D loss: 0.649904, acc: 62.50%] [G loss: 1.869899]\n",
      "epoch:25 step:24044 [D loss: 0.657821, acc: 60.16%] [G loss: 1.839280]\n",
      "epoch:25 step:24045 [D loss: 0.636288, acc: 65.62%] [G loss: 1.860631]\n",
      "epoch:25 step:24046 [D loss: 0.641084, acc: 62.50%] [G loss: 1.811934]\n",
      "epoch:25 step:24047 [D loss: 0.680720, acc: 60.16%] [G loss: 1.856345]\n",
      "epoch:25 step:24048 [D loss: 0.667084, acc: 61.72%] [G loss: 1.760002]\n",
      "epoch:25 step:24049 [D loss: 0.614891, acc: 65.62%] [G loss: 1.953682]\n",
      "epoch:25 step:24050 [D loss: 0.648619, acc: 63.28%] [G loss: 1.893149]\n",
      "epoch:25 step:24051 [D loss: 0.649613, acc: 63.28%] [G loss: 1.752230]\n",
      "epoch:25 step:24052 [D loss: 0.655406, acc: 64.84%] [G loss: 1.822437]\n",
      "epoch:25 step:24053 [D loss: 0.708906, acc: 57.03%] [G loss: 1.863954]\n",
      "epoch:25 step:24054 [D loss: 0.646942, acc: 64.06%] [G loss: 1.893566]\n",
      "epoch:25 step:24055 [D loss: 0.645410, acc: 64.84%] [G loss: 1.929338]\n",
      "epoch:25 step:24056 [D loss: 0.630779, acc: 63.28%] [G loss: 1.845203]\n",
      "epoch:25 step:24057 [D loss: 0.634706, acc: 65.62%] [G loss: 1.900962]\n",
      "epoch:25 step:24058 [D loss: 0.640135, acc: 64.84%] [G loss: 2.054839]\n",
      "epoch:25 step:24059 [D loss: 0.628315, acc: 67.19%] [G loss: 2.119233]\n",
      "epoch:25 step:24060 [D loss: 0.664406, acc: 56.25%] [G loss: 1.822078]\n",
      "epoch:25 step:24061 [D loss: 0.678757, acc: 57.03%] [G loss: 1.873961]\n",
      "epoch:25 step:24062 [D loss: 0.650537, acc: 61.72%] [G loss: 1.923184]\n",
      "epoch:25 step:24063 [D loss: 0.641600, acc: 55.47%] [G loss: 1.813187]\n",
      "epoch:25 step:24064 [D loss: 0.711574, acc: 55.47%] [G loss: 1.833959]\n",
      "epoch:25 step:24065 [D loss: 0.643925, acc: 62.50%] [G loss: 1.896776]\n",
      "epoch:25 step:24066 [D loss: 0.611764, acc: 68.75%] [G loss: 1.840662]\n",
      "epoch:25 step:24067 [D loss: 0.703143, acc: 58.59%] [G loss: 1.773737]\n",
      "epoch:25 step:24068 [D loss: 0.633353, acc: 64.06%] [G loss: 1.828577]\n",
      "epoch:25 step:24069 [D loss: 0.624275, acc: 67.19%] [G loss: 1.939955]\n",
      "epoch:25 step:24070 [D loss: 0.656136, acc: 57.81%] [G loss: 1.859816]\n",
      "epoch:25 step:24071 [D loss: 0.643513, acc: 64.06%] [G loss: 2.045403]\n",
      "epoch:25 step:24072 [D loss: 0.595543, acc: 67.97%] [G loss: 2.074449]\n",
      "epoch:25 step:24073 [D loss: 0.545574, acc: 73.44%] [G loss: 2.343433]\n",
      "epoch:25 step:24074 [D loss: 0.595642, acc: 71.09%] [G loss: 2.385905]\n",
      "epoch:25 step:24075 [D loss: 0.592154, acc: 67.19%] [G loss: 2.337658]\n",
      "epoch:25 step:24076 [D loss: 0.665125, acc: 60.16%] [G loss: 1.966794]\n",
      "epoch:25 step:24077 [D loss: 0.624995, acc: 64.84%] [G loss: 1.810897]\n",
      "epoch:25 step:24078 [D loss: 0.613230, acc: 69.53%] [G loss: 1.968032]\n",
      "epoch:25 step:24079 [D loss: 0.619598, acc: 65.62%] [G loss: 2.111991]\n",
      "epoch:25 step:24080 [D loss: 0.648255, acc: 60.94%] [G loss: 1.965940]\n",
      "epoch:25 step:24081 [D loss: 0.674802, acc: 54.69%] [G loss: 1.823607]\n",
      "epoch:25 step:24082 [D loss: 0.634963, acc: 66.41%] [G loss: 1.744790]\n",
      "epoch:25 step:24083 [D loss: 0.687190, acc: 59.38%] [G loss: 1.847791]\n",
      "epoch:25 step:24084 [D loss: 0.599466, acc: 71.88%] [G loss: 1.881785]\n",
      "epoch:25 step:24085 [D loss: 0.637952, acc: 61.72%] [G loss: 1.931726]\n",
      "epoch:25 step:24086 [D loss: 0.606118, acc: 67.97%] [G loss: 1.934301]\n",
      "epoch:25 step:24087 [D loss: 0.644340, acc: 58.59%] [G loss: 2.084102]\n",
      "epoch:25 step:24088 [D loss: 0.643717, acc: 64.84%] [G loss: 1.911033]\n",
      "epoch:25 step:24089 [D loss: 0.583825, acc: 72.66%] [G loss: 1.734385]\n",
      "epoch:25 step:24090 [D loss: 0.649760, acc: 64.84%] [G loss: 1.913206]\n",
      "epoch:25 step:24091 [D loss: 0.626356, acc: 66.41%] [G loss: 1.795227]\n",
      "epoch:25 step:24092 [D loss: 0.684652, acc: 59.38%] [G loss: 1.765225]\n",
      "epoch:25 step:24093 [D loss: 0.692734, acc: 54.69%] [G loss: 1.731589]\n",
      "epoch:25 step:24094 [D loss: 0.686869, acc: 58.59%] [G loss: 1.728751]\n",
      "epoch:25 step:24095 [D loss: 0.671460, acc: 62.50%] [G loss: 1.764032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24096 [D loss: 0.710807, acc: 53.91%] [G loss: 1.880159]\n",
      "epoch:25 step:24097 [D loss: 0.652459, acc: 58.59%] [G loss: 1.843822]\n",
      "epoch:25 step:24098 [D loss: 0.701673, acc: 53.91%] [G loss: 1.912391]\n",
      "epoch:25 step:24099 [D loss: 0.656346, acc: 67.19%] [G loss: 1.937672]\n",
      "epoch:25 step:24100 [D loss: 0.672720, acc: 56.25%] [G loss: 1.752176]\n",
      "epoch:25 step:24101 [D loss: 0.700906, acc: 54.69%] [G loss: 1.779565]\n",
      "epoch:25 step:24102 [D loss: 0.663367, acc: 56.25%] [G loss: 1.781480]\n",
      "epoch:25 step:24103 [D loss: 0.626598, acc: 60.94%] [G loss: 1.742176]\n",
      "epoch:25 step:24104 [D loss: 0.672350, acc: 53.91%] [G loss: 1.799698]\n",
      "epoch:25 step:24105 [D loss: 0.647202, acc: 64.84%] [G loss: 1.971063]\n",
      "epoch:25 step:24106 [D loss: 0.634969, acc: 62.50%] [G loss: 1.990856]\n",
      "epoch:25 step:24107 [D loss: 0.701788, acc: 54.69%] [G loss: 1.831604]\n",
      "epoch:25 step:24108 [D loss: 0.622723, acc: 65.62%] [G loss: 1.797025]\n",
      "epoch:25 step:24109 [D loss: 0.721285, acc: 51.56%] [G loss: 1.627305]\n",
      "epoch:25 step:24110 [D loss: 0.692080, acc: 53.12%] [G loss: 1.795585]\n",
      "epoch:25 step:24111 [D loss: 0.619496, acc: 67.97%] [G loss: 1.804008]\n",
      "epoch:25 step:24112 [D loss: 0.641211, acc: 63.28%] [G loss: 1.976612]\n",
      "epoch:25 step:24113 [D loss: 0.658786, acc: 66.41%] [G loss: 1.851344]\n",
      "epoch:25 step:24114 [D loss: 0.670149, acc: 57.81%] [G loss: 2.020059]\n",
      "epoch:25 step:24115 [D loss: 0.666697, acc: 59.38%] [G loss: 1.946311]\n",
      "epoch:25 step:24116 [D loss: 0.622149, acc: 64.84%] [G loss: 1.834672]\n",
      "epoch:25 step:24117 [D loss: 0.651348, acc: 59.38%] [G loss: 1.915002]\n",
      "epoch:25 step:24118 [D loss: 0.658595, acc: 53.91%] [G loss: 1.870659]\n",
      "epoch:25 step:24119 [D loss: 0.626300, acc: 60.94%] [G loss: 2.108121]\n",
      "epoch:25 step:24120 [D loss: 0.612631, acc: 67.19%] [G loss: 1.982982]\n",
      "epoch:25 step:24121 [D loss: 0.662712, acc: 60.94%] [G loss: 1.761761]\n",
      "epoch:25 step:24122 [D loss: 0.675839, acc: 58.59%] [G loss: 1.944628]\n",
      "epoch:25 step:24123 [D loss: 0.659290, acc: 57.81%] [G loss: 1.741440]\n",
      "epoch:25 step:24124 [D loss: 0.657434, acc: 63.28%] [G loss: 1.929438]\n",
      "epoch:25 step:24125 [D loss: 0.670684, acc: 61.72%] [G loss: 1.872420]\n",
      "epoch:25 step:24126 [D loss: 0.612064, acc: 73.44%] [G loss: 1.818005]\n",
      "epoch:25 step:24127 [D loss: 0.660176, acc: 62.50%] [G loss: 1.722567]\n",
      "epoch:25 step:24128 [D loss: 0.665621, acc: 60.94%] [G loss: 1.703317]\n",
      "epoch:25 step:24129 [D loss: 0.678303, acc: 59.38%] [G loss: 1.714930]\n",
      "epoch:25 step:24130 [D loss: 0.674720, acc: 55.47%] [G loss: 1.785320]\n",
      "epoch:25 step:24131 [D loss: 0.653637, acc: 59.38%] [G loss: 1.737872]\n",
      "epoch:25 step:24132 [D loss: 0.590338, acc: 70.31%] [G loss: 1.845618]\n",
      "epoch:25 step:24133 [D loss: 0.610276, acc: 64.06%] [G loss: 1.965583]\n",
      "epoch:25 step:24134 [D loss: 0.607420, acc: 66.41%] [G loss: 1.934663]\n",
      "epoch:25 step:24135 [D loss: 0.701490, acc: 53.12%] [G loss: 1.725334]\n",
      "epoch:25 step:24136 [D loss: 0.644382, acc: 66.41%] [G loss: 1.925402]\n",
      "epoch:25 step:24137 [D loss: 0.640555, acc: 61.72%] [G loss: 1.798984]\n",
      "epoch:25 step:24138 [D loss: 0.658243, acc: 61.72%] [G loss: 1.833165]\n",
      "epoch:25 step:24139 [D loss: 0.595761, acc: 69.53%] [G loss: 2.015962]\n",
      "epoch:25 step:24140 [D loss: 0.642778, acc: 64.84%] [G loss: 1.884003]\n",
      "epoch:25 step:24141 [D loss: 0.723451, acc: 52.34%] [G loss: 1.848627]\n",
      "epoch:25 step:24142 [D loss: 0.648384, acc: 64.84%] [G loss: 1.732992]\n",
      "epoch:25 step:24143 [D loss: 0.629436, acc: 61.72%] [G loss: 1.799686]\n",
      "epoch:25 step:24144 [D loss: 0.593510, acc: 69.53%] [G loss: 2.017939]\n",
      "epoch:25 step:24145 [D loss: 0.641930, acc: 62.50%] [G loss: 2.073711]\n",
      "epoch:25 step:24146 [D loss: 0.661947, acc: 64.06%] [G loss: 1.857271]\n",
      "epoch:25 step:24147 [D loss: 0.671940, acc: 58.59%] [G loss: 1.828663]\n",
      "epoch:25 step:24148 [D loss: 0.666759, acc: 60.94%] [G loss: 1.810538]\n",
      "epoch:25 step:24149 [D loss: 0.618040, acc: 68.75%] [G loss: 1.756925]\n",
      "epoch:25 step:24150 [D loss: 0.689798, acc: 55.47%] [G loss: 1.905183]\n",
      "epoch:25 step:24151 [D loss: 0.625372, acc: 64.84%] [G loss: 1.980039]\n",
      "epoch:25 step:24152 [D loss: 0.703053, acc: 58.59%] [G loss: 1.862868]\n",
      "epoch:25 step:24153 [D loss: 0.635071, acc: 66.41%] [G loss: 1.853911]\n",
      "epoch:25 step:24154 [D loss: 0.655665, acc: 59.38%] [G loss: 1.883206]\n",
      "epoch:25 step:24155 [D loss: 0.648221, acc: 60.16%] [G loss: 1.816605]\n",
      "epoch:25 step:24156 [D loss: 0.643460, acc: 60.16%] [G loss: 1.767733]\n",
      "epoch:25 step:24157 [D loss: 0.661392, acc: 57.03%] [G loss: 1.768914]\n",
      "epoch:25 step:24158 [D loss: 0.633674, acc: 64.06%] [G loss: 1.794397]\n",
      "epoch:25 step:24159 [D loss: 0.703340, acc: 55.47%] [G loss: 1.702641]\n",
      "epoch:25 step:24160 [D loss: 0.670059, acc: 60.94%] [G loss: 1.879374]\n",
      "epoch:25 step:24161 [D loss: 0.628189, acc: 67.19%] [G loss: 1.771680]\n",
      "epoch:25 step:24162 [D loss: 0.647985, acc: 63.28%] [G loss: 1.801380]\n",
      "epoch:25 step:24163 [D loss: 0.672566, acc: 59.38%] [G loss: 1.797515]\n",
      "epoch:25 step:24164 [D loss: 0.676971, acc: 60.94%] [G loss: 1.840627]\n",
      "epoch:25 step:24165 [D loss: 0.628246, acc: 61.72%] [G loss: 1.798859]\n",
      "epoch:25 step:24166 [D loss: 0.667891, acc: 57.81%] [G loss: 1.741833]\n",
      "epoch:25 step:24167 [D loss: 0.646809, acc: 58.59%] [G loss: 1.952874]\n",
      "epoch:25 step:24168 [D loss: 0.635717, acc: 60.94%] [G loss: 1.755272]\n",
      "epoch:25 step:24169 [D loss: 0.663324, acc: 59.38%] [G loss: 1.826924]\n",
      "epoch:25 step:24170 [D loss: 0.668146, acc: 57.81%] [G loss: 1.885420]\n",
      "epoch:25 step:24171 [D loss: 0.608488, acc: 67.97%] [G loss: 1.923389]\n",
      "epoch:25 step:24172 [D loss: 0.618633, acc: 66.41%] [G loss: 2.025144]\n",
      "epoch:25 step:24173 [D loss: 0.628212, acc: 66.41%] [G loss: 1.837339]\n",
      "epoch:25 step:24174 [D loss: 0.663394, acc: 58.59%] [G loss: 1.767758]\n",
      "epoch:25 step:24175 [D loss: 0.650056, acc: 64.84%] [G loss: 1.850334]\n",
      "epoch:25 step:24176 [D loss: 0.626940, acc: 74.22%] [G loss: 1.845713]\n",
      "epoch:25 step:24177 [D loss: 0.688848, acc: 57.03%] [G loss: 1.837648]\n",
      "epoch:25 step:24178 [D loss: 0.615729, acc: 69.53%] [G loss: 1.774385]\n",
      "epoch:25 step:24179 [D loss: 0.637231, acc: 71.09%] [G loss: 1.921355]\n",
      "epoch:25 step:24180 [D loss: 0.591323, acc: 67.19%] [G loss: 1.996463]\n",
      "epoch:25 step:24181 [D loss: 0.607646, acc: 66.41%] [G loss: 1.830386]\n",
      "epoch:25 step:24182 [D loss: 0.654670, acc: 64.06%] [G loss: 1.883920]\n",
      "epoch:25 step:24183 [D loss: 0.659410, acc: 64.84%] [G loss: 1.809927]\n",
      "epoch:25 step:24184 [D loss: 0.639139, acc: 60.16%] [G loss: 1.857200]\n",
      "epoch:25 step:24185 [D loss: 0.681437, acc: 63.28%] [G loss: 1.858604]\n",
      "epoch:25 step:24186 [D loss: 0.676047, acc: 56.25%] [G loss: 1.746096]\n",
      "epoch:25 step:24187 [D loss: 0.657945, acc: 60.94%] [G loss: 1.888293]\n",
      "epoch:25 step:24188 [D loss: 0.677001, acc: 57.81%] [G loss: 1.735970]\n",
      "epoch:25 step:24189 [D loss: 0.648858, acc: 60.94%] [G loss: 1.862330]\n",
      "epoch:25 step:24190 [D loss: 0.640615, acc: 64.06%] [G loss: 1.719576]\n",
      "epoch:25 step:24191 [D loss: 0.676254, acc: 59.38%] [G loss: 1.775146]\n",
      "epoch:25 step:24192 [D loss: 0.681888, acc: 62.50%] [G loss: 1.965284]\n",
      "epoch:25 step:24193 [D loss: 0.612273, acc: 67.19%] [G loss: 1.959938]\n",
      "epoch:25 step:24194 [D loss: 0.677177, acc: 58.59%] [G loss: 1.894029]\n",
      "epoch:25 step:24195 [D loss: 0.651117, acc: 63.28%] [G loss: 1.916510]\n",
      "epoch:25 step:24196 [D loss: 0.651703, acc: 62.50%] [G loss: 1.898241]\n",
      "epoch:25 step:24197 [D loss: 0.644210, acc: 62.50%] [G loss: 1.768008]\n",
      "epoch:25 step:24198 [D loss: 0.624861, acc: 61.72%] [G loss: 1.960358]\n",
      "epoch:25 step:24199 [D loss: 0.600588, acc: 68.75%] [G loss: 1.849997]\n",
      "epoch:25 step:24200 [D loss: 0.643083, acc: 59.38%] [G loss: 1.973844]\n",
      "epoch:25 step:24201 [D loss: 0.621712, acc: 65.62%] [G loss: 1.962167]\n",
      "epoch:25 step:24202 [D loss: 0.684345, acc: 58.59%] [G loss: 1.919616]\n",
      "epoch:25 step:24203 [D loss: 0.659781, acc: 62.50%] [G loss: 1.712431]\n",
      "epoch:25 step:24204 [D loss: 0.677066, acc: 58.59%] [G loss: 1.973389]\n",
      "epoch:25 step:24205 [D loss: 0.663472, acc: 58.59%] [G loss: 1.871020]\n",
      "epoch:25 step:24206 [D loss: 0.620078, acc: 64.06%] [G loss: 2.140274]\n",
      "epoch:25 step:24207 [D loss: 0.637581, acc: 64.06%] [G loss: 1.930770]\n",
      "epoch:25 step:24208 [D loss: 0.669252, acc: 63.28%] [G loss: 1.923117]\n",
      "epoch:25 step:24209 [D loss: 0.683668, acc: 56.25%] [G loss: 1.784826]\n",
      "epoch:25 step:24210 [D loss: 0.697723, acc: 57.03%] [G loss: 1.857307]\n",
      "epoch:25 step:24211 [D loss: 0.623497, acc: 60.94%] [G loss: 1.919995]\n",
      "epoch:25 step:24212 [D loss: 0.706949, acc: 53.12%] [G loss: 1.818112]\n",
      "epoch:25 step:24213 [D loss: 0.667493, acc: 59.38%] [G loss: 1.713848]\n",
      "epoch:25 step:24214 [D loss: 0.659185, acc: 63.28%] [G loss: 1.913757]\n",
      "epoch:25 step:24215 [D loss: 0.636312, acc: 65.62%] [G loss: 1.810541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24216 [D loss: 0.667955, acc: 58.59%] [G loss: 1.908400]\n",
      "epoch:25 step:24217 [D loss: 0.590002, acc: 74.22%] [G loss: 2.025310]\n",
      "epoch:25 step:24218 [D loss: 0.679761, acc: 60.94%] [G loss: 1.809829]\n",
      "epoch:25 step:24219 [D loss: 0.700809, acc: 53.12%] [G loss: 1.835578]\n",
      "epoch:25 step:24220 [D loss: 0.740978, acc: 52.34%] [G loss: 1.728667]\n",
      "epoch:25 step:24221 [D loss: 0.647109, acc: 62.50%] [G loss: 1.847141]\n",
      "epoch:25 step:24222 [D loss: 0.675029, acc: 56.25%] [G loss: 1.765799]\n",
      "epoch:25 step:24223 [D loss: 0.600390, acc: 66.41%] [G loss: 1.919987]\n",
      "epoch:25 step:24224 [D loss: 0.699919, acc: 57.81%] [G loss: 1.816562]\n",
      "epoch:25 step:24225 [D loss: 0.685509, acc: 51.56%] [G loss: 1.677055]\n",
      "epoch:25 step:24226 [D loss: 0.698495, acc: 54.69%] [G loss: 1.666321]\n",
      "epoch:25 step:24227 [D loss: 0.646190, acc: 64.06%] [G loss: 1.816121]\n",
      "epoch:25 step:24228 [D loss: 0.652241, acc: 64.06%] [G loss: 1.950828]\n",
      "epoch:25 step:24229 [D loss: 0.579789, acc: 75.78%] [G loss: 2.062160]\n",
      "epoch:25 step:24230 [D loss: 0.615585, acc: 68.75%] [G loss: 2.025103]\n",
      "epoch:25 step:24231 [D loss: 0.650876, acc: 59.38%] [G loss: 1.887826]\n",
      "epoch:25 step:24232 [D loss: 0.632492, acc: 60.94%] [G loss: 1.899344]\n",
      "epoch:25 step:24233 [D loss: 0.582699, acc: 67.97%] [G loss: 1.946625]\n",
      "epoch:25 step:24234 [D loss: 0.645694, acc: 58.59%] [G loss: 1.903306]\n",
      "epoch:25 step:24235 [D loss: 0.671097, acc: 57.03%] [G loss: 1.879225]\n",
      "epoch:25 step:24236 [D loss: 0.620938, acc: 72.66%] [G loss: 1.880919]\n",
      "epoch:25 step:24237 [D loss: 0.700020, acc: 50.78%] [G loss: 1.882948]\n",
      "epoch:25 step:24238 [D loss: 0.659884, acc: 61.72%] [G loss: 1.865826]\n",
      "epoch:25 step:24239 [D loss: 0.657586, acc: 62.50%] [G loss: 1.890788]\n",
      "epoch:25 step:24240 [D loss: 0.657000, acc: 64.06%] [G loss: 1.998942]\n",
      "epoch:25 step:24241 [D loss: 0.621600, acc: 64.06%] [G loss: 2.017579]\n",
      "epoch:25 step:24242 [D loss: 0.694954, acc: 59.38%] [G loss: 1.874117]\n",
      "epoch:25 step:24243 [D loss: 0.656183, acc: 64.84%] [G loss: 1.701294]\n",
      "epoch:25 step:24244 [D loss: 0.664714, acc: 61.72%] [G loss: 1.933862]\n",
      "epoch:25 step:24245 [D loss: 0.686676, acc: 59.38%] [G loss: 1.697859]\n",
      "epoch:25 step:24246 [D loss: 0.640700, acc: 60.94%] [G loss: 1.852452]\n",
      "epoch:25 step:24247 [D loss: 0.650108, acc: 59.38%] [G loss: 1.701074]\n",
      "epoch:25 step:24248 [D loss: 0.595299, acc: 67.97%] [G loss: 1.943827]\n",
      "epoch:25 step:24249 [D loss: 0.620954, acc: 65.62%] [G loss: 1.795136]\n",
      "epoch:25 step:24250 [D loss: 0.678211, acc: 53.91%] [G loss: 1.923573]\n",
      "epoch:25 step:24251 [D loss: 0.716903, acc: 51.56%] [G loss: 1.788851]\n",
      "epoch:25 step:24252 [D loss: 0.648127, acc: 59.38%] [G loss: 1.828449]\n",
      "epoch:25 step:24253 [D loss: 0.685792, acc: 60.94%] [G loss: 1.752684]\n",
      "epoch:25 step:24254 [D loss: 0.682139, acc: 57.81%] [G loss: 1.779990]\n",
      "epoch:25 step:24255 [D loss: 0.637417, acc: 60.16%] [G loss: 1.716006]\n",
      "epoch:25 step:24256 [D loss: 0.669942, acc: 57.03%] [G loss: 1.932903]\n",
      "epoch:25 step:24257 [D loss: 0.678545, acc: 53.12%] [G loss: 1.853480]\n",
      "epoch:25 step:24258 [D loss: 0.628498, acc: 69.53%] [G loss: 1.871436]\n",
      "epoch:25 step:24259 [D loss: 0.612093, acc: 65.62%] [G loss: 1.823678]\n",
      "epoch:25 step:24260 [D loss: 0.647437, acc: 66.41%] [G loss: 1.853947]\n",
      "epoch:25 step:24261 [D loss: 0.633914, acc: 67.19%] [G loss: 1.967193]\n",
      "epoch:25 step:24262 [D loss: 0.690608, acc: 57.81%] [G loss: 1.979177]\n",
      "epoch:25 step:24263 [D loss: 0.582109, acc: 66.41%] [G loss: 1.769614]\n",
      "epoch:25 step:24264 [D loss: 0.633303, acc: 61.72%] [G loss: 1.914060]\n",
      "epoch:25 step:24265 [D loss: 0.649767, acc: 65.62%] [G loss: 2.008044]\n",
      "epoch:25 step:24266 [D loss: 0.605018, acc: 65.62%] [G loss: 1.968001]\n",
      "epoch:25 step:24267 [D loss: 0.560435, acc: 71.88%] [G loss: 1.834288]\n",
      "epoch:25 step:24268 [D loss: 0.632509, acc: 63.28%] [G loss: 1.820435]\n",
      "epoch:25 step:24269 [D loss: 0.688106, acc: 59.38%] [G loss: 1.800704]\n",
      "epoch:25 step:24270 [D loss: 0.655513, acc: 64.84%] [G loss: 1.972927]\n",
      "epoch:25 step:24271 [D loss: 0.627913, acc: 67.97%] [G loss: 1.904549]\n",
      "epoch:25 step:24272 [D loss: 0.630800, acc: 64.84%] [G loss: 1.958910]\n",
      "epoch:25 step:24273 [D loss: 0.619036, acc: 63.28%] [G loss: 1.851603]\n",
      "epoch:25 step:24274 [D loss: 0.618900, acc: 60.94%] [G loss: 1.923696]\n",
      "epoch:25 step:24275 [D loss: 0.709542, acc: 57.03%] [G loss: 1.841020]\n",
      "epoch:25 step:24276 [D loss: 0.661984, acc: 60.94%] [G loss: 1.801172]\n",
      "epoch:25 step:24277 [D loss: 0.668423, acc: 56.25%] [G loss: 1.859156]\n",
      "epoch:25 step:24278 [D loss: 0.679536, acc: 56.25%] [G loss: 1.787441]\n",
      "epoch:25 step:24279 [D loss: 0.661169, acc: 58.59%] [G loss: 1.842961]\n",
      "epoch:25 step:24280 [D loss: 0.731312, acc: 50.00%] [G loss: 1.701707]\n",
      "epoch:25 step:24281 [D loss: 0.635200, acc: 59.38%] [G loss: 1.817047]\n",
      "epoch:25 step:24282 [D loss: 0.624092, acc: 68.75%] [G loss: 1.930653]\n",
      "epoch:25 step:24283 [D loss: 0.672293, acc: 57.81%] [G loss: 1.671718]\n",
      "epoch:25 step:24284 [D loss: 0.706254, acc: 53.91%] [G loss: 1.797059]\n",
      "epoch:25 step:24285 [D loss: 0.654692, acc: 66.41%] [G loss: 1.779812]\n",
      "epoch:25 step:24286 [D loss: 0.725451, acc: 49.22%] [G loss: 1.701185]\n",
      "epoch:25 step:24287 [D loss: 0.665291, acc: 57.81%] [G loss: 1.695289]\n",
      "epoch:25 step:24288 [D loss: 0.656622, acc: 55.47%] [G loss: 1.768382]\n",
      "epoch:25 step:24289 [D loss: 0.702643, acc: 52.34%] [G loss: 1.804407]\n",
      "epoch:25 step:24290 [D loss: 0.739243, acc: 53.91%] [G loss: 1.816234]\n",
      "epoch:25 step:24291 [D loss: 0.657597, acc: 60.94%] [G loss: 1.803021]\n",
      "epoch:25 step:24292 [D loss: 0.686122, acc: 53.12%] [G loss: 1.637003]\n",
      "epoch:25 step:24293 [D loss: 0.689914, acc: 55.47%] [G loss: 1.733044]\n",
      "epoch:25 step:24294 [D loss: 0.618363, acc: 71.09%] [G loss: 1.794533]\n",
      "epoch:25 step:24295 [D loss: 0.679245, acc: 53.91%] [G loss: 1.766879]\n",
      "epoch:25 step:24296 [D loss: 0.677854, acc: 55.47%] [G loss: 1.708271]\n",
      "epoch:25 step:24297 [D loss: 0.650488, acc: 63.28%] [G loss: 1.699124]\n",
      "epoch:25 step:24298 [D loss: 0.667456, acc: 60.94%] [G loss: 1.675941]\n",
      "epoch:25 step:24299 [D loss: 0.658261, acc: 60.16%] [G loss: 1.818953]\n",
      "epoch:25 step:24300 [D loss: 0.669686, acc: 57.03%] [G loss: 1.912009]\n",
      "epoch:25 step:24301 [D loss: 0.635397, acc: 66.41%] [G loss: 1.779282]\n",
      "epoch:25 step:24302 [D loss: 0.641114, acc: 61.72%] [G loss: 1.750357]\n",
      "epoch:25 step:24303 [D loss: 0.651933, acc: 65.62%] [G loss: 1.827738]\n",
      "epoch:25 step:24304 [D loss: 0.661725, acc: 60.94%] [G loss: 1.906061]\n",
      "epoch:25 step:24305 [D loss: 0.623903, acc: 64.84%] [G loss: 1.790307]\n",
      "epoch:25 step:24306 [D loss: 0.637545, acc: 67.97%] [G loss: 1.861728]\n",
      "epoch:25 step:24307 [D loss: 0.638317, acc: 60.16%] [G loss: 1.825219]\n",
      "epoch:25 step:24308 [D loss: 0.651604, acc: 60.16%] [G loss: 1.847689]\n",
      "epoch:25 step:24309 [D loss: 0.641188, acc: 64.06%] [G loss: 1.997707]\n",
      "epoch:25 step:24310 [D loss: 0.612731, acc: 64.84%] [G loss: 1.867975]\n",
      "epoch:25 step:24311 [D loss: 0.670201, acc: 57.81%] [G loss: 1.934942]\n",
      "epoch:25 step:24312 [D loss: 0.650901, acc: 62.50%] [G loss: 1.797269]\n",
      "epoch:25 step:24313 [D loss: 0.685179, acc: 60.16%] [G loss: 1.758549]\n",
      "epoch:25 step:24314 [D loss: 0.652122, acc: 65.62%] [G loss: 1.810648]\n",
      "epoch:25 step:24315 [D loss: 0.618847, acc: 67.97%] [G loss: 1.954351]\n",
      "epoch:25 step:24316 [D loss: 0.653806, acc: 61.72%] [G loss: 1.871343]\n",
      "epoch:25 step:24317 [D loss: 0.671524, acc: 57.03%] [G loss: 1.785113]\n",
      "epoch:25 step:24318 [D loss: 0.659201, acc: 63.28%] [G loss: 1.968628]\n",
      "epoch:25 step:24319 [D loss: 0.583873, acc: 71.09%] [G loss: 1.848423]\n",
      "epoch:25 step:24320 [D loss: 0.668136, acc: 61.72%] [G loss: 1.821096]\n",
      "epoch:25 step:24321 [D loss: 0.684997, acc: 56.25%] [G loss: 1.877435]\n",
      "epoch:25 step:24322 [D loss: 0.625481, acc: 63.28%] [G loss: 1.861143]\n",
      "epoch:25 step:24323 [D loss: 0.627221, acc: 63.28%] [G loss: 1.822559]\n",
      "epoch:25 step:24324 [D loss: 0.608667, acc: 67.97%] [G loss: 1.928598]\n",
      "epoch:25 step:24325 [D loss: 0.649566, acc: 63.28%] [G loss: 1.834670]\n",
      "epoch:25 step:24326 [D loss: 0.661504, acc: 58.59%] [G loss: 1.755207]\n",
      "epoch:25 step:24327 [D loss: 0.659640, acc: 60.16%] [G loss: 1.832214]\n",
      "epoch:25 step:24328 [D loss: 0.664042, acc: 59.38%] [G loss: 1.861651]\n",
      "epoch:25 step:24329 [D loss: 0.606143, acc: 65.62%] [G loss: 1.924627]\n",
      "epoch:25 step:24330 [D loss: 0.663126, acc: 64.84%] [G loss: 2.011384]\n",
      "epoch:25 step:24331 [D loss: 0.614288, acc: 64.84%] [G loss: 1.980789]\n",
      "epoch:25 step:24332 [D loss: 0.671860, acc: 62.50%] [G loss: 1.914502]\n",
      "epoch:25 step:24333 [D loss: 0.612066, acc: 70.31%] [G loss: 1.887434]\n",
      "epoch:25 step:24334 [D loss: 0.623083, acc: 62.50%] [G loss: 2.014477]\n",
      "epoch:25 step:24335 [D loss: 0.645144, acc: 60.16%] [G loss: 1.855771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:24336 [D loss: 0.618845, acc: 63.28%] [G loss: 2.000849]\n",
      "epoch:25 step:24337 [D loss: 0.617864, acc: 67.19%] [G loss: 2.109355]\n",
      "epoch:25 step:24338 [D loss: 0.739979, acc: 52.34%] [G loss: 1.793797]\n",
      "epoch:25 step:24339 [D loss: 0.668872, acc: 57.03%] [G loss: 1.774734]\n",
      "epoch:25 step:24340 [D loss: 0.618696, acc: 67.19%] [G loss: 1.808200]\n",
      "epoch:25 step:24341 [D loss: 0.599632, acc: 65.62%] [G loss: 2.100048]\n",
      "epoch:25 step:24342 [D loss: 0.665154, acc: 61.72%] [G loss: 1.846518]\n",
      "epoch:25 step:24343 [D loss: 0.582592, acc: 68.75%] [G loss: 1.989419]\n",
      "epoch:25 step:24344 [D loss: 0.615675, acc: 62.50%] [G loss: 2.185256]\n",
      "epoch:25 step:24345 [D loss: 0.680085, acc: 59.38%] [G loss: 1.837631]\n",
      "epoch:25 step:24346 [D loss: 0.647183, acc: 59.38%] [G loss: 1.807802]\n",
      "epoch:25 step:24347 [D loss: 0.694079, acc: 52.34%] [G loss: 2.024286]\n",
      "epoch:25 step:24348 [D loss: 0.579110, acc: 69.53%] [G loss: 2.067118]\n",
      "epoch:25 step:24349 [D loss: 0.606867, acc: 67.19%] [G loss: 2.146343]\n",
      "epoch:25 step:24350 [D loss: 0.634912, acc: 65.62%] [G loss: 2.154346]\n",
      "epoch:25 step:24351 [D loss: 0.676477, acc: 67.19%] [G loss: 2.011754]\n",
      "epoch:25 step:24352 [D loss: 0.667033, acc: 59.38%] [G loss: 1.985852]\n",
      "epoch:25 step:24353 [D loss: 0.744797, acc: 47.66%] [G loss: 1.695825]\n",
      "epoch:25 step:24354 [D loss: 0.765194, acc: 48.44%] [G loss: 1.861493]\n",
      "epoch:25 step:24355 [D loss: 0.557293, acc: 76.56%] [G loss: 1.965207]\n",
      "epoch:25 step:24356 [D loss: 0.615550, acc: 64.06%] [G loss: 2.015004]\n",
      "epoch:25 step:24357 [D loss: 0.633460, acc: 65.62%] [G loss: 1.857509]\n",
      "epoch:25 step:24358 [D loss: 0.664910, acc: 60.94%] [G loss: 1.872059]\n",
      "epoch:25 step:24359 [D loss: 0.641568, acc: 64.06%] [G loss: 1.858488]\n",
      "epoch:25 step:24360 [D loss: 0.621209, acc: 66.41%] [G loss: 1.952643]\n",
      "epoch:25 step:24361 [D loss: 0.643399, acc: 64.06%] [G loss: 2.056633]\n",
      "epoch:25 step:24362 [D loss: 0.650366, acc: 56.25%] [G loss: 2.315806]\n",
      "epoch:26 step:24363 [D loss: 0.636424, acc: 64.84%] [G loss: 1.816951]\n",
      "epoch:26 step:24364 [D loss: 0.700942, acc: 62.50%] [G loss: 1.946057]\n",
      "epoch:26 step:24365 [D loss: 0.699383, acc: 57.03%] [G loss: 1.838649]\n",
      "epoch:26 step:24366 [D loss: 0.642289, acc: 61.72%] [G loss: 1.762861]\n",
      "epoch:26 step:24367 [D loss: 0.635530, acc: 63.28%] [G loss: 1.811854]\n",
      "epoch:26 step:24368 [D loss: 0.661004, acc: 63.28%] [G loss: 1.852313]\n",
      "epoch:26 step:24369 [D loss: 0.651145, acc: 64.06%] [G loss: 1.854345]\n",
      "epoch:26 step:24370 [D loss: 0.653892, acc: 64.84%] [G loss: 1.875974]\n",
      "epoch:26 step:24371 [D loss: 0.637022, acc: 65.62%] [G loss: 1.977427]\n",
      "epoch:26 step:24372 [D loss: 0.630323, acc: 61.72%] [G loss: 2.131365]\n",
      "epoch:26 step:24373 [D loss: 0.639699, acc: 60.94%] [G loss: 2.004843]\n",
      "epoch:26 step:24374 [D loss: 0.603584, acc: 73.44%] [G loss: 1.829505]\n",
      "epoch:26 step:24375 [D loss: 0.633259, acc: 65.62%] [G loss: 1.861573]\n",
      "epoch:26 step:24376 [D loss: 0.686789, acc: 61.72%] [G loss: 1.900370]\n",
      "epoch:26 step:24377 [D loss: 0.617516, acc: 65.62%] [G loss: 2.019622]\n",
      "epoch:26 step:24378 [D loss: 0.618747, acc: 64.06%] [G loss: 2.041881]\n",
      "epoch:26 step:24379 [D loss: 0.639931, acc: 65.62%] [G loss: 1.988426]\n",
      "epoch:26 step:24380 [D loss: 0.642726, acc: 68.75%] [G loss: 1.963739]\n",
      "epoch:26 step:24381 [D loss: 0.669285, acc: 61.72%] [G loss: 1.885016]\n",
      "epoch:26 step:24382 [D loss: 0.696446, acc: 60.16%] [G loss: 1.619512]\n",
      "epoch:26 step:24383 [D loss: 0.685974, acc: 60.94%] [G loss: 1.752525]\n",
      "epoch:26 step:24384 [D loss: 0.633196, acc: 64.84%] [G loss: 1.725948]\n",
      "epoch:26 step:24385 [D loss: 0.661565, acc: 62.50%] [G loss: 1.868207]\n",
      "epoch:26 step:24386 [D loss: 0.652074, acc: 61.72%] [G loss: 1.930537]\n",
      "epoch:26 step:24387 [D loss: 0.619061, acc: 70.31%] [G loss: 1.886408]\n",
      "epoch:26 step:24388 [D loss: 0.630300, acc: 64.84%] [G loss: 1.825928]\n",
      "epoch:26 step:24389 [D loss: 0.683690, acc: 58.59%] [G loss: 1.839411]\n",
      "epoch:26 step:24390 [D loss: 0.638915, acc: 64.06%] [G loss: 1.861670]\n",
      "epoch:26 step:24391 [D loss: 0.618985, acc: 69.53%] [G loss: 1.822016]\n",
      "epoch:26 step:24392 [D loss: 0.636823, acc: 62.50%] [G loss: 1.845228]\n",
      "epoch:26 step:24393 [D loss: 0.643098, acc: 64.06%] [G loss: 1.698901]\n",
      "epoch:26 step:24394 [D loss: 0.707496, acc: 45.31%] [G loss: 1.745185]\n",
      "epoch:26 step:24395 [D loss: 0.657804, acc: 57.81%] [G loss: 1.731617]\n",
      "epoch:26 step:24396 [D loss: 0.626851, acc: 68.75%] [G loss: 1.872158]\n",
      "epoch:26 step:24397 [D loss: 0.590061, acc: 66.41%] [G loss: 1.900886]\n",
      "epoch:26 step:24398 [D loss: 0.654947, acc: 60.16%] [G loss: 1.949846]\n",
      "epoch:26 step:24399 [D loss: 0.664399, acc: 65.62%] [G loss: 1.821079]\n",
      "epoch:26 step:24400 [D loss: 0.648522, acc: 58.59%] [G loss: 1.997606]\n",
      "epoch:26 step:24401 [D loss: 0.673236, acc: 58.59%] [G loss: 1.873557]\n",
      "epoch:26 step:24402 [D loss: 0.615532, acc: 64.06%] [G loss: 2.009645]\n",
      "epoch:26 step:24403 [D loss: 0.619059, acc: 67.97%] [G loss: 1.810312]\n",
      "epoch:26 step:24404 [D loss: 0.578458, acc: 70.31%] [G loss: 2.006933]\n",
      "epoch:26 step:24405 [D loss: 0.642717, acc: 61.72%] [G loss: 1.921937]\n",
      "epoch:26 step:24406 [D loss: 0.698352, acc: 57.03%] [G loss: 1.994363]\n",
      "epoch:26 step:24407 [D loss: 0.635867, acc: 65.62%] [G loss: 1.887363]\n",
      "epoch:26 step:24408 [D loss: 0.657535, acc: 63.28%] [G loss: 1.764287]\n",
      "epoch:26 step:24409 [D loss: 0.619742, acc: 63.28%] [G loss: 1.922759]\n",
      "epoch:26 step:24410 [D loss: 0.681276, acc: 60.16%] [G loss: 1.965058]\n",
      "epoch:26 step:24411 [D loss: 0.636396, acc: 60.94%] [G loss: 1.839949]\n",
      "epoch:26 step:24412 [D loss: 0.622482, acc: 64.06%] [G loss: 1.854350]\n",
      "epoch:26 step:24413 [D loss: 0.673961, acc: 64.06%] [G loss: 1.804363]\n",
      "epoch:26 step:24414 [D loss: 0.661801, acc: 57.03%] [G loss: 1.903420]\n",
      "epoch:26 step:24415 [D loss: 0.654324, acc: 60.94%] [G loss: 1.927297]\n",
      "epoch:26 step:24416 [D loss: 0.586220, acc: 65.62%] [G loss: 1.927445]\n",
      "epoch:26 step:24417 [D loss: 0.637100, acc: 60.94%] [G loss: 2.008670]\n",
      "epoch:26 step:24418 [D loss: 0.655859, acc: 67.97%] [G loss: 1.924931]\n",
      "epoch:26 step:24419 [D loss: 0.643578, acc: 66.41%] [G loss: 1.947763]\n",
      "epoch:26 step:24420 [D loss: 0.670964, acc: 62.50%] [G loss: 1.762124]\n",
      "epoch:26 step:24421 [D loss: 0.686016, acc: 61.72%] [G loss: 1.829798]\n",
      "epoch:26 step:24422 [D loss: 0.648837, acc: 59.38%] [G loss: 1.766130]\n",
      "epoch:26 step:24423 [D loss: 0.697570, acc: 54.69%] [G loss: 1.755836]\n",
      "epoch:26 step:24424 [D loss: 0.657301, acc: 63.28%] [G loss: 1.920117]\n",
      "epoch:26 step:24425 [D loss: 0.665000, acc: 60.16%] [G loss: 1.906000]\n",
      "epoch:26 step:24426 [D loss: 0.588565, acc: 64.84%] [G loss: 2.046150]\n",
      "epoch:26 step:24427 [D loss: 0.637155, acc: 62.50%] [G loss: 1.851429]\n",
      "epoch:26 step:24428 [D loss: 0.642186, acc: 60.16%] [G loss: 1.839219]\n",
      "epoch:26 step:24429 [D loss: 0.665602, acc: 66.41%] [G loss: 1.901901]\n",
      "epoch:26 step:24430 [D loss: 0.613492, acc: 69.53%] [G loss: 1.910448]\n",
      "epoch:26 step:24431 [D loss: 0.639138, acc: 65.62%] [G loss: 1.964774]\n",
      "epoch:26 step:24432 [D loss: 0.639424, acc: 63.28%] [G loss: 1.792577]\n",
      "epoch:26 step:24433 [D loss: 0.648953, acc: 66.41%] [G loss: 1.849441]\n",
      "epoch:26 step:24434 [D loss: 0.650518, acc: 64.06%] [G loss: 1.820293]\n",
      "epoch:26 step:24435 [D loss: 0.709302, acc: 52.34%] [G loss: 1.730452]\n",
      "epoch:26 step:24436 [D loss: 0.588404, acc: 69.53%] [G loss: 1.858498]\n",
      "epoch:26 step:24437 [D loss: 0.654979, acc: 64.06%] [G loss: 1.917356]\n",
      "epoch:26 step:24438 [D loss: 0.606116, acc: 71.09%] [G loss: 2.014355]\n",
      "epoch:26 step:24439 [D loss: 0.633981, acc: 64.06%] [G loss: 1.949371]\n",
      "epoch:26 step:24440 [D loss: 0.692868, acc: 54.69%] [G loss: 1.751660]\n",
      "epoch:26 step:24441 [D loss: 0.637264, acc: 60.94%] [G loss: 1.714551]\n",
      "epoch:26 step:24442 [D loss: 0.637675, acc: 60.16%] [G loss: 1.783705]\n",
      "epoch:26 step:24443 [D loss: 0.686874, acc: 53.91%] [G loss: 1.678363]\n",
      "epoch:26 step:24444 [D loss: 0.678416, acc: 61.72%] [G loss: 1.729162]\n",
      "epoch:26 step:24445 [D loss: 0.659583, acc: 57.81%] [G loss: 1.702812]\n",
      "epoch:26 step:24446 [D loss: 0.626605, acc: 61.72%] [G loss: 1.846442]\n",
      "epoch:26 step:24447 [D loss: 0.666269, acc: 60.94%] [G loss: 1.787307]\n",
      "epoch:26 step:24448 [D loss: 0.676133, acc: 60.94%] [G loss: 1.841366]\n",
      "epoch:26 step:24449 [D loss: 0.689324, acc: 60.94%] [G loss: 1.799256]\n",
      "epoch:26 step:24450 [D loss: 0.629186, acc: 65.62%] [G loss: 1.874069]\n",
      "epoch:26 step:24451 [D loss: 0.647287, acc: 62.50%] [G loss: 1.888704]\n",
      "epoch:26 step:24452 [D loss: 0.678174, acc: 53.91%] [G loss: 1.802691]\n",
      "epoch:26 step:24453 [D loss: 0.642421, acc: 64.84%] [G loss: 1.836537]\n",
      "epoch:26 step:24454 [D loss: 0.627827, acc: 65.62%] [G loss: 2.018210]\n",
      "epoch:26 step:24455 [D loss: 0.587388, acc: 70.31%] [G loss: 1.995541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24456 [D loss: 0.638854, acc: 64.84%] [G loss: 1.954042]\n",
      "epoch:26 step:24457 [D loss: 0.686661, acc: 57.81%] [G loss: 1.741119]\n",
      "epoch:26 step:24458 [D loss: 0.632149, acc: 64.06%] [G loss: 1.929684]\n",
      "epoch:26 step:24459 [D loss: 0.687728, acc: 55.47%] [G loss: 1.855483]\n",
      "epoch:26 step:24460 [D loss: 0.681111, acc: 57.03%] [G loss: 1.693677]\n",
      "epoch:26 step:24461 [D loss: 0.696565, acc: 54.69%] [G loss: 1.764375]\n",
      "epoch:26 step:24462 [D loss: 0.640022, acc: 63.28%] [G loss: 1.899099]\n",
      "epoch:26 step:24463 [D loss: 0.613947, acc: 64.06%] [G loss: 1.776402]\n",
      "epoch:26 step:24464 [D loss: 0.624560, acc: 65.62%] [G loss: 1.909361]\n",
      "epoch:26 step:24465 [D loss: 0.642513, acc: 61.72%] [G loss: 1.901719]\n",
      "epoch:26 step:24466 [D loss: 0.651348, acc: 64.06%] [G loss: 1.866728]\n",
      "epoch:26 step:24467 [D loss: 0.682201, acc: 58.59%] [G loss: 1.963784]\n",
      "epoch:26 step:24468 [D loss: 0.577991, acc: 67.97%] [G loss: 2.020896]\n",
      "epoch:26 step:24469 [D loss: 0.608971, acc: 66.41%] [G loss: 2.132730]\n",
      "epoch:26 step:24470 [D loss: 0.687264, acc: 57.03%] [G loss: 1.819799]\n",
      "epoch:26 step:24471 [D loss: 0.660210, acc: 59.38%] [G loss: 1.698280]\n",
      "epoch:26 step:24472 [D loss: 0.630763, acc: 67.19%] [G loss: 1.796548]\n",
      "epoch:26 step:24473 [D loss: 0.619087, acc: 66.41%] [G loss: 1.820510]\n",
      "epoch:26 step:24474 [D loss: 0.632617, acc: 62.50%] [G loss: 2.091940]\n",
      "epoch:26 step:24475 [D loss: 0.634424, acc: 63.28%] [G loss: 2.142422]\n",
      "epoch:26 step:24476 [D loss: 0.632560, acc: 67.97%] [G loss: 2.037801]\n",
      "epoch:26 step:24477 [D loss: 0.622435, acc: 61.72%] [G loss: 2.122124]\n",
      "epoch:26 step:24478 [D loss: 0.564570, acc: 67.97%] [G loss: 2.096770]\n",
      "epoch:26 step:24479 [D loss: 0.648498, acc: 60.16%] [G loss: 2.123309]\n",
      "epoch:26 step:24480 [D loss: 0.672295, acc: 62.50%] [G loss: 2.026881]\n",
      "epoch:26 step:24481 [D loss: 0.593164, acc: 67.97%] [G loss: 2.127966]\n",
      "epoch:26 step:24482 [D loss: 0.648741, acc: 57.03%] [G loss: 1.870566]\n",
      "epoch:26 step:24483 [D loss: 0.675607, acc: 57.81%] [G loss: 1.959950]\n",
      "epoch:26 step:24484 [D loss: 0.626120, acc: 59.38%] [G loss: 1.975781]\n",
      "epoch:26 step:24485 [D loss: 0.662248, acc: 60.94%] [G loss: 1.959621]\n",
      "epoch:26 step:24486 [D loss: 0.702613, acc: 54.69%] [G loss: 1.797229]\n",
      "epoch:26 step:24487 [D loss: 0.697302, acc: 55.47%] [G loss: 1.690693]\n",
      "epoch:26 step:24488 [D loss: 0.613142, acc: 69.53%] [G loss: 1.884107]\n",
      "epoch:26 step:24489 [D loss: 0.677548, acc: 58.59%] [G loss: 1.786381]\n",
      "epoch:26 step:24490 [D loss: 0.665934, acc: 60.94%] [G loss: 1.780709]\n",
      "epoch:26 step:24491 [D loss: 0.683698, acc: 59.38%] [G loss: 1.808931]\n",
      "epoch:26 step:24492 [D loss: 0.629034, acc: 62.50%] [G loss: 1.910489]\n",
      "epoch:26 step:24493 [D loss: 0.631976, acc: 71.09%] [G loss: 2.022070]\n",
      "epoch:26 step:24494 [D loss: 0.656665, acc: 61.72%] [G loss: 1.816636]\n",
      "epoch:26 step:24495 [D loss: 0.683359, acc: 54.69%] [G loss: 1.796284]\n",
      "epoch:26 step:24496 [D loss: 0.633503, acc: 63.28%] [G loss: 1.771672]\n",
      "epoch:26 step:24497 [D loss: 0.678819, acc: 57.81%] [G loss: 1.781235]\n",
      "epoch:26 step:24498 [D loss: 0.648005, acc: 66.41%] [G loss: 1.814581]\n",
      "epoch:26 step:24499 [D loss: 0.679812, acc: 51.56%] [G loss: 1.754732]\n",
      "epoch:26 step:24500 [D loss: 0.653312, acc: 60.94%] [G loss: 1.791011]\n",
      "epoch:26 step:24501 [D loss: 0.691902, acc: 58.59%] [G loss: 1.741483]\n",
      "epoch:26 step:24502 [D loss: 0.695894, acc: 53.12%] [G loss: 1.773579]\n",
      "epoch:26 step:24503 [D loss: 0.654450, acc: 59.38%] [G loss: 1.763088]\n",
      "epoch:26 step:24504 [D loss: 0.690271, acc: 58.59%] [G loss: 1.645436]\n",
      "epoch:26 step:24505 [D loss: 0.682412, acc: 57.03%] [G loss: 1.856860]\n",
      "epoch:26 step:24506 [D loss: 0.653020, acc: 63.28%] [G loss: 1.788698]\n",
      "epoch:26 step:24507 [D loss: 0.673265, acc: 60.16%] [G loss: 1.889878]\n",
      "epoch:26 step:24508 [D loss: 0.626827, acc: 61.72%] [G loss: 1.929674]\n",
      "epoch:26 step:24509 [D loss: 0.685611, acc: 54.69%] [G loss: 1.728712]\n",
      "epoch:26 step:24510 [D loss: 0.670354, acc: 64.84%] [G loss: 1.787465]\n",
      "epoch:26 step:24511 [D loss: 0.629677, acc: 64.84%] [G loss: 1.863327]\n",
      "epoch:26 step:24512 [D loss: 0.665818, acc: 57.03%] [G loss: 1.830849]\n",
      "epoch:26 step:24513 [D loss: 0.625315, acc: 62.50%] [G loss: 1.844343]\n",
      "epoch:26 step:24514 [D loss: 0.647556, acc: 63.28%] [G loss: 1.967244]\n",
      "epoch:26 step:24515 [D loss: 0.661122, acc: 60.16%] [G loss: 1.831804]\n",
      "epoch:26 step:24516 [D loss: 0.678083, acc: 62.50%] [G loss: 1.872847]\n",
      "epoch:26 step:24517 [D loss: 0.635845, acc: 58.59%] [G loss: 1.909839]\n",
      "epoch:26 step:24518 [D loss: 0.639169, acc: 64.06%] [G loss: 1.936478]\n",
      "epoch:26 step:24519 [D loss: 0.658952, acc: 64.06%] [G loss: 1.823781]\n",
      "epoch:26 step:24520 [D loss: 0.669738, acc: 59.38%] [G loss: 1.880117]\n",
      "epoch:26 step:24521 [D loss: 0.596140, acc: 71.09%] [G loss: 1.965786]\n",
      "epoch:26 step:24522 [D loss: 0.670408, acc: 60.16%] [G loss: 1.735196]\n",
      "epoch:26 step:24523 [D loss: 0.681848, acc: 54.69%] [G loss: 1.757115]\n",
      "epoch:26 step:24524 [D loss: 0.623239, acc: 63.28%] [G loss: 1.853322]\n",
      "epoch:26 step:24525 [D loss: 0.662149, acc: 57.81%] [G loss: 1.861250]\n",
      "epoch:26 step:24526 [D loss: 0.616092, acc: 70.31%] [G loss: 1.826216]\n",
      "epoch:26 step:24527 [D loss: 0.647288, acc: 63.28%] [G loss: 1.868529]\n",
      "epoch:26 step:24528 [D loss: 0.706264, acc: 53.12%] [G loss: 1.844608]\n",
      "epoch:26 step:24529 [D loss: 0.668845, acc: 57.81%] [G loss: 1.863030]\n",
      "epoch:26 step:24530 [D loss: 0.638760, acc: 59.38%] [G loss: 1.957219]\n",
      "epoch:26 step:24531 [D loss: 0.645483, acc: 66.41%] [G loss: 1.787385]\n",
      "epoch:26 step:24532 [D loss: 0.691654, acc: 56.25%] [G loss: 1.789674]\n",
      "epoch:26 step:24533 [D loss: 0.651308, acc: 62.50%] [G loss: 1.856238]\n",
      "epoch:26 step:24534 [D loss: 0.655762, acc: 62.50%] [G loss: 1.790112]\n",
      "epoch:26 step:24535 [D loss: 0.653535, acc: 63.28%] [G loss: 1.759548]\n",
      "epoch:26 step:24536 [D loss: 0.642673, acc: 64.84%] [G loss: 1.704219]\n",
      "epoch:26 step:24537 [D loss: 0.654850, acc: 57.03%] [G loss: 1.748352]\n",
      "epoch:26 step:24538 [D loss: 0.684279, acc: 55.47%] [G loss: 1.830197]\n",
      "epoch:26 step:24539 [D loss: 0.648370, acc: 64.06%] [G loss: 1.727192]\n",
      "epoch:26 step:24540 [D loss: 0.650442, acc: 67.19%] [G loss: 1.780648]\n",
      "epoch:26 step:24541 [D loss: 0.649631, acc: 64.84%] [G loss: 1.774918]\n",
      "epoch:26 step:24542 [D loss: 0.636874, acc: 61.72%] [G loss: 1.900998]\n",
      "epoch:26 step:24543 [D loss: 0.676880, acc: 58.59%] [G loss: 1.758502]\n",
      "epoch:26 step:24544 [D loss: 0.653081, acc: 64.06%] [G loss: 1.723134]\n",
      "epoch:26 step:24545 [D loss: 0.675064, acc: 62.50%] [G loss: 1.769893]\n",
      "epoch:26 step:24546 [D loss: 0.657455, acc: 57.81%] [G loss: 1.749103]\n",
      "epoch:26 step:24547 [D loss: 0.633786, acc: 58.59%] [G loss: 1.801535]\n",
      "epoch:26 step:24548 [D loss: 0.667354, acc: 68.75%] [G loss: 1.804912]\n",
      "epoch:26 step:24549 [D loss: 0.650098, acc: 61.72%] [G loss: 1.930912]\n",
      "epoch:26 step:24550 [D loss: 0.604898, acc: 68.75%] [G loss: 1.788004]\n",
      "epoch:26 step:24551 [D loss: 0.619952, acc: 69.53%] [G loss: 1.688062]\n",
      "epoch:26 step:24552 [D loss: 0.619162, acc: 63.28%] [G loss: 1.820903]\n",
      "epoch:26 step:24553 [D loss: 0.668831, acc: 57.81%] [G loss: 1.938646]\n",
      "epoch:26 step:24554 [D loss: 0.614897, acc: 64.84%] [G loss: 2.011181]\n",
      "epoch:26 step:24555 [D loss: 0.647552, acc: 57.81%] [G loss: 1.937743]\n",
      "epoch:26 step:24556 [D loss: 0.602804, acc: 69.53%] [G loss: 1.923445]\n",
      "epoch:26 step:24557 [D loss: 0.670545, acc: 60.94%] [G loss: 1.886337]\n",
      "epoch:26 step:24558 [D loss: 0.658937, acc: 64.06%] [G loss: 1.875665]\n",
      "epoch:26 step:24559 [D loss: 0.635377, acc: 67.97%] [G loss: 1.902540]\n",
      "epoch:26 step:24560 [D loss: 0.610526, acc: 73.44%] [G loss: 1.987771]\n",
      "epoch:26 step:24561 [D loss: 0.671510, acc: 64.06%] [G loss: 2.016456]\n",
      "epoch:26 step:24562 [D loss: 0.698678, acc: 57.03%] [G loss: 1.771679]\n",
      "epoch:26 step:24563 [D loss: 0.608733, acc: 67.19%] [G loss: 1.990614]\n",
      "epoch:26 step:24564 [D loss: 0.676877, acc: 57.03%] [G loss: 1.958698]\n",
      "epoch:26 step:24565 [D loss: 0.644649, acc: 68.75%] [G loss: 1.819419]\n",
      "epoch:26 step:24566 [D loss: 0.652672, acc: 60.16%] [G loss: 1.943620]\n",
      "epoch:26 step:24567 [D loss: 0.668403, acc: 63.28%] [G loss: 1.898401]\n",
      "epoch:26 step:24568 [D loss: 0.685381, acc: 60.16%] [G loss: 2.009239]\n",
      "epoch:26 step:24569 [D loss: 0.626313, acc: 64.06%] [G loss: 1.996449]\n",
      "epoch:26 step:24570 [D loss: 0.658823, acc: 59.38%] [G loss: 2.031433]\n",
      "epoch:26 step:24571 [D loss: 0.574906, acc: 66.41%] [G loss: 2.049679]\n",
      "epoch:26 step:24572 [D loss: 0.644672, acc: 59.38%] [G loss: 1.869479]\n",
      "epoch:26 step:24573 [D loss: 0.717309, acc: 54.69%] [G loss: 1.877939]\n",
      "epoch:26 step:24574 [D loss: 0.672762, acc: 58.59%] [G loss: 1.780295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24575 [D loss: 0.710824, acc: 52.34%] [G loss: 1.907597]\n",
      "epoch:26 step:24576 [D loss: 0.610065, acc: 65.62%] [G loss: 1.882762]\n",
      "epoch:26 step:24577 [D loss: 0.681267, acc: 59.38%] [G loss: 1.848934]\n",
      "epoch:26 step:24578 [D loss: 0.638102, acc: 60.94%] [G loss: 1.878897]\n",
      "epoch:26 step:24579 [D loss: 0.635318, acc: 63.28%] [G loss: 1.909829]\n",
      "epoch:26 step:24580 [D loss: 0.606188, acc: 71.09%] [G loss: 2.174220]\n",
      "epoch:26 step:24581 [D loss: 0.615269, acc: 63.28%] [G loss: 1.956929]\n",
      "epoch:26 step:24582 [D loss: 0.699695, acc: 53.12%] [G loss: 1.772443]\n",
      "epoch:26 step:24583 [D loss: 0.687699, acc: 62.50%] [G loss: 1.808694]\n",
      "epoch:26 step:24584 [D loss: 0.653149, acc: 60.94%] [G loss: 1.870045]\n",
      "epoch:26 step:24585 [D loss: 0.604046, acc: 67.19%] [G loss: 1.973772]\n",
      "epoch:26 step:24586 [D loss: 0.677559, acc: 57.81%] [G loss: 1.735734]\n",
      "epoch:26 step:24587 [D loss: 0.643906, acc: 57.03%] [G loss: 1.851863]\n",
      "epoch:26 step:24588 [D loss: 0.641850, acc: 67.19%] [G loss: 1.825248]\n",
      "epoch:26 step:24589 [D loss: 0.623734, acc: 63.28%] [G loss: 1.799827]\n",
      "epoch:26 step:24590 [D loss: 0.682529, acc: 56.25%] [G loss: 1.709525]\n",
      "epoch:26 step:24591 [D loss: 0.587316, acc: 67.97%] [G loss: 1.939042]\n",
      "epoch:26 step:24592 [D loss: 0.610623, acc: 64.84%] [G loss: 2.118920]\n",
      "epoch:26 step:24593 [D loss: 0.578023, acc: 68.75%] [G loss: 2.353110]\n",
      "epoch:26 step:24594 [D loss: 0.590722, acc: 71.88%] [G loss: 2.384065]\n",
      "epoch:26 step:24595 [D loss: 0.648545, acc: 64.06%] [G loss: 1.956453]\n",
      "epoch:26 step:24596 [D loss: 0.735637, acc: 50.78%] [G loss: 1.838478]\n",
      "epoch:26 step:24597 [D loss: 0.640670, acc: 63.28%] [G loss: 1.845913]\n",
      "epoch:26 step:24598 [D loss: 0.637275, acc: 66.41%] [G loss: 1.939060]\n",
      "epoch:26 step:24599 [D loss: 0.674116, acc: 57.81%] [G loss: 1.878307]\n",
      "epoch:26 step:24600 [D loss: 0.668167, acc: 60.16%] [G loss: 1.931451]\n",
      "epoch:26 step:24601 [D loss: 0.612043, acc: 67.97%] [G loss: 1.875088]\n",
      "epoch:26 step:24602 [D loss: 0.673961, acc: 55.47%] [G loss: 1.696782]\n",
      "epoch:26 step:24603 [D loss: 0.622919, acc: 67.19%] [G loss: 2.038141]\n",
      "epoch:26 step:24604 [D loss: 0.671450, acc: 67.19%] [G loss: 1.886213]\n",
      "epoch:26 step:24605 [D loss: 0.616361, acc: 64.84%] [G loss: 2.028073]\n",
      "epoch:26 step:24606 [D loss: 0.624324, acc: 64.06%] [G loss: 1.916933]\n",
      "epoch:26 step:24607 [D loss: 0.625450, acc: 67.19%] [G loss: 1.853957]\n",
      "epoch:26 step:24608 [D loss: 0.718888, acc: 57.03%] [G loss: 1.987478]\n",
      "epoch:26 step:24609 [D loss: 0.593437, acc: 71.88%] [G loss: 1.838164]\n",
      "epoch:26 step:24610 [D loss: 0.607572, acc: 65.62%] [G loss: 2.051153]\n",
      "epoch:26 step:24611 [D loss: 0.685481, acc: 53.12%] [G loss: 1.830287]\n",
      "epoch:26 step:24612 [D loss: 0.739618, acc: 48.44%] [G loss: 1.709054]\n",
      "epoch:26 step:24613 [D loss: 0.665101, acc: 62.50%] [G loss: 1.657119]\n",
      "epoch:26 step:24614 [D loss: 0.671495, acc: 56.25%] [G loss: 1.742723]\n",
      "epoch:26 step:24615 [D loss: 0.651022, acc: 64.84%] [G loss: 1.756088]\n",
      "epoch:26 step:24616 [D loss: 0.686243, acc: 54.69%] [G loss: 1.867901]\n",
      "epoch:26 step:24617 [D loss: 0.649744, acc: 62.50%] [G loss: 1.932842]\n",
      "epoch:26 step:24618 [D loss: 0.644379, acc: 61.72%] [G loss: 1.850783]\n",
      "epoch:26 step:24619 [D loss: 0.673641, acc: 59.38%] [G loss: 1.832631]\n",
      "epoch:26 step:24620 [D loss: 0.657313, acc: 59.38%] [G loss: 1.783469]\n",
      "epoch:26 step:24621 [D loss: 0.595028, acc: 70.31%] [G loss: 1.784627]\n",
      "epoch:26 step:24622 [D loss: 0.705211, acc: 49.22%] [G loss: 1.835714]\n",
      "epoch:26 step:24623 [D loss: 0.581810, acc: 69.53%] [G loss: 1.943012]\n",
      "epoch:26 step:24624 [D loss: 0.627243, acc: 67.97%] [G loss: 1.898776]\n",
      "epoch:26 step:24625 [D loss: 0.644708, acc: 63.28%] [G loss: 1.879928]\n",
      "epoch:26 step:24626 [D loss: 0.627261, acc: 64.06%] [G loss: 2.063270]\n",
      "epoch:26 step:24627 [D loss: 0.659424, acc: 58.59%] [G loss: 1.811692]\n",
      "epoch:26 step:24628 [D loss: 0.624637, acc: 60.16%] [G loss: 1.838799]\n",
      "epoch:26 step:24629 [D loss: 0.652143, acc: 64.06%] [G loss: 1.813526]\n",
      "epoch:26 step:24630 [D loss: 0.656406, acc: 63.28%] [G loss: 1.829406]\n",
      "epoch:26 step:24631 [D loss: 0.691625, acc: 54.69%] [G loss: 1.835672]\n",
      "epoch:26 step:24632 [D loss: 0.614194, acc: 64.84%] [G loss: 1.857141]\n",
      "epoch:26 step:24633 [D loss: 0.651135, acc: 63.28%] [G loss: 1.898138]\n",
      "epoch:26 step:24634 [D loss: 0.666615, acc: 63.28%] [G loss: 1.977375]\n",
      "epoch:26 step:24635 [D loss: 0.625629, acc: 67.19%] [G loss: 1.964959]\n",
      "epoch:26 step:24636 [D loss: 0.605158, acc: 66.41%] [G loss: 2.336526]\n",
      "epoch:26 step:24637 [D loss: 0.597891, acc: 65.62%] [G loss: 2.033273]\n",
      "epoch:26 step:24638 [D loss: 0.630241, acc: 70.31%] [G loss: 2.308864]\n",
      "epoch:26 step:24639 [D loss: 0.683121, acc: 61.72%] [G loss: 1.897490]\n",
      "epoch:26 step:24640 [D loss: 0.645193, acc: 64.06%] [G loss: 1.889223]\n",
      "epoch:26 step:24641 [D loss: 0.658158, acc: 59.38%] [G loss: 1.910433]\n",
      "epoch:26 step:24642 [D loss: 0.674969, acc: 59.38%] [G loss: 1.942364]\n",
      "epoch:26 step:24643 [D loss: 0.612072, acc: 67.97%] [G loss: 1.776806]\n",
      "epoch:26 step:24644 [D loss: 0.699422, acc: 59.38%] [G loss: 1.897784]\n",
      "epoch:26 step:24645 [D loss: 0.683087, acc: 61.72%] [G loss: 1.878949]\n",
      "epoch:26 step:24646 [D loss: 0.705228, acc: 54.69%] [G loss: 1.843835]\n",
      "epoch:26 step:24647 [D loss: 0.615992, acc: 67.97%] [G loss: 1.814490]\n",
      "epoch:26 step:24648 [D loss: 0.656595, acc: 60.16%] [G loss: 1.979277]\n",
      "epoch:26 step:24649 [D loss: 0.604623, acc: 69.53%] [G loss: 1.828935]\n",
      "epoch:26 step:24650 [D loss: 0.619623, acc: 67.19%] [G loss: 1.949867]\n",
      "epoch:26 step:24651 [D loss: 0.691132, acc: 56.25%] [G loss: 1.915139]\n",
      "epoch:26 step:24652 [D loss: 0.634565, acc: 59.38%] [G loss: 1.737160]\n",
      "epoch:26 step:24653 [D loss: 0.644505, acc: 62.50%] [G loss: 1.732120]\n",
      "epoch:26 step:24654 [D loss: 0.585545, acc: 71.88%] [G loss: 1.877702]\n",
      "epoch:26 step:24655 [D loss: 0.619953, acc: 65.62%] [G loss: 1.907101]\n",
      "epoch:26 step:24656 [D loss: 0.622380, acc: 69.53%] [G loss: 1.828466]\n",
      "epoch:26 step:24657 [D loss: 0.630834, acc: 61.72%] [G loss: 1.842358]\n",
      "epoch:26 step:24658 [D loss: 0.607901, acc: 65.62%] [G loss: 1.963758]\n",
      "epoch:26 step:24659 [D loss: 0.697731, acc: 53.12%] [G loss: 1.899712]\n",
      "epoch:26 step:24660 [D loss: 0.616281, acc: 66.41%] [G loss: 1.877771]\n",
      "epoch:26 step:24661 [D loss: 0.637725, acc: 65.62%] [G loss: 1.808072]\n",
      "epoch:26 step:24662 [D loss: 0.617467, acc: 64.84%] [G loss: 1.828469]\n",
      "epoch:26 step:24663 [D loss: 0.673328, acc: 57.03%] [G loss: 1.922804]\n",
      "epoch:26 step:24664 [D loss: 0.624657, acc: 67.19%] [G loss: 1.937895]\n",
      "epoch:26 step:24665 [D loss: 0.658124, acc: 67.97%] [G loss: 1.848948]\n",
      "epoch:26 step:24666 [D loss: 0.625512, acc: 64.84%] [G loss: 1.823371]\n",
      "epoch:26 step:24667 [D loss: 0.662299, acc: 58.59%] [G loss: 1.795896]\n",
      "epoch:26 step:24668 [D loss: 0.683765, acc: 53.91%] [G loss: 1.732747]\n",
      "epoch:26 step:24669 [D loss: 0.706609, acc: 54.69%] [G loss: 1.748517]\n",
      "epoch:26 step:24670 [D loss: 0.696968, acc: 56.25%] [G loss: 1.738591]\n",
      "epoch:26 step:24671 [D loss: 0.678212, acc: 54.69%] [G loss: 1.877259]\n",
      "epoch:26 step:24672 [D loss: 0.641638, acc: 63.28%] [G loss: 1.749981]\n",
      "epoch:26 step:24673 [D loss: 0.641201, acc: 61.72%] [G loss: 1.736113]\n",
      "epoch:26 step:24674 [D loss: 0.604519, acc: 71.09%] [G loss: 2.164129]\n",
      "epoch:26 step:24675 [D loss: 0.601009, acc: 68.75%] [G loss: 2.032611]\n",
      "epoch:26 step:24676 [D loss: 0.597215, acc: 65.62%] [G loss: 2.028738]\n",
      "epoch:26 step:24677 [D loss: 0.562901, acc: 71.88%] [G loss: 2.074193]\n",
      "epoch:26 step:24678 [D loss: 0.686893, acc: 49.22%] [G loss: 1.812122]\n",
      "epoch:26 step:24679 [D loss: 0.693955, acc: 53.91%] [G loss: 1.870957]\n",
      "epoch:26 step:24680 [D loss: 0.669356, acc: 55.47%] [G loss: 1.952743]\n",
      "epoch:26 step:24681 [D loss: 0.673860, acc: 58.59%] [G loss: 1.895636]\n",
      "epoch:26 step:24682 [D loss: 0.697945, acc: 58.59%] [G loss: 1.871876]\n",
      "epoch:26 step:24683 [D loss: 0.654079, acc: 59.38%] [G loss: 1.828306]\n",
      "epoch:26 step:24684 [D loss: 0.686491, acc: 60.94%] [G loss: 1.818002]\n",
      "epoch:26 step:24685 [D loss: 0.685895, acc: 58.59%] [G loss: 1.707437]\n",
      "epoch:26 step:24686 [D loss: 0.729243, acc: 56.25%] [G loss: 1.701658]\n",
      "epoch:26 step:24687 [D loss: 0.623116, acc: 66.41%] [G loss: 1.813090]\n",
      "epoch:26 step:24688 [D loss: 0.667169, acc: 59.38%] [G loss: 1.754462]\n",
      "epoch:26 step:24689 [D loss: 0.711696, acc: 54.69%] [G loss: 1.861644]\n",
      "epoch:26 step:24690 [D loss: 0.606001, acc: 67.19%] [G loss: 1.934906]\n",
      "epoch:26 step:24691 [D loss: 0.624910, acc: 67.19%] [G loss: 1.903843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24692 [D loss: 0.679680, acc: 53.91%] [G loss: 1.999960]\n",
      "epoch:26 step:24693 [D loss: 0.644863, acc: 64.84%] [G loss: 1.881802]\n",
      "epoch:26 step:24694 [D loss: 0.606974, acc: 67.19%] [G loss: 1.968793]\n",
      "epoch:26 step:24695 [D loss: 0.639274, acc: 63.28%] [G loss: 1.910175]\n",
      "epoch:26 step:24696 [D loss: 0.653089, acc: 61.72%] [G loss: 1.792114]\n",
      "epoch:26 step:24697 [D loss: 0.615565, acc: 68.75%] [G loss: 1.935294]\n",
      "epoch:26 step:24698 [D loss: 0.660366, acc: 62.50%] [G loss: 1.884620]\n",
      "epoch:26 step:24699 [D loss: 0.739450, acc: 48.44%] [G loss: 1.734542]\n",
      "epoch:26 step:24700 [D loss: 0.603233, acc: 71.88%] [G loss: 1.942422]\n",
      "epoch:26 step:24701 [D loss: 0.657841, acc: 60.16%] [G loss: 1.871157]\n",
      "epoch:26 step:24702 [D loss: 0.635752, acc: 64.06%] [G loss: 1.941345]\n",
      "epoch:26 step:24703 [D loss: 0.717409, acc: 51.56%] [G loss: 1.662178]\n",
      "epoch:26 step:24704 [D loss: 0.645250, acc: 66.41%] [G loss: 1.848658]\n",
      "epoch:26 step:24705 [D loss: 0.625640, acc: 64.84%] [G loss: 1.879929]\n",
      "epoch:26 step:24706 [D loss: 0.655882, acc: 59.38%] [G loss: 1.927536]\n",
      "epoch:26 step:24707 [D loss: 0.643730, acc: 66.41%] [G loss: 2.009739]\n",
      "epoch:26 step:24708 [D loss: 0.592194, acc: 68.75%] [G loss: 2.007217]\n",
      "epoch:26 step:24709 [D loss: 0.547373, acc: 73.44%] [G loss: 2.181960]\n",
      "epoch:26 step:24710 [D loss: 0.722960, acc: 54.69%] [G loss: 1.846854]\n",
      "epoch:26 step:24711 [D loss: 0.647881, acc: 58.59%] [G loss: 1.738359]\n",
      "epoch:26 step:24712 [D loss: 0.668788, acc: 63.28%] [G loss: 1.879975]\n",
      "epoch:26 step:24713 [D loss: 0.646038, acc: 59.38%] [G loss: 1.759796]\n",
      "epoch:26 step:24714 [D loss: 0.696385, acc: 58.59%] [G loss: 1.687573]\n",
      "epoch:26 step:24715 [D loss: 0.671363, acc: 59.38%] [G loss: 1.885327]\n",
      "epoch:26 step:24716 [D loss: 0.618439, acc: 64.84%] [G loss: 2.134347]\n",
      "epoch:26 step:24717 [D loss: 0.688933, acc: 53.12%] [G loss: 1.636321]\n",
      "epoch:26 step:24718 [D loss: 0.670976, acc: 60.94%] [G loss: 1.767973]\n",
      "epoch:26 step:24719 [D loss: 0.619981, acc: 64.84%] [G loss: 1.889073]\n",
      "epoch:26 step:24720 [D loss: 0.671661, acc: 55.47%] [G loss: 1.718498]\n",
      "epoch:26 step:24721 [D loss: 0.617189, acc: 70.31%] [G loss: 1.964266]\n",
      "epoch:26 step:24722 [D loss: 0.633908, acc: 61.72%] [G loss: 1.958507]\n",
      "epoch:26 step:24723 [D loss: 0.670186, acc: 61.72%] [G loss: 1.812587]\n",
      "epoch:26 step:24724 [D loss: 0.663058, acc: 59.38%] [G loss: 1.836897]\n",
      "epoch:26 step:24725 [D loss: 0.694214, acc: 54.69%] [G loss: 1.826194]\n",
      "epoch:26 step:24726 [D loss: 0.645599, acc: 64.06%] [G loss: 1.875980]\n",
      "epoch:26 step:24727 [D loss: 0.627759, acc: 59.38%] [G loss: 1.908151]\n",
      "epoch:26 step:24728 [D loss: 0.667192, acc: 59.38%] [G loss: 1.874534]\n",
      "epoch:26 step:24729 [D loss: 0.669935, acc: 56.25%] [G loss: 1.924111]\n",
      "epoch:26 step:24730 [D loss: 0.646989, acc: 60.94%] [G loss: 1.760360]\n",
      "epoch:26 step:24731 [D loss: 0.613858, acc: 69.53%] [G loss: 1.894022]\n",
      "epoch:26 step:24732 [D loss: 0.654867, acc: 58.59%] [G loss: 1.927334]\n",
      "epoch:26 step:24733 [D loss: 0.590442, acc: 71.88%] [G loss: 2.088805]\n",
      "epoch:26 step:24734 [D loss: 0.646628, acc: 57.81%] [G loss: 1.882735]\n",
      "epoch:26 step:24735 [D loss: 0.705167, acc: 53.12%] [G loss: 1.800106]\n",
      "epoch:26 step:24736 [D loss: 0.649374, acc: 64.06%] [G loss: 2.139527]\n",
      "epoch:26 step:24737 [D loss: 0.697345, acc: 52.34%] [G loss: 1.792739]\n",
      "epoch:26 step:24738 [D loss: 0.646008, acc: 57.81%] [G loss: 1.812155]\n",
      "epoch:26 step:24739 [D loss: 0.694797, acc: 53.12%] [G loss: 1.717539]\n",
      "epoch:26 step:24740 [D loss: 0.663448, acc: 58.59%] [G loss: 1.855385]\n",
      "epoch:26 step:24741 [D loss: 0.601370, acc: 65.62%] [G loss: 1.802763]\n",
      "epoch:26 step:24742 [D loss: 0.639736, acc: 63.28%] [G loss: 1.899577]\n",
      "epoch:26 step:24743 [D loss: 0.594443, acc: 71.09%] [G loss: 2.049995]\n",
      "epoch:26 step:24744 [D loss: 0.652269, acc: 63.28%] [G loss: 1.888428]\n",
      "epoch:26 step:24745 [D loss: 0.633477, acc: 64.84%] [G loss: 1.872100]\n",
      "epoch:26 step:24746 [D loss: 0.666887, acc: 64.84%] [G loss: 1.870109]\n",
      "epoch:26 step:24747 [D loss: 0.597394, acc: 68.75%] [G loss: 2.026312]\n",
      "epoch:26 step:24748 [D loss: 0.688997, acc: 54.69%] [G loss: 1.686973]\n",
      "epoch:26 step:24749 [D loss: 0.715818, acc: 52.34%] [G loss: 1.772985]\n",
      "epoch:26 step:24750 [D loss: 0.642583, acc: 62.50%] [G loss: 1.840240]\n",
      "epoch:26 step:24751 [D loss: 0.673296, acc: 61.72%] [G loss: 1.682960]\n",
      "epoch:26 step:24752 [D loss: 0.643609, acc: 65.62%] [G loss: 1.804466]\n",
      "epoch:26 step:24753 [D loss: 0.743369, acc: 53.12%] [G loss: 1.774647]\n",
      "epoch:26 step:24754 [D loss: 0.635162, acc: 63.28%] [G loss: 1.771584]\n",
      "epoch:26 step:24755 [D loss: 0.633266, acc: 66.41%] [G loss: 1.766375]\n",
      "epoch:26 step:24756 [D loss: 0.697561, acc: 60.16%] [G loss: 1.792518]\n",
      "epoch:26 step:24757 [D loss: 0.624446, acc: 60.16%] [G loss: 1.828326]\n",
      "epoch:26 step:24758 [D loss: 0.711086, acc: 55.47%] [G loss: 1.756177]\n",
      "epoch:26 step:24759 [D loss: 0.668626, acc: 54.69%] [G loss: 1.758127]\n",
      "epoch:26 step:24760 [D loss: 0.651306, acc: 60.16%] [G loss: 1.839342]\n",
      "epoch:26 step:24761 [D loss: 0.685990, acc: 53.91%] [G loss: 1.841479]\n",
      "epoch:26 step:24762 [D loss: 0.679440, acc: 57.81%] [G loss: 1.819504]\n",
      "epoch:26 step:24763 [D loss: 0.618873, acc: 59.38%] [G loss: 1.792336]\n",
      "epoch:26 step:24764 [D loss: 0.618147, acc: 66.41%] [G loss: 1.933234]\n",
      "epoch:26 step:24765 [D loss: 0.642608, acc: 67.97%] [G loss: 1.842884]\n",
      "epoch:26 step:24766 [D loss: 0.603662, acc: 64.84%] [G loss: 1.991821]\n",
      "epoch:26 step:24767 [D loss: 0.619255, acc: 64.06%] [G loss: 2.065149]\n",
      "epoch:26 step:24768 [D loss: 0.581547, acc: 68.75%] [G loss: 2.121456]\n",
      "epoch:26 step:24769 [D loss: 0.669711, acc: 59.38%] [G loss: 1.809477]\n",
      "epoch:26 step:24770 [D loss: 0.683812, acc: 52.34%] [G loss: 1.917610]\n",
      "epoch:26 step:24771 [D loss: 0.648144, acc: 62.50%] [G loss: 2.042062]\n",
      "epoch:26 step:24772 [D loss: 0.718559, acc: 59.38%] [G loss: 1.793452]\n",
      "epoch:26 step:24773 [D loss: 0.650225, acc: 60.94%] [G loss: 1.851931]\n",
      "epoch:26 step:24774 [D loss: 0.615881, acc: 66.41%] [G loss: 1.879978]\n",
      "epoch:26 step:24775 [D loss: 0.671106, acc: 62.50%] [G loss: 1.787008]\n",
      "epoch:26 step:24776 [D loss: 0.609431, acc: 67.97%] [G loss: 1.883943]\n",
      "epoch:26 step:24777 [D loss: 0.607473, acc: 64.06%] [G loss: 2.033693]\n",
      "epoch:26 step:24778 [D loss: 0.616101, acc: 70.31%] [G loss: 2.185963]\n",
      "epoch:26 step:24779 [D loss: 0.574141, acc: 69.53%] [G loss: 2.074206]\n",
      "epoch:26 step:24780 [D loss: 0.701416, acc: 58.59%] [G loss: 1.788194]\n",
      "epoch:26 step:24781 [D loss: 0.633334, acc: 61.72%] [G loss: 1.901386]\n",
      "epoch:26 step:24782 [D loss: 0.656317, acc: 60.94%] [G loss: 1.791925]\n",
      "epoch:26 step:24783 [D loss: 0.689955, acc: 62.50%] [G loss: 1.866348]\n",
      "epoch:26 step:24784 [D loss: 0.726710, acc: 49.22%] [G loss: 1.849944]\n",
      "epoch:26 step:24785 [D loss: 0.648066, acc: 58.59%] [G loss: 1.743100]\n",
      "epoch:26 step:24786 [D loss: 0.660008, acc: 57.81%] [G loss: 1.736985]\n",
      "epoch:26 step:24787 [D loss: 0.718518, acc: 55.47%] [G loss: 1.790391]\n",
      "epoch:26 step:24788 [D loss: 0.675085, acc: 55.47%] [G loss: 1.832054]\n",
      "epoch:26 step:24789 [D loss: 0.658260, acc: 57.81%] [G loss: 1.825888]\n",
      "epoch:26 step:24790 [D loss: 0.571523, acc: 74.22%] [G loss: 1.933186]\n",
      "epoch:26 step:24791 [D loss: 0.589639, acc: 67.19%] [G loss: 2.109759]\n",
      "epoch:26 step:24792 [D loss: 0.524624, acc: 79.69%] [G loss: 2.269337]\n",
      "epoch:26 step:24793 [D loss: 0.626224, acc: 67.97%] [G loss: 2.060594]\n",
      "epoch:26 step:24794 [D loss: 0.658917, acc: 65.62%] [G loss: 1.799811]\n",
      "epoch:26 step:24795 [D loss: 0.671474, acc: 57.03%] [G loss: 1.948508]\n",
      "epoch:26 step:24796 [D loss: 0.615591, acc: 64.84%] [G loss: 2.037909]\n",
      "epoch:26 step:24797 [D loss: 0.617638, acc: 62.50%] [G loss: 1.916451]\n",
      "epoch:26 step:24798 [D loss: 0.690807, acc: 58.59%] [G loss: 2.088540]\n",
      "epoch:26 step:24799 [D loss: 0.709513, acc: 53.12%] [G loss: 1.778224]\n",
      "epoch:26 step:24800 [D loss: 0.658911, acc: 57.03%] [G loss: 1.867240]\n",
      "epoch:26 step:24801 [D loss: 0.679968, acc: 53.91%] [G loss: 1.752603]\n",
      "epoch:26 step:24802 [D loss: 0.675032, acc: 61.72%] [G loss: 1.827080]\n",
      "epoch:26 step:24803 [D loss: 0.619107, acc: 64.06%] [G loss: 1.785716]\n",
      "epoch:26 step:24804 [D loss: 0.697988, acc: 59.38%] [G loss: 1.742772]\n",
      "epoch:26 step:24805 [D loss: 0.691256, acc: 58.59%] [G loss: 1.777637]\n",
      "epoch:26 step:24806 [D loss: 0.649381, acc: 64.06%] [G loss: 1.715084]\n",
      "epoch:26 step:24807 [D loss: 0.652638, acc: 61.72%] [G loss: 1.756876]\n",
      "epoch:26 step:24808 [D loss: 0.675351, acc: 54.69%] [G loss: 1.763943]\n",
      "epoch:26 step:24809 [D loss: 0.643645, acc: 60.16%] [G loss: 1.799000]\n",
      "epoch:26 step:24810 [D loss: 0.637913, acc: 67.97%] [G loss: 1.829738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24811 [D loss: 0.647255, acc: 63.28%] [G loss: 1.817137]\n",
      "epoch:26 step:24812 [D loss: 0.622684, acc: 68.75%] [G loss: 1.814041]\n",
      "epoch:26 step:24813 [D loss: 0.621626, acc: 67.97%] [G loss: 1.943288]\n",
      "epoch:26 step:24814 [D loss: 0.700808, acc: 54.69%] [G loss: 1.711976]\n",
      "epoch:26 step:24815 [D loss: 0.623144, acc: 66.41%] [G loss: 1.872211]\n",
      "epoch:26 step:24816 [D loss: 0.615353, acc: 66.41%] [G loss: 2.048789]\n",
      "epoch:26 step:24817 [D loss: 0.584263, acc: 73.44%] [G loss: 1.988940]\n",
      "epoch:26 step:24818 [D loss: 0.610182, acc: 67.97%] [G loss: 2.042605]\n",
      "epoch:26 step:24819 [D loss: 0.605281, acc: 71.09%] [G loss: 2.024530]\n",
      "epoch:26 step:24820 [D loss: 0.674646, acc: 57.03%] [G loss: 1.804331]\n",
      "epoch:26 step:24821 [D loss: 0.703346, acc: 45.31%] [G loss: 1.849627]\n",
      "epoch:26 step:24822 [D loss: 0.689000, acc: 52.34%] [G loss: 1.802784]\n",
      "epoch:26 step:24823 [D loss: 0.646711, acc: 65.62%] [G loss: 1.806583]\n",
      "epoch:26 step:24824 [D loss: 0.656030, acc: 59.38%] [G loss: 1.845132]\n",
      "epoch:26 step:24825 [D loss: 0.657381, acc: 65.62%] [G loss: 1.900314]\n",
      "epoch:26 step:24826 [D loss: 0.734985, acc: 50.78%] [G loss: 1.745796]\n",
      "epoch:26 step:24827 [D loss: 0.691068, acc: 54.69%] [G loss: 2.031951]\n",
      "epoch:26 step:24828 [D loss: 0.665159, acc: 60.16%] [G loss: 1.826696]\n",
      "epoch:26 step:24829 [D loss: 0.637955, acc: 64.06%] [G loss: 1.895349]\n",
      "epoch:26 step:24830 [D loss: 0.684462, acc: 57.03%] [G loss: 1.980945]\n",
      "epoch:26 step:24831 [D loss: 0.624398, acc: 64.06%] [G loss: 2.079973]\n",
      "epoch:26 step:24832 [D loss: 0.683573, acc: 59.38%] [G loss: 1.864946]\n",
      "epoch:26 step:24833 [D loss: 0.543128, acc: 72.66%] [G loss: 2.166890]\n",
      "epoch:26 step:24834 [D loss: 0.596702, acc: 67.97%] [G loss: 2.150043]\n",
      "epoch:26 step:24835 [D loss: 0.769231, acc: 47.66%] [G loss: 1.773577]\n",
      "epoch:26 step:24836 [D loss: 0.636945, acc: 62.50%] [G loss: 1.765743]\n",
      "epoch:26 step:24837 [D loss: 0.692951, acc: 60.94%] [G loss: 1.819939]\n",
      "epoch:26 step:24838 [D loss: 0.646220, acc: 59.38%] [G loss: 1.855696]\n",
      "epoch:26 step:24839 [D loss: 0.737525, acc: 47.66%] [G loss: 1.712887]\n",
      "epoch:26 step:24840 [D loss: 0.654175, acc: 60.94%] [G loss: 1.798132]\n",
      "epoch:26 step:24841 [D loss: 0.596448, acc: 64.84%] [G loss: 1.964373]\n",
      "epoch:26 step:24842 [D loss: 0.640082, acc: 59.38%] [G loss: 1.918026]\n",
      "epoch:26 step:24843 [D loss: 0.593009, acc: 68.75%] [G loss: 1.981506]\n",
      "epoch:26 step:24844 [D loss: 0.645206, acc: 61.72%] [G loss: 1.790423]\n",
      "epoch:26 step:24845 [D loss: 0.654867, acc: 61.72%] [G loss: 1.743319]\n",
      "epoch:26 step:24846 [D loss: 0.661552, acc: 56.25%] [G loss: 1.890726]\n",
      "epoch:26 step:24847 [D loss: 0.626128, acc: 63.28%] [G loss: 1.850837]\n",
      "epoch:26 step:24848 [D loss: 0.623567, acc: 67.19%] [G loss: 1.757309]\n",
      "epoch:26 step:24849 [D loss: 0.637493, acc: 63.28%] [G loss: 1.845732]\n",
      "epoch:26 step:24850 [D loss: 0.624887, acc: 63.28%] [G loss: 2.097759]\n",
      "epoch:26 step:24851 [D loss: 0.761715, acc: 53.91%] [G loss: 1.893210]\n",
      "epoch:26 step:24852 [D loss: 0.667941, acc: 60.94%] [G loss: 1.816726]\n",
      "epoch:26 step:24853 [D loss: 0.615272, acc: 67.97%] [G loss: 1.925560]\n",
      "epoch:26 step:24854 [D loss: 0.669093, acc: 58.59%] [G loss: 1.856260]\n",
      "epoch:26 step:24855 [D loss: 0.660256, acc: 57.03%] [G loss: 1.813024]\n",
      "epoch:26 step:24856 [D loss: 0.607078, acc: 66.41%] [G loss: 2.019231]\n",
      "epoch:26 step:24857 [D loss: 0.621818, acc: 67.19%] [G loss: 2.078779]\n",
      "epoch:26 step:24858 [D loss: 0.635452, acc: 64.84%] [G loss: 2.029027]\n",
      "epoch:26 step:24859 [D loss: 0.663699, acc: 57.03%] [G loss: 1.875817]\n",
      "epoch:26 step:24860 [D loss: 0.644824, acc: 66.41%] [G loss: 1.904039]\n",
      "epoch:26 step:24861 [D loss: 0.588536, acc: 67.19%] [G loss: 1.998182]\n",
      "epoch:26 step:24862 [D loss: 0.685907, acc: 57.03%] [G loss: 1.750790]\n",
      "epoch:26 step:24863 [D loss: 0.748228, acc: 49.22%] [G loss: 1.805419]\n",
      "epoch:26 step:24864 [D loss: 0.691085, acc: 54.69%] [G loss: 1.609178]\n",
      "epoch:26 step:24865 [D loss: 0.663252, acc: 56.25%] [G loss: 1.794972]\n",
      "epoch:26 step:24866 [D loss: 0.626055, acc: 59.38%] [G loss: 1.902875]\n",
      "epoch:26 step:24867 [D loss: 0.628871, acc: 64.06%] [G loss: 1.786482]\n",
      "epoch:26 step:24868 [D loss: 0.715440, acc: 54.69%] [G loss: 1.744794]\n",
      "epoch:26 step:24869 [D loss: 0.690978, acc: 58.59%] [G loss: 1.827558]\n",
      "epoch:26 step:24870 [D loss: 0.602079, acc: 67.19%] [G loss: 2.013733]\n",
      "epoch:26 step:24871 [D loss: 0.637491, acc: 64.84%] [G loss: 1.867663]\n",
      "epoch:26 step:24872 [D loss: 0.645680, acc: 64.06%] [G loss: 1.830203]\n",
      "epoch:26 step:24873 [D loss: 0.691279, acc: 53.12%] [G loss: 1.716568]\n",
      "epoch:26 step:24874 [D loss: 0.696725, acc: 57.03%] [G loss: 1.814571]\n",
      "epoch:26 step:24875 [D loss: 0.675114, acc: 64.84%] [G loss: 1.803658]\n",
      "epoch:26 step:24876 [D loss: 0.665597, acc: 55.47%] [G loss: 1.765822]\n",
      "epoch:26 step:24877 [D loss: 0.676855, acc: 62.50%] [G loss: 1.816251]\n",
      "epoch:26 step:24878 [D loss: 0.584220, acc: 74.22%] [G loss: 1.812903]\n",
      "epoch:26 step:24879 [D loss: 0.615586, acc: 67.97%] [G loss: 1.799245]\n",
      "epoch:26 step:24880 [D loss: 0.685085, acc: 59.38%] [G loss: 1.785846]\n",
      "epoch:26 step:24881 [D loss: 0.616173, acc: 63.28%] [G loss: 1.802162]\n",
      "epoch:26 step:24882 [D loss: 0.613854, acc: 68.75%] [G loss: 1.746478]\n",
      "epoch:26 step:24883 [D loss: 0.638598, acc: 64.06%] [G loss: 1.928804]\n",
      "epoch:26 step:24884 [D loss: 0.654190, acc: 54.69%] [G loss: 1.980241]\n",
      "epoch:26 step:24885 [D loss: 0.636387, acc: 66.41%] [G loss: 1.901857]\n",
      "epoch:26 step:24886 [D loss: 0.683414, acc: 59.38%] [G loss: 1.813779]\n",
      "epoch:26 step:24887 [D loss: 0.655282, acc: 60.94%] [G loss: 1.801525]\n",
      "epoch:26 step:24888 [D loss: 0.617317, acc: 68.75%] [G loss: 1.900777]\n",
      "epoch:26 step:24889 [D loss: 0.659479, acc: 61.72%] [G loss: 1.798164]\n",
      "epoch:26 step:24890 [D loss: 0.677963, acc: 53.12%] [G loss: 1.711353]\n",
      "epoch:26 step:24891 [D loss: 0.672640, acc: 60.94%] [G loss: 1.770266]\n",
      "epoch:26 step:24892 [D loss: 0.715288, acc: 53.91%] [G loss: 1.725838]\n",
      "epoch:26 step:24893 [D loss: 0.701139, acc: 48.44%] [G loss: 1.859828]\n",
      "epoch:26 step:24894 [D loss: 0.657277, acc: 60.94%] [G loss: 1.968422]\n",
      "epoch:26 step:24895 [D loss: 0.617446, acc: 64.06%] [G loss: 1.907489]\n",
      "epoch:26 step:24896 [D loss: 0.629557, acc: 67.97%] [G loss: 1.831166]\n",
      "epoch:26 step:24897 [D loss: 0.666611, acc: 57.81%] [G loss: 1.764800]\n",
      "epoch:26 step:24898 [D loss: 0.590778, acc: 71.09%] [G loss: 1.791916]\n",
      "epoch:26 step:24899 [D loss: 0.632879, acc: 67.19%] [G loss: 1.683577]\n",
      "epoch:26 step:24900 [D loss: 0.650941, acc: 60.16%] [G loss: 1.792713]\n",
      "epoch:26 step:24901 [D loss: 0.600224, acc: 64.06%] [G loss: 1.848578]\n",
      "epoch:26 step:24902 [D loss: 0.722144, acc: 55.47%] [G loss: 1.863073]\n",
      "epoch:26 step:24903 [D loss: 0.604268, acc: 66.41%] [G loss: 1.786121]\n",
      "epoch:26 step:24904 [D loss: 0.614981, acc: 67.97%] [G loss: 1.873476]\n",
      "epoch:26 step:24905 [D loss: 0.659611, acc: 64.06%] [G loss: 1.942723]\n",
      "epoch:26 step:24906 [D loss: 0.708129, acc: 50.00%] [G loss: 1.720516]\n",
      "epoch:26 step:24907 [D loss: 0.659393, acc: 64.84%] [G loss: 1.851241]\n",
      "epoch:26 step:24908 [D loss: 0.636878, acc: 63.28%] [G loss: 1.874493]\n",
      "epoch:26 step:24909 [D loss: 0.695451, acc: 55.47%] [G loss: 1.941471]\n",
      "epoch:26 step:24910 [D loss: 0.647162, acc: 64.06%] [G loss: 1.776806]\n",
      "epoch:26 step:24911 [D loss: 0.650206, acc: 64.06%] [G loss: 1.876500]\n",
      "epoch:26 step:24912 [D loss: 0.659556, acc: 60.94%] [G loss: 1.891229]\n",
      "epoch:26 step:24913 [D loss: 0.622112, acc: 68.75%] [G loss: 1.950182]\n",
      "epoch:26 step:24914 [D loss: 0.666992, acc: 62.50%] [G loss: 2.040005]\n",
      "epoch:26 step:24915 [D loss: 0.704832, acc: 53.91%] [G loss: 1.831557]\n",
      "epoch:26 step:24916 [D loss: 0.676322, acc: 60.16%] [G loss: 2.080925]\n",
      "epoch:26 step:24917 [D loss: 0.609824, acc: 67.97%] [G loss: 2.026779]\n",
      "epoch:26 step:24918 [D loss: 0.598736, acc: 65.62%] [G loss: 1.991313]\n",
      "epoch:26 step:24919 [D loss: 0.599454, acc: 67.97%] [G loss: 2.055501]\n",
      "epoch:26 step:24920 [D loss: 0.581358, acc: 69.53%] [G loss: 2.120579]\n",
      "epoch:26 step:24921 [D loss: 0.705128, acc: 51.56%] [G loss: 1.772503]\n",
      "epoch:26 step:24922 [D loss: 0.685824, acc: 52.34%] [G loss: 1.742815]\n",
      "epoch:26 step:24923 [D loss: 0.663607, acc: 62.50%] [G loss: 1.832496]\n",
      "epoch:26 step:24924 [D loss: 0.620834, acc: 64.06%] [G loss: 1.843904]\n",
      "epoch:26 step:24925 [D loss: 0.637053, acc: 61.72%] [G loss: 1.940067]\n",
      "epoch:26 step:24926 [D loss: 0.624727, acc: 66.41%] [G loss: 2.021293]\n",
      "epoch:26 step:24927 [D loss: 0.648506, acc: 58.59%] [G loss: 1.814134]\n",
      "epoch:26 step:24928 [D loss: 0.665595, acc: 59.38%] [G loss: 1.744093]\n",
      "epoch:26 step:24929 [D loss: 0.681277, acc: 52.34%] [G loss: 1.760871]\n",
      "epoch:26 step:24930 [D loss: 0.628531, acc: 66.41%] [G loss: 1.718009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:24931 [D loss: 0.624230, acc: 65.62%] [G loss: 1.708421]\n",
      "epoch:26 step:24932 [D loss: 0.665801, acc: 61.72%] [G loss: 1.913909]\n",
      "epoch:26 step:24933 [D loss: 0.640711, acc: 66.41%] [G loss: 1.844253]\n",
      "epoch:26 step:24934 [D loss: 0.642810, acc: 57.03%] [G loss: 1.829374]\n",
      "epoch:26 step:24935 [D loss: 0.619126, acc: 70.31%] [G loss: 1.800133]\n",
      "epoch:26 step:24936 [D loss: 0.660423, acc: 61.72%] [G loss: 1.779958]\n",
      "epoch:26 step:24937 [D loss: 0.650492, acc: 63.28%] [G loss: 1.811373]\n",
      "epoch:26 step:24938 [D loss: 0.659645, acc: 58.59%] [G loss: 1.707447]\n",
      "epoch:26 step:24939 [D loss: 0.656669, acc: 62.50%] [G loss: 1.654482]\n",
      "epoch:26 step:24940 [D loss: 0.652030, acc: 65.62%] [G loss: 1.961513]\n",
      "epoch:26 step:24941 [D loss: 0.697258, acc: 53.12%] [G loss: 1.835024]\n",
      "epoch:26 step:24942 [D loss: 0.665933, acc: 60.94%] [G loss: 1.879953]\n",
      "epoch:26 step:24943 [D loss: 0.676068, acc: 60.16%] [G loss: 1.806818]\n",
      "epoch:26 step:24944 [D loss: 0.685449, acc: 53.91%] [G loss: 1.870629]\n",
      "epoch:26 step:24945 [D loss: 0.648072, acc: 62.50%] [G loss: 1.918605]\n",
      "epoch:26 step:24946 [D loss: 0.668173, acc: 58.59%] [G loss: 1.666615]\n",
      "epoch:26 step:24947 [D loss: 0.634039, acc: 64.84%] [G loss: 1.936675]\n",
      "epoch:26 step:24948 [D loss: 0.641780, acc: 62.50%] [G loss: 1.815849]\n",
      "epoch:26 step:24949 [D loss: 0.688252, acc: 55.47%] [G loss: 1.737264]\n",
      "epoch:26 step:24950 [D loss: 0.648410, acc: 60.94%] [G loss: 1.902799]\n",
      "epoch:26 step:24951 [D loss: 0.653842, acc: 58.59%] [G loss: 1.824433]\n",
      "epoch:26 step:24952 [D loss: 0.674518, acc: 57.81%] [G loss: 1.861613]\n",
      "epoch:26 step:24953 [D loss: 0.649123, acc: 63.28%] [G loss: 1.889001]\n",
      "epoch:26 step:24954 [D loss: 0.663201, acc: 57.03%] [G loss: 2.004026]\n",
      "epoch:26 step:24955 [D loss: 0.619901, acc: 68.75%] [G loss: 1.774837]\n",
      "epoch:26 step:24956 [D loss: 0.627167, acc: 64.84%] [G loss: 1.861221]\n",
      "epoch:26 step:24957 [D loss: 0.730209, acc: 55.47%] [G loss: 1.922753]\n",
      "epoch:26 step:24958 [D loss: 0.643239, acc: 60.94%] [G loss: 1.755207]\n",
      "epoch:26 step:24959 [D loss: 0.664230, acc: 61.72%] [G loss: 1.715763]\n",
      "epoch:26 step:24960 [D loss: 0.660294, acc: 62.50%] [G loss: 1.759064]\n",
      "epoch:26 step:24961 [D loss: 0.654218, acc: 56.25%] [G loss: 1.812736]\n",
      "epoch:26 step:24962 [D loss: 0.658283, acc: 60.94%] [G loss: 1.891605]\n",
      "epoch:26 step:24963 [D loss: 0.680256, acc: 57.03%] [G loss: 1.685205]\n",
      "epoch:26 step:24964 [D loss: 0.641165, acc: 61.72%] [G loss: 1.754139]\n",
      "epoch:26 step:24965 [D loss: 0.649707, acc: 63.28%] [G loss: 1.818919]\n",
      "epoch:26 step:24966 [D loss: 0.658439, acc: 62.50%] [G loss: 1.917998]\n",
      "epoch:26 step:24967 [D loss: 0.647460, acc: 57.81%] [G loss: 1.894822]\n",
      "epoch:26 step:24968 [D loss: 0.673032, acc: 57.81%] [G loss: 1.722491]\n",
      "epoch:26 step:24969 [D loss: 0.690378, acc: 60.16%] [G loss: 1.710518]\n",
      "epoch:26 step:24970 [D loss: 0.680927, acc: 57.81%] [G loss: 1.782408]\n",
      "epoch:26 step:24971 [D loss: 0.676857, acc: 58.59%] [G loss: 1.785523]\n",
      "epoch:26 step:24972 [D loss: 0.618641, acc: 65.62%] [G loss: 1.740069]\n",
      "epoch:26 step:24973 [D loss: 0.685612, acc: 57.03%] [G loss: 1.739823]\n",
      "epoch:26 step:24974 [D loss: 0.690714, acc: 55.47%] [G loss: 1.704741]\n",
      "epoch:26 step:24975 [D loss: 0.622896, acc: 66.41%] [G loss: 1.806952]\n",
      "epoch:26 step:24976 [D loss: 0.683364, acc: 53.12%] [G loss: 1.705661]\n",
      "epoch:26 step:24977 [D loss: 0.656936, acc: 57.81%] [G loss: 1.678624]\n",
      "epoch:26 step:24978 [D loss: 0.700991, acc: 56.25%] [G loss: 1.760890]\n",
      "epoch:26 step:24979 [D loss: 0.654251, acc: 63.28%] [G loss: 1.753324]\n",
      "epoch:26 step:24980 [D loss: 0.634980, acc: 60.94%] [G loss: 1.747808]\n",
      "epoch:26 step:24981 [D loss: 0.663853, acc: 66.41%] [G loss: 1.766910]\n",
      "epoch:26 step:24982 [D loss: 0.610061, acc: 71.88%] [G loss: 1.826075]\n",
      "epoch:26 step:24983 [D loss: 0.661450, acc: 62.50%] [G loss: 1.749176]\n",
      "epoch:26 step:24984 [D loss: 0.638292, acc: 64.84%] [G loss: 1.795938]\n",
      "epoch:26 step:24985 [D loss: 0.635227, acc: 63.28%] [G loss: 1.795499]\n",
      "epoch:26 step:24986 [D loss: 0.694154, acc: 56.25%] [G loss: 1.839402]\n",
      "epoch:26 step:24987 [D loss: 0.677052, acc: 56.25%] [G loss: 1.658916]\n",
      "epoch:26 step:24988 [D loss: 0.645507, acc: 65.62%] [G loss: 1.866858]\n",
      "epoch:26 step:24989 [D loss: 0.630011, acc: 67.97%] [G loss: 1.666015]\n",
      "epoch:26 step:24990 [D loss: 0.665128, acc: 53.91%] [G loss: 1.740182]\n",
      "epoch:26 step:24991 [D loss: 0.604935, acc: 67.19%] [G loss: 1.846141]\n",
      "epoch:26 step:24992 [D loss: 0.664911, acc: 61.72%] [G loss: 1.848334]\n",
      "epoch:26 step:24993 [D loss: 0.641405, acc: 65.62%] [G loss: 1.846301]\n",
      "epoch:26 step:24994 [D loss: 0.621387, acc: 66.41%] [G loss: 1.921869]\n",
      "epoch:26 step:24995 [D loss: 0.633251, acc: 65.62%] [G loss: 1.823235]\n",
      "epoch:26 step:24996 [D loss: 0.609196, acc: 71.09%] [G loss: 1.950262]\n",
      "epoch:26 step:24997 [D loss: 0.595836, acc: 67.97%] [G loss: 1.869326]\n",
      "epoch:26 step:24998 [D loss: 0.654033, acc: 60.94%] [G loss: 1.959030]\n",
      "epoch:26 step:24999 [D loss: 0.643680, acc: 63.28%] [G loss: 1.923480]\n",
      "epoch:26 step:25000 [D loss: 0.673295, acc: 58.59%] [G loss: 1.923216]\n",
      "epoch:26 step:25001 [D loss: 0.678526, acc: 57.03%] [G loss: 1.920317]\n",
      "epoch:26 step:25002 [D loss: 0.701819, acc: 57.81%] [G loss: 1.798675]\n",
      "epoch:26 step:25003 [D loss: 0.654394, acc: 59.38%] [G loss: 1.780698]\n",
      "epoch:26 step:25004 [D loss: 0.664560, acc: 57.81%] [G loss: 1.885125]\n",
      "epoch:26 step:25005 [D loss: 0.631478, acc: 65.62%] [G loss: 1.939716]\n",
      "epoch:26 step:25006 [D loss: 0.644185, acc: 64.06%] [G loss: 1.904848]\n",
      "epoch:26 step:25007 [D loss: 0.630092, acc: 61.72%] [G loss: 1.915809]\n",
      "epoch:26 step:25008 [D loss: 0.607598, acc: 65.62%] [G loss: 1.956384]\n",
      "epoch:26 step:25009 [D loss: 0.619893, acc: 67.19%] [G loss: 1.964321]\n",
      "epoch:26 step:25010 [D loss: 0.595274, acc: 66.41%] [G loss: 2.117350]\n",
      "epoch:26 step:25011 [D loss: 0.623870, acc: 65.62%] [G loss: 2.061684]\n",
      "epoch:26 step:25012 [D loss: 0.620904, acc: 61.72%] [G loss: 1.932927]\n",
      "epoch:26 step:25013 [D loss: 0.677558, acc: 60.94%] [G loss: 1.787776]\n",
      "epoch:26 step:25014 [D loss: 0.651208, acc: 62.50%] [G loss: 1.860641]\n",
      "epoch:26 step:25015 [D loss: 0.681841, acc: 56.25%] [G loss: 1.971502]\n",
      "epoch:26 step:25016 [D loss: 0.626064, acc: 64.06%] [G loss: 1.864324]\n",
      "epoch:26 step:25017 [D loss: 0.684031, acc: 60.94%] [G loss: 1.865736]\n",
      "epoch:26 step:25018 [D loss: 0.666199, acc: 62.50%] [G loss: 1.811590]\n",
      "epoch:26 step:25019 [D loss: 0.661030, acc: 57.81%] [G loss: 1.690133]\n",
      "epoch:26 step:25020 [D loss: 0.647153, acc: 63.28%] [G loss: 1.749125]\n",
      "epoch:26 step:25021 [D loss: 0.650290, acc: 65.62%] [G loss: 1.769195]\n",
      "epoch:26 step:25022 [D loss: 0.647900, acc: 61.72%] [G loss: 1.748624]\n",
      "epoch:26 step:25023 [D loss: 0.636207, acc: 64.06%] [G loss: 1.766580]\n",
      "epoch:26 step:25024 [D loss: 0.673145, acc: 62.50%] [G loss: 1.790729]\n",
      "epoch:26 step:25025 [D loss: 0.660521, acc: 59.38%] [G loss: 1.738156]\n",
      "epoch:26 step:25026 [D loss: 0.662766, acc: 64.06%] [G loss: 1.773104]\n",
      "epoch:26 step:25027 [D loss: 0.644744, acc: 64.84%] [G loss: 1.840441]\n",
      "epoch:26 step:25028 [D loss: 0.636456, acc: 64.84%] [G loss: 1.690831]\n",
      "epoch:26 step:25029 [D loss: 0.704712, acc: 53.12%] [G loss: 1.747283]\n",
      "epoch:26 step:25030 [D loss: 0.672253, acc: 59.38%] [G loss: 1.653124]\n",
      "epoch:26 step:25031 [D loss: 0.628818, acc: 64.84%] [G loss: 1.786036]\n",
      "epoch:26 step:25032 [D loss: 0.725652, acc: 53.12%] [G loss: 1.783671]\n",
      "epoch:26 step:25033 [D loss: 0.659338, acc: 56.25%] [G loss: 1.743692]\n",
      "epoch:26 step:25034 [D loss: 0.600687, acc: 67.97%] [G loss: 1.782413]\n",
      "epoch:26 step:25035 [D loss: 0.670499, acc: 60.94%] [G loss: 1.923751]\n",
      "epoch:26 step:25036 [D loss: 0.677157, acc: 57.81%] [G loss: 1.761795]\n",
      "epoch:26 step:25037 [D loss: 0.644173, acc: 61.72%] [G loss: 1.825392]\n",
      "epoch:26 step:25038 [D loss: 0.676986, acc: 51.56%] [G loss: 1.850005]\n",
      "epoch:26 step:25039 [D loss: 0.620782, acc: 71.88%] [G loss: 1.907287]\n",
      "epoch:26 step:25040 [D loss: 0.616002, acc: 64.06%] [G loss: 1.811913]\n",
      "epoch:26 step:25041 [D loss: 0.656207, acc: 65.62%] [G loss: 1.896433]\n",
      "epoch:26 step:25042 [D loss: 0.643497, acc: 64.06%] [G loss: 1.798731]\n",
      "epoch:26 step:25043 [D loss: 0.630584, acc: 59.38%] [G loss: 1.948594]\n",
      "epoch:26 step:25044 [D loss: 0.666661, acc: 59.38%] [G loss: 1.807197]\n",
      "epoch:26 step:25045 [D loss: 0.652363, acc: 58.59%] [G loss: 1.861375]\n",
      "epoch:26 step:25046 [D loss: 0.667801, acc: 61.72%] [G loss: 1.777351]\n",
      "epoch:26 step:25047 [D loss: 0.621163, acc: 66.41%] [G loss: 1.880638]\n",
      "epoch:26 step:25048 [D loss: 0.679306, acc: 62.50%] [G loss: 1.783139]\n",
      "epoch:26 step:25049 [D loss: 0.605674, acc: 61.72%] [G loss: 1.903397]\n",
      "epoch:26 step:25050 [D loss: 0.669516, acc: 66.41%] [G loss: 1.887829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25051 [D loss: 0.629616, acc: 62.50%] [G loss: 1.961450]\n",
      "epoch:26 step:25052 [D loss: 0.611473, acc: 68.75%] [G loss: 2.078050]\n",
      "epoch:26 step:25053 [D loss: 0.640074, acc: 64.06%] [G loss: 2.054554]\n",
      "epoch:26 step:25054 [D loss: 0.609733, acc: 66.41%] [G loss: 1.937412]\n",
      "epoch:26 step:25055 [D loss: 0.625277, acc: 68.75%] [G loss: 2.047779]\n",
      "epoch:26 step:25056 [D loss: 0.591532, acc: 69.53%] [G loss: 2.096364]\n",
      "epoch:26 step:25057 [D loss: 0.626129, acc: 60.16%] [G loss: 1.953168]\n",
      "epoch:26 step:25058 [D loss: 0.696555, acc: 51.56%] [G loss: 1.818403]\n",
      "epoch:26 step:25059 [D loss: 0.665866, acc: 60.16%] [G loss: 1.728872]\n",
      "epoch:26 step:25060 [D loss: 0.663182, acc: 59.38%] [G loss: 1.881743]\n",
      "epoch:26 step:25061 [D loss: 0.635523, acc: 67.19%] [G loss: 1.879983]\n",
      "epoch:26 step:25062 [D loss: 0.658530, acc: 59.38%] [G loss: 1.809175]\n",
      "epoch:26 step:25063 [D loss: 0.645161, acc: 64.06%] [G loss: 1.904046]\n",
      "epoch:26 step:25064 [D loss: 0.708769, acc: 58.59%] [G loss: 1.844747]\n",
      "epoch:26 step:25065 [D loss: 0.612860, acc: 68.75%] [G loss: 1.783867]\n",
      "epoch:26 step:25066 [D loss: 0.624875, acc: 62.50%] [G loss: 1.665999]\n",
      "epoch:26 step:25067 [D loss: 0.677093, acc: 58.59%] [G loss: 1.809405]\n",
      "epoch:26 step:25068 [D loss: 0.694502, acc: 64.06%] [G loss: 1.902782]\n",
      "epoch:26 step:25069 [D loss: 0.606718, acc: 65.62%] [G loss: 2.006633]\n",
      "epoch:26 step:25070 [D loss: 0.595735, acc: 70.31%] [G loss: 1.967697]\n",
      "epoch:26 step:25071 [D loss: 0.622321, acc: 67.19%] [G loss: 1.879320]\n",
      "epoch:26 step:25072 [D loss: 0.637358, acc: 65.62%] [G loss: 1.842750]\n",
      "epoch:26 step:25073 [D loss: 0.601290, acc: 69.53%] [G loss: 1.891268]\n",
      "epoch:26 step:25074 [D loss: 0.626342, acc: 68.75%] [G loss: 2.002611]\n",
      "epoch:26 step:25075 [D loss: 0.697075, acc: 47.66%] [G loss: 1.749141]\n",
      "epoch:26 step:25076 [D loss: 0.609112, acc: 68.75%] [G loss: 1.964772]\n",
      "epoch:26 step:25077 [D loss: 0.612328, acc: 63.28%] [G loss: 1.857100]\n",
      "epoch:26 step:25078 [D loss: 0.625601, acc: 62.50%] [G loss: 1.756941]\n",
      "epoch:26 step:25079 [D loss: 0.685945, acc: 59.38%] [G loss: 1.935275]\n",
      "epoch:26 step:25080 [D loss: 0.682063, acc: 59.38%] [G loss: 1.900941]\n",
      "epoch:26 step:25081 [D loss: 0.644832, acc: 65.62%] [G loss: 2.030658]\n",
      "epoch:26 step:25082 [D loss: 0.655097, acc: 62.50%] [G loss: 2.058617]\n",
      "epoch:26 step:25083 [D loss: 0.640178, acc: 64.06%] [G loss: 1.890283]\n",
      "epoch:26 step:25084 [D loss: 0.701587, acc: 54.69%] [G loss: 1.753345]\n",
      "epoch:26 step:25085 [D loss: 0.683712, acc: 60.94%] [G loss: 1.896378]\n",
      "epoch:26 step:25086 [D loss: 0.634703, acc: 67.19%] [G loss: 1.862943]\n",
      "epoch:26 step:25087 [D loss: 0.630923, acc: 62.50%] [G loss: 1.953348]\n",
      "epoch:26 step:25088 [D loss: 0.609934, acc: 67.19%] [G loss: 1.977290]\n",
      "epoch:26 step:25089 [D loss: 0.629856, acc: 64.06%] [G loss: 1.882201]\n",
      "epoch:26 step:25090 [D loss: 0.635675, acc: 68.75%] [G loss: 1.846882]\n",
      "epoch:26 step:25091 [D loss: 0.679179, acc: 58.59%] [G loss: 1.888185]\n",
      "epoch:26 step:25092 [D loss: 0.657177, acc: 60.94%] [G loss: 1.862450]\n",
      "epoch:26 step:25093 [D loss: 0.704952, acc: 53.91%] [G loss: 1.796452]\n",
      "epoch:26 step:25094 [D loss: 0.621980, acc: 66.41%] [G loss: 1.886322]\n",
      "epoch:26 step:25095 [D loss: 0.654181, acc: 62.50%] [G loss: 1.842861]\n",
      "epoch:26 step:25096 [D loss: 0.660830, acc: 58.59%] [G loss: 1.873896]\n",
      "epoch:26 step:25097 [D loss: 0.664235, acc: 64.84%] [G loss: 1.896252]\n",
      "epoch:26 step:25098 [D loss: 0.664756, acc: 57.03%] [G loss: 1.918098]\n",
      "epoch:26 step:25099 [D loss: 0.633859, acc: 64.84%] [G loss: 1.759153]\n",
      "epoch:26 step:25100 [D loss: 0.623440, acc: 67.19%] [G loss: 1.774392]\n",
      "epoch:26 step:25101 [D loss: 0.685577, acc: 59.38%] [G loss: 1.788663]\n",
      "epoch:26 step:25102 [D loss: 0.611848, acc: 65.62%] [G loss: 1.943584]\n",
      "epoch:26 step:25103 [D loss: 0.691538, acc: 53.12%] [G loss: 1.797313]\n",
      "epoch:26 step:25104 [D loss: 0.700504, acc: 54.69%] [G loss: 1.825044]\n",
      "epoch:26 step:25105 [D loss: 0.626160, acc: 69.53%] [G loss: 1.865597]\n",
      "epoch:26 step:25106 [D loss: 0.653424, acc: 64.84%] [G loss: 1.849484]\n",
      "epoch:26 step:25107 [D loss: 0.705369, acc: 57.03%] [G loss: 1.913309]\n",
      "epoch:26 step:25108 [D loss: 0.679595, acc: 59.38%] [G loss: 1.923958]\n",
      "epoch:26 step:25109 [D loss: 0.643592, acc: 62.50%] [G loss: 1.821905]\n",
      "epoch:26 step:25110 [D loss: 0.649503, acc: 65.62%] [G loss: 1.833225]\n",
      "epoch:26 step:25111 [D loss: 0.672542, acc: 60.94%] [G loss: 1.730775]\n",
      "epoch:26 step:25112 [D loss: 0.640483, acc: 60.94%] [G loss: 1.742648]\n",
      "epoch:26 step:25113 [D loss: 0.697056, acc: 57.81%] [G loss: 1.858182]\n",
      "epoch:26 step:25114 [D loss: 0.628538, acc: 61.72%] [G loss: 1.818039]\n",
      "epoch:26 step:25115 [D loss: 0.623373, acc: 64.06%] [G loss: 1.796428]\n",
      "epoch:26 step:25116 [D loss: 0.614198, acc: 71.09%] [G loss: 1.815487]\n",
      "epoch:26 step:25117 [D loss: 0.640643, acc: 65.62%] [G loss: 1.855166]\n",
      "epoch:26 step:25118 [D loss: 0.638660, acc: 67.19%] [G loss: 1.783744]\n",
      "epoch:26 step:25119 [D loss: 0.642982, acc: 63.28%] [G loss: 1.827418]\n",
      "epoch:26 step:25120 [D loss: 0.679246, acc: 54.69%] [G loss: 1.752589]\n",
      "epoch:26 step:25121 [D loss: 0.659087, acc: 60.16%] [G loss: 1.783480]\n",
      "epoch:26 step:25122 [D loss: 0.684094, acc: 59.38%] [G loss: 1.737621]\n",
      "epoch:26 step:25123 [D loss: 0.660670, acc: 56.25%] [G loss: 1.806044]\n",
      "epoch:26 step:25124 [D loss: 0.701266, acc: 54.69%] [G loss: 1.747091]\n",
      "epoch:26 step:25125 [D loss: 0.652200, acc: 65.62%] [G loss: 1.727549]\n",
      "epoch:26 step:25126 [D loss: 0.608483, acc: 71.88%] [G loss: 1.887449]\n",
      "epoch:26 step:25127 [D loss: 0.674271, acc: 56.25%] [G loss: 1.701499]\n",
      "epoch:26 step:25128 [D loss: 0.677516, acc: 64.06%] [G loss: 1.714811]\n",
      "epoch:26 step:25129 [D loss: 0.671886, acc: 60.16%] [G loss: 1.788392]\n",
      "epoch:26 step:25130 [D loss: 0.663441, acc: 58.59%] [G loss: 1.858540]\n",
      "epoch:26 step:25131 [D loss: 0.613717, acc: 64.06%] [G loss: 2.002680]\n",
      "epoch:26 step:25132 [D loss: 0.664361, acc: 59.38%] [G loss: 1.809573]\n",
      "epoch:26 step:25133 [D loss: 0.649634, acc: 67.19%] [G loss: 1.920490]\n",
      "epoch:26 step:25134 [D loss: 0.649814, acc: 61.72%] [G loss: 1.931495]\n",
      "epoch:26 step:25135 [D loss: 0.635132, acc: 64.06%] [G loss: 1.916498]\n",
      "epoch:26 step:25136 [D loss: 0.641254, acc: 64.84%] [G loss: 1.949244]\n",
      "epoch:26 step:25137 [D loss: 0.633004, acc: 68.75%] [G loss: 2.010082]\n",
      "epoch:26 step:25138 [D loss: 0.675994, acc: 57.03%] [G loss: 1.823454]\n",
      "epoch:26 step:25139 [D loss: 0.615379, acc: 64.84%] [G loss: 1.923560]\n",
      "epoch:26 step:25140 [D loss: 0.687800, acc: 60.16%] [G loss: 1.984572]\n",
      "epoch:26 step:25141 [D loss: 0.644718, acc: 61.72%] [G loss: 1.975653]\n",
      "epoch:26 step:25142 [D loss: 0.634137, acc: 62.50%] [G loss: 1.807275]\n",
      "epoch:26 step:25143 [D loss: 0.576973, acc: 67.19%] [G loss: 1.919349]\n",
      "epoch:26 step:25144 [D loss: 0.622006, acc: 68.75%] [G loss: 2.004633]\n",
      "epoch:26 step:25145 [D loss: 0.692716, acc: 53.91%] [G loss: 1.810444]\n",
      "epoch:26 step:25146 [D loss: 0.681661, acc: 56.25%] [G loss: 1.804424]\n",
      "epoch:26 step:25147 [D loss: 0.681605, acc: 55.47%] [G loss: 1.842119]\n",
      "epoch:26 step:25148 [D loss: 0.617728, acc: 67.19%] [G loss: 1.975058]\n",
      "epoch:26 step:25149 [D loss: 0.667880, acc: 61.72%] [G loss: 1.857398]\n",
      "epoch:26 step:25150 [D loss: 0.655223, acc: 61.72%] [G loss: 1.753711]\n",
      "epoch:26 step:25151 [D loss: 0.612928, acc: 69.53%] [G loss: 1.926596]\n",
      "epoch:26 step:25152 [D loss: 0.658543, acc: 60.94%] [G loss: 1.836295]\n",
      "epoch:26 step:25153 [D loss: 0.683059, acc: 58.59%] [G loss: 1.871216]\n",
      "epoch:26 step:25154 [D loss: 0.619686, acc: 65.62%] [G loss: 1.973225]\n",
      "epoch:26 step:25155 [D loss: 0.617543, acc: 60.94%] [G loss: 1.972409]\n",
      "epoch:26 step:25156 [D loss: 0.735185, acc: 51.56%] [G loss: 1.697898]\n",
      "epoch:26 step:25157 [D loss: 0.699450, acc: 53.91%] [G loss: 1.659728]\n",
      "epoch:26 step:25158 [D loss: 0.633545, acc: 64.06%] [G loss: 1.776801]\n",
      "epoch:26 step:25159 [D loss: 0.682421, acc: 57.03%] [G loss: 1.835544]\n",
      "epoch:26 step:25160 [D loss: 0.666140, acc: 57.03%] [G loss: 1.746787]\n",
      "epoch:26 step:25161 [D loss: 0.629930, acc: 65.62%] [G loss: 1.801174]\n",
      "epoch:26 step:25162 [D loss: 0.713523, acc: 58.59%] [G loss: 1.740459]\n",
      "epoch:26 step:25163 [D loss: 0.678991, acc: 53.91%] [G loss: 1.772813]\n",
      "epoch:26 step:25164 [D loss: 0.664966, acc: 57.03%] [G loss: 1.731030]\n",
      "epoch:26 step:25165 [D loss: 0.634910, acc: 58.59%] [G loss: 1.766574]\n",
      "epoch:26 step:25166 [D loss: 0.617312, acc: 64.84%] [G loss: 1.958824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25167 [D loss: 0.624381, acc: 62.50%] [G loss: 1.948458]\n",
      "epoch:26 step:25168 [D loss: 0.630060, acc: 67.97%] [G loss: 1.901844]\n",
      "epoch:26 step:25169 [D loss: 0.643043, acc: 63.28%] [G loss: 1.826778]\n",
      "epoch:26 step:25170 [D loss: 0.679863, acc: 50.78%] [G loss: 1.863776]\n",
      "epoch:26 step:25171 [D loss: 0.675070, acc: 55.47%] [G loss: 1.937592]\n",
      "epoch:26 step:25172 [D loss: 0.666648, acc: 60.94%] [G loss: 1.803140]\n",
      "epoch:26 step:25173 [D loss: 0.657916, acc: 62.50%] [G loss: 1.837641]\n",
      "epoch:26 step:25174 [D loss: 0.697698, acc: 56.25%] [G loss: 1.825453]\n",
      "epoch:26 step:25175 [D loss: 0.619705, acc: 69.53%] [G loss: 1.834338]\n",
      "epoch:26 step:25176 [D loss: 0.648255, acc: 62.50%] [G loss: 1.905588]\n",
      "epoch:26 step:25177 [D loss: 0.600686, acc: 62.50%] [G loss: 2.127470]\n",
      "epoch:26 step:25178 [D loss: 0.617020, acc: 65.62%] [G loss: 1.983262]\n",
      "epoch:26 step:25179 [D loss: 0.656436, acc: 67.97%] [G loss: 1.839039]\n",
      "epoch:26 step:25180 [D loss: 0.663837, acc: 59.38%] [G loss: 1.812009]\n",
      "epoch:26 step:25181 [D loss: 0.644657, acc: 62.50%] [G loss: 1.782841]\n",
      "epoch:26 step:25182 [D loss: 0.640520, acc: 67.19%] [G loss: 1.647842]\n",
      "epoch:26 step:25183 [D loss: 0.673391, acc: 59.38%] [G loss: 1.794938]\n",
      "epoch:26 step:25184 [D loss: 0.694521, acc: 56.25%] [G loss: 1.888320]\n",
      "epoch:26 step:25185 [D loss: 0.624002, acc: 69.53%] [G loss: 1.898187]\n",
      "epoch:26 step:25186 [D loss: 0.656589, acc: 57.81%] [G loss: 1.769939]\n",
      "epoch:26 step:25187 [D loss: 0.601951, acc: 70.31%] [G loss: 2.161715]\n",
      "epoch:26 step:25188 [D loss: 0.690175, acc: 57.03%] [G loss: 2.072622]\n",
      "epoch:26 step:25189 [D loss: 0.681892, acc: 53.12%] [G loss: 1.810348]\n",
      "epoch:26 step:25190 [D loss: 0.686557, acc: 57.03%] [G loss: 1.627808]\n",
      "epoch:26 step:25191 [D loss: 0.700932, acc: 54.69%] [G loss: 1.723530]\n",
      "epoch:26 step:25192 [D loss: 0.671578, acc: 57.03%] [G loss: 1.724738]\n",
      "epoch:26 step:25193 [D loss: 0.672598, acc: 59.38%] [G loss: 1.813855]\n",
      "epoch:26 step:25194 [D loss: 0.667917, acc: 60.94%] [G loss: 1.884819]\n",
      "epoch:26 step:25195 [D loss: 0.652736, acc: 59.38%] [G loss: 1.826042]\n",
      "epoch:26 step:25196 [D loss: 0.625374, acc: 63.28%] [G loss: 1.768033]\n",
      "epoch:26 step:25197 [D loss: 0.678244, acc: 60.94%] [G loss: 1.734731]\n",
      "epoch:26 step:25198 [D loss: 0.630028, acc: 64.06%] [G loss: 1.874811]\n",
      "epoch:26 step:25199 [D loss: 0.637702, acc: 63.28%] [G loss: 1.965062]\n",
      "epoch:26 step:25200 [D loss: 0.664516, acc: 61.72%] [G loss: 1.804253]\n",
      "epoch:26 step:25201 [D loss: 0.635449, acc: 66.41%] [G loss: 1.884724]\n",
      "epoch:26 step:25202 [D loss: 0.639565, acc: 67.19%] [G loss: 1.893165]\n",
      "epoch:26 step:25203 [D loss: 0.614988, acc: 64.84%] [G loss: 1.981432]\n",
      "epoch:26 step:25204 [D loss: 0.597607, acc: 64.84%] [G loss: 1.949443]\n",
      "epoch:26 step:25205 [D loss: 0.619992, acc: 63.28%] [G loss: 1.830052]\n",
      "epoch:26 step:25206 [D loss: 0.658209, acc: 64.06%] [G loss: 1.890342]\n",
      "epoch:26 step:25207 [D loss: 0.596226, acc: 71.88%] [G loss: 2.051836]\n",
      "epoch:26 step:25208 [D loss: 0.661129, acc: 60.16%] [G loss: 1.858801]\n",
      "epoch:26 step:25209 [D loss: 0.593301, acc: 67.19%] [G loss: 2.006398]\n",
      "epoch:26 step:25210 [D loss: 0.662407, acc: 57.81%] [G loss: 1.844622]\n",
      "epoch:26 step:25211 [D loss: 0.652534, acc: 64.06%] [G loss: 1.968778]\n",
      "epoch:26 step:25212 [D loss: 0.638339, acc: 60.94%] [G loss: 1.905239]\n",
      "epoch:26 step:25213 [D loss: 0.624501, acc: 67.19%] [G loss: 1.780885]\n",
      "epoch:26 step:25214 [D loss: 0.665067, acc: 63.28%] [G loss: 1.845701]\n",
      "epoch:26 step:25215 [D loss: 0.655112, acc: 64.84%] [G loss: 1.811245]\n",
      "epoch:26 step:25216 [D loss: 0.642617, acc: 64.84%] [G loss: 1.807603]\n",
      "epoch:26 step:25217 [D loss: 0.654446, acc: 64.84%] [G loss: 1.941867]\n",
      "epoch:26 step:25218 [D loss: 0.702484, acc: 60.16%] [G loss: 1.752062]\n",
      "epoch:26 step:25219 [D loss: 0.685846, acc: 65.62%] [G loss: 1.858129]\n",
      "epoch:26 step:25220 [D loss: 0.683841, acc: 57.03%] [G loss: 1.850784]\n",
      "epoch:26 step:25221 [D loss: 0.631913, acc: 66.41%] [G loss: 1.906687]\n",
      "epoch:26 step:25222 [D loss: 0.718911, acc: 57.03%] [G loss: 1.795169]\n",
      "epoch:26 step:25223 [D loss: 0.659375, acc: 60.94%] [G loss: 1.749677]\n",
      "epoch:26 step:25224 [D loss: 0.686864, acc: 54.69%] [G loss: 1.690501]\n",
      "epoch:26 step:25225 [D loss: 0.664394, acc: 57.81%] [G loss: 1.830270]\n",
      "epoch:26 step:25226 [D loss: 0.632043, acc: 66.41%] [G loss: 1.772571]\n",
      "epoch:26 step:25227 [D loss: 0.638119, acc: 67.97%] [G loss: 1.842309]\n",
      "epoch:26 step:25228 [D loss: 0.641723, acc: 60.16%] [G loss: 1.841572]\n",
      "epoch:26 step:25229 [D loss: 0.626029, acc: 61.72%] [G loss: 1.875197]\n",
      "epoch:26 step:25230 [D loss: 0.626240, acc: 61.72%] [G loss: 1.733736]\n",
      "epoch:26 step:25231 [D loss: 0.663248, acc: 64.84%] [G loss: 1.730102]\n",
      "epoch:26 step:25232 [D loss: 0.628510, acc: 69.53%] [G loss: 1.800749]\n",
      "epoch:26 step:25233 [D loss: 0.621994, acc: 61.72%] [G loss: 1.929710]\n",
      "epoch:26 step:25234 [D loss: 0.688417, acc: 57.03%] [G loss: 1.725474]\n",
      "epoch:26 step:25235 [D loss: 0.666983, acc: 59.38%] [G loss: 1.778397]\n",
      "epoch:26 step:25236 [D loss: 0.654569, acc: 65.62%] [G loss: 1.871083]\n",
      "epoch:26 step:25237 [D loss: 0.605975, acc: 63.28%] [G loss: 1.850090]\n",
      "epoch:26 step:25238 [D loss: 0.650596, acc: 52.34%] [G loss: 1.896463]\n",
      "epoch:26 step:25239 [D loss: 0.655147, acc: 62.50%] [G loss: 1.904486]\n",
      "epoch:26 step:25240 [D loss: 0.644075, acc: 61.72%] [G loss: 1.828394]\n",
      "epoch:26 step:25241 [D loss: 0.629710, acc: 67.19%] [G loss: 1.794007]\n",
      "epoch:26 step:25242 [D loss: 0.638721, acc: 62.50%] [G loss: 1.953784]\n",
      "epoch:26 step:25243 [D loss: 0.630206, acc: 64.06%] [G loss: 1.840709]\n",
      "epoch:26 step:25244 [D loss: 0.622792, acc: 63.28%] [G loss: 1.882803]\n",
      "epoch:26 step:25245 [D loss: 0.614068, acc: 64.84%] [G loss: 1.759645]\n",
      "epoch:26 step:25246 [D loss: 0.662574, acc: 60.94%] [G loss: 2.035220]\n",
      "epoch:26 step:25247 [D loss: 0.649235, acc: 62.50%] [G loss: 1.905505]\n",
      "epoch:26 step:25248 [D loss: 0.626368, acc: 60.94%] [G loss: 2.054035]\n",
      "epoch:26 step:25249 [D loss: 0.647347, acc: 63.28%] [G loss: 1.839834]\n",
      "epoch:26 step:25250 [D loss: 0.659773, acc: 64.06%] [G loss: 1.907571]\n",
      "epoch:26 step:25251 [D loss: 0.631442, acc: 64.06%] [G loss: 1.823199]\n",
      "epoch:26 step:25252 [D loss: 0.619436, acc: 69.53%] [G loss: 1.783661]\n",
      "epoch:26 step:25253 [D loss: 0.644270, acc: 64.06%] [G loss: 1.910752]\n",
      "epoch:26 step:25254 [D loss: 0.691660, acc: 56.25%] [G loss: 1.837277]\n",
      "epoch:26 step:25255 [D loss: 0.643455, acc: 65.62%] [G loss: 1.956255]\n",
      "epoch:26 step:25256 [D loss: 0.673672, acc: 59.38%] [G loss: 1.923491]\n",
      "epoch:26 step:25257 [D loss: 0.612728, acc: 67.19%] [G loss: 1.870981]\n",
      "epoch:26 step:25258 [D loss: 0.732541, acc: 53.12%] [G loss: 1.757359]\n",
      "epoch:26 step:25259 [D loss: 0.624772, acc: 65.62%] [G loss: 1.883323]\n",
      "epoch:26 step:25260 [D loss: 0.654943, acc: 59.38%] [G loss: 1.897085]\n",
      "epoch:26 step:25261 [D loss: 0.645334, acc: 64.06%] [G loss: 2.034117]\n",
      "epoch:26 step:25262 [D loss: 0.622725, acc: 64.84%] [G loss: 1.942407]\n",
      "epoch:26 step:25263 [D loss: 0.627753, acc: 65.62%] [G loss: 1.969924]\n",
      "epoch:26 step:25264 [D loss: 0.712257, acc: 58.59%] [G loss: 1.751471]\n",
      "epoch:26 step:25265 [D loss: 0.653225, acc: 61.72%] [G loss: 1.945759]\n",
      "epoch:26 step:25266 [D loss: 0.668370, acc: 56.25%] [G loss: 1.852307]\n",
      "epoch:26 step:25267 [D loss: 0.656266, acc: 63.28%] [G loss: 1.934090]\n",
      "epoch:26 step:25268 [D loss: 0.645290, acc: 61.72%] [G loss: 2.080696]\n",
      "epoch:26 step:25269 [D loss: 0.615007, acc: 66.41%] [G loss: 1.827055]\n",
      "epoch:26 step:25270 [D loss: 0.645380, acc: 66.41%] [G loss: 1.843706]\n",
      "epoch:26 step:25271 [D loss: 0.597326, acc: 67.97%] [G loss: 2.030630]\n",
      "epoch:26 step:25272 [D loss: 0.682452, acc: 53.91%] [G loss: 1.953833]\n",
      "epoch:26 step:25273 [D loss: 0.652114, acc: 59.38%] [G loss: 1.791137]\n",
      "epoch:26 step:25274 [D loss: 0.641242, acc: 64.84%] [G loss: 1.904895]\n",
      "epoch:26 step:25275 [D loss: 0.673773, acc: 58.59%] [G loss: 1.726520]\n",
      "epoch:26 step:25276 [D loss: 0.710756, acc: 51.56%] [G loss: 1.933690]\n",
      "epoch:26 step:25277 [D loss: 0.611904, acc: 64.84%] [G loss: 1.899658]\n",
      "epoch:26 step:25278 [D loss: 0.647583, acc: 64.06%] [G loss: 1.865134]\n",
      "epoch:26 step:25279 [D loss: 0.681692, acc: 59.38%] [G loss: 1.881199]\n",
      "epoch:26 step:25280 [D loss: 0.637606, acc: 64.84%] [G loss: 2.020128]\n",
      "epoch:26 step:25281 [D loss: 0.589558, acc: 68.75%] [G loss: 2.058381]\n",
      "epoch:26 step:25282 [D loss: 0.708083, acc: 53.12%] [G loss: 1.916801]\n",
      "epoch:26 step:25283 [D loss: 0.660539, acc: 59.38%] [G loss: 1.906987]\n",
      "epoch:26 step:25284 [D loss: 0.643355, acc: 58.59%] [G loss: 1.981933]\n",
      "epoch:26 step:25285 [D loss: 0.612673, acc: 64.84%] [G loss: 1.910971]\n",
      "epoch:26 step:25286 [D loss: 0.596632, acc: 67.19%] [G loss: 2.150015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:25287 [D loss: 0.632687, acc: 68.75%] [G loss: 1.963052]\n",
      "epoch:26 step:25288 [D loss: 0.570275, acc: 71.09%] [G loss: 2.036407]\n",
      "epoch:26 step:25289 [D loss: 0.638373, acc: 60.94%] [G loss: 1.975023]\n",
      "epoch:26 step:25290 [D loss: 0.756433, acc: 49.22%] [G loss: 1.765109]\n",
      "epoch:26 step:25291 [D loss: 0.792747, acc: 39.84%] [G loss: 1.882304]\n",
      "epoch:26 step:25292 [D loss: 0.630914, acc: 70.31%] [G loss: 1.972402]\n",
      "epoch:26 step:25293 [D loss: 0.620399, acc: 64.84%] [G loss: 2.099244]\n",
      "epoch:26 step:25294 [D loss: 0.705988, acc: 56.25%] [G loss: 1.838223]\n",
      "epoch:26 step:25295 [D loss: 0.634240, acc: 63.28%] [G loss: 1.880701]\n",
      "epoch:26 step:25296 [D loss: 0.631394, acc: 70.31%] [G loss: 1.812728]\n",
      "epoch:26 step:25297 [D loss: 0.613951, acc: 69.53%] [G loss: 2.048448]\n",
      "epoch:26 step:25298 [D loss: 0.590441, acc: 67.97%] [G loss: 2.002402]\n",
      "epoch:26 step:25299 [D loss: 0.593548, acc: 67.19%] [G loss: 2.348934]\n",
      "epoch:27 step:25300 [D loss: 0.635227, acc: 63.28%] [G loss: 1.807039]\n",
      "epoch:27 step:25301 [D loss: 0.662803, acc: 58.59%] [G loss: 1.737919]\n",
      "epoch:27 step:25302 [D loss: 0.612585, acc: 61.72%] [G loss: 1.915705]\n",
      "epoch:27 step:25303 [D loss: 0.685287, acc: 56.25%] [G loss: 1.838064]\n",
      "epoch:27 step:25304 [D loss: 0.649733, acc: 67.19%] [G loss: 1.837373]\n",
      "epoch:27 step:25305 [D loss: 0.670225, acc: 60.94%] [G loss: 1.968353]\n",
      "epoch:27 step:25306 [D loss: 0.689026, acc: 56.25%] [G loss: 1.983290]\n",
      "epoch:27 step:25307 [D loss: 0.635435, acc: 63.28%] [G loss: 1.821088]\n",
      "epoch:27 step:25308 [D loss: 0.595141, acc: 68.75%] [G loss: 1.986851]\n",
      "epoch:27 step:25309 [D loss: 0.595541, acc: 74.22%] [G loss: 1.968768]\n",
      "epoch:27 step:25310 [D loss: 0.600569, acc: 64.84%] [G loss: 2.010110]\n",
      "epoch:27 step:25311 [D loss: 0.646033, acc: 63.28%] [G loss: 2.044250]\n",
      "epoch:27 step:25312 [D loss: 0.643877, acc: 67.19%] [G loss: 1.919439]\n",
      "epoch:27 step:25313 [D loss: 0.619256, acc: 63.28%] [G loss: 1.992493]\n",
      "epoch:27 step:25314 [D loss: 0.593535, acc: 68.75%] [G loss: 2.168893]\n",
      "epoch:27 step:25315 [D loss: 0.593837, acc: 64.06%] [G loss: 1.922140]\n",
      "epoch:27 step:25316 [D loss: 0.674041, acc: 59.38%] [G loss: 1.981649]\n",
      "epoch:27 step:25317 [D loss: 0.682120, acc: 64.06%] [G loss: 1.991440]\n",
      "epoch:27 step:25318 [D loss: 0.610730, acc: 66.41%] [G loss: 1.756466]\n",
      "epoch:27 step:25319 [D loss: 0.739238, acc: 54.69%] [G loss: 1.727985]\n",
      "epoch:27 step:25320 [D loss: 0.740398, acc: 57.03%] [G loss: 1.716189]\n",
      "epoch:27 step:25321 [D loss: 0.651273, acc: 58.59%] [G loss: 1.863381]\n",
      "epoch:27 step:25322 [D loss: 0.612220, acc: 67.19%] [G loss: 1.915438]\n",
      "epoch:27 step:25323 [D loss: 0.634265, acc: 63.28%] [G loss: 1.982846]\n",
      "epoch:27 step:25324 [D loss: 0.606542, acc: 68.75%] [G loss: 2.067920]\n",
      "epoch:27 step:25325 [D loss: 0.684324, acc: 57.81%] [G loss: 1.744917]\n",
      "epoch:27 step:25326 [D loss: 0.704326, acc: 59.38%] [G loss: 1.759927]\n",
      "epoch:27 step:25327 [D loss: 0.639235, acc: 57.81%] [G loss: 1.946339]\n",
      "epoch:27 step:25328 [D loss: 0.645153, acc: 65.62%] [G loss: 1.839944]\n",
      "epoch:27 step:25329 [D loss: 0.702033, acc: 53.91%] [G loss: 1.790331]\n",
      "epoch:27 step:25330 [D loss: 0.670036, acc: 60.94%] [G loss: 1.746052]\n",
      "epoch:27 step:25331 [D loss: 0.726863, acc: 52.34%] [G loss: 1.700051]\n",
      "epoch:27 step:25332 [D loss: 0.586213, acc: 71.09%] [G loss: 1.826217]\n",
      "epoch:27 step:25333 [D loss: 0.679142, acc: 60.94%] [G loss: 1.831804]\n",
      "epoch:27 step:25334 [D loss: 0.606088, acc: 63.28%] [G loss: 1.811219]\n",
      "epoch:27 step:25335 [D loss: 0.597322, acc: 67.19%] [G loss: 1.916342]\n",
      "epoch:27 step:25336 [D loss: 0.674122, acc: 60.94%] [G loss: 1.861840]\n",
      "epoch:27 step:25337 [D loss: 0.676474, acc: 57.81%] [G loss: 1.937033]\n",
      "epoch:27 step:25338 [D loss: 0.661861, acc: 58.59%] [G loss: 1.804497]\n",
      "epoch:27 step:25339 [D loss: 0.618470, acc: 65.62%] [G loss: 1.895330]\n",
      "epoch:27 step:25340 [D loss: 0.647997, acc: 62.50%] [G loss: 1.849746]\n",
      "epoch:27 step:25341 [D loss: 0.575267, acc: 75.00%] [G loss: 2.001283]\n",
      "epoch:27 step:25342 [D loss: 0.632266, acc: 64.84%] [G loss: 1.989061]\n",
      "epoch:27 step:25343 [D loss: 0.655797, acc: 56.25%] [G loss: 1.850172]\n",
      "epoch:27 step:25344 [D loss: 0.662444, acc: 61.72%] [G loss: 1.907659]\n",
      "epoch:27 step:25345 [D loss: 0.650276, acc: 59.38%] [G loss: 1.843402]\n",
      "epoch:27 step:25346 [D loss: 0.577700, acc: 71.88%] [G loss: 1.995253]\n",
      "epoch:27 step:25347 [D loss: 0.615178, acc: 62.50%] [G loss: 2.019093]\n",
      "epoch:27 step:25348 [D loss: 0.601741, acc: 69.53%] [G loss: 2.163441]\n",
      "epoch:27 step:25349 [D loss: 0.631085, acc: 66.41%] [G loss: 1.894518]\n",
      "epoch:27 step:25350 [D loss: 0.697107, acc: 58.59%] [G loss: 1.813563]\n",
      "epoch:27 step:25351 [D loss: 0.655563, acc: 57.81%] [G loss: 1.780802]\n",
      "epoch:27 step:25352 [D loss: 0.585975, acc: 68.75%] [G loss: 1.949855]\n",
      "epoch:27 step:25353 [D loss: 0.617101, acc: 66.41%] [G loss: 2.043326]\n",
      "epoch:27 step:25354 [D loss: 0.604563, acc: 68.75%] [G loss: 2.011636]\n",
      "epoch:27 step:25355 [D loss: 0.582694, acc: 69.53%] [G loss: 1.944301]\n",
      "epoch:27 step:25356 [D loss: 0.650923, acc: 60.16%] [G loss: 1.867334]\n",
      "epoch:27 step:25357 [D loss: 0.701233, acc: 59.38%] [G loss: 1.842808]\n",
      "epoch:27 step:25358 [D loss: 0.639214, acc: 62.50%] [G loss: 1.882464]\n",
      "epoch:27 step:25359 [D loss: 0.634770, acc: 64.06%] [G loss: 1.821025]\n",
      "epoch:27 step:25360 [D loss: 0.653588, acc: 60.16%] [G loss: 1.891849]\n",
      "epoch:27 step:25361 [D loss: 0.660513, acc: 60.16%] [G loss: 1.875637]\n",
      "epoch:27 step:25362 [D loss: 0.673555, acc: 60.94%] [G loss: 1.825260]\n",
      "epoch:27 step:25363 [D loss: 0.642388, acc: 61.72%] [G loss: 1.951360]\n",
      "epoch:27 step:25364 [D loss: 0.637957, acc: 59.38%] [G loss: 1.812445]\n",
      "epoch:27 step:25365 [D loss: 0.682003, acc: 53.91%] [G loss: 1.833805]\n",
      "epoch:27 step:25366 [D loss: 0.667508, acc: 63.28%] [G loss: 1.801888]\n",
      "epoch:27 step:25367 [D loss: 0.640304, acc: 65.62%] [G loss: 1.836606]\n",
      "epoch:27 step:25368 [D loss: 0.691892, acc: 58.59%] [G loss: 2.036303]\n",
      "epoch:27 step:25369 [D loss: 0.599024, acc: 72.66%] [G loss: 2.008889]\n",
      "epoch:27 step:25370 [D loss: 0.679747, acc: 57.81%] [G loss: 1.712268]\n",
      "epoch:27 step:25371 [D loss: 0.656709, acc: 61.72%] [G loss: 1.826845]\n",
      "epoch:27 step:25372 [D loss: 0.689722, acc: 54.69%] [G loss: 1.744751]\n",
      "epoch:27 step:25373 [D loss: 0.607903, acc: 71.09%] [G loss: 1.944430]\n",
      "epoch:27 step:25374 [D loss: 0.664612, acc: 63.28%] [G loss: 1.908650]\n",
      "epoch:27 step:25375 [D loss: 0.608219, acc: 63.28%] [G loss: 2.096975]\n",
      "epoch:27 step:25376 [D loss: 0.659336, acc: 63.28%] [G loss: 2.054633]\n",
      "epoch:27 step:25377 [D loss: 0.683695, acc: 57.03%] [G loss: 1.786637]\n",
      "epoch:27 step:25378 [D loss: 0.695530, acc: 57.03%] [G loss: 1.652053]\n",
      "epoch:27 step:25379 [D loss: 0.655792, acc: 64.84%] [G loss: 1.799749]\n",
      "epoch:27 step:25380 [D loss: 0.691123, acc: 56.25%] [G loss: 1.808022]\n",
      "epoch:27 step:25381 [D loss: 0.626131, acc: 68.75%] [G loss: 1.959984]\n",
      "epoch:27 step:25382 [D loss: 0.622749, acc: 67.19%] [G loss: 1.872739]\n",
      "epoch:27 step:25383 [D loss: 0.632825, acc: 62.50%] [G loss: 1.839724]\n",
      "epoch:27 step:25384 [D loss: 0.635960, acc: 62.50%] [G loss: 1.662797]\n",
      "epoch:27 step:25385 [D loss: 0.715851, acc: 56.25%] [G loss: 1.723759]\n",
      "epoch:27 step:25386 [D loss: 0.616522, acc: 67.97%] [G loss: 1.761839]\n",
      "epoch:27 step:25387 [D loss: 0.614357, acc: 67.97%] [G loss: 1.875746]\n",
      "epoch:27 step:25388 [D loss: 0.623026, acc: 64.06%] [G loss: 2.003228]\n",
      "epoch:27 step:25389 [D loss: 0.679362, acc: 67.19%] [G loss: 1.805902]\n",
      "epoch:27 step:25390 [D loss: 0.680464, acc: 58.59%] [G loss: 1.885390]\n",
      "epoch:27 step:25391 [D loss: 0.635472, acc: 66.41%] [G loss: 1.883727]\n",
      "epoch:27 step:25392 [D loss: 0.591558, acc: 67.97%] [G loss: 2.011722]\n",
      "epoch:27 step:25393 [D loss: 0.677467, acc: 56.25%] [G loss: 1.730998]\n",
      "epoch:27 step:25394 [D loss: 0.684771, acc: 55.47%] [G loss: 1.821609]\n",
      "epoch:27 step:25395 [D loss: 0.614399, acc: 67.97%] [G loss: 1.994475]\n",
      "epoch:27 step:25396 [D loss: 0.696614, acc: 57.03%] [G loss: 1.848975]\n",
      "epoch:27 step:25397 [D loss: 0.654609, acc: 57.81%] [G loss: 1.786934]\n",
      "epoch:27 step:25398 [D loss: 0.649712, acc: 64.84%] [G loss: 1.816097]\n",
      "epoch:27 step:25399 [D loss: 0.649417, acc: 61.72%] [G loss: 1.756907]\n",
      "epoch:27 step:25400 [D loss: 0.641317, acc: 67.19%] [G loss: 1.870158]\n",
      "epoch:27 step:25401 [D loss: 0.678668, acc: 57.81%] [G loss: 1.768676]\n",
      "epoch:27 step:25402 [D loss: 0.613708, acc: 65.62%] [G loss: 1.848561]\n",
      "epoch:27 step:25403 [D loss: 0.643797, acc: 62.50%] [G loss: 1.872303]\n",
      "epoch:27 step:25404 [D loss: 0.635917, acc: 60.94%] [G loss: 1.875649]\n",
      "epoch:27 step:25405 [D loss: 0.666969, acc: 62.50%] [G loss: 1.934417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25406 [D loss: 0.659694, acc: 56.25%] [G loss: 1.944121]\n",
      "epoch:27 step:25407 [D loss: 0.745723, acc: 51.56%] [G loss: 1.719856]\n",
      "epoch:27 step:25408 [D loss: 0.681364, acc: 57.81%] [G loss: 1.762158]\n",
      "epoch:27 step:25409 [D loss: 0.691151, acc: 56.25%] [G loss: 1.791102]\n",
      "epoch:27 step:25410 [D loss: 0.603227, acc: 70.31%] [G loss: 1.960684]\n",
      "epoch:27 step:25411 [D loss: 0.622347, acc: 68.75%] [G loss: 1.899604]\n",
      "epoch:27 step:25412 [D loss: 0.661734, acc: 64.06%] [G loss: 1.994674]\n",
      "epoch:27 step:25413 [D loss: 0.589657, acc: 71.88%] [G loss: 2.107127]\n",
      "epoch:27 step:25414 [D loss: 0.611694, acc: 69.53%] [G loss: 2.180517]\n",
      "epoch:27 step:25415 [D loss: 0.634192, acc: 63.28%] [G loss: 1.978889]\n",
      "epoch:27 step:25416 [D loss: 0.626636, acc: 67.19%] [G loss: 2.023269]\n",
      "epoch:27 step:25417 [D loss: 0.665946, acc: 62.50%] [G loss: 1.879692]\n",
      "epoch:27 step:25418 [D loss: 0.642924, acc: 63.28%] [G loss: 2.073673]\n",
      "epoch:27 step:25419 [D loss: 0.695214, acc: 59.38%] [G loss: 1.855077]\n",
      "epoch:27 step:25420 [D loss: 0.650391, acc: 60.16%] [G loss: 1.864875]\n",
      "epoch:27 step:25421 [D loss: 0.637305, acc: 64.84%] [G loss: 2.036208]\n",
      "epoch:27 step:25422 [D loss: 0.645633, acc: 61.72%] [G loss: 1.941124]\n",
      "epoch:27 step:25423 [D loss: 0.657340, acc: 62.50%] [G loss: 1.862706]\n",
      "epoch:27 step:25424 [D loss: 0.699906, acc: 53.91%] [G loss: 1.689007]\n",
      "epoch:27 step:25425 [D loss: 0.617371, acc: 62.50%] [G loss: 1.855278]\n",
      "epoch:27 step:25426 [D loss: 0.716597, acc: 53.12%] [G loss: 1.784084]\n",
      "epoch:27 step:25427 [D loss: 0.662519, acc: 58.59%] [G loss: 1.815189]\n",
      "epoch:27 step:25428 [D loss: 0.637999, acc: 67.97%] [G loss: 1.812200]\n",
      "epoch:27 step:25429 [D loss: 0.651938, acc: 61.72%] [G loss: 1.891108]\n",
      "epoch:27 step:25430 [D loss: 0.617486, acc: 65.62%] [G loss: 1.900959]\n",
      "epoch:27 step:25431 [D loss: 0.658616, acc: 64.06%] [G loss: 1.832097]\n",
      "epoch:27 step:25432 [D loss: 0.687591, acc: 51.56%] [G loss: 1.852141]\n",
      "epoch:27 step:25433 [D loss: 0.702618, acc: 53.91%] [G loss: 1.800399]\n",
      "epoch:27 step:25434 [D loss: 0.675687, acc: 60.94%] [G loss: 1.759583]\n",
      "epoch:27 step:25435 [D loss: 0.679707, acc: 58.59%] [G loss: 1.751830]\n",
      "epoch:27 step:25436 [D loss: 0.651862, acc: 57.03%] [G loss: 1.783968]\n",
      "epoch:27 step:25437 [D loss: 0.683098, acc: 60.94%] [G loss: 1.796563]\n",
      "epoch:27 step:25438 [D loss: 0.647559, acc: 60.94%] [G loss: 1.849157]\n",
      "epoch:27 step:25439 [D loss: 0.671724, acc: 60.16%] [G loss: 1.821915]\n",
      "epoch:27 step:25440 [D loss: 0.668494, acc: 58.59%] [G loss: 1.691184]\n",
      "epoch:27 step:25441 [D loss: 0.619370, acc: 70.31%] [G loss: 1.755837]\n",
      "epoch:27 step:25442 [D loss: 0.707143, acc: 53.91%] [G loss: 1.793946]\n",
      "epoch:27 step:25443 [D loss: 0.700833, acc: 56.25%] [G loss: 1.733683]\n",
      "epoch:27 step:25444 [D loss: 0.643045, acc: 60.94%] [G loss: 1.699578]\n",
      "epoch:27 step:25445 [D loss: 0.639955, acc: 63.28%] [G loss: 1.901139]\n",
      "epoch:27 step:25446 [D loss: 0.656968, acc: 60.94%] [G loss: 1.824617]\n",
      "epoch:27 step:25447 [D loss: 0.665737, acc: 56.25%] [G loss: 1.734298]\n",
      "epoch:27 step:25448 [D loss: 0.657183, acc: 64.06%] [G loss: 1.880165]\n",
      "epoch:27 step:25449 [D loss: 0.685212, acc: 63.28%] [G loss: 1.748576]\n",
      "epoch:27 step:25450 [D loss: 0.684403, acc: 60.16%] [G loss: 1.893492]\n",
      "epoch:27 step:25451 [D loss: 0.621565, acc: 68.75%] [G loss: 1.847128]\n",
      "epoch:27 step:25452 [D loss: 0.658592, acc: 63.28%] [G loss: 1.788480]\n",
      "epoch:27 step:25453 [D loss: 0.650344, acc: 60.94%] [G loss: 1.830765]\n",
      "epoch:27 step:25454 [D loss: 0.638384, acc: 59.38%] [G loss: 1.845087]\n",
      "epoch:27 step:25455 [D loss: 0.685800, acc: 59.38%] [G loss: 1.965437]\n",
      "epoch:27 step:25456 [D loss: 0.628289, acc: 63.28%] [G loss: 1.853976]\n",
      "epoch:27 step:25457 [D loss: 0.678837, acc: 58.59%] [G loss: 1.792638]\n",
      "epoch:27 step:25458 [D loss: 0.625981, acc: 65.62%] [G loss: 1.858017]\n",
      "epoch:27 step:25459 [D loss: 0.698307, acc: 53.91%] [G loss: 1.736476]\n",
      "epoch:27 step:25460 [D loss: 0.670982, acc: 53.91%] [G loss: 1.817872]\n",
      "epoch:27 step:25461 [D loss: 0.600456, acc: 65.62%] [G loss: 1.842134]\n",
      "epoch:27 step:25462 [D loss: 0.610009, acc: 65.62%] [G loss: 1.976446]\n",
      "epoch:27 step:25463 [D loss: 0.622446, acc: 67.19%] [G loss: 1.793737]\n",
      "epoch:27 step:25464 [D loss: 0.678338, acc: 59.38%] [G loss: 1.853769]\n",
      "epoch:27 step:25465 [D loss: 0.646684, acc: 57.03%] [G loss: 1.759612]\n",
      "epoch:27 step:25466 [D loss: 0.642226, acc: 59.38%] [G loss: 1.787657]\n",
      "epoch:27 step:25467 [D loss: 0.625345, acc: 63.28%] [G loss: 1.733517]\n",
      "epoch:27 step:25468 [D loss: 0.655901, acc: 60.16%] [G loss: 1.916840]\n",
      "epoch:27 step:25469 [D loss: 0.670449, acc: 60.94%] [G loss: 1.800166]\n",
      "epoch:27 step:25470 [D loss: 0.675883, acc: 60.16%] [G loss: 1.848033]\n",
      "epoch:27 step:25471 [D loss: 0.623329, acc: 66.41%] [G loss: 1.772132]\n",
      "epoch:27 step:25472 [D loss: 0.663801, acc: 60.94%] [G loss: 1.816013]\n",
      "epoch:27 step:25473 [D loss: 0.680580, acc: 56.25%] [G loss: 1.750296]\n",
      "epoch:27 step:25474 [D loss: 0.690579, acc: 58.59%] [G loss: 1.691874]\n",
      "epoch:27 step:25475 [D loss: 0.669048, acc: 59.38%] [G loss: 1.769380]\n",
      "epoch:27 step:25476 [D loss: 0.640692, acc: 62.50%] [G loss: 1.714739]\n",
      "epoch:27 step:25477 [D loss: 0.639533, acc: 68.75%] [G loss: 1.792785]\n",
      "epoch:27 step:25478 [D loss: 0.633014, acc: 65.62%] [G loss: 1.860538]\n",
      "epoch:27 step:25479 [D loss: 0.641272, acc: 62.50%] [G loss: 1.783087]\n",
      "epoch:27 step:25480 [D loss: 0.668270, acc: 60.94%] [G loss: 1.833751]\n",
      "epoch:27 step:25481 [D loss: 0.669920, acc: 58.59%] [G loss: 1.749929]\n",
      "epoch:27 step:25482 [D loss: 0.637185, acc: 64.84%] [G loss: 1.865958]\n",
      "epoch:27 step:25483 [D loss: 0.612214, acc: 64.06%] [G loss: 1.886949]\n",
      "epoch:27 step:25484 [D loss: 0.640391, acc: 61.72%] [G loss: 1.918853]\n",
      "epoch:27 step:25485 [D loss: 0.674974, acc: 60.16%] [G loss: 1.845426]\n",
      "epoch:27 step:25486 [D loss: 0.624881, acc: 65.62%] [G loss: 1.982109]\n",
      "epoch:27 step:25487 [D loss: 0.687550, acc: 58.59%] [G loss: 1.825248]\n",
      "epoch:27 step:25488 [D loss: 0.692220, acc: 52.34%] [G loss: 1.778009]\n",
      "epoch:27 step:25489 [D loss: 0.645900, acc: 63.28%] [G loss: 1.863669]\n",
      "epoch:27 step:25490 [D loss: 0.661924, acc: 57.81%] [G loss: 1.795057]\n",
      "epoch:27 step:25491 [D loss: 0.653084, acc: 60.94%] [G loss: 1.849813]\n",
      "epoch:27 step:25492 [D loss: 0.647310, acc: 66.41%] [G loss: 1.875573]\n",
      "epoch:27 step:25493 [D loss: 0.621782, acc: 68.75%] [G loss: 2.098867]\n",
      "epoch:27 step:25494 [D loss: 0.644133, acc: 57.81%] [G loss: 1.804238]\n",
      "epoch:27 step:25495 [D loss: 0.642806, acc: 64.84%] [G loss: 1.863229]\n",
      "epoch:27 step:25496 [D loss: 0.646823, acc: 70.31%] [G loss: 1.953529]\n",
      "epoch:27 step:25497 [D loss: 0.642771, acc: 64.84%] [G loss: 2.052556]\n",
      "epoch:27 step:25498 [D loss: 0.671630, acc: 58.59%] [G loss: 1.881596]\n",
      "epoch:27 step:25499 [D loss: 0.659075, acc: 64.06%] [G loss: 1.721740]\n",
      "epoch:27 step:25500 [D loss: 0.704325, acc: 52.34%] [G loss: 1.841241]\n",
      "epoch:27 step:25501 [D loss: 0.658381, acc: 55.47%] [G loss: 1.736071]\n",
      "epoch:27 step:25502 [D loss: 0.657634, acc: 61.72%] [G loss: 1.820504]\n",
      "epoch:27 step:25503 [D loss: 0.696643, acc: 55.47%] [G loss: 1.740551]\n",
      "epoch:27 step:25504 [D loss: 0.644791, acc: 57.03%] [G loss: 1.891987]\n",
      "epoch:27 step:25505 [D loss: 0.603451, acc: 69.53%] [G loss: 1.780104]\n",
      "epoch:27 step:25506 [D loss: 0.590379, acc: 67.19%] [G loss: 2.042038]\n",
      "epoch:27 step:25507 [D loss: 0.590432, acc: 72.66%] [G loss: 1.974275]\n",
      "epoch:27 step:25508 [D loss: 0.597263, acc: 67.97%] [G loss: 2.033727]\n",
      "epoch:27 step:25509 [D loss: 0.658088, acc: 61.72%] [G loss: 1.804150]\n",
      "epoch:27 step:25510 [D loss: 0.686702, acc: 56.25%] [G loss: 1.727126]\n",
      "epoch:27 step:25511 [D loss: 0.660235, acc: 63.28%] [G loss: 1.931273]\n",
      "epoch:27 step:25512 [D loss: 0.679403, acc: 57.81%] [G loss: 1.657324]\n",
      "epoch:27 step:25513 [D loss: 0.604193, acc: 71.09%] [G loss: 1.680630]\n",
      "epoch:27 step:25514 [D loss: 0.664790, acc: 56.25%] [G loss: 1.914409]\n",
      "epoch:27 step:25515 [D loss: 0.641891, acc: 64.84%] [G loss: 2.008577]\n",
      "epoch:27 step:25516 [D loss: 0.644631, acc: 62.50%] [G loss: 1.865862]\n",
      "epoch:27 step:25517 [D loss: 0.606550, acc: 71.88%] [G loss: 1.947711]\n",
      "epoch:27 step:25518 [D loss: 0.618800, acc: 67.97%] [G loss: 2.174956]\n",
      "epoch:27 step:25519 [D loss: 0.670804, acc: 61.72%] [G loss: 1.809143]\n",
      "epoch:27 step:25520 [D loss: 0.714107, acc: 53.91%] [G loss: 1.835489]\n",
      "epoch:27 step:25521 [D loss: 0.632606, acc: 63.28%] [G loss: 1.904170]\n",
      "epoch:27 step:25522 [D loss: 0.688282, acc: 60.94%] [G loss: 1.861257]\n",
      "epoch:27 step:25523 [D loss: 0.673908, acc: 57.81%] [G loss: 1.799896]\n",
      "epoch:27 step:25524 [D loss: 0.658528, acc: 63.28%] [G loss: 1.896553]\n",
      "epoch:27 step:25525 [D loss: 0.632477, acc: 64.84%] [G loss: 1.753096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25526 [D loss: 0.695044, acc: 58.59%] [G loss: 1.731890]\n",
      "epoch:27 step:25527 [D loss: 0.698198, acc: 51.56%] [G loss: 1.706682]\n",
      "epoch:27 step:25528 [D loss: 0.616023, acc: 67.97%] [G loss: 1.887306]\n",
      "epoch:27 step:25529 [D loss: 0.635643, acc: 60.16%] [G loss: 1.904095]\n",
      "epoch:27 step:25530 [D loss: 0.592888, acc: 68.75%] [G loss: 2.209512]\n",
      "epoch:27 step:25531 [D loss: 0.581681, acc: 71.09%] [G loss: 2.264556]\n",
      "epoch:27 step:25532 [D loss: 0.634094, acc: 63.28%] [G loss: 1.875102]\n",
      "epoch:27 step:25533 [D loss: 0.698351, acc: 58.59%] [G loss: 1.795010]\n",
      "epoch:27 step:25534 [D loss: 0.640324, acc: 67.19%] [G loss: 1.897457]\n",
      "epoch:27 step:25535 [D loss: 0.630608, acc: 62.50%] [G loss: 1.918564]\n",
      "epoch:27 step:25536 [D loss: 0.656729, acc: 63.28%] [G loss: 1.922257]\n",
      "epoch:27 step:25537 [D loss: 0.662375, acc: 63.28%] [G loss: 1.943411]\n",
      "epoch:27 step:25538 [D loss: 0.651833, acc: 60.94%] [G loss: 1.823302]\n",
      "epoch:27 step:25539 [D loss: 0.657607, acc: 63.28%] [G loss: 1.845703]\n",
      "epoch:27 step:25540 [D loss: 0.646473, acc: 64.06%] [G loss: 1.763100]\n",
      "epoch:27 step:25541 [D loss: 0.616456, acc: 64.84%] [G loss: 2.025872]\n",
      "epoch:27 step:25542 [D loss: 0.635066, acc: 64.84%] [G loss: 1.898925]\n",
      "epoch:27 step:25543 [D loss: 0.664132, acc: 58.59%] [G loss: 1.861386]\n",
      "epoch:27 step:25544 [D loss: 0.638544, acc: 60.94%] [G loss: 1.951428]\n",
      "epoch:27 step:25545 [D loss: 0.622950, acc: 65.62%] [G loss: 1.962910]\n",
      "epoch:27 step:25546 [D loss: 0.731239, acc: 49.22%] [G loss: 1.839116]\n",
      "epoch:27 step:25547 [D loss: 0.652490, acc: 61.72%] [G loss: 2.034829]\n",
      "epoch:27 step:25548 [D loss: 0.681701, acc: 57.81%] [G loss: 1.789383]\n",
      "epoch:27 step:25549 [D loss: 0.731595, acc: 53.12%] [G loss: 1.729527]\n",
      "epoch:27 step:25550 [D loss: 0.685922, acc: 53.91%] [G loss: 1.707389]\n",
      "epoch:27 step:25551 [D loss: 0.714622, acc: 48.44%] [G loss: 1.675680]\n",
      "epoch:27 step:25552 [D loss: 0.613199, acc: 65.62%] [G loss: 1.822109]\n",
      "epoch:27 step:25553 [D loss: 0.624898, acc: 64.84%] [G loss: 1.841646]\n",
      "epoch:27 step:25554 [D loss: 0.608442, acc: 62.50%] [G loss: 1.898754]\n",
      "epoch:27 step:25555 [D loss: 0.666407, acc: 59.38%] [G loss: 1.928698]\n",
      "epoch:27 step:25556 [D loss: 0.700045, acc: 57.03%] [G loss: 1.756258]\n",
      "epoch:27 step:25557 [D loss: 0.652729, acc: 58.59%] [G loss: 1.835900]\n",
      "epoch:27 step:25558 [D loss: 0.617322, acc: 63.28%] [G loss: 1.820854]\n",
      "epoch:27 step:25559 [D loss: 0.671321, acc: 62.50%] [G loss: 1.828418]\n",
      "epoch:27 step:25560 [D loss: 0.658717, acc: 61.72%] [G loss: 1.898591]\n",
      "epoch:27 step:25561 [D loss: 0.640762, acc: 61.72%] [G loss: 1.822443]\n",
      "epoch:27 step:25562 [D loss: 0.632304, acc: 67.97%] [G loss: 1.987833]\n",
      "epoch:27 step:25563 [D loss: 0.621923, acc: 64.84%] [G loss: 2.165181]\n",
      "epoch:27 step:25564 [D loss: 0.673324, acc: 58.59%] [G loss: 1.841789]\n",
      "epoch:27 step:25565 [D loss: 0.649935, acc: 60.94%] [G loss: 1.914475]\n",
      "epoch:27 step:25566 [D loss: 0.668891, acc: 54.69%] [G loss: 1.827353]\n",
      "epoch:27 step:25567 [D loss: 0.694630, acc: 53.12%] [G loss: 1.875394]\n",
      "epoch:27 step:25568 [D loss: 0.637785, acc: 57.81%] [G loss: 1.828199]\n",
      "epoch:27 step:25569 [D loss: 0.621265, acc: 67.97%] [G loss: 1.755117]\n",
      "epoch:27 step:25570 [D loss: 0.690323, acc: 57.81%] [G loss: 1.844971]\n",
      "epoch:27 step:25571 [D loss: 0.631164, acc: 65.62%] [G loss: 1.948636]\n",
      "epoch:27 step:25572 [D loss: 0.637266, acc: 64.84%] [G loss: 1.940917]\n",
      "epoch:27 step:25573 [D loss: 0.640345, acc: 60.16%] [G loss: 2.095188]\n",
      "epoch:27 step:25574 [D loss: 0.648640, acc: 61.72%] [G loss: 1.993536]\n",
      "epoch:27 step:25575 [D loss: 0.604142, acc: 68.75%] [G loss: 2.111826]\n",
      "epoch:27 step:25576 [D loss: 0.679493, acc: 53.12%] [G loss: 1.981791]\n",
      "epoch:27 step:25577 [D loss: 0.658631, acc: 64.06%] [G loss: 1.747692]\n",
      "epoch:27 step:25578 [D loss: 0.674701, acc: 62.50%] [G loss: 1.703663]\n",
      "epoch:27 step:25579 [D loss: 0.605836, acc: 71.09%] [G loss: 1.940458]\n",
      "epoch:27 step:25580 [D loss: 0.682181, acc: 55.47%] [G loss: 1.885432]\n",
      "epoch:27 step:25581 [D loss: 0.626470, acc: 60.16%] [G loss: 1.744789]\n",
      "epoch:27 step:25582 [D loss: 0.666722, acc: 58.59%] [G loss: 1.734156]\n",
      "epoch:27 step:25583 [D loss: 0.638272, acc: 64.06%] [G loss: 1.830320]\n",
      "epoch:27 step:25584 [D loss: 0.700097, acc: 57.81%] [G loss: 1.875292]\n",
      "epoch:27 step:25585 [D loss: 0.593832, acc: 70.31%] [G loss: 1.875765]\n",
      "epoch:27 step:25586 [D loss: 0.608441, acc: 68.75%] [G loss: 2.026588]\n",
      "epoch:27 step:25587 [D loss: 0.680856, acc: 57.81%] [G loss: 1.913287]\n",
      "epoch:27 step:25588 [D loss: 0.664385, acc: 62.50%] [G loss: 1.896959]\n",
      "epoch:27 step:25589 [D loss: 0.629380, acc: 66.41%] [G loss: 1.919582]\n",
      "epoch:27 step:25590 [D loss: 0.665982, acc: 62.50%] [G loss: 1.908597]\n",
      "epoch:27 step:25591 [D loss: 0.662850, acc: 60.94%] [G loss: 1.900639]\n",
      "epoch:27 step:25592 [D loss: 0.641120, acc: 65.62%] [G loss: 1.893979]\n",
      "epoch:27 step:25593 [D loss: 0.660739, acc: 62.50%] [G loss: 1.804600]\n",
      "epoch:27 step:25594 [D loss: 0.686175, acc: 63.28%] [G loss: 1.953877]\n",
      "epoch:27 step:25595 [D loss: 0.667342, acc: 61.72%] [G loss: 1.882572]\n",
      "epoch:27 step:25596 [D loss: 0.650989, acc: 60.16%] [G loss: 1.836759]\n",
      "epoch:27 step:25597 [D loss: 0.624217, acc: 66.41%] [G loss: 1.952995]\n",
      "epoch:27 step:25598 [D loss: 0.671788, acc: 60.16%] [G loss: 1.999622]\n",
      "epoch:27 step:25599 [D loss: 0.599294, acc: 67.19%] [G loss: 1.938274]\n",
      "epoch:27 step:25600 [D loss: 0.708626, acc: 52.34%] [G loss: 1.746210]\n",
      "epoch:27 step:25601 [D loss: 0.664018, acc: 57.81%] [G loss: 1.790781]\n",
      "epoch:27 step:25602 [D loss: 0.683071, acc: 58.59%] [G loss: 2.007399]\n",
      "epoch:27 step:25603 [D loss: 0.688910, acc: 59.38%] [G loss: 1.826462]\n",
      "epoch:27 step:25604 [D loss: 0.672760, acc: 60.94%] [G loss: 1.949850]\n",
      "epoch:27 step:25605 [D loss: 0.653864, acc: 59.38%] [G loss: 1.772283]\n",
      "epoch:27 step:25606 [D loss: 0.640600, acc: 57.03%] [G loss: 1.802120]\n",
      "epoch:27 step:25607 [D loss: 0.662000, acc: 61.72%] [G loss: 1.736675]\n",
      "epoch:27 step:25608 [D loss: 0.617976, acc: 69.53%] [G loss: 1.915104]\n",
      "epoch:27 step:25609 [D loss: 0.621428, acc: 64.06%] [G loss: 1.860207]\n",
      "epoch:27 step:25610 [D loss: 0.672517, acc: 64.84%] [G loss: 1.921087]\n",
      "epoch:27 step:25611 [D loss: 0.594824, acc: 68.75%] [G loss: 2.165464]\n",
      "epoch:27 step:25612 [D loss: 0.614729, acc: 65.62%] [G loss: 1.919602]\n",
      "epoch:27 step:25613 [D loss: 0.583450, acc: 70.31%] [G loss: 2.127481]\n",
      "epoch:27 step:25614 [D loss: 0.585909, acc: 71.09%] [G loss: 1.948306]\n",
      "epoch:27 step:25615 [D loss: 0.701876, acc: 53.12%] [G loss: 1.840378]\n",
      "epoch:27 step:25616 [D loss: 0.653229, acc: 63.28%] [G loss: 1.905822]\n",
      "epoch:27 step:25617 [D loss: 0.666645, acc: 59.38%] [G loss: 1.852644]\n",
      "epoch:27 step:25618 [D loss: 0.644399, acc: 63.28%] [G loss: 1.804453]\n",
      "epoch:27 step:25619 [D loss: 0.655014, acc: 59.38%] [G loss: 1.771383]\n",
      "epoch:27 step:25620 [D loss: 0.670844, acc: 62.50%] [G loss: 2.011440]\n",
      "epoch:27 step:25621 [D loss: 0.658421, acc: 56.25%] [G loss: 1.842384]\n",
      "epoch:27 step:25622 [D loss: 0.658225, acc: 60.94%] [G loss: 1.778707]\n",
      "epoch:27 step:25623 [D loss: 0.713758, acc: 57.03%] [G loss: 1.903466]\n",
      "epoch:27 step:25624 [D loss: 0.694455, acc: 50.78%] [G loss: 1.925963]\n",
      "epoch:27 step:25625 [D loss: 0.728418, acc: 57.03%] [G loss: 1.821663]\n",
      "epoch:27 step:25626 [D loss: 0.631284, acc: 64.06%] [G loss: 1.778455]\n",
      "epoch:27 step:25627 [D loss: 0.632042, acc: 60.16%] [G loss: 1.923013]\n",
      "epoch:27 step:25628 [D loss: 0.670950, acc: 61.72%] [G loss: 1.725658]\n",
      "epoch:27 step:25629 [D loss: 0.627662, acc: 65.62%] [G loss: 1.963722]\n",
      "epoch:27 step:25630 [D loss: 0.638580, acc: 67.19%] [G loss: 1.960270]\n",
      "epoch:27 step:25631 [D loss: 0.617376, acc: 69.53%] [G loss: 1.837718]\n",
      "epoch:27 step:25632 [D loss: 0.639363, acc: 65.62%] [G loss: 1.907526]\n",
      "epoch:27 step:25633 [D loss: 0.621988, acc: 61.72%] [G loss: 2.079421]\n",
      "epoch:27 step:25634 [D loss: 0.614807, acc: 68.75%] [G loss: 1.976604]\n",
      "epoch:27 step:25635 [D loss: 0.634364, acc: 60.94%] [G loss: 1.935409]\n",
      "epoch:27 step:25636 [D loss: 0.668114, acc: 64.06%] [G loss: 1.911299]\n",
      "epoch:27 step:25637 [D loss: 0.573876, acc: 75.78%] [G loss: 1.901249]\n",
      "epoch:27 step:25638 [D loss: 0.638165, acc: 67.19%] [G loss: 1.800571]\n",
      "epoch:27 step:25639 [D loss: 0.613652, acc: 65.62%] [G loss: 2.006908]\n",
      "epoch:27 step:25640 [D loss: 0.684627, acc: 60.94%] [G loss: 1.855166]\n",
      "epoch:27 step:25641 [D loss: 0.695347, acc: 53.91%] [G loss: 1.937683]\n",
      "epoch:27 step:25642 [D loss: 0.638566, acc: 61.72%] [G loss: 1.778897]\n",
      "epoch:27 step:25643 [D loss: 0.634741, acc: 64.06%] [G loss: 1.940710]\n",
      "epoch:27 step:25644 [D loss: 0.592041, acc: 69.53%] [G loss: 2.060971]\n",
      "epoch:27 step:25645 [D loss: 0.623314, acc: 71.09%] [G loss: 2.030655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25646 [D loss: 0.605300, acc: 67.97%] [G loss: 2.203336]\n",
      "epoch:27 step:25647 [D loss: 0.694287, acc: 54.69%] [G loss: 1.940445]\n",
      "epoch:27 step:25648 [D loss: 0.670041, acc: 60.16%] [G loss: 1.805655]\n",
      "epoch:27 step:25649 [D loss: 0.639238, acc: 66.41%] [G loss: 1.913677]\n",
      "epoch:27 step:25650 [D loss: 0.636191, acc: 62.50%] [G loss: 1.921536]\n",
      "epoch:27 step:25651 [D loss: 0.659222, acc: 65.62%] [G loss: 1.960938]\n",
      "epoch:27 step:25652 [D loss: 0.695485, acc: 60.16%] [G loss: 1.970092]\n",
      "epoch:27 step:25653 [D loss: 0.631629, acc: 68.75%] [G loss: 1.910394]\n",
      "epoch:27 step:25654 [D loss: 0.718152, acc: 51.56%] [G loss: 1.693697]\n",
      "epoch:27 step:25655 [D loss: 0.682126, acc: 56.25%] [G loss: 1.809214]\n",
      "epoch:27 step:25656 [D loss: 0.665420, acc: 60.94%] [G loss: 1.856517]\n",
      "epoch:27 step:25657 [D loss: 0.641669, acc: 60.16%] [G loss: 1.987314]\n",
      "epoch:27 step:25658 [D loss: 0.671843, acc: 67.19%] [G loss: 2.014770]\n",
      "epoch:27 step:25659 [D loss: 0.648628, acc: 64.06%] [G loss: 1.941564]\n",
      "epoch:27 step:25660 [D loss: 0.638693, acc: 64.06%] [G loss: 1.840192]\n",
      "epoch:27 step:25661 [D loss: 0.653640, acc: 59.38%] [G loss: 1.789243]\n",
      "epoch:27 step:25662 [D loss: 0.699524, acc: 56.25%] [G loss: 1.829297]\n",
      "epoch:27 step:25663 [D loss: 0.685390, acc: 56.25%] [G loss: 1.859434]\n",
      "epoch:27 step:25664 [D loss: 0.638132, acc: 60.16%] [G loss: 1.823768]\n",
      "epoch:27 step:25665 [D loss: 0.633734, acc: 64.84%] [G loss: 1.738127]\n",
      "epoch:27 step:25666 [D loss: 0.662583, acc: 62.50%] [G loss: 1.850578]\n",
      "epoch:27 step:25667 [D loss: 0.640305, acc: 68.75%] [G loss: 1.776066]\n",
      "epoch:27 step:25668 [D loss: 0.696725, acc: 56.25%] [G loss: 1.956756]\n",
      "epoch:27 step:25669 [D loss: 0.635295, acc: 63.28%] [G loss: 1.906365]\n",
      "epoch:27 step:25670 [D loss: 0.583040, acc: 71.09%] [G loss: 1.924251]\n",
      "epoch:27 step:25671 [D loss: 0.690005, acc: 54.69%] [G loss: 1.815523]\n",
      "epoch:27 step:25672 [D loss: 0.705789, acc: 50.78%] [G loss: 1.596748]\n",
      "epoch:27 step:25673 [D loss: 0.628237, acc: 66.41%] [G loss: 1.849690]\n",
      "epoch:27 step:25674 [D loss: 0.661522, acc: 60.16%] [G loss: 1.784290]\n",
      "epoch:27 step:25675 [D loss: 0.627863, acc: 64.06%] [G loss: 1.855056]\n",
      "epoch:27 step:25676 [D loss: 0.688003, acc: 55.47%] [G loss: 1.755010]\n",
      "epoch:27 step:25677 [D loss: 0.668781, acc: 58.59%] [G loss: 1.790297]\n",
      "epoch:27 step:25678 [D loss: 0.661444, acc: 60.16%] [G loss: 1.710448]\n",
      "epoch:27 step:25679 [D loss: 0.655505, acc: 63.28%] [G loss: 1.799352]\n",
      "epoch:27 step:25680 [D loss: 0.625803, acc: 63.28%] [G loss: 1.918621]\n",
      "epoch:27 step:25681 [D loss: 0.666907, acc: 57.03%] [G loss: 1.698307]\n",
      "epoch:27 step:25682 [D loss: 0.644203, acc: 62.50%] [G loss: 1.717926]\n",
      "epoch:27 step:25683 [D loss: 0.656389, acc: 58.59%] [G loss: 1.937966]\n",
      "epoch:27 step:25684 [D loss: 0.613933, acc: 66.41%] [G loss: 1.865669]\n",
      "epoch:27 step:25685 [D loss: 0.701355, acc: 52.34%] [G loss: 1.648374]\n",
      "epoch:27 step:25686 [D loss: 0.649019, acc: 56.25%] [G loss: 1.702164]\n",
      "epoch:27 step:25687 [D loss: 0.696538, acc: 57.81%] [G loss: 1.733942]\n",
      "epoch:27 step:25688 [D loss: 0.682705, acc: 51.56%] [G loss: 1.730798]\n",
      "epoch:27 step:25689 [D loss: 0.690318, acc: 59.38%] [G loss: 1.752971]\n",
      "epoch:27 step:25690 [D loss: 0.677981, acc: 59.38%] [G loss: 1.671114]\n",
      "epoch:27 step:25691 [D loss: 0.647967, acc: 64.84%] [G loss: 1.792976]\n",
      "epoch:27 step:25692 [D loss: 0.651507, acc: 57.81%] [G loss: 1.793125]\n",
      "epoch:27 step:25693 [D loss: 0.682278, acc: 52.34%] [G loss: 1.763442]\n",
      "epoch:27 step:25694 [D loss: 0.674521, acc: 60.94%] [G loss: 1.830424]\n",
      "epoch:27 step:25695 [D loss: 0.684677, acc: 51.56%] [G loss: 1.621251]\n",
      "epoch:27 step:25696 [D loss: 0.663060, acc: 58.59%] [G loss: 1.736415]\n",
      "epoch:27 step:25697 [D loss: 0.631348, acc: 66.41%] [G loss: 1.881813]\n",
      "epoch:27 step:25698 [D loss: 0.633457, acc: 63.28%] [G loss: 1.808641]\n",
      "epoch:27 step:25699 [D loss: 0.639517, acc: 67.19%] [G loss: 1.841815]\n",
      "epoch:27 step:25700 [D loss: 0.634750, acc: 65.62%] [G loss: 1.808801]\n",
      "epoch:27 step:25701 [D loss: 0.612249, acc: 69.53%] [G loss: 1.840985]\n",
      "epoch:27 step:25702 [D loss: 0.613823, acc: 66.41%] [G loss: 1.812474]\n",
      "epoch:27 step:25703 [D loss: 0.657663, acc: 66.41%] [G loss: 1.862757]\n",
      "epoch:27 step:25704 [D loss: 0.623528, acc: 67.19%] [G loss: 1.945435]\n",
      "epoch:27 step:25705 [D loss: 0.607122, acc: 65.62%] [G loss: 2.045803]\n",
      "epoch:27 step:25706 [D loss: 0.632447, acc: 60.16%] [G loss: 1.868760]\n",
      "epoch:27 step:25707 [D loss: 0.607121, acc: 65.62%] [G loss: 1.865817]\n",
      "epoch:27 step:25708 [D loss: 0.666302, acc: 62.50%] [G loss: 1.875136]\n",
      "epoch:27 step:25709 [D loss: 0.659957, acc: 60.94%] [G loss: 1.890490]\n",
      "epoch:27 step:25710 [D loss: 0.669770, acc: 59.38%] [G loss: 1.922531]\n",
      "epoch:27 step:25711 [D loss: 0.591723, acc: 68.75%] [G loss: 1.895226]\n",
      "epoch:27 step:25712 [D loss: 0.607150, acc: 68.75%] [G loss: 1.920064]\n",
      "epoch:27 step:25713 [D loss: 0.603210, acc: 67.19%] [G loss: 2.007305]\n",
      "epoch:27 step:25714 [D loss: 0.672531, acc: 57.81%] [G loss: 1.930233]\n",
      "epoch:27 step:25715 [D loss: 0.604544, acc: 68.75%] [G loss: 1.972826]\n",
      "epoch:27 step:25716 [D loss: 0.665756, acc: 58.59%] [G loss: 1.983352]\n",
      "epoch:27 step:25717 [D loss: 0.685799, acc: 56.25%] [G loss: 1.814167]\n",
      "epoch:27 step:25718 [D loss: 0.639132, acc: 60.16%] [G loss: 1.813200]\n",
      "epoch:27 step:25719 [D loss: 0.687176, acc: 56.25%] [G loss: 1.802127]\n",
      "epoch:27 step:25720 [D loss: 0.651010, acc: 58.59%] [G loss: 1.977084]\n",
      "epoch:27 step:25721 [D loss: 0.675285, acc: 58.59%] [G loss: 1.712794]\n",
      "epoch:27 step:25722 [D loss: 0.714131, acc: 57.03%] [G loss: 1.871339]\n",
      "epoch:27 step:25723 [D loss: 0.674852, acc: 60.94%] [G loss: 1.860522]\n",
      "epoch:27 step:25724 [D loss: 0.667276, acc: 57.03%] [G loss: 1.876523]\n",
      "epoch:27 step:25725 [D loss: 0.658694, acc: 64.06%] [G loss: 1.875310]\n",
      "epoch:27 step:25726 [D loss: 0.698184, acc: 57.03%] [G loss: 1.815505]\n",
      "epoch:27 step:25727 [D loss: 0.617488, acc: 67.97%] [G loss: 1.985832]\n",
      "epoch:27 step:25728 [D loss: 0.644331, acc: 63.28%] [G loss: 1.985046]\n",
      "epoch:27 step:25729 [D loss: 0.583002, acc: 71.09%] [G loss: 1.916565]\n",
      "epoch:27 step:25730 [D loss: 0.632097, acc: 67.19%] [G loss: 1.900354]\n",
      "epoch:27 step:25731 [D loss: 0.686202, acc: 60.16%] [G loss: 1.868340]\n",
      "epoch:27 step:25732 [D loss: 0.631820, acc: 62.50%] [G loss: 1.803360]\n",
      "epoch:27 step:25733 [D loss: 0.599574, acc: 63.28%] [G loss: 1.883467]\n",
      "epoch:27 step:25734 [D loss: 0.671265, acc: 59.38%] [G loss: 1.933084]\n",
      "epoch:27 step:25735 [D loss: 0.648181, acc: 60.16%] [G loss: 1.863359]\n",
      "epoch:27 step:25736 [D loss: 0.720294, acc: 54.69%] [G loss: 1.747660]\n",
      "epoch:27 step:25737 [D loss: 0.654236, acc: 61.72%] [G loss: 1.702068]\n",
      "epoch:27 step:25738 [D loss: 0.652760, acc: 60.16%] [G loss: 1.731479]\n",
      "epoch:27 step:25739 [D loss: 0.665051, acc: 55.47%] [G loss: 1.736061]\n",
      "epoch:27 step:25740 [D loss: 0.635973, acc: 65.62%] [G loss: 1.756995]\n",
      "epoch:27 step:25741 [D loss: 0.674646, acc: 57.03%] [G loss: 1.945273]\n",
      "epoch:27 step:25742 [D loss: 0.664457, acc: 57.03%] [G loss: 1.792624]\n",
      "epoch:27 step:25743 [D loss: 0.648645, acc: 59.38%] [G loss: 1.724539]\n",
      "epoch:27 step:25744 [D loss: 0.645053, acc: 61.72%] [G loss: 1.759330]\n",
      "epoch:27 step:25745 [D loss: 0.640656, acc: 58.59%] [G loss: 1.856880]\n",
      "epoch:27 step:25746 [D loss: 0.608084, acc: 67.97%] [G loss: 1.908492]\n",
      "epoch:27 step:25747 [D loss: 0.699879, acc: 51.56%] [G loss: 1.675563]\n",
      "epoch:27 step:25748 [D loss: 0.699338, acc: 55.47%] [G loss: 1.916777]\n",
      "epoch:27 step:25749 [D loss: 0.590534, acc: 70.31%] [G loss: 1.801625]\n",
      "epoch:27 step:25750 [D loss: 0.643919, acc: 64.06%] [G loss: 1.897789]\n",
      "epoch:27 step:25751 [D loss: 0.616795, acc: 64.84%] [G loss: 1.853000]\n",
      "epoch:27 step:25752 [D loss: 0.607384, acc: 69.53%] [G loss: 1.854907]\n",
      "epoch:27 step:25753 [D loss: 0.649019, acc: 58.59%] [G loss: 1.852639]\n",
      "epoch:27 step:25754 [D loss: 0.632665, acc: 65.62%] [G loss: 1.909637]\n",
      "epoch:27 step:25755 [D loss: 0.618931, acc: 70.31%] [G loss: 2.150084]\n",
      "epoch:27 step:25756 [D loss: 0.643830, acc: 67.19%] [G loss: 2.019093]\n",
      "epoch:27 step:25757 [D loss: 0.674426, acc: 62.50%] [G loss: 1.724511]\n",
      "epoch:27 step:25758 [D loss: 0.658888, acc: 59.38%] [G loss: 1.776705]\n",
      "epoch:27 step:25759 [D loss: 0.651417, acc: 62.50%] [G loss: 1.740684]\n",
      "epoch:27 step:25760 [D loss: 0.633131, acc: 61.72%] [G loss: 1.912774]\n",
      "epoch:27 step:25761 [D loss: 0.641763, acc: 59.38%] [G loss: 1.992600]\n",
      "epoch:27 step:25762 [D loss: 0.645648, acc: 64.06%] [G loss: 1.808462]\n",
      "epoch:27 step:25763 [D loss: 0.615825, acc: 66.41%] [G loss: 1.961165]\n",
      "epoch:27 step:25764 [D loss: 0.650169, acc: 62.50%] [G loss: 2.001345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25765 [D loss: 0.626555, acc: 63.28%] [G loss: 1.880702]\n",
      "epoch:27 step:25766 [D loss: 0.616954, acc: 66.41%] [G loss: 2.042142]\n",
      "epoch:27 step:25767 [D loss: 0.666033, acc: 63.28%] [G loss: 1.960589]\n",
      "epoch:27 step:25768 [D loss: 0.626706, acc: 68.75%] [G loss: 2.213122]\n",
      "epoch:27 step:25769 [D loss: 0.757770, acc: 56.25%] [G loss: 2.005842]\n",
      "epoch:27 step:25770 [D loss: 0.588228, acc: 68.75%] [G loss: 2.121993]\n",
      "epoch:27 step:25771 [D loss: 0.643738, acc: 65.62%] [G loss: 1.967302]\n",
      "epoch:27 step:25772 [D loss: 0.675785, acc: 62.50%] [G loss: 1.767636]\n",
      "epoch:27 step:25773 [D loss: 0.659526, acc: 61.72%] [G loss: 1.933409]\n",
      "epoch:27 step:25774 [D loss: 0.648206, acc: 58.59%] [G loss: 1.797977]\n",
      "epoch:27 step:25775 [D loss: 0.617592, acc: 64.06%] [G loss: 1.895302]\n",
      "epoch:27 step:25776 [D loss: 0.683399, acc: 54.69%] [G loss: 1.749667]\n",
      "epoch:27 step:25777 [D loss: 0.649362, acc: 61.72%] [G loss: 1.842160]\n",
      "epoch:27 step:25778 [D loss: 0.606117, acc: 68.75%] [G loss: 1.958909]\n",
      "epoch:27 step:25779 [D loss: 0.641900, acc: 65.62%] [G loss: 2.031984]\n",
      "epoch:27 step:25780 [D loss: 0.593001, acc: 71.09%] [G loss: 1.882269]\n",
      "epoch:27 step:25781 [D loss: 0.700821, acc: 54.69%] [G loss: 1.779529]\n",
      "epoch:27 step:25782 [D loss: 0.688708, acc: 56.25%] [G loss: 1.983959]\n",
      "epoch:27 step:25783 [D loss: 0.654019, acc: 65.62%] [G loss: 1.840708]\n",
      "epoch:27 step:25784 [D loss: 0.647002, acc: 61.72%] [G loss: 1.750638]\n",
      "epoch:27 step:25785 [D loss: 0.621794, acc: 66.41%] [G loss: 1.910981]\n",
      "epoch:27 step:25786 [D loss: 0.619217, acc: 67.19%] [G loss: 1.904920]\n",
      "epoch:27 step:25787 [D loss: 0.602922, acc: 69.53%] [G loss: 2.120647]\n",
      "epoch:27 step:25788 [D loss: 0.702477, acc: 60.16%] [G loss: 1.869988]\n",
      "epoch:27 step:25789 [D loss: 0.654971, acc: 61.72%] [G loss: 1.876110]\n",
      "epoch:27 step:25790 [D loss: 0.591633, acc: 68.75%] [G loss: 1.954857]\n",
      "epoch:27 step:25791 [D loss: 0.645353, acc: 68.75%] [G loss: 1.862124]\n",
      "epoch:27 step:25792 [D loss: 0.634851, acc: 67.19%] [G loss: 1.891837]\n",
      "epoch:27 step:25793 [D loss: 0.679493, acc: 62.50%] [G loss: 2.170748]\n",
      "epoch:27 step:25794 [D loss: 0.600073, acc: 69.53%] [G loss: 2.019053]\n",
      "epoch:27 step:25795 [D loss: 0.627923, acc: 64.06%] [G loss: 2.119202]\n",
      "epoch:27 step:25796 [D loss: 0.680561, acc: 57.81%] [G loss: 1.895781]\n",
      "epoch:27 step:25797 [D loss: 0.641107, acc: 70.31%] [G loss: 2.015027]\n",
      "epoch:27 step:25798 [D loss: 0.592792, acc: 71.88%] [G loss: 2.089898]\n",
      "epoch:27 step:25799 [D loss: 0.687303, acc: 58.59%] [G loss: 1.787128]\n",
      "epoch:27 step:25800 [D loss: 0.712746, acc: 57.81%] [G loss: 1.662349]\n",
      "epoch:27 step:25801 [D loss: 0.753393, acc: 43.75%] [G loss: 1.632241]\n",
      "epoch:27 step:25802 [D loss: 0.660136, acc: 57.03%] [G loss: 1.786762]\n",
      "epoch:27 step:25803 [D loss: 0.681007, acc: 57.03%] [G loss: 1.936530]\n",
      "epoch:27 step:25804 [D loss: 0.639288, acc: 66.41%] [G loss: 1.884544]\n",
      "epoch:27 step:25805 [D loss: 0.655418, acc: 62.50%] [G loss: 1.817218]\n",
      "epoch:27 step:25806 [D loss: 0.635232, acc: 66.41%] [G loss: 1.914994]\n",
      "epoch:27 step:25807 [D loss: 0.661346, acc: 61.72%] [G loss: 1.816195]\n",
      "epoch:27 step:25808 [D loss: 0.654410, acc: 61.72%] [G loss: 1.941373]\n",
      "epoch:27 step:25809 [D loss: 0.673132, acc: 57.81%] [G loss: 1.734561]\n",
      "epoch:27 step:25810 [D loss: 0.695517, acc: 57.81%] [G loss: 1.733319]\n",
      "epoch:27 step:25811 [D loss: 0.663323, acc: 59.38%] [G loss: 1.832540]\n",
      "epoch:27 step:25812 [D loss: 0.667185, acc: 63.28%] [G loss: 1.773198]\n",
      "epoch:27 step:25813 [D loss: 0.637083, acc: 64.84%] [G loss: 1.758950]\n",
      "epoch:27 step:25814 [D loss: 0.635303, acc: 67.97%] [G loss: 1.792261]\n",
      "epoch:27 step:25815 [D loss: 0.645887, acc: 60.94%] [G loss: 1.875162]\n",
      "epoch:27 step:25816 [D loss: 0.601750, acc: 67.97%] [G loss: 1.861702]\n",
      "epoch:27 step:25817 [D loss: 0.668404, acc: 57.81%] [G loss: 1.755559]\n",
      "epoch:27 step:25818 [D loss: 0.634485, acc: 64.06%] [G loss: 1.917236]\n",
      "epoch:27 step:25819 [D loss: 0.631819, acc: 64.84%] [G loss: 1.835646]\n",
      "epoch:27 step:25820 [D loss: 0.686905, acc: 59.38%] [G loss: 1.804068]\n",
      "epoch:27 step:25821 [D loss: 0.605363, acc: 68.75%] [G loss: 1.954696]\n",
      "epoch:27 step:25822 [D loss: 0.653482, acc: 60.94%] [G loss: 2.125352]\n",
      "epoch:27 step:25823 [D loss: 0.699930, acc: 57.03%] [G loss: 1.803877]\n",
      "epoch:27 step:25824 [D loss: 0.607002, acc: 61.72%] [G loss: 1.772093]\n",
      "epoch:27 step:25825 [D loss: 0.673692, acc: 59.38%] [G loss: 1.831509]\n",
      "epoch:27 step:25826 [D loss: 0.673450, acc: 58.59%] [G loss: 1.737945]\n",
      "epoch:27 step:25827 [D loss: 0.739305, acc: 49.22%] [G loss: 1.723143]\n",
      "epoch:27 step:25828 [D loss: 0.711171, acc: 50.78%] [G loss: 1.676511]\n",
      "epoch:27 step:25829 [D loss: 0.631673, acc: 66.41%] [G loss: 1.727887]\n",
      "epoch:27 step:25830 [D loss: 0.664883, acc: 58.59%] [G loss: 1.800443]\n",
      "epoch:27 step:25831 [D loss: 0.629917, acc: 65.62%] [G loss: 1.942197]\n",
      "epoch:27 step:25832 [D loss: 0.655003, acc: 64.06%] [G loss: 1.901513]\n",
      "epoch:27 step:25833 [D loss: 0.611622, acc: 64.84%] [G loss: 1.922330]\n",
      "epoch:27 step:25834 [D loss: 0.637748, acc: 66.41%] [G loss: 1.721959]\n",
      "epoch:27 step:25835 [D loss: 0.662298, acc: 65.62%] [G loss: 1.813670]\n",
      "epoch:27 step:25836 [D loss: 0.642898, acc: 57.81%] [G loss: 1.767065]\n",
      "epoch:27 step:25837 [D loss: 0.598083, acc: 72.66%] [G loss: 1.761377]\n",
      "epoch:27 step:25838 [D loss: 0.641838, acc: 57.81%] [G loss: 1.982485]\n",
      "epoch:27 step:25839 [D loss: 0.690666, acc: 58.59%] [G loss: 1.878112]\n",
      "epoch:27 step:25840 [D loss: 0.695430, acc: 50.78%] [G loss: 1.866950]\n",
      "epoch:27 step:25841 [D loss: 0.725422, acc: 51.56%] [G loss: 1.809745]\n",
      "epoch:27 step:25842 [D loss: 0.653387, acc: 60.16%] [G loss: 1.930386]\n",
      "epoch:27 step:25843 [D loss: 0.630105, acc: 67.97%] [G loss: 1.894943]\n",
      "epoch:27 step:25844 [D loss: 0.611214, acc: 66.41%] [G loss: 1.871964]\n",
      "epoch:27 step:25845 [D loss: 0.666593, acc: 58.59%] [G loss: 1.830281]\n",
      "epoch:27 step:25846 [D loss: 0.671820, acc: 59.38%] [G loss: 1.742538]\n",
      "epoch:27 step:25847 [D loss: 0.641468, acc: 63.28%] [G loss: 1.834362]\n",
      "epoch:27 step:25848 [D loss: 0.656035, acc: 65.62%] [G loss: 1.791810]\n",
      "epoch:27 step:25849 [D loss: 0.607769, acc: 67.97%] [G loss: 2.062422]\n",
      "epoch:27 step:25850 [D loss: 0.619252, acc: 69.53%] [G loss: 1.987621]\n",
      "epoch:27 step:25851 [D loss: 0.585517, acc: 69.53%] [G loss: 1.902296]\n",
      "epoch:27 step:25852 [D loss: 0.656246, acc: 64.06%] [G loss: 1.766370]\n",
      "epoch:27 step:25853 [D loss: 0.584703, acc: 67.97%] [G loss: 1.972815]\n",
      "epoch:27 step:25854 [D loss: 0.584762, acc: 66.41%] [G loss: 1.932713]\n",
      "epoch:27 step:25855 [D loss: 0.579512, acc: 71.88%] [G loss: 2.107198]\n",
      "epoch:27 step:25856 [D loss: 0.603993, acc: 64.84%] [G loss: 2.022663]\n",
      "epoch:27 step:25857 [D loss: 0.627184, acc: 61.72%] [G loss: 2.093354]\n",
      "epoch:27 step:25858 [D loss: 0.694459, acc: 51.56%] [G loss: 1.795892]\n",
      "epoch:27 step:25859 [D loss: 0.679154, acc: 60.94%] [G loss: 1.920444]\n",
      "epoch:27 step:25860 [D loss: 0.668199, acc: 56.25%] [G loss: 1.876474]\n",
      "epoch:27 step:25861 [D loss: 0.638772, acc: 60.94%] [G loss: 2.014650]\n",
      "epoch:27 step:25862 [D loss: 0.635998, acc: 59.38%] [G loss: 2.006651]\n",
      "epoch:27 step:25863 [D loss: 0.625031, acc: 63.28%] [G loss: 2.095615]\n",
      "epoch:27 step:25864 [D loss: 0.688172, acc: 53.91%] [G loss: 1.848538]\n",
      "epoch:27 step:25865 [D loss: 0.716692, acc: 55.47%] [G loss: 1.728959]\n",
      "epoch:27 step:25866 [D loss: 0.671798, acc: 64.84%] [G loss: 1.609856]\n",
      "epoch:27 step:25867 [D loss: 0.675012, acc: 59.38%] [G loss: 1.758431]\n",
      "epoch:27 step:25868 [D loss: 0.672923, acc: 59.38%] [G loss: 1.857879]\n",
      "epoch:27 step:25869 [D loss: 0.714250, acc: 53.12%] [G loss: 1.814882]\n",
      "epoch:27 step:25870 [D loss: 0.627311, acc: 65.62%] [G loss: 1.843337]\n",
      "epoch:27 step:25871 [D loss: 0.633181, acc: 61.72%] [G loss: 1.906405]\n",
      "epoch:27 step:25872 [D loss: 0.663799, acc: 60.16%] [G loss: 1.800864]\n",
      "epoch:27 step:25873 [D loss: 0.628050, acc: 61.72%] [G loss: 1.910142]\n",
      "epoch:27 step:25874 [D loss: 0.696004, acc: 56.25%] [G loss: 1.877114]\n",
      "epoch:27 step:25875 [D loss: 0.667687, acc: 60.94%] [G loss: 1.665716]\n",
      "epoch:27 step:25876 [D loss: 0.724196, acc: 50.78%] [G loss: 1.619120]\n",
      "epoch:27 step:25877 [D loss: 0.637649, acc: 63.28%] [G loss: 1.780176]\n",
      "epoch:27 step:25878 [D loss: 0.736780, acc: 49.22%] [G loss: 1.797926]\n",
      "epoch:27 step:25879 [D loss: 0.706891, acc: 57.81%] [G loss: 1.871359]\n",
      "epoch:27 step:25880 [D loss: 0.644850, acc: 61.72%] [G loss: 1.777062]\n",
      "epoch:27 step:25881 [D loss: 0.604401, acc: 65.62%] [G loss: 1.907789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25882 [D loss: 0.647523, acc: 65.62%] [G loss: 1.862768]\n",
      "epoch:27 step:25883 [D loss: 0.685808, acc: 53.12%] [G loss: 1.811771]\n",
      "epoch:27 step:25884 [D loss: 0.656696, acc: 60.16%] [G loss: 1.921238]\n",
      "epoch:27 step:25885 [D loss: 0.688967, acc: 57.03%] [G loss: 1.803392]\n",
      "epoch:27 step:25886 [D loss: 0.618686, acc: 60.16%] [G loss: 1.742532]\n",
      "epoch:27 step:25887 [D loss: 0.643824, acc: 62.50%] [G loss: 1.891172]\n",
      "epoch:27 step:25888 [D loss: 0.659449, acc: 60.16%] [G loss: 1.842516]\n",
      "epoch:27 step:25889 [D loss: 0.674898, acc: 54.69%] [G loss: 1.955945]\n",
      "epoch:27 step:25890 [D loss: 0.676330, acc: 60.16%] [G loss: 1.992954]\n",
      "epoch:27 step:25891 [D loss: 0.684897, acc: 54.69%] [G loss: 1.982740]\n",
      "epoch:27 step:25892 [D loss: 0.670801, acc: 62.50%] [G loss: 1.921614]\n",
      "epoch:27 step:25893 [D loss: 0.708056, acc: 58.59%] [G loss: 1.658562]\n",
      "epoch:27 step:25894 [D loss: 0.649898, acc: 66.41%] [G loss: 1.788186]\n",
      "epoch:27 step:25895 [D loss: 0.655852, acc: 63.28%] [G loss: 1.734866]\n",
      "epoch:27 step:25896 [D loss: 0.695849, acc: 49.22%] [G loss: 1.617323]\n",
      "epoch:27 step:25897 [D loss: 0.671115, acc: 60.16%] [G loss: 1.869018]\n",
      "epoch:27 step:25898 [D loss: 0.653825, acc: 60.94%] [G loss: 1.766682]\n",
      "epoch:27 step:25899 [D loss: 0.673033, acc: 56.25%] [G loss: 1.850983]\n",
      "epoch:27 step:25900 [D loss: 0.637670, acc: 60.16%] [G loss: 1.805550]\n",
      "epoch:27 step:25901 [D loss: 0.683456, acc: 55.47%] [G loss: 1.871129]\n",
      "epoch:27 step:25902 [D loss: 0.638761, acc: 62.50%] [G loss: 1.893687]\n",
      "epoch:27 step:25903 [D loss: 0.631222, acc: 64.84%] [G loss: 1.974750]\n",
      "epoch:27 step:25904 [D loss: 0.630660, acc: 64.06%] [G loss: 1.961940]\n",
      "epoch:27 step:25905 [D loss: 0.657628, acc: 62.50%] [G loss: 1.656312]\n",
      "epoch:27 step:25906 [D loss: 0.652622, acc: 63.28%] [G loss: 1.814342]\n",
      "epoch:27 step:25907 [D loss: 0.692728, acc: 54.69%] [G loss: 1.723375]\n",
      "epoch:27 step:25908 [D loss: 0.618297, acc: 68.75%] [G loss: 1.842924]\n",
      "epoch:27 step:25909 [D loss: 0.660041, acc: 63.28%] [G loss: 1.965620]\n",
      "epoch:27 step:25910 [D loss: 0.684887, acc: 53.12%] [G loss: 1.755959]\n",
      "epoch:27 step:25911 [D loss: 0.679438, acc: 57.03%] [G loss: 1.762449]\n",
      "epoch:27 step:25912 [D loss: 0.690211, acc: 60.16%] [G loss: 1.735847]\n",
      "epoch:27 step:25913 [D loss: 0.622723, acc: 64.06%] [G loss: 1.597724]\n",
      "epoch:27 step:25914 [D loss: 0.649243, acc: 60.94%] [G loss: 1.745646]\n",
      "epoch:27 step:25915 [D loss: 0.693758, acc: 57.81%] [G loss: 1.860897]\n",
      "epoch:27 step:25916 [D loss: 0.649908, acc: 64.06%] [G loss: 1.759217]\n",
      "epoch:27 step:25917 [D loss: 0.626868, acc: 62.50%] [G loss: 1.901842]\n",
      "epoch:27 step:25918 [D loss: 0.674423, acc: 58.59%] [G loss: 1.697420]\n",
      "epoch:27 step:25919 [D loss: 0.629709, acc: 64.06%] [G loss: 1.849718]\n",
      "epoch:27 step:25920 [D loss: 0.686094, acc: 63.28%] [G loss: 1.782429]\n",
      "epoch:27 step:25921 [D loss: 0.653552, acc: 60.94%] [G loss: 1.919218]\n",
      "epoch:27 step:25922 [D loss: 0.587812, acc: 75.00%] [G loss: 1.946334]\n",
      "epoch:27 step:25923 [D loss: 0.635046, acc: 62.50%] [G loss: 2.014273]\n",
      "epoch:27 step:25924 [D loss: 0.665850, acc: 56.25%] [G loss: 1.808954]\n",
      "epoch:27 step:25925 [D loss: 0.612529, acc: 67.97%] [G loss: 1.758333]\n",
      "epoch:27 step:25926 [D loss: 0.656653, acc: 57.81%] [G loss: 1.819785]\n",
      "epoch:27 step:25927 [D loss: 0.690221, acc: 53.12%] [G loss: 1.862121]\n",
      "epoch:27 step:25928 [D loss: 0.604346, acc: 69.53%] [G loss: 1.931177]\n",
      "epoch:27 step:25929 [D loss: 0.617182, acc: 69.53%] [G loss: 1.905351]\n",
      "epoch:27 step:25930 [D loss: 0.635109, acc: 64.06%] [G loss: 1.978709]\n",
      "epoch:27 step:25931 [D loss: 0.628433, acc: 65.62%] [G loss: 1.853668]\n",
      "epoch:27 step:25932 [D loss: 0.665535, acc: 59.38%] [G loss: 1.879294]\n",
      "epoch:27 step:25933 [D loss: 0.624760, acc: 68.75%] [G loss: 2.066161]\n",
      "epoch:27 step:25934 [D loss: 0.653597, acc: 64.84%] [G loss: 1.863778]\n",
      "epoch:27 step:25935 [D loss: 0.626089, acc: 64.84%] [G loss: 1.948238]\n",
      "epoch:27 step:25936 [D loss: 0.639135, acc: 63.28%] [G loss: 1.948236]\n",
      "epoch:27 step:25937 [D loss: 0.663961, acc: 66.41%] [G loss: 1.963146]\n",
      "epoch:27 step:25938 [D loss: 0.634575, acc: 60.16%] [G loss: 1.852798]\n",
      "epoch:27 step:25939 [D loss: 0.652931, acc: 60.94%] [G loss: 1.871313]\n",
      "epoch:27 step:25940 [D loss: 0.630618, acc: 67.97%] [G loss: 1.923435]\n",
      "epoch:27 step:25941 [D loss: 0.632474, acc: 61.72%] [G loss: 1.817353]\n",
      "epoch:27 step:25942 [D loss: 0.600964, acc: 66.41%] [G loss: 1.970268]\n",
      "epoch:27 step:25943 [D loss: 0.642376, acc: 62.50%] [G loss: 1.986964]\n",
      "epoch:27 step:25944 [D loss: 0.598424, acc: 67.97%] [G loss: 1.806411]\n",
      "epoch:27 step:25945 [D loss: 0.594861, acc: 71.88%] [G loss: 2.092347]\n",
      "epoch:27 step:25946 [D loss: 0.682952, acc: 60.16%] [G loss: 2.010529]\n",
      "epoch:27 step:25947 [D loss: 0.633651, acc: 66.41%] [G loss: 2.299417]\n",
      "epoch:27 step:25948 [D loss: 0.544840, acc: 75.78%] [G loss: 2.149747]\n",
      "epoch:27 step:25949 [D loss: 0.626452, acc: 63.28%] [G loss: 2.045597]\n",
      "epoch:27 step:25950 [D loss: 0.636152, acc: 62.50%] [G loss: 1.924403]\n",
      "epoch:27 step:25951 [D loss: 0.633340, acc: 67.97%] [G loss: 1.958414]\n",
      "epoch:27 step:25952 [D loss: 0.645762, acc: 67.19%] [G loss: 2.070464]\n",
      "epoch:27 step:25953 [D loss: 0.620086, acc: 67.19%] [G loss: 2.071267]\n",
      "epoch:27 step:25954 [D loss: 0.650683, acc: 60.94%] [G loss: 1.897781]\n",
      "epoch:27 step:25955 [D loss: 0.646300, acc: 63.28%] [G loss: 1.807161]\n",
      "epoch:27 step:25956 [D loss: 0.647077, acc: 62.50%] [G loss: 1.912796]\n",
      "epoch:27 step:25957 [D loss: 0.685313, acc: 54.69%] [G loss: 1.815195]\n",
      "epoch:27 step:25958 [D loss: 0.661191, acc: 56.25%] [G loss: 1.757279]\n",
      "epoch:27 step:25959 [D loss: 0.670491, acc: 58.59%] [G loss: 1.847190]\n",
      "epoch:27 step:25960 [D loss: 0.636396, acc: 62.50%] [G loss: 1.863907]\n",
      "epoch:27 step:25961 [D loss: 0.616148, acc: 64.84%] [G loss: 1.975201]\n",
      "epoch:27 step:25962 [D loss: 0.684847, acc: 55.47%] [G loss: 1.827263]\n",
      "epoch:27 step:25963 [D loss: 0.644922, acc: 63.28%] [G loss: 1.841057]\n",
      "epoch:27 step:25964 [D loss: 0.704514, acc: 57.03%] [G loss: 1.799360]\n",
      "epoch:27 step:25965 [D loss: 0.657296, acc: 59.38%] [G loss: 1.846406]\n",
      "epoch:27 step:25966 [D loss: 0.693171, acc: 56.25%] [G loss: 1.755874]\n",
      "epoch:27 step:25967 [D loss: 0.628676, acc: 60.16%] [G loss: 1.814536]\n",
      "epoch:27 step:25968 [D loss: 0.648474, acc: 60.94%] [G loss: 1.825657]\n",
      "epoch:27 step:25969 [D loss: 0.629079, acc: 63.28%] [G loss: 1.920654]\n",
      "epoch:27 step:25970 [D loss: 0.638746, acc: 62.50%] [G loss: 1.756545]\n",
      "epoch:27 step:25971 [D loss: 0.692660, acc: 57.03%] [G loss: 1.881725]\n",
      "epoch:27 step:25972 [D loss: 0.653413, acc: 61.72%] [G loss: 1.662392]\n",
      "epoch:27 step:25973 [D loss: 0.609934, acc: 70.31%] [G loss: 1.945589]\n",
      "epoch:27 step:25974 [D loss: 0.681736, acc: 53.12%] [G loss: 1.822085]\n",
      "epoch:27 step:25975 [D loss: 0.675830, acc: 56.25%] [G loss: 1.762262]\n",
      "epoch:27 step:25976 [D loss: 0.668869, acc: 64.84%] [G loss: 1.793836]\n",
      "epoch:27 step:25977 [D loss: 0.637848, acc: 60.94%] [G loss: 1.790866]\n",
      "epoch:27 step:25978 [D loss: 0.655162, acc: 63.28%] [G loss: 1.869955]\n",
      "epoch:27 step:25979 [D loss: 0.588825, acc: 68.75%] [G loss: 1.894064]\n",
      "epoch:27 step:25980 [D loss: 0.608268, acc: 67.97%] [G loss: 2.004571]\n",
      "epoch:27 step:25981 [D loss: 0.670988, acc: 58.59%] [G loss: 1.757503]\n",
      "epoch:27 step:25982 [D loss: 0.671587, acc: 61.72%] [G loss: 1.915044]\n",
      "epoch:27 step:25983 [D loss: 0.623074, acc: 65.62%] [G loss: 1.860887]\n",
      "epoch:27 step:25984 [D loss: 0.631222, acc: 64.84%] [G loss: 1.915426]\n",
      "epoch:27 step:25985 [D loss: 0.647712, acc: 60.16%] [G loss: 1.827892]\n",
      "epoch:27 step:25986 [D loss: 0.605802, acc: 67.19%] [G loss: 1.893462]\n",
      "epoch:27 step:25987 [D loss: 0.657053, acc: 64.06%] [G loss: 1.904645]\n",
      "epoch:27 step:25988 [D loss: 0.651936, acc: 60.16%] [G loss: 1.984870]\n",
      "epoch:27 step:25989 [D loss: 0.622149, acc: 64.84%] [G loss: 2.032518]\n",
      "epoch:27 step:25990 [D loss: 0.609885, acc: 71.88%] [G loss: 1.948231]\n",
      "epoch:27 step:25991 [D loss: 0.627221, acc: 64.84%] [G loss: 1.991459]\n",
      "epoch:27 step:25992 [D loss: 0.618817, acc: 64.84%] [G loss: 1.913465]\n",
      "epoch:27 step:25993 [D loss: 0.624796, acc: 68.75%] [G loss: 2.121724]\n",
      "epoch:27 step:25994 [D loss: 0.657423, acc: 62.50%] [G loss: 1.912628]\n",
      "epoch:27 step:25995 [D loss: 0.597147, acc: 71.09%] [G loss: 1.932138]\n",
      "epoch:27 step:25996 [D loss: 0.652078, acc: 60.94%] [G loss: 1.919203]\n",
      "epoch:27 step:25997 [D loss: 0.701726, acc: 53.12%] [G loss: 1.819428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:25998 [D loss: 0.654429, acc: 62.50%] [G loss: 1.928878]\n",
      "epoch:27 step:25999 [D loss: 0.646774, acc: 65.62%] [G loss: 2.015448]\n",
      "epoch:27 step:26000 [D loss: 0.634594, acc: 64.06%] [G loss: 2.112066]\n",
      "epoch:27 step:26001 [D loss: 0.684849, acc: 59.38%] [G loss: 1.832031]\n",
      "epoch:27 step:26002 [D loss: 0.683318, acc: 57.81%] [G loss: 1.736259]\n",
      "epoch:27 step:26003 [D loss: 0.694200, acc: 54.69%] [G loss: 1.762213]\n",
      "epoch:27 step:26004 [D loss: 0.699081, acc: 53.12%] [G loss: 1.707055]\n",
      "epoch:27 step:26005 [D loss: 0.589745, acc: 70.31%] [G loss: 1.960782]\n",
      "epoch:27 step:26006 [D loss: 0.609644, acc: 65.62%] [G loss: 1.909674]\n",
      "epoch:27 step:26007 [D loss: 0.648846, acc: 63.28%] [G loss: 2.128524]\n",
      "epoch:27 step:26008 [D loss: 0.674265, acc: 65.62%] [G loss: 1.825512]\n",
      "epoch:27 step:26009 [D loss: 0.642544, acc: 60.16%] [G loss: 1.978255]\n",
      "epoch:27 step:26010 [D loss: 0.652513, acc: 60.94%] [G loss: 1.938796]\n",
      "epoch:27 step:26011 [D loss: 0.651178, acc: 61.72%] [G loss: 1.967528]\n",
      "epoch:27 step:26012 [D loss: 0.682171, acc: 62.50%] [G loss: 1.840560]\n",
      "epoch:27 step:26013 [D loss: 0.657240, acc: 59.38%] [G loss: 1.922389]\n",
      "epoch:27 step:26014 [D loss: 0.638435, acc: 63.28%] [G loss: 1.883636]\n",
      "epoch:27 step:26015 [D loss: 0.681455, acc: 53.91%] [G loss: 1.721272]\n",
      "epoch:27 step:26016 [D loss: 0.664889, acc: 60.16%] [G loss: 1.855256]\n",
      "epoch:27 step:26017 [D loss: 0.644799, acc: 61.72%] [G loss: 1.943395]\n",
      "epoch:27 step:26018 [D loss: 0.659370, acc: 62.50%] [G loss: 1.989028]\n",
      "epoch:27 step:26019 [D loss: 0.636429, acc: 60.16%] [G loss: 2.029834]\n",
      "epoch:27 step:26020 [D loss: 0.679922, acc: 63.28%] [G loss: 1.912726]\n",
      "epoch:27 step:26021 [D loss: 0.684699, acc: 57.81%] [G loss: 1.818954]\n",
      "epoch:27 step:26022 [D loss: 0.682289, acc: 57.03%] [G loss: 1.934636]\n",
      "epoch:27 step:26023 [D loss: 0.636620, acc: 64.06%] [G loss: 1.794856]\n",
      "epoch:27 step:26024 [D loss: 0.641495, acc: 58.59%] [G loss: 1.899109]\n",
      "epoch:27 step:26025 [D loss: 0.638578, acc: 61.72%] [G loss: 1.995161]\n",
      "epoch:27 step:26026 [D loss: 0.681231, acc: 54.69%] [G loss: 1.791794]\n",
      "epoch:27 step:26027 [D loss: 0.651229, acc: 62.50%] [G loss: 1.767059]\n",
      "epoch:27 step:26028 [D loss: 0.681620, acc: 62.50%] [G loss: 1.734171]\n",
      "epoch:27 step:26029 [D loss: 0.672230, acc: 56.25%] [G loss: 1.752582]\n",
      "epoch:27 step:26030 [D loss: 0.676985, acc: 60.16%] [G loss: 1.723723]\n",
      "epoch:27 step:26031 [D loss: 0.615892, acc: 60.94%] [G loss: 1.940291]\n",
      "epoch:27 step:26032 [D loss: 0.624894, acc: 64.06%] [G loss: 1.854821]\n",
      "epoch:27 step:26033 [D loss: 0.668873, acc: 61.72%] [G loss: 1.757233]\n",
      "epoch:27 step:26034 [D loss: 0.704559, acc: 57.03%] [G loss: 1.909730]\n",
      "epoch:27 step:26035 [D loss: 0.664251, acc: 57.03%] [G loss: 1.793226]\n",
      "epoch:27 step:26036 [D loss: 0.633370, acc: 65.62%] [G loss: 1.870612]\n",
      "epoch:27 step:26037 [D loss: 0.623353, acc: 65.62%] [G loss: 1.816491]\n",
      "epoch:27 step:26038 [D loss: 0.641532, acc: 64.06%] [G loss: 1.822193]\n",
      "epoch:27 step:26039 [D loss: 0.643478, acc: 63.28%] [G loss: 1.867050]\n",
      "epoch:27 step:26040 [D loss: 0.652790, acc: 58.59%] [G loss: 1.849600]\n",
      "epoch:27 step:26041 [D loss: 0.689466, acc: 53.12%] [G loss: 1.825429]\n",
      "epoch:27 step:26042 [D loss: 0.616950, acc: 66.41%] [G loss: 1.773749]\n",
      "epoch:27 step:26043 [D loss: 0.653199, acc: 62.50%] [G loss: 1.852437]\n",
      "epoch:27 step:26044 [D loss: 0.647930, acc: 63.28%] [G loss: 1.916516]\n",
      "epoch:27 step:26045 [D loss: 0.663768, acc: 60.94%] [G loss: 1.818592]\n",
      "epoch:27 step:26046 [D loss: 0.609666, acc: 65.62%] [G loss: 1.884775]\n",
      "epoch:27 step:26047 [D loss: 0.696902, acc: 53.91%] [G loss: 1.710949]\n",
      "epoch:27 step:26048 [D loss: 0.654348, acc: 60.16%] [G loss: 1.838548]\n",
      "epoch:27 step:26049 [D loss: 0.642037, acc: 64.84%] [G loss: 1.782606]\n",
      "epoch:27 step:26050 [D loss: 0.665618, acc: 60.16%] [G loss: 1.809197]\n",
      "epoch:27 step:26051 [D loss: 0.657747, acc: 62.50%] [G loss: 1.702526]\n",
      "epoch:27 step:26052 [D loss: 0.636586, acc: 60.94%] [G loss: 1.759698]\n",
      "epoch:27 step:26053 [D loss: 0.636564, acc: 57.81%] [G loss: 1.776575]\n",
      "epoch:27 step:26054 [D loss: 0.636728, acc: 62.50%] [G loss: 1.873048]\n",
      "epoch:27 step:26055 [D loss: 0.596276, acc: 68.75%] [G loss: 1.792362]\n",
      "epoch:27 step:26056 [D loss: 0.683571, acc: 57.03%] [G loss: 1.799785]\n",
      "epoch:27 step:26057 [D loss: 0.640445, acc: 61.72%] [G loss: 1.780544]\n",
      "epoch:27 step:26058 [D loss: 0.679495, acc: 54.69%] [G loss: 1.783979]\n",
      "epoch:27 step:26059 [D loss: 0.658675, acc: 56.25%] [G loss: 1.780438]\n",
      "epoch:27 step:26060 [D loss: 0.645208, acc: 64.06%] [G loss: 1.660722]\n",
      "epoch:27 step:26061 [D loss: 0.669989, acc: 61.72%] [G loss: 1.737822]\n",
      "epoch:27 step:26062 [D loss: 0.625022, acc: 68.75%] [G loss: 1.820144]\n",
      "epoch:27 step:26063 [D loss: 0.639075, acc: 63.28%] [G loss: 1.819540]\n",
      "epoch:27 step:26064 [D loss: 0.694338, acc: 57.81%] [G loss: 1.698272]\n",
      "epoch:27 step:26065 [D loss: 0.691130, acc: 51.56%] [G loss: 1.806896]\n",
      "epoch:27 step:26066 [D loss: 0.630573, acc: 60.94%] [G loss: 1.776719]\n",
      "epoch:27 step:26067 [D loss: 0.675145, acc: 61.72%] [G loss: 1.749704]\n",
      "epoch:27 step:26068 [D loss: 0.617784, acc: 64.06%] [G loss: 1.898730]\n",
      "epoch:27 step:26069 [D loss: 0.632567, acc: 64.06%] [G loss: 1.886812]\n",
      "epoch:27 step:26070 [D loss: 0.662469, acc: 53.12%] [G loss: 1.881654]\n",
      "epoch:27 step:26071 [D loss: 0.648526, acc: 59.38%] [G loss: 1.889921]\n",
      "epoch:27 step:26072 [D loss: 0.617634, acc: 65.62%] [G loss: 1.885823]\n",
      "epoch:27 step:26073 [D loss: 0.679543, acc: 65.62%] [G loss: 2.057866]\n",
      "epoch:27 step:26074 [D loss: 0.635457, acc: 64.06%] [G loss: 2.013751]\n",
      "epoch:27 step:26075 [D loss: 0.610179, acc: 67.97%] [G loss: 1.903084]\n",
      "epoch:27 step:26076 [D loss: 0.687523, acc: 54.69%] [G loss: 1.688677]\n",
      "epoch:27 step:26077 [D loss: 0.645082, acc: 64.06%] [G loss: 1.861987]\n",
      "epoch:27 step:26078 [D loss: 0.653951, acc: 62.50%] [G loss: 1.799657]\n",
      "epoch:27 step:26079 [D loss: 0.601773, acc: 71.88%] [G loss: 1.953626]\n",
      "epoch:27 step:26080 [D loss: 0.629489, acc: 63.28%] [G loss: 1.968998]\n",
      "epoch:27 step:26081 [D loss: 0.661839, acc: 60.94%] [G loss: 2.068574]\n",
      "epoch:27 step:26082 [D loss: 0.669894, acc: 58.59%] [G loss: 1.870195]\n",
      "epoch:27 step:26083 [D loss: 0.670782, acc: 61.72%] [G loss: 1.754336]\n",
      "epoch:27 step:26084 [D loss: 0.664355, acc: 60.94%] [G loss: 1.873310]\n",
      "epoch:27 step:26085 [D loss: 0.624698, acc: 63.28%] [G loss: 1.948965]\n",
      "epoch:27 step:26086 [D loss: 0.660460, acc: 59.38%] [G loss: 1.813009]\n",
      "epoch:27 step:26087 [D loss: 0.692282, acc: 57.81%] [G loss: 1.792567]\n",
      "epoch:27 step:26088 [D loss: 0.671930, acc: 61.72%] [G loss: 1.869149]\n",
      "epoch:27 step:26089 [D loss: 0.629186, acc: 64.84%] [G loss: 1.864216]\n",
      "epoch:27 step:26090 [D loss: 0.681295, acc: 57.03%] [G loss: 1.840534]\n",
      "epoch:27 step:26091 [D loss: 0.617104, acc: 60.94%] [G loss: 1.859523]\n",
      "epoch:27 step:26092 [D loss: 0.668281, acc: 57.03%] [G loss: 1.875037]\n",
      "epoch:27 step:26093 [D loss: 0.684473, acc: 54.69%] [G loss: 1.723970]\n",
      "epoch:27 step:26094 [D loss: 0.702611, acc: 57.03%] [G loss: 1.766342]\n",
      "epoch:27 step:26095 [D loss: 0.680619, acc: 53.91%] [G loss: 1.844404]\n",
      "epoch:27 step:26096 [D loss: 0.646111, acc: 61.72%] [G loss: 1.790696]\n",
      "epoch:27 step:26097 [D loss: 0.632320, acc: 59.38%] [G loss: 1.828541]\n",
      "epoch:27 step:26098 [D loss: 0.653846, acc: 59.38%] [G loss: 1.856905]\n",
      "epoch:27 step:26099 [D loss: 0.675576, acc: 55.47%] [G loss: 1.755840]\n",
      "epoch:27 step:26100 [D loss: 0.680923, acc: 56.25%] [G loss: 1.833798]\n",
      "epoch:27 step:26101 [D loss: 0.666520, acc: 61.72%] [G loss: 1.869944]\n",
      "epoch:27 step:26102 [D loss: 0.647032, acc: 64.84%] [G loss: 1.778860]\n",
      "epoch:27 step:26103 [D loss: 0.647662, acc: 61.72%] [G loss: 1.928922]\n",
      "epoch:27 step:26104 [D loss: 0.702260, acc: 55.47%] [G loss: 1.844704]\n",
      "epoch:27 step:26105 [D loss: 0.683958, acc: 57.81%] [G loss: 1.791454]\n",
      "epoch:27 step:26106 [D loss: 0.632447, acc: 62.50%] [G loss: 1.752562]\n",
      "epoch:27 step:26107 [D loss: 0.642146, acc: 64.84%] [G loss: 1.869196]\n",
      "epoch:27 step:26108 [D loss: 0.654818, acc: 63.28%] [G loss: 1.903287]\n",
      "epoch:27 step:26109 [D loss: 0.664500, acc: 58.59%] [G loss: 1.850780]\n",
      "epoch:27 step:26110 [D loss: 0.665783, acc: 63.28%] [G loss: 1.832920]\n",
      "epoch:27 step:26111 [D loss: 0.661610, acc: 62.50%] [G loss: 1.788932]\n",
      "epoch:27 step:26112 [D loss: 0.631584, acc: 61.72%] [G loss: 1.841376]\n",
      "epoch:27 step:26113 [D loss: 0.602993, acc: 67.97%] [G loss: 1.956861]\n",
      "epoch:27 step:26114 [D loss: 0.624325, acc: 64.06%] [G loss: 2.051556]\n",
      "epoch:27 step:26115 [D loss: 0.590549, acc: 71.88%] [G loss: 2.129100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26116 [D loss: 0.656740, acc: 57.81%] [G loss: 1.918000]\n",
      "epoch:27 step:26117 [D loss: 0.654430, acc: 60.94%] [G loss: 1.800953]\n",
      "epoch:27 step:26118 [D loss: 0.613970, acc: 65.62%] [G loss: 2.005591]\n",
      "epoch:27 step:26119 [D loss: 0.665194, acc: 60.94%] [G loss: 1.763381]\n",
      "epoch:27 step:26120 [D loss: 0.619595, acc: 61.72%] [G loss: 1.846925]\n",
      "epoch:27 step:26121 [D loss: 0.636106, acc: 61.72%] [G loss: 1.882103]\n",
      "epoch:27 step:26122 [D loss: 0.598818, acc: 69.53%] [G loss: 2.010220]\n",
      "epoch:27 step:26123 [D loss: 0.663399, acc: 61.72%] [G loss: 1.782512]\n",
      "epoch:27 step:26124 [D loss: 0.613152, acc: 65.62%] [G loss: 1.953191]\n",
      "epoch:27 step:26125 [D loss: 0.678462, acc: 58.59%] [G loss: 1.829719]\n",
      "epoch:27 step:26126 [D loss: 0.654065, acc: 61.72%] [G loss: 1.763921]\n",
      "epoch:27 step:26127 [D loss: 0.716272, acc: 54.69%] [G loss: 1.844751]\n",
      "epoch:27 step:26128 [D loss: 0.670221, acc: 54.69%] [G loss: 1.859345]\n",
      "epoch:27 step:26129 [D loss: 0.683206, acc: 57.81%] [G loss: 1.669628]\n",
      "epoch:27 step:26130 [D loss: 0.687877, acc: 60.16%] [G loss: 1.931029]\n",
      "epoch:27 step:26131 [D loss: 0.655856, acc: 60.94%] [G loss: 1.898673]\n",
      "epoch:27 step:26132 [D loss: 0.613343, acc: 60.94%] [G loss: 2.021659]\n",
      "epoch:27 step:26133 [D loss: 0.705683, acc: 51.56%] [G loss: 1.709180]\n",
      "epoch:27 step:26134 [D loss: 0.661109, acc: 60.16%] [G loss: 1.810831]\n",
      "epoch:27 step:26135 [D loss: 0.602193, acc: 71.88%] [G loss: 1.930942]\n",
      "epoch:27 step:26136 [D loss: 0.648554, acc: 67.19%] [G loss: 1.833146]\n",
      "epoch:27 step:26137 [D loss: 0.634869, acc: 65.62%] [G loss: 1.868015]\n",
      "epoch:27 step:26138 [D loss: 0.615183, acc: 70.31%] [G loss: 1.765863]\n",
      "epoch:27 step:26139 [D loss: 0.646129, acc: 63.28%] [G loss: 1.919434]\n",
      "epoch:27 step:26140 [D loss: 0.627242, acc: 64.06%] [G loss: 1.886603]\n",
      "epoch:27 step:26141 [D loss: 0.576419, acc: 71.88%] [G loss: 2.002787]\n",
      "epoch:27 step:26142 [D loss: 0.639416, acc: 60.94%] [G loss: 1.852541]\n",
      "epoch:27 step:26143 [D loss: 0.695372, acc: 50.78%] [G loss: 2.011284]\n",
      "epoch:27 step:26144 [D loss: 0.612981, acc: 67.97%] [G loss: 1.945776]\n",
      "epoch:27 step:26145 [D loss: 0.635299, acc: 57.03%] [G loss: 1.913466]\n",
      "epoch:27 step:26146 [D loss: 0.629280, acc: 64.06%] [G loss: 1.861040]\n",
      "epoch:27 step:26147 [D loss: 0.715089, acc: 53.91%] [G loss: 1.844065]\n",
      "epoch:27 step:26148 [D loss: 0.629683, acc: 64.06%] [G loss: 1.961811]\n",
      "epoch:27 step:26149 [D loss: 0.643241, acc: 66.41%] [G loss: 1.819683]\n",
      "epoch:27 step:26150 [D loss: 0.683927, acc: 58.59%] [G loss: 1.787762]\n",
      "epoch:27 step:26151 [D loss: 0.614500, acc: 67.19%] [G loss: 1.820485]\n",
      "epoch:27 step:26152 [D loss: 0.632035, acc: 64.84%] [G loss: 1.942497]\n",
      "epoch:27 step:26153 [D loss: 0.592071, acc: 67.97%] [G loss: 1.913656]\n",
      "epoch:27 step:26154 [D loss: 0.668658, acc: 51.56%] [G loss: 1.889855]\n",
      "epoch:27 step:26155 [D loss: 0.653809, acc: 57.81%] [G loss: 1.919814]\n",
      "epoch:27 step:26156 [D loss: 0.627238, acc: 63.28%] [G loss: 1.818250]\n",
      "epoch:27 step:26157 [D loss: 0.717778, acc: 52.34%] [G loss: 1.720253]\n",
      "epoch:27 step:26158 [D loss: 0.619073, acc: 67.19%] [G loss: 1.874548]\n",
      "epoch:27 step:26159 [D loss: 0.631611, acc: 60.94%] [G loss: 1.967612]\n",
      "epoch:27 step:26160 [D loss: 0.673479, acc: 57.81%] [G loss: 1.789623]\n",
      "epoch:27 step:26161 [D loss: 0.647207, acc: 64.84%] [G loss: 1.748280]\n",
      "epoch:27 step:26162 [D loss: 0.611996, acc: 67.19%] [G loss: 1.873322]\n",
      "epoch:27 step:26163 [D loss: 0.696666, acc: 54.69%] [G loss: 1.834038]\n",
      "epoch:27 step:26164 [D loss: 0.668468, acc: 63.28%] [G loss: 1.765505]\n",
      "epoch:27 step:26165 [D loss: 0.700800, acc: 55.47%] [G loss: 1.781725]\n",
      "epoch:27 step:26166 [D loss: 0.630888, acc: 64.84%] [G loss: 1.924035]\n",
      "epoch:27 step:26167 [D loss: 0.648838, acc: 63.28%] [G loss: 1.942633]\n",
      "epoch:27 step:26168 [D loss: 0.674875, acc: 60.16%] [G loss: 1.804117]\n",
      "epoch:27 step:26169 [D loss: 0.702068, acc: 54.69%] [G loss: 1.794180]\n",
      "epoch:27 step:26170 [D loss: 0.631756, acc: 65.62%] [G loss: 1.679379]\n",
      "epoch:27 step:26171 [D loss: 0.740458, acc: 53.12%] [G loss: 1.737485]\n",
      "epoch:27 step:26172 [D loss: 0.657076, acc: 67.19%] [G loss: 1.810340]\n",
      "epoch:27 step:26173 [D loss: 0.688686, acc: 53.12%] [G loss: 1.700859]\n",
      "epoch:27 step:26174 [D loss: 0.611757, acc: 67.97%] [G loss: 1.891642]\n",
      "epoch:27 step:26175 [D loss: 0.623664, acc: 57.03%] [G loss: 1.874037]\n",
      "epoch:27 step:26176 [D loss: 0.632092, acc: 65.62%] [G loss: 1.859950]\n",
      "epoch:27 step:26177 [D loss: 0.613288, acc: 70.31%] [G loss: 2.042775]\n",
      "epoch:27 step:26178 [D loss: 0.670378, acc: 55.47%] [G loss: 1.747802]\n",
      "epoch:27 step:26179 [D loss: 0.640814, acc: 64.06%] [G loss: 1.748235]\n",
      "epoch:27 step:26180 [D loss: 0.669960, acc: 52.34%] [G loss: 1.878365]\n",
      "epoch:27 step:26181 [D loss: 0.616657, acc: 71.09%] [G loss: 2.065211]\n",
      "epoch:27 step:26182 [D loss: 0.698902, acc: 58.59%] [G loss: 1.988030]\n",
      "epoch:27 step:26183 [D loss: 0.636215, acc: 61.72%] [G loss: 1.786600]\n",
      "epoch:27 step:26184 [D loss: 0.611668, acc: 69.53%] [G loss: 1.890478]\n",
      "epoch:27 step:26185 [D loss: 0.610349, acc: 63.28%] [G loss: 2.005832]\n",
      "epoch:27 step:26186 [D loss: 0.655529, acc: 59.38%] [G loss: 1.951473]\n",
      "epoch:27 step:26187 [D loss: 0.651377, acc: 64.06%] [G loss: 1.757326]\n",
      "epoch:27 step:26188 [D loss: 0.628543, acc: 62.50%] [G loss: 2.036452]\n",
      "epoch:27 step:26189 [D loss: 0.596398, acc: 68.75%] [G loss: 1.931596]\n",
      "epoch:27 step:26190 [D loss: 0.644103, acc: 59.38%] [G loss: 1.921605]\n",
      "epoch:27 step:26191 [D loss: 0.715490, acc: 56.25%] [G loss: 1.800253]\n",
      "epoch:27 step:26192 [D loss: 0.621673, acc: 64.06%] [G loss: 2.007947]\n",
      "epoch:27 step:26193 [D loss: 0.630478, acc: 64.06%] [G loss: 1.952671]\n",
      "epoch:27 step:26194 [D loss: 0.657426, acc: 60.94%] [G loss: 1.952026]\n",
      "epoch:27 step:26195 [D loss: 0.672177, acc: 58.59%] [G loss: 1.732305]\n",
      "epoch:27 step:26196 [D loss: 0.630973, acc: 68.75%] [G loss: 1.956646]\n",
      "epoch:27 step:26197 [D loss: 0.639871, acc: 64.06%] [G loss: 1.888877]\n",
      "epoch:27 step:26198 [D loss: 0.604325, acc: 69.53%] [G loss: 2.125143]\n",
      "epoch:27 step:26199 [D loss: 0.665585, acc: 60.94%] [G loss: 1.987041]\n",
      "epoch:27 step:26200 [D loss: 0.615233, acc: 64.06%] [G loss: 1.953124]\n",
      "epoch:27 step:26201 [D loss: 0.644609, acc: 65.62%] [G loss: 1.935084]\n",
      "epoch:27 step:26202 [D loss: 0.630118, acc: 65.62%] [G loss: 2.025069]\n",
      "epoch:27 step:26203 [D loss: 0.633240, acc: 57.81%] [G loss: 1.908208]\n",
      "epoch:27 step:26204 [D loss: 0.633999, acc: 64.84%] [G loss: 2.125089]\n",
      "epoch:27 step:26205 [D loss: 0.688492, acc: 56.25%] [G loss: 2.102330]\n",
      "epoch:27 step:26206 [D loss: 0.651201, acc: 63.28%] [G loss: 1.900599]\n",
      "epoch:27 step:26207 [D loss: 0.634900, acc: 66.41%] [G loss: 1.839025]\n",
      "epoch:27 step:26208 [D loss: 0.597462, acc: 68.75%] [G loss: 1.972543]\n",
      "epoch:27 step:26209 [D loss: 0.635422, acc: 60.16%] [G loss: 1.867547]\n",
      "epoch:27 step:26210 [D loss: 0.658098, acc: 59.38%] [G loss: 1.861040]\n",
      "epoch:27 step:26211 [D loss: 0.674113, acc: 66.41%] [G loss: 2.004705]\n",
      "epoch:27 step:26212 [D loss: 0.620924, acc: 67.97%] [G loss: 1.827737]\n",
      "epoch:27 step:26213 [D loss: 0.714419, acc: 52.34%] [G loss: 1.869331]\n",
      "epoch:27 step:26214 [D loss: 0.622024, acc: 67.97%] [G loss: 1.971603]\n",
      "epoch:27 step:26215 [D loss: 0.631943, acc: 64.06%] [G loss: 1.953067]\n",
      "epoch:27 step:26216 [D loss: 0.605555, acc: 64.84%] [G loss: 1.935316]\n",
      "epoch:27 step:26217 [D loss: 0.598257, acc: 71.88%] [G loss: 2.021059]\n",
      "epoch:27 step:26218 [D loss: 0.572459, acc: 68.75%] [G loss: 2.137528]\n",
      "epoch:27 step:26219 [D loss: 0.702364, acc: 57.81%] [G loss: 1.766525]\n",
      "epoch:27 step:26220 [D loss: 0.661716, acc: 60.16%] [G loss: 1.844525]\n",
      "epoch:27 step:26221 [D loss: 0.631614, acc: 64.84%] [G loss: 1.999890]\n",
      "epoch:27 step:26222 [D loss: 0.595528, acc: 71.88%] [G loss: 2.010122]\n",
      "epoch:27 step:26223 [D loss: 0.593359, acc: 74.22%] [G loss: 1.970363]\n",
      "epoch:27 step:26224 [D loss: 0.559347, acc: 67.19%] [G loss: 2.198780]\n",
      "epoch:27 step:26225 [D loss: 0.633428, acc: 63.28%] [G loss: 2.138992]\n",
      "epoch:27 step:26226 [D loss: 0.722432, acc: 60.16%] [G loss: 2.054128]\n",
      "epoch:27 step:26227 [D loss: 0.790997, acc: 50.00%] [G loss: 1.741553]\n",
      "epoch:27 step:26228 [D loss: 0.778247, acc: 47.66%] [G loss: 1.872098]\n",
      "epoch:27 step:26229 [D loss: 0.594976, acc: 68.75%] [G loss: 1.945312]\n",
      "epoch:27 step:26230 [D loss: 0.590256, acc: 70.31%] [G loss: 2.116211]\n",
      "epoch:27 step:26231 [D loss: 0.651429, acc: 64.84%] [G loss: 2.049018]\n",
      "epoch:27 step:26232 [D loss: 0.646116, acc: 67.19%] [G loss: 2.031339]\n",
      "epoch:27 step:26233 [D loss: 0.641898, acc: 67.97%] [G loss: 1.924469]\n",
      "epoch:27 step:26234 [D loss: 0.650789, acc: 63.28%] [G loss: 2.034340]\n",
      "epoch:27 step:26235 [D loss: 0.586240, acc: 75.00%] [G loss: 2.126080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:26236 [D loss: 0.559515, acc: 76.56%] [G loss: 2.456425]\n",
      "epoch:28 step:26237 [D loss: 0.660090, acc: 58.59%] [G loss: 1.966846]\n",
      "epoch:28 step:26238 [D loss: 0.645855, acc: 64.06%] [G loss: 1.965278]\n",
      "epoch:28 step:26239 [D loss: 0.675718, acc: 59.38%] [G loss: 1.868474]\n",
      "epoch:28 step:26240 [D loss: 0.686982, acc: 60.16%] [G loss: 1.807924]\n",
      "epoch:28 step:26241 [D loss: 0.619449, acc: 64.06%] [G loss: 1.916023]\n",
      "epoch:28 step:26242 [D loss: 0.686214, acc: 54.69%] [G loss: 1.842054]\n",
      "epoch:28 step:26243 [D loss: 0.657461, acc: 66.41%] [G loss: 1.891627]\n",
      "epoch:28 step:26244 [D loss: 0.674318, acc: 60.94%] [G loss: 1.940359]\n",
      "epoch:28 step:26245 [D loss: 0.619337, acc: 67.97%] [G loss: 1.964862]\n",
      "epoch:28 step:26246 [D loss: 0.681714, acc: 60.94%] [G loss: 1.948825]\n",
      "epoch:28 step:26247 [D loss: 0.649804, acc: 64.06%] [G loss: 1.917660]\n",
      "epoch:28 step:26248 [D loss: 0.667760, acc: 59.38%] [G loss: 1.851499]\n",
      "epoch:28 step:26249 [D loss: 0.615469, acc: 65.62%] [G loss: 1.885751]\n",
      "epoch:28 step:26250 [D loss: 0.637753, acc: 69.53%] [G loss: 1.970534]\n",
      "epoch:28 step:26251 [D loss: 0.655058, acc: 61.72%] [G loss: 2.056508]\n",
      "epoch:28 step:26252 [D loss: 0.681799, acc: 60.94%] [G loss: 2.018614]\n",
      "epoch:28 step:26253 [D loss: 0.689417, acc: 58.59%] [G loss: 1.773131]\n",
      "epoch:28 step:26254 [D loss: 0.651660, acc: 63.28%] [G loss: 1.747114]\n",
      "epoch:28 step:26255 [D loss: 0.674029, acc: 60.16%] [G loss: 1.742596]\n",
      "epoch:28 step:26256 [D loss: 0.702451, acc: 52.34%] [G loss: 1.749018]\n",
      "epoch:28 step:26257 [D loss: 0.673036, acc: 57.03%] [G loss: 1.810349]\n",
      "epoch:28 step:26258 [D loss: 0.688717, acc: 59.38%] [G loss: 1.779304]\n",
      "epoch:28 step:26259 [D loss: 0.668138, acc: 60.94%] [G loss: 1.902391]\n",
      "epoch:28 step:26260 [D loss: 0.601658, acc: 65.62%] [G loss: 1.808102]\n",
      "epoch:28 step:26261 [D loss: 0.619604, acc: 66.41%] [G loss: 1.923485]\n",
      "epoch:28 step:26262 [D loss: 0.646925, acc: 60.16%] [G loss: 1.896253]\n",
      "epoch:28 step:26263 [D loss: 0.710706, acc: 57.03%] [G loss: 1.726768]\n",
      "epoch:28 step:26264 [D loss: 0.620461, acc: 67.97%] [G loss: 1.820966]\n",
      "epoch:28 step:26265 [D loss: 0.643146, acc: 60.94%] [G loss: 1.862148]\n",
      "epoch:28 step:26266 [D loss: 0.611403, acc: 64.84%] [G loss: 1.755237]\n",
      "epoch:28 step:26267 [D loss: 0.709014, acc: 59.38%] [G loss: 1.726809]\n",
      "epoch:28 step:26268 [D loss: 0.656606, acc: 64.06%] [G loss: 1.784896]\n",
      "epoch:28 step:26269 [D loss: 0.668488, acc: 60.16%] [G loss: 1.826620]\n",
      "epoch:28 step:26270 [D loss: 0.691114, acc: 54.69%] [G loss: 1.712800]\n",
      "epoch:28 step:26271 [D loss: 0.674644, acc: 54.69%] [G loss: 1.810155]\n",
      "epoch:28 step:26272 [D loss: 0.645521, acc: 60.16%] [G loss: 1.827908]\n",
      "epoch:28 step:26273 [D loss: 0.638396, acc: 64.84%] [G loss: 1.845701]\n",
      "epoch:28 step:26274 [D loss: 0.612337, acc: 60.94%] [G loss: 2.023492]\n",
      "epoch:28 step:26275 [D loss: 0.683552, acc: 54.69%] [G loss: 1.876544]\n",
      "epoch:28 step:26276 [D loss: 0.602844, acc: 69.53%] [G loss: 1.961632]\n",
      "epoch:28 step:26277 [D loss: 0.657065, acc: 59.38%] [G loss: 1.744203]\n",
      "epoch:28 step:26278 [D loss: 0.638464, acc: 60.16%] [G loss: 1.895580]\n",
      "epoch:28 step:26279 [D loss: 0.611110, acc: 71.88%] [G loss: 1.918030]\n",
      "epoch:28 step:26280 [D loss: 0.622491, acc: 68.75%] [G loss: 1.831879]\n",
      "epoch:28 step:26281 [D loss: 0.638409, acc: 62.50%] [G loss: 1.911894]\n",
      "epoch:28 step:26282 [D loss: 0.600166, acc: 71.88%] [G loss: 1.871396]\n",
      "epoch:28 step:26283 [D loss: 0.618301, acc: 67.19%] [G loss: 1.923694]\n",
      "epoch:28 step:26284 [D loss: 0.672005, acc: 63.28%] [G loss: 1.838542]\n",
      "epoch:28 step:26285 [D loss: 0.703356, acc: 58.59%] [G loss: 1.867233]\n",
      "epoch:28 step:26286 [D loss: 0.617976, acc: 63.28%] [G loss: 1.895912]\n",
      "epoch:28 step:26287 [D loss: 0.645631, acc: 61.72%] [G loss: 1.652306]\n",
      "epoch:28 step:26288 [D loss: 0.652521, acc: 64.06%] [G loss: 1.721862]\n",
      "epoch:28 step:26289 [D loss: 0.610127, acc: 71.09%] [G loss: 2.163401]\n",
      "epoch:28 step:26290 [D loss: 0.642850, acc: 64.84%] [G loss: 1.835929]\n",
      "epoch:28 step:26291 [D loss: 0.598998, acc: 70.31%] [G loss: 1.860954]\n",
      "epoch:28 step:26292 [D loss: 0.587596, acc: 70.31%] [G loss: 2.050771]\n",
      "epoch:28 step:26293 [D loss: 0.648242, acc: 60.16%] [G loss: 1.940619]\n",
      "epoch:28 step:26294 [D loss: 0.706617, acc: 60.94%] [G loss: 1.919054]\n",
      "epoch:28 step:26295 [D loss: 0.618678, acc: 66.41%] [G loss: 2.026195]\n",
      "epoch:28 step:26296 [D loss: 0.688263, acc: 58.59%] [G loss: 1.950025]\n",
      "epoch:28 step:26297 [D loss: 0.659242, acc: 60.94%] [G loss: 1.875947]\n",
      "epoch:28 step:26298 [D loss: 0.656166, acc: 59.38%] [G loss: 1.817890]\n",
      "epoch:28 step:26299 [D loss: 0.614181, acc: 72.66%] [G loss: 1.859467]\n",
      "epoch:28 step:26300 [D loss: 0.658253, acc: 63.28%] [G loss: 1.866955]\n",
      "epoch:28 step:26301 [D loss: 0.645136, acc: 64.06%] [G loss: 1.810705]\n",
      "epoch:28 step:26302 [D loss: 0.685355, acc: 60.94%] [G loss: 1.903979]\n",
      "epoch:28 step:26303 [D loss: 0.628838, acc: 62.50%] [G loss: 1.884844]\n",
      "epoch:28 step:26304 [D loss: 0.637090, acc: 61.72%] [G loss: 1.946855]\n",
      "epoch:28 step:26305 [D loss: 0.640210, acc: 61.72%] [G loss: 2.040257]\n",
      "epoch:28 step:26306 [D loss: 0.613611, acc: 66.41%] [G loss: 1.859591]\n",
      "epoch:28 step:26307 [D loss: 0.660590, acc: 54.69%] [G loss: 1.795525]\n",
      "epoch:28 step:26308 [D loss: 0.635641, acc: 66.41%] [G loss: 1.794631]\n",
      "epoch:28 step:26309 [D loss: 0.636965, acc: 63.28%] [G loss: 1.858144]\n",
      "epoch:28 step:26310 [D loss: 0.638931, acc: 59.38%] [G loss: 1.908592]\n",
      "epoch:28 step:26311 [D loss: 0.648574, acc: 63.28%] [G loss: 1.975548]\n",
      "epoch:28 step:26312 [D loss: 0.634115, acc: 64.84%] [G loss: 1.996487]\n",
      "epoch:28 step:26313 [D loss: 0.590954, acc: 70.31%] [G loss: 2.107695]\n",
      "epoch:28 step:26314 [D loss: 0.655177, acc: 60.16%] [G loss: 1.809104]\n",
      "epoch:28 step:26315 [D loss: 0.657510, acc: 59.38%] [G loss: 1.895559]\n",
      "epoch:28 step:26316 [D loss: 0.661403, acc: 63.28%] [G loss: 1.835639]\n",
      "epoch:28 step:26317 [D loss: 0.665163, acc: 57.81%] [G loss: 1.692900]\n",
      "epoch:28 step:26318 [D loss: 0.637130, acc: 58.59%] [G loss: 1.939577]\n",
      "epoch:28 step:26319 [D loss: 0.596526, acc: 70.31%] [G loss: 1.845028]\n",
      "epoch:28 step:26320 [D loss: 0.636346, acc: 60.94%] [G loss: 1.941759]\n",
      "epoch:28 step:26321 [D loss: 0.652482, acc: 60.16%] [G loss: 1.844292]\n",
      "epoch:28 step:26322 [D loss: 0.727008, acc: 53.12%] [G loss: 1.789327]\n",
      "epoch:28 step:26323 [D loss: 0.603075, acc: 65.62%] [G loss: 1.827816]\n",
      "epoch:28 step:26324 [D loss: 0.648864, acc: 63.28%] [G loss: 1.796772]\n",
      "epoch:28 step:26325 [D loss: 0.653204, acc: 64.84%] [G loss: 1.880770]\n",
      "epoch:28 step:26326 [D loss: 0.618822, acc: 64.06%] [G loss: 1.878402]\n",
      "epoch:28 step:26327 [D loss: 0.676345, acc: 54.69%] [G loss: 1.848535]\n",
      "epoch:28 step:26328 [D loss: 0.614661, acc: 67.97%] [G loss: 1.897382]\n",
      "epoch:28 step:26329 [D loss: 0.612956, acc: 63.28%] [G loss: 1.893773]\n",
      "epoch:28 step:26330 [D loss: 0.700036, acc: 54.69%] [G loss: 1.787453]\n",
      "epoch:28 step:26331 [D loss: 0.638983, acc: 65.62%] [G loss: 1.884125]\n",
      "epoch:28 step:26332 [D loss: 0.651684, acc: 61.72%] [G loss: 1.988099]\n",
      "epoch:28 step:26333 [D loss: 0.665091, acc: 57.03%] [G loss: 1.882768]\n",
      "epoch:28 step:26334 [D loss: 0.619338, acc: 69.53%] [G loss: 1.802995]\n",
      "epoch:28 step:26335 [D loss: 0.750953, acc: 52.34%] [G loss: 1.703212]\n",
      "epoch:28 step:26336 [D loss: 0.647998, acc: 59.38%] [G loss: 1.813834]\n",
      "epoch:28 step:26337 [D loss: 0.599890, acc: 65.62%] [G loss: 1.798814]\n",
      "epoch:28 step:26338 [D loss: 0.658248, acc: 62.50%] [G loss: 1.801188]\n",
      "epoch:28 step:26339 [D loss: 0.635526, acc: 65.62%] [G loss: 1.905773]\n",
      "epoch:28 step:26340 [D loss: 0.693381, acc: 59.38%] [G loss: 1.869557]\n",
      "epoch:28 step:26341 [D loss: 0.667237, acc: 60.94%] [G loss: 1.887570]\n",
      "epoch:28 step:26342 [D loss: 0.635797, acc: 67.19%] [G loss: 1.959182]\n",
      "epoch:28 step:26343 [D loss: 0.646820, acc: 62.50%] [G loss: 2.018485]\n",
      "epoch:28 step:26344 [D loss: 0.692544, acc: 53.91%] [G loss: 1.782512]\n",
      "epoch:28 step:26345 [D loss: 0.710924, acc: 49.22%] [G loss: 1.648522]\n",
      "epoch:28 step:26346 [D loss: 0.661211, acc: 57.81%] [G loss: 1.847406]\n",
      "epoch:28 step:26347 [D loss: 0.640243, acc: 65.62%] [G loss: 1.970771]\n",
      "epoch:28 step:26348 [D loss: 0.640841, acc: 62.50%] [G loss: 1.986884]\n",
      "epoch:28 step:26349 [D loss: 0.628203, acc: 63.28%] [G loss: 2.010734]\n",
      "epoch:28 step:26350 [D loss: 0.615626, acc: 70.31%] [G loss: 2.078226]\n",
      "epoch:28 step:26351 [D loss: 0.593630, acc: 67.97%] [G loss: 2.176831]\n",
      "epoch:28 step:26352 [D loss: 0.598904, acc: 70.31%] [G loss: 2.202900]\n",
      "epoch:28 step:26353 [D loss: 0.667319, acc: 62.50%] [G loss: 2.175062]\n",
      "epoch:28 step:26354 [D loss: 0.644692, acc: 63.28%] [G loss: 1.964113]\n",
      "epoch:28 step:26355 [D loss: 0.612961, acc: 69.53%] [G loss: 2.203001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26356 [D loss: 0.628754, acc: 66.41%] [G loss: 2.016771]\n",
      "epoch:28 step:26357 [D loss: 0.664201, acc: 59.38%] [G loss: 2.001345]\n",
      "epoch:28 step:26358 [D loss: 0.596462, acc: 68.75%] [G loss: 2.124788]\n",
      "epoch:28 step:26359 [D loss: 0.670217, acc: 60.94%] [G loss: 1.934215]\n",
      "epoch:28 step:26360 [D loss: 0.681356, acc: 55.47%] [G loss: 1.849511]\n",
      "epoch:28 step:26361 [D loss: 0.754573, acc: 47.66%] [G loss: 1.723258]\n",
      "epoch:28 step:26362 [D loss: 0.627044, acc: 58.59%] [G loss: 1.892237]\n",
      "epoch:28 step:26363 [D loss: 0.655217, acc: 57.81%] [G loss: 1.848020]\n",
      "epoch:28 step:26364 [D loss: 0.669491, acc: 60.16%] [G loss: 1.922889]\n",
      "epoch:28 step:26365 [D loss: 0.719654, acc: 53.12%] [G loss: 1.808976]\n",
      "epoch:28 step:26366 [D loss: 0.631423, acc: 63.28%] [G loss: 2.008561]\n",
      "epoch:28 step:26367 [D loss: 0.612218, acc: 67.19%] [G loss: 1.977148]\n",
      "epoch:28 step:26368 [D loss: 0.688691, acc: 54.69%] [G loss: 1.960910]\n",
      "epoch:28 step:26369 [D loss: 0.703807, acc: 52.34%] [G loss: 1.747309]\n",
      "epoch:28 step:26370 [D loss: 0.659149, acc: 58.59%] [G loss: 1.696208]\n",
      "epoch:28 step:26371 [D loss: 0.628615, acc: 63.28%] [G loss: 1.812210]\n",
      "epoch:28 step:26372 [D loss: 0.613870, acc: 67.97%] [G loss: 1.737012]\n",
      "epoch:28 step:26373 [D loss: 0.673160, acc: 53.12%] [G loss: 1.770977]\n",
      "epoch:28 step:26374 [D loss: 0.693485, acc: 56.25%] [G loss: 1.798465]\n",
      "epoch:28 step:26375 [D loss: 0.654698, acc: 60.16%] [G loss: 1.807399]\n",
      "epoch:28 step:26376 [D loss: 0.625276, acc: 65.62%] [G loss: 1.799925]\n",
      "epoch:28 step:26377 [D loss: 0.697030, acc: 57.03%] [G loss: 1.811519]\n",
      "epoch:28 step:26378 [D loss: 0.636469, acc: 65.62%] [G loss: 1.737356]\n",
      "epoch:28 step:26379 [D loss: 0.652755, acc: 58.59%] [G loss: 1.863552]\n",
      "epoch:28 step:26380 [D loss: 0.664131, acc: 60.94%] [G loss: 1.804439]\n",
      "epoch:28 step:26381 [D loss: 0.654616, acc: 64.84%] [G loss: 1.980631]\n",
      "epoch:28 step:26382 [D loss: 0.660309, acc: 60.16%] [G loss: 1.899871]\n",
      "epoch:28 step:26383 [D loss: 0.629568, acc: 61.72%] [G loss: 1.874674]\n",
      "epoch:28 step:26384 [D loss: 0.726536, acc: 52.34%] [G loss: 1.553399]\n",
      "epoch:28 step:26385 [D loss: 0.639525, acc: 67.19%] [G loss: 1.748275]\n",
      "epoch:28 step:26386 [D loss: 0.626257, acc: 66.41%] [G loss: 2.091143]\n",
      "epoch:28 step:26387 [D loss: 0.669338, acc: 61.72%] [G loss: 1.884677]\n",
      "epoch:28 step:26388 [D loss: 0.680299, acc: 57.81%] [G loss: 1.797056]\n",
      "epoch:28 step:26389 [D loss: 0.622723, acc: 61.72%] [G loss: 1.770938]\n",
      "epoch:28 step:26390 [D loss: 0.638014, acc: 60.16%] [G loss: 1.897737]\n",
      "epoch:28 step:26391 [D loss: 0.666340, acc: 61.72%] [G loss: 1.856562]\n",
      "epoch:28 step:26392 [D loss: 0.674478, acc: 61.72%] [G loss: 1.822528]\n",
      "epoch:28 step:26393 [D loss: 0.639304, acc: 65.62%] [G loss: 1.790341]\n",
      "epoch:28 step:26394 [D loss: 0.620900, acc: 64.84%] [G loss: 1.885679]\n",
      "epoch:28 step:26395 [D loss: 0.680581, acc: 54.69%] [G loss: 1.817633]\n",
      "epoch:28 step:26396 [D loss: 0.713859, acc: 49.22%] [G loss: 1.724555]\n",
      "epoch:28 step:26397 [D loss: 0.611736, acc: 68.75%] [G loss: 1.878226]\n",
      "epoch:28 step:26398 [D loss: 0.641500, acc: 63.28%] [G loss: 1.776551]\n",
      "epoch:28 step:26399 [D loss: 0.680688, acc: 64.06%] [G loss: 1.788350]\n",
      "epoch:28 step:26400 [D loss: 0.654752, acc: 60.16%] [G loss: 1.739237]\n",
      "epoch:28 step:26401 [D loss: 0.635643, acc: 60.94%] [G loss: 1.789993]\n",
      "epoch:28 step:26402 [D loss: 0.606111, acc: 66.41%] [G loss: 1.813093]\n",
      "epoch:28 step:26403 [D loss: 0.670661, acc: 63.28%] [G loss: 1.879595]\n",
      "epoch:28 step:26404 [D loss: 0.672799, acc: 63.28%] [G loss: 1.896838]\n",
      "epoch:28 step:26405 [D loss: 0.679831, acc: 60.94%] [G loss: 1.884914]\n",
      "epoch:28 step:26406 [D loss: 0.650071, acc: 62.50%] [G loss: 1.824834]\n",
      "epoch:28 step:26407 [D loss: 0.630635, acc: 61.72%] [G loss: 1.872098]\n",
      "epoch:28 step:26408 [D loss: 0.595931, acc: 67.19%] [G loss: 1.922988]\n",
      "epoch:28 step:26409 [D loss: 0.688047, acc: 53.12%] [G loss: 1.895186]\n",
      "epoch:28 step:26410 [D loss: 0.687292, acc: 53.12%] [G loss: 1.754348]\n",
      "epoch:28 step:26411 [D loss: 0.672309, acc: 62.50%] [G loss: 1.942337]\n",
      "epoch:28 step:26412 [D loss: 0.620688, acc: 69.53%] [G loss: 1.812430]\n",
      "epoch:28 step:26413 [D loss: 0.655156, acc: 60.94%] [G loss: 1.861477]\n",
      "epoch:28 step:26414 [D loss: 0.627865, acc: 64.84%] [G loss: 1.794775]\n",
      "epoch:28 step:26415 [D loss: 0.646679, acc: 62.50%] [G loss: 1.935526]\n",
      "epoch:28 step:26416 [D loss: 0.693474, acc: 58.59%] [G loss: 1.701869]\n",
      "epoch:28 step:26417 [D loss: 0.679767, acc: 59.38%] [G loss: 1.897645]\n",
      "epoch:28 step:26418 [D loss: 0.725609, acc: 54.69%] [G loss: 1.810688]\n",
      "epoch:28 step:26419 [D loss: 0.669466, acc: 58.59%] [G loss: 1.794919]\n",
      "epoch:28 step:26420 [D loss: 0.656910, acc: 60.16%] [G loss: 1.914988]\n",
      "epoch:28 step:26421 [D loss: 0.670492, acc: 56.25%] [G loss: 1.759995]\n",
      "epoch:28 step:26422 [D loss: 0.685879, acc: 61.72%] [G loss: 1.720186]\n",
      "epoch:28 step:26423 [D loss: 0.685106, acc: 56.25%] [G loss: 1.859496]\n",
      "epoch:28 step:26424 [D loss: 0.671975, acc: 60.16%] [G loss: 1.833693]\n",
      "epoch:28 step:26425 [D loss: 0.604635, acc: 73.44%] [G loss: 1.762704]\n",
      "epoch:28 step:26426 [D loss: 0.614439, acc: 67.97%] [G loss: 1.727075]\n",
      "epoch:28 step:26427 [D loss: 0.620020, acc: 64.06%] [G loss: 1.992177]\n",
      "epoch:28 step:26428 [D loss: 0.615891, acc: 69.53%] [G loss: 1.907411]\n",
      "epoch:28 step:26429 [D loss: 0.685224, acc: 61.72%] [G loss: 1.896293]\n",
      "epoch:28 step:26430 [D loss: 0.698201, acc: 55.47%] [G loss: 1.925478]\n",
      "epoch:28 step:26431 [D loss: 0.619161, acc: 65.62%] [G loss: 1.813910]\n",
      "epoch:28 step:26432 [D loss: 0.667818, acc: 58.59%] [G loss: 1.809940]\n",
      "epoch:28 step:26433 [D loss: 0.650020, acc: 64.84%] [G loss: 1.752056]\n",
      "epoch:28 step:26434 [D loss: 0.653996, acc: 63.28%] [G loss: 1.915370]\n",
      "epoch:28 step:26435 [D loss: 0.723304, acc: 57.03%] [G loss: 1.861695]\n",
      "epoch:28 step:26436 [D loss: 0.662358, acc: 58.59%] [G loss: 1.722100]\n",
      "epoch:28 step:26437 [D loss: 0.664479, acc: 64.06%] [G loss: 1.831659]\n",
      "epoch:28 step:26438 [D loss: 0.692501, acc: 61.72%] [G loss: 1.730065]\n",
      "epoch:28 step:26439 [D loss: 0.633557, acc: 64.84%] [G loss: 1.794877]\n",
      "epoch:28 step:26440 [D loss: 0.617251, acc: 67.97%] [G loss: 1.894429]\n",
      "epoch:28 step:26441 [D loss: 0.633380, acc: 60.16%] [G loss: 1.946434]\n",
      "epoch:28 step:26442 [D loss: 0.660184, acc: 61.72%] [G loss: 1.933808]\n",
      "epoch:28 step:26443 [D loss: 0.625522, acc: 67.19%] [G loss: 1.984672]\n",
      "epoch:28 step:26444 [D loss: 0.582768, acc: 67.19%] [G loss: 2.033032]\n",
      "epoch:28 step:26445 [D loss: 0.633080, acc: 61.72%] [G loss: 2.116136]\n",
      "epoch:28 step:26446 [D loss: 0.696076, acc: 55.47%] [G loss: 1.785496]\n",
      "epoch:28 step:26447 [D loss: 0.676567, acc: 56.25%] [G loss: 1.658174]\n",
      "epoch:28 step:26448 [D loss: 0.681408, acc: 60.16%] [G loss: 1.793779]\n",
      "epoch:28 step:26449 [D loss: 0.702137, acc: 51.56%] [G loss: 1.824718]\n",
      "epoch:28 step:26450 [D loss: 0.687797, acc: 53.91%] [G loss: 1.668030]\n",
      "epoch:28 step:26451 [D loss: 0.659834, acc: 60.16%] [G loss: 1.763686]\n",
      "epoch:28 step:26452 [D loss: 0.695127, acc: 63.28%] [G loss: 1.851170]\n",
      "epoch:28 step:26453 [D loss: 0.616124, acc: 64.06%] [G loss: 1.810326]\n",
      "epoch:28 step:26454 [D loss: 0.627955, acc: 66.41%] [G loss: 1.862770]\n",
      "epoch:28 step:26455 [D loss: 0.626822, acc: 64.84%] [G loss: 1.906612]\n",
      "epoch:28 step:26456 [D loss: 0.676496, acc: 57.03%] [G loss: 1.849491]\n",
      "epoch:28 step:26457 [D loss: 0.666691, acc: 59.38%] [G loss: 1.892551]\n",
      "epoch:28 step:26458 [D loss: 0.646830, acc: 60.16%] [G loss: 1.875000]\n",
      "epoch:28 step:26459 [D loss: 0.593436, acc: 66.41%] [G loss: 1.887567]\n",
      "epoch:28 step:26460 [D loss: 0.665185, acc: 57.81%] [G loss: 1.829257]\n",
      "epoch:28 step:26461 [D loss: 0.688265, acc: 56.25%] [G loss: 1.805923]\n",
      "epoch:28 step:26462 [D loss: 0.616862, acc: 67.19%] [G loss: 1.780396]\n",
      "epoch:28 step:26463 [D loss: 0.673155, acc: 58.59%] [G loss: 1.782032]\n",
      "epoch:28 step:26464 [D loss: 0.659796, acc: 64.84%] [G loss: 1.742736]\n",
      "epoch:28 step:26465 [D loss: 0.595755, acc: 69.53%] [G loss: 2.009155]\n",
      "epoch:28 step:26466 [D loss: 0.589599, acc: 67.19%] [G loss: 2.102814]\n",
      "epoch:28 step:26467 [D loss: 0.630993, acc: 67.97%] [G loss: 2.385530]\n",
      "epoch:28 step:26468 [D loss: 0.538997, acc: 76.56%] [G loss: 2.284495]\n",
      "epoch:28 step:26469 [D loss: 0.715166, acc: 53.12%] [G loss: 1.831937]\n",
      "epoch:28 step:26470 [D loss: 0.692867, acc: 55.47%] [G loss: 1.768498]\n",
      "epoch:28 step:26471 [D loss: 0.669208, acc: 55.47%] [G loss: 1.824072]\n",
      "epoch:28 step:26472 [D loss: 0.671397, acc: 57.03%] [G loss: 1.858398]\n",
      "epoch:28 step:26473 [D loss: 0.645022, acc: 68.75%] [G loss: 1.841932]\n",
      "epoch:28 step:26474 [D loss: 0.627979, acc: 68.75%] [G loss: 1.990766]\n",
      "epoch:28 step:26475 [D loss: 0.634518, acc: 64.06%] [G loss: 1.848058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26476 [D loss: 0.727033, acc: 51.56%] [G loss: 1.882531]\n",
      "epoch:28 step:26477 [D loss: 0.661073, acc: 63.28%] [G loss: 1.938282]\n",
      "epoch:28 step:26478 [D loss: 0.652005, acc: 61.72%] [G loss: 1.964089]\n",
      "epoch:28 step:26479 [D loss: 0.610761, acc: 66.41%] [G loss: 1.926411]\n",
      "epoch:28 step:26480 [D loss: 0.658314, acc: 55.47%] [G loss: 1.839271]\n",
      "epoch:28 step:26481 [D loss: 0.604457, acc: 64.06%] [G loss: 1.991925]\n",
      "epoch:28 step:26482 [D loss: 0.653183, acc: 65.62%] [G loss: 1.929872]\n",
      "epoch:28 step:26483 [D loss: 0.664283, acc: 57.03%] [G loss: 1.876197]\n",
      "epoch:28 step:26484 [D loss: 0.619322, acc: 63.28%] [G loss: 1.950273]\n",
      "epoch:28 step:26485 [D loss: 0.694518, acc: 54.69%] [G loss: 1.722891]\n",
      "epoch:28 step:26486 [D loss: 0.689954, acc: 57.03%] [G loss: 1.708173]\n",
      "epoch:28 step:26487 [D loss: 0.717718, acc: 51.56%] [G loss: 1.683603]\n",
      "epoch:28 step:26488 [D loss: 0.714999, acc: 59.38%] [G loss: 1.729979]\n",
      "epoch:28 step:26489 [D loss: 0.608783, acc: 67.97%] [G loss: 1.892462]\n",
      "epoch:28 step:26490 [D loss: 0.659782, acc: 57.81%] [G loss: 1.873558]\n",
      "epoch:28 step:26491 [D loss: 0.614959, acc: 67.19%] [G loss: 1.770156]\n",
      "epoch:28 step:26492 [D loss: 0.699539, acc: 58.59%] [G loss: 1.841687]\n",
      "epoch:28 step:26493 [D loss: 0.669934, acc: 60.16%] [G loss: 1.864353]\n",
      "epoch:28 step:26494 [D loss: 0.674808, acc: 61.72%] [G loss: 1.781642]\n",
      "epoch:28 step:26495 [D loss: 0.610247, acc: 67.19%] [G loss: 1.772013]\n",
      "epoch:28 step:26496 [D loss: 0.683513, acc: 55.47%] [G loss: 1.828074]\n",
      "epoch:28 step:26497 [D loss: 0.631433, acc: 64.06%] [G loss: 1.922956]\n",
      "epoch:28 step:26498 [D loss: 0.633346, acc: 60.94%] [G loss: 1.817707]\n",
      "epoch:28 step:26499 [D loss: 0.654779, acc: 64.06%] [G loss: 1.994691]\n",
      "epoch:28 step:26500 [D loss: 0.623255, acc: 64.84%] [G loss: 1.948771]\n",
      "epoch:28 step:26501 [D loss: 0.653470, acc: 59.38%] [G loss: 1.832716]\n",
      "epoch:28 step:26502 [D loss: 0.643933, acc: 62.50%] [G loss: 1.871764]\n",
      "epoch:28 step:26503 [D loss: 0.705341, acc: 57.81%] [G loss: 1.755278]\n",
      "epoch:28 step:26504 [D loss: 0.689710, acc: 53.91%] [G loss: 1.864902]\n",
      "epoch:28 step:26505 [D loss: 0.710253, acc: 53.12%] [G loss: 1.757064]\n",
      "epoch:28 step:26506 [D loss: 0.659731, acc: 55.47%] [G loss: 1.861781]\n",
      "epoch:28 step:26507 [D loss: 0.641591, acc: 63.28%] [G loss: 1.847898]\n",
      "epoch:28 step:26508 [D loss: 0.597717, acc: 64.84%] [G loss: 1.838830]\n",
      "epoch:28 step:26509 [D loss: 0.639334, acc: 62.50%] [G loss: 1.722771]\n",
      "epoch:28 step:26510 [D loss: 0.638064, acc: 60.16%] [G loss: 1.926288]\n",
      "epoch:28 step:26511 [D loss: 0.611601, acc: 62.50%] [G loss: 2.121758]\n",
      "epoch:28 step:26512 [D loss: 0.623553, acc: 69.53%] [G loss: 2.022885]\n",
      "epoch:28 step:26513 [D loss: 0.662524, acc: 59.38%] [G loss: 1.824250]\n",
      "epoch:28 step:26514 [D loss: 0.682171, acc: 60.94%] [G loss: 1.803416]\n",
      "epoch:28 step:26515 [D loss: 0.675961, acc: 57.03%] [G loss: 1.840111]\n",
      "epoch:28 step:26516 [D loss: 0.638978, acc: 64.06%] [G loss: 1.867464]\n",
      "epoch:28 step:26517 [D loss: 0.730495, acc: 54.69%] [G loss: 1.710497]\n",
      "epoch:28 step:26518 [D loss: 0.669667, acc: 57.81%] [G loss: 1.799595]\n",
      "epoch:28 step:26519 [D loss: 0.601079, acc: 69.53%] [G loss: 1.857166]\n",
      "epoch:28 step:26520 [D loss: 0.665502, acc: 58.59%] [G loss: 1.729951]\n",
      "epoch:28 step:26521 [D loss: 0.663614, acc: 62.50%] [G loss: 1.760779]\n",
      "epoch:28 step:26522 [D loss: 0.636936, acc: 60.94%] [G loss: 1.865953]\n",
      "epoch:28 step:26523 [D loss: 0.647900, acc: 60.94%] [G loss: 1.897636]\n",
      "epoch:28 step:26524 [D loss: 0.659265, acc: 64.84%] [G loss: 1.837378]\n",
      "epoch:28 step:26525 [D loss: 0.644448, acc: 62.50%] [G loss: 1.697425]\n",
      "epoch:28 step:26526 [D loss: 0.650896, acc: 61.72%] [G loss: 1.809491]\n",
      "epoch:28 step:26527 [D loss: 0.618246, acc: 67.19%] [G loss: 1.837681]\n",
      "epoch:28 step:26528 [D loss: 0.705254, acc: 50.00%] [G loss: 1.835726]\n",
      "epoch:28 step:26529 [D loss: 0.625692, acc: 67.19%] [G loss: 1.958194]\n",
      "epoch:28 step:26530 [D loss: 0.645206, acc: 66.41%] [G loss: 1.746492]\n",
      "epoch:28 step:26531 [D loss: 0.654808, acc: 59.38%] [G loss: 1.792321]\n",
      "epoch:28 step:26532 [D loss: 0.626752, acc: 63.28%] [G loss: 1.750762]\n",
      "epoch:28 step:26533 [D loss: 0.638584, acc: 60.16%] [G loss: 2.010579]\n",
      "epoch:28 step:26534 [D loss: 0.645037, acc: 62.50%] [G loss: 1.970030]\n",
      "epoch:28 step:26535 [D loss: 0.654874, acc: 58.59%] [G loss: 1.931155]\n",
      "epoch:28 step:26536 [D loss: 0.631943, acc: 64.84%] [G loss: 2.039534]\n",
      "epoch:28 step:26537 [D loss: 0.670463, acc: 62.50%] [G loss: 1.717270]\n",
      "epoch:28 step:26538 [D loss: 0.653858, acc: 58.59%] [G loss: 1.810080]\n",
      "epoch:28 step:26539 [D loss: 0.681674, acc: 59.38%] [G loss: 1.836798]\n",
      "epoch:28 step:26540 [D loss: 0.673366, acc: 58.59%] [G loss: 1.827086]\n",
      "epoch:28 step:26541 [D loss: 0.609954, acc: 62.50%] [G loss: 1.815871]\n",
      "epoch:28 step:26542 [D loss: 0.656998, acc: 60.94%] [G loss: 1.742396]\n",
      "epoch:28 step:26543 [D loss: 0.657736, acc: 63.28%] [G loss: 1.807788]\n",
      "epoch:28 step:26544 [D loss: 0.660359, acc: 57.81%] [G loss: 1.793255]\n",
      "epoch:28 step:26545 [D loss: 0.674057, acc: 57.81%] [G loss: 1.891899]\n",
      "epoch:28 step:26546 [D loss: 0.619279, acc: 65.62%] [G loss: 1.745236]\n",
      "epoch:28 step:26547 [D loss: 0.683401, acc: 59.38%] [G loss: 1.729880]\n",
      "epoch:28 step:26548 [D loss: 0.589405, acc: 66.41%] [G loss: 2.112079]\n",
      "epoch:28 step:26549 [D loss: 0.626893, acc: 63.28%] [G loss: 2.153412]\n",
      "epoch:28 step:26550 [D loss: 0.593201, acc: 75.78%] [G loss: 2.132192]\n",
      "epoch:28 step:26551 [D loss: 0.574542, acc: 66.41%] [G loss: 1.968257]\n",
      "epoch:28 step:26552 [D loss: 0.633721, acc: 64.84%] [G loss: 1.802811]\n",
      "epoch:28 step:26553 [D loss: 0.702961, acc: 58.59%] [G loss: 1.876477]\n",
      "epoch:28 step:26554 [D loss: 0.600392, acc: 71.88%] [G loss: 1.873961]\n",
      "epoch:28 step:26555 [D loss: 0.641157, acc: 61.72%] [G loss: 1.820043]\n",
      "epoch:28 step:26556 [D loss: 0.693724, acc: 61.72%] [G loss: 1.732998]\n",
      "epoch:28 step:26557 [D loss: 0.614070, acc: 64.06%] [G loss: 2.114774]\n",
      "epoch:28 step:26558 [D loss: 0.633396, acc: 65.62%] [G loss: 1.864664]\n",
      "epoch:28 step:26559 [D loss: 0.644476, acc: 63.28%] [G loss: 1.726391]\n",
      "epoch:28 step:26560 [D loss: 0.632973, acc: 67.19%] [G loss: 1.823321]\n",
      "epoch:28 step:26561 [D loss: 0.631367, acc: 64.84%] [G loss: 1.884376]\n",
      "epoch:28 step:26562 [D loss: 0.654589, acc: 60.16%] [G loss: 1.725770]\n",
      "epoch:28 step:26563 [D loss: 0.671585, acc: 59.38%] [G loss: 1.759230]\n",
      "epoch:28 step:26564 [D loss: 0.618900, acc: 66.41%] [G loss: 1.878857]\n",
      "epoch:28 step:26565 [D loss: 0.620510, acc: 67.19%] [G loss: 1.925129]\n",
      "epoch:28 step:26566 [D loss: 0.630920, acc: 60.94%] [G loss: 1.995312]\n",
      "epoch:28 step:26567 [D loss: 0.617955, acc: 64.06%] [G loss: 2.001927]\n",
      "epoch:28 step:26568 [D loss: 0.583501, acc: 67.19%] [G loss: 2.015505]\n",
      "epoch:28 step:26569 [D loss: 0.699566, acc: 59.38%] [G loss: 1.858555]\n",
      "epoch:28 step:26570 [D loss: 0.640833, acc: 61.72%] [G loss: 1.991832]\n",
      "epoch:28 step:26571 [D loss: 0.619283, acc: 67.19%] [G loss: 2.039499]\n",
      "epoch:28 step:26572 [D loss: 0.670302, acc: 62.50%] [G loss: 2.000263]\n",
      "epoch:28 step:26573 [D loss: 0.639736, acc: 64.06%] [G loss: 1.818223]\n",
      "epoch:28 step:26574 [D loss: 0.674979, acc: 62.50%] [G loss: 1.873154]\n",
      "epoch:28 step:26575 [D loss: 0.661396, acc: 62.50%] [G loss: 1.987694]\n",
      "epoch:28 step:26576 [D loss: 0.636513, acc: 65.62%] [G loss: 2.021155]\n",
      "epoch:28 step:26577 [D loss: 0.679720, acc: 56.25%] [G loss: 1.884088]\n",
      "epoch:28 step:26578 [D loss: 0.694444, acc: 54.69%] [G loss: 1.804562]\n",
      "epoch:28 step:26579 [D loss: 0.648067, acc: 64.06%] [G loss: 1.911855]\n",
      "epoch:28 step:26580 [D loss: 0.668645, acc: 57.81%] [G loss: 1.887138]\n",
      "epoch:28 step:26581 [D loss: 0.619682, acc: 66.41%] [G loss: 2.073098]\n",
      "epoch:28 step:26582 [D loss: 0.636243, acc: 62.50%] [G loss: 2.325078]\n",
      "epoch:28 step:26583 [D loss: 0.577152, acc: 67.97%] [G loss: 2.465638]\n",
      "epoch:28 step:26584 [D loss: 0.733464, acc: 54.69%] [G loss: 1.863066]\n",
      "epoch:28 step:26585 [D loss: 0.697626, acc: 54.69%] [G loss: 1.743656]\n",
      "epoch:28 step:26586 [D loss: 0.704227, acc: 58.59%] [G loss: 1.787999]\n",
      "epoch:28 step:26587 [D loss: 0.648262, acc: 60.94%] [G loss: 1.852541]\n",
      "epoch:28 step:26588 [D loss: 0.658039, acc: 56.25%] [G loss: 1.902106]\n",
      "epoch:28 step:26589 [D loss: 0.647032, acc: 64.84%] [G loss: 1.993039]\n",
      "epoch:28 step:26590 [D loss: 0.648117, acc: 64.06%] [G loss: 1.919562]\n",
      "epoch:28 step:26591 [D loss: 0.682675, acc: 56.25%] [G loss: 1.823897]\n",
      "epoch:28 step:26592 [D loss: 0.684425, acc: 53.91%] [G loss: 1.702465]\n",
      "epoch:28 step:26593 [D loss: 0.641583, acc: 64.06%] [G loss: 1.842441]\n",
      "epoch:28 step:26594 [D loss: 0.600598, acc: 66.41%] [G loss: 2.028803]\n",
      "epoch:28 step:26595 [D loss: 0.590857, acc: 69.53%] [G loss: 2.000311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26596 [D loss: 0.610168, acc: 64.84%] [G loss: 1.887632]\n",
      "epoch:28 step:26597 [D loss: 0.651309, acc: 64.84%] [G loss: 1.954058]\n",
      "epoch:28 step:26598 [D loss: 0.664270, acc: 60.16%] [G loss: 1.905180]\n",
      "epoch:28 step:26599 [D loss: 0.656188, acc: 63.28%] [G loss: 1.759623]\n",
      "epoch:28 step:26600 [D loss: 0.614880, acc: 67.97%] [G loss: 1.771770]\n",
      "epoch:28 step:26601 [D loss: 0.657493, acc: 60.94%] [G loss: 1.872425]\n",
      "epoch:28 step:26602 [D loss: 0.630158, acc: 60.94%] [G loss: 1.915179]\n",
      "epoch:28 step:26603 [D loss: 0.603735, acc: 66.41%] [G loss: 1.890453]\n",
      "epoch:28 step:26604 [D loss: 0.688622, acc: 58.59%] [G loss: 1.782811]\n",
      "epoch:28 step:26605 [D loss: 0.598141, acc: 73.44%] [G loss: 1.848214]\n",
      "epoch:28 step:26606 [D loss: 0.637497, acc: 63.28%] [G loss: 2.000787]\n",
      "epoch:28 step:26607 [D loss: 0.657407, acc: 61.72%] [G loss: 2.103993]\n",
      "epoch:28 step:26608 [D loss: 0.629876, acc: 66.41%] [G loss: 1.954314]\n",
      "epoch:28 step:26609 [D loss: 0.701478, acc: 55.47%] [G loss: 1.818117]\n",
      "epoch:28 step:26610 [D loss: 0.612515, acc: 66.41%] [G loss: 1.934488]\n",
      "epoch:28 step:26611 [D loss: 0.692801, acc: 54.69%] [G loss: 1.684768]\n",
      "epoch:28 step:26612 [D loss: 0.665025, acc: 56.25%] [G loss: 1.769192]\n",
      "epoch:28 step:26613 [D loss: 0.681514, acc: 60.94%] [G loss: 1.700682]\n",
      "epoch:28 step:26614 [D loss: 0.651741, acc: 62.50%] [G loss: 1.841541]\n",
      "epoch:28 step:26615 [D loss: 0.592810, acc: 68.75%] [G loss: 1.949072]\n",
      "epoch:28 step:26616 [D loss: 0.678286, acc: 58.59%] [G loss: 1.921010]\n",
      "epoch:28 step:26617 [D loss: 0.620783, acc: 68.75%] [G loss: 2.098654]\n",
      "epoch:28 step:26618 [D loss: 0.634848, acc: 61.72%] [G loss: 1.915649]\n",
      "epoch:28 step:26619 [D loss: 0.675613, acc: 60.94%] [G loss: 1.769896]\n",
      "epoch:28 step:26620 [D loss: 0.684510, acc: 51.56%] [G loss: 1.818167]\n",
      "epoch:28 step:26621 [D loss: 0.625440, acc: 60.94%] [G loss: 1.812120]\n",
      "epoch:28 step:26622 [D loss: 0.678188, acc: 60.16%] [G loss: 1.702891]\n",
      "epoch:28 step:26623 [D loss: 0.672508, acc: 60.16%] [G loss: 1.750541]\n",
      "epoch:28 step:26624 [D loss: 0.645118, acc: 56.25%] [G loss: 1.802376]\n",
      "epoch:28 step:26625 [D loss: 0.685675, acc: 59.38%] [G loss: 1.891123]\n",
      "epoch:28 step:26626 [D loss: 0.684900, acc: 60.16%] [G loss: 1.813871]\n",
      "epoch:28 step:26627 [D loss: 0.721937, acc: 49.22%] [G loss: 1.758568]\n",
      "epoch:28 step:26628 [D loss: 0.679980, acc: 60.16%] [G loss: 1.805707]\n",
      "epoch:28 step:26629 [D loss: 0.648062, acc: 63.28%] [G loss: 1.781176]\n",
      "epoch:28 step:26630 [D loss: 0.679072, acc: 53.12%] [G loss: 1.664190]\n",
      "epoch:28 step:26631 [D loss: 0.669050, acc: 58.59%] [G loss: 1.866022]\n",
      "epoch:28 step:26632 [D loss: 0.671839, acc: 53.91%] [G loss: 1.696020]\n",
      "epoch:28 step:26633 [D loss: 0.702581, acc: 52.34%] [G loss: 1.742996]\n",
      "epoch:28 step:26634 [D loss: 0.662857, acc: 60.94%] [G loss: 1.959632]\n",
      "epoch:28 step:26635 [D loss: 0.653953, acc: 61.72%] [G loss: 1.956772]\n",
      "epoch:28 step:26636 [D loss: 0.666795, acc: 60.16%] [G loss: 1.803611]\n",
      "epoch:28 step:26637 [D loss: 0.686458, acc: 54.69%] [G loss: 1.880724]\n",
      "epoch:28 step:26638 [D loss: 0.644133, acc: 59.38%] [G loss: 1.821078]\n",
      "epoch:28 step:26639 [D loss: 0.671859, acc: 57.81%] [G loss: 1.862604]\n",
      "epoch:28 step:26640 [D loss: 0.621884, acc: 67.97%] [G loss: 1.836016]\n",
      "epoch:28 step:26641 [D loss: 0.606457, acc: 67.19%] [G loss: 2.017811]\n",
      "epoch:28 step:26642 [D loss: 0.703609, acc: 56.25%] [G loss: 1.902934]\n",
      "epoch:28 step:26643 [D loss: 0.643221, acc: 64.84%] [G loss: 1.837673]\n",
      "epoch:28 step:26644 [D loss: 0.691259, acc: 55.47%] [G loss: 1.763900]\n",
      "epoch:28 step:26645 [D loss: 0.642420, acc: 62.50%] [G loss: 1.912878]\n",
      "epoch:28 step:26646 [D loss: 0.685265, acc: 64.84%] [G loss: 1.773362]\n",
      "epoch:28 step:26647 [D loss: 0.693498, acc: 54.69%] [G loss: 1.827010]\n",
      "epoch:28 step:26648 [D loss: 0.651018, acc: 57.81%] [G loss: 1.775937]\n",
      "epoch:28 step:26649 [D loss: 0.647748, acc: 64.84%] [G loss: 2.019424]\n",
      "epoch:28 step:26650 [D loss: 0.634943, acc: 64.84%] [G loss: 1.969362]\n",
      "epoch:28 step:26651 [D loss: 0.654578, acc: 61.72%] [G loss: 1.964000]\n",
      "epoch:28 step:26652 [D loss: 0.639913, acc: 62.50%] [G loss: 1.990551]\n",
      "epoch:28 step:26653 [D loss: 0.695222, acc: 63.28%] [G loss: 1.944915]\n",
      "epoch:28 step:26654 [D loss: 0.677818, acc: 59.38%] [G loss: 1.752189]\n",
      "epoch:28 step:26655 [D loss: 0.687411, acc: 57.81%] [G loss: 1.946736]\n",
      "epoch:28 step:26656 [D loss: 0.643982, acc: 65.62%] [G loss: 1.814966]\n",
      "epoch:28 step:26657 [D loss: 0.681983, acc: 60.94%] [G loss: 1.802802]\n",
      "epoch:28 step:26658 [D loss: 0.708724, acc: 57.81%] [G loss: 1.737105]\n",
      "epoch:28 step:26659 [D loss: 0.674894, acc: 60.94%] [G loss: 1.773767]\n",
      "epoch:28 step:26660 [D loss: 0.636645, acc: 64.84%] [G loss: 1.820379]\n",
      "epoch:28 step:26661 [D loss: 0.681356, acc: 59.38%] [G loss: 1.800373]\n",
      "epoch:28 step:26662 [D loss: 0.630477, acc: 69.53%] [G loss: 1.869282]\n",
      "epoch:28 step:26663 [D loss: 0.657674, acc: 57.81%] [G loss: 1.823768]\n",
      "epoch:28 step:26664 [D loss: 0.575951, acc: 73.44%] [G loss: 2.006603]\n",
      "epoch:28 step:26665 [D loss: 0.572591, acc: 72.66%] [G loss: 2.111006]\n",
      "epoch:28 step:26666 [D loss: 0.627634, acc: 60.16%] [G loss: 1.981830]\n",
      "epoch:28 step:26667 [D loss: 0.648773, acc: 58.59%] [G loss: 1.854516]\n",
      "epoch:28 step:26668 [D loss: 0.685742, acc: 57.81%] [G loss: 1.792232]\n",
      "epoch:28 step:26669 [D loss: 0.670223, acc: 55.47%] [G loss: 1.770367]\n",
      "epoch:28 step:26670 [D loss: 0.628293, acc: 60.94%] [G loss: 1.875217]\n",
      "epoch:28 step:26671 [D loss: 0.686381, acc: 57.81%] [G loss: 1.822453]\n",
      "epoch:28 step:26672 [D loss: 0.727674, acc: 49.22%] [G loss: 1.914608]\n",
      "epoch:28 step:26673 [D loss: 0.669380, acc: 57.03%] [G loss: 1.690253]\n",
      "epoch:28 step:26674 [D loss: 0.671555, acc: 57.81%] [G loss: 1.800022]\n",
      "epoch:28 step:26675 [D loss: 0.698178, acc: 51.56%] [G loss: 1.699602]\n",
      "epoch:28 step:26676 [D loss: 0.668720, acc: 56.25%] [G loss: 1.679067]\n",
      "epoch:28 step:26677 [D loss: 0.673662, acc: 61.72%] [G loss: 1.722158]\n",
      "epoch:28 step:26678 [D loss: 0.697627, acc: 56.25%] [G loss: 1.608840]\n",
      "epoch:28 step:26679 [D loss: 0.629003, acc: 67.19%] [G loss: 1.700617]\n",
      "epoch:28 step:26680 [D loss: 0.639246, acc: 66.41%] [G loss: 1.774372]\n",
      "epoch:28 step:26681 [D loss: 0.664281, acc: 60.16%] [G loss: 1.768908]\n",
      "epoch:28 step:26682 [D loss: 0.719567, acc: 51.56%] [G loss: 1.713274]\n",
      "epoch:28 step:26683 [D loss: 0.678951, acc: 65.62%] [G loss: 1.741570]\n",
      "epoch:28 step:26684 [D loss: 0.649661, acc: 60.16%] [G loss: 1.748657]\n",
      "epoch:28 step:26685 [D loss: 0.671236, acc: 64.84%] [G loss: 1.804377]\n",
      "epoch:28 step:26686 [D loss: 0.621595, acc: 61.72%] [G loss: 1.849009]\n",
      "epoch:28 step:26687 [D loss: 0.598707, acc: 67.19%] [G loss: 1.987619]\n",
      "epoch:28 step:26688 [D loss: 0.682422, acc: 53.91%] [G loss: 1.733232]\n",
      "epoch:28 step:26689 [D loss: 0.637893, acc: 68.75%] [G loss: 1.812747]\n",
      "epoch:28 step:26690 [D loss: 0.619402, acc: 65.62%] [G loss: 1.819665]\n",
      "epoch:28 step:26691 [D loss: 0.636052, acc: 63.28%] [G loss: 1.845900]\n",
      "epoch:28 step:26692 [D loss: 0.644098, acc: 66.41%] [G loss: 1.989461]\n",
      "epoch:28 step:26693 [D loss: 0.609998, acc: 70.31%] [G loss: 2.020741]\n",
      "epoch:28 step:26694 [D loss: 0.690398, acc: 55.47%] [G loss: 1.759485]\n",
      "epoch:28 step:26695 [D loss: 0.648289, acc: 64.84%] [G loss: 1.759634]\n",
      "epoch:28 step:26696 [D loss: 0.684520, acc: 56.25%] [G loss: 1.739827]\n",
      "epoch:28 step:26697 [D loss: 0.647460, acc: 60.16%] [G loss: 1.886793]\n",
      "epoch:28 step:26698 [D loss: 0.665111, acc: 56.25%] [G loss: 1.878923]\n",
      "epoch:28 step:26699 [D loss: 0.648970, acc: 61.72%] [G loss: 1.794873]\n",
      "epoch:28 step:26700 [D loss: 0.660251, acc: 60.94%] [G loss: 1.779200]\n",
      "epoch:28 step:26701 [D loss: 0.670887, acc: 57.03%] [G loss: 1.875839]\n",
      "epoch:28 step:26702 [D loss: 0.630911, acc: 64.84%] [G loss: 1.841460]\n",
      "epoch:28 step:26703 [D loss: 0.647946, acc: 67.19%] [G loss: 1.921678]\n",
      "epoch:28 step:26704 [D loss: 0.624572, acc: 62.50%] [G loss: 1.874158]\n",
      "epoch:28 step:26705 [D loss: 0.597397, acc: 71.09%] [G loss: 2.034054]\n",
      "epoch:28 step:26706 [D loss: 0.647637, acc: 64.84%] [G loss: 1.993205]\n",
      "epoch:28 step:26707 [D loss: 0.539541, acc: 77.34%] [G loss: 2.397228]\n",
      "epoch:28 step:26708 [D loss: 0.603272, acc: 63.28%] [G loss: 2.226674]\n",
      "epoch:28 step:26709 [D loss: 0.789050, acc: 47.66%] [G loss: 1.778367]\n",
      "epoch:28 step:26710 [D loss: 0.711639, acc: 57.03%] [G loss: 1.809000]\n",
      "epoch:28 step:26711 [D loss: 0.716990, acc: 54.69%] [G loss: 1.732880]\n",
      "epoch:28 step:26712 [D loss: 0.639332, acc: 64.06%] [G loss: 1.707907]\n",
      "epoch:28 step:26713 [D loss: 0.679965, acc: 53.91%] [G loss: 1.707471]\n",
      "epoch:28 step:26714 [D loss: 0.643030, acc: 61.72%] [G loss: 1.902427]\n",
      "epoch:28 step:26715 [D loss: 0.640050, acc: 63.28%] [G loss: 1.928949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26716 [D loss: 0.624482, acc: 64.06%] [G loss: 1.861044]\n",
      "epoch:28 step:26717 [D loss: 0.570952, acc: 69.53%] [G loss: 2.114768]\n",
      "epoch:28 step:26718 [D loss: 0.672573, acc: 57.81%] [G loss: 1.787866]\n",
      "epoch:28 step:26719 [D loss: 0.684363, acc: 57.03%] [G loss: 1.667129]\n",
      "epoch:28 step:26720 [D loss: 0.695615, acc: 55.47%] [G loss: 1.907392]\n",
      "epoch:28 step:26721 [D loss: 0.643927, acc: 63.28%] [G loss: 1.892341]\n",
      "epoch:28 step:26722 [D loss: 0.639711, acc: 64.06%] [G loss: 1.827766]\n",
      "epoch:28 step:26723 [D loss: 0.625281, acc: 64.06%] [G loss: 1.860661]\n",
      "epoch:28 step:26724 [D loss: 0.586008, acc: 73.44%] [G loss: 2.021987]\n",
      "epoch:28 step:26725 [D loss: 0.650724, acc: 64.84%] [G loss: 1.801859]\n",
      "epoch:28 step:26726 [D loss: 0.628187, acc: 67.19%] [G loss: 1.901667]\n",
      "epoch:28 step:26727 [D loss: 0.606177, acc: 66.41%] [G loss: 1.930750]\n",
      "epoch:28 step:26728 [D loss: 0.656693, acc: 61.72%] [G loss: 1.821541]\n",
      "epoch:28 step:26729 [D loss: 0.646623, acc: 62.50%] [G loss: 1.716978]\n",
      "epoch:28 step:26730 [D loss: 0.637228, acc: 66.41%] [G loss: 1.845248]\n",
      "epoch:28 step:26731 [D loss: 0.620156, acc: 67.19%] [G loss: 1.990066]\n",
      "epoch:28 step:26732 [D loss: 0.679461, acc: 62.50%] [G loss: 1.888339]\n",
      "epoch:28 step:26733 [D loss: 0.641421, acc: 61.72%] [G loss: 1.868647]\n",
      "epoch:28 step:26734 [D loss: 0.595693, acc: 71.09%] [G loss: 1.878094]\n",
      "epoch:28 step:26735 [D loss: 0.550488, acc: 73.44%] [G loss: 1.985490]\n",
      "epoch:28 step:26736 [D loss: 0.650191, acc: 58.59%] [G loss: 1.787247]\n",
      "epoch:28 step:26737 [D loss: 0.748965, acc: 48.44%] [G loss: 1.761630]\n",
      "epoch:28 step:26738 [D loss: 0.683009, acc: 60.16%] [G loss: 1.655446]\n",
      "epoch:28 step:26739 [D loss: 0.672417, acc: 57.81%] [G loss: 1.744627]\n",
      "epoch:28 step:26740 [D loss: 0.603013, acc: 70.31%] [G loss: 2.022333]\n",
      "epoch:28 step:26741 [D loss: 0.638171, acc: 65.62%] [G loss: 2.013945]\n",
      "epoch:28 step:26742 [D loss: 0.674953, acc: 59.38%] [G loss: 1.749393]\n",
      "epoch:28 step:26743 [D loss: 0.694493, acc: 58.59%] [G loss: 1.878923]\n",
      "epoch:28 step:26744 [D loss: 0.589627, acc: 72.66%] [G loss: 2.012494]\n",
      "epoch:28 step:26745 [D loss: 0.646502, acc: 58.59%] [G loss: 1.990451]\n",
      "epoch:28 step:26746 [D loss: 0.651367, acc: 64.06%] [G loss: 1.824375]\n",
      "epoch:28 step:26747 [D loss: 0.713702, acc: 55.47%] [G loss: 1.672908]\n",
      "epoch:28 step:26748 [D loss: 0.686270, acc: 62.50%] [G loss: 1.834005]\n",
      "epoch:28 step:26749 [D loss: 0.657048, acc: 64.06%] [G loss: 1.722803]\n",
      "epoch:28 step:26750 [D loss: 0.601619, acc: 68.75%] [G loss: 1.891990]\n",
      "epoch:28 step:26751 [D loss: 0.632454, acc: 62.50%] [G loss: 1.875750]\n",
      "epoch:28 step:26752 [D loss: 0.615623, acc: 64.84%] [G loss: 1.936948]\n",
      "epoch:28 step:26753 [D loss: 0.608850, acc: 68.75%] [G loss: 1.951586]\n",
      "epoch:28 step:26754 [D loss: 0.658935, acc: 62.50%] [G loss: 1.818416]\n",
      "epoch:28 step:26755 [D loss: 0.637948, acc: 61.72%] [G loss: 1.852573]\n",
      "epoch:28 step:26756 [D loss: 0.623221, acc: 65.62%] [G loss: 1.820739]\n",
      "epoch:28 step:26757 [D loss: 0.641295, acc: 59.38%] [G loss: 1.902035]\n",
      "epoch:28 step:26758 [D loss: 0.687518, acc: 57.81%] [G loss: 2.042395]\n",
      "epoch:28 step:26759 [D loss: 0.652807, acc: 62.50%] [G loss: 1.954883]\n",
      "epoch:28 step:26760 [D loss: 0.601540, acc: 69.53%] [G loss: 1.781340]\n",
      "epoch:28 step:26761 [D loss: 0.645219, acc: 58.59%] [G loss: 1.892074]\n",
      "epoch:28 step:26762 [D loss: 0.682190, acc: 61.72%] [G loss: 1.870162]\n",
      "epoch:28 step:26763 [D loss: 0.649960, acc: 61.72%] [G loss: 1.703562]\n",
      "epoch:28 step:26764 [D loss: 0.687184, acc: 54.69%] [G loss: 1.778833]\n",
      "epoch:28 step:26765 [D loss: 0.699196, acc: 57.81%] [G loss: 1.686324]\n",
      "epoch:28 step:26766 [D loss: 0.694160, acc: 56.25%] [G loss: 1.640307]\n",
      "epoch:28 step:26767 [D loss: 0.675072, acc: 62.50%] [G loss: 1.786611]\n",
      "epoch:28 step:26768 [D loss: 0.641094, acc: 63.28%] [G loss: 1.984838]\n",
      "epoch:28 step:26769 [D loss: 0.645418, acc: 57.81%] [G loss: 1.897288]\n",
      "epoch:28 step:26770 [D loss: 0.643792, acc: 64.06%] [G loss: 1.934178]\n",
      "epoch:28 step:26771 [D loss: 0.634809, acc: 67.19%] [G loss: 1.809790]\n",
      "epoch:28 step:26772 [D loss: 0.619015, acc: 65.62%] [G loss: 1.898370]\n",
      "epoch:28 step:26773 [D loss: 0.657620, acc: 61.72%] [G loss: 1.850528]\n",
      "epoch:28 step:26774 [D loss: 0.691867, acc: 60.16%] [G loss: 1.812978]\n",
      "epoch:28 step:26775 [D loss: 0.694775, acc: 59.38%] [G loss: 1.755969]\n",
      "epoch:28 step:26776 [D loss: 0.702438, acc: 54.69%] [G loss: 1.799327]\n",
      "epoch:28 step:26777 [D loss: 0.684685, acc: 53.12%] [G loss: 1.781173]\n",
      "epoch:28 step:26778 [D loss: 0.651772, acc: 57.03%] [G loss: 1.811996]\n",
      "epoch:28 step:26779 [D loss: 0.662761, acc: 58.59%] [G loss: 1.894174]\n",
      "epoch:28 step:26780 [D loss: 0.659675, acc: 61.72%] [G loss: 1.819953]\n",
      "epoch:28 step:26781 [D loss: 0.612055, acc: 67.97%] [G loss: 1.903439]\n",
      "epoch:28 step:26782 [D loss: 0.650776, acc: 63.28%] [G loss: 1.884556]\n",
      "epoch:28 step:26783 [D loss: 0.632919, acc: 65.62%] [G loss: 1.867544]\n",
      "epoch:28 step:26784 [D loss: 0.609622, acc: 70.31%] [G loss: 1.895103]\n",
      "epoch:28 step:26785 [D loss: 0.652752, acc: 60.94%] [G loss: 1.894719]\n",
      "epoch:28 step:26786 [D loss: 0.568435, acc: 73.44%] [G loss: 1.939410]\n",
      "epoch:28 step:26787 [D loss: 0.619507, acc: 70.31%] [G loss: 1.991227]\n",
      "epoch:28 step:26788 [D loss: 0.622604, acc: 69.53%] [G loss: 2.049168]\n",
      "epoch:28 step:26789 [D loss: 0.648699, acc: 64.06%] [G loss: 1.949045]\n",
      "epoch:28 step:26790 [D loss: 0.647486, acc: 63.28%] [G loss: 2.064656]\n",
      "epoch:28 step:26791 [D loss: 0.590612, acc: 71.09%] [G loss: 1.916361]\n",
      "epoch:28 step:26792 [D loss: 0.598897, acc: 65.62%] [G loss: 1.949667]\n",
      "epoch:28 step:26793 [D loss: 0.685105, acc: 60.16%] [G loss: 2.023408]\n",
      "epoch:28 step:26794 [D loss: 0.631610, acc: 63.28%] [G loss: 2.034355]\n",
      "epoch:28 step:26795 [D loss: 0.644196, acc: 59.38%] [G loss: 1.797038]\n",
      "epoch:28 step:26796 [D loss: 0.705365, acc: 50.00%] [G loss: 1.830495]\n",
      "epoch:28 step:26797 [D loss: 0.664238, acc: 59.38%] [G loss: 1.836285]\n",
      "epoch:28 step:26798 [D loss: 0.637879, acc: 67.97%] [G loss: 1.822181]\n",
      "epoch:28 step:26799 [D loss: 0.610099, acc: 70.31%] [G loss: 1.918446]\n",
      "epoch:28 step:26800 [D loss: 0.624683, acc: 63.28%] [G loss: 2.049155]\n",
      "epoch:28 step:26801 [D loss: 0.672222, acc: 60.16%] [G loss: 1.827441]\n",
      "epoch:28 step:26802 [D loss: 0.681055, acc: 57.03%] [G loss: 1.656321]\n",
      "epoch:28 step:26803 [D loss: 0.686405, acc: 59.38%] [G loss: 1.847098]\n",
      "epoch:28 step:26804 [D loss: 0.642291, acc: 60.16%] [G loss: 1.801192]\n",
      "epoch:28 step:26805 [D loss: 0.667417, acc: 60.16%] [G loss: 1.832518]\n",
      "epoch:28 step:26806 [D loss: 0.674928, acc: 60.94%] [G loss: 1.965486]\n",
      "epoch:28 step:26807 [D loss: 0.617528, acc: 67.19%] [G loss: 1.952312]\n",
      "epoch:28 step:26808 [D loss: 0.622317, acc: 64.06%] [G loss: 1.823516]\n",
      "epoch:28 step:26809 [D loss: 0.656577, acc: 63.28%] [G loss: 1.698547]\n",
      "epoch:28 step:26810 [D loss: 0.630331, acc: 64.84%] [G loss: 1.881160]\n",
      "epoch:28 step:26811 [D loss: 0.657584, acc: 61.72%] [G loss: 1.926061]\n",
      "epoch:28 step:26812 [D loss: 0.672935, acc: 57.03%] [G loss: 1.784137]\n",
      "epoch:28 step:26813 [D loss: 0.702230, acc: 58.59%] [G loss: 1.687698]\n",
      "epoch:28 step:26814 [D loss: 0.659228, acc: 62.50%] [G loss: 1.842716]\n",
      "epoch:28 step:26815 [D loss: 0.722374, acc: 54.69%] [G loss: 1.736600]\n",
      "epoch:28 step:26816 [D loss: 0.645642, acc: 64.84%] [G loss: 1.905005]\n",
      "epoch:28 step:26817 [D loss: 0.633887, acc: 66.41%] [G loss: 1.788713]\n",
      "epoch:28 step:26818 [D loss: 0.694928, acc: 57.81%] [G loss: 1.881650]\n",
      "epoch:28 step:26819 [D loss: 0.711313, acc: 52.34%] [G loss: 1.863527]\n",
      "epoch:28 step:26820 [D loss: 0.683235, acc: 59.38%] [G loss: 1.800541]\n",
      "epoch:28 step:26821 [D loss: 0.673599, acc: 59.38%] [G loss: 1.786326]\n",
      "epoch:28 step:26822 [D loss: 0.671443, acc: 64.06%] [G loss: 1.790100]\n",
      "epoch:28 step:26823 [D loss: 0.705021, acc: 53.91%] [G loss: 1.911215]\n",
      "epoch:28 step:26824 [D loss: 0.609064, acc: 67.19%] [G loss: 1.865479]\n",
      "epoch:28 step:26825 [D loss: 0.632527, acc: 64.84%] [G loss: 1.970731]\n",
      "epoch:28 step:26826 [D loss: 0.694841, acc: 59.38%] [G loss: 1.842750]\n",
      "epoch:28 step:26827 [D loss: 0.668702, acc: 60.16%] [G loss: 1.922942]\n",
      "epoch:28 step:26828 [D loss: 0.639795, acc: 67.97%] [G loss: 1.834317]\n",
      "epoch:28 step:26829 [D loss: 0.598905, acc: 63.28%] [G loss: 1.851006]\n",
      "epoch:28 step:26830 [D loss: 0.670055, acc: 57.81%] [G loss: 1.727219]\n",
      "epoch:28 step:26831 [D loss: 0.667026, acc: 53.91%] [G loss: 1.839133]\n",
      "epoch:28 step:26832 [D loss: 0.659371, acc: 64.06%] [G loss: 1.784973]\n",
      "epoch:28 step:26833 [D loss: 0.714912, acc: 51.56%] [G loss: 1.600781]\n",
      "epoch:28 step:26834 [D loss: 0.694751, acc: 57.03%] [G loss: 1.712745]\n",
      "epoch:28 step:26835 [D loss: 0.630724, acc: 66.41%] [G loss: 1.765248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26836 [D loss: 0.681036, acc: 61.72%] [G loss: 1.866853]\n",
      "epoch:28 step:26837 [D loss: 0.661495, acc: 56.25%] [G loss: 1.803765]\n",
      "epoch:28 step:26838 [D loss: 0.664953, acc: 59.38%] [G loss: 1.772075]\n",
      "epoch:28 step:26839 [D loss: 0.658820, acc: 58.59%] [G loss: 1.909155]\n",
      "epoch:28 step:26840 [D loss: 0.617713, acc: 64.06%] [G loss: 1.837396]\n",
      "epoch:28 step:26841 [D loss: 0.657037, acc: 65.62%] [G loss: 1.821294]\n",
      "epoch:28 step:26842 [D loss: 0.654374, acc: 60.94%] [G loss: 1.751562]\n",
      "epoch:28 step:26843 [D loss: 0.662481, acc: 62.50%] [G loss: 1.739061]\n",
      "epoch:28 step:26844 [D loss: 0.663087, acc: 61.72%] [G loss: 1.853047]\n",
      "epoch:28 step:26845 [D loss: 0.658482, acc: 59.38%] [G loss: 1.780179]\n",
      "epoch:28 step:26846 [D loss: 0.618272, acc: 64.84%] [G loss: 1.826019]\n",
      "epoch:28 step:26847 [D loss: 0.627088, acc: 65.62%] [G loss: 1.696838]\n",
      "epoch:28 step:26848 [D loss: 0.679055, acc: 62.50%] [G loss: 1.691100]\n",
      "epoch:28 step:26849 [D loss: 0.697884, acc: 56.25%] [G loss: 1.868622]\n",
      "epoch:28 step:26850 [D loss: 0.635517, acc: 63.28%] [G loss: 1.745215]\n",
      "epoch:28 step:26851 [D loss: 0.654548, acc: 60.94%] [G loss: 1.738704]\n",
      "epoch:28 step:26852 [D loss: 0.687964, acc: 58.59%] [G loss: 1.761067]\n",
      "epoch:28 step:26853 [D loss: 0.671606, acc: 57.03%] [G loss: 1.695239]\n",
      "epoch:28 step:26854 [D loss: 0.629927, acc: 61.72%] [G loss: 1.802415]\n",
      "epoch:28 step:26855 [D loss: 0.651591, acc: 62.50%] [G loss: 1.688684]\n",
      "epoch:28 step:26856 [D loss: 0.645346, acc: 67.97%] [G loss: 1.805646]\n",
      "epoch:28 step:26857 [D loss: 0.617525, acc: 63.28%] [G loss: 1.769244]\n",
      "epoch:28 step:26858 [D loss: 0.664668, acc: 58.59%] [G loss: 1.818170]\n",
      "epoch:28 step:26859 [D loss: 0.700109, acc: 62.50%] [G loss: 1.847223]\n",
      "epoch:28 step:26860 [D loss: 0.603096, acc: 71.88%] [G loss: 1.929762]\n",
      "epoch:28 step:26861 [D loss: 0.651933, acc: 60.16%] [G loss: 1.782814]\n",
      "epoch:28 step:26862 [D loss: 0.635740, acc: 62.50%] [G loss: 1.888155]\n",
      "epoch:28 step:26863 [D loss: 0.665458, acc: 61.72%] [G loss: 1.784252]\n",
      "epoch:28 step:26864 [D loss: 0.687275, acc: 61.72%] [G loss: 1.715712]\n",
      "epoch:28 step:26865 [D loss: 0.605545, acc: 71.09%] [G loss: 1.728691]\n",
      "epoch:28 step:26866 [D loss: 0.658067, acc: 57.03%] [G loss: 1.857235]\n",
      "epoch:28 step:26867 [D loss: 0.655956, acc: 60.16%] [G loss: 1.942750]\n",
      "epoch:28 step:26868 [D loss: 0.670115, acc: 56.25%] [G loss: 1.937094]\n",
      "epoch:28 step:26869 [D loss: 0.645527, acc: 60.94%] [G loss: 1.993754]\n",
      "epoch:28 step:26870 [D loss: 0.603870, acc: 72.66%] [G loss: 1.966671]\n",
      "epoch:28 step:26871 [D loss: 0.651573, acc: 60.16%] [G loss: 1.969483]\n",
      "epoch:28 step:26872 [D loss: 0.652097, acc: 65.62%] [G loss: 1.852462]\n",
      "epoch:28 step:26873 [D loss: 0.642153, acc: 60.94%] [G loss: 1.911386]\n",
      "epoch:28 step:26874 [D loss: 0.677556, acc: 56.25%] [G loss: 1.923529]\n",
      "epoch:28 step:26875 [D loss: 0.701343, acc: 55.47%] [G loss: 1.924461]\n",
      "epoch:28 step:26876 [D loss: 0.650842, acc: 57.81%] [G loss: 1.745829]\n",
      "epoch:28 step:26877 [D loss: 0.605007, acc: 69.53%] [G loss: 1.993914]\n",
      "epoch:28 step:26878 [D loss: 0.689652, acc: 60.16%] [G loss: 1.933376]\n",
      "epoch:28 step:26879 [D loss: 0.698927, acc: 57.03%] [G loss: 1.818511]\n",
      "epoch:28 step:26880 [D loss: 0.682421, acc: 60.94%] [G loss: 2.000684]\n",
      "epoch:28 step:26881 [D loss: 0.645590, acc: 64.84%] [G loss: 1.930937]\n",
      "epoch:28 step:26882 [D loss: 0.684057, acc: 56.25%] [G loss: 2.014221]\n",
      "epoch:28 step:26883 [D loss: 0.612966, acc: 64.06%] [G loss: 2.076848]\n",
      "epoch:28 step:26884 [D loss: 0.568705, acc: 67.97%] [G loss: 2.273533]\n",
      "epoch:28 step:26885 [D loss: 0.595079, acc: 67.97%] [G loss: 2.132974]\n",
      "epoch:28 step:26886 [D loss: 0.636870, acc: 64.06%] [G loss: 1.995573]\n",
      "epoch:28 step:26887 [D loss: 0.631303, acc: 64.84%] [G loss: 1.985800]\n",
      "epoch:28 step:26888 [D loss: 0.699932, acc: 62.50%] [G loss: 1.887525]\n",
      "epoch:28 step:26889 [D loss: 0.671034, acc: 57.81%] [G loss: 1.827413]\n",
      "epoch:28 step:26890 [D loss: 0.606070, acc: 66.41%] [G loss: 1.978057]\n",
      "epoch:28 step:26891 [D loss: 0.736559, acc: 53.91%] [G loss: 1.761204]\n",
      "epoch:28 step:26892 [D loss: 0.659658, acc: 64.06%] [G loss: 1.914795]\n",
      "epoch:28 step:26893 [D loss: 0.642226, acc: 64.06%] [G loss: 1.744992]\n",
      "epoch:28 step:26894 [D loss: 0.667365, acc: 58.59%] [G loss: 1.680667]\n",
      "epoch:28 step:26895 [D loss: 0.667927, acc: 60.94%] [G loss: 1.865041]\n",
      "epoch:28 step:26896 [D loss: 0.633168, acc: 60.94%] [G loss: 1.791347]\n",
      "epoch:28 step:26897 [D loss: 0.614730, acc: 65.62%] [G loss: 1.743017]\n",
      "epoch:28 step:26898 [D loss: 0.645349, acc: 61.72%] [G loss: 1.820938]\n",
      "epoch:28 step:26899 [D loss: 0.652166, acc: 60.94%] [G loss: 1.889319]\n",
      "epoch:28 step:26900 [D loss: 0.614148, acc: 70.31%] [G loss: 1.824721]\n",
      "epoch:28 step:26901 [D loss: 0.637334, acc: 61.72%] [G loss: 1.925054]\n",
      "epoch:28 step:26902 [D loss: 0.640569, acc: 59.38%] [G loss: 1.680730]\n",
      "epoch:28 step:26903 [D loss: 0.678274, acc: 57.81%] [G loss: 1.708498]\n",
      "epoch:28 step:26904 [D loss: 0.612539, acc: 64.84%] [G loss: 1.863230]\n",
      "epoch:28 step:26905 [D loss: 0.643455, acc: 64.06%] [G loss: 1.663933]\n",
      "epoch:28 step:26906 [D loss: 0.683039, acc: 60.16%] [G loss: 1.809124]\n",
      "epoch:28 step:26907 [D loss: 0.662763, acc: 61.72%] [G loss: 1.896747]\n",
      "epoch:28 step:26908 [D loss: 0.633199, acc: 63.28%] [G loss: 1.795086]\n",
      "epoch:28 step:26909 [D loss: 0.672433, acc: 59.38%] [G loss: 1.857728]\n",
      "epoch:28 step:26910 [D loss: 0.666451, acc: 60.94%] [G loss: 2.001937]\n",
      "epoch:28 step:26911 [D loss: 0.685847, acc: 52.34%] [G loss: 1.796394]\n",
      "epoch:28 step:26912 [D loss: 0.669793, acc: 63.28%] [G loss: 1.837715]\n",
      "epoch:28 step:26913 [D loss: 0.585231, acc: 67.97%] [G loss: 1.826656]\n",
      "epoch:28 step:26914 [D loss: 0.660995, acc: 60.94%] [G loss: 1.913763]\n",
      "epoch:28 step:26915 [D loss: 0.640348, acc: 60.94%] [G loss: 1.826965]\n",
      "epoch:28 step:26916 [D loss: 0.679636, acc: 55.47%] [G loss: 1.907277]\n",
      "epoch:28 step:26917 [D loss: 0.634832, acc: 63.28%] [G loss: 1.778116]\n",
      "epoch:28 step:26918 [D loss: 0.720890, acc: 53.91%] [G loss: 1.879318]\n",
      "epoch:28 step:26919 [D loss: 0.682772, acc: 58.59%] [G loss: 1.881492]\n",
      "epoch:28 step:26920 [D loss: 0.677637, acc: 57.81%] [G loss: 1.759134]\n",
      "epoch:28 step:26921 [D loss: 0.629899, acc: 64.06%] [G loss: 1.725623]\n",
      "epoch:28 step:26922 [D loss: 0.671939, acc: 60.16%] [G loss: 1.806587]\n",
      "epoch:28 step:26923 [D loss: 0.668848, acc: 60.94%] [G loss: 1.976223]\n",
      "epoch:28 step:26924 [D loss: 0.666405, acc: 59.38%] [G loss: 1.889818]\n",
      "epoch:28 step:26925 [D loss: 0.640876, acc: 64.06%] [G loss: 1.819025]\n",
      "epoch:28 step:26926 [D loss: 0.606566, acc: 71.09%] [G loss: 1.864113]\n",
      "epoch:28 step:26927 [D loss: 0.602817, acc: 71.09%] [G loss: 2.015123]\n",
      "epoch:28 step:26928 [D loss: 0.627185, acc: 67.19%] [G loss: 1.902615]\n",
      "epoch:28 step:26929 [D loss: 0.651626, acc: 65.62%] [G loss: 1.985808]\n",
      "epoch:28 step:26930 [D loss: 0.616521, acc: 66.41%] [G loss: 2.065903]\n",
      "epoch:28 step:26931 [D loss: 0.665272, acc: 58.59%] [G loss: 1.756898]\n",
      "epoch:28 step:26932 [D loss: 0.661619, acc: 61.72%] [G loss: 1.798262]\n",
      "epoch:28 step:26933 [D loss: 0.710503, acc: 55.47%] [G loss: 1.771866]\n",
      "epoch:28 step:26934 [D loss: 0.677920, acc: 55.47%] [G loss: 1.848996]\n",
      "epoch:28 step:26935 [D loss: 0.661698, acc: 60.16%] [G loss: 1.806883]\n",
      "epoch:28 step:26936 [D loss: 0.653150, acc: 57.81%] [G loss: 1.821571]\n",
      "epoch:28 step:26937 [D loss: 0.643441, acc: 62.50%] [G loss: 1.737877]\n",
      "epoch:28 step:26938 [D loss: 0.708307, acc: 55.47%] [G loss: 1.718895]\n",
      "epoch:28 step:26939 [D loss: 0.674606, acc: 56.25%] [G loss: 1.712745]\n",
      "epoch:28 step:26940 [D loss: 0.665553, acc: 59.38%] [G loss: 1.764686]\n",
      "epoch:28 step:26941 [D loss: 0.624203, acc: 66.41%] [G loss: 1.786637]\n",
      "epoch:28 step:26942 [D loss: 0.623662, acc: 65.62%] [G loss: 1.865635]\n",
      "epoch:28 step:26943 [D loss: 0.607511, acc: 64.06%] [G loss: 1.909801]\n",
      "epoch:28 step:26944 [D loss: 0.649213, acc: 62.50%] [G loss: 1.860627]\n",
      "epoch:28 step:26945 [D loss: 0.647387, acc: 60.16%] [G loss: 2.200949]\n",
      "epoch:28 step:26946 [D loss: 0.661956, acc: 58.59%] [G loss: 1.697188]\n",
      "epoch:28 step:26947 [D loss: 0.621685, acc: 60.94%] [G loss: 1.887028]\n",
      "epoch:28 step:26948 [D loss: 0.671793, acc: 57.03%] [G loss: 1.830728]\n",
      "epoch:28 step:26949 [D loss: 0.680215, acc: 64.06%] [G loss: 1.799608]\n",
      "epoch:28 step:26950 [D loss: 0.657141, acc: 61.72%] [G loss: 1.763263]\n",
      "epoch:28 step:26951 [D loss: 0.656851, acc: 61.72%] [G loss: 1.868542]\n",
      "epoch:28 step:26952 [D loss: 0.686306, acc: 51.56%] [G loss: 1.705395]\n",
      "epoch:28 step:26953 [D loss: 0.629034, acc: 71.09%] [G loss: 1.807132]\n",
      "epoch:28 step:26954 [D loss: 0.678587, acc: 57.81%] [G loss: 1.777269]\n",
      "epoch:28 step:26955 [D loss: 0.604274, acc: 71.09%] [G loss: 1.954366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:26956 [D loss: 0.663714, acc: 62.50%] [G loss: 2.045214]\n",
      "epoch:28 step:26957 [D loss: 0.610027, acc: 67.19%] [G loss: 1.948470]\n",
      "epoch:28 step:26958 [D loss: 0.676389, acc: 60.16%] [G loss: 1.841399]\n",
      "epoch:28 step:26959 [D loss: 0.638686, acc: 64.06%] [G loss: 1.733863]\n",
      "epoch:28 step:26960 [D loss: 0.607749, acc: 68.75%] [G loss: 1.993210]\n",
      "epoch:28 step:26961 [D loss: 0.648737, acc: 60.94%] [G loss: 1.967326]\n",
      "epoch:28 step:26962 [D loss: 0.617663, acc: 65.62%] [G loss: 1.819360]\n",
      "epoch:28 step:26963 [D loss: 0.617486, acc: 62.50%] [G loss: 1.979571]\n",
      "epoch:28 step:26964 [D loss: 0.629868, acc: 62.50%] [G loss: 1.925224]\n",
      "epoch:28 step:26965 [D loss: 0.719934, acc: 50.78%] [G loss: 1.802497]\n",
      "epoch:28 step:26966 [D loss: 0.652049, acc: 60.94%] [G loss: 1.826306]\n",
      "epoch:28 step:26967 [D loss: 0.680396, acc: 55.47%] [G loss: 1.816460]\n",
      "epoch:28 step:26968 [D loss: 0.678974, acc: 57.81%] [G loss: 1.689282]\n",
      "epoch:28 step:26969 [D loss: 0.659643, acc: 60.94%] [G loss: 1.757072]\n",
      "epoch:28 step:26970 [D loss: 0.681065, acc: 53.12%] [G loss: 1.652459]\n",
      "epoch:28 step:26971 [D loss: 0.669811, acc: 60.94%] [G loss: 1.790460]\n",
      "epoch:28 step:26972 [D loss: 0.619240, acc: 60.94%] [G loss: 1.784178]\n",
      "epoch:28 step:26973 [D loss: 0.675918, acc: 57.03%] [G loss: 1.732259]\n",
      "epoch:28 step:26974 [D loss: 0.637759, acc: 65.62%] [G loss: 1.715905]\n",
      "epoch:28 step:26975 [D loss: 0.657086, acc: 62.50%] [G loss: 1.683682]\n",
      "epoch:28 step:26976 [D loss: 0.635406, acc: 60.94%] [G loss: 1.949500]\n",
      "epoch:28 step:26977 [D loss: 0.684738, acc: 55.47%] [G loss: 1.659255]\n",
      "epoch:28 step:26978 [D loss: 0.678347, acc: 61.72%] [G loss: 1.723279]\n",
      "epoch:28 step:26979 [D loss: 0.624763, acc: 62.50%] [G loss: 1.830886]\n",
      "epoch:28 step:26980 [D loss: 0.637323, acc: 64.84%] [G loss: 1.854101]\n",
      "epoch:28 step:26981 [D loss: 0.672723, acc: 60.94%] [G loss: 1.726959]\n",
      "epoch:28 step:26982 [D loss: 0.644011, acc: 64.84%] [G loss: 1.798831]\n",
      "epoch:28 step:26983 [D loss: 0.609787, acc: 66.41%] [G loss: 1.934188]\n",
      "epoch:28 step:26984 [D loss: 0.675346, acc: 56.25%] [G loss: 1.718306]\n",
      "epoch:28 step:26985 [D loss: 0.664593, acc: 64.06%] [G loss: 1.766904]\n",
      "epoch:28 step:26986 [D loss: 0.645696, acc: 59.38%] [G loss: 1.779876]\n",
      "epoch:28 step:26987 [D loss: 0.646806, acc: 60.94%] [G loss: 1.842508]\n",
      "epoch:28 step:26988 [D loss: 0.653560, acc: 60.94%] [G loss: 1.718370]\n",
      "epoch:28 step:26989 [D loss: 0.687104, acc: 57.81%] [G loss: 1.812817]\n",
      "epoch:28 step:26990 [D loss: 0.626517, acc: 67.97%] [G loss: 1.878904]\n",
      "epoch:28 step:26991 [D loss: 0.651721, acc: 64.06%] [G loss: 1.803629]\n",
      "epoch:28 step:26992 [D loss: 0.655529, acc: 60.94%] [G loss: 1.736497]\n",
      "epoch:28 step:26993 [D loss: 0.664845, acc: 57.03%] [G loss: 1.903592]\n",
      "epoch:28 step:26994 [D loss: 0.673609, acc: 60.16%] [G loss: 1.698352]\n",
      "epoch:28 step:26995 [D loss: 0.654114, acc: 59.38%] [G loss: 1.835533]\n",
      "epoch:28 step:26996 [D loss: 0.665896, acc: 60.94%] [G loss: 1.833759]\n",
      "epoch:28 step:26997 [D loss: 0.647940, acc: 62.50%] [G loss: 1.830838]\n",
      "epoch:28 step:26998 [D loss: 0.640374, acc: 64.06%] [G loss: 1.749361]\n",
      "epoch:28 step:26999 [D loss: 0.629097, acc: 64.84%] [G loss: 1.797269]\n",
      "epoch:28 step:27000 [D loss: 0.642497, acc: 66.41%] [G loss: 1.820409]\n",
      "epoch:28 step:27001 [D loss: 0.671353, acc: 53.91%] [G loss: 1.777041]\n",
      "epoch:28 step:27002 [D loss: 0.638054, acc: 61.72%] [G loss: 1.925294]\n",
      "epoch:28 step:27003 [D loss: 0.597049, acc: 65.62%] [G loss: 1.878172]\n",
      "epoch:28 step:27004 [D loss: 0.698101, acc: 57.81%] [G loss: 1.958145]\n",
      "epoch:28 step:27005 [D loss: 0.653762, acc: 60.94%] [G loss: 1.967372]\n",
      "epoch:28 step:27006 [D loss: 0.636569, acc: 59.38%] [G loss: 1.794583]\n",
      "epoch:28 step:27007 [D loss: 0.677398, acc: 54.69%] [G loss: 1.918233]\n",
      "epoch:28 step:27008 [D loss: 0.664681, acc: 55.47%] [G loss: 1.881436]\n",
      "epoch:28 step:27009 [D loss: 0.680547, acc: 64.06%] [G loss: 1.846609]\n",
      "epoch:28 step:27010 [D loss: 0.599833, acc: 73.44%] [G loss: 2.140090]\n",
      "epoch:28 step:27011 [D loss: 0.592170, acc: 70.31%] [G loss: 2.113508]\n",
      "epoch:28 step:27012 [D loss: 0.632040, acc: 68.75%] [G loss: 2.007718]\n",
      "epoch:28 step:27013 [D loss: 0.675788, acc: 58.59%] [G loss: 1.930473]\n",
      "epoch:28 step:27014 [D loss: 0.623552, acc: 67.19%] [G loss: 1.858704]\n",
      "epoch:28 step:27015 [D loss: 0.678227, acc: 51.56%] [G loss: 1.841096]\n",
      "epoch:28 step:27016 [D loss: 0.614798, acc: 68.75%] [G loss: 1.869790]\n",
      "epoch:28 step:27017 [D loss: 0.658185, acc: 65.62%] [G loss: 2.062794]\n",
      "epoch:28 step:27018 [D loss: 0.630268, acc: 64.06%] [G loss: 1.993849]\n",
      "epoch:28 step:27019 [D loss: 0.681063, acc: 57.03%] [G loss: 1.922850]\n",
      "epoch:28 step:27020 [D loss: 0.649096, acc: 60.16%] [G loss: 1.759414]\n",
      "epoch:28 step:27021 [D loss: 0.671586, acc: 60.94%] [G loss: 1.875389]\n",
      "epoch:28 step:27022 [D loss: 0.565165, acc: 67.97%] [G loss: 1.945252]\n",
      "epoch:28 step:27023 [D loss: 0.658108, acc: 58.59%] [G loss: 1.972677]\n",
      "epoch:28 step:27024 [D loss: 0.744231, acc: 50.00%] [G loss: 1.807079]\n",
      "epoch:28 step:27025 [D loss: 0.641996, acc: 68.75%] [G loss: 1.980972]\n",
      "epoch:28 step:27026 [D loss: 0.614080, acc: 62.50%] [G loss: 1.834169]\n",
      "epoch:28 step:27027 [D loss: 0.652640, acc: 61.72%] [G loss: 1.942999]\n",
      "epoch:28 step:27028 [D loss: 0.602129, acc: 71.09%] [G loss: 2.088884]\n",
      "epoch:28 step:27029 [D loss: 0.645159, acc: 62.50%] [G loss: 1.958667]\n",
      "epoch:28 step:27030 [D loss: 0.730232, acc: 46.09%] [G loss: 1.792246]\n",
      "epoch:28 step:27031 [D loss: 0.674952, acc: 57.81%] [G loss: 1.836413]\n",
      "epoch:28 step:27032 [D loss: 0.642241, acc: 59.38%] [G loss: 1.897494]\n",
      "epoch:28 step:27033 [D loss: 0.642006, acc: 64.06%] [G loss: 1.819269]\n",
      "epoch:28 step:27034 [D loss: 0.717726, acc: 50.78%] [G loss: 1.650083]\n",
      "epoch:28 step:27035 [D loss: 0.630440, acc: 60.16%] [G loss: 1.786134]\n",
      "epoch:28 step:27036 [D loss: 0.744428, acc: 48.44%] [G loss: 1.786422]\n",
      "epoch:28 step:27037 [D loss: 0.691596, acc: 57.81%] [G loss: 1.709561]\n",
      "epoch:28 step:27038 [D loss: 0.667037, acc: 60.16%] [G loss: 1.724528]\n",
      "epoch:28 step:27039 [D loss: 0.671109, acc: 58.59%] [G loss: 1.795391]\n",
      "epoch:28 step:27040 [D loss: 0.645680, acc: 61.72%] [G loss: 1.836472]\n",
      "epoch:28 step:27041 [D loss: 0.664980, acc: 60.94%] [G loss: 1.825151]\n",
      "epoch:28 step:27042 [D loss: 0.638625, acc: 64.84%] [G loss: 1.774464]\n",
      "epoch:28 step:27043 [D loss: 0.622241, acc: 64.84%] [G loss: 1.813912]\n",
      "epoch:28 step:27044 [D loss: 0.639698, acc: 62.50%] [G loss: 1.886318]\n",
      "epoch:28 step:27045 [D loss: 0.628497, acc: 67.19%] [G loss: 1.801346]\n",
      "epoch:28 step:27046 [D loss: 0.669122, acc: 56.25%] [G loss: 1.769508]\n",
      "epoch:28 step:27047 [D loss: 0.636806, acc: 61.72%] [G loss: 1.797382]\n",
      "epoch:28 step:27048 [D loss: 0.634756, acc: 65.62%] [G loss: 1.879991]\n",
      "epoch:28 step:27049 [D loss: 0.589382, acc: 75.00%] [G loss: 1.959675]\n",
      "epoch:28 step:27050 [D loss: 0.606776, acc: 65.62%] [G loss: 1.890512]\n",
      "epoch:28 step:27051 [D loss: 0.656832, acc: 65.62%] [G loss: 2.008604]\n",
      "epoch:28 step:27052 [D loss: 0.630449, acc: 67.19%] [G loss: 2.006985]\n",
      "epoch:28 step:27053 [D loss: 0.656471, acc: 63.28%] [G loss: 1.727022]\n",
      "epoch:28 step:27054 [D loss: 0.706720, acc: 57.81%] [G loss: 1.726998]\n",
      "epoch:28 step:27055 [D loss: 0.628272, acc: 63.28%] [G loss: 1.822147]\n",
      "epoch:28 step:27056 [D loss: 0.733677, acc: 50.78%] [G loss: 1.706442]\n",
      "epoch:28 step:27057 [D loss: 0.684735, acc: 53.91%] [G loss: 1.798820]\n",
      "epoch:28 step:27058 [D loss: 0.630941, acc: 64.06%] [G loss: 1.899408]\n",
      "epoch:28 step:27059 [D loss: 0.615663, acc: 66.41%] [G loss: 1.930883]\n",
      "epoch:28 step:27060 [D loss: 0.646236, acc: 63.28%] [G loss: 1.836402]\n",
      "epoch:28 step:27061 [D loss: 0.639029, acc: 61.72%] [G loss: 1.920063]\n",
      "epoch:28 step:27062 [D loss: 0.627890, acc: 64.84%] [G loss: 1.929040]\n",
      "epoch:28 step:27063 [D loss: 0.660646, acc: 59.38%] [G loss: 1.815312]\n",
      "epoch:28 step:27064 [D loss: 0.710789, acc: 48.44%] [G loss: 1.829093]\n",
      "epoch:28 step:27065 [D loss: 0.634749, acc: 65.62%] [G loss: 1.798103]\n",
      "epoch:28 step:27066 [D loss: 0.658821, acc: 61.72%] [G loss: 1.807647]\n",
      "epoch:28 step:27067 [D loss: 0.689384, acc: 54.69%] [G loss: 1.979971]\n",
      "epoch:28 step:27068 [D loss: 0.616506, acc: 67.19%] [G loss: 1.953740]\n",
      "epoch:28 step:27069 [D loss: 0.622525, acc: 62.50%] [G loss: 1.846157]\n",
      "epoch:28 step:27070 [D loss: 0.637674, acc: 67.97%] [G loss: 1.834669]\n",
      "epoch:28 step:27071 [D loss: 0.700401, acc: 52.34%] [G loss: 1.736801]\n",
      "epoch:28 step:27072 [D loss: 0.601319, acc: 64.84%] [G loss: 1.835959]\n",
      "epoch:28 step:27073 [D loss: 0.565370, acc: 71.88%] [G loss: 1.877473]\n",
      "epoch:28 step:27074 [D loss: 0.583701, acc: 67.19%] [G loss: 1.943835]\n",
      "epoch:28 step:27075 [D loss: 0.663450, acc: 61.72%] [G loss: 1.839565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:27076 [D loss: 0.624617, acc: 69.53%] [G loss: 1.871282]\n",
      "epoch:28 step:27077 [D loss: 0.592795, acc: 68.75%] [G loss: 1.978082]\n",
      "epoch:28 step:27078 [D loss: 0.621935, acc: 64.84%] [G loss: 2.058328]\n",
      "epoch:28 step:27079 [D loss: 0.718412, acc: 57.81%] [G loss: 1.890598]\n",
      "epoch:28 step:27080 [D loss: 0.705852, acc: 49.22%] [G loss: 1.883001]\n",
      "epoch:28 step:27081 [D loss: 0.619054, acc: 66.41%] [G loss: 1.998261]\n",
      "epoch:28 step:27082 [D loss: 0.631254, acc: 64.84%] [G loss: 1.922519]\n",
      "epoch:28 step:27083 [D loss: 0.656067, acc: 64.06%] [G loss: 1.876599]\n",
      "epoch:28 step:27084 [D loss: 0.626003, acc: 58.59%] [G loss: 1.858084]\n",
      "epoch:28 step:27085 [D loss: 0.607695, acc: 68.75%] [G loss: 1.982751]\n",
      "epoch:28 step:27086 [D loss: 0.676045, acc: 58.59%] [G loss: 1.975842]\n",
      "epoch:28 step:27087 [D loss: 0.668070, acc: 62.50%] [G loss: 1.768663]\n",
      "epoch:28 step:27088 [D loss: 0.642615, acc: 62.50%] [G loss: 1.811664]\n",
      "epoch:28 step:27089 [D loss: 0.633188, acc: 63.28%] [G loss: 1.942806]\n",
      "epoch:28 step:27090 [D loss: 0.653636, acc: 64.84%] [G loss: 1.864677]\n",
      "epoch:28 step:27091 [D loss: 0.699855, acc: 58.59%] [G loss: 1.676487]\n",
      "epoch:28 step:27092 [D loss: 0.647904, acc: 61.72%] [G loss: 1.894148]\n",
      "epoch:28 step:27093 [D loss: 0.640901, acc: 58.59%] [G loss: 1.990459]\n",
      "epoch:28 step:27094 [D loss: 0.760617, acc: 48.44%] [G loss: 1.863709]\n",
      "epoch:28 step:27095 [D loss: 0.689145, acc: 59.38%] [G loss: 1.751456]\n",
      "epoch:28 step:27096 [D loss: 0.614388, acc: 61.72%] [G loss: 1.978170]\n",
      "epoch:28 step:27097 [D loss: 0.688067, acc: 54.69%] [G loss: 1.781548]\n",
      "epoch:28 step:27098 [D loss: 0.680225, acc: 53.91%] [G loss: 1.751740]\n",
      "epoch:28 step:27099 [D loss: 0.642739, acc: 64.84%] [G loss: 1.977862]\n",
      "epoch:28 step:27100 [D loss: 0.704033, acc: 55.47%] [G loss: 1.794846]\n",
      "epoch:28 step:27101 [D loss: 0.653835, acc: 59.38%] [G loss: 1.692010]\n",
      "epoch:28 step:27102 [D loss: 0.658477, acc: 63.28%] [G loss: 1.907977]\n",
      "epoch:28 step:27103 [D loss: 0.679884, acc: 56.25%] [G loss: 1.803802]\n",
      "epoch:28 step:27104 [D loss: 0.627223, acc: 64.06%] [G loss: 1.796203]\n",
      "epoch:28 step:27105 [D loss: 0.655375, acc: 63.28%] [G loss: 1.797496]\n",
      "epoch:28 step:27106 [D loss: 0.671731, acc: 58.59%] [G loss: 1.823605]\n",
      "epoch:28 step:27107 [D loss: 0.639797, acc: 58.59%] [G loss: 1.803055]\n",
      "epoch:28 step:27108 [D loss: 0.657260, acc: 57.81%] [G loss: 1.862869]\n",
      "epoch:28 step:27109 [D loss: 0.709545, acc: 52.34%] [G loss: 1.788800]\n",
      "epoch:28 step:27110 [D loss: 0.701561, acc: 53.12%] [G loss: 1.750121]\n",
      "epoch:28 step:27111 [D loss: 0.641807, acc: 60.16%] [G loss: 1.832992]\n",
      "epoch:28 step:27112 [D loss: 0.621638, acc: 68.75%] [G loss: 1.773524]\n",
      "epoch:28 step:27113 [D loss: 0.623924, acc: 67.19%] [G loss: 1.747234]\n",
      "epoch:28 step:27114 [D loss: 0.655340, acc: 62.50%] [G loss: 1.796302]\n",
      "epoch:28 step:27115 [D loss: 0.626701, acc: 63.28%] [G loss: 1.749580]\n",
      "epoch:28 step:27116 [D loss: 0.669059, acc: 56.25%] [G loss: 1.859324]\n",
      "epoch:28 step:27117 [D loss: 0.719533, acc: 52.34%] [G loss: 1.748538]\n",
      "epoch:28 step:27118 [D loss: 0.689958, acc: 57.03%] [G loss: 1.918109]\n",
      "epoch:28 step:27119 [D loss: 0.698119, acc: 54.69%] [G loss: 1.897530]\n",
      "epoch:28 step:27120 [D loss: 0.657799, acc: 59.38%] [G loss: 1.922230]\n",
      "epoch:28 step:27121 [D loss: 0.644445, acc: 63.28%] [G loss: 1.817994]\n",
      "epoch:28 step:27122 [D loss: 0.633330, acc: 64.06%] [G loss: 1.854986]\n",
      "epoch:28 step:27123 [D loss: 0.618405, acc: 64.84%] [G loss: 1.792990]\n",
      "epoch:28 step:27124 [D loss: 0.699428, acc: 52.34%] [G loss: 1.783618]\n",
      "epoch:28 step:27125 [D loss: 0.666815, acc: 61.72%] [G loss: 1.934231]\n",
      "epoch:28 step:27126 [D loss: 0.656607, acc: 60.16%] [G loss: 1.941888]\n",
      "epoch:28 step:27127 [D loss: 0.635714, acc: 60.16%] [G loss: 1.874009]\n",
      "epoch:28 step:27128 [D loss: 0.666789, acc: 64.06%] [G loss: 1.850250]\n",
      "epoch:28 step:27129 [D loss: 0.609757, acc: 65.62%] [G loss: 1.905326]\n",
      "epoch:28 step:27130 [D loss: 0.638073, acc: 60.16%] [G loss: 1.902544]\n",
      "epoch:28 step:27131 [D loss: 0.635621, acc: 63.28%] [G loss: 1.902295]\n",
      "epoch:28 step:27132 [D loss: 0.615673, acc: 65.62%] [G loss: 1.892766]\n",
      "epoch:28 step:27133 [D loss: 0.639262, acc: 67.19%] [G loss: 1.718852]\n",
      "epoch:28 step:27134 [D loss: 0.620175, acc: 63.28%] [G loss: 1.798359]\n",
      "epoch:28 step:27135 [D loss: 0.652569, acc: 59.38%] [G loss: 1.951531]\n",
      "epoch:28 step:27136 [D loss: 0.659063, acc: 63.28%] [G loss: 1.889649]\n",
      "epoch:28 step:27137 [D loss: 0.576664, acc: 72.66%] [G loss: 1.909506]\n",
      "epoch:28 step:27138 [D loss: 0.692048, acc: 57.03%] [G loss: 1.979815]\n",
      "epoch:28 step:27139 [D loss: 0.655868, acc: 60.16%] [G loss: 1.936035]\n",
      "epoch:28 step:27140 [D loss: 0.641238, acc: 60.94%] [G loss: 2.032343]\n",
      "epoch:28 step:27141 [D loss: 0.631633, acc: 63.28%] [G loss: 1.990839]\n",
      "epoch:28 step:27142 [D loss: 0.639267, acc: 63.28%] [G loss: 1.963395]\n",
      "epoch:28 step:27143 [D loss: 0.622902, acc: 67.19%] [G loss: 2.044381]\n",
      "epoch:28 step:27144 [D loss: 0.654272, acc: 65.62%] [G loss: 1.865984]\n",
      "epoch:28 step:27145 [D loss: 0.562846, acc: 76.56%] [G loss: 2.049284]\n",
      "epoch:28 step:27146 [D loss: 0.645754, acc: 64.06%] [G loss: 2.037791]\n",
      "epoch:28 step:27147 [D loss: 0.657980, acc: 66.41%] [G loss: 1.953286]\n",
      "epoch:28 step:27148 [D loss: 0.639500, acc: 59.38%] [G loss: 2.075390]\n",
      "epoch:28 step:27149 [D loss: 0.669039, acc: 58.59%] [G loss: 1.874503]\n",
      "epoch:28 step:27150 [D loss: 0.675728, acc: 53.91%] [G loss: 1.907963]\n",
      "epoch:28 step:27151 [D loss: 0.657184, acc: 60.94%] [G loss: 1.995997]\n",
      "epoch:28 step:27152 [D loss: 0.600567, acc: 70.31%] [G loss: 1.997038]\n",
      "epoch:28 step:27153 [D loss: 0.720599, acc: 50.78%] [G loss: 1.831706]\n",
      "epoch:28 step:27154 [D loss: 0.644331, acc: 65.62%] [G loss: 2.088207]\n",
      "epoch:28 step:27155 [D loss: 0.555822, acc: 71.09%] [G loss: 2.183873]\n",
      "epoch:28 step:27156 [D loss: 0.700880, acc: 51.56%] [G loss: 1.844873]\n",
      "epoch:28 step:27157 [D loss: 0.660336, acc: 60.16%] [G loss: 1.920175]\n",
      "epoch:28 step:27158 [D loss: 0.706423, acc: 53.91%] [G loss: 1.855094]\n",
      "epoch:28 step:27159 [D loss: 0.621326, acc: 61.72%] [G loss: 2.015839]\n",
      "epoch:28 step:27160 [D loss: 0.579937, acc: 72.66%] [G loss: 2.051981]\n",
      "epoch:28 step:27161 [D loss: 0.655096, acc: 61.72%] [G loss: 2.073004]\n",
      "epoch:28 step:27162 [D loss: 0.630614, acc: 67.97%] [G loss: 2.012867]\n",
      "epoch:28 step:27163 [D loss: 0.638507, acc: 61.72%] [G loss: 1.941982]\n",
      "epoch:28 step:27164 [D loss: 0.710693, acc: 54.69%] [G loss: 1.815209]\n",
      "epoch:28 step:27165 [D loss: 0.723686, acc: 50.00%] [G loss: 1.845117]\n",
      "epoch:28 step:27166 [D loss: 0.623061, acc: 63.28%] [G loss: 2.054296]\n",
      "epoch:28 step:27167 [D loss: 0.663346, acc: 60.16%] [G loss: 1.908852]\n",
      "epoch:28 step:27168 [D loss: 0.682766, acc: 57.81%] [G loss: 1.924010]\n",
      "epoch:28 step:27169 [D loss: 0.601516, acc: 71.09%] [G loss: 1.958452]\n",
      "epoch:28 step:27170 [D loss: 0.649169, acc: 60.16%] [G loss: 1.924571]\n",
      "epoch:28 step:27171 [D loss: 0.603483, acc: 64.06%] [G loss: 2.032798]\n",
      "epoch:28 step:27172 [D loss: 0.592375, acc: 65.62%] [G loss: 1.972959]\n",
      "epoch:28 step:27173 [D loss: 0.593309, acc: 65.62%] [G loss: 2.541201]\n",
      "epoch:29 step:27174 [D loss: 0.719306, acc: 54.69%] [G loss: 1.801640]\n",
      "epoch:29 step:27175 [D loss: 0.657589, acc: 62.50%] [G loss: 1.911384]\n",
      "epoch:29 step:27176 [D loss: 0.671414, acc: 65.62%] [G loss: 1.906863]\n",
      "epoch:29 step:27177 [D loss: 0.659222, acc: 60.94%] [G loss: 1.702714]\n",
      "epoch:29 step:27178 [D loss: 0.666715, acc: 59.38%] [G loss: 1.890985]\n",
      "epoch:29 step:27179 [D loss: 0.653401, acc: 60.94%] [G loss: 1.881781]\n",
      "epoch:29 step:27180 [D loss: 0.659667, acc: 57.03%] [G loss: 1.896131]\n",
      "epoch:29 step:27181 [D loss: 0.588215, acc: 72.66%] [G loss: 1.942862]\n",
      "epoch:29 step:27182 [D loss: 0.608328, acc: 65.62%] [G loss: 1.948555]\n",
      "epoch:29 step:27183 [D loss: 0.649977, acc: 65.62%] [G loss: 1.953312]\n",
      "epoch:29 step:27184 [D loss: 0.635722, acc: 64.06%] [G loss: 1.991105]\n",
      "epoch:29 step:27185 [D loss: 0.652170, acc: 58.59%] [G loss: 1.847855]\n",
      "epoch:29 step:27186 [D loss: 0.606089, acc: 65.62%] [G loss: 1.813537]\n",
      "epoch:29 step:27187 [D loss: 0.656386, acc: 60.94%] [G loss: 2.044292]\n",
      "epoch:29 step:27188 [D loss: 0.639623, acc: 61.72%] [G loss: 1.915093]\n",
      "epoch:29 step:27189 [D loss: 0.587736, acc: 70.31%] [G loss: 2.065199]\n",
      "epoch:29 step:27190 [D loss: 0.657542, acc: 63.28%] [G loss: 1.896937]\n",
      "epoch:29 step:27191 [D loss: 0.633903, acc: 64.84%] [G loss: 1.995915]\n",
      "epoch:29 step:27192 [D loss: 0.659002, acc: 57.03%] [G loss: 1.713393]\n",
      "epoch:29 step:27193 [D loss: 0.749114, acc: 49.22%] [G loss: 1.615372]\n",
      "epoch:29 step:27194 [D loss: 0.653688, acc: 53.91%] [G loss: 1.858808]\n",
      "epoch:29 step:27195 [D loss: 0.690071, acc: 57.81%] [G loss: 1.742886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27196 [D loss: 0.630341, acc: 62.50%] [G loss: 1.956699]\n",
      "epoch:29 step:27197 [D loss: 0.581120, acc: 68.75%] [G loss: 1.973792]\n",
      "epoch:29 step:27198 [D loss: 0.630011, acc: 65.62%] [G loss: 1.904786]\n",
      "epoch:29 step:27199 [D loss: 0.681251, acc: 56.25%] [G loss: 1.790107]\n",
      "epoch:29 step:27200 [D loss: 0.649934, acc: 59.38%] [G loss: 1.792515]\n",
      "epoch:29 step:27201 [D loss: 0.643084, acc: 58.59%] [G loss: 1.889063]\n",
      "epoch:29 step:27202 [D loss: 0.630010, acc: 64.84%] [G loss: 1.912127]\n",
      "epoch:29 step:27203 [D loss: 0.709940, acc: 53.91%] [G loss: 1.794165]\n",
      "epoch:29 step:27204 [D loss: 0.688229, acc: 53.12%] [G loss: 1.769734]\n",
      "epoch:29 step:27205 [D loss: 0.709908, acc: 53.12%] [G loss: 1.740396]\n",
      "epoch:29 step:27206 [D loss: 0.665629, acc: 60.16%] [G loss: 1.761109]\n",
      "epoch:29 step:27207 [D loss: 0.637617, acc: 67.97%] [G loss: 1.736720]\n",
      "epoch:29 step:27208 [D loss: 0.692057, acc: 58.59%] [G loss: 1.798669]\n",
      "epoch:29 step:27209 [D loss: 0.620070, acc: 65.62%] [G loss: 1.972480]\n",
      "epoch:29 step:27210 [D loss: 0.622241, acc: 67.19%] [G loss: 1.978825]\n",
      "epoch:29 step:27211 [D loss: 0.673120, acc: 62.50%] [G loss: 1.993102]\n",
      "epoch:29 step:27212 [D loss: 0.669366, acc: 55.47%] [G loss: 1.861022]\n",
      "epoch:29 step:27213 [D loss: 0.652141, acc: 62.50%] [G loss: 1.949963]\n",
      "epoch:29 step:27214 [D loss: 0.679707, acc: 60.16%] [G loss: 1.895427]\n",
      "epoch:29 step:27215 [D loss: 0.685690, acc: 52.34%] [G loss: 1.785829]\n",
      "epoch:29 step:27216 [D loss: 0.629075, acc: 61.72%] [G loss: 1.883950]\n",
      "epoch:29 step:27217 [D loss: 0.631117, acc: 64.84%] [G loss: 1.747561]\n",
      "epoch:29 step:27218 [D loss: 0.652221, acc: 63.28%] [G loss: 1.822635]\n",
      "epoch:29 step:27219 [D loss: 0.663042, acc: 63.28%] [G loss: 1.862881]\n",
      "epoch:29 step:27220 [D loss: 0.654636, acc: 57.81%] [G loss: 1.859476]\n",
      "epoch:29 step:27221 [D loss: 0.683139, acc: 60.16%] [G loss: 1.969402]\n",
      "epoch:29 step:27222 [D loss: 0.598118, acc: 65.62%] [G loss: 1.844071]\n",
      "epoch:29 step:27223 [D loss: 0.642852, acc: 63.28%] [G loss: 1.886845]\n",
      "epoch:29 step:27224 [D loss: 0.630345, acc: 71.88%] [G loss: 1.786278]\n",
      "epoch:29 step:27225 [D loss: 0.666724, acc: 58.59%] [G loss: 1.789480]\n",
      "epoch:29 step:27226 [D loss: 0.643469, acc: 62.50%] [G loss: 1.801658]\n",
      "epoch:29 step:27227 [D loss: 0.693654, acc: 60.94%] [G loss: 1.907901]\n",
      "epoch:29 step:27228 [D loss: 0.635955, acc: 60.16%] [G loss: 1.997046]\n",
      "epoch:29 step:27229 [D loss: 0.676878, acc: 57.81%] [G loss: 2.070354]\n",
      "epoch:29 step:27230 [D loss: 0.666840, acc: 56.25%] [G loss: 1.819608]\n",
      "epoch:29 step:27231 [D loss: 0.625871, acc: 61.72%] [G loss: 1.835231]\n",
      "epoch:29 step:27232 [D loss: 0.629026, acc: 64.06%] [G loss: 1.881753]\n",
      "epoch:29 step:27233 [D loss: 0.620955, acc: 67.19%] [G loss: 1.876667]\n",
      "epoch:29 step:27234 [D loss: 0.649112, acc: 58.59%] [G loss: 1.782667]\n",
      "epoch:29 step:27235 [D loss: 0.696619, acc: 58.59%] [G loss: 1.851674]\n",
      "epoch:29 step:27236 [D loss: 0.701001, acc: 54.69%] [G loss: 1.894927]\n",
      "epoch:29 step:27237 [D loss: 0.622832, acc: 67.97%] [G loss: 1.763262]\n",
      "epoch:29 step:27238 [D loss: 0.650902, acc: 64.84%] [G loss: 1.892498]\n",
      "epoch:29 step:27239 [D loss: 0.651197, acc: 62.50%] [G loss: 1.827031]\n",
      "epoch:29 step:27240 [D loss: 0.643437, acc: 61.72%] [G loss: 1.841751]\n",
      "epoch:29 step:27241 [D loss: 0.625165, acc: 67.97%] [G loss: 1.923816]\n",
      "epoch:29 step:27242 [D loss: 0.702577, acc: 56.25%] [G loss: 1.911086]\n",
      "epoch:29 step:27243 [D loss: 0.596508, acc: 66.41%] [G loss: 1.853976]\n",
      "epoch:29 step:27244 [D loss: 0.648492, acc: 62.50%] [G loss: 1.883762]\n",
      "epoch:29 step:27245 [D loss: 0.709353, acc: 51.56%] [G loss: 1.773189]\n",
      "epoch:29 step:27246 [D loss: 0.686244, acc: 57.81%] [G loss: 1.705317]\n",
      "epoch:29 step:27247 [D loss: 0.580645, acc: 68.75%] [G loss: 1.825557]\n",
      "epoch:29 step:27248 [D loss: 0.608182, acc: 70.31%] [G loss: 1.887690]\n",
      "epoch:29 step:27249 [D loss: 0.666033, acc: 62.50%] [G loss: 1.961435]\n",
      "epoch:29 step:27250 [D loss: 0.606774, acc: 69.53%] [G loss: 1.989156]\n",
      "epoch:29 step:27251 [D loss: 0.621733, acc: 60.94%] [G loss: 1.841536]\n",
      "epoch:29 step:27252 [D loss: 0.696974, acc: 51.56%] [G loss: 1.873587]\n",
      "epoch:29 step:27253 [D loss: 0.654777, acc: 64.06%] [G loss: 1.741701]\n",
      "epoch:29 step:27254 [D loss: 0.702875, acc: 60.94%] [G loss: 1.740941]\n",
      "epoch:29 step:27255 [D loss: 0.640806, acc: 61.72%] [G loss: 1.724322]\n",
      "epoch:29 step:27256 [D loss: 0.660045, acc: 57.03%] [G loss: 1.759182]\n",
      "epoch:29 step:27257 [D loss: 0.624872, acc: 64.06%] [G loss: 1.951706]\n",
      "epoch:29 step:27258 [D loss: 0.704919, acc: 53.91%] [G loss: 1.688444]\n",
      "epoch:29 step:27259 [D loss: 0.648350, acc: 63.28%] [G loss: 1.783387]\n",
      "epoch:29 step:27260 [D loss: 0.661592, acc: 62.50%] [G loss: 1.933698]\n",
      "epoch:29 step:27261 [D loss: 0.617467, acc: 64.84%] [G loss: 1.861856]\n",
      "epoch:29 step:27262 [D loss: 0.638125, acc: 64.06%] [G loss: 1.919576]\n",
      "epoch:29 step:27263 [D loss: 0.624313, acc: 62.50%] [G loss: 1.772148]\n",
      "epoch:29 step:27264 [D loss: 0.679721, acc: 56.25%] [G loss: 1.784713]\n",
      "epoch:29 step:27265 [D loss: 0.663816, acc: 61.72%] [G loss: 2.022058]\n",
      "epoch:29 step:27266 [D loss: 0.601067, acc: 70.31%] [G loss: 1.859831]\n",
      "epoch:29 step:27267 [D loss: 0.674186, acc: 60.16%] [G loss: 1.825411]\n",
      "epoch:29 step:27268 [D loss: 0.639984, acc: 67.19%] [G loss: 1.877126]\n",
      "epoch:29 step:27269 [D loss: 0.701332, acc: 57.03%] [G loss: 1.761773]\n",
      "epoch:29 step:27270 [D loss: 0.657245, acc: 61.72%] [G loss: 1.904906]\n",
      "epoch:29 step:27271 [D loss: 0.635172, acc: 65.62%] [G loss: 1.771912]\n",
      "epoch:29 step:27272 [D loss: 0.640180, acc: 60.94%] [G loss: 1.879026]\n",
      "epoch:29 step:27273 [D loss: 0.606948, acc: 67.19%] [G loss: 1.786022]\n",
      "epoch:29 step:27274 [D loss: 0.644029, acc: 64.06%] [G loss: 1.829482]\n",
      "epoch:29 step:27275 [D loss: 0.647114, acc: 61.72%] [G loss: 1.879177]\n",
      "epoch:29 step:27276 [D loss: 0.644700, acc: 60.94%] [G loss: 1.881235]\n",
      "epoch:29 step:27277 [D loss: 0.613447, acc: 67.97%] [G loss: 1.909207]\n",
      "epoch:29 step:27278 [D loss: 0.662889, acc: 58.59%] [G loss: 1.914872]\n",
      "epoch:29 step:27279 [D loss: 0.618756, acc: 67.19%] [G loss: 1.860820]\n",
      "epoch:29 step:27280 [D loss: 0.613237, acc: 71.09%] [G loss: 2.163383]\n",
      "epoch:29 step:27281 [D loss: 0.689711, acc: 58.59%] [G loss: 1.805149]\n",
      "epoch:29 step:27282 [D loss: 0.701174, acc: 53.91%] [G loss: 1.699884]\n",
      "epoch:29 step:27283 [D loss: 0.662522, acc: 60.94%] [G loss: 1.894850]\n",
      "epoch:29 step:27284 [D loss: 0.628337, acc: 64.84%] [G loss: 1.894795]\n",
      "epoch:29 step:27285 [D loss: 0.668012, acc: 57.81%] [G loss: 1.911868]\n",
      "epoch:29 step:27286 [D loss: 0.635201, acc: 65.62%] [G loss: 2.098678]\n",
      "epoch:29 step:27287 [D loss: 0.664140, acc: 62.50%] [G loss: 2.047190]\n",
      "epoch:29 step:27288 [D loss: 0.667325, acc: 64.06%] [G loss: 2.197966]\n",
      "epoch:29 step:27289 [D loss: 0.625720, acc: 66.41%] [G loss: 2.176661]\n",
      "epoch:29 step:27290 [D loss: 0.637511, acc: 70.31%] [G loss: 2.157664]\n",
      "epoch:29 step:27291 [D loss: 0.650086, acc: 67.97%] [G loss: 2.059134]\n",
      "epoch:29 step:27292 [D loss: 0.519725, acc: 75.00%] [G loss: 2.284652]\n",
      "epoch:29 step:27293 [D loss: 0.674202, acc: 60.94%] [G loss: 1.941175]\n",
      "epoch:29 step:27294 [D loss: 0.677086, acc: 63.28%] [G loss: 2.000395]\n",
      "epoch:29 step:27295 [D loss: 0.623695, acc: 70.31%] [G loss: 2.098644]\n",
      "epoch:29 step:27296 [D loss: 0.727210, acc: 50.78%] [G loss: 1.930533]\n",
      "epoch:29 step:27297 [D loss: 0.686932, acc: 61.72%] [G loss: 1.824713]\n",
      "epoch:29 step:27298 [D loss: 0.636299, acc: 59.38%] [G loss: 1.671030]\n",
      "epoch:29 step:27299 [D loss: 0.637525, acc: 62.50%] [G loss: 1.981861]\n",
      "epoch:29 step:27300 [D loss: 0.650174, acc: 60.16%] [G loss: 1.938643]\n",
      "epoch:29 step:27301 [D loss: 0.705094, acc: 53.91%] [G loss: 1.816962]\n",
      "epoch:29 step:27302 [D loss: 0.643835, acc: 61.72%] [G loss: 1.893156]\n",
      "epoch:29 step:27303 [D loss: 0.644581, acc: 61.72%] [G loss: 1.948009]\n",
      "epoch:29 step:27304 [D loss: 0.642246, acc: 63.28%] [G loss: 1.805859]\n",
      "epoch:29 step:27305 [D loss: 0.635934, acc: 62.50%] [G loss: 1.702050]\n",
      "epoch:29 step:27306 [D loss: 0.743101, acc: 47.66%] [G loss: 1.663437]\n",
      "epoch:29 step:27307 [D loss: 0.699136, acc: 53.91%] [G loss: 1.723268]\n",
      "epoch:29 step:27308 [D loss: 0.668828, acc: 60.16%] [G loss: 1.771996]\n",
      "epoch:29 step:27309 [D loss: 0.698195, acc: 58.59%] [G loss: 1.816259]\n",
      "epoch:29 step:27310 [D loss: 0.682784, acc: 56.25%] [G loss: 1.834852]\n",
      "epoch:29 step:27311 [D loss: 0.678982, acc: 54.69%] [G loss: 1.789386]\n",
      "epoch:29 step:27312 [D loss: 0.656195, acc: 67.97%] [G loss: 1.729739]\n",
      "epoch:29 step:27313 [D loss: 0.652642, acc: 60.94%] [G loss: 1.777887]\n",
      "epoch:29 step:27314 [D loss: 0.675444, acc: 53.91%] [G loss: 1.667009]\n",
      "epoch:29 step:27315 [D loss: 0.683612, acc: 54.69%] [G loss: 1.863352]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27316 [D loss: 0.683138, acc: 63.28%] [G loss: 1.673828]\n",
      "epoch:29 step:27317 [D loss: 0.619774, acc: 69.53%] [G loss: 1.772357]\n",
      "epoch:29 step:27318 [D loss: 0.651907, acc: 63.28%] [G loss: 1.843421]\n",
      "epoch:29 step:27319 [D loss: 0.652563, acc: 58.59%] [G loss: 1.718649]\n",
      "epoch:29 step:27320 [D loss: 0.662076, acc: 58.59%] [G loss: 1.911224]\n",
      "epoch:29 step:27321 [D loss: 0.689362, acc: 60.16%] [G loss: 1.744626]\n",
      "epoch:29 step:27322 [D loss: 0.635642, acc: 61.72%] [G loss: 1.736540]\n",
      "epoch:29 step:27323 [D loss: 0.622107, acc: 62.50%] [G loss: 1.906940]\n",
      "epoch:29 step:27324 [D loss: 0.660774, acc: 60.94%] [G loss: 2.037768]\n",
      "epoch:29 step:27325 [D loss: 0.626154, acc: 62.50%] [G loss: 1.764619]\n",
      "epoch:29 step:27326 [D loss: 0.644042, acc: 61.72%] [G loss: 1.730979]\n",
      "epoch:29 step:27327 [D loss: 0.626482, acc: 65.62%] [G loss: 2.018969]\n",
      "epoch:29 step:27328 [D loss: 0.637681, acc: 60.16%] [G loss: 1.844992]\n",
      "epoch:29 step:27329 [D loss: 0.625698, acc: 68.75%] [G loss: 1.932708]\n",
      "epoch:29 step:27330 [D loss: 0.670795, acc: 54.69%] [G loss: 1.813510]\n",
      "epoch:29 step:27331 [D loss: 0.599836, acc: 73.44%] [G loss: 1.831575]\n",
      "epoch:29 step:27332 [D loss: 0.652130, acc: 63.28%] [G loss: 1.853794]\n",
      "epoch:29 step:27333 [D loss: 0.674010, acc: 52.34%] [G loss: 1.733681]\n",
      "epoch:29 step:27334 [D loss: 0.626046, acc: 63.28%] [G loss: 1.858147]\n",
      "epoch:29 step:27335 [D loss: 0.647458, acc: 63.28%] [G loss: 1.841069]\n",
      "epoch:29 step:27336 [D loss: 0.580295, acc: 71.09%] [G loss: 1.824571]\n",
      "epoch:29 step:27337 [D loss: 0.631553, acc: 63.28%] [G loss: 1.863851]\n",
      "epoch:29 step:27338 [D loss: 0.654352, acc: 61.72%] [G loss: 1.843661]\n",
      "epoch:29 step:27339 [D loss: 0.653476, acc: 59.38%] [G loss: 1.853687]\n",
      "epoch:29 step:27340 [D loss: 0.667166, acc: 57.81%] [G loss: 1.793635]\n",
      "epoch:29 step:27341 [D loss: 0.640945, acc: 63.28%] [G loss: 1.927260]\n",
      "epoch:29 step:27342 [D loss: 0.623319, acc: 64.84%] [G loss: 1.795148]\n",
      "epoch:29 step:27343 [D loss: 0.655422, acc: 60.94%] [G loss: 1.735261]\n",
      "epoch:29 step:27344 [D loss: 0.621522, acc: 65.62%] [G loss: 1.900789]\n",
      "epoch:29 step:27345 [D loss: 0.675096, acc: 62.50%] [G loss: 1.952380]\n",
      "epoch:29 step:27346 [D loss: 0.667148, acc: 58.59%] [G loss: 1.864159]\n",
      "epoch:29 step:27347 [D loss: 0.642950, acc: 66.41%] [G loss: 1.749569]\n",
      "epoch:29 step:27348 [D loss: 0.642294, acc: 64.84%] [G loss: 1.820388]\n",
      "epoch:29 step:27349 [D loss: 0.696084, acc: 55.47%] [G loss: 1.964612]\n",
      "epoch:29 step:27350 [D loss: 0.651270, acc: 64.84%] [G loss: 1.835456]\n",
      "epoch:29 step:27351 [D loss: 0.638466, acc: 64.84%] [G loss: 1.788901]\n",
      "epoch:29 step:27352 [D loss: 0.662692, acc: 64.06%] [G loss: 1.882322]\n",
      "epoch:29 step:27353 [D loss: 0.687493, acc: 57.03%] [G loss: 1.766839]\n",
      "epoch:29 step:27354 [D loss: 0.700146, acc: 59.38%] [G loss: 1.849818]\n",
      "epoch:29 step:27355 [D loss: 0.663971, acc: 61.72%] [G loss: 1.728039]\n",
      "epoch:29 step:27356 [D loss: 0.692722, acc: 55.47%] [G loss: 1.629298]\n",
      "epoch:29 step:27357 [D loss: 0.592695, acc: 69.53%] [G loss: 1.893113]\n",
      "epoch:29 step:27358 [D loss: 0.652178, acc: 62.50%] [G loss: 1.850195]\n",
      "epoch:29 step:27359 [D loss: 0.655901, acc: 58.59%] [G loss: 1.824133]\n",
      "epoch:29 step:27360 [D loss: 0.706019, acc: 57.03%] [G loss: 1.846415]\n",
      "epoch:29 step:27361 [D loss: 0.612872, acc: 62.50%] [G loss: 1.800600]\n",
      "epoch:29 step:27362 [D loss: 0.688998, acc: 53.91%] [G loss: 1.926211]\n",
      "epoch:29 step:27363 [D loss: 0.645347, acc: 64.06%] [G loss: 1.910967]\n",
      "epoch:29 step:27364 [D loss: 0.689019, acc: 57.03%] [G loss: 1.864365]\n",
      "epoch:29 step:27365 [D loss: 0.629889, acc: 60.94%] [G loss: 1.884189]\n",
      "epoch:29 step:27366 [D loss: 0.642926, acc: 60.16%] [G loss: 1.777965]\n",
      "epoch:29 step:27367 [D loss: 0.632409, acc: 60.16%] [G loss: 2.054271]\n",
      "epoch:29 step:27368 [D loss: 0.665481, acc: 59.38%] [G loss: 1.824301]\n",
      "epoch:29 step:27369 [D loss: 0.696536, acc: 57.81%] [G loss: 1.780779]\n",
      "epoch:29 step:27370 [D loss: 0.650417, acc: 60.94%] [G loss: 1.902056]\n",
      "epoch:29 step:27371 [D loss: 0.620367, acc: 67.97%] [G loss: 1.915738]\n",
      "epoch:29 step:27372 [D loss: 0.648066, acc: 62.50%] [G loss: 1.869447]\n",
      "epoch:29 step:27373 [D loss: 0.672389, acc: 60.94%] [G loss: 1.804229]\n",
      "epoch:29 step:27374 [D loss: 0.684155, acc: 59.38%] [G loss: 1.860561]\n",
      "epoch:29 step:27375 [D loss: 0.675569, acc: 60.94%] [G loss: 1.871968]\n",
      "epoch:29 step:27376 [D loss: 0.664240, acc: 65.62%] [G loss: 1.701805]\n",
      "epoch:29 step:27377 [D loss: 0.668169, acc: 59.38%] [G loss: 1.883030]\n",
      "epoch:29 step:27378 [D loss: 0.640996, acc: 60.16%] [G loss: 1.926181]\n",
      "epoch:29 step:27379 [D loss: 0.655818, acc: 63.28%] [G loss: 1.889499]\n",
      "epoch:29 step:27380 [D loss: 0.608406, acc: 71.88%] [G loss: 2.073610]\n",
      "epoch:29 step:27381 [D loss: 0.599477, acc: 67.19%] [G loss: 2.010510]\n",
      "epoch:29 step:27382 [D loss: 0.608143, acc: 67.19%] [G loss: 2.188013]\n",
      "epoch:29 step:27383 [D loss: 0.681333, acc: 58.59%] [G loss: 1.739464]\n",
      "epoch:29 step:27384 [D loss: 0.716850, acc: 54.69%] [G loss: 1.732338]\n",
      "epoch:29 step:27385 [D loss: 0.668573, acc: 53.91%] [G loss: 1.752889]\n",
      "epoch:29 step:27386 [D loss: 0.654534, acc: 59.38%] [G loss: 1.887395]\n",
      "epoch:29 step:27387 [D loss: 0.677541, acc: 57.03%] [G loss: 1.827508]\n",
      "epoch:29 step:27388 [D loss: 0.648464, acc: 67.97%] [G loss: 1.681780]\n",
      "epoch:29 step:27389 [D loss: 0.666305, acc: 57.81%] [G loss: 1.838295]\n",
      "epoch:29 step:27390 [D loss: 0.598956, acc: 71.09%] [G loss: 1.838188]\n",
      "epoch:29 step:27391 [D loss: 0.614891, acc: 67.19%] [G loss: 2.131434]\n",
      "epoch:29 step:27392 [D loss: 0.618422, acc: 71.09%] [G loss: 2.001430]\n",
      "epoch:29 step:27393 [D loss: 0.686985, acc: 57.03%] [G loss: 1.773895]\n",
      "epoch:29 step:27394 [D loss: 0.703976, acc: 57.81%] [G loss: 1.985581]\n",
      "epoch:29 step:27395 [D loss: 0.633634, acc: 68.75%] [G loss: 1.781843]\n",
      "epoch:29 step:27396 [D loss: 0.619849, acc: 66.41%] [G loss: 1.879085]\n",
      "epoch:29 step:27397 [D loss: 0.663631, acc: 53.12%] [G loss: 1.943563]\n",
      "epoch:29 step:27398 [D loss: 0.643749, acc: 64.06%] [G loss: 1.888126]\n",
      "epoch:29 step:27399 [D loss: 0.659644, acc: 60.94%] [G loss: 1.885937]\n",
      "epoch:29 step:27400 [D loss: 0.662842, acc: 65.62%] [G loss: 1.818967]\n",
      "epoch:29 step:27401 [D loss: 0.600275, acc: 71.09%] [G loss: 1.854559]\n",
      "epoch:29 step:27402 [D loss: 0.630456, acc: 65.62%] [G loss: 1.871304]\n",
      "epoch:29 step:27403 [D loss: 0.641257, acc: 64.06%] [G loss: 2.073895]\n",
      "epoch:29 step:27404 [D loss: 0.580034, acc: 65.62%] [G loss: 2.289150]\n",
      "epoch:29 step:27405 [D loss: 0.591362, acc: 70.31%] [G loss: 2.236589]\n",
      "epoch:29 step:27406 [D loss: 0.631293, acc: 60.94%] [G loss: 1.956997]\n",
      "epoch:29 step:27407 [D loss: 0.650488, acc: 65.62%] [G loss: 1.999371]\n",
      "epoch:29 step:27408 [D loss: 0.701175, acc: 54.69%] [G loss: 1.809479]\n",
      "epoch:29 step:27409 [D loss: 0.701348, acc: 53.12%] [G loss: 1.782718]\n",
      "epoch:29 step:27410 [D loss: 0.643344, acc: 66.41%] [G loss: 1.846672]\n",
      "epoch:29 step:27411 [D loss: 0.670131, acc: 63.28%] [G loss: 1.891170]\n",
      "epoch:29 step:27412 [D loss: 0.643552, acc: 60.94%] [G loss: 1.878216]\n",
      "epoch:29 step:27413 [D loss: 0.630651, acc: 61.72%] [G loss: 1.892303]\n",
      "epoch:29 step:27414 [D loss: 0.634295, acc: 64.84%] [G loss: 1.862284]\n",
      "epoch:29 step:27415 [D loss: 0.604148, acc: 72.66%] [G loss: 2.006166]\n",
      "epoch:29 step:27416 [D loss: 0.606781, acc: 67.19%] [G loss: 1.941566]\n",
      "epoch:29 step:27417 [D loss: 0.642511, acc: 58.59%] [G loss: 2.046088]\n",
      "epoch:29 step:27418 [D loss: 0.641921, acc: 60.94%] [G loss: 2.026263]\n",
      "epoch:29 step:27419 [D loss: 0.612776, acc: 66.41%] [G loss: 1.871184]\n",
      "epoch:29 step:27420 [D loss: 0.727806, acc: 56.25%] [G loss: 1.831616]\n",
      "epoch:29 step:27421 [D loss: 0.673602, acc: 61.72%] [G loss: 1.962022]\n",
      "epoch:29 step:27422 [D loss: 0.710517, acc: 54.69%] [G loss: 1.901021]\n",
      "epoch:29 step:27423 [D loss: 0.679428, acc: 60.16%] [G loss: 1.664936]\n",
      "epoch:29 step:27424 [D loss: 0.728402, acc: 50.00%] [G loss: 1.568724]\n",
      "epoch:29 step:27425 [D loss: 0.653890, acc: 59.38%] [G loss: 1.657847]\n",
      "epoch:29 step:27426 [D loss: 0.637972, acc: 64.06%] [G loss: 1.684805]\n",
      "epoch:29 step:27427 [D loss: 0.675669, acc: 58.59%] [G loss: 1.799869]\n",
      "epoch:29 step:27428 [D loss: 0.600254, acc: 64.84%] [G loss: 1.965703]\n",
      "epoch:29 step:27429 [D loss: 0.689759, acc: 55.47%] [G loss: 1.854903]\n",
      "epoch:29 step:27430 [D loss: 0.673838, acc: 58.59%] [G loss: 1.682397]\n",
      "epoch:29 step:27431 [D loss: 0.656459, acc: 60.16%] [G loss: 1.753329]\n",
      "epoch:29 step:27432 [D loss: 0.654583, acc: 62.50%] [G loss: 1.779017]\n",
      "epoch:29 step:27433 [D loss: 0.644220, acc: 64.84%] [G loss: 1.930020]\n",
      "epoch:29 step:27434 [D loss: 0.681961, acc: 54.69%] [G loss: 1.901726]\n",
      "epoch:29 step:27435 [D loss: 0.635859, acc: 61.72%] [G loss: 1.890667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27436 [D loss: 0.667805, acc: 64.84%] [G loss: 1.833089]\n",
      "epoch:29 step:27437 [D loss: 0.624233, acc: 64.84%] [G loss: 1.906287]\n",
      "epoch:29 step:27438 [D loss: 0.661567, acc: 54.69%] [G loss: 1.951015]\n",
      "epoch:29 step:27439 [D loss: 0.644046, acc: 64.06%] [G loss: 1.783733]\n",
      "epoch:29 step:27440 [D loss: 0.608571, acc: 73.44%] [G loss: 1.924208]\n",
      "epoch:29 step:27441 [D loss: 0.716628, acc: 50.78%] [G loss: 1.728943]\n",
      "epoch:29 step:27442 [D loss: 0.646168, acc: 57.81%] [G loss: 1.849181]\n",
      "epoch:29 step:27443 [D loss: 0.670536, acc: 65.62%] [G loss: 1.968904]\n",
      "epoch:29 step:27444 [D loss: 0.717022, acc: 47.66%] [G loss: 1.694256]\n",
      "epoch:29 step:27445 [D loss: 0.606163, acc: 66.41%] [G loss: 1.767788]\n",
      "epoch:29 step:27446 [D loss: 0.642362, acc: 68.75%] [G loss: 1.914702]\n",
      "epoch:29 step:27447 [D loss: 0.607121, acc: 67.97%] [G loss: 1.991267]\n",
      "epoch:29 step:27448 [D loss: 0.682364, acc: 57.81%] [G loss: 1.819773]\n",
      "epoch:29 step:27449 [D loss: 0.631070, acc: 60.16%] [G loss: 1.957534]\n",
      "epoch:29 step:27450 [D loss: 0.601672, acc: 63.28%] [G loss: 1.923714]\n",
      "epoch:29 step:27451 [D loss: 0.720600, acc: 53.12%] [G loss: 1.815155]\n",
      "epoch:29 step:27452 [D loss: 0.669824, acc: 64.06%] [G loss: 1.852413]\n",
      "epoch:29 step:27453 [D loss: 0.655798, acc: 64.84%] [G loss: 1.920758]\n",
      "epoch:29 step:27454 [D loss: 0.688899, acc: 62.50%] [G loss: 1.733472]\n",
      "epoch:29 step:27455 [D loss: 0.662510, acc: 60.16%] [G loss: 1.820780]\n",
      "epoch:29 step:27456 [D loss: 0.672434, acc: 57.81%] [G loss: 1.786736]\n",
      "epoch:29 step:27457 [D loss: 0.700459, acc: 50.78%] [G loss: 1.823102]\n",
      "epoch:29 step:27458 [D loss: 0.689571, acc: 58.59%] [G loss: 1.649951]\n",
      "epoch:29 step:27459 [D loss: 0.631270, acc: 61.72%] [G loss: 1.781946]\n",
      "epoch:29 step:27460 [D loss: 0.705516, acc: 53.91%] [G loss: 1.842705]\n",
      "epoch:29 step:27461 [D loss: 0.705412, acc: 53.91%] [G loss: 1.883573]\n",
      "epoch:29 step:27462 [D loss: 0.618491, acc: 67.97%] [G loss: 1.825315]\n",
      "epoch:29 step:27463 [D loss: 0.651221, acc: 65.62%] [G loss: 1.794085]\n",
      "epoch:29 step:27464 [D loss: 0.666824, acc: 54.69%] [G loss: 1.749736]\n",
      "epoch:29 step:27465 [D loss: 0.670130, acc: 60.94%] [G loss: 1.760129]\n",
      "epoch:29 step:27466 [D loss: 0.644184, acc: 62.50%] [G loss: 1.906368]\n",
      "epoch:29 step:27467 [D loss: 0.669108, acc: 60.16%] [G loss: 1.776712]\n",
      "epoch:29 step:27468 [D loss: 0.679819, acc: 59.38%] [G loss: 1.832782]\n",
      "epoch:29 step:27469 [D loss: 0.652105, acc: 61.72%] [G loss: 1.851697]\n",
      "epoch:29 step:27470 [D loss: 0.660516, acc: 58.59%] [G loss: 1.744742]\n",
      "epoch:29 step:27471 [D loss: 0.642789, acc: 63.28%] [G loss: 1.860779]\n",
      "epoch:29 step:27472 [D loss: 0.653395, acc: 60.16%] [G loss: 1.867984]\n",
      "epoch:29 step:27473 [D loss: 0.620610, acc: 64.84%] [G loss: 1.775617]\n",
      "epoch:29 step:27474 [D loss: 0.647319, acc: 64.06%] [G loss: 1.801224]\n",
      "epoch:29 step:27475 [D loss: 0.661675, acc: 60.94%] [G loss: 1.783080]\n",
      "epoch:29 step:27476 [D loss: 0.685352, acc: 53.91%] [G loss: 1.823106]\n",
      "epoch:29 step:27477 [D loss: 0.631284, acc: 64.06%] [G loss: 1.759498]\n",
      "epoch:29 step:27478 [D loss: 0.678015, acc: 53.91%] [G loss: 1.805285]\n",
      "epoch:29 step:27479 [D loss: 0.622604, acc: 64.84%] [G loss: 1.784265]\n",
      "epoch:29 step:27480 [D loss: 0.666571, acc: 58.59%] [G loss: 1.785900]\n",
      "epoch:29 step:27481 [D loss: 0.635653, acc: 64.06%] [G loss: 1.765821]\n",
      "epoch:29 step:27482 [D loss: 0.637460, acc: 62.50%] [G loss: 1.861346]\n",
      "epoch:29 step:27483 [D loss: 0.710389, acc: 51.56%] [G loss: 1.845165]\n",
      "epoch:29 step:27484 [D loss: 0.641988, acc: 62.50%] [G loss: 1.821357]\n",
      "epoch:29 step:27485 [D loss: 0.609665, acc: 69.53%] [G loss: 1.869722]\n",
      "epoch:29 step:27486 [D loss: 0.643877, acc: 64.84%] [G loss: 1.995684]\n",
      "epoch:29 step:27487 [D loss: 0.589411, acc: 70.31%] [G loss: 2.051162]\n",
      "epoch:29 step:27488 [D loss: 0.598485, acc: 69.53%] [G loss: 2.176019]\n",
      "epoch:29 step:27489 [D loss: 0.691990, acc: 57.81%] [G loss: 1.624089]\n",
      "epoch:29 step:27490 [D loss: 0.681971, acc: 58.59%] [G loss: 1.769169]\n",
      "epoch:29 step:27491 [D loss: 0.659448, acc: 60.16%] [G loss: 1.891799]\n",
      "epoch:29 step:27492 [D loss: 0.635129, acc: 63.28%] [G loss: 1.767464]\n",
      "epoch:29 step:27493 [D loss: 0.640454, acc: 60.16%] [G loss: 1.860644]\n",
      "epoch:29 step:27494 [D loss: 0.627384, acc: 60.94%] [G loss: 1.867440]\n",
      "epoch:29 step:27495 [D loss: 0.687691, acc: 50.78%] [G loss: 1.738169]\n",
      "epoch:29 step:27496 [D loss: 0.695974, acc: 54.69%] [G loss: 1.727798]\n",
      "epoch:29 step:27497 [D loss: 0.627071, acc: 63.28%] [G loss: 1.799353]\n",
      "epoch:29 step:27498 [D loss: 0.643278, acc: 57.03%] [G loss: 1.790088]\n",
      "epoch:29 step:27499 [D loss: 0.601198, acc: 70.31%] [G loss: 1.997993]\n",
      "epoch:29 step:27500 [D loss: 0.631074, acc: 66.41%] [G loss: 1.827248]\n",
      "epoch:29 step:27501 [D loss: 0.641332, acc: 62.50%] [G loss: 1.936361]\n",
      "epoch:29 step:27502 [D loss: 0.602707, acc: 66.41%] [G loss: 1.968411]\n",
      "epoch:29 step:27503 [D loss: 0.624366, acc: 64.84%] [G loss: 1.909003]\n",
      "epoch:29 step:27504 [D loss: 0.630609, acc: 67.19%] [G loss: 2.017966]\n",
      "epoch:29 step:27505 [D loss: 0.593444, acc: 71.09%] [G loss: 2.052471]\n",
      "epoch:29 step:27506 [D loss: 0.639117, acc: 60.16%] [G loss: 1.928100]\n",
      "epoch:29 step:27507 [D loss: 0.639113, acc: 59.38%] [G loss: 2.005800]\n",
      "epoch:29 step:27508 [D loss: 0.614634, acc: 66.41%] [G loss: 1.919656]\n",
      "epoch:29 step:27509 [D loss: 0.673591, acc: 57.03%] [G loss: 2.046127]\n",
      "epoch:29 step:27510 [D loss: 0.634130, acc: 61.72%] [G loss: 1.768631]\n",
      "epoch:29 step:27511 [D loss: 0.685563, acc: 59.38%] [G loss: 1.831631]\n",
      "epoch:29 step:27512 [D loss: 0.588389, acc: 73.44%] [G loss: 1.829832]\n",
      "epoch:29 step:27513 [D loss: 0.617875, acc: 64.84%] [G loss: 1.925093]\n",
      "epoch:29 step:27514 [D loss: 0.683740, acc: 57.81%] [G loss: 1.765319]\n",
      "epoch:29 step:27515 [D loss: 0.704186, acc: 49.22%] [G loss: 1.795608]\n",
      "epoch:29 step:27516 [D loss: 0.670017, acc: 60.94%] [G loss: 1.902670]\n",
      "epoch:29 step:27517 [D loss: 0.677097, acc: 60.16%] [G loss: 1.816098]\n",
      "epoch:29 step:27518 [D loss: 0.566856, acc: 67.97%] [G loss: 2.181432]\n",
      "epoch:29 step:27519 [D loss: 0.591001, acc: 69.53%] [G loss: 2.149299]\n",
      "epoch:29 step:27520 [D loss: 0.606213, acc: 72.66%] [G loss: 2.236023]\n",
      "epoch:29 step:27521 [D loss: 0.741575, acc: 47.66%] [G loss: 1.718133]\n",
      "epoch:29 step:27522 [D loss: 0.676478, acc: 57.03%] [G loss: 1.827680]\n",
      "epoch:29 step:27523 [D loss: 0.637711, acc: 64.06%] [G loss: 1.865560]\n",
      "epoch:29 step:27524 [D loss: 0.689317, acc: 58.59%] [G loss: 1.755496]\n",
      "epoch:29 step:27525 [D loss: 0.654239, acc: 61.72%] [G loss: 1.909127]\n",
      "epoch:29 step:27526 [D loss: 0.680541, acc: 54.69%] [G loss: 1.831499]\n",
      "epoch:29 step:27527 [D loss: 0.622786, acc: 68.75%] [G loss: 1.978829]\n",
      "epoch:29 step:27528 [D loss: 0.697667, acc: 53.91%] [G loss: 1.700708]\n",
      "epoch:29 step:27529 [D loss: 0.686520, acc: 60.94%] [G loss: 1.780751]\n",
      "epoch:29 step:27530 [D loss: 0.603964, acc: 69.53%] [G loss: 1.829154]\n",
      "epoch:29 step:27531 [D loss: 0.616237, acc: 63.28%] [G loss: 1.989336]\n",
      "epoch:29 step:27532 [D loss: 0.639616, acc: 62.50%] [G loss: 1.971841]\n",
      "epoch:29 step:27533 [D loss: 0.566899, acc: 70.31%] [G loss: 2.197262]\n",
      "epoch:29 step:27534 [D loss: 0.645958, acc: 64.06%] [G loss: 1.911754]\n",
      "epoch:29 step:27535 [D loss: 0.692441, acc: 57.81%] [G loss: 1.846911]\n",
      "epoch:29 step:27536 [D loss: 0.663117, acc: 60.94%] [G loss: 1.734345]\n",
      "epoch:29 step:27537 [D loss: 0.614290, acc: 71.09%] [G loss: 1.832392]\n",
      "epoch:29 step:27538 [D loss: 0.663366, acc: 65.62%] [G loss: 1.956352]\n",
      "epoch:29 step:27539 [D loss: 0.624852, acc: 62.50%] [G loss: 1.861172]\n",
      "epoch:29 step:27540 [D loss: 0.641130, acc: 60.16%] [G loss: 1.906970]\n",
      "epoch:29 step:27541 [D loss: 0.624632, acc: 68.75%] [G loss: 1.956160]\n",
      "epoch:29 step:27542 [D loss: 0.649976, acc: 62.50%] [G loss: 1.966379]\n",
      "epoch:29 step:27543 [D loss: 0.628255, acc: 64.84%] [G loss: 2.050886]\n",
      "epoch:29 step:27544 [D loss: 0.628969, acc: 60.16%] [G loss: 2.011693]\n",
      "epoch:29 step:27545 [D loss: 0.634433, acc: 64.84%] [G loss: 1.887099]\n",
      "epoch:29 step:27546 [D loss: 0.661600, acc: 57.81%] [G loss: 1.806188]\n",
      "epoch:29 step:27547 [D loss: 0.670080, acc: 60.16%] [G loss: 1.990027]\n",
      "epoch:29 step:27548 [D loss: 0.726466, acc: 52.34%] [G loss: 1.672117]\n",
      "epoch:29 step:27549 [D loss: 0.655189, acc: 61.72%] [G loss: 1.852599]\n",
      "epoch:29 step:27550 [D loss: 0.667151, acc: 57.81%] [G loss: 1.759666]\n",
      "epoch:29 step:27551 [D loss: 0.699040, acc: 59.38%] [G loss: 1.697079]\n",
      "epoch:29 step:27552 [D loss: 0.624047, acc: 63.28%] [G loss: 1.883238]\n",
      "epoch:29 step:27553 [D loss: 0.638037, acc: 68.75%] [G loss: 1.884115]\n",
      "epoch:29 step:27554 [D loss: 0.566728, acc: 70.31%] [G loss: 2.097189]\n",
      "epoch:29 step:27555 [D loss: 0.692153, acc: 56.25%] [G loss: 1.888474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27556 [D loss: 0.694960, acc: 58.59%] [G loss: 1.708143]\n",
      "epoch:29 step:27557 [D loss: 0.671913, acc: 64.06%] [G loss: 1.933252]\n",
      "epoch:29 step:27558 [D loss: 0.607793, acc: 64.06%] [G loss: 1.916514]\n",
      "epoch:29 step:27559 [D loss: 0.657526, acc: 57.03%] [G loss: 1.689690]\n",
      "epoch:29 step:27560 [D loss: 0.662808, acc: 60.94%] [G loss: 1.811866]\n",
      "epoch:29 step:27561 [D loss: 0.653935, acc: 60.94%] [G loss: 1.886207]\n",
      "epoch:29 step:27562 [D loss: 0.638624, acc: 61.72%] [G loss: 1.742035]\n",
      "epoch:29 step:27563 [D loss: 0.638014, acc: 63.28%] [G loss: 1.940146]\n",
      "epoch:29 step:27564 [D loss: 0.683142, acc: 54.69%] [G loss: 1.754909]\n",
      "epoch:29 step:27565 [D loss: 0.617646, acc: 67.19%] [G loss: 1.758259]\n",
      "epoch:29 step:27566 [D loss: 0.651936, acc: 61.72%] [G loss: 1.807390]\n",
      "epoch:29 step:27567 [D loss: 0.652844, acc: 60.94%] [G loss: 1.866590]\n",
      "epoch:29 step:27568 [D loss: 0.656422, acc: 62.50%] [G loss: 1.896402]\n",
      "epoch:29 step:27569 [D loss: 0.715814, acc: 47.66%] [G loss: 1.713464]\n",
      "epoch:29 step:27570 [D loss: 0.645810, acc: 58.59%] [G loss: 1.694995]\n",
      "epoch:29 step:27571 [D loss: 0.628293, acc: 63.28%] [G loss: 1.866629]\n",
      "epoch:29 step:27572 [D loss: 0.621316, acc: 64.84%] [G loss: 1.902806]\n",
      "epoch:29 step:27573 [D loss: 0.618822, acc: 70.31%] [G loss: 1.768643]\n",
      "epoch:29 step:27574 [D loss: 0.739706, acc: 56.25%] [G loss: 1.813787]\n",
      "epoch:29 step:27575 [D loss: 0.636029, acc: 66.41%] [G loss: 1.844241]\n",
      "epoch:29 step:27576 [D loss: 0.629853, acc: 67.19%] [G loss: 1.783472]\n",
      "epoch:29 step:27577 [D loss: 0.676219, acc: 60.94%] [G loss: 1.976176]\n",
      "epoch:29 step:27578 [D loss: 0.562140, acc: 71.09%] [G loss: 2.046512]\n",
      "epoch:29 step:27579 [D loss: 0.636907, acc: 65.62%] [G loss: 2.059936]\n",
      "epoch:29 step:27580 [D loss: 0.599532, acc: 69.53%] [G loss: 1.702128]\n",
      "epoch:29 step:27581 [D loss: 0.684250, acc: 59.38%] [G loss: 1.778776]\n",
      "epoch:29 step:27582 [D loss: 0.635964, acc: 64.06%] [G loss: 2.025334]\n",
      "epoch:29 step:27583 [D loss: 0.647635, acc: 67.19%] [G loss: 1.913967]\n",
      "epoch:29 step:27584 [D loss: 0.640514, acc: 64.84%] [G loss: 1.912563]\n",
      "epoch:29 step:27585 [D loss: 0.640941, acc: 64.06%] [G loss: 1.908861]\n",
      "epoch:29 step:27586 [D loss: 0.709487, acc: 57.81%] [G loss: 1.847588]\n",
      "epoch:29 step:27587 [D loss: 0.645583, acc: 57.81%] [G loss: 2.014033]\n",
      "epoch:29 step:27588 [D loss: 0.624390, acc: 64.84%] [G loss: 1.947556]\n",
      "epoch:29 step:27589 [D loss: 0.580542, acc: 67.97%] [G loss: 2.011127]\n",
      "epoch:29 step:27590 [D loss: 0.658971, acc: 66.41%] [G loss: 1.980302]\n",
      "epoch:29 step:27591 [D loss: 0.714290, acc: 52.34%] [G loss: 1.758114]\n",
      "epoch:29 step:27592 [D loss: 0.659773, acc: 62.50%] [G loss: 1.952674]\n",
      "epoch:29 step:27593 [D loss: 0.627500, acc: 65.62%] [G loss: 1.929751]\n",
      "epoch:29 step:27594 [D loss: 0.667664, acc: 63.28%] [G loss: 1.814205]\n",
      "epoch:29 step:27595 [D loss: 0.706339, acc: 55.47%] [G loss: 1.860304]\n",
      "epoch:29 step:27596 [D loss: 0.672050, acc: 61.72%] [G loss: 1.906040]\n",
      "epoch:29 step:27597 [D loss: 0.643961, acc: 56.25%] [G loss: 1.827484]\n",
      "epoch:29 step:27598 [D loss: 0.653502, acc: 60.16%] [G loss: 1.797896]\n",
      "epoch:29 step:27599 [D loss: 0.644760, acc: 65.62%] [G loss: 1.769938]\n",
      "epoch:29 step:27600 [D loss: 0.616858, acc: 66.41%] [G loss: 1.845813]\n",
      "epoch:29 step:27601 [D loss: 0.615220, acc: 64.84%] [G loss: 2.046950]\n",
      "epoch:29 step:27602 [D loss: 0.595034, acc: 69.53%] [G loss: 1.955903]\n",
      "epoch:29 step:27603 [D loss: 0.601605, acc: 69.53%] [G loss: 1.867517]\n",
      "epoch:29 step:27604 [D loss: 0.577778, acc: 70.31%] [G loss: 1.933031]\n",
      "epoch:29 step:27605 [D loss: 0.687671, acc: 56.25%] [G loss: 1.849646]\n",
      "epoch:29 step:27606 [D loss: 0.649522, acc: 64.06%] [G loss: 1.831994]\n",
      "epoch:29 step:27607 [D loss: 0.634741, acc: 62.50%] [G loss: 1.861726]\n",
      "epoch:29 step:27608 [D loss: 0.667321, acc: 62.50%] [G loss: 1.939571]\n",
      "epoch:29 step:27609 [D loss: 0.643238, acc: 63.28%] [G loss: 1.990024]\n",
      "epoch:29 step:27610 [D loss: 0.745531, acc: 53.12%] [G loss: 1.626472]\n",
      "epoch:29 step:27611 [D loss: 0.700603, acc: 58.59%] [G loss: 1.677454]\n",
      "epoch:29 step:27612 [D loss: 0.686057, acc: 55.47%] [G loss: 1.753437]\n",
      "epoch:29 step:27613 [D loss: 0.697224, acc: 56.25%] [G loss: 1.753729]\n",
      "epoch:29 step:27614 [D loss: 0.714856, acc: 46.88%] [G loss: 1.633229]\n",
      "epoch:29 step:27615 [D loss: 0.682511, acc: 60.16%] [G loss: 1.711699]\n",
      "epoch:29 step:27616 [D loss: 0.623196, acc: 70.31%] [G loss: 1.810821]\n",
      "epoch:29 step:27617 [D loss: 0.659917, acc: 62.50%] [G loss: 1.754650]\n",
      "epoch:29 step:27618 [D loss: 0.629532, acc: 66.41%] [G loss: 1.781702]\n",
      "epoch:29 step:27619 [D loss: 0.659049, acc: 63.28%] [G loss: 1.703019]\n",
      "epoch:29 step:27620 [D loss: 0.663410, acc: 59.38%] [G loss: 1.800219]\n",
      "epoch:29 step:27621 [D loss: 0.690006, acc: 52.34%] [G loss: 1.738593]\n",
      "epoch:29 step:27622 [D loss: 0.648781, acc: 63.28%] [G loss: 1.840689]\n",
      "epoch:29 step:27623 [D loss: 0.697851, acc: 56.25%] [G loss: 1.768417]\n",
      "epoch:29 step:27624 [D loss: 0.635306, acc: 66.41%] [G loss: 1.904616]\n",
      "epoch:29 step:27625 [D loss: 0.725712, acc: 49.22%] [G loss: 1.829861]\n",
      "epoch:29 step:27626 [D loss: 0.686041, acc: 60.94%] [G loss: 1.890878]\n",
      "epoch:29 step:27627 [D loss: 0.640239, acc: 62.50%] [G loss: 1.915374]\n",
      "epoch:29 step:27628 [D loss: 0.613523, acc: 64.84%] [G loss: 1.845180]\n",
      "epoch:29 step:27629 [D loss: 0.624683, acc: 63.28%] [G loss: 2.000295]\n",
      "epoch:29 step:27630 [D loss: 0.590682, acc: 71.09%] [G loss: 1.986532]\n",
      "epoch:29 step:27631 [D loss: 0.634999, acc: 62.50%] [G loss: 1.854547]\n",
      "epoch:29 step:27632 [D loss: 0.669935, acc: 63.28%] [G loss: 1.743199]\n",
      "epoch:29 step:27633 [D loss: 0.736255, acc: 49.22%] [G loss: 1.608957]\n",
      "epoch:29 step:27634 [D loss: 0.737609, acc: 53.12%] [G loss: 1.833271]\n",
      "epoch:29 step:27635 [D loss: 0.614127, acc: 67.19%] [G loss: 1.816045]\n",
      "epoch:29 step:27636 [D loss: 0.663610, acc: 58.59%] [G loss: 1.780088]\n",
      "epoch:29 step:27637 [D loss: 0.677489, acc: 56.25%] [G loss: 1.656121]\n",
      "epoch:29 step:27638 [D loss: 0.601589, acc: 65.62%] [G loss: 1.741461]\n",
      "epoch:29 step:27639 [D loss: 0.670788, acc: 57.81%] [G loss: 1.831825]\n",
      "epoch:29 step:27640 [D loss: 0.635632, acc: 61.72%] [G loss: 2.001903]\n",
      "epoch:29 step:27641 [D loss: 0.682210, acc: 55.47%] [G loss: 1.984018]\n",
      "epoch:29 step:27642 [D loss: 0.677552, acc: 57.81%] [G loss: 1.918388]\n",
      "epoch:29 step:27643 [D loss: 0.635646, acc: 60.16%] [G loss: 1.845323]\n",
      "epoch:29 step:27644 [D loss: 0.610082, acc: 67.97%] [G loss: 2.079378]\n",
      "epoch:29 step:27645 [D loss: 0.649401, acc: 64.84%] [G loss: 2.018520]\n",
      "epoch:29 step:27646 [D loss: 0.695145, acc: 56.25%] [G loss: 1.819075]\n",
      "epoch:29 step:27647 [D loss: 0.644823, acc: 59.38%] [G loss: 1.907011]\n",
      "epoch:29 step:27648 [D loss: 0.689763, acc: 53.12%] [G loss: 1.801091]\n",
      "epoch:29 step:27649 [D loss: 0.634474, acc: 64.06%] [G loss: 1.900262]\n",
      "epoch:29 step:27650 [D loss: 0.673190, acc: 58.59%] [G loss: 1.773273]\n",
      "epoch:29 step:27651 [D loss: 0.680216, acc: 57.03%] [G loss: 1.747073]\n",
      "epoch:29 step:27652 [D loss: 0.584657, acc: 71.09%] [G loss: 1.926303]\n",
      "epoch:29 step:27653 [D loss: 0.640735, acc: 66.41%] [G loss: 1.870931]\n",
      "epoch:29 step:27654 [D loss: 0.641052, acc: 64.84%] [G loss: 2.080193]\n",
      "epoch:29 step:27655 [D loss: 0.693763, acc: 56.25%] [G loss: 1.751152]\n",
      "epoch:29 step:27656 [D loss: 0.691918, acc: 55.47%] [G loss: 1.766747]\n",
      "epoch:29 step:27657 [D loss: 0.660438, acc: 59.38%] [G loss: 1.865392]\n",
      "epoch:29 step:27658 [D loss: 0.642349, acc: 60.94%] [G loss: 1.754664]\n",
      "epoch:29 step:27659 [D loss: 0.609149, acc: 69.53%] [G loss: 1.774538]\n",
      "epoch:29 step:27660 [D loss: 0.645859, acc: 66.41%] [G loss: 1.842435]\n",
      "epoch:29 step:27661 [D loss: 0.615604, acc: 64.06%] [G loss: 1.972132]\n",
      "epoch:29 step:27662 [D loss: 0.643209, acc: 60.94%] [G loss: 1.876306]\n",
      "epoch:29 step:27663 [D loss: 0.656181, acc: 58.59%] [G loss: 1.859500]\n",
      "epoch:29 step:27664 [D loss: 0.672375, acc: 59.38%] [G loss: 1.842441]\n",
      "epoch:29 step:27665 [D loss: 0.651803, acc: 60.94%] [G loss: 1.767773]\n",
      "epoch:29 step:27666 [D loss: 0.638065, acc: 67.19%] [G loss: 1.880686]\n",
      "epoch:29 step:27667 [D loss: 0.591050, acc: 70.31%] [G loss: 1.751079]\n",
      "epoch:29 step:27668 [D loss: 0.600568, acc: 67.97%] [G loss: 1.924698]\n",
      "epoch:29 step:27669 [D loss: 0.672117, acc: 60.94%] [G loss: 1.822212]\n",
      "epoch:29 step:27670 [D loss: 0.633201, acc: 67.97%] [G loss: 1.867746]\n",
      "epoch:29 step:27671 [D loss: 0.654648, acc: 57.03%] [G loss: 1.998489]\n",
      "epoch:29 step:27672 [D loss: 0.590875, acc: 71.88%] [G loss: 2.008790]\n",
      "epoch:29 step:27673 [D loss: 0.654398, acc: 67.19%] [G loss: 1.860132]\n",
      "epoch:29 step:27674 [D loss: 0.766876, acc: 51.56%] [G loss: 1.792693]\n",
      "epoch:29 step:27675 [D loss: 0.697787, acc: 57.81%] [G loss: 1.712486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27676 [D loss: 0.619771, acc: 64.06%] [G loss: 1.722479]\n",
      "epoch:29 step:27677 [D loss: 0.615494, acc: 71.09%] [G loss: 1.997448]\n",
      "epoch:29 step:27678 [D loss: 0.677743, acc: 60.94%] [G loss: 1.777662]\n",
      "epoch:29 step:27679 [D loss: 0.666651, acc: 58.59%] [G loss: 1.762503]\n",
      "epoch:29 step:27680 [D loss: 0.662669, acc: 60.16%] [G loss: 1.885162]\n",
      "epoch:29 step:27681 [D loss: 0.612258, acc: 65.62%] [G loss: 1.937954]\n",
      "epoch:29 step:27682 [D loss: 0.660713, acc: 67.97%] [G loss: 1.826506]\n",
      "epoch:29 step:27683 [D loss: 0.629857, acc: 61.72%] [G loss: 1.786390]\n",
      "epoch:29 step:27684 [D loss: 0.691271, acc: 61.72%] [G loss: 1.792895]\n",
      "epoch:29 step:27685 [D loss: 0.631878, acc: 61.72%] [G loss: 1.871023]\n",
      "epoch:29 step:27686 [D loss: 0.634203, acc: 64.06%] [G loss: 1.721989]\n",
      "epoch:29 step:27687 [D loss: 0.631518, acc: 63.28%] [G loss: 1.950737]\n",
      "epoch:29 step:27688 [D loss: 0.631643, acc: 59.38%] [G loss: 1.817553]\n",
      "epoch:29 step:27689 [D loss: 0.659848, acc: 61.72%] [G loss: 2.037462]\n",
      "epoch:29 step:27690 [D loss: 0.682608, acc: 57.03%] [G loss: 1.883103]\n",
      "epoch:29 step:27691 [D loss: 0.648382, acc: 61.72%] [G loss: 1.770467]\n",
      "epoch:29 step:27692 [D loss: 0.626297, acc: 60.16%] [G loss: 1.893506]\n",
      "epoch:29 step:27693 [D loss: 0.616597, acc: 71.09%] [G loss: 1.694738]\n",
      "epoch:29 step:27694 [D loss: 0.655168, acc: 64.84%] [G loss: 1.831080]\n",
      "epoch:29 step:27695 [D loss: 0.611537, acc: 64.06%] [G loss: 1.878892]\n",
      "epoch:29 step:27696 [D loss: 0.598106, acc: 67.97%] [G loss: 2.028205]\n",
      "epoch:29 step:27697 [D loss: 0.720289, acc: 57.81%] [G loss: 1.938238]\n",
      "epoch:29 step:27698 [D loss: 0.581972, acc: 74.22%] [G loss: 1.859112]\n",
      "epoch:29 step:27699 [D loss: 0.708869, acc: 53.12%] [G loss: 1.779520]\n",
      "epoch:29 step:27700 [D loss: 0.682286, acc: 56.25%] [G loss: 1.787757]\n",
      "epoch:29 step:27701 [D loss: 0.717258, acc: 53.91%] [G loss: 1.604251]\n",
      "epoch:29 step:27702 [D loss: 0.687839, acc: 55.47%] [G loss: 1.679786]\n",
      "epoch:29 step:27703 [D loss: 0.676324, acc: 58.59%] [G loss: 1.768811]\n",
      "epoch:29 step:27704 [D loss: 0.633891, acc: 64.06%] [G loss: 1.818258]\n",
      "epoch:29 step:27705 [D loss: 0.640240, acc: 66.41%] [G loss: 1.899951]\n",
      "epoch:29 step:27706 [D loss: 0.633429, acc: 62.50%] [G loss: 2.008166]\n",
      "epoch:29 step:27707 [D loss: 0.617068, acc: 64.84%] [G loss: 1.912788]\n",
      "epoch:29 step:27708 [D loss: 0.654607, acc: 63.28%] [G loss: 1.768087]\n",
      "epoch:29 step:27709 [D loss: 0.659854, acc: 60.16%] [G loss: 1.882362]\n",
      "epoch:29 step:27710 [D loss: 0.670271, acc: 59.38%] [G loss: 1.760039]\n",
      "epoch:29 step:27711 [D loss: 0.659405, acc: 60.16%] [G loss: 1.728689]\n",
      "epoch:29 step:27712 [D loss: 0.655143, acc: 55.47%] [G loss: 1.829227]\n",
      "epoch:29 step:27713 [D loss: 0.639668, acc: 60.94%] [G loss: 1.756286]\n",
      "epoch:29 step:27714 [D loss: 0.678635, acc: 57.03%] [G loss: 1.780754]\n",
      "epoch:29 step:27715 [D loss: 0.657480, acc: 61.72%] [G loss: 1.822477]\n",
      "epoch:29 step:27716 [D loss: 0.678218, acc: 60.16%] [G loss: 1.863822]\n",
      "epoch:29 step:27717 [D loss: 0.632390, acc: 61.72%] [G loss: 1.754954]\n",
      "epoch:29 step:27718 [D loss: 0.639534, acc: 60.94%] [G loss: 1.840369]\n",
      "epoch:29 step:27719 [D loss: 0.702382, acc: 56.25%] [G loss: 1.766512]\n",
      "epoch:29 step:27720 [D loss: 0.640991, acc: 64.84%] [G loss: 1.833742]\n",
      "epoch:29 step:27721 [D loss: 0.666918, acc: 57.03%] [G loss: 1.797345]\n",
      "epoch:29 step:27722 [D loss: 0.634350, acc: 68.75%] [G loss: 1.917595]\n",
      "epoch:29 step:27723 [D loss: 0.693440, acc: 56.25%] [G loss: 1.873998]\n",
      "epoch:29 step:27724 [D loss: 0.611470, acc: 65.62%] [G loss: 1.864260]\n",
      "epoch:29 step:27725 [D loss: 0.615875, acc: 60.94%] [G loss: 2.021865]\n",
      "epoch:29 step:27726 [D loss: 0.667448, acc: 57.81%] [G loss: 1.772204]\n",
      "epoch:29 step:27727 [D loss: 0.663013, acc: 64.84%] [G loss: 1.994606]\n",
      "epoch:29 step:27728 [D loss: 0.624839, acc: 64.06%] [G loss: 1.875885]\n",
      "epoch:29 step:27729 [D loss: 0.646194, acc: 59.38%] [G loss: 1.959224]\n",
      "epoch:29 step:27730 [D loss: 0.588359, acc: 63.28%] [G loss: 2.006037]\n",
      "epoch:29 step:27731 [D loss: 0.619374, acc: 65.62%] [G loss: 1.985931]\n",
      "epoch:29 step:27732 [D loss: 0.695964, acc: 55.47%] [G loss: 1.660545]\n",
      "epoch:29 step:27733 [D loss: 0.697466, acc: 57.03%] [G loss: 1.802810]\n",
      "epoch:29 step:27734 [D loss: 0.702942, acc: 53.91%] [G loss: 1.834971]\n",
      "epoch:29 step:27735 [D loss: 0.672172, acc: 63.28%] [G loss: 1.829761]\n",
      "epoch:29 step:27736 [D loss: 0.621270, acc: 67.97%] [G loss: 2.025175]\n",
      "epoch:29 step:27737 [D loss: 0.612242, acc: 64.06%] [G loss: 2.095810]\n",
      "epoch:29 step:27738 [D loss: 0.699455, acc: 60.94%] [G loss: 1.938859]\n",
      "epoch:29 step:27739 [D loss: 0.729244, acc: 55.47%] [G loss: 1.605935]\n",
      "epoch:29 step:27740 [D loss: 0.656088, acc: 61.72%] [G loss: 1.788102]\n",
      "epoch:29 step:27741 [D loss: 0.613890, acc: 64.84%] [G loss: 1.746481]\n",
      "epoch:29 step:27742 [D loss: 0.673418, acc: 55.47%] [G loss: 1.798433]\n",
      "epoch:29 step:27743 [D loss: 0.644115, acc: 64.06%] [G loss: 1.927596]\n",
      "epoch:29 step:27744 [D loss: 0.647033, acc: 59.38%] [G loss: 2.040870]\n",
      "epoch:29 step:27745 [D loss: 0.684810, acc: 58.59%] [G loss: 1.651472]\n",
      "epoch:29 step:27746 [D loss: 0.665295, acc: 57.81%] [G loss: 1.742105]\n",
      "epoch:29 step:27747 [D loss: 0.682930, acc: 59.38%] [G loss: 1.819716]\n",
      "epoch:29 step:27748 [D loss: 0.672083, acc: 57.81%] [G loss: 1.751410]\n",
      "epoch:29 step:27749 [D loss: 0.684206, acc: 53.91%] [G loss: 1.645728]\n",
      "epoch:29 step:27750 [D loss: 0.647599, acc: 61.72%] [G loss: 1.668111]\n",
      "epoch:29 step:27751 [D loss: 0.605503, acc: 75.00%] [G loss: 1.767070]\n",
      "epoch:29 step:27752 [D loss: 0.658108, acc: 60.16%] [G loss: 1.793181]\n",
      "epoch:29 step:27753 [D loss: 0.660714, acc: 59.38%] [G loss: 1.807741]\n",
      "epoch:29 step:27754 [D loss: 0.658183, acc: 63.28%] [G loss: 1.909813]\n",
      "epoch:29 step:27755 [D loss: 0.675967, acc: 53.91%] [G loss: 1.830873]\n",
      "epoch:29 step:27756 [D loss: 0.677687, acc: 61.72%] [G loss: 1.943306]\n",
      "epoch:29 step:27757 [D loss: 0.680518, acc: 57.03%] [G loss: 1.743180]\n",
      "epoch:29 step:27758 [D loss: 0.669337, acc: 61.72%] [G loss: 1.815928]\n",
      "epoch:29 step:27759 [D loss: 0.655330, acc: 56.25%] [G loss: 1.820934]\n",
      "epoch:29 step:27760 [D loss: 0.674752, acc: 57.03%] [G loss: 1.788010]\n",
      "epoch:29 step:27761 [D loss: 0.684867, acc: 56.25%] [G loss: 1.896666]\n",
      "epoch:29 step:27762 [D loss: 0.672153, acc: 56.25%] [G loss: 1.866771]\n",
      "epoch:29 step:27763 [D loss: 0.674303, acc: 54.69%] [G loss: 1.726255]\n",
      "epoch:29 step:27764 [D loss: 0.653579, acc: 58.59%] [G loss: 1.814924]\n",
      "epoch:29 step:27765 [D loss: 0.637596, acc: 60.16%] [G loss: 1.809346]\n",
      "epoch:29 step:27766 [D loss: 0.644948, acc: 65.62%] [G loss: 1.844090]\n",
      "epoch:29 step:27767 [D loss: 0.646942, acc: 65.62%] [G loss: 1.804388]\n",
      "epoch:29 step:27768 [D loss: 0.617232, acc: 67.19%] [G loss: 1.943633]\n",
      "epoch:29 step:27769 [D loss: 0.675391, acc: 60.16%] [G loss: 1.822683]\n",
      "epoch:29 step:27770 [D loss: 0.648322, acc: 60.94%] [G loss: 1.750669]\n",
      "epoch:29 step:27771 [D loss: 0.604286, acc: 69.53%] [G loss: 1.761165]\n",
      "epoch:29 step:27772 [D loss: 0.627358, acc: 62.50%] [G loss: 1.864538]\n",
      "epoch:29 step:27773 [D loss: 0.680511, acc: 57.03%] [G loss: 1.790425]\n",
      "epoch:29 step:27774 [D loss: 0.626170, acc: 66.41%] [G loss: 2.063176]\n",
      "epoch:29 step:27775 [D loss: 0.677722, acc: 57.81%] [G loss: 1.780735]\n",
      "epoch:29 step:27776 [D loss: 0.625846, acc: 68.75%] [G loss: 1.867511]\n",
      "epoch:29 step:27777 [D loss: 0.633210, acc: 57.03%] [G loss: 1.990122]\n",
      "epoch:29 step:27778 [D loss: 0.616689, acc: 67.19%] [G loss: 1.896357]\n",
      "epoch:29 step:27779 [D loss: 0.635990, acc: 64.06%] [G loss: 1.764552]\n",
      "epoch:29 step:27780 [D loss: 0.660606, acc: 59.38%] [G loss: 1.886135]\n",
      "epoch:29 step:27781 [D loss: 0.656157, acc: 59.38%] [G loss: 1.934415]\n",
      "epoch:29 step:27782 [D loss: 0.664369, acc: 59.38%] [G loss: 1.907152]\n",
      "epoch:29 step:27783 [D loss: 0.665594, acc: 60.16%] [G loss: 1.843386]\n",
      "epoch:29 step:27784 [D loss: 0.662553, acc: 58.59%] [G loss: 1.783470]\n",
      "epoch:29 step:27785 [D loss: 0.682175, acc: 60.94%] [G loss: 1.723500]\n",
      "epoch:29 step:27786 [D loss: 0.625891, acc: 63.28%] [G loss: 1.756994]\n",
      "epoch:29 step:27787 [D loss: 0.643515, acc: 63.28%] [G loss: 1.725897]\n",
      "epoch:29 step:27788 [D loss: 0.718782, acc: 49.22%] [G loss: 1.740386]\n",
      "epoch:29 step:27789 [D loss: 0.659626, acc: 57.81%] [G loss: 1.749663]\n",
      "epoch:29 step:27790 [D loss: 0.716576, acc: 46.88%] [G loss: 1.881969]\n",
      "epoch:29 step:27791 [D loss: 0.639411, acc: 63.28%] [G loss: 1.716128]\n",
      "epoch:29 step:27792 [D loss: 0.666969, acc: 60.94%] [G loss: 1.671722]\n",
      "epoch:29 step:27793 [D loss: 0.648492, acc: 64.84%] [G loss: 1.732240]\n",
      "epoch:29 step:27794 [D loss: 0.644586, acc: 58.59%] [G loss: 1.819632]\n",
      "epoch:29 step:27795 [D loss: 0.673080, acc: 60.94%] [G loss: 1.884955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27796 [D loss: 0.624827, acc: 66.41%] [G loss: 1.827255]\n",
      "epoch:29 step:27797 [D loss: 0.588632, acc: 74.22%] [G loss: 1.968207]\n",
      "epoch:29 step:27798 [D loss: 0.667510, acc: 55.47%] [G loss: 1.848832]\n",
      "epoch:29 step:27799 [D loss: 0.664269, acc: 62.50%] [G loss: 1.933651]\n",
      "epoch:29 step:27800 [D loss: 0.648175, acc: 60.94%] [G loss: 1.832149]\n",
      "epoch:29 step:27801 [D loss: 0.701065, acc: 57.03%] [G loss: 1.785232]\n",
      "epoch:29 step:27802 [D loss: 0.647296, acc: 59.38%] [G loss: 1.845552]\n",
      "epoch:29 step:27803 [D loss: 0.670272, acc: 54.69%] [G loss: 1.821207]\n",
      "epoch:29 step:27804 [D loss: 0.677173, acc: 57.03%] [G loss: 1.822681]\n",
      "epoch:29 step:27805 [D loss: 0.658550, acc: 60.94%] [G loss: 1.947776]\n",
      "epoch:29 step:27806 [D loss: 0.645080, acc: 67.97%] [G loss: 1.963593]\n",
      "epoch:29 step:27807 [D loss: 0.616299, acc: 71.88%] [G loss: 1.970921]\n",
      "epoch:29 step:27808 [D loss: 0.604912, acc: 67.97%] [G loss: 1.931005]\n",
      "epoch:29 step:27809 [D loss: 0.659173, acc: 60.94%] [G loss: 1.863110]\n",
      "epoch:29 step:27810 [D loss: 0.635952, acc: 61.72%] [G loss: 2.010801]\n",
      "epoch:29 step:27811 [D loss: 0.664549, acc: 60.94%] [G loss: 1.901550]\n",
      "epoch:29 step:27812 [D loss: 0.694922, acc: 55.47%] [G loss: 1.838807]\n",
      "epoch:29 step:27813 [D loss: 0.686248, acc: 59.38%] [G loss: 1.811997]\n",
      "epoch:29 step:27814 [D loss: 0.720877, acc: 56.25%] [G loss: 1.700737]\n",
      "epoch:29 step:27815 [D loss: 0.633972, acc: 57.81%] [G loss: 1.951068]\n",
      "epoch:29 step:27816 [D loss: 0.671081, acc: 61.72%] [G loss: 1.910257]\n",
      "epoch:29 step:27817 [D loss: 0.620309, acc: 68.75%] [G loss: 1.924077]\n",
      "epoch:29 step:27818 [D loss: 0.642087, acc: 61.72%] [G loss: 1.835164]\n",
      "epoch:29 step:27819 [D loss: 0.602052, acc: 67.97%] [G loss: 1.927277]\n",
      "epoch:29 step:27820 [D loss: 0.648008, acc: 64.06%] [G loss: 2.038846]\n",
      "epoch:29 step:27821 [D loss: 0.590462, acc: 67.19%] [G loss: 2.112524]\n",
      "epoch:29 step:27822 [D loss: 0.604081, acc: 70.31%] [G loss: 2.131310]\n",
      "epoch:29 step:27823 [D loss: 0.585684, acc: 71.09%] [G loss: 2.127248]\n",
      "epoch:29 step:27824 [D loss: 0.604175, acc: 66.41%] [G loss: 1.989811]\n",
      "epoch:29 step:27825 [D loss: 0.677061, acc: 58.59%] [G loss: 2.024614]\n",
      "epoch:29 step:27826 [D loss: 0.614145, acc: 68.75%] [G loss: 1.957283]\n",
      "epoch:29 step:27827 [D loss: 0.622601, acc: 65.62%] [G loss: 1.988916]\n",
      "epoch:29 step:27828 [D loss: 0.692282, acc: 56.25%] [G loss: 1.814638]\n",
      "epoch:29 step:27829 [D loss: 0.679684, acc: 56.25%] [G loss: 1.818293]\n",
      "epoch:29 step:27830 [D loss: 0.676306, acc: 60.94%] [G loss: 1.773074]\n",
      "epoch:29 step:27831 [D loss: 0.662036, acc: 62.50%] [G loss: 1.739915]\n",
      "epoch:29 step:27832 [D loss: 0.708662, acc: 51.56%] [G loss: 1.683669]\n",
      "epoch:29 step:27833 [D loss: 0.635512, acc: 66.41%] [G loss: 1.745555]\n",
      "epoch:29 step:27834 [D loss: 0.653360, acc: 67.19%] [G loss: 1.869690]\n",
      "epoch:29 step:27835 [D loss: 0.658094, acc: 63.28%] [G loss: 1.864120]\n",
      "epoch:29 step:27836 [D loss: 0.636996, acc: 60.16%] [G loss: 1.733343]\n",
      "epoch:29 step:27837 [D loss: 0.665382, acc: 57.03%] [G loss: 1.843895]\n",
      "epoch:29 step:27838 [D loss: 0.578820, acc: 71.88%] [G loss: 1.901080]\n",
      "epoch:29 step:27839 [D loss: 0.649404, acc: 64.84%] [G loss: 1.746769]\n",
      "epoch:29 step:27840 [D loss: 0.641934, acc: 60.94%] [G loss: 1.849418]\n",
      "epoch:29 step:27841 [D loss: 0.639055, acc: 62.50%] [G loss: 1.908763]\n",
      "epoch:29 step:27842 [D loss: 0.641558, acc: 64.84%] [G loss: 1.874307]\n",
      "epoch:29 step:27843 [D loss: 0.629984, acc: 65.62%] [G loss: 1.726407]\n",
      "epoch:29 step:27844 [D loss: 0.655406, acc: 63.28%] [G loss: 1.786420]\n",
      "epoch:29 step:27845 [D loss: 0.639848, acc: 68.75%] [G loss: 1.733982]\n",
      "epoch:29 step:27846 [D loss: 0.701480, acc: 55.47%] [G loss: 1.866457]\n",
      "epoch:29 step:27847 [D loss: 0.664271, acc: 58.59%] [G loss: 1.813895]\n",
      "epoch:29 step:27848 [D loss: 0.651606, acc: 59.38%] [G loss: 1.854450]\n",
      "epoch:29 step:27849 [D loss: 0.627516, acc: 65.62%] [G loss: 1.871569]\n",
      "epoch:29 step:27850 [D loss: 0.626407, acc: 61.72%] [G loss: 1.899149]\n",
      "epoch:29 step:27851 [D loss: 0.667752, acc: 60.16%] [G loss: 1.949190]\n",
      "epoch:29 step:27852 [D loss: 0.614061, acc: 67.19%] [G loss: 1.882032]\n",
      "epoch:29 step:27853 [D loss: 0.676477, acc: 57.81%] [G loss: 1.829629]\n",
      "epoch:29 step:27854 [D loss: 0.604023, acc: 64.06%] [G loss: 1.932984]\n",
      "epoch:29 step:27855 [D loss: 0.685232, acc: 56.25%] [G loss: 1.749207]\n",
      "epoch:29 step:27856 [D loss: 0.688264, acc: 57.03%] [G loss: 1.797082]\n",
      "epoch:29 step:27857 [D loss: 0.680216, acc: 60.94%] [G loss: 1.782539]\n",
      "epoch:29 step:27858 [D loss: 0.629218, acc: 67.19%] [G loss: 1.830126]\n",
      "epoch:29 step:27859 [D loss: 0.648845, acc: 67.97%] [G loss: 1.779927]\n",
      "epoch:29 step:27860 [D loss: 0.667067, acc: 57.81%] [G loss: 1.776901]\n",
      "epoch:29 step:27861 [D loss: 0.633583, acc: 67.19%] [G loss: 1.942484]\n",
      "epoch:29 step:27862 [D loss: 0.659161, acc: 60.16%] [G loss: 1.923994]\n",
      "epoch:29 step:27863 [D loss: 0.626299, acc: 63.28%] [G loss: 1.954248]\n",
      "epoch:29 step:27864 [D loss: 0.650107, acc: 61.72%] [G loss: 1.892300]\n",
      "epoch:29 step:27865 [D loss: 0.637594, acc: 60.94%] [G loss: 1.992855]\n",
      "epoch:29 step:27866 [D loss: 0.623512, acc: 65.62%] [G loss: 2.037658]\n",
      "epoch:29 step:27867 [D loss: 0.602103, acc: 69.53%] [G loss: 2.163036]\n",
      "epoch:29 step:27868 [D loss: 0.647727, acc: 62.50%] [G loss: 2.055805]\n",
      "epoch:29 step:27869 [D loss: 0.704052, acc: 50.00%] [G loss: 1.774982]\n",
      "epoch:29 step:27870 [D loss: 0.669943, acc: 56.25%] [G loss: 1.828794]\n",
      "epoch:29 step:27871 [D loss: 0.664680, acc: 56.25%] [G loss: 1.810729]\n",
      "epoch:29 step:27872 [D loss: 0.612603, acc: 65.62%] [G loss: 1.885715]\n",
      "epoch:29 step:27873 [D loss: 0.608586, acc: 67.19%] [G loss: 1.957458]\n",
      "epoch:29 step:27874 [D loss: 0.654083, acc: 59.38%] [G loss: 1.838568]\n",
      "epoch:29 step:27875 [D loss: 0.680352, acc: 57.81%] [G loss: 1.844526]\n",
      "epoch:29 step:27876 [D loss: 0.651157, acc: 54.69%] [G loss: 1.809780]\n",
      "epoch:29 step:27877 [D loss: 0.737114, acc: 54.69%] [G loss: 1.829652]\n",
      "epoch:29 step:27878 [D loss: 0.642539, acc: 60.94%] [G loss: 1.725712]\n",
      "epoch:29 step:27879 [D loss: 0.625363, acc: 63.28%] [G loss: 1.759100]\n",
      "epoch:29 step:27880 [D loss: 0.591635, acc: 72.66%] [G loss: 1.899930]\n",
      "epoch:29 step:27881 [D loss: 0.644122, acc: 61.72%] [G loss: 1.933224]\n",
      "epoch:29 step:27882 [D loss: 0.656418, acc: 59.38%] [G loss: 1.918546]\n",
      "epoch:29 step:27883 [D loss: 0.698941, acc: 56.25%] [G loss: 1.806699]\n",
      "epoch:29 step:27884 [D loss: 0.619916, acc: 66.41%] [G loss: 1.882256]\n",
      "epoch:29 step:27885 [D loss: 0.609625, acc: 70.31%] [G loss: 2.000519]\n",
      "epoch:29 step:27886 [D loss: 0.644065, acc: 64.84%] [G loss: 1.953925]\n",
      "epoch:29 step:27887 [D loss: 0.633322, acc: 62.50%] [G loss: 2.007592]\n",
      "epoch:29 step:27888 [D loss: 0.661154, acc: 60.94%] [G loss: 1.747799]\n",
      "epoch:29 step:27889 [D loss: 0.682025, acc: 60.16%] [G loss: 1.761540]\n",
      "epoch:29 step:27890 [D loss: 0.619788, acc: 62.50%] [G loss: 1.824638]\n",
      "epoch:29 step:27891 [D loss: 0.617316, acc: 63.28%] [G loss: 1.955340]\n",
      "epoch:29 step:27892 [D loss: 0.644582, acc: 67.19%] [G loss: 1.923053]\n",
      "epoch:29 step:27893 [D loss: 0.633482, acc: 59.38%] [G loss: 1.914226]\n",
      "epoch:29 step:27894 [D loss: 0.650702, acc: 65.62%] [G loss: 1.835671]\n",
      "epoch:29 step:27895 [D loss: 0.650229, acc: 60.94%] [G loss: 1.876031]\n",
      "epoch:29 step:27896 [D loss: 0.659652, acc: 53.91%] [G loss: 1.846009]\n",
      "epoch:29 step:27897 [D loss: 0.609753, acc: 64.06%] [G loss: 1.912012]\n",
      "epoch:29 step:27898 [D loss: 0.648909, acc: 64.06%] [G loss: 1.854654]\n",
      "epoch:29 step:27899 [D loss: 0.653174, acc: 60.94%] [G loss: 1.937925]\n",
      "epoch:29 step:27900 [D loss: 0.653504, acc: 64.06%] [G loss: 1.942910]\n",
      "epoch:29 step:27901 [D loss: 0.601932, acc: 67.97%] [G loss: 1.846884]\n",
      "epoch:29 step:27902 [D loss: 0.730322, acc: 53.12%] [G loss: 1.776249]\n",
      "epoch:29 step:27903 [D loss: 0.628377, acc: 67.97%] [G loss: 1.817528]\n",
      "epoch:29 step:27904 [D loss: 0.709053, acc: 51.56%] [G loss: 1.772154]\n",
      "epoch:29 step:27905 [D loss: 0.655558, acc: 60.16%] [G loss: 1.752472]\n",
      "epoch:29 step:27906 [D loss: 0.616075, acc: 66.41%] [G loss: 1.942842]\n",
      "epoch:29 step:27907 [D loss: 0.687593, acc: 58.59%] [G loss: 1.709729]\n",
      "epoch:29 step:27908 [D loss: 0.733564, acc: 48.44%] [G loss: 1.746621]\n",
      "epoch:29 step:27909 [D loss: 0.643958, acc: 63.28%] [G loss: 1.803618]\n",
      "epoch:29 step:27910 [D loss: 0.674665, acc: 64.06%] [G loss: 1.898964]\n",
      "epoch:29 step:27911 [D loss: 0.617991, acc: 64.06%] [G loss: 1.783645]\n",
      "epoch:29 step:27912 [D loss: 0.670713, acc: 63.28%] [G loss: 1.708511]\n",
      "epoch:29 step:27913 [D loss: 0.656230, acc: 61.72%] [G loss: 1.883453]\n",
      "epoch:29 step:27914 [D loss: 0.686656, acc: 63.28%] [G loss: 1.740247]\n",
      "epoch:29 step:27915 [D loss: 0.616742, acc: 68.75%] [G loss: 1.782300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:27916 [D loss: 0.727275, acc: 57.03%] [G loss: 1.823751]\n",
      "epoch:29 step:27917 [D loss: 0.685455, acc: 63.28%] [G loss: 1.774953]\n",
      "epoch:29 step:27918 [D loss: 0.670913, acc: 58.59%] [G loss: 1.848057]\n",
      "epoch:29 step:27919 [D loss: 0.636473, acc: 67.97%] [G loss: 1.904102]\n",
      "epoch:29 step:27920 [D loss: 0.640877, acc: 64.84%] [G loss: 1.806445]\n",
      "epoch:29 step:27921 [D loss: 0.620824, acc: 60.16%] [G loss: 1.711304]\n",
      "epoch:29 step:27922 [D loss: 0.637769, acc: 64.06%] [G loss: 1.958315]\n",
      "epoch:29 step:27923 [D loss: 0.641250, acc: 64.84%] [G loss: 1.814584]\n",
      "epoch:29 step:27924 [D loss: 0.658369, acc: 56.25%] [G loss: 1.824431]\n",
      "epoch:29 step:27925 [D loss: 0.694767, acc: 55.47%] [G loss: 1.711496]\n",
      "epoch:29 step:27926 [D loss: 0.695235, acc: 54.69%] [G loss: 1.668809]\n",
      "epoch:29 step:27927 [D loss: 0.624093, acc: 64.06%] [G loss: 1.795110]\n",
      "epoch:29 step:27928 [D loss: 0.621550, acc: 71.09%] [G loss: 1.870742]\n",
      "epoch:29 step:27929 [D loss: 0.686892, acc: 53.12%] [G loss: 1.801232]\n",
      "epoch:29 step:27930 [D loss: 0.647505, acc: 65.62%] [G loss: 1.873521]\n",
      "epoch:29 step:27931 [D loss: 0.654581, acc: 61.72%] [G loss: 1.801857]\n",
      "epoch:29 step:27932 [D loss: 0.681529, acc: 52.34%] [G loss: 1.772830]\n",
      "epoch:29 step:27933 [D loss: 0.674890, acc: 55.47%] [G loss: 1.692917]\n",
      "epoch:29 step:27934 [D loss: 0.626970, acc: 60.94%] [G loss: 1.806176]\n",
      "epoch:29 step:27935 [D loss: 0.666382, acc: 57.03%] [G loss: 1.807564]\n",
      "epoch:29 step:27936 [D loss: 0.659680, acc: 61.72%] [G loss: 1.802997]\n",
      "epoch:29 step:27937 [D loss: 0.699422, acc: 51.56%] [G loss: 1.804614]\n",
      "epoch:29 step:27938 [D loss: 0.667607, acc: 59.38%] [G loss: 1.700466]\n",
      "epoch:29 step:27939 [D loss: 0.695991, acc: 58.59%] [G loss: 1.764014]\n",
      "epoch:29 step:27940 [D loss: 0.676151, acc: 55.47%] [G loss: 1.727709]\n",
      "epoch:29 step:27941 [D loss: 0.681613, acc: 60.16%] [G loss: 1.832927]\n",
      "epoch:29 step:27942 [D loss: 0.631401, acc: 67.19%] [G loss: 1.776198]\n",
      "epoch:29 step:27943 [D loss: 0.672888, acc: 57.03%] [G loss: 1.770492]\n",
      "epoch:29 step:27944 [D loss: 0.702809, acc: 53.91%] [G loss: 1.765374]\n",
      "epoch:29 step:27945 [D loss: 0.620436, acc: 64.06%] [G loss: 1.815438]\n",
      "epoch:29 step:27946 [D loss: 0.654026, acc: 54.69%] [G loss: 1.817646]\n",
      "epoch:29 step:27947 [D loss: 0.629425, acc: 70.31%] [G loss: 2.001768]\n",
      "epoch:29 step:27948 [D loss: 0.607160, acc: 67.97%] [G loss: 2.051621]\n",
      "epoch:29 step:27949 [D loss: 0.684095, acc: 58.59%] [G loss: 1.980788]\n",
      "epoch:29 step:27950 [D loss: 0.635658, acc: 64.84%] [G loss: 1.966935]\n",
      "epoch:29 step:27951 [D loss: 0.625131, acc: 67.19%] [G loss: 1.912776]\n",
      "epoch:29 step:27952 [D loss: 0.640495, acc: 61.72%] [G loss: 1.817480]\n",
      "epoch:29 step:27953 [D loss: 0.648166, acc: 59.38%] [G loss: 1.915037]\n",
      "epoch:29 step:27954 [D loss: 0.575442, acc: 70.31%] [G loss: 2.038274]\n",
      "epoch:29 step:27955 [D loss: 0.593412, acc: 71.88%] [G loss: 2.069799]\n",
      "epoch:29 step:27956 [D loss: 0.676536, acc: 52.34%] [G loss: 1.796564]\n",
      "epoch:29 step:27957 [D loss: 0.703511, acc: 54.69%] [G loss: 1.764576]\n",
      "epoch:29 step:27958 [D loss: 0.621037, acc: 64.84%] [G loss: 1.829961]\n",
      "epoch:29 step:27959 [D loss: 0.655581, acc: 67.19%] [G loss: 1.848407]\n",
      "epoch:29 step:27960 [D loss: 0.643658, acc: 57.81%] [G loss: 1.871187]\n",
      "epoch:29 step:27961 [D loss: 0.647811, acc: 64.84%] [G loss: 1.711665]\n",
      "epoch:29 step:27962 [D loss: 0.646467, acc: 63.28%] [G loss: 1.911051]\n",
      "epoch:29 step:27963 [D loss: 0.610335, acc: 67.19%] [G loss: 1.925189]\n",
      "epoch:29 step:27964 [D loss: 0.615611, acc: 67.19%] [G loss: 1.829041]\n",
      "epoch:29 step:27965 [D loss: 0.630910, acc: 60.94%] [G loss: 1.911284]\n",
      "epoch:29 step:27966 [D loss: 0.626388, acc: 62.50%] [G loss: 1.908772]\n",
      "epoch:29 step:27967 [D loss: 0.664289, acc: 57.81%] [G loss: 1.671213]\n",
      "epoch:29 step:27968 [D loss: 0.776017, acc: 46.88%] [G loss: 1.789012]\n",
      "epoch:29 step:27969 [D loss: 0.651401, acc: 62.50%] [G loss: 1.718554]\n",
      "epoch:29 step:27970 [D loss: 0.685997, acc: 56.25%] [G loss: 1.785316]\n",
      "epoch:29 step:27971 [D loss: 0.658789, acc: 60.16%] [G loss: 1.825935]\n",
      "epoch:29 step:27972 [D loss: 0.643279, acc: 61.72%] [G loss: 1.835495]\n",
      "epoch:29 step:27973 [D loss: 0.740228, acc: 53.12%] [G loss: 1.749765]\n",
      "epoch:29 step:27974 [D loss: 0.677744, acc: 60.94%] [G loss: 1.745392]\n",
      "epoch:29 step:27975 [D loss: 0.643314, acc: 61.72%] [G loss: 1.896719]\n",
      "epoch:29 step:27976 [D loss: 0.657583, acc: 64.06%] [G loss: 1.796635]\n",
      "epoch:29 step:27977 [D loss: 0.631808, acc: 64.06%] [G loss: 1.852587]\n",
      "epoch:29 step:27978 [D loss: 0.630213, acc: 59.38%] [G loss: 1.842944]\n",
      "epoch:29 step:27979 [D loss: 0.658235, acc: 57.81%] [G loss: 1.799088]\n",
      "epoch:29 step:27980 [D loss: 0.641022, acc: 62.50%] [G loss: 1.878112]\n",
      "epoch:29 step:27981 [D loss: 0.642303, acc: 62.50%] [G loss: 1.716204]\n",
      "epoch:29 step:27982 [D loss: 0.625882, acc: 63.28%] [G loss: 1.742787]\n",
      "epoch:29 step:27983 [D loss: 0.663879, acc: 57.03%] [G loss: 1.729266]\n",
      "epoch:29 step:27984 [D loss: 0.640205, acc: 66.41%] [G loss: 1.880155]\n",
      "epoch:29 step:27985 [D loss: 0.666853, acc: 57.81%] [G loss: 1.791829]\n",
      "epoch:29 step:27986 [D loss: 0.668790, acc: 59.38%] [G loss: 1.822377]\n",
      "epoch:29 step:27987 [D loss: 0.656454, acc: 61.72%] [G loss: 1.933278]\n",
      "epoch:29 step:27988 [D loss: 0.594453, acc: 65.62%] [G loss: 2.000608]\n",
      "epoch:29 step:27989 [D loss: 0.657495, acc: 63.28%] [G loss: 1.899381]\n",
      "epoch:29 step:27990 [D loss: 0.647721, acc: 61.72%] [G loss: 1.884683]\n",
      "epoch:29 step:27991 [D loss: 0.637026, acc: 65.62%] [G loss: 1.771702]\n",
      "epoch:29 step:27992 [D loss: 0.606433, acc: 64.84%] [G loss: 1.888140]\n",
      "epoch:29 step:27993 [D loss: 0.754691, acc: 50.00%] [G loss: 1.738018]\n",
      "epoch:29 step:27994 [D loss: 0.682800, acc: 56.25%] [G loss: 1.852943]\n",
      "epoch:29 step:27995 [D loss: 0.629332, acc: 65.62%] [G loss: 1.901311]\n",
      "epoch:29 step:27996 [D loss: 0.630294, acc: 62.50%] [G loss: 1.965000]\n",
      "epoch:29 step:27997 [D loss: 0.644575, acc: 56.25%] [G loss: 1.744518]\n",
      "epoch:29 step:27998 [D loss: 0.613290, acc: 66.41%] [G loss: 1.902147]\n",
      "epoch:29 step:27999 [D loss: 0.650020, acc: 61.72%] [G loss: 1.868159]\n",
      "epoch:29 step:28000 [D loss: 0.687364, acc: 50.00%] [G loss: 1.828175]\n",
      "epoch:29 step:28001 [D loss: 0.686243, acc: 57.81%] [G loss: 1.780895]\n",
      "epoch:29 step:28002 [D loss: 0.693172, acc: 52.34%] [G loss: 1.710053]\n",
      "epoch:29 step:28003 [D loss: 0.687335, acc: 57.03%] [G loss: 1.683449]\n",
      "epoch:29 step:28004 [D loss: 0.615456, acc: 65.62%] [G loss: 1.907815]\n",
      "epoch:29 step:28005 [D loss: 0.625264, acc: 66.41%] [G loss: 1.924501]\n",
      "epoch:29 step:28006 [D loss: 0.615464, acc: 66.41%] [G loss: 1.895321]\n",
      "epoch:29 step:28007 [D loss: 0.673155, acc: 60.94%] [G loss: 1.833006]\n",
      "epoch:29 step:28008 [D loss: 0.669826, acc: 53.91%] [G loss: 1.756364]\n",
      "epoch:29 step:28009 [D loss: 0.644216, acc: 64.84%] [G loss: 1.776952]\n",
      "epoch:29 step:28010 [D loss: 0.610267, acc: 67.19%] [G loss: 1.840583]\n",
      "epoch:29 step:28011 [D loss: 0.627534, acc: 63.28%] [G loss: 1.969754]\n",
      "epoch:29 step:28012 [D loss: 0.601336, acc: 66.41%] [G loss: 1.745699]\n",
      "epoch:29 step:28013 [D loss: 0.634117, acc: 59.38%] [G loss: 1.856189]\n",
      "epoch:29 step:28014 [D loss: 0.636235, acc: 61.72%] [G loss: 2.057371]\n",
      "epoch:29 step:28015 [D loss: 0.579083, acc: 72.66%] [G loss: 1.979242]\n",
      "epoch:29 step:28016 [D loss: 0.648823, acc: 60.16%] [G loss: 2.030642]\n",
      "epoch:29 step:28017 [D loss: 0.672261, acc: 59.38%] [G loss: 1.955389]\n",
      "epoch:29 step:28018 [D loss: 0.614776, acc: 64.06%] [G loss: 2.006275]\n",
      "epoch:29 step:28019 [D loss: 0.653428, acc: 60.94%] [G loss: 1.759550]\n",
      "epoch:29 step:28020 [D loss: 0.607754, acc: 66.41%] [G loss: 1.952055]\n",
      "epoch:29 step:28021 [D loss: 0.686720, acc: 59.38%] [G loss: 1.819083]\n",
      "epoch:29 step:28022 [D loss: 0.692503, acc: 53.91%] [G loss: 1.970710]\n",
      "epoch:29 step:28023 [D loss: 0.695285, acc: 53.12%] [G loss: 1.736087]\n",
      "epoch:29 step:28024 [D loss: 0.680734, acc: 58.59%] [G loss: 1.751426]\n",
      "epoch:29 step:28025 [D loss: 0.662181, acc: 57.03%] [G loss: 1.759856]\n",
      "epoch:29 step:28026 [D loss: 0.694851, acc: 57.03%] [G loss: 1.834204]\n",
      "epoch:29 step:28027 [D loss: 0.677143, acc: 58.59%] [G loss: 1.755420]\n",
      "epoch:29 step:28028 [D loss: 0.682648, acc: 57.03%] [G loss: 1.781931]\n",
      "epoch:29 step:28029 [D loss: 0.651554, acc: 55.47%] [G loss: 1.764557]\n",
      "epoch:29 step:28030 [D loss: 0.623366, acc: 67.97%] [G loss: 1.978059]\n",
      "epoch:29 step:28031 [D loss: 0.682161, acc: 55.47%] [G loss: 1.823741]\n",
      "epoch:29 step:28032 [D loss: 0.705926, acc: 52.34%] [G loss: 1.850835]\n",
      "epoch:29 step:28033 [D loss: 0.612196, acc: 67.19%] [G loss: 1.895572]\n",
      "epoch:29 step:28034 [D loss: 0.672352, acc: 57.03%] [G loss: 1.809738]\n",
      "epoch:29 step:28035 [D loss: 0.663736, acc: 55.47%] [G loss: 1.803846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:28036 [D loss: 0.638601, acc: 64.06%] [G loss: 1.842610]\n",
      "epoch:29 step:28037 [D loss: 0.633748, acc: 66.41%] [G loss: 1.825848]\n",
      "epoch:29 step:28038 [D loss: 0.718367, acc: 52.34%] [G loss: 1.708552]\n",
      "epoch:29 step:28039 [D loss: 0.632012, acc: 64.84%] [G loss: 1.789893]\n",
      "epoch:29 step:28040 [D loss: 0.647827, acc: 64.84%] [G loss: 1.829101]\n",
      "epoch:29 step:28041 [D loss: 0.693077, acc: 54.69%] [G loss: 1.795893]\n",
      "epoch:29 step:28042 [D loss: 0.658061, acc: 60.94%] [G loss: 1.799752]\n",
      "epoch:29 step:28043 [D loss: 0.662117, acc: 53.91%] [G loss: 1.891876]\n",
      "epoch:29 step:28044 [D loss: 0.708735, acc: 47.66%] [G loss: 1.846421]\n",
      "epoch:29 step:28045 [D loss: 0.687001, acc: 56.25%] [G loss: 1.745193]\n",
      "epoch:29 step:28046 [D loss: 0.651286, acc: 62.50%] [G loss: 1.802354]\n",
      "epoch:29 step:28047 [D loss: 0.667607, acc: 58.59%] [G loss: 1.789759]\n",
      "epoch:29 step:28048 [D loss: 0.596862, acc: 67.97%] [G loss: 1.844720]\n",
      "epoch:29 step:28049 [D loss: 0.643440, acc: 64.84%] [G loss: 1.847373]\n",
      "epoch:29 step:28050 [D loss: 0.627336, acc: 63.28%] [G loss: 1.924646]\n",
      "epoch:29 step:28051 [D loss: 0.623682, acc: 62.50%] [G loss: 1.895950]\n",
      "epoch:29 step:28052 [D loss: 0.595648, acc: 70.31%] [G loss: 1.909149]\n",
      "epoch:29 step:28053 [D loss: 0.710570, acc: 60.94%] [G loss: 1.914349]\n",
      "epoch:29 step:28054 [D loss: 0.659899, acc: 60.94%] [G loss: 1.826833]\n",
      "epoch:29 step:28055 [D loss: 0.614268, acc: 67.19%] [G loss: 2.092257]\n",
      "epoch:29 step:28056 [D loss: 0.683538, acc: 63.28%] [G loss: 1.817318]\n",
      "epoch:29 step:28057 [D loss: 0.689155, acc: 59.38%] [G loss: 2.018199]\n",
      "epoch:29 step:28058 [D loss: 0.646602, acc: 61.72%] [G loss: 1.882809]\n",
      "epoch:29 step:28059 [D loss: 0.618011, acc: 67.19%] [G loss: 1.956709]\n",
      "epoch:29 step:28060 [D loss: 0.632717, acc: 64.06%] [G loss: 1.868239]\n",
      "epoch:29 step:28061 [D loss: 0.640008, acc: 66.41%] [G loss: 1.766007]\n",
      "epoch:29 step:28062 [D loss: 0.631830, acc: 68.75%] [G loss: 1.853413]\n",
      "epoch:29 step:28063 [D loss: 0.644045, acc: 67.19%] [G loss: 2.012375]\n",
      "epoch:29 step:28064 [D loss: 0.675188, acc: 59.38%] [G loss: 1.839320]\n",
      "epoch:29 step:28065 [D loss: 0.681757, acc: 60.94%] [G loss: 1.993439]\n",
      "epoch:29 step:28066 [D loss: 0.617932, acc: 66.41%] [G loss: 1.856805]\n",
      "epoch:29 step:28067 [D loss: 0.666995, acc: 61.72%] [G loss: 1.912720]\n",
      "epoch:29 step:28068 [D loss: 0.648559, acc: 65.62%] [G loss: 1.877381]\n",
      "epoch:29 step:28069 [D loss: 0.654608, acc: 60.16%] [G loss: 1.883624]\n",
      "epoch:29 step:28070 [D loss: 0.596614, acc: 69.53%] [G loss: 1.870860]\n",
      "epoch:29 step:28071 [D loss: 0.661172, acc: 58.59%] [G loss: 1.996897]\n",
      "epoch:29 step:28072 [D loss: 0.634951, acc: 64.06%] [G loss: 1.998488]\n",
      "epoch:29 step:28073 [D loss: 0.619559, acc: 64.84%] [G loss: 1.983225]\n",
      "epoch:29 step:28074 [D loss: 0.642185, acc: 64.84%] [G loss: 2.074513]\n",
      "epoch:29 step:28075 [D loss: 0.625575, acc: 66.41%] [G loss: 1.891542]\n",
      "epoch:29 step:28076 [D loss: 0.624856, acc: 62.50%] [G loss: 1.978873]\n",
      "epoch:29 step:28077 [D loss: 0.648838, acc: 67.97%] [G loss: 1.875242]\n",
      "epoch:29 step:28078 [D loss: 0.675481, acc: 58.59%] [G loss: 2.062882]\n",
      "epoch:29 step:28079 [D loss: 0.624518, acc: 68.75%] [G loss: 2.025238]\n",
      "epoch:29 step:28080 [D loss: 0.635814, acc: 61.72%] [G loss: 1.947283]\n",
      "epoch:29 step:28081 [D loss: 0.670299, acc: 61.72%] [G loss: 1.936950]\n",
      "epoch:29 step:28082 [D loss: 0.602431, acc: 72.66%] [G loss: 1.928207]\n",
      "epoch:29 step:28083 [D loss: 0.661878, acc: 58.59%] [G loss: 1.830714]\n",
      "epoch:29 step:28084 [D loss: 0.656983, acc: 59.38%] [G loss: 2.008276]\n",
      "epoch:29 step:28085 [D loss: 0.652439, acc: 57.81%] [G loss: 1.969834]\n",
      "epoch:29 step:28086 [D loss: 0.694547, acc: 54.69%] [G loss: 1.792407]\n",
      "epoch:29 step:28087 [D loss: 0.670210, acc: 63.28%] [G loss: 1.910012]\n",
      "epoch:29 step:28088 [D loss: 0.674356, acc: 53.12%] [G loss: 1.734682]\n",
      "epoch:29 step:28089 [D loss: 0.633277, acc: 69.53%] [G loss: 1.864189]\n",
      "epoch:29 step:28090 [D loss: 0.655614, acc: 68.75%] [G loss: 1.839766]\n",
      "epoch:29 step:28091 [D loss: 0.593860, acc: 72.66%] [G loss: 2.032429]\n",
      "epoch:29 step:28092 [D loss: 0.592387, acc: 67.19%] [G loss: 2.073549]\n",
      "epoch:29 step:28093 [D loss: 0.780035, acc: 44.53%] [G loss: 1.880622]\n",
      "epoch:29 step:28094 [D loss: 0.673340, acc: 56.25%] [G loss: 1.922549]\n",
      "epoch:29 step:28095 [D loss: 0.625536, acc: 65.62%] [G loss: 1.836295]\n",
      "epoch:29 step:28096 [D loss: 0.593326, acc: 68.75%] [G loss: 2.185515]\n",
      "epoch:29 step:28097 [D loss: 0.598544, acc: 67.19%] [G loss: 2.082908]\n",
      "epoch:29 step:28098 [D loss: 0.614169, acc: 67.19%] [G loss: 2.118991]\n",
      "epoch:29 step:28099 [D loss: 0.598309, acc: 68.75%] [G loss: 2.076704]\n",
      "epoch:29 step:28100 [D loss: 0.654893, acc: 57.81%] [G loss: 2.101525]\n",
      "epoch:29 step:28101 [D loss: 0.688238, acc: 56.25%] [G loss: 1.783274]\n",
      "epoch:29 step:28102 [D loss: 0.701623, acc: 60.94%] [G loss: 1.880811]\n",
      "epoch:29 step:28103 [D loss: 0.579194, acc: 70.31%] [G loss: 2.042421]\n",
      "epoch:29 step:28104 [D loss: 0.637954, acc: 65.62%] [G loss: 1.937249]\n",
      "epoch:29 step:28105 [D loss: 0.666317, acc: 61.72%] [G loss: 1.839979]\n",
      "epoch:29 step:28106 [D loss: 0.652962, acc: 64.84%] [G loss: 1.898673]\n",
      "epoch:29 step:28107 [D loss: 0.697523, acc: 60.94%] [G loss: 1.814815]\n",
      "epoch:29 step:28108 [D loss: 0.644833, acc: 67.97%] [G loss: 2.004679]\n",
      "epoch:29 step:28109 [D loss: 0.578034, acc: 71.09%] [G loss: 2.135943]\n",
      "epoch:29 step:28110 [D loss: 0.635643, acc: 64.06%] [G loss: 2.277113]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist,fashion_mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D, concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class BIGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Build the encoder\n",
    "        self.encoder = self.build_encoder()\n",
    "\n",
    "        # The part of the bigan that trains the discriminator and encoder\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Generate image from sampled noise\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img_ = self.generator(z)\n",
    "\n",
    "        # Encode image\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z_ = self.encoder(img)\n",
    "\n",
    "        # Latent -> img is fake, and img -> latent is valid\n",
    "        fake = self.discriminator([z, img_])\n",
    "        valid = self.discriminator([z_, img])\n",
    "\n",
    "        # Set up and compile the combined model\n",
    "        # Trains generator to fool the discriminator\n",
    "        self.bigan_generator = Model([z, img], [fake, valid])\n",
    "        self.bigan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_encoder(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(self.latent_dim))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        z = model(img)\n",
    "\n",
    "        return Model(img, z)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(512, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        gen_img = model(z)\n",
    "\n",
    "        return Model(z, gen_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        z = Input(shape=(self.latent_dim, ))\n",
    "        img = Input(shape=self.img_shape)\n",
    "        d_in = concatenate([z, Flatten()(img)])\n",
    "\n",
    "        model = Dense(1024)(d_in)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha=0.2)(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        validity = Dense(1, activation=\"sigmoid\")(model)\n",
    "\n",
    "        return Model([z, img], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                z = np.random.normal(size=(batch_size, self.latent_dim))\n",
    "                imgs_ = self.generator.predict(z)\n",
    "\n",
    "                # Select a random batch of images and encode\n",
    "                # idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                # imgs = X_train[idx]\n",
    "                z_ = self.encoder.predict(image_batch)\n",
    "\n",
    "                # Train the discriminator (img -> z is valid, z -> img is fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch([z_, image_batch], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([z, imgs_], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Train the generator (z -> img is valid and img -> z is is invalid)\n",
    "                g_loss = self.bigan_generator.train_on_batch([z, image_batch], [valid, fake])\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch,global_step, d_loss[0],\n",
    "                                                                                   100 * d_loss[1], g_loss[0]))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if global_step % sample_interval == 0:\n",
    "                    self.sample_interval(epoch,global_step)\n",
    "\n",
    "\n",
    "    def sample_interval(self, epoch,global_step):\n",
    "        r, c = 10, 10\n",
    "        z = np.random.normal(size=(r*c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(z)\n",
    "\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        if not os.path.isdir('images_bigan'):\n",
    "            os.mkdir('images_bigan')\n",
    "        fig.savefig(\"images_bigan/epoch_%d_step_%d.png\" % (epoch,global_step))\n",
    "        plt.close()\n",
    "    def sample_interval_mult(self, epoch):\n",
    "        r, c = 20, 20\n",
    "        z = np.random.normal(size=(r*c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(z)\n",
    "\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig1, axs1 = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r//2):\n",
    "            for j in range(c//2):\n",
    "                axs1[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs1[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig2, axs2 = plt.subplots(r, c)\n",
    "        for i in range(r//2):\n",
    "            for j in range(c//2):\n",
    "                axs2[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs2[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        if not os.path.isdir('images_bigan'):\n",
    "            os.mkdir('images_bigan')\n",
    "        fig1.savefig(\"images_bigan/%d_1.png\" % epoch)\n",
    "        fig2.savefig(\"images_bigan/%d_2.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    bigan = BIGAN()\n",
    "    bigan.train(epochs=30, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
