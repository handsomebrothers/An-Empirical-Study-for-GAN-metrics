{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
      "=================================================================\n",
      "Total params: 4,310,400\n",
      "Trainable params: 4,308,480\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 2048)              206848    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 256)         3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 128)         819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 32, 32, 3)         4803      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 4,522,883\n",
      "Trainable params: 4,517,891\n",
      "Non-trainable params: 4,992\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:1 [D loss: 3.709824, acc.: 30.47%] [G loss: 0.363816]\n",
      "epoch:0 step:2 [D loss: 0.877001, acc.: 50.00%] [G loss: 3.849415]\n",
      "epoch:0 step:3 [D loss: 0.933845, acc.: 53.91%] [G loss: 2.861706]\n",
      "epoch:0 step:4 [D loss: 0.786779, acc.: 42.97%] [G loss: 3.696392]\n",
      "epoch:0 step:5 [D loss: 0.290398, acc.: 92.19%] [G loss: 3.537023]\n",
      "epoch:0 step:6 [D loss: 0.957640, acc.: 45.31%] [G loss: 4.715917]\n",
      "epoch:0 step:7 [D loss: 0.332690, acc.: 83.59%] [G loss: 3.808960]\n",
      "epoch:0 step:8 [D loss: 0.319941, acc.: 96.88%] [G loss: 4.356854]\n",
      "epoch:0 step:9 [D loss: 0.089602, acc.: 99.22%] [G loss: 4.339054]\n",
      "epoch:0 step:10 [D loss: 0.672593, acc.: 50.78%] [G loss: 7.162156]\n",
      "epoch:0 step:11 [D loss: 0.255331, acc.: 85.94%] [G loss: 6.342080]\n",
      "epoch:0 step:12 [D loss: 0.137135, acc.: 95.31%] [G loss: 4.453283]\n",
      "epoch:0 step:13 [D loss: 0.086170, acc.: 100.00%] [G loss: 4.860901]\n",
      "epoch:0 step:14 [D loss: 0.033759, acc.: 100.00%] [G loss: 5.045748]\n",
      "epoch:0 step:15 [D loss: 0.386862, acc.: 71.09%] [G loss: 8.104814]\n",
      "epoch:0 step:16 [D loss: 0.216669, acc.: 89.84%] [G loss: 7.123989]\n",
      "epoch:0 step:17 [D loss: 0.080282, acc.: 99.22%] [G loss: 5.082751]\n",
      "epoch:0 step:18 [D loss: 0.046231, acc.: 100.00%] [G loss: 4.445803]\n",
      "epoch:0 step:19 [D loss: 0.101610, acc.: 97.66%] [G loss: 6.784240]\n",
      "epoch:0 step:20 [D loss: 0.025605, acc.: 100.00%] [G loss: 6.374457]\n",
      "epoch:0 step:21 [D loss: 0.042528, acc.: 100.00%] [G loss: 2.555727]\n",
      "epoch:0 step:22 [D loss: 0.790576, acc.: 54.69%] [G loss: 8.713484]\n",
      "epoch:0 step:23 [D loss: 0.735991, acc.: 68.75%] [G loss: 5.566303]\n",
      "epoch:0 step:24 [D loss: 0.052101, acc.: 99.22%] [G loss: 2.988091]\n",
      "epoch:0 step:25 [D loss: 0.051389, acc.: 99.22%] [G loss: 0.892084]\n",
      "epoch:0 step:26 [D loss: 0.100203, acc.: 100.00%] [G loss: 1.346786]\n",
      "epoch:0 step:27 [D loss: 0.187667, acc.: 89.06%] [G loss: 4.935275]\n",
      "epoch:0 step:28 [D loss: 0.532475, acc.: 77.34%] [G loss: 1.503291]\n",
      "epoch:0 step:29 [D loss: 0.455091, acc.: 70.31%] [G loss: 7.960520]\n",
      "epoch:0 step:30 [D loss: 0.738147, acc.: 85.94%] [G loss: 6.264138]\n",
      "epoch:0 step:31 [D loss: 0.299149, acc.: 93.75%] [G loss: 1.943798]\n",
      "epoch:0 step:32 [D loss: 2.017584, acc.: 62.50%] [G loss: 5.570115]\n",
      "epoch:0 step:33 [D loss: 0.314354, acc.: 82.03%] [G loss: 4.911517]\n",
      "epoch:0 step:34 [D loss: 0.316742, acc.: 89.06%] [G loss: 2.660417]\n",
      "epoch:0 step:35 [D loss: 0.094710, acc.: 100.00%] [G loss: 2.791953]\n",
      "epoch:0 step:36 [D loss: 0.073971, acc.: 100.00%] [G loss: 2.828038]\n",
      "epoch:0 step:37 [D loss: 0.933917, acc.: 60.16%] [G loss: 7.811429]\n",
      "epoch:0 step:38 [D loss: 1.299896, acc.: 64.84%] [G loss: 5.450590]\n",
      "epoch:0 step:39 [D loss: 0.303604, acc.: 83.59%] [G loss: 3.300731]\n",
      "epoch:0 step:40 [D loss: 0.102592, acc.: 100.00%] [G loss: 2.568968]\n",
      "epoch:0 step:41 [D loss: 0.066184, acc.: 100.00%] [G loss: 2.693147]\n",
      "epoch:0 step:42 [D loss: 0.071822, acc.: 100.00%] [G loss: 2.548918]\n",
      "epoch:0 step:43 [D loss: 0.090812, acc.: 100.00%] [G loss: 2.169601]\n",
      "epoch:0 step:44 [D loss: 0.177141, acc.: 89.84%] [G loss: 4.893661]\n",
      "epoch:0 step:45 [D loss: 0.207220, acc.: 93.75%] [G loss: 3.673723]\n",
      "epoch:0 step:46 [D loss: 0.134496, acc.: 99.22%] [G loss: 4.816180]\n",
      "epoch:0 step:47 [D loss: 0.211559, acc.: 93.75%] [G loss: 0.745562]\n",
      "epoch:0 step:48 [D loss: 0.387953, acc.: 75.00%] [G loss: 7.553791]\n",
      "epoch:0 step:49 [D loss: 1.514587, acc.: 62.50%] [G loss: 2.889244]\n",
      "epoch:0 step:50 [D loss: 0.279406, acc.: 94.53%] [G loss: 2.515026]\n",
      "epoch:0 step:51 [D loss: 0.038744, acc.: 100.00%] [G loss: 3.276646]\n",
      "epoch:0 step:52 [D loss: 0.042685, acc.: 100.00%] [G loss: 2.690103]\n",
      "epoch:0 step:53 [D loss: 0.060318, acc.: 100.00%] [G loss: 2.920959]\n",
      "epoch:0 step:54 [D loss: 0.042397, acc.: 100.00%] [G loss: 2.668818]\n",
      "epoch:0 step:55 [D loss: 0.184491, acc.: 100.00%] [G loss: 6.758431]\n",
      "epoch:0 step:56 [D loss: 0.122244, acc.: 94.53%] [G loss: 6.889667]\n",
      "epoch:0 step:57 [D loss: 0.076705, acc.: 97.66%] [G loss: 5.506583]\n",
      "epoch:0 step:58 [D loss: 0.026178, acc.: 100.00%] [G loss: 4.674439]\n",
      "epoch:0 step:59 [D loss: 0.050472, acc.: 100.00%] [G loss: 4.774317]\n",
      "epoch:0 step:60 [D loss: 0.026205, acc.: 100.00%] [G loss: 5.040399]\n",
      "epoch:0 step:61 [D loss: 0.028435, acc.: 100.00%] [G loss: 4.679135]\n",
      "epoch:0 step:62 [D loss: 0.033090, acc.: 100.00%] [G loss: 4.360110]\n",
      "epoch:0 step:63 [D loss: 0.056352, acc.: 100.00%] [G loss: 5.357387]\n",
      "epoch:0 step:64 [D loss: 0.024633, acc.: 100.00%] [G loss: 4.797383]\n",
      "epoch:0 step:65 [D loss: 0.087245, acc.: 100.00%] [G loss: 5.945333]\n",
      "epoch:0 step:66 [D loss: 0.064472, acc.: 99.22%] [G loss: 5.850437]\n",
      "epoch:0 step:67 [D loss: 0.041653, acc.: 100.00%] [G loss: 4.746008]\n",
      "epoch:0 step:68 [D loss: 0.190199, acc.: 98.44%] [G loss: 8.467077]\n",
      "epoch:0 step:69 [D loss: 0.832848, acc.: 69.53%] [G loss: 3.719055]\n",
      "epoch:0 step:70 [D loss: 0.102436, acc.: 96.09%] [G loss: 5.275229]\n",
      "epoch:0 step:71 [D loss: 0.046255, acc.: 100.00%] [G loss: 5.793588]\n",
      "epoch:0 step:72 [D loss: 0.010804, acc.: 100.00%] [G loss: 6.289935]\n",
      "epoch:0 step:73 [D loss: 0.021262, acc.: 100.00%] [G loss: 5.994576]\n",
      "epoch:0 step:74 [D loss: 0.061497, acc.: 99.22%] [G loss: 6.191088]\n",
      "epoch:0 step:75 [D loss: 0.039965, acc.: 100.00%] [G loss: 6.192482]\n",
      "epoch:0 step:76 [D loss: 0.043845, acc.: 100.00%] [G loss: 5.894357]\n",
      "epoch:0 step:77 [D loss: 0.244441, acc.: 97.66%] [G loss: 9.076818]\n",
      "epoch:0 step:78 [D loss: 0.718977, acc.: 71.88%] [G loss: 4.546661]\n",
      "epoch:0 step:79 [D loss: 0.035728, acc.: 100.00%] [G loss: 3.788704]\n",
      "epoch:0 step:80 [D loss: 0.021458, acc.: 100.00%] [G loss: 3.760516]\n",
      "epoch:0 step:81 [D loss: 0.041864, acc.: 100.00%] [G loss: 4.294479]\n",
      "epoch:0 step:82 [D loss: 0.016359, acc.: 100.00%] [G loss: 4.088679]\n",
      "epoch:0 step:83 [D loss: 0.134959, acc.: 94.53%] [G loss: 5.973127]\n",
      "epoch:0 step:84 [D loss: 0.022569, acc.: 100.00%] [G loss: 5.466468]\n",
      "epoch:0 step:85 [D loss: 0.332739, acc.: 90.62%] [G loss: 3.652421]\n",
      "epoch:0 step:86 [D loss: 0.120289, acc.: 95.31%] [G loss: 4.413067]\n",
      "epoch:0 step:87 [D loss: 0.012091, acc.: 100.00%] [G loss: 3.805603]\n",
      "epoch:0 step:88 [D loss: 0.199613, acc.: 96.88%] [G loss: 6.596137]\n",
      "epoch:0 step:89 [D loss: 0.165978, acc.: 93.75%] [G loss: 5.692075]\n",
      "epoch:0 step:90 [D loss: 0.204623, acc.: 92.97%] [G loss: 2.348435]\n",
      "epoch:0 step:91 [D loss: 0.524829, acc.: 66.41%] [G loss: 8.659597]\n",
      "epoch:0 step:92 [D loss: 0.650349, acc.: 81.25%] [G loss: 6.997010]\n",
      "epoch:0 step:93 [D loss: 0.187817, acc.: 92.97%] [G loss: 3.848722]\n",
      "epoch:0 step:94 [D loss: 0.169442, acc.: 93.75%] [G loss: 4.056437]\n",
      "epoch:0 step:95 [D loss: 0.041939, acc.: 100.00%] [G loss: 4.098439]\n",
      "epoch:0 step:96 [D loss: 0.090781, acc.: 97.66%] [G loss: 3.825066]\n",
      "epoch:0 step:97 [D loss: 0.088072, acc.: 97.66%] [G loss: 4.859759]\n",
      "epoch:0 step:98 [D loss: 0.056815, acc.: 98.44%] [G loss: 4.263505]\n",
      "epoch:0 step:99 [D loss: 0.192491, acc.: 95.31%] [G loss: 6.557275]\n",
      "epoch:0 step:100 [D loss: 0.099520, acc.: 96.09%] [G loss: 6.714590]\n",
      "epoch:0 step:101 [D loss: 0.049297, acc.: 100.00%] [G loss: 5.677158]\n",
      "epoch:0 step:102 [D loss: 0.026950, acc.: 100.00%] [G loss: 3.987467]\n",
      "epoch:0 step:103 [D loss: 0.123929, acc.: 100.00%] [G loss: 6.056415]\n",
      "epoch:0 step:104 [D loss: 0.044610, acc.: 98.44%] [G loss: 6.373811]\n",
      "epoch:0 step:105 [D loss: 0.126975, acc.: 95.31%] [G loss: 3.865610]\n",
      "epoch:0 step:106 [D loss: 0.196239, acc.: 96.88%] [G loss: 6.491087]\n",
      "epoch:0 step:107 [D loss: 0.108846, acc.: 96.09%] [G loss: 5.805841]\n",
      "epoch:0 step:108 [D loss: 0.138906, acc.: 96.88%] [G loss: 3.775164]\n",
      "epoch:0 step:109 [D loss: 0.391737, acc.: 90.62%] [G loss: 8.431028]\n",
      "epoch:0 step:110 [D loss: 1.253005, acc.: 60.94%] [G loss: 3.165026]\n",
      "epoch:0 step:111 [D loss: 0.387178, acc.: 84.38%] [G loss: 4.897986]\n",
      "epoch:0 step:112 [D loss: 0.022129, acc.: 100.00%] [G loss: 4.725968]\n",
      "epoch:0 step:113 [D loss: 0.181082, acc.: 96.09%] [G loss: 3.209650]\n",
      "epoch:0 step:114 [D loss: 0.408091, acc.: 88.28%] [G loss: 4.899570]\n",
      "epoch:0 step:115 [D loss: 0.134001, acc.: 94.53%] [G loss: 4.374213]\n",
      "epoch:0 step:116 [D loss: 0.257088, acc.: 90.62%] [G loss: 3.170696]\n",
      "epoch:0 step:117 [D loss: 0.125159, acc.: 97.66%] [G loss: 3.512002]\n",
      "epoch:0 step:118 [D loss: 0.129088, acc.: 94.53%] [G loss: 2.979256]\n",
      "epoch:0 step:119 [D loss: 0.231476, acc.: 89.84%] [G loss: 5.515174]\n",
      "epoch:0 step:120 [D loss: 0.353868, acc.: 87.50%] [G loss: 4.793836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:121 [D loss: 0.368158, acc.: 81.25%] [G loss: 6.699224]\n",
      "epoch:0 step:122 [D loss: 0.172715, acc.: 94.53%] [G loss: 5.832705]\n",
      "epoch:0 step:123 [D loss: 0.237150, acc.: 92.19%] [G loss: 4.416577]\n",
      "epoch:0 step:124 [D loss: 0.637341, acc.: 82.03%] [G loss: 6.684715]\n",
      "epoch:0 step:125 [D loss: 0.546957, acc.: 77.34%] [G loss: 3.846003]\n",
      "epoch:0 step:126 [D loss: 0.088042, acc.: 100.00%] [G loss: 3.445774]\n",
      "epoch:0 step:127 [D loss: 0.055817, acc.: 98.44%] [G loss: 2.409361]\n",
      "epoch:0 step:128 [D loss: 0.181096, acc.: 95.31%] [G loss: 2.810153]\n",
      "epoch:0 step:129 [D loss: 0.106473, acc.: 97.66%] [G loss: 3.590302]\n",
      "epoch:0 step:130 [D loss: 0.133945, acc.: 97.66%] [G loss: 3.101332]\n",
      "epoch:0 step:131 [D loss: 0.101992, acc.: 97.66%] [G loss: 1.488558]\n",
      "epoch:0 step:132 [D loss: 0.242752, acc.: 92.19%] [G loss: 6.181052]\n",
      "epoch:0 step:133 [D loss: 0.531168, acc.: 72.66%] [G loss: 0.419150]\n",
      "epoch:0 step:134 [D loss: 0.182424, acc.: 94.53%] [G loss: 3.003994]\n",
      "epoch:0 step:135 [D loss: 0.015083, acc.: 100.00%] [G loss: 2.323280]\n",
      "epoch:0 step:136 [D loss: 0.387230, acc.: 85.16%] [G loss: 5.009920]\n",
      "epoch:0 step:137 [D loss: 0.564530, acc.: 81.25%] [G loss: 0.806933]\n",
      "epoch:0 step:138 [D loss: 0.123792, acc.: 95.31%] [G loss: 2.031137]\n",
      "epoch:0 step:139 [D loss: 0.150169, acc.: 94.53%] [G loss: 0.217037]\n",
      "epoch:0 step:140 [D loss: 0.157084, acc.: 98.44%] [G loss: 2.209316]\n",
      "epoch:0 step:141 [D loss: 0.183985, acc.: 92.97%] [G loss: 0.111464]\n",
      "epoch:0 step:142 [D loss: 0.171593, acc.: 94.53%] [G loss: 3.312815]\n",
      "epoch:0 step:143 [D loss: 0.280211, acc.: 85.94%] [G loss: 0.095260]\n",
      "epoch:0 step:144 [D loss: 0.272573, acc.: 85.94%] [G loss: 6.192764]\n",
      "epoch:0 step:145 [D loss: 0.537050, acc.: 75.00%] [G loss: 0.974598]\n",
      "epoch:0 step:146 [D loss: 0.679462, acc.: 67.97%] [G loss: 7.737731]\n",
      "epoch:0 step:147 [D loss: 1.058550, acc.: 64.06%] [G loss: 3.290325]\n",
      "epoch:0 step:148 [D loss: 0.262596, acc.: 91.41%] [G loss: 4.016820]\n",
      "epoch:0 step:149 [D loss: 0.045611, acc.: 99.22%] [G loss: 4.257640]\n",
      "epoch:0 step:150 [D loss: 0.187841, acc.: 96.09%] [G loss: 3.434489]\n",
      "epoch:0 step:151 [D loss: 0.272774, acc.: 88.28%] [G loss: 4.936664]\n",
      "epoch:0 step:152 [D loss: 0.280510, acc.: 89.84%] [G loss: 2.363903]\n",
      "epoch:0 step:153 [D loss: 0.881038, acc.: 61.72%] [G loss: 5.737071]\n",
      "epoch:0 step:154 [D loss: 1.492417, acc.: 53.91%] [G loss: 2.213797]\n",
      "epoch:0 step:155 [D loss: 0.414535, acc.: 84.38%] [G loss: 2.298188]\n",
      "epoch:0 step:156 [D loss: 0.164907, acc.: 95.31%] [G loss: 2.908182]\n",
      "epoch:0 step:157 [D loss: 0.139927, acc.: 97.66%] [G loss: 2.969033]\n",
      "epoch:0 step:158 [D loss: 0.104027, acc.: 100.00%] [G loss: 2.846350]\n",
      "epoch:0 step:159 [D loss: 0.117873, acc.: 99.22%] [G loss: 3.146112]\n",
      "epoch:0 step:160 [D loss: 0.132864, acc.: 98.44%] [G loss: 3.756446]\n",
      "epoch:0 step:161 [D loss: 0.301108, acc.: 93.75%] [G loss: 4.886945]\n",
      "epoch:0 step:162 [D loss: 0.313278, acc.: 85.94%] [G loss: 3.852783]\n",
      "epoch:0 step:163 [D loss: 0.169651, acc.: 97.66%] [G loss: 4.024826]\n",
      "epoch:0 step:164 [D loss: 0.183253, acc.: 94.53%] [G loss: 3.914833]\n",
      "epoch:0 step:165 [D loss: 0.091460, acc.: 100.00%] [G loss: 4.281573]\n",
      "epoch:0 step:166 [D loss: 0.121796, acc.: 98.44%] [G loss: 4.846152]\n",
      "epoch:0 step:167 [D loss: 0.914333, acc.: 50.78%] [G loss: 6.628920]\n",
      "epoch:0 step:168 [D loss: 2.005501, acc.: 48.44%] [G loss: 2.013427]\n",
      "epoch:0 step:169 [D loss: 0.677621, acc.: 66.41%] [G loss: 2.498683]\n",
      "epoch:0 step:170 [D loss: 0.194951, acc.: 95.31%] [G loss: 3.034965]\n",
      "epoch:0 step:171 [D loss: 0.110942, acc.: 97.66%] [G loss: 3.119555]\n",
      "epoch:0 step:172 [D loss: 0.093337, acc.: 100.00%] [G loss: 3.021304]\n",
      "epoch:0 step:173 [D loss: 0.071963, acc.: 100.00%] [G loss: 3.185893]\n",
      "epoch:0 step:174 [D loss: 0.084422, acc.: 100.00%] [G loss: 3.568210]\n",
      "epoch:0 step:175 [D loss: 0.085635, acc.: 100.00%] [G loss: 3.746781]\n",
      "epoch:0 step:176 [D loss: 0.101797, acc.: 99.22%] [G loss: 4.067682]\n",
      "epoch:0 step:177 [D loss: 0.130100, acc.: 99.22%] [G loss: 4.851005]\n",
      "epoch:0 step:178 [D loss: 0.125076, acc.: 95.31%] [G loss: 4.096866]\n",
      "epoch:0 step:179 [D loss: 0.382060, acc.: 82.81%] [G loss: 7.171296]\n",
      "epoch:0 step:180 [D loss: 0.625977, acc.: 67.19%] [G loss: 5.432292]\n",
      "epoch:0 step:181 [D loss: 0.029066, acc.: 100.00%] [G loss: 4.352277]\n",
      "epoch:0 step:182 [D loss: 0.043008, acc.: 100.00%] [G loss: 4.033644]\n",
      "epoch:0 step:183 [D loss: 0.085580, acc.: 98.44%] [G loss: 4.385477]\n",
      "epoch:0 step:184 [D loss: 0.040756, acc.: 100.00%] [G loss: 4.512243]\n",
      "epoch:0 step:185 [D loss: 0.054682, acc.: 100.00%] [G loss: 4.743990]\n",
      "epoch:0 step:186 [D loss: 0.129878, acc.: 99.22%] [G loss: 5.497008]\n",
      "epoch:0 step:187 [D loss: 0.101928, acc.: 96.88%] [G loss: 5.060125]\n",
      "epoch:0 step:188 [D loss: 0.051661, acc.: 100.00%] [G loss: 4.993076]\n",
      "epoch:0 step:189 [D loss: 0.190006, acc.: 96.09%] [G loss: 7.021207]\n",
      "epoch:0 step:190 [D loss: 0.327697, acc.: 85.94%] [G loss: 5.197272]\n",
      "epoch:0 step:191 [D loss: 0.027658, acc.: 100.00%] [G loss: 4.071639]\n",
      "epoch:0 step:192 [D loss: 0.171630, acc.: 94.53%] [G loss: 6.657209]\n",
      "epoch:0 step:193 [D loss: 0.128939, acc.: 94.53%] [G loss: 6.320108]\n",
      "epoch:0 step:194 [D loss: 0.114148, acc.: 97.66%] [G loss: 4.400446]\n",
      "epoch:0 step:195 [D loss: 0.092298, acc.: 97.66%] [G loss: 5.602479]\n",
      "epoch:0 step:196 [D loss: 0.089030, acc.: 98.44%] [G loss: 5.016720]\n",
      "epoch:0 step:197 [D loss: 0.050690, acc.: 100.00%] [G loss: 5.198057]\n",
      "epoch:0 step:198 [D loss: 0.029893, acc.: 100.00%] [G loss: 3.482593]\n",
      "epoch:0 step:199 [D loss: 1.371630, acc.: 50.00%] [G loss: 9.087084]\n",
      "epoch:0 step:200 [D loss: 3.113503, acc.: 50.00%] [G loss: 3.234193]\n",
      "epoch:0 step:201 [D loss: 0.794379, acc.: 59.38%] [G loss: 1.331202]\n",
      "epoch:0 step:202 [D loss: 0.471182, acc.: 77.34%] [G loss: 1.885258]\n",
      "epoch:0 step:203 [D loss: 0.218904, acc.: 96.88%] [G loss: 2.314452]\n",
      "epoch:0 step:204 [D loss: 0.204958, acc.: 97.66%] [G loss: 2.399203]\n",
      "epoch:0 step:205 [D loss: 0.232509, acc.: 95.31%] [G loss: 2.564533]\n",
      "epoch:0 step:206 [D loss: 0.132829, acc.: 99.22%] [G loss: 2.795541]\n",
      "epoch:0 step:207 [D loss: 0.135279, acc.: 97.66%] [G loss: 2.965143]\n",
      "epoch:0 step:208 [D loss: 0.118209, acc.: 100.00%] [G loss: 3.102644]\n",
      "epoch:0 step:209 [D loss: 0.109714, acc.: 100.00%] [G loss: 3.070248]\n",
      "epoch:0 step:210 [D loss: 0.314978, acc.: 90.62%] [G loss: 3.185171]\n",
      "epoch:0 step:211 [D loss: 0.142553, acc.: 98.44%] [G loss: 3.701662]\n",
      "epoch:0 step:212 [D loss: 0.135380, acc.: 99.22%] [G loss: 2.600179]\n",
      "epoch:0 step:213 [D loss: 0.295186, acc.: 91.41%] [G loss: 3.113339]\n",
      "epoch:0 step:214 [D loss: 0.209952, acc.: 92.97%] [G loss: 4.028418]\n",
      "epoch:0 step:215 [D loss: 0.090264, acc.: 99.22%] [G loss: 3.676208]\n",
      "epoch:0 step:216 [D loss: 0.136670, acc.: 96.88%] [G loss: 3.639552]\n",
      "epoch:0 step:217 [D loss: 0.053321, acc.: 99.22%] [G loss: 3.515170]\n",
      "epoch:0 step:218 [D loss: 0.127002, acc.: 99.22%] [G loss: 3.294834]\n",
      "epoch:0 step:219 [D loss: 0.110021, acc.: 97.66%] [G loss: 4.215854]\n",
      "epoch:0 step:220 [D loss: 0.445101, acc.: 82.81%] [G loss: 5.846585]\n",
      "epoch:0 step:221 [D loss: 0.348008, acc.: 82.03%] [G loss: 4.278864]\n",
      "epoch:0 step:222 [D loss: 0.443247, acc.: 77.34%] [G loss: 6.295104]\n",
      "epoch:0 step:223 [D loss: 0.312664, acc.: 84.38%] [G loss: 5.130250]\n",
      "epoch:0 step:224 [D loss: 0.341296, acc.: 83.59%] [G loss: 5.282399]\n",
      "epoch:0 step:225 [D loss: 0.143502, acc.: 96.09%] [G loss: 4.684186]\n",
      "epoch:0 step:226 [D loss: 0.251340, acc.: 91.41%] [G loss: 6.430312]\n",
      "epoch:0 step:227 [D loss: 0.230854, acc.: 89.06%] [G loss: 4.477686]\n",
      "epoch:0 step:228 [D loss: 0.403273, acc.: 79.69%] [G loss: 6.505462]\n",
      "epoch:0 step:229 [D loss: 0.253816, acc.: 89.84%] [G loss: 4.944095]\n",
      "epoch:0 step:230 [D loss: 0.408737, acc.: 77.34%] [G loss: 5.815989]\n",
      "epoch:0 step:231 [D loss: 0.188935, acc.: 92.97%] [G loss: 3.865353]\n",
      "epoch:0 step:232 [D loss: 0.135155, acc.: 96.88%] [G loss: 4.939134]\n",
      "epoch:0 step:233 [D loss: 0.139776, acc.: 96.88%] [G loss: 5.786014]\n",
      "epoch:0 step:234 [D loss: 0.818493, acc.: 57.81%] [G loss: 6.433025]\n",
      "epoch:0 step:235 [D loss: 0.367473, acc.: 82.03%] [G loss: 4.992183]\n",
      "epoch:0 step:236 [D loss: 0.062971, acc.: 99.22%] [G loss: 4.157470]\n",
      "epoch:0 step:237 [D loss: 0.141054, acc.: 96.09%] [G loss: 3.997645]\n",
      "epoch:0 step:238 [D loss: 0.307351, acc.: 84.38%] [G loss: 5.979138]\n",
      "epoch:0 step:239 [D loss: 0.599241, acc.: 75.00%] [G loss: 4.830697]\n",
      "epoch:0 step:240 [D loss: 0.450615, acc.: 75.78%] [G loss: 5.694689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:241 [D loss: 0.619210, acc.: 75.78%] [G loss: 2.696591]\n",
      "epoch:0 step:242 [D loss: 1.006572, acc.: 53.91%] [G loss: 6.835209]\n",
      "epoch:0 step:243 [D loss: 0.849911, acc.: 64.06%] [G loss: 4.912838]\n",
      "epoch:0 step:244 [D loss: 0.166796, acc.: 93.75%] [G loss: 3.204659]\n",
      "epoch:0 step:245 [D loss: 0.384550, acc.: 80.47%] [G loss: 3.836192]\n",
      "epoch:0 step:246 [D loss: 0.173288, acc.: 92.97%] [G loss: 3.715713]\n",
      "epoch:0 step:247 [D loss: 0.222569, acc.: 93.75%] [G loss: 3.406813]\n",
      "epoch:0 step:248 [D loss: 0.336415, acc.: 91.41%] [G loss: 3.343728]\n",
      "epoch:0 step:249 [D loss: 0.155614, acc.: 97.66%] [G loss: 4.085981]\n",
      "epoch:0 step:250 [D loss: 0.463756, acc.: 74.22%] [G loss: 4.275155]\n",
      "epoch:0 step:251 [D loss: 0.042944, acc.: 100.00%] [G loss: 4.684871]\n",
      "epoch:0 step:252 [D loss: 0.155017, acc.: 96.88%] [G loss: 2.992497]\n",
      "epoch:0 step:253 [D loss: 0.499510, acc.: 75.78%] [G loss: 5.519248]\n",
      "epoch:0 step:254 [D loss: 0.420048, acc.: 75.78%] [G loss: 3.747354]\n",
      "epoch:0 step:255 [D loss: 0.410918, acc.: 83.59%] [G loss: 3.997983]\n",
      "epoch:0 step:256 [D loss: 0.216989, acc.: 91.41%] [G loss: 4.126815]\n",
      "epoch:0 step:257 [D loss: 0.177256, acc.: 93.75%] [G loss: 3.856257]\n",
      "epoch:0 step:258 [D loss: 0.331957, acc.: 86.72%] [G loss: 5.585873]\n",
      "epoch:0 step:259 [D loss: 0.189684, acc.: 92.97%] [G loss: 5.076579]\n",
      "epoch:0 step:260 [D loss: 0.174279, acc.: 92.97%] [G loss: 3.344806]\n",
      "epoch:0 step:261 [D loss: 0.132651, acc.: 98.44%] [G loss: 4.195594]\n",
      "epoch:0 step:262 [D loss: 0.048008, acc.: 99.22%] [G loss: 4.279962]\n",
      "epoch:0 step:263 [D loss: 0.128741, acc.: 96.88%] [G loss: 4.679665]\n",
      "epoch:0 step:264 [D loss: 0.226906, acc.: 94.53%] [G loss: 5.878026]\n",
      "epoch:0 step:265 [D loss: 0.116406, acc.: 96.88%] [G loss: 5.398835]\n",
      "epoch:0 step:266 [D loss: 0.205572, acc.: 90.62%] [G loss: 6.419534]\n",
      "epoch:0 step:267 [D loss: 0.084230, acc.: 97.66%] [G loss: 5.701140]\n",
      "epoch:0 step:268 [D loss: 0.115731, acc.: 98.44%] [G loss: 6.263859]\n",
      "epoch:0 step:269 [D loss: 0.048951, acc.: 100.00%] [G loss: 6.224298]\n",
      "epoch:0 step:270 [D loss: 0.273431, acc.: 90.62%] [G loss: 8.515636]\n",
      "epoch:0 step:271 [D loss: 0.666561, acc.: 75.78%] [G loss: 3.235154]\n",
      "epoch:0 step:272 [D loss: 1.568640, acc.: 51.56%] [G loss: 8.212719]\n",
      "epoch:0 step:273 [D loss: 1.657084, acc.: 67.97%] [G loss: 5.672161]\n",
      "epoch:0 step:274 [D loss: 0.628233, acc.: 76.56%] [G loss: 2.835622]\n",
      "epoch:0 step:275 [D loss: 0.309894, acc.: 87.50%] [G loss: 3.299233]\n",
      "epoch:0 step:276 [D loss: 0.100513, acc.: 100.00%] [G loss: 4.067984]\n",
      "epoch:0 step:277 [D loss: 0.190895, acc.: 98.44%] [G loss: 3.701109]\n",
      "epoch:0 step:278 [D loss: 0.387650, acc.: 85.16%] [G loss: 4.870976]\n",
      "epoch:0 step:279 [D loss: 0.247718, acc.: 89.84%] [G loss: 3.898825]\n",
      "epoch:0 step:280 [D loss: 0.300504, acc.: 90.62%] [G loss: 4.140408]\n",
      "epoch:0 step:281 [D loss: 0.114691, acc.: 98.44%] [G loss: 4.261314]\n",
      "epoch:0 step:282 [D loss: 0.223655, acc.: 92.19%] [G loss: 4.987831]\n",
      "epoch:0 step:283 [D loss: 0.318204, acc.: 85.94%] [G loss: 4.335780]\n",
      "epoch:0 step:284 [D loss: 0.250195, acc.: 90.62%] [G loss: 5.325077]\n",
      "epoch:0 step:285 [D loss: 0.300473, acc.: 87.50%] [G loss: 4.163310]\n",
      "epoch:0 step:286 [D loss: 0.189546, acc.: 94.53%] [G loss: 5.145955]\n",
      "epoch:0 step:287 [D loss: 0.154920, acc.: 94.53%] [G loss: 4.192085]\n",
      "epoch:0 step:288 [D loss: 0.427424, acc.: 76.56%] [G loss: 7.279275]\n",
      "epoch:0 step:289 [D loss: 0.645672, acc.: 70.31%] [G loss: 3.708057]\n",
      "epoch:0 step:290 [D loss: 0.353137, acc.: 85.16%] [G loss: 5.293023]\n",
      "epoch:0 step:291 [D loss: 0.239288, acc.: 92.19%] [G loss: 4.418092]\n",
      "epoch:0 step:292 [D loss: 0.119702, acc.: 98.44%] [G loss: 3.321318]\n",
      "epoch:0 step:293 [D loss: 0.253978, acc.: 89.06%] [G loss: 4.361798]\n",
      "epoch:0 step:294 [D loss: 0.128282, acc.: 94.53%] [G loss: 3.566280]\n",
      "epoch:0 step:295 [D loss: 0.208252, acc.: 92.19%] [G loss: 3.092716]\n",
      "epoch:0 step:296 [D loss: 0.192239, acc.: 96.09%] [G loss: 4.333408]\n",
      "epoch:0 step:297 [D loss: 0.445571, acc.: 80.47%] [G loss: 4.249441]\n",
      "epoch:0 step:298 [D loss: 0.070189, acc.: 99.22%] [G loss: 3.947517]\n",
      "epoch:0 step:299 [D loss: 0.197168, acc.: 96.09%] [G loss: 6.532458]\n",
      "epoch:0 step:300 [D loss: 0.184408, acc.: 92.97%] [G loss: 5.261417]\n",
      "epoch:0 step:301 [D loss: 0.225775, acc.: 92.19%] [G loss: 6.331349]\n",
      "epoch:0 step:302 [D loss: 0.376043, acc.: 87.50%] [G loss: 3.893042]\n",
      "epoch:0 step:303 [D loss: 0.194513, acc.: 92.19%] [G loss: 4.983998]\n",
      "epoch:0 step:304 [D loss: 0.063532, acc.: 100.00%] [G loss: 4.451383]\n",
      "epoch:0 step:305 [D loss: 1.639957, acc.: 34.38%] [G loss: 6.267967]\n",
      "epoch:0 step:306 [D loss: 0.535342, acc.: 72.66%] [G loss: 4.058999]\n",
      "epoch:0 step:307 [D loss: 0.257237, acc.: 89.06%] [G loss: 3.059769]\n",
      "epoch:0 step:308 [D loss: 0.201947, acc.: 94.53%] [G loss: 3.865723]\n",
      "epoch:0 step:309 [D loss: 0.151902, acc.: 96.09%] [G loss: 3.503020]\n",
      "epoch:0 step:310 [D loss: 0.238176, acc.: 93.75%] [G loss: 4.560035]\n",
      "epoch:0 step:311 [D loss: 0.185586, acc.: 92.19%] [G loss: 3.899013]\n",
      "epoch:0 step:312 [D loss: 0.437812, acc.: 82.81%] [G loss: 7.103103]\n",
      "epoch:0 step:313 [D loss: 1.031332, acc.: 53.91%] [G loss: 2.928982]\n",
      "epoch:0 step:314 [D loss: 0.353089, acc.: 82.81%] [G loss: 3.894457]\n",
      "epoch:0 step:315 [D loss: 0.048454, acc.: 100.00%] [G loss: 4.668812]\n",
      "epoch:0 step:316 [D loss: 0.180034, acc.: 95.31%] [G loss: 3.982910]\n",
      "epoch:0 step:317 [D loss: 0.786626, acc.: 53.12%] [G loss: 6.032977]\n",
      "epoch:0 step:318 [D loss: 0.553559, acc.: 71.88%] [G loss: 5.239783]\n",
      "epoch:0 step:319 [D loss: 0.176760, acc.: 93.75%] [G loss: 3.474690]\n",
      "epoch:0 step:320 [D loss: 0.212182, acc.: 91.41%] [G loss: 3.809026]\n",
      "epoch:0 step:321 [D loss: 0.071600, acc.: 100.00%] [G loss: 3.719281]\n",
      "epoch:0 step:322 [D loss: 0.381780, acc.: 89.84%] [G loss: 3.132866]\n",
      "epoch:0 step:323 [D loss: 0.352432, acc.: 84.38%] [G loss: 5.791695]\n",
      "epoch:0 step:324 [D loss: 0.384334, acc.: 79.69%] [G loss: 5.003249]\n",
      "epoch:0 step:325 [D loss: 0.128702, acc.: 96.09%] [G loss: 3.699183]\n",
      "epoch:0 step:326 [D loss: 0.141851, acc.: 96.09%] [G loss: 4.361491]\n",
      "epoch:0 step:327 [D loss: 0.064544, acc.: 100.00%] [G loss: 4.484047]\n",
      "epoch:0 step:328 [D loss: 0.132811, acc.: 96.88%] [G loss: 4.499637]\n",
      "epoch:0 step:329 [D loss: 0.212631, acc.: 96.88%] [G loss: 6.314178]\n",
      "epoch:0 step:330 [D loss: 0.460816, acc.: 78.12%] [G loss: 3.379599]\n",
      "epoch:0 step:331 [D loss: 0.358124, acc.: 78.91%] [G loss: 7.364065]\n",
      "epoch:0 step:332 [D loss: 0.220470, acc.: 88.28%] [G loss: 6.932592]\n",
      "epoch:0 step:333 [D loss: 0.128707, acc.: 95.31%] [G loss: 4.446126]\n",
      "epoch:0 step:334 [D loss: 0.202903, acc.: 89.84%] [G loss: 4.599244]\n",
      "epoch:0 step:335 [D loss: 0.040819, acc.: 99.22%] [G loss: 4.633064]\n",
      "epoch:0 step:336 [D loss: 0.191155, acc.: 96.09%] [G loss: 4.448483]\n",
      "epoch:0 step:337 [D loss: 0.158469, acc.: 96.88%] [G loss: 4.320244]\n",
      "epoch:0 step:338 [D loss: 0.394956, acc.: 80.47%] [G loss: 7.431896]\n",
      "epoch:0 step:339 [D loss: 0.492360, acc.: 71.88%] [G loss: 4.211006]\n",
      "epoch:0 step:340 [D loss: 0.208199, acc.: 92.97%] [G loss: 4.271411]\n",
      "epoch:0 step:341 [D loss: 0.121833, acc.: 96.88%] [G loss: 4.757483]\n",
      "epoch:0 step:342 [D loss: 0.227934, acc.: 95.31%] [G loss: 5.742280]\n",
      "epoch:0 step:343 [D loss: 0.663498, acc.: 65.62%] [G loss: 7.039992]\n",
      "epoch:0 step:344 [D loss: 0.313081, acc.: 83.59%] [G loss: 5.031871]\n",
      "epoch:0 step:345 [D loss: 0.095005, acc.: 96.88%] [G loss: 4.692663]\n",
      "epoch:0 step:346 [D loss: 0.048176, acc.: 100.00%] [G loss: 4.129704]\n",
      "epoch:0 step:347 [D loss: 0.567546, acc.: 71.88%] [G loss: 7.606735]\n",
      "epoch:0 step:348 [D loss: 1.461295, acc.: 53.12%] [G loss: 3.007417]\n",
      "epoch:0 step:349 [D loss: 0.198058, acc.: 92.19%] [G loss: 3.278807]\n",
      "epoch:0 step:350 [D loss: 0.100193, acc.: 98.44%] [G loss: 3.559520]\n",
      "epoch:0 step:351 [D loss: 0.157029, acc.: 99.22%] [G loss: 4.054374]\n",
      "epoch:0 step:352 [D loss: 0.488641, acc.: 79.69%] [G loss: 5.277412]\n",
      "epoch:0 step:353 [D loss: 0.191192, acc.: 92.19%] [G loss: 4.621077]\n",
      "epoch:0 step:354 [D loss: 0.071178, acc.: 100.00%] [G loss: 3.664608]\n",
      "epoch:0 step:355 [D loss: 0.193669, acc.: 94.53%] [G loss: 4.962278]\n",
      "epoch:0 step:356 [D loss: 0.170027, acc.: 94.53%] [G loss: 4.343267]\n",
      "epoch:0 step:357 [D loss: 0.104547, acc.: 100.00%] [G loss: 3.924255]\n",
      "epoch:0 step:358 [D loss: 0.375881, acc.: 84.38%] [G loss: 6.149170]\n",
      "epoch:0 step:359 [D loss: 0.321195, acc.: 86.72%] [G loss: 4.368899]\n",
      "epoch:0 step:360 [D loss: 0.273608, acc.: 86.72%] [G loss: 5.936563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:361 [D loss: 0.203766, acc.: 92.19%] [G loss: 4.976453]\n",
      "epoch:0 step:362 [D loss: 0.156492, acc.: 96.09%] [G loss: 4.556374]\n",
      "epoch:0 step:363 [D loss: 0.198718, acc.: 95.31%] [G loss: 4.418118]\n",
      "epoch:0 step:364 [D loss: 0.166595, acc.: 98.44%] [G loss: 6.228459]\n",
      "epoch:0 step:365 [D loss: 0.353676, acc.: 85.94%] [G loss: 3.589326]\n",
      "epoch:0 step:366 [D loss: 0.210242, acc.: 92.19%] [G loss: 6.579834]\n",
      "epoch:0 step:367 [D loss: 0.407086, acc.: 75.00%] [G loss: 3.249542]\n",
      "epoch:0 step:368 [D loss: 0.476171, acc.: 73.44%] [G loss: 7.135163]\n",
      "epoch:0 step:369 [D loss: 0.281856, acc.: 85.94%] [G loss: 5.586872]\n",
      "epoch:0 step:370 [D loss: 0.088363, acc.: 98.44%] [G loss: 3.451583]\n",
      "epoch:0 step:371 [D loss: 0.115947, acc.: 98.44%] [G loss: 3.795541]\n",
      "epoch:0 step:372 [D loss: 0.130673, acc.: 97.66%] [G loss: 5.282598]\n",
      "epoch:0 step:373 [D loss: 1.329346, acc.: 35.16%] [G loss: 6.836594]\n",
      "epoch:0 step:374 [D loss: 0.741763, acc.: 61.72%] [G loss: 4.363427]\n",
      "epoch:0 step:375 [D loss: 0.416553, acc.: 79.69%] [G loss: 2.365689]\n",
      "epoch:0 step:376 [D loss: 0.348599, acc.: 81.25%] [G loss: 4.212773]\n",
      "epoch:0 step:377 [D loss: 0.125781, acc.: 97.66%] [G loss: 4.224990]\n",
      "epoch:0 step:378 [D loss: 0.380944, acc.: 82.03%] [G loss: 3.831221]\n",
      "epoch:0 step:379 [D loss: 0.087896, acc.: 98.44%] [G loss: 3.245479]\n",
      "epoch:0 step:380 [D loss: 0.738193, acc.: 60.16%] [G loss: 6.562169]\n",
      "epoch:0 step:381 [D loss: 0.761385, acc.: 67.19%] [G loss: 4.234104]\n",
      "epoch:0 step:382 [D loss: 0.140711, acc.: 97.66%] [G loss: 3.565408]\n",
      "epoch:0 step:383 [D loss: 0.131488, acc.: 98.44%] [G loss: 3.163403]\n",
      "epoch:0 step:384 [D loss: 0.326140, acc.: 82.81%] [G loss: 4.991484]\n",
      "epoch:0 step:385 [D loss: 0.291794, acc.: 88.28%] [G loss: 3.904355]\n",
      "epoch:0 step:386 [D loss: 0.267823, acc.: 93.75%] [G loss: 4.001102]\n",
      "epoch:0 step:387 [D loss: 0.421819, acc.: 79.69%] [G loss: 5.561360]\n",
      "epoch:0 step:388 [D loss: 0.335947, acc.: 85.16%] [G loss: 4.407067]\n",
      "epoch:0 step:389 [D loss: 0.226590, acc.: 93.75%] [G loss: 4.795616]\n",
      "epoch:0 step:390 [D loss: 0.239600, acc.: 92.19%] [G loss: 4.166452]\n",
      "epoch:0 step:391 [D loss: 0.356596, acc.: 86.72%] [G loss: 6.220356]\n",
      "epoch:0 step:392 [D loss: 0.271039, acc.: 86.72%] [G loss: 4.910474]\n",
      "epoch:0 step:393 [D loss: 0.344264, acc.: 86.72%] [G loss: 5.543775]\n",
      "epoch:0 step:394 [D loss: 0.089336, acc.: 98.44%] [G loss: 5.097036]\n",
      "epoch:0 step:395 [D loss: 0.543213, acc.: 72.66%] [G loss: 5.268277]\n",
      "epoch:0 step:396 [D loss: 0.137049, acc.: 96.88%] [G loss: 4.532693]\n",
      "epoch:0 step:397 [D loss: 0.484572, acc.: 75.00%] [G loss: 5.593412]\n",
      "epoch:0 step:398 [D loss: 0.349218, acc.: 79.69%] [G loss: 3.461938]\n",
      "epoch:0 step:399 [D loss: 0.224628, acc.: 91.41%] [G loss: 4.311592]\n",
      "epoch:0 step:400 [D loss: 0.070913, acc.: 100.00%] [G loss: 4.173965]\n",
      "epoch:0 step:401 [D loss: 0.328552, acc.: 84.38%] [G loss: 4.934242]\n",
      "epoch:0 step:402 [D loss: 0.201254, acc.: 92.97%] [G loss: 3.715109]\n",
      "epoch:0 step:403 [D loss: 0.204215, acc.: 94.53%] [G loss: 4.726110]\n",
      "epoch:0 step:404 [D loss: 0.174546, acc.: 96.88%] [G loss: 5.682317]\n",
      "epoch:0 step:405 [D loss: 0.131911, acc.: 94.53%] [G loss: 4.389544]\n",
      "epoch:0 step:406 [D loss: 0.233682, acc.: 91.41%] [G loss: 4.945180]\n",
      "epoch:0 step:407 [D loss: 0.367192, acc.: 84.38%] [G loss: 5.913774]\n",
      "epoch:0 step:408 [D loss: 0.204172, acc.: 92.97%] [G loss: 3.590636]\n",
      "epoch:0 step:409 [D loss: 0.525686, acc.: 70.31%] [G loss: 7.308390]\n",
      "epoch:0 step:410 [D loss: 0.676493, acc.: 68.75%] [G loss: 4.805895]\n",
      "epoch:0 step:411 [D loss: 0.074690, acc.: 98.44%] [G loss: 4.012637]\n",
      "epoch:0 step:412 [D loss: 0.045147, acc.: 100.00%] [G loss: 4.120475]\n",
      "epoch:0 step:413 [D loss: 0.083528, acc.: 98.44%] [G loss: 4.411615]\n",
      "epoch:0 step:414 [D loss: 0.115070, acc.: 97.66%] [G loss: 4.206845]\n",
      "epoch:0 step:415 [D loss: 0.153266, acc.: 96.88%] [G loss: 4.847627]\n",
      "epoch:0 step:416 [D loss: 0.168114, acc.: 96.09%] [G loss: 4.259352]\n",
      "epoch:0 step:417 [D loss: 0.226799, acc.: 92.19%] [G loss: 5.561114]\n",
      "epoch:0 step:418 [D loss: 0.075549, acc.: 99.22%] [G loss: 5.539562]\n",
      "epoch:0 step:419 [D loss: 0.204036, acc.: 92.97%] [G loss: 5.909602]\n",
      "epoch:0 step:420 [D loss: 0.133471, acc.: 96.09%] [G loss: 5.195421]\n",
      "epoch:0 step:421 [D loss: 0.256879, acc.: 90.62%] [G loss: 7.101465]\n",
      "epoch:0 step:422 [D loss: 0.376886, acc.: 80.47%] [G loss: 4.761626]\n",
      "epoch:0 step:423 [D loss: 0.140279, acc.: 96.09%] [G loss: 6.373088]\n",
      "epoch:0 step:424 [D loss: 0.025674, acc.: 100.00%] [G loss: 6.599294]\n",
      "epoch:0 step:425 [D loss: 0.046098, acc.: 100.00%] [G loss: 5.331121]\n",
      "epoch:0 step:426 [D loss: 0.225089, acc.: 90.62%] [G loss: 7.460317]\n",
      "epoch:0 step:427 [D loss: 0.141660, acc.: 93.75%] [G loss: 7.268328]\n",
      "epoch:0 step:428 [D loss: 0.030693, acc.: 100.00%] [G loss: 4.517326]\n",
      "epoch:0 step:429 [D loss: 0.359496, acc.: 80.47%] [G loss: 9.754864]\n",
      "epoch:0 step:430 [D loss: 1.081424, acc.: 62.50%] [G loss: 5.378346]\n",
      "epoch:0 step:431 [D loss: 0.098221, acc.: 95.31%] [G loss: 4.899805]\n",
      "epoch:0 step:432 [D loss: 0.028004, acc.: 100.00%] [G loss: 4.460306]\n",
      "epoch:0 step:433 [D loss: 0.065686, acc.: 98.44%] [G loss: 4.301130]\n",
      "epoch:0 step:434 [D loss: 0.183371, acc.: 93.75%] [G loss: 5.039363]\n",
      "epoch:0 step:435 [D loss: 0.113360, acc.: 96.09%] [G loss: 4.247622]\n",
      "epoch:0 step:436 [D loss: 0.579130, acc.: 64.84%] [G loss: 7.297503]\n",
      "epoch:0 step:437 [D loss: 1.076886, acc.: 53.91%] [G loss: 3.583939]\n",
      "epoch:0 step:438 [D loss: 0.130629, acc.: 97.66%] [G loss: 3.689802]\n",
      "epoch:0 step:439 [D loss: 0.042371, acc.: 100.00%] [G loss: 3.854269]\n",
      "epoch:0 step:440 [D loss: 0.065099, acc.: 100.00%] [G loss: 3.425611]\n",
      "epoch:0 step:441 [D loss: 0.089931, acc.: 99.22%] [G loss: 3.609625]\n",
      "epoch:0 step:442 [D loss: 0.152404, acc.: 94.53%] [G loss: 4.459646]\n",
      "epoch:0 step:443 [D loss: 0.211798, acc.: 94.53%] [G loss: 3.915152]\n",
      "epoch:0 step:444 [D loss: 0.126111, acc.: 99.22%] [G loss: 3.679485]\n",
      "epoch:0 step:445 [D loss: 0.170041, acc.: 95.31%] [G loss: 4.832849]\n",
      "epoch:0 step:446 [D loss: 0.386626, acc.: 83.59%] [G loss: 4.537128]\n",
      "epoch:0 step:447 [D loss: 0.057144, acc.: 99.22%] [G loss: 4.400146]\n",
      "epoch:0 step:448 [D loss: 0.269434, acc.: 89.06%] [G loss: 5.856524]\n",
      "epoch:0 step:449 [D loss: 0.175991, acc.: 93.75%] [G loss: 4.961588]\n",
      "epoch:0 step:450 [D loss: 0.115991, acc.: 97.66%] [G loss: 3.700983]\n",
      "epoch:0 step:451 [D loss: 0.117461, acc.: 96.88%] [G loss: 5.055882]\n",
      "epoch:0 step:452 [D loss: 0.059735, acc.: 99.22%] [G loss: 4.742776]\n",
      "epoch:0 step:453 [D loss: 0.311913, acc.: 86.72%] [G loss: 7.450835]\n",
      "epoch:0 step:454 [D loss: 0.562845, acc.: 69.53%] [G loss: 3.295672]\n",
      "epoch:0 step:455 [D loss: 0.177152, acc.: 93.75%] [G loss: 4.367359]\n",
      "epoch:0 step:456 [D loss: 0.126568, acc.: 96.88%] [G loss: 5.676751]\n",
      "epoch:0 step:457 [D loss: 0.067252, acc.: 97.66%] [G loss: 4.792097]\n",
      "epoch:0 step:458 [D loss: 0.248957, acc.: 89.84%] [G loss: 7.627360]\n",
      "epoch:0 step:459 [D loss: 1.174225, acc.: 54.69%] [G loss: 2.945721]\n",
      "epoch:0 step:460 [D loss: 0.138835, acc.: 96.09%] [G loss: 4.287003]\n",
      "epoch:0 step:461 [D loss: 0.025592, acc.: 100.00%] [G loss: 3.863796]\n",
      "epoch:0 step:462 [D loss: 0.102781, acc.: 100.00%] [G loss: 3.663472]\n",
      "epoch:0 step:463 [D loss: 0.190880, acc.: 96.09%] [G loss: 4.653686]\n",
      "epoch:0 step:464 [D loss: 0.266048, acc.: 87.50%] [G loss: 2.675539]\n",
      "epoch:0 step:465 [D loss: 0.469171, acc.: 71.09%] [G loss: 6.882738]\n",
      "epoch:0 step:466 [D loss: 0.496731, acc.: 75.78%] [G loss: 5.925784]\n",
      "epoch:0 step:467 [D loss: 0.188238, acc.: 92.19%] [G loss: 3.841931]\n",
      "epoch:0 step:468 [D loss: 0.135796, acc.: 95.31%] [G loss: 4.453790]\n",
      "epoch:0 step:469 [D loss: 0.021148, acc.: 100.00%] [G loss: 4.834075]\n",
      "epoch:0 step:470 [D loss: 0.045400, acc.: 100.00%] [G loss: 3.901157]\n",
      "epoch:0 step:471 [D loss: 0.153074, acc.: 95.31%] [G loss: 5.122749]\n",
      "epoch:0 step:472 [D loss: 0.072999, acc.: 96.88%] [G loss: 4.970902]\n",
      "epoch:0 step:473 [D loss: 0.146785, acc.: 96.09%] [G loss: 4.690711]\n",
      "epoch:0 step:474 [D loss: 0.198094, acc.: 95.31%] [G loss: 5.849938]\n",
      "epoch:0 step:475 [D loss: 0.083372, acc.: 97.66%] [G loss: 5.628153]\n",
      "epoch:0 step:476 [D loss: 0.103550, acc.: 99.22%] [G loss: 4.938928]\n",
      "epoch:0 step:477 [D loss: 0.062898, acc.: 100.00%] [G loss: 5.085328]\n",
      "epoch:0 step:478 [D loss: 0.123758, acc.: 96.88%] [G loss: 4.170654]\n",
      "epoch:0 step:479 [D loss: 0.134135, acc.: 98.44%] [G loss: 6.931687]\n",
      "epoch:0 step:480 [D loss: 0.214771, acc.: 90.62%] [G loss: 4.814528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:481 [D loss: 0.107269, acc.: 97.66%] [G loss: 6.178265]\n",
      "epoch:0 step:482 [D loss: 0.062826, acc.: 97.66%] [G loss: 5.155770]\n",
      "epoch:0 step:483 [D loss: 0.196319, acc.: 92.97%] [G loss: 6.975348]\n",
      "epoch:0 step:484 [D loss: 0.087977, acc.: 97.66%] [G loss: 6.134167]\n",
      "epoch:0 step:485 [D loss: 0.189263, acc.: 92.97%] [G loss: 4.770864]\n",
      "epoch:0 step:486 [D loss: 0.091543, acc.: 97.66%] [G loss: 6.054358]\n",
      "epoch:0 step:487 [D loss: 0.184473, acc.: 94.53%] [G loss: 5.633842]\n",
      "epoch:0 step:488 [D loss: 0.074072, acc.: 98.44%] [G loss: 6.352154]\n",
      "epoch:0 step:489 [D loss: 0.043593, acc.: 100.00%] [G loss: 5.739656]\n",
      "epoch:0 step:490 [D loss: 0.279763, acc.: 86.72%] [G loss: 9.185326]\n",
      "epoch:0 step:491 [D loss: 0.557029, acc.: 71.09%] [G loss: 4.691481]\n",
      "epoch:0 step:492 [D loss: 0.525831, acc.: 83.59%] [G loss: 6.735550]\n",
      "epoch:0 step:493 [D loss: 0.032106, acc.: 100.00%] [G loss: 7.135657]\n",
      "epoch:0 step:494 [D loss: 0.672588, acc.: 70.31%] [G loss: 4.511733]\n",
      "epoch:0 step:495 [D loss: 0.203234, acc.: 89.84%] [G loss: 5.631356]\n",
      "epoch:0 step:496 [D loss: 0.164126, acc.: 94.53%] [G loss: 6.296536]\n",
      "epoch:0 step:497 [D loss: 0.607583, acc.: 76.56%] [G loss: 6.786139]\n",
      "epoch:0 step:498 [D loss: 0.271593, acc.: 89.84%] [G loss: 4.685265]\n",
      "epoch:0 step:499 [D loss: 0.145888, acc.: 94.53%] [G loss: 5.605745]\n",
      "epoch:0 step:500 [D loss: 0.131562, acc.: 96.09%] [G loss: 3.670175]\n",
      "epoch:0 step:501 [D loss: 0.300271, acc.: 85.94%] [G loss: 7.629453]\n",
      "epoch:0 step:502 [D loss: 0.198107, acc.: 92.97%] [G loss: 6.108209]\n",
      "epoch:0 step:503 [D loss: 0.146121, acc.: 95.31%] [G loss: 3.476545]\n",
      "epoch:0 step:504 [D loss: 0.328626, acc.: 88.28%] [G loss: 7.790756]\n",
      "epoch:0 step:505 [D loss: 0.436680, acc.: 78.91%] [G loss: 3.544581]\n",
      "epoch:0 step:506 [D loss: 0.568499, acc.: 72.66%] [G loss: 8.205467]\n",
      "epoch:0 step:507 [D loss: 0.395324, acc.: 83.59%] [G loss: 7.174770]\n",
      "epoch:0 step:508 [D loss: 0.080277, acc.: 96.88%] [G loss: 5.053084]\n",
      "epoch:0 step:509 [D loss: 0.053938, acc.: 100.00%] [G loss: 3.378715]\n",
      "epoch:0 step:510 [D loss: 0.183978, acc.: 92.97%] [G loss: 5.633065]\n",
      "epoch:0 step:511 [D loss: 0.066867, acc.: 98.44%] [G loss: 5.434777]\n",
      "epoch:0 step:512 [D loss: 0.182795, acc.: 93.75%] [G loss: 5.321693]\n",
      "epoch:0 step:513 [D loss: 0.156866, acc.: 94.53%] [G loss: 5.111633]\n",
      "epoch:0 step:514 [D loss: 0.137628, acc.: 96.88%] [G loss: 4.594363]\n",
      "epoch:0 step:515 [D loss: 0.067637, acc.: 100.00%] [G loss: 4.487714]\n",
      "epoch:0 step:516 [D loss: 0.465557, acc.: 80.47%] [G loss: 9.064631]\n",
      "epoch:0 step:517 [D loss: 0.805463, acc.: 57.03%] [G loss: 4.357904]\n",
      "epoch:0 step:518 [D loss: 0.181815, acc.: 92.97%] [G loss: 4.525759]\n",
      "epoch:0 step:519 [D loss: 0.010701, acc.: 100.00%] [G loss: 4.632535]\n",
      "epoch:0 step:520 [D loss: 0.108204, acc.: 96.09%] [G loss: 4.924691]\n",
      "epoch:0 step:521 [D loss: 0.085967, acc.: 99.22%] [G loss: 4.689317]\n",
      "epoch:0 step:522 [D loss: 0.263572, acc.: 91.41%] [G loss: 6.677031]\n",
      "epoch:0 step:523 [D loss: 0.801259, acc.: 60.94%] [G loss: 3.234616]\n",
      "epoch:0 step:524 [D loss: 0.128324, acc.: 96.88%] [G loss: 4.414005]\n",
      "epoch:0 step:525 [D loss: 0.030023, acc.: 99.22%] [G loss: 4.284975]\n",
      "epoch:0 step:526 [D loss: 0.458794, acc.: 78.12%] [G loss: 7.354427]\n",
      "epoch:0 step:527 [D loss: 0.309836, acc.: 83.59%] [G loss: 7.056972]\n",
      "epoch:0 step:528 [D loss: 0.039744, acc.: 99.22%] [G loss: 5.645494]\n",
      "epoch:0 step:529 [D loss: 0.026114, acc.: 100.00%] [G loss: 4.771193]\n",
      "epoch:0 step:530 [D loss: 0.036825, acc.: 100.00%] [G loss: 3.726800]\n",
      "epoch:0 step:531 [D loss: 0.278709, acc.: 87.50%] [G loss: 6.797987]\n",
      "epoch:0 step:532 [D loss: 0.537677, acc.: 75.00%] [G loss: 4.120899]\n",
      "epoch:0 step:533 [D loss: 0.187835, acc.: 92.19%] [G loss: 5.296273]\n",
      "epoch:0 step:534 [D loss: 0.032196, acc.: 100.00%] [G loss: 5.322592]\n",
      "epoch:0 step:535 [D loss: 0.183746, acc.: 92.97%] [G loss: 4.793276]\n",
      "epoch:0 step:536 [D loss: 0.038464, acc.: 100.00%] [G loss: 4.359049]\n",
      "epoch:0 step:537 [D loss: 0.159050, acc.: 97.66%] [G loss: 5.706445]\n",
      "epoch:0 step:538 [D loss: 0.314189, acc.: 85.94%] [G loss: 4.289407]\n",
      "epoch:0 step:539 [D loss: 0.028295, acc.: 100.00%] [G loss: 3.950840]\n",
      "epoch:0 step:540 [D loss: 0.217204, acc.: 95.31%] [G loss: 6.120863]\n",
      "epoch:0 step:541 [D loss: 0.234674, acc.: 85.94%] [G loss: 3.892576]\n",
      "epoch:0 step:542 [D loss: 0.359487, acc.: 83.59%] [G loss: 7.758043]\n",
      "epoch:0 step:543 [D loss: 0.472934, acc.: 78.12%] [G loss: 6.032190]\n",
      "epoch:0 step:544 [D loss: 0.087911, acc.: 96.88%] [G loss: 4.574678]\n",
      "epoch:0 step:545 [D loss: 0.067377, acc.: 98.44%] [G loss: 4.413980]\n",
      "epoch:0 step:546 [D loss: 0.062547, acc.: 100.00%] [G loss: 4.740748]\n",
      "epoch:0 step:547 [D loss: 0.082930, acc.: 98.44%] [G loss: 5.182240]\n",
      "epoch:0 step:548 [D loss: 0.054438, acc.: 100.00%] [G loss: 4.203445]\n",
      "epoch:0 step:549 [D loss: 0.184683, acc.: 96.09%] [G loss: 5.570050]\n",
      "epoch:0 step:550 [D loss: 0.099199, acc.: 98.44%] [G loss: 4.543156]\n",
      "epoch:0 step:551 [D loss: 0.164069, acc.: 95.31%] [G loss: 5.118303]\n",
      "epoch:0 step:552 [D loss: 0.595630, acc.: 71.88%] [G loss: 6.237649]\n",
      "epoch:0 step:553 [D loss: 0.043930, acc.: 99.22%] [G loss: 6.611111]\n",
      "epoch:0 step:554 [D loss: 0.801486, acc.: 65.62%] [G loss: 5.383982]\n",
      "epoch:0 step:555 [D loss: 0.197980, acc.: 92.19%] [G loss: 5.215611]\n",
      "epoch:0 step:556 [D loss: 0.190843, acc.: 96.09%] [G loss: 6.462057]\n",
      "epoch:0 step:557 [D loss: 0.056374, acc.: 99.22%] [G loss: 6.241503]\n",
      "epoch:0 step:558 [D loss: 0.181552, acc.: 93.75%] [G loss: 6.152257]\n",
      "epoch:0 step:559 [D loss: 0.153607, acc.: 93.75%] [G loss: 5.964087]\n",
      "epoch:0 step:560 [D loss: 0.084682, acc.: 99.22%] [G loss: 4.891097]\n",
      "epoch:0 step:561 [D loss: 0.399707, acc.: 81.25%] [G loss: 9.446693]\n",
      "epoch:0 step:562 [D loss: 1.008971, acc.: 64.84%] [G loss: 4.736895]\n",
      "epoch:0 step:563 [D loss: 0.135177, acc.: 94.53%] [G loss: 4.290300]\n",
      "epoch:0 step:564 [D loss: 0.031699, acc.: 99.22%] [G loss: 3.661109]\n",
      "epoch:0 step:565 [D loss: 0.240444, acc.: 86.72%] [G loss: 4.698594]\n",
      "epoch:0 step:566 [D loss: 0.133608, acc.: 95.31%] [G loss: 3.702407]\n",
      "epoch:0 step:567 [D loss: 0.733738, acc.: 60.16%] [G loss: 6.798957]\n",
      "epoch:0 step:568 [D loss: 1.247038, acc.: 55.47%] [G loss: 3.653221]\n",
      "epoch:0 step:569 [D loss: 0.153492, acc.: 96.88%] [G loss: 3.066333]\n",
      "epoch:0 step:570 [D loss: 0.081213, acc.: 98.44%] [G loss: 3.502659]\n",
      "epoch:0 step:571 [D loss: 0.161099, acc.: 95.31%] [G loss: 3.716042]\n",
      "epoch:0 step:572 [D loss: 0.129745, acc.: 96.88%] [G loss: 4.322703]\n",
      "epoch:0 step:573 [D loss: 0.141700, acc.: 94.53%] [G loss: 3.267551]\n",
      "epoch:0 step:574 [D loss: 0.124218, acc.: 99.22%] [G loss: 4.231997]\n",
      "epoch:0 step:575 [D loss: 0.171526, acc.: 96.88%] [G loss: 5.266067]\n",
      "epoch:0 step:576 [D loss: 0.081000, acc.: 99.22%] [G loss: 4.606717]\n",
      "epoch:0 step:577 [D loss: 0.299600, acc.: 88.28%] [G loss: 6.998047]\n",
      "epoch:0 step:578 [D loss: 0.390297, acc.: 79.69%] [G loss: 5.054812]\n",
      "epoch:0 step:579 [D loss: 0.212096, acc.: 92.19%] [G loss: 5.917655]\n",
      "epoch:0 step:580 [D loss: 0.074028, acc.: 96.88%] [G loss: 5.717927]\n",
      "epoch:0 step:581 [D loss: 0.075227, acc.: 99.22%] [G loss: 3.664832]\n",
      "epoch:0 step:582 [D loss: 0.565546, acc.: 72.66%] [G loss: 7.237570]\n",
      "epoch:0 step:583 [D loss: 0.759575, acc.: 67.97%] [G loss: 3.787799]\n",
      "epoch:0 step:584 [D loss: 0.317407, acc.: 85.94%] [G loss: 3.874048]\n",
      "epoch:0 step:585 [D loss: 0.094598, acc.: 99.22%] [G loss: 4.736333]\n",
      "epoch:0 step:586 [D loss: 0.132580, acc.: 96.09%] [G loss: 3.584483]\n",
      "epoch:0 step:587 [D loss: 0.252320, acc.: 90.62%] [G loss: 5.175362]\n",
      "epoch:0 step:588 [D loss: 0.142576, acc.: 94.53%] [G loss: 5.068336]\n",
      "epoch:0 step:589 [D loss: 0.261928, acc.: 87.50%] [G loss: 4.323708]\n",
      "epoch:0 step:590 [D loss: 0.039497, acc.: 100.00%] [G loss: 4.297185]\n",
      "epoch:0 step:591 [D loss: 0.060823, acc.: 100.00%] [G loss: 4.348884]\n",
      "epoch:0 step:592 [D loss: 0.126190, acc.: 97.66%] [G loss: 5.594586]\n",
      "epoch:0 step:593 [D loss: 0.141062, acc.: 96.09%] [G loss: 4.223104]\n",
      "epoch:0 step:594 [D loss: 0.145176, acc.: 96.88%] [G loss: 5.371786]\n",
      "epoch:0 step:595 [D loss: 0.081162, acc.: 96.88%] [G loss: 4.924434]\n",
      "epoch:0 step:596 [D loss: 0.216372, acc.: 91.41%] [G loss: 5.703672]\n",
      "epoch:0 step:597 [D loss: 0.070080, acc.: 99.22%] [G loss: 4.838953]\n",
      "epoch:0 step:598 [D loss: 0.369414, acc.: 79.69%] [G loss: 8.119462]\n",
      "epoch:0 step:599 [D loss: 0.878075, acc.: 64.06%] [G loss: 3.086300]\n",
      "epoch:0 step:600 [D loss: 0.518256, acc.: 76.56%] [G loss: 6.530025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:601 [D loss: 0.050541, acc.: 99.22%] [G loss: 6.662666]\n",
      "epoch:0 step:602 [D loss: 0.213461, acc.: 89.84%] [G loss: 4.491148]\n",
      "epoch:0 step:603 [D loss: 0.152651, acc.: 94.53%] [G loss: 5.051558]\n",
      "epoch:0 step:604 [D loss: 0.067224, acc.: 99.22%] [G loss: 4.813128]\n",
      "epoch:0 step:605 [D loss: 0.364103, acc.: 83.59%] [G loss: 6.664226]\n",
      "epoch:0 step:606 [D loss: 0.280738, acc.: 88.28%] [G loss: 5.354942]\n",
      "epoch:0 step:607 [D loss: 0.105654, acc.: 97.66%] [G loss: 4.139532]\n",
      "epoch:0 step:608 [D loss: 0.162354, acc.: 92.97%] [G loss: 4.048594]\n",
      "epoch:0 step:609 [D loss: 0.252829, acc.: 93.75%] [G loss: 5.697131]\n",
      "epoch:0 step:610 [D loss: 0.883177, acc.: 54.69%] [G loss: 5.710436]\n",
      "epoch:0 step:611 [D loss: 0.078447, acc.: 98.44%] [G loss: 5.146890]\n",
      "epoch:0 step:612 [D loss: 0.072473, acc.: 98.44%] [G loss: 3.646832]\n",
      "epoch:0 step:613 [D loss: 0.224659, acc.: 86.72%] [G loss: 5.906738]\n",
      "epoch:0 step:614 [D loss: 0.162872, acc.: 92.97%] [G loss: 5.151169]\n",
      "epoch:0 step:615 [D loss: 0.114389, acc.: 96.88%] [G loss: 3.945503]\n",
      "epoch:0 step:616 [D loss: 0.124141, acc.: 96.09%] [G loss: 5.461115]\n",
      "epoch:0 step:617 [D loss: 0.352297, acc.: 86.72%] [G loss: 4.498397]\n",
      "epoch:0 step:618 [D loss: 0.044517, acc.: 99.22%] [G loss: 4.999125]\n",
      "epoch:0 step:619 [D loss: 0.094212, acc.: 97.66%] [G loss: 3.536899]\n",
      "epoch:0 step:620 [D loss: 0.224055, acc.: 90.62%] [G loss: 7.396707]\n",
      "epoch:0 step:621 [D loss: 0.206622, acc.: 88.28%] [G loss: 5.552887]\n",
      "epoch:0 step:622 [D loss: 0.051885, acc.: 99.22%] [G loss: 4.809758]\n",
      "epoch:0 step:623 [D loss: 0.085030, acc.: 99.22%] [G loss: 5.716311]\n",
      "epoch:0 step:624 [D loss: 0.034654, acc.: 100.00%] [G loss: 6.161230]\n",
      "epoch:0 step:625 [D loss: 0.216706, acc.: 94.53%] [G loss: 5.290691]\n",
      "epoch:0 step:626 [D loss: 0.156281, acc.: 95.31%] [G loss: 6.015187]\n",
      "epoch:0 step:627 [D loss: 0.544333, acc.: 73.44%] [G loss: 8.614664]\n",
      "epoch:0 step:628 [D loss: 0.577478, acc.: 73.44%] [G loss: 5.830185]\n",
      "epoch:0 step:629 [D loss: 0.031555, acc.: 100.00%] [G loss: 4.425911]\n",
      "epoch:0 step:630 [D loss: 0.072304, acc.: 98.44%] [G loss: 3.775843]\n",
      "epoch:0 step:631 [D loss: 0.166120, acc.: 93.75%] [G loss: 5.088489]\n",
      "epoch:0 step:632 [D loss: 0.055933, acc.: 100.00%] [G loss: 4.677654]\n",
      "epoch:0 step:633 [D loss: 0.542898, acc.: 71.09%] [G loss: 7.160239]\n",
      "epoch:0 step:634 [D loss: 0.659783, acc.: 71.88%] [G loss: 4.825708]\n",
      "epoch:0 step:635 [D loss: 0.084892, acc.: 98.44%] [G loss: 4.027909]\n",
      "epoch:0 step:636 [D loss: 0.103272, acc.: 97.66%] [G loss: 4.664435]\n",
      "epoch:0 step:637 [D loss: 0.084223, acc.: 99.22%] [G loss: 4.151143]\n",
      "epoch:0 step:638 [D loss: 0.239357, acc.: 90.62%] [G loss: 4.916205]\n",
      "epoch:0 step:639 [D loss: 0.224929, acc.: 92.19%] [G loss: 5.683427]\n",
      "epoch:0 step:640 [D loss: 0.087372, acc.: 97.66%] [G loss: 4.866536]\n",
      "epoch:0 step:641 [D loss: 0.128116, acc.: 96.88%] [G loss: 5.558681]\n",
      "epoch:0 step:642 [D loss: 0.141210, acc.: 96.88%] [G loss: 4.505185]\n",
      "epoch:0 step:643 [D loss: 0.082209, acc.: 98.44%] [G loss: 4.258286]\n",
      "epoch:0 step:644 [D loss: 0.104499, acc.: 96.88%] [G loss: 6.247467]\n",
      "epoch:0 step:645 [D loss: 0.359371, acc.: 85.94%] [G loss: 6.617335]\n",
      "epoch:0 step:646 [D loss: 0.029434, acc.: 99.22%] [G loss: 6.332070]\n",
      "epoch:0 step:647 [D loss: 0.063413, acc.: 99.22%] [G loss: 4.828178]\n",
      "epoch:0 step:648 [D loss: 0.561455, acc.: 71.09%] [G loss: 10.162594]\n",
      "epoch:0 step:649 [D loss: 1.658667, acc.: 50.00%] [G loss: 6.006557]\n",
      "epoch:0 step:650 [D loss: 0.013192, acc.: 100.00%] [G loss: 4.506941]\n",
      "epoch:0 step:651 [D loss: 0.017882, acc.: 99.22%] [G loss: 3.538100]\n",
      "epoch:0 step:652 [D loss: 0.045008, acc.: 100.00%] [G loss: 3.348883]\n",
      "epoch:0 step:653 [D loss: 0.111926, acc.: 96.88%] [G loss: 3.489571]\n",
      "epoch:0 step:654 [D loss: 0.057480, acc.: 100.00%] [G loss: 3.436384]\n",
      "epoch:0 step:655 [D loss: 0.241023, acc.: 88.28%] [G loss: 5.092281]\n",
      "epoch:0 step:656 [D loss: 0.277699, acc.: 86.72%] [G loss: 3.140059]\n",
      "epoch:0 step:657 [D loss: 0.226322, acc.: 91.41%] [G loss: 4.450584]\n",
      "epoch:0 step:658 [D loss: 0.140808, acc.: 96.88%] [G loss: 4.235418]\n",
      "epoch:0 step:659 [D loss: 0.367326, acc.: 87.50%] [G loss: 5.901263]\n",
      "epoch:0 step:660 [D loss: 0.176334, acc.: 92.19%] [G loss: 5.672858]\n",
      "epoch:0 step:661 [D loss: 0.125472, acc.: 96.88%] [G loss: 5.453365]\n",
      "epoch:0 step:662 [D loss: 0.086369, acc.: 98.44%] [G loss: 5.082532]\n",
      "epoch:0 step:663 [D loss: 0.153082, acc.: 93.75%] [G loss: 6.657259]\n",
      "epoch:0 step:664 [D loss: 0.124453, acc.: 95.31%] [G loss: 5.458909]\n",
      "epoch:0 step:665 [D loss: 0.805700, acc.: 52.34%] [G loss: 10.455765]\n",
      "epoch:0 step:666 [D loss: 1.205848, acc.: 57.03%] [G loss: 6.432431]\n",
      "epoch:0 step:667 [D loss: 0.012828, acc.: 100.00%] [G loss: 5.061872]\n",
      "epoch:0 step:668 [D loss: 0.027436, acc.: 99.22%] [G loss: 4.364165]\n",
      "epoch:0 step:669 [D loss: 0.042776, acc.: 99.22%] [G loss: 3.655820]\n",
      "epoch:0 step:670 [D loss: 0.084596, acc.: 100.00%] [G loss: 3.618751]\n",
      "epoch:0 step:671 [D loss: 0.088706, acc.: 99.22%] [G loss: 4.000418]\n",
      "epoch:0 step:672 [D loss: 0.062110, acc.: 99.22%] [G loss: 3.119406]\n",
      "epoch:0 step:673 [D loss: 0.482702, acc.: 78.91%] [G loss: 5.213311]\n",
      "epoch:0 step:674 [D loss: 0.487975, acc.: 77.34%] [G loss: 3.738499]\n",
      "epoch:0 step:675 [D loss: 0.149294, acc.: 99.22%] [G loss: 3.428259]\n",
      "epoch:0 step:676 [D loss: 0.075148, acc.: 99.22%] [G loss: 3.222626]\n",
      "epoch:0 step:677 [D loss: 0.566224, acc.: 69.53%] [G loss: 6.297081]\n",
      "epoch:0 step:678 [D loss: 0.573581, acc.: 73.44%] [G loss: 3.485101]\n",
      "epoch:0 step:679 [D loss: 0.152190, acc.: 95.31%] [G loss: 3.764709]\n",
      "epoch:0 step:680 [D loss: 0.114701, acc.: 98.44%] [G loss: 3.525344]\n",
      "epoch:0 step:681 [D loss: 0.096644, acc.: 100.00%] [G loss: 2.993089]\n",
      "epoch:0 step:682 [D loss: 0.297185, acc.: 88.28%] [G loss: 4.268371]\n",
      "epoch:0 step:683 [D loss: 0.288680, acc.: 89.06%] [G loss: 3.809411]\n",
      "epoch:0 step:684 [D loss: 0.144961, acc.: 97.66%] [G loss: 3.880226]\n",
      "epoch:0 step:685 [D loss: 0.263675, acc.: 90.62%] [G loss: 5.420947]\n",
      "epoch:0 step:686 [D loss: 0.205038, acc.: 89.84%] [G loss: 3.709126]\n",
      "epoch:0 step:687 [D loss: 0.069160, acc.: 100.00%] [G loss: 2.935972]\n",
      "epoch:0 step:688 [D loss: 0.102865, acc.: 98.44%] [G loss: 4.781744]\n",
      "epoch:0 step:689 [D loss: 0.106052, acc.: 97.66%] [G loss: 5.074704]\n",
      "epoch:0 step:690 [D loss: 0.323722, acc.: 85.94%] [G loss: 4.214745]\n",
      "epoch:0 step:691 [D loss: 0.345225, acc.: 85.94%] [G loss: 7.222526]\n",
      "epoch:0 step:692 [D loss: 0.730639, acc.: 67.97%] [G loss: 3.294038]\n",
      "epoch:0 step:693 [D loss: 0.419729, acc.: 73.44%] [G loss: 6.626704]\n",
      "epoch:0 step:694 [D loss: 0.224608, acc.: 89.06%] [G loss: 6.677836]\n",
      "epoch:0 step:695 [D loss: 0.162916, acc.: 94.53%] [G loss: 5.407604]\n",
      "epoch:0 step:696 [D loss: 0.035034, acc.: 100.00%] [G loss: 4.324184]\n",
      "epoch:0 step:697 [D loss: 0.095902, acc.: 98.44%] [G loss: 4.259455]\n",
      "epoch:0 step:698 [D loss: 0.080278, acc.: 99.22%] [G loss: 5.067699]\n",
      "epoch:0 step:699 [D loss: 0.048947, acc.: 99.22%] [G loss: 4.746254]\n",
      "epoch:0 step:700 [D loss: 0.134589, acc.: 96.09%] [G loss: 4.450989]\n",
      "epoch:0 step:701 [D loss: 0.170115, acc.: 93.75%] [G loss: 5.606882]\n",
      "epoch:0 step:702 [D loss: 0.124330, acc.: 95.31%] [G loss: 4.874594]\n",
      "epoch:0 step:703 [D loss: 0.175894, acc.: 95.31%] [G loss: 5.746485]\n",
      "epoch:0 step:704 [D loss: 0.055896, acc.: 98.44%] [G loss: 5.624548]\n",
      "epoch:0 step:705 [D loss: 0.326994, acc.: 84.38%] [G loss: 5.266485]\n",
      "epoch:0 step:706 [D loss: 0.139834, acc.: 93.75%] [G loss: 5.683022]\n",
      "epoch:0 step:707 [D loss: 0.220265, acc.: 92.19%] [G loss: 2.836143]\n",
      "epoch:0 step:708 [D loss: 0.267724, acc.: 89.06%] [G loss: 7.555126]\n",
      "epoch:0 step:709 [D loss: 0.586752, acc.: 73.44%] [G loss: 3.368817]\n",
      "epoch:0 step:710 [D loss: 0.093727, acc.: 97.66%] [G loss: 4.349635]\n",
      "epoch:0 step:711 [D loss: 0.096406, acc.: 96.09%] [G loss: 4.229815]\n",
      "epoch:0 step:712 [D loss: 0.101889, acc.: 97.66%] [G loss: 5.170219]\n",
      "epoch:0 step:713 [D loss: 0.098943, acc.: 99.22%] [G loss: 4.631628]\n",
      "epoch:0 step:714 [D loss: 0.563582, acc.: 69.53%] [G loss: 8.042025]\n",
      "epoch:0 step:715 [D loss: 0.554505, acc.: 71.88%] [G loss: 4.940738]\n",
      "epoch:0 step:716 [D loss: 0.064262, acc.: 98.44%] [G loss: 3.817185]\n",
      "epoch:0 step:717 [D loss: 0.148286, acc.: 93.75%] [G loss: 4.543178]\n",
      "epoch:0 step:718 [D loss: 0.032258, acc.: 100.00%] [G loss: 4.686730]\n",
      "epoch:0 step:719 [D loss: 0.096049, acc.: 97.66%] [G loss: 3.317690]\n",
      "epoch:0 step:720 [D loss: 0.257898, acc.: 92.19%] [G loss: 2.909443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:721 [D loss: 0.106655, acc.: 96.09%] [G loss: 4.680253]\n",
      "epoch:0 step:722 [D loss: 0.201041, acc.: 92.97%] [G loss: 5.627123]\n",
      "epoch:0 step:723 [D loss: 0.063484, acc.: 97.66%] [G loss: 4.896482]\n",
      "epoch:0 step:724 [D loss: 0.425893, acc.: 83.59%] [G loss: 8.665960]\n",
      "epoch:0 step:725 [D loss: 0.570222, acc.: 76.56%] [G loss: 5.705113]\n",
      "epoch:0 step:726 [D loss: 0.039547, acc.: 100.00%] [G loss: 3.954903]\n",
      "epoch:0 step:727 [D loss: 0.232415, acc.: 89.84%] [G loss: 5.675845]\n",
      "epoch:0 step:728 [D loss: 0.071429, acc.: 97.66%] [G loss: 5.925993]\n",
      "epoch:0 step:729 [D loss: 0.037320, acc.: 100.00%] [G loss: 4.487136]\n",
      "epoch:0 step:730 [D loss: 0.183337, acc.: 90.62%] [G loss: 4.718775]\n",
      "epoch:0 step:731 [D loss: 0.262528, acc.: 89.84%] [G loss: 6.014433]\n",
      "epoch:0 step:732 [D loss: 0.228584, acc.: 92.19%] [G loss: 4.416819]\n",
      "epoch:0 step:733 [D loss: 0.142770, acc.: 96.09%] [G loss: 5.809237]\n",
      "epoch:0 step:734 [D loss: 0.031506, acc.: 99.22%] [G loss: 5.429288]\n",
      "epoch:0 step:735 [D loss: 0.307313, acc.: 86.72%] [G loss: 7.462728]\n",
      "epoch:0 step:736 [D loss: 0.247959, acc.: 85.94%] [G loss: 5.390238]\n",
      "epoch:0 step:737 [D loss: 0.021138, acc.: 98.44%] [G loss: 4.479326]\n",
      "epoch:0 step:738 [D loss: 0.035665, acc.: 100.00%] [G loss: 2.924817]\n",
      "epoch:0 step:739 [D loss: 0.105634, acc.: 96.88%] [G loss: 5.578513]\n",
      "epoch:0 step:740 [D loss: 0.071515, acc.: 96.88%] [G loss: 3.686478]\n",
      "epoch:0 step:741 [D loss: 0.330812, acc.: 81.25%] [G loss: 8.465742]\n",
      "epoch:0 step:742 [D loss: 1.247123, acc.: 52.34%] [G loss: 2.112085]\n",
      "epoch:0 step:743 [D loss: 0.728638, acc.: 68.75%] [G loss: 6.993515]\n",
      "epoch:0 step:744 [D loss: 0.073560, acc.: 96.09%] [G loss: 8.069826]\n",
      "epoch:0 step:745 [D loss: 0.348273, acc.: 78.91%] [G loss: 6.614238]\n",
      "epoch:0 step:746 [D loss: 0.005061, acc.: 100.00%] [G loss: 5.731704]\n",
      "epoch:0 step:747 [D loss: 0.006497, acc.: 100.00%] [G loss: 4.751258]\n",
      "epoch:0 step:748 [D loss: 0.066044, acc.: 97.66%] [G loss: 4.268476]\n",
      "epoch:0 step:749 [D loss: 0.146103, acc.: 93.75%] [G loss: 5.012706]\n",
      "epoch:0 step:750 [D loss: 0.046676, acc.: 100.00%] [G loss: 4.011019]\n",
      "epoch:0 step:751 [D loss: 0.382004, acc.: 82.81%] [G loss: 6.950508]\n",
      "epoch:0 step:752 [D loss: 0.571572, acc.: 67.97%] [G loss: 3.590502]\n",
      "epoch:0 step:753 [D loss: 0.151590, acc.: 95.31%] [G loss: 3.886626]\n",
      "epoch:0 step:754 [D loss: 0.032307, acc.: 100.00%] [G loss: 3.733502]\n",
      "epoch:0 step:755 [D loss: 0.043300, acc.: 100.00%] [G loss: 2.901422]\n",
      "epoch:0 step:756 [D loss: 0.556337, acc.: 67.97%] [G loss: 6.773345]\n",
      "epoch:0 step:757 [D loss: 0.532582, acc.: 70.31%] [G loss: 5.283282]\n",
      "epoch:0 step:758 [D loss: 0.135684, acc.: 95.31%] [G loss: 3.796702]\n",
      "epoch:0 step:759 [D loss: 0.056319, acc.: 100.00%] [G loss: 3.223256]\n",
      "epoch:0 step:760 [D loss: 0.081117, acc.: 100.00%] [G loss: 3.237531]\n",
      "epoch:0 step:761 [D loss: 0.156446, acc.: 96.88%] [G loss: 4.951685]\n",
      "epoch:0 step:762 [D loss: 0.145754, acc.: 94.53%] [G loss: 3.975146]\n",
      "epoch:0 step:763 [D loss: 0.272541, acc.: 86.72%] [G loss: 5.398890]\n",
      "epoch:0 step:764 [D loss: 0.146442, acc.: 95.31%] [G loss: 5.683156]\n",
      "epoch:0 step:765 [D loss: 0.052457, acc.: 100.00%] [G loss: 3.589896]\n",
      "epoch:0 step:766 [D loss: 0.352929, acc.: 82.81%] [G loss: 6.920228]\n",
      "epoch:0 step:767 [D loss: 0.400880, acc.: 84.38%] [G loss: 3.632130]\n",
      "epoch:0 step:768 [D loss: 0.350249, acc.: 86.72%] [G loss: 5.518367]\n",
      "epoch:0 step:769 [D loss: 0.081350, acc.: 98.44%] [G loss: 5.216888]\n",
      "epoch:0 step:770 [D loss: 0.438822, acc.: 78.91%] [G loss: 7.963252]\n",
      "epoch:0 step:771 [D loss: 0.579070, acc.: 71.09%] [G loss: 4.578142]\n",
      "epoch:0 step:772 [D loss: 0.157475, acc.: 95.31%] [G loss: 4.806849]\n",
      "epoch:0 step:773 [D loss: 0.074139, acc.: 99.22%] [G loss: 4.073934]\n",
      "epoch:0 step:774 [D loss: 0.074466, acc.: 99.22%] [G loss: 3.976190]\n",
      "epoch:0 step:775 [D loss: 0.276711, acc.: 89.84%] [G loss: 6.783317]\n",
      "epoch:0 step:776 [D loss: 0.186263, acc.: 92.19%] [G loss: 5.415644]\n",
      "epoch:0 step:777 [D loss: 0.595327, acc.: 72.66%] [G loss: 5.861138]\n",
      "epoch:0 step:778 [D loss: 0.048561, acc.: 99.22%] [G loss: 6.751699]\n",
      "epoch:0 step:779 [D loss: 0.300540, acc.: 87.50%] [G loss: 3.151271]\n",
      "epoch:0 step:780 [D loss: 0.343948, acc.: 81.25%] [G loss: 6.250912]\n",
      "epoch:0 step:781 [D loss: 0.116982, acc.: 95.31%] [G loss: 6.809389]\n",
      "epoch:1 step:782 [D loss: 0.218572, acc.: 91.41%] [G loss: 4.018205]\n",
      "epoch:1 step:783 [D loss: 0.088426, acc.: 98.44%] [G loss: 4.086217]\n",
      "epoch:1 step:784 [D loss: 0.061477, acc.: 97.66%] [G loss: 3.584582]\n",
      "epoch:1 step:785 [D loss: 0.133833, acc.: 97.66%] [G loss: 5.044976]\n",
      "epoch:1 step:786 [D loss: 0.124565, acc.: 95.31%] [G loss: 3.744694]\n",
      "epoch:1 step:787 [D loss: 0.205740, acc.: 92.19%] [G loss: 5.791022]\n",
      "epoch:1 step:788 [D loss: 0.191033, acc.: 93.75%] [G loss: 4.714229]\n",
      "epoch:1 step:789 [D loss: 0.233323, acc.: 92.97%] [G loss: 6.076162]\n",
      "epoch:1 step:790 [D loss: 0.169572, acc.: 92.97%] [G loss: 4.615239]\n",
      "epoch:1 step:791 [D loss: 0.246034, acc.: 90.62%] [G loss: 7.001180]\n",
      "epoch:1 step:792 [D loss: 0.129697, acc.: 93.75%] [G loss: 5.464416]\n",
      "epoch:1 step:793 [D loss: 0.430730, acc.: 81.25%] [G loss: 9.322838]\n",
      "epoch:1 step:794 [D loss: 0.611705, acc.: 70.31%] [G loss: 3.365983]\n",
      "epoch:1 step:795 [D loss: 0.317195, acc.: 83.59%] [G loss: 7.443816]\n",
      "epoch:1 step:796 [D loss: 0.040414, acc.: 99.22%] [G loss: 8.559912]\n",
      "epoch:1 step:797 [D loss: 0.190934, acc.: 89.06%] [G loss: 6.194146]\n",
      "epoch:1 step:798 [D loss: 0.062927, acc.: 98.44%] [G loss: 4.089808]\n",
      "epoch:1 step:799 [D loss: 0.137741, acc.: 97.66%] [G loss: 5.742426]\n",
      "epoch:1 step:800 [D loss: 0.046489, acc.: 99.22%] [G loss: 5.981036]\n",
      "epoch:1 step:801 [D loss: 0.057536, acc.: 99.22%] [G loss: 4.535040]\n",
      "epoch:1 step:802 [D loss: 0.187066, acc.: 93.75%] [G loss: 6.299973]\n",
      "epoch:1 step:803 [D loss: 0.397891, acc.: 82.03%] [G loss: 4.088981]\n",
      "epoch:1 step:804 [D loss: 0.180747, acc.: 94.53%] [G loss: 5.822900]\n",
      "epoch:1 step:805 [D loss: 0.172177, acc.: 95.31%] [G loss: 5.254838]\n",
      "epoch:1 step:806 [D loss: 0.123763, acc.: 98.44%] [G loss: 6.366977]\n",
      "epoch:1 step:807 [D loss: 0.215494, acc.: 92.19%] [G loss: 4.875552]\n",
      "epoch:1 step:808 [D loss: 0.446869, acc.: 78.12%] [G loss: 9.506016]\n",
      "epoch:1 step:809 [D loss: 1.081198, acc.: 57.81%] [G loss: 4.330832]\n",
      "epoch:1 step:810 [D loss: 0.211134, acc.: 88.28%] [G loss: 4.342613]\n",
      "epoch:1 step:811 [D loss: 0.136012, acc.: 94.53%] [G loss: 5.136407]\n",
      "epoch:1 step:812 [D loss: 0.131608, acc.: 96.09%] [G loss: 3.884844]\n",
      "epoch:1 step:813 [D loss: 0.584236, acc.: 69.53%] [G loss: 6.508397]\n",
      "epoch:1 step:814 [D loss: 0.613856, acc.: 75.00%] [G loss: 4.093418]\n",
      "epoch:1 step:815 [D loss: 0.135221, acc.: 97.66%] [G loss: 3.598291]\n",
      "epoch:1 step:816 [D loss: 0.100654, acc.: 98.44%] [G loss: 3.466312]\n",
      "epoch:1 step:817 [D loss: 0.172591, acc.: 95.31%] [G loss: 3.578055]\n",
      "epoch:1 step:818 [D loss: 0.097907, acc.: 100.00%] [G loss: 4.012389]\n",
      "epoch:1 step:819 [D loss: 0.751578, acc.: 57.03%] [G loss: 5.955263]\n",
      "epoch:1 step:820 [D loss: 0.148946, acc.: 94.53%] [G loss: 6.417024]\n",
      "epoch:1 step:821 [D loss: 0.128807, acc.: 94.53%] [G loss: 4.369476]\n",
      "epoch:1 step:822 [D loss: 0.164671, acc.: 93.75%] [G loss: 3.962757]\n",
      "epoch:1 step:823 [D loss: 0.109160, acc.: 98.44%] [G loss: 3.563279]\n",
      "epoch:1 step:824 [D loss: 0.437848, acc.: 78.12%] [G loss: 4.686335]\n",
      "epoch:1 step:825 [D loss: 0.135277, acc.: 95.31%] [G loss: 4.993351]\n",
      "epoch:1 step:826 [D loss: 0.597117, acc.: 65.62%] [G loss: 4.705904]\n",
      "epoch:1 step:827 [D loss: 0.131748, acc.: 92.97%] [G loss: 4.898952]\n",
      "epoch:1 step:828 [D loss: 0.069556, acc.: 99.22%] [G loss: 3.508917]\n",
      "epoch:1 step:829 [D loss: 0.179451, acc.: 96.09%] [G loss: 4.624299]\n",
      "epoch:1 step:830 [D loss: 0.114493, acc.: 97.66%] [G loss: 3.982889]\n",
      "epoch:1 step:831 [D loss: 0.153424, acc.: 96.09%] [G loss: 4.647031]\n",
      "epoch:1 step:832 [D loss: 0.062705, acc.: 99.22%] [G loss: 4.423272]\n",
      "epoch:1 step:833 [D loss: 0.235442, acc.: 92.97%] [G loss: 5.373685]\n",
      "epoch:1 step:834 [D loss: 0.119629, acc.: 95.31%] [G loss: 4.272263]\n",
      "epoch:1 step:835 [D loss: 0.173762, acc.: 93.75%] [G loss: 5.899555]\n",
      "epoch:1 step:836 [D loss: 0.090491, acc.: 96.88%] [G loss: 5.992369]\n",
      "epoch:1 step:837 [D loss: 0.045200, acc.: 99.22%] [G loss: 4.174692]\n",
      "epoch:1 step:838 [D loss: 0.788848, acc.: 67.97%] [G loss: 9.507788]\n",
      "epoch:1 step:839 [D loss: 1.083997, acc.: 61.72%] [G loss: 6.857252]\n",
      "epoch:1 step:840 [D loss: 0.046453, acc.: 99.22%] [G loss: 5.540281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:841 [D loss: 0.028845, acc.: 100.00%] [G loss: 4.992877]\n",
      "epoch:1 step:842 [D loss: 0.034467, acc.: 100.00%] [G loss: 3.748138]\n",
      "epoch:1 step:843 [D loss: 0.182446, acc.: 92.97%] [G loss: 4.510256]\n",
      "epoch:1 step:844 [D loss: 0.083361, acc.: 99.22%] [G loss: 3.935336]\n",
      "epoch:1 step:845 [D loss: 0.564429, acc.: 72.66%] [G loss: 6.385739]\n",
      "epoch:1 step:846 [D loss: 0.428555, acc.: 79.69%] [G loss: 4.994940]\n",
      "epoch:1 step:847 [D loss: 0.101770, acc.: 97.66%] [G loss: 3.211549]\n",
      "epoch:1 step:848 [D loss: 0.140044, acc.: 96.88%] [G loss: 2.921957]\n",
      "epoch:1 step:849 [D loss: 0.161164, acc.: 93.75%] [G loss: 2.610017]\n",
      "epoch:1 step:850 [D loss: 0.186008, acc.: 93.75%] [G loss: 3.149382]\n",
      "epoch:1 step:851 [D loss: 0.092467, acc.: 98.44%] [G loss: 2.852398]\n",
      "epoch:1 step:852 [D loss: 1.055438, acc.: 53.91%] [G loss: 7.311701]\n",
      "epoch:1 step:853 [D loss: 0.739691, acc.: 67.19%] [G loss: 5.300930]\n",
      "epoch:1 step:854 [D loss: 0.068110, acc.: 99.22%] [G loss: 4.149711]\n",
      "epoch:1 step:855 [D loss: 0.101040, acc.: 98.44%] [G loss: 3.736131]\n",
      "epoch:1 step:856 [D loss: 0.091099, acc.: 96.88%] [G loss: 3.813234]\n",
      "epoch:1 step:857 [D loss: 0.078644, acc.: 99.22%] [G loss: 2.771787]\n",
      "epoch:1 step:858 [D loss: 0.237530, acc.: 91.41%] [G loss: 4.241510]\n",
      "epoch:1 step:859 [D loss: 0.132262, acc.: 95.31%] [G loss: 3.944306]\n",
      "epoch:1 step:860 [D loss: 0.820216, acc.: 56.25%] [G loss: 4.794865]\n",
      "epoch:1 step:861 [D loss: 0.129006, acc.: 96.88%] [G loss: 5.160763]\n",
      "epoch:1 step:862 [D loss: 0.309499, acc.: 82.81%] [G loss: 2.594241]\n",
      "epoch:1 step:863 [D loss: 0.239353, acc.: 90.62%] [G loss: 3.313096]\n",
      "epoch:1 step:864 [D loss: 0.055244, acc.: 100.00%] [G loss: 4.218748]\n",
      "epoch:1 step:865 [D loss: 0.157893, acc.: 95.31%] [G loss: 3.306420]\n",
      "epoch:1 step:866 [D loss: 0.157691, acc.: 93.75%] [G loss: 3.136980]\n",
      "epoch:1 step:867 [D loss: 0.135731, acc.: 99.22%] [G loss: 4.196864]\n",
      "epoch:1 step:868 [D loss: 0.229308, acc.: 92.19%] [G loss: 3.142581]\n",
      "epoch:1 step:869 [D loss: 0.192128, acc.: 92.97%] [G loss: 4.011461]\n",
      "epoch:1 step:870 [D loss: 1.020193, acc.: 50.00%] [G loss: 5.210679]\n",
      "epoch:1 step:871 [D loss: 0.373387, acc.: 80.47%] [G loss: 4.286071]\n",
      "epoch:1 step:872 [D loss: 0.098881, acc.: 96.09%] [G loss: 2.187401]\n",
      "epoch:1 step:873 [D loss: 0.174547, acc.: 93.75%] [G loss: 3.523246]\n",
      "epoch:1 step:874 [D loss: 0.059808, acc.: 100.00%] [G loss: 4.107503]\n",
      "epoch:1 step:875 [D loss: 0.115855, acc.: 98.44%] [G loss: 3.243366]\n",
      "epoch:1 step:876 [D loss: 0.321821, acc.: 82.81%] [G loss: 5.341542]\n",
      "epoch:1 step:877 [D loss: 0.100607, acc.: 97.66%] [G loss: 6.145885]\n",
      "epoch:1 step:878 [D loss: 0.078150, acc.: 96.88%] [G loss: 3.922630]\n",
      "epoch:1 step:879 [D loss: 0.077493, acc.: 99.22%] [G loss: 4.343499]\n",
      "epoch:1 step:880 [D loss: 0.070637, acc.: 98.44%] [G loss: 3.243229]\n",
      "epoch:1 step:881 [D loss: 0.123749, acc.: 99.22%] [G loss: 5.016414]\n",
      "epoch:1 step:882 [D loss: 0.111624, acc.: 98.44%] [G loss: 4.864671]\n",
      "epoch:1 step:883 [D loss: 0.394922, acc.: 82.03%] [G loss: 5.922742]\n",
      "epoch:1 step:884 [D loss: 0.153820, acc.: 94.53%] [G loss: 4.848190]\n",
      "epoch:1 step:885 [D loss: 0.420058, acc.: 77.34%] [G loss: 7.743307]\n",
      "epoch:1 step:886 [D loss: 0.360173, acc.: 81.25%] [G loss: 4.973270]\n",
      "epoch:1 step:887 [D loss: 0.121685, acc.: 96.09%] [G loss: 4.331133]\n",
      "epoch:1 step:888 [D loss: 0.120068, acc.: 97.66%] [G loss: 4.533094]\n",
      "epoch:1 step:889 [D loss: 0.184503, acc.: 92.19%] [G loss: 4.828039]\n",
      "epoch:1 step:890 [D loss: 0.540889, acc.: 77.34%] [G loss: 6.881378]\n",
      "epoch:1 step:891 [D loss: 0.632024, acc.: 70.31%] [G loss: 2.106493]\n",
      "epoch:1 step:892 [D loss: 0.194454, acc.: 91.41%] [G loss: 4.480730]\n",
      "epoch:1 step:893 [D loss: 0.042910, acc.: 100.00%] [G loss: 4.860175]\n",
      "epoch:1 step:894 [D loss: 0.078205, acc.: 98.44%] [G loss: 3.747135]\n",
      "epoch:1 step:895 [D loss: 0.091222, acc.: 99.22%] [G loss: 3.446331]\n",
      "epoch:1 step:896 [D loss: 0.057315, acc.: 99.22%] [G loss: 2.539723]\n",
      "epoch:1 step:897 [D loss: 0.235335, acc.: 91.41%] [G loss: 5.027698]\n",
      "epoch:1 step:898 [D loss: 0.299277, acc.: 85.94%] [G loss: 2.812453]\n",
      "epoch:1 step:899 [D loss: 0.229100, acc.: 93.75%] [G loss: 3.821965]\n",
      "epoch:1 step:900 [D loss: 0.069912, acc.: 98.44%] [G loss: 4.374492]\n",
      "epoch:1 step:901 [D loss: 0.081529, acc.: 99.22%] [G loss: 3.157477]\n",
      "epoch:1 step:902 [D loss: 0.430044, acc.: 77.34%] [G loss: 6.375281]\n",
      "epoch:1 step:903 [D loss: 0.626612, acc.: 76.56%] [G loss: 3.599286]\n",
      "epoch:1 step:904 [D loss: 0.111541, acc.: 98.44%] [G loss: 3.761598]\n",
      "epoch:1 step:905 [D loss: 0.079804, acc.: 98.44%] [G loss: 4.015777]\n",
      "epoch:1 step:906 [D loss: 0.160134, acc.: 94.53%] [G loss: 4.297936]\n",
      "epoch:1 step:907 [D loss: 0.081472, acc.: 99.22%] [G loss: 3.704011]\n",
      "epoch:1 step:908 [D loss: 0.174676, acc.: 90.62%] [G loss: 5.247857]\n",
      "epoch:1 step:909 [D loss: 0.254662, acc.: 88.28%] [G loss: 3.255801]\n",
      "epoch:1 step:910 [D loss: 0.153240, acc.: 94.53%] [G loss: 5.311174]\n",
      "epoch:1 step:911 [D loss: 0.067979, acc.: 97.66%] [G loss: 5.052562]\n",
      "epoch:1 step:912 [D loss: 0.066949, acc.: 98.44%] [G loss: 3.457994]\n",
      "epoch:1 step:913 [D loss: 0.068790, acc.: 100.00%] [G loss: 3.576978]\n",
      "epoch:1 step:914 [D loss: 0.153106, acc.: 96.88%] [G loss: 5.516584]\n",
      "epoch:1 step:915 [D loss: 0.095121, acc.: 96.09%] [G loss: 4.711999]\n",
      "epoch:1 step:916 [D loss: 0.148903, acc.: 95.31%] [G loss: 3.553566]\n",
      "epoch:1 step:917 [D loss: 0.030071, acc.: 100.00%] [G loss: 3.803208]\n",
      "epoch:1 step:918 [D loss: 0.290380, acc.: 84.38%] [G loss: 9.950463]\n",
      "epoch:1 step:919 [D loss: 1.125467, acc.: 59.38%] [G loss: 1.648041]\n",
      "epoch:1 step:920 [D loss: 0.703300, acc.: 69.53%] [G loss: 8.769203]\n",
      "epoch:1 step:921 [D loss: 0.237333, acc.: 85.94%] [G loss: 9.129448]\n",
      "epoch:1 step:922 [D loss: 0.227951, acc.: 89.06%] [G loss: 6.982477]\n",
      "epoch:1 step:923 [D loss: 0.007824, acc.: 100.00%] [G loss: 5.025625]\n",
      "epoch:1 step:924 [D loss: 0.027245, acc.: 99.22%] [G loss: 4.390796]\n",
      "epoch:1 step:925 [D loss: 0.047633, acc.: 100.00%] [G loss: 2.647862]\n",
      "epoch:1 step:926 [D loss: 0.153564, acc.: 94.53%] [G loss: 3.673332]\n",
      "epoch:1 step:927 [D loss: 0.080929, acc.: 99.22%] [G loss: 4.236066]\n",
      "epoch:1 step:928 [D loss: 0.335878, acc.: 85.94%] [G loss: 6.008724]\n",
      "epoch:1 step:929 [D loss: 1.564292, acc.: 35.94%] [G loss: 4.267887]\n",
      "epoch:1 step:930 [D loss: 0.095262, acc.: 96.88%] [G loss: 4.107822]\n",
      "epoch:1 step:931 [D loss: 0.106597, acc.: 98.44%] [G loss: 3.551912]\n",
      "epoch:1 step:932 [D loss: 0.096960, acc.: 97.66%] [G loss: 3.068426]\n",
      "epoch:1 step:933 [D loss: 0.253906, acc.: 88.28%] [G loss: 4.930296]\n",
      "epoch:1 step:934 [D loss: 0.445744, acc.: 77.34%] [G loss: 1.789957]\n",
      "epoch:1 step:935 [D loss: 0.250962, acc.: 85.16%] [G loss: 4.347447]\n",
      "epoch:1 step:936 [D loss: 0.042897, acc.: 100.00%] [G loss: 4.918431]\n",
      "epoch:1 step:937 [D loss: 0.338351, acc.: 83.59%] [G loss: 3.083767]\n",
      "epoch:1 step:938 [D loss: 0.254921, acc.: 88.28%] [G loss: 5.150495]\n",
      "epoch:1 step:939 [D loss: 0.273644, acc.: 86.72%] [G loss: 3.490604]\n",
      "epoch:1 step:940 [D loss: 0.148249, acc.: 95.31%] [G loss: 3.447901]\n",
      "epoch:1 step:941 [D loss: 0.099812, acc.: 96.88%] [G loss: 3.742779]\n",
      "epoch:1 step:942 [D loss: 0.237104, acc.: 93.75%] [G loss: 5.685055]\n",
      "epoch:1 step:943 [D loss: 0.309680, acc.: 88.28%] [G loss: 4.443640]\n",
      "epoch:1 step:944 [D loss: 0.140715, acc.: 96.09%] [G loss: 4.946950]\n",
      "epoch:1 step:945 [D loss: 0.185656, acc.: 95.31%] [G loss: 6.331561]\n",
      "epoch:1 step:946 [D loss: 0.326163, acc.: 87.50%] [G loss: 4.989359]\n",
      "epoch:1 step:947 [D loss: 0.025229, acc.: 100.00%] [G loss: 5.080373]\n",
      "epoch:1 step:948 [D loss: 0.244351, acc.: 90.62%] [G loss: 7.596510]\n",
      "epoch:1 step:949 [D loss: 0.298058, acc.: 85.94%] [G loss: 3.048225]\n",
      "epoch:1 step:950 [D loss: 0.737759, acc.: 67.97%] [G loss: 8.787357]\n",
      "epoch:1 step:951 [D loss: 1.232809, acc.: 54.69%] [G loss: 4.377463]\n",
      "epoch:1 step:952 [D loss: 0.226511, acc.: 92.19%] [G loss: 3.141185]\n",
      "epoch:1 step:953 [D loss: 0.117116, acc.: 97.66%] [G loss: 3.447554]\n",
      "epoch:1 step:954 [D loss: 0.118219, acc.: 97.66%] [G loss: 3.536343]\n",
      "epoch:1 step:955 [D loss: 0.125317, acc.: 97.66%] [G loss: 3.916904]\n",
      "epoch:1 step:956 [D loss: 0.133327, acc.: 96.88%] [G loss: 3.784121]\n",
      "epoch:1 step:957 [D loss: 0.224041, acc.: 93.75%] [G loss: 3.957836]\n",
      "epoch:1 step:958 [D loss: 0.203784, acc.: 92.19%] [G loss: 3.216212]\n",
      "epoch:1 step:959 [D loss: 0.198247, acc.: 93.75%] [G loss: 4.539647]\n",
      "epoch:1 step:960 [D loss: 0.225723, acc.: 92.19%] [G loss: 2.942832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:961 [D loss: 0.236800, acc.: 92.19%] [G loss: 4.539503]\n",
      "epoch:1 step:962 [D loss: 0.182461, acc.: 92.97%] [G loss: 3.479910]\n",
      "epoch:1 step:963 [D loss: 0.234026, acc.: 92.97%] [G loss: 4.256297]\n",
      "epoch:1 step:964 [D loss: 0.123126, acc.: 96.88%] [G loss: 3.473712]\n",
      "epoch:1 step:965 [D loss: 0.174597, acc.: 95.31%] [G loss: 4.085012]\n",
      "epoch:1 step:966 [D loss: 0.128836, acc.: 94.53%] [G loss: 2.662175]\n",
      "epoch:1 step:967 [D loss: 0.353405, acc.: 84.38%] [G loss: 5.570563]\n",
      "epoch:1 step:968 [D loss: 0.258172, acc.: 88.28%] [G loss: 3.993701]\n",
      "epoch:1 step:969 [D loss: 0.107038, acc.: 98.44%] [G loss: 2.023351]\n",
      "epoch:1 step:970 [D loss: 0.231153, acc.: 89.06%] [G loss: 4.299519]\n",
      "epoch:1 step:971 [D loss: 0.160356, acc.: 93.75%] [G loss: 3.511095]\n",
      "epoch:1 step:972 [D loss: 0.076726, acc.: 98.44%] [G loss: 2.570961]\n",
      "epoch:1 step:973 [D loss: 0.127057, acc.: 96.09%] [G loss: 3.037711]\n",
      "epoch:1 step:974 [D loss: 0.234931, acc.: 89.06%] [G loss: 1.482360]\n",
      "epoch:1 step:975 [D loss: 0.140325, acc.: 96.09%] [G loss: 1.571185]\n",
      "epoch:1 step:976 [D loss: 0.133015, acc.: 97.66%] [G loss: 2.269071]\n",
      "epoch:1 step:977 [D loss: 0.413600, acc.: 77.34%] [G loss: 7.684516]\n",
      "epoch:1 step:978 [D loss: 1.050356, acc.: 57.03%] [G loss: 2.405461]\n",
      "epoch:1 step:979 [D loss: 0.165045, acc.: 92.97%] [G loss: 4.040385]\n",
      "epoch:1 step:980 [D loss: 0.047610, acc.: 98.44%] [G loss: 4.406983]\n",
      "epoch:1 step:981 [D loss: 0.104456, acc.: 98.44%] [G loss: 3.633132]\n",
      "epoch:1 step:982 [D loss: 0.084584, acc.: 98.44%] [G loss: 3.871226]\n",
      "epoch:1 step:983 [D loss: 0.885634, acc.: 55.47%] [G loss: 7.045658]\n",
      "epoch:1 step:984 [D loss: 0.393586, acc.: 81.25%] [G loss: 5.901093]\n",
      "epoch:1 step:985 [D loss: 0.090994, acc.: 97.66%] [G loss: 4.483037]\n",
      "epoch:1 step:986 [D loss: 0.046370, acc.: 100.00%] [G loss: 3.454931]\n",
      "epoch:1 step:987 [D loss: 0.119675, acc.: 97.66%] [G loss: 4.391108]\n",
      "epoch:1 step:988 [D loss: 0.054281, acc.: 100.00%] [G loss: 4.418701]\n",
      "epoch:1 step:989 [D loss: 0.160148, acc.: 97.66%] [G loss: 3.501936]\n",
      "epoch:1 step:990 [D loss: 0.152294, acc.: 96.88%] [G loss: 4.977884]\n",
      "epoch:1 step:991 [D loss: 0.139471, acc.: 96.09%] [G loss: 4.325930]\n",
      "epoch:1 step:992 [D loss: 0.060471, acc.: 100.00%] [G loss: 3.936211]\n",
      "epoch:1 step:993 [D loss: 0.073893, acc.: 100.00%] [G loss: 4.812453]\n",
      "epoch:1 step:994 [D loss: 0.095568, acc.: 97.66%] [G loss: 5.245997]\n",
      "epoch:1 step:995 [D loss: 0.302175, acc.: 88.28%] [G loss: 4.701816]\n",
      "epoch:1 step:996 [D loss: 0.027059, acc.: 100.00%] [G loss: 4.664150]\n",
      "epoch:1 step:997 [D loss: 0.055060, acc.: 99.22%] [G loss: 4.547128]\n",
      "epoch:1 step:998 [D loss: 0.074185, acc.: 100.00%] [G loss: 3.713498]\n",
      "epoch:1 step:999 [D loss: 0.196171, acc.: 95.31%] [G loss: 3.392498]\n",
      "epoch:1 step:1000 [D loss: 0.103271, acc.: 97.66%] [G loss: 2.297158]\n",
      "epoch:1 step:1001 [D loss: 0.082595, acc.: 98.44%] [G loss: 1.187618]\n",
      "epoch:1 step:1002 [D loss: 0.080383, acc.: 98.44%] [G loss: 2.222064]\n",
      "epoch:1 step:1003 [D loss: 0.200253, acc.: 93.75%] [G loss: 3.058327]\n",
      "epoch:1 step:1004 [D loss: 0.071900, acc.: 98.44%] [G loss: 0.944317]\n",
      "epoch:1 step:1005 [D loss: 0.045544, acc.: 100.00%] [G loss: 0.975767]\n",
      "epoch:1 step:1006 [D loss: 0.082380, acc.: 100.00%] [G loss: 0.425627]\n",
      "epoch:1 step:1007 [D loss: 0.098438, acc.: 98.44%] [G loss: 2.295841]\n",
      "epoch:1 step:1008 [D loss: 0.046901, acc.: 100.00%] [G loss: 2.510111]\n",
      "epoch:1 step:1009 [D loss: 0.501570, acc.: 75.00%] [G loss: 12.938070]\n",
      "epoch:1 step:1010 [D loss: 3.508132, acc.: 50.00%] [G loss: 2.648984]\n",
      "epoch:1 step:1011 [D loss: 0.342020, acc.: 85.94%] [G loss: 4.069551]\n",
      "epoch:1 step:1012 [D loss: 0.017614, acc.: 100.00%] [G loss: 4.749268]\n",
      "epoch:1 step:1013 [D loss: 0.022618, acc.: 100.00%] [G loss: 4.278950]\n",
      "epoch:1 step:1014 [D loss: 0.030306, acc.: 100.00%] [G loss: 3.367424]\n",
      "epoch:1 step:1015 [D loss: 0.139714, acc.: 97.66%] [G loss: 4.107368]\n",
      "epoch:1 step:1016 [D loss: 0.070882, acc.: 98.44%] [G loss: 4.172565]\n",
      "epoch:1 step:1017 [D loss: 0.100864, acc.: 100.00%] [G loss: 4.107315]\n",
      "epoch:1 step:1018 [D loss: 0.159076, acc.: 97.66%] [G loss: 4.382167]\n",
      "epoch:1 step:1019 [D loss: 0.247983, acc.: 91.41%] [G loss: 4.461781]\n",
      "epoch:1 step:1020 [D loss: 0.127444, acc.: 97.66%] [G loss: 3.915444]\n",
      "epoch:1 step:1021 [D loss: 0.052621, acc.: 100.00%] [G loss: 3.464535]\n",
      "epoch:1 step:1022 [D loss: 0.057494, acc.: 100.00%] [G loss: 3.001451]\n",
      "epoch:1 step:1023 [D loss: 0.048799, acc.: 100.00%] [G loss: 3.652748]\n",
      "epoch:1 step:1024 [D loss: 0.069814, acc.: 96.88%] [G loss: 1.698034]\n",
      "epoch:1 step:1025 [D loss: 0.244230, acc.: 89.06%] [G loss: 9.106944]\n",
      "epoch:1 step:1026 [D loss: 0.873462, acc.: 65.62%] [G loss: 5.268613]\n",
      "epoch:1 step:1027 [D loss: 0.006361, acc.: 100.00%] [G loss: 3.461507]\n",
      "epoch:1 step:1028 [D loss: 0.023491, acc.: 99.22%] [G loss: 1.438034]\n",
      "epoch:1 step:1029 [D loss: 0.212448, acc.: 84.38%] [G loss: 4.792459]\n",
      "epoch:1 step:1030 [D loss: 0.003633, acc.: 100.00%] [G loss: 5.537149]\n",
      "epoch:1 step:1031 [D loss: 0.097315, acc.: 98.44%] [G loss: 1.227604]\n",
      "epoch:1 step:1032 [D loss: 0.120260, acc.: 96.88%] [G loss: 2.712543]\n",
      "epoch:1 step:1033 [D loss: 0.052009, acc.: 99.22%] [G loss: 1.302167]\n",
      "epoch:1 step:1034 [D loss: 0.185436, acc.: 96.09%] [G loss: 1.306657]\n",
      "epoch:1 step:1035 [D loss: 0.049161, acc.: 99.22%] [G loss: 0.566331]\n",
      "epoch:1 step:1036 [D loss: 0.089285, acc.: 97.66%] [G loss: 0.982710]\n",
      "epoch:1 step:1037 [D loss: 0.152498, acc.: 95.31%] [G loss: 4.154922]\n",
      "epoch:1 step:1038 [D loss: 0.337236, acc.: 84.38%] [G loss: 0.443816]\n",
      "epoch:1 step:1039 [D loss: 0.043430, acc.: 100.00%] [G loss: 0.201678]\n",
      "epoch:1 step:1040 [D loss: 0.124247, acc.: 96.09%] [G loss: 5.137035]\n",
      "epoch:1 step:1041 [D loss: 0.136982, acc.: 92.97%] [G loss: 3.346869]\n",
      "epoch:1 step:1042 [D loss: 0.038911, acc.: 100.00%] [G loss: 0.858019]\n",
      "epoch:1 step:1043 [D loss: 0.081000, acc.: 99.22%] [G loss: 1.233260]\n",
      "epoch:1 step:1044 [D loss: 0.106404, acc.: 97.66%] [G loss: 0.951328]\n",
      "epoch:1 step:1045 [D loss: 0.274842, acc.: 88.28%] [G loss: 6.150006]\n",
      "epoch:1 step:1046 [D loss: 0.175164, acc.: 93.75%] [G loss: 5.340830]\n",
      "epoch:1 step:1047 [D loss: 0.065467, acc.: 98.44%] [G loss: 2.905809]\n",
      "epoch:1 step:1048 [D loss: 0.631956, acc.: 67.19%] [G loss: 7.883788]\n",
      "epoch:1 step:1049 [D loss: 0.483441, acc.: 75.00%] [G loss: 7.184371]\n",
      "epoch:1 step:1050 [D loss: 0.166394, acc.: 94.53%] [G loss: 5.290190]\n",
      "epoch:1 step:1051 [D loss: 0.091375, acc.: 98.44%] [G loss: 5.276985]\n",
      "epoch:1 step:1052 [D loss: 0.008824, acc.: 100.00%] [G loss: 5.283308]\n",
      "epoch:1 step:1053 [D loss: 0.021286, acc.: 100.00%] [G loss: 3.894051]\n",
      "epoch:1 step:1054 [D loss: 0.055805, acc.: 99.22%] [G loss: 3.790917]\n",
      "epoch:1 step:1055 [D loss: 0.091887, acc.: 100.00%] [G loss: 4.866103]\n",
      "epoch:1 step:1056 [D loss: 0.070358, acc.: 98.44%] [G loss: 4.286972]\n",
      "epoch:1 step:1057 [D loss: 0.047817, acc.: 100.00%] [G loss: 4.398427]\n",
      "epoch:1 step:1058 [D loss: 0.060628, acc.: 99.22%] [G loss: 3.494835]\n",
      "epoch:1 step:1059 [D loss: 0.023485, acc.: 100.00%] [G loss: 3.990092]\n",
      "epoch:1 step:1060 [D loss: 0.089056, acc.: 99.22%] [G loss: 5.044850]\n",
      "epoch:1 step:1061 [D loss: 0.052185, acc.: 99.22%] [G loss: 4.198126]\n",
      "epoch:1 step:1062 [D loss: 0.175896, acc.: 95.31%] [G loss: 6.728033]\n",
      "epoch:1 step:1063 [D loss: 0.098439, acc.: 96.09%] [G loss: 6.270629]\n",
      "epoch:1 step:1064 [D loss: 0.039700, acc.: 100.00%] [G loss: 4.110668]\n",
      "epoch:1 step:1065 [D loss: 0.135993, acc.: 96.09%] [G loss: 6.809232]\n",
      "epoch:1 step:1066 [D loss: 0.097628, acc.: 96.09%] [G loss: 5.480517]\n",
      "epoch:1 step:1067 [D loss: 0.092120, acc.: 98.44%] [G loss: 4.439878]\n",
      "epoch:1 step:1068 [D loss: 0.078635, acc.: 99.22%] [G loss: 3.663345]\n",
      "epoch:1 step:1069 [D loss: 0.129862, acc.: 96.09%] [G loss: 7.141610]\n",
      "epoch:1 step:1070 [D loss: 0.147362, acc.: 94.53%] [G loss: 4.643451]\n",
      "epoch:1 step:1071 [D loss: 0.076052, acc.: 96.88%] [G loss: 5.384432]\n",
      "epoch:1 step:1072 [D loss: 0.004505, acc.: 100.00%] [G loss: 6.084229]\n",
      "epoch:1 step:1073 [D loss: 0.090923, acc.: 98.44%] [G loss: 5.487361]\n",
      "epoch:1 step:1074 [D loss: 0.049019, acc.: 99.22%] [G loss: 4.836834]\n",
      "epoch:1 step:1075 [D loss: 0.061646, acc.: 99.22%] [G loss: 5.297280]\n",
      "epoch:1 step:1076 [D loss: 1.564103, acc.: 45.31%] [G loss: 11.984489]\n",
      "epoch:1 step:1077 [D loss: 2.786894, acc.: 50.00%] [G loss: 5.176692]\n",
      "epoch:1 step:1078 [D loss: 0.263506, acc.: 92.19%] [G loss: 3.080892]\n",
      "epoch:1 step:1079 [D loss: 0.047854, acc.: 100.00%] [G loss: 2.782054]\n",
      "epoch:1 step:1080 [D loss: 0.095538, acc.: 98.44%] [G loss: 2.773654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1081 [D loss: 0.067128, acc.: 100.00%] [G loss: 3.171669]\n",
      "epoch:1 step:1082 [D loss: 0.146433, acc.: 96.88%] [G loss: 4.244571]\n",
      "epoch:1 step:1083 [D loss: 0.140496, acc.: 95.31%] [G loss: 3.545707]\n",
      "epoch:1 step:1084 [D loss: 0.378081, acc.: 83.59%] [G loss: 4.680037]\n",
      "epoch:1 step:1085 [D loss: 0.133333, acc.: 96.09%] [G loss: 4.803170]\n",
      "epoch:1 step:1086 [D loss: 0.644928, acc.: 71.09%] [G loss: 5.089334]\n",
      "epoch:1 step:1087 [D loss: 0.232332, acc.: 89.84%] [G loss: 3.834325]\n",
      "epoch:1 step:1088 [D loss: 0.172720, acc.: 96.09%] [G loss: 3.385782]\n",
      "epoch:1 step:1089 [D loss: 0.138381, acc.: 95.31%] [G loss: 4.484625]\n",
      "epoch:1 step:1090 [D loss: 0.195291, acc.: 92.97%] [G loss: 4.053306]\n",
      "epoch:1 step:1091 [D loss: 0.078785, acc.: 99.22%] [G loss: 4.531981]\n",
      "epoch:1 step:1092 [D loss: 0.352501, acc.: 88.28%] [G loss: 4.050120]\n",
      "epoch:1 step:1093 [D loss: 0.202329, acc.: 92.97%] [G loss: 4.763825]\n",
      "epoch:1 step:1094 [D loss: 0.476500, acc.: 82.03%] [G loss: 5.845898]\n",
      "epoch:1 step:1095 [D loss: 0.197654, acc.: 94.53%] [G loss: 5.282455]\n",
      "epoch:1 step:1096 [D loss: 0.125354, acc.: 96.88%] [G loss: 3.503751]\n",
      "epoch:1 step:1097 [D loss: 0.129153, acc.: 96.88%] [G loss: 4.639351]\n",
      "epoch:1 step:1098 [D loss: 0.032516, acc.: 100.00%] [G loss: 4.938766]\n",
      "epoch:1 step:1099 [D loss: 0.087615, acc.: 98.44%] [G loss: 4.126135]\n",
      "epoch:1 step:1100 [D loss: 0.199107, acc.: 90.62%] [G loss: 5.272083]\n",
      "epoch:1 step:1101 [D loss: 0.254315, acc.: 90.62%] [G loss: 4.304277]\n",
      "epoch:1 step:1102 [D loss: 0.138024, acc.: 96.88%] [G loss: 4.599547]\n",
      "epoch:1 step:1103 [D loss: 0.040972, acc.: 100.00%] [G loss: 4.256478]\n",
      "epoch:1 step:1104 [D loss: 0.154025, acc.: 96.09%] [G loss: 5.948466]\n",
      "epoch:1 step:1105 [D loss: 0.119719, acc.: 96.88%] [G loss: 4.478029]\n",
      "epoch:1 step:1106 [D loss: 0.105092, acc.: 98.44%] [G loss: 3.608007]\n",
      "epoch:1 step:1107 [D loss: 0.163484, acc.: 94.53%] [G loss: 6.028089]\n",
      "epoch:1 step:1108 [D loss: 0.203323, acc.: 91.41%] [G loss: 4.699845]\n",
      "epoch:1 step:1109 [D loss: 0.053318, acc.: 100.00%] [G loss: 4.151205]\n",
      "epoch:1 step:1110 [D loss: 0.115251, acc.: 98.44%] [G loss: 5.187318]\n",
      "epoch:1 step:1111 [D loss: 0.135199, acc.: 97.66%] [G loss: 5.261968]\n",
      "epoch:1 step:1112 [D loss: 0.094111, acc.: 98.44%] [G loss: 3.497432]\n",
      "epoch:1 step:1113 [D loss: 0.220886, acc.: 92.97%] [G loss: 6.775787]\n",
      "epoch:1 step:1114 [D loss: 0.363672, acc.: 82.81%] [G loss: 3.181088]\n",
      "epoch:1 step:1115 [D loss: 0.390995, acc.: 79.69%] [G loss: 8.427835]\n",
      "epoch:1 step:1116 [D loss: 0.237740, acc.: 91.41%] [G loss: 7.724808]\n",
      "epoch:1 step:1117 [D loss: 0.519244, acc.: 82.03%] [G loss: 4.825809]\n",
      "epoch:1 step:1118 [D loss: 0.086293, acc.: 96.09%] [G loss: 5.749816]\n",
      "epoch:1 step:1119 [D loss: 0.018495, acc.: 100.00%] [G loss: 6.227425]\n",
      "epoch:1 step:1120 [D loss: 0.048520, acc.: 99.22%] [G loss: 4.129781]\n",
      "epoch:1 step:1121 [D loss: 0.281673, acc.: 89.06%] [G loss: 7.141061]\n",
      "epoch:1 step:1122 [D loss: 0.262449, acc.: 85.94%] [G loss: 4.985966]\n",
      "epoch:1 step:1123 [D loss: 0.430645, acc.: 81.25%] [G loss: 6.425920]\n",
      "epoch:1 step:1124 [D loss: 0.233401, acc.: 91.41%] [G loss: 5.949984]\n",
      "epoch:1 step:1125 [D loss: 0.114408, acc.: 96.09%] [G loss: 4.857359]\n",
      "epoch:1 step:1126 [D loss: 0.047731, acc.: 99.22%] [G loss: 4.418526]\n",
      "epoch:1 step:1127 [D loss: 0.145989, acc.: 94.53%] [G loss: 6.898984]\n",
      "epoch:1 step:1128 [D loss: 0.133419, acc.: 94.53%] [G loss: 5.968894]\n",
      "epoch:1 step:1129 [D loss: 0.041505, acc.: 100.00%] [G loss: 5.005622]\n",
      "epoch:1 step:1130 [D loss: 0.092309, acc.: 97.66%] [G loss: 5.687291]\n",
      "epoch:1 step:1131 [D loss: 0.027459, acc.: 100.00%] [G loss: 5.561580]\n",
      "epoch:1 step:1132 [D loss: 0.563194, acc.: 71.88%] [G loss: 8.578806]\n",
      "epoch:1 step:1133 [D loss: 0.508400, acc.: 80.47%] [G loss: 6.685901]\n",
      "epoch:1 step:1134 [D loss: 0.027235, acc.: 100.00%] [G loss: 5.474950]\n",
      "epoch:1 step:1135 [D loss: 0.084244, acc.: 97.66%] [G loss: 5.800402]\n",
      "epoch:1 step:1136 [D loss: 0.038843, acc.: 98.44%] [G loss: 5.563653]\n",
      "epoch:1 step:1137 [D loss: 0.040310, acc.: 99.22%] [G loss: 4.823043]\n",
      "epoch:1 step:1138 [D loss: 0.224925, acc.: 89.06%] [G loss: 6.114869]\n",
      "epoch:1 step:1139 [D loss: 0.214995, acc.: 92.19%] [G loss: 4.867450]\n",
      "epoch:1 step:1140 [D loss: 0.308763, acc.: 89.06%] [G loss: 5.237034]\n",
      "epoch:1 step:1141 [D loss: 0.186246, acc.: 92.97%] [G loss: 4.387933]\n",
      "epoch:1 step:1142 [D loss: 0.333771, acc.: 86.72%] [G loss: 6.246037]\n",
      "epoch:1 step:1143 [D loss: 0.201391, acc.: 89.84%] [G loss: 5.179457]\n",
      "epoch:1 step:1144 [D loss: 0.151870, acc.: 94.53%] [G loss: 4.280334]\n",
      "epoch:1 step:1145 [D loss: 0.081926, acc.: 98.44%] [G loss: 2.776781]\n",
      "epoch:1 step:1146 [D loss: 0.227656, acc.: 92.97%] [G loss: 5.138315]\n",
      "epoch:1 step:1147 [D loss: 0.327186, acc.: 85.94%] [G loss: 3.042342]\n",
      "epoch:1 step:1148 [D loss: 0.108673, acc.: 96.09%] [G loss: 3.311031]\n",
      "epoch:1 step:1149 [D loss: 0.026980, acc.: 100.00%] [G loss: 3.434695]\n",
      "epoch:1 step:1150 [D loss: 0.092903, acc.: 99.22%] [G loss: 2.943674]\n",
      "epoch:1 step:1151 [D loss: 0.148534, acc.: 95.31%] [G loss: 5.699585]\n",
      "epoch:1 step:1152 [D loss: 0.187213, acc.: 93.75%] [G loss: 2.943012]\n",
      "epoch:1 step:1153 [D loss: 0.233124, acc.: 88.28%] [G loss: 6.444049]\n",
      "epoch:1 step:1154 [D loss: 0.226603, acc.: 91.41%] [G loss: 5.071719]\n",
      "epoch:1 step:1155 [D loss: 0.219090, acc.: 90.62%] [G loss: 6.621874]\n",
      "epoch:1 step:1156 [D loss: 0.133702, acc.: 95.31%] [G loss: 5.849588]\n",
      "epoch:1 step:1157 [D loss: 0.103714, acc.: 98.44%] [G loss: 5.374448]\n",
      "epoch:1 step:1158 [D loss: 0.124849, acc.: 96.09%] [G loss: 6.661252]\n",
      "epoch:1 step:1159 [D loss: 0.705911, acc.: 65.62%] [G loss: 9.134499]\n",
      "epoch:1 step:1160 [D loss: 0.218131, acc.: 89.06%] [G loss: 7.967023]\n",
      "epoch:1 step:1161 [D loss: 0.061203, acc.: 96.88%] [G loss: 6.650373]\n",
      "epoch:1 step:1162 [D loss: 0.039378, acc.: 99.22%] [G loss: 5.752430]\n",
      "epoch:1 step:1163 [D loss: 0.019928, acc.: 100.00%] [G loss: 4.928137]\n",
      "epoch:1 step:1164 [D loss: 0.258640, acc.: 89.84%] [G loss: 8.130103]\n",
      "epoch:1 step:1165 [D loss: 0.279389, acc.: 85.16%] [G loss: 5.004811]\n",
      "epoch:1 step:1166 [D loss: 0.084575, acc.: 98.44%] [G loss: 4.289031]\n",
      "epoch:1 step:1167 [D loss: 0.043899, acc.: 98.44%] [G loss: 4.552580]\n",
      "epoch:1 step:1168 [D loss: 0.097061, acc.: 97.66%] [G loss: 4.106951]\n",
      "epoch:1 step:1169 [D loss: 0.135803, acc.: 97.66%] [G loss: 5.076664]\n",
      "epoch:1 step:1170 [D loss: 0.099587, acc.: 96.88%] [G loss: 4.438344]\n",
      "epoch:1 step:1171 [D loss: 0.258745, acc.: 88.28%] [G loss: 8.650962]\n",
      "epoch:1 step:1172 [D loss: 0.763148, acc.: 67.19%] [G loss: 4.310026]\n",
      "epoch:1 step:1173 [D loss: 0.011874, acc.: 100.00%] [G loss: 4.358375]\n",
      "epoch:1 step:1174 [D loss: 0.185725, acc.: 92.19%] [G loss: 7.913035]\n",
      "epoch:1 step:1175 [D loss: 0.287764, acc.: 85.16%] [G loss: 4.121948]\n",
      "epoch:1 step:1176 [D loss: 0.334380, acc.: 85.94%] [G loss: 7.969059]\n",
      "epoch:1 step:1177 [D loss: 0.153047, acc.: 94.53%] [G loss: 7.449018]\n",
      "epoch:1 step:1178 [D loss: 0.294920, acc.: 88.28%] [G loss: 3.889287]\n",
      "epoch:1 step:1179 [D loss: 0.062438, acc.: 97.66%] [G loss: 3.885997]\n",
      "epoch:1 step:1180 [D loss: 0.073242, acc.: 96.88%] [G loss: 4.529364]\n",
      "epoch:1 step:1181 [D loss: 0.166945, acc.: 94.53%] [G loss: 4.404633]\n",
      "epoch:1 step:1182 [D loss: 0.045919, acc.: 100.00%] [G loss: 4.419254]\n",
      "epoch:1 step:1183 [D loss: 0.427976, acc.: 81.25%] [G loss: 7.362539]\n",
      "epoch:1 step:1184 [D loss: 0.243030, acc.: 87.50%] [G loss: 5.449368]\n",
      "epoch:1 step:1185 [D loss: 0.077940, acc.: 97.66%] [G loss: 5.299969]\n",
      "epoch:1 step:1186 [D loss: 0.026557, acc.: 99.22%] [G loss: 5.406550]\n",
      "epoch:1 step:1187 [D loss: 0.122586, acc.: 96.88%] [G loss: 4.612073]\n",
      "epoch:1 step:1188 [D loss: 0.017882, acc.: 100.00%] [G loss: 4.365313]\n",
      "epoch:1 step:1189 [D loss: 0.109982, acc.: 97.66%] [G loss: 5.877085]\n",
      "epoch:1 step:1190 [D loss: 0.065004, acc.: 96.09%] [G loss: 4.662267]\n",
      "epoch:1 step:1191 [D loss: 0.147247, acc.: 95.31%] [G loss: 7.406928]\n",
      "epoch:1 step:1192 [D loss: 1.079217, acc.: 48.44%] [G loss: 6.925733]\n",
      "epoch:1 step:1193 [D loss: 0.038395, acc.: 99.22%] [G loss: 7.546076]\n",
      "epoch:1 step:1194 [D loss: 0.022936, acc.: 99.22%] [G loss: 7.205678]\n",
      "epoch:1 step:1195 [D loss: 0.048106, acc.: 97.66%] [G loss: 5.574575]\n",
      "epoch:1 step:1196 [D loss: 0.051284, acc.: 99.22%] [G loss: 4.894401]\n",
      "epoch:1 step:1197 [D loss: 0.115428, acc.: 96.09%] [G loss: 6.435084]\n",
      "epoch:1 step:1198 [D loss: 0.589913, acc.: 74.22%] [G loss: 8.001643]\n",
      "epoch:1 step:1199 [D loss: 0.077500, acc.: 98.44%] [G loss: 7.714928]\n",
      "epoch:1 step:1200 [D loss: 0.082595, acc.: 97.66%] [G loss: 5.704174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1201 [D loss: 0.287323, acc.: 87.50%] [G loss: 7.353230]\n",
      "epoch:1 step:1202 [D loss: 0.138068, acc.: 93.75%] [G loss: 6.884565]\n",
      "epoch:1 step:1203 [D loss: 0.386482, acc.: 84.38%] [G loss: 6.323358]\n",
      "epoch:1 step:1204 [D loss: 0.062980, acc.: 98.44%] [G loss: 6.185813]\n",
      "epoch:1 step:1205 [D loss: 0.311028, acc.: 86.72%] [G loss: 6.802602]\n",
      "epoch:1 step:1206 [D loss: 0.192610, acc.: 91.41%] [G loss: 3.559938]\n",
      "epoch:1 step:1207 [D loss: 0.506702, acc.: 79.69%] [G loss: 9.387343]\n",
      "epoch:1 step:1208 [D loss: 0.518457, acc.: 78.91%] [G loss: 6.433640]\n",
      "epoch:1 step:1209 [D loss: 0.113405, acc.: 96.09%] [G loss: 4.118357]\n",
      "epoch:1 step:1210 [D loss: 0.239952, acc.: 86.72%] [G loss: 5.237426]\n",
      "epoch:1 step:1211 [D loss: 0.039916, acc.: 100.00%] [G loss: 5.335500]\n",
      "epoch:1 step:1212 [D loss: 0.203212, acc.: 91.41%] [G loss: 3.396256]\n",
      "epoch:1 step:1213 [D loss: 0.080130, acc.: 97.66%] [G loss: 3.254196]\n",
      "epoch:1 step:1214 [D loss: 0.132497, acc.: 96.09%] [G loss: 4.771571]\n",
      "epoch:1 step:1215 [D loss: 0.110203, acc.: 96.88%] [G loss: 4.493227]\n",
      "epoch:1 step:1216 [D loss: 0.412262, acc.: 78.12%] [G loss: 6.199658]\n",
      "epoch:1 step:1217 [D loss: 0.478026, acc.: 76.56%] [G loss: 3.079123]\n",
      "epoch:1 step:1218 [D loss: 0.068526, acc.: 97.66%] [G loss: 2.867760]\n",
      "epoch:1 step:1219 [D loss: 0.134841, acc.: 96.09%] [G loss: 4.006582]\n",
      "epoch:1 step:1220 [D loss: 0.064960, acc.: 99.22%] [G loss: 3.161373]\n",
      "epoch:1 step:1221 [D loss: 0.151879, acc.: 96.09%] [G loss: 4.195693]\n",
      "epoch:1 step:1222 [D loss: 0.121802, acc.: 96.09%] [G loss: 3.408370]\n",
      "epoch:1 step:1223 [D loss: 0.227531, acc.: 89.84%] [G loss: 5.560047]\n",
      "epoch:1 step:1224 [D loss: 0.239693, acc.: 88.28%] [G loss: 3.302459]\n",
      "epoch:1 step:1225 [D loss: 0.126728, acc.: 92.97%] [G loss: 4.797006]\n",
      "epoch:1 step:1226 [D loss: 0.055036, acc.: 98.44%] [G loss: 4.723771]\n",
      "epoch:1 step:1227 [D loss: 0.026780, acc.: 100.00%] [G loss: 2.954982]\n",
      "epoch:1 step:1228 [D loss: 0.118395, acc.: 94.53%] [G loss: 4.270183]\n",
      "epoch:1 step:1229 [D loss: 0.372903, acc.: 82.03%] [G loss: 5.172475]\n",
      "epoch:1 step:1230 [D loss: 0.032095, acc.: 100.00%] [G loss: 5.877078]\n",
      "epoch:1 step:1231 [D loss: 0.046279, acc.: 99.22%] [G loss: 4.121964]\n",
      "epoch:1 step:1232 [D loss: 0.137107, acc.: 92.97%] [G loss: 5.610274]\n",
      "epoch:1 step:1233 [D loss: 0.053783, acc.: 98.44%] [G loss: 5.095840]\n",
      "epoch:1 step:1234 [D loss: 0.132786, acc.: 96.88%] [G loss: 3.254287]\n",
      "epoch:1 step:1235 [D loss: 0.021942, acc.: 100.00%] [G loss: 3.686618]\n",
      "epoch:1 step:1236 [D loss: 0.122119, acc.: 96.09%] [G loss: 6.451996]\n",
      "epoch:1 step:1237 [D loss: 0.060498, acc.: 99.22%] [G loss: 6.199816]\n",
      "epoch:1 step:1238 [D loss: 0.047784, acc.: 99.22%] [G loss: 3.358947]\n",
      "epoch:1 step:1239 [D loss: 0.031116, acc.: 100.00%] [G loss: 2.731886]\n",
      "epoch:1 step:1240 [D loss: 0.181083, acc.: 91.41%] [G loss: 7.214780]\n",
      "epoch:1 step:1241 [D loss: 0.222591, acc.: 90.62%] [G loss: 3.681005]\n",
      "epoch:1 step:1242 [D loss: 0.032344, acc.: 99.22%] [G loss: 1.519322]\n",
      "epoch:1 step:1243 [D loss: 0.041486, acc.: 98.44%] [G loss: 0.887122]\n",
      "epoch:1 step:1244 [D loss: 0.009397, acc.: 100.00%] [G loss: 1.332093]\n",
      "epoch:1 step:1245 [D loss: 0.054916, acc.: 100.00%] [G loss: 0.469518]\n",
      "epoch:1 step:1246 [D loss: 0.257423, acc.: 89.84%] [G loss: 4.820096]\n",
      "epoch:1 step:1247 [D loss: 0.232683, acc.: 88.28%] [G loss: 0.525339]\n",
      "epoch:1 step:1248 [D loss: 0.179640, acc.: 89.06%] [G loss: 4.153139]\n",
      "epoch:1 step:1249 [D loss: 0.115725, acc.: 96.88%] [G loss: 4.225213]\n",
      "epoch:1 step:1250 [D loss: 0.059936, acc.: 97.66%] [G loss: 0.562764]\n",
      "epoch:1 step:1251 [D loss: 0.229719, acc.: 88.28%] [G loss: 5.128542]\n",
      "epoch:1 step:1252 [D loss: 0.255361, acc.: 89.84%] [G loss: 5.434190]\n",
      "epoch:1 step:1253 [D loss: 0.018229, acc.: 100.00%] [G loss: 2.397912]\n",
      "epoch:1 step:1254 [D loss: 0.142471, acc.: 94.53%] [G loss: 4.224114]\n",
      "epoch:1 step:1255 [D loss: 0.061263, acc.: 97.66%] [G loss: 3.072202]\n",
      "epoch:1 step:1256 [D loss: 0.085696, acc.: 96.88%] [G loss: 2.750716]\n",
      "epoch:1 step:1257 [D loss: 0.234886, acc.: 92.19%] [G loss: 6.972631]\n",
      "epoch:1 step:1258 [D loss: 0.185298, acc.: 91.41%] [G loss: 4.996293]\n",
      "epoch:1 step:1259 [D loss: 0.016507, acc.: 100.00%] [G loss: 2.892630]\n",
      "epoch:1 step:1260 [D loss: 0.732938, acc.: 67.19%] [G loss: 11.563364]\n",
      "epoch:1 step:1261 [D loss: 1.769323, acc.: 51.56%] [G loss: 6.162062]\n",
      "epoch:1 step:1262 [D loss: 0.037336, acc.: 100.00%] [G loss: 3.486333]\n",
      "epoch:1 step:1263 [D loss: 0.080182, acc.: 96.88%] [G loss: 3.602654]\n",
      "epoch:1 step:1264 [D loss: 0.066414, acc.: 98.44%] [G loss: 3.777309]\n",
      "epoch:1 step:1265 [D loss: 0.106168, acc.: 96.88%] [G loss: 4.604125]\n",
      "epoch:1 step:1266 [D loss: 0.097099, acc.: 98.44%] [G loss: 4.276101]\n",
      "epoch:1 step:1267 [D loss: 0.339460, acc.: 86.72%] [G loss: 5.573910]\n",
      "epoch:1 step:1268 [D loss: 0.253527, acc.: 92.97%] [G loss: 5.204095]\n",
      "epoch:1 step:1269 [D loss: 0.111319, acc.: 96.88%] [G loss: 4.875908]\n",
      "epoch:1 step:1270 [D loss: 0.278084, acc.: 89.84%] [G loss: 5.738943]\n",
      "epoch:1 step:1271 [D loss: 0.166004, acc.: 93.75%] [G loss: 5.075432]\n",
      "epoch:1 step:1272 [D loss: 0.048058, acc.: 99.22%] [G loss: 3.311605]\n",
      "epoch:1 step:1273 [D loss: 0.174172, acc.: 92.97%] [G loss: 6.533418]\n",
      "epoch:1 step:1274 [D loss: 0.242910, acc.: 90.62%] [G loss: 6.108071]\n",
      "epoch:1 step:1275 [D loss: 0.042708, acc.: 100.00%] [G loss: 4.809816]\n",
      "epoch:1 step:1276 [D loss: 0.158474, acc.: 96.09%] [G loss: 3.344970]\n",
      "epoch:1 step:1277 [D loss: 1.244655, acc.: 47.66%] [G loss: 9.765104]\n",
      "epoch:1 step:1278 [D loss: 1.122366, acc.: 62.50%] [G loss: 5.435324]\n",
      "epoch:1 step:1279 [D loss: 0.127137, acc.: 94.53%] [G loss: 3.299758]\n",
      "epoch:1 step:1280 [D loss: 0.055454, acc.: 99.22%] [G loss: 3.330047]\n",
      "epoch:1 step:1281 [D loss: 0.049958, acc.: 100.00%] [G loss: 3.365046]\n",
      "epoch:1 step:1282 [D loss: 0.067566, acc.: 100.00%] [G loss: 3.284626]\n",
      "epoch:1 step:1283 [D loss: 0.077535, acc.: 98.44%] [G loss: 3.253800]\n",
      "epoch:1 step:1284 [D loss: 0.066200, acc.: 98.44%] [G loss: 2.858322]\n",
      "epoch:1 step:1285 [D loss: 0.089912, acc.: 98.44%] [G loss: 2.509265]\n",
      "epoch:1 step:1286 [D loss: 0.154985, acc.: 96.88%] [G loss: 3.302495]\n",
      "epoch:1 step:1287 [D loss: 0.116163, acc.: 96.88%] [G loss: 2.901871]\n",
      "epoch:1 step:1288 [D loss: 0.129288, acc.: 96.09%] [G loss: 2.447680]\n",
      "epoch:1 step:1289 [D loss: 0.117040, acc.: 96.88%] [G loss: 3.155581]\n",
      "epoch:1 step:1290 [D loss: 0.069641, acc.: 98.44%] [G loss: 3.736997]\n",
      "epoch:1 step:1291 [D loss: 0.153793, acc.: 95.31%] [G loss: 2.142809]\n",
      "epoch:1 step:1292 [D loss: 0.263024, acc.: 89.06%] [G loss: 5.770857]\n",
      "epoch:1 step:1293 [D loss: 0.090021, acc.: 96.88%] [G loss: 5.313229]\n",
      "epoch:1 step:1294 [D loss: 0.210098, acc.: 92.97%] [G loss: 4.190692]\n",
      "epoch:1 step:1295 [D loss: 0.036280, acc.: 100.00%] [G loss: 3.829137]\n",
      "epoch:1 step:1296 [D loss: 0.123330, acc.: 98.44%] [G loss: 3.890681]\n",
      "epoch:1 step:1297 [D loss: 0.138353, acc.: 96.09%] [G loss: 5.344904]\n",
      "epoch:1 step:1298 [D loss: 0.209900, acc.: 92.19%] [G loss: 3.265077]\n",
      "epoch:1 step:1299 [D loss: 0.182832, acc.: 95.31%] [G loss: 6.134485]\n",
      "epoch:1 step:1300 [D loss: 0.110186, acc.: 96.09%] [G loss: 4.936034]\n",
      "epoch:1 step:1301 [D loss: 0.142259, acc.: 96.88%] [G loss: 4.560908]\n",
      "epoch:1 step:1302 [D loss: 0.051207, acc.: 100.00%] [G loss: 4.318656]\n",
      "epoch:1 step:1303 [D loss: 0.188919, acc.: 93.75%] [G loss: 5.698747]\n",
      "epoch:1 step:1304 [D loss: 0.143930, acc.: 95.31%] [G loss: 4.944316]\n",
      "epoch:1 step:1305 [D loss: 0.092466, acc.: 98.44%] [G loss: 4.017759]\n",
      "epoch:1 step:1306 [D loss: 0.059214, acc.: 100.00%] [G loss: 4.464828]\n",
      "epoch:1 step:1307 [D loss: 0.907561, acc.: 57.03%] [G loss: 10.241199]\n",
      "epoch:1 step:1308 [D loss: 0.852914, acc.: 64.84%] [G loss: 6.348408]\n",
      "epoch:1 step:1309 [D loss: 0.015449, acc.: 100.00%] [G loss: 4.368121]\n",
      "epoch:1 step:1310 [D loss: 0.044108, acc.: 100.00%] [G loss: 3.765951]\n",
      "epoch:1 step:1311 [D loss: 0.099529, acc.: 96.88%] [G loss: 4.661282]\n",
      "epoch:1 step:1312 [D loss: 0.016509, acc.: 100.00%] [G loss: 5.295419]\n",
      "epoch:1 step:1313 [D loss: 0.072751, acc.: 98.44%] [G loss: 3.841618]\n",
      "epoch:1 step:1314 [D loss: 0.308584, acc.: 87.50%] [G loss: 7.413539]\n",
      "epoch:1 step:1315 [D loss: 0.320293, acc.: 89.84%] [G loss: 4.318035]\n",
      "epoch:1 step:1316 [D loss: 0.111467, acc.: 96.88%] [G loss: 3.081970]\n",
      "epoch:1 step:1317 [D loss: 0.213257, acc.: 91.41%] [G loss: 6.242725]\n",
      "epoch:1 step:1318 [D loss: 0.078337, acc.: 96.88%] [G loss: 5.612628]\n",
      "epoch:1 step:1319 [D loss: 0.138214, acc.: 95.31%] [G loss: 3.202816]\n",
      "epoch:1 step:1320 [D loss: 0.121433, acc.: 96.09%] [G loss: 4.291047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1321 [D loss: 0.072716, acc.: 98.44%] [G loss: 4.651008]\n",
      "epoch:1 step:1322 [D loss: 0.151489, acc.: 96.88%] [G loss: 5.072919]\n",
      "epoch:1 step:1323 [D loss: 0.276865, acc.: 92.97%] [G loss: 5.838199]\n",
      "epoch:1 step:1324 [D loss: 0.155120, acc.: 95.31%] [G loss: 4.704912]\n",
      "epoch:1 step:1325 [D loss: 0.134713, acc.: 95.31%] [G loss: 4.372405]\n",
      "epoch:1 step:1326 [D loss: 0.043046, acc.: 99.22%] [G loss: 2.541983]\n",
      "epoch:1 step:1327 [D loss: 0.891980, acc.: 56.25%] [G loss: 9.264152]\n",
      "epoch:1 step:1328 [D loss: 1.840055, acc.: 49.22%] [G loss: 2.932304]\n",
      "epoch:1 step:1329 [D loss: 0.179485, acc.: 92.97%] [G loss: 2.733098]\n",
      "epoch:1 step:1330 [D loss: 0.082255, acc.: 97.66%] [G loss: 2.757558]\n",
      "epoch:1 step:1331 [D loss: 0.150522, acc.: 96.88%] [G loss: 3.048528]\n",
      "epoch:1 step:1332 [D loss: 0.178424, acc.: 95.31%] [G loss: 3.658716]\n",
      "epoch:1 step:1333 [D loss: 0.336755, acc.: 86.72%] [G loss: 3.700024]\n",
      "epoch:1 step:1334 [D loss: 0.047747, acc.: 100.00%] [G loss: 3.237680]\n",
      "epoch:1 step:1335 [D loss: 0.059809, acc.: 100.00%] [G loss: 2.260211]\n",
      "epoch:1 step:1336 [D loss: 0.191844, acc.: 92.97%] [G loss: 4.387674]\n",
      "epoch:1 step:1337 [D loss: 0.287708, acc.: 89.06%] [G loss: 3.697431]\n",
      "epoch:1 step:1338 [D loss: 0.113534, acc.: 95.31%] [G loss: 4.319308]\n",
      "epoch:1 step:1339 [D loss: 0.029404, acc.: 100.00%] [G loss: 4.710164]\n",
      "epoch:1 step:1340 [D loss: 0.071114, acc.: 98.44%] [G loss: 3.768357]\n",
      "epoch:1 step:1341 [D loss: 0.193186, acc.: 94.53%] [G loss: 4.838168]\n",
      "epoch:1 step:1342 [D loss: 0.081565, acc.: 97.66%] [G loss: 4.674241]\n",
      "epoch:1 step:1343 [D loss: 0.249769, acc.: 91.41%] [G loss: 4.235560]\n",
      "epoch:1 step:1344 [D loss: 0.149771, acc.: 93.75%] [G loss: 3.602741]\n",
      "epoch:1 step:1345 [D loss: 0.134625, acc.: 96.09%] [G loss: 5.531079]\n",
      "epoch:1 step:1346 [D loss: 0.053109, acc.: 99.22%] [G loss: 5.675484]\n",
      "epoch:1 step:1347 [D loss: 0.325855, acc.: 88.28%] [G loss: 7.533214]\n",
      "epoch:1 step:1348 [D loss: 0.265705, acc.: 84.38%] [G loss: 3.952480]\n",
      "epoch:1 step:1349 [D loss: 0.128245, acc.: 96.09%] [G loss: 5.582356]\n",
      "epoch:1 step:1350 [D loss: 0.028429, acc.: 99.22%] [G loss: 4.943967]\n",
      "epoch:1 step:1351 [D loss: 0.068387, acc.: 99.22%] [G loss: 3.489250]\n",
      "epoch:1 step:1352 [D loss: 0.349604, acc.: 87.50%] [G loss: 6.786021]\n",
      "epoch:1 step:1353 [D loss: 0.957097, acc.: 60.94%] [G loss: 4.259529]\n",
      "epoch:1 step:1354 [D loss: 0.018486, acc.: 100.00%] [G loss: 3.585541]\n",
      "epoch:1 step:1355 [D loss: 0.255534, acc.: 89.06%] [G loss: 5.257291]\n",
      "epoch:1 step:1356 [D loss: 0.176906, acc.: 92.97%] [G loss: 3.048734]\n",
      "epoch:1 step:1357 [D loss: 0.297563, acc.: 86.72%] [G loss: 5.497225]\n",
      "epoch:1 step:1358 [D loss: 0.498446, acc.: 75.78%] [G loss: 3.089557]\n",
      "epoch:1 step:1359 [D loss: 0.099658, acc.: 96.09%] [G loss: 4.414642]\n",
      "epoch:1 step:1360 [D loss: 0.071377, acc.: 100.00%] [G loss: 3.873903]\n",
      "epoch:1 step:1361 [D loss: 0.258565, acc.: 91.41%] [G loss: 2.725610]\n",
      "epoch:1 step:1362 [D loss: 0.133263, acc.: 95.31%] [G loss: 4.811155]\n",
      "epoch:1 step:1363 [D loss: 0.150284, acc.: 96.09%] [G loss: 4.192923]\n",
      "epoch:1 step:1364 [D loss: 0.180020, acc.: 95.31%] [G loss: 4.287501]\n",
      "epoch:1 step:1365 [D loss: 0.303926, acc.: 89.06%] [G loss: 6.557806]\n",
      "epoch:1 step:1366 [D loss: 0.159010, acc.: 96.09%] [G loss: 5.436758]\n",
      "epoch:1 step:1367 [D loss: 0.068432, acc.: 99.22%] [G loss: 4.648500]\n",
      "epoch:1 step:1368 [D loss: 0.073663, acc.: 97.66%] [G loss: 4.944228]\n",
      "epoch:1 step:1369 [D loss: 0.086397, acc.: 98.44%] [G loss: 5.269090]\n",
      "epoch:1 step:1370 [D loss: 0.208406, acc.: 92.97%] [G loss: 4.959254]\n",
      "epoch:1 step:1371 [D loss: 0.124578, acc.: 96.88%] [G loss: 7.136148]\n",
      "epoch:1 step:1372 [D loss: 0.082182, acc.: 96.88%] [G loss: 6.060825]\n",
      "epoch:1 step:1373 [D loss: 0.061919, acc.: 99.22%] [G loss: 5.169550]\n",
      "epoch:1 step:1374 [D loss: 0.100725, acc.: 96.88%] [G loss: 5.554703]\n",
      "epoch:1 step:1375 [D loss: 0.060326, acc.: 98.44%] [G loss: 4.111701]\n",
      "epoch:1 step:1376 [D loss: 0.290622, acc.: 88.28%] [G loss: 7.928408]\n",
      "epoch:1 step:1377 [D loss: 0.925708, acc.: 59.38%] [G loss: 5.238565]\n",
      "epoch:1 step:1378 [D loss: 0.040113, acc.: 99.22%] [G loss: 5.929715]\n",
      "epoch:1 step:1379 [D loss: 0.054600, acc.: 97.66%] [G loss: 4.288915]\n",
      "epoch:1 step:1380 [D loss: 0.074196, acc.: 98.44%] [G loss: 2.566307]\n",
      "epoch:1 step:1381 [D loss: 0.094306, acc.: 96.09%] [G loss: 3.464730]\n",
      "epoch:1 step:1382 [D loss: 0.049750, acc.: 98.44%] [G loss: 1.814621]\n",
      "epoch:1 step:1383 [D loss: 0.131075, acc.: 96.88%] [G loss: 0.792562]\n",
      "epoch:1 step:1384 [D loss: 0.082993, acc.: 100.00%] [G loss: 2.158840]\n",
      "epoch:1 step:1385 [D loss: 0.258829, acc.: 89.84%] [G loss: 6.468791]\n",
      "epoch:1 step:1386 [D loss: 0.773262, acc.: 65.62%] [G loss: 0.674761]\n",
      "epoch:1 step:1387 [D loss: 0.343930, acc.: 82.03%] [G loss: 5.679584]\n",
      "epoch:1 step:1388 [D loss: 0.036189, acc.: 100.00%] [G loss: 7.084105]\n",
      "epoch:1 step:1389 [D loss: 0.455642, acc.: 78.12%] [G loss: 1.856317]\n",
      "epoch:1 step:1390 [D loss: 0.229906, acc.: 89.06%] [G loss: 4.264392]\n",
      "epoch:1 step:1391 [D loss: 0.064603, acc.: 99.22%] [G loss: 4.830207]\n",
      "epoch:1 step:1392 [D loss: 0.091588, acc.: 98.44%] [G loss: 3.611669]\n",
      "epoch:1 step:1393 [D loss: 0.236528, acc.: 89.84%] [G loss: 5.155575]\n",
      "epoch:1 step:1394 [D loss: 0.157302, acc.: 92.19%] [G loss: 3.974856]\n",
      "epoch:1 step:1395 [D loss: 0.206920, acc.: 94.53%] [G loss: 4.183922]\n",
      "epoch:1 step:1396 [D loss: 0.151443, acc.: 93.75%] [G loss: 4.460984]\n",
      "epoch:1 step:1397 [D loss: 0.101575, acc.: 96.88%] [G loss: 5.018368]\n",
      "epoch:1 step:1398 [D loss: 0.253704, acc.: 91.41%] [G loss: 4.576935]\n",
      "epoch:1 step:1399 [D loss: 0.099336, acc.: 96.88%] [G loss: 3.970447]\n",
      "epoch:1 step:1400 [D loss: 0.071947, acc.: 100.00%] [G loss: 4.350896]\n",
      "epoch:1 step:1401 [D loss: 0.071979, acc.: 99.22%] [G loss: 4.943018]\n",
      "epoch:1 step:1402 [D loss: 0.049881, acc.: 100.00%] [G loss: 4.040747]\n",
      "epoch:1 step:1403 [D loss: 0.443533, acc.: 78.91%] [G loss: 10.146556]\n",
      "epoch:1 step:1404 [D loss: 0.827347, acc.: 67.97%] [G loss: 4.548009]\n",
      "epoch:1 step:1405 [D loss: 0.239000, acc.: 90.62%] [G loss: 5.513454]\n",
      "epoch:1 step:1406 [D loss: 0.013020, acc.: 100.00%] [G loss: 6.656596]\n",
      "epoch:1 step:1407 [D loss: 0.089653, acc.: 96.09%] [G loss: 4.617165]\n",
      "epoch:1 step:1408 [D loss: 0.138353, acc.: 96.88%] [G loss: 4.759975]\n",
      "epoch:1 step:1409 [D loss: 0.067011, acc.: 99.22%] [G loss: 5.159931]\n",
      "epoch:1 step:1410 [D loss: 0.131592, acc.: 94.53%] [G loss: 4.930916]\n",
      "epoch:1 step:1411 [D loss: 0.188835, acc.: 92.97%] [G loss: 5.678022]\n",
      "epoch:1 step:1412 [D loss: 0.127489, acc.: 97.66%] [G loss: 4.085147]\n",
      "epoch:1 step:1413 [D loss: 0.146041, acc.: 92.19%] [G loss: 4.722791]\n",
      "epoch:1 step:1414 [D loss: 0.130909, acc.: 95.31%] [G loss: 3.923958]\n",
      "epoch:1 step:1415 [D loss: 0.434604, acc.: 76.56%] [G loss: 6.567602]\n",
      "epoch:1 step:1416 [D loss: 0.214062, acc.: 91.41%] [G loss: 6.066006]\n",
      "epoch:1 step:1417 [D loss: 0.382835, acc.: 82.03%] [G loss: 4.580401]\n",
      "epoch:1 step:1418 [D loss: 0.025518, acc.: 99.22%] [G loss: 4.358045]\n",
      "epoch:1 step:1419 [D loss: 0.049714, acc.: 98.44%] [G loss: 3.213865]\n",
      "epoch:1 step:1420 [D loss: 0.167088, acc.: 92.97%] [G loss: 4.798788]\n",
      "epoch:1 step:1421 [D loss: 0.107616, acc.: 97.66%] [G loss: 4.324393]\n",
      "epoch:1 step:1422 [D loss: 0.068536, acc.: 99.22%] [G loss: 3.888245]\n",
      "epoch:1 step:1423 [D loss: 0.227955, acc.: 89.84%] [G loss: 5.696370]\n",
      "epoch:1 step:1424 [D loss: 0.050451, acc.: 98.44%] [G loss: 5.188457]\n",
      "epoch:1 step:1425 [D loss: 0.083954, acc.: 97.66%] [G loss: 2.602931]\n",
      "epoch:1 step:1426 [D loss: 0.434734, acc.: 76.56%] [G loss: 8.780851]\n",
      "epoch:1 step:1427 [D loss: 0.854804, acc.: 64.06%] [G loss: 4.677263]\n",
      "epoch:1 step:1428 [D loss: 0.028450, acc.: 100.00%] [G loss: 2.874221]\n",
      "epoch:1 step:1429 [D loss: 0.064768, acc.: 99.22%] [G loss: 4.012557]\n",
      "epoch:1 step:1430 [D loss: 0.045141, acc.: 100.00%] [G loss: 4.227746]\n",
      "epoch:1 step:1431 [D loss: 0.045676, acc.: 100.00%] [G loss: 3.271898]\n",
      "epoch:1 step:1432 [D loss: 0.043011, acc.: 100.00%] [G loss: 4.207875]\n",
      "epoch:1 step:1433 [D loss: 0.201182, acc.: 91.41%] [G loss: 6.112951]\n",
      "epoch:1 step:1434 [D loss: 0.025336, acc.: 100.00%] [G loss: 5.489859]\n",
      "epoch:1 step:1435 [D loss: 0.083782, acc.: 100.00%] [G loss: 3.570155]\n",
      "epoch:1 step:1436 [D loss: 0.100616, acc.: 97.66%] [G loss: 5.278122]\n",
      "epoch:1 step:1437 [D loss: 0.097449, acc.: 99.22%] [G loss: 4.139207]\n",
      "epoch:1 step:1438 [D loss: 0.208005, acc.: 90.62%] [G loss: 8.285784]\n",
      "epoch:1 step:1439 [D loss: 0.462077, acc.: 77.34%] [G loss: 3.131362]\n",
      "epoch:1 step:1440 [D loss: 0.767337, acc.: 67.97%] [G loss: 10.327030]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1441 [D loss: 0.792545, acc.: 68.75%] [G loss: 7.539699]\n",
      "epoch:1 step:1442 [D loss: 0.056443, acc.: 98.44%] [G loss: 6.517766]\n",
      "epoch:1 step:1443 [D loss: 0.009441, acc.: 100.00%] [G loss: 3.187448]\n",
      "epoch:1 step:1444 [D loss: 0.071093, acc.: 96.88%] [G loss: 2.080327]\n",
      "epoch:1 step:1445 [D loss: 0.101744, acc.: 96.88%] [G loss: 2.757749]\n",
      "epoch:1 step:1446 [D loss: 0.046190, acc.: 98.44%] [G loss: 0.750602]\n",
      "epoch:1 step:1447 [D loss: 0.126969, acc.: 95.31%] [G loss: 1.319389]\n",
      "epoch:1 step:1448 [D loss: 0.213651, acc.: 92.97%] [G loss: 0.349390]\n",
      "epoch:1 step:1449 [D loss: 0.110262, acc.: 98.44%] [G loss: 1.550901]\n",
      "epoch:1 step:1450 [D loss: 0.059431, acc.: 96.88%] [G loss: 0.628459]\n",
      "epoch:1 step:1451 [D loss: 0.082528, acc.: 97.66%] [G loss: 1.939230]\n",
      "epoch:1 step:1452 [D loss: 0.151441, acc.: 95.31%] [G loss: 0.270058]\n",
      "epoch:1 step:1453 [D loss: 0.514040, acc.: 80.47%] [G loss: 6.951415]\n",
      "epoch:1 step:1454 [D loss: 1.751893, acc.: 47.66%] [G loss: 3.772279]\n",
      "epoch:1 step:1455 [D loss: 0.107145, acc.: 96.09%] [G loss: 4.002049]\n",
      "epoch:1 step:1456 [D loss: 0.216387, acc.: 94.53%] [G loss: 3.711777]\n",
      "epoch:1 step:1457 [D loss: 0.069785, acc.: 100.00%] [G loss: 3.663007]\n",
      "epoch:1 step:1458 [D loss: 0.069193, acc.: 99.22%] [G loss: 3.276167]\n",
      "epoch:1 step:1459 [D loss: 0.276961, acc.: 88.28%] [G loss: 4.864418]\n",
      "epoch:1 step:1460 [D loss: 0.322694, acc.: 83.59%] [G loss: 2.964476]\n",
      "epoch:1 step:1461 [D loss: 0.306757, acc.: 90.62%] [G loss: 5.578654]\n",
      "epoch:1 step:1462 [D loss: 0.125591, acc.: 96.88%] [G loss: 5.253396]\n",
      "epoch:1 step:1463 [D loss: 0.139654, acc.: 96.09%] [G loss: 3.566824]\n",
      "epoch:1 step:1464 [D loss: 0.192254, acc.: 92.19%] [G loss: 4.269516]\n",
      "epoch:1 step:1465 [D loss: 0.114478, acc.: 95.31%] [G loss: 3.255589]\n",
      "epoch:1 step:1466 [D loss: 0.220809, acc.: 92.19%] [G loss: 5.237029]\n",
      "epoch:1 step:1467 [D loss: 0.177633, acc.: 91.41%] [G loss: 2.343345]\n",
      "epoch:1 step:1468 [D loss: 0.077702, acc.: 98.44%] [G loss: 2.172454]\n",
      "epoch:1 step:1469 [D loss: 0.072397, acc.: 98.44%] [G loss: 2.097912]\n",
      "epoch:1 step:1470 [D loss: 0.106146, acc.: 96.88%] [G loss: 4.566958]\n",
      "epoch:1 step:1471 [D loss: 0.361721, acc.: 83.59%] [G loss: 1.998424]\n",
      "epoch:1 step:1472 [D loss: 0.094247, acc.: 96.09%] [G loss: 3.460855]\n",
      "epoch:1 step:1473 [D loss: 0.205248, acc.: 91.41%] [G loss: 2.761662]\n",
      "epoch:1 step:1474 [D loss: 0.052299, acc.: 99.22%] [G loss: 2.145298]\n",
      "epoch:1 step:1475 [D loss: 0.039707, acc.: 99.22%] [G loss: 1.987623]\n",
      "epoch:1 step:1476 [D loss: 0.103320, acc.: 97.66%] [G loss: 1.861728]\n",
      "epoch:1 step:1477 [D loss: 0.574263, acc.: 69.53%] [G loss: 8.587902]\n",
      "epoch:1 step:1478 [D loss: 1.284650, acc.: 56.25%] [G loss: 3.137648]\n",
      "epoch:1 step:1479 [D loss: 0.191346, acc.: 92.97%] [G loss: 3.496139]\n",
      "epoch:1 step:1480 [D loss: 0.016114, acc.: 100.00%] [G loss: 3.933188]\n",
      "epoch:1 step:1481 [D loss: 0.030530, acc.: 100.00%] [G loss: 2.997552]\n",
      "epoch:1 step:1482 [D loss: 0.081084, acc.: 96.88%] [G loss: 3.125027]\n",
      "epoch:1 step:1483 [D loss: 0.061684, acc.: 98.44%] [G loss: 3.401718]\n",
      "epoch:1 step:1484 [D loss: 0.066359, acc.: 99.22%] [G loss: 2.832562]\n",
      "epoch:1 step:1485 [D loss: 0.215675, acc.: 91.41%] [G loss: 3.297092]\n",
      "epoch:1 step:1486 [D loss: 0.065994, acc.: 99.22%] [G loss: 2.936074]\n",
      "epoch:1 step:1487 [D loss: 0.132060, acc.: 98.44%] [G loss: 2.261569]\n",
      "epoch:1 step:1488 [D loss: 0.350283, acc.: 85.16%] [G loss: 3.882288]\n",
      "epoch:1 step:1489 [D loss: 0.153218, acc.: 93.75%] [G loss: 4.028135]\n",
      "epoch:1 step:1490 [D loss: 0.232603, acc.: 92.19%] [G loss: 3.916095]\n",
      "epoch:1 step:1491 [D loss: 0.102232, acc.: 100.00%] [G loss: 3.925807]\n",
      "epoch:1 step:1492 [D loss: 0.106831, acc.: 96.09%] [G loss: 3.672863]\n",
      "epoch:1 step:1493 [D loss: 0.171862, acc.: 94.53%] [G loss: 4.437327]\n",
      "epoch:1 step:1494 [D loss: 0.108013, acc.: 96.88%] [G loss: 3.886210]\n",
      "epoch:1 step:1495 [D loss: 0.074454, acc.: 99.22%] [G loss: 3.808327]\n",
      "epoch:1 step:1496 [D loss: 0.169103, acc.: 95.31%] [G loss: 5.762459]\n",
      "epoch:1 step:1497 [D loss: 0.239699, acc.: 88.28%] [G loss: 3.916069]\n",
      "epoch:1 step:1498 [D loss: 0.066971, acc.: 98.44%] [G loss: 4.145793]\n",
      "epoch:1 step:1499 [D loss: 0.089059, acc.: 99.22%] [G loss: 3.251173]\n",
      "epoch:1 step:1500 [D loss: 0.097604, acc.: 98.44%] [G loss: 4.997222]\n",
      "epoch:1 step:1501 [D loss: 0.377824, acc.: 84.38%] [G loss: 8.252876]\n",
      "epoch:1 step:1502 [D loss: 1.371228, acc.: 48.44%] [G loss: 2.674185]\n",
      "epoch:1 step:1503 [D loss: 0.086484, acc.: 96.09%] [G loss: 3.171209]\n",
      "epoch:1 step:1504 [D loss: 0.017992, acc.: 100.00%] [G loss: 4.042937]\n",
      "epoch:1 step:1505 [D loss: 0.087889, acc.: 98.44%] [G loss: 3.619802]\n",
      "epoch:1 step:1506 [D loss: 0.417477, acc.: 76.56%] [G loss: 5.765153]\n",
      "epoch:1 step:1507 [D loss: 0.297497, acc.: 85.94%] [G loss: 4.146709]\n",
      "epoch:1 step:1508 [D loss: 0.192285, acc.: 95.31%] [G loss: 5.560299]\n",
      "epoch:1 step:1509 [D loss: 0.163843, acc.: 93.75%] [G loss: 3.315889]\n",
      "epoch:1 step:1510 [D loss: 0.050247, acc.: 100.00%] [G loss: 2.574019]\n",
      "epoch:1 step:1511 [D loss: 0.099005, acc.: 98.44%] [G loss: 2.667094]\n",
      "epoch:1 step:1512 [D loss: 0.116149, acc.: 97.66%] [G loss: 1.840685]\n",
      "epoch:1 step:1513 [D loss: 0.203572, acc.: 93.75%] [G loss: 3.284985]\n",
      "epoch:1 step:1514 [D loss: 0.155037, acc.: 96.88%] [G loss: 4.407001]\n",
      "epoch:1 step:1515 [D loss: 0.484879, acc.: 75.78%] [G loss: 5.849372]\n",
      "epoch:1 step:1516 [D loss: 0.160171, acc.: 92.19%] [G loss: 5.651965]\n",
      "epoch:1 step:1517 [D loss: 0.045648, acc.: 99.22%] [G loss: 5.208435]\n",
      "epoch:1 step:1518 [D loss: 0.031877, acc.: 100.00%] [G loss: 3.554636]\n",
      "epoch:1 step:1519 [D loss: 0.164364, acc.: 94.53%] [G loss: 5.149469]\n",
      "epoch:1 step:1520 [D loss: 0.029220, acc.: 100.00%] [G loss: 4.733612]\n",
      "epoch:1 step:1521 [D loss: 0.303765, acc.: 85.94%] [G loss: 4.157497]\n",
      "epoch:1 step:1522 [D loss: 0.235348, acc.: 90.62%] [G loss: 6.196868]\n",
      "epoch:1 step:1523 [D loss: 0.176831, acc.: 93.75%] [G loss: 5.147779]\n",
      "epoch:1 step:1524 [D loss: 0.095006, acc.: 98.44%] [G loss: 4.749056]\n",
      "epoch:1 step:1525 [D loss: 0.110513, acc.: 96.09%] [G loss: 5.795424]\n",
      "epoch:1 step:1526 [D loss: 0.042972, acc.: 100.00%] [G loss: 5.072604]\n",
      "epoch:1 step:1527 [D loss: 0.162554, acc.: 92.97%] [G loss: 3.787708]\n",
      "epoch:1 step:1528 [D loss: 0.160537, acc.: 95.31%] [G loss: 5.841373]\n",
      "epoch:1 step:1529 [D loss: 0.141609, acc.: 92.97%] [G loss: 5.086277]\n",
      "epoch:1 step:1530 [D loss: 0.271038, acc.: 85.16%] [G loss: 6.476560]\n",
      "epoch:1 step:1531 [D loss: 0.209284, acc.: 89.84%] [G loss: 5.164839]\n",
      "epoch:1 step:1532 [D loss: 0.176194, acc.: 92.97%] [G loss: 6.702240]\n",
      "epoch:1 step:1533 [D loss: 0.127364, acc.: 94.53%] [G loss: 4.573174]\n",
      "epoch:1 step:1534 [D loss: 0.083523, acc.: 97.66%] [G loss: 5.631607]\n",
      "epoch:1 step:1535 [D loss: 0.082930, acc.: 98.44%] [G loss: 4.818350]\n",
      "epoch:1 step:1536 [D loss: 0.179791, acc.: 95.31%] [G loss: 7.397702]\n",
      "epoch:1 step:1537 [D loss: 0.078192, acc.: 97.66%] [G loss: 5.889883]\n",
      "epoch:1 step:1538 [D loss: 0.118898, acc.: 97.66%] [G loss: 4.198748]\n",
      "epoch:1 step:1539 [D loss: 0.135206, acc.: 94.53%] [G loss: 4.794467]\n",
      "epoch:1 step:1540 [D loss: 0.127259, acc.: 92.97%] [G loss: 2.957744]\n",
      "epoch:1 step:1541 [D loss: 0.074049, acc.: 97.66%] [G loss: 2.846835]\n",
      "epoch:1 step:1542 [D loss: 0.021514, acc.: 99.22%] [G loss: 2.116794]\n",
      "epoch:1 step:1543 [D loss: 0.208612, acc.: 92.97%] [G loss: 0.951134]\n",
      "epoch:1 step:1544 [D loss: 0.024647, acc.: 100.00%] [G loss: 1.419564]\n",
      "epoch:1 step:1545 [D loss: 0.031132, acc.: 100.00%] [G loss: 1.534806]\n",
      "epoch:1 step:1546 [D loss: 0.068619, acc.: 98.44%] [G loss: 0.170652]\n",
      "epoch:1 step:1547 [D loss: 0.039152, acc.: 99.22%] [G loss: 0.432367]\n",
      "epoch:1 step:1548 [D loss: 0.048599, acc.: 100.00%] [G loss: 2.473490]\n",
      "epoch:1 step:1549 [D loss: 0.112469, acc.: 96.09%] [G loss: 0.573564]\n",
      "epoch:1 step:1550 [D loss: 0.171485, acc.: 94.53%] [G loss: 5.512716]\n",
      "epoch:1 step:1551 [D loss: 0.223530, acc.: 91.41%] [G loss: 1.310513]\n",
      "epoch:1 step:1552 [D loss: 0.605839, acc.: 78.12%] [G loss: 12.616489]\n",
      "epoch:1 step:1553 [D loss: 0.674365, acc.: 71.88%] [G loss: 10.024275]\n",
      "epoch:1 step:1554 [D loss: 0.072714, acc.: 96.88%] [G loss: 6.896698]\n",
      "epoch:1 step:1555 [D loss: 0.077693, acc.: 96.88%] [G loss: 6.420175]\n",
      "epoch:1 step:1556 [D loss: 0.095480, acc.: 96.88%] [G loss: 5.381548]\n",
      "epoch:1 step:1557 [D loss: 0.098796, acc.: 96.88%] [G loss: 4.939285]\n",
      "epoch:1 step:1558 [D loss: 0.436078, acc.: 84.38%] [G loss: 6.623316]\n",
      "epoch:1 step:1559 [D loss: 0.043736, acc.: 97.66%] [G loss: 7.296383]\n",
      "epoch:1 step:1560 [D loss: 0.429261, acc.: 79.69%] [G loss: 4.714351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 step:1561 [D loss: 0.046362, acc.: 99.22%] [G loss: 6.052511]\n",
      "epoch:1 step:1562 [D loss: 0.034660, acc.: 98.44%] [G loss: 5.439869]\n",
      "epoch:2 step:1563 [D loss: 0.112373, acc.: 96.09%] [G loss: 5.491399]\n",
      "epoch:2 step:1564 [D loss: 0.132242, acc.: 94.53%] [G loss: 5.976627]\n",
      "epoch:2 step:1565 [D loss: 0.195910, acc.: 91.41%] [G loss: 3.099902]\n",
      "epoch:2 step:1566 [D loss: 0.090173, acc.: 98.44%] [G loss: 5.350646]\n",
      "epoch:2 step:1567 [D loss: 0.067828, acc.: 98.44%] [G loss: 5.035054]\n",
      "epoch:2 step:1568 [D loss: 0.104550, acc.: 96.88%] [G loss: 2.921225]\n",
      "epoch:2 step:1569 [D loss: 0.153018, acc.: 95.31%] [G loss: 5.350673]\n",
      "epoch:2 step:1570 [D loss: 0.345636, acc.: 82.81%] [G loss: 0.177797]\n",
      "epoch:2 step:1571 [D loss: 0.836220, acc.: 64.84%] [G loss: 11.048423]\n",
      "epoch:2 step:1572 [D loss: 3.160246, acc.: 50.00%] [G loss: 4.918336]\n",
      "epoch:2 step:1573 [D loss: 0.473887, acc.: 78.91%] [G loss: 2.169037]\n",
      "epoch:2 step:1574 [D loss: 0.138887, acc.: 93.75%] [G loss: 1.860082]\n",
      "epoch:2 step:1575 [D loss: 0.054879, acc.: 99.22%] [G loss: 2.825804]\n",
      "epoch:2 step:1576 [D loss: 0.030694, acc.: 100.00%] [G loss: 3.178296]\n",
      "epoch:2 step:1577 [D loss: 0.046107, acc.: 100.00%] [G loss: 2.790271]\n",
      "epoch:2 step:1578 [D loss: 0.087249, acc.: 99.22%] [G loss: 2.642575]\n",
      "epoch:2 step:1579 [D loss: 0.064654, acc.: 100.00%] [G loss: 2.303737]\n",
      "epoch:2 step:1580 [D loss: 0.199866, acc.: 94.53%] [G loss: 3.265851]\n",
      "epoch:2 step:1581 [D loss: 0.105204, acc.: 97.66%] [G loss: 3.999723]\n",
      "epoch:2 step:1582 [D loss: 0.306303, acc.: 85.94%] [G loss: 3.892299]\n",
      "epoch:2 step:1583 [D loss: 0.050473, acc.: 99.22%] [G loss: 4.718405]\n",
      "epoch:2 step:1584 [D loss: 0.178868, acc.: 93.75%] [G loss: 3.233108]\n",
      "epoch:2 step:1585 [D loss: 0.140312, acc.: 96.09%] [G loss: 4.994523]\n",
      "epoch:2 step:1586 [D loss: 0.108713, acc.: 98.44%] [G loss: 4.447833]\n",
      "epoch:2 step:1587 [D loss: 0.112845, acc.: 97.66%] [G loss: 3.597644]\n",
      "epoch:2 step:1588 [D loss: 0.179067, acc.: 95.31%] [G loss: 6.030838]\n",
      "epoch:2 step:1589 [D loss: 0.205568, acc.: 89.06%] [G loss: 4.857711]\n",
      "epoch:2 step:1590 [D loss: 0.090483, acc.: 97.66%] [G loss: 4.355875]\n",
      "epoch:2 step:1591 [D loss: 0.087733, acc.: 98.44%] [G loss: 5.569467]\n",
      "epoch:2 step:1592 [D loss: 0.161489, acc.: 92.97%] [G loss: 3.414018]\n",
      "epoch:2 step:1593 [D loss: 0.286593, acc.: 85.94%] [G loss: 7.373074]\n",
      "epoch:2 step:1594 [D loss: 0.177464, acc.: 91.41%] [G loss: 5.744153]\n",
      "epoch:2 step:1595 [D loss: 0.052609, acc.: 100.00%] [G loss: 4.503100]\n",
      "epoch:2 step:1596 [D loss: 0.079441, acc.: 99.22%] [G loss: 4.211091]\n",
      "epoch:2 step:1597 [D loss: 0.159232, acc.: 98.44%] [G loss: 4.283656]\n",
      "epoch:2 step:1598 [D loss: 0.201028, acc.: 96.88%] [G loss: 4.424703]\n",
      "epoch:2 step:1599 [D loss: 0.189977, acc.: 93.75%] [G loss: 4.626164]\n",
      "epoch:2 step:1600 [D loss: 0.635376, acc.: 66.41%] [G loss: 7.002968]\n",
      "epoch:2 step:1601 [D loss: 0.240289, acc.: 88.28%] [G loss: 5.512747]\n",
      "epoch:2 step:1602 [D loss: 0.042473, acc.: 99.22%] [G loss: 4.505445]\n",
      "epoch:2 step:1603 [D loss: 0.136183, acc.: 94.53%] [G loss: 4.806493]\n",
      "epoch:2 step:1604 [D loss: 0.062124, acc.: 100.00%] [G loss: 4.571322]\n",
      "epoch:2 step:1605 [D loss: 0.106453, acc.: 100.00%] [G loss: 3.665610]\n",
      "epoch:2 step:1606 [D loss: 0.620018, acc.: 67.19%] [G loss: 8.591104]\n",
      "epoch:2 step:1607 [D loss: 0.975703, acc.: 61.72%] [G loss: 4.268316]\n",
      "epoch:2 step:1608 [D loss: 0.126475, acc.: 96.09%] [G loss: 4.449866]\n",
      "epoch:2 step:1609 [D loss: 0.025144, acc.: 99.22%] [G loss: 4.838546]\n",
      "epoch:2 step:1610 [D loss: 0.018545, acc.: 100.00%] [G loss: 3.956913]\n",
      "epoch:2 step:1611 [D loss: 0.050916, acc.: 100.00%] [G loss: 3.939283]\n",
      "epoch:2 step:1612 [D loss: 0.114734, acc.: 96.09%] [G loss: 4.222467]\n",
      "epoch:2 step:1613 [D loss: 0.136908, acc.: 93.75%] [G loss: 2.706835]\n",
      "epoch:2 step:1614 [D loss: 0.200073, acc.: 91.41%] [G loss: 3.655720]\n",
      "epoch:2 step:1615 [D loss: 0.075462, acc.: 97.66%] [G loss: 4.451286]\n",
      "epoch:2 step:1616 [D loss: 0.164026, acc.: 95.31%] [G loss: 4.811022]\n",
      "epoch:2 step:1617 [D loss: 0.128641, acc.: 96.09%] [G loss: 3.550607]\n",
      "epoch:2 step:1618 [D loss: 0.146750, acc.: 97.66%] [G loss: 5.421693]\n",
      "epoch:2 step:1619 [D loss: 0.170031, acc.: 93.75%] [G loss: 3.140002]\n",
      "epoch:2 step:1620 [D loss: 0.149092, acc.: 95.31%] [G loss: 5.900953]\n",
      "epoch:2 step:1621 [D loss: 0.168391, acc.: 96.88%] [G loss: 4.882001]\n",
      "epoch:2 step:1622 [D loss: 0.073274, acc.: 96.88%] [G loss: 4.726224]\n",
      "epoch:2 step:1623 [D loss: 0.079469, acc.: 99.22%] [G loss: 3.328180]\n",
      "epoch:2 step:1624 [D loss: 0.374563, acc.: 86.72%] [G loss: 6.709817]\n",
      "epoch:2 step:1625 [D loss: 0.790259, acc.: 61.72%] [G loss: 2.732611]\n",
      "epoch:2 step:1626 [D loss: 0.037853, acc.: 99.22%] [G loss: 1.789590]\n",
      "epoch:2 step:1627 [D loss: 0.097179, acc.: 95.31%] [G loss: 3.476855]\n",
      "epoch:2 step:1628 [D loss: 0.018530, acc.: 100.00%] [G loss: 2.335861]\n",
      "epoch:2 step:1629 [D loss: 0.205813, acc.: 92.97%] [G loss: 3.117785]\n",
      "epoch:2 step:1630 [D loss: 0.196908, acc.: 93.75%] [G loss: 2.601351]\n",
      "epoch:2 step:1631 [D loss: 0.042490, acc.: 99.22%] [G loss: 2.983561]\n",
      "epoch:2 step:1632 [D loss: 0.416337, acc.: 81.25%] [G loss: 7.367014]\n",
      "epoch:2 step:1633 [D loss: 0.501242, acc.: 74.22%] [G loss: 4.374560]\n",
      "epoch:2 step:1634 [D loss: 0.207465, acc.: 91.41%] [G loss: 6.485925]\n",
      "epoch:2 step:1635 [D loss: 0.033421, acc.: 98.44%] [G loss: 7.069511]\n",
      "epoch:2 step:1636 [D loss: 0.138361, acc.: 95.31%] [G loss: 5.174853]\n",
      "epoch:2 step:1637 [D loss: 0.061737, acc.: 97.66%] [G loss: 5.613935]\n",
      "epoch:2 step:1638 [D loss: 0.041174, acc.: 99.22%] [G loss: 5.364006]\n",
      "epoch:2 step:1639 [D loss: 0.083106, acc.: 98.44%] [G loss: 4.843617]\n",
      "epoch:2 step:1640 [D loss: 0.062905, acc.: 99.22%] [G loss: 5.115713]\n",
      "epoch:2 step:1641 [D loss: 0.120186, acc.: 97.66%] [G loss: 4.496992]\n",
      "epoch:2 step:1642 [D loss: 0.126620, acc.: 96.09%] [G loss: 2.417835]\n",
      "epoch:2 step:1643 [D loss: 0.153888, acc.: 92.19%] [G loss: 5.338945]\n",
      "epoch:2 step:1644 [D loss: 0.115703, acc.: 95.31%] [G loss: 3.720479]\n",
      "epoch:2 step:1645 [D loss: 0.056389, acc.: 99.22%] [G loss: 2.429953]\n",
      "epoch:2 step:1646 [D loss: 0.045258, acc.: 99.22%] [G loss: 2.187134]\n",
      "epoch:2 step:1647 [D loss: 0.058313, acc.: 99.22%] [G loss: 2.365109]\n",
      "epoch:2 step:1648 [D loss: 0.075922, acc.: 97.66%] [G loss: 0.844750]\n",
      "epoch:2 step:1649 [D loss: 0.072969, acc.: 98.44%] [G loss: 0.343796]\n",
      "epoch:2 step:1650 [D loss: 0.019214, acc.: 100.00%] [G loss: 0.399650]\n",
      "epoch:2 step:1651 [D loss: 0.061748, acc.: 99.22%] [G loss: 0.234350]\n",
      "epoch:2 step:1652 [D loss: 0.268373, acc.: 89.84%] [G loss: 4.950695]\n",
      "epoch:2 step:1653 [D loss: 0.047817, acc.: 98.44%] [G loss: 4.938334]\n",
      "epoch:2 step:1654 [D loss: 0.251950, acc.: 89.06%] [G loss: 0.544793]\n",
      "epoch:2 step:1655 [D loss: 1.030390, acc.: 67.19%] [G loss: 9.612014]\n",
      "epoch:2 step:1656 [D loss: 0.347834, acc.: 84.38%] [G loss: 8.893517]\n",
      "epoch:2 step:1657 [D loss: 0.741515, acc.: 68.75%] [G loss: 2.384692]\n",
      "epoch:2 step:1658 [D loss: 0.524217, acc.: 75.00%] [G loss: 6.281848]\n",
      "epoch:2 step:1659 [D loss: 0.088719, acc.: 97.66%] [G loss: 7.383121]\n",
      "epoch:2 step:1660 [D loss: 0.313745, acc.: 90.62%] [G loss: 4.966901]\n",
      "epoch:2 step:1661 [D loss: 0.066737, acc.: 99.22%] [G loss: 2.630750]\n",
      "epoch:2 step:1662 [D loss: 0.283499, acc.: 83.59%] [G loss: 4.419831]\n",
      "epoch:2 step:1663 [D loss: 0.032738, acc.: 99.22%] [G loss: 5.230954]\n",
      "epoch:2 step:1664 [D loss: 0.394141, acc.: 78.91%] [G loss: 3.355562]\n",
      "epoch:2 step:1665 [D loss: 0.144322, acc.: 96.88%] [G loss: 3.828127]\n",
      "epoch:2 step:1666 [D loss: 0.070597, acc.: 97.66%] [G loss: 3.849882]\n",
      "epoch:2 step:1667 [D loss: 0.339256, acc.: 86.72%] [G loss: 5.084677]\n",
      "epoch:2 step:1668 [D loss: 0.492344, acc.: 73.44%] [G loss: 3.523748]\n",
      "epoch:2 step:1669 [D loss: 0.130586, acc.: 95.31%] [G loss: 3.754458]\n",
      "epoch:2 step:1670 [D loss: 0.096256, acc.: 99.22%] [G loss: 2.884370]\n",
      "epoch:2 step:1671 [D loss: 0.150548, acc.: 94.53%] [G loss: 5.138235]\n",
      "epoch:2 step:1672 [D loss: 0.164409, acc.: 94.53%] [G loss: 4.328614]\n",
      "epoch:2 step:1673 [D loss: 0.091317, acc.: 97.66%] [G loss: 2.918016]\n",
      "epoch:2 step:1674 [D loss: 0.067442, acc.: 97.66%] [G loss: 3.066396]\n",
      "epoch:2 step:1675 [D loss: 0.097633, acc.: 97.66%] [G loss: 1.835496]\n",
      "epoch:2 step:1676 [D loss: 0.301228, acc.: 85.94%] [G loss: 6.253345]\n",
      "epoch:2 step:1677 [D loss: 0.222899, acc.: 89.06%] [G loss: 4.163679]\n",
      "epoch:2 step:1678 [D loss: 0.038246, acc.: 100.00%] [G loss: 2.521130]\n",
      "epoch:2 step:1679 [D loss: 0.035465, acc.: 100.00%] [G loss: 1.818286]\n",
      "epoch:2 step:1680 [D loss: 0.090181, acc.: 97.66%] [G loss: 1.895537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1681 [D loss: 0.244919, acc.: 91.41%] [G loss: 2.474741]\n",
      "epoch:2 step:1682 [D loss: 0.050100, acc.: 100.00%] [G loss: 1.488823]\n",
      "epoch:2 step:1683 [D loss: 0.057114, acc.: 99.22%] [G loss: 1.207393]\n",
      "epoch:2 step:1684 [D loss: 0.037143, acc.: 100.00%] [G loss: 0.906850]\n",
      "epoch:2 step:1685 [D loss: 0.178752, acc.: 93.75%] [G loss: 3.639364]\n",
      "epoch:2 step:1686 [D loss: 0.187915, acc.: 91.41%] [G loss: 1.596858]\n",
      "epoch:2 step:1687 [D loss: 0.045017, acc.: 100.00%] [G loss: 0.806291]\n",
      "epoch:2 step:1688 [D loss: 0.071184, acc.: 99.22%] [G loss: 1.854335]\n",
      "epoch:2 step:1689 [D loss: 0.285013, acc.: 89.84%] [G loss: 1.580739]\n",
      "epoch:2 step:1690 [D loss: 0.346431, acc.: 82.03%] [G loss: 9.270894]\n",
      "epoch:2 step:1691 [D loss: 1.046950, acc.: 61.72%] [G loss: 3.451138]\n",
      "epoch:2 step:1692 [D loss: 0.413727, acc.: 82.81%] [G loss: 6.266665]\n",
      "epoch:2 step:1693 [D loss: 0.054349, acc.: 97.66%] [G loss: 6.888939]\n",
      "epoch:2 step:1694 [D loss: 0.236271, acc.: 90.62%] [G loss: 4.486065]\n",
      "epoch:2 step:1695 [D loss: 0.109331, acc.: 96.88%] [G loss: 4.792768]\n",
      "epoch:2 step:1696 [D loss: 0.020431, acc.: 100.00%] [G loss: 5.134933]\n",
      "epoch:2 step:1697 [D loss: 0.112551, acc.: 97.66%] [G loss: 4.073221]\n",
      "epoch:2 step:1698 [D loss: 0.152527, acc.: 93.75%] [G loss: 5.182248]\n",
      "epoch:2 step:1699 [D loss: 0.134471, acc.: 96.88%] [G loss: 4.330062]\n",
      "epoch:2 step:1700 [D loss: 0.264519, acc.: 87.50%] [G loss: 6.437388]\n",
      "epoch:2 step:1701 [D loss: 0.065115, acc.: 98.44%] [G loss: 6.198231]\n",
      "epoch:2 step:1702 [D loss: 0.138105, acc.: 93.75%] [G loss: 4.497300]\n",
      "epoch:2 step:1703 [D loss: 0.227560, acc.: 89.06%] [G loss: 7.391788]\n",
      "epoch:2 step:1704 [D loss: 0.113106, acc.: 96.09%] [G loss: 7.111827]\n",
      "epoch:2 step:1705 [D loss: 0.059152, acc.: 98.44%] [G loss: 4.761182]\n",
      "epoch:2 step:1706 [D loss: 0.062698, acc.: 98.44%] [G loss: 3.795648]\n",
      "epoch:2 step:1707 [D loss: 0.119696, acc.: 96.88%] [G loss: 4.728614]\n",
      "epoch:2 step:1708 [D loss: 0.070219, acc.: 98.44%] [G loss: 4.382030]\n",
      "epoch:2 step:1709 [D loss: 0.086741, acc.: 100.00%] [G loss: 2.081845]\n",
      "epoch:2 step:1710 [D loss: 0.320817, acc.: 87.50%] [G loss: 6.959133]\n",
      "epoch:2 step:1711 [D loss: 0.694006, acc.: 66.41%] [G loss: 1.264692]\n",
      "epoch:2 step:1712 [D loss: 0.189159, acc.: 89.84%] [G loss: 3.753460]\n",
      "epoch:2 step:1713 [D loss: 0.003960, acc.: 100.00%] [G loss: 4.297865]\n",
      "epoch:2 step:1714 [D loss: 0.004620, acc.: 100.00%] [G loss: 3.350573]\n",
      "epoch:2 step:1715 [D loss: 0.020773, acc.: 100.00%] [G loss: 2.698034]\n",
      "epoch:2 step:1716 [D loss: 0.090923, acc.: 97.66%] [G loss: 3.306754]\n",
      "epoch:2 step:1717 [D loss: 0.057215, acc.: 99.22%] [G loss: 3.614427]\n",
      "epoch:2 step:1718 [D loss: 0.043316, acc.: 100.00%] [G loss: 2.515332]\n",
      "epoch:2 step:1719 [D loss: 0.081587, acc.: 97.66%] [G loss: 2.100492]\n",
      "epoch:2 step:1720 [D loss: 0.147960, acc.: 96.09%] [G loss: 2.654949]\n",
      "epoch:2 step:1721 [D loss: 0.485478, acc.: 78.12%] [G loss: 5.074942]\n",
      "epoch:2 step:1722 [D loss: 0.138435, acc.: 93.75%] [G loss: 4.671985]\n",
      "epoch:2 step:1723 [D loss: 0.060220, acc.: 97.66%] [G loss: 3.162415]\n",
      "epoch:2 step:1724 [D loss: 0.082426, acc.: 98.44%] [G loss: 3.107336]\n",
      "epoch:2 step:1725 [D loss: 0.055983, acc.: 98.44%] [G loss: 2.042286]\n",
      "epoch:2 step:1726 [D loss: 0.238139, acc.: 90.62%] [G loss: 2.904004]\n",
      "epoch:2 step:1727 [D loss: 0.146388, acc.: 92.97%] [G loss: 1.648880]\n",
      "epoch:2 step:1728 [D loss: 0.250567, acc.: 92.97%] [G loss: 3.006931]\n",
      "epoch:2 step:1729 [D loss: 0.092753, acc.: 96.88%] [G loss: 2.530013]\n",
      "epoch:2 step:1730 [D loss: 0.196629, acc.: 93.75%] [G loss: 3.234688]\n",
      "epoch:2 step:1731 [D loss: 0.130597, acc.: 93.75%] [G loss: 2.513492]\n",
      "epoch:2 step:1732 [D loss: 0.395649, acc.: 85.16%] [G loss: 6.106403]\n",
      "epoch:2 step:1733 [D loss: 0.192971, acc.: 90.62%] [G loss: 4.162572]\n",
      "epoch:2 step:1734 [D loss: 0.095780, acc.: 96.09%] [G loss: 4.444105]\n",
      "epoch:2 step:1735 [D loss: 0.039146, acc.: 100.00%] [G loss: 3.964904]\n",
      "epoch:2 step:1736 [D loss: 0.157404, acc.: 96.09%] [G loss: 5.041555]\n",
      "epoch:2 step:1737 [D loss: 0.142908, acc.: 95.31%] [G loss: 5.360235]\n",
      "epoch:2 step:1738 [D loss: 0.121649, acc.: 96.88%] [G loss: 5.748413]\n",
      "epoch:2 step:1739 [D loss: 0.015661, acc.: 100.00%] [G loss: 5.611566]\n",
      "epoch:2 step:1740 [D loss: 0.025362, acc.: 100.00%] [G loss: 5.287773]\n",
      "epoch:2 step:1741 [D loss: 0.233695, acc.: 90.62%] [G loss: 8.219929]\n",
      "epoch:2 step:1742 [D loss: 0.094512, acc.: 96.09%] [G loss: 8.688705]\n",
      "epoch:2 step:1743 [D loss: 0.083711, acc.: 97.66%] [G loss: 7.153944]\n",
      "epoch:2 step:1744 [D loss: 0.012204, acc.: 100.00%] [G loss: 6.425279]\n",
      "epoch:2 step:1745 [D loss: 0.018161, acc.: 100.00%] [G loss: 6.153918]\n",
      "epoch:2 step:1746 [D loss: 0.008387, acc.: 100.00%] [G loss: 5.768504]\n",
      "epoch:2 step:1747 [D loss: 0.050929, acc.: 99.22%] [G loss: 6.607923]\n",
      "epoch:2 step:1748 [D loss: 0.014902, acc.: 100.00%] [G loss: 6.319631]\n",
      "epoch:2 step:1749 [D loss: 0.022252, acc.: 100.00%] [G loss: 5.743184]\n",
      "epoch:2 step:1750 [D loss: 0.047134, acc.: 100.00%] [G loss: 5.924563]\n",
      "epoch:2 step:1751 [D loss: 0.017835, acc.: 100.00%] [G loss: 6.050068]\n",
      "epoch:2 step:1752 [D loss: 0.019209, acc.: 100.00%] [G loss: 5.428510]\n",
      "epoch:2 step:1753 [D loss: 0.026279, acc.: 100.00%] [G loss: 4.437427]\n",
      "epoch:2 step:1754 [D loss: 0.073441, acc.: 99.22%] [G loss: 6.236029]\n",
      "epoch:2 step:1755 [D loss: 0.061052, acc.: 99.22%] [G loss: 6.050123]\n",
      "epoch:2 step:1756 [D loss: 0.092284, acc.: 98.44%] [G loss: 3.769557]\n",
      "epoch:2 step:1757 [D loss: 0.021905, acc.: 100.00%] [G loss: 4.242061]\n",
      "epoch:2 step:1758 [D loss: 0.124389, acc.: 96.09%] [G loss: 8.335623]\n",
      "epoch:2 step:1759 [D loss: 0.086098, acc.: 97.66%] [G loss: 8.241552]\n",
      "epoch:2 step:1760 [D loss: 0.020236, acc.: 99.22%] [G loss: 6.322477]\n",
      "epoch:2 step:1761 [D loss: 0.022191, acc.: 100.00%] [G loss: 3.814862]\n",
      "epoch:2 step:1762 [D loss: 0.012646, acc.: 100.00%] [G loss: 2.200759]\n",
      "epoch:2 step:1763 [D loss: 0.066946, acc.: 99.22%] [G loss: 3.825533]\n",
      "epoch:2 step:1764 [D loss: 0.123577, acc.: 96.88%] [G loss: 0.114765]\n",
      "epoch:2 step:1765 [D loss: 0.019867, acc.: 100.00%] [G loss: 0.073665]\n",
      "epoch:2 step:1766 [D loss: 0.009698, acc.: 100.00%] [G loss: 0.168640]\n",
      "epoch:2 step:1767 [D loss: 0.037677, acc.: 100.00%] [G loss: 0.009200]\n",
      "epoch:2 step:1768 [D loss: 0.047914, acc.: 100.00%] [G loss: 3.712759]\n",
      "epoch:2 step:1769 [D loss: 0.013688, acc.: 100.00%] [G loss: 1.905168]\n",
      "epoch:2 step:1770 [D loss: 0.034313, acc.: 100.00%] [G loss: 0.072085]\n",
      "epoch:2 step:1771 [D loss: 0.080589, acc.: 97.66%] [G loss: 1.084092]\n",
      "epoch:2 step:1772 [D loss: 0.432817, acc.: 81.25%] [G loss: 4.564459]\n",
      "epoch:2 step:1773 [D loss: 0.016927, acc.: 100.00%] [G loss: 4.774072]\n",
      "epoch:2 step:1774 [D loss: 0.204059, acc.: 92.19%] [G loss: 0.166106]\n",
      "epoch:2 step:1775 [D loss: 0.140974, acc.: 92.19%] [G loss: 0.184132]\n",
      "epoch:2 step:1776 [D loss: 0.004281, acc.: 100.00%] [G loss: 1.610836]\n",
      "epoch:2 step:1777 [D loss: 0.277234, acc.: 89.84%] [G loss: 5.191006]\n",
      "epoch:2 step:1778 [D loss: 0.860605, acc.: 62.50%] [G loss: 6.033497]\n",
      "epoch:2 step:1779 [D loss: 0.172327, acc.: 94.53%] [G loss: 7.308199]\n",
      "epoch:2 step:1780 [D loss: 0.094149, acc.: 96.88%] [G loss: 5.911022]\n",
      "epoch:2 step:1781 [D loss: 0.042181, acc.: 100.00%] [G loss: 3.709781]\n",
      "epoch:2 step:1782 [D loss: 0.134658, acc.: 95.31%] [G loss: 7.633860]\n",
      "epoch:2 step:1783 [D loss: 0.117527, acc.: 93.75%] [G loss: 5.510895]\n",
      "epoch:2 step:1784 [D loss: 0.039410, acc.: 100.00%] [G loss: 2.623662]\n",
      "epoch:2 step:1785 [D loss: 0.057571, acc.: 98.44%] [G loss: 3.487985]\n",
      "epoch:2 step:1786 [D loss: 0.337005, acc.: 84.38%] [G loss: 9.356273]\n",
      "epoch:2 step:1787 [D loss: 1.516815, acc.: 53.91%] [G loss: 1.776127]\n",
      "epoch:2 step:1788 [D loss: 0.804279, acc.: 62.50%] [G loss: 9.763603]\n",
      "epoch:2 step:1789 [D loss: 0.330544, acc.: 85.16%] [G loss: 9.569602]\n",
      "epoch:2 step:1790 [D loss: 0.084669, acc.: 96.88%] [G loss: 7.152869]\n",
      "epoch:2 step:1791 [D loss: 0.021303, acc.: 100.00%] [G loss: 3.278584]\n",
      "epoch:2 step:1792 [D loss: 0.177840, acc.: 91.41%] [G loss: 3.771391]\n",
      "epoch:2 step:1793 [D loss: 0.045197, acc.: 98.44%] [G loss: 1.981213]\n",
      "epoch:2 step:1794 [D loss: 0.080747, acc.: 97.66%] [G loss: 1.061643]\n",
      "epoch:2 step:1795 [D loss: 0.040500, acc.: 100.00%] [G loss: 0.397253]\n",
      "epoch:2 step:1796 [D loss: 0.098413, acc.: 99.22%] [G loss: 0.705292]\n",
      "epoch:2 step:1797 [D loss: 0.047151, acc.: 98.44%] [G loss: 0.649899]\n",
      "epoch:2 step:1798 [D loss: 0.092983, acc.: 96.88%] [G loss: 0.120672]\n",
      "epoch:2 step:1799 [D loss: 0.318243, acc.: 82.81%] [G loss: 8.379023]\n",
      "epoch:2 step:1800 [D loss: 0.542285, acc.: 75.78%] [G loss: 3.096031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1801 [D loss: 0.041634, acc.: 99.22%] [G loss: 0.939710]\n",
      "epoch:2 step:1802 [D loss: 0.020823, acc.: 99.22%] [G loss: 0.615851]\n",
      "epoch:2 step:1803 [D loss: 0.085990, acc.: 96.88%] [G loss: 0.518772]\n",
      "epoch:2 step:1804 [D loss: 0.020839, acc.: 100.00%] [G loss: 1.659387]\n",
      "epoch:2 step:1805 [D loss: 0.020504, acc.: 100.00%] [G loss: 0.875653]\n",
      "epoch:2 step:1806 [D loss: 0.045865, acc.: 99.22%] [G loss: 0.177310]\n",
      "epoch:2 step:1807 [D loss: 0.150384, acc.: 92.97%] [G loss: 2.496905]\n",
      "epoch:2 step:1808 [D loss: 0.353219, acc.: 85.94%] [G loss: 1.404679]\n",
      "epoch:2 step:1809 [D loss: 0.147605, acc.: 95.31%] [G loss: 2.096737]\n",
      "epoch:2 step:1810 [D loss: 0.011907, acc.: 100.00%] [G loss: 1.980498]\n",
      "epoch:2 step:1811 [D loss: 0.407066, acc.: 81.25%] [G loss: 6.860340]\n",
      "epoch:2 step:1812 [D loss: 2.653181, acc.: 15.62%] [G loss: 7.507896]\n",
      "epoch:2 step:1813 [D loss: 0.577208, acc.: 80.47%] [G loss: 7.077691]\n",
      "epoch:2 step:1814 [D loss: 0.128660, acc.: 96.09%] [G loss: 4.912410]\n",
      "epoch:2 step:1815 [D loss: 0.008301, acc.: 100.00%] [G loss: 2.643089]\n",
      "epoch:2 step:1816 [D loss: 0.026208, acc.: 100.00%] [G loss: 1.389681]\n",
      "epoch:2 step:1817 [D loss: 0.239243, acc.: 88.28%] [G loss: 4.332664]\n",
      "epoch:2 step:1818 [D loss: 0.039438, acc.: 100.00%] [G loss: 4.352707]\n",
      "epoch:2 step:1819 [D loss: 0.211585, acc.: 92.19%] [G loss: 2.953781]\n",
      "epoch:2 step:1820 [D loss: 0.135440, acc.: 94.53%] [G loss: 3.524671]\n",
      "epoch:2 step:1821 [D loss: 0.067716, acc.: 99.22%] [G loss: 4.346180]\n",
      "epoch:2 step:1822 [D loss: 0.161510, acc.: 95.31%] [G loss: 3.317402]\n",
      "epoch:2 step:1823 [D loss: 0.275641, acc.: 87.50%] [G loss: 4.881624]\n",
      "epoch:2 step:1824 [D loss: 0.106744, acc.: 96.88%] [G loss: 5.817870]\n",
      "epoch:2 step:1825 [D loss: 0.039553, acc.: 100.00%] [G loss: 4.671040]\n",
      "epoch:2 step:1826 [D loss: 0.136564, acc.: 94.53%] [G loss: 5.035251]\n",
      "epoch:2 step:1827 [D loss: 0.032162, acc.: 100.00%] [G loss: 4.505824]\n",
      "epoch:2 step:1828 [D loss: 0.089375, acc.: 96.88%] [G loss: 4.261482]\n",
      "epoch:2 step:1829 [D loss: 0.141563, acc.: 97.66%] [G loss: 3.697166]\n",
      "epoch:2 step:1830 [D loss: 0.054561, acc.: 99.22%] [G loss: 3.228944]\n",
      "epoch:2 step:1831 [D loss: 0.049354, acc.: 100.00%] [G loss: 4.234367]\n",
      "epoch:2 step:1832 [D loss: 0.148423, acc.: 96.09%] [G loss: 5.792693]\n",
      "epoch:2 step:1833 [D loss: 0.316517, acc.: 81.25%] [G loss: 1.193091]\n",
      "epoch:2 step:1834 [D loss: 0.255080, acc.: 85.94%] [G loss: 6.580312]\n",
      "epoch:2 step:1835 [D loss: 0.029923, acc.: 100.00%] [G loss: 7.642663]\n",
      "epoch:2 step:1836 [D loss: 0.497704, acc.: 77.34%] [G loss: 1.494113]\n",
      "epoch:2 step:1837 [D loss: 0.287847, acc.: 82.81%] [G loss: 5.028289]\n",
      "epoch:2 step:1838 [D loss: 0.019976, acc.: 99.22%] [G loss: 6.259504]\n",
      "epoch:2 step:1839 [D loss: 0.048181, acc.: 99.22%] [G loss: 5.438569]\n",
      "epoch:2 step:1840 [D loss: 0.032239, acc.: 100.00%] [G loss: 3.415428]\n",
      "epoch:2 step:1841 [D loss: 0.025797, acc.: 100.00%] [G loss: 1.622131]\n",
      "epoch:2 step:1842 [D loss: 0.093135, acc.: 96.88%] [G loss: 1.642279]\n",
      "epoch:2 step:1843 [D loss: 0.070085, acc.: 100.00%] [G loss: 0.939696]\n",
      "epoch:2 step:1844 [D loss: 0.026938, acc.: 100.00%] [G loss: 0.958069]\n",
      "epoch:2 step:1845 [D loss: 0.314455, acc.: 89.06%] [G loss: 3.406106]\n",
      "epoch:2 step:1846 [D loss: 0.184032, acc.: 92.97%] [G loss: 1.233107]\n",
      "epoch:2 step:1847 [D loss: 0.066727, acc.: 97.66%] [G loss: 0.841202]\n",
      "epoch:2 step:1848 [D loss: 0.104125, acc.: 95.31%] [G loss: 2.907730]\n",
      "epoch:2 step:1849 [D loss: 0.055426, acc.: 98.44%] [G loss: 2.652454]\n",
      "epoch:2 step:1850 [D loss: 0.087916, acc.: 99.22%] [G loss: 1.525246]\n",
      "epoch:2 step:1851 [D loss: 0.373274, acc.: 82.81%] [G loss: 8.376349]\n",
      "epoch:2 step:1852 [D loss: 1.130799, acc.: 57.03%] [G loss: 3.305778]\n",
      "epoch:2 step:1853 [D loss: 0.206774, acc.: 91.41%] [G loss: 5.144266]\n",
      "epoch:2 step:1854 [D loss: 0.017558, acc.: 100.00%] [G loss: 5.569017]\n",
      "epoch:2 step:1855 [D loss: 0.182653, acc.: 92.97%] [G loss: 5.755219]\n",
      "epoch:2 step:1856 [D loss: 0.046506, acc.: 100.00%] [G loss: 5.292021]\n",
      "epoch:2 step:1857 [D loss: 0.084060, acc.: 97.66%] [G loss: 4.852627]\n",
      "epoch:2 step:1858 [D loss: 0.262826, acc.: 89.06%] [G loss: 7.156320]\n",
      "epoch:2 step:1859 [D loss: 0.094285, acc.: 97.66%] [G loss: 6.553495]\n",
      "epoch:2 step:1860 [D loss: 0.276869, acc.: 88.28%] [G loss: 4.876646]\n",
      "epoch:2 step:1861 [D loss: 0.038624, acc.: 98.44%] [G loss: 3.559115]\n",
      "epoch:2 step:1862 [D loss: 0.303424, acc.: 85.94%] [G loss: 3.120321]\n",
      "epoch:2 step:1863 [D loss: 0.342335, acc.: 80.47%] [G loss: 5.919000]\n",
      "epoch:2 step:1864 [D loss: 0.012661, acc.: 100.00%] [G loss: 5.158765]\n",
      "epoch:2 step:1865 [D loss: 0.041520, acc.: 99.22%] [G loss: 3.463738]\n",
      "epoch:2 step:1866 [D loss: 0.046018, acc.: 99.22%] [G loss: 1.006761]\n",
      "epoch:2 step:1867 [D loss: 0.319564, acc.: 85.94%] [G loss: 5.037132]\n",
      "epoch:2 step:1868 [D loss: 0.352594, acc.: 86.72%] [G loss: 1.692379]\n",
      "epoch:2 step:1869 [D loss: 0.195590, acc.: 94.53%] [G loss: 3.164448]\n",
      "epoch:2 step:1870 [D loss: 0.024593, acc.: 100.00%] [G loss: 4.507502]\n",
      "epoch:2 step:1871 [D loss: 1.626641, acc.: 35.16%] [G loss: 6.098837]\n",
      "epoch:2 step:1872 [D loss: 0.191040, acc.: 89.84%] [G loss: 7.354535]\n",
      "epoch:2 step:1873 [D loss: 0.706579, acc.: 63.28%] [G loss: 2.591033]\n",
      "epoch:2 step:1874 [D loss: 0.306610, acc.: 86.72%] [G loss: 2.922699]\n",
      "epoch:2 step:1875 [D loss: 0.045692, acc.: 100.00%] [G loss: 3.915170]\n",
      "epoch:2 step:1876 [D loss: 0.091832, acc.: 98.44%] [G loss: 3.563697]\n",
      "epoch:2 step:1877 [D loss: 0.271417, acc.: 91.41%] [G loss: 3.253021]\n",
      "epoch:2 step:1878 [D loss: 0.116545, acc.: 96.88%] [G loss: 3.395139]\n",
      "epoch:2 step:1879 [D loss: 0.095343, acc.: 97.66%] [G loss: 3.237540]\n",
      "epoch:2 step:1880 [D loss: 0.189417, acc.: 95.31%] [G loss: 4.240749]\n",
      "epoch:2 step:1881 [D loss: 0.154658, acc.: 97.66%] [G loss: 3.777664]\n",
      "epoch:2 step:1882 [D loss: 0.517326, acc.: 78.91%] [G loss: 5.264250]\n",
      "epoch:2 step:1883 [D loss: 0.145611, acc.: 95.31%] [G loss: 5.565737]\n",
      "epoch:2 step:1884 [D loss: 0.145464, acc.: 96.88%] [G loss: 3.999954]\n",
      "epoch:2 step:1885 [D loss: 0.083561, acc.: 98.44%] [G loss: 3.637539]\n",
      "epoch:2 step:1886 [D loss: 0.101137, acc.: 97.66%] [G loss: 4.082600]\n",
      "epoch:2 step:1887 [D loss: 0.063116, acc.: 97.66%] [G loss: 4.260510]\n",
      "epoch:2 step:1888 [D loss: 0.236151, acc.: 92.19%] [G loss: 4.983206]\n",
      "epoch:2 step:1889 [D loss: 0.161470, acc.: 93.75%] [G loss: 4.075013]\n",
      "epoch:2 step:1890 [D loss: 0.113797, acc.: 96.88%] [G loss: 3.754475]\n",
      "epoch:2 step:1891 [D loss: 0.136738, acc.: 96.09%] [G loss: 4.118995]\n",
      "epoch:2 step:1892 [D loss: 0.059978, acc.: 99.22%] [G loss: 3.360689]\n",
      "epoch:2 step:1893 [D loss: 0.299074, acc.: 89.06%] [G loss: 6.317223]\n",
      "epoch:2 step:1894 [D loss: 0.251592, acc.: 88.28%] [G loss: 4.755230]\n",
      "epoch:2 step:1895 [D loss: 0.063673, acc.: 99.22%] [G loss: 4.889883]\n",
      "epoch:2 step:1896 [D loss: 0.103960, acc.: 96.88%] [G loss: 4.427469]\n",
      "epoch:2 step:1897 [D loss: 0.050959, acc.: 99.22%] [G loss: 4.116138]\n",
      "epoch:2 step:1898 [D loss: 0.318787, acc.: 86.72%] [G loss: 5.523495]\n",
      "epoch:2 step:1899 [D loss: 0.043994, acc.: 99.22%] [G loss: 5.175700]\n",
      "epoch:2 step:1900 [D loss: 0.160834, acc.: 92.97%] [G loss: 3.679039]\n",
      "epoch:2 step:1901 [D loss: 0.072636, acc.: 99.22%] [G loss: 4.651077]\n",
      "epoch:2 step:1902 [D loss: 0.130747, acc.: 96.09%] [G loss: 3.444014]\n",
      "epoch:2 step:1903 [D loss: 0.070595, acc.: 97.66%] [G loss: 3.499514]\n",
      "epoch:2 step:1904 [D loss: 0.067553, acc.: 99.22%] [G loss: 2.327076]\n",
      "epoch:2 step:1905 [D loss: 0.326706, acc.: 87.50%] [G loss: 6.749187]\n",
      "epoch:2 step:1906 [D loss: 0.472940, acc.: 79.69%] [G loss: 2.279404]\n",
      "epoch:2 step:1907 [D loss: 0.047547, acc.: 99.22%] [G loss: 1.716475]\n",
      "epoch:2 step:1908 [D loss: 0.157657, acc.: 95.31%] [G loss: 5.337006]\n",
      "epoch:2 step:1909 [D loss: 0.103284, acc.: 97.66%] [G loss: 4.132970]\n",
      "epoch:2 step:1910 [D loss: 0.175287, acc.: 94.53%] [G loss: 0.070803]\n",
      "epoch:2 step:1911 [D loss: 0.073893, acc.: 99.22%] [G loss: 0.343160]\n",
      "epoch:2 step:1912 [D loss: 0.018937, acc.: 100.00%] [G loss: 0.583111]\n",
      "epoch:2 step:1913 [D loss: 0.029318, acc.: 100.00%] [G loss: 0.312401]\n",
      "epoch:2 step:1914 [D loss: 0.015111, acc.: 100.00%] [G loss: 0.133779]\n",
      "epoch:2 step:1915 [D loss: 0.015316, acc.: 100.00%] [G loss: 0.178986]\n",
      "epoch:2 step:1916 [D loss: 0.109026, acc.: 98.44%] [G loss: 1.891733]\n",
      "epoch:2 step:1917 [D loss: 0.225427, acc.: 90.62%] [G loss: 0.489331]\n",
      "epoch:2 step:1918 [D loss: 1.289922, acc.: 53.91%] [G loss: 10.125086]\n",
      "epoch:2 step:1919 [D loss: 1.992085, acc.: 50.00%] [G loss: 4.755836]\n",
      "epoch:2 step:1920 [D loss: 0.130878, acc.: 95.31%] [G loss: 2.849791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:1921 [D loss: 0.149725, acc.: 96.09%] [G loss: 4.480918]\n",
      "epoch:2 step:1922 [D loss: 0.114472, acc.: 97.66%] [G loss: 4.294345]\n",
      "epoch:2 step:1923 [D loss: 0.108155, acc.: 98.44%] [G loss: 4.688678]\n",
      "epoch:2 step:1924 [D loss: 0.208851, acc.: 92.19%] [G loss: 3.557244]\n",
      "epoch:2 step:1925 [D loss: 0.151737, acc.: 96.09%] [G loss: 4.116438]\n",
      "epoch:2 step:1926 [D loss: 0.087734, acc.: 99.22%] [G loss: 4.755013]\n",
      "epoch:2 step:1927 [D loss: 0.165366, acc.: 99.22%] [G loss: 4.426919]\n",
      "epoch:2 step:1928 [D loss: 0.290251, acc.: 89.06%] [G loss: 4.578192]\n",
      "epoch:2 step:1929 [D loss: 0.087872, acc.: 99.22%] [G loss: 4.954556]\n",
      "epoch:2 step:1930 [D loss: 0.137023, acc.: 97.66%] [G loss: 5.350261]\n",
      "epoch:2 step:1931 [D loss: 0.132472, acc.: 96.88%] [G loss: 3.159297]\n",
      "epoch:2 step:1932 [D loss: 0.178597, acc.: 97.66%] [G loss: 3.994920]\n",
      "epoch:2 step:1933 [D loss: 0.128762, acc.: 98.44%] [G loss: 4.438550]\n",
      "epoch:2 step:1934 [D loss: 0.159712, acc.: 96.88%] [G loss: 1.848401]\n",
      "epoch:2 step:1935 [D loss: 0.509651, acc.: 71.09%] [G loss: 8.978865]\n",
      "epoch:2 step:1936 [D loss: 1.563174, acc.: 52.34%] [G loss: 2.245548]\n",
      "epoch:2 step:1937 [D loss: 0.355245, acc.: 81.25%] [G loss: 4.950185]\n",
      "epoch:2 step:1938 [D loss: 0.040687, acc.: 100.00%] [G loss: 5.449206]\n",
      "epoch:2 step:1939 [D loss: 0.083495, acc.: 96.09%] [G loss: 3.733219]\n",
      "epoch:2 step:1940 [D loss: 0.135907, acc.: 96.88%] [G loss: 1.741938]\n",
      "epoch:2 step:1941 [D loss: 0.029942, acc.: 100.00%] [G loss: 1.306088]\n",
      "epoch:2 step:1942 [D loss: 0.148255, acc.: 97.66%] [G loss: 2.513661]\n",
      "epoch:2 step:1943 [D loss: 0.087056, acc.: 97.66%] [G loss: 2.782435]\n",
      "epoch:2 step:1944 [D loss: 0.178580, acc.: 94.53%] [G loss: 1.880985]\n",
      "epoch:2 step:1945 [D loss: 0.048009, acc.: 99.22%] [G loss: 2.231912]\n",
      "epoch:2 step:1946 [D loss: 0.267710, acc.: 90.62%] [G loss: 1.596074]\n",
      "epoch:2 step:1947 [D loss: 0.097294, acc.: 98.44%] [G loss: 2.145390]\n",
      "epoch:2 step:1948 [D loss: 0.122948, acc.: 95.31%] [G loss: 1.129046]\n",
      "epoch:2 step:1949 [D loss: 0.215886, acc.: 91.41%] [G loss: 4.758378]\n",
      "epoch:2 step:1950 [D loss: 0.460531, acc.: 75.00%] [G loss: 3.708916]\n",
      "epoch:2 step:1951 [D loss: 0.100589, acc.: 96.09%] [G loss: 5.322547]\n",
      "epoch:2 step:1952 [D loss: 0.024207, acc.: 100.00%] [G loss: 5.148061]\n",
      "epoch:2 step:1953 [D loss: 0.086020, acc.: 98.44%] [G loss: 3.876032]\n",
      "epoch:2 step:1954 [D loss: 0.123924, acc.: 97.66%] [G loss: 4.202937]\n",
      "epoch:2 step:1955 [D loss: 0.081325, acc.: 98.44%] [G loss: 4.544239]\n",
      "epoch:2 step:1956 [D loss: 0.106258, acc.: 98.44%] [G loss: 4.582347]\n",
      "epoch:2 step:1957 [D loss: 0.102692, acc.: 96.88%] [G loss: 3.614500]\n",
      "epoch:2 step:1958 [D loss: 0.114824, acc.: 98.44%] [G loss: 4.062060]\n",
      "epoch:2 step:1959 [D loss: 0.058343, acc.: 99.22%] [G loss: 4.882289]\n",
      "epoch:2 step:1960 [D loss: 0.122848, acc.: 96.88%] [G loss: 5.523788]\n",
      "epoch:2 step:1961 [D loss: 0.201697, acc.: 94.53%] [G loss: 3.141975]\n",
      "epoch:2 step:1962 [D loss: 0.149895, acc.: 95.31%] [G loss: 5.412601]\n",
      "epoch:2 step:1963 [D loss: 0.280039, acc.: 87.50%] [G loss: 2.938827]\n",
      "epoch:2 step:1964 [D loss: 0.210460, acc.: 91.41%] [G loss: 5.747172]\n",
      "epoch:2 step:1965 [D loss: 0.337422, acc.: 84.38%] [G loss: 3.082942]\n",
      "epoch:2 step:1966 [D loss: 0.072499, acc.: 98.44%] [G loss: 2.038236]\n",
      "epoch:2 step:1967 [D loss: 0.029500, acc.: 100.00%] [G loss: 1.541411]\n",
      "epoch:2 step:1968 [D loss: 0.026268, acc.: 100.00%] [G loss: 0.960172]\n",
      "epoch:2 step:1969 [D loss: 0.075918, acc.: 97.66%] [G loss: 1.458480]\n",
      "epoch:2 step:1970 [D loss: 0.958701, acc.: 46.88%] [G loss: 8.252077]\n",
      "epoch:2 step:1971 [D loss: 1.589118, acc.: 53.91%] [G loss: 3.062387]\n",
      "epoch:2 step:1972 [D loss: 0.215535, acc.: 92.19%] [G loss: 4.656741]\n",
      "epoch:2 step:1973 [D loss: 0.044649, acc.: 100.00%] [G loss: 3.501962]\n",
      "epoch:2 step:1974 [D loss: 0.072744, acc.: 97.66%] [G loss: 2.895796]\n",
      "epoch:2 step:1975 [D loss: 0.113569, acc.: 97.66%] [G loss: 2.671252]\n",
      "epoch:2 step:1976 [D loss: 0.094341, acc.: 97.66%] [G loss: 2.753515]\n",
      "epoch:2 step:1977 [D loss: 0.236237, acc.: 92.97%] [G loss: 2.505091]\n",
      "epoch:2 step:1978 [D loss: 0.127545, acc.: 98.44%] [G loss: 3.128829]\n",
      "epoch:2 step:1979 [D loss: 0.233164, acc.: 93.75%] [G loss: 3.437553]\n",
      "epoch:2 step:1980 [D loss: 0.141084, acc.: 93.75%] [G loss: 2.963293]\n",
      "epoch:2 step:1981 [D loss: 0.172244, acc.: 95.31%] [G loss: 3.815718]\n",
      "epoch:2 step:1982 [D loss: 0.219678, acc.: 92.19%] [G loss: 3.037797]\n",
      "epoch:2 step:1983 [D loss: 0.179649, acc.: 93.75%] [G loss: 2.600341]\n",
      "epoch:2 step:1984 [D loss: 0.151945, acc.: 94.53%] [G loss: 4.088326]\n",
      "epoch:2 step:1985 [D loss: 0.076403, acc.: 98.44%] [G loss: 4.043706]\n",
      "epoch:2 step:1986 [D loss: 0.691555, acc.: 63.28%] [G loss: 6.767024]\n",
      "epoch:2 step:1987 [D loss: 0.387076, acc.: 82.81%] [G loss: 4.964594]\n",
      "epoch:2 step:1988 [D loss: 0.072429, acc.: 98.44%] [G loss: 4.101560]\n",
      "epoch:2 step:1989 [D loss: 0.070582, acc.: 99.22%] [G loss: 3.834129]\n",
      "epoch:2 step:1990 [D loss: 0.110997, acc.: 96.09%] [G loss: 5.205387]\n",
      "epoch:2 step:1991 [D loss: 0.100669, acc.: 98.44%] [G loss: 3.940487]\n",
      "epoch:2 step:1992 [D loss: 0.094173, acc.: 99.22%] [G loss: 3.144027]\n",
      "epoch:2 step:1993 [D loss: 0.256866, acc.: 88.28%] [G loss: 6.510424]\n",
      "epoch:2 step:1994 [D loss: 0.373747, acc.: 85.94%] [G loss: 3.795849]\n",
      "epoch:2 step:1995 [D loss: 0.156480, acc.: 96.88%] [G loss: 3.570573]\n",
      "epoch:2 step:1996 [D loss: 0.160359, acc.: 95.31%] [G loss: 4.904893]\n",
      "epoch:2 step:1997 [D loss: 0.047137, acc.: 100.00%] [G loss: 4.420094]\n",
      "epoch:2 step:1998 [D loss: 0.240435, acc.: 88.28%] [G loss: 7.058428]\n",
      "epoch:2 step:1999 [D loss: 0.585505, acc.: 70.31%] [G loss: 3.109857]\n",
      "epoch:2 step:2000 [D loss: 0.059928, acc.: 100.00%] [G loss: 5.349196]\n",
      "epoch:2 step:2001 [D loss: 0.029533, acc.: 100.00%] [G loss: 4.397048]\n",
      "epoch:2 step:2002 [D loss: 0.042636, acc.: 99.22%] [G loss: 1.720494]\n",
      "epoch:2 step:2003 [D loss: 0.219163, acc.: 89.84%] [G loss: 7.092889]\n",
      "epoch:2 step:2004 [D loss: 0.211995, acc.: 89.06%] [G loss: 5.715388]\n",
      "epoch:2 step:2005 [D loss: 0.076134, acc.: 96.88%] [G loss: 1.139011]\n",
      "epoch:2 step:2006 [D loss: 0.145058, acc.: 93.75%] [G loss: 2.988973]\n",
      "epoch:2 step:2007 [D loss: 0.079531, acc.: 97.66%] [G loss: 1.010402]\n",
      "epoch:2 step:2008 [D loss: 0.014059, acc.: 100.00%] [G loss: 0.264828]\n",
      "epoch:2 step:2009 [D loss: 0.018835, acc.: 100.00%] [G loss: 0.059308]\n",
      "epoch:2 step:2010 [D loss: 0.089038, acc.: 97.66%] [G loss: 0.870737]\n",
      "epoch:2 step:2011 [D loss: 0.033228, acc.: 99.22%] [G loss: 1.421590]\n",
      "epoch:2 step:2012 [D loss: 0.096177, acc.: 96.09%] [G loss: 0.344476]\n",
      "epoch:2 step:2013 [D loss: 0.016269, acc.: 100.00%] [G loss: 0.145426]\n",
      "epoch:2 step:2014 [D loss: 0.061338, acc.: 99.22%] [G loss: 0.584163]\n",
      "epoch:2 step:2015 [D loss: 0.067270, acc.: 98.44%] [G loss: 0.539834]\n",
      "epoch:2 step:2016 [D loss: 0.162975, acc.: 96.09%] [G loss: 1.506207]\n",
      "epoch:2 step:2017 [D loss: 0.108820, acc.: 95.31%] [G loss: 1.411330]\n",
      "epoch:2 step:2018 [D loss: 0.092311, acc.: 97.66%] [G loss: 1.711159]\n",
      "epoch:2 step:2019 [D loss: 0.039868, acc.: 99.22%] [G loss: 2.021045]\n",
      "epoch:2 step:2020 [D loss: 0.197977, acc.: 89.06%] [G loss: 4.627486]\n",
      "epoch:2 step:2021 [D loss: 4.153883, acc.: 1.56%] [G loss: 6.686118]\n",
      "epoch:2 step:2022 [D loss: 0.575859, acc.: 75.00%] [G loss: 5.517212]\n",
      "epoch:2 step:2023 [D loss: 0.041878, acc.: 100.00%] [G loss: 4.523887]\n",
      "epoch:2 step:2024 [D loss: 0.052357, acc.: 100.00%] [G loss: 3.794350]\n",
      "epoch:2 step:2025 [D loss: 0.136431, acc.: 96.88%] [G loss: 4.159692]\n",
      "epoch:2 step:2026 [D loss: 0.062314, acc.: 100.00%] [G loss: 3.987923]\n",
      "epoch:2 step:2027 [D loss: 0.159805, acc.: 96.09%] [G loss: 4.442727]\n",
      "epoch:2 step:2028 [D loss: 0.309805, acc.: 88.28%] [G loss: 4.305208]\n",
      "epoch:2 step:2029 [D loss: 0.065722, acc.: 99.22%] [G loss: 3.603595]\n",
      "epoch:2 step:2030 [D loss: 0.093944, acc.: 99.22%] [G loss: 4.528653]\n",
      "epoch:2 step:2031 [D loss: 0.131195, acc.: 97.66%] [G loss: 4.170215]\n",
      "epoch:2 step:2032 [D loss: 0.086930, acc.: 98.44%] [G loss: 3.867349]\n",
      "epoch:2 step:2033 [D loss: 0.075136, acc.: 99.22%] [G loss: 2.109061]\n",
      "epoch:2 step:2034 [D loss: 0.115149, acc.: 96.09%] [G loss: 4.872628]\n",
      "epoch:2 step:2035 [D loss: 0.027435, acc.: 100.00%] [G loss: 4.386784]\n",
      "epoch:2 step:2036 [D loss: 0.677329, acc.: 64.84%] [G loss: 0.958269]\n",
      "epoch:2 step:2037 [D loss: 0.017161, acc.: 100.00%] [G loss: 3.201203]\n",
      "epoch:2 step:2038 [D loss: 0.041102, acc.: 100.00%] [G loss: 2.121273]\n",
      "epoch:2 step:2039 [D loss: 0.030431, acc.: 99.22%] [G loss: 0.900479]\n",
      "epoch:2 step:2040 [D loss: 0.098065, acc.: 97.66%] [G loss: 0.053993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2041 [D loss: 0.211107, acc.: 92.19%] [G loss: 1.321673]\n",
      "epoch:2 step:2042 [D loss: 0.115973, acc.: 94.53%] [G loss: 1.439275]\n",
      "epoch:2 step:2043 [D loss: 0.016533, acc.: 100.00%] [G loss: 0.111661]\n",
      "epoch:2 step:2044 [D loss: 0.031610, acc.: 100.00%] [G loss: 0.067828]\n",
      "epoch:2 step:2045 [D loss: 0.065106, acc.: 98.44%] [G loss: 0.033922]\n",
      "epoch:2 step:2046 [D loss: 0.025562, acc.: 100.00%] [G loss: 0.033932]\n",
      "epoch:2 step:2047 [D loss: 0.177260, acc.: 94.53%] [G loss: 0.945866]\n",
      "epoch:2 step:2048 [D loss: 0.050902, acc.: 100.00%] [G loss: 3.469102]\n",
      "epoch:2 step:2049 [D loss: 0.177739, acc.: 92.19%] [G loss: 0.021809]\n",
      "epoch:2 step:2050 [D loss: 0.531029, acc.: 69.53%] [G loss: 4.905139]\n",
      "epoch:2 step:2051 [D loss: 1.064340, acc.: 55.47%] [G loss: 0.683865]\n",
      "epoch:2 step:2052 [D loss: 0.070549, acc.: 99.22%] [G loss: 1.011255]\n",
      "epoch:2 step:2053 [D loss: 0.012299, acc.: 100.00%] [G loss: 0.580606]\n",
      "epoch:2 step:2054 [D loss: 0.653866, acc.: 65.62%] [G loss: 2.743960]\n",
      "epoch:2 step:2055 [D loss: 0.200188, acc.: 89.06%] [G loss: 2.845698]\n",
      "epoch:2 step:2056 [D loss: 0.401824, acc.: 78.12%] [G loss: 0.319016]\n",
      "epoch:2 step:2057 [D loss: 0.682689, acc.: 67.19%] [G loss: 3.341341]\n",
      "epoch:2 step:2058 [D loss: 0.136183, acc.: 93.75%] [G loss: 3.152562]\n",
      "epoch:2 step:2059 [D loss: 0.329895, acc.: 81.25%] [G loss: 2.289999]\n",
      "epoch:2 step:2060 [D loss: 0.113183, acc.: 97.66%] [G loss: 2.822670]\n",
      "epoch:2 step:2061 [D loss: 0.351268, acc.: 78.12%] [G loss: 4.274799]\n",
      "epoch:2 step:2062 [D loss: 0.683079, acc.: 67.19%] [G loss: 2.760832]\n",
      "epoch:2 step:2063 [D loss: 0.121019, acc.: 99.22%] [G loss: 3.655167]\n",
      "epoch:2 step:2064 [D loss: 0.120674, acc.: 97.66%] [G loss: 3.356748]\n",
      "epoch:2 step:2065 [D loss: 0.135511, acc.: 97.66%] [G loss: 3.216450]\n",
      "epoch:2 step:2066 [D loss: 0.258601, acc.: 92.97%] [G loss: 5.115106]\n",
      "epoch:2 step:2067 [D loss: 0.098784, acc.: 96.88%] [G loss: 4.183770]\n",
      "epoch:2 step:2068 [D loss: 0.198067, acc.: 92.19%] [G loss: 3.975857]\n",
      "epoch:2 step:2069 [D loss: 0.090956, acc.: 98.44%] [G loss: 3.873633]\n",
      "epoch:2 step:2070 [D loss: 0.090352, acc.: 98.44%] [G loss: 3.626780]\n",
      "epoch:2 step:2071 [D loss: 0.147433, acc.: 96.88%] [G loss: 2.472598]\n",
      "epoch:2 step:2072 [D loss: 0.094111, acc.: 98.44%] [G loss: 2.221663]\n",
      "epoch:2 step:2073 [D loss: 0.133539, acc.: 96.09%] [G loss: 1.688872]\n",
      "epoch:2 step:2074 [D loss: 0.359169, acc.: 89.06%] [G loss: 4.789836]\n",
      "epoch:2 step:2075 [D loss: 0.338651, acc.: 85.94%] [G loss: 2.791520]\n",
      "epoch:2 step:2076 [D loss: 0.084881, acc.: 97.66%] [G loss: 1.195842]\n",
      "epoch:2 step:2077 [D loss: 0.044797, acc.: 100.00%] [G loss: 1.554200]\n",
      "epoch:2 step:2078 [D loss: 0.198834, acc.: 89.84%] [G loss: 4.668558]\n",
      "epoch:2 step:2079 [D loss: 0.103286, acc.: 97.66%] [G loss: 3.093658]\n",
      "epoch:2 step:2080 [D loss: 0.057814, acc.: 100.00%] [G loss: 1.522061]\n",
      "epoch:2 step:2081 [D loss: 0.196694, acc.: 92.19%] [G loss: 2.409818]\n",
      "epoch:2 step:2082 [D loss: 0.269402, acc.: 89.84%] [G loss: 2.339164]\n",
      "epoch:2 step:2083 [D loss: 0.272059, acc.: 90.62%] [G loss: 4.536271]\n",
      "epoch:2 step:2084 [D loss: 0.364895, acc.: 85.94%] [G loss: 2.121464]\n",
      "epoch:2 step:2085 [D loss: 0.526955, acc.: 77.34%] [G loss: 7.977166]\n",
      "epoch:2 step:2086 [D loss: 0.516371, acc.: 75.00%] [G loss: 3.755282]\n",
      "epoch:2 step:2087 [D loss: 0.188029, acc.: 91.41%] [G loss: 3.373873]\n",
      "epoch:2 step:2088 [D loss: 0.112544, acc.: 96.88%] [G loss: 3.061063]\n",
      "epoch:2 step:2089 [D loss: 0.119281, acc.: 96.09%] [G loss: 2.419895]\n",
      "epoch:2 step:2090 [D loss: 0.401051, acc.: 82.03%] [G loss: 6.045255]\n",
      "epoch:2 step:2091 [D loss: 0.498588, acc.: 78.12%] [G loss: 3.771998]\n",
      "epoch:2 step:2092 [D loss: 0.122395, acc.: 96.88%] [G loss: 4.008560]\n",
      "epoch:2 step:2093 [D loss: 0.036202, acc.: 100.00%] [G loss: 3.714734]\n",
      "epoch:2 step:2094 [D loss: 0.111236, acc.: 97.66%] [G loss: 4.087915]\n",
      "epoch:2 step:2095 [D loss: 0.080844, acc.: 100.00%] [G loss: 4.419551]\n",
      "epoch:2 step:2096 [D loss: 0.124322, acc.: 96.09%] [G loss: 3.960160]\n",
      "epoch:2 step:2097 [D loss: 0.057594, acc.: 100.00%] [G loss: 3.906193]\n",
      "epoch:2 step:2098 [D loss: 0.083026, acc.: 97.66%] [G loss: 3.891363]\n",
      "epoch:2 step:2099 [D loss: 0.204203, acc.: 91.41%] [G loss: 4.957197]\n",
      "epoch:2 step:2100 [D loss: 0.111275, acc.: 96.88%] [G loss: 3.657325]\n",
      "epoch:2 step:2101 [D loss: 0.046021, acc.: 100.00%] [G loss: 3.419402]\n",
      "epoch:2 step:2102 [D loss: 0.085227, acc.: 98.44%] [G loss: 3.484417]\n",
      "epoch:2 step:2103 [D loss: 0.077413, acc.: 99.22%] [G loss: 3.810721]\n",
      "epoch:2 step:2104 [D loss: 0.225967, acc.: 93.75%] [G loss: 2.035246]\n",
      "epoch:2 step:2105 [D loss: 0.207628, acc.: 92.97%] [G loss: 6.367586]\n",
      "epoch:2 step:2106 [D loss: 0.500139, acc.: 78.12%] [G loss: 1.409815]\n",
      "epoch:2 step:2107 [D loss: 0.274352, acc.: 85.94%] [G loss: 6.535830]\n",
      "epoch:2 step:2108 [D loss: 0.145454, acc.: 92.97%] [G loss: 6.848721]\n",
      "epoch:2 step:2109 [D loss: 0.025694, acc.: 99.22%] [G loss: 5.681592]\n",
      "epoch:2 step:2110 [D loss: 0.022456, acc.: 100.00%] [G loss: 3.520551]\n",
      "epoch:2 step:2111 [D loss: 0.019478, acc.: 100.00%] [G loss: 1.570218]\n",
      "epoch:2 step:2112 [D loss: 0.125630, acc.: 93.75%] [G loss: 1.860665]\n",
      "epoch:2 step:2113 [D loss: 0.046608, acc.: 99.22%] [G loss: 2.928678]\n",
      "epoch:2 step:2114 [D loss: 0.868524, acc.: 52.34%] [G loss: 6.295712]\n",
      "epoch:2 step:2115 [D loss: 0.249783, acc.: 85.94%] [G loss: 4.900617]\n",
      "epoch:2 step:2116 [D loss: 0.140222, acc.: 96.88%] [G loss: 2.229269]\n",
      "epoch:2 step:2117 [D loss: 0.265201, acc.: 87.50%] [G loss: 5.724448]\n",
      "epoch:2 step:2118 [D loss: 0.276121, acc.: 85.16%] [G loss: 3.506037]\n",
      "epoch:2 step:2119 [D loss: 0.064530, acc.: 100.00%] [G loss: 2.229205]\n",
      "epoch:2 step:2120 [D loss: 0.027100, acc.: 100.00%] [G loss: 2.015336]\n",
      "epoch:2 step:2121 [D loss: 0.166023, acc.: 96.09%] [G loss: 3.112356]\n",
      "epoch:2 step:2122 [D loss: 0.247167, acc.: 87.50%] [G loss: 0.963349]\n",
      "epoch:2 step:2123 [D loss: 0.377622, acc.: 82.03%] [G loss: 5.787519]\n",
      "epoch:2 step:2124 [D loss: 0.347581, acc.: 83.59%] [G loss: 3.878850]\n",
      "epoch:2 step:2125 [D loss: 0.399925, acc.: 83.59%] [G loss: 3.529644]\n",
      "epoch:2 step:2126 [D loss: 0.022823, acc.: 100.00%] [G loss: 4.157454]\n",
      "epoch:2 step:2127 [D loss: 0.034389, acc.: 100.00%] [G loss: 3.556822]\n",
      "epoch:2 step:2128 [D loss: 0.066033, acc.: 99.22%] [G loss: 3.271402]\n",
      "epoch:2 step:2129 [D loss: 0.125245, acc.: 96.09%] [G loss: 4.199567]\n",
      "epoch:2 step:2130 [D loss: 0.098801, acc.: 99.22%] [G loss: 3.326970]\n",
      "epoch:2 step:2131 [D loss: 0.375633, acc.: 82.03%] [G loss: 5.404663]\n",
      "epoch:2 step:2132 [D loss: 0.349133, acc.: 85.16%] [G loss: 3.452799]\n",
      "epoch:2 step:2133 [D loss: 0.102055, acc.: 98.44%] [G loss: 4.756013]\n",
      "epoch:2 step:2134 [D loss: 0.074277, acc.: 99.22%] [G loss: 4.897250]\n",
      "epoch:2 step:2135 [D loss: 0.063491, acc.: 99.22%] [G loss: 4.448915]\n",
      "epoch:2 step:2136 [D loss: 0.156638, acc.: 95.31%] [G loss: 5.909481]\n",
      "epoch:2 step:2137 [D loss: 0.071065, acc.: 98.44%] [G loss: 5.840118]\n",
      "epoch:2 step:2138 [D loss: 0.126245, acc.: 98.44%] [G loss: 3.527531]\n",
      "epoch:2 step:2139 [D loss: 0.270191, acc.: 91.41%] [G loss: 8.092622]\n",
      "epoch:2 step:2140 [D loss: 0.424455, acc.: 81.25%] [G loss: 1.942194]\n",
      "epoch:2 step:2141 [D loss: 0.568041, acc.: 77.34%] [G loss: 8.769380]\n",
      "epoch:2 step:2142 [D loss: 0.309277, acc.: 85.94%] [G loss: 8.248915]\n",
      "epoch:2 step:2143 [D loss: 0.083432, acc.: 97.66%] [G loss: 4.935743]\n",
      "epoch:2 step:2144 [D loss: 0.023132, acc.: 100.00%] [G loss: 2.581049]\n",
      "epoch:2 step:2145 [D loss: 0.045298, acc.: 98.44%] [G loss: 1.742699]\n",
      "epoch:2 step:2146 [D loss: 0.062433, acc.: 99.22%] [G loss: 0.802321]\n",
      "epoch:2 step:2147 [D loss: 0.019774, acc.: 100.00%] [G loss: 1.124311]\n",
      "epoch:2 step:2148 [D loss: 0.050021, acc.: 98.44%] [G loss: 0.579561]\n",
      "epoch:2 step:2149 [D loss: 0.076855, acc.: 98.44%] [G loss: 0.522682]\n",
      "epoch:2 step:2150 [D loss: 0.009763, acc.: 100.00%] [G loss: 0.256633]\n",
      "epoch:2 step:2151 [D loss: 0.128900, acc.: 96.88%] [G loss: 0.595702]\n",
      "epoch:2 step:2152 [D loss: 0.128167, acc.: 96.09%] [G loss: 2.136967]\n",
      "epoch:2 step:2153 [D loss: 0.285757, acc.: 88.28%] [G loss: 0.499970]\n",
      "epoch:2 step:2154 [D loss: 0.043882, acc.: 99.22%] [G loss: 1.182243]\n",
      "epoch:2 step:2155 [D loss: 0.067733, acc.: 99.22%] [G loss: 0.628451]\n",
      "epoch:2 step:2156 [D loss: 0.034433, acc.: 99.22%] [G loss: 1.159831]\n",
      "epoch:2 step:2157 [D loss: 0.214015, acc.: 93.75%] [G loss: 3.671949]\n",
      "epoch:2 step:2158 [D loss: 0.920426, acc.: 59.38%] [G loss: 4.838970]\n",
      "epoch:2 step:2159 [D loss: 0.016726, acc.: 100.00%] [G loss: 5.611111]\n",
      "epoch:2 step:2160 [D loss: 0.126154, acc.: 94.53%] [G loss: 2.354933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2161 [D loss: 0.405021, acc.: 75.78%] [G loss: 5.022133]\n",
      "epoch:2 step:2162 [D loss: 0.236544, acc.: 88.28%] [G loss: 4.192481]\n",
      "epoch:2 step:2163 [D loss: 0.344281, acc.: 83.59%] [G loss: 4.372334]\n",
      "epoch:2 step:2164 [D loss: 0.105236, acc.: 96.88%] [G loss: 5.430419]\n",
      "epoch:2 step:2165 [D loss: 0.146656, acc.: 96.09%] [G loss: 3.453548]\n",
      "epoch:2 step:2166 [D loss: 0.302646, acc.: 89.06%] [G loss: 3.751737]\n",
      "epoch:2 step:2167 [D loss: 0.060159, acc.: 98.44%] [G loss: 4.381755]\n",
      "epoch:2 step:2168 [D loss: 0.046764, acc.: 100.00%] [G loss: 4.613348]\n",
      "epoch:2 step:2169 [D loss: 0.066693, acc.: 99.22%] [G loss: 3.238734]\n",
      "epoch:2 step:2170 [D loss: 0.115248, acc.: 97.66%] [G loss: 4.790688]\n",
      "epoch:2 step:2171 [D loss: 0.416473, acc.: 82.03%] [G loss: 5.070042]\n",
      "epoch:2 step:2172 [D loss: 0.034225, acc.: 98.44%] [G loss: 4.923796]\n",
      "epoch:2 step:2173 [D loss: 0.019833, acc.: 100.00%] [G loss: 2.246987]\n",
      "epoch:2 step:2174 [D loss: 0.033073, acc.: 100.00%] [G loss: 2.297089]\n",
      "epoch:2 step:2175 [D loss: 0.087884, acc.: 98.44%] [G loss: 1.963577]\n",
      "epoch:2 step:2176 [D loss: 0.044146, acc.: 100.00%] [G loss: 1.989635]\n",
      "epoch:2 step:2177 [D loss: 0.042943, acc.: 100.00%] [G loss: 0.135004]\n",
      "epoch:2 step:2178 [D loss: 0.016070, acc.: 100.00%] [G loss: 0.147412]\n",
      "epoch:2 step:2179 [D loss: 0.060561, acc.: 99.22%] [G loss: 0.054876]\n",
      "epoch:2 step:2180 [D loss: 0.088866, acc.: 99.22%] [G loss: 0.107729]\n",
      "epoch:2 step:2181 [D loss: 0.068528, acc.: 99.22%] [G loss: 0.198744]\n",
      "epoch:2 step:2182 [D loss: 0.099849, acc.: 96.09%] [G loss: 2.817864]\n",
      "epoch:2 step:2183 [D loss: 0.086470, acc.: 97.66%] [G loss: 1.607952]\n",
      "epoch:2 step:2184 [D loss: 0.015235, acc.: 100.00%] [G loss: 0.216435]\n",
      "epoch:2 step:2185 [D loss: 0.029406, acc.: 100.00%] [G loss: 0.183338]\n",
      "epoch:2 step:2186 [D loss: 0.056411, acc.: 99.22%] [G loss: 0.638281]\n",
      "epoch:2 step:2187 [D loss: 0.050659, acc.: 98.44%] [G loss: 0.419157]\n",
      "epoch:2 step:2188 [D loss: 0.125658, acc.: 96.09%] [G loss: 6.071168]\n",
      "epoch:2 step:2189 [D loss: 2.519706, acc.: 21.09%] [G loss: 9.361867]\n",
      "epoch:2 step:2190 [D loss: 1.101954, acc.: 61.72%] [G loss: 4.769140]\n",
      "epoch:2 step:2191 [D loss: 0.170233, acc.: 94.53%] [G loss: 4.260152]\n",
      "epoch:2 step:2192 [D loss: 0.033513, acc.: 100.00%] [G loss: 4.861065]\n",
      "epoch:2 step:2193 [D loss: 0.099127, acc.: 97.66%] [G loss: 4.819333]\n",
      "epoch:2 step:2194 [D loss: 0.158730, acc.: 93.75%] [G loss: 4.797787]\n",
      "epoch:2 step:2195 [D loss: 0.076961, acc.: 100.00%] [G loss: 3.464309]\n",
      "epoch:2 step:2196 [D loss: 0.146101, acc.: 95.31%] [G loss: 4.217029]\n",
      "epoch:2 step:2197 [D loss: 0.333415, acc.: 88.28%] [G loss: 5.396556]\n",
      "epoch:2 step:2198 [D loss: 0.256256, acc.: 90.62%] [G loss: 4.728595]\n",
      "epoch:2 step:2199 [D loss: 0.167712, acc.: 95.31%] [G loss: 4.136855]\n",
      "epoch:2 step:2200 [D loss: 0.516487, acc.: 75.78%] [G loss: 7.786031]\n",
      "epoch:2 step:2201 [D loss: 0.763700, acc.: 64.06%] [G loss: 4.374516]\n",
      "epoch:2 step:2202 [D loss: 0.151018, acc.: 92.97%] [G loss: 5.171419]\n",
      "epoch:2 step:2203 [D loss: 0.068970, acc.: 99.22%] [G loss: 5.593717]\n",
      "epoch:2 step:2204 [D loss: 0.077073, acc.: 97.66%] [G loss: 3.658024]\n",
      "epoch:2 step:2205 [D loss: 0.075406, acc.: 98.44%] [G loss: 3.314692]\n",
      "epoch:2 step:2206 [D loss: 0.161732, acc.: 94.53%] [G loss: 3.919918]\n",
      "epoch:2 step:2207 [D loss: 0.143296, acc.: 96.88%] [G loss: 2.290656]\n",
      "epoch:2 step:2208 [D loss: 0.130057, acc.: 96.88%] [G loss: 2.586026]\n",
      "epoch:2 step:2209 [D loss: 0.168948, acc.: 94.53%] [G loss: 2.406929]\n",
      "epoch:2 step:2210 [D loss: 0.206780, acc.: 92.19%] [G loss: 2.142190]\n",
      "epoch:2 step:2211 [D loss: 0.327691, acc.: 88.28%] [G loss: 0.370698]\n",
      "epoch:2 step:2212 [D loss: 0.507158, acc.: 78.91%] [G loss: 6.216033]\n",
      "epoch:2 step:2213 [D loss: 0.357623, acc.: 87.50%] [G loss: 4.940463]\n",
      "epoch:2 step:2214 [D loss: 0.144190, acc.: 92.97%] [G loss: 1.837254]\n",
      "epoch:2 step:2215 [D loss: 0.210080, acc.: 88.28%] [G loss: 3.544492]\n",
      "epoch:2 step:2216 [D loss: 0.165129, acc.: 91.41%] [G loss: 2.126543]\n",
      "epoch:2 step:2217 [D loss: 0.066707, acc.: 99.22%] [G loss: 1.825114]\n",
      "epoch:2 step:2218 [D loss: 0.072845, acc.: 99.22%] [G loss: 1.166619]\n",
      "epoch:2 step:2219 [D loss: 0.114037, acc.: 97.66%] [G loss: 1.570328]\n",
      "epoch:2 step:2220 [D loss: 0.125638, acc.: 98.44%] [G loss: 1.198665]\n",
      "epoch:2 step:2221 [D loss: 0.288963, acc.: 85.94%] [G loss: 3.444014]\n",
      "epoch:2 step:2222 [D loss: 0.399395, acc.: 80.47%] [G loss: 2.553366]\n",
      "epoch:2 step:2223 [D loss: 0.101596, acc.: 98.44%] [G loss: 2.090501]\n",
      "epoch:2 step:2224 [D loss: 0.169840, acc.: 94.53%] [G loss: 4.388369]\n",
      "epoch:2 step:2225 [D loss: 0.095734, acc.: 96.09%] [G loss: 3.744102]\n",
      "epoch:2 step:2226 [D loss: 0.196671, acc.: 95.31%] [G loss: 4.169823]\n",
      "epoch:2 step:2227 [D loss: 0.474873, acc.: 78.12%] [G loss: 6.246529]\n",
      "epoch:2 step:2228 [D loss: 0.396808, acc.: 82.81%] [G loss: 3.863100]\n",
      "epoch:2 step:2229 [D loss: 0.268866, acc.: 88.28%] [G loss: 5.125724]\n",
      "epoch:2 step:2230 [D loss: 0.085778, acc.: 98.44%] [G loss: 5.577174]\n",
      "epoch:2 step:2231 [D loss: 0.299970, acc.: 87.50%] [G loss: 5.355411]\n",
      "epoch:2 step:2232 [D loss: 0.037709, acc.: 100.00%] [G loss: 5.548487]\n",
      "epoch:2 step:2233 [D loss: 0.156299, acc.: 94.53%] [G loss: 3.603440]\n",
      "epoch:2 step:2234 [D loss: 0.281234, acc.: 87.50%] [G loss: 6.621272]\n",
      "epoch:2 step:2235 [D loss: 0.394021, acc.: 81.25%] [G loss: 3.673190]\n",
      "epoch:2 step:2236 [D loss: 0.165597, acc.: 91.41%] [G loss: 4.526691]\n",
      "epoch:2 step:2237 [D loss: 0.089228, acc.: 97.66%] [G loss: 5.372968]\n",
      "epoch:2 step:2238 [D loss: 0.036542, acc.: 98.44%] [G loss: 2.737185]\n",
      "epoch:2 step:2239 [D loss: 0.052131, acc.: 99.22%] [G loss: 1.681082]\n",
      "epoch:2 step:2240 [D loss: 0.260878, acc.: 88.28%] [G loss: 3.402901]\n",
      "epoch:2 step:2241 [D loss: 0.197248, acc.: 89.84%] [G loss: 1.859085]\n",
      "epoch:2 step:2242 [D loss: 0.212069, acc.: 90.62%] [G loss: 3.898220]\n",
      "epoch:2 step:2243 [D loss: 0.103900, acc.: 96.09%] [G loss: 3.603666]\n",
      "epoch:2 step:2244 [D loss: 0.064278, acc.: 98.44%] [G loss: 1.758635]\n",
      "epoch:2 step:2245 [D loss: 0.225745, acc.: 88.28%] [G loss: 3.279214]\n",
      "epoch:2 step:2246 [D loss: 0.092931, acc.: 97.66%] [G loss: 1.003611]\n",
      "epoch:2 step:2247 [D loss: 0.087862, acc.: 98.44%] [G loss: 0.711172]\n",
      "epoch:2 step:2248 [D loss: 0.024603, acc.: 100.00%] [G loss: 0.936391]\n",
      "epoch:2 step:2249 [D loss: 0.037423, acc.: 100.00%] [G loss: 0.067820]\n",
      "epoch:2 step:2250 [D loss: 0.077340, acc.: 98.44%] [G loss: 0.104130]\n",
      "epoch:2 step:2251 [D loss: 0.291968, acc.: 86.72%] [G loss: 5.324162]\n",
      "epoch:2 step:2252 [D loss: 0.497461, acc.: 79.69%] [G loss: 1.318007]\n",
      "epoch:2 step:2253 [D loss: 0.202110, acc.: 93.75%] [G loss: 3.335748]\n",
      "epoch:2 step:2254 [D loss: 0.077126, acc.: 99.22%] [G loss: 1.937298]\n",
      "epoch:2 step:2255 [D loss: 0.168211, acc.: 94.53%] [G loss: 3.485645]\n",
      "epoch:2 step:2256 [D loss: 0.484644, acc.: 78.12%] [G loss: 2.941460]\n",
      "epoch:2 step:2257 [D loss: 0.206081, acc.: 92.19%] [G loss: 2.248980]\n",
      "epoch:2 step:2258 [D loss: 0.162583, acc.: 94.53%] [G loss: 4.448476]\n",
      "epoch:2 step:2259 [D loss: 0.144837, acc.: 96.88%] [G loss: 3.676643]\n",
      "epoch:2 step:2260 [D loss: 0.215110, acc.: 93.75%] [G loss: 3.250794]\n",
      "epoch:2 step:2261 [D loss: 0.073981, acc.: 98.44%] [G loss: 4.568595]\n",
      "epoch:2 step:2262 [D loss: 0.180256, acc.: 93.75%] [G loss: 4.910995]\n",
      "epoch:2 step:2263 [D loss: 0.270640, acc.: 91.41%] [G loss: 5.406192]\n",
      "epoch:2 step:2264 [D loss: 0.072751, acc.: 97.66%] [G loss: 4.253588]\n",
      "epoch:2 step:2265 [D loss: 0.109753, acc.: 96.88%] [G loss: 3.401471]\n",
      "epoch:2 step:2266 [D loss: 0.167983, acc.: 93.75%] [G loss: 3.936661]\n",
      "epoch:2 step:2267 [D loss: 0.281911, acc.: 87.50%] [G loss: 3.349389]\n",
      "epoch:2 step:2268 [D loss: 0.050382, acc.: 97.66%] [G loss: 1.461892]\n",
      "epoch:2 step:2269 [D loss: 0.043721, acc.: 99.22%] [G loss: 0.859982]\n",
      "epoch:2 step:2270 [D loss: 0.013637, acc.: 100.00%] [G loss: 0.572588]\n",
      "epoch:2 step:2271 [D loss: 0.031603, acc.: 99.22%] [G loss: 0.422690]\n",
      "epoch:2 step:2272 [D loss: 0.030196, acc.: 99.22%] [G loss: 0.526384]\n",
      "epoch:2 step:2273 [D loss: 0.008153, acc.: 100.00%] [G loss: 0.362572]\n",
      "epoch:2 step:2274 [D loss: 0.033186, acc.: 99.22%] [G loss: 0.447039]\n",
      "epoch:2 step:2275 [D loss: 0.291859, acc.: 85.16%] [G loss: 4.614103]\n",
      "epoch:2 step:2276 [D loss: 0.061429, acc.: 96.88%] [G loss: 4.172780]\n",
      "epoch:2 step:2277 [D loss: 2.285317, acc.: 30.47%] [G loss: 5.178402]\n",
      "epoch:2 step:2278 [D loss: 0.095215, acc.: 96.09%] [G loss: 6.365347]\n",
      "epoch:2 step:2279 [D loss: 0.410692, acc.: 78.91%] [G loss: 0.575620]\n",
      "epoch:2 step:2280 [D loss: 0.235723, acc.: 89.06%] [G loss: 1.703037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2 step:2281 [D loss: 0.084204, acc.: 96.88%] [G loss: 1.928281]\n",
      "epoch:2 step:2282 [D loss: 0.566797, acc.: 75.00%] [G loss: 3.263702]\n",
      "epoch:2 step:2283 [D loss: 0.169305, acc.: 93.75%] [G loss: 4.124846]\n",
      "epoch:2 step:2284 [D loss: 0.745517, acc.: 62.50%] [G loss: 1.415170]\n",
      "epoch:2 step:2285 [D loss: 0.162617, acc.: 91.41%] [G loss: 2.890509]\n",
      "epoch:2 step:2286 [D loss: 0.283614, acc.: 87.50%] [G loss: 4.031034]\n",
      "epoch:2 step:2287 [D loss: 0.175791, acc.: 94.53%] [G loss: 2.371331]\n",
      "epoch:2 step:2288 [D loss: 0.274992, acc.: 90.62%] [G loss: 3.459811]\n",
      "epoch:2 step:2289 [D loss: 0.550028, acc.: 75.78%] [G loss: 3.956858]\n",
      "epoch:2 step:2290 [D loss: 0.082670, acc.: 99.22%] [G loss: 3.949462]\n",
      "epoch:2 step:2291 [D loss: 0.121745, acc.: 96.88%] [G loss: 3.706620]\n",
      "epoch:2 step:2292 [D loss: 0.244302, acc.: 90.62%] [G loss: 5.789752]\n",
      "epoch:2 step:2293 [D loss: 0.386726, acc.: 80.47%] [G loss: 4.308496]\n",
      "epoch:2 step:2294 [D loss: 0.081265, acc.: 98.44%] [G loss: 4.182741]\n",
      "epoch:2 step:2295 [D loss: 0.215636, acc.: 92.19%] [G loss: 4.556889]\n",
      "epoch:2 step:2296 [D loss: 0.055699, acc.: 98.44%] [G loss: 4.367016]\n",
      "epoch:2 step:2297 [D loss: 0.682891, acc.: 66.41%] [G loss: 7.302359]\n",
      "epoch:2 step:2298 [D loss: 0.805939, acc.: 57.03%] [G loss: 3.892425]\n",
      "epoch:2 step:2299 [D loss: 0.047491, acc.: 99.22%] [G loss: 2.637004]\n",
      "epoch:2 step:2300 [D loss: 0.064235, acc.: 99.22%] [G loss: 1.694568]\n",
      "epoch:2 step:2301 [D loss: 0.065352, acc.: 98.44%] [G loss: 2.115053]\n",
      "epoch:2 step:2302 [D loss: 0.109746, acc.: 96.09%] [G loss: 1.371174]\n",
      "epoch:2 step:2303 [D loss: 0.170621, acc.: 92.19%] [G loss: 3.320961]\n",
      "epoch:2 step:2304 [D loss: 0.075707, acc.: 98.44%] [G loss: 2.416205]\n",
      "epoch:2 step:2305 [D loss: 0.096568, acc.: 96.88%] [G loss: 0.445823]\n",
      "epoch:2 step:2306 [D loss: 0.098317, acc.: 97.66%] [G loss: 0.689381]\n",
      "epoch:2 step:2307 [D loss: 0.031404, acc.: 100.00%] [G loss: 1.343529]\n",
      "epoch:2 step:2308 [D loss: 0.177579, acc.: 92.97%] [G loss: 1.019444]\n",
      "epoch:2 step:2309 [D loss: 0.428069, acc.: 79.69%] [G loss: 1.774143]\n",
      "epoch:2 step:2310 [D loss: 0.023686, acc.: 100.00%] [G loss: 2.217195]\n",
      "epoch:2 step:2311 [D loss: 0.215470, acc.: 92.19%] [G loss: 0.614959]\n",
      "epoch:2 step:2312 [D loss: 0.169111, acc.: 94.53%] [G loss: 1.534977]\n",
      "epoch:2 step:2313 [D loss: 0.079803, acc.: 98.44%] [G loss: 2.880670]\n",
      "epoch:2 step:2314 [D loss: 0.082473, acc.: 98.44%] [G loss: 0.652873]\n",
      "epoch:2 step:2315 [D loss: 0.427457, acc.: 75.00%] [G loss: 6.502784]\n",
      "epoch:2 step:2316 [D loss: 0.464636, acc.: 75.00%] [G loss: 3.814670]\n",
      "epoch:2 step:2317 [D loss: 0.155091, acc.: 94.53%] [G loss: 3.334093]\n",
      "epoch:2 step:2318 [D loss: 0.251605, acc.: 91.41%] [G loss: 3.397209]\n",
      "epoch:2 step:2319 [D loss: 0.234429, acc.: 92.19%] [G loss: 5.710630]\n",
      "epoch:2 step:2320 [D loss: 0.327115, acc.: 84.38%] [G loss: 3.212657]\n",
      "epoch:2 step:2321 [D loss: 0.174507, acc.: 94.53%] [G loss: 4.299481]\n",
      "epoch:2 step:2322 [D loss: 0.278957, acc.: 91.41%] [G loss: 4.721713]\n",
      "epoch:2 step:2323 [D loss: 0.043000, acc.: 100.00%] [G loss: 5.994245]\n",
      "epoch:2 step:2324 [D loss: 0.175414, acc.: 93.75%] [G loss: 2.512469]\n",
      "epoch:2 step:2325 [D loss: 0.316442, acc.: 84.38%] [G loss: 6.514087]\n",
      "epoch:2 step:2326 [D loss: 0.053743, acc.: 99.22%] [G loss: 7.263199]\n",
      "epoch:2 step:2327 [D loss: 0.469598, acc.: 79.69%] [G loss: 1.317056]\n",
      "epoch:2 step:2328 [D loss: 0.391499, acc.: 80.47%] [G loss: 6.365939]\n",
      "epoch:2 step:2329 [D loss: 0.027418, acc.: 99.22%] [G loss: 7.689832]\n",
      "epoch:2 step:2330 [D loss: 0.286384, acc.: 89.06%] [G loss: 3.856095]\n",
      "epoch:2 step:2331 [D loss: 0.049132, acc.: 99.22%] [G loss: 2.861221]\n",
      "epoch:2 step:2332 [D loss: 0.084441, acc.: 99.22%] [G loss: 2.666734]\n",
      "epoch:2 step:2333 [D loss: 0.103042, acc.: 97.66%] [G loss: 3.485444]\n",
      "epoch:2 step:2334 [D loss: 0.045831, acc.: 100.00%] [G loss: 3.884250]\n",
      "epoch:2 step:2335 [D loss: 0.142630, acc.: 96.09%] [G loss: 2.350704]\n",
      "epoch:2 step:2336 [D loss: 0.163644, acc.: 94.53%] [G loss: 3.275723]\n",
      "epoch:2 step:2337 [D loss: 0.103601, acc.: 96.09%] [G loss: 1.905732]\n",
      "epoch:2 step:2338 [D loss: 0.108532, acc.: 96.09%] [G loss: 1.644220]\n",
      "epoch:2 step:2339 [D loss: 0.913978, acc.: 57.03%] [G loss: 6.608051]\n",
      "epoch:2 step:2340 [D loss: 0.321941, acc.: 83.59%] [G loss: 6.793466]\n",
      "epoch:2 step:2341 [D loss: 0.035229, acc.: 98.44%] [G loss: 5.128414]\n",
      "epoch:2 step:2342 [D loss: 0.006683, acc.: 100.00%] [G loss: 3.804214]\n",
      "epoch:2 step:2343 [D loss: 0.014036, acc.: 100.00%] [G loss: 1.652894]\n",
      "epoch:3 step:2344 [D loss: 0.307286, acc.: 89.06%] [G loss: 4.720257]\n",
      "epoch:3 step:2345 [D loss: 0.121601, acc.: 96.09%] [G loss: 4.888221]\n",
      "epoch:3 step:2346 [D loss: 0.897799, acc.: 52.34%] [G loss: 4.604594]\n",
      "epoch:3 step:2347 [D loss: 0.015587, acc.: 100.00%] [G loss: 5.491094]\n",
      "epoch:3 step:2348 [D loss: 0.091959, acc.: 99.22%] [G loss: 5.172985]\n",
      "epoch:3 step:2349 [D loss: 0.140731, acc.: 96.09%] [G loss: 4.829683]\n",
      "epoch:3 step:2350 [D loss: 0.187569, acc.: 91.41%] [G loss: 3.982293]\n",
      "epoch:3 step:2351 [D loss: 0.156222, acc.: 94.53%] [G loss: 4.275839]\n",
      "epoch:3 step:2352 [D loss: 0.505369, acc.: 74.22%] [G loss: 6.669570]\n",
      "epoch:3 step:2353 [D loss: 0.397750, acc.: 82.03%] [G loss: 4.452798]\n",
      "epoch:3 step:2354 [D loss: 0.059143, acc.: 99.22%] [G loss: 2.865526]\n",
      "epoch:3 step:2355 [D loss: 0.224790, acc.: 89.84%] [G loss: 3.603842]\n",
      "epoch:3 step:2356 [D loss: 0.132690, acc.: 94.53%] [G loss: 2.211665]\n",
      "epoch:3 step:2357 [D loss: 0.612722, acc.: 74.22%] [G loss: 6.602062]\n",
      "epoch:3 step:2358 [D loss: 0.573530, acc.: 72.66%] [G loss: 2.564198]\n",
      "epoch:3 step:2359 [D loss: 0.046183, acc.: 100.00%] [G loss: 0.519369]\n",
      "epoch:3 step:2360 [D loss: 0.041407, acc.: 100.00%] [G loss: 1.491442]\n",
      "epoch:3 step:2361 [D loss: 0.041649, acc.: 99.22%] [G loss: 0.572045]\n",
      "epoch:3 step:2362 [D loss: 0.111214, acc.: 96.88%] [G loss: 1.989447]\n",
      "epoch:3 step:2363 [D loss: 0.047472, acc.: 100.00%] [G loss: 0.222800]\n",
      "epoch:3 step:2364 [D loss: 0.066697, acc.: 99.22%] [G loss: 0.387899]\n",
      "epoch:3 step:2365 [D loss: 0.153612, acc.: 96.09%] [G loss: 1.367434]\n",
      "epoch:3 step:2366 [D loss: 0.168259, acc.: 92.19%] [G loss: 0.502679]\n",
      "epoch:3 step:2367 [D loss: 0.161017, acc.: 96.09%] [G loss: 0.671142]\n",
      "epoch:3 step:2368 [D loss: 0.050370, acc.: 98.44%] [G loss: 0.431780]\n",
      "epoch:3 step:2369 [D loss: 1.529894, acc.: 44.53%] [G loss: 8.119909]\n",
      "epoch:3 step:2370 [D loss: 1.846656, acc.: 51.56%] [G loss: 2.830072]\n",
      "epoch:3 step:2371 [D loss: 0.201669, acc.: 92.19%] [G loss: 3.451361]\n",
      "epoch:3 step:2372 [D loss: 0.082070, acc.: 98.44%] [G loss: 3.690431]\n",
      "epoch:3 step:2373 [D loss: 0.294127, acc.: 88.28%] [G loss: 2.815059]\n",
      "epoch:3 step:2374 [D loss: 0.458470, acc.: 78.91%] [G loss: 4.644022]\n",
      "epoch:3 step:2375 [D loss: 0.473488, acc.: 78.91%] [G loss: 3.430511]\n",
      "epoch:3 step:2376 [D loss: 0.162702, acc.: 96.09%] [G loss: 3.294031]\n",
      "epoch:3 step:2377 [D loss: 0.165998, acc.: 96.88%] [G loss: 3.751635]\n",
      "epoch:3 step:2378 [D loss: 0.231936, acc.: 91.41%] [G loss: 4.056211]\n",
      "epoch:3 step:2379 [D loss: 0.292638, acc.: 87.50%] [G loss: 5.173075]\n",
      "epoch:3 step:2380 [D loss: 0.170040, acc.: 96.09%] [G loss: 3.238334]\n",
      "epoch:3 step:2381 [D loss: 0.134154, acc.: 96.09%] [G loss: 2.448754]\n",
      "epoch:3 step:2382 [D loss: 0.251236, acc.: 86.72%] [G loss: 5.917373]\n",
      "epoch:3 step:2383 [D loss: 0.204827, acc.: 90.62%] [G loss: 4.123177]\n",
      "epoch:3 step:2384 [D loss: 0.542120, acc.: 75.00%] [G loss: 4.171679]\n",
      "epoch:3 step:2385 [D loss: 0.101453, acc.: 97.66%] [G loss: 3.253192]\n",
      "epoch:3 step:2386 [D loss: 0.374901, acc.: 81.25%] [G loss: 2.992534]\n",
      "epoch:3 step:2387 [D loss: 1.361900, acc.: 36.72%] [G loss: 5.054823]\n",
      "epoch:3 step:2388 [D loss: 0.074933, acc.: 97.66%] [G loss: 5.785671]\n",
      "epoch:3 step:2389 [D loss: 0.335515, acc.: 86.72%] [G loss: 3.001860]\n",
      "epoch:3 step:2390 [D loss: 0.160460, acc.: 94.53%] [G loss: 2.477178]\n",
      "epoch:3 step:2391 [D loss: 0.052261, acc.: 99.22%] [G loss: 2.431386]\n",
      "epoch:3 step:2392 [D loss: 0.123391, acc.: 97.66%] [G loss: 2.452885]\n",
      "epoch:3 step:2393 [D loss: 0.185085, acc.: 92.97%] [G loss: 2.230614]\n",
      "epoch:3 step:2394 [D loss: 0.254684, acc.: 89.06%] [G loss: 4.010548]\n",
      "epoch:3 step:2395 [D loss: 0.432969, acc.: 80.47%] [G loss: 3.028776]\n",
      "epoch:3 step:2396 [D loss: 0.626619, acc.: 71.88%] [G loss: 4.347831]\n",
      "epoch:3 step:2397 [D loss: 0.175979, acc.: 94.53%] [G loss: 4.556256]\n",
      "epoch:3 step:2398 [D loss: 0.097157, acc.: 97.66%] [G loss: 3.408510]\n",
      "epoch:3 step:2399 [D loss: 0.161287, acc.: 96.88%] [G loss: 3.113023]\n",
      "epoch:3 step:2400 [D loss: 0.094886, acc.: 99.22%] [G loss: 3.867404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2401 [D loss: 0.067834, acc.: 99.22%] [G loss: 3.625402]\n",
      "epoch:3 step:2402 [D loss: 0.187076, acc.: 93.75%] [G loss: 4.458535]\n",
      "epoch:3 step:2403 [D loss: 0.112215, acc.: 99.22%] [G loss: 4.209164]\n",
      "epoch:3 step:2404 [D loss: 0.205470, acc.: 93.75%] [G loss: 2.122118]\n",
      "epoch:3 step:2405 [D loss: 0.124235, acc.: 96.88%] [G loss: 4.340530]\n",
      "epoch:3 step:2406 [D loss: 0.114946, acc.: 95.31%] [G loss: 4.085099]\n",
      "epoch:3 step:2407 [D loss: 0.133634, acc.: 97.66%] [G loss: 2.373984]\n",
      "epoch:3 step:2408 [D loss: 0.210582, acc.: 91.41%] [G loss: 5.286416]\n",
      "epoch:3 step:2409 [D loss: 0.112434, acc.: 97.66%] [G loss: 4.270211]\n",
      "epoch:3 step:2410 [D loss: 1.094087, acc.: 47.66%] [G loss: 6.269702]\n",
      "epoch:3 step:2411 [D loss: 0.419013, acc.: 74.22%] [G loss: 4.097400]\n",
      "epoch:3 step:2412 [D loss: 0.111076, acc.: 97.66%] [G loss: 1.965346]\n",
      "epoch:3 step:2413 [D loss: 0.140040, acc.: 96.88%] [G loss: 2.561950]\n",
      "epoch:3 step:2414 [D loss: 0.271929, acc.: 90.62%] [G loss: 4.019435]\n",
      "epoch:3 step:2415 [D loss: 1.081495, acc.: 39.84%] [G loss: 4.358245]\n",
      "epoch:3 step:2416 [D loss: 0.082318, acc.: 98.44%] [G loss: 5.329492]\n",
      "epoch:3 step:2417 [D loss: 0.187656, acc.: 90.62%] [G loss: 2.437099]\n",
      "epoch:3 step:2418 [D loss: 0.163858, acc.: 94.53%] [G loss: 2.012788]\n",
      "epoch:3 step:2419 [D loss: 0.055367, acc.: 99.22%] [G loss: 1.583945]\n",
      "epoch:3 step:2420 [D loss: 0.342137, acc.: 85.16%] [G loss: 2.480101]\n",
      "epoch:3 step:2421 [D loss: 0.079933, acc.: 98.44%] [G loss: 3.128409]\n",
      "epoch:3 step:2422 [D loss: 0.668952, acc.: 65.62%] [G loss: 3.107421]\n",
      "epoch:3 step:2423 [D loss: 0.246124, acc.: 89.06%] [G loss: 2.218856]\n",
      "epoch:3 step:2424 [D loss: 0.400704, acc.: 80.47%] [G loss: 2.595184]\n",
      "epoch:3 step:2425 [D loss: 0.436421, acc.: 79.69%] [G loss: 3.020292]\n",
      "epoch:3 step:2426 [D loss: 0.741235, acc.: 58.59%] [G loss: 4.271421]\n",
      "epoch:3 step:2427 [D loss: 0.546902, acc.: 71.88%] [G loss: 2.626417]\n",
      "epoch:3 step:2428 [D loss: 0.157452, acc.: 96.88%] [G loss: 4.628282]\n",
      "epoch:3 step:2429 [D loss: 0.107268, acc.: 97.66%] [G loss: 5.538129]\n",
      "epoch:3 step:2430 [D loss: 0.269254, acc.: 88.28%] [G loss: 4.066990]\n",
      "epoch:3 step:2431 [D loss: 0.234009, acc.: 91.41%] [G loss: 4.124945]\n",
      "epoch:3 step:2432 [D loss: 0.504333, acc.: 82.03%] [G loss: 4.260989]\n",
      "epoch:3 step:2433 [D loss: 0.176144, acc.: 92.97%] [G loss: 3.835413]\n",
      "epoch:3 step:2434 [D loss: 0.116910, acc.: 97.66%] [G loss: 3.301495]\n",
      "epoch:3 step:2435 [D loss: 0.097724, acc.: 97.66%] [G loss: 3.280007]\n",
      "epoch:3 step:2436 [D loss: 0.137436, acc.: 96.88%] [G loss: 3.524691]\n",
      "epoch:3 step:2437 [D loss: 0.140478, acc.: 96.09%] [G loss: 3.660915]\n",
      "epoch:3 step:2438 [D loss: 0.231926, acc.: 92.19%] [G loss: 2.612497]\n",
      "epoch:3 step:2439 [D loss: 0.147871, acc.: 92.97%] [G loss: 0.629595]\n",
      "epoch:3 step:2440 [D loss: 0.059161, acc.: 99.22%] [G loss: 0.680262]\n",
      "epoch:3 step:2441 [D loss: 0.079091, acc.: 97.66%] [G loss: 0.325656]\n",
      "epoch:3 step:2442 [D loss: 0.030418, acc.: 99.22%] [G loss: 0.150473]\n",
      "epoch:3 step:2443 [D loss: 0.032030, acc.: 99.22%] [G loss: 0.063168]\n",
      "epoch:3 step:2444 [D loss: 0.047833, acc.: 98.44%] [G loss: 0.175210]\n",
      "epoch:3 step:2445 [D loss: 0.039077, acc.: 100.00%] [G loss: 0.264902]\n",
      "epoch:3 step:2446 [D loss: 0.067436, acc.: 98.44%] [G loss: 0.190721]\n",
      "epoch:3 step:2447 [D loss: 0.038447, acc.: 98.44%] [G loss: 0.352521]\n",
      "epoch:3 step:2448 [D loss: 0.016473, acc.: 100.00%] [G loss: 0.809915]\n",
      "epoch:3 step:2449 [D loss: 0.108565, acc.: 95.31%] [G loss: 1.830510]\n",
      "epoch:3 step:2450 [D loss: 0.356353, acc.: 85.16%] [G loss: 0.313117]\n",
      "epoch:3 step:2451 [D loss: 0.441663, acc.: 78.12%] [G loss: 8.186211]\n",
      "epoch:3 step:2452 [D loss: 1.668020, acc.: 56.25%] [G loss: 1.616840]\n",
      "epoch:3 step:2453 [D loss: 1.171163, acc.: 63.28%] [G loss: 5.382620]\n",
      "epoch:3 step:2454 [D loss: 0.072729, acc.: 97.66%] [G loss: 6.728368]\n",
      "epoch:3 step:2455 [D loss: 0.308264, acc.: 88.28%] [G loss: 3.787486]\n",
      "epoch:3 step:2456 [D loss: 0.107824, acc.: 97.66%] [G loss: 2.818597]\n",
      "epoch:3 step:2457 [D loss: 0.197668, acc.: 90.62%] [G loss: 3.520612]\n",
      "epoch:3 step:2458 [D loss: 0.094008, acc.: 99.22%] [G loss: 3.722304]\n",
      "epoch:3 step:2459 [D loss: 0.358498, acc.: 85.94%] [G loss: 3.642507]\n",
      "epoch:3 step:2460 [D loss: 0.186884, acc.: 96.88%] [G loss: 3.091417]\n",
      "epoch:3 step:2461 [D loss: 0.209821, acc.: 93.75%] [G loss: 4.739087]\n",
      "epoch:3 step:2462 [D loss: 0.166092, acc.: 93.75%] [G loss: 3.587264]\n",
      "epoch:3 step:2463 [D loss: 0.155141, acc.: 96.09%] [G loss: 3.929769]\n",
      "epoch:3 step:2464 [D loss: 0.052061, acc.: 99.22%] [G loss: 3.801571]\n",
      "epoch:3 step:2465 [D loss: 0.146151, acc.: 96.09%] [G loss: 2.492113]\n",
      "epoch:3 step:2466 [D loss: 0.069144, acc.: 98.44%] [G loss: 1.397196]\n",
      "epoch:3 step:2467 [D loss: 0.107812, acc.: 98.44%] [G loss: 1.652884]\n",
      "epoch:3 step:2468 [D loss: 0.102048, acc.: 96.09%] [G loss: 0.486239]\n",
      "epoch:3 step:2469 [D loss: 0.334335, acc.: 87.50%] [G loss: 1.150270]\n",
      "epoch:3 step:2470 [D loss: 0.128549, acc.: 96.88%] [G loss: 1.087106]\n",
      "epoch:3 step:2471 [D loss: 0.062340, acc.: 98.44%] [G loss: 0.177343]\n",
      "epoch:3 step:2472 [D loss: 0.034973, acc.: 100.00%] [G loss: 0.092101]\n",
      "epoch:3 step:2473 [D loss: 0.371640, acc.: 80.47%] [G loss: 1.892581]\n",
      "epoch:3 step:2474 [D loss: 0.158619, acc.: 93.75%] [G loss: 2.281094]\n",
      "epoch:3 step:2475 [D loss: 0.032079, acc.: 100.00%] [G loss: 0.190396]\n",
      "epoch:3 step:2476 [D loss: 0.081669, acc.: 99.22%] [G loss: 0.436894]\n",
      "epoch:3 step:2477 [D loss: 0.017871, acc.: 100.00%] [G loss: 0.787739]\n",
      "epoch:3 step:2478 [D loss: 0.174947, acc.: 93.75%] [G loss: 1.279131]\n",
      "epoch:3 step:2479 [D loss: 0.969811, acc.: 48.44%] [G loss: 5.089437]\n",
      "epoch:3 step:2480 [D loss: 0.114280, acc.: 96.09%] [G loss: 5.506451]\n",
      "epoch:3 step:2481 [D loss: 0.243838, acc.: 89.84%] [G loss: 1.534514]\n",
      "epoch:3 step:2482 [D loss: 0.271185, acc.: 87.50%] [G loss: 3.315991]\n",
      "epoch:3 step:2483 [D loss: 0.189361, acc.: 95.31%] [G loss: 4.379527]\n",
      "epoch:3 step:2484 [D loss: 0.330872, acc.: 85.94%] [G loss: 2.651350]\n",
      "epoch:3 step:2485 [D loss: 0.091212, acc.: 97.66%] [G loss: 3.310651]\n",
      "epoch:3 step:2486 [D loss: 0.113500, acc.: 96.88%] [G loss: 3.185360]\n",
      "epoch:3 step:2487 [D loss: 0.054495, acc.: 100.00%] [G loss: 3.081088]\n",
      "epoch:3 step:2488 [D loss: 0.858500, acc.: 58.59%] [G loss: 5.966710]\n",
      "epoch:3 step:2489 [D loss: 0.547395, acc.: 74.22%] [G loss: 3.749540]\n",
      "epoch:3 step:2490 [D loss: 0.127087, acc.: 95.31%] [G loss: 4.584172]\n",
      "epoch:3 step:2491 [D loss: 0.279518, acc.: 88.28%] [G loss: 5.595404]\n",
      "epoch:3 step:2492 [D loss: 0.273072, acc.: 89.84%] [G loss: 2.998585]\n",
      "epoch:3 step:2493 [D loss: 0.147500, acc.: 93.75%] [G loss: 3.756158]\n",
      "epoch:3 step:2494 [D loss: 0.316035, acc.: 83.59%] [G loss: 5.864084]\n",
      "epoch:3 step:2495 [D loss: 0.374613, acc.: 83.59%] [G loss: 2.362534]\n",
      "epoch:3 step:2496 [D loss: 0.108527, acc.: 96.88%] [G loss: 3.662088]\n",
      "epoch:3 step:2497 [D loss: 0.118476, acc.: 97.66%] [G loss: 3.578932]\n",
      "epoch:3 step:2498 [D loss: 0.107136, acc.: 98.44%] [G loss: 3.838558]\n",
      "epoch:3 step:2499 [D loss: 0.081289, acc.: 99.22%] [G loss: 1.242145]\n",
      "epoch:3 step:2500 [D loss: 0.161663, acc.: 93.75%] [G loss: 2.851507]\n",
      "epoch:3 step:2501 [D loss: 0.450771, acc.: 82.81%] [G loss: 4.649866]\n",
      "epoch:3 step:2502 [D loss: 0.663161, acc.: 68.75%] [G loss: 4.018504]\n",
      "epoch:3 step:2503 [D loss: 0.292058, acc.: 87.50%] [G loss: 2.532739]\n",
      "epoch:3 step:2504 [D loss: 0.089782, acc.: 98.44%] [G loss: 1.800473]\n",
      "epoch:3 step:2505 [D loss: 0.111366, acc.: 97.66%] [G loss: 2.165235]\n",
      "epoch:3 step:2506 [D loss: 0.091698, acc.: 98.44%] [G loss: 2.916493]\n",
      "epoch:3 step:2507 [D loss: 0.262402, acc.: 91.41%] [G loss: 2.419235]\n",
      "epoch:3 step:2508 [D loss: 0.434945, acc.: 78.12%] [G loss: 5.220773]\n",
      "epoch:3 step:2509 [D loss: 0.542904, acc.: 76.56%] [G loss: 1.334804]\n",
      "epoch:3 step:2510 [D loss: 0.203866, acc.: 95.31%] [G loss: 2.168099]\n",
      "epoch:3 step:2511 [D loss: 0.034989, acc.: 99.22%] [G loss: 2.938432]\n",
      "epoch:3 step:2512 [D loss: 0.271828, acc.: 89.84%] [G loss: 0.858897]\n",
      "epoch:3 step:2513 [D loss: 0.078073, acc.: 100.00%] [G loss: 0.632884]\n",
      "epoch:3 step:2514 [D loss: 0.238086, acc.: 90.62%] [G loss: 1.745788]\n",
      "epoch:3 step:2515 [D loss: 0.222767, acc.: 91.41%] [G loss: 0.742717]\n",
      "epoch:3 step:2516 [D loss: 0.159322, acc.: 96.88%] [G loss: 2.022571]\n",
      "epoch:3 step:2517 [D loss: 0.461203, acc.: 82.81%] [G loss: 2.001312]\n",
      "epoch:3 step:2518 [D loss: 0.169594, acc.: 89.84%] [G loss: 0.994379]\n",
      "epoch:3 step:2519 [D loss: 0.345811, acc.: 88.28%] [G loss: 2.865550]\n",
      "epoch:3 step:2520 [D loss: 0.162719, acc.: 92.97%] [G loss: 2.270266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2521 [D loss: 0.321841, acc.: 86.72%] [G loss: 0.850033]\n",
      "epoch:3 step:2522 [D loss: 0.063983, acc.: 97.66%] [G loss: 0.469279]\n",
      "epoch:3 step:2523 [D loss: 0.326435, acc.: 85.16%] [G loss: 2.599729]\n",
      "epoch:3 step:2524 [D loss: 0.072508, acc.: 98.44%] [G loss: 3.004240]\n",
      "epoch:3 step:2525 [D loss: 0.148149, acc.: 95.31%] [G loss: 4.622045]\n",
      "epoch:3 step:2526 [D loss: 0.163890, acc.: 93.75%] [G loss: 2.093979]\n",
      "epoch:3 step:2527 [D loss: 0.587163, acc.: 75.78%] [G loss: 7.170504]\n",
      "epoch:3 step:2528 [D loss: 0.800562, acc.: 67.19%] [G loss: 4.411204]\n",
      "epoch:3 step:2529 [D loss: 0.111822, acc.: 96.88%] [G loss: 3.775864]\n",
      "epoch:3 step:2530 [D loss: 0.202435, acc.: 89.84%] [G loss: 5.904372]\n",
      "epoch:3 step:2531 [D loss: 0.070871, acc.: 99.22%] [G loss: 5.735959]\n",
      "epoch:3 step:2532 [D loss: 0.055027, acc.: 100.00%] [G loss: 3.130816]\n",
      "epoch:3 step:2533 [D loss: 0.028787, acc.: 100.00%] [G loss: 2.988884]\n",
      "epoch:3 step:2534 [D loss: 0.049644, acc.: 98.44%] [G loss: 1.309521]\n",
      "epoch:3 step:2535 [D loss: 0.052803, acc.: 98.44%] [G loss: 1.389904]\n",
      "epoch:3 step:2536 [D loss: 0.030198, acc.: 100.00%] [G loss: 0.804539]\n",
      "epoch:3 step:2537 [D loss: 0.302473, acc.: 85.94%] [G loss: 5.936604]\n",
      "epoch:3 step:2538 [D loss: 0.746525, acc.: 61.72%] [G loss: 0.280755]\n",
      "epoch:3 step:2539 [D loss: 0.931101, acc.: 66.41%] [G loss: 4.471690]\n",
      "epoch:3 step:2540 [D loss: 0.163598, acc.: 93.75%] [G loss: 5.920003]\n",
      "epoch:3 step:2541 [D loss: 0.444919, acc.: 78.12%] [G loss: 1.633943]\n",
      "epoch:3 step:2542 [D loss: 0.024993, acc.: 99.22%] [G loss: 0.277316]\n",
      "epoch:3 step:2543 [D loss: 0.030652, acc.: 100.00%] [G loss: 0.164667]\n",
      "epoch:3 step:2544 [D loss: 0.025073, acc.: 100.00%] [G loss: 0.194633]\n",
      "epoch:3 step:2545 [D loss: 0.042180, acc.: 100.00%] [G loss: 0.133070]\n",
      "epoch:3 step:2546 [D loss: 0.104122, acc.: 96.88%] [G loss: 0.690610]\n",
      "epoch:3 step:2547 [D loss: 0.029975, acc.: 100.00%] [G loss: 1.239516]\n",
      "epoch:3 step:2548 [D loss: 0.305368, acc.: 85.94%] [G loss: 0.241297]\n",
      "epoch:3 step:2549 [D loss: 0.260610, acc.: 86.72%] [G loss: 2.518266]\n",
      "epoch:3 step:2550 [D loss: 0.197741, acc.: 92.19%] [G loss: 2.590935]\n",
      "epoch:3 step:2551 [D loss: 0.032728, acc.: 100.00%] [G loss: 0.741884]\n",
      "epoch:3 step:2552 [D loss: 0.453786, acc.: 79.69%] [G loss: 5.026414]\n",
      "epoch:3 step:2553 [D loss: 0.201260, acc.: 92.97%] [G loss: 4.686348]\n",
      "epoch:3 step:2554 [D loss: 0.301623, acc.: 87.50%] [G loss: 1.939388]\n",
      "epoch:3 step:2555 [D loss: 0.146538, acc.: 94.53%] [G loss: 1.106765]\n",
      "epoch:3 step:2556 [D loss: 0.388616, acc.: 84.38%] [G loss: 3.508495]\n",
      "epoch:3 step:2557 [D loss: 0.159497, acc.: 94.53%] [G loss: 3.428330]\n",
      "epoch:3 step:2558 [D loss: 2.034470, acc.: 18.75%] [G loss: 4.491660]\n",
      "epoch:3 step:2559 [D loss: 0.247779, acc.: 91.41%] [G loss: 3.197706]\n",
      "epoch:3 step:2560 [D loss: 0.379701, acc.: 82.03%] [G loss: 3.907225]\n",
      "epoch:3 step:2561 [D loss: 0.207273, acc.: 90.62%] [G loss: 3.747613]\n",
      "epoch:3 step:2562 [D loss: 0.156808, acc.: 96.09%] [G loss: 4.234640]\n",
      "epoch:3 step:2563 [D loss: 0.156901, acc.: 96.09%] [G loss: 3.632617]\n",
      "epoch:3 step:2564 [D loss: 0.064362, acc.: 100.00%] [G loss: 3.037723]\n",
      "epoch:3 step:2565 [D loss: 0.168726, acc.: 92.97%] [G loss: 3.206500]\n",
      "epoch:3 step:2566 [D loss: 0.218055, acc.: 94.53%] [G loss: 2.758593]\n",
      "epoch:3 step:2567 [D loss: 0.106423, acc.: 96.09%] [G loss: 2.570550]\n",
      "epoch:3 step:2568 [D loss: 0.157978, acc.: 92.97%] [G loss: 0.666087]\n",
      "epoch:3 step:2569 [D loss: 0.151430, acc.: 96.88%] [G loss: 0.802409]\n",
      "epoch:3 step:2570 [D loss: 0.036246, acc.: 100.00%] [G loss: 0.820863]\n",
      "epoch:3 step:2571 [D loss: 0.054691, acc.: 99.22%] [G loss: 0.268846]\n",
      "epoch:3 step:2572 [D loss: 0.043673, acc.: 100.00%] [G loss: 0.208441]\n",
      "epoch:3 step:2573 [D loss: 0.124444, acc.: 96.88%] [G loss: 0.472645]\n",
      "epoch:3 step:2574 [D loss: 0.035159, acc.: 99.22%] [G loss: 1.086120]\n",
      "epoch:3 step:2575 [D loss: 0.053161, acc.: 97.66%] [G loss: 0.284268]\n",
      "epoch:3 step:2576 [D loss: 0.062633, acc.: 98.44%] [G loss: 0.378846]\n",
      "epoch:3 step:2577 [D loss: 0.070504, acc.: 98.44%] [G loss: 0.283557]\n",
      "epoch:3 step:2578 [D loss: 0.069688, acc.: 98.44%] [G loss: 0.385708]\n",
      "epoch:3 step:2579 [D loss: 0.460356, acc.: 80.47%] [G loss: 1.193703]\n",
      "epoch:3 step:2580 [D loss: 0.034249, acc.: 98.44%] [G loss: 1.998054]\n",
      "epoch:3 step:2581 [D loss: 0.027632, acc.: 100.00%] [G loss: 0.359789]\n",
      "epoch:3 step:2582 [D loss: 0.019139, acc.: 99.22%] [G loss: 0.164770]\n",
      "epoch:3 step:2583 [D loss: 0.414521, acc.: 82.03%] [G loss: 0.389382]\n",
      "epoch:3 step:2584 [D loss: 0.047095, acc.: 99.22%] [G loss: 2.181900]\n",
      "epoch:3 step:2585 [D loss: 0.017672, acc.: 100.00%] [G loss: 1.698895]\n",
      "epoch:3 step:2586 [D loss: 0.057618, acc.: 99.22%] [G loss: 0.290218]\n",
      "epoch:3 step:2587 [D loss: 0.044510, acc.: 99.22%] [G loss: 0.218218]\n",
      "epoch:3 step:2588 [D loss: 0.206934, acc.: 90.62%] [G loss: 2.259322]\n",
      "epoch:3 step:2589 [D loss: 1.692337, acc.: 33.59%] [G loss: 3.284909]\n",
      "epoch:3 step:2590 [D loss: 0.277791, acc.: 87.50%] [G loss: 2.692876]\n",
      "epoch:3 step:2591 [D loss: 0.210107, acc.: 90.62%] [G loss: 3.274465]\n",
      "epoch:3 step:2592 [D loss: 0.122521, acc.: 96.09%] [G loss: 3.232849]\n",
      "epoch:3 step:2593 [D loss: 0.148751, acc.: 94.53%] [G loss: 2.098952]\n",
      "epoch:3 step:2594 [D loss: 0.340571, acc.: 86.72%] [G loss: 4.482712]\n",
      "epoch:3 step:2595 [D loss: 0.285347, acc.: 85.94%] [G loss: 2.967599]\n",
      "epoch:3 step:2596 [D loss: 0.592103, acc.: 70.31%] [G loss: 4.722785]\n",
      "epoch:3 step:2597 [D loss: 0.177214, acc.: 91.41%] [G loss: 4.605632]\n",
      "epoch:3 step:2598 [D loss: 0.413926, acc.: 82.03%] [G loss: 3.623032]\n",
      "epoch:3 step:2599 [D loss: 0.217921, acc.: 92.19%] [G loss: 3.819353]\n",
      "epoch:3 step:2600 [D loss: 0.157500, acc.: 96.09%] [G loss: 3.831842]\n",
      "epoch:3 step:2601 [D loss: 0.111131, acc.: 98.44%] [G loss: 3.348634]\n",
      "epoch:3 step:2602 [D loss: 0.264084, acc.: 89.06%] [G loss: 2.932225]\n",
      "epoch:3 step:2603 [D loss: 0.199419, acc.: 93.75%] [G loss: 4.198583]\n",
      "epoch:3 step:2604 [D loss: 0.179676, acc.: 95.31%] [G loss: 3.693924]\n",
      "epoch:3 step:2605 [D loss: 0.054850, acc.: 100.00%] [G loss: 2.184376]\n",
      "epoch:3 step:2606 [D loss: 0.084497, acc.: 98.44%] [G loss: 1.568977]\n",
      "epoch:3 step:2607 [D loss: 0.136769, acc.: 95.31%] [G loss: 0.560798]\n",
      "epoch:3 step:2608 [D loss: 0.044089, acc.: 99.22%] [G loss: 0.646740]\n",
      "epoch:3 step:2609 [D loss: 0.048500, acc.: 99.22%] [G loss: 1.235839]\n",
      "epoch:3 step:2610 [D loss: 1.719829, acc.: 34.38%] [G loss: 5.565355]\n",
      "epoch:3 step:2611 [D loss: 0.388946, acc.: 83.59%] [G loss: 5.273016]\n",
      "epoch:3 step:2612 [D loss: 0.219004, acc.: 89.84%] [G loss: 1.741396]\n",
      "epoch:3 step:2613 [D loss: 0.095847, acc.: 96.88%] [G loss: 1.631881]\n",
      "epoch:3 step:2614 [D loss: 0.073286, acc.: 98.44%] [G loss: 2.037251]\n",
      "epoch:3 step:2615 [D loss: 0.095509, acc.: 97.66%] [G loss: 2.039178]\n",
      "epoch:3 step:2616 [D loss: 0.092485, acc.: 98.44%] [G loss: 1.863606]\n",
      "epoch:3 step:2617 [D loss: 0.798700, acc.: 58.59%] [G loss: 4.232087]\n",
      "epoch:3 step:2618 [D loss: 0.310248, acc.: 82.03%] [G loss: 3.661721]\n",
      "epoch:3 step:2619 [D loss: 0.287799, acc.: 87.50%] [G loss: 3.167108]\n",
      "epoch:3 step:2620 [D loss: 0.052192, acc.: 98.44%] [G loss: 2.506642]\n",
      "epoch:3 step:2621 [D loss: 0.197496, acc.: 92.97%] [G loss: 1.482374]\n",
      "epoch:3 step:2622 [D loss: 0.205859, acc.: 96.09%] [G loss: 2.688719]\n",
      "epoch:3 step:2623 [D loss: 0.454829, acc.: 78.12%] [G loss: 2.363226]\n",
      "epoch:3 step:2624 [D loss: 0.134494, acc.: 96.09%] [G loss: 2.419950]\n",
      "epoch:3 step:2625 [D loss: 0.059494, acc.: 97.66%] [G loss: 1.787437]\n",
      "epoch:3 step:2626 [D loss: 0.124095, acc.: 98.44%] [G loss: 1.997282]\n",
      "epoch:3 step:2627 [D loss: 0.210015, acc.: 93.75%] [G loss: 3.815453]\n",
      "epoch:3 step:2628 [D loss: 0.349705, acc.: 85.16%] [G loss: 2.073217]\n",
      "epoch:3 step:2629 [D loss: 0.069706, acc.: 99.22%] [G loss: 1.178766]\n",
      "epoch:3 step:2630 [D loss: 0.081278, acc.: 98.44%] [G loss: 1.726726]\n",
      "epoch:3 step:2631 [D loss: 0.063790, acc.: 99.22%] [G loss: 2.343411]\n",
      "epoch:3 step:2632 [D loss: 0.309338, acc.: 86.72%] [G loss: 4.067722]\n",
      "epoch:3 step:2633 [D loss: 0.141717, acc.: 96.09%] [G loss: 3.631318]\n",
      "epoch:3 step:2634 [D loss: 0.337025, acc.: 84.38%] [G loss: 1.282272]\n",
      "epoch:3 step:2635 [D loss: 0.091702, acc.: 97.66%] [G loss: 3.139251]\n",
      "epoch:3 step:2636 [D loss: 0.108519, acc.: 97.66%] [G loss: 2.525673]\n",
      "epoch:3 step:2637 [D loss: 0.353525, acc.: 83.59%] [G loss: 6.094101]\n",
      "epoch:3 step:2638 [D loss: 0.202463, acc.: 92.19%] [G loss: 4.537199]\n",
      "epoch:3 step:2639 [D loss: 0.180853, acc.: 95.31%] [G loss: 3.384761]\n",
      "epoch:3 step:2640 [D loss: 0.128477, acc.: 98.44%] [G loss: 4.985797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2641 [D loss: 0.278943, acc.: 87.50%] [G loss: 4.185629]\n",
      "epoch:3 step:2642 [D loss: 0.130519, acc.: 96.09%] [G loss: 5.027995]\n",
      "epoch:3 step:2643 [D loss: 0.092747, acc.: 98.44%] [G loss: 4.488660]\n",
      "epoch:3 step:2644 [D loss: 0.660408, acc.: 64.06%] [G loss: 5.687397]\n",
      "epoch:3 step:2645 [D loss: 0.033074, acc.: 100.00%] [G loss: 6.460377]\n",
      "epoch:3 step:2646 [D loss: 0.158634, acc.: 92.97%] [G loss: 4.782011]\n",
      "epoch:3 step:2647 [D loss: 0.318920, acc.: 82.81%] [G loss: 6.989989]\n",
      "epoch:3 step:2648 [D loss: 0.290563, acc.: 84.38%] [G loss: 4.505429]\n",
      "epoch:3 step:2649 [D loss: 0.074749, acc.: 98.44%] [G loss: 1.754950]\n",
      "epoch:3 step:2650 [D loss: 0.024751, acc.: 100.00%] [G loss: 1.877518]\n",
      "epoch:3 step:2651 [D loss: 0.078088, acc.: 96.88%] [G loss: 2.393631]\n",
      "epoch:3 step:2652 [D loss: 0.134881, acc.: 96.09%] [G loss: 1.497636]\n",
      "epoch:3 step:2653 [D loss: 0.031669, acc.: 100.00%] [G loss: 1.017837]\n",
      "epoch:3 step:2654 [D loss: 0.076249, acc.: 98.44%] [G loss: 1.092768]\n",
      "epoch:3 step:2655 [D loss: 0.044638, acc.: 99.22%] [G loss: 0.626537]\n",
      "epoch:3 step:2656 [D loss: 0.047905, acc.: 100.00%] [G loss: 0.502062]\n",
      "epoch:3 step:2657 [D loss: 0.056605, acc.: 99.22%] [G loss: 0.423509]\n",
      "epoch:3 step:2658 [D loss: 2.229116, acc.: 24.22%] [G loss: 6.536151]\n",
      "epoch:3 step:2659 [D loss: 0.285510, acc.: 88.28%] [G loss: 8.261171]\n",
      "epoch:3 step:2660 [D loss: 0.492712, acc.: 75.00%] [G loss: 2.842386]\n",
      "epoch:3 step:2661 [D loss: 0.060688, acc.: 98.44%] [G loss: 1.614868]\n",
      "epoch:3 step:2662 [D loss: 0.319383, acc.: 83.59%] [G loss: 4.159047]\n",
      "epoch:3 step:2663 [D loss: 0.334943, acc.: 82.81%] [G loss: 2.714540]\n",
      "epoch:3 step:2664 [D loss: 0.269698, acc.: 85.94%] [G loss: 2.321663]\n",
      "epoch:3 step:2665 [D loss: 0.330369, acc.: 85.16%] [G loss: 4.998930]\n",
      "epoch:3 step:2666 [D loss: 0.296395, acc.: 87.50%] [G loss: 3.780240]\n",
      "epoch:3 step:2667 [D loss: 0.106485, acc.: 98.44%] [G loss: 3.521537]\n",
      "epoch:3 step:2668 [D loss: 0.267293, acc.: 91.41%] [G loss: 3.238367]\n",
      "epoch:3 step:2669 [D loss: 0.218766, acc.: 90.62%] [G loss: 4.114037]\n",
      "epoch:3 step:2670 [D loss: 0.115544, acc.: 97.66%] [G loss: 4.029312]\n",
      "epoch:3 step:2671 [D loss: 0.082409, acc.: 99.22%] [G loss: 3.056408]\n",
      "epoch:3 step:2672 [D loss: 0.091327, acc.: 100.00%] [G loss: 3.196548]\n",
      "epoch:3 step:2673 [D loss: 0.166138, acc.: 94.53%] [G loss: 3.629493]\n",
      "epoch:3 step:2674 [D loss: 0.107972, acc.: 98.44%] [G loss: 2.861687]\n",
      "epoch:3 step:2675 [D loss: 0.090023, acc.: 97.66%] [G loss: 2.079604]\n",
      "epoch:3 step:2676 [D loss: 0.251473, acc.: 89.84%] [G loss: 5.203582]\n",
      "epoch:3 step:2677 [D loss: 0.546711, acc.: 67.19%] [G loss: 1.232384]\n",
      "epoch:3 step:2678 [D loss: 0.227214, acc.: 89.84%] [G loss: 2.491545]\n",
      "epoch:3 step:2679 [D loss: 0.096483, acc.: 95.31%] [G loss: 3.200008]\n",
      "epoch:3 step:2680 [D loss: 0.037721, acc.: 100.00%] [G loss: 1.799961]\n",
      "epoch:3 step:2681 [D loss: 0.062647, acc.: 98.44%] [G loss: 0.602442]\n",
      "epoch:3 step:2682 [D loss: 0.066024, acc.: 100.00%] [G loss: 0.782096]\n",
      "epoch:3 step:2683 [D loss: 0.047641, acc.: 100.00%] [G loss: 0.338693]\n",
      "epoch:3 step:2684 [D loss: 0.058920, acc.: 99.22%] [G loss: 0.139297]\n",
      "epoch:3 step:2685 [D loss: 0.348435, acc.: 80.47%] [G loss: 6.443619]\n",
      "epoch:3 step:2686 [D loss: 1.477630, acc.: 50.78%] [G loss: 0.708833]\n",
      "epoch:3 step:2687 [D loss: 0.086767, acc.: 98.44%] [G loss: 0.926366]\n",
      "epoch:3 step:2688 [D loss: 0.264130, acc.: 91.41%] [G loss: 2.221791]\n",
      "epoch:3 step:2689 [D loss: 0.036004, acc.: 100.00%] [G loss: 2.234717]\n",
      "epoch:3 step:2690 [D loss: 0.093492, acc.: 99.22%] [G loss: 1.749618]\n",
      "epoch:3 step:2691 [D loss: 0.214264, acc.: 93.75%] [G loss: 0.985102]\n",
      "epoch:3 step:2692 [D loss: 0.333410, acc.: 86.72%] [G loss: 2.526326]\n",
      "epoch:3 step:2693 [D loss: 0.300940, acc.: 83.59%] [G loss: 2.070604]\n",
      "epoch:3 step:2694 [D loss: 0.300183, acc.: 87.50%] [G loss: 2.373737]\n",
      "epoch:3 step:2695 [D loss: 0.151228, acc.: 96.09%] [G loss: 3.092917]\n",
      "epoch:3 step:2696 [D loss: 0.128298, acc.: 97.66%] [G loss: 3.207105]\n",
      "epoch:3 step:2697 [D loss: 0.599860, acc.: 72.66%] [G loss: 5.062100]\n",
      "epoch:3 step:2698 [D loss: 0.177422, acc.: 92.19%] [G loss: 4.032041]\n",
      "epoch:3 step:2699 [D loss: 0.133349, acc.: 97.66%] [G loss: 2.259444]\n",
      "epoch:3 step:2700 [D loss: 0.120040, acc.: 96.09%] [G loss: 1.547499]\n",
      "epoch:3 step:2701 [D loss: 0.078740, acc.: 98.44%] [G loss: 1.377864]\n",
      "epoch:3 step:2702 [D loss: 0.052885, acc.: 98.44%] [G loss: 0.743339]\n",
      "epoch:3 step:2703 [D loss: 0.066171, acc.: 98.44%] [G loss: 0.252330]\n",
      "epoch:3 step:2704 [D loss: 0.139813, acc.: 96.88%] [G loss: 0.282113]\n",
      "epoch:3 step:2705 [D loss: 0.008970, acc.: 100.00%] [G loss: 0.550758]\n",
      "epoch:3 step:2706 [D loss: 0.050965, acc.: 99.22%] [G loss: 0.141132]\n",
      "epoch:3 step:2707 [D loss: 0.030703, acc.: 100.00%] [G loss: 0.121816]\n",
      "epoch:3 step:2708 [D loss: 0.024870, acc.: 100.00%] [G loss: 0.151392]\n",
      "epoch:3 step:2709 [D loss: 0.116825, acc.: 96.09%] [G loss: 0.429671]\n",
      "epoch:3 step:2710 [D loss: 0.083148, acc.: 96.88%] [G loss: 0.728623]\n",
      "epoch:3 step:2711 [D loss: 0.067599, acc.: 97.66%] [G loss: 0.230601]\n",
      "epoch:3 step:2712 [D loss: 0.198401, acc.: 89.06%] [G loss: 1.540406]\n",
      "epoch:3 step:2713 [D loss: 0.071734, acc.: 97.66%] [G loss: 1.958762]\n",
      "epoch:3 step:2714 [D loss: 0.134404, acc.: 92.97%] [G loss: 0.278172]\n",
      "epoch:3 step:2715 [D loss: 0.222884, acc.: 89.84%] [G loss: 1.931518]\n",
      "epoch:3 step:2716 [D loss: 0.024274, acc.: 99.22%] [G loss: 2.891205]\n",
      "epoch:3 step:2717 [D loss: 0.447067, acc.: 80.47%] [G loss: 0.288454]\n",
      "epoch:3 step:2718 [D loss: 0.562437, acc.: 79.69%] [G loss: 3.898757]\n",
      "epoch:3 step:2719 [D loss: 0.233484, acc.: 92.19%] [G loss: 2.932064]\n",
      "epoch:3 step:2720 [D loss: 0.206592, acc.: 91.41%] [G loss: 1.484856]\n",
      "epoch:3 step:2721 [D loss: 0.036503, acc.: 100.00%] [G loss: 0.785207]\n",
      "epoch:3 step:2722 [D loss: 0.022532, acc.: 100.00%] [G loss: 1.332510]\n",
      "epoch:3 step:2723 [D loss: 0.049717, acc.: 99.22%] [G loss: 1.041898]\n",
      "epoch:3 step:2724 [D loss: 0.067055, acc.: 99.22%] [G loss: 1.797997]\n",
      "epoch:3 step:2725 [D loss: 0.242027, acc.: 90.62%] [G loss: 2.802092]\n",
      "epoch:3 step:2726 [D loss: 0.074885, acc.: 99.22%] [G loss: 4.500703]\n",
      "epoch:3 step:2727 [D loss: 0.336719, acc.: 82.03%] [G loss: 4.002576]\n",
      "epoch:3 step:2728 [D loss: 0.069853, acc.: 99.22%] [G loss: 4.865698]\n",
      "epoch:3 step:2729 [D loss: 0.146783, acc.: 93.75%] [G loss: 3.266668]\n",
      "epoch:3 step:2730 [D loss: 0.540443, acc.: 71.88%] [G loss: 8.578046]\n",
      "epoch:3 step:2731 [D loss: 0.380309, acc.: 79.69%] [G loss: 7.326673]\n",
      "epoch:3 step:2732 [D loss: 0.050905, acc.: 99.22%] [G loss: 5.897877]\n",
      "epoch:3 step:2733 [D loss: 0.086786, acc.: 96.09%] [G loss: 5.802815]\n",
      "epoch:3 step:2734 [D loss: 0.044892, acc.: 99.22%] [G loss: 4.722285]\n",
      "epoch:3 step:2735 [D loss: 0.104157, acc.: 97.66%] [G loss: 3.946661]\n",
      "epoch:3 step:2736 [D loss: 0.056987, acc.: 100.00%] [G loss: 0.625733]\n",
      "epoch:3 step:2737 [D loss: 0.058054, acc.: 99.22%] [G loss: 1.324425]\n",
      "epoch:3 step:2738 [D loss: 0.072421, acc.: 98.44%] [G loss: 1.252599]\n",
      "epoch:3 step:2739 [D loss: 0.056989, acc.: 97.66%] [G loss: 0.433125]\n",
      "epoch:3 step:2740 [D loss: 0.049761, acc.: 99.22%] [G loss: 0.056597]\n",
      "epoch:3 step:2741 [D loss: 0.332999, acc.: 85.94%] [G loss: 3.605443]\n",
      "epoch:3 step:2742 [D loss: 0.888126, acc.: 63.28%] [G loss: 0.159573]\n",
      "epoch:3 step:2743 [D loss: 0.045409, acc.: 98.44%] [G loss: 0.430981]\n",
      "epoch:3 step:2744 [D loss: 0.188764, acc.: 92.97%] [G loss: 2.521595]\n",
      "epoch:3 step:2745 [D loss: 0.105621, acc.: 97.66%] [G loss: 1.832841]\n",
      "epoch:3 step:2746 [D loss: 0.150700, acc.: 96.09%] [G loss: 0.247262]\n",
      "epoch:3 step:2747 [D loss: 0.848747, acc.: 67.19%] [G loss: 6.155872]\n",
      "epoch:3 step:2748 [D loss: 0.634061, acc.: 78.12%] [G loss: 2.594914]\n",
      "epoch:3 step:2749 [D loss: 0.220481, acc.: 92.19%] [G loss: 3.680204]\n",
      "epoch:3 step:2750 [D loss: 0.089987, acc.: 98.44%] [G loss: 3.166989]\n",
      "epoch:3 step:2751 [D loss: 0.256221, acc.: 92.19%] [G loss: 2.310695]\n",
      "epoch:3 step:2752 [D loss: 0.232583, acc.: 89.84%] [G loss: 4.965767]\n",
      "epoch:3 step:2753 [D loss: 0.106304, acc.: 98.44%] [G loss: 5.604943]\n",
      "epoch:3 step:2754 [D loss: 0.270850, acc.: 89.06%] [G loss: 2.726259]\n",
      "epoch:3 step:2755 [D loss: 0.177268, acc.: 89.84%] [G loss: 5.223625]\n",
      "epoch:3 step:2756 [D loss: 0.080934, acc.: 98.44%] [G loss: 4.750449]\n",
      "epoch:3 step:2757 [D loss: 0.169331, acc.: 95.31%] [G loss: 4.486804]\n",
      "epoch:3 step:2758 [D loss: 0.113394, acc.: 97.66%] [G loss: 4.646764]\n",
      "epoch:3 step:2759 [D loss: 0.123184, acc.: 96.88%] [G loss: 3.062773]\n",
      "epoch:3 step:2760 [D loss: 0.118599, acc.: 95.31%] [G loss: 4.649362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2761 [D loss: 0.132306, acc.: 95.31%] [G loss: 2.586679]\n",
      "epoch:3 step:2762 [D loss: 0.238425, acc.: 91.41%] [G loss: 6.990686]\n",
      "epoch:3 step:2763 [D loss: 0.579628, acc.: 74.22%] [G loss: 1.205814]\n",
      "epoch:3 step:2764 [D loss: 0.083726, acc.: 99.22%] [G loss: 1.394153]\n",
      "epoch:3 step:2765 [D loss: 0.020229, acc.: 100.00%] [G loss: 1.198420]\n",
      "epoch:3 step:2766 [D loss: 0.109552, acc.: 96.88%] [G loss: 1.018080]\n",
      "epoch:3 step:2767 [D loss: 0.028122, acc.: 100.00%] [G loss: 0.495499]\n",
      "epoch:3 step:2768 [D loss: 0.053845, acc.: 100.00%] [G loss: 0.261090]\n",
      "epoch:3 step:2769 [D loss: 0.076066, acc.: 99.22%] [G loss: 0.882233]\n",
      "epoch:3 step:2770 [D loss: 0.051303, acc.: 98.44%] [G loss: 0.345024]\n",
      "epoch:3 step:2771 [D loss: 0.159633, acc.: 96.09%] [G loss: 0.025462]\n",
      "epoch:3 step:2772 [D loss: 0.123636, acc.: 96.88%] [G loss: 0.518252]\n",
      "epoch:3 step:2773 [D loss: 0.815159, acc.: 63.28%] [G loss: 4.776184]\n",
      "epoch:3 step:2774 [D loss: 0.198934, acc.: 90.62%] [G loss: 3.507475]\n",
      "epoch:3 step:2775 [D loss: 0.197087, acc.: 92.97%] [G loss: 2.090274]\n",
      "epoch:3 step:2776 [D loss: 0.260446, acc.: 87.50%] [G loss: 5.386307]\n",
      "epoch:3 step:2777 [D loss: 0.112941, acc.: 96.09%] [G loss: 3.677601]\n",
      "epoch:3 step:2778 [D loss: 0.312354, acc.: 83.59%] [G loss: 2.964695]\n",
      "epoch:3 step:2779 [D loss: 1.766866, acc.: 31.25%] [G loss: 7.047087]\n",
      "epoch:3 step:2780 [D loss: 0.436466, acc.: 80.47%] [G loss: 5.436507]\n",
      "epoch:3 step:2781 [D loss: 0.278305, acc.: 87.50%] [G loss: 3.055655]\n",
      "epoch:3 step:2782 [D loss: 0.139471, acc.: 93.75%] [G loss: 3.375238]\n",
      "epoch:3 step:2783 [D loss: 0.065636, acc.: 98.44%] [G loss: 3.303600]\n",
      "epoch:3 step:2784 [D loss: 0.151772, acc.: 95.31%] [G loss: 2.876417]\n",
      "epoch:3 step:2785 [D loss: 0.313795, acc.: 85.94%] [G loss: 4.569416]\n",
      "epoch:3 step:2786 [D loss: 0.670625, acc.: 61.72%] [G loss: 2.871479]\n",
      "epoch:3 step:2787 [D loss: 0.314609, acc.: 87.50%] [G loss: 4.722198]\n",
      "epoch:3 step:2788 [D loss: 0.119869, acc.: 95.31%] [G loss: 4.201952]\n",
      "epoch:3 step:2789 [D loss: 0.088716, acc.: 98.44%] [G loss: 3.637159]\n",
      "epoch:3 step:2790 [D loss: 0.234588, acc.: 92.19%] [G loss: 4.302632]\n",
      "epoch:3 step:2791 [D loss: 0.138202, acc.: 96.88%] [G loss: 3.485613]\n",
      "epoch:3 step:2792 [D loss: 0.131665, acc.: 96.88%] [G loss: 3.560346]\n",
      "epoch:3 step:2793 [D loss: 0.239025, acc.: 90.62%] [G loss: 4.711629]\n",
      "epoch:3 step:2794 [D loss: 0.155078, acc.: 92.97%] [G loss: 4.184684]\n",
      "epoch:3 step:2795 [D loss: 0.164603, acc.: 93.75%] [G loss: 4.583202]\n",
      "epoch:3 step:2796 [D loss: 0.157651, acc.: 94.53%] [G loss: 4.166596]\n",
      "epoch:3 step:2797 [D loss: 0.104631, acc.: 98.44%] [G loss: 3.338006]\n",
      "epoch:3 step:2798 [D loss: 0.160392, acc.: 96.09%] [G loss: 4.058790]\n",
      "epoch:3 step:2799 [D loss: 0.097141, acc.: 98.44%] [G loss: 3.462126]\n",
      "epoch:3 step:2800 [D loss: 0.062730, acc.: 98.44%] [G loss: 2.613629]\n",
      "epoch:3 step:2801 [D loss: 0.133979, acc.: 96.88%] [G loss: 2.270345]\n",
      "epoch:3 step:2802 [D loss: 0.090797, acc.: 99.22%] [G loss: 2.070873]\n",
      "epoch:3 step:2803 [D loss: 0.208794, acc.: 94.53%] [G loss: 0.727816]\n",
      "epoch:3 step:2804 [D loss: 0.157102, acc.: 96.88%] [G loss: 1.460601]\n",
      "epoch:3 step:2805 [D loss: 0.082359, acc.: 99.22%] [G loss: 0.980484]\n",
      "epoch:3 step:2806 [D loss: 0.039114, acc.: 98.44%] [G loss: 1.737693]\n",
      "epoch:3 step:2807 [D loss: 0.020651, acc.: 100.00%] [G loss: 0.594765]\n",
      "epoch:3 step:2808 [D loss: 0.097109, acc.: 98.44%] [G loss: 0.981332]\n",
      "epoch:3 step:2809 [D loss: 0.079868, acc.: 98.44%] [G loss: 0.306219]\n",
      "epoch:3 step:2810 [D loss: 0.068021, acc.: 96.88%] [G loss: 0.142803]\n",
      "epoch:3 step:2811 [D loss: 0.107328, acc.: 95.31%] [G loss: 0.023372]\n",
      "epoch:3 step:2812 [D loss: 1.122933, acc.: 56.25%] [G loss: 9.714305]\n",
      "epoch:3 step:2813 [D loss: 3.343280, acc.: 50.00%] [G loss: 5.050156]\n",
      "epoch:3 step:2814 [D loss: 0.421185, acc.: 81.25%] [G loss: 3.307724]\n",
      "epoch:3 step:2815 [D loss: 0.183897, acc.: 93.75%] [G loss: 2.551867]\n",
      "epoch:3 step:2816 [D loss: 0.153807, acc.: 95.31%] [G loss: 3.011478]\n",
      "epoch:3 step:2817 [D loss: 0.147825, acc.: 97.66%] [G loss: 2.992293]\n",
      "epoch:3 step:2818 [D loss: 0.136475, acc.: 97.66%] [G loss: 2.615622]\n",
      "epoch:3 step:2819 [D loss: 0.272100, acc.: 92.19%] [G loss: 2.311849]\n",
      "epoch:3 step:2820 [D loss: 0.295318, acc.: 87.50%] [G loss: 3.987680]\n",
      "epoch:3 step:2821 [D loss: 0.346672, acc.: 85.16%] [G loss: 3.329813]\n",
      "epoch:3 step:2822 [D loss: 0.430736, acc.: 78.12%] [G loss: 3.948332]\n",
      "epoch:3 step:2823 [D loss: 0.316143, acc.: 84.38%] [G loss: 2.845817]\n",
      "epoch:3 step:2824 [D loss: 0.265943, acc.: 88.28%] [G loss: 2.644309]\n",
      "epoch:3 step:2825 [D loss: 0.166758, acc.: 93.75%] [G loss: 1.761124]\n",
      "epoch:3 step:2826 [D loss: 0.223767, acc.: 92.19%] [G loss: 2.226672]\n",
      "epoch:3 step:2827 [D loss: 0.132257, acc.: 96.09%] [G loss: 2.321656]\n",
      "epoch:3 step:2828 [D loss: 0.087196, acc.: 96.88%] [G loss: 1.760418]\n",
      "epoch:3 step:2829 [D loss: 0.084719, acc.: 99.22%] [G loss: 0.220089]\n",
      "epoch:3 step:2830 [D loss: 0.160799, acc.: 92.97%] [G loss: 0.848382]\n",
      "epoch:3 step:2831 [D loss: 0.068610, acc.: 98.44%] [G loss: 0.486237]\n",
      "epoch:3 step:2832 [D loss: 0.089100, acc.: 97.66%] [G loss: 0.099504]\n",
      "epoch:3 step:2833 [D loss: 0.020816, acc.: 100.00%] [G loss: 0.070663]\n",
      "epoch:3 step:2834 [D loss: 0.050124, acc.: 99.22%] [G loss: 0.023273]\n",
      "epoch:3 step:2835 [D loss: 0.053915, acc.: 100.00%] [G loss: 0.251161]\n",
      "epoch:3 step:2836 [D loss: 0.017285, acc.: 100.00%] [G loss: 0.283541]\n",
      "epoch:3 step:2837 [D loss: 0.034571, acc.: 100.00%] [G loss: 0.403258]\n",
      "epoch:3 step:2838 [D loss: 0.015101, acc.: 100.00%] [G loss: 0.622234]\n",
      "epoch:3 step:2839 [D loss: 0.083140, acc.: 97.66%] [G loss: 0.319548]\n",
      "epoch:3 step:2840 [D loss: 0.102568, acc.: 97.66%] [G loss: 0.879369]\n",
      "epoch:3 step:2841 [D loss: 0.326617, acc.: 86.72%] [G loss: 2.377848]\n",
      "epoch:3 step:2842 [D loss: 0.769747, acc.: 64.06%] [G loss: 0.648555]\n",
      "epoch:3 step:2843 [D loss: 0.041136, acc.: 99.22%] [G loss: 1.878713]\n",
      "epoch:3 step:2844 [D loss: 0.023738, acc.: 100.00%] [G loss: 1.370326]\n",
      "epoch:3 step:2845 [D loss: 0.412803, acc.: 80.47%] [G loss: 4.736048]\n",
      "epoch:3 step:2846 [D loss: 0.523151, acc.: 76.56%] [G loss: 1.199318]\n",
      "epoch:3 step:2847 [D loss: 0.280127, acc.: 88.28%] [G loss: 3.266551]\n",
      "epoch:3 step:2848 [D loss: 0.166497, acc.: 92.97%] [G loss: 2.537107]\n",
      "epoch:3 step:2849 [D loss: 0.310198, acc.: 85.94%] [G loss: 2.493678]\n",
      "epoch:3 step:2850 [D loss: 0.210154, acc.: 93.75%] [G loss: 2.899190]\n",
      "epoch:3 step:2851 [D loss: 0.199910, acc.: 91.41%] [G loss: 2.885664]\n",
      "epoch:3 step:2852 [D loss: 0.467900, acc.: 76.56%] [G loss: 3.964428]\n",
      "epoch:3 step:2853 [D loss: 0.084589, acc.: 97.66%] [G loss: 4.296227]\n",
      "epoch:3 step:2854 [D loss: 0.433290, acc.: 82.03%] [G loss: 4.112445]\n",
      "epoch:3 step:2855 [D loss: 0.128302, acc.: 96.88%] [G loss: 4.191434]\n",
      "epoch:3 step:2856 [D loss: 0.349242, acc.: 82.03%] [G loss: 4.140581]\n",
      "epoch:3 step:2857 [D loss: 0.351667, acc.: 85.94%] [G loss: 4.936314]\n",
      "epoch:3 step:2858 [D loss: 0.114498, acc.: 96.09%] [G loss: 4.139030]\n",
      "epoch:3 step:2859 [D loss: 0.386458, acc.: 79.69%] [G loss: 5.525208]\n",
      "epoch:3 step:2860 [D loss: 0.227241, acc.: 90.62%] [G loss: 4.801230]\n",
      "epoch:3 step:2861 [D loss: 0.218624, acc.: 90.62%] [G loss: 4.592687]\n",
      "epoch:3 step:2862 [D loss: 0.350683, acc.: 86.72%] [G loss: 3.852174]\n",
      "epoch:3 step:2863 [D loss: 0.050742, acc.: 99.22%] [G loss: 3.930128]\n",
      "epoch:3 step:2864 [D loss: 0.397568, acc.: 85.94%] [G loss: 5.064216]\n",
      "epoch:3 step:2865 [D loss: 0.086682, acc.: 98.44%] [G loss: 4.872323]\n",
      "epoch:3 step:2866 [D loss: 0.261258, acc.: 90.62%] [G loss: 1.530771]\n",
      "epoch:3 step:2867 [D loss: 0.233982, acc.: 90.62%] [G loss: 3.943524]\n",
      "epoch:3 step:2868 [D loss: 0.081339, acc.: 97.66%] [G loss: 4.248025]\n",
      "epoch:3 step:2869 [D loss: 0.194129, acc.: 92.97%] [G loss: 1.207884]\n",
      "epoch:3 step:2870 [D loss: 0.201079, acc.: 92.19%] [G loss: 1.200416]\n",
      "epoch:3 step:2871 [D loss: 0.083458, acc.: 96.88%] [G loss: 0.653421]\n",
      "epoch:3 step:2872 [D loss: 0.384285, acc.: 80.47%] [G loss: 5.452136]\n",
      "epoch:3 step:2873 [D loss: 0.928616, acc.: 59.38%] [G loss: 1.531349]\n",
      "epoch:3 step:2874 [D loss: 0.098101, acc.: 96.88%] [G loss: 2.195696]\n",
      "epoch:3 step:2875 [D loss: 0.028926, acc.: 100.00%] [G loss: 3.058193]\n",
      "epoch:3 step:2876 [D loss: 0.070041, acc.: 98.44%] [G loss: 2.117650]\n",
      "epoch:3 step:2877 [D loss: 0.049038, acc.: 99.22%] [G loss: 1.367319]\n",
      "epoch:3 step:2878 [D loss: 0.184294, acc.: 94.53%] [G loss: 1.266082]\n",
      "epoch:3 step:2879 [D loss: 0.184522, acc.: 93.75%] [G loss: 2.032657]\n",
      "epoch:3 step:2880 [D loss: 0.079056, acc.: 97.66%] [G loss: 1.429632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:2881 [D loss: 0.381207, acc.: 82.03%] [G loss: 2.022130]\n",
      "epoch:3 step:2882 [D loss: 0.301000, acc.: 87.50%] [G loss: 0.300847]\n",
      "epoch:3 step:2883 [D loss: 0.072273, acc.: 98.44%] [G loss: 0.493073]\n",
      "epoch:3 step:2884 [D loss: 0.169889, acc.: 92.97%] [G loss: 2.966849]\n",
      "epoch:3 step:2885 [D loss: 0.216134, acc.: 89.84%] [G loss: 2.396926]\n",
      "epoch:3 step:2886 [D loss: 0.228934, acc.: 92.97%] [G loss: 4.620627]\n",
      "epoch:3 step:2887 [D loss: 0.433958, acc.: 78.12%] [G loss: 3.549868]\n",
      "epoch:3 step:2888 [D loss: 0.088243, acc.: 97.66%] [G loss: 3.341791]\n",
      "epoch:3 step:2889 [D loss: 0.715198, acc.: 66.41%] [G loss: 7.547137]\n",
      "epoch:3 step:2890 [D loss: 0.622497, acc.: 73.44%] [G loss: 2.860682]\n",
      "epoch:3 step:2891 [D loss: 0.313508, acc.: 83.59%] [G loss: 6.517747]\n",
      "epoch:3 step:2892 [D loss: 0.220938, acc.: 91.41%] [G loss: 5.493286]\n",
      "epoch:3 step:2893 [D loss: 0.123141, acc.: 97.66%] [G loss: 5.293255]\n",
      "epoch:3 step:2894 [D loss: 0.093141, acc.: 99.22%] [G loss: 4.186196]\n",
      "epoch:3 step:2895 [D loss: 0.350499, acc.: 86.72%] [G loss: 3.078216]\n",
      "epoch:3 step:2896 [D loss: 0.015969, acc.: 100.00%] [G loss: 1.707501]\n",
      "epoch:3 step:2897 [D loss: 0.278246, acc.: 87.50%] [G loss: 4.388615]\n",
      "epoch:3 step:2898 [D loss: 0.232093, acc.: 89.06%] [G loss: 1.613611]\n",
      "epoch:3 step:2899 [D loss: 0.151573, acc.: 93.75%] [G loss: 3.514236]\n",
      "epoch:3 step:2900 [D loss: 0.105131, acc.: 96.09%] [G loss: 1.520999]\n",
      "epoch:3 step:2901 [D loss: 0.085690, acc.: 99.22%] [G loss: 0.412204]\n",
      "epoch:3 step:2902 [D loss: 0.041184, acc.: 100.00%] [G loss: 0.696161]\n",
      "epoch:3 step:2903 [D loss: 0.022619, acc.: 99.22%] [G loss: 0.134209]\n",
      "epoch:3 step:2904 [D loss: 0.045300, acc.: 99.22%] [G loss: 0.371747]\n",
      "epoch:3 step:2905 [D loss: 0.274020, acc.: 85.16%] [G loss: 4.717502]\n",
      "epoch:3 step:2906 [D loss: 0.297475, acc.: 89.06%] [G loss: 2.980112]\n",
      "epoch:3 step:2907 [D loss: 0.576916, acc.: 70.31%] [G loss: 4.640592]\n",
      "epoch:3 step:2908 [D loss: 0.215086, acc.: 91.41%] [G loss: 1.161833]\n",
      "epoch:3 step:2909 [D loss: 0.073489, acc.: 96.88%] [G loss: 0.272815]\n",
      "epoch:3 step:2910 [D loss: 0.010848, acc.: 100.00%] [G loss: 0.042674]\n",
      "epoch:3 step:2911 [D loss: 0.267007, acc.: 89.06%] [G loss: 0.529674]\n",
      "epoch:3 step:2912 [D loss: 0.005035, acc.: 100.00%] [G loss: 2.660997]\n",
      "epoch:3 step:2913 [D loss: 0.186761, acc.: 90.62%] [G loss: 0.173041]\n",
      "epoch:3 step:2914 [D loss: 0.401334, acc.: 76.56%] [G loss: 2.366832]\n",
      "epoch:3 step:2915 [D loss: 0.152438, acc.: 93.75%] [G loss: 2.932398]\n",
      "epoch:3 step:2916 [D loss: 0.534289, acc.: 77.34%] [G loss: 1.410383]\n",
      "epoch:3 step:2917 [D loss: 0.039464, acc.: 99.22%] [G loss: 2.134709]\n",
      "epoch:3 step:2918 [D loss: 0.040709, acc.: 98.44%] [G loss: 1.149154]\n",
      "epoch:3 step:2919 [D loss: 0.152946, acc.: 95.31%] [G loss: 2.127151]\n",
      "epoch:3 step:2920 [D loss: 0.071854, acc.: 99.22%] [G loss: 2.132518]\n",
      "epoch:3 step:2921 [D loss: 0.196689, acc.: 93.75%] [G loss: 1.224684]\n",
      "epoch:3 step:2922 [D loss: 0.341736, acc.: 82.81%] [G loss: 5.554496]\n",
      "epoch:3 step:2923 [D loss: 0.861148, acc.: 60.16%] [G loss: 1.594134]\n",
      "epoch:3 step:2924 [D loss: 0.343919, acc.: 83.59%] [G loss: 5.593565]\n",
      "epoch:3 step:2925 [D loss: 0.156340, acc.: 92.19%] [G loss: 5.529677]\n",
      "epoch:3 step:2926 [D loss: 0.043835, acc.: 99.22%] [G loss: 3.802469]\n",
      "epoch:3 step:2927 [D loss: 0.163674, acc.: 94.53%] [G loss: 4.603638]\n",
      "epoch:3 step:2928 [D loss: 0.080050, acc.: 98.44%] [G loss: 4.697803]\n",
      "epoch:3 step:2929 [D loss: 0.051739, acc.: 99.22%] [G loss: 3.762251]\n",
      "epoch:3 step:2930 [D loss: 0.125456, acc.: 96.88%] [G loss: 3.327763]\n",
      "epoch:3 step:2931 [D loss: 0.036781, acc.: 100.00%] [G loss: 3.786346]\n",
      "epoch:3 step:2932 [D loss: 0.157673, acc.: 94.53%] [G loss: 1.061029]\n",
      "epoch:3 step:2933 [D loss: 0.056011, acc.: 100.00%] [G loss: 0.351327]\n",
      "epoch:3 step:2934 [D loss: 0.064948, acc.: 97.66%] [G loss: 0.629384]\n",
      "epoch:3 step:2935 [D loss: 0.024485, acc.: 100.00%] [G loss: 0.422326]\n",
      "epoch:3 step:2936 [D loss: 0.053819, acc.: 99.22%] [G loss: 0.181060]\n",
      "epoch:3 step:2937 [D loss: 0.076767, acc.: 98.44%] [G loss: 0.085435]\n",
      "epoch:3 step:2938 [D loss: 0.044879, acc.: 100.00%] [G loss: 0.144541]\n",
      "epoch:3 step:2939 [D loss: 0.025834, acc.: 100.00%] [G loss: 0.257039]\n",
      "epoch:3 step:2940 [D loss: 0.016403, acc.: 100.00%] [G loss: 0.189095]\n",
      "epoch:3 step:2941 [D loss: 0.031295, acc.: 100.00%] [G loss: 0.201178]\n",
      "epoch:3 step:2942 [D loss: 0.048204, acc.: 100.00%] [G loss: 0.629815]\n",
      "epoch:3 step:2943 [D loss: 0.177814, acc.: 93.75%] [G loss: 2.566288]\n",
      "epoch:3 step:2944 [D loss: 0.077711, acc.: 96.88%] [G loss: 1.427193]\n",
      "epoch:3 step:2945 [D loss: 0.062269, acc.: 98.44%] [G loss: 0.052971]\n",
      "epoch:3 step:2946 [D loss: 0.340266, acc.: 82.03%] [G loss: 3.980930]\n",
      "epoch:3 step:2947 [D loss: 0.496369, acc.: 73.44%] [G loss: 0.152195]\n",
      "epoch:3 step:2948 [D loss: 0.082004, acc.: 96.88%] [G loss: 0.195704]\n",
      "epoch:3 step:2949 [D loss: 0.011601, acc.: 100.00%] [G loss: 0.389423]\n",
      "epoch:3 step:2950 [D loss: 0.023939, acc.: 99.22%] [G loss: 0.495434]\n",
      "epoch:3 step:2951 [D loss: 0.090244, acc.: 99.22%] [G loss: 0.375882]\n",
      "epoch:3 step:2952 [D loss: 0.123614, acc.: 96.09%] [G loss: 0.782522]\n",
      "epoch:3 step:2953 [D loss: 0.768407, acc.: 64.06%] [G loss: 7.870333]\n",
      "epoch:3 step:2954 [D loss: 1.212487, acc.: 61.72%] [G loss: 2.775598]\n",
      "epoch:3 step:2955 [D loss: 0.242469, acc.: 89.06%] [G loss: 3.736421]\n",
      "epoch:3 step:2956 [D loss: 0.077210, acc.: 96.88%] [G loss: 4.125350]\n",
      "epoch:3 step:2957 [D loss: 0.183460, acc.: 94.53%] [G loss: 1.498166]\n",
      "epoch:3 step:2958 [D loss: 0.598021, acc.: 71.09%] [G loss: 5.684859]\n",
      "epoch:3 step:2959 [D loss: 0.701726, acc.: 71.09%] [G loss: 3.614271]\n",
      "epoch:3 step:2960 [D loss: 0.249064, acc.: 91.41%] [G loss: 4.953916]\n",
      "epoch:3 step:2961 [D loss: 0.055389, acc.: 99.22%] [G loss: 5.605946]\n",
      "epoch:3 step:2962 [D loss: 0.162276, acc.: 94.53%] [G loss: 4.021099]\n",
      "epoch:3 step:2963 [D loss: 0.173948, acc.: 96.88%] [G loss: 5.244012]\n",
      "epoch:3 step:2964 [D loss: 0.130296, acc.: 95.31%] [G loss: 5.280640]\n",
      "epoch:3 step:2965 [D loss: 0.350656, acc.: 86.72%] [G loss: 3.288778]\n",
      "epoch:3 step:2966 [D loss: 0.171905, acc.: 95.31%] [G loss: 4.551828]\n",
      "epoch:3 step:2967 [D loss: 0.151106, acc.: 95.31%] [G loss: 4.261295]\n",
      "epoch:3 step:2968 [D loss: 0.369097, acc.: 87.50%] [G loss: 5.576969]\n",
      "epoch:3 step:2969 [D loss: 0.321728, acc.: 83.59%] [G loss: 3.168044]\n",
      "epoch:3 step:2970 [D loss: 0.218810, acc.: 92.19%] [G loss: 3.301088]\n",
      "epoch:3 step:2971 [D loss: 0.353681, acc.: 79.69%] [G loss: 4.136115]\n",
      "epoch:3 step:2972 [D loss: 0.145867, acc.: 95.31%] [G loss: 1.208739]\n",
      "epoch:3 step:2973 [D loss: 0.038021, acc.: 100.00%] [G loss: 0.886246]\n",
      "epoch:3 step:2974 [D loss: 0.606653, acc.: 67.97%] [G loss: 7.574835]\n",
      "epoch:3 step:2975 [D loss: 0.911372, acc.: 63.28%] [G loss: 3.307075]\n",
      "epoch:3 step:2976 [D loss: 0.026518, acc.: 100.00%] [G loss: 0.664250]\n",
      "epoch:3 step:2977 [D loss: 0.134310, acc.: 95.31%] [G loss: 0.635115]\n",
      "epoch:3 step:2978 [D loss: 0.146389, acc.: 95.31%] [G loss: 2.222001]\n",
      "epoch:3 step:2979 [D loss: 0.090003, acc.: 94.53%] [G loss: 0.876483]\n",
      "epoch:3 step:2980 [D loss: 0.447531, acc.: 75.00%] [G loss: 1.019620]\n",
      "epoch:3 step:2981 [D loss: 0.039630, acc.: 99.22%] [G loss: 1.038733]\n",
      "epoch:3 step:2982 [D loss: 0.151731, acc.: 96.88%] [G loss: 0.592120]\n",
      "epoch:3 step:2983 [D loss: 0.182651, acc.: 93.75%] [G loss: 1.064181]\n",
      "epoch:3 step:2984 [D loss: 0.190315, acc.: 92.19%] [G loss: 0.677223]\n",
      "epoch:3 step:2985 [D loss: 0.154592, acc.: 92.97%] [G loss: 0.914994]\n",
      "epoch:3 step:2986 [D loss: 0.092687, acc.: 96.88%] [G loss: 1.970140]\n",
      "epoch:3 step:2987 [D loss: 0.295389, acc.: 89.06%] [G loss: 4.514279]\n",
      "epoch:3 step:2988 [D loss: 0.236792, acc.: 88.28%] [G loss: 2.438985]\n",
      "epoch:3 step:2989 [D loss: 2.552103, acc.: 21.88%] [G loss: 4.875830]\n",
      "epoch:3 step:2990 [D loss: 0.110742, acc.: 96.09%] [G loss: 5.980438]\n",
      "epoch:3 step:2991 [D loss: 0.348434, acc.: 84.38%] [G loss: 3.845754]\n",
      "epoch:3 step:2992 [D loss: 0.247519, acc.: 92.19%] [G loss: 3.660957]\n",
      "epoch:3 step:2993 [D loss: 0.088545, acc.: 99.22%] [G loss: 4.035771]\n",
      "epoch:3 step:2994 [D loss: 0.161382, acc.: 96.09%] [G loss: 3.725621]\n",
      "epoch:3 step:2995 [D loss: 0.191509, acc.: 96.09%] [G loss: 3.621152]\n",
      "epoch:3 step:2996 [D loss: 0.125221, acc.: 98.44%] [G loss: 3.841804]\n",
      "epoch:3 step:2997 [D loss: 0.219244, acc.: 93.75%] [G loss: 4.064241]\n",
      "epoch:3 step:2998 [D loss: 0.094084, acc.: 96.88%] [G loss: 4.108824]\n",
      "epoch:3 step:2999 [D loss: 0.561721, acc.: 71.09%] [G loss: 5.969268]\n",
      "epoch:3 step:3000 [D loss: 0.104719, acc.: 97.66%] [G loss: 5.333367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3001 [D loss: 0.193517, acc.: 91.41%] [G loss: 2.481349]\n",
      "epoch:3 step:3002 [D loss: 0.163844, acc.: 92.97%] [G loss: 2.622066]\n",
      "epoch:3 step:3003 [D loss: 0.039689, acc.: 100.00%] [G loss: 1.940783]\n",
      "epoch:3 step:3004 [D loss: 0.196189, acc.: 92.19%] [G loss: 4.794406]\n",
      "epoch:3 step:3005 [D loss: 0.185420, acc.: 94.53%] [G loss: 2.270071]\n",
      "epoch:3 step:3006 [D loss: 0.028123, acc.: 100.00%] [G loss: 0.486432]\n",
      "epoch:3 step:3007 [D loss: 0.095831, acc.: 96.88%] [G loss: 1.192932]\n",
      "epoch:3 step:3008 [D loss: 0.092683, acc.: 98.44%] [G loss: 0.396417]\n",
      "epoch:3 step:3009 [D loss: 0.045754, acc.: 100.00%] [G loss: 0.168853]\n",
      "epoch:3 step:3010 [D loss: 0.052742, acc.: 99.22%] [G loss: 0.253932]\n",
      "epoch:3 step:3011 [D loss: 0.075412, acc.: 98.44%] [G loss: 0.041389]\n",
      "epoch:3 step:3012 [D loss: 0.188316, acc.: 94.53%] [G loss: 1.878510]\n",
      "epoch:3 step:3013 [D loss: 0.051249, acc.: 100.00%] [G loss: 1.533962]\n",
      "epoch:3 step:3014 [D loss: 0.424449, acc.: 85.94%] [G loss: 0.245941]\n",
      "epoch:3 step:3015 [D loss: 0.242602, acc.: 89.06%] [G loss: 1.015883]\n",
      "epoch:3 step:3016 [D loss: 0.517507, acc.: 75.78%] [G loss: 0.281360]\n",
      "epoch:3 step:3017 [D loss: 0.047429, acc.: 100.00%] [G loss: 0.411961]\n",
      "epoch:3 step:3018 [D loss: 0.008417, acc.: 100.00%] [G loss: 0.156445]\n",
      "epoch:3 step:3019 [D loss: 0.016184, acc.: 100.00%] [G loss: 0.559934]\n",
      "epoch:3 step:3020 [D loss: 0.022369, acc.: 100.00%] [G loss: 0.448091]\n",
      "epoch:3 step:3021 [D loss: 0.078008, acc.: 98.44%] [G loss: 0.221197]\n",
      "epoch:3 step:3022 [D loss: 0.236450, acc.: 92.97%] [G loss: 0.437847]\n",
      "epoch:3 step:3023 [D loss: 0.043033, acc.: 100.00%] [G loss: 2.745696]\n",
      "epoch:3 step:3024 [D loss: 0.117212, acc.: 98.44%] [G loss: 2.081408]\n",
      "epoch:3 step:3025 [D loss: 0.173676, acc.: 94.53%] [G loss: 3.723771]\n",
      "epoch:3 step:3026 [D loss: 0.376940, acc.: 82.03%] [G loss: 1.186519]\n",
      "epoch:3 step:3027 [D loss: 0.899599, acc.: 59.38%] [G loss: 7.373989]\n",
      "epoch:3 step:3028 [D loss: 1.021689, acc.: 57.81%] [G loss: 4.912730]\n",
      "epoch:3 step:3029 [D loss: 0.259070, acc.: 91.41%] [G loss: 5.404624]\n",
      "epoch:3 step:3030 [D loss: 0.065756, acc.: 99.22%] [G loss: 5.185644]\n",
      "epoch:3 step:3031 [D loss: 0.176177, acc.: 92.97%] [G loss: 3.594982]\n",
      "epoch:3 step:3032 [D loss: 0.228426, acc.: 92.19%] [G loss: 3.355499]\n",
      "epoch:3 step:3033 [D loss: 0.189744, acc.: 91.41%] [G loss: 5.661678]\n",
      "epoch:3 step:3034 [D loss: 0.087694, acc.: 97.66%] [G loss: 4.920012]\n",
      "epoch:3 step:3035 [D loss: 0.459022, acc.: 76.56%] [G loss: 3.285783]\n",
      "epoch:3 step:3036 [D loss: 0.058579, acc.: 99.22%] [G loss: 2.817893]\n",
      "epoch:3 step:3037 [D loss: 0.043517, acc.: 98.44%] [G loss: 1.729441]\n",
      "epoch:3 step:3038 [D loss: 0.131144, acc.: 96.88%] [G loss: 1.740769]\n",
      "epoch:3 step:3039 [D loss: 0.102044, acc.: 96.88%] [G loss: 0.585178]\n",
      "epoch:3 step:3040 [D loss: 0.094391, acc.: 97.66%] [G loss: 0.477261]\n",
      "epoch:3 step:3041 [D loss: 0.115943, acc.: 97.66%] [G loss: 1.648043]\n",
      "epoch:3 step:3042 [D loss: 0.060919, acc.: 96.88%] [G loss: 0.704457]\n",
      "epoch:3 step:3043 [D loss: 0.224723, acc.: 90.62%] [G loss: 1.816163]\n",
      "epoch:3 step:3044 [D loss: 0.149404, acc.: 95.31%] [G loss: 0.270869]\n",
      "epoch:3 step:3045 [D loss: 0.065705, acc.: 98.44%] [G loss: 0.013337]\n",
      "epoch:3 step:3046 [D loss: 0.289659, acc.: 83.59%] [G loss: 2.772488]\n",
      "epoch:3 step:3047 [D loss: 0.904039, acc.: 56.25%] [G loss: 0.478222]\n",
      "epoch:3 step:3048 [D loss: 0.183418, acc.: 89.06%] [G loss: 1.951847]\n",
      "epoch:3 step:3049 [D loss: 0.017876, acc.: 100.00%] [G loss: 2.889419]\n",
      "epoch:3 step:3050 [D loss: 0.076843, acc.: 96.09%] [G loss: 0.583602]\n",
      "epoch:3 step:3051 [D loss: 0.202513, acc.: 92.97%] [G loss: 1.341994]\n",
      "epoch:3 step:3052 [D loss: 0.039373, acc.: 100.00%] [G loss: 1.623397]\n",
      "epoch:3 step:3053 [D loss: 0.062385, acc.: 98.44%] [G loss: 0.388818]\n",
      "epoch:3 step:3054 [D loss: 0.048399, acc.: 99.22%] [G loss: 0.361310]\n",
      "epoch:3 step:3055 [D loss: 0.329240, acc.: 84.38%] [G loss: 3.639874]\n",
      "epoch:3 step:3056 [D loss: 1.564732, acc.: 45.31%] [G loss: 0.257389]\n",
      "epoch:3 step:3057 [D loss: 0.300313, acc.: 84.38%] [G loss: 3.157802]\n",
      "epoch:3 step:3058 [D loss: 0.075592, acc.: 98.44%] [G loss: 4.194450]\n",
      "epoch:3 step:3059 [D loss: 0.213024, acc.: 91.41%] [G loss: 2.833558]\n",
      "epoch:3 step:3060 [D loss: 0.100545, acc.: 99.22%] [G loss: 2.172567]\n",
      "epoch:3 step:3061 [D loss: 0.274817, acc.: 89.06%] [G loss: 2.873389]\n",
      "epoch:3 step:3062 [D loss: 0.132570, acc.: 96.88%] [G loss: 3.147378]\n",
      "epoch:3 step:3063 [D loss: 0.947196, acc.: 50.00%] [G loss: 5.167173]\n",
      "epoch:3 step:3064 [D loss: 0.174420, acc.: 92.19%] [G loss: 5.374822]\n",
      "epoch:3 step:3065 [D loss: 0.115266, acc.: 96.88%] [G loss: 4.309758]\n",
      "epoch:3 step:3066 [D loss: 0.098521, acc.: 97.66%] [G loss: 5.036905]\n",
      "epoch:3 step:3067 [D loss: 0.192040, acc.: 92.97%] [G loss: 4.235931]\n",
      "epoch:3 step:3068 [D loss: 0.088791, acc.: 97.66%] [G loss: 3.277486]\n",
      "epoch:3 step:3069 [D loss: 0.053396, acc.: 100.00%] [G loss: 1.981112]\n",
      "epoch:3 step:3070 [D loss: 0.354085, acc.: 82.81%] [G loss: 4.930560]\n",
      "epoch:3 step:3071 [D loss: 0.210932, acc.: 90.62%] [G loss: 3.088238]\n",
      "epoch:3 step:3072 [D loss: 0.046566, acc.: 100.00%] [G loss: 1.060000]\n",
      "epoch:3 step:3073 [D loss: 0.031294, acc.: 100.00%] [G loss: 0.431685]\n",
      "epoch:3 step:3074 [D loss: 0.041539, acc.: 100.00%] [G loss: 0.386164]\n",
      "epoch:3 step:3075 [D loss: 0.070047, acc.: 99.22%] [G loss: 0.167053]\n",
      "epoch:3 step:3076 [D loss: 0.093538, acc.: 97.66%] [G loss: 0.263659]\n",
      "epoch:3 step:3077 [D loss: 0.058214, acc.: 99.22%] [G loss: 0.390162]\n",
      "epoch:3 step:3078 [D loss: 0.103536, acc.: 96.88%] [G loss: 0.129022]\n",
      "epoch:3 step:3079 [D loss: 0.120119, acc.: 96.88%] [G loss: 0.715900]\n",
      "epoch:3 step:3080 [D loss: 0.032136, acc.: 100.00%] [G loss: 0.572665]\n",
      "epoch:3 step:3081 [D loss: 0.061194, acc.: 100.00%] [G loss: 0.120990]\n",
      "epoch:3 step:3082 [D loss: 0.067023, acc.: 99.22%] [G loss: 0.194391]\n",
      "epoch:3 step:3083 [D loss: 0.281943, acc.: 87.50%] [G loss: 0.260010]\n",
      "epoch:3 step:3084 [D loss: 0.012119, acc.: 100.00%] [G loss: 0.535535]\n",
      "epoch:3 step:3085 [D loss: 0.024939, acc.: 100.00%] [G loss: 0.812256]\n",
      "epoch:3 step:3086 [D loss: 0.040556, acc.: 100.00%] [G loss: 0.731669]\n",
      "epoch:3 step:3087 [D loss: 0.128976, acc.: 95.31%] [G loss: 2.991701]\n",
      "epoch:3 step:3088 [D loss: 0.095909, acc.: 97.66%] [G loss: 1.342692]\n",
      "epoch:3 step:3089 [D loss: 0.096297, acc.: 98.44%] [G loss: 0.245554]\n",
      "epoch:3 step:3090 [D loss: 0.170514, acc.: 97.66%] [G loss: 0.261699]\n",
      "epoch:3 step:3091 [D loss: 0.124387, acc.: 96.09%] [G loss: 1.929990]\n",
      "epoch:3 step:3092 [D loss: 0.980238, acc.: 57.03%] [G loss: 8.112370]\n",
      "epoch:3 step:3093 [D loss: 2.199877, acc.: 50.00%] [G loss: 2.679973]\n",
      "epoch:3 step:3094 [D loss: 0.646370, acc.: 78.12%] [G loss: 4.128480]\n",
      "epoch:3 step:3095 [D loss: 0.159205, acc.: 92.97%] [G loss: 4.362249]\n",
      "epoch:3 step:3096 [D loss: 0.083800, acc.: 98.44%] [G loss: 3.901257]\n",
      "epoch:3 step:3097 [D loss: 0.179651, acc.: 94.53%] [G loss: 2.388975]\n",
      "epoch:3 step:3098 [D loss: 0.273125, acc.: 88.28%] [G loss: 3.280107]\n",
      "epoch:3 step:3099 [D loss: 0.250706, acc.: 91.41%] [G loss: 3.619159]\n",
      "epoch:3 step:3100 [D loss: 0.442319, acc.: 80.47%] [G loss: 4.665462]\n",
      "epoch:3 step:3101 [D loss: 0.111805, acc.: 98.44%] [G loss: 4.292360]\n",
      "epoch:3 step:3102 [D loss: 0.848958, acc.: 54.69%] [G loss: 4.745534]\n",
      "epoch:3 step:3103 [D loss: 0.211644, acc.: 92.19%] [G loss: 4.117031]\n",
      "epoch:3 step:3104 [D loss: 0.151342, acc.: 96.09%] [G loss: 2.625859]\n",
      "epoch:3 step:3105 [D loss: 0.105845, acc.: 97.66%] [G loss: 1.387267]\n",
      "epoch:3 step:3106 [D loss: 0.301177, acc.: 88.28%] [G loss: 4.393148]\n",
      "epoch:3 step:3107 [D loss: 0.182684, acc.: 93.75%] [G loss: 3.798829]\n",
      "epoch:3 step:3108 [D loss: 0.155437, acc.: 96.09%] [G loss: 2.431329]\n",
      "epoch:3 step:3109 [D loss: 0.435849, acc.: 75.00%] [G loss: 6.079675]\n",
      "epoch:3 step:3110 [D loss: 0.326494, acc.: 84.38%] [G loss: 3.617769]\n",
      "epoch:3 step:3111 [D loss: 0.246936, acc.: 92.97%] [G loss: 3.015596]\n",
      "epoch:3 step:3112 [D loss: 0.177757, acc.: 93.75%] [G loss: 4.187339]\n",
      "epoch:3 step:3113 [D loss: 0.242578, acc.: 89.84%] [G loss: 3.201819]\n",
      "epoch:3 step:3114 [D loss: 0.265915, acc.: 90.62%] [G loss: 3.576492]\n",
      "epoch:3 step:3115 [D loss: 0.060793, acc.: 100.00%] [G loss: 4.956685]\n",
      "epoch:3 step:3116 [D loss: 0.098259, acc.: 96.88%] [G loss: 3.541951]\n",
      "epoch:3 step:3117 [D loss: 0.155727, acc.: 96.88%] [G loss: 3.057679]\n",
      "epoch:3 step:3118 [D loss: 0.059409, acc.: 100.00%] [G loss: 3.486131]\n",
      "epoch:3 step:3119 [D loss: 0.073318, acc.: 98.44%] [G loss: 2.533722]\n",
      "epoch:3 step:3120 [D loss: 0.800256, acc.: 59.38%] [G loss: 5.313563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3 step:3121 [D loss: 0.227477, acc.: 89.06%] [G loss: 4.470910]\n",
      "epoch:3 step:3122 [D loss: 0.200658, acc.: 92.97%] [G loss: 2.719568]\n",
      "epoch:3 step:3123 [D loss: 0.270498, acc.: 89.06%] [G loss: 3.423537]\n",
      "epoch:3 step:3124 [D loss: 0.073038, acc.: 99.22%] [G loss: 3.223284]\n",
      "epoch:4 step:3125 [D loss: 0.524165, acc.: 77.34%] [G loss: 3.990974]\n",
      "epoch:4 step:3126 [D loss: 0.196244, acc.: 91.41%] [G loss: 2.742945]\n",
      "epoch:4 step:3127 [D loss: 0.276012, acc.: 89.06%] [G loss: 3.771177]\n",
      "epoch:4 step:3128 [D loss: 0.095036, acc.: 98.44%] [G loss: 3.617993]\n",
      "epoch:4 step:3129 [D loss: 0.129321, acc.: 98.44%] [G loss: 2.631134]\n",
      "epoch:4 step:3130 [D loss: 0.134452, acc.: 97.66%] [G loss: 1.962529]\n",
      "epoch:4 step:3131 [D loss: 0.598352, acc.: 69.53%] [G loss: 6.348538]\n",
      "epoch:4 step:3132 [D loss: 0.627728, acc.: 69.53%] [G loss: 3.941032]\n",
      "epoch:4 step:3133 [D loss: 0.114855, acc.: 96.88%] [G loss: 2.492744]\n",
      "epoch:4 step:3134 [D loss: 0.090769, acc.: 99.22%] [G loss: 3.476101]\n",
      "epoch:4 step:3135 [D loss: 0.163098, acc.: 92.97%] [G loss: 3.563091]\n",
      "epoch:4 step:3136 [D loss: 0.143504, acc.: 96.88%] [G loss: 3.308073]\n",
      "epoch:4 step:3137 [D loss: 0.247932, acc.: 89.84%] [G loss: 4.528895]\n",
      "epoch:4 step:3138 [D loss: 0.340553, acc.: 85.94%] [G loss: 3.269212]\n",
      "epoch:4 step:3139 [D loss: 0.270742, acc.: 89.06%] [G loss: 4.817533]\n",
      "epoch:4 step:3140 [D loss: 0.156347, acc.: 95.31%] [G loss: 5.302077]\n",
      "epoch:4 step:3141 [D loss: 0.227682, acc.: 92.19%] [G loss: 3.965322]\n",
      "epoch:4 step:3142 [D loss: 0.093813, acc.: 99.22%] [G loss: 2.563896]\n",
      "epoch:4 step:3143 [D loss: 0.324414, acc.: 86.72%] [G loss: 2.408305]\n",
      "epoch:4 step:3144 [D loss: 0.200576, acc.: 90.62%] [G loss: 4.624228]\n",
      "epoch:4 step:3145 [D loss: 0.069557, acc.: 99.22%] [G loss: 0.579180]\n",
      "epoch:4 step:3146 [D loss: 0.135854, acc.: 96.88%] [G loss: 2.156005]\n",
      "epoch:4 step:3147 [D loss: 0.071740, acc.: 99.22%] [G loss: 1.867787]\n",
      "epoch:4 step:3148 [D loss: 0.112014, acc.: 97.66%] [G loss: 0.553800]\n",
      "epoch:4 step:3149 [D loss: 0.134124, acc.: 95.31%] [G loss: 0.023529]\n",
      "epoch:4 step:3150 [D loss: 0.798871, acc.: 58.59%] [G loss: 8.174014]\n",
      "epoch:4 step:3151 [D loss: 2.617723, acc.: 50.00%] [G loss: 4.921223]\n",
      "epoch:4 step:3152 [D loss: 0.391876, acc.: 82.81%] [G loss: 0.981164]\n",
      "epoch:4 step:3153 [D loss: 0.229433, acc.: 90.62%] [G loss: 1.941930]\n",
      "epoch:4 step:3154 [D loss: 0.122995, acc.: 96.88%] [G loss: 1.818961]\n",
      "epoch:4 step:3155 [D loss: 0.156012, acc.: 96.09%] [G loss: 1.302773]\n",
      "epoch:4 step:3156 [D loss: 0.099262, acc.: 99.22%] [G loss: 1.399122]\n",
      "epoch:4 step:3157 [D loss: 0.248785, acc.: 93.75%] [G loss: 1.155947]\n",
      "epoch:4 step:3158 [D loss: 0.376006, acc.: 85.16%] [G loss: 3.495752]\n",
      "epoch:4 step:3159 [D loss: 0.296973, acc.: 84.38%] [G loss: 1.805711]\n",
      "epoch:4 step:3160 [D loss: 0.459435, acc.: 75.78%] [G loss: 3.961493]\n",
      "epoch:4 step:3161 [D loss: 0.493025, acc.: 73.44%] [G loss: 2.800840]\n",
      "epoch:4 step:3162 [D loss: 0.220398, acc.: 94.53%] [G loss: 2.816983]\n",
      "epoch:4 step:3163 [D loss: 0.174279, acc.: 95.31%] [G loss: 3.207996]\n",
      "epoch:4 step:3164 [D loss: 0.180227, acc.: 95.31%] [G loss: 3.305116]\n",
      "epoch:4 step:3165 [D loss: 0.227142, acc.: 92.97%] [G loss: 2.762627]\n",
      "epoch:4 step:3166 [D loss: 0.275606, acc.: 89.84%] [G loss: 5.220790]\n",
      "epoch:4 step:3167 [D loss: 0.555260, acc.: 72.66%] [G loss: 4.384137]\n",
      "epoch:4 step:3168 [D loss: 0.131871, acc.: 95.31%] [G loss: 4.903571]\n",
      "epoch:4 step:3169 [D loss: 0.108905, acc.: 98.44%] [G loss: 3.417618]\n",
      "epoch:4 step:3170 [D loss: 0.289696, acc.: 88.28%] [G loss: 4.315621]\n",
      "epoch:4 step:3171 [D loss: 0.150402, acc.: 96.88%] [G loss: 3.390872]\n",
      "epoch:4 step:3172 [D loss: 0.171819, acc.: 93.75%] [G loss: 3.765826]\n",
      "epoch:4 step:3173 [D loss: 0.133638, acc.: 96.09%] [G loss: 4.013534]\n",
      "epoch:4 step:3174 [D loss: 0.183757, acc.: 92.97%] [G loss: 0.363464]\n",
      "epoch:4 step:3175 [D loss: 0.270312, acc.: 87.50%] [G loss: 3.435359]\n",
      "epoch:4 step:3176 [D loss: 0.204409, acc.: 91.41%] [G loss: 3.269952]\n",
      "epoch:4 step:3177 [D loss: 0.105958, acc.: 95.31%] [G loss: 0.589560]\n",
      "epoch:4 step:3178 [D loss: 0.117781, acc.: 96.09%] [G loss: 1.215167]\n",
      "epoch:4 step:3179 [D loss: 0.034331, acc.: 99.22%] [G loss: 0.805137]\n",
      "epoch:4 step:3180 [D loss: 0.216427, acc.: 92.97%] [G loss: 0.071488]\n",
      "epoch:4 step:3181 [D loss: 0.754207, acc.: 65.62%] [G loss: 5.739715]\n",
      "epoch:4 step:3182 [D loss: 0.614250, acc.: 70.31%] [G loss: 3.094580]\n",
      "epoch:4 step:3183 [D loss: 0.055795, acc.: 99.22%] [G loss: 1.766649]\n",
      "epoch:4 step:3184 [D loss: 0.211627, acc.: 89.06%] [G loss: 3.085282]\n",
      "epoch:4 step:3185 [D loss: 0.137305, acc.: 96.09%] [G loss: 2.401947]\n",
      "epoch:4 step:3186 [D loss: 0.107679, acc.: 97.66%] [G loss: 1.000102]\n",
      "epoch:4 step:3187 [D loss: 0.936411, acc.: 61.72%] [G loss: 5.760710]\n",
      "epoch:4 step:3188 [D loss: 1.734758, acc.: 46.09%] [G loss: 2.997169]\n",
      "epoch:4 step:3189 [D loss: 0.305211, acc.: 89.06%] [G loss: 3.223493]\n",
      "epoch:4 step:3190 [D loss: 0.144821, acc.: 98.44%] [G loss: 3.449533]\n",
      "epoch:4 step:3191 [D loss: 0.274321, acc.: 90.62%] [G loss: 2.748562]\n",
      "epoch:4 step:3192 [D loss: 0.164343, acc.: 94.53%] [G loss: 3.750552]\n",
      "epoch:4 step:3193 [D loss: 0.114987, acc.: 99.22%] [G loss: 3.215951]\n",
      "epoch:4 step:3194 [D loss: 0.379173, acc.: 88.28%] [G loss: 3.593862]\n",
      "epoch:4 step:3195 [D loss: 0.259835, acc.: 90.62%] [G loss: 2.412182]\n",
      "epoch:4 step:3196 [D loss: 0.660580, acc.: 64.84%] [G loss: 4.491271]\n",
      "epoch:4 step:3197 [D loss: 0.366449, acc.: 83.59%] [G loss: 2.358393]\n",
      "epoch:4 step:3198 [D loss: 0.146178, acc.: 96.09%] [G loss: 3.142527]\n",
      "epoch:4 step:3199 [D loss: 0.055227, acc.: 100.00%] [G loss: 2.490632]\n",
      "epoch:4 step:3200 [D loss: 0.127194, acc.: 97.66%] [G loss: 1.387877]\n",
      "epoch:4 step:3201 [D loss: 0.463716, acc.: 77.34%] [G loss: 4.682065]\n",
      "epoch:4 step:3202 [D loss: 0.330346, acc.: 83.59%] [G loss: 2.985929]\n",
      "epoch:4 step:3203 [D loss: 0.237269, acc.: 91.41%] [G loss: 1.283535]\n",
      "epoch:4 step:3204 [D loss: 0.070966, acc.: 98.44%] [G loss: 1.593493]\n",
      "epoch:4 step:3205 [D loss: 0.166475, acc.: 96.88%] [G loss: 1.912379]\n",
      "epoch:4 step:3206 [D loss: 0.180122, acc.: 93.75%] [G loss: 0.712036]\n",
      "epoch:4 step:3207 [D loss: 0.495092, acc.: 74.22%] [G loss: 4.196327]\n",
      "epoch:4 step:3208 [D loss: 1.624890, acc.: 50.00%] [G loss: 0.904004]\n",
      "epoch:4 step:3209 [D loss: 0.326157, acc.: 84.38%] [G loss: 1.696105]\n",
      "epoch:4 step:3210 [D loss: 0.078688, acc.: 99.22%] [G loss: 3.096158]\n",
      "epoch:4 step:3211 [D loss: 0.394211, acc.: 82.81%] [G loss: 3.335982]\n",
      "epoch:4 step:3212 [D loss: 0.210587, acc.: 91.41%] [G loss: 2.479703]\n",
      "epoch:4 step:3213 [D loss: 0.262290, acc.: 88.28%] [G loss: 1.918206]\n",
      "epoch:4 step:3214 [D loss: 0.278344, acc.: 88.28%] [G loss: 4.176362]\n",
      "epoch:4 step:3215 [D loss: 0.240827, acc.: 92.19%] [G loss: 4.430732]\n",
      "epoch:4 step:3216 [D loss: 0.523261, acc.: 81.25%] [G loss: 2.869201]\n",
      "epoch:4 step:3217 [D loss: 0.077771, acc.: 99.22%] [G loss: 2.384580]\n",
      "epoch:4 step:3218 [D loss: 0.113046, acc.: 96.88%] [G loss: 3.654737]\n",
      "epoch:4 step:3219 [D loss: 0.099410, acc.: 99.22%] [G loss: 3.539479]\n",
      "epoch:4 step:3220 [D loss: 0.202093, acc.: 92.97%] [G loss: 3.996846]\n",
      "epoch:4 step:3221 [D loss: 0.227950, acc.: 89.84%] [G loss: 2.465889]\n",
      "epoch:4 step:3222 [D loss: 0.214199, acc.: 92.97%] [G loss: 3.250985]\n",
      "epoch:4 step:3223 [D loss: 0.199158, acc.: 94.53%] [G loss: 2.014145]\n",
      "epoch:4 step:3224 [D loss: 0.105091, acc.: 99.22%] [G loss: 3.067369]\n",
      "epoch:4 step:3225 [D loss: 0.073032, acc.: 99.22%] [G loss: 1.159016]\n",
      "epoch:4 step:3226 [D loss: 0.058019, acc.: 100.00%] [G loss: 0.686172]\n",
      "epoch:4 step:3227 [D loss: 0.130680, acc.: 97.66%] [G loss: 0.437979]\n",
      "epoch:4 step:3228 [D loss: 0.034512, acc.: 100.00%] [G loss: 0.599805]\n",
      "epoch:4 step:3229 [D loss: 0.057757, acc.: 100.00%] [G loss: 0.661647]\n",
      "epoch:4 step:3230 [D loss: 0.142286, acc.: 95.31%] [G loss: 0.749695]\n",
      "epoch:4 step:3231 [D loss: 0.139031, acc.: 96.09%] [G loss: 1.014058]\n",
      "epoch:4 step:3232 [D loss: 0.034593, acc.: 100.00%] [G loss: 1.230232]\n",
      "epoch:4 step:3233 [D loss: 0.079286, acc.: 98.44%] [G loss: 0.348304]\n",
      "epoch:4 step:3234 [D loss: 0.085746, acc.: 98.44%] [G loss: 0.794546]\n",
      "epoch:4 step:3235 [D loss: 0.701591, acc.: 58.59%] [G loss: 4.589041]\n",
      "epoch:4 step:3236 [D loss: 0.057090, acc.: 97.66%] [G loss: 6.468801]\n",
      "epoch:4 step:3237 [D loss: 0.180590, acc.: 90.62%] [G loss: 2.578130]\n",
      "epoch:4 step:3238 [D loss: 0.085662, acc.: 98.44%] [G loss: 0.951250]\n",
      "epoch:4 step:3239 [D loss: 0.022614, acc.: 100.00%] [G loss: 1.209331]\n",
      "epoch:4 step:3240 [D loss: 0.194806, acc.: 94.53%] [G loss: 1.782258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3241 [D loss: 0.069188, acc.: 98.44%] [G loss: 4.095433]\n",
      "epoch:4 step:3242 [D loss: 0.277175, acc.: 89.06%] [G loss: 3.616644]\n",
      "epoch:4 step:3243 [D loss: 0.307475, acc.: 86.72%] [G loss: 4.515673]\n",
      "epoch:4 step:3244 [D loss: 0.202389, acc.: 93.75%] [G loss: 3.466237]\n",
      "epoch:4 step:3245 [D loss: 0.161180, acc.: 93.75%] [G loss: 4.622724]\n",
      "epoch:4 step:3246 [D loss: 0.054933, acc.: 99.22%] [G loss: 5.015644]\n",
      "epoch:4 step:3247 [D loss: 0.898416, acc.: 58.59%] [G loss: 5.245053]\n",
      "epoch:4 step:3248 [D loss: 0.016653, acc.: 100.00%] [G loss: 6.542000]\n",
      "epoch:4 step:3249 [D loss: 0.192178, acc.: 91.41%] [G loss: 4.330739]\n",
      "epoch:4 step:3250 [D loss: 0.158911, acc.: 94.53%] [G loss: 4.714492]\n",
      "epoch:4 step:3251 [D loss: 0.089830, acc.: 96.88%] [G loss: 3.733886]\n",
      "epoch:4 step:3252 [D loss: 0.066401, acc.: 99.22%] [G loss: 4.413136]\n",
      "epoch:4 step:3253 [D loss: 0.058834, acc.: 98.44%] [G loss: 3.390943]\n",
      "epoch:4 step:3254 [D loss: 0.188779, acc.: 96.88%] [G loss: 1.107482]\n",
      "epoch:4 step:3255 [D loss: 0.040309, acc.: 99.22%] [G loss: 0.946389]\n",
      "epoch:4 step:3256 [D loss: 0.058666, acc.: 100.00%] [G loss: 2.461381]\n",
      "epoch:4 step:3257 [D loss: 0.058029, acc.: 100.00%] [G loss: 0.615100]\n",
      "epoch:4 step:3258 [D loss: 0.082272, acc.: 98.44%] [G loss: 0.823748]\n",
      "epoch:4 step:3259 [D loss: 0.104665, acc.: 98.44%] [G loss: 1.937822]\n",
      "epoch:4 step:3260 [D loss: 0.926977, acc.: 48.44%] [G loss: 0.820939]\n",
      "epoch:4 step:3261 [D loss: 0.007470, acc.: 100.00%] [G loss: 2.918091]\n",
      "epoch:4 step:3262 [D loss: 0.197127, acc.: 92.19%] [G loss: 0.582685]\n",
      "epoch:4 step:3263 [D loss: 0.056738, acc.: 99.22%] [G loss: 0.501970]\n",
      "epoch:4 step:3264 [D loss: 0.053170, acc.: 99.22%] [G loss: 0.237318]\n",
      "epoch:4 step:3265 [D loss: 0.039537, acc.: 100.00%] [G loss: 0.200408]\n",
      "epoch:4 step:3266 [D loss: 0.027283, acc.: 100.00%] [G loss: 0.126467]\n",
      "epoch:4 step:3267 [D loss: 0.230437, acc.: 90.62%] [G loss: 3.367975]\n",
      "epoch:4 step:3268 [D loss: 1.233441, acc.: 37.50%] [G loss: 7.525913]\n",
      "epoch:4 step:3269 [D loss: 1.335540, acc.: 52.34%] [G loss: 1.898567]\n",
      "epoch:4 step:3270 [D loss: 0.502023, acc.: 80.47%] [G loss: 5.257342]\n",
      "epoch:4 step:3271 [D loss: 0.036211, acc.: 100.00%] [G loss: 5.930713]\n",
      "epoch:4 step:3272 [D loss: 0.777937, acc.: 60.94%] [G loss: 2.728524]\n",
      "epoch:4 step:3273 [D loss: 0.304004, acc.: 83.59%] [G loss: 3.752817]\n",
      "epoch:4 step:3274 [D loss: 0.062535, acc.: 100.00%] [G loss: 4.327469]\n",
      "epoch:4 step:3275 [D loss: 0.138750, acc.: 95.31%] [G loss: 3.730409]\n",
      "epoch:4 step:3276 [D loss: 0.389904, acc.: 82.81%] [G loss: 2.886489]\n",
      "epoch:4 step:3277 [D loss: 0.104561, acc.: 96.09%] [G loss: 2.665340]\n",
      "epoch:4 step:3278 [D loss: 0.071334, acc.: 99.22%] [G loss: 1.360552]\n",
      "epoch:4 step:3279 [D loss: 0.218456, acc.: 91.41%] [G loss: 1.979515]\n",
      "epoch:4 step:3280 [D loss: 0.753878, acc.: 65.62%] [G loss: 1.931680]\n",
      "epoch:4 step:3281 [D loss: 0.070613, acc.: 98.44%] [G loss: 1.921661]\n",
      "epoch:4 step:3282 [D loss: 0.164850, acc.: 95.31%] [G loss: 0.515974]\n",
      "epoch:4 step:3283 [D loss: 0.575482, acc.: 68.75%] [G loss: 3.321326]\n",
      "epoch:4 step:3284 [D loss: 1.414846, acc.: 53.12%] [G loss: 0.516397]\n",
      "epoch:4 step:3285 [D loss: 0.768510, acc.: 59.38%] [G loss: 2.755019]\n",
      "epoch:4 step:3286 [D loss: 0.174796, acc.: 93.75%] [G loss: 3.042377]\n",
      "epoch:4 step:3287 [D loss: 0.183575, acc.: 92.97%] [G loss: 2.084737]\n",
      "epoch:4 step:3288 [D loss: 0.137780, acc.: 98.44%] [G loss: 1.475188]\n",
      "epoch:4 step:3289 [D loss: 0.169430, acc.: 95.31%] [G loss: 1.582528]\n",
      "epoch:4 step:3290 [D loss: 0.277367, acc.: 91.41%] [G loss: 0.810928]\n",
      "epoch:4 step:3291 [D loss: 0.224037, acc.: 92.97%] [G loss: 2.181384]\n",
      "epoch:4 step:3292 [D loss: 0.065507, acc.: 100.00%] [G loss: 2.303248]\n",
      "epoch:4 step:3293 [D loss: 0.177667, acc.: 98.44%] [G loss: 2.041647]\n",
      "epoch:4 step:3294 [D loss: 0.270408, acc.: 89.84%] [G loss: 2.161338]\n",
      "epoch:4 step:3295 [D loss: 0.259878, acc.: 87.50%] [G loss: 3.264745]\n",
      "epoch:4 step:3296 [D loss: 0.159547, acc.: 96.09%] [G loss: 2.404109]\n",
      "epoch:4 step:3297 [D loss: 0.162181, acc.: 96.09%] [G loss: 2.027401]\n",
      "epoch:4 step:3298 [D loss: 0.538535, acc.: 75.00%] [G loss: 3.837445]\n",
      "epoch:4 step:3299 [D loss: 0.424671, acc.: 80.47%] [G loss: 2.011536]\n",
      "epoch:4 step:3300 [D loss: 0.546294, acc.: 76.56%] [G loss: 4.206248]\n",
      "epoch:4 step:3301 [D loss: 0.169655, acc.: 92.97%] [G loss: 4.079128]\n",
      "epoch:4 step:3302 [D loss: 0.350814, acc.: 88.28%] [G loss: 2.943326]\n",
      "epoch:4 step:3303 [D loss: 0.311238, acc.: 89.06%] [G loss: 3.504009]\n",
      "epoch:4 step:3304 [D loss: 0.178187, acc.: 94.53%] [G loss: 3.456852]\n",
      "epoch:4 step:3305 [D loss: 0.122957, acc.: 96.09%] [G loss: 3.951013]\n",
      "epoch:4 step:3306 [D loss: 0.206776, acc.: 92.97%] [G loss: 3.162678]\n",
      "epoch:4 step:3307 [D loss: 0.314837, acc.: 89.06%] [G loss: 4.421916]\n",
      "epoch:4 step:3308 [D loss: 0.241670, acc.: 90.62%] [G loss: 3.293819]\n",
      "epoch:4 step:3309 [D loss: 0.419876, acc.: 83.59%] [G loss: 4.343738]\n",
      "epoch:4 step:3310 [D loss: 0.184076, acc.: 92.97%] [G loss: 3.638891]\n",
      "epoch:4 step:3311 [D loss: 0.294256, acc.: 87.50%] [G loss: 3.831913]\n",
      "epoch:4 step:3312 [D loss: 0.362112, acc.: 83.59%] [G loss: 3.910987]\n",
      "epoch:4 step:3313 [D loss: 0.224502, acc.: 94.53%] [G loss: 4.839864]\n",
      "epoch:4 step:3314 [D loss: 0.247594, acc.: 90.62%] [G loss: 3.544244]\n",
      "epoch:4 step:3315 [D loss: 0.102354, acc.: 99.22%] [G loss: 3.194291]\n",
      "epoch:4 step:3316 [D loss: 0.167993, acc.: 92.97%] [G loss: 3.005715]\n",
      "epoch:4 step:3317 [D loss: 0.086433, acc.: 99.22%] [G loss: 1.583117]\n",
      "epoch:4 step:3318 [D loss: 0.078393, acc.: 98.44%] [G loss: 0.630219]\n",
      "epoch:4 step:3319 [D loss: 0.103608, acc.: 99.22%] [G loss: 0.281807]\n",
      "epoch:4 step:3320 [D loss: 0.577915, acc.: 68.75%] [G loss: 0.865726]\n",
      "epoch:4 step:3321 [D loss: 0.010895, acc.: 100.00%] [G loss: 3.917919]\n",
      "epoch:4 step:3322 [D loss: 0.156768, acc.: 94.53%] [G loss: 0.571541]\n",
      "epoch:4 step:3323 [D loss: 0.018138, acc.: 100.00%] [G loss: 0.318903]\n",
      "epoch:4 step:3324 [D loss: 0.373224, acc.: 82.03%] [G loss: 2.995800]\n",
      "epoch:4 step:3325 [D loss: 0.531882, acc.: 68.75%] [G loss: 0.768611]\n",
      "epoch:4 step:3326 [D loss: 0.102408, acc.: 96.88%] [G loss: 0.247066]\n",
      "epoch:4 step:3327 [D loss: 0.138966, acc.: 95.31%] [G loss: 0.768419]\n",
      "epoch:4 step:3328 [D loss: 0.021117, acc.: 100.00%] [G loss: 0.842414]\n",
      "epoch:4 step:3329 [D loss: 0.439371, acc.: 80.47%] [G loss: 3.065244]\n",
      "epoch:4 step:3330 [D loss: 0.163482, acc.: 91.41%] [G loss: 2.871115]\n",
      "epoch:4 step:3331 [D loss: 0.532982, acc.: 72.66%] [G loss: 3.321873]\n",
      "epoch:4 step:3332 [D loss: 0.075196, acc.: 99.22%] [G loss: 2.922350]\n",
      "epoch:4 step:3333 [D loss: 0.198442, acc.: 96.09%] [G loss: 3.945360]\n",
      "epoch:4 step:3334 [D loss: 0.163861, acc.: 96.09%] [G loss: 2.040535]\n",
      "epoch:4 step:3335 [D loss: 0.257070, acc.: 92.19%] [G loss: 2.846600]\n",
      "epoch:4 step:3336 [D loss: 0.754488, acc.: 62.50%] [G loss: 4.623996]\n",
      "epoch:4 step:3337 [D loss: 0.697410, acc.: 64.84%] [G loss: 5.019947]\n",
      "epoch:4 step:3338 [D loss: 0.141967, acc.: 97.66%] [G loss: 4.346395]\n",
      "epoch:4 step:3339 [D loss: 0.124162, acc.: 98.44%] [G loss: 4.920978]\n",
      "epoch:4 step:3340 [D loss: 0.142033, acc.: 96.09%] [G loss: 3.933012]\n",
      "epoch:4 step:3341 [D loss: 0.252585, acc.: 91.41%] [G loss: 4.524666]\n",
      "epoch:4 step:3342 [D loss: 0.338932, acc.: 85.16%] [G loss: 2.634724]\n",
      "epoch:4 step:3343 [D loss: 0.495562, acc.: 73.44%] [G loss: 7.024148]\n",
      "epoch:4 step:3344 [D loss: 0.427195, acc.: 79.69%] [G loss: 2.690157]\n",
      "epoch:4 step:3345 [D loss: 0.072142, acc.: 97.66%] [G loss: 1.727584]\n",
      "epoch:4 step:3346 [D loss: 0.104986, acc.: 96.88%] [G loss: 3.023606]\n",
      "epoch:4 step:3347 [D loss: 0.061643, acc.: 99.22%] [G loss: 1.608118]\n",
      "epoch:4 step:3348 [D loss: 0.058529, acc.: 99.22%] [G loss: 0.803285]\n",
      "epoch:4 step:3349 [D loss: 0.160886, acc.: 94.53%] [G loss: 1.288171]\n",
      "epoch:4 step:3350 [D loss: 0.176871, acc.: 93.75%] [G loss: 0.562016]\n",
      "epoch:4 step:3351 [D loss: 0.027540, acc.: 100.00%] [G loss: 0.714214]\n",
      "epoch:4 step:3352 [D loss: 0.037109, acc.: 100.00%] [G loss: 0.136165]\n",
      "epoch:4 step:3353 [D loss: 0.173327, acc.: 95.31%] [G loss: 0.064972]\n",
      "epoch:4 step:3354 [D loss: 0.098852, acc.: 96.88%] [G loss: 1.307411]\n",
      "epoch:4 step:3355 [D loss: 0.015429, acc.: 100.00%] [G loss: 1.543059]\n",
      "epoch:4 step:3356 [D loss: 0.068853, acc.: 99.22%] [G loss: 1.533082]\n",
      "epoch:4 step:3357 [D loss: 0.282273, acc.: 87.50%] [G loss: 2.438595]\n",
      "epoch:4 step:3358 [D loss: 0.179627, acc.: 94.53%] [G loss: 1.149667]\n",
      "epoch:4 step:3359 [D loss: 0.128526, acc.: 94.53%] [G loss: 2.772267]\n",
      "epoch:4 step:3360 [D loss: 3.377194, acc.: 9.38%] [G loss: 5.653100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3361 [D loss: 0.207851, acc.: 88.28%] [G loss: 6.458385]\n",
      "epoch:4 step:3362 [D loss: 0.333260, acc.: 83.59%] [G loss: 4.320461]\n",
      "epoch:4 step:3363 [D loss: 0.073242, acc.: 100.00%] [G loss: 2.799352]\n",
      "epoch:4 step:3364 [D loss: 0.186504, acc.: 94.53%] [G loss: 3.213303]\n",
      "epoch:4 step:3365 [D loss: 0.177786, acc.: 94.53%] [G loss: 3.376187]\n",
      "epoch:4 step:3366 [D loss: 0.066385, acc.: 98.44%] [G loss: 2.992538]\n",
      "epoch:4 step:3367 [D loss: 0.077311, acc.: 99.22%] [G loss: 3.210752]\n",
      "epoch:4 step:3368 [D loss: 0.111043, acc.: 98.44%] [G loss: 2.910176]\n",
      "epoch:4 step:3369 [D loss: 0.062033, acc.: 100.00%] [G loss: 3.028986]\n",
      "epoch:4 step:3370 [D loss: 0.147596, acc.: 96.88%] [G loss: 3.725886]\n",
      "epoch:4 step:3371 [D loss: 0.104584, acc.: 97.66%] [G loss: 2.712357]\n",
      "epoch:4 step:3372 [D loss: 0.239517, acc.: 92.19%] [G loss: 3.726354]\n",
      "epoch:4 step:3373 [D loss: 0.124440, acc.: 94.53%] [G loss: 2.563215]\n",
      "epoch:4 step:3374 [D loss: 0.301175, acc.: 84.38%] [G loss: 0.237403]\n",
      "epoch:4 step:3375 [D loss: 0.039062, acc.: 100.00%] [G loss: 0.602010]\n",
      "epoch:4 step:3376 [D loss: 0.023308, acc.: 100.00%] [G loss: 0.241700]\n",
      "epoch:4 step:3377 [D loss: 0.125416, acc.: 95.31%] [G loss: 1.591576]\n",
      "epoch:4 step:3378 [D loss: 0.076540, acc.: 98.44%] [G loss: 0.073351]\n",
      "epoch:4 step:3379 [D loss: 0.075666, acc.: 98.44%] [G loss: 0.045719]\n",
      "epoch:4 step:3380 [D loss: 0.124568, acc.: 98.44%] [G loss: 0.274884]\n",
      "epoch:4 step:3381 [D loss: 0.055569, acc.: 98.44%] [G loss: 0.128428]\n",
      "epoch:4 step:3382 [D loss: 0.097560, acc.: 99.22%] [G loss: 0.229455]\n",
      "epoch:4 step:3383 [D loss: 0.078563, acc.: 98.44%] [G loss: 2.361023]\n",
      "epoch:4 step:3384 [D loss: 0.015446, acc.: 100.00%] [G loss: 0.021230]\n",
      "epoch:4 step:3385 [D loss: 0.377110, acc.: 81.25%] [G loss: 4.348123]\n",
      "epoch:4 step:3386 [D loss: 1.163087, acc.: 55.47%] [G loss: 0.623959]\n",
      "epoch:4 step:3387 [D loss: 0.308594, acc.: 89.06%] [G loss: 1.271364]\n",
      "epoch:4 step:3388 [D loss: 0.113163, acc.: 98.44%] [G loss: 3.301114]\n",
      "epoch:4 step:3389 [D loss: 0.094630, acc.: 97.66%] [G loss: 0.904123]\n",
      "epoch:4 step:3390 [D loss: 0.112900, acc.: 97.66%] [G loss: 0.275093]\n",
      "epoch:4 step:3391 [D loss: 1.450694, acc.: 29.69%] [G loss: 2.876211]\n",
      "epoch:4 step:3392 [D loss: 0.113657, acc.: 96.88%] [G loss: 3.483073]\n",
      "epoch:4 step:3393 [D loss: 0.443206, acc.: 82.03%] [G loss: 1.658661]\n",
      "epoch:4 step:3394 [D loss: 0.099217, acc.: 98.44%] [G loss: 1.385318]\n",
      "epoch:4 step:3395 [D loss: 0.183352, acc.: 96.09%] [G loss: 2.296997]\n",
      "epoch:4 step:3396 [D loss: 0.205186, acc.: 92.19%] [G loss: 2.631060]\n",
      "epoch:4 step:3397 [D loss: 0.080602, acc.: 98.44%] [G loss: 2.648977]\n",
      "epoch:4 step:3398 [D loss: 0.323094, acc.: 86.72%] [G loss: 3.349893]\n",
      "epoch:4 step:3399 [D loss: 0.188784, acc.: 94.53%] [G loss: 2.554048]\n",
      "epoch:4 step:3400 [D loss: 0.195841, acc.: 95.31%] [G loss: 3.226752]\n",
      "epoch:4 step:3401 [D loss: 0.213610, acc.: 94.53%] [G loss: 3.728480]\n",
      "epoch:4 step:3402 [D loss: 0.112382, acc.: 96.09%] [G loss: 3.651345]\n",
      "epoch:4 step:3403 [D loss: 0.049261, acc.: 99.22%] [G loss: 3.267558]\n",
      "epoch:4 step:3404 [D loss: 0.111784, acc.: 96.88%] [G loss: 1.834077]\n",
      "epoch:4 step:3405 [D loss: 0.092441, acc.: 97.66%] [G loss: 0.335426]\n",
      "epoch:4 step:3406 [D loss: 0.060806, acc.: 100.00%] [G loss: 0.487737]\n",
      "epoch:4 step:3407 [D loss: 0.032920, acc.: 100.00%] [G loss: 0.158349]\n",
      "epoch:4 step:3408 [D loss: 0.050778, acc.: 100.00%] [G loss: 0.081999]\n",
      "epoch:4 step:3409 [D loss: 0.034556, acc.: 100.00%] [G loss: 0.203222]\n",
      "epoch:4 step:3410 [D loss: 0.020635, acc.: 100.00%] [G loss: 0.176643]\n",
      "epoch:4 step:3411 [D loss: 0.025593, acc.: 100.00%] [G loss: 0.102990]\n",
      "epoch:4 step:3412 [D loss: 0.029866, acc.: 100.00%] [G loss: 0.077322]\n",
      "epoch:4 step:3413 [D loss: 0.058283, acc.: 99.22%] [G loss: 0.092003]\n",
      "epoch:4 step:3414 [D loss: 0.009240, acc.: 100.00%] [G loss: 0.119320]\n",
      "epoch:4 step:3415 [D loss: 0.007594, acc.: 100.00%] [G loss: 0.125786]\n",
      "epoch:4 step:3416 [D loss: 0.028299, acc.: 100.00%] [G loss: 0.131133]\n",
      "epoch:4 step:3417 [D loss: 0.021732, acc.: 100.00%] [G loss: 0.109220]\n",
      "epoch:4 step:3418 [D loss: 0.068937, acc.: 98.44%] [G loss: 0.482747]\n",
      "epoch:4 step:3419 [D loss: 0.391590, acc.: 80.47%] [G loss: 6.131795]\n",
      "epoch:4 step:3420 [D loss: 0.185023, acc.: 90.62%] [G loss: 5.163598]\n",
      "epoch:4 step:3421 [D loss: 0.212582, acc.: 92.19%] [G loss: 0.260679]\n",
      "epoch:4 step:3422 [D loss: 0.007045, acc.: 100.00%] [G loss: 0.035415]\n",
      "epoch:4 step:3423 [D loss: 0.003272, acc.: 100.00%] [G loss: 0.031757]\n",
      "epoch:4 step:3424 [D loss: 0.041701, acc.: 99.22%] [G loss: 0.017864]\n",
      "epoch:4 step:3425 [D loss: 0.406313, acc.: 82.81%] [G loss: 0.772234]\n",
      "epoch:4 step:3426 [D loss: 0.087867, acc.: 96.09%] [G loss: 2.658307]\n",
      "epoch:4 step:3427 [D loss: 0.461923, acc.: 76.56%] [G loss: 0.645899]\n",
      "epoch:4 step:3428 [D loss: 0.086374, acc.: 96.88%] [G loss: 0.904030]\n",
      "epoch:4 step:3429 [D loss: 0.692705, acc.: 68.75%] [G loss: 7.165593]\n",
      "epoch:4 step:3430 [D loss: 1.802652, acc.: 50.78%] [G loss: 0.831042]\n",
      "epoch:4 step:3431 [D loss: 1.449327, acc.: 57.03%] [G loss: 4.049389]\n",
      "epoch:4 step:3432 [D loss: 0.352718, acc.: 84.38%] [G loss: 3.540486]\n",
      "epoch:4 step:3433 [D loss: 0.384150, acc.: 79.69%] [G loss: 2.525135]\n",
      "epoch:4 step:3434 [D loss: 0.275660, acc.: 90.62%] [G loss: 3.387913]\n",
      "epoch:4 step:3435 [D loss: 0.193570, acc.: 93.75%] [G loss: 3.271336]\n",
      "epoch:4 step:3436 [D loss: 0.413215, acc.: 78.91%] [G loss: 3.475394]\n",
      "epoch:4 step:3437 [D loss: 0.286440, acc.: 90.62%] [G loss: 4.440434]\n",
      "epoch:4 step:3438 [D loss: 0.210192, acc.: 92.97%] [G loss: 4.167078]\n",
      "epoch:4 step:3439 [D loss: 0.212687, acc.: 92.19%] [G loss: 2.495149]\n",
      "epoch:4 step:3440 [D loss: 0.220585, acc.: 90.62%] [G loss: 3.637963]\n",
      "epoch:4 step:3441 [D loss: 0.198796, acc.: 92.97%] [G loss: 3.012913]\n",
      "epoch:4 step:3442 [D loss: 0.028039, acc.: 100.00%] [G loss: 2.644064]\n",
      "epoch:4 step:3443 [D loss: 0.042355, acc.: 100.00%] [G loss: 1.001703]\n",
      "epoch:4 step:3444 [D loss: 0.167356, acc.: 92.19%] [G loss: 2.287591]\n",
      "epoch:4 step:3445 [D loss: 0.046396, acc.: 98.44%] [G loss: 1.459260]\n",
      "epoch:4 step:3446 [D loss: 0.239244, acc.: 89.84%] [G loss: 0.105335]\n",
      "epoch:4 step:3447 [D loss: 0.112150, acc.: 96.88%] [G loss: 0.210059]\n",
      "epoch:4 step:3448 [D loss: 0.007427, acc.: 100.00%] [G loss: 0.127344]\n",
      "epoch:4 step:3449 [D loss: 0.007574, acc.: 100.00%] [G loss: 0.246069]\n",
      "epoch:4 step:3450 [D loss: 0.010725, acc.: 100.00%] [G loss: 0.183636]\n",
      "epoch:4 step:3451 [D loss: 0.208159, acc.: 91.41%] [G loss: 1.926177]\n",
      "epoch:4 step:3452 [D loss: 0.208152, acc.: 92.19%] [G loss: 1.099715]\n",
      "epoch:4 step:3453 [D loss: 0.047320, acc.: 100.00%] [G loss: 0.293192]\n",
      "epoch:4 step:3454 [D loss: 0.078856, acc.: 99.22%] [G loss: 0.413135]\n",
      "epoch:4 step:3455 [D loss: 0.023137, acc.: 100.00%] [G loss: 0.309044]\n",
      "epoch:4 step:3456 [D loss: 0.010869, acc.: 100.00%] [G loss: 0.676469]\n",
      "epoch:4 step:3457 [D loss: 0.141110, acc.: 95.31%] [G loss: 1.657519]\n",
      "epoch:4 step:3458 [D loss: 0.266693, acc.: 91.41%] [G loss: 0.503610]\n",
      "epoch:4 step:3459 [D loss: 0.154508, acc.: 95.31%] [G loss: 1.804254]\n",
      "epoch:4 step:3460 [D loss: 0.375727, acc.: 83.59%] [G loss: 4.573240]\n",
      "epoch:4 step:3461 [D loss: 0.391744, acc.: 81.25%] [G loss: 1.728940]\n",
      "epoch:4 step:3462 [D loss: 0.302796, acc.: 83.59%] [G loss: 4.471748]\n",
      "epoch:4 step:3463 [D loss: 1.101964, acc.: 43.75%] [G loss: 5.695227]\n",
      "epoch:4 step:3464 [D loss: 0.139173, acc.: 93.75%] [G loss: 6.526553]\n",
      "epoch:4 step:3465 [D loss: 0.209082, acc.: 92.19%] [G loss: 4.303289]\n",
      "epoch:4 step:3466 [D loss: 0.120494, acc.: 96.88%] [G loss: 3.110841]\n",
      "epoch:4 step:3467 [D loss: 0.112930, acc.: 97.66%] [G loss: 2.290205]\n",
      "epoch:4 step:3468 [D loss: 0.074421, acc.: 99.22%] [G loss: 3.386556]\n",
      "epoch:4 step:3469 [D loss: 0.093824, acc.: 98.44%] [G loss: 1.700074]\n",
      "epoch:4 step:3470 [D loss: 0.168240, acc.: 93.75%] [G loss: 1.887334]\n",
      "epoch:4 step:3471 [D loss: 0.155836, acc.: 94.53%] [G loss: 1.340235]\n",
      "epoch:4 step:3472 [D loss: 0.108649, acc.: 98.44%] [G loss: 0.108732]\n",
      "epoch:4 step:3473 [D loss: 0.050222, acc.: 99.22%] [G loss: 0.093775]\n",
      "epoch:4 step:3474 [D loss: 0.139218, acc.: 97.66%] [G loss: 0.391781]\n",
      "epoch:4 step:3475 [D loss: 0.013160, acc.: 100.00%] [G loss: 0.662352]\n",
      "epoch:4 step:3476 [D loss: 0.216811, acc.: 90.62%] [G loss: 0.041565]\n",
      "epoch:4 step:3477 [D loss: 0.822365, acc.: 62.50%] [G loss: 5.478844]\n",
      "epoch:4 step:3478 [D loss: 0.827395, acc.: 64.06%] [G loss: 4.179772]\n",
      "epoch:4 step:3479 [D loss: 0.094339, acc.: 97.66%] [G loss: 1.613015]\n",
      "epoch:4 step:3480 [D loss: 0.069760, acc.: 100.00%] [G loss: 0.602735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3481 [D loss: 0.014452, acc.: 100.00%] [G loss: 0.808220]\n",
      "epoch:4 step:3482 [D loss: 0.061417, acc.: 98.44%] [G loss: 0.537970]\n",
      "epoch:4 step:3483 [D loss: 0.079032, acc.: 100.00%] [G loss: 1.407336]\n",
      "epoch:4 step:3484 [D loss: 0.036508, acc.: 100.00%] [G loss: 1.798268]\n",
      "epoch:4 step:3485 [D loss: 0.496750, acc.: 73.44%] [G loss: 1.014095]\n",
      "epoch:4 step:3486 [D loss: 0.075722, acc.: 98.44%] [G loss: 2.803703]\n",
      "epoch:4 step:3487 [D loss: 0.203378, acc.: 92.19%] [G loss: 1.289354]\n",
      "epoch:4 step:3488 [D loss: 0.246999, acc.: 92.19%] [G loss: 2.363190]\n",
      "epoch:4 step:3489 [D loss: 0.327587, acc.: 85.94%] [G loss: 4.384141]\n",
      "epoch:4 step:3490 [D loss: 0.183642, acc.: 95.31%] [G loss: 3.685580]\n",
      "epoch:4 step:3491 [D loss: 0.184860, acc.: 95.31%] [G loss: 3.654486]\n",
      "epoch:4 step:3492 [D loss: 0.126571, acc.: 97.66%] [G loss: 3.970573]\n",
      "epoch:4 step:3493 [D loss: 0.485097, acc.: 75.00%] [G loss: 5.537962]\n",
      "epoch:4 step:3494 [D loss: 0.453375, acc.: 75.78%] [G loss: 2.819056]\n",
      "epoch:4 step:3495 [D loss: 0.117811, acc.: 96.88%] [G loss: 3.436777]\n",
      "epoch:4 step:3496 [D loss: 0.148209, acc.: 95.31%] [G loss: 3.642874]\n",
      "epoch:4 step:3497 [D loss: 0.135962, acc.: 96.88%] [G loss: 3.573837]\n",
      "epoch:4 step:3498 [D loss: 0.288486, acc.: 88.28%] [G loss: 4.917367]\n",
      "epoch:4 step:3499 [D loss: 0.334310, acc.: 88.28%] [G loss: 1.909659]\n",
      "epoch:4 step:3500 [D loss: 0.098159, acc.: 98.44%] [G loss: 1.322676]\n",
      "epoch:4 step:3501 [D loss: 0.074235, acc.: 99.22%] [G loss: 1.435772]\n",
      "epoch:4 step:3502 [D loss: 0.100001, acc.: 99.22%] [G loss: 0.969355]\n",
      "epoch:4 step:3503 [D loss: 0.095841, acc.: 98.44%] [G loss: 0.074544]\n",
      "epoch:4 step:3504 [D loss: 0.065570, acc.: 100.00%] [G loss: 0.248138]\n",
      "epoch:4 step:3505 [D loss: 0.032054, acc.: 99.22%] [G loss: 0.643492]\n",
      "epoch:4 step:3506 [D loss: 0.288471, acc.: 86.72%] [G loss: 2.164022]\n",
      "epoch:4 step:3507 [D loss: 1.723579, acc.: 31.25%] [G loss: 3.573643]\n",
      "epoch:4 step:3508 [D loss: 0.309874, acc.: 81.25%] [G loss: 2.637219]\n",
      "epoch:4 step:3509 [D loss: 0.101249, acc.: 95.31%] [G loss: 0.500066]\n",
      "epoch:4 step:3510 [D loss: 0.360130, acc.: 83.59%] [G loss: 1.433457]\n",
      "epoch:4 step:3511 [D loss: 0.145913, acc.: 94.53%] [G loss: 1.299912]\n",
      "epoch:4 step:3512 [D loss: 0.026049, acc.: 100.00%] [G loss: 1.624937]\n",
      "epoch:4 step:3513 [D loss: 0.175940, acc.: 93.75%] [G loss: 1.972314]\n",
      "epoch:4 step:3514 [D loss: 0.467210, acc.: 72.66%] [G loss: 0.417852]\n",
      "epoch:4 step:3515 [D loss: 1.220412, acc.: 57.03%] [G loss: 5.555404]\n",
      "epoch:4 step:3516 [D loss: 0.587417, acc.: 72.66%] [G loss: 5.029948]\n",
      "epoch:4 step:3517 [D loss: 0.527330, acc.: 73.44%] [G loss: 2.497777]\n",
      "epoch:4 step:3518 [D loss: 0.214625, acc.: 89.84%] [G loss: 2.267324]\n",
      "epoch:4 step:3519 [D loss: 0.089573, acc.: 98.44%] [G loss: 2.870062]\n",
      "epoch:4 step:3520 [D loss: 0.270586, acc.: 90.62%] [G loss: 4.102476]\n",
      "epoch:4 step:3521 [D loss: 0.180935, acc.: 95.31%] [G loss: 3.184912]\n",
      "epoch:4 step:3522 [D loss: 0.682703, acc.: 61.72%] [G loss: 3.438509]\n",
      "epoch:4 step:3523 [D loss: 0.241646, acc.: 90.62%] [G loss: 3.344750]\n",
      "epoch:4 step:3524 [D loss: 0.293134, acc.: 87.50%] [G loss: 3.334664]\n",
      "epoch:4 step:3525 [D loss: 0.199497, acc.: 94.53%] [G loss: 4.242095]\n",
      "epoch:4 step:3526 [D loss: 0.346874, acc.: 85.94%] [G loss: 4.020974]\n",
      "epoch:4 step:3527 [D loss: 0.200935, acc.: 92.97%] [G loss: 4.121797]\n",
      "epoch:4 step:3528 [D loss: 0.152438, acc.: 98.44%] [G loss: 3.670951]\n",
      "epoch:4 step:3529 [D loss: 0.043193, acc.: 100.00%] [G loss: 3.048303]\n",
      "epoch:4 step:3530 [D loss: 0.126355, acc.: 96.88%] [G loss: 2.810674]\n",
      "epoch:4 step:3531 [D loss: 0.164116, acc.: 93.75%] [G loss: 2.642333]\n",
      "epoch:4 step:3532 [D loss: 0.042992, acc.: 99.22%] [G loss: 1.962186]\n",
      "epoch:4 step:3533 [D loss: 0.078827, acc.: 99.22%] [G loss: 0.746210]\n",
      "epoch:4 step:3534 [D loss: 0.061878, acc.: 99.22%] [G loss: 0.623487]\n",
      "epoch:4 step:3535 [D loss: 1.011320, acc.: 50.78%] [G loss: 6.297926]\n",
      "epoch:4 step:3536 [D loss: 0.577997, acc.: 69.53%] [G loss: 5.118608]\n",
      "epoch:4 step:3537 [D loss: 0.294199, acc.: 90.62%] [G loss: 0.944193]\n",
      "epoch:4 step:3538 [D loss: 0.106600, acc.: 96.88%] [G loss: 0.431172]\n",
      "epoch:4 step:3539 [D loss: 0.014863, acc.: 100.00%] [G loss: 0.872578]\n",
      "epoch:4 step:3540 [D loss: 0.021792, acc.: 100.00%] [G loss: 1.001307]\n",
      "epoch:4 step:3541 [D loss: 0.242805, acc.: 86.72%] [G loss: 1.504529]\n",
      "epoch:4 step:3542 [D loss: 0.130332, acc.: 96.09%] [G loss: 1.866874]\n",
      "epoch:4 step:3543 [D loss: 0.131400, acc.: 94.53%] [G loss: 0.374503]\n",
      "epoch:4 step:3544 [D loss: 0.665501, acc.: 68.75%] [G loss: 4.227963]\n",
      "epoch:4 step:3545 [D loss: 0.262457, acc.: 86.72%] [G loss: 3.875271]\n",
      "epoch:4 step:3546 [D loss: 0.508997, acc.: 77.34%] [G loss: 1.428315]\n",
      "epoch:4 step:3547 [D loss: 0.287772, acc.: 85.94%] [G loss: 2.515696]\n",
      "epoch:4 step:3548 [D loss: 0.135127, acc.: 96.88%] [G loss: 3.048496]\n",
      "epoch:4 step:3549 [D loss: 0.284310, acc.: 89.06%] [G loss: 2.096807]\n",
      "epoch:4 step:3550 [D loss: 0.330806, acc.: 80.47%] [G loss: 4.025136]\n",
      "epoch:4 step:3551 [D loss: 0.316325, acc.: 84.38%] [G loss: 3.349785]\n",
      "epoch:4 step:3552 [D loss: 0.748849, acc.: 58.59%] [G loss: 2.788851]\n",
      "epoch:4 step:3553 [D loss: 0.144921, acc.: 96.88%] [G loss: 3.431213]\n",
      "epoch:4 step:3554 [D loss: 0.534241, acc.: 72.66%] [G loss: 2.790028]\n",
      "epoch:4 step:3555 [D loss: 0.316066, acc.: 88.28%] [G loss: 3.374199]\n",
      "epoch:4 step:3556 [D loss: 0.187453, acc.: 94.53%] [G loss: 4.120244]\n",
      "epoch:4 step:3557 [D loss: 0.164389, acc.: 96.09%] [G loss: 3.555849]\n",
      "epoch:4 step:3558 [D loss: 0.141573, acc.: 96.09%] [G loss: 2.374363]\n",
      "epoch:4 step:3559 [D loss: 0.069452, acc.: 98.44%] [G loss: 1.848054]\n",
      "epoch:4 step:3560 [D loss: 0.204292, acc.: 92.19%] [G loss: 2.692418]\n",
      "epoch:4 step:3561 [D loss: 0.748700, acc.: 59.38%] [G loss: 4.894139]\n",
      "epoch:4 step:3562 [D loss: 0.591709, acc.: 70.31%] [G loss: 2.787588]\n",
      "epoch:4 step:3563 [D loss: 0.170480, acc.: 95.31%] [G loss: 2.327843]\n",
      "epoch:4 step:3564 [D loss: 0.074186, acc.: 98.44%] [G loss: 2.097708]\n",
      "epoch:4 step:3565 [D loss: 0.210094, acc.: 92.97%] [G loss: 2.164056]\n",
      "epoch:4 step:3566 [D loss: 0.084225, acc.: 96.88%] [G loss: 1.015638]\n",
      "epoch:4 step:3567 [D loss: 0.110680, acc.: 100.00%] [G loss: 0.579263]\n",
      "epoch:4 step:3568 [D loss: 0.073955, acc.: 98.44%] [G loss: 1.015675]\n",
      "epoch:4 step:3569 [D loss: 0.133105, acc.: 97.66%] [G loss: 1.689812]\n",
      "epoch:4 step:3570 [D loss: 0.381569, acc.: 82.81%] [G loss: 1.499606]\n",
      "epoch:4 step:3571 [D loss: 0.313853, acc.: 86.72%] [G loss: 0.738399]\n",
      "epoch:4 step:3572 [D loss: 0.702755, acc.: 63.28%] [G loss: 6.200721]\n",
      "epoch:4 step:3573 [D loss: 1.538905, acc.: 52.34%] [G loss: 3.590384]\n",
      "epoch:4 step:3574 [D loss: 0.152201, acc.: 93.75%] [G loss: 2.738841]\n",
      "epoch:4 step:3575 [D loss: 0.197231, acc.: 92.97%] [G loss: 3.509449]\n",
      "epoch:4 step:3576 [D loss: 0.104655, acc.: 99.22%] [G loss: 3.342749]\n",
      "epoch:4 step:3577 [D loss: 0.173372, acc.: 98.44%] [G loss: 2.787856]\n",
      "epoch:4 step:3578 [D loss: 0.299026, acc.: 88.28%] [G loss: 3.203375]\n",
      "epoch:4 step:3579 [D loss: 0.507058, acc.: 72.66%] [G loss: 2.716602]\n",
      "epoch:4 step:3580 [D loss: 0.132559, acc.: 97.66%] [G loss: 2.723182]\n",
      "epoch:4 step:3581 [D loss: 0.247892, acc.: 88.28%] [G loss: 2.203126]\n",
      "epoch:4 step:3582 [D loss: 0.182374, acc.: 96.88%] [G loss: 2.846952]\n",
      "epoch:4 step:3583 [D loss: 0.107661, acc.: 98.44%] [G loss: 2.706434]\n",
      "epoch:4 step:3584 [D loss: 0.666791, acc.: 67.19%] [G loss: 4.841612]\n",
      "epoch:4 step:3585 [D loss: 0.637504, acc.: 66.41%] [G loss: 3.466932]\n",
      "epoch:4 step:3586 [D loss: 0.194785, acc.: 94.53%] [G loss: 3.858691]\n",
      "epoch:4 step:3587 [D loss: 0.104154, acc.: 97.66%] [G loss: 3.488899]\n",
      "epoch:4 step:3588 [D loss: 0.095075, acc.: 98.44%] [G loss: 3.504203]\n",
      "epoch:4 step:3589 [D loss: 0.409595, acc.: 86.72%] [G loss: 4.235607]\n",
      "epoch:4 step:3590 [D loss: 0.193901, acc.: 92.19%] [G loss: 3.439807]\n",
      "epoch:4 step:3591 [D loss: 0.280088, acc.: 88.28%] [G loss: 2.198879]\n",
      "epoch:4 step:3592 [D loss: 0.132656, acc.: 98.44%] [G loss: 3.491782]\n",
      "epoch:4 step:3593 [D loss: 0.075927, acc.: 99.22%] [G loss: 1.270184]\n",
      "epoch:4 step:3594 [D loss: 0.113626, acc.: 97.66%] [G loss: 1.525243]\n",
      "epoch:4 step:3595 [D loss: 0.081148, acc.: 97.66%] [G loss: 1.214809]\n",
      "epoch:4 step:3596 [D loss: 0.074529, acc.: 98.44%] [G loss: 0.457550]\n",
      "epoch:4 step:3597 [D loss: 0.474830, acc.: 74.22%] [G loss: 4.864227]\n",
      "epoch:4 step:3598 [D loss: 0.268220, acc.: 88.28%] [G loss: 1.674304]\n",
      "epoch:4 step:3599 [D loss: 0.261734, acc.: 87.50%] [G loss: 0.087433]\n",
      "epoch:4 step:3600 [D loss: 0.477583, acc.: 75.00%] [G loss: 2.276092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3601 [D loss: 0.074887, acc.: 96.88%] [G loss: 3.250149]\n",
      "epoch:4 step:3602 [D loss: 0.176158, acc.: 92.97%] [G loss: 1.503760]\n",
      "epoch:4 step:3603 [D loss: 0.234308, acc.: 89.84%] [G loss: 2.041447]\n",
      "epoch:4 step:3604 [D loss: 0.226661, acc.: 89.84%] [G loss: 1.378794]\n",
      "epoch:4 step:3605 [D loss: 0.306380, acc.: 86.72%] [G loss: 0.599023]\n",
      "epoch:4 step:3606 [D loss: 0.488326, acc.: 75.78%] [G loss: 3.399731]\n",
      "epoch:4 step:3607 [D loss: 0.151937, acc.: 92.97%] [G loss: 2.555869]\n",
      "epoch:4 step:3608 [D loss: 0.084090, acc.: 98.44%] [G loss: 1.364127]\n",
      "epoch:4 step:3609 [D loss: 0.371086, acc.: 78.12%] [G loss: 3.778051]\n",
      "epoch:4 step:3610 [D loss: 0.910032, acc.: 60.16%] [G loss: 2.750211]\n",
      "epoch:4 step:3611 [D loss: 0.119447, acc.: 99.22%] [G loss: 3.242685]\n",
      "epoch:4 step:3612 [D loss: 0.207904, acc.: 92.19%] [G loss: 3.307956]\n",
      "epoch:4 step:3613 [D loss: 0.229193, acc.: 89.06%] [G loss: 3.442003]\n",
      "epoch:4 step:3614 [D loss: 0.280870, acc.: 92.19%] [G loss: 2.472826]\n",
      "epoch:4 step:3615 [D loss: 0.367877, acc.: 83.59%] [G loss: 3.371608]\n",
      "epoch:4 step:3616 [D loss: 0.154985, acc.: 97.66%] [G loss: 3.620640]\n",
      "epoch:4 step:3617 [D loss: 0.160212, acc.: 93.75%] [G loss: 4.157267]\n",
      "epoch:4 step:3618 [D loss: 0.247732, acc.: 92.97%] [G loss: 4.150334]\n",
      "epoch:4 step:3619 [D loss: 0.103538, acc.: 98.44%] [G loss: 3.437432]\n",
      "epoch:4 step:3620 [D loss: 0.169752, acc.: 94.53%] [G loss: 3.244616]\n",
      "epoch:4 step:3621 [D loss: 0.076897, acc.: 98.44%] [G loss: 4.158171]\n",
      "epoch:4 step:3622 [D loss: 0.435607, acc.: 81.25%] [G loss: 3.943260]\n",
      "epoch:4 step:3623 [D loss: 0.074696, acc.: 98.44%] [G loss: 3.584413]\n",
      "epoch:4 step:3624 [D loss: 0.803930, acc.: 57.81%] [G loss: 4.940193]\n",
      "epoch:4 step:3625 [D loss: 0.542670, acc.: 69.53%] [G loss: 3.321590]\n",
      "epoch:4 step:3626 [D loss: 0.055466, acc.: 100.00%] [G loss: 2.361573]\n",
      "epoch:4 step:3627 [D loss: 0.221200, acc.: 89.06%] [G loss: 4.132669]\n",
      "epoch:4 step:3628 [D loss: 0.150210, acc.: 94.53%] [G loss: 2.326740]\n",
      "epoch:4 step:3629 [D loss: 0.035295, acc.: 100.00%] [G loss: 1.941484]\n",
      "epoch:4 step:3630 [D loss: 0.357373, acc.: 82.03%] [G loss: 3.655258]\n",
      "epoch:4 step:3631 [D loss: 0.298954, acc.: 84.38%] [G loss: 1.845894]\n",
      "epoch:4 step:3632 [D loss: 0.461980, acc.: 75.78%] [G loss: 1.467596]\n",
      "epoch:4 step:3633 [D loss: 0.094410, acc.: 98.44%] [G loss: 4.778068]\n",
      "epoch:4 step:3634 [D loss: 0.263937, acc.: 88.28%] [G loss: 1.155396]\n",
      "epoch:4 step:3635 [D loss: 0.497460, acc.: 72.66%] [G loss: 5.769005]\n",
      "epoch:4 step:3636 [D loss: 0.438121, acc.: 74.22%] [G loss: 3.173963]\n",
      "epoch:4 step:3637 [D loss: 0.405745, acc.: 85.16%] [G loss: 3.901724]\n",
      "epoch:4 step:3638 [D loss: 0.220224, acc.: 93.75%] [G loss: 3.268773]\n",
      "epoch:4 step:3639 [D loss: 0.193176, acc.: 94.53%] [G loss: 3.697896]\n",
      "epoch:4 step:3640 [D loss: 0.530414, acc.: 73.44%] [G loss: 2.624384]\n",
      "epoch:4 step:3641 [D loss: 0.199379, acc.: 92.19%] [G loss: 4.100438]\n",
      "epoch:4 step:3642 [D loss: 0.181488, acc.: 93.75%] [G loss: 2.184967]\n",
      "epoch:4 step:3643 [D loss: 1.340061, acc.: 37.50%] [G loss: 5.116346]\n",
      "epoch:4 step:3644 [D loss: 0.318798, acc.: 84.38%] [G loss: 5.019199]\n",
      "epoch:4 step:3645 [D loss: 0.318562, acc.: 83.59%] [G loss: 3.302243]\n",
      "epoch:4 step:3646 [D loss: 0.195168, acc.: 94.53%] [G loss: 3.021659]\n",
      "epoch:4 step:3647 [D loss: 0.127523, acc.: 95.31%] [G loss: 3.715503]\n",
      "epoch:4 step:3648 [D loss: 0.298656, acc.: 89.06%] [G loss: 2.792231]\n",
      "epoch:4 step:3649 [D loss: 0.131141, acc.: 98.44%] [G loss: 2.501379]\n",
      "epoch:4 step:3650 [D loss: 0.466420, acc.: 76.56%] [G loss: 3.428793]\n",
      "epoch:4 step:3651 [D loss: 0.298465, acc.: 85.94%] [G loss: 4.348017]\n",
      "epoch:4 step:3652 [D loss: 0.232837, acc.: 93.75%] [G loss: 4.476811]\n",
      "epoch:4 step:3653 [D loss: 0.193936, acc.: 91.41%] [G loss: 1.922619]\n",
      "epoch:4 step:3654 [D loss: 0.230732, acc.: 90.62%] [G loss: 3.392951]\n",
      "epoch:4 step:3655 [D loss: 0.079005, acc.: 98.44%] [G loss: 3.555910]\n",
      "epoch:4 step:3656 [D loss: 0.419762, acc.: 81.25%] [G loss: 2.939708]\n",
      "epoch:4 step:3657 [D loss: 0.103497, acc.: 96.88%] [G loss: 1.874184]\n",
      "epoch:4 step:3658 [D loss: 0.112509, acc.: 96.88%] [G loss: 0.556920]\n",
      "epoch:4 step:3659 [D loss: 0.152673, acc.: 95.31%] [G loss: 0.843816]\n",
      "epoch:4 step:3660 [D loss: 0.042246, acc.: 99.22%] [G loss: 1.187324]\n",
      "epoch:4 step:3661 [D loss: 0.917342, acc.: 53.91%] [G loss: 3.368202]\n",
      "epoch:4 step:3662 [D loss: 0.022577, acc.: 100.00%] [G loss: 6.857983]\n",
      "epoch:4 step:3663 [D loss: 0.414984, acc.: 78.12%] [G loss: 1.054994]\n",
      "epoch:4 step:3664 [D loss: 0.163405, acc.: 93.75%] [G loss: 1.091331]\n",
      "epoch:4 step:3665 [D loss: 0.028642, acc.: 100.00%] [G loss: 1.341412]\n",
      "epoch:4 step:3666 [D loss: 0.113395, acc.: 97.66%] [G loss: 0.720146]\n",
      "epoch:4 step:3667 [D loss: 0.126062, acc.: 95.31%] [G loss: 1.092544]\n",
      "epoch:4 step:3668 [D loss: 0.027619, acc.: 100.00%] [G loss: 0.829957]\n",
      "epoch:4 step:3669 [D loss: 0.473046, acc.: 80.47%] [G loss: 0.364682]\n",
      "epoch:4 step:3670 [D loss: 0.254323, acc.: 89.84%] [G loss: 4.015283]\n",
      "epoch:4 step:3671 [D loss: 0.903091, acc.: 57.81%] [G loss: 1.740772]\n",
      "epoch:4 step:3672 [D loss: 0.038448, acc.: 100.00%] [G loss: 1.798134]\n",
      "epoch:4 step:3673 [D loss: 0.025292, acc.: 99.22%] [G loss: 1.005876]\n",
      "epoch:4 step:3674 [D loss: 0.564045, acc.: 70.31%] [G loss: 5.650403]\n",
      "epoch:4 step:3675 [D loss: 0.526636, acc.: 74.22%] [G loss: 4.205815]\n",
      "epoch:4 step:3676 [D loss: 0.396898, acc.: 83.59%] [G loss: 4.790957]\n",
      "epoch:4 step:3677 [D loss: 0.030982, acc.: 100.00%] [G loss: 4.192893]\n",
      "epoch:4 step:3678 [D loss: 0.167600, acc.: 96.88%] [G loss: 3.131477]\n",
      "epoch:4 step:3679 [D loss: 0.195689, acc.: 92.19%] [G loss: 3.215014]\n",
      "epoch:4 step:3680 [D loss: 0.232784, acc.: 92.97%] [G loss: 4.289263]\n",
      "epoch:4 step:3681 [D loss: 0.181126, acc.: 92.97%] [G loss: 2.456509]\n",
      "epoch:4 step:3682 [D loss: 0.325230, acc.: 84.38%] [G loss: 3.447741]\n",
      "epoch:4 step:3683 [D loss: 0.269896, acc.: 89.06%] [G loss: 2.274532]\n",
      "epoch:4 step:3684 [D loss: 0.276637, acc.: 89.84%] [G loss: 2.097280]\n",
      "epoch:4 step:3685 [D loss: 0.110662, acc.: 96.88%] [G loss: 1.404684]\n",
      "epoch:4 step:3686 [D loss: 0.101754, acc.: 99.22%] [G loss: 0.360570]\n",
      "epoch:4 step:3687 [D loss: 0.184954, acc.: 92.19%] [G loss: 0.524954]\n",
      "epoch:4 step:3688 [D loss: 0.078200, acc.: 99.22%] [G loss: 1.410438]\n",
      "epoch:4 step:3689 [D loss: 0.081631, acc.: 99.22%] [G loss: 1.956052]\n",
      "epoch:4 step:3690 [D loss: 0.523040, acc.: 75.00%] [G loss: 4.508109]\n",
      "epoch:4 step:3691 [D loss: 0.299566, acc.: 82.81%] [G loss: 2.619281]\n",
      "epoch:4 step:3692 [D loss: 0.200798, acc.: 91.41%] [G loss: 0.810224]\n",
      "epoch:4 step:3693 [D loss: 0.581075, acc.: 73.44%] [G loss: 3.719922]\n",
      "epoch:4 step:3694 [D loss: 0.390034, acc.: 76.56%] [G loss: 2.499717]\n",
      "epoch:4 step:3695 [D loss: 0.065317, acc.: 98.44%] [G loss: 1.345280]\n",
      "epoch:4 step:3696 [D loss: 0.088193, acc.: 97.66%] [G loss: 1.103115]\n",
      "epoch:4 step:3697 [D loss: 0.144539, acc.: 94.53%] [G loss: 2.087634]\n",
      "epoch:4 step:3698 [D loss: 0.709226, acc.: 63.28%] [G loss: 2.819879]\n",
      "epoch:4 step:3699 [D loss: 0.139519, acc.: 95.31%] [G loss: 1.575056]\n",
      "epoch:4 step:3700 [D loss: 0.075713, acc.: 96.88%] [G loss: 2.136178]\n",
      "epoch:4 step:3701 [D loss: 0.158095, acc.: 95.31%] [G loss: 1.524709]\n",
      "epoch:4 step:3702 [D loss: 0.137150, acc.: 96.09%] [G loss: 1.997947]\n",
      "epoch:4 step:3703 [D loss: 0.224427, acc.: 90.62%] [G loss: 1.587099]\n",
      "epoch:4 step:3704 [D loss: 0.368104, acc.: 85.16%] [G loss: 4.150736]\n",
      "epoch:4 step:3705 [D loss: 0.234545, acc.: 89.84%] [G loss: 2.891240]\n",
      "epoch:4 step:3706 [D loss: 0.433910, acc.: 76.56%] [G loss: 4.371715]\n",
      "epoch:4 step:3707 [D loss: 0.128098, acc.: 96.88%] [G loss: 4.111339]\n",
      "epoch:4 step:3708 [D loss: 0.544690, acc.: 71.88%] [G loss: 4.119893]\n",
      "epoch:4 step:3709 [D loss: 0.341944, acc.: 86.72%] [G loss: 6.207232]\n",
      "epoch:4 step:3710 [D loss: 0.102583, acc.: 98.44%] [G loss: 4.727307]\n",
      "epoch:4 step:3711 [D loss: 0.127771, acc.: 96.88%] [G loss: 3.692510]\n",
      "epoch:4 step:3712 [D loss: 0.152754, acc.: 95.31%] [G loss: 4.435979]\n",
      "epoch:4 step:3713 [D loss: 0.099646, acc.: 97.66%] [G loss: 3.162627]\n",
      "epoch:4 step:3714 [D loss: 0.137050, acc.: 96.88%] [G loss: 4.914505]\n",
      "epoch:4 step:3715 [D loss: 0.255851, acc.: 86.72%] [G loss: 3.517011]\n",
      "epoch:4 step:3716 [D loss: 0.123259, acc.: 96.09%] [G loss: 2.621891]\n",
      "epoch:4 step:3717 [D loss: 0.087554, acc.: 97.66%] [G loss: 2.138445]\n",
      "epoch:4 step:3718 [D loss: 0.039656, acc.: 99.22%] [G loss: 0.890293]\n",
      "epoch:4 step:3719 [D loss: 0.263125, acc.: 90.62%] [G loss: 5.594661]\n",
      "epoch:4 step:3720 [D loss: 0.369996, acc.: 81.25%] [G loss: 1.326255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3721 [D loss: 0.250094, acc.: 92.19%] [G loss: 3.080279]\n",
      "epoch:4 step:3722 [D loss: 0.054383, acc.: 97.66%] [G loss: 2.422934]\n",
      "epoch:4 step:3723 [D loss: 0.084576, acc.: 97.66%] [G loss: 0.386505]\n",
      "epoch:4 step:3724 [D loss: 0.029489, acc.: 100.00%] [G loss: 0.107775]\n",
      "epoch:4 step:3725 [D loss: 0.075013, acc.: 98.44%] [G loss: 0.207711]\n",
      "epoch:4 step:3726 [D loss: 0.017584, acc.: 100.00%] [G loss: 0.248312]\n",
      "epoch:4 step:3727 [D loss: 0.204183, acc.: 91.41%] [G loss: 3.876943]\n",
      "epoch:4 step:3728 [D loss: 0.722306, acc.: 65.62%] [G loss: 0.158233]\n",
      "epoch:4 step:3729 [D loss: 0.527523, acc.: 77.34%] [G loss: 4.779794]\n",
      "epoch:4 step:3730 [D loss: 0.056798, acc.: 99.22%] [G loss: 5.056679]\n",
      "epoch:4 step:3731 [D loss: 0.638819, acc.: 71.09%] [G loss: 1.206339]\n",
      "epoch:4 step:3732 [D loss: 0.105912, acc.: 97.66%] [G loss: 2.172993]\n",
      "epoch:4 step:3733 [D loss: 0.037242, acc.: 99.22%] [G loss: 3.087929]\n",
      "epoch:4 step:3734 [D loss: 0.046339, acc.: 100.00%] [G loss: 2.876179]\n",
      "epoch:4 step:3735 [D loss: 0.249269, acc.: 90.62%] [G loss: 4.464841]\n",
      "epoch:4 step:3736 [D loss: 0.208027, acc.: 91.41%] [G loss: 1.322317]\n",
      "epoch:4 step:3737 [D loss: 0.747336, acc.: 64.06%] [G loss: 4.338130]\n",
      "epoch:4 step:3738 [D loss: 0.017994, acc.: 100.00%] [G loss: 5.413060]\n",
      "epoch:4 step:3739 [D loss: 0.217962, acc.: 91.41%] [G loss: 3.749302]\n",
      "epoch:4 step:3740 [D loss: 0.144554, acc.: 95.31%] [G loss: 4.695405]\n",
      "epoch:4 step:3741 [D loss: 0.123414, acc.: 96.88%] [G loss: 3.798014]\n",
      "epoch:4 step:3742 [D loss: 0.311396, acc.: 86.72%] [G loss: 4.306635]\n",
      "epoch:4 step:3743 [D loss: 0.101021, acc.: 99.22%] [G loss: 4.014801]\n",
      "epoch:4 step:3744 [D loss: 0.253891, acc.: 92.19%] [G loss: 4.254532]\n",
      "epoch:4 step:3745 [D loss: 0.138967, acc.: 96.09%] [G loss: 5.133659]\n",
      "epoch:4 step:3746 [D loss: 0.131540, acc.: 95.31%] [G loss: 4.389493]\n",
      "epoch:4 step:3747 [D loss: 0.753392, acc.: 56.25%] [G loss: 5.572762]\n",
      "epoch:4 step:3748 [D loss: 0.037425, acc.: 100.00%] [G loss: 6.546662]\n",
      "epoch:4 step:3749 [D loss: 0.240521, acc.: 85.94%] [G loss: 2.704609]\n",
      "epoch:4 step:3750 [D loss: 0.065399, acc.: 98.44%] [G loss: 3.251631]\n",
      "epoch:4 step:3751 [D loss: 0.052522, acc.: 100.00%] [G loss: 2.342857]\n",
      "epoch:4 step:3752 [D loss: 0.048423, acc.: 99.22%] [G loss: 1.904810]\n",
      "epoch:4 step:3753 [D loss: 0.237960, acc.: 91.41%] [G loss: 3.589710]\n",
      "epoch:4 step:3754 [D loss: 0.245844, acc.: 86.72%] [G loss: 2.447788]\n",
      "epoch:4 step:3755 [D loss: 0.166423, acc.: 95.31%] [G loss: 0.310516]\n",
      "epoch:4 step:3756 [D loss: 0.239379, acc.: 92.19%] [G loss: 1.317870]\n",
      "epoch:4 step:3757 [D loss: 0.104667, acc.: 97.66%] [G loss: 1.162387]\n",
      "epoch:4 step:3758 [D loss: 0.024899, acc.: 100.00%] [G loss: 2.611352]\n",
      "epoch:4 step:3759 [D loss: 0.492879, acc.: 76.56%] [G loss: 2.246311]\n",
      "epoch:4 step:3760 [D loss: 0.042961, acc.: 99.22%] [G loss: 2.568748]\n",
      "epoch:4 step:3761 [D loss: 0.019655, acc.: 99.22%] [G loss: 1.046724]\n",
      "epoch:4 step:3762 [D loss: 0.330002, acc.: 86.72%] [G loss: 0.672455]\n",
      "epoch:4 step:3763 [D loss: 0.054508, acc.: 98.44%] [G loss: 1.320467]\n",
      "epoch:4 step:3764 [D loss: 0.094385, acc.: 96.88%] [G loss: 1.031800]\n",
      "epoch:4 step:3765 [D loss: 0.070662, acc.: 99.22%] [G loss: 0.481314]\n",
      "epoch:4 step:3766 [D loss: 0.431180, acc.: 75.00%] [G loss: 5.997641]\n",
      "epoch:4 step:3767 [D loss: 1.609321, acc.: 51.56%] [G loss: 2.096473]\n",
      "epoch:4 step:3768 [D loss: 0.324312, acc.: 84.38%] [G loss: 3.945187]\n",
      "epoch:4 step:3769 [D loss: 0.082998, acc.: 96.88%] [G loss: 4.350163]\n",
      "epoch:4 step:3770 [D loss: 0.088947, acc.: 97.66%] [G loss: 3.700383]\n",
      "epoch:4 step:3771 [D loss: 0.230126, acc.: 89.84%] [G loss: 2.881052]\n",
      "epoch:4 step:3772 [D loss: 0.125508, acc.: 98.44%] [G loss: 4.123651]\n",
      "epoch:4 step:3773 [D loss: 0.155512, acc.: 95.31%] [G loss: 2.118133]\n",
      "epoch:4 step:3774 [D loss: 0.123784, acc.: 98.44%] [G loss: 1.544958]\n",
      "epoch:4 step:3775 [D loss: 0.095042, acc.: 97.66%] [G loss: 1.049067]\n",
      "epoch:4 step:3776 [D loss: 0.131602, acc.: 98.44%] [G loss: 0.459052]\n",
      "epoch:4 step:3777 [D loss: 0.029048, acc.: 99.22%] [G loss: 0.737140]\n",
      "epoch:4 step:3778 [D loss: 0.026583, acc.: 100.00%] [G loss: 0.146082]\n",
      "epoch:4 step:3779 [D loss: 0.020528, acc.: 100.00%] [G loss: 0.156480]\n",
      "epoch:4 step:3780 [D loss: 0.061901, acc.: 99.22%] [G loss: 0.056430]\n",
      "epoch:4 step:3781 [D loss: 0.033748, acc.: 100.00%] [G loss: 0.051613]\n",
      "epoch:4 step:3782 [D loss: 0.022466, acc.: 100.00%] [G loss: 0.099818]\n",
      "epoch:4 step:3783 [D loss: 0.195831, acc.: 92.97%] [G loss: 2.368581]\n",
      "epoch:4 step:3784 [D loss: 0.036393, acc.: 100.00%] [G loss: 3.445041]\n",
      "epoch:4 step:3785 [D loss: 0.744869, acc.: 67.97%] [G loss: 0.063771]\n",
      "epoch:4 step:3786 [D loss: 0.052336, acc.: 100.00%] [G loss: 0.093884]\n",
      "epoch:4 step:3787 [D loss: 0.046308, acc.: 97.66%] [G loss: 0.247991]\n",
      "epoch:4 step:3788 [D loss: 0.048370, acc.: 100.00%] [G loss: 0.415801]\n",
      "epoch:4 step:3789 [D loss: 0.068023, acc.: 97.66%] [G loss: 0.520113]\n",
      "epoch:4 step:3790 [D loss: 0.006146, acc.: 100.00%] [G loss: 0.479164]\n",
      "epoch:4 step:3791 [D loss: 0.003000, acc.: 100.00%] [G loss: 0.216515]\n",
      "epoch:4 step:3792 [D loss: 0.017099, acc.: 100.00%] [G loss: 0.255136]\n",
      "epoch:4 step:3793 [D loss: 0.453363, acc.: 74.22%] [G loss: 8.381996]\n",
      "epoch:4 step:3794 [D loss: 1.467839, acc.: 56.25%] [G loss: 2.887364]\n",
      "epoch:4 step:3795 [D loss: 0.154989, acc.: 92.19%] [G loss: 2.619183]\n",
      "epoch:4 step:3796 [D loss: 0.127858, acc.: 96.88%] [G loss: 3.667408]\n",
      "epoch:4 step:3797 [D loss: 0.084121, acc.: 98.44%] [G loss: 4.181177]\n",
      "epoch:4 step:3798 [D loss: 0.087310, acc.: 98.44%] [G loss: 3.311416]\n",
      "epoch:4 step:3799 [D loss: 0.467929, acc.: 79.69%] [G loss: 3.691435]\n",
      "epoch:4 step:3800 [D loss: 0.471743, acc.: 75.78%] [G loss: 3.130725]\n",
      "epoch:4 step:3801 [D loss: 0.097541, acc.: 97.66%] [G loss: 4.220999]\n",
      "epoch:4 step:3802 [D loss: 0.071910, acc.: 99.22%] [G loss: 3.980003]\n",
      "epoch:4 step:3803 [D loss: 0.137618, acc.: 95.31%] [G loss: 3.382433]\n",
      "epoch:4 step:3804 [D loss: 0.334018, acc.: 86.72%] [G loss: 5.023395]\n",
      "epoch:4 step:3805 [D loss: 0.350092, acc.: 85.94%] [G loss: 3.281369]\n",
      "epoch:4 step:3806 [D loss: 0.327595, acc.: 87.50%] [G loss: 4.307613]\n",
      "epoch:4 step:3807 [D loss: 0.379935, acc.: 81.25%] [G loss: 1.143602]\n",
      "epoch:4 step:3808 [D loss: 0.185025, acc.: 92.97%] [G loss: 2.273469]\n",
      "epoch:4 step:3809 [D loss: 0.061340, acc.: 99.22%] [G loss: 2.149825]\n",
      "epoch:4 step:3810 [D loss: 0.087297, acc.: 98.44%] [G loss: 0.514583]\n",
      "epoch:4 step:3811 [D loss: 0.048582, acc.: 100.00%] [G loss: 0.907803]\n",
      "epoch:4 step:3812 [D loss: 0.153731, acc.: 92.97%] [G loss: 0.288226]\n",
      "epoch:4 step:3813 [D loss: 0.049159, acc.: 99.22%] [G loss: 0.149196]\n",
      "epoch:4 step:3814 [D loss: 0.123634, acc.: 96.88%] [G loss: 0.483373]\n",
      "epoch:4 step:3815 [D loss: 0.152967, acc.: 93.75%] [G loss: 0.324388]\n",
      "epoch:4 step:3816 [D loss: 0.153240, acc.: 95.31%] [G loss: 2.083880]\n",
      "epoch:4 step:3817 [D loss: 0.217094, acc.: 89.06%] [G loss: 0.350644]\n",
      "epoch:4 step:3818 [D loss: 0.152821, acc.: 95.31%] [G loss: 1.590710]\n",
      "epoch:4 step:3819 [D loss: 0.068234, acc.: 99.22%] [G loss: 1.867269]\n",
      "epoch:4 step:3820 [D loss: 0.047984, acc.: 100.00%] [G loss: 0.998186]\n",
      "epoch:4 step:3821 [D loss: 0.102147, acc.: 97.66%] [G loss: 1.012649]\n",
      "epoch:4 step:3822 [D loss: 0.744161, acc.: 60.16%] [G loss: 3.284826]\n",
      "epoch:4 step:3823 [D loss: 0.325053, acc.: 86.72%] [G loss: 1.621704]\n",
      "epoch:4 step:3824 [D loss: 0.039087, acc.: 100.00%] [G loss: 0.854010]\n",
      "epoch:4 step:3825 [D loss: 0.231892, acc.: 87.50%] [G loss: 4.008416]\n",
      "epoch:4 step:3826 [D loss: 0.099804, acc.: 96.88%] [G loss: 4.409864]\n",
      "epoch:4 step:3827 [D loss: 0.947578, acc.: 50.78%] [G loss: 7.389160]\n",
      "epoch:4 step:3828 [D loss: 0.324987, acc.: 82.03%] [G loss: 6.176588]\n",
      "epoch:4 step:3829 [D loss: 1.333209, acc.: 36.72%] [G loss: 4.046224]\n",
      "epoch:4 step:3830 [D loss: 0.129976, acc.: 97.66%] [G loss: 4.909723]\n",
      "epoch:4 step:3831 [D loss: 0.084228, acc.: 98.44%] [G loss: 4.539967]\n",
      "epoch:4 step:3832 [D loss: 0.193476, acc.: 95.31%] [G loss: 4.700961]\n",
      "epoch:4 step:3833 [D loss: 0.215083, acc.: 92.97%] [G loss: 2.975997]\n",
      "epoch:4 step:3834 [D loss: 0.210528, acc.: 91.41%] [G loss: 4.452741]\n",
      "epoch:4 step:3835 [D loss: 0.121465, acc.: 96.09%] [G loss: 4.348732]\n",
      "epoch:4 step:3836 [D loss: 0.062196, acc.: 100.00%] [G loss: 2.276957]\n",
      "epoch:4 step:3837 [D loss: 0.154398, acc.: 96.09%] [G loss: 1.256201]\n",
      "epoch:4 step:3838 [D loss: 0.052032, acc.: 98.44%] [G loss: 1.064994]\n",
      "epoch:4 step:3839 [D loss: 0.108751, acc.: 97.66%] [G loss: 0.355425]\n",
      "epoch:4 step:3840 [D loss: 0.085130, acc.: 97.66%] [G loss: 0.261830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4 step:3841 [D loss: 0.083048, acc.: 97.66%] [G loss: 0.415030]\n",
      "epoch:4 step:3842 [D loss: 0.048453, acc.: 97.66%] [G loss: 0.274834]\n",
      "epoch:4 step:3843 [D loss: 0.044582, acc.: 100.00%] [G loss: 0.605393]\n",
      "epoch:4 step:3844 [D loss: 0.017191, acc.: 100.00%] [G loss: 0.251352]\n",
      "epoch:4 step:3845 [D loss: 0.016484, acc.: 100.00%] [G loss: 0.166446]\n",
      "epoch:4 step:3846 [D loss: 0.096611, acc.: 97.66%] [G loss: 1.040312]\n",
      "epoch:4 step:3847 [D loss: 0.023764, acc.: 100.00%] [G loss: 2.204877]\n",
      "epoch:4 step:3848 [D loss: 0.627060, acc.: 71.09%] [G loss: 6.457736]\n",
      "epoch:4 step:3849 [D loss: 0.847371, acc.: 61.72%] [G loss: 1.063881]\n",
      "epoch:4 step:3850 [D loss: 0.288322, acc.: 85.94%] [G loss: 2.474870]\n",
      "epoch:4 step:3851 [D loss: 0.018357, acc.: 100.00%] [G loss: 3.601871]\n",
      "epoch:4 step:3852 [D loss: 0.072919, acc.: 97.66%] [G loss: 1.653330]\n",
      "epoch:4 step:3853 [D loss: 0.031537, acc.: 100.00%] [G loss: 0.814287]\n",
      "epoch:4 step:3854 [D loss: 1.203938, acc.: 60.16%] [G loss: 6.651159]\n",
      "epoch:4 step:3855 [D loss: 0.839723, acc.: 60.16%] [G loss: 2.485909]\n",
      "epoch:4 step:3856 [D loss: 0.096349, acc.: 98.44%] [G loss: 2.529571]\n",
      "epoch:4 step:3857 [D loss: 0.343192, acc.: 87.50%] [G loss: 4.150295]\n",
      "epoch:4 step:3858 [D loss: 0.173794, acc.: 95.31%] [G loss: 4.357973]\n",
      "epoch:4 step:3859 [D loss: 0.334740, acc.: 86.72%] [G loss: 5.036071]\n",
      "epoch:4 step:3860 [D loss: 0.189047, acc.: 92.97%] [G loss: 3.661107]\n",
      "epoch:4 step:3861 [D loss: 0.291636, acc.: 85.94%] [G loss: 2.323622]\n",
      "epoch:4 step:3862 [D loss: 0.095196, acc.: 98.44%] [G loss: 2.538763]\n",
      "epoch:4 step:3863 [D loss: 0.102004, acc.: 98.44%] [G loss: 1.654613]\n",
      "epoch:4 step:3864 [D loss: 0.090764, acc.: 100.00%] [G loss: 0.346262]\n",
      "epoch:4 step:3865 [D loss: 0.265767, acc.: 86.72%] [G loss: 5.172186]\n",
      "epoch:4 step:3866 [D loss: 0.493667, acc.: 75.78%] [G loss: 0.519715]\n",
      "epoch:4 step:3867 [D loss: 0.066753, acc.: 97.66%] [G loss: 1.181578]\n",
      "epoch:4 step:3868 [D loss: 0.008706, acc.: 100.00%] [G loss: 0.141484]\n",
      "epoch:4 step:3869 [D loss: 0.179003, acc.: 89.84%] [G loss: 2.700861]\n",
      "epoch:4 step:3870 [D loss: 0.046884, acc.: 100.00%] [G loss: 0.800394]\n",
      "epoch:4 step:3871 [D loss: 2.046775, acc.: 26.56%] [G loss: 5.951710]\n",
      "epoch:4 step:3872 [D loss: 0.496307, acc.: 72.66%] [G loss: 5.604752]\n",
      "epoch:4 step:3873 [D loss: 0.550639, acc.: 70.31%] [G loss: 1.835137]\n",
      "epoch:4 step:3874 [D loss: 0.548549, acc.: 76.56%] [G loss: 3.260590]\n",
      "epoch:4 step:3875 [D loss: 0.212124, acc.: 92.97%] [G loss: 3.551896]\n",
      "epoch:4 step:3876 [D loss: 0.126969, acc.: 97.66%] [G loss: 3.170792]\n",
      "epoch:4 step:3877 [D loss: 0.110159, acc.: 98.44%] [G loss: 1.803512]\n",
      "epoch:4 step:3878 [D loss: 0.202802, acc.: 95.31%] [G loss: 1.032474]\n",
      "epoch:4 step:3879 [D loss: 0.204025, acc.: 94.53%] [G loss: 2.378496]\n",
      "epoch:4 step:3880 [D loss: 0.652051, acc.: 63.28%] [G loss: 2.867003]\n",
      "epoch:4 step:3881 [D loss: 0.333332, acc.: 83.59%] [G loss: 3.032406]\n",
      "epoch:4 step:3882 [D loss: 0.351639, acc.: 83.59%] [G loss: 3.627909]\n",
      "epoch:4 step:3883 [D loss: 0.291119, acc.: 89.06%] [G loss: 2.507824]\n",
      "epoch:4 step:3884 [D loss: 0.091643, acc.: 99.22%] [G loss: 1.920686]\n",
      "epoch:4 step:3885 [D loss: 0.141973, acc.: 96.88%] [G loss: 1.315381]\n",
      "epoch:4 step:3886 [D loss: 0.083516, acc.: 99.22%] [G loss: 0.980753]\n",
      "epoch:4 step:3887 [D loss: 0.075291, acc.: 100.00%] [G loss: 0.267228]\n",
      "epoch:4 step:3888 [D loss: 0.070168, acc.: 100.00%] [G loss: 0.144951]\n",
      "epoch:4 step:3889 [D loss: 0.150856, acc.: 96.88%] [G loss: 0.446086]\n",
      "epoch:4 step:3890 [D loss: 0.094885, acc.: 97.66%] [G loss: 0.630306]\n",
      "epoch:4 step:3891 [D loss: 0.014435, acc.: 100.00%] [G loss: 0.459618]\n",
      "epoch:4 step:3892 [D loss: 0.135529, acc.: 95.31%] [G loss: 0.369034]\n",
      "epoch:4 step:3893 [D loss: 0.029913, acc.: 100.00%] [G loss: 0.600471]\n",
      "epoch:4 step:3894 [D loss: 0.124541, acc.: 98.44%] [G loss: 0.298507]\n",
      "epoch:4 step:3895 [D loss: 0.971013, acc.: 50.00%] [G loss: 2.149457]\n",
      "epoch:4 step:3896 [D loss: 0.102920, acc.: 94.53%] [G loss: 2.953430]\n",
      "epoch:4 step:3897 [D loss: 0.448175, acc.: 80.47%] [G loss: 0.373636]\n",
      "epoch:4 step:3898 [D loss: 0.614154, acc.: 69.53%] [G loss: 2.336416]\n",
      "epoch:4 step:3899 [D loss: 0.073008, acc.: 96.88%] [G loss: 3.037510]\n",
      "epoch:4 step:3900 [D loss: 0.259805, acc.: 87.50%] [G loss: 0.871770]\n",
      "epoch:4 step:3901 [D loss: 0.649563, acc.: 68.75%] [G loss: 3.233584]\n",
      "epoch:4 step:3902 [D loss: 0.415918, acc.: 77.34%] [G loss: 1.851941]\n",
      "epoch:4 step:3903 [D loss: 0.129813, acc.: 95.31%] [G loss: 1.395696]\n",
      "epoch:4 step:3904 [D loss: 0.139388, acc.: 95.31%] [G loss: 1.348256]\n",
      "epoch:4 step:3905 [D loss: 0.167796, acc.: 96.09%] [G loss: 1.785459]\n",
      "epoch:5 step:3906 [D loss: 0.974821, acc.: 46.09%] [G loss: 2.277318]\n",
      "epoch:5 step:3907 [D loss: 0.050715, acc.: 100.00%] [G loss: 4.094972]\n",
      "epoch:5 step:3908 [D loss: 0.396723, acc.: 81.25%] [G loss: 2.323140]\n",
      "epoch:5 step:3909 [D loss: 0.294460, acc.: 86.72%] [G loss: 3.530847]\n",
      "epoch:5 step:3910 [D loss: 0.062447, acc.: 100.00%] [G loss: 3.535389]\n",
      "epoch:5 step:3911 [D loss: 0.201059, acc.: 93.75%] [G loss: 2.411881]\n",
      "epoch:5 step:3912 [D loss: 0.131718, acc.: 96.09%] [G loss: 1.441084]\n",
      "epoch:5 step:3913 [D loss: 0.097672, acc.: 98.44%] [G loss: 2.282045]\n",
      "epoch:5 step:3914 [D loss: 0.077709, acc.: 99.22%] [G loss: 0.645172]\n",
      "epoch:5 step:3915 [D loss: 0.051199, acc.: 99.22%] [G loss: 0.447823]\n",
      "epoch:5 step:3916 [D loss: 0.113121, acc.: 96.88%] [G loss: 0.992170]\n",
      "epoch:5 step:3917 [D loss: 0.114984, acc.: 96.88%] [G loss: 0.277380]\n",
      "epoch:5 step:3918 [D loss: 0.040133, acc.: 100.00%] [G loss: 0.071141]\n",
      "epoch:5 step:3919 [D loss: 0.068961, acc.: 100.00%] [G loss: 0.064626]\n",
      "epoch:5 step:3920 [D loss: 0.083169, acc.: 97.66%] [G loss: 0.339703]\n",
      "epoch:5 step:3921 [D loss: 0.031270, acc.: 100.00%] [G loss: 0.141317]\n",
      "epoch:5 step:3922 [D loss: 0.045715, acc.: 100.00%] [G loss: 0.044187]\n",
      "epoch:5 step:3923 [D loss: 0.034095, acc.: 100.00%] [G loss: 0.090823]\n",
      "epoch:5 step:3924 [D loss: 0.011527, acc.: 100.00%] [G loss: 0.070033]\n",
      "epoch:5 step:3925 [D loss: 0.032467, acc.: 100.00%] [G loss: 0.121608]\n",
      "epoch:5 step:3926 [D loss: 0.213217, acc.: 95.31%] [G loss: 4.510451]\n",
      "epoch:5 step:3927 [D loss: 0.422705, acc.: 77.34%] [G loss: 1.221709]\n",
      "epoch:5 step:3928 [D loss: 0.018649, acc.: 100.00%] [G loss: 0.279889]\n",
      "epoch:5 step:3929 [D loss: 0.394422, acc.: 81.25%] [G loss: 3.527987]\n",
      "epoch:5 step:3930 [D loss: 0.350336, acc.: 83.59%] [G loss: 2.983389]\n",
      "epoch:5 step:3931 [D loss: 0.569625, acc.: 70.31%] [G loss: 0.475762]\n",
      "epoch:5 step:3932 [D loss: 0.441035, acc.: 75.78%] [G loss: 3.686492]\n",
      "epoch:5 step:3933 [D loss: 0.066524, acc.: 97.66%] [G loss: 4.408367]\n",
      "epoch:5 step:3934 [D loss: 0.503876, acc.: 76.56%] [G loss: 3.293923]\n",
      "epoch:5 step:3935 [D loss: 0.303533, acc.: 89.06%] [G loss: 3.989455]\n",
      "epoch:5 step:3936 [D loss: 0.262335, acc.: 92.97%] [G loss: 3.680684]\n",
      "epoch:5 step:3937 [D loss: 0.091242, acc.: 98.44%] [G loss: 4.280888]\n",
      "epoch:5 step:3938 [D loss: 0.195773, acc.: 95.31%] [G loss: 4.094199]\n",
      "epoch:5 step:3939 [D loss: 0.104955, acc.: 98.44%] [G loss: 3.718085]\n",
      "epoch:5 step:3940 [D loss: 0.235826, acc.: 93.75%] [G loss: 4.514758]\n",
      "epoch:5 step:3941 [D loss: 0.105812, acc.: 96.09%] [G loss: 3.134296]\n",
      "epoch:5 step:3942 [D loss: 0.255327, acc.: 91.41%] [G loss: 1.681116]\n",
      "epoch:5 step:3943 [D loss: 0.127830, acc.: 96.88%] [G loss: 1.512506]\n",
      "epoch:5 step:3944 [D loss: 0.047385, acc.: 99.22%] [G loss: 0.752003]\n",
      "epoch:5 step:3945 [D loss: 0.021142, acc.: 100.00%] [G loss: 0.416936]\n",
      "epoch:5 step:3946 [D loss: 0.135866, acc.: 97.66%] [G loss: 0.716482]\n",
      "epoch:5 step:3947 [D loss: 0.064560, acc.: 100.00%] [G loss: 0.969529]\n",
      "epoch:5 step:3948 [D loss: 0.086817, acc.: 97.66%] [G loss: 0.143654]\n",
      "epoch:5 step:3949 [D loss: 0.583848, acc.: 69.53%] [G loss: 4.791028]\n",
      "epoch:5 step:3950 [D loss: 1.037520, acc.: 56.25%] [G loss: 1.218968]\n",
      "epoch:5 step:3951 [D loss: 0.093394, acc.: 96.88%] [G loss: 1.057669]\n",
      "epoch:5 step:3952 [D loss: 0.098106, acc.: 97.66%] [G loss: 1.228042]\n",
      "epoch:5 step:3953 [D loss: 0.026462, acc.: 100.00%] [G loss: 2.110766]\n",
      "epoch:5 step:3954 [D loss: 0.021978, acc.: 100.00%] [G loss: 1.080999]\n",
      "epoch:5 step:3955 [D loss: 0.103572, acc.: 99.22%] [G loss: 0.536348]\n",
      "epoch:5 step:3956 [D loss: 1.065477, acc.: 57.03%] [G loss: 7.237353]\n",
      "epoch:5 step:3957 [D loss: 1.176753, acc.: 54.69%] [G loss: 4.048823]\n",
      "epoch:5 step:3958 [D loss: 0.414282, acc.: 81.25%] [G loss: 2.583934]\n",
      "epoch:5 step:3959 [D loss: 0.238710, acc.: 91.41%] [G loss: 3.828606]\n",
      "epoch:5 step:3960 [D loss: 0.112105, acc.: 96.09%] [G loss: 3.995471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:3961 [D loss: 0.190321, acc.: 92.19%] [G loss: 3.100170]\n",
      "epoch:5 step:3962 [D loss: 0.139803, acc.: 97.66%] [G loss: 2.580447]\n",
      "epoch:5 step:3963 [D loss: 0.161398, acc.: 96.88%] [G loss: 3.621938]\n",
      "epoch:5 step:3964 [D loss: 0.186585, acc.: 93.75%] [G loss: 3.081395]\n",
      "epoch:5 step:3965 [D loss: 0.094218, acc.: 99.22%] [G loss: 2.447757]\n",
      "epoch:5 step:3966 [D loss: 0.176810, acc.: 97.66%] [G loss: 1.816478]\n",
      "epoch:5 step:3967 [D loss: 0.166750, acc.: 95.31%] [G loss: 1.638004]\n",
      "epoch:5 step:3968 [D loss: 0.229377, acc.: 90.62%] [G loss: 2.627387]\n",
      "epoch:5 step:3969 [D loss: 0.187175, acc.: 94.53%] [G loss: 2.636993]\n",
      "epoch:5 step:3970 [D loss: 0.202267, acc.: 89.84%] [G loss: 2.193934]\n",
      "epoch:5 step:3971 [D loss: 0.061277, acc.: 100.00%] [G loss: 0.631642]\n",
      "epoch:5 step:3972 [D loss: 2.810635, acc.: 21.09%] [G loss: 4.713816]\n",
      "epoch:5 step:3973 [D loss: 0.820459, acc.: 58.59%] [G loss: 3.818069]\n",
      "epoch:5 step:3974 [D loss: 0.423815, acc.: 78.91%] [G loss: 1.994619]\n",
      "epoch:5 step:3975 [D loss: 0.324052, acc.: 87.50%] [G loss: 2.360777]\n",
      "epoch:5 step:3976 [D loss: 0.159795, acc.: 94.53%] [G loss: 2.862058]\n",
      "epoch:5 step:3977 [D loss: 0.421718, acc.: 79.69%] [G loss: 1.579934]\n",
      "epoch:5 step:3978 [D loss: 0.430424, acc.: 78.12%] [G loss: 2.771770]\n",
      "epoch:5 step:3979 [D loss: 0.199137, acc.: 92.97%] [G loss: 3.060269]\n",
      "epoch:5 step:3980 [D loss: 0.356098, acc.: 82.03%] [G loss: 2.154156]\n",
      "epoch:5 step:3981 [D loss: 0.355706, acc.: 87.50%] [G loss: 2.591412]\n",
      "epoch:5 step:3982 [D loss: 0.286751, acc.: 87.50%] [G loss: 1.616973]\n",
      "epoch:5 step:3983 [D loss: 0.250013, acc.: 92.19%] [G loss: 2.494392]\n",
      "epoch:5 step:3984 [D loss: 0.207012, acc.: 96.09%] [G loss: 3.341792]\n",
      "epoch:5 step:3985 [D loss: 0.229724, acc.: 89.84%] [G loss: 2.936960]\n",
      "epoch:5 step:3986 [D loss: 0.178956, acc.: 96.09%] [G loss: 2.311794]\n",
      "epoch:5 step:3987 [D loss: 0.357380, acc.: 83.59%] [G loss: 3.621485]\n",
      "epoch:5 step:3988 [D loss: 0.159973, acc.: 93.75%] [G loss: 3.882772]\n",
      "epoch:5 step:3989 [D loss: 0.099480, acc.: 96.88%] [G loss: 1.681327]\n",
      "epoch:5 step:3990 [D loss: 0.274742, acc.: 86.72%] [G loss: 3.214581]\n",
      "epoch:5 step:3991 [D loss: 0.179568, acc.: 91.41%] [G loss: 2.462594]\n",
      "epoch:5 step:3992 [D loss: 0.058919, acc.: 100.00%] [G loss: 0.925528]\n",
      "epoch:5 step:3993 [D loss: 0.043236, acc.: 100.00%] [G loss: 0.487078]\n",
      "epoch:5 step:3994 [D loss: 0.087233, acc.: 99.22%] [G loss: 0.749331]\n",
      "epoch:5 step:3995 [D loss: 0.772082, acc.: 64.06%] [G loss: 1.818641]\n",
      "epoch:5 step:3996 [D loss: 0.030489, acc.: 100.00%] [G loss: 3.253965]\n",
      "epoch:5 step:3997 [D loss: 0.145933, acc.: 96.09%] [G loss: 1.315847]\n",
      "epoch:5 step:3998 [D loss: 0.062424, acc.: 99.22%] [G loss: 0.556486]\n",
      "epoch:5 step:3999 [D loss: 0.015801, acc.: 100.00%] [G loss: 0.130007]\n",
      "epoch:5 step:4000 [D loss: 0.017730, acc.: 100.00%] [G loss: 0.071418]\n",
      "epoch:5 step:4001 [D loss: 0.224045, acc.: 89.84%] [G loss: 0.803591]\n",
      "epoch:5 step:4002 [D loss: 0.073007, acc.: 99.22%] [G loss: 0.976435]\n",
      "epoch:5 step:4003 [D loss: 0.611761, acc.: 70.31%] [G loss: 2.239185]\n",
      "epoch:5 step:4004 [D loss: 0.153502, acc.: 92.97%] [G loss: 1.816234]\n",
      "epoch:5 step:4005 [D loss: 0.075243, acc.: 98.44%] [G loss: 1.392678]\n",
      "epoch:5 step:4006 [D loss: 0.207651, acc.: 93.75%] [G loss: 2.071492]\n",
      "epoch:5 step:4007 [D loss: 0.028976, acc.: 100.00%] [G loss: 2.020632]\n",
      "epoch:5 step:4008 [D loss: 0.685783, acc.: 69.53%] [G loss: 5.065324]\n",
      "epoch:5 step:4009 [D loss: 0.161418, acc.: 92.97%] [G loss: 5.198596]\n",
      "epoch:5 step:4010 [D loss: 0.481038, acc.: 78.12%] [G loss: 2.929436]\n",
      "epoch:5 step:4011 [D loss: 0.283330, acc.: 85.94%] [G loss: 4.726023]\n",
      "epoch:5 step:4012 [D loss: 0.297410, acc.: 88.28%] [G loss: 4.514621]\n",
      "epoch:5 step:4013 [D loss: 0.167491, acc.: 94.53%] [G loss: 4.056428]\n",
      "epoch:5 step:4014 [D loss: 0.060368, acc.: 100.00%] [G loss: 3.264673]\n",
      "epoch:5 step:4015 [D loss: 0.128954, acc.: 98.44%] [G loss: 3.576408]\n",
      "epoch:5 step:4016 [D loss: 0.059594, acc.: 100.00%] [G loss: 1.269090]\n",
      "epoch:5 step:4017 [D loss: 0.286672, acc.: 91.41%] [G loss: 2.983047]\n",
      "epoch:5 step:4018 [D loss: 0.310947, acc.: 89.06%] [G loss: 2.234969]\n",
      "epoch:5 step:4019 [D loss: 0.171527, acc.: 93.75%] [G loss: 2.150583]\n",
      "epoch:5 step:4020 [D loss: 0.031240, acc.: 100.00%] [G loss: 1.948543]\n",
      "epoch:5 step:4021 [D loss: 0.177190, acc.: 91.41%] [G loss: 0.243506]\n",
      "epoch:5 step:4022 [D loss: 0.183277, acc.: 96.88%] [G loss: 0.548246]\n",
      "epoch:5 step:4023 [D loss: 0.090083, acc.: 97.66%] [G loss: 0.143910]\n",
      "epoch:5 step:4024 [D loss: 1.122421, acc.: 54.69%] [G loss: 8.136614]\n",
      "epoch:5 step:4025 [D loss: 1.710085, acc.: 50.00%] [G loss: 3.018440]\n",
      "epoch:5 step:4026 [D loss: 0.223913, acc.: 92.19%] [G loss: 3.069142]\n",
      "epoch:5 step:4027 [D loss: 0.067253, acc.: 98.44%] [G loss: 3.018163]\n",
      "epoch:5 step:4028 [D loss: 0.102765, acc.: 97.66%] [G loss: 2.370688]\n",
      "epoch:5 step:4029 [D loss: 0.222560, acc.: 92.19%] [G loss: 2.958495]\n",
      "epoch:5 step:4030 [D loss: 0.132290, acc.: 97.66%] [G loss: 2.487670]\n",
      "epoch:5 step:4031 [D loss: 0.824510, acc.: 53.91%] [G loss: 2.772454]\n",
      "epoch:5 step:4032 [D loss: 0.186246, acc.: 92.97%] [G loss: 2.102971]\n",
      "epoch:5 step:4033 [D loss: 0.343874, acc.: 85.16%] [G loss: 3.277384]\n",
      "epoch:5 step:4034 [D loss: 0.526024, acc.: 74.22%] [G loss: 2.924445]\n",
      "epoch:5 step:4035 [D loss: 0.594416, acc.: 75.00%] [G loss: 4.058345]\n",
      "epoch:5 step:4036 [D loss: 0.111965, acc.: 98.44%] [G loss: 3.704899]\n",
      "epoch:5 step:4037 [D loss: 0.289670, acc.: 88.28%] [G loss: 1.638666]\n",
      "epoch:5 step:4038 [D loss: 0.295356, acc.: 87.50%] [G loss: 2.400830]\n",
      "epoch:5 step:4039 [D loss: 0.108401, acc.: 97.66%] [G loss: 2.659700]\n",
      "epoch:5 step:4040 [D loss: 0.413608, acc.: 79.69%] [G loss: 0.566768]\n",
      "epoch:5 step:4041 [D loss: 0.727359, acc.: 62.50%] [G loss: 4.615996]\n",
      "epoch:5 step:4042 [D loss: 0.234892, acc.: 91.41%] [G loss: 4.648168]\n",
      "epoch:5 step:4043 [D loss: 0.237195, acc.: 91.41%] [G loss: 2.448998]\n",
      "epoch:5 step:4044 [D loss: 0.366001, acc.: 83.59%] [G loss: 3.476074]\n",
      "epoch:5 step:4045 [D loss: 0.194672, acc.: 93.75%] [G loss: 3.072880]\n",
      "epoch:5 step:4046 [D loss: 0.203426, acc.: 93.75%] [G loss: 2.757654]\n",
      "epoch:5 step:4047 [D loss: 0.148699, acc.: 97.66%] [G loss: 2.866551]\n",
      "epoch:5 step:4048 [D loss: 0.246329, acc.: 89.84%] [G loss: 3.015524]\n",
      "epoch:5 step:4049 [D loss: 0.218643, acc.: 94.53%] [G loss: 3.705704]\n",
      "epoch:5 step:4050 [D loss: 0.179302, acc.: 91.41%] [G loss: 3.445951]\n",
      "epoch:5 step:4051 [D loss: 0.220662, acc.: 91.41%] [G loss: 3.847259]\n",
      "epoch:5 step:4052 [D loss: 0.154321, acc.: 96.09%] [G loss: 2.397519]\n",
      "epoch:5 step:4053 [D loss: 0.284264, acc.: 87.50%] [G loss: 3.720319]\n",
      "epoch:5 step:4054 [D loss: 0.560794, acc.: 72.66%] [G loss: 4.968039]\n",
      "epoch:5 step:4055 [D loss: 0.244135, acc.: 90.62%] [G loss: 4.993243]\n",
      "epoch:5 step:4056 [D loss: 0.406212, acc.: 80.47%] [G loss: 1.754803]\n",
      "epoch:5 step:4057 [D loss: 0.225107, acc.: 89.84%] [G loss: 4.108619]\n",
      "epoch:5 step:4058 [D loss: 0.258702, acc.: 85.94%] [G loss: 2.368134]\n",
      "epoch:5 step:4059 [D loss: 0.132093, acc.: 96.88%] [G loss: 2.077451]\n",
      "epoch:5 step:4060 [D loss: 0.052707, acc.: 100.00%] [G loss: 1.915542]\n",
      "epoch:5 step:4061 [D loss: 0.105008, acc.: 98.44%] [G loss: 1.036395]\n",
      "epoch:5 step:4062 [D loss: 0.108501, acc.: 98.44%] [G loss: 1.366418]\n",
      "epoch:5 step:4063 [D loss: 0.464133, acc.: 76.56%] [G loss: 0.250581]\n",
      "epoch:5 step:4064 [D loss: 0.035713, acc.: 100.00%] [G loss: 0.650144]\n",
      "epoch:5 step:4065 [D loss: 0.170269, acc.: 93.75%] [G loss: 0.149781]\n",
      "epoch:5 step:4066 [D loss: 1.079496, acc.: 53.91%] [G loss: 7.231945]\n",
      "epoch:5 step:4067 [D loss: 0.915969, acc.: 59.38%] [G loss: 4.516718]\n",
      "epoch:5 step:4068 [D loss: 0.154562, acc.: 93.75%] [G loss: 2.692320]\n",
      "epoch:5 step:4069 [D loss: 0.150574, acc.: 95.31%] [G loss: 1.820040]\n",
      "epoch:5 step:4070 [D loss: 0.294696, acc.: 82.81%] [G loss: 4.057343]\n",
      "epoch:5 step:4071 [D loss: 0.217089, acc.: 91.41%] [G loss: 3.573194]\n",
      "epoch:5 step:4072 [D loss: 0.063949, acc.: 100.00%] [G loss: 3.031300]\n",
      "epoch:5 step:4073 [D loss: 0.060322, acc.: 99.22%] [G loss: 1.942348]\n",
      "epoch:5 step:4074 [D loss: 0.069748, acc.: 99.22%] [G loss: 1.541194]\n",
      "epoch:5 step:4075 [D loss: 0.035005, acc.: 100.00%] [G loss: 1.120928]\n",
      "epoch:5 step:4076 [D loss: 0.089295, acc.: 98.44%] [G loss: 0.411925]\n",
      "epoch:5 step:4077 [D loss: 0.959691, acc.: 58.59%] [G loss: 4.997398]\n",
      "epoch:5 step:4078 [D loss: 0.973560, acc.: 55.47%] [G loss: 1.834876]\n",
      "epoch:5 step:4079 [D loss: 0.871642, acc.: 53.12%] [G loss: 2.699104]\n",
      "epoch:5 step:4080 [D loss: 0.185409, acc.: 91.41%] [G loss: 3.021418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4081 [D loss: 0.408308, acc.: 77.34%] [G loss: 2.104044]\n",
      "epoch:5 step:4082 [D loss: 0.192149, acc.: 94.53%] [G loss: 2.917552]\n",
      "epoch:5 step:4083 [D loss: 0.154619, acc.: 96.88%] [G loss: 2.917562]\n",
      "epoch:5 step:4084 [D loss: 0.481658, acc.: 75.78%] [G loss: 2.624467]\n",
      "epoch:5 step:4085 [D loss: 0.173562, acc.: 96.88%] [G loss: 3.049517]\n",
      "epoch:5 step:4086 [D loss: 0.154806, acc.: 94.53%] [G loss: 3.411119]\n",
      "epoch:5 step:4087 [D loss: 0.121656, acc.: 98.44%] [G loss: 3.046288]\n",
      "epoch:5 step:4088 [D loss: 0.333876, acc.: 87.50%] [G loss: 3.286523]\n",
      "epoch:5 step:4089 [D loss: 0.182806, acc.: 94.53%] [G loss: 3.356131]\n",
      "epoch:5 step:4090 [D loss: 0.430361, acc.: 82.03%] [G loss: 1.661965]\n",
      "epoch:5 step:4091 [D loss: 0.138790, acc.: 96.88%] [G loss: 2.206839]\n",
      "epoch:5 step:4092 [D loss: 0.177382, acc.: 96.88%] [G loss: 2.373579]\n",
      "epoch:5 step:4093 [D loss: 0.417604, acc.: 78.91%] [G loss: 3.114465]\n",
      "epoch:5 step:4094 [D loss: 0.140434, acc.: 96.09%] [G loss: 2.552345]\n",
      "epoch:5 step:4095 [D loss: 0.192883, acc.: 96.09%] [G loss: 2.080916]\n",
      "epoch:5 step:4096 [D loss: 0.184970, acc.: 96.09%] [G loss: 0.942058]\n",
      "epoch:5 step:4097 [D loss: 0.274782, acc.: 89.84%] [G loss: 1.262573]\n",
      "epoch:5 step:4098 [D loss: 0.132011, acc.: 97.66%] [G loss: 2.437783]\n",
      "epoch:5 step:4099 [D loss: 0.404096, acc.: 82.81%] [G loss: 2.053040]\n",
      "epoch:5 step:4100 [D loss: 0.416002, acc.: 82.03%] [G loss: 3.266099]\n",
      "epoch:5 step:4101 [D loss: 0.675878, acc.: 62.50%] [G loss: 4.534960]\n",
      "epoch:5 step:4102 [D loss: 0.036301, acc.: 100.00%] [G loss: 3.728208]\n",
      "epoch:5 step:4103 [D loss: 0.341078, acc.: 85.16%] [G loss: 3.286465]\n",
      "epoch:5 step:4104 [D loss: 0.103165, acc.: 97.66%] [G loss: 3.071745]\n",
      "epoch:5 step:4105 [D loss: 0.132337, acc.: 94.53%] [G loss: 3.228465]\n",
      "epoch:5 step:4106 [D loss: 0.179274, acc.: 95.31%] [G loss: 2.791891]\n",
      "epoch:5 step:4107 [D loss: 0.466132, acc.: 75.78%] [G loss: 3.403651]\n",
      "epoch:5 step:4108 [D loss: 0.139147, acc.: 95.31%] [G loss: 4.125137]\n",
      "epoch:5 step:4109 [D loss: 0.282365, acc.: 87.50%] [G loss: 3.772964]\n",
      "epoch:5 step:4110 [D loss: 0.356155, acc.: 85.94%] [G loss: 3.081105]\n",
      "epoch:5 step:4111 [D loss: 0.166262, acc.: 95.31%] [G loss: 3.363276]\n",
      "epoch:5 step:4112 [D loss: 0.173025, acc.: 96.09%] [G loss: 3.947004]\n",
      "epoch:5 step:4113 [D loss: 0.347490, acc.: 85.16%] [G loss: 3.756472]\n",
      "epoch:5 step:4114 [D loss: 0.291614, acc.: 87.50%] [G loss: 1.711659]\n",
      "epoch:5 step:4115 [D loss: 0.433586, acc.: 73.44%] [G loss: 5.438777]\n",
      "epoch:5 step:4116 [D loss: 0.434702, acc.: 76.56%] [G loss: 2.970794]\n",
      "epoch:5 step:4117 [D loss: 0.325501, acc.: 85.16%] [G loss: 3.514289]\n",
      "epoch:5 step:4118 [D loss: 0.270774, acc.: 91.41%] [G loss: 2.763791]\n",
      "epoch:5 step:4119 [D loss: 0.330029, acc.: 84.38%] [G loss: 2.082215]\n",
      "epoch:5 step:4120 [D loss: 0.098985, acc.: 97.66%] [G loss: 3.111603]\n",
      "epoch:5 step:4121 [D loss: 0.213770, acc.: 91.41%] [G loss: 3.297513]\n",
      "epoch:5 step:4122 [D loss: 0.744557, acc.: 60.16%] [G loss: 5.436439]\n",
      "epoch:5 step:4123 [D loss: 0.738226, acc.: 63.28%] [G loss: 3.021289]\n",
      "epoch:5 step:4124 [D loss: 0.162712, acc.: 96.09%] [G loss: 3.491261]\n",
      "epoch:5 step:4125 [D loss: 0.201233, acc.: 90.62%] [G loss: 3.576695]\n",
      "epoch:5 step:4126 [D loss: 0.327838, acc.: 89.06%] [G loss: 4.578813]\n",
      "epoch:5 step:4127 [D loss: 0.227413, acc.: 94.53%] [G loss: 4.063686]\n",
      "epoch:5 step:4128 [D loss: 0.476953, acc.: 73.44%] [G loss: 4.307226]\n",
      "epoch:5 step:4129 [D loss: 0.242696, acc.: 89.84%] [G loss: 2.501303]\n",
      "epoch:5 step:4130 [D loss: 0.307243, acc.: 84.38%] [G loss: 3.982732]\n",
      "epoch:5 step:4131 [D loss: 0.472886, acc.: 75.78%] [G loss: 2.755102]\n",
      "epoch:5 step:4132 [D loss: 0.046564, acc.: 100.00%] [G loss: 2.191446]\n",
      "epoch:5 step:4133 [D loss: 0.141643, acc.: 96.09%] [G loss: 2.276563]\n",
      "epoch:5 step:4134 [D loss: 0.213414, acc.: 92.97%] [G loss: 1.462497]\n",
      "epoch:5 step:4135 [D loss: 0.104121, acc.: 96.88%] [G loss: 0.629006]\n",
      "epoch:5 step:4136 [D loss: 0.354213, acc.: 81.25%] [G loss: 3.709053]\n",
      "epoch:5 step:4137 [D loss: 0.205774, acc.: 91.41%] [G loss: 4.046965]\n",
      "epoch:5 step:4138 [D loss: 0.549974, acc.: 71.88%] [G loss: 0.065236]\n",
      "epoch:5 step:4139 [D loss: 0.757967, acc.: 63.28%] [G loss: 2.801582]\n",
      "epoch:5 step:4140 [D loss: 0.068257, acc.: 98.44%] [G loss: 5.584272]\n",
      "epoch:5 step:4141 [D loss: 1.976507, acc.: 40.62%] [G loss: 1.210780]\n",
      "epoch:5 step:4142 [D loss: 0.429689, acc.: 77.34%] [G loss: 2.430362]\n",
      "epoch:5 step:4143 [D loss: 0.076628, acc.: 100.00%] [G loss: 2.981013]\n",
      "epoch:5 step:4144 [D loss: 0.179397, acc.: 93.75%] [G loss: 1.342407]\n",
      "epoch:5 step:4145 [D loss: 0.867754, acc.: 58.59%] [G loss: 2.348427]\n",
      "epoch:5 step:4146 [D loss: 0.185732, acc.: 92.97%] [G loss: 3.142769]\n",
      "epoch:5 step:4147 [D loss: 0.198334, acc.: 92.97%] [G loss: 2.317819]\n",
      "epoch:5 step:4148 [D loss: 0.205739, acc.: 92.19%] [G loss: 2.597419]\n",
      "epoch:5 step:4149 [D loss: 0.242217, acc.: 92.97%] [G loss: 3.094705]\n",
      "epoch:5 step:4150 [D loss: 0.380054, acc.: 82.03%] [G loss: 3.693521]\n",
      "epoch:5 step:4151 [D loss: 0.496768, acc.: 78.91%] [G loss: 2.585297]\n",
      "epoch:5 step:4152 [D loss: 0.544784, acc.: 75.00%] [G loss: 3.964531]\n",
      "epoch:5 step:4153 [D loss: 0.223424, acc.: 88.28%] [G loss: 3.537332]\n",
      "epoch:5 step:4154 [D loss: 0.229224, acc.: 93.75%] [G loss: 3.351623]\n",
      "epoch:5 step:4155 [D loss: 0.231624, acc.: 92.19%] [G loss: 3.215720]\n",
      "epoch:5 step:4156 [D loss: 0.194355, acc.: 93.75%] [G loss: 3.300298]\n",
      "epoch:5 step:4157 [D loss: 0.106750, acc.: 97.66%] [G loss: 2.794363]\n",
      "epoch:5 step:4158 [D loss: 0.197231, acc.: 94.53%] [G loss: 2.237797]\n",
      "epoch:5 step:4159 [D loss: 0.242442, acc.: 91.41%] [G loss: 1.684107]\n",
      "epoch:5 step:4160 [D loss: 0.286649, acc.: 86.72%] [G loss: 1.719387]\n",
      "epoch:5 step:4161 [D loss: 0.078437, acc.: 98.44%] [G loss: 1.509819]\n",
      "epoch:5 step:4162 [D loss: 0.193936, acc.: 94.53%] [G loss: 0.993883]\n",
      "epoch:5 step:4163 [D loss: 0.113212, acc.: 96.88%] [G loss: 1.478961]\n",
      "epoch:5 step:4164 [D loss: 0.084355, acc.: 99.22%] [G loss: 1.080017]\n",
      "epoch:5 step:4165 [D loss: 0.042890, acc.: 100.00%] [G loss: 0.475919]\n",
      "epoch:5 step:4166 [D loss: 0.070308, acc.: 99.22%] [G loss: 0.787398]\n",
      "epoch:5 step:4167 [D loss: 0.073372, acc.: 99.22%] [G loss: 0.667062]\n",
      "epoch:5 step:4168 [D loss: 0.213714, acc.: 91.41%] [G loss: 2.089327]\n",
      "epoch:5 step:4169 [D loss: 1.354675, acc.: 45.31%] [G loss: 6.807498]\n",
      "epoch:5 step:4170 [D loss: 0.608239, acc.: 69.53%] [G loss: 4.653173]\n",
      "epoch:5 step:4171 [D loss: 0.384621, acc.: 78.12%] [G loss: 3.705178]\n",
      "epoch:5 step:4172 [D loss: 0.473948, acc.: 79.69%] [G loss: 2.574437]\n",
      "epoch:5 step:4173 [D loss: 0.232243, acc.: 92.19%] [G loss: 4.531290]\n",
      "epoch:5 step:4174 [D loss: 0.105496, acc.: 98.44%] [G loss: 3.897231]\n",
      "epoch:5 step:4175 [D loss: 0.181487, acc.: 95.31%] [G loss: 2.308052]\n",
      "epoch:5 step:4176 [D loss: 0.400371, acc.: 82.81%] [G loss: 3.613485]\n",
      "epoch:5 step:4177 [D loss: 0.381839, acc.: 85.16%] [G loss: 3.167243]\n",
      "epoch:5 step:4178 [D loss: 0.225200, acc.: 91.41%] [G loss: 4.148722]\n",
      "epoch:5 step:4179 [D loss: 0.292009, acc.: 86.72%] [G loss: 3.256448]\n",
      "epoch:5 step:4180 [D loss: 0.176467, acc.: 95.31%] [G loss: 3.089048]\n",
      "epoch:5 step:4181 [D loss: 0.258226, acc.: 90.62%] [G loss: 4.394661]\n",
      "epoch:5 step:4182 [D loss: 0.451557, acc.: 80.47%] [G loss: 2.827068]\n",
      "epoch:5 step:4183 [D loss: 0.172721, acc.: 96.88%] [G loss: 3.329749]\n",
      "epoch:5 step:4184 [D loss: 0.114512, acc.: 99.22%] [G loss: 3.127729]\n",
      "epoch:5 step:4185 [D loss: 0.570375, acc.: 70.31%] [G loss: 5.023423]\n",
      "epoch:5 step:4186 [D loss: 0.692996, acc.: 65.62%] [G loss: 2.454820]\n",
      "epoch:5 step:4187 [D loss: 0.228692, acc.: 90.62%] [G loss: 2.394092]\n",
      "epoch:5 step:4188 [D loss: 0.119384, acc.: 96.09%] [G loss: 2.744639]\n",
      "epoch:5 step:4189 [D loss: 0.224135, acc.: 91.41%] [G loss: 1.382822]\n",
      "epoch:5 step:4190 [D loss: 0.350698, acc.: 82.81%] [G loss: 4.280572]\n",
      "epoch:5 step:4191 [D loss: 0.547677, acc.: 73.44%] [G loss: 1.985895]\n",
      "epoch:5 step:4192 [D loss: 0.192871, acc.: 92.19%] [G loss: 2.653378]\n",
      "epoch:5 step:4193 [D loss: 0.123565, acc.: 97.66%] [G loss: 2.934578]\n",
      "epoch:5 step:4194 [D loss: 0.273809, acc.: 88.28%] [G loss: 3.643547]\n",
      "epoch:5 step:4195 [D loss: 0.096828, acc.: 97.66%] [G loss: 3.185545]\n",
      "epoch:5 step:4196 [D loss: 0.165106, acc.: 94.53%] [G loss: 1.695062]\n",
      "epoch:5 step:4197 [D loss: 0.472803, acc.: 75.78%] [G loss: 2.441005]\n",
      "epoch:5 step:4198 [D loss: 0.097652, acc.: 98.44%] [G loss: 2.965966]\n",
      "epoch:5 step:4199 [D loss: 0.241089, acc.: 91.41%] [G loss: 2.301600]\n",
      "epoch:5 step:4200 [D loss: 0.571764, acc.: 73.44%] [G loss: 1.076701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4201 [D loss: 0.061764, acc.: 98.44%] [G loss: 1.604023]\n",
      "epoch:5 step:4202 [D loss: 0.186278, acc.: 93.75%] [G loss: 3.349526]\n",
      "epoch:5 step:4203 [D loss: 0.111736, acc.: 98.44%] [G loss: 2.118273]\n",
      "epoch:5 step:4204 [D loss: 0.716156, acc.: 62.50%] [G loss: 3.274401]\n",
      "epoch:5 step:4205 [D loss: 0.562954, acc.: 67.97%] [G loss: 1.918247]\n",
      "epoch:5 step:4206 [D loss: 0.372179, acc.: 81.25%] [G loss: 4.414579]\n",
      "epoch:5 step:4207 [D loss: 0.224317, acc.: 91.41%] [G loss: 3.633229]\n",
      "epoch:5 step:4208 [D loss: 0.328050, acc.: 85.16%] [G loss: 2.648034]\n",
      "epoch:5 step:4209 [D loss: 0.084786, acc.: 99.22%] [G loss: 1.754974]\n",
      "epoch:5 step:4210 [D loss: 0.484084, acc.: 73.44%] [G loss: 4.005356]\n",
      "epoch:5 step:4211 [D loss: 0.503087, acc.: 75.78%] [G loss: 2.530082]\n",
      "epoch:5 step:4212 [D loss: 0.298683, acc.: 86.72%] [G loss: 3.550140]\n",
      "epoch:5 step:4213 [D loss: 0.163241, acc.: 92.97%] [G loss: 3.335915]\n",
      "epoch:5 step:4214 [D loss: 0.245031, acc.: 90.62%] [G loss: 2.494171]\n",
      "epoch:5 step:4215 [D loss: 0.088547, acc.: 98.44%] [G loss: 2.369604]\n",
      "epoch:5 step:4216 [D loss: 0.385192, acc.: 84.38%] [G loss: 4.326962]\n",
      "epoch:5 step:4217 [D loss: 0.392820, acc.: 82.03%] [G loss: 2.405704]\n",
      "epoch:5 step:4218 [D loss: 0.370385, acc.: 82.03%] [G loss: 4.904362]\n",
      "epoch:5 step:4219 [D loss: 0.157277, acc.: 92.97%] [G loss: 4.248154]\n",
      "epoch:5 step:4220 [D loss: 0.456716, acc.: 78.91%] [G loss: 1.435409]\n",
      "epoch:5 step:4221 [D loss: 0.160220, acc.: 92.19%] [G loss: 2.625483]\n",
      "epoch:5 step:4222 [D loss: 0.191306, acc.: 93.75%] [G loss: 2.921548]\n",
      "epoch:5 step:4223 [D loss: 0.102883, acc.: 97.66%] [G loss: 1.676617]\n",
      "epoch:5 step:4224 [D loss: 0.101328, acc.: 98.44%] [G loss: 0.715885]\n",
      "epoch:5 step:4225 [D loss: 0.029440, acc.: 100.00%] [G loss: 0.318760]\n",
      "epoch:5 step:4226 [D loss: 0.218725, acc.: 89.84%] [G loss: 1.971096]\n",
      "epoch:5 step:4227 [D loss: 0.792794, acc.: 63.28%] [G loss: 0.222308]\n",
      "epoch:5 step:4228 [D loss: 0.050387, acc.: 99.22%] [G loss: 1.253540]\n",
      "epoch:5 step:4229 [D loss: 0.040657, acc.: 99.22%] [G loss: 0.681277]\n",
      "epoch:5 step:4230 [D loss: 0.017637, acc.: 100.00%] [G loss: 0.419094]\n",
      "epoch:5 step:4231 [D loss: 0.012685, acc.: 100.00%] [G loss: 0.064761]\n",
      "epoch:5 step:4232 [D loss: 0.021305, acc.: 100.00%] [G loss: 0.050029]\n",
      "epoch:5 step:4233 [D loss: 0.794052, acc.: 57.03%] [G loss: 7.771581]\n",
      "epoch:5 step:4234 [D loss: 2.575862, acc.: 50.00%] [G loss: 4.216429]\n",
      "epoch:5 step:4235 [D loss: 0.378595, acc.: 83.59%] [G loss: 1.775886]\n",
      "epoch:5 step:4236 [D loss: 0.305931, acc.: 86.72%] [G loss: 2.654099]\n",
      "epoch:5 step:4237 [D loss: 0.113891, acc.: 97.66%] [G loss: 2.821650]\n",
      "epoch:5 step:4238 [D loss: 0.117838, acc.: 97.66%] [G loss: 2.997201]\n",
      "epoch:5 step:4239 [D loss: 0.249910, acc.: 91.41%] [G loss: 3.013853]\n",
      "epoch:5 step:4240 [D loss: 0.157872, acc.: 96.09%] [G loss: 2.653051]\n",
      "epoch:5 step:4241 [D loss: 0.323837, acc.: 84.38%] [G loss: 2.638947]\n",
      "epoch:5 step:4242 [D loss: 0.184893, acc.: 96.09%] [G loss: 2.337694]\n",
      "epoch:5 step:4243 [D loss: 0.192581, acc.: 94.53%] [G loss: 1.722312]\n",
      "epoch:5 step:4244 [D loss: 0.149821, acc.: 95.31%] [G loss: 2.126530]\n",
      "epoch:5 step:4245 [D loss: 0.256024, acc.: 88.28%] [G loss: 0.657718]\n",
      "epoch:5 step:4246 [D loss: 0.182417, acc.: 92.97%] [G loss: 1.119154]\n",
      "epoch:5 step:4247 [D loss: 0.183240, acc.: 95.31%] [G loss: 0.615645]\n",
      "epoch:5 step:4248 [D loss: 0.153404, acc.: 94.53%] [G loss: 1.201673]\n",
      "epoch:5 step:4249 [D loss: 0.144774, acc.: 94.53%] [G loss: 0.371543]\n",
      "epoch:5 step:4250 [D loss: 0.131666, acc.: 95.31%] [G loss: 1.244502]\n",
      "epoch:5 step:4251 [D loss: 0.148187, acc.: 94.53%] [G loss: 0.497348]\n",
      "epoch:5 step:4252 [D loss: 0.119154, acc.: 97.66%] [G loss: 0.711248]\n",
      "epoch:5 step:4253 [D loss: 0.212304, acc.: 93.75%] [G loss: 2.114436]\n",
      "epoch:5 step:4254 [D loss: 0.163790, acc.: 92.97%] [G loss: 0.923650]\n",
      "epoch:5 step:4255 [D loss: 0.322238, acc.: 84.38%] [G loss: 1.949979]\n",
      "epoch:5 step:4256 [D loss: 0.079219, acc.: 98.44%] [G loss: 2.955635]\n",
      "epoch:5 step:4257 [D loss: 0.803728, acc.: 60.16%] [G loss: 2.039417]\n",
      "epoch:5 step:4258 [D loss: 0.223935, acc.: 92.97%] [G loss: 2.617755]\n",
      "epoch:5 step:4259 [D loss: 0.064037, acc.: 100.00%] [G loss: 2.931014]\n",
      "epoch:5 step:4260 [D loss: 0.242637, acc.: 92.19%] [G loss: 3.994240]\n",
      "epoch:5 step:4261 [D loss: 0.250146, acc.: 90.62%] [G loss: 4.497008]\n",
      "epoch:5 step:4262 [D loss: 0.161755, acc.: 95.31%] [G loss: 4.580079]\n",
      "epoch:5 step:4263 [D loss: 0.245335, acc.: 91.41%] [G loss: 3.877602]\n",
      "epoch:5 step:4264 [D loss: 0.059302, acc.: 99.22%] [G loss: 3.533341]\n",
      "epoch:5 step:4265 [D loss: 0.126051, acc.: 96.09%] [G loss: 2.549132]\n",
      "epoch:5 step:4266 [D loss: 0.050416, acc.: 99.22%] [G loss: 2.284687]\n",
      "epoch:5 step:4267 [D loss: 0.068270, acc.: 100.00%] [G loss: 1.546680]\n",
      "epoch:5 step:4268 [D loss: 0.387745, acc.: 82.81%] [G loss: 4.058700]\n",
      "epoch:5 step:4269 [D loss: 0.401761, acc.: 80.47%] [G loss: 1.591024]\n",
      "epoch:5 step:4270 [D loss: 0.114656, acc.: 96.88%] [G loss: 1.421006]\n",
      "epoch:5 step:4271 [D loss: 0.113418, acc.: 97.66%] [G loss: 1.177603]\n",
      "epoch:5 step:4272 [D loss: 0.291772, acc.: 88.28%] [G loss: 1.258174]\n",
      "epoch:5 step:4273 [D loss: 0.029623, acc.: 100.00%] [G loss: 1.828980]\n",
      "epoch:5 step:4274 [D loss: 0.023865, acc.: 100.00%] [G loss: 0.694556]\n",
      "epoch:5 step:4275 [D loss: 0.242040, acc.: 90.62%] [G loss: 1.499327]\n",
      "epoch:5 step:4276 [D loss: 0.156363, acc.: 95.31%] [G loss: 1.317271]\n",
      "epoch:5 step:4277 [D loss: 0.772769, acc.: 63.28%] [G loss: 6.851799]\n",
      "epoch:5 step:4278 [D loss: 2.689661, acc.: 50.00%] [G loss: 3.774809]\n",
      "epoch:5 step:4279 [D loss: 0.202361, acc.: 93.75%] [G loss: 3.014894]\n",
      "epoch:5 step:4280 [D loss: 0.281168, acc.: 89.84%] [G loss: 2.771444]\n",
      "epoch:5 step:4281 [D loss: 0.063430, acc.: 99.22%] [G loss: 3.319968]\n",
      "epoch:5 step:4282 [D loss: 0.149149, acc.: 96.09%] [G loss: 2.663337]\n",
      "epoch:5 step:4283 [D loss: 0.114214, acc.: 98.44%] [G loss: 1.927542]\n",
      "epoch:5 step:4284 [D loss: 0.233819, acc.: 91.41%] [G loss: 2.732004]\n",
      "epoch:5 step:4285 [D loss: 0.123626, acc.: 96.88%] [G loss: 3.478782]\n",
      "epoch:5 step:4286 [D loss: 0.182892, acc.: 94.53%] [G loss: 3.048751]\n",
      "epoch:5 step:4287 [D loss: 0.228065, acc.: 89.84%] [G loss: 2.827070]\n",
      "epoch:5 step:4288 [D loss: 0.142542, acc.: 96.88%] [G loss: 3.341877]\n",
      "epoch:5 step:4289 [D loss: 0.103776, acc.: 99.22%] [G loss: 3.390155]\n",
      "epoch:5 step:4290 [D loss: 0.259059, acc.: 89.84%] [G loss: 2.178801]\n",
      "epoch:5 step:4291 [D loss: 0.149135, acc.: 96.88%] [G loss: 3.107835]\n",
      "epoch:5 step:4292 [D loss: 0.278804, acc.: 90.62%] [G loss: 2.068268]\n",
      "epoch:5 step:4293 [D loss: 0.080819, acc.: 99.22%] [G loss: 3.790711]\n",
      "epoch:5 step:4294 [D loss: 0.255481, acc.: 92.19%] [G loss: 2.186804]\n",
      "epoch:5 step:4295 [D loss: 0.200704, acc.: 93.75%] [G loss: 2.072742]\n",
      "epoch:5 step:4296 [D loss: 0.579891, acc.: 69.53%] [G loss: 3.620194]\n",
      "epoch:5 step:4297 [D loss: 0.075362, acc.: 96.88%] [G loss: 4.766154]\n",
      "epoch:5 step:4298 [D loss: 0.083370, acc.: 97.66%] [G loss: 2.848666]\n",
      "epoch:5 step:4299 [D loss: 0.161005, acc.: 95.31%] [G loss: 1.729564]\n",
      "epoch:5 step:4300 [D loss: 0.287424, acc.: 85.94%] [G loss: 3.751184]\n",
      "epoch:5 step:4301 [D loss: 0.291989, acc.: 84.38%] [G loss: 2.300596]\n",
      "epoch:5 step:4302 [D loss: 0.125676, acc.: 96.09%] [G loss: 1.504564]\n",
      "epoch:5 step:4303 [D loss: 0.039309, acc.: 99.22%] [G loss: 2.082603]\n",
      "epoch:5 step:4304 [D loss: 0.039564, acc.: 100.00%] [G loss: 1.155772]\n",
      "epoch:5 step:4305 [D loss: 0.069378, acc.: 98.44%] [G loss: 0.984865]\n",
      "epoch:5 step:4306 [D loss: 0.268289, acc.: 87.50%] [G loss: 2.090144]\n",
      "epoch:5 step:4307 [D loss: 0.266446, acc.: 91.41%] [G loss: 2.552412]\n",
      "epoch:5 step:4308 [D loss: 0.304534, acc.: 86.72%] [G loss: 4.385088]\n",
      "epoch:5 step:4309 [D loss: 0.208744, acc.: 91.41%] [G loss: 1.823820]\n",
      "epoch:5 step:4310 [D loss: 0.468623, acc.: 81.25%] [G loss: 4.931445]\n",
      "epoch:5 step:4311 [D loss: 0.412843, acc.: 78.12%] [G loss: 2.435450]\n",
      "epoch:5 step:4312 [D loss: 0.137323, acc.: 94.53%] [G loss: 4.523210]\n",
      "epoch:5 step:4313 [D loss: 0.098541, acc.: 97.66%] [G loss: 5.359417]\n",
      "epoch:5 step:4314 [D loss: 0.227023, acc.: 92.97%] [G loss: 2.361991]\n",
      "epoch:5 step:4315 [D loss: 0.509791, acc.: 73.44%] [G loss: 7.157370]\n",
      "epoch:5 step:4316 [D loss: 0.758483, acc.: 63.28%] [G loss: 4.471680]\n",
      "epoch:5 step:4317 [D loss: 0.210681, acc.: 89.06%] [G loss: 5.647095]\n",
      "epoch:5 step:4318 [D loss: 0.093062, acc.: 99.22%] [G loss: 5.383313]\n",
      "epoch:5 step:4319 [D loss: 0.033902, acc.: 99.22%] [G loss: 5.524968]\n",
      "epoch:5 step:4320 [D loss: 0.107135, acc.: 96.88%] [G loss: 3.779562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4321 [D loss: 0.165661, acc.: 95.31%] [G loss: 2.673273]\n",
      "epoch:5 step:4322 [D loss: 0.148633, acc.: 94.53%] [G loss: 2.674345]\n",
      "epoch:5 step:4323 [D loss: 0.218673, acc.: 92.19%] [G loss: 3.800524]\n",
      "epoch:5 step:4324 [D loss: 0.136625, acc.: 96.88%] [G loss: 3.504888]\n",
      "epoch:5 step:4325 [D loss: 0.381839, acc.: 81.25%] [G loss: 1.592786]\n",
      "epoch:5 step:4326 [D loss: 0.659846, acc.: 66.41%] [G loss: 4.751576]\n",
      "epoch:5 step:4327 [D loss: 0.405502, acc.: 83.59%] [G loss: 2.791359]\n",
      "epoch:5 step:4328 [D loss: 0.117058, acc.: 96.88%] [G loss: 2.649738]\n",
      "epoch:5 step:4329 [D loss: 0.075866, acc.: 99.22%] [G loss: 3.124871]\n",
      "epoch:5 step:4330 [D loss: 0.285862, acc.: 92.19%] [G loss: 3.530967]\n",
      "epoch:5 step:4331 [D loss: 0.161023, acc.: 96.88%] [G loss: 3.326146]\n",
      "epoch:5 step:4332 [D loss: 0.066728, acc.: 100.00%] [G loss: 4.025136]\n",
      "epoch:5 step:4333 [D loss: 0.458837, acc.: 79.69%] [G loss: 3.763901]\n",
      "epoch:5 step:4334 [D loss: 0.534313, acc.: 72.66%] [G loss: 2.892104]\n",
      "epoch:5 step:4335 [D loss: 0.109085, acc.: 96.88%] [G loss: 1.885298]\n",
      "epoch:5 step:4336 [D loss: 0.229807, acc.: 91.41%] [G loss: 1.690857]\n",
      "epoch:5 step:4337 [D loss: 0.210849, acc.: 92.19%] [G loss: 5.037963]\n",
      "epoch:5 step:4338 [D loss: 0.494065, acc.: 76.56%] [G loss: 2.381582]\n",
      "epoch:5 step:4339 [D loss: 0.402885, acc.: 81.25%] [G loss: 4.138523]\n",
      "epoch:5 step:4340 [D loss: 0.190605, acc.: 92.97%] [G loss: 2.589613]\n",
      "epoch:5 step:4341 [D loss: 0.095409, acc.: 99.22%] [G loss: 2.379736]\n",
      "epoch:5 step:4342 [D loss: 0.507281, acc.: 77.34%] [G loss: 5.994847]\n",
      "epoch:5 step:4343 [D loss: 0.447674, acc.: 77.34%] [G loss: 2.695684]\n",
      "epoch:5 step:4344 [D loss: 0.245048, acc.: 91.41%] [G loss: 2.335088]\n",
      "epoch:5 step:4345 [D loss: 0.100427, acc.: 99.22%] [G loss: 2.411207]\n",
      "epoch:5 step:4346 [D loss: 0.113227, acc.: 98.44%] [G loss: 4.097356]\n",
      "epoch:5 step:4347 [D loss: 0.131355, acc.: 96.09%] [G loss: 2.126941]\n",
      "epoch:5 step:4348 [D loss: 0.204683, acc.: 89.84%] [G loss: 1.580626]\n",
      "epoch:5 step:4349 [D loss: 0.061631, acc.: 98.44%] [G loss: 2.087348]\n",
      "epoch:5 step:4350 [D loss: 0.135008, acc.: 96.09%] [G loss: 0.751450]\n",
      "epoch:5 step:4351 [D loss: 0.419520, acc.: 78.91%] [G loss: 5.638136]\n",
      "epoch:5 step:4352 [D loss: 0.201084, acc.: 89.84%] [G loss: 4.813121]\n",
      "epoch:5 step:4353 [D loss: 0.603198, acc.: 72.66%] [G loss: 3.476202]\n",
      "epoch:5 step:4354 [D loss: 0.038722, acc.: 100.00%] [G loss: 3.795912]\n",
      "epoch:5 step:4355 [D loss: 0.080441, acc.: 98.44%] [G loss: 1.689771]\n",
      "epoch:5 step:4356 [D loss: 0.401982, acc.: 84.38%] [G loss: 4.428850]\n",
      "epoch:5 step:4357 [D loss: 0.121434, acc.: 96.88%] [G loss: 3.185502]\n",
      "epoch:5 step:4358 [D loss: 0.562606, acc.: 74.22%] [G loss: 2.765407]\n",
      "epoch:5 step:4359 [D loss: 0.082872, acc.: 96.88%] [G loss: 3.547366]\n",
      "epoch:5 step:4360 [D loss: 0.167529, acc.: 95.31%] [G loss: 2.497034]\n",
      "epoch:5 step:4361 [D loss: 0.124795, acc.: 94.53%] [G loss: 1.925258]\n",
      "epoch:5 step:4362 [D loss: 0.235669, acc.: 89.06%] [G loss: 4.371190]\n",
      "epoch:5 step:4363 [D loss: 0.213315, acc.: 86.72%] [G loss: 2.839287]\n",
      "epoch:5 step:4364 [D loss: 0.343349, acc.: 89.06%] [G loss: 4.914456]\n",
      "epoch:5 step:4365 [D loss: 0.749197, acc.: 60.94%] [G loss: 4.856836]\n",
      "epoch:5 step:4366 [D loss: 0.040073, acc.: 100.00%] [G loss: 5.601486]\n",
      "epoch:5 step:4367 [D loss: 0.160110, acc.: 93.75%] [G loss: 3.767052]\n",
      "epoch:5 step:4368 [D loss: 0.135242, acc.: 96.88%] [G loss: 4.417703]\n",
      "epoch:5 step:4369 [D loss: 0.047661, acc.: 100.00%] [G loss: 4.545411]\n",
      "epoch:5 step:4370 [D loss: 0.069720, acc.: 99.22%] [G loss: 3.240980]\n",
      "epoch:5 step:4371 [D loss: 0.140083, acc.: 96.88%] [G loss: 2.577117]\n",
      "epoch:5 step:4372 [D loss: 0.111321, acc.: 97.66%] [G loss: 3.679569]\n",
      "epoch:5 step:4373 [D loss: 0.227919, acc.: 92.97%] [G loss: 2.125561]\n",
      "epoch:5 step:4374 [D loss: 0.335304, acc.: 85.94%] [G loss: 6.086170]\n",
      "epoch:5 step:4375 [D loss: 0.577375, acc.: 71.88%] [G loss: 3.015287]\n",
      "epoch:5 step:4376 [D loss: 0.098225, acc.: 96.88%] [G loss: 3.871285]\n",
      "epoch:5 step:4377 [D loss: 0.077221, acc.: 100.00%] [G loss: 3.773962]\n",
      "epoch:5 step:4378 [D loss: 0.183064, acc.: 94.53%] [G loss: 3.908672]\n",
      "epoch:5 step:4379 [D loss: 0.232973, acc.: 89.06%] [G loss: 3.140389]\n",
      "epoch:5 step:4380 [D loss: 0.320553, acc.: 85.16%] [G loss: 5.277830]\n",
      "epoch:5 step:4381 [D loss: 0.479592, acc.: 78.91%] [G loss: 1.342701]\n",
      "epoch:5 step:4382 [D loss: 0.554400, acc.: 75.78%] [G loss: 5.495866]\n",
      "epoch:5 step:4383 [D loss: 0.889220, acc.: 59.38%] [G loss: 1.955215]\n",
      "epoch:5 step:4384 [D loss: 0.315953, acc.: 85.94%] [G loss: 3.916185]\n",
      "epoch:5 step:4385 [D loss: 0.133320, acc.: 94.53%] [G loss: 3.726197]\n",
      "epoch:5 step:4386 [D loss: 0.170755, acc.: 95.31%] [G loss: 3.478637]\n",
      "epoch:5 step:4387 [D loss: 0.349900, acc.: 85.16%] [G loss: 4.623315]\n",
      "epoch:5 step:4388 [D loss: 0.310890, acc.: 84.38%] [G loss: 3.271704]\n",
      "epoch:5 step:4389 [D loss: 0.242307, acc.: 90.62%] [G loss: 3.857008]\n",
      "epoch:5 step:4390 [D loss: 0.194670, acc.: 92.97%] [G loss: 2.629373]\n",
      "epoch:5 step:4391 [D loss: 0.107621, acc.: 97.66%] [G loss: 3.764273]\n",
      "epoch:5 step:4392 [D loss: 0.119275, acc.: 94.53%] [G loss: 3.015563]\n",
      "epoch:5 step:4393 [D loss: 0.653545, acc.: 64.84%] [G loss: 6.212692]\n",
      "epoch:5 step:4394 [D loss: 0.490471, acc.: 78.12%] [G loss: 2.634221]\n",
      "epoch:5 step:4395 [D loss: 0.187415, acc.: 93.75%] [G loss: 4.272750]\n",
      "epoch:5 step:4396 [D loss: 0.172940, acc.: 95.31%] [G loss: 1.256932]\n",
      "epoch:5 step:4397 [D loss: 0.087108, acc.: 97.66%] [G loss: 1.565017]\n",
      "epoch:5 step:4398 [D loss: 0.124212, acc.: 97.66%] [G loss: 2.157833]\n",
      "epoch:5 step:4399 [D loss: 0.071673, acc.: 99.22%] [G loss: 1.789795]\n",
      "epoch:5 step:4400 [D loss: 0.381155, acc.: 85.16%] [G loss: 3.426581]\n",
      "epoch:5 step:4401 [D loss: 0.777307, acc.: 67.97%] [G loss: 1.039237]\n",
      "epoch:5 step:4402 [D loss: 0.068243, acc.: 100.00%] [G loss: 0.844205]\n",
      "epoch:5 step:4403 [D loss: 0.416467, acc.: 80.47%] [G loss: 4.627343]\n",
      "epoch:5 step:4404 [D loss: 0.833042, acc.: 64.06%] [G loss: 2.232832]\n",
      "epoch:5 step:4405 [D loss: 0.734542, acc.: 67.19%] [G loss: 5.351244]\n",
      "epoch:5 step:4406 [D loss: 0.433741, acc.: 78.12%] [G loss: 4.218521]\n",
      "epoch:5 step:4407 [D loss: 0.270119, acc.: 88.28%] [G loss: 3.414933]\n",
      "epoch:5 step:4408 [D loss: 0.059590, acc.: 100.00%] [G loss: 3.948389]\n",
      "epoch:5 step:4409 [D loss: 0.219158, acc.: 92.19%] [G loss: 3.105243]\n",
      "epoch:5 step:4410 [D loss: 0.173362, acc.: 94.53%] [G loss: 3.746964]\n",
      "epoch:5 step:4411 [D loss: 0.115469, acc.: 98.44%] [G loss: 3.569177]\n",
      "epoch:5 step:4412 [D loss: 0.375583, acc.: 82.81%] [G loss: 2.926789]\n",
      "epoch:5 step:4413 [D loss: 0.156524, acc.: 95.31%] [G loss: 4.371220]\n",
      "epoch:5 step:4414 [D loss: 0.256573, acc.: 92.19%] [G loss: 2.999936]\n",
      "epoch:5 step:4415 [D loss: 0.142350, acc.: 95.31%] [G loss: 3.222847]\n",
      "epoch:5 step:4416 [D loss: 0.052844, acc.: 100.00%] [G loss: 3.703756]\n",
      "epoch:5 step:4417 [D loss: 0.100590, acc.: 97.66%] [G loss: 3.640138]\n",
      "epoch:5 step:4418 [D loss: 0.176883, acc.: 97.66%] [G loss: 2.849353]\n",
      "epoch:5 step:4419 [D loss: 0.126409, acc.: 98.44%] [G loss: 2.056954]\n",
      "epoch:5 step:4420 [D loss: 0.253353, acc.: 87.50%] [G loss: 3.503737]\n",
      "epoch:5 step:4421 [D loss: 0.162194, acc.: 93.75%] [G loss: 1.891989]\n",
      "epoch:5 step:4422 [D loss: 0.202680, acc.: 90.62%] [G loss: 3.206257]\n",
      "epoch:5 step:4423 [D loss: 0.216460, acc.: 92.19%] [G loss: 1.730397]\n",
      "epoch:5 step:4424 [D loss: 0.334616, acc.: 85.16%] [G loss: 1.290311]\n",
      "epoch:5 step:4425 [D loss: 0.041594, acc.: 100.00%] [G loss: 2.792903]\n",
      "epoch:5 step:4426 [D loss: 0.183985, acc.: 92.19%] [G loss: 1.114461]\n",
      "epoch:5 step:4427 [D loss: 0.167985, acc.: 95.31%] [G loss: 1.190180]\n",
      "epoch:5 step:4428 [D loss: 0.041458, acc.: 100.00%] [G loss: 0.716323]\n",
      "epoch:5 step:4429 [D loss: 0.322700, acc.: 85.94%] [G loss: 2.647963]\n",
      "epoch:5 step:4430 [D loss: 0.319837, acc.: 83.59%] [G loss: 0.601048]\n",
      "epoch:5 step:4431 [D loss: 0.217472, acc.: 92.19%] [G loss: 1.785197]\n",
      "epoch:5 step:4432 [D loss: 0.052088, acc.: 98.44%] [G loss: 0.776001]\n",
      "epoch:5 step:4433 [D loss: 0.126446, acc.: 96.88%] [G loss: 0.226528]\n",
      "epoch:5 step:4434 [D loss: 0.181110, acc.: 92.19%] [G loss: 1.632678]\n",
      "epoch:5 step:4435 [D loss: 0.159551, acc.: 94.53%] [G loss: 0.903749]\n",
      "epoch:5 step:4436 [D loss: 0.115224, acc.: 97.66%] [G loss: 1.908150]\n",
      "epoch:5 step:4437 [D loss: 0.102926, acc.: 97.66%] [G loss: 1.524693]\n",
      "epoch:5 step:4438 [D loss: 0.094180, acc.: 96.09%] [G loss: 0.497934]\n",
      "epoch:5 step:4439 [D loss: 0.130394, acc.: 97.66%] [G loss: 1.505528]\n",
      "epoch:5 step:4440 [D loss: 0.426954, acc.: 78.91%] [G loss: 7.086409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4441 [D loss: 1.305993, acc.: 53.12%] [G loss: 2.661793]\n",
      "epoch:5 step:4442 [D loss: 0.183396, acc.: 94.53%] [G loss: 3.605083]\n",
      "epoch:5 step:4443 [D loss: 0.048938, acc.: 99.22%] [G loss: 4.585492]\n",
      "epoch:5 step:4444 [D loss: 0.238544, acc.: 93.75%] [G loss: 4.475782]\n",
      "epoch:5 step:4445 [D loss: 0.104335, acc.: 98.44%] [G loss: 4.812342]\n",
      "epoch:5 step:4446 [D loss: 0.102125, acc.: 97.66%] [G loss: 4.443837]\n",
      "epoch:5 step:4447 [D loss: 0.210271, acc.: 92.19%] [G loss: 2.102938]\n",
      "epoch:5 step:4448 [D loss: 0.057739, acc.: 99.22%] [G loss: 0.782280]\n",
      "epoch:5 step:4449 [D loss: 0.058055, acc.: 99.22%] [G loss: 1.055978]\n",
      "epoch:5 step:4450 [D loss: 0.026205, acc.: 100.00%] [G loss: 1.006330]\n",
      "epoch:5 step:4451 [D loss: 0.032763, acc.: 99.22%] [G loss: 0.492818]\n",
      "epoch:5 step:4452 [D loss: 0.027415, acc.: 100.00%] [G loss: 0.360996]\n",
      "epoch:5 step:4453 [D loss: 0.031447, acc.: 99.22%] [G loss: 0.125751]\n",
      "epoch:5 step:4454 [D loss: 0.095746, acc.: 97.66%] [G loss: 0.394774]\n",
      "epoch:5 step:4455 [D loss: 0.092608, acc.: 96.88%] [G loss: 0.489522]\n",
      "epoch:5 step:4456 [D loss: 0.096298, acc.: 98.44%] [G loss: 0.670406]\n",
      "epoch:5 step:4457 [D loss: 0.538771, acc.: 71.09%] [G loss: 9.167687]\n",
      "epoch:5 step:4458 [D loss: 1.471988, acc.: 57.03%] [G loss: 3.198566]\n",
      "epoch:5 step:4459 [D loss: 0.145016, acc.: 92.19%] [G loss: 2.538330]\n",
      "epoch:5 step:4460 [D loss: 0.032976, acc.: 100.00%] [G loss: 2.314721]\n",
      "epoch:5 step:4461 [D loss: 0.073473, acc.: 99.22%] [G loss: 2.116078]\n",
      "epoch:5 step:4462 [D loss: 0.444267, acc.: 77.34%] [G loss: 4.767305]\n",
      "epoch:5 step:4463 [D loss: 0.609355, acc.: 69.53%] [G loss: 2.815896]\n",
      "epoch:5 step:4464 [D loss: 0.160540, acc.: 93.75%] [G loss: 2.894007]\n",
      "epoch:5 step:4465 [D loss: 0.160068, acc.: 94.53%] [G loss: 3.680885]\n",
      "epoch:5 step:4466 [D loss: 0.203504, acc.: 95.31%] [G loss: 2.735306]\n",
      "epoch:5 step:4467 [D loss: 0.345534, acc.: 85.16%] [G loss: 3.481492]\n",
      "epoch:5 step:4468 [D loss: 0.097874, acc.: 96.88%] [G loss: 4.680911]\n",
      "epoch:5 step:4469 [D loss: 0.354201, acc.: 82.81%] [G loss: 2.740129]\n",
      "epoch:5 step:4470 [D loss: 0.152819, acc.: 95.31%] [G loss: 3.392486]\n",
      "epoch:5 step:4471 [D loss: 0.249966, acc.: 92.19%] [G loss: 4.259576]\n",
      "epoch:5 step:4472 [D loss: 0.369758, acc.: 82.81%] [G loss: 3.648053]\n",
      "epoch:5 step:4473 [D loss: 0.071109, acc.: 98.44%] [G loss: 3.890133]\n",
      "epoch:5 step:4474 [D loss: 0.345455, acc.: 85.94%] [G loss: 5.308830]\n",
      "epoch:5 step:4475 [D loss: 0.240918, acc.: 87.50%] [G loss: 2.736855]\n",
      "epoch:5 step:4476 [D loss: 0.679226, acc.: 65.62%] [G loss: 6.395163]\n",
      "epoch:5 step:4477 [D loss: 0.507092, acc.: 74.22%] [G loss: 4.277715]\n",
      "epoch:5 step:4478 [D loss: 0.104306, acc.: 97.66%] [G loss: 2.864266]\n",
      "epoch:5 step:4479 [D loss: 0.125956, acc.: 97.66%] [G loss: 3.230267]\n",
      "epoch:5 step:4480 [D loss: 0.432083, acc.: 78.91%] [G loss: 4.894653]\n",
      "epoch:5 step:4481 [D loss: 0.115701, acc.: 96.09%] [G loss: 4.684148]\n",
      "epoch:5 step:4482 [D loss: 0.142655, acc.: 95.31%] [G loss: 3.901104]\n",
      "epoch:5 step:4483 [D loss: 0.181173, acc.: 92.97%] [G loss: 3.765644]\n",
      "epoch:5 step:4484 [D loss: 0.173974, acc.: 96.09%] [G loss: 4.463124]\n",
      "epoch:5 step:4485 [D loss: 0.444049, acc.: 77.34%] [G loss: 2.258547]\n",
      "epoch:5 step:4486 [D loss: 0.126276, acc.: 98.44%] [G loss: 2.781331]\n",
      "epoch:5 step:4487 [D loss: 0.092402, acc.: 98.44%] [G loss: 1.812995]\n",
      "epoch:5 step:4488 [D loss: 0.103842, acc.: 98.44%] [G loss: 2.328445]\n",
      "epoch:5 step:4489 [D loss: 0.157523, acc.: 96.88%] [G loss: 1.707482]\n",
      "epoch:5 step:4490 [D loss: 0.406925, acc.: 82.03%] [G loss: 2.429946]\n",
      "epoch:5 step:4491 [D loss: 0.066967, acc.: 98.44%] [G loss: 2.086938]\n",
      "epoch:5 step:4492 [D loss: 0.943843, acc.: 51.56%] [G loss: 3.525386]\n",
      "epoch:5 step:4493 [D loss: 0.437986, acc.: 77.34%] [G loss: 2.449667]\n",
      "epoch:5 step:4494 [D loss: 0.286742, acc.: 87.50%] [G loss: 4.362462]\n",
      "epoch:5 step:4495 [D loss: 0.075854, acc.: 99.22%] [G loss: 4.127258]\n",
      "epoch:5 step:4496 [D loss: 0.120381, acc.: 97.66%] [G loss: 3.214603]\n",
      "epoch:5 step:4497 [D loss: 0.511489, acc.: 78.91%] [G loss: 4.169172]\n",
      "epoch:5 step:4498 [D loss: 0.187760, acc.: 90.62%] [G loss: 3.925858]\n",
      "epoch:5 step:4499 [D loss: 0.704825, acc.: 64.06%] [G loss: 5.937222]\n",
      "epoch:5 step:4500 [D loss: 0.385070, acc.: 81.25%] [G loss: 3.910018]\n",
      "epoch:5 step:4501 [D loss: 0.272706, acc.: 92.19%] [G loss: 4.775377]\n",
      "epoch:5 step:4502 [D loss: 0.029334, acc.: 99.22%] [G loss: 5.033853]\n",
      "epoch:5 step:4503 [D loss: 0.147024, acc.: 97.66%] [G loss: 3.003725]\n",
      "epoch:5 step:4504 [D loss: 0.178088, acc.: 93.75%] [G loss: 3.852864]\n",
      "epoch:5 step:4505 [D loss: 0.092254, acc.: 96.88%] [G loss: 4.297032]\n",
      "epoch:5 step:4506 [D loss: 0.555001, acc.: 75.78%] [G loss: 2.909654]\n",
      "epoch:5 step:4507 [D loss: 0.149425, acc.: 96.09%] [G loss: 5.003390]\n",
      "epoch:5 step:4508 [D loss: 1.395460, acc.: 28.91%] [G loss: 5.228055]\n",
      "epoch:5 step:4509 [D loss: 0.069436, acc.: 99.22%] [G loss: 6.238171]\n",
      "epoch:5 step:4510 [D loss: 0.155938, acc.: 92.97%] [G loss: 3.621660]\n",
      "epoch:5 step:4511 [D loss: 0.122643, acc.: 96.09%] [G loss: 4.127007]\n",
      "epoch:5 step:4512 [D loss: 0.133827, acc.: 96.09%] [G loss: 4.586690]\n",
      "epoch:5 step:4513 [D loss: 0.230989, acc.: 91.41%] [G loss: 3.161465]\n",
      "epoch:5 step:4514 [D loss: 0.215022, acc.: 92.19%] [G loss: 3.983927]\n",
      "epoch:5 step:4515 [D loss: 0.256933, acc.: 89.06%] [G loss: 3.244099]\n",
      "epoch:5 step:4516 [D loss: 0.160473, acc.: 96.09%] [G loss: 3.463880]\n",
      "epoch:5 step:4517 [D loss: 0.062915, acc.: 99.22%] [G loss: 2.922915]\n",
      "epoch:5 step:4518 [D loss: 2.204055, acc.: 32.81%] [G loss: 7.268984]\n",
      "epoch:5 step:4519 [D loss: 0.462140, acc.: 75.78%] [G loss: 7.337035]\n",
      "epoch:5 step:4520 [D loss: 0.401847, acc.: 82.03%] [G loss: 3.678314]\n",
      "epoch:5 step:4521 [D loss: 0.041380, acc.: 99.22%] [G loss: 1.873396]\n",
      "epoch:5 step:4522 [D loss: 0.131710, acc.: 94.53%] [G loss: 2.166909]\n",
      "epoch:5 step:4523 [D loss: 0.088751, acc.: 98.44%] [G loss: 1.364459]\n",
      "epoch:5 step:4524 [D loss: 0.037419, acc.: 100.00%] [G loss: 1.030458]\n",
      "epoch:5 step:4525 [D loss: 0.036746, acc.: 100.00%] [G loss: 0.459967]\n",
      "epoch:5 step:4526 [D loss: 0.112801, acc.: 98.44%] [G loss: 0.959141]\n",
      "epoch:5 step:4527 [D loss: 0.046264, acc.: 100.00%] [G loss: 0.702081]\n",
      "epoch:5 step:4528 [D loss: 0.498058, acc.: 74.22%] [G loss: 0.059387]\n",
      "epoch:5 step:4529 [D loss: 1.421672, acc.: 53.12%] [G loss: 3.858926]\n",
      "epoch:5 step:4530 [D loss: 0.249795, acc.: 86.72%] [G loss: 4.858496]\n",
      "epoch:5 step:4531 [D loss: 0.856806, acc.: 60.94%] [G loss: 0.939523]\n",
      "epoch:5 step:4532 [D loss: 0.358623, acc.: 80.47%] [G loss: 2.900620]\n",
      "epoch:5 step:4533 [D loss: 0.101480, acc.: 98.44%] [G loss: 2.816299]\n",
      "epoch:5 step:4534 [D loss: 0.139830, acc.: 97.66%] [G loss: 2.449412]\n",
      "epoch:5 step:4535 [D loss: 0.256545, acc.: 90.62%] [G loss: 2.124469]\n",
      "epoch:5 step:4536 [D loss: 0.140157, acc.: 96.88%] [G loss: 3.123410]\n",
      "epoch:5 step:4537 [D loss: 0.170760, acc.: 97.66%] [G loss: 2.740963]\n",
      "epoch:5 step:4538 [D loss: 0.270723, acc.: 92.19%] [G loss: 3.117323]\n",
      "epoch:5 step:4539 [D loss: 0.207946, acc.: 94.53%] [G loss: 3.151628]\n",
      "epoch:5 step:4540 [D loss: 0.256169, acc.: 90.62%] [G loss: 2.118667]\n",
      "epoch:5 step:4541 [D loss: 0.106297, acc.: 100.00%] [G loss: 2.003509]\n",
      "epoch:5 step:4542 [D loss: 0.320230, acc.: 88.28%] [G loss: 2.785033]\n",
      "epoch:5 step:4543 [D loss: 0.262753, acc.: 89.06%] [G loss: 2.411589]\n",
      "epoch:5 step:4544 [D loss: 0.128953, acc.: 98.44%] [G loss: 1.341229]\n",
      "epoch:5 step:4545 [D loss: 0.103855, acc.: 100.00%] [G loss: 1.931161]\n",
      "epoch:5 step:4546 [D loss: 0.062401, acc.: 100.00%] [G loss: 1.091920]\n",
      "epoch:5 step:4547 [D loss: 0.102315, acc.: 97.66%] [G loss: 0.372028]\n",
      "epoch:5 step:4548 [D loss: 0.060311, acc.: 99.22%] [G loss: 0.427269]\n",
      "epoch:5 step:4549 [D loss: 0.083168, acc.: 98.44%] [G loss: 1.688377]\n",
      "epoch:5 step:4550 [D loss: 0.515708, acc.: 75.78%] [G loss: 3.694681]\n",
      "epoch:5 step:4551 [D loss: 0.459087, acc.: 75.78%] [G loss: 1.517192]\n",
      "epoch:5 step:4552 [D loss: 0.184469, acc.: 96.88%] [G loss: 1.463259]\n",
      "epoch:5 step:4553 [D loss: 0.180153, acc.: 95.31%] [G loss: 2.518075]\n",
      "epoch:5 step:4554 [D loss: 0.105091, acc.: 98.44%] [G loss: 3.509011]\n",
      "epoch:5 step:4555 [D loss: 0.305034, acc.: 86.72%] [G loss: 1.094010]\n",
      "epoch:5 step:4556 [D loss: 0.193902, acc.: 91.41%] [G loss: 2.933014]\n",
      "epoch:5 step:4557 [D loss: 0.319813, acc.: 87.50%] [G loss: 3.769101]\n",
      "epoch:5 step:4558 [D loss: 0.476404, acc.: 76.56%] [G loss: 4.278067]\n",
      "epoch:5 step:4559 [D loss: 0.118311, acc.: 96.88%] [G loss: 4.854688]\n",
      "epoch:5 step:4560 [D loss: 0.245113, acc.: 89.84%] [G loss: 3.639188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4561 [D loss: 0.416109, acc.: 82.81%] [G loss: 5.785962]\n",
      "epoch:5 step:4562 [D loss: 0.312561, acc.: 86.72%] [G loss: 3.926373]\n",
      "epoch:5 step:4563 [D loss: 0.124870, acc.: 97.66%] [G loss: 4.234108]\n",
      "epoch:5 step:4564 [D loss: 0.075516, acc.: 97.66%] [G loss: 2.517166]\n",
      "epoch:5 step:4565 [D loss: 0.072555, acc.: 100.00%] [G loss: 1.942987]\n",
      "epoch:5 step:4566 [D loss: 0.278757, acc.: 89.06%] [G loss: 2.819023]\n",
      "epoch:5 step:4567 [D loss: 0.142901, acc.: 93.75%] [G loss: 1.544401]\n",
      "epoch:5 step:4568 [D loss: 0.079397, acc.: 98.44%] [G loss: 1.738696]\n",
      "epoch:5 step:4569 [D loss: 0.036934, acc.: 100.00%] [G loss: 1.808006]\n",
      "epoch:5 step:4570 [D loss: 0.911891, acc.: 54.69%] [G loss: 4.221231]\n",
      "epoch:5 step:4571 [D loss: 0.155937, acc.: 92.19%] [G loss: 5.252616]\n",
      "epoch:5 step:4572 [D loss: 0.107237, acc.: 95.31%] [G loss: 2.973304]\n",
      "epoch:5 step:4573 [D loss: 0.088572, acc.: 99.22%] [G loss: 0.960991]\n",
      "epoch:5 step:4574 [D loss: 0.057021, acc.: 98.44%] [G loss: 0.645336]\n",
      "epoch:5 step:4575 [D loss: 0.208659, acc.: 91.41%] [G loss: 2.127696]\n",
      "epoch:5 step:4576 [D loss: 0.077414, acc.: 97.66%] [G loss: 2.210871]\n",
      "epoch:5 step:4577 [D loss: 0.078914, acc.: 97.66%] [G loss: 1.235578]\n",
      "epoch:5 step:4578 [D loss: 0.117427, acc.: 96.88%] [G loss: 0.844534]\n",
      "epoch:5 step:4579 [D loss: 0.538253, acc.: 73.44%] [G loss: 4.604196]\n",
      "epoch:5 step:4580 [D loss: 0.697785, acc.: 63.28%] [G loss: 1.735770]\n",
      "epoch:5 step:4581 [D loss: 0.098274, acc.: 98.44%] [G loss: 2.192886]\n",
      "epoch:5 step:4582 [D loss: 0.114240, acc.: 96.88%] [G loss: 2.138676]\n",
      "epoch:5 step:4583 [D loss: 0.371024, acc.: 80.47%] [G loss: 5.933957]\n",
      "epoch:5 step:4584 [D loss: 0.191854, acc.: 92.19%] [G loss: 5.019327]\n",
      "epoch:5 step:4585 [D loss: 1.023396, acc.: 44.53%] [G loss: 4.943614]\n",
      "epoch:5 step:4586 [D loss: 0.035985, acc.: 100.00%] [G loss: 4.767705]\n",
      "epoch:5 step:4587 [D loss: 0.351104, acc.: 84.38%] [G loss: 2.832470]\n",
      "epoch:5 step:4588 [D loss: 0.232668, acc.: 91.41%] [G loss: 4.408749]\n",
      "epoch:5 step:4589 [D loss: 0.047174, acc.: 99.22%] [G loss: 3.994580]\n",
      "epoch:5 step:4590 [D loss: 0.456819, acc.: 75.78%] [G loss: 4.110011]\n",
      "epoch:5 step:4591 [D loss: 0.106493, acc.: 97.66%] [G loss: 3.970309]\n",
      "epoch:5 step:4592 [D loss: 0.125398, acc.: 96.09%] [G loss: 4.000228]\n",
      "epoch:5 step:4593 [D loss: 0.172834, acc.: 92.97%] [G loss: 2.446977]\n",
      "epoch:5 step:4594 [D loss: 0.146659, acc.: 96.09%] [G loss: 2.290520]\n",
      "epoch:5 step:4595 [D loss: 0.044408, acc.: 100.00%] [G loss: 2.039880]\n",
      "epoch:5 step:4596 [D loss: 0.209918, acc.: 93.75%] [G loss: 3.106745]\n",
      "epoch:5 step:4597 [D loss: 0.244106, acc.: 89.84%] [G loss: 1.247734]\n",
      "epoch:5 step:4598 [D loss: 0.099827, acc.: 97.66%] [G loss: 1.490644]\n",
      "epoch:5 step:4599 [D loss: 0.125559, acc.: 98.44%] [G loss: 2.266454]\n",
      "epoch:5 step:4600 [D loss: 0.091174, acc.: 97.66%] [G loss: 3.010072]\n",
      "epoch:5 step:4601 [D loss: 0.106542, acc.: 97.66%] [G loss: 0.879249]\n",
      "epoch:5 step:4602 [D loss: 0.258312, acc.: 90.62%] [G loss: 4.845932]\n",
      "epoch:5 step:4603 [D loss: 2.339225, acc.: 15.62%] [G loss: 7.938140]\n",
      "epoch:5 step:4604 [D loss: 1.053502, acc.: 58.59%] [G loss: 4.597905]\n",
      "epoch:5 step:4605 [D loss: 0.148945, acc.: 96.88%] [G loss: 4.350408]\n",
      "epoch:5 step:4606 [D loss: 0.166980, acc.: 94.53%] [G loss: 4.665004]\n",
      "epoch:5 step:4607 [D loss: 0.154796, acc.: 96.09%] [G loss: 3.779387]\n",
      "epoch:5 step:4608 [D loss: 0.130805, acc.: 96.88%] [G loss: 3.144198]\n",
      "epoch:5 step:4609 [D loss: 0.182754, acc.: 96.88%] [G loss: 3.292735]\n",
      "epoch:5 step:4610 [D loss: 0.434438, acc.: 73.44%] [G loss: 4.922412]\n",
      "epoch:5 step:4611 [D loss: 0.611225, acc.: 70.31%] [G loss: 4.185626]\n",
      "epoch:5 step:4612 [D loss: 0.213137, acc.: 92.19%] [G loss: 2.309658]\n",
      "epoch:5 step:4613 [D loss: 0.259854, acc.: 89.06%] [G loss: 4.359475]\n",
      "epoch:5 step:4614 [D loss: 0.248308, acc.: 89.84%] [G loss: 3.592312]\n",
      "epoch:5 step:4615 [D loss: 0.204877, acc.: 93.75%] [G loss: 3.407819]\n",
      "epoch:5 step:4616 [D loss: 0.076558, acc.: 99.22%] [G loss: 2.244159]\n",
      "epoch:5 step:4617 [D loss: 0.114889, acc.: 98.44%] [G loss: 1.811472]\n",
      "epoch:5 step:4618 [D loss: 0.560517, acc.: 72.66%] [G loss: 3.292356]\n",
      "epoch:5 step:4619 [D loss: 0.077071, acc.: 98.44%] [G loss: 4.406170]\n",
      "epoch:5 step:4620 [D loss: 0.737218, acc.: 60.16%] [G loss: 1.956806]\n",
      "epoch:5 step:4621 [D loss: 0.088068, acc.: 98.44%] [G loss: 2.330701]\n",
      "epoch:5 step:4622 [D loss: 0.089116, acc.: 99.22%] [G loss: 2.549664]\n",
      "epoch:5 step:4623 [D loss: 0.154192, acc.: 96.88%] [G loss: 1.715482]\n",
      "epoch:5 step:4624 [D loss: 0.164113, acc.: 94.53%] [G loss: 1.428413]\n",
      "epoch:5 step:4625 [D loss: 0.126395, acc.: 96.88%] [G loss: 2.436570]\n",
      "epoch:5 step:4626 [D loss: 0.210169, acc.: 92.19%] [G loss: 2.861156]\n",
      "epoch:5 step:4627 [D loss: 0.261222, acc.: 89.84%] [G loss: 1.672320]\n",
      "epoch:5 step:4628 [D loss: 0.131968, acc.: 97.66%] [G loss: 3.391703]\n",
      "epoch:5 step:4629 [D loss: 0.284692, acc.: 91.41%] [G loss: 2.132102]\n",
      "epoch:5 step:4630 [D loss: 0.591452, acc.: 67.97%] [G loss: 5.119054]\n",
      "epoch:5 step:4631 [D loss: 0.332269, acc.: 84.38%] [G loss: 4.437081]\n",
      "epoch:5 step:4632 [D loss: 0.187223, acc.: 92.19%] [G loss: 2.896720]\n",
      "epoch:5 step:4633 [D loss: 0.279433, acc.: 85.16%] [G loss: 5.499089]\n",
      "epoch:5 step:4634 [D loss: 0.284736, acc.: 88.28%] [G loss: 5.584352]\n",
      "epoch:5 step:4635 [D loss: 0.106586, acc.: 100.00%] [G loss: 4.187409]\n",
      "epoch:5 step:4636 [D loss: 0.121702, acc.: 97.66%] [G loss: 2.786925]\n",
      "epoch:5 step:4637 [D loss: 0.106651, acc.: 97.66%] [G loss: 4.567323]\n",
      "epoch:5 step:4638 [D loss: 0.564709, acc.: 70.31%] [G loss: 6.103143]\n",
      "epoch:5 step:4639 [D loss: 0.232866, acc.: 86.72%] [G loss: 3.964942]\n",
      "epoch:5 step:4640 [D loss: 0.409278, acc.: 80.47%] [G loss: 3.577871]\n",
      "epoch:5 step:4641 [D loss: 0.085514, acc.: 97.66%] [G loss: 2.005574]\n",
      "epoch:5 step:4642 [D loss: 0.044035, acc.: 100.00%] [G loss: 1.575238]\n",
      "epoch:5 step:4643 [D loss: 0.048959, acc.: 100.00%] [G loss: 0.829846]\n",
      "epoch:5 step:4644 [D loss: 0.152245, acc.: 96.09%] [G loss: 2.015955]\n",
      "epoch:5 step:4645 [D loss: 0.359021, acc.: 83.59%] [G loss: 1.355875]\n",
      "epoch:5 step:4646 [D loss: 0.089341, acc.: 99.22%] [G loss: 2.755873]\n",
      "epoch:5 step:4647 [D loss: 0.061210, acc.: 97.66%] [G loss: 1.543901]\n",
      "epoch:5 step:4648 [D loss: 0.073479, acc.: 99.22%] [G loss: 0.412117]\n",
      "epoch:5 step:4649 [D loss: 0.365940, acc.: 79.69%] [G loss: 5.173625]\n",
      "epoch:5 step:4650 [D loss: 0.820009, acc.: 57.03%] [G loss: 0.797754]\n",
      "epoch:5 step:4651 [D loss: 0.969885, acc.: 59.38%] [G loss: 6.002327]\n",
      "epoch:5 step:4652 [D loss: 0.867523, acc.: 58.59%] [G loss: 3.962379]\n",
      "epoch:5 step:4653 [D loss: 0.093145, acc.: 96.88%] [G loss: 2.987814]\n",
      "epoch:5 step:4654 [D loss: 0.068215, acc.: 99.22%] [G loss: 2.900172]\n",
      "epoch:5 step:4655 [D loss: 0.166765, acc.: 94.53%] [G loss: 3.252129]\n",
      "epoch:5 step:4656 [D loss: 0.328424, acc.: 85.94%] [G loss: 3.073226]\n",
      "epoch:5 step:4657 [D loss: 0.080091, acc.: 97.66%] [G loss: 3.114242]\n",
      "epoch:5 step:4658 [D loss: 0.386521, acc.: 82.81%] [G loss: 3.945302]\n",
      "epoch:5 step:4659 [D loss: 0.350865, acc.: 84.38%] [G loss: 1.766585]\n",
      "epoch:5 step:4660 [D loss: 0.218184, acc.: 90.62%] [G loss: 2.245255]\n",
      "epoch:5 step:4661 [D loss: 0.025059, acc.: 100.00%] [G loss: 2.678647]\n",
      "epoch:5 step:4662 [D loss: 0.103650, acc.: 97.66%] [G loss: 1.646709]\n",
      "epoch:5 step:4663 [D loss: 0.098325, acc.: 99.22%] [G loss: 0.769266]\n",
      "epoch:5 step:4664 [D loss: 0.058558, acc.: 100.00%] [G loss: 0.478723]\n",
      "epoch:5 step:4665 [D loss: 0.051998, acc.: 100.00%] [G loss: 0.442122]\n",
      "epoch:5 step:4666 [D loss: 0.122717, acc.: 97.66%] [G loss: 1.386457]\n",
      "epoch:5 step:4667 [D loss: 0.399531, acc.: 82.81%] [G loss: 0.143776]\n",
      "epoch:5 step:4668 [D loss: 0.361931, acc.: 84.38%] [G loss: 1.835268]\n",
      "epoch:5 step:4669 [D loss: 0.042010, acc.: 99.22%] [G loss: 2.142344]\n",
      "epoch:5 step:4670 [D loss: 0.530165, acc.: 73.44%] [G loss: 0.185371]\n",
      "epoch:5 step:4671 [D loss: 0.566766, acc.: 71.88%] [G loss: 2.218317]\n",
      "epoch:5 step:4672 [D loss: 0.038474, acc.: 99.22%] [G loss: 3.936841]\n",
      "epoch:5 step:4673 [D loss: 0.605079, acc.: 72.66%] [G loss: 0.449434]\n",
      "epoch:5 step:4674 [D loss: 0.340201, acc.: 81.25%] [G loss: 1.820531]\n",
      "epoch:5 step:4675 [D loss: 0.060053, acc.: 100.00%] [G loss: 2.356528]\n",
      "epoch:5 step:4676 [D loss: 0.396995, acc.: 81.25%] [G loss: 1.279078]\n",
      "epoch:5 step:4677 [D loss: 0.319722, acc.: 83.59%] [G loss: 4.129738]\n",
      "epoch:5 step:4678 [D loss: 0.297948, acc.: 86.72%] [G loss: 3.375079]\n",
      "epoch:5 step:4679 [D loss: 0.121712, acc.: 96.09%] [G loss: 3.237587]\n",
      "epoch:5 step:4680 [D loss: 0.204710, acc.: 92.97%] [G loss: 3.672911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5 step:4681 [D loss: 0.206999, acc.: 92.97%] [G loss: 2.513299]\n",
      "epoch:5 step:4682 [D loss: 0.384002, acc.: 79.69%] [G loss: 4.277649]\n",
      "epoch:5 step:4683 [D loss: 0.245790, acc.: 86.72%] [G loss: 2.974831]\n",
      "epoch:5 step:4684 [D loss: 0.318288, acc.: 90.62%] [G loss: 1.763682]\n",
      "epoch:5 step:4685 [D loss: 0.105670, acc.: 98.44%] [G loss: 1.665148]\n",
      "epoch:5 step:4686 [D loss: 0.060935, acc.: 100.00%] [G loss: 1.919966]\n",
      "epoch:6 step:4687 [D loss: 0.128527, acc.: 97.66%] [G loss: 1.531131]\n",
      "epoch:6 step:4688 [D loss: 0.183068, acc.: 92.97%] [G loss: 4.378469]\n",
      "epoch:6 step:4689 [D loss: 0.380129, acc.: 80.47%] [G loss: 1.171683]\n",
      "epoch:6 step:4690 [D loss: 0.077435, acc.: 98.44%] [G loss: 2.013197]\n",
      "epoch:6 step:4691 [D loss: 0.213701, acc.: 95.31%] [G loss: 1.711593]\n",
      "epoch:6 step:4692 [D loss: 0.077900, acc.: 98.44%] [G loss: 0.940201]\n",
      "epoch:6 step:4693 [D loss: 0.268608, acc.: 87.50%] [G loss: 3.358208]\n",
      "epoch:6 step:4694 [D loss: 0.090464, acc.: 97.66%] [G loss: 3.152220]\n",
      "epoch:6 step:4695 [D loss: 0.888667, acc.: 51.56%] [G loss: 4.569034]\n",
      "epoch:6 step:4696 [D loss: 0.214388, acc.: 89.06%] [G loss: 4.731544]\n",
      "epoch:6 step:4697 [D loss: 0.207045, acc.: 89.84%] [G loss: 1.480852]\n",
      "epoch:6 step:4698 [D loss: 0.476250, acc.: 77.34%] [G loss: 4.347379]\n",
      "epoch:6 step:4699 [D loss: 0.149017, acc.: 92.97%] [G loss: 4.534931]\n",
      "epoch:6 step:4700 [D loss: 0.236826, acc.: 88.28%] [G loss: 1.314758]\n",
      "epoch:6 step:4701 [D loss: 0.328852, acc.: 84.38%] [G loss: 3.227831]\n",
      "epoch:6 step:4702 [D loss: 0.113117, acc.: 95.31%] [G loss: 2.609437]\n",
      "epoch:6 step:4703 [D loss: 0.187315, acc.: 93.75%] [G loss: 3.429020]\n",
      "epoch:6 step:4704 [D loss: 0.187840, acc.: 92.97%] [G loss: 3.419909]\n",
      "epoch:6 step:4705 [D loss: 0.207614, acc.: 92.19%] [G loss: 4.011205]\n",
      "epoch:6 step:4706 [D loss: 0.301750, acc.: 87.50%] [G loss: 2.196302]\n",
      "epoch:6 step:4707 [D loss: 0.212496, acc.: 93.75%] [G loss: 3.526259]\n",
      "epoch:6 step:4708 [D loss: 0.076382, acc.: 98.44%] [G loss: 3.799302]\n",
      "epoch:6 step:4709 [D loss: 0.105830, acc.: 98.44%] [G loss: 3.355015]\n",
      "epoch:6 step:4710 [D loss: 0.261727, acc.: 90.62%] [G loss: 3.777849]\n",
      "epoch:6 step:4711 [D loss: 0.159045, acc.: 96.09%] [G loss: 3.991413]\n",
      "epoch:6 step:4712 [D loss: 0.282297, acc.: 87.50%] [G loss: 5.268044]\n",
      "epoch:6 step:4713 [D loss: 0.162503, acc.: 93.75%] [G loss: 3.573124]\n",
      "epoch:6 step:4714 [D loss: 0.312812, acc.: 85.94%] [G loss: 5.037450]\n",
      "epoch:6 step:4715 [D loss: 0.185041, acc.: 92.19%] [G loss: 2.605003]\n",
      "epoch:6 step:4716 [D loss: 0.070529, acc.: 98.44%] [G loss: 1.602674]\n",
      "epoch:6 step:4717 [D loss: 0.102807, acc.: 97.66%] [G loss: 0.954011]\n",
      "epoch:6 step:4718 [D loss: 0.115602, acc.: 97.66%] [G loss: 0.778439]\n",
      "epoch:6 step:4719 [D loss: 0.098957, acc.: 97.66%] [G loss: 1.883375]\n",
      "epoch:6 step:4720 [D loss: 0.168264, acc.: 92.97%] [G loss: 2.648568]\n",
      "epoch:6 step:4721 [D loss: 2.738146, acc.: 17.97%] [G loss: 5.978295]\n",
      "epoch:6 step:4722 [D loss: 0.694801, acc.: 65.62%] [G loss: 3.724861]\n",
      "epoch:6 step:4723 [D loss: 0.738146, acc.: 65.62%] [G loss: 3.577679]\n",
      "epoch:6 step:4724 [D loss: 0.353243, acc.: 85.16%] [G loss: 3.827802]\n",
      "epoch:6 step:4725 [D loss: 0.112339, acc.: 98.44%] [G loss: 3.784757]\n",
      "epoch:6 step:4726 [D loss: 0.187214, acc.: 96.09%] [G loss: 3.367881]\n",
      "epoch:6 step:4727 [D loss: 0.244669, acc.: 90.62%] [G loss: 2.641486]\n",
      "epoch:6 step:4728 [D loss: 0.164285, acc.: 96.88%] [G loss: 2.823989]\n",
      "epoch:6 step:4729 [D loss: 0.598105, acc.: 71.09%] [G loss: 4.334608]\n",
      "epoch:6 step:4730 [D loss: 0.303181, acc.: 85.94%] [G loss: 2.643061]\n",
      "epoch:6 step:4731 [D loss: 0.410732, acc.: 79.69%] [G loss: 4.476838]\n",
      "epoch:6 step:4732 [D loss: 0.431388, acc.: 76.56%] [G loss: 2.788378]\n",
      "epoch:6 step:4733 [D loss: 0.339584, acc.: 85.16%] [G loss: 4.141259]\n",
      "epoch:6 step:4734 [D loss: 0.214914, acc.: 87.50%] [G loss: 3.611287]\n",
      "epoch:6 step:4735 [D loss: 1.135178, acc.: 35.94%] [G loss: 2.762295]\n",
      "epoch:6 step:4736 [D loss: 0.213178, acc.: 96.09%] [G loss: 3.691090]\n",
      "epoch:6 step:4737 [D loss: 0.096120, acc.: 96.88%] [G loss: 3.164006]\n",
      "epoch:6 step:4738 [D loss: 0.196108, acc.: 93.75%] [G loss: 3.920307]\n",
      "epoch:6 step:4739 [D loss: 0.242332, acc.: 89.84%] [G loss: 3.038190]\n",
      "epoch:6 step:4740 [D loss: 0.572387, acc.: 71.09%] [G loss: 3.933659]\n",
      "epoch:6 step:4741 [D loss: 0.105363, acc.: 96.09%] [G loss: 3.593115]\n",
      "epoch:6 step:4742 [D loss: 0.187859, acc.: 95.31%] [G loss: 2.238402]\n",
      "epoch:6 step:4743 [D loss: 0.490398, acc.: 73.44%] [G loss: 3.655750]\n",
      "epoch:6 step:4744 [D loss: 0.210465, acc.: 93.75%] [G loss: 3.256136]\n",
      "epoch:6 step:4745 [D loss: 0.204048, acc.: 93.75%] [G loss: 4.205027]\n",
      "epoch:6 step:4746 [D loss: 0.232953, acc.: 91.41%] [G loss: 3.440025]\n",
      "epoch:6 step:4747 [D loss: 0.415658, acc.: 80.47%] [G loss: 3.345459]\n",
      "epoch:6 step:4748 [D loss: 0.420912, acc.: 80.47%] [G loss: 3.820225]\n",
      "epoch:6 step:4749 [D loss: 0.159618, acc.: 95.31%] [G loss: 3.500171]\n",
      "epoch:6 step:4750 [D loss: 0.266046, acc.: 92.19%] [G loss: 3.520437]\n",
      "epoch:6 step:4751 [D loss: 0.220506, acc.: 92.19%] [G loss: 2.462111]\n",
      "epoch:6 step:4752 [D loss: 0.154708, acc.: 96.88%] [G loss: 2.199918]\n",
      "epoch:6 step:4753 [D loss: 0.158740, acc.: 97.66%] [G loss: 1.753412]\n",
      "epoch:6 step:4754 [D loss: 0.107563, acc.: 95.31%] [G loss: 2.940443]\n",
      "epoch:6 step:4755 [D loss: 0.078849, acc.: 99.22%] [G loss: 1.139880]\n",
      "epoch:6 step:4756 [D loss: 0.114912, acc.: 99.22%] [G loss: 1.085782]\n",
      "epoch:6 step:4757 [D loss: 0.164175, acc.: 94.53%] [G loss: 0.228517]\n",
      "epoch:6 step:4758 [D loss: 0.069787, acc.: 98.44%] [G loss: 0.368298]\n",
      "epoch:6 step:4759 [D loss: 0.043406, acc.: 99.22%] [G loss: 0.599077]\n",
      "epoch:6 step:4760 [D loss: 0.018569, acc.: 100.00%] [G loss: 0.264495]\n",
      "epoch:6 step:4761 [D loss: 0.030891, acc.: 100.00%] [G loss: 0.678820]\n",
      "epoch:6 step:4762 [D loss: 0.085028, acc.: 98.44%] [G loss: 0.116996]\n",
      "epoch:6 step:4763 [D loss: 0.214846, acc.: 93.75%] [G loss: 0.444913]\n",
      "epoch:6 step:4764 [D loss: 0.029749, acc.: 100.00%] [G loss: 0.839156]\n",
      "epoch:6 step:4765 [D loss: 0.176379, acc.: 93.75%] [G loss: 0.157108]\n",
      "epoch:6 step:4766 [D loss: 0.158945, acc.: 92.97%] [G loss: 1.040877]\n",
      "epoch:6 step:4767 [D loss: 0.016910, acc.: 100.00%] [G loss: 1.806482]\n",
      "epoch:6 step:4768 [D loss: 0.064871, acc.: 97.66%] [G loss: 1.363838]\n",
      "epoch:6 step:4769 [D loss: 0.044546, acc.: 100.00%] [G loss: 0.475524]\n",
      "epoch:6 step:4770 [D loss: 0.171680, acc.: 93.75%] [G loss: 4.350291]\n",
      "epoch:6 step:4771 [D loss: 0.189894, acc.: 90.62%] [G loss: 2.453728]\n",
      "epoch:6 step:4772 [D loss: 0.760984, acc.: 64.06%] [G loss: 8.456242]\n",
      "epoch:6 step:4773 [D loss: 1.209347, acc.: 55.47%] [G loss: 3.638356]\n",
      "epoch:6 step:4774 [D loss: 0.271137, acc.: 87.50%] [G loss: 6.444374]\n",
      "epoch:6 step:4775 [D loss: 0.482495, acc.: 77.34%] [G loss: 4.055641]\n",
      "epoch:6 step:4776 [D loss: 0.424026, acc.: 81.25%] [G loss: 5.737715]\n",
      "epoch:6 step:4777 [D loss: 0.067699, acc.: 97.66%] [G loss: 6.182510]\n",
      "epoch:6 step:4778 [D loss: 0.134771, acc.: 96.88%] [G loss: 2.855972]\n",
      "epoch:6 step:4779 [D loss: 0.046843, acc.: 98.44%] [G loss: 1.774833]\n",
      "epoch:6 step:4780 [D loss: 0.089761, acc.: 97.66%] [G loss: 0.799826]\n",
      "epoch:6 step:4781 [D loss: 0.139104, acc.: 96.09%] [G loss: 1.389936]\n",
      "epoch:6 step:4782 [D loss: 0.075464, acc.: 98.44%] [G loss: 1.805577]\n",
      "epoch:6 step:4783 [D loss: 0.037823, acc.: 100.00%] [G loss: 0.281594]\n",
      "epoch:6 step:4784 [D loss: 0.069868, acc.: 99.22%] [G loss: 0.366102]\n",
      "epoch:6 step:4785 [D loss: 0.025139, acc.: 100.00%] [G loss: 0.244571]\n",
      "epoch:6 step:4786 [D loss: 0.080819, acc.: 98.44%] [G loss: 0.091095]\n",
      "epoch:6 step:4787 [D loss: 0.653658, acc.: 67.19%] [G loss: 7.881612]\n",
      "epoch:6 step:4788 [D loss: 2.821682, acc.: 50.00%] [G loss: 3.305397]\n",
      "epoch:6 step:4789 [D loss: 0.113538, acc.: 97.66%] [G loss: 1.245251]\n",
      "epoch:6 step:4790 [D loss: 0.269477, acc.: 90.62%] [G loss: 0.625626]\n",
      "epoch:6 step:4791 [D loss: 0.291769, acc.: 86.72%] [G loss: 2.457594]\n",
      "epoch:6 step:4792 [D loss: 0.097631, acc.: 98.44%] [G loss: 3.381702]\n",
      "epoch:6 step:4793 [D loss: 0.268575, acc.: 83.59%] [G loss: 1.420043]\n",
      "epoch:6 step:4794 [D loss: 0.203044, acc.: 94.53%] [G loss: 1.085676]\n",
      "epoch:6 step:4795 [D loss: 0.351413, acc.: 87.50%] [G loss: 3.808159]\n",
      "epoch:6 step:4796 [D loss: 0.657634, acc.: 65.62%] [G loss: 2.452276]\n",
      "epoch:6 step:4797 [D loss: 0.103716, acc.: 99.22%] [G loss: 1.695912]\n",
      "epoch:6 step:4798 [D loss: 0.250693, acc.: 91.41%] [G loss: 2.642021]\n",
      "epoch:6 step:4799 [D loss: 0.311734, acc.: 89.84%] [G loss: 3.069452]\n",
      "epoch:6 step:4800 [D loss: 0.213275, acc.: 92.19%] [G loss: 2.925513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4801 [D loss: 0.132917, acc.: 95.31%] [G loss: 3.003261]\n",
      "epoch:6 step:4802 [D loss: 0.221644, acc.: 92.97%] [G loss: 3.234669]\n",
      "epoch:6 step:4803 [D loss: 0.782716, acc.: 57.03%] [G loss: 3.959192]\n",
      "epoch:6 step:4804 [D loss: 0.210555, acc.: 90.62%] [G loss: 3.917033]\n",
      "epoch:6 step:4805 [D loss: 0.135937, acc.: 96.88%] [G loss: 2.907979]\n",
      "epoch:6 step:4806 [D loss: 0.096694, acc.: 98.44%] [G loss: 2.519563]\n",
      "epoch:6 step:4807 [D loss: 0.168519, acc.: 93.75%] [G loss: 3.283678]\n",
      "epoch:6 step:4808 [D loss: 0.254232, acc.: 88.28%] [G loss: 1.607890]\n",
      "epoch:6 step:4809 [D loss: 0.716974, acc.: 59.38%] [G loss: 5.254324]\n",
      "epoch:6 step:4810 [D loss: 0.980106, acc.: 58.59%] [G loss: 1.693229]\n",
      "epoch:6 step:4811 [D loss: 0.202142, acc.: 91.41%] [G loss: 2.706083]\n",
      "epoch:6 step:4812 [D loss: 0.068517, acc.: 99.22%] [G loss: 3.314936]\n",
      "epoch:6 step:4813 [D loss: 0.098516, acc.: 100.00%] [G loss: 2.411015]\n",
      "epoch:6 step:4814 [D loss: 0.241054, acc.: 90.62%] [G loss: 2.100425]\n",
      "epoch:6 step:4815 [D loss: 0.148240, acc.: 96.88%] [G loss: 2.691632]\n",
      "epoch:6 step:4816 [D loss: 0.169349, acc.: 93.75%] [G loss: 2.816125]\n",
      "epoch:6 step:4817 [D loss: 0.119895, acc.: 97.66%] [G loss: 2.897462]\n",
      "epoch:6 step:4818 [D loss: 0.324453, acc.: 89.84%] [G loss: 2.938734]\n",
      "epoch:6 step:4819 [D loss: 0.102256, acc.: 99.22%] [G loss: 2.646590]\n",
      "epoch:6 step:4820 [D loss: 0.144289, acc.: 95.31%] [G loss: 1.881431]\n",
      "epoch:6 step:4821 [D loss: 0.157634, acc.: 96.88%] [G loss: 2.094660]\n",
      "epoch:6 step:4822 [D loss: 0.097907, acc.: 98.44%] [G loss: 2.322816]\n",
      "epoch:6 step:4823 [D loss: 0.145927, acc.: 95.31%] [G loss: 1.783059]\n",
      "epoch:6 step:4824 [D loss: 0.414492, acc.: 77.34%] [G loss: 4.334673]\n",
      "epoch:6 step:4825 [D loss: 0.616447, acc.: 69.53%] [G loss: 2.231696]\n",
      "epoch:6 step:4826 [D loss: 0.116953, acc.: 98.44%] [G loss: 2.280299]\n",
      "epoch:6 step:4827 [D loss: 0.269927, acc.: 85.94%] [G loss: 3.926548]\n",
      "epoch:6 step:4828 [D loss: 0.416135, acc.: 82.81%] [G loss: 3.195586]\n",
      "epoch:6 step:4829 [D loss: 0.193631, acc.: 96.09%] [G loss: 3.524709]\n",
      "epoch:6 step:4830 [D loss: 0.169590, acc.: 96.09%] [G loss: 2.717764]\n",
      "epoch:6 step:4831 [D loss: 0.167278, acc.: 95.31%] [G loss: 3.319183]\n",
      "epoch:6 step:4832 [D loss: 0.108486, acc.: 97.66%] [G loss: 2.316241]\n",
      "epoch:6 step:4833 [D loss: 1.159046, acc.: 48.44%] [G loss: 5.976745]\n",
      "epoch:6 step:4834 [D loss: 1.426181, acc.: 52.34%] [G loss: 3.154716]\n",
      "epoch:6 step:4835 [D loss: 0.190279, acc.: 96.88%] [G loss: 2.253388]\n",
      "epoch:6 step:4836 [D loss: 0.213298, acc.: 93.75%] [G loss: 2.768447]\n",
      "epoch:6 step:4837 [D loss: 0.183517, acc.: 92.97%] [G loss: 3.695431]\n",
      "epoch:6 step:4838 [D loss: 0.181407, acc.: 94.53%] [G loss: 2.593482]\n",
      "epoch:6 step:4839 [D loss: 0.411362, acc.: 82.03%] [G loss: 4.044246]\n",
      "epoch:6 step:4840 [D loss: 0.103835, acc.: 98.44%] [G loss: 2.989600]\n",
      "epoch:6 step:4841 [D loss: 0.157374, acc.: 96.88%] [G loss: 2.606235]\n",
      "epoch:6 step:4842 [D loss: 0.631193, acc.: 67.97%] [G loss: 5.057651]\n",
      "epoch:6 step:4843 [D loss: 0.480233, acc.: 75.00%] [G loss: 3.357922]\n",
      "epoch:6 step:4844 [D loss: 0.625114, acc.: 68.75%] [G loss: 3.560097]\n",
      "epoch:6 step:4845 [D loss: 0.093523, acc.: 97.66%] [G loss: 4.055530]\n",
      "epoch:6 step:4846 [D loss: 0.398343, acc.: 78.91%] [G loss: 2.456024]\n",
      "epoch:6 step:4847 [D loss: 0.211241, acc.: 92.97%] [G loss: 3.633982]\n",
      "epoch:6 step:4848 [D loss: 0.055266, acc.: 100.00%] [G loss: 3.996111]\n",
      "epoch:6 step:4849 [D loss: 0.103739, acc.: 97.66%] [G loss: 3.136300]\n",
      "epoch:6 step:4850 [D loss: 0.148358, acc.: 97.66%] [G loss: 3.015173]\n",
      "epoch:6 step:4851 [D loss: 0.325146, acc.: 83.59%] [G loss: 3.482107]\n",
      "epoch:6 step:4852 [D loss: 0.175307, acc.: 93.75%] [G loss: 3.859700]\n",
      "epoch:6 step:4853 [D loss: 0.204164, acc.: 95.31%] [G loss: 3.575583]\n",
      "epoch:6 step:4854 [D loss: 0.249939, acc.: 92.97%] [G loss: 3.960333]\n",
      "epoch:6 step:4855 [D loss: 0.103169, acc.: 97.66%] [G loss: 3.415452]\n",
      "epoch:6 step:4856 [D loss: 0.340608, acc.: 85.94%] [G loss: 3.375004]\n",
      "epoch:6 step:4857 [D loss: 0.359574, acc.: 82.81%] [G loss: 3.313631]\n",
      "epoch:6 step:4858 [D loss: 0.051239, acc.: 100.00%] [G loss: 3.392774]\n",
      "epoch:6 step:4859 [D loss: 0.435719, acc.: 83.59%] [G loss: 3.603741]\n",
      "epoch:6 step:4860 [D loss: 0.172745, acc.: 94.53%] [G loss: 3.652724]\n",
      "epoch:6 step:4861 [D loss: 0.085705, acc.: 98.44%] [G loss: 3.949045]\n",
      "epoch:6 step:4862 [D loss: 0.333642, acc.: 85.94%] [G loss: 0.633919]\n",
      "epoch:6 step:4863 [D loss: 0.381188, acc.: 81.25%] [G loss: 3.911999]\n",
      "epoch:6 step:4864 [D loss: 0.168083, acc.: 92.19%] [G loss: 4.296429]\n",
      "epoch:6 step:4865 [D loss: 0.312915, acc.: 87.50%] [G loss: 2.213077]\n",
      "epoch:6 step:4866 [D loss: 0.057335, acc.: 98.44%] [G loss: 1.633235]\n",
      "epoch:6 step:4867 [D loss: 0.186311, acc.: 96.09%] [G loss: 0.935308]\n",
      "epoch:6 step:4868 [D loss: 0.018644, acc.: 100.00%] [G loss: 0.535932]\n",
      "epoch:6 step:4869 [D loss: 0.265583, acc.: 91.41%] [G loss: 0.325457]\n",
      "epoch:6 step:4870 [D loss: 0.391864, acc.: 82.03%] [G loss: 4.233332]\n",
      "epoch:6 step:4871 [D loss: 1.271466, acc.: 53.12%] [G loss: 1.144154]\n",
      "epoch:6 step:4872 [D loss: 0.241838, acc.: 92.19%] [G loss: 2.545696]\n",
      "epoch:6 step:4873 [D loss: 0.058189, acc.: 100.00%] [G loss: 2.840088]\n",
      "epoch:6 step:4874 [D loss: 0.171960, acc.: 94.53%] [G loss: 1.529197]\n",
      "epoch:6 step:4875 [D loss: 0.110571, acc.: 99.22%] [G loss: 1.963905]\n",
      "epoch:6 step:4876 [D loss: 0.178326, acc.: 93.75%] [G loss: 3.200215]\n",
      "epoch:6 step:4877 [D loss: 0.280056, acc.: 89.06%] [G loss: 3.583732]\n",
      "epoch:6 step:4878 [D loss: 0.637533, acc.: 67.19%] [G loss: 1.992005]\n",
      "epoch:6 step:4879 [D loss: 0.247978, acc.: 89.84%] [G loss: 4.281879]\n",
      "epoch:6 step:4880 [D loss: 0.166157, acc.: 96.09%] [G loss: 3.198324]\n",
      "epoch:6 step:4881 [D loss: 0.236768, acc.: 89.84%] [G loss: 4.278048]\n",
      "epoch:6 step:4882 [D loss: 0.419905, acc.: 77.34%] [G loss: 2.782423]\n",
      "epoch:6 step:4883 [D loss: 0.070834, acc.: 98.44%] [G loss: 2.784216]\n",
      "epoch:6 step:4884 [D loss: 0.084080, acc.: 98.44%] [G loss: 1.281288]\n",
      "epoch:6 step:4885 [D loss: 0.077700, acc.: 99.22%] [G loss: 1.147569]\n",
      "epoch:6 step:4886 [D loss: 0.060725, acc.: 100.00%] [G loss: 0.757886]\n",
      "epoch:6 step:4887 [D loss: 0.070101, acc.: 97.66%] [G loss: 0.434321]\n",
      "epoch:6 step:4888 [D loss: 0.151420, acc.: 95.31%] [G loss: 0.615071]\n",
      "epoch:6 step:4889 [D loss: 0.046480, acc.: 98.44%] [G loss: 0.954518]\n",
      "epoch:6 step:4890 [D loss: 0.046129, acc.: 100.00%] [G loss: 0.193825]\n",
      "epoch:6 step:4891 [D loss: 0.086132, acc.: 96.88%] [G loss: 0.374948]\n",
      "epoch:6 step:4892 [D loss: 0.119638, acc.: 96.88%] [G loss: 0.093209]\n",
      "epoch:6 step:4893 [D loss: 0.031620, acc.: 100.00%] [G loss: 0.144155]\n",
      "epoch:6 step:4894 [D loss: 0.281033, acc.: 87.50%] [G loss: 3.136609]\n",
      "epoch:6 step:4895 [D loss: 1.758142, acc.: 32.03%] [G loss: 2.106523]\n",
      "epoch:6 step:4896 [D loss: 0.020808, acc.: 100.00%] [G loss: 3.143144]\n",
      "epoch:6 step:4897 [D loss: 0.113361, acc.: 96.09%] [G loss: 3.311244]\n",
      "epoch:6 step:4898 [D loss: 0.015416, acc.: 100.00%] [G loss: 2.674447]\n",
      "epoch:6 step:4899 [D loss: 0.094077, acc.: 97.66%] [G loss: 1.928309]\n",
      "epoch:6 step:4900 [D loss: 0.609445, acc.: 64.84%] [G loss: 3.063539]\n",
      "epoch:6 step:4901 [D loss: 0.076821, acc.: 98.44%] [G loss: 3.441044]\n",
      "epoch:6 step:4902 [D loss: 0.220757, acc.: 90.62%] [G loss: 1.838998]\n",
      "epoch:6 step:4903 [D loss: 0.152986, acc.: 93.75%] [G loss: 2.678211]\n",
      "epoch:6 step:4904 [D loss: 0.078174, acc.: 97.66%] [G loss: 3.245527]\n",
      "epoch:6 step:4905 [D loss: 0.329535, acc.: 87.50%] [G loss: 4.203190]\n",
      "epoch:6 step:4906 [D loss: 0.717351, acc.: 65.62%] [G loss: 2.280448]\n",
      "epoch:6 step:4907 [D loss: 0.194037, acc.: 92.19%] [G loss: 3.726626]\n",
      "epoch:6 step:4908 [D loss: 0.086375, acc.: 100.00%] [G loss: 3.951499]\n",
      "epoch:6 step:4909 [D loss: 0.132340, acc.: 98.44%] [G loss: 3.025750]\n",
      "epoch:6 step:4910 [D loss: 0.151303, acc.: 94.53%] [G loss: 3.474715]\n",
      "epoch:6 step:4911 [D loss: 0.072130, acc.: 100.00%] [G loss: 3.871397]\n",
      "epoch:6 step:4912 [D loss: 0.305224, acc.: 88.28%] [G loss: 3.018213]\n",
      "epoch:6 step:4913 [D loss: 0.126449, acc.: 96.88%] [G loss: 2.658477]\n",
      "epoch:6 step:4914 [D loss: 0.227408, acc.: 92.97%] [G loss: 4.789236]\n",
      "epoch:6 step:4915 [D loss: 0.441857, acc.: 79.69%] [G loss: 1.928281]\n",
      "epoch:6 step:4916 [D loss: 0.233494, acc.: 89.84%] [G loss: 3.867878]\n",
      "epoch:6 step:4917 [D loss: 0.072673, acc.: 98.44%] [G loss: 4.008266]\n",
      "epoch:6 step:4918 [D loss: 1.383845, acc.: 38.28%] [G loss: 5.772712]\n",
      "epoch:6 step:4919 [D loss: 0.394361, acc.: 82.03%] [G loss: 4.609610]\n",
      "epoch:6 step:4920 [D loss: 0.920500, acc.: 50.00%] [G loss: 4.372903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:4921 [D loss: 0.070336, acc.: 97.66%] [G loss: 5.329222]\n",
      "epoch:6 step:4922 [D loss: 0.190792, acc.: 90.62%] [G loss: 3.570882]\n",
      "epoch:6 step:4923 [D loss: 0.076561, acc.: 98.44%] [G loss: 2.198539]\n",
      "epoch:6 step:4924 [D loss: 0.288065, acc.: 85.16%] [G loss: 3.643992]\n",
      "epoch:6 step:4925 [D loss: 0.189435, acc.: 93.75%] [G loss: 2.934001]\n",
      "epoch:6 step:4926 [D loss: 0.386787, acc.: 85.16%] [G loss: 4.074743]\n",
      "epoch:6 step:4927 [D loss: 0.280309, acc.: 86.72%] [G loss: 2.653227]\n",
      "epoch:6 step:4928 [D loss: 0.457131, acc.: 79.69%] [G loss: 3.938279]\n",
      "epoch:6 step:4929 [D loss: 0.263452, acc.: 85.94%] [G loss: 2.696793]\n",
      "epoch:6 step:4930 [D loss: 0.273715, acc.: 85.16%] [G loss: 3.930676]\n",
      "epoch:6 step:4931 [D loss: 0.240886, acc.: 89.84%] [G loss: 3.346116]\n",
      "epoch:6 step:4932 [D loss: 0.213652, acc.: 92.97%] [G loss: 2.163600]\n",
      "epoch:6 step:4933 [D loss: 0.212496, acc.: 92.97%] [G loss: 3.461102]\n",
      "epoch:6 step:4934 [D loss: 0.120094, acc.: 98.44%] [G loss: 3.448949]\n",
      "epoch:6 step:4935 [D loss: 0.390190, acc.: 80.47%] [G loss: 3.650764]\n",
      "epoch:6 step:4936 [D loss: 0.366053, acc.: 84.38%] [G loss: 3.342909]\n",
      "epoch:6 step:4937 [D loss: 0.061692, acc.: 99.22%] [G loss: 3.691980]\n",
      "epoch:6 step:4938 [D loss: 0.283730, acc.: 89.06%] [G loss: 3.831872]\n",
      "epoch:6 step:4939 [D loss: 0.152820, acc.: 95.31%] [G loss: 2.941736]\n",
      "epoch:6 step:4940 [D loss: 0.234494, acc.: 92.97%] [G loss: 2.957257]\n",
      "epoch:6 step:4941 [D loss: 0.190414, acc.: 92.19%] [G loss: 2.116173]\n",
      "epoch:6 step:4942 [D loss: 0.097298, acc.: 97.66%] [G loss: 0.944410]\n",
      "epoch:6 step:4943 [D loss: 0.046667, acc.: 99.22%] [G loss: 0.898166]\n",
      "epoch:6 step:4944 [D loss: 0.076373, acc.: 96.88%] [G loss: 0.391034]\n",
      "epoch:6 step:4945 [D loss: 0.155933, acc.: 94.53%] [G loss: 2.056459]\n",
      "epoch:6 step:4946 [D loss: 0.234265, acc.: 89.84%] [G loss: 0.625245]\n",
      "epoch:6 step:4947 [D loss: 0.263383, acc.: 91.41%] [G loss: 2.146431]\n",
      "epoch:6 step:4948 [D loss: 0.039880, acc.: 100.00%] [G loss: 2.421333]\n",
      "epoch:6 step:4949 [D loss: 0.124554, acc.: 96.88%] [G loss: 1.198380]\n",
      "epoch:6 step:4950 [D loss: 0.123862, acc.: 97.66%] [G loss: 0.213081]\n",
      "epoch:6 step:4951 [D loss: 0.398703, acc.: 80.47%] [G loss: 3.697862]\n",
      "epoch:6 step:4952 [D loss: 0.445729, acc.: 78.12%] [G loss: 2.142581]\n",
      "epoch:6 step:4953 [D loss: 0.502940, acc.: 78.12%] [G loss: 5.121803]\n",
      "epoch:6 step:4954 [D loss: 0.234402, acc.: 89.06%] [G loss: 4.632942]\n",
      "epoch:6 step:4955 [D loss: 0.623160, acc.: 73.44%] [G loss: 5.859905]\n",
      "epoch:6 step:4956 [D loss: 0.489834, acc.: 78.12%] [G loss: 4.143141]\n",
      "epoch:6 step:4957 [D loss: 0.178741, acc.: 91.41%] [G loss: 4.356178]\n",
      "epoch:6 step:4958 [D loss: 0.070477, acc.: 99.22%] [G loss: 5.152862]\n",
      "epoch:6 step:4959 [D loss: 0.152539, acc.: 95.31%] [G loss: 4.058676]\n",
      "epoch:6 step:4960 [D loss: 0.074673, acc.: 98.44%] [G loss: 3.641714]\n",
      "epoch:6 step:4961 [D loss: 0.213139, acc.: 93.75%] [G loss: 3.974437]\n",
      "epoch:6 step:4962 [D loss: 0.176664, acc.: 96.88%] [G loss: 2.320104]\n",
      "epoch:6 step:4963 [D loss: 0.352563, acc.: 84.38%] [G loss: 5.653983]\n",
      "epoch:6 step:4964 [D loss: 0.462222, acc.: 77.34%] [G loss: 1.190016]\n",
      "epoch:6 step:4965 [D loss: 0.539419, acc.: 74.22%] [G loss: 5.998928]\n",
      "epoch:6 step:4966 [D loss: 0.370360, acc.: 82.03%] [G loss: 3.723605]\n",
      "epoch:6 step:4967 [D loss: 0.157275, acc.: 95.31%] [G loss: 3.186584]\n",
      "epoch:6 step:4968 [D loss: 0.145949, acc.: 92.97%] [G loss: 3.369181]\n",
      "epoch:6 step:4969 [D loss: 0.172826, acc.: 91.41%] [G loss: 3.967300]\n",
      "epoch:6 step:4970 [D loss: 0.163305, acc.: 94.53%] [G loss: 3.428176]\n",
      "epoch:6 step:4971 [D loss: 0.385194, acc.: 81.25%] [G loss: 4.788045]\n",
      "epoch:6 step:4972 [D loss: 0.293387, acc.: 87.50%] [G loss: 2.759068]\n",
      "epoch:6 step:4973 [D loss: 0.217563, acc.: 94.53%] [G loss: 3.804521]\n",
      "epoch:6 step:4974 [D loss: 0.196652, acc.: 94.53%] [G loss: 2.683094]\n",
      "epoch:6 step:4975 [D loss: 0.213999, acc.: 93.75%] [G loss: 3.113661]\n",
      "epoch:6 step:4976 [D loss: 0.056861, acc.: 99.22%] [G loss: 3.426367]\n",
      "epoch:6 step:4977 [D loss: 0.265437, acc.: 88.28%] [G loss: 4.452575]\n",
      "epoch:6 step:4978 [D loss: 0.661658, acc.: 68.75%] [G loss: 4.825981]\n",
      "epoch:6 step:4979 [D loss: 0.075564, acc.: 98.44%] [G loss: 4.913875]\n",
      "epoch:6 step:4980 [D loss: 0.265213, acc.: 86.72%] [G loss: 4.500498]\n",
      "epoch:6 step:4981 [D loss: 0.361848, acc.: 84.38%] [G loss: 2.739574]\n",
      "epoch:6 step:4982 [D loss: 0.121133, acc.: 96.09%] [G loss: 3.644707]\n",
      "epoch:6 step:4983 [D loss: 0.206675, acc.: 92.19%] [G loss: 4.357626]\n",
      "epoch:6 step:4984 [D loss: 0.134223, acc.: 96.88%] [G loss: 1.677123]\n",
      "epoch:6 step:4985 [D loss: 0.230928, acc.: 92.97%] [G loss: 3.795399]\n",
      "epoch:6 step:4986 [D loss: 0.045551, acc.: 98.44%] [G loss: 4.263992]\n",
      "epoch:6 step:4987 [D loss: 0.160002, acc.: 96.88%] [G loss: 2.577674]\n",
      "epoch:6 step:4988 [D loss: 0.408616, acc.: 84.38%] [G loss: 1.504379]\n",
      "epoch:6 step:4989 [D loss: 0.066691, acc.: 96.88%] [G loss: 1.304333]\n",
      "epoch:6 step:4990 [D loss: 0.061723, acc.: 99.22%] [G loss: 0.712817]\n",
      "epoch:6 step:4991 [D loss: 0.030695, acc.: 100.00%] [G loss: 0.481171]\n",
      "epoch:6 step:4992 [D loss: 0.078688, acc.: 99.22%] [G loss: 0.883181]\n",
      "epoch:6 step:4993 [D loss: 0.770809, acc.: 61.72%] [G loss: 3.283289]\n",
      "epoch:6 step:4994 [D loss: 0.095430, acc.: 98.44%] [G loss: 5.182027]\n",
      "epoch:6 step:4995 [D loss: 0.406621, acc.: 79.69%] [G loss: 1.184804]\n",
      "epoch:6 step:4996 [D loss: 0.447934, acc.: 79.69%] [G loss: 5.894140]\n",
      "epoch:6 step:4997 [D loss: 0.075795, acc.: 97.66%] [G loss: 6.161841]\n",
      "epoch:6 step:4998 [D loss: 0.704499, acc.: 66.41%] [G loss: 2.824714]\n",
      "epoch:6 step:4999 [D loss: 0.578746, acc.: 72.66%] [G loss: 7.731520]\n",
      "epoch:6 step:5000 [D loss: 0.421967, acc.: 78.12%] [G loss: 4.218665]\n",
      "epoch:6 step:5001 [D loss: 0.873305, acc.: 59.38%] [G loss: 6.224267]\n",
      "epoch:6 step:5002 [D loss: 0.246975, acc.: 86.72%] [G loss: 6.107583]\n",
      "epoch:6 step:5003 [D loss: 0.156037, acc.: 92.97%] [G loss: 3.039828]\n",
      "epoch:6 step:5004 [D loss: 0.056239, acc.: 99.22%] [G loss: 2.626641]\n",
      "epoch:6 step:5005 [D loss: 0.394940, acc.: 80.47%] [G loss: 6.036063]\n",
      "epoch:6 step:5006 [D loss: 0.499246, acc.: 74.22%] [G loss: 3.178181]\n",
      "epoch:6 step:5007 [D loss: 0.109752, acc.: 94.53%] [G loss: 2.023252]\n",
      "epoch:6 step:5008 [D loss: 0.099049, acc.: 96.09%] [G loss: 2.283128]\n",
      "epoch:6 step:5009 [D loss: 0.114816, acc.: 96.09%] [G loss: 2.487720]\n",
      "epoch:6 step:5010 [D loss: 0.049669, acc.: 100.00%] [G loss: 2.375399]\n",
      "epoch:6 step:5011 [D loss: 0.250706, acc.: 89.84%] [G loss: 3.769857]\n",
      "epoch:6 step:5012 [D loss: 0.308304, acc.: 85.16%] [G loss: 0.833612]\n",
      "epoch:6 step:5013 [D loss: 1.119075, acc.: 55.47%] [G loss: 6.165313]\n",
      "epoch:6 step:5014 [D loss: 0.242006, acc.: 89.06%] [G loss: 5.618165]\n",
      "epoch:6 step:5015 [D loss: 0.148486, acc.: 94.53%] [G loss: 4.081069]\n",
      "epoch:6 step:5016 [D loss: 0.048228, acc.: 100.00%] [G loss: 1.856572]\n",
      "epoch:6 step:5017 [D loss: 0.697615, acc.: 64.06%] [G loss: 5.097444]\n",
      "epoch:6 step:5018 [D loss: 0.462642, acc.: 77.34%] [G loss: 3.422020]\n",
      "epoch:6 step:5019 [D loss: 0.217001, acc.: 92.97%] [G loss: 2.567904]\n",
      "epoch:6 step:5020 [D loss: 0.214384, acc.: 90.62%] [G loss: 3.157278]\n",
      "epoch:6 step:5021 [D loss: 0.331856, acc.: 82.03%] [G loss: 1.377746]\n",
      "epoch:6 step:5022 [D loss: 1.286691, acc.: 47.66%] [G loss: 6.699656]\n",
      "epoch:6 step:5023 [D loss: 0.750103, acc.: 60.94%] [G loss: 4.515705]\n",
      "epoch:6 step:5024 [D loss: 0.388151, acc.: 83.59%] [G loss: 4.085596]\n",
      "epoch:6 step:5025 [D loss: 0.269951, acc.: 89.06%] [G loss: 4.458969]\n",
      "epoch:6 step:5026 [D loss: 0.123472, acc.: 96.88%] [G loss: 3.920913]\n",
      "epoch:6 step:5027 [D loss: 0.183604, acc.: 95.31%] [G loss: 3.434696]\n",
      "epoch:6 step:5028 [D loss: 0.088947, acc.: 99.22%] [G loss: 2.766430]\n",
      "epoch:6 step:5029 [D loss: 0.169164, acc.: 96.09%] [G loss: 2.147490]\n",
      "epoch:6 step:5030 [D loss: 0.168316, acc.: 96.09%] [G loss: 3.341180]\n",
      "epoch:6 step:5031 [D loss: 0.235533, acc.: 90.62%] [G loss: 1.885433]\n",
      "epoch:6 step:5032 [D loss: 0.174577, acc.: 95.31%] [G loss: 1.672461]\n",
      "epoch:6 step:5033 [D loss: 0.189309, acc.: 95.31%] [G loss: 2.520927]\n",
      "epoch:6 step:5034 [D loss: 0.170417, acc.: 95.31%] [G loss: 2.784687]\n",
      "epoch:6 step:5035 [D loss: 0.179570, acc.: 91.41%] [G loss: 1.478871]\n",
      "epoch:6 step:5036 [D loss: 0.342028, acc.: 82.03%] [G loss: 3.743570]\n",
      "epoch:6 step:5037 [D loss: 0.290313, acc.: 86.72%] [G loss: 1.729233]\n",
      "epoch:6 step:5038 [D loss: 0.035298, acc.: 100.00%] [G loss: 1.000155]\n",
      "epoch:6 step:5039 [D loss: 0.128051, acc.: 96.88%] [G loss: 1.439586]\n",
      "epoch:6 step:5040 [D loss: 0.081015, acc.: 96.09%] [G loss: 1.162455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5041 [D loss: 0.234641, acc.: 95.31%] [G loss: 1.304955]\n",
      "epoch:6 step:5042 [D loss: 0.153859, acc.: 96.09%] [G loss: 1.374134]\n",
      "epoch:6 step:5043 [D loss: 0.343343, acc.: 84.38%] [G loss: 4.721457]\n",
      "epoch:6 step:5044 [D loss: 0.968697, acc.: 57.81%] [G loss: 1.872066]\n",
      "epoch:6 step:5045 [D loss: 0.504108, acc.: 74.22%] [G loss: 4.890724]\n",
      "epoch:6 step:5046 [D loss: 0.328572, acc.: 82.03%] [G loss: 4.311327]\n",
      "epoch:6 step:5047 [D loss: 0.342592, acc.: 86.72%] [G loss: 2.695880]\n",
      "epoch:6 step:5048 [D loss: 0.139016, acc.: 97.66%] [G loss: 3.926536]\n",
      "epoch:6 step:5049 [D loss: 0.049670, acc.: 99.22%] [G loss: 3.980631]\n",
      "epoch:6 step:5050 [D loss: 0.139962, acc.: 96.88%] [G loss: 3.812106]\n",
      "epoch:6 step:5051 [D loss: 0.100242, acc.: 99.22%] [G loss: 3.352300]\n",
      "epoch:6 step:5052 [D loss: 0.308590, acc.: 88.28%] [G loss: 2.788617]\n",
      "epoch:6 step:5053 [D loss: 0.351688, acc.: 86.72%] [G loss: 2.856743]\n",
      "epoch:6 step:5054 [D loss: 0.102032, acc.: 98.44%] [G loss: 2.663881]\n",
      "epoch:6 step:5055 [D loss: 0.229284, acc.: 92.19%] [G loss: 3.293679]\n",
      "epoch:6 step:5056 [D loss: 0.123281, acc.: 95.31%] [G loss: 2.383963]\n",
      "epoch:6 step:5057 [D loss: 0.077512, acc.: 99.22%] [G loss: 1.290195]\n",
      "epoch:6 step:5058 [D loss: 0.051889, acc.: 99.22%] [G loss: 1.089658]\n",
      "epoch:6 step:5059 [D loss: 0.269856, acc.: 91.41%] [G loss: 1.894242]\n",
      "epoch:6 step:5060 [D loss: 0.319594, acc.: 81.25%] [G loss: 0.415772]\n",
      "epoch:6 step:5061 [D loss: 0.510229, acc.: 78.12%] [G loss: 2.754078]\n",
      "epoch:6 step:5062 [D loss: 0.153649, acc.: 93.75%] [G loss: 2.450161]\n",
      "epoch:6 step:5063 [D loss: 0.098547, acc.: 97.66%] [G loss: 0.536815]\n",
      "epoch:6 step:5064 [D loss: 0.121134, acc.: 96.88%] [G loss: 0.820780]\n",
      "epoch:6 step:5065 [D loss: 0.041116, acc.: 99.22%] [G loss: 1.233265]\n",
      "epoch:6 step:5066 [D loss: 0.044136, acc.: 100.00%] [G loss: 0.381638]\n",
      "epoch:6 step:5067 [D loss: 0.134002, acc.: 96.09%] [G loss: 0.683654]\n",
      "epoch:6 step:5068 [D loss: 0.125539, acc.: 96.88%] [G loss: 1.037070]\n",
      "epoch:6 step:5069 [D loss: 0.026761, acc.: 100.00%] [G loss: 0.972805]\n",
      "epoch:6 step:5070 [D loss: 0.023181, acc.: 100.00%] [G loss: 0.458507]\n",
      "epoch:6 step:5071 [D loss: 0.533522, acc.: 74.22%] [G loss: 5.705464]\n",
      "epoch:6 step:5072 [D loss: 1.468277, acc.: 51.56%] [G loss: 1.604641]\n",
      "epoch:6 step:5073 [D loss: 0.389725, acc.: 81.25%] [G loss: 4.424347]\n",
      "epoch:6 step:5074 [D loss: 0.216236, acc.: 90.62%] [G loss: 4.077278]\n",
      "epoch:6 step:5075 [D loss: 0.118629, acc.: 96.09%] [G loss: 4.113389]\n",
      "epoch:6 step:5076 [D loss: 0.040794, acc.: 100.00%] [G loss: 3.103942]\n",
      "epoch:6 step:5077 [D loss: 0.098750, acc.: 98.44%] [G loss: 2.312326]\n",
      "epoch:6 step:5078 [D loss: 0.021102, acc.: 100.00%] [G loss: 1.984466]\n",
      "epoch:6 step:5079 [D loss: 0.125419, acc.: 95.31%] [G loss: 0.317605]\n",
      "epoch:6 step:5080 [D loss: 0.070296, acc.: 98.44%] [G loss: 0.290538]\n",
      "epoch:6 step:5081 [D loss: 0.082227, acc.: 99.22%] [G loss: 0.637755]\n",
      "epoch:6 step:5082 [D loss: 0.041462, acc.: 100.00%] [G loss: 0.839109]\n",
      "epoch:6 step:5083 [D loss: 0.033368, acc.: 100.00%] [G loss: 0.837934]\n",
      "epoch:6 step:5084 [D loss: 0.131593, acc.: 95.31%] [G loss: 1.411477]\n",
      "epoch:6 step:5085 [D loss: 0.184641, acc.: 93.75%] [G loss: 0.280493]\n",
      "epoch:6 step:5086 [D loss: 0.169399, acc.: 94.53%] [G loss: 1.402421]\n",
      "epoch:6 step:5087 [D loss: 0.069698, acc.: 99.22%] [G loss: 2.293547]\n",
      "epoch:6 step:5088 [D loss: 0.165395, acc.: 93.75%] [G loss: 0.275361]\n",
      "epoch:6 step:5089 [D loss: 0.533636, acc.: 71.88%] [G loss: 4.227489]\n",
      "epoch:6 step:5090 [D loss: 0.397815, acc.: 84.38%] [G loss: 3.453606]\n",
      "epoch:6 step:5091 [D loss: 0.336588, acc.: 85.94%] [G loss: 0.406179]\n",
      "epoch:6 step:5092 [D loss: 0.650584, acc.: 65.62%] [G loss: 6.392390]\n",
      "epoch:6 step:5093 [D loss: 0.824090, acc.: 60.94%] [G loss: 3.467067]\n",
      "epoch:6 step:5094 [D loss: 0.096303, acc.: 98.44%] [G loss: 3.383250]\n",
      "epoch:6 step:5095 [D loss: 0.092731, acc.: 96.88%] [G loss: 2.659959]\n",
      "epoch:6 step:5096 [D loss: 0.081315, acc.: 97.66%] [G loss: 3.349077]\n",
      "epoch:6 step:5097 [D loss: 0.202069, acc.: 93.75%] [G loss: 3.168383]\n",
      "epoch:6 step:5098 [D loss: 0.326384, acc.: 86.72%] [G loss: 5.411239]\n",
      "epoch:6 step:5099 [D loss: 0.152488, acc.: 93.75%] [G loss: 4.071479]\n",
      "epoch:6 step:5100 [D loss: 0.457639, acc.: 76.56%] [G loss: 3.178051]\n",
      "epoch:6 step:5101 [D loss: 0.116368, acc.: 98.44%] [G loss: 4.018000]\n",
      "epoch:6 step:5102 [D loss: 0.283828, acc.: 85.16%] [G loss: 2.695021]\n",
      "epoch:6 step:5103 [D loss: 0.212390, acc.: 90.62%] [G loss: 1.949350]\n",
      "epoch:6 step:5104 [D loss: 0.070706, acc.: 99.22%] [G loss: 1.373026]\n",
      "epoch:6 step:5105 [D loss: 0.119964, acc.: 99.22%] [G loss: 1.867871]\n",
      "epoch:6 step:5106 [D loss: 0.236294, acc.: 92.19%] [G loss: 0.183981]\n",
      "epoch:6 step:5107 [D loss: 0.186116, acc.: 91.41%] [G loss: 1.588509]\n",
      "epoch:6 step:5108 [D loss: 0.090235, acc.: 96.88%] [G loss: 1.291524]\n",
      "epoch:6 step:5109 [D loss: 0.234671, acc.: 95.31%] [G loss: 0.854613]\n",
      "epoch:6 step:5110 [D loss: 0.055470, acc.: 100.00%] [G loss: 0.434558]\n",
      "epoch:6 step:5111 [D loss: 0.123675, acc.: 96.09%] [G loss: 0.985558]\n",
      "epoch:6 step:5112 [D loss: 0.044945, acc.: 100.00%] [G loss: 2.050948]\n",
      "epoch:6 step:5113 [D loss: 0.015183, acc.: 100.00%] [G loss: 0.843806]\n",
      "epoch:6 step:5114 [D loss: 1.853594, acc.: 34.38%] [G loss: 6.241478]\n",
      "epoch:6 step:5115 [D loss: 0.738206, acc.: 64.06%] [G loss: 4.141198]\n",
      "epoch:6 step:5116 [D loss: 0.625137, acc.: 67.97%] [G loss: 1.666209]\n",
      "epoch:6 step:5117 [D loss: 0.284402, acc.: 82.81%] [G loss: 3.522972]\n",
      "epoch:6 step:5118 [D loss: 0.098089, acc.: 97.66%] [G loss: 4.688607]\n",
      "epoch:6 step:5119 [D loss: 0.161505, acc.: 96.09%] [G loss: 3.128016]\n",
      "epoch:6 step:5120 [D loss: 0.329428, acc.: 87.50%] [G loss: 3.018723]\n",
      "epoch:6 step:5121 [D loss: 0.130046, acc.: 96.88%] [G loss: 3.678380]\n",
      "epoch:6 step:5122 [D loss: 0.365221, acc.: 85.16%] [G loss: 3.404992]\n",
      "epoch:6 step:5123 [D loss: 0.318390, acc.: 85.94%] [G loss: 4.483437]\n",
      "epoch:6 step:5124 [D loss: 0.171252, acc.: 95.31%] [G loss: 4.052713]\n",
      "epoch:6 step:5125 [D loss: 0.166980, acc.: 93.75%] [G loss: 2.205223]\n",
      "epoch:6 step:5126 [D loss: 0.228372, acc.: 89.84%] [G loss: 3.263005]\n",
      "epoch:6 step:5127 [D loss: 0.446366, acc.: 76.56%] [G loss: 4.344068]\n",
      "epoch:6 step:5128 [D loss: 0.277134, acc.: 87.50%] [G loss: 2.935569]\n",
      "epoch:6 step:5129 [D loss: 0.287297, acc.: 86.72%] [G loss: 4.238926]\n",
      "epoch:6 step:5130 [D loss: 0.372542, acc.: 84.38%] [G loss: 3.591263]\n",
      "epoch:6 step:5131 [D loss: 0.074113, acc.: 97.66%] [G loss: 3.311759]\n",
      "epoch:6 step:5132 [D loss: 0.101956, acc.: 99.22%] [G loss: 3.058141]\n",
      "epoch:6 step:5133 [D loss: 0.209939, acc.: 93.75%] [G loss: 1.931864]\n",
      "epoch:6 step:5134 [D loss: 0.337789, acc.: 83.59%] [G loss: 4.102522]\n",
      "epoch:6 step:5135 [D loss: 1.537723, acc.: 22.66%] [G loss: 5.064596]\n",
      "epoch:6 step:5136 [D loss: 0.257809, acc.: 89.06%] [G loss: 4.910614]\n",
      "epoch:6 step:5137 [D loss: 0.137895, acc.: 96.09%] [G loss: 4.485036]\n",
      "epoch:6 step:5138 [D loss: 0.064529, acc.: 100.00%] [G loss: 3.588854]\n",
      "epoch:6 step:5139 [D loss: 0.168276, acc.: 97.66%] [G loss: 3.459231]\n",
      "epoch:6 step:5140 [D loss: 0.255782, acc.: 88.28%] [G loss: 3.903740]\n",
      "epoch:6 step:5141 [D loss: 0.322470, acc.: 83.59%] [G loss: 2.656914]\n",
      "epoch:6 step:5142 [D loss: 0.266726, acc.: 92.19%] [G loss: 3.511729]\n",
      "epoch:6 step:5143 [D loss: 0.179061, acc.: 95.31%] [G loss: 3.663872]\n",
      "epoch:6 step:5144 [D loss: 0.181579, acc.: 92.97%] [G loss: 4.167257]\n",
      "epoch:6 step:5145 [D loss: 0.486046, acc.: 76.56%] [G loss: 2.537512]\n",
      "epoch:6 step:5146 [D loss: 0.370324, acc.: 82.81%] [G loss: 3.477880]\n",
      "epoch:6 step:5147 [D loss: 0.124777, acc.: 97.66%] [G loss: 2.961663]\n",
      "epoch:6 step:5148 [D loss: 0.200844, acc.: 95.31%] [G loss: 1.800816]\n",
      "epoch:6 step:5149 [D loss: 0.310409, acc.: 88.28%] [G loss: 3.912827]\n",
      "epoch:6 step:5150 [D loss: 0.300347, acc.: 86.72%] [G loss: 3.044462]\n",
      "epoch:6 step:5151 [D loss: 0.168981, acc.: 95.31%] [G loss: 2.947903]\n",
      "epoch:6 step:5152 [D loss: 0.093184, acc.: 97.66%] [G loss: 1.985595]\n",
      "epoch:6 step:5153 [D loss: 0.188340, acc.: 95.31%] [G loss: 1.424550]\n",
      "epoch:6 step:5154 [D loss: 0.159211, acc.: 93.75%] [G loss: 2.722189]\n",
      "epoch:6 step:5155 [D loss: 0.045740, acc.: 99.22%] [G loss: 2.510944]\n",
      "epoch:6 step:5156 [D loss: 0.126187, acc.: 96.88%] [G loss: 2.841348]\n",
      "epoch:6 step:5157 [D loss: 0.305812, acc.: 91.41%] [G loss: 3.582429]\n",
      "epoch:6 step:5158 [D loss: 1.109276, acc.: 41.41%] [G loss: 4.767764]\n",
      "epoch:6 step:5159 [D loss: 0.094896, acc.: 96.09%] [G loss: 5.442249]\n",
      "epoch:6 step:5160 [D loss: 0.152062, acc.: 93.75%] [G loss: 4.358088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5161 [D loss: 0.187771, acc.: 93.75%] [G loss: 4.226916]\n",
      "epoch:6 step:5162 [D loss: 0.053273, acc.: 100.00%] [G loss: 3.255460]\n",
      "epoch:6 step:5163 [D loss: 0.066738, acc.: 100.00%] [G loss: 3.737368]\n",
      "epoch:6 step:5164 [D loss: 0.249439, acc.: 92.19%] [G loss: 4.759582]\n",
      "epoch:6 step:5165 [D loss: 0.156295, acc.: 94.53%] [G loss: 4.330954]\n",
      "epoch:6 step:5166 [D loss: 0.316785, acc.: 84.38%] [G loss: 1.742740]\n",
      "epoch:6 step:5167 [D loss: 0.173165, acc.: 94.53%] [G loss: 2.060741]\n",
      "epoch:6 step:5168 [D loss: 0.252549, acc.: 91.41%] [G loss: 4.329694]\n",
      "epoch:6 step:5169 [D loss: 0.288446, acc.: 87.50%] [G loss: 2.558014]\n",
      "epoch:6 step:5170 [D loss: 0.240346, acc.: 87.50%] [G loss: 4.537349]\n",
      "epoch:6 step:5171 [D loss: 0.136804, acc.: 95.31%] [G loss: 4.217158]\n",
      "epoch:6 step:5172 [D loss: 0.272290, acc.: 90.62%] [G loss: 1.241066]\n",
      "epoch:6 step:5173 [D loss: 1.060853, acc.: 52.34%] [G loss: 6.591794]\n",
      "epoch:6 step:5174 [D loss: 0.766020, acc.: 62.50%] [G loss: 3.559377]\n",
      "epoch:6 step:5175 [D loss: 0.653412, acc.: 74.22%] [G loss: 4.512547]\n",
      "epoch:6 step:5176 [D loss: 0.504431, acc.: 77.34%] [G loss: 3.027812]\n",
      "epoch:6 step:5177 [D loss: 0.214019, acc.: 94.53%] [G loss: 3.885530]\n",
      "epoch:6 step:5178 [D loss: 0.169981, acc.: 93.75%] [G loss: 3.668371]\n",
      "epoch:6 step:5179 [D loss: 0.473831, acc.: 75.78%] [G loss: 3.280813]\n",
      "epoch:6 step:5180 [D loss: 0.136296, acc.: 96.09%] [G loss: 3.237074]\n",
      "epoch:6 step:5181 [D loss: 0.139105, acc.: 96.09%] [G loss: 3.253410]\n",
      "epoch:6 step:5182 [D loss: 0.215682, acc.: 93.75%] [G loss: 2.447841]\n",
      "epoch:6 step:5183 [D loss: 0.182086, acc.: 96.88%] [G loss: 3.597960]\n",
      "epoch:6 step:5184 [D loss: 0.138823, acc.: 96.09%] [G loss: 1.928349]\n",
      "epoch:6 step:5185 [D loss: 0.210270, acc.: 95.31%] [G loss: 1.996292]\n",
      "epoch:6 step:5186 [D loss: 0.073594, acc.: 99.22%] [G loss: 1.227459]\n",
      "epoch:6 step:5187 [D loss: 0.109734, acc.: 96.88%] [G loss: 0.526550]\n",
      "epoch:6 step:5188 [D loss: 0.285480, acc.: 89.06%] [G loss: 3.207929]\n",
      "epoch:6 step:5189 [D loss: 0.464001, acc.: 75.78%] [G loss: 1.952663]\n",
      "epoch:6 step:5190 [D loss: 0.241193, acc.: 91.41%] [G loss: 5.032670]\n",
      "epoch:6 step:5191 [D loss: 0.068222, acc.: 99.22%] [G loss: 5.823605]\n",
      "epoch:6 step:5192 [D loss: 0.751307, acc.: 63.28%] [G loss: 5.665913]\n",
      "epoch:6 step:5193 [D loss: 0.096783, acc.: 96.88%] [G loss: 5.883631]\n",
      "epoch:6 step:5194 [D loss: 0.345539, acc.: 89.84%] [G loss: 3.565918]\n",
      "epoch:6 step:5195 [D loss: 0.147077, acc.: 95.31%] [G loss: 4.449555]\n",
      "epoch:6 step:5196 [D loss: 0.209170, acc.: 93.75%] [G loss: 4.307237]\n",
      "epoch:6 step:5197 [D loss: 0.200056, acc.: 92.19%] [G loss: 2.444314]\n",
      "epoch:6 step:5198 [D loss: 0.281474, acc.: 85.16%] [G loss: 4.763788]\n",
      "epoch:6 step:5199 [D loss: 0.174936, acc.: 92.97%] [G loss: 3.546898]\n",
      "epoch:6 step:5200 [D loss: 0.628206, acc.: 67.19%] [G loss: 5.058096]\n",
      "epoch:6 step:5201 [D loss: 0.365656, acc.: 85.94%] [G loss: 4.355785]\n",
      "epoch:6 step:5202 [D loss: 0.216316, acc.: 93.75%] [G loss: 3.942268]\n",
      "epoch:6 step:5203 [D loss: 0.078625, acc.: 99.22%] [G loss: 3.126359]\n",
      "epoch:6 step:5204 [D loss: 0.233591, acc.: 90.62%] [G loss: 5.436403]\n",
      "epoch:6 step:5205 [D loss: 1.152389, acc.: 42.97%] [G loss: 4.329885]\n",
      "epoch:6 step:5206 [D loss: 0.146316, acc.: 95.31%] [G loss: 4.084605]\n",
      "epoch:6 step:5207 [D loss: 0.307365, acc.: 87.50%] [G loss: 3.677462]\n",
      "epoch:6 step:5208 [D loss: 0.284963, acc.: 88.28%] [G loss: 3.586647]\n",
      "epoch:6 step:5209 [D loss: 0.277732, acc.: 91.41%] [G loss: 5.578260]\n",
      "epoch:6 step:5210 [D loss: 0.450718, acc.: 81.25%] [G loss: 2.544161]\n",
      "epoch:6 step:5211 [D loss: 0.198520, acc.: 90.62%] [G loss: 4.456811]\n",
      "epoch:6 step:5212 [D loss: 0.248625, acc.: 90.62%] [G loss: 3.578924]\n",
      "epoch:6 step:5213 [D loss: 0.333192, acc.: 85.16%] [G loss: 6.172800]\n",
      "epoch:6 step:5214 [D loss: 0.318181, acc.: 82.81%] [G loss: 4.823012]\n",
      "epoch:6 step:5215 [D loss: 0.089518, acc.: 98.44%] [G loss: 4.047317]\n",
      "epoch:6 step:5216 [D loss: 0.124136, acc.: 96.09%] [G loss: 3.781973]\n",
      "epoch:6 step:5217 [D loss: 0.166093, acc.: 94.53%] [G loss: 4.329776]\n",
      "epoch:6 step:5218 [D loss: 0.144299, acc.: 95.31%] [G loss: 4.385649]\n",
      "epoch:6 step:5219 [D loss: 0.456786, acc.: 77.34%] [G loss: 4.460017]\n",
      "epoch:6 step:5220 [D loss: 0.107142, acc.: 97.66%] [G loss: 4.339255]\n",
      "epoch:6 step:5221 [D loss: 0.207007, acc.: 92.97%] [G loss: 3.643508]\n",
      "epoch:6 step:5222 [D loss: 0.118737, acc.: 98.44%] [G loss: 2.888564]\n",
      "epoch:6 step:5223 [D loss: 0.511370, acc.: 75.78%] [G loss: 5.123812]\n",
      "epoch:6 step:5224 [D loss: 0.203324, acc.: 92.97%] [G loss: 3.704576]\n",
      "epoch:6 step:5225 [D loss: 0.101869, acc.: 97.66%] [G loss: 4.287355]\n",
      "epoch:6 step:5226 [D loss: 0.091799, acc.: 96.88%] [G loss: 3.884481]\n",
      "epoch:6 step:5227 [D loss: 0.201287, acc.: 95.31%] [G loss: 4.348145]\n",
      "epoch:6 step:5228 [D loss: 1.345902, acc.: 39.06%] [G loss: 6.460084]\n",
      "epoch:6 step:5229 [D loss: 0.535485, acc.: 75.78%] [G loss: 4.173582]\n",
      "epoch:6 step:5230 [D loss: 0.057010, acc.: 99.22%] [G loss: 4.733649]\n",
      "epoch:6 step:5231 [D loss: 0.076244, acc.: 97.66%] [G loss: 3.872908]\n",
      "epoch:6 step:5232 [D loss: 0.144867, acc.: 95.31%] [G loss: 4.296320]\n",
      "epoch:6 step:5233 [D loss: 0.227584, acc.: 90.62%] [G loss: 3.325448]\n",
      "epoch:6 step:5234 [D loss: 0.327408, acc.: 88.28%] [G loss: 3.699098]\n",
      "epoch:6 step:5235 [D loss: 0.173113, acc.: 94.53%] [G loss: 1.654372]\n",
      "epoch:6 step:5236 [D loss: 0.492984, acc.: 72.66%] [G loss: 5.532073]\n",
      "epoch:6 step:5237 [D loss: 0.543835, acc.: 68.75%] [G loss: 3.686241]\n",
      "epoch:6 step:5238 [D loss: 1.110101, acc.: 43.75%] [G loss: 4.030800]\n",
      "epoch:6 step:5239 [D loss: 0.034088, acc.: 99.22%] [G loss: 5.603436]\n",
      "epoch:6 step:5240 [D loss: 0.569816, acc.: 65.62%] [G loss: 2.109798]\n",
      "epoch:6 step:5241 [D loss: 0.261547, acc.: 90.62%] [G loss: 4.545243]\n",
      "epoch:6 step:5242 [D loss: 0.044436, acc.: 100.00%] [G loss: 5.010683]\n",
      "epoch:6 step:5243 [D loss: 0.220541, acc.: 91.41%] [G loss: 3.361415]\n",
      "epoch:6 step:5244 [D loss: 0.176424, acc.: 92.19%] [G loss: 3.220375]\n",
      "epoch:6 step:5245 [D loss: 0.123242, acc.: 96.09%] [G loss: 3.007720]\n",
      "epoch:6 step:5246 [D loss: 0.135929, acc.: 96.09%] [G loss: 3.688702]\n",
      "epoch:6 step:5247 [D loss: 0.291691, acc.: 88.28%] [G loss: 4.281885]\n",
      "epoch:6 step:5248 [D loss: 0.424403, acc.: 76.56%] [G loss: 2.202411]\n",
      "epoch:6 step:5249 [D loss: 0.267453, acc.: 88.28%] [G loss: 4.152345]\n",
      "epoch:6 step:5250 [D loss: 0.077540, acc.: 99.22%] [G loss: 4.116370]\n",
      "epoch:6 step:5251 [D loss: 0.297771, acc.: 91.41%] [G loss: 3.439837]\n",
      "epoch:6 step:5252 [D loss: 0.287612, acc.: 89.84%] [G loss: 3.476083]\n",
      "epoch:6 step:5253 [D loss: 0.259459, acc.: 89.06%] [G loss: 3.797533]\n",
      "epoch:6 step:5254 [D loss: 0.231796, acc.: 91.41%] [G loss: 3.766900]\n",
      "epoch:6 step:5255 [D loss: 0.179361, acc.: 96.09%] [G loss: 3.215343]\n",
      "epoch:6 step:5256 [D loss: 0.104304, acc.: 98.44%] [G loss: 3.802935]\n",
      "epoch:6 step:5257 [D loss: 0.658153, acc.: 67.19%] [G loss: 5.748901]\n",
      "epoch:6 step:5258 [D loss: 0.215322, acc.: 89.06%] [G loss: 5.399777]\n",
      "epoch:6 step:5259 [D loss: 0.401456, acc.: 82.81%] [G loss: 3.768872]\n",
      "epoch:6 step:5260 [D loss: 0.252923, acc.: 91.41%] [G loss: 5.491245]\n",
      "epoch:6 step:5261 [D loss: 0.508246, acc.: 74.22%] [G loss: 3.027970]\n",
      "epoch:6 step:5262 [D loss: 0.152707, acc.: 96.88%] [G loss: 4.620895]\n",
      "epoch:6 step:5263 [D loss: 0.164644, acc.: 93.75%] [G loss: 3.767776]\n",
      "epoch:6 step:5264 [D loss: 0.071642, acc.: 99.22%] [G loss: 3.823278]\n",
      "epoch:6 step:5265 [D loss: 0.139114, acc.: 98.44%] [G loss: 4.062534]\n",
      "epoch:6 step:5266 [D loss: 0.200980, acc.: 91.41%] [G loss: 3.488877]\n",
      "epoch:6 step:5267 [D loss: 0.220434, acc.: 92.19%] [G loss: 3.870998]\n",
      "epoch:6 step:5268 [D loss: 0.137875, acc.: 95.31%] [G loss: 4.313356]\n",
      "epoch:6 step:5269 [D loss: 0.127932, acc.: 97.66%] [G loss: 3.886435]\n",
      "epoch:6 step:5270 [D loss: 0.303590, acc.: 85.94%] [G loss: 5.665899]\n",
      "epoch:6 step:5271 [D loss: 0.499363, acc.: 76.56%] [G loss: 3.250418]\n",
      "epoch:6 step:5272 [D loss: 0.054999, acc.: 98.44%] [G loss: 4.217992]\n",
      "epoch:6 step:5273 [D loss: 0.104715, acc.: 97.66%] [G loss: 4.768219]\n",
      "epoch:6 step:5274 [D loss: 0.132119, acc.: 96.09%] [G loss: 3.180089]\n",
      "epoch:6 step:5275 [D loss: 0.233048, acc.: 94.53%] [G loss: 4.998083]\n",
      "epoch:6 step:5276 [D loss: 0.138731, acc.: 94.53%] [G loss: 4.226538]\n",
      "epoch:6 step:5277 [D loss: 0.099336, acc.: 98.44%] [G loss: 3.817535]\n",
      "epoch:6 step:5278 [D loss: 0.043754, acc.: 100.00%] [G loss: 3.363012]\n",
      "epoch:6 step:5279 [D loss: 0.474128, acc.: 77.34%] [G loss: 3.770026]\n",
      "epoch:6 step:5280 [D loss: 0.105853, acc.: 97.66%] [G loss: 5.031240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5281 [D loss: 0.538084, acc.: 72.66%] [G loss: 5.358372]\n",
      "epoch:6 step:5282 [D loss: 0.131867, acc.: 93.75%] [G loss: 3.804410]\n",
      "epoch:6 step:5283 [D loss: 0.096740, acc.: 98.44%] [G loss: 4.673184]\n",
      "epoch:6 step:5284 [D loss: 0.248762, acc.: 87.50%] [G loss: 4.448833]\n",
      "epoch:6 step:5285 [D loss: 0.915099, acc.: 52.34%] [G loss: 6.799187]\n",
      "epoch:6 step:5286 [D loss: 0.140387, acc.: 92.97%] [G loss: 6.645917]\n",
      "epoch:6 step:5287 [D loss: 0.379105, acc.: 84.38%] [G loss: 1.670548]\n",
      "epoch:6 step:5288 [D loss: 0.216506, acc.: 91.41%] [G loss: 4.850316]\n",
      "epoch:6 step:5289 [D loss: 0.024810, acc.: 100.00%] [G loss: 5.932511]\n",
      "epoch:6 step:5290 [D loss: 0.494319, acc.: 75.78%] [G loss: 2.613419]\n",
      "epoch:6 step:5291 [D loss: 0.162025, acc.: 92.97%] [G loss: 3.031777]\n",
      "epoch:6 step:5292 [D loss: 0.106644, acc.: 97.66%] [G loss: 4.632960]\n",
      "epoch:6 step:5293 [D loss: 0.062059, acc.: 97.66%] [G loss: 4.235614]\n",
      "epoch:6 step:5294 [D loss: 0.129983, acc.: 96.88%] [G loss: 3.541115]\n",
      "epoch:6 step:5295 [D loss: 0.373111, acc.: 79.69%] [G loss: 2.054498]\n",
      "epoch:6 step:5296 [D loss: 0.256840, acc.: 87.50%] [G loss: 6.065902]\n",
      "epoch:6 step:5297 [D loss: 0.384706, acc.: 82.03%] [G loss: 2.708538]\n",
      "epoch:6 step:5298 [D loss: 0.083511, acc.: 99.22%] [G loss: 2.379947]\n",
      "epoch:6 step:5299 [D loss: 0.772878, acc.: 57.81%] [G loss: 5.290874]\n",
      "epoch:6 step:5300 [D loss: 0.051896, acc.: 98.44%] [G loss: 5.489349]\n",
      "epoch:6 step:5301 [D loss: 0.283826, acc.: 89.06%] [G loss: 2.999091]\n",
      "epoch:6 step:5302 [D loss: 0.219596, acc.: 90.62%] [G loss: 3.263428]\n",
      "epoch:6 step:5303 [D loss: 0.164466, acc.: 94.53%] [G loss: 4.103897]\n",
      "epoch:6 step:5304 [D loss: 0.451633, acc.: 82.03%] [G loss: 4.119989]\n",
      "epoch:6 step:5305 [D loss: 0.317106, acc.: 86.72%] [G loss: 3.484970]\n",
      "epoch:6 step:5306 [D loss: 0.332565, acc.: 85.94%] [G loss: 4.752140]\n",
      "epoch:6 step:5307 [D loss: 0.121557, acc.: 96.88%] [G loss: 3.018182]\n",
      "epoch:6 step:5308 [D loss: 0.355305, acc.: 84.38%] [G loss: 3.283885]\n",
      "epoch:6 step:5309 [D loss: 0.327956, acc.: 86.72%] [G loss: 3.571993]\n",
      "epoch:6 step:5310 [D loss: 0.059297, acc.: 100.00%] [G loss: 3.007718]\n",
      "epoch:6 step:5311 [D loss: 0.055866, acc.: 99.22%] [G loss: 4.108754]\n",
      "epoch:6 step:5312 [D loss: 0.146888, acc.: 96.09%] [G loss: 2.874601]\n",
      "epoch:6 step:5313 [D loss: 0.095415, acc.: 98.44%] [G loss: 2.331449]\n",
      "epoch:6 step:5314 [D loss: 0.480764, acc.: 81.25%] [G loss: 4.417009]\n",
      "epoch:6 step:5315 [D loss: 0.113471, acc.: 96.09%] [G loss: 4.958262]\n",
      "epoch:6 step:5316 [D loss: 0.192472, acc.: 92.97%] [G loss: 3.782212]\n",
      "epoch:6 step:5317 [D loss: 0.442738, acc.: 77.34%] [G loss: 5.578135]\n",
      "epoch:6 step:5318 [D loss: 0.372296, acc.: 82.03%] [G loss: 3.379855]\n",
      "epoch:6 step:5319 [D loss: 0.122146, acc.: 96.88%] [G loss: 4.530147]\n",
      "epoch:6 step:5320 [D loss: 0.131188, acc.: 96.88%] [G loss: 3.807374]\n",
      "epoch:6 step:5321 [D loss: 0.132712, acc.: 95.31%] [G loss: 4.834332]\n",
      "epoch:6 step:5322 [D loss: 0.102965, acc.: 96.88%] [G loss: 5.398866]\n",
      "epoch:6 step:5323 [D loss: 0.530599, acc.: 78.12%] [G loss: 4.297791]\n",
      "epoch:6 step:5324 [D loss: 0.054701, acc.: 100.00%] [G loss: 3.784630]\n",
      "epoch:6 step:5325 [D loss: 0.271803, acc.: 91.41%] [G loss: 2.863762]\n",
      "epoch:6 step:5326 [D loss: 0.094821, acc.: 97.66%] [G loss: 4.472829]\n",
      "epoch:6 step:5327 [D loss: 0.085409, acc.: 99.22%] [G loss: 4.489165]\n",
      "epoch:6 step:5328 [D loss: 0.202407, acc.: 93.75%] [G loss: 4.152822]\n",
      "epoch:6 step:5329 [D loss: 0.124577, acc.: 96.09%] [G loss: 4.076987]\n",
      "epoch:6 step:5330 [D loss: 0.269847, acc.: 90.62%] [G loss: 4.678590]\n",
      "epoch:6 step:5331 [D loss: 0.122761, acc.: 96.09%] [G loss: 4.328692]\n",
      "epoch:6 step:5332 [D loss: 0.298855, acc.: 89.06%] [G loss: 6.294322]\n",
      "epoch:6 step:5333 [D loss: 0.242339, acc.: 89.06%] [G loss: 5.544247]\n",
      "epoch:6 step:5334 [D loss: 0.154124, acc.: 94.53%] [G loss: 3.558019]\n",
      "epoch:6 step:5335 [D loss: 0.184083, acc.: 91.41%] [G loss: 7.218301]\n",
      "epoch:6 step:5336 [D loss: 0.147383, acc.: 95.31%] [G loss: 5.207619]\n",
      "epoch:6 step:5337 [D loss: 0.030819, acc.: 100.00%] [G loss: 3.471288]\n",
      "epoch:6 step:5338 [D loss: 0.127845, acc.: 93.75%] [G loss: 7.455750]\n",
      "epoch:6 step:5339 [D loss: 0.154760, acc.: 94.53%] [G loss: 4.148201]\n",
      "epoch:6 step:5340 [D loss: 0.239070, acc.: 91.41%] [G loss: 5.963399]\n",
      "epoch:6 step:5341 [D loss: 0.052277, acc.: 99.22%] [G loss: 6.427016]\n",
      "epoch:6 step:5342 [D loss: 0.361526, acc.: 82.81%] [G loss: 2.142506]\n",
      "epoch:6 step:5343 [D loss: 0.274862, acc.: 87.50%] [G loss: 8.145345]\n",
      "epoch:6 step:5344 [D loss: 0.775458, acc.: 63.28%] [G loss: 3.627873]\n",
      "epoch:6 step:5345 [D loss: 0.048053, acc.: 100.00%] [G loss: 4.507203]\n",
      "epoch:6 step:5346 [D loss: 0.068477, acc.: 98.44%] [G loss: 4.965868]\n",
      "epoch:6 step:5347 [D loss: 0.667593, acc.: 66.41%] [G loss: 7.463899]\n",
      "epoch:6 step:5348 [D loss: 0.910525, acc.: 63.28%] [G loss: 5.229144]\n",
      "epoch:6 step:5349 [D loss: 0.034106, acc.: 99.22%] [G loss: 5.576540]\n",
      "epoch:6 step:5350 [D loss: 0.079977, acc.: 97.66%] [G loss: 3.795255]\n",
      "epoch:6 step:5351 [D loss: 0.186337, acc.: 91.41%] [G loss: 4.848122]\n",
      "epoch:6 step:5352 [D loss: 0.127308, acc.: 94.53%] [G loss: 3.404375]\n",
      "epoch:6 step:5353 [D loss: 0.210571, acc.: 91.41%] [G loss: 1.886380]\n",
      "epoch:6 step:5354 [D loss: 0.311649, acc.: 86.72%] [G loss: 4.617214]\n",
      "epoch:6 step:5355 [D loss: 1.105519, acc.: 47.66%] [G loss: 5.863573]\n",
      "epoch:6 step:5356 [D loss: 0.029104, acc.: 100.00%] [G loss: 6.250644]\n",
      "epoch:6 step:5357 [D loss: 0.135456, acc.: 95.31%] [G loss: 3.266811]\n",
      "epoch:6 step:5358 [D loss: 0.256463, acc.: 87.50%] [G loss: 3.503731]\n",
      "epoch:6 step:5359 [D loss: 0.104874, acc.: 95.31%] [G loss: 4.661268]\n",
      "epoch:6 step:5360 [D loss: 0.119826, acc.: 97.66%] [G loss: 4.410547]\n",
      "epoch:6 step:5361 [D loss: 0.219473, acc.: 89.84%] [G loss: 3.206150]\n",
      "epoch:6 step:5362 [D loss: 0.220052, acc.: 91.41%] [G loss: 5.297548]\n",
      "epoch:6 step:5363 [D loss: 0.252728, acc.: 87.50%] [G loss: 3.864606]\n",
      "epoch:6 step:5364 [D loss: 0.180906, acc.: 93.75%] [G loss: 2.683585]\n",
      "epoch:6 step:5365 [D loss: 0.110989, acc.: 96.88%] [G loss: 2.989117]\n",
      "epoch:6 step:5366 [D loss: 0.021881, acc.: 100.00%] [G loss: 2.755280]\n",
      "epoch:6 step:5367 [D loss: 0.138201, acc.: 96.88%] [G loss: 0.832753]\n",
      "epoch:6 step:5368 [D loss: 0.034774, acc.: 99.22%] [G loss: 0.440891]\n",
      "epoch:6 step:5369 [D loss: 0.656480, acc.: 65.62%] [G loss: 7.608868]\n",
      "epoch:6 step:5370 [D loss: 1.245228, acc.: 56.25%] [G loss: 1.866261]\n",
      "epoch:6 step:5371 [D loss: 0.129640, acc.: 95.31%] [G loss: 0.505205]\n",
      "epoch:6 step:5372 [D loss: 0.050665, acc.: 98.44%] [G loss: 1.140987]\n",
      "epoch:6 step:5373 [D loss: 0.018365, acc.: 100.00%] [G loss: 1.006055]\n",
      "epoch:6 step:5374 [D loss: 0.050558, acc.: 99.22%] [G loss: 0.901650]\n",
      "epoch:6 step:5375 [D loss: 0.011368, acc.: 100.00%] [G loss: 0.692938]\n",
      "epoch:6 step:5376 [D loss: 0.029130, acc.: 100.00%] [G loss: 0.582602]\n",
      "epoch:6 step:5377 [D loss: 0.160483, acc.: 96.09%] [G loss: 0.939111]\n",
      "epoch:6 step:5378 [D loss: 0.478610, acc.: 73.44%] [G loss: 0.053904]\n",
      "epoch:6 step:5379 [D loss: 0.476845, acc.: 73.44%] [G loss: 2.945721]\n",
      "epoch:6 step:5380 [D loss: 0.084588, acc.: 94.53%] [G loss: 5.016414]\n",
      "epoch:6 step:5381 [D loss: 0.208399, acc.: 88.28%] [G loss: 1.977630]\n",
      "epoch:6 step:5382 [D loss: 0.188588, acc.: 92.19%] [G loss: 1.919800]\n",
      "epoch:6 step:5383 [D loss: 0.222030, acc.: 90.62%] [G loss: 3.056052]\n",
      "epoch:6 step:5384 [D loss: 0.164487, acc.: 95.31%] [G loss: 3.869508]\n",
      "epoch:6 step:5385 [D loss: 0.219637, acc.: 90.62%] [G loss: 2.115458]\n",
      "epoch:6 step:5386 [D loss: 0.228619, acc.: 88.28%] [G loss: 4.838284]\n",
      "epoch:6 step:5387 [D loss: 0.098462, acc.: 97.66%] [G loss: 3.931547]\n",
      "epoch:6 step:5388 [D loss: 1.319342, acc.: 36.72%] [G loss: 4.943244]\n",
      "epoch:6 step:5389 [D loss: 0.271361, acc.: 88.28%] [G loss: 4.906832]\n",
      "epoch:6 step:5390 [D loss: 0.522774, acc.: 75.78%] [G loss: 4.098288]\n",
      "epoch:6 step:5391 [D loss: 0.028274, acc.: 100.00%] [G loss: 4.300308]\n",
      "epoch:6 step:5392 [D loss: 0.038461, acc.: 100.00%] [G loss: 3.683018]\n",
      "epoch:6 step:5393 [D loss: 0.085850, acc.: 100.00%] [G loss: 2.369824]\n",
      "epoch:6 step:5394 [D loss: 0.061064, acc.: 99.22%] [G loss: 3.551480]\n",
      "epoch:6 step:5395 [D loss: 0.062756, acc.: 100.00%] [G loss: 1.273749]\n",
      "epoch:6 step:5396 [D loss: 0.372646, acc.: 80.47%] [G loss: 3.363023]\n",
      "epoch:6 step:5397 [D loss: 0.855199, acc.: 57.81%] [G loss: 0.300258]\n",
      "epoch:6 step:5398 [D loss: 0.030097, acc.: 100.00%] [G loss: 1.392827]\n",
      "epoch:6 step:5399 [D loss: 0.340877, acc.: 82.03%] [G loss: 3.297753]\n",
      "epoch:6 step:5400 [D loss: 0.172299, acc.: 93.75%] [G loss: 3.088917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6 step:5401 [D loss: 0.107019, acc.: 97.66%] [G loss: 1.034278]\n",
      "epoch:6 step:5402 [D loss: 0.270100, acc.: 89.84%] [G loss: 3.171809]\n",
      "epoch:6 step:5403 [D loss: 0.123755, acc.: 97.66%] [G loss: 2.811987]\n",
      "epoch:6 step:5404 [D loss: 0.194281, acc.: 92.97%] [G loss: 1.821070]\n",
      "epoch:6 step:5405 [D loss: 0.091012, acc.: 98.44%] [G loss: 1.730269]\n",
      "epoch:6 step:5406 [D loss: 0.132875, acc.: 95.31%] [G loss: 1.840124]\n",
      "epoch:6 step:5407 [D loss: 0.642990, acc.: 63.28%] [G loss: 5.399939]\n",
      "epoch:6 step:5408 [D loss: 0.232108, acc.: 86.72%] [G loss: 4.078600]\n",
      "epoch:6 step:5409 [D loss: 0.105008, acc.: 97.66%] [G loss: 3.611397]\n",
      "epoch:6 step:5410 [D loss: 0.079867, acc.: 99.22%] [G loss: 3.844500]\n",
      "epoch:6 step:5411 [D loss: 0.332213, acc.: 87.50%] [G loss: 4.265730]\n",
      "epoch:6 step:5412 [D loss: 0.132108, acc.: 96.09%] [G loss: 4.920952]\n",
      "epoch:6 step:5413 [D loss: 0.143998, acc.: 94.53%] [G loss: 3.080531]\n",
      "epoch:6 step:5414 [D loss: 0.176402, acc.: 95.31%] [G loss: 3.485289]\n",
      "epoch:6 step:5415 [D loss: 0.131495, acc.: 96.88%] [G loss: 4.878985]\n",
      "epoch:6 step:5416 [D loss: 0.240802, acc.: 89.06%] [G loss: 3.167540]\n",
      "epoch:6 step:5417 [D loss: 0.059836, acc.: 99.22%] [G loss: 2.485230]\n",
      "epoch:6 step:5418 [D loss: 0.066142, acc.: 100.00%] [G loss: 2.549596]\n",
      "epoch:6 step:5419 [D loss: 0.244637, acc.: 92.19%] [G loss: 1.841576]\n",
      "epoch:6 step:5420 [D loss: 0.074391, acc.: 98.44%] [G loss: 1.376539]\n",
      "epoch:6 step:5421 [D loss: 0.095892, acc.: 96.88%] [G loss: 1.005641]\n",
      "epoch:6 step:5422 [D loss: 0.059226, acc.: 100.00%] [G loss: 1.592887]\n",
      "epoch:6 step:5423 [D loss: 0.192662, acc.: 93.75%] [G loss: 0.328614]\n",
      "epoch:6 step:5424 [D loss: 0.048559, acc.: 99.22%] [G loss: 0.818529]\n",
      "epoch:6 step:5425 [D loss: 0.503570, acc.: 74.22%] [G loss: 5.953640]\n",
      "epoch:6 step:5426 [D loss: 0.786003, acc.: 60.94%] [G loss: 1.930440]\n",
      "epoch:6 step:5427 [D loss: 0.131700, acc.: 93.75%] [G loss: 0.612865]\n",
      "epoch:6 step:5428 [D loss: 0.030244, acc.: 100.00%] [G loss: 0.345847]\n",
      "epoch:6 step:5429 [D loss: 0.091415, acc.: 98.44%] [G loss: 0.681732]\n",
      "epoch:6 step:5430 [D loss: 0.261994, acc.: 88.28%] [G loss: 4.370911]\n",
      "epoch:6 step:5431 [D loss: 0.450297, acc.: 78.91%] [G loss: 1.911019]\n",
      "epoch:6 step:5432 [D loss: 0.112257, acc.: 94.53%] [G loss: 2.339004]\n",
      "epoch:6 step:5433 [D loss: 0.066418, acc.: 99.22%] [G loss: 2.679539]\n",
      "epoch:6 step:5434 [D loss: 0.225514, acc.: 91.41%] [G loss: 1.123447]\n",
      "epoch:6 step:5435 [D loss: 0.354805, acc.: 78.91%] [G loss: 5.291005]\n",
      "epoch:6 step:5436 [D loss: 1.380630, acc.: 47.66%] [G loss: 3.394755]\n",
      "epoch:6 step:5437 [D loss: 0.111376, acc.: 96.88%] [G loss: 3.841283]\n",
      "epoch:6 step:5438 [D loss: 0.138057, acc.: 94.53%] [G loss: 2.570252]\n",
      "epoch:6 step:5439 [D loss: 0.531335, acc.: 75.00%] [G loss: 4.194414]\n",
      "epoch:6 step:5440 [D loss: 0.480780, acc.: 76.56%] [G loss: 3.533819]\n",
      "epoch:6 step:5441 [D loss: 0.114969, acc.: 96.88%] [G loss: 3.551932]\n",
      "epoch:6 step:5442 [D loss: 0.061734, acc.: 99.22%] [G loss: 3.919176]\n",
      "epoch:6 step:5443 [D loss: 0.170063, acc.: 93.75%] [G loss: 3.177793]\n",
      "epoch:6 step:5444 [D loss: 0.142534, acc.: 96.88%] [G loss: 3.573048]\n",
      "epoch:6 step:5445 [D loss: 0.244970, acc.: 92.19%] [G loss: 4.042074]\n",
      "epoch:6 step:5446 [D loss: 0.133928, acc.: 97.66%] [G loss: 4.725596]\n",
      "epoch:6 step:5447 [D loss: 0.121896, acc.: 96.09%] [G loss: 3.793665]\n",
      "epoch:6 step:5448 [D loss: 0.512630, acc.: 78.12%] [G loss: 4.695413]\n",
      "epoch:6 step:5449 [D loss: 0.108818, acc.: 97.66%] [G loss: 5.012054]\n",
      "epoch:6 step:5450 [D loss: 0.171050, acc.: 93.75%] [G loss: 2.913494]\n",
      "epoch:6 step:5451 [D loss: 0.263362, acc.: 90.62%] [G loss: 4.866224]\n",
      "epoch:6 step:5452 [D loss: 0.253296, acc.: 86.72%] [G loss: 2.367889]\n",
      "epoch:6 step:5453 [D loss: 0.424008, acc.: 76.56%] [G loss: 6.057222]\n",
      "epoch:6 step:5454 [D loss: 0.259771, acc.: 86.72%] [G loss: 4.485306]\n",
      "epoch:6 step:5455 [D loss: 0.132679, acc.: 96.09%] [G loss: 3.332856]\n",
      "epoch:6 step:5456 [D loss: 0.113577, acc.: 97.66%] [G loss: 2.432694]\n",
      "epoch:6 step:5457 [D loss: 0.138190, acc.: 97.66%] [G loss: 2.988540]\n",
      "epoch:6 step:5458 [D loss: 0.102953, acc.: 98.44%] [G loss: 2.206199]\n",
      "epoch:6 step:5459 [D loss: 0.462522, acc.: 73.44%] [G loss: 6.250986]\n",
      "epoch:6 step:5460 [D loss: 0.995048, acc.: 57.81%] [G loss: 1.665681]\n",
      "epoch:6 step:5461 [D loss: 0.074977, acc.: 99.22%] [G loss: 2.761744]\n",
      "epoch:6 step:5462 [D loss: 0.126958, acc.: 96.09%] [G loss: 2.847775]\n",
      "epoch:6 step:5463 [D loss: 0.154549, acc.: 97.66%] [G loss: 1.597160]\n",
      "epoch:6 step:5464 [D loss: 0.122088, acc.: 96.88%] [G loss: 3.154692]\n",
      "epoch:6 step:5465 [D loss: 1.567865, acc.: 32.03%] [G loss: 5.726811]\n",
      "epoch:6 step:5466 [D loss: 1.246967, acc.: 53.12%] [G loss: 3.877889]\n",
      "epoch:6 step:5467 [D loss: 0.388135, acc.: 82.81%] [G loss: 3.586400]\n",
      "epoch:7 step:5468 [D loss: 0.141863, acc.: 96.09%] [G loss: 4.141821]\n",
      "epoch:7 step:5469 [D loss: 0.163805, acc.: 96.09%] [G loss: 3.026117]\n",
      "epoch:7 step:5470 [D loss: 0.190531, acc.: 93.75%] [G loss: 3.767884]\n",
      "epoch:7 step:5471 [D loss: 0.156086, acc.: 95.31%] [G loss: 4.026433]\n",
      "epoch:7 step:5472 [D loss: 0.197343, acc.: 95.31%] [G loss: 3.261089]\n",
      "epoch:7 step:5473 [D loss: 0.363872, acc.: 85.16%] [G loss: 2.899271]\n",
      "epoch:7 step:5474 [D loss: 0.219793, acc.: 92.97%] [G loss: 3.339456]\n",
      "epoch:7 step:5475 [D loss: 0.130420, acc.: 98.44%] [G loss: 3.915950]\n",
      "epoch:7 step:5476 [D loss: 0.173723, acc.: 96.09%] [G loss: 3.407821]\n",
      "epoch:7 step:5477 [D loss: 0.273056, acc.: 89.84%] [G loss: 3.601612]\n",
      "epoch:7 step:5478 [D loss: 0.145567, acc.: 96.09%] [G loss: 3.677258]\n",
      "epoch:7 step:5479 [D loss: 0.344788, acc.: 85.16%] [G loss: 4.699800]\n",
      "epoch:7 step:5480 [D loss: 0.148190, acc.: 95.31%] [G loss: 3.609895]\n",
      "epoch:7 step:5481 [D loss: 0.141013, acc.: 95.31%] [G loss: 1.594010]\n",
      "epoch:7 step:5482 [D loss: 0.155379, acc.: 95.31%] [G loss: 2.785194]\n",
      "epoch:7 step:5483 [D loss: 0.222563, acc.: 91.41%] [G loss: 3.776960]\n",
      "epoch:7 step:5484 [D loss: 0.133183, acc.: 96.09%] [G loss: 2.737914]\n",
      "epoch:7 step:5485 [D loss: 0.304057, acc.: 87.50%] [G loss: 4.487526]\n",
      "epoch:7 step:5486 [D loss: 0.468601, acc.: 83.59%] [G loss: 4.232264]\n",
      "epoch:7 step:5487 [D loss: 0.191796, acc.: 93.75%] [G loss: 4.791301]\n",
      "epoch:7 step:5488 [D loss: 0.118822, acc.: 97.66%] [G loss: 3.712142]\n",
      "epoch:7 step:5489 [D loss: 0.303556, acc.: 82.03%] [G loss: 4.820591]\n",
      "epoch:7 step:5490 [D loss: 0.174229, acc.: 94.53%] [G loss: 3.953269]\n",
      "epoch:7 step:5491 [D loss: 0.057480, acc.: 99.22%] [G loss: 4.093029]\n",
      "epoch:7 step:5492 [D loss: 0.125670, acc.: 97.66%] [G loss: 3.351939]\n",
      "epoch:7 step:5493 [D loss: 0.155308, acc.: 95.31%] [G loss: 3.700103]\n",
      "epoch:7 step:5494 [D loss: 0.082422, acc.: 100.00%] [G loss: 3.802285]\n",
      "epoch:7 step:5495 [D loss: 0.383785, acc.: 85.94%] [G loss: 3.687743]\n",
      "epoch:7 step:5496 [D loss: 0.081948, acc.: 99.22%] [G loss: 3.559840]\n",
      "epoch:7 step:5497 [D loss: 0.347406, acc.: 84.38%] [G loss: 4.272738]\n",
      "epoch:7 step:5498 [D loss: 0.219481, acc.: 92.97%] [G loss: 3.134304]\n",
      "epoch:7 step:5499 [D loss: 0.202614, acc.: 93.75%] [G loss: 5.664100]\n",
      "epoch:7 step:5500 [D loss: 0.542181, acc.: 74.22%] [G loss: 5.631751]\n",
      "epoch:7 step:5501 [D loss: 0.084002, acc.: 98.44%] [G loss: 5.198619]\n",
      "epoch:7 step:5502 [D loss: 0.236635, acc.: 90.62%] [G loss: 2.761784]\n",
      "epoch:7 step:5503 [D loss: 0.413801, acc.: 80.47%] [G loss: 6.667559]\n",
      "epoch:7 step:5504 [D loss: 0.238857, acc.: 87.50%] [G loss: 5.042271]\n",
      "epoch:7 step:5505 [D loss: 0.098098, acc.: 95.31%] [G loss: 3.645395]\n",
      "epoch:7 step:5506 [D loss: 0.203605, acc.: 91.41%] [G loss: 6.099567]\n",
      "epoch:7 step:5507 [D loss: 0.177830, acc.: 90.62%] [G loss: 4.288169]\n",
      "epoch:7 step:5508 [D loss: 0.121978, acc.: 95.31%] [G loss: 5.287793]\n",
      "epoch:7 step:5509 [D loss: 0.060520, acc.: 99.22%] [G loss: 3.997099]\n",
      "epoch:7 step:5510 [D loss: 0.544731, acc.: 71.09%] [G loss: 4.565568]\n",
      "epoch:7 step:5511 [D loss: 1.189996, acc.: 50.78%] [G loss: 5.044013]\n",
      "epoch:7 step:5512 [D loss: 0.233646, acc.: 89.06%] [G loss: 3.502065]\n",
      "epoch:7 step:5513 [D loss: 0.143496, acc.: 98.44%] [G loss: 3.263390]\n",
      "epoch:7 step:5514 [D loss: 0.088901, acc.: 98.44%] [G loss: 4.545422]\n",
      "epoch:7 step:5515 [D loss: 0.222512, acc.: 93.75%] [G loss: 3.201212]\n",
      "epoch:7 step:5516 [D loss: 0.351939, acc.: 82.81%] [G loss: 4.520542]\n",
      "epoch:7 step:5517 [D loss: 0.256254, acc.: 89.84%] [G loss: 3.571375]\n",
      "epoch:7 step:5518 [D loss: 0.067416, acc.: 99.22%] [G loss: 4.223119]\n",
      "epoch:7 step:5519 [D loss: 0.073782, acc.: 98.44%] [G loss: 3.622584]\n",
      "epoch:7 step:5520 [D loss: 0.190467, acc.: 94.53%] [G loss: 4.178665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5521 [D loss: 0.135999, acc.: 93.75%] [G loss: 3.017841]\n",
      "epoch:7 step:5522 [D loss: 0.075154, acc.: 99.22%] [G loss: 1.800721]\n",
      "epoch:7 step:5523 [D loss: 0.267320, acc.: 88.28%] [G loss: 4.965553]\n",
      "epoch:7 step:5524 [D loss: 0.274067, acc.: 87.50%] [G loss: 2.178310]\n",
      "epoch:7 step:5525 [D loss: 0.102787, acc.: 96.09%] [G loss: 3.030357]\n",
      "epoch:7 step:5526 [D loss: 0.013131, acc.: 100.00%] [G loss: 2.233632]\n",
      "epoch:7 step:5527 [D loss: 0.038883, acc.: 99.22%] [G loss: 1.478127]\n",
      "epoch:7 step:5528 [D loss: 0.281797, acc.: 90.62%] [G loss: 4.511407]\n",
      "epoch:7 step:5529 [D loss: 0.554868, acc.: 72.66%] [G loss: 1.394523]\n",
      "epoch:7 step:5530 [D loss: 0.103469, acc.: 96.09%] [G loss: 2.682055]\n",
      "epoch:7 step:5531 [D loss: 0.430429, acc.: 81.25%] [G loss: 7.499041]\n",
      "epoch:7 step:5532 [D loss: 1.275122, acc.: 56.25%] [G loss: 1.762602]\n",
      "epoch:7 step:5533 [D loss: 0.470729, acc.: 77.34%] [G loss: 5.518434]\n",
      "epoch:7 step:5534 [D loss: 0.881043, acc.: 60.94%] [G loss: 2.095158]\n",
      "epoch:7 step:5535 [D loss: 0.190696, acc.: 92.19%] [G loss: 3.598752]\n",
      "epoch:7 step:5536 [D loss: 0.026443, acc.: 100.00%] [G loss: 3.802797]\n",
      "epoch:7 step:5537 [D loss: 0.081634, acc.: 100.00%] [G loss: 3.737798]\n",
      "epoch:7 step:5538 [D loss: 0.127052, acc.: 93.75%] [G loss: 3.150319]\n",
      "epoch:7 step:5539 [D loss: 0.121722, acc.: 98.44%] [G loss: 1.333596]\n",
      "epoch:7 step:5540 [D loss: 0.070520, acc.: 99.22%] [G loss: 1.304940]\n",
      "epoch:7 step:5541 [D loss: 0.041133, acc.: 100.00%] [G loss: 0.916109]\n",
      "epoch:7 step:5542 [D loss: 0.075551, acc.: 100.00%] [G loss: 1.031754]\n",
      "epoch:7 step:5543 [D loss: 0.096463, acc.: 98.44%] [G loss: 1.658611]\n",
      "epoch:7 step:5544 [D loss: 0.506130, acc.: 78.12%] [G loss: 0.953502]\n",
      "epoch:7 step:5545 [D loss: 0.054867, acc.: 99.22%] [G loss: 1.443823]\n",
      "epoch:7 step:5546 [D loss: 0.101233, acc.: 95.31%] [G loss: 0.396851]\n",
      "epoch:7 step:5547 [D loss: 0.728892, acc.: 61.72%] [G loss: 6.567214]\n",
      "epoch:7 step:5548 [D loss: 0.626377, acc.: 70.31%] [G loss: 5.289993]\n",
      "epoch:7 step:5549 [D loss: 0.169657, acc.: 92.97%] [G loss: 2.326107]\n",
      "epoch:7 step:5550 [D loss: 0.097170, acc.: 96.09%] [G loss: 2.177964]\n",
      "epoch:7 step:5551 [D loss: 0.045526, acc.: 98.44%] [G loss: 2.380667]\n",
      "epoch:7 step:5552 [D loss: 0.053947, acc.: 99.22%] [G loss: 2.538524]\n",
      "epoch:7 step:5553 [D loss: 0.273616, acc.: 89.06%] [G loss: 3.804734]\n",
      "epoch:7 step:5554 [D loss: 0.053608, acc.: 100.00%] [G loss: 3.759507]\n",
      "epoch:7 step:5555 [D loss: 0.264920, acc.: 86.72%] [G loss: 3.002392]\n",
      "epoch:7 step:5556 [D loss: 0.593368, acc.: 65.62%] [G loss: 6.549729]\n",
      "epoch:7 step:5557 [D loss: 0.947085, acc.: 61.72%] [G loss: 4.060847]\n",
      "epoch:7 step:5558 [D loss: 0.102597, acc.: 98.44%] [G loss: 2.960892]\n",
      "epoch:7 step:5559 [D loss: 0.036237, acc.: 100.00%] [G loss: 2.504239]\n",
      "epoch:7 step:5560 [D loss: 0.105330, acc.: 99.22%] [G loss: 4.138322]\n",
      "epoch:7 step:5561 [D loss: 0.142430, acc.: 96.09%] [G loss: 1.751706]\n",
      "epoch:7 step:5562 [D loss: 0.365392, acc.: 81.25%] [G loss: 3.960282]\n",
      "epoch:7 step:5563 [D loss: 0.487794, acc.: 74.22%] [G loss: 1.108179]\n",
      "epoch:7 step:5564 [D loss: 0.200512, acc.: 91.41%] [G loss: 2.042868]\n",
      "epoch:7 step:5565 [D loss: 0.030168, acc.: 100.00%] [G loss: 3.406929]\n",
      "epoch:7 step:5566 [D loss: 0.098585, acc.: 98.44%] [G loss: 1.526709]\n",
      "epoch:7 step:5567 [D loss: 0.422726, acc.: 80.47%] [G loss: 3.727104]\n",
      "epoch:7 step:5568 [D loss: 0.120961, acc.: 94.53%] [G loss: 4.189779]\n",
      "epoch:7 step:5569 [D loss: 0.333311, acc.: 87.50%] [G loss: 0.379693]\n",
      "epoch:7 step:5570 [D loss: 1.355081, acc.: 53.12%] [G loss: 6.938428]\n",
      "epoch:7 step:5571 [D loss: 1.664543, acc.: 51.56%] [G loss: 3.234120]\n",
      "epoch:7 step:5572 [D loss: 0.318395, acc.: 85.16%] [G loss: 2.590662]\n",
      "epoch:7 step:5573 [D loss: 0.108427, acc.: 98.44%] [G loss: 2.908380]\n",
      "epoch:7 step:5574 [D loss: 0.420796, acc.: 79.69%] [G loss: 3.704214]\n",
      "epoch:7 step:5575 [D loss: 0.630841, acc.: 63.28%] [G loss: 3.348612]\n",
      "epoch:7 step:5576 [D loss: 0.184698, acc.: 92.97%] [G loss: 3.682626]\n",
      "epoch:7 step:5577 [D loss: 0.326898, acc.: 87.50%] [G loss: 4.102232]\n",
      "epoch:7 step:5578 [D loss: 0.183670, acc.: 94.53%] [G loss: 2.919161]\n",
      "epoch:7 step:5579 [D loss: 0.513922, acc.: 72.66%] [G loss: 4.425263]\n",
      "epoch:7 step:5580 [D loss: 0.560393, acc.: 73.44%] [G loss: 3.753972]\n",
      "epoch:7 step:5581 [D loss: 0.313672, acc.: 84.38%] [G loss: 3.149998]\n",
      "epoch:7 step:5582 [D loss: 0.112977, acc.: 97.66%] [G loss: 2.613832]\n",
      "epoch:7 step:5583 [D loss: 0.611654, acc.: 65.62%] [G loss: 4.170330]\n",
      "epoch:7 step:5584 [D loss: 0.213777, acc.: 92.97%] [G loss: 4.049526]\n",
      "epoch:7 step:5585 [D loss: 0.156150, acc.: 96.88%] [G loss: 3.396430]\n",
      "epoch:7 step:5586 [D loss: 0.193228, acc.: 93.75%] [G loss: 4.103012]\n",
      "epoch:7 step:5587 [D loss: 0.168810, acc.: 95.31%] [G loss: 3.284309]\n",
      "epoch:7 step:5588 [D loss: 0.166513, acc.: 96.09%] [G loss: 2.821480]\n",
      "epoch:7 step:5589 [D loss: 0.226940, acc.: 93.75%] [G loss: 3.573530]\n",
      "epoch:7 step:5590 [D loss: 0.375029, acc.: 85.16%] [G loss: 2.945944]\n",
      "epoch:7 step:5591 [D loss: 0.299719, acc.: 85.94%] [G loss: 4.069953]\n",
      "epoch:7 step:5592 [D loss: 0.127946, acc.: 94.53%] [G loss: 3.690564]\n",
      "epoch:7 step:5593 [D loss: 0.131666, acc.: 97.66%] [G loss: 3.733781]\n",
      "epoch:7 step:5594 [D loss: 0.066987, acc.: 99.22%] [G loss: 3.744997]\n",
      "epoch:7 step:5595 [D loss: 0.417653, acc.: 79.69%] [G loss: 3.292950]\n",
      "epoch:7 step:5596 [D loss: 0.069839, acc.: 99.22%] [G loss: 3.472478]\n",
      "epoch:7 step:5597 [D loss: 0.167450, acc.: 96.09%] [G loss: 3.623314]\n",
      "epoch:7 step:5598 [D loss: 0.063684, acc.: 99.22%] [G loss: 3.287965]\n",
      "epoch:7 step:5599 [D loss: 0.379289, acc.: 84.38%] [G loss: 4.211763]\n",
      "epoch:7 step:5600 [D loss: 0.128495, acc.: 97.66%] [G loss: 4.095107]\n",
      "epoch:7 step:5601 [D loss: 0.157975, acc.: 96.88%] [G loss: 2.873715]\n",
      "epoch:7 step:5602 [D loss: 0.117771, acc.: 96.88%] [G loss: 3.730543]\n",
      "epoch:7 step:5603 [D loss: 0.464251, acc.: 75.78%] [G loss: 4.471606]\n",
      "epoch:7 step:5604 [D loss: 0.111306, acc.: 96.88%] [G loss: 4.513324]\n",
      "epoch:7 step:5605 [D loss: 0.385694, acc.: 85.16%] [G loss: 4.170897]\n",
      "epoch:7 step:5606 [D loss: 0.173762, acc.: 96.09%] [G loss: 3.637556]\n",
      "epoch:7 step:5607 [D loss: 0.114531, acc.: 99.22%] [G loss: 4.071418]\n",
      "epoch:7 step:5608 [D loss: 0.047192, acc.: 100.00%] [G loss: 3.108958]\n",
      "epoch:7 step:5609 [D loss: 0.166538, acc.: 95.31%] [G loss: 3.633106]\n",
      "epoch:7 step:5610 [D loss: 0.214672, acc.: 92.19%] [G loss: 4.324598]\n",
      "epoch:7 step:5611 [D loss: 0.205966, acc.: 91.41%] [G loss: 3.147423]\n",
      "epoch:7 step:5612 [D loss: 0.329959, acc.: 82.03%] [G loss: 4.901403]\n",
      "epoch:7 step:5613 [D loss: 0.324227, acc.: 88.28%] [G loss: 3.242630]\n",
      "epoch:7 step:5614 [D loss: 0.091460, acc.: 98.44%] [G loss: 4.437793]\n",
      "epoch:7 step:5615 [D loss: 0.057136, acc.: 99.22%] [G loss: 3.359574]\n",
      "epoch:7 step:5616 [D loss: 0.215325, acc.: 92.19%] [G loss: 3.915192]\n",
      "epoch:7 step:5617 [D loss: 0.086260, acc.: 99.22%] [G loss: 4.351694]\n",
      "epoch:7 step:5618 [D loss: 0.125797, acc.: 98.44%] [G loss: 2.786909]\n",
      "epoch:7 step:5619 [D loss: 0.120007, acc.: 99.22%] [G loss: 2.824173]\n",
      "epoch:7 step:5620 [D loss: 1.018057, acc.: 48.44%] [G loss: 7.037531]\n",
      "epoch:7 step:5621 [D loss: 0.988555, acc.: 58.59%] [G loss: 4.660507]\n",
      "epoch:7 step:5622 [D loss: 0.279206, acc.: 88.28%] [G loss: 3.120714]\n",
      "epoch:7 step:5623 [D loss: 0.187872, acc.: 93.75%] [G loss: 2.802512]\n",
      "epoch:7 step:5624 [D loss: 0.076508, acc.: 98.44%] [G loss: 3.630971]\n",
      "epoch:7 step:5625 [D loss: 0.425959, acc.: 78.12%] [G loss: 4.184381]\n",
      "epoch:7 step:5626 [D loss: 0.134402, acc.: 97.66%] [G loss: 3.953262]\n",
      "epoch:7 step:5627 [D loss: 0.142842, acc.: 96.09%] [G loss: 4.057477]\n",
      "epoch:7 step:5628 [D loss: 0.198252, acc.: 94.53%] [G loss: 3.092873]\n",
      "epoch:7 step:5629 [D loss: 0.086995, acc.: 98.44%] [G loss: 3.614305]\n",
      "epoch:7 step:5630 [D loss: 0.179174, acc.: 94.53%] [G loss: 3.460722]\n",
      "epoch:7 step:5631 [D loss: 0.329257, acc.: 84.38%] [G loss: 4.052946]\n",
      "epoch:7 step:5632 [D loss: 0.136620, acc.: 96.88%] [G loss: 3.807457]\n",
      "epoch:7 step:5633 [D loss: 0.154751, acc.: 96.09%] [G loss: 3.406009]\n",
      "epoch:7 step:5634 [D loss: 0.291832, acc.: 88.28%] [G loss: 3.542980]\n",
      "epoch:7 step:5635 [D loss: 0.127161, acc.: 96.88%] [G loss: 3.006004]\n",
      "epoch:7 step:5636 [D loss: 0.148750, acc.: 95.31%] [G loss: 3.573170]\n",
      "epoch:7 step:5637 [D loss: 0.421699, acc.: 82.81%] [G loss: 6.086057]\n",
      "epoch:7 step:5638 [D loss: 0.306367, acc.: 85.16%] [G loss: 2.361941]\n",
      "epoch:7 step:5639 [D loss: 0.233659, acc.: 89.84%] [G loss: 5.043104]\n",
      "epoch:7 step:5640 [D loss: 0.340155, acc.: 84.38%] [G loss: 2.954542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5641 [D loss: 0.351671, acc.: 81.25%] [G loss: 6.538482]\n",
      "epoch:7 step:5642 [D loss: 0.280407, acc.: 87.50%] [G loss: 3.546260]\n",
      "epoch:7 step:5643 [D loss: 0.544095, acc.: 74.22%] [G loss: 4.704641]\n",
      "epoch:7 step:5644 [D loss: 0.144066, acc.: 94.53%] [G loss: 3.563941]\n",
      "epoch:7 step:5645 [D loss: 0.412928, acc.: 79.69%] [G loss: 5.428734]\n",
      "epoch:7 step:5646 [D loss: 0.258743, acc.: 88.28%] [G loss: 3.560359]\n",
      "epoch:7 step:5647 [D loss: 0.253388, acc.: 92.19%] [G loss: 4.530193]\n",
      "epoch:7 step:5648 [D loss: 0.129781, acc.: 98.44%] [G loss: 4.472307]\n",
      "epoch:7 step:5649 [D loss: 0.147783, acc.: 93.75%] [G loss: 3.547126]\n",
      "epoch:7 step:5650 [D loss: 0.317967, acc.: 85.16%] [G loss: 5.139737]\n",
      "epoch:7 step:5651 [D loss: 0.351548, acc.: 83.59%] [G loss: 3.482761]\n",
      "epoch:7 step:5652 [D loss: 0.155591, acc.: 94.53%] [G loss: 2.041477]\n",
      "epoch:7 step:5653 [D loss: 0.274047, acc.: 86.72%] [G loss: 4.525192]\n",
      "epoch:7 step:5654 [D loss: 0.269190, acc.: 88.28%] [G loss: 3.322443]\n",
      "epoch:7 step:5655 [D loss: 0.320180, acc.: 82.81%] [G loss: 5.769181]\n",
      "epoch:7 step:5656 [D loss: 0.251268, acc.: 89.06%] [G loss: 4.150076]\n",
      "epoch:7 step:5657 [D loss: 0.266829, acc.: 89.06%] [G loss: 4.824616]\n",
      "epoch:7 step:5658 [D loss: 0.160482, acc.: 94.53%] [G loss: 4.432592]\n",
      "epoch:7 step:5659 [D loss: 0.150636, acc.: 94.53%] [G loss: 4.868546]\n",
      "epoch:7 step:5660 [D loss: 0.231119, acc.: 93.75%] [G loss: 2.944195]\n",
      "epoch:7 step:5661 [D loss: 0.200359, acc.: 91.41%] [G loss: 5.373869]\n",
      "epoch:7 step:5662 [D loss: 0.178297, acc.: 93.75%] [G loss: 3.471314]\n",
      "epoch:7 step:5663 [D loss: 0.223459, acc.: 93.75%] [G loss: 3.621002]\n",
      "epoch:7 step:5664 [D loss: 0.025605, acc.: 100.00%] [G loss: 4.384631]\n",
      "epoch:7 step:5665 [D loss: 0.046644, acc.: 100.00%] [G loss: 3.755266]\n",
      "epoch:7 step:5666 [D loss: 0.070621, acc.: 99.22%] [G loss: 3.110183]\n",
      "epoch:7 step:5667 [D loss: 0.087451, acc.: 97.66%] [G loss: 0.643962]\n",
      "epoch:7 step:5668 [D loss: 0.472518, acc.: 77.34%] [G loss: 4.517106]\n",
      "epoch:7 step:5669 [D loss: 0.568042, acc.: 71.09%] [G loss: 0.985881]\n",
      "epoch:7 step:5670 [D loss: 0.154927, acc.: 95.31%] [G loss: 2.882030]\n",
      "epoch:7 step:5671 [D loss: 0.143606, acc.: 96.09%] [G loss: 1.844761]\n",
      "epoch:7 step:5672 [D loss: 0.086563, acc.: 98.44%] [G loss: 1.593160]\n",
      "epoch:7 step:5673 [D loss: 0.102830, acc.: 98.44%] [G loss: 2.345711]\n",
      "epoch:7 step:5674 [D loss: 0.345561, acc.: 84.38%] [G loss: 6.462070]\n",
      "epoch:7 step:5675 [D loss: 2.245636, acc.: 47.66%] [G loss: 1.695744]\n",
      "epoch:7 step:5676 [D loss: 0.670121, acc.: 70.31%] [G loss: 5.618961]\n",
      "epoch:7 step:5677 [D loss: 0.433455, acc.: 77.34%] [G loss: 4.629048]\n",
      "epoch:7 step:5678 [D loss: 0.096149, acc.: 98.44%] [G loss: 2.562386]\n",
      "epoch:7 step:5679 [D loss: 0.087705, acc.: 100.00%] [G loss: 2.781799]\n",
      "epoch:7 step:5680 [D loss: 0.116814, acc.: 98.44%] [G loss: 2.549893]\n",
      "epoch:7 step:5681 [D loss: 0.781550, acc.: 64.06%] [G loss: 4.878983]\n",
      "epoch:7 step:5682 [D loss: 0.801450, acc.: 63.28%] [G loss: 2.712795]\n",
      "epoch:7 step:5683 [D loss: 0.145600, acc.: 96.09%] [G loss: 2.459734]\n",
      "epoch:7 step:5684 [D loss: 0.077923, acc.: 100.00%] [G loss: 1.809259]\n",
      "epoch:7 step:5685 [D loss: 0.175514, acc.: 94.53%] [G loss: 2.377845]\n",
      "epoch:7 step:5686 [D loss: 0.069721, acc.: 100.00%] [G loss: 2.412051]\n",
      "epoch:7 step:5687 [D loss: 0.128272, acc.: 96.09%] [G loss: 1.064146]\n",
      "epoch:7 step:5688 [D loss: 0.164303, acc.: 92.97%] [G loss: 2.764841]\n",
      "epoch:7 step:5689 [D loss: 0.082861, acc.: 100.00%] [G loss: 1.333244]\n",
      "epoch:7 step:5690 [D loss: 1.180476, acc.: 50.78%] [G loss: 5.113720]\n",
      "epoch:7 step:5691 [D loss: 1.473555, acc.: 52.34%] [G loss: 2.650959]\n",
      "epoch:7 step:5692 [D loss: 0.765469, acc.: 61.72%] [G loss: 3.087294]\n",
      "epoch:7 step:5693 [D loss: 0.348072, acc.: 85.94%] [G loss: 3.108492]\n",
      "epoch:7 step:5694 [D loss: 0.219049, acc.: 92.19%] [G loss: 2.133120]\n",
      "epoch:7 step:5695 [D loss: 0.110584, acc.: 98.44%] [G loss: 2.486509]\n",
      "epoch:7 step:5696 [D loss: 0.153481, acc.: 98.44%] [G loss: 2.345581]\n",
      "epoch:7 step:5697 [D loss: 0.118256, acc.: 98.44%] [G loss: 3.251131]\n",
      "epoch:7 step:5698 [D loss: 0.116151, acc.: 98.44%] [G loss: 2.204729]\n",
      "epoch:7 step:5699 [D loss: 0.533376, acc.: 71.88%] [G loss: 3.554469]\n",
      "epoch:7 step:5700 [D loss: 0.045960, acc.: 99.22%] [G loss: 3.766338]\n",
      "epoch:7 step:5701 [D loss: 0.241289, acc.: 90.62%] [G loss: 1.774617]\n",
      "epoch:7 step:5702 [D loss: 0.121446, acc.: 96.88%] [G loss: 2.296284]\n",
      "epoch:7 step:5703 [D loss: 0.137244, acc.: 96.88%] [G loss: 2.646867]\n",
      "epoch:7 step:5704 [D loss: 0.108224, acc.: 97.66%] [G loss: 1.810733]\n",
      "epoch:7 step:5705 [D loss: 0.452260, acc.: 74.22%] [G loss: 4.337255]\n",
      "epoch:7 step:5706 [D loss: 0.263806, acc.: 89.06%] [G loss: 3.508190]\n",
      "epoch:7 step:5707 [D loss: 0.769619, acc.: 60.16%] [G loss: 3.012970]\n",
      "epoch:7 step:5708 [D loss: 0.245280, acc.: 92.97%] [G loss: 3.613770]\n",
      "epoch:7 step:5709 [D loss: 0.209596, acc.: 92.19%] [G loss: 2.190075]\n",
      "epoch:7 step:5710 [D loss: 0.246554, acc.: 89.84%] [G loss: 3.935520]\n",
      "epoch:7 step:5711 [D loss: 0.099080, acc.: 98.44%] [G loss: 4.241847]\n",
      "epoch:7 step:5712 [D loss: 0.265413, acc.: 89.84%] [G loss: 2.056765]\n",
      "epoch:7 step:5713 [D loss: 0.188225, acc.: 95.31%] [G loss: 3.733078]\n",
      "epoch:7 step:5714 [D loss: 0.198932, acc.: 95.31%] [G loss: 3.176010]\n",
      "epoch:7 step:5715 [D loss: 0.145477, acc.: 95.31%] [G loss: 4.108193]\n",
      "epoch:7 step:5716 [D loss: 0.194965, acc.: 94.53%] [G loss: 3.412236]\n",
      "epoch:7 step:5717 [D loss: 0.268480, acc.: 88.28%] [G loss: 3.074944]\n",
      "epoch:7 step:5718 [D loss: 0.053138, acc.: 100.00%] [G loss: 3.071526]\n",
      "epoch:7 step:5719 [D loss: 0.204624, acc.: 92.19%] [G loss: 2.885175]\n",
      "epoch:7 step:5720 [D loss: 0.136286, acc.: 96.88%] [G loss: 2.608913]\n",
      "epoch:7 step:5721 [D loss: 0.098422, acc.: 96.88%] [G loss: 3.435096]\n",
      "epoch:7 step:5722 [D loss: 0.231208, acc.: 90.62%] [G loss: 2.193229]\n",
      "epoch:7 step:5723 [D loss: 0.149815, acc.: 97.66%] [G loss: 1.449973]\n",
      "epoch:7 step:5724 [D loss: 0.240332, acc.: 89.06%] [G loss: 4.143612]\n",
      "epoch:7 step:5725 [D loss: 0.315873, acc.: 82.81%] [G loss: 1.435255]\n",
      "epoch:7 step:5726 [D loss: 0.179695, acc.: 92.97%] [G loss: 1.407673]\n",
      "epoch:7 step:5727 [D loss: 0.154809, acc.: 96.09%] [G loss: 2.984125]\n",
      "epoch:7 step:5728 [D loss: 0.272677, acc.: 89.06%] [G loss: 2.013986]\n",
      "epoch:7 step:5729 [D loss: 0.076302, acc.: 97.66%] [G loss: 2.551094]\n",
      "epoch:7 step:5730 [D loss: 0.048324, acc.: 100.00%] [G loss: 3.228455]\n",
      "epoch:7 step:5731 [D loss: 0.269285, acc.: 91.41%] [G loss: 2.044122]\n",
      "epoch:7 step:5732 [D loss: 0.231669, acc.: 91.41%] [G loss: 1.207702]\n",
      "epoch:7 step:5733 [D loss: 0.252867, acc.: 89.06%] [G loss: 4.615240]\n",
      "epoch:7 step:5734 [D loss: 0.245998, acc.: 87.50%] [G loss: 2.698491]\n",
      "epoch:7 step:5735 [D loss: 0.051138, acc.: 99.22%] [G loss: 1.664882]\n",
      "epoch:7 step:5736 [D loss: 0.216770, acc.: 92.97%] [G loss: 3.281062]\n",
      "epoch:7 step:5737 [D loss: 0.994470, acc.: 55.47%] [G loss: 2.480327]\n",
      "epoch:7 step:5738 [D loss: 0.069438, acc.: 98.44%] [G loss: 3.950562]\n",
      "epoch:7 step:5739 [D loss: 0.057852, acc.: 97.66%] [G loss: 3.342193]\n",
      "epoch:7 step:5740 [D loss: 0.080541, acc.: 99.22%] [G loss: 1.696456]\n",
      "epoch:7 step:5741 [D loss: 0.276871, acc.: 89.06%] [G loss: 4.318702]\n",
      "epoch:7 step:5742 [D loss: 0.877852, acc.: 62.50%] [G loss: 3.205042]\n",
      "epoch:7 step:5743 [D loss: 0.098934, acc.: 97.66%] [G loss: 3.697474]\n",
      "epoch:7 step:5744 [D loss: 0.417351, acc.: 80.47%] [G loss: 3.506876]\n",
      "epoch:7 step:5745 [D loss: 0.025275, acc.: 100.00%] [G loss: 4.954727]\n",
      "epoch:7 step:5746 [D loss: 0.078315, acc.: 99.22%] [G loss: 3.950405]\n",
      "epoch:7 step:5747 [D loss: 0.079904, acc.: 97.66%] [G loss: 3.692551]\n",
      "epoch:7 step:5748 [D loss: 0.085222, acc.: 98.44%] [G loss: 3.911318]\n",
      "epoch:7 step:5749 [D loss: 0.076404, acc.: 99.22%] [G loss: 3.955066]\n",
      "epoch:7 step:5750 [D loss: 0.187498, acc.: 93.75%] [G loss: 3.775690]\n",
      "epoch:7 step:5751 [D loss: 0.082652, acc.: 99.22%] [G loss: 3.367523]\n",
      "epoch:7 step:5752 [D loss: 0.116614, acc.: 96.88%] [G loss: 3.008497]\n",
      "epoch:7 step:5753 [D loss: 0.646058, acc.: 67.97%] [G loss: 1.649953]\n",
      "epoch:7 step:5754 [D loss: 0.029479, acc.: 99.22%] [G loss: 1.913370]\n",
      "epoch:7 step:5755 [D loss: 0.041605, acc.: 100.00%] [G loss: 1.325022]\n",
      "epoch:7 step:5756 [D loss: 0.051168, acc.: 100.00%] [G loss: 0.247374]\n",
      "epoch:7 step:5757 [D loss: 0.067877, acc.: 99.22%] [G loss: 1.746876]\n",
      "epoch:7 step:5758 [D loss: 0.305523, acc.: 86.72%] [G loss: 0.648690]\n",
      "epoch:7 step:5759 [D loss: 0.046448, acc.: 100.00%] [G loss: 1.864845]\n",
      "epoch:7 step:5760 [D loss: 0.157637, acc.: 94.53%] [G loss: 0.367459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5761 [D loss: 0.525272, acc.: 72.66%] [G loss: 6.232918]\n",
      "epoch:7 step:5762 [D loss: 1.067220, acc.: 56.25%] [G loss: 2.085470]\n",
      "epoch:7 step:5763 [D loss: 0.201574, acc.: 93.75%] [G loss: 1.460144]\n",
      "epoch:7 step:5764 [D loss: 0.232550, acc.: 93.75%] [G loss: 1.632780]\n",
      "epoch:7 step:5765 [D loss: 0.035528, acc.: 100.00%] [G loss: 2.733549]\n",
      "epoch:7 step:5766 [D loss: 0.155280, acc.: 95.31%] [G loss: 2.198018]\n",
      "epoch:7 step:5767 [D loss: 0.286625, acc.: 85.94%] [G loss: 4.057148]\n",
      "epoch:7 step:5768 [D loss: 0.156915, acc.: 96.09%] [G loss: 4.561749]\n",
      "epoch:7 step:5769 [D loss: 0.399573, acc.: 81.25%] [G loss: 3.865331]\n",
      "epoch:7 step:5770 [D loss: 0.111818, acc.: 96.09%] [G loss: 4.724981]\n",
      "epoch:7 step:5771 [D loss: 0.053506, acc.: 99.22%] [G loss: 4.369119]\n",
      "epoch:7 step:5772 [D loss: 0.063601, acc.: 99.22%] [G loss: 3.759321]\n",
      "epoch:7 step:5773 [D loss: 0.295411, acc.: 88.28%] [G loss: 5.216627]\n",
      "epoch:7 step:5774 [D loss: 0.199340, acc.: 89.06%] [G loss: 4.385231]\n",
      "epoch:7 step:5775 [D loss: 0.062292, acc.: 100.00%] [G loss: 2.567840]\n",
      "epoch:7 step:5776 [D loss: 0.051659, acc.: 100.00%] [G loss: 2.044664]\n",
      "epoch:7 step:5777 [D loss: 0.048827, acc.: 100.00%] [G loss: 1.850012]\n",
      "epoch:7 step:5778 [D loss: 0.032032, acc.: 100.00%] [G loss: 1.517925]\n",
      "epoch:7 step:5779 [D loss: 0.058858, acc.: 99.22%] [G loss: 0.559188]\n",
      "epoch:7 step:5780 [D loss: 0.055849, acc.: 100.00%] [G loss: 0.233523]\n",
      "epoch:7 step:5781 [D loss: 0.019422, acc.: 100.00%] [G loss: 0.245644]\n",
      "epoch:7 step:5782 [D loss: 0.999081, acc.: 44.53%] [G loss: 0.262231]\n",
      "epoch:7 step:5783 [D loss: 0.007410, acc.: 100.00%] [G loss: 1.806756]\n",
      "epoch:7 step:5784 [D loss: 0.015807, acc.: 100.00%] [G loss: 1.061302]\n",
      "epoch:7 step:5785 [D loss: 0.012216, acc.: 100.00%] [G loss: 0.504124]\n",
      "epoch:7 step:5786 [D loss: 0.172804, acc.: 92.19%] [G loss: 2.984354]\n",
      "epoch:7 step:5787 [D loss: 0.524505, acc.: 77.34%] [G loss: 0.386175]\n",
      "epoch:7 step:5788 [D loss: 0.473883, acc.: 78.12%] [G loss: 4.126472]\n",
      "epoch:7 step:5789 [D loss: 0.219988, acc.: 87.50%] [G loss: 5.101807]\n",
      "epoch:7 step:5790 [D loss: 0.209373, acc.: 88.28%] [G loss: 1.571090]\n",
      "epoch:7 step:5791 [D loss: 0.027637, acc.: 100.00%] [G loss: 0.800381]\n",
      "epoch:7 step:5792 [D loss: 0.333482, acc.: 86.72%] [G loss: 3.330854]\n",
      "epoch:7 step:5793 [D loss: 0.245552, acc.: 88.28%] [G loss: 2.514914]\n",
      "epoch:7 step:5794 [D loss: 0.091486, acc.: 97.66%] [G loss: 1.941328]\n",
      "epoch:7 step:5795 [D loss: 0.387670, acc.: 82.81%] [G loss: 4.273606]\n",
      "epoch:7 step:5796 [D loss: 0.075325, acc.: 96.88%] [G loss: 5.266585]\n",
      "epoch:7 step:5797 [D loss: 0.302059, acc.: 85.94%] [G loss: 2.053377]\n",
      "epoch:7 step:5798 [D loss: 0.215249, acc.: 92.19%] [G loss: 2.043434]\n",
      "epoch:7 step:5799 [D loss: 0.031462, acc.: 100.00%] [G loss: 3.228069]\n",
      "epoch:7 step:5800 [D loss: 0.140180, acc.: 96.09%] [G loss: 2.647713]\n",
      "epoch:7 step:5801 [D loss: 0.183506, acc.: 92.19%] [G loss: 2.256701]\n",
      "epoch:7 step:5802 [D loss: 0.181833, acc.: 94.53%] [G loss: 3.883376]\n",
      "epoch:7 step:5803 [D loss: 0.136933, acc.: 96.09%] [G loss: 3.243407]\n",
      "epoch:7 step:5804 [D loss: 0.044614, acc.: 99.22%] [G loss: 2.585892]\n",
      "epoch:7 step:5805 [D loss: 0.350811, acc.: 87.50%] [G loss: 3.483596]\n",
      "epoch:7 step:5806 [D loss: 0.027459, acc.: 100.00%] [G loss: 4.055832]\n",
      "epoch:7 step:5807 [D loss: 2.483830, acc.: 16.41%] [G loss: 5.605844]\n",
      "epoch:7 step:5808 [D loss: 0.529196, acc.: 75.00%] [G loss: 3.773552]\n",
      "epoch:7 step:5809 [D loss: 0.223313, acc.: 90.62%] [G loss: 3.440041]\n",
      "epoch:7 step:5810 [D loss: 0.149549, acc.: 96.09%] [G loss: 4.240985]\n",
      "epoch:7 step:5811 [D loss: 0.131498, acc.: 97.66%] [G loss: 4.496884]\n",
      "epoch:7 step:5812 [D loss: 0.176904, acc.: 92.97%] [G loss: 4.444776]\n",
      "epoch:7 step:5813 [D loss: 0.272791, acc.: 88.28%] [G loss: 5.619693]\n",
      "epoch:7 step:5814 [D loss: 0.747466, acc.: 62.50%] [G loss: 4.953446]\n",
      "epoch:7 step:5815 [D loss: 0.037520, acc.: 100.00%] [G loss: 5.069814]\n",
      "epoch:7 step:5816 [D loss: 0.093106, acc.: 97.66%] [G loss: 5.107057]\n",
      "epoch:7 step:5817 [D loss: 0.398937, acc.: 82.81%] [G loss: 5.511845]\n",
      "epoch:7 step:5818 [D loss: 0.164867, acc.: 92.97%] [G loss: 3.674873]\n",
      "epoch:7 step:5819 [D loss: 0.042429, acc.: 100.00%] [G loss: 2.029083]\n",
      "epoch:7 step:5820 [D loss: 0.134371, acc.: 96.88%] [G loss: 2.953090]\n",
      "epoch:7 step:5821 [D loss: 0.045445, acc.: 100.00%] [G loss: 1.970065]\n",
      "epoch:7 step:5822 [D loss: 0.507823, acc.: 78.12%] [G loss: 2.153333]\n",
      "epoch:7 step:5823 [D loss: 0.094220, acc.: 97.66%] [G loss: 3.812404]\n",
      "epoch:7 step:5824 [D loss: 0.071657, acc.: 97.66%] [G loss: 1.078836]\n",
      "epoch:7 step:5825 [D loss: 0.223568, acc.: 92.97%] [G loss: 0.055211]\n",
      "epoch:7 step:5826 [D loss: 0.099777, acc.: 98.44%] [G loss: 0.699316]\n",
      "epoch:7 step:5827 [D loss: 0.020071, acc.: 100.00%] [G loss: 0.790999]\n",
      "epoch:7 step:5828 [D loss: 0.197405, acc.: 92.97%] [G loss: 2.430344]\n",
      "epoch:7 step:5829 [D loss: 0.695118, acc.: 64.06%] [G loss: 4.126758]\n",
      "epoch:7 step:5830 [D loss: 0.056165, acc.: 99.22%] [G loss: 4.130607]\n",
      "epoch:7 step:5831 [D loss: 0.142611, acc.: 92.97%] [G loss: 1.808575]\n",
      "epoch:7 step:5832 [D loss: 0.145749, acc.: 93.75%] [G loss: 2.738734]\n",
      "epoch:7 step:5833 [D loss: 0.295749, acc.: 88.28%] [G loss: 2.692834]\n",
      "epoch:7 step:5834 [D loss: 0.656598, acc.: 65.62%] [G loss: 4.431197]\n",
      "epoch:7 step:5835 [D loss: 0.122129, acc.: 95.31%] [G loss: 4.677071]\n",
      "epoch:7 step:5836 [D loss: 0.093597, acc.: 99.22%] [G loss: 3.459478]\n",
      "epoch:7 step:5837 [D loss: 0.462202, acc.: 77.34%] [G loss: 4.752355]\n",
      "epoch:7 step:5838 [D loss: 0.089994, acc.: 97.66%] [G loss: 4.463178]\n",
      "epoch:7 step:5839 [D loss: 0.736792, acc.: 69.53%] [G loss: 0.832283]\n",
      "epoch:7 step:5840 [D loss: 0.870955, acc.: 63.28%] [G loss: 6.457363]\n",
      "epoch:7 step:5841 [D loss: 0.590733, acc.: 74.22%] [G loss: 3.134632]\n",
      "epoch:7 step:5842 [D loss: 0.193033, acc.: 92.97%] [G loss: 1.280587]\n",
      "epoch:7 step:5843 [D loss: 0.152985, acc.: 95.31%] [G loss: 2.512953]\n",
      "epoch:7 step:5844 [D loss: 0.142142, acc.: 96.09%] [G loss: 4.241481]\n",
      "epoch:7 step:5845 [D loss: 0.194196, acc.: 89.84%] [G loss: 3.660996]\n",
      "epoch:7 step:5846 [D loss: 0.067746, acc.: 99.22%] [G loss: 1.769037]\n",
      "epoch:7 step:5847 [D loss: 0.165543, acc.: 95.31%] [G loss: 3.060443]\n",
      "epoch:7 step:5848 [D loss: 0.066041, acc.: 98.44%] [G loss: 2.674141]\n",
      "epoch:7 step:5849 [D loss: 0.274538, acc.: 89.84%] [G loss: 2.484114]\n",
      "epoch:7 step:5850 [D loss: 0.118865, acc.: 98.44%] [G loss: 3.134934]\n",
      "epoch:7 step:5851 [D loss: 0.238421, acc.: 89.84%] [G loss: 3.274793]\n",
      "epoch:7 step:5852 [D loss: 0.195326, acc.: 92.97%] [G loss: 1.987942]\n",
      "epoch:7 step:5853 [D loss: 0.591232, acc.: 76.56%] [G loss: 5.540651]\n",
      "epoch:7 step:5854 [D loss: 0.183611, acc.: 91.41%] [G loss: 5.514611]\n",
      "epoch:7 step:5855 [D loss: 1.148146, acc.: 39.84%] [G loss: 4.348686]\n",
      "epoch:7 step:5856 [D loss: 0.032892, acc.: 99.22%] [G loss: 4.553963]\n",
      "epoch:7 step:5857 [D loss: 0.200133, acc.: 89.84%] [G loss: 3.161224]\n",
      "epoch:7 step:5858 [D loss: 0.380600, acc.: 81.25%] [G loss: 5.140505]\n",
      "epoch:7 step:5859 [D loss: 0.185197, acc.: 92.97%] [G loss: 3.908985]\n",
      "epoch:7 step:5860 [D loss: 0.538582, acc.: 72.66%] [G loss: 3.058295]\n",
      "epoch:7 step:5861 [D loss: 0.104801, acc.: 98.44%] [G loss: 3.024956]\n",
      "epoch:7 step:5862 [D loss: 0.433466, acc.: 80.47%] [G loss: 3.804806]\n",
      "epoch:7 step:5863 [D loss: 0.139021, acc.: 95.31%] [G loss: 3.164775]\n",
      "epoch:7 step:5864 [D loss: 0.308357, acc.: 85.94%] [G loss: 4.627568]\n",
      "epoch:7 step:5865 [D loss: 0.152041, acc.: 94.53%] [G loss: 3.895315]\n",
      "epoch:7 step:5866 [D loss: 0.122720, acc.: 99.22%] [G loss: 3.273356]\n",
      "epoch:7 step:5867 [D loss: 0.097726, acc.: 96.88%] [G loss: 3.618653]\n",
      "epoch:7 step:5868 [D loss: 0.422840, acc.: 79.69%] [G loss: 2.993914]\n",
      "epoch:7 step:5869 [D loss: 0.099754, acc.: 98.44%] [G loss: 2.240069]\n",
      "epoch:7 step:5870 [D loss: 0.346757, acc.: 85.94%] [G loss: 4.087560]\n",
      "epoch:7 step:5871 [D loss: 0.141427, acc.: 95.31%] [G loss: 3.199717]\n",
      "epoch:7 step:5872 [D loss: 0.286677, acc.: 89.84%] [G loss: 4.980722]\n",
      "epoch:7 step:5873 [D loss: 0.297748, acc.: 87.50%] [G loss: 2.886057]\n",
      "epoch:7 step:5874 [D loss: 0.178480, acc.: 92.97%] [G loss: 3.656391]\n",
      "epoch:7 step:5875 [D loss: 0.063592, acc.: 100.00%] [G loss: 3.803452]\n",
      "epoch:7 step:5876 [D loss: 0.522418, acc.: 77.34%] [G loss: 2.556040]\n",
      "epoch:7 step:5877 [D loss: 0.073051, acc.: 98.44%] [G loss: 4.319988]\n",
      "epoch:7 step:5878 [D loss: 0.442545, acc.: 80.47%] [G loss: 4.457519]\n",
      "epoch:7 step:5879 [D loss: 0.084366, acc.: 97.66%] [G loss: 5.361491]\n",
      "epoch:7 step:5880 [D loss: 0.241679, acc.: 90.62%] [G loss: 1.118793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:5881 [D loss: 0.724347, acc.: 64.84%] [G loss: 7.187321]\n",
      "epoch:7 step:5882 [D loss: 0.745138, acc.: 64.84%] [G loss: 4.054725]\n",
      "epoch:7 step:5883 [D loss: 0.169790, acc.: 94.53%] [G loss: 3.832464]\n",
      "epoch:7 step:5884 [D loss: 0.106651, acc.: 96.88%] [G loss: 4.286595]\n",
      "epoch:7 step:5885 [D loss: 0.121172, acc.: 97.66%] [G loss: 3.137421]\n",
      "epoch:7 step:5886 [D loss: 0.284780, acc.: 86.72%] [G loss: 4.635999]\n",
      "epoch:7 step:5887 [D loss: 0.600780, acc.: 71.88%] [G loss: 3.626358]\n",
      "epoch:7 step:5888 [D loss: 0.167806, acc.: 96.09%] [G loss: 3.215443]\n",
      "epoch:7 step:5889 [D loss: 0.206941, acc.: 94.53%] [G loss: 3.902098]\n",
      "epoch:7 step:5890 [D loss: 0.122603, acc.: 93.75%] [G loss: 4.310080]\n",
      "epoch:7 step:5891 [D loss: 0.240804, acc.: 92.19%] [G loss: 1.818788]\n",
      "epoch:7 step:5892 [D loss: 0.289788, acc.: 85.94%] [G loss: 5.359026]\n",
      "epoch:7 step:5893 [D loss: 0.709500, acc.: 67.19%] [G loss: 2.126405]\n",
      "epoch:7 step:5894 [D loss: 0.042421, acc.: 99.22%] [G loss: 1.829225]\n",
      "epoch:7 step:5895 [D loss: 0.280653, acc.: 85.94%] [G loss: 2.427902]\n",
      "epoch:7 step:5896 [D loss: 0.056824, acc.: 99.22%] [G loss: 2.996186]\n",
      "epoch:7 step:5897 [D loss: 0.155752, acc.: 91.41%] [G loss: 1.647156]\n",
      "epoch:7 step:5898 [D loss: 0.447775, acc.: 81.25%] [G loss: 3.831848]\n",
      "epoch:7 step:5899 [D loss: 0.174282, acc.: 92.19%] [G loss: 3.195926]\n",
      "epoch:7 step:5900 [D loss: 1.611404, acc.: 28.12%] [G loss: 4.432319]\n",
      "epoch:7 step:5901 [D loss: 1.217974, acc.: 39.84%] [G loss: 4.433892]\n",
      "epoch:7 step:5902 [D loss: 0.417889, acc.: 73.44%] [G loss: 3.534451]\n",
      "epoch:7 step:5903 [D loss: 0.137245, acc.: 95.31%] [G loss: 3.175933]\n",
      "epoch:7 step:5904 [D loss: 0.189039, acc.: 95.31%] [G loss: 2.965870]\n",
      "epoch:7 step:5905 [D loss: 0.136949, acc.: 96.88%] [G loss: 3.085003]\n",
      "epoch:7 step:5906 [D loss: 0.098032, acc.: 100.00%] [G loss: 3.623739]\n",
      "epoch:7 step:5907 [D loss: 0.141536, acc.: 96.09%] [G loss: 3.560462]\n",
      "epoch:7 step:5908 [D loss: 0.122237, acc.: 97.66%] [G loss: 3.346046]\n",
      "epoch:7 step:5909 [D loss: 0.211520, acc.: 95.31%] [G loss: 2.880312]\n",
      "epoch:7 step:5910 [D loss: 0.184847, acc.: 92.97%] [G loss: 3.614651]\n",
      "epoch:7 step:5911 [D loss: 0.371015, acc.: 81.25%] [G loss: 2.483652]\n",
      "epoch:7 step:5912 [D loss: 0.216480, acc.: 89.06%] [G loss: 3.625611]\n",
      "epoch:7 step:5913 [D loss: 0.061260, acc.: 99.22%] [G loss: 3.741905]\n",
      "epoch:7 step:5914 [D loss: 0.256843, acc.: 90.62%] [G loss: 1.537941]\n",
      "epoch:7 step:5915 [D loss: 0.639837, acc.: 60.94%] [G loss: 5.175684]\n",
      "epoch:7 step:5916 [D loss: 0.134215, acc.: 95.31%] [G loss: 4.931451]\n",
      "epoch:7 step:5917 [D loss: 0.453355, acc.: 80.47%] [G loss: 1.791301]\n",
      "epoch:7 step:5918 [D loss: 0.304837, acc.: 85.94%] [G loss: 3.972901]\n",
      "epoch:7 step:5919 [D loss: 0.024680, acc.: 100.00%] [G loss: 4.935699]\n",
      "epoch:7 step:5920 [D loss: 0.342307, acc.: 89.84%] [G loss: 2.515673]\n",
      "epoch:7 step:5921 [D loss: 0.172697, acc.: 93.75%] [G loss: 2.907727]\n",
      "epoch:7 step:5922 [D loss: 0.064974, acc.: 99.22%] [G loss: 3.794426]\n",
      "epoch:7 step:5923 [D loss: 0.066795, acc.: 98.44%] [G loss: 3.523393]\n",
      "epoch:7 step:5924 [D loss: 0.107239, acc.: 98.44%] [G loss: 3.202667]\n",
      "epoch:7 step:5925 [D loss: 0.149683, acc.: 93.75%] [G loss: 2.986697]\n",
      "epoch:7 step:5926 [D loss: 0.281491, acc.: 91.41%] [G loss: 3.023691]\n",
      "epoch:7 step:5927 [D loss: 0.222831, acc.: 92.19%] [G loss: 2.012611]\n",
      "epoch:7 step:5928 [D loss: 0.196234, acc.: 91.41%] [G loss: 4.510443]\n",
      "epoch:7 step:5929 [D loss: 0.101383, acc.: 96.88%] [G loss: 5.152967]\n",
      "epoch:7 step:5930 [D loss: 0.151311, acc.: 96.09%] [G loss: 4.179334]\n",
      "epoch:7 step:5931 [D loss: 0.057232, acc.: 100.00%] [G loss: 2.751197]\n",
      "epoch:7 step:5932 [D loss: 0.042624, acc.: 99.22%] [G loss: 2.612131]\n",
      "epoch:7 step:5933 [D loss: 0.259681, acc.: 90.62%] [G loss: 4.441512]\n",
      "epoch:7 step:5934 [D loss: 0.335730, acc.: 87.50%] [G loss: 1.542718]\n",
      "epoch:7 step:5935 [D loss: 0.371493, acc.: 80.47%] [G loss: 5.253493]\n",
      "epoch:7 step:5936 [D loss: 0.164293, acc.: 95.31%] [G loss: 5.660238]\n",
      "epoch:7 step:5937 [D loss: 0.185539, acc.: 93.75%] [G loss: 2.762330]\n",
      "epoch:7 step:5938 [D loss: 0.144090, acc.: 94.53%] [G loss: 2.971850]\n",
      "epoch:7 step:5939 [D loss: 0.202411, acc.: 92.19%] [G loss: 3.343239]\n",
      "epoch:7 step:5940 [D loss: 0.353559, acc.: 83.59%] [G loss: 3.964583]\n",
      "epoch:7 step:5941 [D loss: 0.373085, acc.: 78.12%] [G loss: 1.499852]\n",
      "epoch:7 step:5942 [D loss: 0.078093, acc.: 99.22%] [G loss: 0.881120]\n",
      "epoch:7 step:5943 [D loss: 0.190368, acc.: 92.19%] [G loss: 3.715844]\n",
      "epoch:7 step:5944 [D loss: 0.048112, acc.: 99.22%] [G loss: 4.661093]\n",
      "epoch:7 step:5945 [D loss: 0.158191, acc.: 96.09%] [G loss: 2.160729]\n",
      "epoch:7 step:5946 [D loss: 0.248755, acc.: 94.53%] [G loss: 2.865213]\n",
      "epoch:7 step:5947 [D loss: 0.204618, acc.: 92.97%] [G loss: 3.375796]\n",
      "epoch:7 step:5948 [D loss: 0.366684, acc.: 81.25%] [G loss: 2.928982]\n",
      "epoch:7 step:5949 [D loss: 0.241598, acc.: 92.97%] [G loss: 4.835095]\n",
      "epoch:7 step:5950 [D loss: 0.044961, acc.: 100.00%] [G loss: 4.768190]\n",
      "epoch:7 step:5951 [D loss: 0.257930, acc.: 91.41%] [G loss: 4.245409]\n",
      "epoch:7 step:5952 [D loss: 0.075376, acc.: 99.22%] [G loss: 3.874749]\n",
      "epoch:7 step:5953 [D loss: 0.933966, acc.: 58.59%] [G loss: 7.285897]\n",
      "epoch:7 step:5954 [D loss: 0.491676, acc.: 71.88%] [G loss: 5.118915]\n",
      "epoch:7 step:5955 [D loss: 0.079911, acc.: 99.22%] [G loss: 3.637615]\n",
      "epoch:7 step:5956 [D loss: 0.087414, acc.: 97.66%] [G loss: 4.696618]\n",
      "epoch:7 step:5957 [D loss: 0.017211, acc.: 100.00%] [G loss: 4.587952]\n",
      "epoch:7 step:5958 [D loss: 0.641808, acc.: 69.53%] [G loss: 6.392798]\n",
      "epoch:7 step:5959 [D loss: 0.706380, acc.: 67.97%] [G loss: 2.657652]\n",
      "epoch:7 step:5960 [D loss: 0.045794, acc.: 100.00%] [G loss: 2.004191]\n",
      "epoch:7 step:5961 [D loss: 0.030708, acc.: 100.00%] [G loss: 1.631560]\n",
      "epoch:7 step:5962 [D loss: 0.274286, acc.: 88.28%] [G loss: 4.544438]\n",
      "epoch:7 step:5963 [D loss: 0.150902, acc.: 95.31%] [G loss: 3.021561]\n",
      "epoch:7 step:5964 [D loss: 0.132485, acc.: 94.53%] [G loss: 1.441971]\n",
      "epoch:7 step:5965 [D loss: 0.503676, acc.: 75.78%] [G loss: 5.932219]\n",
      "epoch:7 step:5966 [D loss: 0.268053, acc.: 87.50%] [G loss: 5.645624]\n",
      "epoch:7 step:5967 [D loss: 0.436683, acc.: 75.78%] [G loss: 2.357195]\n",
      "epoch:7 step:5968 [D loss: 0.098915, acc.: 97.66%] [G loss: 3.231192]\n",
      "epoch:7 step:5969 [D loss: 0.046854, acc.: 99.22%] [G loss: 3.134049]\n",
      "epoch:7 step:5970 [D loss: 0.136737, acc.: 96.88%] [G loss: 2.500948]\n",
      "epoch:7 step:5971 [D loss: 0.031666, acc.: 100.00%] [G loss: 2.087855]\n",
      "epoch:7 step:5972 [D loss: 0.288377, acc.: 85.94%] [G loss: 5.880603]\n",
      "epoch:7 step:5973 [D loss: 0.974203, acc.: 58.59%] [G loss: 3.506764]\n",
      "epoch:7 step:5974 [D loss: 0.322702, acc.: 83.59%] [G loss: 6.773190]\n",
      "epoch:7 step:5975 [D loss: 1.113889, acc.: 53.91%] [G loss: 2.661052]\n",
      "epoch:7 step:5976 [D loss: 0.156234, acc.: 92.19%] [G loss: 3.441982]\n",
      "epoch:7 step:5977 [D loss: 0.054592, acc.: 100.00%] [G loss: 3.985868]\n",
      "epoch:7 step:5978 [D loss: 0.085974, acc.: 98.44%] [G loss: 3.121066]\n",
      "epoch:7 step:5979 [D loss: 0.226122, acc.: 92.19%] [G loss: 3.527277]\n",
      "epoch:7 step:5980 [D loss: 0.195451, acc.: 94.53%] [G loss: 2.536879]\n",
      "epoch:7 step:5981 [D loss: 0.264341, acc.: 90.62%] [G loss: 4.633737]\n",
      "epoch:7 step:5982 [D loss: 0.083395, acc.: 98.44%] [G loss: 3.759555]\n",
      "epoch:7 step:5983 [D loss: 0.167311, acc.: 96.09%] [G loss: 1.771459]\n",
      "epoch:7 step:5984 [D loss: 0.102570, acc.: 98.44%] [G loss: 2.559443]\n",
      "epoch:7 step:5985 [D loss: 0.191286, acc.: 92.97%] [G loss: 5.401530]\n",
      "epoch:7 step:5986 [D loss: 0.887327, acc.: 55.47%] [G loss: 4.412076]\n",
      "epoch:7 step:5987 [D loss: 0.035391, acc.: 99.22%] [G loss: 4.534107]\n",
      "epoch:7 step:5988 [D loss: 0.094961, acc.: 97.66%] [G loss: 2.939568]\n",
      "epoch:7 step:5989 [D loss: 0.169588, acc.: 91.41%] [G loss: 3.914935]\n",
      "epoch:7 step:5990 [D loss: 0.057090, acc.: 99.22%] [G loss: 4.550426]\n",
      "epoch:7 step:5991 [D loss: 0.915202, acc.: 55.47%] [G loss: 3.004062]\n",
      "epoch:7 step:5992 [D loss: 0.128998, acc.: 92.97%] [G loss: 5.607584]\n",
      "epoch:7 step:5993 [D loss: 1.319106, acc.: 46.88%] [G loss: 2.955720]\n",
      "epoch:7 step:5994 [D loss: 0.166071, acc.: 94.53%] [G loss: 4.793465]\n",
      "epoch:7 step:5995 [D loss: 0.093995, acc.: 97.66%] [G loss: 3.827349]\n",
      "epoch:7 step:5996 [D loss: 0.092255, acc.: 97.66%] [G loss: 3.541889]\n",
      "epoch:7 step:5997 [D loss: 0.113821, acc.: 99.22%] [G loss: 3.193175]\n",
      "epoch:7 step:5998 [D loss: 0.155237, acc.: 95.31%] [G loss: 3.429021]\n",
      "epoch:7 step:5999 [D loss: 0.296945, acc.: 84.38%] [G loss: 3.749420]\n",
      "epoch:7 step:6000 [D loss: 0.261301, acc.: 89.84%] [G loss: 2.451582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6001 [D loss: 0.293938, acc.: 88.28%] [G loss: 4.473663]\n",
      "epoch:7 step:6002 [D loss: 0.169092, acc.: 92.19%] [G loss: 2.902756]\n",
      "epoch:7 step:6003 [D loss: 0.112758, acc.: 98.44%] [G loss: 3.575909]\n",
      "epoch:7 step:6004 [D loss: 0.148578, acc.: 96.88%] [G loss: 3.060109]\n",
      "epoch:7 step:6005 [D loss: 0.075503, acc.: 100.00%] [G loss: 4.404782]\n",
      "epoch:7 step:6006 [D loss: 0.083929, acc.: 99.22%] [G loss: 4.286950]\n",
      "epoch:7 step:6007 [D loss: 0.056450, acc.: 100.00%] [G loss: 3.981085]\n",
      "epoch:7 step:6008 [D loss: 0.062132, acc.: 99.22%] [G loss: 3.572583]\n",
      "epoch:7 step:6009 [D loss: 0.176269, acc.: 95.31%] [G loss: 2.144628]\n",
      "epoch:7 step:6010 [D loss: 0.430217, acc.: 78.12%] [G loss: 6.352054]\n",
      "epoch:7 step:6011 [D loss: 0.548641, acc.: 68.75%] [G loss: 4.057842]\n",
      "epoch:7 step:6012 [D loss: 0.132680, acc.: 96.09%] [G loss: 3.205123]\n",
      "epoch:7 step:6013 [D loss: 0.042822, acc.: 100.00%] [G loss: 3.223428]\n",
      "epoch:7 step:6014 [D loss: 0.219894, acc.: 92.97%] [G loss: 3.775496]\n",
      "epoch:7 step:6015 [D loss: 0.057172, acc.: 99.22%] [G loss: 3.472573]\n",
      "epoch:7 step:6016 [D loss: 0.058952, acc.: 99.22%] [G loss: 3.097025]\n",
      "epoch:7 step:6017 [D loss: 0.534340, acc.: 72.66%] [G loss: 5.609447]\n",
      "epoch:7 step:6018 [D loss: 0.189770, acc.: 89.84%] [G loss: 5.050349]\n",
      "epoch:7 step:6019 [D loss: 0.477206, acc.: 73.44%] [G loss: 3.121452]\n",
      "epoch:7 step:6020 [D loss: 0.058353, acc.: 98.44%] [G loss: 3.036852]\n",
      "epoch:7 step:6021 [D loss: 0.162720, acc.: 96.88%] [G loss: 4.473040]\n",
      "epoch:7 step:6022 [D loss: 0.186826, acc.: 92.97%] [G loss: 2.978512]\n",
      "epoch:7 step:6023 [D loss: 0.073391, acc.: 99.22%] [G loss: 3.383861]\n",
      "epoch:7 step:6024 [D loss: 0.145847, acc.: 96.88%] [G loss: 2.876930]\n",
      "epoch:7 step:6025 [D loss: 0.624929, acc.: 66.41%] [G loss: 3.569465]\n",
      "epoch:7 step:6026 [D loss: 0.050921, acc.: 100.00%] [G loss: 3.806229]\n",
      "epoch:7 step:6027 [D loss: 0.122809, acc.: 96.09%] [G loss: 2.903672]\n",
      "epoch:7 step:6028 [D loss: 0.243082, acc.: 90.62%] [G loss: 3.758986]\n",
      "epoch:7 step:6029 [D loss: 0.351496, acc.: 82.81%] [G loss: 3.235839]\n",
      "epoch:7 step:6030 [D loss: 0.081321, acc.: 99.22%] [G loss: 3.836321]\n",
      "epoch:7 step:6031 [D loss: 0.245375, acc.: 89.84%] [G loss: 1.467405]\n",
      "epoch:7 step:6032 [D loss: 0.399603, acc.: 78.91%] [G loss: 4.599909]\n",
      "epoch:7 step:6033 [D loss: 0.698800, acc.: 62.50%] [G loss: 3.089927]\n",
      "epoch:7 step:6034 [D loss: 0.082903, acc.: 98.44%] [G loss: 4.222071]\n",
      "epoch:7 step:6035 [D loss: 0.060481, acc.: 99.22%] [G loss: 3.690367]\n",
      "epoch:7 step:6036 [D loss: 0.231633, acc.: 92.19%] [G loss: 4.297670]\n",
      "epoch:7 step:6037 [D loss: 0.072940, acc.: 98.44%] [G loss: 3.971496]\n",
      "epoch:7 step:6038 [D loss: 0.128918, acc.: 96.09%] [G loss: 3.837585]\n",
      "epoch:7 step:6039 [D loss: 0.054610, acc.: 99.22%] [G loss: 3.982851]\n",
      "epoch:7 step:6040 [D loss: 0.135972, acc.: 97.66%] [G loss: 4.476032]\n",
      "epoch:7 step:6041 [D loss: 0.420036, acc.: 78.91%] [G loss: 5.280152]\n",
      "epoch:7 step:6042 [D loss: 0.397076, acc.: 82.81%] [G loss: 2.606907]\n",
      "epoch:7 step:6043 [D loss: 0.271778, acc.: 86.72%] [G loss: 5.946809]\n",
      "epoch:7 step:6044 [D loss: 0.349879, acc.: 82.81%] [G loss: 3.919745]\n",
      "epoch:7 step:6045 [D loss: 0.136970, acc.: 94.53%] [G loss: 4.450316]\n",
      "epoch:7 step:6046 [D loss: 0.024106, acc.: 99.22%] [G loss: 4.405647]\n",
      "epoch:7 step:6047 [D loss: 0.148296, acc.: 96.09%] [G loss: 3.045599]\n",
      "epoch:7 step:6048 [D loss: 0.277172, acc.: 89.06%] [G loss: 5.673701]\n",
      "epoch:7 step:6049 [D loss: 0.181083, acc.: 92.19%] [G loss: 4.483828]\n",
      "epoch:7 step:6050 [D loss: 0.156410, acc.: 93.75%] [G loss: 4.307819]\n",
      "epoch:7 step:6051 [D loss: 0.100154, acc.: 98.44%] [G loss: 4.659129]\n",
      "epoch:7 step:6052 [D loss: 0.104777, acc.: 97.66%] [G loss: 3.402971]\n",
      "epoch:7 step:6053 [D loss: 0.143582, acc.: 95.31%] [G loss: 4.667444]\n",
      "epoch:7 step:6054 [D loss: 0.097576, acc.: 98.44%] [G loss: 4.254432]\n",
      "epoch:7 step:6055 [D loss: 0.060272, acc.: 98.44%] [G loss: 1.984555]\n",
      "epoch:7 step:6056 [D loss: 0.288083, acc.: 86.72%] [G loss: 6.264641]\n",
      "epoch:7 step:6057 [D loss: 0.489997, acc.: 74.22%] [G loss: 3.358377]\n",
      "epoch:7 step:6058 [D loss: 0.195115, acc.: 93.75%] [G loss: 4.378035]\n",
      "epoch:7 step:6059 [D loss: 0.033848, acc.: 99.22%] [G loss: 5.622030]\n",
      "epoch:7 step:6060 [D loss: 0.330238, acc.: 86.72%] [G loss: 4.277152]\n",
      "epoch:7 step:6061 [D loss: 0.108757, acc.: 96.09%] [G loss: 4.880408]\n",
      "epoch:7 step:6062 [D loss: 0.116261, acc.: 97.66%] [G loss: 4.393928]\n",
      "epoch:7 step:6063 [D loss: 0.316208, acc.: 86.72%] [G loss: 5.286481]\n",
      "epoch:7 step:6064 [D loss: 0.106057, acc.: 95.31%] [G loss: 5.437969]\n",
      "epoch:7 step:6065 [D loss: 0.797977, acc.: 59.38%] [G loss: 3.745870]\n",
      "epoch:7 step:6066 [D loss: 0.078634, acc.: 98.44%] [G loss: 5.486545]\n",
      "epoch:7 step:6067 [D loss: 0.337532, acc.: 88.28%] [G loss: 4.431843]\n",
      "epoch:7 step:6068 [D loss: 0.253584, acc.: 89.84%] [G loss: 1.614989]\n",
      "epoch:7 step:6069 [D loss: 0.223289, acc.: 90.62%] [G loss: 4.841311]\n",
      "epoch:7 step:6070 [D loss: 0.223246, acc.: 89.84%] [G loss: 3.886436]\n",
      "epoch:7 step:6071 [D loss: 0.059241, acc.: 98.44%] [G loss: 4.080541]\n",
      "epoch:7 step:6072 [D loss: 0.152506, acc.: 95.31%] [G loss: 5.807823]\n",
      "epoch:7 step:6073 [D loss: 0.144075, acc.: 94.53%] [G loss: 4.746090]\n",
      "epoch:7 step:6074 [D loss: 0.083648, acc.: 98.44%] [G loss: 3.602410]\n",
      "epoch:7 step:6075 [D loss: 0.043222, acc.: 100.00%] [G loss: 4.172523]\n",
      "epoch:7 step:6076 [D loss: 0.216207, acc.: 89.84%] [G loss: 4.438796]\n",
      "epoch:7 step:6077 [D loss: 0.199806, acc.: 89.84%] [G loss: 3.430933]\n",
      "epoch:7 step:6078 [D loss: 0.166014, acc.: 93.75%] [G loss: 6.705177]\n",
      "epoch:7 step:6079 [D loss: 0.132760, acc.: 96.88%] [G loss: 5.291269]\n",
      "epoch:7 step:6080 [D loss: 0.644154, acc.: 67.97%] [G loss: 5.862138]\n",
      "epoch:7 step:6081 [D loss: 0.006524, acc.: 100.00%] [G loss: 6.823181]\n",
      "epoch:7 step:6082 [D loss: 0.090564, acc.: 97.66%] [G loss: 4.541779]\n",
      "epoch:7 step:6083 [D loss: 0.276280, acc.: 91.41%] [G loss: 4.858785]\n",
      "epoch:7 step:6084 [D loss: 0.050646, acc.: 97.66%] [G loss: 5.051340]\n",
      "epoch:7 step:6085 [D loss: 0.140531, acc.: 95.31%] [G loss: 2.185986]\n",
      "epoch:7 step:6086 [D loss: 0.046519, acc.: 100.00%] [G loss: 1.540787]\n",
      "epoch:7 step:6087 [D loss: 0.089904, acc.: 96.88%] [G loss: 1.966935]\n",
      "epoch:7 step:6088 [D loss: 0.016694, acc.: 100.00%] [G loss: 4.199407]\n",
      "epoch:7 step:6089 [D loss: 0.096326, acc.: 96.88%] [G loss: 1.161513]\n",
      "epoch:7 step:6090 [D loss: 2.046485, acc.: 43.75%] [G loss: 9.707401]\n",
      "epoch:7 step:6091 [D loss: 2.216000, acc.: 50.00%] [G loss: 5.125975]\n",
      "epoch:7 step:6092 [D loss: 0.409286, acc.: 81.25%] [G loss: 0.552847]\n",
      "epoch:7 step:6093 [D loss: 0.255866, acc.: 88.28%] [G loss: 2.264189]\n",
      "epoch:7 step:6094 [D loss: 0.037497, acc.: 100.00%] [G loss: 1.574956]\n",
      "epoch:7 step:6095 [D loss: 0.136685, acc.: 95.31%] [G loss: 2.659776]\n",
      "epoch:7 step:6096 [D loss: 0.155033, acc.: 92.97%] [G loss: 0.958765]\n",
      "epoch:7 step:6097 [D loss: 0.272073, acc.: 86.72%] [G loss: 3.703697]\n",
      "epoch:7 step:6098 [D loss: 0.167986, acc.: 93.75%] [G loss: 4.255985]\n",
      "epoch:7 step:6099 [D loss: 0.130219, acc.: 96.88%] [G loss: 3.680160]\n",
      "epoch:7 step:6100 [D loss: 0.224788, acc.: 91.41%] [G loss: 3.042345]\n",
      "epoch:7 step:6101 [D loss: 0.304171, acc.: 86.72%] [G loss: 4.589464]\n",
      "epoch:7 step:6102 [D loss: 0.149104, acc.: 96.88%] [G loss: 3.504391]\n",
      "epoch:7 step:6103 [D loss: 0.102871, acc.: 99.22%] [G loss: 3.938720]\n",
      "epoch:7 step:6104 [D loss: 0.133378, acc.: 96.88%] [G loss: 3.625144]\n",
      "epoch:7 step:6105 [D loss: 0.067014, acc.: 98.44%] [G loss: 3.612276]\n",
      "epoch:7 step:6106 [D loss: 0.270586, acc.: 89.84%] [G loss: 4.102636]\n",
      "epoch:7 step:6107 [D loss: 0.107789, acc.: 96.09%] [G loss: 4.167663]\n",
      "epoch:7 step:6108 [D loss: 0.060504, acc.: 99.22%] [G loss: 3.306108]\n",
      "epoch:7 step:6109 [D loss: 0.123685, acc.: 96.09%] [G loss: 3.182151]\n",
      "epoch:7 step:6110 [D loss: 0.301839, acc.: 89.84%] [G loss: 5.342832]\n",
      "epoch:7 step:6111 [D loss: 0.202534, acc.: 92.19%] [G loss: 4.702976]\n",
      "epoch:7 step:6112 [D loss: 0.095101, acc.: 98.44%] [G loss: 3.041348]\n",
      "epoch:7 step:6113 [D loss: 0.220659, acc.: 90.62%] [G loss: 2.956663]\n",
      "epoch:7 step:6114 [D loss: 0.111595, acc.: 96.88%] [G loss: 4.240915]\n",
      "epoch:7 step:6115 [D loss: 0.226368, acc.: 89.84%] [G loss: 1.916510]\n",
      "epoch:7 step:6116 [D loss: 0.210902, acc.: 92.19%] [G loss: 4.334365]\n",
      "epoch:7 step:6117 [D loss: 0.092239, acc.: 96.88%] [G loss: 4.681236]\n",
      "epoch:7 step:6118 [D loss: 0.066880, acc.: 100.00%] [G loss: 2.857543]\n",
      "epoch:7 step:6119 [D loss: 0.053876, acc.: 99.22%] [G loss: 1.550619]\n",
      "epoch:7 step:6120 [D loss: 0.043885, acc.: 100.00%] [G loss: 1.371063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6121 [D loss: 0.202239, acc.: 92.97%] [G loss: 3.825035]\n",
      "epoch:7 step:6122 [D loss: 0.104471, acc.: 96.88%] [G loss: 3.446481]\n",
      "epoch:7 step:6123 [D loss: 1.533478, acc.: 40.62%] [G loss: 6.143181]\n",
      "epoch:7 step:6124 [D loss: 1.060152, acc.: 57.03%] [G loss: 2.150845]\n",
      "epoch:7 step:6125 [D loss: 0.394954, acc.: 81.25%] [G loss: 4.517470]\n",
      "epoch:7 step:6126 [D loss: 0.406506, acc.: 79.69%] [G loss: 3.881693]\n",
      "epoch:7 step:6127 [D loss: 0.533506, acc.: 72.66%] [G loss: 5.070503]\n",
      "epoch:7 step:6128 [D loss: 0.120774, acc.: 96.88%] [G loss: 4.468579]\n",
      "epoch:7 step:6129 [D loss: 0.088530, acc.: 96.88%] [G loss: 2.882016]\n",
      "epoch:7 step:6130 [D loss: 0.084461, acc.: 99.22%] [G loss: 3.313992]\n",
      "epoch:7 step:6131 [D loss: 0.049002, acc.: 99.22%] [G loss: 2.731240]\n",
      "epoch:7 step:6132 [D loss: 0.729171, acc.: 62.50%] [G loss: 1.970834]\n",
      "epoch:7 step:6133 [D loss: 0.056439, acc.: 98.44%] [G loss: 2.984550]\n",
      "epoch:7 step:6134 [D loss: 0.074452, acc.: 98.44%] [G loss: 2.150132]\n",
      "epoch:7 step:6135 [D loss: 0.112676, acc.: 96.88%] [G loss: 1.747889]\n",
      "epoch:7 step:6136 [D loss: 0.108240, acc.: 97.66%] [G loss: 1.940578]\n",
      "epoch:7 step:6137 [D loss: 0.183649, acc.: 94.53%] [G loss: 1.827757]\n",
      "epoch:7 step:6138 [D loss: 0.429833, acc.: 81.25%] [G loss: 5.513384]\n",
      "epoch:7 step:6139 [D loss: 0.253808, acc.: 89.84%] [G loss: 4.036394]\n",
      "epoch:7 step:6140 [D loss: 0.249836, acc.: 92.19%] [G loss: 3.290597]\n",
      "epoch:7 step:6141 [D loss: 0.348824, acc.: 82.81%] [G loss: 5.022848]\n",
      "epoch:7 step:6142 [D loss: 0.163162, acc.: 92.19%] [G loss: 4.294147]\n",
      "epoch:7 step:6143 [D loss: 0.149380, acc.: 93.75%] [G loss: 3.424436]\n",
      "epoch:7 step:6144 [D loss: 0.073170, acc.: 100.00%] [G loss: 3.418388]\n",
      "epoch:7 step:6145 [D loss: 0.241569, acc.: 92.19%] [G loss: 4.559970]\n",
      "epoch:7 step:6146 [D loss: 0.077409, acc.: 99.22%] [G loss: 4.670394]\n",
      "epoch:7 step:6147 [D loss: 0.185380, acc.: 96.09%] [G loss: 4.772768]\n",
      "epoch:7 step:6148 [D loss: 0.188695, acc.: 93.75%] [G loss: 3.788547]\n",
      "epoch:7 step:6149 [D loss: 1.047973, acc.: 46.09%] [G loss: 7.075202]\n",
      "epoch:7 step:6150 [D loss: 0.506938, acc.: 75.00%] [G loss: 5.566213]\n",
      "epoch:7 step:6151 [D loss: 0.063207, acc.: 98.44%] [G loss: 3.718750]\n",
      "epoch:7 step:6152 [D loss: 0.081940, acc.: 97.66%] [G loss: 2.504195]\n",
      "epoch:7 step:6153 [D loss: 0.124476, acc.: 96.09%] [G loss: 2.878176]\n",
      "epoch:7 step:6154 [D loss: 0.169665, acc.: 95.31%] [G loss: 3.391465]\n",
      "epoch:7 step:6155 [D loss: 0.105440, acc.: 96.88%] [G loss: 3.883905]\n",
      "epoch:7 step:6156 [D loss: 0.238176, acc.: 93.75%] [G loss: 4.131665]\n",
      "epoch:7 step:6157 [D loss: 0.109452, acc.: 95.31%] [G loss: 3.964162]\n",
      "epoch:7 step:6158 [D loss: 0.062256, acc.: 100.00%] [G loss: 3.045778]\n",
      "epoch:7 step:6159 [D loss: 0.376654, acc.: 80.47%] [G loss: 5.532315]\n",
      "epoch:7 step:6160 [D loss: 0.253384, acc.: 88.28%] [G loss: 3.711373]\n",
      "epoch:7 step:6161 [D loss: 0.545862, acc.: 75.00%] [G loss: 5.440854]\n",
      "epoch:7 step:6162 [D loss: 0.173796, acc.: 92.19%] [G loss: 4.788062]\n",
      "epoch:7 step:6163 [D loss: 0.108942, acc.: 98.44%] [G loss: 3.775984]\n",
      "epoch:7 step:6164 [D loss: 0.052168, acc.: 99.22%] [G loss: 4.324827]\n",
      "epoch:7 step:6165 [D loss: 0.082472, acc.: 100.00%] [G loss: 3.987593]\n",
      "epoch:7 step:6166 [D loss: 0.151972, acc.: 97.66%] [G loss: 2.999010]\n",
      "epoch:7 step:6167 [D loss: 0.113294, acc.: 96.88%] [G loss: 4.402441]\n",
      "epoch:7 step:6168 [D loss: 0.223741, acc.: 91.41%] [G loss: 4.671691]\n",
      "epoch:7 step:6169 [D loss: 0.222488, acc.: 89.84%] [G loss: 4.585346]\n",
      "epoch:7 step:6170 [D loss: 0.235323, acc.: 89.06%] [G loss: 5.555926]\n",
      "epoch:7 step:6171 [D loss: 0.092522, acc.: 97.66%] [G loss: 4.378084]\n",
      "epoch:7 step:6172 [D loss: 0.169001, acc.: 93.75%] [G loss: 5.493021]\n",
      "epoch:7 step:6173 [D loss: 0.256866, acc.: 87.50%] [G loss: 3.173603]\n",
      "epoch:7 step:6174 [D loss: 0.648493, acc.: 75.78%] [G loss: 7.291456]\n",
      "epoch:7 step:6175 [D loss: 1.657029, acc.: 52.34%] [G loss: 2.039793]\n",
      "epoch:7 step:6176 [D loss: 0.257339, acc.: 88.28%] [G loss: 3.519716]\n",
      "epoch:7 step:6177 [D loss: 0.096244, acc.: 98.44%] [G loss: 4.161897]\n",
      "epoch:7 step:6178 [D loss: 0.136230, acc.: 96.09%] [G loss: 4.482132]\n",
      "epoch:7 step:6179 [D loss: 0.150173, acc.: 95.31%] [G loss: 3.097611]\n",
      "epoch:7 step:6180 [D loss: 0.263516, acc.: 86.72%] [G loss: 1.067137]\n",
      "epoch:7 step:6181 [D loss: 0.841174, acc.: 65.62%] [G loss: 5.117203]\n",
      "epoch:7 step:6182 [D loss: 0.362527, acc.: 78.91%] [G loss: 4.562488]\n",
      "epoch:7 step:6183 [D loss: 0.113164, acc.: 96.88%] [G loss: 3.838416]\n",
      "epoch:7 step:6184 [D loss: 0.087258, acc.: 99.22%] [G loss: 2.957351]\n",
      "epoch:7 step:6185 [D loss: 0.080654, acc.: 100.00%] [G loss: 3.200781]\n",
      "epoch:7 step:6186 [D loss: 0.215356, acc.: 93.75%] [G loss: 3.206450]\n",
      "epoch:7 step:6187 [D loss: 0.117258, acc.: 97.66%] [G loss: 4.546190]\n",
      "epoch:7 step:6188 [D loss: 0.327527, acc.: 89.06%] [G loss: 1.889672]\n",
      "epoch:7 step:6189 [D loss: 0.234852, acc.: 89.06%] [G loss: 4.445043]\n",
      "epoch:7 step:6190 [D loss: 0.066430, acc.: 99.22%] [G loss: 5.388699]\n",
      "epoch:7 step:6191 [D loss: 0.114137, acc.: 96.88%] [G loss: 4.215349]\n",
      "epoch:7 step:6192 [D loss: 0.175295, acc.: 93.75%] [G loss: 2.354631]\n",
      "epoch:7 step:6193 [D loss: 0.464113, acc.: 76.56%] [G loss: 6.084347]\n",
      "epoch:7 step:6194 [D loss: 0.499763, acc.: 73.44%] [G loss: 3.483057]\n",
      "epoch:7 step:6195 [D loss: 0.146572, acc.: 95.31%] [G loss: 3.245353]\n",
      "epoch:7 step:6196 [D loss: 0.091534, acc.: 98.44%] [G loss: 4.383289]\n",
      "epoch:7 step:6197 [D loss: 0.040300, acc.: 100.00%] [G loss: 3.778441]\n",
      "epoch:7 step:6198 [D loss: 0.081086, acc.: 99.22%] [G loss: 3.266071]\n",
      "epoch:7 step:6199 [D loss: 0.191695, acc.: 94.53%] [G loss: 3.501498]\n",
      "epoch:7 step:6200 [D loss: 0.407509, acc.: 78.91%] [G loss: 4.548354]\n",
      "epoch:7 step:6201 [D loss: 0.033276, acc.: 99.22%] [G loss: 5.166176]\n",
      "epoch:7 step:6202 [D loss: 0.445061, acc.: 77.34%] [G loss: 3.563487]\n",
      "epoch:7 step:6203 [D loss: 0.111417, acc.: 96.09%] [G loss: 3.105690]\n",
      "epoch:7 step:6204 [D loss: 0.087298, acc.: 97.66%] [G loss: 4.068289]\n",
      "epoch:7 step:6205 [D loss: 0.207833, acc.: 94.53%] [G loss: 3.872331]\n",
      "epoch:7 step:6206 [D loss: 0.147744, acc.: 97.66%] [G loss: 4.240000]\n",
      "epoch:7 step:6207 [D loss: 0.463545, acc.: 78.12%] [G loss: 5.431565]\n",
      "epoch:7 step:6208 [D loss: 0.212212, acc.: 91.41%] [G loss: 4.274110]\n",
      "epoch:7 step:6209 [D loss: 0.102637, acc.: 96.88%] [G loss: 4.233298]\n",
      "epoch:7 step:6210 [D loss: 0.061737, acc.: 99.22%] [G loss: 3.709805]\n",
      "epoch:7 step:6211 [D loss: 0.142509, acc.: 96.09%] [G loss: 3.272959]\n",
      "epoch:7 step:6212 [D loss: 0.096353, acc.: 98.44%] [G loss: 3.394902]\n",
      "epoch:7 step:6213 [D loss: 0.434318, acc.: 78.91%] [G loss: 4.402805]\n",
      "epoch:7 step:6214 [D loss: 0.322996, acc.: 87.50%] [G loss: 2.439669]\n",
      "epoch:7 step:6215 [D loss: 0.082465, acc.: 96.88%] [G loss: 3.334531]\n",
      "epoch:7 step:6216 [D loss: 0.154182, acc.: 96.09%] [G loss: 4.845448]\n",
      "epoch:7 step:6217 [D loss: 0.274744, acc.: 88.28%] [G loss: 1.945262]\n",
      "epoch:7 step:6218 [D loss: 0.147535, acc.: 96.88%] [G loss: 4.123847]\n",
      "epoch:7 step:6219 [D loss: 0.215619, acc.: 91.41%] [G loss: 4.206339]\n",
      "epoch:7 step:6220 [D loss: 0.413253, acc.: 78.91%] [G loss: 2.972748]\n",
      "epoch:7 step:6221 [D loss: 0.117718, acc.: 96.88%] [G loss: 3.195392]\n",
      "epoch:7 step:6222 [D loss: 0.151891, acc.: 92.97%] [G loss: 4.119106]\n",
      "epoch:7 step:6223 [D loss: 0.145013, acc.: 96.09%] [G loss: 3.620053]\n",
      "epoch:7 step:6224 [D loss: 0.263204, acc.: 84.38%] [G loss: 2.223876]\n",
      "epoch:7 step:6225 [D loss: 0.128277, acc.: 96.09%] [G loss: 3.914055]\n",
      "epoch:7 step:6226 [D loss: 0.201430, acc.: 92.97%] [G loss: 5.018951]\n",
      "epoch:7 step:6227 [D loss: 0.232889, acc.: 89.84%] [G loss: 4.749056]\n",
      "epoch:7 step:6228 [D loss: 0.055732, acc.: 100.00%] [G loss: 4.111647]\n",
      "epoch:7 step:6229 [D loss: 1.847333, acc.: 28.91%] [G loss: 6.952332]\n",
      "epoch:7 step:6230 [D loss: 0.631958, acc.: 67.97%] [G loss: 3.401125]\n",
      "epoch:7 step:6231 [D loss: 0.197563, acc.: 91.41%] [G loss: 4.562685]\n",
      "epoch:7 step:6232 [D loss: 0.064898, acc.: 99.22%] [G loss: 5.826281]\n",
      "epoch:7 step:6233 [D loss: 0.117317, acc.: 95.31%] [G loss: 2.888611]\n",
      "epoch:7 step:6234 [D loss: 0.281938, acc.: 85.94%] [G loss: 4.920674]\n",
      "epoch:7 step:6235 [D loss: 0.229466, acc.: 92.19%] [G loss: 3.624416]\n",
      "epoch:7 step:6236 [D loss: 0.152494, acc.: 94.53%] [G loss: 4.364524]\n",
      "epoch:7 step:6237 [D loss: 0.170644, acc.: 95.31%] [G loss: 2.950136]\n",
      "epoch:7 step:6238 [D loss: 0.236086, acc.: 91.41%] [G loss: 2.136004]\n",
      "epoch:7 step:6239 [D loss: 0.207112, acc.: 93.75%] [G loss: 3.939475]\n",
      "epoch:7 step:6240 [D loss: 0.040605, acc.: 100.00%] [G loss: 4.692245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7 step:6241 [D loss: 0.120583, acc.: 97.66%] [G loss: 2.766393]\n",
      "epoch:7 step:6242 [D loss: 0.077884, acc.: 96.88%] [G loss: 3.114041]\n",
      "epoch:7 step:6243 [D loss: 0.818401, acc.: 63.28%] [G loss: 7.401816]\n",
      "epoch:7 step:6244 [D loss: 1.537746, acc.: 44.53%] [G loss: 4.148222]\n",
      "epoch:7 step:6245 [D loss: 0.117777, acc.: 95.31%] [G loss: 3.471370]\n",
      "epoch:7 step:6246 [D loss: 0.355263, acc.: 84.38%] [G loss: 3.165580]\n",
      "epoch:7 step:6247 [D loss: 0.033367, acc.: 100.00%] [G loss: 3.463105]\n",
      "epoch:7 step:6248 [D loss: 0.045705, acc.: 100.00%] [G loss: 3.606536]\n",
      "epoch:8 step:6249 [D loss: 0.132515, acc.: 97.66%] [G loss: 3.143119]\n",
      "epoch:8 step:6250 [D loss: 0.053247, acc.: 99.22%] [G loss: 3.291688]\n",
      "epoch:8 step:6251 [D loss: 0.272857, acc.: 88.28%] [G loss: 3.331495]\n",
      "epoch:8 step:6252 [D loss: 0.040639, acc.: 100.00%] [G loss: 4.030590]\n",
      "epoch:8 step:6253 [D loss: 0.344301, acc.: 82.81%] [G loss: 2.924324]\n",
      "epoch:8 step:6254 [D loss: 0.116101, acc.: 96.09%] [G loss: 4.363444]\n",
      "epoch:8 step:6255 [D loss: 0.160059, acc.: 94.53%] [G loss: 3.914212]\n",
      "epoch:8 step:6256 [D loss: 0.196811, acc.: 92.19%] [G loss: 4.513538]\n",
      "epoch:8 step:6257 [D loss: 0.123900, acc.: 96.88%] [G loss: 4.117885]\n",
      "epoch:8 step:6258 [D loss: 0.176663, acc.: 96.88%] [G loss: 3.183683]\n",
      "epoch:8 step:6259 [D loss: 0.095156, acc.: 97.66%] [G loss: 3.455590]\n",
      "epoch:8 step:6260 [D loss: 0.267333, acc.: 89.84%] [G loss: 3.505177]\n",
      "epoch:8 step:6261 [D loss: 0.103883, acc.: 97.66%] [G loss: 3.077770]\n",
      "epoch:8 step:6262 [D loss: 0.141980, acc.: 96.09%] [G loss: 4.463265]\n",
      "epoch:8 step:6263 [D loss: 0.095871, acc.: 98.44%] [G loss: 3.704401]\n",
      "epoch:8 step:6264 [D loss: 0.511974, acc.: 77.34%] [G loss: 5.995908]\n",
      "epoch:8 step:6265 [D loss: 0.377894, acc.: 79.69%] [G loss: 4.160166]\n",
      "epoch:8 step:6266 [D loss: 0.061989, acc.: 98.44%] [G loss: 3.721263]\n",
      "epoch:8 step:6267 [D loss: 0.072199, acc.: 99.22%] [G loss: 3.904896]\n",
      "epoch:8 step:6268 [D loss: 0.060671, acc.: 99.22%] [G loss: 2.404136]\n",
      "epoch:8 step:6269 [D loss: 0.172515, acc.: 96.88%] [G loss: 4.966331]\n",
      "epoch:8 step:6270 [D loss: 0.303926, acc.: 85.94%] [G loss: 3.228575]\n",
      "epoch:8 step:6271 [D loss: 0.079652, acc.: 99.22%] [G loss: 4.368217]\n",
      "epoch:8 step:6272 [D loss: 0.060968, acc.: 100.00%] [G loss: 3.794292]\n",
      "epoch:8 step:6273 [D loss: 0.357741, acc.: 89.06%] [G loss: 5.349787]\n",
      "epoch:8 step:6274 [D loss: 0.136288, acc.: 96.09%] [G loss: 5.137447]\n",
      "epoch:8 step:6275 [D loss: 0.071971, acc.: 99.22%] [G loss: 4.670093]\n",
      "epoch:8 step:6276 [D loss: 0.065488, acc.: 98.44%] [G loss: 4.262682]\n",
      "epoch:8 step:6277 [D loss: 0.428614, acc.: 80.47%] [G loss: 6.373350]\n",
      "epoch:8 step:6278 [D loss: 0.574942, acc.: 75.00%] [G loss: 4.110210]\n",
      "epoch:8 step:6279 [D loss: 0.212271, acc.: 92.97%] [G loss: 5.400599]\n",
      "epoch:8 step:6280 [D loss: 0.180051, acc.: 94.53%] [G loss: 4.828791]\n",
      "epoch:8 step:6281 [D loss: 0.051219, acc.: 98.44%] [G loss: 4.174679]\n",
      "epoch:8 step:6282 [D loss: 0.030906, acc.: 100.00%] [G loss: 3.914724]\n",
      "epoch:8 step:6283 [D loss: 0.230738, acc.: 89.84%] [G loss: 5.338151]\n",
      "epoch:8 step:6284 [D loss: 0.432391, acc.: 78.12%] [G loss: 4.326351]\n",
      "epoch:8 step:6285 [D loss: 0.104277, acc.: 96.09%] [G loss: 5.055034]\n",
      "epoch:8 step:6286 [D loss: 0.139524, acc.: 96.09%] [G loss: 4.477672]\n",
      "epoch:8 step:6287 [D loss: 0.137088, acc.: 94.53%] [G loss: 6.137711]\n",
      "epoch:8 step:6288 [D loss: 0.111870, acc.: 96.09%] [G loss: 4.917274]\n",
      "epoch:8 step:6289 [D loss: 0.125191, acc.: 97.66%] [G loss: 4.769858]\n",
      "epoch:8 step:6290 [D loss: 0.031970, acc.: 100.00%] [G loss: 4.458620]\n",
      "epoch:8 step:6291 [D loss: 0.055845, acc.: 100.00%] [G loss: 4.117123]\n",
      "epoch:8 step:6292 [D loss: 0.362354, acc.: 85.16%] [G loss: 5.825799]\n",
      "epoch:8 step:6293 [D loss: 0.240940, acc.: 91.41%] [G loss: 3.155138]\n",
      "epoch:8 step:6294 [D loss: 0.327204, acc.: 85.16%] [G loss: 5.752510]\n",
      "epoch:8 step:6295 [D loss: 0.040523, acc.: 98.44%] [G loss: 6.938118]\n",
      "epoch:8 step:6296 [D loss: 0.184936, acc.: 93.75%] [G loss: 4.294565]\n",
      "epoch:8 step:6297 [D loss: 0.035734, acc.: 99.22%] [G loss: 4.020949]\n",
      "epoch:8 step:6298 [D loss: 0.518587, acc.: 75.00%] [G loss: 8.361132]\n",
      "epoch:8 step:6299 [D loss: 1.045092, acc.: 60.16%] [G loss: 0.857413]\n",
      "epoch:8 step:6300 [D loss: 1.557517, acc.: 53.91%] [G loss: 8.442436]\n",
      "epoch:8 step:6301 [D loss: 1.526546, acc.: 52.34%] [G loss: 2.901130]\n",
      "epoch:8 step:6302 [D loss: 0.315582, acc.: 86.72%] [G loss: 3.280255]\n",
      "epoch:8 step:6303 [D loss: 0.233439, acc.: 92.19%] [G loss: 3.942129]\n",
      "epoch:8 step:6304 [D loss: 0.849517, acc.: 62.50%] [G loss: 3.438781]\n",
      "epoch:8 step:6305 [D loss: 0.213162, acc.: 92.97%] [G loss: 3.192681]\n",
      "epoch:8 step:6306 [D loss: 0.336500, acc.: 86.72%] [G loss: 2.468566]\n",
      "epoch:8 step:6307 [D loss: 0.247515, acc.: 88.28%] [G loss: 3.695809]\n",
      "epoch:8 step:6308 [D loss: 0.197771, acc.: 91.41%] [G loss: 2.576629]\n",
      "epoch:8 step:6309 [D loss: 0.232609, acc.: 95.31%] [G loss: 1.238746]\n",
      "epoch:8 step:6310 [D loss: 0.283916, acc.: 88.28%] [G loss: 2.416389]\n",
      "epoch:8 step:6311 [D loss: 0.210410, acc.: 93.75%] [G loss: 0.561079]\n",
      "epoch:8 step:6312 [D loss: 0.191622, acc.: 92.97%] [G loss: 1.977265]\n",
      "epoch:8 step:6313 [D loss: 0.365491, acc.: 82.03%] [G loss: 2.485573]\n",
      "epoch:8 step:6314 [D loss: 0.620335, acc.: 67.19%] [G loss: 0.523055]\n",
      "epoch:8 step:6315 [D loss: 0.538482, acc.: 70.31%] [G loss: 2.899628]\n",
      "epoch:8 step:6316 [D loss: 0.337294, acc.: 86.72%] [G loss: 3.769035]\n",
      "epoch:8 step:6317 [D loss: 0.591005, acc.: 71.09%] [G loss: 0.526439]\n",
      "epoch:8 step:6318 [D loss: 0.464715, acc.: 75.78%] [G loss: 4.098629]\n",
      "epoch:8 step:6319 [D loss: 0.108074, acc.: 96.88%] [G loss: 4.511310]\n",
      "epoch:8 step:6320 [D loss: 1.250299, acc.: 46.09%] [G loss: 3.299553]\n",
      "epoch:8 step:6321 [D loss: 0.294218, acc.: 90.62%] [G loss: 3.737898]\n",
      "epoch:8 step:6322 [D loss: 0.208609, acc.: 92.97%] [G loss: 3.645825]\n",
      "epoch:8 step:6323 [D loss: 0.263665, acc.: 88.28%] [G loss: 2.202833]\n",
      "epoch:8 step:6324 [D loss: 0.200408, acc.: 92.19%] [G loss: 3.582134]\n",
      "epoch:8 step:6325 [D loss: 0.091374, acc.: 96.88%] [G loss: 3.085499]\n",
      "epoch:8 step:6326 [D loss: 0.077661, acc.: 98.44%] [G loss: 2.650326]\n",
      "epoch:8 step:6327 [D loss: 0.068298, acc.: 99.22%] [G loss: 0.667444]\n",
      "epoch:8 step:6328 [D loss: 0.072033, acc.: 98.44%] [G loss: 0.382920]\n",
      "epoch:8 step:6329 [D loss: 0.022111, acc.: 100.00%] [G loss: 0.075679]\n",
      "epoch:8 step:6330 [D loss: 0.084950, acc.: 97.66%] [G loss: 0.322926]\n",
      "epoch:8 step:6331 [D loss: 0.027030, acc.: 100.00%] [G loss: 0.586777]\n",
      "epoch:8 step:6332 [D loss: 0.054522, acc.: 99.22%] [G loss: 0.691350]\n",
      "epoch:8 step:6333 [D loss: 0.014382, acc.: 100.00%] [G loss: 0.652417]\n",
      "epoch:8 step:6334 [D loss: 0.030241, acc.: 100.00%] [G loss: 0.235693]\n",
      "epoch:8 step:6335 [D loss: 0.029862, acc.: 100.00%] [G loss: 0.058480]\n",
      "epoch:8 step:6336 [D loss: 0.026296, acc.: 100.00%] [G loss: 0.033723]\n",
      "epoch:8 step:6337 [D loss: 0.095260, acc.: 97.66%] [G loss: 0.104605]\n",
      "epoch:8 step:6338 [D loss: 0.204470, acc.: 92.97%] [G loss: 0.115540]\n",
      "epoch:8 step:6339 [D loss: 0.044257, acc.: 99.22%] [G loss: 0.244331]\n",
      "epoch:8 step:6340 [D loss: 0.099422, acc.: 96.88%] [G loss: 1.422309]\n",
      "epoch:8 step:6341 [D loss: 0.038595, acc.: 99.22%] [G loss: 1.343816]\n",
      "epoch:8 step:6342 [D loss: 0.188847, acc.: 92.97%] [G loss: 2.343157]\n",
      "epoch:8 step:6343 [D loss: 0.658449, acc.: 66.41%] [G loss: 0.012211]\n",
      "epoch:8 step:6344 [D loss: 0.252589, acc.: 89.06%] [G loss: 0.343630]\n",
      "epoch:8 step:6345 [D loss: 0.078171, acc.: 97.66%] [G loss: 1.158194]\n",
      "epoch:8 step:6346 [D loss: 0.292943, acc.: 89.06%] [G loss: 0.139761]\n",
      "epoch:8 step:6347 [D loss: 0.189180, acc.: 92.97%] [G loss: 1.287839]\n",
      "epoch:8 step:6348 [D loss: 0.176381, acc.: 95.31%] [G loss: 1.661098]\n",
      "epoch:8 step:6349 [D loss: 0.788047, acc.: 62.50%] [G loss: 2.563909]\n",
      "epoch:8 step:6350 [D loss: 0.057903, acc.: 98.44%] [G loss: 4.131890]\n",
      "epoch:8 step:6351 [D loss: 0.863835, acc.: 60.16%] [G loss: 3.779516]\n",
      "epoch:8 step:6352 [D loss: 0.104763, acc.: 98.44%] [G loss: 4.761057]\n",
      "epoch:8 step:6353 [D loss: 0.183307, acc.: 93.75%] [G loss: 4.067359]\n",
      "epoch:8 step:6354 [D loss: 0.071214, acc.: 98.44%] [G loss: 4.317317]\n",
      "epoch:8 step:6355 [D loss: 0.246708, acc.: 90.62%] [G loss: 3.183582]\n",
      "epoch:8 step:6356 [D loss: 0.148621, acc.: 96.09%] [G loss: 2.551619]\n",
      "epoch:8 step:6357 [D loss: 0.026303, acc.: 100.00%] [G loss: 1.782093]\n",
      "epoch:8 step:6358 [D loss: 0.044942, acc.: 100.00%] [G loss: 1.303011]\n",
      "epoch:8 step:6359 [D loss: 0.178977, acc.: 92.97%] [G loss: 1.434562]\n",
      "epoch:8 step:6360 [D loss: 0.083938, acc.: 97.66%] [G loss: 1.014873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6361 [D loss: 0.067222, acc.: 99.22%] [G loss: 0.558330]\n",
      "epoch:8 step:6362 [D loss: 0.062811, acc.: 99.22%] [G loss: 0.834236]\n",
      "epoch:8 step:6363 [D loss: 0.027046, acc.: 100.00%] [G loss: 1.021525]\n",
      "epoch:8 step:6364 [D loss: 0.198262, acc.: 93.75%] [G loss: 0.308503]\n",
      "epoch:8 step:6365 [D loss: 0.034482, acc.: 100.00%] [G loss: 0.105980]\n",
      "epoch:8 step:6366 [D loss: 0.233132, acc.: 89.84%] [G loss: 2.987154]\n",
      "epoch:8 step:6367 [D loss: 0.305707, acc.: 85.94%] [G loss: 0.225578]\n",
      "epoch:8 step:6368 [D loss: 0.417941, acc.: 80.47%] [G loss: 6.189730]\n",
      "epoch:8 step:6369 [D loss: 0.345329, acc.: 82.03%] [G loss: 3.439479]\n",
      "epoch:8 step:6370 [D loss: 0.099906, acc.: 97.66%] [G loss: 1.952599]\n",
      "epoch:8 step:6371 [D loss: 0.381947, acc.: 82.03%] [G loss: 5.490968]\n",
      "epoch:8 step:6372 [D loss: 0.806251, acc.: 58.59%] [G loss: 5.750223]\n",
      "epoch:8 step:6373 [D loss: 0.328068, acc.: 83.59%] [G loss: 3.003728]\n",
      "epoch:8 step:6374 [D loss: 0.081187, acc.: 98.44%] [G loss: 5.346100]\n",
      "epoch:8 step:6375 [D loss: 0.041957, acc.: 100.00%] [G loss: 4.450810]\n",
      "epoch:8 step:6376 [D loss: 0.100546, acc.: 97.66%] [G loss: 4.794662]\n",
      "epoch:8 step:6377 [D loss: 0.205227, acc.: 89.84%] [G loss: 4.659481]\n",
      "epoch:8 step:6378 [D loss: 0.245962, acc.: 86.72%] [G loss: 0.643949]\n",
      "epoch:8 step:6379 [D loss: 0.045920, acc.: 100.00%] [G loss: 0.320828]\n",
      "epoch:8 step:6380 [D loss: 0.528343, acc.: 74.22%] [G loss: 5.023711]\n",
      "epoch:8 step:6381 [D loss: 0.742203, acc.: 67.97%] [G loss: 1.887205]\n",
      "epoch:8 step:6382 [D loss: 0.039211, acc.: 100.00%] [G loss: 1.502802]\n",
      "epoch:8 step:6383 [D loss: 0.208560, acc.: 90.62%] [G loss: 2.771379]\n",
      "epoch:8 step:6384 [D loss: 0.025174, acc.: 100.00%] [G loss: 4.466121]\n",
      "epoch:8 step:6385 [D loss: 0.243835, acc.: 91.41%] [G loss: 1.436782]\n",
      "epoch:8 step:6386 [D loss: 0.342296, acc.: 85.94%] [G loss: 3.712666]\n",
      "epoch:8 step:6387 [D loss: 0.359782, acc.: 80.47%] [G loss: 1.956145]\n",
      "epoch:8 step:6388 [D loss: 0.104450, acc.: 98.44%] [G loss: 0.844186]\n",
      "epoch:8 step:6389 [D loss: 0.470178, acc.: 75.78%] [G loss: 5.076380]\n",
      "epoch:8 step:6390 [D loss: 1.040727, acc.: 56.25%] [G loss: 1.252297]\n",
      "epoch:8 step:6391 [D loss: 0.741128, acc.: 67.19%] [G loss: 4.781898]\n",
      "epoch:8 step:6392 [D loss: 0.231598, acc.: 88.28%] [G loss: 4.532564]\n",
      "epoch:8 step:6393 [D loss: 0.378688, acc.: 81.25%] [G loss: 2.896281]\n",
      "epoch:8 step:6394 [D loss: 0.267164, acc.: 89.06%] [G loss: 3.841273]\n",
      "epoch:8 step:6395 [D loss: 0.195636, acc.: 94.53%] [G loss: 3.581644]\n",
      "epoch:8 step:6396 [D loss: 0.305397, acc.: 86.72%] [G loss: 4.001666]\n",
      "epoch:8 step:6397 [D loss: 0.143422, acc.: 96.09%] [G loss: 4.008515]\n",
      "epoch:8 step:6398 [D loss: 0.132695, acc.: 96.88%] [G loss: 3.181989]\n",
      "epoch:8 step:6399 [D loss: 0.089051, acc.: 99.22%] [G loss: 2.487930]\n",
      "epoch:8 step:6400 [D loss: 0.177150, acc.: 95.31%] [G loss: 2.840575]\n",
      "epoch:8 step:6401 [D loss: 0.237211, acc.: 92.19%] [G loss: 4.186730]\n",
      "epoch:8 step:6402 [D loss: 0.216008, acc.: 91.41%] [G loss: 3.729651]\n",
      "epoch:8 step:6403 [D loss: 0.143211, acc.: 95.31%] [G loss: 2.350404]\n",
      "epoch:8 step:6404 [D loss: 0.168657, acc.: 95.31%] [G loss: 3.411341]\n",
      "epoch:8 step:6405 [D loss: 0.136602, acc.: 96.09%] [G loss: 3.775895]\n",
      "epoch:8 step:6406 [D loss: 0.289302, acc.: 85.16%] [G loss: 3.786082]\n",
      "epoch:8 step:6407 [D loss: 0.102425, acc.: 95.31%] [G loss: 3.307435]\n",
      "epoch:8 step:6408 [D loss: 0.395116, acc.: 81.25%] [G loss: 4.706678]\n",
      "epoch:8 step:6409 [D loss: 0.116534, acc.: 96.88%] [G loss: 4.344000]\n",
      "epoch:8 step:6410 [D loss: 0.048754, acc.: 99.22%] [G loss: 2.849350]\n",
      "epoch:8 step:6411 [D loss: 0.237586, acc.: 90.62%] [G loss: 4.143937]\n",
      "epoch:8 step:6412 [D loss: 0.186530, acc.: 94.53%] [G loss: 3.065140]\n",
      "epoch:8 step:6413 [D loss: 0.098270, acc.: 97.66%] [G loss: 3.702388]\n",
      "epoch:8 step:6414 [D loss: 0.222165, acc.: 91.41%] [G loss: 3.349570]\n",
      "epoch:8 step:6415 [D loss: 0.171512, acc.: 94.53%] [G loss: 3.094457]\n",
      "epoch:8 step:6416 [D loss: 0.157250, acc.: 94.53%] [G loss: 4.775957]\n",
      "epoch:8 step:6417 [D loss: 0.289286, acc.: 89.84%] [G loss: 3.411564]\n",
      "epoch:8 step:6418 [D loss: 0.311458, acc.: 85.94%] [G loss: 6.215083]\n",
      "epoch:8 step:6419 [D loss: 0.533850, acc.: 72.66%] [G loss: 3.041974]\n",
      "epoch:8 step:6420 [D loss: 0.086880, acc.: 99.22%] [G loss: 2.645492]\n",
      "epoch:8 step:6421 [D loss: 0.322897, acc.: 83.59%] [G loss: 4.789212]\n",
      "epoch:8 step:6422 [D loss: 0.266879, acc.: 87.50%] [G loss: 2.276749]\n",
      "epoch:8 step:6423 [D loss: 0.170247, acc.: 93.75%] [G loss: 2.911958]\n",
      "epoch:8 step:6424 [D loss: 0.506905, acc.: 75.78%] [G loss: 4.860756]\n",
      "epoch:8 step:6425 [D loss: 0.247391, acc.: 89.06%] [G loss: 4.501900]\n",
      "epoch:8 step:6426 [D loss: 0.191104, acc.: 94.53%] [G loss: 4.567735]\n",
      "epoch:8 step:6427 [D loss: 0.056927, acc.: 100.00%] [G loss: 4.327435]\n",
      "epoch:8 step:6428 [D loss: 0.134205, acc.: 95.31%] [G loss: 3.091819]\n",
      "epoch:8 step:6429 [D loss: 0.122964, acc.: 95.31%] [G loss: 3.024195]\n",
      "epoch:8 step:6430 [D loss: 0.046809, acc.: 100.00%] [G loss: 2.971644]\n",
      "epoch:8 step:6431 [D loss: 0.143181, acc.: 96.88%] [G loss: 4.292750]\n",
      "epoch:8 step:6432 [D loss: 0.317978, acc.: 85.94%] [G loss: 5.052747]\n",
      "epoch:8 step:6433 [D loss: 0.544293, acc.: 74.22%] [G loss: 4.685246]\n",
      "epoch:8 step:6434 [D loss: 0.107797, acc.: 96.88%] [G loss: 4.335596]\n",
      "epoch:8 step:6435 [D loss: 0.121503, acc.: 96.88%] [G loss: 2.320881]\n",
      "epoch:8 step:6436 [D loss: 0.157847, acc.: 94.53%] [G loss: 3.615224]\n",
      "epoch:8 step:6437 [D loss: 0.089976, acc.: 98.44%] [G loss: 4.060607]\n",
      "epoch:8 step:6438 [D loss: 0.459566, acc.: 83.59%] [G loss: 1.153523]\n",
      "epoch:8 step:6439 [D loss: 0.047373, acc.: 99.22%] [G loss: 1.983702]\n",
      "epoch:8 step:6440 [D loss: 0.090402, acc.: 95.31%] [G loss: 3.591803]\n",
      "epoch:8 step:6441 [D loss: 0.921031, acc.: 54.69%] [G loss: 2.836779]\n",
      "epoch:8 step:6442 [D loss: 0.019312, acc.: 100.00%] [G loss: 4.000239]\n",
      "epoch:8 step:6443 [D loss: 0.253538, acc.: 89.84%] [G loss: 3.055115]\n",
      "epoch:8 step:6444 [D loss: 0.026674, acc.: 100.00%] [G loss: 2.938689]\n",
      "epoch:8 step:6445 [D loss: 0.104333, acc.: 96.09%] [G loss: 0.843466]\n",
      "epoch:8 step:6446 [D loss: 0.175964, acc.: 92.19%] [G loss: 1.285282]\n",
      "epoch:8 step:6447 [D loss: 0.051893, acc.: 99.22%] [G loss: 2.957379]\n",
      "epoch:8 step:6448 [D loss: 0.399467, acc.: 80.47%] [G loss: 1.546819]\n",
      "epoch:8 step:6449 [D loss: 0.432905, acc.: 77.34%] [G loss: 6.225991]\n",
      "epoch:8 step:6450 [D loss: 0.410538, acc.: 78.12%] [G loss: 4.905874]\n",
      "epoch:8 step:6451 [D loss: 0.240014, acc.: 86.72%] [G loss: 1.434580]\n",
      "epoch:8 step:6452 [D loss: 0.511537, acc.: 77.34%] [G loss: 7.322432]\n",
      "epoch:8 step:6453 [D loss: 0.327647, acc.: 85.16%] [G loss: 5.236018]\n",
      "epoch:8 step:6454 [D loss: 0.177910, acc.: 94.53%] [G loss: 2.504387]\n",
      "epoch:8 step:6455 [D loss: 0.288368, acc.: 87.50%] [G loss: 5.350478]\n",
      "epoch:8 step:6456 [D loss: 0.257961, acc.: 88.28%] [G loss: 4.446421]\n",
      "epoch:8 step:6457 [D loss: 0.228612, acc.: 92.97%] [G loss: 4.617517]\n",
      "epoch:8 step:6458 [D loss: 0.025342, acc.: 100.00%] [G loss: 4.382328]\n",
      "epoch:8 step:6459 [D loss: 0.048592, acc.: 99.22%] [G loss: 4.157704]\n",
      "epoch:8 step:6460 [D loss: 0.056463, acc.: 100.00%] [G loss: 3.384398]\n",
      "epoch:8 step:6461 [D loss: 0.064023, acc.: 98.44%] [G loss: 2.554614]\n",
      "epoch:8 step:6462 [D loss: 2.010787, acc.: 22.66%] [G loss: 7.028489]\n",
      "epoch:8 step:6463 [D loss: 0.797749, acc.: 63.28%] [G loss: 3.335581]\n",
      "epoch:8 step:6464 [D loss: 0.138439, acc.: 93.75%] [G loss: 2.978730]\n",
      "epoch:8 step:6465 [D loss: 0.053375, acc.: 100.00%] [G loss: 2.432247]\n",
      "epoch:8 step:6466 [D loss: 0.111174, acc.: 96.09%] [G loss: 2.378601]\n",
      "epoch:8 step:6467 [D loss: 0.056554, acc.: 98.44%] [G loss: 1.547226]\n",
      "epoch:8 step:6468 [D loss: 0.160969, acc.: 94.53%] [G loss: 0.578239]\n",
      "epoch:8 step:6469 [D loss: 0.671870, acc.: 71.09%] [G loss: 6.360106]\n",
      "epoch:8 step:6470 [D loss: 1.065322, acc.: 57.81%] [G loss: 1.381413]\n",
      "epoch:8 step:6471 [D loss: 0.928749, acc.: 56.25%] [G loss: 5.085403]\n",
      "epoch:8 step:6472 [D loss: 0.970791, acc.: 57.03%] [G loss: 3.920680]\n",
      "epoch:8 step:6473 [D loss: 0.103451, acc.: 96.88%] [G loss: 3.042422]\n",
      "epoch:8 step:6474 [D loss: 0.058267, acc.: 99.22%] [G loss: 2.948773]\n",
      "epoch:8 step:6475 [D loss: 0.111997, acc.: 99.22%] [G loss: 2.573269]\n",
      "epoch:8 step:6476 [D loss: 0.033556, acc.: 100.00%] [G loss: 2.273207]\n",
      "epoch:8 step:6477 [D loss: 0.152296, acc.: 95.31%] [G loss: 2.580884]\n",
      "epoch:8 step:6478 [D loss: 0.075149, acc.: 100.00%] [G loss: 2.774767]\n",
      "epoch:8 step:6479 [D loss: 0.168901, acc.: 95.31%] [G loss: 3.218753]\n",
      "epoch:8 step:6480 [D loss: 0.720272, acc.: 59.38%] [G loss: 3.077487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6481 [D loss: 0.042315, acc.: 100.00%] [G loss: 2.885725]\n",
      "epoch:8 step:6482 [D loss: 0.541007, acc.: 67.97%] [G loss: 2.808346]\n",
      "epoch:8 step:6483 [D loss: 0.111403, acc.: 96.88%] [G loss: 3.954158]\n",
      "epoch:8 step:6484 [D loss: 0.384609, acc.: 82.03%] [G loss: 3.676272]\n",
      "epoch:8 step:6485 [D loss: 0.113409, acc.: 97.66%] [G loss: 2.947083]\n",
      "epoch:8 step:6486 [D loss: 0.249638, acc.: 91.41%] [G loss: 3.584354]\n",
      "epoch:8 step:6487 [D loss: 0.079731, acc.: 99.22%] [G loss: 3.460373]\n",
      "epoch:8 step:6488 [D loss: 0.780961, acc.: 57.03%] [G loss: 3.966730]\n",
      "epoch:8 step:6489 [D loss: 0.270132, acc.: 86.72%] [G loss: 3.112039]\n",
      "epoch:8 step:6490 [D loss: 0.066307, acc.: 98.44%] [G loss: 2.488578]\n",
      "epoch:8 step:6491 [D loss: 0.080073, acc.: 98.44%] [G loss: 2.204989]\n",
      "epoch:8 step:6492 [D loss: 0.054218, acc.: 100.00%] [G loss: 2.386125]\n",
      "epoch:8 step:6493 [D loss: 0.076396, acc.: 97.66%] [G loss: 2.263595]\n",
      "epoch:8 step:6494 [D loss: 0.287244, acc.: 88.28%] [G loss: 2.623302]\n",
      "epoch:8 step:6495 [D loss: 0.274119, acc.: 89.84%] [G loss: 2.902749]\n",
      "epoch:8 step:6496 [D loss: 0.056070, acc.: 100.00%] [G loss: 2.893845]\n",
      "epoch:8 step:6497 [D loss: 0.240220, acc.: 92.19%] [G loss: 2.114652]\n",
      "epoch:8 step:6498 [D loss: 0.429775, acc.: 82.81%] [G loss: 1.135322]\n",
      "epoch:8 step:6499 [D loss: 0.040480, acc.: 99.22%] [G loss: 1.715882]\n",
      "epoch:8 step:6500 [D loss: 0.176979, acc.: 96.09%] [G loss: 2.341614]\n",
      "epoch:8 step:6501 [D loss: 0.060308, acc.: 100.00%] [G loss: 1.944064]\n",
      "epoch:8 step:6502 [D loss: 0.026116, acc.: 100.00%] [G loss: 2.203720]\n",
      "epoch:8 step:6503 [D loss: 0.151588, acc.: 94.53%] [G loss: 0.752474]\n",
      "epoch:8 step:6504 [D loss: 0.162761, acc.: 96.09%] [G loss: 1.950247]\n",
      "epoch:8 step:6505 [D loss: 0.062678, acc.: 100.00%] [G loss: 2.117207]\n",
      "epoch:8 step:6506 [D loss: 0.435682, acc.: 79.69%] [G loss: 3.863834]\n",
      "epoch:8 step:6507 [D loss: 0.170707, acc.: 93.75%] [G loss: 3.031816]\n",
      "epoch:8 step:6508 [D loss: 0.375678, acc.: 78.91%] [G loss: 0.813401]\n",
      "epoch:8 step:6509 [D loss: 0.587264, acc.: 73.44%] [G loss: 6.112458]\n",
      "epoch:8 step:6510 [D loss: 0.337111, acc.: 82.03%] [G loss: 5.362502]\n",
      "epoch:8 step:6511 [D loss: 0.037846, acc.: 99.22%] [G loss: 4.903498]\n",
      "epoch:8 step:6512 [D loss: 0.257793, acc.: 89.84%] [G loss: 2.545897]\n",
      "epoch:8 step:6513 [D loss: 0.147312, acc.: 95.31%] [G loss: 3.429945]\n",
      "epoch:8 step:6514 [D loss: 0.109324, acc.: 96.88%] [G loss: 2.383922]\n",
      "epoch:8 step:6515 [D loss: 0.366221, acc.: 84.38%] [G loss: 4.002029]\n",
      "epoch:8 step:6516 [D loss: 0.031704, acc.: 100.00%] [G loss: 4.472313]\n",
      "epoch:8 step:6517 [D loss: 0.536389, acc.: 76.56%] [G loss: 4.343054]\n",
      "epoch:8 step:6518 [D loss: 0.410139, acc.: 79.69%] [G loss: 4.857470]\n",
      "epoch:8 step:6519 [D loss: 0.129956, acc.: 95.31%] [G loss: 3.143407]\n",
      "epoch:8 step:6520 [D loss: 0.054294, acc.: 98.44%] [G loss: 3.287493]\n",
      "epoch:8 step:6521 [D loss: 0.148919, acc.: 97.66%] [G loss: 3.063656]\n",
      "epoch:8 step:6522 [D loss: 0.069197, acc.: 99.22%] [G loss: 3.481981]\n",
      "epoch:8 step:6523 [D loss: 0.075900, acc.: 98.44%] [G loss: 2.443173]\n",
      "epoch:8 step:6524 [D loss: 0.046716, acc.: 100.00%] [G loss: 1.221588]\n",
      "epoch:8 step:6525 [D loss: 0.157798, acc.: 96.09%] [G loss: 1.155581]\n",
      "epoch:8 step:6526 [D loss: 0.204623, acc.: 91.41%] [G loss: 3.241117]\n",
      "epoch:8 step:6527 [D loss: 0.198759, acc.: 92.19%] [G loss: 1.447999]\n",
      "epoch:8 step:6528 [D loss: 0.082473, acc.: 98.44%] [G loss: 0.686922]\n",
      "epoch:8 step:6529 [D loss: 0.331219, acc.: 85.16%] [G loss: 5.160470]\n",
      "epoch:8 step:6530 [D loss: 0.239791, acc.: 89.06%] [G loss: 4.364923]\n",
      "epoch:8 step:6531 [D loss: 0.909643, acc.: 55.47%] [G loss: 3.093221]\n",
      "epoch:8 step:6532 [D loss: 0.027786, acc.: 100.00%] [G loss: 4.625270]\n",
      "epoch:8 step:6533 [D loss: 0.382477, acc.: 82.81%] [G loss: 1.965611]\n",
      "epoch:8 step:6534 [D loss: 0.451824, acc.: 76.56%] [G loss: 6.544088]\n",
      "epoch:8 step:6535 [D loss: 0.879166, acc.: 62.50%] [G loss: 3.541665]\n",
      "epoch:8 step:6536 [D loss: 0.192059, acc.: 91.41%] [G loss: 5.009500]\n",
      "epoch:8 step:6537 [D loss: 0.102773, acc.: 95.31%] [G loss: 3.884094]\n",
      "epoch:8 step:6538 [D loss: 0.060828, acc.: 98.44%] [G loss: 3.017628]\n",
      "epoch:8 step:6539 [D loss: 0.260059, acc.: 89.84%] [G loss: 3.887999]\n",
      "epoch:8 step:6540 [D loss: 0.061380, acc.: 98.44%] [G loss: 4.548148]\n",
      "epoch:8 step:6541 [D loss: 0.121437, acc.: 96.88%] [G loss: 4.725920]\n",
      "epoch:8 step:6542 [D loss: 0.086004, acc.: 99.22%] [G loss: 3.172277]\n",
      "epoch:8 step:6543 [D loss: 0.098247, acc.: 97.66%] [G loss: 2.443402]\n",
      "epoch:8 step:6544 [D loss: 0.274345, acc.: 85.16%] [G loss: 5.318757]\n",
      "epoch:8 step:6545 [D loss: 0.267822, acc.: 88.28%] [G loss: 3.136910]\n",
      "epoch:8 step:6546 [D loss: 0.032637, acc.: 100.00%] [G loss: 1.635947]\n",
      "epoch:8 step:6547 [D loss: 0.278437, acc.: 88.28%] [G loss: 4.578613]\n",
      "epoch:8 step:6548 [D loss: 0.802624, acc.: 64.06%] [G loss: 1.900062]\n",
      "epoch:8 step:6549 [D loss: 0.242453, acc.: 88.28%] [G loss: 3.818202]\n",
      "epoch:8 step:6550 [D loss: 0.210255, acc.: 90.62%] [G loss: 4.029361]\n",
      "epoch:8 step:6551 [D loss: 0.506164, acc.: 74.22%] [G loss: 3.727931]\n",
      "epoch:8 step:6552 [D loss: 0.075256, acc.: 98.44%] [G loss: 4.715895]\n",
      "epoch:8 step:6553 [D loss: 0.182588, acc.: 92.97%] [G loss: 3.552711]\n",
      "epoch:8 step:6554 [D loss: 0.187538, acc.: 93.75%] [G loss: 4.012473]\n",
      "epoch:8 step:6555 [D loss: 0.101727, acc.: 98.44%] [G loss: 3.726461]\n",
      "epoch:8 step:6556 [D loss: 0.053446, acc.: 99.22%] [G loss: 2.964749]\n",
      "epoch:8 step:6557 [D loss: 0.275213, acc.: 89.84%] [G loss: 4.029615]\n",
      "epoch:8 step:6558 [D loss: 0.048602, acc.: 100.00%] [G loss: 3.772183]\n",
      "epoch:8 step:6559 [D loss: 0.432169, acc.: 81.25%] [G loss: 3.197013]\n",
      "epoch:8 step:6560 [D loss: 0.150132, acc.: 94.53%] [G loss: 3.498780]\n",
      "epoch:8 step:6561 [D loss: 0.053463, acc.: 99.22%] [G loss: 2.597600]\n",
      "epoch:8 step:6562 [D loss: 0.264456, acc.: 89.06%] [G loss: 5.641367]\n",
      "epoch:8 step:6563 [D loss: 1.904378, acc.: 26.56%] [G loss: 4.908534]\n",
      "epoch:8 step:6564 [D loss: 0.126027, acc.: 93.75%] [G loss: 5.793031]\n",
      "epoch:8 step:6565 [D loss: 0.047592, acc.: 99.22%] [G loss: 4.027498]\n",
      "epoch:8 step:6566 [D loss: 0.062784, acc.: 98.44%] [G loss: 3.588912]\n",
      "epoch:8 step:6567 [D loss: 0.143327, acc.: 95.31%] [G loss: 3.887123]\n",
      "epoch:8 step:6568 [D loss: 0.165625, acc.: 96.88%] [G loss: 2.592587]\n",
      "epoch:8 step:6569 [D loss: 0.348124, acc.: 85.16%] [G loss: 4.687381]\n",
      "epoch:8 step:6570 [D loss: 0.415861, acc.: 77.34%] [G loss: 2.342192]\n",
      "epoch:8 step:6571 [D loss: 0.432778, acc.: 75.00%] [G loss: 5.262763]\n",
      "epoch:8 step:6572 [D loss: 0.549113, acc.: 69.53%] [G loss: 3.701079]\n",
      "epoch:8 step:6573 [D loss: 0.201013, acc.: 94.53%] [G loss: 3.743912]\n",
      "epoch:8 step:6574 [D loss: 0.084944, acc.: 100.00%] [G loss: 3.093545]\n",
      "epoch:8 step:6575 [D loss: 0.377814, acc.: 83.59%] [G loss: 4.104627]\n",
      "epoch:8 step:6576 [D loss: 0.230347, acc.: 89.84%] [G loss: 3.123374]\n",
      "epoch:8 step:6577 [D loss: 0.104985, acc.: 96.88%] [G loss: 3.089734]\n",
      "epoch:8 step:6578 [D loss: 0.080229, acc.: 100.00%] [G loss: 2.998256]\n",
      "epoch:8 step:6579 [D loss: 0.428502, acc.: 75.78%] [G loss: 4.311585]\n",
      "epoch:8 step:6580 [D loss: 0.132951, acc.: 95.31%] [G loss: 3.820545]\n",
      "epoch:8 step:6581 [D loss: 0.160814, acc.: 94.53%] [G loss: 3.260844]\n",
      "epoch:8 step:6582 [D loss: 0.176274, acc.: 94.53%] [G loss: 4.146454]\n",
      "epoch:8 step:6583 [D loss: 0.070507, acc.: 98.44%] [G loss: 4.606001]\n",
      "epoch:8 step:6584 [D loss: 0.589642, acc.: 74.22%] [G loss: 2.465367]\n",
      "epoch:8 step:6585 [D loss: 0.243973, acc.: 91.41%] [G loss: 4.993590]\n",
      "epoch:8 step:6586 [D loss: 0.983756, acc.: 53.12%] [G loss: 3.829721]\n",
      "epoch:8 step:6587 [D loss: 0.041843, acc.: 99.22%] [G loss: 5.194834]\n",
      "epoch:8 step:6588 [D loss: 0.263137, acc.: 90.62%] [G loss: 2.894500]\n",
      "epoch:8 step:6589 [D loss: 0.140311, acc.: 95.31%] [G loss: 3.567077]\n",
      "epoch:8 step:6590 [D loss: 0.179410, acc.: 94.53%] [G loss: 2.348274]\n",
      "epoch:8 step:6591 [D loss: 0.156337, acc.: 94.53%] [G loss: 3.894578]\n",
      "epoch:8 step:6592 [D loss: 0.040656, acc.: 100.00%] [G loss: 4.104194]\n",
      "epoch:8 step:6593 [D loss: 0.082614, acc.: 98.44%] [G loss: 1.910229]\n",
      "epoch:8 step:6594 [D loss: 0.187369, acc.: 94.53%] [G loss: 3.020094]\n",
      "epoch:8 step:6595 [D loss: 0.104604, acc.: 98.44%] [G loss: 2.916047]\n",
      "epoch:8 step:6596 [D loss: 0.220122, acc.: 91.41%] [G loss: 4.872226]\n",
      "epoch:8 step:6597 [D loss: 0.278502, acc.: 90.62%] [G loss: 1.472188]\n",
      "epoch:8 step:6598 [D loss: 0.312606, acc.: 88.28%] [G loss: 3.944731]\n",
      "epoch:8 step:6599 [D loss: 0.035793, acc.: 100.00%] [G loss: 4.766033]\n",
      "epoch:8 step:6600 [D loss: 0.457379, acc.: 80.47%] [G loss: 3.168093]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6601 [D loss: 0.285250, acc.: 85.16%] [G loss: 4.790061]\n",
      "epoch:8 step:6602 [D loss: 0.211406, acc.: 90.62%] [G loss: 4.741852]\n",
      "epoch:8 step:6603 [D loss: 0.052842, acc.: 100.00%] [G loss: 3.200587]\n",
      "epoch:8 step:6604 [D loss: 0.117413, acc.: 96.88%] [G loss: 3.564113]\n",
      "epoch:8 step:6605 [D loss: 0.078718, acc.: 98.44%] [G loss: 4.150002]\n",
      "epoch:8 step:6606 [D loss: 1.067380, acc.: 49.22%] [G loss: 6.190795]\n",
      "epoch:8 step:6607 [D loss: 0.284308, acc.: 82.03%] [G loss: 5.156147]\n",
      "epoch:8 step:6608 [D loss: 0.057178, acc.: 99.22%] [G loss: 3.337281]\n",
      "epoch:8 step:6609 [D loss: 0.192167, acc.: 94.53%] [G loss: 4.074907]\n",
      "epoch:8 step:6610 [D loss: 0.022279, acc.: 100.00%] [G loss: 4.679365]\n",
      "epoch:8 step:6611 [D loss: 0.197140, acc.: 94.53%] [G loss: 3.447351]\n",
      "epoch:8 step:6612 [D loss: 0.180955, acc.: 96.88%] [G loss: 4.144403]\n",
      "epoch:8 step:6613 [D loss: 0.043904, acc.: 100.00%] [G loss: 3.944000]\n",
      "epoch:8 step:6614 [D loss: 0.182639, acc.: 92.19%] [G loss: 4.332011]\n",
      "epoch:8 step:6615 [D loss: 0.084409, acc.: 98.44%] [G loss: 3.692254]\n",
      "epoch:8 step:6616 [D loss: 0.065679, acc.: 99.22%] [G loss: 3.653416]\n",
      "epoch:8 step:6617 [D loss: 0.081037, acc.: 100.00%] [G loss: 3.268181]\n",
      "epoch:8 step:6618 [D loss: 0.584726, acc.: 68.75%] [G loss: 5.767083]\n",
      "epoch:8 step:6619 [D loss: 0.157369, acc.: 96.09%] [G loss: 5.774022]\n",
      "epoch:8 step:6620 [D loss: 0.445093, acc.: 78.91%] [G loss: 3.024998]\n",
      "epoch:8 step:6621 [D loss: 0.124613, acc.: 96.88%] [G loss: 3.812918]\n",
      "epoch:8 step:6622 [D loss: 0.118323, acc.: 96.88%] [G loss: 3.762379]\n",
      "epoch:8 step:6623 [D loss: 0.174680, acc.: 94.53%] [G loss: 3.569276]\n",
      "epoch:8 step:6624 [D loss: 0.093921, acc.: 99.22%] [G loss: 4.285113]\n",
      "epoch:8 step:6625 [D loss: 0.064558, acc.: 100.00%] [G loss: 4.257282]\n",
      "epoch:8 step:6626 [D loss: 0.292346, acc.: 89.06%] [G loss: 4.893988]\n",
      "epoch:8 step:6627 [D loss: 0.050298, acc.: 100.00%] [G loss: 4.421989]\n",
      "epoch:8 step:6628 [D loss: 0.036299, acc.: 100.00%] [G loss: 3.085751]\n",
      "epoch:8 step:6629 [D loss: 0.054581, acc.: 100.00%] [G loss: 3.365897]\n",
      "epoch:8 step:6630 [D loss: 0.178031, acc.: 95.31%] [G loss: 4.043695]\n",
      "epoch:8 step:6631 [D loss: 0.070146, acc.: 99.22%] [G loss: 4.891041]\n",
      "epoch:8 step:6632 [D loss: 0.276997, acc.: 88.28%] [G loss: 3.536294]\n",
      "epoch:8 step:6633 [D loss: 0.141881, acc.: 92.97%] [G loss: 4.148776]\n",
      "epoch:8 step:6634 [D loss: 0.133994, acc.: 97.66%] [G loss: 3.204801]\n",
      "epoch:8 step:6635 [D loss: 0.038538, acc.: 99.22%] [G loss: 3.674774]\n",
      "epoch:8 step:6636 [D loss: 0.161907, acc.: 96.09%] [G loss: 2.447866]\n",
      "epoch:8 step:6637 [D loss: 0.092715, acc.: 99.22%] [G loss: 3.495268]\n",
      "epoch:8 step:6638 [D loss: 0.072790, acc.: 99.22%] [G loss: 2.825685]\n",
      "epoch:8 step:6639 [D loss: 0.157371, acc.: 94.53%] [G loss: 1.062599]\n",
      "epoch:8 step:6640 [D loss: 0.022192, acc.: 100.00%] [G loss: 1.498688]\n",
      "epoch:8 step:6641 [D loss: 0.689033, acc.: 64.84%] [G loss: 8.821152]\n",
      "epoch:8 step:6642 [D loss: 2.173651, acc.: 50.78%] [G loss: 2.883804]\n",
      "epoch:8 step:6643 [D loss: 0.404384, acc.: 84.38%] [G loss: 3.164711]\n",
      "epoch:8 step:6644 [D loss: 0.185587, acc.: 96.09%] [G loss: 2.985228]\n",
      "epoch:8 step:6645 [D loss: 0.398677, acc.: 80.47%] [G loss: 2.765645]\n",
      "epoch:8 step:6646 [D loss: 0.112685, acc.: 97.66%] [G loss: 3.801897]\n",
      "epoch:8 step:6647 [D loss: 0.118801, acc.: 96.88%] [G loss: 3.395687]\n",
      "epoch:8 step:6648 [D loss: 0.097299, acc.: 98.44%] [G loss: 3.234139]\n",
      "epoch:8 step:6649 [D loss: 0.243758, acc.: 91.41%] [G loss: 3.849810]\n",
      "epoch:8 step:6650 [D loss: 0.226196, acc.: 90.62%] [G loss: 3.042212]\n",
      "epoch:8 step:6651 [D loss: 0.251223, acc.: 92.97%] [G loss: 4.078855]\n",
      "epoch:8 step:6652 [D loss: 0.160334, acc.: 94.53%] [G loss: 4.442079]\n",
      "epoch:8 step:6653 [D loss: 0.434902, acc.: 80.47%] [G loss: 3.979899]\n",
      "epoch:8 step:6654 [D loss: 0.290725, acc.: 88.28%] [G loss: 4.645040]\n",
      "epoch:8 step:6655 [D loss: 0.024808, acc.: 100.00%] [G loss: 4.679785]\n",
      "epoch:8 step:6656 [D loss: 0.133405, acc.: 96.09%] [G loss: 5.262866]\n",
      "epoch:8 step:6657 [D loss: 0.160949, acc.: 94.53%] [G loss: 4.236534]\n",
      "epoch:8 step:6658 [D loss: 0.089842, acc.: 99.22%] [G loss: 3.657414]\n",
      "epoch:8 step:6659 [D loss: 0.312072, acc.: 85.16%] [G loss: 5.002585]\n",
      "epoch:8 step:6660 [D loss: 0.067304, acc.: 99.22%] [G loss: 5.228295]\n",
      "epoch:8 step:6661 [D loss: 0.275200, acc.: 88.28%] [G loss: 2.669706]\n",
      "epoch:8 step:6662 [D loss: 0.285870, acc.: 88.28%] [G loss: 5.671063]\n",
      "epoch:8 step:6663 [D loss: 0.071771, acc.: 97.66%] [G loss: 6.418423]\n",
      "epoch:8 step:6664 [D loss: 0.061947, acc.: 97.66%] [G loss: 4.417138]\n",
      "epoch:8 step:6665 [D loss: 0.211917, acc.: 90.62%] [G loss: 3.277881]\n",
      "epoch:8 step:6666 [D loss: 0.030534, acc.: 100.00%] [G loss: 2.884629]\n",
      "epoch:8 step:6667 [D loss: 0.079379, acc.: 97.66%] [G loss: 2.768774]\n",
      "epoch:8 step:6668 [D loss: 0.542939, acc.: 74.22%] [G loss: 6.974861]\n",
      "epoch:8 step:6669 [D loss: 0.948213, acc.: 59.38%] [G loss: 1.824069]\n",
      "epoch:8 step:6670 [D loss: 0.618182, acc.: 68.75%] [G loss: 6.478388]\n",
      "epoch:8 step:6671 [D loss: 0.926497, acc.: 60.94%] [G loss: 3.075533]\n",
      "epoch:8 step:6672 [D loss: 0.449981, acc.: 78.12%] [G loss: 5.155415]\n",
      "epoch:8 step:6673 [D loss: 0.208879, acc.: 90.62%] [G loss: 4.668003]\n",
      "epoch:8 step:6674 [D loss: 0.034929, acc.: 100.00%] [G loss: 4.334164]\n",
      "epoch:8 step:6675 [D loss: 0.017880, acc.: 100.00%] [G loss: 3.514961]\n",
      "epoch:8 step:6676 [D loss: 0.385995, acc.: 78.91%] [G loss: 5.629709]\n",
      "epoch:8 step:6677 [D loss: 0.516961, acc.: 73.44%] [G loss: 3.881672]\n",
      "epoch:8 step:6678 [D loss: 0.103861, acc.: 96.88%] [G loss: 2.056626]\n",
      "epoch:8 step:6679 [D loss: 0.148217, acc.: 96.88%] [G loss: 2.616070]\n",
      "epoch:8 step:6680 [D loss: 0.073230, acc.: 98.44%] [G loss: 3.859943]\n",
      "epoch:8 step:6681 [D loss: 0.053707, acc.: 99.22%] [G loss: 1.995568]\n",
      "epoch:8 step:6682 [D loss: 0.125108, acc.: 96.88%] [G loss: 3.074830]\n",
      "epoch:8 step:6683 [D loss: 0.037480, acc.: 100.00%] [G loss: 1.492281]\n",
      "epoch:8 step:6684 [D loss: 0.042820, acc.: 100.00%] [G loss: 1.447393]\n",
      "epoch:8 step:6685 [D loss: 0.745152, acc.: 64.84%] [G loss: 5.411612]\n",
      "epoch:8 step:6686 [D loss: 1.215036, acc.: 56.25%] [G loss: 3.163024]\n",
      "epoch:8 step:6687 [D loss: 0.517532, acc.: 75.78%] [G loss: 2.107070]\n",
      "epoch:8 step:6688 [D loss: 0.045796, acc.: 100.00%] [G loss: 2.624917]\n",
      "epoch:8 step:6689 [D loss: 0.353955, acc.: 82.03%] [G loss: 4.090384]\n",
      "epoch:8 step:6690 [D loss: 0.350590, acc.: 81.25%] [G loss: 2.152560]\n",
      "epoch:8 step:6691 [D loss: 0.073525, acc.: 98.44%] [G loss: 2.063279]\n",
      "epoch:8 step:6692 [D loss: 0.051330, acc.: 100.00%] [G loss: 1.675969]\n",
      "epoch:8 step:6693 [D loss: 0.034031, acc.: 100.00%] [G loss: 2.061700]\n",
      "epoch:8 step:6694 [D loss: 0.164609, acc.: 94.53%] [G loss: 3.374694]\n",
      "epoch:8 step:6695 [D loss: 0.339974, acc.: 83.59%] [G loss: 2.670506]\n",
      "epoch:8 step:6696 [D loss: 0.138894, acc.: 96.88%] [G loss: 3.501627]\n",
      "epoch:8 step:6697 [D loss: 0.087591, acc.: 97.66%] [G loss: 3.113858]\n",
      "epoch:8 step:6698 [D loss: 0.297720, acc.: 90.62%] [G loss: 3.649604]\n",
      "epoch:8 step:6699 [D loss: 0.069815, acc.: 100.00%] [G loss: 3.518093]\n",
      "epoch:8 step:6700 [D loss: 0.086604, acc.: 98.44%] [G loss: 2.075230]\n",
      "epoch:8 step:6701 [D loss: 0.070604, acc.: 100.00%] [G loss: 3.179739]\n",
      "epoch:8 step:6702 [D loss: 0.068180, acc.: 98.44%] [G loss: 1.548607]\n",
      "epoch:8 step:6703 [D loss: 0.051095, acc.: 100.00%] [G loss: 1.225685]\n",
      "epoch:8 step:6704 [D loss: 0.171548, acc.: 96.88%] [G loss: 1.904923]\n",
      "epoch:8 step:6705 [D loss: 0.070944, acc.: 96.88%] [G loss: 1.623836]\n",
      "epoch:8 step:6706 [D loss: 0.073399, acc.: 99.22%] [G loss: 0.718194]\n",
      "epoch:8 step:6707 [D loss: 0.052815, acc.: 99.22%] [G loss: 0.183423]\n",
      "epoch:8 step:6708 [D loss: 0.216294, acc.: 92.19%] [G loss: 0.955961]\n",
      "epoch:8 step:6709 [D loss: 0.060903, acc.: 99.22%] [G loss: 1.438821]\n",
      "epoch:8 step:6710 [D loss: 0.256373, acc.: 87.50%] [G loss: 3.803612]\n",
      "epoch:8 step:6711 [D loss: 0.836983, acc.: 61.72%] [G loss: 0.136665]\n",
      "epoch:8 step:6712 [D loss: 1.287802, acc.: 53.91%] [G loss: 7.162976]\n",
      "epoch:8 step:6713 [D loss: 0.372324, acc.: 75.78%] [G loss: 6.271639]\n",
      "epoch:8 step:6714 [D loss: 0.154385, acc.: 92.97%] [G loss: 4.249502]\n",
      "epoch:8 step:6715 [D loss: 0.169583, acc.: 95.31%] [G loss: 2.744214]\n",
      "epoch:8 step:6716 [D loss: 0.024757, acc.: 100.00%] [G loss: 2.781179]\n",
      "epoch:8 step:6717 [D loss: 0.018849, acc.: 100.00%] [G loss: 1.694054]\n",
      "epoch:8 step:6718 [D loss: 0.201619, acc.: 91.41%] [G loss: 2.662270]\n",
      "epoch:8 step:6719 [D loss: 0.204528, acc.: 90.62%] [G loss: 3.969623]\n",
      "epoch:8 step:6720 [D loss: 0.375714, acc.: 83.59%] [G loss: 3.493432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6721 [D loss: 0.107502, acc.: 95.31%] [G loss: 3.967002]\n",
      "epoch:8 step:6722 [D loss: 0.073854, acc.: 99.22%] [G loss: 2.469374]\n",
      "epoch:8 step:6723 [D loss: 0.284618, acc.: 92.19%] [G loss: 4.377789]\n",
      "epoch:8 step:6724 [D loss: 0.085820, acc.: 99.22%] [G loss: 3.671551]\n",
      "epoch:8 step:6725 [D loss: 0.070555, acc.: 99.22%] [G loss: 3.148289]\n",
      "epoch:8 step:6726 [D loss: 0.285674, acc.: 88.28%] [G loss: 4.803563]\n",
      "epoch:8 step:6727 [D loss: 0.443171, acc.: 79.69%] [G loss: 2.939769]\n",
      "epoch:8 step:6728 [D loss: 0.178762, acc.: 92.19%] [G loss: 3.949873]\n",
      "epoch:8 step:6729 [D loss: 0.068009, acc.: 100.00%] [G loss: 3.597770]\n",
      "epoch:8 step:6730 [D loss: 0.333411, acc.: 86.72%] [G loss: 3.239361]\n",
      "epoch:8 step:6731 [D loss: 0.035358, acc.: 99.22%] [G loss: 2.451580]\n",
      "epoch:8 step:6732 [D loss: 0.045881, acc.: 99.22%] [G loss: 3.412766]\n",
      "epoch:8 step:6733 [D loss: 0.093606, acc.: 98.44%] [G loss: 0.985858]\n",
      "epoch:8 step:6734 [D loss: 0.286039, acc.: 84.38%] [G loss: 5.066006]\n",
      "epoch:8 step:6735 [D loss: 0.377379, acc.: 82.03%] [G loss: 2.685804]\n",
      "epoch:8 step:6736 [D loss: 0.183653, acc.: 94.53%] [G loss: 0.764510]\n",
      "epoch:8 step:6737 [D loss: 0.217153, acc.: 89.84%] [G loss: 3.901271]\n",
      "epoch:8 step:6738 [D loss: 0.033724, acc.: 100.00%] [G loss: 4.806012]\n",
      "epoch:8 step:6739 [D loss: 0.296736, acc.: 89.06%] [G loss: 0.588715]\n",
      "epoch:8 step:6740 [D loss: 0.820779, acc.: 61.72%] [G loss: 6.750623]\n",
      "epoch:8 step:6741 [D loss: 1.063011, acc.: 57.81%] [G loss: 2.452562]\n",
      "epoch:8 step:6742 [D loss: 0.114484, acc.: 96.88%] [G loss: 3.339979]\n",
      "epoch:8 step:6743 [D loss: 0.095997, acc.: 97.66%] [G loss: 3.656097]\n",
      "epoch:8 step:6744 [D loss: 0.109558, acc.: 96.09%] [G loss: 4.166750]\n",
      "epoch:8 step:6745 [D loss: 0.288421, acc.: 87.50%] [G loss: 3.318293]\n",
      "epoch:8 step:6746 [D loss: 0.100437, acc.: 97.66%] [G loss: 3.457032]\n",
      "epoch:8 step:6747 [D loss: 0.069494, acc.: 99.22%] [G loss: 2.541396]\n",
      "epoch:8 step:6748 [D loss: 0.562415, acc.: 70.31%] [G loss: 6.012733]\n",
      "epoch:8 step:6749 [D loss: 1.052199, acc.: 55.47%] [G loss: 2.707492]\n",
      "epoch:8 step:6750 [D loss: 0.226540, acc.: 88.28%] [G loss: 4.327647]\n",
      "epoch:8 step:6751 [D loss: 0.118307, acc.: 94.53%] [G loss: 4.057360]\n",
      "epoch:8 step:6752 [D loss: 0.092737, acc.: 97.66%] [G loss: 3.207517]\n",
      "epoch:8 step:6753 [D loss: 0.063640, acc.: 98.44%] [G loss: 2.472634]\n",
      "epoch:8 step:6754 [D loss: 0.207231, acc.: 93.75%] [G loss: 3.570743]\n",
      "epoch:8 step:6755 [D loss: 0.136909, acc.: 95.31%] [G loss: 3.667361]\n",
      "epoch:8 step:6756 [D loss: 0.579498, acc.: 70.31%] [G loss: 2.880808]\n",
      "epoch:8 step:6757 [D loss: 0.241011, acc.: 89.84%] [G loss: 3.476795]\n",
      "epoch:8 step:6758 [D loss: 0.167708, acc.: 95.31%] [G loss: 3.770678]\n",
      "epoch:8 step:6759 [D loss: 0.242670, acc.: 91.41%] [G loss: 4.248951]\n",
      "epoch:8 step:6760 [D loss: 0.118820, acc.: 96.88%] [G loss: 3.526093]\n",
      "epoch:8 step:6761 [D loss: 0.217098, acc.: 89.84%] [G loss: 2.086620]\n",
      "epoch:8 step:6762 [D loss: 0.143271, acc.: 95.31%] [G loss: 3.339023]\n",
      "epoch:8 step:6763 [D loss: 0.054286, acc.: 100.00%] [G loss: 3.675185]\n",
      "epoch:8 step:6764 [D loss: 0.043342, acc.: 99.22%] [G loss: 2.044007]\n",
      "epoch:8 step:6765 [D loss: 0.031258, acc.: 100.00%] [G loss: 1.406861]\n",
      "epoch:8 step:6766 [D loss: 0.179078, acc.: 93.75%] [G loss: 1.850430]\n",
      "epoch:8 step:6767 [D loss: 0.106683, acc.: 96.88%] [G loss: 1.926496]\n",
      "epoch:8 step:6768 [D loss: 0.089623, acc.: 96.88%] [G loss: 0.510217]\n",
      "epoch:8 step:6769 [D loss: 0.045930, acc.: 98.44%] [G loss: 0.599916]\n",
      "epoch:8 step:6770 [D loss: 0.357951, acc.: 82.03%] [G loss: 3.856888]\n",
      "epoch:8 step:6771 [D loss: 0.230889, acc.: 89.06%] [G loss: 3.670185]\n",
      "epoch:8 step:6772 [D loss: 1.157065, acc.: 46.88%] [G loss: 0.636675]\n",
      "epoch:8 step:6773 [D loss: 0.065978, acc.: 100.00%] [G loss: 2.249304]\n",
      "epoch:8 step:6774 [D loss: 0.893640, acc.: 51.56%] [G loss: 1.995378]\n",
      "epoch:8 step:6775 [D loss: 0.109375, acc.: 96.88%] [G loss: 3.334910]\n",
      "epoch:8 step:6776 [D loss: 0.048382, acc.: 98.44%] [G loss: 4.118861]\n",
      "epoch:8 step:6777 [D loss: 0.151790, acc.: 95.31%] [G loss: 3.724681]\n",
      "epoch:8 step:6778 [D loss: 0.046192, acc.: 100.00%] [G loss: 4.303086]\n",
      "epoch:8 step:6779 [D loss: 0.075680, acc.: 99.22%] [G loss: 3.787285]\n",
      "epoch:8 step:6780 [D loss: 0.309035, acc.: 86.72%] [G loss: 2.044166]\n",
      "epoch:8 step:6781 [D loss: 0.115364, acc.: 98.44%] [G loss: 4.157710]\n",
      "epoch:8 step:6782 [D loss: 0.130752, acc.: 96.88%] [G loss: 2.946569]\n",
      "epoch:8 step:6783 [D loss: 0.048972, acc.: 100.00%] [G loss: 3.035187]\n",
      "epoch:8 step:6784 [D loss: 0.042533, acc.: 99.22%] [G loss: 2.854289]\n",
      "epoch:8 step:6785 [D loss: 0.403432, acc.: 82.03%] [G loss: 4.884873]\n",
      "epoch:8 step:6786 [D loss: 0.306398, acc.: 85.16%] [G loss: 2.914100]\n",
      "epoch:8 step:6787 [D loss: 0.056528, acc.: 100.00%] [G loss: 2.599279]\n",
      "epoch:8 step:6788 [D loss: 0.113968, acc.: 96.09%] [G loss: 3.855227]\n",
      "epoch:8 step:6789 [D loss: 0.082920, acc.: 97.66%] [G loss: 2.773520]\n",
      "epoch:8 step:6790 [D loss: 0.208129, acc.: 92.97%] [G loss: 3.747839]\n",
      "epoch:8 step:6791 [D loss: 0.054617, acc.: 100.00%] [G loss: 3.780520]\n",
      "epoch:8 step:6792 [D loss: 0.124349, acc.: 97.66%] [G loss: 2.104570]\n",
      "epoch:8 step:6793 [D loss: 0.140402, acc.: 96.88%] [G loss: 3.446147]\n",
      "epoch:8 step:6794 [D loss: 0.113584, acc.: 96.88%] [G loss: 3.552259]\n",
      "epoch:8 step:6795 [D loss: 0.165177, acc.: 92.97%] [G loss: 3.268914]\n",
      "epoch:8 step:6796 [D loss: 0.171994, acc.: 93.75%] [G loss: 1.348370]\n",
      "epoch:8 step:6797 [D loss: 0.366496, acc.: 83.59%] [G loss: 4.929737]\n",
      "epoch:8 step:6798 [D loss: 0.573274, acc.: 67.97%] [G loss: 3.009191]\n",
      "epoch:8 step:6799 [D loss: 0.225519, acc.: 89.84%] [G loss: 4.497427]\n",
      "epoch:8 step:6800 [D loss: 0.136814, acc.: 92.97%] [G loss: 3.494510]\n",
      "epoch:8 step:6801 [D loss: 0.047305, acc.: 99.22%] [G loss: 2.058263]\n",
      "epoch:8 step:6802 [D loss: 0.048257, acc.: 100.00%] [G loss: 2.132459]\n",
      "epoch:8 step:6803 [D loss: 0.135606, acc.: 94.53%] [G loss: 3.305159]\n",
      "epoch:8 step:6804 [D loss: 0.085782, acc.: 96.88%] [G loss: 3.336486]\n",
      "epoch:8 step:6805 [D loss: 0.183179, acc.: 92.97%] [G loss: 3.172650]\n",
      "epoch:8 step:6806 [D loss: 0.077657, acc.: 100.00%] [G loss: 1.870424]\n",
      "epoch:8 step:6807 [D loss: 0.126666, acc.: 96.09%] [G loss: 3.173357]\n",
      "epoch:8 step:6808 [D loss: 0.057925, acc.: 99.22%] [G loss: 3.707447]\n",
      "epoch:8 step:6809 [D loss: 0.587277, acc.: 71.88%] [G loss: 5.573110]\n",
      "epoch:8 step:6810 [D loss: 0.769228, acc.: 61.72%] [G loss: 1.191294]\n",
      "epoch:8 step:6811 [D loss: 1.182209, acc.: 60.16%] [G loss: 6.962451]\n",
      "epoch:8 step:6812 [D loss: 0.835209, acc.: 63.28%] [G loss: 3.409397]\n",
      "epoch:8 step:6813 [D loss: 0.108978, acc.: 97.66%] [G loss: 3.444887]\n",
      "epoch:8 step:6814 [D loss: 0.101275, acc.: 97.66%] [G loss: 3.368697]\n",
      "epoch:8 step:6815 [D loss: 0.111816, acc.: 98.44%] [G loss: 2.518271]\n",
      "epoch:8 step:6816 [D loss: 0.165177, acc.: 92.97%] [G loss: 3.316159]\n",
      "epoch:8 step:6817 [D loss: 0.036653, acc.: 99.22%] [G loss: 2.937852]\n",
      "epoch:8 step:6818 [D loss: 0.086182, acc.: 98.44%] [G loss: 2.309398]\n",
      "epoch:8 step:6819 [D loss: 0.067434, acc.: 100.00%] [G loss: 2.954790]\n",
      "epoch:8 step:6820 [D loss: 0.037603, acc.: 100.00%] [G loss: 1.316906]\n",
      "epoch:8 step:6821 [D loss: 0.066443, acc.: 100.00%] [G loss: 1.747635]\n",
      "epoch:8 step:6822 [D loss: 0.708158, acc.: 65.62%] [G loss: 5.760815]\n",
      "epoch:8 step:6823 [D loss: 1.736621, acc.: 50.00%] [G loss: 2.950991]\n",
      "epoch:8 step:6824 [D loss: 0.232525, acc.: 90.62%] [G loss: 2.344952]\n",
      "epoch:8 step:6825 [D loss: 0.064876, acc.: 100.00%] [G loss: 2.203412]\n",
      "epoch:8 step:6826 [D loss: 0.055317, acc.: 99.22%] [G loss: 2.382309]\n",
      "epoch:8 step:6827 [D loss: 0.177794, acc.: 94.53%] [G loss: 2.944490]\n",
      "epoch:8 step:6828 [D loss: 0.173808, acc.: 96.88%] [G loss: 3.220152]\n",
      "epoch:8 step:6829 [D loss: 0.050463, acc.: 99.22%] [G loss: 2.620066]\n",
      "epoch:8 step:6830 [D loss: 0.245217, acc.: 89.06%] [G loss: 3.808717]\n",
      "epoch:8 step:6831 [D loss: 0.128687, acc.: 96.09%] [G loss: 3.490706]\n",
      "epoch:8 step:6832 [D loss: 0.497077, acc.: 75.00%] [G loss: 3.777691]\n",
      "epoch:8 step:6833 [D loss: 0.190581, acc.: 92.19%] [G loss: 3.531120]\n",
      "epoch:8 step:6834 [D loss: 0.061841, acc.: 99.22%] [G loss: 2.838593]\n",
      "epoch:8 step:6835 [D loss: 0.259917, acc.: 92.97%] [G loss: 3.937263]\n",
      "epoch:8 step:6836 [D loss: 0.395764, acc.: 78.12%] [G loss: 1.268350]\n",
      "epoch:8 step:6837 [D loss: 0.069730, acc.: 99.22%] [G loss: 1.718363]\n",
      "epoch:8 step:6838 [D loss: 0.033754, acc.: 100.00%] [G loss: 1.470593]\n",
      "epoch:8 step:6839 [D loss: 0.042974, acc.: 99.22%] [G loss: 0.886492]\n",
      "epoch:8 step:6840 [D loss: 0.022623, acc.: 100.00%] [G loss: 0.789611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6841 [D loss: 0.058035, acc.: 100.00%] [G loss: 1.119898]\n",
      "epoch:8 step:6842 [D loss: 0.049511, acc.: 99.22%] [G loss: 1.268582]\n",
      "epoch:8 step:6843 [D loss: 0.036850, acc.: 100.00%] [G loss: 1.352902]\n",
      "epoch:8 step:6844 [D loss: 0.087709, acc.: 99.22%] [G loss: 1.418487]\n",
      "epoch:8 step:6845 [D loss: 0.033743, acc.: 100.00%] [G loss: 1.478278]\n",
      "epoch:8 step:6846 [D loss: 0.132572, acc.: 99.22%] [G loss: 1.664933]\n",
      "epoch:8 step:6847 [D loss: 0.107988, acc.: 97.66%] [G loss: 1.648018]\n",
      "epoch:8 step:6848 [D loss: 0.325870, acc.: 85.16%] [G loss: 3.421659]\n",
      "epoch:8 step:6849 [D loss: 0.393614, acc.: 78.12%] [G loss: 2.384788]\n",
      "epoch:8 step:6850 [D loss: 0.193192, acc.: 90.62%] [G loss: 4.127922]\n",
      "epoch:8 step:6851 [D loss: 0.043324, acc.: 99.22%] [G loss: 4.152189]\n",
      "epoch:8 step:6852 [D loss: 1.093498, acc.: 39.84%] [G loss: 3.366921]\n",
      "epoch:8 step:6853 [D loss: 0.103509, acc.: 97.66%] [G loss: 4.418295]\n",
      "epoch:8 step:6854 [D loss: 0.051551, acc.: 100.00%] [G loss: 3.756651]\n",
      "epoch:8 step:6855 [D loss: 0.133298, acc.: 94.53%] [G loss: 2.527910]\n",
      "epoch:8 step:6856 [D loss: 0.149840, acc.: 96.09%] [G loss: 4.134801]\n",
      "epoch:8 step:6857 [D loss: 0.307370, acc.: 87.50%] [G loss: 3.512814]\n",
      "epoch:8 step:6858 [D loss: 0.214451, acc.: 92.97%] [G loss: 3.515181]\n",
      "epoch:8 step:6859 [D loss: 0.047125, acc.: 100.00%] [G loss: 4.360256]\n",
      "epoch:8 step:6860 [D loss: 0.058552, acc.: 99.22%] [G loss: 2.593076]\n",
      "epoch:8 step:6861 [D loss: 0.521410, acc.: 80.47%] [G loss: 5.360752]\n",
      "epoch:8 step:6862 [D loss: 0.314631, acc.: 84.38%] [G loss: 2.858638]\n",
      "epoch:8 step:6863 [D loss: 0.064183, acc.: 100.00%] [G loss: 3.503365]\n",
      "epoch:8 step:6864 [D loss: 0.202225, acc.: 90.62%] [G loss: 4.184559]\n",
      "epoch:8 step:6865 [D loss: 0.102368, acc.: 96.09%] [G loss: 3.296784]\n",
      "epoch:8 step:6866 [D loss: 0.145454, acc.: 96.88%] [G loss: 4.796975]\n",
      "epoch:8 step:6867 [D loss: 0.264774, acc.: 89.06%] [G loss: 2.482953]\n",
      "epoch:8 step:6868 [D loss: 0.113911, acc.: 96.09%] [G loss: 4.883859]\n",
      "epoch:8 step:6869 [D loss: 0.126362, acc.: 96.88%] [G loss: 3.209612]\n",
      "epoch:8 step:6870 [D loss: 0.176595, acc.: 94.53%] [G loss: 4.429613]\n",
      "epoch:8 step:6871 [D loss: 0.142077, acc.: 96.09%] [G loss: 3.785205]\n",
      "epoch:8 step:6872 [D loss: 0.104966, acc.: 97.66%] [G loss: 3.448814]\n",
      "epoch:8 step:6873 [D loss: 0.076109, acc.: 98.44%] [G loss: 2.931689]\n",
      "epoch:8 step:6874 [D loss: 0.142121, acc.: 94.53%] [G loss: 4.839942]\n",
      "epoch:8 step:6875 [D loss: 0.201255, acc.: 92.19%] [G loss: 3.505546]\n",
      "epoch:8 step:6876 [D loss: 0.190912, acc.: 95.31%] [G loss: 3.645633]\n",
      "epoch:8 step:6877 [D loss: 0.085612, acc.: 97.66%] [G loss: 5.363636]\n",
      "epoch:8 step:6878 [D loss: 0.066656, acc.: 99.22%] [G loss: 3.494904]\n",
      "epoch:8 step:6879 [D loss: 0.298797, acc.: 85.16%] [G loss: 6.051624]\n",
      "epoch:8 step:6880 [D loss: 0.336178, acc.: 85.94%] [G loss: 2.238327]\n",
      "epoch:8 step:6881 [D loss: 0.060998, acc.: 98.44%] [G loss: 2.181610]\n",
      "epoch:8 step:6882 [D loss: 0.005473, acc.: 100.00%] [G loss: 1.674890]\n",
      "epoch:8 step:6883 [D loss: 0.098960, acc.: 97.66%] [G loss: 3.255190]\n",
      "epoch:8 step:6884 [D loss: 0.029511, acc.: 99.22%] [G loss: 2.251389]\n",
      "epoch:8 step:6885 [D loss: 1.545175, acc.: 40.62%] [G loss: 7.274014]\n",
      "epoch:8 step:6886 [D loss: 0.867804, acc.: 65.62%] [G loss: 4.063344]\n",
      "epoch:8 step:6887 [D loss: 0.123012, acc.: 96.88%] [G loss: 2.607168]\n",
      "epoch:8 step:6888 [D loss: 0.048356, acc.: 100.00%] [G loss: 1.644843]\n",
      "epoch:8 step:6889 [D loss: 0.231106, acc.: 85.94%] [G loss: 4.824920]\n",
      "epoch:8 step:6890 [D loss: 0.246141, acc.: 89.84%] [G loss: 4.788273]\n",
      "epoch:8 step:6891 [D loss: 0.058740, acc.: 99.22%] [G loss: 2.659456]\n",
      "epoch:8 step:6892 [D loss: 0.202397, acc.: 89.84%] [G loss: 4.260684]\n",
      "epoch:8 step:6893 [D loss: 0.053230, acc.: 98.44%] [G loss: 5.197814]\n",
      "epoch:8 step:6894 [D loss: 2.580194, acc.: 8.59%] [G loss: 5.024269]\n",
      "epoch:8 step:6895 [D loss: 0.282541, acc.: 89.84%] [G loss: 5.862499]\n",
      "epoch:8 step:6896 [D loss: 0.379592, acc.: 80.47%] [G loss: 4.371730]\n",
      "epoch:8 step:6897 [D loss: 0.116823, acc.: 96.88%] [G loss: 2.878954]\n",
      "epoch:8 step:6898 [D loss: 0.137977, acc.: 94.53%] [G loss: 3.689901]\n",
      "epoch:8 step:6899 [D loss: 0.025995, acc.: 100.00%] [G loss: 3.783984]\n",
      "epoch:8 step:6900 [D loss: 0.090793, acc.: 96.88%] [G loss: 2.717816]\n",
      "epoch:8 step:6901 [D loss: 0.121838, acc.: 97.66%] [G loss: 3.071433]\n",
      "epoch:8 step:6902 [D loss: 0.049482, acc.: 100.00%] [G loss: 3.401004]\n",
      "epoch:8 step:6903 [D loss: 0.219887, acc.: 92.97%] [G loss: 2.879485]\n",
      "epoch:8 step:6904 [D loss: 0.154269, acc.: 98.44%] [G loss: 3.658090]\n",
      "epoch:8 step:6905 [D loss: 0.161114, acc.: 96.09%] [G loss: 3.595402]\n",
      "epoch:8 step:6906 [D loss: 0.094102, acc.: 97.66%] [G loss: 2.602415]\n",
      "epoch:8 step:6907 [D loss: 0.159134, acc.: 94.53%] [G loss: 2.977815]\n",
      "epoch:8 step:6908 [D loss: 0.136261, acc.: 96.09%] [G loss: 3.874438]\n",
      "epoch:8 step:6909 [D loss: 0.366766, acc.: 82.03%] [G loss: 2.613442]\n",
      "epoch:8 step:6910 [D loss: 0.134820, acc.: 97.66%] [G loss: 2.760794]\n",
      "epoch:8 step:6911 [D loss: 0.278601, acc.: 88.28%] [G loss: 5.284370]\n",
      "epoch:8 step:6912 [D loss: 0.153918, acc.: 94.53%] [G loss: 4.414522]\n",
      "epoch:8 step:6913 [D loss: 0.127224, acc.: 98.44%] [G loss: 2.638341]\n",
      "epoch:8 step:6914 [D loss: 0.044345, acc.: 100.00%] [G loss: 1.946134]\n",
      "epoch:8 step:6915 [D loss: 0.034513, acc.: 100.00%] [G loss: 2.412465]\n",
      "epoch:8 step:6916 [D loss: 0.039326, acc.: 100.00%] [G loss: 0.982055]\n",
      "epoch:8 step:6917 [D loss: 0.168378, acc.: 92.97%] [G loss: 4.318954]\n",
      "epoch:8 step:6918 [D loss: 0.225613, acc.: 89.84%] [G loss: 2.996829]\n",
      "epoch:8 step:6919 [D loss: 0.081053, acc.: 99.22%] [G loss: 1.834082]\n",
      "epoch:8 step:6920 [D loss: 0.145942, acc.: 97.66%] [G loss: 1.691249]\n",
      "epoch:8 step:6921 [D loss: 0.087945, acc.: 98.44%] [G loss: 1.633153]\n",
      "epoch:8 step:6922 [D loss: 0.118614, acc.: 95.31%] [G loss: 2.655862]\n",
      "epoch:8 step:6923 [D loss: 0.556614, acc.: 72.66%] [G loss: 3.514836]\n",
      "epoch:8 step:6924 [D loss: 0.032025, acc.: 99.22%] [G loss: 3.835775]\n",
      "epoch:8 step:6925 [D loss: 0.215465, acc.: 90.62%] [G loss: 0.854699]\n",
      "epoch:8 step:6926 [D loss: 0.349084, acc.: 82.81%] [G loss: 5.483183]\n",
      "epoch:8 step:6927 [D loss: 0.533618, acc.: 69.53%] [G loss: 2.301708]\n",
      "epoch:8 step:6928 [D loss: 0.048287, acc.: 100.00%] [G loss: 2.065469]\n",
      "epoch:8 step:6929 [D loss: 0.229035, acc.: 88.28%] [G loss: 5.496611]\n",
      "epoch:8 step:6930 [D loss: 0.385517, acc.: 82.03%] [G loss: 3.505470]\n",
      "epoch:8 step:6931 [D loss: 0.059055, acc.: 98.44%] [G loss: 3.384062]\n",
      "epoch:8 step:6932 [D loss: 0.114842, acc.: 96.88%] [G loss: 4.784349]\n",
      "epoch:8 step:6933 [D loss: 0.445181, acc.: 80.47%] [G loss: 3.732487]\n",
      "epoch:8 step:6934 [D loss: 0.075758, acc.: 99.22%] [G loss: 4.110151]\n",
      "epoch:8 step:6935 [D loss: 0.551676, acc.: 75.00%] [G loss: 6.420895]\n",
      "epoch:8 step:6936 [D loss: 0.623275, acc.: 71.09%] [G loss: 2.045755]\n",
      "epoch:8 step:6937 [D loss: 0.011841, acc.: 100.00%] [G loss: 1.480376]\n",
      "epoch:8 step:6938 [D loss: 0.272864, acc.: 86.72%] [G loss: 4.176732]\n",
      "epoch:8 step:6939 [D loss: 0.012775, acc.: 100.00%] [G loss: 6.577553]\n",
      "epoch:8 step:6940 [D loss: 1.515411, acc.: 52.34%] [G loss: 0.348221]\n",
      "epoch:8 step:6941 [D loss: 1.332357, acc.: 55.47%] [G loss: 5.757621]\n",
      "epoch:8 step:6942 [D loss: 0.364267, acc.: 80.47%] [G loss: 4.121507]\n",
      "epoch:8 step:6943 [D loss: 0.245958, acc.: 89.84%] [G loss: 2.396624]\n",
      "epoch:8 step:6944 [D loss: 0.063566, acc.: 98.44%] [G loss: 2.323863]\n",
      "epoch:8 step:6945 [D loss: 0.061522, acc.: 100.00%] [G loss: 2.171859]\n",
      "epoch:8 step:6946 [D loss: 0.065653, acc.: 99.22%] [G loss: 3.275331]\n",
      "epoch:8 step:6947 [D loss: 0.039471, acc.: 99.22%] [G loss: 1.789640]\n",
      "epoch:8 step:6948 [D loss: 0.051464, acc.: 99.22%] [G loss: 1.150817]\n",
      "epoch:8 step:6949 [D loss: 0.046391, acc.: 100.00%] [G loss: 0.478122]\n",
      "epoch:8 step:6950 [D loss: 0.108424, acc.: 99.22%] [G loss: 1.057504]\n",
      "epoch:8 step:6951 [D loss: 0.094194, acc.: 99.22%] [G loss: 0.807048]\n",
      "epoch:8 step:6952 [D loss: 0.248673, acc.: 90.62%] [G loss: 0.075496]\n",
      "epoch:8 step:6953 [D loss: 0.205690, acc.: 90.62%] [G loss: 1.852130]\n",
      "epoch:8 step:6954 [D loss: 0.061499, acc.: 97.66%] [G loss: 2.264381]\n",
      "epoch:8 step:6955 [D loss: 0.937034, acc.: 47.66%] [G loss: 2.154213]\n",
      "epoch:8 step:6956 [D loss: 0.142123, acc.: 96.88%] [G loss: 3.613455]\n",
      "epoch:8 step:6957 [D loss: 0.049267, acc.: 99.22%] [G loss: 2.154727]\n",
      "epoch:8 step:6958 [D loss: 0.068729, acc.: 97.66%] [G loss: 1.326138]\n",
      "epoch:8 step:6959 [D loss: 0.076791, acc.: 99.22%] [G loss: 0.908966]\n",
      "epoch:8 step:6960 [D loss: 0.288636, acc.: 87.50%] [G loss: 3.684691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8 step:6961 [D loss: 1.750883, acc.: 45.31%] [G loss: 2.081748]\n",
      "epoch:8 step:6962 [D loss: 0.204243, acc.: 89.84%] [G loss: 2.227101]\n",
      "epoch:8 step:6963 [D loss: 0.598422, acc.: 74.22%] [G loss: 3.181668]\n",
      "epoch:8 step:6964 [D loss: 0.075503, acc.: 99.22%] [G loss: 4.065256]\n",
      "epoch:8 step:6965 [D loss: 0.123531, acc.: 97.66%] [G loss: 3.374756]\n",
      "epoch:8 step:6966 [D loss: 0.101946, acc.: 98.44%] [G loss: 2.258990]\n",
      "epoch:8 step:6967 [D loss: 0.280880, acc.: 89.06%] [G loss: 3.584796]\n",
      "epoch:8 step:6968 [D loss: 0.062077, acc.: 100.00%] [G loss: 2.488391]\n",
      "epoch:8 step:6969 [D loss: 0.173981, acc.: 94.53%] [G loss: 3.611022]\n",
      "epoch:8 step:6970 [D loss: 0.148356, acc.: 96.88%] [G loss: 4.214273]\n",
      "epoch:8 step:6971 [D loss: 0.138244, acc.: 96.88%] [G loss: 3.689150]\n",
      "epoch:8 step:6972 [D loss: 0.126582, acc.: 97.66%] [G loss: 2.734160]\n",
      "epoch:8 step:6973 [D loss: 0.217700, acc.: 95.31%] [G loss: 2.329465]\n",
      "epoch:8 step:6974 [D loss: 0.106194, acc.: 97.66%] [G loss: 3.331029]\n",
      "epoch:8 step:6975 [D loss: 0.199976, acc.: 96.09%] [G loss: 3.100373]\n",
      "epoch:8 step:6976 [D loss: 0.099603, acc.: 96.09%] [G loss: 3.388499]\n",
      "epoch:8 step:6977 [D loss: 0.138069, acc.: 96.09%] [G loss: 2.763577]\n",
      "epoch:8 step:6978 [D loss: 0.068978, acc.: 97.66%] [G loss: 2.349656]\n",
      "epoch:8 step:6979 [D loss: 0.186105, acc.: 96.88%] [G loss: 2.204420]\n",
      "epoch:8 step:6980 [D loss: 0.094745, acc.: 96.88%] [G loss: 1.959827]\n",
      "epoch:8 step:6981 [D loss: 0.409249, acc.: 80.47%] [G loss: 4.705245]\n",
      "epoch:8 step:6982 [D loss: 0.055933, acc.: 99.22%] [G loss: 4.777563]\n",
      "epoch:8 step:6983 [D loss: 0.703551, acc.: 67.19%] [G loss: 1.456821]\n",
      "epoch:8 step:6984 [D loss: 0.171954, acc.: 92.19%] [G loss: 3.529162]\n",
      "epoch:8 step:6985 [D loss: 0.117826, acc.: 96.88%] [G loss: 4.001639]\n",
      "epoch:8 step:6986 [D loss: 0.083597, acc.: 99.22%] [G loss: 3.861058]\n",
      "epoch:8 step:6987 [D loss: 0.096015, acc.: 99.22%] [G loss: 3.232495]\n",
      "epoch:8 step:6988 [D loss: 0.100552, acc.: 97.66%] [G loss: 3.788536]\n",
      "epoch:8 step:6989 [D loss: 0.051904, acc.: 100.00%] [G loss: 3.939703]\n",
      "epoch:8 step:6990 [D loss: 0.072450, acc.: 100.00%] [G loss: 3.242316]\n",
      "epoch:8 step:6991 [D loss: 0.283678, acc.: 88.28%] [G loss: 3.407023]\n",
      "epoch:8 step:6992 [D loss: 0.128192, acc.: 96.09%] [G loss: 4.284651]\n",
      "epoch:8 step:6993 [D loss: 0.152119, acc.: 95.31%] [G loss: 3.155729]\n",
      "epoch:8 step:6994 [D loss: 0.151451, acc.: 96.09%] [G loss: 4.712437]\n",
      "epoch:8 step:6995 [D loss: 0.723780, acc.: 59.38%] [G loss: 6.569185]\n",
      "epoch:8 step:6996 [D loss: 0.615948, acc.: 68.75%] [G loss: 2.788985]\n",
      "epoch:8 step:6997 [D loss: 0.141817, acc.: 96.88%] [G loss: 3.149554]\n",
      "epoch:8 step:6998 [D loss: 0.041732, acc.: 100.00%] [G loss: 4.012125]\n",
      "epoch:8 step:6999 [D loss: 0.021482, acc.: 100.00%] [G loss: 3.305732]\n",
      "epoch:8 step:7000 [D loss: 0.055077, acc.: 98.44%] [G loss: 1.735512]\n",
      "epoch:8 step:7001 [D loss: 0.060986, acc.: 100.00%] [G loss: 0.635765]\n",
      "epoch:8 step:7002 [D loss: 0.361558, acc.: 82.81%] [G loss: 4.063848]\n",
      "epoch:8 step:7003 [D loss: 0.141005, acc.: 94.53%] [G loss: 3.480514]\n",
      "epoch:8 step:7004 [D loss: 0.072400, acc.: 97.66%] [G loss: 2.387518]\n",
      "epoch:8 step:7005 [D loss: 0.106056, acc.: 98.44%] [G loss: 2.724830]\n",
      "epoch:8 step:7006 [D loss: 0.028318, acc.: 100.00%] [G loss: 2.227546]\n",
      "epoch:8 step:7007 [D loss: 0.128784, acc.: 98.44%] [G loss: 1.725822]\n",
      "epoch:8 step:7008 [D loss: 0.027921, acc.: 100.00%] [G loss: 1.680346]\n",
      "epoch:8 step:7009 [D loss: 0.113831, acc.: 97.66%] [G loss: 3.226251]\n",
      "epoch:8 step:7010 [D loss: 1.788775, acc.: 23.44%] [G loss: 4.874116]\n",
      "epoch:8 step:7011 [D loss: 0.569664, acc.: 71.88%] [G loss: 4.416560]\n",
      "epoch:8 step:7012 [D loss: 0.235634, acc.: 89.84%] [G loss: 2.500100]\n",
      "epoch:8 step:7013 [D loss: 0.084435, acc.: 99.22%] [G loss: 3.600654]\n",
      "epoch:8 step:7014 [D loss: 0.128579, acc.: 94.53%] [G loss: 4.597255]\n",
      "epoch:8 step:7015 [D loss: 0.042527, acc.: 99.22%] [G loss: 3.185361]\n",
      "epoch:8 step:7016 [D loss: 0.142265, acc.: 95.31%] [G loss: 2.530218]\n",
      "epoch:8 step:7017 [D loss: 0.167609, acc.: 92.97%] [G loss: 3.496550]\n",
      "epoch:8 step:7018 [D loss: 0.083202, acc.: 97.66%] [G loss: 3.898213]\n",
      "epoch:8 step:7019 [D loss: 0.471793, acc.: 76.56%] [G loss: 2.046154]\n",
      "epoch:8 step:7020 [D loss: 0.174650, acc.: 92.19%] [G loss: 4.185138]\n",
      "epoch:8 step:7021 [D loss: 0.033065, acc.: 99.22%] [G loss: 4.984001]\n",
      "epoch:8 step:7022 [D loss: 0.048745, acc.: 100.00%] [G loss: 3.779265]\n",
      "epoch:8 step:7023 [D loss: 0.059425, acc.: 100.00%] [G loss: 3.928867]\n",
      "epoch:8 step:7024 [D loss: 0.047907, acc.: 100.00%] [G loss: 3.555013]\n",
      "epoch:8 step:7025 [D loss: 0.127109, acc.: 96.88%] [G loss: 4.119027]\n",
      "epoch:8 step:7026 [D loss: 0.080761, acc.: 99.22%] [G loss: 4.454882]\n",
      "epoch:8 step:7027 [D loss: 0.175373, acc.: 96.09%] [G loss: 3.844961]\n",
      "epoch:8 step:7028 [D loss: 0.116921, acc.: 98.44%] [G loss: 3.811525]\n",
      "epoch:8 step:7029 [D loss: 0.069503, acc.: 100.00%] [G loss: 4.534459]\n",
      "epoch:9 step:7030 [D loss: 0.193167, acc.: 90.62%] [G loss: 3.295232]\n",
      "epoch:9 step:7031 [D loss: 0.070057, acc.: 99.22%] [G loss: 3.383413]\n",
      "epoch:9 step:7032 [D loss: 0.098457, acc.: 97.66%] [G loss: 3.259625]\n",
      "epoch:9 step:7033 [D loss: 0.152109, acc.: 95.31%] [G loss: 3.556958]\n",
      "epoch:9 step:7034 [D loss: 0.083552, acc.: 96.09%] [G loss: 4.232169]\n",
      "epoch:9 step:7035 [D loss: 0.061394, acc.: 99.22%] [G loss: 4.396264]\n",
      "epoch:9 step:7036 [D loss: 0.285581, acc.: 89.06%] [G loss: 1.307654]\n",
      "epoch:9 step:7037 [D loss: 0.043896, acc.: 99.22%] [G loss: 1.231617]\n",
      "epoch:9 step:7038 [D loss: 0.074114, acc.: 96.88%] [G loss: 2.352925]\n",
      "epoch:9 step:7039 [D loss: 0.053637, acc.: 99.22%] [G loss: 0.437670]\n",
      "epoch:9 step:7040 [D loss: 0.063857, acc.: 98.44%] [G loss: 3.384597]\n",
      "epoch:9 step:7041 [D loss: 2.748782, acc.: 20.31%] [G loss: 7.119781]\n",
      "epoch:9 step:7042 [D loss: 1.847464, acc.: 49.22%] [G loss: 3.113315]\n",
      "epoch:9 step:7043 [D loss: 0.437949, acc.: 80.47%] [G loss: 2.804510]\n",
      "epoch:9 step:7044 [D loss: 0.215486, acc.: 92.97%] [G loss: 2.713751]\n",
      "epoch:9 step:7045 [D loss: 0.364896, acc.: 84.38%] [G loss: 3.109767]\n",
      "epoch:9 step:7046 [D loss: 0.132067, acc.: 96.09%] [G loss: 2.910842]\n",
      "epoch:9 step:7047 [D loss: 0.240829, acc.: 92.97%] [G loss: 3.350075]\n",
      "epoch:9 step:7048 [D loss: 0.146176, acc.: 98.44%] [G loss: 2.319924]\n",
      "epoch:9 step:7049 [D loss: 0.321368, acc.: 85.16%] [G loss: 4.624869]\n",
      "epoch:9 step:7050 [D loss: 0.552492, acc.: 66.41%] [G loss: 2.221115]\n",
      "epoch:9 step:7051 [D loss: 0.117801, acc.: 98.44%] [G loss: 1.930128]\n",
      "epoch:9 step:7052 [D loss: 0.154097, acc.: 92.97%] [G loss: 2.888242]\n",
      "epoch:9 step:7053 [D loss: 0.195184, acc.: 92.19%] [G loss: 1.158590]\n",
      "epoch:9 step:7054 [D loss: 0.097392, acc.: 99.22%] [G loss: 0.760975]\n",
      "epoch:9 step:7055 [D loss: 0.127775, acc.: 96.88%] [G loss: 0.508732]\n",
      "epoch:9 step:7056 [D loss: 0.044384, acc.: 100.00%] [G loss: 1.187532]\n",
      "epoch:9 step:7057 [D loss: 0.118311, acc.: 95.31%] [G loss: 0.416882]\n",
      "epoch:9 step:7058 [D loss: 0.076310, acc.: 100.00%] [G loss: 0.167817]\n",
      "epoch:9 step:7059 [D loss: 0.747967, acc.: 64.84%] [G loss: 5.626954]\n",
      "epoch:9 step:7060 [D loss: 1.258174, acc.: 51.56%] [G loss: 0.999020]\n",
      "epoch:9 step:7061 [D loss: 0.360353, acc.: 82.81%] [G loss: 1.729977]\n",
      "epoch:9 step:7062 [D loss: 0.140706, acc.: 93.75%] [G loss: 2.527558]\n",
      "epoch:9 step:7063 [D loss: 0.107550, acc.: 97.66%] [G loss: 3.244395]\n",
      "epoch:9 step:7064 [D loss: 0.268461, acc.: 89.06%] [G loss: 1.288806]\n",
      "epoch:9 step:7065 [D loss: 0.322920, acc.: 88.28%] [G loss: 2.528556]\n",
      "epoch:9 step:7066 [D loss: 0.030493, acc.: 100.00%] [G loss: 2.995275]\n",
      "epoch:9 step:7067 [D loss: 0.534247, acc.: 71.88%] [G loss: 2.791692]\n",
      "epoch:9 step:7068 [D loss: 0.102123, acc.: 96.88%] [G loss: 3.514856]\n",
      "epoch:9 step:7069 [D loss: 0.066706, acc.: 98.44%] [G loss: 2.051677]\n",
      "epoch:9 step:7070 [D loss: 0.163095, acc.: 96.88%] [G loss: 1.555867]\n",
      "epoch:9 step:7071 [D loss: 0.119174, acc.: 98.44%] [G loss: 1.964528]\n",
      "epoch:9 step:7072 [D loss: 0.543531, acc.: 77.34%] [G loss: 3.433812]\n",
      "epoch:9 step:7073 [D loss: 0.723964, acc.: 64.84%] [G loss: 3.105237]\n",
      "epoch:9 step:7074 [D loss: 0.055223, acc.: 100.00%] [G loss: 3.186336]\n",
      "epoch:9 step:7075 [D loss: 0.089972, acc.: 99.22%] [G loss: 2.260919]\n",
      "epoch:9 step:7076 [D loss: 0.239384, acc.: 91.41%] [G loss: 2.901660]\n",
      "epoch:9 step:7077 [D loss: 0.064278, acc.: 100.00%] [G loss: 3.722794]\n",
      "epoch:9 step:7078 [D loss: 0.103729, acc.: 98.44%] [G loss: 2.762740]\n",
      "epoch:9 step:7079 [D loss: 0.294827, acc.: 84.38%] [G loss: 4.801158]\n",
      "epoch:9 step:7080 [D loss: 0.051048, acc.: 100.00%] [G loss: 5.584105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7081 [D loss: 0.457868, acc.: 78.91%] [G loss: 3.176807]\n",
      "epoch:9 step:7082 [D loss: 0.211992, acc.: 92.19%] [G loss: 4.478477]\n",
      "epoch:9 step:7083 [D loss: 0.055224, acc.: 99.22%] [G loss: 4.481967]\n",
      "epoch:9 step:7084 [D loss: 0.033398, acc.: 100.00%] [G loss: 4.139174]\n",
      "epoch:9 step:7085 [D loss: 0.239847, acc.: 90.62%] [G loss: 1.364264]\n",
      "epoch:9 step:7086 [D loss: 0.123839, acc.: 95.31%] [G loss: 2.773587]\n",
      "epoch:9 step:7087 [D loss: 0.034907, acc.: 100.00%] [G loss: 3.331398]\n",
      "epoch:9 step:7088 [D loss: 0.138635, acc.: 96.88%] [G loss: 4.594884]\n",
      "epoch:9 step:7089 [D loss: 0.087458, acc.: 97.66%] [G loss: 3.731629]\n",
      "epoch:9 step:7090 [D loss: 0.870519, acc.: 53.12%] [G loss: 5.307398]\n",
      "epoch:9 step:7091 [D loss: 0.656583, acc.: 68.75%] [G loss: 3.452668]\n",
      "epoch:9 step:7092 [D loss: 0.143204, acc.: 94.53%] [G loss: 2.161269]\n",
      "epoch:9 step:7093 [D loss: 0.078597, acc.: 99.22%] [G loss: 3.788519]\n",
      "epoch:9 step:7094 [D loss: 0.163861, acc.: 95.31%] [G loss: 3.326908]\n",
      "epoch:9 step:7095 [D loss: 0.060579, acc.: 99.22%] [G loss: 3.052232]\n",
      "epoch:9 step:7096 [D loss: 0.119498, acc.: 98.44%] [G loss: 2.013917]\n",
      "epoch:9 step:7097 [D loss: 0.183929, acc.: 96.09%] [G loss: 2.957794]\n",
      "epoch:9 step:7098 [D loss: 0.060475, acc.: 98.44%] [G loss: 3.732652]\n",
      "epoch:9 step:7099 [D loss: 0.182618, acc.: 92.97%] [G loss: 3.441271]\n",
      "epoch:9 step:7100 [D loss: 0.080794, acc.: 99.22%] [G loss: 2.698109]\n",
      "epoch:9 step:7101 [D loss: 0.169329, acc.: 96.88%] [G loss: 2.907720]\n",
      "epoch:9 step:7102 [D loss: 0.086946, acc.: 97.66%] [G loss: 2.281992]\n",
      "epoch:9 step:7103 [D loss: 0.025394, acc.: 100.00%] [G loss: 2.500354]\n",
      "epoch:9 step:7104 [D loss: 0.139054, acc.: 96.09%] [G loss: 2.881751]\n",
      "epoch:9 step:7105 [D loss: 0.638730, acc.: 67.97%] [G loss: 0.977058]\n",
      "epoch:9 step:7106 [D loss: 0.555404, acc.: 75.00%] [G loss: 7.305986]\n",
      "epoch:9 step:7107 [D loss: 1.406167, acc.: 51.56%] [G loss: 1.977258]\n",
      "epoch:9 step:7108 [D loss: 0.409891, acc.: 79.69%] [G loss: 2.989521]\n",
      "epoch:9 step:7109 [D loss: 0.143527, acc.: 98.44%] [G loss: 4.327534]\n",
      "epoch:9 step:7110 [D loss: 0.326360, acc.: 86.72%] [G loss: 2.512604]\n",
      "epoch:9 step:7111 [D loss: 0.160842, acc.: 97.66%] [G loss: 2.332285]\n",
      "epoch:9 step:7112 [D loss: 0.044186, acc.: 100.00%] [G loss: 2.747235]\n",
      "epoch:9 step:7113 [D loss: 0.272157, acc.: 91.41%] [G loss: 3.498921]\n",
      "epoch:9 step:7114 [D loss: 0.302864, acc.: 85.16%] [G loss: 4.116136]\n",
      "epoch:9 step:7115 [D loss: 0.068180, acc.: 99.22%] [G loss: 4.444293]\n",
      "epoch:9 step:7116 [D loss: 0.098014, acc.: 96.88%] [G loss: 3.274101]\n",
      "epoch:9 step:7117 [D loss: 0.193610, acc.: 93.75%] [G loss: 3.839329]\n",
      "epoch:9 step:7118 [D loss: 0.104317, acc.: 97.66%] [G loss: 4.136389]\n",
      "epoch:9 step:7119 [D loss: 0.387387, acc.: 82.03%] [G loss: 0.862787]\n",
      "epoch:9 step:7120 [D loss: 0.974909, acc.: 57.03%] [G loss: 6.405409]\n",
      "epoch:9 step:7121 [D loss: 0.568920, acc.: 65.62%] [G loss: 4.947218]\n",
      "epoch:9 step:7122 [D loss: 0.063179, acc.: 99.22%] [G loss: 3.545646]\n",
      "epoch:9 step:7123 [D loss: 0.109528, acc.: 97.66%] [G loss: 3.167500]\n",
      "epoch:9 step:7124 [D loss: 0.052627, acc.: 100.00%] [G loss: 3.149453]\n",
      "epoch:9 step:7125 [D loss: 0.079277, acc.: 97.66%] [G loss: 3.052823]\n",
      "epoch:9 step:7126 [D loss: 0.092226, acc.: 99.22%] [G loss: 3.157679]\n",
      "epoch:9 step:7127 [D loss: 0.077826, acc.: 99.22%] [G loss: 3.546422]\n",
      "epoch:9 step:7128 [D loss: 0.463650, acc.: 76.56%] [G loss: 4.590218]\n",
      "epoch:9 step:7129 [D loss: 0.151333, acc.: 95.31%] [G loss: 3.518738]\n",
      "epoch:9 step:7130 [D loss: 0.048385, acc.: 99.22%] [G loss: 2.437980]\n",
      "epoch:9 step:7131 [D loss: 0.069524, acc.: 99.22%] [G loss: 1.807407]\n",
      "epoch:9 step:7132 [D loss: 0.069120, acc.: 97.66%] [G loss: 1.681828]\n",
      "epoch:9 step:7133 [D loss: 0.074368, acc.: 97.66%] [G loss: 1.870072]\n",
      "epoch:9 step:7134 [D loss: 0.092301, acc.: 98.44%] [G loss: 2.843536]\n",
      "epoch:9 step:7135 [D loss: 0.046606, acc.: 99.22%] [G loss: 2.110724]\n",
      "epoch:9 step:7136 [D loss: 0.142731, acc.: 94.53%] [G loss: 0.394718]\n",
      "epoch:9 step:7137 [D loss: 0.702497, acc.: 65.62%] [G loss: 5.245210]\n",
      "epoch:9 step:7138 [D loss: 0.906736, acc.: 59.38%] [G loss: 2.850354]\n",
      "epoch:9 step:7139 [D loss: 0.045082, acc.: 100.00%] [G loss: 1.287317]\n",
      "epoch:9 step:7140 [D loss: 0.397172, acc.: 82.81%] [G loss: 3.991727]\n",
      "epoch:9 step:7141 [D loss: 0.158729, acc.: 94.53%] [G loss: 3.463873]\n",
      "epoch:9 step:7142 [D loss: 0.098241, acc.: 96.88%] [G loss: 2.280477]\n",
      "epoch:9 step:7143 [D loss: 0.173120, acc.: 95.31%] [G loss: 2.400419]\n",
      "epoch:9 step:7144 [D loss: 0.124584, acc.: 94.53%] [G loss: 1.808258]\n",
      "epoch:9 step:7145 [D loss: 0.292778, acc.: 85.94%] [G loss: 2.336983]\n",
      "epoch:9 step:7146 [D loss: 0.085762, acc.: 98.44%] [G loss: 3.518534]\n",
      "epoch:9 step:7147 [D loss: 0.096828, acc.: 97.66%] [G loss: 2.425146]\n",
      "epoch:9 step:7148 [D loss: 0.085268, acc.: 98.44%] [G loss: 1.981618]\n",
      "epoch:9 step:7149 [D loss: 0.393764, acc.: 82.03%] [G loss: 4.866808]\n",
      "epoch:9 step:7150 [D loss: 0.459645, acc.: 75.00%] [G loss: 3.755763]\n",
      "epoch:9 step:7151 [D loss: 0.096076, acc.: 96.88%] [G loss: 3.279685]\n",
      "epoch:9 step:7152 [D loss: 0.153053, acc.: 96.88%] [G loss: 3.345309]\n",
      "epoch:9 step:7153 [D loss: 0.070727, acc.: 98.44%] [G loss: 4.398672]\n",
      "epoch:9 step:7154 [D loss: 0.080725, acc.: 98.44%] [G loss: 3.170039]\n",
      "epoch:9 step:7155 [D loss: 0.051392, acc.: 100.00%] [G loss: 3.157661]\n",
      "epoch:9 step:7156 [D loss: 0.079509, acc.: 97.66%] [G loss: 4.376981]\n",
      "epoch:9 step:7157 [D loss: 0.092885, acc.: 98.44%] [G loss: 2.776229]\n",
      "epoch:9 step:7158 [D loss: 0.349853, acc.: 85.94%] [G loss: 5.837966]\n",
      "epoch:9 step:7159 [D loss: 0.072454, acc.: 98.44%] [G loss: 3.958642]\n",
      "epoch:9 step:7160 [D loss: 0.353475, acc.: 85.16%] [G loss: 2.369798]\n",
      "epoch:9 step:7161 [D loss: 0.259463, acc.: 90.62%] [G loss: 3.855016]\n",
      "epoch:9 step:7162 [D loss: 0.062257, acc.: 99.22%] [G loss: 4.381222]\n",
      "epoch:9 step:7163 [D loss: 0.075499, acc.: 99.22%] [G loss: 5.205083]\n",
      "epoch:9 step:7164 [D loss: 0.222204, acc.: 93.75%] [G loss: 4.662451]\n",
      "epoch:9 step:7165 [D loss: 0.250123, acc.: 90.62%] [G loss: 3.196923]\n",
      "epoch:9 step:7166 [D loss: 0.060086, acc.: 100.00%] [G loss: 2.993736]\n",
      "epoch:9 step:7167 [D loss: 0.053881, acc.: 99.22%] [G loss: 3.999104]\n",
      "epoch:9 step:7168 [D loss: 0.093244, acc.: 98.44%] [G loss: 3.872568]\n",
      "epoch:9 step:7169 [D loss: 0.366458, acc.: 79.69%] [G loss: 4.329438]\n",
      "epoch:9 step:7170 [D loss: 0.310058, acc.: 86.72%] [G loss: 3.564909]\n",
      "epoch:9 step:7171 [D loss: 0.052133, acc.: 99.22%] [G loss: 3.306889]\n",
      "epoch:9 step:7172 [D loss: 0.031897, acc.: 100.00%] [G loss: 3.323421]\n",
      "epoch:9 step:7173 [D loss: 0.094295, acc.: 97.66%] [G loss: 5.194308]\n",
      "epoch:9 step:7174 [D loss: 0.042022, acc.: 100.00%] [G loss: 3.485709]\n",
      "epoch:9 step:7175 [D loss: 0.139747, acc.: 97.66%] [G loss: 2.432380]\n",
      "epoch:9 step:7176 [D loss: 0.126858, acc.: 96.09%] [G loss: 1.814466]\n",
      "epoch:9 step:7177 [D loss: 0.059623, acc.: 100.00%] [G loss: 3.079087]\n",
      "epoch:9 step:7178 [D loss: 0.486592, acc.: 72.66%] [G loss: 6.003830]\n",
      "epoch:9 step:7179 [D loss: 0.902578, acc.: 58.59%] [G loss: 2.676301]\n",
      "epoch:9 step:7180 [D loss: 0.355663, acc.: 82.03%] [G loss: 4.446958]\n",
      "epoch:9 step:7181 [D loss: 0.037433, acc.: 99.22%] [G loss: 6.123981]\n",
      "epoch:9 step:7182 [D loss: 0.627275, acc.: 71.09%] [G loss: 2.251337]\n",
      "epoch:9 step:7183 [D loss: 0.480843, acc.: 78.12%] [G loss: 4.856721]\n",
      "epoch:9 step:7184 [D loss: 0.101175, acc.: 97.66%] [G loss: 6.442248]\n",
      "epoch:9 step:7185 [D loss: 0.191789, acc.: 92.19%] [G loss: 2.738334]\n",
      "epoch:9 step:7186 [D loss: 0.276182, acc.: 86.72%] [G loss: 4.079681]\n",
      "epoch:9 step:7187 [D loss: 0.094373, acc.: 97.66%] [G loss: 4.133399]\n",
      "epoch:9 step:7188 [D loss: 0.059227, acc.: 99.22%] [G loss: 3.698819]\n",
      "epoch:9 step:7189 [D loss: 0.143417, acc.: 94.53%] [G loss: 3.092493]\n",
      "epoch:9 step:7190 [D loss: 0.153127, acc.: 96.09%] [G loss: 3.594554]\n",
      "epoch:9 step:7191 [D loss: 0.027745, acc.: 99.22%] [G loss: 3.894416]\n",
      "epoch:9 step:7192 [D loss: 0.256109, acc.: 92.19%] [G loss: 2.376348]\n",
      "epoch:9 step:7193 [D loss: 0.372635, acc.: 80.47%] [G loss: 4.829484]\n",
      "epoch:9 step:7194 [D loss: 0.118810, acc.: 96.88%] [G loss: 3.622418]\n",
      "epoch:9 step:7195 [D loss: 0.131998, acc.: 96.88%] [G loss: 3.526766]\n",
      "epoch:9 step:7196 [D loss: 0.092513, acc.: 96.88%] [G loss: 3.298554]\n",
      "epoch:9 step:7197 [D loss: 0.059010, acc.: 100.00%] [G loss: 3.970827]\n",
      "epoch:9 step:7198 [D loss: 0.142629, acc.: 93.75%] [G loss: 2.989010]\n",
      "epoch:9 step:7199 [D loss: 0.246977, acc.: 90.62%] [G loss: 5.310764]\n",
      "epoch:9 step:7200 [D loss: 0.217362, acc.: 92.19%] [G loss: 3.663959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7201 [D loss: 0.087460, acc.: 98.44%] [G loss: 2.471237]\n",
      "epoch:9 step:7202 [D loss: 0.396474, acc.: 84.38%] [G loss: 4.214338]\n",
      "epoch:9 step:7203 [D loss: 0.122127, acc.: 96.09%] [G loss: 3.967829]\n",
      "epoch:9 step:7204 [D loss: 0.095163, acc.: 97.66%] [G loss: 2.153007]\n",
      "epoch:9 step:7205 [D loss: 0.515698, acc.: 76.56%] [G loss: 5.909519]\n",
      "epoch:9 step:7206 [D loss: 0.860852, acc.: 60.94%] [G loss: 3.428806]\n",
      "epoch:9 step:7207 [D loss: 0.074864, acc.: 100.00%] [G loss: 3.913172]\n",
      "epoch:9 step:7208 [D loss: 0.086706, acc.: 97.66%] [G loss: 3.949503]\n",
      "epoch:9 step:7209 [D loss: 0.061795, acc.: 98.44%] [G loss: 3.425339]\n",
      "epoch:9 step:7210 [D loss: 0.163346, acc.: 96.09%] [G loss: 5.101126]\n",
      "epoch:9 step:7211 [D loss: 0.252925, acc.: 89.06%] [G loss: 2.904623]\n",
      "epoch:9 step:7212 [D loss: 0.217137, acc.: 92.97%] [G loss: 4.848330]\n",
      "epoch:9 step:7213 [D loss: 0.224861, acc.: 86.72%] [G loss: 2.565406]\n",
      "epoch:9 step:7214 [D loss: 0.171360, acc.: 95.31%] [G loss: 3.622014]\n",
      "epoch:9 step:7215 [D loss: 0.049821, acc.: 99.22%] [G loss: 3.164544]\n",
      "epoch:9 step:7216 [D loss: 0.290829, acc.: 88.28%] [G loss: 5.038202]\n",
      "epoch:9 step:7217 [D loss: 0.147218, acc.: 94.53%] [G loss: 5.175576]\n",
      "epoch:9 step:7218 [D loss: 0.052019, acc.: 100.00%] [G loss: 2.871782]\n",
      "epoch:9 step:7219 [D loss: 0.193559, acc.: 92.97%] [G loss: 2.501932]\n",
      "epoch:9 step:7220 [D loss: 0.089235, acc.: 97.66%] [G loss: 3.461580]\n",
      "epoch:9 step:7221 [D loss: 0.014762, acc.: 100.00%] [G loss: 3.176774]\n",
      "epoch:9 step:7222 [D loss: 0.729473, acc.: 61.72%] [G loss: 3.607887]\n",
      "epoch:9 step:7223 [D loss: 0.081395, acc.: 96.88%] [G loss: 3.152120]\n",
      "epoch:9 step:7224 [D loss: 0.205077, acc.: 93.75%] [G loss: 1.644992]\n",
      "epoch:9 step:7225 [D loss: 0.221594, acc.: 91.41%] [G loss: 3.362585]\n",
      "epoch:9 step:7226 [D loss: 0.443351, acc.: 77.34%] [G loss: 0.830920]\n",
      "epoch:9 step:7227 [D loss: 0.280940, acc.: 86.72%] [G loss: 4.732045]\n",
      "epoch:9 step:7228 [D loss: 0.096027, acc.: 96.09%] [G loss: 4.052841]\n",
      "epoch:9 step:7229 [D loss: 0.036148, acc.: 99.22%] [G loss: 3.751402]\n",
      "epoch:9 step:7230 [D loss: 0.295180, acc.: 89.84%] [G loss: 5.094698]\n",
      "epoch:9 step:7231 [D loss: 0.009171, acc.: 100.00%] [G loss: 5.886889]\n",
      "epoch:9 step:7232 [D loss: 0.800262, acc.: 64.06%] [G loss: 0.126408]\n",
      "epoch:9 step:7233 [D loss: 1.632189, acc.: 53.91%] [G loss: 4.358828]\n",
      "epoch:9 step:7234 [D loss: 0.149469, acc.: 93.75%] [G loss: 5.738608]\n",
      "epoch:9 step:7235 [D loss: 0.969629, acc.: 60.16%] [G loss: 0.612650]\n",
      "epoch:9 step:7236 [D loss: 0.231941, acc.: 90.62%] [G loss: 0.780304]\n",
      "epoch:9 step:7237 [D loss: 0.177213, acc.: 89.84%] [G loss: 2.787133]\n",
      "epoch:9 step:7238 [D loss: 0.064913, acc.: 99.22%] [G loss: 2.790136]\n",
      "epoch:9 step:7239 [D loss: 0.091143, acc.: 96.88%] [G loss: 1.683985]\n",
      "epoch:9 step:7240 [D loss: 0.157818, acc.: 94.53%] [G loss: 2.549013]\n",
      "epoch:9 step:7241 [D loss: 0.053702, acc.: 99.22%] [G loss: 3.062629]\n",
      "epoch:9 step:7242 [D loss: 0.026903, acc.: 100.00%] [G loss: 2.165279]\n",
      "epoch:9 step:7243 [D loss: 2.739426, acc.: 10.94%] [G loss: 2.713666]\n",
      "epoch:9 step:7244 [D loss: 0.113225, acc.: 98.44%] [G loss: 5.378872]\n",
      "epoch:9 step:7245 [D loss: 0.541677, acc.: 71.09%] [G loss: 2.816091]\n",
      "epoch:9 step:7246 [D loss: 0.247267, acc.: 87.50%] [G loss: 4.406960]\n",
      "epoch:9 step:7247 [D loss: 0.141985, acc.: 95.31%] [G loss: 3.194784]\n",
      "epoch:9 step:7248 [D loss: 0.083616, acc.: 100.00%] [G loss: 3.562309]\n",
      "epoch:9 step:7249 [D loss: 0.100499, acc.: 99.22%] [G loss: 1.924469]\n",
      "epoch:9 step:7250 [D loss: 0.119853, acc.: 97.66%] [G loss: 2.812559]\n",
      "epoch:9 step:7251 [D loss: 0.087078, acc.: 98.44%] [G loss: 2.842282]\n",
      "epoch:9 step:7252 [D loss: 0.088108, acc.: 99.22%] [G loss: 2.952865]\n",
      "epoch:9 step:7253 [D loss: 0.079900, acc.: 99.22%] [G loss: 1.719081]\n",
      "epoch:9 step:7254 [D loss: 0.065781, acc.: 100.00%] [G loss: 2.632924]\n",
      "epoch:9 step:7255 [D loss: 0.041252, acc.: 100.00%] [G loss: 1.528266]\n",
      "epoch:9 step:7256 [D loss: 0.044176, acc.: 100.00%] [G loss: 2.370243]\n",
      "epoch:9 step:7257 [D loss: 0.041681, acc.: 100.00%] [G loss: 0.586282]\n",
      "epoch:9 step:7258 [D loss: 0.211801, acc.: 92.97%] [G loss: 1.492080]\n",
      "epoch:9 step:7259 [D loss: 0.065539, acc.: 99.22%] [G loss: 3.428645]\n",
      "epoch:9 step:7260 [D loss: 0.120079, acc.: 95.31%] [G loss: 0.197739]\n",
      "epoch:9 step:7261 [D loss: 0.458373, acc.: 76.56%] [G loss: 4.807129]\n",
      "epoch:9 step:7262 [D loss: 0.349804, acc.: 78.91%] [G loss: 3.016221]\n",
      "epoch:9 step:7263 [D loss: 0.300822, acc.: 88.28%] [G loss: 0.723745]\n",
      "epoch:9 step:7264 [D loss: 0.357365, acc.: 78.12%] [G loss: 5.616593]\n",
      "epoch:9 step:7265 [D loss: 0.280179, acc.: 83.59%] [G loss: 3.317698]\n",
      "epoch:9 step:7266 [D loss: 0.194649, acc.: 91.41%] [G loss: 2.029261]\n",
      "epoch:9 step:7267 [D loss: 0.205535, acc.: 90.62%] [G loss: 3.651402]\n",
      "epoch:9 step:7268 [D loss: 0.055655, acc.: 98.44%] [G loss: 4.474410]\n",
      "epoch:9 step:7269 [D loss: 0.220527, acc.: 91.41%] [G loss: 1.233065]\n",
      "epoch:9 step:7270 [D loss: 0.673075, acc.: 64.06%] [G loss: 5.877534]\n",
      "epoch:9 step:7271 [D loss: 0.539290, acc.: 68.75%] [G loss: 3.477174]\n",
      "epoch:9 step:7272 [D loss: 0.219653, acc.: 90.62%] [G loss: 3.617988]\n",
      "epoch:9 step:7273 [D loss: 0.142008, acc.: 95.31%] [G loss: 3.257553]\n",
      "epoch:9 step:7274 [D loss: 0.115977, acc.: 96.09%] [G loss: 2.894095]\n",
      "epoch:9 step:7275 [D loss: 0.128578, acc.: 95.31%] [G loss: 1.797387]\n",
      "epoch:9 step:7276 [D loss: 0.173151, acc.: 97.66%] [G loss: 2.793167]\n",
      "epoch:9 step:7277 [D loss: 0.044706, acc.: 100.00%] [G loss: 2.678820]\n",
      "epoch:9 step:7278 [D loss: 0.219180, acc.: 94.53%] [G loss: 1.357421]\n",
      "epoch:9 step:7279 [D loss: 0.351449, acc.: 84.38%] [G loss: 4.122776]\n",
      "epoch:9 step:7280 [D loss: 0.179839, acc.: 93.75%] [G loss: 4.153116]\n",
      "epoch:9 step:7281 [D loss: 0.235476, acc.: 92.97%] [G loss: 3.204134]\n",
      "epoch:9 step:7282 [D loss: 0.125869, acc.: 96.88%] [G loss: 2.350406]\n",
      "epoch:9 step:7283 [D loss: 0.139514, acc.: 96.88%] [G loss: 2.981022]\n",
      "epoch:9 step:7284 [D loss: 0.079661, acc.: 99.22%] [G loss: 3.346044]\n",
      "epoch:9 step:7285 [D loss: 0.130716, acc.: 98.44%] [G loss: 2.294356]\n",
      "epoch:9 step:7286 [D loss: 0.282612, acc.: 89.06%] [G loss: 2.237499]\n",
      "epoch:9 step:7287 [D loss: 0.215697, acc.: 92.19%] [G loss: 3.004399]\n",
      "epoch:9 step:7288 [D loss: 0.222335, acc.: 92.19%] [G loss: 1.549931]\n",
      "epoch:9 step:7289 [D loss: 0.378863, acc.: 81.25%] [G loss: 4.898549]\n",
      "epoch:9 step:7290 [D loss: 0.514838, acc.: 74.22%] [G loss: 3.067644]\n",
      "epoch:9 step:7291 [D loss: 0.114459, acc.: 97.66%] [G loss: 4.609166]\n",
      "epoch:9 step:7292 [D loss: 0.025493, acc.: 100.00%] [G loss: 4.729914]\n",
      "epoch:9 step:7293 [D loss: 0.169957, acc.: 94.53%] [G loss: 3.244659]\n",
      "epoch:9 step:7294 [D loss: 0.244886, acc.: 93.75%] [G loss: 2.343976]\n",
      "epoch:9 step:7295 [D loss: 0.091924, acc.: 97.66%] [G loss: 4.551877]\n",
      "epoch:9 step:7296 [D loss: 0.186126, acc.: 93.75%] [G loss: 2.909613]\n",
      "epoch:9 step:7297 [D loss: 0.076358, acc.: 99.22%] [G loss: 4.451118]\n",
      "epoch:9 step:7298 [D loss: 0.072036, acc.: 98.44%] [G loss: 3.219762]\n",
      "epoch:9 step:7299 [D loss: 0.173465, acc.: 92.97%] [G loss: 3.762627]\n",
      "epoch:9 step:7300 [D loss: 0.148287, acc.: 95.31%] [G loss: 4.619448]\n",
      "epoch:9 step:7301 [D loss: 0.069101, acc.: 96.88%] [G loss: 4.234875]\n",
      "epoch:9 step:7302 [D loss: 0.119271, acc.: 96.09%] [G loss: 2.669303]\n",
      "epoch:9 step:7303 [D loss: 0.102844, acc.: 97.66%] [G loss: 3.960601]\n",
      "epoch:9 step:7304 [D loss: 0.126798, acc.: 96.88%] [G loss: 1.506041]\n",
      "epoch:9 step:7305 [D loss: 0.070016, acc.: 98.44%] [G loss: 0.518450]\n",
      "epoch:9 step:7306 [D loss: 0.540342, acc.: 75.78%] [G loss: 8.100228]\n",
      "epoch:9 step:7307 [D loss: 1.332673, acc.: 53.91%] [G loss: 3.858867]\n",
      "epoch:9 step:7308 [D loss: 0.060408, acc.: 100.00%] [G loss: 2.112832]\n",
      "epoch:9 step:7309 [D loss: 0.092314, acc.: 98.44%] [G loss: 2.682986]\n",
      "epoch:9 step:7310 [D loss: 0.068717, acc.: 99.22%] [G loss: 2.778982]\n",
      "epoch:9 step:7311 [D loss: 0.066476, acc.: 99.22%] [G loss: 2.670569]\n",
      "epoch:9 step:7312 [D loss: 0.129009, acc.: 96.88%] [G loss: 2.999804]\n",
      "epoch:9 step:7313 [D loss: 1.013489, acc.: 55.47%] [G loss: 6.931327]\n",
      "epoch:9 step:7314 [D loss: 1.563768, acc.: 51.56%] [G loss: 4.226426]\n",
      "epoch:9 step:7315 [D loss: 0.346806, acc.: 84.38%] [G loss: 3.242905]\n",
      "epoch:9 step:7316 [D loss: 0.108237, acc.: 99.22%] [G loss: 2.485045]\n",
      "epoch:9 step:7317 [D loss: 0.050310, acc.: 98.44%] [G loss: 3.495046]\n",
      "epoch:9 step:7318 [D loss: 0.165916, acc.: 93.75%] [G loss: 3.081876]\n",
      "epoch:9 step:7319 [D loss: 0.036716, acc.: 100.00%] [G loss: 4.094460]\n",
      "epoch:9 step:7320 [D loss: 0.106921, acc.: 96.88%] [G loss: 2.779477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7321 [D loss: 0.198181, acc.: 95.31%] [G loss: 2.969651]\n",
      "epoch:9 step:7322 [D loss: 0.118737, acc.: 96.88%] [G loss: 3.164192]\n",
      "epoch:9 step:7323 [D loss: 0.048316, acc.: 100.00%] [G loss: 3.575195]\n",
      "epoch:9 step:7324 [D loss: 0.476106, acc.: 75.00%] [G loss: 4.705082]\n",
      "epoch:9 step:7325 [D loss: 0.070625, acc.: 98.44%] [G loss: 4.484269]\n",
      "epoch:9 step:7326 [D loss: 0.325928, acc.: 85.16%] [G loss: 1.572226]\n",
      "epoch:9 step:7327 [D loss: 0.181957, acc.: 94.53%] [G loss: 2.167764]\n",
      "epoch:9 step:7328 [D loss: 0.059395, acc.: 99.22%] [G loss: 4.025907]\n",
      "epoch:9 step:7329 [D loss: 0.796238, acc.: 58.59%] [G loss: 3.676549]\n",
      "epoch:9 step:7330 [D loss: 0.050073, acc.: 98.44%] [G loss: 4.611391]\n",
      "epoch:9 step:7331 [D loss: 0.090398, acc.: 97.66%] [G loss: 4.372060]\n",
      "epoch:9 step:7332 [D loss: 0.162716, acc.: 95.31%] [G loss: 3.049568]\n",
      "epoch:9 step:7333 [D loss: 0.035248, acc.: 100.00%] [G loss: 3.100912]\n",
      "epoch:9 step:7334 [D loss: 0.045677, acc.: 100.00%] [G loss: 2.899627]\n",
      "epoch:9 step:7335 [D loss: 0.103651, acc.: 97.66%] [G loss: 2.316311]\n",
      "epoch:9 step:7336 [D loss: 0.143096, acc.: 95.31%] [G loss: 2.412017]\n",
      "epoch:9 step:7337 [D loss: 0.072556, acc.: 99.22%] [G loss: 2.258173]\n",
      "epoch:9 step:7338 [D loss: 0.072437, acc.: 99.22%] [G loss: 1.174560]\n",
      "epoch:9 step:7339 [D loss: 0.044485, acc.: 100.00%] [G loss: 1.444123]\n",
      "epoch:9 step:7340 [D loss: 0.112473, acc.: 97.66%] [G loss: 1.357706]\n",
      "epoch:9 step:7341 [D loss: 0.066916, acc.: 99.22%] [G loss: 1.328992]\n",
      "epoch:9 step:7342 [D loss: 0.375080, acc.: 82.81%] [G loss: 3.450124]\n",
      "epoch:9 step:7343 [D loss: 0.062579, acc.: 98.44%] [G loss: 3.633615]\n",
      "epoch:9 step:7344 [D loss: 1.756738, acc.: 39.06%] [G loss: 1.076779]\n",
      "epoch:9 step:7345 [D loss: 0.260931, acc.: 86.72%] [G loss: 3.742154]\n",
      "epoch:9 step:7346 [D loss: 0.036383, acc.: 100.00%] [G loss: 4.218246]\n",
      "epoch:9 step:7347 [D loss: 0.082225, acc.: 98.44%] [G loss: 3.747802]\n",
      "epoch:9 step:7348 [D loss: 0.031468, acc.: 100.00%] [G loss: 2.639838]\n",
      "epoch:9 step:7349 [D loss: 0.487419, acc.: 78.12%] [G loss: 4.557799]\n",
      "epoch:9 step:7350 [D loss: 0.148237, acc.: 96.09%] [G loss: 4.103627]\n",
      "epoch:9 step:7351 [D loss: 0.085348, acc.: 97.66%] [G loss: 4.199060]\n",
      "epoch:9 step:7352 [D loss: 0.036410, acc.: 100.00%] [G loss: 3.471665]\n",
      "epoch:9 step:7353 [D loss: 0.280334, acc.: 90.62%] [G loss: 4.301640]\n",
      "epoch:9 step:7354 [D loss: 0.062072, acc.: 100.00%] [G loss: 2.049119]\n",
      "epoch:9 step:7355 [D loss: 0.064456, acc.: 99.22%] [G loss: 3.794073]\n",
      "epoch:9 step:7356 [D loss: 0.187957, acc.: 96.88%] [G loss: 2.391852]\n",
      "epoch:9 step:7357 [D loss: 0.051623, acc.: 100.00%] [G loss: 3.766992]\n",
      "epoch:9 step:7358 [D loss: 0.071689, acc.: 99.22%] [G loss: 2.652589]\n",
      "epoch:9 step:7359 [D loss: 0.036709, acc.: 100.00%] [G loss: 3.493714]\n",
      "epoch:9 step:7360 [D loss: 0.199070, acc.: 92.19%] [G loss: 3.903031]\n",
      "epoch:9 step:7361 [D loss: 0.102388, acc.: 96.88%] [G loss: 3.711710]\n",
      "epoch:9 step:7362 [D loss: 0.102923, acc.: 97.66%] [G loss: 2.355243]\n",
      "epoch:9 step:7363 [D loss: 0.077740, acc.: 100.00%] [G loss: 3.293502]\n",
      "epoch:9 step:7364 [D loss: 0.297104, acc.: 87.50%] [G loss: 5.347096]\n",
      "epoch:9 step:7365 [D loss: 1.295186, acc.: 40.62%] [G loss: 2.594843]\n",
      "epoch:9 step:7366 [D loss: 0.038905, acc.: 99.22%] [G loss: 4.865469]\n",
      "epoch:9 step:7367 [D loss: 0.325343, acc.: 83.59%] [G loss: 2.084625]\n",
      "epoch:9 step:7368 [D loss: 0.244274, acc.: 91.41%] [G loss: 4.694356]\n",
      "epoch:9 step:7369 [D loss: 0.166303, acc.: 92.97%] [G loss: 3.582813]\n",
      "epoch:9 step:7370 [D loss: 0.055083, acc.: 100.00%] [G loss: 2.751679]\n",
      "epoch:9 step:7371 [D loss: 0.136779, acc.: 93.75%] [G loss: 3.401581]\n",
      "epoch:9 step:7372 [D loss: 0.202044, acc.: 95.31%] [G loss: 3.091647]\n",
      "epoch:9 step:7373 [D loss: 0.137812, acc.: 95.31%] [G loss: 3.536060]\n",
      "epoch:9 step:7374 [D loss: 0.240442, acc.: 90.62%] [G loss: 4.441657]\n",
      "epoch:9 step:7375 [D loss: 0.244986, acc.: 92.19%] [G loss: 1.534996]\n",
      "epoch:9 step:7376 [D loss: 0.048226, acc.: 100.00%] [G loss: 1.102846]\n",
      "epoch:9 step:7377 [D loss: 0.156690, acc.: 96.88%] [G loss: 3.424927]\n",
      "epoch:9 step:7378 [D loss: 0.036917, acc.: 100.00%] [G loss: 3.357727]\n",
      "epoch:9 step:7379 [D loss: 0.248239, acc.: 90.62%] [G loss: 1.976058]\n",
      "epoch:9 step:7380 [D loss: 0.094117, acc.: 98.44%] [G loss: 3.542701]\n",
      "epoch:9 step:7381 [D loss: 0.062985, acc.: 99.22%] [G loss: 3.712631]\n",
      "epoch:9 step:7382 [D loss: 0.182264, acc.: 92.19%] [G loss: 0.881195]\n",
      "epoch:9 step:7383 [D loss: 0.297251, acc.: 84.38%] [G loss: 4.495590]\n",
      "epoch:9 step:7384 [D loss: 0.113584, acc.: 96.09%] [G loss: 5.283426]\n",
      "epoch:9 step:7385 [D loss: 0.159666, acc.: 94.53%] [G loss: 2.581684]\n",
      "epoch:9 step:7386 [D loss: 0.111648, acc.: 97.66%] [G loss: 2.642308]\n",
      "epoch:9 step:7387 [D loss: 0.035088, acc.: 100.00%] [G loss: 2.800450]\n",
      "epoch:9 step:7388 [D loss: 0.033524, acc.: 99.22%] [G loss: 2.358667]\n",
      "epoch:9 step:7389 [D loss: 0.016254, acc.: 100.00%] [G loss: 2.135771]\n",
      "epoch:9 step:7390 [D loss: 0.716630, acc.: 60.16%] [G loss: 3.542588]\n",
      "epoch:9 step:7391 [D loss: 0.061053, acc.: 98.44%] [G loss: 5.008695]\n",
      "epoch:9 step:7392 [D loss: 0.169962, acc.: 94.53%] [G loss: 2.956739]\n",
      "epoch:9 step:7393 [D loss: 0.070985, acc.: 100.00%] [G loss: 0.976471]\n",
      "epoch:9 step:7394 [D loss: 0.405955, acc.: 82.03%] [G loss: 5.471980]\n",
      "epoch:9 step:7395 [D loss: 0.353913, acc.: 80.47%] [G loss: 3.984189]\n",
      "epoch:9 step:7396 [D loss: 0.235955, acc.: 89.06%] [G loss: 3.126034]\n",
      "epoch:9 step:7397 [D loss: 0.014433, acc.: 100.00%] [G loss: 2.162015]\n",
      "epoch:9 step:7398 [D loss: 0.068274, acc.: 97.66%] [G loss: 3.561577]\n",
      "epoch:9 step:7399 [D loss: 0.037045, acc.: 99.22%] [G loss: 2.145589]\n",
      "epoch:9 step:7400 [D loss: 0.055434, acc.: 98.44%] [G loss: 2.497897]\n",
      "epoch:9 step:7401 [D loss: 0.057720, acc.: 99.22%] [G loss: 2.728210]\n",
      "epoch:9 step:7402 [D loss: 0.312275, acc.: 83.59%] [G loss: 6.378680]\n",
      "epoch:9 step:7403 [D loss: 1.809650, acc.: 28.91%] [G loss: 4.646198]\n",
      "epoch:9 step:7404 [D loss: 0.371887, acc.: 82.81%] [G loss: 2.221768]\n",
      "epoch:9 step:7405 [D loss: 0.269244, acc.: 85.94%] [G loss: 4.226413]\n",
      "epoch:9 step:7406 [D loss: 0.031050, acc.: 100.00%] [G loss: 5.194907]\n",
      "epoch:9 step:7407 [D loss: 0.124565, acc.: 94.53%] [G loss: 2.503932]\n",
      "epoch:9 step:7408 [D loss: 0.109364, acc.: 97.66%] [G loss: 3.532014]\n",
      "epoch:9 step:7409 [D loss: 0.077589, acc.: 99.22%] [G loss: 2.938787]\n",
      "epoch:9 step:7410 [D loss: 0.023302, acc.: 100.00%] [G loss: 2.380166]\n",
      "epoch:9 step:7411 [D loss: 0.652176, acc.: 72.66%] [G loss: 6.336528]\n",
      "epoch:9 step:7412 [D loss: 0.626697, acc.: 67.97%] [G loss: 3.049178]\n",
      "epoch:9 step:7413 [D loss: 0.085533, acc.: 97.66%] [G loss: 1.213318]\n",
      "epoch:9 step:7414 [D loss: 0.069874, acc.: 98.44%] [G loss: 1.729889]\n",
      "epoch:9 step:7415 [D loss: 0.036558, acc.: 100.00%] [G loss: 2.559719]\n",
      "epoch:9 step:7416 [D loss: 0.032092, acc.: 100.00%] [G loss: 1.156412]\n",
      "epoch:9 step:7417 [D loss: 0.053268, acc.: 99.22%] [G loss: 1.717509]\n",
      "epoch:9 step:7418 [D loss: 0.069358, acc.: 100.00%] [G loss: 1.671227]\n",
      "epoch:9 step:7419 [D loss: 0.672532, acc.: 67.97%] [G loss: 5.965875]\n",
      "epoch:9 step:7420 [D loss: 1.072249, acc.: 57.03%] [G loss: 1.127185]\n",
      "epoch:9 step:7421 [D loss: 0.269852, acc.: 89.84%] [G loss: 2.047575]\n",
      "epoch:9 step:7422 [D loss: 0.202252, acc.: 92.19%] [G loss: 1.504786]\n",
      "epoch:9 step:7423 [D loss: 0.026372, acc.: 100.00%] [G loss: 0.769451]\n",
      "epoch:9 step:7424 [D loss: 0.065796, acc.: 97.66%] [G loss: 1.081151]\n",
      "epoch:9 step:7425 [D loss: 0.218795, acc.: 89.84%] [G loss: 2.670207]\n",
      "epoch:9 step:7426 [D loss: 0.243966, acc.: 93.75%] [G loss: 2.799671]\n",
      "epoch:9 step:7427 [D loss: 0.133018, acc.: 93.75%] [G loss: 1.806427]\n",
      "epoch:9 step:7428 [D loss: 0.063914, acc.: 99.22%] [G loss: 1.050301]\n",
      "epoch:9 step:7429 [D loss: 0.234631, acc.: 92.97%] [G loss: 4.589184]\n",
      "epoch:9 step:7430 [D loss: 0.627737, acc.: 70.31%] [G loss: 2.857882]\n",
      "epoch:9 step:7431 [D loss: 0.206494, acc.: 94.53%] [G loss: 3.468054]\n",
      "epoch:9 step:7432 [D loss: 0.124336, acc.: 96.88%] [G loss: 4.061986]\n",
      "epoch:9 step:7433 [D loss: 0.054718, acc.: 100.00%] [G loss: 3.187733]\n",
      "epoch:9 step:7434 [D loss: 0.225573, acc.: 92.19%] [G loss: 4.025784]\n",
      "epoch:9 step:7435 [D loss: 0.070549, acc.: 99.22%] [G loss: 3.402152]\n",
      "epoch:9 step:7436 [D loss: 0.047940, acc.: 99.22%] [G loss: 4.214864]\n",
      "epoch:9 step:7437 [D loss: 0.071927, acc.: 99.22%] [G loss: 3.982332]\n",
      "epoch:9 step:7438 [D loss: 0.175454, acc.: 96.09%] [G loss: 4.633843]\n",
      "epoch:9 step:7439 [D loss: 0.398713, acc.: 80.47%] [G loss: 2.748812]\n",
      "epoch:9 step:7440 [D loss: 0.166117, acc.: 95.31%] [G loss: 2.369966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7441 [D loss: 0.045714, acc.: 100.00%] [G loss: 2.984895]\n",
      "epoch:9 step:7442 [D loss: 0.035509, acc.: 100.00%] [G loss: 2.216952]\n",
      "epoch:9 step:7443 [D loss: 0.424070, acc.: 80.47%] [G loss: 5.306476]\n",
      "epoch:9 step:7444 [D loss: 0.181484, acc.: 90.62%] [G loss: 4.133617]\n",
      "epoch:9 step:7445 [D loss: 0.076344, acc.: 100.00%] [G loss: 3.076941]\n",
      "epoch:9 step:7446 [D loss: 0.077398, acc.: 99.22%] [G loss: 2.011383]\n",
      "epoch:9 step:7447 [D loss: 0.071207, acc.: 100.00%] [G loss: 3.435483]\n",
      "epoch:9 step:7448 [D loss: 0.122380, acc.: 96.09%] [G loss: 4.373871]\n",
      "epoch:9 step:7449 [D loss: 0.595354, acc.: 68.75%] [G loss: 4.071468]\n",
      "epoch:9 step:7450 [D loss: 0.036556, acc.: 100.00%] [G loss: 4.329425]\n",
      "epoch:9 step:7451 [D loss: 0.071158, acc.: 99.22%] [G loss: 3.403503]\n",
      "epoch:9 step:7452 [D loss: 0.072417, acc.: 99.22%] [G loss: 3.611796]\n",
      "epoch:9 step:7453 [D loss: 0.386401, acc.: 79.69%] [G loss: 5.573091]\n",
      "epoch:9 step:7454 [D loss: 0.160485, acc.: 92.97%] [G loss: 4.834145]\n",
      "epoch:9 step:7455 [D loss: 0.095453, acc.: 97.66%] [G loss: 3.184851]\n",
      "epoch:9 step:7456 [D loss: 0.095672, acc.: 95.31%] [G loss: 4.229230]\n",
      "epoch:9 step:7457 [D loss: 0.207707, acc.: 92.97%] [G loss: 3.050011]\n",
      "epoch:9 step:7458 [D loss: 0.102863, acc.: 97.66%] [G loss: 5.315228]\n",
      "epoch:9 step:7459 [D loss: 0.265599, acc.: 86.72%] [G loss: 5.196507]\n",
      "epoch:9 step:7460 [D loss: 0.115571, acc.: 96.88%] [G loss: 3.212885]\n",
      "epoch:9 step:7461 [D loss: 0.071578, acc.: 98.44%] [G loss: 4.360897]\n",
      "epoch:9 step:7462 [D loss: 0.055331, acc.: 98.44%] [G loss: 3.616709]\n",
      "epoch:9 step:7463 [D loss: 0.232001, acc.: 90.62%] [G loss: 3.927092]\n",
      "epoch:9 step:7464 [D loss: 0.030330, acc.: 100.00%] [G loss: 3.992601]\n",
      "epoch:9 step:7465 [D loss: 1.033388, acc.: 57.03%] [G loss: 8.110168]\n",
      "epoch:9 step:7466 [D loss: 2.700987, acc.: 50.00%] [G loss: 4.450528]\n",
      "epoch:9 step:7467 [D loss: 0.711684, acc.: 60.94%] [G loss: 3.479774]\n",
      "epoch:9 step:7468 [D loss: 0.078532, acc.: 99.22%] [G loss: 3.211127]\n",
      "epoch:9 step:7469 [D loss: 0.139087, acc.: 96.09%] [G loss: 2.393037]\n",
      "epoch:9 step:7470 [D loss: 0.261397, acc.: 92.19%] [G loss: 1.814325]\n",
      "epoch:9 step:7471 [D loss: 0.218740, acc.: 90.62%] [G loss: 2.688426]\n",
      "epoch:9 step:7472 [D loss: 0.095925, acc.: 99.22%] [G loss: 2.452108]\n",
      "epoch:9 step:7473 [D loss: 0.401474, acc.: 82.81%] [G loss: 2.338332]\n",
      "epoch:9 step:7474 [D loss: 0.147728, acc.: 96.88%] [G loss: 2.411219]\n",
      "epoch:9 step:7475 [D loss: 0.051860, acc.: 100.00%] [G loss: 2.063529]\n",
      "epoch:9 step:7476 [D loss: 0.092382, acc.: 99.22%] [G loss: 1.204361]\n",
      "epoch:9 step:7477 [D loss: 0.553014, acc.: 70.31%] [G loss: 4.191718]\n",
      "epoch:9 step:7478 [D loss: 0.107709, acc.: 96.88%] [G loss: 3.796372]\n",
      "epoch:9 step:7479 [D loss: 0.206462, acc.: 92.97%] [G loss: 3.461011]\n",
      "epoch:9 step:7480 [D loss: 0.082474, acc.: 98.44%] [G loss: 1.107517]\n",
      "epoch:9 step:7481 [D loss: 0.055001, acc.: 99.22%] [G loss: 1.671598]\n",
      "epoch:9 step:7482 [D loss: 0.080616, acc.: 98.44%] [G loss: 1.441520]\n",
      "epoch:9 step:7483 [D loss: 0.022096, acc.: 100.00%] [G loss: 1.071832]\n",
      "epoch:9 step:7484 [D loss: 0.044799, acc.: 100.00%] [G loss: 1.369650]\n",
      "epoch:9 step:7485 [D loss: 0.034598, acc.: 100.00%] [G loss: 0.546696]\n",
      "epoch:9 step:7486 [D loss: 0.171481, acc.: 92.97%] [G loss: 2.356370]\n",
      "epoch:9 step:7487 [D loss: 0.031220, acc.: 100.00%] [G loss: 2.384383]\n",
      "epoch:9 step:7488 [D loss: 0.762410, acc.: 55.47%] [G loss: 3.680371]\n",
      "epoch:9 step:7489 [D loss: 0.786304, acc.: 63.28%] [G loss: 1.747795]\n",
      "epoch:9 step:7490 [D loss: 0.366133, acc.: 80.47%] [G loss: 3.833125]\n",
      "epoch:9 step:7491 [D loss: 0.045775, acc.: 98.44%] [G loss: 5.227192]\n",
      "epoch:9 step:7492 [D loss: 0.164742, acc.: 93.75%] [G loss: 3.537915]\n",
      "epoch:9 step:7493 [D loss: 0.125891, acc.: 96.09%] [G loss: 3.551976]\n",
      "epoch:9 step:7494 [D loss: 0.031442, acc.: 100.00%] [G loss: 3.673807]\n",
      "epoch:9 step:7495 [D loss: 0.033197, acc.: 100.00%] [G loss: 3.880174]\n",
      "epoch:9 step:7496 [D loss: 0.062426, acc.: 97.66%] [G loss: 3.079605]\n",
      "epoch:9 step:7497 [D loss: 0.073844, acc.: 98.44%] [G loss: 4.377949]\n",
      "epoch:9 step:7498 [D loss: 0.032281, acc.: 99.22%] [G loss: 4.181411]\n",
      "epoch:9 step:7499 [D loss: 0.023643, acc.: 100.00%] [G loss: 3.391420]\n",
      "epoch:9 step:7500 [D loss: 0.042636, acc.: 100.00%] [G loss: 2.157451]\n",
      "epoch:9 step:7501 [D loss: 0.048439, acc.: 100.00%] [G loss: 2.907925]\n",
      "epoch:9 step:7502 [D loss: 0.046007, acc.: 100.00%] [G loss: 1.781942]\n",
      "epoch:9 step:7503 [D loss: 0.078244, acc.: 100.00%] [G loss: 0.775181]\n",
      "epoch:9 step:7504 [D loss: 0.065339, acc.: 99.22%] [G loss: 0.949135]\n",
      "epoch:9 step:7505 [D loss: 0.056028, acc.: 100.00%] [G loss: 2.125994]\n",
      "epoch:9 step:7506 [D loss: 0.014216, acc.: 100.00%] [G loss: 1.751565]\n",
      "epoch:9 step:7507 [D loss: 0.048208, acc.: 100.00%] [G loss: 0.559057]\n",
      "epoch:9 step:7508 [D loss: 0.042245, acc.: 99.22%] [G loss: 0.111935]\n",
      "epoch:9 step:7509 [D loss: 0.047267, acc.: 100.00%] [G loss: 0.695418]\n",
      "epoch:9 step:7510 [D loss: 0.041599, acc.: 100.00%] [G loss: 1.732628]\n",
      "epoch:9 step:7511 [D loss: 0.160363, acc.: 95.31%] [G loss: 1.552354]\n",
      "epoch:9 step:7512 [D loss: 0.022067, acc.: 100.00%] [G loss: 0.886379]\n",
      "epoch:9 step:7513 [D loss: 0.030044, acc.: 100.00%] [G loss: 1.072083]\n",
      "epoch:9 step:7514 [D loss: 0.128821, acc.: 95.31%] [G loss: 0.237767]\n",
      "epoch:9 step:7515 [D loss: 0.086920, acc.: 98.44%] [G loss: 2.905992]\n",
      "epoch:9 step:7516 [D loss: 0.115948, acc.: 97.66%] [G loss: 2.773594]\n",
      "epoch:9 step:7517 [D loss: 2.477635, acc.: 11.72%] [G loss: 6.664336]\n",
      "epoch:9 step:7518 [D loss: 1.149720, acc.: 53.91%] [G loss: 4.463611]\n",
      "epoch:9 step:7519 [D loss: 0.333326, acc.: 84.38%] [G loss: 2.485456]\n",
      "epoch:9 step:7520 [D loss: 0.246535, acc.: 90.62%] [G loss: 3.264550]\n",
      "epoch:9 step:7521 [D loss: 0.152002, acc.: 96.09%] [G loss: 3.807382]\n",
      "epoch:9 step:7522 [D loss: 0.042791, acc.: 98.44%] [G loss: 4.204808]\n",
      "epoch:9 step:7523 [D loss: 0.071360, acc.: 99.22%] [G loss: 4.010735]\n",
      "epoch:9 step:7524 [D loss: 0.091740, acc.: 99.22%] [G loss: 3.624530]\n",
      "epoch:9 step:7525 [D loss: 0.046856, acc.: 100.00%] [G loss: 2.875540]\n",
      "epoch:9 step:7526 [D loss: 0.283764, acc.: 92.19%] [G loss: 2.692127]\n",
      "epoch:9 step:7527 [D loss: 0.266654, acc.: 90.62%] [G loss: 3.907336]\n",
      "epoch:9 step:7528 [D loss: 0.482537, acc.: 79.69%] [G loss: 4.114424]\n",
      "epoch:9 step:7529 [D loss: 0.231808, acc.: 92.97%] [G loss: 3.955092]\n",
      "epoch:9 step:7530 [D loss: 0.183723, acc.: 89.84%] [G loss: 2.508663]\n",
      "epoch:9 step:7531 [D loss: 0.128634, acc.: 97.66%] [G loss: 2.669485]\n",
      "epoch:9 step:7532 [D loss: 0.082471, acc.: 98.44%] [G loss: 2.562801]\n",
      "epoch:9 step:7533 [D loss: 0.188913, acc.: 93.75%] [G loss: 3.227555]\n",
      "epoch:9 step:7534 [D loss: 0.188097, acc.: 91.41%] [G loss: 1.398052]\n",
      "epoch:9 step:7535 [D loss: 0.065168, acc.: 100.00%] [G loss: 1.835975]\n",
      "epoch:9 step:7536 [D loss: 0.111073, acc.: 96.88%] [G loss: 0.387775]\n",
      "epoch:9 step:7537 [D loss: 0.145435, acc.: 96.09%] [G loss: 0.288262]\n",
      "epoch:9 step:7538 [D loss: 0.194959, acc.: 91.41%] [G loss: 0.144059]\n",
      "epoch:9 step:7539 [D loss: 0.188747, acc.: 94.53%] [G loss: 0.813843]\n",
      "epoch:9 step:7540 [D loss: 0.074923, acc.: 98.44%] [G loss: 0.846633]\n",
      "epoch:9 step:7541 [D loss: 0.095838, acc.: 96.88%] [G loss: 1.935567]\n",
      "epoch:9 step:7542 [D loss: 0.161997, acc.: 91.41%] [G loss: 0.968749]\n",
      "epoch:9 step:7543 [D loss: 0.129346, acc.: 95.31%] [G loss: 2.121735]\n",
      "epoch:9 step:7544 [D loss: 0.028430, acc.: 100.00%] [G loss: 2.270140]\n",
      "epoch:9 step:7545 [D loss: 0.087146, acc.: 96.88%] [G loss: 1.129448]\n",
      "epoch:9 step:7546 [D loss: 0.379309, acc.: 80.47%] [G loss: 5.083657]\n",
      "epoch:9 step:7547 [D loss: 0.176852, acc.: 90.62%] [G loss: 3.846619]\n",
      "epoch:9 step:7548 [D loss: 0.098055, acc.: 96.88%] [G loss: 2.685276]\n",
      "epoch:9 step:7549 [D loss: 0.080621, acc.: 98.44%] [G loss: 0.676724]\n",
      "epoch:9 step:7550 [D loss: 0.048806, acc.: 100.00%] [G loss: 0.608317]\n",
      "epoch:9 step:7551 [D loss: 0.375385, acc.: 79.69%] [G loss: 4.754119]\n",
      "epoch:9 step:7552 [D loss: 0.361558, acc.: 82.81%] [G loss: 2.180231]\n",
      "epoch:9 step:7553 [D loss: 1.523747, acc.: 34.38%] [G loss: 5.324718]\n",
      "epoch:9 step:7554 [D loss: 0.107130, acc.: 95.31%] [G loss: 5.947216]\n",
      "epoch:9 step:7555 [D loss: 1.714963, acc.: 46.88%] [G loss: 2.369879]\n",
      "epoch:9 step:7556 [D loss: 0.171184, acc.: 93.75%] [G loss: 2.187327]\n",
      "epoch:9 step:7557 [D loss: 0.228479, acc.: 90.62%] [G loss: 3.929646]\n",
      "epoch:9 step:7558 [D loss: 0.061585, acc.: 98.44%] [G loss: 3.912564]\n",
      "epoch:9 step:7559 [D loss: 0.039740, acc.: 100.00%] [G loss: 3.714455]\n",
      "epoch:9 step:7560 [D loss: 0.312935, acc.: 86.72%] [G loss: 3.036585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7561 [D loss: 0.278379, acc.: 89.06%] [G loss: 3.431332]\n",
      "epoch:9 step:7562 [D loss: 0.146686, acc.: 94.53%] [G loss: 3.880923]\n",
      "epoch:9 step:7563 [D loss: 0.209107, acc.: 92.97%] [G loss: 2.612574]\n",
      "epoch:9 step:7564 [D loss: 0.175211, acc.: 96.09%] [G loss: 3.770088]\n",
      "epoch:9 step:7565 [D loss: 0.115406, acc.: 96.09%] [G loss: 3.303814]\n",
      "epoch:9 step:7566 [D loss: 0.222291, acc.: 92.19%] [G loss: 1.440431]\n",
      "epoch:9 step:7567 [D loss: 0.058379, acc.: 100.00%] [G loss: 0.782528]\n",
      "epoch:9 step:7568 [D loss: 0.086155, acc.: 99.22%] [G loss: 3.012490]\n",
      "epoch:9 step:7569 [D loss: 0.071067, acc.: 98.44%] [G loss: 3.120251]\n",
      "epoch:9 step:7570 [D loss: 0.083906, acc.: 99.22%] [G loss: 2.735985]\n",
      "epoch:9 step:7571 [D loss: 0.239699, acc.: 91.41%] [G loss: 0.773200]\n",
      "epoch:9 step:7572 [D loss: 0.078903, acc.: 97.66%] [G loss: 1.891663]\n",
      "epoch:9 step:7573 [D loss: 0.060654, acc.: 98.44%] [G loss: 2.862079]\n",
      "epoch:9 step:7574 [D loss: 0.029445, acc.: 98.44%] [G loss: 1.817894]\n",
      "epoch:9 step:7575 [D loss: 0.058539, acc.: 99.22%] [G loss: 0.129405]\n",
      "epoch:9 step:7576 [D loss: 0.597896, acc.: 67.97%] [G loss: 6.379952]\n",
      "epoch:9 step:7577 [D loss: 1.593523, acc.: 52.34%] [G loss: 3.455761]\n",
      "epoch:9 step:7578 [D loss: 0.627410, acc.: 68.75%] [G loss: 1.815839]\n",
      "epoch:9 step:7579 [D loss: 0.076997, acc.: 100.00%] [G loss: 2.355023]\n",
      "epoch:9 step:7580 [D loss: 0.063118, acc.: 99.22%] [G loss: 2.987551]\n",
      "epoch:9 step:7581 [D loss: 0.190913, acc.: 92.97%] [G loss: 2.834804]\n",
      "epoch:9 step:7582 [D loss: 0.045971, acc.: 100.00%] [G loss: 3.642550]\n",
      "epoch:9 step:7583 [D loss: 0.243095, acc.: 91.41%] [G loss: 2.754552]\n",
      "epoch:9 step:7584 [D loss: 0.073920, acc.: 99.22%] [G loss: 3.133035]\n",
      "epoch:9 step:7585 [D loss: 0.210940, acc.: 94.53%] [G loss: 3.380269]\n",
      "epoch:9 step:7586 [D loss: 0.133983, acc.: 96.88%] [G loss: 4.015664]\n",
      "epoch:9 step:7587 [D loss: 0.048003, acc.: 100.00%] [G loss: 3.594375]\n",
      "epoch:9 step:7588 [D loss: 0.089093, acc.: 99.22%] [G loss: 3.811680]\n",
      "epoch:9 step:7589 [D loss: 0.069783, acc.: 99.22%] [G loss: 2.838636]\n",
      "epoch:9 step:7590 [D loss: 0.093924, acc.: 99.22%] [G loss: 2.589757]\n",
      "epoch:9 step:7591 [D loss: 0.136196, acc.: 96.09%] [G loss: 2.959082]\n",
      "epoch:9 step:7592 [D loss: 0.432610, acc.: 77.34%] [G loss: 5.393135]\n",
      "epoch:9 step:7593 [D loss: 0.429121, acc.: 75.00%] [G loss: 2.918446]\n",
      "epoch:9 step:7594 [D loss: 0.127985, acc.: 95.31%] [G loss: 4.393866]\n",
      "epoch:9 step:7595 [D loss: 0.072581, acc.: 99.22%] [G loss: 2.680300]\n",
      "epoch:9 step:7596 [D loss: 0.023833, acc.: 99.22%] [G loss: 3.453183]\n",
      "epoch:9 step:7597 [D loss: 0.015425, acc.: 100.00%] [G loss: 1.231546]\n",
      "epoch:9 step:7598 [D loss: 0.027447, acc.: 100.00%] [G loss: 1.025363]\n",
      "epoch:9 step:7599 [D loss: 0.032222, acc.: 100.00%] [G loss: 1.353928]\n",
      "epoch:9 step:7600 [D loss: 0.063710, acc.: 100.00%] [G loss: 1.575730]\n",
      "epoch:9 step:7601 [D loss: 0.029547, acc.: 100.00%] [G loss: 2.293857]\n",
      "epoch:9 step:7602 [D loss: 0.176056, acc.: 97.66%] [G loss: 0.765088]\n",
      "epoch:9 step:7603 [D loss: 0.069287, acc.: 99.22%] [G loss: 1.767423]\n",
      "epoch:9 step:7604 [D loss: 0.066994, acc.: 99.22%] [G loss: 0.580597]\n",
      "epoch:9 step:7605 [D loss: 0.114153, acc.: 99.22%] [G loss: 1.246573]\n",
      "epoch:9 step:7606 [D loss: 0.054403, acc.: 100.00%] [G loss: 1.628087]\n",
      "epoch:9 step:7607 [D loss: 0.041106, acc.: 100.00%] [G loss: 1.404275]\n",
      "epoch:9 step:7608 [D loss: 0.057870, acc.: 99.22%] [G loss: 0.511711]\n",
      "epoch:9 step:7609 [D loss: 0.639797, acc.: 65.62%] [G loss: 7.031402]\n",
      "epoch:9 step:7610 [D loss: 1.328697, acc.: 53.12%] [G loss: 4.029950]\n",
      "epoch:9 step:7611 [D loss: 0.164030, acc.: 94.53%] [G loss: 4.249411]\n",
      "epoch:9 step:7612 [D loss: 0.058562, acc.: 99.22%] [G loss: 3.544774]\n",
      "epoch:9 step:7613 [D loss: 0.081284, acc.: 98.44%] [G loss: 3.728148]\n",
      "epoch:9 step:7614 [D loss: 0.033465, acc.: 100.00%] [G loss: 3.537786]\n",
      "epoch:9 step:7615 [D loss: 0.083671, acc.: 98.44%] [G loss: 3.693096]\n",
      "epoch:9 step:7616 [D loss: 0.095658, acc.: 98.44%] [G loss: 3.464232]\n",
      "epoch:9 step:7617 [D loss: 0.154639, acc.: 96.09%] [G loss: 4.676303]\n",
      "epoch:9 step:7618 [D loss: 0.271163, acc.: 91.41%] [G loss: 3.470731]\n",
      "epoch:9 step:7619 [D loss: 0.161312, acc.: 95.31%] [G loss: 4.878763]\n",
      "epoch:9 step:7620 [D loss: 0.191333, acc.: 94.53%] [G loss: 4.037261]\n",
      "epoch:9 step:7621 [D loss: 0.038618, acc.: 100.00%] [G loss: 3.464794]\n",
      "epoch:9 step:7622 [D loss: 0.073838, acc.: 100.00%] [G loss: 3.854801]\n",
      "epoch:9 step:7623 [D loss: 0.044252, acc.: 100.00%] [G loss: 4.066590]\n",
      "epoch:9 step:7624 [D loss: 0.031745, acc.: 100.00%] [G loss: 4.182235]\n",
      "epoch:9 step:7625 [D loss: 0.208539, acc.: 93.75%] [G loss: 3.909329]\n",
      "epoch:9 step:7626 [D loss: 0.029761, acc.: 100.00%] [G loss: 4.106131]\n",
      "epoch:9 step:7627 [D loss: 0.036419, acc.: 100.00%] [G loss: 3.844549]\n",
      "epoch:9 step:7628 [D loss: 0.158053, acc.: 96.88%] [G loss: 2.285792]\n",
      "epoch:9 step:7629 [D loss: 0.386704, acc.: 77.34%] [G loss: 7.024861]\n",
      "epoch:9 step:7630 [D loss: 1.411784, acc.: 52.34%] [G loss: 2.551411]\n",
      "epoch:9 step:7631 [D loss: 0.122777, acc.: 95.31%] [G loss: 4.027839]\n",
      "epoch:9 step:7632 [D loss: 0.055332, acc.: 98.44%] [G loss: 2.898911]\n",
      "epoch:9 step:7633 [D loss: 0.301823, acc.: 88.28%] [G loss: 3.157075]\n",
      "epoch:9 step:7634 [D loss: 0.021539, acc.: 99.22%] [G loss: 2.979384]\n",
      "epoch:9 step:7635 [D loss: 0.067278, acc.: 99.22%] [G loss: 2.333825]\n",
      "epoch:9 step:7636 [D loss: 0.040262, acc.: 100.00%] [G loss: 1.054418]\n",
      "epoch:9 step:7637 [D loss: 0.225847, acc.: 86.72%] [G loss: 3.967954]\n",
      "epoch:9 step:7638 [D loss: 0.592362, acc.: 67.97%] [G loss: 2.457296]\n",
      "epoch:9 step:7639 [D loss: 0.183915, acc.: 95.31%] [G loss: 3.971270]\n",
      "epoch:9 step:7640 [D loss: 0.020223, acc.: 100.00%] [G loss: 4.105860]\n",
      "epoch:9 step:7641 [D loss: 0.161393, acc.: 96.09%] [G loss: 2.230881]\n",
      "epoch:9 step:7642 [D loss: 0.518808, acc.: 73.44%] [G loss: 5.793108]\n",
      "epoch:9 step:7643 [D loss: 0.121488, acc.: 96.09%] [G loss: 6.003009]\n",
      "epoch:9 step:7644 [D loss: 0.393400, acc.: 81.25%] [G loss: 1.257524]\n",
      "epoch:9 step:7645 [D loss: 0.193433, acc.: 92.19%] [G loss: 3.286918]\n",
      "epoch:9 step:7646 [D loss: 0.018358, acc.: 100.00%] [G loss: 3.411211]\n",
      "epoch:9 step:7647 [D loss: 0.131734, acc.: 95.31%] [G loss: 1.582795]\n",
      "epoch:9 step:7648 [D loss: 0.219478, acc.: 90.62%] [G loss: 2.279095]\n",
      "epoch:9 step:7649 [D loss: 0.051715, acc.: 99.22%] [G loss: 2.475729]\n",
      "epoch:9 step:7650 [D loss: 0.028003, acc.: 100.00%] [G loss: 1.532803]\n",
      "epoch:9 step:7651 [D loss: 0.235511, acc.: 86.72%] [G loss: 2.503070]\n",
      "epoch:9 step:7652 [D loss: 0.434391, acc.: 78.12%] [G loss: 0.717029]\n",
      "epoch:9 step:7653 [D loss: 0.038698, acc.: 100.00%] [G loss: 0.378936]\n",
      "epoch:9 step:7654 [D loss: 0.048390, acc.: 99.22%] [G loss: 0.409824]\n",
      "epoch:9 step:7655 [D loss: 0.093970, acc.: 95.31%] [G loss: 1.122843]\n",
      "epoch:9 step:7656 [D loss: 0.024072, acc.: 100.00%] [G loss: 1.824177]\n",
      "epoch:9 step:7657 [D loss: 0.187489, acc.: 95.31%] [G loss: 1.825392]\n",
      "epoch:9 step:7658 [D loss: 0.236044, acc.: 89.84%] [G loss: 4.051079]\n",
      "epoch:9 step:7659 [D loss: 0.257666, acc.: 89.84%] [G loss: 3.187751]\n",
      "epoch:9 step:7660 [D loss: 0.230992, acc.: 93.75%] [G loss: 3.470315]\n",
      "epoch:9 step:7661 [D loss: 0.026899, acc.: 100.00%] [G loss: 4.396171]\n",
      "epoch:9 step:7662 [D loss: 0.246110, acc.: 90.62%] [G loss: 4.058566]\n",
      "epoch:9 step:7663 [D loss: 0.053687, acc.: 100.00%] [G loss: 4.455898]\n",
      "epoch:9 step:7664 [D loss: 0.076359, acc.: 98.44%] [G loss: 4.101706]\n",
      "epoch:9 step:7665 [D loss: 0.121422, acc.: 94.53%] [G loss: 4.997137]\n",
      "epoch:9 step:7666 [D loss: 0.282321, acc.: 88.28%] [G loss: 4.630725]\n",
      "epoch:9 step:7667 [D loss: 0.022034, acc.: 100.00%] [G loss: 5.275304]\n",
      "epoch:9 step:7668 [D loss: 0.094932, acc.: 96.88%] [G loss: 3.194348]\n",
      "epoch:9 step:7669 [D loss: 0.120819, acc.: 97.66%] [G loss: 5.132587]\n",
      "epoch:9 step:7670 [D loss: 0.014020, acc.: 100.00%] [G loss: 5.929601]\n",
      "epoch:9 step:7671 [D loss: 1.396187, acc.: 36.72%] [G loss: 6.566591]\n",
      "epoch:9 step:7672 [D loss: 0.356180, acc.: 78.91%] [G loss: 4.810787]\n",
      "epoch:9 step:7673 [D loss: 0.037119, acc.: 100.00%] [G loss: 3.656952]\n",
      "epoch:9 step:7674 [D loss: 0.044630, acc.: 99.22%] [G loss: 4.186158]\n",
      "epoch:9 step:7675 [D loss: 0.254318, acc.: 91.41%] [G loss: 4.438894]\n",
      "epoch:9 step:7676 [D loss: 0.027736, acc.: 100.00%] [G loss: 4.823992]\n",
      "epoch:9 step:7677 [D loss: 0.166833, acc.: 94.53%] [G loss: 1.837945]\n",
      "epoch:9 step:7678 [D loss: 0.420464, acc.: 80.47%] [G loss: 6.650223]\n",
      "epoch:9 step:7679 [D loss: 1.085787, acc.: 52.34%] [G loss: 1.894037]\n",
      "epoch:9 step:7680 [D loss: 0.115642, acc.: 96.09%] [G loss: 1.842223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7681 [D loss: 0.083604, acc.: 97.66%] [G loss: 4.274844]\n",
      "epoch:9 step:7682 [D loss: 0.112630, acc.: 96.88%] [G loss: 3.906044]\n",
      "epoch:9 step:7683 [D loss: 0.050002, acc.: 99.22%] [G loss: 2.702270]\n",
      "epoch:9 step:7684 [D loss: 0.061521, acc.: 99.22%] [G loss: 1.327248]\n",
      "epoch:9 step:7685 [D loss: 1.222352, acc.: 51.56%] [G loss: 7.189323]\n",
      "epoch:9 step:7686 [D loss: 2.000203, acc.: 50.00%] [G loss: 3.739354]\n",
      "epoch:9 step:7687 [D loss: 0.694117, acc.: 72.66%] [G loss: 3.626545]\n",
      "epoch:9 step:7688 [D loss: 0.258345, acc.: 89.06%] [G loss: 2.651625]\n",
      "epoch:9 step:7689 [D loss: 0.283948, acc.: 87.50%] [G loss: 2.372387]\n",
      "epoch:9 step:7690 [D loss: 0.319695, acc.: 86.72%] [G loss: 3.158560]\n",
      "epoch:9 step:7691 [D loss: 0.803151, acc.: 53.12%] [G loss: 2.887972]\n",
      "epoch:9 step:7692 [D loss: 0.158518, acc.: 97.66%] [G loss: 3.266443]\n",
      "epoch:9 step:7693 [D loss: 0.089568, acc.: 98.44%] [G loss: 3.150001]\n",
      "epoch:9 step:7694 [D loss: 0.403583, acc.: 83.59%] [G loss: 2.533102]\n",
      "epoch:9 step:7695 [D loss: 0.263422, acc.: 92.19%] [G loss: 4.114934]\n",
      "epoch:9 step:7696 [D loss: 1.042380, acc.: 48.44%] [G loss: 3.032683]\n",
      "epoch:9 step:7697 [D loss: 0.120663, acc.: 96.09%] [G loss: 3.598011]\n",
      "epoch:9 step:7698 [D loss: 0.360034, acc.: 82.03%] [G loss: 4.116371]\n",
      "epoch:9 step:7699 [D loss: 0.226903, acc.: 91.41%] [G loss: 3.059463]\n",
      "epoch:9 step:7700 [D loss: 0.241751, acc.: 94.53%] [G loss: 3.870380]\n",
      "epoch:9 step:7701 [D loss: 0.158460, acc.: 98.44%] [G loss: 3.649444]\n",
      "epoch:9 step:7702 [D loss: 0.366959, acc.: 86.72%] [G loss: 2.836032]\n",
      "epoch:9 step:7703 [D loss: 0.383334, acc.: 85.16%] [G loss: 3.598127]\n",
      "epoch:9 step:7704 [D loss: 0.131045, acc.: 95.31%] [G loss: 2.554272]\n",
      "epoch:9 step:7705 [D loss: 0.085217, acc.: 98.44%] [G loss: 3.013000]\n",
      "epoch:9 step:7706 [D loss: 0.099791, acc.: 97.66%] [G loss: 2.119283]\n",
      "epoch:9 step:7707 [D loss: 0.116863, acc.: 98.44%] [G loss: 2.695546]\n",
      "epoch:9 step:7708 [D loss: 0.152309, acc.: 96.88%] [G loss: 3.283967]\n",
      "epoch:9 step:7709 [D loss: 0.201525, acc.: 93.75%] [G loss: 3.294865]\n",
      "epoch:9 step:7710 [D loss: 0.186506, acc.: 94.53%] [G loss: 2.869498]\n",
      "epoch:9 step:7711 [D loss: 0.104680, acc.: 96.09%] [G loss: 2.274375]\n",
      "epoch:9 step:7712 [D loss: 0.253253, acc.: 89.84%] [G loss: 5.121761]\n",
      "epoch:9 step:7713 [D loss: 0.129387, acc.: 96.88%] [G loss: 5.038100]\n",
      "epoch:9 step:7714 [D loss: 1.264847, acc.: 39.06%] [G loss: 5.536686]\n",
      "epoch:9 step:7715 [D loss: 0.307732, acc.: 83.59%] [G loss: 4.351846]\n",
      "epoch:9 step:7716 [D loss: 0.190744, acc.: 92.97%] [G loss: 3.467680]\n",
      "epoch:9 step:7717 [D loss: 0.133100, acc.: 96.88%] [G loss: 3.142068]\n",
      "epoch:9 step:7718 [D loss: 0.057835, acc.: 98.44%] [G loss: 3.759406]\n",
      "epoch:9 step:7719 [D loss: 0.078911, acc.: 99.22%] [G loss: 3.616910]\n",
      "epoch:9 step:7720 [D loss: 0.087495, acc.: 98.44%] [G loss: 2.614007]\n",
      "epoch:9 step:7721 [D loss: 0.074785, acc.: 98.44%] [G loss: 3.504751]\n",
      "epoch:9 step:7722 [D loss: 0.077492, acc.: 99.22%] [G loss: 2.380948]\n",
      "epoch:9 step:7723 [D loss: 0.176754, acc.: 96.09%] [G loss: 3.603511]\n",
      "epoch:9 step:7724 [D loss: 0.077943, acc.: 99.22%] [G loss: 2.597061]\n",
      "epoch:9 step:7725 [D loss: 0.522814, acc.: 74.22%] [G loss: 5.019604]\n",
      "epoch:9 step:7726 [D loss: 0.208142, acc.: 89.84%] [G loss: 3.599492]\n",
      "epoch:9 step:7727 [D loss: 0.029664, acc.: 100.00%] [G loss: 2.787756]\n",
      "epoch:9 step:7728 [D loss: 0.090012, acc.: 99.22%] [G loss: 3.614712]\n",
      "epoch:9 step:7729 [D loss: 0.054852, acc.: 99.22%] [G loss: 3.702405]\n",
      "epoch:9 step:7730 [D loss: 0.057795, acc.: 100.00%] [G loss: 3.446483]\n",
      "epoch:9 step:7731 [D loss: 0.207575, acc.: 94.53%] [G loss: 2.691354]\n",
      "epoch:9 step:7732 [D loss: 0.218602, acc.: 92.19%] [G loss: 4.691580]\n",
      "epoch:9 step:7733 [D loss: 0.630525, acc.: 67.19%] [G loss: 2.148231]\n",
      "epoch:9 step:7734 [D loss: 0.096829, acc.: 96.88%] [G loss: 4.898268]\n",
      "epoch:9 step:7735 [D loss: 0.048295, acc.: 98.44%] [G loss: 5.275859]\n",
      "epoch:9 step:7736 [D loss: 0.061942, acc.: 100.00%] [G loss: 3.584168]\n",
      "epoch:9 step:7737 [D loss: 0.215368, acc.: 92.97%] [G loss: 5.590465]\n",
      "epoch:9 step:7738 [D loss: 0.238892, acc.: 88.28%] [G loss: 4.606032]\n",
      "epoch:9 step:7739 [D loss: 0.133470, acc.: 96.09%] [G loss: 4.222627]\n",
      "epoch:9 step:7740 [D loss: 0.064033, acc.: 100.00%] [G loss: 2.716350]\n",
      "epoch:9 step:7741 [D loss: 0.205722, acc.: 93.75%] [G loss: 4.578442]\n",
      "epoch:9 step:7742 [D loss: 1.490208, acc.: 33.59%] [G loss: 6.008554]\n",
      "epoch:9 step:7743 [D loss: 0.985047, acc.: 59.38%] [G loss: 3.136476]\n",
      "epoch:9 step:7744 [D loss: 0.450873, acc.: 76.56%] [G loss: 2.041316]\n",
      "epoch:9 step:7745 [D loss: 0.141010, acc.: 93.75%] [G loss: 3.373092]\n",
      "epoch:9 step:7746 [D loss: 0.037830, acc.: 99.22%] [G loss: 4.028172]\n",
      "epoch:9 step:7747 [D loss: 0.055836, acc.: 100.00%] [G loss: 3.605898]\n",
      "epoch:9 step:7748 [D loss: 0.064458, acc.: 100.00%] [G loss: 3.095804]\n",
      "epoch:9 step:7749 [D loss: 0.034015, acc.: 100.00%] [G loss: 3.868072]\n",
      "epoch:9 step:7750 [D loss: 0.040767, acc.: 99.22%] [G loss: 3.341546]\n",
      "epoch:9 step:7751 [D loss: 0.125700, acc.: 96.88%] [G loss: 3.675869]\n",
      "epoch:9 step:7752 [D loss: 0.069341, acc.: 100.00%] [G loss: 2.821964]\n",
      "epoch:9 step:7753 [D loss: 0.104403, acc.: 99.22%] [G loss: 2.212981]\n",
      "epoch:9 step:7754 [D loss: 0.635967, acc.: 71.88%] [G loss: 5.363933]\n",
      "epoch:9 step:7755 [D loss: 0.329967, acc.: 85.16%] [G loss: 3.325761]\n",
      "epoch:9 step:7756 [D loss: 0.438132, acc.: 80.47%] [G loss: 2.637015]\n",
      "epoch:9 step:7757 [D loss: 0.234804, acc.: 89.84%] [G loss: 4.811514]\n",
      "epoch:9 step:7758 [D loss: 0.159378, acc.: 92.97%] [G loss: 3.714083]\n",
      "epoch:9 step:7759 [D loss: 0.093200, acc.: 97.66%] [G loss: 2.807209]\n",
      "epoch:9 step:7760 [D loss: 0.062082, acc.: 98.44%] [G loss: 4.096336]\n",
      "epoch:9 step:7761 [D loss: 0.041086, acc.: 100.00%] [G loss: 3.703856]\n",
      "epoch:9 step:7762 [D loss: 0.084758, acc.: 96.88%] [G loss: 3.095227]\n",
      "epoch:9 step:7763 [D loss: 0.070720, acc.: 100.00%] [G loss: 2.823071]\n",
      "epoch:9 step:7764 [D loss: 0.195391, acc.: 92.19%] [G loss: 4.804388]\n",
      "epoch:9 step:7765 [D loss: 0.274696, acc.: 91.41%] [G loss: 3.576712]\n",
      "epoch:9 step:7766 [D loss: 0.205728, acc.: 91.41%] [G loss: 1.583876]\n",
      "epoch:9 step:7767 [D loss: 0.138105, acc.: 96.88%] [G loss: 4.124325]\n",
      "epoch:9 step:7768 [D loss: 0.092543, acc.: 99.22%] [G loss: 3.419561]\n",
      "epoch:9 step:7769 [D loss: 0.312898, acc.: 84.38%] [G loss: 3.574355]\n",
      "epoch:9 step:7770 [D loss: 0.144900, acc.: 94.53%] [G loss: 2.061306]\n",
      "epoch:9 step:7771 [D loss: 0.035732, acc.: 99.22%] [G loss: 1.845207]\n",
      "epoch:9 step:7772 [D loss: 0.092697, acc.: 96.88%] [G loss: 2.419027]\n",
      "epoch:9 step:7773 [D loss: 0.030269, acc.: 100.00%] [G loss: 4.213046]\n",
      "epoch:9 step:7774 [D loss: 0.053335, acc.: 99.22%] [G loss: 2.322721]\n",
      "epoch:9 step:7775 [D loss: 0.168563, acc.: 92.97%] [G loss: 4.133510]\n",
      "epoch:9 step:7776 [D loss: 0.145910, acc.: 96.88%] [G loss: 3.532042]\n",
      "epoch:9 step:7777 [D loss: 0.124410, acc.: 96.09%] [G loss: 3.662738]\n",
      "epoch:9 step:7778 [D loss: 0.185875, acc.: 95.31%] [G loss: 2.808331]\n",
      "epoch:9 step:7779 [D loss: 0.119649, acc.: 96.88%] [G loss: 4.639122]\n",
      "epoch:9 step:7780 [D loss: 0.103412, acc.: 97.66%] [G loss: 3.749638]\n",
      "epoch:9 step:7781 [D loss: 0.095454, acc.: 98.44%] [G loss: 4.078600]\n",
      "epoch:9 step:7782 [D loss: 0.055269, acc.: 99.22%] [G loss: 3.606802]\n",
      "epoch:9 step:7783 [D loss: 0.437678, acc.: 83.59%] [G loss: 6.166367]\n",
      "epoch:9 step:7784 [D loss: 0.389454, acc.: 81.25%] [G loss: 3.591480]\n",
      "epoch:9 step:7785 [D loss: 0.279130, acc.: 88.28%] [G loss: 6.239457]\n",
      "epoch:9 step:7786 [D loss: 0.136403, acc.: 91.41%] [G loss: 4.743954]\n",
      "epoch:9 step:7787 [D loss: 0.040862, acc.: 99.22%] [G loss: 4.203830]\n",
      "epoch:9 step:7788 [D loss: 0.029905, acc.: 100.00%] [G loss: 3.481153]\n",
      "epoch:9 step:7789 [D loss: 0.134721, acc.: 96.09%] [G loss: 5.305459]\n",
      "epoch:9 step:7790 [D loss: 0.018003, acc.: 100.00%] [G loss: 5.113604]\n",
      "epoch:9 step:7791 [D loss: 0.681992, acc.: 67.19%] [G loss: 2.449120]\n",
      "epoch:9 step:7792 [D loss: 0.124411, acc.: 95.31%] [G loss: 5.293518]\n",
      "epoch:9 step:7793 [D loss: 0.022901, acc.: 100.00%] [G loss: 5.563619]\n",
      "epoch:9 step:7794 [D loss: 0.098420, acc.: 96.88%] [G loss: 2.915257]\n",
      "epoch:9 step:7795 [D loss: 0.126458, acc.: 96.09%] [G loss: 5.206476]\n",
      "epoch:9 step:7796 [D loss: 0.029772, acc.: 100.00%] [G loss: 4.470401]\n",
      "epoch:9 step:7797 [D loss: 0.060013, acc.: 98.44%] [G loss: 3.675220]\n",
      "epoch:9 step:7798 [D loss: 0.033549, acc.: 100.00%] [G loss: 4.439142]\n",
      "epoch:9 step:7799 [D loss: 0.234257, acc.: 93.75%] [G loss: 3.359851]\n",
      "epoch:9 step:7800 [D loss: 0.331258, acc.: 85.94%] [G loss: 3.842247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9 step:7801 [D loss: 0.105537, acc.: 97.66%] [G loss: 4.933948]\n",
      "epoch:9 step:7802 [D loss: 0.038100, acc.: 99.22%] [G loss: 4.928883]\n",
      "epoch:9 step:7803 [D loss: 0.037925, acc.: 100.00%] [G loss: 3.969754]\n",
      "epoch:9 step:7804 [D loss: 0.045035, acc.: 99.22%] [G loss: 3.617354]\n",
      "epoch:9 step:7805 [D loss: 0.071812, acc.: 99.22%] [G loss: 1.917763]\n",
      "epoch:9 step:7806 [D loss: 0.081562, acc.: 98.44%] [G loss: 4.596864]\n",
      "epoch:9 step:7807 [D loss: 0.100835, acc.: 99.22%] [G loss: 3.619560]\n",
      "epoch:9 step:7808 [D loss: 0.670073, acc.: 63.28%] [G loss: 2.917008]\n",
      "epoch:9 step:7809 [D loss: 0.029497, acc.: 99.22%] [G loss: 4.130288]\n",
      "epoch:9 step:7810 [D loss: 0.036801, acc.: 99.22%] [G loss: 2.991332]\n",
      "epoch:10 step:7811 [D loss: 0.320906, acc.: 85.94%] [G loss: 3.281223]\n",
      "epoch:10 step:7812 [D loss: 0.113651, acc.: 96.09%] [G loss: 3.729762]\n",
      "epoch:10 step:7813 [D loss: 0.056815, acc.: 100.00%] [G loss: 2.060967]\n",
      "epoch:10 step:7814 [D loss: 0.191451, acc.: 90.62%] [G loss: 5.668149]\n",
      "epoch:10 step:7815 [D loss: 0.072358, acc.: 100.00%] [G loss: 5.304434]\n",
      "epoch:10 step:7816 [D loss: 0.443941, acc.: 77.34%] [G loss: 4.035159]\n",
      "epoch:10 step:7817 [D loss: 0.065290, acc.: 100.00%] [G loss: 3.441010]\n",
      "epoch:10 step:7818 [D loss: 0.082903, acc.: 96.88%] [G loss: 5.167383]\n",
      "epoch:10 step:7819 [D loss: 0.040776, acc.: 99.22%] [G loss: 4.308192]\n",
      "epoch:10 step:7820 [D loss: 0.117390, acc.: 96.09%] [G loss: 1.234953]\n",
      "epoch:10 step:7821 [D loss: 0.301031, acc.: 86.72%] [G loss: 6.690799]\n",
      "epoch:10 step:7822 [D loss: 0.387786, acc.: 82.03%] [G loss: 4.121327]\n",
      "epoch:10 step:7823 [D loss: 0.059050, acc.: 99.22%] [G loss: 4.402674]\n",
      "epoch:10 step:7824 [D loss: 0.067212, acc.: 98.44%] [G loss: 3.535299]\n",
      "epoch:10 step:7825 [D loss: 0.034335, acc.: 100.00%] [G loss: 2.890557]\n",
      "epoch:10 step:7826 [D loss: 0.023886, acc.: 100.00%] [G loss: 2.326288]\n",
      "epoch:10 step:7827 [D loss: 0.020925, acc.: 100.00%] [G loss: 1.063717]\n",
      "epoch:10 step:7828 [D loss: 0.075720, acc.: 96.88%] [G loss: 1.744741]\n",
      "epoch:10 step:7829 [D loss: 0.279883, acc.: 86.72%] [G loss: 5.757895]\n",
      "epoch:10 step:7830 [D loss: 0.678770, acc.: 67.97%] [G loss: 2.210456]\n",
      "epoch:10 step:7831 [D loss: 0.041746, acc.: 100.00%] [G loss: 3.189874]\n",
      "epoch:10 step:7832 [D loss: 0.059080, acc.: 97.66%] [G loss: 4.063715]\n",
      "epoch:10 step:7833 [D loss: 0.182823, acc.: 94.53%] [G loss: 6.009024]\n",
      "epoch:10 step:7834 [D loss: 2.026637, acc.: 25.00%] [G loss: 9.063283]\n",
      "epoch:10 step:7835 [D loss: 2.408474, acc.: 50.00%] [G loss: 5.951421]\n",
      "epoch:10 step:7836 [D loss: 0.655052, acc.: 69.53%] [G loss: 2.885129]\n",
      "epoch:10 step:7837 [D loss: 0.163934, acc.: 95.31%] [G loss: 3.418878]\n",
      "epoch:10 step:7838 [D loss: 0.053141, acc.: 100.00%] [G loss: 2.946319]\n",
      "epoch:10 step:7839 [D loss: 0.103480, acc.: 98.44%] [G loss: 3.151185]\n",
      "epoch:10 step:7840 [D loss: 0.086401, acc.: 98.44%] [G loss: 3.070994]\n",
      "epoch:10 step:7841 [D loss: 0.136800, acc.: 95.31%] [G loss: 3.378134]\n",
      "epoch:10 step:7842 [D loss: 0.107806, acc.: 98.44%] [G loss: 2.610182]\n",
      "epoch:10 step:7843 [D loss: 0.459232, acc.: 75.78%] [G loss: 2.898019]\n",
      "epoch:10 step:7844 [D loss: 0.193486, acc.: 92.19%] [G loss: 1.524784]\n",
      "epoch:10 step:7845 [D loss: 0.176669, acc.: 98.44%] [G loss: 2.342785]\n",
      "epoch:10 step:7846 [D loss: 0.959473, acc.: 53.91%] [G loss: 4.153725]\n",
      "epoch:10 step:7847 [D loss: 0.567158, acc.: 67.19%] [G loss: 3.586670]\n",
      "epoch:10 step:7848 [D loss: 0.511227, acc.: 76.56%] [G loss: 1.652272]\n",
      "epoch:10 step:7849 [D loss: 0.044330, acc.: 100.00%] [G loss: 2.489204]\n",
      "epoch:10 step:7850 [D loss: 0.063577, acc.: 100.00%] [G loss: 2.814754]\n",
      "epoch:10 step:7851 [D loss: 0.113290, acc.: 98.44%] [G loss: 2.910624]\n",
      "epoch:10 step:7852 [D loss: 0.111044, acc.: 96.88%] [G loss: 3.531557]\n",
      "epoch:10 step:7853 [D loss: 0.490503, acc.: 74.22%] [G loss: 5.130188]\n",
      "epoch:10 step:7854 [D loss: 0.216369, acc.: 89.84%] [G loss: 3.933904]\n",
      "epoch:10 step:7855 [D loss: 0.131912, acc.: 95.31%] [G loss: 2.942985]\n",
      "epoch:10 step:7856 [D loss: 0.163358, acc.: 92.97%] [G loss: 3.321446]\n",
      "epoch:10 step:7857 [D loss: 0.065427, acc.: 98.44%] [G loss: 3.362141]\n",
      "epoch:10 step:7858 [D loss: 0.069770, acc.: 99.22%] [G loss: 3.064497]\n",
      "epoch:10 step:7859 [D loss: 0.503995, acc.: 72.66%] [G loss: 4.979424]\n",
      "epoch:10 step:7860 [D loss: 0.508027, acc.: 74.22%] [G loss: 3.875923]\n",
      "epoch:10 step:7861 [D loss: 0.062756, acc.: 99.22%] [G loss: 2.025964]\n",
      "epoch:10 step:7862 [D loss: 0.054847, acc.: 99.22%] [G loss: 3.868873]\n",
      "epoch:10 step:7863 [D loss: 0.142135, acc.: 95.31%] [G loss: 2.557384]\n",
      "epoch:10 step:7864 [D loss: 0.071380, acc.: 99.22%] [G loss: 3.808624]\n",
      "epoch:10 step:7865 [D loss: 0.069692, acc.: 98.44%] [G loss: 4.025908]\n",
      "epoch:10 step:7866 [D loss: 0.455197, acc.: 78.12%] [G loss: 2.119648]\n",
      "epoch:10 step:7867 [D loss: 0.705588, acc.: 64.06%] [G loss: 7.101979]\n",
      "epoch:10 step:7868 [D loss: 1.134296, acc.: 57.03%] [G loss: 4.849832]\n",
      "epoch:10 step:7869 [D loss: 0.036773, acc.: 100.00%] [G loss: 2.470068]\n",
      "epoch:10 step:7870 [D loss: 0.088894, acc.: 99.22%] [G loss: 2.613137]\n",
      "epoch:10 step:7871 [D loss: 0.073450, acc.: 97.66%] [G loss: 2.865017]\n",
      "epoch:10 step:7872 [D loss: 0.111110, acc.: 96.09%] [G loss: 3.965319]\n",
      "epoch:10 step:7873 [D loss: 0.148649, acc.: 96.88%] [G loss: 3.935778]\n",
      "epoch:10 step:7874 [D loss: 0.170115, acc.: 96.09%] [G loss: 4.012549]\n",
      "epoch:10 step:7875 [D loss: 0.330783, acc.: 83.59%] [G loss: 3.453868]\n",
      "epoch:10 step:7876 [D loss: 0.247726, acc.: 89.06%] [G loss: 4.798651]\n",
      "epoch:10 step:7877 [D loss: 0.166000, acc.: 91.41%] [G loss: 3.615353]\n",
      "epoch:10 step:7878 [D loss: 0.136259, acc.: 96.09%] [G loss: 3.553797]\n",
      "epoch:10 step:7879 [D loss: 0.056077, acc.: 98.44%] [G loss: 4.296981]\n",
      "epoch:10 step:7880 [D loss: 0.080045, acc.: 100.00%] [G loss: 2.552444]\n",
      "epoch:10 step:7881 [D loss: 0.124984, acc.: 98.44%] [G loss: 3.465777]\n",
      "epoch:10 step:7882 [D loss: 0.110438, acc.: 96.88%] [G loss: 4.417953]\n",
      "epoch:10 step:7883 [D loss: 0.888819, acc.: 52.34%] [G loss: 5.255313]\n",
      "epoch:10 step:7884 [D loss: 0.789078, acc.: 64.84%] [G loss: 3.631160]\n",
      "epoch:10 step:7885 [D loss: 0.186021, acc.: 93.75%] [G loss: 3.424058]\n",
      "epoch:10 step:7886 [D loss: 0.218335, acc.: 91.41%] [G loss: 3.152824]\n",
      "epoch:10 step:7887 [D loss: 0.191355, acc.: 93.75%] [G loss: 3.254307]\n",
      "epoch:10 step:7888 [D loss: 0.093619, acc.: 97.66%] [G loss: 3.824601]\n",
      "epoch:10 step:7889 [D loss: 0.250914, acc.: 93.75%] [G loss: 4.361964]\n",
      "epoch:10 step:7890 [D loss: 0.041270, acc.: 100.00%] [G loss: 3.148950]\n",
      "epoch:10 step:7891 [D loss: 0.053646, acc.: 99.22%] [G loss: 4.362556]\n",
      "epoch:10 step:7892 [D loss: 0.150786, acc.: 95.31%] [G loss: 3.587528]\n",
      "epoch:10 step:7893 [D loss: 0.398795, acc.: 84.38%] [G loss: 5.403351]\n",
      "epoch:10 step:7894 [D loss: 0.223487, acc.: 89.06%] [G loss: 4.522655]\n",
      "epoch:10 step:7895 [D loss: 0.205748, acc.: 92.97%] [G loss: 4.424713]\n",
      "epoch:10 step:7896 [D loss: 0.027484, acc.: 100.00%] [G loss: 4.949966]\n",
      "epoch:10 step:7897 [D loss: 0.048770, acc.: 98.44%] [G loss: 3.457669]\n",
      "epoch:10 step:7898 [D loss: 0.133049, acc.: 95.31%] [G loss: 3.929140]\n",
      "epoch:10 step:7899 [D loss: 0.087568, acc.: 98.44%] [G loss: 4.211541]\n",
      "epoch:10 step:7900 [D loss: 1.076225, acc.: 46.09%] [G loss: 3.721450]\n",
      "epoch:10 step:7901 [D loss: 0.016724, acc.: 100.00%] [G loss: 5.000688]\n",
      "epoch:10 step:7902 [D loss: 0.124477, acc.: 94.53%] [G loss: 3.796441]\n",
      "epoch:10 step:7903 [D loss: 0.084233, acc.: 97.66%] [G loss: 3.644597]\n",
      "epoch:10 step:7904 [D loss: 0.072390, acc.: 100.00%] [G loss: 3.234189]\n",
      "epoch:10 step:7905 [D loss: 0.160125, acc.: 94.53%] [G loss: 4.233327]\n",
      "epoch:10 step:7906 [D loss: 0.122030, acc.: 95.31%] [G loss: 3.221971]\n",
      "epoch:10 step:7907 [D loss: 0.158885, acc.: 96.88%] [G loss: 3.674793]\n",
      "epoch:10 step:7908 [D loss: 0.140771, acc.: 96.88%] [G loss: 3.519131]\n",
      "epoch:10 step:7909 [D loss: 0.262742, acc.: 88.28%] [G loss: 2.904904]\n",
      "epoch:10 step:7910 [D loss: 0.062638, acc.: 99.22%] [G loss: 4.273615]\n",
      "epoch:10 step:7911 [D loss: 0.026766, acc.: 100.00%] [G loss: 3.105617]\n",
      "epoch:10 step:7912 [D loss: 0.071815, acc.: 100.00%] [G loss: 3.523132]\n",
      "epoch:10 step:7913 [D loss: 0.099365, acc.: 97.66%] [G loss: 2.805142]\n",
      "epoch:10 step:7914 [D loss: 0.096625, acc.: 98.44%] [G loss: 3.475106]\n",
      "epoch:10 step:7915 [D loss: 0.109820, acc.: 97.66%] [G loss: 3.904143]\n",
      "epoch:10 step:7916 [D loss: 0.059774, acc.: 99.22%] [G loss: 3.005256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:7917 [D loss: 0.480655, acc.: 76.56%] [G loss: 6.183503]\n",
      "epoch:10 step:7918 [D loss: 0.426402, acc.: 75.78%] [G loss: 3.705425]\n",
      "epoch:10 step:7919 [D loss: 0.060543, acc.: 98.44%] [G loss: 3.228191]\n",
      "epoch:10 step:7920 [D loss: 0.026712, acc.: 99.22%] [G loss: 3.682409]\n",
      "epoch:10 step:7921 [D loss: 0.149770, acc.: 96.09%] [G loss: 1.960420]\n",
      "epoch:10 step:7922 [D loss: 0.131018, acc.: 96.88%] [G loss: 3.798671]\n",
      "epoch:10 step:7923 [D loss: 0.129135, acc.: 95.31%] [G loss: 4.180553]\n",
      "epoch:10 step:7924 [D loss: 0.104218, acc.: 99.22%] [G loss: 3.774664]\n",
      "epoch:10 step:7925 [D loss: 0.057881, acc.: 99.22%] [G loss: 2.051166]\n",
      "epoch:10 step:7926 [D loss: 0.434944, acc.: 79.69%] [G loss: 5.056958]\n",
      "epoch:10 step:7927 [D loss: 0.103470, acc.: 96.88%] [G loss: 5.472851]\n",
      "epoch:10 step:7928 [D loss: 0.251560, acc.: 88.28%] [G loss: 3.743238]\n",
      "epoch:10 step:7929 [D loss: 0.067188, acc.: 98.44%] [G loss: 2.954088]\n",
      "epoch:10 step:7930 [D loss: 0.131705, acc.: 97.66%] [G loss: 6.276344]\n",
      "epoch:10 step:7931 [D loss: 0.047219, acc.: 100.00%] [G loss: 5.316502]\n",
      "epoch:10 step:7932 [D loss: 0.147728, acc.: 94.53%] [G loss: 3.463407]\n",
      "epoch:10 step:7933 [D loss: 0.221854, acc.: 91.41%] [G loss: 4.953041]\n",
      "epoch:10 step:7934 [D loss: 0.046329, acc.: 98.44%] [G loss: 5.702572]\n",
      "epoch:10 step:7935 [D loss: 0.176944, acc.: 92.97%] [G loss: 1.981719]\n",
      "epoch:10 step:7936 [D loss: 0.284740, acc.: 84.38%] [G loss: 6.038990]\n",
      "epoch:10 step:7937 [D loss: 0.012813, acc.: 100.00%] [G loss: 8.352180]\n",
      "epoch:10 step:7938 [D loss: 0.921038, acc.: 57.81%] [G loss: 2.018557]\n",
      "epoch:10 step:7939 [D loss: 0.422292, acc.: 79.69%] [G loss: 4.140273]\n",
      "epoch:10 step:7940 [D loss: 0.007331, acc.: 100.00%] [G loss: 4.584876]\n",
      "epoch:10 step:7941 [D loss: 0.120808, acc.: 95.31%] [G loss: 2.731711]\n",
      "epoch:10 step:7942 [D loss: 0.139617, acc.: 96.09%] [G loss: 3.352085]\n",
      "epoch:10 step:7943 [D loss: 0.021089, acc.: 99.22%] [G loss: 3.403843]\n",
      "epoch:10 step:7944 [D loss: 0.050069, acc.: 100.00%] [G loss: 2.123313]\n",
      "epoch:10 step:7945 [D loss: 0.147137, acc.: 96.88%] [G loss: 1.980627]\n",
      "epoch:10 step:7946 [D loss: 0.088280, acc.: 99.22%] [G loss: 2.485966]\n",
      "epoch:10 step:7947 [D loss: 0.278675, acc.: 87.50%] [G loss: 4.504295]\n",
      "epoch:10 step:7948 [D loss: 0.071274, acc.: 98.44%] [G loss: 4.045773]\n",
      "epoch:10 step:7949 [D loss: 0.185021, acc.: 95.31%] [G loss: 1.688964]\n",
      "epoch:10 step:7950 [D loss: 0.294716, acc.: 88.28%] [G loss: 5.073712]\n",
      "epoch:10 step:7951 [D loss: 0.139567, acc.: 93.75%] [G loss: 5.483958]\n",
      "epoch:10 step:7952 [D loss: 0.143605, acc.: 94.53%] [G loss: 2.213200]\n",
      "epoch:10 step:7953 [D loss: 0.046126, acc.: 100.00%] [G loss: 1.805822]\n",
      "epoch:10 step:7954 [D loss: 0.074845, acc.: 99.22%] [G loss: 2.825148]\n",
      "epoch:10 step:7955 [D loss: 0.074310, acc.: 98.44%] [G loss: 4.191800]\n",
      "epoch:10 step:7956 [D loss: 0.406374, acc.: 83.59%] [G loss: 5.172479]\n",
      "epoch:10 step:7957 [D loss: 0.309695, acc.: 86.72%] [G loss: 2.991769]\n",
      "epoch:10 step:7958 [D loss: 0.061334, acc.: 99.22%] [G loss: 4.414312]\n",
      "epoch:10 step:7959 [D loss: 0.027261, acc.: 99.22%] [G loss: 5.321198]\n",
      "epoch:10 step:7960 [D loss: 0.034628, acc.: 100.00%] [G loss: 4.749193]\n",
      "epoch:10 step:7961 [D loss: 0.046840, acc.: 98.44%] [G loss: 4.891704]\n",
      "epoch:10 step:7962 [D loss: 0.037483, acc.: 98.44%] [G loss: 4.815339]\n",
      "epoch:10 step:7963 [D loss: 0.855607, acc.: 57.03%] [G loss: 6.321408]\n",
      "epoch:10 step:7964 [D loss: 1.279898, acc.: 45.31%] [G loss: 3.144969]\n",
      "epoch:10 step:7965 [D loss: 0.057737, acc.: 96.88%] [G loss: 5.314742]\n",
      "epoch:10 step:7966 [D loss: 0.085928, acc.: 97.66%] [G loss: 3.204765]\n",
      "epoch:10 step:7967 [D loss: 0.110579, acc.: 97.66%] [G loss: 4.208322]\n",
      "epoch:10 step:7968 [D loss: 0.625754, acc.: 67.97%] [G loss: 5.097711]\n",
      "epoch:10 step:7969 [D loss: 0.013149, acc.: 100.00%] [G loss: 6.362717]\n",
      "epoch:10 step:7970 [D loss: 0.509322, acc.: 75.78%] [G loss: 1.666164]\n",
      "epoch:10 step:7971 [D loss: 0.994546, acc.: 58.59%] [G loss: 7.083557]\n",
      "epoch:10 step:7972 [D loss: 0.492374, acc.: 78.12%] [G loss: 6.059024]\n",
      "epoch:10 step:7973 [D loss: 0.619016, acc.: 70.31%] [G loss: 2.328560]\n",
      "epoch:10 step:7974 [D loss: 0.077686, acc.: 100.00%] [G loss: 3.318875]\n",
      "epoch:10 step:7975 [D loss: 0.058974, acc.: 99.22%] [G loss: 2.406667]\n",
      "epoch:10 step:7976 [D loss: 0.047710, acc.: 99.22%] [G loss: 2.105627]\n",
      "epoch:10 step:7977 [D loss: 0.046075, acc.: 100.00%] [G loss: 2.620126]\n",
      "epoch:10 step:7978 [D loss: 0.027622, acc.: 99.22%] [G loss: 0.895172]\n",
      "epoch:10 step:7979 [D loss: 0.200232, acc.: 91.41%] [G loss: 3.675580]\n",
      "epoch:10 step:7980 [D loss: 0.114220, acc.: 95.31%] [G loss: 1.625661]\n",
      "epoch:10 step:7981 [D loss: 0.024665, acc.: 100.00%] [G loss: 1.888704]\n",
      "epoch:10 step:7982 [D loss: 0.518573, acc.: 73.44%] [G loss: 5.436688]\n",
      "epoch:10 step:7983 [D loss: 1.188273, acc.: 49.22%] [G loss: 2.790660]\n",
      "epoch:10 step:7984 [D loss: 0.127105, acc.: 96.88%] [G loss: 4.197985]\n",
      "epoch:10 step:7985 [D loss: 0.031446, acc.: 100.00%] [G loss: 3.830113]\n",
      "epoch:10 step:7986 [D loss: 0.348566, acc.: 86.72%] [G loss: 2.908813]\n",
      "epoch:10 step:7987 [D loss: 0.103572, acc.: 99.22%] [G loss: 3.569714]\n",
      "epoch:10 step:7988 [D loss: 0.119963, acc.: 95.31%] [G loss: 1.637694]\n",
      "epoch:10 step:7989 [D loss: 0.614032, acc.: 71.09%] [G loss: 5.857681]\n",
      "epoch:10 step:7990 [D loss: 0.547466, acc.: 71.88%] [G loss: 3.799231]\n",
      "epoch:10 step:7991 [D loss: 0.103202, acc.: 96.88%] [G loss: 3.790203]\n",
      "epoch:10 step:7992 [D loss: 0.036451, acc.: 100.00%] [G loss: 3.431412]\n",
      "epoch:10 step:7993 [D loss: 0.086775, acc.: 98.44%] [G loss: 2.766091]\n",
      "epoch:10 step:7994 [D loss: 0.164661, acc.: 92.19%] [G loss: 4.480281]\n",
      "epoch:10 step:7995 [D loss: 0.116617, acc.: 98.44%] [G loss: 3.677719]\n",
      "epoch:10 step:7996 [D loss: 0.027741, acc.: 100.00%] [G loss: 3.512991]\n",
      "epoch:10 step:7997 [D loss: 0.687201, acc.: 66.41%] [G loss: 5.712364]\n",
      "epoch:10 step:7998 [D loss: 0.541716, acc.: 71.88%] [G loss: 3.710984]\n",
      "epoch:10 step:7999 [D loss: 0.022882, acc.: 100.00%] [G loss: 3.072107]\n",
      "epoch:10 step:8000 [D loss: 0.051108, acc.: 100.00%] [G loss: 3.125579]\n",
      "epoch:10 step:8001 [D loss: 0.083233, acc.: 98.44%] [G loss: 3.938828]\n",
      "epoch:10 step:8002 [D loss: 0.041850, acc.: 100.00%] [G loss: 4.502875]\n",
      "epoch:10 step:8003 [D loss: 0.113293, acc.: 97.66%] [G loss: 3.724975]\n",
      "epoch:10 step:8004 [D loss: 0.026106, acc.: 100.00%] [G loss: 3.023574]\n",
      "epoch:10 step:8005 [D loss: 0.055836, acc.: 100.00%] [G loss: 3.098606]\n",
      "epoch:10 step:8006 [D loss: 0.331294, acc.: 86.72%] [G loss: 4.839901]\n",
      "epoch:10 step:8007 [D loss: 0.082289, acc.: 98.44%] [G loss: 4.562593]\n",
      "epoch:10 step:8008 [D loss: 0.267849, acc.: 88.28%] [G loss: 1.422362]\n",
      "epoch:10 step:8009 [D loss: 0.476926, acc.: 74.22%] [G loss: 6.501980]\n",
      "epoch:10 step:8010 [D loss: 0.567787, acc.: 67.19%] [G loss: 3.614579]\n",
      "epoch:10 step:8011 [D loss: 0.098053, acc.: 97.66%] [G loss: 2.953132]\n",
      "epoch:10 step:8012 [D loss: 0.089064, acc.: 96.88%] [G loss: 3.370429]\n",
      "epoch:10 step:8013 [D loss: 0.019061, acc.: 100.00%] [G loss: 3.825564]\n",
      "epoch:10 step:8014 [D loss: 0.073512, acc.: 97.66%] [G loss: 2.917522]\n",
      "epoch:10 step:8015 [D loss: 0.339959, acc.: 83.59%] [G loss: 4.481689]\n",
      "epoch:10 step:8016 [D loss: 0.263965, acc.: 84.38%] [G loss: 3.133046]\n",
      "epoch:10 step:8017 [D loss: 0.053301, acc.: 98.44%] [G loss: 3.415778]\n",
      "epoch:10 step:8018 [D loss: 0.047855, acc.: 99.22%] [G loss: 2.434884]\n",
      "epoch:10 step:8019 [D loss: 0.127960, acc.: 96.09%] [G loss: 3.704501]\n",
      "epoch:10 step:8020 [D loss: 0.038754, acc.: 100.00%] [G loss: 3.082251]\n",
      "epoch:10 step:8021 [D loss: 0.091965, acc.: 97.66%] [G loss: 4.042669]\n",
      "epoch:10 step:8022 [D loss: 0.058903, acc.: 99.22%] [G loss: 3.432607]\n",
      "epoch:10 step:8023 [D loss: 0.309840, acc.: 90.62%] [G loss: 4.259545]\n",
      "epoch:10 step:8024 [D loss: 1.257162, acc.: 35.16%] [G loss: 4.704780]\n",
      "epoch:10 step:8025 [D loss: 0.134068, acc.: 95.31%] [G loss: 5.699317]\n",
      "epoch:10 step:8026 [D loss: 0.071195, acc.: 99.22%] [G loss: 4.269926]\n",
      "epoch:10 step:8027 [D loss: 0.036089, acc.: 100.00%] [G loss: 2.520012]\n",
      "epoch:10 step:8028 [D loss: 0.094760, acc.: 98.44%] [G loss: 3.711870]\n",
      "epoch:10 step:8029 [D loss: 0.027227, acc.: 100.00%] [G loss: 3.079313]\n",
      "epoch:10 step:8030 [D loss: 0.210645, acc.: 93.75%] [G loss: 3.669415]\n",
      "epoch:10 step:8031 [D loss: 0.028146, acc.: 100.00%] [G loss: 3.707478]\n",
      "epoch:10 step:8032 [D loss: 0.124734, acc.: 96.88%] [G loss: 3.142623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8033 [D loss: 0.110478, acc.: 96.88%] [G loss: 2.397246]\n",
      "epoch:10 step:8034 [D loss: 0.150489, acc.: 95.31%] [G loss: 3.976445]\n",
      "epoch:10 step:8035 [D loss: 0.037485, acc.: 100.00%] [G loss: 4.073998]\n",
      "epoch:10 step:8036 [D loss: 0.266914, acc.: 92.97%] [G loss: 4.662322]\n",
      "epoch:10 step:8037 [D loss: 0.040619, acc.: 100.00%] [G loss: 4.990360]\n",
      "epoch:10 step:8038 [D loss: 0.110627, acc.: 96.09%] [G loss: 2.569664]\n",
      "epoch:10 step:8039 [D loss: 0.216454, acc.: 96.09%] [G loss: 3.401527]\n",
      "epoch:10 step:8040 [D loss: 0.023093, acc.: 100.00%] [G loss: 4.012589]\n",
      "epoch:10 step:8041 [D loss: 0.013603, acc.: 100.00%] [G loss: 4.309002]\n",
      "epoch:10 step:8042 [D loss: 0.089966, acc.: 99.22%] [G loss: 2.322435]\n",
      "epoch:10 step:8043 [D loss: 0.199091, acc.: 92.97%] [G loss: 4.745645]\n",
      "epoch:10 step:8044 [D loss: 0.530023, acc.: 75.00%] [G loss: 3.409148]\n",
      "epoch:10 step:8045 [D loss: 0.016047, acc.: 100.00%] [G loss: 4.359223]\n",
      "epoch:10 step:8046 [D loss: 0.143641, acc.: 94.53%] [G loss: 3.957626]\n",
      "epoch:10 step:8047 [D loss: 0.090775, acc.: 97.66%] [G loss: 5.039476]\n",
      "epoch:10 step:8048 [D loss: 0.045187, acc.: 99.22%] [G loss: 5.011365]\n",
      "epoch:10 step:8049 [D loss: 0.023808, acc.: 100.00%] [G loss: 4.723039]\n",
      "epoch:10 step:8050 [D loss: 1.102594, acc.: 48.44%] [G loss: 7.574663]\n",
      "epoch:10 step:8051 [D loss: 1.250701, acc.: 52.34%] [G loss: 4.790127]\n",
      "epoch:10 step:8052 [D loss: 0.048266, acc.: 100.00%] [G loss: 3.360704]\n",
      "epoch:10 step:8053 [D loss: 0.153126, acc.: 95.31%] [G loss: 3.417308]\n",
      "epoch:10 step:8054 [D loss: 0.098033, acc.: 96.09%] [G loss: 4.120370]\n",
      "epoch:10 step:8055 [D loss: 0.061614, acc.: 99.22%] [G loss: 4.124632]\n",
      "epoch:10 step:8056 [D loss: 0.117440, acc.: 97.66%] [G loss: 2.853108]\n",
      "epoch:10 step:8057 [D loss: 0.055004, acc.: 100.00%] [G loss: 3.753912]\n",
      "epoch:10 step:8058 [D loss: 0.080416, acc.: 98.44%] [G loss: 4.174632]\n",
      "epoch:10 step:8059 [D loss: 0.033128, acc.: 100.00%] [G loss: 4.022064]\n",
      "epoch:10 step:8060 [D loss: 0.421111, acc.: 78.91%] [G loss: 4.955073]\n",
      "epoch:10 step:8061 [D loss: 0.029738, acc.: 100.00%] [G loss: 5.372423]\n",
      "epoch:10 step:8062 [D loss: 0.522220, acc.: 78.12%] [G loss: 2.031561]\n",
      "epoch:10 step:8063 [D loss: 0.216614, acc.: 90.62%] [G loss: 3.884218]\n",
      "epoch:10 step:8064 [D loss: 0.014457, acc.: 100.00%] [G loss: 5.192660]\n",
      "epoch:10 step:8065 [D loss: 0.071187, acc.: 99.22%] [G loss: 4.634341]\n",
      "epoch:10 step:8066 [D loss: 0.062944, acc.: 98.44%] [G loss: 4.053605]\n",
      "epoch:10 step:8067 [D loss: 0.179500, acc.: 94.53%] [G loss: 5.198987]\n",
      "epoch:10 step:8068 [D loss: 0.086510, acc.: 97.66%] [G loss: 4.375576]\n",
      "epoch:10 step:8069 [D loss: 0.087611, acc.: 100.00%] [G loss: 3.616667]\n",
      "epoch:10 step:8070 [D loss: 0.096392, acc.: 98.44%] [G loss: 3.468800]\n",
      "epoch:10 step:8071 [D loss: 0.027078, acc.: 100.00%] [G loss: 3.447614]\n",
      "epoch:10 step:8072 [D loss: 0.024811, acc.: 100.00%] [G loss: 4.012870]\n",
      "epoch:10 step:8073 [D loss: 0.274589, acc.: 88.28%] [G loss: 5.132506]\n",
      "epoch:10 step:8074 [D loss: 0.994704, acc.: 53.12%] [G loss: 4.769250]\n",
      "epoch:10 step:8075 [D loss: 0.092148, acc.: 97.66%] [G loss: 3.915300]\n",
      "epoch:10 step:8076 [D loss: 0.037897, acc.: 100.00%] [G loss: 3.813310]\n",
      "epoch:10 step:8077 [D loss: 0.135951, acc.: 96.88%] [G loss: 3.094183]\n",
      "epoch:10 step:8078 [D loss: 0.016793, acc.: 100.00%] [G loss: 2.571187]\n",
      "epoch:10 step:8079 [D loss: 0.159286, acc.: 92.97%] [G loss: 4.274632]\n",
      "epoch:10 step:8080 [D loss: 0.754968, acc.: 64.06%] [G loss: 5.709477]\n",
      "epoch:10 step:8081 [D loss: 0.431384, acc.: 77.34%] [G loss: 2.030281]\n",
      "epoch:10 step:8082 [D loss: 0.367170, acc.: 80.47%] [G loss: 6.303939]\n",
      "epoch:10 step:8083 [D loss: 0.385691, acc.: 82.81%] [G loss: 4.440825]\n",
      "epoch:10 step:8084 [D loss: 0.091755, acc.: 97.66%] [G loss: 2.454554]\n",
      "epoch:10 step:8085 [D loss: 0.183092, acc.: 92.19%] [G loss: 4.642292]\n",
      "epoch:10 step:8086 [D loss: 0.285037, acc.: 88.28%] [G loss: 3.738806]\n",
      "epoch:10 step:8087 [D loss: 0.451425, acc.: 78.91%] [G loss: 3.610831]\n",
      "epoch:10 step:8088 [D loss: 0.008016, acc.: 100.00%] [G loss: 3.951170]\n",
      "epoch:10 step:8089 [D loss: 0.044847, acc.: 100.00%] [G loss: 2.732844]\n",
      "epoch:10 step:8090 [D loss: 0.090711, acc.: 98.44%] [G loss: 3.097533]\n",
      "epoch:10 step:8091 [D loss: 0.066024, acc.: 98.44%] [G loss: 2.627952]\n",
      "epoch:10 step:8092 [D loss: 0.475256, acc.: 81.25%] [G loss: 4.753430]\n",
      "epoch:10 step:8093 [D loss: 0.172924, acc.: 92.97%] [G loss: 4.243808]\n",
      "epoch:10 step:8094 [D loss: 0.311339, acc.: 88.28%] [G loss: 5.504884]\n",
      "epoch:10 step:8095 [D loss: 0.032828, acc.: 100.00%] [G loss: 5.550381]\n",
      "epoch:10 step:8096 [D loss: 0.534564, acc.: 75.00%] [G loss: 2.057773]\n",
      "epoch:10 step:8097 [D loss: 0.310609, acc.: 85.16%] [G loss: 5.144521]\n",
      "epoch:10 step:8098 [D loss: 0.018186, acc.: 100.00%] [G loss: 6.062151]\n",
      "epoch:10 step:8099 [D loss: 0.160419, acc.: 91.41%] [G loss: 5.188779]\n",
      "epoch:10 step:8100 [D loss: 0.104085, acc.: 95.31%] [G loss: 3.805219]\n",
      "epoch:10 step:8101 [D loss: 0.038957, acc.: 99.22%] [G loss: 4.626007]\n",
      "epoch:10 step:8102 [D loss: 0.044408, acc.: 100.00%] [G loss: 2.871119]\n",
      "epoch:10 step:8103 [D loss: 0.158944, acc.: 95.31%] [G loss: 4.127788]\n",
      "epoch:10 step:8104 [D loss: 0.051515, acc.: 100.00%] [G loss: 5.055871]\n",
      "epoch:10 step:8105 [D loss: 0.174862, acc.: 93.75%] [G loss: 3.121365]\n",
      "epoch:10 step:8106 [D loss: 0.134299, acc.: 97.66%] [G loss: 4.234091]\n",
      "epoch:10 step:8107 [D loss: 0.158274, acc.: 94.53%] [G loss: 3.044562]\n",
      "epoch:10 step:8108 [D loss: 0.041483, acc.: 100.00%] [G loss: 4.228258]\n",
      "epoch:10 step:8109 [D loss: 0.331209, acc.: 89.84%] [G loss: 6.902233]\n",
      "epoch:10 step:8110 [D loss: 0.293764, acc.: 85.16%] [G loss: 3.137113]\n",
      "epoch:10 step:8111 [D loss: 0.167944, acc.: 92.97%] [G loss: 5.282299]\n",
      "epoch:10 step:8112 [D loss: 0.057414, acc.: 100.00%] [G loss: 6.029395]\n",
      "epoch:10 step:8113 [D loss: 0.315982, acc.: 89.06%] [G loss: 4.589842]\n",
      "epoch:10 step:8114 [D loss: 0.011891, acc.: 100.00%] [G loss: 5.402288]\n",
      "epoch:10 step:8115 [D loss: 0.023406, acc.: 100.00%] [G loss: 5.005352]\n",
      "epoch:10 step:8116 [D loss: 0.132153, acc.: 96.09%] [G loss: 3.801495]\n",
      "epoch:10 step:8117 [D loss: 0.287398, acc.: 85.94%] [G loss: 6.902831]\n",
      "epoch:10 step:8118 [D loss: 0.413596, acc.: 81.25%] [G loss: 2.995966]\n",
      "epoch:10 step:8119 [D loss: 0.150252, acc.: 94.53%] [G loss: 5.384330]\n",
      "epoch:10 step:8120 [D loss: 0.104684, acc.: 96.88%] [G loss: 3.752835]\n",
      "epoch:10 step:8121 [D loss: 0.045100, acc.: 100.00%] [G loss: 2.609866]\n",
      "epoch:10 step:8122 [D loss: 0.099535, acc.: 94.53%] [G loss: 0.596884]\n",
      "epoch:10 step:8123 [D loss: 0.470009, acc.: 74.22%] [G loss: 7.862920]\n",
      "epoch:10 step:8124 [D loss: 1.333491, acc.: 53.12%] [G loss: 2.372358]\n",
      "epoch:10 step:8125 [D loss: 0.185184, acc.: 92.97%] [G loss: 1.752716]\n",
      "epoch:10 step:8126 [D loss: 0.222329, acc.: 89.06%] [G loss: 2.455073]\n",
      "epoch:10 step:8127 [D loss: 0.028858, acc.: 99.22%] [G loss: 4.673149]\n",
      "epoch:10 step:8128 [D loss: 0.090803, acc.: 96.88%] [G loss: 2.803351]\n",
      "epoch:10 step:8129 [D loss: 0.019780, acc.: 100.00%] [G loss: 1.781264]\n",
      "epoch:10 step:8130 [D loss: 0.169731, acc.: 94.53%] [G loss: 1.377952]\n",
      "epoch:10 step:8131 [D loss: 0.370474, acc.: 79.69%] [G loss: 3.965427]\n",
      "epoch:10 step:8132 [D loss: 0.039517, acc.: 100.00%] [G loss: 4.559561]\n",
      "epoch:10 step:8133 [D loss: 0.674152, acc.: 70.31%] [G loss: 2.140551]\n",
      "epoch:10 step:8134 [D loss: 0.622258, acc.: 73.44%] [G loss: 5.001813]\n",
      "epoch:10 step:8135 [D loss: 0.175258, acc.: 89.84%] [G loss: 4.599609]\n",
      "epoch:10 step:8136 [D loss: 0.166503, acc.: 94.53%] [G loss: 4.049578]\n",
      "epoch:10 step:8137 [D loss: 0.031295, acc.: 100.00%] [G loss: 3.750812]\n",
      "epoch:10 step:8138 [D loss: 0.032305, acc.: 100.00%] [G loss: 4.270097]\n",
      "epoch:10 step:8139 [D loss: 0.021399, acc.: 100.00%] [G loss: 4.197001]\n",
      "epoch:10 step:8140 [D loss: 0.048606, acc.: 100.00%] [G loss: 3.978911]\n",
      "epoch:10 step:8141 [D loss: 0.088439, acc.: 98.44%] [G loss: 3.669451]\n",
      "epoch:10 step:8142 [D loss: 0.033791, acc.: 100.00%] [G loss: 2.503805]\n",
      "epoch:10 step:8143 [D loss: 0.125385, acc.: 96.88%] [G loss: 4.076781]\n",
      "epoch:10 step:8144 [D loss: 0.131510, acc.: 96.09%] [G loss: 4.263490]\n",
      "epoch:10 step:8145 [D loss: 0.064462, acc.: 98.44%] [G loss: 3.078526]\n",
      "epoch:10 step:8146 [D loss: 0.171342, acc.: 92.97%] [G loss: 3.557128]\n",
      "epoch:10 step:8147 [D loss: 0.063847, acc.: 98.44%] [G loss: 3.801347]\n",
      "epoch:10 step:8148 [D loss: 0.134748, acc.: 95.31%] [G loss: 3.847452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8149 [D loss: 0.032729, acc.: 100.00%] [G loss: 4.105424]\n",
      "epoch:10 step:8150 [D loss: 0.622763, acc.: 69.53%] [G loss: 5.518472]\n",
      "epoch:10 step:8151 [D loss: 0.054719, acc.: 100.00%] [G loss: 5.900244]\n",
      "epoch:10 step:8152 [D loss: 0.182904, acc.: 92.19%] [G loss: 1.507644]\n",
      "epoch:10 step:8153 [D loss: 0.456590, acc.: 78.91%] [G loss: 6.443304]\n",
      "epoch:10 step:8154 [D loss: 0.116510, acc.: 96.88%] [G loss: 6.062243]\n",
      "epoch:10 step:8155 [D loss: 0.252951, acc.: 89.84%] [G loss: 1.627085]\n",
      "epoch:10 step:8156 [D loss: 0.046234, acc.: 99.22%] [G loss: 3.430475]\n",
      "epoch:10 step:8157 [D loss: 0.038074, acc.: 100.00%] [G loss: 3.652629]\n",
      "epoch:10 step:8158 [D loss: 0.013997, acc.: 100.00%] [G loss: 3.422487]\n",
      "epoch:10 step:8159 [D loss: 0.025680, acc.: 100.00%] [G loss: 2.201799]\n",
      "epoch:10 step:8160 [D loss: 0.371762, acc.: 79.69%] [G loss: 6.493562]\n",
      "epoch:10 step:8161 [D loss: 0.736407, acc.: 59.38%] [G loss: 3.302416]\n",
      "epoch:10 step:8162 [D loss: 0.047996, acc.: 100.00%] [G loss: 4.249858]\n",
      "epoch:10 step:8163 [D loss: 0.034670, acc.: 99.22%] [G loss: 5.277657]\n",
      "epoch:10 step:8164 [D loss: 0.047830, acc.: 100.00%] [G loss: 5.555243]\n",
      "epoch:10 step:8165 [D loss: 0.030666, acc.: 100.00%] [G loss: 4.764995]\n",
      "epoch:10 step:8166 [D loss: 0.044929, acc.: 99.22%] [G loss: 1.912017]\n",
      "epoch:10 step:8167 [D loss: 0.016802, acc.: 100.00%] [G loss: 2.246848]\n",
      "epoch:10 step:8168 [D loss: 0.220503, acc.: 92.97%] [G loss: 4.516477]\n",
      "epoch:10 step:8169 [D loss: 0.039207, acc.: 99.22%] [G loss: 3.991407]\n",
      "epoch:10 step:8170 [D loss: 0.017818, acc.: 100.00%] [G loss: 2.653384]\n",
      "epoch:10 step:8171 [D loss: 0.195699, acc.: 92.97%] [G loss: 1.831272]\n",
      "epoch:10 step:8172 [D loss: 0.265530, acc.: 88.28%] [G loss: 5.737320]\n",
      "epoch:10 step:8173 [D loss: 0.072838, acc.: 98.44%] [G loss: 5.929688]\n",
      "epoch:10 step:8174 [D loss: 0.507676, acc.: 75.78%] [G loss: 0.576643]\n",
      "epoch:10 step:8175 [D loss: 0.097785, acc.: 96.88%] [G loss: 1.417315]\n",
      "epoch:10 step:8176 [D loss: 0.006280, acc.: 100.00%] [G loss: 3.562194]\n",
      "epoch:10 step:8177 [D loss: 0.041467, acc.: 100.00%] [G loss: 2.669601]\n",
      "epoch:10 step:8178 [D loss: 0.021391, acc.: 100.00%] [G loss: 1.902977]\n",
      "epoch:10 step:8179 [D loss: 0.055494, acc.: 100.00%] [G loss: 0.862369]\n",
      "epoch:10 step:8180 [D loss: 0.065834, acc.: 99.22%] [G loss: 1.313796]\n",
      "epoch:10 step:8181 [D loss: 0.339747, acc.: 85.16%] [G loss: 6.669078]\n",
      "epoch:10 step:8182 [D loss: 0.915515, acc.: 60.94%] [G loss: 3.375453]\n",
      "epoch:10 step:8183 [D loss: 0.317843, acc.: 84.38%] [G loss: 5.870067]\n",
      "epoch:10 step:8184 [D loss: 0.741977, acc.: 66.41%] [G loss: 2.137168]\n",
      "epoch:10 step:8185 [D loss: 0.214618, acc.: 91.41%] [G loss: 3.376807]\n",
      "epoch:10 step:8186 [D loss: 0.067260, acc.: 99.22%] [G loss: 4.919635]\n",
      "epoch:10 step:8187 [D loss: 0.075078, acc.: 98.44%] [G loss: 4.962481]\n",
      "epoch:10 step:8188 [D loss: 0.022591, acc.: 100.00%] [G loss: 2.924324]\n",
      "epoch:10 step:8189 [D loss: 0.011873, acc.: 100.00%] [G loss: 3.621791]\n",
      "epoch:10 step:8190 [D loss: 0.042893, acc.: 100.00%] [G loss: 2.553398]\n",
      "epoch:10 step:8191 [D loss: 0.069967, acc.: 96.88%] [G loss: 3.035090]\n",
      "epoch:10 step:8192 [D loss: 0.128985, acc.: 96.09%] [G loss: 2.897597]\n",
      "epoch:10 step:8193 [D loss: 0.095134, acc.: 96.88%] [G loss: 4.845216]\n",
      "epoch:10 step:8194 [D loss: 0.053565, acc.: 99.22%] [G loss: 4.573039]\n",
      "epoch:10 step:8195 [D loss: 0.027754, acc.: 100.00%] [G loss: 3.668248]\n",
      "epoch:10 step:8196 [D loss: 0.091024, acc.: 99.22%] [G loss: 3.118351]\n",
      "epoch:10 step:8197 [D loss: 0.053474, acc.: 99.22%] [G loss: 3.846832]\n",
      "epoch:10 step:8198 [D loss: 0.028926, acc.: 100.00%] [G loss: 4.310164]\n",
      "epoch:10 step:8199 [D loss: 0.015240, acc.: 100.00%] [G loss: 4.080873]\n",
      "epoch:10 step:8200 [D loss: 0.052494, acc.: 99.22%] [G loss: 2.667899]\n",
      "epoch:10 step:8201 [D loss: 0.052077, acc.: 99.22%] [G loss: 3.174823]\n",
      "epoch:10 step:8202 [D loss: 0.042651, acc.: 100.00%] [G loss: 3.688809]\n",
      "epoch:10 step:8203 [D loss: 0.121000, acc.: 96.09%] [G loss: 2.700421]\n",
      "epoch:10 step:8204 [D loss: 0.028924, acc.: 100.00%] [G loss: 2.409587]\n",
      "epoch:10 step:8205 [D loss: 0.224728, acc.: 89.84%] [G loss: 0.658985]\n",
      "epoch:10 step:8206 [D loss: 0.260288, acc.: 88.28%] [G loss: 5.206836]\n",
      "epoch:10 step:8207 [D loss: 0.039844, acc.: 98.44%] [G loss: 6.908935]\n",
      "epoch:10 step:8208 [D loss: 0.093852, acc.: 96.88%] [G loss: 3.113142]\n",
      "epoch:10 step:8209 [D loss: 0.012525, acc.: 100.00%] [G loss: 3.505886]\n",
      "epoch:10 step:8210 [D loss: 0.006225, acc.: 100.00%] [G loss: 1.507889]\n",
      "epoch:10 step:8211 [D loss: 0.175148, acc.: 93.75%] [G loss: 0.806153]\n",
      "epoch:10 step:8212 [D loss: 0.045237, acc.: 98.44%] [G loss: 2.297272]\n",
      "epoch:10 step:8213 [D loss: 1.300564, acc.: 38.28%] [G loss: 7.905190]\n",
      "epoch:10 step:8214 [D loss: 1.496557, acc.: 52.34%] [G loss: 4.285696]\n",
      "epoch:10 step:8215 [D loss: 0.323302, acc.: 85.16%] [G loss: 1.826632]\n",
      "epoch:10 step:8216 [D loss: 0.157723, acc.: 94.53%] [G loss: 3.581735]\n",
      "epoch:10 step:8217 [D loss: 0.059634, acc.: 99.22%] [G loss: 4.232662]\n",
      "epoch:10 step:8218 [D loss: 0.056809, acc.: 99.22%] [G loss: 2.534114]\n",
      "epoch:10 step:8219 [D loss: 0.153079, acc.: 96.09%] [G loss: 3.563873]\n",
      "epoch:10 step:8220 [D loss: 0.072519, acc.: 100.00%] [G loss: 3.823737]\n",
      "epoch:10 step:8221 [D loss: 0.729785, acc.: 64.84%] [G loss: 5.631845]\n",
      "epoch:10 step:8222 [D loss: 0.277620, acc.: 87.50%] [G loss: 4.066684]\n",
      "epoch:10 step:8223 [D loss: 0.132733, acc.: 97.66%] [G loss: 4.932405]\n",
      "epoch:10 step:8224 [D loss: 0.016918, acc.: 100.00%] [G loss: 5.011481]\n",
      "epoch:10 step:8225 [D loss: 0.089213, acc.: 97.66%] [G loss: 3.946756]\n",
      "epoch:10 step:8226 [D loss: 0.062438, acc.: 99.22%] [G loss: 3.933220]\n",
      "epoch:10 step:8227 [D loss: 0.078041, acc.: 99.22%] [G loss: 3.123394]\n",
      "epoch:10 step:8228 [D loss: 0.167034, acc.: 91.41%] [G loss: 4.717927]\n",
      "epoch:10 step:8229 [D loss: 0.135481, acc.: 95.31%] [G loss: 3.968138]\n",
      "epoch:10 step:8230 [D loss: 0.149904, acc.: 93.75%] [G loss: 4.483089]\n",
      "epoch:10 step:8231 [D loss: 0.049405, acc.: 100.00%] [G loss: 3.664853]\n",
      "epoch:10 step:8232 [D loss: 0.073890, acc.: 99.22%] [G loss: 4.764275]\n",
      "epoch:10 step:8233 [D loss: 0.026474, acc.: 100.00%] [G loss: 3.397229]\n",
      "epoch:10 step:8234 [D loss: 0.185353, acc.: 92.97%] [G loss: 1.840761]\n",
      "epoch:10 step:8235 [D loss: 0.105846, acc.: 97.66%] [G loss: 2.893110]\n",
      "epoch:10 step:8236 [D loss: 0.015795, acc.: 100.00%] [G loss: 3.198536]\n",
      "epoch:10 step:8237 [D loss: 0.023591, acc.: 100.00%] [G loss: 2.186913]\n",
      "epoch:10 step:8238 [D loss: 0.382544, acc.: 83.59%] [G loss: 1.489168]\n",
      "epoch:10 step:8239 [D loss: 0.113014, acc.: 96.09%] [G loss: 4.618502]\n",
      "epoch:10 step:8240 [D loss: 0.427520, acc.: 75.00%] [G loss: 1.556414]\n",
      "epoch:10 step:8241 [D loss: 0.050606, acc.: 99.22%] [G loss: 1.921746]\n",
      "epoch:10 step:8242 [D loss: 0.024031, acc.: 100.00%] [G loss: 2.441185]\n",
      "epoch:10 step:8243 [D loss: 0.013630, acc.: 100.00%] [G loss: 2.801742]\n",
      "epoch:10 step:8244 [D loss: 0.063462, acc.: 97.66%] [G loss: 4.301860]\n",
      "epoch:10 step:8245 [D loss: 0.014755, acc.: 100.00%] [G loss: 3.341589]\n",
      "epoch:10 step:8246 [D loss: 0.078520, acc.: 99.22%] [G loss: 2.475379]\n",
      "epoch:10 step:8247 [D loss: 0.075198, acc.: 98.44%] [G loss: 1.440270]\n",
      "epoch:10 step:8248 [D loss: 0.013666, acc.: 100.00%] [G loss: 2.074368]\n",
      "epoch:10 step:8249 [D loss: 0.067891, acc.: 99.22%] [G loss: 1.972741]\n",
      "epoch:10 step:8250 [D loss: 0.069408, acc.: 98.44%] [G loss: 3.205853]\n",
      "epoch:10 step:8251 [D loss: 0.370104, acc.: 81.25%] [G loss: 0.725443]\n",
      "epoch:10 step:8252 [D loss: 0.062279, acc.: 98.44%] [G loss: 1.452218]\n",
      "epoch:10 step:8253 [D loss: 0.030752, acc.: 100.00%] [G loss: 2.115679]\n",
      "epoch:10 step:8254 [D loss: 0.067606, acc.: 99.22%] [G loss: 1.543189]\n",
      "epoch:10 step:8255 [D loss: 0.159736, acc.: 93.75%] [G loss: 3.446798]\n",
      "epoch:10 step:8256 [D loss: 0.045198, acc.: 99.22%] [G loss: 5.060333]\n",
      "epoch:10 step:8257 [D loss: 0.018572, acc.: 100.00%] [G loss: 4.033717]\n",
      "epoch:10 step:8258 [D loss: 0.143935, acc.: 94.53%] [G loss: 1.450899]\n",
      "epoch:10 step:8259 [D loss: 0.237067, acc.: 88.28%] [G loss: 4.488007]\n",
      "epoch:10 step:8260 [D loss: 0.026559, acc.: 100.00%] [G loss: 6.771365]\n",
      "epoch:10 step:8261 [D loss: 1.106817, acc.: 47.66%] [G loss: 6.647552]\n",
      "epoch:10 step:8262 [D loss: 0.056181, acc.: 98.44%] [G loss: 6.911239]\n",
      "epoch:10 step:8263 [D loss: 1.260038, acc.: 55.47%] [G loss: 0.584135]\n",
      "epoch:10 step:8264 [D loss: 0.991782, acc.: 66.41%] [G loss: 3.773408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8265 [D loss: 0.009431, acc.: 100.00%] [G loss: 6.260100]\n",
      "epoch:10 step:8266 [D loss: 0.554442, acc.: 71.88%] [G loss: 1.067595]\n",
      "epoch:10 step:8267 [D loss: 0.139659, acc.: 95.31%] [G loss: 1.536138]\n",
      "epoch:10 step:8268 [D loss: 0.018859, acc.: 100.00%] [G loss: 1.023239]\n",
      "epoch:10 step:8269 [D loss: 0.186237, acc.: 92.97%] [G loss: 2.600410]\n",
      "epoch:10 step:8270 [D loss: 0.272860, acc.: 87.50%] [G loss: 1.426310]\n",
      "epoch:10 step:8271 [D loss: 0.100687, acc.: 98.44%] [G loss: 1.969134]\n",
      "epoch:10 step:8272 [D loss: 0.098527, acc.: 97.66%] [G loss: 3.145522]\n",
      "epoch:10 step:8273 [D loss: 0.078407, acc.: 96.88%] [G loss: 3.798421]\n",
      "epoch:10 step:8274 [D loss: 0.212242, acc.: 93.75%] [G loss: 4.118294]\n",
      "epoch:10 step:8275 [D loss: 0.018512, acc.: 100.00%] [G loss: 4.488828]\n",
      "epoch:10 step:8276 [D loss: 0.054830, acc.: 99.22%] [G loss: 4.356486]\n",
      "epoch:10 step:8277 [D loss: 0.242883, acc.: 89.84%] [G loss: 4.549870]\n",
      "epoch:10 step:8278 [D loss: 0.044816, acc.: 98.44%] [G loss: 5.275862]\n",
      "epoch:10 step:8279 [D loss: 0.012595, acc.: 100.00%] [G loss: 4.968004]\n",
      "epoch:10 step:8280 [D loss: 0.015767, acc.: 100.00%] [G loss: 4.966392]\n",
      "epoch:10 step:8281 [D loss: 0.026250, acc.: 100.00%] [G loss: 4.663559]\n",
      "epoch:10 step:8282 [D loss: 0.028484, acc.: 100.00%] [G loss: 4.237107]\n",
      "epoch:10 step:8283 [D loss: 0.017172, acc.: 100.00%] [G loss: 2.621915]\n",
      "epoch:10 step:8284 [D loss: 0.070086, acc.: 99.22%] [G loss: 0.141335]\n",
      "epoch:10 step:8285 [D loss: 0.222964, acc.: 91.41%] [G loss: 3.635623]\n",
      "epoch:10 step:8286 [D loss: 0.029714, acc.: 100.00%] [G loss: 4.997695]\n",
      "epoch:10 step:8287 [D loss: 0.151930, acc.: 94.53%] [G loss: 3.779099]\n",
      "epoch:10 step:8288 [D loss: 0.408225, acc.: 77.34%] [G loss: 5.393124]\n",
      "epoch:10 step:8289 [D loss: 0.681213, acc.: 67.97%] [G loss: 4.283524]\n",
      "epoch:10 step:8290 [D loss: 0.038504, acc.: 99.22%] [G loss: 3.588305]\n",
      "epoch:10 step:8291 [D loss: 0.067980, acc.: 99.22%] [G loss: 3.153597]\n",
      "epoch:10 step:8292 [D loss: 0.183312, acc.: 94.53%] [G loss: 4.035407]\n",
      "epoch:10 step:8293 [D loss: 0.015705, acc.: 100.00%] [G loss: 1.605330]\n",
      "epoch:10 step:8294 [D loss: 0.048296, acc.: 100.00%] [G loss: 1.456535]\n",
      "epoch:10 step:8295 [D loss: 0.102119, acc.: 98.44%] [G loss: 4.381729]\n",
      "epoch:10 step:8296 [D loss: 0.099875, acc.: 97.66%] [G loss: 2.287332]\n",
      "epoch:10 step:8297 [D loss: 0.296292, acc.: 87.50%] [G loss: 4.742630]\n",
      "epoch:10 step:8298 [D loss: 0.862454, acc.: 54.69%] [G loss: 5.613597]\n",
      "epoch:10 step:8299 [D loss: 0.070918, acc.: 97.66%] [G loss: 5.699245]\n",
      "epoch:10 step:8300 [D loss: 0.021437, acc.: 100.00%] [G loss: 4.704876]\n",
      "epoch:10 step:8301 [D loss: 0.094335, acc.: 97.66%] [G loss: 2.909462]\n",
      "epoch:10 step:8302 [D loss: 0.177963, acc.: 94.53%] [G loss: 4.672217]\n",
      "epoch:10 step:8303 [D loss: 0.012433, acc.: 100.00%] [G loss: 5.236666]\n",
      "epoch:10 step:8304 [D loss: 0.057430, acc.: 99.22%] [G loss: 3.566570]\n",
      "epoch:10 step:8305 [D loss: 0.065090, acc.: 98.44%] [G loss: 3.063182]\n",
      "epoch:10 step:8306 [D loss: 0.038995, acc.: 100.00%] [G loss: 3.345533]\n",
      "epoch:10 step:8307 [D loss: 0.027644, acc.: 100.00%] [G loss: 3.333833]\n",
      "epoch:10 step:8308 [D loss: 0.375547, acc.: 83.59%] [G loss: 4.066905]\n",
      "epoch:10 step:8309 [D loss: 0.401302, acc.: 84.38%] [G loss: 2.974689]\n",
      "epoch:10 step:8310 [D loss: 0.239740, acc.: 89.06%] [G loss: 6.174168]\n",
      "epoch:10 step:8311 [D loss: 0.674130, acc.: 67.19%] [G loss: 2.853879]\n",
      "epoch:10 step:8312 [D loss: 0.127567, acc.: 94.53%] [G loss: 4.492218]\n",
      "epoch:10 step:8313 [D loss: 0.077283, acc.: 97.66%] [G loss: 3.670034]\n",
      "epoch:10 step:8314 [D loss: 0.073255, acc.: 98.44%] [G loss: 4.776009]\n",
      "epoch:10 step:8315 [D loss: 0.024820, acc.: 100.00%] [G loss: 3.991229]\n",
      "epoch:10 step:8316 [D loss: 0.114103, acc.: 96.88%] [G loss: 4.728014]\n",
      "epoch:10 step:8317 [D loss: 0.186905, acc.: 94.53%] [G loss: 5.319067]\n",
      "epoch:10 step:8318 [D loss: 0.122639, acc.: 95.31%] [G loss: 2.911953]\n",
      "epoch:10 step:8319 [D loss: 0.101035, acc.: 98.44%] [G loss: 2.325788]\n",
      "epoch:10 step:8320 [D loss: 0.173873, acc.: 94.53%] [G loss: 6.599561]\n",
      "epoch:10 step:8321 [D loss: 0.356797, acc.: 81.25%] [G loss: 3.181516]\n",
      "epoch:10 step:8322 [D loss: 0.111432, acc.: 96.88%] [G loss: 5.473278]\n",
      "epoch:10 step:8323 [D loss: 0.020013, acc.: 100.00%] [G loss: 5.689983]\n",
      "epoch:10 step:8324 [D loss: 0.068637, acc.: 99.22%] [G loss: 3.739303]\n",
      "epoch:10 step:8325 [D loss: 0.148571, acc.: 95.31%] [G loss: 5.814607]\n",
      "epoch:10 step:8326 [D loss: 0.151534, acc.: 96.09%] [G loss: 3.988527]\n",
      "epoch:10 step:8327 [D loss: 0.017812, acc.: 100.00%] [G loss: 3.880272]\n",
      "epoch:10 step:8328 [D loss: 0.119606, acc.: 98.44%] [G loss: 6.036143]\n",
      "epoch:10 step:8329 [D loss: 1.368514, acc.: 29.69%] [G loss: 7.473564]\n",
      "epoch:10 step:8330 [D loss: 1.153581, acc.: 55.47%] [G loss: 3.671969]\n",
      "epoch:10 step:8331 [D loss: 0.699543, acc.: 68.75%] [G loss: 6.447733]\n",
      "epoch:10 step:8332 [D loss: 0.322072, acc.: 82.03%] [G loss: 6.135058]\n",
      "epoch:10 step:8333 [D loss: 0.066823, acc.: 96.88%] [G loss: 4.715973]\n",
      "epoch:10 step:8334 [D loss: 0.055349, acc.: 99.22%] [G loss: 4.571051]\n",
      "epoch:10 step:8335 [D loss: 0.051304, acc.: 100.00%] [G loss: 3.896604]\n",
      "epoch:10 step:8336 [D loss: 0.072625, acc.: 99.22%] [G loss: 4.076530]\n",
      "epoch:10 step:8337 [D loss: 0.106516, acc.: 96.88%] [G loss: 3.637206]\n",
      "epoch:10 step:8338 [D loss: 0.078223, acc.: 99.22%] [G loss: 3.902504]\n",
      "epoch:10 step:8339 [D loss: 0.052761, acc.: 100.00%] [G loss: 3.892601]\n",
      "epoch:10 step:8340 [D loss: 0.045785, acc.: 100.00%] [G loss: 2.629405]\n",
      "epoch:10 step:8341 [D loss: 0.247810, acc.: 92.97%] [G loss: 4.182564]\n",
      "epoch:10 step:8342 [D loss: 0.480488, acc.: 74.22%] [G loss: 3.979879]\n",
      "epoch:10 step:8343 [D loss: 0.139931, acc.: 96.09%] [G loss: 4.342497]\n",
      "epoch:10 step:8344 [D loss: 0.087785, acc.: 98.44%] [G loss: 3.727232]\n",
      "epoch:10 step:8345 [D loss: 0.024315, acc.: 100.00%] [G loss: 4.204784]\n",
      "epoch:10 step:8346 [D loss: 0.146370, acc.: 96.88%] [G loss: 4.031623]\n",
      "epoch:10 step:8347 [D loss: 0.099582, acc.: 100.00%] [G loss: 3.983071]\n",
      "epoch:10 step:8348 [D loss: 0.037234, acc.: 100.00%] [G loss: 3.688298]\n",
      "epoch:10 step:8349 [D loss: 0.173142, acc.: 92.97%] [G loss: 4.234860]\n",
      "epoch:10 step:8350 [D loss: 0.014043, acc.: 100.00%] [G loss: 4.030126]\n",
      "epoch:10 step:8351 [D loss: 0.045168, acc.: 100.00%] [G loss: 3.759356]\n",
      "epoch:10 step:8352 [D loss: 1.997325, acc.: 28.91%] [G loss: 6.971529]\n",
      "epoch:10 step:8353 [D loss: 1.574168, acc.: 50.78%] [G loss: 2.965333]\n",
      "epoch:10 step:8354 [D loss: 0.528732, acc.: 73.44%] [G loss: 2.669041]\n",
      "epoch:10 step:8355 [D loss: 0.182314, acc.: 91.41%] [G loss: 1.834298]\n",
      "epoch:10 step:8356 [D loss: 0.663293, acc.: 67.19%] [G loss: 3.712284]\n",
      "epoch:10 step:8357 [D loss: 0.636487, acc.: 65.62%] [G loss: 2.377377]\n",
      "epoch:10 step:8358 [D loss: 0.275728, acc.: 89.06%] [G loss: 2.062481]\n",
      "epoch:10 step:8359 [D loss: 0.226551, acc.: 90.62%] [G loss: 2.801896]\n",
      "epoch:10 step:8360 [D loss: 0.310969, acc.: 84.38%] [G loss: 2.051893]\n",
      "epoch:10 step:8361 [D loss: 0.183820, acc.: 92.97%] [G loss: 2.201822]\n",
      "epoch:10 step:8362 [D loss: 0.622447, acc.: 69.53%] [G loss: 1.934534]\n",
      "epoch:10 step:8363 [D loss: 0.020995, acc.: 100.00%] [G loss: 3.178001]\n",
      "epoch:10 step:8364 [D loss: 0.223943, acc.: 93.75%] [G loss: 1.526734]\n",
      "epoch:10 step:8365 [D loss: 0.159679, acc.: 93.75%] [G loss: 2.391741]\n",
      "epoch:10 step:8366 [D loss: 0.059712, acc.: 100.00%] [G loss: 2.976785]\n",
      "epoch:10 step:8367 [D loss: 0.317124, acc.: 86.72%] [G loss: 2.588613]\n",
      "epoch:10 step:8368 [D loss: 0.131147, acc.: 96.88%] [G loss: 3.989263]\n",
      "epoch:10 step:8369 [D loss: 0.061334, acc.: 98.44%] [G loss: 3.374283]\n",
      "epoch:10 step:8370 [D loss: 0.159453, acc.: 98.44%] [G loss: 2.745353]\n",
      "epoch:10 step:8371 [D loss: 0.060446, acc.: 99.22%] [G loss: 1.359189]\n",
      "epoch:10 step:8372 [D loss: 0.045431, acc.: 99.22%] [G loss: 1.465248]\n",
      "epoch:10 step:8373 [D loss: 0.100064, acc.: 97.66%] [G loss: 2.274036]\n",
      "epoch:10 step:8374 [D loss: 0.016423, acc.: 100.00%] [G loss: 1.957371]\n",
      "epoch:10 step:8375 [D loss: 0.099352, acc.: 97.66%] [G loss: 1.479776]\n",
      "epoch:10 step:8376 [D loss: 0.032653, acc.: 100.00%] [G loss: 0.820767]\n",
      "epoch:10 step:8377 [D loss: 0.047895, acc.: 98.44%] [G loss: 0.377422]\n",
      "epoch:10 step:8378 [D loss: 0.067090, acc.: 98.44%] [G loss: 0.578185]\n",
      "epoch:10 step:8379 [D loss: 0.188761, acc.: 92.97%] [G loss: 2.526757]\n",
      "epoch:10 step:8380 [D loss: 0.163658, acc.: 93.75%] [G loss: 2.506778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8381 [D loss: 0.117934, acc.: 96.88%] [G loss: 0.532287]\n",
      "epoch:10 step:8382 [D loss: 0.009612, acc.: 100.00%] [G loss: 0.522152]\n",
      "epoch:10 step:8383 [D loss: 0.086325, acc.: 98.44%] [G loss: 0.843579]\n",
      "epoch:10 step:8384 [D loss: 0.009177, acc.: 100.00%] [G loss: 0.677162]\n",
      "epoch:10 step:8385 [D loss: 0.079097, acc.: 99.22%] [G loss: 0.944968]\n",
      "epoch:10 step:8386 [D loss: 0.215749, acc.: 89.84%] [G loss: 0.167952]\n",
      "epoch:10 step:8387 [D loss: 0.034439, acc.: 99.22%] [G loss: 0.326842]\n",
      "epoch:10 step:8388 [D loss: 0.011553, acc.: 100.00%] [G loss: 0.194844]\n",
      "epoch:10 step:8389 [D loss: 0.425896, acc.: 77.34%] [G loss: 3.963552]\n",
      "epoch:10 step:8390 [D loss: 1.024073, acc.: 57.03%] [G loss: 1.731512]\n",
      "epoch:10 step:8391 [D loss: 0.183073, acc.: 90.62%] [G loss: 1.511972]\n",
      "epoch:10 step:8392 [D loss: 0.008689, acc.: 100.00%] [G loss: 1.984435]\n",
      "epoch:10 step:8393 [D loss: 0.060826, acc.: 99.22%] [G loss: 1.700080]\n",
      "epoch:10 step:8394 [D loss: 0.122117, acc.: 96.09%] [G loss: 2.677965]\n",
      "epoch:10 step:8395 [D loss: 0.017909, acc.: 99.22%] [G loss: 2.833396]\n",
      "epoch:10 step:8396 [D loss: 0.031002, acc.: 100.00%] [G loss: 2.173829]\n",
      "epoch:10 step:8397 [D loss: 0.077035, acc.: 97.66%] [G loss: 3.113415]\n",
      "epoch:10 step:8398 [D loss: 0.034206, acc.: 100.00%] [G loss: 2.823725]\n",
      "epoch:10 step:8399 [D loss: 0.047650, acc.: 98.44%] [G loss: 1.437099]\n",
      "epoch:10 step:8400 [D loss: 0.109499, acc.: 96.88%] [G loss: 2.348183]\n",
      "epoch:10 step:8401 [D loss: 0.185051, acc.: 95.31%] [G loss: 3.157874]\n",
      "epoch:10 step:8402 [D loss: 0.075871, acc.: 98.44%] [G loss: 2.699380]\n",
      "epoch:10 step:8403 [D loss: 0.307155, acc.: 89.84%] [G loss: 3.925726]\n",
      "epoch:10 step:8404 [D loss: 0.029971, acc.: 100.00%] [G loss: 4.042518]\n",
      "epoch:10 step:8405 [D loss: 0.013130, acc.: 100.00%] [G loss: 4.735000]\n",
      "epoch:10 step:8406 [D loss: 0.046609, acc.: 100.00%] [G loss: 4.144395]\n",
      "epoch:10 step:8407 [D loss: 0.027313, acc.: 100.00%] [G loss: 4.957561]\n",
      "epoch:10 step:8408 [D loss: 0.021450, acc.: 100.00%] [G loss: 3.935001]\n",
      "epoch:10 step:8409 [D loss: 0.106740, acc.: 98.44%] [G loss: 3.685317]\n",
      "epoch:10 step:8410 [D loss: 0.033422, acc.: 100.00%] [G loss: 3.564820]\n",
      "epoch:10 step:8411 [D loss: 0.029298, acc.: 100.00%] [G loss: 2.303071]\n",
      "epoch:10 step:8412 [D loss: 0.075511, acc.: 98.44%] [G loss: 4.598785]\n",
      "epoch:10 step:8413 [D loss: 0.080388, acc.: 98.44%] [G loss: 3.770712]\n",
      "epoch:10 step:8414 [D loss: 1.613384, acc.: 35.94%] [G loss: 7.042645]\n",
      "epoch:10 step:8415 [D loss: 0.944664, acc.: 59.38%] [G loss: 5.104335]\n",
      "epoch:10 step:8416 [D loss: 0.123633, acc.: 96.88%] [G loss: 2.773359]\n",
      "epoch:10 step:8417 [D loss: 0.182896, acc.: 92.97%] [G loss: 3.776638]\n",
      "epoch:10 step:8418 [D loss: 0.040909, acc.: 99.22%] [G loss: 3.649732]\n",
      "epoch:10 step:8419 [D loss: 0.048993, acc.: 99.22%] [G loss: 3.829356]\n",
      "epoch:10 step:8420 [D loss: 0.289188, acc.: 87.50%] [G loss: 1.000252]\n",
      "epoch:10 step:8421 [D loss: 0.113224, acc.: 96.88%] [G loss: 3.528172]\n",
      "epoch:10 step:8422 [D loss: 0.047777, acc.: 100.00%] [G loss: 4.000823]\n",
      "epoch:10 step:8423 [D loss: 0.429544, acc.: 78.91%] [G loss: 4.185087]\n",
      "epoch:10 step:8424 [D loss: 0.040780, acc.: 100.00%] [G loss: 4.500703]\n",
      "epoch:10 step:8425 [D loss: 0.079551, acc.: 100.00%] [G loss: 2.782228]\n",
      "epoch:10 step:8426 [D loss: 0.830297, acc.: 59.38%] [G loss: 6.288874]\n",
      "epoch:10 step:8427 [D loss: 0.505055, acc.: 73.44%] [G loss: 5.422757]\n",
      "epoch:10 step:8428 [D loss: 0.053718, acc.: 99.22%] [G loss: 3.300568]\n",
      "epoch:10 step:8429 [D loss: 0.154085, acc.: 95.31%] [G loss: 3.883192]\n",
      "epoch:10 step:8430 [D loss: 0.027622, acc.: 100.00%] [G loss: 4.041066]\n",
      "epoch:10 step:8431 [D loss: 0.066318, acc.: 97.66%] [G loss: 3.732405]\n",
      "epoch:10 step:8432 [D loss: 0.073281, acc.: 100.00%] [G loss: 2.829659]\n",
      "epoch:10 step:8433 [D loss: 0.114508, acc.: 99.22%] [G loss: 3.282039]\n",
      "epoch:10 step:8434 [D loss: 0.091924, acc.: 99.22%] [G loss: 4.564267]\n",
      "epoch:10 step:8435 [D loss: 0.049499, acc.: 100.00%] [G loss: 3.814332]\n",
      "epoch:10 step:8436 [D loss: 0.263185, acc.: 92.97%] [G loss: 3.194178]\n",
      "epoch:10 step:8437 [D loss: 0.323825, acc.: 85.16%] [G loss: 2.854986]\n",
      "epoch:10 step:8438 [D loss: 0.669828, acc.: 63.28%] [G loss: 7.516498]\n",
      "epoch:10 step:8439 [D loss: 1.111066, acc.: 55.47%] [G loss: 5.528932]\n",
      "epoch:10 step:8440 [D loss: 0.022466, acc.: 100.00%] [G loss: 2.858144]\n",
      "epoch:10 step:8441 [D loss: 0.050942, acc.: 100.00%] [G loss: 2.640850]\n",
      "epoch:10 step:8442 [D loss: 0.110690, acc.: 97.66%] [G loss: 3.352794]\n",
      "epoch:10 step:8443 [D loss: 0.029843, acc.: 100.00%] [G loss: 3.552540]\n",
      "epoch:10 step:8444 [D loss: 0.042479, acc.: 100.00%] [G loss: 2.226006]\n",
      "epoch:10 step:8445 [D loss: 0.178238, acc.: 96.09%] [G loss: 4.414742]\n",
      "epoch:10 step:8446 [D loss: 0.112522, acc.: 96.88%] [G loss: 4.167604]\n",
      "epoch:10 step:8447 [D loss: 0.201426, acc.: 93.75%] [G loss: 3.059864]\n",
      "epoch:10 step:8448 [D loss: 0.078673, acc.: 99.22%] [G loss: 4.110725]\n",
      "epoch:10 step:8449 [D loss: 0.108921, acc.: 96.88%] [G loss: 2.333730]\n",
      "epoch:10 step:8450 [D loss: 0.079163, acc.: 99.22%] [G loss: 3.957850]\n",
      "epoch:10 step:8451 [D loss: 0.016605, acc.: 100.00%] [G loss: 3.791001]\n",
      "epoch:10 step:8452 [D loss: 0.102066, acc.: 98.44%] [G loss: 3.389144]\n",
      "epoch:10 step:8453 [D loss: 0.122330, acc.: 96.88%] [G loss: 4.532424]\n",
      "epoch:10 step:8454 [D loss: 0.054194, acc.: 100.00%] [G loss: 3.334408]\n",
      "epoch:10 step:8455 [D loss: 0.149489, acc.: 94.53%] [G loss: 3.504997]\n",
      "epoch:10 step:8456 [D loss: 0.970160, acc.: 50.78%] [G loss: 7.038918]\n",
      "epoch:10 step:8457 [D loss: 0.625107, acc.: 66.41%] [G loss: 3.711831]\n",
      "epoch:10 step:8458 [D loss: 0.303785, acc.: 86.72%] [G loss: 5.876438]\n",
      "epoch:10 step:8459 [D loss: 0.041380, acc.: 100.00%] [G loss: 5.833516]\n",
      "epoch:10 step:8460 [D loss: 0.070251, acc.: 98.44%] [G loss: 6.114381]\n",
      "epoch:10 step:8461 [D loss: 0.032981, acc.: 99.22%] [G loss: 3.332779]\n",
      "epoch:10 step:8462 [D loss: 0.079197, acc.: 97.66%] [G loss: 1.779464]\n",
      "epoch:10 step:8463 [D loss: 0.099421, acc.: 96.88%] [G loss: 3.997281]\n",
      "epoch:10 step:8464 [D loss: 0.055628, acc.: 99.22%] [G loss: 3.485393]\n",
      "epoch:10 step:8465 [D loss: 0.385038, acc.: 82.03%] [G loss: 2.855582]\n",
      "epoch:10 step:8466 [D loss: 0.357323, acc.: 86.72%] [G loss: 3.373338]\n",
      "epoch:10 step:8467 [D loss: 0.072262, acc.: 99.22%] [G loss: 4.791153]\n",
      "epoch:10 step:8468 [D loss: 0.103545, acc.: 97.66%] [G loss: 2.945128]\n",
      "epoch:10 step:8469 [D loss: 0.083334, acc.: 98.44%] [G loss: 5.107520]\n",
      "epoch:10 step:8470 [D loss: 0.385510, acc.: 82.81%] [G loss: 3.888768]\n",
      "epoch:10 step:8471 [D loss: 0.090221, acc.: 97.66%] [G loss: 5.469392]\n",
      "epoch:10 step:8472 [D loss: 0.112298, acc.: 95.31%] [G loss: 3.104949]\n",
      "epoch:10 step:8473 [D loss: 0.182266, acc.: 91.41%] [G loss: 4.395218]\n",
      "epoch:10 step:8474 [D loss: 0.052998, acc.: 97.66%] [G loss: 5.349270]\n",
      "epoch:10 step:8475 [D loss: 0.485837, acc.: 79.69%] [G loss: 2.223873]\n",
      "epoch:10 step:8476 [D loss: 0.125386, acc.: 95.31%] [G loss: 4.162624]\n",
      "epoch:10 step:8477 [D loss: 0.018060, acc.: 100.00%] [G loss: 5.009976]\n",
      "epoch:10 step:8478 [D loss: 0.260362, acc.: 90.62%] [G loss: 3.769351]\n",
      "epoch:10 step:8479 [D loss: 0.050882, acc.: 99.22%] [G loss: 2.768891]\n",
      "epoch:10 step:8480 [D loss: 0.147699, acc.: 93.75%] [G loss: 4.740800]\n",
      "epoch:10 step:8481 [D loss: 0.047641, acc.: 99.22%] [G loss: 4.745991]\n",
      "epoch:10 step:8482 [D loss: 0.475122, acc.: 77.34%] [G loss: 1.312543]\n",
      "epoch:10 step:8483 [D loss: 0.126098, acc.: 94.53%] [G loss: 4.492307]\n",
      "epoch:10 step:8484 [D loss: 0.026958, acc.: 99.22%] [G loss: 4.859572]\n",
      "epoch:10 step:8485 [D loss: 0.206611, acc.: 93.75%] [G loss: 4.042233]\n",
      "epoch:10 step:8486 [D loss: 0.030016, acc.: 100.00%] [G loss: 4.293971]\n",
      "epoch:10 step:8487 [D loss: 0.082446, acc.: 97.66%] [G loss: 2.242518]\n",
      "epoch:10 step:8488 [D loss: 0.303500, acc.: 86.72%] [G loss: 5.187260]\n",
      "epoch:10 step:8489 [D loss: 0.246375, acc.: 86.72%] [G loss: 2.379326]\n",
      "epoch:10 step:8490 [D loss: 0.082662, acc.: 97.66%] [G loss: 4.688595]\n",
      "epoch:10 step:8491 [D loss: 0.014659, acc.: 100.00%] [G loss: 4.920093]\n",
      "epoch:10 step:8492 [D loss: 0.027978, acc.: 100.00%] [G loss: 4.532716]\n",
      "epoch:10 step:8493 [D loss: 0.073098, acc.: 97.66%] [G loss: 4.895529]\n",
      "epoch:10 step:8494 [D loss: 0.049910, acc.: 98.44%] [G loss: 4.063088]\n",
      "epoch:10 step:8495 [D loss: 0.438808, acc.: 78.12%] [G loss: 3.336938]\n",
      "epoch:10 step:8496 [D loss: 0.048904, acc.: 100.00%] [G loss: 3.684544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10 step:8497 [D loss: 0.105294, acc.: 96.88%] [G loss: 1.977409]\n",
      "epoch:10 step:8498 [D loss: 0.038800, acc.: 99.22%] [G loss: 3.430049]\n",
      "epoch:10 step:8499 [D loss: 0.014991, acc.: 100.00%] [G loss: 3.014702]\n",
      "epoch:10 step:8500 [D loss: 0.079246, acc.: 98.44%] [G loss: 2.805839]\n",
      "epoch:10 step:8501 [D loss: 0.025814, acc.: 100.00%] [G loss: 3.955234]\n",
      "epoch:10 step:8502 [D loss: 0.595744, acc.: 74.22%] [G loss: 2.395723]\n",
      "epoch:10 step:8503 [D loss: 0.007860, acc.: 100.00%] [G loss: 3.307981]\n",
      "epoch:10 step:8504 [D loss: 0.026880, acc.: 99.22%] [G loss: 2.401815]\n",
      "epoch:10 step:8505 [D loss: 0.241847, acc.: 90.62%] [G loss: 5.579981]\n",
      "epoch:10 step:8506 [D loss: 0.078135, acc.: 97.66%] [G loss: 5.596045]\n",
      "epoch:10 step:8507 [D loss: 0.030232, acc.: 100.00%] [G loss: 4.262326]\n",
      "epoch:10 step:8508 [D loss: 0.014429, acc.: 100.00%] [G loss: 4.245070]\n",
      "epoch:10 step:8509 [D loss: 0.030623, acc.: 100.00%] [G loss: 3.612006]\n",
      "epoch:10 step:8510 [D loss: 0.066971, acc.: 99.22%] [G loss: 3.738581]\n",
      "epoch:10 step:8511 [D loss: 0.005246, acc.: 100.00%] [G loss: 3.119991]\n",
      "epoch:10 step:8512 [D loss: 0.031014, acc.: 100.00%] [G loss: 3.891320]\n",
      "epoch:10 step:8513 [D loss: 0.148202, acc.: 95.31%] [G loss: 3.878496]\n",
      "epoch:10 step:8514 [D loss: 1.140990, acc.: 52.34%] [G loss: 6.638452]\n",
      "epoch:10 step:8515 [D loss: 0.425009, acc.: 81.25%] [G loss: 4.032952]\n",
      "epoch:10 step:8516 [D loss: 0.009113, acc.: 100.00%] [G loss: 2.939869]\n",
      "epoch:10 step:8517 [D loss: 0.073581, acc.: 99.22%] [G loss: 3.681844]\n",
      "epoch:10 step:8518 [D loss: 0.028197, acc.: 100.00%] [G loss: 4.170527]\n",
      "epoch:10 step:8519 [D loss: 0.031683, acc.: 100.00%] [G loss: 4.567291]\n",
      "epoch:10 step:8520 [D loss: 0.383091, acc.: 81.25%] [G loss: 6.001288]\n",
      "epoch:10 step:8521 [D loss: 0.205931, acc.: 92.19%] [G loss: 4.915331]\n",
      "epoch:10 step:8522 [D loss: 0.077900, acc.: 99.22%] [G loss: 3.609064]\n",
      "epoch:10 step:8523 [D loss: 0.404674, acc.: 82.03%] [G loss: 2.469218]\n",
      "epoch:10 step:8524 [D loss: 0.010761, acc.: 100.00%] [G loss: 6.163830]\n",
      "epoch:10 step:8525 [D loss: 1.714393, acc.: 17.97%] [G loss: 5.785951]\n",
      "epoch:10 step:8526 [D loss: 0.369996, acc.: 81.25%] [G loss: 4.589576]\n",
      "epoch:10 step:8527 [D loss: 0.047896, acc.: 99.22%] [G loss: 3.889820]\n",
      "epoch:10 step:8528 [D loss: 0.051706, acc.: 98.44%] [G loss: 4.256863]\n",
      "epoch:10 step:8529 [D loss: 0.033400, acc.: 100.00%] [G loss: 2.240167]\n",
      "epoch:10 step:8530 [D loss: 0.038909, acc.: 100.00%] [G loss: 4.016006]\n",
      "epoch:10 step:8531 [D loss: 0.190262, acc.: 95.31%] [G loss: 2.545392]\n",
      "epoch:10 step:8532 [D loss: 0.063736, acc.: 99.22%] [G loss: 5.027477]\n",
      "epoch:10 step:8533 [D loss: 0.040520, acc.: 100.00%] [G loss: 4.512690]\n",
      "epoch:10 step:8534 [D loss: 0.205339, acc.: 95.31%] [G loss: 4.073092]\n",
      "epoch:10 step:8535 [D loss: 0.279057, acc.: 90.62%] [G loss: 3.601287]\n",
      "epoch:10 step:8536 [D loss: 0.061725, acc.: 99.22%] [G loss: 4.384703]\n",
      "epoch:10 step:8537 [D loss: 0.361467, acc.: 84.38%] [G loss: 5.415272]\n",
      "epoch:10 step:8538 [D loss: 0.030505, acc.: 100.00%] [G loss: 5.152295]\n",
      "epoch:10 step:8539 [D loss: 0.147155, acc.: 98.44%] [G loss: 2.871293]\n",
      "epoch:10 step:8540 [D loss: 0.172147, acc.: 91.41%] [G loss: 4.840760]\n",
      "epoch:10 step:8541 [D loss: 0.081745, acc.: 97.66%] [G loss: 4.313572]\n",
      "epoch:10 step:8542 [D loss: 0.136862, acc.: 94.53%] [G loss: 0.743217]\n",
      "epoch:10 step:8543 [D loss: 0.105968, acc.: 95.31%] [G loss: 4.613892]\n",
      "epoch:10 step:8544 [D loss: 0.024976, acc.: 100.00%] [G loss: 2.330525]\n",
      "epoch:10 step:8545 [D loss: 0.349230, acc.: 82.81%] [G loss: 2.945369]\n",
      "epoch:10 step:8546 [D loss: 0.119898, acc.: 95.31%] [G loss: 2.103292]\n",
      "epoch:10 step:8547 [D loss: 0.245687, acc.: 92.19%] [G loss: 2.923083]\n",
      "epoch:10 step:8548 [D loss: 0.078562, acc.: 98.44%] [G loss: 2.185300]\n",
      "epoch:10 step:8549 [D loss: 0.233439, acc.: 91.41%] [G loss: 4.289571]\n",
      "epoch:10 step:8550 [D loss: 0.088875, acc.: 96.09%] [G loss: 3.778931]\n",
      "epoch:10 step:8551 [D loss: 0.049902, acc.: 99.22%] [G loss: 3.014271]\n",
      "epoch:10 step:8552 [D loss: 0.035239, acc.: 100.00%] [G loss: 2.779109]\n",
      "epoch:10 step:8553 [D loss: 0.191916, acc.: 92.97%] [G loss: 4.202984]\n",
      "epoch:10 step:8554 [D loss: 0.279293, acc.: 84.38%] [G loss: 1.130281]\n",
      "epoch:10 step:8555 [D loss: 0.142045, acc.: 94.53%] [G loss: 4.304573]\n",
      "epoch:10 step:8556 [D loss: 0.057652, acc.: 98.44%] [G loss: 4.601480]\n",
      "epoch:10 step:8557 [D loss: 0.097483, acc.: 96.09%] [G loss: 5.073938]\n",
      "epoch:10 step:8558 [D loss: 0.062004, acc.: 99.22%] [G loss: 3.936989]\n",
      "epoch:10 step:8559 [D loss: 0.046893, acc.: 99.22%] [G loss: 3.239435]\n",
      "epoch:10 step:8560 [D loss: 0.080279, acc.: 98.44%] [G loss: 3.667018]\n",
      "epoch:10 step:8561 [D loss: 0.051798, acc.: 99.22%] [G loss: 3.790243]\n",
      "epoch:10 step:8562 [D loss: 0.048392, acc.: 100.00%] [G loss: 2.734445]\n",
      "epoch:10 step:8563 [D loss: 0.086535, acc.: 97.66%] [G loss: 4.387868]\n",
      "epoch:10 step:8564 [D loss: 0.451110, acc.: 81.25%] [G loss: 7.247398]\n",
      "epoch:10 step:8565 [D loss: 0.311952, acc.: 82.03%] [G loss: 6.707041]\n",
      "epoch:10 step:8566 [D loss: 0.023334, acc.: 100.00%] [G loss: 5.342288]\n",
      "epoch:10 step:8567 [D loss: 0.028606, acc.: 98.44%] [G loss: 5.913709]\n",
      "epoch:10 step:8568 [D loss: 0.059080, acc.: 97.66%] [G loss: 5.677938]\n",
      "epoch:10 step:8569 [D loss: 0.029667, acc.: 100.00%] [G loss: 5.470295]\n",
      "epoch:10 step:8570 [D loss: 0.104083, acc.: 98.44%] [G loss: 4.172091]\n",
      "epoch:10 step:8571 [D loss: 0.041197, acc.: 100.00%] [G loss: 3.635667]\n",
      "epoch:10 step:8572 [D loss: 0.621912, acc.: 68.75%] [G loss: 9.283133]\n",
      "epoch:10 step:8573 [D loss: 0.751444, acc.: 65.62%] [G loss: 5.232800]\n",
      "epoch:10 step:8574 [D loss: 0.057434, acc.: 98.44%] [G loss: 4.708671]\n",
      "epoch:10 step:8575 [D loss: 0.012389, acc.: 100.00%] [G loss: 4.223685]\n",
      "epoch:10 step:8576 [D loss: 0.016306, acc.: 99.22%] [G loss: 3.438964]\n",
      "epoch:10 step:8577 [D loss: 0.050964, acc.: 99.22%] [G loss: 3.130799]\n",
      "epoch:10 step:8578 [D loss: 0.278919, acc.: 87.50%] [G loss: 5.907117]\n",
      "epoch:10 step:8579 [D loss: 0.443661, acc.: 82.81%] [G loss: 2.856114]\n",
      "epoch:10 step:8580 [D loss: 0.048758, acc.: 99.22%] [G loss: 2.163901]\n",
      "epoch:10 step:8581 [D loss: 0.273757, acc.: 89.06%] [G loss: 5.556594]\n",
      "epoch:10 step:8582 [D loss: 0.013534, acc.: 100.00%] [G loss: 5.783117]\n",
      "epoch:10 step:8583 [D loss: 0.094021, acc.: 96.88%] [G loss: 3.207213]\n",
      "epoch:10 step:8584 [D loss: 0.144409, acc.: 92.97%] [G loss: 4.167648]\n",
      "epoch:10 step:8585 [D loss: 0.006940, acc.: 100.00%] [G loss: 4.249521]\n",
      "epoch:10 step:8586 [D loss: 0.051557, acc.: 98.44%] [G loss: 4.806031]\n",
      "epoch:10 step:8587 [D loss: 0.130096, acc.: 96.09%] [G loss: 3.220734]\n",
      "epoch:10 step:8588 [D loss: 0.233732, acc.: 89.84%] [G loss: 6.090165]\n",
      "epoch:10 step:8589 [D loss: 0.145957, acc.: 93.75%] [G loss: 6.792693]\n",
      "epoch:10 step:8590 [D loss: 0.019921, acc.: 99.22%] [G loss: 5.088962]\n",
      "epoch:10 step:8591 [D loss: 0.043565, acc.: 98.44%] [G loss: 4.022223]\n",
      "epoch:11 step:8592 [D loss: 0.067680, acc.: 98.44%] [G loss: 4.212173]\n",
      "epoch:11 step:8593 [D loss: 0.048528, acc.: 99.22%] [G loss: 4.079617]\n",
      "epoch:11 step:8594 [D loss: 0.084685, acc.: 98.44%] [G loss: 2.469024]\n",
      "epoch:11 step:8595 [D loss: 0.146720, acc.: 96.09%] [G loss: 5.905558]\n",
      "epoch:11 step:8596 [D loss: 0.006997, acc.: 100.00%] [G loss: 5.984787]\n",
      "epoch:11 step:8597 [D loss: 0.131583, acc.: 93.75%] [G loss: 3.819010]\n",
      "epoch:11 step:8598 [D loss: 0.133928, acc.: 96.09%] [G loss: 4.795013]\n",
      "epoch:11 step:8599 [D loss: 0.021211, acc.: 99.22%] [G loss: 4.782798]\n",
      "epoch:11 step:8600 [D loss: 0.067507, acc.: 100.00%] [G loss: 3.700270]\n",
      "epoch:11 step:8601 [D loss: 0.018494, acc.: 100.00%] [G loss: 1.818534]\n",
      "epoch:11 step:8602 [D loss: 0.097297, acc.: 96.09%] [G loss: 5.097130]\n",
      "epoch:11 step:8603 [D loss: 0.085719, acc.: 96.88%] [G loss: 4.589579]\n",
      "epoch:11 step:8604 [D loss: 0.026139, acc.: 100.00%] [G loss: 3.584791]\n",
      "epoch:11 step:8605 [D loss: 0.217550, acc.: 90.62%] [G loss: 3.954862]\n",
      "epoch:11 step:8606 [D loss: 0.019299, acc.: 100.00%] [G loss: 3.505555]\n",
      "epoch:11 step:8607 [D loss: 0.045144, acc.: 100.00%] [G loss: 2.104820]\n",
      "epoch:11 step:8608 [D loss: 0.466306, acc.: 78.91%] [G loss: 9.262980]\n",
      "epoch:11 step:8609 [D loss: 0.978944, acc.: 60.94%] [G loss: 2.726959]\n",
      "epoch:11 step:8610 [D loss: 0.249262, acc.: 90.62%] [G loss: 7.293912]\n",
      "epoch:11 step:8611 [D loss: 0.083717, acc.: 96.88%] [G loss: 8.489658]\n",
      "epoch:11 step:8612 [D loss: 0.185628, acc.: 92.19%] [G loss: 5.167384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8613 [D loss: 0.513081, acc.: 74.22%] [G loss: 9.820870]\n",
      "epoch:11 step:8614 [D loss: 1.044527, acc.: 57.81%] [G loss: 3.249088]\n",
      "epoch:11 step:8615 [D loss: 0.159964, acc.: 92.97%] [G loss: 3.920858]\n",
      "epoch:11 step:8616 [D loss: 0.013126, acc.: 99.22%] [G loss: 6.152794]\n",
      "epoch:11 step:8617 [D loss: 0.108441, acc.: 96.88%] [G loss: 3.204494]\n",
      "epoch:11 step:8618 [D loss: 0.066035, acc.: 98.44%] [G loss: 4.053880]\n",
      "epoch:11 step:8619 [D loss: 0.061366, acc.: 97.66%] [G loss: 0.913293]\n",
      "epoch:11 step:8620 [D loss: 0.021767, acc.: 100.00%] [G loss: 1.408953]\n",
      "epoch:11 step:8621 [D loss: 0.007551, acc.: 100.00%] [G loss: 0.180759]\n",
      "epoch:11 step:8622 [D loss: 0.199469, acc.: 90.62%] [G loss: 2.255332]\n",
      "epoch:11 step:8623 [D loss: 0.113491, acc.: 93.75%] [G loss: 1.427943]\n",
      "epoch:11 step:8624 [D loss: 0.090894, acc.: 98.44%] [G loss: 0.146108]\n",
      "epoch:11 step:8625 [D loss: 0.009142, acc.: 100.00%] [G loss: 0.193304]\n",
      "epoch:11 step:8626 [D loss: 0.116814, acc.: 95.31%] [G loss: 1.847042]\n",
      "epoch:11 step:8627 [D loss: 0.233494, acc.: 89.84%] [G loss: 0.314843]\n",
      "epoch:11 step:8628 [D loss: 0.086554, acc.: 97.66%] [G loss: 0.796890]\n",
      "epoch:11 step:8629 [D loss: 0.036915, acc.: 99.22%] [G loss: 1.660139]\n",
      "epoch:11 step:8630 [D loss: 0.004398, acc.: 100.00%] [G loss: 2.172902]\n",
      "epoch:11 step:8631 [D loss: 0.010252, acc.: 100.00%] [G loss: 0.954751]\n",
      "epoch:11 step:8632 [D loss: 0.041007, acc.: 99.22%] [G loss: 2.233085]\n",
      "epoch:11 step:8633 [D loss: 0.112196, acc.: 97.66%] [G loss: 2.312604]\n",
      "epoch:11 step:8634 [D loss: 1.863593, acc.: 21.09%] [G loss: 6.025000]\n",
      "epoch:11 step:8635 [D loss: 2.175510, acc.: 50.00%] [G loss: 3.920292]\n",
      "epoch:11 step:8636 [D loss: 0.221454, acc.: 92.19%] [G loss: 0.952683]\n",
      "epoch:11 step:8637 [D loss: 0.058246, acc.: 99.22%] [G loss: 3.383074]\n",
      "epoch:11 step:8638 [D loss: 0.143250, acc.: 96.09%] [G loss: 2.357279]\n",
      "epoch:11 step:8639 [D loss: 0.145407, acc.: 97.66%] [G loss: 3.121872]\n",
      "epoch:11 step:8640 [D loss: 0.063436, acc.: 99.22%] [G loss: 3.227893]\n",
      "epoch:11 step:8641 [D loss: 0.169986, acc.: 95.31%] [G loss: 1.983645]\n",
      "epoch:11 step:8642 [D loss: 0.057481, acc.: 100.00%] [G loss: 2.991568]\n",
      "epoch:11 step:8643 [D loss: 0.300377, acc.: 88.28%] [G loss: 0.792613]\n",
      "epoch:11 step:8644 [D loss: 0.067720, acc.: 100.00%] [G loss: 1.852272]\n",
      "epoch:11 step:8645 [D loss: 0.098354, acc.: 97.66%] [G loss: 3.190647]\n",
      "epoch:11 step:8646 [D loss: 0.059614, acc.: 100.00%] [G loss: 3.408739]\n",
      "epoch:11 step:8647 [D loss: 0.172380, acc.: 94.53%] [G loss: 2.192774]\n",
      "epoch:11 step:8648 [D loss: 0.055989, acc.: 99.22%] [G loss: 1.607357]\n",
      "epoch:11 step:8649 [D loss: 0.030951, acc.: 100.00%] [G loss: 2.253810]\n",
      "epoch:11 step:8650 [D loss: 0.029207, acc.: 100.00%] [G loss: 2.004348]\n",
      "epoch:11 step:8651 [D loss: 0.075665, acc.: 98.44%] [G loss: 2.319972]\n",
      "epoch:11 step:8652 [D loss: 0.654327, acc.: 68.75%] [G loss: 1.097829]\n",
      "epoch:11 step:8653 [D loss: 0.078288, acc.: 97.66%] [G loss: 3.988575]\n",
      "epoch:11 step:8654 [D loss: 0.127691, acc.: 94.53%] [G loss: 1.921224]\n",
      "epoch:11 step:8655 [D loss: 0.056323, acc.: 98.44%] [G loss: 2.392696]\n",
      "epoch:11 step:8656 [D loss: 0.134199, acc.: 96.88%] [G loss: 2.691342]\n",
      "epoch:11 step:8657 [D loss: 0.023745, acc.: 100.00%] [G loss: 1.331984]\n",
      "epoch:11 step:8658 [D loss: 0.121394, acc.: 96.88%] [G loss: 1.693216]\n",
      "epoch:11 step:8659 [D loss: 0.043504, acc.: 100.00%] [G loss: 2.777390]\n",
      "epoch:11 step:8660 [D loss: 0.864477, acc.: 58.59%] [G loss: 7.961205]\n",
      "epoch:11 step:8661 [D loss: 2.284437, acc.: 50.00%] [G loss: 5.397859]\n",
      "epoch:11 step:8662 [D loss: 0.344158, acc.: 86.72%] [G loss: 1.943318]\n",
      "epoch:11 step:8663 [D loss: 0.113919, acc.: 97.66%] [G loss: 3.622372]\n",
      "epoch:11 step:8664 [D loss: 0.057874, acc.: 98.44%] [G loss: 3.817529]\n",
      "epoch:11 step:8665 [D loss: 0.032876, acc.: 100.00%] [G loss: 3.883651]\n",
      "epoch:11 step:8666 [D loss: 0.052094, acc.: 99.22%] [G loss: 3.796442]\n",
      "epoch:11 step:8667 [D loss: 0.118879, acc.: 94.53%] [G loss: 3.853392]\n",
      "epoch:11 step:8668 [D loss: 0.137584, acc.: 95.31%] [G loss: 3.125122]\n",
      "epoch:11 step:8669 [D loss: 0.059934, acc.: 100.00%] [G loss: 3.244705]\n",
      "epoch:11 step:8670 [D loss: 0.034897, acc.: 100.00%] [G loss: 3.452724]\n",
      "epoch:11 step:8671 [D loss: 0.119811, acc.: 98.44%] [G loss: 3.152095]\n",
      "epoch:11 step:8672 [D loss: 0.048992, acc.: 99.22%] [G loss: 2.427727]\n",
      "epoch:11 step:8673 [D loss: 0.028626, acc.: 100.00%] [G loss: 2.585511]\n",
      "epoch:11 step:8674 [D loss: 0.108606, acc.: 98.44%] [G loss: 3.385258]\n",
      "epoch:11 step:8675 [D loss: 0.067918, acc.: 98.44%] [G loss: 4.104935]\n",
      "epoch:11 step:8676 [D loss: 0.171816, acc.: 95.31%] [G loss: 1.853950]\n",
      "epoch:11 step:8677 [D loss: 0.056184, acc.: 98.44%] [G loss: 1.150854]\n",
      "epoch:11 step:8678 [D loss: 0.072236, acc.: 98.44%] [G loss: 2.261365]\n",
      "epoch:11 step:8679 [D loss: 0.069622, acc.: 99.22%] [G loss: 2.345922]\n",
      "epoch:11 step:8680 [D loss: 0.047263, acc.: 100.00%] [G loss: 1.591625]\n",
      "epoch:11 step:8681 [D loss: 0.828487, acc.: 53.12%] [G loss: 3.843090]\n",
      "epoch:11 step:8682 [D loss: 0.026797, acc.: 100.00%] [G loss: 5.417916]\n",
      "epoch:11 step:8683 [D loss: 0.180260, acc.: 92.19%] [G loss: 4.120696]\n",
      "epoch:11 step:8684 [D loss: 0.030355, acc.: 100.00%] [G loss: 2.834872]\n",
      "epoch:11 step:8685 [D loss: 0.095953, acc.: 95.31%] [G loss: 3.715897]\n",
      "epoch:11 step:8686 [D loss: 0.021833, acc.: 100.00%] [G loss: 3.342012]\n",
      "epoch:11 step:8687 [D loss: 0.064153, acc.: 97.66%] [G loss: 1.995940]\n",
      "epoch:11 step:8688 [D loss: 0.040267, acc.: 100.00%] [G loss: 1.726863]\n",
      "epoch:11 step:8689 [D loss: 0.233345, acc.: 91.41%] [G loss: 3.454186]\n",
      "epoch:11 step:8690 [D loss: 0.543420, acc.: 75.00%] [G loss: 1.703120]\n",
      "epoch:11 step:8691 [D loss: 0.526583, acc.: 74.22%] [G loss: 5.990400]\n",
      "epoch:11 step:8692 [D loss: 0.356619, acc.: 79.69%] [G loss: 4.837262]\n",
      "epoch:11 step:8693 [D loss: 0.048335, acc.: 99.22%] [G loss: 5.308162]\n",
      "epoch:11 step:8694 [D loss: 0.057754, acc.: 99.22%] [G loss: 4.740016]\n",
      "epoch:11 step:8695 [D loss: 0.060771, acc.: 100.00%] [G loss: 4.513319]\n",
      "epoch:11 step:8696 [D loss: 0.044803, acc.: 99.22%] [G loss: 3.778121]\n",
      "epoch:11 step:8697 [D loss: 0.032119, acc.: 100.00%] [G loss: 3.224140]\n",
      "epoch:11 step:8698 [D loss: 0.316124, acc.: 85.16%] [G loss: 4.975606]\n",
      "epoch:11 step:8699 [D loss: 0.122898, acc.: 95.31%] [G loss: 4.103934]\n",
      "epoch:11 step:8700 [D loss: 0.069024, acc.: 99.22%] [G loss: 3.892979]\n",
      "epoch:11 step:8701 [D loss: 0.024098, acc.: 100.00%] [G loss: 5.289109]\n",
      "epoch:11 step:8702 [D loss: 0.264817, acc.: 91.41%] [G loss: 5.497848]\n",
      "epoch:11 step:8703 [D loss: 0.044704, acc.: 98.44%] [G loss: 4.623976]\n",
      "epoch:11 step:8704 [D loss: 0.025360, acc.: 100.00%] [G loss: 3.975109]\n",
      "epoch:11 step:8705 [D loss: 0.125348, acc.: 97.66%] [G loss: 4.604585]\n",
      "epoch:11 step:8706 [D loss: 0.018690, acc.: 100.00%] [G loss: 4.406843]\n",
      "epoch:11 step:8707 [D loss: 0.219874, acc.: 90.62%] [G loss: 2.074921]\n",
      "epoch:11 step:8708 [D loss: 0.361599, acc.: 82.81%] [G loss: 5.212653]\n",
      "epoch:11 step:8709 [D loss: 0.399425, acc.: 76.56%] [G loss: 3.419077]\n",
      "epoch:11 step:8710 [D loss: 0.060435, acc.: 98.44%] [G loss: 4.318441]\n",
      "epoch:11 step:8711 [D loss: 0.037132, acc.: 99.22%] [G loss: 3.404861]\n",
      "epoch:11 step:8712 [D loss: 0.048506, acc.: 99.22%] [G loss: 4.259698]\n",
      "epoch:11 step:8713 [D loss: 0.055549, acc.: 100.00%] [G loss: 3.360614]\n",
      "epoch:11 step:8714 [D loss: 0.056183, acc.: 98.44%] [G loss: 3.896789]\n",
      "epoch:11 step:8715 [D loss: 0.035157, acc.: 100.00%] [G loss: 3.644884]\n",
      "epoch:11 step:8716 [D loss: 0.109038, acc.: 97.66%] [G loss: 4.557149]\n",
      "epoch:11 step:8717 [D loss: 0.041466, acc.: 98.44%] [G loss: 3.442186]\n",
      "epoch:11 step:8718 [D loss: 0.038510, acc.: 99.22%] [G loss: 3.513161]\n",
      "epoch:11 step:8719 [D loss: 0.060389, acc.: 97.66%] [G loss: 2.759699]\n",
      "epoch:11 step:8720 [D loss: 0.070397, acc.: 99.22%] [G loss: 1.737232]\n",
      "epoch:11 step:8721 [D loss: 0.186128, acc.: 92.19%] [G loss: 6.340841]\n",
      "epoch:11 step:8722 [D loss: 0.270565, acc.: 89.06%] [G loss: 4.489334]\n",
      "epoch:11 step:8723 [D loss: 0.053667, acc.: 100.00%] [G loss: 2.730226]\n",
      "epoch:11 step:8724 [D loss: 0.008552, acc.: 100.00%] [G loss: 4.802763]\n",
      "epoch:11 step:8725 [D loss: 0.017253, acc.: 100.00%] [G loss: 3.860218]\n",
      "epoch:11 step:8726 [D loss: 0.027530, acc.: 100.00%] [G loss: 2.088808]\n",
      "epoch:11 step:8727 [D loss: 0.067598, acc.: 98.44%] [G loss: 2.822467]\n",
      "epoch:11 step:8728 [D loss: 0.025971, acc.: 100.00%] [G loss: 2.993599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8729 [D loss: 0.026952, acc.: 100.00%] [G loss: 2.418207]\n",
      "epoch:11 step:8730 [D loss: 0.513566, acc.: 77.34%] [G loss: 6.708661]\n",
      "epoch:11 step:8731 [D loss: 1.496908, acc.: 51.56%] [G loss: 2.394648]\n",
      "epoch:11 step:8732 [D loss: 0.284638, acc.: 88.28%] [G loss: 5.110612]\n",
      "epoch:11 step:8733 [D loss: 0.033461, acc.: 99.22%] [G loss: 4.901356]\n",
      "epoch:11 step:8734 [D loss: 0.330440, acc.: 86.72%] [G loss: 1.730025]\n",
      "epoch:11 step:8735 [D loss: 0.176262, acc.: 92.97%] [G loss: 3.517731]\n",
      "epoch:11 step:8736 [D loss: 0.028970, acc.: 100.00%] [G loss: 4.277493]\n",
      "epoch:11 step:8737 [D loss: 0.092513, acc.: 96.88%] [G loss: 3.872032]\n",
      "epoch:11 step:8738 [D loss: 0.137570, acc.: 96.09%] [G loss: 4.256911]\n",
      "epoch:11 step:8739 [D loss: 0.086275, acc.: 98.44%] [G loss: 3.932765]\n",
      "epoch:11 step:8740 [D loss: 0.363182, acc.: 86.72%] [G loss: 4.170650]\n",
      "epoch:11 step:8741 [D loss: 0.019060, acc.: 100.00%] [G loss: 5.000289]\n",
      "epoch:11 step:8742 [D loss: 0.068384, acc.: 99.22%] [G loss: 4.950706]\n",
      "epoch:11 step:8743 [D loss: 0.023810, acc.: 100.00%] [G loss: 3.159170]\n",
      "epoch:11 step:8744 [D loss: 0.288193, acc.: 89.06%] [G loss: 3.339521]\n",
      "epoch:11 step:8745 [D loss: 0.032218, acc.: 100.00%] [G loss: 3.810389]\n",
      "epoch:11 step:8746 [D loss: 0.197043, acc.: 92.97%] [G loss: 4.683634]\n",
      "epoch:11 step:8747 [D loss: 0.090389, acc.: 97.66%] [G loss: 3.506113]\n",
      "epoch:11 step:8748 [D loss: 0.118668, acc.: 98.44%] [G loss: 2.400454]\n",
      "epoch:11 step:8749 [D loss: 0.134335, acc.: 93.75%] [G loss: 4.509341]\n",
      "epoch:11 step:8750 [D loss: 0.009816, acc.: 100.00%] [G loss: 5.918452]\n",
      "epoch:11 step:8751 [D loss: 0.327719, acc.: 87.50%] [G loss: 1.989347]\n",
      "epoch:11 step:8752 [D loss: 0.061461, acc.: 99.22%] [G loss: 4.770541]\n",
      "epoch:11 step:8753 [D loss: 0.007150, acc.: 100.00%] [G loss: 4.146910]\n",
      "epoch:11 step:8754 [D loss: 0.090259, acc.: 96.88%] [G loss: 3.970202]\n",
      "epoch:11 step:8755 [D loss: 0.149094, acc.: 96.09%] [G loss: 4.632376]\n",
      "epoch:11 step:8756 [D loss: 0.013456, acc.: 100.00%] [G loss: 5.094222]\n",
      "epoch:11 step:8757 [D loss: 0.277061, acc.: 86.72%] [G loss: 1.385748]\n",
      "epoch:11 step:8758 [D loss: 0.490172, acc.: 74.22%] [G loss: 7.505578]\n",
      "epoch:11 step:8759 [D loss: 1.310092, acc.: 52.34%] [G loss: 4.463320]\n",
      "epoch:11 step:8760 [D loss: 0.093935, acc.: 97.66%] [G loss: 2.653207]\n",
      "epoch:11 step:8761 [D loss: 0.057713, acc.: 98.44%] [G loss: 2.794540]\n",
      "epoch:11 step:8762 [D loss: 0.014810, acc.: 100.00%] [G loss: 3.280172]\n",
      "epoch:11 step:8763 [D loss: 0.029480, acc.: 100.00%] [G loss: 2.833216]\n",
      "epoch:11 step:8764 [D loss: 0.146816, acc.: 95.31%] [G loss: 3.642582]\n",
      "epoch:11 step:8765 [D loss: 0.114916, acc.: 96.09%] [G loss: 3.221629]\n",
      "epoch:11 step:8766 [D loss: 0.209911, acc.: 90.62%] [G loss: 4.374521]\n",
      "epoch:11 step:8767 [D loss: 0.841902, acc.: 56.25%] [G loss: 4.375155]\n",
      "epoch:11 step:8768 [D loss: 0.069311, acc.: 97.66%] [G loss: 3.658686]\n",
      "epoch:11 step:8769 [D loss: 0.370069, acc.: 81.25%] [G loss: 1.478397]\n",
      "epoch:11 step:8770 [D loss: 0.147852, acc.: 93.75%] [G loss: 1.050512]\n",
      "epoch:11 step:8771 [D loss: 0.041478, acc.: 100.00%] [G loss: 2.962406]\n",
      "epoch:11 step:8772 [D loss: 0.006240, acc.: 100.00%] [G loss: 3.664237]\n",
      "epoch:11 step:8773 [D loss: 0.015964, acc.: 100.00%] [G loss: 3.016561]\n",
      "epoch:11 step:8774 [D loss: 0.125786, acc.: 96.09%] [G loss: 2.888044]\n",
      "epoch:11 step:8775 [D loss: 0.174476, acc.: 96.09%] [G loss: 3.477856]\n",
      "epoch:11 step:8776 [D loss: 0.018249, acc.: 99.22%] [G loss: 4.038815]\n",
      "epoch:11 step:8777 [D loss: 0.111762, acc.: 96.88%] [G loss: 5.110390]\n",
      "epoch:11 step:8778 [D loss: 1.525533, acc.: 37.50%] [G loss: 6.796640]\n",
      "epoch:11 step:8779 [D loss: 0.725811, acc.: 62.50%] [G loss: 5.166638]\n",
      "epoch:11 step:8780 [D loss: 0.176462, acc.: 92.97%] [G loss: 3.306457]\n",
      "epoch:11 step:8781 [D loss: 0.161866, acc.: 93.75%] [G loss: 4.080085]\n",
      "epoch:11 step:8782 [D loss: 0.068138, acc.: 97.66%] [G loss: 4.164711]\n",
      "epoch:11 step:8783 [D loss: 0.049247, acc.: 100.00%] [G loss: 4.560064]\n",
      "epoch:11 step:8784 [D loss: 0.060878, acc.: 99.22%] [G loss: 4.800778]\n",
      "epoch:11 step:8785 [D loss: 0.048581, acc.: 100.00%] [G loss: 2.428682]\n",
      "epoch:11 step:8786 [D loss: 0.141190, acc.: 97.66%] [G loss: 3.515421]\n",
      "epoch:11 step:8787 [D loss: 0.172885, acc.: 96.88%] [G loss: 4.196002]\n",
      "epoch:11 step:8788 [D loss: 0.037946, acc.: 100.00%] [G loss: 2.084947]\n",
      "epoch:11 step:8789 [D loss: 0.186609, acc.: 92.97%] [G loss: 4.207498]\n",
      "epoch:11 step:8790 [D loss: 0.033657, acc.: 100.00%] [G loss: 2.696437]\n",
      "epoch:11 step:8791 [D loss: 0.099395, acc.: 98.44%] [G loss: 3.197845]\n",
      "epoch:11 step:8792 [D loss: 0.124306, acc.: 96.88%] [G loss: 3.962048]\n",
      "epoch:11 step:8793 [D loss: 0.044128, acc.: 100.00%] [G loss: 2.963750]\n",
      "epoch:11 step:8794 [D loss: 0.029534, acc.: 100.00%] [G loss: 3.925763]\n",
      "epoch:11 step:8795 [D loss: 0.055254, acc.: 100.00%] [G loss: 3.254283]\n",
      "epoch:11 step:8796 [D loss: 0.057123, acc.: 99.22%] [G loss: 4.139482]\n",
      "epoch:11 step:8797 [D loss: 0.083279, acc.: 98.44%] [G loss: 3.359236]\n",
      "epoch:11 step:8798 [D loss: 0.039480, acc.: 100.00%] [G loss: 3.468083]\n",
      "epoch:11 step:8799 [D loss: 0.073491, acc.: 98.44%] [G loss: 4.558736]\n",
      "epoch:11 step:8800 [D loss: 0.223804, acc.: 92.19%] [G loss: 2.167468]\n",
      "epoch:11 step:8801 [D loss: 0.156210, acc.: 91.41%] [G loss: 5.577504]\n",
      "epoch:11 step:8802 [D loss: 0.013439, acc.: 100.00%] [G loss: 5.146765]\n",
      "epoch:11 step:8803 [D loss: 0.294572, acc.: 84.38%] [G loss: 2.461817]\n",
      "epoch:11 step:8804 [D loss: 0.935175, acc.: 60.94%] [G loss: 8.414394]\n",
      "epoch:11 step:8805 [D loss: 1.901501, acc.: 50.00%] [G loss: 4.067896]\n",
      "epoch:11 step:8806 [D loss: 0.607929, acc.: 67.19%] [G loss: 1.170357]\n",
      "epoch:11 step:8807 [D loss: 0.131367, acc.: 96.88%] [G loss: 3.053650]\n",
      "epoch:11 step:8808 [D loss: 0.068874, acc.: 98.44%] [G loss: 3.776944]\n",
      "epoch:11 step:8809 [D loss: 0.067221, acc.: 99.22%] [G loss: 3.525034]\n",
      "epoch:11 step:8810 [D loss: 0.034916, acc.: 100.00%] [G loss: 4.607729]\n",
      "epoch:11 step:8811 [D loss: 0.098546, acc.: 99.22%] [G loss: 2.960264]\n",
      "epoch:11 step:8812 [D loss: 0.017020, acc.: 100.00%] [G loss: 4.041644]\n",
      "epoch:11 step:8813 [D loss: 0.097299, acc.: 98.44%] [G loss: 3.811695]\n",
      "epoch:11 step:8814 [D loss: 0.096206, acc.: 97.66%] [G loss: 3.045767]\n",
      "epoch:11 step:8815 [D loss: 0.106467, acc.: 98.44%] [G loss: 3.989480]\n",
      "epoch:11 step:8816 [D loss: 0.028880, acc.: 100.00%] [G loss: 3.971138]\n",
      "epoch:11 step:8817 [D loss: 0.082990, acc.: 98.44%] [G loss: 4.203953]\n",
      "epoch:11 step:8818 [D loss: 0.032562, acc.: 99.22%] [G loss: 4.204934]\n",
      "epoch:11 step:8819 [D loss: 0.092192, acc.: 98.44%] [G loss: 3.337889]\n",
      "epoch:11 step:8820 [D loss: 0.134303, acc.: 98.44%] [G loss: 2.919537]\n",
      "epoch:11 step:8821 [D loss: 0.043941, acc.: 100.00%] [G loss: 3.986557]\n",
      "epoch:11 step:8822 [D loss: 0.038198, acc.: 100.00%] [G loss: 3.682899]\n",
      "epoch:11 step:8823 [D loss: 0.150297, acc.: 98.44%] [G loss: 2.212502]\n",
      "epoch:11 step:8824 [D loss: 0.149297, acc.: 95.31%] [G loss: 3.472689]\n",
      "epoch:11 step:8825 [D loss: 0.125368, acc.: 99.22%] [G loss: 4.301182]\n",
      "epoch:11 step:8826 [D loss: 0.011183, acc.: 100.00%] [G loss: 4.117355]\n",
      "epoch:11 step:8827 [D loss: 0.095462, acc.: 99.22%] [G loss: 3.302179]\n",
      "epoch:11 step:8828 [D loss: 0.033441, acc.: 100.00%] [G loss: 3.967397]\n",
      "epoch:11 step:8829 [D loss: 0.072377, acc.: 99.22%] [G loss: 3.825842]\n",
      "epoch:11 step:8830 [D loss: 0.045123, acc.: 100.00%] [G loss: 4.276982]\n",
      "epoch:11 step:8831 [D loss: 1.811076, acc.: 33.59%] [G loss: 6.143199]\n",
      "epoch:11 step:8832 [D loss: 1.609559, acc.: 50.78%] [G loss: 4.373992]\n",
      "epoch:11 step:8833 [D loss: 0.144008, acc.: 95.31%] [G loss: 2.817092]\n",
      "epoch:11 step:8834 [D loss: 0.122620, acc.: 95.31%] [G loss: 3.284240]\n",
      "epoch:11 step:8835 [D loss: 0.063223, acc.: 100.00%] [G loss: 4.008881]\n",
      "epoch:11 step:8836 [D loss: 0.044569, acc.: 100.00%] [G loss: 4.033331]\n",
      "epoch:11 step:8837 [D loss: 0.268296, acc.: 88.28%] [G loss: 3.112605]\n",
      "epoch:11 step:8838 [D loss: 0.077133, acc.: 99.22%] [G loss: 3.278953]\n",
      "epoch:11 step:8839 [D loss: 0.045458, acc.: 99.22%] [G loss: 2.774260]\n",
      "epoch:11 step:8840 [D loss: 0.049534, acc.: 100.00%] [G loss: 3.688850]\n",
      "epoch:11 step:8841 [D loss: 0.451524, acc.: 78.91%] [G loss: 5.455997]\n",
      "epoch:11 step:8842 [D loss: 0.115729, acc.: 97.66%] [G loss: 5.298780]\n",
      "epoch:11 step:8843 [D loss: 0.476399, acc.: 77.34%] [G loss: 2.281377]\n",
      "epoch:11 step:8844 [D loss: 0.305392, acc.: 88.28%] [G loss: 4.351798]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8845 [D loss: 0.086130, acc.: 98.44%] [G loss: 4.349607]\n",
      "epoch:11 step:8846 [D loss: 0.409217, acc.: 82.03%] [G loss: 3.377918]\n",
      "epoch:11 step:8847 [D loss: 0.047382, acc.: 100.00%] [G loss: 2.666645]\n",
      "epoch:11 step:8848 [D loss: 0.085700, acc.: 99.22%] [G loss: 3.391993]\n",
      "epoch:11 step:8849 [D loss: 0.039499, acc.: 100.00%] [G loss: 3.178712]\n",
      "epoch:11 step:8850 [D loss: 0.135725, acc.: 96.09%] [G loss: 5.089989]\n",
      "epoch:11 step:8851 [D loss: 0.108167, acc.: 95.31%] [G loss: 2.847584]\n",
      "epoch:11 step:8852 [D loss: 0.065407, acc.: 99.22%] [G loss: 3.668438]\n",
      "epoch:11 step:8853 [D loss: 0.062325, acc.: 99.22%] [G loss: 4.787799]\n",
      "epoch:11 step:8854 [D loss: 0.056391, acc.: 99.22%] [G loss: 3.780714]\n",
      "epoch:11 step:8855 [D loss: 0.360790, acc.: 84.38%] [G loss: 5.098583]\n",
      "epoch:11 step:8856 [D loss: 0.142222, acc.: 94.53%] [G loss: 3.405064]\n",
      "epoch:11 step:8857 [D loss: 0.124071, acc.: 97.66%] [G loss: 3.178592]\n",
      "epoch:11 step:8858 [D loss: 0.073920, acc.: 99.22%] [G loss: 2.783322]\n",
      "epoch:11 step:8859 [D loss: 0.014150, acc.: 100.00%] [G loss: 2.672460]\n",
      "epoch:11 step:8860 [D loss: 0.089547, acc.: 98.44%] [G loss: 3.659390]\n",
      "epoch:11 step:8861 [D loss: 0.038198, acc.: 100.00%] [G loss: 3.558619]\n",
      "epoch:11 step:8862 [D loss: 0.049449, acc.: 100.00%] [G loss: 2.650524]\n",
      "epoch:11 step:8863 [D loss: 0.043009, acc.: 100.00%] [G loss: 3.005395]\n",
      "epoch:11 step:8864 [D loss: 0.054024, acc.: 99.22%] [G loss: 4.103775]\n",
      "epoch:11 step:8865 [D loss: 0.050904, acc.: 99.22%] [G loss: 2.456246]\n",
      "epoch:11 step:8866 [D loss: 0.123629, acc.: 96.09%] [G loss: 1.100651]\n",
      "epoch:11 step:8867 [D loss: 0.121088, acc.: 96.88%] [G loss: 2.215563]\n",
      "epoch:11 step:8868 [D loss: 0.102010, acc.: 96.09%] [G loss: 2.881105]\n",
      "epoch:11 step:8869 [D loss: 0.038804, acc.: 100.00%] [G loss: 2.093040]\n",
      "epoch:11 step:8870 [D loss: 0.035834, acc.: 100.00%] [G loss: 3.456668]\n",
      "epoch:11 step:8871 [D loss: 0.084312, acc.: 99.22%] [G loss: 3.808555]\n",
      "epoch:11 step:8872 [D loss: 0.036871, acc.: 100.00%] [G loss: 3.629766]\n",
      "epoch:11 step:8873 [D loss: 0.042533, acc.: 100.00%] [G loss: 2.572733]\n",
      "epoch:11 step:8874 [D loss: 0.174076, acc.: 92.19%] [G loss: 3.899490]\n",
      "epoch:11 step:8875 [D loss: 0.159055, acc.: 93.75%] [G loss: 3.632827]\n",
      "epoch:11 step:8876 [D loss: 0.054484, acc.: 99.22%] [G loss: 3.792926]\n",
      "epoch:11 step:8877 [D loss: 0.355785, acc.: 84.38%] [G loss: 5.187212]\n",
      "epoch:11 step:8878 [D loss: 0.116750, acc.: 95.31%] [G loss: 5.402004]\n",
      "epoch:11 step:8879 [D loss: 0.033732, acc.: 100.00%] [G loss: 3.759185]\n",
      "epoch:11 step:8880 [D loss: 0.048760, acc.: 99.22%] [G loss: 2.516312]\n",
      "epoch:11 step:8881 [D loss: 0.034778, acc.: 100.00%] [G loss: 3.634478]\n",
      "epoch:11 step:8882 [D loss: 0.522243, acc.: 70.31%] [G loss: 6.578164]\n",
      "epoch:11 step:8883 [D loss: 0.377264, acc.: 83.59%] [G loss: 5.108074]\n",
      "epoch:11 step:8884 [D loss: 0.017212, acc.: 100.00%] [G loss: 3.804854]\n",
      "epoch:11 step:8885 [D loss: 0.019053, acc.: 100.00%] [G loss: 4.941775]\n",
      "epoch:11 step:8886 [D loss: 0.289697, acc.: 89.06%] [G loss: 6.240704]\n",
      "epoch:11 step:8887 [D loss: 0.080407, acc.: 96.09%] [G loss: 6.712920]\n",
      "epoch:11 step:8888 [D loss: 0.106448, acc.: 97.66%] [G loss: 4.449458]\n",
      "epoch:11 step:8889 [D loss: 0.045974, acc.: 99.22%] [G loss: 4.222312]\n",
      "epoch:11 step:8890 [D loss: 0.035555, acc.: 100.00%] [G loss: 4.448748]\n",
      "epoch:11 step:8891 [D loss: 0.032509, acc.: 100.00%] [G loss: 3.657597]\n",
      "epoch:11 step:8892 [D loss: 0.010963, acc.: 100.00%] [G loss: 5.325703]\n",
      "epoch:11 step:8893 [D loss: 0.062451, acc.: 98.44%] [G loss: 2.489474]\n",
      "epoch:11 step:8894 [D loss: 0.069975, acc.: 97.66%] [G loss: 4.517892]\n",
      "epoch:11 step:8895 [D loss: 0.073434, acc.: 98.44%] [G loss: 4.833371]\n",
      "epoch:11 step:8896 [D loss: 0.222242, acc.: 90.62%] [G loss: 4.217546]\n",
      "epoch:11 step:8897 [D loss: 0.060409, acc.: 100.00%] [G loss: 4.387585]\n",
      "epoch:11 step:8898 [D loss: 0.060781, acc.: 98.44%] [G loss: 4.166365]\n",
      "epoch:11 step:8899 [D loss: 0.073631, acc.: 96.88%] [G loss: 5.455857]\n",
      "epoch:11 step:8900 [D loss: 0.390144, acc.: 82.03%] [G loss: 4.995253]\n",
      "epoch:11 step:8901 [D loss: 0.085049, acc.: 98.44%] [G loss: 5.245664]\n",
      "epoch:11 step:8902 [D loss: 0.020165, acc.: 100.00%] [G loss: 5.652584]\n",
      "epoch:11 step:8903 [D loss: 1.630314, acc.: 42.97%] [G loss: 9.199759]\n",
      "epoch:11 step:8904 [D loss: 2.812688, acc.: 50.00%] [G loss: 5.251693]\n",
      "epoch:11 step:8905 [D loss: 0.702396, acc.: 63.28%] [G loss: 2.061923]\n",
      "epoch:11 step:8906 [D loss: 0.162532, acc.: 96.88%] [G loss: 1.547463]\n",
      "epoch:11 step:8907 [D loss: 0.146739, acc.: 97.66%] [G loss: 2.469609]\n",
      "epoch:11 step:8908 [D loss: 0.117549, acc.: 97.66%] [G loss: 2.989545]\n",
      "epoch:11 step:8909 [D loss: 0.259156, acc.: 89.84%] [G loss: 2.737110]\n",
      "epoch:11 step:8910 [D loss: 0.284305, acc.: 89.84%] [G loss: 2.143039]\n",
      "epoch:11 step:8911 [D loss: 0.184308, acc.: 93.75%] [G loss: 2.378201]\n",
      "epoch:11 step:8912 [D loss: 0.156173, acc.: 95.31%] [G loss: 2.531927]\n",
      "epoch:11 step:8913 [D loss: 0.276846, acc.: 89.84%] [G loss: 4.197456]\n",
      "epoch:11 step:8914 [D loss: 0.356801, acc.: 84.38%] [G loss: 2.802375]\n",
      "epoch:11 step:8915 [D loss: 0.137104, acc.: 97.66%] [G loss: 2.439885]\n",
      "epoch:11 step:8916 [D loss: 0.307736, acc.: 89.06%] [G loss: 4.942931]\n",
      "epoch:11 step:8917 [D loss: 0.422448, acc.: 79.69%] [G loss: 2.807472]\n",
      "epoch:11 step:8918 [D loss: 0.331907, acc.: 86.72%] [G loss: 4.039552]\n",
      "epoch:11 step:8919 [D loss: 0.145689, acc.: 96.09%] [G loss: 3.578115]\n",
      "epoch:11 step:8920 [D loss: 0.097024, acc.: 96.09%] [G loss: 1.995016]\n",
      "epoch:11 step:8921 [D loss: 0.160573, acc.: 95.31%] [G loss: 2.459559]\n",
      "epoch:11 step:8922 [D loss: 0.228763, acc.: 91.41%] [G loss: 2.695209]\n",
      "epoch:11 step:8923 [D loss: 0.100113, acc.: 97.66%] [G loss: 2.534096]\n",
      "epoch:11 step:8924 [D loss: 0.168251, acc.: 95.31%] [G loss: 2.113769]\n",
      "epoch:11 step:8925 [D loss: 0.218605, acc.: 92.97%] [G loss: 1.763318]\n",
      "epoch:11 step:8926 [D loss: 0.022279, acc.: 100.00%] [G loss: 0.958697]\n",
      "epoch:11 step:8927 [D loss: 0.072024, acc.: 97.66%] [G loss: 0.845297]\n",
      "epoch:11 step:8928 [D loss: 0.065766, acc.: 99.22%] [G loss: 0.430330]\n",
      "epoch:11 step:8929 [D loss: 0.190791, acc.: 93.75%] [G loss: 0.171114]\n",
      "epoch:11 step:8930 [D loss: 0.122303, acc.: 96.88%] [G loss: 0.479272]\n",
      "epoch:11 step:8931 [D loss: 0.149544, acc.: 94.53%] [G loss: 0.348389]\n",
      "epoch:11 step:8932 [D loss: 0.449349, acc.: 81.25%] [G loss: 3.980109]\n",
      "epoch:11 step:8933 [D loss: 1.011305, acc.: 58.59%] [G loss: 1.021721]\n",
      "epoch:11 step:8934 [D loss: 0.903286, acc.: 66.41%] [G loss: 6.064458]\n",
      "epoch:11 step:8935 [D loss: 0.280101, acc.: 85.16%] [G loss: 5.982073]\n",
      "epoch:11 step:8936 [D loss: 0.224562, acc.: 90.62%] [G loss: 2.814706]\n",
      "epoch:11 step:8937 [D loss: 0.076013, acc.: 97.66%] [G loss: 0.466295]\n",
      "epoch:11 step:8938 [D loss: 0.126431, acc.: 95.31%] [G loss: 0.916585]\n",
      "epoch:11 step:8939 [D loss: 0.067381, acc.: 98.44%] [G loss: 2.834230]\n",
      "epoch:11 step:8940 [D loss: 0.179996, acc.: 95.31%] [G loss: 2.884738]\n",
      "epoch:11 step:8941 [D loss: 0.851654, acc.: 54.69%] [G loss: 2.650882]\n",
      "epoch:11 step:8942 [D loss: 0.033973, acc.: 100.00%] [G loss: 3.464710]\n",
      "epoch:11 step:8943 [D loss: 0.343874, acc.: 83.59%] [G loss: 3.413460]\n",
      "epoch:11 step:8944 [D loss: 0.095455, acc.: 96.88%] [G loss: 3.516732]\n",
      "epoch:11 step:8945 [D loss: 0.264015, acc.: 89.06%] [G loss: 3.767864]\n",
      "epoch:11 step:8946 [D loss: 0.429770, acc.: 81.25%] [G loss: 3.864454]\n",
      "epoch:11 step:8947 [D loss: 0.081194, acc.: 96.09%] [G loss: 4.004232]\n",
      "epoch:11 step:8948 [D loss: 0.118126, acc.: 96.09%] [G loss: 3.707134]\n",
      "epoch:11 step:8949 [D loss: 0.433262, acc.: 81.25%] [G loss: 4.314078]\n",
      "epoch:11 step:8950 [D loss: 0.030541, acc.: 100.00%] [G loss: 4.559511]\n",
      "epoch:11 step:8951 [D loss: 0.090471, acc.: 96.09%] [G loss: 2.992489]\n",
      "epoch:11 step:8952 [D loss: 0.446408, acc.: 77.34%] [G loss: 2.605938]\n",
      "epoch:11 step:8953 [D loss: 0.035846, acc.: 99.22%] [G loss: 3.040516]\n",
      "epoch:11 step:8954 [D loss: 0.032176, acc.: 100.00%] [G loss: 1.772310]\n",
      "epoch:11 step:8955 [D loss: 0.157463, acc.: 95.31%] [G loss: 3.809154]\n",
      "epoch:11 step:8956 [D loss: 0.023033, acc.: 100.00%] [G loss: 3.492704]\n",
      "epoch:11 step:8957 [D loss: 0.073516, acc.: 96.88%] [G loss: 2.676485]\n",
      "epoch:11 step:8958 [D loss: 0.132240, acc.: 96.09%] [G loss: 2.409380]\n",
      "epoch:11 step:8959 [D loss: 0.032600, acc.: 100.00%] [G loss: 3.345837]\n",
      "epoch:11 step:8960 [D loss: 0.158678, acc.: 98.44%] [G loss: 4.000811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:8961 [D loss: 0.540672, acc.: 77.34%] [G loss: 1.831786]\n",
      "epoch:11 step:8962 [D loss: 0.021642, acc.: 100.00%] [G loss: 2.518056]\n",
      "epoch:11 step:8963 [D loss: 0.092116, acc.: 98.44%] [G loss: 2.205505]\n",
      "epoch:11 step:8964 [D loss: 1.926675, acc.: 29.69%] [G loss: 5.897378]\n",
      "epoch:11 step:8965 [D loss: 0.373632, acc.: 83.59%] [G loss: 5.996305]\n",
      "epoch:11 step:8966 [D loss: 0.577977, acc.: 71.09%] [G loss: 1.806284]\n",
      "epoch:11 step:8967 [D loss: 0.192454, acc.: 93.75%] [G loss: 1.712977]\n",
      "epoch:11 step:8968 [D loss: 0.123212, acc.: 93.75%] [G loss: 3.183143]\n",
      "epoch:11 step:8969 [D loss: 0.051402, acc.: 99.22%] [G loss: 2.622553]\n",
      "epoch:11 step:8970 [D loss: 0.274518, acc.: 88.28%] [G loss: 1.804530]\n",
      "epoch:11 step:8971 [D loss: 0.126545, acc.: 97.66%] [G loss: 4.359829]\n",
      "epoch:11 step:8972 [D loss: 0.056480, acc.: 100.00%] [G loss: 3.496017]\n",
      "epoch:11 step:8973 [D loss: 0.234264, acc.: 89.84%] [G loss: 2.462080]\n",
      "epoch:11 step:8974 [D loss: 0.078560, acc.: 98.44%] [G loss: 4.766208]\n",
      "epoch:11 step:8975 [D loss: 0.180680, acc.: 95.31%] [G loss: 3.984593]\n",
      "epoch:11 step:8976 [D loss: 0.155674, acc.: 93.75%] [G loss: 3.837523]\n",
      "epoch:11 step:8977 [D loss: 0.106432, acc.: 96.88%] [G loss: 3.648964]\n",
      "epoch:11 step:8978 [D loss: 0.052884, acc.: 100.00%] [G loss: 4.731130]\n",
      "epoch:11 step:8979 [D loss: 0.078792, acc.: 97.66%] [G loss: 2.794666]\n",
      "epoch:11 step:8980 [D loss: 0.101372, acc.: 97.66%] [G loss: 2.947448]\n",
      "epoch:11 step:8981 [D loss: 0.019087, acc.: 100.00%] [G loss: 3.905542]\n",
      "epoch:11 step:8982 [D loss: 0.115096, acc.: 96.88%] [G loss: 1.909378]\n",
      "epoch:11 step:8983 [D loss: 0.118700, acc.: 96.09%] [G loss: 4.274007]\n",
      "epoch:11 step:8984 [D loss: 0.113370, acc.: 96.09%] [G loss: 2.471032]\n",
      "epoch:11 step:8985 [D loss: 0.050973, acc.: 100.00%] [G loss: 1.790941]\n",
      "epoch:11 step:8986 [D loss: 0.064620, acc.: 99.22%] [G loss: 0.979256]\n",
      "epoch:11 step:8987 [D loss: 0.272641, acc.: 86.72%] [G loss: 3.787042]\n",
      "epoch:11 step:8988 [D loss: 0.140297, acc.: 93.75%] [G loss: 4.561271]\n",
      "epoch:11 step:8989 [D loss: 0.031378, acc.: 100.00%] [G loss: 2.160989]\n",
      "epoch:11 step:8990 [D loss: 0.019086, acc.: 100.00%] [G loss: 2.629288]\n",
      "epoch:11 step:8991 [D loss: 0.018946, acc.: 100.00%] [G loss: 2.021130]\n",
      "epoch:11 step:8992 [D loss: 0.099116, acc.: 97.66%] [G loss: 1.818235]\n",
      "epoch:11 step:8993 [D loss: 0.123370, acc.: 96.88%] [G loss: 3.113262]\n",
      "epoch:11 step:8994 [D loss: 0.238859, acc.: 89.84%] [G loss: 3.031150]\n",
      "epoch:11 step:8995 [D loss: 0.070525, acc.: 97.66%] [G loss: 3.009709]\n",
      "epoch:11 step:8996 [D loss: 0.162778, acc.: 93.75%] [G loss: 0.549309]\n",
      "epoch:11 step:8997 [D loss: 0.225470, acc.: 89.06%] [G loss: 4.214991]\n",
      "epoch:11 step:8998 [D loss: 0.005803, acc.: 100.00%] [G loss: 7.538943]\n",
      "epoch:11 step:8999 [D loss: 0.095199, acc.: 96.09%] [G loss: 4.222286]\n",
      "epoch:11 step:9000 [D loss: 0.072288, acc.: 97.66%] [G loss: 3.709099]\n",
      "epoch:11 step:9001 [D loss: 0.068353, acc.: 97.66%] [G loss: 2.138763]\n",
      "epoch:11 step:9002 [D loss: 0.339011, acc.: 84.38%] [G loss: 3.664805]\n",
      "epoch:11 step:9003 [D loss: 0.038001, acc.: 99.22%] [G loss: 5.319320]\n",
      "epoch:11 step:9004 [D loss: 0.336358, acc.: 81.25%] [G loss: 0.708389]\n",
      "epoch:11 step:9005 [D loss: 2.084143, acc.: 51.56%] [G loss: 7.456196]\n",
      "epoch:11 step:9006 [D loss: 2.313786, acc.: 50.00%] [G loss: 4.742243]\n",
      "epoch:11 step:9007 [D loss: 0.352245, acc.: 80.47%] [G loss: 1.560516]\n",
      "epoch:11 step:9008 [D loss: 0.256437, acc.: 90.62%] [G loss: 1.599270]\n",
      "epoch:11 step:9009 [D loss: 0.084076, acc.: 99.22%] [G loss: 2.776512]\n",
      "epoch:11 step:9010 [D loss: 0.237451, acc.: 92.97%] [G loss: 2.530870]\n",
      "epoch:11 step:9011 [D loss: 0.364948, acc.: 82.81%] [G loss: 2.733051]\n",
      "epoch:11 step:9012 [D loss: 0.085612, acc.: 99.22%] [G loss: 2.612309]\n",
      "epoch:11 step:9013 [D loss: 0.668306, acc.: 61.72%] [G loss: 4.217634]\n",
      "epoch:11 step:9014 [D loss: 0.105957, acc.: 96.09%] [G loss: 5.328494]\n",
      "epoch:11 step:9015 [D loss: 0.347445, acc.: 80.47%] [G loss: 2.034308]\n",
      "epoch:11 step:9016 [D loss: 0.507558, acc.: 72.66%] [G loss: 5.208867]\n",
      "epoch:11 step:9017 [D loss: 0.089125, acc.: 96.88%] [G loss: 5.429111]\n",
      "epoch:11 step:9018 [D loss: 0.143490, acc.: 93.75%] [G loss: 4.897084]\n",
      "epoch:11 step:9019 [D loss: 0.331604, acc.: 84.38%] [G loss: 3.581843]\n",
      "epoch:11 step:9020 [D loss: 0.094545, acc.: 96.09%] [G loss: 3.672067]\n",
      "epoch:11 step:9021 [D loss: 0.064256, acc.: 98.44%] [G loss: 4.028526]\n",
      "epoch:11 step:9022 [D loss: 0.136517, acc.: 96.09%] [G loss: 3.112077]\n",
      "epoch:11 step:9023 [D loss: 0.060911, acc.: 99.22%] [G loss: 3.517396]\n",
      "epoch:11 step:9024 [D loss: 0.010735, acc.: 100.00%] [G loss: 3.109089]\n",
      "epoch:11 step:9025 [D loss: 0.028683, acc.: 100.00%] [G loss: 3.215302]\n",
      "epoch:11 step:9026 [D loss: 0.136248, acc.: 96.09%] [G loss: 3.756481]\n",
      "epoch:11 step:9027 [D loss: 0.042469, acc.: 100.00%] [G loss: 2.457861]\n",
      "epoch:11 step:9028 [D loss: 0.133149, acc.: 97.66%] [G loss: 3.875209]\n",
      "epoch:11 step:9029 [D loss: 0.036661, acc.: 100.00%] [G loss: 4.003582]\n",
      "epoch:11 step:9030 [D loss: 0.069979, acc.: 99.22%] [G loss: 1.159973]\n",
      "epoch:11 step:9031 [D loss: 0.177329, acc.: 94.53%] [G loss: 4.278890]\n",
      "epoch:11 step:9032 [D loss: 0.111712, acc.: 94.53%] [G loss: 3.571852]\n",
      "epoch:11 step:9033 [D loss: 0.029408, acc.: 100.00%] [G loss: 2.908152]\n",
      "epoch:11 step:9034 [D loss: 0.061469, acc.: 100.00%] [G loss: 2.403927]\n",
      "epoch:11 step:9035 [D loss: 0.053074, acc.: 99.22%] [G loss: 1.552483]\n",
      "epoch:11 step:9036 [D loss: 0.038437, acc.: 100.00%] [G loss: 2.610763]\n",
      "epoch:11 step:9037 [D loss: 0.133356, acc.: 96.09%] [G loss: 3.814408]\n",
      "epoch:11 step:9038 [D loss: 0.794732, acc.: 61.72%] [G loss: 1.616079]\n",
      "epoch:11 step:9039 [D loss: 0.127557, acc.: 96.88%] [G loss: 2.559510]\n",
      "epoch:11 step:9040 [D loss: 0.121957, acc.: 96.88%] [G loss: 1.824707]\n",
      "epoch:11 step:9041 [D loss: 0.146345, acc.: 94.53%] [G loss: 4.896041]\n",
      "epoch:11 step:9042 [D loss: 0.258272, acc.: 89.06%] [G loss: 0.882359]\n",
      "epoch:11 step:9043 [D loss: 0.138819, acc.: 93.75%] [G loss: 2.902344]\n",
      "epoch:11 step:9044 [D loss: 0.036625, acc.: 99.22%] [G loss: 3.751906]\n",
      "epoch:11 step:9045 [D loss: 0.118316, acc.: 97.66%] [G loss: 3.786997]\n",
      "epoch:11 step:9046 [D loss: 0.297578, acc.: 90.62%] [G loss: 4.863542]\n",
      "epoch:11 step:9047 [D loss: 0.096812, acc.: 98.44%] [G loss: 3.747635]\n",
      "epoch:11 step:9048 [D loss: 0.044308, acc.: 100.00%] [G loss: 4.542977]\n",
      "epoch:11 step:9049 [D loss: 0.039665, acc.: 100.00%] [G loss: 4.237521]\n",
      "epoch:11 step:9050 [D loss: 0.140123, acc.: 96.09%] [G loss: 4.381715]\n",
      "epoch:11 step:9051 [D loss: 0.199337, acc.: 92.97%] [G loss: 2.790848]\n",
      "epoch:11 step:9052 [D loss: 0.174176, acc.: 92.97%] [G loss: 5.509303]\n",
      "epoch:11 step:9053 [D loss: 0.069444, acc.: 97.66%] [G loss: 5.941607]\n",
      "epoch:11 step:9054 [D loss: 0.036031, acc.: 99.22%] [G loss: 4.692910]\n",
      "epoch:11 step:9055 [D loss: 0.019589, acc.: 100.00%] [G loss: 4.369666]\n",
      "epoch:11 step:9056 [D loss: 0.044065, acc.: 100.00%] [G loss: 4.018620]\n",
      "epoch:11 step:9057 [D loss: 0.026794, acc.: 100.00%] [G loss: 4.378213]\n",
      "epoch:11 step:9058 [D loss: 0.101922, acc.: 100.00%] [G loss: 3.684130]\n",
      "epoch:11 step:9059 [D loss: 0.085090, acc.: 98.44%] [G loss: 3.391959]\n",
      "epoch:11 step:9060 [D loss: 0.010598, acc.: 100.00%] [G loss: 3.725297]\n",
      "epoch:11 step:9061 [D loss: 0.084898, acc.: 98.44%] [G loss: 2.214532]\n",
      "epoch:11 step:9062 [D loss: 0.077009, acc.: 97.66%] [G loss: 3.096920]\n",
      "epoch:11 step:9063 [D loss: 0.056708, acc.: 100.00%] [G loss: 3.887231]\n",
      "epoch:11 step:9064 [D loss: 0.075201, acc.: 99.22%] [G loss: 5.283282]\n",
      "epoch:11 step:9065 [D loss: 1.840464, acc.: 28.91%] [G loss: 7.621546]\n",
      "epoch:11 step:9066 [D loss: 2.430673, acc.: 50.78%] [G loss: 5.946443]\n",
      "epoch:11 step:9067 [D loss: 0.701737, acc.: 61.72%] [G loss: 1.780299]\n",
      "epoch:11 step:9068 [D loss: 0.116882, acc.: 96.09%] [G loss: 2.259820]\n",
      "epoch:11 step:9069 [D loss: 0.065840, acc.: 99.22%] [G loss: 1.845715]\n",
      "epoch:11 step:9070 [D loss: 0.104572, acc.: 97.66%] [G loss: 3.470185]\n",
      "epoch:11 step:9071 [D loss: 0.043653, acc.: 100.00%] [G loss: 4.310832]\n",
      "epoch:11 step:9072 [D loss: 0.099295, acc.: 97.66%] [G loss: 3.442693]\n",
      "epoch:11 step:9073 [D loss: 0.132063, acc.: 96.09%] [G loss: 2.493364]\n",
      "epoch:11 step:9074 [D loss: 0.081171, acc.: 99.22%] [G loss: 1.785531]\n",
      "epoch:11 step:9075 [D loss: 0.135653, acc.: 96.09%] [G loss: 1.103468]\n",
      "epoch:11 step:9076 [D loss: 0.467815, acc.: 76.56%] [G loss: 4.616599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9077 [D loss: 0.937962, acc.: 57.81%] [G loss: 1.714938]\n",
      "epoch:11 step:9078 [D loss: 0.245132, acc.: 91.41%] [G loss: 3.054456]\n",
      "epoch:11 step:9079 [D loss: 0.162130, acc.: 93.75%] [G loss: 1.714314]\n",
      "epoch:11 step:9080 [D loss: 0.147199, acc.: 96.88%] [G loss: 2.585292]\n",
      "epoch:11 step:9081 [D loss: 0.268401, acc.: 89.84%] [G loss: 1.737388]\n",
      "epoch:11 step:9082 [D loss: 0.343507, acc.: 83.59%] [G loss: 1.554820]\n",
      "epoch:11 step:9083 [D loss: 0.404031, acc.: 85.16%] [G loss: 4.859803]\n",
      "epoch:11 step:9084 [D loss: 0.185279, acc.: 89.84%] [G loss: 1.652556]\n",
      "epoch:11 step:9085 [D loss: 0.072806, acc.: 97.66%] [G loss: 2.385814]\n",
      "epoch:11 step:9086 [D loss: 0.139337, acc.: 97.66%] [G loss: 3.538167]\n",
      "epoch:11 step:9087 [D loss: 0.076488, acc.: 99.22%] [G loss: 2.969644]\n",
      "epoch:11 step:9088 [D loss: 0.125562, acc.: 96.88%] [G loss: 3.440623]\n",
      "epoch:11 step:9089 [D loss: 0.128778, acc.: 98.44%] [G loss: 1.960722]\n",
      "epoch:11 step:9090 [D loss: 0.269227, acc.: 89.84%] [G loss: 4.230505]\n",
      "epoch:11 step:9091 [D loss: 0.258581, acc.: 88.28%] [G loss: 0.678865]\n",
      "epoch:11 step:9092 [D loss: 0.115843, acc.: 97.66%] [G loss: 2.633720]\n",
      "epoch:11 step:9093 [D loss: 0.037357, acc.: 100.00%] [G loss: 5.375772]\n",
      "epoch:11 step:9094 [D loss: 0.030602, acc.: 100.00%] [G loss: 3.627281]\n",
      "epoch:11 step:9095 [D loss: 0.065866, acc.: 98.44%] [G loss: 1.340693]\n",
      "epoch:11 step:9096 [D loss: 0.077986, acc.: 100.00%] [G loss: 0.942679]\n",
      "epoch:11 step:9097 [D loss: 0.088175, acc.: 99.22%] [G loss: 2.952959]\n",
      "epoch:11 step:9098 [D loss: 0.175807, acc.: 95.31%] [G loss: 1.831848]\n",
      "epoch:11 step:9099 [D loss: 0.095944, acc.: 99.22%] [G loss: 2.595621]\n",
      "epoch:11 step:9100 [D loss: 1.473782, acc.: 39.84%] [G loss: 7.580155]\n",
      "epoch:11 step:9101 [D loss: 1.088954, acc.: 58.59%] [G loss: 5.070449]\n",
      "epoch:11 step:9102 [D loss: 0.181617, acc.: 91.41%] [G loss: 3.225802]\n",
      "epoch:11 step:9103 [D loss: 0.228738, acc.: 92.97%] [G loss: 3.169194]\n",
      "epoch:11 step:9104 [D loss: 0.041708, acc.: 100.00%] [G loss: 4.515141]\n",
      "epoch:11 step:9105 [D loss: 0.047137, acc.: 98.44%] [G loss: 3.957795]\n",
      "epoch:11 step:9106 [D loss: 0.042249, acc.: 100.00%] [G loss: 4.030153]\n",
      "epoch:11 step:9107 [D loss: 0.039675, acc.: 100.00%] [G loss: 3.187675]\n",
      "epoch:11 step:9108 [D loss: 0.036380, acc.: 100.00%] [G loss: 3.281564]\n",
      "epoch:11 step:9109 [D loss: 0.055995, acc.: 100.00%] [G loss: 3.834098]\n",
      "epoch:11 step:9110 [D loss: 0.043462, acc.: 100.00%] [G loss: 3.451591]\n",
      "epoch:11 step:9111 [D loss: 0.063727, acc.: 100.00%] [G loss: 3.582329]\n",
      "epoch:11 step:9112 [D loss: 0.097813, acc.: 99.22%] [G loss: 3.108256]\n",
      "epoch:11 step:9113 [D loss: 0.113061, acc.: 98.44%] [G loss: 3.611250]\n",
      "epoch:11 step:9114 [D loss: 0.091729, acc.: 97.66%] [G loss: 3.600952]\n",
      "epoch:11 step:9115 [D loss: 0.137154, acc.: 96.88%] [G loss: 2.432097]\n",
      "epoch:11 step:9116 [D loss: 0.077141, acc.: 99.22%] [G loss: 2.820348]\n",
      "epoch:11 step:9117 [D loss: 0.289775, acc.: 89.84%] [G loss: 2.867601]\n",
      "epoch:11 step:9118 [D loss: 0.016395, acc.: 100.00%] [G loss: 3.949631]\n",
      "epoch:11 step:9119 [D loss: 0.025138, acc.: 100.00%] [G loss: 3.887203]\n",
      "epoch:11 step:9120 [D loss: 0.038991, acc.: 100.00%] [G loss: 3.867723]\n",
      "epoch:11 step:9121 [D loss: 0.055473, acc.: 99.22%] [G loss: 3.933908]\n",
      "epoch:11 step:9122 [D loss: 0.123064, acc.: 99.22%] [G loss: 2.781265]\n",
      "epoch:11 step:9123 [D loss: 0.286488, acc.: 88.28%] [G loss: 3.501812]\n",
      "epoch:11 step:9124 [D loss: 0.053814, acc.: 99.22%] [G loss: 3.810567]\n",
      "epoch:11 step:9125 [D loss: 0.084216, acc.: 98.44%] [G loss: 3.833866]\n",
      "epoch:11 step:9126 [D loss: 0.024221, acc.: 100.00%] [G loss: 3.750165]\n",
      "epoch:11 step:9127 [D loss: 0.090222, acc.: 98.44%] [G loss: 1.854343]\n",
      "epoch:11 step:9128 [D loss: 0.172270, acc.: 97.66%] [G loss: 1.967808]\n",
      "epoch:11 step:9129 [D loss: 0.161338, acc.: 94.53%] [G loss: 5.904529]\n",
      "epoch:11 step:9130 [D loss: 0.127765, acc.: 95.31%] [G loss: 3.272384]\n",
      "epoch:11 step:9131 [D loss: 0.021198, acc.: 100.00%] [G loss: 2.475861]\n",
      "epoch:11 step:9132 [D loss: 0.046831, acc.: 99.22%] [G loss: 2.384596]\n",
      "epoch:11 step:9133 [D loss: 0.472736, acc.: 78.91%] [G loss: 4.832081]\n",
      "epoch:11 step:9134 [D loss: 0.342945, acc.: 84.38%] [G loss: 5.050089]\n",
      "epoch:11 step:9135 [D loss: 0.025680, acc.: 100.00%] [G loss: 4.780770]\n",
      "epoch:11 step:9136 [D loss: 0.074753, acc.: 100.00%] [G loss: 3.075163]\n",
      "epoch:11 step:9137 [D loss: 0.015379, acc.: 100.00%] [G loss: 2.229193]\n",
      "epoch:11 step:9138 [D loss: 0.216634, acc.: 92.19%] [G loss: 3.345973]\n",
      "epoch:11 step:9139 [D loss: 0.120606, acc.: 95.31%] [G loss: 4.494370]\n",
      "epoch:11 step:9140 [D loss: 0.028812, acc.: 100.00%] [G loss: 4.625749]\n",
      "epoch:11 step:9141 [D loss: 0.130714, acc.: 97.66%] [G loss: 3.944081]\n",
      "epoch:11 step:9142 [D loss: 0.032635, acc.: 100.00%] [G loss: 3.467230]\n",
      "epoch:11 step:9143 [D loss: 0.717121, acc.: 61.72%] [G loss: 6.490571]\n",
      "epoch:11 step:9144 [D loss: 0.579413, acc.: 73.44%] [G loss: 3.787466]\n",
      "epoch:11 step:9145 [D loss: 0.038893, acc.: 100.00%] [G loss: 2.806153]\n",
      "epoch:11 step:9146 [D loss: 0.042260, acc.: 100.00%] [G loss: 4.079624]\n",
      "epoch:11 step:9147 [D loss: 0.023159, acc.: 100.00%] [G loss: 3.942351]\n",
      "epoch:11 step:9148 [D loss: 0.031392, acc.: 100.00%] [G loss: 3.343241]\n",
      "epoch:11 step:9149 [D loss: 0.307307, acc.: 86.72%] [G loss: 4.973392]\n",
      "epoch:11 step:9150 [D loss: 0.107174, acc.: 93.75%] [G loss: 4.224727]\n",
      "epoch:11 step:9151 [D loss: 0.029159, acc.: 100.00%] [G loss: 3.292096]\n",
      "epoch:11 step:9152 [D loss: 0.087572, acc.: 99.22%] [G loss: 4.094587]\n",
      "epoch:11 step:9153 [D loss: 0.123925, acc.: 93.75%] [G loss: 1.983740]\n",
      "epoch:11 step:9154 [D loss: 0.686611, acc.: 64.84%] [G loss: 6.505730]\n",
      "epoch:11 step:9155 [D loss: 1.295688, acc.: 55.47%] [G loss: 2.453655]\n",
      "epoch:11 step:9156 [D loss: 0.505525, acc.: 73.44%] [G loss: 4.117020]\n",
      "epoch:11 step:9157 [D loss: 0.494380, acc.: 79.69%] [G loss: 2.390431]\n",
      "epoch:11 step:9158 [D loss: 0.203999, acc.: 93.75%] [G loss: 3.757168]\n",
      "epoch:11 step:9159 [D loss: 0.059461, acc.: 98.44%] [G loss: 3.057854]\n",
      "epoch:11 step:9160 [D loss: 0.058505, acc.: 100.00%] [G loss: 3.729599]\n",
      "epoch:11 step:9161 [D loss: 0.062117, acc.: 100.00%] [G loss: 3.241123]\n",
      "epoch:11 step:9162 [D loss: 0.133792, acc.: 98.44%] [G loss: 4.123384]\n",
      "epoch:11 step:9163 [D loss: 0.267398, acc.: 89.84%] [G loss: 4.550965]\n",
      "epoch:11 step:9164 [D loss: 0.059168, acc.: 100.00%] [G loss: 2.517578]\n",
      "epoch:11 step:9165 [D loss: 0.042466, acc.: 100.00%] [G loss: 3.519506]\n",
      "epoch:11 step:9166 [D loss: 0.174834, acc.: 92.97%] [G loss: 2.567260]\n",
      "epoch:11 step:9167 [D loss: 0.090664, acc.: 100.00%] [G loss: 2.397070]\n",
      "epoch:11 step:9168 [D loss: 0.101985, acc.: 96.88%] [G loss: 3.868249]\n",
      "epoch:11 step:9169 [D loss: 0.017713, acc.: 100.00%] [G loss: 3.493057]\n",
      "epoch:11 step:9170 [D loss: 0.251141, acc.: 91.41%] [G loss: 4.110832]\n",
      "epoch:11 step:9171 [D loss: 0.031666, acc.: 100.00%] [G loss: 3.795433]\n",
      "epoch:11 step:9172 [D loss: 0.112389, acc.: 98.44%] [G loss: 2.281571]\n",
      "epoch:11 step:9173 [D loss: 0.110055, acc.: 96.09%] [G loss: 5.258131]\n",
      "epoch:11 step:9174 [D loss: 0.022498, acc.: 100.00%] [G loss: 4.501965]\n",
      "epoch:11 step:9175 [D loss: 0.462254, acc.: 76.56%] [G loss: 4.012249]\n",
      "epoch:11 step:9176 [D loss: 0.013965, acc.: 100.00%] [G loss: 5.084818]\n",
      "epoch:11 step:9177 [D loss: 0.546998, acc.: 68.75%] [G loss: 3.855879]\n",
      "epoch:11 step:9178 [D loss: 0.016905, acc.: 100.00%] [G loss: 3.790116]\n",
      "epoch:11 step:9179 [D loss: 0.022480, acc.: 100.00%] [G loss: 4.824131]\n",
      "epoch:11 step:9180 [D loss: 0.100119, acc.: 97.66%] [G loss: 4.759756]\n",
      "epoch:11 step:9181 [D loss: 0.064737, acc.: 100.00%] [G loss: 4.044162]\n",
      "epoch:11 step:9182 [D loss: 0.076550, acc.: 100.00%] [G loss: 5.005379]\n",
      "epoch:11 step:9183 [D loss: 0.047479, acc.: 99.22%] [G loss: 5.197260]\n",
      "epoch:11 step:9184 [D loss: 0.356414, acc.: 85.16%] [G loss: 4.899470]\n",
      "epoch:11 step:9185 [D loss: 0.050718, acc.: 97.66%] [G loss: 5.231444]\n",
      "epoch:11 step:9186 [D loss: 0.083017, acc.: 97.66%] [G loss: 3.238039]\n",
      "epoch:11 step:9187 [D loss: 0.083139, acc.: 99.22%] [G loss: 2.569815]\n",
      "epoch:11 step:9188 [D loss: 0.048346, acc.: 99.22%] [G loss: 5.911577]\n",
      "epoch:11 step:9189 [D loss: 0.032326, acc.: 100.00%] [G loss: 5.378263]\n",
      "epoch:11 step:9190 [D loss: 0.144283, acc.: 97.66%] [G loss: 1.677489]\n",
      "epoch:11 step:9191 [D loss: 0.152626, acc.: 94.53%] [G loss: 4.965582]\n",
      "epoch:11 step:9192 [D loss: 0.185451, acc.: 93.75%] [G loss: 3.775726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9193 [D loss: 0.044722, acc.: 100.00%] [G loss: 4.688346]\n",
      "epoch:11 step:9194 [D loss: 0.015803, acc.: 100.00%] [G loss: 4.565326]\n",
      "epoch:11 step:9195 [D loss: 0.378165, acc.: 81.25%] [G loss: 4.763744]\n",
      "epoch:11 step:9196 [D loss: 0.005037, acc.: 100.00%] [G loss: 5.919289]\n",
      "epoch:11 step:9197 [D loss: 0.200898, acc.: 90.62%] [G loss: 5.279291]\n",
      "epoch:11 step:9198 [D loss: 0.028552, acc.: 100.00%] [G loss: 4.390767]\n",
      "epoch:11 step:9199 [D loss: 0.048838, acc.: 100.00%] [G loss: 4.740643]\n",
      "epoch:11 step:9200 [D loss: 0.009498, acc.: 100.00%] [G loss: 3.946280]\n",
      "epoch:11 step:9201 [D loss: 0.186884, acc.: 93.75%] [G loss: 2.763291]\n",
      "epoch:11 step:9202 [D loss: 0.006188, acc.: 100.00%] [G loss: 2.107439]\n",
      "epoch:11 step:9203 [D loss: 0.186608, acc.: 92.19%] [G loss: 5.567234]\n",
      "epoch:11 step:9204 [D loss: 1.116024, acc.: 44.53%] [G loss: 6.225867]\n",
      "epoch:11 step:9205 [D loss: 0.008438, acc.: 100.00%] [G loss: 7.138150]\n",
      "epoch:11 step:9206 [D loss: 0.220978, acc.: 90.62%] [G loss: 4.224403]\n",
      "epoch:11 step:9207 [D loss: 0.058518, acc.: 98.44%] [G loss: 3.673724]\n",
      "epoch:11 step:9208 [D loss: 0.032951, acc.: 100.00%] [G loss: 5.042222]\n",
      "epoch:11 step:9209 [D loss: 0.017608, acc.: 100.00%] [G loss: 4.969516]\n",
      "epoch:11 step:9210 [D loss: 0.032868, acc.: 100.00%] [G loss: 3.303339]\n",
      "epoch:11 step:9211 [D loss: 0.091472, acc.: 99.22%] [G loss: 5.467744]\n",
      "epoch:11 step:9212 [D loss: 0.023290, acc.: 99.22%] [G loss: 5.819527]\n",
      "epoch:11 step:9213 [D loss: 0.235774, acc.: 91.41%] [G loss: 3.055635]\n",
      "epoch:11 step:9214 [D loss: 0.058047, acc.: 100.00%] [G loss: 5.042348]\n",
      "epoch:11 step:9215 [D loss: 0.028987, acc.: 99.22%] [G loss: 4.608635]\n",
      "epoch:11 step:9216 [D loss: 0.053856, acc.: 99.22%] [G loss: 5.318879]\n",
      "epoch:11 step:9217 [D loss: 0.010875, acc.: 100.00%] [G loss: 5.354810]\n",
      "epoch:11 step:9218 [D loss: 0.044475, acc.: 99.22%] [G loss: 4.328814]\n",
      "epoch:11 step:9219 [D loss: 0.068779, acc.: 99.22%] [G loss: 4.977080]\n",
      "epoch:11 step:9220 [D loss: 0.036043, acc.: 100.00%] [G loss: 4.551079]\n",
      "epoch:11 step:9221 [D loss: 0.059554, acc.: 99.22%] [G loss: 5.112612]\n",
      "epoch:11 step:9222 [D loss: 0.122695, acc.: 96.09%] [G loss: 3.908418]\n",
      "epoch:11 step:9223 [D loss: 0.024449, acc.: 100.00%] [G loss: 3.536224]\n",
      "epoch:11 step:9224 [D loss: 0.035040, acc.: 100.00%] [G loss: 4.832739]\n",
      "epoch:11 step:9225 [D loss: 0.012032, acc.: 100.00%] [G loss: 4.894147]\n",
      "epoch:11 step:9226 [D loss: 0.089719, acc.: 99.22%] [G loss: 2.179695]\n",
      "epoch:11 step:9227 [D loss: 0.077046, acc.: 97.66%] [G loss: 5.095743]\n",
      "epoch:11 step:9228 [D loss: 0.032827, acc.: 100.00%] [G loss: 4.799166]\n",
      "epoch:11 step:9229 [D loss: 0.009248, acc.: 100.00%] [G loss: 2.581922]\n",
      "epoch:11 step:9230 [D loss: 0.051319, acc.: 100.00%] [G loss: 4.785993]\n",
      "epoch:11 step:9231 [D loss: 0.043714, acc.: 100.00%] [G loss: 5.574574]\n",
      "epoch:11 step:9232 [D loss: 0.033766, acc.: 100.00%] [G loss: 3.009423]\n",
      "epoch:11 step:9233 [D loss: 1.817658, acc.: 38.28%] [G loss: 9.645972]\n",
      "epoch:11 step:9234 [D loss: 3.382017, acc.: 50.00%] [G loss: 6.318102]\n",
      "epoch:11 step:9235 [D loss: 1.852167, acc.: 50.00%] [G loss: 2.565756]\n",
      "epoch:11 step:9236 [D loss: 0.937127, acc.: 53.12%] [G loss: 3.569837]\n",
      "epoch:11 step:9237 [D loss: 0.212876, acc.: 89.84%] [G loss: 3.746882]\n",
      "epoch:11 step:9238 [D loss: 0.376516, acc.: 78.91%] [G loss: 1.958521]\n",
      "epoch:11 step:9239 [D loss: 0.162558, acc.: 94.53%] [G loss: 2.554443]\n",
      "epoch:11 step:9240 [D loss: 0.126027, acc.: 99.22%] [G loss: 2.621679]\n",
      "epoch:11 step:9241 [D loss: 0.151475, acc.: 97.66%] [G loss: 1.887939]\n",
      "epoch:11 step:9242 [D loss: 0.164253, acc.: 95.31%] [G loss: 1.880108]\n",
      "epoch:11 step:9243 [D loss: 0.352994, acc.: 84.38%] [G loss: 3.084538]\n",
      "epoch:11 step:9244 [D loss: 0.184297, acc.: 92.97%] [G loss: 3.291210]\n",
      "epoch:11 step:9245 [D loss: 0.349808, acc.: 86.72%] [G loss: 2.424030]\n",
      "epoch:11 step:9246 [D loss: 0.137610, acc.: 97.66%] [G loss: 2.958622]\n",
      "epoch:11 step:9247 [D loss: 0.265418, acc.: 89.06%] [G loss: 2.454508]\n",
      "epoch:11 step:9248 [D loss: 0.222831, acc.: 94.53%] [G loss: 2.465494]\n",
      "epoch:11 step:9249 [D loss: 0.288002, acc.: 89.06%] [G loss: 3.446338]\n",
      "epoch:11 step:9250 [D loss: 0.690686, acc.: 63.28%] [G loss: 2.868258]\n",
      "epoch:11 step:9251 [D loss: 0.185304, acc.: 93.75%] [G loss: 3.567687]\n",
      "epoch:11 step:9252 [D loss: 0.319626, acc.: 88.28%] [G loss: 3.430008]\n",
      "epoch:11 step:9253 [D loss: 0.183565, acc.: 92.97%] [G loss: 2.797175]\n",
      "epoch:11 step:9254 [D loss: 0.129297, acc.: 97.66%] [G loss: 2.945505]\n",
      "epoch:11 step:9255 [D loss: 0.091741, acc.: 99.22%] [G loss: 1.533789]\n",
      "epoch:11 step:9256 [D loss: 0.399795, acc.: 83.59%] [G loss: 2.925344]\n",
      "epoch:11 step:9257 [D loss: 0.052735, acc.: 99.22%] [G loss: 2.985727]\n",
      "epoch:11 step:9258 [D loss: 0.065998, acc.: 98.44%] [G loss: 2.266978]\n",
      "epoch:11 step:9259 [D loss: 0.108750, acc.: 98.44%] [G loss: 1.279672]\n",
      "epoch:11 step:9260 [D loss: 0.098417, acc.: 98.44%] [G loss: 1.704760]\n",
      "epoch:11 step:9261 [D loss: 0.155560, acc.: 96.09%] [G loss: 0.593604]\n",
      "epoch:11 step:9262 [D loss: 0.363343, acc.: 83.59%] [G loss: 3.933009]\n",
      "epoch:11 step:9263 [D loss: 0.067825, acc.: 99.22%] [G loss: 3.876495]\n",
      "epoch:11 step:9264 [D loss: 0.318403, acc.: 88.28%] [G loss: 1.530235]\n",
      "epoch:11 step:9265 [D loss: 0.091475, acc.: 98.44%] [G loss: 0.554427]\n",
      "epoch:11 step:9266 [D loss: 0.064844, acc.: 99.22%] [G loss: 1.425105]\n",
      "epoch:11 step:9267 [D loss: 0.119332, acc.: 96.09%] [G loss: 0.851460]\n",
      "epoch:11 step:9268 [D loss: 0.278776, acc.: 87.50%] [G loss: 2.244327]\n",
      "epoch:11 step:9269 [D loss: 0.103293, acc.: 96.88%] [G loss: 2.807419]\n",
      "epoch:11 step:9270 [D loss: 0.083415, acc.: 97.66%] [G loss: 1.905651]\n",
      "epoch:11 step:9271 [D loss: 0.112485, acc.: 95.31%] [G loss: 0.725204]\n",
      "epoch:11 step:9272 [D loss: 0.253420, acc.: 90.62%] [G loss: 2.015391]\n",
      "epoch:11 step:9273 [D loss: 0.137979, acc.: 92.97%] [G loss: 2.133496]\n",
      "epoch:11 step:9274 [D loss: 0.049925, acc.: 100.00%] [G loss: 1.916584]\n",
      "epoch:11 step:9275 [D loss: 0.042216, acc.: 99.22%] [G loss: 1.255695]\n",
      "epoch:11 step:9276 [D loss: 0.751016, acc.: 62.50%] [G loss: 4.069969]\n",
      "epoch:11 step:9277 [D loss: 0.250853, acc.: 89.06%] [G loss: 4.319433]\n",
      "epoch:11 step:9278 [D loss: 0.202992, acc.: 93.75%] [G loss: 3.588345]\n",
      "epoch:11 step:9279 [D loss: 0.158092, acc.: 95.31%] [G loss: 4.001075]\n",
      "epoch:11 step:9280 [D loss: 0.015281, acc.: 100.00%] [G loss: 4.208897]\n",
      "epoch:11 step:9281 [D loss: 0.028046, acc.: 100.00%] [G loss: 4.233567]\n",
      "epoch:11 step:9282 [D loss: 0.016776, acc.: 100.00%] [G loss: 3.871537]\n",
      "epoch:11 step:9283 [D loss: 0.058203, acc.: 99.22%] [G loss: 3.680703]\n",
      "epoch:11 step:9284 [D loss: 0.081503, acc.: 97.66%] [G loss: 3.047966]\n",
      "epoch:11 step:9285 [D loss: 0.069941, acc.: 98.44%] [G loss: 3.383929]\n",
      "epoch:11 step:9286 [D loss: 0.036098, acc.: 100.00%] [G loss: 3.235902]\n",
      "epoch:11 step:9287 [D loss: 0.045343, acc.: 98.44%] [G loss: 3.329739]\n",
      "epoch:11 step:9288 [D loss: 0.080854, acc.: 97.66%] [G loss: 3.652017]\n",
      "epoch:11 step:9289 [D loss: 0.038696, acc.: 100.00%] [G loss: 3.592812]\n",
      "epoch:11 step:9290 [D loss: 0.113662, acc.: 97.66%] [G loss: 3.839223]\n",
      "epoch:11 step:9291 [D loss: 0.035774, acc.: 100.00%] [G loss: 3.381034]\n",
      "epoch:11 step:9292 [D loss: 0.070139, acc.: 98.44%] [G loss: 4.465698]\n",
      "epoch:11 step:9293 [D loss: 0.059901, acc.: 99.22%] [G loss: 4.839497]\n",
      "epoch:11 step:9294 [D loss: 0.121659, acc.: 96.09%] [G loss: 3.882135]\n",
      "epoch:11 step:9295 [D loss: 0.085625, acc.: 99.22%] [G loss: 4.857260]\n",
      "epoch:11 step:9296 [D loss: 0.065042, acc.: 98.44%] [G loss: 4.293595]\n",
      "epoch:11 step:9297 [D loss: 0.026447, acc.: 100.00%] [G loss: 4.895561]\n",
      "epoch:11 step:9298 [D loss: 0.239080, acc.: 90.62%] [G loss: 5.136462]\n",
      "epoch:11 step:9299 [D loss: 0.065983, acc.: 99.22%] [G loss: 4.915132]\n",
      "epoch:11 step:9300 [D loss: 0.047978, acc.: 100.00%] [G loss: 4.396940]\n",
      "epoch:11 step:9301 [D loss: 0.080449, acc.: 98.44%] [G loss: 2.332351]\n",
      "epoch:11 step:9302 [D loss: 0.024545, acc.: 100.00%] [G loss: 3.744036]\n",
      "epoch:11 step:9303 [D loss: 0.040052, acc.: 99.22%] [G loss: 3.816580]\n",
      "epoch:11 step:9304 [D loss: 2.975987, acc.: 26.56%] [G loss: 7.769406]\n",
      "epoch:11 step:9305 [D loss: 2.745746, acc.: 50.00%] [G loss: 5.290808]\n",
      "epoch:11 step:9306 [D loss: 1.442739, acc.: 52.34%] [G loss: 1.584272]\n",
      "epoch:11 step:9307 [D loss: 0.644610, acc.: 69.53%] [G loss: 2.658105]\n",
      "epoch:11 step:9308 [D loss: 0.242217, acc.: 90.62%] [G loss: 2.640997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11 step:9309 [D loss: 0.166555, acc.: 95.31%] [G loss: 2.228004]\n",
      "epoch:11 step:9310 [D loss: 0.251348, acc.: 91.41%] [G loss: 1.840242]\n",
      "epoch:11 step:9311 [D loss: 0.362572, acc.: 88.28%] [G loss: 2.042247]\n",
      "epoch:11 step:9312 [D loss: 0.328231, acc.: 85.94%] [G loss: 2.128644]\n",
      "epoch:11 step:9313 [D loss: 0.304343, acc.: 89.84%] [G loss: 2.870780]\n",
      "epoch:11 step:9314 [D loss: 0.269591, acc.: 90.62%] [G loss: 2.391745]\n",
      "epoch:11 step:9315 [D loss: 0.348978, acc.: 84.38%] [G loss: 2.379586]\n",
      "epoch:11 step:9316 [D loss: 0.200461, acc.: 95.31%] [G loss: 2.546620]\n",
      "epoch:11 step:9317 [D loss: 0.374582, acc.: 85.94%] [G loss: 3.457250]\n",
      "epoch:11 step:9318 [D loss: 0.406558, acc.: 79.69%] [G loss: 2.319254]\n",
      "epoch:11 step:9319 [D loss: 0.336898, acc.: 87.50%] [G loss: 2.876891]\n",
      "epoch:11 step:9320 [D loss: 0.241912, acc.: 94.53%] [G loss: 1.769657]\n",
      "epoch:11 step:9321 [D loss: 0.192554, acc.: 95.31%] [G loss: 2.431747]\n",
      "epoch:11 step:9322 [D loss: 0.207314, acc.: 95.31%] [G loss: 2.280798]\n",
      "epoch:11 step:9323 [D loss: 0.147709, acc.: 96.88%] [G loss: 2.655260]\n",
      "epoch:11 step:9324 [D loss: 0.211135, acc.: 95.31%] [G loss: 2.551903]\n",
      "epoch:11 step:9325 [D loss: 0.140023, acc.: 97.66%] [G loss: 1.624084]\n",
      "epoch:11 step:9326 [D loss: 0.363408, acc.: 82.81%] [G loss: 3.122640]\n",
      "epoch:11 step:9327 [D loss: 0.090090, acc.: 98.44%] [G loss: 3.705527]\n",
      "epoch:11 step:9328 [D loss: 0.531427, acc.: 71.88%] [G loss: 1.191627]\n",
      "epoch:11 step:9329 [D loss: 0.185185, acc.: 95.31%] [G loss: 2.281169]\n",
      "epoch:11 step:9330 [D loss: 0.132229, acc.: 96.88%] [G loss: 1.026189]\n",
      "epoch:11 step:9331 [D loss: 0.192424, acc.: 94.53%] [G loss: 1.535437]\n",
      "epoch:11 step:9332 [D loss: 0.157595, acc.: 97.66%] [G loss: 2.046438]\n",
      "epoch:11 step:9333 [D loss: 0.460059, acc.: 76.56%] [G loss: 2.777878]\n",
      "epoch:11 step:9334 [D loss: 0.239532, acc.: 91.41%] [G loss: 1.134769]\n",
      "epoch:11 step:9335 [D loss: 0.244674, acc.: 92.19%] [G loss: 1.106331]\n",
      "epoch:11 step:9336 [D loss: 0.104232, acc.: 96.09%] [G loss: 2.107481]\n",
      "epoch:11 step:9337 [D loss: 0.281491, acc.: 88.28%] [G loss: 0.970945]\n",
      "epoch:11 step:9338 [D loss: 0.063802, acc.: 99.22%] [G loss: 0.964222]\n",
      "epoch:11 step:9339 [D loss: 0.183390, acc.: 94.53%] [G loss: 1.529129]\n",
      "epoch:11 step:9340 [D loss: 0.146379, acc.: 96.09%] [G loss: 1.011612]\n",
      "epoch:11 step:9341 [D loss: 0.116407, acc.: 96.88%] [G loss: 0.760930]\n",
      "epoch:11 step:9342 [D loss: 0.029605, acc.: 100.00%] [G loss: 0.840383]\n",
      "epoch:11 step:9343 [D loss: 0.338321, acc.: 85.16%] [G loss: 3.137050]\n",
      "epoch:11 step:9344 [D loss: 0.693252, acc.: 69.53%] [G loss: 0.728757]\n",
      "epoch:11 step:9345 [D loss: 0.144176, acc.: 95.31%] [G loss: 0.371432]\n",
      "epoch:11 step:9346 [D loss: 0.274883, acc.: 83.59%] [G loss: 1.592855]\n",
      "epoch:11 step:9347 [D loss: 0.093881, acc.: 97.66%] [G loss: 3.183411]\n",
      "epoch:11 step:9348 [D loss: 0.091548, acc.: 96.09%] [G loss: 2.583151]\n",
      "epoch:11 step:9349 [D loss: 0.427587, acc.: 78.12%] [G loss: 3.054411]\n",
      "epoch:11 step:9350 [D loss: 0.298698, acc.: 85.94%] [G loss: 3.506739]\n",
      "epoch:11 step:9351 [D loss: 0.115164, acc.: 97.66%] [G loss: 3.352643]\n",
      "epoch:11 step:9352 [D loss: 0.162480, acc.: 95.31%] [G loss: 3.537946]\n",
      "epoch:11 step:9353 [D loss: 0.486228, acc.: 77.34%] [G loss: 3.519083]\n",
      "epoch:11 step:9354 [D loss: 0.058903, acc.: 99.22%] [G loss: 4.306748]\n",
      "epoch:11 step:9355 [D loss: 0.105708, acc.: 97.66%] [G loss: 3.507957]\n",
      "epoch:11 step:9356 [D loss: 0.054280, acc.: 100.00%] [G loss: 3.463182]\n",
      "epoch:11 step:9357 [D loss: 0.091227, acc.: 97.66%] [G loss: 3.879376]\n",
      "epoch:11 step:9358 [D loss: 0.104771, acc.: 99.22%] [G loss: 3.908748]\n",
      "epoch:11 step:9359 [D loss: 0.138196, acc.: 96.09%] [G loss: 3.554431]\n",
      "epoch:11 step:9360 [D loss: 0.079767, acc.: 100.00%] [G loss: 3.602783]\n",
      "epoch:11 step:9361 [D loss: 0.176332, acc.: 95.31%] [G loss: 3.281942]\n",
      "epoch:11 step:9362 [D loss: 0.181320, acc.: 93.75%] [G loss: 3.407689]\n",
      "epoch:11 step:9363 [D loss: 0.113903, acc.: 96.88%] [G loss: 3.000528]\n",
      "epoch:11 step:9364 [D loss: 0.083120, acc.: 98.44%] [G loss: 3.444427]\n",
      "epoch:11 step:9365 [D loss: 0.058965, acc.: 99.22%] [G loss: 3.673259]\n",
      "epoch:11 step:9366 [D loss: 0.022288, acc.: 100.00%] [G loss: 3.222827]\n",
      "epoch:11 step:9367 [D loss: 0.090033, acc.: 98.44%] [G loss: 2.972062]\n",
      "epoch:11 step:9368 [D loss: 0.033807, acc.: 99.22%] [G loss: 2.625174]\n",
      "epoch:11 step:9369 [D loss: 0.086058, acc.: 97.66%] [G loss: 3.311073]\n",
      "epoch:11 step:9370 [D loss: 0.759124, acc.: 63.28%] [G loss: 5.707899]\n",
      "epoch:11 step:9371 [D loss: 0.675123, acc.: 65.62%] [G loss: 1.544355]\n",
      "epoch:11 step:9372 [D loss: 0.303575, acc.: 86.72%] [G loss: 3.043678]\n",
      "epoch:12 step:9373 [D loss: 0.006549, acc.: 100.00%] [G loss: 5.463481]\n",
      "epoch:12 step:9374 [D loss: 0.159188, acc.: 93.75%] [G loss: 2.248610]\n",
      "epoch:12 step:9375 [D loss: 0.036375, acc.: 100.00%] [G loss: 1.824342]\n",
      "epoch:12 step:9376 [D loss: 0.130508, acc.: 95.31%] [G loss: 4.593700]\n",
      "epoch:12 step:9377 [D loss: 0.025066, acc.: 100.00%] [G loss: 5.809075]\n",
      "epoch:12 step:9378 [D loss: 0.048593, acc.: 98.44%] [G loss: 3.667699]\n",
      "epoch:12 step:9379 [D loss: 0.238311, acc.: 92.19%] [G loss: 2.887869]\n",
      "epoch:12 step:9380 [D loss: 0.070245, acc.: 98.44%] [G loss: 3.467707]\n",
      "epoch:12 step:9381 [D loss: 0.051921, acc.: 99.22%] [G loss: 2.531969]\n",
      "epoch:12 step:9382 [D loss: 0.148340, acc.: 97.66%] [G loss: 2.558320]\n",
      "epoch:12 step:9383 [D loss: 0.169893, acc.: 94.53%] [G loss: 4.519339]\n",
      "epoch:12 step:9384 [D loss: 0.258966, acc.: 89.06%] [G loss: 3.654573]\n",
      "epoch:12 step:9385 [D loss: 0.075402, acc.: 98.44%] [G loss: 3.418478]\n",
      "epoch:12 step:9386 [D loss: 0.177124, acc.: 94.53%] [G loss: 3.137895]\n",
      "epoch:12 step:9387 [D loss: 0.067662, acc.: 98.44%] [G loss: 3.928514]\n",
      "epoch:12 step:9388 [D loss: 0.096695, acc.: 97.66%] [G loss: 3.573073]\n",
      "epoch:12 step:9389 [D loss: 0.047362, acc.: 99.22%] [G loss: 4.235858]\n",
      "epoch:12 step:9390 [D loss: 0.012925, acc.: 100.00%] [G loss: 3.779813]\n",
      "epoch:12 step:9391 [D loss: 0.039595, acc.: 100.00%] [G loss: 2.554805]\n",
      "epoch:12 step:9392 [D loss: 0.046954, acc.: 100.00%] [G loss: 3.239881]\n",
      "epoch:12 step:9393 [D loss: 0.014442, acc.: 100.00%] [G loss: 3.747028]\n",
      "epoch:12 step:9394 [D loss: 0.402540, acc.: 83.59%] [G loss: 4.868533]\n",
      "epoch:12 step:9395 [D loss: 0.198814, acc.: 91.41%] [G loss: 3.095092]\n",
      "epoch:12 step:9396 [D loss: 0.022873, acc.: 100.00%] [G loss: 3.675364]\n",
      "epoch:12 step:9397 [D loss: 0.064337, acc.: 99.22%] [G loss: 2.012317]\n",
      "epoch:12 step:9398 [D loss: 0.060367, acc.: 100.00%] [G loss: 2.203001]\n",
      "epoch:12 step:9399 [D loss: 0.052706, acc.: 99.22%] [G loss: 3.000651]\n",
      "epoch:12 step:9400 [D loss: 0.195273, acc.: 94.53%] [G loss: 0.828661]\n",
      "epoch:12 step:9401 [D loss: 0.014868, acc.: 100.00%] [G loss: 0.232590]\n",
      "epoch:12 step:9402 [D loss: 0.213182, acc.: 89.06%] [G loss: 3.704284]\n",
      "epoch:12 step:9403 [D loss: 0.398453, acc.: 81.25%] [G loss: 0.928625]\n",
      "epoch:12 step:9404 [D loss: 0.872123, acc.: 60.94%] [G loss: 8.243266]\n",
      "epoch:12 step:9405 [D loss: 2.419964, acc.: 50.00%] [G loss: 6.005523]\n",
      "epoch:12 step:9406 [D loss: 0.248426, acc.: 85.16%] [G loss: 3.825697]\n",
      "epoch:12 step:9407 [D loss: 0.024517, acc.: 100.00%] [G loss: 1.877674]\n",
      "epoch:12 step:9408 [D loss: 0.042806, acc.: 99.22%] [G loss: 1.403982]\n",
      "epoch:12 step:9409 [D loss: 0.247103, acc.: 90.62%] [G loss: 3.308892]\n",
      "epoch:12 step:9410 [D loss: 0.094966, acc.: 97.66%] [G loss: 4.782208]\n",
      "epoch:12 step:9411 [D loss: 0.088031, acc.: 98.44%] [G loss: 2.240931]\n",
      "epoch:12 step:9412 [D loss: 0.077878, acc.: 98.44%] [G loss: 3.657031]\n",
      "epoch:12 step:9413 [D loss: 0.019301, acc.: 100.00%] [G loss: 3.847528]\n",
      "epoch:12 step:9414 [D loss: 0.021391, acc.: 100.00%] [G loss: 3.502462]\n",
      "epoch:12 step:9415 [D loss: 0.067400, acc.: 99.22%] [G loss: 3.254115]\n",
      "epoch:12 step:9416 [D loss: 0.567300, acc.: 71.88%] [G loss: 4.857671]\n",
      "epoch:12 step:9417 [D loss: 0.532363, acc.: 68.75%] [G loss: 1.962595]\n",
      "epoch:12 step:9418 [D loss: 0.131758, acc.: 95.31%] [G loss: 1.714993]\n",
      "epoch:12 step:9419 [D loss: 0.029250, acc.: 100.00%] [G loss: 3.781141]\n",
      "epoch:12 step:9420 [D loss: 0.021425, acc.: 100.00%] [G loss: 4.374919]\n",
      "epoch:12 step:9421 [D loss: 0.027956, acc.: 100.00%] [G loss: 4.363032]\n",
      "epoch:12 step:9422 [D loss: 0.181673, acc.: 93.75%] [G loss: 3.865431]\n",
      "epoch:12 step:9423 [D loss: 0.032525, acc.: 99.22%] [G loss: 2.207469]\n",
      "epoch:12 step:9424 [D loss: 0.141376, acc.: 96.88%] [G loss: 2.931208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9425 [D loss: 0.218676, acc.: 92.19%] [G loss: 3.423887]\n",
      "epoch:12 step:9426 [D loss: 0.089355, acc.: 96.88%] [G loss: 4.918278]\n",
      "epoch:12 step:9427 [D loss: 0.074506, acc.: 98.44%] [G loss: 3.948571]\n",
      "epoch:12 step:9428 [D loss: 0.140062, acc.: 96.88%] [G loss: 0.317162]\n",
      "epoch:12 step:9429 [D loss: 0.376213, acc.: 85.94%] [G loss: 5.458078]\n",
      "epoch:12 step:9430 [D loss: 0.799490, acc.: 62.50%] [G loss: 2.324449]\n",
      "epoch:12 step:9431 [D loss: 0.062641, acc.: 99.22%] [G loss: 1.130833]\n",
      "epoch:12 step:9432 [D loss: 0.052602, acc.: 99.22%] [G loss: 1.295603]\n",
      "epoch:12 step:9433 [D loss: 0.053294, acc.: 100.00%] [G loss: 3.756166]\n",
      "epoch:12 step:9434 [D loss: 0.023394, acc.: 100.00%] [G loss: 2.687555]\n",
      "epoch:12 step:9435 [D loss: 0.113528, acc.: 96.88%] [G loss: 2.094380]\n",
      "epoch:12 step:9436 [D loss: 0.154520, acc.: 95.31%] [G loss: 2.389490]\n",
      "epoch:12 step:9437 [D loss: 0.183101, acc.: 95.31%] [G loss: 4.942064]\n",
      "epoch:12 step:9438 [D loss: 0.032211, acc.: 100.00%] [G loss: 2.338784]\n",
      "epoch:12 step:9439 [D loss: 0.096031, acc.: 98.44%] [G loss: 1.865608]\n",
      "epoch:12 step:9440 [D loss: 0.056440, acc.: 99.22%] [G loss: 4.082142]\n",
      "epoch:12 step:9441 [D loss: 0.016652, acc.: 100.00%] [G loss: 4.168079]\n",
      "epoch:12 step:9442 [D loss: 0.046864, acc.: 98.44%] [G loss: 1.711705]\n",
      "epoch:12 step:9443 [D loss: 0.159056, acc.: 92.97%] [G loss: 4.807443]\n",
      "epoch:12 step:9444 [D loss: 0.677697, acc.: 68.75%] [G loss: 2.510352]\n",
      "epoch:12 step:9445 [D loss: 0.101797, acc.: 96.88%] [G loss: 4.337247]\n",
      "epoch:12 step:9446 [D loss: 0.020592, acc.: 100.00%] [G loss: 3.611086]\n",
      "epoch:12 step:9447 [D loss: 0.021934, acc.: 100.00%] [G loss: 3.945348]\n",
      "epoch:12 step:9448 [D loss: 0.683634, acc.: 67.97%] [G loss: 5.370856]\n",
      "epoch:12 step:9449 [D loss: 1.797504, acc.: 51.56%] [G loss: 1.702012]\n",
      "epoch:12 step:9450 [D loss: 0.807065, acc.: 59.38%] [G loss: 3.801740]\n",
      "epoch:12 step:9451 [D loss: 0.433935, acc.: 80.47%] [G loss: 3.491972]\n",
      "epoch:12 step:9452 [D loss: 0.097753, acc.: 98.44%] [G loss: 3.583993]\n",
      "epoch:12 step:9453 [D loss: 0.049989, acc.: 99.22%] [G loss: 3.553196]\n",
      "epoch:12 step:9454 [D loss: 0.062761, acc.: 100.00%] [G loss: 2.709843]\n",
      "epoch:12 step:9455 [D loss: 0.138068, acc.: 97.66%] [G loss: 2.816012]\n",
      "epoch:12 step:9456 [D loss: 0.075070, acc.: 98.44%] [G loss: 2.588730]\n",
      "epoch:12 step:9457 [D loss: 0.091442, acc.: 98.44%] [G loss: 3.082345]\n",
      "epoch:12 step:9458 [D loss: 0.050748, acc.: 100.00%] [G loss: 3.325196]\n",
      "epoch:12 step:9459 [D loss: 0.035488, acc.: 100.00%] [G loss: 2.474849]\n",
      "epoch:12 step:9460 [D loss: 0.154307, acc.: 95.31%] [G loss: 3.735010]\n",
      "epoch:12 step:9461 [D loss: 0.039894, acc.: 100.00%] [G loss: 4.199830]\n",
      "epoch:12 step:9462 [D loss: 0.233018, acc.: 92.19%] [G loss: 2.972067]\n",
      "epoch:12 step:9463 [D loss: 0.041178, acc.: 99.22%] [G loss: 1.988389]\n",
      "epoch:12 step:9464 [D loss: 0.053395, acc.: 100.00%] [G loss: 3.681730]\n",
      "epoch:12 step:9465 [D loss: 0.061090, acc.: 98.44%] [G loss: 2.354672]\n",
      "epoch:12 step:9466 [D loss: 0.534548, acc.: 70.31%] [G loss: 1.202222]\n",
      "epoch:12 step:9467 [D loss: 0.103305, acc.: 97.66%] [G loss: 5.069657]\n",
      "epoch:12 step:9468 [D loss: 0.030912, acc.: 99.22%] [G loss: 4.747821]\n",
      "epoch:12 step:9469 [D loss: 0.070738, acc.: 97.66%] [G loss: 3.575801]\n",
      "epoch:12 step:9470 [D loss: 0.037464, acc.: 99.22%] [G loss: 1.607294]\n",
      "epoch:12 step:9471 [D loss: 0.115769, acc.: 99.22%] [G loss: 3.892841]\n",
      "epoch:12 step:9472 [D loss: 0.102690, acc.: 98.44%] [G loss: 3.548658]\n",
      "epoch:12 step:9473 [D loss: 0.045008, acc.: 98.44%] [G loss: 3.172847]\n",
      "epoch:12 step:9474 [D loss: 0.021124, acc.: 100.00%] [G loss: 3.831354]\n",
      "epoch:12 step:9475 [D loss: 0.057625, acc.: 99.22%] [G loss: 1.015081]\n",
      "epoch:12 step:9476 [D loss: 0.012713, acc.: 100.00%] [G loss: 1.353735]\n",
      "epoch:12 step:9477 [D loss: 0.130108, acc.: 96.88%] [G loss: 3.153380]\n",
      "epoch:12 step:9478 [D loss: 0.061108, acc.: 97.66%] [G loss: 3.721394]\n",
      "epoch:12 step:9479 [D loss: 0.122336, acc.: 94.53%] [G loss: 0.944050]\n",
      "epoch:12 step:9480 [D loss: 0.622479, acc.: 66.41%] [G loss: 6.310284]\n",
      "epoch:12 step:9481 [D loss: 1.203513, acc.: 53.91%] [G loss: 2.335858]\n",
      "epoch:12 step:9482 [D loss: 0.370724, acc.: 82.03%] [G loss: 4.811717]\n",
      "epoch:12 step:9483 [D loss: 0.347653, acc.: 82.03%] [G loss: 3.006343]\n",
      "epoch:12 step:9484 [D loss: 0.030765, acc.: 100.00%] [G loss: 2.224075]\n",
      "epoch:12 step:9485 [D loss: 0.313386, acc.: 84.38%] [G loss: 5.757059]\n",
      "epoch:12 step:9486 [D loss: 0.222192, acc.: 89.84%] [G loss: 5.542230]\n",
      "epoch:12 step:9487 [D loss: 0.089631, acc.: 96.88%] [G loss: 4.732643]\n",
      "epoch:12 step:9488 [D loss: 0.096637, acc.: 96.09%] [G loss: 3.715221]\n",
      "epoch:12 step:9489 [D loss: 0.073994, acc.: 99.22%] [G loss: 3.474630]\n",
      "epoch:12 step:9490 [D loss: 0.015171, acc.: 100.00%] [G loss: 1.683109]\n",
      "epoch:12 step:9491 [D loss: 0.033171, acc.: 100.00%] [G loss: 3.374736]\n",
      "epoch:12 step:9492 [D loss: 0.072031, acc.: 98.44%] [G loss: 5.265277]\n",
      "epoch:12 step:9493 [D loss: 0.018424, acc.: 100.00%] [G loss: 2.375638]\n",
      "epoch:12 step:9494 [D loss: 0.065935, acc.: 100.00%] [G loss: 3.941092]\n",
      "epoch:12 step:9495 [D loss: 0.108393, acc.: 97.66%] [G loss: 3.631151]\n",
      "epoch:12 step:9496 [D loss: 0.035358, acc.: 100.00%] [G loss: 1.822001]\n",
      "epoch:12 step:9497 [D loss: 0.135067, acc.: 99.22%] [G loss: 2.736323]\n",
      "epoch:12 step:9498 [D loss: 0.048096, acc.: 99.22%] [G loss: 5.129106]\n",
      "epoch:12 step:9499 [D loss: 0.208505, acc.: 94.53%] [G loss: 5.401468]\n",
      "epoch:12 step:9500 [D loss: 0.249044, acc.: 90.62%] [G loss: 3.052085]\n",
      "epoch:12 step:9501 [D loss: 0.065268, acc.: 98.44%] [G loss: 3.575693]\n",
      "epoch:12 step:9502 [D loss: 0.052012, acc.: 99.22%] [G loss: 4.049582]\n",
      "epoch:12 step:9503 [D loss: 0.047635, acc.: 99.22%] [G loss: 3.176950]\n",
      "epoch:12 step:9504 [D loss: 0.510096, acc.: 76.56%] [G loss: 4.880863]\n",
      "epoch:12 step:9505 [D loss: 0.102409, acc.: 97.66%] [G loss: 5.345923]\n",
      "epoch:12 step:9506 [D loss: 0.106517, acc.: 96.88%] [G loss: 3.788734]\n",
      "epoch:12 step:9507 [D loss: 0.015941, acc.: 100.00%] [G loss: 3.502850]\n",
      "epoch:12 step:9508 [D loss: 0.284251, acc.: 86.72%] [G loss: 5.224987]\n",
      "epoch:12 step:9509 [D loss: 0.137986, acc.: 96.09%] [G loss: 5.333284]\n",
      "epoch:12 step:9510 [D loss: 0.060858, acc.: 99.22%] [G loss: 4.864837]\n",
      "epoch:12 step:9511 [D loss: 0.071382, acc.: 97.66%] [G loss: 3.371936]\n",
      "epoch:12 step:9512 [D loss: 0.026433, acc.: 100.00%] [G loss: 3.764837]\n",
      "epoch:12 step:9513 [D loss: 0.023566, acc.: 100.00%] [G loss: 3.791495]\n",
      "epoch:12 step:9514 [D loss: 0.015850, acc.: 100.00%] [G loss: 3.231420]\n",
      "epoch:12 step:9515 [D loss: 0.079983, acc.: 98.44%] [G loss: 4.994059]\n",
      "epoch:12 step:9516 [D loss: 0.020654, acc.: 100.00%] [G loss: 4.543535]\n",
      "epoch:12 step:9517 [D loss: 0.039990, acc.: 99.22%] [G loss: 3.442190]\n",
      "epoch:12 step:9518 [D loss: 0.024432, acc.: 99.22%] [G loss: 2.794376]\n",
      "epoch:12 step:9519 [D loss: 0.035508, acc.: 100.00%] [G loss: 3.269142]\n",
      "epoch:12 step:9520 [D loss: 0.070824, acc.: 98.44%] [G loss: 4.264979]\n",
      "epoch:12 step:9521 [D loss: 0.059327, acc.: 99.22%] [G loss: 3.728928]\n",
      "epoch:12 step:9522 [D loss: 0.022904, acc.: 100.00%] [G loss: 4.480225]\n",
      "epoch:12 step:9523 [D loss: 0.075072, acc.: 99.22%] [G loss: 2.718961]\n",
      "epoch:12 step:9524 [D loss: 0.028102, acc.: 100.00%] [G loss: 1.917938]\n",
      "epoch:12 step:9525 [D loss: 0.938221, acc.: 58.59%] [G loss: 6.763338]\n",
      "epoch:12 step:9526 [D loss: 0.254233, acc.: 85.16%] [G loss: 5.687043]\n",
      "epoch:12 step:9527 [D loss: 0.026229, acc.: 99.22%] [G loss: 5.003875]\n",
      "epoch:12 step:9528 [D loss: 0.033477, acc.: 99.22%] [G loss: 6.045800]\n",
      "epoch:12 step:9529 [D loss: 0.010837, acc.: 100.00%] [G loss: 5.549856]\n",
      "epoch:12 step:9530 [D loss: 0.024231, acc.: 100.00%] [G loss: 4.129458]\n",
      "epoch:12 step:9531 [D loss: 0.074889, acc.: 97.66%] [G loss: 1.878946]\n",
      "epoch:12 step:9532 [D loss: 0.018060, acc.: 100.00%] [G loss: 5.073350]\n",
      "epoch:12 step:9533 [D loss: 0.057527, acc.: 99.22%] [G loss: 4.563825]\n",
      "epoch:12 step:9534 [D loss: 0.056145, acc.: 98.44%] [G loss: 4.958082]\n",
      "epoch:12 step:9535 [D loss: 0.052244, acc.: 98.44%] [G loss: 3.527994]\n",
      "epoch:12 step:9536 [D loss: 0.080859, acc.: 99.22%] [G loss: 3.512307]\n",
      "epoch:12 step:9537 [D loss: 0.035931, acc.: 100.00%] [G loss: 4.692148]\n",
      "epoch:12 step:9538 [D loss: 0.146752, acc.: 96.09%] [G loss: 1.081734]\n",
      "epoch:12 step:9539 [D loss: 0.336266, acc.: 81.25%] [G loss: 6.579272]\n",
      "epoch:12 step:9540 [D loss: 1.780475, acc.: 50.78%] [G loss: 4.412745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9541 [D loss: 0.042454, acc.: 99.22%] [G loss: 3.568187]\n",
      "epoch:12 step:9542 [D loss: 0.073976, acc.: 99.22%] [G loss: 3.352386]\n",
      "epoch:12 step:9543 [D loss: 0.051183, acc.: 99.22%] [G loss: 4.258963]\n",
      "epoch:12 step:9544 [D loss: 0.036360, acc.: 99.22%] [G loss: 3.998387]\n",
      "epoch:12 step:9545 [D loss: 0.034565, acc.: 100.00%] [G loss: 3.527971]\n",
      "epoch:12 step:9546 [D loss: 0.092096, acc.: 96.88%] [G loss: 3.272230]\n",
      "epoch:12 step:9547 [D loss: 0.039592, acc.: 100.00%] [G loss: 3.738850]\n",
      "epoch:12 step:9548 [D loss: 0.783710, acc.: 61.72%] [G loss: 5.277337]\n",
      "epoch:12 step:9549 [D loss: 0.044042, acc.: 99.22%] [G loss: 6.319749]\n",
      "epoch:12 step:9550 [D loss: 0.593807, acc.: 73.44%] [G loss: 1.913983]\n",
      "epoch:12 step:9551 [D loss: 0.497390, acc.: 78.91%] [G loss: 6.361888]\n",
      "epoch:12 step:9552 [D loss: 0.130100, acc.: 96.09%] [G loss: 6.020404]\n",
      "epoch:12 step:9553 [D loss: 0.061149, acc.: 98.44%] [G loss: 5.042085]\n",
      "epoch:12 step:9554 [D loss: 0.034718, acc.: 100.00%] [G loss: 3.618375]\n",
      "epoch:12 step:9555 [D loss: 0.029167, acc.: 99.22%] [G loss: 4.727799]\n",
      "epoch:12 step:9556 [D loss: 0.137680, acc.: 96.88%] [G loss: 4.097184]\n",
      "epoch:12 step:9557 [D loss: 0.032988, acc.: 99.22%] [G loss: 3.981668]\n",
      "epoch:12 step:9558 [D loss: 0.053978, acc.: 99.22%] [G loss: 4.741282]\n",
      "epoch:12 step:9559 [D loss: 0.643449, acc.: 64.84%] [G loss: 4.530251]\n",
      "epoch:12 step:9560 [D loss: 0.007581, acc.: 100.00%] [G loss: 5.688448]\n",
      "epoch:12 step:9561 [D loss: 0.104449, acc.: 94.53%] [G loss: 3.315290]\n",
      "epoch:12 step:9562 [D loss: 0.017904, acc.: 100.00%] [G loss: 5.061895]\n",
      "epoch:12 step:9563 [D loss: 0.045840, acc.: 99.22%] [G loss: 3.048284]\n",
      "epoch:12 step:9564 [D loss: 0.076785, acc.: 98.44%] [G loss: 3.303335]\n",
      "epoch:12 step:9565 [D loss: 0.048168, acc.: 100.00%] [G loss: 4.114262]\n",
      "epoch:12 step:9566 [D loss: 0.063607, acc.: 98.44%] [G loss: 3.149660]\n",
      "epoch:12 step:9567 [D loss: 0.046276, acc.: 100.00%] [G loss: 3.435618]\n",
      "epoch:12 step:9568 [D loss: 0.023948, acc.: 100.00%] [G loss: 2.476492]\n",
      "epoch:12 step:9569 [D loss: 0.029390, acc.: 100.00%] [G loss: 1.726630]\n",
      "epoch:12 step:9570 [D loss: 0.040627, acc.: 100.00%] [G loss: 1.919329]\n",
      "epoch:12 step:9571 [D loss: 0.034686, acc.: 99.22%] [G loss: 2.274035]\n",
      "epoch:12 step:9572 [D loss: 0.400517, acc.: 82.81%] [G loss: 5.582185]\n",
      "epoch:12 step:9573 [D loss: 1.450609, acc.: 51.56%] [G loss: 2.372693]\n",
      "epoch:12 step:9574 [D loss: 0.139817, acc.: 96.09%] [G loss: 3.029992]\n",
      "epoch:12 step:9575 [D loss: 0.052807, acc.: 100.00%] [G loss: 2.694151]\n",
      "epoch:12 step:9576 [D loss: 0.071179, acc.: 100.00%] [G loss: 4.092048]\n",
      "epoch:12 step:9577 [D loss: 0.040224, acc.: 100.00%] [G loss: 3.825988]\n",
      "epoch:12 step:9578 [D loss: 0.050204, acc.: 99.22%] [G loss: 2.692983]\n",
      "epoch:12 step:9579 [D loss: 0.046360, acc.: 100.00%] [G loss: 3.394116]\n",
      "epoch:12 step:9580 [D loss: 0.039611, acc.: 100.00%] [G loss: 3.418633]\n",
      "epoch:12 step:9581 [D loss: 0.454651, acc.: 78.91%] [G loss: 5.827147]\n",
      "epoch:12 step:9582 [D loss: 0.516021, acc.: 69.53%] [G loss: 2.028668]\n",
      "epoch:12 step:9583 [D loss: 0.100074, acc.: 98.44%] [G loss: 3.017385]\n",
      "epoch:12 step:9584 [D loss: 0.040138, acc.: 99.22%] [G loss: 3.611012]\n",
      "epoch:12 step:9585 [D loss: 0.005812, acc.: 100.00%] [G loss: 4.084114]\n",
      "epoch:12 step:9586 [D loss: 0.126623, acc.: 99.22%] [G loss: 3.989229]\n",
      "epoch:12 step:9587 [D loss: 0.053317, acc.: 99.22%] [G loss: 4.882000]\n",
      "epoch:12 step:9588 [D loss: 0.018377, acc.: 100.00%] [G loss: 4.465234]\n",
      "epoch:12 step:9589 [D loss: 0.021615, acc.: 100.00%] [G loss: 3.951334]\n",
      "epoch:12 step:9590 [D loss: 0.218803, acc.: 93.75%] [G loss: 4.104963]\n",
      "epoch:12 step:9591 [D loss: 0.060411, acc.: 97.66%] [G loss: 4.002046]\n",
      "epoch:12 step:9592 [D loss: 0.133976, acc.: 95.31%] [G loss: 3.524465]\n",
      "epoch:12 step:9593 [D loss: 0.033471, acc.: 100.00%] [G loss: 4.162878]\n",
      "epoch:12 step:9594 [D loss: 0.033802, acc.: 100.00%] [G loss: 3.722715]\n",
      "epoch:12 step:9595 [D loss: 0.015535, acc.: 100.00%] [G loss: 3.775106]\n",
      "epoch:12 step:9596 [D loss: 0.051707, acc.: 99.22%] [G loss: 4.111429]\n",
      "epoch:12 step:9597 [D loss: 0.069707, acc.: 99.22%] [G loss: 4.710841]\n",
      "epoch:12 step:9598 [D loss: 0.070932, acc.: 97.66%] [G loss: 3.940952]\n",
      "epoch:12 step:9599 [D loss: 0.035084, acc.: 100.00%] [G loss: 1.936470]\n",
      "epoch:12 step:9600 [D loss: 0.041704, acc.: 99.22%] [G loss: 1.213293]\n",
      "epoch:12 step:9601 [D loss: 0.409560, acc.: 82.03%] [G loss: 6.556479]\n",
      "epoch:12 step:9602 [D loss: 1.385288, acc.: 49.22%] [G loss: 2.844194]\n",
      "epoch:12 step:9603 [D loss: 0.055618, acc.: 99.22%] [G loss: 5.591092]\n",
      "epoch:12 step:9604 [D loss: 0.083198, acc.: 96.88%] [G loss: 4.395720]\n",
      "epoch:12 step:9605 [D loss: 0.018686, acc.: 100.00%] [G loss: 4.930536]\n",
      "epoch:12 step:9606 [D loss: 0.030119, acc.: 99.22%] [G loss: 2.637969]\n",
      "epoch:12 step:9607 [D loss: 0.047202, acc.: 99.22%] [G loss: 3.030300]\n",
      "epoch:12 step:9608 [D loss: 0.040902, acc.: 100.00%] [G loss: 1.868526]\n",
      "epoch:12 step:9609 [D loss: 0.116652, acc.: 97.66%] [G loss: 5.371407]\n",
      "epoch:12 step:9610 [D loss: 0.056587, acc.: 99.22%] [G loss: 5.586228]\n",
      "epoch:12 step:9611 [D loss: 0.041949, acc.: 100.00%] [G loss: 2.166920]\n",
      "epoch:12 step:9612 [D loss: 0.166684, acc.: 93.75%] [G loss: 2.913002]\n",
      "epoch:12 step:9613 [D loss: 0.022107, acc.: 100.00%] [G loss: 3.128383]\n",
      "epoch:12 step:9614 [D loss: 0.165033, acc.: 92.97%] [G loss: 5.348610]\n",
      "epoch:12 step:9615 [D loss: 0.013544, acc.: 100.00%] [G loss: 3.929355]\n",
      "epoch:12 step:9616 [D loss: 0.038023, acc.: 100.00%] [G loss: 3.950356]\n",
      "epoch:12 step:9617 [D loss: 0.006843, acc.: 100.00%] [G loss: 5.070256]\n",
      "epoch:12 step:9618 [D loss: 0.020543, acc.: 100.00%] [G loss: 3.672049]\n",
      "epoch:12 step:9619 [D loss: 0.104543, acc.: 98.44%] [G loss: 2.183022]\n",
      "epoch:12 step:9620 [D loss: 0.099055, acc.: 98.44%] [G loss: 3.190741]\n",
      "epoch:12 step:9621 [D loss: 0.046421, acc.: 100.00%] [G loss: 4.358727]\n",
      "epoch:12 step:9622 [D loss: 0.314728, acc.: 89.06%] [G loss: 5.650909]\n",
      "epoch:12 step:9623 [D loss: 0.033679, acc.: 100.00%] [G loss: 6.431613]\n",
      "epoch:12 step:9624 [D loss: 0.173041, acc.: 91.41%] [G loss: 2.353404]\n",
      "epoch:12 step:9625 [D loss: 0.031867, acc.: 99.22%] [G loss: 4.981161]\n",
      "epoch:12 step:9626 [D loss: 0.010840, acc.: 100.00%] [G loss: 4.891630]\n",
      "epoch:12 step:9627 [D loss: 0.022080, acc.: 100.00%] [G loss: 2.951139]\n",
      "epoch:12 step:9628 [D loss: 0.086905, acc.: 97.66%] [G loss: 5.236846]\n",
      "epoch:12 step:9629 [D loss: 0.076964, acc.: 98.44%] [G loss: 3.390954]\n",
      "epoch:12 step:9630 [D loss: 0.013249, acc.: 100.00%] [G loss: 2.540337]\n",
      "epoch:12 step:9631 [D loss: 0.169392, acc.: 93.75%] [G loss: 5.929037]\n",
      "epoch:12 step:9632 [D loss: 0.122132, acc.: 93.75%] [G loss: 5.319507]\n",
      "epoch:12 step:9633 [D loss: 0.044074, acc.: 99.22%] [G loss: 1.242531]\n",
      "epoch:12 step:9634 [D loss: 0.172016, acc.: 96.09%] [G loss: 5.439310]\n",
      "epoch:12 step:9635 [D loss: 0.004228, acc.: 100.00%] [G loss: 6.719555]\n",
      "epoch:12 step:9636 [D loss: 1.607144, acc.: 35.94%] [G loss: 7.524797]\n",
      "epoch:12 step:9637 [D loss: 0.607240, acc.: 72.66%] [G loss: 4.400842]\n",
      "epoch:12 step:9638 [D loss: 0.142773, acc.: 94.53%] [G loss: 1.440577]\n",
      "epoch:12 step:9639 [D loss: 0.008819, acc.: 100.00%] [G loss: 2.377285]\n",
      "epoch:12 step:9640 [D loss: 0.039667, acc.: 99.22%] [G loss: 3.075449]\n",
      "epoch:12 step:9641 [D loss: 0.145822, acc.: 94.53%] [G loss: 2.745638]\n",
      "epoch:12 step:9642 [D loss: 0.026819, acc.: 100.00%] [G loss: 2.006564]\n",
      "epoch:12 step:9643 [D loss: 0.018611, acc.: 100.00%] [G loss: 3.921337]\n",
      "epoch:12 step:9644 [D loss: 0.037435, acc.: 100.00%] [G loss: 2.262361]\n",
      "epoch:12 step:9645 [D loss: 0.169540, acc.: 94.53%] [G loss: 4.430978]\n",
      "epoch:12 step:9646 [D loss: 0.023435, acc.: 100.00%] [G loss: 2.521743]\n",
      "epoch:12 step:9647 [D loss: 0.201357, acc.: 89.84%] [G loss: 3.920892]\n",
      "epoch:12 step:9648 [D loss: 0.024746, acc.: 100.00%] [G loss: 1.565060]\n",
      "epoch:12 step:9649 [D loss: 0.105758, acc.: 97.66%] [G loss: 0.142986]\n",
      "epoch:12 step:9650 [D loss: 0.390849, acc.: 81.25%] [G loss: 5.295464]\n",
      "epoch:12 step:9651 [D loss: 0.616633, acc.: 69.53%] [G loss: 3.216001]\n",
      "epoch:12 step:9652 [D loss: 0.132697, acc.: 96.09%] [G loss: 1.767382]\n",
      "epoch:12 step:9653 [D loss: 0.019441, acc.: 99.22%] [G loss: 2.393691]\n",
      "epoch:12 step:9654 [D loss: 0.108846, acc.: 96.09%] [G loss: 3.466129]\n",
      "epoch:12 step:9655 [D loss: 0.069948, acc.: 99.22%] [G loss: 1.537681]\n",
      "epoch:12 step:9656 [D loss: 0.039785, acc.: 98.44%] [G loss: 0.642929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9657 [D loss: 0.017288, acc.: 100.00%] [G loss: 1.466752]\n",
      "epoch:12 step:9658 [D loss: 0.087929, acc.: 98.44%] [G loss: 0.552861]\n",
      "epoch:12 step:9659 [D loss: 0.090686, acc.: 97.66%] [G loss: 1.301477]\n",
      "epoch:12 step:9660 [D loss: 0.050251, acc.: 99.22%] [G loss: 5.094018]\n",
      "epoch:12 step:9661 [D loss: 0.124995, acc.: 97.66%] [G loss: 0.544595]\n",
      "epoch:12 step:9662 [D loss: 0.003966, acc.: 100.00%] [G loss: 3.114922]\n",
      "epoch:12 step:9663 [D loss: 0.128681, acc.: 97.66%] [G loss: 5.282733]\n",
      "epoch:12 step:9664 [D loss: 0.258132, acc.: 87.50%] [G loss: 1.718322]\n",
      "epoch:12 step:9665 [D loss: 0.064354, acc.: 98.44%] [G loss: 5.434195]\n",
      "epoch:12 step:9666 [D loss: 0.010953, acc.: 100.00%] [G loss: 5.278850]\n",
      "epoch:12 step:9667 [D loss: 1.353536, acc.: 42.97%] [G loss: 8.007528]\n",
      "epoch:12 step:9668 [D loss: 2.191828, acc.: 50.00%] [G loss: 3.625981]\n",
      "epoch:12 step:9669 [D loss: 0.695849, acc.: 68.75%] [G loss: 2.404032]\n",
      "epoch:12 step:9670 [D loss: 0.139578, acc.: 96.88%] [G loss: 2.931045]\n",
      "epoch:12 step:9671 [D loss: 0.276633, acc.: 89.84%] [G loss: 3.265882]\n",
      "epoch:12 step:9672 [D loss: 0.177658, acc.: 93.75%] [G loss: 2.844833]\n",
      "epoch:12 step:9673 [D loss: 0.206364, acc.: 90.62%] [G loss: 2.631607]\n",
      "epoch:12 step:9674 [D loss: 0.076101, acc.: 99.22%] [G loss: 3.613826]\n",
      "epoch:12 step:9675 [D loss: 0.196889, acc.: 93.75%] [G loss: 1.790045]\n",
      "epoch:12 step:9676 [D loss: 0.050605, acc.: 98.44%] [G loss: 2.769669]\n",
      "epoch:12 step:9677 [D loss: 0.072036, acc.: 100.00%] [G loss: 2.426167]\n",
      "epoch:12 step:9678 [D loss: 0.123594, acc.: 98.44%] [G loss: 4.106447]\n",
      "epoch:12 step:9679 [D loss: 0.451275, acc.: 79.69%] [G loss: 5.064352]\n",
      "epoch:12 step:9680 [D loss: 0.038555, acc.: 99.22%] [G loss: 5.948183]\n",
      "epoch:12 step:9681 [D loss: 1.126975, acc.: 55.47%] [G loss: 0.512455]\n",
      "epoch:12 step:9682 [D loss: 0.640154, acc.: 70.31%] [G loss: 3.460893]\n",
      "epoch:12 step:9683 [D loss: 0.016343, acc.: 100.00%] [G loss: 5.759706]\n",
      "epoch:12 step:9684 [D loss: 0.800273, acc.: 64.06%] [G loss: 3.327036]\n",
      "epoch:12 step:9685 [D loss: 0.041074, acc.: 100.00%] [G loss: 1.282232]\n",
      "epoch:12 step:9686 [D loss: 0.365518, acc.: 82.03%] [G loss: 4.818408]\n",
      "epoch:12 step:9687 [D loss: 0.162534, acc.: 92.97%] [G loss: 4.545585]\n",
      "epoch:12 step:9688 [D loss: 0.259116, acc.: 89.06%] [G loss: 2.968393]\n",
      "epoch:12 step:9689 [D loss: 0.094659, acc.: 97.66%] [G loss: 2.315189]\n",
      "epoch:12 step:9690 [D loss: 0.065981, acc.: 99.22%] [G loss: 2.844039]\n",
      "epoch:12 step:9691 [D loss: 0.043450, acc.: 100.00%] [G loss: 2.763108]\n",
      "epoch:12 step:9692 [D loss: 0.270344, acc.: 90.62%] [G loss: 0.894074]\n",
      "epoch:12 step:9693 [D loss: 0.090477, acc.: 99.22%] [G loss: 3.866153]\n",
      "epoch:12 step:9694 [D loss: 0.024137, acc.: 100.00%] [G loss: 3.033699]\n",
      "epoch:12 step:9695 [D loss: 0.144995, acc.: 96.09%] [G loss: 1.910657]\n",
      "epoch:12 step:9696 [D loss: 0.069321, acc.: 99.22%] [G loss: 2.164105]\n",
      "epoch:12 step:9697 [D loss: 0.198147, acc.: 94.53%] [G loss: 3.814087]\n",
      "epoch:12 step:9698 [D loss: 0.168538, acc.: 96.09%] [G loss: 3.505072]\n",
      "epoch:12 step:9699 [D loss: 0.067506, acc.: 97.66%] [G loss: 3.152916]\n",
      "epoch:12 step:9700 [D loss: 0.057986, acc.: 100.00%] [G loss: 3.185922]\n",
      "epoch:12 step:9701 [D loss: 0.064303, acc.: 98.44%] [G loss: 4.460768]\n",
      "epoch:12 step:9702 [D loss: 0.084300, acc.: 98.44%] [G loss: 3.524519]\n",
      "epoch:12 step:9703 [D loss: 0.083699, acc.: 99.22%] [G loss: 1.653105]\n",
      "epoch:12 step:9704 [D loss: 0.387045, acc.: 77.34%] [G loss: 5.726724]\n",
      "epoch:12 step:9705 [D loss: 0.716942, acc.: 68.75%] [G loss: 3.738530]\n",
      "epoch:12 step:9706 [D loss: 0.019704, acc.: 100.00%] [G loss: 3.250798]\n",
      "epoch:12 step:9707 [D loss: 0.008742, acc.: 100.00%] [G loss: 3.085944]\n",
      "epoch:12 step:9708 [D loss: 0.111857, acc.: 96.09%] [G loss: 2.673562]\n",
      "epoch:12 step:9709 [D loss: 0.018011, acc.: 100.00%] [G loss: 2.399405]\n",
      "epoch:12 step:9710 [D loss: 0.129696, acc.: 97.66%] [G loss: 1.007328]\n",
      "epoch:12 step:9711 [D loss: 0.033666, acc.: 99.22%] [G loss: 0.633692]\n",
      "epoch:12 step:9712 [D loss: 0.097462, acc.: 98.44%] [G loss: 1.181755]\n",
      "epoch:12 step:9713 [D loss: 0.036292, acc.: 100.00%] [G loss: 1.409477]\n",
      "epoch:12 step:9714 [D loss: 0.030642, acc.: 99.22%] [G loss: 0.808339]\n",
      "epoch:12 step:9715 [D loss: 0.135757, acc.: 95.31%] [G loss: 0.195743]\n",
      "epoch:12 step:9716 [D loss: 0.131199, acc.: 94.53%] [G loss: 2.502243]\n",
      "epoch:12 step:9717 [D loss: 0.053695, acc.: 99.22%] [G loss: 1.388117]\n",
      "epoch:12 step:9718 [D loss: 0.434556, acc.: 82.03%] [G loss: 0.450104]\n",
      "epoch:12 step:9719 [D loss: 0.034957, acc.: 100.00%] [G loss: 0.959279]\n",
      "epoch:12 step:9720 [D loss: 0.009026, acc.: 100.00%] [G loss: 0.570791]\n",
      "epoch:12 step:9721 [D loss: 0.144113, acc.: 93.75%] [G loss: 3.206767]\n",
      "epoch:12 step:9722 [D loss: 0.405573, acc.: 78.91%] [G loss: 0.576417]\n",
      "epoch:12 step:9723 [D loss: 0.178171, acc.: 92.97%] [G loss: 3.639833]\n",
      "epoch:12 step:9724 [D loss: 0.015079, acc.: 100.00%] [G loss: 2.695656]\n",
      "epoch:12 step:9725 [D loss: 0.128806, acc.: 97.66%] [G loss: 3.869522]\n",
      "epoch:12 step:9726 [D loss: 0.085434, acc.: 96.88%] [G loss: 1.901452]\n",
      "epoch:12 step:9727 [D loss: 0.100191, acc.: 97.66%] [G loss: 4.802932]\n",
      "epoch:12 step:9728 [D loss: 0.090057, acc.: 97.66%] [G loss: 3.923507]\n",
      "epoch:12 step:9729 [D loss: 0.113937, acc.: 96.88%] [G loss: 3.684855]\n",
      "epoch:12 step:9730 [D loss: 0.385867, acc.: 83.59%] [G loss: 4.102834]\n",
      "epoch:12 step:9731 [D loss: 0.024968, acc.: 100.00%] [G loss: 5.107543]\n",
      "epoch:12 step:9732 [D loss: 0.013075, acc.: 100.00%] [G loss: 3.784276]\n",
      "epoch:12 step:9733 [D loss: 0.060027, acc.: 98.44%] [G loss: 4.963466]\n",
      "epoch:12 step:9734 [D loss: 0.023621, acc.: 99.22%] [G loss: 2.858503]\n",
      "epoch:12 step:9735 [D loss: 0.030807, acc.: 100.00%] [G loss: 1.074062]\n",
      "epoch:12 step:9736 [D loss: 0.135426, acc.: 97.66%] [G loss: 2.078088]\n",
      "epoch:12 step:9737 [D loss: 0.040861, acc.: 100.00%] [G loss: 4.989605]\n",
      "epoch:12 step:9738 [D loss: 0.551961, acc.: 72.66%] [G loss: 7.232548]\n",
      "epoch:12 step:9739 [D loss: 0.527846, acc.: 71.88%] [G loss: 4.512248]\n",
      "epoch:12 step:9740 [D loss: 0.119064, acc.: 94.53%] [G loss: 3.976829]\n",
      "epoch:12 step:9741 [D loss: 0.016228, acc.: 99.22%] [G loss: 5.527717]\n",
      "epoch:12 step:9742 [D loss: 0.009752, acc.: 100.00%] [G loss: 4.491356]\n",
      "epoch:12 step:9743 [D loss: 0.020195, acc.: 100.00%] [G loss: 5.631525]\n",
      "epoch:12 step:9744 [D loss: 0.025745, acc.: 100.00%] [G loss: 5.583062]\n",
      "epoch:12 step:9745 [D loss: 0.018769, acc.: 100.00%] [G loss: 5.896738]\n",
      "epoch:12 step:9746 [D loss: 0.041980, acc.: 98.44%] [G loss: 5.173591]\n",
      "epoch:12 step:9747 [D loss: 0.035114, acc.: 100.00%] [G loss: 3.422781]\n",
      "epoch:12 step:9748 [D loss: 0.051621, acc.: 100.00%] [G loss: 3.930965]\n",
      "epoch:12 step:9749 [D loss: 0.037711, acc.: 100.00%] [G loss: 4.717509]\n",
      "epoch:12 step:9750 [D loss: 0.015334, acc.: 100.00%] [G loss: 3.751093]\n",
      "epoch:12 step:9751 [D loss: 0.021429, acc.: 100.00%] [G loss: 3.667491]\n",
      "epoch:12 step:9752 [D loss: 0.034267, acc.: 100.00%] [G loss: 5.181299]\n",
      "epoch:12 step:9753 [D loss: 0.059783, acc.: 98.44%] [G loss: 3.595398]\n",
      "epoch:12 step:9754 [D loss: 0.109430, acc.: 97.66%] [G loss: 1.938280]\n",
      "epoch:12 step:9755 [D loss: 0.098539, acc.: 96.09%] [G loss: 3.732419]\n",
      "epoch:12 step:9756 [D loss: 0.068690, acc.: 100.00%] [G loss: 5.689429]\n",
      "epoch:12 step:9757 [D loss: 0.568810, acc.: 70.31%] [G loss: 7.033628]\n",
      "epoch:12 step:9758 [D loss: 0.625225, acc.: 71.09%] [G loss: 0.711674]\n",
      "epoch:12 step:9759 [D loss: 0.057112, acc.: 98.44%] [G loss: 4.428170]\n",
      "epoch:12 step:9760 [D loss: 0.084273, acc.: 98.44%] [G loss: 5.844223]\n",
      "epoch:12 step:9761 [D loss: 0.023267, acc.: 99.22%] [G loss: 4.425080]\n",
      "epoch:12 step:9762 [D loss: 0.073882, acc.: 98.44%] [G loss: 2.415682]\n",
      "epoch:12 step:9763 [D loss: 0.086835, acc.: 96.88%] [G loss: 3.985219]\n",
      "epoch:12 step:9764 [D loss: 0.134774, acc.: 96.88%] [G loss: 6.154268]\n",
      "epoch:12 step:9765 [D loss: 0.321210, acc.: 86.72%] [G loss: 0.587248]\n",
      "epoch:12 step:9766 [D loss: 0.022717, acc.: 99.22%] [G loss: 2.372858]\n",
      "epoch:12 step:9767 [D loss: 0.113466, acc.: 96.88%] [G loss: 0.686912]\n",
      "epoch:12 step:9768 [D loss: 0.031961, acc.: 100.00%] [G loss: 2.189799]\n",
      "epoch:12 step:9769 [D loss: 0.012767, acc.: 100.00%] [G loss: 3.835410]\n",
      "epoch:12 step:9770 [D loss: 0.005004, acc.: 100.00%] [G loss: 3.536316]\n",
      "epoch:12 step:9771 [D loss: 0.016582, acc.: 100.00%] [G loss: 4.472922]\n",
      "epoch:12 step:9772 [D loss: 0.018239, acc.: 100.00%] [G loss: 2.054596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9773 [D loss: 1.520026, acc.: 43.75%] [G loss: 9.253108]\n",
      "epoch:12 step:9774 [D loss: 3.242519, acc.: 50.00%] [G loss: 6.508005]\n",
      "epoch:12 step:9775 [D loss: 2.015401, acc.: 50.00%] [G loss: 2.350898]\n",
      "epoch:12 step:9776 [D loss: 0.334270, acc.: 85.94%] [G loss: 2.239448]\n",
      "epoch:12 step:9777 [D loss: 0.221611, acc.: 93.75%] [G loss: 1.828108]\n",
      "epoch:12 step:9778 [D loss: 0.136396, acc.: 99.22%] [G loss: 2.886049]\n",
      "epoch:12 step:9779 [D loss: 0.118750, acc.: 97.66%] [G loss: 2.626079]\n",
      "epoch:12 step:9780 [D loss: 0.076004, acc.: 100.00%] [G loss: 3.574150]\n",
      "epoch:12 step:9781 [D loss: 0.100935, acc.: 97.66%] [G loss: 2.015534]\n",
      "epoch:12 step:9782 [D loss: 0.366193, acc.: 83.59%] [G loss: 3.339147]\n",
      "epoch:12 step:9783 [D loss: 0.516690, acc.: 70.31%] [G loss: 1.773715]\n",
      "epoch:12 step:9784 [D loss: 0.172431, acc.: 96.09%] [G loss: 1.733048]\n",
      "epoch:12 step:9785 [D loss: 0.056361, acc.: 100.00%] [G loss: 2.350755]\n",
      "epoch:12 step:9786 [D loss: 0.130110, acc.: 95.31%] [G loss: 2.917690]\n",
      "epoch:12 step:9787 [D loss: 0.083049, acc.: 99.22%] [G loss: 2.269550]\n",
      "epoch:12 step:9788 [D loss: 0.022111, acc.: 100.00%] [G loss: 2.155436]\n",
      "epoch:12 step:9789 [D loss: 0.149821, acc.: 94.53%] [G loss: 3.073357]\n",
      "epoch:12 step:9790 [D loss: 0.413398, acc.: 81.25%] [G loss: 2.293731]\n",
      "epoch:12 step:9791 [D loss: 0.077420, acc.: 100.00%] [G loss: 1.488892]\n",
      "epoch:12 step:9792 [D loss: 0.144097, acc.: 96.09%] [G loss: 1.850160]\n",
      "epoch:12 step:9793 [D loss: 0.063612, acc.: 98.44%] [G loss: 1.629580]\n",
      "epoch:12 step:9794 [D loss: 0.035009, acc.: 100.00%] [G loss: 0.997714]\n",
      "epoch:12 step:9795 [D loss: 0.064212, acc.: 99.22%] [G loss: 1.056113]\n",
      "epoch:12 step:9796 [D loss: 0.089638, acc.: 95.31%] [G loss: 0.424125]\n",
      "epoch:12 step:9797 [D loss: 0.140154, acc.: 96.09%] [G loss: 0.455053]\n",
      "epoch:12 step:9798 [D loss: 0.038923, acc.: 99.22%] [G loss: 1.033896]\n",
      "epoch:12 step:9799 [D loss: 0.045822, acc.: 99.22%] [G loss: 1.241860]\n",
      "epoch:12 step:9800 [D loss: 0.799237, acc.: 59.38%] [G loss: 2.210456]\n",
      "epoch:12 step:9801 [D loss: 0.098881, acc.: 97.66%] [G loss: 3.492481]\n",
      "epoch:12 step:9802 [D loss: 0.172560, acc.: 95.31%] [G loss: 1.789963]\n",
      "epoch:12 step:9803 [D loss: 0.221855, acc.: 91.41%] [G loss: 2.669945]\n",
      "epoch:12 step:9804 [D loss: 0.018598, acc.: 100.00%] [G loss: 3.302767]\n",
      "epoch:12 step:9805 [D loss: 0.024311, acc.: 100.00%] [G loss: 3.332410]\n",
      "epoch:12 step:9806 [D loss: 0.034376, acc.: 99.22%] [G loss: 3.045175]\n",
      "epoch:12 step:9807 [D loss: 0.058327, acc.: 99.22%] [G loss: 2.769507]\n",
      "epoch:12 step:9808 [D loss: 0.126697, acc.: 98.44%] [G loss: 3.875873]\n",
      "epoch:12 step:9809 [D loss: 0.032865, acc.: 100.00%] [G loss: 3.651032]\n",
      "epoch:12 step:9810 [D loss: 0.119366, acc.: 98.44%] [G loss: 3.148808]\n",
      "epoch:12 step:9811 [D loss: 0.041911, acc.: 99.22%] [G loss: 3.584501]\n",
      "epoch:12 step:9812 [D loss: 0.068375, acc.: 99.22%] [G loss: 3.528488]\n",
      "epoch:12 step:9813 [D loss: 0.074815, acc.: 98.44%] [G loss: 3.753738]\n",
      "epoch:12 step:9814 [D loss: 0.015922, acc.: 100.00%] [G loss: 3.902056]\n",
      "epoch:12 step:9815 [D loss: 0.039144, acc.: 100.00%] [G loss: 3.554076]\n",
      "epoch:12 step:9816 [D loss: 0.019796, acc.: 100.00%] [G loss: 4.256713]\n",
      "epoch:12 step:9817 [D loss: 0.059571, acc.: 99.22%] [G loss: 3.412091]\n",
      "epoch:12 step:9818 [D loss: 0.036602, acc.: 100.00%] [G loss: 2.795279]\n",
      "epoch:12 step:9819 [D loss: 0.025996, acc.: 100.00%] [G loss: 3.155221]\n",
      "epoch:12 step:9820 [D loss: 0.041384, acc.: 100.00%] [G loss: 3.145111]\n",
      "epoch:12 step:9821 [D loss: 0.013086, acc.: 100.00%] [G loss: 2.424290]\n",
      "epoch:12 step:9822 [D loss: 0.057133, acc.: 99.22%] [G loss: 2.090916]\n",
      "epoch:12 step:9823 [D loss: 0.084346, acc.: 98.44%] [G loss: 0.262454]\n",
      "epoch:12 step:9824 [D loss: 0.312776, acc.: 84.38%] [G loss: 6.060442]\n",
      "epoch:12 step:9825 [D loss: 0.555719, acc.: 71.88%] [G loss: 3.150957]\n",
      "epoch:12 step:9826 [D loss: 0.009393, acc.: 100.00%] [G loss: 2.015751]\n",
      "epoch:12 step:9827 [D loss: 0.083194, acc.: 97.66%] [G loss: 3.040015]\n",
      "epoch:12 step:9828 [D loss: 0.010314, acc.: 100.00%] [G loss: 2.865331]\n",
      "epoch:12 step:9829 [D loss: 0.010885, acc.: 100.00%] [G loss: 1.674053]\n",
      "epoch:12 step:9830 [D loss: 0.039885, acc.: 99.22%] [G loss: 2.770471]\n",
      "epoch:12 step:9831 [D loss: 0.188934, acc.: 92.97%] [G loss: 2.328731]\n",
      "epoch:12 step:9832 [D loss: 0.165523, acc.: 94.53%] [G loss: 2.729187]\n",
      "epoch:12 step:9833 [D loss: 0.091217, acc.: 98.44%] [G loss: 1.797364]\n",
      "epoch:12 step:9834 [D loss: 0.071380, acc.: 98.44%] [G loss: 3.168190]\n",
      "epoch:12 step:9835 [D loss: 0.007141, acc.: 100.00%] [G loss: 2.433159]\n",
      "epoch:12 step:9836 [D loss: 0.016780, acc.: 100.00%] [G loss: 1.919633]\n",
      "epoch:12 step:9837 [D loss: 0.118944, acc.: 98.44%] [G loss: 4.124764]\n",
      "epoch:12 step:9838 [D loss: 0.187067, acc.: 94.53%] [G loss: 1.596095]\n",
      "epoch:12 step:9839 [D loss: 0.192782, acc.: 89.84%] [G loss: 4.938507]\n",
      "epoch:12 step:9840 [D loss: 0.523325, acc.: 75.00%] [G loss: 1.653431]\n",
      "epoch:12 step:9841 [D loss: 0.746894, acc.: 64.06%] [G loss: 7.340026]\n",
      "epoch:12 step:9842 [D loss: 0.741017, acc.: 66.41%] [G loss: 5.823537]\n",
      "epoch:12 step:9843 [D loss: 0.563207, acc.: 72.66%] [G loss: 4.326970]\n",
      "epoch:12 step:9844 [D loss: 0.062634, acc.: 98.44%] [G loss: 1.470048]\n",
      "epoch:12 step:9845 [D loss: 0.017050, acc.: 100.00%] [G loss: 2.345323]\n",
      "epoch:12 step:9846 [D loss: 0.089440, acc.: 98.44%] [G loss: 3.144494]\n",
      "epoch:12 step:9847 [D loss: 0.012161, acc.: 100.00%] [G loss: 4.461230]\n",
      "epoch:12 step:9848 [D loss: 0.006621, acc.: 100.00%] [G loss: 2.457748]\n",
      "epoch:12 step:9849 [D loss: 0.015072, acc.: 100.00%] [G loss: 1.859463]\n",
      "epoch:12 step:9850 [D loss: 0.032290, acc.: 100.00%] [G loss: 2.782621]\n",
      "epoch:12 step:9851 [D loss: 0.007855, acc.: 100.00%] [G loss: 2.260551]\n",
      "epoch:12 step:9852 [D loss: 0.023893, acc.: 100.00%] [G loss: 1.909442]\n",
      "epoch:12 step:9853 [D loss: 0.054964, acc.: 98.44%] [G loss: 1.612000]\n",
      "epoch:12 step:9854 [D loss: 0.027433, acc.: 100.00%] [G loss: 1.545495]\n",
      "epoch:12 step:9855 [D loss: 0.027413, acc.: 100.00%] [G loss: 1.284785]\n",
      "epoch:12 step:9856 [D loss: 0.074454, acc.: 98.44%] [G loss: 1.587540]\n",
      "epoch:12 step:9857 [D loss: 0.036750, acc.: 99.22%] [G loss: 1.663160]\n",
      "epoch:12 step:9858 [D loss: 0.161525, acc.: 93.75%] [G loss: 0.489430]\n",
      "epoch:12 step:9859 [D loss: 0.050814, acc.: 100.00%] [G loss: 0.154272]\n",
      "epoch:12 step:9860 [D loss: 0.248035, acc.: 87.50%] [G loss: 4.576199]\n",
      "epoch:12 step:9861 [D loss: 0.117854, acc.: 94.53%] [G loss: 3.813196]\n",
      "epoch:12 step:9862 [D loss: 0.172139, acc.: 95.31%] [G loss: 2.310170]\n",
      "epoch:12 step:9863 [D loss: 0.158034, acc.: 93.75%] [G loss: 3.455176]\n",
      "epoch:12 step:9864 [D loss: 0.015339, acc.: 100.00%] [G loss: 4.947346]\n",
      "epoch:12 step:9865 [D loss: 0.003573, acc.: 100.00%] [G loss: 4.301473]\n",
      "epoch:12 step:9866 [D loss: 0.007091, acc.: 100.00%] [G loss: 2.738407]\n",
      "epoch:12 step:9867 [D loss: 0.036200, acc.: 99.22%] [G loss: 2.012872]\n",
      "epoch:12 step:9868 [D loss: 0.097899, acc.: 97.66%] [G loss: 3.749782]\n",
      "epoch:12 step:9869 [D loss: 0.244834, acc.: 89.84%] [G loss: 1.221766]\n",
      "epoch:12 step:9870 [D loss: 0.338963, acc.: 86.72%] [G loss: 5.633583]\n",
      "epoch:12 step:9871 [D loss: 0.591985, acc.: 74.22%] [G loss: 3.949112]\n",
      "epoch:12 step:9872 [D loss: 0.041713, acc.: 98.44%] [G loss: 3.022714]\n",
      "epoch:12 step:9873 [D loss: 0.081875, acc.: 97.66%] [G loss: 4.423025]\n",
      "epoch:12 step:9874 [D loss: 0.014388, acc.: 100.00%] [G loss: 4.390176]\n",
      "epoch:12 step:9875 [D loss: 0.007977, acc.: 100.00%] [G loss: 4.718211]\n",
      "epoch:12 step:9876 [D loss: 0.015847, acc.: 100.00%] [G loss: 4.587531]\n",
      "epoch:12 step:9877 [D loss: 0.082466, acc.: 97.66%] [G loss: 3.966440]\n",
      "epoch:12 step:9878 [D loss: 0.022240, acc.: 100.00%] [G loss: 4.089552]\n",
      "epoch:12 step:9879 [D loss: 0.101905, acc.: 98.44%] [G loss: 3.005068]\n",
      "epoch:12 step:9880 [D loss: 0.501028, acc.: 75.00%] [G loss: 6.951474]\n",
      "epoch:12 step:9881 [D loss: 1.370627, acc.: 51.56%] [G loss: 3.047546]\n",
      "epoch:12 step:9882 [D loss: 0.111530, acc.: 96.09%] [G loss: 3.569914]\n",
      "epoch:12 step:9883 [D loss: 0.053298, acc.: 98.44%] [G loss: 3.756936]\n",
      "epoch:12 step:9884 [D loss: 0.062137, acc.: 97.66%] [G loss: 3.996805]\n",
      "epoch:12 step:9885 [D loss: 0.469399, acc.: 80.47%] [G loss: 5.743427]\n",
      "epoch:12 step:9886 [D loss: 0.358095, acc.: 78.91%] [G loss: 2.954953]\n",
      "epoch:12 step:9887 [D loss: 0.420098, acc.: 78.91%] [G loss: 6.408620]\n",
      "epoch:12 step:9888 [D loss: 0.516710, acc.: 75.00%] [G loss: 3.918491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:9889 [D loss: 0.150633, acc.: 96.09%] [G loss: 4.391814]\n",
      "epoch:12 step:9890 [D loss: 0.060407, acc.: 98.44%] [G loss: 4.565150]\n",
      "epoch:12 step:9891 [D loss: 0.206334, acc.: 90.62%] [G loss: 3.046728]\n",
      "epoch:12 step:9892 [D loss: 0.032800, acc.: 100.00%] [G loss: 3.628869]\n",
      "epoch:12 step:9893 [D loss: 0.043942, acc.: 100.00%] [G loss: 3.881685]\n",
      "epoch:12 step:9894 [D loss: 0.022319, acc.: 100.00%] [G loss: 4.460641]\n",
      "epoch:12 step:9895 [D loss: 0.047340, acc.: 100.00%] [G loss: 2.925286]\n",
      "epoch:12 step:9896 [D loss: 0.179937, acc.: 91.41%] [G loss: 4.624924]\n",
      "epoch:12 step:9897 [D loss: 0.094322, acc.: 96.88%] [G loss: 3.888944]\n",
      "epoch:12 step:9898 [D loss: 0.295835, acc.: 90.62%] [G loss: 2.573647]\n",
      "epoch:12 step:9899 [D loss: 0.054208, acc.: 98.44%] [G loss: 2.723654]\n",
      "epoch:12 step:9900 [D loss: 0.021165, acc.: 100.00%] [G loss: 3.820511]\n",
      "epoch:12 step:9901 [D loss: 0.012801, acc.: 100.00%] [G loss: 3.505125]\n",
      "epoch:12 step:9902 [D loss: 0.033500, acc.: 99.22%] [G loss: 2.301718]\n",
      "epoch:12 step:9903 [D loss: 0.270107, acc.: 90.62%] [G loss: 1.256711]\n",
      "epoch:12 step:9904 [D loss: 0.033349, acc.: 100.00%] [G loss: 3.708485]\n",
      "epoch:12 step:9905 [D loss: 0.135994, acc.: 97.66%] [G loss: 3.660097]\n",
      "epoch:12 step:9906 [D loss: 0.073477, acc.: 99.22%] [G loss: 3.527326]\n",
      "epoch:12 step:9907 [D loss: 0.029202, acc.: 100.00%] [G loss: 3.795856]\n",
      "epoch:12 step:9908 [D loss: 0.309359, acc.: 84.38%] [G loss: 4.659212]\n",
      "epoch:12 step:9909 [D loss: 0.095501, acc.: 97.66%] [G loss: 4.425917]\n",
      "epoch:12 step:9910 [D loss: 0.029868, acc.: 100.00%] [G loss: 4.299421]\n",
      "epoch:12 step:9911 [D loss: 0.029119, acc.: 99.22%] [G loss: 3.832493]\n",
      "epoch:12 step:9912 [D loss: 0.012375, acc.: 100.00%] [G loss: 3.627420]\n",
      "epoch:12 step:9913 [D loss: 0.042929, acc.: 100.00%] [G loss: 4.597532]\n",
      "epoch:12 step:9914 [D loss: 0.042159, acc.: 100.00%] [G loss: 4.637408]\n",
      "epoch:12 step:9915 [D loss: 0.092060, acc.: 99.22%] [G loss: 5.207282]\n",
      "epoch:12 step:9916 [D loss: 0.040893, acc.: 98.44%] [G loss: 3.743297]\n",
      "epoch:12 step:9917 [D loss: 0.022347, acc.: 100.00%] [G loss: 4.548269]\n",
      "epoch:12 step:9918 [D loss: 0.017257, acc.: 100.00%] [G loss: 4.351293]\n",
      "epoch:12 step:9919 [D loss: 0.208097, acc.: 90.62%] [G loss: 5.417684]\n",
      "epoch:12 step:9920 [D loss: 0.072304, acc.: 97.66%] [G loss: 5.292501]\n",
      "epoch:12 step:9921 [D loss: 0.006479, acc.: 100.00%] [G loss: 3.844768]\n",
      "epoch:12 step:9922 [D loss: 0.061154, acc.: 98.44%] [G loss: 3.492218]\n",
      "epoch:12 step:9923 [D loss: 0.020222, acc.: 100.00%] [G loss: 2.177393]\n",
      "epoch:12 step:9924 [D loss: 0.016679, acc.: 100.00%] [G loss: 3.161366]\n",
      "epoch:12 step:9925 [D loss: 0.016095, acc.: 100.00%] [G loss: 3.046984]\n",
      "epoch:12 step:9926 [D loss: 0.033850, acc.: 100.00%] [G loss: 2.292356]\n",
      "epoch:12 step:9927 [D loss: 0.004952, acc.: 100.00%] [G loss: 3.899859]\n",
      "epoch:12 step:9928 [D loss: 0.007159, acc.: 100.00%] [G loss: 0.970883]\n",
      "epoch:12 step:9929 [D loss: 0.017537, acc.: 100.00%] [G loss: 1.211498]\n",
      "epoch:12 step:9930 [D loss: 0.010030, acc.: 100.00%] [G loss: 2.154155]\n",
      "epoch:12 step:9931 [D loss: 0.069653, acc.: 97.66%] [G loss: 1.556951]\n",
      "epoch:12 step:9932 [D loss: 0.003454, acc.: 100.00%] [G loss: 1.918656]\n",
      "epoch:12 step:9933 [D loss: 0.103591, acc.: 99.22%] [G loss: 1.927871]\n",
      "epoch:12 step:9934 [D loss: 0.063983, acc.: 98.44%] [G loss: 0.320549]\n",
      "epoch:12 step:9935 [D loss: 0.159266, acc.: 95.31%] [G loss: 3.428068]\n",
      "epoch:12 step:9936 [D loss: 0.059038, acc.: 99.22%] [G loss: 5.016578]\n",
      "epoch:12 step:9937 [D loss: 0.272774, acc.: 90.62%] [G loss: 0.068875]\n",
      "epoch:12 step:9938 [D loss: 0.883707, acc.: 68.75%] [G loss: 7.486038]\n",
      "epoch:12 step:9939 [D loss: 0.813103, acc.: 64.84%] [G loss: 4.675710]\n",
      "epoch:12 step:9940 [D loss: 0.138496, acc.: 96.09%] [G loss: 4.397218]\n",
      "epoch:12 step:9941 [D loss: 0.007008, acc.: 100.00%] [G loss: 5.201580]\n",
      "epoch:12 step:9942 [D loss: 0.014946, acc.: 100.00%] [G loss: 3.750264]\n",
      "epoch:12 step:9943 [D loss: 0.058329, acc.: 98.44%] [G loss: 1.582441]\n",
      "epoch:12 step:9944 [D loss: 0.121211, acc.: 96.09%] [G loss: 2.947287]\n",
      "epoch:12 step:9945 [D loss: 0.151899, acc.: 96.09%] [G loss: 5.084572]\n",
      "epoch:12 step:9946 [D loss: 0.059202, acc.: 100.00%] [G loss: 1.879695]\n",
      "epoch:12 step:9947 [D loss: 0.514431, acc.: 75.00%] [G loss: 7.261916]\n",
      "epoch:12 step:9948 [D loss: 0.683879, acc.: 71.88%] [G loss: 5.461365]\n",
      "epoch:12 step:9949 [D loss: 0.101090, acc.: 97.66%] [G loss: 5.386340]\n",
      "epoch:12 step:9950 [D loss: 0.014934, acc.: 100.00%] [G loss: 5.465637]\n",
      "epoch:12 step:9951 [D loss: 0.042252, acc.: 100.00%] [G loss: 5.522333]\n",
      "epoch:12 step:9952 [D loss: 0.097151, acc.: 96.88%] [G loss: 5.706775]\n",
      "epoch:12 step:9953 [D loss: 0.017547, acc.: 100.00%] [G loss: 4.289341]\n",
      "epoch:12 step:9954 [D loss: 0.019712, acc.: 100.00%] [G loss: 4.323507]\n",
      "epoch:12 step:9955 [D loss: 0.110613, acc.: 95.31%] [G loss: 4.537583]\n",
      "epoch:12 step:9956 [D loss: 0.039458, acc.: 100.00%] [G loss: 3.561518]\n",
      "epoch:12 step:9957 [D loss: 0.055538, acc.: 99.22%] [G loss: 4.400084]\n",
      "epoch:12 step:9958 [D loss: 0.029513, acc.: 100.00%] [G loss: 4.791736]\n",
      "epoch:12 step:9959 [D loss: 0.106813, acc.: 96.09%] [G loss: 3.884726]\n",
      "epoch:12 step:9960 [D loss: 0.066176, acc.: 97.66%] [G loss: 2.566335]\n",
      "epoch:12 step:9961 [D loss: 0.054251, acc.: 99.22%] [G loss: 4.704780]\n",
      "epoch:12 step:9962 [D loss: 0.032322, acc.: 100.00%] [G loss: 2.888426]\n",
      "epoch:12 step:9963 [D loss: 0.099341, acc.: 98.44%] [G loss: 5.491982]\n",
      "epoch:12 step:9964 [D loss: 0.021825, acc.: 100.00%] [G loss: 6.297257]\n",
      "epoch:12 step:9965 [D loss: 0.842991, acc.: 63.28%] [G loss: 4.735779]\n",
      "epoch:12 step:9966 [D loss: 0.003551, acc.: 100.00%] [G loss: 6.336797]\n",
      "epoch:12 step:9967 [D loss: 0.273662, acc.: 88.28%] [G loss: 0.782438]\n",
      "epoch:12 step:9968 [D loss: 0.582209, acc.: 75.00%] [G loss: 6.550677]\n",
      "epoch:12 step:9969 [D loss: 0.417585, acc.: 76.56%] [G loss: 4.142997]\n",
      "epoch:12 step:9970 [D loss: 0.011612, acc.: 100.00%] [G loss: 3.445637]\n",
      "epoch:12 step:9971 [D loss: 0.039362, acc.: 99.22%] [G loss: 4.924440]\n",
      "epoch:12 step:9972 [D loss: 0.007967, acc.: 100.00%] [G loss: 2.411406]\n",
      "epoch:12 step:9973 [D loss: 0.061621, acc.: 100.00%] [G loss: 2.588532]\n",
      "epoch:12 step:9974 [D loss: 0.046334, acc.: 99.22%] [G loss: 4.390265]\n",
      "epoch:12 step:9975 [D loss: 0.024102, acc.: 100.00%] [G loss: 2.669595]\n",
      "epoch:12 step:9976 [D loss: 5.350173, acc.: 14.06%] [G loss: 6.649297]\n",
      "epoch:12 step:9977 [D loss: 1.804527, acc.: 51.56%] [G loss: 4.622468]\n",
      "epoch:12 step:9978 [D loss: 0.959339, acc.: 60.16%] [G loss: 0.651030]\n",
      "epoch:12 step:9979 [D loss: 1.121443, acc.: 58.59%] [G loss: 3.579659]\n",
      "epoch:12 step:9980 [D loss: 0.476981, acc.: 74.22%] [G loss: 3.043171]\n",
      "epoch:12 step:9981 [D loss: 0.624250, acc.: 70.31%] [G loss: 1.567626]\n",
      "epoch:12 step:9982 [D loss: 0.417945, acc.: 83.59%] [G loss: 3.351530]\n",
      "epoch:12 step:9983 [D loss: 0.143143, acc.: 96.88%] [G loss: 1.626278]\n",
      "epoch:12 step:9984 [D loss: 0.207434, acc.: 96.09%] [G loss: 1.991757]\n",
      "epoch:12 step:9985 [D loss: 0.371349, acc.: 88.28%] [G loss: 2.158257]\n",
      "epoch:12 step:9986 [D loss: 0.136637, acc.: 96.88%] [G loss: 2.069800]\n",
      "epoch:12 step:9987 [D loss: 0.137732, acc.: 97.66%] [G loss: 2.733523]\n",
      "epoch:12 step:9988 [D loss: 0.242689, acc.: 91.41%] [G loss: 2.584377]\n",
      "epoch:12 step:9989 [D loss: 0.070404, acc.: 99.22%] [G loss: 3.042755]\n",
      "epoch:12 step:9990 [D loss: 0.182917, acc.: 92.19%] [G loss: 2.015634]\n",
      "epoch:12 step:9991 [D loss: 0.224842, acc.: 94.53%] [G loss: 1.720953]\n",
      "epoch:12 step:9992 [D loss: 0.314296, acc.: 85.94%] [G loss: 3.143795]\n",
      "epoch:12 step:9993 [D loss: 0.275214, acc.: 87.50%] [G loss: 3.540577]\n",
      "epoch:12 step:9994 [D loss: 0.414905, acc.: 80.47%] [G loss: 2.084949]\n",
      "epoch:12 step:9995 [D loss: 0.347885, acc.: 85.16%] [G loss: 1.905646]\n",
      "epoch:12 step:9996 [D loss: 0.187598, acc.: 94.53%] [G loss: 3.031466]\n",
      "epoch:12 step:9997 [D loss: 0.070852, acc.: 97.66%] [G loss: 4.193816]\n",
      "epoch:12 step:9998 [D loss: 0.264119, acc.: 89.06%] [G loss: 2.417209]\n",
      "epoch:12 step:9999 [D loss: 0.438176, acc.: 76.56%] [G loss: 3.594606]\n",
      "epoch:12 step:10000 [D loss: 0.216422, acc.: 92.19%] [G loss: 2.883786]\n",
      "epoch:12 step:10001 [D loss: 0.227926, acc.: 90.62%] [G loss: 3.099884]\n",
      "epoch:12 step:10002 [D loss: 0.037981, acc.: 100.00%] [G loss: 3.292949]\n",
      "epoch:12 step:10003 [D loss: 0.778869, acc.: 60.16%] [G loss: 3.258501]\n",
      "epoch:12 step:10004 [D loss: 0.063404, acc.: 99.22%] [G loss: 4.531770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:10005 [D loss: 0.075316, acc.: 98.44%] [G loss: 3.969239]\n",
      "epoch:12 step:10006 [D loss: 0.072681, acc.: 98.44%] [G loss: 1.499868]\n",
      "epoch:12 step:10007 [D loss: 0.221235, acc.: 92.97%] [G loss: 3.787444]\n",
      "epoch:12 step:10008 [D loss: 0.065321, acc.: 100.00%] [G loss: 3.916489]\n",
      "epoch:12 step:10009 [D loss: 0.055579, acc.: 98.44%] [G loss: 3.421809]\n",
      "epoch:12 step:10010 [D loss: 0.087827, acc.: 97.66%] [G loss: 3.079521]\n",
      "epoch:12 step:10011 [D loss: 0.082116, acc.: 99.22%] [G loss: 1.698567]\n",
      "epoch:12 step:10012 [D loss: 0.058797, acc.: 99.22%] [G loss: 3.836440]\n",
      "epoch:12 step:10013 [D loss: 0.019743, acc.: 100.00%] [G loss: 3.134114]\n",
      "epoch:12 step:10014 [D loss: 0.035715, acc.: 100.00%] [G loss: 1.016100]\n",
      "epoch:12 step:10015 [D loss: 0.101128, acc.: 96.88%] [G loss: 2.473491]\n",
      "epoch:12 step:10016 [D loss: 0.113758, acc.: 98.44%] [G loss: 3.270975]\n",
      "epoch:12 step:10017 [D loss: 0.076769, acc.: 99.22%] [G loss: 3.374851]\n",
      "epoch:12 step:10018 [D loss: 1.368670, acc.: 37.50%] [G loss: 5.968442]\n",
      "epoch:12 step:10019 [D loss: 0.604707, acc.: 69.53%] [G loss: 5.123708]\n",
      "epoch:12 step:10020 [D loss: 0.217293, acc.: 90.62%] [G loss: 2.379099]\n",
      "epoch:12 step:10021 [D loss: 0.123866, acc.: 96.88%] [G loss: 1.846406]\n",
      "epoch:12 step:10022 [D loss: 0.042384, acc.: 100.00%] [G loss: 2.205703]\n",
      "epoch:12 step:10023 [D loss: 0.066501, acc.: 97.66%] [G loss: 1.684423]\n",
      "epoch:12 step:10024 [D loss: 0.038280, acc.: 100.00%] [G loss: 1.867286]\n",
      "epoch:12 step:10025 [D loss: 0.150769, acc.: 95.31%] [G loss: 3.354892]\n",
      "epoch:12 step:10026 [D loss: 0.086551, acc.: 97.66%] [G loss: 3.145240]\n",
      "epoch:12 step:10027 [D loss: 0.072965, acc.: 100.00%] [G loss: 1.968768]\n",
      "epoch:12 step:10028 [D loss: 0.182268, acc.: 93.75%] [G loss: 1.833753]\n",
      "epoch:12 step:10029 [D loss: 0.103490, acc.: 97.66%] [G loss: 3.199428]\n",
      "epoch:12 step:10030 [D loss: 0.025586, acc.: 100.00%] [G loss: 3.987540]\n",
      "epoch:12 step:10031 [D loss: 0.129556, acc.: 96.09%] [G loss: 2.691879]\n",
      "epoch:12 step:10032 [D loss: 0.102074, acc.: 99.22%] [G loss: 4.627872]\n",
      "epoch:12 step:10033 [D loss: 0.047533, acc.: 100.00%] [G loss: 4.928067]\n",
      "epoch:12 step:10034 [D loss: 0.096867, acc.: 97.66%] [G loss: 4.289239]\n",
      "epoch:12 step:10035 [D loss: 0.049949, acc.: 99.22%] [G loss: 3.916114]\n",
      "epoch:12 step:10036 [D loss: 0.040997, acc.: 100.00%] [G loss: 3.196799]\n",
      "epoch:12 step:10037 [D loss: 0.074726, acc.: 100.00%] [G loss: 4.039255]\n",
      "epoch:12 step:10038 [D loss: 0.020523, acc.: 100.00%] [G loss: 3.958364]\n",
      "epoch:12 step:10039 [D loss: 0.040655, acc.: 100.00%] [G loss: 3.600319]\n",
      "epoch:12 step:10040 [D loss: 0.234464, acc.: 91.41%] [G loss: 5.067839]\n",
      "epoch:12 step:10041 [D loss: 0.102258, acc.: 96.09%] [G loss: 4.075581]\n",
      "epoch:12 step:10042 [D loss: 0.191157, acc.: 93.75%] [G loss: 2.405324]\n",
      "epoch:12 step:10043 [D loss: 0.045475, acc.: 100.00%] [G loss: 3.878137]\n",
      "epoch:12 step:10044 [D loss: 0.036122, acc.: 100.00%] [G loss: 3.574781]\n",
      "epoch:12 step:10045 [D loss: 0.089700, acc.: 98.44%] [G loss: 2.768574]\n",
      "epoch:12 step:10046 [D loss: 0.028215, acc.: 100.00%] [G loss: 3.755446]\n",
      "epoch:12 step:10047 [D loss: 0.044266, acc.: 99.22%] [G loss: 3.956084]\n",
      "epoch:12 step:10048 [D loss: 0.038959, acc.: 99.22%] [G loss: 3.348104]\n",
      "epoch:12 step:10049 [D loss: 0.048992, acc.: 100.00%] [G loss: 2.718955]\n",
      "epoch:12 step:10050 [D loss: 0.068522, acc.: 100.00%] [G loss: 2.111488]\n",
      "epoch:12 step:10051 [D loss: 0.049841, acc.: 100.00%] [G loss: 3.003035]\n",
      "epoch:12 step:10052 [D loss: 0.025890, acc.: 100.00%] [G loss: 3.454827]\n",
      "epoch:12 step:10053 [D loss: 0.122080, acc.: 97.66%] [G loss: 2.712728]\n",
      "epoch:12 step:10054 [D loss: 0.038209, acc.: 100.00%] [G loss: 3.762273]\n",
      "epoch:12 step:10055 [D loss: 0.112982, acc.: 99.22%] [G loss: 3.204731]\n",
      "epoch:12 step:10056 [D loss: 0.250955, acc.: 89.84%] [G loss: 5.431646]\n",
      "epoch:12 step:10057 [D loss: 1.206985, acc.: 53.12%] [G loss: 2.981258]\n",
      "epoch:12 step:10058 [D loss: 0.226287, acc.: 87.50%] [G loss: 2.211981]\n",
      "epoch:12 step:10059 [D loss: 0.060702, acc.: 99.22%] [G loss: 3.665439]\n",
      "epoch:12 step:10060 [D loss: 0.089823, acc.: 96.88%] [G loss: 3.570405]\n",
      "epoch:12 step:10061 [D loss: 0.028352, acc.: 100.00%] [G loss: 1.317010]\n",
      "epoch:12 step:10062 [D loss: 0.724350, acc.: 60.94%] [G loss: 6.218522]\n",
      "epoch:12 step:10063 [D loss: 0.726298, acc.: 64.84%] [G loss: 3.934511]\n",
      "epoch:12 step:10064 [D loss: 0.301988, acc.: 86.72%] [G loss: 4.135950]\n",
      "epoch:12 step:10065 [D loss: 0.071369, acc.: 98.44%] [G loss: 3.362424]\n",
      "epoch:12 step:10066 [D loss: 0.016285, acc.: 100.00%] [G loss: 3.378976]\n",
      "epoch:12 step:10067 [D loss: 0.016356, acc.: 100.00%] [G loss: 2.855710]\n",
      "epoch:12 step:10068 [D loss: 0.026873, acc.: 100.00%] [G loss: 2.982362]\n",
      "epoch:12 step:10069 [D loss: 0.017250, acc.: 100.00%] [G loss: 2.210256]\n",
      "epoch:12 step:10070 [D loss: 0.088410, acc.: 100.00%] [G loss: 2.031199]\n",
      "epoch:12 step:10071 [D loss: 0.027279, acc.: 100.00%] [G loss: 3.522638]\n",
      "epoch:12 step:10072 [D loss: 0.082880, acc.: 96.88%] [G loss: 1.905170]\n",
      "epoch:12 step:10073 [D loss: 0.017330, acc.: 100.00%] [G loss: 2.394289]\n",
      "epoch:12 step:10074 [D loss: 0.120060, acc.: 97.66%] [G loss: 2.931537]\n",
      "epoch:12 step:10075 [D loss: 0.484195, acc.: 75.78%] [G loss: 5.156054]\n",
      "epoch:12 step:10076 [D loss: 0.128351, acc.: 95.31%] [G loss: 4.341813]\n",
      "epoch:12 step:10077 [D loss: 0.041562, acc.: 99.22%] [G loss: 3.337033]\n",
      "epoch:12 step:10078 [D loss: 0.035180, acc.: 100.00%] [G loss: 3.662005]\n",
      "epoch:12 step:10079 [D loss: 0.013708, acc.: 100.00%] [G loss: 4.080290]\n",
      "epoch:12 step:10080 [D loss: 0.047399, acc.: 100.00%] [G loss: 3.796658]\n",
      "epoch:12 step:10081 [D loss: 0.013705, acc.: 100.00%] [G loss: 4.209906]\n",
      "epoch:12 step:10082 [D loss: 0.041883, acc.: 100.00%] [G loss: 3.859652]\n",
      "epoch:12 step:10083 [D loss: 0.030208, acc.: 100.00%] [G loss: 2.987233]\n",
      "epoch:12 step:10084 [D loss: 0.023602, acc.: 100.00%] [G loss: 3.594591]\n",
      "epoch:12 step:10085 [D loss: 0.226252, acc.: 90.62%] [G loss: 1.555670]\n",
      "epoch:12 step:10086 [D loss: 0.011728, acc.: 100.00%] [G loss: 1.224810]\n",
      "epoch:12 step:10087 [D loss: 0.119759, acc.: 96.09%] [G loss: 1.114511]\n",
      "epoch:12 step:10088 [D loss: 0.384042, acc.: 77.34%] [G loss: 4.828167]\n",
      "epoch:12 step:10089 [D loss: 0.710617, acc.: 65.62%] [G loss: 1.882783]\n",
      "epoch:12 step:10090 [D loss: 0.046686, acc.: 99.22%] [G loss: 1.685498]\n",
      "epoch:12 step:10091 [D loss: 0.114495, acc.: 96.09%] [G loss: 2.386946]\n",
      "epoch:12 step:10092 [D loss: 0.028905, acc.: 100.00%] [G loss: 2.990034]\n",
      "epoch:12 step:10093 [D loss: 0.012146, acc.: 100.00%] [G loss: 2.475493]\n",
      "epoch:12 step:10094 [D loss: 0.017726, acc.: 100.00%] [G loss: 2.772511]\n",
      "epoch:12 step:10095 [D loss: 0.032960, acc.: 100.00%] [G loss: 3.759043]\n",
      "epoch:12 step:10096 [D loss: 0.060825, acc.: 100.00%] [G loss: 2.171384]\n",
      "epoch:12 step:10097 [D loss: 0.189628, acc.: 93.75%] [G loss: 2.958297]\n",
      "epoch:12 step:10098 [D loss: 0.021788, acc.: 100.00%] [G loss: 3.727496]\n",
      "epoch:12 step:10099 [D loss: 0.069040, acc.: 99.22%] [G loss: 3.013722]\n",
      "epoch:12 step:10100 [D loss: 0.044160, acc.: 100.00%] [G loss: 2.624604]\n",
      "epoch:12 step:10101 [D loss: 0.038083, acc.: 100.00%] [G loss: 3.095999]\n",
      "epoch:12 step:10102 [D loss: 0.017827, acc.: 100.00%] [G loss: 3.480633]\n",
      "epoch:12 step:10103 [D loss: 0.041959, acc.: 100.00%] [G loss: 3.823654]\n",
      "epoch:12 step:10104 [D loss: 0.052299, acc.: 99.22%] [G loss: 3.988785]\n",
      "epoch:12 step:10105 [D loss: 0.050500, acc.: 99.22%] [G loss: 2.491072]\n",
      "epoch:12 step:10106 [D loss: 0.024595, acc.: 99.22%] [G loss: 2.938222]\n",
      "epoch:12 step:10107 [D loss: 0.378753, acc.: 80.47%] [G loss: 5.001259]\n",
      "epoch:12 step:10108 [D loss: 0.418239, acc.: 80.47%] [G loss: 1.993945]\n",
      "epoch:12 step:10109 [D loss: 0.119829, acc.: 96.09%] [G loss: 3.280853]\n",
      "epoch:12 step:10110 [D loss: 0.048407, acc.: 100.00%] [G loss: 3.225803]\n",
      "epoch:12 step:10111 [D loss: 0.048053, acc.: 98.44%] [G loss: 4.629648]\n",
      "epoch:12 step:10112 [D loss: 0.099659, acc.: 97.66%] [G loss: 4.645378]\n",
      "epoch:12 step:10113 [D loss: 0.104046, acc.: 96.88%] [G loss: 2.847262]\n",
      "epoch:12 step:10114 [D loss: 0.023491, acc.: 99.22%] [G loss: 4.426883]\n",
      "epoch:12 step:10115 [D loss: 0.034288, acc.: 100.00%] [G loss: 4.360120]\n",
      "epoch:12 step:10116 [D loss: 0.047777, acc.: 100.00%] [G loss: 4.345697]\n",
      "epoch:12 step:10117 [D loss: 0.028051, acc.: 100.00%] [G loss: 5.555304]\n",
      "epoch:12 step:10118 [D loss: 0.022501, acc.: 100.00%] [G loss: 3.605525]\n",
      "epoch:12 step:10119 [D loss: 0.164800, acc.: 96.09%] [G loss: 2.416940]\n",
      "epoch:12 step:10120 [D loss: 0.026949, acc.: 100.00%] [G loss: 3.559420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12 step:10121 [D loss: 0.041822, acc.: 99.22%] [G loss: 4.188031]\n",
      "epoch:12 step:10122 [D loss: 0.054087, acc.: 99.22%] [G loss: 3.541761]\n",
      "epoch:12 step:10123 [D loss: 0.012191, acc.: 100.00%] [G loss: 3.353222]\n",
      "epoch:12 step:10124 [D loss: 0.108130, acc.: 96.88%] [G loss: 4.208831]\n",
      "epoch:12 step:10125 [D loss: 0.093968, acc.: 99.22%] [G loss: 1.349805]\n",
      "epoch:12 step:10126 [D loss: 0.263811, acc.: 91.41%] [G loss: 5.892504]\n",
      "epoch:12 step:10127 [D loss: 0.062869, acc.: 98.44%] [G loss: 5.164100]\n",
      "epoch:12 step:10128 [D loss: 0.137747, acc.: 94.53%] [G loss: 3.860078]\n",
      "epoch:12 step:10129 [D loss: 0.031131, acc.: 98.44%] [G loss: 4.166698]\n",
      "epoch:12 step:10130 [D loss: 0.007591, acc.: 100.00%] [G loss: 4.528153]\n",
      "epoch:12 step:10131 [D loss: 0.114022, acc.: 99.22%] [G loss: 4.866838]\n",
      "epoch:12 step:10132 [D loss: 0.085039, acc.: 96.09%] [G loss: 3.870948]\n",
      "epoch:12 step:10133 [D loss: 0.026662, acc.: 100.00%] [G loss: 3.503641]\n",
      "epoch:12 step:10134 [D loss: 0.782308, acc.: 62.50%] [G loss: 8.520391]\n",
      "epoch:12 step:10135 [D loss: 0.943290, acc.: 57.81%] [G loss: 4.610690]\n",
      "epoch:12 step:10136 [D loss: 0.039178, acc.: 99.22%] [G loss: 4.439414]\n",
      "epoch:12 step:10137 [D loss: 0.062672, acc.: 99.22%] [G loss: 4.465474]\n",
      "epoch:12 step:10138 [D loss: 0.035003, acc.: 98.44%] [G loss: 5.427750]\n",
      "epoch:12 step:10139 [D loss: 0.046773, acc.: 99.22%] [G loss: 4.705640]\n",
      "epoch:12 step:10140 [D loss: 0.042604, acc.: 100.00%] [G loss: 4.714796]\n",
      "epoch:12 step:10141 [D loss: 0.023407, acc.: 100.00%] [G loss: 4.636882]\n",
      "epoch:12 step:10142 [D loss: 0.016647, acc.: 100.00%] [G loss: 4.145632]\n",
      "epoch:12 step:10143 [D loss: 0.061665, acc.: 100.00%] [G loss: 3.825077]\n",
      "epoch:12 step:10144 [D loss: 0.066392, acc.: 100.00%] [G loss: 4.458109]\n",
      "epoch:12 step:10145 [D loss: 0.020744, acc.: 100.00%] [G loss: 4.372098]\n",
      "epoch:12 step:10146 [D loss: 0.088477, acc.: 97.66%] [G loss: 3.990077]\n",
      "epoch:12 step:10147 [D loss: 0.016163, acc.: 99.22%] [G loss: 4.673273]\n",
      "epoch:12 step:10148 [D loss: 0.011949, acc.: 100.00%] [G loss: 4.361211]\n",
      "epoch:12 step:10149 [D loss: 0.119882, acc.: 96.88%] [G loss: 4.836170]\n",
      "epoch:12 step:10150 [D loss: 0.012615, acc.: 100.00%] [G loss: 5.264410]\n",
      "epoch:12 step:10151 [D loss: 0.115649, acc.: 96.09%] [G loss: 4.428182]\n",
      "epoch:12 step:10152 [D loss: 0.047779, acc.: 100.00%] [G loss: 0.794045]\n",
      "epoch:12 step:10153 [D loss: 0.034781, acc.: 100.00%] [G loss: 5.273785]\n",
      "epoch:13 step:10154 [D loss: 0.017683, acc.: 100.00%] [G loss: 5.512589]\n",
      "epoch:13 step:10155 [D loss: 0.020874, acc.: 100.00%] [G loss: 3.312644]\n",
      "epoch:13 step:10156 [D loss: 0.055290, acc.: 99.22%] [G loss: 3.769964]\n",
      "epoch:13 step:10157 [D loss: 0.045687, acc.: 100.00%] [G loss: 0.341958]\n",
      "epoch:13 step:10158 [D loss: 0.011893, acc.: 100.00%] [G loss: 5.682722]\n",
      "epoch:13 step:10159 [D loss: 0.026503, acc.: 99.22%] [G loss: 4.570087]\n",
      "epoch:13 step:10160 [D loss: 0.265752, acc.: 91.41%] [G loss: 6.605329]\n",
      "epoch:13 step:10161 [D loss: 0.078464, acc.: 97.66%] [G loss: 7.548502]\n",
      "epoch:13 step:10162 [D loss: 0.009475, acc.: 100.00%] [G loss: 3.570726]\n",
      "epoch:13 step:10163 [D loss: 0.129346, acc.: 93.75%] [G loss: 4.727843]\n",
      "epoch:13 step:10164 [D loss: 0.326444, acc.: 85.16%] [G loss: 6.640219]\n",
      "epoch:13 step:10165 [D loss: 0.348725, acc.: 82.81%] [G loss: 5.778629]\n",
      "epoch:13 step:10166 [D loss: 0.001894, acc.: 100.00%] [G loss: 4.526675]\n",
      "epoch:13 step:10167 [D loss: 0.010128, acc.: 100.00%] [G loss: 2.191033]\n",
      "epoch:13 step:10168 [D loss: 0.005524, acc.: 100.00%] [G loss: 3.601654]\n",
      "epoch:13 step:10169 [D loss: 0.008937, acc.: 100.00%] [G loss: 3.156100]\n",
      "epoch:13 step:10170 [D loss: 0.013207, acc.: 100.00%] [G loss: 1.243663]\n",
      "epoch:13 step:10171 [D loss: 0.024838, acc.: 100.00%] [G loss: 0.173842]\n",
      "epoch:13 step:10172 [D loss: 0.149072, acc.: 93.75%] [G loss: 3.369889]\n",
      "epoch:13 step:10173 [D loss: 0.020071, acc.: 100.00%] [G loss: 1.862867]\n",
      "epoch:13 step:10174 [D loss: 0.040169, acc.: 99.22%] [G loss: 4.308887]\n",
      "epoch:13 step:10175 [D loss: 0.140999, acc.: 94.53%] [G loss: 0.516684]\n",
      "epoch:13 step:10176 [D loss: 0.027396, acc.: 100.00%] [G loss: 3.191058]\n",
      "epoch:13 step:10177 [D loss: 0.069213, acc.: 99.22%] [G loss: 4.432482]\n",
      "epoch:13 step:10178 [D loss: 0.048505, acc.: 99.22%] [G loss: 0.832006]\n",
      "epoch:13 step:10179 [D loss: 0.003161, acc.: 100.00%] [G loss: 3.058367]\n",
      "epoch:13 step:10180 [D loss: 0.092904, acc.: 96.09%] [G loss: 3.927149]\n",
      "epoch:13 step:10181 [D loss: 0.146071, acc.: 96.88%] [G loss: 3.883043]\n",
      "epoch:13 step:10182 [D loss: 0.042317, acc.: 98.44%] [G loss: 1.864938]\n",
      "epoch:13 step:10183 [D loss: 0.100404, acc.: 96.88%] [G loss: 5.767567]\n",
      "epoch:13 step:10184 [D loss: 0.024634, acc.: 100.00%] [G loss: 3.853251]\n",
      "epoch:13 step:10185 [D loss: 0.070989, acc.: 98.44%] [G loss: 5.030650]\n",
      "epoch:13 step:10186 [D loss: 0.138197, acc.: 94.53%] [G loss: 6.260995]\n",
      "epoch:13 step:10187 [D loss: 0.005387, acc.: 100.00%] [G loss: 6.573937]\n",
      "epoch:13 step:10188 [D loss: 0.231644, acc.: 89.06%] [G loss: 2.180921]\n",
      "epoch:13 step:10189 [D loss: 0.979311, acc.: 65.62%] [G loss: 9.458604]\n",
      "epoch:13 step:10190 [D loss: 2.760281, acc.: 50.00%] [G loss: 6.833786]\n",
      "epoch:13 step:10191 [D loss: 0.255448, acc.: 92.97%] [G loss: 3.602660]\n",
      "epoch:13 step:10192 [D loss: 0.115482, acc.: 96.88%] [G loss: 3.859639]\n",
      "epoch:13 step:10193 [D loss: 0.023732, acc.: 100.00%] [G loss: 4.254674]\n",
      "epoch:13 step:10194 [D loss: 0.043313, acc.: 100.00%] [G loss: 4.557248]\n",
      "epoch:13 step:10195 [D loss: 0.044198, acc.: 100.00%] [G loss: 3.896590]\n",
      "epoch:13 step:10196 [D loss: 0.041573, acc.: 99.22%] [G loss: 3.513953]\n",
      "epoch:13 step:10197 [D loss: 0.056838, acc.: 99.22%] [G loss: 3.359836]\n",
      "epoch:13 step:10198 [D loss: 0.209831, acc.: 92.19%] [G loss: 5.950234]\n",
      "epoch:13 step:10199 [D loss: 1.846112, acc.: 23.44%] [G loss: 5.693225]\n",
      "epoch:13 step:10200 [D loss: 0.078447, acc.: 99.22%] [G loss: 5.837778]\n",
      "epoch:13 step:10201 [D loss: 0.218468, acc.: 89.06%] [G loss: 3.556490]\n",
      "epoch:13 step:10202 [D loss: 0.232437, acc.: 90.62%] [G loss: 4.523622]\n",
      "epoch:13 step:10203 [D loss: 0.055798, acc.: 99.22%] [G loss: 4.472649]\n",
      "epoch:13 step:10204 [D loss: 0.014925, acc.: 100.00%] [G loss: 5.248423]\n",
      "epoch:13 step:10205 [D loss: 0.070997, acc.: 97.66%] [G loss: 4.733707]\n",
      "epoch:13 step:10206 [D loss: 0.095354, acc.: 95.31%] [G loss: 4.917562]\n",
      "epoch:13 step:10207 [D loss: 0.038441, acc.: 99.22%] [G loss: 4.487123]\n",
      "epoch:13 step:10208 [D loss: 0.025543, acc.: 100.00%] [G loss: 4.063881]\n",
      "epoch:13 step:10209 [D loss: 0.215450, acc.: 94.53%] [G loss: 3.053050]\n",
      "epoch:13 step:10210 [D loss: 0.038985, acc.: 100.00%] [G loss: 1.844295]\n",
      "epoch:13 step:10211 [D loss: 0.139061, acc.: 94.53%] [G loss: 4.566606]\n",
      "epoch:13 step:10212 [D loss: 0.040822, acc.: 100.00%] [G loss: 3.946446]\n",
      "epoch:13 step:10213 [D loss: 0.095151, acc.: 96.09%] [G loss: 2.194809]\n",
      "epoch:13 step:10214 [D loss: 0.275720, acc.: 88.28%] [G loss: 3.244744]\n",
      "epoch:13 step:10215 [D loss: 0.067424, acc.: 99.22%] [G loss: 6.139104]\n",
      "epoch:13 step:10216 [D loss: 0.090871, acc.: 97.66%] [G loss: 5.392336]\n",
      "epoch:13 step:10217 [D loss: 0.086875, acc.: 99.22%] [G loss: 3.534994]\n",
      "epoch:13 step:10218 [D loss: 0.095258, acc.: 98.44%] [G loss: 0.152254]\n",
      "epoch:13 step:10219 [D loss: 0.019166, acc.: 100.00%] [G loss: 4.064720]\n",
      "epoch:13 step:10220 [D loss: 0.034905, acc.: 100.00%] [G loss: 2.260086]\n",
      "epoch:13 step:10221 [D loss: 0.047653, acc.: 100.00%] [G loss: 1.585250]\n",
      "epoch:13 step:10222 [D loss: 0.031397, acc.: 100.00%] [G loss: 4.424198]\n",
      "epoch:13 step:10223 [D loss: 0.038076, acc.: 99.22%] [G loss: 5.132041]\n",
      "epoch:13 step:10224 [D loss: 0.027091, acc.: 100.00%] [G loss: 0.629041]\n",
      "epoch:13 step:10225 [D loss: 0.083797, acc.: 98.44%] [G loss: 4.820948]\n",
      "epoch:13 step:10226 [D loss: 0.173752, acc.: 93.75%] [G loss: 4.558225]\n",
      "epoch:13 step:10227 [D loss: 0.084680, acc.: 98.44%] [G loss: 5.974384]\n",
      "epoch:13 step:10228 [D loss: 0.019495, acc.: 100.00%] [G loss: 6.042950]\n",
      "epoch:13 step:10229 [D loss: 1.655604, acc.: 31.25%] [G loss: 7.589138]\n",
      "epoch:13 step:10230 [D loss: 2.572392, acc.: 50.78%] [G loss: 4.014892]\n",
      "epoch:13 step:10231 [D loss: 1.686701, acc.: 35.94%] [G loss: 3.251808]\n",
      "epoch:13 step:10232 [D loss: 0.745332, acc.: 70.31%] [G loss: 3.304458]\n",
      "epoch:13 step:10233 [D loss: 0.194345, acc.: 93.75%] [G loss: 2.653155]\n",
      "epoch:13 step:10234 [D loss: 0.368860, acc.: 87.50%] [G loss: 2.551405]\n",
      "epoch:13 step:10235 [D loss: 0.197230, acc.: 95.31%] [G loss: 2.424890]\n",
      "epoch:13 step:10236 [D loss: 0.227149, acc.: 91.41%] [G loss: 2.533868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10237 [D loss: 0.203653, acc.: 96.88%] [G loss: 2.708490]\n",
      "epoch:13 step:10238 [D loss: 0.120607, acc.: 99.22%] [G loss: 2.343883]\n",
      "epoch:13 step:10239 [D loss: 0.195190, acc.: 96.88%] [G loss: 2.482775]\n",
      "epoch:13 step:10240 [D loss: 0.103986, acc.: 98.44%] [G loss: 2.901168]\n",
      "epoch:13 step:10241 [D loss: 0.472548, acc.: 75.78%] [G loss: 2.709651]\n",
      "epoch:13 step:10242 [D loss: 0.086496, acc.: 99.22%] [G loss: 2.866790]\n",
      "epoch:13 step:10243 [D loss: 0.192241, acc.: 94.53%] [G loss: 2.360712]\n",
      "epoch:13 step:10244 [D loss: 0.038129, acc.: 100.00%] [G loss: 3.420164]\n",
      "epoch:13 step:10245 [D loss: 0.070795, acc.: 99.22%] [G loss: 2.815383]\n",
      "epoch:13 step:10246 [D loss: 0.040230, acc.: 99.22%] [G loss: 3.643390]\n",
      "epoch:13 step:10247 [D loss: 0.238708, acc.: 92.19%] [G loss: 3.925998]\n",
      "epoch:13 step:10248 [D loss: 0.026485, acc.: 100.00%] [G loss: 3.577154]\n",
      "epoch:13 step:10249 [D loss: 0.079321, acc.: 98.44%] [G loss: 2.516536]\n",
      "epoch:13 step:10250 [D loss: 0.054826, acc.: 100.00%] [G loss: 2.728597]\n",
      "epoch:13 step:10251 [D loss: 0.031191, acc.: 100.00%] [G loss: 3.909913]\n",
      "epoch:13 step:10252 [D loss: 0.075026, acc.: 98.44%] [G loss: 2.670546]\n",
      "epoch:13 step:10253 [D loss: 0.171394, acc.: 94.53%] [G loss: 3.321597]\n",
      "epoch:13 step:10254 [D loss: 0.041809, acc.: 99.22%] [G loss: 3.436418]\n",
      "epoch:13 step:10255 [D loss: 0.100752, acc.: 96.09%] [G loss: 2.090044]\n",
      "epoch:13 step:10256 [D loss: 0.043998, acc.: 100.00%] [G loss: 2.748678]\n",
      "epoch:13 step:10257 [D loss: 0.059695, acc.: 99.22%] [G loss: 2.802495]\n",
      "epoch:13 step:10258 [D loss: 0.100273, acc.: 98.44%] [G loss: 4.430162]\n",
      "epoch:13 step:10259 [D loss: 0.095584, acc.: 100.00%] [G loss: 3.780137]\n",
      "epoch:13 step:10260 [D loss: 0.095387, acc.: 98.44%] [G loss: 3.148086]\n",
      "epoch:13 step:10261 [D loss: 0.105453, acc.: 96.09%] [G loss: 4.550430]\n",
      "epoch:13 step:10262 [D loss: 0.090384, acc.: 97.66%] [G loss: 4.253284]\n",
      "epoch:13 step:10263 [D loss: 0.034579, acc.: 100.00%] [G loss: 3.441308]\n",
      "epoch:13 step:10264 [D loss: 0.168147, acc.: 93.75%] [G loss: 4.662280]\n",
      "epoch:13 step:10265 [D loss: 0.030606, acc.: 100.00%] [G loss: 5.346931]\n",
      "epoch:13 step:10266 [D loss: 0.070785, acc.: 100.00%] [G loss: 2.542482]\n",
      "epoch:13 step:10267 [D loss: 0.084678, acc.: 98.44%] [G loss: 3.747001]\n",
      "epoch:13 step:10268 [D loss: 0.021321, acc.: 100.00%] [G loss: 3.966932]\n",
      "epoch:13 step:10269 [D loss: 0.469490, acc.: 79.69%] [G loss: 5.288010]\n",
      "epoch:13 step:10270 [D loss: 0.052870, acc.: 98.44%] [G loss: 5.323977]\n",
      "epoch:13 step:10271 [D loss: 0.171885, acc.: 91.41%] [G loss: 3.020187]\n",
      "epoch:13 step:10272 [D loss: 0.068441, acc.: 97.66%] [G loss: 3.521973]\n",
      "epoch:13 step:10273 [D loss: 0.011079, acc.: 100.00%] [G loss: 4.205578]\n",
      "epoch:13 step:10274 [D loss: 0.011542, acc.: 100.00%] [G loss: 3.901845]\n",
      "epoch:13 step:10275 [D loss: 0.024803, acc.: 100.00%] [G loss: 3.224554]\n",
      "epoch:13 step:10276 [D loss: 0.041311, acc.: 99.22%] [G loss: 2.877810]\n",
      "epoch:13 step:10277 [D loss: 0.195033, acc.: 94.53%] [G loss: 4.947960]\n",
      "epoch:13 step:10278 [D loss: 0.139446, acc.: 96.09%] [G loss: 4.576310]\n",
      "epoch:13 step:10279 [D loss: 0.014447, acc.: 100.00%] [G loss: 4.106940]\n",
      "epoch:13 step:10280 [D loss: 0.019752, acc.: 100.00%] [G loss: 2.767844]\n",
      "epoch:13 step:10281 [D loss: 0.019012, acc.: 100.00%] [G loss: 3.879307]\n",
      "epoch:13 step:10282 [D loss: 0.053430, acc.: 99.22%] [G loss: 4.148491]\n",
      "epoch:13 step:10283 [D loss: 0.009591, acc.: 100.00%] [G loss: 3.607971]\n",
      "epoch:13 step:10284 [D loss: 0.014361, acc.: 100.00%] [G loss: 2.481520]\n",
      "epoch:13 step:10285 [D loss: 0.318624, acc.: 85.16%] [G loss: 4.801209]\n",
      "epoch:13 step:10286 [D loss: 0.067435, acc.: 97.66%] [G loss: 6.610586]\n",
      "epoch:13 step:10287 [D loss: 0.131631, acc.: 93.75%] [G loss: 3.430876]\n",
      "epoch:13 step:10288 [D loss: 0.063369, acc.: 96.88%] [G loss: 4.686799]\n",
      "epoch:13 step:10289 [D loss: 0.011589, acc.: 100.00%] [G loss: 4.889099]\n",
      "epoch:13 step:10290 [D loss: 0.011426, acc.: 100.00%] [G loss: 3.289010]\n",
      "epoch:13 step:10291 [D loss: 0.006555, acc.: 100.00%] [G loss: 3.873799]\n",
      "epoch:13 step:10292 [D loss: 0.035422, acc.: 99.22%] [G loss: 3.897672]\n",
      "epoch:13 step:10293 [D loss: 0.120601, acc.: 96.88%] [G loss: 5.410870]\n",
      "epoch:13 step:10294 [D loss: 0.070513, acc.: 96.88%] [G loss: 3.771842]\n",
      "epoch:13 step:10295 [D loss: 0.016364, acc.: 100.00%] [G loss: 4.489534]\n",
      "epoch:13 step:10296 [D loss: 0.026251, acc.: 100.00%] [G loss: 2.111050]\n",
      "epoch:13 step:10297 [D loss: 0.035453, acc.: 100.00%] [G loss: 4.621379]\n",
      "epoch:13 step:10298 [D loss: 0.061593, acc.: 98.44%] [G loss: 5.096874]\n",
      "epoch:13 step:10299 [D loss: 0.027839, acc.: 100.00%] [G loss: 4.194381]\n",
      "epoch:13 step:10300 [D loss: 0.080438, acc.: 97.66%] [G loss: 3.134402]\n",
      "epoch:13 step:10301 [D loss: 0.027005, acc.: 100.00%] [G loss: 4.449835]\n",
      "epoch:13 step:10302 [D loss: 0.026791, acc.: 99.22%] [G loss: 4.661478]\n",
      "epoch:13 step:10303 [D loss: 0.097461, acc.: 96.88%] [G loss: 4.870304]\n",
      "epoch:13 step:10304 [D loss: 0.013765, acc.: 100.00%] [G loss: 4.711934]\n",
      "epoch:13 step:10305 [D loss: 0.042429, acc.: 98.44%] [G loss: 4.516419]\n",
      "epoch:13 step:10306 [D loss: 0.467596, acc.: 81.25%] [G loss: 5.665337]\n",
      "epoch:13 step:10307 [D loss: 0.096658, acc.: 97.66%] [G loss: 6.848784]\n",
      "epoch:13 step:10308 [D loss: 0.100817, acc.: 96.09%] [G loss: 5.877028]\n",
      "epoch:13 step:10309 [D loss: 0.033660, acc.: 99.22%] [G loss: 4.854329]\n",
      "epoch:13 step:10310 [D loss: 0.015459, acc.: 100.00%] [G loss: 3.826053]\n",
      "epoch:13 step:10311 [D loss: 0.061981, acc.: 98.44%] [G loss: 2.811272]\n",
      "epoch:13 step:10312 [D loss: 0.024071, acc.: 100.00%] [G loss: 3.874878]\n",
      "epoch:13 step:10313 [D loss: 0.031863, acc.: 100.00%] [G loss: 4.226152]\n",
      "epoch:13 step:10314 [D loss: 0.020394, acc.: 100.00%] [G loss: 3.559502]\n",
      "epoch:13 step:10315 [D loss: 0.022763, acc.: 100.00%] [G loss: 4.197171]\n",
      "epoch:13 step:10316 [D loss: 0.075540, acc.: 99.22%] [G loss: 4.781678]\n",
      "epoch:13 step:10317 [D loss: 0.242481, acc.: 92.19%] [G loss: 6.131042]\n",
      "epoch:13 step:10318 [D loss: 0.026120, acc.: 99.22%] [G loss: 4.952969]\n",
      "epoch:13 step:10319 [D loss: 0.049916, acc.: 98.44%] [G loss: 5.623391]\n",
      "epoch:13 step:10320 [D loss: 0.030942, acc.: 100.00%] [G loss: 4.013761]\n",
      "epoch:13 step:10321 [D loss: 0.007645, acc.: 100.00%] [G loss: 4.302607]\n",
      "epoch:13 step:10322 [D loss: 0.012416, acc.: 100.00%] [G loss: 1.516525]\n",
      "epoch:13 step:10323 [D loss: 0.043508, acc.: 100.00%] [G loss: 4.701147]\n",
      "epoch:13 step:10324 [D loss: 0.024704, acc.: 99.22%] [G loss: 2.584685]\n",
      "epoch:13 step:10325 [D loss: 0.004286, acc.: 100.00%] [G loss: 0.643411]\n",
      "epoch:13 step:10326 [D loss: 0.123145, acc.: 96.88%] [G loss: 4.475014]\n",
      "epoch:13 step:10327 [D loss: 4.514350, acc.: 14.06%] [G loss: 10.222406]\n",
      "epoch:13 step:10328 [D loss: 3.105191, acc.: 50.00%] [G loss: 6.439867]\n",
      "epoch:13 step:10329 [D loss: 1.537575, acc.: 50.78%] [G loss: 2.798631]\n",
      "epoch:13 step:10330 [D loss: 0.268233, acc.: 89.84%] [G loss: 1.022300]\n",
      "epoch:13 step:10331 [D loss: 0.276086, acc.: 92.97%] [G loss: 2.323425]\n",
      "epoch:13 step:10332 [D loss: 0.135572, acc.: 98.44%] [G loss: 1.215276]\n",
      "epoch:13 step:10333 [D loss: 0.116871, acc.: 97.66%] [G loss: 1.734250]\n",
      "epoch:13 step:10334 [D loss: 0.129166, acc.: 97.66%] [G loss: 1.467294]\n",
      "epoch:13 step:10335 [D loss: 0.146952, acc.: 98.44%] [G loss: 1.235996]\n",
      "epoch:13 step:10336 [D loss: 0.150720, acc.: 96.88%] [G loss: 1.756166]\n",
      "epoch:13 step:10337 [D loss: 0.221580, acc.: 92.19%] [G loss: 1.699371]\n",
      "epoch:13 step:10338 [D loss: 0.141087, acc.: 99.22%] [G loss: 1.841944]\n",
      "epoch:13 step:10339 [D loss: 0.158160, acc.: 96.09%] [G loss: 2.078174]\n",
      "epoch:13 step:10340 [D loss: 0.355768, acc.: 86.72%] [G loss: 3.070128]\n",
      "epoch:13 step:10341 [D loss: 0.134255, acc.: 96.88%] [G loss: 2.419796]\n",
      "epoch:13 step:10342 [D loss: 0.422517, acc.: 81.25%] [G loss: 3.688274]\n",
      "epoch:13 step:10343 [D loss: 0.576206, acc.: 75.78%] [G loss: 2.376122]\n",
      "epoch:13 step:10344 [D loss: 0.680517, acc.: 67.19%] [G loss: 3.288018]\n",
      "epoch:13 step:10345 [D loss: 0.312517, acc.: 84.38%] [G loss: 4.307337]\n",
      "epoch:13 step:10346 [D loss: 0.251357, acc.: 91.41%] [G loss: 2.828820]\n",
      "epoch:13 step:10347 [D loss: 0.179242, acc.: 96.88%] [G loss: 2.452558]\n",
      "epoch:13 step:10348 [D loss: 0.121516, acc.: 96.88%] [G loss: 3.024606]\n",
      "epoch:13 step:10349 [D loss: 0.094820, acc.: 100.00%] [G loss: 2.604211]\n",
      "epoch:13 step:10350 [D loss: 0.234028, acc.: 91.41%] [G loss: 1.923433]\n",
      "epoch:13 step:10351 [D loss: 0.044221, acc.: 100.00%] [G loss: 1.422822]\n",
      "epoch:13 step:10352 [D loss: 0.296082, acc.: 89.06%] [G loss: 1.456264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10353 [D loss: 0.094225, acc.: 98.44%] [G loss: 3.154703]\n",
      "epoch:13 step:10354 [D loss: 0.295131, acc.: 91.41%] [G loss: 2.176295]\n",
      "epoch:13 step:10355 [D loss: 0.263201, acc.: 90.62%] [G loss: 2.981376]\n",
      "epoch:13 step:10356 [D loss: 0.267981, acc.: 92.19%] [G loss: 4.118145]\n",
      "epoch:13 step:10357 [D loss: 0.439095, acc.: 81.25%] [G loss: 3.492793]\n",
      "epoch:13 step:10358 [D loss: 0.253771, acc.: 92.19%] [G loss: 2.523528]\n",
      "epoch:13 step:10359 [D loss: 0.116583, acc.: 96.88%] [G loss: 2.618734]\n",
      "epoch:13 step:10360 [D loss: 0.038002, acc.: 100.00%] [G loss: 3.314847]\n",
      "epoch:13 step:10361 [D loss: 0.095731, acc.: 96.88%] [G loss: 2.100875]\n",
      "epoch:13 step:10362 [D loss: 0.275848, acc.: 85.94%] [G loss: 4.369160]\n",
      "epoch:13 step:10363 [D loss: 0.321069, acc.: 83.59%] [G loss: 1.938957]\n",
      "epoch:13 step:10364 [D loss: 0.041859, acc.: 100.00%] [G loss: 2.956041]\n",
      "epoch:13 step:10365 [D loss: 0.100534, acc.: 98.44%] [G loss: 2.349243]\n",
      "epoch:13 step:10366 [D loss: 0.265468, acc.: 89.06%] [G loss: 5.054648]\n",
      "epoch:13 step:10367 [D loss: 0.539073, acc.: 73.44%] [G loss: 2.921231]\n",
      "epoch:13 step:10368 [D loss: 0.080500, acc.: 100.00%] [G loss: 2.218566]\n",
      "epoch:13 step:10369 [D loss: 0.129097, acc.: 96.88%] [G loss: 2.441552]\n",
      "epoch:13 step:10370 [D loss: 0.153456, acc.: 95.31%] [G loss: 1.930567]\n",
      "epoch:13 step:10371 [D loss: 0.076453, acc.: 97.66%] [G loss: 3.069228]\n",
      "epoch:13 step:10372 [D loss: 0.068866, acc.: 98.44%] [G loss: 3.280527]\n",
      "epoch:13 step:10373 [D loss: 0.169259, acc.: 94.53%] [G loss: 1.712073]\n",
      "epoch:13 step:10374 [D loss: 0.054691, acc.: 100.00%] [G loss: 1.727709]\n",
      "epoch:13 step:10375 [D loss: 0.088100, acc.: 97.66%] [G loss: 0.738802]\n",
      "epoch:13 step:10376 [D loss: 0.050043, acc.: 99.22%] [G loss: 1.481964]\n",
      "epoch:13 step:10377 [D loss: 0.102962, acc.: 99.22%] [G loss: 2.175387]\n",
      "epoch:13 step:10378 [D loss: 0.232309, acc.: 92.19%] [G loss: 0.516441]\n",
      "epoch:13 step:10379 [D loss: 0.068118, acc.: 98.44%] [G loss: 2.979338]\n",
      "epoch:13 step:10380 [D loss: 0.010130, acc.: 100.00%] [G loss: 1.369247]\n",
      "epoch:13 step:10381 [D loss: 0.017013, acc.: 100.00%] [G loss: 0.924818]\n",
      "epoch:13 step:10382 [D loss: 0.134652, acc.: 96.09%] [G loss: 1.762475]\n",
      "epoch:13 step:10383 [D loss: 0.024608, acc.: 100.00%] [G loss: 2.437450]\n",
      "epoch:13 step:10384 [D loss: 0.026189, acc.: 100.00%] [G loss: 0.688470]\n",
      "epoch:13 step:10385 [D loss: 0.033689, acc.: 100.00%] [G loss: 0.975042]\n",
      "epoch:13 step:10386 [D loss: 0.149010, acc.: 93.75%] [G loss: 4.241840]\n",
      "epoch:13 step:10387 [D loss: 0.288523, acc.: 84.38%] [G loss: 1.813185]\n",
      "epoch:13 step:10388 [D loss: 0.153204, acc.: 92.19%] [G loss: 2.846342]\n",
      "epoch:13 step:10389 [D loss: 0.167491, acc.: 93.75%] [G loss: 2.845166]\n",
      "epoch:13 step:10390 [D loss: 0.016170, acc.: 100.00%] [G loss: 2.398533]\n",
      "epoch:13 step:10391 [D loss: 0.006941, acc.: 100.00%] [G loss: 2.101698]\n",
      "epoch:13 step:10392 [D loss: 0.138808, acc.: 96.09%] [G loss: 4.723384]\n",
      "epoch:13 step:10393 [D loss: 0.501515, acc.: 73.44%] [G loss: 1.360724]\n",
      "epoch:13 step:10394 [D loss: 0.322588, acc.: 82.81%] [G loss: 5.766287]\n",
      "epoch:13 step:10395 [D loss: 0.201518, acc.: 89.84%] [G loss: 5.467783]\n",
      "epoch:13 step:10396 [D loss: 0.009706, acc.: 100.00%] [G loss: 5.579605]\n",
      "epoch:13 step:10397 [D loss: 0.016531, acc.: 100.00%] [G loss: 4.193954]\n",
      "epoch:13 step:10398 [D loss: 0.040530, acc.: 99.22%] [G loss: 4.527852]\n",
      "epoch:13 step:10399 [D loss: 0.019222, acc.: 100.00%] [G loss: 4.155692]\n",
      "epoch:13 step:10400 [D loss: 0.102129, acc.: 97.66%] [G loss: 3.158412]\n",
      "epoch:13 step:10401 [D loss: 0.016233, acc.: 100.00%] [G loss: 3.085562]\n",
      "epoch:13 step:10402 [D loss: 0.064616, acc.: 99.22%] [G loss: 2.441773]\n",
      "epoch:13 step:10403 [D loss: 0.098217, acc.: 98.44%] [G loss: 2.442893]\n",
      "epoch:13 step:10404 [D loss: 0.060345, acc.: 99.22%] [G loss: 2.796755]\n",
      "epoch:13 step:10405 [D loss: 0.029182, acc.: 100.00%] [G loss: 1.126987]\n",
      "epoch:13 step:10406 [D loss: 0.021028, acc.: 100.00%] [G loss: 0.645678]\n",
      "epoch:13 step:10407 [D loss: 0.026646, acc.: 100.00%] [G loss: 1.002503]\n",
      "epoch:13 step:10408 [D loss: 0.052989, acc.: 99.22%] [G loss: 1.152384]\n",
      "epoch:13 step:10409 [D loss: 0.232221, acc.: 90.62%] [G loss: 5.479902]\n",
      "epoch:13 step:10410 [D loss: 1.239877, acc.: 53.12%] [G loss: 2.283636]\n",
      "epoch:13 step:10411 [D loss: 0.248682, acc.: 89.06%] [G loss: 4.842729]\n",
      "epoch:13 step:10412 [D loss: 0.012717, acc.: 100.00%] [G loss: 6.044156]\n",
      "epoch:13 step:10413 [D loss: 0.319076, acc.: 88.28%] [G loss: 2.273795]\n",
      "epoch:13 step:10414 [D loss: 0.249514, acc.: 89.06%] [G loss: 3.716287]\n",
      "epoch:13 step:10415 [D loss: 0.007856, acc.: 100.00%] [G loss: 3.325844]\n",
      "epoch:13 step:10416 [D loss: 0.021261, acc.: 100.00%] [G loss: 5.022332]\n",
      "epoch:13 step:10417 [D loss: 0.142451, acc.: 95.31%] [G loss: 1.163553]\n",
      "epoch:13 step:10418 [D loss: 0.015725, acc.: 100.00%] [G loss: 0.851065]\n",
      "epoch:13 step:10419 [D loss: 0.074524, acc.: 96.88%] [G loss: 3.518061]\n",
      "epoch:13 step:10420 [D loss: 0.023777, acc.: 100.00%] [G loss: 1.027710]\n",
      "epoch:13 step:10421 [D loss: 0.097139, acc.: 97.66%] [G loss: 0.824693]\n",
      "epoch:13 step:10422 [D loss: 0.012715, acc.: 100.00%] [G loss: 4.235373]\n",
      "epoch:13 step:10423 [D loss: 0.066211, acc.: 99.22%] [G loss: 1.000156]\n",
      "epoch:13 step:10424 [D loss: 0.016074, acc.: 100.00%] [G loss: 3.071249]\n",
      "epoch:13 step:10425 [D loss: 0.011894, acc.: 100.00%] [G loss: 0.249472]\n",
      "epoch:13 step:10426 [D loss: 0.012195, acc.: 100.00%] [G loss: 0.332945]\n",
      "epoch:13 step:10427 [D loss: 0.054924, acc.: 98.44%] [G loss: 1.916168]\n",
      "epoch:13 step:10428 [D loss: 0.051608, acc.: 99.22%] [G loss: 2.386040]\n",
      "epoch:13 step:10429 [D loss: 0.036375, acc.: 99.22%] [G loss: 0.555891]\n",
      "epoch:13 step:10430 [D loss: 0.173045, acc.: 94.53%] [G loss: 0.420691]\n",
      "epoch:13 step:10431 [D loss: 0.376756, acc.: 81.25%] [G loss: 6.886809]\n",
      "epoch:13 step:10432 [D loss: 0.605217, acc.: 72.66%] [G loss: 2.269614]\n",
      "epoch:13 step:10433 [D loss: 0.010638, acc.: 100.00%] [G loss: 2.485043]\n",
      "epoch:13 step:10434 [D loss: 0.004507, acc.: 100.00%] [G loss: 2.943423]\n",
      "epoch:13 step:10435 [D loss: 0.020221, acc.: 100.00%] [G loss: 0.540306]\n",
      "epoch:13 step:10436 [D loss: 0.081053, acc.: 96.88%] [G loss: 3.247853]\n",
      "epoch:13 step:10437 [D loss: 0.033575, acc.: 99.22%] [G loss: 3.896338]\n",
      "epoch:13 step:10438 [D loss: 0.019955, acc.: 100.00%] [G loss: 4.212644]\n",
      "epoch:13 step:10439 [D loss: 0.316335, acc.: 82.81%] [G loss: 0.716619]\n",
      "epoch:13 step:10440 [D loss: 0.018685, acc.: 100.00%] [G loss: 0.223754]\n",
      "epoch:13 step:10441 [D loss: 0.190378, acc.: 92.97%] [G loss: 1.829740]\n",
      "epoch:13 step:10442 [D loss: 0.006587, acc.: 100.00%] [G loss: 4.987687]\n",
      "epoch:13 step:10443 [D loss: 0.037192, acc.: 99.22%] [G loss: 2.869711]\n",
      "epoch:13 step:10444 [D loss: 0.036373, acc.: 100.00%] [G loss: 4.296152]\n",
      "epoch:13 step:10445 [D loss: 0.044149, acc.: 100.00%] [G loss: 2.306158]\n",
      "epoch:13 step:10446 [D loss: 0.065098, acc.: 98.44%] [G loss: 3.372919]\n",
      "epoch:13 step:10447 [D loss: 0.015521, acc.: 100.00%] [G loss: 4.286688]\n",
      "epoch:13 step:10448 [D loss: 0.161956, acc.: 95.31%] [G loss: 2.068780]\n",
      "epoch:13 step:10449 [D loss: 0.198294, acc.: 94.53%] [G loss: 5.046412]\n",
      "epoch:13 step:10450 [D loss: 0.048715, acc.: 100.00%] [G loss: 5.792151]\n",
      "epoch:13 step:10451 [D loss: 0.126970, acc.: 94.53%] [G loss: 4.715282]\n",
      "epoch:13 step:10452 [D loss: 0.102178, acc.: 96.09%] [G loss: 4.177248]\n",
      "epoch:13 step:10453 [D loss: 0.072718, acc.: 98.44%] [G loss: 4.986879]\n",
      "epoch:13 step:10454 [D loss: 0.011690, acc.: 100.00%] [G loss: 3.348553]\n",
      "epoch:13 step:10455 [D loss: 0.019818, acc.: 100.00%] [G loss: 5.103498]\n",
      "epoch:13 step:10456 [D loss: 0.043446, acc.: 100.00%] [G loss: 5.236552]\n",
      "epoch:13 step:10457 [D loss: 0.016552, acc.: 100.00%] [G loss: 5.099703]\n",
      "epoch:13 step:10458 [D loss: 0.038993, acc.: 99.22%] [G loss: 0.862235]\n",
      "epoch:13 step:10459 [D loss: 0.144386, acc.: 93.75%] [G loss: 6.260189]\n",
      "epoch:13 step:10460 [D loss: 0.435354, acc.: 81.25%] [G loss: 2.550748]\n",
      "epoch:13 step:10461 [D loss: 0.160809, acc.: 92.97%] [G loss: 5.563831]\n",
      "epoch:13 step:10462 [D loss: 0.051839, acc.: 99.22%] [G loss: 5.265489]\n",
      "epoch:13 step:10463 [D loss: 0.044273, acc.: 100.00%] [G loss: 3.176785]\n",
      "epoch:13 step:10464 [D loss: 0.065460, acc.: 97.66%] [G loss: 4.360188]\n",
      "epoch:13 step:10465 [D loss: 0.102416, acc.: 96.88%] [G loss: 3.350459]\n",
      "epoch:13 step:10466 [D loss: 0.030092, acc.: 100.00%] [G loss: 1.155382]\n",
      "epoch:13 step:10467 [D loss: 0.086467, acc.: 98.44%] [G loss: 4.382923]\n",
      "epoch:13 step:10468 [D loss: 0.502085, acc.: 75.00%] [G loss: 0.394828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10469 [D loss: 0.280856, acc.: 84.38%] [G loss: 6.727091]\n",
      "epoch:13 step:10470 [D loss: 0.690878, acc.: 68.75%] [G loss: 3.150276]\n",
      "epoch:13 step:10471 [D loss: 0.039187, acc.: 98.44%] [G loss: 2.201765]\n",
      "epoch:13 step:10472 [D loss: 0.018111, acc.: 100.00%] [G loss: 1.384879]\n",
      "epoch:13 step:10473 [D loss: 0.042495, acc.: 99.22%] [G loss: 0.674113]\n",
      "epoch:13 step:10474 [D loss: 0.319151, acc.: 84.38%] [G loss: 4.832955]\n",
      "epoch:13 step:10475 [D loss: 0.038857, acc.: 99.22%] [G loss: 6.074337]\n",
      "epoch:13 step:10476 [D loss: 0.248602, acc.: 89.06%] [G loss: 2.493906]\n",
      "epoch:13 step:10477 [D loss: 0.457207, acc.: 76.56%] [G loss: 5.539936]\n",
      "epoch:13 step:10478 [D loss: 0.462849, acc.: 77.34%] [G loss: 5.212074]\n",
      "epoch:13 step:10479 [D loss: 0.047206, acc.: 99.22%] [G loss: 2.982575]\n",
      "epoch:13 step:10480 [D loss: 0.006271, acc.: 100.00%] [G loss: 3.287172]\n",
      "epoch:13 step:10481 [D loss: 0.029181, acc.: 100.00%] [G loss: 2.893374]\n",
      "epoch:13 step:10482 [D loss: 0.011771, acc.: 100.00%] [G loss: 4.192004]\n",
      "epoch:13 step:10483 [D loss: 0.006029, acc.: 100.00%] [G loss: 2.857725]\n",
      "epoch:13 step:10484 [D loss: 0.128655, acc.: 95.31%] [G loss: 2.495304]\n",
      "epoch:13 step:10485 [D loss: 0.055506, acc.: 96.88%] [G loss: 4.788994]\n",
      "epoch:13 step:10486 [D loss: 0.012149, acc.: 100.00%] [G loss: 3.496098]\n",
      "epoch:13 step:10487 [D loss: 0.060215, acc.: 98.44%] [G loss: 2.770154]\n",
      "epoch:13 step:10488 [D loss: 0.043693, acc.: 99.22%] [G loss: 2.339231]\n",
      "epoch:13 step:10489 [D loss: 0.097004, acc.: 98.44%] [G loss: 4.148643]\n",
      "epoch:13 step:10490 [D loss: 0.046421, acc.: 98.44%] [G loss: 3.356382]\n",
      "epoch:13 step:10491 [D loss: 0.190016, acc.: 91.41%] [G loss: 3.400419]\n",
      "epoch:13 step:10492 [D loss: 0.024554, acc.: 100.00%] [G loss: 4.100205]\n",
      "epoch:13 step:10493 [D loss: 0.059476, acc.: 99.22%] [G loss: 3.746291]\n",
      "epoch:13 step:10494 [D loss: 0.054914, acc.: 98.44%] [G loss: 4.464589]\n",
      "epoch:13 step:10495 [D loss: 0.016069, acc.: 100.00%] [G loss: 4.046616]\n",
      "epoch:13 step:10496 [D loss: 0.732434, acc.: 57.03%] [G loss: 5.799365]\n",
      "epoch:13 step:10497 [D loss: 0.326631, acc.: 81.25%] [G loss: 4.821644]\n",
      "epoch:13 step:10498 [D loss: 0.010226, acc.: 100.00%] [G loss: 1.705181]\n",
      "epoch:13 step:10499 [D loss: 0.243022, acc.: 89.06%] [G loss: 6.283805]\n",
      "epoch:13 step:10500 [D loss: 0.055327, acc.: 97.66%] [G loss: 5.818825]\n",
      "epoch:13 step:10501 [D loss: 0.006841, acc.: 100.00%] [G loss: 5.784163]\n",
      "epoch:13 step:10502 [D loss: 0.012468, acc.: 100.00%] [G loss: 4.768625]\n",
      "epoch:13 step:10503 [D loss: 0.171916, acc.: 92.19%] [G loss: 1.667917]\n",
      "epoch:13 step:10504 [D loss: 0.743313, acc.: 69.53%] [G loss: 7.805696]\n",
      "epoch:13 step:10505 [D loss: 0.723192, acc.: 64.06%] [G loss: 5.986685]\n",
      "epoch:13 step:10506 [D loss: 0.023370, acc.: 100.00%] [G loss: 4.822638]\n",
      "epoch:13 step:10507 [D loss: 0.009607, acc.: 100.00%] [G loss: 5.453479]\n",
      "epoch:13 step:10508 [D loss: 0.043954, acc.: 98.44%] [G loss: 5.521859]\n",
      "epoch:13 step:10509 [D loss: 0.011029, acc.: 100.00%] [G loss: 5.139749]\n",
      "epoch:13 step:10510 [D loss: 0.007377, acc.: 100.00%] [G loss: 4.785717]\n",
      "epoch:13 step:10511 [D loss: 0.046109, acc.: 99.22%] [G loss: 3.090880]\n",
      "epoch:13 step:10512 [D loss: 0.012401, acc.: 100.00%] [G loss: 5.019491]\n",
      "epoch:13 step:10513 [D loss: 0.004457, acc.: 100.00%] [G loss: 4.740421]\n",
      "epoch:13 step:10514 [D loss: 0.015026, acc.: 100.00%] [G loss: 1.565977]\n",
      "epoch:13 step:10515 [D loss: 0.028040, acc.: 100.00%] [G loss: 3.695288]\n",
      "epoch:13 step:10516 [D loss: 0.022160, acc.: 100.00%] [G loss: 1.642898]\n",
      "epoch:13 step:10517 [D loss: 0.162084, acc.: 94.53%] [G loss: 4.775693]\n",
      "epoch:13 step:10518 [D loss: 0.029767, acc.: 99.22%] [G loss: 5.339370]\n",
      "epoch:13 step:10519 [D loss: 0.232154, acc.: 90.62%] [G loss: 0.806028]\n",
      "epoch:13 step:10520 [D loss: 0.479625, acc.: 75.78%] [G loss: 7.149550]\n",
      "epoch:13 step:10521 [D loss: 0.123507, acc.: 95.31%] [G loss: 7.564068]\n",
      "epoch:13 step:10522 [D loss: 0.869510, acc.: 64.84%] [G loss: 1.099172]\n",
      "epoch:13 step:10523 [D loss: 0.556910, acc.: 75.78%] [G loss: 6.350848]\n",
      "epoch:13 step:10524 [D loss: 0.050082, acc.: 98.44%] [G loss: 6.274083]\n",
      "epoch:13 step:10525 [D loss: 0.251636, acc.: 89.06%] [G loss: 3.675002]\n",
      "epoch:13 step:10526 [D loss: 0.197268, acc.: 92.19%] [G loss: 0.696922]\n",
      "epoch:13 step:10527 [D loss: 0.009893, acc.: 100.00%] [G loss: 5.034634]\n",
      "epoch:13 step:10528 [D loss: 0.092511, acc.: 96.09%] [G loss: 5.225920]\n",
      "epoch:13 step:10529 [D loss: 0.117456, acc.: 97.66%] [G loss: 1.851143]\n",
      "epoch:13 step:10530 [D loss: 0.028039, acc.: 100.00%] [G loss: 0.925221]\n",
      "epoch:13 step:10531 [D loss: 0.133283, acc.: 94.53%] [G loss: 4.564178]\n",
      "epoch:13 step:10532 [D loss: 0.088518, acc.: 100.00%] [G loss: 6.100121]\n",
      "epoch:13 step:10533 [D loss: 0.102010, acc.: 96.88%] [G loss: 5.483619]\n",
      "epoch:13 step:10534 [D loss: 0.151349, acc.: 96.09%] [G loss: 5.734785]\n",
      "epoch:13 step:10535 [D loss: 0.105175, acc.: 94.53%] [G loss: 4.193306]\n",
      "epoch:13 step:10536 [D loss: 0.023370, acc.: 100.00%] [G loss: 4.055995]\n",
      "epoch:13 step:10537 [D loss: 0.157142, acc.: 94.53%] [G loss: 2.871637]\n",
      "epoch:13 step:10538 [D loss: 0.142168, acc.: 96.09%] [G loss: 5.206649]\n",
      "epoch:13 step:10539 [D loss: 0.748688, acc.: 67.19%] [G loss: 7.045131]\n",
      "epoch:13 step:10540 [D loss: 1.165279, acc.: 56.25%] [G loss: 3.893744]\n",
      "epoch:13 step:10541 [D loss: 0.085951, acc.: 98.44%] [G loss: 1.221236]\n",
      "epoch:13 step:10542 [D loss: 0.078398, acc.: 98.44%] [G loss: 0.823795]\n",
      "epoch:13 step:10543 [D loss: 0.012572, acc.: 100.00%] [G loss: 2.385990]\n",
      "epoch:13 step:10544 [D loss: 0.113569, acc.: 95.31%] [G loss: 3.653337]\n",
      "epoch:13 step:10545 [D loss: 0.074970, acc.: 97.66%] [G loss: 3.341870]\n",
      "epoch:13 step:10546 [D loss: 0.021542, acc.: 100.00%] [G loss: 3.938327]\n",
      "epoch:13 step:10547 [D loss: 0.022573, acc.: 100.00%] [G loss: 2.927819]\n",
      "epoch:13 step:10548 [D loss: 0.146452, acc.: 96.09%] [G loss: 0.968049]\n",
      "epoch:13 step:10549 [D loss: 0.677859, acc.: 68.75%] [G loss: 5.614876]\n",
      "epoch:13 step:10550 [D loss: 1.771357, acc.: 53.91%] [G loss: 3.501816]\n",
      "epoch:13 step:10551 [D loss: 0.215273, acc.: 91.41%] [G loss: 1.975592]\n",
      "epoch:13 step:10552 [D loss: 0.319310, acc.: 83.59%] [G loss: 3.935933]\n",
      "epoch:13 step:10553 [D loss: 0.016225, acc.: 100.00%] [G loss: 4.805048]\n",
      "epoch:13 step:10554 [D loss: 0.503910, acc.: 75.78%] [G loss: 2.427350]\n",
      "epoch:13 step:10555 [D loss: 0.178023, acc.: 91.41%] [G loss: 2.548231]\n",
      "epoch:13 step:10556 [D loss: 0.212192, acc.: 89.84%] [G loss: 4.582901]\n",
      "epoch:13 step:10557 [D loss: 0.041603, acc.: 100.00%] [G loss: 4.159885]\n",
      "epoch:13 step:10558 [D loss: 0.645148, acc.: 71.09%] [G loss: 1.771554]\n",
      "epoch:13 step:10559 [D loss: 0.201086, acc.: 94.53%] [G loss: 2.546372]\n",
      "epoch:13 step:10560 [D loss: 0.043190, acc.: 98.44%] [G loss: 1.015790]\n",
      "epoch:13 step:10561 [D loss: 0.018892, acc.: 100.00%] [G loss: 3.797146]\n",
      "epoch:13 step:10562 [D loss: 0.025692, acc.: 100.00%] [G loss: 2.995110]\n",
      "epoch:13 step:10563 [D loss: 0.022591, acc.: 100.00%] [G loss: 1.599145]\n",
      "epoch:13 step:10564 [D loss: 0.130891, acc.: 96.88%] [G loss: 0.714876]\n",
      "epoch:13 step:10565 [D loss: 0.015848, acc.: 100.00%] [G loss: 1.181207]\n",
      "epoch:13 step:10566 [D loss: 0.078433, acc.: 99.22%] [G loss: 1.729396]\n",
      "epoch:13 step:10567 [D loss: 0.118341, acc.: 97.66%] [G loss: 2.371639]\n",
      "epoch:13 step:10568 [D loss: 0.026315, acc.: 100.00%] [G loss: 2.913621]\n",
      "epoch:13 step:10569 [D loss: 0.068037, acc.: 98.44%] [G loss: 2.934568]\n",
      "epoch:13 step:10570 [D loss: 0.108893, acc.: 98.44%] [G loss: 3.608928]\n",
      "epoch:13 step:10571 [D loss: 0.017125, acc.: 100.00%] [G loss: 2.071698]\n",
      "epoch:13 step:10572 [D loss: 0.099314, acc.: 96.09%] [G loss: 3.176203]\n",
      "epoch:13 step:10573 [D loss: 0.072449, acc.: 97.66%] [G loss: 3.634715]\n",
      "epoch:13 step:10574 [D loss: 0.046934, acc.: 99.22%] [G loss: 4.180120]\n",
      "epoch:13 step:10575 [D loss: 0.024742, acc.: 100.00%] [G loss: 3.679710]\n",
      "epoch:13 step:10576 [D loss: 0.045613, acc.: 100.00%] [G loss: 3.829279]\n",
      "epoch:13 step:10577 [D loss: 0.066360, acc.: 99.22%] [G loss: 3.906107]\n",
      "epoch:13 step:10578 [D loss: 0.063936, acc.: 99.22%] [G loss: 2.754757]\n",
      "epoch:13 step:10579 [D loss: 0.074043, acc.: 99.22%] [G loss: 3.327277]\n",
      "epoch:13 step:10580 [D loss: 0.011670, acc.: 100.00%] [G loss: 2.959818]\n",
      "epoch:13 step:10581 [D loss: 0.051446, acc.: 98.44%] [G loss: 2.798033]\n",
      "epoch:13 step:10582 [D loss: 0.064267, acc.: 100.00%] [G loss: 2.786187]\n",
      "epoch:13 step:10583 [D loss: 0.109708, acc.: 98.44%] [G loss: 3.288385]\n",
      "epoch:13 step:10584 [D loss: 0.204982, acc.: 93.75%] [G loss: 3.926053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10585 [D loss: 0.020086, acc.: 100.00%] [G loss: 4.025375]\n",
      "epoch:13 step:10586 [D loss: 0.013000, acc.: 100.00%] [G loss: 3.892789]\n",
      "epoch:13 step:10587 [D loss: 0.080996, acc.: 98.44%] [G loss: 2.564833]\n",
      "epoch:13 step:10588 [D loss: 0.027329, acc.: 100.00%] [G loss: 2.363959]\n",
      "epoch:13 step:10589 [D loss: 0.032879, acc.: 100.00%] [G loss: 2.566929]\n",
      "epoch:13 step:10590 [D loss: 0.090341, acc.: 97.66%] [G loss: 1.652632]\n",
      "epoch:13 step:10591 [D loss: 0.012677, acc.: 100.00%] [G loss: 2.012145]\n",
      "epoch:13 step:10592 [D loss: 0.007877, acc.: 100.00%] [G loss: 2.190766]\n",
      "epoch:13 step:10593 [D loss: 0.049609, acc.: 100.00%] [G loss: 1.844334]\n",
      "epoch:13 step:10594 [D loss: 0.113552, acc.: 98.44%] [G loss: 2.832632]\n",
      "epoch:13 step:10595 [D loss: 0.048901, acc.: 99.22%] [G loss: 2.403288]\n",
      "epoch:13 step:10596 [D loss: 0.085053, acc.: 98.44%] [G loss: 2.701801]\n",
      "epoch:13 step:10597 [D loss: 0.045307, acc.: 99.22%] [G loss: 3.609569]\n",
      "epoch:13 step:10598 [D loss: 0.035079, acc.: 99.22%] [G loss: 3.080391]\n",
      "epoch:13 step:10599 [D loss: 0.031350, acc.: 100.00%] [G loss: 2.770408]\n",
      "epoch:13 step:10600 [D loss: 0.301833, acc.: 85.94%] [G loss: 5.692182]\n",
      "epoch:13 step:10601 [D loss: 0.819898, acc.: 66.41%] [G loss: 1.228746]\n",
      "epoch:13 step:10602 [D loss: 0.493626, acc.: 74.22%] [G loss: 6.608681]\n",
      "epoch:13 step:10603 [D loss: 0.362767, acc.: 83.59%] [G loss: 5.339527]\n",
      "epoch:13 step:10604 [D loss: 0.006696, acc.: 100.00%] [G loss: 4.863309]\n",
      "epoch:13 step:10605 [D loss: 0.028339, acc.: 100.00%] [G loss: 3.693901]\n",
      "epoch:13 step:10606 [D loss: 0.033112, acc.: 100.00%] [G loss: 1.541178]\n",
      "epoch:13 step:10607 [D loss: 0.039717, acc.: 99.22%] [G loss: 4.266631]\n",
      "epoch:13 step:10608 [D loss: 0.017603, acc.: 100.00%] [G loss: 2.031839]\n",
      "epoch:13 step:10609 [D loss: 0.102539, acc.: 97.66%] [G loss: 4.707775]\n",
      "epoch:13 step:10610 [D loss: 0.027562, acc.: 100.00%] [G loss: 2.487893]\n",
      "epoch:13 step:10611 [D loss: 0.206754, acc.: 92.19%] [G loss: 0.297706]\n",
      "epoch:13 step:10612 [D loss: 0.268973, acc.: 87.50%] [G loss: 3.724580]\n",
      "epoch:13 step:10613 [D loss: 0.978383, acc.: 60.16%] [G loss: 3.841842]\n",
      "epoch:13 step:10614 [D loss: 0.041616, acc.: 99.22%] [G loss: 3.378172]\n",
      "epoch:13 step:10615 [D loss: 0.033281, acc.: 100.00%] [G loss: 3.957445]\n",
      "epoch:13 step:10616 [D loss: 0.048603, acc.: 100.00%] [G loss: 4.160547]\n",
      "epoch:13 step:10617 [D loss: 0.072871, acc.: 99.22%] [G loss: 3.489039]\n",
      "epoch:13 step:10618 [D loss: 0.176081, acc.: 92.97%] [G loss: 3.297770]\n",
      "epoch:13 step:10619 [D loss: 0.014471, acc.: 100.00%] [G loss: 3.942288]\n",
      "epoch:13 step:10620 [D loss: 0.111881, acc.: 97.66%] [G loss: 5.016181]\n",
      "epoch:13 step:10621 [D loss: 0.028373, acc.: 100.00%] [G loss: 3.452042]\n",
      "epoch:13 step:10622 [D loss: 0.028369, acc.: 100.00%] [G loss: 4.144546]\n",
      "epoch:13 step:10623 [D loss: 0.065801, acc.: 99.22%] [G loss: 3.323419]\n",
      "epoch:13 step:10624 [D loss: 0.046840, acc.: 100.00%] [G loss: 4.089889]\n",
      "epoch:13 step:10625 [D loss: 0.039514, acc.: 99.22%] [G loss: 3.930944]\n",
      "epoch:13 step:10626 [D loss: 0.022052, acc.: 100.00%] [G loss: 3.151225]\n",
      "epoch:13 step:10627 [D loss: 0.223996, acc.: 90.62%] [G loss: 5.632167]\n",
      "epoch:13 step:10628 [D loss: 0.524092, acc.: 75.78%] [G loss: 1.610582]\n",
      "epoch:13 step:10629 [D loss: 0.944454, acc.: 60.16%] [G loss: 7.876347]\n",
      "epoch:13 step:10630 [D loss: 2.352619, acc.: 50.00%] [G loss: 5.355718]\n",
      "epoch:13 step:10631 [D loss: 0.995151, acc.: 55.47%] [G loss: 2.010926]\n",
      "epoch:13 step:10632 [D loss: 0.227864, acc.: 92.19%] [G loss: 1.721357]\n",
      "epoch:13 step:10633 [D loss: 0.113694, acc.: 97.66%] [G loss: 0.564679]\n",
      "epoch:13 step:10634 [D loss: 0.323201, acc.: 87.50%] [G loss: 2.332502]\n",
      "epoch:13 step:10635 [D loss: 0.268801, acc.: 89.84%] [G loss: 0.207702]\n",
      "epoch:13 step:10636 [D loss: 0.042282, acc.: 100.00%] [G loss: 1.310916]\n",
      "epoch:13 step:10637 [D loss: 0.050884, acc.: 100.00%] [G loss: 2.248391]\n",
      "epoch:13 step:10638 [D loss: 0.030304, acc.: 100.00%] [G loss: 0.213656]\n",
      "epoch:13 step:10639 [D loss: 0.125612, acc.: 97.66%] [G loss: 0.490965]\n",
      "epoch:13 step:10640 [D loss: 0.091040, acc.: 99.22%] [G loss: 0.355936]\n",
      "epoch:13 step:10641 [D loss: 0.095253, acc.: 98.44%] [G loss: 0.523309]\n",
      "epoch:13 step:10642 [D loss: 0.039753, acc.: 99.22%] [G loss: 0.997851]\n",
      "epoch:13 step:10643 [D loss: 0.317851, acc.: 83.59%] [G loss: 4.718091]\n",
      "epoch:13 step:10644 [D loss: 1.166488, acc.: 54.69%] [G loss: 1.112177]\n",
      "epoch:13 step:10645 [D loss: 0.226008, acc.: 90.62%] [G loss: 3.044197]\n",
      "epoch:13 step:10646 [D loss: 0.044927, acc.: 100.00%] [G loss: 2.980052]\n",
      "epoch:13 step:10647 [D loss: 0.053073, acc.: 99.22%] [G loss: 3.252937]\n",
      "epoch:13 step:10648 [D loss: 0.150507, acc.: 96.88%] [G loss: 1.217121]\n",
      "epoch:13 step:10649 [D loss: 0.074446, acc.: 99.22%] [G loss: 1.840295]\n",
      "epoch:13 step:10650 [D loss: 0.078988, acc.: 99.22%] [G loss: 2.047245]\n",
      "epoch:13 step:10651 [D loss: 0.235829, acc.: 93.75%] [G loss: 2.393529]\n",
      "epoch:13 step:10652 [D loss: 0.296969, acc.: 89.06%] [G loss: 2.453118]\n",
      "epoch:13 step:10653 [D loss: 0.057148, acc.: 98.44%] [G loss: 3.247394]\n",
      "epoch:13 step:10654 [D loss: 0.302927, acc.: 87.50%] [G loss: 1.729573]\n",
      "epoch:13 step:10655 [D loss: 0.595542, acc.: 75.78%] [G loss: 4.044349]\n",
      "epoch:13 step:10656 [D loss: 0.480099, acc.: 75.00%] [G loss: 3.709187]\n",
      "epoch:13 step:10657 [D loss: 0.076158, acc.: 99.22%] [G loss: 2.720691]\n",
      "epoch:13 step:10658 [D loss: 0.160743, acc.: 95.31%] [G loss: 1.969742]\n",
      "epoch:13 step:10659 [D loss: 0.038665, acc.: 100.00%] [G loss: 3.040365]\n",
      "epoch:13 step:10660 [D loss: 0.093963, acc.: 98.44%] [G loss: 2.390820]\n",
      "epoch:13 step:10661 [D loss: 0.173178, acc.: 93.75%] [G loss: 3.012407]\n",
      "epoch:13 step:10662 [D loss: 0.424274, acc.: 82.03%] [G loss: 2.753735]\n",
      "epoch:13 step:10663 [D loss: 0.057247, acc.: 97.66%] [G loss: 3.254342]\n",
      "epoch:13 step:10664 [D loss: 0.198902, acc.: 91.41%] [G loss: 1.986184]\n",
      "epoch:13 step:10665 [D loss: 0.115421, acc.: 98.44%] [G loss: 2.001260]\n",
      "epoch:13 step:10666 [D loss: 0.139866, acc.: 96.09%] [G loss: 4.142926]\n",
      "epoch:13 step:10667 [D loss: 0.063900, acc.: 98.44%] [G loss: 3.110541]\n",
      "epoch:13 step:10668 [D loss: 0.168411, acc.: 95.31%] [G loss: 3.088084]\n",
      "epoch:13 step:10669 [D loss: 0.136654, acc.: 96.88%] [G loss: 3.811776]\n",
      "epoch:13 step:10670 [D loss: 0.043834, acc.: 100.00%] [G loss: 3.302262]\n",
      "epoch:13 step:10671 [D loss: 0.120253, acc.: 97.66%] [G loss: 3.407053]\n",
      "epoch:13 step:10672 [D loss: 0.162264, acc.: 95.31%] [G loss: 2.701277]\n",
      "epoch:13 step:10673 [D loss: 0.066108, acc.: 100.00%] [G loss: 3.534427]\n",
      "epoch:13 step:10674 [D loss: 0.217821, acc.: 89.84%] [G loss: 4.837868]\n",
      "epoch:13 step:10675 [D loss: 0.662322, acc.: 67.97%] [G loss: 3.589378]\n",
      "epoch:13 step:10676 [D loss: 0.046407, acc.: 99.22%] [G loss: 4.445536]\n",
      "epoch:13 step:10677 [D loss: 0.042445, acc.: 99.22%] [G loss: 4.682630]\n",
      "epoch:13 step:10678 [D loss: 0.018826, acc.: 100.00%] [G loss: 4.583615]\n",
      "epoch:13 step:10679 [D loss: 0.036931, acc.: 100.00%] [G loss: 3.663541]\n",
      "epoch:13 step:10680 [D loss: 0.039693, acc.: 99.22%] [G loss: 2.737484]\n",
      "epoch:13 step:10681 [D loss: 0.178195, acc.: 92.97%] [G loss: 1.899341]\n",
      "epoch:13 step:10682 [D loss: 0.142123, acc.: 92.97%] [G loss: 5.287468]\n",
      "epoch:13 step:10683 [D loss: 0.057371, acc.: 98.44%] [G loss: 5.065658]\n",
      "epoch:13 step:10684 [D loss: 0.095327, acc.: 99.22%] [G loss: 5.590003]\n",
      "epoch:13 step:10685 [D loss: 0.254406, acc.: 87.50%] [G loss: 0.777163]\n",
      "epoch:13 step:10686 [D loss: 0.222713, acc.: 88.28%] [G loss: 5.228157]\n",
      "epoch:13 step:10687 [D loss: 0.133725, acc.: 96.09%] [G loss: 4.923132]\n",
      "epoch:13 step:10688 [D loss: 0.063654, acc.: 99.22%] [G loss: 4.894444]\n",
      "epoch:13 step:10689 [D loss: 0.210519, acc.: 92.97%] [G loss: 3.959624]\n",
      "epoch:13 step:10690 [D loss: 0.233907, acc.: 90.62%] [G loss: 5.645653]\n",
      "epoch:13 step:10691 [D loss: 0.077897, acc.: 98.44%] [G loss: 4.711705]\n",
      "epoch:13 step:10692 [D loss: 0.016091, acc.: 100.00%] [G loss: 4.949548]\n",
      "epoch:13 step:10693 [D loss: 0.018208, acc.: 100.00%] [G loss: 3.716380]\n",
      "epoch:13 step:10694 [D loss: 0.055778, acc.: 98.44%] [G loss: 5.226677]\n",
      "epoch:13 step:10695 [D loss: 0.044750, acc.: 99.22%] [G loss: 3.520185]\n",
      "epoch:13 step:10696 [D loss: 0.038333, acc.: 100.00%] [G loss: 1.478089]\n",
      "epoch:13 step:10697 [D loss: 0.038156, acc.: 98.44%] [G loss: 4.476722]\n",
      "epoch:13 step:10698 [D loss: 0.034228, acc.: 100.00%] [G loss: 0.938295]\n",
      "epoch:13 step:10699 [D loss: 0.116066, acc.: 97.66%] [G loss: 4.137618]\n",
      "epoch:13 step:10700 [D loss: 0.591732, acc.: 67.97%] [G loss: 0.950289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10701 [D loss: 0.068589, acc.: 97.66%] [G loss: 2.449941]\n",
      "epoch:13 step:10702 [D loss: 0.019586, acc.: 100.00%] [G loss: 1.052810]\n",
      "epoch:13 step:10703 [D loss: 0.327040, acc.: 83.59%] [G loss: 0.325826]\n",
      "epoch:13 step:10704 [D loss: 1.747563, acc.: 53.12%] [G loss: 6.205026]\n",
      "epoch:13 step:10705 [D loss: 0.832508, acc.: 60.94%] [G loss: 2.132342]\n",
      "epoch:13 step:10706 [D loss: 0.129067, acc.: 95.31%] [G loss: 0.487534]\n",
      "epoch:13 step:10707 [D loss: 0.078768, acc.: 96.88%] [G loss: 1.006597]\n",
      "epoch:13 step:10708 [D loss: 0.028928, acc.: 100.00%] [G loss: 1.404620]\n",
      "epoch:13 step:10709 [D loss: 0.165168, acc.: 94.53%] [G loss: 0.179045]\n",
      "epoch:13 step:10710 [D loss: 0.146280, acc.: 94.53%] [G loss: 0.620845]\n",
      "epoch:13 step:10711 [D loss: 0.315999, acc.: 90.62%] [G loss: 1.388144]\n",
      "epoch:13 step:10712 [D loss: 0.061206, acc.: 98.44%] [G loss: 3.557232]\n",
      "epoch:13 step:10713 [D loss: 0.938623, acc.: 58.59%] [G loss: 0.950072]\n",
      "epoch:13 step:10714 [D loss: 0.906984, acc.: 64.06%] [G loss: 0.356637]\n",
      "epoch:13 step:10715 [D loss: 0.149597, acc.: 95.31%] [G loss: 2.101124]\n",
      "epoch:13 step:10716 [D loss: 0.013265, acc.: 100.00%] [G loss: 5.604859]\n",
      "epoch:13 step:10717 [D loss: 0.167745, acc.: 93.75%] [G loss: 4.027438]\n",
      "epoch:13 step:10718 [D loss: 0.013322, acc.: 100.00%] [G loss: 1.499233]\n",
      "epoch:13 step:10719 [D loss: 0.152882, acc.: 93.75%] [G loss: 2.441168]\n",
      "epoch:13 step:10720 [D loss: 0.014295, acc.: 100.00%] [G loss: 1.190033]\n",
      "epoch:13 step:10721 [D loss: 0.030975, acc.: 98.44%] [G loss: 4.177398]\n",
      "epoch:13 step:10722 [D loss: 0.026078, acc.: 100.00%] [G loss: 4.424056]\n",
      "epoch:13 step:10723 [D loss: 0.015276, acc.: 100.00%] [G loss: 3.362990]\n",
      "epoch:13 step:10724 [D loss: 0.040881, acc.: 100.00%] [G loss: 0.218400]\n",
      "epoch:13 step:10725 [D loss: 0.027936, acc.: 100.00%] [G loss: 1.660268]\n",
      "epoch:13 step:10726 [D loss: 0.277418, acc.: 89.84%] [G loss: 4.882564]\n",
      "epoch:13 step:10727 [D loss: 0.072949, acc.: 96.88%] [G loss: 5.008584]\n",
      "epoch:13 step:10728 [D loss: 0.566440, acc.: 73.44%] [G loss: 3.592828]\n",
      "epoch:13 step:10729 [D loss: 0.020018, acc.: 100.00%] [G loss: 4.305858]\n",
      "epoch:13 step:10730 [D loss: 0.042220, acc.: 100.00%] [G loss: 1.507219]\n",
      "epoch:13 step:10731 [D loss: 0.008464, acc.: 100.00%] [G loss: 1.477029]\n",
      "epoch:13 step:10732 [D loss: 0.043889, acc.: 100.00%] [G loss: 1.636784]\n",
      "epoch:13 step:10733 [D loss: 0.024891, acc.: 100.00%] [G loss: 4.920506]\n",
      "epoch:13 step:10734 [D loss: 0.020844, acc.: 100.00%] [G loss: 2.114964]\n",
      "epoch:13 step:10735 [D loss: 0.033519, acc.: 99.22%] [G loss: 1.145019]\n",
      "epoch:13 step:10736 [D loss: 0.123302, acc.: 95.31%] [G loss: 4.872963]\n",
      "epoch:13 step:10737 [D loss: 0.053706, acc.: 98.44%] [G loss: 4.603717]\n",
      "epoch:13 step:10738 [D loss: 0.029198, acc.: 99.22%] [G loss: 3.807204]\n",
      "epoch:13 step:10739 [D loss: 0.025631, acc.: 99.22%] [G loss: 1.269902]\n",
      "epoch:13 step:10740 [D loss: 0.049140, acc.: 99.22%] [G loss: 2.234164]\n",
      "epoch:13 step:10741 [D loss: 0.033998, acc.: 100.00%] [G loss: 2.984140]\n",
      "epoch:13 step:10742 [D loss: 0.025044, acc.: 99.22%] [G loss: 2.202680]\n",
      "epoch:13 step:10743 [D loss: 0.059854, acc.: 98.44%] [G loss: 4.688907]\n",
      "epoch:13 step:10744 [D loss: 0.248206, acc.: 89.06%] [G loss: 5.108402]\n",
      "epoch:13 step:10745 [D loss: 0.007272, acc.: 100.00%] [G loss: 6.650334]\n",
      "epoch:13 step:10746 [D loss: 0.423212, acc.: 81.25%] [G loss: 0.830812]\n",
      "epoch:13 step:10747 [D loss: 0.029433, acc.: 100.00%] [G loss: 4.455539]\n",
      "epoch:13 step:10748 [D loss: 0.008037, acc.: 100.00%] [G loss: 4.267317]\n",
      "epoch:13 step:10749 [D loss: 0.047236, acc.: 97.66%] [G loss: 1.301684]\n",
      "epoch:13 step:10750 [D loss: 0.046305, acc.: 100.00%] [G loss: 3.393854]\n",
      "epoch:13 step:10751 [D loss: 0.006814, acc.: 100.00%] [G loss: 4.727680]\n",
      "epoch:13 step:10752 [D loss: 0.708815, acc.: 66.41%] [G loss: 7.618568]\n",
      "epoch:13 step:10753 [D loss: 0.681261, acc.: 65.62%] [G loss: 3.462481]\n",
      "epoch:13 step:10754 [D loss: 0.081424, acc.: 97.66%] [G loss: 3.754560]\n",
      "epoch:13 step:10755 [D loss: 0.003860, acc.: 100.00%] [G loss: 4.261869]\n",
      "epoch:13 step:10756 [D loss: 0.023745, acc.: 99.22%] [G loss: 3.767088]\n",
      "epoch:13 step:10757 [D loss: 0.047762, acc.: 98.44%] [G loss: 4.761068]\n",
      "epoch:13 step:10758 [D loss: 0.007191, acc.: 100.00%] [G loss: 4.166280]\n",
      "epoch:13 step:10759 [D loss: 0.018551, acc.: 99.22%] [G loss: 2.344985]\n",
      "epoch:13 step:10760 [D loss: 0.017995, acc.: 100.00%] [G loss: 2.440818]\n",
      "epoch:13 step:10761 [D loss: 0.007056, acc.: 100.00%] [G loss: 2.453963]\n",
      "epoch:13 step:10762 [D loss: 0.011638, acc.: 100.00%] [G loss: 3.496574]\n",
      "epoch:13 step:10763 [D loss: 0.194146, acc.: 91.41%] [G loss: 4.745185]\n",
      "epoch:13 step:10764 [D loss: 0.067397, acc.: 97.66%] [G loss: 5.097549]\n",
      "epoch:13 step:10765 [D loss: 0.071698, acc.: 96.09%] [G loss: 0.742064]\n",
      "epoch:13 step:10766 [D loss: 0.420997, acc.: 82.03%] [G loss: 7.307058]\n",
      "epoch:13 step:10767 [D loss: 0.673221, acc.: 66.41%] [G loss: 2.519752]\n",
      "epoch:13 step:10768 [D loss: 0.382079, acc.: 79.69%] [G loss: 6.421752]\n",
      "epoch:13 step:10769 [D loss: 0.048266, acc.: 98.44%] [G loss: 7.295305]\n",
      "epoch:13 step:10770 [D loss: 0.398149, acc.: 78.91%] [G loss: 4.428318]\n",
      "epoch:13 step:10771 [D loss: 0.281326, acc.: 87.50%] [G loss: 3.419041]\n",
      "epoch:13 step:10772 [D loss: 0.004184, acc.: 100.00%] [G loss: 6.619706]\n",
      "epoch:13 step:10773 [D loss: 0.124447, acc.: 95.31%] [G loss: 5.338110]\n",
      "epoch:13 step:10774 [D loss: 0.003129, acc.: 100.00%] [G loss: 3.024322]\n",
      "epoch:13 step:10775 [D loss: 0.004836, acc.: 100.00%] [G loss: 3.936125]\n",
      "epoch:13 step:10776 [D loss: 0.021256, acc.: 100.00%] [G loss: 2.895211]\n",
      "epoch:13 step:10777 [D loss: 0.008720, acc.: 100.00%] [G loss: 3.302345]\n",
      "epoch:13 step:10778 [D loss: 0.005256, acc.: 100.00%] [G loss: 1.162174]\n",
      "epoch:13 step:10779 [D loss: 0.108902, acc.: 95.31%] [G loss: 3.713977]\n",
      "epoch:13 step:10780 [D loss: 0.035853, acc.: 98.44%] [G loss: 4.581479]\n",
      "epoch:13 step:10781 [D loss: 0.011816, acc.: 100.00%] [G loss: 3.399282]\n",
      "epoch:13 step:10782 [D loss: 0.032564, acc.: 100.00%] [G loss: 0.747682]\n",
      "epoch:13 step:10783 [D loss: 0.019606, acc.: 100.00%] [G loss: 1.577679]\n",
      "epoch:13 step:10784 [D loss: 0.188169, acc.: 93.75%] [G loss: 4.305013]\n",
      "epoch:13 step:10785 [D loss: 0.034673, acc.: 99.22%] [G loss: 4.374625]\n",
      "epoch:13 step:10786 [D loss: 0.265532, acc.: 89.84%] [G loss: 2.076163]\n",
      "epoch:13 step:10787 [D loss: 0.038683, acc.: 99.22%] [G loss: 0.131224]\n",
      "epoch:13 step:10788 [D loss: 0.121603, acc.: 97.66%] [G loss: 2.669714]\n",
      "epoch:13 step:10789 [D loss: 0.060694, acc.: 97.66%] [G loss: 4.475618]\n",
      "epoch:13 step:10790 [D loss: 0.450641, acc.: 75.78%] [G loss: 4.717840]\n",
      "epoch:13 step:10791 [D loss: 0.052761, acc.: 97.66%] [G loss: 6.020827]\n",
      "epoch:13 step:10792 [D loss: 0.100097, acc.: 97.66%] [G loss: 1.286773]\n",
      "epoch:13 step:10793 [D loss: 0.045497, acc.: 100.00%] [G loss: 4.334326]\n",
      "epoch:13 step:10794 [D loss: 0.020957, acc.: 100.00%] [G loss: 4.865907]\n",
      "epoch:13 step:10795 [D loss: 0.037006, acc.: 99.22%] [G loss: 4.018369]\n",
      "epoch:13 step:10796 [D loss: 0.007571, acc.: 100.00%] [G loss: 4.303166]\n",
      "epoch:13 step:10797 [D loss: 0.037080, acc.: 98.44%] [G loss: 4.121535]\n",
      "epoch:13 step:10798 [D loss: 0.077148, acc.: 98.44%] [G loss: 3.601222]\n",
      "epoch:13 step:10799 [D loss: 1.197364, acc.: 40.62%] [G loss: 3.201128]\n",
      "epoch:13 step:10800 [D loss: 0.174687, acc.: 90.62%] [G loss: 5.226469]\n",
      "epoch:13 step:10801 [D loss: 0.151067, acc.: 94.53%] [G loss: 2.616862]\n",
      "epoch:13 step:10802 [D loss: 0.046674, acc.: 100.00%] [G loss: 3.274133]\n",
      "epoch:13 step:10803 [D loss: 0.041298, acc.: 100.00%] [G loss: 1.353096]\n",
      "epoch:13 step:10804 [D loss: 0.025292, acc.: 100.00%] [G loss: 4.751529]\n",
      "epoch:13 step:10805 [D loss: 0.075035, acc.: 97.66%] [G loss: 1.747603]\n",
      "epoch:13 step:10806 [D loss: 0.034915, acc.: 100.00%] [G loss: 4.138730]\n",
      "epoch:13 step:10807 [D loss: 0.135284, acc.: 95.31%] [G loss: 4.345745]\n",
      "epoch:13 step:10808 [D loss: 0.053391, acc.: 98.44%] [G loss: 3.687006]\n",
      "epoch:13 step:10809 [D loss: 0.093917, acc.: 96.88%] [G loss: 5.823654]\n",
      "epoch:13 step:10810 [D loss: 0.040545, acc.: 100.00%] [G loss: 5.405910]\n",
      "epoch:13 step:10811 [D loss: 0.066604, acc.: 100.00%] [G loss: 4.229827]\n",
      "epoch:13 step:10812 [D loss: 0.079732, acc.: 100.00%] [G loss: 2.039986]\n",
      "epoch:13 step:10813 [D loss: 0.181753, acc.: 94.53%] [G loss: 4.034038]\n",
      "epoch:13 step:10814 [D loss: 0.081696, acc.: 97.66%] [G loss: 5.484081]\n",
      "epoch:13 step:10815 [D loss: 0.045306, acc.: 99.22%] [G loss: 4.553487]\n",
      "epoch:13 step:10816 [D loss: 0.064901, acc.: 98.44%] [G loss: 4.215532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10817 [D loss: 0.021579, acc.: 100.00%] [G loss: 4.322059]\n",
      "epoch:13 step:10818 [D loss: 0.375871, acc.: 82.81%] [G loss: 1.938360]\n",
      "epoch:13 step:10819 [D loss: 0.127576, acc.: 96.09%] [G loss: 5.009439]\n",
      "epoch:13 step:10820 [D loss: 0.057807, acc.: 98.44%] [G loss: 4.798045]\n",
      "epoch:13 step:10821 [D loss: 0.102000, acc.: 98.44%] [G loss: 3.822711]\n",
      "epoch:13 step:10822 [D loss: 0.005634, acc.: 100.00%] [G loss: 3.508761]\n",
      "epoch:13 step:10823 [D loss: 0.028715, acc.: 100.00%] [G loss: 2.415073]\n",
      "epoch:13 step:10824 [D loss: 0.004414, acc.: 100.00%] [G loss: 2.635657]\n",
      "epoch:13 step:10825 [D loss: 0.037150, acc.: 100.00%] [G loss: 1.454603]\n",
      "epoch:13 step:10826 [D loss: 0.062573, acc.: 99.22%] [G loss: 4.074810]\n",
      "epoch:13 step:10827 [D loss: 0.015372, acc.: 99.22%] [G loss: 5.594038]\n",
      "epoch:13 step:10828 [D loss: 0.019204, acc.: 100.00%] [G loss: 5.512226]\n",
      "epoch:13 step:10829 [D loss: 0.063740, acc.: 100.00%] [G loss: 5.007807]\n",
      "epoch:13 step:10830 [D loss: 0.025101, acc.: 100.00%] [G loss: 3.967654]\n",
      "epoch:13 step:10831 [D loss: 0.621924, acc.: 69.53%] [G loss: 6.873346]\n",
      "epoch:13 step:10832 [D loss: 1.958605, acc.: 50.78%] [G loss: 0.966921]\n",
      "epoch:13 step:10833 [D loss: 0.653901, acc.: 71.88%] [G loss: 6.171359]\n",
      "epoch:13 step:10834 [D loss: 0.963970, acc.: 60.94%] [G loss: 4.554129]\n",
      "epoch:13 step:10835 [D loss: 0.037607, acc.: 100.00%] [G loss: 0.956086]\n",
      "epoch:13 step:10836 [D loss: 0.044196, acc.: 100.00%] [G loss: 0.023046]\n",
      "epoch:13 step:10837 [D loss: 0.015543, acc.: 100.00%] [G loss: 5.147461]\n",
      "epoch:13 step:10838 [D loss: 0.012361, acc.: 100.00%] [G loss: 0.354516]\n",
      "epoch:13 step:10839 [D loss: 0.004469, acc.: 100.00%] [G loss: 3.613887]\n",
      "epoch:13 step:10840 [D loss: 0.045713, acc.: 98.44%] [G loss: 0.015919]\n",
      "epoch:13 step:10841 [D loss: 0.007464, acc.: 100.00%] [G loss: 0.955857]\n",
      "epoch:13 step:10842 [D loss: 0.072515, acc.: 99.22%] [G loss: 0.436639]\n",
      "epoch:13 step:10843 [D loss: 0.013998, acc.: 100.00%] [G loss: 0.356986]\n",
      "epoch:13 step:10844 [D loss: 0.014162, acc.: 100.00%] [G loss: 3.281764]\n",
      "epoch:13 step:10845 [D loss: 0.234803, acc.: 87.50%] [G loss: 0.002227]\n",
      "epoch:13 step:10846 [D loss: 1.938536, acc.: 51.56%] [G loss: 6.182526]\n",
      "epoch:13 step:10847 [D loss: 0.902536, acc.: 60.16%] [G loss: 5.747676]\n",
      "epoch:13 step:10848 [D loss: 0.441701, acc.: 77.34%] [G loss: 2.278292]\n",
      "epoch:13 step:10849 [D loss: 0.219634, acc.: 92.97%] [G loss: 1.661205]\n",
      "epoch:13 step:10850 [D loss: 0.037508, acc.: 100.00%] [G loss: 2.123740]\n",
      "epoch:13 step:10851 [D loss: 0.183273, acc.: 96.88%] [G loss: 1.195992]\n",
      "epoch:13 step:10852 [D loss: 0.046233, acc.: 100.00%] [G loss: 4.180467]\n",
      "epoch:13 step:10853 [D loss: 0.117441, acc.: 97.66%] [G loss: 3.087568]\n",
      "epoch:13 step:10854 [D loss: 0.038479, acc.: 100.00%] [G loss: 4.979881]\n",
      "epoch:13 step:10855 [D loss: 0.321629, acc.: 86.72%] [G loss: 4.632886]\n",
      "epoch:13 step:10856 [D loss: 0.391920, acc.: 82.81%] [G loss: 2.869285]\n",
      "epoch:13 step:10857 [D loss: 0.140635, acc.: 96.88%] [G loss: 3.697247]\n",
      "epoch:13 step:10858 [D loss: 0.014774, acc.: 100.00%] [G loss: 4.464872]\n",
      "epoch:13 step:10859 [D loss: 0.017238, acc.: 100.00%] [G loss: 4.427257]\n",
      "epoch:13 step:10860 [D loss: 0.089308, acc.: 99.22%] [G loss: 3.078295]\n",
      "epoch:13 step:10861 [D loss: 0.092740, acc.: 97.66%] [G loss: 4.075895]\n",
      "epoch:13 step:10862 [D loss: 0.013226, acc.: 100.00%] [G loss: 4.019189]\n",
      "epoch:13 step:10863 [D loss: 0.118108, acc.: 97.66%] [G loss: 3.157008]\n",
      "epoch:13 step:10864 [D loss: 0.161711, acc.: 95.31%] [G loss: 4.466078]\n",
      "epoch:13 step:10865 [D loss: 0.185178, acc.: 91.41%] [G loss: 3.972155]\n",
      "epoch:13 step:10866 [D loss: 0.357754, acc.: 84.38%] [G loss: 3.011531]\n",
      "epoch:13 step:10867 [D loss: 0.100345, acc.: 98.44%] [G loss: 3.436773]\n",
      "epoch:13 step:10868 [D loss: 0.479558, acc.: 75.78%] [G loss: 2.066076]\n",
      "epoch:13 step:10869 [D loss: 0.042793, acc.: 100.00%] [G loss: 1.777553]\n",
      "epoch:13 step:10870 [D loss: 0.014305, acc.: 100.00%] [G loss: 3.174704]\n",
      "epoch:13 step:10871 [D loss: 0.023295, acc.: 100.00%] [G loss: 2.535062]\n",
      "epoch:13 step:10872 [D loss: 0.034949, acc.: 100.00%] [G loss: 2.662700]\n",
      "epoch:13 step:10873 [D loss: 0.049241, acc.: 100.00%] [G loss: 1.538033]\n",
      "epoch:13 step:10874 [D loss: 0.049993, acc.: 99.22%] [G loss: 2.850451]\n",
      "epoch:13 step:10875 [D loss: 0.037472, acc.: 99.22%] [G loss: 3.137793]\n",
      "epoch:13 step:10876 [D loss: 0.043219, acc.: 100.00%] [G loss: 2.529633]\n",
      "epoch:13 step:10877 [D loss: 0.103711, acc.: 99.22%] [G loss: 2.290367]\n",
      "epoch:13 step:10878 [D loss: 0.311996, acc.: 89.06%] [G loss: 1.738958]\n",
      "epoch:13 step:10879 [D loss: 0.022175, acc.: 100.00%] [G loss: 3.211702]\n",
      "epoch:13 step:10880 [D loss: 0.073691, acc.: 98.44%] [G loss: 2.497629]\n",
      "epoch:13 step:10881 [D loss: 0.122039, acc.: 97.66%] [G loss: 3.759671]\n",
      "epoch:13 step:10882 [D loss: 0.030341, acc.: 100.00%] [G loss: 3.616518]\n",
      "epoch:13 step:10883 [D loss: 0.020873, acc.: 100.00%] [G loss: 3.836629]\n",
      "epoch:13 step:10884 [D loss: 0.044195, acc.: 100.00%] [G loss: 3.509907]\n",
      "epoch:13 step:10885 [D loss: 0.141620, acc.: 97.66%] [G loss: 4.715433]\n",
      "epoch:13 step:10886 [D loss: 0.033817, acc.: 99.22%] [G loss: 4.305831]\n",
      "epoch:13 step:10887 [D loss: 0.052753, acc.: 99.22%] [G loss: 3.925756]\n",
      "epoch:13 step:10888 [D loss: 0.172026, acc.: 95.31%] [G loss: 3.361689]\n",
      "epoch:13 step:10889 [D loss: 0.072884, acc.: 99.22%] [G loss: 3.854184]\n",
      "epoch:13 step:10890 [D loss: 0.011515, acc.: 100.00%] [G loss: 4.726749]\n",
      "epoch:13 step:10891 [D loss: 0.017398, acc.: 100.00%] [G loss: 3.112199]\n",
      "epoch:13 step:10892 [D loss: 0.330729, acc.: 85.94%] [G loss: 5.930487]\n",
      "epoch:13 step:10893 [D loss: 0.289774, acc.: 86.72%] [G loss: 2.378206]\n",
      "epoch:13 step:10894 [D loss: 0.043272, acc.: 99.22%] [G loss: 1.422983]\n",
      "epoch:13 step:10895 [D loss: 0.091644, acc.: 95.31%] [G loss: 5.074779]\n",
      "epoch:13 step:10896 [D loss: 0.018678, acc.: 100.00%] [G loss: 5.481495]\n",
      "epoch:13 step:10897 [D loss: 0.040822, acc.: 97.66%] [G loss: 3.573591]\n",
      "epoch:13 step:10898 [D loss: 0.027708, acc.: 100.00%] [G loss: 3.447724]\n",
      "epoch:13 step:10899 [D loss: 0.022168, acc.: 100.00%] [G loss: 2.239486]\n",
      "epoch:13 step:10900 [D loss: 0.019794, acc.: 100.00%] [G loss: 3.670418]\n",
      "epoch:13 step:10901 [D loss: 0.030542, acc.: 100.00%] [G loss: 4.016574]\n",
      "epoch:13 step:10902 [D loss: 0.037613, acc.: 99.22%] [G loss: 3.120081]\n",
      "epoch:13 step:10903 [D loss: 0.043383, acc.: 100.00%] [G loss: 4.130094]\n",
      "epoch:13 step:10904 [D loss: 0.017732, acc.: 100.00%] [G loss: 3.630777]\n",
      "epoch:13 step:10905 [D loss: 0.071394, acc.: 98.44%] [G loss: 2.689112]\n",
      "epoch:13 step:10906 [D loss: 0.114378, acc.: 96.09%] [G loss: 3.537186]\n",
      "epoch:13 step:10907 [D loss: 0.171509, acc.: 93.75%] [G loss: 2.601683]\n",
      "epoch:13 step:10908 [D loss: 0.276142, acc.: 84.38%] [G loss: 6.882333]\n",
      "epoch:13 step:10909 [D loss: 0.558874, acc.: 73.44%] [G loss: 4.828090]\n",
      "epoch:13 step:10910 [D loss: 0.008701, acc.: 100.00%] [G loss: 2.156054]\n",
      "epoch:13 step:10911 [D loss: 0.015769, acc.: 100.00%] [G loss: 2.131682]\n",
      "epoch:13 step:10912 [D loss: 0.107660, acc.: 95.31%] [G loss: 3.447196]\n",
      "epoch:13 step:10913 [D loss: 0.048415, acc.: 100.00%] [G loss: 5.054461]\n",
      "epoch:13 step:10914 [D loss: 0.011819, acc.: 100.00%] [G loss: 4.688346]\n",
      "epoch:13 step:10915 [D loss: 0.227711, acc.: 90.62%] [G loss: 1.268618]\n",
      "epoch:13 step:10916 [D loss: 0.022630, acc.: 100.00%] [G loss: 2.616162]\n",
      "epoch:13 step:10917 [D loss: 0.030915, acc.: 100.00%] [G loss: 3.550068]\n",
      "epoch:13 step:10918 [D loss: 0.072255, acc.: 98.44%] [G loss: 2.809144]\n",
      "epoch:13 step:10919 [D loss: 0.029330, acc.: 100.00%] [G loss: 4.550119]\n",
      "epoch:13 step:10920 [D loss: 0.027205, acc.: 100.00%] [G loss: 3.009312]\n",
      "epoch:13 step:10921 [D loss: 0.013267, acc.: 100.00%] [G loss: 2.630482]\n",
      "epoch:13 step:10922 [D loss: 0.011481, acc.: 100.00%] [G loss: 3.000436]\n",
      "epoch:13 step:10923 [D loss: 0.202476, acc.: 93.75%] [G loss: 4.514459]\n",
      "epoch:13 step:10924 [D loss: 0.271007, acc.: 86.72%] [G loss: 0.276126]\n",
      "epoch:13 step:10925 [D loss: 0.178754, acc.: 92.19%] [G loss: 5.442814]\n",
      "epoch:13 step:10926 [D loss: 0.144907, acc.: 91.41%] [G loss: 4.669093]\n",
      "epoch:13 step:10927 [D loss: 0.017356, acc.: 100.00%] [G loss: 5.193042]\n",
      "epoch:13 step:10928 [D loss: 0.014967, acc.: 100.00%] [G loss: 4.452450]\n",
      "epoch:13 step:10929 [D loss: 0.005242, acc.: 100.00%] [G loss: 5.133835]\n",
      "epoch:13 step:10930 [D loss: 0.021289, acc.: 100.00%] [G loss: 3.197100]\n",
      "epoch:13 step:10931 [D loss: 0.020077, acc.: 100.00%] [G loss: 3.519572]\n",
      "epoch:13 step:10932 [D loss: 0.053835, acc.: 100.00%] [G loss: 3.914379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13 step:10933 [D loss: 0.035884, acc.: 100.00%] [G loss: 2.706747]\n",
      "epoch:13 step:10934 [D loss: 0.006897, acc.: 100.00%] [G loss: 4.814660]\n",
      "epoch:14 step:10935 [D loss: 0.142036, acc.: 96.09%] [G loss: 4.441524]\n",
      "epoch:14 step:10936 [D loss: 0.020469, acc.: 99.22%] [G loss: 5.830113]\n",
      "epoch:14 step:10937 [D loss: 0.077254, acc.: 96.88%] [G loss: 4.837795]\n",
      "epoch:14 step:10938 [D loss: 0.045224, acc.: 99.22%] [G loss: 4.519788]\n",
      "epoch:14 step:10939 [D loss: 0.005780, acc.: 100.00%] [G loss: 5.660727]\n",
      "epoch:14 step:10940 [D loss: 0.013429, acc.: 100.00%] [G loss: 4.033999]\n",
      "epoch:14 step:10941 [D loss: 0.153031, acc.: 95.31%] [G loss: 4.270211]\n",
      "epoch:14 step:10942 [D loss: 0.004418, acc.: 100.00%] [G loss: 5.646574]\n",
      "epoch:14 step:10943 [D loss: 0.007121, acc.: 100.00%] [G loss: 3.737278]\n",
      "epoch:14 step:10944 [D loss: 0.102799, acc.: 96.09%] [G loss: 4.274587]\n",
      "epoch:14 step:10945 [D loss: 0.013849, acc.: 100.00%] [G loss: 3.322866]\n",
      "epoch:14 step:10946 [D loss: 0.023538, acc.: 99.22%] [G loss: 5.075891]\n",
      "epoch:14 step:10947 [D loss: 0.013932, acc.: 100.00%] [G loss: 3.751295]\n",
      "epoch:14 step:10948 [D loss: 1.148846, acc.: 50.78%] [G loss: 8.819004]\n",
      "epoch:14 step:10949 [D loss: 2.963502, acc.: 50.00%] [G loss: 4.841514]\n",
      "epoch:14 step:10950 [D loss: 0.461054, acc.: 80.47%] [G loss: 3.074079]\n",
      "epoch:14 step:10951 [D loss: 0.102534, acc.: 98.44%] [G loss: 3.967465]\n",
      "epoch:14 step:10952 [D loss: 0.055969, acc.: 98.44%] [G loss: 1.832716]\n",
      "epoch:14 step:10953 [D loss: 0.234346, acc.: 90.62%] [G loss: 3.686351]\n",
      "epoch:14 step:10954 [D loss: 0.304203, acc.: 85.94%] [G loss: 3.322013]\n",
      "epoch:14 step:10955 [D loss: 0.040810, acc.: 100.00%] [G loss: 4.883905]\n",
      "epoch:14 step:10956 [D loss: 1.108925, acc.: 50.00%] [G loss: 3.634112]\n",
      "epoch:14 step:10957 [D loss: 0.186876, acc.: 94.53%] [G loss: 2.578977]\n",
      "epoch:14 step:10958 [D loss: 0.119188, acc.: 96.88%] [G loss: 4.422264]\n",
      "epoch:14 step:10959 [D loss: 0.057503, acc.: 99.22%] [G loss: 2.650739]\n",
      "epoch:14 step:10960 [D loss: 0.064656, acc.: 99.22%] [G loss: 1.996873]\n",
      "epoch:14 step:10961 [D loss: 0.106478, acc.: 96.88%] [G loss: 3.378237]\n",
      "epoch:14 step:10962 [D loss: 0.490757, acc.: 75.78%] [G loss: 2.008220]\n",
      "epoch:14 step:10963 [D loss: 0.041248, acc.: 99.22%] [G loss: 4.044240]\n",
      "epoch:14 step:10964 [D loss: 0.042325, acc.: 99.22%] [G loss: 3.445470]\n",
      "epoch:14 step:10965 [D loss: 0.038380, acc.: 99.22%] [G loss: 2.467147]\n",
      "epoch:14 step:10966 [D loss: 0.047063, acc.: 98.44%] [G loss: 2.087890]\n",
      "epoch:14 step:10967 [D loss: 0.317108, acc.: 87.50%] [G loss: 5.644888]\n",
      "epoch:14 step:10968 [D loss: 0.061446, acc.: 97.66%] [G loss: 5.937176]\n",
      "epoch:14 step:10969 [D loss: 0.393155, acc.: 83.59%] [G loss: 2.547672]\n",
      "epoch:14 step:10970 [D loss: 0.070643, acc.: 99.22%] [G loss: 0.346048]\n",
      "epoch:14 step:10971 [D loss: 0.149828, acc.: 92.19%] [G loss: 4.588712]\n",
      "epoch:14 step:10972 [D loss: 0.023084, acc.: 100.00%] [G loss: 4.897181]\n",
      "epoch:14 step:10973 [D loss: 0.027234, acc.: 99.22%] [G loss: 1.959766]\n",
      "epoch:14 step:10974 [D loss: 0.096631, acc.: 99.22%] [G loss: 4.275239]\n",
      "epoch:14 step:10975 [D loss: 0.015680, acc.: 100.00%] [G loss: 1.091627]\n",
      "epoch:14 step:10976 [D loss: 0.037267, acc.: 100.00%] [G loss: 4.559300]\n",
      "epoch:14 step:10977 [D loss: 0.087438, acc.: 97.66%] [G loss: 4.017226]\n",
      "epoch:14 step:10978 [D loss: 0.241100, acc.: 89.06%] [G loss: 4.966802]\n",
      "epoch:14 step:10979 [D loss: 0.232755, acc.: 90.62%] [G loss: 3.279604]\n",
      "epoch:14 step:10980 [D loss: 0.070783, acc.: 99.22%] [G loss: 2.259595]\n",
      "epoch:14 step:10981 [D loss: 0.008045, acc.: 100.00%] [G loss: 3.853516]\n",
      "epoch:14 step:10982 [D loss: 0.012848, acc.: 100.00%] [G loss: 4.507491]\n",
      "epoch:14 step:10983 [D loss: 0.036619, acc.: 100.00%] [G loss: 4.486087]\n",
      "epoch:14 step:10984 [D loss: 0.053742, acc.: 98.44%] [G loss: 3.014890]\n",
      "epoch:14 step:10985 [D loss: 0.018896, acc.: 100.00%] [G loss: 3.230190]\n",
      "epoch:14 step:10986 [D loss: 0.042219, acc.: 100.00%] [G loss: 2.637884]\n",
      "epoch:14 step:10987 [D loss: 0.014046, acc.: 100.00%] [G loss: 2.645651]\n",
      "epoch:14 step:10988 [D loss: 0.055199, acc.: 100.00%] [G loss: 3.256384]\n",
      "epoch:14 step:10989 [D loss: 0.034988, acc.: 100.00%] [G loss: 4.892343]\n",
      "epoch:14 step:10990 [D loss: 0.461991, acc.: 76.56%] [G loss: 5.508122]\n",
      "epoch:14 step:10991 [D loss: 0.043959, acc.: 98.44%] [G loss: 4.819113]\n",
      "epoch:14 step:10992 [D loss: 0.051736, acc.: 98.44%] [G loss: 3.954903]\n",
      "epoch:14 step:10993 [D loss: 0.009279, acc.: 100.00%] [G loss: 3.311945]\n",
      "epoch:14 step:10994 [D loss: 0.015106, acc.: 100.00%] [G loss: 3.248567]\n",
      "epoch:14 step:10995 [D loss: 0.057632, acc.: 98.44%] [G loss: 3.695388]\n",
      "epoch:14 step:10996 [D loss: 0.008831, acc.: 100.00%] [G loss: 2.094081]\n",
      "epoch:14 step:10997 [D loss: 0.012358, acc.: 100.00%] [G loss: 2.238991]\n",
      "epoch:14 step:10998 [D loss: 0.045646, acc.: 100.00%] [G loss: 2.740813]\n",
      "epoch:14 step:10999 [D loss: 0.094931, acc.: 98.44%] [G loss: 2.795314]\n",
      "epoch:14 step:11000 [D loss: 0.085908, acc.: 96.88%] [G loss: 4.105460]\n",
      "epoch:14 step:11001 [D loss: 0.011332, acc.: 100.00%] [G loss: 3.517578]\n",
      "epoch:14 step:11002 [D loss: 0.092280, acc.: 96.88%] [G loss: 2.985262]\n",
      "epoch:14 step:11003 [D loss: 0.070601, acc.: 97.66%] [G loss: 3.677703]\n",
      "epoch:14 step:11004 [D loss: 0.003731, acc.: 100.00%] [G loss: 4.586570]\n",
      "epoch:14 step:11005 [D loss: 0.014239, acc.: 100.00%] [G loss: 4.005245]\n",
      "epoch:14 step:11006 [D loss: 0.007769, acc.: 100.00%] [G loss: 4.455855]\n",
      "epoch:14 step:11007 [D loss: 0.047245, acc.: 100.00%] [G loss: 4.361479]\n",
      "epoch:14 step:11008 [D loss: 0.102364, acc.: 96.09%] [G loss: 5.399350]\n",
      "epoch:14 step:11009 [D loss: 0.056557, acc.: 99.22%] [G loss: 4.161607]\n",
      "epoch:14 step:11010 [D loss: 0.045248, acc.: 98.44%] [G loss: 4.773273]\n",
      "epoch:14 step:11011 [D loss: 0.064836, acc.: 100.00%] [G loss: 1.639866]\n",
      "epoch:14 step:11012 [D loss: 0.065513, acc.: 99.22%] [G loss: 4.648666]\n",
      "epoch:14 step:11013 [D loss: 0.044790, acc.: 98.44%] [G loss: 3.916720]\n",
      "epoch:14 step:11014 [D loss: 0.013696, acc.: 100.00%] [G loss: 4.873813]\n",
      "epoch:14 step:11015 [D loss: 0.027899, acc.: 99.22%] [G loss: 3.731911]\n",
      "epoch:14 step:11016 [D loss: 0.041701, acc.: 100.00%] [G loss: 4.219552]\n",
      "epoch:14 step:11017 [D loss: 0.054562, acc.: 98.44%] [G loss: 5.425323]\n",
      "epoch:14 step:11018 [D loss: 0.039155, acc.: 99.22%] [G loss: 4.619269]\n",
      "epoch:14 step:11019 [D loss: 0.069008, acc.: 97.66%] [G loss: 4.560601]\n",
      "epoch:14 step:11020 [D loss: 0.015004, acc.: 100.00%] [G loss: 1.498955]\n",
      "epoch:14 step:11021 [D loss: 0.021016, acc.: 100.00%] [G loss: 2.521045]\n",
      "epoch:14 step:11022 [D loss: 0.186750, acc.: 94.53%] [G loss: 2.572793]\n",
      "epoch:14 step:11023 [D loss: 0.013648, acc.: 100.00%] [G loss: 7.708565]\n",
      "epoch:14 step:11024 [D loss: 6.279799, acc.: 2.34%] [G loss: 6.005770]\n",
      "epoch:14 step:11025 [D loss: 1.525532, acc.: 53.91%] [G loss: 4.621349]\n",
      "epoch:14 step:11026 [D loss: 0.135838, acc.: 95.31%] [G loss: 2.092626]\n",
      "epoch:14 step:11027 [D loss: 0.015458, acc.: 100.00%] [G loss: 3.509325]\n",
      "epoch:14 step:11028 [D loss: 0.053287, acc.: 99.22%] [G loss: 3.649497]\n",
      "epoch:14 step:11029 [D loss: 0.033972, acc.: 100.00%] [G loss: 0.322230]\n",
      "epoch:14 step:11030 [D loss: 0.044928, acc.: 99.22%] [G loss: 0.413999]\n",
      "epoch:14 step:11031 [D loss: 0.150049, acc.: 94.53%] [G loss: 3.778756]\n",
      "epoch:14 step:11032 [D loss: 0.060714, acc.: 99.22%] [G loss: 3.930271]\n",
      "epoch:14 step:11033 [D loss: 0.087299, acc.: 99.22%] [G loss: 2.908937]\n",
      "epoch:14 step:11034 [D loss: 0.034649, acc.: 99.22%] [G loss: 2.830594]\n",
      "epoch:14 step:11035 [D loss: 0.056119, acc.: 100.00%] [G loss: 2.722167]\n",
      "epoch:14 step:11036 [D loss: 0.122823, acc.: 96.09%] [G loss: 2.756658]\n",
      "epoch:14 step:11037 [D loss: 0.096239, acc.: 96.88%] [G loss: 1.095559]\n",
      "epoch:14 step:11038 [D loss: 0.094300, acc.: 98.44%] [G loss: 0.660706]\n",
      "epoch:14 step:11039 [D loss: 0.051282, acc.: 99.22%] [G loss: 0.545749]\n",
      "epoch:14 step:11040 [D loss: 0.139046, acc.: 95.31%] [G loss: 2.356291]\n",
      "epoch:14 step:11041 [D loss: 0.478035, acc.: 73.44%] [G loss: 4.762925]\n",
      "epoch:14 step:11042 [D loss: 1.309770, acc.: 54.69%] [G loss: 1.459891]\n",
      "epoch:14 step:11043 [D loss: 0.185002, acc.: 93.75%] [G loss: 0.107927]\n",
      "epoch:14 step:11044 [D loss: 0.047471, acc.: 98.44%] [G loss: 0.070430]\n",
      "epoch:14 step:11045 [D loss: 0.493277, acc.: 75.00%] [G loss: 6.636760]\n",
      "epoch:14 step:11046 [D loss: 0.254098, acc.: 88.28%] [G loss: 5.975204]\n",
      "epoch:14 step:11047 [D loss: 0.105263, acc.: 96.88%] [G loss: 3.307908]\n",
      "epoch:14 step:11048 [D loss: 0.018949, acc.: 100.00%] [G loss: 2.146276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11049 [D loss: 0.010422, acc.: 100.00%] [G loss: 5.360398]\n",
      "epoch:14 step:11050 [D loss: 0.014492, acc.: 100.00%] [G loss: 3.876888]\n",
      "epoch:14 step:11051 [D loss: 0.096707, acc.: 96.88%] [G loss: 2.201619]\n",
      "epoch:14 step:11052 [D loss: 0.047582, acc.: 100.00%] [G loss: 5.159201]\n",
      "epoch:14 step:11053 [D loss: 0.045287, acc.: 99.22%] [G loss: 3.404889]\n",
      "epoch:14 step:11054 [D loss: 0.039328, acc.: 100.00%] [G loss: 2.941937]\n",
      "epoch:14 step:11055 [D loss: 0.053681, acc.: 100.00%] [G loss: 4.367608]\n",
      "epoch:14 step:11056 [D loss: 0.025964, acc.: 100.00%] [G loss: 4.352193]\n",
      "epoch:14 step:11057 [D loss: 0.091730, acc.: 96.88%] [G loss: 2.990850]\n",
      "epoch:14 step:11058 [D loss: 0.056209, acc.: 99.22%] [G loss: 3.274135]\n",
      "epoch:14 step:11059 [D loss: 0.265609, acc.: 89.06%] [G loss: 4.988946]\n",
      "epoch:14 step:11060 [D loss: 0.214806, acc.: 89.06%] [G loss: 3.874400]\n",
      "epoch:14 step:11061 [D loss: 0.036372, acc.: 100.00%] [G loss: 3.732915]\n",
      "epoch:14 step:11062 [D loss: 0.079757, acc.: 100.00%] [G loss: 3.339398]\n",
      "epoch:14 step:11063 [D loss: 0.111214, acc.: 96.88%] [G loss: 2.922812]\n",
      "epoch:14 step:11064 [D loss: 0.043080, acc.: 100.00%] [G loss: 4.420499]\n",
      "epoch:14 step:11065 [D loss: 0.028607, acc.: 100.00%] [G loss: 4.968689]\n",
      "epoch:14 step:11066 [D loss: 0.895565, acc.: 55.47%] [G loss: 6.530445]\n",
      "epoch:14 step:11067 [D loss: 0.146891, acc.: 94.53%] [G loss: 7.221974]\n",
      "epoch:14 step:11068 [D loss: 0.141937, acc.: 94.53%] [G loss: 4.751667]\n",
      "epoch:14 step:11069 [D loss: 0.040773, acc.: 100.00%] [G loss: 3.371614]\n",
      "epoch:14 step:11070 [D loss: 0.039126, acc.: 100.00%] [G loss: 3.716609]\n",
      "epoch:14 step:11071 [D loss: 0.035575, acc.: 100.00%] [G loss: 3.583257]\n",
      "epoch:14 step:11072 [D loss: 0.078842, acc.: 100.00%] [G loss: 6.159999]\n",
      "epoch:14 step:11073 [D loss: 0.139262, acc.: 94.53%] [G loss: 2.772863]\n",
      "epoch:14 step:11074 [D loss: 0.069548, acc.: 98.44%] [G loss: 5.277936]\n",
      "epoch:14 step:11075 [D loss: 0.026685, acc.: 100.00%] [G loss: 5.035761]\n",
      "epoch:14 step:11076 [D loss: 0.017869, acc.: 100.00%] [G loss: 4.839773]\n",
      "epoch:14 step:11077 [D loss: 0.082739, acc.: 99.22%] [G loss: 5.211205]\n",
      "epoch:14 step:11078 [D loss: 0.028160, acc.: 100.00%] [G loss: 4.932589]\n",
      "epoch:14 step:11079 [D loss: 0.031763, acc.: 100.00%] [G loss: 3.936419]\n",
      "epoch:14 step:11080 [D loss: 0.036354, acc.: 100.00%] [G loss: 4.288889]\n",
      "epoch:14 step:11081 [D loss: 0.071756, acc.: 98.44%] [G loss: 4.577673]\n",
      "epoch:14 step:11082 [D loss: 0.054357, acc.: 100.00%] [G loss: 4.011496]\n",
      "epoch:14 step:11083 [D loss: 0.242222, acc.: 90.62%] [G loss: 5.018630]\n",
      "epoch:14 step:11084 [D loss: 0.042653, acc.: 100.00%] [G loss: 5.741829]\n",
      "epoch:14 step:11085 [D loss: 0.085640, acc.: 98.44%] [G loss: 4.457568]\n",
      "epoch:14 step:11086 [D loss: 0.026802, acc.: 100.00%] [G loss: 4.863350]\n",
      "epoch:14 step:11087 [D loss: 0.577761, acc.: 68.75%] [G loss: 5.577956]\n",
      "epoch:14 step:11088 [D loss: 0.024621, acc.: 100.00%] [G loss: 6.398080]\n",
      "epoch:14 step:11089 [D loss: 0.195550, acc.: 91.41%] [G loss: 3.895130]\n",
      "epoch:14 step:11090 [D loss: 0.024830, acc.: 100.00%] [G loss: 2.538443]\n",
      "epoch:14 step:11091 [D loss: 0.136033, acc.: 95.31%] [G loss: 4.065530]\n",
      "epoch:14 step:11092 [D loss: 0.079605, acc.: 96.88%] [G loss: 3.829570]\n",
      "epoch:14 step:11093 [D loss: 0.026901, acc.: 100.00%] [G loss: 3.378567]\n",
      "epoch:14 step:11094 [D loss: 0.051381, acc.: 100.00%] [G loss: 3.171257]\n",
      "epoch:14 step:11095 [D loss: 0.082051, acc.: 98.44%] [G loss: 3.896186]\n",
      "epoch:14 step:11096 [D loss: 0.015234, acc.: 100.00%] [G loss: 3.729956]\n",
      "epoch:14 step:11097 [D loss: 0.020256, acc.: 100.00%] [G loss: 3.360084]\n",
      "epoch:14 step:11098 [D loss: 0.120591, acc.: 96.09%] [G loss: 3.422779]\n",
      "epoch:14 step:11099 [D loss: 0.194438, acc.: 89.84%] [G loss: 5.801747]\n",
      "epoch:14 step:11100 [D loss: 0.322631, acc.: 84.38%] [G loss: 4.242700]\n",
      "epoch:14 step:11101 [D loss: 0.140055, acc.: 95.31%] [G loss: 5.148907]\n",
      "epoch:14 step:11102 [D loss: 0.002336, acc.: 100.00%] [G loss: 6.265820]\n",
      "epoch:14 step:11103 [D loss: 0.063498, acc.: 98.44%] [G loss: 5.258703]\n",
      "epoch:14 step:11104 [D loss: 0.024925, acc.: 100.00%] [G loss: 5.653130]\n",
      "epoch:14 step:11105 [D loss: 0.006685, acc.: 100.00%] [G loss: 4.751054]\n",
      "epoch:14 step:11106 [D loss: 0.010683, acc.: 100.00%] [G loss: 2.967906]\n",
      "epoch:14 step:11107 [D loss: 0.010882, acc.: 100.00%] [G loss: 3.650405]\n",
      "epoch:14 step:11108 [D loss: 0.043698, acc.: 100.00%] [G loss: 3.430623]\n",
      "epoch:14 step:11109 [D loss: 0.025222, acc.: 100.00%] [G loss: 4.416062]\n",
      "epoch:14 step:11110 [D loss: 0.936777, acc.: 51.56%] [G loss: 7.042562]\n",
      "epoch:14 step:11111 [D loss: 0.667273, acc.: 68.75%] [G loss: 3.360859]\n",
      "epoch:14 step:11112 [D loss: 0.017972, acc.: 100.00%] [G loss: 0.741928]\n",
      "epoch:14 step:11113 [D loss: 0.046083, acc.: 99.22%] [G loss: 3.417115]\n",
      "epoch:14 step:11114 [D loss: 0.047469, acc.: 98.44%] [G loss: 0.891745]\n",
      "epoch:14 step:11115 [D loss: 0.012264, acc.: 100.00%] [G loss: 4.442254]\n",
      "epoch:14 step:11116 [D loss: 0.047847, acc.: 100.00%] [G loss: 1.141279]\n",
      "epoch:14 step:11117 [D loss: 0.030249, acc.: 100.00%] [G loss: 3.120069]\n",
      "epoch:14 step:11118 [D loss: 0.019321, acc.: 100.00%] [G loss: 2.164721]\n",
      "epoch:14 step:11119 [D loss: 0.152990, acc.: 96.09%] [G loss: 1.800484]\n",
      "epoch:14 step:11120 [D loss: 0.025136, acc.: 100.00%] [G loss: 2.799454]\n",
      "epoch:14 step:11121 [D loss: 1.096030, acc.: 49.22%] [G loss: 3.438842]\n",
      "epoch:14 step:11122 [D loss: 0.032029, acc.: 99.22%] [G loss: 6.555813]\n",
      "epoch:14 step:11123 [D loss: 0.625322, acc.: 68.75%] [G loss: 0.829041]\n",
      "epoch:14 step:11124 [D loss: 0.259608, acc.: 88.28%] [G loss: 5.848553]\n",
      "epoch:14 step:11125 [D loss: 0.069683, acc.: 97.66%] [G loss: 5.817080]\n",
      "epoch:14 step:11126 [D loss: 0.004919, acc.: 100.00%] [G loss: 4.119005]\n",
      "epoch:14 step:11127 [D loss: 0.039218, acc.: 98.44%] [G loss: 2.874729]\n",
      "epoch:14 step:11128 [D loss: 0.019963, acc.: 100.00%] [G loss: 2.456257]\n",
      "epoch:14 step:11129 [D loss: 0.041387, acc.: 98.44%] [G loss: 2.876695]\n",
      "epoch:14 step:11130 [D loss: 0.010962, acc.: 100.00%] [G loss: 2.174298]\n",
      "epoch:14 step:11131 [D loss: 0.023467, acc.: 99.22%] [G loss: 1.568444]\n",
      "epoch:14 step:11132 [D loss: 0.013594, acc.: 100.00%] [G loss: 1.413421]\n",
      "epoch:14 step:11133 [D loss: 0.025311, acc.: 99.22%] [G loss: 1.112797]\n",
      "epoch:14 step:11134 [D loss: 0.027023, acc.: 100.00%] [G loss: 2.148092]\n",
      "epoch:14 step:11135 [D loss: 0.024905, acc.: 100.00%] [G loss: 1.638039]\n",
      "epoch:14 step:11136 [D loss: 0.020408, acc.: 100.00%] [G loss: 2.090983]\n",
      "epoch:14 step:11137 [D loss: 0.035664, acc.: 98.44%] [G loss: 3.102613]\n",
      "epoch:14 step:11138 [D loss: 0.044583, acc.: 100.00%] [G loss: 2.197074]\n",
      "epoch:14 step:11139 [D loss: 0.063817, acc.: 100.00%] [G loss: 1.786038]\n",
      "epoch:14 step:11140 [D loss: 0.060761, acc.: 98.44%] [G loss: 3.280396]\n",
      "epoch:14 step:11141 [D loss: 0.142727, acc.: 94.53%] [G loss: 3.916686]\n",
      "epoch:14 step:11142 [D loss: 0.242605, acc.: 88.28%] [G loss: 2.110663]\n",
      "epoch:14 step:11143 [D loss: 0.141238, acc.: 96.09%] [G loss: 2.370139]\n",
      "epoch:14 step:11144 [D loss: 0.033290, acc.: 100.00%] [G loss: 4.864141]\n",
      "epoch:14 step:11145 [D loss: 0.006611, acc.: 100.00%] [G loss: 2.811505]\n",
      "epoch:14 step:11146 [D loss: 0.010889, acc.: 100.00%] [G loss: 5.100395]\n",
      "epoch:14 step:11147 [D loss: 0.060027, acc.: 98.44%] [G loss: 4.167192]\n",
      "epoch:14 step:11148 [D loss: 1.374921, acc.: 38.28%] [G loss: 4.457986]\n",
      "epoch:14 step:11149 [D loss: 0.074950, acc.: 98.44%] [G loss: 4.565030]\n",
      "epoch:14 step:11150 [D loss: 0.470557, acc.: 78.12%] [G loss: 0.418711]\n",
      "epoch:14 step:11151 [D loss: 0.122479, acc.: 94.53%] [G loss: 1.907606]\n",
      "epoch:14 step:11152 [D loss: 0.049543, acc.: 99.22%] [G loss: 0.715203]\n",
      "epoch:14 step:11153 [D loss: 0.019369, acc.: 100.00%] [G loss: 2.169297]\n",
      "epoch:14 step:11154 [D loss: 0.096043, acc.: 97.66%] [G loss: 0.775242]\n",
      "epoch:14 step:11155 [D loss: 0.008732, acc.: 100.00%] [G loss: 0.509083]\n",
      "epoch:14 step:11156 [D loss: 0.082915, acc.: 99.22%] [G loss: 2.481050]\n",
      "epoch:14 step:11157 [D loss: 0.008819, acc.: 100.00%] [G loss: 1.228363]\n",
      "epoch:14 step:11158 [D loss: 0.048795, acc.: 99.22%] [G loss: 0.750259]\n",
      "epoch:14 step:11159 [D loss: 0.262131, acc.: 91.41%] [G loss: 5.449909]\n",
      "epoch:14 step:11160 [D loss: 0.204561, acc.: 91.41%] [G loss: 4.631989]\n",
      "epoch:14 step:11161 [D loss: 0.026881, acc.: 100.00%] [G loss: 3.131391]\n",
      "epoch:14 step:11162 [D loss: 0.019828, acc.: 100.00%] [G loss: 4.593950]\n",
      "epoch:14 step:11163 [D loss: 0.022646, acc.: 100.00%] [G loss: 4.268331]\n",
      "epoch:14 step:11164 [D loss: 0.023819, acc.: 100.00%] [G loss: 4.279326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11165 [D loss: 0.065238, acc.: 98.44%] [G loss: 3.205490]\n",
      "epoch:14 step:11166 [D loss: 0.061853, acc.: 99.22%] [G loss: 3.378989]\n",
      "epoch:14 step:11167 [D loss: 0.021037, acc.: 100.00%] [G loss: 1.750943]\n",
      "epoch:14 step:11168 [D loss: 0.134822, acc.: 97.66%] [G loss: 1.696352]\n",
      "epoch:14 step:11169 [D loss: 0.025590, acc.: 100.00%] [G loss: 2.691133]\n",
      "epoch:14 step:11170 [D loss: 0.038982, acc.: 99.22%] [G loss: 2.649289]\n",
      "epoch:14 step:11171 [D loss: 0.011185, acc.: 100.00%] [G loss: 3.142514]\n",
      "epoch:14 step:11172 [D loss: 0.020572, acc.: 99.22%] [G loss: 2.245065]\n",
      "epoch:14 step:11173 [D loss: 0.025222, acc.: 100.00%] [G loss: 3.958051]\n",
      "epoch:14 step:11174 [D loss: 3.514131, acc.: 19.53%] [G loss: 7.293512]\n",
      "epoch:14 step:11175 [D loss: 0.831385, acc.: 57.03%] [G loss: 3.688313]\n",
      "epoch:14 step:11176 [D loss: 0.035529, acc.: 100.00%] [G loss: 3.231566]\n",
      "epoch:14 step:11177 [D loss: 0.125050, acc.: 96.09%] [G loss: 3.814484]\n",
      "epoch:14 step:11178 [D loss: 0.032899, acc.: 99.22%] [G loss: 4.020481]\n",
      "epoch:14 step:11179 [D loss: 0.050921, acc.: 100.00%] [G loss: 4.206140]\n",
      "epoch:14 step:11180 [D loss: 0.082728, acc.: 99.22%] [G loss: 4.106549]\n",
      "epoch:14 step:11181 [D loss: 0.230667, acc.: 92.19%] [G loss: 5.265595]\n",
      "epoch:14 step:11182 [D loss: 0.046114, acc.: 100.00%] [G loss: 5.482547]\n",
      "epoch:14 step:11183 [D loss: 0.102902, acc.: 96.88%] [G loss: 4.350047]\n",
      "epoch:14 step:11184 [D loss: 0.072518, acc.: 99.22%] [G loss: 0.853421]\n",
      "epoch:14 step:11185 [D loss: 0.096450, acc.: 96.88%] [G loss: 5.551206]\n",
      "epoch:14 step:11186 [D loss: 0.015664, acc.: 100.00%] [G loss: 5.889329]\n",
      "epoch:14 step:11187 [D loss: 0.056917, acc.: 99.22%] [G loss: 5.884455]\n",
      "epoch:14 step:11188 [D loss: 0.076523, acc.: 98.44%] [G loss: 4.862267]\n",
      "epoch:14 step:11189 [D loss: 0.021744, acc.: 100.00%] [G loss: 4.098141]\n",
      "epoch:14 step:11190 [D loss: 0.039971, acc.: 100.00%] [G loss: 5.011168]\n",
      "epoch:14 step:11191 [D loss: 0.086406, acc.: 98.44%] [G loss: 0.527393]\n",
      "epoch:14 step:11192 [D loss: 0.055929, acc.: 100.00%] [G loss: 6.057323]\n",
      "epoch:14 step:11193 [D loss: 0.301213, acc.: 89.84%] [G loss: 6.085958]\n",
      "epoch:14 step:11194 [D loss: 0.008159, acc.: 100.00%] [G loss: 1.839895]\n",
      "epoch:14 step:11195 [D loss: 0.070200, acc.: 98.44%] [G loss: 5.199212]\n",
      "epoch:14 step:11196 [D loss: 0.011567, acc.: 100.00%] [G loss: 5.232879]\n",
      "epoch:14 step:11197 [D loss: 0.081160, acc.: 97.66%] [G loss: 5.102565]\n",
      "epoch:14 step:11198 [D loss: 0.106470, acc.: 96.88%] [G loss: 4.516176]\n",
      "epoch:14 step:11199 [D loss: 0.039643, acc.: 99.22%] [G loss: 0.290275]\n",
      "epoch:14 step:11200 [D loss: 0.069037, acc.: 99.22%] [G loss: 3.548031]\n",
      "epoch:14 step:11201 [D loss: 0.211097, acc.: 93.75%] [G loss: 6.147629]\n",
      "epoch:14 step:11202 [D loss: 0.050752, acc.: 98.44%] [G loss: 1.912254]\n",
      "epoch:14 step:11203 [D loss: 0.300101, acc.: 87.50%] [G loss: 5.085946]\n",
      "epoch:14 step:11204 [D loss: 0.075764, acc.: 97.66%] [G loss: 0.262222]\n",
      "epoch:14 step:11205 [D loss: 0.018855, acc.: 100.00%] [G loss: 0.694477]\n",
      "epoch:14 step:11206 [D loss: 0.023823, acc.: 99.22%] [G loss: 5.609492]\n",
      "epoch:14 step:11207 [D loss: 0.091082, acc.: 99.22%] [G loss: 1.578850]\n",
      "epoch:14 step:11208 [D loss: 0.095924, acc.: 98.44%] [G loss: 1.257066]\n",
      "epoch:14 step:11209 [D loss: 0.035048, acc.: 98.44%] [G loss: 5.572925]\n",
      "epoch:14 step:11210 [D loss: 0.084314, acc.: 98.44%] [G loss: 2.124566]\n",
      "epoch:14 step:11211 [D loss: 0.053516, acc.: 99.22%] [G loss: 0.549065]\n",
      "epoch:14 step:11212 [D loss: 0.049494, acc.: 100.00%] [G loss: 5.379116]\n",
      "epoch:14 step:11213 [D loss: 0.116529, acc.: 98.44%] [G loss: 2.852126]\n",
      "epoch:14 step:11214 [D loss: 0.008329, acc.: 100.00%] [G loss: 3.986686]\n",
      "epoch:14 step:11215 [D loss: 0.018659, acc.: 100.00%] [G loss: 1.294127]\n",
      "epoch:14 step:11216 [D loss: 0.154690, acc.: 94.53%] [G loss: 2.385325]\n",
      "epoch:14 step:11217 [D loss: 0.041225, acc.: 99.22%] [G loss: 5.384190]\n",
      "epoch:14 step:11218 [D loss: 0.168282, acc.: 94.53%] [G loss: 4.152440]\n",
      "epoch:14 step:11219 [D loss: 0.019638, acc.: 100.00%] [G loss: 3.110777]\n",
      "epoch:14 step:11220 [D loss: 0.084290, acc.: 98.44%] [G loss: 1.682532]\n",
      "epoch:14 step:11221 [D loss: 0.009735, acc.: 100.00%] [G loss: 3.417180]\n",
      "epoch:14 step:11222 [D loss: 0.016953, acc.: 100.00%] [G loss: 2.419007]\n",
      "epoch:14 step:11223 [D loss: 0.008495, acc.: 100.00%] [G loss: 2.458190]\n",
      "epoch:14 step:11224 [D loss: 0.015936, acc.: 100.00%] [G loss: 1.409506]\n",
      "epoch:14 step:11225 [D loss: 0.018350, acc.: 100.00%] [G loss: 1.855227]\n",
      "epoch:14 step:11226 [D loss: 0.031506, acc.: 99.22%] [G loss: 1.000354]\n",
      "epoch:14 step:11227 [D loss: 0.055080, acc.: 100.00%] [G loss: 4.498339]\n",
      "epoch:14 step:11228 [D loss: 0.052857, acc.: 100.00%] [G loss: 3.837771]\n",
      "epoch:14 step:11229 [D loss: 0.772295, acc.: 58.59%] [G loss: 4.739933]\n",
      "epoch:14 step:11230 [D loss: 1.489300, acc.: 54.69%] [G loss: 0.840847]\n",
      "epoch:14 step:11231 [D loss: 0.535874, acc.: 72.66%] [G loss: 3.052670]\n",
      "epoch:14 step:11232 [D loss: 0.022958, acc.: 100.00%] [G loss: 3.841807]\n",
      "epoch:14 step:11233 [D loss: 0.100312, acc.: 97.66%] [G loss: 3.345447]\n",
      "epoch:14 step:11234 [D loss: 0.050137, acc.: 99.22%] [G loss: 1.499140]\n",
      "epoch:14 step:11235 [D loss: 0.039877, acc.: 100.00%] [G loss: 2.900209]\n",
      "epoch:14 step:11236 [D loss: 0.008554, acc.: 100.00%] [G loss: 4.044326]\n",
      "epoch:14 step:11237 [D loss: 0.011296, acc.: 100.00%] [G loss: 1.373803]\n",
      "epoch:14 step:11238 [D loss: 0.037690, acc.: 100.00%] [G loss: 0.798081]\n",
      "epoch:14 step:11239 [D loss: 0.077471, acc.: 97.66%] [G loss: 3.434205]\n",
      "epoch:14 step:11240 [D loss: 0.014877, acc.: 100.00%] [G loss: 2.570901]\n",
      "epoch:14 step:11241 [D loss: 0.124837, acc.: 96.09%] [G loss: 5.162119]\n",
      "epoch:14 step:11242 [D loss: 0.015866, acc.: 100.00%] [G loss: 2.781642]\n",
      "epoch:14 step:11243 [D loss: 0.053683, acc.: 97.66%] [G loss: 4.505749]\n",
      "epoch:14 step:11244 [D loss: 0.025637, acc.: 100.00%] [G loss: 0.581013]\n",
      "epoch:14 step:11245 [D loss: 0.027291, acc.: 100.00%] [G loss: 4.349425]\n",
      "epoch:14 step:11246 [D loss: 0.033337, acc.: 100.00%] [G loss: 4.672833]\n",
      "epoch:14 step:11247 [D loss: 0.024510, acc.: 100.00%] [G loss: 1.561253]\n",
      "epoch:14 step:11248 [D loss: 0.026033, acc.: 100.00%] [G loss: 4.147682]\n",
      "epoch:14 step:11249 [D loss: 0.630919, acc.: 64.84%] [G loss: 0.026152]\n",
      "epoch:14 step:11250 [D loss: 0.894074, acc.: 58.59%] [G loss: 7.145598]\n",
      "epoch:14 step:11251 [D loss: 1.000786, acc.: 60.16%] [G loss: 2.717724]\n",
      "epoch:14 step:11252 [D loss: 0.023165, acc.: 98.44%] [G loss: 4.208667]\n",
      "epoch:14 step:11253 [D loss: 0.046795, acc.: 98.44%] [G loss: 4.090425]\n",
      "epoch:14 step:11254 [D loss: 0.520220, acc.: 75.00%] [G loss: 3.355315]\n",
      "epoch:14 step:11255 [D loss: 0.004481, acc.: 100.00%] [G loss: 2.057997]\n",
      "epoch:14 step:11256 [D loss: 0.010526, acc.: 100.00%] [G loss: 2.105072]\n",
      "epoch:14 step:11257 [D loss: 0.017455, acc.: 100.00%] [G loss: 4.361361]\n",
      "epoch:14 step:11258 [D loss: 0.060115, acc.: 99.22%] [G loss: 1.367118]\n",
      "epoch:14 step:11259 [D loss: 0.069606, acc.: 99.22%] [G loss: 5.591420]\n",
      "epoch:14 step:11260 [D loss: 0.054298, acc.: 98.44%] [G loss: 0.981930]\n",
      "epoch:14 step:11261 [D loss: 0.430927, acc.: 81.25%] [G loss: 3.849800]\n",
      "epoch:14 step:11262 [D loss: 0.236930, acc.: 90.62%] [G loss: 3.965653]\n",
      "epoch:14 step:11263 [D loss: 0.099977, acc.: 97.66%] [G loss: 2.030674]\n",
      "epoch:14 step:11264 [D loss: 0.032132, acc.: 100.00%] [G loss: 1.547521]\n",
      "epoch:14 step:11265 [D loss: 0.975126, acc.: 48.44%] [G loss: 5.415168]\n",
      "epoch:14 step:11266 [D loss: 0.117856, acc.: 94.53%] [G loss: 7.165979]\n",
      "epoch:14 step:11267 [D loss: 0.289862, acc.: 85.16%] [G loss: 0.627432]\n",
      "epoch:14 step:11268 [D loss: 0.381310, acc.: 83.59%] [G loss: 5.489383]\n",
      "epoch:14 step:11269 [D loss: 0.011937, acc.: 100.00%] [G loss: 6.504869]\n",
      "epoch:14 step:11270 [D loss: 0.596391, acc.: 75.00%] [G loss: 3.177941]\n",
      "epoch:14 step:11271 [D loss: 0.306348, acc.: 85.94%] [G loss: 4.781252]\n",
      "epoch:14 step:11272 [D loss: 0.009254, acc.: 100.00%] [G loss: 5.539418]\n",
      "epoch:14 step:11273 [D loss: 0.061408, acc.: 97.66%] [G loss: 5.204872]\n",
      "epoch:14 step:11274 [D loss: 0.070761, acc.: 97.66%] [G loss: 3.917850]\n",
      "epoch:14 step:11275 [D loss: 0.077843, acc.: 99.22%] [G loss: 3.750590]\n",
      "epoch:14 step:11276 [D loss: 0.019821, acc.: 99.22%] [G loss: 5.158244]\n",
      "epoch:14 step:11277 [D loss: 0.094023, acc.: 97.66%] [G loss: 4.219587]\n",
      "epoch:14 step:11278 [D loss: 0.024249, acc.: 100.00%] [G loss: 3.857489]\n",
      "epoch:14 step:11279 [D loss: 0.299067, acc.: 89.06%] [G loss: 3.634644]\n",
      "epoch:14 step:11280 [D loss: 0.125176, acc.: 96.88%] [G loss: 3.791422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11281 [D loss: 0.033012, acc.: 99.22%] [G loss: 4.962219]\n",
      "epoch:14 step:11282 [D loss: 0.039734, acc.: 99.22%] [G loss: 2.825027]\n",
      "epoch:14 step:11283 [D loss: 0.020254, acc.: 100.00%] [G loss: 3.168690]\n",
      "epoch:14 step:11284 [D loss: 0.392125, acc.: 82.81%] [G loss: 6.109981]\n",
      "epoch:14 step:11285 [D loss: 0.147539, acc.: 94.53%] [G loss: 5.245146]\n",
      "epoch:14 step:11286 [D loss: 0.011463, acc.: 100.00%] [G loss: 5.797826]\n",
      "epoch:14 step:11287 [D loss: 0.039156, acc.: 99.22%] [G loss: 3.727086]\n",
      "epoch:14 step:11288 [D loss: 0.049703, acc.: 99.22%] [G loss: 3.149621]\n",
      "epoch:14 step:11289 [D loss: 0.040814, acc.: 99.22%] [G loss: 3.883084]\n",
      "epoch:14 step:11290 [D loss: 0.066670, acc.: 99.22%] [G loss: 3.367406]\n",
      "epoch:14 step:11291 [D loss: 0.019765, acc.: 100.00%] [G loss: 3.650114]\n",
      "epoch:14 step:11292 [D loss: 0.125968, acc.: 97.66%] [G loss: 3.515018]\n",
      "epoch:14 step:11293 [D loss: 0.051295, acc.: 100.00%] [G loss: 3.924442]\n",
      "epoch:14 step:11294 [D loss: 0.017154, acc.: 100.00%] [G loss: 3.357376]\n",
      "epoch:14 step:11295 [D loss: 0.216187, acc.: 94.53%] [G loss: 4.790653]\n",
      "epoch:14 step:11296 [D loss: 0.055371, acc.: 98.44%] [G loss: 4.383345]\n",
      "epoch:14 step:11297 [D loss: 0.076268, acc.: 99.22%] [G loss: 3.463784]\n",
      "epoch:14 step:11298 [D loss: 0.102963, acc.: 97.66%] [G loss: 4.199702]\n",
      "epoch:14 step:11299 [D loss: 0.016534, acc.: 100.00%] [G loss: 5.047778]\n",
      "epoch:14 step:11300 [D loss: 0.029173, acc.: 100.00%] [G loss: 4.360158]\n",
      "epoch:14 step:11301 [D loss: 0.016682, acc.: 100.00%] [G loss: 4.425597]\n",
      "epoch:14 step:11302 [D loss: 0.014687, acc.: 100.00%] [G loss: 4.787513]\n",
      "epoch:14 step:11303 [D loss: 0.019818, acc.: 100.00%] [G loss: 4.331761]\n",
      "epoch:14 step:11304 [D loss: 0.105666, acc.: 96.88%] [G loss: 1.961590]\n",
      "epoch:14 step:11305 [D loss: 0.049042, acc.: 99.22%] [G loss: 4.784870]\n",
      "epoch:14 step:11306 [D loss: 0.016347, acc.: 100.00%] [G loss: 5.629574]\n",
      "epoch:14 step:11307 [D loss: 0.043502, acc.: 99.22%] [G loss: 4.888796]\n",
      "epoch:14 step:11308 [D loss: 0.044152, acc.: 99.22%] [G loss: 3.376921]\n",
      "epoch:14 step:11309 [D loss: 0.251571, acc.: 90.62%] [G loss: 6.742477]\n",
      "epoch:14 step:11310 [D loss: 0.932019, acc.: 57.81%] [G loss: 4.196827]\n",
      "epoch:14 step:11311 [D loss: 0.024103, acc.: 100.00%] [G loss: 4.389990]\n",
      "epoch:14 step:11312 [D loss: 0.047940, acc.: 100.00%] [G loss: 1.808881]\n",
      "epoch:14 step:11313 [D loss: 0.015809, acc.: 100.00%] [G loss: 0.927797]\n",
      "epoch:14 step:11314 [D loss: 0.030577, acc.: 99.22%] [G loss: 1.381893]\n",
      "epoch:14 step:11315 [D loss: 0.035359, acc.: 100.00%] [G loss: 4.840673]\n",
      "epoch:14 step:11316 [D loss: 0.064194, acc.: 98.44%] [G loss: 6.043843]\n",
      "epoch:14 step:11317 [D loss: 0.008978, acc.: 100.00%] [G loss: 2.603770]\n",
      "epoch:14 step:11318 [D loss: 0.149928, acc.: 94.53%] [G loss: 3.920906]\n",
      "epoch:14 step:11319 [D loss: 0.206538, acc.: 89.06%] [G loss: 5.787148]\n",
      "epoch:14 step:11320 [D loss: 0.533732, acc.: 75.00%] [G loss: 3.610872]\n",
      "epoch:14 step:11321 [D loss: 0.295733, acc.: 88.28%] [G loss: 4.961913]\n",
      "epoch:14 step:11322 [D loss: 0.084203, acc.: 96.88%] [G loss: 5.377044]\n",
      "epoch:14 step:11323 [D loss: 0.025507, acc.: 99.22%] [G loss: 5.276559]\n",
      "epoch:14 step:11324 [D loss: 0.010097, acc.: 100.00%] [G loss: 4.583100]\n",
      "epoch:14 step:11325 [D loss: 0.016335, acc.: 100.00%] [G loss: 4.257477]\n",
      "epoch:14 step:11326 [D loss: 0.012870, acc.: 100.00%] [G loss: 3.934119]\n",
      "epoch:14 step:11327 [D loss: 0.039778, acc.: 98.44%] [G loss: 4.182990]\n",
      "epoch:14 step:11328 [D loss: 0.005987, acc.: 100.00%] [G loss: 2.056912]\n",
      "epoch:14 step:11329 [D loss: 0.060214, acc.: 99.22%] [G loss: 1.846144]\n",
      "epoch:14 step:11330 [D loss: 0.049394, acc.: 100.00%] [G loss: 3.552030]\n",
      "epoch:14 step:11331 [D loss: 0.054864, acc.: 98.44%] [G loss: 3.976561]\n",
      "epoch:14 step:11332 [D loss: 0.011296, acc.: 100.00%] [G loss: 4.725671]\n",
      "epoch:14 step:11333 [D loss: 0.015825, acc.: 100.00%] [G loss: 5.764043]\n",
      "epoch:14 step:11334 [D loss: 0.028374, acc.: 100.00%] [G loss: 4.367110]\n",
      "epoch:14 step:11335 [D loss: 0.400734, acc.: 81.25%] [G loss: 4.832431]\n",
      "epoch:14 step:11336 [D loss: 0.032050, acc.: 100.00%] [G loss: 3.607252]\n",
      "epoch:14 step:11337 [D loss: 0.528270, acc.: 75.78%] [G loss: 3.365150]\n",
      "epoch:14 step:11338 [D loss: 0.017425, acc.: 100.00%] [G loss: 1.803255]\n",
      "epoch:14 step:11339 [D loss: 0.022515, acc.: 100.00%] [G loss: 4.347568]\n",
      "epoch:14 step:11340 [D loss: 0.028597, acc.: 99.22%] [G loss: 3.155783]\n",
      "epoch:14 step:11341 [D loss: 0.009035, acc.: 100.00%] [G loss: 5.611995]\n",
      "epoch:14 step:11342 [D loss: 0.004006, acc.: 100.00%] [G loss: 2.581265]\n",
      "epoch:14 step:11343 [D loss: 0.012908, acc.: 100.00%] [G loss: 2.997689]\n",
      "epoch:14 step:11344 [D loss: 0.055768, acc.: 97.66%] [G loss: 4.628331]\n",
      "epoch:14 step:11345 [D loss: 0.102569, acc.: 97.66%] [G loss: 4.496683]\n",
      "epoch:14 step:11346 [D loss: 0.039337, acc.: 100.00%] [G loss: 4.169593]\n",
      "epoch:14 step:11347 [D loss: 0.063321, acc.: 98.44%] [G loss: 3.290636]\n",
      "epoch:14 step:11348 [D loss: 0.006129, acc.: 100.00%] [G loss: 4.760773]\n",
      "epoch:14 step:11349 [D loss: 0.013641, acc.: 100.00%] [G loss: 3.780566]\n",
      "epoch:14 step:11350 [D loss: 0.056672, acc.: 98.44%] [G loss: 4.308552]\n",
      "epoch:14 step:11351 [D loss: 0.179665, acc.: 92.19%] [G loss: 2.448309]\n",
      "epoch:14 step:11352 [D loss: 0.182031, acc.: 92.97%] [G loss: 6.293328]\n",
      "epoch:14 step:11353 [D loss: 0.084885, acc.: 98.44%] [G loss: 5.353120]\n",
      "epoch:14 step:11354 [D loss: 0.821884, acc.: 63.28%] [G loss: 5.378840]\n",
      "epoch:14 step:11355 [D loss: 0.002660, acc.: 100.00%] [G loss: 3.498732]\n",
      "epoch:14 step:11356 [D loss: 0.171622, acc.: 92.19%] [G loss: 1.357511]\n",
      "epoch:14 step:11357 [D loss: 0.012814, acc.: 100.00%] [G loss: 0.965332]\n",
      "epoch:14 step:11358 [D loss: 0.004942, acc.: 100.00%] [G loss: 1.322688]\n",
      "epoch:14 step:11359 [D loss: 0.077015, acc.: 96.09%] [G loss: 2.174943]\n",
      "epoch:14 step:11360 [D loss: 0.007074, acc.: 100.00%] [G loss: 2.112907]\n",
      "epoch:14 step:11361 [D loss: 0.052365, acc.: 99.22%] [G loss: 1.714258]\n",
      "epoch:14 step:11362 [D loss: 0.034783, acc.: 99.22%] [G loss: 4.919942]\n",
      "epoch:14 step:11363 [D loss: 0.063861, acc.: 98.44%] [G loss: 1.843909]\n",
      "epoch:14 step:11364 [D loss: 0.145110, acc.: 94.53%] [G loss: 3.570311]\n",
      "epoch:14 step:11365 [D loss: 0.102853, acc.: 96.09%] [G loss: 2.742122]\n",
      "epoch:14 step:11366 [D loss: 0.115610, acc.: 94.53%] [G loss: 5.691136]\n",
      "epoch:14 step:11367 [D loss: 0.002788, acc.: 100.00%] [G loss: 6.289509]\n",
      "epoch:14 step:11368 [D loss: 0.013037, acc.: 100.00%] [G loss: 6.075583]\n",
      "epoch:14 step:11369 [D loss: 0.069578, acc.: 98.44%] [G loss: 3.934153]\n",
      "epoch:14 step:11370 [D loss: 0.007132, acc.: 100.00%] [G loss: 3.320215]\n",
      "epoch:14 step:11371 [D loss: 0.035319, acc.: 99.22%] [G loss: 4.341298]\n",
      "epoch:14 step:11372 [D loss: 0.013619, acc.: 100.00%] [G loss: 2.271889]\n",
      "epoch:14 step:11373 [D loss: 0.015781, acc.: 100.00%] [G loss: 5.097763]\n",
      "epoch:14 step:11374 [D loss: 0.005511, acc.: 100.00%] [G loss: 5.615232]\n",
      "epoch:14 step:11375 [D loss: 0.050991, acc.: 98.44%] [G loss: 4.358958]\n",
      "epoch:14 step:11376 [D loss: 0.008951, acc.: 100.00%] [G loss: 4.313306]\n",
      "epoch:14 step:11377 [D loss: 0.015021, acc.: 100.00%] [G loss: 1.810598]\n",
      "epoch:14 step:11378 [D loss: 0.032320, acc.: 100.00%] [G loss: 3.709002]\n",
      "epoch:14 step:11379 [D loss: 0.068015, acc.: 99.22%] [G loss: 4.723904]\n",
      "epoch:14 step:11380 [D loss: 0.016801, acc.: 100.00%] [G loss: 6.060034]\n",
      "epoch:14 step:11381 [D loss: 0.044693, acc.: 100.00%] [G loss: 5.153845]\n",
      "epoch:14 step:11382 [D loss: 0.067613, acc.: 97.66%] [G loss: 4.303624]\n",
      "epoch:14 step:11383 [D loss: 0.013647, acc.: 100.00%] [G loss: 5.182306]\n",
      "epoch:14 step:11384 [D loss: 0.008737, acc.: 100.00%] [G loss: 5.440310]\n",
      "epoch:14 step:11385 [D loss: 0.053081, acc.: 100.00%] [G loss: 1.516687]\n",
      "epoch:14 step:11386 [D loss: 0.076923, acc.: 98.44%] [G loss: 6.097645]\n",
      "epoch:14 step:11387 [D loss: 0.193294, acc.: 91.41%] [G loss: 5.559871]\n",
      "epoch:14 step:11388 [D loss: 0.011577, acc.: 100.00%] [G loss: 2.973323]\n",
      "epoch:14 step:11389 [D loss: 0.007570, acc.: 100.00%] [G loss: 5.821906]\n",
      "epoch:14 step:11390 [D loss: 0.007318, acc.: 100.00%] [G loss: 5.939414]\n",
      "epoch:14 step:11391 [D loss: 0.007249, acc.: 100.00%] [G loss: 5.263111]\n",
      "epoch:14 step:11392 [D loss: 0.022764, acc.: 98.44%] [G loss: 1.702627]\n",
      "epoch:14 step:11393 [D loss: 2.285020, acc.: 39.06%] [G loss: 7.774936]\n",
      "epoch:14 step:11394 [D loss: 2.455948, acc.: 50.78%] [G loss: 6.212555]\n",
      "epoch:14 step:11395 [D loss: 0.753886, acc.: 67.19%] [G loss: 0.442652]\n",
      "epoch:14 step:11396 [D loss: 0.211320, acc.: 92.19%] [G loss: 1.630748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11397 [D loss: 0.045359, acc.: 99.22%] [G loss: 2.617142]\n",
      "epoch:14 step:11398 [D loss: 0.070754, acc.: 97.66%] [G loss: 1.083191]\n",
      "epoch:14 step:11399 [D loss: 0.193690, acc.: 90.62%] [G loss: 4.804295]\n",
      "epoch:14 step:11400 [D loss: 0.104294, acc.: 96.09%] [G loss: 4.637983]\n",
      "epoch:14 step:11401 [D loss: 0.182093, acc.: 92.97%] [G loss: 2.128901]\n",
      "epoch:14 step:11402 [D loss: 0.112368, acc.: 97.66%] [G loss: 1.432327]\n",
      "epoch:14 step:11403 [D loss: 0.027768, acc.: 99.22%] [G loss: 1.481249]\n",
      "epoch:14 step:11404 [D loss: 0.104471, acc.: 96.88%] [G loss: 1.322927]\n",
      "epoch:14 step:11405 [D loss: 0.103283, acc.: 97.66%] [G loss: 2.884894]\n",
      "epoch:14 step:11406 [D loss: 0.218187, acc.: 89.84%] [G loss: 2.651619]\n",
      "epoch:14 step:11407 [D loss: 0.152503, acc.: 94.53%] [G loss: 1.268117]\n",
      "epoch:14 step:11408 [D loss: 0.075787, acc.: 98.44%] [G loss: 0.961267]\n",
      "epoch:14 step:11409 [D loss: 0.116903, acc.: 96.88%] [G loss: 0.125355]\n",
      "epoch:14 step:11410 [D loss: 0.029263, acc.: 100.00%] [G loss: 0.731781]\n",
      "epoch:14 step:11411 [D loss: 0.236812, acc.: 86.72%] [G loss: 1.500358]\n",
      "epoch:14 step:11412 [D loss: 0.083917, acc.: 97.66%] [G loss: 4.139802]\n",
      "epoch:14 step:11413 [D loss: 0.203824, acc.: 92.97%] [G loss: 2.319451]\n",
      "epoch:14 step:11414 [D loss: 0.038802, acc.: 99.22%] [G loss: 1.432609]\n",
      "epoch:14 step:11415 [D loss: 0.096371, acc.: 98.44%] [G loss: 0.623430]\n",
      "epoch:14 step:11416 [D loss: 0.253839, acc.: 89.06%] [G loss: 0.425719]\n",
      "epoch:14 step:11417 [D loss: 0.041363, acc.: 98.44%] [G loss: 0.364914]\n",
      "epoch:14 step:11418 [D loss: 0.026650, acc.: 100.00%] [G loss: 0.541416]\n",
      "epoch:14 step:11419 [D loss: 0.047800, acc.: 97.66%] [G loss: 0.563428]\n",
      "epoch:14 step:11420 [D loss: 0.033559, acc.: 100.00%] [G loss: 0.851523]\n",
      "epoch:14 step:11421 [D loss: 0.258791, acc.: 91.41%] [G loss: 0.422017]\n",
      "epoch:14 step:11422 [D loss: 0.008490, acc.: 100.00%] [G loss: 1.063436]\n",
      "epoch:14 step:11423 [D loss: 0.069942, acc.: 98.44%] [G loss: 0.810267]\n",
      "epoch:14 step:11424 [D loss: 0.066089, acc.: 98.44%] [G loss: 0.308276]\n",
      "epoch:14 step:11425 [D loss: 0.481463, acc.: 76.56%] [G loss: 4.638168]\n",
      "epoch:14 step:11426 [D loss: 0.136114, acc.: 93.75%] [G loss: 2.630814]\n",
      "epoch:14 step:11427 [D loss: 0.097070, acc.: 96.09%] [G loss: 1.070905]\n",
      "epoch:14 step:11428 [D loss: 0.052559, acc.: 98.44%] [G loss: 1.587226]\n",
      "epoch:14 step:11429 [D loss: 0.033067, acc.: 100.00%] [G loss: 0.550220]\n",
      "epoch:14 step:11430 [D loss: 0.118991, acc.: 93.75%] [G loss: 2.785865]\n",
      "epoch:14 step:11431 [D loss: 0.297871, acc.: 85.16%] [G loss: 0.451926]\n",
      "epoch:14 step:11432 [D loss: 0.613894, acc.: 75.00%] [G loss: 6.045313]\n",
      "epoch:14 step:11433 [D loss: 0.540737, acc.: 73.44%] [G loss: 5.892389]\n",
      "epoch:14 step:11434 [D loss: 0.082968, acc.: 96.88%] [G loss: 1.927129]\n",
      "epoch:14 step:11435 [D loss: 0.296293, acc.: 87.50%] [G loss: 0.768004]\n",
      "epoch:14 step:11436 [D loss: 0.032969, acc.: 100.00%] [G loss: 3.735163]\n",
      "epoch:14 step:11437 [D loss: 0.026640, acc.: 100.00%] [G loss: 3.172330]\n",
      "epoch:14 step:11438 [D loss: 0.014722, acc.: 100.00%] [G loss: 1.045190]\n",
      "epoch:14 step:11439 [D loss: 0.035974, acc.: 100.00%] [G loss: 0.786051]\n",
      "epoch:14 step:11440 [D loss: 0.045945, acc.: 100.00%] [G loss: 1.660918]\n",
      "epoch:14 step:11441 [D loss: 0.024043, acc.: 99.22%] [G loss: 2.916529]\n",
      "epoch:14 step:11442 [D loss: 0.036668, acc.: 100.00%] [G loss: 0.851069]\n",
      "epoch:14 step:11443 [D loss: 0.062164, acc.: 98.44%] [G loss: 0.620935]\n",
      "epoch:14 step:11444 [D loss: 0.127675, acc.: 96.88%] [G loss: 3.090471]\n",
      "epoch:14 step:11445 [D loss: 0.106678, acc.: 95.31%] [G loss: 1.191072]\n",
      "epoch:14 step:11446 [D loss: 0.200545, acc.: 92.19%] [G loss: 1.928803]\n",
      "epoch:14 step:11447 [D loss: 0.075605, acc.: 98.44%] [G loss: 2.142173]\n",
      "epoch:14 step:11448 [D loss: 0.061823, acc.: 98.44%] [G loss: 3.297505]\n",
      "epoch:14 step:11449 [D loss: 0.024916, acc.: 100.00%] [G loss: 1.513107]\n",
      "epoch:14 step:11450 [D loss: 0.083436, acc.: 97.66%] [G loss: 1.814795]\n",
      "epoch:14 step:11451 [D loss: 0.123848, acc.: 96.09%] [G loss: 1.452719]\n",
      "epoch:14 step:11452 [D loss: 0.174905, acc.: 92.97%] [G loss: 2.561654]\n",
      "epoch:14 step:11453 [D loss: 0.208142, acc.: 92.97%] [G loss: 3.641025]\n",
      "epoch:14 step:11454 [D loss: 0.047957, acc.: 99.22%] [G loss: 1.110181]\n",
      "epoch:14 step:11455 [D loss: 0.016842, acc.: 100.00%] [G loss: 0.476327]\n",
      "epoch:14 step:11456 [D loss: 0.019716, acc.: 100.00%] [G loss: 2.837527]\n",
      "epoch:14 step:11457 [D loss: 0.055796, acc.: 100.00%] [G loss: 0.585758]\n",
      "epoch:14 step:11458 [D loss: 0.056736, acc.: 98.44%] [G loss: 2.153037]\n",
      "epoch:14 step:11459 [D loss: 0.035808, acc.: 100.00%] [G loss: 1.946050]\n",
      "epoch:14 step:11460 [D loss: 0.146519, acc.: 96.88%] [G loss: 0.614039]\n",
      "epoch:14 step:11461 [D loss: 0.025413, acc.: 99.22%] [G loss: 3.998947]\n",
      "epoch:14 step:11462 [D loss: 0.068792, acc.: 99.22%] [G loss: 0.305512]\n",
      "epoch:14 step:11463 [D loss: 0.004185, acc.: 100.00%] [G loss: 2.477479]\n",
      "epoch:14 step:11464 [D loss: 0.034951, acc.: 99.22%] [G loss: 2.689180]\n",
      "epoch:14 step:11465 [D loss: 0.116953, acc.: 96.88%] [G loss: 0.966071]\n",
      "epoch:14 step:11466 [D loss: 0.281793, acc.: 85.94%] [G loss: 0.305837]\n",
      "epoch:14 step:11467 [D loss: 0.288747, acc.: 86.72%] [G loss: 1.784497]\n",
      "epoch:14 step:11468 [D loss: 0.005628, acc.: 100.00%] [G loss: 8.907586]\n",
      "epoch:14 step:11469 [D loss: 0.745385, acc.: 68.75%] [G loss: 0.142103]\n",
      "epoch:14 step:11470 [D loss: 1.196366, acc.: 60.16%] [G loss: 5.274740]\n",
      "epoch:14 step:11471 [D loss: 2.053370, acc.: 50.00%] [G loss: 4.009252]\n",
      "epoch:14 step:11472 [D loss: 0.229428, acc.: 92.19%] [G loss: 0.117657]\n",
      "epoch:14 step:11473 [D loss: 0.247791, acc.: 89.84%] [G loss: 0.576272]\n",
      "epoch:14 step:11474 [D loss: 0.116649, acc.: 95.31%] [G loss: 5.240917]\n",
      "epoch:14 step:11475 [D loss: 0.090645, acc.: 96.88%] [G loss: 0.532784]\n",
      "epoch:14 step:11476 [D loss: 0.071122, acc.: 98.44%] [G loss: 3.165951]\n",
      "epoch:14 step:11477 [D loss: 0.075179, acc.: 99.22%] [G loss: 0.263052]\n",
      "epoch:14 step:11478 [D loss: 0.235464, acc.: 92.97%] [G loss: 3.680700]\n",
      "epoch:14 step:11479 [D loss: 0.068035, acc.: 99.22%] [G loss: 4.141815]\n",
      "epoch:14 step:11480 [D loss: 0.281814, acc.: 90.62%] [G loss: 3.781796]\n",
      "epoch:14 step:11481 [D loss: 0.174244, acc.: 95.31%] [G loss: 0.766335]\n",
      "epoch:14 step:11482 [D loss: 0.098439, acc.: 99.22%] [G loss: 2.701355]\n",
      "epoch:14 step:11483 [D loss: 0.193430, acc.: 91.41%] [G loss: 0.480588]\n",
      "epoch:14 step:11484 [D loss: 0.290703, acc.: 87.50%] [G loss: 0.260392]\n",
      "epoch:14 step:11485 [D loss: 0.116994, acc.: 95.31%] [G loss: 0.064553]\n",
      "epoch:14 step:11486 [D loss: 0.179527, acc.: 92.97%] [G loss: 0.795024]\n",
      "epoch:14 step:11487 [D loss: 0.221552, acc.: 92.19%] [G loss: 1.742509]\n",
      "epoch:14 step:11488 [D loss: 0.254420, acc.: 86.72%] [G loss: 6.626281]\n",
      "epoch:14 step:11489 [D loss: 0.065468, acc.: 96.88%] [G loss: 3.281630]\n",
      "epoch:14 step:11490 [D loss: 0.225007, acc.: 88.28%] [G loss: 0.190018]\n",
      "epoch:14 step:11491 [D loss: 0.232195, acc.: 90.62%] [G loss: 4.241451]\n",
      "epoch:14 step:11492 [D loss: 0.091834, acc.: 97.66%] [G loss: 2.679536]\n",
      "epoch:14 step:11493 [D loss: 0.079481, acc.: 98.44%] [G loss: 0.931763]\n",
      "epoch:14 step:11494 [D loss: 0.160290, acc.: 95.31%] [G loss: 0.635356]\n",
      "epoch:14 step:11495 [D loss: 0.067297, acc.: 99.22%] [G loss: 1.286422]\n",
      "epoch:14 step:11496 [D loss: 0.049933, acc.: 100.00%] [G loss: 0.938955]\n",
      "epoch:14 step:11497 [D loss: 0.178351, acc.: 93.75%] [G loss: 4.659071]\n",
      "epoch:14 step:11498 [D loss: 0.390655, acc.: 82.03%] [G loss: 5.224061]\n",
      "epoch:14 step:11499 [D loss: 0.305272, acc.: 88.28%] [G loss: 3.171358]\n",
      "epoch:14 step:11500 [D loss: 0.144695, acc.: 96.88%] [G loss: 2.745789]\n",
      "epoch:14 step:11501 [D loss: 0.087962, acc.: 97.66%] [G loss: 3.086941]\n",
      "epoch:14 step:11502 [D loss: 0.091233, acc.: 96.09%] [G loss: 5.012237]\n",
      "epoch:14 step:11503 [D loss: 0.070101, acc.: 98.44%] [G loss: 5.096772]\n",
      "epoch:14 step:11504 [D loss: 0.095780, acc.: 95.31%] [G loss: 3.633856]\n",
      "epoch:14 step:11505 [D loss: 0.180111, acc.: 94.53%] [G loss: 3.918295]\n",
      "epoch:14 step:11506 [D loss: 0.079028, acc.: 98.44%] [G loss: 2.924762]\n",
      "epoch:14 step:11507 [D loss: 0.291006, acc.: 88.28%] [G loss: 3.239764]\n",
      "epoch:14 step:11508 [D loss: 0.024323, acc.: 100.00%] [G loss: 0.792468]\n",
      "epoch:14 step:11509 [D loss: 0.187292, acc.: 91.41%] [G loss: 4.992122]\n",
      "epoch:14 step:11510 [D loss: 0.053416, acc.: 98.44%] [G loss: 5.227315]\n",
      "epoch:14 step:11511 [D loss: 0.441265, acc.: 82.03%] [G loss: 0.635300]\n",
      "epoch:14 step:11512 [D loss: 0.082784, acc.: 97.66%] [G loss: 0.953951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11513 [D loss: 0.036716, acc.: 99.22%] [G loss: 2.816673]\n",
      "epoch:14 step:11514 [D loss: 0.059438, acc.: 98.44%] [G loss: 2.220446]\n",
      "epoch:14 step:11515 [D loss: 0.022616, acc.: 100.00%] [G loss: 2.525714]\n",
      "epoch:14 step:11516 [D loss: 0.009139, acc.: 100.00%] [G loss: 1.543902]\n",
      "epoch:14 step:11517 [D loss: 0.088977, acc.: 97.66%] [G loss: 2.668041]\n",
      "epoch:14 step:11518 [D loss: 0.017930, acc.: 100.00%] [G loss: 2.083658]\n",
      "epoch:14 step:11519 [D loss: 0.024703, acc.: 99.22%] [G loss: 2.710210]\n",
      "epoch:14 step:11520 [D loss: 0.053252, acc.: 99.22%] [G loss: 1.229975]\n",
      "epoch:14 step:11521 [D loss: 0.103894, acc.: 97.66%] [G loss: 3.454613]\n",
      "epoch:14 step:11522 [D loss: 0.012507, acc.: 100.00%] [G loss: 4.293174]\n",
      "epoch:14 step:11523 [D loss: 0.080462, acc.: 97.66%] [G loss: 2.095153]\n",
      "epoch:14 step:11524 [D loss: 0.061489, acc.: 98.44%] [G loss: 2.505455]\n",
      "epoch:14 step:11525 [D loss: 0.024706, acc.: 100.00%] [G loss: 2.892407]\n",
      "epoch:14 step:11526 [D loss: 0.051348, acc.: 100.00%] [G loss: 3.376729]\n",
      "epoch:14 step:11527 [D loss: 0.055704, acc.: 98.44%] [G loss: 3.744633]\n",
      "epoch:14 step:11528 [D loss: 0.025446, acc.: 99.22%] [G loss: 3.791719]\n",
      "epoch:14 step:11529 [D loss: 0.012022, acc.: 100.00%] [G loss: 4.502773]\n",
      "epoch:14 step:11530 [D loss: 0.031599, acc.: 99.22%] [G loss: 2.949968]\n",
      "epoch:14 step:11531 [D loss: 0.163417, acc.: 94.53%] [G loss: 5.026474]\n",
      "epoch:14 step:11532 [D loss: 0.031302, acc.: 99.22%] [G loss: 7.021668]\n",
      "epoch:14 step:11533 [D loss: 0.383479, acc.: 78.91%] [G loss: 0.114604]\n",
      "epoch:14 step:11534 [D loss: 0.375584, acc.: 82.03%] [G loss: 7.201884]\n",
      "epoch:14 step:11535 [D loss: 0.012823, acc.: 100.00%] [G loss: 8.988697]\n",
      "epoch:14 step:11536 [D loss: 0.115364, acc.: 95.31%] [G loss: 5.391089]\n",
      "epoch:14 step:11537 [D loss: 0.006390, acc.: 100.00%] [G loss: 7.874969]\n",
      "epoch:14 step:11538 [D loss: 0.052279, acc.: 99.22%] [G loss: 6.838422]\n",
      "epoch:14 step:11539 [D loss: 0.006453, acc.: 100.00%] [G loss: 0.447609]\n",
      "epoch:14 step:11540 [D loss: 0.010217, acc.: 100.00%] [G loss: 7.033993]\n",
      "epoch:14 step:11541 [D loss: 0.010706, acc.: 100.00%] [G loss: 0.547453]\n",
      "epoch:14 step:11542 [D loss: 0.023109, acc.: 100.00%] [G loss: 7.202343]\n",
      "epoch:14 step:11543 [D loss: 0.027061, acc.: 100.00%] [G loss: 4.460042]\n",
      "epoch:14 step:11544 [D loss: 0.016000, acc.: 100.00%] [G loss: 7.258986]\n",
      "epoch:14 step:11545 [D loss: 0.010565, acc.: 100.00%] [G loss: 1.050449]\n",
      "epoch:14 step:11546 [D loss: 0.005962, acc.: 100.00%] [G loss: 6.821471]\n",
      "epoch:14 step:11547 [D loss: 0.075712, acc.: 98.44%] [G loss: 6.246991]\n",
      "epoch:14 step:11548 [D loss: 0.013044, acc.: 100.00%] [G loss: 3.617469]\n",
      "epoch:14 step:11549 [D loss: 0.013734, acc.: 100.00%] [G loss: 0.161617]\n",
      "epoch:14 step:11550 [D loss: 0.080499, acc.: 98.44%] [G loss: 5.029730]\n",
      "epoch:14 step:11551 [D loss: 0.023647, acc.: 100.00%] [G loss: 7.527661]\n",
      "epoch:14 step:11552 [D loss: 0.453167, acc.: 79.69%] [G loss: 3.792420]\n",
      "epoch:14 step:11553 [D loss: 0.194244, acc.: 92.97%] [G loss: 3.029451]\n",
      "epoch:14 step:11554 [D loss: 0.013965, acc.: 100.00%] [G loss: 6.170660]\n",
      "epoch:14 step:11555 [D loss: 0.018379, acc.: 100.00%] [G loss: 5.456721]\n",
      "epoch:14 step:11556 [D loss: 0.008421, acc.: 100.00%] [G loss: 1.143589]\n",
      "epoch:14 step:11557 [D loss: 0.241282, acc.: 92.19%] [G loss: 7.167436]\n",
      "epoch:14 step:11558 [D loss: 0.137281, acc.: 92.97%] [G loss: 4.789658]\n",
      "epoch:14 step:11559 [D loss: 0.034118, acc.: 100.00%] [G loss: 6.623063]\n",
      "epoch:14 step:11560 [D loss: 0.026460, acc.: 99.22%] [G loss: 5.329169]\n",
      "epoch:14 step:11561 [D loss: 0.050414, acc.: 99.22%] [G loss: 1.631241]\n",
      "epoch:14 step:11562 [D loss: 0.124134, acc.: 95.31%] [G loss: 4.786717]\n",
      "epoch:14 step:11563 [D loss: 0.094580, acc.: 96.09%] [G loss: 6.709720]\n",
      "epoch:14 step:11564 [D loss: 0.022676, acc.: 100.00%] [G loss: 6.051542]\n",
      "epoch:14 step:11565 [D loss: 0.015837, acc.: 100.00%] [G loss: 4.932079]\n",
      "epoch:14 step:11566 [D loss: 0.012615, acc.: 100.00%] [G loss: 5.119075]\n",
      "epoch:14 step:11567 [D loss: 0.019791, acc.: 100.00%] [G loss: 4.867493]\n",
      "epoch:14 step:11568 [D loss: 0.010660, acc.: 100.00%] [G loss: 3.627339]\n",
      "epoch:14 step:11569 [D loss: 0.050877, acc.: 99.22%] [G loss: 4.500491]\n",
      "epoch:14 step:11570 [D loss: 0.172999, acc.: 94.53%] [G loss: 3.966503]\n",
      "epoch:14 step:11571 [D loss: 0.128300, acc.: 95.31%] [G loss: 5.146794]\n",
      "epoch:14 step:11572 [D loss: 0.027549, acc.: 100.00%] [G loss: 5.489589]\n",
      "epoch:14 step:11573 [D loss: 0.014776, acc.: 100.00%] [G loss: 2.873757]\n",
      "epoch:14 step:11574 [D loss: 0.016890, acc.: 100.00%] [G loss: 4.969107]\n",
      "epoch:14 step:11575 [D loss: 0.011234, acc.: 100.00%] [G loss: 4.770741]\n",
      "epoch:14 step:11576 [D loss: 0.081152, acc.: 97.66%] [G loss: 2.323107]\n",
      "epoch:14 step:11577 [D loss: 0.009570, acc.: 100.00%] [G loss: 5.142907]\n",
      "epoch:14 step:11578 [D loss: 0.004070, acc.: 100.00%] [G loss: 4.796589]\n",
      "epoch:14 step:11579 [D loss: 0.025055, acc.: 100.00%] [G loss: 4.047523]\n",
      "epoch:14 step:11580 [D loss: 0.188423, acc.: 96.88%] [G loss: 0.721133]\n",
      "epoch:14 step:11581 [D loss: 0.006648, acc.: 100.00%] [G loss: 1.017195]\n",
      "epoch:14 step:11582 [D loss: 0.012523, acc.: 100.00%] [G loss: 5.334249]\n",
      "epoch:14 step:11583 [D loss: 0.060036, acc.: 99.22%] [G loss: 0.191519]\n",
      "epoch:14 step:11584 [D loss: 0.023246, acc.: 99.22%] [G loss: 5.837839]\n",
      "epoch:14 step:11585 [D loss: 0.011061, acc.: 100.00%] [G loss: 1.507661]\n",
      "epoch:14 step:11586 [D loss: 0.013064, acc.: 100.00%] [G loss: 0.875589]\n",
      "epoch:14 step:11587 [D loss: 0.005854, acc.: 100.00%] [G loss: 0.465093]\n",
      "epoch:14 step:11588 [D loss: 0.006349, acc.: 100.00%] [G loss: 6.264797]\n",
      "epoch:14 step:11589 [D loss: 0.009717, acc.: 100.00%] [G loss: 5.196208]\n",
      "epoch:14 step:11590 [D loss: 0.020708, acc.: 100.00%] [G loss: 1.948922]\n",
      "epoch:14 step:11591 [D loss: 0.003999, acc.: 100.00%] [G loss: 4.782741]\n",
      "epoch:14 step:11592 [D loss: 0.022863, acc.: 100.00%] [G loss: 4.352228]\n",
      "epoch:14 step:11593 [D loss: 0.068043, acc.: 97.66%] [G loss: 1.586637]\n",
      "epoch:14 step:11594 [D loss: 0.021638, acc.: 100.00%] [G loss: 1.559942]\n",
      "epoch:14 step:11595 [D loss: 0.088709, acc.: 97.66%] [G loss: 6.429418]\n",
      "epoch:14 step:11596 [D loss: 0.127046, acc.: 96.09%] [G loss: 1.535585]\n",
      "epoch:14 step:11597 [D loss: 0.038068, acc.: 99.22%] [G loss: 0.950804]\n",
      "epoch:14 step:11598 [D loss: 0.002399, acc.: 100.00%] [G loss: 2.044482]\n",
      "epoch:14 step:11599 [D loss: 1.918604, acc.: 42.19%] [G loss: 13.291344]\n",
      "epoch:14 step:11600 [D loss: 4.300201, acc.: 50.00%] [G loss: 7.265796]\n",
      "epoch:14 step:11601 [D loss: 1.963050, acc.: 49.22%] [G loss: 1.221707]\n",
      "epoch:14 step:11602 [D loss: 0.580532, acc.: 78.12%] [G loss: 3.298196]\n",
      "epoch:14 step:11603 [D loss: 0.066360, acc.: 99.22%] [G loss: 4.109018]\n",
      "epoch:14 step:11604 [D loss: 0.355114, acc.: 82.81%] [G loss: 2.244851]\n",
      "epoch:14 step:11605 [D loss: 0.230802, acc.: 90.62%] [G loss: 3.648040]\n",
      "epoch:14 step:11606 [D loss: 0.052921, acc.: 99.22%] [G loss: 3.924394]\n",
      "epoch:14 step:11607 [D loss: 0.135539, acc.: 96.09%] [G loss: 1.419130]\n",
      "epoch:14 step:11608 [D loss: 0.118252, acc.: 99.22%] [G loss: 2.400684]\n",
      "epoch:14 step:11609 [D loss: 0.079562, acc.: 98.44%] [G loss: 2.768883]\n",
      "epoch:14 step:11610 [D loss: 0.071976, acc.: 97.66%] [G loss: 3.106990]\n",
      "epoch:14 step:11611 [D loss: 0.477588, acc.: 75.00%] [G loss: 3.772332]\n",
      "epoch:14 step:11612 [D loss: 0.234074, acc.: 90.62%] [G loss: 2.787826]\n",
      "epoch:14 step:11613 [D loss: 0.126436, acc.: 96.88%] [G loss: 3.110336]\n",
      "epoch:14 step:11614 [D loss: 0.072987, acc.: 99.22%] [G loss: 3.738704]\n",
      "epoch:14 step:11615 [D loss: 0.537221, acc.: 75.78%] [G loss: 4.682137]\n",
      "epoch:14 step:11616 [D loss: 0.180551, acc.: 91.41%] [G loss: 4.028240]\n",
      "epoch:14 step:11617 [D loss: 0.118425, acc.: 98.44%] [G loss: 3.190749]\n",
      "epoch:14 step:11618 [D loss: 0.062310, acc.: 99.22%] [G loss: 3.292790]\n",
      "epoch:14 step:11619 [D loss: 0.515616, acc.: 74.22%] [G loss: 4.882806]\n",
      "epoch:14 step:11620 [D loss: 0.189116, acc.: 92.19%] [G loss: 4.001621]\n",
      "epoch:14 step:11621 [D loss: 0.156907, acc.: 94.53%] [G loss: 3.232160]\n",
      "epoch:14 step:11622 [D loss: 0.067133, acc.: 99.22%] [G loss: 3.260871]\n",
      "epoch:14 step:11623 [D loss: 0.038110, acc.: 100.00%] [G loss: 2.849803]\n",
      "epoch:14 step:11624 [D loss: 0.139004, acc.: 96.09%] [G loss: 3.497821]\n",
      "epoch:14 step:11625 [D loss: 0.173205, acc.: 95.31%] [G loss: 3.666807]\n",
      "epoch:14 step:11626 [D loss: 0.140834, acc.: 96.88%] [G loss: 1.761388]\n",
      "epoch:14 step:11627 [D loss: 0.089141, acc.: 98.44%] [G loss: 4.573667]\n",
      "epoch:14 step:11628 [D loss: 0.010987, acc.: 100.00%] [G loss: 5.275637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14 step:11629 [D loss: 0.060416, acc.: 98.44%] [G loss: 3.530266]\n",
      "epoch:14 step:11630 [D loss: 0.057437, acc.: 97.66%] [G loss: 2.287726]\n",
      "epoch:14 step:11631 [D loss: 0.025335, acc.: 100.00%] [G loss: 3.023875]\n",
      "epoch:14 step:11632 [D loss: 0.027840, acc.: 100.00%] [G loss: 2.499246]\n",
      "epoch:14 step:11633 [D loss: 0.079576, acc.: 98.44%] [G loss: 3.661218]\n",
      "epoch:14 step:11634 [D loss: 0.086199, acc.: 97.66%] [G loss: 2.489062]\n",
      "epoch:14 step:11635 [D loss: 0.023488, acc.: 99.22%] [G loss: 2.442040]\n",
      "epoch:14 step:11636 [D loss: 0.024233, acc.: 100.00%] [G loss: 2.767387]\n",
      "epoch:14 step:11637 [D loss: 0.226249, acc.: 92.19%] [G loss: 2.750679]\n",
      "epoch:14 step:11638 [D loss: 0.591950, acc.: 73.44%] [G loss: 0.131871]\n",
      "epoch:14 step:11639 [D loss: 0.326793, acc.: 84.38%] [G loss: 4.618431]\n",
      "epoch:14 step:11640 [D loss: 0.100424, acc.: 96.88%] [G loss: 3.252376]\n",
      "epoch:14 step:11641 [D loss: 0.913000, acc.: 55.47%] [G loss: 0.637049]\n",
      "epoch:14 step:11642 [D loss: 0.247367, acc.: 88.28%] [G loss: 0.943274]\n",
      "epoch:14 step:11643 [D loss: 0.025800, acc.: 100.00%] [G loss: 4.306544]\n",
      "epoch:14 step:11644 [D loss: 0.114364, acc.: 96.88%] [G loss: 2.919583]\n",
      "epoch:14 step:11645 [D loss: 0.031073, acc.: 100.00%] [G loss: 2.708312]\n",
      "epoch:14 step:11646 [D loss: 0.029392, acc.: 100.00%] [G loss: 2.437166]\n",
      "epoch:14 step:11647 [D loss: 0.079289, acc.: 98.44%] [G loss: 0.817269]\n",
      "epoch:14 step:11648 [D loss: 0.019650, acc.: 100.00%] [G loss: 1.231010]\n",
      "epoch:14 step:11649 [D loss: 0.064542, acc.: 99.22%] [G loss: 1.367263]\n",
      "epoch:14 step:11650 [D loss: 0.018976, acc.: 100.00%] [G loss: 0.922229]\n",
      "epoch:14 step:11651 [D loss: 0.236733, acc.: 92.97%] [G loss: 2.539641]\n",
      "epoch:14 step:11652 [D loss: 0.068418, acc.: 98.44%] [G loss: 3.261641]\n",
      "epoch:14 step:11653 [D loss: 0.299044, acc.: 84.38%] [G loss: 0.450004]\n",
      "epoch:14 step:11654 [D loss: 0.108558, acc.: 96.88%] [G loss: 1.644814]\n",
      "epoch:14 step:11655 [D loss: 0.035425, acc.: 100.00%] [G loss: 2.334825]\n",
      "epoch:14 step:11656 [D loss: 0.029996, acc.: 100.00%] [G loss: 2.008561]\n",
      "epoch:14 step:11657 [D loss: 0.025060, acc.: 100.00%] [G loss: 2.369063]\n",
      "epoch:14 step:11658 [D loss: 0.039339, acc.: 100.00%] [G loss: 1.902731]\n",
      "epoch:14 step:11659 [D loss: 0.234800, acc.: 89.84%] [G loss: 1.953421]\n",
      "epoch:14 step:11660 [D loss: 0.068511, acc.: 98.44%] [G loss: 2.467035]\n",
      "epoch:14 step:11661 [D loss: 0.024204, acc.: 100.00%] [G loss: 3.248472]\n",
      "epoch:14 step:11662 [D loss: 0.115711, acc.: 95.31%] [G loss: 0.310618]\n",
      "epoch:14 step:11663 [D loss: 0.051088, acc.: 98.44%] [G loss: 2.823905]\n",
      "epoch:14 step:11664 [D loss: 0.051551, acc.: 99.22%] [G loss: 2.753655]\n",
      "epoch:14 step:11665 [D loss: 0.067770, acc.: 99.22%] [G loss: 4.822749]\n",
      "epoch:14 step:11666 [D loss: 0.125824, acc.: 96.88%] [G loss: 4.277988]\n",
      "epoch:14 step:11667 [D loss: 0.106077, acc.: 96.88%] [G loss: 3.054033]\n",
      "epoch:14 step:11668 [D loss: 0.041526, acc.: 99.22%] [G loss: 3.129205]\n",
      "epoch:14 step:11669 [D loss: 0.448677, acc.: 78.91%] [G loss: 5.483253]\n",
      "epoch:14 step:11670 [D loss: 0.070591, acc.: 97.66%] [G loss: 6.179340]\n",
      "epoch:14 step:11671 [D loss: 0.573034, acc.: 75.00%] [G loss: 2.365946]\n",
      "epoch:14 step:11672 [D loss: 0.478190, acc.: 78.91%] [G loss: 2.993354]\n",
      "epoch:14 step:11673 [D loss: 0.017659, acc.: 100.00%] [G loss: 7.405665]\n",
      "epoch:14 step:11674 [D loss: 0.195206, acc.: 91.41%] [G loss: 2.284897]\n",
      "epoch:14 step:11675 [D loss: 0.032951, acc.: 100.00%] [G loss: 4.998236]\n",
      "epoch:14 step:11676 [D loss: 0.004631, acc.: 100.00%] [G loss: 4.481715]\n",
      "epoch:14 step:11677 [D loss: 0.009534, acc.: 100.00%] [G loss: 4.938493]\n",
      "epoch:14 step:11678 [D loss: 0.022029, acc.: 100.00%] [G loss: 0.974515]\n",
      "epoch:14 step:11679 [D loss: 0.006955, acc.: 100.00%] [G loss: 1.232621]\n",
      "epoch:14 step:11680 [D loss: 0.013155, acc.: 100.00%] [G loss: 3.268017]\n",
      "epoch:14 step:11681 [D loss: 0.046269, acc.: 99.22%] [G loss: 0.895153]\n",
      "epoch:14 step:11682 [D loss: 0.027889, acc.: 100.00%] [G loss: 1.813420]\n",
      "epoch:14 step:11683 [D loss: 0.023918, acc.: 100.00%] [G loss: 4.488343]\n",
      "epoch:14 step:11684 [D loss: 0.031145, acc.: 99.22%] [G loss: 4.039965]\n",
      "epoch:14 step:11685 [D loss: 0.055060, acc.: 100.00%] [G loss: 4.297038]\n",
      "epoch:14 step:11686 [D loss: 0.092793, acc.: 97.66%] [G loss: 2.715942]\n",
      "epoch:14 step:11687 [D loss: 0.113775, acc.: 96.88%] [G loss: 4.053625]\n",
      "epoch:14 step:11688 [D loss: 0.041646, acc.: 99.22%] [G loss: 4.798765]\n",
      "epoch:14 step:11689 [D loss: 0.022765, acc.: 100.00%] [G loss: 2.742410]\n",
      "epoch:14 step:11690 [D loss: 0.016924, acc.: 100.00%] [G loss: 3.657159]\n",
      "epoch:14 step:11691 [D loss: 0.038575, acc.: 100.00%] [G loss: 3.434406]\n",
      "epoch:14 step:11692 [D loss: 0.014412, acc.: 100.00%] [G loss: 0.914730]\n",
      "epoch:14 step:11693 [D loss: 0.063984, acc.: 100.00%] [G loss: 2.837782]\n",
      "epoch:14 step:11694 [D loss: 0.129266, acc.: 94.53%] [G loss: 4.206647]\n",
      "epoch:14 step:11695 [D loss: 0.016614, acc.: 99.22%] [G loss: 5.877259]\n",
      "epoch:14 step:11696 [D loss: 1.212999, acc.: 43.75%] [G loss: 2.404510]\n",
      "epoch:14 step:11697 [D loss: 0.005954, acc.: 100.00%] [G loss: 0.799148]\n",
      "epoch:14 step:11698 [D loss: 0.161084, acc.: 93.75%] [G loss: 0.455756]\n",
      "epoch:14 step:11699 [D loss: 0.002366, acc.: 100.00%] [G loss: 0.040263]\n",
      "epoch:14 step:11700 [D loss: 0.002379, acc.: 100.00%] [G loss: 0.009288]\n",
      "epoch:14 step:11701 [D loss: 0.001725, acc.: 100.00%] [G loss: 0.027104]\n",
      "epoch:14 step:11702 [D loss: 0.005159, acc.: 100.00%] [G loss: 3.072878]\n",
      "epoch:14 step:11703 [D loss: 0.009953, acc.: 100.00%] [G loss: 3.562412]\n",
      "epoch:14 step:11704 [D loss: 0.101790, acc.: 96.09%] [G loss: 0.055812]\n",
      "epoch:14 step:11705 [D loss: 0.015990, acc.: 100.00%] [G loss: 0.065750]\n",
      "epoch:14 step:11706 [D loss: 0.004458, acc.: 100.00%] [G loss: 0.279225]\n",
      "epoch:14 step:11707 [D loss: 0.001657, acc.: 100.00%] [G loss: 5.380965]\n",
      "epoch:14 step:11708 [D loss: 0.066480, acc.: 98.44%] [G loss: 5.378043]\n",
      "epoch:14 step:11709 [D loss: 0.012789, acc.: 100.00%] [G loss: 0.748492]\n",
      "epoch:14 step:11710 [D loss: 0.080173, acc.: 97.66%] [G loss: 0.114461]\n",
      "epoch:14 step:11711 [D loss: 0.003885, acc.: 100.00%] [G loss: 0.058692]\n",
      "epoch:14 step:11712 [D loss: 0.007881, acc.: 100.00%] [G loss: 0.018855]\n",
      "epoch:14 step:11713 [D loss: 0.014291, acc.: 99.22%] [G loss: 0.112021]\n",
      "epoch:14 step:11714 [D loss: 0.075140, acc.: 99.22%] [G loss: 6.210539]\n",
      "epoch:14 step:11715 [D loss: 0.006105, acc.: 100.00%] [G loss: 3.015259]\n",
      "epoch:15 step:11716 [D loss: 0.093657, acc.: 96.88%] [G loss: 0.014093]\n",
      "epoch:15 step:11717 [D loss: 0.011609, acc.: 100.00%] [G loss: 0.092437]\n",
      "epoch:15 step:11718 [D loss: 0.014777, acc.: 100.00%] [G loss: 0.057600]\n",
      "epoch:15 step:11719 [D loss: 0.006246, acc.: 100.00%] [G loss: 0.143606]\n",
      "epoch:15 step:11720 [D loss: 0.013061, acc.: 100.00%] [G loss: 4.732300]\n",
      "epoch:15 step:11721 [D loss: 0.009476, acc.: 100.00%] [G loss: 3.802069]\n",
      "epoch:15 step:11722 [D loss: 0.013231, acc.: 100.00%] [G loss: 3.881807]\n",
      "epoch:15 step:11723 [D loss: 0.006148, acc.: 100.00%] [G loss: 0.045620]\n",
      "epoch:15 step:11724 [D loss: 0.009754, acc.: 100.00%] [G loss: 2.700524]\n",
      "epoch:15 step:11725 [D loss: 0.026940, acc.: 99.22%] [G loss: 0.077505]\n",
      "epoch:15 step:11726 [D loss: 0.002102, acc.: 100.00%] [G loss: 2.060699]\n",
      "epoch:15 step:11727 [D loss: 0.037754, acc.: 99.22%] [G loss: 5.254895]\n",
      "epoch:15 step:11728 [D loss: 0.010495, acc.: 100.00%] [G loss: 0.414713]\n",
      "epoch:15 step:11729 [D loss: 0.782598, acc.: 63.28%] [G loss: 6.056137]\n",
      "epoch:15 step:11730 [D loss: 3.218415, acc.: 50.00%] [G loss: 3.577858]\n",
      "epoch:15 step:11731 [D loss: 1.292604, acc.: 57.81%] [G loss: 0.088294]\n",
      "epoch:15 step:11732 [D loss: 0.228067, acc.: 91.41%] [G loss: 2.501909]\n",
      "epoch:15 step:11733 [D loss: 0.042059, acc.: 99.22%] [G loss: 3.485713]\n",
      "epoch:15 step:11734 [D loss: 0.048567, acc.: 98.44%] [G loss: 0.688645]\n",
      "epoch:15 step:11735 [D loss: 0.029593, acc.: 100.00%] [G loss: 3.540826]\n",
      "epoch:15 step:11736 [D loss: 0.035808, acc.: 100.00%] [G loss: 1.046899]\n",
      "epoch:15 step:11737 [D loss: 0.038786, acc.: 99.22%] [G loss: 2.801793]\n",
      "epoch:15 step:11738 [D loss: 0.048561, acc.: 100.00%] [G loss: 0.936614]\n",
      "epoch:15 step:11739 [D loss: 0.104580, acc.: 100.00%] [G loss: 0.716696]\n",
      "epoch:15 step:11740 [D loss: 0.023246, acc.: 100.00%] [G loss: 0.744347]\n",
      "epoch:15 step:11741 [D loss: 0.043450, acc.: 100.00%] [G loss: 1.137968]\n",
      "epoch:15 step:11742 [D loss: 0.019109, acc.: 100.00%] [G loss: 1.407835]\n",
      "epoch:15 step:11743 [D loss: 0.044843, acc.: 100.00%] [G loss: 1.888500]\n",
      "epoch:15 step:11744 [D loss: 0.032307, acc.: 100.00%] [G loss: 1.999330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11745 [D loss: 0.071334, acc.: 98.44%] [G loss: 2.669963]\n",
      "epoch:15 step:11746 [D loss: 0.058655, acc.: 100.00%] [G loss: 2.325511]\n",
      "epoch:15 step:11747 [D loss: 0.032825, acc.: 100.00%] [G loss: 2.883722]\n",
      "epoch:15 step:11748 [D loss: 0.032369, acc.: 100.00%] [G loss: 4.583253]\n",
      "epoch:15 step:11749 [D loss: 0.016118, acc.: 100.00%] [G loss: 4.698744]\n",
      "epoch:15 step:11750 [D loss: 0.013166, acc.: 100.00%] [G loss: 2.916640]\n",
      "epoch:15 step:11751 [D loss: 0.090455, acc.: 99.22%] [G loss: 2.040128]\n",
      "epoch:15 step:11752 [D loss: 0.042804, acc.: 99.22%] [G loss: 4.037633]\n",
      "epoch:15 step:11753 [D loss: 0.051879, acc.: 99.22%] [G loss: 2.645947]\n",
      "epoch:15 step:11754 [D loss: 0.064900, acc.: 99.22%] [G loss: 2.914217]\n",
      "epoch:15 step:11755 [D loss: 0.052815, acc.: 98.44%] [G loss: 3.231339]\n",
      "epoch:15 step:11756 [D loss: 0.038566, acc.: 100.00%] [G loss: 3.072732]\n",
      "epoch:15 step:11757 [D loss: 0.024883, acc.: 100.00%] [G loss: 4.214811]\n",
      "epoch:15 step:11758 [D loss: 0.022654, acc.: 99.22%] [G loss: 4.543186]\n",
      "epoch:15 step:11759 [D loss: 1.143862, acc.: 46.88%] [G loss: 6.804401]\n",
      "epoch:15 step:11760 [D loss: 0.848155, acc.: 60.16%] [G loss: 4.980310]\n",
      "epoch:15 step:11761 [D loss: 0.054522, acc.: 99.22%] [G loss: 2.642547]\n",
      "epoch:15 step:11762 [D loss: 0.008958, acc.: 100.00%] [G loss: 4.157491]\n",
      "epoch:15 step:11763 [D loss: 0.038678, acc.: 98.44%] [G loss: 2.033561]\n",
      "epoch:15 step:11764 [D loss: 0.012400, acc.: 100.00%] [G loss: 1.908471]\n",
      "epoch:15 step:11765 [D loss: 0.082010, acc.: 98.44%] [G loss: 1.740507]\n",
      "epoch:15 step:11766 [D loss: 0.012139, acc.: 100.00%] [G loss: 4.756590]\n",
      "epoch:15 step:11767 [D loss: 0.046830, acc.: 99.22%] [G loss: 3.415128]\n",
      "epoch:15 step:11768 [D loss: 0.095716, acc.: 98.44%] [G loss: 4.645136]\n",
      "epoch:15 step:11769 [D loss: 0.040385, acc.: 100.00%] [G loss: 1.962114]\n",
      "epoch:15 step:11770 [D loss: 0.027461, acc.: 100.00%] [G loss: 4.258228]\n",
      "epoch:15 step:11771 [D loss: 0.110955, acc.: 97.66%] [G loss: 0.687079]\n",
      "epoch:15 step:11772 [D loss: 0.064024, acc.: 99.22%] [G loss: 3.852280]\n",
      "epoch:15 step:11773 [D loss: 0.010947, acc.: 100.00%] [G loss: 0.860885]\n",
      "epoch:15 step:11774 [D loss: 0.023375, acc.: 100.00%] [G loss: 0.831879]\n",
      "epoch:15 step:11775 [D loss: 0.016706, acc.: 100.00%] [G loss: 0.969060]\n",
      "epoch:15 step:11776 [D loss: 0.125485, acc.: 96.09%] [G loss: 0.192614]\n",
      "epoch:15 step:11777 [D loss: 0.038395, acc.: 100.00%] [G loss: 0.616833]\n",
      "epoch:15 step:11778 [D loss: 0.070700, acc.: 99.22%] [G loss: 3.855353]\n",
      "epoch:15 step:11779 [D loss: 0.020645, acc.: 100.00%] [G loss: 3.065091]\n",
      "epoch:15 step:11780 [D loss: 0.039793, acc.: 99.22%] [G loss: 0.020204]\n",
      "epoch:15 step:11781 [D loss: 0.072456, acc.: 97.66%] [G loss: 3.213943]\n",
      "epoch:15 step:11782 [D loss: 0.003471, acc.: 100.00%] [G loss: 4.100421]\n",
      "epoch:15 step:11783 [D loss: 0.067953, acc.: 97.66%] [G loss: 0.390366]\n",
      "epoch:15 step:11784 [D loss: 0.101763, acc.: 96.88%] [G loss: 1.339529]\n",
      "epoch:15 step:11785 [D loss: 0.004603, acc.: 100.00%] [G loss: 5.245914]\n",
      "epoch:15 step:11786 [D loss: 0.601156, acc.: 74.22%] [G loss: 1.560447]\n",
      "epoch:15 step:11787 [D loss: 0.012335, acc.: 100.00%] [G loss: 2.022007]\n",
      "epoch:15 step:11788 [D loss: 0.298127, acc.: 83.59%] [G loss: 0.014915]\n",
      "epoch:15 step:11789 [D loss: 0.671654, acc.: 73.44%] [G loss: 6.068065]\n",
      "epoch:15 step:11790 [D loss: 0.006410, acc.: 100.00%] [G loss: 6.945844]\n",
      "epoch:15 step:11791 [D loss: 1.805315, acc.: 50.00%] [G loss: 0.716108]\n",
      "epoch:15 step:11792 [D loss: 1.367285, acc.: 57.81%] [G loss: 5.422648]\n",
      "epoch:15 step:11793 [D loss: 0.659548, acc.: 67.19%] [G loss: 4.053123]\n",
      "epoch:15 step:11794 [D loss: 0.243222, acc.: 86.72%] [G loss: 3.443947]\n",
      "epoch:15 step:11795 [D loss: 0.033132, acc.: 100.00%] [G loss: 1.088900]\n",
      "epoch:15 step:11796 [D loss: 0.042624, acc.: 99.22%] [G loss: 2.242499]\n",
      "epoch:15 step:11797 [D loss: 0.041293, acc.: 100.00%] [G loss: 2.425791]\n",
      "epoch:15 step:11798 [D loss: 0.024095, acc.: 100.00%] [G loss: 3.492070]\n",
      "epoch:15 step:11799 [D loss: 0.016093, acc.: 100.00%] [G loss: 2.306061]\n",
      "epoch:15 step:11800 [D loss: 0.043152, acc.: 100.00%] [G loss: 0.900162]\n",
      "epoch:15 step:11801 [D loss: 0.086351, acc.: 98.44%] [G loss: 1.412998]\n",
      "epoch:15 step:11802 [D loss: 0.036643, acc.: 100.00%] [G loss: 2.448647]\n",
      "epoch:15 step:11803 [D loss: 0.032454, acc.: 100.00%] [G loss: 2.343679]\n",
      "epoch:15 step:11804 [D loss: 0.038671, acc.: 100.00%] [G loss: 2.363509]\n",
      "epoch:15 step:11805 [D loss: 0.297230, acc.: 85.94%] [G loss: 1.011939]\n",
      "epoch:15 step:11806 [D loss: 0.027613, acc.: 100.00%] [G loss: 0.795856]\n",
      "epoch:15 step:11807 [D loss: 0.032958, acc.: 100.00%] [G loss: 2.312615]\n",
      "epoch:15 step:11808 [D loss: 0.054528, acc.: 100.00%] [G loss: 2.613352]\n",
      "epoch:15 step:11809 [D loss: 0.009072, acc.: 100.00%] [G loss: 3.524456]\n",
      "epoch:15 step:11810 [D loss: 0.021386, acc.: 100.00%] [G loss: 2.710975]\n",
      "epoch:15 step:11811 [D loss: 0.021985, acc.: 100.00%] [G loss: 2.128262]\n",
      "epoch:15 step:11812 [D loss: 0.257384, acc.: 89.84%] [G loss: 3.398585]\n",
      "epoch:15 step:11813 [D loss: 0.050327, acc.: 100.00%] [G loss: 4.715214]\n",
      "epoch:15 step:11814 [D loss: 0.488876, acc.: 73.44%] [G loss: 3.944448]\n",
      "epoch:15 step:11815 [D loss: 1.175949, acc.: 55.47%] [G loss: 6.399991]\n",
      "epoch:15 step:11816 [D loss: 0.341481, acc.: 82.03%] [G loss: 6.957831]\n",
      "epoch:15 step:11817 [D loss: 0.392029, acc.: 73.44%] [G loss: 4.788650]\n",
      "epoch:15 step:11818 [D loss: 0.017566, acc.: 100.00%] [G loss: 5.047240]\n",
      "epoch:15 step:11819 [D loss: 0.022101, acc.: 100.00%] [G loss: 2.435910]\n",
      "epoch:15 step:11820 [D loss: 0.071033, acc.: 99.22%] [G loss: 4.861300]\n",
      "epoch:15 step:11821 [D loss: 0.011919, acc.: 100.00%] [G loss: 4.563842]\n",
      "epoch:15 step:11822 [D loss: 0.075964, acc.: 97.66%] [G loss: 4.625568]\n",
      "epoch:15 step:11823 [D loss: 0.016865, acc.: 100.00%] [G loss: 4.880057]\n",
      "epoch:15 step:11824 [D loss: 0.011905, acc.: 100.00%] [G loss: 4.389817]\n",
      "epoch:15 step:11825 [D loss: 0.014791, acc.: 100.00%] [G loss: 4.350810]\n",
      "epoch:15 step:11826 [D loss: 0.073226, acc.: 99.22%] [G loss: 4.176108]\n",
      "epoch:15 step:11827 [D loss: 0.015963, acc.: 100.00%] [G loss: 3.995532]\n",
      "epoch:15 step:11828 [D loss: 0.021478, acc.: 100.00%] [G loss: 2.489217]\n",
      "epoch:15 step:11829 [D loss: 0.045268, acc.: 100.00%] [G loss: 4.276853]\n",
      "epoch:15 step:11830 [D loss: 0.012402, acc.: 100.00%] [G loss: 2.925677]\n",
      "epoch:15 step:11831 [D loss: 0.144580, acc.: 94.53%] [G loss: 3.748036]\n",
      "epoch:15 step:11832 [D loss: 0.032640, acc.: 100.00%] [G loss: 3.338828]\n",
      "epoch:15 step:11833 [D loss: 0.017950, acc.: 100.00%] [G loss: 4.876040]\n",
      "epoch:15 step:11834 [D loss: 0.016418, acc.: 100.00%] [G loss: 4.360504]\n",
      "epoch:15 step:11835 [D loss: 0.044001, acc.: 100.00%] [G loss: 3.499120]\n",
      "epoch:15 step:11836 [D loss: 0.024152, acc.: 100.00%] [G loss: 5.058228]\n",
      "epoch:15 step:11837 [D loss: 0.023115, acc.: 100.00%] [G loss: 1.151390]\n",
      "epoch:15 step:11838 [D loss: 0.156334, acc.: 95.31%] [G loss: 5.920599]\n",
      "epoch:15 step:11839 [D loss: 0.079642, acc.: 96.88%] [G loss: 5.876070]\n",
      "epoch:15 step:11840 [D loss: 0.062156, acc.: 99.22%] [G loss: 5.448665]\n",
      "epoch:15 step:11841 [D loss: 0.040334, acc.: 99.22%] [G loss: 4.720733]\n",
      "epoch:15 step:11842 [D loss: 0.017663, acc.: 100.00%] [G loss: 4.967617]\n",
      "epoch:15 step:11843 [D loss: 0.031842, acc.: 100.00%] [G loss: 3.986356]\n",
      "epoch:15 step:11844 [D loss: 0.013627, acc.: 100.00%] [G loss: 4.503786]\n",
      "epoch:15 step:11845 [D loss: 0.036279, acc.: 99.22%] [G loss: 4.055701]\n",
      "epoch:15 step:11846 [D loss: 0.023612, acc.: 100.00%] [G loss: 4.688886]\n",
      "epoch:15 step:11847 [D loss: 0.029452, acc.: 100.00%] [G loss: 4.579412]\n",
      "epoch:15 step:11848 [D loss: 0.006207, acc.: 100.00%] [G loss: 2.384553]\n",
      "epoch:15 step:11849 [D loss: 0.032862, acc.: 100.00%] [G loss: 3.860973]\n",
      "epoch:15 step:11850 [D loss: 0.012846, acc.: 100.00%] [G loss: 0.722025]\n",
      "epoch:15 step:11851 [D loss: 0.177359, acc.: 96.09%] [G loss: 4.572044]\n",
      "epoch:15 step:11852 [D loss: 0.023206, acc.: 100.00%] [G loss: 0.652726]\n",
      "epoch:15 step:11853 [D loss: 0.178621, acc.: 94.53%] [G loss: 3.541254]\n",
      "epoch:15 step:11854 [D loss: 1.089699, acc.: 44.53%] [G loss: 6.045391]\n",
      "epoch:15 step:11855 [D loss: 0.169087, acc.: 92.97%] [G loss: 4.150867]\n",
      "epoch:15 step:11856 [D loss: 0.017044, acc.: 100.00%] [G loss: 5.649847]\n",
      "epoch:15 step:11857 [D loss: 0.007842, acc.: 100.00%] [G loss: 5.296619]\n",
      "epoch:15 step:11858 [D loss: 0.018746, acc.: 100.00%] [G loss: 5.082065]\n",
      "epoch:15 step:11859 [D loss: 0.021588, acc.: 100.00%] [G loss: 4.607168]\n",
      "epoch:15 step:11860 [D loss: 0.038200, acc.: 100.00%] [G loss: 3.962774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11861 [D loss: 0.034683, acc.: 100.00%] [G loss: 2.986368]\n",
      "epoch:15 step:11862 [D loss: 0.024077, acc.: 100.00%] [G loss: 1.991777]\n",
      "epoch:15 step:11863 [D loss: 0.037985, acc.: 100.00%] [G loss: 4.765939]\n",
      "epoch:15 step:11864 [D loss: 0.016396, acc.: 100.00%] [G loss: 3.581334]\n",
      "epoch:15 step:11865 [D loss: 0.026807, acc.: 100.00%] [G loss: 3.303113]\n",
      "epoch:15 step:11866 [D loss: 0.026213, acc.: 100.00%] [G loss: 4.128983]\n",
      "epoch:15 step:11867 [D loss: 0.040518, acc.: 100.00%] [G loss: 3.742337]\n",
      "epoch:15 step:11868 [D loss: 0.162103, acc.: 96.09%] [G loss: 1.655032]\n",
      "epoch:15 step:11869 [D loss: 0.026403, acc.: 99.22%] [G loss: 3.136099]\n",
      "epoch:15 step:11870 [D loss: 0.008532, acc.: 100.00%] [G loss: 3.488747]\n",
      "epoch:15 step:11871 [D loss: 0.037926, acc.: 100.00%] [G loss: 2.698686]\n",
      "epoch:15 step:11872 [D loss: 0.038171, acc.: 99.22%] [G loss: 3.854151]\n",
      "epoch:15 step:11873 [D loss: 0.413204, acc.: 82.81%] [G loss: 4.454228]\n",
      "epoch:15 step:11874 [D loss: 0.012921, acc.: 100.00%] [G loss: 4.914579]\n",
      "epoch:15 step:11875 [D loss: 1.490569, acc.: 31.25%] [G loss: 4.948951]\n",
      "epoch:15 step:11876 [D loss: 0.060395, acc.: 98.44%] [G loss: 5.305412]\n",
      "epoch:15 step:11877 [D loss: 0.221879, acc.: 89.06%] [G loss: 3.068247]\n",
      "epoch:15 step:11878 [D loss: 0.014108, acc.: 100.00%] [G loss: 3.784027]\n",
      "epoch:15 step:11879 [D loss: 0.091970, acc.: 96.88%] [G loss: 4.505071]\n",
      "epoch:15 step:11880 [D loss: 0.005011, acc.: 100.00%] [G loss: 5.112672]\n",
      "epoch:15 step:11881 [D loss: 0.004964, acc.: 100.00%] [G loss: 4.017472]\n",
      "epoch:15 step:11882 [D loss: 0.025806, acc.: 100.00%] [G loss: 4.047158]\n",
      "epoch:15 step:11883 [D loss: 0.012920, acc.: 100.00%] [G loss: 2.913609]\n",
      "epoch:15 step:11884 [D loss: 0.056234, acc.: 99.22%] [G loss: 2.821849]\n",
      "epoch:15 step:11885 [D loss: 0.055059, acc.: 98.44%] [G loss: 2.438418]\n",
      "epoch:15 step:11886 [D loss: 0.035073, acc.: 99.22%] [G loss: 3.394862]\n",
      "epoch:15 step:11887 [D loss: 0.059698, acc.: 99.22%] [G loss: 3.602542]\n",
      "epoch:15 step:11888 [D loss: 0.032363, acc.: 100.00%] [G loss: 4.383488]\n",
      "epoch:15 step:11889 [D loss: 0.057938, acc.: 98.44%] [G loss: 1.538059]\n",
      "epoch:15 step:11890 [D loss: 0.050190, acc.: 100.00%] [G loss: 3.304291]\n",
      "epoch:15 step:11891 [D loss: 0.482930, acc.: 76.56%] [G loss: 4.700265]\n",
      "epoch:15 step:11892 [D loss: 0.027709, acc.: 100.00%] [G loss: 2.093774]\n",
      "epoch:15 step:11893 [D loss: 0.898649, acc.: 56.25%] [G loss: 0.373377]\n",
      "epoch:15 step:11894 [D loss: 0.025138, acc.: 100.00%] [G loss: 2.453673]\n",
      "epoch:15 step:11895 [D loss: 0.275437, acc.: 88.28%] [G loss: 5.095745]\n",
      "epoch:15 step:11896 [D loss: 0.019762, acc.: 100.00%] [G loss: 5.482959]\n",
      "epoch:15 step:11897 [D loss: 0.284266, acc.: 89.06%] [G loss: 4.363965]\n",
      "epoch:15 step:11898 [D loss: 0.041161, acc.: 99.22%] [G loss: 3.222450]\n",
      "epoch:15 step:11899 [D loss: 0.014447, acc.: 100.00%] [G loss: 3.431330]\n",
      "epoch:15 step:11900 [D loss: 0.035534, acc.: 99.22%] [G loss: 1.077659]\n",
      "epoch:15 step:11901 [D loss: 0.014903, acc.: 100.00%] [G loss: 4.486735]\n",
      "epoch:15 step:11902 [D loss: 0.099708, acc.: 97.66%] [G loss: 2.901076]\n",
      "epoch:15 step:11903 [D loss: 0.054702, acc.: 99.22%] [G loss: 0.787249]\n",
      "epoch:15 step:11904 [D loss: 0.031450, acc.: 100.00%] [G loss: 4.103889]\n",
      "epoch:15 step:11905 [D loss: 0.144941, acc.: 95.31%] [G loss: 3.323870]\n",
      "epoch:15 step:11906 [D loss: 0.023342, acc.: 100.00%] [G loss: 0.205881]\n",
      "epoch:15 step:11907 [D loss: 0.019789, acc.: 100.00%] [G loss: 0.217393]\n",
      "epoch:15 step:11908 [D loss: 0.026387, acc.: 99.22%] [G loss: 0.551133]\n",
      "epoch:15 step:11909 [D loss: 0.008008, acc.: 100.00%] [G loss: 3.559836]\n",
      "epoch:15 step:11910 [D loss: 0.015903, acc.: 100.00%] [G loss: 3.711220]\n",
      "epoch:15 step:11911 [D loss: 0.018837, acc.: 100.00%] [G loss: 0.862179]\n",
      "epoch:15 step:11912 [D loss: 0.025404, acc.: 100.00%] [G loss: 3.405086]\n",
      "epoch:15 step:11913 [D loss: 0.007735, acc.: 100.00%] [G loss: 4.062570]\n",
      "epoch:15 step:11914 [D loss: 0.022012, acc.: 100.00%] [G loss: 1.011708]\n",
      "epoch:15 step:11915 [D loss: 0.052573, acc.: 100.00%] [G loss: 3.668870]\n",
      "epoch:15 step:11916 [D loss: 0.009963, acc.: 100.00%] [G loss: 1.211732]\n",
      "epoch:15 step:11917 [D loss: 0.048387, acc.: 99.22%] [G loss: 4.211724]\n",
      "epoch:15 step:11918 [D loss: 0.019682, acc.: 100.00%] [G loss: 4.168806]\n",
      "epoch:15 step:11919 [D loss: 0.048255, acc.: 99.22%] [G loss: 2.734689]\n",
      "epoch:15 step:11920 [D loss: 0.034118, acc.: 100.00%] [G loss: 0.669072]\n",
      "epoch:15 step:11921 [D loss: 0.050464, acc.: 100.00%] [G loss: 3.701283]\n",
      "epoch:15 step:11922 [D loss: 0.018720, acc.: 100.00%] [G loss: 0.990955]\n",
      "epoch:15 step:11923 [D loss: 0.006732, acc.: 100.00%] [G loss: 3.841569]\n",
      "epoch:15 step:11924 [D loss: 0.126161, acc.: 96.09%] [G loss: 2.982494]\n",
      "epoch:15 step:11925 [D loss: 0.022069, acc.: 100.00%] [G loss: 0.648430]\n",
      "epoch:15 step:11926 [D loss: 0.002511, acc.: 100.00%] [G loss: 4.470674]\n",
      "epoch:15 step:11927 [D loss: 0.006994, acc.: 100.00%] [G loss: 0.442725]\n",
      "epoch:15 step:11928 [D loss: 0.035692, acc.: 98.44%] [G loss: 3.174435]\n",
      "epoch:15 step:11929 [D loss: 0.230401, acc.: 89.84%] [G loss: 2.983479]\n",
      "epoch:15 step:11930 [D loss: 0.028565, acc.: 100.00%] [G loss: 0.272900]\n",
      "epoch:15 step:11931 [D loss: 0.001571, acc.: 100.00%] [G loss: 4.186231]\n",
      "epoch:15 step:11932 [D loss: 0.009940, acc.: 100.00%] [G loss: 0.068986]\n",
      "epoch:15 step:11933 [D loss: 0.147681, acc.: 95.31%] [G loss: 0.151474]\n",
      "epoch:15 step:11934 [D loss: 0.009126, acc.: 100.00%] [G loss: 0.007042]\n",
      "epoch:15 step:11935 [D loss: 0.006960, acc.: 100.00%] [G loss: 0.022959]\n",
      "epoch:15 step:11936 [D loss: 0.001529, acc.: 100.00%] [G loss: 0.006058]\n",
      "epoch:15 step:11937 [D loss: 0.056662, acc.: 99.22%] [G loss: 6.028960]\n",
      "epoch:15 step:11938 [D loss: 0.005744, acc.: 100.00%] [G loss: 6.597535]\n",
      "epoch:15 step:11939 [D loss: 0.030438, acc.: 99.22%] [G loss: 0.920854]\n",
      "epoch:15 step:11940 [D loss: 0.001808, acc.: 100.00%] [G loss: 0.082655]\n",
      "epoch:15 step:11941 [D loss: 0.005964, acc.: 100.00%] [G loss: 0.035542]\n",
      "epoch:15 step:11942 [D loss: 0.031109, acc.: 99.22%] [G loss: 5.854428]\n",
      "epoch:15 step:11943 [D loss: 0.000922, acc.: 100.00%] [G loss: 1.064406]\n",
      "epoch:15 step:11944 [D loss: 0.139650, acc.: 94.53%] [G loss: 0.001183]\n",
      "epoch:15 step:11945 [D loss: 0.032852, acc.: 100.00%] [G loss: 0.006022]\n",
      "epoch:15 step:11946 [D loss: 0.005593, acc.: 100.00%] [G loss: 2.563150]\n",
      "epoch:15 step:11947 [D loss: 0.003664, acc.: 100.00%] [G loss: 0.031644]\n",
      "epoch:15 step:11948 [D loss: 0.004000, acc.: 100.00%] [G loss: 2.049097]\n",
      "epoch:15 step:11949 [D loss: 0.013481, acc.: 100.00%] [G loss: 0.251998]\n",
      "epoch:15 step:11950 [D loss: 0.007793, acc.: 100.00%] [G loss: 0.063468]\n",
      "epoch:15 step:11951 [D loss: 0.143158, acc.: 95.31%] [G loss: 6.121780]\n",
      "epoch:15 step:11952 [D loss: 0.378997, acc.: 82.03%] [G loss: 0.748919]\n",
      "epoch:15 step:11953 [D loss: 0.042607, acc.: 99.22%] [G loss: 1.169634]\n",
      "epoch:15 step:11954 [D loss: 0.010185, acc.: 100.00%] [G loss: 2.005737]\n",
      "epoch:15 step:11955 [D loss: 0.046403, acc.: 99.22%] [G loss: 0.408658]\n",
      "epoch:15 step:11956 [D loss: 0.020329, acc.: 100.00%] [G loss: 0.344053]\n",
      "epoch:15 step:11957 [D loss: 0.004673, acc.: 100.00%] [G loss: 1.633668]\n",
      "epoch:15 step:11958 [D loss: 0.008673, acc.: 100.00%] [G loss: 1.019377]\n",
      "epoch:15 step:11959 [D loss: 0.014367, acc.: 100.00%] [G loss: 0.820486]\n",
      "epoch:15 step:11960 [D loss: 0.002902, acc.: 100.00%] [G loss: 0.967252]\n",
      "epoch:15 step:11961 [D loss: 0.052172, acc.: 98.44%] [G loss: 0.905267]\n",
      "epoch:15 step:11962 [D loss: 0.068817, acc.: 97.66%] [G loss: 1.821501]\n",
      "epoch:15 step:11963 [D loss: 0.079566, acc.: 97.66%] [G loss: 1.812627]\n",
      "epoch:15 step:11964 [D loss: 0.044189, acc.: 98.44%] [G loss: 1.570935]\n",
      "epoch:15 step:11965 [D loss: 0.070396, acc.: 97.66%] [G loss: 0.998632]\n",
      "epoch:15 step:11966 [D loss: 0.014775, acc.: 100.00%] [G loss: 0.959149]\n",
      "epoch:15 step:11967 [D loss: 0.019349, acc.: 100.00%] [G loss: 3.028898]\n",
      "epoch:15 step:11968 [D loss: 0.012843, acc.: 100.00%] [G loss: 4.857871]\n",
      "epoch:15 step:11969 [D loss: 0.067568, acc.: 98.44%] [G loss: 4.286500]\n",
      "epoch:15 step:11970 [D loss: 0.112082, acc.: 96.09%] [G loss: 4.885829]\n",
      "epoch:15 step:11971 [D loss: 0.032141, acc.: 100.00%] [G loss: 5.140791]\n",
      "epoch:15 step:11972 [D loss: 0.007660, acc.: 100.00%] [G loss: 1.626227]\n",
      "epoch:15 step:11973 [D loss: 0.157313, acc.: 90.62%] [G loss: 7.814089]\n",
      "epoch:15 step:11974 [D loss: 1.347882, acc.: 38.28%] [G loss: 7.220952]\n",
      "epoch:15 step:11975 [D loss: 0.618876, acc.: 69.53%] [G loss: 5.640172]\n",
      "epoch:15 step:11976 [D loss: 0.315912, acc.: 85.16%] [G loss: 6.229484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:11977 [D loss: 0.000727, acc.: 100.00%] [G loss: 8.629869]\n",
      "epoch:15 step:11978 [D loss: 0.034411, acc.: 99.22%] [G loss: 7.166935]\n",
      "epoch:15 step:11979 [D loss: 0.432017, acc.: 82.03%] [G loss: 1.794218]\n",
      "epoch:15 step:11980 [D loss: 0.376406, acc.: 82.03%] [G loss: 5.549359]\n",
      "epoch:15 step:11981 [D loss: 0.024298, acc.: 99.22%] [G loss: 5.815678]\n",
      "epoch:15 step:11982 [D loss: 0.235343, acc.: 89.06%] [G loss: 4.837430]\n",
      "epoch:15 step:11983 [D loss: 0.054852, acc.: 98.44%] [G loss: 4.427247]\n",
      "epoch:15 step:11984 [D loss: 0.074955, acc.: 96.88%] [G loss: 4.251812]\n",
      "epoch:15 step:11985 [D loss: 0.013711, acc.: 100.00%] [G loss: 5.489798]\n",
      "epoch:15 step:11986 [D loss: 0.033468, acc.: 100.00%] [G loss: 3.474248]\n",
      "epoch:15 step:11987 [D loss: 0.018994, acc.: 99.22%] [G loss: 4.838911]\n",
      "epoch:15 step:11988 [D loss: 0.028824, acc.: 99.22%] [G loss: 5.387869]\n",
      "epoch:15 step:11989 [D loss: 0.017355, acc.: 100.00%] [G loss: 0.505307]\n",
      "epoch:15 step:11990 [D loss: 0.020868, acc.: 100.00%] [G loss: 5.372105]\n",
      "epoch:15 step:11991 [D loss: 0.047224, acc.: 99.22%] [G loss: 4.587529]\n",
      "epoch:15 step:11992 [D loss: 0.025991, acc.: 100.00%] [G loss: 5.065939]\n",
      "epoch:15 step:11993 [D loss: 0.125927, acc.: 95.31%] [G loss: 7.133004]\n",
      "epoch:15 step:11994 [D loss: 0.059469, acc.: 99.22%] [G loss: 6.400514]\n",
      "epoch:15 step:11995 [D loss: 0.043148, acc.: 100.00%] [G loss: 0.973513]\n",
      "epoch:15 step:11996 [D loss: 0.005889, acc.: 100.00%] [G loss: 5.971581]\n",
      "epoch:15 step:11997 [D loss: 0.037659, acc.: 98.44%] [G loss: 4.064044]\n",
      "epoch:15 step:11998 [D loss: 0.013163, acc.: 100.00%] [G loss: 5.561059]\n",
      "epoch:15 step:11999 [D loss: 0.019762, acc.: 99.22%] [G loss: 3.382629]\n",
      "epoch:15 step:12000 [D loss: 0.020289, acc.: 100.00%] [G loss: 2.311598]\n",
      "epoch:15 step:12001 [D loss: 1.584305, acc.: 35.94%] [G loss: 6.195295]\n",
      "epoch:15 step:12002 [D loss: 0.564052, acc.: 78.91%] [G loss: 4.952452]\n",
      "epoch:15 step:12003 [D loss: 0.012394, acc.: 100.00%] [G loss: 3.206457]\n",
      "epoch:15 step:12004 [D loss: 0.012605, acc.: 100.00%] [G loss: 5.134417]\n",
      "epoch:15 step:12005 [D loss: 0.012657, acc.: 100.00%] [G loss: 4.461250]\n",
      "epoch:15 step:12006 [D loss: 0.059034, acc.: 97.66%] [G loss: 2.934680]\n",
      "epoch:15 step:12007 [D loss: 0.021332, acc.: 100.00%] [G loss: 3.399130]\n",
      "epoch:15 step:12008 [D loss: 0.044521, acc.: 99.22%] [G loss: 3.232957]\n",
      "epoch:15 step:12009 [D loss: 0.051104, acc.: 98.44%] [G loss: 3.986680]\n",
      "epoch:15 step:12010 [D loss: 0.288409, acc.: 92.19%] [G loss: 2.857204]\n",
      "epoch:15 step:12011 [D loss: 0.004500, acc.: 100.00%] [G loss: 4.476430]\n",
      "epoch:15 step:12012 [D loss: 0.047170, acc.: 98.44%] [G loss: 5.422030]\n",
      "epoch:15 step:12013 [D loss: 0.013364, acc.: 100.00%] [G loss: 4.553667]\n",
      "epoch:15 step:12014 [D loss: 0.142355, acc.: 94.53%] [G loss: 2.645792]\n",
      "epoch:15 step:12015 [D loss: 0.136340, acc.: 95.31%] [G loss: 5.446179]\n",
      "epoch:15 step:12016 [D loss: 0.012366, acc.: 100.00%] [G loss: 4.927665]\n",
      "epoch:15 step:12017 [D loss: 0.085577, acc.: 97.66%] [G loss: 0.976199]\n",
      "epoch:15 step:12018 [D loss: 0.056035, acc.: 99.22%] [G loss: 6.613727]\n",
      "epoch:15 step:12019 [D loss: 0.008583, acc.: 100.00%] [G loss: 1.367770]\n",
      "epoch:15 step:12020 [D loss: 0.093204, acc.: 97.66%] [G loss: 5.247337]\n",
      "epoch:15 step:12021 [D loss: 0.028386, acc.: 99.22%] [G loss: 5.463616]\n",
      "epoch:15 step:12022 [D loss: 0.047706, acc.: 99.22%] [G loss: 0.832389]\n",
      "epoch:15 step:12023 [D loss: 0.104653, acc.: 95.31%] [G loss: 4.635785]\n",
      "epoch:15 step:12024 [D loss: 0.188441, acc.: 90.62%] [G loss: 1.935092]\n",
      "epoch:15 step:12025 [D loss: 0.060565, acc.: 100.00%] [G loss: 2.545799]\n",
      "epoch:15 step:12026 [D loss: 0.005441, acc.: 100.00%] [G loss: 2.084816]\n",
      "epoch:15 step:12027 [D loss: 0.038512, acc.: 99.22%] [G loss: 1.727009]\n",
      "epoch:15 step:12028 [D loss: 0.008371, acc.: 100.00%] [G loss: 1.714861]\n",
      "epoch:15 step:12029 [D loss: 0.005929, acc.: 100.00%] [G loss: 1.866336]\n",
      "epoch:15 step:12030 [D loss: 0.080563, acc.: 97.66%] [G loss: 4.752829]\n",
      "epoch:15 step:12031 [D loss: 0.019075, acc.: 100.00%] [G loss: 5.915765]\n",
      "epoch:15 step:12032 [D loss: 0.003085, acc.: 100.00%] [G loss: 4.802509]\n",
      "epoch:15 step:12033 [D loss: 0.002958, acc.: 100.00%] [G loss: 1.642807]\n",
      "epoch:15 step:12034 [D loss: 0.018243, acc.: 100.00%] [G loss: 5.118964]\n",
      "epoch:15 step:12035 [D loss: 0.124687, acc.: 96.09%] [G loss: 0.128525]\n",
      "epoch:15 step:12036 [D loss: 0.367107, acc.: 80.47%] [G loss: 3.297251]\n",
      "epoch:15 step:12037 [D loss: 0.939546, acc.: 64.06%] [G loss: 0.400867]\n",
      "epoch:15 step:12038 [D loss: 0.213215, acc.: 89.06%] [G loss: 4.015141]\n",
      "epoch:15 step:12039 [D loss: 0.016395, acc.: 99.22%] [G loss: 5.505225]\n",
      "epoch:15 step:12040 [D loss: 0.056741, acc.: 98.44%] [G loss: 5.242481]\n",
      "epoch:15 step:12041 [D loss: 0.008581, acc.: 100.00%] [G loss: 4.077110]\n",
      "epoch:15 step:12042 [D loss: 0.009605, acc.: 100.00%] [G loss: 3.598751]\n",
      "epoch:15 step:12043 [D loss: 0.004306, acc.: 100.00%] [G loss: 3.295763]\n",
      "epoch:15 step:12044 [D loss: 0.008181, acc.: 100.00%] [G loss: 2.457172]\n",
      "epoch:15 step:12045 [D loss: 0.020753, acc.: 99.22%] [G loss: 4.341403]\n",
      "epoch:15 step:12046 [D loss: 0.006075, acc.: 100.00%] [G loss: 2.460051]\n",
      "epoch:15 step:12047 [D loss: 0.007807, acc.: 100.00%] [G loss: 4.411801]\n",
      "epoch:15 step:12048 [D loss: 0.009948, acc.: 100.00%] [G loss: 2.850881]\n",
      "epoch:15 step:12049 [D loss: 0.023128, acc.: 100.00%] [G loss: 1.594676]\n",
      "epoch:15 step:12050 [D loss: 0.006361, acc.: 100.00%] [G loss: 3.567406]\n",
      "epoch:15 step:12051 [D loss: 0.267734, acc.: 90.62%] [G loss: 1.455519]\n",
      "epoch:15 step:12052 [D loss: 0.026506, acc.: 100.00%] [G loss: 5.444794]\n",
      "epoch:15 step:12053 [D loss: 0.122172, acc.: 96.88%] [G loss: 1.527505]\n",
      "epoch:15 step:12054 [D loss: 0.077796, acc.: 98.44%] [G loss: 3.655811]\n",
      "epoch:15 step:12055 [D loss: 0.010899, acc.: 100.00%] [G loss: 3.971533]\n",
      "epoch:15 step:12056 [D loss: 0.002809, acc.: 100.00%] [G loss: 4.554106]\n",
      "epoch:15 step:12057 [D loss: 0.008137, acc.: 100.00%] [G loss: 4.085551]\n",
      "epoch:15 step:12058 [D loss: 0.029984, acc.: 100.00%] [G loss: 2.777974]\n",
      "epoch:15 step:12059 [D loss: 0.050086, acc.: 99.22%] [G loss: 4.269958]\n",
      "epoch:15 step:12060 [D loss: 0.006293, acc.: 100.00%] [G loss: 4.848303]\n",
      "epoch:15 step:12061 [D loss: 0.125020, acc.: 96.09%] [G loss: 0.061273]\n",
      "epoch:15 step:12062 [D loss: 0.093783, acc.: 95.31%] [G loss: 4.769316]\n",
      "epoch:15 step:12063 [D loss: 0.001187, acc.: 100.00%] [G loss: 5.952836]\n",
      "epoch:15 step:12064 [D loss: 0.001456, acc.: 100.00%] [G loss: 5.966866]\n",
      "epoch:15 step:12065 [D loss: 0.022013, acc.: 100.00%] [G loss: 5.310709]\n",
      "epoch:15 step:12066 [D loss: 0.003042, acc.: 100.00%] [G loss: 4.896814]\n",
      "epoch:15 step:12067 [D loss: 0.004305, acc.: 100.00%] [G loss: 3.882607]\n",
      "epoch:15 step:12068 [D loss: 0.009938, acc.: 100.00%] [G loss: 0.810011]\n",
      "epoch:15 step:12069 [D loss: 0.009000, acc.: 100.00%] [G loss: 6.075176]\n",
      "epoch:15 step:12070 [D loss: 0.009543, acc.: 100.00%] [G loss: 4.966146]\n",
      "epoch:15 step:12071 [D loss: 0.009235, acc.: 100.00%] [G loss: 2.985343]\n",
      "epoch:15 step:12072 [D loss: 0.014122, acc.: 100.00%] [G loss: 4.364253]\n",
      "epoch:15 step:12073 [D loss: 0.028584, acc.: 99.22%] [G loss: 5.308715]\n",
      "epoch:15 step:12074 [D loss: 0.014769, acc.: 100.00%] [G loss: 2.236057]\n",
      "epoch:15 step:12075 [D loss: 0.005167, acc.: 100.00%] [G loss: 1.540579]\n",
      "epoch:15 step:12076 [D loss: 0.026548, acc.: 100.00%] [G loss: 5.659303]\n",
      "epoch:15 step:12077 [D loss: 0.009418, acc.: 100.00%] [G loss: 5.516335]\n",
      "epoch:15 step:12078 [D loss: 0.013469, acc.: 100.00%] [G loss: 5.112301]\n",
      "epoch:15 step:12079 [D loss: 0.134482, acc.: 93.75%] [G loss: 0.061943]\n",
      "epoch:15 step:12080 [D loss: 0.914445, acc.: 55.47%] [G loss: 9.320002]\n",
      "epoch:15 step:12081 [D loss: 3.607347, acc.: 50.00%] [G loss: 7.222244]\n",
      "epoch:15 step:12082 [D loss: 1.685857, acc.: 50.78%] [G loss: 0.328271]\n",
      "epoch:15 step:12083 [D loss: 0.152185, acc.: 96.88%] [G loss: 1.201717]\n",
      "epoch:15 step:12084 [D loss: 0.173069, acc.: 94.53%] [G loss: 3.130988]\n",
      "epoch:15 step:12085 [D loss: 0.082062, acc.: 97.66%] [G loss: 1.836291]\n",
      "epoch:15 step:12086 [D loss: 0.110703, acc.: 97.66%] [G loss: 1.403930]\n",
      "epoch:15 step:12087 [D loss: 0.069363, acc.: 98.44%] [G loss: 1.678244]\n",
      "epoch:15 step:12088 [D loss: 0.063921, acc.: 100.00%] [G loss: 3.289133]\n",
      "epoch:15 step:12089 [D loss: 0.184704, acc.: 93.75%] [G loss: 1.816669]\n",
      "epoch:15 step:12090 [D loss: 0.126517, acc.: 96.09%] [G loss: 1.088423]\n",
      "epoch:15 step:12091 [D loss: 0.255078, acc.: 91.41%] [G loss: 2.712054]\n",
      "epoch:15 step:12092 [D loss: 0.109064, acc.: 96.09%] [G loss: 2.972157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12093 [D loss: 0.105905, acc.: 97.66%] [G loss: 1.964120]\n",
      "epoch:15 step:12094 [D loss: 0.088657, acc.: 96.88%] [G loss: 2.240404]\n",
      "epoch:15 step:12095 [D loss: 0.150198, acc.: 93.75%] [G loss: 3.203448]\n",
      "epoch:15 step:12096 [D loss: 0.016562, acc.: 100.00%] [G loss: 2.471462]\n",
      "epoch:15 step:12097 [D loss: 0.143281, acc.: 96.09%] [G loss: 3.326062]\n",
      "epoch:15 step:12098 [D loss: 0.028675, acc.: 100.00%] [G loss: 0.570093]\n",
      "epoch:15 step:12099 [D loss: 0.173974, acc.: 96.88%] [G loss: 2.064185]\n",
      "epoch:15 step:12100 [D loss: 0.299329, acc.: 86.72%] [G loss: 0.457302]\n",
      "epoch:15 step:12101 [D loss: 0.037185, acc.: 100.00%] [G loss: 0.660656]\n",
      "epoch:15 step:12102 [D loss: 0.126932, acc.: 98.44%] [G loss: 1.494672]\n",
      "epoch:15 step:12103 [D loss: 0.138593, acc.: 96.88%] [G loss: 1.464506]\n",
      "epoch:15 step:12104 [D loss: 0.068766, acc.: 98.44%] [G loss: 1.931242]\n",
      "epoch:15 step:12105 [D loss: 0.021683, acc.: 100.00%] [G loss: 1.770663]\n",
      "epoch:15 step:12106 [D loss: 0.087125, acc.: 98.44%] [G loss: 1.492803]\n",
      "epoch:15 step:12107 [D loss: 0.069163, acc.: 98.44%] [G loss: 0.702597]\n",
      "epoch:15 step:12108 [D loss: 0.137294, acc.: 96.09%] [G loss: 4.058327]\n",
      "epoch:15 step:12109 [D loss: 0.235672, acc.: 89.06%] [G loss: 0.310645]\n",
      "epoch:15 step:12110 [D loss: 0.224847, acc.: 91.41%] [G loss: 2.337573]\n",
      "epoch:15 step:12111 [D loss: 0.115557, acc.: 96.88%] [G loss: 1.196553]\n",
      "epoch:15 step:12112 [D loss: 0.095463, acc.: 95.31%] [G loss: 0.377974]\n",
      "epoch:15 step:12113 [D loss: 0.140544, acc.: 94.53%] [G loss: 0.682903]\n",
      "epoch:15 step:12114 [D loss: 0.005924, acc.: 100.00%] [G loss: 4.270462]\n",
      "epoch:15 step:12115 [D loss: 0.021900, acc.: 100.00%] [G loss: 1.597799]\n",
      "epoch:15 step:12116 [D loss: 0.067767, acc.: 96.88%] [G loss: 0.121350]\n",
      "epoch:15 step:12117 [D loss: 0.074926, acc.: 99.22%] [G loss: 0.041763]\n",
      "epoch:15 step:12118 [D loss: 0.008922, acc.: 100.00%] [G loss: 0.577049]\n",
      "epoch:15 step:12119 [D loss: 0.031361, acc.: 99.22%] [G loss: 0.189909]\n",
      "epoch:15 step:12120 [D loss: 0.040776, acc.: 99.22%] [G loss: 0.170566]\n",
      "epoch:15 step:12121 [D loss: 0.004574, acc.: 100.00%] [G loss: 0.052635]\n",
      "epoch:15 step:12122 [D loss: 0.031487, acc.: 99.22%] [G loss: 0.124410]\n",
      "epoch:15 step:12123 [D loss: 0.002850, acc.: 100.00%] [G loss: 2.623896]\n",
      "epoch:15 step:12124 [D loss: 0.012685, acc.: 100.00%] [G loss: 0.034948]\n",
      "epoch:15 step:12125 [D loss: 0.009344, acc.: 100.00%] [G loss: 1.036083]\n",
      "epoch:15 step:12126 [D loss: 0.330040, acc.: 85.16%] [G loss: 0.010191]\n",
      "epoch:15 step:12127 [D loss: 0.063726, acc.: 98.44%] [G loss: 2.378361]\n",
      "epoch:15 step:12128 [D loss: 0.054504, acc.: 99.22%] [G loss: 2.950629]\n",
      "epoch:15 step:12129 [D loss: 0.152491, acc.: 93.75%] [G loss: 0.358792]\n",
      "epoch:15 step:12130 [D loss: 0.025051, acc.: 99.22%] [G loss: 0.580259]\n",
      "epoch:15 step:12131 [D loss: 0.025881, acc.: 100.00%] [G loss: 0.208443]\n",
      "epoch:15 step:12132 [D loss: 0.048855, acc.: 98.44%] [G loss: 1.943615]\n",
      "epoch:15 step:12133 [D loss: 0.040063, acc.: 100.00%] [G loss: 0.086637]\n",
      "epoch:15 step:12134 [D loss: 0.016127, acc.: 100.00%] [G loss: 0.054077]\n",
      "epoch:15 step:12135 [D loss: 0.344920, acc.: 87.50%] [G loss: 5.397809]\n",
      "epoch:15 step:12136 [D loss: 0.108263, acc.: 96.09%] [G loss: 3.376995]\n",
      "epoch:15 step:12137 [D loss: 0.070068, acc.: 97.66%] [G loss: 2.423031]\n",
      "epoch:15 step:12138 [D loss: 0.079071, acc.: 97.66%] [G loss: 3.192085]\n",
      "epoch:15 step:12139 [D loss: 0.084877, acc.: 96.09%] [G loss: 2.065536]\n",
      "epoch:15 step:12140 [D loss: 0.064249, acc.: 99.22%] [G loss: 2.077756]\n",
      "epoch:15 step:12141 [D loss: 0.028509, acc.: 100.00%] [G loss: 2.462154]\n",
      "epoch:15 step:12142 [D loss: 0.091224, acc.: 99.22%] [G loss: 1.362274]\n",
      "epoch:15 step:12143 [D loss: 0.260497, acc.: 89.06%] [G loss: 1.787690]\n",
      "epoch:15 step:12144 [D loss: 0.371808, acc.: 78.91%] [G loss: 7.173370]\n",
      "epoch:15 step:12145 [D loss: 0.553157, acc.: 69.53%] [G loss: 3.889232]\n",
      "epoch:15 step:12146 [D loss: 0.043496, acc.: 100.00%] [G loss: 2.800498]\n",
      "epoch:15 step:12147 [D loss: 0.018024, acc.: 100.00%] [G loss: 0.712540]\n",
      "epoch:15 step:12148 [D loss: 0.044568, acc.: 99.22%] [G loss: 0.593930]\n",
      "epoch:15 step:12149 [D loss: 0.065252, acc.: 99.22%] [G loss: 5.432116]\n",
      "epoch:15 step:12150 [D loss: 0.012633, acc.: 100.00%] [G loss: 5.460766]\n",
      "epoch:15 step:12151 [D loss: 0.145432, acc.: 94.53%] [G loss: 0.951551]\n",
      "epoch:15 step:12152 [D loss: 2.143114, acc.: 50.78%] [G loss: 6.867230]\n",
      "epoch:15 step:12153 [D loss: 3.110584, acc.: 50.00%] [G loss: 5.140774]\n",
      "epoch:15 step:12154 [D loss: 2.210325, acc.: 50.00%] [G loss: 1.440623]\n",
      "epoch:15 step:12155 [D loss: 1.069125, acc.: 50.78%] [G loss: 0.099483]\n",
      "epoch:15 step:12156 [D loss: 0.235269, acc.: 93.75%] [G loss: 0.096597]\n",
      "epoch:15 step:12157 [D loss: 0.115595, acc.: 100.00%] [G loss: 0.131623]\n",
      "epoch:15 step:12158 [D loss: 0.112779, acc.: 99.22%] [G loss: 0.129375]\n",
      "epoch:15 step:12159 [D loss: 0.221526, acc.: 92.19%] [G loss: 0.217033]\n",
      "epoch:15 step:12160 [D loss: 0.188191, acc.: 90.62%] [G loss: 3.942890]\n",
      "epoch:15 step:12161 [D loss: 0.133867, acc.: 97.66%] [G loss: 3.295302]\n",
      "epoch:15 step:12162 [D loss: 0.099264, acc.: 98.44%] [G loss: 0.032456]\n",
      "epoch:15 step:12163 [D loss: 0.058265, acc.: 100.00%] [G loss: 1.993312]\n",
      "epoch:15 step:12164 [D loss: 0.086043, acc.: 99.22%] [G loss: 2.122399]\n",
      "epoch:15 step:12165 [D loss: 0.331205, acc.: 88.28%] [G loss: 0.769813]\n",
      "epoch:15 step:12166 [D loss: 0.369820, acc.: 81.25%] [G loss: 0.555263]\n",
      "epoch:15 step:12167 [D loss: 1.083008, acc.: 39.84%] [G loss: 4.285092]\n",
      "epoch:15 step:12168 [D loss: 0.272364, acc.: 88.28%] [G loss: 1.758870]\n",
      "epoch:15 step:12169 [D loss: 0.387278, acc.: 81.25%] [G loss: 3.499254]\n",
      "epoch:15 step:12170 [D loss: 0.089530, acc.: 99.22%] [G loss: 0.408702]\n",
      "epoch:15 step:12171 [D loss: 0.133779, acc.: 96.09%] [G loss: 0.641269]\n",
      "epoch:15 step:12172 [D loss: 0.097887, acc.: 99.22%] [G loss: 0.445007]\n",
      "epoch:15 step:12173 [D loss: 0.285153, acc.: 92.97%] [G loss: 1.201770]\n",
      "epoch:15 step:12174 [D loss: 0.137192, acc.: 95.31%] [G loss: 4.144166]\n",
      "epoch:15 step:12175 [D loss: 0.134275, acc.: 97.66%] [G loss: 0.388565]\n",
      "epoch:15 step:12176 [D loss: 0.108304, acc.: 97.66%] [G loss: 0.289694]\n",
      "epoch:15 step:12177 [D loss: 0.077981, acc.: 100.00%] [G loss: 0.414373]\n",
      "epoch:15 step:12178 [D loss: 0.080568, acc.: 97.66%] [G loss: 1.898606]\n",
      "epoch:15 step:12179 [D loss: 0.099281, acc.: 99.22%] [G loss: 0.929170]\n",
      "epoch:15 step:12180 [D loss: 0.137954, acc.: 97.66%] [G loss: 1.074913]\n",
      "epoch:15 step:12181 [D loss: 0.189919, acc.: 95.31%] [G loss: 2.407261]\n",
      "epoch:15 step:12182 [D loss: 0.366675, acc.: 80.47%] [G loss: 2.570802]\n",
      "epoch:15 step:12183 [D loss: 0.381279, acc.: 83.59%] [G loss: 1.915952]\n",
      "epoch:15 step:12184 [D loss: 0.133685, acc.: 95.31%] [G loss: 3.197377]\n",
      "epoch:15 step:12185 [D loss: 0.100006, acc.: 98.44%] [G loss: 3.185662]\n",
      "epoch:15 step:12186 [D loss: 0.084155, acc.: 98.44%] [G loss: 4.343112]\n",
      "epoch:15 step:12187 [D loss: 0.039529, acc.: 99.22%] [G loss: 3.954050]\n",
      "epoch:15 step:12188 [D loss: 0.077035, acc.: 98.44%] [G loss: 4.597045]\n",
      "epoch:15 step:12189 [D loss: 0.309498, acc.: 87.50%] [G loss: 1.645869]\n",
      "epoch:15 step:12190 [D loss: 0.348285, acc.: 82.81%] [G loss: 5.341940]\n",
      "epoch:15 step:12191 [D loss: 0.137488, acc.: 92.97%] [G loss: 2.799131]\n",
      "epoch:15 step:12192 [D loss: 0.133875, acc.: 94.53%] [G loss: 1.081978]\n",
      "epoch:15 step:12193 [D loss: 0.158118, acc.: 94.53%] [G loss: 4.144687]\n",
      "epoch:15 step:12194 [D loss: 0.018843, acc.: 99.22%] [G loss: 5.179545]\n",
      "epoch:15 step:12195 [D loss: 0.050071, acc.: 99.22%] [G loss: 4.390624]\n",
      "epoch:15 step:12196 [D loss: 0.077195, acc.: 98.44%] [G loss: 2.582868]\n",
      "epoch:15 step:12197 [D loss: 0.427868, acc.: 76.56%] [G loss: 5.367574]\n",
      "epoch:15 step:12198 [D loss: 0.553686, acc.: 75.78%] [G loss: 4.402324]\n",
      "epoch:15 step:12199 [D loss: 0.036059, acc.: 99.22%] [G loss: 3.080029]\n",
      "epoch:15 step:12200 [D loss: 0.060034, acc.: 99.22%] [G loss: 3.246277]\n",
      "epoch:15 step:12201 [D loss: 0.084369, acc.: 97.66%] [G loss: 4.090672]\n",
      "epoch:15 step:12202 [D loss: 0.053248, acc.: 99.22%] [G loss: 3.823728]\n",
      "epoch:15 step:12203 [D loss: 0.153623, acc.: 94.53%] [G loss: 3.024338]\n",
      "epoch:15 step:12204 [D loss: 0.064465, acc.: 98.44%] [G loss: 2.003408]\n",
      "epoch:15 step:12205 [D loss: 0.028079, acc.: 100.00%] [G loss: 4.001676]\n",
      "epoch:15 step:12206 [D loss: 0.117784, acc.: 96.09%] [G loss: 2.707278]\n",
      "epoch:15 step:12207 [D loss: 0.131498, acc.: 96.09%] [G loss: 3.432105]\n",
      "epoch:15 step:12208 [D loss: 0.050607, acc.: 100.00%] [G loss: 3.236648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12209 [D loss: 0.080523, acc.: 96.88%] [G loss: 0.842983]\n",
      "epoch:15 step:12210 [D loss: 0.032719, acc.: 100.00%] [G loss: 0.851327]\n",
      "epoch:15 step:12211 [D loss: 0.095204, acc.: 98.44%] [G loss: 0.943062]\n",
      "epoch:15 step:12212 [D loss: 0.088591, acc.: 100.00%] [G loss: 0.747920]\n",
      "epoch:15 step:12213 [D loss: 1.113710, acc.: 53.12%] [G loss: 6.229700]\n",
      "epoch:15 step:12214 [D loss: 1.132489, acc.: 60.16%] [G loss: 4.325845]\n",
      "epoch:15 step:12215 [D loss: 0.128564, acc.: 94.53%] [G loss: 4.156042]\n",
      "epoch:15 step:12216 [D loss: 0.227924, acc.: 89.06%] [G loss: 2.303343]\n",
      "epoch:15 step:12217 [D loss: 0.086376, acc.: 98.44%] [G loss: 0.928520]\n",
      "epoch:15 step:12218 [D loss: 0.018692, acc.: 99.22%] [G loss: 2.697719]\n",
      "epoch:15 step:12219 [D loss: 0.027564, acc.: 100.00%] [G loss: 1.389014]\n",
      "epoch:15 step:12220 [D loss: 0.038578, acc.: 100.00%] [G loss: 1.058425]\n",
      "epoch:15 step:12221 [D loss: 0.023057, acc.: 100.00%] [G loss: 4.166041]\n",
      "epoch:15 step:12222 [D loss: 0.034770, acc.: 100.00%] [G loss: 3.755592]\n",
      "epoch:15 step:12223 [D loss: 0.059393, acc.: 98.44%] [G loss: 0.522187]\n",
      "epoch:15 step:12224 [D loss: 0.071713, acc.: 99.22%] [G loss: 1.234405]\n",
      "epoch:15 step:12225 [D loss: 0.098722, acc.: 96.88%] [G loss: 3.411417]\n",
      "epoch:15 step:12226 [D loss: 0.069889, acc.: 97.66%] [G loss: 3.993495]\n",
      "epoch:15 step:12227 [D loss: 0.201371, acc.: 91.41%] [G loss: 3.187009]\n",
      "epoch:15 step:12228 [D loss: 0.190482, acc.: 92.97%] [G loss: 1.580169]\n",
      "epoch:15 step:12229 [D loss: 0.041309, acc.: 98.44%] [G loss: 3.665540]\n",
      "epoch:15 step:12230 [D loss: 0.277490, acc.: 89.84%] [G loss: 2.640873]\n",
      "epoch:15 step:12231 [D loss: 0.085419, acc.: 99.22%] [G loss: 3.132286]\n",
      "epoch:15 step:12232 [D loss: 0.026546, acc.: 100.00%] [G loss: 4.780115]\n",
      "epoch:15 step:12233 [D loss: 0.056717, acc.: 100.00%] [G loss: 2.964888]\n",
      "epoch:15 step:12234 [D loss: 0.030398, acc.: 100.00%] [G loss: 2.968981]\n",
      "epoch:15 step:12235 [D loss: 0.024195, acc.: 100.00%] [G loss: 2.950455]\n",
      "epoch:15 step:12236 [D loss: 0.077207, acc.: 99.22%] [G loss: 2.284388]\n",
      "epoch:15 step:12237 [D loss: 0.037732, acc.: 100.00%] [G loss: 2.861381]\n",
      "epoch:15 step:12238 [D loss: 0.046296, acc.: 100.00%] [G loss: 2.201160]\n",
      "epoch:15 step:12239 [D loss: 0.088812, acc.: 98.44%] [G loss: 2.864884]\n",
      "epoch:15 step:12240 [D loss: 0.017745, acc.: 100.00%] [G loss: 3.735581]\n",
      "epoch:15 step:12241 [D loss: 0.290816, acc.: 86.72%] [G loss: 2.537082]\n",
      "epoch:15 step:12242 [D loss: 0.060907, acc.: 99.22%] [G loss: 3.443928]\n",
      "epoch:15 step:12243 [D loss: 0.129058, acc.: 96.88%] [G loss: 5.875502]\n",
      "epoch:15 step:12244 [D loss: 0.024032, acc.: 100.00%] [G loss: 6.076509]\n",
      "epoch:15 step:12245 [D loss: 0.035886, acc.: 100.00%] [G loss: 5.693927]\n",
      "epoch:15 step:12246 [D loss: 0.033369, acc.: 100.00%] [G loss: 4.691986]\n",
      "epoch:15 step:12247 [D loss: 0.028034, acc.: 100.00%] [G loss: 4.964925]\n",
      "epoch:15 step:12248 [D loss: 0.068685, acc.: 100.00%] [G loss: 1.183474]\n",
      "epoch:15 step:12249 [D loss: 0.062801, acc.: 99.22%] [G loss: 5.361333]\n",
      "epoch:15 step:12250 [D loss: 0.010114, acc.: 100.00%] [G loss: 5.500661]\n",
      "epoch:15 step:12251 [D loss: 0.151197, acc.: 92.19%] [G loss: 3.794466]\n",
      "epoch:15 step:12252 [D loss: 0.066568, acc.: 99.22%] [G loss: 0.451658]\n",
      "epoch:15 step:12253 [D loss: 0.179009, acc.: 94.53%] [G loss: 1.938823]\n",
      "epoch:15 step:12254 [D loss: 0.016760, acc.: 100.00%] [G loss: 5.936783]\n",
      "epoch:15 step:12255 [D loss: 0.160227, acc.: 93.75%] [G loss: 0.379207]\n",
      "epoch:15 step:12256 [D loss: 0.389259, acc.: 80.47%] [G loss: 7.338421]\n",
      "epoch:15 step:12257 [D loss: 0.329441, acc.: 84.38%] [G loss: 6.714645]\n",
      "epoch:15 step:12258 [D loss: 0.044109, acc.: 100.00%] [G loss: 0.969556]\n",
      "epoch:15 step:12259 [D loss: 0.030639, acc.: 100.00%] [G loss: 5.781059]\n",
      "epoch:15 step:12260 [D loss: 0.016647, acc.: 100.00%] [G loss: 2.884612]\n",
      "epoch:15 step:12261 [D loss: 0.038318, acc.: 99.22%] [G loss: 5.890768]\n",
      "epoch:15 step:12262 [D loss: 0.027996, acc.: 99.22%] [G loss: 5.926223]\n",
      "epoch:15 step:12263 [D loss: 0.006396, acc.: 100.00%] [G loss: 2.901937]\n",
      "epoch:15 step:12264 [D loss: 0.005343, acc.: 100.00%] [G loss: 4.784873]\n",
      "epoch:15 step:12265 [D loss: 0.039208, acc.: 99.22%] [G loss: 5.837120]\n",
      "epoch:15 step:12266 [D loss: 0.012680, acc.: 100.00%] [G loss: 3.036897]\n",
      "epoch:15 step:12267 [D loss: 0.290965, acc.: 85.94%] [G loss: 6.705656]\n",
      "epoch:15 step:12268 [D loss: 0.804656, acc.: 64.84%] [G loss: 1.424101]\n",
      "epoch:15 step:12269 [D loss: 0.250609, acc.: 89.06%] [G loss: 4.623336]\n",
      "epoch:15 step:12270 [D loss: 0.034391, acc.: 98.44%] [G loss: 3.853727]\n",
      "epoch:15 step:12271 [D loss: 0.158557, acc.: 92.19%] [G loss: 0.560238]\n",
      "epoch:15 step:12272 [D loss: 0.003406, acc.: 100.00%] [G loss: 1.854003]\n",
      "epoch:15 step:12273 [D loss: 0.018881, acc.: 100.00%] [G loss: 0.454502]\n",
      "epoch:15 step:12274 [D loss: 0.005725, acc.: 100.00%] [G loss: 0.204138]\n",
      "epoch:15 step:12275 [D loss: 0.002537, acc.: 100.00%] [G loss: 0.366250]\n",
      "epoch:15 step:12276 [D loss: 0.003716, acc.: 100.00%] [G loss: 2.620393]\n",
      "epoch:15 step:12277 [D loss: 0.004891, acc.: 100.00%] [G loss: 0.065898]\n",
      "epoch:15 step:12278 [D loss: 0.015459, acc.: 100.00%] [G loss: 0.061876]\n",
      "epoch:15 step:12279 [D loss: 0.021265, acc.: 100.00%] [G loss: 0.616397]\n",
      "epoch:15 step:12280 [D loss: 0.004838, acc.: 100.00%] [G loss: 0.271844]\n",
      "epoch:15 step:12281 [D loss: 0.031265, acc.: 98.44%] [G loss: 0.103539]\n",
      "epoch:15 step:12282 [D loss: 0.030825, acc.: 99.22%] [G loss: 0.168640]\n",
      "epoch:15 step:12283 [D loss: 0.016238, acc.: 100.00%] [G loss: 0.461893]\n",
      "epoch:15 step:12284 [D loss: 0.008633, acc.: 100.00%] [G loss: 0.564838]\n",
      "epoch:15 step:12285 [D loss: 0.003173, acc.: 100.00%] [G loss: 3.037927]\n",
      "epoch:15 step:12286 [D loss: 0.027181, acc.: 100.00%] [G loss: 0.484911]\n",
      "epoch:15 step:12287 [D loss: 0.004989, acc.: 100.00%] [G loss: 0.497195]\n",
      "epoch:15 step:12288 [D loss: 0.178170, acc.: 93.75%] [G loss: 2.560127]\n",
      "epoch:15 step:12289 [D loss: 0.003220, acc.: 100.00%] [G loss: 6.163722]\n",
      "epoch:15 step:12290 [D loss: 0.395152, acc.: 79.69%] [G loss: 0.546001]\n",
      "epoch:15 step:12291 [D loss: 0.026013, acc.: 99.22%] [G loss: 1.614219]\n",
      "epoch:15 step:12292 [D loss: 0.021718, acc.: 100.00%] [G loss: 1.632365]\n",
      "epoch:15 step:12293 [D loss: 0.004524, acc.: 100.00%] [G loss: 1.462373]\n",
      "epoch:15 step:12294 [D loss: 0.006132, acc.: 100.00%] [G loss: 2.961799]\n",
      "epoch:15 step:12295 [D loss: 0.038236, acc.: 100.00%] [G loss: 1.475089]\n",
      "epoch:15 step:12296 [D loss: 0.007944, acc.: 100.00%] [G loss: 2.517657]\n",
      "epoch:15 step:12297 [D loss: 0.017994, acc.: 100.00%] [G loss: 2.119191]\n",
      "epoch:15 step:12298 [D loss: 0.029559, acc.: 100.00%] [G loss: 2.123459]\n",
      "epoch:15 step:12299 [D loss: 0.009063, acc.: 100.00%] [G loss: 0.678039]\n",
      "epoch:15 step:12300 [D loss: 0.033960, acc.: 100.00%] [G loss: 0.524400]\n",
      "epoch:15 step:12301 [D loss: 0.015910, acc.: 100.00%] [G loss: 3.717789]\n",
      "epoch:15 step:12302 [D loss: 0.023046, acc.: 99.22%] [G loss: 3.040570]\n",
      "epoch:15 step:12303 [D loss: 0.063696, acc.: 98.44%] [G loss: 3.776079]\n",
      "epoch:15 step:12304 [D loss: 0.041904, acc.: 100.00%] [G loss: 4.342815]\n",
      "epoch:15 step:12305 [D loss: 0.220495, acc.: 90.62%] [G loss: 3.947468]\n",
      "epoch:15 step:12306 [D loss: 0.066658, acc.: 98.44%] [G loss: 4.697062]\n",
      "epoch:15 step:12307 [D loss: 0.003423, acc.: 100.00%] [G loss: 5.751991]\n",
      "epoch:15 step:12308 [D loss: 0.082499, acc.: 96.88%] [G loss: 4.660467]\n",
      "epoch:15 step:12309 [D loss: 0.051554, acc.: 98.44%] [G loss: 4.914502]\n",
      "epoch:15 step:12310 [D loss: 0.004370, acc.: 100.00%] [G loss: 6.317103]\n",
      "epoch:15 step:12311 [D loss: 0.008009, acc.: 100.00%] [G loss: 1.458997]\n",
      "epoch:15 step:12312 [D loss: 0.021381, acc.: 100.00%] [G loss: 1.580179]\n",
      "epoch:15 step:12313 [D loss: 0.020223, acc.: 100.00%] [G loss: 1.998339]\n",
      "epoch:15 step:12314 [D loss: 0.048602, acc.: 99.22%] [G loss: 7.580712]\n",
      "epoch:15 step:12315 [D loss: 0.084586, acc.: 98.44%] [G loss: 8.013517]\n",
      "epoch:15 step:12316 [D loss: 0.113997, acc.: 96.88%] [G loss: 0.198379]\n",
      "epoch:15 step:12317 [D loss: 0.342583, acc.: 84.38%] [G loss: 6.038730]\n",
      "epoch:15 step:12318 [D loss: 0.305524, acc.: 88.28%] [G loss: 4.848870]\n",
      "epoch:15 step:12319 [D loss: 0.211301, acc.: 90.62%] [G loss: 1.051020]\n",
      "epoch:15 step:12320 [D loss: 1.688361, acc.: 53.12%] [G loss: 8.413385]\n",
      "epoch:15 step:12321 [D loss: 3.129024, acc.: 50.00%] [G loss: 6.524472]\n",
      "epoch:15 step:12322 [D loss: 0.298855, acc.: 84.38%] [G loss: 3.714598]\n",
      "epoch:15 step:12323 [D loss: 0.025612, acc.: 100.00%] [G loss: 2.427910]\n",
      "epoch:15 step:12324 [D loss: 0.046896, acc.: 100.00%] [G loss: 3.336425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12325 [D loss: 0.020739, acc.: 99.22%] [G loss: 3.719856]\n",
      "epoch:15 step:12326 [D loss: 0.031484, acc.: 100.00%] [G loss: 2.439456]\n",
      "epoch:15 step:12327 [D loss: 0.192322, acc.: 92.97%] [G loss: 2.898058]\n",
      "epoch:15 step:12328 [D loss: 0.043094, acc.: 100.00%] [G loss: 3.343787]\n",
      "epoch:15 step:12329 [D loss: 0.110737, acc.: 99.22%] [G loss: 3.795824]\n",
      "epoch:15 step:12330 [D loss: 0.077781, acc.: 99.22%] [G loss: 2.011818]\n",
      "epoch:15 step:12331 [D loss: 0.160761, acc.: 95.31%] [G loss: 3.213374]\n",
      "epoch:15 step:12332 [D loss: 0.127836, acc.: 98.44%] [G loss: 2.148587]\n",
      "epoch:15 step:12333 [D loss: 0.059530, acc.: 100.00%] [G loss: 4.147959]\n",
      "epoch:15 step:12334 [D loss: 0.516552, acc.: 73.44%] [G loss: 2.519961]\n",
      "epoch:15 step:12335 [D loss: 0.099229, acc.: 97.66%] [G loss: 3.893533]\n",
      "epoch:15 step:12336 [D loss: 0.035152, acc.: 100.00%] [G loss: 1.286770]\n",
      "epoch:15 step:12337 [D loss: 0.071323, acc.: 99.22%] [G loss: 4.226104]\n",
      "epoch:15 step:12338 [D loss: 0.908893, acc.: 49.22%] [G loss: 5.609563]\n",
      "epoch:15 step:12339 [D loss: 1.063462, acc.: 57.81%] [G loss: 3.920280]\n",
      "epoch:15 step:12340 [D loss: 0.107081, acc.: 97.66%] [G loss: 3.431276]\n",
      "epoch:15 step:12341 [D loss: 0.030541, acc.: 100.00%] [G loss: 1.759594]\n",
      "epoch:15 step:12342 [D loss: 0.055204, acc.: 100.00%] [G loss: 1.881895]\n",
      "epoch:15 step:12343 [D loss: 0.025484, acc.: 100.00%] [G loss: 2.781505]\n",
      "epoch:15 step:12344 [D loss: 0.035018, acc.: 100.00%] [G loss: 1.312188]\n",
      "epoch:15 step:12345 [D loss: 0.014600, acc.: 100.00%] [G loss: 1.795577]\n",
      "epoch:15 step:12346 [D loss: 0.086832, acc.: 99.22%] [G loss: 0.695237]\n",
      "epoch:15 step:12347 [D loss: 0.058329, acc.: 100.00%] [G loss: 2.869703]\n",
      "epoch:15 step:12348 [D loss: 0.053478, acc.: 100.00%] [G loss: 2.192304]\n",
      "epoch:15 step:12349 [D loss: 0.070620, acc.: 97.66%] [G loss: 3.441289]\n",
      "epoch:15 step:12350 [D loss: 0.154417, acc.: 94.53%] [G loss: 1.708575]\n",
      "epoch:15 step:12351 [D loss: 0.058286, acc.: 98.44%] [G loss: 3.548515]\n",
      "epoch:15 step:12352 [D loss: 0.043061, acc.: 100.00%] [G loss: 1.041633]\n",
      "epoch:15 step:12353 [D loss: 0.047852, acc.: 100.00%] [G loss: 1.287646]\n",
      "epoch:15 step:12354 [D loss: 0.018391, acc.: 100.00%] [G loss: 2.524319]\n",
      "epoch:15 step:12355 [D loss: 0.011893, acc.: 100.00%] [G loss: 2.788580]\n",
      "epoch:15 step:12356 [D loss: 0.017962, acc.: 100.00%] [G loss: 0.388730]\n",
      "epoch:15 step:12357 [D loss: 0.120466, acc.: 97.66%] [G loss: 1.631522]\n",
      "epoch:15 step:12358 [D loss: 0.037167, acc.: 99.22%] [G loss: 1.748386]\n",
      "epoch:15 step:12359 [D loss: 0.020149, acc.: 100.00%] [G loss: 2.279815]\n",
      "epoch:15 step:12360 [D loss: 0.021765, acc.: 100.00%] [G loss: 0.411163]\n",
      "epoch:15 step:12361 [D loss: 0.264833, acc.: 89.84%] [G loss: 1.080331]\n",
      "epoch:15 step:12362 [D loss: 0.068321, acc.: 98.44%] [G loss: 1.707702]\n",
      "epoch:15 step:12363 [D loss: 0.092148, acc.: 96.88%] [G loss: 4.518817]\n",
      "epoch:15 step:12364 [D loss: 0.031176, acc.: 100.00%] [G loss: 0.968163]\n",
      "epoch:15 step:12365 [D loss: 0.011394, acc.: 100.00%] [G loss: 0.817384]\n",
      "epoch:15 step:12366 [D loss: 0.004378, acc.: 100.00%] [G loss: 4.121530]\n",
      "epoch:15 step:12367 [D loss: 0.076009, acc.: 97.66%] [G loss: 5.220931]\n",
      "epoch:15 step:12368 [D loss: 0.034352, acc.: 99.22%] [G loss: 2.249865]\n",
      "epoch:15 step:12369 [D loss: 0.124301, acc.: 94.53%] [G loss: 0.335274]\n",
      "epoch:15 step:12370 [D loss: 0.159499, acc.: 95.31%] [G loss: 1.389296]\n",
      "epoch:15 step:12371 [D loss: 0.003634, acc.: 100.00%] [G loss: 4.281929]\n",
      "epoch:15 step:12372 [D loss: 0.034127, acc.: 100.00%] [G loss: 2.137887]\n",
      "epoch:15 step:12373 [D loss: 0.039736, acc.: 98.44%] [G loss: 4.498819]\n",
      "epoch:15 step:12374 [D loss: 0.024962, acc.: 100.00%] [G loss: 4.442346]\n",
      "epoch:15 step:12375 [D loss: 0.018153, acc.: 100.00%] [G loss: 0.943693]\n",
      "epoch:15 step:12376 [D loss: 0.009555, acc.: 100.00%] [G loss: 0.497803]\n",
      "epoch:15 step:12377 [D loss: 0.042296, acc.: 100.00%] [G loss: 4.658689]\n",
      "epoch:15 step:12378 [D loss: 0.086056, acc.: 98.44%] [G loss: 2.279490]\n",
      "epoch:15 step:12379 [D loss: 0.019037, acc.: 100.00%] [G loss: 3.679571]\n",
      "epoch:15 step:12380 [D loss: 0.184831, acc.: 94.53%] [G loss: 1.515331]\n",
      "epoch:15 step:12381 [D loss: 0.351938, acc.: 81.25%] [G loss: 6.712588]\n",
      "epoch:15 step:12382 [D loss: 0.111464, acc.: 96.09%] [G loss: 7.006403]\n",
      "epoch:15 step:12383 [D loss: 0.463313, acc.: 70.31%] [G loss: 2.878867]\n",
      "epoch:15 step:12384 [D loss: 0.369660, acc.: 82.03%] [G loss: 5.720083]\n",
      "epoch:15 step:12385 [D loss: 0.015563, acc.: 100.00%] [G loss: 3.844463]\n",
      "epoch:15 step:12386 [D loss: 0.028275, acc.: 99.22%] [G loss: 7.097473]\n",
      "epoch:15 step:12387 [D loss: 0.111814, acc.: 96.88%] [G loss: 5.988267]\n",
      "epoch:15 step:12388 [D loss: 0.030977, acc.: 99.22%] [G loss: 5.282950]\n",
      "epoch:15 step:12389 [D loss: 0.058047, acc.: 100.00%] [G loss: 3.887309]\n",
      "epoch:15 step:12390 [D loss: 0.005967, acc.: 100.00%] [G loss: 6.132377]\n",
      "epoch:15 step:12391 [D loss: 0.010234, acc.: 100.00%] [G loss: 2.427809]\n",
      "epoch:15 step:12392 [D loss: 0.049404, acc.: 100.00%] [G loss: 2.225581]\n",
      "epoch:15 step:12393 [D loss: 0.052105, acc.: 99.22%] [G loss: 6.426239]\n",
      "epoch:15 step:12394 [D loss: 0.014558, acc.: 100.00%] [G loss: 1.995438]\n",
      "epoch:15 step:12395 [D loss: 0.020064, acc.: 100.00%] [G loss: 5.882127]\n",
      "epoch:15 step:12396 [D loss: 0.022386, acc.: 99.22%] [G loss: 5.491976]\n",
      "epoch:15 step:12397 [D loss: 0.076964, acc.: 98.44%] [G loss: 2.621830]\n",
      "epoch:15 step:12398 [D loss: 0.016520, acc.: 100.00%] [G loss: 0.832960]\n",
      "epoch:15 step:12399 [D loss: 0.059660, acc.: 99.22%] [G loss: 5.091112]\n",
      "epoch:15 step:12400 [D loss: 1.049698, acc.: 55.47%] [G loss: 6.760448]\n",
      "epoch:15 step:12401 [D loss: 1.781341, acc.: 52.34%] [G loss: 4.485648]\n",
      "epoch:15 step:12402 [D loss: 0.354337, acc.: 82.03%] [G loss: 1.317966]\n",
      "epoch:15 step:12403 [D loss: 0.088038, acc.: 98.44%] [G loss: 2.321697]\n",
      "epoch:15 step:12404 [D loss: 0.056970, acc.: 98.44%] [G loss: 0.905515]\n",
      "epoch:15 step:12405 [D loss: 0.014274, acc.: 100.00%] [G loss: 2.295704]\n",
      "epoch:15 step:12406 [D loss: 0.030455, acc.: 100.00%] [G loss: 1.098152]\n",
      "epoch:15 step:12407 [D loss: 0.020899, acc.: 100.00%] [G loss: 1.480804]\n",
      "epoch:15 step:12408 [D loss: 0.049491, acc.: 100.00%] [G loss: 2.552148]\n",
      "epoch:15 step:12409 [D loss: 0.016663, acc.: 100.00%] [G loss: 1.368408]\n",
      "epoch:15 step:12410 [D loss: 0.020680, acc.: 100.00%] [G loss: 3.353156]\n",
      "epoch:15 step:12411 [D loss: 0.014904, acc.: 100.00%] [G loss: 2.181253]\n",
      "epoch:15 step:12412 [D loss: 0.021505, acc.: 100.00%] [G loss: 3.672010]\n",
      "epoch:15 step:12413 [D loss: 0.031550, acc.: 100.00%] [G loss: 3.064300]\n",
      "epoch:15 step:12414 [D loss: 0.023165, acc.: 100.00%] [G loss: 2.458892]\n",
      "epoch:15 step:12415 [D loss: 0.032914, acc.: 99.22%] [G loss: 2.349667]\n",
      "epoch:15 step:12416 [D loss: 0.009140, acc.: 100.00%] [G loss: 4.855620]\n",
      "epoch:15 step:12417 [D loss: 0.033738, acc.: 100.00%] [G loss: 3.345220]\n",
      "epoch:15 step:12418 [D loss: 0.081148, acc.: 96.88%] [G loss: 2.024442]\n",
      "epoch:15 step:12419 [D loss: 0.039967, acc.: 99.22%] [G loss: 3.353067]\n",
      "epoch:15 step:12420 [D loss: 0.020841, acc.: 100.00%] [G loss: 3.193295]\n",
      "epoch:15 step:12421 [D loss: 0.014669, acc.: 100.00%] [G loss: 3.288677]\n",
      "epoch:15 step:12422 [D loss: 0.018537, acc.: 100.00%] [G loss: 3.827670]\n",
      "epoch:15 step:12423 [D loss: 0.021173, acc.: 100.00%] [G loss: 3.761779]\n",
      "epoch:15 step:12424 [D loss: 0.006957, acc.: 100.00%] [G loss: 4.286318]\n",
      "epoch:15 step:12425 [D loss: 0.017415, acc.: 100.00%] [G loss: 2.401546]\n",
      "epoch:15 step:12426 [D loss: 0.021859, acc.: 100.00%] [G loss: 1.495244]\n",
      "epoch:15 step:12427 [D loss: 0.031202, acc.: 100.00%] [G loss: 1.530664]\n",
      "epoch:15 step:12428 [D loss: 0.262975, acc.: 88.28%] [G loss: 3.150424]\n",
      "epoch:15 step:12429 [D loss: 0.051048, acc.: 100.00%] [G loss: 4.107102]\n",
      "epoch:15 step:12430 [D loss: 0.087229, acc.: 96.09%] [G loss: 1.889490]\n",
      "epoch:15 step:12431 [D loss: 0.086704, acc.: 97.66%] [G loss: 3.323170]\n",
      "epoch:15 step:12432 [D loss: 0.004404, acc.: 100.00%] [G loss: 4.558607]\n",
      "epoch:15 step:12433 [D loss: 0.019450, acc.: 99.22%] [G loss: 3.213799]\n",
      "epoch:15 step:12434 [D loss: 0.009425, acc.: 100.00%] [G loss: 3.019121]\n",
      "epoch:15 step:12435 [D loss: 0.020119, acc.: 100.00%] [G loss: 1.869824]\n",
      "epoch:15 step:12436 [D loss: 0.036625, acc.: 99.22%] [G loss: 2.302838]\n",
      "epoch:15 step:12437 [D loss: 0.017064, acc.: 100.00%] [G loss: 3.856404]\n",
      "epoch:15 step:12438 [D loss: 0.006098, acc.: 100.00%] [G loss: 2.171861]\n",
      "epoch:15 step:12439 [D loss: 0.078882, acc.: 98.44%] [G loss: 3.842465]\n",
      "epoch:15 step:12440 [D loss: 1.335929, acc.: 32.03%] [G loss: 4.190801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15 step:12441 [D loss: 0.301815, acc.: 82.03%] [G loss: 4.890206]\n",
      "epoch:15 step:12442 [D loss: 0.055001, acc.: 100.00%] [G loss: 2.329008]\n",
      "epoch:15 step:12443 [D loss: 0.007900, acc.: 100.00%] [G loss: 2.192399]\n",
      "epoch:15 step:12444 [D loss: 0.018041, acc.: 99.22%] [G loss: 2.137235]\n",
      "epoch:15 step:12445 [D loss: 0.013994, acc.: 100.00%] [G loss: 2.074365]\n",
      "epoch:15 step:12446 [D loss: 0.010374, acc.: 100.00%] [G loss: 1.503005]\n",
      "epoch:15 step:12447 [D loss: 0.049317, acc.: 99.22%] [G loss: 3.005080]\n",
      "epoch:15 step:12448 [D loss: 0.009631, acc.: 100.00%] [G loss: 3.545741]\n",
      "epoch:15 step:12449 [D loss: 0.005279, acc.: 100.00%] [G loss: 3.423429]\n",
      "epoch:15 step:12450 [D loss: 0.057765, acc.: 97.66%] [G loss: 1.536290]\n",
      "epoch:15 step:12451 [D loss: 0.166533, acc.: 93.75%] [G loss: 1.487206]\n",
      "epoch:15 step:12452 [D loss: 0.006576, acc.: 100.00%] [G loss: 2.981312]\n",
      "epoch:15 step:12453 [D loss: 0.033215, acc.: 100.00%] [G loss: 1.541843]\n",
      "epoch:15 step:12454 [D loss: 0.047012, acc.: 99.22%] [G loss: 3.012459]\n",
      "epoch:15 step:12455 [D loss: 0.065772, acc.: 98.44%] [G loss: 1.018412]\n",
      "epoch:15 step:12456 [D loss: 0.009405, acc.: 100.00%] [G loss: 3.923681]\n",
      "epoch:15 step:12457 [D loss: 0.010773, acc.: 100.00%] [G loss: 1.970539]\n",
      "epoch:15 step:12458 [D loss: 0.114252, acc.: 96.88%] [G loss: 1.141938]\n",
      "epoch:15 step:12459 [D loss: 0.038033, acc.: 100.00%] [G loss: 3.015912]\n",
      "epoch:15 step:12460 [D loss: 0.016000, acc.: 100.00%] [G loss: 3.716891]\n",
      "epoch:15 step:12461 [D loss: 0.025876, acc.: 100.00%] [G loss: 2.904702]\n",
      "epoch:15 step:12462 [D loss: 0.036199, acc.: 99.22%] [G loss: 2.567690]\n",
      "epoch:15 step:12463 [D loss: 0.007243, acc.: 100.00%] [G loss: 4.155807]\n",
      "epoch:15 step:12464 [D loss: 0.058321, acc.: 99.22%] [G loss: 3.623136]\n",
      "epoch:15 step:12465 [D loss: 0.025422, acc.: 100.00%] [G loss: 4.938350]\n",
      "epoch:15 step:12466 [D loss: 0.008097, acc.: 100.00%] [G loss: 4.663879]\n",
      "epoch:15 step:12467 [D loss: 0.066169, acc.: 99.22%] [G loss: 4.269785]\n",
      "epoch:15 step:12468 [D loss: 0.010244, acc.: 100.00%] [G loss: 4.184955]\n",
      "epoch:15 step:12469 [D loss: 0.027389, acc.: 100.00%] [G loss: 3.110567]\n",
      "epoch:15 step:12470 [D loss: 0.062562, acc.: 99.22%] [G loss: 4.810649]\n",
      "epoch:15 step:12471 [D loss: 0.009548, acc.: 100.00%] [G loss: 5.250903]\n",
      "epoch:15 step:12472 [D loss: 0.043073, acc.: 99.22%] [G loss: 4.253843]\n",
      "epoch:15 step:12473 [D loss: 0.013210, acc.: 100.00%] [G loss: 3.654533]\n",
      "epoch:15 step:12474 [D loss: 0.024084, acc.: 100.00%] [G loss: 3.362396]\n",
      "epoch:15 step:12475 [D loss: 0.011008, acc.: 100.00%] [G loss: 2.772918]\n",
      "epoch:15 step:12476 [D loss: 0.009239, acc.: 100.00%] [G loss: 2.838532]\n",
      "epoch:15 step:12477 [D loss: 0.346344, acc.: 88.28%] [G loss: 5.991386]\n",
      "epoch:15 step:12478 [D loss: 0.063574, acc.: 99.22%] [G loss: 6.735121]\n",
      "epoch:15 step:12479 [D loss: 0.063326, acc.: 97.66%] [G loss: 2.442885]\n",
      "epoch:15 step:12480 [D loss: 0.002809, acc.: 100.00%] [G loss: 5.431093]\n",
      "epoch:15 step:12481 [D loss: 0.008817, acc.: 100.00%] [G loss: 1.816529]\n",
      "epoch:15 step:12482 [D loss: 0.007181, acc.: 100.00%] [G loss: 0.812541]\n",
      "epoch:15 step:12483 [D loss: 0.073066, acc.: 97.66%] [G loss: 5.679713]\n",
      "epoch:15 step:12484 [D loss: 0.052993, acc.: 98.44%] [G loss: 5.494843]\n",
      "epoch:15 step:12485 [D loss: 0.073956, acc.: 97.66%] [G loss: 4.039763]\n",
      "epoch:15 step:12486 [D loss: 0.080279, acc.: 98.44%] [G loss: 3.583505]\n",
      "epoch:15 step:12487 [D loss: 0.052010, acc.: 99.22%] [G loss: 2.223865]\n",
      "epoch:15 step:12488 [D loss: 0.012694, acc.: 100.00%] [G loss: 3.787542]\n",
      "epoch:15 step:12489 [D loss: 0.012098, acc.: 100.00%] [G loss: 3.186918]\n",
      "epoch:15 step:12490 [D loss: 0.002314, acc.: 100.00%] [G loss: 4.713151]\n",
      "epoch:15 step:12491 [D loss: 0.013276, acc.: 100.00%] [G loss: 5.061187]\n",
      "epoch:15 step:12492 [D loss: 0.008513, acc.: 100.00%] [G loss: 3.743494]\n",
      "epoch:15 step:12493 [D loss: 0.057803, acc.: 99.22%] [G loss: 1.838282]\n",
      "epoch:15 step:12494 [D loss: 0.035903, acc.: 100.00%] [G loss: 2.793244]\n",
      "epoch:15 step:12495 [D loss: 0.013252, acc.: 100.00%] [G loss: 2.490913]\n",
      "epoch:15 step:12496 [D loss: 0.003784, acc.: 100.00%] [G loss: 3.098018]\n",
      "epoch:16 step:12497 [D loss: 0.008395, acc.: 100.00%] [G loss: 1.126460]\n",
      "epoch:16 step:12498 [D loss: 0.018394, acc.: 100.00%] [G loss: 2.668179]\n",
      "epoch:16 step:12499 [D loss: 0.058848, acc.: 99.22%] [G loss: 2.297373]\n",
      "epoch:16 step:12500 [D loss: 0.029333, acc.: 100.00%] [G loss: 4.637074]\n",
      "epoch:16 step:12501 [D loss: 0.016673, acc.: 100.00%] [G loss: 1.604270]\n",
      "epoch:16 step:12502 [D loss: 0.020078, acc.: 100.00%] [G loss: 0.708401]\n",
      "epoch:16 step:12503 [D loss: 0.059573, acc.: 99.22%] [G loss: 2.295776]\n",
      "epoch:16 step:12504 [D loss: 0.009939, acc.: 100.00%] [G loss: 2.050200]\n",
      "epoch:16 step:12505 [D loss: 0.029064, acc.: 100.00%] [G loss: 1.111591]\n",
      "epoch:16 step:12506 [D loss: 0.088665, acc.: 96.88%] [G loss: 2.279433]\n",
      "epoch:16 step:12507 [D loss: 0.003068, acc.: 100.00%] [G loss: 4.523179]\n",
      "epoch:16 step:12508 [D loss: 0.702050, acc.: 70.31%] [G loss: 10.580166]\n",
      "epoch:16 step:12509 [D loss: 2.371555, acc.: 50.78%] [G loss: 2.758244]\n",
      "epoch:16 step:12510 [D loss: 0.591085, acc.: 70.31%] [G loss: 6.528613]\n",
      "epoch:16 step:12511 [D loss: 0.243836, acc.: 86.72%] [G loss: 6.912319]\n",
      "epoch:16 step:12512 [D loss: 0.155781, acc.: 92.97%] [G loss: 0.511809]\n",
      "epoch:16 step:12513 [D loss: 0.023911, acc.: 100.00%] [G loss: 5.760958]\n",
      "epoch:16 step:12514 [D loss: 0.017891, acc.: 100.00%] [G loss: 4.266771]\n",
      "epoch:16 step:12515 [D loss: 0.059153, acc.: 99.22%] [G loss: 5.822022]\n",
      "epoch:16 step:12516 [D loss: 0.016619, acc.: 100.00%] [G loss: 0.091574]\n",
      "epoch:16 step:12517 [D loss: 0.104757, acc.: 98.44%] [G loss: 6.855549]\n",
      "epoch:16 step:12518 [D loss: 0.071507, acc.: 98.44%] [G loss: 0.743249]\n",
      "epoch:16 step:12519 [D loss: 0.269242, acc.: 90.62%] [G loss: 6.820093]\n",
      "epoch:16 step:12520 [D loss: 0.036059, acc.: 99.22%] [G loss: 6.829678]\n",
      "epoch:16 step:12521 [D loss: 0.241264, acc.: 89.84%] [G loss: 5.871395]\n",
      "epoch:16 step:12522 [D loss: 0.005065, acc.: 100.00%] [G loss: 5.571244]\n",
      "epoch:16 step:12523 [D loss: 0.074254, acc.: 97.66%] [G loss: 5.714740]\n",
      "epoch:16 step:12524 [D loss: 0.008514, acc.: 100.00%] [G loss: 6.288216]\n",
      "epoch:16 step:12525 [D loss: 0.025996, acc.: 100.00%] [G loss: 6.008715]\n",
      "epoch:16 step:12526 [D loss: 0.108520, acc.: 97.66%] [G loss: 5.199132]\n",
      "epoch:16 step:12527 [D loss: 0.009872, acc.: 100.00%] [G loss: 5.062460]\n",
      "epoch:16 step:12528 [D loss: 0.011802, acc.: 100.00%] [G loss: 5.795340]\n",
      "epoch:16 step:12529 [D loss: 0.006187, acc.: 100.00%] [G loss: 5.411227]\n",
      "epoch:16 step:12530 [D loss: 0.006798, acc.: 100.00%] [G loss: 4.156815]\n",
      "epoch:16 step:12531 [D loss: 0.009213, acc.: 100.00%] [G loss: 4.904772]\n",
      "epoch:16 step:12532 [D loss: 0.049192, acc.: 100.00%] [G loss: 2.005255]\n",
      "epoch:16 step:12533 [D loss: 0.038630, acc.: 99.22%] [G loss: 4.628648]\n",
      "epoch:16 step:12534 [D loss: 0.013119, acc.: 100.00%] [G loss: 5.404894]\n",
      "epoch:16 step:12535 [D loss: 0.027084, acc.: 99.22%] [G loss: 5.039184]\n",
      "epoch:16 step:12536 [D loss: 0.007976, acc.: 100.00%] [G loss: 1.264350]\n",
      "epoch:16 step:12537 [D loss: 0.009814, acc.: 100.00%] [G loss: 4.582061]\n",
      "epoch:16 step:12538 [D loss: 0.008850, acc.: 100.00%] [G loss: 4.940378]\n",
      "epoch:16 step:12539 [D loss: 0.008413, acc.: 100.00%] [G loss: 4.217505]\n",
      "epoch:16 step:12540 [D loss: 0.059551, acc.: 100.00%] [G loss: 0.154664]\n",
      "epoch:16 step:12541 [D loss: 0.258458, acc.: 86.72%] [G loss: 2.341475]\n",
      "epoch:16 step:12542 [D loss: 1.187677, acc.: 56.25%] [G loss: 1.593615]\n",
      "epoch:16 step:12543 [D loss: 1.065087, acc.: 60.16%] [G loss: 3.767228]\n",
      "epoch:16 step:12544 [D loss: 1.612289, acc.: 50.78%] [G loss: 2.582762]\n",
      "epoch:16 step:12545 [D loss: 0.147476, acc.: 92.97%] [G loss: 0.548674]\n",
      "epoch:16 step:12546 [D loss: 0.096853, acc.: 95.31%] [G loss: 0.779434]\n",
      "epoch:16 step:12547 [D loss: 0.189918, acc.: 90.62%] [G loss: 1.672828]\n",
      "epoch:16 step:12548 [D loss: 0.036933, acc.: 100.00%] [G loss: 1.903425]\n",
      "epoch:16 step:12549 [D loss: 0.026671, acc.: 100.00%] [G loss: 3.896258]\n",
      "epoch:16 step:12550 [D loss: 0.043769, acc.: 99.22%] [G loss: 3.574053]\n",
      "epoch:16 step:12551 [D loss: 0.013553, acc.: 100.00%] [G loss: 1.751876]\n",
      "epoch:16 step:12552 [D loss: 0.249987, acc.: 91.41%] [G loss: 1.009665]\n",
      "epoch:16 step:12553 [D loss: 0.141316, acc.: 92.97%] [G loss: 0.980812]\n",
      "epoch:16 step:12554 [D loss: 0.004752, acc.: 100.00%] [G loss: 4.043207]\n",
      "epoch:16 step:12555 [D loss: 0.007440, acc.: 100.00%] [G loss: 3.762815]\n",
      "epoch:16 step:12556 [D loss: 0.007587, acc.: 100.00%] [G loss: 1.849977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12557 [D loss: 0.042423, acc.: 98.44%] [G loss: 0.968521]\n",
      "epoch:16 step:12558 [D loss: 0.014053, acc.: 100.00%] [G loss: 1.066830]\n",
      "epoch:16 step:12559 [D loss: 0.019118, acc.: 100.00%] [G loss: 3.342061]\n",
      "epoch:16 step:12560 [D loss: 0.118426, acc.: 98.44%] [G loss: 2.885885]\n",
      "epoch:16 step:12561 [D loss: 0.068392, acc.: 97.66%] [G loss: 0.003229]\n",
      "epoch:16 step:12562 [D loss: 0.270016, acc.: 85.16%] [G loss: 2.476831]\n",
      "epoch:16 step:12563 [D loss: 0.066215, acc.: 98.44%] [G loss: 6.085284]\n",
      "epoch:16 step:12564 [D loss: 0.552383, acc.: 76.56%] [G loss: 2.528330]\n",
      "epoch:16 step:12565 [D loss: 0.005903, acc.: 100.00%] [G loss: 0.002638]\n",
      "epoch:16 step:12566 [D loss: 0.115807, acc.: 95.31%] [G loss: 2.072395]\n",
      "epoch:16 step:12567 [D loss: 0.027663, acc.: 100.00%] [G loss: 0.319810]\n",
      "epoch:16 step:12568 [D loss: 0.017905, acc.: 100.00%] [G loss: 4.083436]\n",
      "epoch:16 step:12569 [D loss: 0.017271, acc.: 100.00%] [G loss: 4.601482]\n",
      "epoch:16 step:12570 [D loss: 0.010960, acc.: 100.00%] [G loss: 0.213965]\n",
      "epoch:16 step:12571 [D loss: 0.011024, acc.: 100.00%] [G loss: 1.819933]\n",
      "epoch:16 step:12572 [D loss: 0.009969, acc.: 100.00%] [G loss: 0.351390]\n",
      "epoch:16 step:12573 [D loss: 0.675699, acc.: 60.16%] [G loss: 1.290243]\n",
      "epoch:16 step:12574 [D loss: 0.457723, acc.: 76.56%] [G loss: 4.538407]\n",
      "epoch:16 step:12575 [D loss: 0.007493, acc.: 100.00%] [G loss: 0.167850]\n",
      "epoch:16 step:12576 [D loss: 0.015724, acc.: 100.00%] [G loss: 3.463602]\n",
      "epoch:16 step:12577 [D loss: 0.014079, acc.: 100.00%] [G loss: 0.205821]\n",
      "epoch:16 step:12578 [D loss: 0.005943, acc.: 100.00%] [G loss: 4.348064]\n",
      "epoch:16 step:12579 [D loss: 0.003289, acc.: 100.00%] [G loss: 0.206665]\n",
      "epoch:16 step:12580 [D loss: 0.004170, acc.: 100.00%] [G loss: 1.301706]\n",
      "epoch:16 step:12581 [D loss: 0.022860, acc.: 100.00%] [G loss: 0.026582]\n",
      "epoch:16 step:12582 [D loss: 0.048809, acc.: 99.22%] [G loss: 4.772310]\n",
      "epoch:16 step:12583 [D loss: 0.003867, acc.: 100.00%] [G loss: 4.516872]\n",
      "epoch:16 step:12584 [D loss: 0.021425, acc.: 100.00%] [G loss: 3.657129]\n",
      "epoch:16 step:12585 [D loss: 0.011557, acc.: 100.00%] [G loss: 0.111967]\n",
      "epoch:16 step:12586 [D loss: 0.077376, acc.: 97.66%] [G loss: 0.000424]\n",
      "epoch:16 step:12587 [D loss: 0.149054, acc.: 95.31%] [G loss: 0.095450]\n",
      "epoch:16 step:12588 [D loss: 0.002031, acc.: 100.00%] [G loss: 4.673691]\n",
      "epoch:16 step:12589 [D loss: 0.010044, acc.: 100.00%] [G loss: 1.168215]\n",
      "epoch:16 step:12590 [D loss: 0.054228, acc.: 98.44%] [G loss: 4.397134]\n",
      "epoch:16 step:12591 [D loss: 0.004969, acc.: 100.00%] [G loss: 0.040486]\n",
      "epoch:16 step:12592 [D loss: 0.009131, acc.: 100.00%] [G loss: 0.039793]\n",
      "epoch:16 step:12593 [D loss: 0.047322, acc.: 99.22%] [G loss: 4.443686]\n",
      "epoch:16 step:12594 [D loss: 0.003031, acc.: 100.00%] [G loss: 0.892260]\n",
      "epoch:16 step:12595 [D loss: 0.041036, acc.: 99.22%] [G loss: 0.040640]\n",
      "epoch:16 step:12596 [D loss: 0.042329, acc.: 100.00%] [G loss: 4.567480]\n",
      "epoch:16 step:12597 [D loss: 0.002554, acc.: 100.00%] [G loss: 0.538617]\n",
      "epoch:16 step:12598 [D loss: 0.003743, acc.: 100.00%] [G loss: 0.445104]\n",
      "epoch:16 step:12599 [D loss: 0.026457, acc.: 100.00%] [G loss: 0.121680]\n",
      "epoch:16 step:12600 [D loss: 0.045453, acc.: 100.00%] [G loss: 0.413676]\n",
      "epoch:16 step:12601 [D loss: 0.021939, acc.: 100.00%] [G loss: 0.985347]\n",
      "epoch:16 step:12602 [D loss: 0.044441, acc.: 99.22%] [G loss: 5.187637]\n",
      "epoch:16 step:12603 [D loss: 0.036226, acc.: 100.00%] [G loss: 4.894795]\n",
      "epoch:16 step:12604 [D loss: 0.002018, acc.: 100.00%] [G loss: 0.171866]\n",
      "epoch:16 step:12605 [D loss: 0.029740, acc.: 99.22%] [G loss: 4.726707]\n",
      "epoch:16 step:12606 [D loss: 0.055096, acc.: 98.44%] [G loss: 0.111302]\n",
      "epoch:16 step:12607 [D loss: 0.014497, acc.: 100.00%] [G loss: 3.243834]\n",
      "epoch:16 step:12608 [D loss: 0.006035, acc.: 100.00%] [G loss: 0.245165]\n",
      "epoch:16 step:12609 [D loss: 0.011979, acc.: 100.00%] [G loss: 1.244254]\n",
      "epoch:16 step:12610 [D loss: 0.067173, acc.: 98.44%] [G loss: 0.281970]\n",
      "epoch:16 step:12611 [D loss: 0.005162, acc.: 100.00%] [G loss: 0.241621]\n",
      "epoch:16 step:12612 [D loss: 0.249747, acc.: 90.62%] [G loss: 3.943108]\n",
      "epoch:16 step:12613 [D loss: 0.038129, acc.: 99.22%] [G loss: 4.724716]\n",
      "epoch:16 step:12614 [D loss: 0.071634, acc.: 96.88%] [G loss: 2.330981]\n",
      "epoch:16 step:12615 [D loss: 0.014084, acc.: 100.00%] [G loss: 5.610406]\n",
      "epoch:16 step:12616 [D loss: 0.002590, acc.: 100.00%] [G loss: 4.087436]\n",
      "epoch:16 step:12617 [D loss: 0.001924, acc.: 100.00%] [G loss: 1.022703]\n",
      "epoch:16 step:12618 [D loss: 0.097343, acc.: 96.09%] [G loss: 3.611902]\n",
      "epoch:16 step:12619 [D loss: 0.154744, acc.: 89.84%] [G loss: 1.249292]\n",
      "epoch:16 step:12620 [D loss: 0.069845, acc.: 98.44%] [G loss: 2.956380]\n",
      "epoch:16 step:12621 [D loss: 0.005613, acc.: 100.00%] [G loss: 4.404743]\n",
      "epoch:16 step:12622 [D loss: 0.007708, acc.: 100.00%] [G loss: 3.696048]\n",
      "epoch:16 step:12623 [D loss: 0.006727, acc.: 100.00%] [G loss: 2.937891]\n",
      "epoch:16 step:12624 [D loss: 0.017136, acc.: 100.00%] [G loss: 5.829300]\n",
      "epoch:16 step:12625 [D loss: 0.030450, acc.: 100.00%] [G loss: 0.950763]\n",
      "epoch:16 step:12626 [D loss: 0.015164, acc.: 100.00%] [G loss: 4.774402]\n",
      "epoch:16 step:12627 [D loss: 0.023208, acc.: 100.00%] [G loss: 3.497793]\n",
      "epoch:16 step:12628 [D loss: 0.024601, acc.: 99.22%] [G loss: 1.983488]\n",
      "epoch:16 step:12629 [D loss: 0.030277, acc.: 100.00%] [G loss: 3.740851]\n",
      "epoch:16 step:12630 [D loss: 0.008510, acc.: 100.00%] [G loss: 5.201493]\n",
      "epoch:16 step:12631 [D loss: 0.019823, acc.: 100.00%] [G loss: 4.865394]\n",
      "epoch:16 step:12632 [D loss: 0.021040, acc.: 100.00%] [G loss: 3.904423]\n",
      "epoch:16 step:12633 [D loss: 0.019485, acc.: 100.00%] [G loss: 4.052978]\n",
      "epoch:16 step:12634 [D loss: 0.008795, acc.: 100.00%] [G loss: 4.150210]\n",
      "epoch:16 step:12635 [D loss: 0.060692, acc.: 99.22%] [G loss: 3.518989]\n",
      "epoch:16 step:12636 [D loss: 0.317411, acc.: 88.28%] [G loss: 7.072241]\n",
      "epoch:16 step:12637 [D loss: 1.517936, acc.: 29.69%] [G loss: 3.370559]\n",
      "epoch:16 step:12638 [D loss: 0.002924, acc.: 100.00%] [G loss: 7.684535]\n",
      "epoch:16 step:12639 [D loss: 0.544599, acc.: 71.88%] [G loss: 5.241676]\n",
      "epoch:16 step:12640 [D loss: 0.168139, acc.: 92.97%] [G loss: 0.571154]\n",
      "epoch:16 step:12641 [D loss: 0.006172, acc.: 100.00%] [G loss: 5.776566]\n",
      "epoch:16 step:12642 [D loss: 0.002863, acc.: 100.00%] [G loss: 5.414320]\n",
      "epoch:16 step:12643 [D loss: 0.013144, acc.: 100.00%] [G loss: 4.698295]\n",
      "epoch:16 step:12644 [D loss: 0.006762, acc.: 100.00%] [G loss: 1.063045]\n",
      "epoch:16 step:12645 [D loss: 0.009730, acc.: 100.00%] [G loss: 3.766634]\n",
      "epoch:16 step:12646 [D loss: 0.002708, acc.: 100.00%] [G loss: 0.531932]\n",
      "epoch:16 step:12647 [D loss: 0.008798, acc.: 100.00%] [G loss: 3.305638]\n",
      "epoch:16 step:12648 [D loss: 0.007407, acc.: 100.00%] [G loss: 3.752812]\n",
      "epoch:16 step:12649 [D loss: 0.128514, acc.: 96.09%] [G loss: 1.506454]\n",
      "epoch:16 step:12650 [D loss: 0.490201, acc.: 76.56%] [G loss: 4.295627]\n",
      "epoch:16 step:12651 [D loss: 0.124420, acc.: 93.75%] [G loss: 5.745213]\n",
      "epoch:16 step:12652 [D loss: 0.297355, acc.: 83.59%] [G loss: 2.203187]\n",
      "epoch:16 step:12653 [D loss: 0.007416, acc.: 100.00%] [G loss: 1.344316]\n",
      "epoch:16 step:12654 [D loss: 0.019939, acc.: 100.00%] [G loss: 2.887027]\n",
      "epoch:16 step:12655 [D loss: 0.041386, acc.: 99.22%] [G loss: 1.851317]\n",
      "epoch:16 step:12656 [D loss: 0.001969, acc.: 100.00%] [G loss: 1.474465]\n",
      "epoch:16 step:12657 [D loss: 0.005336, acc.: 100.00%] [G loss: 2.228697]\n",
      "epoch:16 step:12658 [D loss: 0.008823, acc.: 100.00%] [G loss: 1.230887]\n",
      "epoch:16 step:12659 [D loss: 0.059049, acc.: 99.22%] [G loss: 2.349967]\n",
      "epoch:16 step:12660 [D loss: 0.002553, acc.: 100.00%] [G loss: 5.056356]\n",
      "epoch:16 step:12661 [D loss: 0.002835, acc.: 100.00%] [G loss: 4.144432]\n",
      "epoch:16 step:12662 [D loss: 0.030218, acc.: 100.00%] [G loss: 4.377172]\n",
      "epoch:16 step:12663 [D loss: 0.006352, acc.: 100.00%] [G loss: 3.947438]\n",
      "epoch:16 step:12664 [D loss: 0.007353, acc.: 100.00%] [G loss: 3.595694]\n",
      "epoch:16 step:12665 [D loss: 0.009955, acc.: 100.00%] [G loss: 3.862147]\n",
      "epoch:16 step:12666 [D loss: 0.003155, acc.: 100.00%] [G loss: 4.411115]\n",
      "epoch:16 step:12667 [D loss: 0.004245, acc.: 100.00%] [G loss: 3.640706]\n",
      "epoch:16 step:12668 [D loss: 0.009500, acc.: 100.00%] [G loss: 3.848927]\n",
      "epoch:16 step:12669 [D loss: 0.018197, acc.: 100.00%] [G loss: 4.260530]\n",
      "epoch:16 step:12670 [D loss: 0.009856, acc.: 100.00%] [G loss: 4.676879]\n",
      "epoch:16 step:12671 [D loss: 0.015522, acc.: 99.22%] [G loss: 4.531874]\n",
      "epoch:16 step:12672 [D loss: 0.107142, acc.: 95.31%] [G loss: 4.060804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12673 [D loss: 0.053123, acc.: 98.44%] [G loss: 5.104848]\n",
      "epoch:16 step:12674 [D loss: 0.004469, acc.: 100.00%] [G loss: 4.768132]\n",
      "epoch:16 step:12675 [D loss: 0.037364, acc.: 100.00%] [G loss: 5.340082]\n",
      "epoch:16 step:12676 [D loss: 0.012489, acc.: 100.00%] [G loss: 5.898126]\n",
      "epoch:16 step:12677 [D loss: 0.002898, acc.: 100.00%] [G loss: 0.413371]\n",
      "epoch:16 step:12678 [D loss: 0.181035, acc.: 92.97%] [G loss: 7.163271]\n",
      "epoch:16 step:12679 [D loss: 0.031726, acc.: 98.44%] [G loss: 7.292793]\n",
      "epoch:16 step:12680 [D loss: 0.018504, acc.: 100.00%] [G loss: 7.520201]\n",
      "epoch:16 step:12681 [D loss: 0.028185, acc.: 99.22%] [G loss: 7.006366]\n",
      "epoch:16 step:12682 [D loss: 0.002877, acc.: 100.00%] [G loss: 6.790077]\n",
      "epoch:16 step:12683 [D loss: 0.109861, acc.: 96.09%] [G loss: 5.837073]\n",
      "epoch:16 step:12684 [D loss: 0.005117, acc.: 100.00%] [G loss: 5.439240]\n",
      "epoch:16 step:12685 [D loss: 0.003646, acc.: 100.00%] [G loss: 5.793594]\n",
      "epoch:16 step:12686 [D loss: 0.002714, acc.: 100.00%] [G loss: 5.368187]\n",
      "epoch:16 step:12687 [D loss: 0.004495, acc.: 100.00%] [G loss: 5.798977]\n",
      "epoch:16 step:12688 [D loss: 0.004708, acc.: 100.00%] [G loss: 5.729670]\n",
      "epoch:16 step:12689 [D loss: 0.006881, acc.: 100.00%] [G loss: 5.221434]\n",
      "epoch:16 step:12690 [D loss: 0.006036, acc.: 100.00%] [G loss: 5.380450]\n",
      "epoch:16 step:12691 [D loss: 0.006614, acc.: 100.00%] [G loss: 5.239279]\n",
      "epoch:16 step:12692 [D loss: 0.006119, acc.: 100.00%] [G loss: 5.051869]\n",
      "epoch:16 step:12693 [D loss: 0.007161, acc.: 100.00%] [G loss: 5.172539]\n",
      "epoch:16 step:12694 [D loss: 0.002830, acc.: 100.00%] [G loss: 4.940590]\n",
      "epoch:16 step:12695 [D loss: 0.002317, acc.: 100.00%] [G loss: 5.244970]\n",
      "epoch:16 step:12696 [D loss: 0.014287, acc.: 100.00%] [G loss: 4.586397]\n",
      "epoch:16 step:12697 [D loss: 0.002768, acc.: 100.00%] [G loss: 5.083432]\n",
      "epoch:16 step:12698 [D loss: 0.006159, acc.: 100.00%] [G loss: 4.173956]\n",
      "epoch:16 step:12699 [D loss: 0.017164, acc.: 100.00%] [G loss: 5.301704]\n",
      "epoch:16 step:12700 [D loss: 0.003937, acc.: 100.00%] [G loss: 5.771336]\n",
      "epoch:16 step:12701 [D loss: 0.011773, acc.: 99.22%] [G loss: 5.035419]\n",
      "epoch:16 step:12702 [D loss: 0.019426, acc.: 100.00%] [G loss: 4.471514]\n",
      "epoch:16 step:12703 [D loss: 0.004519, acc.: 100.00%] [G loss: 5.898838]\n",
      "epoch:16 step:12704 [D loss: 0.003702, acc.: 100.00%] [G loss: 0.722853]\n",
      "epoch:16 step:12705 [D loss: 0.028536, acc.: 100.00%] [G loss: 3.174321]\n",
      "epoch:16 step:12706 [D loss: 0.008268, acc.: 100.00%] [G loss: 6.041948]\n",
      "epoch:16 step:12707 [D loss: 0.007507, acc.: 100.00%] [G loss: 6.453146]\n",
      "epoch:16 step:12708 [D loss: 0.003509, acc.: 100.00%] [G loss: 6.281224]\n",
      "epoch:16 step:12709 [D loss: 0.010619, acc.: 100.00%] [G loss: 6.601906]\n",
      "epoch:16 step:12710 [D loss: 0.199305, acc.: 92.19%] [G loss: 4.470779]\n",
      "epoch:16 step:12711 [D loss: 0.028760, acc.: 99.22%] [G loss: 1.510199]\n",
      "epoch:16 step:12712 [D loss: 0.009129, acc.: 100.00%] [G loss: 0.985665]\n",
      "epoch:16 step:12713 [D loss: 0.019736, acc.: 100.00%] [G loss: 6.624949]\n",
      "epoch:16 step:12714 [D loss: 0.008877, acc.: 100.00%] [G loss: 5.846808]\n",
      "epoch:16 step:12715 [D loss: 0.005671, acc.: 100.00%] [G loss: 6.234933]\n",
      "epoch:16 step:12716 [D loss: 0.005312, acc.: 100.00%] [G loss: 0.669489]\n",
      "epoch:16 step:12717 [D loss: 0.004140, acc.: 100.00%] [G loss: 6.257512]\n",
      "epoch:16 step:12718 [D loss: 0.002868, acc.: 100.00%] [G loss: 5.678342]\n",
      "epoch:16 step:12719 [D loss: 0.002386, acc.: 100.00%] [G loss: 0.176693]\n",
      "epoch:16 step:12720 [D loss: 0.165886, acc.: 91.41%] [G loss: 4.200730]\n",
      "epoch:16 step:12721 [D loss: 0.012883, acc.: 100.00%] [G loss: 9.360947]\n",
      "epoch:16 step:12722 [D loss: 0.102273, acc.: 96.09%] [G loss: 8.048820]\n",
      "epoch:16 step:12723 [D loss: 0.007668, acc.: 100.00%] [G loss: 7.868214]\n",
      "epoch:16 step:12724 [D loss: 0.003086, acc.: 100.00%] [G loss: 7.602762]\n",
      "epoch:16 step:12725 [D loss: 0.002404, acc.: 100.00%] [G loss: 7.011020]\n",
      "epoch:16 step:12726 [D loss: 0.000925, acc.: 100.00%] [G loss: 3.226851]\n",
      "epoch:16 step:12727 [D loss: 0.002658, acc.: 100.00%] [G loss: 7.024110]\n",
      "epoch:16 step:12728 [D loss: 0.007979, acc.: 100.00%] [G loss: 6.488019]\n",
      "epoch:16 step:12729 [D loss: 0.003049, acc.: 100.00%] [G loss: 6.033648]\n",
      "epoch:16 step:12730 [D loss: 0.020237, acc.: 99.22%] [G loss: 2.872461]\n",
      "epoch:16 step:12731 [D loss: 0.006582, acc.: 100.00%] [G loss: 6.637948]\n",
      "epoch:16 step:12732 [D loss: 0.010751, acc.: 100.00%] [G loss: 7.179564]\n",
      "epoch:16 step:12733 [D loss: 0.003035, acc.: 100.00%] [G loss: 6.849524]\n",
      "epoch:16 step:12734 [D loss: 0.006797, acc.: 100.00%] [G loss: 7.163170]\n",
      "epoch:16 step:12735 [D loss: 0.001977, acc.: 100.00%] [G loss: 0.896757]\n",
      "epoch:16 step:12736 [D loss: 0.154978, acc.: 94.53%] [G loss: 2.912267]\n",
      "epoch:16 step:12737 [D loss: 0.010964, acc.: 100.00%] [G loss: 6.665324]\n",
      "epoch:16 step:12738 [D loss: 0.034343, acc.: 99.22%] [G loss: 7.836088]\n",
      "epoch:16 step:12739 [D loss: 0.004756, acc.: 100.00%] [G loss: 4.425022]\n",
      "epoch:16 step:12740 [D loss: 0.007200, acc.: 100.00%] [G loss: 7.484032]\n",
      "epoch:16 step:12741 [D loss: 0.001552, acc.: 100.00%] [G loss: 7.187966]\n",
      "epoch:16 step:12742 [D loss: 0.041282, acc.: 98.44%] [G loss: 4.770659]\n",
      "epoch:16 step:12743 [D loss: 0.018661, acc.: 100.00%] [G loss: 6.226867]\n",
      "epoch:16 step:12744 [D loss: 0.022516, acc.: 100.00%] [G loss: 6.257564]\n",
      "epoch:16 step:12745 [D loss: 0.002854, acc.: 100.00%] [G loss: 3.427577]\n",
      "epoch:16 step:12746 [D loss: 0.031088, acc.: 98.44%] [G loss: 6.162044]\n",
      "epoch:16 step:12747 [D loss: 0.005066, acc.: 100.00%] [G loss: 1.686664]\n",
      "epoch:16 step:12748 [D loss: 0.037665, acc.: 99.22%] [G loss: 7.315563]\n",
      "epoch:16 step:12749 [D loss: 0.001586, acc.: 100.00%] [G loss: 1.698610]\n",
      "epoch:16 step:12750 [D loss: 0.030793, acc.: 99.22%] [G loss: 1.047888]\n",
      "epoch:16 step:12751 [D loss: 0.017903, acc.: 100.00%] [G loss: 3.945447]\n",
      "epoch:16 step:12752 [D loss: 0.114835, acc.: 97.66%] [G loss: 3.790293]\n",
      "epoch:16 step:12753 [D loss: 0.011944, acc.: 100.00%] [G loss: 3.555515]\n",
      "epoch:16 step:12754 [D loss: 0.036553, acc.: 100.00%] [G loss: 6.837120]\n",
      "epoch:16 step:12755 [D loss: 0.004403, acc.: 100.00%] [G loss: 2.307109]\n",
      "epoch:16 step:12756 [D loss: 0.011861, acc.: 100.00%] [G loss: 7.213028]\n",
      "epoch:16 step:12757 [D loss: 0.054159, acc.: 100.00%] [G loss: 7.303614]\n",
      "epoch:16 step:12758 [D loss: 0.001140, acc.: 100.00%] [G loss: 1.625087]\n",
      "epoch:16 step:12759 [D loss: 0.001952, acc.: 100.00%] [G loss: 7.721605]\n",
      "epoch:16 step:12760 [D loss: 0.076807, acc.: 96.88%] [G loss: 0.308854]\n",
      "epoch:16 step:12761 [D loss: 0.864059, acc.: 62.50%] [G loss: 11.192237]\n",
      "epoch:16 step:12762 [D loss: 3.862010, acc.: 50.00%] [G loss: 7.830529]\n",
      "epoch:16 step:12763 [D loss: 0.167485, acc.: 92.97%] [G loss: 4.652260]\n",
      "epoch:16 step:12764 [D loss: 0.000522, acc.: 100.00%] [G loss: 6.544760]\n",
      "epoch:16 step:12765 [D loss: 0.001555, acc.: 100.00%] [G loss: 5.120733]\n",
      "epoch:16 step:12766 [D loss: 0.003635, acc.: 100.00%] [G loss: 5.003901]\n",
      "epoch:16 step:12767 [D loss: 0.002133, acc.: 100.00%] [G loss: 4.739849]\n",
      "epoch:16 step:12768 [D loss: 0.006261, acc.: 100.00%] [G loss: 2.534823]\n",
      "epoch:16 step:12769 [D loss: 0.029330, acc.: 99.22%] [G loss: 3.926147]\n",
      "epoch:16 step:12770 [D loss: 0.077212, acc.: 97.66%] [G loss: 3.036609]\n",
      "epoch:16 step:12771 [D loss: 0.004868, acc.: 100.00%] [G loss: 5.488290]\n",
      "epoch:16 step:12772 [D loss: 0.004892, acc.: 100.00%] [G loss: 4.862203]\n",
      "epoch:16 step:12773 [D loss: 0.042784, acc.: 100.00%] [G loss: 1.596475]\n",
      "epoch:16 step:12774 [D loss: 0.019565, acc.: 100.00%] [G loss: 3.750833]\n",
      "epoch:16 step:12775 [D loss: 0.004060, acc.: 100.00%] [G loss: 4.565692]\n",
      "epoch:16 step:12776 [D loss: 0.155464, acc.: 92.97%] [G loss: 2.338531]\n",
      "epoch:16 step:12777 [D loss: 0.001873, acc.: 100.00%] [G loss: 4.316554]\n",
      "epoch:16 step:12778 [D loss: 0.023734, acc.: 100.00%] [G loss: 2.276434]\n",
      "epoch:16 step:12779 [D loss: 0.003591, acc.: 100.00%] [G loss: 1.488837]\n",
      "epoch:16 step:12780 [D loss: 0.002735, acc.: 100.00%] [G loss: 1.201592]\n",
      "epoch:16 step:12781 [D loss: 0.003760, acc.: 100.00%] [G loss: 6.628169]\n",
      "epoch:16 step:12782 [D loss: 0.064005, acc.: 98.44%] [G loss: 5.935636]\n",
      "epoch:16 step:12783 [D loss: 0.006984, acc.: 100.00%] [G loss: 0.123031]\n",
      "epoch:16 step:12784 [D loss: 0.001314, acc.: 100.00%] [G loss: 5.325246]\n",
      "epoch:16 step:12785 [D loss: 0.002794, acc.: 100.00%] [G loss: 0.061326]\n",
      "epoch:16 step:12786 [D loss: 0.001957, acc.: 100.00%] [G loss: 0.008573]\n",
      "epoch:16 step:12787 [D loss: 0.079398, acc.: 97.66%] [G loss: 5.666799]\n",
      "epoch:16 step:12788 [D loss: 0.010812, acc.: 100.00%] [G loss: 6.898098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12789 [D loss: 0.002756, acc.: 100.00%] [G loss: 6.654171]\n",
      "epoch:16 step:12790 [D loss: 0.006339, acc.: 100.00%] [G loss: 0.570214]\n",
      "epoch:16 step:12791 [D loss: 0.141539, acc.: 94.53%] [G loss: 0.002909]\n",
      "epoch:16 step:12792 [D loss: 0.033787, acc.: 100.00%] [G loss: 0.032336]\n",
      "epoch:16 step:12793 [D loss: 0.005641, acc.: 100.00%] [G loss: 0.005451]\n",
      "epoch:16 step:12794 [D loss: 0.020625, acc.: 100.00%] [G loss: 0.060098]\n",
      "epoch:16 step:12795 [D loss: 0.009327, acc.: 100.00%] [G loss: 0.066240]\n",
      "epoch:16 step:12796 [D loss: 0.002096, acc.: 100.00%] [G loss: 0.360035]\n",
      "epoch:16 step:12797 [D loss: 0.015409, acc.: 100.00%] [G loss: 0.096013]\n",
      "epoch:16 step:12798 [D loss: 0.024752, acc.: 100.00%] [G loss: 7.465836]\n",
      "epoch:16 step:12799 [D loss: 0.001880, acc.: 100.00%] [G loss: 7.422082]\n",
      "epoch:16 step:12800 [D loss: 0.001514, acc.: 100.00%] [G loss: 7.301879]\n",
      "epoch:16 step:12801 [D loss: 0.001385, acc.: 100.00%] [G loss: 6.944517]\n",
      "epoch:16 step:12802 [D loss: 0.005408, acc.: 100.00%] [G loss: 6.611653]\n",
      "epoch:16 step:12803 [D loss: 0.002985, acc.: 100.00%] [G loss: 0.898913]\n",
      "epoch:16 step:12804 [D loss: 0.001121, acc.: 100.00%] [G loss: 0.488553]\n",
      "epoch:16 step:12805 [D loss: 0.007129, acc.: 100.00%] [G loss: 6.364849]\n",
      "epoch:16 step:12806 [D loss: 0.008220, acc.: 100.00%] [G loss: 0.057298]\n",
      "epoch:16 step:12807 [D loss: 0.005014, acc.: 100.00%] [G loss: 0.067686]\n",
      "epoch:16 step:12808 [D loss: 0.007366, acc.: 100.00%] [G loss: 0.001772]\n",
      "epoch:16 step:12809 [D loss: 0.002671, acc.: 100.00%] [G loss: 0.018750]\n",
      "epoch:16 step:12810 [D loss: 0.001008, acc.: 100.00%] [G loss: 0.020138]\n",
      "epoch:16 step:12811 [D loss: 0.044102, acc.: 100.00%] [G loss: 0.013703]\n",
      "epoch:16 step:12812 [D loss: 0.003903, acc.: 100.00%] [G loss: 6.546534]\n",
      "epoch:16 step:12813 [D loss: 0.000811, acc.: 100.00%] [G loss: 0.105517]\n",
      "epoch:16 step:12814 [D loss: 0.001702, acc.: 100.00%] [G loss: 0.070760]\n",
      "epoch:16 step:12815 [D loss: 0.000741, acc.: 100.00%] [G loss: 7.017194]\n",
      "epoch:16 step:12816 [D loss: 0.199959, acc.: 92.97%] [G loss: 5.613879]\n",
      "epoch:16 step:12817 [D loss: 0.098082, acc.: 96.88%] [G loss: 0.025790]\n",
      "epoch:16 step:12818 [D loss: 0.000867, acc.: 100.00%] [G loss: 6.863052]\n",
      "epoch:16 step:12819 [D loss: 0.000536, acc.: 100.00%] [G loss: 7.182213]\n",
      "epoch:16 step:12820 [D loss: 0.003887, acc.: 100.00%] [G loss: 0.596466]\n",
      "epoch:16 step:12821 [D loss: 0.007876, acc.: 100.00%] [G loss: 7.120575]\n",
      "epoch:16 step:12822 [D loss: 0.001072, acc.: 100.00%] [G loss: 7.221472]\n",
      "epoch:16 step:12823 [D loss: 0.001930, acc.: 100.00%] [G loss: 6.094365]\n",
      "epoch:16 step:12824 [D loss: 0.001052, acc.: 100.00%] [G loss: 0.105360]\n",
      "epoch:16 step:12825 [D loss: 0.005388, acc.: 100.00%] [G loss: 0.133235]\n",
      "epoch:16 step:12826 [D loss: 0.001338, acc.: 100.00%] [G loss: 4.229198]\n",
      "epoch:16 step:12827 [D loss: 0.135159, acc.: 95.31%] [G loss: 2.710935]\n",
      "epoch:16 step:12828 [D loss: 0.018400, acc.: 99.22%] [G loss: 8.038332]\n",
      "epoch:16 step:12829 [D loss: 0.145437, acc.: 94.53%] [G loss: 7.267461]\n",
      "epoch:16 step:12830 [D loss: 0.032402, acc.: 99.22%] [G loss: 0.085016]\n",
      "epoch:16 step:12831 [D loss: 0.002490, acc.: 100.00%] [G loss: 0.238415]\n",
      "epoch:16 step:12832 [D loss: 0.004928, acc.: 100.00%] [G loss: 0.036835]\n",
      "epoch:16 step:12833 [D loss: 0.002450, acc.: 100.00%] [G loss: 0.045570]\n",
      "epoch:16 step:12834 [D loss: 0.022300, acc.: 99.22%] [G loss: 0.001225]\n",
      "epoch:16 step:12835 [D loss: 0.682187, acc.: 65.62%] [G loss: 10.497034]\n",
      "epoch:16 step:12836 [D loss: 4.714843, acc.: 50.00%] [G loss: 7.906558]\n",
      "epoch:16 step:12837 [D loss: 3.404822, acc.: 50.00%] [G loss: 5.548782]\n",
      "epoch:16 step:12838 [D loss: 2.101470, acc.: 50.00%] [G loss: 2.815840]\n",
      "epoch:16 step:12839 [D loss: 1.002581, acc.: 53.12%] [G loss: 2.329859]\n",
      "epoch:16 step:12840 [D loss: 0.389305, acc.: 85.94%] [G loss: 2.042062]\n",
      "epoch:16 step:12841 [D loss: 0.240015, acc.: 96.88%] [G loss: 0.979046]\n",
      "epoch:16 step:12842 [D loss: 0.272458, acc.: 91.41%] [G loss: 1.995800]\n",
      "epoch:16 step:12843 [D loss: 0.120042, acc.: 100.00%] [G loss: 2.254078]\n",
      "epoch:16 step:12844 [D loss: 0.105259, acc.: 100.00%] [G loss: 2.762596]\n",
      "epoch:16 step:12845 [D loss: 0.100861, acc.: 98.44%] [G loss: 2.611147]\n",
      "epoch:16 step:12846 [D loss: 0.160277, acc.: 99.22%] [G loss: 0.684291]\n",
      "epoch:16 step:12847 [D loss: 0.095669, acc.: 100.00%] [G loss: 0.969238]\n",
      "epoch:16 step:12848 [D loss: 0.051540, acc.: 100.00%] [G loss: 3.167471]\n",
      "epoch:16 step:12849 [D loss: 0.090538, acc.: 99.22%] [G loss: 1.004759]\n",
      "epoch:16 step:12850 [D loss: 0.103349, acc.: 99.22%] [G loss: 2.886009]\n",
      "epoch:16 step:12851 [D loss: 0.097266, acc.: 100.00%] [G loss: 0.709984]\n",
      "epoch:16 step:12852 [D loss: 0.032047, acc.: 100.00%] [G loss: 2.115022]\n",
      "epoch:16 step:12853 [D loss: 0.048208, acc.: 100.00%] [G loss: 0.874754]\n",
      "epoch:16 step:12854 [D loss: 0.033962, acc.: 100.00%] [G loss: 0.531621]\n",
      "epoch:16 step:12855 [D loss: 0.064430, acc.: 100.00%] [G loss: 0.402516]\n",
      "epoch:16 step:12856 [D loss: 0.052924, acc.: 99.22%] [G loss: 0.674844]\n",
      "epoch:16 step:12857 [D loss: 0.123892, acc.: 96.88%] [G loss: 1.146771]\n",
      "epoch:16 step:12858 [D loss: 0.016711, acc.: 100.00%] [G loss: 4.082260]\n",
      "epoch:16 step:12859 [D loss: 0.082333, acc.: 99.22%] [G loss: 1.071145]\n",
      "epoch:16 step:12860 [D loss: 0.043970, acc.: 100.00%] [G loss: 0.413363]\n",
      "epoch:16 step:12861 [D loss: 0.073843, acc.: 98.44%] [G loss: 3.700463]\n",
      "epoch:16 step:12862 [D loss: 0.021538, acc.: 100.00%] [G loss: 1.073284]\n",
      "epoch:16 step:12863 [D loss: 0.037306, acc.: 100.00%] [G loss: 3.250997]\n",
      "epoch:16 step:12864 [D loss: 0.041985, acc.: 100.00%] [G loss: 1.269255]\n",
      "epoch:16 step:12865 [D loss: 0.042031, acc.: 100.00%] [G loss: 3.275834]\n",
      "epoch:16 step:12866 [D loss: 0.018158, acc.: 100.00%] [G loss: 1.899538]\n",
      "epoch:16 step:12867 [D loss: 0.154242, acc.: 97.66%] [G loss: 3.084067]\n",
      "epoch:16 step:12868 [D loss: 0.060538, acc.: 98.44%] [G loss: 2.224828]\n",
      "epoch:16 step:12869 [D loss: 0.164444, acc.: 91.41%] [G loss: 1.725630]\n",
      "epoch:16 step:12870 [D loss: 0.024238, acc.: 99.22%] [G loss: 0.028014]\n",
      "epoch:16 step:12871 [D loss: 0.006285, acc.: 100.00%] [G loss: 0.001184]\n",
      "epoch:16 step:12872 [D loss: 0.031579, acc.: 100.00%] [G loss: 0.037106]\n",
      "epoch:16 step:12873 [D loss: 0.021505, acc.: 100.00%] [G loss: 0.181064]\n",
      "epoch:16 step:12874 [D loss: 0.036576, acc.: 100.00%] [G loss: 0.266575]\n",
      "epoch:16 step:12875 [D loss: 0.003419, acc.: 100.00%] [G loss: 0.279687]\n",
      "epoch:16 step:12876 [D loss: 0.015672, acc.: 100.00%] [G loss: 0.184574]\n",
      "epoch:16 step:12877 [D loss: 0.005224, acc.: 100.00%] [G loss: 0.263356]\n",
      "epoch:16 step:12878 [D loss: 0.008017, acc.: 100.00%] [G loss: 0.113264]\n",
      "epoch:16 step:12879 [D loss: 0.002999, acc.: 100.00%] [G loss: 0.134136]\n",
      "epoch:16 step:12880 [D loss: 0.021014, acc.: 100.00%] [G loss: 0.474565]\n",
      "epoch:16 step:12881 [D loss: 0.010608, acc.: 100.00%] [G loss: 0.162135]\n",
      "epoch:16 step:12882 [D loss: 0.099163, acc.: 98.44%] [G loss: 0.353684]\n",
      "epoch:16 step:12883 [D loss: 0.010624, acc.: 100.00%] [G loss: 2.249270]\n",
      "epoch:16 step:12884 [D loss: 4.296877, acc.: 29.69%] [G loss: 5.042501]\n",
      "epoch:16 step:12885 [D loss: 1.016934, acc.: 57.03%] [G loss: 3.565930]\n",
      "epoch:16 step:12886 [D loss: 0.246488, acc.: 83.59%] [G loss: 1.689335]\n",
      "epoch:16 step:12887 [D loss: 0.086020, acc.: 97.66%] [G loss: 1.975078]\n",
      "epoch:16 step:12888 [D loss: 0.058806, acc.: 98.44%] [G loss: 1.303316]\n",
      "epoch:16 step:12889 [D loss: 0.132273, acc.: 96.09%] [G loss: 1.365781]\n",
      "epoch:16 step:12890 [D loss: 0.041881, acc.: 99.22%] [G loss: 2.072850]\n",
      "epoch:16 step:12891 [D loss: 0.079691, acc.: 98.44%] [G loss: 0.759937]\n",
      "epoch:16 step:12892 [D loss: 0.028802, acc.: 100.00%] [G loss: 0.819531]\n",
      "epoch:16 step:12893 [D loss: 0.027615, acc.: 100.00%] [G loss: 0.434949]\n",
      "epoch:16 step:12894 [D loss: 0.459619, acc.: 82.03%] [G loss: 3.756868]\n",
      "epoch:16 step:12895 [D loss: 0.188221, acc.: 92.19%] [G loss: 3.508494]\n",
      "epoch:16 step:12896 [D loss: 0.182119, acc.: 90.62%] [G loss: 2.776503]\n",
      "epoch:16 step:12897 [D loss: 0.218661, acc.: 91.41%] [G loss: 2.513648]\n",
      "epoch:16 step:12898 [D loss: 0.037899, acc.: 100.00%] [G loss: 2.129975]\n",
      "epoch:16 step:12899 [D loss: 0.073073, acc.: 99.22%] [G loss: 0.525245]\n",
      "epoch:16 step:12900 [D loss: 0.070741, acc.: 98.44%] [G loss: 1.441992]\n",
      "epoch:16 step:12901 [D loss: 0.248579, acc.: 90.62%] [G loss: 2.207539]\n",
      "epoch:16 step:12902 [D loss: 0.088279, acc.: 99.22%] [G loss: 1.274987]\n",
      "epoch:16 step:12903 [D loss: 0.199068, acc.: 92.19%] [G loss: 0.979874]\n",
      "epoch:16 step:12904 [D loss: 0.070467, acc.: 98.44%] [G loss: 3.817653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:12905 [D loss: 0.379729, acc.: 84.38%] [G loss: 2.283893]\n",
      "epoch:16 step:12906 [D loss: 0.136872, acc.: 93.75%] [G loss: 3.099436]\n",
      "epoch:16 step:12907 [D loss: 0.306258, acc.: 85.94%] [G loss: 1.747810]\n",
      "epoch:16 step:12908 [D loss: 0.294828, acc.: 82.81%] [G loss: 3.743594]\n",
      "epoch:16 step:12909 [D loss: 0.083874, acc.: 96.09%] [G loss: 4.058501]\n",
      "epoch:16 step:12910 [D loss: 0.172688, acc.: 93.75%] [G loss: 2.833176]\n",
      "epoch:16 step:12911 [D loss: 0.032639, acc.: 100.00%] [G loss: 3.314507]\n",
      "epoch:16 step:12912 [D loss: 0.051291, acc.: 100.00%] [G loss: 3.312160]\n",
      "epoch:16 step:12913 [D loss: 0.057740, acc.: 100.00%] [G loss: 3.259212]\n",
      "epoch:16 step:12914 [D loss: 0.020691, acc.: 100.00%] [G loss: 3.351011]\n",
      "epoch:16 step:12915 [D loss: 0.029565, acc.: 99.22%] [G loss: 3.267872]\n",
      "epoch:16 step:12916 [D loss: 0.079124, acc.: 98.44%] [G loss: 1.337465]\n",
      "epoch:16 step:12917 [D loss: 0.082626, acc.: 98.44%] [G loss: 3.679132]\n",
      "epoch:16 step:12918 [D loss: 0.032399, acc.: 100.00%] [G loss: 4.145595]\n",
      "epoch:16 step:12919 [D loss: 0.018562, acc.: 100.00%] [G loss: 4.315719]\n",
      "epoch:16 step:12920 [D loss: 0.116717, acc.: 96.09%] [G loss: 0.901522]\n",
      "epoch:16 step:12921 [D loss: 0.064254, acc.: 99.22%] [G loss: 2.216163]\n",
      "epoch:16 step:12922 [D loss: 0.051007, acc.: 99.22%] [G loss: 4.842464]\n",
      "epoch:16 step:12923 [D loss: 0.038878, acc.: 99.22%] [G loss: 5.207171]\n",
      "epoch:16 step:12924 [D loss: 0.105194, acc.: 96.88%] [G loss: 3.877063]\n",
      "epoch:16 step:12925 [D loss: 0.076033, acc.: 96.88%] [G loss: 3.538621]\n",
      "epoch:16 step:12926 [D loss: 0.015021, acc.: 100.00%] [G loss: 1.245981]\n",
      "epoch:16 step:12927 [D loss: 0.222280, acc.: 94.53%] [G loss: 3.538675]\n",
      "epoch:16 step:12928 [D loss: 0.101434, acc.: 98.44%] [G loss: 0.282776]\n",
      "epoch:16 step:12929 [D loss: 0.038425, acc.: 100.00%] [G loss: 0.529724]\n",
      "epoch:16 step:12930 [D loss: 0.035738, acc.: 100.00%] [G loss: 4.931996]\n",
      "epoch:16 step:12931 [D loss: 0.067449, acc.: 100.00%] [G loss: 4.938484]\n",
      "epoch:16 step:12932 [D loss: 0.058083, acc.: 100.00%] [G loss: 5.039708]\n",
      "epoch:16 step:12933 [D loss: 0.100335, acc.: 96.88%] [G loss: 0.156531]\n",
      "epoch:16 step:12934 [D loss: 0.120318, acc.: 96.09%] [G loss: 4.729276]\n",
      "epoch:16 step:12935 [D loss: 0.013968, acc.: 100.00%] [G loss: 5.868937]\n",
      "epoch:16 step:12936 [D loss: 0.106884, acc.: 94.53%] [G loss: 1.399550]\n",
      "epoch:16 step:12937 [D loss: 0.181516, acc.: 91.41%] [G loss: 2.671509]\n",
      "epoch:16 step:12938 [D loss: 0.022690, acc.: 100.00%] [G loss: 2.959734]\n",
      "epoch:16 step:12939 [D loss: 0.462611, acc.: 80.47%] [G loss: 1.457900]\n",
      "epoch:16 step:12940 [D loss: 0.044234, acc.: 98.44%] [G loss: 2.995937]\n",
      "epoch:16 step:12941 [D loss: 0.038954, acc.: 99.22%] [G loss: 1.771919]\n",
      "epoch:16 step:12942 [D loss: 0.084119, acc.: 96.88%] [G loss: 4.253054]\n",
      "epoch:16 step:12943 [D loss: 0.065420, acc.: 99.22%] [G loss: 5.248609]\n",
      "epoch:16 step:12944 [D loss: 0.291141, acc.: 87.50%] [G loss: 1.852668]\n",
      "epoch:16 step:12945 [D loss: 0.049061, acc.: 100.00%] [G loss: 4.393968]\n",
      "epoch:16 step:12946 [D loss: 0.142546, acc.: 95.31%] [G loss: 1.776552]\n",
      "epoch:16 step:12947 [D loss: 0.094567, acc.: 98.44%] [G loss: 3.670851]\n",
      "epoch:16 step:12948 [D loss: 0.023344, acc.: 100.00%] [G loss: 5.084435]\n",
      "epoch:16 step:12949 [D loss: 0.085806, acc.: 99.22%] [G loss: 0.595046]\n",
      "epoch:16 step:12950 [D loss: 0.015840, acc.: 100.00%] [G loss: 1.849893]\n",
      "epoch:16 step:12951 [D loss: 0.112566, acc.: 98.44%] [G loss: 4.301177]\n",
      "epoch:16 step:12952 [D loss: 0.029039, acc.: 100.00%] [G loss: 5.681890]\n",
      "epoch:16 step:12953 [D loss: 0.027687, acc.: 100.00%] [G loss: 3.378408]\n",
      "epoch:16 step:12954 [D loss: 0.171524, acc.: 93.75%] [G loss: 1.471700]\n",
      "epoch:16 step:12955 [D loss: 0.778917, acc.: 67.19%] [G loss: 0.049787]\n",
      "epoch:16 step:12956 [D loss: 1.991619, acc.: 53.91%] [G loss: 9.154088]\n",
      "epoch:16 step:12957 [D loss: 1.702084, acc.: 51.56%] [G loss: 3.051269]\n",
      "epoch:16 step:12958 [D loss: 0.605216, acc.: 70.31%] [G loss: 5.842636]\n",
      "epoch:16 step:12959 [D loss: 0.044608, acc.: 97.66%] [G loss: 0.044879]\n",
      "epoch:16 step:12960 [D loss: 0.012838, acc.: 100.00%] [G loss: 0.019968]\n",
      "epoch:16 step:12961 [D loss: 0.014701, acc.: 100.00%] [G loss: 4.464403]\n",
      "epoch:16 step:12962 [D loss: 0.005800, acc.: 100.00%] [G loss: 0.028736]\n",
      "epoch:16 step:12963 [D loss: 0.009402, acc.: 100.00%] [G loss: 0.050380]\n",
      "epoch:16 step:12964 [D loss: 0.004620, acc.: 100.00%] [G loss: 0.313379]\n",
      "epoch:16 step:12965 [D loss: 0.008714, acc.: 100.00%] [G loss: 4.126204]\n",
      "epoch:16 step:12966 [D loss: 0.007655, acc.: 100.00%] [G loss: 3.626682]\n",
      "epoch:16 step:12967 [D loss: 0.009641, acc.: 100.00%] [G loss: 2.815581]\n",
      "epoch:16 step:12968 [D loss: 0.014798, acc.: 100.00%] [G loss: 2.653644]\n",
      "epoch:16 step:12969 [D loss: 0.033063, acc.: 100.00%] [G loss: 0.020821]\n",
      "epoch:16 step:12970 [D loss: 0.025468, acc.: 100.00%] [G loss: 2.321433]\n",
      "epoch:16 step:12971 [D loss: 0.044824, acc.: 100.00%] [G loss: 0.024368]\n",
      "epoch:16 step:12972 [D loss: 0.040954, acc.: 100.00%] [G loss: 3.310638]\n",
      "epoch:16 step:12973 [D loss: 0.017305, acc.: 100.00%] [G loss: 0.050781]\n",
      "epoch:16 step:12974 [D loss: 0.031103, acc.: 100.00%] [G loss: 0.092724]\n",
      "epoch:16 step:12975 [D loss: 0.021593, acc.: 100.00%] [G loss: 0.063249]\n",
      "epoch:16 step:12976 [D loss: 0.016403, acc.: 100.00%] [G loss: 0.069855]\n",
      "epoch:16 step:12977 [D loss: 0.052094, acc.: 99.22%] [G loss: 0.330167]\n",
      "epoch:16 step:12978 [D loss: 0.037692, acc.: 100.00%] [G loss: 0.009442]\n",
      "epoch:16 step:12979 [D loss: 0.021518, acc.: 100.00%] [G loss: 1.656906]\n",
      "epoch:16 step:12980 [D loss: 0.011443, acc.: 100.00%] [G loss: 2.588997]\n",
      "epoch:16 step:12981 [D loss: 0.042429, acc.: 100.00%] [G loss: 0.022712]\n",
      "epoch:16 step:12982 [D loss: 0.050606, acc.: 99.22%] [G loss: 0.003013]\n",
      "epoch:16 step:12983 [D loss: 0.059080, acc.: 99.22%] [G loss: 1.801767]\n",
      "epoch:16 step:12984 [D loss: 0.131881, acc.: 94.53%] [G loss: 3.645555]\n",
      "epoch:16 step:12985 [D loss: 0.029458, acc.: 100.00%] [G loss: 3.626051]\n",
      "epoch:16 step:12986 [D loss: 0.379765, acc.: 85.16%] [G loss: 4.628571]\n",
      "epoch:16 step:12987 [D loss: 0.219771, acc.: 86.72%] [G loss: 1.856275]\n",
      "epoch:16 step:12988 [D loss: 0.134550, acc.: 94.53%] [G loss: 0.100712]\n",
      "epoch:16 step:12989 [D loss: 0.011270, acc.: 100.00%] [G loss: 0.129469]\n",
      "epoch:16 step:12990 [D loss: 0.008215, acc.: 100.00%] [G loss: 3.389741]\n",
      "epoch:16 step:12991 [D loss: 0.030527, acc.: 99.22%] [G loss: 0.074293]\n",
      "epoch:16 step:12992 [D loss: 0.004220, acc.: 100.00%] [G loss: 3.338101]\n",
      "epoch:16 step:12993 [D loss: 0.018967, acc.: 100.00%] [G loss: 1.623629]\n",
      "epoch:16 step:12994 [D loss: 0.047964, acc.: 98.44%] [G loss: 0.008194]\n",
      "epoch:16 step:12995 [D loss: 0.021709, acc.: 100.00%] [G loss: 0.320146]\n",
      "epoch:16 step:12996 [D loss: 0.139319, acc.: 93.75%] [G loss: 0.378118]\n",
      "epoch:16 step:12997 [D loss: 0.007907, acc.: 100.00%] [G loss: 0.155704]\n",
      "epoch:16 step:12998 [D loss: 0.094472, acc.: 93.75%] [G loss: 0.032164]\n",
      "epoch:16 step:12999 [D loss: 0.001344, acc.: 100.00%] [G loss: 0.004753]\n",
      "epoch:16 step:13000 [D loss: 0.089020, acc.: 97.66%] [G loss: 1.290014]\n",
      "epoch:16 step:13001 [D loss: 0.017032, acc.: 100.00%] [G loss: 0.076489]\n",
      "epoch:16 step:13002 [D loss: 0.049620, acc.: 99.22%] [G loss: 1.447654]\n",
      "epoch:16 step:13003 [D loss: 0.015512, acc.: 100.00%] [G loss: 0.729826]\n",
      "epoch:16 step:13004 [D loss: 0.103310, acc.: 96.88%] [G loss: 3.944584]\n",
      "epoch:16 step:13005 [D loss: 0.218185, acc.: 89.84%] [G loss: 0.001403]\n",
      "epoch:16 step:13006 [D loss: 0.362802, acc.: 86.72%] [G loss: 0.381110]\n",
      "epoch:16 step:13007 [D loss: 0.436474, acc.: 78.91%] [G loss: 4.091419]\n",
      "epoch:16 step:13008 [D loss: 0.012289, acc.: 100.00%] [G loss: 2.483377]\n",
      "epoch:16 step:13009 [D loss: 0.035183, acc.: 97.66%] [G loss: 0.275006]\n",
      "epoch:16 step:13010 [D loss: 0.114921, acc.: 94.53%] [G loss: 0.065721]\n",
      "epoch:16 step:13011 [D loss: 0.006006, acc.: 100.00%] [G loss: 0.508341]\n",
      "epoch:16 step:13012 [D loss: 0.025430, acc.: 99.22%] [G loss: 0.186089]\n",
      "epoch:16 step:13013 [D loss: 0.036699, acc.: 100.00%] [G loss: 0.911125]\n",
      "epoch:16 step:13014 [D loss: 0.006059, acc.: 100.00%] [G loss: 0.017778]\n",
      "epoch:16 step:13015 [D loss: 0.011288, acc.: 100.00%] [G loss: 0.027650]\n",
      "epoch:16 step:13016 [D loss: 0.031740, acc.: 100.00%] [G loss: 0.027499]\n",
      "epoch:16 step:13017 [D loss: 0.008481, acc.: 100.00%] [G loss: 0.028807]\n",
      "epoch:16 step:13018 [D loss: 0.012823, acc.: 100.00%] [G loss: 0.038140]\n",
      "epoch:16 step:13019 [D loss: 0.302456, acc.: 89.06%] [G loss: 3.443360]\n",
      "epoch:16 step:13020 [D loss: 0.506365, acc.: 73.44%] [G loss: 3.739764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13021 [D loss: 0.106288, acc.: 95.31%] [G loss: 4.313286]\n",
      "epoch:16 step:13022 [D loss: 0.065094, acc.: 98.44%] [G loss: 4.332070]\n",
      "epoch:16 step:13023 [D loss: 0.012215, acc.: 100.00%] [G loss: 2.090804]\n",
      "epoch:16 step:13024 [D loss: 0.031416, acc.: 99.22%] [G loss: 1.580179]\n",
      "epoch:16 step:13025 [D loss: 0.027199, acc.: 99.22%] [G loss: 1.560404]\n",
      "epoch:16 step:13026 [D loss: 0.023463, acc.: 99.22%] [G loss: 0.848834]\n",
      "epoch:16 step:13027 [D loss: 0.042745, acc.: 100.00%] [G loss: 0.675401]\n",
      "epoch:16 step:13028 [D loss: 0.051080, acc.: 99.22%] [G loss: 0.427091]\n",
      "epoch:16 step:13029 [D loss: 0.048746, acc.: 99.22%] [G loss: 0.705060]\n",
      "epoch:16 step:13030 [D loss: 0.034905, acc.: 99.22%] [G loss: 0.386337]\n",
      "epoch:16 step:13031 [D loss: 0.053539, acc.: 99.22%] [G loss: 2.087100]\n",
      "epoch:16 step:13032 [D loss: 0.062168, acc.: 98.44%] [G loss: 1.984285]\n",
      "epoch:16 step:13033 [D loss: 0.335676, acc.: 85.94%] [G loss: 2.554669]\n",
      "epoch:16 step:13034 [D loss: 0.031822, acc.: 100.00%] [G loss: 3.504608]\n",
      "epoch:16 step:13035 [D loss: 0.035392, acc.: 99.22%] [G loss: 3.768132]\n",
      "epoch:16 step:13036 [D loss: 0.007551, acc.: 100.00%] [G loss: 3.917529]\n",
      "epoch:16 step:13037 [D loss: 0.005915, acc.: 100.00%] [G loss: 4.183049]\n",
      "epoch:16 step:13038 [D loss: 0.046383, acc.: 98.44%] [G loss: 3.102219]\n",
      "epoch:16 step:13039 [D loss: 0.022339, acc.: 100.00%] [G loss: 3.125797]\n",
      "epoch:16 step:13040 [D loss: 0.007898, acc.: 100.00%] [G loss: 3.395777]\n",
      "epoch:16 step:13041 [D loss: 0.048745, acc.: 99.22%] [G loss: 4.518767]\n",
      "epoch:16 step:13042 [D loss: 0.010957, acc.: 100.00%] [G loss: 5.041975]\n",
      "epoch:16 step:13043 [D loss: 0.149551, acc.: 92.97%] [G loss: 3.420006]\n",
      "epoch:16 step:13044 [D loss: 0.189111, acc.: 92.19%] [G loss: 5.903037]\n",
      "epoch:16 step:13045 [D loss: 0.017721, acc.: 100.00%] [G loss: 5.294925]\n",
      "epoch:16 step:13046 [D loss: 0.233525, acc.: 90.62%] [G loss: 4.918410]\n",
      "epoch:16 step:13047 [D loss: 0.043248, acc.: 99.22%] [G loss: 4.606990]\n",
      "epoch:16 step:13048 [D loss: 0.011549, acc.: 100.00%] [G loss: 4.356987]\n",
      "epoch:16 step:13049 [D loss: 0.006332, acc.: 100.00%] [G loss: 3.791389]\n",
      "epoch:16 step:13050 [D loss: 0.005518, acc.: 100.00%] [G loss: 4.763052]\n",
      "epoch:16 step:13051 [D loss: 0.009860, acc.: 100.00%] [G loss: 5.253081]\n",
      "epoch:16 step:13052 [D loss: 0.022681, acc.: 100.00%] [G loss: 2.983611]\n",
      "epoch:16 step:13053 [D loss: 0.017360, acc.: 100.00%] [G loss: 2.931055]\n",
      "epoch:16 step:13054 [D loss: 0.025167, acc.: 100.00%] [G loss: 5.299047]\n",
      "epoch:16 step:13055 [D loss: 0.013868, acc.: 100.00%] [G loss: 5.046865]\n",
      "epoch:16 step:13056 [D loss: 0.011054, acc.: 100.00%] [G loss: 4.298637]\n",
      "epoch:16 step:13057 [D loss: 0.018512, acc.: 100.00%] [G loss: 4.604525]\n",
      "epoch:16 step:13058 [D loss: 0.038176, acc.: 99.22%] [G loss: 5.087414]\n",
      "epoch:16 step:13059 [D loss: 0.032864, acc.: 99.22%] [G loss: 3.290538]\n",
      "epoch:16 step:13060 [D loss: 0.004290, acc.: 100.00%] [G loss: 2.960298]\n",
      "epoch:16 step:13061 [D loss: 0.022765, acc.: 100.00%] [G loss: 2.310682]\n",
      "epoch:16 step:13062 [D loss: 0.133801, acc.: 95.31%] [G loss: 3.967755]\n",
      "epoch:16 step:13063 [D loss: 0.009315, acc.: 100.00%] [G loss: 3.673172]\n",
      "epoch:16 step:13064 [D loss: 0.006881, acc.: 100.00%] [G loss: 4.877693]\n",
      "epoch:16 step:13065 [D loss: 0.017788, acc.: 100.00%] [G loss: 3.557067]\n",
      "epoch:16 step:13066 [D loss: 0.014845, acc.: 100.00%] [G loss: 2.072839]\n",
      "epoch:16 step:13067 [D loss: 0.009085, acc.: 100.00%] [G loss: 3.176053]\n",
      "epoch:16 step:13068 [D loss: 0.024127, acc.: 100.00%] [G loss: 2.292593]\n",
      "epoch:16 step:13069 [D loss: 0.025842, acc.: 99.22%] [G loss: 1.697476]\n",
      "epoch:16 step:13070 [D loss: 0.010884, acc.: 100.00%] [G loss: 3.659722]\n",
      "epoch:16 step:13071 [D loss: 0.005344, acc.: 100.00%] [G loss: 3.474861]\n",
      "epoch:16 step:13072 [D loss: 0.043911, acc.: 98.44%] [G loss: 1.754434]\n",
      "epoch:16 step:13073 [D loss: 0.085306, acc.: 98.44%] [G loss: 1.442737]\n",
      "epoch:16 step:13074 [D loss: 0.055677, acc.: 98.44%] [G loss: 2.226210]\n",
      "epoch:16 step:13075 [D loss: 0.003859, acc.: 100.00%] [G loss: 3.987298]\n",
      "epoch:16 step:13076 [D loss: 0.453148, acc.: 81.25%] [G loss: 4.450169]\n",
      "epoch:16 step:13077 [D loss: 0.028704, acc.: 99.22%] [G loss: 7.106091]\n",
      "epoch:16 step:13078 [D loss: 0.034125, acc.: 99.22%] [G loss: 5.805953]\n",
      "epoch:16 step:13079 [D loss: 0.007761, acc.: 100.00%] [G loss: 4.547871]\n",
      "epoch:16 step:13080 [D loss: 0.001182, acc.: 100.00%] [G loss: 5.776427]\n",
      "epoch:16 step:13081 [D loss: 0.004181, acc.: 100.00%] [G loss: 5.424319]\n",
      "epoch:16 step:13082 [D loss: 0.008152, acc.: 99.22%] [G loss: 4.471889]\n",
      "epoch:16 step:13083 [D loss: 0.001758, acc.: 100.00%] [G loss: 4.514560]\n",
      "epoch:16 step:13084 [D loss: 0.005639, acc.: 100.00%] [G loss: 4.172493]\n",
      "epoch:16 step:13085 [D loss: 0.003630, acc.: 100.00%] [G loss: 4.568862]\n",
      "epoch:16 step:13086 [D loss: 0.004710, acc.: 100.00%] [G loss: 1.868314]\n",
      "epoch:16 step:13087 [D loss: 0.027391, acc.: 99.22%] [G loss: 3.789986]\n",
      "epoch:16 step:13088 [D loss: 0.002009, acc.: 100.00%] [G loss: 4.030580]\n",
      "epoch:16 step:13089 [D loss: 0.016904, acc.: 100.00%] [G loss: 2.436488]\n",
      "epoch:16 step:13090 [D loss: 0.006316, acc.: 100.00%] [G loss: 3.351391]\n",
      "epoch:16 step:13091 [D loss: 0.002230, acc.: 100.00%] [G loss: 3.482762]\n",
      "epoch:16 step:13092 [D loss: 0.036260, acc.: 100.00%] [G loss: 1.293777]\n",
      "epoch:16 step:13093 [D loss: 0.005264, acc.: 100.00%] [G loss: 5.599287]\n",
      "epoch:16 step:13094 [D loss: 0.003936, acc.: 100.00%] [G loss: 5.806916]\n",
      "epoch:16 step:13095 [D loss: 0.040190, acc.: 99.22%] [G loss: 4.888814]\n",
      "epoch:16 step:13096 [D loss: 0.030539, acc.: 100.00%] [G loss: 5.255153]\n",
      "epoch:16 step:13097 [D loss: 0.003062, acc.: 100.00%] [G loss: 5.004387]\n",
      "epoch:16 step:13098 [D loss: 0.002859, acc.: 100.00%] [G loss: 5.550647]\n",
      "epoch:16 step:13099 [D loss: 0.002095, acc.: 100.00%] [G loss: 5.976970]\n",
      "epoch:16 step:13100 [D loss: 0.124786, acc.: 96.09%] [G loss: 5.144734]\n",
      "epoch:16 step:13101 [D loss: 0.004095, acc.: 100.00%] [G loss: 5.309093]\n",
      "epoch:16 step:13102 [D loss: 0.004076, acc.: 100.00%] [G loss: 4.987944]\n",
      "epoch:16 step:13103 [D loss: 0.004244, acc.: 100.00%] [G loss: 5.276571]\n",
      "epoch:16 step:13104 [D loss: 0.003675, acc.: 100.00%] [G loss: 1.289180]\n",
      "epoch:16 step:13105 [D loss: 0.005280, acc.: 100.00%] [G loss: 5.645051]\n",
      "epoch:16 step:13106 [D loss: 0.002148, acc.: 100.00%] [G loss: 0.880769]\n",
      "epoch:16 step:13107 [D loss: 0.007736, acc.: 100.00%] [G loss: 5.627995]\n",
      "epoch:16 step:13108 [D loss: 0.006230, acc.: 100.00%] [G loss: 6.226237]\n",
      "epoch:16 step:13109 [D loss: 0.033553, acc.: 100.00%] [G loss: 1.173831]\n",
      "epoch:16 step:13110 [D loss: 0.124291, acc.: 94.53%] [G loss: 8.214569]\n",
      "epoch:16 step:13111 [D loss: 0.004230, acc.: 100.00%] [G loss: 8.893950]\n",
      "epoch:16 step:13112 [D loss: 0.181104, acc.: 94.53%] [G loss: 0.792008]\n",
      "epoch:16 step:13113 [D loss: 0.004740, acc.: 100.00%] [G loss: 5.870646]\n",
      "epoch:16 step:13114 [D loss: 0.001490, acc.: 100.00%] [G loss: 5.385477]\n",
      "epoch:16 step:13115 [D loss: 0.002982, acc.: 100.00%] [G loss: 0.582023]\n",
      "epoch:16 step:13116 [D loss: 0.002087, acc.: 100.00%] [G loss: 5.944654]\n",
      "epoch:16 step:13117 [D loss: 0.002008, acc.: 100.00%] [G loss: 5.387582]\n",
      "epoch:16 step:13118 [D loss: 0.005832, acc.: 100.00%] [G loss: 0.434851]\n",
      "epoch:16 step:13119 [D loss: 0.098220, acc.: 96.88%] [G loss: 7.109695]\n",
      "epoch:16 step:13120 [D loss: 0.001684, acc.: 100.00%] [G loss: 6.898461]\n",
      "epoch:16 step:13121 [D loss: 0.011273, acc.: 100.00%] [G loss: 8.450262]\n",
      "epoch:16 step:13122 [D loss: 0.004066, acc.: 100.00%] [G loss: 6.948167]\n",
      "epoch:16 step:13123 [D loss: 0.005595, acc.: 100.00%] [G loss: 6.557702]\n",
      "epoch:16 step:13124 [D loss: 0.000913, acc.: 100.00%] [G loss: 5.873865]\n",
      "epoch:16 step:13125 [D loss: 0.003274, acc.: 100.00%] [G loss: 6.121841]\n",
      "epoch:16 step:13126 [D loss: 0.000812, acc.: 100.00%] [G loss: 5.616591]\n",
      "epoch:16 step:13127 [D loss: 0.002427, acc.: 100.00%] [G loss: 5.721272]\n",
      "epoch:16 step:13128 [D loss: 0.001766, acc.: 100.00%] [G loss: 5.177567]\n",
      "epoch:16 step:13129 [D loss: 0.000807, acc.: 100.00%] [G loss: 5.292421]\n",
      "epoch:16 step:13130 [D loss: 0.003504, acc.: 100.00%] [G loss: 5.383449]\n",
      "epoch:16 step:13131 [D loss: 0.001992, acc.: 100.00%] [G loss: 5.075147]\n",
      "epoch:16 step:13132 [D loss: 0.001368, acc.: 100.00%] [G loss: 4.553840]\n",
      "epoch:16 step:13133 [D loss: 0.004598, acc.: 100.00%] [G loss: 5.520257]\n",
      "epoch:16 step:13134 [D loss: 0.006324, acc.: 100.00%] [G loss: 3.408482]\n",
      "epoch:16 step:13135 [D loss: 0.002994, acc.: 100.00%] [G loss: 3.997652]\n",
      "epoch:16 step:13136 [D loss: 0.011258, acc.: 100.00%] [G loss: 5.278677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13137 [D loss: 0.005200, acc.: 100.00%] [G loss: 1.152631]\n",
      "epoch:16 step:13138 [D loss: 0.011703, acc.: 100.00%] [G loss: 5.478180]\n",
      "epoch:16 step:13139 [D loss: 0.005194, acc.: 100.00%] [G loss: 5.089721]\n",
      "epoch:16 step:13140 [D loss: 0.001930, acc.: 100.00%] [G loss: 5.654851]\n",
      "epoch:16 step:13141 [D loss: 0.002488, acc.: 100.00%] [G loss: 5.033786]\n",
      "epoch:16 step:13142 [D loss: 0.036923, acc.: 99.22%] [G loss: 4.825736]\n",
      "epoch:16 step:13143 [D loss: 0.007151, acc.: 100.00%] [G loss: 5.294227]\n",
      "epoch:16 step:13144 [D loss: 0.004613, acc.: 100.00%] [G loss: 6.029117]\n",
      "epoch:16 step:13145 [D loss: 0.003012, acc.: 100.00%] [G loss: 5.549581]\n",
      "epoch:16 step:13146 [D loss: 0.002659, acc.: 100.00%] [G loss: 5.101735]\n",
      "epoch:16 step:13147 [D loss: 0.001755, acc.: 100.00%] [G loss: 5.673982]\n",
      "epoch:16 step:13148 [D loss: 0.010951, acc.: 100.00%] [G loss: 3.886022]\n",
      "epoch:16 step:13149 [D loss: 0.001570, acc.: 100.00%] [G loss: 3.339810]\n",
      "epoch:16 step:13150 [D loss: 0.007197, acc.: 100.00%] [G loss: 0.435398]\n",
      "epoch:16 step:13151 [D loss: 0.008281, acc.: 100.00%] [G loss: 6.738476]\n",
      "epoch:16 step:13152 [D loss: 0.006554, acc.: 100.00%] [G loss: 7.314611]\n",
      "epoch:16 step:13153 [D loss: 0.007174, acc.: 100.00%] [G loss: 3.815243]\n",
      "epoch:16 step:13154 [D loss: 0.001623, acc.: 100.00%] [G loss: 7.522922]\n",
      "epoch:16 step:13155 [D loss: 0.005304, acc.: 100.00%] [G loss: 7.952995]\n",
      "epoch:16 step:13156 [D loss: 0.004439, acc.: 100.00%] [G loss: 8.267533]\n",
      "epoch:16 step:13157 [D loss: 0.001826, acc.: 100.00%] [G loss: 0.708415]\n",
      "epoch:16 step:13158 [D loss: 0.011960, acc.: 100.00%] [G loss: 2.331428]\n",
      "epoch:16 step:13159 [D loss: 0.013163, acc.: 100.00%] [G loss: 8.884037]\n",
      "epoch:16 step:13160 [D loss: 0.000607, acc.: 100.00%] [G loss: 9.519593]\n",
      "epoch:16 step:13161 [D loss: 0.020910, acc.: 100.00%] [G loss: 8.560274]\n",
      "epoch:16 step:13162 [D loss: 0.000463, acc.: 100.00%] [G loss: 1.086882]\n",
      "epoch:16 step:13163 [D loss: 0.083643, acc.: 96.09%] [G loss: 4.983696]\n",
      "epoch:16 step:13164 [D loss: 0.030838, acc.: 99.22%] [G loss: 10.324551]\n",
      "epoch:16 step:13165 [D loss: 0.037448, acc.: 99.22%] [G loss: 6.743760]\n",
      "epoch:16 step:13166 [D loss: 0.019780, acc.: 100.00%] [G loss: 2.194370]\n",
      "epoch:16 step:13167 [D loss: 0.023575, acc.: 100.00%] [G loss: 8.717437]\n",
      "epoch:16 step:13168 [D loss: 0.016646, acc.: 99.22%] [G loss: 7.889155]\n",
      "epoch:16 step:13169 [D loss: 0.007213, acc.: 100.00%] [G loss: 4.259892]\n",
      "epoch:16 step:13170 [D loss: 0.003587, acc.: 100.00%] [G loss: 3.585140]\n",
      "epoch:16 step:13171 [D loss: 0.139970, acc.: 94.53%] [G loss: 11.636356]\n",
      "epoch:16 step:13172 [D loss: 0.161056, acc.: 94.53%] [G loss: 10.193972]\n",
      "epoch:16 step:13173 [D loss: 0.026155, acc.: 98.44%] [G loss: 11.436042]\n",
      "epoch:16 step:13174 [D loss: 0.005026, acc.: 100.00%] [G loss: 9.248335]\n",
      "epoch:16 step:13175 [D loss: 0.001828, acc.: 100.00%] [G loss: 7.935994]\n",
      "epoch:16 step:13176 [D loss: 0.003733, acc.: 100.00%] [G loss: 10.905156]\n",
      "epoch:16 step:13177 [D loss: 0.001284, acc.: 100.00%] [G loss: 7.178448]\n",
      "epoch:16 step:13178 [D loss: 0.001305, acc.: 100.00%] [G loss: 7.139601]\n",
      "epoch:16 step:13179 [D loss: 0.007198, acc.: 100.00%] [G loss: 10.873235]\n",
      "epoch:16 step:13180 [D loss: 0.002873, acc.: 100.00%] [G loss: 6.710977]\n",
      "epoch:16 step:13181 [D loss: 0.284890, acc.: 91.41%] [G loss: 10.162647]\n",
      "epoch:16 step:13182 [D loss: 0.000240, acc.: 100.00%] [G loss: 9.841000]\n",
      "epoch:16 step:13183 [D loss: 2.387739, acc.: 52.34%] [G loss: 6.270053]\n",
      "epoch:16 step:13184 [D loss: 0.385873, acc.: 83.59%] [G loss: 6.631408]\n",
      "epoch:16 step:13185 [D loss: 0.000667, acc.: 100.00%] [G loss: 7.483287]\n",
      "epoch:16 step:13186 [D loss: 0.692158, acc.: 75.00%] [G loss: 4.930798]\n",
      "epoch:16 step:13187 [D loss: 0.016666, acc.: 100.00%] [G loss: 3.784604]\n",
      "epoch:16 step:13188 [D loss: 0.058652, acc.: 100.00%] [G loss: 2.534228]\n",
      "epoch:16 step:13189 [D loss: 0.002284, acc.: 100.00%] [G loss: 3.762973]\n",
      "epoch:16 step:13190 [D loss: 0.004748, acc.: 100.00%] [G loss: 2.943435]\n",
      "epoch:16 step:13191 [D loss: 0.034554, acc.: 100.00%] [G loss: 1.723419]\n",
      "epoch:16 step:13192 [D loss: 0.021599, acc.: 100.00%] [G loss: 0.933168]\n",
      "epoch:16 step:13193 [D loss: 0.017916, acc.: 100.00%] [G loss: 3.003890]\n",
      "epoch:16 step:13194 [D loss: 0.004355, acc.: 100.00%] [G loss: 2.134582]\n",
      "epoch:16 step:13195 [D loss: 0.044511, acc.: 99.22%] [G loss: 0.376438]\n",
      "epoch:16 step:13196 [D loss: 0.164632, acc.: 94.53%] [G loss: 6.766065]\n",
      "epoch:16 step:13197 [D loss: 0.083682, acc.: 97.66%] [G loss: 7.194820]\n",
      "epoch:16 step:13198 [D loss: 0.082057, acc.: 96.09%] [G loss: 0.857915]\n",
      "epoch:16 step:13199 [D loss: 0.053721, acc.: 98.44%] [G loss: 4.874728]\n",
      "epoch:16 step:13200 [D loss: 0.009053, acc.: 100.00%] [G loss: 1.596076]\n",
      "epoch:16 step:13201 [D loss: 0.007724, acc.: 100.00%] [G loss: 1.010574]\n",
      "epoch:16 step:13202 [D loss: 0.014723, acc.: 100.00%] [G loss: 6.882349]\n",
      "epoch:16 step:13203 [D loss: 0.046043, acc.: 98.44%] [G loss: 0.656355]\n",
      "epoch:16 step:13204 [D loss: 0.079482, acc.: 96.09%] [G loss: 6.486358]\n",
      "epoch:16 step:13205 [D loss: 0.050793, acc.: 98.44%] [G loss: 2.920110]\n",
      "epoch:16 step:13206 [D loss: 0.018297, acc.: 100.00%] [G loss: 5.576491]\n",
      "epoch:16 step:13207 [D loss: 0.038682, acc.: 98.44%] [G loss: 6.046222]\n",
      "epoch:16 step:13208 [D loss: 0.036069, acc.: 100.00%] [G loss: 5.711784]\n",
      "epoch:16 step:13209 [D loss: 0.048585, acc.: 98.44%] [G loss: 2.972602]\n",
      "epoch:16 step:13210 [D loss: 0.013308, acc.: 100.00%] [G loss: 6.553152]\n",
      "epoch:16 step:13211 [D loss: 0.009335, acc.: 100.00%] [G loss: 1.919220]\n",
      "epoch:16 step:13212 [D loss: 0.014949, acc.: 100.00%] [G loss: 1.282235]\n",
      "epoch:16 step:13213 [D loss: 0.009214, acc.: 100.00%] [G loss: 6.222548]\n",
      "epoch:16 step:13214 [D loss: 0.008377, acc.: 100.00%] [G loss: 5.336257]\n",
      "epoch:16 step:13215 [D loss: 0.275807, acc.: 86.72%] [G loss: 10.340073]\n",
      "epoch:16 step:13216 [D loss: 2.072590, acc.: 53.91%] [G loss: 4.343884]\n",
      "epoch:16 step:13217 [D loss: 0.165743, acc.: 94.53%] [G loss: 6.279435]\n",
      "epoch:16 step:13218 [D loss: 0.051044, acc.: 98.44%] [G loss: 5.645098]\n",
      "epoch:16 step:13219 [D loss: 0.000672, acc.: 100.00%] [G loss: 6.408524]\n",
      "epoch:16 step:13220 [D loss: 0.005208, acc.: 100.00%] [G loss: 3.851029]\n",
      "epoch:16 step:13221 [D loss: 0.491040, acc.: 79.69%] [G loss: 7.156582]\n",
      "epoch:16 step:13222 [D loss: 0.254632, acc.: 89.84%] [G loss: 7.329735]\n",
      "epoch:16 step:13223 [D loss: 0.366313, acc.: 82.81%] [G loss: 4.295754]\n",
      "epoch:16 step:13224 [D loss: 0.163721, acc.: 95.31%] [G loss: 1.977888]\n",
      "epoch:16 step:13225 [D loss: 0.006221, acc.: 100.00%] [G loss: 2.241836]\n",
      "epoch:16 step:13226 [D loss: 0.004416, acc.: 100.00%] [G loss: 1.299844]\n",
      "epoch:16 step:13227 [D loss: 0.001876, acc.: 100.00%] [G loss: 0.431044]\n",
      "epoch:16 step:13228 [D loss: 0.021797, acc.: 100.00%] [G loss: 0.072929]\n",
      "epoch:16 step:13229 [D loss: 0.019711, acc.: 100.00%] [G loss: 4.571392]\n",
      "epoch:16 step:13230 [D loss: 0.136085, acc.: 94.53%] [G loss: 6.348500]\n",
      "epoch:16 step:13231 [D loss: 0.078892, acc.: 97.66%] [G loss: 6.892361]\n",
      "epoch:16 step:13232 [D loss: 0.111165, acc.: 96.88%] [G loss: 0.401853]\n",
      "epoch:16 step:13233 [D loss: 0.046160, acc.: 99.22%] [G loss: 0.417140]\n",
      "epoch:16 step:13234 [D loss: 0.014546, acc.: 100.00%] [G loss: 1.228607]\n",
      "epoch:16 step:13235 [D loss: 0.084387, acc.: 98.44%] [G loss: 0.128325]\n",
      "epoch:16 step:13236 [D loss: 0.049237, acc.: 97.66%] [G loss: 5.712241]\n",
      "epoch:16 step:13237 [D loss: 0.003447, acc.: 100.00%] [G loss: 4.308636]\n",
      "epoch:16 step:13238 [D loss: 0.116455, acc.: 95.31%] [G loss: 0.294399]\n",
      "epoch:16 step:13239 [D loss: 0.039524, acc.: 98.44%] [G loss: 1.181662]\n",
      "epoch:16 step:13240 [D loss: 0.006944, acc.: 100.00%] [G loss: 1.463548]\n",
      "epoch:16 step:13241 [D loss: 0.018079, acc.: 100.00%] [G loss: 1.167281]\n",
      "epoch:16 step:13242 [D loss: 0.091706, acc.: 96.09%] [G loss: 2.479400]\n",
      "epoch:16 step:13243 [D loss: 0.014130, acc.: 100.00%] [G loss: 7.575169]\n",
      "epoch:16 step:13244 [D loss: 0.022398, acc.: 100.00%] [G loss: 6.622540]\n",
      "epoch:16 step:13245 [D loss: 0.002375, acc.: 100.00%] [G loss: 5.054353]\n",
      "epoch:16 step:13246 [D loss: 0.031796, acc.: 99.22%] [G loss: 1.465457]\n",
      "epoch:16 step:13247 [D loss: 0.009255, acc.: 100.00%] [G loss: 2.021696]\n",
      "epoch:16 step:13248 [D loss: 0.014398, acc.: 100.00%] [G loss: 2.055289]\n",
      "epoch:16 step:13249 [D loss: 0.570372, acc.: 73.44%] [G loss: 6.936298]\n",
      "epoch:16 step:13250 [D loss: 1.203457, acc.: 56.25%] [G loss: 1.915858]\n",
      "epoch:16 step:13251 [D loss: 0.007264, acc.: 100.00%] [G loss: 5.440699]\n",
      "epoch:16 step:13252 [D loss: 0.092753, acc.: 95.31%] [G loss: 5.984133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16 step:13253 [D loss: 0.002389, acc.: 100.00%] [G loss: 4.346329]\n",
      "epoch:16 step:13254 [D loss: 0.003847, acc.: 100.00%] [G loss: 3.624457]\n",
      "epoch:16 step:13255 [D loss: 0.024160, acc.: 100.00%] [G loss: 3.256858]\n",
      "epoch:16 step:13256 [D loss: 0.209142, acc.: 91.41%] [G loss: 4.397701]\n",
      "epoch:16 step:13257 [D loss: 0.044448, acc.: 97.66%] [G loss: 4.711222]\n",
      "epoch:16 step:13258 [D loss: 0.464408, acc.: 80.47%] [G loss: 0.767963]\n",
      "epoch:16 step:13259 [D loss: 1.182351, acc.: 60.94%] [G loss: 9.769069]\n",
      "epoch:16 step:13260 [D loss: 0.580818, acc.: 77.34%] [G loss: 4.955906]\n",
      "epoch:16 step:13261 [D loss: 0.404842, acc.: 80.47%] [G loss: 4.261115]\n",
      "epoch:16 step:13262 [D loss: 0.078484, acc.: 96.09%] [G loss: 0.329022]\n",
      "epoch:16 step:13263 [D loss: 0.058393, acc.: 98.44%] [G loss: 1.614748]\n",
      "epoch:16 step:13264 [D loss: 0.010600, acc.: 100.00%] [G loss: 1.305845]\n",
      "epoch:16 step:13265 [D loss: 0.018393, acc.: 100.00%] [G loss: 1.131500]\n",
      "epoch:16 step:13266 [D loss: 0.034455, acc.: 99.22%] [G loss: 4.403119]\n",
      "epoch:16 step:13267 [D loss: 0.055366, acc.: 97.66%] [G loss: 1.871508]\n",
      "epoch:16 step:13268 [D loss: 0.364968, acc.: 83.59%] [G loss: 5.664949]\n",
      "epoch:16 step:13269 [D loss: 0.377708, acc.: 80.47%] [G loss: 3.967147]\n",
      "epoch:16 step:13270 [D loss: 0.014215, acc.: 100.00%] [G loss: 2.168848]\n",
      "epoch:16 step:13271 [D loss: 0.166277, acc.: 94.53%] [G loss: 5.644462]\n",
      "epoch:16 step:13272 [D loss: 0.025730, acc.: 100.00%] [G loss: 5.854959]\n",
      "epoch:16 step:13273 [D loss: 0.118399, acc.: 96.88%] [G loss: 3.908401]\n",
      "epoch:16 step:13274 [D loss: 0.023731, acc.: 99.22%] [G loss: 3.155878]\n",
      "epoch:16 step:13275 [D loss: 0.010487, acc.: 100.00%] [G loss: 4.271295]\n",
      "epoch:16 step:13276 [D loss: 0.140260, acc.: 94.53%] [G loss: 4.036622]\n",
      "epoch:16 step:13277 [D loss: 0.045498, acc.: 98.44%] [G loss: 3.812443]\n",
      "epoch:17 step:13278 [D loss: 0.045211, acc.: 98.44%] [G loss: 2.784049]\n",
      "epoch:17 step:13279 [D loss: 0.020841, acc.: 99.22%] [G loss: 3.010771]\n",
      "epoch:17 step:13280 [D loss: 0.067560, acc.: 98.44%] [G loss: 3.117208]\n",
      "epoch:17 step:13281 [D loss: 0.012794, acc.: 100.00%] [G loss: 2.709289]\n",
      "epoch:17 step:13282 [D loss: 0.019321, acc.: 99.22%] [G loss: 2.179776]\n",
      "epoch:17 step:13283 [D loss: 0.012934, acc.: 100.00%] [G loss: 1.611608]\n",
      "epoch:17 step:13284 [D loss: 0.023039, acc.: 100.00%] [G loss: 1.778565]\n",
      "epoch:17 step:13285 [D loss: 0.038389, acc.: 100.00%] [G loss: 1.433208]\n",
      "epoch:17 step:13286 [D loss: 0.008047, acc.: 100.00%] [G loss: 2.204584]\n",
      "epoch:17 step:13287 [D loss: 0.072317, acc.: 98.44%] [G loss: 2.923018]\n",
      "epoch:17 step:13288 [D loss: 0.023596, acc.: 100.00%] [G loss: 2.851752]\n",
      "epoch:17 step:13289 [D loss: 0.037977, acc.: 100.00%] [G loss: 3.052406]\n",
      "epoch:17 step:13290 [D loss: 0.009510, acc.: 100.00%] [G loss: 1.351984]\n",
      "epoch:17 step:13291 [D loss: 0.110773, acc.: 95.31%] [G loss: 0.725098]\n",
      "epoch:17 step:13292 [D loss: 0.063480, acc.: 98.44%] [G loss: 2.027000]\n",
      "epoch:17 step:13293 [D loss: 0.003118, acc.: 100.00%] [G loss: 1.333386]\n",
      "epoch:17 step:13294 [D loss: 0.014786, acc.: 100.00%] [G loss: 2.230077]\n",
      "epoch:17 step:13295 [D loss: 0.028191, acc.: 100.00%] [G loss: 4.570416]\n",
      "epoch:17 step:13296 [D loss: 0.033643, acc.: 100.00%] [G loss: 0.702148]\n",
      "epoch:17 step:13297 [D loss: 0.033134, acc.: 100.00%] [G loss: 1.929328]\n",
      "epoch:17 step:13298 [D loss: 0.003587, acc.: 100.00%] [G loss: 2.740911]\n",
      "epoch:17 step:13299 [D loss: 0.025004, acc.: 100.00%] [G loss: 1.572242]\n",
      "epoch:17 step:13300 [D loss: 0.076198, acc.: 96.88%] [G loss: 2.073770]\n",
      "epoch:17 step:13301 [D loss: 0.003953, acc.: 100.00%] [G loss: 1.666927]\n",
      "epoch:17 step:13302 [D loss: 0.053333, acc.: 99.22%] [G loss: 0.462712]\n",
      "epoch:17 step:13303 [D loss: 0.018292, acc.: 100.00%] [G loss: 0.958227]\n",
      "epoch:17 step:13304 [D loss: 0.045223, acc.: 99.22%] [G loss: 4.411598]\n",
      "epoch:17 step:13305 [D loss: 0.048417, acc.: 98.44%] [G loss: 1.810929]\n",
      "epoch:17 step:13306 [D loss: 0.027471, acc.: 99.22%] [G loss: 1.809278]\n",
      "epoch:17 step:13307 [D loss: 0.025731, acc.: 100.00%] [G loss: 2.423541]\n",
      "epoch:17 step:13308 [D loss: 0.017280, acc.: 100.00%] [G loss: 4.283467]\n",
      "epoch:17 step:13309 [D loss: 0.022333, acc.: 100.00%] [G loss: 3.413978]\n",
      "epoch:17 step:13310 [D loss: 0.030811, acc.: 100.00%] [G loss: 1.409191]\n",
      "epoch:17 step:13311 [D loss: 0.008833, acc.: 100.00%] [G loss: 1.671759]\n",
      "epoch:17 step:13312 [D loss: 0.024031, acc.: 100.00%] [G loss: 3.673952]\n",
      "epoch:17 step:13313 [D loss: 0.335186, acc.: 85.16%] [G loss: 6.220774]\n",
      "epoch:17 step:13314 [D loss: 0.010638, acc.: 100.00%] [G loss: 9.037697]\n",
      "epoch:17 step:13315 [D loss: 0.464403, acc.: 75.78%] [G loss: 0.495171]\n",
      "epoch:17 step:13316 [D loss: 1.013511, acc.: 57.81%] [G loss: 8.164352]\n",
      "epoch:17 step:13317 [D loss: 2.525342, acc.: 50.00%] [G loss: 4.423953]\n",
      "epoch:17 step:13318 [D loss: 0.729838, acc.: 71.88%] [G loss: 3.274324]\n",
      "epoch:17 step:13319 [D loss: 0.311258, acc.: 85.94%] [G loss: 0.577328]\n",
      "epoch:17 step:13320 [D loss: 0.038357, acc.: 97.66%] [G loss: 5.297843]\n",
      "epoch:17 step:13321 [D loss: 0.066605, acc.: 97.66%] [G loss: 0.836452]\n",
      "epoch:17 step:13322 [D loss: 0.010612, acc.: 100.00%] [G loss: 0.613809]\n",
      "epoch:17 step:13323 [D loss: 0.039183, acc.: 98.44%] [G loss: 3.772982]\n",
      "epoch:17 step:13324 [D loss: 0.044362, acc.: 100.00%] [G loss: 0.524746]\n",
      "epoch:17 step:13325 [D loss: 0.069426, acc.: 98.44%] [G loss: 0.561335]\n",
      "epoch:17 step:13326 [D loss: 0.041829, acc.: 100.00%] [G loss: 5.115020]\n",
      "epoch:17 step:13327 [D loss: 0.167136, acc.: 96.09%] [G loss: 4.208869]\n",
      "epoch:17 step:13328 [D loss: 0.044494, acc.: 99.22%] [G loss: 0.645563]\n",
      "epoch:17 step:13329 [D loss: 0.100289, acc.: 98.44%] [G loss: 1.059735]\n",
      "epoch:17 step:13330 [D loss: 0.026333, acc.: 100.00%] [G loss: 4.125129]\n",
      "epoch:17 step:13331 [D loss: 0.278720, acc.: 86.72%] [G loss: 4.868233]\n",
      "epoch:17 step:13332 [D loss: 0.029583, acc.: 100.00%] [G loss: 1.289973]\n",
      "epoch:17 step:13333 [D loss: 0.036141, acc.: 98.44%] [G loss: 0.935897]\n",
      "epoch:17 step:13334 [D loss: 0.067117, acc.: 99.22%] [G loss: 0.871814]\n",
      "epoch:17 step:13335 [D loss: 0.056334, acc.: 98.44%] [G loss: 1.332878]\n",
      "epoch:17 step:13336 [D loss: 0.102872, acc.: 98.44%] [G loss: 2.894413]\n",
      "epoch:17 step:13337 [D loss: 0.103432, acc.: 97.66%] [G loss: 4.793295]\n",
      "epoch:17 step:13338 [D loss: 0.348265, acc.: 81.25%] [G loss: 4.687677]\n",
      "epoch:17 step:13339 [D loss: 0.013961, acc.: 100.00%] [G loss: 1.025667]\n",
      "epoch:17 step:13340 [D loss: 0.386186, acc.: 84.38%] [G loss: 0.638232]\n",
      "epoch:17 step:13341 [D loss: 0.047300, acc.: 100.00%] [G loss: 4.319651]\n",
      "epoch:17 step:13342 [D loss: 1.008304, acc.: 59.38%] [G loss: 1.671860]\n",
      "epoch:17 step:13343 [D loss: 0.087803, acc.: 97.66%] [G loss: 2.789721]\n",
      "epoch:17 step:13344 [D loss: 0.019307, acc.: 99.22%] [G loss: 4.461391]\n",
      "epoch:17 step:13345 [D loss: 0.029914, acc.: 99.22%] [G loss: 3.517952]\n",
      "epoch:17 step:13346 [D loss: 0.071889, acc.: 98.44%] [G loss: 0.622393]\n",
      "epoch:17 step:13347 [D loss: 0.101924, acc.: 96.09%] [G loss: 1.383616]\n",
      "epoch:17 step:13348 [D loss: 0.524270, acc.: 71.88%] [G loss: 1.724295]\n",
      "epoch:17 step:13349 [D loss: 0.033578, acc.: 100.00%] [G loss: 3.983581]\n",
      "epoch:17 step:13350 [D loss: 0.052881, acc.: 99.22%] [G loss: 2.279993]\n",
      "epoch:17 step:13351 [D loss: 0.051912, acc.: 99.22%] [G loss: 4.366178]\n",
      "epoch:17 step:13352 [D loss: 0.029061, acc.: 100.00%] [G loss: 2.373131]\n",
      "epoch:17 step:13353 [D loss: 0.081760, acc.: 99.22%] [G loss: 2.110621]\n",
      "epoch:17 step:13354 [D loss: 0.620758, acc.: 71.88%] [G loss: 7.307461]\n",
      "epoch:17 step:13355 [D loss: 0.131574, acc.: 92.97%] [G loss: 8.021510]\n",
      "epoch:17 step:13356 [D loss: 0.400776, acc.: 81.25%] [G loss: 5.873362]\n",
      "epoch:17 step:13357 [D loss: 0.011111, acc.: 100.00%] [G loss: 5.758355]\n",
      "epoch:17 step:13358 [D loss: 0.003289, acc.: 100.00%] [G loss: 5.174548]\n",
      "epoch:17 step:13359 [D loss: 0.012756, acc.: 100.00%] [G loss: 4.795894]\n",
      "epoch:17 step:13360 [D loss: 0.023767, acc.: 100.00%] [G loss: 4.091551]\n",
      "epoch:17 step:13361 [D loss: 0.005789, acc.: 100.00%] [G loss: 4.506004]\n",
      "epoch:17 step:13362 [D loss: 0.006441, acc.: 100.00%] [G loss: 4.128719]\n",
      "epoch:17 step:13363 [D loss: 0.006487, acc.: 100.00%] [G loss: 3.272373]\n",
      "epoch:17 step:13364 [D loss: 0.022168, acc.: 100.00%] [G loss: 3.122114]\n",
      "epoch:17 step:13365 [D loss: 0.044692, acc.: 100.00%] [G loss: 2.204698]\n",
      "epoch:17 step:13366 [D loss: 0.057851, acc.: 98.44%] [G loss: 4.447410]\n",
      "epoch:17 step:13367 [D loss: 0.055092, acc.: 99.22%] [G loss: 5.007020]\n",
      "epoch:17 step:13368 [D loss: 0.093602, acc.: 96.09%] [G loss: 0.813922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13369 [D loss: 0.019384, acc.: 100.00%] [G loss: 2.547314]\n",
      "epoch:17 step:13370 [D loss: 0.044797, acc.: 99.22%] [G loss: 1.399066]\n",
      "epoch:17 step:13371 [D loss: 1.217619, acc.: 59.38%] [G loss: 8.737951]\n",
      "epoch:17 step:13372 [D loss: 1.956367, acc.: 50.78%] [G loss: 5.159423]\n",
      "epoch:17 step:13373 [D loss: 0.056770, acc.: 98.44%] [G loss: 4.043153]\n",
      "epoch:17 step:13374 [D loss: 0.033610, acc.: 100.00%] [G loss: 3.495139]\n",
      "epoch:17 step:13375 [D loss: 0.017144, acc.: 99.22%] [G loss: 2.522665]\n",
      "epoch:17 step:13376 [D loss: 0.110787, acc.: 96.88%] [G loss: 2.130310]\n",
      "epoch:17 step:13377 [D loss: 0.036237, acc.: 99.22%] [G loss: 2.736028]\n",
      "epoch:17 step:13378 [D loss: 0.007085, acc.: 100.00%] [G loss: 2.443067]\n",
      "epoch:17 step:13379 [D loss: 0.022753, acc.: 100.00%] [G loss: 2.097036]\n",
      "epoch:17 step:13380 [D loss: 0.019154, acc.: 100.00%] [G loss: 1.761274]\n",
      "epoch:17 step:13381 [D loss: 0.049799, acc.: 99.22%] [G loss: 0.751935]\n",
      "epoch:17 step:13382 [D loss: 0.097252, acc.: 96.88%] [G loss: 0.311430]\n",
      "epoch:17 step:13383 [D loss: 0.018620, acc.: 100.00%] [G loss: 0.311651]\n",
      "epoch:17 step:13384 [D loss: 0.041715, acc.: 100.00%] [G loss: 0.805904]\n",
      "epoch:17 step:13385 [D loss: 0.029724, acc.: 100.00%] [G loss: 1.830951]\n",
      "epoch:17 step:13386 [D loss: 0.044457, acc.: 99.22%] [G loss: 1.102765]\n",
      "epoch:17 step:13387 [D loss: 0.012875, acc.: 100.00%] [G loss: 1.068426]\n",
      "epoch:17 step:13388 [D loss: 0.119647, acc.: 96.88%] [G loss: 0.215204]\n",
      "epoch:17 step:13389 [D loss: 0.139092, acc.: 96.09%] [G loss: 0.633809]\n",
      "epoch:17 step:13390 [D loss: 0.002756, acc.: 100.00%] [G loss: 2.456040]\n",
      "epoch:17 step:13391 [D loss: 0.183342, acc.: 91.41%] [G loss: 0.460166]\n",
      "epoch:17 step:13392 [D loss: 0.016931, acc.: 99.22%] [G loss: 0.039348]\n",
      "epoch:17 step:13393 [D loss: 0.170708, acc.: 94.53%] [G loss: 0.916048]\n",
      "epoch:17 step:13394 [D loss: 0.003634, acc.: 100.00%] [G loss: 2.370561]\n",
      "epoch:17 step:13395 [D loss: 0.009674, acc.: 100.00%] [G loss: 1.908738]\n",
      "epoch:17 step:13396 [D loss: 0.081394, acc.: 96.88%] [G loss: 1.139083]\n",
      "epoch:17 step:13397 [D loss: 0.031664, acc.: 99.22%] [G loss: 0.647123]\n",
      "epoch:17 step:13398 [D loss: 0.005723, acc.: 100.00%] [G loss: 0.701538]\n",
      "epoch:17 step:13399 [D loss: 0.014797, acc.: 99.22%] [G loss: 0.665115]\n",
      "epoch:17 step:13400 [D loss: 0.032538, acc.: 100.00%] [G loss: 0.552392]\n",
      "epoch:17 step:13401 [D loss: 0.020114, acc.: 100.00%] [G loss: 0.485318]\n",
      "epoch:17 step:13402 [D loss: 0.007125, acc.: 100.00%] [G loss: 0.271788]\n",
      "epoch:17 step:13403 [D loss: 0.213347, acc.: 91.41%] [G loss: 2.949943]\n",
      "epoch:17 step:13404 [D loss: 0.062060, acc.: 96.88%] [G loss: 3.647961]\n",
      "epoch:17 step:13405 [D loss: 0.472836, acc.: 74.22%] [G loss: 0.027173]\n",
      "epoch:17 step:13406 [D loss: 0.139367, acc.: 92.19%] [G loss: 0.138918]\n",
      "epoch:17 step:13407 [D loss: 0.003714, acc.: 100.00%] [G loss: 0.332927]\n",
      "epoch:17 step:13408 [D loss: 0.006248, acc.: 100.00%] [G loss: 0.261403]\n",
      "epoch:17 step:13409 [D loss: 0.011453, acc.: 100.00%] [G loss: 1.092638]\n",
      "epoch:17 step:13410 [D loss: 0.056683, acc.: 99.22%] [G loss: 0.522345]\n",
      "epoch:17 step:13411 [D loss: 0.002834, acc.: 100.00%] [G loss: 2.633643]\n",
      "epoch:17 step:13412 [D loss: 0.037802, acc.: 99.22%] [G loss: 0.253477]\n",
      "epoch:17 step:13413 [D loss: 0.018945, acc.: 100.00%] [G loss: 0.433721]\n",
      "epoch:17 step:13414 [D loss: 0.015678, acc.: 100.00%] [G loss: 0.346720]\n",
      "epoch:17 step:13415 [D loss: 0.327178, acc.: 82.03%] [G loss: 5.890816]\n",
      "epoch:17 step:13416 [D loss: 1.226264, acc.: 57.03%] [G loss: 1.779095]\n",
      "epoch:17 step:13417 [D loss: 0.322109, acc.: 84.38%] [G loss: 4.578021]\n",
      "epoch:17 step:13418 [D loss: 0.023886, acc.: 100.00%] [G loss: 4.644540]\n",
      "epoch:17 step:13419 [D loss: 0.508373, acc.: 75.78%] [G loss: 2.569693]\n",
      "epoch:17 step:13420 [D loss: 0.104617, acc.: 96.09%] [G loss: 1.420477]\n",
      "epoch:17 step:13421 [D loss: 0.072005, acc.: 97.66%] [G loss: 3.314611]\n",
      "epoch:17 step:13422 [D loss: 0.021829, acc.: 100.00%] [G loss: 3.462726]\n",
      "epoch:17 step:13423 [D loss: 0.038201, acc.: 100.00%] [G loss: 1.764017]\n",
      "epoch:17 step:13424 [D loss: 0.035286, acc.: 100.00%] [G loss: 2.868283]\n",
      "epoch:17 step:13425 [D loss: 0.021517, acc.: 100.00%] [G loss: 1.470690]\n",
      "epoch:17 step:13426 [D loss: 0.024725, acc.: 100.00%] [G loss: 1.742772]\n",
      "epoch:17 step:13427 [D loss: 0.046532, acc.: 100.00%] [G loss: 2.879109]\n",
      "epoch:17 step:13428 [D loss: 0.026217, acc.: 100.00%] [G loss: 2.626276]\n",
      "epoch:17 step:13429 [D loss: 0.055226, acc.: 98.44%] [G loss: 2.030268]\n",
      "epoch:17 step:13430 [D loss: 0.128021, acc.: 95.31%] [G loss: 1.305660]\n",
      "epoch:17 step:13431 [D loss: 0.032865, acc.: 100.00%] [G loss: 2.113853]\n",
      "epoch:17 step:13432 [D loss: 0.043924, acc.: 99.22%] [G loss: 2.553473]\n",
      "epoch:17 step:13433 [D loss: 0.012357, acc.: 100.00%] [G loss: 1.249242]\n",
      "epoch:17 step:13434 [D loss: 0.021790, acc.: 100.00%] [G loss: 0.804056]\n",
      "epoch:17 step:13435 [D loss: 0.087395, acc.: 97.66%] [G loss: 1.058430]\n",
      "epoch:17 step:13436 [D loss: 0.054515, acc.: 99.22%] [G loss: 1.222317]\n",
      "epoch:17 step:13437 [D loss: 0.019247, acc.: 100.00%] [G loss: 1.743120]\n",
      "epoch:17 step:13438 [D loss: 0.037993, acc.: 99.22%] [G loss: 0.778577]\n",
      "epoch:17 step:13439 [D loss: 0.021475, acc.: 100.00%] [G loss: 1.032057]\n",
      "epoch:17 step:13440 [D loss: 0.029054, acc.: 100.00%] [G loss: 1.331552]\n",
      "epoch:17 step:13441 [D loss: 0.016351, acc.: 100.00%] [G loss: 0.815141]\n",
      "epoch:17 step:13442 [D loss: 0.030253, acc.: 100.00%] [G loss: 2.615614]\n",
      "epoch:17 step:13443 [D loss: 0.035045, acc.: 99.22%] [G loss: 0.696535]\n",
      "epoch:17 step:13444 [D loss: 0.005117, acc.: 100.00%] [G loss: 0.851113]\n",
      "epoch:17 step:13445 [D loss: 0.002147, acc.: 100.00%] [G loss: 0.468749]\n",
      "epoch:17 step:13446 [D loss: 0.008127, acc.: 100.00%] [G loss: 3.266137]\n",
      "epoch:17 step:13447 [D loss: 0.015593, acc.: 100.00%] [G loss: 0.286885]\n",
      "epoch:17 step:13448 [D loss: 0.021377, acc.: 100.00%] [G loss: 0.500952]\n",
      "epoch:17 step:13449 [D loss: 0.016543, acc.: 100.00%] [G loss: 1.511032]\n",
      "epoch:17 step:13450 [D loss: 0.009263, acc.: 100.00%] [G loss: 0.655873]\n",
      "epoch:17 step:13451 [D loss: 0.139140, acc.: 96.09%] [G loss: 2.958024]\n",
      "epoch:17 step:13452 [D loss: 0.004018, acc.: 100.00%] [G loss: 3.003580]\n",
      "epoch:17 step:13453 [D loss: 1.084032, acc.: 46.09%] [G loss: 4.750062]\n",
      "epoch:17 step:13454 [D loss: 0.201487, acc.: 90.62%] [G loss: 6.138786]\n",
      "epoch:17 step:13455 [D loss: 0.811301, acc.: 67.19%] [G loss: 0.156795]\n",
      "epoch:17 step:13456 [D loss: 1.085497, acc.: 62.50%] [G loss: 5.070456]\n",
      "epoch:17 step:13457 [D loss: 0.064321, acc.: 97.66%] [G loss: 7.162778]\n",
      "epoch:17 step:13458 [D loss: 0.264743, acc.: 91.41%] [G loss: 5.480958]\n",
      "epoch:17 step:13459 [D loss: 0.017755, acc.: 99.22%] [G loss: 6.327328]\n",
      "epoch:17 step:13460 [D loss: 0.004068, acc.: 100.00%] [G loss: 4.285427]\n",
      "epoch:17 step:13461 [D loss: 0.005243, acc.: 100.00%] [G loss: 3.752163]\n",
      "epoch:17 step:13462 [D loss: 0.007513, acc.: 100.00%] [G loss: 3.098991]\n",
      "epoch:17 step:13463 [D loss: 0.018208, acc.: 100.00%] [G loss: 3.108277]\n",
      "epoch:17 step:13464 [D loss: 0.117762, acc.: 95.31%] [G loss: 2.774413]\n",
      "epoch:17 step:13465 [D loss: 0.027865, acc.: 100.00%] [G loss: 2.711517]\n",
      "epoch:17 step:13466 [D loss: 0.011852, acc.: 100.00%] [G loss: 3.460778]\n",
      "epoch:17 step:13467 [D loss: 0.108297, acc.: 96.88%] [G loss: 0.917373]\n",
      "epoch:17 step:13468 [D loss: 0.005035, acc.: 100.00%] [G loss: 2.973089]\n",
      "epoch:17 step:13469 [D loss: 0.018404, acc.: 100.00%] [G loss: 3.100447]\n",
      "epoch:17 step:13470 [D loss: 0.019542, acc.: 100.00%] [G loss: 0.190107]\n",
      "epoch:17 step:13471 [D loss: 0.003380, acc.: 100.00%] [G loss: 4.070275]\n",
      "epoch:17 step:13472 [D loss: 0.011161, acc.: 100.00%] [G loss: 2.492023]\n",
      "epoch:17 step:13473 [D loss: 0.006189, acc.: 100.00%] [G loss: 2.416930]\n",
      "epoch:17 step:13474 [D loss: 0.024086, acc.: 100.00%] [G loss: 0.160346]\n",
      "epoch:17 step:13475 [D loss: 0.011565, acc.: 100.00%] [G loss: 2.495418]\n",
      "epoch:17 step:13476 [D loss: 0.046467, acc.: 99.22%] [G loss: 0.201360]\n",
      "epoch:17 step:13477 [D loss: 0.053045, acc.: 100.00%] [G loss: 2.878470]\n",
      "epoch:17 step:13478 [D loss: 0.019195, acc.: 100.00%] [G loss: 3.121990]\n",
      "epoch:17 step:13479 [D loss: 0.002715, acc.: 100.00%] [G loss: 2.629405]\n",
      "epoch:17 step:13480 [D loss: 0.055132, acc.: 97.66%] [G loss: 2.337379]\n",
      "epoch:17 step:13481 [D loss: 0.144855, acc.: 95.31%] [G loss: 4.266793]\n",
      "epoch:17 step:13482 [D loss: 0.073826, acc.: 96.88%] [G loss: 4.010878]\n",
      "epoch:17 step:13483 [D loss: 0.139929, acc.: 93.75%] [G loss: 2.940622]\n",
      "epoch:17 step:13484 [D loss: 0.073493, acc.: 98.44%] [G loss: 2.833643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13485 [D loss: 0.005455, acc.: 100.00%] [G loss: 3.433794]\n",
      "epoch:17 step:13486 [D loss: 0.250347, acc.: 90.62%] [G loss: 3.881741]\n",
      "epoch:17 step:13487 [D loss: 0.008679, acc.: 100.00%] [G loss: 5.307640]\n",
      "epoch:17 step:13488 [D loss: 0.002954, acc.: 100.00%] [G loss: 5.322159]\n",
      "epoch:17 step:13489 [D loss: 0.010835, acc.: 100.00%] [G loss: 5.958659]\n",
      "epoch:17 step:13490 [D loss: 0.011929, acc.: 100.00%] [G loss: 4.792365]\n",
      "epoch:17 step:13491 [D loss: 0.097025, acc.: 96.09%] [G loss: 0.462065]\n",
      "epoch:17 step:13492 [D loss: 0.089254, acc.: 98.44%] [G loss: 0.564703]\n",
      "epoch:17 step:13493 [D loss: 0.010739, acc.: 100.00%] [G loss: 1.001360]\n",
      "epoch:17 step:13494 [D loss: 0.013810, acc.: 100.00%] [G loss: 5.626220]\n",
      "epoch:17 step:13495 [D loss: 0.037749, acc.: 99.22%] [G loss: 5.711505]\n",
      "epoch:17 step:13496 [D loss: 0.033748, acc.: 100.00%] [G loss: 0.268107]\n",
      "epoch:17 step:13497 [D loss: 0.012875, acc.: 100.00%] [G loss: 5.398973]\n",
      "epoch:17 step:13498 [D loss: 0.010482, acc.: 100.00%] [G loss: 5.841802]\n",
      "epoch:17 step:13499 [D loss: 0.012273, acc.: 100.00%] [G loss: 5.461027]\n",
      "epoch:17 step:13500 [D loss: 0.005324, acc.: 100.00%] [G loss: 0.538030]\n",
      "epoch:17 step:13501 [D loss: 0.006439, acc.: 100.00%] [G loss: 0.548004]\n",
      "epoch:17 step:13502 [D loss: 0.028449, acc.: 100.00%] [G loss: 5.802756]\n",
      "epoch:17 step:13503 [D loss: 0.014544, acc.: 100.00%] [G loss: 5.957071]\n",
      "epoch:17 step:13504 [D loss: 0.004166, acc.: 100.00%] [G loss: 0.331932]\n",
      "epoch:17 step:13505 [D loss: 0.069496, acc.: 96.88%] [G loss: 6.090158]\n",
      "epoch:17 step:13506 [D loss: 0.224650, acc.: 92.19%] [G loss: 0.263909]\n",
      "epoch:17 step:13507 [D loss: 0.704071, acc.: 64.84%] [G loss: 6.653326]\n",
      "epoch:17 step:13508 [D loss: 1.981672, acc.: 51.56%] [G loss: 2.556655]\n",
      "epoch:17 step:13509 [D loss: 0.845074, acc.: 65.62%] [G loss: 1.515941]\n",
      "epoch:17 step:13510 [D loss: 0.136113, acc.: 97.66%] [G loss: 2.190588]\n",
      "epoch:17 step:13511 [D loss: 0.073896, acc.: 98.44%] [G loss: 2.581765]\n",
      "epoch:17 step:13512 [D loss: 0.023889, acc.: 100.00%] [G loss: 0.088097]\n",
      "epoch:17 step:13513 [D loss: 0.070314, acc.: 97.66%] [G loss: 0.058177]\n",
      "epoch:17 step:13514 [D loss: 0.025534, acc.: 100.00%] [G loss: 0.151330]\n",
      "epoch:17 step:13515 [D loss: 0.011893, acc.: 100.00%] [G loss: 1.951922]\n",
      "epoch:17 step:13516 [D loss: 0.040933, acc.: 100.00%] [G loss: 2.975963]\n",
      "epoch:17 step:13517 [D loss: 0.039430, acc.: 100.00%] [G loss: 0.801097]\n",
      "epoch:17 step:13518 [D loss: 0.086274, acc.: 98.44%] [G loss: 0.249182]\n",
      "epoch:17 step:13519 [D loss: 0.031363, acc.: 99.22%] [G loss: 0.420988]\n",
      "epoch:17 step:13520 [D loss: 0.062472, acc.: 97.66%] [G loss: 0.079305]\n",
      "epoch:17 step:13521 [D loss: 0.079056, acc.: 99.22%] [G loss: 0.073538]\n",
      "epoch:17 step:13522 [D loss: 0.027456, acc.: 99.22%] [G loss: 1.601772]\n",
      "epoch:17 step:13523 [D loss: 0.076410, acc.: 96.88%] [G loss: 0.243451]\n",
      "epoch:17 step:13524 [D loss: 0.018595, acc.: 100.00%] [G loss: 0.164653]\n",
      "epoch:17 step:13525 [D loss: 0.435729, acc.: 80.47%] [G loss: 3.917032]\n",
      "epoch:17 step:13526 [D loss: 0.349385, acc.: 83.59%] [G loss: 1.394744]\n",
      "epoch:17 step:13527 [D loss: 0.129686, acc.: 94.53%] [G loss: 0.483501]\n",
      "epoch:17 step:13528 [D loss: 0.009904, acc.: 100.00%] [G loss: 0.185304]\n",
      "epoch:17 step:13529 [D loss: 0.002810, acc.: 100.00%] [G loss: 0.131134]\n",
      "epoch:17 step:13530 [D loss: 0.100347, acc.: 96.09%] [G loss: 4.342746]\n",
      "epoch:17 step:13531 [D loss: 0.003371, acc.: 100.00%] [G loss: 1.124871]\n",
      "epoch:17 step:13532 [D loss: 0.002546, acc.: 100.00%] [G loss: 4.149234]\n",
      "epoch:17 step:13533 [D loss: 0.008436, acc.: 100.00%] [G loss: 0.630277]\n",
      "epoch:17 step:13534 [D loss: 0.006971, acc.: 100.00%] [G loss: 0.312327]\n",
      "epoch:17 step:13535 [D loss: 0.005044, acc.: 100.00%] [G loss: 0.238554]\n",
      "epoch:17 step:13536 [D loss: 0.002427, acc.: 100.00%] [G loss: 0.181084]\n",
      "epoch:17 step:13537 [D loss: 0.008864, acc.: 100.00%] [G loss: 2.978757]\n",
      "epoch:17 step:13538 [D loss: 0.055557, acc.: 97.66%] [G loss: 2.368938]\n",
      "epoch:17 step:13539 [D loss: 0.011456, acc.: 100.00%] [G loss: 0.029146]\n",
      "epoch:17 step:13540 [D loss: 0.002807, acc.: 100.00%] [G loss: 0.036268]\n",
      "epoch:17 step:13541 [D loss: 0.045669, acc.: 99.22%] [G loss: 0.082696]\n",
      "epoch:17 step:13542 [D loss: 0.032841, acc.: 100.00%] [G loss: 2.729935]\n",
      "epoch:17 step:13543 [D loss: 0.002830, acc.: 100.00%] [G loss: 3.221384]\n",
      "epoch:17 step:13544 [D loss: 0.023319, acc.: 100.00%] [G loss: 0.182839]\n",
      "epoch:17 step:13545 [D loss: 0.029815, acc.: 99.22%] [G loss: 0.882550]\n",
      "epoch:17 step:13546 [D loss: 0.005576, acc.: 100.00%] [G loss: 2.026557]\n",
      "epoch:17 step:13547 [D loss: 0.086181, acc.: 96.09%] [G loss: 0.467010]\n",
      "epoch:17 step:13548 [D loss: 0.032208, acc.: 99.22%] [G loss: 2.264448]\n",
      "epoch:17 step:13549 [D loss: 0.005013, acc.: 100.00%] [G loss: 2.228918]\n",
      "epoch:17 step:13550 [D loss: 0.024800, acc.: 100.00%] [G loss: 0.449931]\n",
      "epoch:17 step:13551 [D loss: 0.044025, acc.: 100.00%] [G loss: 2.491324]\n",
      "epoch:17 step:13552 [D loss: 0.079868, acc.: 97.66%] [G loss: 1.674543]\n",
      "epoch:17 step:13553 [D loss: 0.551109, acc.: 74.22%] [G loss: 4.326089]\n",
      "epoch:17 step:13554 [D loss: 1.465836, acc.: 53.12%] [G loss: 3.925823]\n",
      "epoch:17 step:13555 [D loss: 0.124381, acc.: 96.88%] [G loss: 4.498331]\n",
      "epoch:17 step:13556 [D loss: 0.077978, acc.: 96.88%] [G loss: 3.534687]\n",
      "epoch:17 step:13557 [D loss: 0.145910, acc.: 94.53%] [G loss: 5.331111]\n",
      "epoch:17 step:13558 [D loss: 0.049943, acc.: 99.22%] [G loss: 3.630743]\n",
      "epoch:17 step:13559 [D loss: 0.044966, acc.: 97.66%] [G loss: 4.889013]\n",
      "epoch:17 step:13560 [D loss: 0.023319, acc.: 99.22%] [G loss: 2.683395]\n",
      "epoch:17 step:13561 [D loss: 0.046343, acc.: 99.22%] [G loss: 3.575658]\n",
      "epoch:17 step:13562 [D loss: 0.017665, acc.: 100.00%] [G loss: 2.996143]\n",
      "epoch:17 step:13563 [D loss: 0.053569, acc.: 100.00%] [G loss: 1.964042]\n",
      "epoch:17 step:13564 [D loss: 0.038257, acc.: 100.00%] [G loss: 3.462297]\n",
      "epoch:17 step:13565 [D loss: 0.014468, acc.: 100.00%] [G loss: 4.619967]\n",
      "epoch:17 step:13566 [D loss: 0.023697, acc.: 100.00%] [G loss: 3.629529]\n",
      "epoch:17 step:13567 [D loss: 0.030621, acc.: 100.00%] [G loss: 2.008736]\n",
      "epoch:17 step:13568 [D loss: 0.104425, acc.: 99.22%] [G loss: 3.001502]\n",
      "epoch:17 step:13569 [D loss: 0.041286, acc.: 100.00%] [G loss: 2.517725]\n",
      "epoch:17 step:13570 [D loss: 0.102962, acc.: 96.88%] [G loss: 3.201284]\n",
      "epoch:17 step:13571 [D loss: 0.164385, acc.: 94.53%] [G loss: 2.582373]\n",
      "epoch:17 step:13572 [D loss: 0.080309, acc.: 96.09%] [G loss: 5.466746]\n",
      "epoch:17 step:13573 [D loss: 0.011245, acc.: 100.00%] [G loss: 2.008777]\n",
      "epoch:17 step:13574 [D loss: 0.023177, acc.: 100.00%] [G loss: 2.164999]\n",
      "epoch:17 step:13575 [D loss: 0.015514, acc.: 100.00%] [G loss: 1.737122]\n",
      "epoch:17 step:13576 [D loss: 0.027208, acc.: 99.22%] [G loss: 4.877877]\n",
      "epoch:17 step:13577 [D loss: 0.015239, acc.: 100.00%] [G loss: 3.458003]\n",
      "epoch:17 step:13578 [D loss: 0.019020, acc.: 99.22%] [G loss: 4.589384]\n",
      "epoch:17 step:13579 [D loss: 0.033568, acc.: 99.22%] [G loss: 1.755409]\n",
      "epoch:17 step:13580 [D loss: 0.105799, acc.: 98.44%] [G loss: 0.999868]\n",
      "epoch:17 step:13581 [D loss: 0.012094, acc.: 100.00%] [G loss: 2.678107]\n",
      "epoch:17 step:13582 [D loss: 0.096348, acc.: 96.88%] [G loss: 2.947383]\n",
      "epoch:17 step:13583 [D loss: 0.033808, acc.: 100.00%] [G loss: 4.070298]\n",
      "epoch:17 step:13584 [D loss: 0.084513, acc.: 99.22%] [G loss: 1.481132]\n",
      "epoch:17 step:13585 [D loss: 0.065951, acc.: 98.44%] [G loss: 2.155486]\n",
      "epoch:17 step:13586 [D loss: 0.035569, acc.: 100.00%] [G loss: 5.217044]\n",
      "epoch:17 step:13587 [D loss: 0.019567, acc.: 100.00%] [G loss: 0.996270]\n",
      "epoch:17 step:13588 [D loss: 0.010247, acc.: 100.00%] [G loss: 0.854414]\n",
      "epoch:17 step:13589 [D loss: 0.020478, acc.: 100.00%] [G loss: 0.256725]\n",
      "epoch:17 step:13590 [D loss: 0.003761, acc.: 100.00%] [G loss: 0.242389]\n",
      "epoch:17 step:13591 [D loss: 0.020139, acc.: 100.00%] [G loss: 3.699222]\n",
      "epoch:17 step:13592 [D loss: 0.363641, acc.: 83.59%] [G loss: 5.218925]\n",
      "epoch:17 step:13593 [D loss: 0.196663, acc.: 89.84%] [G loss: 1.831003]\n",
      "epoch:17 step:13594 [D loss: 0.003161, acc.: 100.00%] [G loss: 1.569988]\n",
      "epoch:17 step:13595 [D loss: 0.007449, acc.: 100.00%] [G loss: 2.937087]\n",
      "epoch:17 step:13596 [D loss: 0.002581, acc.: 100.00%] [G loss: 1.805781]\n",
      "epoch:17 step:13597 [D loss: 0.076663, acc.: 96.88%] [G loss: 0.643129]\n",
      "epoch:17 step:13598 [D loss: 0.017469, acc.: 99.22%] [G loss: 0.122882]\n",
      "epoch:17 step:13599 [D loss: 0.048106, acc.: 99.22%] [G loss: 0.302842]\n",
      "epoch:17 step:13600 [D loss: 0.000994, acc.: 100.00%] [G loss: 4.806848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13601 [D loss: 0.005475, acc.: 100.00%] [G loss: 1.596364]\n",
      "epoch:17 step:13602 [D loss: 0.065188, acc.: 97.66%] [G loss: 4.155056]\n",
      "epoch:17 step:13603 [D loss: 0.005767, acc.: 100.00%] [G loss: 6.015939]\n",
      "epoch:17 step:13604 [D loss: 0.035475, acc.: 99.22%] [G loss: 2.354108]\n",
      "epoch:17 step:13605 [D loss: 0.005891, acc.: 100.00%] [G loss: 1.872544]\n",
      "epoch:17 step:13606 [D loss: 0.005852, acc.: 100.00%] [G loss: 3.670446]\n",
      "epoch:17 step:13607 [D loss: 0.004343, acc.: 100.00%] [G loss: 3.663440]\n",
      "epoch:17 step:13608 [D loss: 0.008441, acc.: 100.00%] [G loss: 1.378661]\n",
      "epoch:17 step:13609 [D loss: 0.101110, acc.: 96.09%] [G loss: 6.003553]\n",
      "epoch:17 step:13610 [D loss: 0.007397, acc.: 100.00%] [G loss: 5.317439]\n",
      "epoch:17 step:13611 [D loss: 0.085665, acc.: 96.88%] [G loss: 2.170713]\n",
      "epoch:17 step:13612 [D loss: 0.004249, acc.: 100.00%] [G loss: 1.708166]\n",
      "epoch:17 step:13613 [D loss: 0.024432, acc.: 100.00%] [G loss: 1.721521]\n",
      "epoch:17 step:13614 [D loss: 0.002926, acc.: 100.00%] [G loss: 2.228868]\n",
      "epoch:17 step:13615 [D loss: 0.029873, acc.: 99.22%] [G loss: 1.265086]\n",
      "epoch:17 step:13616 [D loss: 0.004107, acc.: 100.00%] [G loss: 1.511793]\n",
      "epoch:17 step:13617 [D loss: 0.087843, acc.: 98.44%] [G loss: 0.870554]\n",
      "epoch:17 step:13618 [D loss: 0.043695, acc.: 99.22%] [G loss: 1.381065]\n",
      "epoch:17 step:13619 [D loss: 0.001905, acc.: 100.00%] [G loss: 4.593601]\n",
      "epoch:17 step:13620 [D loss: 0.011390, acc.: 100.00%] [G loss: 3.821264]\n",
      "epoch:17 step:13621 [D loss: 0.002381, acc.: 100.00%] [G loss: 3.045016]\n",
      "epoch:17 step:13622 [D loss: 0.005154, acc.: 100.00%] [G loss: 3.589486]\n",
      "epoch:17 step:13623 [D loss: 0.021328, acc.: 100.00%] [G loss: 2.892138]\n",
      "epoch:17 step:13624 [D loss: 0.011469, acc.: 100.00%] [G loss: 0.711630]\n",
      "epoch:17 step:13625 [D loss: 0.004109, acc.: 100.00%] [G loss: 2.415689]\n",
      "epoch:17 step:13626 [D loss: 0.007283, acc.: 100.00%] [G loss: 3.135028]\n",
      "epoch:17 step:13627 [D loss: 0.048639, acc.: 100.00%] [G loss: 0.885492]\n",
      "epoch:17 step:13628 [D loss: 0.002369, acc.: 100.00%] [G loss: 3.840714]\n",
      "epoch:17 step:13629 [D loss: 0.003536, acc.: 100.00%] [G loss: 3.171556]\n",
      "epoch:17 step:13630 [D loss: 0.010665, acc.: 100.00%] [G loss: 4.156468]\n",
      "epoch:17 step:13631 [D loss: 0.015306, acc.: 100.00%] [G loss: 0.036009]\n",
      "epoch:17 step:13632 [D loss: 0.018923, acc.: 100.00%] [G loss: 2.557024]\n",
      "epoch:17 step:13633 [D loss: 0.059333, acc.: 98.44%] [G loss: 6.143245]\n",
      "epoch:17 step:13634 [D loss: 0.002281, acc.: 100.00%] [G loss: 7.871202]\n",
      "epoch:17 step:13635 [D loss: 0.112499, acc.: 95.31%] [G loss: 0.306624]\n",
      "epoch:17 step:13636 [D loss: 0.018294, acc.: 100.00%] [G loss: 6.106279]\n",
      "epoch:17 step:13637 [D loss: 0.000878, acc.: 100.00%] [G loss: 5.606600]\n",
      "epoch:17 step:13638 [D loss: 0.004415, acc.: 100.00%] [G loss: 0.567948]\n",
      "epoch:17 step:13639 [D loss: 0.007103, acc.: 100.00%] [G loss: 0.498749]\n",
      "epoch:17 step:13640 [D loss: 0.024278, acc.: 100.00%] [G loss: 1.352909]\n",
      "epoch:17 step:13641 [D loss: 0.015689, acc.: 100.00%] [G loss: 6.090669]\n",
      "epoch:17 step:13642 [D loss: 0.009602, acc.: 100.00%] [G loss: 5.197224]\n",
      "epoch:17 step:13643 [D loss: 0.011662, acc.: 100.00%] [G loss: 5.838487]\n",
      "epoch:17 step:13644 [D loss: 0.002437, acc.: 100.00%] [G loss: 1.975272]\n",
      "epoch:17 step:13645 [D loss: 0.009077, acc.: 100.00%] [G loss: 0.241821]\n",
      "epoch:17 step:13646 [D loss: 0.064316, acc.: 99.22%] [G loss: 5.260886]\n",
      "epoch:17 step:13647 [D loss: 0.067802, acc.: 97.66%] [G loss: 5.046486]\n",
      "epoch:17 step:13648 [D loss: 0.003649, acc.: 100.00%] [G loss: 4.772270]\n",
      "epoch:17 step:13649 [D loss: 0.001456, acc.: 100.00%] [G loss: 4.046059]\n",
      "epoch:17 step:13650 [D loss: 0.000238, acc.: 100.00%] [G loss: 2.616867]\n",
      "epoch:17 step:13651 [D loss: 0.001075, acc.: 100.00%] [G loss: 3.946202]\n",
      "epoch:17 step:13652 [D loss: 0.012093, acc.: 100.00%] [G loss: 1.232861]\n",
      "epoch:17 step:13653 [D loss: 0.003081, acc.: 100.00%] [G loss: 2.110831]\n",
      "epoch:17 step:13654 [D loss: 0.012101, acc.: 100.00%] [G loss: 0.650890]\n",
      "epoch:17 step:13655 [D loss: 0.002665, acc.: 100.00%] [G loss: 1.461822]\n",
      "epoch:17 step:13656 [D loss: 0.002834, acc.: 100.00%] [G loss: 1.351956]\n",
      "epoch:17 step:13657 [D loss: 0.006404, acc.: 100.00%] [G loss: 0.845034]\n",
      "epoch:17 step:13658 [D loss: 0.041147, acc.: 100.00%] [G loss: 3.257587]\n",
      "epoch:17 step:13659 [D loss: 0.034696, acc.: 100.00%] [G loss: 2.984876]\n",
      "epoch:17 step:13660 [D loss: 0.000808, acc.: 100.00%] [G loss: 2.589032]\n",
      "epoch:17 step:13661 [D loss: 0.037162, acc.: 99.22%] [G loss: 2.934316]\n",
      "epoch:17 step:13662 [D loss: 0.010218, acc.: 100.00%] [G loss: 6.261609]\n",
      "epoch:17 step:13663 [D loss: 0.088386, acc.: 97.66%] [G loss: 4.740429]\n",
      "epoch:17 step:13664 [D loss: 0.001632, acc.: 100.00%] [G loss: 2.745736]\n",
      "epoch:17 step:13665 [D loss: 0.009492, acc.: 100.00%] [G loss: 4.837111]\n",
      "epoch:17 step:13666 [D loss: 0.003279, acc.: 100.00%] [G loss: 2.275155]\n",
      "epoch:17 step:13667 [D loss: 0.031746, acc.: 99.22%] [G loss: 4.235488]\n",
      "epoch:17 step:13668 [D loss: 0.012776, acc.: 100.00%] [G loss: 6.037047]\n",
      "epoch:17 step:13669 [D loss: 0.004444, acc.: 100.00%] [G loss: 4.860328]\n",
      "epoch:17 step:13670 [D loss: 0.001366, acc.: 100.00%] [G loss: 5.314117]\n",
      "epoch:17 step:13671 [D loss: 0.002595, acc.: 100.00%] [G loss: 0.976033]\n",
      "epoch:17 step:13672 [D loss: 0.099431, acc.: 96.88%] [G loss: 0.005993]\n",
      "epoch:17 step:13673 [D loss: 0.457927, acc.: 76.56%] [G loss: 7.535069]\n",
      "epoch:17 step:13674 [D loss: 3.777451, acc.: 50.00%] [G loss: 4.117022]\n",
      "epoch:17 step:13675 [D loss: 0.000996, acc.: 100.00%] [G loss: 5.625587]\n",
      "epoch:17 step:13676 [D loss: 0.000936, acc.: 100.00%] [G loss: 5.765036]\n",
      "epoch:17 step:13677 [D loss: 0.001062, acc.: 100.00%] [G loss: 2.567731]\n",
      "epoch:17 step:13678 [D loss: 0.025905, acc.: 99.22%] [G loss: 2.949775]\n",
      "epoch:17 step:13679 [D loss: 0.003766, acc.: 100.00%] [G loss: 3.932774]\n",
      "epoch:17 step:13680 [D loss: 0.008424, acc.: 100.00%] [G loss: 0.847635]\n",
      "epoch:17 step:13681 [D loss: 0.003920, acc.: 100.00%] [G loss: 1.242539]\n",
      "epoch:17 step:13682 [D loss: 0.112244, acc.: 96.09%] [G loss: 3.648269]\n",
      "epoch:17 step:13683 [D loss: 0.002677, acc.: 100.00%] [G loss: 3.196183]\n",
      "epoch:17 step:13684 [D loss: 0.004759, acc.: 100.00%] [G loss: 2.605266]\n",
      "epoch:17 step:13685 [D loss: 0.004874, acc.: 100.00%] [G loss: 3.192866]\n",
      "epoch:17 step:13686 [D loss: 0.013344, acc.: 100.00%] [G loss: 1.233408]\n",
      "epoch:17 step:13687 [D loss: 0.007354, acc.: 100.00%] [G loss: 2.649835]\n",
      "epoch:17 step:13688 [D loss: 0.173294, acc.: 93.75%] [G loss: 0.156045]\n",
      "epoch:17 step:13689 [D loss: 0.060949, acc.: 99.22%] [G loss: 0.470881]\n",
      "epoch:17 step:13690 [D loss: 0.014326, acc.: 100.00%] [G loss: 1.003060]\n",
      "epoch:17 step:13691 [D loss: 0.002127, acc.: 100.00%] [G loss: 1.436604]\n",
      "epoch:17 step:13692 [D loss: 0.002234, acc.: 100.00%] [G loss: 1.242004]\n",
      "epoch:17 step:13693 [D loss: 0.015703, acc.: 100.00%] [G loss: 0.544189]\n",
      "epoch:17 step:13694 [D loss: 0.003545, acc.: 100.00%] [G loss: 0.951715]\n",
      "epoch:17 step:13695 [D loss: 0.006428, acc.: 100.00%] [G loss: 3.365683]\n",
      "epoch:17 step:13696 [D loss: 0.002707, acc.: 100.00%] [G loss: 0.792210]\n",
      "epoch:17 step:13697 [D loss: 0.017086, acc.: 100.00%] [G loss: 0.097297]\n",
      "epoch:17 step:13698 [D loss: 0.015705, acc.: 100.00%] [G loss: 2.595248]\n",
      "epoch:17 step:13699 [D loss: 0.007690, acc.: 100.00%] [G loss: 2.529982]\n",
      "epoch:17 step:13700 [D loss: 0.007154, acc.: 100.00%] [G loss: 0.767044]\n",
      "epoch:17 step:13701 [D loss: 0.055332, acc.: 97.66%] [G loss: 0.215015]\n",
      "epoch:17 step:13702 [D loss: 0.144286, acc.: 95.31%] [G loss: 0.587700]\n",
      "epoch:17 step:13703 [D loss: 0.000791, acc.: 100.00%] [G loss: 4.978044]\n",
      "epoch:17 step:13704 [D loss: 0.001438, acc.: 100.00%] [G loss: 1.556324]\n",
      "epoch:17 step:13705 [D loss: 0.747257, acc.: 60.94%] [G loss: 4.514265]\n",
      "epoch:17 step:13706 [D loss: 0.013489, acc.: 100.00%] [G loss: 5.523518]\n",
      "epoch:17 step:13707 [D loss: 0.412826, acc.: 83.59%] [G loss: 1.065144]\n",
      "epoch:17 step:13708 [D loss: 0.113328, acc.: 96.09%] [G loss: 2.722104]\n",
      "epoch:17 step:13709 [D loss: 0.007455, acc.: 100.00%] [G loss: 3.223116]\n",
      "epoch:17 step:13710 [D loss: 0.009833, acc.: 100.00%] [G loss: 3.057805]\n",
      "epoch:17 step:13711 [D loss: 0.022516, acc.: 100.00%] [G loss: 3.751568]\n",
      "epoch:17 step:13712 [D loss: 0.009734, acc.: 100.00%] [G loss: 3.688549]\n",
      "epoch:17 step:13713 [D loss: 0.003540, acc.: 100.00%] [G loss: 4.445493]\n",
      "epoch:17 step:13714 [D loss: 0.004973, acc.: 100.00%] [G loss: 4.245898]\n",
      "epoch:17 step:13715 [D loss: 0.015140, acc.: 100.00%] [G loss: 4.212348]\n",
      "epoch:17 step:13716 [D loss: 0.011811, acc.: 100.00%] [G loss: 2.174196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13717 [D loss: 0.081648, acc.: 99.22%] [G loss: 3.508748]\n",
      "epoch:17 step:13718 [D loss: 0.043948, acc.: 97.66%] [G loss: 3.182791]\n",
      "epoch:17 step:13719 [D loss: 0.017611, acc.: 99.22%] [G loss: 4.510900]\n",
      "epoch:17 step:13720 [D loss: 0.031221, acc.: 100.00%] [G loss: 0.491817]\n",
      "epoch:17 step:13721 [D loss: 0.022733, acc.: 100.00%] [G loss: 0.922389]\n",
      "epoch:17 step:13722 [D loss: 0.015740, acc.: 100.00%] [G loss: 1.122897]\n",
      "epoch:17 step:13723 [D loss: 0.019137, acc.: 100.00%] [G loss: 1.340765]\n",
      "epoch:17 step:13724 [D loss: 0.023871, acc.: 100.00%] [G loss: 4.800042]\n",
      "epoch:17 step:13725 [D loss: 0.006070, acc.: 100.00%] [G loss: 1.082531]\n",
      "epoch:17 step:13726 [D loss: 0.001264, acc.: 100.00%] [G loss: 2.534859]\n",
      "epoch:17 step:13727 [D loss: 0.001584, acc.: 100.00%] [G loss: 0.543652]\n",
      "epoch:17 step:13728 [D loss: 0.005894, acc.: 100.00%] [G loss: 0.310033]\n",
      "epoch:17 step:13729 [D loss: 0.000626, acc.: 100.00%] [G loss: 1.867354]\n",
      "epoch:17 step:13730 [D loss: 0.040508, acc.: 100.00%] [G loss: 0.321389]\n",
      "epoch:17 step:13731 [D loss: 0.001092, acc.: 100.00%] [G loss: 0.634770]\n",
      "epoch:17 step:13732 [D loss: 0.002081, acc.: 100.00%] [G loss: 0.170264]\n",
      "epoch:17 step:13733 [D loss: 0.001824, acc.: 100.00%] [G loss: 1.064294]\n",
      "epoch:17 step:13734 [D loss: 0.000369, acc.: 100.00%] [G loss: 4.163045]\n",
      "epoch:17 step:13735 [D loss: 0.000997, acc.: 100.00%] [G loss: 1.079761]\n",
      "epoch:17 step:13736 [D loss: 0.076317, acc.: 97.66%] [G loss: 0.115360]\n",
      "epoch:17 step:13737 [D loss: 0.146227, acc.: 94.53%] [G loss: 0.024455]\n",
      "epoch:17 step:13738 [D loss: 0.003600, acc.: 100.00%] [G loss: 0.011226]\n",
      "epoch:17 step:13739 [D loss: 0.001898, acc.: 100.00%] [G loss: 0.004094]\n",
      "epoch:17 step:13740 [D loss: 0.002854, acc.: 100.00%] [G loss: 0.269174]\n",
      "epoch:17 step:13741 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.035723]\n",
      "epoch:17 step:13742 [D loss: 0.000979, acc.: 100.00%] [G loss: 0.001895]\n",
      "epoch:17 step:13743 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.019427]\n",
      "epoch:17 step:13744 [D loss: 0.001794, acc.: 100.00%] [G loss: 0.008546]\n",
      "epoch:17 step:13745 [D loss: 0.004054, acc.: 100.00%] [G loss: 0.097742]\n",
      "epoch:17 step:13746 [D loss: 0.001063, acc.: 100.00%] [G loss: 0.023599]\n",
      "epoch:17 step:13747 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.031278]\n",
      "epoch:17 step:13748 [D loss: 0.000486, acc.: 100.00%] [G loss: 0.023515]\n",
      "epoch:17 step:13749 [D loss: 0.008631, acc.: 100.00%] [G loss: 0.007875]\n",
      "epoch:17 step:13750 [D loss: 0.005090, acc.: 100.00%] [G loss: 0.005465]\n",
      "epoch:17 step:13751 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.527639]\n",
      "epoch:17 step:13752 [D loss: 0.001770, acc.: 100.00%] [G loss: 0.015975]\n",
      "epoch:17 step:13753 [D loss: 0.001495, acc.: 100.00%] [G loss: 0.115849]\n",
      "epoch:17 step:13754 [D loss: 0.025980, acc.: 100.00%] [G loss: 0.344296]\n",
      "epoch:17 step:13755 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.169281]\n",
      "epoch:17 step:13756 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.159770]\n",
      "epoch:17 step:13757 [D loss: 0.001427, acc.: 100.00%] [G loss: 0.120702]\n",
      "epoch:17 step:13758 [D loss: 0.000460, acc.: 100.00%] [G loss: 0.982627]\n",
      "epoch:17 step:13759 [D loss: 0.000876, acc.: 100.00%] [G loss: 0.086458]\n",
      "epoch:17 step:13760 [D loss: 0.000667, acc.: 100.00%] [G loss: 1.158518]\n",
      "epoch:17 step:13761 [D loss: 0.000428, acc.: 100.00%] [G loss: 0.039241]\n",
      "epoch:17 step:13762 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.111469]\n",
      "epoch:17 step:13763 [D loss: 0.021536, acc.: 100.00%] [G loss: 0.170836]\n",
      "epoch:17 step:13764 [D loss: 0.004148, acc.: 100.00%] [G loss: 0.487428]\n",
      "epoch:17 step:13765 [D loss: 0.002965, acc.: 100.00%] [G loss: 0.088377]\n",
      "epoch:17 step:13766 [D loss: 0.002753, acc.: 100.00%] [G loss: 0.375054]\n",
      "epoch:17 step:13767 [D loss: 0.000675, acc.: 100.00%] [G loss: 0.215842]\n",
      "epoch:17 step:13768 [D loss: 0.000942, acc.: 100.00%] [G loss: 0.345733]\n",
      "epoch:17 step:13769 [D loss: 0.040295, acc.: 99.22%] [G loss: 0.358968]\n",
      "epoch:17 step:13770 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.221990]\n",
      "epoch:17 step:13771 [D loss: 0.001096, acc.: 100.00%] [G loss: 0.928062]\n",
      "epoch:17 step:13772 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.435119]\n",
      "epoch:17 step:13773 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.190054]\n",
      "epoch:17 step:13774 [D loss: 0.022983, acc.: 100.00%] [G loss: 0.175094]\n",
      "epoch:17 step:13775 [D loss: 0.049506, acc.: 99.22%] [G loss: 0.079627]\n",
      "epoch:17 step:13776 [D loss: 0.003415, acc.: 100.00%] [G loss: 0.007492]\n",
      "epoch:17 step:13777 [D loss: 0.003681, acc.: 100.00%] [G loss: 0.112656]\n",
      "epoch:17 step:13778 [D loss: 0.001886, acc.: 100.00%] [G loss: 0.018328]\n",
      "epoch:17 step:13779 [D loss: 0.002203, acc.: 100.00%] [G loss: 0.031514]\n",
      "epoch:17 step:13780 [D loss: 0.005106, acc.: 100.00%] [G loss: 1.529111]\n",
      "epoch:17 step:13781 [D loss: 0.005737, acc.: 100.00%] [G loss: 0.142512]\n",
      "epoch:17 step:13782 [D loss: 0.001783, acc.: 100.00%] [G loss: 0.036786]\n",
      "epoch:17 step:13783 [D loss: 0.002826, acc.: 100.00%] [G loss: 0.059032]\n",
      "epoch:17 step:13784 [D loss: 0.011155, acc.: 100.00%] [G loss: 0.593158]\n",
      "epoch:17 step:13785 [D loss: 0.051820, acc.: 100.00%] [G loss: 1.770136]\n",
      "epoch:17 step:13786 [D loss: 1.909938, acc.: 35.16%] [G loss: 10.901545]\n",
      "epoch:17 step:13787 [D loss: 3.650427, acc.: 50.00%] [G loss: 6.300481]\n",
      "epoch:17 step:13788 [D loss: 2.130734, acc.: 50.00%] [G loss: 3.636025]\n",
      "epoch:17 step:13789 [D loss: 0.223584, acc.: 92.19%] [G loss: 0.376317]\n",
      "epoch:17 step:13790 [D loss: 0.185258, acc.: 93.75%] [G loss: 1.685202]\n",
      "epoch:17 step:13791 [D loss: 0.073032, acc.: 100.00%] [G loss: 1.698226]\n",
      "epoch:17 step:13792 [D loss: 0.123105, acc.: 96.88%] [G loss: 0.645561]\n",
      "epoch:17 step:13793 [D loss: 0.055365, acc.: 100.00%] [G loss: 1.373791]\n",
      "epoch:17 step:13794 [D loss: 0.140912, acc.: 99.22%] [G loss: 0.660113]\n",
      "epoch:17 step:13795 [D loss: 0.035287, acc.: 100.00%] [G loss: 0.770096]\n",
      "epoch:17 step:13796 [D loss: 0.054042, acc.: 100.00%] [G loss: 0.827416]\n",
      "epoch:17 step:13797 [D loss: 0.088712, acc.: 97.66%] [G loss: 2.118809]\n",
      "epoch:17 step:13798 [D loss: 0.046166, acc.: 99.22%] [G loss: 0.407711]\n",
      "epoch:17 step:13799 [D loss: 0.126719, acc.: 97.66%] [G loss: 0.938470]\n",
      "epoch:17 step:13800 [D loss: 0.131866, acc.: 96.09%] [G loss: 2.480922]\n",
      "epoch:17 step:13801 [D loss: 0.090794, acc.: 98.44%] [G loss: 0.972608]\n",
      "epoch:17 step:13802 [D loss: 0.065812, acc.: 99.22%] [G loss: 1.097020]\n",
      "epoch:17 step:13803 [D loss: 0.602372, acc.: 70.31%] [G loss: 2.984297]\n",
      "epoch:17 step:13804 [D loss: 0.057793, acc.: 96.88%] [G loss: 4.140519]\n",
      "epoch:17 step:13805 [D loss: 0.152955, acc.: 90.62%] [G loss: 4.033313]\n",
      "epoch:17 step:13806 [D loss: 0.022351, acc.: 100.00%] [G loss: 1.853870]\n",
      "epoch:17 step:13807 [D loss: 0.029695, acc.: 100.00%] [G loss: 2.626704]\n",
      "epoch:17 step:13808 [D loss: 0.013591, acc.: 100.00%] [G loss: 2.047892]\n",
      "epoch:17 step:13809 [D loss: 0.028813, acc.: 100.00%] [G loss: 1.613310]\n",
      "epoch:17 step:13810 [D loss: 0.018009, acc.: 100.00%] [G loss: 3.129463]\n",
      "epoch:17 step:13811 [D loss: 0.036211, acc.: 100.00%] [G loss: 2.254338]\n",
      "epoch:17 step:13812 [D loss: 0.040234, acc.: 100.00%] [G loss: 1.419505]\n",
      "epoch:17 step:13813 [D loss: 0.025377, acc.: 100.00%] [G loss: 1.286484]\n",
      "epoch:17 step:13814 [D loss: 0.254872, acc.: 89.06%] [G loss: 1.731713]\n",
      "epoch:17 step:13815 [D loss: 0.009271, acc.: 100.00%] [G loss: 5.263966]\n",
      "epoch:17 step:13816 [D loss: 0.031468, acc.: 99.22%] [G loss: 4.172677]\n",
      "epoch:17 step:13817 [D loss: 0.024097, acc.: 100.00%] [G loss: 3.529295]\n",
      "epoch:17 step:13818 [D loss: 0.012560, acc.: 100.00%] [G loss: 3.349688]\n",
      "epoch:17 step:13819 [D loss: 0.016362, acc.: 100.00%] [G loss: 4.617213]\n",
      "epoch:17 step:13820 [D loss: 0.011766, acc.: 100.00%] [G loss: 3.852810]\n",
      "epoch:17 step:13821 [D loss: 0.033458, acc.: 100.00%] [G loss: 3.629714]\n",
      "epoch:17 step:13822 [D loss: 0.056628, acc.: 99.22%] [G loss: 2.803741]\n",
      "epoch:17 step:13823 [D loss: 0.285439, acc.: 89.84%] [G loss: 3.547126]\n",
      "epoch:17 step:13824 [D loss: 0.011340, acc.: 100.00%] [G loss: 5.009030]\n",
      "epoch:17 step:13825 [D loss: 0.020112, acc.: 100.00%] [G loss: 4.186129]\n",
      "epoch:17 step:13826 [D loss: 0.009659, acc.: 100.00%] [G loss: 4.617202]\n",
      "epoch:17 step:13827 [D loss: 0.016169, acc.: 100.00%] [G loss: 4.427874]\n",
      "epoch:17 step:13828 [D loss: 0.024669, acc.: 100.00%] [G loss: 3.975320]\n",
      "epoch:17 step:13829 [D loss: 0.008942, acc.: 100.00%] [G loss: 0.064485]\n",
      "epoch:17 step:13830 [D loss: 0.031178, acc.: 99.22%] [G loss: 4.308267]\n",
      "epoch:17 step:13831 [D loss: 0.030742, acc.: 100.00%] [G loss: 0.615101]\n",
      "epoch:17 step:13832 [D loss: 0.036133, acc.: 100.00%] [G loss: 5.116848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13833 [D loss: 0.018462, acc.: 100.00%] [G loss: 5.623654]\n",
      "epoch:17 step:13834 [D loss: 0.066277, acc.: 98.44%] [G loss: 3.537095]\n",
      "epoch:17 step:13835 [D loss: 0.024517, acc.: 100.00%] [G loss: 4.160407]\n",
      "epoch:17 step:13836 [D loss: 0.028054, acc.: 100.00%] [G loss: 3.812672]\n",
      "epoch:17 step:13837 [D loss: 0.011876, acc.: 100.00%] [G loss: 0.507618]\n",
      "epoch:17 step:13838 [D loss: 0.015245, acc.: 100.00%] [G loss: 2.912219]\n",
      "epoch:17 step:13839 [D loss: 0.024247, acc.: 99.22%] [G loss: 4.279711]\n",
      "epoch:17 step:13840 [D loss: 0.070187, acc.: 99.22%] [G loss: 0.357158]\n",
      "epoch:17 step:13841 [D loss: 0.093524, acc.: 98.44%] [G loss: 1.462354]\n",
      "epoch:17 step:13842 [D loss: 0.041292, acc.: 100.00%] [G loss: 1.078958]\n",
      "epoch:17 step:13843 [D loss: 0.203700, acc.: 93.75%] [G loss: 0.196994]\n",
      "epoch:17 step:13844 [D loss: 0.257411, acc.: 89.84%] [G loss: 6.350516]\n",
      "epoch:17 step:13845 [D loss: 0.320616, acc.: 83.59%] [G loss: 3.130916]\n",
      "epoch:17 step:13846 [D loss: 0.010674, acc.: 100.00%] [G loss: 2.671031]\n",
      "epoch:17 step:13847 [D loss: 0.011184, acc.: 100.00%] [G loss: 1.402998]\n",
      "epoch:17 step:13848 [D loss: 0.018902, acc.: 100.00%] [G loss: 1.254715]\n",
      "epoch:17 step:13849 [D loss: 0.037449, acc.: 100.00%] [G loss: 4.509462]\n",
      "epoch:17 step:13850 [D loss: 0.041770, acc.: 98.44%] [G loss: 1.442785]\n",
      "epoch:17 step:13851 [D loss: 0.063248, acc.: 98.44%] [G loss: 2.344371]\n",
      "epoch:17 step:13852 [D loss: 0.169448, acc.: 95.31%] [G loss: 4.231981]\n",
      "epoch:17 step:13853 [D loss: 0.016598, acc.: 100.00%] [G loss: 2.423463]\n",
      "epoch:17 step:13854 [D loss: 0.999136, acc.: 60.94%] [G loss: 7.721943]\n",
      "epoch:17 step:13855 [D loss: 1.471756, acc.: 53.91%] [G loss: 1.021044]\n",
      "epoch:17 step:13856 [D loss: 0.140120, acc.: 93.75%] [G loss: 0.429247]\n",
      "epoch:17 step:13857 [D loss: 0.023671, acc.: 100.00%] [G loss: 4.053051]\n",
      "epoch:17 step:13858 [D loss: 0.013968, acc.: 100.00%] [G loss: 0.024350]\n",
      "epoch:17 step:13859 [D loss: 0.003213, acc.: 100.00%] [G loss: 0.062932]\n",
      "epoch:17 step:13860 [D loss: 0.005592, acc.: 100.00%] [G loss: 3.094284]\n",
      "epoch:17 step:13861 [D loss: 0.005874, acc.: 100.00%] [G loss: 0.070941]\n",
      "epoch:17 step:13862 [D loss: 0.006526, acc.: 100.00%] [G loss: 0.067088]\n",
      "epoch:17 step:13863 [D loss: 0.008565, acc.: 100.00%] [G loss: 0.037377]\n",
      "epoch:17 step:13864 [D loss: 0.015170, acc.: 100.00%] [G loss: 0.007327]\n",
      "epoch:17 step:13865 [D loss: 0.022593, acc.: 100.00%] [G loss: 3.437045]\n",
      "epoch:17 step:13866 [D loss: 0.006772, acc.: 100.00%] [G loss: 2.046885]\n",
      "epoch:17 step:13867 [D loss: 0.013188, acc.: 100.00%] [G loss: 0.003683]\n",
      "epoch:17 step:13868 [D loss: 0.143328, acc.: 93.75%] [G loss: 1.211507]\n",
      "epoch:17 step:13869 [D loss: 0.051743, acc.: 99.22%] [G loss: 3.992181]\n",
      "epoch:17 step:13870 [D loss: 0.180856, acc.: 94.53%] [G loss: 3.826966]\n",
      "epoch:17 step:13871 [D loss: 0.006101, acc.: 100.00%] [G loss: 3.958193]\n",
      "epoch:17 step:13872 [D loss: 0.011121, acc.: 100.00%] [G loss: 1.842299]\n",
      "epoch:17 step:13873 [D loss: 0.015237, acc.: 100.00%] [G loss: 1.443715]\n",
      "epoch:17 step:13874 [D loss: 0.070120, acc.: 98.44%] [G loss: 2.757702]\n",
      "epoch:17 step:13875 [D loss: 0.025603, acc.: 100.00%] [G loss: 0.748892]\n",
      "epoch:17 step:13876 [D loss: 0.093613, acc.: 98.44%] [G loss: 0.024531]\n",
      "epoch:17 step:13877 [D loss: 0.013528, acc.: 99.22%] [G loss: 0.028430]\n",
      "epoch:17 step:13878 [D loss: 0.154469, acc.: 96.09%] [G loss: 0.005538]\n",
      "epoch:17 step:13879 [D loss: 0.012072, acc.: 100.00%] [G loss: 0.008668]\n",
      "epoch:17 step:13880 [D loss: 0.024264, acc.: 99.22%] [G loss: 0.022678]\n",
      "epoch:17 step:13881 [D loss: 0.059863, acc.: 97.66%] [G loss: 0.001822]\n",
      "epoch:17 step:13882 [D loss: 0.087266, acc.: 97.66%] [G loss: 4.577477]\n",
      "epoch:17 step:13883 [D loss: 0.001923, acc.: 100.00%] [G loss: 0.098097]\n",
      "epoch:17 step:13884 [D loss: 0.042140, acc.: 99.22%] [G loss: 0.066567]\n",
      "epoch:17 step:13885 [D loss: 0.007773, acc.: 100.00%] [G loss: 4.674607]\n",
      "epoch:17 step:13886 [D loss: 0.004038, acc.: 100.00%] [G loss: 0.234857]\n",
      "epoch:17 step:13887 [D loss: 0.001719, acc.: 100.00%] [G loss: 0.273421]\n",
      "epoch:17 step:13888 [D loss: 0.008311, acc.: 100.00%] [G loss: 0.149938]\n",
      "epoch:17 step:13889 [D loss: 0.002566, acc.: 100.00%] [G loss: 0.037906]\n",
      "epoch:17 step:13890 [D loss: 0.070600, acc.: 96.88%] [G loss: 0.007753]\n",
      "epoch:17 step:13891 [D loss: 0.016098, acc.: 100.00%] [G loss: 0.001816]\n",
      "epoch:17 step:13892 [D loss: 0.105309, acc.: 96.88%] [G loss: 4.794298]\n",
      "epoch:17 step:13893 [D loss: 0.003769, acc.: 100.00%] [G loss: 3.050142]\n",
      "epoch:17 step:13894 [D loss: 0.002678, acc.: 100.00%] [G loss: 0.702813]\n",
      "epoch:17 step:13895 [D loss: 0.007412, acc.: 100.00%] [G loss: 0.817689]\n",
      "epoch:17 step:13896 [D loss: 0.053252, acc.: 98.44%] [G loss: 4.635210]\n",
      "epoch:17 step:13897 [D loss: 0.012502, acc.: 100.00%] [G loss: 0.283137]\n",
      "epoch:17 step:13898 [D loss: 0.001974, acc.: 100.00%] [G loss: 4.619837]\n",
      "epoch:17 step:13899 [D loss: 0.008962, acc.: 100.00%] [G loss: 3.120653]\n",
      "epoch:17 step:13900 [D loss: 0.284268, acc.: 87.50%] [G loss: 3.484504]\n",
      "epoch:17 step:13901 [D loss: 0.006530, acc.: 100.00%] [G loss: 0.600148]\n",
      "epoch:17 step:13902 [D loss: 0.019773, acc.: 100.00%] [G loss: 0.198739]\n",
      "epoch:17 step:13903 [D loss: 0.170884, acc.: 92.19%] [G loss: 2.801772]\n",
      "epoch:17 step:13904 [D loss: 0.042198, acc.: 99.22%] [G loss: 0.026767]\n",
      "epoch:17 step:13905 [D loss: 0.005473, acc.: 100.00%] [G loss: 0.022128]\n",
      "epoch:17 step:13906 [D loss: 0.004217, acc.: 100.00%] [G loss: 0.015451]\n",
      "epoch:17 step:13907 [D loss: 0.010567, acc.: 100.00%] [G loss: 0.043108]\n",
      "epoch:17 step:13908 [D loss: 0.040792, acc.: 99.22%] [G loss: 2.842805]\n",
      "epoch:17 step:13909 [D loss: 0.015008, acc.: 100.00%] [G loss: 0.277782]\n",
      "epoch:17 step:13910 [D loss: 0.011949, acc.: 100.00%] [G loss: 0.523001]\n",
      "epoch:17 step:13911 [D loss: 0.024412, acc.: 100.00%] [G loss: 0.412000]\n",
      "epoch:17 step:13912 [D loss: 0.020549, acc.: 100.00%] [G loss: 0.444494]\n",
      "epoch:17 step:13913 [D loss: 0.006434, acc.: 100.00%] [G loss: 0.547173]\n",
      "epoch:17 step:13914 [D loss: 0.029072, acc.: 100.00%] [G loss: 0.263619]\n",
      "epoch:17 step:13915 [D loss: 0.006847, acc.: 100.00%] [G loss: 0.267340]\n",
      "epoch:17 step:13916 [D loss: 0.042962, acc.: 99.22%] [G loss: 0.268375]\n",
      "epoch:17 step:13917 [D loss: 0.022633, acc.: 100.00%] [G loss: 1.858093]\n",
      "epoch:17 step:13918 [D loss: 0.003581, acc.: 100.00%] [G loss: 3.286718]\n",
      "epoch:17 step:13919 [D loss: 0.012906, acc.: 100.00%] [G loss: 4.001762]\n",
      "epoch:17 step:13920 [D loss: 0.024980, acc.: 99.22%] [G loss: 2.615435]\n",
      "epoch:17 step:13921 [D loss: 0.013731, acc.: 100.00%] [G loss: 0.587173]\n",
      "epoch:17 step:13922 [D loss: 0.005704, acc.: 100.00%] [G loss: 3.117949]\n",
      "epoch:17 step:13923 [D loss: 0.270665, acc.: 88.28%] [G loss: 0.482201]\n",
      "epoch:17 step:13924 [D loss: 0.003435, acc.: 100.00%] [G loss: 4.051558]\n",
      "epoch:17 step:13925 [D loss: 0.015595, acc.: 100.00%] [G loss: 0.932690]\n",
      "epoch:17 step:13926 [D loss: 0.018271, acc.: 100.00%] [G loss: 0.501831]\n",
      "epoch:17 step:13927 [D loss: 0.009778, acc.: 100.00%] [G loss: 0.559765]\n",
      "epoch:17 step:13928 [D loss: 0.016044, acc.: 100.00%] [G loss: 0.454725]\n",
      "epoch:17 step:13929 [D loss: 0.011934, acc.: 100.00%] [G loss: 1.165256]\n",
      "epoch:17 step:13930 [D loss: 0.019164, acc.: 99.22%] [G loss: 5.013321]\n",
      "epoch:17 step:13931 [D loss: 0.074218, acc.: 98.44%] [G loss: 3.938746]\n",
      "epoch:17 step:13932 [D loss: 0.014140, acc.: 100.00%] [G loss: 5.769573]\n",
      "epoch:17 step:13933 [D loss: 0.041344, acc.: 99.22%] [G loss: 2.516186]\n",
      "epoch:17 step:13934 [D loss: 0.007499, acc.: 100.00%] [G loss: 2.904461]\n",
      "epoch:17 step:13935 [D loss: 0.097293, acc.: 96.09%] [G loss: 2.146879]\n",
      "epoch:17 step:13936 [D loss: 0.128798, acc.: 96.09%] [G loss: 7.739025]\n",
      "epoch:17 step:13937 [D loss: 0.004201, acc.: 100.00%] [G loss: 7.788280]\n",
      "epoch:17 step:13938 [D loss: 0.003413, acc.: 100.00%] [G loss: 7.155365]\n",
      "epoch:17 step:13939 [D loss: 0.034040, acc.: 99.22%] [G loss: 6.682060]\n",
      "epoch:17 step:13940 [D loss: 0.005776, acc.: 100.00%] [G loss: 5.622198]\n",
      "epoch:17 step:13941 [D loss: 0.010046, acc.: 100.00%] [G loss: 6.285202]\n",
      "epoch:17 step:13942 [D loss: 0.048039, acc.: 100.00%] [G loss: 0.924256]\n",
      "epoch:17 step:13943 [D loss: 0.003669, acc.: 100.00%] [G loss: 7.761960]\n",
      "epoch:17 step:13944 [D loss: 0.013531, acc.: 100.00%] [G loss: 7.611109]\n",
      "epoch:17 step:13945 [D loss: 0.011376, acc.: 100.00%] [G loss: 6.701057]\n",
      "epoch:17 step:13946 [D loss: 0.007386, acc.: 100.00%] [G loss: 6.975468]\n",
      "epoch:17 step:13947 [D loss: 0.056804, acc.: 97.66%] [G loss: 5.099846]\n",
      "epoch:17 step:13948 [D loss: 0.064651, acc.: 97.66%] [G loss: 7.751436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17 step:13949 [D loss: 0.001853, acc.: 100.00%] [G loss: 8.607143]\n",
      "epoch:17 step:13950 [D loss: 0.361621, acc.: 82.81%] [G loss: 4.047238]\n",
      "epoch:17 step:13951 [D loss: 0.150779, acc.: 92.19%] [G loss: 8.706717]\n",
      "epoch:17 step:13952 [D loss: 0.000256, acc.: 100.00%] [G loss: 10.436848]\n",
      "epoch:17 step:13953 [D loss: 0.020015, acc.: 98.44%] [G loss: 9.507324]\n",
      "epoch:17 step:13954 [D loss: 0.016614, acc.: 98.44%] [G loss: 9.506936]\n",
      "epoch:17 step:13955 [D loss: 0.001138, acc.: 100.00%] [G loss: 8.845119]\n",
      "epoch:17 step:13956 [D loss: 0.002297, acc.: 100.00%] [G loss: 8.996693]\n",
      "epoch:17 step:13957 [D loss: 0.002452, acc.: 100.00%] [G loss: 8.028566]\n",
      "epoch:17 step:13958 [D loss: 0.044588, acc.: 98.44%] [G loss: 8.074005]\n",
      "epoch:17 step:13959 [D loss: 0.008975, acc.: 99.22%] [G loss: 7.107246]\n",
      "epoch:17 step:13960 [D loss: 0.005364, acc.: 100.00%] [G loss: 7.308423]\n",
      "epoch:17 step:13961 [D loss: 0.003153, acc.: 100.00%] [G loss: 7.445681]\n",
      "epoch:17 step:13962 [D loss: 0.040687, acc.: 98.44%] [G loss: 6.660007]\n",
      "epoch:17 step:13963 [D loss: 0.010436, acc.: 100.00%] [G loss: 0.394804]\n",
      "epoch:17 step:13964 [D loss: 0.040864, acc.: 98.44%] [G loss: 7.836299]\n",
      "epoch:17 step:13965 [D loss: 0.033926, acc.: 100.00%] [G loss: 8.289716]\n",
      "epoch:17 step:13966 [D loss: 0.057257, acc.: 96.88%] [G loss: 8.837051]\n",
      "epoch:17 step:13967 [D loss: 0.006042, acc.: 100.00%] [G loss: 9.150518]\n",
      "epoch:17 step:13968 [D loss: 0.006291, acc.: 100.00%] [G loss: 7.976493]\n",
      "epoch:17 step:13969 [D loss: 0.012883, acc.: 100.00%] [G loss: 8.437660]\n",
      "epoch:17 step:13970 [D loss: 0.004855, acc.: 100.00%] [G loss: 7.992157]\n",
      "epoch:17 step:13971 [D loss: 0.006888, acc.: 100.00%] [G loss: 7.292023]\n",
      "epoch:17 step:13972 [D loss: 0.001838, acc.: 100.00%] [G loss: 0.016962]\n",
      "epoch:17 step:13973 [D loss: 0.137422, acc.: 94.53%] [G loss: 9.325727]\n",
      "epoch:17 step:13974 [D loss: 0.006969, acc.: 100.00%] [G loss: 10.345604]\n",
      "epoch:17 step:13975 [D loss: 0.276711, acc.: 85.94%] [G loss: 4.540402]\n",
      "epoch:17 step:13976 [D loss: 0.221087, acc.: 93.75%] [G loss: 7.025231]\n",
      "epoch:17 step:13977 [D loss: 0.000208, acc.: 100.00%] [G loss: 7.029165]\n",
      "epoch:17 step:13978 [D loss: 0.000620, acc.: 100.00%] [G loss: 8.646595]\n",
      "epoch:17 step:13979 [D loss: 0.003195, acc.: 100.00%] [G loss: 7.777170]\n",
      "epoch:17 step:13980 [D loss: 0.043388, acc.: 97.66%] [G loss: 6.355997]\n",
      "epoch:17 step:13981 [D loss: 0.042125, acc.: 98.44%] [G loss: 4.952017]\n",
      "epoch:17 step:13982 [D loss: 0.117588, acc.: 96.09%] [G loss: 7.624169]\n",
      "epoch:17 step:13983 [D loss: 0.001863, acc.: 100.00%] [G loss: 0.538200]\n",
      "epoch:17 step:13984 [D loss: 0.006942, acc.: 100.00%] [G loss: 7.555544]\n",
      "epoch:17 step:13985 [D loss: 0.005192, acc.: 100.00%] [G loss: 7.063787]\n",
      "epoch:17 step:13986 [D loss: 0.008244, acc.: 100.00%] [G loss: 0.169456]\n",
      "epoch:17 step:13987 [D loss: 0.068085, acc.: 99.22%] [G loss: 6.907016]\n",
      "epoch:17 step:13988 [D loss: 0.004548, acc.: 100.00%] [G loss: 6.016367]\n",
      "epoch:17 step:13989 [D loss: 0.024769, acc.: 99.22%] [G loss: 4.262100]\n",
      "epoch:17 step:13990 [D loss: 0.414315, acc.: 80.47%] [G loss: 7.353849]\n",
      "epoch:17 step:13991 [D loss: 0.011438, acc.: 100.00%] [G loss: 5.328477]\n",
      "epoch:17 step:13992 [D loss: 2.291859, acc.: 36.72%] [G loss: 0.902920]\n",
      "epoch:17 step:13993 [D loss: 0.003065, acc.: 100.00%] [G loss: 5.966774]\n",
      "epoch:17 step:13994 [D loss: 0.085914, acc.: 96.09%] [G loss: 5.133202]\n",
      "epoch:17 step:13995 [D loss: 0.044231, acc.: 97.66%] [G loss: 2.839315]\n",
      "epoch:17 step:13996 [D loss: 0.010939, acc.: 100.00%] [G loss: 3.304913]\n",
      "epoch:17 step:13997 [D loss: 0.031538, acc.: 100.00%] [G loss: 1.406573]\n",
      "epoch:17 step:13998 [D loss: 0.003622, acc.: 100.00%] [G loss: 2.424043]\n",
      "epoch:17 step:13999 [D loss: 0.042043, acc.: 99.22%] [G loss: 2.194981]\n",
      "epoch:17 step:14000 [D loss: 0.002762, acc.: 100.00%] [G loss: 2.775842]\n",
      "epoch:17 step:14001 [D loss: 0.128353, acc.: 96.09%] [G loss: 3.290173]\n",
      "epoch:17 step:14002 [D loss: 0.223929, acc.: 90.62%] [G loss: 1.235628]\n",
      "epoch:17 step:14003 [D loss: 0.222552, acc.: 87.50%] [G loss: 3.612490]\n",
      "epoch:17 step:14004 [D loss: 0.028302, acc.: 98.44%] [G loss: 4.942692]\n",
      "epoch:17 step:14005 [D loss: 0.053278, acc.: 96.88%] [G loss: 2.851918]\n",
      "epoch:17 step:14006 [D loss: 0.034727, acc.: 98.44%] [G loss: 5.469854]\n",
      "epoch:17 step:14007 [D loss: 0.007478, acc.: 100.00%] [G loss: 2.495906]\n",
      "epoch:17 step:14008 [D loss: 0.030589, acc.: 100.00%] [G loss: 5.045618]\n",
      "epoch:17 step:14009 [D loss: 0.008317, acc.: 100.00%] [G loss: 2.180970]\n",
      "epoch:17 step:14010 [D loss: 0.006304, acc.: 100.00%] [G loss: 2.698376]\n",
      "epoch:17 step:14011 [D loss: 0.017545, acc.: 100.00%] [G loss: 1.571003]\n",
      "epoch:17 step:14012 [D loss: 0.083207, acc.: 97.66%] [G loss: 3.478456]\n",
      "epoch:17 step:14013 [D loss: 0.270260, acc.: 90.62%] [G loss: 0.740341]\n",
      "epoch:17 step:14014 [D loss: 0.020343, acc.: 100.00%] [G loss: 3.727871]\n",
      "epoch:17 step:14015 [D loss: 0.003029, acc.: 100.00%] [G loss: 0.892109]\n",
      "epoch:17 step:14016 [D loss: 0.044152, acc.: 99.22%] [G loss: 1.280618]\n",
      "epoch:17 step:14017 [D loss: 0.007703, acc.: 100.00%] [G loss: 0.685941]\n",
      "epoch:17 step:14018 [D loss: 0.020024, acc.: 100.00%] [G loss: 0.885228]\n",
      "epoch:17 step:14019 [D loss: 0.001663, acc.: 100.00%] [G loss: 1.069688]\n",
      "epoch:17 step:14020 [D loss: 0.010036, acc.: 100.00%] [G loss: 0.176437]\n",
      "epoch:17 step:14021 [D loss: 0.185175, acc.: 92.19%] [G loss: 6.459447]\n",
      "epoch:17 step:14022 [D loss: 0.288802, acc.: 85.16%] [G loss: 1.969225]\n",
      "epoch:17 step:14023 [D loss: 0.079236, acc.: 97.66%] [G loss: 3.590322]\n",
      "epoch:17 step:14024 [D loss: 0.013359, acc.: 100.00%] [G loss: 4.542168]\n",
      "epoch:17 step:14025 [D loss: 0.010154, acc.: 100.00%] [G loss: 5.189803]\n",
      "epoch:17 step:14026 [D loss: 0.005474, acc.: 100.00%] [G loss: 3.390029]\n",
      "epoch:17 step:14027 [D loss: 0.028336, acc.: 99.22%] [G loss: 2.279775]\n",
      "epoch:17 step:14028 [D loss: 0.009724, acc.: 100.00%] [G loss: 0.951533]\n",
      "epoch:17 step:14029 [D loss: 0.018229, acc.: 99.22%] [G loss: 1.952925]\n",
      "epoch:17 step:14030 [D loss: 0.018310, acc.: 99.22%] [G loss: 2.023532]\n",
      "epoch:17 step:14031 [D loss: 0.012063, acc.: 100.00%] [G loss: 0.123877]\n",
      "epoch:17 step:14032 [D loss: 0.026014, acc.: 99.22%] [G loss: 1.418147]\n",
      "epoch:17 step:14033 [D loss: 0.002883, acc.: 100.00%] [G loss: 0.354582]\n",
      "epoch:17 step:14034 [D loss: 0.004039, acc.: 100.00%] [G loss: 1.352496]\n",
      "epoch:17 step:14035 [D loss: 0.002087, acc.: 100.00%] [G loss: 0.690233]\n",
      "epoch:17 step:14036 [D loss: 0.048412, acc.: 99.22%] [G loss: 0.121817]\n",
      "epoch:17 step:14037 [D loss: 0.003522, acc.: 100.00%] [G loss: 0.192951]\n",
      "epoch:17 step:14038 [D loss: 0.179425, acc.: 93.75%] [G loss: 2.870195]\n",
      "epoch:17 step:14039 [D loss: 0.550295, acc.: 71.88%] [G loss: 0.002283]\n",
      "epoch:17 step:14040 [D loss: 0.208749, acc.: 91.41%] [G loss: 0.330427]\n",
      "epoch:17 step:14041 [D loss: 0.000329, acc.: 100.00%] [G loss: 4.368848]\n",
      "epoch:17 step:14042 [D loss: 0.195042, acc.: 90.62%] [G loss: 0.883751]\n",
      "epoch:17 step:14043 [D loss: 0.025541, acc.: 99.22%] [G loss: 1.230811]\n",
      "epoch:17 step:14044 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.259862]\n",
      "epoch:17 step:14045 [D loss: 0.001751, acc.: 100.00%] [G loss: 0.097541]\n",
      "epoch:17 step:14046 [D loss: 0.009238, acc.: 100.00%] [G loss: 0.920964]\n",
      "epoch:17 step:14047 [D loss: 0.011444, acc.: 100.00%] [G loss: 0.062603]\n",
      "epoch:17 step:14048 [D loss: 0.026336, acc.: 99.22%] [G loss: 0.176053]\n",
      "epoch:17 step:14049 [D loss: 0.016820, acc.: 100.00%] [G loss: 0.034606]\n",
      "epoch:17 step:14050 [D loss: 0.004418, acc.: 100.00%] [G loss: 0.013742]\n",
      "epoch:17 step:14051 [D loss: 0.008319, acc.: 100.00%] [G loss: 0.116907]\n",
      "epoch:17 step:14052 [D loss: 0.000575, acc.: 100.00%] [G loss: 0.039465]\n",
      "epoch:17 step:14053 [D loss: 0.011152, acc.: 100.00%] [G loss: 0.077013]\n",
      "epoch:17 step:14054 [D loss: 0.000819, acc.: 100.00%] [G loss: 0.045827]\n",
      "epoch:17 step:14055 [D loss: 0.009578, acc.: 100.00%] [G loss: 0.043029]\n",
      "epoch:17 step:14056 [D loss: 0.001306, acc.: 100.00%] [G loss: 0.086626]\n",
      "epoch:17 step:14057 [D loss: 0.015735, acc.: 100.00%] [G loss: 0.585547]\n",
      "epoch:17 step:14058 [D loss: 0.002125, acc.: 100.00%] [G loss: 0.455295]\n",
      "epoch:18 step:14059 [D loss: 0.006582, acc.: 100.00%] [G loss: 1.004498]\n",
      "epoch:18 step:14060 [D loss: 0.103338, acc.: 97.66%] [G loss: 0.420287]\n",
      "epoch:18 step:14061 [D loss: 0.231263, acc.: 89.06%] [G loss: 0.011720]\n",
      "epoch:18 step:14062 [D loss: 0.393469, acc.: 85.94%] [G loss: 0.963345]\n",
      "epoch:18 step:14063 [D loss: 0.007070, acc.: 100.00%] [G loss: 3.459151]\n",
      "epoch:18 step:14064 [D loss: 0.680009, acc.: 80.47%] [G loss: 0.124152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14065 [D loss: 0.003183, acc.: 100.00%] [G loss: 0.070005]\n",
      "epoch:18 step:14066 [D loss: 0.114242, acc.: 96.09%] [G loss: 0.228783]\n",
      "epoch:18 step:14067 [D loss: 0.003174, acc.: 100.00%] [G loss: 0.262203]\n",
      "epoch:18 step:14068 [D loss: 0.003541, acc.: 100.00%] [G loss: 0.195506]\n",
      "epoch:18 step:14069 [D loss: 0.008688, acc.: 100.00%] [G loss: 0.268422]\n",
      "epoch:18 step:14070 [D loss: 0.005617, acc.: 100.00%] [G loss: 4.685860]\n",
      "epoch:18 step:14071 [D loss: 0.002493, acc.: 100.00%] [G loss: 0.530266]\n",
      "epoch:18 step:14072 [D loss: 0.023468, acc.: 100.00%] [G loss: 0.569610]\n",
      "epoch:18 step:14073 [D loss: 0.541833, acc.: 81.25%] [G loss: 9.383392]\n",
      "epoch:18 step:14074 [D loss: 0.298711, acc.: 86.72%] [G loss: 8.549916]\n",
      "epoch:18 step:14075 [D loss: 0.008015, acc.: 100.00%] [G loss: 0.913830]\n",
      "epoch:18 step:14076 [D loss: 0.003698, acc.: 100.00%] [G loss: 0.287705]\n",
      "epoch:18 step:14077 [D loss: 0.063777, acc.: 97.66%] [G loss: 7.996489]\n",
      "epoch:18 step:14078 [D loss: 0.003944, acc.: 100.00%] [G loss: 8.444785]\n",
      "epoch:18 step:14079 [D loss: 0.014082, acc.: 100.00%] [G loss: 5.640146]\n",
      "epoch:18 step:14080 [D loss: 0.011526, acc.: 100.00%] [G loss: 7.410714]\n",
      "epoch:18 step:14081 [D loss: 0.012260, acc.: 100.00%] [G loss: 5.315390]\n",
      "epoch:18 step:14082 [D loss: 0.028152, acc.: 100.00%] [G loss: 6.657849]\n",
      "epoch:18 step:14083 [D loss: 0.013498, acc.: 100.00%] [G loss: 0.322016]\n",
      "epoch:18 step:14084 [D loss: 0.726591, acc.: 67.19%] [G loss: 9.381887]\n",
      "epoch:18 step:14085 [D loss: 2.755662, acc.: 50.78%] [G loss: 4.490704]\n",
      "epoch:18 step:14086 [D loss: 0.713646, acc.: 68.75%] [G loss: 0.801576]\n",
      "epoch:18 step:14087 [D loss: 0.170205, acc.: 92.97%] [G loss: 1.516010]\n",
      "epoch:18 step:14088 [D loss: 0.010442, acc.: 100.00%] [G loss: 2.859133]\n",
      "epoch:18 step:14089 [D loss: 0.150795, acc.: 96.09%] [G loss: 0.979263]\n",
      "epoch:18 step:14090 [D loss: 0.012518, acc.: 100.00%] [G loss: 1.714468]\n",
      "epoch:18 step:14091 [D loss: 0.004900, acc.: 100.00%] [G loss: 1.148427]\n",
      "epoch:18 step:14092 [D loss: 0.002392, acc.: 100.00%] [G loss: 1.189154]\n",
      "epoch:18 step:14093 [D loss: 0.022994, acc.: 100.00%] [G loss: 0.783423]\n",
      "epoch:18 step:14094 [D loss: 0.013028, acc.: 100.00%] [G loss: 0.165569]\n",
      "epoch:18 step:14095 [D loss: 0.006435, acc.: 100.00%] [G loss: 0.016881]\n",
      "epoch:18 step:14096 [D loss: 0.015113, acc.: 100.00%] [G loss: 0.046589]\n",
      "epoch:18 step:14097 [D loss: 0.057276, acc.: 100.00%] [G loss: 0.757708]\n",
      "epoch:18 step:14098 [D loss: 0.002120, acc.: 100.00%] [G loss: 0.235328]\n",
      "epoch:18 step:14099 [D loss: 0.009584, acc.: 100.00%] [G loss: 0.533671]\n",
      "epoch:18 step:14100 [D loss: 0.013905, acc.: 100.00%] [G loss: 0.927667]\n",
      "epoch:18 step:14101 [D loss: 0.052603, acc.: 99.22%] [G loss: 0.630959]\n",
      "epoch:18 step:14102 [D loss: 0.042956, acc.: 98.44%] [G loss: 0.359672]\n",
      "epoch:18 step:14103 [D loss: 0.011958, acc.: 100.00%] [G loss: 0.231686]\n",
      "epoch:18 step:14104 [D loss: 0.002089, acc.: 100.00%] [G loss: 0.068597]\n",
      "epoch:18 step:14105 [D loss: 0.027776, acc.: 100.00%] [G loss: 0.112865]\n",
      "epoch:18 step:14106 [D loss: 0.009486, acc.: 100.00%] [G loss: 0.299929]\n",
      "epoch:18 step:14107 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.247219]\n",
      "epoch:18 step:14108 [D loss: 0.015933, acc.: 99.22%] [G loss: 0.041931]\n",
      "epoch:18 step:14109 [D loss: 0.001434, acc.: 100.00%] [G loss: 0.350556]\n",
      "epoch:18 step:14110 [D loss: 0.024389, acc.: 100.00%] [G loss: 0.180502]\n",
      "epoch:18 step:14111 [D loss: 0.011005, acc.: 100.00%] [G loss: 0.331665]\n",
      "epoch:18 step:14112 [D loss: 0.009306, acc.: 100.00%] [G loss: 0.196322]\n",
      "epoch:18 step:14113 [D loss: 0.001988, acc.: 100.00%] [G loss: 0.304113]\n",
      "epoch:18 step:14114 [D loss: 0.126516, acc.: 95.31%] [G loss: 0.005633]\n",
      "epoch:18 step:14115 [D loss: 2.618685, acc.: 52.34%] [G loss: 6.945613]\n",
      "epoch:18 step:14116 [D loss: 2.056911, acc.: 50.78%] [G loss: 5.491066]\n",
      "epoch:18 step:14117 [D loss: 1.852076, acc.: 49.22%] [G loss: 1.654406]\n",
      "epoch:18 step:14118 [D loss: 0.587485, acc.: 67.19%] [G loss: 1.381767]\n",
      "epoch:18 step:14119 [D loss: 0.158842, acc.: 94.53%] [G loss: 1.854895]\n",
      "epoch:18 step:14120 [D loss: 0.124299, acc.: 98.44%] [G loss: 1.847748]\n",
      "epoch:18 step:14121 [D loss: 0.109800, acc.: 99.22%] [G loss: 1.503378]\n",
      "epoch:18 step:14122 [D loss: 0.104648, acc.: 99.22%] [G loss: 0.603906]\n",
      "epoch:18 step:14123 [D loss: 0.120755, acc.: 98.44%] [G loss: 0.153611]\n",
      "epoch:18 step:14124 [D loss: 0.055744, acc.: 99.22%] [G loss: 0.735783]\n",
      "epoch:18 step:14125 [D loss: 0.194608, acc.: 95.31%] [G loss: 0.704675]\n",
      "epoch:18 step:14126 [D loss: 0.076475, acc.: 100.00%] [G loss: 1.788676]\n",
      "epoch:18 step:14127 [D loss: 0.177638, acc.: 96.88%] [G loss: 3.544404]\n",
      "epoch:18 step:14128 [D loss: 0.215754, acc.: 92.97%] [G loss: 3.982411]\n",
      "epoch:18 step:14129 [D loss: 0.640139, acc.: 64.84%] [G loss: 2.265472]\n",
      "epoch:18 step:14130 [D loss: 0.114382, acc.: 98.44%] [G loss: 3.400157]\n",
      "epoch:18 step:14131 [D loss: 0.081757, acc.: 99.22%] [G loss: 1.925581]\n",
      "epoch:18 step:14132 [D loss: 0.128309, acc.: 98.44%] [G loss: 3.728185]\n",
      "epoch:18 step:14133 [D loss: 0.092495, acc.: 99.22%] [G loss: 3.362571]\n",
      "epoch:18 step:14134 [D loss: 0.056507, acc.: 100.00%] [G loss: 1.817055]\n",
      "epoch:18 step:14135 [D loss: 0.249725, acc.: 91.41%] [G loss: 2.121590]\n",
      "epoch:18 step:14136 [D loss: 0.057869, acc.: 99.22%] [G loss: 3.022316]\n",
      "epoch:18 step:14137 [D loss: 0.045606, acc.: 100.00%] [G loss: 2.379417]\n",
      "epoch:18 step:14138 [D loss: 0.035435, acc.: 100.00%] [G loss: 2.195322]\n",
      "epoch:18 step:14139 [D loss: 0.085291, acc.: 100.00%] [G loss: 2.104986]\n",
      "epoch:18 step:14140 [D loss: 0.031610, acc.: 100.00%] [G loss: 3.231752]\n",
      "epoch:18 step:14141 [D loss: 0.077150, acc.: 98.44%] [G loss: 1.455002]\n",
      "epoch:18 step:14142 [D loss: 0.032666, acc.: 100.00%] [G loss: 1.632305]\n",
      "epoch:18 step:14143 [D loss: 0.036930, acc.: 100.00%] [G loss: 1.846068]\n",
      "epoch:18 step:14144 [D loss: 0.031376, acc.: 99.22%] [G loss: 1.103040]\n",
      "epoch:18 step:14145 [D loss: 0.007288, acc.: 100.00%] [G loss: 1.792594]\n",
      "epoch:18 step:14146 [D loss: 0.042244, acc.: 99.22%] [G loss: 1.556344]\n",
      "epoch:18 step:14147 [D loss: 0.019099, acc.: 100.00%] [G loss: 0.388675]\n",
      "epoch:18 step:14148 [D loss: 0.337060, acc.: 86.72%] [G loss: 0.841234]\n",
      "epoch:18 step:14149 [D loss: 0.006821, acc.: 100.00%] [G loss: 1.742078]\n",
      "epoch:18 step:14150 [D loss: 0.027352, acc.: 99.22%] [G loss: 1.979260]\n",
      "epoch:18 step:14151 [D loss: 0.016647, acc.: 100.00%] [G loss: 1.095776]\n",
      "epoch:18 step:14152 [D loss: 0.005354, acc.: 100.00%] [G loss: 0.713988]\n",
      "epoch:18 step:14153 [D loss: 0.009800, acc.: 100.00%] [G loss: 0.504857]\n",
      "epoch:18 step:14154 [D loss: 0.008912, acc.: 100.00%] [G loss: 2.648128]\n",
      "epoch:18 step:14155 [D loss: 0.018497, acc.: 100.00%] [G loss: 1.964130]\n",
      "epoch:18 step:14156 [D loss: 0.015610, acc.: 100.00%] [G loss: 0.215569]\n",
      "epoch:18 step:14157 [D loss: 0.114282, acc.: 98.44%] [G loss: 1.628018]\n",
      "epoch:18 step:14158 [D loss: 0.011868, acc.: 100.00%] [G loss: 0.112465]\n",
      "epoch:18 step:14159 [D loss: 0.018362, acc.: 100.00%] [G loss: 0.239219]\n",
      "epoch:18 step:14160 [D loss: 0.008770, acc.: 100.00%] [G loss: 1.226380]\n",
      "epoch:18 step:14161 [D loss: 0.021354, acc.: 100.00%] [G loss: 0.101959]\n",
      "epoch:18 step:14162 [D loss: 0.006892, acc.: 100.00%] [G loss: 0.177676]\n",
      "epoch:18 step:14163 [D loss: 0.031472, acc.: 100.00%] [G loss: 0.317064]\n",
      "epoch:18 step:14164 [D loss: 0.006825, acc.: 100.00%] [G loss: 0.118634]\n",
      "epoch:18 step:14165 [D loss: 0.072357, acc.: 98.44%] [G loss: 0.540743]\n",
      "epoch:18 step:14166 [D loss: 0.011859, acc.: 100.00%] [G loss: 0.347547]\n",
      "epoch:18 step:14167 [D loss: 0.027086, acc.: 100.00%] [G loss: 0.569009]\n",
      "epoch:18 step:14168 [D loss: 0.038732, acc.: 100.00%] [G loss: 0.705002]\n",
      "epoch:18 step:14169 [D loss: 0.155302, acc.: 93.75%] [G loss: 0.015272]\n",
      "epoch:18 step:14170 [D loss: 0.090982, acc.: 97.66%] [G loss: 0.545449]\n",
      "epoch:18 step:14171 [D loss: 0.068993, acc.: 99.22%] [G loss: 2.332384]\n",
      "epoch:18 step:14172 [D loss: 0.014602, acc.: 100.00%] [G loss: 1.789801]\n",
      "epoch:18 step:14173 [D loss: 0.021043, acc.: 100.00%] [G loss: 0.698518]\n",
      "epoch:18 step:14174 [D loss: 0.110601, acc.: 98.44%] [G loss: 0.845490]\n",
      "epoch:18 step:14175 [D loss: 0.005601, acc.: 100.00%] [G loss: 0.750055]\n",
      "epoch:18 step:14176 [D loss: 0.019446, acc.: 100.00%] [G loss: 0.724295]\n",
      "epoch:18 step:14177 [D loss: 0.342342, acc.: 85.16%] [G loss: 5.800450]\n",
      "epoch:18 step:14178 [D loss: 0.395574, acc.: 80.47%] [G loss: 5.918124]\n",
      "epoch:18 step:14179 [D loss: 0.092573, acc.: 97.66%] [G loss: 3.830844]\n",
      "epoch:18 step:14180 [D loss: 0.091095, acc.: 96.09%] [G loss: 4.281600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14181 [D loss: 0.004260, acc.: 100.00%] [G loss: 3.851088]\n",
      "epoch:18 step:14182 [D loss: 0.044623, acc.: 98.44%] [G loss: 1.214343]\n",
      "epoch:18 step:14183 [D loss: 0.008523, acc.: 100.00%] [G loss: 0.989771]\n",
      "epoch:18 step:14184 [D loss: 0.012299, acc.: 100.00%] [G loss: 2.720689]\n",
      "epoch:18 step:14185 [D loss: 0.017138, acc.: 100.00%] [G loss: 3.372534]\n",
      "epoch:18 step:14186 [D loss: 0.035429, acc.: 99.22%] [G loss: 0.281325]\n",
      "epoch:18 step:14187 [D loss: 0.033900, acc.: 100.00%] [G loss: 0.556067]\n",
      "epoch:18 step:14188 [D loss: 0.068051, acc.: 98.44%] [G loss: 3.390980]\n",
      "epoch:18 step:14189 [D loss: 0.006000, acc.: 100.00%] [G loss: 1.612567]\n",
      "epoch:18 step:14190 [D loss: 0.369439, acc.: 85.16%] [G loss: 1.420371]\n",
      "epoch:18 step:14191 [D loss: 0.012437, acc.: 100.00%] [G loss: 0.000535]\n",
      "epoch:18 step:14192 [D loss: 0.089379, acc.: 98.44%] [G loss: 2.377897]\n",
      "epoch:18 step:14193 [D loss: 0.002196, acc.: 100.00%] [G loss: 0.907266]\n",
      "epoch:18 step:14194 [D loss: 0.003754, acc.: 100.00%] [G loss: 0.781995]\n",
      "epoch:18 step:14195 [D loss: 0.023749, acc.: 100.00%] [G loss: 0.272072]\n",
      "epoch:18 step:14196 [D loss: 0.004403, acc.: 100.00%] [G loss: 0.107755]\n",
      "epoch:18 step:14197 [D loss: 0.017060, acc.: 99.22%] [G loss: 2.186841]\n",
      "epoch:18 step:14198 [D loss: 0.021359, acc.: 98.44%] [G loss: 1.117354]\n",
      "epoch:18 step:14199 [D loss: 0.005356, acc.: 100.00%] [G loss: 0.021583]\n",
      "epoch:18 step:14200 [D loss: 0.036196, acc.: 100.00%] [G loss: 0.105563]\n",
      "epoch:18 step:14201 [D loss: 0.031178, acc.: 99.22%] [G loss: 1.151660]\n",
      "epoch:18 step:14202 [D loss: 0.005934, acc.: 100.00%] [G loss: 0.726597]\n",
      "epoch:18 step:14203 [D loss: 0.004641, acc.: 100.00%] [G loss: 0.665528]\n",
      "epoch:18 step:14204 [D loss: 0.005765, acc.: 100.00%] [G loss: 0.033805]\n",
      "epoch:18 step:14205 [D loss: 0.264416, acc.: 85.94%] [G loss: 2.924693]\n",
      "epoch:18 step:14206 [D loss: 1.250098, acc.: 57.03%] [G loss: 0.519440]\n",
      "epoch:18 step:14207 [D loss: 0.037475, acc.: 98.44%] [G loss: 0.197890]\n",
      "epoch:18 step:14208 [D loss: 0.049800, acc.: 100.00%] [G loss: 0.205903]\n",
      "epoch:18 step:14209 [D loss: 0.017368, acc.: 100.00%] [G loss: 0.719714]\n",
      "epoch:18 step:14210 [D loss: 0.062965, acc.: 98.44%] [G loss: 0.407447]\n",
      "epoch:18 step:14211 [D loss: 0.040563, acc.: 100.00%] [G loss: 1.350803]\n",
      "epoch:18 step:14212 [D loss: 0.002340, acc.: 100.00%] [G loss: 0.534801]\n",
      "epoch:18 step:14213 [D loss: 0.032135, acc.: 100.00%] [G loss: 0.625192]\n",
      "epoch:18 step:14214 [D loss: 0.005201, acc.: 100.00%] [G loss: 0.911383]\n",
      "epoch:18 step:14215 [D loss: 0.043617, acc.: 99.22%] [G loss: 0.078831]\n",
      "epoch:18 step:14216 [D loss: 0.016871, acc.: 100.00%] [G loss: 0.049687]\n",
      "epoch:18 step:14217 [D loss: 0.051555, acc.: 100.00%] [G loss: 0.571484]\n",
      "epoch:18 step:14218 [D loss: 0.020169, acc.: 100.00%] [G loss: 0.796778]\n",
      "epoch:18 step:14219 [D loss: 0.005332, acc.: 100.00%] [G loss: 0.589074]\n",
      "epoch:18 step:14220 [D loss: 0.014959, acc.: 100.00%] [G loss: 0.440235]\n",
      "epoch:18 step:14221 [D loss: 0.060981, acc.: 98.44%] [G loss: 0.735685]\n",
      "epoch:18 step:14222 [D loss: 0.002686, acc.: 100.00%] [G loss: 2.452053]\n",
      "epoch:18 step:14223 [D loss: 0.062994, acc.: 99.22%] [G loss: 0.286009]\n",
      "epoch:18 step:14224 [D loss: 0.148578, acc.: 96.88%] [G loss: 0.784267]\n",
      "epoch:18 step:14225 [D loss: 0.407940, acc.: 77.34%] [G loss: 4.570888]\n",
      "epoch:18 step:14226 [D loss: 0.568987, acc.: 71.09%] [G loss: 1.696627]\n",
      "epoch:18 step:14227 [D loss: 0.002765, acc.: 100.00%] [G loss: 0.102470]\n",
      "epoch:18 step:14228 [D loss: 0.003642, acc.: 100.00%] [G loss: 0.652691]\n",
      "epoch:18 step:14229 [D loss: 0.003062, acc.: 100.00%] [G loss: 0.033122]\n",
      "epoch:18 step:14230 [D loss: 0.002120, acc.: 100.00%] [G loss: 3.639455]\n",
      "epoch:18 step:14231 [D loss: 0.006837, acc.: 100.00%] [G loss: 0.181792]\n",
      "epoch:18 step:14232 [D loss: 0.004343, acc.: 100.00%] [G loss: 3.613085]\n",
      "epoch:18 step:14233 [D loss: 0.007339, acc.: 100.00%] [G loss: 2.932741]\n",
      "epoch:18 step:14234 [D loss: 0.053736, acc.: 100.00%] [G loss: 3.773056]\n",
      "epoch:18 step:14235 [D loss: 0.022509, acc.: 100.00%] [G loss: 3.272033]\n",
      "epoch:18 step:14236 [D loss: 0.027512, acc.: 100.00%] [G loss: 3.311199]\n",
      "epoch:18 step:14237 [D loss: 0.314890, acc.: 89.06%] [G loss: 2.185781]\n",
      "epoch:18 step:14238 [D loss: 0.032654, acc.: 100.00%] [G loss: 2.996167]\n",
      "epoch:18 step:14239 [D loss: 0.015689, acc.: 100.00%] [G loss: 0.240891]\n",
      "epoch:18 step:14240 [D loss: 0.121731, acc.: 96.88%] [G loss: 0.074619]\n",
      "epoch:18 step:14241 [D loss: 0.052571, acc.: 100.00%] [G loss: 0.107503]\n",
      "epoch:18 step:14242 [D loss: 0.029843, acc.: 99.22%] [G loss: 4.698540]\n",
      "epoch:18 step:14243 [D loss: 0.025164, acc.: 100.00%] [G loss: 4.322553]\n",
      "epoch:18 step:14244 [D loss: 0.004972, acc.: 100.00%] [G loss: 2.871911]\n",
      "epoch:18 step:14245 [D loss: 0.142869, acc.: 96.09%] [G loss: 2.354042]\n",
      "epoch:18 step:14246 [D loss: 0.033996, acc.: 100.00%] [G loss: 0.007269]\n",
      "epoch:18 step:14247 [D loss: 0.010372, acc.: 100.00%] [G loss: 0.054438]\n",
      "epoch:18 step:14248 [D loss: 0.020542, acc.: 100.00%] [G loss: 2.566098]\n",
      "epoch:18 step:14249 [D loss: 0.044131, acc.: 100.00%] [G loss: 0.045594]\n",
      "epoch:18 step:14250 [D loss: 0.009867, acc.: 100.00%] [G loss: 3.372756]\n",
      "epoch:18 step:14251 [D loss: 0.012152, acc.: 100.00%] [G loss: 3.789965]\n",
      "epoch:18 step:14252 [D loss: 0.019654, acc.: 100.00%] [G loss: 3.613016]\n",
      "epoch:18 step:14253 [D loss: 0.019814, acc.: 100.00%] [G loss: 0.004424]\n",
      "epoch:18 step:14254 [D loss: 0.060086, acc.: 99.22%] [G loss: 0.054004]\n",
      "epoch:18 step:14255 [D loss: 0.004859, acc.: 100.00%] [G loss: 0.007069]\n",
      "epoch:18 step:14256 [D loss: 0.065502, acc.: 97.66%] [G loss: 0.173031]\n",
      "epoch:18 step:14257 [D loss: 0.018835, acc.: 100.00%] [G loss: 5.033483]\n",
      "epoch:18 step:14258 [D loss: 0.016648, acc.: 100.00%] [G loss: 2.678731]\n",
      "epoch:18 step:14259 [D loss: 0.005661, acc.: 100.00%] [G loss: 3.704144]\n",
      "epoch:18 step:14260 [D loss: 0.028658, acc.: 99.22%] [G loss: 0.497514]\n",
      "epoch:18 step:14261 [D loss: 0.008044, acc.: 100.00%] [G loss: 2.015452]\n",
      "epoch:18 step:14262 [D loss: 0.041711, acc.: 99.22%] [G loss: 4.184223]\n",
      "epoch:18 step:14263 [D loss: 0.004367, acc.: 100.00%] [G loss: 0.007125]\n",
      "epoch:18 step:14264 [D loss: 0.024661, acc.: 100.00%] [G loss: 0.181165]\n",
      "epoch:18 step:14265 [D loss: 0.009237, acc.: 100.00%] [G loss: 0.019964]\n",
      "epoch:18 step:14266 [D loss: 0.007307, acc.: 100.00%] [G loss: 0.930409]\n",
      "epoch:18 step:14267 [D loss: 0.546789, acc.: 73.44%] [G loss: 6.694645]\n",
      "epoch:18 step:14268 [D loss: 1.662090, acc.: 50.78%] [G loss: 5.103055]\n",
      "epoch:18 step:14269 [D loss: 0.056237, acc.: 98.44%] [G loss: 2.317721]\n",
      "epoch:18 step:14270 [D loss: 0.011837, acc.: 100.00%] [G loss: 1.918214]\n",
      "epoch:18 step:14271 [D loss: 0.007531, acc.: 100.00%] [G loss: 1.929396]\n",
      "epoch:18 step:14272 [D loss: 0.004516, acc.: 100.00%] [G loss: 1.333870]\n",
      "epoch:18 step:14273 [D loss: 0.108134, acc.: 96.09%] [G loss: 1.962476]\n",
      "epoch:18 step:14274 [D loss: 0.031507, acc.: 100.00%] [G loss: 1.576885]\n",
      "epoch:18 step:14275 [D loss: 0.020518, acc.: 100.00%] [G loss: 2.600118]\n",
      "epoch:18 step:14276 [D loss: 0.041431, acc.: 100.00%] [G loss: 0.585759]\n",
      "epoch:18 step:14277 [D loss: 0.045308, acc.: 100.00%] [G loss: 1.126886]\n",
      "epoch:18 step:14278 [D loss: 0.030577, acc.: 100.00%] [G loss: 1.223073]\n",
      "epoch:18 step:14279 [D loss: 0.026198, acc.: 99.22%] [G loss: 0.571245]\n",
      "epoch:18 step:14280 [D loss: 0.019250, acc.: 100.00%] [G loss: 0.265349]\n",
      "epoch:18 step:14281 [D loss: 0.014419, acc.: 100.00%] [G loss: 0.281097]\n",
      "epoch:18 step:14282 [D loss: 0.746805, acc.: 65.62%] [G loss: 5.567382]\n",
      "epoch:18 step:14283 [D loss: 2.520155, acc.: 50.78%] [G loss: 3.997808]\n",
      "epoch:18 step:14284 [D loss: 0.861030, acc.: 64.84%] [G loss: 1.224103]\n",
      "epoch:18 step:14285 [D loss: 0.047330, acc.: 99.22%] [G loss: 1.368023]\n",
      "epoch:18 step:14286 [D loss: 0.093312, acc.: 98.44%] [G loss: 0.505498]\n",
      "epoch:18 step:14287 [D loss: 0.022258, acc.: 100.00%] [G loss: 0.803381]\n",
      "epoch:18 step:14288 [D loss: 0.097176, acc.: 96.09%] [G loss: 0.512141]\n",
      "epoch:18 step:14289 [D loss: 0.028345, acc.: 100.00%] [G loss: 0.636235]\n",
      "epoch:18 step:14290 [D loss: 0.014580, acc.: 100.00%] [G loss: 0.402985]\n",
      "epoch:18 step:14291 [D loss: 0.042667, acc.: 100.00%] [G loss: 1.374425]\n",
      "epoch:18 step:14292 [D loss: 0.034395, acc.: 100.00%] [G loss: 0.345654]\n",
      "epoch:18 step:14293 [D loss: 0.058616, acc.: 100.00%] [G loss: 0.200920]\n",
      "epoch:18 step:14294 [D loss: 0.135078, acc.: 98.44%] [G loss: 0.389527]\n",
      "epoch:18 step:14295 [D loss: 0.019354, acc.: 100.00%] [G loss: 0.312268]\n",
      "epoch:18 step:14296 [D loss: 0.091313, acc.: 96.88%] [G loss: 2.283180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14297 [D loss: 0.019271, acc.: 100.00%] [G loss: 2.333341]\n",
      "epoch:18 step:14298 [D loss: 0.222930, acc.: 91.41%] [G loss: 0.164103]\n",
      "epoch:18 step:14299 [D loss: 0.049762, acc.: 100.00%] [G loss: 0.124303]\n",
      "epoch:18 step:14300 [D loss: 0.011673, acc.: 100.00%] [G loss: 0.162773]\n",
      "epoch:18 step:14301 [D loss: 0.024351, acc.: 100.00%] [G loss: 0.436710]\n",
      "epoch:18 step:14302 [D loss: 0.093932, acc.: 97.66%] [G loss: 0.506479]\n",
      "epoch:18 step:14303 [D loss: 0.021802, acc.: 100.00%] [G loss: 0.807073]\n",
      "epoch:18 step:14304 [D loss: 0.194722, acc.: 96.09%] [G loss: 1.806818]\n",
      "epoch:18 step:14305 [D loss: 0.133990, acc.: 92.97%] [G loss: 0.679246]\n",
      "epoch:18 step:14306 [D loss: 0.011019, acc.: 100.00%] [G loss: 0.575123]\n",
      "epoch:18 step:14307 [D loss: 0.015037, acc.: 100.00%] [G loss: 0.188512]\n",
      "epoch:18 step:14308 [D loss: 0.006247, acc.: 100.00%] [G loss: 0.197503]\n",
      "epoch:18 step:14309 [D loss: 0.008117, acc.: 100.00%] [G loss: 0.213296]\n",
      "epoch:18 step:14310 [D loss: 0.004272, acc.: 100.00%] [G loss: 0.372157]\n",
      "epoch:18 step:14311 [D loss: 0.014108, acc.: 100.00%] [G loss: 0.282482]\n",
      "epoch:18 step:14312 [D loss: 0.003285, acc.: 100.00%] [G loss: 0.211665]\n",
      "epoch:18 step:14313 [D loss: 0.010346, acc.: 100.00%] [G loss: 0.149378]\n",
      "epoch:18 step:14314 [D loss: 0.022358, acc.: 100.00%] [G loss: 2.119461]\n",
      "epoch:18 step:14315 [D loss: 0.017082, acc.: 100.00%] [G loss: 0.109185]\n",
      "epoch:18 step:14316 [D loss: 0.030383, acc.: 100.00%] [G loss: 0.514282]\n",
      "epoch:18 step:14317 [D loss: 0.014016, acc.: 100.00%] [G loss: 1.298527]\n",
      "epoch:18 step:14318 [D loss: 0.034199, acc.: 100.00%] [G loss: 0.365393]\n",
      "epoch:18 step:14319 [D loss: 0.072518, acc.: 97.66%] [G loss: 0.186636]\n",
      "epoch:18 step:14320 [D loss: 0.349663, acc.: 80.47%] [G loss: 2.243886]\n",
      "epoch:18 step:14321 [D loss: 0.098224, acc.: 95.31%] [G loss: 1.703825]\n",
      "epoch:18 step:14322 [D loss: 0.201311, acc.: 92.19%] [G loss: 0.351492]\n",
      "epoch:18 step:14323 [D loss: 0.101772, acc.: 96.88%] [G loss: 1.758610]\n",
      "epoch:18 step:14324 [D loss: 0.079325, acc.: 97.66%] [G loss: 0.005550]\n",
      "epoch:18 step:14325 [D loss: 0.003926, acc.: 100.00%] [G loss: 2.030617]\n",
      "epoch:18 step:14326 [D loss: 0.007763, acc.: 100.00%] [G loss: 0.029256]\n",
      "epoch:18 step:14327 [D loss: 0.011285, acc.: 100.00%] [G loss: 0.066636]\n",
      "epoch:18 step:14328 [D loss: 0.029510, acc.: 100.00%] [G loss: 0.226928]\n",
      "epoch:18 step:14329 [D loss: 0.015122, acc.: 100.00%] [G loss: 1.997288]\n",
      "epoch:18 step:14330 [D loss: 0.018432, acc.: 100.00%] [G loss: 0.122365]\n",
      "epoch:18 step:14331 [D loss: 0.008138, acc.: 100.00%] [G loss: 0.078527]\n",
      "epoch:18 step:14332 [D loss: 0.001963, acc.: 100.00%] [G loss: 2.368297]\n",
      "epoch:18 step:14333 [D loss: 0.018249, acc.: 100.00%] [G loss: 1.273362]\n",
      "epoch:18 step:14334 [D loss: 0.023888, acc.: 100.00%] [G loss: 0.128854]\n",
      "epoch:18 step:14335 [D loss: 1.087531, acc.: 50.78%] [G loss: 4.116352]\n",
      "epoch:18 step:14336 [D loss: 0.128391, acc.: 94.53%] [G loss: 5.202526]\n",
      "epoch:18 step:14337 [D loss: 0.397227, acc.: 79.69%] [G loss: 3.936213]\n",
      "epoch:18 step:14338 [D loss: 0.015253, acc.: 100.00%] [G loss: 0.410998]\n",
      "epoch:18 step:14339 [D loss: 0.011930, acc.: 100.00%] [G loss: 0.394260]\n",
      "epoch:18 step:14340 [D loss: 0.026141, acc.: 100.00%] [G loss: 0.319493]\n",
      "epoch:18 step:14341 [D loss: 0.009972, acc.: 100.00%] [G loss: 0.513224]\n",
      "epoch:18 step:14342 [D loss: 0.005520, acc.: 100.00%] [G loss: 3.840363]\n",
      "epoch:18 step:14343 [D loss: 0.003548, acc.: 100.00%] [G loss: 3.773690]\n",
      "epoch:18 step:14344 [D loss: 0.006779, acc.: 100.00%] [G loss: 1.220706]\n",
      "epoch:18 step:14345 [D loss: 0.046740, acc.: 100.00%] [G loss: 3.620508]\n",
      "epoch:18 step:14346 [D loss: 0.024860, acc.: 100.00%] [G loss: 3.122049]\n",
      "epoch:18 step:14347 [D loss: 0.132382, acc.: 95.31%] [G loss: 4.209079]\n",
      "epoch:18 step:14348 [D loss: 0.030073, acc.: 99.22%] [G loss: 3.012891]\n",
      "epoch:18 step:14349 [D loss: 0.043788, acc.: 99.22%] [G loss: 1.218658]\n",
      "epoch:18 step:14350 [D loss: 0.027855, acc.: 100.00%] [G loss: 1.264478]\n",
      "epoch:18 step:14351 [D loss: 0.007959, acc.: 100.00%] [G loss: 0.408128]\n",
      "epoch:18 step:14352 [D loss: 0.086222, acc.: 98.44%] [G loss: 2.627108]\n",
      "epoch:18 step:14353 [D loss: 0.179827, acc.: 95.31%] [G loss: 0.467095]\n",
      "epoch:18 step:14354 [D loss: 0.004496, acc.: 100.00%] [G loss: 4.174673]\n",
      "epoch:18 step:14355 [D loss: 0.022404, acc.: 100.00%] [G loss: 1.491232]\n",
      "epoch:18 step:14356 [D loss: 0.040525, acc.: 99.22%] [G loss: 2.826600]\n",
      "epoch:18 step:14357 [D loss: 0.020012, acc.: 100.00%] [G loss: 0.257884]\n",
      "epoch:18 step:14358 [D loss: 0.038537, acc.: 99.22%] [G loss: 0.118874]\n",
      "epoch:18 step:14359 [D loss: 0.029700, acc.: 100.00%] [G loss: 0.319223]\n",
      "epoch:18 step:14360 [D loss: 0.005096, acc.: 100.00%] [G loss: 3.183519]\n",
      "epoch:18 step:14361 [D loss: 0.003133, acc.: 100.00%] [G loss: 2.770398]\n",
      "epoch:18 step:14362 [D loss: 0.005370, acc.: 100.00%] [G loss: 0.119965]\n",
      "epoch:18 step:14363 [D loss: 0.004089, acc.: 100.00%] [G loss: 1.981298]\n",
      "epoch:18 step:14364 [D loss: 0.008123, acc.: 100.00%] [G loss: 0.082571]\n",
      "epoch:18 step:14365 [D loss: 0.013793, acc.: 100.00%] [G loss: 2.810773]\n",
      "epoch:18 step:14366 [D loss: 0.003146, acc.: 100.00%] [G loss: 1.409087]\n",
      "epoch:18 step:14367 [D loss: 0.009162, acc.: 100.00%] [G loss: 0.021658]\n",
      "epoch:18 step:14368 [D loss: 0.022446, acc.: 100.00%] [G loss: 0.051359]\n",
      "epoch:18 step:14369 [D loss: 0.016251, acc.: 100.00%] [G loss: 0.048540]\n",
      "epoch:18 step:14370 [D loss: 0.005488, acc.: 100.00%] [G loss: 0.064366]\n",
      "epoch:18 step:14371 [D loss: 0.010290, acc.: 100.00%] [G loss: 0.473042]\n",
      "epoch:18 step:14372 [D loss: 0.001494, acc.: 100.00%] [G loss: 0.020360]\n",
      "epoch:18 step:14373 [D loss: 0.328908, acc.: 90.62%] [G loss: 0.084899]\n",
      "epoch:18 step:14374 [D loss: 0.001602, acc.: 100.00%] [G loss: 0.027720]\n",
      "epoch:18 step:14375 [D loss: 0.000706, acc.: 100.00%] [G loss: 0.324455]\n",
      "epoch:18 step:14376 [D loss: 0.001293, acc.: 100.00%] [G loss: 1.121511]\n",
      "epoch:18 step:14377 [D loss: 0.010773, acc.: 100.00%] [G loss: 1.557381]\n",
      "epoch:18 step:14378 [D loss: 0.141883, acc.: 96.09%] [G loss: 1.082295]\n",
      "epoch:18 step:14379 [D loss: 0.004189, acc.: 100.00%] [G loss: 0.030142]\n",
      "epoch:18 step:14380 [D loss: 0.000631, acc.: 100.00%] [G loss: 0.065511]\n",
      "epoch:18 step:14381 [D loss: 0.000767, acc.: 100.00%] [G loss: 0.063517]\n",
      "epoch:18 step:14382 [D loss: 0.002200, acc.: 100.00%] [G loss: 1.328460]\n",
      "epoch:18 step:14383 [D loss: 0.018817, acc.: 99.22%] [G loss: 0.387585]\n",
      "epoch:18 step:14384 [D loss: 0.001577, acc.: 100.00%] [G loss: 0.009318]\n",
      "epoch:18 step:14385 [D loss: 0.001109, acc.: 100.00%] [G loss: 0.882884]\n",
      "epoch:18 step:14386 [D loss: 0.000871, acc.: 100.00%] [G loss: 0.005507]\n",
      "epoch:18 step:14387 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.006008]\n",
      "epoch:18 step:14388 [D loss: 0.004184, acc.: 100.00%] [G loss: 0.008404]\n",
      "epoch:18 step:14389 [D loss: 0.003015, acc.: 100.00%] [G loss: 0.008472]\n",
      "epoch:18 step:14390 [D loss: 0.001968, acc.: 100.00%] [G loss: 0.005377]\n",
      "epoch:18 step:14391 [D loss: 0.005280, acc.: 100.00%] [G loss: 0.015274]\n",
      "epoch:18 step:14392 [D loss: 0.005857, acc.: 100.00%] [G loss: 0.009207]\n",
      "epoch:18 step:14393 [D loss: 0.011138, acc.: 100.00%] [G loss: 0.014762]\n",
      "epoch:18 step:14394 [D loss: 0.008911, acc.: 100.00%] [G loss: 1.224850]\n",
      "epoch:18 step:14395 [D loss: 0.016039, acc.: 100.00%] [G loss: 0.028572]\n",
      "epoch:18 step:14396 [D loss: 0.577632, acc.: 72.66%] [G loss: 5.902879]\n",
      "epoch:18 step:14397 [D loss: 0.408780, acc.: 81.25%] [G loss: 5.574347]\n",
      "epoch:18 step:14398 [D loss: 0.613126, acc.: 69.53%] [G loss: 0.014628]\n",
      "epoch:18 step:14399 [D loss: 2.330652, acc.: 54.69%] [G loss: 8.027058]\n",
      "epoch:18 step:14400 [D loss: 1.787885, acc.: 50.78%] [G loss: 6.371840]\n",
      "epoch:18 step:14401 [D loss: 0.890041, acc.: 65.62%] [G loss: 3.513870]\n",
      "epoch:18 step:14402 [D loss: 0.113259, acc.: 96.88%] [G loss: 2.926225]\n",
      "epoch:18 step:14403 [D loss: 0.151805, acc.: 95.31%] [G loss: 3.840566]\n",
      "epoch:18 step:14404 [D loss: 0.063517, acc.: 99.22%] [G loss: 4.245637]\n",
      "epoch:18 step:14405 [D loss: 0.089507, acc.: 95.31%] [G loss: 3.809229]\n",
      "epoch:18 step:14406 [D loss: 0.059903, acc.: 100.00%] [G loss: 4.206728]\n",
      "epoch:18 step:14407 [D loss: 0.070790, acc.: 100.00%] [G loss: 4.892494]\n",
      "epoch:18 step:14408 [D loss: 0.142372, acc.: 96.09%] [G loss: 4.124174]\n",
      "epoch:18 step:14409 [D loss: 0.081057, acc.: 97.66%] [G loss: 4.263502]\n",
      "epoch:18 step:14410 [D loss: 0.102436, acc.: 96.88%] [G loss: 4.691474]\n",
      "epoch:18 step:14411 [D loss: 0.094176, acc.: 97.66%] [G loss: 0.104269]\n",
      "epoch:18 step:14412 [D loss: 0.050601, acc.: 99.22%] [G loss: 2.202828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14413 [D loss: 0.212580, acc.: 89.84%] [G loss: 5.084364]\n",
      "epoch:18 step:14414 [D loss: 0.126053, acc.: 94.53%] [G loss: 0.016964]\n",
      "epoch:18 step:14415 [D loss: 0.039804, acc.: 99.22%] [G loss: 0.145609]\n",
      "epoch:18 step:14416 [D loss: 0.374497, acc.: 85.94%] [G loss: 0.019493]\n",
      "epoch:18 step:14417 [D loss: 0.052951, acc.: 99.22%] [G loss: 4.288063]\n",
      "epoch:18 step:14418 [D loss: 0.029871, acc.: 100.00%] [G loss: 4.698211]\n",
      "epoch:18 step:14419 [D loss: 0.054676, acc.: 98.44%] [G loss: 3.388778]\n",
      "epoch:18 step:14420 [D loss: 0.039534, acc.: 100.00%] [G loss: 0.749397]\n",
      "epoch:18 step:14421 [D loss: 0.051542, acc.: 100.00%] [G loss: 3.314720]\n",
      "epoch:18 step:14422 [D loss: 0.126675, acc.: 97.66%] [G loss: 1.956972]\n",
      "epoch:18 step:14423 [D loss: 0.043351, acc.: 100.00%] [G loss: 2.069997]\n",
      "epoch:18 step:14424 [D loss: 0.008663, acc.: 100.00%] [G loss: 1.066011]\n",
      "epoch:18 step:14425 [D loss: 0.121105, acc.: 95.31%] [G loss: 3.125631]\n",
      "epoch:18 step:14426 [D loss: 0.029053, acc.: 100.00%] [G loss: 2.271523]\n",
      "epoch:18 step:14427 [D loss: 0.732449, acc.: 64.06%] [G loss: 5.154021]\n",
      "epoch:18 step:14428 [D loss: 0.426642, acc.: 75.00%] [G loss: 3.385089]\n",
      "epoch:18 step:14429 [D loss: 0.258625, acc.: 91.41%] [G loss: 2.325544]\n",
      "epoch:18 step:14430 [D loss: 0.022997, acc.: 100.00%] [G loss: 2.777067]\n",
      "epoch:18 step:14431 [D loss: 0.043068, acc.: 99.22%] [G loss: 2.116922]\n",
      "epoch:18 step:14432 [D loss: 0.112221, acc.: 99.22%] [G loss: 0.239921]\n",
      "epoch:18 step:14433 [D loss: 0.173526, acc.: 92.97%] [G loss: 1.760992]\n",
      "epoch:18 step:14434 [D loss: 0.129950, acc.: 96.88%] [G loss: 2.962250]\n",
      "epoch:18 step:14435 [D loss: 0.009522, acc.: 100.00%] [G loss: 3.112713]\n",
      "epoch:18 step:14436 [D loss: 0.046634, acc.: 100.00%] [G loss: 3.140600]\n",
      "epoch:18 step:14437 [D loss: 0.055447, acc.: 98.44%] [G loss: 2.344357]\n",
      "epoch:18 step:14438 [D loss: 0.049220, acc.: 99.22%] [G loss: 0.041455]\n",
      "epoch:18 step:14439 [D loss: 0.016970, acc.: 99.22%] [G loss: 0.057616]\n",
      "epoch:18 step:14440 [D loss: 0.013820, acc.: 100.00%] [G loss: 1.826931]\n",
      "epoch:18 step:14441 [D loss: 0.012852, acc.: 100.00%] [G loss: 0.028937]\n",
      "epoch:18 step:14442 [D loss: 0.306075, acc.: 84.38%] [G loss: 3.011638]\n",
      "epoch:18 step:14443 [D loss: 0.090623, acc.: 96.88%] [G loss: 3.998525]\n",
      "epoch:18 step:14444 [D loss: 0.534174, acc.: 72.66%] [G loss: 0.313787]\n",
      "epoch:18 step:14445 [D loss: 0.034943, acc.: 100.00%] [G loss: 0.207086]\n",
      "epoch:18 step:14446 [D loss: 1.653672, acc.: 53.91%] [G loss: 3.763314]\n",
      "epoch:18 step:14447 [D loss: 0.764461, acc.: 64.06%] [G loss: 5.389050]\n",
      "epoch:18 step:14448 [D loss: 0.338667, acc.: 82.03%] [G loss: 4.195714]\n",
      "epoch:18 step:14449 [D loss: 0.342650, acc.: 85.16%] [G loss: 1.537171]\n",
      "epoch:18 step:14450 [D loss: 0.054051, acc.: 99.22%] [G loss: 0.968794]\n",
      "epoch:18 step:14451 [D loss: 0.042169, acc.: 100.00%] [G loss: 1.190928]\n",
      "epoch:18 step:14452 [D loss: 0.013573, acc.: 100.00%] [G loss: 1.389641]\n",
      "epoch:18 step:14453 [D loss: 0.032182, acc.: 99.22%] [G loss: 1.254619]\n",
      "epoch:18 step:14454 [D loss: 0.047736, acc.: 99.22%] [G loss: 1.079207]\n",
      "epoch:18 step:14455 [D loss: 0.019574, acc.: 100.00%] [G loss: 0.511149]\n",
      "epoch:18 step:14456 [D loss: 0.009037, acc.: 100.00%] [G loss: 1.245273]\n",
      "epoch:18 step:14457 [D loss: 0.045300, acc.: 100.00%] [G loss: 0.295793]\n",
      "epoch:18 step:14458 [D loss: 0.009089, acc.: 100.00%] [G loss: 1.359286]\n",
      "epoch:18 step:14459 [D loss: 0.020078, acc.: 100.00%] [G loss: 0.644877]\n",
      "epoch:18 step:14460 [D loss: 0.019876, acc.: 100.00%] [G loss: 0.115856]\n",
      "epoch:18 step:14461 [D loss: 0.070550, acc.: 100.00%] [G loss: 0.618178]\n",
      "epoch:18 step:14462 [D loss: 0.015957, acc.: 99.22%] [G loss: 0.712129]\n",
      "epoch:18 step:14463 [D loss: 0.061257, acc.: 98.44%] [G loss: 0.210783]\n",
      "epoch:18 step:14464 [D loss: 0.006801, acc.: 100.00%] [G loss: 0.264623]\n",
      "epoch:18 step:14465 [D loss: 0.007119, acc.: 100.00%] [G loss: 0.193908]\n",
      "epoch:18 step:14466 [D loss: 0.002187, acc.: 100.00%] [G loss: 0.091464]\n",
      "epoch:18 step:14467 [D loss: 0.008251, acc.: 100.00%] [G loss: 0.120977]\n",
      "epoch:18 step:14468 [D loss: 0.035585, acc.: 100.00%] [G loss: 0.112764]\n",
      "epoch:18 step:14469 [D loss: 0.029974, acc.: 100.00%] [G loss: 0.194339]\n",
      "epoch:18 step:14470 [D loss: 0.117462, acc.: 98.44%] [G loss: 0.408855]\n",
      "epoch:18 step:14471 [D loss: 0.044319, acc.: 98.44%] [G loss: 0.617587]\n",
      "epoch:18 step:14472 [D loss: 0.147741, acc.: 93.75%] [G loss: 0.255671]\n",
      "epoch:18 step:14473 [D loss: 0.036487, acc.: 100.00%] [G loss: 0.402251]\n",
      "epoch:18 step:14474 [D loss: 0.167454, acc.: 92.19%] [G loss: 1.029224]\n",
      "epoch:18 step:14475 [D loss: 0.026022, acc.: 100.00%] [G loss: 2.587842]\n",
      "epoch:18 step:14476 [D loss: 0.163210, acc.: 92.19%] [G loss: 0.956363]\n",
      "epoch:18 step:14477 [D loss: 0.053310, acc.: 99.22%] [G loss: 0.237665]\n",
      "epoch:18 step:14478 [D loss: 0.026418, acc.: 99.22%] [G loss: 0.676775]\n",
      "epoch:18 step:14479 [D loss: 0.002602, acc.: 100.00%] [G loss: 0.281270]\n",
      "epoch:18 step:14480 [D loss: 0.017847, acc.: 100.00%] [G loss: 0.230776]\n",
      "epoch:18 step:14481 [D loss: 0.023410, acc.: 100.00%] [G loss: 0.125014]\n",
      "epoch:18 step:14482 [D loss: 0.262629, acc.: 87.50%] [G loss: 2.187059]\n",
      "epoch:18 step:14483 [D loss: 0.223048, acc.: 90.62%] [G loss: 1.299731]\n",
      "epoch:18 step:14484 [D loss: 0.024445, acc.: 100.00%] [G loss: 0.695337]\n",
      "epoch:18 step:14485 [D loss: 0.004859, acc.: 100.00%] [G loss: 0.673067]\n",
      "epoch:18 step:14486 [D loss: 0.064223, acc.: 99.22%] [G loss: 0.327837]\n",
      "epoch:18 step:14487 [D loss: 0.107129, acc.: 96.88%] [G loss: 0.626193]\n",
      "epoch:18 step:14488 [D loss: 0.367711, acc.: 81.25%] [G loss: 0.978663]\n",
      "epoch:18 step:14489 [D loss: 0.446894, acc.: 75.78%] [G loss: 4.577463]\n",
      "epoch:18 step:14490 [D loss: 0.345574, acc.: 82.81%] [G loss: 1.892459]\n",
      "epoch:18 step:14491 [D loss: 0.005425, acc.: 100.00%] [G loss: 5.416617]\n",
      "epoch:18 step:14492 [D loss: 0.011652, acc.: 100.00%] [G loss: 4.597832]\n",
      "epoch:18 step:14493 [D loss: 0.033024, acc.: 100.00%] [G loss: 4.654175]\n",
      "epoch:18 step:14494 [D loss: 0.032870, acc.: 99.22%] [G loss: 4.481560]\n",
      "epoch:18 step:14495 [D loss: 0.015434, acc.: 100.00%] [G loss: 4.533024]\n",
      "epoch:18 step:14496 [D loss: 0.014654, acc.: 100.00%] [G loss: 4.559704]\n",
      "epoch:18 step:14497 [D loss: 0.017421, acc.: 100.00%] [G loss: 4.519142]\n",
      "epoch:18 step:14498 [D loss: 0.025863, acc.: 100.00%] [G loss: 0.391410]\n",
      "epoch:18 step:14499 [D loss: 0.011375, acc.: 100.00%] [G loss: 3.849017]\n",
      "epoch:18 step:14500 [D loss: 0.018082, acc.: 100.00%] [G loss: 0.318117]\n",
      "epoch:18 step:14501 [D loss: 0.016388, acc.: 100.00%] [G loss: 0.318117]\n",
      "epoch:18 step:14502 [D loss: 0.071408, acc.: 100.00%] [G loss: 3.841103]\n",
      "epoch:18 step:14503 [D loss: 0.041206, acc.: 99.22%] [G loss: 2.886999]\n",
      "epoch:18 step:14504 [D loss: 0.011893, acc.: 100.00%] [G loss: 2.709499]\n",
      "epoch:18 step:14505 [D loss: 0.022876, acc.: 100.00%] [G loss: 0.634240]\n",
      "epoch:18 step:14506 [D loss: 0.056544, acc.: 100.00%] [G loss: 1.760070]\n",
      "epoch:18 step:14507 [D loss: 0.019751, acc.: 100.00%] [G loss: 2.063013]\n",
      "epoch:18 step:14508 [D loss: 0.066701, acc.: 97.66%] [G loss: 1.629444]\n",
      "epoch:18 step:14509 [D loss: 0.092464, acc.: 99.22%] [G loss: 0.725462]\n",
      "epoch:18 step:14510 [D loss: 0.036391, acc.: 100.00%] [G loss: 0.758039]\n",
      "epoch:18 step:14511 [D loss: 0.029900, acc.: 99.22%] [G loss: 0.664647]\n",
      "epoch:18 step:14512 [D loss: 0.019495, acc.: 100.00%] [G loss: 0.555576]\n",
      "epoch:18 step:14513 [D loss: 0.062863, acc.: 97.66%] [G loss: 0.702905]\n",
      "epoch:18 step:14514 [D loss: 0.008460, acc.: 100.00%] [G loss: 3.662473]\n",
      "epoch:18 step:14515 [D loss: 0.067983, acc.: 98.44%] [G loss: 3.560988]\n",
      "epoch:18 step:14516 [D loss: 0.163760, acc.: 93.75%] [G loss: 4.755363]\n",
      "epoch:18 step:14517 [D loss: 0.254319, acc.: 92.19%] [G loss: 4.277409]\n",
      "epoch:18 step:14518 [D loss: 0.484341, acc.: 77.34%] [G loss: 6.509717]\n",
      "epoch:18 step:14519 [D loss: 0.392992, acc.: 81.25%] [G loss: 6.098866]\n",
      "epoch:18 step:14520 [D loss: 0.030449, acc.: 99.22%] [G loss: 4.830772]\n",
      "epoch:18 step:14521 [D loss: 0.012099, acc.: 100.00%] [G loss: 1.373114]\n",
      "epoch:18 step:14522 [D loss: 0.026304, acc.: 99.22%] [G loss: 3.798068]\n",
      "epoch:18 step:14523 [D loss: 0.015516, acc.: 100.00%] [G loss: 3.231591]\n",
      "epoch:18 step:14524 [D loss: 0.004918, acc.: 100.00%] [G loss: 3.003169]\n",
      "epoch:18 step:14525 [D loss: 0.022132, acc.: 100.00%] [G loss: 2.947226]\n",
      "epoch:18 step:14526 [D loss: 0.005244, acc.: 100.00%] [G loss: 2.781846]\n",
      "epoch:18 step:14527 [D loss: 0.009500, acc.: 100.00%] [G loss: 2.586353]\n",
      "epoch:18 step:14528 [D loss: 0.011086, acc.: 100.00%] [G loss: 1.049357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14529 [D loss: 0.002677, acc.: 100.00%] [G loss: 0.731101]\n",
      "epoch:18 step:14530 [D loss: 0.003365, acc.: 100.00%] [G loss: 1.693583]\n",
      "epoch:18 step:14531 [D loss: 0.016925, acc.: 100.00%] [G loss: 1.761014]\n",
      "epoch:18 step:14532 [D loss: 0.008192, acc.: 100.00%] [G loss: 2.674208]\n",
      "epoch:18 step:14533 [D loss: 0.031500, acc.: 99.22%] [G loss: 0.830335]\n",
      "epoch:18 step:14534 [D loss: 0.140318, acc.: 92.97%] [G loss: 2.518517]\n",
      "epoch:18 step:14535 [D loss: 0.011294, acc.: 100.00%] [G loss: 3.202268]\n",
      "epoch:18 step:14536 [D loss: 0.137197, acc.: 92.19%] [G loss: 1.451949]\n",
      "epoch:18 step:14537 [D loss: 0.023463, acc.: 99.22%] [G loss: 1.310265]\n",
      "epoch:18 step:14538 [D loss: 0.020853, acc.: 100.00%] [G loss: 1.100414]\n",
      "epoch:18 step:14539 [D loss: 0.021207, acc.: 100.00%] [G loss: 2.333592]\n",
      "epoch:18 step:14540 [D loss: 0.023480, acc.: 100.00%] [G loss: 1.167700]\n",
      "epoch:18 step:14541 [D loss: 0.042023, acc.: 99.22%] [G loss: 1.912446]\n",
      "epoch:18 step:14542 [D loss: 0.006208, acc.: 100.00%] [G loss: 2.777139]\n",
      "epoch:18 step:14543 [D loss: 0.019974, acc.: 99.22%] [G loss: 5.011862]\n",
      "epoch:18 step:14544 [D loss: 0.033735, acc.: 99.22%] [G loss: 1.983198]\n",
      "epoch:18 step:14545 [D loss: 0.049508, acc.: 100.00%] [G loss: 0.100651]\n",
      "epoch:18 step:14546 [D loss: 0.001175, acc.: 100.00%] [G loss: 3.654242]\n",
      "epoch:18 step:14547 [D loss: 0.006917, acc.: 100.00%] [G loss: 3.152400]\n",
      "epoch:18 step:14548 [D loss: 0.015237, acc.: 100.00%] [G loss: 2.921636]\n",
      "epoch:18 step:14549 [D loss: 0.010275, acc.: 100.00%] [G loss: 0.182206]\n",
      "epoch:18 step:14550 [D loss: 0.007148, acc.: 100.00%] [G loss: 1.212039]\n",
      "epoch:18 step:14551 [D loss: 0.004539, acc.: 100.00%] [G loss: 1.115093]\n",
      "epoch:18 step:14552 [D loss: 0.004838, acc.: 100.00%] [G loss: 0.867683]\n",
      "epoch:18 step:14553 [D loss: 0.002106, acc.: 100.00%] [G loss: 0.791407]\n",
      "epoch:18 step:14554 [D loss: 0.033218, acc.: 99.22%] [G loss: 0.876050]\n",
      "epoch:18 step:14555 [D loss: 0.036778, acc.: 99.22%] [G loss: 0.860554]\n",
      "epoch:18 step:14556 [D loss: 0.049900, acc.: 98.44%] [G loss: 0.852765]\n",
      "epoch:18 step:14557 [D loss: 0.030477, acc.: 100.00%] [G loss: 0.764700]\n",
      "epoch:18 step:14558 [D loss: 0.007610, acc.: 100.00%] [G loss: 1.285332]\n",
      "epoch:18 step:14559 [D loss: 0.069189, acc.: 98.44%] [G loss: 0.018594]\n",
      "epoch:18 step:14560 [D loss: 0.012005, acc.: 100.00%] [G loss: 1.398224]\n",
      "epoch:18 step:14561 [D loss: 0.015179, acc.: 100.00%] [G loss: 1.020601]\n",
      "epoch:18 step:14562 [D loss: 0.005417, acc.: 100.00%] [G loss: 1.655285]\n",
      "epoch:18 step:14563 [D loss: 0.022257, acc.: 100.00%] [G loss: 4.266699]\n",
      "epoch:18 step:14564 [D loss: 0.006385, acc.: 100.00%] [G loss: 1.231464]\n",
      "epoch:18 step:14565 [D loss: 0.013292, acc.: 100.00%] [G loss: 0.912410]\n",
      "epoch:18 step:14566 [D loss: 0.012764, acc.: 100.00%] [G loss: 1.620648]\n",
      "epoch:18 step:14567 [D loss: 0.057082, acc.: 98.44%] [G loss: 0.188909]\n",
      "epoch:18 step:14568 [D loss: 0.017334, acc.: 100.00%] [G loss: 2.675601]\n",
      "epoch:18 step:14569 [D loss: 0.006998, acc.: 100.00%] [G loss: 3.141059]\n",
      "epoch:18 step:14570 [D loss: 0.003532, acc.: 100.00%] [G loss: 0.724462]\n",
      "epoch:18 step:14571 [D loss: 0.022230, acc.: 100.00%] [G loss: 1.905524]\n",
      "epoch:18 step:14572 [D loss: 0.007182, acc.: 100.00%] [G loss: 1.721320]\n",
      "epoch:18 step:14573 [D loss: 0.014326, acc.: 100.00%] [G loss: 2.351747]\n",
      "epoch:18 step:14574 [D loss: 0.004487, acc.: 100.00%] [G loss: 0.688237]\n",
      "epoch:18 step:14575 [D loss: 0.004723, acc.: 100.00%] [G loss: 2.992103]\n",
      "epoch:18 step:14576 [D loss: 0.007262, acc.: 100.00%] [G loss: 2.004666]\n",
      "epoch:18 step:14577 [D loss: 0.017606, acc.: 99.22%] [G loss: 5.715453]\n",
      "epoch:18 step:14578 [D loss: 0.048934, acc.: 98.44%] [G loss: 6.343991]\n",
      "epoch:18 step:14579 [D loss: 0.014086, acc.: 99.22%] [G loss: 7.031621]\n",
      "epoch:18 step:14580 [D loss: 0.043074, acc.: 99.22%] [G loss: 5.448640]\n",
      "epoch:18 step:14581 [D loss: 0.011785, acc.: 100.00%] [G loss: 5.813656]\n",
      "epoch:18 step:14582 [D loss: 0.014607, acc.: 99.22%] [G loss: 4.367201]\n",
      "epoch:18 step:14583 [D loss: 0.003644, acc.: 100.00%] [G loss: 4.616488]\n",
      "epoch:18 step:14584 [D loss: 0.168583, acc.: 91.41%] [G loss: 3.534594]\n",
      "epoch:18 step:14585 [D loss: 0.059353, acc.: 98.44%] [G loss: 6.295249]\n",
      "epoch:18 step:14586 [D loss: 0.002084, acc.: 100.00%] [G loss: 0.020277]\n",
      "epoch:18 step:14587 [D loss: 0.123392, acc.: 97.66%] [G loss: 9.482103]\n",
      "epoch:18 step:14588 [D loss: 0.000411, acc.: 100.00%] [G loss: 11.700788]\n",
      "epoch:18 step:14589 [D loss: 0.022975, acc.: 99.22%] [G loss: 3.323080]\n",
      "epoch:18 step:14590 [D loss: 0.060663, acc.: 96.88%] [G loss: 8.032812]\n",
      "epoch:18 step:14591 [D loss: 0.003469, acc.: 100.00%] [G loss: 0.043293]\n",
      "epoch:18 step:14592 [D loss: 0.045685, acc.: 98.44%] [G loss: 0.149369]\n",
      "epoch:18 step:14593 [D loss: 0.007347, acc.: 100.00%] [G loss: 0.285760]\n",
      "epoch:18 step:14594 [D loss: 0.011696, acc.: 100.00%] [G loss: 8.758940]\n",
      "epoch:18 step:14595 [D loss: 0.020764, acc.: 99.22%] [G loss: 8.851986]\n",
      "epoch:18 step:14596 [D loss: 0.005143, acc.: 100.00%] [G loss: 7.515052]\n",
      "epoch:18 step:14597 [D loss: 0.003019, acc.: 100.00%] [G loss: 8.498186]\n",
      "epoch:18 step:14598 [D loss: 0.002878, acc.: 100.00%] [G loss: 9.367735]\n",
      "epoch:18 step:14599 [D loss: 0.030517, acc.: 100.00%] [G loss: 8.768583]\n",
      "epoch:18 step:14600 [D loss: 0.003102, acc.: 100.00%] [G loss: 5.894725]\n",
      "epoch:18 step:14601 [D loss: 0.005352, acc.: 100.00%] [G loss: 5.611473]\n",
      "epoch:18 step:14602 [D loss: 0.021881, acc.: 100.00%] [G loss: 6.161194]\n",
      "epoch:18 step:14603 [D loss: 0.007605, acc.: 100.00%] [G loss: 2.027496]\n",
      "epoch:18 step:14604 [D loss: 0.012866, acc.: 100.00%] [G loss: 6.032271]\n",
      "epoch:18 step:14605 [D loss: 0.100375, acc.: 98.44%] [G loss: 2.678170]\n",
      "epoch:18 step:14606 [D loss: 0.004007, acc.: 100.00%] [G loss: 1.369342]\n",
      "epoch:18 step:14607 [D loss: 0.002193, acc.: 100.00%] [G loss: 1.512246]\n",
      "epoch:18 step:14608 [D loss: 0.083646, acc.: 97.66%] [G loss: 5.531435]\n",
      "epoch:18 step:14609 [D loss: 0.001660, acc.: 100.00%] [G loss: 4.035111]\n",
      "epoch:18 step:14610 [D loss: 0.033493, acc.: 99.22%] [G loss: 8.540439]\n",
      "epoch:18 step:14611 [D loss: 0.006296, acc.: 100.00%] [G loss: 1.826720]\n",
      "epoch:18 step:14612 [D loss: 0.023293, acc.: 99.22%] [G loss: 5.747418]\n",
      "epoch:18 step:14613 [D loss: 0.003020, acc.: 100.00%] [G loss: 3.897368]\n",
      "epoch:18 step:14614 [D loss: 0.003214, acc.: 100.00%] [G loss: 2.682075]\n",
      "epoch:18 step:14615 [D loss: 0.030350, acc.: 100.00%] [G loss: 4.448979]\n",
      "epoch:18 step:14616 [D loss: 0.010400, acc.: 100.00%] [G loss: 2.543925]\n",
      "epoch:18 step:14617 [D loss: 0.164345, acc.: 93.75%] [G loss: 9.761608]\n",
      "epoch:18 step:14618 [D loss: 0.060337, acc.: 97.66%] [G loss: 12.377903]\n",
      "epoch:18 step:14619 [D loss: 0.057530, acc.: 99.22%] [G loss: 9.379009]\n",
      "epoch:18 step:14620 [D loss: 0.000819, acc.: 100.00%] [G loss: 9.235584]\n",
      "epoch:18 step:14621 [D loss: 0.000280, acc.: 100.00%] [G loss: 8.494665]\n",
      "epoch:18 step:14622 [D loss: 0.000257, acc.: 100.00%] [G loss: 1.243069]\n",
      "epoch:18 step:14623 [D loss: 0.008211, acc.: 100.00%] [G loss: 7.770223]\n",
      "epoch:18 step:14624 [D loss: 0.006149, acc.: 100.00%] [G loss: 0.994534]\n",
      "epoch:18 step:14625 [D loss: 0.004705, acc.: 100.00%] [G loss: 9.036472]\n",
      "epoch:18 step:14626 [D loss: 0.001149, acc.: 100.00%] [G loss: 7.192977]\n",
      "epoch:18 step:14627 [D loss: 0.000975, acc.: 100.00%] [G loss: 6.997870]\n",
      "epoch:18 step:14628 [D loss: 0.001543, acc.: 100.00%] [G loss: 6.715550]\n",
      "epoch:18 step:14629 [D loss: 0.004301, acc.: 100.00%] [G loss: 7.023927]\n",
      "epoch:18 step:14630 [D loss: 0.000946, acc.: 100.00%] [G loss: 6.307745]\n",
      "epoch:18 step:14631 [D loss: 0.001555, acc.: 100.00%] [G loss: 6.256817]\n",
      "epoch:18 step:14632 [D loss: 0.002544, acc.: 100.00%] [G loss: 5.003796]\n",
      "epoch:18 step:14633 [D loss: 0.007338, acc.: 100.00%] [G loss: 4.174459]\n",
      "epoch:18 step:14634 [D loss: 0.005593, acc.: 100.00%] [G loss: 4.946620]\n",
      "epoch:18 step:14635 [D loss: 0.032540, acc.: 99.22%] [G loss: 2.404207]\n",
      "epoch:18 step:14636 [D loss: 0.001325, acc.: 100.00%] [G loss: 6.142834]\n",
      "epoch:18 step:14637 [D loss: 0.002377, acc.: 100.00%] [G loss: 6.748982]\n",
      "epoch:18 step:14638 [D loss: 0.005913, acc.: 100.00%] [G loss: 0.588753]\n",
      "epoch:18 step:14639 [D loss: 0.001536, acc.: 100.00%] [G loss: 6.799174]\n",
      "epoch:18 step:14640 [D loss: 0.013225, acc.: 99.22%] [G loss: 0.259822]\n",
      "epoch:18 step:14641 [D loss: 0.003917, acc.: 100.00%] [G loss: 0.112248]\n",
      "epoch:18 step:14642 [D loss: 0.009137, acc.: 100.00%] [G loss: 0.055752]\n",
      "epoch:18 step:14643 [D loss: 0.003573, acc.: 100.00%] [G loss: 5.581637]\n",
      "epoch:18 step:14644 [D loss: 0.004088, acc.: 100.00%] [G loss: 5.721004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14645 [D loss: 0.023083, acc.: 100.00%] [G loss: 4.664061]\n",
      "epoch:18 step:14646 [D loss: 0.001968, acc.: 100.00%] [G loss: 4.591372]\n",
      "epoch:18 step:14647 [D loss: 0.000891, acc.: 100.00%] [G loss: 4.379394]\n",
      "epoch:18 step:14648 [D loss: 0.013252, acc.: 99.22%] [G loss: 0.781748]\n",
      "epoch:18 step:14649 [D loss: 0.005001, acc.: 100.00%] [G loss: 1.785988]\n",
      "epoch:18 step:14650 [D loss: 0.124931, acc.: 95.31%] [G loss: 6.948111]\n",
      "epoch:18 step:14651 [D loss: 0.100360, acc.: 93.75%] [G loss: 4.969938]\n",
      "epoch:18 step:14652 [D loss: 0.003854, acc.: 100.00%] [G loss: 1.044537]\n",
      "epoch:18 step:14653 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.607484]\n",
      "epoch:18 step:14654 [D loss: 0.001510, acc.: 100.00%] [G loss: 0.886251]\n",
      "epoch:18 step:14655 [D loss: 0.004644, acc.: 100.00%] [G loss: 3.296844]\n",
      "epoch:18 step:14656 [D loss: 0.008047, acc.: 100.00%] [G loss: 0.043776]\n",
      "epoch:18 step:14657 [D loss: 0.000631, acc.: 100.00%] [G loss: 3.501791]\n",
      "epoch:18 step:14658 [D loss: 0.000679, acc.: 100.00%] [G loss: 7.078234]\n",
      "epoch:18 step:14659 [D loss: 0.000648, acc.: 100.00%] [G loss: 3.931080]\n",
      "epoch:18 step:14660 [D loss: 0.102744, acc.: 96.88%] [G loss: 7.118362]\n",
      "epoch:18 step:14661 [D loss: 0.029912, acc.: 100.00%] [G loss: 5.775358]\n",
      "epoch:18 step:14662 [D loss: 4.413199, acc.: 12.50%] [G loss: 7.627789]\n",
      "epoch:18 step:14663 [D loss: 0.007561, acc.: 100.00%] [G loss: 11.091425]\n",
      "epoch:18 step:14664 [D loss: 1.197219, acc.: 67.19%] [G loss: 3.539171]\n",
      "epoch:18 step:14665 [D loss: 0.066771, acc.: 96.88%] [G loss: 4.672609]\n",
      "epoch:18 step:14666 [D loss: 0.014464, acc.: 100.00%] [G loss: 4.099149]\n",
      "epoch:18 step:14667 [D loss: 0.013918, acc.: 100.00%] [G loss: 2.552381]\n",
      "epoch:18 step:14668 [D loss: 0.009700, acc.: 100.00%] [G loss: 0.897855]\n",
      "epoch:18 step:14669 [D loss: 0.010326, acc.: 100.00%] [G loss: 1.843431]\n",
      "epoch:18 step:14670 [D loss: 0.006707, acc.: 100.00%] [G loss: 0.290609]\n",
      "epoch:18 step:14671 [D loss: 0.056204, acc.: 99.22%] [G loss: 0.340063]\n",
      "epoch:18 step:14672 [D loss: 0.013453, acc.: 100.00%] [G loss: 0.226565]\n",
      "epoch:18 step:14673 [D loss: 0.008381, acc.: 100.00%] [G loss: 2.353169]\n",
      "epoch:18 step:14674 [D loss: 0.201641, acc.: 93.75%] [G loss: 2.176352]\n",
      "epoch:18 step:14675 [D loss: 0.003425, acc.: 100.00%] [G loss: 5.092642]\n",
      "epoch:18 step:14676 [D loss: 0.216349, acc.: 91.41%] [G loss: 2.817502]\n",
      "epoch:18 step:14677 [D loss: 0.035202, acc.: 99.22%] [G loss: 2.479156]\n",
      "epoch:18 step:14678 [D loss: 0.011220, acc.: 100.00%] [G loss: 1.627872]\n",
      "epoch:18 step:14679 [D loss: 0.003523, acc.: 100.00%] [G loss: 1.605414]\n",
      "epoch:18 step:14680 [D loss: 0.005344, acc.: 100.00%] [G loss: 1.186793]\n",
      "epoch:18 step:14681 [D loss: 0.027023, acc.: 100.00%] [G loss: 1.059095]\n",
      "epoch:18 step:14682 [D loss: 0.007704, acc.: 100.00%] [G loss: 0.868232]\n",
      "epoch:18 step:14683 [D loss: 0.010884, acc.: 100.00%] [G loss: 0.422527]\n",
      "epoch:18 step:14684 [D loss: 0.004514, acc.: 100.00%] [G loss: 1.378382]\n",
      "epoch:18 step:14685 [D loss: 0.013075, acc.: 100.00%] [G loss: 0.417218]\n",
      "epoch:18 step:14686 [D loss: 0.002375, acc.: 100.00%] [G loss: 0.558398]\n",
      "epoch:18 step:14687 [D loss: 0.077261, acc.: 99.22%] [G loss: 1.183406]\n",
      "epoch:18 step:14688 [D loss: 0.012380, acc.: 99.22%] [G loss: 2.315297]\n",
      "epoch:18 step:14689 [D loss: 0.029106, acc.: 98.44%] [G loss: 5.470267]\n",
      "epoch:18 step:14690 [D loss: 0.008422, acc.: 100.00%] [G loss: 1.064976]\n",
      "epoch:18 step:14691 [D loss: 0.010198, acc.: 100.00%] [G loss: 2.308200]\n",
      "epoch:18 step:14692 [D loss: 0.010278, acc.: 100.00%] [G loss: 0.857645]\n",
      "epoch:18 step:14693 [D loss: 0.007212, acc.: 100.00%] [G loss: 1.969096]\n",
      "epoch:18 step:14694 [D loss: 0.003055, acc.: 100.00%] [G loss: 2.492497]\n",
      "epoch:18 step:14695 [D loss: 0.013285, acc.: 100.00%] [G loss: 1.692168]\n",
      "epoch:18 step:14696 [D loss: 0.040990, acc.: 99.22%] [G loss: 2.962028]\n",
      "epoch:18 step:14697 [D loss: 0.001995, acc.: 100.00%] [G loss: 5.684810]\n",
      "epoch:18 step:14698 [D loss: 0.003258, acc.: 100.00%] [G loss: 5.051038]\n",
      "epoch:18 step:14699 [D loss: 0.003726, acc.: 100.00%] [G loss: 5.154555]\n",
      "epoch:18 step:14700 [D loss: 0.004657, acc.: 100.00%] [G loss: 3.550906]\n",
      "epoch:18 step:14701 [D loss: 0.003260, acc.: 100.00%] [G loss: 0.627294]\n",
      "epoch:18 step:14702 [D loss: 0.013859, acc.: 100.00%] [G loss: 2.748051]\n",
      "epoch:18 step:14703 [D loss: 0.008543, acc.: 100.00%] [G loss: 2.479356]\n",
      "epoch:18 step:14704 [D loss: 0.018438, acc.: 100.00%] [G loss: 2.266383]\n",
      "epoch:18 step:14705 [D loss: 0.006896, acc.: 100.00%] [G loss: 0.271580]\n",
      "epoch:18 step:14706 [D loss: 0.007332, acc.: 100.00%] [G loss: 3.681874]\n",
      "epoch:18 step:14707 [D loss: 0.308970, acc.: 85.94%] [G loss: 11.493335]\n",
      "epoch:18 step:14708 [D loss: 0.904737, acc.: 65.62%] [G loss: 10.791142]\n",
      "epoch:18 step:14709 [D loss: 0.000251, acc.: 100.00%] [G loss: 9.623932]\n",
      "epoch:18 step:14710 [D loss: 0.000446, acc.: 100.00%] [G loss: 4.863358]\n",
      "epoch:18 step:14711 [D loss: 0.010166, acc.: 100.00%] [G loss: 10.138214]\n",
      "epoch:18 step:14712 [D loss: 0.014301, acc.: 99.22%] [G loss: 9.993689]\n",
      "epoch:18 step:14713 [D loss: 0.001011, acc.: 100.00%] [G loss: 8.355171]\n",
      "epoch:18 step:14714 [D loss: 0.002018, acc.: 100.00%] [G loss: 9.747183]\n",
      "epoch:18 step:14715 [D loss: 0.001972, acc.: 100.00%] [G loss: 9.445507]\n",
      "epoch:18 step:14716 [D loss: 0.000863, acc.: 100.00%] [G loss: 0.005392]\n",
      "epoch:18 step:14717 [D loss: 0.319630, acc.: 85.94%] [G loss: 1.780963]\n",
      "epoch:18 step:14718 [D loss: 0.453795, acc.: 78.91%] [G loss: 9.096800]\n",
      "epoch:18 step:14719 [D loss: 0.069110, acc.: 96.88%] [G loss: 8.807545]\n",
      "epoch:18 step:14720 [D loss: 0.000529, acc.: 100.00%] [G loss: 0.917869]\n",
      "epoch:18 step:14721 [D loss: 0.002342, acc.: 100.00%] [G loss: 1.587373]\n",
      "epoch:18 step:14722 [D loss: 0.000979, acc.: 100.00%] [G loss: 8.714001]\n",
      "epoch:18 step:14723 [D loss: 0.155224, acc.: 93.75%] [G loss: 7.440784]\n",
      "epoch:18 step:14724 [D loss: 0.119800, acc.: 95.31%] [G loss: 7.388580]\n",
      "epoch:18 step:14725 [D loss: 0.000587, acc.: 100.00%] [G loss: 7.157619]\n",
      "epoch:18 step:14726 [D loss: 0.010504, acc.: 100.00%] [G loss: 0.188722]\n",
      "epoch:18 step:14727 [D loss: 0.000681, acc.: 100.00%] [G loss: 0.081001]\n",
      "epoch:18 step:14728 [D loss: 0.008295, acc.: 100.00%] [G loss: 0.053670]\n",
      "epoch:18 step:14729 [D loss: 0.000898, acc.: 100.00%] [G loss: 5.681036]\n",
      "epoch:18 step:14730 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.263800]\n",
      "epoch:18 step:14731 [D loss: 0.001294, acc.: 100.00%] [G loss: 0.363752]\n",
      "epoch:18 step:14732 [D loss: 0.005602, acc.: 100.00%] [G loss: 5.315803]\n",
      "epoch:18 step:14733 [D loss: 0.012603, acc.: 100.00%] [G loss: 0.018403]\n",
      "epoch:18 step:14734 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.029162]\n",
      "epoch:18 step:14735 [D loss: 0.002566, acc.: 100.00%] [G loss: 4.790069]\n",
      "epoch:18 step:14736 [D loss: 0.000864, acc.: 100.00%] [G loss: 6.374586]\n",
      "epoch:18 step:14737 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.017954]\n",
      "epoch:18 step:14738 [D loss: 0.001635, acc.: 100.00%] [G loss: 4.325829]\n",
      "epoch:18 step:14739 [D loss: 0.001329, acc.: 100.00%] [G loss: 3.298837]\n",
      "epoch:18 step:14740 [D loss: 0.000722, acc.: 100.00%] [G loss: 5.437196]\n",
      "epoch:18 step:14741 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.018809]\n",
      "epoch:18 step:14742 [D loss: 0.439713, acc.: 78.12%] [G loss: 8.490301]\n",
      "epoch:18 step:14743 [D loss: 1.301722, acc.: 57.03%] [G loss: 1.919395]\n",
      "epoch:18 step:14744 [D loss: 0.016170, acc.: 100.00%] [G loss: 0.522558]\n",
      "epoch:18 step:14745 [D loss: 0.084434, acc.: 96.88%] [G loss: 0.673090]\n",
      "epoch:18 step:14746 [D loss: 0.008679, acc.: 99.22%] [G loss: 0.976327]\n",
      "epoch:18 step:14747 [D loss: 0.000702, acc.: 100.00%] [G loss: 0.743397]\n",
      "epoch:18 step:14748 [D loss: 0.005698, acc.: 100.00%] [G loss: 3.444436]\n",
      "epoch:18 step:14749 [D loss: 0.002016, acc.: 100.00%] [G loss: 1.035897]\n",
      "epoch:18 step:14750 [D loss: 0.008350, acc.: 100.00%] [G loss: 0.300275]\n",
      "epoch:18 step:14751 [D loss: 0.002680, acc.: 100.00%] [G loss: 0.597677]\n",
      "epoch:18 step:14752 [D loss: 0.004287, acc.: 100.00%] [G loss: 0.250885]\n",
      "epoch:18 step:14753 [D loss: 0.050046, acc.: 100.00%] [G loss: 0.819904]\n",
      "epoch:18 step:14754 [D loss: 0.009897, acc.: 100.00%] [G loss: 1.242735]\n",
      "epoch:18 step:14755 [D loss: 0.007091, acc.: 100.00%] [G loss: 0.788958]\n",
      "epoch:18 step:14756 [D loss: 0.021180, acc.: 99.22%] [G loss: 0.306075]\n",
      "epoch:18 step:14757 [D loss: 0.004682, acc.: 100.00%] [G loss: 0.526165]\n",
      "epoch:18 step:14758 [D loss: 0.009555, acc.: 100.00%] [G loss: 0.393778]\n",
      "epoch:18 step:14759 [D loss: 0.007348, acc.: 100.00%] [G loss: 1.329277]\n",
      "epoch:18 step:14760 [D loss: 0.135011, acc.: 96.09%] [G loss: 0.602580]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18 step:14761 [D loss: 0.025541, acc.: 99.22%] [G loss: 2.522074]\n",
      "epoch:18 step:14762 [D loss: 0.029373, acc.: 99.22%] [G loss: 0.139931]\n",
      "epoch:18 step:14763 [D loss: 0.017222, acc.: 99.22%] [G loss: 0.139515]\n",
      "epoch:18 step:14764 [D loss: 0.012262, acc.: 100.00%] [G loss: 0.190713]\n",
      "epoch:18 step:14765 [D loss: 0.030813, acc.: 100.00%] [G loss: 0.523085]\n",
      "epoch:18 step:14766 [D loss: 0.015662, acc.: 100.00%] [G loss: 0.760138]\n",
      "epoch:18 step:14767 [D loss: 0.001573, acc.: 100.00%] [G loss: 0.217589]\n",
      "epoch:18 step:14768 [D loss: 0.002687, acc.: 100.00%] [G loss: 0.105919]\n",
      "epoch:18 step:14769 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.148448]\n",
      "epoch:18 step:14770 [D loss: 0.000752, acc.: 100.00%] [G loss: 0.161613]\n",
      "epoch:18 step:14771 [D loss: 0.114193, acc.: 96.09%] [G loss: 0.017140]\n",
      "epoch:18 step:14772 [D loss: 0.026611, acc.: 99.22%] [G loss: 0.001035]\n",
      "epoch:18 step:14773 [D loss: 0.002178, acc.: 100.00%] [G loss: 0.005450]\n",
      "epoch:18 step:14774 [D loss: 0.075820, acc.: 98.44%] [G loss: 0.347285]\n",
      "epoch:18 step:14775 [D loss: 0.000098, acc.: 100.00%] [G loss: 1.713826]\n",
      "epoch:18 step:14776 [D loss: 0.009494, acc.: 100.00%] [G loss: 0.117248]\n",
      "epoch:18 step:14777 [D loss: 0.007497, acc.: 100.00%] [G loss: 1.697542]\n",
      "epoch:18 step:14778 [D loss: 0.001619, acc.: 100.00%] [G loss: 0.413219]\n",
      "epoch:18 step:14779 [D loss: 0.002485, acc.: 100.00%] [G loss: 0.305821]\n",
      "epoch:18 step:14780 [D loss: 0.017396, acc.: 99.22%] [G loss: 0.123199]\n",
      "epoch:18 step:14781 [D loss: 0.002975, acc.: 100.00%] [G loss: 0.055663]\n",
      "epoch:18 step:14782 [D loss: 0.002920, acc.: 100.00%] [G loss: 0.062878]\n",
      "epoch:18 step:14783 [D loss: 0.049633, acc.: 98.44%] [G loss: 0.031494]\n",
      "epoch:18 step:14784 [D loss: 0.001495, acc.: 100.00%] [G loss: 0.456538]\n",
      "epoch:18 step:14785 [D loss: 0.011355, acc.: 100.00%] [G loss: 0.106283]\n",
      "epoch:18 step:14786 [D loss: 0.000886, acc.: 100.00%] [G loss: 0.488839]\n",
      "epoch:18 step:14787 [D loss: 0.132316, acc.: 93.75%] [G loss: 2.988548]\n",
      "epoch:18 step:14788 [D loss: 0.181939, acc.: 91.41%] [G loss: 0.214454]\n",
      "epoch:18 step:14789 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.834483]\n",
      "epoch:18 step:14790 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.209398]\n",
      "epoch:18 step:14791 [D loss: 0.019765, acc.: 100.00%] [G loss: 0.277042]\n",
      "epoch:18 step:14792 [D loss: 0.001226, acc.: 100.00%] [G loss: 0.420184]\n",
      "epoch:18 step:14793 [D loss: 0.017098, acc.: 100.00%] [G loss: 0.092017]\n",
      "epoch:18 step:14794 [D loss: 0.012100, acc.: 100.00%] [G loss: 0.278349]\n",
      "epoch:18 step:14795 [D loss: 0.002661, acc.: 100.00%] [G loss: 0.397184]\n",
      "epoch:18 step:14796 [D loss: 0.001515, acc.: 100.00%] [G loss: 0.982373]\n",
      "epoch:18 step:14797 [D loss: 0.011708, acc.: 100.00%] [G loss: 0.016095]\n",
      "epoch:18 step:14798 [D loss: 0.073034, acc.: 98.44%] [G loss: 0.096223]\n",
      "epoch:18 step:14799 [D loss: 0.006371, acc.: 100.00%] [G loss: 3.697394]\n",
      "epoch:18 step:14800 [D loss: 0.035833, acc.: 98.44%] [G loss: 0.439624]\n",
      "epoch:18 step:14801 [D loss: 0.003212, acc.: 100.00%] [G loss: 1.552474]\n",
      "epoch:18 step:14802 [D loss: 0.002080, acc.: 100.00%] [G loss: 1.097368]\n",
      "epoch:18 step:14803 [D loss: 0.001327, acc.: 100.00%] [G loss: 0.005436]\n",
      "epoch:18 step:14804 [D loss: 0.001222, acc.: 100.00%] [G loss: 0.118565]\n",
      "epoch:18 step:14805 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.003761]\n",
      "epoch:18 step:14806 [D loss: 0.004966, acc.: 100.00%] [G loss: 0.006551]\n",
      "epoch:18 step:14807 [D loss: 0.002390, acc.: 100.00%] [G loss: 0.000508]\n",
      "epoch:18 step:14808 [D loss: 0.004947, acc.: 100.00%] [G loss: 0.091240]\n",
      "epoch:18 step:14809 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.015832]\n",
      "epoch:18 step:14810 [D loss: 0.007835, acc.: 100.00%] [G loss: 0.081357]\n",
      "epoch:18 step:14811 [D loss: 0.003232, acc.: 100.00%] [G loss: 0.246319]\n",
      "epoch:18 step:14812 [D loss: 0.049690, acc.: 99.22%] [G loss: 0.287296]\n",
      "epoch:18 step:14813 [D loss: 0.000940, acc.: 100.00%] [G loss: 0.254564]\n",
      "epoch:18 step:14814 [D loss: 0.018158, acc.: 100.00%] [G loss: 0.013460]\n",
      "epoch:18 step:14815 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.520851]\n",
      "epoch:18 step:14816 [D loss: 0.003163, acc.: 100.00%] [G loss: 0.148583]\n",
      "epoch:18 step:14817 [D loss: 0.031644, acc.: 99.22%] [G loss: 0.031372]\n",
      "epoch:18 step:14818 [D loss: 0.010303, acc.: 100.00%] [G loss: 0.000633]\n",
      "epoch:18 step:14819 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.002820]\n",
      "epoch:18 step:14820 [D loss: 0.013887, acc.: 100.00%] [G loss: 0.001033]\n",
      "epoch:18 step:14821 [D loss: 0.001971, acc.: 100.00%] [G loss: 0.077388]\n",
      "epoch:18 step:14822 [D loss: 0.018387, acc.: 100.00%] [G loss: 0.004998]\n",
      "epoch:18 step:14823 [D loss: 0.000594, acc.: 100.00%] [G loss: 0.005990]\n",
      "epoch:18 step:14824 [D loss: 0.008962, acc.: 100.00%] [G loss: 0.002378]\n",
      "epoch:18 step:14825 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.001062]\n",
      "epoch:18 step:14826 [D loss: 0.012875, acc.: 100.00%] [G loss: 0.006728]\n",
      "epoch:18 step:14827 [D loss: 0.002468, acc.: 100.00%] [G loss: 0.003471]\n",
      "epoch:18 step:14828 [D loss: 0.006890, acc.: 100.00%] [G loss: 0.408826]\n",
      "epoch:18 step:14829 [D loss: 0.381338, acc.: 83.59%] [G loss: 0.002200]\n",
      "epoch:18 step:14830 [D loss: 0.009425, acc.: 100.00%] [G loss: 0.013539]\n",
      "epoch:18 step:14831 [D loss: 0.006748, acc.: 100.00%] [G loss: 1.710193]\n",
      "epoch:18 step:14832 [D loss: 0.005746, acc.: 100.00%] [G loss: 1.990167]\n",
      "epoch:18 step:14833 [D loss: 0.001367, acc.: 100.00%] [G loss: 0.405078]\n",
      "epoch:18 step:14834 [D loss: 0.016258, acc.: 100.00%] [G loss: 0.347963]\n",
      "epoch:18 step:14835 [D loss: 0.001444, acc.: 100.00%] [G loss: 2.080401]\n",
      "epoch:18 step:14836 [D loss: 0.016111, acc.: 100.00%] [G loss: 0.107537]\n",
      "epoch:18 step:14837 [D loss: 0.180219, acc.: 91.41%] [G loss: 4.997308]\n",
      "epoch:18 step:14838 [D loss: 0.032918, acc.: 99.22%] [G loss: 6.904611]\n",
      "epoch:18 step:14839 [D loss: 0.942864, acc.: 73.44%] [G loss: 4.489528]\n",
      "epoch:19 step:14840 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.405665]\n",
      "epoch:19 step:14841 [D loss: 0.490398, acc.: 82.81%] [G loss: 6.058468]\n",
      "epoch:19 step:14842 [D loss: 0.104228, acc.: 96.88%] [G loss: 0.521220]\n",
      "epoch:19 step:14843 [D loss: 0.308619, acc.: 89.06%] [G loss: 0.393768]\n",
      "epoch:19 step:14844 [D loss: 0.007199, acc.: 100.00%] [G loss: 0.335439]\n",
      "epoch:19 step:14845 [D loss: 0.448817, acc.: 82.03%] [G loss: 1.355757]\n",
      "epoch:19 step:14846 [D loss: 0.828836, acc.: 66.41%] [G loss: 0.063350]\n",
      "epoch:19 step:14847 [D loss: 0.006207, acc.: 100.00%] [G loss: 0.074840]\n",
      "epoch:19 step:14848 [D loss: 0.002588, acc.: 100.00%] [G loss: 0.004788]\n",
      "epoch:19 step:14849 [D loss: 0.022568, acc.: 99.22%] [G loss: 0.079941]\n",
      "epoch:19 step:14850 [D loss: 0.008622, acc.: 100.00%] [G loss: 0.058147]\n",
      "epoch:19 step:14851 [D loss: 0.021087, acc.: 99.22%] [G loss: 2.023949]\n",
      "epoch:19 step:14852 [D loss: 0.000038, acc.: 100.00%] [G loss: 1.403469]\n",
      "epoch:19 step:14853 [D loss: 0.002069, acc.: 100.00%] [G loss: 0.103771]\n",
      "epoch:19 step:14854 [D loss: 0.001361, acc.: 100.00%] [G loss: 0.481830]\n",
      "epoch:19 step:14855 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.057573]\n",
      "epoch:19 step:14856 [D loss: 0.000137, acc.: 100.00%] [G loss: 1.972918]\n",
      "epoch:19 step:14857 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.724776]\n",
      "epoch:19 step:14858 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.335084]\n",
      "epoch:19 step:14859 [D loss: 0.066124, acc.: 97.66%] [G loss: 0.085851]\n",
      "epoch:19 step:14860 [D loss: 0.003304, acc.: 100.00%] [G loss: 0.222521]\n",
      "epoch:19 step:14861 [D loss: 0.006724, acc.: 100.00%] [G loss: 0.053053]\n",
      "epoch:19 step:14862 [D loss: 0.001072, acc.: 100.00%] [G loss: 1.790445]\n",
      "epoch:19 step:14863 [D loss: 0.002123, acc.: 100.00%] [G loss: 0.025531]\n",
      "epoch:19 step:14864 [D loss: 0.013638, acc.: 99.22%] [G loss: 0.025348]\n",
      "epoch:19 step:14865 [D loss: 0.002189, acc.: 100.00%] [G loss: 1.328561]\n",
      "epoch:19 step:14866 [D loss: 0.017489, acc.: 99.22%] [G loss: 0.020310]\n",
      "epoch:19 step:14867 [D loss: 0.007739, acc.: 100.00%] [G loss: 2.661988]\n",
      "epoch:19 step:14868 [D loss: 0.013734, acc.: 100.00%] [G loss: 0.020363]\n",
      "epoch:19 step:14869 [D loss: 0.020803, acc.: 100.00%] [G loss: 0.005363]\n",
      "epoch:19 step:14870 [D loss: 0.079802, acc.: 97.66%] [G loss: 1.085933]\n",
      "epoch:19 step:14871 [D loss: 0.014496, acc.: 100.00%] [G loss: 2.307244]\n",
      "epoch:19 step:14872 [D loss: 0.008164, acc.: 100.00%] [G loss: 0.212830]\n",
      "epoch:19 step:14873 [D loss: 0.008025, acc.: 99.22%] [G loss: 0.141218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:14874 [D loss: 0.147551, acc.: 93.75%] [G loss: 0.018233]\n",
      "epoch:19 step:14875 [D loss: 0.004685, acc.: 100.00%] [G loss: 0.043029]\n",
      "epoch:19 step:14876 [D loss: 0.001280, acc.: 100.00%] [G loss: 0.059069]\n",
      "epoch:19 step:14877 [D loss: 0.002237, acc.: 100.00%] [G loss: 0.187776]\n",
      "epoch:19 step:14878 [D loss: 0.011746, acc.: 100.00%] [G loss: 0.301867]\n",
      "epoch:19 step:14879 [D loss: 0.002648, acc.: 100.00%] [G loss: 0.016179]\n",
      "epoch:19 step:14880 [D loss: 0.003508, acc.: 100.00%] [G loss: 0.247768]\n",
      "epoch:19 step:14881 [D loss: 0.030042, acc.: 100.00%] [G loss: 1.061281]\n",
      "epoch:19 step:14882 [D loss: 0.001089, acc.: 100.00%] [G loss: 0.108891]\n",
      "epoch:19 step:14883 [D loss: 0.038089, acc.: 100.00%] [G loss: 0.094895]\n",
      "epoch:19 step:14884 [D loss: 0.009625, acc.: 100.00%] [G loss: 1.770055]\n",
      "epoch:19 step:14885 [D loss: 0.003996, acc.: 100.00%] [G loss: 1.321506]\n",
      "epoch:19 step:14886 [D loss: 0.033113, acc.: 100.00%] [G loss: 0.855517]\n",
      "epoch:19 step:14887 [D loss: 0.011827, acc.: 100.00%] [G loss: 0.038280]\n",
      "epoch:19 step:14888 [D loss: 0.002686, acc.: 100.00%] [G loss: 0.608041]\n",
      "epoch:19 step:14889 [D loss: 0.025160, acc.: 99.22%] [G loss: 0.334263]\n",
      "epoch:19 step:14890 [D loss: 0.019653, acc.: 100.00%] [G loss: 0.066452]\n",
      "epoch:19 step:14891 [D loss: 0.031103, acc.: 100.00%] [G loss: 3.555478]\n",
      "epoch:19 step:14892 [D loss: 0.001728, acc.: 100.00%] [G loss: 0.143917]\n",
      "epoch:19 step:14893 [D loss: 0.024788, acc.: 99.22%] [G loss: 0.028314]\n",
      "epoch:19 step:14894 [D loss: 0.156027, acc.: 95.31%] [G loss: 7.226315]\n",
      "epoch:19 step:14895 [D loss: 2.136076, acc.: 48.44%] [G loss: 0.185885]\n",
      "epoch:19 step:14896 [D loss: 0.028076, acc.: 100.00%] [G loss: 3.854675]\n",
      "epoch:19 step:14897 [D loss: 0.077713, acc.: 97.66%] [G loss: 0.071584]\n",
      "epoch:19 step:14898 [D loss: 0.002030, acc.: 100.00%] [G loss: 0.293470]\n",
      "epoch:19 step:14899 [D loss: 0.004300, acc.: 100.00%] [G loss: 0.130342]\n",
      "epoch:19 step:14900 [D loss: 0.010982, acc.: 99.22%] [G loss: 0.237974]\n",
      "epoch:19 step:14901 [D loss: 0.002410, acc.: 100.00%] [G loss: 0.026476]\n",
      "epoch:19 step:14902 [D loss: 0.001986, acc.: 100.00%] [G loss: 6.935497]\n",
      "epoch:19 step:14903 [D loss: 0.001375, acc.: 100.00%] [G loss: 0.056123]\n",
      "epoch:19 step:14904 [D loss: 0.023589, acc.: 99.22%] [G loss: 0.044834]\n",
      "epoch:19 step:14905 [D loss: 0.001395, acc.: 100.00%] [G loss: 0.027148]\n",
      "epoch:19 step:14906 [D loss: 0.001253, acc.: 100.00%] [G loss: 0.001934]\n",
      "epoch:19 step:14907 [D loss: 0.000764, acc.: 100.00%] [G loss: 5.392807]\n",
      "epoch:19 step:14908 [D loss: 0.007575, acc.: 100.00%] [G loss: 0.005722]\n",
      "epoch:19 step:14909 [D loss: 0.006436, acc.: 100.00%] [G loss: 0.004887]\n",
      "epoch:19 step:14910 [D loss: 0.011715, acc.: 100.00%] [G loss: 0.022745]\n",
      "epoch:19 step:14911 [D loss: 0.014013, acc.: 100.00%] [G loss: 5.325014]\n",
      "epoch:19 step:14912 [D loss: 0.000893, acc.: 100.00%] [G loss: 4.838200]\n",
      "epoch:19 step:14913 [D loss: 0.000875, acc.: 100.00%] [G loss: 0.017884]\n",
      "epoch:19 step:14914 [D loss: 0.000656, acc.: 100.00%] [G loss: 3.117905]\n",
      "epoch:19 step:14915 [D loss: 0.002326, acc.: 100.00%] [G loss: 0.292792]\n",
      "epoch:19 step:14916 [D loss: 0.017585, acc.: 100.00%] [G loss: 0.002314]\n",
      "epoch:19 step:14917 [D loss: 0.003800, acc.: 100.00%] [G loss: 1.446629]\n",
      "epoch:19 step:14918 [D loss: 0.010791, acc.: 100.00%] [G loss: 0.004216]\n",
      "epoch:19 step:14919 [D loss: 0.046443, acc.: 99.22%] [G loss: 0.014759]\n",
      "epoch:19 step:14920 [D loss: 0.005442, acc.: 100.00%] [G loss: 0.079106]\n",
      "epoch:19 step:14921 [D loss: 0.003235, acc.: 100.00%] [G loss: 4.057556]\n",
      "epoch:19 step:14922 [D loss: 0.006003, acc.: 100.00%] [G loss: 1.448491]\n",
      "epoch:19 step:14923 [D loss: 0.001960, acc.: 100.00%] [G loss: 0.015641]\n",
      "epoch:19 step:14924 [D loss: 0.003760, acc.: 100.00%] [G loss: 1.159271]\n",
      "epoch:19 step:14925 [D loss: 0.007512, acc.: 100.00%] [G loss: 0.904743]\n",
      "epoch:19 step:14926 [D loss: 0.005203, acc.: 100.00%] [G loss: 0.668889]\n",
      "epoch:19 step:14927 [D loss: 0.004622, acc.: 100.00%] [G loss: 0.013423]\n",
      "epoch:19 step:14928 [D loss: 0.129253, acc.: 95.31%] [G loss: 3.546228]\n",
      "epoch:19 step:14929 [D loss: 0.448232, acc.: 78.91%] [G loss: 0.000230]\n",
      "epoch:19 step:14930 [D loss: 0.025592, acc.: 100.00%] [G loss: 0.004528]\n",
      "epoch:19 step:14931 [D loss: 0.001581, acc.: 100.00%] [G loss: 0.001077]\n",
      "epoch:19 step:14932 [D loss: 0.003408, acc.: 100.00%] [G loss: 0.467052]\n",
      "epoch:19 step:14933 [D loss: 0.008348, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:19 step:14934 [D loss: 0.002723, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:19 step:14935 [D loss: 0.000800, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:19 step:14936 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.701180]\n",
      "epoch:19 step:14937 [D loss: 0.001166, acc.: 100.00%] [G loss: 0.526502]\n",
      "epoch:19 step:14938 [D loss: 0.001084, acc.: 100.00%] [G loss: 0.000286]\n",
      "epoch:19 step:14939 [D loss: 0.001230, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:19 step:14940 [D loss: 0.000539, acc.: 100.00%] [G loss: 0.118281]\n",
      "epoch:19 step:14941 [D loss: 0.001520, acc.: 100.00%] [G loss: 0.005428]\n",
      "epoch:19 step:14942 [D loss: 0.001663, acc.: 100.00%] [G loss: 0.303801]\n",
      "epoch:19 step:14943 [D loss: 0.006456, acc.: 100.00%] [G loss: 0.004698]\n",
      "epoch:19 step:14944 [D loss: 0.002275, acc.: 100.00%] [G loss: 0.000461]\n",
      "epoch:19 step:14945 [D loss: 0.001020, acc.: 100.00%] [G loss: 0.022150]\n",
      "epoch:19 step:14946 [D loss: 0.007283, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:19 step:14947 [D loss: 0.000914, acc.: 100.00%] [G loss: 0.141938]\n",
      "epoch:19 step:14948 [D loss: 0.001699, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:19 step:14949 [D loss: 0.030318, acc.: 100.00%] [G loss: 0.577466]\n",
      "epoch:19 step:14950 [D loss: 0.012606, acc.: 100.00%] [G loss: 1.429133]\n",
      "epoch:19 step:14951 [D loss: 0.002343, acc.: 100.00%] [G loss: 0.774455]\n",
      "epoch:19 step:14952 [D loss: 0.009120, acc.: 100.00%] [G loss: 0.004760]\n",
      "epoch:19 step:14953 [D loss: 0.002083, acc.: 100.00%] [G loss: 0.011133]\n",
      "epoch:19 step:14954 [D loss: 0.002867, acc.: 100.00%] [G loss: 1.051576]\n",
      "epoch:19 step:14955 [D loss: 0.004887, acc.: 100.00%] [G loss: 0.020768]\n",
      "epoch:19 step:14956 [D loss: 0.091374, acc.: 98.44%] [G loss: 0.131262]\n",
      "epoch:19 step:14957 [D loss: 0.001928, acc.: 100.00%] [G loss: 6.196542]\n",
      "epoch:19 step:14958 [D loss: 0.057639, acc.: 98.44%] [G loss: 0.260948]\n",
      "epoch:19 step:14959 [D loss: 0.001021, acc.: 100.00%] [G loss: 4.244971]\n",
      "epoch:19 step:14960 [D loss: 0.004972, acc.: 100.00%] [G loss: 1.033108]\n",
      "epoch:19 step:14961 [D loss: 0.000590, acc.: 100.00%] [G loss: 0.021764]\n",
      "epoch:19 step:14962 [D loss: 0.006093, acc.: 100.00%] [G loss: 0.021907]\n",
      "epoch:19 step:14963 [D loss: 0.000414, acc.: 100.00%] [G loss: 0.037923]\n",
      "epoch:19 step:14964 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.009680]\n",
      "epoch:19 step:14965 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.006839]\n",
      "epoch:19 step:14966 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.031401]\n",
      "epoch:19 step:14967 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.006894]\n",
      "epoch:19 step:14968 [D loss: 0.000654, acc.: 100.00%] [G loss: 3.021792]\n",
      "epoch:19 step:14969 [D loss: 0.004608, acc.: 100.00%] [G loss: 1.419551]\n",
      "epoch:19 step:14970 [D loss: 0.000894, acc.: 100.00%] [G loss: 1.612217]\n",
      "epoch:19 step:14971 [D loss: 0.000613, acc.: 100.00%] [G loss: 0.853673]\n",
      "epoch:19 step:14972 [D loss: 0.001577, acc.: 100.00%] [G loss: 0.327576]\n",
      "epoch:19 step:14973 [D loss: 0.000479, acc.: 100.00%] [G loss: 0.931203]\n",
      "epoch:19 step:14974 [D loss: 0.003519, acc.: 100.00%] [G loss: 0.124611]\n",
      "epoch:19 step:14975 [D loss: 0.004651, acc.: 100.00%] [G loss: 0.190038]\n",
      "epoch:19 step:14976 [D loss: 0.387638, acc.: 83.59%] [G loss: 5.684300]\n",
      "epoch:19 step:14977 [D loss: 0.882273, acc.: 70.31%] [G loss: 0.056922]\n",
      "epoch:19 step:14978 [D loss: 0.003214, acc.: 100.00%] [G loss: 3.664867]\n",
      "epoch:19 step:14979 [D loss: 0.020649, acc.: 99.22%] [G loss: 0.023573]\n",
      "epoch:19 step:14980 [D loss: 0.001052, acc.: 100.00%] [G loss: 0.031950]\n",
      "epoch:19 step:14981 [D loss: 0.001340, acc.: 100.00%] [G loss: 0.079639]\n",
      "epoch:19 step:14982 [D loss: 0.008942, acc.: 99.22%] [G loss: 0.040644]\n",
      "epoch:19 step:14983 [D loss: 0.001115, acc.: 100.00%] [G loss: 0.096763]\n",
      "epoch:19 step:14984 [D loss: 0.000651, acc.: 100.00%] [G loss: 0.003848]\n",
      "epoch:19 step:14985 [D loss: 0.001458, acc.: 100.00%] [G loss: 1.628862]\n",
      "epoch:19 step:14986 [D loss: 0.000786, acc.: 100.00%] [G loss: 0.012633]\n",
      "epoch:19 step:14987 [D loss: 0.169811, acc.: 92.19%] [G loss: 0.510652]\n",
      "epoch:19 step:14988 [D loss: 0.044002, acc.: 99.22%] [G loss: 0.769967]\n",
      "epoch:19 step:14989 [D loss: 0.080145, acc.: 96.09%] [G loss: 0.124403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:14990 [D loss: 0.011772, acc.: 100.00%] [G loss: 4.043280]\n",
      "epoch:19 step:14991 [D loss: 0.001248, acc.: 100.00%] [G loss: 0.057813]\n",
      "epoch:19 step:14992 [D loss: 0.019259, acc.: 100.00%] [G loss: 0.018149]\n",
      "epoch:19 step:14993 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.005314]\n",
      "epoch:19 step:14994 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.003159]\n",
      "epoch:19 step:14995 [D loss: 0.000643, acc.: 100.00%] [G loss: 0.004768]\n",
      "epoch:19 step:14996 [D loss: 0.001946, acc.: 100.00%] [G loss: 0.012704]\n",
      "epoch:19 step:14997 [D loss: 0.016974, acc.: 99.22%] [G loss: 0.000202]\n",
      "epoch:19 step:14998 [D loss: 0.005233, acc.: 100.00%] [G loss: 0.664041]\n",
      "epoch:19 step:14999 [D loss: 0.003128, acc.: 100.00%] [G loss: 0.400146]\n",
      "epoch:19 step:15000 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.008759]\n",
      "epoch:19 step:15001 [D loss: 0.000631, acc.: 100.00%] [G loss: 0.000509]\n",
      "epoch:19 step:15002 [D loss: 0.000458, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:19 step:15003 [D loss: 0.002543, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:19 step:15004 [D loss: 0.017202, acc.: 100.00%] [G loss: 0.047595]\n",
      "epoch:19 step:15005 [D loss: 0.002402, acc.: 100.00%] [G loss: 0.263427]\n",
      "epoch:19 step:15006 [D loss: 0.008574, acc.: 100.00%] [G loss: 0.002578]\n",
      "epoch:19 step:15007 [D loss: 0.008239, acc.: 100.00%] [G loss: 0.140230]\n",
      "epoch:19 step:15008 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.005667]\n",
      "epoch:19 step:15009 [D loss: 0.002972, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:19 step:15010 [D loss: 0.004503, acc.: 100.00%] [G loss: 0.001113]\n",
      "epoch:19 step:15011 [D loss: 0.010243, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:19 step:15012 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.001613]\n",
      "epoch:19 step:15013 [D loss: 0.001304, acc.: 100.00%] [G loss: 0.000605]\n",
      "epoch:19 step:15014 [D loss: 0.001446, acc.: 100.00%] [G loss: 0.611937]\n",
      "epoch:19 step:15015 [D loss: 0.003424, acc.: 100.00%] [G loss: 0.209252]\n",
      "epoch:19 step:15016 [D loss: 0.000821, acc.: 100.00%] [G loss: 0.059517]\n",
      "epoch:19 step:15017 [D loss: 0.013324, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:19 step:15018 [D loss: 0.002670, acc.: 100.00%] [G loss: 0.002243]\n",
      "epoch:19 step:15019 [D loss: 0.002863, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:19 step:15020 [D loss: 0.000516, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:19 step:15021 [D loss: 0.000888, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:19 step:15022 [D loss: 0.001841, acc.: 100.00%] [G loss: 0.015745]\n",
      "epoch:19 step:15023 [D loss: 0.003996, acc.: 100.00%] [G loss: 0.006244]\n",
      "epoch:19 step:15024 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.000440]\n",
      "epoch:19 step:15025 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.020347]\n",
      "epoch:19 step:15026 [D loss: 0.281841, acc.: 85.94%] [G loss: 1.768203]\n",
      "epoch:19 step:15027 [D loss: 1.108890, acc.: 61.72%] [G loss: 0.046034]\n",
      "epoch:19 step:15028 [D loss: 1.214650, acc.: 55.47%] [G loss: 7.014901]\n",
      "epoch:19 step:15029 [D loss: 1.462089, acc.: 54.69%] [G loss: 0.498508]\n",
      "epoch:19 step:15030 [D loss: 0.056497, acc.: 99.22%] [G loss: 2.677126]\n",
      "epoch:19 step:15031 [D loss: 0.017719, acc.: 100.00%] [G loss: 0.008697]\n",
      "epoch:19 step:15032 [D loss: 0.020863, acc.: 100.00%] [G loss: 0.639317]\n",
      "epoch:19 step:15033 [D loss: 0.017167, acc.: 100.00%] [G loss: 0.982579]\n",
      "epoch:19 step:15034 [D loss: 0.129777, acc.: 95.31%] [G loss: 0.227201]\n",
      "epoch:19 step:15035 [D loss: 0.001611, acc.: 100.00%] [G loss: 0.228396]\n",
      "epoch:19 step:15036 [D loss: 0.252241, acc.: 88.28%] [G loss: 0.002230]\n",
      "epoch:19 step:15037 [D loss: 0.042429, acc.: 99.22%] [G loss: 0.008842]\n",
      "epoch:19 step:15038 [D loss: 0.007893, acc.: 100.00%] [G loss: 0.002374]\n",
      "epoch:19 step:15039 [D loss: 0.012937, acc.: 100.00%] [G loss: 0.002137]\n",
      "epoch:19 step:15040 [D loss: 0.000957, acc.: 100.00%] [G loss: 0.007501]\n",
      "epoch:19 step:15041 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.922932]\n",
      "epoch:19 step:15042 [D loss: 0.012703, acc.: 100.00%] [G loss: 0.067223]\n",
      "epoch:19 step:15043 [D loss: 0.006584, acc.: 100.00%] [G loss: 0.002515]\n",
      "epoch:19 step:15044 [D loss: 0.003456, acc.: 100.00%] [G loss: 0.009648]\n",
      "epoch:19 step:15045 [D loss: 0.004158, acc.: 100.00%] [G loss: 0.035844]\n",
      "epoch:19 step:15046 [D loss: 0.010855, acc.: 100.00%] [G loss: 0.009543]\n",
      "epoch:19 step:15047 [D loss: 0.006803, acc.: 100.00%] [G loss: 0.617180]\n",
      "epoch:19 step:15048 [D loss: 0.005860, acc.: 100.00%] [G loss: 0.002520]\n",
      "epoch:19 step:15049 [D loss: 0.004577, acc.: 100.00%] [G loss: 0.020090]\n",
      "epoch:19 step:15050 [D loss: 0.002185, acc.: 100.00%] [G loss: 0.033531]\n",
      "epoch:19 step:15051 [D loss: 0.002008, acc.: 100.00%] [G loss: 0.219702]\n",
      "epoch:19 step:15052 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.004117]\n",
      "epoch:19 step:15053 [D loss: 0.007897, acc.: 100.00%] [G loss: 0.017879]\n",
      "epoch:19 step:15054 [D loss: 0.004960, acc.: 100.00%] [G loss: 0.004385]\n",
      "epoch:19 step:15055 [D loss: 0.003528, acc.: 100.00%] [G loss: 0.020752]\n",
      "epoch:19 step:15056 [D loss: 0.005992, acc.: 100.00%] [G loss: 0.087282]\n",
      "epoch:19 step:15057 [D loss: 0.119350, acc.: 95.31%] [G loss: 1.504328]\n",
      "epoch:19 step:15058 [D loss: 0.014403, acc.: 100.00%] [G loss: 1.320909]\n",
      "epoch:19 step:15059 [D loss: 0.111790, acc.: 94.53%] [G loss: 1.550583]\n",
      "epoch:19 step:15060 [D loss: 0.032836, acc.: 99.22%] [G loss: 0.028976]\n",
      "epoch:19 step:15061 [D loss: 0.016270, acc.: 100.00%] [G loss: 0.107903]\n",
      "epoch:19 step:15062 [D loss: 0.025206, acc.: 99.22%] [G loss: 0.783046]\n",
      "epoch:19 step:15063 [D loss: 0.009517, acc.: 100.00%] [G loss: 2.049366]\n",
      "epoch:19 step:15064 [D loss: 0.061141, acc.: 97.66%] [G loss: 1.159699]\n",
      "epoch:19 step:15065 [D loss: 0.019551, acc.: 100.00%] [G loss: 1.457972]\n",
      "epoch:19 step:15066 [D loss: 0.011553, acc.: 100.00%] [G loss: 3.244664]\n",
      "epoch:19 step:15067 [D loss: 0.069424, acc.: 98.44%] [G loss: 0.107731]\n",
      "epoch:19 step:15068 [D loss: 0.229148, acc.: 93.75%] [G loss: 2.754117]\n",
      "epoch:19 step:15069 [D loss: 0.010768, acc.: 100.00%] [G loss: 0.952035]\n",
      "epoch:19 step:15070 [D loss: 0.008650, acc.: 100.00%] [G loss: 1.039446]\n",
      "epoch:19 step:15071 [D loss: 0.172755, acc.: 93.75%] [G loss: 1.471132]\n",
      "epoch:19 step:15072 [D loss: 0.008102, acc.: 100.00%] [G loss: 0.033191]\n",
      "epoch:19 step:15073 [D loss: 0.001662, acc.: 100.00%] [G loss: 0.066335]\n",
      "epoch:19 step:15074 [D loss: 0.001226, acc.: 100.00%] [G loss: 0.062283]\n",
      "epoch:19 step:15075 [D loss: 0.014061, acc.: 100.00%] [G loss: 0.302014]\n",
      "epoch:19 step:15076 [D loss: 0.001088, acc.: 100.00%] [G loss: 0.055096]\n",
      "epoch:19 step:15077 [D loss: 0.002053, acc.: 100.00%] [G loss: 0.074816]\n",
      "epoch:19 step:15078 [D loss: 0.003070, acc.: 100.00%] [G loss: 0.028544]\n",
      "epoch:19 step:15079 [D loss: 0.028096, acc.: 99.22%] [G loss: 0.003583]\n",
      "epoch:19 step:15080 [D loss: 0.005974, acc.: 100.00%] [G loss: 0.013485]\n",
      "epoch:19 step:15081 [D loss: 0.006251, acc.: 100.00%] [G loss: 0.074666]\n",
      "epoch:19 step:15082 [D loss: 0.042754, acc.: 99.22%] [G loss: 0.136807]\n",
      "epoch:19 step:15083 [D loss: 0.003809, acc.: 100.00%] [G loss: 1.730400]\n",
      "epoch:19 step:15084 [D loss: 0.003119, acc.: 100.00%] [G loss: 1.709097]\n",
      "epoch:19 step:15085 [D loss: 0.013868, acc.: 100.00%] [G loss: 0.913660]\n",
      "epoch:19 step:15086 [D loss: 0.048664, acc.: 97.66%] [G loss: 0.128151]\n",
      "epoch:19 step:15087 [D loss: 0.000734, acc.: 100.00%] [G loss: 4.603799]\n",
      "epoch:19 step:15088 [D loss: 0.004720, acc.: 100.00%] [G loss: 0.161762]\n",
      "epoch:19 step:15089 [D loss: 0.002621, acc.: 100.00%] [G loss: 0.158421]\n",
      "epoch:19 step:15090 [D loss: 0.002707, acc.: 100.00%] [G loss: 0.503925]\n",
      "epoch:19 step:15091 [D loss: 0.000683, acc.: 100.00%] [G loss: 0.014280]\n",
      "epoch:19 step:15092 [D loss: 0.005034, acc.: 100.00%] [G loss: 0.043060]\n",
      "epoch:19 step:15093 [D loss: 0.002226, acc.: 100.00%] [G loss: 0.007622]\n",
      "epoch:19 step:15094 [D loss: 0.002850, acc.: 100.00%] [G loss: 0.038108]\n",
      "epoch:19 step:15095 [D loss: 0.003952, acc.: 100.00%] [G loss: 0.013551]\n",
      "epoch:19 step:15096 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.036542]\n",
      "epoch:19 step:15097 [D loss: 0.001437, acc.: 100.00%] [G loss: 0.074251]\n",
      "epoch:19 step:15098 [D loss: 0.004932, acc.: 100.00%] [G loss: 0.035814]\n",
      "epoch:19 step:15099 [D loss: 0.004904, acc.: 100.00%] [G loss: 0.015546]\n",
      "epoch:19 step:15100 [D loss: 0.002115, acc.: 100.00%] [G loss: 0.023801]\n",
      "epoch:19 step:15101 [D loss: 0.004724, acc.: 100.00%] [G loss: 0.090701]\n",
      "epoch:19 step:15102 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.009899]\n",
      "epoch:19 step:15103 [D loss: 0.000707, acc.: 100.00%] [G loss: 0.022889]\n",
      "epoch:19 step:15104 [D loss: 0.002830, acc.: 100.00%] [G loss: 0.030384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15105 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.010354]\n",
      "epoch:19 step:15106 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.064376]\n",
      "epoch:19 step:15107 [D loss: 0.000528, acc.: 100.00%] [G loss: 0.051366]\n",
      "epoch:19 step:15108 [D loss: 0.000528, acc.: 100.00%] [G loss: 0.017143]\n",
      "epoch:19 step:15109 [D loss: 0.001014, acc.: 100.00%] [G loss: 0.004615]\n",
      "epoch:19 step:15110 [D loss: 0.001564, acc.: 100.00%] [G loss: 0.009333]\n",
      "epoch:19 step:15111 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.011645]\n",
      "epoch:19 step:15112 [D loss: 0.003151, acc.: 100.00%] [G loss: 0.020323]\n",
      "epoch:19 step:15113 [D loss: 0.014279, acc.: 100.00%] [G loss: 2.070308]\n",
      "epoch:19 step:15114 [D loss: 0.005911, acc.: 100.00%] [G loss: 0.011111]\n",
      "epoch:19 step:15115 [D loss: 0.060179, acc.: 98.44%] [G loss: 1.001971]\n",
      "epoch:19 step:15116 [D loss: 0.122389, acc.: 96.09%] [G loss: 0.300219]\n",
      "epoch:19 step:15117 [D loss: 0.270153, acc.: 89.84%] [G loss: 1.066714]\n",
      "epoch:19 step:15118 [D loss: 0.149764, acc.: 92.97%] [G loss: 2.049068]\n",
      "epoch:19 step:15119 [D loss: 0.094711, acc.: 94.53%] [G loss: 2.052896]\n",
      "epoch:19 step:15120 [D loss: 0.004686, acc.: 100.00%] [G loss: 0.053717]\n",
      "epoch:19 step:15121 [D loss: 0.014543, acc.: 99.22%] [G loss: 0.006548]\n",
      "epoch:19 step:15122 [D loss: 0.012285, acc.: 99.22%] [G loss: 0.032164]\n",
      "epoch:19 step:15123 [D loss: 0.001361, acc.: 100.00%] [G loss: 0.006104]\n",
      "epoch:19 step:15124 [D loss: 0.002837, acc.: 100.00%] [G loss: 0.012146]\n",
      "epoch:19 step:15125 [D loss: 0.019699, acc.: 100.00%] [G loss: 0.029366]\n",
      "epoch:19 step:15126 [D loss: 0.028298, acc.: 100.00%] [G loss: 0.103450]\n",
      "epoch:19 step:15127 [D loss: 0.004534, acc.: 100.00%] [G loss: 0.147464]\n",
      "epoch:19 step:15128 [D loss: 0.010876, acc.: 100.00%] [G loss: 2.296014]\n",
      "epoch:19 step:15129 [D loss: 0.006125, acc.: 100.00%] [G loss: 0.009832]\n",
      "epoch:19 step:15130 [D loss: 0.001017, acc.: 100.00%] [G loss: 0.003839]\n",
      "epoch:19 step:15131 [D loss: 0.004896, acc.: 100.00%] [G loss: 0.034990]\n",
      "epoch:19 step:15132 [D loss: 0.007080, acc.: 100.00%] [G loss: 0.006187]\n",
      "epoch:19 step:15133 [D loss: 0.001075, acc.: 100.00%] [G loss: 0.237518]\n",
      "epoch:19 step:15134 [D loss: 0.127667, acc.: 96.09%] [G loss: 0.003588]\n",
      "epoch:19 step:15135 [D loss: 0.054811, acc.: 99.22%] [G loss: 0.085734]\n",
      "epoch:19 step:15136 [D loss: 0.033895, acc.: 100.00%] [G loss: 0.002452]\n",
      "epoch:19 step:15137 [D loss: 0.001579, acc.: 100.00%] [G loss: 0.132002]\n",
      "epoch:19 step:15138 [D loss: 0.002544, acc.: 100.00%] [G loss: 3.029168]\n",
      "epoch:19 step:15139 [D loss: 0.298151, acc.: 83.59%] [G loss: 0.000001]\n",
      "epoch:19 step:15140 [D loss: 0.008844, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:19 step:15141 [D loss: 0.248942, acc.: 85.94%] [G loss: 2.662954]\n",
      "epoch:19 step:15142 [D loss: 0.000789, acc.: 100.00%] [G loss: 0.303697]\n",
      "epoch:19 step:15143 [D loss: 0.317881, acc.: 86.72%] [G loss: 0.000066]\n",
      "epoch:19 step:15144 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:19 step:15145 [D loss: 0.000173, acc.: 100.00%] [G loss: 1.129142]\n",
      "epoch:19 step:15146 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.718390]\n",
      "epoch:19 step:15147 [D loss: 0.002228, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:19 step:15148 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.154081]\n",
      "epoch:19 step:15149 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.239145]\n",
      "epoch:19 step:15150 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:19 step:15151 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:19 step:15152 [D loss: 0.000758, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:19 step:15153 [D loss: 0.000765, acc.: 100.00%] [G loss: 0.139322]\n",
      "epoch:19 step:15154 [D loss: 0.025692, acc.: 100.00%] [G loss: 0.020976]\n",
      "epoch:19 step:15155 [D loss: 0.001271, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:19 step:15156 [D loss: 0.000512, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:19 step:15157 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:19 step:15158 [D loss: 0.003512, acc.: 100.00%] [G loss: 0.017829]\n",
      "epoch:19 step:15159 [D loss: 0.348105, acc.: 78.91%] [G loss: 3.168482]\n",
      "epoch:19 step:15160 [D loss: 0.020163, acc.: 99.22%] [G loss: 4.620823]\n",
      "epoch:19 step:15161 [D loss: 0.073579, acc.: 97.66%] [G loss: 0.013209]\n",
      "epoch:19 step:15162 [D loss: 0.007505, acc.: 100.00%] [G loss: 1.216570]\n",
      "epoch:19 step:15163 [D loss: 0.006577, acc.: 100.00%] [G loss: 0.021268]\n",
      "epoch:19 step:15164 [D loss: 0.003507, acc.: 100.00%] [G loss: 0.036686]\n",
      "epoch:19 step:15165 [D loss: 0.004994, acc.: 100.00%] [G loss: 0.313395]\n",
      "epoch:19 step:15166 [D loss: 0.004357, acc.: 100.00%] [G loss: 0.111645]\n",
      "epoch:19 step:15167 [D loss: 0.003789, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:19 step:15168 [D loss: 0.000651, acc.: 100.00%] [G loss: 0.479597]\n",
      "epoch:19 step:15169 [D loss: 0.001354, acc.: 100.00%] [G loss: 1.439270]\n",
      "epoch:19 step:15170 [D loss: 0.019761, acc.: 99.22%] [G loss: 0.026557]\n",
      "epoch:19 step:15171 [D loss: 0.046972, acc.: 100.00%] [G loss: 1.306933]\n",
      "epoch:19 step:15172 [D loss: 0.003654, acc.: 100.00%] [G loss: 1.769764]\n",
      "epoch:19 step:15173 [D loss: 0.015375, acc.: 100.00%] [G loss: 0.023270]\n",
      "epoch:19 step:15174 [D loss: 0.019282, acc.: 99.22%] [G loss: 0.000499]\n",
      "epoch:19 step:15175 [D loss: 0.004527, acc.: 100.00%] [G loss: 1.288400]\n",
      "epoch:19 step:15176 [D loss: 0.002120, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:19 step:15177 [D loss: 0.272094, acc.: 89.84%] [G loss: 0.009867]\n",
      "epoch:19 step:15178 [D loss: 0.000044, acc.: 100.00%] [G loss: 5.623102]\n",
      "epoch:19 step:15179 [D loss: 0.604111, acc.: 71.88%] [G loss: 0.004661]\n",
      "epoch:19 step:15180 [D loss: 0.091130, acc.: 96.09%] [G loss: 0.000359]\n",
      "epoch:19 step:15181 [D loss: 0.032369, acc.: 99.22%] [G loss: 0.019207]\n",
      "epoch:19 step:15182 [D loss: 0.036378, acc.: 100.00%] [G loss: 0.077515]\n",
      "epoch:19 step:15183 [D loss: 0.015501, acc.: 100.00%] [G loss: 0.054421]\n",
      "epoch:19 step:15184 [D loss: 0.002587, acc.: 100.00%] [G loss: 0.130537]\n",
      "epoch:19 step:15185 [D loss: 0.002366, acc.: 100.00%] [G loss: 0.119171]\n",
      "epoch:19 step:15186 [D loss: 0.015792, acc.: 100.00%] [G loss: 3.777941]\n",
      "epoch:19 step:15187 [D loss: 0.020327, acc.: 100.00%] [G loss: 0.062233]\n",
      "epoch:19 step:15188 [D loss: 0.113928, acc.: 96.09%] [G loss: 3.416192]\n",
      "epoch:19 step:15189 [D loss: 0.124392, acc.: 95.31%] [G loss: 3.318314]\n",
      "epoch:19 step:15190 [D loss: 0.059163, acc.: 97.66%] [G loss: 4.796309]\n",
      "epoch:19 step:15191 [D loss: 0.003113, acc.: 100.00%] [G loss: 4.405289]\n",
      "epoch:19 step:15192 [D loss: 0.027887, acc.: 100.00%] [G loss: 2.616919]\n",
      "epoch:19 step:15193 [D loss: 0.052920, acc.: 99.22%] [G loss: 2.772957]\n",
      "epoch:19 step:15194 [D loss: 0.004850, acc.: 100.00%] [G loss: 4.303108]\n",
      "epoch:19 step:15195 [D loss: 0.030539, acc.: 99.22%] [G loss: 2.376337]\n",
      "epoch:19 step:15196 [D loss: 0.005171, acc.: 100.00%] [G loss: 2.337268]\n",
      "epoch:19 step:15197 [D loss: 0.001692, acc.: 100.00%] [G loss: 2.072613]\n",
      "epoch:19 step:15198 [D loss: 0.001912, acc.: 100.00%] [G loss: 1.794549]\n",
      "epoch:19 step:15199 [D loss: 0.003975, acc.: 100.00%] [G loss: 0.273225]\n",
      "epoch:19 step:15200 [D loss: 0.013466, acc.: 99.22%] [G loss: 0.136511]\n",
      "epoch:19 step:15201 [D loss: 0.008762, acc.: 100.00%] [G loss: 1.092304]\n",
      "epoch:19 step:15202 [D loss: 0.001892, acc.: 100.00%] [G loss: 0.286340]\n",
      "epoch:19 step:15203 [D loss: 0.121762, acc.: 97.66%] [G loss: 1.186390]\n",
      "epoch:19 step:15204 [D loss: 0.014419, acc.: 100.00%] [G loss: 2.004303]\n",
      "epoch:19 step:15205 [D loss: 0.833499, acc.: 64.06%] [G loss: 0.213550]\n",
      "epoch:19 step:15206 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.521155]\n",
      "epoch:19 step:15207 [D loss: 0.003079, acc.: 100.00%] [G loss: 0.978682]\n",
      "epoch:19 step:15208 [D loss: 0.002875, acc.: 100.00%] [G loss: 0.921480]\n",
      "epoch:19 step:15209 [D loss: 0.049871, acc.: 97.66%] [G loss: 1.137435]\n",
      "epoch:19 step:15210 [D loss: 0.001087, acc.: 100.00%] [G loss: 5.706703]\n",
      "epoch:19 step:15211 [D loss: 0.000557, acc.: 100.00%] [G loss: 1.147004]\n",
      "epoch:19 step:15212 [D loss: 0.001091, acc.: 100.00%] [G loss: 6.563956]\n",
      "epoch:19 step:15213 [D loss: 0.052653, acc.: 99.22%] [G loss: 4.296165]\n",
      "epoch:19 step:15214 [D loss: 0.234968, acc.: 91.41%] [G loss: 3.485423]\n",
      "epoch:19 step:15215 [D loss: 0.103009, acc.: 94.53%] [G loss: 6.349955]\n",
      "epoch:19 step:15216 [D loss: 0.013141, acc.: 100.00%] [G loss: 2.668712]\n",
      "epoch:19 step:15217 [D loss: 0.008462, acc.: 100.00%] [G loss: 1.088353]\n",
      "epoch:19 step:15218 [D loss: 0.000608, acc.: 100.00%] [G loss: 1.068921]\n",
      "epoch:19 step:15219 [D loss: 0.001037, acc.: 100.00%] [G loss: 0.336853]\n",
      "epoch:19 step:15220 [D loss: 0.010289, acc.: 100.00%] [G loss: 0.171971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15221 [D loss: 0.006290, acc.: 100.00%] [G loss: 0.248524]\n",
      "epoch:19 step:15222 [D loss: 0.009738, acc.: 100.00%] [G loss: 0.194985]\n",
      "epoch:19 step:15223 [D loss: 0.008475, acc.: 100.00%] [G loss: 2.808335]\n",
      "epoch:19 step:15224 [D loss: 0.000383, acc.: 100.00%] [G loss: 3.082957]\n",
      "epoch:19 step:15225 [D loss: 0.002558, acc.: 100.00%] [G loss: 1.806511]\n",
      "epoch:19 step:15226 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.955981]\n",
      "epoch:19 step:15227 [D loss: 0.012651, acc.: 100.00%] [G loss: 0.357388]\n",
      "epoch:19 step:15228 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.787637]\n",
      "epoch:19 step:15229 [D loss: 0.022606, acc.: 100.00%] [G loss: 0.039596]\n",
      "epoch:19 step:15230 [D loss: 0.001039, acc.: 100.00%] [G loss: 0.083869]\n",
      "epoch:19 step:15231 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.090650]\n",
      "epoch:19 step:15232 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.041776]\n",
      "epoch:19 step:15233 [D loss: 0.002680, acc.: 100.00%] [G loss: 0.035645]\n",
      "epoch:19 step:15234 [D loss: 0.078756, acc.: 98.44%] [G loss: 0.001942]\n",
      "epoch:19 step:15235 [D loss: 0.007572, acc.: 100.00%] [G loss: 0.000610]\n",
      "epoch:19 step:15236 [D loss: 0.026208, acc.: 100.00%] [G loss: 0.008934]\n",
      "epoch:19 step:15237 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.002968]\n",
      "epoch:19 step:15238 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.018050]\n",
      "epoch:19 step:15239 [D loss: 0.001256, acc.: 100.00%] [G loss: 0.161014]\n",
      "epoch:19 step:15240 [D loss: 0.006754, acc.: 100.00%] [G loss: 0.060825]\n",
      "epoch:19 step:15241 [D loss: 0.001147, acc.: 100.00%] [G loss: 0.231826]\n",
      "epoch:19 step:15242 [D loss: 0.027020, acc.: 100.00%] [G loss: 0.001661]\n",
      "epoch:19 step:15243 [D loss: 0.001451, acc.: 100.00%] [G loss: 1.077574]\n",
      "epoch:19 step:15244 [D loss: 0.006998, acc.: 100.00%] [G loss: 0.005082]\n",
      "epoch:19 step:15245 [D loss: 0.001040, acc.: 100.00%] [G loss: 0.020498]\n",
      "epoch:19 step:15246 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.010893]\n",
      "epoch:19 step:15247 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.018987]\n",
      "epoch:19 step:15248 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.111790]\n",
      "epoch:19 step:15249 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.006192]\n",
      "epoch:19 step:15250 [D loss: 0.017222, acc.: 99.22%] [G loss: 0.023058]\n",
      "epoch:19 step:15251 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.002056]\n",
      "epoch:19 step:15252 [D loss: 0.000780, acc.: 100.00%] [G loss: 0.017232]\n",
      "epoch:19 step:15253 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.004101]\n",
      "epoch:19 step:15254 [D loss: 0.000494, acc.: 100.00%] [G loss: 0.021047]\n",
      "epoch:19 step:15255 [D loss: 0.001636, acc.: 100.00%] [G loss: 0.001192]\n",
      "epoch:19 step:15256 [D loss: 0.000725, acc.: 100.00%] [G loss: 1.023127]\n",
      "epoch:19 step:15257 [D loss: 0.007363, acc.: 100.00%] [G loss: 0.038746]\n",
      "epoch:19 step:15258 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.057445]\n",
      "epoch:19 step:15259 [D loss: 0.002643, acc.: 100.00%] [G loss: 0.017527]\n",
      "epoch:19 step:15260 [D loss: 0.028400, acc.: 98.44%] [G loss: 0.027002]\n",
      "epoch:19 step:15261 [D loss: 0.001560, acc.: 100.00%] [G loss: 0.062869]\n",
      "epoch:19 step:15262 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.011200]\n",
      "epoch:19 step:15263 [D loss: 0.003872, acc.: 100.00%] [G loss: 0.588968]\n",
      "epoch:19 step:15264 [D loss: 0.003293, acc.: 100.00%] [G loss: 0.870810]\n",
      "epoch:19 step:15265 [D loss: 0.000635, acc.: 100.00%] [G loss: 0.395712]\n",
      "epoch:19 step:15266 [D loss: 0.083444, acc.: 97.66%] [G loss: 0.234491]\n",
      "epoch:19 step:15267 [D loss: 1.698187, acc.: 34.38%] [G loss: 1.913816]\n",
      "epoch:19 step:15268 [D loss: 1.799355, acc.: 53.12%] [G loss: 2.818452]\n",
      "epoch:19 step:15269 [D loss: 0.736749, acc.: 72.66%] [G loss: 0.695719]\n",
      "epoch:19 step:15270 [D loss: 0.071431, acc.: 96.88%] [G loss: 1.468062]\n",
      "epoch:19 step:15271 [D loss: 0.441946, acc.: 83.59%] [G loss: 0.176700]\n",
      "epoch:19 step:15272 [D loss: 0.126887, acc.: 96.09%] [G loss: 0.364392]\n",
      "epoch:19 step:15273 [D loss: 0.044497, acc.: 100.00%] [G loss: 4.506829]\n",
      "epoch:19 step:15274 [D loss: 0.082716, acc.: 96.88%] [G loss: 0.618641]\n",
      "epoch:19 step:15275 [D loss: 0.096492, acc.: 96.88%] [G loss: 0.860674]\n",
      "epoch:19 step:15276 [D loss: 0.019213, acc.: 100.00%] [G loss: 0.816374]\n",
      "epoch:19 step:15277 [D loss: 0.054375, acc.: 99.22%] [G loss: 1.270643]\n",
      "epoch:19 step:15278 [D loss: 0.030584, acc.: 100.00%] [G loss: 0.920043]\n",
      "epoch:19 step:15279 [D loss: 0.065902, acc.: 98.44%] [G loss: 0.617335]\n",
      "epoch:19 step:15280 [D loss: 0.180695, acc.: 95.31%] [G loss: 4.742948]\n",
      "epoch:19 step:15281 [D loss: 0.090566, acc.: 96.88%] [G loss: 6.387257]\n",
      "epoch:19 step:15282 [D loss: 0.070229, acc.: 96.88%] [G loss: 0.891567]\n",
      "epoch:19 step:15283 [D loss: 0.027003, acc.: 100.00%] [G loss: 4.751111]\n",
      "epoch:19 step:15284 [D loss: 0.025407, acc.: 100.00%] [G loss: 0.264898]\n",
      "epoch:19 step:15285 [D loss: 0.334539, acc.: 82.81%] [G loss: 4.224925]\n",
      "epoch:19 step:15286 [D loss: 0.247830, acc.: 89.06%] [G loss: 4.945605]\n",
      "epoch:19 step:15287 [D loss: 0.167314, acc.: 90.62%] [G loss: 1.212560]\n",
      "epoch:19 step:15288 [D loss: 0.007865, acc.: 100.00%] [G loss: 2.405695]\n",
      "epoch:19 step:15289 [D loss: 0.004233, acc.: 100.00%] [G loss: 0.071814]\n",
      "epoch:19 step:15290 [D loss: 0.056792, acc.: 98.44%] [G loss: 0.216058]\n",
      "epoch:19 step:15291 [D loss: 0.003241, acc.: 100.00%] [G loss: 0.396309]\n",
      "epoch:19 step:15292 [D loss: 0.005558, acc.: 100.00%] [G loss: 0.217355]\n",
      "epoch:19 step:15293 [D loss: 0.001457, acc.: 100.00%] [G loss: 0.271079]\n",
      "epoch:19 step:15294 [D loss: 0.003048, acc.: 100.00%] [G loss: 0.275508]\n",
      "epoch:19 step:15295 [D loss: 0.002407, acc.: 100.00%] [G loss: 0.042029]\n",
      "epoch:19 step:15296 [D loss: 0.010613, acc.: 99.22%] [G loss: 0.029133]\n",
      "epoch:19 step:15297 [D loss: 0.001522, acc.: 100.00%] [G loss: 0.025936]\n",
      "epoch:19 step:15298 [D loss: 0.001430, acc.: 100.00%] [G loss: 0.014390]\n",
      "epoch:19 step:15299 [D loss: 0.003871, acc.: 100.00%] [G loss: 0.039093]\n",
      "epoch:19 step:15300 [D loss: 0.001655, acc.: 100.00%] [G loss: 0.760852]\n",
      "epoch:19 step:15301 [D loss: 0.005632, acc.: 100.00%] [G loss: 0.003827]\n",
      "epoch:19 step:15302 [D loss: 0.010351, acc.: 100.00%] [G loss: 0.003773]\n",
      "epoch:19 step:15303 [D loss: 0.008382, acc.: 100.00%] [G loss: 0.068036]\n",
      "epoch:19 step:15304 [D loss: 0.001532, acc.: 100.00%] [G loss: 0.007777]\n",
      "epoch:19 step:15305 [D loss: 0.000597, acc.: 100.00%] [G loss: 0.679640]\n",
      "epoch:19 step:15306 [D loss: 0.004699, acc.: 100.00%] [G loss: 0.010649]\n",
      "epoch:19 step:15307 [D loss: 0.001451, acc.: 100.00%] [G loss: 0.005055]\n",
      "epoch:19 step:15308 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.009074]\n",
      "epoch:19 step:15309 [D loss: 0.002299, acc.: 100.00%] [G loss: 0.092863]\n",
      "epoch:19 step:15310 [D loss: 0.000991, acc.: 100.00%] [G loss: 0.039308]\n",
      "epoch:19 step:15311 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.109946]\n",
      "epoch:19 step:15312 [D loss: 0.001334, acc.: 100.00%] [G loss: 0.020955]\n",
      "epoch:19 step:15313 [D loss: 0.002224, acc.: 100.00%] [G loss: 0.015063]\n",
      "epoch:19 step:15314 [D loss: 0.002592, acc.: 100.00%] [G loss: 0.001601]\n",
      "epoch:19 step:15315 [D loss: 0.020378, acc.: 100.00%] [G loss: 0.005623]\n",
      "epoch:19 step:15316 [D loss: 0.000795, acc.: 100.00%] [G loss: 0.014280]\n",
      "epoch:19 step:15317 [D loss: 0.001121, acc.: 100.00%] [G loss: 0.154279]\n",
      "epoch:19 step:15318 [D loss: 0.003073, acc.: 100.00%] [G loss: 0.034358]\n",
      "epoch:19 step:15319 [D loss: 0.018912, acc.: 100.00%] [G loss: 0.008873]\n",
      "epoch:19 step:15320 [D loss: 0.008096, acc.: 100.00%] [G loss: 0.004703]\n",
      "epoch:19 step:15321 [D loss: 0.027874, acc.: 100.00%] [G loss: 0.104242]\n",
      "epoch:19 step:15322 [D loss: 0.003615, acc.: 100.00%] [G loss: 0.028421]\n",
      "epoch:19 step:15323 [D loss: 0.014492, acc.: 100.00%] [G loss: 0.005831]\n",
      "epoch:19 step:15324 [D loss: 0.002056, acc.: 100.00%] [G loss: 0.001835]\n",
      "epoch:19 step:15325 [D loss: 0.003221, acc.: 100.00%] [G loss: 0.001749]\n",
      "epoch:19 step:15326 [D loss: 0.004345, acc.: 100.00%] [G loss: 0.100259]\n",
      "epoch:19 step:15327 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.022635]\n",
      "epoch:19 step:15328 [D loss: 0.004591, acc.: 100.00%] [G loss: 0.002925]\n",
      "epoch:19 step:15329 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.183019]\n",
      "epoch:19 step:15330 [D loss: 0.002100, acc.: 100.00%] [G loss: 0.000780]\n",
      "epoch:19 step:15331 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.001745]\n",
      "epoch:19 step:15332 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.003826]\n",
      "epoch:19 step:15333 [D loss: 0.000912, acc.: 100.00%] [G loss: 0.000870]\n",
      "epoch:19 step:15334 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000418]\n",
      "epoch:19 step:15335 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.008901]\n",
      "epoch:19 step:15336 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.001689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15337 [D loss: 0.007304, acc.: 100.00%] [G loss: 0.001558]\n",
      "epoch:19 step:15338 [D loss: 0.002384, acc.: 100.00%] [G loss: 0.001377]\n",
      "epoch:19 step:15339 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.002037]\n",
      "epoch:19 step:15340 [D loss: 0.001877, acc.: 100.00%] [G loss: 0.002540]\n",
      "epoch:19 step:15341 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.001927]\n",
      "epoch:19 step:15342 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.015858]\n",
      "epoch:19 step:15343 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.001787]\n",
      "epoch:19 step:15344 [D loss: 0.002653, acc.: 100.00%] [G loss: 0.004821]\n",
      "epoch:19 step:15345 [D loss: 0.001981, acc.: 100.00%] [G loss: 0.000983]\n",
      "epoch:19 step:15346 [D loss: 0.002367, acc.: 100.00%] [G loss: 0.000753]\n",
      "epoch:19 step:15347 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.000376]\n",
      "epoch:19 step:15348 [D loss: 0.002453, acc.: 100.00%] [G loss: 0.000486]\n",
      "epoch:19 step:15349 [D loss: 0.000769, acc.: 100.00%] [G loss: 0.003499]\n",
      "epoch:19 step:15350 [D loss: 0.012755, acc.: 100.00%] [G loss: 0.001186]\n",
      "epoch:19 step:15351 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.001575]\n",
      "epoch:19 step:15352 [D loss: 0.018443, acc.: 99.22%] [G loss: 0.001568]\n",
      "epoch:19 step:15353 [D loss: 0.000987, acc.: 100.00%] [G loss: 0.002094]\n",
      "epoch:19 step:15354 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:19 step:15355 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:19 step:15356 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.000866]\n",
      "epoch:19 step:15357 [D loss: 0.000526, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:19 step:15358 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:19 step:15359 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000483]\n",
      "epoch:19 step:15360 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:19 step:15361 [D loss: 0.000533, acc.: 100.00%] [G loss: 0.001557]\n",
      "epoch:19 step:15362 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:19 step:15363 [D loss: 0.005937, acc.: 100.00%] [G loss: 0.000744]\n",
      "epoch:19 step:15364 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:19 step:15365 [D loss: 0.002839, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:19 step:15366 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:19 step:15367 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000924]\n",
      "epoch:19 step:15368 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.001122]\n",
      "epoch:19 step:15369 [D loss: 0.000427, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:19 step:15370 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.180638]\n",
      "epoch:19 step:15371 [D loss: 0.001428, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:19 step:15372 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:19 step:15373 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:19 step:15374 [D loss: 0.002410, acc.: 100.00%] [G loss: 0.000827]\n",
      "epoch:19 step:15375 [D loss: 0.008682, acc.: 100.00%] [G loss: 0.098330]\n",
      "epoch:19 step:15376 [D loss: 0.006415, acc.: 100.00%] [G loss: 0.000748]\n",
      "epoch:19 step:15377 [D loss: 0.000787, acc.: 100.00%] [G loss: 0.000581]\n",
      "epoch:19 step:15378 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:19 step:15379 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.008811]\n",
      "epoch:19 step:15380 [D loss: 0.435279, acc.: 81.25%] [G loss: 8.428288]\n",
      "epoch:19 step:15381 [D loss: 1.564632, acc.: 58.59%] [G loss: 4.417325]\n",
      "epoch:19 step:15382 [D loss: 0.056394, acc.: 97.66%] [G loss: 3.945567]\n",
      "epoch:19 step:15383 [D loss: 0.003279, acc.: 100.00%] [G loss: 3.019548]\n",
      "epoch:19 step:15384 [D loss: 0.023088, acc.: 100.00%] [G loss: 2.630224]\n",
      "epoch:19 step:15385 [D loss: 0.058120, acc.: 97.66%] [G loss: 1.483843]\n",
      "epoch:19 step:15386 [D loss: 0.001786, acc.: 100.00%] [G loss: 2.933980]\n",
      "epoch:19 step:15387 [D loss: 0.003692, acc.: 100.00%] [G loss: 1.720301]\n",
      "epoch:19 step:15388 [D loss: 0.004044, acc.: 100.00%] [G loss: 1.111443]\n",
      "epoch:19 step:15389 [D loss: 0.019043, acc.: 99.22%] [G loss: 1.017361]\n",
      "epoch:19 step:15390 [D loss: 0.053841, acc.: 99.22%] [G loss: 1.052627]\n",
      "epoch:19 step:15391 [D loss: 0.028340, acc.: 100.00%] [G loss: 0.895311]\n",
      "epoch:19 step:15392 [D loss: 0.003243, acc.: 100.00%] [G loss: 1.017966]\n",
      "epoch:19 step:15393 [D loss: 0.085627, acc.: 98.44%] [G loss: 0.915684]\n",
      "epoch:19 step:15394 [D loss: 0.009510, acc.: 100.00%] [G loss: 0.240373]\n",
      "epoch:19 step:15395 [D loss: 0.009824, acc.: 100.00%] [G loss: 0.269109]\n",
      "epoch:19 step:15396 [D loss: 0.008763, acc.: 100.00%] [G loss: 0.096347]\n",
      "epoch:19 step:15397 [D loss: 0.029129, acc.: 100.00%] [G loss: 0.617262]\n",
      "epoch:19 step:15398 [D loss: 0.046207, acc.: 99.22%] [G loss: 0.052319]\n",
      "epoch:19 step:15399 [D loss: 0.002698, acc.: 100.00%] [G loss: 0.158901]\n",
      "epoch:19 step:15400 [D loss: 0.011305, acc.: 100.00%] [G loss: 0.879623]\n",
      "epoch:19 step:15401 [D loss: 0.012980, acc.: 100.00%] [G loss: 0.140946]\n",
      "epoch:19 step:15402 [D loss: 0.009197, acc.: 100.00%] [G loss: 0.156467]\n",
      "epoch:19 step:15403 [D loss: 0.000992, acc.: 100.00%] [G loss: 0.059074]\n",
      "epoch:19 step:15404 [D loss: 0.004425, acc.: 100.00%] [G loss: 0.018308]\n",
      "epoch:19 step:15405 [D loss: 0.028474, acc.: 99.22%] [G loss: 0.013323]\n",
      "epoch:19 step:15406 [D loss: 0.005613, acc.: 100.00%] [G loss: 0.063410]\n",
      "epoch:19 step:15407 [D loss: 0.003177, acc.: 100.00%] [G loss: 0.044030]\n",
      "epoch:19 step:15408 [D loss: 0.001302, acc.: 100.00%] [G loss: 0.007352]\n",
      "epoch:19 step:15409 [D loss: 0.000719, acc.: 100.00%] [G loss: 0.045232]\n",
      "epoch:19 step:15410 [D loss: 0.003092, acc.: 100.00%] [G loss: 0.004643]\n",
      "epoch:19 step:15411 [D loss: 0.002134, acc.: 100.00%] [G loss: 0.023664]\n",
      "epoch:19 step:15412 [D loss: 0.005220, acc.: 100.00%] [G loss: 0.006245]\n",
      "epoch:19 step:15413 [D loss: 0.000721, acc.: 100.00%] [G loss: 0.006507]\n",
      "epoch:19 step:15414 [D loss: 0.002397, acc.: 100.00%] [G loss: 0.008701]\n",
      "epoch:19 step:15415 [D loss: 0.000994, acc.: 100.00%] [G loss: 0.024465]\n",
      "epoch:19 step:15416 [D loss: 0.002967, acc.: 100.00%] [G loss: 0.007457]\n",
      "epoch:19 step:15417 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.003118]\n",
      "epoch:19 step:15418 [D loss: 0.001021, acc.: 100.00%] [G loss: 0.044098]\n",
      "epoch:19 step:15419 [D loss: 0.002035, acc.: 100.00%] [G loss: 0.004792]\n",
      "epoch:19 step:15420 [D loss: 0.001429, acc.: 100.00%] [G loss: 0.003012]\n",
      "epoch:19 step:15421 [D loss: 0.000859, acc.: 100.00%] [G loss: 0.022780]\n",
      "epoch:19 step:15422 [D loss: 0.000585, acc.: 100.00%] [G loss: 0.006623]\n",
      "epoch:19 step:15423 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.001471]\n",
      "epoch:19 step:15424 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.078159]\n",
      "epoch:19 step:15425 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.011559]\n",
      "epoch:19 step:15426 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.002411]\n",
      "epoch:19 step:15427 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.008251]\n",
      "epoch:19 step:15428 [D loss: 0.000763, acc.: 100.00%] [G loss: 0.020800]\n",
      "epoch:19 step:15429 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.021570]\n",
      "epoch:19 step:15430 [D loss: 0.003392, acc.: 100.00%] [G loss: 0.006488]\n",
      "epoch:19 step:15431 [D loss: 0.010718, acc.: 100.00%] [G loss: 0.021172]\n",
      "epoch:19 step:15432 [D loss: 0.005149, acc.: 100.00%] [G loss: 0.007078]\n",
      "epoch:19 step:15433 [D loss: 0.004049, acc.: 100.00%] [G loss: 0.014232]\n",
      "epoch:19 step:15434 [D loss: 0.005901, acc.: 100.00%] [G loss: 0.015275]\n",
      "epoch:19 step:15435 [D loss: 0.001479, acc.: 100.00%] [G loss: 0.004276]\n",
      "epoch:19 step:15436 [D loss: 0.009835, acc.: 100.00%] [G loss: 0.003075]\n",
      "epoch:19 step:15437 [D loss: 0.005062, acc.: 100.00%] [G loss: 0.081894]\n",
      "epoch:19 step:15438 [D loss: 0.005013, acc.: 100.00%] [G loss: 0.151278]\n",
      "epoch:19 step:15439 [D loss: 0.006787, acc.: 100.00%] [G loss: 0.111081]\n",
      "epoch:19 step:15440 [D loss: 0.014223, acc.: 100.00%] [G loss: 0.042326]\n",
      "epoch:19 step:15441 [D loss: 0.001329, acc.: 100.00%] [G loss: 0.028570]\n",
      "epoch:19 step:15442 [D loss: 0.001923, acc.: 100.00%] [G loss: 0.032809]\n",
      "epoch:19 step:15443 [D loss: 0.001848, acc.: 100.00%] [G loss: 0.042485]\n",
      "epoch:19 step:15444 [D loss: 0.001049, acc.: 100.00%] [G loss: 0.016191]\n",
      "epoch:19 step:15445 [D loss: 0.001522, acc.: 100.00%] [G loss: 0.013063]\n",
      "epoch:19 step:15446 [D loss: 0.002897, acc.: 100.00%] [G loss: 0.019559]\n",
      "epoch:19 step:15447 [D loss: 0.000639, acc.: 100.00%] [G loss: 0.016644]\n",
      "epoch:19 step:15448 [D loss: 0.000498, acc.: 100.00%] [G loss: 0.020955]\n",
      "epoch:19 step:15449 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.017220]\n",
      "epoch:19 step:15450 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.003025]\n",
      "epoch:19 step:15451 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.005417]\n",
      "epoch:19 step:15452 [D loss: 0.015205, acc.: 100.00%] [G loss: 0.007647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15453 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.002646]\n",
      "epoch:19 step:15454 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.005369]\n",
      "epoch:19 step:15455 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.001925]\n",
      "epoch:19 step:15456 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.005188]\n",
      "epoch:19 step:15457 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000868]\n",
      "epoch:19 step:15458 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.002261]\n",
      "epoch:19 step:15459 [D loss: 0.000544, acc.: 100.00%] [G loss: 0.004532]\n",
      "epoch:19 step:15460 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.001385]\n",
      "epoch:19 step:15461 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.004268]\n",
      "epoch:19 step:15462 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.010671]\n",
      "epoch:19 step:15463 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000904]\n",
      "epoch:19 step:15464 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.002466]\n",
      "epoch:19 step:15465 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.001304]\n",
      "epoch:19 step:15466 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.001609]\n",
      "epoch:19 step:15467 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.005688]\n",
      "epoch:19 step:15468 [D loss: 0.001707, acc.: 100.00%] [G loss: 0.006674]\n",
      "epoch:19 step:15469 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.000435]\n",
      "epoch:19 step:15470 [D loss: 0.002232, acc.: 100.00%] [G loss: 0.005208]\n",
      "epoch:19 step:15471 [D loss: 0.000652, acc.: 100.00%] [G loss: 0.009368]\n",
      "epoch:19 step:15472 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.040773]\n",
      "epoch:19 step:15473 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.000402]\n",
      "epoch:19 step:15474 [D loss: 0.000736, acc.: 100.00%] [G loss: 0.001730]\n",
      "epoch:19 step:15475 [D loss: 0.000607, acc.: 100.00%] [G loss: 0.004255]\n",
      "epoch:19 step:15476 [D loss: 0.003725, acc.: 100.00%] [G loss: 0.005467]\n",
      "epoch:19 step:15477 [D loss: 0.000742, acc.: 100.00%] [G loss: 0.001475]\n",
      "epoch:19 step:15478 [D loss: 0.002674, acc.: 100.00%] [G loss: 0.004571]\n",
      "epoch:19 step:15479 [D loss: 0.000468, acc.: 100.00%] [G loss: 0.000865]\n",
      "epoch:19 step:15480 [D loss: 0.015053, acc.: 100.00%] [G loss: 0.008993]\n",
      "epoch:19 step:15481 [D loss: 0.554251, acc.: 75.00%] [G loss: 8.857681]\n",
      "epoch:19 step:15482 [D loss: 1.087141, acc.: 60.16%] [G loss: 6.440277]\n",
      "epoch:19 step:15483 [D loss: 0.008033, acc.: 100.00%] [G loss: 3.949757]\n",
      "epoch:19 step:15484 [D loss: 0.245989, acc.: 91.41%] [G loss: 6.146608]\n",
      "epoch:19 step:15485 [D loss: 0.100307, acc.: 96.88%] [G loss: 5.971529]\n",
      "epoch:19 step:15486 [D loss: 0.117839, acc.: 96.09%] [G loss: 5.481321]\n",
      "epoch:19 step:15487 [D loss: 0.018901, acc.: 100.00%] [G loss: 0.032220]\n",
      "epoch:19 step:15488 [D loss: 0.009733, acc.: 100.00%] [G loss: 2.484807]\n",
      "epoch:19 step:15489 [D loss: 0.000956, acc.: 100.00%] [G loss: 0.197455]\n",
      "epoch:19 step:15490 [D loss: 0.045776, acc.: 99.22%] [G loss: 3.671993]\n",
      "epoch:19 step:15491 [D loss: 0.003531, acc.: 100.00%] [G loss: 2.429756]\n",
      "epoch:19 step:15492 [D loss: 0.145862, acc.: 96.09%] [G loss: 4.239456]\n",
      "epoch:19 step:15493 [D loss: 0.079778, acc.: 96.88%] [G loss: 3.608404]\n",
      "epoch:19 step:15494 [D loss: 0.007745, acc.: 100.00%] [G loss: 0.839422]\n",
      "epoch:19 step:15495 [D loss: 0.004371, acc.: 100.00%] [G loss: 0.743940]\n",
      "epoch:19 step:15496 [D loss: 0.004624, acc.: 100.00%] [G loss: 1.839299]\n",
      "epoch:19 step:15497 [D loss: 0.010687, acc.: 100.00%] [G loss: 1.176053]\n",
      "epoch:19 step:15498 [D loss: 0.008403, acc.: 100.00%] [G loss: 0.014784]\n",
      "epoch:19 step:15499 [D loss: 0.054803, acc.: 99.22%] [G loss: 1.623320]\n",
      "epoch:19 step:15500 [D loss: 0.177846, acc.: 93.75%] [G loss: 1.446234]\n",
      "epoch:19 step:15501 [D loss: 0.019591, acc.: 100.00%] [G loss: 0.324881]\n",
      "epoch:19 step:15502 [D loss: 0.009276, acc.: 100.00%] [G loss: 0.571523]\n",
      "epoch:19 step:15503 [D loss: 0.003679, acc.: 100.00%] [G loss: 0.022723]\n",
      "epoch:19 step:15504 [D loss: 0.291394, acc.: 86.72%] [G loss: 0.000008]\n",
      "epoch:19 step:15505 [D loss: 1.569135, acc.: 53.91%] [G loss: 5.227763]\n",
      "epoch:19 step:15506 [D loss: 0.916983, acc.: 60.94%] [G loss: 6.283241]\n",
      "epoch:19 step:15507 [D loss: 0.381326, acc.: 82.81%] [G loss: 0.669705]\n",
      "epoch:19 step:15508 [D loss: 0.010183, acc.: 100.00%] [G loss: 0.485347]\n",
      "epoch:19 step:15509 [D loss: 0.024405, acc.: 100.00%] [G loss: 0.189087]\n",
      "epoch:19 step:15510 [D loss: 0.003887, acc.: 100.00%] [G loss: 0.244594]\n",
      "epoch:19 step:15511 [D loss: 0.001038, acc.: 100.00%] [G loss: 1.550873]\n",
      "epoch:19 step:15512 [D loss: 0.011530, acc.: 100.00%] [G loss: 0.084354]\n",
      "epoch:19 step:15513 [D loss: 0.005616, acc.: 100.00%] [G loss: 0.058718]\n",
      "epoch:19 step:15514 [D loss: 0.044413, acc.: 99.22%] [G loss: 0.200154]\n",
      "epoch:19 step:15515 [D loss: 0.016013, acc.: 100.00%] [G loss: 0.322067]\n",
      "epoch:19 step:15516 [D loss: 0.030942, acc.: 100.00%] [G loss: 2.453585]\n",
      "epoch:19 step:15517 [D loss: 0.096809, acc.: 97.66%] [G loss: 3.315286]\n",
      "epoch:19 step:15518 [D loss: 0.061269, acc.: 97.66%] [G loss: 1.007856]\n",
      "epoch:19 step:15519 [D loss: 0.164854, acc.: 91.41%] [G loss: 0.450017]\n",
      "epoch:19 step:15520 [D loss: 0.003572, acc.: 100.00%] [G loss: 0.156524]\n",
      "epoch:19 step:15521 [D loss: 0.008435, acc.: 100.00%] [G loss: 0.114338]\n",
      "epoch:19 step:15522 [D loss: 0.003487, acc.: 100.00%] [G loss: 0.021663]\n",
      "epoch:19 step:15523 [D loss: 0.003407, acc.: 100.00%] [G loss: 0.003856]\n",
      "epoch:19 step:15524 [D loss: 0.006380, acc.: 100.00%] [G loss: 0.013042]\n",
      "epoch:19 step:15525 [D loss: 0.008900, acc.: 100.00%] [G loss: 0.015416]\n",
      "epoch:19 step:15526 [D loss: 0.017896, acc.: 100.00%] [G loss: 0.035330]\n",
      "epoch:19 step:15527 [D loss: 0.004095, acc.: 100.00%] [G loss: 0.029875]\n",
      "epoch:19 step:15528 [D loss: 0.007240, acc.: 100.00%] [G loss: 0.063422]\n",
      "epoch:19 step:15529 [D loss: 0.001218, acc.: 100.00%] [G loss: 0.027364]\n",
      "epoch:19 step:15530 [D loss: 0.002574, acc.: 100.00%] [G loss: 0.105541]\n",
      "epoch:19 step:15531 [D loss: 0.018157, acc.: 100.00%] [G loss: 0.071738]\n",
      "epoch:19 step:15532 [D loss: 0.007881, acc.: 100.00%] [G loss: 0.101398]\n",
      "epoch:19 step:15533 [D loss: 0.001347, acc.: 100.00%] [G loss: 0.034014]\n",
      "epoch:19 step:15534 [D loss: 0.002909, acc.: 100.00%] [G loss: 0.077462]\n",
      "epoch:19 step:15535 [D loss: 0.004877, acc.: 100.00%] [G loss: 0.026105]\n",
      "epoch:19 step:15536 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.293116]\n",
      "epoch:19 step:15537 [D loss: 0.008625, acc.: 100.00%] [G loss: 0.014333]\n",
      "epoch:19 step:15538 [D loss: 0.000595, acc.: 100.00%] [G loss: 0.037808]\n",
      "epoch:19 step:15539 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.009595]\n",
      "epoch:19 step:15540 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.135086]\n",
      "epoch:19 step:15541 [D loss: 0.001073, acc.: 100.00%] [G loss: 0.011879]\n",
      "epoch:19 step:15542 [D loss: 0.000992, acc.: 100.00%] [G loss: 0.033961]\n",
      "epoch:19 step:15543 [D loss: 0.024627, acc.: 99.22%] [G loss: 0.005877]\n",
      "epoch:19 step:15544 [D loss: 0.001547, acc.: 100.00%] [G loss: 0.033636]\n",
      "epoch:19 step:15545 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.052203]\n",
      "epoch:19 step:15546 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.001280]\n",
      "epoch:19 step:15547 [D loss: 0.000996, acc.: 100.00%] [G loss: 0.005356]\n",
      "epoch:19 step:15548 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.048075]\n",
      "epoch:19 step:15549 [D loss: 0.001029, acc.: 100.00%] [G loss: 0.003389]\n",
      "epoch:19 step:15550 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.002634]\n",
      "epoch:19 step:15551 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.001570]\n",
      "epoch:19 step:15552 [D loss: 0.028697, acc.: 99.22%] [G loss: 0.003756]\n",
      "epoch:19 step:15553 [D loss: 0.000440, acc.: 100.00%] [G loss: 0.000918]\n",
      "epoch:19 step:15554 [D loss: 0.006245, acc.: 100.00%] [G loss: 0.001974]\n",
      "epoch:19 step:15555 [D loss: 0.004018, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:19 step:15556 [D loss: 0.003098, acc.: 100.00%] [G loss: 0.001963]\n",
      "epoch:19 step:15557 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.002093]\n",
      "epoch:19 step:15558 [D loss: 0.001070, acc.: 100.00%] [G loss: 0.000465]\n",
      "epoch:19 step:15559 [D loss: 0.023537, acc.: 100.00%] [G loss: 0.003856]\n",
      "epoch:19 step:15560 [D loss: 0.071010, acc.: 97.66%] [G loss: 0.029500]\n",
      "epoch:19 step:15561 [D loss: 0.009711, acc.: 100.00%] [G loss: 1.565796]\n",
      "epoch:19 step:15562 [D loss: 0.001101, acc.: 100.00%] [G loss: 0.770303]\n",
      "epoch:19 step:15563 [D loss: 0.003533, acc.: 100.00%] [G loss: 0.020339]\n",
      "epoch:19 step:15564 [D loss: 0.018213, acc.: 99.22%] [G loss: 0.420075]\n",
      "epoch:19 step:15565 [D loss: 0.002236, acc.: 100.00%] [G loss: 0.019177]\n",
      "epoch:19 step:15566 [D loss: 0.009527, acc.: 100.00%] [G loss: 0.001523]\n",
      "epoch:19 step:15567 [D loss: 0.002515, acc.: 100.00%] [G loss: 0.021403]\n",
      "epoch:19 step:15568 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.001126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19 step:15569 [D loss: 0.001032, acc.: 100.00%] [G loss: 0.008241]\n",
      "epoch:19 step:15570 [D loss: 0.003847, acc.: 100.00%] [G loss: 0.007356]\n",
      "epoch:19 step:15571 [D loss: 0.006282, acc.: 100.00%] [G loss: 0.025278]\n",
      "epoch:19 step:15572 [D loss: 0.181589, acc.: 94.53%] [G loss: 2.472265]\n",
      "epoch:19 step:15573 [D loss: 0.238414, acc.: 90.62%] [G loss: 0.471932]\n",
      "epoch:19 step:15574 [D loss: 0.000764, acc.: 100.00%] [G loss: 0.050707]\n",
      "epoch:19 step:15575 [D loss: 0.001240, acc.: 100.00%] [G loss: 0.149885]\n",
      "epoch:19 step:15576 [D loss: 0.145419, acc.: 94.53%] [G loss: 0.331233]\n",
      "epoch:19 step:15577 [D loss: 0.013506, acc.: 100.00%] [G loss: 3.880014]\n",
      "epoch:19 step:15578 [D loss: 0.509690, acc.: 78.12%] [G loss: 0.111695]\n",
      "epoch:19 step:15579 [D loss: 0.705458, acc.: 75.00%] [G loss: 1.909867]\n",
      "epoch:19 step:15580 [D loss: 0.485780, acc.: 79.69%] [G loss: 1.830760]\n",
      "epoch:19 step:15581 [D loss: 0.178833, acc.: 91.41%] [G loss: 2.083550]\n",
      "epoch:19 step:15582 [D loss: 0.091770, acc.: 99.22%] [G loss: 0.162478]\n",
      "epoch:19 step:15583 [D loss: 0.062341, acc.: 98.44%] [G loss: 5.010147]\n",
      "epoch:19 step:15584 [D loss: 0.033495, acc.: 100.00%] [G loss: 2.365923]\n",
      "epoch:19 step:15585 [D loss: 0.057055, acc.: 99.22%] [G loss: 0.279020]\n",
      "epoch:19 step:15586 [D loss: 0.299619, acc.: 85.16%] [G loss: 5.932286]\n",
      "epoch:19 step:15587 [D loss: 0.326088, acc.: 82.81%] [G loss: 2.559286]\n",
      "epoch:19 step:15588 [D loss: 0.105329, acc.: 96.09%] [G loss: 2.220730]\n",
      "epoch:19 step:15589 [D loss: 0.006909, acc.: 100.00%] [G loss: 0.650650]\n",
      "epoch:19 step:15590 [D loss: 0.015049, acc.: 99.22%] [G loss: 0.411027]\n",
      "epoch:19 step:15591 [D loss: 0.266074, acc.: 86.72%] [G loss: 1.652255]\n",
      "epoch:19 step:15592 [D loss: 0.241744, acc.: 86.72%] [G loss: 0.819041]\n",
      "epoch:19 step:15593 [D loss: 0.069311, acc.: 99.22%] [G loss: 0.231400]\n",
      "epoch:19 step:15594 [D loss: 0.370846, acc.: 82.81%] [G loss: 7.194394]\n",
      "epoch:19 step:15595 [D loss: 0.099406, acc.: 95.31%] [G loss: 4.568880]\n",
      "epoch:19 step:15596 [D loss: 0.362566, acc.: 82.03%] [G loss: 1.253597]\n",
      "epoch:19 step:15597 [D loss: 0.026715, acc.: 100.00%] [G loss: 0.435625]\n",
      "epoch:19 step:15598 [D loss: 0.070714, acc.: 98.44%] [G loss: 1.409121]\n",
      "epoch:19 step:15599 [D loss: 0.005940, acc.: 100.00%] [G loss: 2.743677]\n",
      "epoch:19 step:15600 [D loss: 0.001417, acc.: 100.00%] [G loss: 2.612675]\n",
      "epoch:19 step:15601 [D loss: 0.016970, acc.: 100.00%] [G loss: 1.868015]\n",
      "epoch:19 step:15602 [D loss: 0.004331, acc.: 100.00%] [G loss: 0.516298]\n",
      "epoch:19 step:15603 [D loss: 0.002380, acc.: 100.00%] [G loss: 0.429582]\n",
      "epoch:19 step:15604 [D loss: 0.002636, acc.: 100.00%] [G loss: 0.833653]\n",
      "epoch:19 step:15605 [D loss: 0.004976, acc.: 100.00%] [G loss: 0.698288]\n",
      "epoch:19 step:15606 [D loss: 0.002403, acc.: 100.00%] [G loss: 0.447090]\n",
      "epoch:19 step:15607 [D loss: 0.003962, acc.: 100.00%] [G loss: 0.258990]\n",
      "epoch:19 step:15608 [D loss: 0.003805, acc.: 100.00%] [G loss: 0.378276]\n",
      "epoch:19 step:15609 [D loss: 0.002631, acc.: 100.00%] [G loss: 0.009943]\n",
      "epoch:19 step:15610 [D loss: 0.064684, acc.: 97.66%] [G loss: 0.079162]\n",
      "epoch:19 step:15611 [D loss: 0.003298, acc.: 100.00%] [G loss: 0.003912]\n",
      "epoch:19 step:15612 [D loss: 0.006087, acc.: 100.00%] [G loss: 0.044741]\n",
      "epoch:19 step:15613 [D loss: 0.001177, acc.: 100.00%] [G loss: 0.088883]\n",
      "epoch:19 step:15614 [D loss: 0.004372, acc.: 100.00%] [G loss: 0.059584]\n",
      "epoch:19 step:15615 [D loss: 0.005456, acc.: 100.00%] [G loss: 0.039195]\n",
      "epoch:19 step:15616 [D loss: 0.008297, acc.: 100.00%] [G loss: 0.025842]\n",
      "epoch:19 step:15617 [D loss: 0.004353, acc.: 100.00%] [G loss: 0.046383]\n",
      "epoch:19 step:15618 [D loss: 0.003827, acc.: 100.00%] [G loss: 1.347953]\n",
      "epoch:19 step:15619 [D loss: 0.034921, acc.: 99.22%] [G loss: 0.014906]\n",
      "epoch:19 step:15620 [D loss: 0.011826, acc.: 100.00%] [G loss: 0.008560]\n",
      "epoch:20 step:15621 [D loss: 0.004429, acc.: 100.00%] [G loss: 0.022559]\n",
      "epoch:20 step:15622 [D loss: 0.001907, acc.: 100.00%] [G loss: 0.037071]\n",
      "epoch:20 step:15623 [D loss: 0.005857, acc.: 100.00%] [G loss: 0.034748]\n",
      "epoch:20 step:15624 [D loss: 0.000567, acc.: 100.00%] [G loss: 0.024017]\n",
      "epoch:20 step:15625 [D loss: 0.000690, acc.: 100.00%] [G loss: 0.013240]\n",
      "epoch:20 step:15626 [D loss: 0.000745, acc.: 100.00%] [G loss: 0.010839]\n",
      "epoch:20 step:15627 [D loss: 0.001036, acc.: 100.00%] [G loss: 0.545094]\n",
      "epoch:20 step:15628 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.019968]\n",
      "epoch:20 step:15629 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.012685]\n",
      "epoch:20 step:15630 [D loss: 0.002021, acc.: 100.00%] [G loss: 0.003140]\n",
      "epoch:20 step:15631 [D loss: 0.000817, acc.: 100.00%] [G loss: 0.007680]\n",
      "epoch:20 step:15632 [D loss: 0.000440, acc.: 100.00%] [G loss: 0.014724]\n",
      "epoch:20 step:15633 [D loss: 0.000885, acc.: 100.00%] [G loss: 0.008703]\n",
      "epoch:20 step:15634 [D loss: 0.001026, acc.: 100.00%] [G loss: 0.011326]\n",
      "epoch:20 step:15635 [D loss: 0.013954, acc.: 100.00%] [G loss: 0.011858]\n",
      "epoch:20 step:15636 [D loss: 0.020639, acc.: 100.00%] [G loss: 0.019449]\n",
      "epoch:20 step:15637 [D loss: 0.011181, acc.: 100.00%] [G loss: 0.021352]\n",
      "epoch:20 step:15638 [D loss: 0.026515, acc.: 99.22%] [G loss: 0.098035]\n",
      "epoch:20 step:15639 [D loss: 0.006117, acc.: 100.00%] [G loss: 0.113090]\n",
      "epoch:20 step:15640 [D loss: 0.012728, acc.: 100.00%] [G loss: 0.364522]\n",
      "epoch:20 step:15641 [D loss: 0.009011, acc.: 100.00%] [G loss: 0.205065]\n",
      "epoch:20 step:15642 [D loss: 0.130819, acc.: 93.75%] [G loss: 7.548787]\n",
      "epoch:20 step:15643 [D loss: 0.072731, acc.: 98.44%] [G loss: 0.440664]\n",
      "epoch:20 step:15644 [D loss: 0.027175, acc.: 99.22%] [G loss: 6.455872]\n",
      "epoch:20 step:15645 [D loss: 0.012317, acc.: 100.00%] [G loss: 0.020421]\n",
      "epoch:20 step:15646 [D loss: 0.003393, acc.: 100.00%] [G loss: 0.051287]\n",
      "epoch:20 step:15647 [D loss: 0.000670, acc.: 100.00%] [G loss: 5.288350]\n",
      "epoch:20 step:15648 [D loss: 0.000544, acc.: 100.00%] [G loss: 4.807683]\n",
      "epoch:20 step:15649 [D loss: 0.008840, acc.: 100.00%] [G loss: 0.088631]\n",
      "epoch:20 step:15650 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.023079]\n",
      "epoch:20 step:15651 [D loss: 0.001456, acc.: 100.00%] [G loss: 2.970915]\n",
      "epoch:20 step:15652 [D loss: 0.000285, acc.: 100.00%] [G loss: 2.734868]\n",
      "epoch:20 step:15653 [D loss: 0.000978, acc.: 100.00%] [G loss: 2.680413]\n",
      "epoch:20 step:15654 [D loss: 0.000425, acc.: 100.00%] [G loss: 1.553313]\n",
      "epoch:20 step:15655 [D loss: 0.003290, acc.: 100.00%] [G loss: 0.003712]\n",
      "epoch:20 step:15656 [D loss: 0.005337, acc.: 100.00%] [G loss: 1.517939]\n",
      "epoch:20 step:15657 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.001103]\n",
      "epoch:20 step:15658 [D loss: 0.001322, acc.: 100.00%] [G loss: 0.562883]\n",
      "epoch:20 step:15659 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.001082]\n",
      "epoch:20 step:15660 [D loss: 0.002185, acc.: 100.00%] [G loss: 0.355478]\n",
      "epoch:20 step:15661 [D loss: 0.001499, acc.: 100.00%] [G loss: 0.139546]\n",
      "epoch:20 step:15662 [D loss: 0.009551, acc.: 100.00%] [G loss: 0.308019]\n",
      "epoch:20 step:15663 [D loss: 0.001630, acc.: 100.00%] [G loss: 0.256043]\n",
      "epoch:20 step:15664 [D loss: 0.005703, acc.: 100.00%] [G loss: 0.055609]\n",
      "epoch:20 step:15665 [D loss: 0.003199, acc.: 100.00%] [G loss: 0.060170]\n",
      "epoch:20 step:15666 [D loss: 0.007558, acc.: 100.00%] [G loss: 0.032958]\n",
      "epoch:20 step:15667 [D loss: 0.003652, acc.: 100.00%] [G loss: 0.011584]\n",
      "epoch:20 step:15668 [D loss: 0.021597, acc.: 99.22%] [G loss: 0.137860]\n",
      "epoch:20 step:15669 [D loss: 0.000908, acc.: 100.00%] [G loss: 0.491296]\n",
      "epoch:20 step:15670 [D loss: 0.004240, acc.: 100.00%] [G loss: 0.092067]\n",
      "epoch:20 step:15671 [D loss: 0.004829, acc.: 100.00%] [G loss: 0.217799]\n",
      "epoch:20 step:15672 [D loss: 0.030469, acc.: 100.00%] [G loss: 0.150546]\n",
      "epoch:20 step:15673 [D loss: 0.001750, acc.: 100.00%] [G loss: 0.698649]\n",
      "epoch:20 step:15674 [D loss: 0.013753, acc.: 100.00%] [G loss: 0.292783]\n",
      "epoch:20 step:15675 [D loss: 0.005728, acc.: 100.00%] [G loss: 0.148498]\n",
      "epoch:20 step:15676 [D loss: 0.457222, acc.: 76.56%] [G loss: 3.444580]\n",
      "epoch:20 step:15677 [D loss: 0.308578, acc.: 83.59%] [G loss: 1.660883]\n",
      "epoch:20 step:15678 [D loss: 0.001140, acc.: 100.00%] [G loss: 1.301517]\n",
      "epoch:20 step:15679 [D loss: 0.001416, acc.: 100.00%] [G loss: 0.615959]\n",
      "epoch:20 step:15680 [D loss: 0.020159, acc.: 98.44%] [G loss: 1.309779]\n",
      "epoch:20 step:15681 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.005811]\n",
      "epoch:20 step:15682 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.009109]\n",
      "epoch:20 step:15683 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.088048]\n",
      "epoch:20 step:15684 [D loss: 0.012943, acc.: 100.00%] [G loss: 0.007493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15685 [D loss: 0.027983, acc.: 99.22%] [G loss: 0.023892]\n",
      "epoch:20 step:15686 [D loss: 0.001099, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:20 step:15687 [D loss: 0.000641, acc.: 100.00%] [G loss: 0.017698]\n",
      "epoch:20 step:15688 [D loss: 0.002031, acc.: 100.00%] [G loss: 0.037566]\n",
      "epoch:20 step:15689 [D loss: 0.001020, acc.: 100.00%] [G loss: 0.012393]\n",
      "epoch:20 step:15690 [D loss: 0.001402, acc.: 100.00%] [G loss: 0.036516]\n",
      "epoch:20 step:15691 [D loss: 0.000848, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:20 step:15692 [D loss: 0.001059, acc.: 100.00%] [G loss: 0.026811]\n",
      "epoch:20 step:15693 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:20 step:15694 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.013989]\n",
      "epoch:20 step:15695 [D loss: 0.017572, acc.: 99.22%] [G loss: 0.017703]\n",
      "epoch:20 step:15696 [D loss: 0.013407, acc.: 100.00%] [G loss: 0.062172]\n",
      "epoch:20 step:15697 [D loss: 0.004062, acc.: 100.00%] [G loss: 0.043702]\n",
      "epoch:20 step:15698 [D loss: 0.001366, acc.: 100.00%] [G loss: 0.026918]\n",
      "epoch:20 step:15699 [D loss: 0.010417, acc.: 100.00%] [G loss: 0.028705]\n",
      "epoch:20 step:15700 [D loss: 0.009263, acc.: 100.00%] [G loss: 0.122719]\n",
      "epoch:20 step:15701 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.017033]\n",
      "epoch:20 step:15702 [D loss: 0.014392, acc.: 100.00%] [G loss: 0.012062]\n",
      "epoch:20 step:15703 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.003163]\n",
      "epoch:20 step:15704 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.010812]\n",
      "epoch:20 step:15705 [D loss: 0.000700, acc.: 100.00%] [G loss: 0.059654]\n",
      "epoch:20 step:15706 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.016665]\n",
      "epoch:20 step:15707 [D loss: 0.000086, acc.: 100.00%] [G loss: 2.180048]\n",
      "epoch:20 step:15708 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.081923]\n",
      "epoch:20 step:15709 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.012882]\n",
      "epoch:20 step:15710 [D loss: 0.069052, acc.: 97.66%] [G loss: 0.000703]\n",
      "epoch:20 step:15711 [D loss: 0.001030, acc.: 100.00%] [G loss: 0.020018]\n",
      "epoch:20 step:15712 [D loss: 0.001425, acc.: 100.00%] [G loss: 0.050164]\n",
      "epoch:20 step:15713 [D loss: 0.002204, acc.: 100.00%] [G loss: 0.001889]\n",
      "epoch:20 step:15714 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.054650]\n",
      "epoch:20 step:15715 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.035840]\n",
      "epoch:20 step:15716 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.006489]\n",
      "epoch:20 step:15717 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.001570]\n",
      "epoch:20 step:15718 [D loss: 0.001500, acc.: 100.00%] [G loss: 0.414144]\n",
      "epoch:20 step:15719 [D loss: 0.016523, acc.: 100.00%] [G loss: 0.000329]\n",
      "epoch:20 step:15720 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.036591]\n",
      "epoch:20 step:15721 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.004064]\n",
      "epoch:20 step:15722 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000558]\n",
      "epoch:20 step:15723 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.001210]\n",
      "epoch:20 step:15724 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.028772]\n",
      "epoch:20 step:15725 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:20 step:15726 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:20 step:15727 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:20 step:15728 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000414]\n",
      "epoch:20 step:15729 [D loss: 0.007095, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:20 step:15730 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.002620]\n",
      "epoch:20 step:15731 [D loss: 0.004861, acc.: 100.00%] [G loss: 0.000820]\n",
      "epoch:20 step:15732 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:20 step:15733 [D loss: 0.001662, acc.: 100.00%] [G loss: 0.001621]\n",
      "epoch:20 step:15734 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.002087]\n",
      "epoch:20 step:15735 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.017279]\n",
      "epoch:20 step:15736 [D loss: 0.001954, acc.: 100.00%] [G loss: 0.034544]\n",
      "epoch:20 step:15737 [D loss: 0.001717, acc.: 100.00%] [G loss: 0.005784]\n",
      "epoch:20 step:15738 [D loss: 0.740175, acc.: 64.06%] [G loss: 2.924147]\n",
      "epoch:20 step:15739 [D loss: 2.193394, acc.: 52.34%] [G loss: 5.569674]\n",
      "epoch:20 step:15740 [D loss: 0.322935, acc.: 86.72%] [G loss: 1.568133]\n",
      "epoch:20 step:15741 [D loss: 0.007916, acc.: 100.00%] [G loss: 0.014802]\n",
      "epoch:20 step:15742 [D loss: 0.023368, acc.: 100.00%] [G loss: 0.336578]\n",
      "epoch:20 step:15743 [D loss: 0.029529, acc.: 100.00%] [G loss: 0.024802]\n",
      "epoch:20 step:15744 [D loss: 0.007267, acc.: 100.00%] [G loss: 0.030796]\n",
      "epoch:20 step:15745 [D loss: 0.012614, acc.: 100.00%] [G loss: 0.825961]\n",
      "epoch:20 step:15746 [D loss: 0.001802, acc.: 100.00%] [G loss: 0.021532]\n",
      "epoch:20 step:15747 [D loss: 0.004589, acc.: 100.00%] [G loss: 0.022603]\n",
      "epoch:20 step:15748 [D loss: 0.004766, acc.: 100.00%] [G loss: 0.184931]\n",
      "epoch:20 step:15749 [D loss: 0.006225, acc.: 100.00%] [G loss: 0.561452]\n",
      "epoch:20 step:15750 [D loss: 0.027449, acc.: 100.00%] [G loss: 0.006284]\n",
      "epoch:20 step:15751 [D loss: 0.003280, acc.: 100.00%] [G loss: 0.040790]\n",
      "epoch:20 step:15752 [D loss: 0.124361, acc.: 96.88%] [G loss: 0.163592]\n",
      "epoch:20 step:15753 [D loss: 0.002861, acc.: 100.00%] [G loss: 0.059855]\n",
      "epoch:20 step:15754 [D loss: 0.018054, acc.: 100.00%] [G loss: 0.400925]\n",
      "epoch:20 step:15755 [D loss: 0.038119, acc.: 97.66%] [G loss: 0.037423]\n",
      "epoch:20 step:15756 [D loss: 0.003690, acc.: 100.00%] [G loss: 0.008261]\n",
      "epoch:20 step:15757 [D loss: 0.006615, acc.: 100.00%] [G loss: 0.009620]\n",
      "epoch:20 step:15758 [D loss: 0.004900, acc.: 100.00%] [G loss: 0.093522]\n",
      "epoch:20 step:15759 [D loss: 0.014799, acc.: 99.22%] [G loss: 0.005829]\n",
      "epoch:20 step:15760 [D loss: 0.007155, acc.: 100.00%] [G loss: 0.004979]\n",
      "epoch:20 step:15761 [D loss: 0.000501, acc.: 100.00%] [G loss: 0.005050]\n",
      "epoch:20 step:15762 [D loss: 0.001701, acc.: 100.00%] [G loss: 0.151082]\n",
      "epoch:20 step:15763 [D loss: 0.035915, acc.: 99.22%] [G loss: 0.002735]\n",
      "epoch:20 step:15764 [D loss: 0.020789, acc.: 100.00%] [G loss: 0.002673]\n",
      "epoch:20 step:15765 [D loss: 0.003578, acc.: 100.00%] [G loss: 0.002854]\n",
      "epoch:20 step:15766 [D loss: 0.001146, acc.: 100.00%] [G loss: 0.006081]\n",
      "epoch:20 step:15767 [D loss: 0.010580, acc.: 100.00%] [G loss: 0.002107]\n",
      "epoch:20 step:15768 [D loss: 0.001991, acc.: 100.00%] [G loss: 0.003225]\n",
      "epoch:20 step:15769 [D loss: 0.014374, acc.: 100.00%] [G loss: 0.075484]\n",
      "epoch:20 step:15770 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.012641]\n",
      "epoch:20 step:15771 [D loss: 0.001736, acc.: 100.00%] [G loss: 0.010106]\n",
      "epoch:20 step:15772 [D loss: 0.014251, acc.: 100.00%] [G loss: 0.010898]\n",
      "epoch:20 step:15773 [D loss: 0.022060, acc.: 100.00%] [G loss: 0.102335]\n",
      "epoch:20 step:15774 [D loss: 0.001389, acc.: 100.00%] [G loss: 0.082032]\n",
      "epoch:20 step:15775 [D loss: 0.001337, acc.: 100.00%] [G loss: 0.018956]\n",
      "epoch:20 step:15776 [D loss: 0.001354, acc.: 100.00%] [G loss: 0.025109]\n",
      "epoch:20 step:15777 [D loss: 0.002603, acc.: 100.00%] [G loss: 0.020556]\n",
      "epoch:20 step:15778 [D loss: 0.014339, acc.: 100.00%] [G loss: 0.136519]\n",
      "epoch:20 step:15779 [D loss: 0.001688, acc.: 100.00%] [G loss: 0.522142]\n",
      "epoch:20 step:15780 [D loss: 0.020504, acc.: 100.00%] [G loss: 0.001319]\n",
      "epoch:20 step:15781 [D loss: 0.014046, acc.: 100.00%] [G loss: 0.008780]\n",
      "epoch:20 step:15782 [D loss: 0.028314, acc.: 100.00%] [G loss: 0.005944]\n",
      "epoch:20 step:15783 [D loss: 0.005108, acc.: 100.00%] [G loss: 0.157766]\n",
      "epoch:20 step:15784 [D loss: 0.008061, acc.: 100.00%] [G loss: 0.117654]\n",
      "epoch:20 step:15785 [D loss: 0.002735, acc.: 100.00%] [G loss: 0.044966]\n",
      "epoch:20 step:15786 [D loss: 0.057097, acc.: 96.88%] [G loss: 0.005988]\n",
      "epoch:20 step:15787 [D loss: 0.001526, acc.: 100.00%] [G loss: 0.004068]\n",
      "epoch:20 step:15788 [D loss: 0.000712, acc.: 100.00%] [G loss: 0.005284]\n",
      "epoch:20 step:15789 [D loss: 0.006663, acc.: 100.00%] [G loss: 0.005710]\n",
      "epoch:20 step:15790 [D loss: 0.011344, acc.: 100.00%] [G loss: 0.001646]\n",
      "epoch:20 step:15791 [D loss: 0.009520, acc.: 100.00%] [G loss: 0.006723]\n",
      "epoch:20 step:15792 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.003977]\n",
      "epoch:20 step:15793 [D loss: 0.003498, acc.: 100.00%] [G loss: 0.016936]\n",
      "epoch:20 step:15794 [D loss: 0.004007, acc.: 100.00%] [G loss: 0.026428]\n",
      "epoch:20 step:15795 [D loss: 0.008136, acc.: 100.00%] [G loss: 0.004062]\n",
      "epoch:20 step:15796 [D loss: 0.006650, acc.: 100.00%] [G loss: 0.008191]\n",
      "epoch:20 step:15797 [D loss: 0.004322, acc.: 100.00%] [G loss: 0.045545]\n",
      "epoch:20 step:15798 [D loss: 0.068088, acc.: 99.22%] [G loss: 1.989618]\n",
      "epoch:20 step:15799 [D loss: 0.018233, acc.: 100.00%] [G loss: 0.432452]\n",
      "epoch:20 step:15800 [D loss: 0.074681, acc.: 98.44%] [G loss: 0.016357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15801 [D loss: 0.002853, acc.: 100.00%] [G loss: 1.435914]\n",
      "epoch:20 step:15802 [D loss: 0.005451, acc.: 100.00%] [G loss: 0.377997]\n",
      "epoch:20 step:15803 [D loss: 0.018800, acc.: 99.22%] [G loss: 2.804015]\n",
      "epoch:20 step:15804 [D loss: 0.107406, acc.: 97.66%] [G loss: 0.174934]\n",
      "epoch:20 step:15805 [D loss: 0.001016, acc.: 100.00%] [G loss: 1.913784]\n",
      "epoch:20 step:15806 [D loss: 0.046624, acc.: 99.22%] [G loss: 0.359247]\n",
      "epoch:20 step:15807 [D loss: 0.063368, acc.: 99.22%] [G loss: 0.000545]\n",
      "epoch:20 step:15808 [D loss: 0.037903, acc.: 99.22%] [G loss: 0.059497]\n",
      "epoch:20 step:15809 [D loss: 0.004826, acc.: 100.00%] [G loss: 0.005520]\n",
      "epoch:20 step:15810 [D loss: 0.003652, acc.: 100.00%] [G loss: 0.007990]\n",
      "epoch:20 step:15811 [D loss: 0.014993, acc.: 99.22%] [G loss: 0.008770]\n",
      "epoch:20 step:15812 [D loss: 0.000811, acc.: 100.00%] [G loss: 0.001713]\n",
      "epoch:20 step:15813 [D loss: 0.010421, acc.: 99.22%] [G loss: 0.007319]\n",
      "epoch:20 step:15814 [D loss: 0.002026, acc.: 100.00%] [G loss: 0.000968]\n",
      "epoch:20 step:15815 [D loss: 0.000670, acc.: 100.00%] [G loss: 0.001189]\n",
      "epoch:20 step:15816 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.002182]\n",
      "epoch:20 step:15817 [D loss: 0.001706, acc.: 100.00%] [G loss: 0.001864]\n",
      "epoch:20 step:15818 [D loss: 0.004923, acc.: 100.00%] [G loss: 0.001138]\n",
      "epoch:20 step:15819 [D loss: 0.003882, acc.: 100.00%] [G loss: 0.003174]\n",
      "epoch:20 step:15820 [D loss: 0.009698, acc.: 100.00%] [G loss: 0.041105]\n",
      "epoch:20 step:15821 [D loss: 0.005149, acc.: 100.00%] [G loss: 0.001198]\n",
      "epoch:20 step:15822 [D loss: 0.080801, acc.: 98.44%] [G loss: 2.938544]\n",
      "epoch:20 step:15823 [D loss: 0.010648, acc.: 100.00%] [G loss: 2.914056]\n",
      "epoch:20 step:15824 [D loss: 0.108441, acc.: 96.09%] [G loss: 0.001609]\n",
      "epoch:20 step:15825 [D loss: 0.005859, acc.: 100.00%] [G loss: 1.194596]\n",
      "epoch:20 step:15826 [D loss: 0.024738, acc.: 98.44%] [G loss: 0.405683]\n",
      "epoch:20 step:15827 [D loss: 0.020456, acc.: 100.00%] [G loss: 0.265620]\n",
      "epoch:20 step:15828 [D loss: 2.091567, acc.: 48.44%] [G loss: 11.897235]\n",
      "epoch:20 step:15829 [D loss: 4.307444, acc.: 50.00%] [G loss: 3.899181]\n",
      "epoch:20 step:15830 [D loss: 1.206573, acc.: 57.03%] [G loss: 3.168993]\n",
      "epoch:20 step:15831 [D loss: 0.367788, acc.: 84.38%] [G loss: 1.660482]\n",
      "epoch:20 step:15832 [D loss: 0.029752, acc.: 100.00%] [G loss: 1.487161]\n",
      "epoch:20 step:15833 [D loss: 0.160604, acc.: 92.19%] [G loss: 3.739893]\n",
      "epoch:20 step:15834 [D loss: 0.113419, acc.: 96.09%] [G loss: 3.033994]\n",
      "epoch:20 step:15835 [D loss: 0.068714, acc.: 99.22%] [G loss: 1.812355]\n",
      "epoch:20 step:15836 [D loss: 0.144634, acc.: 97.66%] [G loss: 0.519251]\n",
      "epoch:20 step:15837 [D loss: 0.115887, acc.: 97.66%] [G loss: 3.601690]\n",
      "epoch:20 step:15838 [D loss: 0.140022, acc.: 94.53%] [G loss: 0.196392]\n",
      "epoch:20 step:15839 [D loss: 0.078440, acc.: 98.44%] [G loss: 0.088357]\n",
      "epoch:20 step:15840 [D loss: 0.051439, acc.: 99.22%] [G loss: 2.760475]\n",
      "epoch:20 step:15841 [D loss: 0.028786, acc.: 100.00%] [G loss: 3.172503]\n",
      "epoch:20 step:15842 [D loss: 0.079796, acc.: 99.22%] [G loss: 0.040860]\n",
      "epoch:20 step:15843 [D loss: 0.039517, acc.: 100.00%] [G loss: 2.110991]\n",
      "epoch:20 step:15844 [D loss: 0.091974, acc.: 99.22%] [G loss: 0.141288]\n",
      "epoch:20 step:15845 [D loss: 0.032420, acc.: 100.00%] [G loss: 0.102219]\n",
      "epoch:20 step:15846 [D loss: 0.073067, acc.: 98.44%] [G loss: 1.279048]\n",
      "epoch:20 step:15847 [D loss: 0.011128, acc.: 100.00%] [G loss: 2.483185]\n",
      "epoch:20 step:15848 [D loss: 0.150274, acc.: 96.09%] [G loss: 1.355608]\n",
      "epoch:20 step:15849 [D loss: 0.046181, acc.: 100.00%] [G loss: 0.009325]\n",
      "epoch:20 step:15850 [D loss: 0.069169, acc.: 100.00%] [G loss: 1.562814]\n",
      "epoch:20 step:15851 [D loss: 0.008075, acc.: 100.00%] [G loss: 1.582131]\n",
      "epoch:20 step:15852 [D loss: 0.025028, acc.: 100.00%] [G loss: 0.861043]\n",
      "epoch:20 step:15853 [D loss: 0.231892, acc.: 88.28%] [G loss: 2.628903]\n",
      "epoch:20 step:15854 [D loss: 0.352826, acc.: 83.59%] [G loss: 0.012138]\n",
      "epoch:20 step:15855 [D loss: 0.080816, acc.: 99.22%] [G loss: 1.459982]\n",
      "epoch:20 step:15856 [D loss: 0.106947, acc.: 97.66%] [G loss: 0.006506]\n",
      "epoch:20 step:15857 [D loss: 0.015058, acc.: 100.00%] [G loss: 2.400810]\n",
      "epoch:20 step:15858 [D loss: 0.014723, acc.: 100.00%] [G loss: 1.377752]\n",
      "epoch:20 step:15859 [D loss: 0.023010, acc.: 100.00%] [G loss: 0.006040]\n",
      "epoch:20 step:15860 [D loss: 0.754403, acc.: 66.41%] [G loss: 0.838534]\n",
      "epoch:20 step:15861 [D loss: 1.550730, acc.: 50.78%] [G loss: 0.293734]\n",
      "epoch:20 step:15862 [D loss: 0.194885, acc.: 92.19%] [G loss: 0.014539]\n",
      "epoch:20 step:15863 [D loss: 0.011686, acc.: 99.22%] [G loss: 0.006665]\n",
      "epoch:20 step:15864 [D loss: 0.003758, acc.: 100.00%] [G loss: 0.001753]\n",
      "epoch:20 step:15865 [D loss: 0.002484, acc.: 100.00%] [G loss: 0.001917]\n",
      "epoch:20 step:15866 [D loss: 0.010624, acc.: 100.00%] [G loss: 3.385446]\n",
      "epoch:20 step:15867 [D loss: 0.018868, acc.: 100.00%] [G loss: 2.820901]\n",
      "epoch:20 step:15868 [D loss: 0.003570, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:20 step:15869 [D loss: 0.004245, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:20 step:15870 [D loss: 0.011726, acc.: 100.00%] [G loss: 1.482570]\n",
      "epoch:20 step:15871 [D loss: 0.013636, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:20 step:15872 [D loss: 0.003256, acc.: 100.00%] [G loss: 0.000774]\n",
      "epoch:20 step:15873 [D loss: 0.005746, acc.: 100.00%] [G loss: 0.894602]\n",
      "epoch:20 step:15874 [D loss: 0.001859, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:20 step:15875 [D loss: 0.003938, acc.: 100.00%] [G loss: 0.003256]\n",
      "epoch:20 step:15876 [D loss: 0.036284, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:20 step:15877 [D loss: 0.009284, acc.: 100.00%] [G loss: 0.000613]\n",
      "epoch:20 step:15878 [D loss: 0.016149, acc.: 100.00%] [G loss: 1.392651]\n",
      "epoch:20 step:15879 [D loss: 0.001840, acc.: 100.00%] [G loss: 0.002113]\n",
      "epoch:20 step:15880 [D loss: 0.009425, acc.: 100.00%] [G loss: 0.014286]\n",
      "epoch:20 step:15881 [D loss: 0.011691, acc.: 100.00%] [G loss: 0.222358]\n",
      "epoch:20 step:15882 [D loss: 0.002489, acc.: 100.00%] [G loss: 0.000571]\n",
      "epoch:20 step:15883 [D loss: 0.009232, acc.: 100.00%] [G loss: 0.287549]\n",
      "epoch:20 step:15884 [D loss: 0.014718, acc.: 100.00%] [G loss: 0.001393]\n",
      "epoch:20 step:15885 [D loss: 0.023400, acc.: 100.00%] [G loss: 0.000426]\n",
      "epoch:20 step:15886 [D loss: 0.217544, acc.: 92.97%] [G loss: 0.035173]\n",
      "epoch:20 step:15887 [D loss: 0.016840, acc.: 100.00%] [G loss: 4.243816]\n",
      "epoch:20 step:15888 [D loss: 0.067484, acc.: 98.44%] [G loss: 0.175589]\n",
      "epoch:20 step:15889 [D loss: 0.016263, acc.: 100.00%] [G loss: 0.038880]\n",
      "epoch:20 step:15890 [D loss: 0.212760, acc.: 91.41%] [G loss: 0.001463]\n",
      "epoch:20 step:15891 [D loss: 0.012281, acc.: 100.00%] [G loss: 0.339940]\n",
      "epoch:20 step:15892 [D loss: 0.006555, acc.: 100.00%] [G loss: 0.000833]\n",
      "epoch:20 step:15893 [D loss: 0.354849, acc.: 82.03%] [G loss: 3.229213]\n",
      "epoch:20 step:15894 [D loss: 0.010426, acc.: 99.22%] [G loss: 2.465670]\n",
      "epoch:20 step:15895 [D loss: 1.530914, acc.: 53.12%] [G loss: 0.210574]\n",
      "epoch:20 step:15896 [D loss: 0.043879, acc.: 99.22%] [G loss: 0.219312]\n",
      "epoch:20 step:15897 [D loss: 0.080457, acc.: 98.44%] [G loss: 0.098388]\n",
      "epoch:20 step:15898 [D loss: 0.006404, acc.: 100.00%] [G loss: 0.057925]\n",
      "epoch:20 step:15899 [D loss: 0.029678, acc.: 100.00%] [G loss: 1.202841]\n",
      "epoch:20 step:15900 [D loss: 0.009930, acc.: 100.00%] [G loss: 0.276602]\n",
      "epoch:20 step:15901 [D loss: 0.003306, acc.: 100.00%] [G loss: 0.085744]\n",
      "epoch:20 step:15902 [D loss: 0.010571, acc.: 100.00%] [G loss: 0.047608]\n",
      "epoch:20 step:15903 [D loss: 0.002437, acc.: 100.00%] [G loss: 0.108016]\n",
      "epoch:20 step:15904 [D loss: 0.007077, acc.: 100.00%] [G loss: 0.086408]\n",
      "epoch:20 step:15905 [D loss: 0.002507, acc.: 100.00%] [G loss: 0.058618]\n",
      "epoch:20 step:15906 [D loss: 0.040330, acc.: 100.00%] [G loss: 0.045695]\n",
      "epoch:20 step:15907 [D loss: 0.005315, acc.: 100.00%] [G loss: 0.102643]\n",
      "epoch:20 step:15908 [D loss: 0.011097, acc.: 100.00%] [G loss: 0.137523]\n",
      "epoch:20 step:15909 [D loss: 0.013050, acc.: 100.00%] [G loss: 0.040363]\n",
      "epoch:20 step:15910 [D loss: 0.006559, acc.: 100.00%] [G loss: 0.031695]\n",
      "epoch:20 step:15911 [D loss: 0.004828, acc.: 100.00%] [G loss: 0.043725]\n",
      "epoch:20 step:15912 [D loss: 0.007367, acc.: 100.00%] [G loss: 0.022485]\n",
      "epoch:20 step:15913 [D loss: 0.002385, acc.: 100.00%] [G loss: 0.017919]\n",
      "epoch:20 step:15914 [D loss: 0.002257, acc.: 100.00%] [G loss: 0.044991]\n",
      "epoch:20 step:15915 [D loss: 0.009877, acc.: 100.00%] [G loss: 1.426591]\n",
      "epoch:20 step:15916 [D loss: 0.016876, acc.: 100.00%] [G loss: 0.010395]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:15917 [D loss: 0.018131, acc.: 100.00%] [G loss: 0.022865]\n",
      "epoch:20 step:15918 [D loss: 0.003735, acc.: 100.00%] [G loss: 0.012013]\n",
      "epoch:20 step:15919 [D loss: 0.002389, acc.: 100.00%] [G loss: 0.003775]\n",
      "epoch:20 step:15920 [D loss: 0.066029, acc.: 97.66%] [G loss: 0.003104]\n",
      "epoch:20 step:15921 [D loss: 0.006792, acc.: 100.00%] [G loss: 0.001198]\n",
      "epoch:20 step:15922 [D loss: 0.003175, acc.: 100.00%] [G loss: 0.000370]\n",
      "epoch:20 step:15923 [D loss: 0.004280, acc.: 100.00%] [G loss: 0.001067]\n",
      "epoch:20 step:15924 [D loss: 0.040241, acc.: 99.22%] [G loss: 0.035023]\n",
      "epoch:20 step:15925 [D loss: 0.010832, acc.: 100.00%] [G loss: 0.000523]\n",
      "epoch:20 step:15926 [D loss: 0.012823, acc.: 100.00%] [G loss: 0.001995]\n",
      "epoch:20 step:15927 [D loss: 0.008989, acc.: 100.00%] [G loss: 0.001636]\n",
      "epoch:20 step:15928 [D loss: 0.040560, acc.: 100.00%] [G loss: 0.004485]\n",
      "epoch:20 step:15929 [D loss: 0.004308, acc.: 100.00%] [G loss: 0.004497]\n",
      "epoch:20 step:15930 [D loss: 0.022871, acc.: 99.22%] [G loss: 0.364936]\n",
      "epoch:20 step:15931 [D loss: 0.186804, acc.: 92.97%] [G loss: 3.533767]\n",
      "epoch:20 step:15932 [D loss: 0.069992, acc.: 99.22%] [G loss: 2.420802]\n",
      "epoch:20 step:15933 [D loss: 0.099288, acc.: 97.66%] [G loss: 0.961124]\n",
      "epoch:20 step:15934 [D loss: 0.021055, acc.: 100.00%] [G loss: 0.982790]\n",
      "epoch:20 step:15935 [D loss: 1.081841, acc.: 44.53%] [G loss: 3.661795]\n",
      "epoch:20 step:15936 [D loss: 0.106643, acc.: 95.31%] [G loss: 7.637370]\n",
      "epoch:20 step:15937 [D loss: 0.882555, acc.: 63.28%] [G loss: 1.933516]\n",
      "epoch:20 step:15938 [D loss: 0.013385, acc.: 100.00%] [G loss: 0.990449]\n",
      "epoch:20 step:15939 [D loss: 0.014121, acc.: 100.00%] [G loss: 0.266263]\n",
      "epoch:20 step:15940 [D loss: 0.013051, acc.: 100.00%] [G loss: 0.076670]\n",
      "epoch:20 step:15941 [D loss: 0.051624, acc.: 100.00%] [G loss: 0.478364]\n",
      "epoch:20 step:15942 [D loss: 0.077025, acc.: 97.66%] [G loss: 0.060173]\n",
      "epoch:20 step:15943 [D loss: 0.004770, acc.: 100.00%] [G loss: 1.174311]\n",
      "epoch:20 step:15944 [D loss: 0.007788, acc.: 100.00%] [G loss: 0.037180]\n",
      "epoch:20 step:15945 [D loss: 0.017236, acc.: 99.22%] [G loss: 0.407104]\n",
      "epoch:20 step:15946 [D loss: 0.016109, acc.: 100.00%] [G loss: 0.203211]\n",
      "epoch:20 step:15947 [D loss: 0.007836, acc.: 100.00%] [G loss: 0.020261]\n",
      "epoch:20 step:15948 [D loss: 0.014426, acc.: 99.22%] [G loss: 0.008192]\n",
      "epoch:20 step:15949 [D loss: 0.005216, acc.: 100.00%] [G loss: 0.004092]\n",
      "epoch:20 step:15950 [D loss: 0.002224, acc.: 100.00%] [G loss: 0.133036]\n",
      "epoch:20 step:15951 [D loss: 0.007576, acc.: 100.00%] [G loss: 0.007857]\n",
      "epoch:20 step:15952 [D loss: 0.006116, acc.: 100.00%] [G loss: 0.003381]\n",
      "epoch:20 step:15953 [D loss: 0.001481, acc.: 100.00%] [G loss: 0.032339]\n",
      "epoch:20 step:15954 [D loss: 0.004065, acc.: 100.00%] [G loss: 0.034182]\n",
      "epoch:20 step:15955 [D loss: 0.002161, acc.: 100.00%] [G loss: 0.063268]\n",
      "epoch:20 step:15956 [D loss: 0.003568, acc.: 100.00%] [G loss: 0.003249]\n",
      "epoch:20 step:15957 [D loss: 0.002028, acc.: 100.00%] [G loss: 0.002448]\n",
      "epoch:20 step:15958 [D loss: 0.009908, acc.: 100.00%] [G loss: 0.003851]\n",
      "epoch:20 step:15959 [D loss: 0.004430, acc.: 100.00%] [G loss: 0.001571]\n",
      "epoch:20 step:15960 [D loss: 0.002501, acc.: 100.00%] [G loss: 0.000920]\n",
      "epoch:20 step:15961 [D loss: 0.001996, acc.: 100.00%] [G loss: 0.005253]\n",
      "epoch:20 step:15962 [D loss: 0.005306, acc.: 100.00%] [G loss: 0.015965]\n",
      "epoch:20 step:15963 [D loss: 0.001370, acc.: 100.00%] [G loss: 0.021601]\n",
      "epoch:20 step:15964 [D loss: 0.003378, acc.: 100.00%] [G loss: 1.416231]\n",
      "epoch:20 step:15965 [D loss: 0.002467, acc.: 100.00%] [G loss: 0.016129]\n",
      "epoch:20 step:15966 [D loss: 0.007064, acc.: 100.00%] [G loss: 0.397074]\n",
      "epoch:20 step:15967 [D loss: 0.004944, acc.: 100.00%] [G loss: 0.000914]\n",
      "epoch:20 step:15968 [D loss: 0.009265, acc.: 100.00%] [G loss: 0.287629]\n",
      "epoch:20 step:15969 [D loss: 0.037993, acc.: 100.00%] [G loss: 0.033253]\n",
      "epoch:20 step:15970 [D loss: 0.023134, acc.: 100.00%] [G loss: 0.044290]\n",
      "epoch:20 step:15971 [D loss: 0.012460, acc.: 100.00%] [G loss: 0.001516]\n",
      "epoch:20 step:15972 [D loss: 0.023818, acc.: 99.22%] [G loss: 0.067769]\n",
      "epoch:20 step:15973 [D loss: 0.024185, acc.: 99.22%] [G loss: 0.000547]\n",
      "epoch:20 step:15974 [D loss: 0.003035, acc.: 100.00%] [G loss: 0.001333]\n",
      "epoch:20 step:15975 [D loss: 0.012434, acc.: 100.00%] [G loss: 0.000441]\n",
      "epoch:20 step:15976 [D loss: 0.001854, acc.: 100.00%] [G loss: 0.000534]\n",
      "epoch:20 step:15977 [D loss: 0.003677, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:20 step:15978 [D loss: 0.009651, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:20 step:15979 [D loss: 0.079110, acc.: 96.09%] [G loss: 0.001968]\n",
      "epoch:20 step:15980 [D loss: 0.023833, acc.: 99.22%] [G loss: 0.041919]\n",
      "epoch:20 step:15981 [D loss: 0.106761, acc.: 95.31%] [G loss: 0.017425]\n",
      "epoch:20 step:15982 [D loss: 0.010109, acc.: 100.00%] [G loss: 0.011212]\n",
      "epoch:20 step:15983 [D loss: 0.177272, acc.: 94.53%] [G loss: 1.180887]\n",
      "epoch:20 step:15984 [D loss: 0.011100, acc.: 100.00%] [G loss: 3.075304]\n",
      "epoch:20 step:15985 [D loss: 0.063449, acc.: 96.88%] [G loss: 2.333694]\n",
      "epoch:20 step:15986 [D loss: 0.010004, acc.: 100.00%] [G loss: 1.159856]\n",
      "epoch:20 step:15987 [D loss: 0.013142, acc.: 99.22%] [G loss: 0.929537]\n",
      "epoch:20 step:15988 [D loss: 0.014272, acc.: 100.00%] [G loss: 0.301094]\n",
      "epoch:20 step:15989 [D loss: 0.045448, acc.: 98.44%] [G loss: 3.755450]\n",
      "epoch:20 step:15990 [D loss: 0.005591, acc.: 100.00%] [G loss: 0.094557]\n",
      "epoch:20 step:15991 [D loss: 0.020806, acc.: 100.00%] [G loss: 0.125471]\n",
      "epoch:20 step:15992 [D loss: 0.046113, acc.: 100.00%] [G loss: 0.672495]\n",
      "epoch:20 step:15993 [D loss: 0.025326, acc.: 100.00%] [G loss: 0.831586]\n",
      "epoch:20 step:15994 [D loss: 0.343342, acc.: 82.03%] [G loss: 0.562078]\n",
      "epoch:20 step:15995 [D loss: 0.036744, acc.: 99.22%] [G loss: 2.223755]\n",
      "epoch:20 step:15996 [D loss: 0.021972, acc.: 99.22%] [G loss: 0.274717]\n",
      "epoch:20 step:15997 [D loss: 0.022618, acc.: 100.00%] [G loss: 0.116840]\n",
      "epoch:20 step:15998 [D loss: 0.005240, acc.: 100.00%] [G loss: 3.547203]\n",
      "epoch:20 step:15999 [D loss: 0.001335, acc.: 100.00%] [G loss: 3.189848]\n",
      "epoch:20 step:16000 [D loss: 0.016991, acc.: 100.00%] [G loss: 0.130191]\n",
      "epoch:20 step:16001 [D loss: 0.002649, acc.: 100.00%] [G loss: 1.682780]\n",
      "epoch:20 step:16002 [D loss: 0.015634, acc.: 99.22%] [G loss: 0.734282]\n",
      "epoch:20 step:16003 [D loss: 0.000824, acc.: 100.00%] [G loss: 0.153724]\n",
      "epoch:20 step:16004 [D loss: 0.001034, acc.: 100.00%] [G loss: 1.599552]\n",
      "epoch:20 step:16005 [D loss: 0.001747, acc.: 100.00%] [G loss: 0.398031]\n",
      "epoch:20 step:16006 [D loss: 0.002589, acc.: 100.00%] [G loss: 0.097391]\n",
      "epoch:20 step:16007 [D loss: 0.011060, acc.: 100.00%] [G loss: 0.065544]\n",
      "epoch:20 step:16008 [D loss: 0.035517, acc.: 100.00%] [G loss: 0.176524]\n",
      "epoch:20 step:16009 [D loss: 0.037794, acc.: 98.44%] [G loss: 0.442197]\n",
      "epoch:20 step:16010 [D loss: 0.005164, acc.: 100.00%] [G loss: 0.287038]\n",
      "epoch:20 step:16011 [D loss: 0.051881, acc.: 98.44%] [G loss: 0.050694]\n",
      "epoch:20 step:16012 [D loss: 0.006541, acc.: 100.00%] [G loss: 0.141920]\n",
      "epoch:20 step:16013 [D loss: 0.029819, acc.: 100.00%] [G loss: 0.153321]\n",
      "epoch:20 step:16014 [D loss: 0.004217, acc.: 100.00%] [G loss: 0.113500]\n",
      "epoch:20 step:16015 [D loss: 0.153380, acc.: 92.97%] [G loss: 0.000749]\n",
      "epoch:20 step:16016 [D loss: 0.104633, acc.: 96.09%] [G loss: 0.005785]\n",
      "epoch:20 step:16017 [D loss: 0.001067, acc.: 100.00%] [G loss: 0.986761]\n",
      "epoch:20 step:16018 [D loss: 0.002203, acc.: 100.00%] [G loss: 0.109413]\n",
      "epoch:20 step:16019 [D loss: 0.004006, acc.: 100.00%] [G loss: 0.129372]\n",
      "epoch:20 step:16020 [D loss: 0.003206, acc.: 100.00%] [G loss: 1.608656]\n",
      "epoch:20 step:16021 [D loss: 0.003981, acc.: 100.00%] [G loss: 0.064499]\n",
      "epoch:20 step:16022 [D loss: 0.006552, acc.: 100.00%] [G loss: 0.186488]\n",
      "epoch:20 step:16023 [D loss: 0.007428, acc.: 100.00%] [G loss: 0.054690]\n",
      "epoch:20 step:16024 [D loss: 0.011330, acc.: 100.00%] [G loss: 0.192767]\n",
      "epoch:20 step:16025 [D loss: 0.058802, acc.: 98.44%] [G loss: 0.181240]\n",
      "epoch:20 step:16026 [D loss: 0.008364, acc.: 100.00%] [G loss: 0.163587]\n",
      "epoch:20 step:16027 [D loss: 0.003718, acc.: 100.00%] [G loss: 0.240623]\n",
      "epoch:20 step:16028 [D loss: 0.003201, acc.: 100.00%] [G loss: 0.008378]\n",
      "epoch:20 step:16029 [D loss: 0.001927, acc.: 100.00%] [G loss: 0.003009]\n",
      "epoch:20 step:16030 [D loss: 0.001454, acc.: 100.00%] [G loss: 0.020972]\n",
      "epoch:20 step:16031 [D loss: 0.005265, acc.: 100.00%] [G loss: 1.289182]\n",
      "epoch:20 step:16032 [D loss: 0.002740, acc.: 100.00%] [G loss: 0.000966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16033 [D loss: 0.002525, acc.: 100.00%] [G loss: 0.000553]\n",
      "epoch:20 step:16034 [D loss: 0.007814, acc.: 100.00%] [G loss: 0.612911]\n",
      "epoch:20 step:16035 [D loss: 0.000683, acc.: 100.00%] [G loss: 0.418519]\n",
      "epoch:20 step:16036 [D loss: 0.002419, acc.: 100.00%] [G loss: 0.913606]\n",
      "epoch:20 step:16037 [D loss: 0.001052, acc.: 100.00%] [G loss: 0.001622]\n",
      "epoch:20 step:16038 [D loss: 0.001357, acc.: 100.00%] [G loss: 0.237603]\n",
      "epoch:20 step:16039 [D loss: 0.001041, acc.: 100.00%] [G loss: 0.085247]\n",
      "epoch:20 step:16040 [D loss: 0.005563, acc.: 100.00%] [G loss: 0.013613]\n",
      "epoch:20 step:16041 [D loss: 0.001308, acc.: 100.00%] [G loss: 3.582163]\n",
      "epoch:20 step:16042 [D loss: 0.006315, acc.: 100.00%] [G loss: 0.019361]\n",
      "epoch:20 step:16043 [D loss: 0.002586, acc.: 100.00%] [G loss: 0.056700]\n",
      "epoch:20 step:16044 [D loss: 0.109186, acc.: 96.09%] [G loss: 1.078030]\n",
      "epoch:20 step:16045 [D loss: 0.150429, acc.: 92.97%] [G loss: 0.884815]\n",
      "epoch:20 step:16046 [D loss: 0.049032, acc.: 97.66%] [G loss: 0.275795]\n",
      "epoch:20 step:16047 [D loss: 0.003339, acc.: 100.00%] [G loss: 0.323076]\n",
      "epoch:20 step:16048 [D loss: 0.009726, acc.: 100.00%] [G loss: 0.495379]\n",
      "epoch:20 step:16049 [D loss: 0.008518, acc.: 100.00%] [G loss: 0.342115]\n",
      "epoch:20 step:16050 [D loss: 0.143540, acc.: 94.53%] [G loss: 0.000025]\n",
      "epoch:20 step:16051 [D loss: 0.062326, acc.: 97.66%] [G loss: 0.714690]\n",
      "epoch:20 step:16052 [D loss: 0.003735, acc.: 100.00%] [G loss: 0.452850]\n",
      "epoch:20 step:16053 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.166748]\n",
      "epoch:20 step:16054 [D loss: 0.001630, acc.: 100.00%] [G loss: 0.007443]\n",
      "epoch:20 step:16055 [D loss: 0.019463, acc.: 100.00%] [G loss: 0.041769]\n",
      "epoch:20 step:16056 [D loss: 0.003441, acc.: 100.00%] [G loss: 0.029553]\n",
      "epoch:20 step:16057 [D loss: 0.010717, acc.: 100.00%] [G loss: 0.287174]\n",
      "epoch:20 step:16058 [D loss: 0.001715, acc.: 100.00%] [G loss: 0.184453]\n",
      "epoch:20 step:16059 [D loss: 0.002109, acc.: 100.00%] [G loss: 0.043558]\n",
      "epoch:20 step:16060 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.084237]\n",
      "epoch:20 step:16061 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.071466]\n",
      "epoch:20 step:16062 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.069185]\n",
      "epoch:20 step:16063 [D loss: 0.000701, acc.: 100.00%] [G loss: 0.519132]\n",
      "epoch:20 step:16064 [D loss: 0.004337, acc.: 100.00%] [G loss: 0.146876]\n",
      "epoch:20 step:16065 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.052628]\n",
      "epoch:20 step:16066 [D loss: 0.003369, acc.: 100.00%] [G loss: 0.031585]\n",
      "epoch:20 step:16067 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.003249]\n",
      "epoch:20 step:16068 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.006616]\n",
      "epoch:20 step:16069 [D loss: 0.029329, acc.: 100.00%] [G loss: 0.115571]\n",
      "epoch:20 step:16070 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.209132]\n",
      "epoch:20 step:16071 [D loss: 0.001054, acc.: 100.00%] [G loss: 0.068638]\n",
      "epoch:20 step:16072 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.133160]\n",
      "epoch:20 step:16073 [D loss: 0.005801, acc.: 100.00%] [G loss: 0.033429]\n",
      "epoch:20 step:16074 [D loss: 0.000770, acc.: 100.00%] [G loss: 0.036100]\n",
      "epoch:20 step:16075 [D loss: 0.000822, acc.: 100.00%] [G loss: 0.025564]\n",
      "epoch:20 step:16076 [D loss: 0.000150, acc.: 100.00%] [G loss: 1.577471]\n",
      "epoch:20 step:16077 [D loss: 0.000693, acc.: 100.00%] [G loss: 0.044631]\n",
      "epoch:20 step:16078 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.059803]\n",
      "epoch:20 step:16079 [D loss: 0.002780, acc.: 100.00%] [G loss: 0.051424]\n",
      "epoch:20 step:16080 [D loss: 0.001409, acc.: 100.00%] [G loss: 0.060298]\n",
      "epoch:20 step:16081 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.032739]\n",
      "epoch:20 step:16082 [D loss: 0.002680, acc.: 100.00%] [G loss: 0.018917]\n",
      "epoch:20 step:16083 [D loss: 0.000694, acc.: 100.00%] [G loss: 0.021564]\n",
      "epoch:20 step:16084 [D loss: 0.001962, acc.: 100.00%] [G loss: 0.037946]\n",
      "epoch:20 step:16085 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.076421]\n",
      "epoch:20 step:16086 [D loss: 0.000134, acc.: 100.00%] [G loss: 4.033709]\n",
      "epoch:20 step:16087 [D loss: 0.001773, acc.: 100.00%] [G loss: 0.014117]\n",
      "epoch:20 step:16088 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.022629]\n",
      "epoch:20 step:16089 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.177934]\n",
      "epoch:20 step:16090 [D loss: 0.030313, acc.: 99.22%] [G loss: 0.112766]\n",
      "epoch:20 step:16091 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.096535]\n",
      "epoch:20 step:16092 [D loss: 0.001350, acc.: 100.00%] [G loss: 0.179959]\n",
      "epoch:20 step:16093 [D loss: 0.001995, acc.: 100.00%] [G loss: 0.034173]\n",
      "epoch:20 step:16094 [D loss: 0.001084, acc.: 100.00%] [G loss: 0.010275]\n",
      "epoch:20 step:16095 [D loss: 0.003816, acc.: 100.00%] [G loss: 0.278507]\n",
      "epoch:20 step:16096 [D loss: 0.001206, acc.: 100.00%] [G loss: 0.062845]\n",
      "epoch:20 step:16097 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.008072]\n",
      "epoch:20 step:16098 [D loss: 0.000822, acc.: 100.00%] [G loss: 0.013208]\n",
      "epoch:20 step:16099 [D loss: 0.002169, acc.: 100.00%] [G loss: 0.012432]\n",
      "epoch:20 step:16100 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.181378]\n",
      "epoch:20 step:16101 [D loss: 0.001743, acc.: 100.00%] [G loss: 0.004959]\n",
      "epoch:20 step:16102 [D loss: 0.005052, acc.: 100.00%] [G loss: 0.033171]\n",
      "epoch:20 step:16103 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.005091]\n",
      "epoch:20 step:16104 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.012202]\n",
      "epoch:20 step:16105 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.002539]\n",
      "epoch:20 step:16106 [D loss: 0.000806, acc.: 100.00%] [G loss: 0.004621]\n",
      "epoch:20 step:16107 [D loss: 0.000472, acc.: 100.00%] [G loss: 0.006103]\n",
      "epoch:20 step:16108 [D loss: 0.001385, acc.: 100.00%] [G loss: 0.001850]\n",
      "epoch:20 step:16109 [D loss: 0.003267, acc.: 100.00%] [G loss: 0.002960]\n",
      "epoch:20 step:16110 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.003355]\n",
      "epoch:20 step:16111 [D loss: 0.003775, acc.: 100.00%] [G loss: 0.004437]\n",
      "epoch:20 step:16112 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.001771]\n",
      "epoch:20 step:16113 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:20 step:16114 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000459]\n",
      "epoch:20 step:16115 [D loss: 0.000936, acc.: 100.00%] [G loss: 0.001136]\n",
      "epoch:20 step:16116 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.042487]\n",
      "epoch:20 step:16117 [D loss: 0.000438, acc.: 100.00%] [G loss: 0.004061]\n",
      "epoch:20 step:16118 [D loss: 0.003046, acc.: 100.00%] [G loss: 0.002544]\n",
      "epoch:20 step:16119 [D loss: 0.002622, acc.: 100.00%] [G loss: 0.005114]\n",
      "epoch:20 step:16120 [D loss: 0.001222, acc.: 100.00%] [G loss: 0.016748]\n",
      "epoch:20 step:16121 [D loss: 0.008251, acc.: 100.00%] [G loss: 0.000903]\n",
      "epoch:20 step:16122 [D loss: 0.000449, acc.: 100.00%] [G loss: 0.014174]\n",
      "epoch:20 step:16123 [D loss: 0.001099, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:20 step:16124 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.002831]\n",
      "epoch:20 step:16125 [D loss: 0.001187, acc.: 100.00%] [G loss: 0.004376]\n",
      "epoch:20 step:16126 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.003227]\n",
      "epoch:20 step:16127 [D loss: 0.015483, acc.: 100.00%] [G loss: 0.000551]\n",
      "epoch:20 step:16128 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.043001]\n",
      "epoch:20 step:16129 [D loss: 0.002356, acc.: 100.00%] [G loss: 0.000667]\n",
      "epoch:20 step:16130 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.003900]\n",
      "epoch:20 step:16131 [D loss: 0.000909, acc.: 100.00%] [G loss: 0.013485]\n",
      "epoch:20 step:16132 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.001571]\n",
      "epoch:20 step:16133 [D loss: 0.002779, acc.: 100.00%] [G loss: 0.009373]\n",
      "epoch:20 step:16134 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.006662]\n",
      "epoch:20 step:16135 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.000369]\n",
      "epoch:20 step:16136 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:20 step:16137 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.001790]\n",
      "epoch:20 step:16138 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.001023]\n",
      "epoch:20 step:16139 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.089916]\n",
      "epoch:20 step:16140 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.004382]\n",
      "epoch:20 step:16141 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.000646]\n",
      "epoch:20 step:16142 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.005131]\n",
      "epoch:20 step:16143 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.003642]\n",
      "epoch:20 step:16144 [D loss: 0.003831, acc.: 100.00%] [G loss: 0.000825]\n",
      "epoch:20 step:16145 [D loss: 0.002070, acc.: 100.00%] [G loss: 0.000616]\n",
      "epoch:20 step:16146 [D loss: 0.028564, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:20 step:16147 [D loss: 0.005358, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:20 step:16148 [D loss: 0.015553, acc.: 100.00%] [G loss: 0.000042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16149 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000934]\n",
      "epoch:20 step:16150 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.001465]\n",
      "epoch:20 step:16151 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:20 step:16152 [D loss: 0.000555, acc.: 100.00%] [G loss: 0.011275]\n",
      "epoch:20 step:16153 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000982]\n",
      "epoch:20 step:16154 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:20 step:16155 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.001447]\n",
      "epoch:20 step:16156 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:20 step:16157 [D loss: 0.014348, acc.: 100.00%] [G loss: 0.000996]\n",
      "epoch:20 step:16158 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:20 step:16159 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:20 step:16160 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:20 step:16161 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:20 step:16162 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:20 step:16163 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:20 step:16164 [D loss: 0.002213, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:20 step:16165 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000578]\n",
      "epoch:20 step:16166 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:20 step:16167 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.000802]\n",
      "epoch:20 step:16168 [D loss: 0.000529, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:20 step:16169 [D loss: 0.090689, acc.: 97.66%] [G loss: 0.124416]\n",
      "epoch:20 step:16170 [D loss: 0.016598, acc.: 99.22%] [G loss: 0.219412]\n",
      "epoch:20 step:16171 [D loss: 0.182340, acc.: 91.41%] [G loss: 0.000010]\n",
      "epoch:20 step:16172 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:20 step:16173 [D loss: 0.001971, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:20 step:16174 [D loss: 0.016439, acc.: 99.22%] [G loss: 0.024583]\n",
      "epoch:20 step:16175 [D loss: 0.002468, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:20 step:16176 [D loss: 0.000798, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:20 step:16177 [D loss: 0.001510, acc.: 100.00%] [G loss: 0.001029]\n",
      "epoch:20 step:16178 [D loss: 0.003251, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:20 step:16179 [D loss: 0.005263, acc.: 100.00%] [G loss: 0.001480]\n",
      "epoch:20 step:16180 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:20 step:16181 [D loss: 0.044461, acc.: 99.22%] [G loss: 0.017388]\n",
      "epoch:20 step:16182 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.007843]\n",
      "epoch:20 step:16183 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.535643]\n",
      "epoch:20 step:16184 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.004856]\n",
      "epoch:20 step:16185 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.307603]\n",
      "epoch:20 step:16186 [D loss: 0.008392, acc.: 100.00%] [G loss: 0.024281]\n",
      "epoch:20 step:16187 [D loss: 0.000637, acc.: 100.00%] [G loss: 0.003117]\n",
      "epoch:20 step:16188 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.044917]\n",
      "epoch:20 step:16189 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.003369]\n",
      "epoch:20 step:16190 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.006078]\n",
      "epoch:20 step:16191 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.025213]\n",
      "epoch:20 step:16192 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.006420]\n",
      "epoch:20 step:16193 [D loss: 0.000763, acc.: 100.00%] [G loss: 0.002756]\n",
      "epoch:20 step:16194 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.007813]\n",
      "epoch:20 step:16195 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.025435]\n",
      "epoch:20 step:16196 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.011943]\n",
      "epoch:20 step:16197 [D loss: 0.001585, acc.: 100.00%] [G loss: 0.050781]\n",
      "epoch:20 step:16198 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.050587]\n",
      "epoch:20 step:16199 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.000663]\n",
      "epoch:20 step:16200 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000499]\n",
      "epoch:20 step:16201 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.003909]\n",
      "epoch:20 step:16202 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.009600]\n",
      "epoch:20 step:16203 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.000426]\n",
      "epoch:20 step:16204 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.005725]\n",
      "epoch:20 step:16205 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:20 step:16206 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.019476]\n",
      "epoch:20 step:16207 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.001430]\n",
      "epoch:20 step:16208 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.001134]\n",
      "epoch:20 step:16209 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.006916]\n",
      "epoch:20 step:16210 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.000386]\n",
      "epoch:20 step:16211 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.004879]\n",
      "epoch:20 step:16212 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.002962]\n",
      "epoch:20 step:16213 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.001189]\n",
      "epoch:20 step:16214 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.007302]\n",
      "epoch:20 step:16215 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.001102]\n",
      "epoch:20 step:16216 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.003668]\n",
      "epoch:20 step:16217 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000734]\n",
      "epoch:20 step:16218 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.001447]\n",
      "epoch:20 step:16219 [D loss: 0.000712, acc.: 100.00%] [G loss: 0.466061]\n",
      "epoch:20 step:16220 [D loss: 0.003816, acc.: 100.00%] [G loss: 0.000516]\n",
      "epoch:20 step:16221 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.001381]\n",
      "epoch:20 step:16222 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.024517]\n",
      "epoch:20 step:16223 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.021712]\n",
      "epoch:20 step:16224 [D loss: 0.001150, acc.: 100.00%] [G loss: 0.001443]\n",
      "epoch:20 step:16225 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.026397]\n",
      "epoch:20 step:16226 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:20 step:16227 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.014808]\n",
      "epoch:20 step:16228 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.001331]\n",
      "epoch:20 step:16229 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:20 step:16230 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.053567]\n",
      "epoch:20 step:16231 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.002847]\n",
      "epoch:20 step:16232 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000616]\n",
      "epoch:20 step:16233 [D loss: 0.010263, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:20 step:16234 [D loss: 0.005644, acc.: 100.00%] [G loss: 0.072576]\n",
      "epoch:20 step:16235 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001422]\n",
      "epoch:20 step:16236 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000636]\n",
      "epoch:20 step:16237 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000750]\n",
      "epoch:20 step:16238 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001586]\n",
      "epoch:20 step:16239 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000876]\n",
      "epoch:20 step:16240 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000669]\n",
      "epoch:20 step:16241 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.006468]\n",
      "epoch:20 step:16242 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000642]\n",
      "epoch:20 step:16243 [D loss: 0.000885, acc.: 100.00%] [G loss: 0.000490]\n",
      "epoch:20 step:16244 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001834]\n",
      "epoch:20 step:16245 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001100]\n",
      "epoch:20 step:16246 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:20 step:16247 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:20 step:16248 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.001141]\n",
      "epoch:20 step:16249 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:20 step:16250 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.005509]\n",
      "epoch:20 step:16251 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.013823]\n",
      "epoch:20 step:16252 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000657]\n",
      "epoch:20 step:16253 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:20 step:16254 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001042]\n",
      "epoch:20 step:16255 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.007833]\n",
      "epoch:20 step:16256 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000559]\n",
      "epoch:20 step:16257 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:20 step:16258 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001164]\n",
      "epoch:20 step:16259 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.005484]\n",
      "epoch:20 step:16260 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001623]\n",
      "epoch:20 step:16261 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:20 step:16262 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.025959]\n",
      "epoch:20 step:16263 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:20 step:16264 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.001046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16265 [D loss: 0.000624, acc.: 100.00%] [G loss: 0.003128]\n",
      "epoch:20 step:16266 [D loss: 0.000449, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:20 step:16267 [D loss: 0.002492, acc.: 100.00%] [G loss: 0.019591]\n",
      "epoch:20 step:16268 [D loss: 0.019821, acc.: 100.00%] [G loss: 0.002300]\n",
      "epoch:20 step:16269 [D loss: 0.016179, acc.: 99.22%] [G loss: 0.153703]\n",
      "epoch:20 step:16270 [D loss: 0.018333, acc.: 100.00%] [G loss: 0.035429]\n",
      "epoch:20 step:16271 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.032653]\n",
      "epoch:20 step:16272 [D loss: 0.001117, acc.: 100.00%] [G loss: 0.254901]\n",
      "epoch:20 step:16273 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.031254]\n",
      "epoch:20 step:16274 [D loss: 0.010960, acc.: 100.00%] [G loss: 0.005107]\n",
      "epoch:20 step:16275 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.020610]\n",
      "epoch:20 step:16276 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.032725]\n",
      "epoch:20 step:16277 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.008618]\n",
      "epoch:20 step:16278 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.900812]\n",
      "epoch:20 step:16279 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.009409]\n",
      "epoch:20 step:16280 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.037623]\n",
      "epoch:20 step:16281 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.002582]\n",
      "epoch:20 step:16282 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.199684]\n",
      "epoch:20 step:16283 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.002818]\n",
      "epoch:20 step:16284 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.013285]\n",
      "epoch:20 step:16285 [D loss: 0.001158, acc.: 100.00%] [G loss: 0.014348]\n",
      "epoch:20 step:16286 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.004360]\n",
      "epoch:20 step:16287 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.002180]\n",
      "epoch:20 step:16288 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.000999]\n",
      "epoch:20 step:16289 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:20 step:16290 [D loss: 0.001341, acc.: 100.00%] [G loss: 0.000968]\n",
      "epoch:20 step:16291 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.002077]\n",
      "epoch:20 step:16292 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:20 step:16293 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000936]\n",
      "epoch:20 step:16294 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001725]\n",
      "epoch:20 step:16295 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000510]\n",
      "epoch:20 step:16296 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.031390]\n",
      "epoch:20 step:16297 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.002468]\n",
      "epoch:20 step:16298 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001120]\n",
      "epoch:20 step:16299 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000813]\n",
      "epoch:20 step:16300 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000400]\n",
      "epoch:20 step:16301 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.001036]\n",
      "epoch:20 step:16302 [D loss: 0.001617, acc.: 100.00%] [G loss: 0.002361]\n",
      "epoch:20 step:16303 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.004852]\n",
      "epoch:20 step:16304 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000409]\n",
      "epoch:20 step:16305 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.108793]\n",
      "epoch:20 step:16306 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.001468]\n",
      "epoch:20 step:16307 [D loss: 0.000845, acc.: 100.00%] [G loss: 0.000330]\n",
      "epoch:20 step:16308 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:20 step:16309 [D loss: 0.004461, acc.: 100.00%] [G loss: 0.001303]\n",
      "epoch:20 step:16310 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.003584]\n",
      "epoch:20 step:16311 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.012203]\n",
      "epoch:20 step:16312 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.004940]\n",
      "epoch:20 step:16313 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.009666]\n",
      "epoch:20 step:16314 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.003889]\n",
      "epoch:20 step:16315 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.003637]\n",
      "epoch:20 step:16316 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001732]\n",
      "epoch:20 step:16317 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:20 step:16318 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.002985]\n",
      "epoch:20 step:16319 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.003994]\n",
      "epoch:20 step:16320 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.002934]\n",
      "epoch:20 step:16321 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.003145]\n",
      "epoch:20 step:16322 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.001465]\n",
      "epoch:20 step:16323 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000830]\n",
      "epoch:20 step:16324 [D loss: 0.002690, acc.: 100.00%] [G loss: 0.001015]\n",
      "epoch:20 step:16325 [D loss: 0.000399, acc.: 100.00%] [G loss: 0.002056]\n",
      "epoch:20 step:16326 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:20 step:16327 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.012950]\n",
      "epoch:20 step:16328 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.000968]\n",
      "epoch:20 step:16329 [D loss: 0.011915, acc.: 100.00%] [G loss: 0.000718]\n",
      "epoch:20 step:16330 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000660]\n",
      "epoch:20 step:16331 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.001538]\n",
      "epoch:20 step:16332 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.019716]\n",
      "epoch:20 step:16333 [D loss: 0.152096, acc.: 94.53%] [G loss: 0.000017]\n",
      "epoch:20 step:16334 [D loss: 0.063989, acc.: 98.44%] [G loss: 0.000002]\n",
      "epoch:20 step:16335 [D loss: 0.002654, acc.: 100.00%] [G loss: 0.044432]\n",
      "epoch:20 step:16336 [D loss: 0.823933, acc.: 72.66%] [G loss: 9.728824]\n",
      "epoch:20 step:16337 [D loss: 3.359652, acc.: 48.44%] [G loss: 0.927758]\n",
      "epoch:20 step:16338 [D loss: 0.254864, acc.: 90.62%] [G loss: 0.215703]\n",
      "epoch:20 step:16339 [D loss: 0.101519, acc.: 97.66%] [G loss: 0.048179]\n",
      "epoch:20 step:16340 [D loss: 0.091998, acc.: 96.09%] [G loss: 0.001244]\n",
      "epoch:20 step:16341 [D loss: 0.018935, acc.: 100.00%] [G loss: 0.002547]\n",
      "epoch:20 step:16342 [D loss: 0.013651, acc.: 100.00%] [G loss: 0.877719]\n",
      "epoch:20 step:16343 [D loss: 0.073324, acc.: 99.22%] [G loss: 0.000164]\n",
      "epoch:20 step:16344 [D loss: 0.016304, acc.: 100.00%] [G loss: 0.000743]\n",
      "epoch:20 step:16345 [D loss: 0.015938, acc.: 100.00%] [G loss: 1.992925]\n",
      "epoch:20 step:16346 [D loss: 0.028213, acc.: 100.00%] [G loss: 0.003054]\n",
      "epoch:20 step:16347 [D loss: 0.006308, acc.: 100.00%] [G loss: 1.187921]\n",
      "epoch:20 step:16348 [D loss: 0.177161, acc.: 94.53%] [G loss: 0.043141]\n",
      "epoch:20 step:16349 [D loss: 0.094118, acc.: 96.88%] [G loss: 0.019930]\n",
      "epoch:20 step:16350 [D loss: 0.106376, acc.: 94.53%] [G loss: 1.902268]\n",
      "epoch:20 step:16351 [D loss: 0.005530, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:20 step:16352 [D loss: 0.006725, acc.: 100.00%] [G loss: 0.392260]\n",
      "epoch:20 step:16353 [D loss: 0.023932, acc.: 99.22%] [G loss: 0.000362]\n",
      "epoch:20 step:16354 [D loss: 0.038215, acc.: 99.22%] [G loss: 1.605000]\n",
      "epoch:20 step:16355 [D loss: 0.000739, acc.: 100.00%] [G loss: 2.246958]\n",
      "epoch:20 step:16356 [D loss: 0.001872, acc.: 100.00%] [G loss: 0.239659]\n",
      "epoch:20 step:16357 [D loss: 0.028062, acc.: 100.00%] [G loss: 0.033300]\n",
      "epoch:20 step:16358 [D loss: 0.084873, acc.: 99.22%] [G loss: 0.012228]\n",
      "epoch:20 step:16359 [D loss: 0.089726, acc.: 95.31%] [G loss: 0.000026]\n",
      "epoch:20 step:16360 [D loss: 0.004290, acc.: 100.00%] [G loss: 0.001210]\n",
      "epoch:20 step:16361 [D loss: 0.002341, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:20 step:16362 [D loss: 0.035542, acc.: 100.00%] [G loss: 0.001453]\n",
      "epoch:20 step:16363 [D loss: 0.003273, acc.: 100.00%] [G loss: 0.000903]\n",
      "epoch:20 step:16364 [D loss: 0.006475, acc.: 100.00%] [G loss: 0.119173]\n",
      "epoch:20 step:16365 [D loss: 0.026706, acc.: 99.22%] [G loss: 0.753540]\n",
      "epoch:20 step:16366 [D loss: 0.004660, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:20 step:16367 [D loss: 0.025622, acc.: 99.22%] [G loss: 0.367131]\n",
      "epoch:20 step:16368 [D loss: 0.001915, acc.: 100.00%] [G loss: 0.000246]\n",
      "epoch:20 step:16369 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:20 step:16370 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:20 step:16371 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:20 step:16372 [D loss: 0.000423, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:20 step:16373 [D loss: 0.001067, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:20 step:16374 [D loss: 0.003612, acc.: 100.00%] [G loss: 0.029036]\n",
      "epoch:20 step:16375 [D loss: 0.002400, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:20 step:16376 [D loss: 0.000892, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:20 step:16377 [D loss: 0.000848, acc.: 100.00%] [G loss: 0.001051]\n",
      "epoch:20 step:16378 [D loss: 0.003127, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:20 step:16379 [D loss: 0.004583, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:20 step:16380 [D loss: 0.069496, acc.: 99.22%] [G loss: 0.352997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20 step:16381 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.043814]\n",
      "epoch:20 step:16382 [D loss: 0.180811, acc.: 93.75%] [G loss: 0.000058]\n",
      "epoch:20 step:16383 [D loss: 2.891963, acc.: 53.12%] [G loss: 6.888383]\n",
      "epoch:20 step:16384 [D loss: 1.992227, acc.: 50.00%] [G loss: 5.321923]\n",
      "epoch:20 step:16385 [D loss: 1.178210, acc.: 57.03%] [G loss: 1.909800]\n",
      "epoch:20 step:16386 [D loss: 0.090176, acc.: 96.88%] [G loss: 1.237323]\n",
      "epoch:20 step:16387 [D loss: 0.107111, acc.: 96.09%] [G loss: 0.930236]\n",
      "epoch:20 step:16388 [D loss: 0.112259, acc.: 97.66%] [G loss: 1.238556]\n",
      "epoch:20 step:16389 [D loss: 0.081130, acc.: 99.22%] [G loss: 1.898543]\n",
      "epoch:20 step:16390 [D loss: 0.067091, acc.: 100.00%] [G loss: 2.314135]\n",
      "epoch:20 step:16391 [D loss: 0.193078, acc.: 92.19%] [G loss: 1.186963]\n",
      "epoch:20 step:16392 [D loss: 0.250874, acc.: 91.41%] [G loss: 3.384042]\n",
      "epoch:20 step:16393 [D loss: 0.019923, acc.: 100.00%] [G loss: 4.227150]\n",
      "epoch:20 step:16394 [D loss: 0.122750, acc.: 94.53%] [G loss: 3.281030]\n",
      "epoch:20 step:16395 [D loss: 0.040969, acc.: 98.44%] [G loss: 2.244171]\n",
      "epoch:20 step:16396 [D loss: 0.061130, acc.: 99.22%] [G loss: 2.927518]\n",
      "epoch:20 step:16397 [D loss: 0.036154, acc.: 99.22%] [G loss: 4.538469]\n",
      "epoch:20 step:16398 [D loss: 0.045547, acc.: 100.00%] [G loss: 0.166525]\n",
      "epoch:20 step:16399 [D loss: 0.343594, acc.: 84.38%] [G loss: 1.317171]\n",
      "epoch:20 step:16400 [D loss: 0.650618, acc.: 65.62%] [G loss: 0.057010]\n",
      "epoch:20 step:16401 [D loss: 0.026584, acc.: 100.00%] [G loss: 3.147223]\n",
      "epoch:21 step:16402 [D loss: 0.122952, acc.: 95.31%] [G loss: 1.647061]\n",
      "epoch:21 step:16403 [D loss: 0.024725, acc.: 100.00%] [G loss: 0.004596]\n",
      "epoch:21 step:16404 [D loss: 0.029163, acc.: 100.00%] [G loss: 0.528577]\n",
      "epoch:21 step:16405 [D loss: 0.016426, acc.: 100.00%] [G loss: 0.299724]\n",
      "epoch:21 step:16406 [D loss: 0.003520, acc.: 100.00%] [G loss: 0.121116]\n",
      "epoch:21 step:16407 [D loss: 0.007994, acc.: 100.00%] [G loss: 0.268453]\n",
      "epoch:21 step:16408 [D loss: 0.006948, acc.: 100.00%] [G loss: 0.249259]\n",
      "epoch:21 step:16409 [D loss: 0.004480, acc.: 100.00%] [G loss: 0.117899]\n",
      "epoch:21 step:16410 [D loss: 0.002938, acc.: 100.00%] [G loss: 0.012446]\n",
      "epoch:21 step:16411 [D loss: 0.005840, acc.: 100.00%] [G loss: 0.003238]\n",
      "epoch:21 step:16412 [D loss: 0.004920, acc.: 100.00%] [G loss: 0.005139]\n",
      "epoch:21 step:16413 [D loss: 0.020983, acc.: 100.00%] [G loss: 0.444514]\n",
      "epoch:21 step:16414 [D loss: 0.008097, acc.: 100.00%] [G loss: 0.615002]\n",
      "epoch:21 step:16415 [D loss: 0.010939, acc.: 100.00%] [G loss: 0.029145]\n",
      "epoch:21 step:16416 [D loss: 0.007476, acc.: 100.00%] [G loss: 0.044977]\n",
      "epoch:21 step:16417 [D loss: 0.002712, acc.: 100.00%] [G loss: 0.023754]\n",
      "epoch:21 step:16418 [D loss: 0.003370, acc.: 100.00%] [G loss: 0.011270]\n",
      "epoch:21 step:16419 [D loss: 0.007191, acc.: 100.00%] [G loss: 0.023839]\n",
      "epoch:21 step:16420 [D loss: 0.011298, acc.: 100.00%] [G loss: 0.038047]\n",
      "epoch:21 step:16421 [D loss: 0.013355, acc.: 100.00%] [G loss: 0.013842]\n",
      "epoch:21 step:16422 [D loss: 0.011373, acc.: 100.00%] [G loss: 0.031306]\n",
      "epoch:21 step:16423 [D loss: 0.007136, acc.: 100.00%] [G loss: 0.016847]\n",
      "epoch:21 step:16424 [D loss: 0.012159, acc.: 100.00%] [G loss: 0.031719]\n",
      "epoch:21 step:16425 [D loss: 0.004372, acc.: 100.00%] [G loss: 0.008010]\n",
      "epoch:21 step:16426 [D loss: 0.005272, acc.: 100.00%] [G loss: 0.020043]\n",
      "epoch:21 step:16427 [D loss: 0.002411, acc.: 100.00%] [G loss: 0.013342]\n",
      "epoch:21 step:16428 [D loss: 0.002259, acc.: 100.00%] [G loss: 0.018605]\n",
      "epoch:21 step:16429 [D loss: 0.008017, acc.: 100.00%] [G loss: 0.009390]\n",
      "epoch:21 step:16430 [D loss: 0.006745, acc.: 100.00%] [G loss: 0.019610]\n",
      "epoch:21 step:16431 [D loss: 0.019025, acc.: 100.00%] [G loss: 0.603355]\n",
      "epoch:21 step:16432 [D loss: 0.327929, acc.: 81.25%] [G loss: 1.495745]\n",
      "epoch:21 step:16433 [D loss: 0.222192, acc.: 89.84%] [G loss: 0.809975]\n",
      "epoch:21 step:16434 [D loss: 0.168518, acc.: 92.19%] [G loss: 0.065637]\n",
      "epoch:21 step:16435 [D loss: 0.007435, acc.: 100.00%] [G loss: 0.082516]\n",
      "epoch:21 step:16436 [D loss: 0.017580, acc.: 100.00%] [G loss: 0.001227]\n",
      "epoch:21 step:16437 [D loss: 0.006829, acc.: 100.00%] [G loss: 0.031064]\n",
      "epoch:21 step:16438 [D loss: 0.006557, acc.: 100.00%] [G loss: 0.000493]\n",
      "epoch:21 step:16439 [D loss: 0.002190, acc.: 100.00%] [G loss: 0.000377]\n",
      "epoch:21 step:16440 [D loss: 0.003980, acc.: 100.00%] [G loss: 0.021051]\n",
      "epoch:21 step:16441 [D loss: 0.002011, acc.: 100.00%] [G loss: 0.001160]\n",
      "epoch:21 step:16442 [D loss: 0.004993, acc.: 100.00%] [G loss: 0.582072]\n",
      "epoch:21 step:16443 [D loss: 0.007522, acc.: 100.00%] [G loss: 0.008530]\n",
      "epoch:21 step:16444 [D loss: 0.004650, acc.: 100.00%] [G loss: 0.027174]\n",
      "epoch:21 step:16445 [D loss: 0.020346, acc.: 100.00%] [G loss: 0.034884]\n",
      "epoch:21 step:16446 [D loss: 0.001721, acc.: 100.00%] [G loss: 0.004507]\n",
      "epoch:21 step:16447 [D loss: 0.000973, acc.: 100.00%] [G loss: 0.017106]\n",
      "epoch:21 step:16448 [D loss: 0.014477, acc.: 100.00%] [G loss: 0.003330]\n",
      "epoch:21 step:16449 [D loss: 0.000510, acc.: 100.00%] [G loss: 0.005647]\n",
      "epoch:21 step:16450 [D loss: 0.001420, acc.: 100.00%] [G loss: 0.000644]\n",
      "epoch:21 step:16451 [D loss: 0.001653, acc.: 100.00%] [G loss: 0.002908]\n",
      "epoch:21 step:16452 [D loss: 0.006608, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:21 step:16453 [D loss: 0.003579, acc.: 100.00%] [G loss: 0.000585]\n",
      "epoch:21 step:16454 [D loss: 0.002592, acc.: 100.00%] [G loss: 0.016504]\n",
      "epoch:21 step:16455 [D loss: 0.006820, acc.: 100.00%] [G loss: 0.004298]\n",
      "epoch:21 step:16456 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.003998]\n",
      "epoch:21 step:16457 [D loss: 0.001628, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:21 step:16458 [D loss: 0.002406, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:21 step:16459 [D loss: 0.011996, acc.: 100.00%] [G loss: 0.035267]\n",
      "epoch:21 step:16460 [D loss: 0.003507, acc.: 100.00%] [G loss: 0.006661]\n",
      "epoch:21 step:16461 [D loss: 0.001144, acc.: 100.00%] [G loss: 0.002610]\n",
      "epoch:21 step:16462 [D loss: 0.001335, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:21 step:16463 [D loss: 0.000719, acc.: 100.00%] [G loss: 0.005923]\n",
      "epoch:21 step:16464 [D loss: 0.001538, acc.: 100.00%] [G loss: 0.000528]\n",
      "epoch:21 step:16465 [D loss: 0.002271, acc.: 100.00%] [G loss: 0.002132]\n",
      "epoch:21 step:16466 [D loss: 0.009568, acc.: 100.00%] [G loss: 0.002687]\n",
      "epoch:21 step:16467 [D loss: 0.002096, acc.: 100.00%] [G loss: 0.002122]\n",
      "epoch:21 step:16468 [D loss: 0.000601, acc.: 100.00%] [G loss: 0.004053]\n",
      "epoch:21 step:16469 [D loss: 0.002188, acc.: 100.00%] [G loss: 0.016256]\n",
      "epoch:21 step:16470 [D loss: 0.000915, acc.: 100.00%] [G loss: 0.006717]\n",
      "epoch:21 step:16471 [D loss: 0.002752, acc.: 100.00%] [G loss: 0.010718]\n",
      "epoch:21 step:16472 [D loss: 0.010357, acc.: 100.00%] [G loss: 0.009017]\n",
      "epoch:21 step:16473 [D loss: 0.071416, acc.: 98.44%] [G loss: 0.004682]\n",
      "epoch:21 step:16474 [D loss: 0.000887, acc.: 100.00%] [G loss: 0.015701]\n",
      "epoch:21 step:16475 [D loss: 0.013928, acc.: 100.00%] [G loss: 0.014286]\n",
      "epoch:21 step:16476 [D loss: 0.001093, acc.: 100.00%] [G loss: 0.036827]\n",
      "epoch:21 step:16477 [D loss: 0.003853, acc.: 100.00%] [G loss: 0.043369]\n",
      "epoch:21 step:16478 [D loss: 0.068619, acc.: 98.44%] [G loss: 0.000187]\n",
      "epoch:21 step:16479 [D loss: 0.002884, acc.: 100.00%] [G loss: 0.000402]\n",
      "epoch:21 step:16480 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.000452]\n",
      "epoch:21 step:16481 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.001795]\n",
      "epoch:21 step:16482 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.003006]\n",
      "epoch:21 step:16483 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:21 step:16484 [D loss: 0.001237, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:21 step:16485 [D loss: 0.002653, acc.: 100.00%] [G loss: 0.000641]\n",
      "epoch:21 step:16486 [D loss: 0.006655, acc.: 100.00%] [G loss: 0.001490]\n",
      "epoch:21 step:16487 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.033393]\n",
      "epoch:21 step:16488 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.001031]\n",
      "epoch:21 step:16489 [D loss: 0.000779, acc.: 100.00%] [G loss: 0.001885]\n",
      "epoch:21 step:16490 [D loss: 0.002487, acc.: 100.00%] [G loss: 0.003237]\n",
      "epoch:21 step:16491 [D loss: 0.003244, acc.: 100.00%] [G loss: 0.008146]\n",
      "epoch:21 step:16492 [D loss: 0.000877, acc.: 100.00%] [G loss: 0.118238]\n",
      "epoch:21 step:16493 [D loss: 0.003165, acc.: 100.00%] [G loss: 0.003718]\n",
      "epoch:21 step:16494 [D loss: 0.001038, acc.: 100.00%] [G loss: 0.003762]\n",
      "epoch:21 step:16495 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.001645]\n",
      "epoch:21 step:16496 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.014542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16497 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.002157]\n",
      "epoch:21 step:16498 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.001835]\n",
      "epoch:21 step:16499 [D loss: 0.001067, acc.: 100.00%] [G loss: 0.001853]\n",
      "epoch:21 step:16500 [D loss: 0.001832, acc.: 100.00%] [G loss: 0.001338]\n",
      "epoch:21 step:16501 [D loss: 0.001729, acc.: 100.00%] [G loss: 0.006820]\n",
      "epoch:21 step:16502 [D loss: 0.003093, acc.: 100.00%] [G loss: 0.002934]\n",
      "epoch:21 step:16503 [D loss: 0.069368, acc.: 100.00%] [G loss: 0.010285]\n",
      "epoch:21 step:16504 [D loss: 0.033767, acc.: 100.00%] [G loss: 0.026478]\n",
      "epoch:21 step:16505 [D loss: 0.016928, acc.: 100.00%] [G loss: 0.037953]\n",
      "epoch:21 step:16506 [D loss: 0.097115, acc.: 98.44%] [G loss: 0.052665]\n",
      "epoch:21 step:16507 [D loss: 0.029928, acc.: 98.44%] [G loss: 0.788696]\n",
      "epoch:21 step:16508 [D loss: 0.118891, acc.: 93.75%] [G loss: 0.373637]\n",
      "epoch:21 step:16509 [D loss: 0.000903, acc.: 100.00%] [G loss: 0.000566]\n",
      "epoch:21 step:16510 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.177496]\n",
      "epoch:21 step:16511 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.150661]\n",
      "epoch:21 step:16512 [D loss: 0.009044, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:21 step:16513 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.039860]\n",
      "epoch:21 step:16514 [D loss: 0.001032, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:21 step:16515 [D loss: 0.001620, acc.: 100.00%] [G loss: 0.048244]\n",
      "epoch:21 step:16516 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.041708]\n",
      "epoch:21 step:16517 [D loss: 0.002466, acc.: 100.00%] [G loss: 0.038586]\n",
      "epoch:21 step:16518 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.016233]\n",
      "epoch:21 step:16519 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.050038]\n",
      "epoch:21 step:16520 [D loss: 0.002474, acc.: 100.00%] [G loss: 0.015252]\n",
      "epoch:21 step:16521 [D loss: 0.001257, acc.: 100.00%] [G loss: 0.023779]\n",
      "epoch:21 step:16522 [D loss: 0.003939, acc.: 100.00%] [G loss: 0.028067]\n",
      "epoch:21 step:16523 [D loss: 0.006277, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:21 step:16524 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.016574]\n",
      "epoch:21 step:16525 [D loss: 0.001992, acc.: 100.00%] [G loss: 0.010237]\n",
      "epoch:21 step:16526 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.013135]\n",
      "epoch:21 step:16527 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:21 step:16528 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:21 step:16529 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.009148]\n",
      "epoch:21 step:16530 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:21 step:16531 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.010660]\n",
      "epoch:21 step:16532 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.001849]\n",
      "epoch:21 step:16533 [D loss: 0.002227, acc.: 100.00%] [G loss: 0.006713]\n",
      "epoch:21 step:16534 [D loss: 0.000300, acc.: 100.00%] [G loss: 1.201356]\n",
      "epoch:21 step:16535 [D loss: 0.008139, acc.: 100.00%] [G loss: 0.014519]\n",
      "epoch:21 step:16536 [D loss: 0.002507, acc.: 100.00%] [G loss: 0.002201]\n",
      "epoch:21 step:16537 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.003650]\n",
      "epoch:21 step:16538 [D loss: 0.000617, acc.: 100.00%] [G loss: 0.002214]\n",
      "epoch:21 step:16539 [D loss: 0.001825, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:21 step:16540 [D loss: 0.003228, acc.: 100.00%] [G loss: 0.001688]\n",
      "epoch:21 step:16541 [D loss: 0.006529, acc.: 100.00%] [G loss: 0.004570]\n",
      "epoch:21 step:16542 [D loss: 0.005096, acc.: 100.00%] [G loss: 0.005689]\n",
      "epoch:21 step:16543 [D loss: 0.030469, acc.: 100.00%] [G loss: 0.004665]\n",
      "epoch:21 step:16544 [D loss: 0.024840, acc.: 100.00%] [G loss: 0.014023]\n",
      "epoch:21 step:16545 [D loss: 0.002543, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:21 step:16546 [D loss: 0.002582, acc.: 100.00%] [G loss: 0.009326]\n",
      "epoch:21 step:16547 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.004222]\n",
      "epoch:21 step:16548 [D loss: 0.004702, acc.: 100.00%] [G loss: 0.005205]\n",
      "epoch:21 step:16549 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.003261]\n",
      "epoch:21 step:16550 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.004656]\n",
      "epoch:21 step:16551 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.001480]\n",
      "epoch:21 step:16552 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:21 step:16553 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.003641]\n",
      "epoch:21 step:16554 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.007695]\n",
      "epoch:21 step:16555 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000225]\n",
      "epoch:21 step:16556 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000895]\n",
      "epoch:21 step:16557 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.004092]\n",
      "epoch:21 step:16558 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.003494]\n",
      "epoch:21 step:16559 [D loss: 0.000643, acc.: 100.00%] [G loss: 0.004645]\n",
      "epoch:21 step:16560 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.013761]\n",
      "epoch:21 step:16561 [D loss: 0.001321, acc.: 100.00%] [G loss: 0.238054]\n",
      "epoch:21 step:16562 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.001175]\n",
      "epoch:21 step:16563 [D loss: 0.000416, acc.: 100.00%] [G loss: 0.000702]\n",
      "epoch:21 step:16564 [D loss: 0.000849, acc.: 100.00%] [G loss: 0.001590]\n",
      "epoch:21 step:16565 [D loss: 0.018783, acc.: 100.00%] [G loss: 0.011194]\n",
      "epoch:21 step:16566 [D loss: 0.007037, acc.: 100.00%] [G loss: 0.012327]\n",
      "epoch:21 step:16567 [D loss: 0.027957, acc.: 100.00%] [G loss: 0.028672]\n",
      "epoch:21 step:16568 [D loss: 0.012163, acc.: 100.00%] [G loss: 0.055082]\n",
      "epoch:21 step:16569 [D loss: 0.050822, acc.: 98.44%] [G loss: 0.545539]\n",
      "epoch:21 step:16570 [D loss: 0.038531, acc.: 100.00%] [G loss: 0.938256]\n",
      "epoch:21 step:16571 [D loss: 0.318423, acc.: 89.06%] [G loss: 5.597050]\n",
      "epoch:21 step:16572 [D loss: 0.069948, acc.: 96.88%] [G loss: 6.168147]\n",
      "epoch:21 step:16573 [D loss: 0.003678, acc.: 100.00%] [G loss: 6.092374]\n",
      "epoch:21 step:16574 [D loss: 0.037075, acc.: 99.22%] [G loss: 0.408651]\n",
      "epoch:21 step:16575 [D loss: 0.056869, acc.: 97.66%] [G loss: 5.842783]\n",
      "epoch:21 step:16576 [D loss: 0.096656, acc.: 96.09%] [G loss: 6.911106]\n",
      "epoch:21 step:16577 [D loss: 0.074512, acc.: 98.44%] [G loss: 0.084428]\n",
      "epoch:21 step:16578 [D loss: 0.014170, acc.: 100.00%] [G loss: 5.061511]\n",
      "epoch:21 step:16579 [D loss: 0.051217, acc.: 97.66%] [G loss: 3.238997]\n",
      "epoch:21 step:16580 [D loss: 0.019725, acc.: 99.22%] [G loss: 0.000756]\n",
      "epoch:21 step:16581 [D loss: 0.029497, acc.: 99.22%] [G loss: 1.188832]\n",
      "epoch:21 step:16582 [D loss: 0.010077, acc.: 100.00%] [G loss: 1.177429]\n",
      "epoch:21 step:16583 [D loss: 0.033143, acc.: 99.22%] [G loss: 0.007350]\n",
      "epoch:21 step:16584 [D loss: 0.000311, acc.: 100.00%] [G loss: 1.965270]\n",
      "epoch:21 step:16585 [D loss: 0.009437, acc.: 100.00%] [G loss: 0.604015]\n",
      "epoch:21 step:16586 [D loss: 0.001434, acc.: 100.00%] [G loss: 0.694076]\n",
      "epoch:21 step:16587 [D loss: 0.132472, acc.: 95.31%] [G loss: 2.389545]\n",
      "epoch:21 step:16588 [D loss: 0.281598, acc.: 85.94%] [G loss: 0.504256]\n",
      "epoch:21 step:16589 [D loss: 0.019603, acc.: 100.00%] [G loss: 0.003447]\n",
      "epoch:21 step:16590 [D loss: 0.005152, acc.: 100.00%] [G loss: 0.577965]\n",
      "epoch:21 step:16591 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.053836]\n",
      "epoch:21 step:16592 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.108422]\n",
      "epoch:21 step:16593 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.022174]\n",
      "epoch:21 step:16594 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.066472]\n",
      "epoch:21 step:16595 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.629666]\n",
      "epoch:21 step:16596 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.012416]\n",
      "epoch:21 step:16597 [D loss: 0.000856, acc.: 100.00%] [G loss: 0.203025]\n",
      "epoch:21 step:16598 [D loss: 0.003108, acc.: 100.00%] [G loss: 0.010007]\n",
      "epoch:21 step:16599 [D loss: 0.001910, acc.: 100.00%] [G loss: 0.008734]\n",
      "epoch:21 step:16600 [D loss: 0.011086, acc.: 100.00%] [G loss: 0.005725]\n",
      "epoch:21 step:16601 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.025282]\n",
      "epoch:21 step:16602 [D loss: 0.004047, acc.: 100.00%] [G loss: 0.239013]\n",
      "epoch:21 step:16603 [D loss: 0.000979, acc.: 100.00%] [G loss: 0.001918]\n",
      "epoch:21 step:16604 [D loss: 0.010099, acc.: 100.00%] [G loss: 0.004693]\n",
      "epoch:21 step:16605 [D loss: 0.011403, acc.: 100.00%] [G loss: 0.008552]\n",
      "epoch:21 step:16606 [D loss: 0.002019, acc.: 100.00%] [G loss: 0.036891]\n",
      "epoch:21 step:16607 [D loss: 0.001762, acc.: 100.00%] [G loss: 0.000455]\n",
      "epoch:21 step:16608 [D loss: 0.002347, acc.: 100.00%] [G loss: 0.017377]\n",
      "epoch:21 step:16609 [D loss: 0.225681, acc.: 90.62%] [G loss: 2.113372]\n",
      "epoch:21 step:16610 [D loss: 0.541272, acc.: 78.12%] [G loss: 3.629778]\n",
      "epoch:21 step:16611 [D loss: 0.051027, acc.: 98.44%] [G loss: 2.909060]\n",
      "epoch:21 step:16612 [D loss: 0.096971, acc.: 95.31%] [G loss: 3.741258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16613 [D loss: 0.001188, acc.: 100.00%] [G loss: 3.142242]\n",
      "epoch:21 step:16614 [D loss: 0.019812, acc.: 99.22%] [G loss: 2.968330]\n",
      "epoch:21 step:16615 [D loss: 0.054474, acc.: 97.66%] [G loss: 2.302688]\n",
      "epoch:21 step:16616 [D loss: 0.238665, acc.: 89.06%] [G loss: 3.469736]\n",
      "epoch:21 step:16617 [D loss: 0.338667, acc.: 82.81%] [G loss: 1.735170]\n",
      "epoch:21 step:16618 [D loss: 0.001190, acc.: 100.00%] [G loss: 0.009034]\n",
      "epoch:21 step:16619 [D loss: 0.005796, acc.: 100.00%] [G loss: 0.013035]\n",
      "epoch:21 step:16620 [D loss: 0.003272, acc.: 100.00%] [G loss: 0.001327]\n",
      "epoch:21 step:16621 [D loss: 0.026872, acc.: 99.22%] [G loss: 0.808969]\n",
      "epoch:21 step:16622 [D loss: 0.000965, acc.: 100.00%] [G loss: 0.350520]\n",
      "epoch:21 step:16623 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.879490]\n",
      "epoch:21 step:16624 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.266521]\n",
      "epoch:21 step:16625 [D loss: 0.000984, acc.: 100.00%] [G loss: 0.189476]\n",
      "epoch:21 step:16626 [D loss: 0.000716, acc.: 100.00%] [G loss: 0.026903]\n",
      "epoch:21 step:16627 [D loss: 0.002445, acc.: 100.00%] [G loss: 0.034881]\n",
      "epoch:21 step:16628 [D loss: 0.000657, acc.: 100.00%] [G loss: 0.003824]\n",
      "epoch:21 step:16629 [D loss: 0.002951, acc.: 100.00%] [G loss: 0.045023]\n",
      "epoch:21 step:16630 [D loss: 0.001429, acc.: 100.00%] [G loss: 0.034669]\n",
      "epoch:21 step:16631 [D loss: 0.013488, acc.: 99.22%] [G loss: 0.012608]\n",
      "epoch:21 step:16632 [D loss: 0.004432, acc.: 100.00%] [G loss: 0.004822]\n",
      "epoch:21 step:16633 [D loss: 0.003587, acc.: 100.00%] [G loss: 0.007768]\n",
      "epoch:21 step:16634 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:21 step:16635 [D loss: 0.002698, acc.: 100.00%] [G loss: 0.003652]\n",
      "epoch:21 step:16636 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.008009]\n",
      "epoch:21 step:16637 [D loss: 0.012963, acc.: 99.22%] [G loss: 0.001713]\n",
      "epoch:21 step:16638 [D loss: 0.000890, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:21 step:16639 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.002409]\n",
      "epoch:21 step:16640 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.007829]\n",
      "epoch:21 step:16641 [D loss: 0.000719, acc.: 100.00%] [G loss: 0.002069]\n",
      "epoch:21 step:16642 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:21 step:16643 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000996]\n",
      "epoch:21 step:16644 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:21 step:16645 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000194]\n",
      "epoch:21 step:16646 [D loss: 0.002154, acc.: 100.00%] [G loss: 0.004526]\n",
      "epoch:21 step:16647 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.012516]\n",
      "epoch:21 step:16648 [D loss: 0.001051, acc.: 100.00%] [G loss: 0.020408]\n",
      "epoch:21 step:16649 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.002259]\n",
      "epoch:21 step:16650 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:21 step:16651 [D loss: 0.001537, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:21 step:16652 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.002429]\n",
      "epoch:21 step:16653 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.001031]\n",
      "epoch:21 step:16654 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.001159]\n",
      "epoch:21 step:16655 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000553]\n",
      "epoch:21 step:16656 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.000500]\n",
      "epoch:21 step:16657 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.001063]\n",
      "epoch:21 step:16658 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:21 step:16659 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.001123]\n",
      "epoch:21 step:16660 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000630]\n",
      "epoch:21 step:16661 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.243842]\n",
      "epoch:21 step:16662 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.004421]\n",
      "epoch:21 step:16663 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.004968]\n",
      "epoch:21 step:16664 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000471]\n",
      "epoch:21 step:16665 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:21 step:16666 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.013819]\n",
      "epoch:21 step:16667 [D loss: 0.018716, acc.: 99.22%] [G loss: 0.002115]\n",
      "epoch:21 step:16668 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:21 step:16669 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.010180]\n",
      "epoch:21 step:16670 [D loss: 0.001400, acc.: 100.00%] [G loss: 0.018802]\n",
      "epoch:21 step:16671 [D loss: 0.000751, acc.: 100.00%] [G loss: 0.001663]\n",
      "epoch:21 step:16672 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:21 step:16673 [D loss: 0.001174, acc.: 100.00%] [G loss: 0.001806]\n",
      "epoch:21 step:16674 [D loss: 0.085438, acc.: 96.88%] [G loss: 0.020640]\n",
      "epoch:21 step:16675 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.176674]\n",
      "epoch:21 step:16676 [D loss: 0.035042, acc.: 98.44%] [G loss: 0.004995]\n",
      "epoch:21 step:16677 [D loss: 0.000805, acc.: 100.00%] [G loss: 0.013248]\n",
      "epoch:21 step:16678 [D loss: 0.254297, acc.: 88.28%] [G loss: 0.000410]\n",
      "epoch:21 step:16679 [D loss: 0.029241, acc.: 99.22%] [G loss: 0.000882]\n",
      "epoch:21 step:16680 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.002365]\n",
      "epoch:21 step:16681 [D loss: 0.096992, acc.: 96.88%] [G loss: 0.000924]\n",
      "epoch:21 step:16682 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.019782]\n",
      "epoch:21 step:16683 [D loss: 0.000765, acc.: 100.00%] [G loss: 0.055361]\n",
      "epoch:21 step:16684 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.015585]\n",
      "epoch:21 step:16685 [D loss: 0.004945, acc.: 100.00%] [G loss: 0.030091]\n",
      "epoch:21 step:16686 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.006855]\n",
      "epoch:21 step:16687 [D loss: 0.003654, acc.: 100.00%] [G loss: 0.706559]\n",
      "epoch:21 step:16688 [D loss: 0.144208, acc.: 90.62%] [G loss: 2.921555]\n",
      "epoch:21 step:16689 [D loss: 0.013438, acc.: 100.00%] [G loss: 5.622713]\n",
      "epoch:21 step:16690 [D loss: 0.024046, acc.: 100.00%] [G loss: 5.112360]\n",
      "epoch:21 step:16691 [D loss: 0.006543, acc.: 100.00%] [G loss: 4.823699]\n",
      "epoch:21 step:16692 [D loss: 0.007984, acc.: 100.00%] [G loss: 3.911317]\n",
      "epoch:21 step:16693 [D loss: 0.035462, acc.: 99.22%] [G loss: 3.002665]\n",
      "epoch:21 step:16694 [D loss: 0.006363, acc.: 100.00%] [G loss: 1.022843]\n",
      "epoch:21 step:16695 [D loss: 0.004824, acc.: 100.00%] [G loss: 2.266879]\n",
      "epoch:21 step:16696 [D loss: 0.032607, acc.: 99.22%] [G loss: 1.828354]\n",
      "epoch:21 step:16697 [D loss: 0.005645, acc.: 100.00%] [G loss: 1.854327]\n",
      "epoch:21 step:16698 [D loss: 0.012328, acc.: 100.00%] [G loss: 5.462297]\n",
      "epoch:21 step:16699 [D loss: 0.036820, acc.: 99.22%] [G loss: 3.113821]\n",
      "epoch:21 step:16700 [D loss: 0.010774, acc.: 100.00%] [G loss: 3.457888]\n",
      "epoch:21 step:16701 [D loss: 0.120949, acc.: 96.09%] [G loss: 2.151300]\n",
      "epoch:21 step:16702 [D loss: 0.005980, acc.: 100.00%] [G loss: 0.013312]\n",
      "epoch:21 step:16703 [D loss: 0.004875, acc.: 100.00%] [G loss: 1.690234]\n",
      "epoch:21 step:16704 [D loss: 0.218819, acc.: 89.06%] [G loss: 7.133408]\n",
      "epoch:21 step:16705 [D loss: 0.162744, acc.: 92.19%] [G loss: 1.799785]\n",
      "epoch:21 step:16706 [D loss: 0.534552, acc.: 78.12%] [G loss: 0.552612]\n",
      "epoch:21 step:16707 [D loss: 0.243909, acc.: 92.97%] [G loss: 0.306236]\n",
      "epoch:21 step:16708 [D loss: 0.013909, acc.: 100.00%] [G loss: 0.538263]\n",
      "epoch:21 step:16709 [D loss: 0.002878, acc.: 100.00%] [G loss: 0.396004]\n",
      "epoch:21 step:16710 [D loss: 0.006906, acc.: 100.00%] [G loss: 0.031356]\n",
      "epoch:21 step:16711 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.394773]\n",
      "epoch:21 step:16712 [D loss: 0.000828, acc.: 100.00%] [G loss: 0.032481]\n",
      "epoch:21 step:16713 [D loss: 0.000614, acc.: 100.00%] [G loss: 0.307680]\n",
      "epoch:21 step:16714 [D loss: 0.007116, acc.: 100.00%] [G loss: 0.069059]\n",
      "epoch:21 step:16715 [D loss: 0.006637, acc.: 100.00%] [G loss: 0.037068]\n",
      "epoch:21 step:16716 [D loss: 0.151121, acc.: 93.75%] [G loss: 0.002455]\n",
      "epoch:21 step:16717 [D loss: 0.172445, acc.: 96.09%] [G loss: 0.090798]\n",
      "epoch:21 step:16718 [D loss: 0.000512, acc.: 100.00%] [G loss: 0.427805]\n",
      "epoch:21 step:16719 [D loss: 0.000952, acc.: 100.00%] [G loss: 0.237165]\n",
      "epoch:21 step:16720 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.406846]\n",
      "epoch:21 step:16721 [D loss: 0.006751, acc.: 100.00%] [G loss: 0.329846]\n",
      "epoch:21 step:16722 [D loss: 0.004579, acc.: 100.00%] [G loss: 0.172681]\n",
      "epoch:21 step:16723 [D loss: 0.003499, acc.: 100.00%] [G loss: 0.041430]\n",
      "epoch:21 step:16724 [D loss: 0.002135, acc.: 100.00%] [G loss: 0.026877]\n",
      "epoch:21 step:16725 [D loss: 0.004155, acc.: 100.00%] [G loss: 0.033801]\n",
      "epoch:21 step:16726 [D loss: 0.004576, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:21 step:16727 [D loss: 0.000755, acc.: 100.00%] [G loss: 0.065544]\n",
      "epoch:21 step:16728 [D loss: 0.004761, acc.: 100.00%] [G loss: 0.101095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16729 [D loss: 0.002686, acc.: 100.00%] [G loss: 3.438653]\n",
      "epoch:21 step:16730 [D loss: 0.111938, acc.: 94.53%] [G loss: 2.552154]\n",
      "epoch:21 step:16731 [D loss: 0.015464, acc.: 99.22%] [G loss: 3.773141]\n",
      "epoch:21 step:16732 [D loss: 0.080347, acc.: 96.09%] [G loss: 4.986536]\n",
      "epoch:21 step:16733 [D loss: 0.043800, acc.: 97.66%] [G loss: 5.529508]\n",
      "epoch:21 step:16734 [D loss: 0.004730, acc.: 100.00%] [G loss: 4.448046]\n",
      "epoch:21 step:16735 [D loss: 0.018393, acc.: 100.00%] [G loss: 4.525761]\n",
      "epoch:21 step:16736 [D loss: 0.019337, acc.: 100.00%] [G loss: 0.084243]\n",
      "epoch:21 step:16737 [D loss: 0.038716, acc.: 100.00%] [G loss: 7.632165]\n",
      "epoch:21 step:16738 [D loss: 0.003882, acc.: 100.00%] [G loss: 6.552022]\n",
      "epoch:21 step:16739 [D loss: 0.132381, acc.: 94.53%] [G loss: 4.297792]\n",
      "epoch:21 step:16740 [D loss: 0.018235, acc.: 100.00%] [G loss: 3.691936]\n",
      "epoch:21 step:16741 [D loss: 0.001723, acc.: 100.00%] [G loss: 4.894626]\n",
      "epoch:21 step:16742 [D loss: 0.003487, acc.: 100.00%] [G loss: 4.343916]\n",
      "epoch:21 step:16743 [D loss: 0.006056, acc.: 100.00%] [G loss: 3.757695]\n",
      "epoch:21 step:16744 [D loss: 0.000986, acc.: 100.00%] [G loss: 3.503048]\n",
      "epoch:21 step:16745 [D loss: 0.003007, acc.: 100.00%] [G loss: 6.058016]\n",
      "epoch:21 step:16746 [D loss: 0.003908, acc.: 100.00%] [G loss: 4.832359]\n",
      "epoch:21 step:16747 [D loss: 0.004524, acc.: 100.00%] [G loss: 3.986099]\n",
      "epoch:21 step:16748 [D loss: 0.007581, acc.: 100.00%] [G loss: 5.140533]\n",
      "epoch:21 step:16749 [D loss: 0.001320, acc.: 100.00%] [G loss: 4.927902]\n",
      "epoch:21 step:16750 [D loss: 0.002230, acc.: 100.00%] [G loss: 4.932501]\n",
      "epoch:21 step:16751 [D loss: 0.001959, acc.: 100.00%] [G loss: 5.217258]\n",
      "epoch:21 step:16752 [D loss: 0.016381, acc.: 100.00%] [G loss: 6.590641]\n",
      "epoch:21 step:16753 [D loss: 0.001017, acc.: 100.00%] [G loss: 7.352290]\n",
      "epoch:21 step:16754 [D loss: 0.000782, acc.: 100.00%] [G loss: 6.893156]\n",
      "epoch:21 step:16755 [D loss: 0.002425, acc.: 100.00%] [G loss: 6.778199]\n",
      "epoch:21 step:16756 [D loss: 0.011318, acc.: 100.00%] [G loss: 6.490808]\n",
      "epoch:21 step:16757 [D loss: 0.001588, acc.: 100.00%] [G loss: 9.118198]\n",
      "epoch:21 step:16758 [D loss: 0.001698, acc.: 100.00%] [G loss: 6.173269]\n",
      "epoch:21 step:16759 [D loss: 0.008531, acc.: 100.00%] [G loss: 5.980678]\n",
      "epoch:21 step:16760 [D loss: 0.003755, acc.: 100.00%] [G loss: 6.970648]\n",
      "epoch:21 step:16761 [D loss: 0.003404, acc.: 100.00%] [G loss: 6.556752]\n",
      "epoch:21 step:16762 [D loss: 0.003177, acc.: 100.00%] [G loss: 7.823837]\n",
      "epoch:21 step:16763 [D loss: 0.001154, acc.: 100.00%] [G loss: 6.413538]\n",
      "epoch:21 step:16764 [D loss: 0.004486, acc.: 100.00%] [G loss: 6.734605]\n",
      "epoch:21 step:16765 [D loss: 0.002360, acc.: 100.00%] [G loss: 12.450844]\n",
      "epoch:21 step:16766 [D loss: 0.000758, acc.: 100.00%] [G loss: 6.421735]\n",
      "epoch:21 step:16767 [D loss: 0.005054, acc.: 100.00%] [G loss: 7.138355]\n",
      "epoch:21 step:16768 [D loss: 0.007802, acc.: 100.00%] [G loss: 6.818333]\n",
      "epoch:21 step:16769 [D loss: 0.000476, acc.: 100.00%] [G loss: 6.968878]\n",
      "epoch:21 step:16770 [D loss: 0.000485, acc.: 100.00%] [G loss: 11.325085]\n",
      "epoch:21 step:16771 [D loss: 0.001328, acc.: 100.00%] [G loss: 6.791113]\n",
      "epoch:21 step:16772 [D loss: 0.001764, acc.: 100.00%] [G loss: 7.373356]\n",
      "epoch:21 step:16773 [D loss: 0.002334, acc.: 100.00%] [G loss: 7.111631]\n",
      "epoch:21 step:16774 [D loss: 0.009223, acc.: 100.00%] [G loss: 6.006645]\n",
      "epoch:21 step:16775 [D loss: 0.005673, acc.: 100.00%] [G loss: 9.846044]\n",
      "epoch:21 step:16776 [D loss: 0.000426, acc.: 100.00%] [G loss: 8.370190]\n",
      "epoch:21 step:16777 [D loss: 0.000796, acc.: 100.00%] [G loss: 8.105178]\n",
      "epoch:21 step:16778 [D loss: 0.000622, acc.: 100.00%] [G loss: 7.954959]\n",
      "epoch:21 step:16779 [D loss: 0.000438, acc.: 100.00%] [G loss: 7.902004]\n",
      "epoch:21 step:16780 [D loss: 0.000769, acc.: 100.00%] [G loss: 7.939163]\n",
      "epoch:21 step:16781 [D loss: 0.000579, acc.: 100.00%] [G loss: 7.245829]\n",
      "epoch:21 step:16782 [D loss: 0.001631, acc.: 100.00%] [G loss: 7.155359]\n",
      "epoch:21 step:16783 [D loss: 0.001927, acc.: 100.00%] [G loss: 7.390372]\n",
      "epoch:21 step:16784 [D loss: 0.007924, acc.: 100.00%] [G loss: 8.428455]\n",
      "epoch:21 step:16785 [D loss: 0.001241, acc.: 100.00%] [G loss: 9.839861]\n",
      "epoch:21 step:16786 [D loss: 0.003702, acc.: 100.00%] [G loss: 9.149536]\n",
      "epoch:21 step:16787 [D loss: 0.000546, acc.: 100.00%] [G loss: 8.876442]\n",
      "epoch:21 step:16788 [D loss: 0.000646, acc.: 100.00%] [G loss: 9.284033]\n",
      "epoch:21 step:16789 [D loss: 0.000099, acc.: 100.00%] [G loss: 8.405058]\n",
      "epoch:21 step:16790 [D loss: 0.000420, acc.: 100.00%] [G loss: 8.434472]\n",
      "epoch:21 step:16791 [D loss: 0.000071, acc.: 100.00%] [G loss: 8.621863]\n",
      "epoch:21 step:16792 [D loss: 0.000391, acc.: 100.00%] [G loss: 7.886496]\n",
      "epoch:21 step:16793 [D loss: 0.002784, acc.: 100.00%] [G loss: 6.950425]\n",
      "epoch:21 step:16794 [D loss: 0.005118, acc.: 100.00%] [G loss: 3.485212]\n",
      "epoch:21 step:16795 [D loss: 0.000457, acc.: 100.00%] [G loss: 8.921494]\n",
      "epoch:21 step:16796 [D loss: 0.018808, acc.: 99.22%] [G loss: 7.245762]\n",
      "epoch:21 step:16797 [D loss: 0.002536, acc.: 100.00%] [G loss: 7.230725]\n",
      "epoch:21 step:16798 [D loss: 0.002444, acc.: 100.00%] [G loss: 8.185077]\n",
      "epoch:21 step:16799 [D loss: 0.001066, acc.: 100.00%] [G loss: 8.386780]\n",
      "epoch:21 step:16800 [D loss: 0.000846, acc.: 100.00%] [G loss: 8.936813]\n",
      "epoch:21 step:16801 [D loss: 0.001803, acc.: 100.00%] [G loss: 7.679917]\n",
      "epoch:21 step:16802 [D loss: 0.001786, acc.: 100.00%] [G loss: 7.849905]\n",
      "epoch:21 step:16803 [D loss: 0.003183, acc.: 100.00%] [G loss: 8.189850]\n",
      "epoch:21 step:16804 [D loss: 0.003003, acc.: 100.00%] [G loss: 9.974726]\n",
      "epoch:21 step:16805 [D loss: 0.001439, acc.: 100.00%] [G loss: 2.284584]\n",
      "epoch:21 step:16806 [D loss: 0.014716, acc.: 100.00%] [G loss: 11.162687]\n",
      "epoch:21 step:16807 [D loss: 0.002007, acc.: 100.00%] [G loss: 9.940447]\n",
      "epoch:21 step:16808 [D loss: 0.002846, acc.: 100.00%] [G loss: 7.761231]\n",
      "epoch:21 step:16809 [D loss: 0.044038, acc.: 99.22%] [G loss: 8.598547]\n",
      "epoch:21 step:16810 [D loss: 0.002003, acc.: 100.00%] [G loss: 8.488193]\n",
      "epoch:21 step:16811 [D loss: 0.000463, acc.: 100.00%] [G loss: 5.920436]\n",
      "epoch:21 step:16812 [D loss: 0.000875, acc.: 100.00%] [G loss: 2.697106]\n",
      "epoch:21 step:16813 [D loss: 0.081594, acc.: 97.66%] [G loss: 1.259442]\n",
      "epoch:21 step:16814 [D loss: 0.002506, acc.: 100.00%] [G loss: 13.698624]\n",
      "epoch:21 step:16815 [D loss: 0.000601, acc.: 100.00%] [G loss: 11.420817]\n",
      "epoch:21 step:16816 [D loss: 0.050850, acc.: 96.88%] [G loss: 3.849312]\n",
      "epoch:21 step:16817 [D loss: 0.025491, acc.: 98.44%] [G loss: 0.339701]\n",
      "epoch:21 step:16818 [D loss: 0.023570, acc.: 100.00%] [G loss: 0.514627]\n",
      "epoch:21 step:16819 [D loss: 0.021514, acc.: 99.22%] [G loss: 5.528680]\n",
      "epoch:21 step:16820 [D loss: 0.001554, acc.: 100.00%] [G loss: 5.764498]\n",
      "epoch:21 step:16821 [D loss: 0.971481, acc.: 76.56%] [G loss: 13.822903]\n",
      "epoch:21 step:16822 [D loss: 4.481384, acc.: 50.00%] [G loss: 6.248905]\n",
      "epoch:21 step:16823 [D loss: 1.597005, acc.: 70.31%] [G loss: 11.601591]\n",
      "epoch:21 step:16824 [D loss: 0.001183, acc.: 100.00%] [G loss: 8.267962]\n",
      "epoch:21 step:16825 [D loss: 0.157427, acc.: 92.19%] [G loss: 12.161047]\n",
      "epoch:21 step:16826 [D loss: 0.029364, acc.: 99.22%] [G loss: 10.221352]\n",
      "epoch:21 step:16827 [D loss: 0.001076, acc.: 100.00%] [G loss: 11.056715]\n",
      "epoch:21 step:16828 [D loss: 0.001059, acc.: 100.00%] [G loss: 2.288452]\n",
      "epoch:21 step:16829 [D loss: 0.012991, acc.: 100.00%] [G loss: 10.019083]\n",
      "epoch:21 step:16830 [D loss: 0.002060, acc.: 100.00%] [G loss: 9.512477]\n",
      "epoch:21 step:16831 [D loss: 0.050979, acc.: 97.66%] [G loss: 7.394645]\n",
      "epoch:21 step:16832 [D loss: 0.031487, acc.: 99.22%] [G loss: 5.602026]\n",
      "epoch:21 step:16833 [D loss: 0.016448, acc.: 100.00%] [G loss: 4.239723]\n",
      "epoch:21 step:16834 [D loss: 0.035034, acc.: 100.00%] [G loss: 1.901056]\n",
      "epoch:21 step:16835 [D loss: 0.030004, acc.: 100.00%] [G loss: 1.116523]\n",
      "epoch:21 step:16836 [D loss: 0.051768, acc.: 98.44%] [G loss: 0.061503]\n",
      "epoch:21 step:16837 [D loss: 0.006331, acc.: 100.00%] [G loss: 3.295158]\n",
      "epoch:21 step:16838 [D loss: 0.007053, acc.: 100.00%] [G loss: 1.598272]\n",
      "epoch:21 step:16839 [D loss: 0.145431, acc.: 95.31%] [G loss: 5.062348]\n",
      "epoch:21 step:16840 [D loss: 0.110795, acc.: 94.53%] [G loss: 2.492354]\n",
      "epoch:21 step:16841 [D loss: 0.005337, acc.: 100.00%] [G loss: 3.895886]\n",
      "epoch:21 step:16842 [D loss: 0.018351, acc.: 99.22%] [G loss: 3.451124]\n",
      "epoch:21 step:16843 [D loss: 0.027337, acc.: 99.22%] [G loss: 3.321098]\n",
      "epoch:21 step:16844 [D loss: 0.025161, acc.: 99.22%] [G loss: 0.310261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16845 [D loss: 0.095552, acc.: 96.88%] [G loss: 0.305107]\n",
      "epoch:21 step:16846 [D loss: 0.144149, acc.: 94.53%] [G loss: 2.332340]\n",
      "epoch:21 step:16847 [D loss: 0.035494, acc.: 100.00%] [G loss: 2.508577]\n",
      "epoch:21 step:16848 [D loss: 0.002648, acc.: 100.00%] [G loss: 1.806150]\n",
      "epoch:21 step:16849 [D loss: 0.209022, acc.: 90.62%] [G loss: 6.382330]\n",
      "epoch:21 step:16850 [D loss: 0.000824, acc.: 100.00%] [G loss: 9.141316]\n",
      "epoch:21 step:16851 [D loss: 0.345119, acc.: 87.50%] [G loss: 4.651287]\n",
      "epoch:21 step:16852 [D loss: 0.038282, acc.: 99.22%] [G loss: 3.767919]\n",
      "epoch:21 step:16853 [D loss: 0.004383, acc.: 100.00%] [G loss: 2.156260]\n",
      "epoch:21 step:16854 [D loss: 0.026429, acc.: 100.00%] [G loss: 1.692052]\n",
      "epoch:21 step:16855 [D loss: 0.220263, acc.: 88.28%] [G loss: 7.614950]\n",
      "epoch:21 step:16856 [D loss: 0.124816, acc.: 94.53%] [G loss: 8.666887]\n",
      "epoch:21 step:16857 [D loss: 0.056962, acc.: 97.66%] [G loss: 5.886791]\n",
      "epoch:21 step:16858 [D loss: 0.022802, acc.: 99.22%] [G loss: 5.812490]\n",
      "epoch:21 step:16859 [D loss: 0.002855, acc.: 100.00%] [G loss: 2.801717]\n",
      "epoch:21 step:16860 [D loss: 0.367201, acc.: 82.81%] [G loss: 7.113865]\n",
      "epoch:21 step:16861 [D loss: 0.692015, acc.: 75.78%] [G loss: 5.380663]\n",
      "epoch:21 step:16862 [D loss: 0.014144, acc.: 100.00%] [G loss: 3.912158]\n",
      "epoch:21 step:16863 [D loss: 0.039969, acc.: 99.22%] [G loss: 2.968798]\n",
      "epoch:21 step:16864 [D loss: 0.114055, acc.: 96.09%] [G loss: 4.494391]\n",
      "epoch:21 step:16865 [D loss: 0.033478, acc.: 99.22%] [G loss: 3.787923]\n",
      "epoch:21 step:16866 [D loss: 0.029236, acc.: 100.00%] [G loss: 4.067243]\n",
      "epoch:21 step:16867 [D loss: 0.037878, acc.: 99.22%] [G loss: 3.143187]\n",
      "epoch:21 step:16868 [D loss: 0.243456, acc.: 89.84%] [G loss: 7.559119]\n",
      "epoch:21 step:16869 [D loss: 0.092880, acc.: 96.09%] [G loss: 8.704237]\n",
      "epoch:21 step:16870 [D loss: 0.027987, acc.: 100.00%] [G loss: 7.276814]\n",
      "epoch:21 step:16871 [D loss: 0.023051, acc.: 99.22%] [G loss: 5.678450]\n",
      "epoch:21 step:16872 [D loss: 0.012561, acc.: 100.00%] [G loss: 5.834943]\n",
      "epoch:21 step:16873 [D loss: 0.033306, acc.: 100.00%] [G loss: 5.372496]\n",
      "epoch:21 step:16874 [D loss: 0.051709, acc.: 99.22%] [G loss: 4.620964]\n",
      "epoch:21 step:16875 [D loss: 0.069594, acc.: 98.44%] [G loss: 4.917575]\n",
      "epoch:21 step:16876 [D loss: 0.061102, acc.: 98.44%] [G loss: 6.068215]\n",
      "epoch:21 step:16877 [D loss: 0.007824, acc.: 100.00%] [G loss: 4.869161]\n",
      "epoch:21 step:16878 [D loss: 0.042234, acc.: 99.22%] [G loss: 5.628824]\n",
      "epoch:21 step:16879 [D loss: 0.031868, acc.: 99.22%] [G loss: 4.794153]\n",
      "epoch:21 step:16880 [D loss: 0.035138, acc.: 99.22%] [G loss: 3.530369]\n",
      "epoch:21 step:16881 [D loss: 0.055614, acc.: 98.44%] [G loss: 4.081450]\n",
      "epoch:21 step:16882 [D loss: 0.015799, acc.: 100.00%] [G loss: 3.304714]\n",
      "epoch:21 step:16883 [D loss: 0.158625, acc.: 93.75%] [G loss: 8.388014]\n",
      "epoch:21 step:16884 [D loss: 0.168866, acc.: 93.75%] [G loss: 2.291697]\n",
      "epoch:21 step:16885 [D loss: 0.039911, acc.: 99.22%] [G loss: 7.265601]\n",
      "epoch:21 step:16886 [D loss: 0.036435, acc.: 97.66%] [G loss: 6.270461]\n",
      "epoch:21 step:16887 [D loss: 0.016809, acc.: 100.00%] [G loss: 4.538006]\n",
      "epoch:21 step:16888 [D loss: 0.011600, acc.: 100.00%] [G loss: 3.883615]\n",
      "epoch:21 step:16889 [D loss: 0.108423, acc.: 96.09%] [G loss: 6.381078]\n",
      "epoch:21 step:16890 [D loss: 0.092585, acc.: 95.31%] [G loss: 5.448291]\n",
      "epoch:21 step:16891 [D loss: 0.009460, acc.: 100.00%] [G loss: 3.948384]\n",
      "epoch:21 step:16892 [D loss: 0.232172, acc.: 90.62%] [G loss: 8.186275]\n",
      "epoch:21 step:16893 [D loss: 0.976726, acc.: 60.16%] [G loss: 8.314659]\n",
      "epoch:21 step:16894 [D loss: 0.017857, acc.: 99.22%] [G loss: 8.679583]\n",
      "epoch:21 step:16895 [D loss: 0.374791, acc.: 86.72%] [G loss: 5.456107]\n",
      "epoch:21 step:16896 [D loss: 0.333050, acc.: 91.41%] [G loss: 6.408200]\n",
      "epoch:21 step:16897 [D loss: 0.093902, acc.: 96.88%] [G loss: 7.637702]\n",
      "epoch:21 step:16898 [D loss: 0.106625, acc.: 96.88%] [G loss: 5.748413]\n",
      "epoch:21 step:16899 [D loss: 0.102628, acc.: 95.31%] [G loss: 5.480642]\n",
      "epoch:21 step:16900 [D loss: 0.097906, acc.: 96.09%] [G loss: 6.127485]\n",
      "epoch:21 step:16901 [D loss: 0.007571, acc.: 100.00%] [G loss: 6.191453]\n",
      "epoch:21 step:16902 [D loss: 0.124213, acc.: 96.88%] [G loss: 6.222885]\n",
      "epoch:21 step:16903 [D loss: 0.020027, acc.: 100.00%] [G loss: 6.316591]\n",
      "epoch:21 step:16904 [D loss: 0.041061, acc.: 100.00%] [G loss: 4.671696]\n",
      "epoch:21 step:16905 [D loss: 0.060913, acc.: 99.22%] [G loss: 5.879147]\n",
      "epoch:21 step:16906 [D loss: 0.092650, acc.: 97.66%] [G loss: 4.767644]\n",
      "epoch:21 step:16907 [D loss: 0.060754, acc.: 99.22%] [G loss: 5.585175]\n",
      "epoch:21 step:16908 [D loss: 0.026664, acc.: 100.00%] [G loss: 6.093415]\n",
      "epoch:21 step:16909 [D loss: 0.075759, acc.: 99.22%] [G loss: 3.456080]\n",
      "epoch:21 step:16910 [D loss: 0.147001, acc.: 92.19%] [G loss: 8.445948]\n",
      "epoch:21 step:16911 [D loss: 0.167796, acc.: 92.97%] [G loss: 8.066797]\n",
      "epoch:21 step:16912 [D loss: 0.095278, acc.: 96.88%] [G loss: 5.281228]\n",
      "epoch:21 step:16913 [D loss: 0.033262, acc.: 99.22%] [G loss: 4.775578]\n",
      "epoch:21 step:16914 [D loss: 0.008826, acc.: 100.00%] [G loss: 3.444414]\n",
      "epoch:21 step:16915 [D loss: 0.042312, acc.: 100.00%] [G loss: 4.222927]\n",
      "epoch:21 step:16916 [D loss: 0.024889, acc.: 100.00%] [G loss: 5.646974]\n",
      "epoch:21 step:16917 [D loss: 0.029273, acc.: 99.22%] [G loss: 4.714241]\n",
      "epoch:21 step:16918 [D loss: 0.064978, acc.: 99.22%] [G loss: 3.836905]\n",
      "epoch:21 step:16919 [D loss: 0.073435, acc.: 98.44%] [G loss: 5.044472]\n",
      "epoch:21 step:16920 [D loss: 0.060625, acc.: 96.88%] [G loss: 2.165512]\n",
      "epoch:21 step:16921 [D loss: 0.030681, acc.: 100.00%] [G loss: 2.285300]\n",
      "epoch:21 step:16922 [D loss: 0.038492, acc.: 99.22%] [G loss: 3.928853]\n",
      "epoch:21 step:16923 [D loss: 0.051721, acc.: 99.22%] [G loss: 2.244309]\n",
      "epoch:21 step:16924 [D loss: 0.958505, acc.: 59.38%] [G loss: 12.733586]\n",
      "epoch:21 step:16925 [D loss: 4.171287, acc.: 50.00%] [G loss: 8.068206]\n",
      "epoch:21 step:16926 [D loss: 1.894230, acc.: 51.56%] [G loss: 3.120065]\n",
      "epoch:21 step:16927 [D loss: 0.446188, acc.: 80.47%] [G loss: 2.845129]\n",
      "epoch:21 step:16928 [D loss: 0.095057, acc.: 99.22%] [G loss: 0.906302]\n",
      "epoch:21 step:16929 [D loss: 0.146131, acc.: 96.09%] [G loss: 3.603918]\n",
      "epoch:21 step:16930 [D loss: 0.158322, acc.: 94.53%] [G loss: 2.366289]\n",
      "epoch:21 step:16931 [D loss: 0.093246, acc.: 98.44%] [G loss: 3.669866]\n",
      "epoch:21 step:16932 [D loss: 0.052156, acc.: 100.00%] [G loss: 3.345334]\n",
      "epoch:21 step:16933 [D loss: 0.155488, acc.: 96.09%] [G loss: 3.266926]\n",
      "epoch:21 step:16934 [D loss: 0.057529, acc.: 99.22%] [G loss: 0.608060]\n",
      "epoch:21 step:16935 [D loss: 0.088484, acc.: 97.66%] [G loss: 3.287148]\n",
      "epoch:21 step:16936 [D loss: 0.165617, acc.: 96.09%] [G loss: 2.857246]\n",
      "epoch:21 step:16937 [D loss: 0.175319, acc.: 93.75%] [G loss: 3.751614]\n",
      "epoch:21 step:16938 [D loss: 0.280592, acc.: 88.28%] [G loss: 1.005499]\n",
      "epoch:21 step:16939 [D loss: 0.112761, acc.: 97.66%] [G loss: 3.538309]\n",
      "epoch:21 step:16940 [D loss: 0.049789, acc.: 99.22%] [G loss: 3.952885]\n",
      "epoch:21 step:16941 [D loss: 0.080375, acc.: 98.44%] [G loss: 3.587909]\n",
      "epoch:21 step:16942 [D loss: 0.100071, acc.: 99.22%] [G loss: 3.580849]\n",
      "epoch:21 step:16943 [D loss: 0.137492, acc.: 95.31%] [G loss: 2.893329]\n",
      "epoch:21 step:16944 [D loss: 0.103722, acc.: 96.09%] [G loss: 3.774003]\n",
      "epoch:21 step:16945 [D loss: 0.036497, acc.: 100.00%] [G loss: 3.596273]\n",
      "epoch:21 step:16946 [D loss: 0.169217, acc.: 94.53%] [G loss: 3.246723]\n",
      "epoch:21 step:16947 [D loss: 0.202220, acc.: 92.19%] [G loss: 3.050083]\n",
      "epoch:21 step:16948 [D loss: 0.133429, acc.: 93.75%] [G loss: 2.491285]\n",
      "epoch:21 step:16949 [D loss: 0.028975, acc.: 100.00%] [G loss: 1.829551]\n",
      "epoch:21 step:16950 [D loss: 0.135241, acc.: 96.09%] [G loss: 3.908263]\n",
      "epoch:21 step:16951 [D loss: 0.087480, acc.: 97.66%] [G loss: 4.023991]\n",
      "epoch:21 step:16952 [D loss: 0.066713, acc.: 98.44%] [G loss: 2.655207]\n",
      "epoch:21 step:16953 [D loss: 0.037867, acc.: 100.00%] [G loss: 1.489040]\n",
      "epoch:21 step:16954 [D loss: 0.025319, acc.: 99.22%] [G loss: 1.938203]\n",
      "epoch:21 step:16955 [D loss: 0.027488, acc.: 100.00%] [G loss: 0.810909]\n",
      "epoch:21 step:16956 [D loss: 0.011530, acc.: 100.00%] [G loss: 1.935389]\n",
      "epoch:21 step:16957 [D loss: 0.016881, acc.: 100.00%] [G loss: 1.707435]\n",
      "epoch:21 step:16958 [D loss: 0.055252, acc.: 99.22%] [G loss: 1.502409]\n",
      "epoch:21 step:16959 [D loss: 0.037352, acc.: 99.22%] [G loss: 0.460488]\n",
      "epoch:21 step:16960 [D loss: 0.012080, acc.: 100.00%] [G loss: 1.500224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:16961 [D loss: 0.009762, acc.: 100.00%] [G loss: 0.197871]\n",
      "epoch:21 step:16962 [D loss: 0.048557, acc.: 100.00%] [G loss: 0.390346]\n",
      "epoch:21 step:16963 [D loss: 0.008291, acc.: 100.00%] [G loss: 1.505371]\n",
      "epoch:21 step:16964 [D loss: 0.016329, acc.: 100.00%] [G loss: 0.472392]\n",
      "epoch:21 step:16965 [D loss: 0.008505, acc.: 100.00%] [G loss: 0.216988]\n",
      "epoch:21 step:16966 [D loss: 0.028464, acc.: 99.22%] [G loss: 0.101500]\n",
      "epoch:21 step:16967 [D loss: 0.043767, acc.: 98.44%] [G loss: 0.022004]\n",
      "epoch:21 step:16968 [D loss: 0.096438, acc.: 96.88%] [G loss: 0.734584]\n",
      "epoch:21 step:16969 [D loss: 0.007255, acc.: 100.00%] [G loss: 0.294667]\n",
      "epoch:21 step:16970 [D loss: 0.009490, acc.: 100.00%] [G loss: 1.083475]\n",
      "epoch:21 step:16971 [D loss: 0.002536, acc.: 100.00%] [G loss: 0.193336]\n",
      "epoch:21 step:16972 [D loss: 0.036372, acc.: 98.44%] [G loss: 0.036165]\n",
      "epoch:21 step:16973 [D loss: 0.008232, acc.: 99.22%] [G loss: 0.001992]\n",
      "epoch:21 step:16974 [D loss: 0.003883, acc.: 100.00%] [G loss: 0.007437]\n",
      "epoch:21 step:16975 [D loss: 0.002754, acc.: 100.00%] [G loss: 0.002162]\n",
      "epoch:21 step:16976 [D loss: 0.002928, acc.: 100.00%] [G loss: 0.008586]\n",
      "epoch:21 step:16977 [D loss: 0.002335, acc.: 100.00%] [G loss: 0.037411]\n",
      "epoch:21 step:16978 [D loss: 0.050638, acc.: 99.22%] [G loss: 0.035238]\n",
      "epoch:21 step:16979 [D loss: 0.001948, acc.: 100.00%] [G loss: 0.195226]\n",
      "epoch:21 step:16980 [D loss: 0.011067, acc.: 100.00%] [G loss: 0.075709]\n",
      "epoch:21 step:16981 [D loss: 0.021534, acc.: 100.00%] [G loss: 0.063979]\n",
      "epoch:21 step:16982 [D loss: 0.054935, acc.: 97.66%] [G loss: 0.047693]\n",
      "epoch:21 step:16983 [D loss: 0.010582, acc.: 100.00%] [G loss: 0.246883]\n",
      "epoch:21 step:16984 [D loss: 0.008147, acc.: 100.00%] [G loss: 0.009780]\n",
      "epoch:21 step:16985 [D loss: 0.004531, acc.: 100.00%] [G loss: 0.258501]\n",
      "epoch:21 step:16986 [D loss: 0.018231, acc.: 100.00%] [G loss: 0.027194]\n",
      "epoch:21 step:16987 [D loss: 0.019008, acc.: 100.00%] [G loss: 0.781673]\n",
      "epoch:21 step:16988 [D loss: 0.001846, acc.: 100.00%] [G loss: 1.673656]\n",
      "epoch:21 step:16989 [D loss: 0.002230, acc.: 100.00%] [G loss: 0.584470]\n",
      "epoch:21 step:16990 [D loss: 0.009875, acc.: 100.00%] [G loss: 0.698712]\n",
      "epoch:21 step:16991 [D loss: 0.101495, acc.: 96.88%] [G loss: 1.618646]\n",
      "epoch:21 step:16992 [D loss: 0.551155, acc.: 79.69%] [G loss: 0.314985]\n",
      "epoch:21 step:16993 [D loss: 0.002158, acc.: 100.00%] [G loss: 0.587446]\n",
      "epoch:21 step:16994 [D loss: 0.022564, acc.: 100.00%] [G loss: 0.694742]\n",
      "epoch:21 step:16995 [D loss: 0.022315, acc.: 100.00%] [G loss: 0.894387]\n",
      "epoch:21 step:16996 [D loss: 0.012696, acc.: 100.00%] [G loss: 1.590300]\n",
      "epoch:21 step:16997 [D loss: 0.015357, acc.: 100.00%] [G loss: 0.571502]\n",
      "epoch:21 step:16998 [D loss: 0.020678, acc.: 99.22%] [G loss: 0.323375]\n",
      "epoch:21 step:16999 [D loss: 0.049137, acc.: 99.22%] [G loss: 1.091043]\n",
      "epoch:21 step:17000 [D loss: 0.199613, acc.: 94.53%] [G loss: 6.493141]\n",
      "epoch:21 step:17001 [D loss: 0.089255, acc.: 97.66%] [G loss: 8.268574]\n",
      "epoch:21 step:17002 [D loss: 0.259839, acc.: 89.06%] [G loss: 3.334798]\n",
      "epoch:21 step:17003 [D loss: 0.106185, acc.: 96.88%] [G loss: 4.813070]\n",
      "epoch:21 step:17004 [D loss: 0.006702, acc.: 100.00%] [G loss: 5.111596]\n",
      "epoch:21 step:17005 [D loss: 0.135994, acc.: 95.31%] [G loss: 0.634566]\n",
      "epoch:21 step:17006 [D loss: 0.032227, acc.: 99.22%] [G loss: 0.519983]\n",
      "epoch:21 step:17007 [D loss: 0.009370, acc.: 100.00%] [G loss: 6.316571]\n",
      "epoch:21 step:17008 [D loss: 0.034534, acc.: 98.44%] [G loss: 4.598402]\n",
      "epoch:21 step:17009 [D loss: 0.009075, acc.: 100.00%] [G loss: 3.272636]\n",
      "epoch:21 step:17010 [D loss: 0.017942, acc.: 100.00%] [G loss: 2.067857]\n",
      "epoch:21 step:17011 [D loss: 0.021902, acc.: 99.22%] [G loss: 2.029752]\n",
      "epoch:21 step:17012 [D loss: 0.008572, acc.: 100.00%] [G loss: 0.757979]\n",
      "epoch:21 step:17013 [D loss: 0.008975, acc.: 100.00%] [G loss: 0.777677]\n",
      "epoch:21 step:17014 [D loss: 0.065824, acc.: 96.88%] [G loss: 0.231799]\n",
      "epoch:21 step:17015 [D loss: 0.077149, acc.: 97.66%] [G loss: 0.318341]\n",
      "epoch:21 step:17016 [D loss: 0.003076, acc.: 100.00%] [G loss: 3.922204]\n",
      "epoch:21 step:17017 [D loss: 0.001581, acc.: 100.00%] [G loss: 0.443575]\n",
      "epoch:21 step:17018 [D loss: 0.010115, acc.: 100.00%] [G loss: 4.042983]\n",
      "epoch:21 step:17019 [D loss: 0.013459, acc.: 100.00%] [G loss: 2.873839]\n",
      "epoch:21 step:17020 [D loss: 0.016048, acc.: 100.00%] [G loss: 1.528632]\n",
      "epoch:21 step:17021 [D loss: 0.016153, acc.: 100.00%] [G loss: 1.892478]\n",
      "epoch:21 step:17022 [D loss: 0.012052, acc.: 100.00%] [G loss: 0.705689]\n",
      "epoch:21 step:17023 [D loss: 0.016523, acc.: 100.00%] [G loss: 0.490494]\n",
      "epoch:21 step:17024 [D loss: 0.015263, acc.: 100.00%] [G loss: 0.546283]\n",
      "epoch:21 step:17025 [D loss: 0.010032, acc.: 100.00%] [G loss: 1.863882]\n",
      "epoch:21 step:17026 [D loss: 0.008073, acc.: 100.00%] [G loss: 0.491885]\n",
      "epoch:21 step:17027 [D loss: 0.008584, acc.: 100.00%] [G loss: 4.481897]\n",
      "epoch:21 step:17028 [D loss: 0.131815, acc.: 96.09%] [G loss: 0.118306]\n",
      "epoch:21 step:17029 [D loss: 0.003113, acc.: 100.00%] [G loss: 0.270498]\n",
      "epoch:21 step:17030 [D loss: 0.068913, acc.: 96.88%] [G loss: 4.778387]\n",
      "epoch:21 step:17031 [D loss: 0.027998, acc.: 99.22%] [G loss: 4.032353]\n",
      "epoch:21 step:17032 [D loss: 0.011818, acc.: 100.00%] [G loss: 2.808993]\n",
      "epoch:21 step:17033 [D loss: 0.004600, acc.: 100.00%] [G loss: 2.783311]\n",
      "epoch:21 step:17034 [D loss: 0.001976, acc.: 100.00%] [G loss: 2.130692]\n",
      "epoch:21 step:17035 [D loss: 0.018967, acc.: 99.22%] [G loss: 0.012057]\n",
      "epoch:21 step:17036 [D loss: 0.003192, acc.: 100.00%] [G loss: 0.012099]\n",
      "epoch:21 step:17037 [D loss: 0.001067, acc.: 100.00%] [G loss: 1.330348]\n",
      "epoch:21 step:17038 [D loss: 0.005942, acc.: 100.00%] [G loss: 0.833628]\n",
      "epoch:21 step:17039 [D loss: 0.000497, acc.: 100.00%] [G loss: 0.687267]\n",
      "epoch:21 step:17040 [D loss: 0.001838, acc.: 100.00%] [G loss: 0.754457]\n",
      "epoch:21 step:17041 [D loss: 0.001028, acc.: 100.00%] [G loss: 0.657996]\n",
      "epoch:21 step:17042 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.149379]\n",
      "epoch:21 step:17043 [D loss: 0.001039, acc.: 100.00%] [G loss: 0.353023]\n",
      "epoch:21 step:17044 [D loss: 0.002346, acc.: 100.00%] [G loss: 0.280056]\n",
      "epoch:21 step:17045 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.176130]\n",
      "epoch:21 step:17046 [D loss: 0.002764, acc.: 100.00%] [G loss: 0.064539]\n",
      "epoch:21 step:17047 [D loss: 0.005978, acc.: 100.00%] [G loss: 0.087855]\n",
      "epoch:21 step:17048 [D loss: 0.026386, acc.: 99.22%] [G loss: 0.133689]\n",
      "epoch:21 step:17049 [D loss: 0.008569, acc.: 100.00%] [G loss: 0.298819]\n",
      "epoch:21 step:17050 [D loss: 0.003906, acc.: 100.00%] [G loss: 0.306763]\n",
      "epoch:21 step:17051 [D loss: 0.003429, acc.: 100.00%] [G loss: 0.128879]\n",
      "epoch:21 step:17052 [D loss: 0.002897, acc.: 100.00%] [G loss: 0.108557]\n",
      "epoch:21 step:17053 [D loss: 0.013089, acc.: 100.00%] [G loss: 0.382411]\n",
      "epoch:21 step:17054 [D loss: 0.011500, acc.: 100.00%] [G loss: 0.856986]\n",
      "epoch:21 step:17055 [D loss: 0.014942, acc.: 100.00%] [G loss: 2.319543]\n",
      "epoch:21 step:17056 [D loss: 0.009815, acc.: 100.00%] [G loss: 0.063425]\n",
      "epoch:21 step:17057 [D loss: 0.006093, acc.: 100.00%] [G loss: 3.458374]\n",
      "epoch:21 step:17058 [D loss: 0.000680, acc.: 100.00%] [G loss: 1.351835]\n",
      "epoch:21 step:17059 [D loss: 0.000922, acc.: 100.00%] [G loss: 2.816523]\n",
      "epoch:21 step:17060 [D loss: 0.031241, acc.: 99.22%] [G loss: 0.070041]\n",
      "epoch:21 step:17061 [D loss: 0.008818, acc.: 100.00%] [G loss: 3.270141]\n",
      "epoch:21 step:17062 [D loss: 0.007199, acc.: 100.00%] [G loss: 1.460739]\n",
      "epoch:21 step:17063 [D loss: 0.010235, acc.: 100.00%] [G loss: 0.834743]\n",
      "epoch:21 step:17064 [D loss: 0.001994, acc.: 100.00%] [G loss: 3.710903]\n",
      "epoch:21 step:17065 [D loss: 0.007086, acc.: 100.00%] [G loss: 0.586347]\n",
      "epoch:21 step:17066 [D loss: 0.008681, acc.: 100.00%] [G loss: 0.044790]\n",
      "epoch:21 step:17067 [D loss: 0.004701, acc.: 100.00%] [G loss: 0.217113]\n",
      "epoch:21 step:17068 [D loss: 0.000461, acc.: 100.00%] [G loss: 1.021381]\n",
      "epoch:21 step:17069 [D loss: 0.009887, acc.: 99.22%] [G loss: 0.321099]\n",
      "epoch:21 step:17070 [D loss: 0.079247, acc.: 97.66%] [G loss: 0.163158]\n",
      "epoch:21 step:17071 [D loss: 0.218558, acc.: 90.62%] [G loss: 0.499874]\n",
      "epoch:21 step:17072 [D loss: 0.005915, acc.: 100.00%] [G loss: 0.002285]\n",
      "epoch:21 step:17073 [D loss: 0.003424, acc.: 100.00%] [G loss: 0.314196]\n",
      "epoch:21 step:17074 [D loss: 0.049453, acc.: 97.66%] [G loss: 3.014585]\n",
      "epoch:21 step:17075 [D loss: 0.046432, acc.: 99.22%] [G loss: 1.500678]\n",
      "epoch:21 step:17076 [D loss: 0.002238, acc.: 100.00%] [G loss: 1.324901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:21 step:17077 [D loss: 0.075167, acc.: 96.88%] [G loss: 0.412521]\n",
      "epoch:21 step:17078 [D loss: 0.010668, acc.: 100.00%] [G loss: 0.508209]\n",
      "epoch:21 step:17079 [D loss: 0.033074, acc.: 100.00%] [G loss: 2.285794]\n",
      "epoch:21 step:17080 [D loss: 0.010109, acc.: 99.22%] [G loss: 4.429757]\n",
      "epoch:21 step:17081 [D loss: 0.053328, acc.: 98.44%] [G loss: 2.410943]\n",
      "epoch:21 step:17082 [D loss: 0.056585, acc.: 97.66%] [G loss: 0.720490]\n",
      "epoch:21 step:17083 [D loss: 0.011860, acc.: 100.00%] [G loss: 2.455388]\n",
      "epoch:21 step:17084 [D loss: 0.004792, acc.: 100.00%] [G loss: 0.586943]\n",
      "epoch:21 step:17085 [D loss: 0.002060, acc.: 100.00%] [G loss: 0.917479]\n",
      "epoch:21 step:17086 [D loss: 0.051961, acc.: 96.09%] [G loss: 0.343071]\n",
      "epoch:21 step:17087 [D loss: 0.051103, acc.: 98.44%] [G loss: 0.236738]\n",
      "epoch:21 step:17088 [D loss: 0.007731, acc.: 100.00%] [G loss: 0.731809]\n",
      "epoch:21 step:17089 [D loss: 0.006535, acc.: 100.00%] [G loss: 0.554918]\n",
      "epoch:21 step:17090 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.171091]\n",
      "epoch:21 step:17091 [D loss: 0.030955, acc.: 100.00%] [G loss: 1.737418]\n",
      "epoch:21 step:17092 [D loss: 0.003265, acc.: 100.00%] [G loss: 2.047166]\n",
      "epoch:21 step:17093 [D loss: 0.015743, acc.: 100.00%] [G loss: 1.664500]\n",
      "epoch:21 step:17094 [D loss: 0.023270, acc.: 100.00%] [G loss: 1.728855]\n",
      "epoch:21 step:17095 [D loss: 0.008019, acc.: 100.00%] [G loss: 2.714118]\n",
      "epoch:21 step:17096 [D loss: 0.016409, acc.: 100.00%] [G loss: 3.409170]\n",
      "epoch:21 step:17097 [D loss: 0.016729, acc.: 100.00%] [G loss: 1.782449]\n",
      "epoch:21 step:17098 [D loss: 0.004194, acc.: 100.00%] [G loss: 4.340533]\n",
      "epoch:21 step:17099 [D loss: 0.008111, acc.: 100.00%] [G loss: 4.134419]\n",
      "epoch:21 step:17100 [D loss: 0.051448, acc.: 98.44%] [G loss: 7.810861]\n",
      "epoch:21 step:17101 [D loss: 0.044043, acc.: 99.22%] [G loss: 5.278416]\n",
      "epoch:21 step:17102 [D loss: 0.013158, acc.: 100.00%] [G loss: 5.404059]\n",
      "epoch:21 step:17103 [D loss: 0.007970, acc.: 100.00%] [G loss: 6.735053]\n",
      "epoch:21 step:17104 [D loss: 0.044326, acc.: 98.44%] [G loss: 0.001926]\n",
      "epoch:21 step:17105 [D loss: 0.207908, acc.: 91.41%] [G loss: 10.960279]\n",
      "epoch:21 step:17106 [D loss: 2.942973, acc.: 23.44%] [G loss: 12.404444]\n",
      "epoch:21 step:17107 [D loss: 1.415647, acc.: 57.81%] [G loss: 5.087566]\n",
      "epoch:21 step:17108 [D loss: 0.035889, acc.: 99.22%] [G loss: 5.241214]\n",
      "epoch:21 step:17109 [D loss: 0.093942, acc.: 95.31%] [G loss: 6.066494]\n",
      "epoch:21 step:17110 [D loss: 0.005147, acc.: 100.00%] [G loss: 6.196328]\n",
      "epoch:21 step:17111 [D loss: 0.031064, acc.: 99.22%] [G loss: 0.000471]\n",
      "epoch:21 step:17112 [D loss: 0.090768, acc.: 94.53%] [G loss: 5.673961]\n",
      "epoch:21 step:17113 [D loss: 0.007411, acc.: 100.00%] [G loss: 5.569386]\n",
      "epoch:21 step:17114 [D loss: 0.190710, acc.: 90.62%] [G loss: 1.419529]\n",
      "epoch:21 step:17115 [D loss: 0.033767, acc.: 100.00%] [G loss: 3.216543]\n",
      "epoch:21 step:17116 [D loss: 0.132988, acc.: 93.75%] [G loss: 0.065096]\n",
      "epoch:21 step:17117 [D loss: 0.016694, acc.: 100.00%] [G loss: 0.004777]\n",
      "epoch:21 step:17118 [D loss: 0.010678, acc.: 100.00%] [G loss: 0.001824]\n",
      "epoch:21 step:17119 [D loss: 0.026059, acc.: 100.00%] [G loss: 0.081184]\n",
      "epoch:21 step:17120 [D loss: 0.015241, acc.: 100.00%] [G loss: 5.363783]\n",
      "epoch:21 step:17121 [D loss: 0.009503, acc.: 100.00%] [G loss: 5.166027]\n",
      "epoch:21 step:17122 [D loss: 0.053750, acc.: 98.44%] [G loss: 4.747162]\n",
      "epoch:21 step:17123 [D loss: 0.167646, acc.: 92.97%] [G loss: 0.000100]\n",
      "epoch:21 step:17124 [D loss: 0.087352, acc.: 96.88%] [G loss: 1.182715]\n",
      "epoch:21 step:17125 [D loss: 0.006919, acc.: 100.00%] [G loss: 0.010523]\n",
      "epoch:21 step:17126 [D loss: 0.161639, acc.: 95.31%] [G loss: 0.004407]\n",
      "epoch:21 step:17127 [D loss: 0.002184, acc.: 100.00%] [G loss: 1.067392]\n",
      "epoch:21 step:17128 [D loss: 0.007104, acc.: 100.00%] [G loss: 0.583856]\n",
      "epoch:21 step:17129 [D loss: 0.018816, acc.: 99.22%] [G loss: 0.291396]\n",
      "epoch:21 step:17130 [D loss: 0.002767, acc.: 100.00%] [G loss: 0.114348]\n",
      "epoch:21 step:17131 [D loss: 0.003814, acc.: 100.00%] [G loss: 0.002764]\n",
      "epoch:21 step:17132 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.044251]\n",
      "epoch:21 step:17133 [D loss: 0.015170, acc.: 99.22%] [G loss: 0.031997]\n",
      "epoch:21 step:17134 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.131712]\n",
      "epoch:21 step:17135 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.002509]\n",
      "epoch:21 step:17136 [D loss: 0.000980, acc.: 100.00%] [G loss: 0.168811]\n",
      "epoch:21 step:17137 [D loss: 0.015396, acc.: 100.00%] [G loss: 0.004730]\n",
      "epoch:21 step:17138 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.000502]\n",
      "epoch:21 step:17139 [D loss: 0.008400, acc.: 100.00%] [G loss: 0.009191]\n",
      "epoch:21 step:17140 [D loss: 0.013218, acc.: 100.00%] [G loss: 0.003369]\n",
      "epoch:21 step:17141 [D loss: 0.006353, acc.: 100.00%] [G loss: 0.000943]\n",
      "epoch:21 step:17142 [D loss: 0.009870, acc.: 100.00%] [G loss: 0.004270]\n",
      "epoch:21 step:17143 [D loss: 0.030416, acc.: 99.22%] [G loss: 0.004197]\n",
      "epoch:21 step:17144 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.012583]\n",
      "epoch:21 step:17145 [D loss: 0.440673, acc.: 77.34%] [G loss: 8.870355]\n",
      "epoch:21 step:17146 [D loss: 0.579772, acc.: 72.66%] [G loss: 3.085086]\n",
      "epoch:21 step:17147 [D loss: 0.017762, acc.: 100.00%] [G loss: 0.002832]\n",
      "epoch:21 step:17148 [D loss: 0.128910, acc.: 93.75%] [G loss: 0.003730]\n",
      "epoch:21 step:17149 [D loss: 0.503471, acc.: 80.47%] [G loss: 3.976793]\n",
      "epoch:21 step:17150 [D loss: 0.693572, acc.: 71.09%] [G loss: 7.367123]\n",
      "epoch:21 step:17151 [D loss: 0.032963, acc.: 99.22%] [G loss: 5.403265]\n",
      "epoch:21 step:17152 [D loss: 0.003126, acc.: 100.00%] [G loss: 1.321804]\n",
      "epoch:21 step:17153 [D loss: 0.074117, acc.: 96.88%] [G loss: 1.714265]\n",
      "epoch:21 step:17154 [D loss: 0.031310, acc.: 100.00%] [G loss: 3.027444]\n",
      "epoch:21 step:17155 [D loss: 0.023685, acc.: 100.00%] [G loss: 1.186972]\n",
      "epoch:21 step:17156 [D loss: 0.053889, acc.: 100.00%] [G loss: 0.974454]\n",
      "epoch:21 step:17157 [D loss: 0.042451, acc.: 99.22%] [G loss: 3.949478]\n",
      "epoch:21 step:17158 [D loss: 0.024897, acc.: 100.00%] [G loss: 1.662853]\n",
      "epoch:21 step:17159 [D loss: 0.062834, acc.: 98.44%] [G loss: 2.536265]\n",
      "epoch:21 step:17160 [D loss: 0.061964, acc.: 99.22%] [G loss: 0.482898]\n",
      "epoch:21 step:17161 [D loss: 0.209838, acc.: 92.19%] [G loss: 1.010324]\n",
      "epoch:21 step:17162 [D loss: 0.019287, acc.: 99.22%] [G loss: 6.761292]\n",
      "epoch:21 step:17163 [D loss: 0.771447, acc.: 67.19%] [G loss: 0.005789]\n",
      "epoch:21 step:17164 [D loss: 0.794628, acc.: 68.75%] [G loss: 4.972473]\n",
      "epoch:21 step:17165 [D loss: 0.013214, acc.: 100.00%] [G loss: 5.008288]\n",
      "epoch:21 step:17166 [D loss: 0.547764, acc.: 74.22%] [G loss: 0.924012]\n",
      "epoch:21 step:17167 [D loss: 0.066409, acc.: 96.88%] [G loss: 0.149446]\n",
      "epoch:21 step:17168 [D loss: 0.004871, acc.: 100.00%] [G loss: 0.038212]\n",
      "epoch:21 step:17169 [D loss: 0.022889, acc.: 100.00%] [G loss: 4.151465]\n",
      "epoch:21 step:17170 [D loss: 0.018328, acc.: 99.22%] [G loss: 0.022038]\n",
      "epoch:21 step:17171 [D loss: 0.041600, acc.: 99.22%] [G loss: 0.040584]\n",
      "epoch:21 step:17172 [D loss: 0.029437, acc.: 100.00%] [G loss: 0.006144]\n",
      "epoch:21 step:17173 [D loss: 0.013329, acc.: 100.00%] [G loss: 0.071654]\n",
      "epoch:21 step:17174 [D loss: 0.024780, acc.: 100.00%] [G loss: 5.171189]\n",
      "epoch:21 step:17175 [D loss: 0.004651, acc.: 100.00%] [G loss: 5.096264]\n",
      "epoch:21 step:17176 [D loss: 0.028065, acc.: 99.22%] [G loss: 3.664142]\n",
      "epoch:21 step:17177 [D loss: 0.002933, acc.: 100.00%] [G loss: 0.088209]\n",
      "epoch:21 step:17178 [D loss: 0.003947, acc.: 100.00%] [G loss: 0.004501]\n",
      "epoch:21 step:17179 [D loss: 0.005088, acc.: 100.00%] [G loss: 0.006412]\n",
      "epoch:21 step:17180 [D loss: 0.010514, acc.: 100.00%] [G loss: 0.006729]\n",
      "epoch:21 step:17181 [D loss: 0.002156, acc.: 100.00%] [G loss: 0.013821]\n",
      "epoch:21 step:17182 [D loss: 0.001259, acc.: 100.00%] [G loss: 0.010807]\n",
      "epoch:22 step:17183 [D loss: 0.001756, acc.: 100.00%] [G loss: 0.027342]\n",
      "epoch:22 step:17184 [D loss: 0.003970, acc.: 100.00%] [G loss: 1.966093]\n",
      "epoch:22 step:17185 [D loss: 0.003944, acc.: 100.00%] [G loss: 0.002027]\n",
      "epoch:22 step:17186 [D loss: 0.000594, acc.: 100.00%] [G loss: 0.000587]\n",
      "epoch:22 step:17187 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.003463]\n",
      "epoch:22 step:17188 [D loss: 0.003732, acc.: 100.00%] [G loss: 0.011873]\n",
      "epoch:22 step:17189 [D loss: 0.007821, acc.: 100.00%] [G loss: 0.004860]\n",
      "epoch:22 step:17190 [D loss: 0.001720, acc.: 100.00%] [G loss: 0.004583]\n",
      "epoch:22 step:17191 [D loss: 0.002073, acc.: 100.00%] [G loss: 0.000975]\n",
      "epoch:22 step:17192 [D loss: 0.015594, acc.: 100.00%] [G loss: 1.372199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17193 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.006191]\n",
      "epoch:22 step:17194 [D loss: 0.000972, acc.: 100.00%] [G loss: 0.010512]\n",
      "epoch:22 step:17195 [D loss: 0.003456, acc.: 100.00%] [G loss: 0.846289]\n",
      "epoch:22 step:17196 [D loss: 0.113918, acc.: 96.09%] [G loss: 2.576709]\n",
      "epoch:22 step:17197 [D loss: 0.065131, acc.: 96.88%] [G loss: 1.181538]\n",
      "epoch:22 step:17198 [D loss: 0.009578, acc.: 100.00%] [G loss: 0.809285]\n",
      "epoch:22 step:17199 [D loss: 0.015635, acc.: 99.22%] [G loss: 0.926809]\n",
      "epoch:22 step:17200 [D loss: 0.005479, acc.: 100.00%] [G loss: 0.193738]\n",
      "epoch:22 step:17201 [D loss: 0.002084, acc.: 100.00%] [G loss: 0.014451]\n",
      "epoch:22 step:17202 [D loss: 0.027953, acc.: 99.22%] [G loss: 0.235521]\n",
      "epoch:22 step:17203 [D loss: 0.022033, acc.: 100.00%] [G loss: 0.678289]\n",
      "epoch:22 step:17204 [D loss: 0.004636, acc.: 100.00%] [G loss: 0.603493]\n",
      "epoch:22 step:17205 [D loss: 0.005163, acc.: 100.00%] [G loss: 0.217801]\n",
      "epoch:22 step:17206 [D loss: 0.002832, acc.: 100.00%] [G loss: 0.073041]\n",
      "epoch:22 step:17207 [D loss: 0.024225, acc.: 99.22%] [G loss: 0.004895]\n",
      "epoch:22 step:17208 [D loss: 0.001870, acc.: 100.00%] [G loss: 0.001831]\n",
      "epoch:22 step:17209 [D loss: 0.001358, acc.: 100.00%] [G loss: 0.062531]\n",
      "epoch:22 step:17210 [D loss: 0.000913, acc.: 100.00%] [G loss: 0.006457]\n",
      "epoch:22 step:17211 [D loss: 0.001126, acc.: 100.00%] [G loss: 0.002800]\n",
      "epoch:22 step:17212 [D loss: 0.001060, acc.: 100.00%] [G loss: 0.001599]\n",
      "epoch:22 step:17213 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.272745]\n",
      "epoch:22 step:17214 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.005160]\n",
      "epoch:22 step:17215 [D loss: 0.001218, acc.: 100.00%] [G loss: 0.104347]\n",
      "epoch:22 step:17216 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.054282]\n",
      "epoch:22 step:17217 [D loss: 0.002061, acc.: 100.00%] [G loss: 0.000362]\n",
      "epoch:22 step:17218 [D loss: 0.002993, acc.: 100.00%] [G loss: 0.010251]\n",
      "epoch:22 step:17219 [D loss: 0.006396, acc.: 100.00%] [G loss: 0.001563]\n",
      "epoch:22 step:17220 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.000498]\n",
      "epoch:22 step:17221 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.209810]\n",
      "epoch:22 step:17222 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.003841]\n",
      "epoch:22 step:17223 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.087172]\n",
      "epoch:22 step:17224 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.071451]\n",
      "epoch:22 step:17225 [D loss: 0.001248, acc.: 100.00%] [G loss: 0.035287]\n",
      "epoch:22 step:17226 [D loss: 0.002768, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:22 step:17227 [D loss: 0.001472, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:22 step:17228 [D loss: 0.002951, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:22 step:17229 [D loss: 0.003255, acc.: 100.00%] [G loss: 0.004599]\n",
      "epoch:22 step:17230 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.003454]\n",
      "epoch:22 step:17231 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.053712]\n",
      "epoch:22 step:17232 [D loss: 0.007596, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:22 step:17233 [D loss: 0.001414, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:22 step:17234 [D loss: 0.001597, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:22 step:17235 [D loss: 0.003451, acc.: 100.00%] [G loss: 0.012543]\n",
      "epoch:22 step:17236 [D loss: 0.002284, acc.: 100.00%] [G loss: 0.003624]\n",
      "epoch:22 step:17237 [D loss: 0.002661, acc.: 100.00%] [G loss: 0.000974]\n",
      "epoch:22 step:17238 [D loss: 0.005717, acc.: 100.00%] [G loss: 0.001012]\n",
      "epoch:22 step:17239 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.044131]\n",
      "epoch:22 step:17240 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.002966]\n",
      "epoch:22 step:17241 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.006316]\n",
      "epoch:22 step:17242 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:22 step:17243 [D loss: 0.009113, acc.: 100.00%] [G loss: 0.001945]\n",
      "epoch:22 step:17244 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.086819]\n",
      "epoch:22 step:17245 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.058952]\n",
      "epoch:22 step:17246 [D loss: 0.009005, acc.: 100.00%] [G loss: 0.010197]\n",
      "epoch:22 step:17247 [D loss: 0.033906, acc.: 98.44%] [G loss: 0.007425]\n",
      "epoch:22 step:17248 [D loss: 0.001025, acc.: 100.00%] [G loss: 0.009811]\n",
      "epoch:22 step:17249 [D loss: 0.001618, acc.: 100.00%] [G loss: 0.001872]\n",
      "epoch:22 step:17250 [D loss: 0.197757, acc.: 91.41%] [G loss: 0.375901]\n",
      "epoch:22 step:17251 [D loss: 0.020285, acc.: 100.00%] [G loss: 11.421222]\n",
      "epoch:22 step:17252 [D loss: 0.268527, acc.: 86.72%] [G loss: 0.001338]\n",
      "epoch:22 step:17253 [D loss: 0.003119, acc.: 100.00%] [G loss: 3.951067]\n",
      "epoch:22 step:17254 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000331]\n",
      "epoch:22 step:17255 [D loss: 0.000668, acc.: 100.00%] [G loss: 0.628676]\n",
      "epoch:22 step:17256 [D loss: 0.002071, acc.: 100.00%] [G loss: 0.000825]\n",
      "epoch:22 step:17257 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.648334]\n",
      "epoch:22 step:17258 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.002360]\n",
      "epoch:22 step:17259 [D loss: 0.001489, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:22 step:17260 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.055243]\n",
      "epoch:22 step:17261 [D loss: 0.000973, acc.: 100.00%] [G loss: 0.000745]\n",
      "epoch:22 step:17262 [D loss: 0.001339, acc.: 100.00%] [G loss: 0.283340]\n",
      "epoch:22 step:17263 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:22 step:17264 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:22 step:17265 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:22 step:17266 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000257]\n",
      "epoch:22 step:17267 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.098064]\n",
      "epoch:22 step:17268 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.005035]\n",
      "epoch:22 step:17269 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.060866]\n",
      "epoch:22 step:17270 [D loss: 0.000464, acc.: 100.00%] [G loss: 0.000496]\n",
      "epoch:22 step:17271 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000575]\n",
      "epoch:22 step:17272 [D loss: 0.001563, acc.: 100.00%] [G loss: 0.007189]\n",
      "epoch:22 step:17273 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.119078]\n",
      "epoch:22 step:17274 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001222]\n",
      "epoch:22 step:17275 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:22 step:17276 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.012796]\n",
      "epoch:22 step:17277 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.002390]\n",
      "epoch:22 step:17278 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.004006]\n",
      "epoch:22 step:17279 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:22 step:17280 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:22 step:17281 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:22 step:17282 [D loss: 0.001342, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:22 step:17283 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:22 step:17284 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.052460]\n",
      "epoch:22 step:17285 [D loss: 0.000851, acc.: 100.00%] [G loss: 0.003228]\n",
      "epoch:22 step:17286 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:22 step:17287 [D loss: 0.110576, acc.: 96.09%] [G loss: 1.007355]\n",
      "epoch:22 step:17288 [D loss: 0.012393, acc.: 100.00%] [G loss: 2.196979]\n",
      "epoch:22 step:17289 [D loss: 0.328639, acc.: 85.94%] [G loss: 0.003616]\n",
      "epoch:22 step:17290 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.001646]\n",
      "epoch:22 step:17291 [D loss: 0.181047, acc.: 91.41%] [G loss: 0.031705]\n",
      "epoch:22 step:17292 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.633175]\n",
      "epoch:22 step:17293 [D loss: 0.171111, acc.: 94.53%] [G loss: 0.073631]\n",
      "epoch:22 step:17294 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.025014]\n",
      "epoch:22 step:17295 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.002866]\n",
      "epoch:22 step:17296 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.249326]\n",
      "epoch:22 step:17297 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.005340]\n",
      "epoch:22 step:17298 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.023495]\n",
      "epoch:22 step:17299 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.001627]\n",
      "epoch:22 step:17300 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000833]\n",
      "epoch:22 step:17301 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.013940]\n",
      "epoch:22 step:17302 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001729]\n",
      "epoch:22 step:17303 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.006074]\n",
      "epoch:22 step:17304 [D loss: 0.001454, acc.: 100.00%] [G loss: 0.003724]\n",
      "epoch:22 step:17305 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.001438]\n",
      "epoch:22 step:17306 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.002469]\n",
      "epoch:22 step:17307 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.014822]\n",
      "epoch:22 step:17308 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.348266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17309 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000954]\n",
      "epoch:22 step:17310 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001158]\n",
      "epoch:22 step:17311 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000974]\n",
      "epoch:22 step:17312 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001890]\n",
      "epoch:22 step:17313 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.004061]\n",
      "epoch:22 step:17314 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.002348]\n",
      "epoch:22 step:17315 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.128504]\n",
      "epoch:22 step:17316 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000664]\n",
      "epoch:22 step:17317 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.001294]\n",
      "epoch:22 step:17318 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.004303]\n",
      "epoch:22 step:17319 [D loss: 0.001697, acc.: 100.00%] [G loss: 0.009036]\n",
      "epoch:22 step:17320 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001659]\n",
      "epoch:22 step:17321 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.002679]\n",
      "epoch:22 step:17322 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001444]\n",
      "epoch:22 step:17323 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.001534]\n",
      "epoch:22 step:17324 [D loss: 0.001426, acc.: 100.00%] [G loss: 0.003377]\n",
      "epoch:22 step:17325 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000665]\n",
      "epoch:22 step:17326 [D loss: 0.000877, acc.: 100.00%] [G loss: 0.034847]\n",
      "epoch:22 step:17327 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:22 step:17328 [D loss: 0.000501, acc.: 100.00%] [G loss: 0.000661]\n",
      "epoch:22 step:17329 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000326]\n",
      "epoch:22 step:17330 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000824]\n",
      "epoch:22 step:17331 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.001236]\n",
      "epoch:22 step:17332 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001045]\n",
      "epoch:22 step:17333 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001663]\n",
      "epoch:22 step:17334 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000736]\n",
      "epoch:22 step:17335 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.002522]\n",
      "epoch:22 step:17336 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000445]\n",
      "epoch:22 step:17337 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.001464]\n",
      "epoch:22 step:17338 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000360]\n",
      "epoch:22 step:17339 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000604]\n",
      "epoch:22 step:17340 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000264]\n",
      "epoch:22 step:17341 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000512]\n",
      "epoch:22 step:17342 [D loss: 0.000852, acc.: 100.00%] [G loss: 0.001101]\n",
      "epoch:22 step:17343 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.001258]\n",
      "epoch:22 step:17344 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.033561]\n",
      "epoch:22 step:17345 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000556]\n",
      "epoch:22 step:17346 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000919]\n",
      "epoch:22 step:17347 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000864]\n",
      "epoch:22 step:17348 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.102994]\n",
      "epoch:22 step:17349 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.005890]\n",
      "epoch:22 step:17350 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.001279]\n",
      "epoch:22 step:17351 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:22 step:17352 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000664]\n",
      "epoch:22 step:17353 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.001196]\n",
      "epoch:22 step:17354 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.005919]\n",
      "epoch:22 step:17355 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.005277]\n",
      "epoch:22 step:17356 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000934]\n",
      "epoch:22 step:17357 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000571]\n",
      "epoch:22 step:17358 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.001101]\n",
      "epoch:22 step:17359 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.002732]\n",
      "epoch:22 step:17360 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.000461]\n",
      "epoch:22 step:17361 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.001792]\n",
      "epoch:22 step:17362 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:22 step:17363 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000746]\n",
      "epoch:22 step:17364 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000465]\n",
      "epoch:22 step:17365 [D loss: 0.005876, acc.: 100.00%] [G loss: 0.000380]\n",
      "epoch:22 step:17366 [D loss: 0.016151, acc.: 100.00%] [G loss: 0.003763]\n",
      "epoch:22 step:17367 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.006823]\n",
      "epoch:22 step:17368 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.003101]\n",
      "epoch:22 step:17369 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.015128]\n",
      "epoch:22 step:17370 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.004253]\n",
      "epoch:22 step:17371 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.004788]\n",
      "epoch:22 step:17372 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.005284]\n",
      "epoch:22 step:17373 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.005789]\n",
      "epoch:22 step:17374 [D loss: 0.019334, acc.: 100.00%] [G loss: 0.004258]\n",
      "epoch:22 step:17375 [D loss: 0.001725, acc.: 100.00%] [G loss: 0.005353]\n",
      "epoch:22 step:17376 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.010120]\n",
      "epoch:22 step:17377 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.026658]\n",
      "epoch:22 step:17378 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.046773]\n",
      "epoch:22 step:17379 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.011885]\n",
      "epoch:22 step:17380 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.005085]\n",
      "epoch:22 step:17381 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.004830]\n",
      "epoch:22 step:17382 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.004653]\n",
      "epoch:22 step:17383 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.047662]\n",
      "epoch:22 step:17384 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.013599]\n",
      "epoch:22 step:17385 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.003164]\n",
      "epoch:22 step:17386 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.015096]\n",
      "epoch:22 step:17387 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.554806]\n",
      "epoch:22 step:17388 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.015900]\n",
      "epoch:22 step:17389 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.005873]\n",
      "epoch:22 step:17390 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.039831]\n",
      "epoch:22 step:17391 [D loss: 0.000948, acc.: 100.00%] [G loss: 0.005228]\n",
      "epoch:22 step:17392 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.008818]\n",
      "epoch:22 step:17393 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.019652]\n",
      "epoch:22 step:17394 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.001428]\n",
      "epoch:22 step:17395 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.016212]\n",
      "epoch:22 step:17396 [D loss: 0.005835, acc.: 100.00%] [G loss: 0.002271]\n",
      "epoch:22 step:17397 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.003801]\n",
      "epoch:22 step:17398 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.002759]\n",
      "epoch:22 step:17399 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.002559]\n",
      "epoch:22 step:17400 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.003657]\n",
      "epoch:22 step:17401 [D loss: 0.001932, acc.: 100.00%] [G loss: 0.002305]\n",
      "epoch:22 step:17402 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.001993]\n",
      "epoch:22 step:17403 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000963]\n",
      "epoch:22 step:17404 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.002207]\n",
      "epoch:22 step:17405 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.001327]\n",
      "epoch:22 step:17406 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.003860]\n",
      "epoch:22 step:17407 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001901]\n",
      "epoch:22 step:17408 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.042600]\n",
      "epoch:22 step:17409 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.007373]\n",
      "epoch:22 step:17410 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000976]\n",
      "epoch:22 step:17411 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.000809]\n",
      "epoch:22 step:17412 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.008100]\n",
      "epoch:22 step:17413 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.004202]\n",
      "epoch:22 step:17414 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.005861]\n",
      "epoch:22 step:17415 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.001002]\n",
      "epoch:22 step:17416 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000850]\n",
      "epoch:22 step:17417 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.002532]\n",
      "epoch:22 step:17418 [D loss: 0.004049, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:22 step:17419 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000264]\n",
      "epoch:22 step:17420 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000907]\n",
      "epoch:22 step:17421 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000477]\n",
      "epoch:22 step:17422 [D loss: 0.005253, acc.: 100.00%] [G loss: 0.011458]\n",
      "epoch:22 step:17423 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.007491]\n",
      "epoch:22 step:17424 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.011293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17425 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:22 step:17426 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.001187]\n",
      "epoch:22 step:17427 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.003706]\n",
      "epoch:22 step:17428 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.002345]\n",
      "epoch:22 step:17429 [D loss: 0.014956, acc.: 99.22%] [G loss: 0.000068]\n",
      "epoch:22 step:17430 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:22 step:17431 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000525]\n",
      "epoch:22 step:17432 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:22 step:17433 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.004054]\n",
      "epoch:22 step:17434 [D loss: 0.000622, acc.: 100.00%] [G loss: 0.001555]\n",
      "epoch:22 step:17435 [D loss: 0.011837, acc.: 100.00%] [G loss: 0.001183]\n",
      "epoch:22 step:17436 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.008227]\n",
      "epoch:22 step:17437 [D loss: 0.020035, acc.: 100.00%] [G loss: 0.000749]\n",
      "epoch:22 step:17438 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000401]\n",
      "epoch:22 step:17439 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.497127]\n",
      "epoch:22 step:17440 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.057000]\n",
      "epoch:22 step:17441 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.005302]\n",
      "epoch:22 step:17442 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.194757]\n",
      "epoch:22 step:17443 [D loss: 0.005513, acc.: 100.00%] [G loss: 0.001717]\n",
      "epoch:22 step:17444 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:22 step:17445 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:22 step:17446 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000500]\n",
      "epoch:22 step:17447 [D loss: 0.004410, acc.: 100.00%] [G loss: 0.000710]\n",
      "epoch:22 step:17448 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.002905]\n",
      "epoch:22 step:17449 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000980]\n",
      "epoch:22 step:17450 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.001165]\n",
      "epoch:22 step:17451 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.003631]\n",
      "epoch:22 step:17452 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:22 step:17453 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:22 step:17454 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.006919]\n",
      "epoch:22 step:17455 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:22 step:17456 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000611]\n",
      "epoch:22 step:17457 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.001174]\n",
      "epoch:22 step:17458 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:22 step:17459 [D loss: 0.002676, acc.: 100.00%] [G loss: 0.008811]\n",
      "epoch:22 step:17460 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.001794]\n",
      "epoch:22 step:17461 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.028985]\n",
      "epoch:22 step:17462 [D loss: 0.001834, acc.: 100.00%] [G loss: 0.002225]\n",
      "epoch:22 step:17463 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.002930]\n",
      "epoch:22 step:17464 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.004727]\n",
      "epoch:22 step:17465 [D loss: 0.027736, acc.: 100.00%] [G loss: 0.002197]\n",
      "epoch:22 step:17466 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.013195]\n",
      "epoch:22 step:17467 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.018651]\n",
      "epoch:22 step:17468 [D loss: 0.012365, acc.: 99.22%] [G loss: 0.007173]\n",
      "epoch:22 step:17469 [D loss: 0.004354, acc.: 100.00%] [G loss: 0.229865]\n",
      "epoch:22 step:17470 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.003548]\n",
      "epoch:22 step:17471 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000539]\n",
      "epoch:22 step:17472 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.009943]\n",
      "epoch:22 step:17473 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.843300]\n",
      "epoch:22 step:17474 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.002353]\n",
      "epoch:22 step:17475 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.002418]\n",
      "epoch:22 step:17476 [D loss: 0.001037, acc.: 100.00%] [G loss: 0.000820]\n",
      "epoch:22 step:17477 [D loss: 0.002401, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:22 step:17478 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.030622]\n",
      "epoch:22 step:17479 [D loss: 0.004906, acc.: 100.00%] [G loss: 0.001183]\n",
      "epoch:22 step:17480 [D loss: 0.007765, acc.: 100.00%] [G loss: 0.003107]\n",
      "epoch:22 step:17481 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.005142]\n",
      "epoch:22 step:17482 [D loss: 0.062834, acc.: 98.44%] [G loss: 0.002581]\n",
      "epoch:22 step:17483 [D loss: 0.001168, acc.: 100.00%] [G loss: 0.001435]\n",
      "epoch:22 step:17484 [D loss: 0.002988, acc.: 100.00%] [G loss: 0.006241]\n",
      "epoch:22 step:17485 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.278507]\n",
      "epoch:22 step:17486 [D loss: 0.168609, acc.: 94.53%] [G loss: 9.890684]\n",
      "epoch:22 step:17487 [D loss: 1.566035, acc.: 57.81%] [G loss: 1.872913]\n",
      "epoch:22 step:17488 [D loss: 0.378173, acc.: 83.59%] [G loss: 8.881089]\n",
      "epoch:22 step:17489 [D loss: 0.000319, acc.: 100.00%] [G loss: 9.493386]\n",
      "epoch:22 step:17490 [D loss: 0.071017, acc.: 96.88%] [G loss: 9.072536]\n",
      "epoch:22 step:17491 [D loss: 0.104446, acc.: 95.31%] [G loss: 7.178055]\n",
      "epoch:22 step:17492 [D loss: 0.036968, acc.: 99.22%] [G loss: 5.376400]\n",
      "epoch:22 step:17493 [D loss: 0.059105, acc.: 98.44%] [G loss: 4.370561]\n",
      "epoch:22 step:17494 [D loss: 0.037546, acc.: 99.22%] [G loss: 6.326612]\n",
      "epoch:22 step:17495 [D loss: 0.011016, acc.: 100.00%] [G loss: 0.726186]\n",
      "epoch:22 step:17496 [D loss: 0.010372, acc.: 100.00%] [G loss: 5.811611]\n",
      "epoch:22 step:17497 [D loss: 0.136991, acc.: 93.75%] [G loss: 3.656807]\n",
      "epoch:22 step:17498 [D loss: 0.117380, acc.: 96.09%] [G loss: 0.112702]\n",
      "epoch:22 step:17499 [D loss: 0.015076, acc.: 99.22%] [G loss: 5.311323]\n",
      "epoch:22 step:17500 [D loss: 0.038917, acc.: 98.44%] [G loss: 0.136142]\n",
      "epoch:22 step:17501 [D loss: 0.012684, acc.: 100.00%] [G loss: 3.200940]\n",
      "epoch:22 step:17502 [D loss: 0.130429, acc.: 95.31%] [G loss: 0.873351]\n",
      "epoch:22 step:17503 [D loss: 0.006931, acc.: 100.00%] [G loss: 0.298809]\n",
      "epoch:22 step:17504 [D loss: 0.001312, acc.: 100.00%] [G loss: 0.156798]\n",
      "epoch:22 step:17505 [D loss: 0.001755, acc.: 100.00%] [G loss: 0.018146]\n",
      "epoch:22 step:17506 [D loss: 0.002794, acc.: 100.00%] [G loss: 0.012557]\n",
      "epoch:22 step:17507 [D loss: 0.001919, acc.: 100.00%] [G loss: 0.039312]\n",
      "epoch:22 step:17508 [D loss: 0.004509, acc.: 100.00%] [G loss: 0.058398]\n",
      "epoch:22 step:17509 [D loss: 0.002896, acc.: 100.00%] [G loss: 0.020775]\n",
      "epoch:22 step:17510 [D loss: 0.000923, acc.: 100.00%] [G loss: 0.306569]\n",
      "epoch:22 step:17511 [D loss: 0.144925, acc.: 95.31%] [G loss: 0.824798]\n",
      "epoch:22 step:17512 [D loss: 0.040926, acc.: 98.44%] [G loss: 2.477182]\n",
      "epoch:22 step:17513 [D loss: 0.176476, acc.: 92.19%] [G loss: 0.083491]\n",
      "epoch:22 step:17514 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.246104]\n",
      "epoch:22 step:17515 [D loss: 0.001995, acc.: 100.00%] [G loss: 0.172407]\n",
      "epoch:22 step:17516 [D loss: 0.000714, acc.: 100.00%] [G loss: 0.105711]\n",
      "epoch:22 step:17517 [D loss: 0.004371, acc.: 100.00%] [G loss: 1.264126]\n",
      "epoch:22 step:17518 [D loss: 0.001916, acc.: 100.00%] [G loss: 0.062360]\n",
      "epoch:22 step:17519 [D loss: 0.002545, acc.: 100.00%] [G loss: 1.156062]\n",
      "epoch:22 step:17520 [D loss: 0.034834, acc.: 100.00%] [G loss: 0.004423]\n",
      "epoch:22 step:17521 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.020455]\n",
      "epoch:22 step:17522 [D loss: 0.002793, acc.: 100.00%] [G loss: 0.056709]\n",
      "epoch:22 step:17523 [D loss: 0.000777, acc.: 100.00%] [G loss: 0.749095]\n",
      "epoch:22 step:17524 [D loss: 0.086358, acc.: 98.44%] [G loss: 1.128118]\n",
      "epoch:22 step:17525 [D loss: 0.008769, acc.: 99.22%] [G loss: 0.057607]\n",
      "epoch:22 step:17526 [D loss: 0.024535, acc.: 99.22%] [G loss: 0.033511]\n",
      "epoch:22 step:17527 [D loss: 0.026712, acc.: 98.44%] [G loss: 0.174315]\n",
      "epoch:22 step:17528 [D loss: 0.002645, acc.: 100.00%] [G loss: 0.013093]\n",
      "epoch:22 step:17529 [D loss: 0.009708, acc.: 100.00%] [G loss: 0.029810]\n",
      "epoch:22 step:17530 [D loss: 0.017541, acc.: 100.00%] [G loss: 0.022285]\n",
      "epoch:22 step:17531 [D loss: 0.009552, acc.: 100.00%] [G loss: 0.825122]\n",
      "epoch:22 step:17532 [D loss: 0.015522, acc.: 100.00%] [G loss: 0.004270]\n",
      "epoch:22 step:17533 [D loss: 0.023544, acc.: 100.00%] [G loss: 0.016495]\n",
      "epoch:22 step:17534 [D loss: 0.004969, acc.: 100.00%] [G loss: 0.067773]\n",
      "epoch:22 step:17535 [D loss: 0.031151, acc.: 98.44%] [G loss: 0.050034]\n",
      "epoch:22 step:17536 [D loss: 0.001105, acc.: 100.00%] [G loss: 0.013524]\n",
      "epoch:22 step:17537 [D loss: 0.002461, acc.: 100.00%] [G loss: 0.016507]\n",
      "epoch:22 step:17538 [D loss: 0.004085, acc.: 100.00%] [G loss: 0.008503]\n",
      "epoch:22 step:17539 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.001428]\n",
      "epoch:22 step:17540 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.029272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17541 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.012281]\n",
      "epoch:22 step:17542 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.001940]\n",
      "epoch:22 step:17543 [D loss: 0.010290, acc.: 99.22%] [G loss: 0.002750]\n",
      "epoch:22 step:17544 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.000421]\n",
      "epoch:22 step:17545 [D loss: 0.001358, acc.: 100.00%] [G loss: 0.004210]\n",
      "epoch:22 step:17546 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.002372]\n",
      "epoch:22 step:17547 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.014566]\n",
      "epoch:22 step:17548 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000731]\n",
      "epoch:22 step:17549 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.018551]\n",
      "epoch:22 step:17550 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.002743]\n",
      "epoch:22 step:17551 [D loss: 0.001749, acc.: 100.00%] [G loss: 0.009981]\n",
      "epoch:22 step:17552 [D loss: 0.000512, acc.: 100.00%] [G loss: 0.012182]\n",
      "epoch:22 step:17553 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000452]\n",
      "epoch:22 step:17554 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.070053]\n",
      "epoch:22 step:17555 [D loss: 0.001897, acc.: 100.00%] [G loss: 0.004429]\n",
      "epoch:22 step:17556 [D loss: 0.003220, acc.: 100.00%] [G loss: 0.011295]\n",
      "epoch:22 step:17557 [D loss: 0.004936, acc.: 100.00%] [G loss: 0.017879]\n",
      "epoch:22 step:17558 [D loss: 0.000785, acc.: 100.00%] [G loss: 0.003121]\n",
      "epoch:22 step:17559 [D loss: 0.000501, acc.: 100.00%] [G loss: 0.014992]\n",
      "epoch:22 step:17560 [D loss: 0.001587, acc.: 100.00%] [G loss: 0.020454]\n",
      "epoch:22 step:17561 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.270994]\n",
      "epoch:22 step:17562 [D loss: 0.007153, acc.: 100.00%] [G loss: 0.395588]\n",
      "epoch:22 step:17563 [D loss: 0.506183, acc.: 75.78%] [G loss: 12.185504]\n",
      "epoch:22 step:17564 [D loss: 2.477070, acc.: 51.56%] [G loss: 0.740179]\n",
      "epoch:22 step:17565 [D loss: 0.137778, acc.: 97.66%] [G loss: 0.299204]\n",
      "epoch:22 step:17566 [D loss: 0.003035, acc.: 100.00%] [G loss: 0.100747]\n",
      "epoch:22 step:17567 [D loss: 0.003572, acc.: 100.00%] [G loss: 2.687440]\n",
      "epoch:22 step:17568 [D loss: 0.003585, acc.: 100.00%] [G loss: 0.037352]\n",
      "epoch:22 step:17569 [D loss: 0.004452, acc.: 100.00%] [G loss: 0.021423]\n",
      "epoch:22 step:17570 [D loss: 0.004095, acc.: 100.00%] [G loss: 0.011990]\n",
      "epoch:22 step:17571 [D loss: 0.015940, acc.: 99.22%] [G loss: 0.851077]\n",
      "epoch:22 step:17572 [D loss: 0.001875, acc.: 100.00%] [G loss: 0.029729]\n",
      "epoch:22 step:17573 [D loss: 0.001468, acc.: 100.00%] [G loss: 0.427039]\n",
      "epoch:22 step:17574 [D loss: 0.002207, acc.: 100.00%] [G loss: 1.443152]\n",
      "epoch:22 step:17575 [D loss: 0.004448, acc.: 100.00%] [G loss: 0.011032]\n",
      "epoch:22 step:17576 [D loss: 0.002631, acc.: 100.00%] [G loss: 0.006133]\n",
      "epoch:22 step:17577 [D loss: 0.001784, acc.: 100.00%] [G loss: 0.025521]\n",
      "epoch:22 step:17578 [D loss: 0.002536, acc.: 100.00%] [G loss: 0.001362]\n",
      "epoch:22 step:17579 [D loss: 0.002305, acc.: 100.00%] [G loss: 0.079736]\n",
      "epoch:22 step:17580 [D loss: 0.003831, acc.: 100.00%] [G loss: 0.091787]\n",
      "epoch:22 step:17581 [D loss: 0.002702, acc.: 100.00%] [G loss: 0.005252]\n",
      "epoch:22 step:17582 [D loss: 0.003227, acc.: 100.00%] [G loss: 0.204684]\n",
      "epoch:22 step:17583 [D loss: 0.009175, acc.: 100.00%] [G loss: 0.119442]\n",
      "epoch:22 step:17584 [D loss: 0.002253, acc.: 100.00%] [G loss: 0.053184]\n",
      "epoch:22 step:17585 [D loss: 0.003098, acc.: 100.00%] [G loss: 0.444699]\n",
      "epoch:22 step:17586 [D loss: 0.045360, acc.: 99.22%] [G loss: 0.167354]\n",
      "epoch:22 step:17587 [D loss: 0.016650, acc.: 100.00%] [G loss: 0.023506]\n",
      "epoch:22 step:17588 [D loss: 0.011734, acc.: 100.00%] [G loss: 0.070620]\n",
      "epoch:22 step:17589 [D loss: 0.001691, acc.: 100.00%] [G loss: 0.003882]\n",
      "epoch:22 step:17590 [D loss: 0.001824, acc.: 100.00%] [G loss: 0.002364]\n",
      "epoch:22 step:17591 [D loss: 0.005508, acc.: 100.00%] [G loss: 0.227776]\n",
      "epoch:22 step:17592 [D loss: 0.001571, acc.: 100.00%] [G loss: 0.005113]\n",
      "epoch:22 step:17593 [D loss: 0.013787, acc.: 100.00%] [G loss: 0.000919]\n",
      "epoch:22 step:17594 [D loss: 0.008859, acc.: 100.00%] [G loss: 0.299993]\n",
      "epoch:22 step:17595 [D loss: 0.025483, acc.: 100.00%] [G loss: 0.012513]\n",
      "epoch:22 step:17596 [D loss: 0.008400, acc.: 100.00%] [G loss: 0.318626]\n",
      "epoch:22 step:17597 [D loss: 0.003256, acc.: 100.00%] [G loss: 0.015858]\n",
      "epoch:22 step:17598 [D loss: 0.052008, acc.: 100.00%] [G loss: 0.016143]\n",
      "epoch:22 step:17599 [D loss: 0.010316, acc.: 100.00%] [G loss: 0.172335]\n",
      "epoch:22 step:17600 [D loss: 0.001341, acc.: 100.00%] [G loss: 0.018071]\n",
      "epoch:22 step:17601 [D loss: 0.006783, acc.: 100.00%] [G loss: 0.023609]\n",
      "epoch:22 step:17602 [D loss: 0.010877, acc.: 100.00%] [G loss: 0.042051]\n",
      "epoch:22 step:17603 [D loss: 0.010620, acc.: 100.00%] [G loss: 0.008954]\n",
      "epoch:22 step:17604 [D loss: 0.023536, acc.: 99.22%] [G loss: 1.629371]\n",
      "epoch:22 step:17605 [D loss: 0.307295, acc.: 86.72%] [G loss: 5.252101]\n",
      "epoch:22 step:17606 [D loss: 0.844623, acc.: 67.19%] [G loss: 0.005247]\n",
      "epoch:22 step:17607 [D loss: 0.250796, acc.: 91.41%] [G loss: 3.996361]\n",
      "epoch:22 step:17608 [D loss: 0.004496, acc.: 100.00%] [G loss: 1.518344]\n",
      "epoch:22 step:17609 [D loss: 0.045719, acc.: 96.88%] [G loss: 4.186741]\n",
      "epoch:22 step:17610 [D loss: 0.026611, acc.: 99.22%] [G loss: 2.532372]\n",
      "epoch:22 step:17611 [D loss: 0.005855, acc.: 100.00%] [G loss: 2.246789]\n",
      "epoch:22 step:17612 [D loss: 0.012591, acc.: 100.00%] [G loss: 0.729158]\n",
      "epoch:22 step:17613 [D loss: 0.004962, acc.: 100.00%] [G loss: 0.063588]\n",
      "epoch:22 step:17614 [D loss: 0.034648, acc.: 98.44%] [G loss: 0.138675]\n",
      "epoch:22 step:17615 [D loss: 0.001472, acc.: 100.00%] [G loss: 0.107468]\n",
      "epoch:22 step:17616 [D loss: 0.002812, acc.: 100.00%] [G loss: 0.082631]\n",
      "epoch:22 step:17617 [D loss: 0.001183, acc.: 100.00%] [G loss: 0.060231]\n",
      "epoch:22 step:17618 [D loss: 0.001278, acc.: 100.00%] [G loss: 0.025182]\n",
      "epoch:22 step:17619 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.949741]\n",
      "epoch:22 step:17620 [D loss: 0.045135, acc.: 100.00%] [G loss: 0.062721]\n",
      "epoch:22 step:17621 [D loss: 0.009215, acc.: 99.22%] [G loss: 0.219779]\n",
      "epoch:22 step:17622 [D loss: 0.001361, acc.: 100.00%] [G loss: 0.034530]\n",
      "epoch:22 step:17623 [D loss: 0.004231, acc.: 100.00%] [G loss: 0.144271]\n",
      "epoch:22 step:17624 [D loss: 0.000515, acc.: 100.00%] [G loss: 0.094497]\n",
      "epoch:22 step:17625 [D loss: 0.000686, acc.: 100.00%] [G loss: 0.031907]\n",
      "epoch:22 step:17626 [D loss: 0.010314, acc.: 99.22%] [G loss: 0.024257]\n",
      "epoch:22 step:17627 [D loss: 0.002078, acc.: 100.00%] [G loss: 0.024834]\n",
      "epoch:22 step:17628 [D loss: 0.003711, acc.: 100.00%] [G loss: 0.002541]\n",
      "epoch:22 step:17629 [D loss: 0.001124, acc.: 100.00%] [G loss: 0.006630]\n",
      "epoch:22 step:17630 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.016116]\n",
      "epoch:22 step:17631 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.011022]\n",
      "epoch:22 step:17632 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.095756]\n",
      "epoch:22 step:17633 [D loss: 0.000735, acc.: 100.00%] [G loss: 0.007980]\n",
      "epoch:22 step:17634 [D loss: 0.006389, acc.: 100.00%] [G loss: 0.011980]\n",
      "epoch:22 step:17635 [D loss: 0.008796, acc.: 100.00%] [G loss: 0.016676]\n",
      "epoch:22 step:17636 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.002284]\n",
      "epoch:22 step:17637 [D loss: 0.000820, acc.: 100.00%] [G loss: 0.781144]\n",
      "epoch:22 step:17638 [D loss: 0.001285, acc.: 100.00%] [G loss: 0.020685]\n",
      "epoch:22 step:17639 [D loss: 0.025064, acc.: 100.00%] [G loss: 0.005711]\n",
      "epoch:22 step:17640 [D loss: 0.000698, acc.: 100.00%] [G loss: 0.128429]\n",
      "epoch:22 step:17641 [D loss: 0.005915, acc.: 100.00%] [G loss: 0.077603]\n",
      "epoch:22 step:17642 [D loss: 0.008213, acc.: 100.00%] [G loss: 0.035169]\n",
      "epoch:22 step:17643 [D loss: 0.100439, acc.: 98.44%] [G loss: 0.455762]\n",
      "epoch:22 step:17644 [D loss: 0.010300, acc.: 100.00%] [G loss: 1.953134]\n",
      "epoch:22 step:17645 [D loss: 0.257219, acc.: 89.84%] [G loss: 0.013505]\n",
      "epoch:22 step:17646 [D loss: 0.090105, acc.: 96.88%] [G loss: 0.060883]\n",
      "epoch:22 step:17647 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.707648]\n",
      "epoch:22 step:17648 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.268233]\n",
      "epoch:22 step:17649 [D loss: 0.007650, acc.: 100.00%] [G loss: 0.418686]\n",
      "epoch:22 step:17650 [D loss: 0.000593, acc.: 100.00%] [G loss: 1.134429]\n",
      "epoch:22 step:17651 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.327383]\n",
      "epoch:22 step:17652 [D loss: 0.000770, acc.: 100.00%] [G loss: 0.230377]\n",
      "epoch:22 step:17653 [D loss: 0.000932, acc.: 100.00%] [G loss: 0.033682]\n",
      "epoch:22 step:17654 [D loss: 0.009252, acc.: 100.00%] [G loss: 0.014900]\n",
      "epoch:22 step:17655 [D loss: 0.001582, acc.: 100.00%] [G loss: 0.151138]\n",
      "epoch:22 step:17656 [D loss: 0.036382, acc.: 100.00%] [G loss: 0.015774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17657 [D loss: 0.001182, acc.: 100.00%] [G loss: 0.209216]\n",
      "epoch:22 step:17658 [D loss: 0.001844, acc.: 100.00%] [G loss: 1.156936]\n",
      "epoch:22 step:17659 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.168520]\n",
      "epoch:22 step:17660 [D loss: 0.028206, acc.: 99.22%] [G loss: 0.006920]\n",
      "epoch:22 step:17661 [D loss: 0.000941, acc.: 100.00%] [G loss: 0.595421]\n",
      "epoch:22 step:17662 [D loss: 0.004071, acc.: 100.00%] [G loss: 0.003068]\n",
      "epoch:22 step:17663 [D loss: 0.002117, acc.: 100.00%] [G loss: 0.143832]\n",
      "epoch:22 step:17664 [D loss: 0.005829, acc.: 100.00%] [G loss: 0.002059]\n",
      "epoch:22 step:17665 [D loss: 0.000687, acc.: 100.00%] [G loss: 0.002233]\n",
      "epoch:22 step:17666 [D loss: 0.002042, acc.: 100.00%] [G loss: 0.019981]\n",
      "epoch:22 step:17667 [D loss: 0.000587, acc.: 100.00%] [G loss: 0.114442]\n",
      "epoch:22 step:17668 [D loss: 0.001474, acc.: 100.00%] [G loss: 0.096422]\n",
      "epoch:22 step:17669 [D loss: 0.002232, acc.: 100.00%] [G loss: 0.001335]\n",
      "epoch:22 step:17670 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:22 step:17671 [D loss: 0.004601, acc.: 100.00%] [G loss: 0.021459]\n",
      "epoch:22 step:17672 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.021438]\n",
      "epoch:22 step:17673 [D loss: 0.001220, acc.: 100.00%] [G loss: 0.058924]\n",
      "epoch:22 step:17674 [D loss: 0.002134, acc.: 100.00%] [G loss: 0.006153]\n",
      "epoch:22 step:17675 [D loss: 0.078246, acc.: 98.44%] [G loss: 0.063291]\n",
      "epoch:22 step:17676 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.571961]\n",
      "epoch:22 step:17677 [D loss: 0.069760, acc.: 96.88%] [G loss: 3.309888]\n",
      "epoch:22 step:17678 [D loss: 0.056265, acc.: 98.44%] [G loss: 0.001714]\n",
      "epoch:22 step:17679 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:22 step:17680 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.001122]\n",
      "epoch:22 step:17681 [D loss: 0.001552, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:22 step:17682 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.001936]\n",
      "epoch:22 step:17683 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.009754]\n",
      "epoch:22 step:17684 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.001110]\n",
      "epoch:22 step:17685 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000537]\n",
      "epoch:22 step:17686 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:22 step:17687 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.000398]\n",
      "epoch:22 step:17688 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.447233]\n",
      "epoch:22 step:17689 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.284994]\n",
      "epoch:22 step:17690 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000706]\n",
      "epoch:22 step:17691 [D loss: 0.000913, acc.: 100.00%] [G loss: 0.001170]\n",
      "epoch:22 step:17692 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:22 step:17693 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:22 step:17694 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.058381]\n",
      "epoch:22 step:17695 [D loss: 0.001175, acc.: 100.00%] [G loss: 0.000490]\n",
      "epoch:22 step:17696 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.055481]\n",
      "epoch:22 step:17697 [D loss: 0.005671, acc.: 100.00%] [G loss: 0.037430]\n",
      "epoch:22 step:17698 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:22 step:17699 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.142841]\n",
      "epoch:22 step:17700 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.088452]\n",
      "epoch:22 step:17701 [D loss: 0.003556, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:22 step:17702 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000326]\n",
      "epoch:22 step:17703 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000358]\n",
      "epoch:22 step:17704 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:22 step:17705 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:22 step:17706 [D loss: 0.002622, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:22 step:17707 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000194]\n",
      "epoch:22 step:17708 [D loss: 0.012974, acc.: 100.00%] [G loss: 0.007365]\n",
      "epoch:22 step:17709 [D loss: 0.001106, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:22 step:17710 [D loss: 0.226153, acc.: 92.19%] [G loss: 2.923341]\n",
      "epoch:22 step:17711 [D loss: 0.404695, acc.: 82.81%] [G loss: 0.437372]\n",
      "epoch:22 step:17712 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.150913]\n",
      "epoch:22 step:17713 [D loss: 0.002490, acc.: 100.00%] [G loss: 0.278695]\n",
      "epoch:22 step:17714 [D loss: 0.090618, acc.: 97.66%] [G loss: 0.578116]\n",
      "epoch:22 step:17715 [D loss: 0.000543, acc.: 100.00%] [G loss: 0.499963]\n",
      "epoch:22 step:17716 [D loss: 0.002495, acc.: 100.00%] [G loss: 1.053357]\n",
      "epoch:22 step:17717 [D loss: 0.016361, acc.: 99.22%] [G loss: 0.414435]\n",
      "epoch:22 step:17718 [D loss: 0.015888, acc.: 99.22%] [G loss: 0.139048]\n",
      "epoch:22 step:17719 [D loss: 0.363271, acc.: 83.59%] [G loss: 1.466652]\n",
      "epoch:22 step:17720 [D loss: 0.001009, acc.: 100.00%] [G loss: 3.212630]\n",
      "epoch:22 step:17721 [D loss: 0.058149, acc.: 98.44%] [G loss: 1.786045]\n",
      "epoch:22 step:17722 [D loss: 0.010535, acc.: 100.00%] [G loss: 0.852246]\n",
      "epoch:22 step:17723 [D loss: 0.053537, acc.: 98.44%] [G loss: 0.012777]\n",
      "epoch:22 step:17724 [D loss: 0.051830, acc.: 96.88%] [G loss: 0.172316]\n",
      "epoch:22 step:17725 [D loss: 0.004997, acc.: 100.00%] [G loss: 0.182341]\n",
      "epoch:22 step:17726 [D loss: 0.102622, acc.: 96.88%] [G loss: 4.232285]\n",
      "epoch:22 step:17727 [D loss: 0.007891, acc.: 100.00%] [G loss: 5.389857]\n",
      "epoch:22 step:17728 [D loss: 0.060780, acc.: 98.44%] [G loss: 5.130673]\n",
      "epoch:22 step:17729 [D loss: 0.082534, acc.: 96.88%] [G loss: 2.846029]\n",
      "epoch:22 step:17730 [D loss: 0.036541, acc.: 100.00%] [G loss: 4.609970]\n",
      "epoch:22 step:17731 [D loss: 0.007917, acc.: 100.00%] [G loss: 4.112638]\n",
      "epoch:22 step:17732 [D loss: 0.027576, acc.: 100.00%] [G loss: 3.906353]\n",
      "epoch:22 step:17733 [D loss: 0.022060, acc.: 100.00%] [G loss: 3.097959]\n",
      "epoch:22 step:17734 [D loss: 0.029286, acc.: 100.00%] [G loss: 4.105992]\n",
      "epoch:22 step:17735 [D loss: 0.009089, acc.: 100.00%] [G loss: 1.173407]\n",
      "epoch:22 step:17736 [D loss: 0.026467, acc.: 98.44%] [G loss: 4.672718]\n",
      "epoch:22 step:17737 [D loss: 0.035838, acc.: 99.22%] [G loss: 3.248501]\n",
      "epoch:22 step:17738 [D loss: 0.047117, acc.: 98.44%] [G loss: 4.819476]\n",
      "epoch:22 step:17739 [D loss: 0.044288, acc.: 99.22%] [G loss: 3.299743]\n",
      "epoch:22 step:17740 [D loss: 0.014331, acc.: 99.22%] [G loss: 2.595660]\n",
      "epoch:22 step:17741 [D loss: 0.023973, acc.: 99.22%] [G loss: 0.222660]\n",
      "epoch:22 step:17742 [D loss: 0.001680, acc.: 100.00%] [G loss: 5.615194]\n",
      "epoch:22 step:17743 [D loss: 0.024902, acc.: 98.44%] [G loss: 3.318943]\n",
      "epoch:22 step:17744 [D loss: 0.004706, acc.: 100.00%] [G loss: 3.194967]\n",
      "epoch:22 step:17745 [D loss: 0.015128, acc.: 100.00%] [G loss: 0.029502]\n",
      "epoch:22 step:17746 [D loss: 0.005425, acc.: 100.00%] [G loss: 1.730859]\n",
      "epoch:22 step:17747 [D loss: 0.051944, acc.: 99.22%] [G loss: 1.334028]\n",
      "epoch:22 step:17748 [D loss: 0.020609, acc.: 99.22%] [G loss: 0.844815]\n",
      "epoch:22 step:17749 [D loss: 0.042522, acc.: 100.00%] [G loss: 0.449110]\n",
      "epoch:22 step:17750 [D loss: 0.002225, acc.: 100.00%] [G loss: 0.042689]\n",
      "epoch:22 step:17751 [D loss: 0.001856, acc.: 100.00%] [G loss: 0.032118]\n",
      "epoch:22 step:17752 [D loss: 0.003738, acc.: 100.00%] [G loss: 1.162293]\n",
      "epoch:22 step:17753 [D loss: 0.035154, acc.: 98.44%] [G loss: 0.260711]\n",
      "epoch:22 step:17754 [D loss: 0.012243, acc.: 100.00%] [G loss: 0.124623]\n",
      "epoch:22 step:17755 [D loss: 0.005591, acc.: 100.00%] [G loss: 0.117372]\n",
      "epoch:22 step:17756 [D loss: 0.010882, acc.: 100.00%] [G loss: 0.007069]\n",
      "epoch:22 step:17757 [D loss: 0.002345, acc.: 100.00%] [G loss: 0.210858]\n",
      "epoch:22 step:17758 [D loss: 0.008377, acc.: 100.00%] [G loss: 0.043052]\n",
      "epoch:22 step:17759 [D loss: 0.007356, acc.: 100.00%] [G loss: 0.000564]\n",
      "epoch:22 step:17760 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.210001]\n",
      "epoch:22 step:17761 [D loss: 0.000167, acc.: 100.00%] [G loss: 1.039628]\n",
      "epoch:22 step:17762 [D loss: 0.009636, acc.: 100.00%] [G loss: 0.022912]\n",
      "epoch:22 step:17763 [D loss: 0.043404, acc.: 99.22%] [G loss: 0.197856]\n",
      "epoch:22 step:17764 [D loss: 0.016563, acc.: 99.22%] [G loss: 0.624961]\n",
      "epoch:22 step:17765 [D loss: 0.031070, acc.: 99.22%] [G loss: 0.063335]\n",
      "epoch:22 step:17766 [D loss: 0.286833, acc.: 86.72%] [G loss: 7.931883]\n",
      "epoch:22 step:17767 [D loss: 0.544410, acc.: 75.00%] [G loss: 2.308910]\n",
      "epoch:22 step:17768 [D loss: 0.039082, acc.: 99.22%] [G loss: 3.518401]\n",
      "epoch:22 step:17769 [D loss: 0.069608, acc.: 97.66%] [G loss: 0.661530]\n",
      "epoch:22 step:17770 [D loss: 0.019530, acc.: 100.00%] [G loss: 2.303381]\n",
      "epoch:22 step:17771 [D loss: 0.016473, acc.: 100.00%] [G loss: 1.104017]\n",
      "epoch:22 step:17772 [D loss: 0.001940, acc.: 100.00%] [G loss: 1.066581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17773 [D loss: 0.408070, acc.: 85.16%] [G loss: 1.581594]\n",
      "epoch:22 step:17774 [D loss: 0.017112, acc.: 99.22%] [G loss: 2.867764]\n",
      "epoch:22 step:17775 [D loss: 0.710847, acc.: 71.09%] [G loss: 0.000383]\n",
      "epoch:22 step:17776 [D loss: 0.046076, acc.: 97.66%] [G loss: 1.380096]\n",
      "epoch:22 step:17777 [D loss: 0.017332, acc.: 100.00%] [G loss: 0.053959]\n",
      "epoch:22 step:17778 [D loss: 0.005838, acc.: 100.00%] [G loss: 0.825862]\n",
      "epoch:22 step:17779 [D loss: 0.027221, acc.: 99.22%] [G loss: 0.061403]\n",
      "epoch:22 step:17780 [D loss: 0.121202, acc.: 95.31%] [G loss: 2.577980]\n",
      "epoch:22 step:17781 [D loss: 0.017622, acc.: 100.00%] [G loss: 4.966962]\n",
      "epoch:22 step:17782 [D loss: 0.041679, acc.: 99.22%] [G loss: 1.249015]\n",
      "epoch:22 step:17783 [D loss: 0.372314, acc.: 88.28%] [G loss: 8.541323]\n",
      "epoch:22 step:17784 [D loss: 0.071826, acc.: 96.09%] [G loss: 5.757757]\n",
      "epoch:22 step:17785 [D loss: 0.110407, acc.: 96.88%] [G loss: 6.280771]\n",
      "epoch:22 step:17786 [D loss: 0.056913, acc.: 98.44%] [G loss: 3.577139]\n",
      "epoch:22 step:17787 [D loss: 0.003709, acc.: 100.00%] [G loss: 2.860855]\n",
      "epoch:22 step:17788 [D loss: 0.000827, acc.: 100.00%] [G loss: 1.612056]\n",
      "epoch:22 step:17789 [D loss: 0.010014, acc.: 100.00%] [G loss: 1.870121]\n",
      "epoch:22 step:17790 [D loss: 0.009900, acc.: 100.00%] [G loss: 1.136551]\n",
      "epoch:22 step:17791 [D loss: 0.313015, acc.: 88.28%] [G loss: 6.678291]\n",
      "epoch:22 step:17792 [D loss: 0.467852, acc.: 77.34%] [G loss: 2.777863]\n",
      "epoch:22 step:17793 [D loss: 0.009588, acc.: 100.00%] [G loss: 0.668626]\n",
      "epoch:22 step:17794 [D loss: 0.000890, acc.: 100.00%] [G loss: 0.769315]\n",
      "epoch:22 step:17795 [D loss: 0.092732, acc.: 98.44%] [G loss: 1.352319]\n",
      "epoch:22 step:17796 [D loss: 0.001834, acc.: 100.00%] [G loss: 3.598735]\n",
      "epoch:22 step:17797 [D loss: 0.017256, acc.: 100.00%] [G loss: 1.333676]\n",
      "epoch:22 step:17798 [D loss: 0.031202, acc.: 100.00%] [G loss: 3.022228]\n",
      "epoch:22 step:17799 [D loss: 0.201156, acc.: 93.75%] [G loss: 0.406458]\n",
      "epoch:22 step:17800 [D loss: 0.010041, acc.: 100.00%] [G loss: 0.368772]\n",
      "epoch:22 step:17801 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.170100]\n",
      "epoch:22 step:17802 [D loss: 0.009819, acc.: 99.22%] [G loss: 0.084298]\n",
      "epoch:22 step:17803 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.011893]\n",
      "epoch:22 step:17804 [D loss: 0.002643, acc.: 100.00%] [G loss: 4.365471]\n",
      "epoch:22 step:17805 [D loss: 0.039350, acc.: 99.22%] [G loss: 0.165600]\n",
      "epoch:22 step:17806 [D loss: 0.077674, acc.: 96.88%] [G loss: 0.061879]\n",
      "epoch:22 step:17807 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.224187]\n",
      "epoch:22 step:17808 [D loss: 0.001933, acc.: 100.00%] [G loss: 4.997597]\n",
      "epoch:22 step:17809 [D loss: 0.011409, acc.: 100.00%] [G loss: 0.065749]\n",
      "epoch:22 step:17810 [D loss: 0.006152, acc.: 100.00%] [G loss: 3.589285]\n",
      "epoch:22 step:17811 [D loss: 0.006225, acc.: 100.00%] [G loss: 0.077568]\n",
      "epoch:22 step:17812 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.006578]\n",
      "epoch:22 step:17813 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.017151]\n",
      "epoch:22 step:17814 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.011558]\n",
      "epoch:22 step:17815 [D loss: 0.000111, acc.: 100.00%] [G loss: 1.350550]\n",
      "epoch:22 step:17816 [D loss: 0.000871, acc.: 100.00%] [G loss: 0.477312]\n",
      "epoch:22 step:17817 [D loss: 0.001290, acc.: 100.00%] [G loss: 0.005945]\n",
      "epoch:22 step:17818 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.016124]\n",
      "epoch:22 step:17819 [D loss: 0.001880, acc.: 100.00%] [G loss: 0.002466]\n",
      "epoch:22 step:17820 [D loss: 0.012531, acc.: 100.00%] [G loss: 0.040059]\n",
      "epoch:22 step:17821 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.070276]\n",
      "epoch:22 step:17822 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.006930]\n",
      "epoch:22 step:17823 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.029431]\n",
      "epoch:22 step:17824 [D loss: 0.006206, acc.: 100.00%] [G loss: 0.001893]\n",
      "epoch:22 step:17825 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.001287]\n",
      "epoch:22 step:17826 [D loss: 0.003552, acc.: 100.00%] [G loss: 0.010963]\n",
      "epoch:22 step:17827 [D loss: 0.004778, acc.: 100.00%] [G loss: 0.158382]\n",
      "epoch:22 step:17828 [D loss: 0.125203, acc.: 95.31%] [G loss: 0.295989]\n",
      "epoch:22 step:17829 [D loss: 0.000489, acc.: 100.00%] [G loss: 0.233489]\n",
      "epoch:22 step:17830 [D loss: 0.006567, acc.: 100.00%] [G loss: 1.244405]\n",
      "epoch:22 step:17831 [D loss: 0.061411, acc.: 96.88%] [G loss: 0.000412]\n",
      "epoch:22 step:17832 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.001606]\n",
      "epoch:22 step:17833 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.000648]\n",
      "epoch:22 step:17834 [D loss: 0.002189, acc.: 100.00%] [G loss: 0.001236]\n",
      "epoch:22 step:17835 [D loss: 0.026822, acc.: 100.00%] [G loss: 0.002459]\n",
      "epoch:22 step:17836 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.030547]\n",
      "epoch:22 step:17837 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.874016]\n",
      "epoch:22 step:17838 [D loss: 0.004296, acc.: 100.00%] [G loss: 0.007205]\n",
      "epoch:22 step:17839 [D loss: 0.107719, acc.: 96.09%] [G loss: 4.909620]\n",
      "epoch:22 step:17840 [D loss: 0.137148, acc.: 92.97%] [G loss: 1.557461]\n",
      "epoch:22 step:17841 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.735707]\n",
      "epoch:22 step:17842 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.333121]\n",
      "epoch:22 step:17843 [D loss: 0.001499, acc.: 100.00%] [G loss: 0.147993]\n",
      "epoch:22 step:17844 [D loss: 0.002887, acc.: 100.00%] [G loss: 0.829441]\n",
      "epoch:22 step:17845 [D loss: 0.001435, acc.: 100.00%] [G loss: 0.113075]\n",
      "epoch:22 step:17846 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.004861]\n",
      "epoch:22 step:17847 [D loss: 0.090428, acc.: 95.31%] [G loss: 0.000103]\n",
      "epoch:22 step:17848 [D loss: 0.060972, acc.: 98.44%] [G loss: 0.030924]\n",
      "epoch:22 step:17849 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.011928]\n",
      "epoch:22 step:17850 [D loss: 0.009752, acc.: 99.22%] [G loss: 0.093671]\n",
      "epoch:22 step:17851 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.018068]\n",
      "epoch:22 step:17852 [D loss: 0.035982, acc.: 99.22%] [G loss: 0.007673]\n",
      "epoch:22 step:17853 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.263644]\n",
      "epoch:22 step:17854 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.001701]\n",
      "epoch:22 step:17855 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.002096]\n",
      "epoch:22 step:17856 [D loss: 0.000017, acc.: 100.00%] [G loss: 1.845823]\n",
      "epoch:22 step:17857 [D loss: 0.005858, acc.: 100.00%] [G loss: 0.019323]\n",
      "epoch:22 step:17858 [D loss: 0.018434, acc.: 100.00%] [G loss: 0.050191]\n",
      "epoch:22 step:17859 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.121652]\n",
      "epoch:22 step:17860 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.028301]\n",
      "epoch:22 step:17861 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.003967]\n",
      "epoch:22 step:17862 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.119217]\n",
      "epoch:22 step:17863 [D loss: 0.000428, acc.: 100.00%] [G loss: 0.006279]\n",
      "epoch:22 step:17864 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.001081]\n",
      "epoch:22 step:17865 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.137961]\n",
      "epoch:22 step:17866 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.011227]\n",
      "epoch:22 step:17867 [D loss: 0.000799, acc.: 100.00%] [G loss: 0.004600]\n",
      "epoch:22 step:17868 [D loss: 0.000783, acc.: 100.00%] [G loss: 0.044146]\n",
      "epoch:22 step:17869 [D loss: 0.018761, acc.: 99.22%] [G loss: 0.003321]\n",
      "epoch:22 step:17870 [D loss: 0.021601, acc.: 100.00%] [G loss: 0.002396]\n",
      "epoch:22 step:17871 [D loss: 0.004968, acc.: 100.00%] [G loss: 0.120730]\n",
      "epoch:22 step:17872 [D loss: 0.000808, acc.: 100.00%] [G loss: 0.178208]\n",
      "epoch:22 step:17873 [D loss: 0.040269, acc.: 98.44%] [G loss: 0.227212]\n",
      "epoch:22 step:17874 [D loss: 1.742364, acc.: 32.03%] [G loss: 8.035461]\n",
      "epoch:22 step:17875 [D loss: 0.150596, acc.: 94.53%] [G loss: 5.142717]\n",
      "epoch:22 step:17876 [D loss: 0.844530, acc.: 70.31%] [G loss: 0.043846]\n",
      "epoch:22 step:17877 [D loss: 1.022370, acc.: 70.31%] [G loss: 7.178654]\n",
      "epoch:22 step:17878 [D loss: 0.017896, acc.: 100.00%] [G loss: 7.846529]\n",
      "epoch:22 step:17879 [D loss: 2.185637, acc.: 40.62%] [G loss: 0.110356]\n",
      "epoch:22 step:17880 [D loss: 0.047556, acc.: 99.22%] [G loss: 1.746727]\n",
      "epoch:22 step:17881 [D loss: 0.056833, acc.: 98.44%] [G loss: 1.816547]\n",
      "epoch:22 step:17882 [D loss: 0.531008, acc.: 74.22%] [G loss: 5.248153]\n",
      "epoch:22 step:17883 [D loss: 0.110504, acc.: 96.88%] [G loss: 5.843547]\n",
      "epoch:22 step:17884 [D loss: 0.175027, acc.: 92.19%] [G loss: 0.268217]\n",
      "epoch:22 step:17885 [D loss: 0.028738, acc.: 98.44%] [G loss: 3.680053]\n",
      "epoch:22 step:17886 [D loss: 0.139613, acc.: 93.75%] [G loss: 0.003211]\n",
      "epoch:22 step:17887 [D loss: 0.060662, acc.: 99.22%] [G loss: 0.021787]\n",
      "epoch:22 step:17888 [D loss: 0.007088, acc.: 100.00%] [G loss: 0.017393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22 step:17889 [D loss: 0.002553, acc.: 100.00%] [G loss: 2.339519]\n",
      "epoch:22 step:17890 [D loss: 0.002023, acc.: 100.00%] [G loss: 1.860543]\n",
      "epoch:22 step:17891 [D loss: 0.308023, acc.: 87.50%] [G loss: 0.259992]\n",
      "epoch:22 step:17892 [D loss: 0.041199, acc.: 99.22%] [G loss: 5.041465]\n",
      "epoch:22 step:17893 [D loss: 0.030777, acc.: 99.22%] [G loss: 4.370117]\n",
      "epoch:22 step:17894 [D loss: 0.047212, acc.: 99.22%] [G loss: 2.792001]\n",
      "epoch:22 step:17895 [D loss: 0.377065, acc.: 83.59%] [G loss: 2.607357]\n",
      "epoch:22 step:17896 [D loss: 0.059313, acc.: 99.22%] [G loss: 0.113719]\n",
      "epoch:22 step:17897 [D loss: 0.076409, acc.: 98.44%] [G loss: 4.127633]\n",
      "epoch:22 step:17898 [D loss: 0.018304, acc.: 99.22%] [G loss: 0.102115]\n",
      "epoch:22 step:17899 [D loss: 0.029704, acc.: 100.00%] [G loss: 4.386924]\n",
      "epoch:22 step:17900 [D loss: 0.013097, acc.: 100.00%] [G loss: 4.032097]\n",
      "epoch:22 step:17901 [D loss: 0.028479, acc.: 99.22%] [G loss: 0.084523]\n",
      "epoch:22 step:17902 [D loss: 0.025049, acc.: 100.00%] [G loss: 3.062289]\n",
      "epoch:22 step:17903 [D loss: 0.033385, acc.: 100.00%] [G loss: 3.504240]\n",
      "epoch:22 step:17904 [D loss: 0.043392, acc.: 99.22%] [G loss: 0.015746]\n",
      "epoch:22 step:17905 [D loss: 0.018009, acc.: 100.00%] [G loss: 3.926539]\n",
      "epoch:22 step:17906 [D loss: 0.072374, acc.: 98.44%] [G loss: 0.058333]\n",
      "epoch:22 step:17907 [D loss: 0.141092, acc.: 94.53%] [G loss: 0.083523]\n",
      "epoch:22 step:17908 [D loss: 0.023143, acc.: 99.22%] [G loss: 0.210477]\n",
      "epoch:22 step:17909 [D loss: 0.087031, acc.: 96.09%] [G loss: 4.571274]\n",
      "epoch:22 step:17910 [D loss: 0.014034, acc.: 100.00%] [G loss: 2.401001]\n",
      "epoch:22 step:17911 [D loss: 0.054269, acc.: 100.00%] [G loss: 0.012116]\n",
      "epoch:22 step:17912 [D loss: 0.003358, acc.: 100.00%] [G loss: 0.029458]\n",
      "epoch:22 step:17913 [D loss: 0.012474, acc.: 100.00%] [G loss: 5.331750]\n",
      "epoch:22 step:17914 [D loss: 0.018642, acc.: 100.00%] [G loss: 0.153515]\n",
      "epoch:22 step:17915 [D loss: 0.004320, acc.: 100.00%] [G loss: 0.100270]\n",
      "epoch:22 step:17916 [D loss: 0.078480, acc.: 99.22%] [G loss: 0.006635]\n",
      "epoch:22 step:17917 [D loss: 0.003026, acc.: 100.00%] [G loss: 0.293668]\n",
      "epoch:22 step:17918 [D loss: 0.067768, acc.: 97.66%] [G loss: 0.011884]\n",
      "epoch:22 step:17919 [D loss: 0.040667, acc.: 97.66%] [G loss: 4.923610]\n",
      "epoch:22 step:17920 [D loss: 0.010323, acc.: 100.00%] [G loss: 0.023418]\n",
      "epoch:22 step:17921 [D loss: 0.076562, acc.: 99.22%] [G loss: 4.036947]\n",
      "epoch:22 step:17922 [D loss: 0.016323, acc.: 100.00%] [G loss: 5.676397]\n",
      "epoch:22 step:17923 [D loss: 0.006636, acc.: 100.00%] [G loss: 0.066891]\n",
      "epoch:22 step:17924 [D loss: 0.085361, acc.: 96.09%] [G loss: 3.000968]\n",
      "epoch:22 step:17925 [D loss: 0.005795, acc.: 100.00%] [G loss: 0.000742]\n",
      "epoch:22 step:17926 [D loss: 0.171291, acc.: 91.41%] [G loss: 0.719298]\n",
      "epoch:22 step:17927 [D loss: 0.012582, acc.: 99.22%] [G loss: 5.618445]\n",
      "epoch:22 step:17928 [D loss: 0.398426, acc.: 83.59%] [G loss: 0.879150]\n",
      "epoch:22 step:17929 [D loss: 0.275586, acc.: 88.28%] [G loss: 0.764179]\n",
      "epoch:22 step:17930 [D loss: 0.100290, acc.: 94.53%] [G loss: 6.866860]\n",
      "epoch:22 step:17931 [D loss: 0.027391, acc.: 98.44%] [G loss: 5.511011]\n",
      "epoch:22 step:17932 [D loss: 0.018390, acc.: 100.00%] [G loss: 4.684706]\n",
      "epoch:22 step:17933 [D loss: 0.164976, acc.: 92.19%] [G loss: 2.497220]\n",
      "epoch:22 step:17934 [D loss: 0.034264, acc.: 100.00%] [G loss: 2.013991]\n",
      "epoch:22 step:17935 [D loss: 1.126293, acc.: 53.12%] [G loss: 4.520373]\n",
      "epoch:22 step:17936 [D loss: 0.087914, acc.: 96.88%] [G loss: 1.354958]\n",
      "epoch:22 step:17937 [D loss: 0.015515, acc.: 100.00%] [G loss: 0.827979]\n",
      "epoch:22 step:17938 [D loss: 0.054142, acc.: 98.44%] [G loss: 0.234851]\n",
      "epoch:22 step:17939 [D loss: 0.027112, acc.: 99.22%] [G loss: 0.033934]\n",
      "epoch:22 step:17940 [D loss: 0.007109, acc.: 100.00%] [G loss: 0.009762]\n",
      "epoch:22 step:17941 [D loss: 0.005988, acc.: 100.00%] [G loss: 0.010239]\n",
      "epoch:22 step:17942 [D loss: 0.090722, acc.: 97.66%] [G loss: 1.501308]\n",
      "epoch:22 step:17943 [D loss: 0.000861, acc.: 100.00%] [G loss: 5.091859]\n",
      "epoch:22 step:17944 [D loss: 0.091793, acc.: 96.88%] [G loss: 7.429000]\n",
      "epoch:22 step:17945 [D loss: 0.009567, acc.: 100.00%] [G loss: 3.845199]\n",
      "epoch:22 step:17946 [D loss: 0.023410, acc.: 99.22%] [G loss: 1.765978]\n",
      "epoch:22 step:17947 [D loss: 0.086076, acc.: 97.66%] [G loss: 5.513940]\n",
      "epoch:22 step:17948 [D loss: 0.026704, acc.: 99.22%] [G loss: 5.851199]\n",
      "epoch:22 step:17949 [D loss: 0.337263, acc.: 85.16%] [G loss: 8.595325]\n",
      "epoch:22 step:17950 [D loss: 0.313406, acc.: 84.38%] [G loss: 5.224068]\n",
      "epoch:22 step:17951 [D loss: 0.014342, acc.: 99.22%] [G loss: 4.979640]\n",
      "epoch:22 step:17952 [D loss: 0.036244, acc.: 100.00%] [G loss: 6.080148]\n",
      "epoch:22 step:17953 [D loss: 0.021539, acc.: 100.00%] [G loss: 6.333302]\n",
      "epoch:22 step:17954 [D loss: 0.002996, acc.: 100.00%] [G loss: 5.670767]\n",
      "epoch:22 step:17955 [D loss: 0.005970, acc.: 100.00%] [G loss: 4.671810]\n",
      "epoch:22 step:17956 [D loss: 0.009473, acc.: 100.00%] [G loss: 4.440324]\n",
      "epoch:22 step:17957 [D loss: 0.009079, acc.: 100.00%] [G loss: 4.348825]\n",
      "epoch:22 step:17958 [D loss: 0.049815, acc.: 99.22%] [G loss: 7.327624]\n",
      "epoch:22 step:17959 [D loss: 0.015832, acc.: 100.00%] [G loss: 8.513216]\n",
      "epoch:22 step:17960 [D loss: 0.037912, acc.: 99.22%] [G loss: 6.844919]\n",
      "epoch:22 step:17961 [D loss: 0.023646, acc.: 100.00%] [G loss: 5.696571]\n",
      "epoch:22 step:17962 [D loss: 0.012617, acc.: 100.00%] [G loss: 5.629550]\n",
      "epoch:22 step:17963 [D loss: 0.009372, acc.: 100.00%] [G loss: 3.578254]\n",
      "epoch:23 step:17964 [D loss: 0.002619, acc.: 100.00%] [G loss: 6.033292]\n",
      "epoch:23 step:17965 [D loss: 0.001118, acc.: 100.00%] [G loss: 5.239861]\n",
      "epoch:23 step:17966 [D loss: 0.001747, acc.: 100.00%] [G loss: 2.584497]\n",
      "epoch:23 step:17967 [D loss: 0.004065, acc.: 100.00%] [G loss: 3.275585]\n",
      "epoch:23 step:17968 [D loss: 0.036312, acc.: 100.00%] [G loss: 3.900876]\n",
      "epoch:23 step:17969 [D loss: 0.053970, acc.: 98.44%] [G loss: 2.400943]\n",
      "epoch:23 step:17970 [D loss: 0.100004, acc.: 98.44%] [G loss: 5.537478]\n",
      "epoch:23 step:17971 [D loss: 0.011408, acc.: 100.00%] [G loss: 5.497451]\n",
      "epoch:23 step:17972 [D loss: 0.047136, acc.: 98.44%] [G loss: 3.210249]\n",
      "epoch:23 step:17973 [D loss: 0.036900, acc.: 99.22%] [G loss: 1.457221]\n",
      "epoch:23 step:17974 [D loss: 0.269660, acc.: 85.94%] [G loss: 8.553813]\n",
      "epoch:23 step:17975 [D loss: 0.710073, acc.: 67.97%] [G loss: 3.132961]\n",
      "epoch:23 step:17976 [D loss: 0.010722, acc.: 100.00%] [G loss: 0.003442]\n",
      "epoch:23 step:17977 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.730783]\n",
      "epoch:23 step:17978 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.462564]\n",
      "epoch:23 step:17979 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.643945]\n",
      "epoch:23 step:17980 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.285896]\n",
      "epoch:23 step:17981 [D loss: 0.009627, acc.: 100.00%] [G loss: 0.064039]\n",
      "epoch:23 step:17982 [D loss: 0.015021, acc.: 100.00%] [G loss: 0.060542]\n",
      "epoch:23 step:17983 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.143079]\n",
      "epoch:23 step:17984 [D loss: 0.105631, acc.: 96.88%] [G loss: 0.673349]\n",
      "epoch:23 step:17985 [D loss: 0.006073, acc.: 100.00%] [G loss: 1.525908]\n",
      "epoch:23 step:17986 [D loss: 0.036672, acc.: 99.22%] [G loss: 0.494974]\n",
      "epoch:23 step:17987 [D loss: 0.002971, acc.: 100.00%] [G loss: 0.368966]\n",
      "epoch:23 step:17988 [D loss: 0.061597, acc.: 98.44%] [G loss: 0.061534]\n",
      "epoch:23 step:17989 [D loss: 0.003588, acc.: 100.00%] [G loss: 0.049921]\n",
      "epoch:23 step:17990 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.094170]\n",
      "epoch:23 step:17991 [D loss: 0.001576, acc.: 100.00%] [G loss: 0.036315]\n",
      "epoch:23 step:17992 [D loss: 0.000939, acc.: 100.00%] [G loss: 0.075586]\n",
      "epoch:23 step:17993 [D loss: 0.002040, acc.: 100.00%] [G loss: 0.061612]\n",
      "epoch:23 step:17994 [D loss: 0.001657, acc.: 100.00%] [G loss: 0.050156]\n",
      "epoch:23 step:17995 [D loss: 0.005706, acc.: 100.00%] [G loss: 0.119660]\n",
      "epoch:23 step:17996 [D loss: 0.013527, acc.: 100.00%] [G loss: 0.105662]\n",
      "epoch:23 step:17997 [D loss: 0.004422, acc.: 100.00%] [G loss: 0.075935]\n",
      "epoch:23 step:17998 [D loss: 0.083835, acc.: 97.66%] [G loss: 1.654009]\n",
      "epoch:23 step:17999 [D loss: 0.164842, acc.: 92.19%] [G loss: 1.694099]\n",
      "epoch:23 step:18000 [D loss: 0.041401, acc.: 97.66%] [G loss: 4.569655]\n",
      "epoch:23 step:18001 [D loss: 0.014921, acc.: 100.00%] [G loss: 3.454614]\n",
      "epoch:23 step:18002 [D loss: 0.005308, acc.: 100.00%] [G loss: 0.001366]\n",
      "epoch:23 step:18003 [D loss: 0.000051, acc.: 100.00%] [G loss: 3.589664]\n",
      "epoch:23 step:18004 [D loss: 0.000926, acc.: 100.00%] [G loss: 0.007730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18005 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.318993]\n",
      "epoch:23 step:18006 [D loss: 0.021664, acc.: 99.22%] [G loss: 2.001280]\n",
      "epoch:23 step:18007 [D loss: 0.005211, acc.: 100.00%] [G loss: 2.240518]\n",
      "epoch:23 step:18008 [D loss: 0.013158, acc.: 100.00%] [G loss: 0.015260]\n",
      "epoch:23 step:18009 [D loss: 0.004067, acc.: 100.00%] [G loss: 1.080645]\n",
      "epoch:23 step:18010 [D loss: 0.504934, acc.: 72.66%] [G loss: 4.389931]\n",
      "epoch:23 step:18011 [D loss: 2.062284, acc.: 51.56%] [G loss: 0.023854]\n",
      "epoch:23 step:18012 [D loss: 0.001306, acc.: 100.00%] [G loss: 0.235889]\n",
      "epoch:23 step:18013 [D loss: 0.009362, acc.: 100.00%] [G loss: 0.105385]\n",
      "epoch:23 step:18014 [D loss: 0.000588, acc.: 100.00%] [G loss: 0.125508]\n",
      "epoch:23 step:18015 [D loss: 0.099126, acc.: 96.09%] [G loss: 0.220131]\n",
      "epoch:23 step:18016 [D loss: 0.002127, acc.: 100.00%] [G loss: 4.249164]\n",
      "epoch:23 step:18017 [D loss: 0.015760, acc.: 100.00%] [G loss: 0.198899]\n",
      "epoch:23 step:18018 [D loss: 0.003480, acc.: 100.00%] [G loss: 0.369742]\n",
      "epoch:23 step:18019 [D loss: 0.012085, acc.: 100.00%] [G loss: 0.487094]\n",
      "epoch:23 step:18020 [D loss: 0.018479, acc.: 100.00%] [G loss: 0.680006]\n",
      "epoch:23 step:18021 [D loss: 0.009547, acc.: 100.00%] [G loss: 0.109259]\n",
      "epoch:23 step:18022 [D loss: 0.015394, acc.: 100.00%] [G loss: 0.372003]\n",
      "epoch:23 step:18023 [D loss: 0.002252, acc.: 100.00%] [G loss: 0.562440]\n",
      "epoch:23 step:18024 [D loss: 0.022229, acc.: 100.00%] [G loss: 0.130297]\n",
      "epoch:23 step:18025 [D loss: 0.005458, acc.: 100.00%] [G loss: 0.282019]\n",
      "epoch:23 step:18026 [D loss: 0.011357, acc.: 100.00%] [G loss: 0.021452]\n",
      "epoch:23 step:18027 [D loss: 0.004068, acc.: 100.00%] [G loss: 0.022248]\n",
      "epoch:23 step:18028 [D loss: 0.032726, acc.: 100.00%] [G loss: 0.028824]\n",
      "epoch:23 step:18029 [D loss: 0.040092, acc.: 99.22%] [G loss: 0.003185]\n",
      "epoch:23 step:18030 [D loss: 0.004060, acc.: 100.00%] [G loss: 0.851564]\n",
      "epoch:23 step:18031 [D loss: 0.012410, acc.: 100.00%] [G loss: 0.180688]\n",
      "epoch:23 step:18032 [D loss: 0.008470, acc.: 100.00%] [G loss: 0.257616]\n",
      "epoch:23 step:18033 [D loss: 0.002241, acc.: 100.00%] [G loss: 0.302505]\n",
      "epoch:23 step:18034 [D loss: 0.036110, acc.: 99.22%] [G loss: 0.155906]\n",
      "epoch:23 step:18035 [D loss: 0.014717, acc.: 100.00%] [G loss: 0.251022]\n",
      "epoch:23 step:18036 [D loss: 0.004067, acc.: 100.00%] [G loss: 0.146030]\n",
      "epoch:23 step:18037 [D loss: 0.013008, acc.: 100.00%] [G loss: 0.331824]\n",
      "epoch:23 step:18038 [D loss: 0.006396, acc.: 100.00%] [G loss: 0.247950]\n",
      "epoch:23 step:18039 [D loss: 0.009324, acc.: 100.00%] [G loss: 0.036148]\n",
      "epoch:23 step:18040 [D loss: 0.171318, acc.: 95.31%] [G loss: 0.133155]\n",
      "epoch:23 step:18041 [D loss: 0.011203, acc.: 99.22%] [G loss: 0.390439]\n",
      "epoch:23 step:18042 [D loss: 0.011603, acc.: 99.22%] [G loss: 0.397057]\n",
      "epoch:23 step:18043 [D loss: 0.003992, acc.: 100.00%] [G loss: 0.255561]\n",
      "epoch:23 step:18044 [D loss: 0.004728, acc.: 100.00%] [G loss: 0.289750]\n",
      "epoch:23 step:18045 [D loss: 0.001890, acc.: 100.00%] [G loss: 0.170567]\n",
      "epoch:23 step:18046 [D loss: 0.017950, acc.: 100.00%] [G loss: 0.495935]\n",
      "epoch:23 step:18047 [D loss: 0.003769, acc.: 100.00%] [G loss: 0.115510]\n",
      "epoch:23 step:18048 [D loss: 0.005533, acc.: 100.00%] [G loss: 0.426581]\n",
      "epoch:23 step:18049 [D loss: 0.002682, acc.: 100.00%] [G loss: 0.158747]\n",
      "epoch:23 step:18050 [D loss: 0.001495, acc.: 100.00%] [G loss: 5.053190]\n",
      "epoch:23 step:18051 [D loss: 0.005909, acc.: 100.00%] [G loss: 0.342701]\n",
      "epoch:23 step:18052 [D loss: 0.005831, acc.: 100.00%] [G loss: 0.223928]\n",
      "epoch:23 step:18053 [D loss: 0.073162, acc.: 99.22%] [G loss: 0.069323]\n",
      "epoch:23 step:18054 [D loss: 0.007324, acc.: 100.00%] [G loss: 0.220175]\n",
      "epoch:23 step:18055 [D loss: 0.072027, acc.: 98.44%] [G loss: 1.297223]\n",
      "epoch:23 step:18056 [D loss: 0.004760, acc.: 100.00%] [G loss: 1.016949]\n",
      "epoch:23 step:18057 [D loss: 0.002303, acc.: 100.00%] [G loss: 0.995356]\n",
      "epoch:23 step:18058 [D loss: 0.005495, acc.: 100.00%] [G loss: 0.446043]\n",
      "epoch:23 step:18059 [D loss: 0.003849, acc.: 100.00%] [G loss: 0.471282]\n",
      "epoch:23 step:18060 [D loss: 0.026997, acc.: 100.00%] [G loss: 0.071452]\n",
      "epoch:23 step:18061 [D loss: 0.013182, acc.: 100.00%] [G loss: 0.051509]\n",
      "epoch:23 step:18062 [D loss: 0.061567, acc.: 98.44%] [G loss: 0.134269]\n",
      "epoch:23 step:18063 [D loss: 0.003266, acc.: 100.00%] [G loss: 0.239369]\n",
      "epoch:23 step:18064 [D loss: 0.005487, acc.: 100.00%] [G loss: 1.393167]\n",
      "epoch:23 step:18065 [D loss: 0.003803, acc.: 100.00%] [G loss: 2.043659]\n",
      "epoch:23 step:18066 [D loss: 0.007199, acc.: 100.00%] [G loss: 0.922171]\n",
      "epoch:23 step:18067 [D loss: 0.007203, acc.: 100.00%] [G loss: 0.415392]\n",
      "epoch:23 step:18068 [D loss: 0.013462, acc.: 99.22%] [G loss: 0.207423]\n",
      "epoch:23 step:18069 [D loss: 0.002288, acc.: 100.00%] [G loss: 0.162588]\n",
      "epoch:23 step:18070 [D loss: 0.007809, acc.: 100.00%] [G loss: 1.331748]\n",
      "epoch:23 step:18071 [D loss: 0.085472, acc.: 97.66%] [G loss: 1.479245]\n",
      "epoch:23 step:18072 [D loss: 0.016166, acc.: 99.22%] [G loss: 1.676159]\n",
      "epoch:23 step:18073 [D loss: 0.031342, acc.: 99.22%] [G loss: 6.190123]\n",
      "epoch:23 step:18074 [D loss: 0.018592, acc.: 100.00%] [G loss: 1.167312]\n",
      "epoch:23 step:18075 [D loss: 0.005831, acc.: 100.00%] [G loss: 0.176867]\n",
      "epoch:23 step:18076 [D loss: 0.022202, acc.: 100.00%] [G loss: 0.544205]\n",
      "epoch:23 step:18077 [D loss: 0.006814, acc.: 100.00%] [G loss: 0.429929]\n",
      "epoch:23 step:18078 [D loss: 0.006166, acc.: 100.00%] [G loss: 0.614755]\n",
      "epoch:23 step:18079 [D loss: 0.062744, acc.: 96.88%] [G loss: 0.021984]\n",
      "epoch:23 step:18080 [D loss: 0.030347, acc.: 100.00%] [G loss: 0.088447]\n",
      "epoch:23 step:18081 [D loss: 0.002869, acc.: 100.00%] [G loss: 4.693661]\n",
      "epoch:23 step:18082 [D loss: 0.011835, acc.: 100.00%] [G loss: 1.342816]\n",
      "epoch:23 step:18083 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.022263]\n",
      "epoch:23 step:18084 [D loss: 0.000882, acc.: 100.00%] [G loss: 0.152793]\n",
      "epoch:23 step:18085 [D loss: 0.001593, acc.: 100.00%] [G loss: 0.211020]\n",
      "epoch:23 step:18086 [D loss: 0.181873, acc.: 92.19%] [G loss: 7.232053]\n",
      "epoch:23 step:18087 [D loss: 0.263675, acc.: 87.50%] [G loss: 6.082724]\n",
      "epoch:23 step:18088 [D loss: 0.001815, acc.: 100.00%] [G loss: 1.070381]\n",
      "epoch:23 step:18089 [D loss: 0.001893, acc.: 100.00%] [G loss: 4.948326]\n",
      "epoch:23 step:18090 [D loss: 0.005042, acc.: 100.00%] [G loss: 0.060971]\n",
      "epoch:23 step:18091 [D loss: 0.000316, acc.: 100.00%] [G loss: 1.011428]\n",
      "epoch:23 step:18092 [D loss: 0.006249, acc.: 100.00%] [G loss: 0.125203]\n",
      "epoch:23 step:18093 [D loss: 0.321721, acc.: 82.03%] [G loss: 6.681870]\n",
      "epoch:23 step:18094 [D loss: 0.124321, acc.: 95.31%] [G loss: 8.922345]\n",
      "epoch:23 step:18095 [D loss: 0.682176, acc.: 71.88%] [G loss: 0.015344]\n",
      "epoch:23 step:18096 [D loss: 0.829812, acc.: 67.19%] [G loss: 7.847218]\n",
      "epoch:23 step:18097 [D loss: 0.022030, acc.: 99.22%] [G loss: 8.376886]\n",
      "epoch:23 step:18098 [D loss: 2.085313, acc.: 53.12%] [G loss: 0.692729]\n",
      "epoch:23 step:18099 [D loss: 0.070593, acc.: 96.09%] [G loss: 1.895608]\n",
      "epoch:23 step:18100 [D loss: 0.002529, acc.: 100.00%] [G loss: 0.330942]\n",
      "epoch:23 step:18101 [D loss: 0.001974, acc.: 100.00%] [G loss: 0.275557]\n",
      "epoch:23 step:18102 [D loss: 0.001343, acc.: 100.00%] [G loss: 0.694772]\n",
      "epoch:23 step:18103 [D loss: 0.011086, acc.: 100.00%] [G loss: 0.941755]\n",
      "epoch:23 step:18104 [D loss: 0.033098, acc.: 99.22%] [G loss: 0.032927]\n",
      "epoch:23 step:18105 [D loss: 0.082484, acc.: 96.88%] [G loss: 0.336455]\n",
      "epoch:23 step:18106 [D loss: 0.006611, acc.: 100.00%] [G loss: 0.147844]\n",
      "epoch:23 step:18107 [D loss: 0.001117, acc.: 100.00%] [G loss: 3.499187]\n",
      "epoch:23 step:18108 [D loss: 0.015815, acc.: 99.22%] [G loss: 2.225333]\n",
      "epoch:23 step:18109 [D loss: 0.001478, acc.: 100.00%] [G loss: 0.120717]\n",
      "epoch:23 step:18110 [D loss: 0.005009, acc.: 100.00%] [G loss: 0.019074]\n",
      "epoch:23 step:18111 [D loss: 0.001313, acc.: 100.00%] [G loss: 0.062540]\n",
      "epoch:23 step:18112 [D loss: 0.001490, acc.: 100.00%] [G loss: 0.019066]\n",
      "epoch:23 step:18113 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.030644]\n",
      "epoch:23 step:18114 [D loss: 0.002611, acc.: 100.00%] [G loss: 0.007215]\n",
      "epoch:23 step:18115 [D loss: 0.000901, acc.: 100.00%] [G loss: 0.007125]\n",
      "epoch:23 step:18116 [D loss: 0.009631, acc.: 100.00%] [G loss: 0.024145]\n",
      "epoch:23 step:18117 [D loss: 0.003360, acc.: 100.00%] [G loss: 0.020917]\n",
      "epoch:23 step:18118 [D loss: 0.014535, acc.: 100.00%] [G loss: 0.654786]\n",
      "epoch:23 step:18119 [D loss: 0.007001, acc.: 100.00%] [G loss: 0.044935]\n",
      "epoch:23 step:18120 [D loss: 0.004824, acc.: 100.00%] [G loss: 0.016466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18121 [D loss: 0.869576, acc.: 68.75%] [G loss: 4.561472]\n",
      "epoch:23 step:18122 [D loss: 1.123279, acc.: 57.03%] [G loss: 5.954017]\n",
      "epoch:23 step:18123 [D loss: 0.805302, acc.: 70.31%] [G loss: 0.145010]\n",
      "epoch:23 step:18124 [D loss: 0.449519, acc.: 80.47%] [G loss: 2.170279]\n",
      "epoch:23 step:18125 [D loss: 0.006536, acc.: 100.00%] [G loss: 2.573042]\n",
      "epoch:23 step:18126 [D loss: 0.011711, acc.: 100.00%] [G loss: 3.149141]\n",
      "epoch:23 step:18127 [D loss: 0.021357, acc.: 100.00%] [G loss: 3.919118]\n",
      "epoch:23 step:18128 [D loss: 0.081032, acc.: 98.44%] [G loss: 1.553746]\n",
      "epoch:23 step:18129 [D loss: 0.089597, acc.: 98.44%] [G loss: 1.588066]\n",
      "epoch:23 step:18130 [D loss: 0.048573, acc.: 100.00%] [G loss: 1.219547]\n",
      "epoch:23 step:18131 [D loss: 0.083656, acc.: 99.22%] [G loss: 1.145677]\n",
      "epoch:23 step:18132 [D loss: 0.044096, acc.: 98.44%] [G loss: 1.844575]\n",
      "epoch:23 step:18133 [D loss: 0.306064, acc.: 88.28%] [G loss: 0.857015]\n",
      "epoch:23 step:18134 [D loss: 0.099909, acc.: 98.44%] [G loss: 1.985559]\n",
      "epoch:23 step:18135 [D loss: 0.019893, acc.: 100.00%] [G loss: 3.843767]\n",
      "epoch:23 step:18136 [D loss: 0.005337, acc.: 100.00%] [G loss: 4.543343]\n",
      "epoch:23 step:18137 [D loss: 0.060681, acc.: 98.44%] [G loss: 0.615220]\n",
      "epoch:23 step:18138 [D loss: 0.005245, acc.: 100.00%] [G loss: 0.579431]\n",
      "epoch:23 step:18139 [D loss: 0.035866, acc.: 99.22%] [G loss: 0.171004]\n",
      "epoch:23 step:18140 [D loss: 0.012756, acc.: 99.22%] [G loss: 0.686183]\n",
      "epoch:23 step:18141 [D loss: 0.057802, acc.: 97.66%] [G loss: 0.027343]\n",
      "epoch:23 step:18142 [D loss: 0.214761, acc.: 92.97%] [G loss: 0.013206]\n",
      "epoch:23 step:18143 [D loss: 0.007163, acc.: 100.00%] [G loss: 2.230849]\n",
      "epoch:23 step:18144 [D loss: 0.004127, acc.: 100.00%] [G loss: 0.688568]\n",
      "epoch:23 step:18145 [D loss: 0.012262, acc.: 100.00%] [G loss: 0.020004]\n",
      "epoch:23 step:18146 [D loss: 0.006101, acc.: 100.00%] [G loss: 0.023031]\n",
      "epoch:23 step:18147 [D loss: 0.053454, acc.: 97.66%] [G loss: 0.108378]\n",
      "epoch:23 step:18148 [D loss: 0.006817, acc.: 100.00%] [G loss: 2.506836]\n",
      "epoch:23 step:18149 [D loss: 0.248503, acc.: 88.28%] [G loss: 2.110397]\n",
      "epoch:23 step:18150 [D loss: 0.259890, acc.: 85.94%] [G loss: 1.468468]\n",
      "epoch:23 step:18151 [D loss: 0.021124, acc.: 100.00%] [G loss: 0.565407]\n",
      "epoch:23 step:18152 [D loss: 0.002844, acc.: 100.00%] [G loss: 0.195459]\n",
      "epoch:23 step:18153 [D loss: 0.104529, acc.: 96.88%] [G loss: 1.308067]\n",
      "epoch:23 step:18154 [D loss: 0.133029, acc.: 94.53%] [G loss: 0.430028]\n",
      "epoch:23 step:18155 [D loss: 0.006772, acc.: 100.00%] [G loss: 0.074893]\n",
      "epoch:23 step:18156 [D loss: 0.001651, acc.: 100.00%] [G loss: 0.808685]\n",
      "epoch:23 step:18157 [D loss: 0.014073, acc.: 100.00%] [G loss: 1.346414]\n",
      "epoch:23 step:18158 [D loss: 0.021133, acc.: 100.00%] [G loss: 0.047688]\n",
      "epoch:23 step:18159 [D loss: 0.001174, acc.: 100.00%] [G loss: 0.244018]\n",
      "epoch:23 step:18160 [D loss: 0.008352, acc.: 100.00%] [G loss: 0.012875]\n",
      "epoch:23 step:18161 [D loss: 0.001147, acc.: 100.00%] [G loss: 0.035434]\n",
      "epoch:23 step:18162 [D loss: 0.002817, acc.: 100.00%] [G loss: 0.140597]\n",
      "epoch:23 step:18163 [D loss: 0.003978, acc.: 100.00%] [G loss: 0.060846]\n",
      "epoch:23 step:18164 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.028538]\n",
      "epoch:23 step:18165 [D loss: 0.000664, acc.: 100.00%] [G loss: 0.043651]\n",
      "epoch:23 step:18166 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.020558]\n",
      "epoch:23 step:18167 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.043845]\n",
      "epoch:23 step:18168 [D loss: 0.001675, acc.: 100.00%] [G loss: 1.117094]\n",
      "epoch:23 step:18169 [D loss: 0.000928, acc.: 100.00%] [G loss: 0.009326]\n",
      "epoch:23 step:18170 [D loss: 0.002186, acc.: 100.00%] [G loss: 0.087126]\n",
      "epoch:23 step:18171 [D loss: 0.010194, acc.: 100.00%] [G loss: 0.006913]\n",
      "epoch:23 step:18172 [D loss: 0.010039, acc.: 100.00%] [G loss: 0.045287]\n",
      "epoch:23 step:18173 [D loss: 0.011102, acc.: 100.00%] [G loss: 0.131450]\n",
      "epoch:23 step:18174 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.010333]\n",
      "epoch:23 step:18175 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.048389]\n",
      "epoch:23 step:18176 [D loss: 0.000847, acc.: 100.00%] [G loss: 0.013880]\n",
      "epoch:23 step:18177 [D loss: 0.008873, acc.: 100.00%] [G loss: 0.007620]\n",
      "epoch:23 step:18178 [D loss: 0.000663, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:23 step:18179 [D loss: 0.000933, acc.: 100.00%] [G loss: 0.002680]\n",
      "epoch:23 step:18180 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.004833]\n",
      "epoch:23 step:18181 [D loss: 0.006277, acc.: 100.00%] [G loss: 0.021783]\n",
      "epoch:23 step:18182 [D loss: 0.001656, acc.: 100.00%] [G loss: 0.012652]\n",
      "epoch:23 step:18183 [D loss: 0.002378, acc.: 100.00%] [G loss: 0.007011]\n",
      "epoch:23 step:18184 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.820047]\n",
      "epoch:23 step:18185 [D loss: 0.000707, acc.: 100.00%] [G loss: 0.002080]\n",
      "epoch:23 step:18186 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.011568]\n",
      "epoch:23 step:18187 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.007716]\n",
      "epoch:23 step:18188 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.014596]\n",
      "epoch:23 step:18189 [D loss: 0.001398, acc.: 100.00%] [G loss: 0.018081]\n",
      "epoch:23 step:18190 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.001350]\n",
      "epoch:23 step:18191 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.050478]\n",
      "epoch:23 step:18192 [D loss: 0.001398, acc.: 100.00%] [G loss: 0.012504]\n",
      "epoch:23 step:18193 [D loss: 0.001431, acc.: 100.00%] [G loss: 0.001056]\n",
      "epoch:23 step:18194 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000868]\n",
      "epoch:23 step:18195 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.003705]\n",
      "epoch:23 step:18196 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.007720]\n",
      "epoch:23 step:18197 [D loss: 0.004903, acc.: 100.00%] [G loss: 0.002703]\n",
      "epoch:23 step:18198 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.001749]\n",
      "epoch:23 step:18199 [D loss: 0.001940, acc.: 100.00%] [G loss: 0.001470]\n",
      "epoch:23 step:18200 [D loss: 0.000807, acc.: 100.00%] [G loss: 0.002918]\n",
      "epoch:23 step:18201 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:23 step:18202 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:23 step:18203 [D loss: 0.004337, acc.: 100.00%] [G loss: 0.005718]\n",
      "epoch:23 step:18204 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.001598]\n",
      "epoch:23 step:18205 [D loss: 0.000855, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:23 step:18206 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.002119]\n",
      "epoch:23 step:18207 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000952]\n",
      "epoch:23 step:18208 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.000994]\n",
      "epoch:23 step:18209 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.005306]\n",
      "epoch:23 step:18210 [D loss: 0.060246, acc.: 98.44%] [G loss: 0.032902]\n",
      "epoch:23 step:18211 [D loss: 0.003012, acc.: 100.00%] [G loss: 0.374563]\n",
      "epoch:23 step:18212 [D loss: 0.018001, acc.: 100.00%] [G loss: 0.043027]\n",
      "epoch:23 step:18213 [D loss: 0.051255, acc.: 100.00%] [G loss: 0.002100]\n",
      "epoch:23 step:18214 [D loss: 0.038095, acc.: 100.00%] [G loss: 0.011664]\n",
      "epoch:23 step:18215 [D loss: 0.003466, acc.: 100.00%] [G loss: 0.007962]\n",
      "epoch:23 step:18216 [D loss: 0.004408, acc.: 100.00%] [G loss: 0.311983]\n",
      "epoch:23 step:18217 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.016309]\n",
      "epoch:23 step:18218 [D loss: 0.006383, acc.: 100.00%] [G loss: 0.344391]\n",
      "epoch:23 step:18219 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.002013]\n",
      "epoch:23 step:18220 [D loss: 0.000708, acc.: 100.00%] [G loss: 0.015612]\n",
      "epoch:23 step:18221 [D loss: 0.007293, acc.: 99.22%] [G loss: 0.085942]\n",
      "epoch:23 step:18222 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.002025]\n",
      "epoch:23 step:18223 [D loss: 0.002373, acc.: 100.00%] [G loss: 0.000890]\n",
      "epoch:23 step:18224 [D loss: 0.024844, acc.: 100.00%] [G loss: 0.001998]\n",
      "epoch:23 step:18225 [D loss: 0.034551, acc.: 100.00%] [G loss: 0.027395]\n",
      "epoch:23 step:18226 [D loss: 0.068672, acc.: 96.88%] [G loss: 0.357549]\n",
      "epoch:23 step:18227 [D loss: 0.046651, acc.: 97.66%] [G loss: 0.953004]\n",
      "epoch:23 step:18228 [D loss: 0.855915, acc.: 61.72%] [G loss: 0.627008]\n",
      "epoch:23 step:18229 [D loss: 0.005392, acc.: 100.00%] [G loss: 7.273951]\n",
      "epoch:23 step:18230 [D loss: 0.293297, acc.: 86.72%] [G loss: 3.082399]\n",
      "epoch:23 step:18231 [D loss: 0.001491, acc.: 100.00%] [G loss: 1.404137]\n",
      "epoch:23 step:18232 [D loss: 0.002431, acc.: 100.00%] [G loss: 0.000868]\n",
      "epoch:23 step:18233 [D loss: 0.001251, acc.: 100.00%] [G loss: 3.859029]\n",
      "epoch:23 step:18234 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.362998]\n",
      "epoch:23 step:18235 [D loss: 0.000526, acc.: 100.00%] [G loss: 0.000559]\n",
      "epoch:23 step:18236 [D loss: 0.000767, acc.: 100.00%] [G loss: 0.092627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18237 [D loss: 0.000937, acc.: 100.00%] [G loss: 0.004867]\n",
      "epoch:23 step:18238 [D loss: 0.003349, acc.: 100.00%] [G loss: 0.015947]\n",
      "epoch:23 step:18239 [D loss: 0.009773, acc.: 100.00%] [G loss: 0.255427]\n",
      "epoch:23 step:18240 [D loss: 0.122241, acc.: 96.88%] [G loss: 0.000442]\n",
      "epoch:23 step:18241 [D loss: 0.000657, acc.: 100.00%] [G loss: 1.634008]\n",
      "epoch:23 step:18242 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.017042]\n",
      "epoch:23 step:18243 [D loss: 0.016275, acc.: 99.22%] [G loss: 0.019455]\n",
      "epoch:23 step:18244 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.007175]\n",
      "epoch:23 step:18245 [D loss: 0.105436, acc.: 95.31%] [G loss: 0.000026]\n",
      "epoch:23 step:18246 [D loss: 0.000759, acc.: 100.00%] [G loss: 0.321879]\n",
      "epoch:23 step:18247 [D loss: 0.080453, acc.: 98.44%] [G loss: 0.004147]\n",
      "epoch:23 step:18248 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.064485]\n",
      "epoch:23 step:18249 [D loss: 0.071753, acc.: 96.88%] [G loss: 0.002675]\n",
      "epoch:23 step:18250 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.003445]\n",
      "epoch:23 step:18251 [D loss: 0.004892, acc.: 100.00%] [G loss: 0.004303]\n",
      "epoch:23 step:18252 [D loss: 0.006937, acc.: 100.00%] [G loss: 0.015704]\n",
      "epoch:23 step:18253 [D loss: 0.265843, acc.: 90.62%] [G loss: 4.209510]\n",
      "epoch:23 step:18254 [D loss: 0.002671, acc.: 100.00%] [G loss: 6.329173]\n",
      "epoch:23 step:18255 [D loss: 0.829798, acc.: 70.31%] [G loss: 3.493305]\n",
      "epoch:23 step:18256 [D loss: 0.124613, acc.: 94.53%] [G loss: 0.665308]\n",
      "epoch:23 step:18257 [D loss: 0.000783, acc.: 100.00%] [G loss: 3.234552]\n",
      "epoch:23 step:18258 [D loss: 0.005508, acc.: 100.00%] [G loss: 2.358342]\n",
      "epoch:23 step:18259 [D loss: 0.000808, acc.: 100.00%] [G loss: 1.518278]\n",
      "epoch:23 step:18260 [D loss: 0.000768, acc.: 100.00%] [G loss: 0.613373]\n",
      "epoch:23 step:18261 [D loss: 0.001177, acc.: 100.00%] [G loss: 0.353993]\n",
      "epoch:23 step:18262 [D loss: 0.000666, acc.: 100.00%] [G loss: 0.143183]\n",
      "epoch:23 step:18263 [D loss: 0.001112, acc.: 100.00%] [G loss: 0.126290]\n",
      "epoch:23 step:18264 [D loss: 0.000815, acc.: 100.00%] [G loss: 0.064116]\n",
      "epoch:23 step:18265 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.244925]\n",
      "epoch:23 step:18266 [D loss: 0.003598, acc.: 100.00%] [G loss: 0.480102]\n",
      "epoch:23 step:18267 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.073433]\n",
      "epoch:23 step:18268 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.035076]\n",
      "epoch:23 step:18269 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.084888]\n",
      "epoch:23 step:18270 [D loss: 0.001034, acc.: 100.00%] [G loss: 0.022841]\n",
      "epoch:23 step:18271 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.009010]\n",
      "epoch:23 step:18272 [D loss: 0.000778, acc.: 100.00%] [G loss: 0.017307]\n",
      "epoch:23 step:18273 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.029940]\n",
      "epoch:23 step:18274 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.013298]\n",
      "epoch:23 step:18275 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.010528]\n",
      "epoch:23 step:18276 [D loss: 0.001323, acc.: 100.00%] [G loss: 0.046921]\n",
      "epoch:23 step:18277 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.046130]\n",
      "epoch:23 step:18278 [D loss: 0.091523, acc.: 96.09%] [G loss: 0.001552]\n",
      "epoch:23 step:18279 [D loss: 0.943032, acc.: 60.16%] [G loss: 0.576980]\n",
      "epoch:23 step:18280 [D loss: 0.117588, acc.: 93.75%] [G loss: 5.763072]\n",
      "epoch:23 step:18281 [D loss: 0.942976, acc.: 61.72%] [G loss: 0.054885]\n",
      "epoch:23 step:18282 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.015575]\n",
      "epoch:23 step:18283 [D loss: 0.009511, acc.: 100.00%] [G loss: 0.047380]\n",
      "epoch:23 step:18284 [D loss: 0.003098, acc.: 100.00%] [G loss: 0.017239]\n",
      "epoch:23 step:18285 [D loss: 0.008489, acc.: 100.00%] [G loss: 0.005879]\n",
      "epoch:23 step:18286 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.757336]\n",
      "epoch:23 step:18287 [D loss: 0.017668, acc.: 100.00%] [G loss: 0.003351]\n",
      "epoch:23 step:18288 [D loss: 0.047764, acc.: 99.22%] [G loss: 0.007316]\n",
      "epoch:23 step:18289 [D loss: 0.000857, acc.: 100.00%] [G loss: 0.002615]\n",
      "epoch:23 step:18290 [D loss: 0.011459, acc.: 99.22%] [G loss: 0.028671]\n",
      "epoch:23 step:18291 [D loss: 0.000537, acc.: 100.00%] [G loss: 0.004727]\n",
      "epoch:23 step:18292 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.035475]\n",
      "epoch:23 step:18293 [D loss: 0.022521, acc.: 100.00%] [G loss: 0.251938]\n",
      "epoch:23 step:18294 [D loss: 0.001752, acc.: 100.00%] [G loss: 0.004202]\n",
      "epoch:23 step:18295 [D loss: 0.000842, acc.: 100.00%] [G loss: 0.001559]\n",
      "epoch:23 step:18296 [D loss: 0.003838, acc.: 100.00%] [G loss: 0.004088]\n",
      "epoch:23 step:18297 [D loss: 0.005801, acc.: 100.00%] [G loss: 0.001265]\n",
      "epoch:23 step:18298 [D loss: 0.000390, acc.: 100.00%] [G loss: 0.003092]\n",
      "epoch:23 step:18299 [D loss: 0.015539, acc.: 100.00%] [G loss: 0.038347]\n",
      "epoch:23 step:18300 [D loss: 0.000940, acc.: 100.00%] [G loss: 0.006076]\n",
      "epoch:23 step:18301 [D loss: 0.018533, acc.: 100.00%] [G loss: 0.002120]\n",
      "epoch:23 step:18302 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.274175]\n",
      "epoch:23 step:18303 [D loss: 0.001250, acc.: 100.00%] [G loss: 0.038853]\n",
      "epoch:23 step:18304 [D loss: 0.001293, acc.: 100.00%] [G loss: 0.250423]\n",
      "epoch:23 step:18305 [D loss: 0.037896, acc.: 100.00%] [G loss: 0.027322]\n",
      "epoch:23 step:18306 [D loss: 0.003204, acc.: 100.00%] [G loss: 0.418716]\n",
      "epoch:23 step:18307 [D loss: 0.057651, acc.: 100.00%] [G loss: 1.031425]\n",
      "epoch:23 step:18308 [D loss: 0.023803, acc.: 100.00%] [G loss: 0.041350]\n",
      "epoch:23 step:18309 [D loss: 0.497532, acc.: 79.69%] [G loss: 6.937260]\n",
      "epoch:23 step:18310 [D loss: 0.700858, acc.: 71.09%] [G loss: 0.839182]\n",
      "epoch:23 step:18311 [D loss: 0.005672, acc.: 100.00%] [G loss: 3.113118]\n",
      "epoch:23 step:18312 [D loss: 0.004947, acc.: 100.00%] [G loss: 0.304418]\n",
      "epoch:23 step:18313 [D loss: 0.096106, acc.: 95.31%] [G loss: 0.029558]\n",
      "epoch:23 step:18314 [D loss: 0.070642, acc.: 97.66%] [G loss: 0.128651]\n",
      "epoch:23 step:18315 [D loss: 0.011312, acc.: 100.00%] [G loss: 0.071253]\n",
      "epoch:23 step:18316 [D loss: 0.008816, acc.: 100.00%] [G loss: 0.073764]\n",
      "epoch:23 step:18317 [D loss: 0.018274, acc.: 100.00%] [G loss: 0.088267]\n",
      "epoch:23 step:18318 [D loss: 0.004494, acc.: 100.00%] [G loss: 0.259252]\n",
      "epoch:23 step:18319 [D loss: 0.008782, acc.: 100.00%] [G loss: 2.600628]\n",
      "epoch:23 step:18320 [D loss: 0.012524, acc.: 100.00%] [G loss: 1.452659]\n",
      "epoch:23 step:18321 [D loss: 0.007239, acc.: 100.00%] [G loss: 1.985771]\n",
      "epoch:23 step:18322 [D loss: 0.096729, acc.: 95.31%] [G loss: 0.718388]\n",
      "epoch:23 step:18323 [D loss: 0.004138, acc.: 100.00%] [G loss: 0.952858]\n",
      "epoch:23 step:18324 [D loss: 0.183737, acc.: 92.19%] [G loss: 0.004415]\n",
      "epoch:23 step:18325 [D loss: 0.030802, acc.: 99.22%] [G loss: 0.082738]\n",
      "epoch:23 step:18326 [D loss: 0.017992, acc.: 100.00%] [G loss: 0.175481]\n",
      "epoch:23 step:18327 [D loss: 0.025112, acc.: 99.22%] [G loss: 0.010597]\n",
      "epoch:23 step:18328 [D loss: 0.012084, acc.: 100.00%] [G loss: 0.072492]\n",
      "epoch:23 step:18329 [D loss: 0.001169, acc.: 100.00%] [G loss: 0.042311]\n",
      "epoch:23 step:18330 [D loss: 0.003736, acc.: 100.00%] [G loss: 0.017298]\n",
      "epoch:23 step:18331 [D loss: 0.001247, acc.: 100.00%] [G loss: 0.051250]\n",
      "epoch:23 step:18332 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.065469]\n",
      "epoch:23 step:18333 [D loss: 0.001096, acc.: 100.00%] [G loss: 1.513786]\n",
      "epoch:23 step:18334 [D loss: 0.002357, acc.: 100.00%] [G loss: 0.031144]\n",
      "epoch:23 step:18335 [D loss: 0.002408, acc.: 100.00%] [G loss: 0.015851]\n",
      "epoch:23 step:18336 [D loss: 1.450360, acc.: 51.56%] [G loss: 8.887362]\n",
      "epoch:23 step:18337 [D loss: 2.610216, acc.: 50.00%] [G loss: 5.842972]\n",
      "epoch:23 step:18338 [D loss: 1.052927, acc.: 58.59%] [G loss: 3.264942]\n",
      "epoch:23 step:18339 [D loss: 0.148104, acc.: 96.09%] [G loss: 2.003435]\n",
      "epoch:23 step:18340 [D loss: 0.046651, acc.: 99.22%] [G loss: 1.549171]\n",
      "epoch:23 step:18341 [D loss: 0.058177, acc.: 100.00%] [G loss: 2.443254]\n",
      "epoch:23 step:18342 [D loss: 0.013474, acc.: 100.00%] [G loss: 0.774507]\n",
      "epoch:23 step:18343 [D loss: 0.030412, acc.: 99.22%] [G loss: 0.260451]\n",
      "epoch:23 step:18344 [D loss: 0.013268, acc.: 100.00%] [G loss: 0.262595]\n",
      "epoch:23 step:18345 [D loss: 0.024475, acc.: 100.00%] [G loss: 0.446253]\n",
      "epoch:23 step:18346 [D loss: 0.153299, acc.: 96.88%] [G loss: 0.365576]\n",
      "epoch:23 step:18347 [D loss: 0.041899, acc.: 100.00%] [G loss: 0.384168]\n",
      "epoch:23 step:18348 [D loss: 0.060225, acc.: 99.22%] [G loss: 0.443515]\n",
      "epoch:23 step:18349 [D loss: 0.078466, acc.: 97.66%] [G loss: 2.953327]\n",
      "epoch:23 step:18350 [D loss: 0.011143, acc.: 100.00%] [G loss: 0.029139]\n",
      "epoch:23 step:18351 [D loss: 0.027020, acc.: 99.22%] [G loss: 1.397936]\n",
      "epoch:23 step:18352 [D loss: 0.009357, acc.: 100.00%] [G loss: 0.017539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18353 [D loss: 0.005379, acc.: 100.00%] [G loss: 0.015392]\n",
      "epoch:23 step:18354 [D loss: 0.015249, acc.: 100.00%] [G loss: 0.004942]\n",
      "epoch:23 step:18355 [D loss: 0.002729, acc.: 100.00%] [G loss: 0.008385]\n",
      "epoch:23 step:18356 [D loss: 0.007591, acc.: 100.00%] [G loss: 0.576476]\n",
      "epoch:23 step:18357 [D loss: 0.000992, acc.: 100.00%] [G loss: 0.010668]\n",
      "epoch:23 step:18358 [D loss: 0.010874, acc.: 100.00%] [G loss: 0.177867]\n",
      "epoch:23 step:18359 [D loss: 0.005670, acc.: 100.00%] [G loss: 0.028302]\n",
      "epoch:23 step:18360 [D loss: 0.003175, acc.: 100.00%] [G loss: 0.186117]\n",
      "epoch:23 step:18361 [D loss: 0.002148, acc.: 100.00%] [G loss: 0.185080]\n",
      "epoch:23 step:18362 [D loss: 0.002612, acc.: 100.00%] [G loss: 0.013752]\n",
      "epoch:23 step:18363 [D loss: 0.008751, acc.: 100.00%] [G loss: 0.001871]\n",
      "epoch:23 step:18364 [D loss: 0.006868, acc.: 100.00%] [G loss: 0.005131]\n",
      "epoch:23 step:18365 [D loss: 0.000870, acc.: 100.00%] [G loss: 0.076041]\n",
      "epoch:23 step:18366 [D loss: 0.008384, acc.: 100.00%] [G loss: 0.002171]\n",
      "epoch:23 step:18367 [D loss: 0.001957, acc.: 100.00%] [G loss: 0.086008]\n",
      "epoch:23 step:18368 [D loss: 0.005375, acc.: 100.00%] [G loss: 0.098667]\n",
      "epoch:23 step:18369 [D loss: 0.003838, acc.: 100.00%] [G loss: 0.010544]\n",
      "epoch:23 step:18370 [D loss: 0.010079, acc.: 100.00%] [G loss: 0.004217]\n",
      "epoch:23 step:18371 [D loss: 0.007071, acc.: 100.00%] [G loss: 0.081979]\n",
      "epoch:23 step:18372 [D loss: 0.007159, acc.: 100.00%] [G loss: 0.004129]\n",
      "epoch:23 step:18373 [D loss: 0.003707, acc.: 100.00%] [G loss: 0.009942]\n",
      "epoch:23 step:18374 [D loss: 0.003448, acc.: 100.00%] [G loss: 0.165878]\n",
      "epoch:23 step:18375 [D loss: 0.002683, acc.: 100.00%] [G loss: 0.100272]\n",
      "epoch:23 step:18376 [D loss: 0.017852, acc.: 99.22%] [G loss: 0.046260]\n",
      "epoch:23 step:18377 [D loss: 0.006502, acc.: 100.00%] [G loss: 0.022123]\n",
      "epoch:23 step:18378 [D loss: 0.002653, acc.: 100.00%] [G loss: 0.025391]\n",
      "epoch:23 step:18379 [D loss: 0.067784, acc.: 99.22%] [G loss: 0.007088]\n",
      "epoch:23 step:18380 [D loss: 0.007269, acc.: 100.00%] [G loss: 0.011868]\n",
      "epoch:23 step:18381 [D loss: 0.002175, acc.: 100.00%] [G loss: 0.033280]\n",
      "epoch:23 step:18382 [D loss: 0.012898, acc.: 100.00%] [G loss: 0.460544]\n",
      "epoch:23 step:18383 [D loss: 0.016159, acc.: 100.00%] [G loss: 0.002139]\n",
      "epoch:23 step:18384 [D loss: 0.000972, acc.: 100.00%] [G loss: 0.011682]\n",
      "epoch:23 step:18385 [D loss: 0.010155, acc.: 100.00%] [G loss: 0.009397]\n",
      "epoch:23 step:18386 [D loss: 0.001999, acc.: 100.00%] [G loss: 0.043284]\n",
      "epoch:23 step:18387 [D loss: 0.004749, acc.: 100.00%] [G loss: 0.033453]\n",
      "epoch:23 step:18388 [D loss: 0.003858, acc.: 100.00%] [G loss: 0.004151]\n",
      "epoch:23 step:18389 [D loss: 0.000743, acc.: 100.00%] [G loss: 0.026150]\n",
      "epoch:23 step:18390 [D loss: 0.055945, acc.: 99.22%] [G loss: 0.013364]\n",
      "epoch:23 step:18391 [D loss: 0.043238, acc.: 98.44%] [G loss: 0.012441]\n",
      "epoch:23 step:18392 [D loss: 0.005945, acc.: 100.00%] [G loss: 0.004921]\n",
      "epoch:23 step:18393 [D loss: 0.185366, acc.: 92.19%] [G loss: 0.000016]\n",
      "epoch:23 step:18394 [D loss: 0.110402, acc.: 97.66%] [G loss: 0.020375]\n",
      "epoch:23 step:18395 [D loss: 0.003995, acc.: 100.00%] [G loss: 0.118464]\n",
      "epoch:23 step:18396 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.232686]\n",
      "epoch:23 step:18397 [D loss: 0.070438, acc.: 99.22%] [G loss: 0.017878]\n",
      "epoch:23 step:18398 [D loss: 0.002259, acc.: 100.00%] [G loss: 1.087761]\n",
      "epoch:23 step:18399 [D loss: 0.010871, acc.: 100.00%] [G loss: 0.535608]\n",
      "epoch:23 step:18400 [D loss: 0.002116, acc.: 100.00%] [G loss: 0.384124]\n",
      "epoch:23 step:18401 [D loss: 0.012386, acc.: 100.00%] [G loss: 0.010783]\n",
      "epoch:23 step:18402 [D loss: 0.001043, acc.: 100.00%] [G loss: 0.006187]\n",
      "epoch:23 step:18403 [D loss: 0.001080, acc.: 100.00%] [G loss: 0.014104]\n",
      "epoch:23 step:18404 [D loss: 0.001439, acc.: 100.00%] [G loss: 0.074790]\n",
      "epoch:23 step:18405 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.061348]\n",
      "epoch:23 step:18406 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.044259]\n",
      "epoch:23 step:18407 [D loss: 0.008233, acc.: 100.00%] [G loss: 0.004518]\n",
      "epoch:23 step:18408 [D loss: 0.010097, acc.: 100.00%] [G loss: 0.007528]\n",
      "epoch:23 step:18409 [D loss: 0.000599, acc.: 100.00%] [G loss: 0.018164]\n",
      "epoch:23 step:18410 [D loss: 0.002500, acc.: 100.00%] [G loss: 0.007060]\n",
      "epoch:23 step:18411 [D loss: 0.005769, acc.: 100.00%] [G loss: 0.004879]\n",
      "epoch:23 step:18412 [D loss: 0.002425, acc.: 100.00%] [G loss: 0.003483]\n",
      "epoch:23 step:18413 [D loss: 0.001046, acc.: 100.00%] [G loss: 0.007029]\n",
      "epoch:23 step:18414 [D loss: 0.068703, acc.: 99.22%] [G loss: 0.114592]\n",
      "epoch:23 step:18415 [D loss: 0.029198, acc.: 100.00%] [G loss: 0.067482]\n",
      "epoch:23 step:18416 [D loss: 0.104616, acc.: 96.88%] [G loss: 0.004320]\n",
      "epoch:23 step:18417 [D loss: 0.001210, acc.: 100.00%] [G loss: 0.003554]\n",
      "epoch:23 step:18418 [D loss: 0.000438, acc.: 100.00%] [G loss: 0.002851]\n",
      "epoch:23 step:18419 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.021940]\n",
      "epoch:23 step:18420 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.310669]\n",
      "epoch:23 step:18421 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.248569]\n",
      "epoch:23 step:18422 [D loss: 0.000650, acc.: 100.00%] [G loss: 0.085044]\n",
      "epoch:23 step:18423 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.005908]\n",
      "epoch:23 step:18424 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.140464]\n",
      "epoch:23 step:18425 [D loss: 0.001055, acc.: 100.00%] [G loss: 0.003622]\n",
      "epoch:23 step:18426 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.010007]\n",
      "epoch:23 step:18427 [D loss: 0.001016, acc.: 100.00%] [G loss: 0.007266]\n",
      "epoch:23 step:18428 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.003431]\n",
      "epoch:23 step:18429 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.001195]\n",
      "epoch:23 step:18430 [D loss: 0.001594, acc.: 100.00%] [G loss: 0.121057]\n",
      "epoch:23 step:18431 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.002156]\n",
      "epoch:23 step:18432 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.024856]\n",
      "epoch:23 step:18433 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000697]\n",
      "epoch:23 step:18434 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.001282]\n",
      "epoch:23 step:18435 [D loss: 0.000476, acc.: 100.00%] [G loss: 0.002819]\n",
      "epoch:23 step:18436 [D loss: 0.001323, acc.: 100.00%] [G loss: 0.010913]\n",
      "epoch:23 step:18437 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.020647]\n",
      "epoch:23 step:18438 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:23 step:18439 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.002412]\n",
      "epoch:23 step:18440 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000598]\n",
      "epoch:23 step:18441 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:23 step:18442 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.004184]\n",
      "epoch:23 step:18443 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.016032]\n",
      "epoch:23 step:18444 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.007370]\n",
      "epoch:23 step:18445 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.000545]\n",
      "epoch:23 step:18446 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.005699]\n",
      "epoch:23 step:18447 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.012226]\n",
      "epoch:23 step:18448 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:23 step:18449 [D loss: 0.000521, acc.: 100.00%] [G loss: 0.006007]\n",
      "epoch:23 step:18450 [D loss: 0.000548, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:23 step:18451 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000793]\n",
      "epoch:23 step:18452 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.008240]\n",
      "epoch:23 step:18453 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000628]\n",
      "epoch:23 step:18454 [D loss: 0.001555, acc.: 100.00%] [G loss: 0.002482]\n",
      "epoch:23 step:18455 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.012350]\n",
      "epoch:23 step:18456 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000777]\n",
      "epoch:23 step:18457 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.002884]\n",
      "epoch:23 step:18458 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.003946]\n",
      "epoch:23 step:18459 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.002650]\n",
      "epoch:23 step:18460 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000729]\n",
      "epoch:23 step:18461 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.001348]\n",
      "epoch:23 step:18462 [D loss: 0.000650, acc.: 100.00%] [G loss: 0.003814]\n",
      "epoch:23 step:18463 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000554]\n",
      "epoch:23 step:18464 [D loss: 0.000679, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:23 step:18465 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.001994]\n",
      "epoch:23 step:18466 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.004527]\n",
      "epoch:23 step:18467 [D loss: 0.001345, acc.: 100.00%] [G loss: 0.000672]\n",
      "epoch:23 step:18468 [D loss: 0.000510, acc.: 100.00%] [G loss: 0.000231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18469 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:23 step:18470 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.001009]\n",
      "epoch:23 step:18471 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.002365]\n",
      "epoch:23 step:18472 [D loss: 0.000796, acc.: 100.00%] [G loss: 0.000670]\n",
      "epoch:23 step:18473 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.000756]\n",
      "epoch:23 step:18474 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.000725]\n",
      "epoch:23 step:18475 [D loss: 0.001005, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:23 step:18476 [D loss: 0.000477, acc.: 100.00%] [G loss: 0.000552]\n",
      "epoch:23 step:18477 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.002852]\n",
      "epoch:23 step:18478 [D loss: 0.001329, acc.: 100.00%] [G loss: 0.001325]\n",
      "epoch:23 step:18479 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:23 step:18480 [D loss: 0.009153, acc.: 100.00%] [G loss: 0.000870]\n",
      "epoch:23 step:18481 [D loss: 0.002130, acc.: 100.00%] [G loss: 0.001400]\n",
      "epoch:23 step:18482 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.001370]\n",
      "epoch:23 step:18483 [D loss: 0.001711, acc.: 100.00%] [G loss: 0.000530]\n",
      "epoch:23 step:18484 [D loss: 0.001243, acc.: 100.00%] [G loss: 0.001676]\n",
      "epoch:23 step:18485 [D loss: 0.104516, acc.: 95.31%] [G loss: 0.217741]\n",
      "epoch:23 step:18486 [D loss: 0.005006, acc.: 100.00%] [G loss: 0.917083]\n",
      "epoch:23 step:18487 [D loss: 0.338950, acc.: 88.28%] [G loss: 0.017325]\n",
      "epoch:23 step:18488 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001416]\n",
      "epoch:23 step:18489 [D loss: 0.004293, acc.: 100.00%] [G loss: 0.034160]\n",
      "epoch:23 step:18490 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.026621]\n",
      "epoch:23 step:18491 [D loss: 0.002270, acc.: 100.00%] [G loss: 0.010905]\n",
      "epoch:23 step:18492 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.049640]\n",
      "epoch:23 step:18493 [D loss: 0.001530, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:23 step:18494 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.017133]\n",
      "epoch:23 step:18495 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000336]\n",
      "epoch:23 step:18496 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.018168]\n",
      "epoch:23 step:18497 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.006744]\n",
      "epoch:23 step:18498 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.021273]\n",
      "epoch:23 step:18499 [D loss: 0.009236, acc.: 100.00%] [G loss: 0.033861]\n",
      "epoch:23 step:18500 [D loss: 0.001135, acc.: 100.00%] [G loss: 0.004687]\n",
      "epoch:23 step:18501 [D loss: 0.002834, acc.: 100.00%] [G loss: 0.114343]\n",
      "epoch:23 step:18502 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.022106]\n",
      "epoch:23 step:18503 [D loss: 0.675952, acc.: 71.09%] [G loss: 7.066599]\n",
      "epoch:23 step:18504 [D loss: 0.740956, acc.: 68.75%] [G loss: 2.206147]\n",
      "epoch:23 step:18505 [D loss: 0.488016, acc.: 78.91%] [G loss: 0.303610]\n",
      "epoch:23 step:18506 [D loss: 0.113745, acc.: 95.31%] [G loss: 0.061653]\n",
      "epoch:23 step:18507 [D loss: 0.024930, acc.: 100.00%] [G loss: 0.812279]\n",
      "epoch:23 step:18508 [D loss: 0.003953, acc.: 100.00%] [G loss: 0.283722]\n",
      "epoch:23 step:18509 [D loss: 0.055619, acc.: 99.22%] [G loss: 0.359836]\n",
      "epoch:23 step:18510 [D loss: 0.052807, acc.: 99.22%] [G loss: 0.068561]\n",
      "epoch:23 step:18511 [D loss: 0.010101, acc.: 100.00%] [G loss: 0.287180]\n",
      "epoch:23 step:18512 [D loss: 0.018009, acc.: 100.00%] [G loss: 0.072457]\n",
      "epoch:23 step:18513 [D loss: 0.233010, acc.: 93.75%] [G loss: 0.646461]\n",
      "epoch:23 step:18514 [D loss: 0.081268, acc.: 96.88%] [G loss: 2.149369]\n",
      "epoch:23 step:18515 [D loss: 0.024113, acc.: 100.00%] [G loss: 0.374268]\n",
      "epoch:23 step:18516 [D loss: 0.005937, acc.: 100.00%] [G loss: 0.313106]\n",
      "epoch:23 step:18517 [D loss: 0.010277, acc.: 100.00%] [G loss: 0.066782]\n",
      "epoch:23 step:18518 [D loss: 0.005164, acc.: 100.00%] [G loss: 0.107415]\n",
      "epoch:23 step:18519 [D loss: 0.009518, acc.: 100.00%] [G loss: 0.024842]\n",
      "epoch:23 step:18520 [D loss: 0.004336, acc.: 100.00%] [G loss: 0.046995]\n",
      "epoch:23 step:18521 [D loss: 0.004209, acc.: 100.00%] [G loss: 3.690600]\n",
      "epoch:23 step:18522 [D loss: 0.002808, acc.: 100.00%] [G loss: 0.192139]\n",
      "epoch:23 step:18523 [D loss: 0.002211, acc.: 100.00%] [G loss: 0.004417]\n",
      "epoch:23 step:18524 [D loss: 0.001788, acc.: 100.00%] [G loss: 0.592459]\n",
      "epoch:23 step:18525 [D loss: 0.002264, acc.: 100.00%] [G loss: 0.011994]\n",
      "epoch:23 step:18526 [D loss: 0.004630, acc.: 100.00%] [G loss: 0.009185]\n",
      "epoch:23 step:18527 [D loss: 0.001574, acc.: 100.00%] [G loss: 0.058852]\n",
      "epoch:23 step:18528 [D loss: 0.016754, acc.: 100.00%] [G loss: 0.084778]\n",
      "epoch:23 step:18529 [D loss: 0.028445, acc.: 99.22%] [G loss: 0.004402]\n",
      "epoch:23 step:18530 [D loss: 0.003375, acc.: 100.00%] [G loss: 0.034566]\n",
      "epoch:23 step:18531 [D loss: 0.002064, acc.: 100.00%] [G loss: 0.058441]\n",
      "epoch:23 step:18532 [D loss: 0.208774, acc.: 88.28%] [G loss: 0.051339]\n",
      "epoch:23 step:18533 [D loss: 0.019637, acc.: 99.22%] [G loss: 0.646239]\n",
      "epoch:23 step:18534 [D loss: 0.359364, acc.: 79.69%] [G loss: 0.001271]\n",
      "epoch:23 step:18535 [D loss: 0.000459, acc.: 100.00%] [G loss: 1.054097]\n",
      "epoch:23 step:18536 [D loss: 0.001121, acc.: 100.00%] [G loss: 0.541139]\n",
      "epoch:23 step:18537 [D loss: 0.000394, acc.: 100.00%] [G loss: 0.378965]\n",
      "epoch:23 step:18538 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:23 step:18539 [D loss: 0.020827, acc.: 100.00%] [G loss: 0.141043]\n",
      "epoch:23 step:18540 [D loss: 0.000904, acc.: 100.00%] [G loss: 0.000422]\n",
      "epoch:23 step:18541 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.306322]\n",
      "epoch:23 step:18542 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:23 step:18543 [D loss: 0.000968, acc.: 100.00%] [G loss: 0.268583]\n",
      "epoch:23 step:18544 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.083081]\n",
      "epoch:23 step:18545 [D loss: 0.001145, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:23 step:18546 [D loss: 0.004688, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:23 step:18547 [D loss: 0.000952, acc.: 100.00%] [G loss: 0.017191]\n",
      "epoch:23 step:18548 [D loss: 0.001739, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:23 step:18549 [D loss: 0.000921, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:23 step:18550 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.069625]\n",
      "epoch:23 step:18551 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.108556]\n",
      "epoch:23 step:18552 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:23 step:18553 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:23 step:18554 [D loss: 0.002922, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:23 step:18555 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:23 step:18556 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:23 step:18557 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.007217]\n",
      "epoch:23 step:18558 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.021594]\n",
      "epoch:23 step:18559 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.108750]\n",
      "epoch:23 step:18560 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.111112]\n",
      "epoch:23 step:18561 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.007988]\n",
      "epoch:23 step:18562 [D loss: 0.016702, acc.: 100.00%] [G loss: 0.015705]\n",
      "epoch:23 step:18563 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.079406]\n",
      "epoch:23 step:18564 [D loss: 0.002003, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:23 step:18565 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.146783]\n",
      "epoch:23 step:18566 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:23 step:18567 [D loss: 0.000920, acc.: 100.00%] [G loss: 0.035595]\n",
      "epoch:23 step:18568 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:23 step:18569 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.015145]\n",
      "epoch:23 step:18570 [D loss: 0.001184, acc.: 100.00%] [G loss: 0.012042]\n",
      "epoch:23 step:18571 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.018771]\n",
      "epoch:23 step:18572 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.092968]\n",
      "epoch:23 step:18573 [D loss: 0.000741, acc.: 100.00%] [G loss: 0.043450]\n",
      "epoch:23 step:18574 [D loss: 0.047823, acc.: 98.44%] [G loss: 0.160286]\n",
      "epoch:23 step:18575 [D loss: 0.000413, acc.: 100.00%] [G loss: 0.000835]\n",
      "epoch:23 step:18576 [D loss: 0.130407, acc.: 92.19%] [G loss: 0.015115]\n",
      "epoch:23 step:18577 [D loss: 0.211245, acc.: 89.84%] [G loss: 2.331043]\n",
      "epoch:23 step:18578 [D loss: 0.000796, acc.: 100.00%] [G loss: 0.004959]\n",
      "epoch:23 step:18579 [D loss: 0.065289, acc.: 96.88%] [G loss: 3.916458]\n",
      "epoch:23 step:18580 [D loss: 0.003765, acc.: 100.00%] [G loss: 2.457511]\n",
      "epoch:23 step:18581 [D loss: 0.055337, acc.: 97.66%] [G loss: 0.003571]\n",
      "epoch:23 step:18582 [D loss: 0.002930, acc.: 100.00%] [G loss: 0.340886]\n",
      "epoch:23 step:18583 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.000386]\n",
      "epoch:23 step:18584 [D loss: 0.019351, acc.: 100.00%] [G loss: 0.650482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18585 [D loss: 0.025055, acc.: 100.00%] [G loss: 0.002569]\n",
      "epoch:23 step:18586 [D loss: 0.636635, acc.: 67.19%] [G loss: 0.878694]\n",
      "epoch:23 step:18587 [D loss: 0.000120, acc.: 100.00%] [G loss: 2.916049]\n",
      "epoch:23 step:18588 [D loss: 0.022190, acc.: 99.22%] [G loss: 3.426116]\n",
      "epoch:23 step:18589 [D loss: 0.056900, acc.: 96.09%] [G loss: 1.630743]\n",
      "epoch:23 step:18590 [D loss: 0.011283, acc.: 100.00%] [G loss: 0.005867]\n",
      "epoch:23 step:18591 [D loss: 0.026695, acc.: 99.22%] [G loss: 0.000656]\n",
      "epoch:23 step:18592 [D loss: 0.002954, acc.: 100.00%] [G loss: 0.338429]\n",
      "epoch:23 step:18593 [D loss: 0.000743, acc.: 100.00%] [G loss: 0.001761]\n",
      "epoch:23 step:18594 [D loss: 0.003769, acc.: 100.00%] [G loss: 0.206302]\n",
      "epoch:23 step:18595 [D loss: 0.266271, acc.: 86.72%] [G loss: 0.013106]\n",
      "epoch:23 step:18596 [D loss: 0.037836, acc.: 100.00%] [G loss: 0.246555]\n",
      "epoch:23 step:18597 [D loss: 0.865120, acc.: 63.28%] [G loss: 0.000010]\n",
      "epoch:23 step:18598 [D loss: 0.009375, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:23 step:18599 [D loss: 0.003628, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:23 step:18600 [D loss: 0.376631, acc.: 82.81%] [G loss: 0.000028]\n",
      "epoch:23 step:18601 [D loss: 0.000130, acc.: 100.00%] [G loss: 3.294532]\n",
      "epoch:23 step:18602 [D loss: 0.002365, acc.: 100.00%] [G loss: 2.965944]\n",
      "epoch:23 step:18603 [D loss: 0.006156, acc.: 99.22%] [G loss: 1.830854]\n",
      "epoch:23 step:18604 [D loss: 0.003042, acc.: 100.00%] [G loss: 0.000416]\n",
      "epoch:23 step:18605 [D loss: 0.009890, acc.: 99.22%] [G loss: 0.000118]\n",
      "epoch:23 step:18606 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:23 step:18607 [D loss: 0.001472, acc.: 100.00%] [G loss: 0.000940]\n",
      "epoch:23 step:18608 [D loss: 0.001890, acc.: 100.00%] [G loss: 0.631905]\n",
      "epoch:23 step:18609 [D loss: 0.059483, acc.: 97.66%] [G loss: 0.674363]\n",
      "epoch:23 step:18610 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:23 step:18611 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000336]\n",
      "epoch:23 step:18612 [D loss: 0.001315, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:23 step:18613 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.069835]\n",
      "epoch:23 step:18614 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:23 step:18615 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:23 step:18616 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:23 step:18617 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:23 step:18618 [D loss: 0.002217, acc.: 100.00%] [G loss: 0.135448]\n",
      "epoch:23 step:18619 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001337]\n",
      "epoch:23 step:18620 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.066212]\n",
      "epoch:23 step:18621 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.030542]\n",
      "epoch:23 step:18622 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:23 step:18623 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.007242]\n",
      "epoch:23 step:18624 [D loss: 0.000734, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:23 step:18625 [D loss: 0.000931, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:23 step:18626 [D loss: 0.000870, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:23 step:18627 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.000916]\n",
      "epoch:23 step:18628 [D loss: 0.001446, acc.: 100.00%] [G loss: 0.034101]\n",
      "epoch:23 step:18629 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.061540]\n",
      "epoch:23 step:18630 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:23 step:18631 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:23 step:18632 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.015173]\n",
      "epoch:23 step:18633 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:23 step:18634 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.005695]\n",
      "epoch:23 step:18635 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:23 step:18636 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.035280]\n",
      "epoch:23 step:18637 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.023307]\n",
      "epoch:23 step:18638 [D loss: 0.002758, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:23 step:18639 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:23 step:18640 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:23 step:18641 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000299]\n",
      "epoch:23 step:18642 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:23 step:18643 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.032828]\n",
      "epoch:23 step:18644 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:23 step:18645 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.008258]\n",
      "epoch:23 step:18646 [D loss: 0.000864, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:23 step:18647 [D loss: 0.001802, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:23 step:18648 [D loss: 0.169119, acc.: 93.75%] [G loss: 0.000615]\n",
      "epoch:23 step:18649 [D loss: 0.000515, acc.: 100.00%] [G loss: 2.161150]\n",
      "epoch:23 step:18650 [D loss: 0.178247, acc.: 92.97%] [G loss: 0.695489]\n",
      "epoch:23 step:18651 [D loss: 0.001933, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:23 step:18652 [D loss: 0.000750, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:23 step:18653 [D loss: 0.001212, acc.: 100.00%] [G loss: 0.225878]\n",
      "epoch:23 step:18654 [D loss: 0.001150, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:23 step:18655 [D loss: 0.005092, acc.: 100.00%] [G loss: 0.136516]\n",
      "epoch:23 step:18656 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.140756]\n",
      "epoch:23 step:18657 [D loss: 0.000773, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:23 step:18658 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.028564]\n",
      "epoch:23 step:18659 [D loss: 0.001218, acc.: 100.00%] [G loss: 0.066129]\n",
      "epoch:23 step:18660 [D loss: 0.025489, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:23 step:18661 [D loss: 0.003355, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:23 step:18662 [D loss: 0.003466, acc.: 100.00%] [G loss: 0.185009]\n",
      "epoch:23 step:18663 [D loss: 0.000985, acc.: 100.00%] [G loss: 0.069264]\n",
      "epoch:23 step:18664 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000335]\n",
      "epoch:23 step:18665 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.028237]\n",
      "epoch:23 step:18666 [D loss: 0.001824, acc.: 100.00%] [G loss: 0.058053]\n",
      "epoch:23 step:18667 [D loss: 0.099842, acc.: 98.44%] [G loss: 0.000016]\n",
      "epoch:23 step:18668 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.090462]\n",
      "epoch:23 step:18669 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:23 step:18670 [D loss: 0.002436, acc.: 100.00%] [G loss: 0.047652]\n",
      "epoch:23 step:18671 [D loss: 0.003439, acc.: 100.00%] [G loss: 0.117396]\n",
      "epoch:23 step:18672 [D loss: 0.004934, acc.: 100.00%] [G loss: 0.035680]\n",
      "epoch:23 step:18673 [D loss: 0.122020, acc.: 96.88%] [G loss: 0.314340]\n",
      "epoch:23 step:18674 [D loss: 0.006463, acc.: 100.00%] [G loss: 0.011657]\n",
      "epoch:23 step:18675 [D loss: 0.011251, acc.: 100.00%] [G loss: 0.000729]\n",
      "epoch:23 step:18676 [D loss: 1.164589, acc.: 59.38%] [G loss: 0.000012]\n",
      "epoch:23 step:18677 [D loss: 2.372688, acc.: 52.34%] [G loss: 0.355245]\n",
      "epoch:23 step:18678 [D loss: 0.506878, acc.: 71.88%] [G loss: 0.453772]\n",
      "epoch:23 step:18679 [D loss: 0.356137, acc.: 83.59%] [G loss: 0.065615]\n",
      "epoch:23 step:18680 [D loss: 0.031920, acc.: 100.00%] [G loss: 0.003629]\n",
      "epoch:23 step:18681 [D loss: 0.025208, acc.: 99.22%] [G loss: 0.002270]\n",
      "epoch:23 step:18682 [D loss: 0.012086, acc.: 100.00%] [G loss: 0.000879]\n",
      "epoch:23 step:18683 [D loss: 0.020626, acc.: 100.00%] [G loss: 1.722826]\n",
      "epoch:23 step:18684 [D loss: 0.005563, acc.: 100.00%] [G loss: 0.000580]\n",
      "epoch:23 step:18685 [D loss: 0.004306, acc.: 100.00%] [G loss: 1.187904]\n",
      "epoch:23 step:18686 [D loss: 0.017562, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:23 step:18687 [D loss: 0.006892, acc.: 100.00%] [G loss: 0.000405]\n",
      "epoch:23 step:18688 [D loss: 0.015263, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:23 step:18689 [D loss: 0.007706, acc.: 100.00%] [G loss: 1.063878]\n",
      "epoch:23 step:18690 [D loss: 0.058709, acc.: 99.22%] [G loss: 0.000191]\n",
      "epoch:23 step:18691 [D loss: 0.006891, acc.: 100.00%] [G loss: 1.237684]\n",
      "epoch:23 step:18692 [D loss: 0.010203, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:23 step:18693 [D loss: 0.047929, acc.: 99.22%] [G loss: 0.000746]\n",
      "epoch:23 step:18694 [D loss: 0.012748, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:23 step:18695 [D loss: 0.023086, acc.: 100.00%] [G loss: 0.838509]\n",
      "epoch:23 step:18696 [D loss: 0.002066, acc.: 100.00%] [G loss: 0.631591]\n",
      "epoch:23 step:18697 [D loss: 0.014598, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:23 step:18698 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.208275]\n",
      "epoch:23 step:18699 [D loss: 0.005544, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:23 step:18700 [D loss: 0.006242, acc.: 100.00%] [G loss: 0.087843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:23 step:18701 [D loss: 0.001612, acc.: 100.00%] [G loss: 0.143382]\n",
      "epoch:23 step:18702 [D loss: 0.022568, acc.: 100.00%] [G loss: 0.031592]\n",
      "epoch:23 step:18703 [D loss: 0.007914, acc.: 100.00%] [G loss: 0.067896]\n",
      "epoch:23 step:18704 [D loss: 0.001895, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:23 step:18705 [D loss: 0.013663, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:23 step:18706 [D loss: 0.001120, acc.: 100.00%] [G loss: 0.018477]\n",
      "epoch:23 step:18707 [D loss: 0.005872, acc.: 100.00%] [G loss: 0.019770]\n",
      "epoch:23 step:18708 [D loss: 0.005129, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:23 step:18709 [D loss: 0.099966, acc.: 97.66%] [G loss: 0.000184]\n",
      "epoch:23 step:18710 [D loss: 0.003699, acc.: 100.00%] [G loss: 0.000889]\n",
      "epoch:23 step:18711 [D loss: 0.044677, acc.: 98.44%] [G loss: 0.000184]\n",
      "epoch:23 step:18712 [D loss: 0.003679, acc.: 100.00%] [G loss: 0.553339]\n",
      "epoch:23 step:18713 [D loss: 0.011081, acc.: 100.00%] [G loss: 0.256216]\n",
      "epoch:23 step:18714 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.140359]\n",
      "epoch:23 step:18715 [D loss: 0.005704, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:23 step:18716 [D loss: 0.026783, acc.: 99.22%] [G loss: 0.031350]\n",
      "epoch:23 step:18717 [D loss: 0.011775, acc.: 100.00%] [G loss: 0.018602]\n",
      "epoch:23 step:18718 [D loss: 0.016829, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:23 step:18719 [D loss: 0.010342, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:23 step:18720 [D loss: 0.001842, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:23 step:18721 [D loss: 0.003431, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:23 step:18722 [D loss: 0.005930, acc.: 100.00%] [G loss: 0.058174]\n",
      "epoch:23 step:18723 [D loss: 0.001074, acc.: 100.00%] [G loss: 0.052476]\n",
      "epoch:23 step:18724 [D loss: 0.004784, acc.: 100.00%] [G loss: 0.094811]\n",
      "epoch:23 step:18725 [D loss: 0.006767, acc.: 100.00%] [G loss: 0.140551]\n",
      "epoch:23 step:18726 [D loss: 0.002187, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:23 step:18727 [D loss: 0.006753, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:23 step:18728 [D loss: 0.003440, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:23 step:18729 [D loss: 0.004026, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:23 step:18730 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:23 step:18731 [D loss: 0.006178, acc.: 100.00%] [G loss: 0.026263]\n",
      "epoch:23 step:18732 [D loss: 0.000867, acc.: 100.00%] [G loss: 0.069102]\n",
      "epoch:23 step:18733 [D loss: 0.024339, acc.: 100.00%] [G loss: 0.031479]\n",
      "epoch:23 step:18734 [D loss: 0.067163, acc.: 98.44%] [G loss: 0.000002]\n",
      "epoch:23 step:18735 [D loss: 0.005272, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:23 step:18736 [D loss: 0.003322, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:23 step:18737 [D loss: 0.001758, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:23 step:18738 [D loss: 0.003718, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:23 step:18739 [D loss: 0.007262, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:23 step:18740 [D loss: 0.001658, acc.: 100.00%] [G loss: 0.009865]\n",
      "epoch:23 step:18741 [D loss: 0.045775, acc.: 99.22%] [G loss: 0.000012]\n",
      "epoch:23 step:18742 [D loss: 0.004466, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:23 step:18743 [D loss: 0.037020, acc.: 98.44%] [G loss: 0.001215]\n",
      "epoch:23 step:18744 [D loss: 0.024009, acc.: 100.00%] [G loss: 0.001036]\n",
      "epoch:24 step:18745 [D loss: 0.052376, acc.: 99.22%] [G loss: 0.878252]\n",
      "epoch:24 step:18746 [D loss: 0.007164, acc.: 100.00%] [G loss: 1.018479]\n",
      "epoch:24 step:18747 [D loss: 0.027283, acc.: 99.22%] [G loss: 0.000038]\n",
      "epoch:24 step:18748 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.024740]\n",
      "epoch:24 step:18749 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:24 step:18750 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:24 step:18751 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:24 step:18752 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.020768]\n",
      "epoch:24 step:18753 [D loss: 0.001939, acc.: 100.00%] [G loss: 0.036486]\n",
      "epoch:24 step:18754 [D loss: 0.005383, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:24 step:18755 [D loss: 0.004690, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:24 step:18756 [D loss: 0.001055, acc.: 100.00%] [G loss: 0.010990]\n",
      "epoch:24 step:18757 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:18758 [D loss: 0.001445, acc.: 100.00%] [G loss: 0.008493]\n",
      "epoch:24 step:18759 [D loss: 0.007146, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:24 step:18760 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:18761 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.005185]\n",
      "epoch:24 step:18762 [D loss: 0.000738, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:18763 [D loss: 0.000702, acc.: 100.00%] [G loss: 0.010043]\n",
      "epoch:24 step:18764 [D loss: 0.000874, acc.: 100.00%] [G loss: 0.011768]\n",
      "epoch:24 step:18765 [D loss: 0.026510, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:24 step:18766 [D loss: 0.000815, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:24 step:18767 [D loss: 0.012160, acc.: 100.00%] [G loss: 0.000296]\n",
      "epoch:24 step:18768 [D loss: 0.007111, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:24 step:18769 [D loss: 0.022130, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:24 step:18770 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.083298]\n",
      "epoch:24 step:18771 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.090075]\n",
      "epoch:24 step:18772 [D loss: 0.026975, acc.: 98.44%] [G loss: 0.017879]\n",
      "epoch:24 step:18773 [D loss: 0.000727, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:24 step:18774 [D loss: 0.001661, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:18775 [D loss: 0.008401, acc.: 100.00%] [G loss: 0.016683]\n",
      "epoch:24 step:18776 [D loss: 0.017320, acc.: 100.00%] [G loss: 0.013165]\n",
      "epoch:24 step:18777 [D loss: 0.000537, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:18778 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:24 step:18779 [D loss: 0.024607, acc.: 98.44%] [G loss: 0.000002]\n",
      "epoch:24 step:18780 [D loss: 0.001485, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:18781 [D loss: 0.001630, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:24 step:18782 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.007003]\n",
      "epoch:24 step:18783 [D loss: 0.047163, acc.: 99.22%] [G loss: 0.000045]\n",
      "epoch:24 step:18784 [D loss: 0.000636, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:24 step:18785 [D loss: 0.003593, acc.: 100.00%] [G loss: 0.167371]\n",
      "epoch:24 step:18786 [D loss: 0.000440, acc.: 100.00%] [G loss: 0.665261]\n",
      "epoch:24 step:18787 [D loss: 0.016277, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:24 step:18788 [D loss: 0.010401, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:24 step:18789 [D loss: 0.004666, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:24 step:18790 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.016536]\n",
      "epoch:24 step:18791 [D loss: 0.015522, acc.: 99.22%] [G loss: 0.008256]\n",
      "epoch:24 step:18792 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:24 step:18793 [D loss: 0.008030, acc.: 100.00%] [G loss: 0.004041]\n",
      "epoch:24 step:18794 [D loss: 0.014490, acc.: 100.00%] [G loss: 0.020259]\n",
      "epoch:24 step:18795 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.014792]\n",
      "epoch:24 step:18796 [D loss: 0.000956, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:24 step:18797 [D loss: 0.001096, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:24 step:18798 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:24 step:18799 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:24 step:18800 [D loss: 0.043093, acc.: 98.44%] [G loss: 0.000034]\n",
      "epoch:24 step:18801 [D loss: 0.001222, acc.: 100.00%] [G loss: 0.009741]\n",
      "epoch:24 step:18802 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.002842]\n",
      "epoch:24 step:18803 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:18804 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:24 step:18805 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.005065]\n",
      "epoch:24 step:18806 [D loss: 0.004207, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:24 step:18807 [D loss: 0.000977, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:24 step:18808 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.004044]\n",
      "epoch:24 step:18809 [D loss: 0.006946, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:24 step:18810 [D loss: 0.008275, acc.: 100.00%] [G loss: 0.002322]\n",
      "epoch:24 step:18811 [D loss: 0.000713, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:18812 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:24 step:18813 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.008794]\n",
      "epoch:24 step:18814 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.009265]\n",
      "epoch:24 step:18815 [D loss: 0.003165, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:24 step:18816 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18817 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.011372]\n",
      "epoch:24 step:18818 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:24 step:18819 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:18820 [D loss: 0.000872, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:24 step:18821 [D loss: 0.001780, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:24 step:18822 [D loss: 0.176943, acc.: 94.53%] [G loss: 0.066957]\n",
      "epoch:24 step:18823 [D loss: 0.171309, acc.: 91.41%] [G loss: 0.181623]\n",
      "epoch:24 step:18824 [D loss: 0.003349, acc.: 100.00%] [G loss: 0.124873]\n",
      "epoch:24 step:18825 [D loss: 0.002245, acc.: 100.00%] [G loss: 0.040264]\n",
      "epoch:24 step:18826 [D loss: 0.049926, acc.: 99.22%] [G loss: 0.768387]\n",
      "epoch:24 step:18827 [D loss: 0.007784, acc.: 100.00%] [G loss: 2.798919]\n",
      "epoch:24 step:18828 [D loss: 0.000543, acc.: 100.00%] [G loss: 4.358379]\n",
      "epoch:24 step:18829 [D loss: 0.009094, acc.: 100.00%] [G loss: 0.661396]\n",
      "epoch:24 step:18830 [D loss: 0.030618, acc.: 99.22%] [G loss: 0.573192]\n",
      "epoch:24 step:18831 [D loss: 0.030896, acc.: 100.00%] [G loss: 0.591833]\n",
      "epoch:24 step:18832 [D loss: 0.023178, acc.: 99.22%] [G loss: 0.422495]\n",
      "epoch:24 step:18833 [D loss: 0.010030, acc.: 100.00%] [G loss: 0.344836]\n",
      "epoch:24 step:18834 [D loss: 0.220822, acc.: 90.62%] [G loss: 0.004521]\n",
      "epoch:24 step:18835 [D loss: 0.076575, acc.: 96.88%] [G loss: 0.002027]\n",
      "epoch:24 step:18836 [D loss: 0.001432, acc.: 100.00%] [G loss: 0.869524]\n",
      "epoch:24 step:18837 [D loss: 0.001027, acc.: 100.00%] [G loss: 0.150397]\n",
      "epoch:24 step:18838 [D loss: 0.061605, acc.: 97.66%] [G loss: 0.704048]\n",
      "epoch:24 step:18839 [D loss: 0.002026, acc.: 100.00%] [G loss: 0.038643]\n",
      "epoch:24 step:18840 [D loss: 0.002712, acc.: 100.00%] [G loss: 0.032994]\n",
      "epoch:24 step:18841 [D loss: 0.007842, acc.: 100.00%] [G loss: 1.003882]\n",
      "epoch:24 step:18842 [D loss: 0.008066, acc.: 100.00%] [G loss: 0.015625]\n",
      "epoch:24 step:18843 [D loss: 0.002744, acc.: 100.00%] [G loss: 0.002495]\n",
      "epoch:24 step:18844 [D loss: 0.000996, acc.: 100.00%] [G loss: 0.813807]\n",
      "epoch:24 step:18845 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.003976]\n",
      "epoch:24 step:18846 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.001759]\n",
      "epoch:24 step:18847 [D loss: 0.012913, acc.: 100.00%] [G loss: 0.004970]\n",
      "epoch:24 step:18848 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.007326]\n",
      "epoch:24 step:18849 [D loss: 0.040658, acc.: 99.22%] [G loss: 0.001875]\n",
      "epoch:24 step:18850 [D loss: 0.000912, acc.: 100.00%] [G loss: 0.997404]\n",
      "epoch:24 step:18851 [D loss: 0.005290, acc.: 100.00%] [G loss: 0.020437]\n",
      "epoch:24 step:18852 [D loss: 0.002318, acc.: 100.00%] [G loss: 0.006945]\n",
      "epoch:24 step:18853 [D loss: 0.001348, acc.: 100.00%] [G loss: 0.350519]\n",
      "epoch:24 step:18854 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.432330]\n",
      "epoch:24 step:18855 [D loss: 0.078068, acc.: 96.88%] [G loss: 0.001880]\n",
      "epoch:24 step:18856 [D loss: 0.005634, acc.: 100.00%] [G loss: 0.002266]\n",
      "epoch:24 step:18857 [D loss: 0.001373, acc.: 100.00%] [G loss: 0.373749]\n",
      "epoch:24 step:18858 [D loss: 0.046083, acc.: 99.22%] [G loss: 0.050761]\n",
      "epoch:24 step:18859 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.301340]\n",
      "epoch:24 step:18860 [D loss: 0.010593, acc.: 100.00%] [G loss: 0.496570]\n",
      "epoch:24 step:18861 [D loss: 0.010866, acc.: 99.22%] [G loss: 0.023827]\n",
      "epoch:24 step:18862 [D loss: 0.000911, acc.: 100.00%] [G loss: 0.090156]\n",
      "epoch:24 step:18863 [D loss: 0.040854, acc.: 100.00%] [G loss: 0.000245]\n",
      "epoch:24 step:18864 [D loss: 0.003738, acc.: 100.00%] [G loss: 0.000789]\n",
      "epoch:24 step:18865 [D loss: 0.001591, acc.: 100.00%] [G loss: 1.260597]\n",
      "epoch:24 step:18866 [D loss: 0.005790, acc.: 100.00%] [G loss: 0.001637]\n",
      "epoch:24 step:18867 [D loss: 0.094865, acc.: 96.09%] [G loss: 0.070626]\n",
      "epoch:24 step:18868 [D loss: 0.057925, acc.: 98.44%] [G loss: 0.023433]\n",
      "epoch:24 step:18869 [D loss: 0.010126, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:24 step:18870 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:24 step:18871 [D loss: 0.000292, acc.: 100.00%] [G loss: 1.232226]\n",
      "epoch:24 step:18872 [D loss: 0.001794, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:24 step:18873 [D loss: 0.001341, acc.: 100.00%] [G loss: 0.193949]\n",
      "epoch:24 step:18874 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.601224]\n",
      "epoch:24 step:18875 [D loss: 0.007103, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:24 step:18876 [D loss: 0.002952, acc.: 100.00%] [G loss: 0.016558]\n",
      "epoch:24 step:18877 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:24 step:18878 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:24 step:18879 [D loss: 0.013590, acc.: 100.00%] [G loss: 0.009457]\n",
      "epoch:24 step:18880 [D loss: 0.002542, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:24 step:18881 [D loss: 0.001322, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:24 step:18882 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:24 step:18883 [D loss: 0.000806, acc.: 100.00%] [G loss: 0.000857]\n",
      "epoch:24 step:18884 [D loss: 0.000719, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:24 step:18885 [D loss: 0.003742, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:24 step:18886 [D loss: 0.002052, acc.: 100.00%] [G loss: 0.031505]\n",
      "epoch:24 step:18887 [D loss: 0.006121, acc.: 100.00%] [G loss: 0.015404]\n",
      "epoch:24 step:18888 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:24 step:18889 [D loss: 0.035120, acc.: 99.22%] [G loss: 0.058590]\n",
      "epoch:24 step:18890 [D loss: 0.006056, acc.: 100.00%] [G loss: 0.001530]\n",
      "epoch:24 step:18891 [D loss: 0.018322, acc.: 98.44%] [G loss: 0.200509]\n",
      "epoch:24 step:18892 [D loss: 0.020174, acc.: 100.00%] [G loss: 0.102903]\n",
      "epoch:24 step:18893 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.200076]\n",
      "epoch:24 step:18894 [D loss: 0.006657, acc.: 99.22%] [G loss: 0.079608]\n",
      "epoch:24 step:18895 [D loss: 0.002848, acc.: 100.00%] [G loss: 0.112996]\n",
      "epoch:24 step:18896 [D loss: 0.003630, acc.: 100.00%] [G loss: 0.152914]\n",
      "epoch:24 step:18897 [D loss: 0.001040, acc.: 100.00%] [G loss: 0.062298]\n",
      "epoch:24 step:18898 [D loss: 0.000515, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:24 step:18899 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:24 step:18900 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.002185]\n",
      "epoch:24 step:18901 [D loss: 0.003682, acc.: 100.00%] [G loss: 0.000546]\n",
      "epoch:24 step:18902 [D loss: 0.026291, acc.: 98.44%] [G loss: 0.000060]\n",
      "epoch:24 step:18903 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.006803]\n",
      "epoch:24 step:18904 [D loss: 0.000837, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:24 step:18905 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.007862]\n",
      "epoch:24 step:18906 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001029]\n",
      "epoch:24 step:18907 [D loss: 0.000983, acc.: 100.00%] [G loss: 0.007204]\n",
      "epoch:24 step:18908 [D loss: 0.004017, acc.: 100.00%] [G loss: 0.001042]\n",
      "epoch:24 step:18909 [D loss: 0.080234, acc.: 96.09%] [G loss: 0.002093]\n",
      "epoch:24 step:18910 [D loss: 0.032185, acc.: 99.22%] [G loss: 0.004055]\n",
      "epoch:24 step:18911 [D loss: 0.000050, acc.: 100.00%] [G loss: 1.614415]\n",
      "epoch:24 step:18912 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.014022]\n",
      "epoch:24 step:18913 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.001816]\n",
      "epoch:24 step:18914 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.725798]\n",
      "epoch:24 step:18915 [D loss: 0.002207, acc.: 100.00%] [G loss: 0.938182]\n",
      "epoch:24 step:18916 [D loss: 0.003306, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:24 step:18917 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.052780]\n",
      "epoch:24 step:18918 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:24 step:18919 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.005522]\n",
      "epoch:24 step:18920 [D loss: 0.021578, acc.: 99.22%] [G loss: 0.001748]\n",
      "epoch:24 step:18921 [D loss: 0.011610, acc.: 100.00%] [G loss: 0.027181]\n",
      "epoch:24 step:18922 [D loss: 0.001463, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:24 step:18923 [D loss: 0.018355, acc.: 99.22%] [G loss: 0.000268]\n",
      "epoch:24 step:18924 [D loss: 0.009137, acc.: 100.00%] [G loss: 0.000493]\n",
      "epoch:24 step:18925 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.005835]\n",
      "epoch:24 step:18926 [D loss: 0.002076, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:24 step:18927 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:24 step:18928 [D loss: 0.002863, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:24 step:18929 [D loss: 0.005718, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:24 step:18930 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:24 step:18931 [D loss: 0.003974, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:24 step:18932 [D loss: 0.000431, acc.: 100.00%] [G loss: 0.000029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:18933 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.087613]\n",
      "epoch:24 step:18934 [D loss: 0.001033, acc.: 100.00%] [G loss: 0.000923]\n",
      "epoch:24 step:18935 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.000224]\n",
      "epoch:24 step:18936 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000505]\n",
      "epoch:24 step:18937 [D loss: 0.002247, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:24 step:18938 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:24 step:18939 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.003283]\n",
      "epoch:24 step:18940 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:24 step:18941 [D loss: 0.005119, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:24 step:18942 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:24 step:18943 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000644]\n",
      "epoch:24 step:18944 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:24 step:18945 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:24 step:18946 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:24 step:18947 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:24 step:18948 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:18949 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:24 step:18950 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:24 step:18951 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001391]\n",
      "epoch:24 step:18952 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:24 step:18953 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:24 step:18954 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:24 step:18955 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:24 step:18956 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000344]\n",
      "epoch:24 step:18957 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.006243]\n",
      "epoch:24 step:18958 [D loss: 0.001032, acc.: 100.00%] [G loss: 0.000575]\n",
      "epoch:24 step:18959 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:24 step:18960 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:24 step:18961 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:24 step:18962 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:18963 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:24 step:18964 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.001420]\n",
      "epoch:24 step:18965 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:24 step:18966 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:24 step:18967 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:24 step:18968 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:18969 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:24 step:18970 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:24 step:18971 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:24 step:18972 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:24 step:18973 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:24 step:18974 [D loss: 0.004836, acc.: 100.00%] [G loss: 0.000418]\n",
      "epoch:24 step:18975 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:24 step:18976 [D loss: 0.001136, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:18977 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:18978 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:24 step:18979 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:18980 [D loss: 0.001942, acc.: 100.00%] [G loss: 0.000507]\n",
      "epoch:24 step:18981 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:24 step:18982 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:24 step:18983 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:24 step:18984 [D loss: 0.011863, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:18985 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:24 step:18986 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:18987 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000583]\n",
      "epoch:24 step:18988 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:24 step:18989 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:18990 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:24 step:18991 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:18992 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:24 step:18993 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:24 step:18994 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:18995 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:24 step:18996 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:24 step:18997 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:24 step:18998 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:18999 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:19000 [D loss: 0.000668, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:19001 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:19002 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:24 step:19003 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:24 step:19004 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:24 step:19005 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:24 step:19006 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:19007 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:24 step:19008 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:24 step:19009 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:19010 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:24 step:19011 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:24 step:19012 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:24 step:19013 [D loss: 0.001450, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:24 step:19014 [D loss: 0.003875, acc.: 100.00%] [G loss: 0.001138]\n",
      "epoch:24 step:19015 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000476]\n",
      "epoch:24 step:19016 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000444]\n",
      "epoch:24 step:19017 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000840]\n",
      "epoch:24 step:19018 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:24 step:19019 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.002332]\n",
      "epoch:24 step:19020 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:24 step:19021 [D loss: 0.001583, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:24 step:19022 [D loss: 0.001483, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:24 step:19023 [D loss: 2.305601, acc.: 52.34%] [G loss: 13.453663]\n",
      "epoch:24 step:19024 [D loss: 2.408079, acc.: 51.56%] [G loss: 0.792927]\n",
      "epoch:24 step:19025 [D loss: 0.239012, acc.: 89.06%] [G loss: 0.141554]\n",
      "epoch:24 step:19026 [D loss: 0.096250, acc.: 96.88%] [G loss: 1.637172]\n",
      "epoch:24 step:19027 [D loss: 0.217678, acc.: 88.28%] [G loss: 1.849895]\n",
      "epoch:24 step:19028 [D loss: 0.026444, acc.: 100.00%] [G loss: 2.968141]\n",
      "epoch:24 step:19029 [D loss: 0.058333, acc.: 100.00%] [G loss: 0.029530]\n",
      "epoch:24 step:19030 [D loss: 0.142871, acc.: 96.09%] [G loss: 2.275660]\n",
      "epoch:24 step:19031 [D loss: 0.091713, acc.: 98.44%] [G loss: 0.007080]\n",
      "epoch:24 step:19032 [D loss: 0.044789, acc.: 100.00%] [G loss: 2.462362]\n",
      "epoch:24 step:19033 [D loss: 0.129678, acc.: 96.88%] [G loss: 1.364549]\n",
      "epoch:24 step:19034 [D loss: 0.059955, acc.: 100.00%] [G loss: 4.169383]\n",
      "epoch:24 step:19035 [D loss: 0.053074, acc.: 98.44%] [G loss: 3.021035]\n",
      "epoch:24 step:19036 [D loss: 0.487280, acc.: 73.44%] [G loss: 2.896367]\n",
      "epoch:24 step:19037 [D loss: 0.511596, acc.: 79.69%] [G loss: 0.993456]\n",
      "epoch:24 step:19038 [D loss: 0.211106, acc.: 92.19%] [G loss: 0.004681]\n",
      "epoch:24 step:19039 [D loss: 0.093508, acc.: 98.44%] [G loss: 5.442106]\n",
      "epoch:24 step:19040 [D loss: 0.024845, acc.: 100.00%] [G loss: 5.113347]\n",
      "epoch:24 step:19041 [D loss: 0.036417, acc.: 100.00%] [G loss: 0.009442]\n",
      "epoch:24 step:19042 [D loss: 0.056883, acc.: 99.22%] [G loss: 3.089733]\n",
      "epoch:24 step:19043 [D loss: 0.083542, acc.: 99.22%] [G loss: 0.025591]\n",
      "epoch:24 step:19044 [D loss: 0.269486, acc.: 89.84%] [G loss: 0.044575]\n",
      "epoch:24 step:19045 [D loss: 0.087169, acc.: 98.44%] [G loss: 0.033817]\n",
      "epoch:24 step:19046 [D loss: 0.036154, acc.: 100.00%] [G loss: 4.796268]\n",
      "epoch:24 step:19047 [D loss: 0.020895, acc.: 100.00%] [G loss: 0.030895]\n",
      "epoch:24 step:19048 [D loss: 0.030439, acc.: 99.22%] [G loss: 0.003765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19049 [D loss: 0.028395, acc.: 99.22%] [G loss: 0.009985]\n",
      "epoch:24 step:19050 [D loss: 0.026666, acc.: 99.22%] [G loss: 1.988964]\n",
      "epoch:24 step:19051 [D loss: 0.338792, acc.: 81.25%] [G loss: 1.306905]\n",
      "epoch:24 step:19052 [D loss: 0.437959, acc.: 78.91%] [G loss: 7.012138]\n",
      "epoch:24 step:19053 [D loss: 0.032850, acc.: 98.44%] [G loss: 3.013015]\n",
      "epoch:24 step:19054 [D loss: 0.062627, acc.: 98.44%] [G loss: 1.353153]\n",
      "epoch:24 step:19055 [D loss: 0.045998, acc.: 98.44%] [G loss: 0.528893]\n",
      "epoch:24 step:19056 [D loss: 0.034505, acc.: 98.44%] [G loss: 0.000127]\n",
      "epoch:24 step:19057 [D loss: 0.018575, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:24 step:19058 [D loss: 0.018808, acc.: 100.00%] [G loss: 0.004955]\n",
      "epoch:24 step:19059 [D loss: 5.663820, acc.: 7.03%] [G loss: 0.242340]\n",
      "epoch:24 step:19060 [D loss: 0.033946, acc.: 99.22%] [G loss: 0.885312]\n",
      "epoch:24 step:19061 [D loss: 0.339747, acc.: 82.03%] [G loss: 0.145622]\n",
      "epoch:24 step:19062 [D loss: 0.242259, acc.: 90.62%] [G loss: 3.741845]\n",
      "epoch:24 step:19063 [D loss: 0.035194, acc.: 99.22%] [G loss: 0.034318]\n",
      "epoch:24 step:19064 [D loss: 0.078245, acc.: 98.44%] [G loss: 0.000765]\n",
      "epoch:24 step:19065 [D loss: 0.019649, acc.: 100.00%] [G loss: 0.000412]\n",
      "epoch:24 step:19066 [D loss: 0.023280, acc.: 100.00%] [G loss: 0.000570]\n",
      "epoch:24 step:19067 [D loss: 0.119051, acc.: 96.88%] [G loss: 0.002178]\n",
      "epoch:24 step:19068 [D loss: 0.058450, acc.: 98.44%] [G loss: 0.002600]\n",
      "epoch:24 step:19069 [D loss: 0.031955, acc.: 100.00%] [G loss: 3.469165]\n",
      "epoch:24 step:19070 [D loss: 0.009450, acc.: 100.00%] [G loss: 0.015613]\n",
      "epoch:24 step:19071 [D loss: 0.040801, acc.: 99.22%] [G loss: 0.001212]\n",
      "epoch:24 step:19072 [D loss: 0.004317, acc.: 100.00%] [G loss: 1.433661]\n",
      "epoch:24 step:19073 [D loss: 0.018294, acc.: 100.00%] [G loss: 1.575837]\n",
      "epoch:24 step:19074 [D loss: 0.100858, acc.: 98.44%] [G loss: 0.000503]\n",
      "epoch:24 step:19075 [D loss: 0.229744, acc.: 95.31%] [G loss: 3.199882]\n",
      "epoch:24 step:19076 [D loss: 0.174522, acc.: 92.97%] [G loss: 0.025285]\n",
      "epoch:24 step:19077 [D loss: 0.057584, acc.: 99.22%] [G loss: 0.043886]\n",
      "epoch:24 step:19078 [D loss: 0.077433, acc.: 99.22%] [G loss: 0.027413]\n",
      "epoch:24 step:19079 [D loss: 0.074868, acc.: 98.44%] [G loss: 1.873533]\n",
      "epoch:24 step:19080 [D loss: 0.020405, acc.: 100.00%] [G loss: 2.562621]\n",
      "epoch:24 step:19081 [D loss: 0.080868, acc.: 99.22%] [G loss: 0.156187]\n",
      "epoch:24 step:19082 [D loss: 0.534988, acc.: 67.97%] [G loss: 0.645071]\n",
      "epoch:24 step:19083 [D loss: 0.008767, acc.: 100.00%] [G loss: 2.848858]\n",
      "epoch:24 step:19084 [D loss: 0.255951, acc.: 87.50%] [G loss: 1.275480]\n",
      "epoch:24 step:19085 [D loss: 0.029808, acc.: 99.22%] [G loss: 0.349281]\n",
      "epoch:24 step:19086 [D loss: 0.026883, acc.: 100.00%] [G loss: 0.134244]\n",
      "epoch:24 step:19087 [D loss: 0.027495, acc.: 100.00%] [G loss: 0.047008]\n",
      "epoch:24 step:19088 [D loss: 0.015123, acc.: 100.00%] [G loss: 0.025004]\n",
      "epoch:24 step:19089 [D loss: 0.006882, acc.: 100.00%] [G loss: 0.012315]\n",
      "epoch:24 step:19090 [D loss: 0.008685, acc.: 100.00%] [G loss: 0.467441]\n",
      "epoch:24 step:19091 [D loss: 0.014226, acc.: 100.00%] [G loss: 0.005008]\n",
      "epoch:24 step:19092 [D loss: 0.118951, acc.: 96.09%] [G loss: 0.735336]\n",
      "epoch:24 step:19093 [D loss: 0.045554, acc.: 100.00%] [G loss: 0.115347]\n",
      "epoch:24 step:19094 [D loss: 0.137258, acc.: 92.97%] [G loss: 1.207976]\n",
      "epoch:24 step:19095 [D loss: 0.456400, acc.: 71.09%] [G loss: 3.598261]\n",
      "epoch:24 step:19096 [D loss: 0.239537, acc.: 90.62%] [G loss: 2.523378]\n",
      "epoch:24 step:19097 [D loss: 0.342306, acc.: 83.59%] [G loss: 0.400480]\n",
      "epoch:24 step:19098 [D loss: 0.121386, acc.: 96.88%] [G loss: 0.376109]\n",
      "epoch:24 step:19099 [D loss: 0.039728, acc.: 99.22%] [G loss: 0.606781]\n",
      "epoch:24 step:19100 [D loss: 0.114713, acc.: 96.88%] [G loss: 0.304791]\n",
      "epoch:24 step:19101 [D loss: 0.026533, acc.: 100.00%] [G loss: 0.352008]\n",
      "epoch:24 step:19102 [D loss: 0.149263, acc.: 95.31%] [G loss: 0.548271]\n",
      "epoch:24 step:19103 [D loss: 0.046796, acc.: 100.00%] [G loss: 0.727838]\n",
      "epoch:24 step:19104 [D loss: 0.051138, acc.: 100.00%] [G loss: 0.241883]\n",
      "epoch:24 step:19105 [D loss: 0.053080, acc.: 100.00%] [G loss: 0.208511]\n",
      "epoch:24 step:19106 [D loss: 0.013566, acc.: 100.00%] [G loss: 0.164496]\n",
      "epoch:24 step:19107 [D loss: 0.016491, acc.: 100.00%] [G loss: 0.346989]\n",
      "epoch:24 step:19108 [D loss: 0.025189, acc.: 100.00%] [G loss: 0.027984]\n",
      "epoch:24 step:19109 [D loss: 0.043153, acc.: 100.00%] [G loss: 0.063047]\n",
      "epoch:24 step:19110 [D loss: 0.006802, acc.: 100.00%] [G loss: 0.182816]\n",
      "epoch:24 step:19111 [D loss: 0.012991, acc.: 100.00%] [G loss: 0.065830]\n",
      "epoch:24 step:19112 [D loss: 0.036148, acc.: 98.44%] [G loss: 0.018518]\n",
      "epoch:24 step:19113 [D loss: 0.008381, acc.: 100.00%] [G loss: 5.426385]\n",
      "epoch:24 step:19114 [D loss: 0.038774, acc.: 100.00%] [G loss: 0.008159]\n",
      "epoch:24 step:19115 [D loss: 0.052112, acc.: 100.00%] [G loss: 1.690968]\n",
      "epoch:24 step:19116 [D loss: 0.046721, acc.: 99.22%] [G loss: 0.200859]\n",
      "epoch:24 step:19117 [D loss: 0.181711, acc.: 92.97%] [G loss: 1.650972]\n",
      "epoch:24 step:19118 [D loss: 0.207070, acc.: 89.84%] [G loss: 2.662675]\n",
      "epoch:24 step:19119 [D loss: 0.161583, acc.: 94.53%] [G loss: 0.317212]\n",
      "epoch:24 step:19120 [D loss: 0.030523, acc.: 100.00%] [G loss: 0.095758]\n",
      "epoch:24 step:19121 [D loss: 0.019137, acc.: 100.00%] [G loss: 0.085061]\n",
      "epoch:24 step:19122 [D loss: 0.054884, acc.: 99.22%] [G loss: 0.039010]\n",
      "epoch:24 step:19123 [D loss: 0.011961, acc.: 100.00%] [G loss: 0.053771]\n",
      "epoch:24 step:19124 [D loss: 0.025141, acc.: 99.22%] [G loss: 1.098238]\n",
      "epoch:24 step:19125 [D loss: 0.077842, acc.: 98.44%] [G loss: 0.794999]\n",
      "epoch:24 step:19126 [D loss: 0.022763, acc.: 99.22%] [G loss: 0.076107]\n",
      "epoch:24 step:19127 [D loss: 0.032707, acc.: 99.22%] [G loss: 0.388034]\n",
      "epoch:24 step:19128 [D loss: 0.025017, acc.: 100.00%] [G loss: 0.023415]\n",
      "epoch:24 step:19129 [D loss: 0.014809, acc.: 100.00%] [G loss: 0.006351]\n",
      "epoch:24 step:19130 [D loss: 0.021626, acc.: 100.00%] [G loss: 0.215580]\n",
      "epoch:24 step:19131 [D loss: 0.015939, acc.: 100.00%] [G loss: 0.004324]\n",
      "epoch:24 step:19132 [D loss: 0.051359, acc.: 98.44%] [G loss: 0.027742]\n",
      "epoch:24 step:19133 [D loss: 0.004873, acc.: 100.00%] [G loss: 0.004326]\n",
      "epoch:24 step:19134 [D loss: 0.004558, acc.: 100.00%] [G loss: 0.004874]\n",
      "epoch:24 step:19135 [D loss: 0.147461, acc.: 95.31%] [G loss: 1.626950]\n",
      "epoch:24 step:19136 [D loss: 0.003765, acc.: 100.00%] [G loss: 2.049169]\n",
      "epoch:24 step:19137 [D loss: 0.027215, acc.: 99.22%] [G loss: 0.024919]\n",
      "epoch:24 step:19138 [D loss: 0.083324, acc.: 96.09%] [G loss: 0.001880]\n",
      "epoch:24 step:19139 [D loss: 0.069061, acc.: 97.66%] [G loss: 0.000023]\n",
      "epoch:24 step:19140 [D loss: 0.038177, acc.: 100.00%] [G loss: 0.002285]\n",
      "epoch:24 step:19141 [D loss: 0.001260, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:24 step:19142 [D loss: 0.001451, acc.: 100.00%] [G loss: 0.005567]\n",
      "epoch:24 step:19143 [D loss: 0.168858, acc.: 93.75%] [G loss: 0.001431]\n",
      "epoch:24 step:19144 [D loss: 0.001749, acc.: 100.00%] [G loss: 0.183091]\n",
      "epoch:24 step:19145 [D loss: 0.803901, acc.: 64.84%] [G loss: 0.060738]\n",
      "epoch:24 step:19146 [D loss: 0.002865, acc.: 100.00%] [G loss: 0.008223]\n",
      "epoch:24 step:19147 [D loss: 0.005992, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:24 step:19148 [D loss: 0.027887, acc.: 100.00%] [G loss: 0.005074]\n",
      "epoch:24 step:19149 [D loss: 0.038374, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:24 step:19150 [D loss: 0.105444, acc.: 97.66%] [G loss: 0.000002]\n",
      "epoch:24 step:19151 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:24 step:19152 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:24 step:19153 [D loss: 0.000059, acc.: 100.00%] [G loss: 1.667906]\n",
      "epoch:24 step:19154 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:19155 [D loss: 0.014949, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:19156 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.400925]\n",
      "epoch:24 step:19157 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:19158 [D loss: 0.013000, acc.: 100.00%] [G loss: 0.454215]\n",
      "epoch:24 step:19159 [D loss: 0.002116, acc.: 100.00%] [G loss: 0.083540]\n",
      "epoch:24 step:19160 [D loss: 0.112565, acc.: 96.09%] [G loss: 0.000011]\n",
      "epoch:24 step:19161 [D loss: 0.001296, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:24 step:19162 [D loss: 0.018613, acc.: 99.22%] [G loss: 0.000005]\n",
      "epoch:24 step:19163 [D loss: 0.036955, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:24 step:19164 [D loss: 0.060812, acc.: 98.44%] [G loss: 0.000002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19165 [D loss: 0.020677, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:24 step:19166 [D loss: 0.055670, acc.: 99.22%] [G loss: 0.000001]\n",
      "epoch:24 step:19167 [D loss: 0.004985, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:24 step:19168 [D loss: 0.004558, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:24 step:19169 [D loss: 0.026693, acc.: 99.22%] [G loss: 2.761748]\n",
      "epoch:24 step:19170 [D loss: 0.000779, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:24 step:19171 [D loss: 0.004291, acc.: 100.00%] [G loss: 0.236717]\n",
      "epoch:24 step:19172 [D loss: 0.134523, acc.: 98.44%] [G loss: 0.704806]\n",
      "epoch:24 step:19173 [D loss: 0.012294, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:24 step:19174 [D loss: 0.214288, acc.: 91.41%] [G loss: 0.032451]\n",
      "epoch:24 step:19175 [D loss: 0.649850, acc.: 62.50%] [G loss: 1.070080]\n",
      "epoch:24 step:19176 [D loss: 0.724437, acc.: 67.19%] [G loss: 0.142376]\n",
      "epoch:24 step:19177 [D loss: 0.011280, acc.: 100.00%] [G loss: 0.128965]\n",
      "epoch:24 step:19178 [D loss: 0.010600, acc.: 100.00%] [G loss: 7.331064]\n",
      "epoch:24 step:19179 [D loss: 0.012905, acc.: 100.00%] [G loss: 0.019156]\n",
      "epoch:24 step:19180 [D loss: 0.016298, acc.: 100.00%] [G loss: 2.876315]\n",
      "epoch:24 step:19181 [D loss: 0.065946, acc.: 100.00%] [G loss: 0.010654]\n",
      "epoch:24 step:19182 [D loss: 0.016941, acc.: 100.00%] [G loss: 4.347782]\n",
      "epoch:24 step:19183 [D loss: 0.048982, acc.: 100.00%] [G loss: 0.010592]\n",
      "epoch:24 step:19184 [D loss: 0.021248, acc.: 100.00%] [G loss: 0.017858]\n",
      "epoch:24 step:19185 [D loss: 0.029260, acc.: 100.00%] [G loss: 0.030061]\n",
      "epoch:24 step:19186 [D loss: 0.005971, acc.: 100.00%] [G loss: 1.879133]\n",
      "epoch:24 step:19187 [D loss: 0.046043, acc.: 100.00%] [G loss: 1.982444]\n",
      "epoch:24 step:19188 [D loss: 0.056460, acc.: 99.22%] [G loss: 0.016026]\n",
      "epoch:24 step:19189 [D loss: 0.139281, acc.: 96.88%] [G loss: 0.006656]\n",
      "epoch:24 step:19190 [D loss: 0.015365, acc.: 100.00%] [G loss: 2.264654]\n",
      "epoch:24 step:19191 [D loss: 0.021495, acc.: 99.22%] [G loss: 1.678178]\n",
      "epoch:24 step:19192 [D loss: 0.029668, acc.: 99.22%] [G loss: 0.024354]\n",
      "epoch:24 step:19193 [D loss: 0.011275, acc.: 100.00%] [G loss: 0.705217]\n",
      "epoch:24 step:19194 [D loss: 0.009110, acc.: 100.00%] [G loss: 0.119074]\n",
      "epoch:24 step:19195 [D loss: 0.705064, acc.: 65.62%] [G loss: 3.868933]\n",
      "epoch:24 step:19196 [D loss: 0.782661, acc.: 65.62%] [G loss: 7.265185]\n",
      "epoch:24 step:19197 [D loss: 0.060180, acc.: 97.66%] [G loss: 0.147715]\n",
      "epoch:24 step:19198 [D loss: 0.025641, acc.: 100.00%] [G loss: 0.042279]\n",
      "epoch:24 step:19199 [D loss: 0.003369, acc.: 100.00%] [G loss: 0.010389]\n",
      "epoch:24 step:19200 [D loss: 0.017012, acc.: 100.00%] [G loss: 0.278188]\n",
      "epoch:24 step:19201 [D loss: 0.008066, acc.: 100.00%] [G loss: 5.492598]\n",
      "epoch:24 step:19202 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.003096]\n",
      "epoch:24 step:19203 [D loss: 0.006371, acc.: 99.22%] [G loss: 0.000453]\n",
      "epoch:24 step:19204 [D loss: 0.019406, acc.: 100.00%] [G loss: 0.000447]\n",
      "epoch:24 step:19205 [D loss: 0.000969, acc.: 100.00%] [G loss: 1.574499]\n",
      "epoch:24 step:19206 [D loss: 0.003525, acc.: 100.00%] [G loss: 0.000440]\n",
      "epoch:24 step:19207 [D loss: 0.002788, acc.: 100.00%] [G loss: 0.000604]\n",
      "epoch:24 step:19208 [D loss: 0.049589, acc.: 100.00%] [G loss: 0.839656]\n",
      "epoch:24 step:19209 [D loss: 0.007728, acc.: 100.00%] [G loss: 0.008326]\n",
      "epoch:24 step:19210 [D loss: 0.007199, acc.: 100.00%] [G loss: 2.246462]\n",
      "epoch:24 step:19211 [D loss: 0.014397, acc.: 100.00%] [G loss: 0.000483]\n",
      "epoch:24 step:19212 [D loss: 0.055697, acc.: 98.44%] [G loss: 0.601276]\n",
      "epoch:24 step:19213 [D loss: 0.004049, acc.: 100.00%] [G loss: 0.006654]\n",
      "epoch:24 step:19214 [D loss: 0.029585, acc.: 99.22%] [G loss: 2.139627]\n",
      "epoch:24 step:19215 [D loss: 0.084750, acc.: 95.31%] [G loss: 0.049913]\n",
      "epoch:24 step:19216 [D loss: 0.097160, acc.: 97.66%] [G loss: 0.067887]\n",
      "epoch:24 step:19217 [D loss: 0.057037, acc.: 97.66%] [G loss: 1.168505]\n",
      "epoch:24 step:19218 [D loss: 0.044991, acc.: 99.22%] [G loss: 0.002569]\n",
      "epoch:24 step:19219 [D loss: 0.021068, acc.: 100.00%] [G loss: 0.000320]\n",
      "epoch:24 step:19220 [D loss: 0.164218, acc.: 96.09%] [G loss: 0.089305]\n",
      "epoch:24 step:19221 [D loss: 0.057509, acc.: 98.44%] [G loss: 0.204010]\n",
      "epoch:24 step:19222 [D loss: 0.287883, acc.: 86.72%] [G loss: 0.000945]\n",
      "epoch:24 step:19223 [D loss: 0.006343, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:24 step:19224 [D loss: 0.018937, acc.: 100.00%] [G loss: 0.002104]\n",
      "epoch:24 step:19225 [D loss: 0.061843, acc.: 99.22%] [G loss: 0.023746]\n",
      "epoch:24 step:19226 [D loss: 0.024023, acc.: 99.22%] [G loss: 0.358187]\n",
      "epoch:24 step:19227 [D loss: 0.004200, acc.: 100.00%] [G loss: 2.736240]\n",
      "epoch:24 step:19228 [D loss: 0.000556, acc.: 100.00%] [G loss: 2.009374]\n",
      "epoch:24 step:19229 [D loss: 0.002184, acc.: 100.00%] [G loss: 0.045750]\n",
      "epoch:24 step:19230 [D loss: 0.007155, acc.: 100.00%] [G loss: 0.027179]\n",
      "epoch:24 step:19231 [D loss: 0.012779, acc.: 100.00%] [G loss: 0.022756]\n",
      "epoch:24 step:19232 [D loss: 0.003385, acc.: 100.00%] [G loss: 0.085702]\n",
      "epoch:24 step:19233 [D loss: 0.010748, acc.: 100.00%] [G loss: 0.015997]\n",
      "epoch:24 step:19234 [D loss: 0.011155, acc.: 100.00%] [G loss: 0.010836]\n",
      "epoch:24 step:19235 [D loss: 0.028905, acc.: 99.22%] [G loss: 0.265916]\n",
      "epoch:24 step:19236 [D loss: 0.001192, acc.: 100.00%] [G loss: 0.243432]\n",
      "epoch:24 step:19237 [D loss: 0.007011, acc.: 100.00%] [G loss: 0.106713]\n",
      "epoch:24 step:19238 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.012168]\n",
      "epoch:24 step:19239 [D loss: 0.000747, acc.: 100.00%] [G loss: 0.039819]\n",
      "epoch:24 step:19240 [D loss: 0.001319, acc.: 100.00%] [G loss: 2.676747]\n",
      "epoch:24 step:19241 [D loss: 0.003972, acc.: 100.00%] [G loss: 0.024397]\n",
      "epoch:24 step:19242 [D loss: 1.105937, acc.: 58.59%] [G loss: 2.227451]\n",
      "epoch:24 step:19243 [D loss: 2.728076, acc.: 50.00%] [G loss: 0.966262]\n",
      "epoch:24 step:19244 [D loss: 0.035864, acc.: 100.00%] [G loss: 0.313561]\n",
      "epoch:24 step:19245 [D loss: 0.074525, acc.: 96.88%] [G loss: 0.276075]\n",
      "epoch:24 step:19246 [D loss: 0.010752, acc.: 100.00%] [G loss: 2.959635]\n",
      "epoch:24 step:19247 [D loss: 0.014972, acc.: 100.00%] [G loss: 2.480488]\n",
      "epoch:24 step:19248 [D loss: 0.034926, acc.: 100.00%] [G loss: 0.013372]\n",
      "epoch:24 step:19249 [D loss: 0.076584, acc.: 99.22%] [G loss: 3.218181]\n",
      "epoch:24 step:19250 [D loss: 0.066499, acc.: 99.22%] [G loss: 0.476786]\n",
      "epoch:24 step:19251 [D loss: 0.068405, acc.: 100.00%] [G loss: 0.019249]\n",
      "epoch:24 step:19252 [D loss: 0.043029, acc.: 99.22%] [G loss: 0.263506]\n",
      "epoch:24 step:19253 [D loss: 0.041761, acc.: 100.00%] [G loss: 0.237425]\n",
      "epoch:24 step:19254 [D loss: 0.264715, acc.: 88.28%] [G loss: 1.104233]\n",
      "epoch:24 step:19255 [D loss: 0.191306, acc.: 92.97%] [G loss: 1.668992]\n",
      "epoch:24 step:19256 [D loss: 0.083792, acc.: 96.88%] [G loss: 5.975183]\n",
      "epoch:24 step:19257 [D loss: 0.074373, acc.: 97.66%] [G loss: 3.638484]\n",
      "epoch:24 step:19258 [D loss: 0.081165, acc.: 97.66%] [G loss: 0.513766]\n",
      "epoch:24 step:19259 [D loss: 0.024174, acc.: 99.22%] [G loss: 5.000520]\n",
      "epoch:24 step:19260 [D loss: 0.176385, acc.: 92.97%] [G loss: 4.036477]\n",
      "epoch:24 step:19261 [D loss: 0.182597, acc.: 92.19%] [G loss: 0.921212]\n",
      "epoch:24 step:19262 [D loss: 0.038590, acc.: 100.00%] [G loss: 0.405600]\n",
      "epoch:24 step:19263 [D loss: 0.009601, acc.: 100.00%] [G loss: 0.360018]\n",
      "epoch:24 step:19264 [D loss: 0.044826, acc.: 99.22%] [G loss: 0.577296]\n",
      "epoch:24 step:19265 [D loss: 0.021663, acc.: 100.00%] [G loss: 0.038331]\n",
      "epoch:24 step:19266 [D loss: 0.053326, acc.: 99.22%] [G loss: 0.057866]\n",
      "epoch:24 step:19267 [D loss: 0.011460, acc.: 100.00%] [G loss: 1.416100]\n",
      "epoch:24 step:19268 [D loss: 0.016621, acc.: 100.00%] [G loss: 0.079198]\n",
      "epoch:24 step:19269 [D loss: 0.014820, acc.: 100.00%] [G loss: 0.290733]\n",
      "epoch:24 step:19270 [D loss: 0.312348, acc.: 85.94%] [G loss: 0.741402]\n",
      "epoch:24 step:19271 [D loss: 0.029082, acc.: 99.22%] [G loss: 2.695993]\n",
      "epoch:24 step:19272 [D loss: 0.053903, acc.: 96.88%] [G loss: 2.442379]\n",
      "epoch:24 step:19273 [D loss: 0.017046, acc.: 100.00%] [G loss: 1.161387]\n",
      "epoch:24 step:19274 [D loss: 0.004226, acc.: 100.00%] [G loss: 1.080522]\n",
      "epoch:24 step:19275 [D loss: 0.001215, acc.: 100.00%] [G loss: 0.435868]\n",
      "epoch:24 step:19276 [D loss: 0.022483, acc.: 99.22%] [G loss: 0.501251]\n",
      "epoch:24 step:19277 [D loss: 0.006338, acc.: 100.00%] [G loss: 0.696164]\n",
      "epoch:24 step:19278 [D loss: 0.015403, acc.: 100.00%] [G loss: 0.227100]\n",
      "epoch:24 step:19279 [D loss: 0.031438, acc.: 100.00%] [G loss: 3.605867]\n",
      "epoch:24 step:19280 [D loss: 0.005116, acc.: 100.00%] [G loss: 3.983548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19281 [D loss: 0.038753, acc.: 99.22%] [G loss: 5.340222]\n",
      "epoch:24 step:19282 [D loss: 0.046796, acc.: 98.44%] [G loss: 3.436738]\n",
      "epoch:24 step:19283 [D loss: 0.031582, acc.: 99.22%] [G loss: 4.309015]\n",
      "epoch:24 step:19284 [D loss: 0.012286, acc.: 100.00%] [G loss: 4.058399]\n",
      "epoch:24 step:19285 [D loss: 0.017396, acc.: 100.00%] [G loss: 4.871735]\n",
      "epoch:24 step:19286 [D loss: 0.055858, acc.: 98.44%] [G loss: 0.217218]\n",
      "epoch:24 step:19287 [D loss: 0.640023, acc.: 70.31%] [G loss: 9.811010]\n",
      "epoch:24 step:19288 [D loss: 2.441363, acc.: 49.22%] [G loss: 2.158718]\n",
      "epoch:24 step:19289 [D loss: 0.082616, acc.: 97.66%] [G loss: 2.107020]\n",
      "epoch:24 step:19290 [D loss: 0.049364, acc.: 100.00%] [G loss: 1.678706]\n",
      "epoch:24 step:19291 [D loss: 0.074086, acc.: 97.66%] [G loss: 0.440430]\n",
      "epoch:24 step:19292 [D loss: 0.122582, acc.: 97.66%] [G loss: 1.410604]\n",
      "epoch:24 step:19293 [D loss: 0.016060, acc.: 100.00%] [G loss: 1.019727]\n",
      "epoch:24 step:19294 [D loss: 0.035877, acc.: 100.00%] [G loss: 3.298079]\n",
      "epoch:24 step:19295 [D loss: 0.031426, acc.: 100.00%] [G loss: 0.550251]\n",
      "epoch:24 step:19296 [D loss: 0.048729, acc.: 99.22%] [G loss: 1.967149]\n",
      "epoch:24 step:19297 [D loss: 0.042448, acc.: 99.22%] [G loss: 0.964276]\n",
      "epoch:24 step:19298 [D loss: 0.054935, acc.: 98.44%] [G loss: 1.041833]\n",
      "epoch:24 step:19299 [D loss: 0.014270, acc.: 100.00%] [G loss: 0.941174]\n",
      "epoch:24 step:19300 [D loss: 0.045198, acc.: 100.00%] [G loss: 0.591556]\n",
      "epoch:24 step:19301 [D loss: 0.080843, acc.: 98.44%] [G loss: 0.118457]\n",
      "epoch:24 step:19302 [D loss: 0.056511, acc.: 100.00%] [G loss: 0.823827]\n",
      "epoch:24 step:19303 [D loss: 0.062749, acc.: 96.88%] [G loss: 0.465236]\n",
      "epoch:24 step:19304 [D loss: 0.022470, acc.: 100.00%] [G loss: 0.562067]\n",
      "epoch:24 step:19305 [D loss: 0.035606, acc.: 100.00%] [G loss: 0.354090]\n",
      "epoch:24 step:19306 [D loss: 0.029435, acc.: 99.22%] [G loss: 0.093667]\n",
      "epoch:24 step:19307 [D loss: 0.016802, acc.: 100.00%] [G loss: 0.049288]\n",
      "epoch:24 step:19308 [D loss: 0.008369, acc.: 100.00%] [G loss: 0.012721]\n",
      "epoch:24 step:19309 [D loss: 0.036958, acc.: 100.00%] [G loss: 0.031705]\n",
      "epoch:24 step:19310 [D loss: 0.041790, acc.: 99.22%] [G loss: 0.025628]\n",
      "epoch:24 step:19311 [D loss: 0.024749, acc.: 100.00%] [G loss: 0.002447]\n",
      "epoch:24 step:19312 [D loss: 0.012466, acc.: 100.00%] [G loss: 0.160826]\n",
      "epoch:24 step:19313 [D loss: 0.100645, acc.: 96.88%] [G loss: 0.732177]\n",
      "epoch:24 step:19314 [D loss: 0.004279, acc.: 100.00%] [G loss: 0.216374]\n",
      "epoch:24 step:19315 [D loss: 0.077301, acc.: 97.66%] [G loss: 0.532241]\n",
      "epoch:24 step:19316 [D loss: 0.010509, acc.: 100.00%] [G loss: 0.604638]\n",
      "epoch:24 step:19317 [D loss: 0.077418, acc.: 96.88%] [G loss: 0.037919]\n",
      "epoch:24 step:19318 [D loss: 0.003218, acc.: 100.00%] [G loss: 0.055852]\n",
      "epoch:24 step:19319 [D loss: 0.037530, acc.: 100.00%] [G loss: 0.044029]\n",
      "epoch:24 step:19320 [D loss: 0.035137, acc.: 100.00%] [G loss: 0.043291]\n",
      "epoch:24 step:19321 [D loss: 0.041425, acc.: 100.00%] [G loss: 0.671087]\n",
      "epoch:24 step:19322 [D loss: 0.025267, acc.: 99.22%] [G loss: 0.817716]\n",
      "epoch:24 step:19323 [D loss: 0.264895, acc.: 85.16%] [G loss: 4.212161]\n",
      "epoch:24 step:19324 [D loss: 0.713375, acc.: 71.09%] [G loss: 1.390613]\n",
      "epoch:24 step:19325 [D loss: 0.103486, acc.: 98.44%] [G loss: 2.809615]\n",
      "epoch:24 step:19326 [D loss: 0.033071, acc.: 100.00%] [G loss: 5.004791]\n",
      "epoch:24 step:19327 [D loss: 0.050136, acc.: 98.44%] [G loss: 0.782852]\n",
      "epoch:24 step:19328 [D loss: 0.051727, acc.: 100.00%] [G loss: 1.817860]\n",
      "epoch:24 step:19329 [D loss: 0.033207, acc.: 100.00%] [G loss: 2.548619]\n",
      "epoch:24 step:19330 [D loss: 0.026567, acc.: 100.00%] [G loss: 2.301754]\n",
      "epoch:24 step:19331 [D loss: 0.175099, acc.: 95.31%] [G loss: 1.695647]\n",
      "epoch:24 step:19332 [D loss: 0.280022, acc.: 88.28%] [G loss: 0.871461]\n",
      "epoch:24 step:19333 [D loss: 0.078481, acc.: 96.88%] [G loss: 0.120251]\n",
      "epoch:24 step:19334 [D loss: 0.011505, acc.: 100.00%] [G loss: 2.836911]\n",
      "epoch:24 step:19335 [D loss: 0.145341, acc.: 93.75%] [G loss: 0.679542]\n",
      "epoch:24 step:19336 [D loss: 0.139864, acc.: 93.75%] [G loss: 5.060125]\n",
      "epoch:24 step:19337 [D loss: 0.013929, acc.: 99.22%] [G loss: 0.014799]\n",
      "epoch:24 step:19338 [D loss: 0.002982, acc.: 100.00%] [G loss: 0.025418]\n",
      "epoch:24 step:19339 [D loss: 0.019449, acc.: 99.22%] [G loss: 0.029759]\n",
      "epoch:24 step:19340 [D loss: 0.011139, acc.: 100.00%] [G loss: 2.070551]\n",
      "epoch:24 step:19341 [D loss: 0.002003, acc.: 100.00%] [G loss: 1.755653]\n",
      "epoch:24 step:19342 [D loss: 0.011717, acc.: 100.00%] [G loss: 0.002877]\n",
      "epoch:24 step:19343 [D loss: 0.004081, acc.: 100.00%] [G loss: 2.844172]\n",
      "epoch:24 step:19344 [D loss: 0.001808, acc.: 100.00%] [G loss: 0.001867]\n",
      "epoch:24 step:19345 [D loss: 0.023606, acc.: 99.22%] [G loss: 0.268005]\n",
      "epoch:24 step:19346 [D loss: 0.001720, acc.: 100.00%] [G loss: 0.015793]\n",
      "epoch:24 step:19347 [D loss: 0.022082, acc.: 100.00%] [G loss: 0.009904]\n",
      "epoch:24 step:19348 [D loss: 0.052277, acc.: 99.22%] [G loss: 0.013225]\n",
      "epoch:24 step:19349 [D loss: 0.007059, acc.: 100.00%] [G loss: 0.101336]\n",
      "epoch:24 step:19350 [D loss: 0.029617, acc.: 99.22%] [G loss: 1.431343]\n",
      "epoch:24 step:19351 [D loss: 0.008067, acc.: 100.00%] [G loss: 0.891981]\n",
      "epoch:24 step:19352 [D loss: 0.001620, acc.: 100.00%] [G loss: 0.310271]\n",
      "epoch:24 step:19353 [D loss: 0.010822, acc.: 100.00%] [G loss: 0.009809]\n",
      "epoch:24 step:19354 [D loss: 0.005576, acc.: 100.00%] [G loss: 0.009123]\n",
      "epoch:24 step:19355 [D loss: 0.002629, acc.: 100.00%] [G loss: 0.013317]\n",
      "epoch:24 step:19356 [D loss: 0.001588, acc.: 100.00%] [G loss: 0.010932]\n",
      "epoch:24 step:19357 [D loss: 0.078246, acc.: 97.66%] [G loss: 0.000241]\n",
      "epoch:24 step:19358 [D loss: 0.087764, acc.: 98.44%] [G loss: 0.030151]\n",
      "epoch:24 step:19359 [D loss: 0.002560, acc.: 100.00%] [G loss: 1.829906]\n",
      "epoch:24 step:19360 [D loss: 0.025709, acc.: 99.22%] [G loss: 2.203480]\n",
      "epoch:24 step:19361 [D loss: 0.105588, acc.: 96.09%] [G loss: 3.873923]\n",
      "epoch:24 step:19362 [D loss: 0.508916, acc.: 78.12%] [G loss: 6.083455]\n",
      "epoch:24 step:19363 [D loss: 0.055436, acc.: 99.22%] [G loss: 2.867717]\n",
      "epoch:24 step:19364 [D loss: 0.107560, acc.: 96.88%] [G loss: 1.643139]\n",
      "epoch:24 step:19365 [D loss: 0.012191, acc.: 100.00%] [G loss: 3.624063]\n",
      "epoch:24 step:19366 [D loss: 0.018754, acc.: 99.22%] [G loss: 2.714652]\n",
      "epoch:24 step:19367 [D loss: 0.151652, acc.: 91.41%] [G loss: 4.408392]\n",
      "epoch:24 step:19368 [D loss: 0.020007, acc.: 100.00%] [G loss: 0.696858]\n",
      "epoch:24 step:19369 [D loss: 0.114898, acc.: 96.09%] [G loss: 0.005013]\n",
      "epoch:24 step:19370 [D loss: 0.064258, acc.: 100.00%] [G loss: 0.461561]\n",
      "epoch:24 step:19371 [D loss: 0.023018, acc.: 100.00%] [G loss: 0.017654]\n",
      "epoch:24 step:19372 [D loss: 0.040073, acc.: 98.44%] [G loss: 0.050336]\n",
      "epoch:24 step:19373 [D loss: 0.043288, acc.: 99.22%] [G loss: 0.033358]\n",
      "epoch:24 step:19374 [D loss: 0.001762, acc.: 100.00%] [G loss: 0.109696]\n",
      "epoch:24 step:19375 [D loss: 0.009879, acc.: 100.00%] [G loss: 0.292408]\n",
      "epoch:24 step:19376 [D loss: 0.000963, acc.: 100.00%] [G loss: 0.031364]\n",
      "epoch:24 step:19377 [D loss: 0.001374, acc.: 100.00%] [G loss: 0.030360]\n",
      "epoch:24 step:19378 [D loss: 0.001090, acc.: 100.00%] [G loss: 1.293667]\n",
      "epoch:24 step:19379 [D loss: 0.002793, acc.: 100.00%] [G loss: 0.020540]\n",
      "epoch:24 step:19380 [D loss: 0.001984, acc.: 100.00%] [G loss: 0.038701]\n",
      "epoch:24 step:19381 [D loss: 0.011402, acc.: 100.00%] [G loss: 0.026075]\n",
      "epoch:24 step:19382 [D loss: 0.020437, acc.: 99.22%] [G loss: 0.005651]\n",
      "epoch:24 step:19383 [D loss: 0.003744, acc.: 100.00%] [G loss: 0.076524]\n",
      "epoch:24 step:19384 [D loss: 0.003979, acc.: 100.00%] [G loss: 0.026555]\n",
      "epoch:24 step:19385 [D loss: 0.004254, acc.: 100.00%] [G loss: 0.015611]\n",
      "epoch:24 step:19386 [D loss: 0.008338, acc.: 100.00%] [G loss: 0.008523]\n",
      "epoch:24 step:19387 [D loss: 0.004357, acc.: 100.00%] [G loss: 0.132400]\n",
      "epoch:24 step:19388 [D loss: 0.002921, acc.: 100.00%] [G loss: 0.017629]\n",
      "epoch:24 step:19389 [D loss: 0.004705, acc.: 100.00%] [G loss: 0.054346]\n",
      "epoch:24 step:19390 [D loss: 0.064699, acc.: 97.66%] [G loss: 0.203087]\n",
      "epoch:24 step:19391 [D loss: 0.017627, acc.: 100.00%] [G loss: 2.002575]\n",
      "epoch:24 step:19392 [D loss: 0.022177, acc.: 100.00%] [G loss: 3.587640]\n",
      "epoch:24 step:19393 [D loss: 0.009388, acc.: 99.22%] [G loss: 4.620780]\n",
      "epoch:24 step:19394 [D loss: 0.001533, acc.: 100.00%] [G loss: 3.420082]\n",
      "epoch:24 step:19395 [D loss: 0.001659, acc.: 100.00%] [G loss: 2.387411]\n",
      "epoch:24 step:19396 [D loss: 0.023110, acc.: 100.00%] [G loss: 3.643013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19397 [D loss: 0.012514, acc.: 100.00%] [G loss: 5.502542]\n",
      "epoch:24 step:19398 [D loss: 0.018672, acc.: 99.22%] [G loss: 5.758602]\n",
      "epoch:24 step:19399 [D loss: 0.008461, acc.: 100.00%] [G loss: 6.028669]\n",
      "epoch:24 step:19400 [D loss: 0.008332, acc.: 100.00%] [G loss: 6.374635]\n",
      "epoch:24 step:19401 [D loss: 0.005271, acc.: 100.00%] [G loss: 5.089829]\n",
      "epoch:24 step:19402 [D loss: 0.020923, acc.: 100.00%] [G loss: 0.250003]\n",
      "epoch:24 step:19403 [D loss: 0.010625, acc.: 99.22%] [G loss: 0.055855]\n",
      "epoch:24 step:19404 [D loss: 0.004967, acc.: 100.00%] [G loss: 9.936838]\n",
      "epoch:24 step:19405 [D loss: 0.018659, acc.: 99.22%] [G loss: 6.897681]\n",
      "epoch:24 step:19406 [D loss: 0.024663, acc.: 100.00%] [G loss: 0.043821]\n",
      "epoch:24 step:19407 [D loss: 0.002856, acc.: 100.00%] [G loss: 9.094574]\n",
      "epoch:24 step:19408 [D loss: 0.008106, acc.: 100.00%] [G loss: 7.973395]\n",
      "epoch:24 step:19409 [D loss: 0.048018, acc.: 98.44%] [G loss: 6.843494]\n",
      "epoch:24 step:19410 [D loss: 0.002128, acc.: 100.00%] [G loss: 5.128597]\n",
      "epoch:24 step:19411 [D loss: 0.012557, acc.: 100.00%] [G loss: 7.492517]\n",
      "epoch:24 step:19412 [D loss: 0.029036, acc.: 99.22%] [G loss: 3.742018]\n",
      "epoch:24 step:19413 [D loss: 0.003378, acc.: 100.00%] [G loss: 1.407687]\n",
      "epoch:24 step:19414 [D loss: 0.005068, acc.: 100.00%] [G loss: 0.277553]\n",
      "epoch:24 step:19415 [D loss: 0.009064, acc.: 100.00%] [G loss: 0.014305]\n",
      "epoch:24 step:19416 [D loss: 0.297911, acc.: 86.72%] [G loss: 11.024869]\n",
      "epoch:24 step:19417 [D loss: 4.590586, acc.: 50.00%] [G loss: 0.254807]\n",
      "epoch:24 step:19418 [D loss: 0.060093, acc.: 98.44%] [G loss: 2.212283]\n",
      "epoch:24 step:19419 [D loss: 0.291776, acc.: 89.84%] [G loss: 1.755904]\n",
      "epoch:24 step:19420 [D loss: 0.086647, acc.: 96.88%] [G loss: 5.347789]\n",
      "epoch:24 step:19421 [D loss: 0.137632, acc.: 94.53%] [G loss: 3.180385]\n",
      "epoch:24 step:19422 [D loss: 0.080349, acc.: 96.88%] [G loss: 2.096469]\n",
      "epoch:24 step:19423 [D loss: 0.031692, acc.: 100.00%] [G loss: 3.522931]\n",
      "epoch:24 step:19424 [D loss: 0.105220, acc.: 96.88%] [G loss: 3.495870]\n",
      "epoch:24 step:19425 [D loss: 0.262506, acc.: 89.06%] [G loss: 0.487834]\n",
      "epoch:24 step:19426 [D loss: 0.011704, acc.: 100.00%] [G loss: 0.427198]\n",
      "epoch:24 step:19427 [D loss: 0.073015, acc.: 97.66%] [G loss: 0.349637]\n",
      "epoch:24 step:19428 [D loss: 0.011350, acc.: 100.00%] [G loss: 0.264909]\n",
      "epoch:24 step:19429 [D loss: 0.048758, acc.: 97.66%] [G loss: 0.225038]\n",
      "epoch:24 step:19430 [D loss: 0.014840, acc.: 100.00%] [G loss: 0.114306]\n",
      "epoch:24 step:19431 [D loss: 0.001171, acc.: 100.00%] [G loss: 0.061582]\n",
      "epoch:24 step:19432 [D loss: 0.001416, acc.: 100.00%] [G loss: 0.387009]\n",
      "epoch:24 step:19433 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.068213]\n",
      "epoch:24 step:19434 [D loss: 0.000888, acc.: 100.00%] [G loss: 0.007483]\n",
      "epoch:24 step:19435 [D loss: 0.001311, acc.: 100.00%] [G loss: 0.003874]\n",
      "epoch:24 step:19436 [D loss: 0.008644, acc.: 100.00%] [G loss: 0.004120]\n",
      "epoch:24 step:19437 [D loss: 0.136555, acc.: 92.19%] [G loss: 0.533174]\n",
      "epoch:24 step:19438 [D loss: 0.023027, acc.: 100.00%] [G loss: 0.594086]\n",
      "epoch:24 step:19439 [D loss: 0.080463, acc.: 95.31%] [G loss: 0.391899]\n",
      "epoch:24 step:19440 [D loss: 0.027709, acc.: 99.22%] [G loss: 0.009345]\n",
      "epoch:24 step:19441 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.256945]\n",
      "epoch:24 step:19442 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.009787]\n",
      "epoch:24 step:19443 [D loss: 0.001802, acc.: 100.00%] [G loss: 0.025237]\n",
      "epoch:24 step:19444 [D loss: 0.000463, acc.: 100.00%] [G loss: 0.023102]\n",
      "epoch:24 step:19445 [D loss: 0.000791, acc.: 100.00%] [G loss: 0.001880]\n",
      "epoch:24 step:19446 [D loss: 0.004715, acc.: 100.00%] [G loss: 0.002741]\n",
      "epoch:24 step:19447 [D loss: 0.003637, acc.: 100.00%] [G loss: 0.001211]\n",
      "epoch:24 step:19448 [D loss: 0.015069, acc.: 99.22%] [G loss: 0.024977]\n",
      "epoch:24 step:19449 [D loss: 0.025952, acc.: 99.22%] [G loss: 0.002896]\n",
      "epoch:24 step:19450 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.006697]\n",
      "epoch:24 step:19451 [D loss: 0.004017, acc.: 100.00%] [G loss: 0.003379]\n",
      "epoch:24 step:19452 [D loss: 0.001520, acc.: 100.00%] [G loss: 0.001941]\n",
      "epoch:24 step:19453 [D loss: 0.000464, acc.: 100.00%] [G loss: 0.002387]\n",
      "epoch:24 step:19454 [D loss: 0.000958, acc.: 100.00%] [G loss: 0.005183]\n",
      "epoch:24 step:19455 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.002166]\n",
      "epoch:24 step:19456 [D loss: 0.000767, acc.: 100.00%] [G loss: 0.004005]\n",
      "epoch:24 step:19457 [D loss: 0.036649, acc.: 100.00%] [G loss: 0.000473]\n",
      "epoch:24 step:19458 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:24 step:19459 [D loss: 0.017022, acc.: 99.22%] [G loss: 0.023604]\n",
      "epoch:24 step:19460 [D loss: 0.000648, acc.: 100.00%] [G loss: 0.110866]\n",
      "epoch:24 step:19461 [D loss: 0.001398, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:24 step:19462 [D loss: 0.017782, acc.: 100.00%] [G loss: 0.000480]\n",
      "epoch:24 step:19463 [D loss: 0.012972, acc.: 100.00%] [G loss: 0.066950]\n",
      "epoch:24 step:19464 [D loss: 0.017516, acc.: 100.00%] [G loss: 0.000886]\n",
      "epoch:24 step:19465 [D loss: 0.044924, acc.: 100.00%] [G loss: 0.350355]\n",
      "epoch:24 step:19466 [D loss: 0.001920, acc.: 100.00%] [G loss: 0.018429]\n",
      "epoch:24 step:19467 [D loss: 0.019629, acc.: 98.44%] [G loss: 0.004435]\n",
      "epoch:24 step:19468 [D loss: 0.002201, acc.: 100.00%] [G loss: 0.014547]\n",
      "epoch:24 step:19469 [D loss: 0.015276, acc.: 100.00%] [G loss: 0.001810]\n",
      "epoch:24 step:19470 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.077050]\n",
      "epoch:24 step:19471 [D loss: 0.015836, acc.: 99.22%] [G loss: 0.008145]\n",
      "epoch:24 step:19472 [D loss: 0.003204, acc.: 100.00%] [G loss: 0.001570]\n",
      "epoch:24 step:19473 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.091991]\n",
      "epoch:24 step:19474 [D loss: 0.004043, acc.: 100.00%] [G loss: 0.417746]\n",
      "epoch:24 step:19475 [D loss: 0.001163, acc.: 100.00%] [G loss: 0.002612]\n",
      "epoch:24 step:19476 [D loss: 0.001237, acc.: 100.00%] [G loss: 0.027030]\n",
      "epoch:24 step:19477 [D loss: 0.016186, acc.: 100.00%] [G loss: 0.019853]\n",
      "epoch:24 step:19478 [D loss: 0.001971, acc.: 100.00%] [G loss: 0.007747]\n",
      "epoch:24 step:19479 [D loss: 0.002043, acc.: 100.00%] [G loss: 0.016956]\n",
      "epoch:24 step:19480 [D loss: 0.002802, acc.: 100.00%] [G loss: 0.003256]\n",
      "epoch:24 step:19481 [D loss: 0.007094, acc.: 100.00%] [G loss: 0.056602]\n",
      "epoch:24 step:19482 [D loss: 0.003034, acc.: 100.00%] [G loss: 0.014873]\n",
      "epoch:24 step:19483 [D loss: 0.048051, acc.: 98.44%] [G loss: 0.011018]\n",
      "epoch:24 step:19484 [D loss: 0.001353, acc.: 100.00%] [G loss: 0.057557]\n",
      "epoch:24 step:19485 [D loss: 0.008756, acc.: 100.00%] [G loss: 0.242606]\n",
      "epoch:24 step:19486 [D loss: 1.044021, acc.: 57.81%] [G loss: 7.944987]\n",
      "epoch:24 step:19487 [D loss: 0.603229, acc.: 78.91%] [G loss: 6.636310]\n",
      "epoch:24 step:19488 [D loss: 0.138336, acc.: 92.19%] [G loss: 3.960964]\n",
      "epoch:24 step:19489 [D loss: 0.008951, acc.: 99.22%] [G loss: 0.442538]\n",
      "epoch:24 step:19490 [D loss: 0.028048, acc.: 99.22%] [G loss: 1.779901]\n",
      "epoch:24 step:19491 [D loss: 0.035631, acc.: 99.22%] [G loss: 0.349780]\n",
      "epoch:24 step:19492 [D loss: 0.012266, acc.: 100.00%] [G loss: 0.981161]\n",
      "epoch:24 step:19493 [D loss: 0.013845, acc.: 99.22%] [G loss: 0.868549]\n",
      "epoch:24 step:19494 [D loss: 0.030808, acc.: 98.44%] [G loss: 0.003148]\n",
      "epoch:24 step:19495 [D loss: 0.009150, acc.: 99.22%] [G loss: 0.168436]\n",
      "epoch:24 step:19496 [D loss: 0.038674, acc.: 100.00%] [G loss: 0.182369]\n",
      "epoch:24 step:19497 [D loss: 0.010958, acc.: 100.00%] [G loss: 0.104802]\n",
      "epoch:24 step:19498 [D loss: 0.003660, acc.: 100.00%] [G loss: 0.244462]\n",
      "epoch:24 step:19499 [D loss: 0.002315, acc.: 100.00%] [G loss: 0.013721]\n",
      "epoch:24 step:19500 [D loss: 0.001564, acc.: 100.00%] [G loss: 0.036733]\n",
      "epoch:24 step:19501 [D loss: 0.008179, acc.: 100.00%] [G loss: 0.115921]\n",
      "epoch:24 step:19502 [D loss: 0.012659, acc.: 100.00%] [G loss: 0.011067]\n",
      "epoch:24 step:19503 [D loss: 0.004958, acc.: 100.00%] [G loss: 0.109916]\n",
      "epoch:24 step:19504 [D loss: 0.002666, acc.: 100.00%] [G loss: 0.025601]\n",
      "epoch:24 step:19505 [D loss: 0.002519, acc.: 100.00%] [G loss: 0.115332]\n",
      "epoch:24 step:19506 [D loss: 0.003914, acc.: 100.00%] [G loss: 0.171872]\n",
      "epoch:24 step:19507 [D loss: 0.020559, acc.: 99.22%] [G loss: 0.002078]\n",
      "epoch:24 step:19508 [D loss: 0.302304, acc.: 85.94%] [G loss: 1.309793]\n",
      "epoch:24 step:19509 [D loss: 0.290968, acc.: 86.72%] [G loss: 2.258004]\n",
      "epoch:24 step:19510 [D loss: 0.022193, acc.: 99.22%] [G loss: 0.471040]\n",
      "epoch:24 step:19511 [D loss: 0.004252, acc.: 100.00%] [G loss: 0.177118]\n",
      "epoch:24 step:19512 [D loss: 0.008153, acc.: 100.00%] [G loss: 0.150435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:24 step:19513 [D loss: 0.003118, acc.: 100.00%] [G loss: 0.072119]\n",
      "epoch:24 step:19514 [D loss: 0.003994, acc.: 100.00%] [G loss: 4.046911]\n",
      "epoch:24 step:19515 [D loss: 0.031167, acc.: 99.22%] [G loss: 0.014031]\n",
      "epoch:24 step:19516 [D loss: 0.299964, acc.: 84.38%] [G loss: 1.150346]\n",
      "epoch:24 step:19517 [D loss: 0.045046, acc.: 98.44%] [G loss: 2.131034]\n",
      "epoch:24 step:19518 [D loss: 0.273225, acc.: 89.06%] [G loss: 0.490617]\n",
      "epoch:24 step:19519 [D loss: 0.021164, acc.: 100.00%] [G loss: 0.378470]\n",
      "epoch:24 step:19520 [D loss: 0.004584, acc.: 100.00%] [G loss: 0.062426]\n",
      "epoch:24 step:19521 [D loss: 0.018028, acc.: 100.00%] [G loss: 0.080381]\n",
      "epoch:24 step:19522 [D loss: 0.076200, acc.: 99.22%] [G loss: 0.044482]\n",
      "epoch:24 step:19523 [D loss: 0.006327, acc.: 100.00%] [G loss: 0.340802]\n",
      "epoch:24 step:19524 [D loss: 0.000815, acc.: 100.00%] [G loss: 0.429166]\n",
      "epoch:24 step:19525 [D loss: 0.030461, acc.: 98.44%] [G loss: 0.121951]\n",
      "epoch:25 step:19526 [D loss: 0.008946, acc.: 100.00%] [G loss: 0.248384]\n",
      "epoch:25 step:19527 [D loss: 0.003084, acc.: 100.00%] [G loss: 0.008538]\n",
      "epoch:25 step:19528 [D loss: 0.011344, acc.: 100.00%] [G loss: 0.015676]\n",
      "epoch:25 step:19529 [D loss: 0.006640, acc.: 100.00%] [G loss: 0.066100]\n",
      "epoch:25 step:19530 [D loss: 0.001431, acc.: 100.00%] [G loss: 0.189441]\n",
      "epoch:25 step:19531 [D loss: 0.004062, acc.: 100.00%] [G loss: 0.057875]\n",
      "epoch:25 step:19532 [D loss: 0.014716, acc.: 100.00%] [G loss: 0.048221]\n",
      "epoch:25 step:19533 [D loss: 0.003385, acc.: 100.00%] [G loss: 0.049356]\n",
      "epoch:25 step:19534 [D loss: 0.000811, acc.: 100.00%] [G loss: 0.052485]\n",
      "epoch:25 step:19535 [D loss: 0.019403, acc.: 100.00%] [G loss: 0.028848]\n",
      "epoch:25 step:19536 [D loss: 0.001089, acc.: 100.00%] [G loss: 0.022502]\n",
      "epoch:25 step:19537 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.028285]\n",
      "epoch:25 step:19538 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.005641]\n",
      "epoch:25 step:19539 [D loss: 0.001866, acc.: 100.00%] [G loss: 0.011651]\n",
      "epoch:25 step:19540 [D loss: 0.000867, acc.: 100.00%] [G loss: 0.761986]\n",
      "epoch:25 step:19541 [D loss: 0.000709, acc.: 100.00%] [G loss: 0.058655]\n",
      "epoch:25 step:19542 [D loss: 0.000841, acc.: 100.00%] [G loss: 0.003831]\n",
      "epoch:25 step:19543 [D loss: 0.010290, acc.: 100.00%] [G loss: 2.406135]\n",
      "epoch:25 step:19544 [D loss: 0.081605, acc.: 98.44%] [G loss: 1.060896]\n",
      "epoch:25 step:19545 [D loss: 0.006526, acc.: 100.00%] [G loss: 0.216348]\n",
      "epoch:25 step:19546 [D loss: 0.028823, acc.: 99.22%] [G loss: 0.445879]\n",
      "epoch:25 step:19547 [D loss: 0.009887, acc.: 100.00%] [G loss: 0.012420]\n",
      "epoch:25 step:19548 [D loss: 0.127931, acc.: 94.53%] [G loss: 0.112674]\n",
      "epoch:25 step:19549 [D loss: 0.035979, acc.: 98.44%] [G loss: 3.137329]\n",
      "epoch:25 step:19550 [D loss: 0.465831, acc.: 75.78%] [G loss: 0.005890]\n",
      "epoch:25 step:19551 [D loss: 0.019270, acc.: 99.22%] [G loss: 0.003398]\n",
      "epoch:25 step:19552 [D loss: 0.912433, acc.: 62.50%] [G loss: 1.126451]\n",
      "epoch:25 step:19553 [D loss: 0.549535, acc.: 79.69%] [G loss: 1.157619]\n",
      "epoch:25 step:19554 [D loss: 0.214061, acc.: 89.84%] [G loss: 0.289967]\n",
      "epoch:25 step:19555 [D loss: 0.004262, acc.: 100.00%] [G loss: 4.787242]\n",
      "epoch:25 step:19556 [D loss: 0.019399, acc.: 100.00%] [G loss: 0.017732]\n",
      "epoch:25 step:19557 [D loss: 0.001992, acc.: 100.00%] [G loss: 0.020596]\n",
      "epoch:25 step:19558 [D loss: 0.170992, acc.: 92.19%] [G loss: 0.725527]\n",
      "epoch:25 step:19559 [D loss: 0.007314, acc.: 100.00%] [G loss: 1.867945]\n",
      "epoch:25 step:19560 [D loss: 0.403380, acc.: 81.25%] [G loss: 0.420967]\n",
      "epoch:25 step:19561 [D loss: 0.246673, acc.: 89.84%] [G loss: 3.911672]\n",
      "epoch:25 step:19562 [D loss: 0.104524, acc.: 96.09%] [G loss: 1.982339]\n",
      "epoch:25 step:19563 [D loss: 0.008493, acc.: 100.00%] [G loss: 5.647095]\n",
      "epoch:25 step:19564 [D loss: 0.031016, acc.: 99.22%] [G loss: 4.952885]\n",
      "epoch:25 step:19565 [D loss: 0.022978, acc.: 99.22%] [G loss: 3.867297]\n",
      "epoch:25 step:19566 [D loss: 0.029676, acc.: 99.22%] [G loss: 2.959244]\n",
      "epoch:25 step:19567 [D loss: 0.007833, acc.: 100.00%] [G loss: 2.329391]\n",
      "epoch:25 step:19568 [D loss: 0.009548, acc.: 100.00%] [G loss: 0.311853]\n",
      "epoch:25 step:19569 [D loss: 0.025640, acc.: 99.22%] [G loss: 1.565573]\n",
      "epoch:25 step:19570 [D loss: 0.009261, acc.: 100.00%] [G loss: 1.566246]\n",
      "epoch:25 step:19571 [D loss: 0.021651, acc.: 100.00%] [G loss: 1.112139]\n",
      "epoch:25 step:19572 [D loss: 0.013484, acc.: 100.00%] [G loss: 1.269361]\n",
      "epoch:25 step:19573 [D loss: 0.018954, acc.: 100.00%] [G loss: 1.560744]\n",
      "epoch:25 step:19574 [D loss: 0.009719, acc.: 100.00%] [G loss: 2.149343]\n",
      "epoch:25 step:19575 [D loss: 0.023825, acc.: 99.22%] [G loss: 0.699918]\n",
      "epoch:25 step:19576 [D loss: 0.009108, acc.: 100.00%] [G loss: 1.040838]\n",
      "epoch:25 step:19577 [D loss: 0.012843, acc.: 100.00%] [G loss: 0.756651]\n",
      "epoch:25 step:19578 [D loss: 0.007221, acc.: 100.00%] [G loss: 0.714985]\n",
      "epoch:25 step:19579 [D loss: 0.062428, acc.: 99.22%] [G loss: 0.018524]\n",
      "epoch:25 step:19580 [D loss: 0.003805, acc.: 100.00%] [G loss: 3.296721]\n",
      "epoch:25 step:19581 [D loss: 0.009783, acc.: 100.00%] [G loss: 3.575490]\n",
      "epoch:25 step:19582 [D loss: 0.031055, acc.: 99.22%] [G loss: 0.061592]\n",
      "epoch:25 step:19583 [D loss: 0.024246, acc.: 99.22%] [G loss: 1.729828]\n",
      "epoch:25 step:19584 [D loss: 0.074786, acc.: 97.66%] [G loss: 6.285563]\n",
      "epoch:25 step:19585 [D loss: 0.013928, acc.: 100.00%] [G loss: 0.100741]\n",
      "epoch:25 step:19586 [D loss: 0.719225, acc.: 61.72%] [G loss: 7.674932]\n",
      "epoch:25 step:19587 [D loss: 0.945410, acc.: 61.72%] [G loss: 4.460771]\n",
      "epoch:25 step:19588 [D loss: 0.100370, acc.: 97.66%] [G loss: 0.631823]\n",
      "epoch:25 step:19589 [D loss: 0.009446, acc.: 100.00%] [G loss: 1.662124]\n",
      "epoch:25 step:19590 [D loss: 0.045216, acc.: 98.44%] [G loss: 1.412136]\n",
      "epoch:25 step:19591 [D loss: 0.002735, acc.: 100.00%] [G loss: 0.056506]\n",
      "epoch:25 step:19592 [D loss: 0.005307, acc.: 100.00%] [G loss: 2.863919]\n",
      "epoch:25 step:19593 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.337020]\n",
      "epoch:25 step:19594 [D loss: 0.002793, acc.: 100.00%] [G loss: 0.291999]\n",
      "epoch:25 step:19595 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.508047]\n",
      "epoch:25 step:19596 [D loss: 0.000590, acc.: 100.00%] [G loss: 0.242078]\n",
      "epoch:25 step:19597 [D loss: 0.003602, acc.: 100.00%] [G loss: 0.002216]\n",
      "epoch:25 step:19598 [D loss: 0.000703, acc.: 100.00%] [G loss: 0.162169]\n",
      "epoch:25 step:19599 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.144796]\n",
      "epoch:25 step:19600 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.153384]\n",
      "epoch:25 step:19601 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.132334]\n",
      "epoch:25 step:19602 [D loss: 0.001829, acc.: 100.00%] [G loss: 1.688364]\n",
      "epoch:25 step:19603 [D loss: 0.062716, acc.: 98.44%] [G loss: 0.204726]\n",
      "epoch:25 step:19604 [D loss: 0.049761, acc.: 99.22%] [G loss: 0.326388]\n",
      "epoch:25 step:19605 [D loss: 0.020651, acc.: 100.00%] [G loss: 0.323318]\n",
      "epoch:25 step:19606 [D loss: 0.002269, acc.: 100.00%] [G loss: 0.452388]\n",
      "epoch:25 step:19607 [D loss: 0.005432, acc.: 100.00%] [G loss: 0.197564]\n",
      "epoch:25 step:19608 [D loss: 0.003176, acc.: 100.00%] [G loss: 0.117677]\n",
      "epoch:25 step:19609 [D loss: 0.000659, acc.: 100.00%] [G loss: 0.060315]\n",
      "epoch:25 step:19610 [D loss: 0.008096, acc.: 100.00%] [G loss: 0.235753]\n",
      "epoch:25 step:19611 [D loss: 0.010352, acc.: 100.00%] [G loss: 0.172722]\n",
      "epoch:25 step:19612 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.072274]\n",
      "epoch:25 step:19613 [D loss: 0.003770, acc.: 100.00%] [G loss: 2.284658]\n",
      "epoch:25 step:19614 [D loss: 0.000567, acc.: 100.00%] [G loss: 0.063426]\n",
      "epoch:25 step:19615 [D loss: 0.022873, acc.: 100.00%] [G loss: 0.055839]\n",
      "epoch:25 step:19616 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.037971]\n",
      "epoch:25 step:19617 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.053302]\n",
      "epoch:25 step:19618 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.001287]\n",
      "epoch:25 step:19619 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.023271]\n",
      "epoch:25 step:19620 [D loss: 0.000737, acc.: 100.00%] [G loss: 0.067994]\n",
      "epoch:25 step:19621 [D loss: 0.000652, acc.: 100.00%] [G loss: 0.006057]\n",
      "epoch:25 step:19622 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.037711]\n",
      "epoch:25 step:19623 [D loss: 0.002947, acc.: 100.00%] [G loss: 0.008361]\n",
      "epoch:25 step:19624 [D loss: 0.001141, acc.: 100.00%] [G loss: 0.015602]\n",
      "epoch:25 step:19625 [D loss: 0.007469, acc.: 100.00%] [G loss: 0.043649]\n",
      "epoch:25 step:19626 [D loss: 0.001763, acc.: 100.00%] [G loss: 0.020255]\n",
      "epoch:25 step:19627 [D loss: 0.002073, acc.: 100.00%] [G loss: 0.122184]\n",
      "epoch:25 step:19628 [D loss: 0.012523, acc.: 100.00%] [G loss: 0.054405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19629 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.010771]\n",
      "epoch:25 step:19630 [D loss: 0.003666, acc.: 100.00%] [G loss: 0.001117]\n",
      "epoch:25 step:19631 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.013850]\n",
      "epoch:25 step:19632 [D loss: 0.000525, acc.: 100.00%] [G loss: 0.011517]\n",
      "epoch:25 step:19633 [D loss: 0.002507, acc.: 100.00%] [G loss: 0.022857]\n",
      "epoch:25 step:19634 [D loss: 0.000915, acc.: 100.00%] [G loss: 0.013898]\n",
      "epoch:25 step:19635 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.022842]\n",
      "epoch:25 step:19636 [D loss: 0.013314, acc.: 99.22%] [G loss: 0.003436]\n",
      "epoch:25 step:19637 [D loss: 0.008116, acc.: 100.00%] [G loss: 0.011250]\n",
      "epoch:25 step:19638 [D loss: 0.012340, acc.: 100.00%] [G loss: 0.000626]\n",
      "epoch:25 step:19639 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.023814]\n",
      "epoch:25 step:19640 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.001067]\n",
      "epoch:25 step:19641 [D loss: 0.002952, acc.: 100.00%] [G loss: 0.007861]\n",
      "epoch:25 step:19642 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.001703]\n",
      "epoch:25 step:19643 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.007617]\n",
      "epoch:25 step:19644 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.005349]\n",
      "epoch:25 step:19645 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.005213]\n",
      "epoch:25 step:19646 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.011709]\n",
      "epoch:25 step:19647 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.008506]\n",
      "epoch:25 step:19648 [D loss: 0.002243, acc.: 100.00%] [G loss: 0.024370]\n",
      "epoch:25 step:19649 [D loss: 0.001430, acc.: 100.00%] [G loss: 0.019442]\n",
      "epoch:25 step:19650 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.015722]\n",
      "epoch:25 step:19651 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.005912]\n",
      "epoch:25 step:19652 [D loss: 0.006507, acc.: 100.00%] [G loss: 0.007404]\n",
      "epoch:25 step:19653 [D loss: 0.002998, acc.: 100.00%] [G loss: 0.004022]\n",
      "epoch:25 step:19654 [D loss: 0.068671, acc.: 99.22%] [G loss: 0.024589]\n",
      "epoch:25 step:19655 [D loss: 0.000958, acc.: 100.00%] [G loss: 0.237340]\n",
      "epoch:25 step:19656 [D loss: 0.008205, acc.: 100.00%] [G loss: 0.242101]\n",
      "epoch:25 step:19657 [D loss: 0.400481, acc.: 85.94%] [G loss: 2.214038]\n",
      "epoch:25 step:19658 [D loss: 0.034119, acc.: 97.66%] [G loss: 2.720259]\n",
      "epoch:25 step:19659 [D loss: 0.019668, acc.: 99.22%] [G loss: 3.962991]\n",
      "epoch:25 step:19660 [D loss: 0.053133, acc.: 97.66%] [G loss: 3.069645]\n",
      "epoch:25 step:19661 [D loss: 0.004745, acc.: 100.00%] [G loss: 0.025023]\n",
      "epoch:25 step:19662 [D loss: 0.007356, acc.: 100.00%] [G loss: 3.877877]\n",
      "epoch:25 step:19663 [D loss: 0.002498, acc.: 100.00%] [G loss: 1.386707]\n",
      "epoch:25 step:19664 [D loss: 0.010175, acc.: 100.00%] [G loss: 1.321119]\n",
      "epoch:25 step:19665 [D loss: 0.025414, acc.: 100.00%] [G loss: 0.949681]\n",
      "epoch:25 step:19666 [D loss: 0.003071, acc.: 100.00%] [G loss: 1.644425]\n",
      "epoch:25 step:19667 [D loss: 0.006982, acc.: 100.00%] [G loss: 0.896003]\n",
      "epoch:25 step:19668 [D loss: 0.023958, acc.: 99.22%] [G loss: 1.172770]\n",
      "epoch:25 step:19669 [D loss: 0.007328, acc.: 100.00%] [G loss: 1.603799]\n",
      "epoch:25 step:19670 [D loss: 0.009509, acc.: 100.00%] [G loss: 2.135596]\n",
      "epoch:25 step:19671 [D loss: 0.029653, acc.: 100.00%] [G loss: 3.584205]\n",
      "epoch:25 step:19672 [D loss: 0.010637, acc.: 100.00%] [G loss: 0.405495]\n",
      "epoch:25 step:19673 [D loss: 0.014744, acc.: 100.00%] [G loss: 6.109840]\n",
      "epoch:25 step:19674 [D loss: 0.002213, acc.: 100.00%] [G loss: 4.964164]\n",
      "epoch:25 step:19675 [D loss: 0.002182, acc.: 100.00%] [G loss: 4.915565]\n",
      "epoch:25 step:19676 [D loss: 0.006260, acc.: 100.00%] [G loss: 3.483851]\n",
      "epoch:25 step:19677 [D loss: 0.005582, acc.: 100.00%] [G loss: 0.022201]\n",
      "epoch:25 step:19678 [D loss: 0.007457, acc.: 100.00%] [G loss: 3.607828]\n",
      "epoch:25 step:19679 [D loss: 0.008569, acc.: 100.00%] [G loss: 2.910523]\n",
      "epoch:25 step:19680 [D loss: 0.032164, acc.: 99.22%] [G loss: 3.651287]\n",
      "epoch:25 step:19681 [D loss: 0.005696, acc.: 100.00%] [G loss: 3.703755]\n",
      "epoch:25 step:19682 [D loss: 0.005538, acc.: 100.00%] [G loss: 2.810709]\n",
      "epoch:25 step:19683 [D loss: 0.038669, acc.: 98.44%] [G loss: 1.866046]\n",
      "epoch:25 step:19684 [D loss: 0.003247, acc.: 100.00%] [G loss: 1.752453]\n",
      "epoch:25 step:19685 [D loss: 0.002882, acc.: 100.00%] [G loss: 1.418012]\n",
      "epoch:25 step:19686 [D loss: 0.004698, acc.: 100.00%] [G loss: 1.177920]\n",
      "epoch:25 step:19687 [D loss: 0.002045, acc.: 100.00%] [G loss: 0.016859]\n",
      "epoch:25 step:19688 [D loss: 0.008581, acc.: 100.00%] [G loss: 0.023981]\n",
      "epoch:25 step:19689 [D loss: 0.002304, acc.: 100.00%] [G loss: 1.550605]\n",
      "epoch:25 step:19690 [D loss: 0.012136, acc.: 100.00%] [G loss: 2.257460]\n",
      "epoch:25 step:19691 [D loss: 0.016257, acc.: 100.00%] [G loss: 1.368853]\n",
      "epoch:25 step:19692 [D loss: 0.005861, acc.: 100.00%] [G loss: 1.372659]\n",
      "epoch:25 step:19693 [D loss: 0.003584, acc.: 100.00%] [G loss: 0.041489]\n",
      "epoch:25 step:19694 [D loss: 0.000737, acc.: 100.00%] [G loss: 1.277312]\n",
      "epoch:25 step:19695 [D loss: 0.028636, acc.: 99.22%] [G loss: 1.170366]\n",
      "epoch:25 step:19696 [D loss: 0.016621, acc.: 100.00%] [G loss: 0.851088]\n",
      "epoch:25 step:19697 [D loss: 0.001583, acc.: 100.00%] [G loss: 0.557207]\n",
      "epoch:25 step:19698 [D loss: 0.001152, acc.: 100.00%] [G loss: 0.524619]\n",
      "epoch:25 step:19699 [D loss: 0.007751, acc.: 100.00%] [G loss: 0.739237]\n",
      "epoch:25 step:19700 [D loss: 0.001031, acc.: 100.00%] [G loss: 2.267329]\n",
      "epoch:25 step:19701 [D loss: 0.047055, acc.: 99.22%] [G loss: 0.660531]\n",
      "epoch:25 step:19702 [D loss: 0.004192, acc.: 100.00%] [G loss: 0.858220]\n",
      "epoch:25 step:19703 [D loss: 0.003907, acc.: 100.00%] [G loss: 1.358736]\n",
      "epoch:25 step:19704 [D loss: 0.015838, acc.: 100.00%] [G loss: 1.382673]\n",
      "epoch:25 step:19705 [D loss: 0.001328, acc.: 100.00%] [G loss: 2.012482]\n",
      "epoch:25 step:19706 [D loss: 0.001921, acc.: 100.00%] [G loss: 2.499296]\n",
      "epoch:25 step:19707 [D loss: 0.005715, acc.: 100.00%] [G loss: 2.312408]\n",
      "epoch:25 step:19708 [D loss: 0.001742, acc.: 100.00%] [G loss: 2.076869]\n",
      "epoch:25 step:19709 [D loss: 0.005097, acc.: 100.00%] [G loss: 3.148836]\n",
      "epoch:25 step:19710 [D loss: 0.001342, acc.: 100.00%] [G loss: 0.009511]\n",
      "epoch:25 step:19711 [D loss: 0.000337, acc.: 100.00%] [G loss: 4.119899]\n",
      "epoch:25 step:19712 [D loss: 0.015572, acc.: 99.22%] [G loss: 4.303600]\n",
      "epoch:25 step:19713 [D loss: 0.000512, acc.: 100.00%] [G loss: 2.673503]\n",
      "epoch:25 step:19714 [D loss: 0.001731, acc.: 100.00%] [G loss: 2.458686]\n",
      "epoch:25 step:19715 [D loss: 0.004234, acc.: 100.00%] [G loss: 8.321585]\n",
      "epoch:25 step:19716 [D loss: 0.012790, acc.: 100.00%] [G loss: 4.571083]\n",
      "epoch:25 step:19717 [D loss: 0.000445, acc.: 100.00%] [G loss: 4.279655]\n",
      "epoch:25 step:19718 [D loss: 0.001316, acc.: 100.00%] [G loss: 3.945873]\n",
      "epoch:25 step:19719 [D loss: 0.000946, acc.: 100.00%] [G loss: 0.028785]\n",
      "epoch:25 step:19720 [D loss: 0.000111, acc.: 100.00%] [G loss: 3.974232]\n",
      "epoch:25 step:19721 [D loss: 0.000352, acc.: 100.00%] [G loss: 2.621397]\n",
      "epoch:25 step:19722 [D loss: 0.000410, acc.: 100.00%] [G loss: 2.723552]\n",
      "epoch:25 step:19723 [D loss: 0.001594, acc.: 100.00%] [G loss: 2.511380]\n",
      "epoch:25 step:19724 [D loss: 0.000779, acc.: 100.00%] [G loss: 2.160496]\n",
      "epoch:25 step:19725 [D loss: 0.001467, acc.: 100.00%] [G loss: 3.403468]\n",
      "epoch:25 step:19726 [D loss: 0.001575, acc.: 100.00%] [G loss: 2.205790]\n",
      "epoch:25 step:19727 [D loss: 0.003308, acc.: 100.00%] [G loss: 2.082206]\n",
      "epoch:25 step:19728 [D loss: 0.004517, acc.: 100.00%] [G loss: 0.542321]\n",
      "epoch:25 step:19729 [D loss: 0.001165, acc.: 100.00%] [G loss: 4.569173]\n",
      "epoch:25 step:19730 [D loss: 0.007393, acc.: 100.00%] [G loss: 2.781992]\n",
      "epoch:25 step:19731 [D loss: 0.000545, acc.: 100.00%] [G loss: 3.632045]\n",
      "epoch:25 step:19732 [D loss: 0.002056, acc.: 100.00%] [G loss: 2.621227]\n",
      "epoch:25 step:19733 [D loss: 0.001206, acc.: 100.00%] [G loss: 2.428025]\n",
      "epoch:25 step:19734 [D loss: 0.009320, acc.: 100.00%] [G loss: 9.689381]\n",
      "epoch:25 step:19735 [D loss: 0.000983, acc.: 100.00%] [G loss: 0.000777]\n",
      "epoch:25 step:19736 [D loss: 0.003383, acc.: 100.00%] [G loss: 6.632389]\n",
      "epoch:25 step:19737 [D loss: 0.001510, acc.: 100.00%] [G loss: 0.036194]\n",
      "epoch:25 step:19738 [D loss: 0.010978, acc.: 100.00%] [G loss: 7.128623]\n",
      "epoch:25 step:19739 [D loss: 0.051872, acc.: 96.88%] [G loss: 8.168566]\n",
      "epoch:25 step:19740 [D loss: 0.001368, acc.: 100.00%] [G loss: 6.938747]\n",
      "epoch:25 step:19741 [D loss: 0.002470, acc.: 100.00%] [G loss: 6.040357]\n",
      "epoch:25 step:19742 [D loss: 0.005486, acc.: 100.00%] [G loss: 5.055185]\n",
      "epoch:25 step:19743 [D loss: 0.004902, acc.: 100.00%] [G loss: 8.262403]\n",
      "epoch:25 step:19744 [D loss: 0.002227, acc.: 100.00%] [G loss: 6.152857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19745 [D loss: 0.003309, acc.: 100.00%] [G loss: 7.097414]\n",
      "epoch:25 step:19746 [D loss: 0.027582, acc.: 99.22%] [G loss: 9.328301]\n",
      "epoch:25 step:19747 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.097023]\n",
      "epoch:25 step:19748 [D loss: 0.000610, acc.: 100.00%] [G loss: 11.520104]\n",
      "epoch:25 step:19749 [D loss: 0.000969, acc.: 100.00%] [G loss: 11.142777]\n",
      "epoch:25 step:19750 [D loss: 0.000126, acc.: 100.00%] [G loss: 10.956448]\n",
      "epoch:25 step:19751 [D loss: 0.000256, acc.: 100.00%] [G loss: 10.543076]\n",
      "epoch:25 step:19752 [D loss: 0.000297, acc.: 100.00%] [G loss: 9.567032]\n",
      "epoch:25 step:19753 [D loss: 0.000220, acc.: 100.00%] [G loss: 10.030637]\n",
      "epoch:25 step:19754 [D loss: 0.001839, acc.: 100.00%] [G loss: 9.713680]\n",
      "epoch:25 step:19755 [D loss: 0.000106, acc.: 100.00%] [G loss: 9.328794]\n",
      "epoch:25 step:19756 [D loss: 0.000179, acc.: 100.00%] [G loss: 9.388608]\n",
      "epoch:25 step:19757 [D loss: 0.000961, acc.: 100.00%] [G loss: 8.863366]\n",
      "epoch:25 step:19758 [D loss: 0.000244, acc.: 100.00%] [G loss: 8.726954]\n",
      "epoch:25 step:19759 [D loss: 0.002774, acc.: 100.00%] [G loss: 0.061031]\n",
      "epoch:25 step:19760 [D loss: 0.000587, acc.: 100.00%] [G loss: 8.064101]\n",
      "epoch:25 step:19761 [D loss: 0.000632, acc.: 100.00%] [G loss: 1.907165]\n",
      "epoch:25 step:19762 [D loss: 0.001797, acc.: 100.00%] [G loss: 8.029903]\n",
      "epoch:25 step:19763 [D loss: 0.002434, acc.: 100.00%] [G loss: 8.265549]\n",
      "epoch:25 step:19764 [D loss: 0.000692, acc.: 100.00%] [G loss: 7.697747]\n",
      "epoch:25 step:19765 [D loss: 0.001733, acc.: 100.00%] [G loss: 0.194732]\n",
      "epoch:25 step:19766 [D loss: 0.258359, acc.: 88.28%] [G loss: 15.601994]\n",
      "epoch:25 step:19767 [D loss: 0.601967, acc.: 77.34%] [G loss: 10.724631]\n",
      "epoch:25 step:19768 [D loss: 0.014977, acc.: 99.22%] [G loss: 8.609816]\n",
      "epoch:25 step:19769 [D loss: 0.016104, acc.: 99.22%] [G loss: 4.658775]\n",
      "epoch:25 step:19770 [D loss: 0.011180, acc.: 100.00%] [G loss: 8.918012]\n",
      "epoch:25 step:19771 [D loss: 0.000210, acc.: 100.00%] [G loss: 10.323025]\n",
      "epoch:25 step:19772 [D loss: 0.000493, acc.: 100.00%] [G loss: 10.284842]\n",
      "epoch:25 step:19773 [D loss: 0.000305, acc.: 100.00%] [G loss: 10.846505]\n",
      "epoch:25 step:19774 [D loss: 0.000130, acc.: 100.00%] [G loss: 9.359672]\n",
      "epoch:25 step:19775 [D loss: 0.001020, acc.: 100.00%] [G loss: 7.795911]\n",
      "epoch:25 step:19776 [D loss: 0.000657, acc.: 100.00%] [G loss: 7.047327]\n",
      "epoch:25 step:19777 [D loss: 0.040543, acc.: 98.44%] [G loss: 9.270180]\n",
      "epoch:25 step:19778 [D loss: 0.000048, acc.: 100.00%] [G loss: 11.924515]\n",
      "epoch:25 step:19779 [D loss: 0.000036, acc.: 100.00%] [G loss: 11.112703]\n",
      "epoch:25 step:19780 [D loss: 0.000177, acc.: 100.00%] [G loss: 11.088274]\n",
      "epoch:25 step:19781 [D loss: 0.000052, acc.: 100.00%] [G loss: 10.721945]\n",
      "epoch:25 step:19782 [D loss: 0.000134, acc.: 100.00%] [G loss: 10.194359]\n",
      "epoch:25 step:19783 [D loss: 0.000295, acc.: 100.00%] [G loss: 9.620930]\n",
      "epoch:25 step:19784 [D loss: 0.000491, acc.: 100.00%] [G loss: 9.359802]\n",
      "epoch:25 step:19785 [D loss: 0.002150, acc.: 100.00%] [G loss: 8.173021]\n",
      "epoch:25 step:19786 [D loss: 0.002454, acc.: 100.00%] [G loss: 7.951415]\n",
      "epoch:25 step:19787 [D loss: 0.005889, acc.: 100.00%] [G loss: 7.975953]\n",
      "epoch:25 step:19788 [D loss: 0.001458, acc.: 100.00%] [G loss: 0.003931]\n",
      "epoch:25 step:19789 [D loss: 0.046281, acc.: 99.22%] [G loss: 11.695787]\n",
      "epoch:25 step:19790 [D loss: 0.001240, acc.: 100.00%] [G loss: 12.645892]\n",
      "epoch:25 step:19791 [D loss: 0.011403, acc.: 100.00%] [G loss: 0.183495]\n",
      "epoch:25 step:19792 [D loss: 0.052971, acc.: 99.22%] [G loss: 12.135963]\n",
      "epoch:25 step:19793 [D loss: 0.006499, acc.: 100.00%] [G loss: 12.655046]\n",
      "epoch:25 step:19794 [D loss: 0.000705, acc.: 100.00%] [G loss: 11.217096]\n",
      "epoch:25 step:19795 [D loss: 0.004661, acc.: 100.00%] [G loss: 12.962156]\n",
      "epoch:25 step:19796 [D loss: 0.019304, acc.: 99.22%] [G loss: 2.830785]\n",
      "epoch:25 step:19797 [D loss: 0.001916, acc.: 100.00%] [G loss: 10.358908]\n",
      "epoch:25 step:19798 [D loss: 0.001936, acc.: 100.00%] [G loss: 0.001273]\n",
      "epoch:25 step:19799 [D loss: 0.083550, acc.: 97.66%] [G loss: 8.069082]\n",
      "epoch:25 step:19800 [D loss: 0.031336, acc.: 98.44%] [G loss: 0.704008]\n",
      "epoch:25 step:19801 [D loss: 1.323817, acc.: 67.19%] [G loss: 15.329099]\n",
      "epoch:25 step:19802 [D loss: 5.613811, acc.: 50.00%] [G loss: 5.521133]\n",
      "epoch:25 step:19803 [D loss: 1.570458, acc.: 49.22%] [G loss: 4.102530]\n",
      "epoch:25 step:19804 [D loss: 0.161297, acc.: 94.53%] [G loss: 3.436377]\n",
      "epoch:25 step:19805 [D loss: 0.038242, acc.: 99.22%] [G loss: 2.625636]\n",
      "epoch:25 step:19806 [D loss: 0.038607, acc.: 99.22%] [G loss: 1.576712]\n",
      "epoch:25 step:19807 [D loss: 0.080656, acc.: 95.31%] [G loss: 1.335947]\n",
      "epoch:25 step:19808 [D loss: 0.002774, acc.: 100.00%] [G loss: 0.307141]\n",
      "epoch:25 step:19809 [D loss: 0.024164, acc.: 100.00%] [G loss: 1.166074]\n",
      "epoch:25 step:19810 [D loss: 0.558646, acc.: 72.66%] [G loss: 1.961044]\n",
      "epoch:25 step:19811 [D loss: 0.630551, acc.: 67.97%] [G loss: 1.890328]\n",
      "epoch:25 step:19812 [D loss: 0.089496, acc.: 96.88%] [G loss: 0.201569]\n",
      "epoch:25 step:19813 [D loss: 0.027098, acc.: 98.44%] [G loss: 0.108079]\n",
      "epoch:25 step:19814 [D loss: 0.013079, acc.: 100.00%] [G loss: 0.123183]\n",
      "epoch:25 step:19815 [D loss: 0.025507, acc.: 100.00%] [G loss: 0.124626]\n",
      "epoch:25 step:19816 [D loss: 0.009284, acc.: 100.00%] [G loss: 0.042231]\n",
      "epoch:25 step:19817 [D loss: 0.029893, acc.: 99.22%] [G loss: 0.042598]\n",
      "epoch:25 step:19818 [D loss: 0.019865, acc.: 100.00%] [G loss: 0.243011]\n",
      "epoch:25 step:19819 [D loss: 0.008489, acc.: 100.00%] [G loss: 0.007530]\n",
      "epoch:25 step:19820 [D loss: 0.012098, acc.: 100.00%] [G loss: 0.018666]\n",
      "epoch:25 step:19821 [D loss: 0.011731, acc.: 100.00%] [G loss: 0.023707]\n",
      "epoch:25 step:19822 [D loss: 0.003844, acc.: 100.00%] [G loss: 0.013373]\n",
      "epoch:25 step:19823 [D loss: 0.030302, acc.: 100.00%] [G loss: 0.035362]\n",
      "epoch:25 step:19824 [D loss: 0.014226, acc.: 100.00%] [G loss: 0.086910]\n",
      "epoch:25 step:19825 [D loss: 0.021168, acc.: 99.22%] [G loss: 0.013259]\n",
      "epoch:25 step:19826 [D loss: 0.005534, acc.: 100.00%] [G loss: 0.013266]\n",
      "epoch:25 step:19827 [D loss: 0.003542, acc.: 100.00%] [G loss: 0.064170]\n",
      "epoch:25 step:19828 [D loss: 0.005501, acc.: 100.00%] [G loss: 0.002952]\n",
      "epoch:25 step:19829 [D loss: 0.003729, acc.: 100.00%] [G loss: 0.022176]\n",
      "epoch:25 step:19830 [D loss: 0.003933, acc.: 100.00%] [G loss: 0.025741]\n",
      "epoch:25 step:19831 [D loss: 0.003480, acc.: 100.00%] [G loss: 0.010372]\n",
      "epoch:25 step:19832 [D loss: 0.004847, acc.: 100.00%] [G loss: 0.009031]\n",
      "epoch:25 step:19833 [D loss: 0.117074, acc.: 97.66%] [G loss: 0.176559]\n",
      "epoch:25 step:19834 [D loss: 0.006735, acc.: 100.00%] [G loss: 0.437817]\n",
      "epoch:25 step:19835 [D loss: 0.002609, acc.: 100.00%] [G loss: 0.344355]\n",
      "epoch:25 step:19836 [D loss: 0.050186, acc.: 99.22%] [G loss: 0.050650]\n",
      "epoch:25 step:19837 [D loss: 0.002010, acc.: 100.00%] [G loss: 0.042860]\n",
      "epoch:25 step:19838 [D loss: 0.010028, acc.: 100.00%] [G loss: 0.013896]\n",
      "epoch:25 step:19839 [D loss: 0.001336, acc.: 100.00%] [G loss: 0.470931]\n",
      "epoch:25 step:19840 [D loss: 0.219001, acc.: 91.41%] [G loss: 0.000826]\n",
      "epoch:25 step:19841 [D loss: 0.095657, acc.: 96.09%] [G loss: 0.004680]\n",
      "epoch:25 step:19842 [D loss: 0.012748, acc.: 100.00%] [G loss: 0.027509]\n",
      "epoch:25 step:19843 [D loss: 0.002591, acc.: 100.00%] [G loss: 0.088078]\n",
      "epoch:25 step:19844 [D loss: 0.002142, acc.: 100.00%] [G loss: 0.088464]\n",
      "epoch:25 step:19845 [D loss: 0.021583, acc.: 99.22%] [G loss: 0.009896]\n",
      "epoch:25 step:19846 [D loss: 0.000916, acc.: 100.00%] [G loss: 0.048607]\n",
      "epoch:25 step:19847 [D loss: 0.000658, acc.: 100.00%] [G loss: 0.002288]\n",
      "epoch:25 step:19848 [D loss: 0.000902, acc.: 100.00%] [G loss: 0.004996]\n",
      "epoch:25 step:19849 [D loss: 0.000543, acc.: 100.00%] [G loss: 0.001883]\n",
      "epoch:25 step:19850 [D loss: 0.000762, acc.: 100.00%] [G loss: 0.028643]\n",
      "epoch:25 step:19851 [D loss: 0.001091, acc.: 100.00%] [G loss: 0.007557]\n",
      "epoch:25 step:19852 [D loss: 0.001612, acc.: 100.00%] [G loss: 0.004008]\n",
      "epoch:25 step:19853 [D loss: 0.000718, acc.: 100.00%] [G loss: 0.003004]\n",
      "epoch:25 step:19854 [D loss: 0.001894, acc.: 100.00%] [G loss: 0.017163]\n",
      "epoch:25 step:19855 [D loss: 0.001647, acc.: 100.00%] [G loss: 0.004691]\n",
      "epoch:25 step:19856 [D loss: 0.002368, acc.: 100.00%] [G loss: 0.013765]\n",
      "epoch:25 step:19857 [D loss: 0.004299, acc.: 100.00%] [G loss: 0.007313]\n",
      "epoch:25 step:19858 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.786194]\n",
      "epoch:25 step:19859 [D loss: 0.002466, acc.: 100.00%] [G loss: 0.002806]\n",
      "epoch:25 step:19860 [D loss: 0.000794, acc.: 100.00%] [G loss: 0.006365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19861 [D loss: 0.011133, acc.: 100.00%] [G loss: 0.003743]\n",
      "epoch:25 step:19862 [D loss: 0.006455, acc.: 100.00%] [G loss: 0.001203]\n",
      "epoch:25 step:19863 [D loss: 0.009180, acc.: 100.00%] [G loss: 0.012040]\n",
      "epoch:25 step:19864 [D loss: 0.000769, acc.: 100.00%] [G loss: 0.014896]\n",
      "epoch:25 step:19865 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.418967]\n",
      "epoch:25 step:19866 [D loss: 0.001415, acc.: 100.00%] [G loss: 0.042302]\n",
      "epoch:25 step:19867 [D loss: 0.003024, acc.: 100.00%] [G loss: 0.005286]\n",
      "epoch:25 step:19868 [D loss: 0.002006, acc.: 100.00%] [G loss: 0.002798]\n",
      "epoch:25 step:19869 [D loss: 0.000920, acc.: 100.00%] [G loss: 0.009444]\n",
      "epoch:25 step:19870 [D loss: 0.014240, acc.: 100.00%] [G loss: 0.337823]\n",
      "epoch:25 step:19871 [D loss: 0.132209, acc.: 96.88%] [G loss: 0.263877]\n",
      "epoch:25 step:19872 [D loss: 0.007370, acc.: 100.00%] [G loss: 0.512116]\n",
      "epoch:25 step:19873 [D loss: 0.104886, acc.: 94.53%] [G loss: 1.128004]\n",
      "epoch:25 step:19874 [D loss: 0.007516, acc.: 100.00%] [G loss: 0.023063]\n",
      "epoch:25 step:19875 [D loss: 0.019752, acc.: 100.00%] [G loss: 0.020519]\n",
      "epoch:25 step:19876 [D loss: 0.024002, acc.: 100.00%] [G loss: 0.037908]\n",
      "epoch:25 step:19877 [D loss: 0.009101, acc.: 100.00%] [G loss: 0.032968]\n",
      "epoch:25 step:19878 [D loss: 0.018174, acc.: 99.22%] [G loss: 0.298159]\n",
      "epoch:25 step:19879 [D loss: 0.001803, acc.: 100.00%] [G loss: 0.001632]\n",
      "epoch:25 step:19880 [D loss: 0.001607, acc.: 100.00%] [G loss: 0.003154]\n",
      "epoch:25 step:19881 [D loss: 0.222295, acc.: 90.62%] [G loss: 0.548400]\n",
      "epoch:25 step:19882 [D loss: 0.007408, acc.: 100.00%] [G loss: 1.987018]\n",
      "epoch:25 step:19883 [D loss: 0.386235, acc.: 83.59%] [G loss: 0.073171]\n",
      "epoch:25 step:19884 [D loss: 0.002827, acc.: 100.00%] [G loss: 0.014900]\n",
      "epoch:25 step:19885 [D loss: 0.005159, acc.: 100.00%] [G loss: 0.006813]\n",
      "epoch:25 step:19886 [D loss: 0.001852, acc.: 100.00%] [G loss: 0.003925]\n",
      "epoch:25 step:19887 [D loss: 0.020783, acc.: 100.00%] [G loss: 0.040376]\n",
      "epoch:25 step:19888 [D loss: 0.000399, acc.: 100.00%] [G loss: 0.026255]\n",
      "epoch:25 step:19889 [D loss: 0.001456, acc.: 100.00%] [G loss: 0.020389]\n",
      "epoch:25 step:19890 [D loss: 0.002843, acc.: 100.00%] [G loss: 0.015787]\n",
      "epoch:25 step:19891 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.014581]\n",
      "epoch:25 step:19892 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.026518]\n",
      "epoch:25 step:19893 [D loss: 0.002916, acc.: 100.00%] [G loss: 0.018112]\n",
      "epoch:25 step:19894 [D loss: 0.002836, acc.: 100.00%] [G loss: 0.306971]\n",
      "epoch:25 step:19895 [D loss: 0.000768, acc.: 100.00%] [G loss: 0.009960]\n",
      "epoch:25 step:19896 [D loss: 0.001912, acc.: 100.00%] [G loss: 0.005176]\n",
      "epoch:25 step:19897 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.001328]\n",
      "epoch:25 step:19898 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.000983]\n",
      "epoch:25 step:19899 [D loss: 0.002331, acc.: 100.00%] [G loss: 0.001798]\n",
      "epoch:25 step:19900 [D loss: 0.001695, acc.: 100.00%] [G loss: 0.003628]\n",
      "epoch:25 step:19901 [D loss: 0.000394, acc.: 100.00%] [G loss: 0.001130]\n",
      "epoch:25 step:19902 [D loss: 0.000915, acc.: 100.00%] [G loss: 0.367633]\n",
      "epoch:25 step:19903 [D loss: 0.001771, acc.: 100.00%] [G loss: 0.003261]\n",
      "epoch:25 step:19904 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.000657]\n",
      "epoch:25 step:19905 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.001061]\n",
      "epoch:25 step:19906 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.001064]\n",
      "epoch:25 step:19907 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.001889]\n",
      "epoch:25 step:19908 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.001619]\n",
      "epoch:25 step:19909 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.004802]\n",
      "epoch:25 step:19910 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.002537]\n",
      "epoch:25 step:19911 [D loss: 0.000589, acc.: 100.00%] [G loss: 0.001002]\n",
      "epoch:25 step:19912 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:25 step:19913 [D loss: 0.005243, acc.: 100.00%] [G loss: 0.001916]\n",
      "epoch:25 step:19914 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.001175]\n",
      "epoch:25 step:19915 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:25 step:19916 [D loss: 0.000725, acc.: 100.00%] [G loss: 0.001040]\n",
      "epoch:25 step:19917 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.000952]\n",
      "epoch:25 step:19918 [D loss: 0.005402, acc.: 100.00%] [G loss: 0.000725]\n",
      "epoch:25 step:19919 [D loss: 0.002172, acc.: 100.00%] [G loss: 0.000648]\n",
      "epoch:25 step:19920 [D loss: 0.006068, acc.: 100.00%] [G loss: 0.008464]\n",
      "epoch:25 step:19921 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.011423]\n",
      "epoch:25 step:19922 [D loss: 0.004175, acc.: 100.00%] [G loss: 0.001901]\n",
      "epoch:25 step:19923 [D loss: 0.103270, acc.: 96.09%] [G loss: 0.007195]\n",
      "epoch:25 step:19924 [D loss: 0.007438, acc.: 100.00%] [G loss: 0.084268]\n",
      "epoch:25 step:19925 [D loss: 0.001357, acc.: 100.00%] [G loss: 0.016452]\n",
      "epoch:25 step:19926 [D loss: 0.018005, acc.: 99.22%] [G loss: 1.104424]\n",
      "epoch:25 step:19927 [D loss: 0.005398, acc.: 100.00%] [G loss: 0.012413]\n",
      "epoch:25 step:19928 [D loss: 0.005993, acc.: 100.00%] [G loss: 0.388858]\n",
      "epoch:25 step:19929 [D loss: 0.020510, acc.: 99.22%] [G loss: 0.032253]\n",
      "epoch:25 step:19930 [D loss: 0.007110, acc.: 100.00%] [G loss: 0.006179]\n",
      "epoch:25 step:19931 [D loss: 0.031031, acc.: 100.00%] [G loss: 0.021292]\n",
      "epoch:25 step:19932 [D loss: 0.003969, acc.: 100.00%] [G loss: 0.070378]\n",
      "epoch:25 step:19933 [D loss: 0.020397, acc.: 99.22%] [G loss: 0.030039]\n",
      "epoch:25 step:19934 [D loss: 0.007730, acc.: 100.00%] [G loss: 0.015214]\n",
      "epoch:25 step:19935 [D loss: 0.002243, acc.: 100.00%] [G loss: 0.009138]\n",
      "epoch:25 step:19936 [D loss: 0.336435, acc.: 87.50%] [G loss: 0.000073]\n",
      "epoch:25 step:19937 [D loss: 0.689204, acc.: 64.06%] [G loss: 2.340606]\n",
      "epoch:25 step:19938 [D loss: 0.846717, acc.: 67.19%] [G loss: 2.234889]\n",
      "epoch:25 step:19939 [D loss: 0.027235, acc.: 100.00%] [G loss: 0.238985]\n",
      "epoch:25 step:19940 [D loss: 0.030100, acc.: 100.00%] [G loss: 0.183427]\n",
      "epoch:25 step:19941 [D loss: 0.052204, acc.: 100.00%] [G loss: 0.538784]\n",
      "epoch:25 step:19942 [D loss: 0.113768, acc.: 95.31%] [G loss: 0.655996]\n",
      "epoch:25 step:19943 [D loss: 0.074716, acc.: 96.88%] [G loss: 4.201591]\n",
      "epoch:25 step:19944 [D loss: 0.074092, acc.: 96.88%] [G loss: 0.006682]\n",
      "epoch:25 step:19945 [D loss: 0.066326, acc.: 98.44%] [G loss: 0.005601]\n",
      "epoch:25 step:19946 [D loss: 0.056144, acc.: 97.66%] [G loss: 1.162771]\n",
      "epoch:25 step:19947 [D loss: 0.018083, acc.: 99.22%] [G loss: 0.014470]\n",
      "epoch:25 step:19948 [D loss: 0.001989, acc.: 100.00%] [G loss: 0.009879]\n",
      "epoch:25 step:19949 [D loss: 0.006821, acc.: 100.00%] [G loss: 1.413354]\n",
      "epoch:25 step:19950 [D loss: 0.001494, acc.: 100.00%] [G loss: 0.531463]\n",
      "epoch:25 step:19951 [D loss: 0.001848, acc.: 100.00%] [G loss: 0.018989]\n",
      "epoch:25 step:19952 [D loss: 0.001944, acc.: 100.00%] [G loss: 0.259195]\n",
      "epoch:25 step:19953 [D loss: 0.100965, acc.: 96.09%] [G loss: 0.000424]\n",
      "epoch:25 step:19954 [D loss: 0.003177, acc.: 100.00%] [G loss: 0.001255]\n",
      "epoch:25 step:19955 [D loss: 0.270890, acc.: 85.94%] [G loss: 0.584344]\n",
      "epoch:25 step:19956 [D loss: 0.541189, acc.: 75.78%] [G loss: 0.109884]\n",
      "epoch:25 step:19957 [D loss: 0.033019, acc.: 100.00%] [G loss: 0.007450]\n",
      "epoch:25 step:19958 [D loss: 0.003626, acc.: 100.00%] [G loss: 0.007230]\n",
      "epoch:25 step:19959 [D loss: 0.003842, acc.: 100.00%] [G loss: 0.010382]\n",
      "epoch:25 step:19960 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.008411]\n",
      "epoch:25 step:19961 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.006309]\n",
      "epoch:25 step:19962 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.010588]\n",
      "epoch:25 step:19963 [D loss: 0.000675, acc.: 100.00%] [G loss: 0.003288]\n",
      "epoch:25 step:19964 [D loss: 0.000584, acc.: 100.00%] [G loss: 0.024857]\n",
      "epoch:25 step:19965 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.091946]\n",
      "epoch:25 step:19966 [D loss: 0.001009, acc.: 100.00%] [G loss: 0.005950]\n",
      "epoch:25 step:19967 [D loss: 0.000514, acc.: 100.00%] [G loss: 0.012787]\n",
      "epoch:25 step:19968 [D loss: 0.001934, acc.: 100.00%] [G loss: 0.005444]\n",
      "epoch:25 step:19969 [D loss: 0.000516, acc.: 100.00%] [G loss: 0.046807]\n",
      "epoch:25 step:19970 [D loss: 0.001286, acc.: 100.00%] [G loss: 0.004697]\n",
      "epoch:25 step:19971 [D loss: 0.002166, acc.: 100.00%] [G loss: 0.028083]\n",
      "epoch:25 step:19972 [D loss: 0.006841, acc.: 100.00%] [G loss: 0.054214]\n",
      "epoch:25 step:19973 [D loss: 0.002173, acc.: 100.00%] [G loss: 0.059766]\n",
      "epoch:25 step:19974 [D loss: 0.035044, acc.: 100.00%] [G loss: 0.088940]\n",
      "epoch:25 step:19975 [D loss: 0.003861, acc.: 100.00%] [G loss: 0.031186]\n",
      "epoch:25 step:19976 [D loss: 0.007728, acc.: 100.00%] [G loss: 0.019453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:19977 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.084907]\n",
      "epoch:25 step:19978 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.073835]\n",
      "epoch:25 step:19979 [D loss: 0.007543, acc.: 99.22%] [G loss: 0.015696]\n",
      "epoch:25 step:19980 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.003043]\n",
      "epoch:25 step:19981 [D loss: 0.001288, acc.: 100.00%] [G loss: 0.006412]\n",
      "epoch:25 step:19982 [D loss: 0.001823, acc.: 100.00%] [G loss: 0.006114]\n",
      "epoch:25 step:19983 [D loss: 0.002346, acc.: 100.00%] [G loss: 0.301564]\n",
      "epoch:25 step:19984 [D loss: 0.002949, acc.: 100.00%] [G loss: 0.011944]\n",
      "epoch:25 step:19985 [D loss: 0.005789, acc.: 100.00%] [G loss: 0.097117]\n",
      "epoch:25 step:19986 [D loss: 0.023633, acc.: 100.00%] [G loss: 0.012473]\n",
      "epoch:25 step:19987 [D loss: 0.002583, acc.: 100.00%] [G loss: 0.018620]\n",
      "epoch:25 step:19988 [D loss: 0.031616, acc.: 99.22%] [G loss: 0.299644]\n",
      "epoch:25 step:19989 [D loss: 0.008875, acc.: 100.00%] [G loss: 0.102097]\n",
      "epoch:25 step:19990 [D loss: 0.003673, acc.: 100.00%] [G loss: 0.069823]\n",
      "epoch:25 step:19991 [D loss: 0.001103, acc.: 100.00%] [G loss: 0.030172]\n",
      "epoch:25 step:19992 [D loss: 0.017186, acc.: 99.22%] [G loss: 0.033509]\n",
      "epoch:25 step:19993 [D loss: 0.000703, acc.: 100.00%] [G loss: 1.176306]\n",
      "epoch:25 step:19994 [D loss: 0.015451, acc.: 100.00%] [G loss: 0.050722]\n",
      "epoch:25 step:19995 [D loss: 0.004143, acc.: 100.00%] [G loss: 0.147526]\n",
      "epoch:25 step:19996 [D loss: 0.002126, acc.: 100.00%] [G loss: 0.048057]\n",
      "epoch:25 step:19997 [D loss: 0.005609, acc.: 100.00%] [G loss: 0.064850]\n",
      "epoch:25 step:19998 [D loss: 0.039034, acc.: 99.22%] [G loss: 0.043377]\n",
      "epoch:25 step:19999 [D loss: 0.018959, acc.: 100.00%] [G loss: 0.755653]\n",
      "epoch:25 step:20000 [D loss: 0.068234, acc.: 98.44%] [G loss: 0.297328]\n",
      "epoch:25 step:20001 [D loss: 0.010445, acc.: 100.00%] [G loss: 1.336069]\n",
      "epoch:25 step:20002 [D loss: 0.005000, acc.: 100.00%] [G loss: 0.830463]\n",
      "epoch:25 step:20003 [D loss: 0.017262, acc.: 99.22%] [G loss: 0.140021]\n",
      "epoch:25 step:20004 [D loss: 0.006799, acc.: 100.00%] [G loss: 0.134133]\n",
      "epoch:25 step:20005 [D loss: 0.004873, acc.: 100.00%] [G loss: 0.524184]\n",
      "epoch:25 step:20006 [D loss: 0.012578, acc.: 100.00%] [G loss: 0.282752]\n",
      "epoch:25 step:20007 [D loss: 0.060864, acc.: 100.00%] [G loss: 0.067219]\n",
      "epoch:25 step:20008 [D loss: 0.003249, acc.: 100.00%] [G loss: 0.070979]\n",
      "epoch:25 step:20009 [D loss: 0.001313, acc.: 100.00%] [G loss: 3.706586]\n",
      "epoch:25 step:20010 [D loss: 0.018202, acc.: 100.00%] [G loss: 0.204050]\n",
      "epoch:25 step:20011 [D loss: 0.065670, acc.: 99.22%] [G loss: 0.042568]\n",
      "epoch:25 step:20012 [D loss: 0.022822, acc.: 99.22%] [G loss: 0.304799]\n",
      "epoch:25 step:20013 [D loss: 0.321328, acc.: 85.16%] [G loss: 2.728645]\n",
      "epoch:25 step:20014 [D loss: 0.524796, acc.: 76.56%] [G loss: 0.297718]\n",
      "epoch:25 step:20015 [D loss: 0.021361, acc.: 99.22%] [G loss: 0.034481]\n",
      "epoch:25 step:20016 [D loss: 0.018314, acc.: 100.00%] [G loss: 2.757752]\n",
      "epoch:25 step:20017 [D loss: 0.012760, acc.: 100.00%] [G loss: 4.149857]\n",
      "epoch:25 step:20018 [D loss: 0.048901, acc.: 98.44%] [G loss: 0.022259]\n",
      "epoch:25 step:20019 [D loss: 0.000581, acc.: 100.00%] [G loss: 0.052974]\n",
      "epoch:25 step:20020 [D loss: 0.004752, acc.: 100.00%] [G loss: 0.042121]\n",
      "epoch:25 step:20021 [D loss: 0.001364, acc.: 100.00%] [G loss: 0.038005]\n",
      "epoch:25 step:20022 [D loss: 0.004008, acc.: 100.00%] [G loss: 3.367450]\n",
      "epoch:25 step:20023 [D loss: 0.007313, acc.: 100.00%] [G loss: 0.015435]\n",
      "epoch:25 step:20024 [D loss: 0.007373, acc.: 100.00%] [G loss: 0.015950]\n",
      "epoch:25 step:20025 [D loss: 0.003082, acc.: 100.00%] [G loss: 1.045200]\n",
      "epoch:25 step:20026 [D loss: 0.001903, acc.: 100.00%] [G loss: 0.004461]\n",
      "epoch:25 step:20027 [D loss: 0.008948, acc.: 100.00%] [G loss: 0.018745]\n",
      "epoch:25 step:20028 [D loss: 0.000516, acc.: 100.00%] [G loss: 0.391563]\n",
      "epoch:25 step:20029 [D loss: 0.001450, acc.: 100.00%] [G loss: 0.002800]\n",
      "epoch:25 step:20030 [D loss: 0.009858, acc.: 100.00%] [G loss: 0.004599]\n",
      "epoch:25 step:20031 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.001853]\n",
      "epoch:25 step:20032 [D loss: 0.001356, acc.: 100.00%] [G loss: 0.225858]\n",
      "epoch:25 step:20033 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.002346]\n",
      "epoch:25 step:20034 [D loss: 0.006988, acc.: 100.00%] [G loss: 0.001720]\n",
      "epoch:25 step:20035 [D loss: 0.001373, acc.: 100.00%] [G loss: 0.000793]\n",
      "epoch:25 step:20036 [D loss: 0.003107, acc.: 100.00%] [G loss: 0.001035]\n",
      "epoch:25 step:20037 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000763]\n",
      "epoch:25 step:20038 [D loss: 0.001424, acc.: 100.00%] [G loss: 0.003144]\n",
      "epoch:25 step:20039 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.091285]\n",
      "epoch:25 step:20040 [D loss: 0.002300, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:25 step:20041 [D loss: 0.001868, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:25 step:20042 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.009023]\n",
      "epoch:25 step:20043 [D loss: 0.004941, acc.: 100.00%] [G loss: 0.001694]\n",
      "epoch:25 step:20044 [D loss: 0.000423, acc.: 100.00%] [G loss: 1.197950]\n",
      "epoch:25 step:20045 [D loss: 0.015098, acc.: 100.00%] [G loss: 0.044591]\n",
      "epoch:25 step:20046 [D loss: 0.036277, acc.: 100.00%] [G loss: 0.003327]\n",
      "epoch:25 step:20047 [D loss: 0.011294, acc.: 100.00%] [G loss: 0.642554]\n",
      "epoch:25 step:20048 [D loss: 0.002820, acc.: 100.00%] [G loss: 0.022035]\n",
      "epoch:25 step:20049 [D loss: 0.011267, acc.: 99.22%] [G loss: 0.017723]\n",
      "epoch:25 step:20050 [D loss: 0.000633, acc.: 100.00%] [G loss: 0.117403]\n",
      "epoch:25 step:20051 [D loss: 0.134960, acc.: 96.09%] [G loss: 0.762214]\n",
      "epoch:25 step:20052 [D loss: 0.002439, acc.: 100.00%] [G loss: 0.132478]\n",
      "epoch:25 step:20053 [D loss: 0.012206, acc.: 100.00%] [G loss: 0.067606]\n",
      "epoch:25 step:20054 [D loss: 0.005041, acc.: 100.00%] [G loss: 0.033242]\n",
      "epoch:25 step:20055 [D loss: 0.001985, acc.: 100.00%] [G loss: 0.024961]\n",
      "epoch:25 step:20056 [D loss: 0.001305, acc.: 100.00%] [G loss: 0.518091]\n",
      "epoch:25 step:20057 [D loss: 0.050542, acc.: 98.44%] [G loss: 0.414666]\n",
      "epoch:25 step:20058 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.411793]\n",
      "epoch:25 step:20059 [D loss: 0.000772, acc.: 100.00%] [G loss: 0.001677]\n",
      "epoch:25 step:20060 [D loss: 0.011069, acc.: 100.00%] [G loss: 0.418402]\n",
      "epoch:25 step:20061 [D loss: 0.003026, acc.: 100.00%] [G loss: 0.007968]\n",
      "epoch:25 step:20062 [D loss: 0.076564, acc.: 96.88%] [G loss: 0.000365]\n",
      "epoch:25 step:20063 [D loss: 0.001686, acc.: 100.00%] [G loss: 0.000638]\n",
      "epoch:25 step:20064 [D loss: 0.005946, acc.: 100.00%] [G loss: 0.002589]\n",
      "epoch:25 step:20065 [D loss: 0.001177, acc.: 100.00%] [G loss: 0.082642]\n",
      "epoch:25 step:20066 [D loss: 0.002377, acc.: 100.00%] [G loss: 0.112147]\n",
      "epoch:25 step:20067 [D loss: 0.053803, acc.: 99.22%] [G loss: 0.001095]\n",
      "epoch:25 step:20068 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.022871]\n",
      "epoch:25 step:20069 [D loss: 0.003062, acc.: 100.00%] [G loss: 0.009600]\n",
      "epoch:25 step:20070 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.974817]\n",
      "epoch:25 step:20071 [D loss: 0.002706, acc.: 100.00%] [G loss: 0.006218]\n",
      "epoch:25 step:20072 [D loss: 0.005300, acc.: 100.00%] [G loss: 0.011742]\n",
      "epoch:25 step:20073 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.122087]\n",
      "epoch:25 step:20074 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.135638]\n",
      "epoch:25 step:20075 [D loss: 0.001531, acc.: 100.00%] [G loss: 0.002646]\n",
      "epoch:25 step:20076 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.113890]\n",
      "epoch:25 step:20077 [D loss: 0.005119, acc.: 100.00%] [G loss: 0.001482]\n",
      "epoch:25 step:20078 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.008314]\n",
      "epoch:25 step:20079 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.056712]\n",
      "epoch:25 step:20080 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.026133]\n",
      "epoch:25 step:20081 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.003683]\n",
      "epoch:25 step:20082 [D loss: 0.001643, acc.: 100.00%] [G loss: 0.005280]\n",
      "epoch:25 step:20083 [D loss: 0.001893, acc.: 100.00%] [G loss: 0.066430]\n",
      "epoch:25 step:20084 [D loss: 0.001176, acc.: 100.00%] [G loss: 0.002240]\n",
      "epoch:25 step:20085 [D loss: 0.000950, acc.: 100.00%] [G loss: 0.002164]\n",
      "epoch:25 step:20086 [D loss: 0.002406, acc.: 100.00%] [G loss: 0.002348]\n",
      "epoch:25 step:20087 [D loss: 0.001226, acc.: 100.00%] [G loss: 0.001615]\n",
      "epoch:25 step:20088 [D loss: 0.003164, acc.: 100.00%] [G loss: 0.023848]\n",
      "epoch:25 step:20089 [D loss: 0.001148, acc.: 100.00%] [G loss: 0.003089]\n",
      "epoch:25 step:20090 [D loss: 0.000685, acc.: 100.00%] [G loss: 0.002244]\n",
      "epoch:25 step:20091 [D loss: 0.004104, acc.: 100.00%] [G loss: 0.001153]\n",
      "epoch:25 step:20092 [D loss: 0.001106, acc.: 100.00%] [G loss: 0.019470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20093 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.001985]\n",
      "epoch:25 step:20094 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.004518]\n",
      "epoch:25 step:20095 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.003836]\n",
      "epoch:25 step:20096 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.011129]\n",
      "epoch:25 step:20097 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.002724]\n",
      "epoch:25 step:20098 [D loss: 0.002856, acc.: 100.00%] [G loss: 0.002336]\n",
      "epoch:25 step:20099 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.001839]\n",
      "epoch:25 step:20100 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.001465]\n",
      "epoch:25 step:20101 [D loss: 0.027669, acc.: 100.00%] [G loss: 0.018761]\n",
      "epoch:25 step:20102 [D loss: 0.023253, acc.: 99.22%] [G loss: 0.336052]\n",
      "epoch:25 step:20103 [D loss: 0.000641, acc.: 100.00%] [G loss: 0.132197]\n",
      "epoch:25 step:20104 [D loss: 0.000712, acc.: 100.00%] [G loss: 0.115478]\n",
      "epoch:25 step:20105 [D loss: 0.003172, acc.: 100.00%] [G loss: 4.319315]\n",
      "epoch:25 step:20106 [D loss: 0.007608, acc.: 100.00%] [G loss: 0.012294]\n",
      "epoch:25 step:20107 [D loss: 0.075611, acc.: 98.44%] [G loss: 0.276754]\n",
      "epoch:25 step:20108 [D loss: 0.023953, acc.: 100.00%] [G loss: 0.203323]\n",
      "epoch:25 step:20109 [D loss: 0.004190, acc.: 100.00%] [G loss: 2.923188]\n",
      "epoch:25 step:20110 [D loss: 0.061284, acc.: 97.66%] [G loss: 1.282103]\n",
      "epoch:25 step:20111 [D loss: 0.005755, acc.: 100.00%] [G loss: 1.771592]\n",
      "epoch:25 step:20112 [D loss: 0.034629, acc.: 100.00%] [G loss: 0.713112]\n",
      "epoch:25 step:20113 [D loss: 0.009688, acc.: 100.00%] [G loss: 0.656914]\n",
      "epoch:25 step:20114 [D loss: 0.326261, acc.: 85.16%] [G loss: 4.248738]\n",
      "epoch:25 step:20115 [D loss: 0.489273, acc.: 78.91%] [G loss: 0.990255]\n",
      "epoch:25 step:20116 [D loss: 0.023283, acc.: 99.22%] [G loss: 0.032444]\n",
      "epoch:25 step:20117 [D loss: 0.001562, acc.: 100.00%] [G loss: 0.016889]\n",
      "epoch:25 step:20118 [D loss: 0.001658, acc.: 100.00%] [G loss: 0.064728]\n",
      "epoch:25 step:20119 [D loss: 0.000627, acc.: 100.00%] [G loss: 1.092855]\n",
      "epoch:25 step:20120 [D loss: 0.024542, acc.: 99.22%] [G loss: 3.052849]\n",
      "epoch:25 step:20121 [D loss: 0.024946, acc.: 99.22%] [G loss: 0.004888]\n",
      "epoch:25 step:20122 [D loss: 0.090633, acc.: 97.66%] [G loss: 0.131422]\n",
      "epoch:25 step:20123 [D loss: 0.044318, acc.: 96.88%] [G loss: 0.153621]\n",
      "epoch:25 step:20124 [D loss: 0.017586, acc.: 99.22%] [G loss: 0.029996]\n",
      "epoch:25 step:20125 [D loss: 0.043182, acc.: 97.66%] [G loss: 0.004291]\n",
      "epoch:25 step:20126 [D loss: 0.008756, acc.: 100.00%] [G loss: 0.000735]\n",
      "epoch:25 step:20127 [D loss: 0.037733, acc.: 97.66%] [G loss: 0.010063]\n",
      "epoch:25 step:20128 [D loss: 0.003372, acc.: 100.00%] [G loss: 2.150549]\n",
      "epoch:25 step:20129 [D loss: 0.024892, acc.: 99.22%] [G loss: 0.096209]\n",
      "epoch:25 step:20130 [D loss: 0.006011, acc.: 100.00%] [G loss: 0.019231]\n",
      "epoch:25 step:20131 [D loss: 0.002619, acc.: 100.00%] [G loss: 0.008793]\n",
      "epoch:25 step:20132 [D loss: 0.006744, acc.: 100.00%] [G loss: 0.002581]\n",
      "epoch:25 step:20133 [D loss: 0.001817, acc.: 100.00%] [G loss: 2.746695]\n",
      "epoch:25 step:20134 [D loss: 0.108355, acc.: 98.44%] [G loss: 0.481023]\n",
      "epoch:25 step:20135 [D loss: 0.010019, acc.: 100.00%] [G loss: 1.195047]\n",
      "epoch:25 step:20136 [D loss: 0.028434, acc.: 100.00%] [G loss: 2.187320]\n",
      "epoch:25 step:20137 [D loss: 0.014639, acc.: 99.22%] [G loss: 0.171951]\n",
      "epoch:25 step:20138 [D loss: 4.166827, acc.: 22.66%] [G loss: 2.651875]\n",
      "epoch:25 step:20139 [D loss: 0.021461, acc.: 99.22%] [G loss: 4.653181]\n",
      "epoch:25 step:20140 [D loss: 0.767093, acc.: 71.88%] [G loss: 0.191106]\n",
      "epoch:25 step:20141 [D loss: 0.052209, acc.: 99.22%] [G loss: 3.445378]\n",
      "epoch:25 step:20142 [D loss: 0.017456, acc.: 100.00%] [G loss: 0.016775]\n",
      "epoch:25 step:20143 [D loss: 0.117227, acc.: 95.31%] [G loss: 0.140217]\n",
      "epoch:25 step:20144 [D loss: 0.013536, acc.: 100.00%] [G loss: 1.673066]\n",
      "epoch:25 step:20145 [D loss: 0.120640, acc.: 96.09%] [G loss: 3.077380]\n",
      "epoch:25 step:20146 [D loss: 0.051581, acc.: 98.44%] [G loss: 2.012528]\n",
      "epoch:25 step:20147 [D loss: 0.016976, acc.: 100.00%] [G loss: 0.128583]\n",
      "epoch:25 step:20148 [D loss: 0.057222, acc.: 99.22%] [G loss: 0.092841]\n",
      "epoch:25 step:20149 [D loss: 0.126736, acc.: 97.66%] [G loss: 1.280383]\n",
      "epoch:25 step:20150 [D loss: 0.009386, acc.: 100.00%] [G loss: 1.975869]\n",
      "epoch:25 step:20151 [D loss: 0.116980, acc.: 93.75%] [G loss: 0.251838]\n",
      "epoch:25 step:20152 [D loss: 0.017554, acc.: 99.22%] [G loss: 0.097315]\n",
      "epoch:25 step:20153 [D loss: 0.008994, acc.: 100.00%] [G loss: 0.466555]\n",
      "epoch:25 step:20154 [D loss: 0.209343, acc.: 89.84%] [G loss: 3.720423]\n",
      "epoch:25 step:20155 [D loss: 0.030416, acc.: 99.22%] [G loss: 4.638425]\n",
      "epoch:25 step:20156 [D loss: 0.617346, acc.: 69.53%] [G loss: 0.703472]\n",
      "epoch:25 step:20157 [D loss: 0.294145, acc.: 83.59%] [G loss: 1.330933]\n",
      "epoch:25 step:20158 [D loss: 0.043681, acc.: 98.44%] [G loss: 2.163958]\n",
      "epoch:25 step:20159 [D loss: 0.093026, acc.: 96.88%] [G loss: 0.983558]\n",
      "epoch:25 step:20160 [D loss: 0.099139, acc.: 96.09%] [G loss: 4.118351]\n",
      "epoch:25 step:20161 [D loss: 0.001321, acc.: 100.00%] [G loss: 0.064051]\n",
      "epoch:25 step:20162 [D loss: 0.013964, acc.: 99.22%] [G loss: 2.872297]\n",
      "epoch:25 step:20163 [D loss: 0.001624, acc.: 100.00%] [G loss: 0.081064]\n",
      "epoch:25 step:20164 [D loss: 0.005096, acc.: 100.00%] [G loss: 0.022993]\n",
      "epoch:25 step:20165 [D loss: 0.000720, acc.: 100.00%] [G loss: 0.077323]\n",
      "epoch:25 step:20166 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.036741]\n",
      "epoch:25 step:20167 [D loss: 0.005975, acc.: 100.00%] [G loss: 0.052967]\n",
      "epoch:25 step:20168 [D loss: 0.001341, acc.: 100.00%] [G loss: 0.008537]\n",
      "epoch:25 step:20169 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.025547]\n",
      "epoch:25 step:20170 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.003572]\n",
      "epoch:25 step:20171 [D loss: 0.010463, acc.: 100.00%] [G loss: 0.005152]\n",
      "epoch:25 step:20172 [D loss: 0.002759, acc.: 100.00%] [G loss: 0.021261]\n",
      "epoch:25 step:20173 [D loss: 0.001008, acc.: 100.00%] [G loss: 0.006704]\n",
      "epoch:25 step:20174 [D loss: 0.027359, acc.: 100.00%] [G loss: 1.268617]\n",
      "epoch:25 step:20175 [D loss: 0.040932, acc.: 100.00%] [G loss: 0.050664]\n",
      "epoch:25 step:20176 [D loss: 0.002779, acc.: 100.00%] [G loss: 0.024547]\n",
      "epoch:25 step:20177 [D loss: 0.023999, acc.: 100.00%] [G loss: 0.031284]\n",
      "epoch:25 step:20178 [D loss: 0.007080, acc.: 100.00%] [G loss: 0.027791]\n",
      "epoch:25 step:20179 [D loss: 0.020333, acc.: 100.00%] [G loss: 0.052662]\n",
      "epoch:25 step:20180 [D loss: 0.043307, acc.: 100.00%] [G loss: 0.260104]\n",
      "epoch:25 step:20181 [D loss: 0.004937, acc.: 100.00%] [G loss: 4.119069]\n",
      "epoch:25 step:20182 [D loss: 0.018921, acc.: 99.22%] [G loss: 0.182318]\n",
      "epoch:25 step:20183 [D loss: 0.001357, acc.: 100.00%] [G loss: 2.147083]\n",
      "epoch:25 step:20184 [D loss: 0.008357, acc.: 100.00%] [G loss: 0.260288]\n",
      "epoch:25 step:20185 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.092054]\n",
      "epoch:25 step:20186 [D loss: 0.028057, acc.: 100.00%] [G loss: 0.077865]\n",
      "epoch:25 step:20187 [D loss: 0.010467, acc.: 99.22%] [G loss: 0.428423]\n",
      "epoch:25 step:20188 [D loss: 0.004374, acc.: 100.00%] [G loss: 0.320651]\n",
      "epoch:25 step:20189 [D loss: 0.000990, acc.: 100.00%] [G loss: 0.074175]\n",
      "epoch:25 step:20190 [D loss: 0.014708, acc.: 100.00%] [G loss: 0.015753]\n",
      "epoch:25 step:20191 [D loss: 0.000795, acc.: 100.00%] [G loss: 0.010160]\n",
      "epoch:25 step:20192 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.027693]\n",
      "epoch:25 step:20193 [D loss: 0.000916, acc.: 100.00%] [G loss: 0.419250]\n",
      "epoch:25 step:20194 [D loss: 0.001248, acc.: 100.00%] [G loss: 0.015744]\n",
      "epoch:25 step:20195 [D loss: 0.004036, acc.: 100.00%] [G loss: 0.015714]\n",
      "epoch:25 step:20196 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.032155]\n",
      "epoch:25 step:20197 [D loss: 0.002686, acc.: 100.00%] [G loss: 0.003486]\n",
      "epoch:25 step:20198 [D loss: 0.001190, acc.: 100.00%] [G loss: 0.008831]\n",
      "epoch:25 step:20199 [D loss: 0.001224, acc.: 100.00%] [G loss: 0.021489]\n",
      "epoch:25 step:20200 [D loss: 0.002493, acc.: 100.00%] [G loss: 0.036986]\n",
      "epoch:25 step:20201 [D loss: 0.005973, acc.: 100.00%] [G loss: 0.013699]\n",
      "epoch:25 step:20202 [D loss: 0.004542, acc.: 100.00%] [G loss: 0.013293]\n",
      "epoch:25 step:20203 [D loss: 0.006100, acc.: 100.00%] [G loss: 0.017879]\n",
      "epoch:25 step:20204 [D loss: 0.002943, acc.: 100.00%] [G loss: 0.004876]\n",
      "epoch:25 step:20205 [D loss: 0.003232, acc.: 100.00%] [G loss: 0.009130]\n",
      "epoch:25 step:20206 [D loss: 0.001232, acc.: 100.00%] [G loss: 0.010064]\n",
      "epoch:25 step:20207 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.035551]\n",
      "epoch:25 step:20208 [D loss: 0.002337, acc.: 100.00%] [G loss: 0.003657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:25 step:20209 [D loss: 0.001014, acc.: 100.00%] [G loss: 0.009795]\n",
      "epoch:25 step:20210 [D loss: 0.002987, acc.: 100.00%] [G loss: 0.013085]\n",
      "epoch:25 step:20211 [D loss: 0.001421, acc.: 100.00%] [G loss: 0.002553]\n",
      "epoch:25 step:20212 [D loss: 0.010581, acc.: 100.00%] [G loss: 0.002120]\n",
      "epoch:25 step:20213 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.031693]\n",
      "epoch:25 step:20214 [D loss: 0.010400, acc.: 100.00%] [G loss: 0.001134]\n",
      "epoch:25 step:20215 [D loss: 0.000451, acc.: 100.00%] [G loss: 0.045280]\n",
      "epoch:25 step:20216 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.030598]\n",
      "epoch:25 step:20217 [D loss: 0.009250, acc.: 100.00%] [G loss: 0.003109]\n",
      "epoch:25 step:20218 [D loss: 0.085121, acc.: 98.44%] [G loss: 0.020840]\n",
      "epoch:25 step:20219 [D loss: 0.004377, acc.: 100.00%] [G loss: 0.022787]\n",
      "epoch:25 step:20220 [D loss: 0.021737, acc.: 98.44%] [G loss: 0.040432]\n",
      "epoch:25 step:20221 [D loss: 0.009760, acc.: 100.00%] [G loss: 0.018913]\n",
      "epoch:25 step:20222 [D loss: 0.004785, acc.: 100.00%] [G loss: 1.144453]\n",
      "epoch:25 step:20223 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.022485]\n",
      "epoch:25 step:20224 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.790847]\n",
      "epoch:25 step:20225 [D loss: 0.002417, acc.: 100.00%] [G loss: 0.009671]\n",
      "epoch:25 step:20226 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.125305]\n",
      "epoch:25 step:20227 [D loss: 0.041854, acc.: 99.22%] [G loss: 0.365366]\n",
      "epoch:25 step:20228 [D loss: 0.001013, acc.: 100.00%] [G loss: 1.084171]\n",
      "epoch:25 step:20229 [D loss: 0.282622, acc.: 87.50%] [G loss: 0.000609]\n",
      "epoch:25 step:20230 [D loss: 0.007191, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:25 step:20231 [D loss: 0.002519, acc.: 100.00%] [G loss: 0.000386]\n",
      "epoch:25 step:20232 [D loss: 0.005284, acc.: 100.00%] [G loss: 0.000349]\n",
      "epoch:25 step:20233 [D loss: 0.000818, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:25 step:20234 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:25 step:20235 [D loss: 0.003083, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:25 step:20236 [D loss: 0.080118, acc.: 98.44%] [G loss: 0.001614]\n",
      "epoch:25 step:20237 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.008932]\n",
      "epoch:25 step:20238 [D loss: 0.026238, acc.: 99.22%] [G loss: 0.005216]\n",
      "epoch:25 step:20239 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.027244]\n",
      "epoch:25 step:20240 [D loss: 0.003045, acc.: 100.00%] [G loss: 0.001788]\n",
      "epoch:25 step:20241 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.001112]\n",
      "epoch:25 step:20242 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.003114]\n",
      "epoch:25 step:20243 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.001565]\n",
      "epoch:25 step:20244 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.003026]\n",
      "epoch:25 step:20245 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.003517]\n",
      "epoch:25 step:20246 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.013906]\n",
      "epoch:25 step:20247 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.002919]\n",
      "epoch:25 step:20248 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.001046]\n",
      "epoch:25 step:20249 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.001064]\n",
      "epoch:25 step:20250 [D loss: 0.001202, acc.: 100.00%] [G loss: 0.001342]\n",
      "epoch:25 step:20251 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000781]\n",
      "epoch:25 step:20252 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000976]\n",
      "epoch:25 step:20253 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.001262]\n",
      "epoch:25 step:20254 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.002992]\n",
      "epoch:25 step:20255 [D loss: 0.003621, acc.: 100.00%] [G loss: 0.001354]\n",
      "epoch:25 step:20256 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.055360]\n",
      "epoch:25 step:20257 [D loss: 0.048633, acc.: 100.00%] [G loss: 0.336815]\n",
      "epoch:25 step:20258 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.085886]\n",
      "epoch:25 step:20259 [D loss: 0.003220, acc.: 100.00%] [G loss: 0.178593]\n",
      "epoch:25 step:20260 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.169102]\n",
      "epoch:25 step:20261 [D loss: 0.005301, acc.: 100.00%] [G loss: 0.043445]\n",
      "epoch:25 step:20262 [D loss: 0.006571, acc.: 100.00%] [G loss: 0.080583]\n",
      "epoch:25 step:20263 [D loss: 0.007880, acc.: 100.00%] [G loss: 0.100746]\n",
      "epoch:25 step:20264 [D loss: 0.011389, acc.: 100.00%] [G loss: 0.067295]\n",
      "epoch:25 step:20265 [D loss: 0.031156, acc.: 100.00%] [G loss: 0.215799]\n",
      "epoch:25 step:20266 [D loss: 0.009108, acc.: 100.00%] [G loss: 0.503200]\n",
      "epoch:25 step:20267 [D loss: 0.031235, acc.: 100.00%] [G loss: 0.555992]\n",
      "epoch:25 step:20268 [D loss: 0.179378, acc.: 97.66%] [G loss: 2.697001]\n",
      "epoch:25 step:20269 [D loss: 0.002126, acc.: 100.00%] [G loss: 4.445649]\n",
      "epoch:25 step:20270 [D loss: 0.069500, acc.: 96.88%] [G loss: 1.018422]\n",
      "epoch:25 step:20271 [D loss: 0.010072, acc.: 100.00%] [G loss: 4.280700]\n",
      "epoch:25 step:20272 [D loss: 0.001286, acc.: 100.00%] [G loss: 2.946489]\n",
      "epoch:25 step:20273 [D loss: 0.008866, acc.: 100.00%] [G loss: 1.672129]\n",
      "epoch:25 step:20274 [D loss: 0.001743, acc.: 100.00%] [G loss: 2.344813]\n",
      "epoch:25 step:20275 [D loss: 0.032370, acc.: 99.22%] [G loss: 1.010746]\n",
      "epoch:25 step:20276 [D loss: 0.016147, acc.: 100.00%] [G loss: 2.113482]\n",
      "epoch:25 step:20277 [D loss: 0.391574, acc.: 79.69%] [G loss: 7.206965]\n",
      "epoch:25 step:20278 [D loss: 1.659048, acc.: 52.34%] [G loss: 4.266599]\n",
      "epoch:25 step:20279 [D loss: 0.062322, acc.: 97.66%] [G loss: 3.749375]\n",
      "epoch:25 step:20280 [D loss: 0.007267, acc.: 100.00%] [G loss: 1.552881]\n",
      "epoch:25 step:20281 [D loss: 0.017492, acc.: 100.00%] [G loss: 0.840762]\n",
      "epoch:25 step:20282 [D loss: 0.058141, acc.: 98.44%] [G loss: 4.768047]\n",
      "epoch:25 step:20283 [D loss: 0.011262, acc.: 100.00%] [G loss: 4.029241]\n",
      "epoch:25 step:20284 [D loss: 0.009578, acc.: 100.00%] [G loss: 4.291608]\n",
      "epoch:25 step:20285 [D loss: 0.045655, acc.: 99.22%] [G loss: 3.192351]\n",
      "epoch:25 step:20286 [D loss: 0.010903, acc.: 100.00%] [G loss: 3.629151]\n",
      "epoch:25 step:20287 [D loss: 0.054596, acc.: 98.44%] [G loss: 0.101661]\n",
      "epoch:25 step:20288 [D loss: 0.080863, acc.: 99.22%] [G loss: 3.597959]\n",
      "epoch:25 step:20289 [D loss: 0.037867, acc.: 98.44%] [G loss: 3.859725]\n",
      "epoch:25 step:20290 [D loss: 0.058606, acc.: 98.44%] [G loss: 3.471452]\n",
      "epoch:25 step:20291 [D loss: 0.019643, acc.: 100.00%] [G loss: 2.466556]\n",
      "epoch:25 step:20292 [D loss: 0.045602, acc.: 100.00%] [G loss: 3.016301]\n",
      "epoch:25 step:20293 [D loss: 0.038799, acc.: 100.00%] [G loss: 4.922229]\n",
      "epoch:25 step:20294 [D loss: 0.009258, acc.: 100.00%] [G loss: 3.830760]\n",
      "epoch:25 step:20295 [D loss: 0.065515, acc.: 98.44%] [G loss: 3.464702]\n",
      "epoch:25 step:20296 [D loss: 0.087035, acc.: 98.44%] [G loss: 2.981883]\n",
      "epoch:25 step:20297 [D loss: 0.048054, acc.: 100.00%] [G loss: 3.668871]\n",
      "epoch:25 step:20298 [D loss: 0.017116, acc.: 100.00%] [G loss: 4.285375]\n",
      "epoch:25 step:20299 [D loss: 0.008473, acc.: 100.00%] [G loss: 4.004233]\n",
      "epoch:25 step:20300 [D loss: 0.013526, acc.: 100.00%] [G loss: 4.428834]\n",
      "epoch:25 step:20301 [D loss: 0.048958, acc.: 98.44%] [G loss: 4.740712]\n",
      "epoch:25 step:20302 [D loss: 0.011110, acc.: 100.00%] [G loss: 4.888251]\n",
      "epoch:25 step:20303 [D loss: 0.028984, acc.: 99.22%] [G loss: 0.672292]\n",
      "epoch:25 step:20304 [D loss: 0.027098, acc.: 100.00%] [G loss: 5.849864]\n",
      "epoch:25 step:20305 [D loss: 0.018089, acc.: 100.00%] [G loss: 5.527023]\n",
      "epoch:25 step:20306 [D loss: 0.010264, acc.: 100.00%] [G loss: 6.204338]\n",
      "epoch:26 step:20307 [D loss: 0.018039, acc.: 100.00%] [G loss: 4.961888]\n",
      "epoch:26 step:20308 [D loss: 0.014180, acc.: 100.00%] [G loss: 5.144090]\n",
      "epoch:26 step:20309 [D loss: 0.040641, acc.: 99.22%] [G loss: 5.779755]\n",
      "epoch:26 step:20310 [D loss: 0.018443, acc.: 100.00%] [G loss: 5.504515]\n",
      "epoch:26 step:20311 [D loss: 0.015380, acc.: 100.00%] [G loss: 5.993452]\n",
      "epoch:26 step:20312 [D loss: 0.046847, acc.: 99.22%] [G loss: 6.393371]\n",
      "epoch:26 step:20313 [D loss: 0.162255, acc.: 92.19%] [G loss: 5.399157]\n",
      "epoch:26 step:20314 [D loss: 0.008399, acc.: 100.00%] [G loss: 4.890630]\n",
      "epoch:26 step:20315 [D loss: 0.049257, acc.: 99.22%] [G loss: 0.060113]\n",
      "epoch:26 step:20316 [D loss: 0.006619, acc.: 100.00%] [G loss: 7.366689]\n",
      "epoch:26 step:20317 [D loss: 0.009453, acc.: 100.00%] [G loss: 5.981694]\n",
      "epoch:26 step:20318 [D loss: 0.012055, acc.: 100.00%] [G loss: 0.118228]\n",
      "epoch:26 step:20319 [D loss: 0.003468, acc.: 100.00%] [G loss: 5.477587]\n",
      "epoch:26 step:20320 [D loss: 0.046009, acc.: 98.44%] [G loss: 3.134559]\n",
      "epoch:26 step:20321 [D loss: 0.005127, acc.: 100.00%] [G loss: 1.412475]\n",
      "epoch:26 step:20322 [D loss: 0.008138, acc.: 100.00%] [G loss: 0.489931]\n",
      "epoch:26 step:20323 [D loss: 0.002407, acc.: 100.00%] [G loss: 0.308540]\n",
      "epoch:26 step:20324 [D loss: 0.018276, acc.: 100.00%] [G loss: 0.258477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20325 [D loss: 0.024107, acc.: 100.00%] [G loss: 0.543643]\n",
      "epoch:26 step:20326 [D loss: 0.006599, acc.: 100.00%] [G loss: 0.683091]\n",
      "epoch:26 step:20327 [D loss: 0.005134, acc.: 100.00%] [G loss: 0.716828]\n",
      "epoch:26 step:20328 [D loss: 0.023686, acc.: 100.00%] [G loss: 0.640096]\n",
      "epoch:26 step:20329 [D loss: 0.012719, acc.: 100.00%] [G loss: 0.299516]\n",
      "epoch:26 step:20330 [D loss: 0.282384, acc.: 85.94%] [G loss: 10.107258]\n",
      "epoch:26 step:20331 [D loss: 3.166138, acc.: 50.00%] [G loss: 3.053212]\n",
      "epoch:26 step:20332 [D loss: 0.042924, acc.: 99.22%] [G loss: 1.113307]\n",
      "epoch:26 step:20333 [D loss: 0.022487, acc.: 100.00%] [G loss: 0.752398]\n",
      "epoch:26 step:20334 [D loss: 0.010320, acc.: 100.00%] [G loss: 0.045479]\n",
      "epoch:26 step:20335 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.462490]\n",
      "epoch:26 step:20336 [D loss: 0.004724, acc.: 100.00%] [G loss: 0.264762]\n",
      "epoch:26 step:20337 [D loss: 0.004482, acc.: 100.00%] [G loss: 0.405934]\n",
      "epoch:26 step:20338 [D loss: 0.000690, acc.: 100.00%] [G loss: 0.191542]\n",
      "epoch:26 step:20339 [D loss: 0.000575, acc.: 100.00%] [G loss: 0.125015]\n",
      "epoch:26 step:20340 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.331848]\n",
      "epoch:26 step:20341 [D loss: 0.003156, acc.: 100.00%] [G loss: 0.010243]\n",
      "epoch:26 step:20342 [D loss: 0.007756, acc.: 100.00%] [G loss: 0.033797]\n",
      "epoch:26 step:20343 [D loss: 0.006140, acc.: 100.00%] [G loss: 0.048805]\n",
      "epoch:26 step:20344 [D loss: 0.023923, acc.: 100.00%] [G loss: 0.021701]\n",
      "epoch:26 step:20345 [D loss: 0.003890, acc.: 100.00%] [G loss: 0.023882]\n",
      "epoch:26 step:20346 [D loss: 0.001228, acc.: 100.00%] [G loss: 0.110135]\n",
      "epoch:26 step:20347 [D loss: 0.003223, acc.: 100.00%] [G loss: 0.036315]\n",
      "epoch:26 step:20348 [D loss: 0.001549, acc.: 100.00%] [G loss: 0.057246]\n",
      "epoch:26 step:20349 [D loss: 0.002964, acc.: 100.00%] [G loss: 0.043250]\n",
      "epoch:26 step:20350 [D loss: 0.006294, acc.: 100.00%] [G loss: 0.003557]\n",
      "epoch:26 step:20351 [D loss: 0.006213, acc.: 100.00%] [G loss: 0.017918]\n",
      "epoch:26 step:20352 [D loss: 0.013447, acc.: 100.00%] [G loss: 0.050155]\n",
      "epoch:26 step:20353 [D loss: 0.048851, acc.: 98.44%] [G loss: 0.066021]\n",
      "epoch:26 step:20354 [D loss: 0.002917, acc.: 100.00%] [G loss: 0.009100]\n",
      "epoch:26 step:20355 [D loss: 0.001997, acc.: 100.00%] [G loss: 0.687028]\n",
      "epoch:26 step:20356 [D loss: 0.007786, acc.: 100.00%] [G loss: 0.119911]\n",
      "epoch:26 step:20357 [D loss: 0.006191, acc.: 100.00%] [G loss: 0.028232]\n",
      "epoch:26 step:20358 [D loss: 0.008954, acc.: 100.00%] [G loss: 0.371246]\n",
      "epoch:26 step:20359 [D loss: 0.000447, acc.: 100.00%] [G loss: 0.084067]\n",
      "epoch:26 step:20360 [D loss: 0.011978, acc.: 100.00%] [G loss: 0.011493]\n",
      "epoch:26 step:20361 [D loss: 0.004605, acc.: 100.00%] [G loss: 0.012416]\n",
      "epoch:26 step:20362 [D loss: 0.020635, acc.: 100.00%] [G loss: 0.007042]\n",
      "epoch:26 step:20363 [D loss: 0.040568, acc.: 100.00%] [G loss: 0.003105]\n",
      "epoch:26 step:20364 [D loss: 0.000639, acc.: 100.00%] [G loss: 0.157872]\n",
      "epoch:26 step:20365 [D loss: 0.002020, acc.: 100.00%] [G loss: 0.358709]\n",
      "epoch:26 step:20366 [D loss: 0.000828, acc.: 100.00%] [G loss: 0.065727]\n",
      "epoch:26 step:20367 [D loss: 0.018047, acc.: 99.22%] [G loss: 0.103178]\n",
      "epoch:26 step:20368 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.006606]\n",
      "epoch:26 step:20369 [D loss: 0.000780, acc.: 100.00%] [G loss: 0.003848]\n",
      "epoch:26 step:20370 [D loss: 0.002534, acc.: 100.00%] [G loss: 0.040167]\n",
      "epoch:26 step:20371 [D loss: 0.047912, acc.: 98.44%] [G loss: 0.008087]\n",
      "epoch:26 step:20372 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.009335]\n",
      "epoch:26 step:20373 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.002503]\n",
      "epoch:26 step:20374 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:26 step:20375 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000494]\n",
      "epoch:26 step:20376 [D loss: 0.003984, acc.: 100.00%] [G loss: 0.001912]\n",
      "epoch:26 step:20377 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.043795]\n",
      "epoch:26 step:20378 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.004187]\n",
      "epoch:26 step:20379 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.001692]\n",
      "epoch:26 step:20380 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:26 step:20381 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.002251]\n",
      "epoch:26 step:20382 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.004178]\n",
      "epoch:26 step:20383 [D loss: 0.001279, acc.: 100.00%] [G loss: 0.004050]\n",
      "epoch:26 step:20384 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:26 step:20385 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.003500]\n",
      "epoch:26 step:20386 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.001761]\n",
      "epoch:26 step:20387 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.006718]\n",
      "epoch:26 step:20388 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.053720]\n",
      "epoch:26 step:20389 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.001647]\n",
      "epoch:26 step:20390 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.003521]\n",
      "epoch:26 step:20391 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:26 step:20392 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.001144]\n",
      "epoch:26 step:20393 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:26 step:20394 [D loss: 0.001895, acc.: 100.00%] [G loss: 0.000627]\n",
      "epoch:26 step:20395 [D loss: 0.003210, acc.: 100.00%] [G loss: 0.004820]\n",
      "epoch:26 step:20396 [D loss: 0.004139, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:26 step:20397 [D loss: 0.010580, acc.: 100.00%] [G loss: 0.005453]\n",
      "epoch:26 step:20398 [D loss: 0.002532, acc.: 100.00%] [G loss: 0.000400]\n",
      "epoch:26 step:20399 [D loss: 0.022861, acc.: 100.00%] [G loss: 0.000596]\n",
      "epoch:26 step:20400 [D loss: 0.049555, acc.: 98.44%] [G loss: 3.061268]\n",
      "epoch:26 step:20401 [D loss: 0.001654, acc.: 100.00%] [G loss: 0.319574]\n",
      "epoch:26 step:20402 [D loss: 0.006685, acc.: 100.00%] [G loss: 0.577493]\n",
      "epoch:26 step:20403 [D loss: 0.023988, acc.: 99.22%] [G loss: 0.067102]\n",
      "epoch:26 step:20404 [D loss: 0.003793, acc.: 100.00%] [G loss: 0.017363]\n",
      "epoch:26 step:20405 [D loss: 0.047106, acc.: 100.00%] [G loss: 0.001563]\n",
      "epoch:26 step:20406 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.072315]\n",
      "epoch:26 step:20407 [D loss: 0.015315, acc.: 99.22%] [G loss: 0.011403]\n",
      "epoch:26 step:20408 [D loss: 0.010062, acc.: 99.22%] [G loss: 0.035900]\n",
      "epoch:26 step:20409 [D loss: 0.025649, acc.: 100.00%] [G loss: 0.008050]\n",
      "epoch:26 step:20410 [D loss: 0.035008, acc.: 100.00%] [G loss: 0.301583]\n",
      "epoch:26 step:20411 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.059135]\n",
      "epoch:26 step:20412 [D loss: 0.026090, acc.: 99.22%] [G loss: 0.093496]\n",
      "epoch:26 step:20413 [D loss: 0.002366, acc.: 100.00%] [G loss: 0.023070]\n",
      "epoch:26 step:20414 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.048095]\n",
      "epoch:26 step:20415 [D loss: 0.002450, acc.: 100.00%] [G loss: 0.111111]\n",
      "epoch:26 step:20416 [D loss: 0.087003, acc.: 96.09%] [G loss: 1.153052]\n",
      "epoch:26 step:20417 [D loss: 0.340764, acc.: 87.50%] [G loss: 0.010898]\n",
      "epoch:26 step:20418 [D loss: 0.001658, acc.: 100.00%] [G loss: 0.159417]\n",
      "epoch:26 step:20419 [D loss: 0.005766, acc.: 100.00%] [G loss: 0.111559]\n",
      "epoch:26 step:20420 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.126518]\n",
      "epoch:26 step:20421 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.149257]\n",
      "epoch:26 step:20422 [D loss: 0.002845, acc.: 100.00%] [G loss: 0.025154]\n",
      "epoch:26 step:20423 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.016206]\n",
      "epoch:26 step:20424 [D loss: 0.001727, acc.: 100.00%] [G loss: 0.012474]\n",
      "epoch:26 step:20425 [D loss: 0.001409, acc.: 100.00%] [G loss: 0.006375]\n",
      "epoch:26 step:20426 [D loss: 0.005787, acc.: 100.00%] [G loss: 0.022368]\n",
      "epoch:26 step:20427 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.431735]\n",
      "epoch:26 step:20428 [D loss: 0.001170, acc.: 100.00%] [G loss: 0.058191]\n",
      "epoch:26 step:20429 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.021285]\n",
      "epoch:26 step:20430 [D loss: 0.001477, acc.: 100.00%] [G loss: 0.011260]\n",
      "epoch:26 step:20431 [D loss: 0.016182, acc.: 100.00%] [G loss: 0.008569]\n",
      "epoch:26 step:20432 [D loss: 0.001681, acc.: 100.00%] [G loss: 0.271895]\n",
      "epoch:26 step:20433 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.038640]\n",
      "epoch:26 step:20434 [D loss: 0.002281, acc.: 100.00%] [G loss: 0.071181]\n",
      "epoch:26 step:20435 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.055121]\n",
      "epoch:26 step:20436 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.022530]\n",
      "epoch:26 step:20437 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.079806]\n",
      "epoch:26 step:20438 [D loss: 0.002139, acc.: 100.00%] [G loss: 0.112936]\n",
      "epoch:26 step:20439 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.006064]\n",
      "epoch:26 step:20440 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.012273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20441 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.016807]\n",
      "epoch:26 step:20442 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.254597]\n",
      "epoch:26 step:20443 [D loss: 0.001190, acc.: 100.00%] [G loss: 0.013626]\n",
      "epoch:26 step:20444 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.003498]\n",
      "epoch:26 step:20445 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.005334]\n",
      "epoch:26 step:20446 [D loss: 0.001411, acc.: 100.00%] [G loss: 0.021776]\n",
      "epoch:26 step:20447 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.026952]\n",
      "epoch:26 step:20448 [D loss: 0.003090, acc.: 100.00%] [G loss: 0.028571]\n",
      "epoch:26 step:20449 [D loss: 0.001190, acc.: 100.00%] [G loss: 0.001461]\n",
      "epoch:26 step:20450 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.004143]\n",
      "epoch:26 step:20451 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.004024]\n",
      "epoch:26 step:20452 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.023111]\n",
      "epoch:26 step:20453 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.009391]\n",
      "epoch:26 step:20454 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.021286]\n",
      "epoch:26 step:20455 [D loss: 0.000894, acc.: 100.00%] [G loss: 0.004246]\n",
      "epoch:26 step:20456 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.004755]\n",
      "epoch:26 step:20457 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.002887]\n",
      "epoch:26 step:20458 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.002911]\n",
      "epoch:26 step:20459 [D loss: 0.003159, acc.: 100.00%] [G loss: 0.002448]\n",
      "epoch:26 step:20460 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.007770]\n",
      "epoch:26 step:20461 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.004415]\n",
      "epoch:26 step:20462 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.002833]\n",
      "epoch:26 step:20463 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.003178]\n",
      "epoch:26 step:20464 [D loss: 0.000913, acc.: 100.00%] [G loss: 0.011379]\n",
      "epoch:26 step:20465 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000646]\n",
      "epoch:26 step:20466 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.002496]\n",
      "epoch:26 step:20467 [D loss: 0.000597, acc.: 100.00%] [G loss: 0.007237]\n",
      "epoch:26 step:20468 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.007099]\n",
      "epoch:26 step:20469 [D loss: 0.001964, acc.: 100.00%] [G loss: 0.001578]\n",
      "epoch:26 step:20470 [D loss: 0.000776, acc.: 100.00%] [G loss: 0.003913]\n",
      "epoch:26 step:20471 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.001953]\n",
      "epoch:26 step:20472 [D loss: 0.004642, acc.: 100.00%] [G loss: 0.003308]\n",
      "epoch:26 step:20473 [D loss: 0.004994, acc.: 100.00%] [G loss: 0.000879]\n",
      "epoch:26 step:20474 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.007224]\n",
      "epoch:26 step:20475 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.004637]\n",
      "epoch:26 step:20476 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.025608]\n",
      "epoch:26 step:20477 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.026861]\n",
      "epoch:26 step:20478 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.002404]\n",
      "epoch:26 step:20479 [D loss: 0.000498, acc.: 100.00%] [G loss: 0.003189]\n",
      "epoch:26 step:20480 [D loss: 0.001000, acc.: 100.00%] [G loss: 0.004915]\n",
      "epoch:26 step:20481 [D loss: 0.001646, acc.: 100.00%] [G loss: 0.009592]\n",
      "epoch:26 step:20482 [D loss: 0.008524, acc.: 100.00%] [G loss: 0.005073]\n",
      "epoch:26 step:20483 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.003682]\n",
      "epoch:26 step:20484 [D loss: 0.000625, acc.: 100.00%] [G loss: 0.007112]\n",
      "epoch:26 step:20485 [D loss: 0.019176, acc.: 99.22%] [G loss: 0.000965]\n",
      "epoch:26 step:20486 [D loss: 0.001422, acc.: 100.00%] [G loss: 0.000669]\n",
      "epoch:26 step:20487 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.001398]\n",
      "epoch:26 step:20488 [D loss: 0.003542, acc.: 100.00%] [G loss: 0.002329]\n",
      "epoch:26 step:20489 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.001345]\n",
      "epoch:26 step:20490 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.000859]\n",
      "epoch:26 step:20491 [D loss: 0.000842, acc.: 100.00%] [G loss: 0.004121]\n",
      "epoch:26 step:20492 [D loss: 0.008116, acc.: 100.00%] [G loss: 0.001202]\n",
      "epoch:26 step:20493 [D loss: 0.000885, acc.: 100.00%] [G loss: 0.067166]\n",
      "epoch:26 step:20494 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.001462]\n",
      "epoch:26 step:20495 [D loss: 0.009966, acc.: 100.00%] [G loss: 0.003723]\n",
      "epoch:26 step:20496 [D loss: 0.007353, acc.: 100.00%] [G loss: 0.116229]\n",
      "epoch:26 step:20497 [D loss: 0.083244, acc.: 97.66%] [G loss: 4.150555]\n",
      "epoch:26 step:20498 [D loss: 0.032912, acc.: 99.22%] [G loss: 1.134339]\n",
      "epoch:26 step:20499 [D loss: 0.977038, acc.: 66.41%] [G loss: 0.029319]\n",
      "epoch:26 step:20500 [D loss: 0.001994, acc.: 100.00%] [G loss: 0.934676]\n",
      "epoch:26 step:20501 [D loss: 0.063894, acc.: 97.66%] [G loss: 3.029531]\n",
      "epoch:26 step:20502 [D loss: 0.002851, acc.: 100.00%] [G loss: 2.245264]\n",
      "epoch:26 step:20503 [D loss: 0.144888, acc.: 95.31%] [G loss: 0.553168]\n",
      "epoch:26 step:20504 [D loss: 0.185171, acc.: 93.75%] [G loss: 3.261050]\n",
      "epoch:26 step:20505 [D loss: 0.016144, acc.: 100.00%] [G loss: 1.150566]\n",
      "epoch:26 step:20506 [D loss: 0.066688, acc.: 96.88%] [G loss: 0.332516]\n",
      "epoch:26 step:20507 [D loss: 0.013678, acc.: 100.00%] [G loss: 0.983713]\n",
      "epoch:26 step:20508 [D loss: 0.001232, acc.: 100.00%] [G loss: 3.330487]\n",
      "epoch:26 step:20509 [D loss: 0.018122, acc.: 99.22%] [G loss: 0.185088]\n",
      "epoch:26 step:20510 [D loss: 0.039474, acc.: 99.22%] [G loss: 1.012948]\n",
      "epoch:26 step:20511 [D loss: 0.007966, acc.: 100.00%] [G loss: 0.052916]\n",
      "epoch:26 step:20512 [D loss: 0.131462, acc.: 95.31%] [G loss: 3.447833]\n",
      "epoch:26 step:20513 [D loss: 0.033792, acc.: 99.22%] [G loss: 1.877730]\n",
      "epoch:26 step:20514 [D loss: 1.703293, acc.: 45.31%] [G loss: 6.083740]\n",
      "epoch:26 step:20515 [D loss: 3.554137, acc.: 50.00%] [G loss: 4.974101]\n",
      "epoch:26 step:20516 [D loss: 0.612250, acc.: 66.41%] [G loss: 0.283874]\n",
      "epoch:26 step:20517 [D loss: 0.235723, acc.: 89.84%] [G loss: 0.286804]\n",
      "epoch:26 step:20518 [D loss: 0.037836, acc.: 99.22%] [G loss: 3.410516]\n",
      "epoch:26 step:20519 [D loss: 0.100088, acc.: 96.09%] [G loss: 0.005534]\n",
      "epoch:26 step:20520 [D loss: 0.046234, acc.: 100.00%] [G loss: 0.018583]\n",
      "epoch:26 step:20521 [D loss: 0.034419, acc.: 100.00%] [G loss: 0.012105]\n",
      "epoch:26 step:20522 [D loss: 0.040917, acc.: 100.00%] [G loss: 0.017955]\n",
      "epoch:26 step:20523 [D loss: 0.049096, acc.: 99.22%] [G loss: 0.023032]\n",
      "epoch:26 step:20524 [D loss: 0.058450, acc.: 100.00%] [G loss: 0.064245]\n",
      "epoch:26 step:20525 [D loss: 0.022054, acc.: 100.00%] [G loss: 0.031909]\n",
      "epoch:26 step:20526 [D loss: 0.120498, acc.: 100.00%] [G loss: 0.380635]\n",
      "epoch:26 step:20527 [D loss: 0.023939, acc.: 99.22%] [G loss: 4.549138]\n",
      "epoch:26 step:20528 [D loss: 0.051626, acc.: 99.22%] [G loss: 0.191862]\n",
      "epoch:26 step:20529 [D loss: 0.057363, acc.: 100.00%] [G loss: 0.090678]\n",
      "epoch:26 step:20530 [D loss: 0.025297, acc.: 100.00%] [G loss: 2.426080]\n",
      "epoch:26 step:20531 [D loss: 0.023914, acc.: 100.00%] [G loss: 0.036110]\n",
      "epoch:26 step:20532 [D loss: 0.016296, acc.: 100.00%] [G loss: 1.032925]\n",
      "epoch:26 step:20533 [D loss: 0.137976, acc.: 93.75%] [G loss: 0.572993]\n",
      "epoch:26 step:20534 [D loss: 0.146681, acc.: 92.97%] [G loss: 1.140403]\n",
      "epoch:26 step:20535 [D loss: 0.118387, acc.: 96.88%] [G loss: 0.409240]\n",
      "epoch:26 step:20536 [D loss: 0.052094, acc.: 99.22%] [G loss: 0.086717]\n",
      "epoch:26 step:20537 [D loss: 0.024993, acc.: 100.00%] [G loss: 0.211661]\n",
      "epoch:26 step:20538 [D loss: 0.253790, acc.: 89.06%] [G loss: 1.645888]\n",
      "epoch:26 step:20539 [D loss: 0.012440, acc.: 100.00%] [G loss: 3.050326]\n",
      "epoch:26 step:20540 [D loss: 0.294218, acc.: 86.72%] [G loss: 1.829525]\n",
      "epoch:26 step:20541 [D loss: 0.151255, acc.: 94.53%] [G loss: 0.971696]\n",
      "epoch:26 step:20542 [D loss: 0.048657, acc.: 99.22%] [G loss: 0.505730]\n",
      "epoch:26 step:20543 [D loss: 0.026361, acc.: 100.00%] [G loss: 2.161973]\n",
      "epoch:26 step:20544 [D loss: 0.056496, acc.: 98.44%] [G loss: 0.018652]\n",
      "epoch:26 step:20545 [D loss: 0.083448, acc.: 98.44%] [G loss: 1.224918]\n",
      "epoch:26 step:20546 [D loss: 0.133563, acc.: 92.19%] [G loss: 0.174531]\n",
      "epoch:26 step:20547 [D loss: 0.029874, acc.: 100.00%] [G loss: 1.666637]\n",
      "epoch:26 step:20548 [D loss: 0.277418, acc.: 87.50%] [G loss: 0.900857]\n",
      "epoch:26 step:20549 [D loss: 0.384585, acc.: 82.03%] [G loss: 0.058398]\n",
      "epoch:26 step:20550 [D loss: 0.034910, acc.: 99.22%] [G loss: 3.867270]\n",
      "epoch:26 step:20551 [D loss: 0.058613, acc.: 98.44%] [G loss: 0.075074]\n",
      "epoch:26 step:20552 [D loss: 0.077226, acc.: 98.44%] [G loss: 0.303249]\n",
      "epoch:26 step:20553 [D loss: 0.159110, acc.: 94.53%] [G loss: 0.104095]\n",
      "epoch:26 step:20554 [D loss: 0.059977, acc.: 99.22%] [G loss: 0.413061]\n",
      "epoch:26 step:20555 [D loss: 0.038396, acc.: 99.22%] [G loss: 0.651954]\n",
      "epoch:26 step:20556 [D loss: 0.037498, acc.: 99.22%] [G loss: 0.953863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20557 [D loss: 0.005611, acc.: 100.00%] [G loss: 2.529261]\n",
      "epoch:26 step:20558 [D loss: 0.148639, acc.: 93.75%] [G loss: 2.538249]\n",
      "epoch:26 step:20559 [D loss: 0.157091, acc.: 93.75%] [G loss: 2.029036]\n",
      "epoch:26 step:20560 [D loss: 0.047231, acc.: 98.44%] [G loss: 2.088483]\n",
      "epoch:26 step:20561 [D loss: 0.050123, acc.: 98.44%] [G loss: 2.218277]\n",
      "epoch:26 step:20562 [D loss: 0.058730, acc.: 99.22%] [G loss: 1.300121]\n",
      "epoch:26 step:20563 [D loss: 0.017453, acc.: 100.00%] [G loss: 1.672955]\n",
      "epoch:26 step:20564 [D loss: 0.032678, acc.: 100.00%] [G loss: 1.487065]\n",
      "epoch:26 step:20565 [D loss: 0.045375, acc.: 100.00%] [G loss: 1.692236]\n",
      "epoch:26 step:20566 [D loss: 0.146198, acc.: 93.75%] [G loss: 0.891889]\n",
      "epoch:26 step:20567 [D loss: 0.304601, acc.: 85.94%] [G loss: 6.676784]\n",
      "epoch:26 step:20568 [D loss: 0.151104, acc.: 93.75%] [G loss: 7.660711]\n",
      "epoch:26 step:20569 [D loss: 0.041054, acc.: 100.00%] [G loss: 0.737304]\n",
      "epoch:26 step:20570 [D loss: 0.098152, acc.: 97.66%] [G loss: 6.716435]\n",
      "epoch:26 step:20571 [D loss: 0.057651, acc.: 99.22%] [G loss: 4.915842]\n",
      "epoch:26 step:20572 [D loss: 0.127636, acc.: 92.19%] [G loss: 5.871164]\n",
      "epoch:26 step:20573 [D loss: 0.098273, acc.: 96.09%] [G loss: 5.393644]\n",
      "epoch:26 step:20574 [D loss: 0.038592, acc.: 99.22%] [G loss: 0.195278]\n",
      "epoch:26 step:20575 [D loss: 0.092378, acc.: 97.66%] [G loss: 3.771339]\n",
      "epoch:26 step:20576 [D loss: 0.097730, acc.: 97.66%] [G loss: 0.073760]\n",
      "epoch:26 step:20577 [D loss: 0.017238, acc.: 100.00%] [G loss: 4.046561]\n",
      "epoch:26 step:20578 [D loss: 0.013483, acc.: 100.00%] [G loss: 2.617795]\n",
      "epoch:26 step:20579 [D loss: 0.232850, acc.: 87.50%] [G loss: 7.023904]\n",
      "epoch:26 step:20580 [D loss: 0.209749, acc.: 90.62%] [G loss: 2.642523]\n",
      "epoch:26 step:20581 [D loss: 0.051033, acc.: 98.44%] [G loss: 2.768021]\n",
      "epoch:26 step:20582 [D loss: 0.023450, acc.: 99.22%] [G loss: 2.536814]\n",
      "epoch:26 step:20583 [D loss: 0.135693, acc.: 96.88%] [G loss: 0.036518]\n",
      "epoch:26 step:20584 [D loss: 0.082816, acc.: 96.09%] [G loss: 2.338513]\n",
      "epoch:26 step:20585 [D loss: 0.008352, acc.: 100.00%] [G loss: 2.075109]\n",
      "epoch:26 step:20586 [D loss: 0.029736, acc.: 98.44%] [G loss: 1.423872]\n",
      "epoch:26 step:20587 [D loss: 0.014886, acc.: 100.00%] [G loss: 0.405551]\n",
      "epoch:26 step:20588 [D loss: 0.234670, acc.: 90.62%] [G loss: 4.767450]\n",
      "epoch:26 step:20589 [D loss: 0.006904, acc.: 100.00%] [G loss: 6.230158]\n",
      "epoch:26 step:20590 [D loss: 0.350290, acc.: 85.16%] [G loss: 3.132969]\n",
      "epoch:26 step:20591 [D loss: 0.044424, acc.: 99.22%] [G loss: 2.715756]\n",
      "epoch:26 step:20592 [D loss: 0.039628, acc.: 100.00%] [G loss: 3.238006]\n",
      "epoch:26 step:20593 [D loss: 0.008793, acc.: 100.00%] [G loss: 2.892091]\n",
      "epoch:26 step:20594 [D loss: 0.051227, acc.: 98.44%] [G loss: 3.729873]\n",
      "epoch:26 step:20595 [D loss: 0.025477, acc.: 100.00%] [G loss: 4.323730]\n",
      "epoch:26 step:20596 [D loss: 0.027144, acc.: 99.22%] [G loss: 4.629709]\n",
      "epoch:26 step:20597 [D loss: 0.048971, acc.: 97.66%] [G loss: 0.639750]\n",
      "epoch:26 step:20598 [D loss: 0.030275, acc.: 100.00%] [G loss: 5.239096]\n",
      "epoch:26 step:20599 [D loss: 0.003911, acc.: 100.00%] [G loss: 4.792034]\n",
      "epoch:26 step:20600 [D loss: 0.029715, acc.: 99.22%] [G loss: 0.013394]\n",
      "epoch:26 step:20601 [D loss: 1.089811, acc.: 60.16%] [G loss: 6.188987]\n",
      "epoch:26 step:20602 [D loss: 0.964279, acc.: 64.06%] [G loss: 1.998946]\n",
      "epoch:26 step:20603 [D loss: 0.245431, acc.: 88.28%] [G loss: 2.748446]\n",
      "epoch:26 step:20604 [D loss: 0.009044, acc.: 100.00%] [G loss: 1.306890]\n",
      "epoch:26 step:20605 [D loss: 0.059202, acc.: 98.44%] [G loss: 0.332755]\n",
      "epoch:26 step:20606 [D loss: 0.040980, acc.: 97.66%] [G loss: 0.011606]\n",
      "epoch:26 step:20607 [D loss: 0.002073, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:26 step:20608 [D loss: 0.004063, acc.: 100.00%] [G loss: 0.001156]\n",
      "epoch:26 step:20609 [D loss: 0.005060, acc.: 100.00%] [G loss: 0.005036]\n",
      "epoch:26 step:20610 [D loss: 0.000501, acc.: 100.00%] [G loss: 0.007702]\n",
      "epoch:26 step:20611 [D loss: 0.001095, acc.: 100.00%] [G loss: 0.000862]\n",
      "epoch:26 step:20612 [D loss: 0.002669, acc.: 100.00%] [G loss: 0.005604]\n",
      "epoch:26 step:20613 [D loss: 0.064738, acc.: 99.22%] [G loss: 0.003504]\n",
      "epoch:26 step:20614 [D loss: 0.003053, acc.: 100.00%] [G loss: 0.005820]\n",
      "epoch:26 step:20615 [D loss: 0.043184, acc.: 100.00%] [G loss: 0.013007]\n",
      "epoch:26 step:20616 [D loss: 0.001992, acc.: 100.00%] [G loss: 0.072647]\n",
      "epoch:26 step:20617 [D loss: 0.066509, acc.: 97.66%] [G loss: 0.003226]\n",
      "epoch:26 step:20618 [D loss: 0.026813, acc.: 98.44%] [G loss: 0.004212]\n",
      "epoch:26 step:20619 [D loss: 0.003525, acc.: 100.00%] [G loss: 0.415090]\n",
      "epoch:26 step:20620 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.001596]\n",
      "epoch:26 step:20621 [D loss: 0.375447, acc.: 81.25%] [G loss: 0.000001]\n",
      "epoch:26 step:20622 [D loss: 0.255799, acc.: 86.72%] [G loss: 0.000173]\n",
      "epoch:26 step:20623 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000768]\n",
      "epoch:26 step:20624 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.815369]\n",
      "epoch:26 step:20625 [D loss: 0.000124, acc.: 100.00%] [G loss: 1.242728]\n",
      "epoch:26 step:20626 [D loss: 0.001113, acc.: 100.00%] [G loss: 0.486681]\n",
      "epoch:26 step:20627 [D loss: 0.003342, acc.: 100.00%] [G loss: 0.003504]\n",
      "epoch:26 step:20628 [D loss: 0.002964, acc.: 100.00%] [G loss: 0.001114]\n",
      "epoch:26 step:20629 [D loss: 0.000555, acc.: 100.00%] [G loss: 0.038338]\n",
      "epoch:26 step:20630 [D loss: 0.419231, acc.: 84.38%] [G loss: 0.856063]\n",
      "epoch:26 step:20631 [D loss: 0.197287, acc.: 91.41%] [G loss: 0.586860]\n",
      "epoch:26 step:20632 [D loss: 0.094875, acc.: 95.31%] [G loss: 0.034663]\n",
      "epoch:26 step:20633 [D loss: 0.003948, acc.: 100.00%] [G loss: 0.031504]\n",
      "epoch:26 step:20634 [D loss: 0.000861, acc.: 100.00%] [G loss: 0.000656]\n",
      "epoch:26 step:20635 [D loss: 0.000730, acc.: 100.00%] [G loss: 1.390192]\n",
      "epoch:26 step:20636 [D loss: 0.003106, acc.: 100.00%] [G loss: 0.276751]\n",
      "epoch:26 step:20637 [D loss: 0.007550, acc.: 100.00%] [G loss: 0.000617]\n",
      "epoch:26 step:20638 [D loss: 0.060769, acc.: 98.44%] [G loss: 0.028169]\n",
      "epoch:26 step:20639 [D loss: 0.001151, acc.: 100.00%] [G loss: 0.090803]\n",
      "epoch:26 step:20640 [D loss: 0.025990, acc.: 99.22%] [G loss: 0.029380]\n",
      "epoch:26 step:20641 [D loss: 0.059546, acc.: 99.22%] [G loss: 0.944478]\n",
      "epoch:26 step:20642 [D loss: 0.035401, acc.: 99.22%] [G loss: 0.087841]\n",
      "epoch:26 step:20643 [D loss: 0.010460, acc.: 100.00%] [G loss: 0.064069]\n",
      "epoch:26 step:20644 [D loss: 0.373517, acc.: 82.03%] [G loss: 0.006638]\n",
      "epoch:26 step:20645 [D loss: 0.110172, acc.: 94.53%] [G loss: 0.584610]\n",
      "epoch:26 step:20646 [D loss: 0.006081, acc.: 100.00%] [G loss: 2.017738]\n",
      "epoch:26 step:20647 [D loss: 0.004047, acc.: 100.00%] [G loss: 1.086679]\n",
      "epoch:26 step:20648 [D loss: 0.041457, acc.: 98.44%] [G loss: 0.333168]\n",
      "epoch:26 step:20649 [D loss: 0.014473, acc.: 100.00%] [G loss: 0.036038]\n",
      "epoch:26 step:20650 [D loss: 0.001275, acc.: 100.00%] [G loss: 0.081309]\n",
      "epoch:26 step:20651 [D loss: 0.001607, acc.: 100.00%] [G loss: 0.013459]\n",
      "epoch:26 step:20652 [D loss: 0.003538, acc.: 100.00%] [G loss: 0.336044]\n",
      "epoch:26 step:20653 [D loss: 0.160233, acc.: 96.09%] [G loss: 0.283731]\n",
      "epoch:26 step:20654 [D loss: 0.004569, acc.: 100.00%] [G loss: 0.784932]\n",
      "epoch:26 step:20655 [D loss: 0.089511, acc.: 96.88%] [G loss: 0.146534]\n",
      "epoch:26 step:20656 [D loss: 0.157259, acc.: 94.53%] [G loss: 0.006112]\n",
      "epoch:26 step:20657 [D loss: 0.010410, acc.: 100.00%] [G loss: 0.038790]\n",
      "epoch:26 step:20658 [D loss: 0.015629, acc.: 99.22%] [G loss: 0.000555]\n",
      "epoch:26 step:20659 [D loss: 0.044542, acc.: 100.00%] [G loss: 0.000988]\n",
      "epoch:26 step:20660 [D loss: 0.004100, acc.: 100.00%] [G loss: 0.034322]\n",
      "epoch:26 step:20661 [D loss: 0.028238, acc.: 99.22%] [G loss: 0.004350]\n",
      "epoch:26 step:20662 [D loss: 0.002696, acc.: 100.00%] [G loss: 0.008076]\n",
      "epoch:26 step:20663 [D loss: 0.003172, acc.: 100.00%] [G loss: 0.030024]\n",
      "epoch:26 step:20664 [D loss: 0.001346, acc.: 100.00%] [G loss: 0.033073]\n",
      "epoch:26 step:20665 [D loss: 0.000503, acc.: 100.00%] [G loss: 2.938424]\n",
      "epoch:26 step:20666 [D loss: 0.009783, acc.: 100.00%] [G loss: 0.047267]\n",
      "epoch:26 step:20667 [D loss: 0.028278, acc.: 100.00%] [G loss: 0.009098]\n",
      "epoch:26 step:20668 [D loss: 0.006211, acc.: 100.00%] [G loss: 0.059492]\n",
      "epoch:26 step:20669 [D loss: 0.003080, acc.: 100.00%] [G loss: 4.531894]\n",
      "epoch:26 step:20670 [D loss: 0.004307, acc.: 100.00%] [G loss: 0.280174]\n",
      "epoch:26 step:20671 [D loss: 0.017637, acc.: 99.22%] [G loss: 0.022463]\n",
      "epoch:26 step:20672 [D loss: 0.002137, acc.: 100.00%] [G loss: 0.111656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20673 [D loss: 0.009927, acc.: 100.00%] [G loss: 0.325124]\n",
      "epoch:26 step:20674 [D loss: 0.026476, acc.: 100.00%] [G loss: 0.042591]\n",
      "epoch:26 step:20675 [D loss: 0.022098, acc.: 100.00%] [G loss: 0.079138]\n",
      "epoch:26 step:20676 [D loss: 0.016724, acc.: 100.00%] [G loss: 0.023404]\n",
      "epoch:26 step:20677 [D loss: 0.002249, acc.: 100.00%] [G loss: 0.131405]\n",
      "epoch:26 step:20678 [D loss: 0.013340, acc.: 100.00%] [G loss: 0.015049]\n",
      "epoch:26 step:20679 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.227884]\n",
      "epoch:26 step:20680 [D loss: 0.011503, acc.: 100.00%] [G loss: 0.165389]\n",
      "epoch:26 step:20681 [D loss: 0.055293, acc.: 97.66%] [G loss: 0.000149]\n",
      "epoch:26 step:20682 [D loss: 0.004363, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:26 step:20683 [D loss: 0.087829, acc.: 98.44%] [G loss: 0.014283]\n",
      "epoch:26 step:20684 [D loss: 0.001360, acc.: 100.00%] [G loss: 0.474004]\n",
      "epoch:26 step:20685 [D loss: 0.003049, acc.: 100.00%] [G loss: 1.961036]\n",
      "epoch:26 step:20686 [D loss: 0.006102, acc.: 100.00%] [G loss: 0.165600]\n",
      "epoch:26 step:20687 [D loss: 0.008416, acc.: 100.00%] [G loss: 0.011377]\n",
      "epoch:26 step:20688 [D loss: 0.003001, acc.: 100.00%] [G loss: 0.004951]\n",
      "epoch:26 step:20689 [D loss: 0.002405, acc.: 100.00%] [G loss: 0.003810]\n",
      "epoch:26 step:20690 [D loss: 0.000975, acc.: 100.00%] [G loss: 0.001312]\n",
      "epoch:26 step:20691 [D loss: 0.005115, acc.: 100.00%] [G loss: 0.002416]\n",
      "epoch:26 step:20692 [D loss: 0.046020, acc.: 98.44%] [G loss: 0.001404]\n",
      "epoch:26 step:20693 [D loss: 0.012556, acc.: 100.00%] [G loss: 0.000317]\n",
      "epoch:26 step:20694 [D loss: 0.227784, acc.: 91.41%] [G loss: 0.053339]\n",
      "epoch:26 step:20695 [D loss: 0.014244, acc.: 100.00%] [G loss: 0.317487]\n",
      "epoch:26 step:20696 [D loss: 0.011032, acc.: 100.00%] [G loss: 0.416947]\n",
      "epoch:26 step:20697 [D loss: 0.035368, acc.: 100.00%] [G loss: 1.470287]\n",
      "epoch:26 step:20698 [D loss: 0.006257, acc.: 100.00%] [G loss: 1.719523]\n",
      "epoch:26 step:20699 [D loss: 0.019179, acc.: 100.00%] [G loss: 2.751307]\n",
      "epoch:26 step:20700 [D loss: 0.049040, acc.: 100.00%] [G loss: 6.693766]\n",
      "epoch:26 step:20701 [D loss: 0.688372, acc.: 62.50%] [G loss: 10.456828]\n",
      "epoch:26 step:20702 [D loss: 0.057457, acc.: 97.66%] [G loss: 10.609818]\n",
      "epoch:26 step:20703 [D loss: 0.149015, acc.: 91.41%] [G loss: 9.677282]\n",
      "epoch:26 step:20704 [D loss: 0.010011, acc.: 100.00%] [G loss: 8.308084]\n",
      "epoch:26 step:20705 [D loss: 0.000770, acc.: 100.00%] [G loss: 8.081059]\n",
      "epoch:26 step:20706 [D loss: 0.001706, acc.: 100.00%] [G loss: 7.026366]\n",
      "epoch:26 step:20707 [D loss: 0.008643, acc.: 100.00%] [G loss: 6.507926]\n",
      "epoch:26 step:20708 [D loss: 0.007445, acc.: 100.00%] [G loss: 0.499320]\n",
      "epoch:26 step:20709 [D loss: 0.004168, acc.: 100.00%] [G loss: 5.800560]\n",
      "epoch:26 step:20710 [D loss: 0.014546, acc.: 100.00%] [G loss: 5.483981]\n",
      "epoch:26 step:20711 [D loss: 0.006436, acc.: 100.00%] [G loss: 0.072854]\n",
      "epoch:26 step:20712 [D loss: 0.186844, acc.: 92.19%] [G loss: 0.959200]\n",
      "epoch:26 step:20713 [D loss: 0.016853, acc.: 99.22%] [G loss: 5.917464]\n",
      "epoch:26 step:20714 [D loss: 0.200532, acc.: 93.75%] [G loss: 2.654680]\n",
      "epoch:26 step:20715 [D loss: 0.002621, acc.: 100.00%] [G loss: 0.919435]\n",
      "epoch:26 step:20716 [D loss: 0.004425, acc.: 100.00%] [G loss: 1.405808]\n",
      "epoch:26 step:20717 [D loss: 0.016345, acc.: 100.00%] [G loss: 0.488684]\n",
      "epoch:26 step:20718 [D loss: 0.021614, acc.: 99.22%] [G loss: 0.004471]\n",
      "epoch:26 step:20719 [D loss: 0.007099, acc.: 100.00%] [G loss: 1.182191]\n",
      "epoch:26 step:20720 [D loss: 0.002705, acc.: 100.00%] [G loss: 0.912600]\n",
      "epoch:26 step:20721 [D loss: 0.003995, acc.: 100.00%] [G loss: 4.883545]\n",
      "epoch:26 step:20722 [D loss: 0.051431, acc.: 100.00%] [G loss: 0.001813]\n",
      "epoch:26 step:20723 [D loss: 0.005237, acc.: 100.00%] [G loss: 2.277723]\n",
      "epoch:26 step:20724 [D loss: 0.001658, acc.: 100.00%] [G loss: 2.279296]\n",
      "epoch:26 step:20725 [D loss: 0.002459, acc.: 100.00%] [G loss: 0.004077]\n",
      "epoch:26 step:20726 [D loss: 0.032735, acc.: 99.22%] [G loss: 1.235882]\n",
      "epoch:26 step:20727 [D loss: 0.001194, acc.: 100.00%] [G loss: 0.146325]\n",
      "epoch:26 step:20728 [D loss: 0.003377, acc.: 100.00%] [G loss: 0.706085]\n",
      "epoch:26 step:20729 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.396962]\n",
      "epoch:26 step:20730 [D loss: 0.020285, acc.: 100.00%] [G loss: 0.000797]\n",
      "epoch:26 step:20731 [D loss: 0.001510, acc.: 100.00%] [G loss: 4.477533]\n",
      "epoch:26 step:20732 [D loss: 0.003531, acc.: 100.00%] [G loss: 0.086429]\n",
      "epoch:26 step:20733 [D loss: 0.118757, acc.: 96.09%] [G loss: 1.558244]\n",
      "epoch:26 step:20734 [D loss: 0.113701, acc.: 96.09%] [G loss: 1.148306]\n",
      "epoch:26 step:20735 [D loss: 0.029852, acc.: 98.44%] [G loss: 0.337325]\n",
      "epoch:26 step:20736 [D loss: 0.013366, acc.: 100.00%] [G loss: 0.370011]\n",
      "epoch:26 step:20737 [D loss: 0.030881, acc.: 100.00%] [G loss: 0.218318]\n",
      "epoch:26 step:20738 [D loss: 0.003802, acc.: 100.00%] [G loss: 0.150830]\n",
      "epoch:26 step:20739 [D loss: 0.000767, acc.: 100.00%] [G loss: 0.037786]\n",
      "epoch:26 step:20740 [D loss: 0.002204, acc.: 100.00%] [G loss: 0.048946]\n",
      "epoch:26 step:20741 [D loss: 0.003683, acc.: 100.00%] [G loss: 0.025696]\n",
      "epoch:26 step:20742 [D loss: 0.005901, acc.: 100.00%] [G loss: 0.030889]\n",
      "epoch:26 step:20743 [D loss: 0.002095, acc.: 100.00%] [G loss: 0.012446]\n",
      "epoch:26 step:20744 [D loss: 0.001313, acc.: 100.00%] [G loss: 0.056987]\n",
      "epoch:26 step:20745 [D loss: 0.001243, acc.: 100.00%] [G loss: 0.025197]\n",
      "epoch:26 step:20746 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.039176]\n",
      "epoch:26 step:20747 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.051717]\n",
      "epoch:26 step:20748 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.029795]\n",
      "epoch:26 step:20749 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.002710]\n",
      "epoch:26 step:20750 [D loss: 0.002967, acc.: 100.00%] [G loss: 0.058483]\n",
      "epoch:26 step:20751 [D loss: 0.000572, acc.: 100.00%] [G loss: 2.335393]\n",
      "epoch:26 step:20752 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.035188]\n",
      "epoch:26 step:20753 [D loss: 0.011286, acc.: 100.00%] [G loss: 0.016722]\n",
      "epoch:26 step:20754 [D loss: 0.005200, acc.: 100.00%] [G loss: 0.026546]\n",
      "epoch:26 step:20755 [D loss: 0.004655, acc.: 100.00%] [G loss: 0.003048]\n",
      "epoch:26 step:20756 [D loss: 0.002623, acc.: 100.00%] [G loss: 0.052778]\n",
      "epoch:26 step:20757 [D loss: 0.001328, acc.: 100.00%] [G loss: 0.264668]\n",
      "epoch:26 step:20758 [D loss: 0.032711, acc.: 100.00%] [G loss: 0.467999]\n",
      "epoch:26 step:20759 [D loss: 0.444576, acc.: 77.34%] [G loss: 9.314369]\n",
      "epoch:26 step:20760 [D loss: 2.658456, acc.: 50.78%] [G loss: 6.437728]\n",
      "epoch:26 step:20761 [D loss: 0.063244, acc.: 97.66%] [G loss: 2.837158]\n",
      "epoch:26 step:20762 [D loss: 0.047173, acc.: 100.00%] [G loss: 5.960164]\n",
      "epoch:26 step:20763 [D loss: 0.016897, acc.: 100.00%] [G loss: 0.851880]\n",
      "epoch:26 step:20764 [D loss: 0.038333, acc.: 99.22%] [G loss: 0.908909]\n",
      "epoch:26 step:20765 [D loss: 0.077633, acc.: 98.44%] [G loss: 5.215817]\n",
      "epoch:26 step:20766 [D loss: 0.039502, acc.: 99.22%] [G loss: 4.765681]\n",
      "epoch:26 step:20767 [D loss: 0.006994, acc.: 100.00%] [G loss: 0.003534]\n",
      "epoch:26 step:20768 [D loss: 0.087241, acc.: 99.22%] [G loss: 0.114453]\n",
      "epoch:26 step:20769 [D loss: 0.142849, acc.: 96.09%] [G loss: 5.941521]\n",
      "epoch:26 step:20770 [D loss: 0.233024, acc.: 89.06%] [G loss: 0.377036]\n",
      "epoch:26 step:20771 [D loss: 0.018077, acc.: 99.22%] [G loss: 0.495508]\n",
      "epoch:26 step:20772 [D loss: 0.021287, acc.: 100.00%] [G loss: 0.566717]\n",
      "epoch:26 step:20773 [D loss: 0.024229, acc.: 100.00%] [G loss: 4.558885]\n",
      "epoch:26 step:20774 [D loss: 0.087083, acc.: 97.66%] [G loss: 3.781927]\n",
      "epoch:26 step:20775 [D loss: 0.003931, acc.: 100.00%] [G loss: 4.675776]\n",
      "epoch:26 step:20776 [D loss: 0.423661, acc.: 85.94%] [G loss: 3.467659]\n",
      "epoch:26 step:20777 [D loss: 0.025441, acc.: 100.00%] [G loss: 2.531961]\n",
      "epoch:26 step:20778 [D loss: 0.089810, acc.: 95.31%] [G loss: 0.509819]\n",
      "epoch:26 step:20779 [D loss: 0.004018, acc.: 100.00%] [G loss: 3.574072]\n",
      "epoch:26 step:20780 [D loss: 0.005544, acc.: 100.00%] [G loss: 0.321673]\n",
      "epoch:26 step:20781 [D loss: 0.002549, acc.: 100.00%] [G loss: 2.695649]\n",
      "epoch:26 step:20782 [D loss: 0.010810, acc.: 100.00%] [G loss: 0.147354]\n",
      "epoch:26 step:20783 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.173766]\n",
      "epoch:26 step:20784 [D loss: 0.001576, acc.: 100.00%] [G loss: 0.287981]\n",
      "epoch:26 step:20785 [D loss: 0.001954, acc.: 100.00%] [G loss: 0.094365]\n",
      "epoch:26 step:20786 [D loss: 0.001277, acc.: 100.00%] [G loss: 2.260611]\n",
      "epoch:26 step:20787 [D loss: 0.002092, acc.: 100.00%] [G loss: 0.668932]\n",
      "epoch:26 step:20788 [D loss: 0.000755, acc.: 100.00%] [G loss: 0.041321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20789 [D loss: 0.000880, acc.: 100.00%] [G loss: 0.015115]\n",
      "epoch:26 step:20790 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.019023]\n",
      "epoch:26 step:20791 [D loss: 0.000931, acc.: 100.00%] [G loss: 0.276199]\n",
      "epoch:26 step:20792 [D loss: 0.001228, acc.: 100.00%] [G loss: 0.252545]\n",
      "epoch:26 step:20793 [D loss: 0.001655, acc.: 100.00%] [G loss: 0.236666]\n",
      "epoch:26 step:20794 [D loss: 0.024044, acc.: 100.00%] [G loss: 0.723366]\n",
      "epoch:26 step:20795 [D loss: 0.002862, acc.: 100.00%] [G loss: 0.275003]\n",
      "epoch:26 step:20796 [D loss: 0.011391, acc.: 100.00%] [G loss: 0.060777]\n",
      "epoch:26 step:20797 [D loss: 0.017079, acc.: 100.00%] [G loss: 0.049326]\n",
      "epoch:26 step:20798 [D loss: 0.005684, acc.: 100.00%] [G loss: 0.031540]\n",
      "epoch:26 step:20799 [D loss: 0.002699, acc.: 100.00%] [G loss: 0.031062]\n",
      "epoch:26 step:20800 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.147514]\n",
      "epoch:26 step:20801 [D loss: 0.005430, acc.: 100.00%] [G loss: 0.008944]\n",
      "epoch:26 step:20802 [D loss: 0.002485, acc.: 100.00%] [G loss: 0.161150]\n",
      "epoch:26 step:20803 [D loss: 0.002369, acc.: 100.00%] [G loss: 0.014492]\n",
      "epoch:26 step:20804 [D loss: 0.004176, acc.: 100.00%] [G loss: 0.015063]\n",
      "epoch:26 step:20805 [D loss: 0.002372, acc.: 100.00%] [G loss: 0.014266]\n",
      "epoch:26 step:20806 [D loss: 0.000655, acc.: 100.00%] [G loss: 0.085390]\n",
      "epoch:26 step:20807 [D loss: 0.020085, acc.: 99.22%] [G loss: 0.066827]\n",
      "epoch:26 step:20808 [D loss: 0.006682, acc.: 100.00%] [G loss: 0.012234]\n",
      "epoch:26 step:20809 [D loss: 0.001374, acc.: 100.00%] [G loss: 0.004485]\n",
      "epoch:26 step:20810 [D loss: 0.003077, acc.: 100.00%] [G loss: 0.091740]\n",
      "epoch:26 step:20811 [D loss: 0.003583, acc.: 100.00%] [G loss: 0.033753]\n",
      "epoch:26 step:20812 [D loss: 0.002037, acc.: 100.00%] [G loss: 0.005916]\n",
      "epoch:26 step:20813 [D loss: 0.002919, acc.: 100.00%] [G loss: 0.011856]\n",
      "epoch:26 step:20814 [D loss: 0.000698, acc.: 100.00%] [G loss: 0.005277]\n",
      "epoch:26 step:20815 [D loss: 0.002267, acc.: 100.00%] [G loss: 0.025163]\n",
      "epoch:26 step:20816 [D loss: 0.002047, acc.: 100.00%] [G loss: 0.494521]\n",
      "epoch:26 step:20817 [D loss: 0.009818, acc.: 100.00%] [G loss: 0.104818]\n",
      "epoch:26 step:20818 [D loss: 0.136068, acc.: 96.09%] [G loss: 0.284174]\n",
      "epoch:26 step:20819 [D loss: 0.198612, acc.: 91.41%] [G loss: 0.234342]\n",
      "epoch:26 step:20820 [D loss: 0.008118, acc.: 100.00%] [G loss: 0.865125]\n",
      "epoch:26 step:20821 [D loss: 0.007287, acc.: 100.00%] [G loss: 2.680062]\n",
      "epoch:26 step:20822 [D loss: 0.771159, acc.: 68.75%] [G loss: 4.907673]\n",
      "epoch:26 step:20823 [D loss: 1.402907, acc.: 57.81%] [G loss: 3.193395]\n",
      "epoch:26 step:20824 [D loss: 0.034881, acc.: 99.22%] [G loss: 1.153204]\n",
      "epoch:26 step:20825 [D loss: 0.042725, acc.: 99.22%] [G loss: 0.944122]\n",
      "epoch:26 step:20826 [D loss: 0.020362, acc.: 100.00%] [G loss: 0.785023]\n",
      "epoch:26 step:20827 [D loss: 0.041650, acc.: 99.22%] [G loss: 0.316529]\n",
      "epoch:26 step:20828 [D loss: 0.010939, acc.: 100.00%] [G loss: 0.461476]\n",
      "epoch:26 step:20829 [D loss: 0.035913, acc.: 100.00%] [G loss: 1.674107]\n",
      "epoch:26 step:20830 [D loss: 0.016677, acc.: 100.00%] [G loss: 0.783238]\n",
      "epoch:26 step:20831 [D loss: 0.012640, acc.: 100.00%] [G loss: 2.234792]\n",
      "epoch:26 step:20832 [D loss: 0.689464, acc.: 69.53%] [G loss: 7.184569]\n",
      "epoch:26 step:20833 [D loss: 0.785304, acc.: 60.94%] [G loss: 1.935452]\n",
      "epoch:26 step:20834 [D loss: 0.349403, acc.: 82.81%] [G loss: 3.704339]\n",
      "epoch:26 step:20835 [D loss: 0.030627, acc.: 98.44%] [G loss: 3.730531]\n",
      "epoch:26 step:20836 [D loss: 0.305335, acc.: 85.16%] [G loss: 0.695142]\n",
      "epoch:26 step:20837 [D loss: 0.003310, acc.: 100.00%] [G loss: 3.641860]\n",
      "epoch:26 step:20838 [D loss: 0.017649, acc.: 100.00%] [G loss: 2.508154]\n",
      "epoch:26 step:20839 [D loss: 0.024193, acc.: 100.00%] [G loss: 0.442323]\n",
      "epoch:26 step:20840 [D loss: 0.026252, acc.: 99.22%] [G loss: 2.084438]\n",
      "epoch:26 step:20841 [D loss: 0.019846, acc.: 100.00%] [G loss: 1.446020]\n",
      "epoch:26 step:20842 [D loss: 0.025417, acc.: 100.00%] [G loss: 2.110097]\n",
      "epoch:26 step:20843 [D loss: 0.235795, acc.: 91.41%] [G loss: 0.152590]\n",
      "epoch:26 step:20844 [D loss: 0.029018, acc.: 98.44%] [G loss: 3.756451]\n",
      "epoch:26 step:20845 [D loss: 0.019999, acc.: 100.00%] [G loss: 0.073425]\n",
      "epoch:26 step:20846 [D loss: 0.010665, acc.: 100.00%] [G loss: 3.396930]\n",
      "epoch:26 step:20847 [D loss: 0.002555, acc.: 100.00%] [G loss: 2.934225]\n",
      "epoch:26 step:20848 [D loss: 0.218666, acc.: 94.53%] [G loss: 2.320313]\n",
      "epoch:26 step:20849 [D loss: 0.040555, acc.: 100.00%] [G loss: 0.166759]\n",
      "epoch:26 step:20850 [D loss: 0.060089, acc.: 99.22%] [G loss: 0.112902]\n",
      "epoch:26 step:20851 [D loss: 0.045602, acc.: 100.00%] [G loss: 2.598881]\n",
      "epoch:26 step:20852 [D loss: 0.022588, acc.: 100.00%] [G loss: 0.025470]\n",
      "epoch:26 step:20853 [D loss: 0.041282, acc.: 98.44%] [G loss: 0.033228]\n",
      "epoch:26 step:20854 [D loss: 0.013263, acc.: 100.00%] [G loss: 0.212461]\n",
      "epoch:26 step:20855 [D loss: 0.002290, acc.: 100.00%] [G loss: 0.047779]\n",
      "epoch:26 step:20856 [D loss: 0.027736, acc.: 99.22%] [G loss: 0.008399]\n",
      "epoch:26 step:20857 [D loss: 0.002105, acc.: 100.00%] [G loss: 1.568091]\n",
      "epoch:26 step:20858 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.039380]\n",
      "epoch:26 step:20859 [D loss: 0.001295, acc.: 100.00%] [G loss: 0.001799]\n",
      "epoch:26 step:20860 [D loss: 0.000604, acc.: 100.00%] [G loss: 1.094312]\n",
      "epoch:26 step:20861 [D loss: 0.001064, acc.: 100.00%] [G loss: 0.004593]\n",
      "epoch:26 step:20862 [D loss: 0.003889, acc.: 100.00%] [G loss: 0.001456]\n",
      "epoch:26 step:20863 [D loss: 0.001084, acc.: 100.00%] [G loss: 0.159347]\n",
      "epoch:26 step:20864 [D loss: 0.005632, acc.: 100.00%] [G loss: 0.000378]\n",
      "epoch:26 step:20865 [D loss: 0.001527, acc.: 100.00%] [G loss: 0.367960]\n",
      "epoch:26 step:20866 [D loss: 0.006866, acc.: 100.00%] [G loss: 0.061574]\n",
      "epoch:26 step:20867 [D loss: 0.002748, acc.: 100.00%] [G loss: 0.001229]\n",
      "epoch:26 step:20868 [D loss: 0.007511, acc.: 100.00%] [G loss: 0.001659]\n",
      "epoch:26 step:20869 [D loss: 0.001415, acc.: 100.00%] [G loss: 0.001414]\n",
      "epoch:26 step:20870 [D loss: 0.005508, acc.: 100.00%] [G loss: 0.002607]\n",
      "epoch:26 step:20871 [D loss: 0.001910, acc.: 100.00%] [G loss: 0.005971]\n",
      "epoch:26 step:20872 [D loss: 0.007733, acc.: 99.22%] [G loss: 0.000980]\n",
      "epoch:26 step:20873 [D loss: 0.000728, acc.: 100.00%] [G loss: 0.001036]\n",
      "epoch:26 step:20874 [D loss: 0.000612, acc.: 100.00%] [G loss: 0.024935]\n",
      "epoch:26 step:20875 [D loss: 0.001036, acc.: 100.00%] [G loss: 0.005733]\n",
      "epoch:26 step:20876 [D loss: 0.001012, acc.: 100.00%] [G loss: 0.013298]\n",
      "epoch:26 step:20877 [D loss: 0.010429, acc.: 100.00%] [G loss: 0.003812]\n",
      "epoch:26 step:20878 [D loss: 0.002566, acc.: 100.00%] [G loss: 0.141024]\n",
      "epoch:26 step:20879 [D loss: 0.006614, acc.: 100.00%] [G loss: 0.001757]\n",
      "epoch:26 step:20880 [D loss: 0.001744, acc.: 100.00%] [G loss: 0.003765]\n",
      "epoch:26 step:20881 [D loss: 0.007799, acc.: 100.00%] [G loss: 0.002133]\n",
      "epoch:26 step:20882 [D loss: 0.000660, acc.: 100.00%] [G loss: 0.001228]\n",
      "epoch:26 step:20883 [D loss: 0.003062, acc.: 100.00%] [G loss: 0.051638]\n",
      "epoch:26 step:20884 [D loss: 0.001497, acc.: 100.00%] [G loss: 0.015280]\n",
      "epoch:26 step:20885 [D loss: 0.006309, acc.: 100.00%] [G loss: 0.001693]\n",
      "epoch:26 step:20886 [D loss: 0.004899, acc.: 100.00%] [G loss: 0.011131]\n",
      "epoch:26 step:20887 [D loss: 0.002064, acc.: 100.00%] [G loss: 0.011912]\n",
      "epoch:26 step:20888 [D loss: 0.004111, acc.: 100.00%] [G loss: 0.000969]\n",
      "epoch:26 step:20889 [D loss: 0.006613, acc.: 100.00%] [G loss: 0.522988]\n",
      "epoch:26 step:20890 [D loss: 0.027872, acc.: 99.22%] [G loss: 0.844690]\n",
      "epoch:26 step:20891 [D loss: 0.009397, acc.: 100.00%] [G loss: 0.684359]\n",
      "epoch:26 step:20892 [D loss: 0.013361, acc.: 99.22%] [G loss: 0.146826]\n",
      "epoch:26 step:20893 [D loss: 0.546687, acc.: 70.31%] [G loss: 9.056994]\n",
      "epoch:26 step:20894 [D loss: 1.726032, acc.: 52.34%] [G loss: 6.251456]\n",
      "epoch:26 step:20895 [D loss: 0.213997, acc.: 94.53%] [G loss: 3.088738]\n",
      "epoch:26 step:20896 [D loss: 0.130889, acc.: 95.31%] [G loss: 0.261349]\n",
      "epoch:26 step:20897 [D loss: 0.024282, acc.: 100.00%] [G loss: 0.188577]\n",
      "epoch:26 step:20898 [D loss: 0.015791, acc.: 100.00%] [G loss: 0.030085]\n",
      "epoch:26 step:20899 [D loss: 0.029798, acc.: 100.00%] [G loss: 0.007531]\n",
      "epoch:26 step:20900 [D loss: 0.001822, acc.: 100.00%] [G loss: 0.010116]\n",
      "epoch:26 step:20901 [D loss: 0.002517, acc.: 100.00%] [G loss: 0.001608]\n",
      "epoch:26 step:20902 [D loss: 0.004663, acc.: 100.00%] [G loss: 0.007134]\n",
      "epoch:26 step:20903 [D loss: 0.001665, acc.: 100.00%] [G loss: 2.808206]\n",
      "epoch:26 step:20904 [D loss: 0.004007, acc.: 100.00%] [G loss: 0.005893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:20905 [D loss: 0.002043, acc.: 100.00%] [G loss: 0.002133]\n",
      "epoch:26 step:20906 [D loss: 0.001265, acc.: 100.00%] [G loss: 1.856089]\n",
      "epoch:26 step:20907 [D loss: 0.011465, acc.: 100.00%] [G loss: 0.001116]\n",
      "epoch:26 step:20908 [D loss: 0.002107, acc.: 100.00%] [G loss: 1.051165]\n",
      "epoch:26 step:20909 [D loss: 0.001089, acc.: 100.00%] [G loss: 0.875366]\n",
      "epoch:26 step:20910 [D loss: 0.235910, acc.: 89.06%] [G loss: 0.049966]\n",
      "epoch:26 step:20911 [D loss: 0.173967, acc.: 92.19%] [G loss: 0.079585]\n",
      "epoch:26 step:20912 [D loss: 0.116336, acc.: 96.09%] [G loss: 2.566386]\n",
      "epoch:26 step:20913 [D loss: 0.020145, acc.: 100.00%] [G loss: 1.405195]\n",
      "epoch:26 step:20914 [D loss: 0.065738, acc.: 99.22%] [G loss: 0.004387]\n",
      "epoch:26 step:20915 [D loss: 0.001122, acc.: 100.00%] [G loss: 0.001168]\n",
      "epoch:26 step:20916 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.002793]\n",
      "epoch:26 step:20917 [D loss: 0.000801, acc.: 100.00%] [G loss: 0.006789]\n",
      "epoch:26 step:20918 [D loss: 0.000780, acc.: 100.00%] [G loss: 0.004391]\n",
      "epoch:26 step:20919 [D loss: 0.086586, acc.: 96.88%] [G loss: 1.547673]\n",
      "epoch:26 step:20920 [D loss: 0.009431, acc.: 100.00%] [G loss: 0.000673]\n",
      "epoch:26 step:20921 [D loss: 0.002288, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:26 step:20922 [D loss: 0.005500, acc.: 100.00%] [G loss: 0.141760]\n",
      "epoch:26 step:20923 [D loss: 0.026483, acc.: 99.22%] [G loss: 0.363502]\n",
      "epoch:26 step:20924 [D loss: 0.037505, acc.: 99.22%] [G loss: 0.805198]\n",
      "epoch:26 step:20925 [D loss: 0.001656, acc.: 100.00%] [G loss: 2.562343]\n",
      "epoch:26 step:20926 [D loss: 0.024741, acc.: 100.00%] [G loss: 2.469256]\n",
      "epoch:26 step:20927 [D loss: 0.014127, acc.: 100.00%] [G loss: 2.223314]\n",
      "epoch:26 step:20928 [D loss: 0.106109, acc.: 98.44%] [G loss: 3.796494]\n",
      "epoch:26 step:20929 [D loss: 0.938502, acc.: 53.91%] [G loss: 1.971514]\n",
      "epoch:26 step:20930 [D loss: 0.010107, acc.: 100.00%] [G loss: 1.759610]\n",
      "epoch:26 step:20931 [D loss: 0.093126, acc.: 96.09%] [G loss: 6.804459]\n",
      "epoch:26 step:20932 [D loss: 0.055070, acc.: 97.66%] [G loss: 0.031016]\n",
      "epoch:26 step:20933 [D loss: 0.060131, acc.: 97.66%] [G loss: 0.155813]\n",
      "epoch:26 step:20934 [D loss: 0.012285, acc.: 100.00%] [G loss: 4.076936]\n",
      "epoch:26 step:20935 [D loss: 0.009591, acc.: 100.00%] [G loss: 0.052177]\n",
      "epoch:26 step:20936 [D loss: 0.017641, acc.: 100.00%] [G loss: 0.008548]\n",
      "epoch:26 step:20937 [D loss: 0.025866, acc.: 100.00%] [G loss: 5.186625]\n",
      "epoch:26 step:20938 [D loss: 0.006503, acc.: 100.00%] [G loss: 0.020989]\n",
      "epoch:26 step:20939 [D loss: 0.026902, acc.: 99.22%] [G loss: 4.725102]\n",
      "epoch:26 step:20940 [D loss: 0.007764, acc.: 100.00%] [G loss: 0.001311]\n",
      "epoch:26 step:20941 [D loss: 0.003335, acc.: 100.00%] [G loss: 0.384175]\n",
      "epoch:26 step:20942 [D loss: 0.002643, acc.: 100.00%] [G loss: 3.666011]\n",
      "epoch:26 step:20943 [D loss: 0.003820, acc.: 100.00%] [G loss: 0.000940]\n",
      "epoch:26 step:20944 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.000885]\n",
      "epoch:26 step:20945 [D loss: 0.005311, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:26 step:20946 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:26 step:20947 [D loss: 0.000257, acc.: 100.00%] [G loss: 2.659728]\n",
      "epoch:26 step:20948 [D loss: 0.003365, acc.: 100.00%] [G loss: 0.000287]\n",
      "epoch:26 step:20949 [D loss: 0.001030, acc.: 100.00%] [G loss: 0.004387]\n",
      "epoch:26 step:20950 [D loss: 0.000623, acc.: 100.00%] [G loss: 0.002535]\n",
      "epoch:26 step:20951 [D loss: 0.000472, acc.: 100.00%] [G loss: 0.000763]\n",
      "epoch:26 step:20952 [D loss: 0.018593, acc.: 99.22%] [G loss: 1.671124]\n",
      "epoch:26 step:20953 [D loss: 0.000538, acc.: 100.00%] [G loss: 0.001130]\n",
      "epoch:26 step:20954 [D loss: 0.002249, acc.: 100.00%] [G loss: 0.094021]\n",
      "epoch:26 step:20955 [D loss: 0.003436, acc.: 100.00%] [G loss: 0.000556]\n",
      "epoch:26 step:20956 [D loss: 0.002248, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:26 step:20957 [D loss: 0.000981, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:26 step:20958 [D loss: 0.001237, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:26 step:20959 [D loss: 0.002148, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:26 step:20960 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.000663]\n",
      "epoch:26 step:20961 [D loss: 0.002894, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:26 step:20962 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.106269]\n",
      "epoch:26 step:20963 [D loss: 0.002857, acc.: 100.00%] [G loss: 0.004860]\n",
      "epoch:26 step:20964 [D loss: 0.003767, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:26 step:20965 [D loss: 0.049835, acc.: 99.22%] [G loss: 0.008494]\n",
      "epoch:26 step:20966 [D loss: 0.001468, acc.: 100.00%] [G loss: 0.001626]\n",
      "epoch:26 step:20967 [D loss: 0.034756, acc.: 99.22%] [G loss: 1.001036]\n",
      "epoch:26 step:20968 [D loss: 0.006031, acc.: 100.00%] [G loss: 0.011638]\n",
      "epoch:26 step:20969 [D loss: 0.002238, acc.: 100.00%] [G loss: 0.155679]\n",
      "epoch:26 step:20970 [D loss: 0.000789, acc.: 100.00%] [G loss: 0.006309]\n",
      "epoch:26 step:20971 [D loss: 0.080205, acc.: 96.88%] [G loss: 0.006475]\n",
      "epoch:26 step:20972 [D loss: 0.001501, acc.: 100.00%] [G loss: 0.019996]\n",
      "epoch:26 step:20973 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:26 step:20974 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:26 step:20975 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:26 step:20976 [D loss: 0.001176, acc.: 100.00%] [G loss: 0.038463]\n",
      "epoch:26 step:20977 [D loss: 0.001578, acc.: 100.00%] [G loss: 0.000761]\n",
      "epoch:26 step:20978 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:26 step:20979 [D loss: 0.006845, acc.: 100.00%] [G loss: 0.000618]\n",
      "epoch:26 step:20980 [D loss: 0.003481, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:26 step:20981 [D loss: 0.001727, acc.: 100.00%] [G loss: 0.010604]\n",
      "epoch:26 step:20982 [D loss: 0.001224, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:26 step:20983 [D loss: 0.020713, acc.: 100.00%] [G loss: 0.336146]\n",
      "epoch:26 step:20984 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.000681]\n",
      "epoch:26 step:20985 [D loss: 0.001292, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:26 step:20986 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.033880]\n",
      "epoch:26 step:20987 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.043247]\n",
      "epoch:26 step:20988 [D loss: 0.062371, acc.: 97.66%] [G loss: 0.012739]\n",
      "epoch:26 step:20989 [D loss: 0.001307, acc.: 100.00%] [G loss: 3.341861]\n",
      "epoch:26 step:20990 [D loss: 0.019726, acc.: 100.00%] [G loss: 0.517473]\n",
      "epoch:26 step:20991 [D loss: 0.112407, acc.: 98.44%] [G loss: 0.053850]\n",
      "epoch:26 step:20992 [D loss: 0.066403, acc.: 97.66%] [G loss: 0.029760]\n",
      "epoch:26 step:20993 [D loss: 0.062206, acc.: 97.66%] [G loss: 0.009799]\n",
      "epoch:26 step:20994 [D loss: 0.001190, acc.: 100.00%] [G loss: 2.766831]\n",
      "epoch:26 step:20995 [D loss: 0.003919, acc.: 100.00%] [G loss: 1.182112]\n",
      "epoch:26 step:20996 [D loss: 0.012232, acc.: 99.22%] [G loss: 1.574392]\n",
      "epoch:26 step:20997 [D loss: 0.029564, acc.: 99.22%] [G loss: 0.268281]\n",
      "epoch:26 step:20998 [D loss: 0.053155, acc.: 97.66%] [G loss: 0.474885]\n",
      "epoch:26 step:20999 [D loss: 0.000920, acc.: 100.00%] [G loss: 0.073858]\n",
      "epoch:26 step:21000 [D loss: 0.078307, acc.: 99.22%] [G loss: 0.139926]\n",
      "epoch:26 step:21001 [D loss: 0.000995, acc.: 100.00%] [G loss: 0.059871]\n",
      "epoch:26 step:21002 [D loss: 0.021209, acc.: 100.00%] [G loss: 0.019235]\n",
      "epoch:26 step:21003 [D loss: 0.024584, acc.: 100.00%] [G loss: 0.000317]\n",
      "epoch:26 step:21004 [D loss: 0.000629, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:26 step:21005 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:26 step:21006 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.001664]\n",
      "epoch:26 step:21007 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:26 step:21008 [D loss: 0.001084, acc.: 100.00%] [G loss: 1.558247]\n",
      "epoch:26 step:21009 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:26 step:21010 [D loss: 0.008168, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:26 step:21011 [D loss: 0.000621, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:26 step:21012 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:26 step:21013 [D loss: 0.002406, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:26 step:21014 [D loss: 0.021752, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:26 step:21015 [D loss: 0.001409, acc.: 100.00%] [G loss: 0.000391]\n",
      "epoch:26 step:21016 [D loss: 0.001377, acc.: 100.00%] [G loss: 0.007826]\n",
      "epoch:26 step:21017 [D loss: 0.007463, acc.: 100.00%] [G loss: 0.001467]\n",
      "epoch:26 step:21018 [D loss: 0.002174, acc.: 100.00%] [G loss: 0.000531]\n",
      "epoch:26 step:21019 [D loss: 0.116378, acc.: 94.53%] [G loss: 0.000006]\n",
      "epoch:26 step:21020 [D loss: 2.269937, acc.: 54.69%] [G loss: 11.577209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:26 step:21021 [D loss: 3.288234, acc.: 50.00%] [G loss: 7.015871]\n",
      "epoch:26 step:21022 [D loss: 0.710378, acc.: 67.97%] [G loss: 3.746911]\n",
      "epoch:26 step:21023 [D loss: 0.166509, acc.: 92.19%] [G loss: 3.068203]\n",
      "epoch:26 step:21024 [D loss: 0.143354, acc.: 97.66%] [G loss: 0.285641]\n",
      "epoch:26 step:21025 [D loss: 0.087904, acc.: 97.66%] [G loss: 0.081758]\n",
      "epoch:26 step:21026 [D loss: 0.080763, acc.: 98.44%] [G loss: 2.984437]\n",
      "epoch:26 step:21027 [D loss: 0.093606, acc.: 97.66%] [G loss: 2.388175]\n",
      "epoch:26 step:21028 [D loss: 0.036733, acc.: 100.00%] [G loss: 2.050838]\n",
      "epoch:26 step:21029 [D loss: 0.023148, acc.: 100.00%] [G loss: 1.246496]\n",
      "epoch:26 step:21030 [D loss: 0.035313, acc.: 100.00%] [G loss: 0.518151]\n",
      "epoch:26 step:21031 [D loss: 0.061685, acc.: 99.22%] [G loss: 0.014133]\n",
      "epoch:26 step:21032 [D loss: 0.154292, acc.: 95.31%] [G loss: 0.132052]\n",
      "epoch:26 step:21033 [D loss: 0.105556, acc.: 97.66%] [G loss: 0.076086]\n",
      "epoch:26 step:21034 [D loss: 0.060321, acc.: 100.00%] [G loss: 0.075516]\n",
      "epoch:26 step:21035 [D loss: 0.070307, acc.: 100.00%] [G loss: 2.557546]\n",
      "epoch:26 step:21036 [D loss: 0.104341, acc.: 97.66%] [G loss: 0.679882]\n",
      "epoch:26 step:21037 [D loss: 0.010175, acc.: 100.00%] [G loss: 0.144779]\n",
      "epoch:26 step:21038 [D loss: 0.054653, acc.: 100.00%] [G loss: 0.249733]\n",
      "epoch:26 step:21039 [D loss: 0.022065, acc.: 99.22%] [G loss: 0.050494]\n",
      "epoch:26 step:21040 [D loss: 0.023968, acc.: 100.00%] [G loss: 0.209309]\n",
      "epoch:26 step:21041 [D loss: 0.082114, acc.: 99.22%] [G loss: 0.228317]\n",
      "epoch:26 step:21042 [D loss: 0.016437, acc.: 100.00%] [G loss: 0.441487]\n",
      "epoch:26 step:21043 [D loss: 0.043722, acc.: 100.00%] [G loss: 0.237426]\n",
      "epoch:26 step:21044 [D loss: 0.013571, acc.: 100.00%] [G loss: 0.135386]\n",
      "epoch:26 step:21045 [D loss: 0.019929, acc.: 99.22%] [G loss: 0.014080]\n",
      "epoch:26 step:21046 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.007819]\n",
      "epoch:26 step:21047 [D loss: 0.003172, acc.: 100.00%] [G loss: 0.029739]\n",
      "epoch:26 step:21048 [D loss: 0.005786, acc.: 100.00%] [G loss: 0.004973]\n",
      "epoch:26 step:21049 [D loss: 0.003711, acc.: 100.00%] [G loss: 0.019780]\n",
      "epoch:26 step:21050 [D loss: 0.001596, acc.: 100.00%] [G loss: 0.019658]\n",
      "epoch:26 step:21051 [D loss: 0.007533, acc.: 100.00%] [G loss: 0.005840]\n",
      "epoch:26 step:21052 [D loss: 0.051950, acc.: 100.00%] [G loss: 0.003563]\n",
      "epoch:26 step:21053 [D loss: 0.005504, acc.: 100.00%] [G loss: 0.064768]\n",
      "epoch:26 step:21054 [D loss: 0.009756, acc.: 100.00%] [G loss: 0.031593]\n",
      "epoch:26 step:21055 [D loss: 0.004585, acc.: 100.00%] [G loss: 0.029659]\n",
      "epoch:26 step:21056 [D loss: 0.010687, acc.: 100.00%] [G loss: 1.610895]\n",
      "epoch:26 step:21057 [D loss: 0.009733, acc.: 100.00%] [G loss: 0.012365]\n",
      "epoch:26 step:21058 [D loss: 0.018652, acc.: 100.00%] [G loss: 0.015936]\n",
      "epoch:26 step:21059 [D loss: 0.005829, acc.: 100.00%] [G loss: 0.013410]\n",
      "epoch:26 step:21060 [D loss: 0.032181, acc.: 99.22%] [G loss: 0.001472]\n",
      "epoch:26 step:21061 [D loss: 0.008905, acc.: 100.00%] [G loss: 0.002439]\n",
      "epoch:26 step:21062 [D loss: 0.012555, acc.: 100.00%] [G loss: 0.001118]\n",
      "epoch:26 step:21063 [D loss: 0.067465, acc.: 100.00%] [G loss: 0.034549]\n",
      "epoch:26 step:21064 [D loss: 0.004542, acc.: 100.00%] [G loss: 0.763520]\n",
      "epoch:26 step:21065 [D loss: 0.043807, acc.: 99.22%] [G loss: 0.034455]\n",
      "epoch:26 step:21066 [D loss: 0.006183, acc.: 100.00%] [G loss: 0.432479]\n",
      "epoch:26 step:21067 [D loss: 0.013103, acc.: 100.00%] [G loss: 0.008600]\n",
      "epoch:26 step:21068 [D loss: 0.006357, acc.: 100.00%] [G loss: 0.018077]\n",
      "epoch:26 step:21069 [D loss: 0.018296, acc.: 99.22%] [G loss: 0.005665]\n",
      "epoch:26 step:21070 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.001814]\n",
      "epoch:26 step:21071 [D loss: 0.001369, acc.: 100.00%] [G loss: 0.020457]\n",
      "epoch:26 step:21072 [D loss: 0.003792, acc.: 100.00%] [G loss: 0.006855]\n",
      "epoch:26 step:21073 [D loss: 0.001394, acc.: 100.00%] [G loss: 0.013263]\n",
      "epoch:26 step:21074 [D loss: 0.005576, acc.: 100.00%] [G loss: 0.001369]\n",
      "epoch:26 step:21075 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.009697]\n",
      "epoch:26 step:21076 [D loss: 0.000706, acc.: 100.00%] [G loss: 0.002004]\n",
      "epoch:26 step:21077 [D loss: 0.118795, acc.: 95.31%] [G loss: 0.000605]\n",
      "epoch:26 step:21078 [D loss: 0.001823, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:26 step:21079 [D loss: 0.628277, acc.: 72.66%] [G loss: 5.912242]\n",
      "epoch:26 step:21080 [D loss: 1.134545, acc.: 57.03%] [G loss: 3.334486]\n",
      "epoch:26 step:21081 [D loss: 0.073342, acc.: 97.66%] [G loss: 2.678066]\n",
      "epoch:26 step:21082 [D loss: 0.011236, acc.: 100.00%] [G loss: 2.481850]\n",
      "epoch:26 step:21083 [D loss: 0.013489, acc.: 99.22%] [G loss: 0.285582]\n",
      "epoch:26 step:21084 [D loss: 0.011333, acc.: 100.00%] [G loss: 0.160083]\n",
      "epoch:26 step:21085 [D loss: 0.016573, acc.: 100.00%] [G loss: 1.220726]\n",
      "epoch:26 step:21086 [D loss: 0.007005, acc.: 100.00%] [G loss: 0.162317]\n",
      "epoch:26 step:21087 [D loss: 0.005900, acc.: 100.00%] [G loss: 0.389322]\n",
      "epoch:27 step:21088 [D loss: 0.007686, acc.: 100.00%] [G loss: 0.053360]\n",
      "epoch:27 step:21089 [D loss: 0.037884, acc.: 99.22%] [G loss: 0.498085]\n",
      "epoch:27 step:21090 [D loss: 0.013379, acc.: 100.00%] [G loss: 0.143698]\n",
      "epoch:27 step:21091 [D loss: 0.004102, acc.: 100.00%] [G loss: 0.380827]\n",
      "epoch:27 step:21092 [D loss: 0.002202, acc.: 100.00%] [G loss: 0.282931]\n",
      "epoch:27 step:21093 [D loss: 0.001774, acc.: 100.00%] [G loss: 0.019015]\n",
      "epoch:27 step:21094 [D loss: 0.012061, acc.: 100.00%] [G loss: 0.158318]\n",
      "epoch:27 step:21095 [D loss: 0.002571, acc.: 100.00%] [G loss: 0.305867]\n",
      "epoch:27 step:21096 [D loss: 0.003233, acc.: 100.00%] [G loss: 0.010667]\n",
      "epoch:27 step:21097 [D loss: 0.009586, acc.: 100.00%] [G loss: 0.382166]\n",
      "epoch:27 step:21098 [D loss: 0.032361, acc.: 100.00%] [G loss: 0.082277]\n",
      "epoch:27 step:21099 [D loss: 0.001596, acc.: 100.00%] [G loss: 0.045263]\n",
      "epoch:27 step:21100 [D loss: 0.009802, acc.: 100.00%] [G loss: 0.069456]\n",
      "epoch:27 step:21101 [D loss: 0.021932, acc.: 100.00%] [G loss: 0.037614]\n",
      "epoch:27 step:21102 [D loss: 0.009797, acc.: 100.00%] [G loss: 0.061062]\n",
      "epoch:27 step:21103 [D loss: 0.002891, acc.: 100.00%] [G loss: 0.045860]\n",
      "epoch:27 step:21104 [D loss: 0.002965, acc.: 100.00%] [G loss: 0.058590]\n",
      "epoch:27 step:21105 [D loss: 0.010092, acc.: 100.00%] [G loss: 0.025302]\n",
      "epoch:27 step:21106 [D loss: 0.007645, acc.: 100.00%] [G loss: 0.082345]\n",
      "epoch:27 step:21107 [D loss: 0.001973, acc.: 100.00%] [G loss: 0.010972]\n",
      "epoch:27 step:21108 [D loss: 0.007514, acc.: 100.00%] [G loss: 0.061959]\n",
      "epoch:27 step:21109 [D loss: 0.001910, acc.: 100.00%] [G loss: 0.013243]\n",
      "epoch:27 step:21110 [D loss: 0.003488, acc.: 100.00%] [G loss: 0.015415]\n",
      "epoch:27 step:21111 [D loss: 0.004192, acc.: 100.00%] [G loss: 0.043296]\n",
      "epoch:27 step:21112 [D loss: 0.009188, acc.: 99.22%] [G loss: 0.017014]\n",
      "epoch:27 step:21113 [D loss: 0.002210, acc.: 100.00%] [G loss: 0.027757]\n",
      "epoch:27 step:21114 [D loss: 0.001036, acc.: 100.00%] [G loss: 0.013909]\n",
      "epoch:27 step:21115 [D loss: 0.007119, acc.: 100.00%] [G loss: 0.008406]\n",
      "epoch:27 step:21116 [D loss: 0.011253, acc.: 99.22%] [G loss: 0.018798]\n",
      "epoch:27 step:21117 [D loss: 0.002896, acc.: 100.00%] [G loss: 0.162148]\n",
      "epoch:27 step:21118 [D loss: 0.001850, acc.: 100.00%] [G loss: 0.010251]\n",
      "epoch:27 step:21119 [D loss: 0.001405, acc.: 100.00%] [G loss: 0.002006]\n",
      "epoch:27 step:21120 [D loss: 0.004712, acc.: 100.00%] [G loss: 0.001409]\n",
      "epoch:27 step:21121 [D loss: 0.002156, acc.: 100.00%] [G loss: 0.004167]\n",
      "epoch:27 step:21122 [D loss: 0.004543, acc.: 100.00%] [G loss: 0.002393]\n",
      "epoch:27 step:21123 [D loss: 0.002814, acc.: 100.00%] [G loss: 0.002634]\n",
      "epoch:27 step:21124 [D loss: 0.009533, acc.: 100.00%] [G loss: 0.002024]\n",
      "epoch:27 step:21125 [D loss: 0.008542, acc.: 100.00%] [G loss: 0.005635]\n",
      "epoch:27 step:21126 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.006446]\n",
      "epoch:27 step:21127 [D loss: 0.018291, acc.: 100.00%] [G loss: 0.003095]\n",
      "epoch:27 step:21128 [D loss: 0.002402, acc.: 100.00%] [G loss: 0.038523]\n",
      "epoch:27 step:21129 [D loss: 0.002207, acc.: 100.00%] [G loss: 0.012560]\n",
      "epoch:27 step:21130 [D loss: 0.010939, acc.: 100.00%] [G loss: 0.014234]\n",
      "epoch:27 step:21131 [D loss: 0.472918, acc.: 75.78%] [G loss: 3.743834]\n",
      "epoch:27 step:21132 [D loss: 0.722323, acc.: 64.84%] [G loss: 2.649857]\n",
      "epoch:27 step:21133 [D loss: 0.033932, acc.: 99.22%] [G loss: 1.912570]\n",
      "epoch:27 step:21134 [D loss: 0.024300, acc.: 100.00%] [G loss: 0.732136]\n",
      "epoch:27 step:21135 [D loss: 0.035442, acc.: 98.44%] [G loss: 0.091450]\n",
      "epoch:27 step:21136 [D loss: 0.002012, acc.: 100.00%] [G loss: 1.036008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21137 [D loss: 0.027073, acc.: 99.22%] [G loss: 0.143759]\n",
      "epoch:27 step:21138 [D loss: 0.004196, acc.: 100.00%] [G loss: 0.052074]\n",
      "epoch:27 step:21139 [D loss: 0.007049, acc.: 100.00%] [G loss: 0.022492]\n",
      "epoch:27 step:21140 [D loss: 0.023467, acc.: 100.00%] [G loss: 1.366714]\n",
      "epoch:27 step:21141 [D loss: 0.042925, acc.: 100.00%] [G loss: 1.637003]\n",
      "epoch:27 step:21142 [D loss: 0.042268, acc.: 99.22%] [G loss: 0.053389]\n",
      "epoch:27 step:21143 [D loss: 0.089386, acc.: 99.22%] [G loss: 0.031840]\n",
      "epoch:27 step:21144 [D loss: 0.017681, acc.: 100.00%] [G loss: 0.680124]\n",
      "epoch:27 step:21145 [D loss: 0.015349, acc.: 100.00%] [G loss: 0.579898]\n",
      "epoch:27 step:21146 [D loss: 0.006974, acc.: 100.00%] [G loss: 0.302038]\n",
      "epoch:27 step:21147 [D loss: 0.052980, acc.: 98.44%] [G loss: 0.593710]\n",
      "epoch:27 step:21148 [D loss: 0.022525, acc.: 98.44%] [G loss: 0.129960]\n",
      "epoch:27 step:21149 [D loss: 0.008167, acc.: 100.00%] [G loss: 0.113673]\n",
      "epoch:27 step:21150 [D loss: 0.008460, acc.: 100.00%] [G loss: 0.100772]\n",
      "epoch:27 step:21151 [D loss: 0.036632, acc.: 100.00%] [G loss: 0.386203]\n",
      "epoch:27 step:21152 [D loss: 0.549382, acc.: 66.41%] [G loss: 0.975284]\n",
      "epoch:27 step:21153 [D loss: 0.510727, acc.: 73.44%] [G loss: 4.326304]\n",
      "epoch:27 step:21154 [D loss: 0.018948, acc.: 99.22%] [G loss: 2.667888]\n",
      "epoch:27 step:21155 [D loss: 0.040446, acc.: 100.00%] [G loss: 1.988650]\n",
      "epoch:27 step:21156 [D loss: 0.301168, acc.: 84.38%] [G loss: 0.566719]\n",
      "epoch:27 step:21157 [D loss: 0.051089, acc.: 98.44%] [G loss: 1.084273]\n",
      "epoch:27 step:21158 [D loss: 0.821561, acc.: 65.62%] [G loss: 0.494808]\n",
      "epoch:27 step:21159 [D loss: 0.078416, acc.: 98.44%] [G loss: 0.028653]\n",
      "epoch:27 step:21160 [D loss: 0.020814, acc.: 100.00%] [G loss: 3.697631]\n",
      "epoch:27 step:21161 [D loss: 0.043442, acc.: 100.00%] [G loss: 4.023061]\n",
      "epoch:27 step:21162 [D loss: 0.011616, acc.: 100.00%] [G loss: 0.019144]\n",
      "epoch:27 step:21163 [D loss: 0.004948, acc.: 100.00%] [G loss: 0.037500]\n",
      "epoch:27 step:21164 [D loss: 0.037233, acc.: 99.22%] [G loss: 0.078225]\n",
      "epoch:27 step:21165 [D loss: 0.010012, acc.: 100.00%] [G loss: 2.236634]\n",
      "epoch:27 step:21166 [D loss: 0.019347, acc.: 100.00%] [G loss: 0.011432]\n",
      "epoch:27 step:21167 [D loss: 0.032592, acc.: 99.22%] [G loss: 0.017251]\n",
      "epoch:27 step:21168 [D loss: 0.005346, acc.: 100.00%] [G loss: 0.048849]\n",
      "epoch:27 step:21169 [D loss: 0.004095, acc.: 100.00%] [G loss: 0.006153]\n",
      "epoch:27 step:21170 [D loss: 0.009357, acc.: 100.00%] [G loss: 0.005751]\n",
      "epoch:27 step:21171 [D loss: 0.002715, acc.: 100.00%] [G loss: 0.041612]\n",
      "epoch:27 step:21172 [D loss: 0.010999, acc.: 100.00%] [G loss: 0.009124]\n",
      "epoch:27 step:21173 [D loss: 0.018121, acc.: 99.22%] [G loss: 0.007176]\n",
      "epoch:27 step:21174 [D loss: 0.050873, acc.: 100.00%] [G loss: 0.010390]\n",
      "epoch:27 step:21175 [D loss: 0.036229, acc.: 99.22%] [G loss: 0.049444]\n",
      "epoch:27 step:21176 [D loss: 0.017133, acc.: 100.00%] [G loss: 0.063210]\n",
      "epoch:27 step:21177 [D loss: 0.087389, acc.: 98.44%] [G loss: 2.362731]\n",
      "epoch:27 step:21178 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.016960]\n",
      "epoch:27 step:21179 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.006560]\n",
      "epoch:27 step:21180 [D loss: 0.003838, acc.: 100.00%] [G loss: 0.012680]\n",
      "epoch:27 step:21181 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.007620]\n",
      "epoch:27 step:21182 [D loss: 0.001195, acc.: 100.00%] [G loss: 0.007045]\n",
      "epoch:27 step:21183 [D loss: 0.000782, acc.: 100.00%] [G loss: 0.984299]\n",
      "epoch:27 step:21184 [D loss: 0.001747, acc.: 100.00%] [G loss: 0.431161]\n",
      "epoch:27 step:21185 [D loss: 0.002329, acc.: 100.00%] [G loss: 2.130321]\n",
      "epoch:27 step:21186 [D loss: 0.008716, acc.: 100.00%] [G loss: 0.026278]\n",
      "epoch:27 step:21187 [D loss: 0.001356, acc.: 100.00%] [G loss: 0.051508]\n",
      "epoch:27 step:21188 [D loss: 0.002329, acc.: 100.00%] [G loss: 0.375570]\n",
      "epoch:27 step:21189 [D loss: 0.018660, acc.: 100.00%] [G loss: 0.124387]\n",
      "epoch:27 step:21190 [D loss: 0.080123, acc.: 98.44%] [G loss: 0.018630]\n",
      "epoch:27 step:21191 [D loss: 0.004171, acc.: 100.00%] [G loss: 0.900970]\n",
      "epoch:27 step:21192 [D loss: 0.010856, acc.: 100.00%] [G loss: 0.068174]\n",
      "epoch:27 step:21193 [D loss: 0.013045, acc.: 100.00%] [G loss: 0.063787]\n",
      "epoch:27 step:21194 [D loss: 0.015486, acc.: 100.00%] [G loss: 0.466070]\n",
      "epoch:27 step:21195 [D loss: 0.024621, acc.: 99.22%] [G loss: 0.058526]\n",
      "epoch:27 step:21196 [D loss: 0.005682, acc.: 100.00%] [G loss: 0.016882]\n",
      "epoch:27 step:21197 [D loss: 0.001817, acc.: 100.00%] [G loss: 0.198164]\n",
      "epoch:27 step:21198 [D loss: 0.020312, acc.: 100.00%] [G loss: 0.012428]\n",
      "epoch:27 step:21199 [D loss: 0.031036, acc.: 100.00%] [G loss: 0.240541]\n",
      "epoch:27 step:21200 [D loss: 0.007404, acc.: 100.00%] [G loss: 0.010608]\n",
      "epoch:27 step:21201 [D loss: 0.011193, acc.: 100.00%] [G loss: 0.025686]\n",
      "epoch:27 step:21202 [D loss: 0.010471, acc.: 100.00%] [G loss: 0.137880]\n",
      "epoch:27 step:21203 [D loss: 0.034774, acc.: 99.22%] [G loss: 0.028099]\n",
      "epoch:27 step:21204 [D loss: 0.035674, acc.: 100.00%] [G loss: 0.002120]\n",
      "epoch:27 step:21205 [D loss: 0.001714, acc.: 100.00%] [G loss: 0.024735]\n",
      "epoch:27 step:21206 [D loss: 0.003546, acc.: 100.00%] [G loss: 0.019879]\n",
      "epoch:27 step:21207 [D loss: 0.001481, acc.: 100.00%] [G loss: 0.006874]\n",
      "epoch:27 step:21208 [D loss: 0.000905, acc.: 100.00%] [G loss: 0.005086]\n",
      "epoch:27 step:21209 [D loss: 0.001605, acc.: 100.00%] [G loss: 0.323125]\n",
      "epoch:27 step:21210 [D loss: 0.005489, acc.: 100.00%] [G loss: 0.423447]\n",
      "epoch:27 step:21211 [D loss: 0.010950, acc.: 100.00%] [G loss: 0.008652]\n",
      "epoch:27 step:21212 [D loss: 0.001103, acc.: 100.00%] [G loss: 0.007230]\n",
      "epoch:27 step:21213 [D loss: 0.001019, acc.: 100.00%] [G loss: 0.001366]\n",
      "epoch:27 step:21214 [D loss: 0.001185, acc.: 100.00%] [G loss: 0.590436]\n",
      "epoch:27 step:21215 [D loss: 0.023132, acc.: 100.00%] [G loss: 0.005176]\n",
      "epoch:27 step:21216 [D loss: 0.002336, acc.: 100.00%] [G loss: 0.010534]\n",
      "epoch:27 step:21217 [D loss: 0.001213, acc.: 100.00%] [G loss: 0.004287]\n",
      "epoch:27 step:21218 [D loss: 0.002361, acc.: 100.00%] [G loss: 0.003003]\n",
      "epoch:27 step:21219 [D loss: 0.007635, acc.: 100.00%] [G loss: 0.088048]\n",
      "epoch:27 step:21220 [D loss: 0.001172, acc.: 100.00%] [G loss: 0.075939]\n",
      "epoch:27 step:21221 [D loss: 0.000845, acc.: 100.00%] [G loss: 0.004667]\n",
      "epoch:27 step:21222 [D loss: 0.008576, acc.: 100.00%] [G loss: 0.002749]\n",
      "epoch:27 step:21223 [D loss: 0.000745, acc.: 100.00%] [G loss: 0.009158]\n",
      "epoch:27 step:21224 [D loss: 0.001255, acc.: 100.00%] [G loss: 0.002712]\n",
      "epoch:27 step:21225 [D loss: 0.002581, acc.: 100.00%] [G loss: 0.007117]\n",
      "epoch:27 step:21226 [D loss: 0.001934, acc.: 100.00%] [G loss: 0.277410]\n",
      "epoch:27 step:21227 [D loss: 0.006787, acc.: 100.00%] [G loss: 0.562631]\n",
      "epoch:27 step:21228 [D loss: 0.030460, acc.: 100.00%] [G loss: 0.089602]\n",
      "epoch:27 step:21229 [D loss: 0.004508, acc.: 100.00%] [G loss: 0.052989]\n",
      "epoch:27 step:21230 [D loss: 0.063104, acc.: 99.22%] [G loss: 0.251143]\n",
      "epoch:27 step:21231 [D loss: 0.006318, acc.: 100.00%] [G loss: 0.185063]\n",
      "epoch:27 step:21232 [D loss: 0.015807, acc.: 100.00%] [G loss: 0.030946]\n",
      "epoch:27 step:21233 [D loss: 0.004703, acc.: 100.00%] [G loss: 0.159636]\n",
      "epoch:27 step:21234 [D loss: 0.005245, acc.: 100.00%] [G loss: 0.041072]\n",
      "epoch:27 step:21235 [D loss: 0.105890, acc.: 96.09%] [G loss: 0.464391]\n",
      "epoch:27 step:21236 [D loss: 0.011514, acc.: 99.22%] [G loss: 0.135730]\n",
      "epoch:27 step:21237 [D loss: 0.013903, acc.: 100.00%] [G loss: 0.013555]\n",
      "epoch:27 step:21238 [D loss: 0.036656, acc.: 99.22%] [G loss: 0.026614]\n",
      "epoch:27 step:21239 [D loss: 0.011761, acc.: 100.00%] [G loss: 0.001185]\n",
      "epoch:27 step:21240 [D loss: 0.060848, acc.: 97.66%] [G loss: 0.000439]\n",
      "epoch:27 step:21241 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:27 step:21242 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.263551]\n",
      "epoch:27 step:21243 [D loss: 0.000917, acc.: 100.00%] [G loss: 0.723412]\n",
      "epoch:27 step:21244 [D loss: 0.000967, acc.: 100.00%] [G loss: 0.018259]\n",
      "epoch:27 step:21245 [D loss: 0.013721, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:27 step:21246 [D loss: 0.138274, acc.: 93.75%] [G loss: 0.145346]\n",
      "epoch:27 step:21247 [D loss: 0.147580, acc.: 96.88%] [G loss: 0.054940]\n",
      "epoch:27 step:21248 [D loss: 0.012208, acc.: 100.00%] [G loss: 0.021093]\n",
      "epoch:27 step:21249 [D loss: 0.004062, acc.: 100.00%] [G loss: 1.658194]\n",
      "epoch:27 step:21250 [D loss: 0.007815, acc.: 100.00%] [G loss: 0.008708]\n",
      "epoch:27 step:21251 [D loss: 0.237354, acc.: 87.50%] [G loss: 5.017969]\n",
      "epoch:27 step:21252 [D loss: 0.055147, acc.: 98.44%] [G loss: 5.009931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21253 [D loss: 0.782924, acc.: 69.53%] [G loss: 0.816115]\n",
      "epoch:27 step:21254 [D loss: 0.313908, acc.: 85.16%] [G loss: 0.700092]\n",
      "epoch:27 step:21255 [D loss: 0.000305, acc.: 100.00%] [G loss: 1.576079]\n",
      "epoch:27 step:21256 [D loss: 0.005416, acc.: 100.00%] [G loss: 4.301447]\n",
      "epoch:27 step:21257 [D loss: 0.037242, acc.: 97.66%] [G loss: 2.661839]\n",
      "epoch:27 step:21258 [D loss: 0.030727, acc.: 98.44%] [G loss: 0.981942]\n",
      "epoch:27 step:21259 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.874721]\n",
      "epoch:27 step:21260 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.984599]\n",
      "epoch:27 step:21261 [D loss: 0.004744, acc.: 100.00%] [G loss: 0.107577]\n",
      "epoch:27 step:21262 [D loss: 0.001315, acc.: 100.00%] [G loss: 0.290883]\n",
      "epoch:27 step:21263 [D loss: 0.002447, acc.: 100.00%] [G loss: 0.338205]\n",
      "epoch:27 step:21264 [D loss: 0.001281, acc.: 100.00%] [G loss: 0.064679]\n",
      "epoch:27 step:21265 [D loss: 0.001089, acc.: 100.00%] [G loss: 0.035055]\n",
      "epoch:27 step:21266 [D loss: 0.006751, acc.: 100.00%] [G loss: 3.204365]\n",
      "epoch:27 step:21267 [D loss: 0.168353, acc.: 94.53%] [G loss: 0.435465]\n",
      "epoch:27 step:21268 [D loss: 0.003691, acc.: 100.00%] [G loss: 1.568991]\n",
      "epoch:27 step:21269 [D loss: 0.138208, acc.: 94.53%] [G loss: 0.298076]\n",
      "epoch:27 step:21270 [D loss: 0.045087, acc.: 99.22%] [G loss: 0.593594]\n",
      "epoch:27 step:21271 [D loss: 0.011794, acc.: 100.00%] [G loss: 0.473784]\n",
      "epoch:27 step:21272 [D loss: 0.010795, acc.: 100.00%] [G loss: 1.162999]\n",
      "epoch:27 step:21273 [D loss: 0.026291, acc.: 100.00%] [G loss: 1.069553]\n",
      "epoch:27 step:21274 [D loss: 0.050674, acc.: 99.22%] [G loss: 0.982213]\n",
      "epoch:27 step:21275 [D loss: 0.010247, acc.: 100.00%] [G loss: 2.656091]\n",
      "epoch:27 step:21276 [D loss: 0.022952, acc.: 100.00%] [G loss: 2.422095]\n",
      "epoch:27 step:21277 [D loss: 0.112448, acc.: 99.22%] [G loss: 5.042595]\n",
      "epoch:27 step:21278 [D loss: 0.010657, acc.: 100.00%] [G loss: 6.367817]\n",
      "epoch:27 step:21279 [D loss: 0.002485, acc.: 100.00%] [G loss: 5.432569]\n",
      "epoch:27 step:21280 [D loss: 0.004218, acc.: 100.00%] [G loss: 3.936757]\n",
      "epoch:27 step:21281 [D loss: 0.010553, acc.: 100.00%] [G loss: 0.310918]\n",
      "epoch:27 step:21282 [D loss: 0.047796, acc.: 99.22%] [G loss: 1.044354]\n",
      "epoch:27 step:21283 [D loss: 0.002761, acc.: 100.00%] [G loss: 0.128017]\n",
      "epoch:27 step:21284 [D loss: 0.062705, acc.: 99.22%] [G loss: 0.044060]\n",
      "epoch:27 step:21285 [D loss: 0.197556, acc.: 94.53%] [G loss: 4.620251]\n",
      "epoch:27 step:21286 [D loss: 0.700617, acc.: 70.31%] [G loss: 1.228195]\n",
      "epoch:27 step:21287 [D loss: 0.032056, acc.: 100.00%] [G loss: 5.549668]\n",
      "epoch:27 step:21288 [D loss: 0.000738, acc.: 100.00%] [G loss: 4.234907]\n",
      "epoch:27 step:21289 [D loss: 0.003171, acc.: 100.00%] [G loss: 2.759436]\n",
      "epoch:27 step:21290 [D loss: 0.001104, acc.: 100.00%] [G loss: 1.534364]\n",
      "epoch:27 step:21291 [D loss: 0.002409, acc.: 100.00%] [G loss: 0.809818]\n",
      "epoch:27 step:21292 [D loss: 0.005400, acc.: 100.00%] [G loss: 1.361712]\n",
      "epoch:27 step:21293 [D loss: 0.001740, acc.: 100.00%] [G loss: 0.199435]\n",
      "epoch:27 step:21294 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.711687]\n",
      "epoch:27 step:21295 [D loss: 0.001365, acc.: 100.00%] [G loss: 0.126544]\n",
      "epoch:27 step:21296 [D loss: 0.005878, acc.: 100.00%] [G loss: 0.048150]\n",
      "epoch:27 step:21297 [D loss: 0.001353, acc.: 100.00%] [G loss: 0.048466]\n",
      "epoch:27 step:21298 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.012712]\n",
      "epoch:27 step:21299 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.003631]\n",
      "epoch:27 step:21300 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.308520]\n",
      "epoch:27 step:21301 [D loss: 0.024224, acc.: 100.00%] [G loss: 0.006091]\n",
      "epoch:27 step:21302 [D loss: 0.000605, acc.: 100.00%] [G loss: 0.001003]\n",
      "epoch:27 step:21303 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.002330]\n",
      "epoch:27 step:21304 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.004035]\n",
      "epoch:27 step:21305 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.005680]\n",
      "epoch:27 step:21306 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.051463]\n",
      "epoch:27 step:21307 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.126375]\n",
      "epoch:27 step:21308 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.066557]\n",
      "epoch:27 step:21309 [D loss: 0.005490, acc.: 100.00%] [G loss: 0.030009]\n",
      "epoch:27 step:21310 [D loss: 0.008602, acc.: 100.00%] [G loss: 0.037260]\n",
      "epoch:27 step:21311 [D loss: 0.007588, acc.: 100.00%] [G loss: 0.006520]\n",
      "epoch:27 step:21312 [D loss: 0.019342, acc.: 100.00%] [G loss: 0.016059]\n",
      "epoch:27 step:21313 [D loss: 0.001209, acc.: 100.00%] [G loss: 0.020173]\n",
      "epoch:27 step:21314 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.007727]\n",
      "epoch:27 step:21315 [D loss: 0.001408, acc.: 100.00%] [G loss: 0.005493]\n",
      "epoch:27 step:21316 [D loss: 0.004730, acc.: 100.00%] [G loss: 0.023110]\n",
      "epoch:27 step:21317 [D loss: 0.002005, acc.: 100.00%] [G loss: 0.003796]\n",
      "epoch:27 step:21318 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.149464]\n",
      "epoch:27 step:21319 [D loss: 0.000679, acc.: 100.00%] [G loss: 0.006672]\n",
      "epoch:27 step:21320 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.125056]\n",
      "epoch:27 step:21321 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.005238]\n",
      "epoch:27 step:21322 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.005600]\n",
      "epoch:27 step:21323 [D loss: 0.021445, acc.: 99.22%] [G loss: 0.001959]\n",
      "epoch:27 step:21324 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.062064]\n",
      "epoch:27 step:21325 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.001727]\n",
      "epoch:27 step:21326 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.003550]\n",
      "epoch:27 step:21327 [D loss: 0.002573, acc.: 100.00%] [G loss: 0.002278]\n",
      "epoch:27 step:21328 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.003686]\n",
      "epoch:27 step:21329 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.001973]\n",
      "epoch:27 step:21330 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.003934]\n",
      "epoch:27 step:21331 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.002579]\n",
      "epoch:27 step:21332 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.004840]\n",
      "epoch:27 step:21333 [D loss: 0.002866, acc.: 100.00%] [G loss: 0.000754]\n",
      "epoch:27 step:21334 [D loss: 0.002318, acc.: 100.00%] [G loss: 0.001211]\n",
      "epoch:27 step:21335 [D loss: 0.000655, acc.: 100.00%] [G loss: 0.000524]\n",
      "epoch:27 step:21336 [D loss: 0.001333, acc.: 100.00%] [G loss: 0.013959]\n",
      "epoch:27 step:21337 [D loss: 0.009936, acc.: 100.00%] [G loss: 0.003663]\n",
      "epoch:27 step:21338 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.001777]\n",
      "epoch:27 step:21339 [D loss: 0.000717, acc.: 100.00%] [G loss: 0.004631]\n",
      "epoch:27 step:21340 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.108162]\n",
      "epoch:27 step:21341 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000647]\n",
      "epoch:27 step:21342 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.006493]\n",
      "epoch:27 step:21343 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001508]\n",
      "epoch:27 step:21344 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000974]\n",
      "epoch:27 step:21345 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000823]\n",
      "epoch:27 step:21346 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.003304]\n",
      "epoch:27 step:21347 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.001144]\n",
      "epoch:27 step:21348 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.015780]\n",
      "epoch:27 step:21349 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.006569]\n",
      "epoch:27 step:21350 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.001383]\n",
      "epoch:27 step:21351 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.001176]\n",
      "epoch:27 step:21352 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.001205]\n",
      "epoch:27 step:21353 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.002980]\n",
      "epoch:27 step:21354 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.001578]\n",
      "epoch:27 step:21355 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.002284]\n",
      "epoch:27 step:21356 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.002287]\n",
      "epoch:27 step:21357 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000904]\n",
      "epoch:27 step:21358 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.009137]\n",
      "epoch:27 step:21359 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.001131]\n",
      "epoch:27 step:21360 [D loss: 0.000732, acc.: 100.00%] [G loss: 0.005003]\n",
      "epoch:27 step:21361 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.001734]\n",
      "epoch:27 step:21362 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.002958]\n",
      "epoch:27 step:21363 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000598]\n",
      "epoch:27 step:21364 [D loss: 0.002734, acc.: 100.00%] [G loss: 0.001117]\n",
      "epoch:27 step:21365 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.002214]\n",
      "epoch:27 step:21366 [D loss: 0.000593, acc.: 100.00%] [G loss: 0.050277]\n",
      "epoch:27 step:21367 [D loss: 0.310944, acc.: 82.81%] [G loss: 2.670606]\n",
      "epoch:27 step:21368 [D loss: 0.052852, acc.: 97.66%] [G loss: 4.060486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21369 [D loss: 2.212659, acc.: 51.56%] [G loss: 0.204495]\n",
      "epoch:27 step:21370 [D loss: 0.204656, acc.: 90.62%] [G loss: 0.126692]\n",
      "epoch:27 step:21371 [D loss: 0.005664, acc.: 100.00%] [G loss: 0.264994]\n",
      "epoch:27 step:21372 [D loss: 0.008325, acc.: 100.00%] [G loss: 0.197306]\n",
      "epoch:27 step:21373 [D loss: 0.072167, acc.: 96.88%] [G loss: 0.452771]\n",
      "epoch:27 step:21374 [D loss: 0.005111, acc.: 100.00%] [G loss: 0.214993]\n",
      "epoch:27 step:21375 [D loss: 0.002598, acc.: 100.00%] [G loss: 1.841082]\n",
      "epoch:27 step:21376 [D loss: 0.002663, acc.: 100.00%] [G loss: 2.048219]\n",
      "epoch:27 step:21377 [D loss: 0.000800, acc.: 100.00%] [G loss: 1.319773]\n",
      "epoch:27 step:21378 [D loss: 0.001757, acc.: 100.00%] [G loss: 0.235287]\n",
      "epoch:27 step:21379 [D loss: 0.024470, acc.: 100.00%] [G loss: 0.077695]\n",
      "epoch:27 step:21380 [D loss: 0.003118, acc.: 100.00%] [G loss: 0.036925]\n",
      "epoch:27 step:21381 [D loss: 0.000828, acc.: 100.00%] [G loss: 0.048408]\n",
      "epoch:27 step:21382 [D loss: 0.136648, acc.: 93.75%] [G loss: 0.919090]\n",
      "epoch:27 step:21383 [D loss: 0.002862, acc.: 100.00%] [G loss: 2.305397]\n",
      "epoch:27 step:21384 [D loss: 0.020871, acc.: 99.22%] [G loss: 0.111987]\n",
      "epoch:27 step:21385 [D loss: 0.003705, acc.: 100.00%] [G loss: 1.575411]\n",
      "epoch:27 step:21386 [D loss: 0.015084, acc.: 100.00%] [G loss: 0.888239]\n",
      "epoch:27 step:21387 [D loss: 0.073461, acc.: 96.88%] [G loss: 0.301491]\n",
      "epoch:27 step:21388 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.222177]\n",
      "epoch:27 step:21389 [D loss: 0.003126, acc.: 100.00%] [G loss: 0.121799]\n",
      "epoch:27 step:21390 [D loss: 0.006404, acc.: 100.00%] [G loss: 0.110080]\n",
      "epoch:27 step:21391 [D loss: 0.002516, acc.: 100.00%] [G loss: 0.550626]\n",
      "epoch:27 step:21392 [D loss: 0.240517, acc.: 88.28%] [G loss: 1.448552]\n",
      "epoch:27 step:21393 [D loss: 0.002799, acc.: 100.00%] [G loss: 3.993921]\n",
      "epoch:27 step:21394 [D loss: 0.020041, acc.: 100.00%] [G loss: 3.851453]\n",
      "epoch:27 step:21395 [D loss: 0.063372, acc.: 98.44%] [G loss: 1.473382]\n",
      "epoch:27 step:21396 [D loss: 0.012286, acc.: 100.00%] [G loss: 2.064543]\n",
      "epoch:27 step:21397 [D loss: 0.004877, acc.: 100.00%] [G loss: 0.004902]\n",
      "epoch:27 step:21398 [D loss: 0.002881, acc.: 100.00%] [G loss: 0.926638]\n",
      "epoch:27 step:21399 [D loss: 0.002332, acc.: 100.00%] [G loss: 0.301359]\n",
      "epoch:27 step:21400 [D loss: 0.002012, acc.: 100.00%] [G loss: 0.379340]\n",
      "epoch:27 step:21401 [D loss: 0.003733, acc.: 100.00%] [G loss: 0.763804]\n",
      "epoch:27 step:21402 [D loss: 0.372230, acc.: 80.47%] [G loss: 0.000023]\n",
      "epoch:27 step:21403 [D loss: 0.001171, acc.: 100.00%] [G loss: 0.336394]\n",
      "epoch:27 step:21404 [D loss: 0.001889, acc.: 100.00%] [G loss: 0.391822]\n",
      "epoch:27 step:21405 [D loss: 0.001527, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:27 step:21406 [D loss: 0.003610, acc.: 100.00%] [G loss: 0.281901]\n",
      "epoch:27 step:21407 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.251506]\n",
      "epoch:27 step:21408 [D loss: 0.000828, acc.: 100.00%] [G loss: 0.206883]\n",
      "epoch:27 step:21409 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.169023]\n",
      "epoch:27 step:21410 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.220952]\n",
      "epoch:27 step:21411 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.097083]\n",
      "epoch:27 step:21412 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:27 step:21413 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.020914]\n",
      "epoch:27 step:21414 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.136574]\n",
      "epoch:27 step:21415 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.020125]\n",
      "epoch:27 step:21416 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.001037]\n",
      "epoch:27 step:21417 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.170508]\n",
      "epoch:27 step:21418 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000692]\n",
      "epoch:27 step:21419 [D loss: 0.000659, acc.: 100.00%] [G loss: 0.002066]\n",
      "epoch:27 step:21420 [D loss: 0.185103, acc.: 90.62%] [G loss: 0.312935]\n",
      "epoch:27 step:21421 [D loss: 0.001397, acc.: 100.00%] [G loss: 1.439840]\n",
      "epoch:27 step:21422 [D loss: 0.030081, acc.: 99.22%] [G loss: 1.103324]\n",
      "epoch:27 step:21423 [D loss: 0.060835, acc.: 98.44%] [G loss: 0.111659]\n",
      "epoch:27 step:21424 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.030581]\n",
      "epoch:27 step:21425 [D loss: 0.002605, acc.: 100.00%] [G loss: 0.014236]\n",
      "epoch:27 step:21426 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.012934]\n",
      "epoch:27 step:21427 [D loss: 0.000651, acc.: 100.00%] [G loss: 0.025842]\n",
      "epoch:27 step:21428 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.021219]\n",
      "epoch:27 step:21429 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.022559]\n",
      "epoch:27 step:21430 [D loss: 0.000682, acc.: 100.00%] [G loss: 0.010411]\n",
      "epoch:27 step:21431 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.001751]\n",
      "epoch:27 step:21432 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.006675]\n",
      "epoch:27 step:21433 [D loss: 0.001001, acc.: 100.00%] [G loss: 0.119547]\n",
      "epoch:27 step:21434 [D loss: 0.001811, acc.: 100.00%] [G loss: 0.061047]\n",
      "epoch:27 step:21435 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.013845]\n",
      "epoch:27 step:21436 [D loss: 0.002721, acc.: 100.00%] [G loss: 0.009864]\n",
      "epoch:27 step:21437 [D loss: 0.007399, acc.: 100.00%] [G loss: 0.013076]\n",
      "epoch:27 step:21438 [D loss: 0.000822, acc.: 100.00%] [G loss: 0.791291]\n",
      "epoch:27 step:21439 [D loss: 0.002976, acc.: 100.00%] [G loss: 0.084771]\n",
      "epoch:27 step:21440 [D loss: 0.001832, acc.: 100.00%] [G loss: 0.003405]\n",
      "epoch:27 step:21441 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.000862]\n",
      "epoch:27 step:21442 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000746]\n",
      "epoch:27 step:21443 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.003881]\n",
      "epoch:27 step:21444 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.002780]\n",
      "epoch:27 step:21445 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.001320]\n",
      "epoch:27 step:21446 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.001112]\n",
      "epoch:27 step:21447 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.005774]\n",
      "epoch:27 step:21448 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.002620]\n",
      "epoch:27 step:21449 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.034526]\n",
      "epoch:27 step:21450 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.035966]\n",
      "epoch:27 step:21451 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.003890]\n",
      "epoch:27 step:21452 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000484]\n",
      "epoch:27 step:21453 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:27 step:21454 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.002714]\n",
      "epoch:27 step:21455 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.045116]\n",
      "epoch:27 step:21456 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:27 step:21457 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.001327]\n",
      "epoch:27 step:21458 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.010968]\n",
      "epoch:27 step:21459 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.005594]\n",
      "epoch:27 step:21460 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000798]\n",
      "epoch:27 step:21461 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.001795]\n",
      "epoch:27 step:21462 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.008624]\n",
      "epoch:27 step:21463 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.000959]\n",
      "epoch:27 step:21464 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.009993]\n",
      "epoch:27 step:21465 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.001783]\n",
      "epoch:27 step:21466 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.001229]\n",
      "epoch:27 step:21467 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.002300]\n",
      "epoch:27 step:21468 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.002205]\n",
      "epoch:27 step:21469 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000921]\n",
      "epoch:27 step:21470 [D loss: 0.001046, acc.: 100.00%] [G loss: 0.002589]\n",
      "epoch:27 step:21471 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.001336]\n",
      "epoch:27 step:21472 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.002323]\n",
      "epoch:27 step:21473 [D loss: 0.004258, acc.: 100.00%] [G loss: 0.000639]\n",
      "epoch:27 step:21474 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001255]\n",
      "epoch:27 step:21475 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.007868]\n",
      "epoch:27 step:21476 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000728]\n",
      "epoch:27 step:21477 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.002511]\n",
      "epoch:27 step:21478 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:27 step:21479 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.001331]\n",
      "epoch:27 step:21480 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.002591]\n",
      "epoch:27 step:21481 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.003457]\n",
      "epoch:27 step:21482 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.005587]\n",
      "epoch:27 step:21483 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.006695]\n",
      "epoch:27 step:21484 [D loss: 0.000758, acc.: 100.00%] [G loss: 0.001732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21485 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.010459]\n",
      "epoch:27 step:21486 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.004823]\n",
      "epoch:27 step:21487 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:27 step:21488 [D loss: 0.000637, acc.: 100.00%] [G loss: 0.005780]\n",
      "epoch:27 step:21489 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000814]\n",
      "epoch:27 step:21490 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.003630]\n",
      "epoch:27 step:21491 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000982]\n",
      "epoch:27 step:21492 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.032578]\n",
      "epoch:27 step:21493 [D loss: 0.000331, acc.: 100.00%] [G loss: 0.002220]\n",
      "epoch:27 step:21494 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.000722]\n",
      "epoch:27 step:21495 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:27 step:21496 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000360]\n",
      "epoch:27 step:21497 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.005889]\n",
      "epoch:27 step:21498 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.014377]\n",
      "epoch:27 step:21499 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000848]\n",
      "epoch:27 step:21500 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000467]\n",
      "epoch:27 step:21501 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.007472]\n",
      "epoch:27 step:21502 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000880]\n",
      "epoch:27 step:21503 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.004136]\n",
      "epoch:27 step:21504 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.003478]\n",
      "epoch:27 step:21505 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.002809]\n",
      "epoch:27 step:21506 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.001586]\n",
      "epoch:27 step:21507 [D loss: 0.000478, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:27 step:21508 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000714]\n",
      "epoch:27 step:21509 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000662]\n",
      "epoch:27 step:21510 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.001218]\n",
      "epoch:27 step:21511 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:27 step:21512 [D loss: 0.000391, acc.: 100.00%] [G loss: 0.000735]\n",
      "epoch:27 step:21513 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:27 step:21514 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.003123]\n",
      "epoch:27 step:21515 [D loss: 0.001612, acc.: 100.00%] [G loss: 0.000667]\n",
      "epoch:27 step:21516 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.007619]\n",
      "epoch:27 step:21517 [D loss: 0.006380, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:27 step:21518 [D loss: 0.001055, acc.: 100.00%] [G loss: 0.002232]\n",
      "epoch:27 step:21519 [D loss: 0.001967, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:27 step:21520 [D loss: 0.000927, acc.: 100.00%] [G loss: 0.000300]\n",
      "epoch:27 step:21521 [D loss: 0.000638, acc.: 100.00%] [G loss: 0.002006]\n",
      "epoch:27 step:21522 [D loss: 0.000593, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:27 step:21523 [D loss: 0.006961, acc.: 100.00%] [G loss: 0.364129]\n",
      "epoch:27 step:21524 [D loss: 0.394763, acc.: 78.91%] [G loss: 1.702450]\n",
      "epoch:27 step:21525 [D loss: 0.717511, acc.: 70.31%] [G loss: 0.392994]\n",
      "epoch:27 step:21526 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.526334]\n",
      "epoch:27 step:21527 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:27 step:21528 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.090628]\n",
      "epoch:27 step:21529 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.013635]\n",
      "epoch:27 step:21530 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.412155]\n",
      "epoch:27 step:21531 [D loss: 0.007883, acc.: 99.22%] [G loss: 0.004581]\n",
      "epoch:27 step:21532 [D loss: 0.002705, acc.: 100.00%] [G loss: 0.004629]\n",
      "epoch:27 step:21533 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:27 step:21534 [D loss: 0.054741, acc.: 99.22%] [G loss: 0.303097]\n",
      "epoch:27 step:21535 [D loss: 0.001931, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:27 step:21536 [D loss: 0.009607, acc.: 100.00%] [G loss: 0.001456]\n",
      "epoch:27 step:21537 [D loss: 0.000636, acc.: 100.00%] [G loss: 0.042916]\n",
      "epoch:27 step:21538 [D loss: 0.006064, acc.: 100.00%] [G loss: 0.000420]\n",
      "epoch:27 step:21539 [D loss: 0.004918, acc.: 100.00%] [G loss: 0.001129]\n",
      "epoch:27 step:21540 [D loss: 0.044779, acc.: 99.22%] [G loss: 0.048900]\n",
      "epoch:27 step:21541 [D loss: 0.010748, acc.: 100.00%] [G loss: 0.144416]\n",
      "epoch:27 step:21542 [D loss: 0.049122, acc.: 100.00%] [G loss: 0.002753]\n",
      "epoch:27 step:21543 [D loss: 0.002332, acc.: 100.00%] [G loss: 0.444049]\n",
      "epoch:27 step:21544 [D loss: 0.014904, acc.: 100.00%] [G loss: 0.021403]\n",
      "epoch:27 step:21545 [D loss: 0.007537, acc.: 100.00%] [G loss: 0.132004]\n",
      "epoch:27 step:21546 [D loss: 0.022054, acc.: 99.22%] [G loss: 0.213155]\n",
      "epoch:27 step:21547 [D loss: 0.015151, acc.: 100.00%] [G loss: 0.003268]\n",
      "epoch:27 step:21548 [D loss: 0.004737, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:27 step:21549 [D loss: 0.006990, acc.: 100.00%] [G loss: 0.001287]\n",
      "epoch:27 step:21550 [D loss: 0.015226, acc.: 100.00%] [G loss: 0.000882]\n",
      "epoch:27 step:21551 [D loss: 0.022093, acc.: 100.00%] [G loss: 0.279826]\n",
      "epoch:27 step:21552 [D loss: 0.016214, acc.: 100.00%] [G loss: 0.033872]\n",
      "epoch:27 step:21553 [D loss: 0.001663, acc.: 100.00%] [G loss: 0.034207]\n",
      "epoch:27 step:21554 [D loss: 0.029594, acc.: 99.22%] [G loss: 0.001484]\n",
      "epoch:27 step:21555 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.003541]\n",
      "epoch:27 step:21556 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.002322]\n",
      "epoch:27 step:21557 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.192369]\n",
      "epoch:27 step:21558 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:27 step:21559 [D loss: 0.001169, acc.: 100.00%] [G loss: 0.000757]\n",
      "epoch:27 step:21560 [D loss: 0.001165, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:27 step:21561 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.001405]\n",
      "epoch:27 step:21562 [D loss: 0.001898, acc.: 100.00%] [G loss: 0.000420]\n",
      "epoch:27 step:21563 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:27 step:21564 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:27 step:21565 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.003791]\n",
      "epoch:27 step:21566 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.001344]\n",
      "epoch:27 step:21567 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.001015]\n",
      "epoch:27 step:21568 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.053432]\n",
      "epoch:27 step:21569 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.005891]\n",
      "epoch:27 step:21570 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:27 step:21571 [D loss: 0.002299, acc.: 100.00%] [G loss: 0.000478]\n",
      "epoch:27 step:21572 [D loss: 0.005741, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:27 step:21573 [D loss: 0.000933, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:27 step:21574 [D loss: 0.000526, acc.: 100.00%] [G loss: 0.001542]\n",
      "epoch:27 step:21575 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.007891]\n",
      "epoch:27 step:21576 [D loss: 0.000821, acc.: 100.00%] [G loss: 0.001668]\n",
      "epoch:27 step:21577 [D loss: 0.024700, acc.: 100.00%] [G loss: 0.009175]\n",
      "epoch:27 step:21578 [D loss: 0.002288, acc.: 100.00%] [G loss: 0.004498]\n",
      "epoch:27 step:21579 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.003948]\n",
      "epoch:27 step:21580 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.010272]\n",
      "epoch:27 step:21581 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.010189]\n",
      "epoch:27 step:21582 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.002035]\n",
      "epoch:27 step:21583 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.002047]\n",
      "epoch:27 step:21584 [D loss: 0.000601, acc.: 100.00%] [G loss: 0.004862]\n",
      "epoch:27 step:21585 [D loss: 0.024321, acc.: 99.22%] [G loss: 0.000602]\n",
      "epoch:27 step:21586 [D loss: 0.013722, acc.: 99.22%] [G loss: 0.015232]\n",
      "epoch:27 step:21587 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:27 step:21588 [D loss: 0.004782, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:27 step:21589 [D loss: 0.002237, acc.: 100.00%] [G loss: 0.000785]\n",
      "epoch:27 step:21590 [D loss: 0.054467, acc.: 98.44%] [G loss: 0.017845]\n",
      "epoch:27 step:21591 [D loss: 0.002229, acc.: 100.00%] [G loss: 1.471405]\n",
      "epoch:27 step:21592 [D loss: 0.028083, acc.: 100.00%] [G loss: 0.094617]\n",
      "epoch:27 step:21593 [D loss: 0.017584, acc.: 99.22%] [G loss: 0.005785]\n",
      "epoch:27 step:21594 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.123096]\n",
      "epoch:27 step:21595 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.140452]\n",
      "epoch:27 step:21596 [D loss: 0.011322, acc.: 100.00%] [G loss: 0.009342]\n",
      "epoch:27 step:21597 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.027757]\n",
      "epoch:27 step:21598 [D loss: 0.001010, acc.: 100.00%] [G loss: 0.002299]\n",
      "epoch:27 step:21599 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.002080]\n",
      "epoch:27 step:21600 [D loss: 0.000731, acc.: 100.00%] [G loss: 0.010957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21601 [D loss: 0.003747, acc.: 100.00%] [G loss: 0.001897]\n",
      "epoch:27 step:21602 [D loss: 0.001035, acc.: 100.00%] [G loss: 0.003739]\n",
      "epoch:27 step:21603 [D loss: 0.003224, acc.: 100.00%] [G loss: 0.020552]\n",
      "epoch:27 step:21604 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:27 step:21605 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.032943]\n",
      "epoch:27 step:21606 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.008578]\n",
      "epoch:27 step:21607 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:27 step:21608 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.008674]\n",
      "epoch:27 step:21609 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.004229]\n",
      "epoch:27 step:21610 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.007941]\n",
      "epoch:27 step:21611 [D loss: 0.000541, acc.: 100.00%] [G loss: 0.010304]\n",
      "epoch:27 step:21612 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.004834]\n",
      "epoch:27 step:21613 [D loss: 0.136148, acc.: 94.53%] [G loss: 0.000004]\n",
      "epoch:27 step:21614 [D loss: 0.001224, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:27 step:21615 [D loss: 0.005301, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:27 step:21616 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:27 step:21617 [D loss: 0.002072, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:27 step:21618 [D loss: 0.006548, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:27 step:21619 [D loss: 0.003061, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:27 step:21620 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:27 step:21621 [D loss: 0.000779, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:27 step:21622 [D loss: 0.001106, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:27 step:21623 [D loss: 0.003182, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:27 step:21624 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:27 step:21625 [D loss: 0.011958, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:27 step:21626 [D loss: 0.000451, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:27 step:21627 [D loss: 0.007563, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:27 step:21628 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:27 step:21629 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:27 step:21630 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:27 step:21631 [D loss: 0.001541, acc.: 100.00%] [G loss: 0.000245]\n",
      "epoch:27 step:21632 [D loss: 0.000468, acc.: 100.00%] [G loss: 0.036492]\n",
      "epoch:27 step:21633 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.003182]\n",
      "epoch:27 step:21634 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:27 step:21635 [D loss: 0.003108, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:27 step:21636 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:27 step:21637 [D loss: 0.002644, acc.: 100.00%] [G loss: 0.001192]\n",
      "epoch:27 step:21638 [D loss: 0.019643, acc.: 100.00%] [G loss: 0.000412]\n",
      "epoch:27 step:21639 [D loss: 0.006321, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:27 step:21640 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.008650]\n",
      "epoch:27 step:21641 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.003313]\n",
      "epoch:27 step:21642 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.004626]\n",
      "epoch:27 step:21643 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.018626]\n",
      "epoch:27 step:21644 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.001205]\n",
      "epoch:27 step:21645 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.013248]\n",
      "epoch:27 step:21646 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.001689]\n",
      "epoch:27 step:21647 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.428566]\n",
      "epoch:27 step:21648 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.000700]\n",
      "epoch:27 step:21649 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000922]\n",
      "epoch:27 step:21650 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.010300]\n",
      "epoch:27 step:21651 [D loss: 0.001302, acc.: 100.00%] [G loss: 0.003001]\n",
      "epoch:27 step:21652 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.001962]\n",
      "epoch:27 step:21653 [D loss: 0.001908, acc.: 100.00%] [G loss: 0.027808]\n",
      "epoch:27 step:21654 [D loss: 0.004312, acc.: 100.00%] [G loss: 0.001393]\n",
      "epoch:27 step:21655 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.001323]\n",
      "epoch:27 step:21656 [D loss: 0.003511, acc.: 100.00%] [G loss: 0.002268]\n",
      "epoch:27 step:21657 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.004701]\n",
      "epoch:27 step:21658 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.003672]\n",
      "epoch:27 step:21659 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.001075]\n",
      "epoch:27 step:21660 [D loss: 0.000504, acc.: 100.00%] [G loss: 0.001941]\n",
      "epoch:27 step:21661 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.002698]\n",
      "epoch:27 step:21662 [D loss: 0.000999, acc.: 100.00%] [G loss: 0.003339]\n",
      "epoch:27 step:21663 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.001838]\n",
      "epoch:27 step:21664 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.001665]\n",
      "epoch:27 step:21665 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.002474]\n",
      "epoch:27 step:21666 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.004913]\n",
      "epoch:27 step:21667 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.002025]\n",
      "epoch:27 step:21668 [D loss: 0.024041, acc.: 100.00%] [G loss: 0.005067]\n",
      "epoch:27 step:21669 [D loss: 0.001536, acc.: 100.00%] [G loss: 0.019353]\n",
      "epoch:27 step:21670 [D loss: 0.027228, acc.: 99.22%] [G loss: 0.014975]\n",
      "epoch:27 step:21671 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.002247]\n",
      "epoch:27 step:21672 [D loss: 0.001604, acc.: 100.00%] [G loss: 0.006042]\n",
      "epoch:27 step:21673 [D loss: 0.174957, acc.: 95.31%] [G loss: 0.413385]\n",
      "epoch:27 step:21674 [D loss: 0.073119, acc.: 96.88%] [G loss: 9.897100]\n",
      "epoch:27 step:21675 [D loss: 0.071078, acc.: 96.88%] [G loss: 5.703278]\n",
      "epoch:27 step:21676 [D loss: 0.004017, acc.: 100.00%] [G loss: 0.001732]\n",
      "epoch:27 step:21677 [D loss: 0.009863, acc.: 100.00%] [G loss: 3.721062]\n",
      "epoch:27 step:21678 [D loss: 0.059283, acc.: 99.22%] [G loss: 1.972948]\n",
      "epoch:27 step:21679 [D loss: 0.019531, acc.: 100.00%] [G loss: 0.031056]\n",
      "epoch:27 step:21680 [D loss: 0.194920, acc.: 89.84%] [G loss: 0.000262]\n",
      "epoch:27 step:21681 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.001203]\n",
      "epoch:27 step:21682 [D loss: 0.004348, acc.: 100.00%] [G loss: 0.009415]\n",
      "epoch:27 step:21683 [D loss: 0.002270, acc.: 100.00%] [G loss: 2.478339]\n",
      "epoch:27 step:21684 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.007811]\n",
      "epoch:27 step:21685 [D loss: 0.002569, acc.: 100.00%] [G loss: 0.004459]\n",
      "epoch:27 step:21686 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.009735]\n",
      "epoch:27 step:21687 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.001391]\n",
      "epoch:27 step:21688 [D loss: 0.011872, acc.: 100.00%] [G loss: 0.133534]\n",
      "epoch:27 step:21689 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.000844]\n",
      "epoch:27 step:21690 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:27 step:21691 [D loss: 0.001201, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:27 step:21692 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:27 step:21693 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:27 step:21694 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.218744]\n",
      "epoch:27 step:21695 [D loss: 0.000526, acc.: 100.00%] [G loss: 0.000509]\n",
      "epoch:27 step:21696 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.001588]\n",
      "epoch:27 step:21697 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.068803]\n",
      "epoch:27 step:21698 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:27 step:21699 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:27 step:21700 [D loss: 0.001249, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:27 step:21701 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:27 step:21702 [D loss: 0.001357, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:27 step:21703 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:27 step:21704 [D loss: 0.001500, acc.: 100.00%] [G loss: 0.000362]\n",
      "epoch:27 step:21705 [D loss: 0.005100, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:27 step:21706 [D loss: 0.000879, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:27 step:21707 [D loss: 0.109586, acc.: 97.66%] [G loss: 0.751083]\n",
      "epoch:27 step:21708 [D loss: 0.001536, acc.: 100.00%] [G loss: 2.211579]\n",
      "epoch:27 step:21709 [D loss: 0.805013, acc.: 61.72%] [G loss: 7.615596]\n",
      "epoch:27 step:21710 [D loss: 2.617942, acc.: 28.91%] [G loss: 5.815462]\n",
      "epoch:27 step:21711 [D loss: 0.006995, acc.: 100.00%] [G loss: 2.281773]\n",
      "epoch:27 step:21712 [D loss: 0.150005, acc.: 92.97%] [G loss: 6.485396]\n",
      "epoch:27 step:21713 [D loss: 0.062464, acc.: 97.66%] [G loss: 6.214331]\n",
      "epoch:27 step:21714 [D loss: 0.007217, acc.: 100.00%] [G loss: 1.718953]\n",
      "epoch:27 step:21715 [D loss: 0.053237, acc.: 98.44%] [G loss: 4.708324]\n",
      "epoch:27 step:21716 [D loss: 0.011123, acc.: 100.00%] [G loss: 0.015448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21717 [D loss: 0.220112, acc.: 89.84%] [G loss: 5.796601]\n",
      "epoch:27 step:21718 [D loss: 0.084966, acc.: 96.09%] [G loss: 4.842625]\n",
      "epoch:27 step:21719 [D loss: 0.061333, acc.: 97.66%] [G loss: 3.756246]\n",
      "epoch:27 step:21720 [D loss: 0.006306, acc.: 100.00%] [G loss: 0.124848]\n",
      "epoch:27 step:21721 [D loss: 0.020009, acc.: 100.00%] [G loss: 1.266237]\n",
      "epoch:27 step:21722 [D loss: 0.003811, acc.: 100.00%] [G loss: 0.033450]\n",
      "epoch:27 step:21723 [D loss: 0.001095, acc.: 100.00%] [G loss: 1.161902]\n",
      "epoch:27 step:21724 [D loss: 0.019765, acc.: 99.22%] [G loss: 0.781489]\n",
      "epoch:27 step:21725 [D loss: 0.001587, acc.: 100.00%] [G loss: 0.993473]\n",
      "epoch:27 step:21726 [D loss: 0.005763, acc.: 100.00%] [G loss: 0.510255]\n",
      "epoch:27 step:21727 [D loss: 0.006114, acc.: 100.00%] [G loss: 0.312166]\n",
      "epoch:27 step:21728 [D loss: 0.020341, acc.: 100.00%] [G loss: 0.230837]\n",
      "epoch:27 step:21729 [D loss: 0.007719, acc.: 100.00%] [G loss: 0.225946]\n",
      "epoch:27 step:21730 [D loss: 0.032537, acc.: 100.00%] [G loss: 0.278370]\n",
      "epoch:27 step:21731 [D loss: 0.030700, acc.: 99.22%] [G loss: 0.067538]\n",
      "epoch:27 step:21732 [D loss: 0.008265, acc.: 100.00%] [G loss: 0.126047]\n",
      "epoch:27 step:21733 [D loss: 0.408793, acc.: 82.81%] [G loss: 0.205687]\n",
      "epoch:27 step:21734 [D loss: 0.008836, acc.: 99.22%] [G loss: 2.454300]\n",
      "epoch:27 step:21735 [D loss: 0.004339, acc.: 100.00%] [G loss: 2.808969]\n",
      "epoch:27 step:21736 [D loss: 0.074450, acc.: 96.88%] [G loss: 0.907623]\n",
      "epoch:27 step:21737 [D loss: 0.013065, acc.: 100.00%] [G loss: 0.147934]\n",
      "epoch:27 step:21738 [D loss: 0.001180, acc.: 100.00%] [G loss: 0.273670]\n",
      "epoch:27 step:21739 [D loss: 0.015152, acc.: 100.00%] [G loss: 0.469884]\n",
      "epoch:27 step:21740 [D loss: 0.016825, acc.: 100.00%] [G loss: 0.318061]\n",
      "epoch:27 step:21741 [D loss: 0.007039, acc.: 100.00%] [G loss: 0.186403]\n",
      "epoch:27 step:21742 [D loss: 0.009421, acc.: 100.00%] [G loss: 0.789763]\n",
      "epoch:27 step:21743 [D loss: 0.021723, acc.: 100.00%] [G loss: 2.105645]\n",
      "epoch:27 step:21744 [D loss: 0.063591, acc.: 100.00%] [G loss: 1.560737]\n",
      "epoch:27 step:21745 [D loss: 0.005437, acc.: 100.00%] [G loss: 0.766967]\n",
      "epoch:27 step:21746 [D loss: 0.027912, acc.: 99.22%] [G loss: 7.349221]\n",
      "epoch:27 step:21747 [D loss: 0.001462, acc.: 100.00%] [G loss: 0.031451]\n",
      "epoch:27 step:21748 [D loss: 0.018105, acc.: 100.00%] [G loss: 0.003542]\n",
      "epoch:27 step:21749 [D loss: 0.015810, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:27 step:21750 [D loss: 0.015786, acc.: 100.00%] [G loss: 5.502290]\n",
      "epoch:27 step:21751 [D loss: 0.008403, acc.: 100.00%] [G loss: 4.254085]\n",
      "epoch:27 step:21752 [D loss: 0.050925, acc.: 97.66%] [G loss: 2.779589]\n",
      "epoch:27 step:21753 [D loss: 0.058711, acc.: 100.00%] [G loss: 2.340035]\n",
      "epoch:27 step:21754 [D loss: 0.000444, acc.: 100.00%] [G loss: 2.597875]\n",
      "epoch:27 step:21755 [D loss: 0.029183, acc.: 99.22%] [G loss: 0.258823]\n",
      "epoch:27 step:21756 [D loss: 0.037979, acc.: 99.22%] [G loss: 1.316334]\n",
      "epoch:27 step:21757 [D loss: 0.280241, acc.: 86.72%] [G loss: 0.937804]\n",
      "epoch:27 step:21758 [D loss: 0.584413, acc.: 73.44%] [G loss: 0.019407]\n",
      "epoch:27 step:21759 [D loss: 0.428183, acc.: 92.97%] [G loss: 2.280404]\n",
      "epoch:27 step:21760 [D loss: 0.032057, acc.: 99.22%] [G loss: 0.688826]\n",
      "epoch:27 step:21761 [D loss: 0.191625, acc.: 96.88%] [G loss: 0.236623]\n",
      "epoch:27 step:21762 [D loss: 0.001681, acc.: 100.00%] [G loss: 1.169769]\n",
      "epoch:27 step:21763 [D loss: 0.011515, acc.: 100.00%] [G loss: 0.048293]\n",
      "epoch:27 step:21764 [D loss: 0.011735, acc.: 99.22%] [G loss: 0.020197]\n",
      "epoch:27 step:21765 [D loss: 0.123826, acc.: 94.53%] [G loss: 3.489019]\n",
      "epoch:27 step:21766 [D loss: 0.002368, acc.: 100.00%] [G loss: 1.372148]\n",
      "epoch:27 step:21767 [D loss: 0.145671, acc.: 95.31%] [G loss: 0.423262]\n",
      "epoch:27 step:21768 [D loss: 0.016945, acc.: 100.00%] [G loss: 0.291571]\n",
      "epoch:27 step:21769 [D loss: 0.004499, acc.: 100.00%] [G loss: 0.101486]\n",
      "epoch:27 step:21770 [D loss: 0.000871, acc.: 100.00%] [G loss: 0.035914]\n",
      "epoch:27 step:21771 [D loss: 0.000666, acc.: 100.00%] [G loss: 0.116620]\n",
      "epoch:27 step:21772 [D loss: 0.007627, acc.: 100.00%] [G loss: 0.008268]\n",
      "epoch:27 step:21773 [D loss: 0.001443, acc.: 100.00%] [G loss: 0.010512]\n",
      "epoch:27 step:21774 [D loss: 0.015273, acc.: 99.22%] [G loss: 0.005919]\n",
      "epoch:27 step:21775 [D loss: 0.002993, acc.: 100.00%] [G loss: 0.179853]\n",
      "epoch:27 step:21776 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.001379]\n",
      "epoch:27 step:21777 [D loss: 0.041061, acc.: 97.66%] [G loss: 0.111102]\n",
      "epoch:27 step:21778 [D loss: 0.000966, acc.: 100.00%] [G loss: 0.859268]\n",
      "epoch:27 step:21779 [D loss: 0.088837, acc.: 96.88%] [G loss: 0.007372]\n",
      "epoch:27 step:21780 [D loss: 0.005339, acc.: 100.00%] [G loss: 0.016334]\n",
      "epoch:27 step:21781 [D loss: 0.001099, acc.: 100.00%] [G loss: 0.011453]\n",
      "epoch:27 step:21782 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.008882]\n",
      "epoch:27 step:21783 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.008296]\n",
      "epoch:27 step:21784 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.003497]\n",
      "epoch:27 step:21785 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.003727]\n",
      "epoch:27 step:21786 [D loss: 0.013422, acc.: 99.22%] [G loss: 0.110272]\n",
      "epoch:27 step:21787 [D loss: 0.003031, acc.: 100.00%] [G loss: 0.109135]\n",
      "epoch:27 step:21788 [D loss: 0.010561, acc.: 100.00%] [G loss: 0.006466]\n",
      "epoch:27 step:21789 [D loss: 0.012958, acc.: 100.00%] [G loss: 0.011696]\n",
      "epoch:27 step:21790 [D loss: 0.000496, acc.: 100.00%] [G loss: 0.020871]\n",
      "epoch:27 step:21791 [D loss: 0.027116, acc.: 99.22%] [G loss: 0.013751]\n",
      "epoch:27 step:21792 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.002097]\n",
      "epoch:27 step:21793 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.001935]\n",
      "epoch:27 step:21794 [D loss: 0.004240, acc.: 100.00%] [G loss: 0.001229]\n",
      "epoch:27 step:21795 [D loss: 0.001060, acc.: 100.00%] [G loss: 0.006109]\n",
      "epoch:27 step:21796 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.001451]\n",
      "epoch:27 step:21797 [D loss: 0.005095, acc.: 100.00%] [G loss: 0.004587]\n",
      "epoch:27 step:21798 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.218926]\n",
      "epoch:27 step:21799 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.003922]\n",
      "epoch:27 step:21800 [D loss: 0.071323, acc.: 96.88%] [G loss: 0.001409]\n",
      "epoch:27 step:21801 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:27 step:21802 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:27 step:21803 [D loss: 0.003978, acc.: 100.00%] [G loss: 0.000437]\n",
      "epoch:27 step:21804 [D loss: 0.002641, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:27 step:21805 [D loss: 0.001607, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:27 step:21806 [D loss: 0.000638, acc.: 100.00%] [G loss: 0.000721]\n",
      "epoch:27 step:21807 [D loss: 0.113091, acc.: 96.88%] [G loss: 0.396661]\n",
      "epoch:27 step:21808 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.797664]\n",
      "epoch:27 step:21809 [D loss: 0.023238, acc.: 98.44%] [G loss: 0.247513]\n",
      "epoch:27 step:21810 [D loss: 0.008517, acc.: 100.00%] [G loss: 0.210040]\n",
      "epoch:27 step:21811 [D loss: 0.002816, acc.: 100.00%] [G loss: 0.041550]\n",
      "epoch:27 step:21812 [D loss: 0.082938, acc.: 95.31%] [G loss: 0.004659]\n",
      "epoch:27 step:21813 [D loss: 0.000717, acc.: 100.00%] [G loss: 0.004640]\n",
      "epoch:27 step:21814 [D loss: 0.004205, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:27 step:21815 [D loss: 0.015916, acc.: 100.00%] [G loss: 0.001431]\n",
      "epoch:27 step:21816 [D loss: 0.003908, acc.: 100.00%] [G loss: 0.011283]\n",
      "epoch:27 step:21817 [D loss: 0.003481, acc.: 100.00%] [G loss: 0.121823]\n",
      "epoch:27 step:21818 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.013027]\n",
      "epoch:27 step:21819 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.014439]\n",
      "epoch:27 step:21820 [D loss: 0.001235, acc.: 100.00%] [G loss: 0.004098]\n",
      "epoch:27 step:21821 [D loss: 0.007562, acc.: 100.00%] [G loss: 0.009219]\n",
      "epoch:27 step:21822 [D loss: 0.000621, acc.: 100.00%] [G loss: 0.021571]\n",
      "epoch:27 step:21823 [D loss: 0.001131, acc.: 100.00%] [G loss: 0.005744]\n",
      "epoch:27 step:21824 [D loss: 0.002007, acc.: 100.00%] [G loss: 0.069964]\n",
      "epoch:27 step:21825 [D loss: 0.017330, acc.: 100.00%] [G loss: 0.084766]\n",
      "epoch:27 step:21826 [D loss: 0.000760, acc.: 100.00%] [G loss: 0.111447]\n",
      "epoch:27 step:21827 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.230168]\n",
      "epoch:27 step:21828 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.097924]\n",
      "epoch:27 step:21829 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.065296]\n",
      "epoch:27 step:21830 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.019443]\n",
      "epoch:27 step:21831 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.059282]\n",
      "epoch:27 step:21832 [D loss: 0.000013, acc.: 100.00%] [G loss: 3.496531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:27 step:21833 [D loss: 0.002183, acc.: 100.00%] [G loss: 0.014351]\n",
      "epoch:27 step:21834 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.061467]\n",
      "epoch:27 step:21835 [D loss: 0.003103, acc.: 100.00%] [G loss: 2.577213]\n",
      "epoch:27 step:21836 [D loss: 0.323907, acc.: 85.94%] [G loss: 7.041713]\n",
      "epoch:27 step:21837 [D loss: 1.039384, acc.: 63.28%] [G loss: 2.721608]\n",
      "epoch:27 step:21838 [D loss: 0.226292, acc.: 89.84%] [G loss: 5.483919]\n",
      "epoch:27 step:21839 [D loss: 0.018005, acc.: 100.00%] [G loss: 7.264678]\n",
      "epoch:27 step:21840 [D loss: 0.108303, acc.: 96.09%] [G loss: 0.067311]\n",
      "epoch:27 step:21841 [D loss: 0.010646, acc.: 100.00%] [G loss: 5.131567]\n",
      "epoch:27 step:21842 [D loss: 0.000775, acc.: 100.00%] [G loss: 4.266889]\n",
      "epoch:27 step:21843 [D loss: 0.010643, acc.: 100.00%] [G loss: 2.821135]\n",
      "epoch:27 step:21844 [D loss: 0.035316, acc.: 99.22%] [G loss: 2.750485]\n",
      "epoch:27 step:21845 [D loss: 0.002050, acc.: 100.00%] [G loss: 2.807355]\n",
      "epoch:27 step:21846 [D loss: 0.019061, acc.: 99.22%] [G loss: 2.337788]\n",
      "epoch:27 step:21847 [D loss: 0.007489, acc.: 100.00%] [G loss: 1.792839]\n",
      "epoch:27 step:21848 [D loss: 0.062089, acc.: 97.66%] [G loss: 4.322986]\n",
      "epoch:27 step:21849 [D loss: 0.049000, acc.: 99.22%] [G loss: 5.150568]\n",
      "epoch:27 step:21850 [D loss: 0.127826, acc.: 94.53%] [G loss: 4.075006]\n",
      "epoch:27 step:21851 [D loss: 0.041798, acc.: 100.00%] [G loss: 6.137647]\n",
      "epoch:27 step:21852 [D loss: 0.002784, acc.: 100.00%] [G loss: 6.934267]\n",
      "epoch:27 step:21853 [D loss: 0.005441, acc.: 100.00%] [G loss: 0.129985]\n",
      "epoch:27 step:21854 [D loss: 0.002430, acc.: 100.00%] [G loss: 5.556346]\n",
      "epoch:27 step:21855 [D loss: 0.006640, acc.: 100.00%] [G loss: 5.888937]\n",
      "epoch:27 step:21856 [D loss: 0.000814, acc.: 100.00%] [G loss: 5.432786]\n",
      "epoch:27 step:21857 [D loss: 0.003020, acc.: 100.00%] [G loss: 4.001840]\n",
      "epoch:27 step:21858 [D loss: 0.076154, acc.: 99.22%] [G loss: 5.527969]\n",
      "epoch:27 step:21859 [D loss: 0.000624, acc.: 100.00%] [G loss: 7.966485]\n",
      "epoch:27 step:21860 [D loss: 0.002846, acc.: 100.00%] [G loss: 6.843262]\n",
      "epoch:27 step:21861 [D loss: 0.027190, acc.: 97.66%] [G loss: 7.222439]\n",
      "epoch:27 step:21862 [D loss: 0.004398, acc.: 100.00%] [G loss: 6.952349]\n",
      "epoch:27 step:21863 [D loss: 0.002279, acc.: 100.00%] [G loss: 6.626422]\n",
      "epoch:27 step:21864 [D loss: 0.053372, acc.: 97.66%] [G loss: 0.141116]\n",
      "epoch:27 step:21865 [D loss: 0.029290, acc.: 98.44%] [G loss: 0.319315]\n",
      "epoch:27 step:21866 [D loss: 0.266211, acc.: 89.06%] [G loss: 6.914124]\n",
      "epoch:27 step:21867 [D loss: 0.000521, acc.: 100.00%] [G loss: 5.091707]\n",
      "epoch:27 step:21868 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.041645]\n",
      "epoch:28 step:21869 [D loss: 0.000588, acc.: 100.00%] [G loss: 4.594542]\n",
      "epoch:28 step:21870 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.000343]\n",
      "epoch:28 step:21871 [D loss: 0.009703, acc.: 100.00%] [G loss: 0.000311]\n",
      "epoch:28 step:21872 [D loss: 0.033143, acc.: 100.00%] [G loss: 2.444709]\n",
      "epoch:28 step:21873 [D loss: 0.000151, acc.: 100.00%] [G loss: 2.394243]\n",
      "epoch:28 step:21874 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.004389]\n",
      "epoch:28 step:21875 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.067670]\n",
      "epoch:28 step:21876 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.690317]\n",
      "epoch:28 step:21877 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.266947]\n",
      "epoch:28 step:21878 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.367013]\n",
      "epoch:28 step:21879 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.000645]\n",
      "epoch:28 step:21880 [D loss: 0.014609, acc.: 100.00%] [G loss: 0.099002]\n",
      "epoch:28 step:21881 [D loss: 0.000740, acc.: 100.00%] [G loss: 0.097349]\n",
      "epoch:28 step:21882 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.173026]\n",
      "epoch:28 step:21883 [D loss: 0.016564, acc.: 99.22%] [G loss: 0.085918]\n",
      "epoch:28 step:21884 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.021586]\n",
      "epoch:28 step:21885 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.003370]\n",
      "epoch:28 step:21886 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.005961]\n",
      "epoch:28 step:21887 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.006011]\n",
      "epoch:28 step:21888 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.059286]\n",
      "epoch:28 step:21889 [D loss: 0.000449, acc.: 100.00%] [G loss: 0.050987]\n",
      "epoch:28 step:21890 [D loss: 0.007253, acc.: 100.00%] [G loss: 0.001956]\n",
      "epoch:28 step:21891 [D loss: 0.010253, acc.: 100.00%] [G loss: 0.000929]\n",
      "epoch:28 step:21892 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.004178]\n",
      "epoch:28 step:21893 [D loss: 0.003495, acc.: 100.00%] [G loss: 0.206711]\n",
      "epoch:28 step:21894 [D loss: 0.022459, acc.: 100.00%] [G loss: 0.031966]\n",
      "epoch:28 step:21895 [D loss: 0.021001, acc.: 100.00%] [G loss: 0.069958]\n",
      "epoch:28 step:21896 [D loss: 0.005822, acc.: 100.00%] [G loss: 0.058399]\n",
      "epoch:28 step:21897 [D loss: 0.016473, acc.: 99.22%] [G loss: 0.151355]\n",
      "epoch:28 step:21898 [D loss: 0.002230, acc.: 100.00%] [G loss: 0.035993]\n",
      "epoch:28 step:21899 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.025566]\n",
      "epoch:28 step:21900 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.019678]\n",
      "epoch:28 step:21901 [D loss: 0.002214, acc.: 100.00%] [G loss: 0.071056]\n",
      "epoch:28 step:21902 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.004900]\n",
      "epoch:28 step:21903 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.080112]\n",
      "epoch:28 step:21904 [D loss: 0.006800, acc.: 100.00%] [G loss: 0.003498]\n",
      "epoch:28 step:21905 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.032868]\n",
      "epoch:28 step:21906 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.003109]\n",
      "epoch:28 step:21907 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.001657]\n",
      "epoch:28 step:21908 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:28 step:21909 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.008624]\n",
      "epoch:28 step:21910 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.038962]\n",
      "epoch:28 step:21911 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000352]\n",
      "epoch:28 step:21912 [D loss: 0.002234, acc.: 100.00%] [G loss: 0.000760]\n",
      "epoch:28 step:21913 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:28 step:21914 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:28 step:21915 [D loss: 0.000709, acc.: 100.00%] [G loss: 3.300332]\n",
      "epoch:28 step:21916 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.329031]\n",
      "epoch:28 step:21917 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.223632]\n",
      "epoch:28 step:21918 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.104068]\n",
      "epoch:28 step:21919 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.008739]\n",
      "epoch:28 step:21920 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.004040]\n",
      "epoch:28 step:21921 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000910]\n",
      "epoch:28 step:21922 [D loss: 0.000427, acc.: 100.00%] [G loss: 0.008825]\n",
      "epoch:28 step:21923 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.001277]\n",
      "epoch:28 step:21924 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.001365]\n",
      "epoch:28 step:21925 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.007940]\n",
      "epoch:28 step:21926 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.002119]\n",
      "epoch:28 step:21927 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.001203]\n",
      "epoch:28 step:21928 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000854]\n",
      "epoch:28 step:21929 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.001287]\n",
      "epoch:28 step:21930 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.113952]\n",
      "epoch:28 step:21931 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.001618]\n",
      "epoch:28 step:21932 [D loss: 0.002484, acc.: 100.00%] [G loss: 0.000674]\n",
      "epoch:28 step:21933 [D loss: 0.004222, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:28 step:21934 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000830]\n",
      "epoch:28 step:21935 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:28 step:21936 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.000499]\n",
      "epoch:28 step:21937 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:28 step:21938 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.008589]\n",
      "epoch:28 step:21939 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.057188]\n",
      "epoch:28 step:21940 [D loss: 0.001157, acc.: 100.00%] [G loss: 0.002062]\n",
      "epoch:28 step:21941 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.000707]\n",
      "epoch:28 step:21942 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.001159]\n",
      "epoch:28 step:21943 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:28 step:21944 [D loss: 0.002505, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:28 step:21945 [D loss: 0.003933, acc.: 100.00%] [G loss: 0.003732]\n",
      "epoch:28 step:21946 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.002660]\n",
      "epoch:28 step:21947 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000817]\n",
      "epoch:28 step:21948 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.002123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:21949 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.001201]\n",
      "epoch:28 step:21950 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.001005]\n",
      "epoch:28 step:21951 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:28 step:21952 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.001752]\n",
      "epoch:28 step:21953 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:28 step:21954 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:28 step:21955 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:28 step:21956 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000390]\n",
      "epoch:28 step:21957 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000430]\n",
      "epoch:28 step:21958 [D loss: 0.016996, acc.: 100.00%] [G loss: 0.000664]\n",
      "epoch:28 step:21959 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000308]\n",
      "epoch:28 step:21960 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.004380]\n",
      "epoch:28 step:21961 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:28 step:21962 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:28 step:21963 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.001511]\n",
      "epoch:28 step:21964 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000412]\n",
      "epoch:28 step:21965 [D loss: 0.007709, acc.: 99.22%] [G loss: 0.001031]\n",
      "epoch:28 step:21966 [D loss: 0.003622, acc.: 100.00%] [G loss: 0.000836]\n",
      "epoch:28 step:21967 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.012208]\n",
      "epoch:28 step:21968 [D loss: 0.035546, acc.: 100.00%] [G loss: 0.049281]\n",
      "epoch:28 step:21969 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.004652]\n",
      "epoch:28 step:21970 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.823633]\n",
      "epoch:28 step:21971 [D loss: 0.004452, acc.: 100.00%] [G loss: 0.101965]\n",
      "epoch:28 step:21972 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.067173]\n",
      "epoch:28 step:21973 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.053875]\n",
      "epoch:28 step:21974 [D loss: 0.000983, acc.: 100.00%] [G loss: 0.072383]\n",
      "epoch:28 step:21975 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.005898]\n",
      "epoch:28 step:21976 [D loss: 0.001739, acc.: 100.00%] [G loss: 0.050920]\n",
      "epoch:28 step:21977 [D loss: 0.000997, acc.: 100.00%] [G loss: 0.015161]\n",
      "epoch:28 step:21978 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.004407]\n",
      "epoch:28 step:21979 [D loss: 0.021663, acc.: 100.00%] [G loss: 3.544470]\n",
      "epoch:28 step:21980 [D loss: 0.055556, acc.: 100.00%] [G loss: 2.817267]\n",
      "epoch:28 step:21981 [D loss: 0.001184, acc.: 100.00%] [G loss: 5.068447]\n",
      "epoch:28 step:21982 [D loss: 0.056356, acc.: 98.44%] [G loss: 0.028259]\n",
      "epoch:28 step:21983 [D loss: 0.000331, acc.: 100.00%] [G loss: 2.581332]\n",
      "epoch:28 step:21984 [D loss: 0.005210, acc.: 100.00%] [G loss: 0.955078]\n",
      "epoch:28 step:21985 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.024043]\n",
      "epoch:28 step:21986 [D loss: 0.001815, acc.: 100.00%] [G loss: 0.178011]\n",
      "epoch:28 step:21987 [D loss: 0.002274, acc.: 100.00%] [G loss: 0.081821]\n",
      "epoch:28 step:21988 [D loss: 0.001243, acc.: 100.00%] [G loss: 0.014022]\n",
      "epoch:28 step:21989 [D loss: 0.025073, acc.: 100.00%] [G loss: 0.904144]\n",
      "epoch:28 step:21990 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.519339]\n",
      "epoch:28 step:21991 [D loss: 0.001587, acc.: 100.00%] [G loss: 0.952913]\n",
      "epoch:28 step:21992 [D loss: 0.015743, acc.: 100.00%] [G loss: 0.001693]\n",
      "epoch:28 step:21993 [D loss: 0.003051, acc.: 100.00%] [G loss: 0.088780]\n",
      "epoch:28 step:21994 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.265767]\n",
      "epoch:28 step:21995 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.120139]\n",
      "epoch:28 step:21996 [D loss: 0.001518, acc.: 100.00%] [G loss: 0.086724]\n",
      "epoch:28 step:21997 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.001564]\n",
      "epoch:28 step:21998 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.002737]\n",
      "epoch:28 step:21999 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.001913]\n",
      "epoch:28 step:22000 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.004825]\n",
      "epoch:28 step:22001 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.006354]\n",
      "epoch:28 step:22002 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.006840]\n",
      "epoch:28 step:22003 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.002375]\n",
      "epoch:28 step:22004 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.010590]\n",
      "epoch:28 step:22005 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.049370]\n",
      "epoch:28 step:22006 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.022319]\n",
      "epoch:28 step:22007 [D loss: 0.000136, acc.: 100.00%] [G loss: 6.183653]\n",
      "epoch:28 step:22008 [D loss: 0.003658, acc.: 100.00%] [G loss: 0.071936]\n",
      "epoch:28 step:22009 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:28 step:22010 [D loss: 0.000629, acc.: 100.00%] [G loss: 0.069908]\n",
      "epoch:28 step:22011 [D loss: 0.004215, acc.: 100.00%] [G loss: 0.120813]\n",
      "epoch:28 step:22012 [D loss: 0.000833, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:28 step:22013 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.028011]\n",
      "epoch:28 step:22014 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.134620]\n",
      "epoch:28 step:22015 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.003644]\n",
      "epoch:28 step:22016 [D loss: 0.001571, acc.: 100.00%] [G loss: 0.008798]\n",
      "epoch:28 step:22017 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000903]\n",
      "epoch:28 step:22018 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:28 step:22019 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.002948]\n",
      "epoch:28 step:22020 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.005529]\n",
      "epoch:28 step:22021 [D loss: 0.009634, acc.: 100.00%] [G loss: 0.000259]\n",
      "epoch:28 step:22022 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.001844]\n",
      "epoch:28 step:22023 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.039618]\n",
      "epoch:28 step:22024 [D loss: 0.001749, acc.: 100.00%] [G loss: 0.002390]\n",
      "epoch:28 step:22025 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.519626]\n",
      "epoch:28 step:22026 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:28 step:22027 [D loss: 0.000525, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:28 step:22028 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.021627]\n",
      "epoch:28 step:22029 [D loss: 0.001593, acc.: 100.00%] [G loss: 0.003087]\n",
      "epoch:28 step:22030 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.003850]\n",
      "epoch:28 step:22031 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22032 [D loss: 0.004371, acc.: 100.00%] [G loss: 0.003195]\n",
      "epoch:28 step:22033 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:28 step:22034 [D loss: 0.013757, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:28 step:22035 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.142659]\n",
      "epoch:28 step:22036 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.001775]\n",
      "epoch:28 step:22037 [D loss: 0.048541, acc.: 99.22%] [G loss: 2.817219]\n",
      "epoch:28 step:22038 [D loss: 0.024156, acc.: 99.22%] [G loss: 0.443094]\n",
      "epoch:28 step:22039 [D loss: 0.031964, acc.: 98.44%] [G loss: 0.259831]\n",
      "epoch:28 step:22040 [D loss: 0.416992, acc.: 84.38%] [G loss: 6.650526]\n",
      "epoch:28 step:22041 [D loss: 2.834777, acc.: 50.78%] [G loss: 0.522088]\n",
      "epoch:28 step:22042 [D loss: 0.338740, acc.: 87.50%] [G loss: 1.157438]\n",
      "epoch:28 step:22043 [D loss: 0.006978, acc.: 100.00%] [G loss: 0.439304]\n",
      "epoch:28 step:22044 [D loss: 0.081142, acc.: 96.09%] [G loss: 0.093281]\n",
      "epoch:28 step:22045 [D loss: 0.029326, acc.: 99.22%] [G loss: 0.028275]\n",
      "epoch:28 step:22046 [D loss: 0.236270, acc.: 92.19%] [G loss: 0.054609]\n",
      "epoch:28 step:22047 [D loss: 0.395098, acc.: 84.38%] [G loss: 0.109397]\n",
      "epoch:28 step:22048 [D loss: 0.071401, acc.: 99.22%] [G loss: 1.312520]\n",
      "epoch:28 step:22049 [D loss: 0.029744, acc.: 99.22%] [G loss: 0.363395]\n",
      "epoch:28 step:22050 [D loss: 0.003755, acc.: 100.00%] [G loss: 0.158946]\n",
      "epoch:28 step:22051 [D loss: 0.003051, acc.: 100.00%] [G loss: 0.063964]\n",
      "epoch:28 step:22052 [D loss: 0.023951, acc.: 100.00%] [G loss: 0.054920]\n",
      "epoch:28 step:22053 [D loss: 0.000799, acc.: 100.00%] [G loss: 0.028602]\n",
      "epoch:28 step:22054 [D loss: 0.004710, acc.: 100.00%] [G loss: 0.053116]\n",
      "epoch:28 step:22055 [D loss: 0.012834, acc.: 100.00%] [G loss: 0.089979]\n",
      "epoch:28 step:22056 [D loss: 0.002745, acc.: 100.00%] [G loss: 0.043518]\n",
      "epoch:28 step:22057 [D loss: 0.003576, acc.: 100.00%] [G loss: 0.028527]\n",
      "epoch:28 step:22058 [D loss: 0.029678, acc.: 100.00%] [G loss: 2.138241]\n",
      "epoch:28 step:22059 [D loss: 0.248005, acc.: 89.84%] [G loss: 1.923725]\n",
      "epoch:28 step:22060 [D loss: 0.034793, acc.: 99.22%] [G loss: 4.732168]\n",
      "epoch:28 step:22061 [D loss: 0.210789, acc.: 92.19%] [G loss: 1.071155]\n",
      "epoch:28 step:22062 [D loss: 0.001346, acc.: 100.00%] [G loss: 0.002435]\n",
      "epoch:28 step:22063 [D loss: 0.009677, acc.: 100.00%] [G loss: 0.005240]\n",
      "epoch:28 step:22064 [D loss: 0.022965, acc.: 100.00%] [G loss: 0.130422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22065 [D loss: 0.021681, acc.: 100.00%] [G loss: 3.753678]\n",
      "epoch:28 step:22066 [D loss: 0.000668, acc.: 100.00%] [G loss: 1.530912]\n",
      "epoch:28 step:22067 [D loss: 0.016558, acc.: 100.00%] [G loss: 1.132646]\n",
      "epoch:28 step:22068 [D loss: 0.020694, acc.: 100.00%] [G loss: 1.949773]\n",
      "epoch:28 step:22069 [D loss: 0.017059, acc.: 100.00%] [G loss: 0.751579]\n",
      "epoch:28 step:22070 [D loss: 0.017516, acc.: 100.00%] [G loss: 0.775025]\n",
      "epoch:28 step:22071 [D loss: 0.047710, acc.: 100.00%] [G loss: 1.497185]\n",
      "epoch:28 step:22072 [D loss: 0.103522, acc.: 95.31%] [G loss: 0.690745]\n",
      "epoch:28 step:22073 [D loss: 0.007888, acc.: 100.00%] [G loss: 3.530200]\n",
      "epoch:28 step:22074 [D loss: 0.183474, acc.: 91.41%] [G loss: 1.167056]\n",
      "epoch:28 step:22075 [D loss: 0.145310, acc.: 92.97%] [G loss: 0.377905]\n",
      "epoch:28 step:22076 [D loss: 0.006737, acc.: 100.00%] [G loss: 0.355491]\n",
      "epoch:28 step:22077 [D loss: 0.024658, acc.: 100.00%] [G loss: 0.060385]\n",
      "epoch:28 step:22078 [D loss: 0.010048, acc.: 100.00%] [G loss: 0.017122]\n",
      "epoch:28 step:22079 [D loss: 0.007122, acc.: 100.00%] [G loss: 0.315776]\n",
      "epoch:28 step:22080 [D loss: 0.003237, acc.: 100.00%] [G loss: 0.074297]\n",
      "epoch:28 step:22081 [D loss: 0.004562, acc.: 100.00%] [G loss: 0.134932]\n",
      "epoch:28 step:22082 [D loss: 0.032781, acc.: 100.00%] [G loss: 0.054637]\n",
      "epoch:28 step:22083 [D loss: 0.002782, acc.: 100.00%] [G loss: 0.148154]\n",
      "epoch:28 step:22084 [D loss: 0.005733, acc.: 100.00%] [G loss: 0.233039]\n",
      "epoch:28 step:22085 [D loss: 0.002770, acc.: 100.00%] [G loss: 0.118438]\n",
      "epoch:28 step:22086 [D loss: 0.005671, acc.: 100.00%] [G loss: 0.671754]\n",
      "epoch:28 step:22087 [D loss: 0.005635, acc.: 100.00%] [G loss: 0.531751]\n",
      "epoch:28 step:22088 [D loss: 0.008042, acc.: 100.00%] [G loss: 1.253866]\n",
      "epoch:28 step:22089 [D loss: 0.003057, acc.: 100.00%] [G loss: 1.482295]\n",
      "epoch:28 step:22090 [D loss: 0.001313, acc.: 100.00%] [G loss: 0.600425]\n",
      "epoch:28 step:22091 [D loss: 0.002600, acc.: 100.00%] [G loss: 1.086438]\n",
      "epoch:28 step:22092 [D loss: 0.003157, acc.: 100.00%] [G loss: 3.002997]\n",
      "epoch:28 step:22093 [D loss: 0.000170, acc.: 100.00%] [G loss: 2.327140]\n",
      "epoch:28 step:22094 [D loss: 0.002147, acc.: 100.00%] [G loss: 1.551180]\n",
      "epoch:28 step:22095 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.564453]\n",
      "epoch:28 step:22096 [D loss: 0.001109, acc.: 100.00%] [G loss: 0.389796]\n",
      "epoch:28 step:22097 [D loss: 0.002195, acc.: 100.00%] [G loss: 0.154327]\n",
      "epoch:28 step:22098 [D loss: 0.000944, acc.: 100.00%] [G loss: 0.090984]\n",
      "epoch:28 step:22099 [D loss: 0.002419, acc.: 100.00%] [G loss: 0.174624]\n",
      "epoch:28 step:22100 [D loss: 0.001918, acc.: 100.00%] [G loss: 0.188380]\n",
      "epoch:28 step:22101 [D loss: 0.001785, acc.: 100.00%] [G loss: 0.268602]\n",
      "epoch:28 step:22102 [D loss: 0.002507, acc.: 100.00%] [G loss: 0.031386]\n",
      "epoch:28 step:22103 [D loss: 0.005450, acc.: 100.00%] [G loss: 0.207890]\n",
      "epoch:28 step:22104 [D loss: 0.004461, acc.: 100.00%] [G loss: 0.257535]\n",
      "epoch:28 step:22105 [D loss: 0.002386, acc.: 100.00%] [G loss: 0.139121]\n",
      "epoch:28 step:22106 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.286558]\n",
      "epoch:28 step:22107 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.748303]\n",
      "epoch:28 step:22108 [D loss: 0.008779, acc.: 100.00%] [G loss: 0.244329]\n",
      "epoch:28 step:22109 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.292306]\n",
      "epoch:28 step:22110 [D loss: 0.001353, acc.: 100.00%] [G loss: 0.034390]\n",
      "epoch:28 step:22111 [D loss: 0.001256, acc.: 100.00%] [G loss: 1.327132]\n",
      "epoch:28 step:22112 [D loss: 0.001583, acc.: 100.00%] [G loss: 2.440528]\n",
      "epoch:28 step:22113 [D loss: 0.000956, acc.: 100.00%] [G loss: 0.367553]\n",
      "epoch:28 step:22114 [D loss: 0.001382, acc.: 100.00%] [G loss: 0.093712]\n",
      "epoch:28 step:22115 [D loss: 0.012093, acc.: 99.22%] [G loss: 0.826467]\n",
      "epoch:28 step:22116 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.063869]\n",
      "epoch:28 step:22117 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.265989]\n",
      "epoch:28 step:22118 [D loss: 0.007928, acc.: 100.00%] [G loss: 0.014874]\n",
      "epoch:28 step:22119 [D loss: 0.001242, acc.: 100.00%] [G loss: 0.045494]\n",
      "epoch:28 step:22120 [D loss: 0.001684, acc.: 100.00%] [G loss: 0.053770]\n",
      "epoch:28 step:22121 [D loss: 0.002804, acc.: 100.00%] [G loss: 0.007689]\n",
      "epoch:28 step:22122 [D loss: 0.002431, acc.: 100.00%] [G loss: 0.935088]\n",
      "epoch:28 step:22123 [D loss: 0.002408, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:28 step:22124 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.089929]\n",
      "epoch:28 step:22125 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.044569]\n",
      "epoch:28 step:22126 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:28 step:22127 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:28 step:22128 [D loss: 0.004247, acc.: 100.00%] [G loss: 5.436093]\n",
      "epoch:28 step:22129 [D loss: 0.001427, acc.: 100.00%] [G loss: 0.113964]\n",
      "epoch:28 step:22130 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000424]\n",
      "epoch:28 step:22131 [D loss: 0.003298, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:28 step:22132 [D loss: 0.010205, acc.: 100.00%] [G loss: 0.195652]\n",
      "epoch:28 step:22133 [D loss: 0.013072, acc.: 100.00%] [G loss: 0.026118]\n",
      "epoch:28 step:22134 [D loss: 0.000730, acc.: 100.00%] [G loss: 0.125825]\n",
      "epoch:28 step:22135 [D loss: 0.001195, acc.: 100.00%] [G loss: 0.780815]\n",
      "epoch:28 step:22136 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.038990]\n",
      "epoch:28 step:22137 [D loss: 0.010567, acc.: 100.00%] [G loss: 0.077343]\n",
      "epoch:28 step:22138 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.039809]\n",
      "epoch:28 step:22139 [D loss: 0.003090, acc.: 100.00%] [G loss: 0.162259]\n",
      "epoch:28 step:22140 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.062887]\n",
      "epoch:28 step:22141 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.014445]\n",
      "epoch:28 step:22142 [D loss: 0.003898, acc.: 100.00%] [G loss: 0.089106]\n",
      "epoch:28 step:22143 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.002374]\n",
      "epoch:28 step:22144 [D loss: 0.000548, acc.: 100.00%] [G loss: 0.032204]\n",
      "epoch:28 step:22145 [D loss: 0.001544, acc.: 100.00%] [G loss: 0.004576]\n",
      "epoch:28 step:22146 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.004076]\n",
      "epoch:28 step:22147 [D loss: 0.001198, acc.: 100.00%] [G loss: 0.000944]\n",
      "epoch:28 step:22148 [D loss: 0.000516, acc.: 100.00%] [G loss: 0.001870]\n",
      "epoch:28 step:22149 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.002872]\n",
      "epoch:28 step:22150 [D loss: 0.003966, acc.: 100.00%] [G loss: 0.005288]\n",
      "epoch:28 step:22151 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.002920]\n",
      "epoch:28 step:22152 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.089267]\n",
      "epoch:28 step:22153 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.001459]\n",
      "epoch:28 step:22154 [D loss: 0.005938, acc.: 100.00%] [G loss: 0.001831]\n",
      "epoch:28 step:22155 [D loss: 0.004765, acc.: 100.00%] [G loss: 0.058574]\n",
      "epoch:28 step:22156 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.048989]\n",
      "epoch:28 step:22157 [D loss: 0.002547, acc.: 100.00%] [G loss: 0.016586]\n",
      "epoch:28 step:22158 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.003073]\n",
      "epoch:28 step:22159 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.002717]\n",
      "epoch:28 step:22160 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.016780]\n",
      "epoch:28 step:22161 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.304590]\n",
      "epoch:28 step:22162 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.005131]\n",
      "epoch:28 step:22163 [D loss: 0.006926, acc.: 100.00%] [G loss: 0.167606]\n",
      "epoch:28 step:22164 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.001758]\n",
      "epoch:28 step:22165 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000909]\n",
      "epoch:28 step:22166 [D loss: 0.000479, acc.: 100.00%] [G loss: 0.028936]\n",
      "epoch:28 step:22167 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000642]\n",
      "epoch:28 step:22168 [D loss: 0.000814, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:28 step:22169 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.001026]\n",
      "epoch:28 step:22170 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.004872]\n",
      "epoch:28 step:22171 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.012971]\n",
      "epoch:28 step:22172 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000321]\n",
      "epoch:28 step:22173 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.005767]\n",
      "epoch:28 step:22174 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:28 step:22175 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.001446]\n",
      "epoch:28 step:22176 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.031689]\n",
      "epoch:28 step:22177 [D loss: 0.001185, acc.: 100.00%] [G loss: 0.005816]\n",
      "epoch:28 step:22178 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.002311]\n",
      "epoch:28 step:22179 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.001164]\n",
      "epoch:28 step:22180 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.002701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22181 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.009431]\n",
      "epoch:28 step:22182 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.001528]\n",
      "epoch:28 step:22183 [D loss: 0.024375, acc.: 99.22%] [G loss: 0.000346]\n",
      "epoch:28 step:22184 [D loss: 0.033128, acc.: 98.44%] [G loss: 0.000240]\n",
      "epoch:28 step:22185 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.025595]\n",
      "epoch:28 step:22186 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000479]\n",
      "epoch:28 step:22187 [D loss: 0.002210, acc.: 100.00%] [G loss: 0.000748]\n",
      "epoch:28 step:22188 [D loss: 0.009311, acc.: 99.22%] [G loss: 0.000309]\n",
      "epoch:28 step:22189 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.350580]\n",
      "epoch:28 step:22190 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.004552]\n",
      "epoch:28 step:22191 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:28 step:22192 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:28 step:22193 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:28 step:22194 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.004006]\n",
      "epoch:28 step:22195 [D loss: 0.000824, acc.: 100.00%] [G loss: 0.000777]\n",
      "epoch:28 step:22196 [D loss: 0.000903, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:28 step:22197 [D loss: 0.007650, acc.: 100.00%] [G loss: 0.006246]\n",
      "epoch:28 step:22198 [D loss: 0.014148, acc.: 100.00%] [G loss: 0.003610]\n",
      "epoch:28 step:22199 [D loss: 0.002219, acc.: 100.00%] [G loss: 0.158260]\n",
      "epoch:28 step:22200 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.010709]\n",
      "epoch:28 step:22201 [D loss: 0.023804, acc.: 100.00%] [G loss: 0.100968]\n",
      "epoch:28 step:22202 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.173838]\n",
      "epoch:28 step:22203 [D loss: 0.000898, acc.: 100.00%] [G loss: 3.375689]\n",
      "epoch:28 step:22204 [D loss: 0.008627, acc.: 100.00%] [G loss: 0.425404]\n",
      "epoch:28 step:22205 [D loss: 0.014094, acc.: 100.00%] [G loss: 0.010226]\n",
      "epoch:28 step:22206 [D loss: 0.000702, acc.: 100.00%] [G loss: 0.582063]\n",
      "epoch:28 step:22207 [D loss: 0.001564, acc.: 100.00%] [G loss: 0.069956]\n",
      "epoch:28 step:22208 [D loss: 0.000588, acc.: 100.00%] [G loss: 0.135334]\n",
      "epoch:28 step:22209 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.253431]\n",
      "epoch:28 step:22210 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.112550]\n",
      "epoch:28 step:22211 [D loss: 0.000390, acc.: 100.00%] [G loss: 0.005423]\n",
      "epoch:28 step:22212 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.113331]\n",
      "epoch:28 step:22213 [D loss: 0.001586, acc.: 100.00%] [G loss: 0.001467]\n",
      "epoch:28 step:22214 [D loss: 0.001431, acc.: 100.00%] [G loss: 0.003824]\n",
      "epoch:28 step:22215 [D loss: 0.021329, acc.: 99.22%] [G loss: 0.007731]\n",
      "epoch:28 step:22216 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.003033]\n",
      "epoch:28 step:22217 [D loss: 0.001400, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:28 step:22218 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.012017]\n",
      "epoch:28 step:22219 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.008360]\n",
      "epoch:28 step:22220 [D loss: 0.006098, acc.: 100.00%] [G loss: 0.001051]\n",
      "epoch:28 step:22221 [D loss: 0.007215, acc.: 100.00%] [G loss: 0.151940]\n",
      "epoch:28 step:22222 [D loss: 0.003048, acc.: 100.00%] [G loss: 0.009830]\n",
      "epoch:28 step:22223 [D loss: 0.008274, acc.: 100.00%] [G loss: 0.005148]\n",
      "epoch:28 step:22224 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.010243]\n",
      "epoch:28 step:22225 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.028383]\n",
      "epoch:28 step:22226 [D loss: 0.000904, acc.: 100.00%] [G loss: 0.039849]\n",
      "epoch:28 step:22227 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.035684]\n",
      "epoch:28 step:22228 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.044894]\n",
      "epoch:28 step:22229 [D loss: 0.005309, acc.: 100.00%] [G loss: 0.032332]\n",
      "epoch:28 step:22230 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.070152]\n",
      "epoch:28 step:22231 [D loss: 0.000380, acc.: 100.00%] [G loss: 0.004753]\n",
      "epoch:28 step:22232 [D loss: 0.004796, acc.: 100.00%] [G loss: 0.004272]\n",
      "epoch:28 step:22233 [D loss: 0.001455, acc.: 100.00%] [G loss: 0.014624]\n",
      "epoch:28 step:22234 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.003327]\n",
      "epoch:28 step:22235 [D loss: 0.002140, acc.: 100.00%] [G loss: 0.015206]\n",
      "epoch:28 step:22236 [D loss: 0.000401, acc.: 100.00%] [G loss: 3.772038]\n",
      "epoch:28 step:22237 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.103891]\n",
      "epoch:28 step:22238 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.262667]\n",
      "epoch:28 step:22239 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.019488]\n",
      "epoch:28 step:22240 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.114760]\n",
      "epoch:28 step:22241 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.035419]\n",
      "epoch:28 step:22242 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.001033]\n",
      "epoch:28 step:22243 [D loss: 0.000447, acc.: 100.00%] [G loss: 0.021287]\n",
      "epoch:28 step:22244 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.045725]\n",
      "epoch:28 step:22245 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.006999]\n",
      "epoch:28 step:22246 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.005156]\n",
      "epoch:28 step:22247 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.010727]\n",
      "epoch:28 step:22248 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.009707]\n",
      "epoch:28 step:22249 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.003347]\n",
      "epoch:28 step:22250 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.032026]\n",
      "epoch:28 step:22251 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.032060]\n",
      "epoch:28 step:22252 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.002562]\n",
      "epoch:28 step:22253 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.006067]\n",
      "epoch:28 step:22254 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.158393]\n",
      "epoch:28 step:22255 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.096134]\n",
      "epoch:28 step:22256 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:28 step:22257 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.003658]\n",
      "epoch:28 step:22258 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.001324]\n",
      "epoch:28 step:22259 [D loss: 0.000836, acc.: 100.00%] [G loss: 0.004229]\n",
      "epoch:28 step:22260 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.002891]\n",
      "epoch:28 step:22261 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:28 step:22262 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.011608]\n",
      "epoch:28 step:22263 [D loss: 0.016319, acc.: 99.22%] [G loss: 0.000256]\n",
      "epoch:28 step:22264 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.001667]\n",
      "epoch:28 step:22265 [D loss: 0.002866, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:28 step:22266 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:28 step:22267 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:28 step:22268 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000585]\n",
      "epoch:28 step:22269 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:28 step:22270 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000245]\n",
      "epoch:28 step:22271 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:28 step:22272 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:28 step:22273 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:28 step:22274 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000348]\n",
      "epoch:28 step:22275 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:28 step:22276 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:28 step:22277 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:28 step:22278 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:28 step:22279 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:28 step:22280 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:28 step:22281 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000650]\n",
      "epoch:28 step:22282 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:28 step:22283 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:28 step:22284 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.002258]\n",
      "epoch:28 step:22285 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:28 step:22286 [D loss: 0.001263, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:28 step:22287 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:28 step:22288 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:28 step:22289 [D loss: 0.000649, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:28 step:22290 [D loss: 0.036659, acc.: 100.00%] [G loss: 0.050462]\n",
      "epoch:28 step:22291 [D loss: 0.000368, acc.: 100.00%] [G loss: 2.224975]\n",
      "epoch:28 step:22292 [D loss: 0.011427, acc.: 100.00%] [G loss: 0.219540]\n",
      "epoch:28 step:22293 [D loss: 0.011393, acc.: 100.00%] [G loss: 0.011570]\n",
      "epoch:28 step:22294 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.072753]\n",
      "epoch:28 step:22295 [D loss: 0.000917, acc.: 100.00%] [G loss: 0.005704]\n",
      "epoch:28 step:22296 [D loss: 0.246445, acc.: 89.84%] [G loss: 0.032598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22297 [D loss: 1.294976, acc.: 64.06%] [G loss: 0.029958]\n",
      "epoch:28 step:22298 [D loss: 0.401305, acc.: 83.59%] [G loss: 0.000302]\n",
      "epoch:28 step:22299 [D loss: 0.127681, acc.: 94.53%] [G loss: 0.137278]\n",
      "epoch:28 step:22300 [D loss: 0.205213, acc.: 93.75%] [G loss: 0.002492]\n",
      "epoch:28 step:22301 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.034069]\n",
      "epoch:28 step:22302 [D loss: 0.001302, acc.: 100.00%] [G loss: 0.003765]\n",
      "epoch:28 step:22303 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.003010]\n",
      "epoch:28 step:22304 [D loss: 0.001425, acc.: 100.00%] [G loss: 6.223275]\n",
      "epoch:28 step:22305 [D loss: 0.071523, acc.: 96.88%] [G loss: 0.006382]\n",
      "epoch:28 step:22306 [D loss: 0.025102, acc.: 99.22%] [G loss: 6.092585]\n",
      "epoch:28 step:22307 [D loss: 0.013916, acc.: 100.00%] [G loss: 0.019888]\n",
      "epoch:28 step:22308 [D loss: 0.121648, acc.: 96.09%] [G loss: 8.386630]\n",
      "epoch:28 step:22309 [D loss: 0.024935, acc.: 99.22%] [G loss: 0.014630]\n",
      "epoch:28 step:22310 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.011826]\n",
      "epoch:28 step:22311 [D loss: 0.110744, acc.: 97.66%] [G loss: 0.005170]\n",
      "epoch:28 step:22312 [D loss: 0.027965, acc.: 99.22%] [G loss: 0.004071]\n",
      "epoch:28 step:22313 [D loss: 0.008580, acc.: 100.00%] [G loss: 0.000464]\n",
      "epoch:28 step:22314 [D loss: 0.016847, acc.: 100.00%] [G loss: 5.353446]\n",
      "epoch:28 step:22315 [D loss: 0.031169, acc.: 99.22%] [G loss: 0.000708]\n",
      "epoch:28 step:22316 [D loss: 0.004078, acc.: 100.00%] [G loss: 4.944540]\n",
      "epoch:28 step:22317 [D loss: 0.040662, acc.: 100.00%] [G loss: 0.039337]\n",
      "epoch:28 step:22318 [D loss: 0.000691, acc.: 100.00%] [G loss: 2.796076]\n",
      "epoch:28 step:22319 [D loss: 0.040191, acc.: 100.00%] [G loss: 5.823930]\n",
      "epoch:28 step:22320 [D loss: 0.009394, acc.: 100.00%] [G loss: 5.038387]\n",
      "epoch:28 step:22321 [D loss: 2.193346, acc.: 44.53%] [G loss: 6.736187]\n",
      "epoch:28 step:22322 [D loss: 4.630327, acc.: 50.00%] [G loss: 4.337992]\n",
      "epoch:28 step:22323 [D loss: 1.831424, acc.: 50.00%] [G loss: 0.487569]\n",
      "epoch:28 step:22324 [D loss: 0.049141, acc.: 98.44%] [G loss: 0.154103]\n",
      "epoch:28 step:22325 [D loss: 0.079547, acc.: 96.88%] [G loss: 1.801957]\n",
      "epoch:28 step:22326 [D loss: 0.010681, acc.: 100.00%] [G loss: 0.016035]\n",
      "epoch:28 step:22327 [D loss: 0.028574, acc.: 100.00%] [G loss: 2.520397]\n",
      "epoch:28 step:22328 [D loss: 0.078806, acc.: 99.22%] [G loss: 1.928148]\n",
      "epoch:28 step:22329 [D loss: 0.082873, acc.: 100.00%] [G loss: 0.022693]\n",
      "epoch:28 step:22330 [D loss: 0.022769, acc.: 100.00%] [G loss: 1.582033]\n",
      "epoch:28 step:22331 [D loss: 0.084752, acc.: 96.88%] [G loss: 0.034366]\n",
      "epoch:28 step:22332 [D loss: 0.099612, acc.: 100.00%] [G loss: 0.039148]\n",
      "epoch:28 step:22333 [D loss: 0.028224, acc.: 100.00%] [G loss: 0.054770]\n",
      "epoch:28 step:22334 [D loss: 0.064000, acc.: 99.22%] [G loss: 0.024650]\n",
      "epoch:28 step:22335 [D loss: 0.053732, acc.: 99.22%] [G loss: 0.022692]\n",
      "epoch:28 step:22336 [D loss: 0.024819, acc.: 100.00%] [G loss: 0.032754]\n",
      "epoch:28 step:22337 [D loss: 0.035098, acc.: 100.00%] [G loss: 0.017504]\n",
      "epoch:28 step:22338 [D loss: 0.034853, acc.: 100.00%] [G loss: 2.882078]\n",
      "epoch:28 step:22339 [D loss: 0.017000, acc.: 100.00%] [G loss: 2.806000]\n",
      "epoch:28 step:22340 [D loss: 0.030442, acc.: 100.00%] [G loss: 0.068758]\n",
      "epoch:28 step:22341 [D loss: 0.098198, acc.: 98.44%] [G loss: 0.079174]\n",
      "epoch:28 step:22342 [D loss: 0.054881, acc.: 98.44%] [G loss: 1.376687]\n",
      "epoch:28 step:22343 [D loss: 0.032583, acc.: 99.22%] [G loss: 0.108424]\n",
      "epoch:28 step:22344 [D loss: 0.026462, acc.: 100.00%] [G loss: 1.870057]\n",
      "epoch:28 step:22345 [D loss: 0.066540, acc.: 100.00%] [G loss: 0.682930]\n",
      "epoch:28 step:22346 [D loss: 0.027617, acc.: 100.00%] [G loss: 0.384249]\n",
      "epoch:28 step:22347 [D loss: 0.015290, acc.: 100.00%] [G loss: 0.128034]\n",
      "epoch:28 step:22348 [D loss: 0.027119, acc.: 99.22%] [G loss: 0.486042]\n",
      "epoch:28 step:22349 [D loss: 0.022469, acc.: 100.00%] [G loss: 2.422107]\n",
      "epoch:28 step:22350 [D loss: 0.147076, acc.: 97.66%] [G loss: 3.094739]\n",
      "epoch:28 step:22351 [D loss: 0.126976, acc.: 92.97%] [G loss: 1.954092]\n",
      "epoch:28 step:22352 [D loss: 0.085261, acc.: 100.00%] [G loss: 5.256566]\n",
      "epoch:28 step:22353 [D loss: 0.089094, acc.: 96.09%] [G loss: 3.931664]\n",
      "epoch:28 step:22354 [D loss: 0.061602, acc.: 98.44%] [G loss: 0.637185]\n",
      "epoch:28 step:22355 [D loss: 0.075501, acc.: 96.88%] [G loss: 5.931114]\n",
      "epoch:28 step:22356 [D loss: 0.016645, acc.: 100.00%] [G loss: 1.019021]\n",
      "epoch:28 step:22357 [D loss: 0.016258, acc.: 100.00%] [G loss: 1.752998]\n",
      "epoch:28 step:22358 [D loss: 0.068001, acc.: 97.66%] [G loss: 0.465108]\n",
      "epoch:28 step:22359 [D loss: 0.046725, acc.: 99.22%] [G loss: 4.216879]\n",
      "epoch:28 step:22360 [D loss: 0.026712, acc.: 100.00%] [G loss: 0.033505]\n",
      "epoch:28 step:22361 [D loss: 0.015717, acc.: 100.00%] [G loss: 0.056152]\n",
      "epoch:28 step:22362 [D loss: 0.004199, acc.: 100.00%] [G loss: 0.044599]\n",
      "epoch:28 step:22363 [D loss: 0.015370, acc.: 100.00%] [G loss: 2.058970]\n",
      "epoch:28 step:22364 [D loss: 0.024191, acc.: 100.00%] [G loss: 0.033636]\n",
      "epoch:28 step:22365 [D loss: 0.046716, acc.: 99.22%] [G loss: 0.009235]\n",
      "epoch:28 step:22366 [D loss: 0.045546, acc.: 99.22%] [G loss: 0.004224]\n",
      "epoch:28 step:22367 [D loss: 0.040672, acc.: 100.00%] [G loss: 0.013932]\n",
      "epoch:28 step:22368 [D loss: 0.007573, acc.: 100.00%] [G loss: 0.183004]\n",
      "epoch:28 step:22369 [D loss: 0.006317, acc.: 100.00%] [G loss: 0.894340]\n",
      "epoch:28 step:22370 [D loss: 0.208522, acc.: 92.19%] [G loss: 6.107859]\n",
      "epoch:28 step:22371 [D loss: 0.077743, acc.: 98.44%] [G loss: 1.560218]\n",
      "epoch:28 step:22372 [D loss: 0.436798, acc.: 76.56%] [G loss: 0.243844]\n",
      "epoch:28 step:22373 [D loss: 0.642736, acc.: 72.66%] [G loss: 0.383293]\n",
      "epoch:28 step:22374 [D loss: 0.146684, acc.: 93.75%] [G loss: 1.353289]\n",
      "epoch:28 step:22375 [D loss: 0.344934, acc.: 82.81%] [G loss: 0.016019]\n",
      "epoch:28 step:22376 [D loss: 0.012466, acc.: 100.00%] [G loss: 0.006337]\n",
      "epoch:28 step:22377 [D loss: 0.004903, acc.: 100.00%] [G loss: 3.073809]\n",
      "epoch:28 step:22378 [D loss: 0.002603, acc.: 100.00%] [G loss: 0.000865]\n",
      "epoch:28 step:22379 [D loss: 0.001556, acc.: 100.00%] [G loss: 0.000606]\n",
      "epoch:28 step:22380 [D loss: 0.004590, acc.: 100.00%] [G loss: 0.010200]\n",
      "epoch:28 step:22381 [D loss: 0.001814, acc.: 100.00%] [G loss: 0.003238]\n",
      "epoch:28 step:22382 [D loss: 0.004436, acc.: 100.00%] [G loss: 3.442370]\n",
      "epoch:28 step:22383 [D loss: 0.014522, acc.: 100.00%] [G loss: 0.000886]\n",
      "epoch:28 step:22384 [D loss: 0.038368, acc.: 99.22%] [G loss: 0.022663]\n",
      "epoch:28 step:22385 [D loss: 0.035962, acc.: 100.00%] [G loss: 0.037352]\n",
      "epoch:28 step:22386 [D loss: 0.012712, acc.: 100.00%] [G loss: 2.507077]\n",
      "epoch:28 step:22387 [D loss: 0.006387, acc.: 100.00%] [G loss: 0.109897]\n",
      "epoch:28 step:22388 [D loss: 0.005673, acc.: 100.00%] [G loss: 1.614485]\n",
      "epoch:28 step:22389 [D loss: 0.009357, acc.: 100.00%] [G loss: 0.398501]\n",
      "epoch:28 step:22390 [D loss: 0.042083, acc.: 99.22%] [G loss: 0.027166]\n",
      "epoch:28 step:22391 [D loss: 0.009114, acc.: 100.00%] [G loss: 0.027764]\n",
      "epoch:28 step:22392 [D loss: 0.012868, acc.: 100.00%] [G loss: 0.044034]\n",
      "epoch:28 step:22393 [D loss: 0.003060, acc.: 100.00%] [G loss: 0.155125]\n",
      "epoch:28 step:22394 [D loss: 0.017254, acc.: 100.00%] [G loss: 0.013885]\n",
      "epoch:28 step:22395 [D loss: 0.009783, acc.: 100.00%] [G loss: 1.563186]\n",
      "epoch:28 step:22396 [D loss: 0.004142, acc.: 100.00%] [G loss: 0.017958]\n",
      "epoch:28 step:22397 [D loss: 0.004200, acc.: 100.00%] [G loss: 0.003408]\n",
      "epoch:28 step:22398 [D loss: 0.004842, acc.: 100.00%] [G loss: 0.011893]\n",
      "epoch:28 step:22399 [D loss: 0.014689, acc.: 100.00%] [G loss: 0.023639]\n",
      "epoch:28 step:22400 [D loss: 0.006474, acc.: 100.00%] [G loss: 0.045273]\n",
      "epoch:28 step:22401 [D loss: 0.001302, acc.: 100.00%] [G loss: 0.032244]\n",
      "epoch:28 step:22402 [D loss: 0.003980, acc.: 100.00%] [G loss: 0.053906]\n",
      "epoch:28 step:22403 [D loss: 0.012570, acc.: 100.00%] [G loss: 0.080836]\n",
      "epoch:28 step:22404 [D loss: 0.009624, acc.: 100.00%] [G loss: 0.006043]\n",
      "epoch:28 step:22405 [D loss: 0.041695, acc.: 99.22%] [G loss: 0.007439]\n",
      "epoch:28 step:22406 [D loss: 0.056436, acc.: 100.00%] [G loss: 0.080305]\n",
      "epoch:28 step:22407 [D loss: 0.002818, acc.: 100.00%] [G loss: 0.613910]\n",
      "epoch:28 step:22408 [D loss: 0.001761, acc.: 100.00%] [G loss: 0.690442]\n",
      "epoch:28 step:22409 [D loss: 0.001416, acc.: 100.00%] [G loss: 0.219838]\n",
      "epoch:28 step:22410 [D loss: 0.029430, acc.: 98.44%] [G loss: 4.996372]\n",
      "epoch:28 step:22411 [D loss: 0.017960, acc.: 100.00%] [G loss: 0.248845]\n",
      "epoch:28 step:22412 [D loss: 0.015129, acc.: 100.00%] [G loss: 0.958783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22413 [D loss: 0.004695, acc.: 100.00%] [G loss: 1.481525]\n",
      "epoch:28 step:22414 [D loss: 0.011585, acc.: 100.00%] [G loss: 1.568129]\n",
      "epoch:28 step:22415 [D loss: 0.008588, acc.: 100.00%] [G loss: 1.142206]\n",
      "epoch:28 step:22416 [D loss: 0.006439, acc.: 100.00%] [G loss: 1.644949]\n",
      "epoch:28 step:22417 [D loss: 0.006814, acc.: 100.00%] [G loss: 5.921653]\n",
      "epoch:28 step:22418 [D loss: 0.014707, acc.: 99.22%] [G loss: 3.063770]\n",
      "epoch:28 step:22419 [D loss: 0.002723, acc.: 100.00%] [G loss: 3.090500]\n",
      "epoch:28 step:22420 [D loss: 0.012620, acc.: 100.00%] [G loss: 3.823655]\n",
      "epoch:28 step:22421 [D loss: 0.001062, acc.: 100.00%] [G loss: 5.642815]\n",
      "epoch:28 step:22422 [D loss: 0.001912, acc.: 100.00%] [G loss: 4.924640]\n",
      "epoch:28 step:22423 [D loss: 0.011810, acc.: 100.00%] [G loss: 5.725583]\n",
      "epoch:28 step:22424 [D loss: 0.005897, acc.: 100.00%] [G loss: 0.000549]\n",
      "epoch:28 step:22425 [D loss: 0.007179, acc.: 100.00%] [G loss: 0.004328]\n",
      "epoch:28 step:22426 [D loss: 0.066622, acc.: 98.44%] [G loss: 8.138208]\n",
      "epoch:28 step:22427 [D loss: 0.019863, acc.: 99.22%] [G loss: 0.378756]\n",
      "epoch:28 step:22428 [D loss: 0.056039, acc.: 99.22%] [G loss: 12.745398]\n",
      "epoch:28 step:22429 [D loss: 0.027019, acc.: 99.22%] [G loss: 10.126433]\n",
      "epoch:28 step:22430 [D loss: 0.011147, acc.: 100.00%] [G loss: 7.977798]\n",
      "epoch:28 step:22431 [D loss: 0.020027, acc.: 100.00%] [G loss: 7.237215]\n",
      "epoch:28 step:22432 [D loss: 0.022016, acc.: 100.00%] [G loss: 8.782372]\n",
      "epoch:28 step:22433 [D loss: 0.007856, acc.: 100.00%] [G loss: 9.217180]\n",
      "epoch:28 step:22434 [D loss: 0.014278, acc.: 100.00%] [G loss: 8.673697]\n",
      "epoch:28 step:22435 [D loss: 0.010117, acc.: 100.00%] [G loss: 2.227911]\n",
      "epoch:28 step:22436 [D loss: 0.074568, acc.: 98.44%] [G loss: 0.268239]\n",
      "epoch:28 step:22437 [D loss: 0.023725, acc.: 99.22%] [G loss: 11.527229]\n",
      "epoch:28 step:22438 [D loss: 0.089473, acc.: 96.09%] [G loss: 7.856716]\n",
      "epoch:28 step:22439 [D loss: 0.011111, acc.: 100.00%] [G loss: 5.633076]\n",
      "epoch:28 step:22440 [D loss: 0.111339, acc.: 96.09%] [G loss: 1.818535]\n",
      "epoch:28 step:22441 [D loss: 0.358277, acc.: 84.38%] [G loss: 9.201702]\n",
      "epoch:28 step:22442 [D loss: 0.016611, acc.: 100.00%] [G loss: 6.576423]\n",
      "epoch:28 step:22443 [D loss: 0.006258, acc.: 100.00%] [G loss: 0.007216]\n",
      "epoch:28 step:22444 [D loss: 0.002775, acc.: 100.00%] [G loss: 0.002241]\n",
      "epoch:28 step:22445 [D loss: 0.100351, acc.: 97.66%] [G loss: 4.665621]\n",
      "epoch:28 step:22446 [D loss: 0.003062, acc.: 100.00%] [G loss: 3.537143]\n",
      "epoch:28 step:22447 [D loss: 0.000739, acc.: 100.00%] [G loss: 2.536650]\n",
      "epoch:28 step:22448 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.018193]\n",
      "epoch:28 step:22449 [D loss: 0.002246, acc.: 100.00%] [G loss: 0.081816]\n",
      "epoch:28 step:22450 [D loss: 0.006063, acc.: 100.00%] [G loss: 1.765457]\n",
      "epoch:28 step:22451 [D loss: 0.007611, acc.: 100.00%] [G loss: 2.871153]\n",
      "epoch:28 step:22452 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.005519]\n",
      "epoch:28 step:22453 [D loss: 0.031542, acc.: 100.00%] [G loss: 4.564183]\n",
      "epoch:28 step:22454 [D loss: 0.000533, acc.: 100.00%] [G loss: 2.805816]\n",
      "epoch:28 step:22455 [D loss: 0.015706, acc.: 100.00%] [G loss: 2.814758]\n",
      "epoch:28 step:22456 [D loss: 0.002812, acc.: 100.00%] [G loss: 2.452830]\n",
      "epoch:28 step:22457 [D loss: 0.055376, acc.: 98.44%] [G loss: 3.643202]\n",
      "epoch:28 step:22458 [D loss: 0.023705, acc.: 99.22%] [G loss: 1.709908]\n",
      "epoch:28 step:22459 [D loss: 0.147495, acc.: 92.97%] [G loss: 0.000460]\n",
      "epoch:28 step:22460 [D loss: 1.711750, acc.: 51.56%] [G loss: 9.965875]\n",
      "epoch:28 step:22461 [D loss: 3.990390, acc.: 50.00%] [G loss: 6.547884]\n",
      "epoch:28 step:22462 [D loss: 1.704265, acc.: 52.34%] [G loss: 3.923457]\n",
      "epoch:28 step:22463 [D loss: 0.515105, acc.: 75.00%] [G loss: 2.199968]\n",
      "epoch:28 step:22464 [D loss: 0.136357, acc.: 95.31%] [G loss: 1.024470]\n",
      "epoch:28 step:22465 [D loss: 0.014568, acc.: 100.00%] [G loss: 1.059826]\n",
      "epoch:28 step:22466 [D loss: 0.012633, acc.: 100.00%] [G loss: 0.220583]\n",
      "epoch:28 step:22467 [D loss: 0.022082, acc.: 99.22%] [G loss: 1.897322]\n",
      "epoch:28 step:22468 [D loss: 0.042753, acc.: 98.44%] [G loss: 0.105264]\n",
      "epoch:28 step:22469 [D loss: 0.039220, acc.: 100.00%] [G loss: 0.093051]\n",
      "epoch:28 step:22470 [D loss: 0.014108, acc.: 100.00%] [G loss: 0.079404]\n",
      "epoch:28 step:22471 [D loss: 0.035161, acc.: 100.00%] [G loss: 0.103532]\n",
      "epoch:28 step:22472 [D loss: 0.047361, acc.: 98.44%] [G loss: 0.216558]\n",
      "epoch:28 step:22473 [D loss: 0.025352, acc.: 99.22%] [G loss: 0.026321]\n",
      "epoch:28 step:22474 [D loss: 0.017727, acc.: 100.00%] [G loss: 0.188417]\n",
      "epoch:28 step:22475 [D loss: 0.014675, acc.: 100.00%] [G loss: 0.092697]\n",
      "epoch:28 step:22476 [D loss: 0.008643, acc.: 100.00%] [G loss: 0.161397]\n",
      "epoch:28 step:22477 [D loss: 0.011567, acc.: 100.00%] [G loss: 0.031868]\n",
      "epoch:28 step:22478 [D loss: 0.013124, acc.: 100.00%] [G loss: 0.014250]\n",
      "epoch:28 step:22479 [D loss: 0.018346, acc.: 100.00%] [G loss: 0.019701]\n",
      "epoch:28 step:22480 [D loss: 0.006413, acc.: 100.00%] [G loss: 0.031999]\n",
      "epoch:28 step:22481 [D loss: 0.045081, acc.: 100.00%] [G loss: 0.023075]\n",
      "epoch:28 step:22482 [D loss: 0.040214, acc.: 100.00%] [G loss: 0.025767]\n",
      "epoch:28 step:22483 [D loss: 0.069077, acc.: 98.44%] [G loss: 2.096715]\n",
      "epoch:28 step:22484 [D loss: 0.012012, acc.: 100.00%] [G loss: 0.877146]\n",
      "epoch:28 step:22485 [D loss: 0.016833, acc.: 99.22%] [G loss: 0.508157]\n",
      "epoch:28 step:22486 [D loss: 0.043908, acc.: 98.44%] [G loss: 0.886849]\n",
      "epoch:28 step:22487 [D loss: 0.040635, acc.: 98.44%] [G loss: 0.105492]\n",
      "epoch:28 step:22488 [D loss: 0.013107, acc.: 100.00%] [G loss: 0.050078]\n",
      "epoch:28 step:22489 [D loss: 0.010610, acc.: 100.00%] [G loss: 0.763538]\n",
      "epoch:28 step:22490 [D loss: 0.108199, acc.: 96.88%] [G loss: 2.065269]\n",
      "epoch:28 step:22491 [D loss: 0.221170, acc.: 88.28%] [G loss: 0.735588]\n",
      "epoch:28 step:22492 [D loss: 0.011101, acc.: 100.00%] [G loss: 0.263283]\n",
      "epoch:28 step:22493 [D loss: 0.090615, acc.: 97.66%] [G loss: 0.190811]\n",
      "epoch:28 step:22494 [D loss: 0.155419, acc.: 92.97%] [G loss: 0.016017]\n",
      "epoch:28 step:22495 [D loss: 0.005986, acc.: 100.00%] [G loss: 0.514056]\n",
      "epoch:28 step:22496 [D loss: 0.129355, acc.: 97.66%] [G loss: 0.021402]\n",
      "epoch:28 step:22497 [D loss: 0.016655, acc.: 100.00%] [G loss: 2.981586]\n",
      "epoch:28 step:22498 [D loss: 0.020919, acc.: 100.00%] [G loss: 0.014743]\n",
      "epoch:28 step:22499 [D loss: 0.018119, acc.: 100.00%] [G loss: 0.005109]\n",
      "epoch:28 step:22500 [D loss: 0.004715, acc.: 100.00%] [G loss: 0.005000]\n",
      "epoch:28 step:22501 [D loss: 0.011823, acc.: 100.00%] [G loss: 0.013199]\n",
      "epoch:28 step:22502 [D loss: 0.019809, acc.: 100.00%] [G loss: 0.584358]\n",
      "epoch:28 step:22503 [D loss: 0.037119, acc.: 100.00%] [G loss: 1.365255]\n",
      "epoch:28 step:22504 [D loss: 0.021047, acc.: 100.00%] [G loss: 0.018149]\n",
      "epoch:28 step:22505 [D loss: 0.047181, acc.: 99.22%] [G loss: 1.157730]\n",
      "epoch:28 step:22506 [D loss: 0.014780, acc.: 100.00%] [G loss: 0.019226]\n",
      "epoch:28 step:22507 [D loss: 0.886358, acc.: 64.06%] [G loss: 2.866408]\n",
      "epoch:28 step:22508 [D loss: 1.307697, acc.: 57.03%] [G loss: 0.805043]\n",
      "epoch:28 step:22509 [D loss: 0.800054, acc.: 64.84%] [G loss: 0.209963]\n",
      "epoch:28 step:22510 [D loss: 0.046402, acc.: 99.22%] [G loss: 0.245801]\n",
      "epoch:28 step:22511 [D loss: 0.023222, acc.: 100.00%] [G loss: 0.053681]\n",
      "epoch:28 step:22512 [D loss: 0.042125, acc.: 100.00%] [G loss: 0.035347]\n",
      "epoch:28 step:22513 [D loss: 0.018347, acc.: 100.00%] [G loss: 0.025361]\n",
      "epoch:28 step:22514 [D loss: 0.070394, acc.: 99.22%] [G loss: 3.254181]\n",
      "epoch:28 step:22515 [D loss: 0.012476, acc.: 100.00%] [G loss: 0.071747]\n",
      "epoch:28 step:22516 [D loss: 0.009667, acc.: 100.00%] [G loss: 0.025432]\n",
      "epoch:28 step:22517 [D loss: 0.014269, acc.: 100.00%] [G loss: 0.013693]\n",
      "epoch:28 step:22518 [D loss: 0.021611, acc.: 99.22%] [G loss: 0.060072]\n",
      "epoch:28 step:22519 [D loss: 0.020508, acc.: 100.00%] [G loss: 2.959400]\n",
      "epoch:28 step:22520 [D loss: 0.007609, acc.: 100.00%] [G loss: 3.707454]\n",
      "epoch:28 step:22521 [D loss: 0.007700, acc.: 100.00%] [G loss: 0.021993]\n",
      "epoch:28 step:22522 [D loss: 0.014042, acc.: 100.00%] [G loss: 0.045876]\n",
      "epoch:28 step:22523 [D loss: 0.012359, acc.: 100.00%] [G loss: 0.010092]\n",
      "epoch:28 step:22524 [D loss: 0.018852, acc.: 100.00%] [G loss: 0.006648]\n",
      "epoch:28 step:22525 [D loss: 0.014042, acc.: 100.00%] [G loss: 0.008175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22526 [D loss: 0.015573, acc.: 100.00%] [G loss: 0.473835]\n",
      "epoch:28 step:22527 [D loss: 0.031998, acc.: 100.00%] [G loss: 2.593894]\n",
      "epoch:28 step:22528 [D loss: 0.004437, acc.: 100.00%] [G loss: 0.020615]\n",
      "epoch:28 step:22529 [D loss: 0.024642, acc.: 100.00%] [G loss: 0.025348]\n",
      "epoch:28 step:22530 [D loss: 0.010174, acc.: 100.00%] [G loss: 2.670332]\n",
      "epoch:28 step:22531 [D loss: 0.026879, acc.: 100.00%] [G loss: 0.427562]\n",
      "epoch:28 step:22532 [D loss: 0.010433, acc.: 100.00%] [G loss: 2.228649]\n",
      "epoch:28 step:22533 [D loss: 0.032254, acc.: 100.00%] [G loss: 0.025056]\n",
      "epoch:28 step:22534 [D loss: 0.019058, acc.: 100.00%] [G loss: 0.010395]\n",
      "epoch:28 step:22535 [D loss: 0.020554, acc.: 100.00%] [G loss: 0.009124]\n",
      "epoch:28 step:22536 [D loss: 0.017049, acc.: 100.00%] [G loss: 1.279335]\n",
      "epoch:28 step:22537 [D loss: 0.004973, acc.: 100.00%] [G loss: 0.025654]\n",
      "epoch:28 step:22538 [D loss: 0.012350, acc.: 100.00%] [G loss: 0.005118]\n",
      "epoch:28 step:22539 [D loss: 0.014572, acc.: 100.00%] [G loss: 0.617929]\n",
      "epoch:28 step:22540 [D loss: 0.007789, acc.: 100.00%] [G loss: 0.043696]\n",
      "epoch:28 step:22541 [D loss: 0.008934, acc.: 100.00%] [G loss: 0.022111]\n",
      "epoch:28 step:22542 [D loss: 0.021532, acc.: 100.00%] [G loss: 0.033999]\n",
      "epoch:28 step:22543 [D loss: 0.084228, acc.: 97.66%] [G loss: 0.135810]\n",
      "epoch:28 step:22544 [D loss: 0.058371, acc.: 98.44%] [G loss: 0.367480]\n",
      "epoch:28 step:22545 [D loss: 0.020564, acc.: 99.22%] [G loss: 0.006887]\n",
      "epoch:28 step:22546 [D loss: 0.064359, acc.: 97.66%] [G loss: 0.047397]\n",
      "epoch:28 step:22547 [D loss: 0.022793, acc.: 100.00%] [G loss: 0.228403]\n",
      "epoch:28 step:22548 [D loss: 0.013355, acc.: 100.00%] [G loss: 4.175856]\n",
      "epoch:28 step:22549 [D loss: 0.047255, acc.: 99.22%] [G loss: 0.071591]\n",
      "epoch:28 step:22550 [D loss: 0.011350, acc.: 100.00%] [G loss: 0.017923]\n",
      "epoch:28 step:22551 [D loss: 0.028817, acc.: 100.00%] [G loss: 0.439651]\n",
      "epoch:28 step:22552 [D loss: 0.020702, acc.: 100.00%] [G loss: 0.245784]\n",
      "epoch:28 step:22553 [D loss: 0.031355, acc.: 99.22%] [G loss: 0.094826]\n",
      "epoch:28 step:22554 [D loss: 0.063616, acc.: 99.22%] [G loss: 1.162049]\n",
      "epoch:28 step:22555 [D loss: 0.010131, acc.: 100.00%] [G loss: 3.212827]\n",
      "epoch:28 step:22556 [D loss: 0.022276, acc.: 99.22%] [G loss: 1.251417]\n",
      "epoch:28 step:22557 [D loss: 0.005125, acc.: 100.00%] [G loss: 2.350286]\n",
      "epoch:28 step:22558 [D loss: 0.003714, acc.: 100.00%] [G loss: 0.364161]\n",
      "epoch:28 step:22559 [D loss: 0.007834, acc.: 100.00%] [G loss: 0.658087]\n",
      "epoch:28 step:22560 [D loss: 0.062352, acc.: 97.66%] [G loss: 0.574625]\n",
      "epoch:28 step:22561 [D loss: 0.008733, acc.: 100.00%] [G loss: 0.071086]\n",
      "epoch:28 step:22562 [D loss: 0.158437, acc.: 96.09%] [G loss: 1.157315]\n",
      "epoch:28 step:22563 [D loss: 0.001146, acc.: 100.00%] [G loss: 3.200414]\n",
      "epoch:28 step:22564 [D loss: 0.088917, acc.: 96.88%] [G loss: 1.420969]\n",
      "epoch:28 step:22565 [D loss: 0.041294, acc.: 99.22%] [G loss: 0.243483]\n",
      "epoch:28 step:22566 [D loss: 0.001221, acc.: 100.00%] [G loss: 0.453233]\n",
      "epoch:28 step:22567 [D loss: 0.001043, acc.: 100.00%] [G loss: 0.074124]\n",
      "epoch:28 step:22568 [D loss: 0.002954, acc.: 100.00%] [G loss: 0.103465]\n",
      "epoch:28 step:22569 [D loss: 0.001143, acc.: 100.00%] [G loss: 0.159362]\n",
      "epoch:28 step:22570 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.058241]\n",
      "epoch:28 step:22571 [D loss: 0.001222, acc.: 100.00%] [G loss: 0.099305]\n",
      "epoch:28 step:22572 [D loss: 0.004177, acc.: 100.00%] [G loss: 0.035400]\n",
      "epoch:28 step:22573 [D loss: 0.001554, acc.: 100.00%] [G loss: 0.071263]\n",
      "epoch:28 step:22574 [D loss: 0.004082, acc.: 100.00%] [G loss: 0.130634]\n",
      "epoch:28 step:22575 [D loss: 0.002870, acc.: 100.00%] [G loss: 0.110380]\n",
      "epoch:28 step:22576 [D loss: 0.008080, acc.: 100.00%] [G loss: 0.043932]\n",
      "epoch:28 step:22577 [D loss: 0.002118, acc.: 100.00%] [G loss: 0.026300]\n",
      "epoch:28 step:22578 [D loss: 0.002626, acc.: 100.00%] [G loss: 0.015672]\n",
      "epoch:28 step:22579 [D loss: 0.002123, acc.: 100.00%] [G loss: 0.046303]\n",
      "epoch:28 step:22580 [D loss: 0.002069, acc.: 100.00%] [G loss: 0.081810]\n",
      "epoch:28 step:22581 [D loss: 0.008180, acc.: 100.00%] [G loss: 0.007831]\n",
      "epoch:28 step:22582 [D loss: 0.002316, acc.: 100.00%] [G loss: 0.160298]\n",
      "epoch:28 step:22583 [D loss: 0.003376, acc.: 100.00%] [G loss: 5.774083]\n",
      "epoch:28 step:22584 [D loss: 0.067235, acc.: 97.66%] [G loss: 0.066681]\n",
      "epoch:28 step:22585 [D loss: 0.009965, acc.: 100.00%] [G loss: 1.326706]\n",
      "epoch:28 step:22586 [D loss: 0.047699, acc.: 100.00%] [G loss: 0.320257]\n",
      "epoch:28 step:22587 [D loss: 0.006991, acc.: 100.00%] [G loss: 0.193098]\n",
      "epoch:28 step:22588 [D loss: 0.005288, acc.: 100.00%] [G loss: 0.145708]\n",
      "epoch:28 step:22589 [D loss: 0.006852, acc.: 100.00%] [G loss: 0.863410]\n",
      "epoch:28 step:22590 [D loss: 0.009023, acc.: 100.00%] [G loss: 0.453371]\n",
      "epoch:28 step:22591 [D loss: 0.058806, acc.: 98.44%] [G loss: 0.031513]\n",
      "epoch:28 step:22592 [D loss: 0.011960, acc.: 100.00%] [G loss: 0.048785]\n",
      "epoch:28 step:22593 [D loss: 0.015423, acc.: 99.22%] [G loss: 0.091551]\n",
      "epoch:28 step:22594 [D loss: 0.024023, acc.: 100.00%] [G loss: 0.123402]\n",
      "epoch:28 step:22595 [D loss: 0.014883, acc.: 100.00%] [G loss: 0.030121]\n",
      "epoch:28 step:22596 [D loss: 0.001758, acc.: 100.00%] [G loss: 0.034522]\n",
      "epoch:28 step:22597 [D loss: 0.001482, acc.: 100.00%] [G loss: 1.143208]\n",
      "epoch:28 step:22598 [D loss: 0.020798, acc.: 100.00%] [G loss: 0.027718]\n",
      "epoch:28 step:22599 [D loss: 0.002063, acc.: 100.00%] [G loss: 0.055613]\n",
      "epoch:28 step:22600 [D loss: 0.005461, acc.: 100.00%] [G loss: 0.431252]\n",
      "epoch:28 step:22601 [D loss: 0.006234, acc.: 100.00%] [G loss: 0.463245]\n",
      "epoch:28 step:22602 [D loss: 0.027257, acc.: 98.44%] [G loss: 0.008601]\n",
      "epoch:28 step:22603 [D loss: 0.070922, acc.: 99.22%] [G loss: 0.039333]\n",
      "epoch:28 step:22604 [D loss: 0.019895, acc.: 100.00%] [G loss: 4.371717]\n",
      "epoch:28 step:22605 [D loss: 0.009000, acc.: 100.00%] [G loss: 1.068681]\n",
      "epoch:28 step:22606 [D loss: 0.004358, acc.: 100.00%] [G loss: 2.939519]\n",
      "epoch:28 step:22607 [D loss: 0.035381, acc.: 98.44%] [G loss: 0.095989]\n",
      "epoch:28 step:22608 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.034446]\n",
      "epoch:28 step:22609 [D loss: 0.001577, acc.: 100.00%] [G loss: 0.018099]\n",
      "epoch:28 step:22610 [D loss: 0.000672, acc.: 100.00%] [G loss: 0.017728]\n",
      "epoch:28 step:22611 [D loss: 0.000868, acc.: 100.00%] [G loss: 0.003099]\n",
      "epoch:28 step:22612 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.011655]\n",
      "epoch:28 step:22613 [D loss: 0.004271, acc.: 100.00%] [G loss: 0.068084]\n",
      "epoch:28 step:22614 [D loss: 0.000902, acc.: 100.00%] [G loss: 0.077462]\n",
      "epoch:28 step:22615 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.013084]\n",
      "epoch:28 step:22616 [D loss: 0.001422, acc.: 100.00%] [G loss: 0.037574]\n",
      "epoch:28 step:22617 [D loss: 0.001268, acc.: 100.00%] [G loss: 0.002132]\n",
      "epoch:28 step:22618 [D loss: 0.000720, acc.: 100.00%] [G loss: 0.013356]\n",
      "epoch:28 step:22619 [D loss: 0.000593, acc.: 100.00%] [G loss: 0.001147]\n",
      "epoch:28 step:22620 [D loss: 0.003547, acc.: 100.00%] [G loss: 0.004341]\n",
      "epoch:28 step:22621 [D loss: 0.002906, acc.: 100.00%] [G loss: 0.001077]\n",
      "epoch:28 step:22622 [D loss: 0.015004, acc.: 100.00%] [G loss: 0.006261]\n",
      "epoch:28 step:22623 [D loss: 0.002899, acc.: 100.00%] [G loss: 0.010319]\n",
      "epoch:28 step:22624 [D loss: 0.008009, acc.: 100.00%] [G loss: 1.805086]\n",
      "epoch:28 step:22625 [D loss: 0.258812, acc.: 85.94%] [G loss: 9.393562]\n",
      "epoch:28 step:22626 [D loss: 0.476657, acc.: 81.25%] [G loss: 9.756492]\n",
      "epoch:28 step:22627 [D loss: 0.002566, acc.: 100.00%] [G loss: 8.849373]\n",
      "epoch:28 step:22628 [D loss: 0.003488, acc.: 100.00%] [G loss: 7.911275]\n",
      "epoch:28 step:22629 [D loss: 0.001072, acc.: 100.00%] [G loss: 7.555779]\n",
      "epoch:28 step:22630 [D loss: 0.003062, acc.: 100.00%] [G loss: 7.897606]\n",
      "epoch:28 step:22631 [D loss: 0.006152, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:28 step:22632 [D loss: 0.282746, acc.: 86.72%] [G loss: 8.524934]\n",
      "epoch:28 step:22633 [D loss: 0.049574, acc.: 99.22%] [G loss: 9.265520]\n",
      "epoch:28 step:22634 [D loss: 0.581458, acc.: 78.91%] [G loss: 6.570678]\n",
      "epoch:28 step:22635 [D loss: 0.004152, acc.: 100.00%] [G loss: 5.494383]\n",
      "epoch:28 step:22636 [D loss: 0.004054, acc.: 100.00%] [G loss: 5.051012]\n",
      "epoch:28 step:22637 [D loss: 0.002017, acc.: 100.00%] [G loss: 4.889944]\n",
      "epoch:28 step:22638 [D loss: 0.002959, acc.: 100.00%] [G loss: 3.999114]\n",
      "epoch:28 step:22639 [D loss: 0.011654, acc.: 100.00%] [G loss: 0.000669]\n",
      "epoch:28 step:22640 [D loss: 0.003275, acc.: 100.00%] [G loss: 3.729737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:28 step:22641 [D loss: 0.004002, acc.: 100.00%] [G loss: 3.385332]\n",
      "epoch:28 step:22642 [D loss: 0.001179, acc.: 100.00%] [G loss: 3.673100]\n",
      "epoch:28 step:22643 [D loss: 0.001636, acc.: 100.00%] [G loss: 3.563871]\n",
      "epoch:28 step:22644 [D loss: 0.002589, acc.: 100.00%] [G loss: 2.980935]\n",
      "epoch:28 step:22645 [D loss: 0.005747, acc.: 100.00%] [G loss: 2.590679]\n",
      "epoch:28 step:22646 [D loss: 0.004392, acc.: 100.00%] [G loss: 2.822520]\n",
      "epoch:28 step:22647 [D loss: 0.003509, acc.: 100.00%] [G loss: 0.000817]\n",
      "epoch:28 step:22648 [D loss: 0.005070, acc.: 100.00%] [G loss: 2.549705]\n",
      "epoch:28 step:22649 [D loss: 0.006308, acc.: 100.00%] [G loss: 0.010511]\n",
      "epoch:29 step:22650 [D loss: 0.029067, acc.: 99.22%] [G loss: 2.395908]\n",
      "epoch:29 step:22651 [D loss: 0.004634, acc.: 100.00%] [G loss: 2.812974]\n",
      "epoch:29 step:22652 [D loss: 0.001425, acc.: 100.00%] [G loss: 2.776025]\n",
      "epoch:29 step:22653 [D loss: 0.001230, acc.: 100.00%] [G loss: 0.004870]\n",
      "epoch:29 step:22654 [D loss: 0.001940, acc.: 100.00%] [G loss: 2.012098]\n",
      "epoch:29 step:22655 [D loss: 0.000766, acc.: 100.00%] [G loss: 1.371705]\n",
      "epoch:29 step:22656 [D loss: 0.004227, acc.: 100.00%] [G loss: 0.839888]\n",
      "epoch:29 step:22657 [D loss: 0.002320, acc.: 100.00%] [G loss: 1.251163]\n",
      "epoch:29 step:22658 [D loss: 0.006678, acc.: 100.00%] [G loss: 0.724382]\n",
      "epoch:29 step:22659 [D loss: 0.001231, acc.: 100.00%] [G loss: 0.740303]\n",
      "epoch:29 step:22660 [D loss: 0.006698, acc.: 100.00%] [G loss: 0.259511]\n",
      "epoch:29 step:22661 [D loss: 0.002505, acc.: 100.00%] [G loss: 0.461208]\n",
      "epoch:29 step:22662 [D loss: 0.000674, acc.: 100.00%] [G loss: 1.202871]\n",
      "epoch:29 step:22663 [D loss: 0.019654, acc.: 100.00%] [G loss: 2.085129]\n",
      "epoch:29 step:22664 [D loss: 0.002566, acc.: 100.00%] [G loss: 2.243249]\n",
      "epoch:29 step:22665 [D loss: 0.001749, acc.: 100.00%] [G loss: 1.932766]\n",
      "epoch:29 step:22666 [D loss: 0.001722, acc.: 100.00%] [G loss: 1.806181]\n",
      "epoch:29 step:22667 [D loss: 0.006397, acc.: 100.00%] [G loss: 0.075995]\n",
      "epoch:29 step:22668 [D loss: 0.001917, acc.: 100.00%] [G loss: 1.134979]\n",
      "epoch:29 step:22669 [D loss: 0.001164, acc.: 100.00%] [G loss: 0.828878]\n",
      "epoch:29 step:22670 [D loss: 0.003340, acc.: 100.00%] [G loss: 0.865261]\n",
      "epoch:29 step:22671 [D loss: 0.003529, acc.: 100.00%] [G loss: 0.524413]\n",
      "epoch:29 step:22672 [D loss: 0.005026, acc.: 100.00%] [G loss: 0.451796]\n",
      "epoch:29 step:22673 [D loss: 0.001095, acc.: 100.00%] [G loss: 0.994859]\n",
      "epoch:29 step:22674 [D loss: 0.002308, acc.: 100.00%] [G loss: 2.791135]\n",
      "epoch:29 step:22675 [D loss: 0.149413, acc.: 92.19%] [G loss: 12.255314]\n",
      "epoch:29 step:22676 [D loss: 0.161509, acc.: 95.31%] [G loss: 12.042532]\n",
      "epoch:29 step:22677 [D loss: 0.008735, acc.: 100.00%] [G loss: 12.288339]\n",
      "epoch:29 step:22678 [D loss: 0.014296, acc.: 99.22%] [G loss: 11.903696]\n",
      "epoch:29 step:22679 [D loss: 0.000753, acc.: 100.00%] [G loss: 12.333642]\n",
      "epoch:29 step:22680 [D loss: 0.000130, acc.: 100.00%] [G loss: 4.978225]\n",
      "epoch:29 step:22681 [D loss: 0.000140, acc.: 100.00%] [G loss: 11.447867]\n",
      "epoch:29 step:22682 [D loss: 0.000587, acc.: 100.00%] [G loss: 11.062964]\n",
      "epoch:29 step:22683 [D loss: 0.000148, acc.: 100.00%] [G loss: 10.411747]\n",
      "epoch:29 step:22684 [D loss: 0.000789, acc.: 100.00%] [G loss: 0.008911]\n",
      "epoch:29 step:22685 [D loss: 0.027455, acc.: 100.00%] [G loss: 10.684868]\n",
      "epoch:29 step:22686 [D loss: 0.000646, acc.: 100.00%] [G loss: 10.883429]\n",
      "epoch:29 step:22687 [D loss: 0.005944, acc.: 100.00%] [G loss: 10.890068]\n",
      "epoch:29 step:22688 [D loss: 0.002247, acc.: 100.00%] [G loss: 10.357059]\n",
      "epoch:29 step:22689 [D loss: 0.002065, acc.: 100.00%] [G loss: 10.403086]\n",
      "epoch:29 step:22690 [D loss: 0.001470, acc.: 100.00%] [G loss: 10.223431]\n",
      "epoch:29 step:22691 [D loss: 0.000448, acc.: 100.00%] [G loss: 9.431110]\n",
      "epoch:29 step:22692 [D loss: 0.003620, acc.: 100.00%] [G loss: 9.689875]\n",
      "epoch:29 step:22693 [D loss: 0.001132, acc.: 100.00%] [G loss: 9.473238]\n",
      "epoch:29 step:22694 [D loss: 0.001134, acc.: 100.00%] [G loss: 8.705818]\n",
      "epoch:29 step:22695 [D loss: 0.004925, acc.: 100.00%] [G loss: 8.610058]\n",
      "epoch:29 step:22696 [D loss: 0.003424, acc.: 100.00%] [G loss: 9.350222]\n",
      "epoch:29 step:22697 [D loss: 0.001271, acc.: 100.00%] [G loss: 9.207914]\n",
      "epoch:29 step:22698 [D loss: 0.002484, acc.: 100.00%] [G loss: 9.279787]\n",
      "epoch:29 step:22699 [D loss: 0.001875, acc.: 100.00%] [G loss: 7.814231]\n",
      "epoch:29 step:22700 [D loss: 0.002017, acc.: 100.00%] [G loss: 8.402403]\n",
      "epoch:29 step:22701 [D loss: 0.019814, acc.: 99.22%] [G loss: 8.589767]\n",
      "epoch:29 step:22702 [D loss: 0.003875, acc.: 100.00%] [G loss: 0.040899]\n",
      "epoch:29 step:22703 [D loss: 0.021548, acc.: 99.22%] [G loss: 8.750225]\n",
      "epoch:29 step:22704 [D loss: 0.004813, acc.: 100.00%] [G loss: 8.572785]\n",
      "epoch:29 step:22705 [D loss: 0.024586, acc.: 100.00%] [G loss: 9.060722]\n",
      "epoch:29 step:22706 [D loss: 0.001243, acc.: 100.00%] [G loss: 9.436636]\n",
      "epoch:29 step:22707 [D loss: 0.002186, acc.: 100.00%] [G loss: 8.790161]\n",
      "epoch:29 step:22708 [D loss: 0.001913, acc.: 100.00%] [G loss: 4.646116]\n",
      "epoch:29 step:22709 [D loss: 0.003892, acc.: 100.00%] [G loss: 8.322186]\n",
      "epoch:29 step:22710 [D loss: 0.019599, acc.: 99.22%] [G loss: 7.540952]\n",
      "epoch:29 step:22711 [D loss: 0.011484, acc.: 100.00%] [G loss: 7.514509]\n",
      "epoch:29 step:22712 [D loss: 0.008266, acc.: 100.00%] [G loss: 0.098149]\n",
      "epoch:29 step:22713 [D loss: 0.015289, acc.: 100.00%] [G loss: 7.271347]\n",
      "epoch:29 step:22714 [D loss: 0.020370, acc.: 100.00%] [G loss: 7.681434]\n",
      "epoch:29 step:22715 [D loss: 0.008945, acc.: 100.00%] [G loss: 6.350484]\n",
      "epoch:29 step:22716 [D loss: 0.023842, acc.: 100.00%] [G loss: 6.358173]\n",
      "epoch:29 step:22717 [D loss: 0.014367, acc.: 99.22%] [G loss: 5.128615]\n",
      "epoch:29 step:22718 [D loss: 0.039648, acc.: 100.00%] [G loss: 6.462233]\n",
      "epoch:29 step:22719 [D loss: 0.021857, acc.: 100.00%] [G loss: 5.682611]\n",
      "epoch:29 step:22720 [D loss: 0.018742, acc.: 100.00%] [G loss: 4.733000]\n",
      "epoch:29 step:22721 [D loss: 0.136114, acc.: 96.09%] [G loss: 7.040396]\n",
      "epoch:29 step:22722 [D loss: 0.188083, acc.: 92.97%] [G loss: 2.533617]\n",
      "epoch:29 step:22723 [D loss: 1.666201, acc.: 59.38%] [G loss: 15.729143]\n",
      "epoch:29 step:22724 [D loss: 4.770228, acc.: 50.00%] [G loss: 9.721698]\n",
      "epoch:29 step:22725 [D loss: 0.658862, acc.: 75.00%] [G loss: 7.278857]\n",
      "epoch:29 step:22726 [D loss: 0.079304, acc.: 96.09%] [G loss: 6.316183]\n",
      "epoch:29 step:22727 [D loss: 0.011771, acc.: 100.00%] [G loss: 6.045256]\n",
      "epoch:29 step:22728 [D loss: 0.012627, acc.: 100.00%] [G loss: 5.135143]\n",
      "epoch:29 step:22729 [D loss: 0.027994, acc.: 99.22%] [G loss: 4.873121]\n",
      "epoch:29 step:22730 [D loss: 0.010454, acc.: 100.00%] [G loss: 4.714211]\n",
      "epoch:29 step:22731 [D loss: 0.033728, acc.: 100.00%] [G loss: 4.312695]\n",
      "epoch:29 step:22732 [D loss: 0.075853, acc.: 98.44%] [G loss: 4.857217]\n",
      "epoch:29 step:22733 [D loss: 0.014011, acc.: 100.00%] [G loss: 4.396025]\n",
      "epoch:29 step:22734 [D loss: 0.021638, acc.: 99.22%] [G loss: 0.229924]\n",
      "epoch:29 step:22735 [D loss: 0.021146, acc.: 100.00%] [G loss: 4.129235]\n",
      "epoch:29 step:22736 [D loss: 0.022150, acc.: 100.00%] [G loss: 4.467983]\n",
      "epoch:29 step:22737 [D loss: 0.053408, acc.: 100.00%] [G loss: 3.652340]\n",
      "epoch:29 step:22738 [D loss: 0.215937, acc.: 90.62%] [G loss: 5.785145]\n",
      "epoch:29 step:22739 [D loss: 0.757261, acc.: 65.62%] [G loss: 0.049520]\n",
      "epoch:29 step:22740 [D loss: 0.035236, acc.: 100.00%] [G loss: 2.891930]\n",
      "epoch:29 step:22741 [D loss: 0.044276, acc.: 100.00%] [G loss: 2.372362]\n",
      "epoch:29 step:22742 [D loss: 0.024675, acc.: 100.00%] [G loss: 0.472152]\n",
      "epoch:29 step:22743 [D loss: 0.031760, acc.: 100.00%] [G loss: 2.435230]\n",
      "epoch:29 step:22744 [D loss: 0.021087, acc.: 100.00%] [G loss: 1.830486]\n",
      "epoch:29 step:22745 [D loss: 0.024460, acc.: 100.00%] [G loss: 0.123962]\n",
      "epoch:29 step:22746 [D loss: 0.024525, acc.: 100.00%] [G loss: 0.076299]\n",
      "epoch:29 step:22747 [D loss: 0.024801, acc.: 99.22%] [G loss: 0.565380]\n",
      "epoch:29 step:22748 [D loss: 0.134045, acc.: 96.88%] [G loss: 0.251295]\n",
      "epoch:29 step:22749 [D loss: 0.044991, acc.: 100.00%] [G loss: 1.796311]\n",
      "epoch:29 step:22750 [D loss: 0.018232, acc.: 100.00%] [G loss: 0.734400]\n",
      "epoch:29 step:22751 [D loss: 0.023584, acc.: 100.00%] [G loss: 0.507958]\n",
      "epoch:29 step:22752 [D loss: 0.197482, acc.: 92.19%] [G loss: 0.353255]\n",
      "epoch:29 step:22753 [D loss: 0.002680, acc.: 100.00%] [G loss: 0.420945]\n",
      "epoch:29 step:22754 [D loss: 0.398777, acc.: 81.25%] [G loss: 5.898179]\n",
      "epoch:29 step:22755 [D loss: 0.605867, acc.: 67.19%] [G loss: 4.335404]\n",
      "epoch:29 step:22756 [D loss: 0.087382, acc.: 96.09%] [G loss: 2.911022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22757 [D loss: 0.048177, acc.: 99.22%] [G loss: 1.489675]\n",
      "epoch:29 step:22758 [D loss: 0.032220, acc.: 99.22%] [G loss: 1.147298]\n",
      "epoch:29 step:22759 [D loss: 0.052491, acc.: 99.22%] [G loss: 1.663519]\n",
      "epoch:29 step:22760 [D loss: 0.139108, acc.: 96.88%] [G loss: 1.642148]\n",
      "epoch:29 step:22761 [D loss: 0.026125, acc.: 100.00%] [G loss: 1.927448]\n",
      "epoch:29 step:22762 [D loss: 0.038305, acc.: 100.00%] [G loss: 1.666844]\n",
      "epoch:29 step:22763 [D loss: 0.059910, acc.: 97.66%] [G loss: 0.464335]\n",
      "epoch:29 step:22764 [D loss: 0.676554, acc.: 68.75%] [G loss: 6.973687]\n",
      "epoch:29 step:22765 [D loss: 1.314706, acc.: 52.34%] [G loss: 5.068371]\n",
      "epoch:29 step:22766 [D loss: 0.391376, acc.: 79.69%] [G loss: 3.235193]\n",
      "epoch:29 step:22767 [D loss: 0.063866, acc.: 98.44%] [G loss: 2.192719]\n",
      "epoch:29 step:22768 [D loss: 0.062123, acc.: 100.00%] [G loss: 1.622844]\n",
      "epoch:29 step:22769 [D loss: 0.207475, acc.: 90.62%] [G loss: 3.015538]\n",
      "epoch:29 step:22770 [D loss: 0.057327, acc.: 100.00%] [G loss: 3.091365]\n",
      "epoch:29 step:22771 [D loss: 0.165041, acc.: 95.31%] [G loss: 3.552334]\n",
      "epoch:29 step:22772 [D loss: 0.128101, acc.: 98.44%] [G loss: 2.293022]\n",
      "epoch:29 step:22773 [D loss: 0.218936, acc.: 96.09%] [G loss: 4.869392]\n",
      "epoch:29 step:22774 [D loss: 0.092954, acc.: 98.44%] [G loss: 3.496536]\n",
      "epoch:29 step:22775 [D loss: 0.055784, acc.: 100.00%] [G loss: 3.245233]\n",
      "epoch:29 step:22776 [D loss: 0.047583, acc.: 99.22%] [G loss: 2.537547]\n",
      "epoch:29 step:22777 [D loss: 0.086306, acc.: 97.66%] [G loss: 3.410494]\n",
      "epoch:29 step:22778 [D loss: 0.096127, acc.: 98.44%] [G loss: 3.029542]\n",
      "epoch:29 step:22779 [D loss: 0.120402, acc.: 97.66%] [G loss: 3.824682]\n",
      "epoch:29 step:22780 [D loss: 0.116606, acc.: 96.88%] [G loss: 3.981379]\n",
      "epoch:29 step:22781 [D loss: 0.111651, acc.: 96.88%] [G loss: 3.262342]\n",
      "epoch:29 step:22782 [D loss: 0.072785, acc.: 99.22%] [G loss: 4.646125]\n",
      "epoch:29 step:22783 [D loss: 0.011027, acc.: 100.00%] [G loss: 4.268463]\n",
      "epoch:29 step:22784 [D loss: 0.509502, acc.: 79.69%] [G loss: 5.487756]\n",
      "epoch:29 step:22785 [D loss: 0.172492, acc.: 92.97%] [G loss: 5.115629]\n",
      "epoch:29 step:22786 [D loss: 0.121624, acc.: 96.88%] [G loss: 3.938511]\n",
      "epoch:29 step:22787 [D loss: 0.039113, acc.: 100.00%] [G loss: 3.497173]\n",
      "epoch:29 step:22788 [D loss: 0.022675, acc.: 100.00%] [G loss: 3.516349]\n",
      "epoch:29 step:22789 [D loss: 0.124523, acc.: 94.53%] [G loss: 4.589102]\n",
      "epoch:29 step:22790 [D loss: 0.098587, acc.: 97.66%] [G loss: 4.710565]\n",
      "epoch:29 step:22791 [D loss: 0.052759, acc.: 99.22%] [G loss: 3.646741]\n",
      "epoch:29 step:22792 [D loss: 0.097250, acc.: 97.66%] [G loss: 3.180511]\n",
      "epoch:29 step:22793 [D loss: 0.041470, acc.: 99.22%] [G loss: 3.768330]\n",
      "epoch:29 step:22794 [D loss: 0.216038, acc.: 93.75%] [G loss: 4.643427]\n",
      "epoch:29 step:22795 [D loss: 0.112419, acc.: 96.09%] [G loss: 6.246274]\n",
      "epoch:29 step:22796 [D loss: 0.048514, acc.: 97.66%] [G loss: 4.527672]\n",
      "epoch:29 step:22797 [D loss: 0.088176, acc.: 96.09%] [G loss: 0.783224]\n",
      "epoch:29 step:22798 [D loss: 0.107898, acc.: 95.31%] [G loss: 4.291476]\n",
      "epoch:29 step:22799 [D loss: 0.031310, acc.: 99.22%] [G loss: 3.481680]\n",
      "epoch:29 step:22800 [D loss: 0.075825, acc.: 100.00%] [G loss: 3.314981]\n",
      "epoch:29 step:22801 [D loss: 0.065755, acc.: 97.66%] [G loss: 4.142968]\n",
      "epoch:29 step:22802 [D loss: 0.271881, acc.: 91.41%] [G loss: 0.673388]\n",
      "epoch:29 step:22803 [D loss: 0.348980, acc.: 80.47%] [G loss: 4.477942]\n",
      "epoch:29 step:22804 [D loss: 0.040240, acc.: 99.22%] [G loss: 6.814481]\n",
      "epoch:29 step:22805 [D loss: 0.884704, acc.: 62.50%] [G loss: 1.630748]\n",
      "epoch:29 step:22806 [D loss: 0.064953, acc.: 97.66%] [G loss: 1.405467]\n",
      "epoch:29 step:22807 [D loss: 0.107466, acc.: 96.88%] [G loss: 2.261748]\n",
      "epoch:29 step:22808 [D loss: 0.008815, acc.: 100.00%] [G loss: 2.676988]\n",
      "epoch:29 step:22809 [D loss: 0.139163, acc.: 96.88%] [G loss: 0.633633]\n",
      "epoch:29 step:22810 [D loss: 0.150830, acc.: 96.88%] [G loss: 2.228344]\n",
      "epoch:29 step:22811 [D loss: 0.003735, acc.: 100.00%] [G loss: 2.940180]\n",
      "epoch:29 step:22812 [D loss: 0.007600, acc.: 100.00%] [G loss: 1.676121]\n",
      "epoch:29 step:22813 [D loss: 0.020387, acc.: 99.22%] [G loss: 0.456493]\n",
      "epoch:29 step:22814 [D loss: 0.010933, acc.: 100.00%] [G loss: 0.348180]\n",
      "epoch:29 step:22815 [D loss: 0.314331, acc.: 85.94%] [G loss: 0.395398]\n",
      "epoch:29 step:22816 [D loss: 0.030693, acc.: 99.22%] [G loss: 0.617139]\n",
      "epoch:29 step:22817 [D loss: 0.013195, acc.: 100.00%] [G loss: 0.559145]\n",
      "epoch:29 step:22818 [D loss: 0.199809, acc.: 94.53%] [G loss: 4.146067]\n",
      "epoch:29 step:22819 [D loss: 0.511473, acc.: 75.00%] [G loss: 1.346306]\n",
      "epoch:29 step:22820 [D loss: 0.268455, acc.: 87.50%] [G loss: 3.898591]\n",
      "epoch:29 step:22821 [D loss: 0.026132, acc.: 100.00%] [G loss: 6.404823]\n",
      "epoch:29 step:22822 [D loss: 0.004172, acc.: 100.00%] [G loss: 4.319447]\n",
      "epoch:29 step:22823 [D loss: 0.494117, acc.: 75.00%] [G loss: 2.796358]\n",
      "epoch:29 step:22824 [D loss: 0.052915, acc.: 100.00%] [G loss: 3.202799]\n",
      "epoch:29 step:22825 [D loss: 0.171753, acc.: 94.53%] [G loss: 3.809798]\n",
      "epoch:29 step:22826 [D loss: 0.031265, acc.: 100.00%] [G loss: 4.311414]\n",
      "epoch:29 step:22827 [D loss: 0.722876, acc.: 64.06%] [G loss: 5.718341]\n",
      "epoch:29 step:22828 [D loss: 0.294785, acc.: 85.16%] [G loss: 3.567560]\n",
      "epoch:29 step:22829 [D loss: 0.042463, acc.: 98.44%] [G loss: 4.589948]\n",
      "epoch:29 step:22830 [D loss: 0.008913, acc.: 100.00%] [G loss: 4.044625]\n",
      "epoch:29 step:22831 [D loss: 0.097503, acc.: 96.09%] [G loss: 2.866104]\n",
      "epoch:29 step:22832 [D loss: 0.024949, acc.: 100.00%] [G loss: 3.117739]\n",
      "epoch:29 step:22833 [D loss: 0.124181, acc.: 97.66%] [G loss: 3.082244]\n",
      "epoch:29 step:22834 [D loss: 0.056875, acc.: 99.22%] [G loss: 2.672150]\n",
      "epoch:29 step:22835 [D loss: 0.213245, acc.: 90.62%] [G loss: 6.228589]\n",
      "epoch:29 step:22836 [D loss: 0.292876, acc.: 86.72%] [G loss: 4.948081]\n",
      "epoch:29 step:22837 [D loss: 0.174300, acc.: 93.75%] [G loss: 3.518026]\n",
      "epoch:29 step:22838 [D loss: 0.094223, acc.: 96.88%] [G loss: 3.431989]\n",
      "epoch:29 step:22839 [D loss: 0.039962, acc.: 100.00%] [G loss: 2.777776]\n",
      "epoch:29 step:22840 [D loss: 0.081676, acc.: 98.44%] [G loss: 3.867888]\n",
      "epoch:29 step:22841 [D loss: 0.093960, acc.: 98.44%] [G loss: 2.089559]\n",
      "epoch:29 step:22842 [D loss: 0.125546, acc.: 96.88%] [G loss: 3.056468]\n",
      "epoch:29 step:22843 [D loss: 0.185398, acc.: 92.19%] [G loss: 2.896573]\n",
      "epoch:29 step:22844 [D loss: 0.093933, acc.: 98.44%] [G loss: 2.775404]\n",
      "epoch:29 step:22845 [D loss: 0.233229, acc.: 90.62%] [G loss: 3.454324]\n",
      "epoch:29 step:22846 [D loss: 0.102022, acc.: 94.53%] [G loss: 1.576177]\n",
      "epoch:29 step:22847 [D loss: 0.267379, acc.: 89.06%] [G loss: 3.948828]\n",
      "epoch:29 step:22848 [D loss: 0.078218, acc.: 96.88%] [G loss: 4.584610]\n",
      "epoch:29 step:22849 [D loss: 0.509070, acc.: 80.47%] [G loss: 1.178698]\n",
      "epoch:29 step:22850 [D loss: 0.170452, acc.: 92.19%] [G loss: 4.992806]\n",
      "epoch:29 step:22851 [D loss: 0.095717, acc.: 96.09%] [G loss: 4.087240]\n",
      "epoch:29 step:22852 [D loss: 0.095052, acc.: 97.66%] [G loss: 1.338423]\n",
      "epoch:29 step:22853 [D loss: 0.052688, acc.: 99.22%] [G loss: 2.869382]\n",
      "epoch:29 step:22854 [D loss: 0.166941, acc.: 96.09%] [G loss: 0.953881]\n",
      "epoch:29 step:22855 [D loss: 0.025482, acc.: 100.00%] [G loss: 0.732532]\n",
      "epoch:29 step:22856 [D loss: 0.205105, acc.: 90.62%] [G loss: 5.928715]\n",
      "epoch:29 step:22857 [D loss: 1.012922, acc.: 59.38%] [G loss: 3.483429]\n",
      "epoch:29 step:22858 [D loss: 0.095340, acc.: 97.66%] [G loss: 4.034519]\n",
      "epoch:29 step:22859 [D loss: 0.045540, acc.: 99.22%] [G loss: 4.313564]\n",
      "epoch:29 step:22860 [D loss: 0.023983, acc.: 98.44%] [G loss: 3.454810]\n",
      "epoch:29 step:22861 [D loss: 0.053896, acc.: 97.66%] [G loss: 3.510174]\n",
      "epoch:29 step:22862 [D loss: 0.065910, acc.: 98.44%] [G loss: 2.092661]\n",
      "epoch:29 step:22863 [D loss: 0.445912, acc.: 80.47%] [G loss: 6.201526]\n",
      "epoch:29 step:22864 [D loss: 0.174150, acc.: 89.84%] [G loss: 5.728160]\n",
      "epoch:29 step:22865 [D loss: 0.231711, acc.: 88.28%] [G loss: 4.392803]\n",
      "epoch:29 step:22866 [D loss: 0.092347, acc.: 97.66%] [G loss: 3.739873]\n",
      "epoch:29 step:22867 [D loss: 0.035174, acc.: 99.22%] [G loss: 3.779230]\n",
      "epoch:29 step:22868 [D loss: 0.072892, acc.: 96.88%] [G loss: 4.737455]\n",
      "epoch:29 step:22869 [D loss: 0.035208, acc.: 99.22%] [G loss: 3.589477]\n",
      "epoch:29 step:22870 [D loss: 0.035773, acc.: 99.22%] [G loss: 3.244290]\n",
      "epoch:29 step:22871 [D loss: 0.137881, acc.: 96.09%] [G loss: 3.111917]\n",
      "epoch:29 step:22872 [D loss: 0.135033, acc.: 96.88%] [G loss: 4.375598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22873 [D loss: 0.046461, acc.: 98.44%] [G loss: 3.799592]\n",
      "epoch:29 step:22874 [D loss: 0.028598, acc.: 99.22%] [G loss: 3.933579]\n",
      "epoch:29 step:22875 [D loss: 0.046284, acc.: 97.66%] [G loss: 3.116522]\n",
      "epoch:29 step:22876 [D loss: 0.306233, acc.: 85.16%] [G loss: 6.419677]\n",
      "epoch:29 step:22877 [D loss: 0.341351, acc.: 80.47%] [G loss: 5.596473]\n",
      "epoch:29 step:22878 [D loss: 0.215253, acc.: 90.62%] [G loss: 4.707656]\n",
      "epoch:29 step:22879 [D loss: 0.075611, acc.: 97.66%] [G loss: 4.362035]\n",
      "epoch:29 step:22880 [D loss: 0.051723, acc.: 99.22%] [G loss: 3.484033]\n",
      "epoch:29 step:22881 [D loss: 0.029292, acc.: 100.00%] [G loss: 5.856521]\n",
      "epoch:29 step:22882 [D loss: 0.032867, acc.: 100.00%] [G loss: 4.375990]\n",
      "epoch:29 step:22883 [D loss: 0.055904, acc.: 99.22%] [G loss: 3.960490]\n",
      "epoch:29 step:22884 [D loss: 0.182260, acc.: 94.53%] [G loss: 5.604198]\n",
      "epoch:29 step:22885 [D loss: 0.619443, acc.: 67.97%] [G loss: 5.659701]\n",
      "epoch:29 step:22886 [D loss: 0.014789, acc.: 100.00%] [G loss: 6.049018]\n",
      "epoch:29 step:22887 [D loss: 0.050112, acc.: 98.44%] [G loss: 5.956392]\n",
      "epoch:29 step:22888 [D loss: 0.009317, acc.: 100.00%] [G loss: 5.477592]\n",
      "epoch:29 step:22889 [D loss: 0.154709, acc.: 93.75%] [G loss: 4.527308]\n",
      "epoch:29 step:22890 [D loss: 0.085513, acc.: 96.88%] [G loss: 4.758904]\n",
      "epoch:29 step:22891 [D loss: 0.017045, acc.: 100.00%] [G loss: 5.099195]\n",
      "epoch:29 step:22892 [D loss: 0.008579, acc.: 100.00%] [G loss: 2.185076]\n",
      "epoch:29 step:22893 [D loss: 0.022772, acc.: 100.00%] [G loss: 4.578149]\n",
      "epoch:29 step:22894 [D loss: 0.111790, acc.: 97.66%] [G loss: 5.883360]\n",
      "epoch:29 step:22895 [D loss: 0.097109, acc.: 96.09%] [G loss: 5.997941]\n",
      "epoch:29 step:22896 [D loss: 0.225868, acc.: 92.97%] [G loss: 4.697628]\n",
      "epoch:29 step:22897 [D loss: 0.017940, acc.: 100.00%] [G loss: 6.118391]\n",
      "epoch:29 step:22898 [D loss: 0.089440, acc.: 96.88%] [G loss: 4.703684]\n",
      "epoch:29 step:22899 [D loss: 0.180056, acc.: 92.19%] [G loss: 5.561182]\n",
      "epoch:29 step:22900 [D loss: 0.002727, acc.: 100.00%] [G loss: 5.971387]\n",
      "epoch:29 step:22901 [D loss: 0.076908, acc.: 97.66%] [G loss: 4.997001]\n",
      "epoch:29 step:22902 [D loss: 0.177352, acc.: 93.75%] [G loss: 7.778234]\n",
      "epoch:29 step:22903 [D loss: 0.042920, acc.: 98.44%] [G loss: 7.788379]\n",
      "epoch:29 step:22904 [D loss: 0.388403, acc.: 81.25%] [G loss: 3.685621]\n",
      "epoch:29 step:22905 [D loss: 0.229569, acc.: 87.50%] [G loss: 8.693934]\n",
      "epoch:29 step:22906 [D loss: 0.135205, acc.: 96.09%] [G loss: 8.214565]\n",
      "epoch:29 step:22907 [D loss: 0.024828, acc.: 99.22%] [G loss: 7.040171]\n",
      "epoch:29 step:22908 [D loss: 0.006863, acc.: 100.00%] [G loss: 5.426466]\n",
      "epoch:29 step:22909 [D loss: 0.023860, acc.: 99.22%] [G loss: 3.807832]\n",
      "epoch:29 step:22910 [D loss: 0.913369, acc.: 67.97%] [G loss: 9.160452]\n",
      "epoch:29 step:22911 [D loss: 1.647801, acc.: 53.12%] [G loss: 5.934459]\n",
      "epoch:29 step:22912 [D loss: 0.193972, acc.: 89.84%] [G loss: 3.850351]\n",
      "epoch:29 step:22913 [D loss: 0.099456, acc.: 97.66%] [G loss: 3.637401]\n",
      "epoch:29 step:22914 [D loss: 0.112766, acc.: 98.44%] [G loss: 3.540173]\n",
      "epoch:29 step:22915 [D loss: 0.044672, acc.: 98.44%] [G loss: 3.570717]\n",
      "epoch:29 step:22916 [D loss: 0.073768, acc.: 98.44%] [G loss: 3.414485]\n",
      "epoch:29 step:22917 [D loss: 0.052065, acc.: 99.22%] [G loss: 3.367282]\n",
      "epoch:29 step:22918 [D loss: 0.095125, acc.: 98.44%] [G loss: 3.769223]\n",
      "epoch:29 step:22919 [D loss: 0.048270, acc.: 99.22%] [G loss: 3.205979]\n",
      "epoch:29 step:22920 [D loss: 0.130961, acc.: 99.22%] [G loss: 3.202869]\n",
      "epoch:29 step:22921 [D loss: 0.060195, acc.: 99.22%] [G loss: 3.247885]\n",
      "epoch:29 step:22922 [D loss: 0.454228, acc.: 78.91%] [G loss: 4.996354]\n",
      "epoch:29 step:22923 [D loss: 0.156513, acc.: 92.97%] [G loss: 3.605917]\n",
      "epoch:29 step:22924 [D loss: 0.456809, acc.: 78.91%] [G loss: 5.274452]\n",
      "epoch:29 step:22925 [D loss: 0.114942, acc.: 94.53%] [G loss: 5.496943]\n",
      "epoch:29 step:22926 [D loss: 0.614051, acc.: 71.09%] [G loss: 3.355498]\n",
      "epoch:29 step:22927 [D loss: 0.039051, acc.: 100.00%] [G loss: 4.139889]\n",
      "epoch:29 step:22928 [D loss: 0.086920, acc.: 96.09%] [G loss: 2.387215]\n",
      "epoch:29 step:22929 [D loss: 0.069720, acc.: 99.22%] [G loss: 3.810727]\n",
      "epoch:29 step:22930 [D loss: 0.047307, acc.: 99.22%] [G loss: 2.497167]\n",
      "epoch:29 step:22931 [D loss: 0.043746, acc.: 99.22%] [G loss: 3.339971]\n",
      "epoch:29 step:22932 [D loss: 0.097677, acc.: 96.88%] [G loss: 3.320780]\n",
      "epoch:29 step:22933 [D loss: 0.054261, acc.: 98.44%] [G loss: 3.427276]\n",
      "epoch:29 step:22934 [D loss: 0.074257, acc.: 99.22%] [G loss: 3.025238]\n",
      "epoch:29 step:22935 [D loss: 0.748966, acc.: 64.84%] [G loss: 7.851376]\n",
      "epoch:29 step:22936 [D loss: 0.777340, acc.: 67.19%] [G loss: 6.204926]\n",
      "epoch:29 step:22937 [D loss: 0.098119, acc.: 95.31%] [G loss: 4.503494]\n",
      "epoch:29 step:22938 [D loss: 0.034381, acc.: 99.22%] [G loss: 5.025045]\n",
      "epoch:29 step:22939 [D loss: 0.021368, acc.: 99.22%] [G loss: 3.954800]\n",
      "epoch:29 step:22940 [D loss: 0.142460, acc.: 96.88%] [G loss: 3.809338]\n",
      "epoch:29 step:22941 [D loss: 0.026882, acc.: 99.22%] [G loss: 3.409680]\n",
      "epoch:29 step:22942 [D loss: 0.043547, acc.: 99.22%] [G loss: 4.210623]\n",
      "epoch:29 step:22943 [D loss: 0.058562, acc.: 99.22%] [G loss: 2.764441]\n",
      "epoch:29 step:22944 [D loss: 0.066158, acc.: 97.66%] [G loss: 2.463511]\n",
      "epoch:29 step:22945 [D loss: 0.356446, acc.: 83.59%] [G loss: 4.696104]\n",
      "epoch:29 step:22946 [D loss: 0.352011, acc.: 83.59%] [G loss: 4.907759]\n",
      "epoch:29 step:22947 [D loss: 0.049342, acc.: 98.44%] [G loss: 3.497617]\n",
      "epoch:29 step:22948 [D loss: 0.040627, acc.: 100.00%] [G loss: 4.016705]\n",
      "epoch:29 step:22949 [D loss: 0.039336, acc.: 98.44%] [G loss: 3.438540]\n",
      "epoch:29 step:22950 [D loss: 0.143550, acc.: 93.75%] [G loss: 4.910905]\n",
      "epoch:29 step:22951 [D loss: 0.091328, acc.: 96.88%] [G loss: 5.338344]\n",
      "epoch:29 step:22952 [D loss: 0.078116, acc.: 97.66%] [G loss: 3.610677]\n",
      "epoch:29 step:22953 [D loss: 0.029463, acc.: 100.00%] [G loss: 4.822081]\n",
      "epoch:29 step:22954 [D loss: 0.279470, acc.: 85.16%] [G loss: 3.746620]\n",
      "epoch:29 step:22955 [D loss: 0.047294, acc.: 100.00%] [G loss: 2.564851]\n",
      "epoch:29 step:22956 [D loss: 0.008671, acc.: 100.00%] [G loss: 1.781785]\n",
      "epoch:29 step:22957 [D loss: 0.024585, acc.: 100.00%] [G loss: 0.643037]\n",
      "epoch:29 step:22958 [D loss: 0.121575, acc.: 95.31%] [G loss: 2.294163]\n",
      "epoch:29 step:22959 [D loss: 0.034554, acc.: 99.22%] [G loss: 1.194263]\n",
      "epoch:29 step:22960 [D loss: 0.038779, acc.: 99.22%] [G loss: 0.909398]\n",
      "epoch:29 step:22961 [D loss: 0.065241, acc.: 98.44%] [G loss: 0.375437]\n",
      "epoch:29 step:22962 [D loss: 0.021892, acc.: 100.00%] [G loss: 0.787616]\n",
      "epoch:29 step:22963 [D loss: 0.025598, acc.: 100.00%] [G loss: 0.245793]\n",
      "epoch:29 step:22964 [D loss: 1.644850, acc.: 39.06%] [G loss: 7.269474]\n",
      "epoch:29 step:22965 [D loss: 0.640066, acc.: 71.88%] [G loss: 8.236661]\n",
      "epoch:29 step:22966 [D loss: 0.186849, acc.: 89.84%] [G loss: 3.097561]\n",
      "epoch:29 step:22967 [D loss: 0.007106, acc.: 100.00%] [G loss: 3.696967]\n",
      "epoch:29 step:22968 [D loss: 0.021916, acc.: 99.22%] [G loss: 1.302039]\n",
      "epoch:29 step:22969 [D loss: 0.221268, acc.: 89.06%] [G loss: 3.149150]\n",
      "epoch:29 step:22970 [D loss: 0.155963, acc.: 93.75%] [G loss: 3.102559]\n",
      "epoch:29 step:22971 [D loss: 0.015893, acc.: 100.00%] [G loss: 0.797560]\n",
      "epoch:29 step:22972 [D loss: 0.004512, acc.: 100.00%] [G loss: 1.116292]\n",
      "epoch:29 step:22973 [D loss: 0.121983, acc.: 94.53%] [G loss: 0.113803]\n",
      "epoch:29 step:22974 [D loss: 0.124908, acc.: 96.88%] [G loss: 1.201519]\n",
      "epoch:29 step:22975 [D loss: 0.034642, acc.: 99.22%] [G loss: 4.936478]\n",
      "epoch:29 step:22976 [D loss: 0.152251, acc.: 94.53%] [G loss: 0.298547]\n",
      "epoch:29 step:22977 [D loss: 0.074265, acc.: 100.00%] [G loss: 0.381008]\n",
      "epoch:29 step:22978 [D loss: 0.018201, acc.: 100.00%] [G loss: 1.343790]\n",
      "epoch:29 step:22979 [D loss: 0.057324, acc.: 99.22%] [G loss: 0.417605]\n",
      "epoch:29 step:22980 [D loss: 0.049928, acc.: 98.44%] [G loss: 0.580407]\n",
      "epoch:29 step:22981 [D loss: 0.329813, acc.: 85.16%] [G loss: 5.704090]\n",
      "epoch:29 step:22982 [D loss: 0.600446, acc.: 77.34%] [G loss: 4.066752]\n",
      "epoch:29 step:22983 [D loss: 0.446874, acc.: 78.12%] [G loss: 5.669056]\n",
      "epoch:29 step:22984 [D loss: 0.132233, acc.: 92.19%] [G loss: 5.714784]\n",
      "epoch:29 step:22985 [D loss: 0.112453, acc.: 95.31%] [G loss: 5.616270]\n",
      "epoch:29 step:22986 [D loss: 0.014806, acc.: 100.00%] [G loss: 2.978152]\n",
      "epoch:29 step:22987 [D loss: 0.094233, acc.: 96.09%] [G loss: 2.935781]\n",
      "epoch:29 step:22988 [D loss: 0.003428, acc.: 100.00%] [G loss: 4.589024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:22989 [D loss: 0.037407, acc.: 98.44%] [G loss: 2.411816]\n",
      "epoch:29 step:22990 [D loss: 0.019304, acc.: 100.00%] [G loss: 1.849372]\n",
      "epoch:29 step:22991 [D loss: 0.012595, acc.: 100.00%] [G loss: 2.762609]\n",
      "epoch:29 step:22992 [D loss: 0.025819, acc.: 100.00%] [G loss: 1.605411]\n",
      "epoch:29 step:22993 [D loss: 0.027508, acc.: 100.00%] [G loss: 0.637310]\n",
      "epoch:29 step:22994 [D loss: 0.045509, acc.: 99.22%] [G loss: 1.065908]\n",
      "epoch:29 step:22995 [D loss: 0.044595, acc.: 99.22%] [G loss: 2.503015]\n",
      "epoch:29 step:22996 [D loss: 0.054266, acc.: 99.22%] [G loss: 2.129115]\n",
      "epoch:29 step:22997 [D loss: 0.113818, acc.: 96.09%] [G loss: 1.410835]\n",
      "epoch:29 step:22998 [D loss: 0.008288, acc.: 100.00%] [G loss: 1.678795]\n",
      "epoch:29 step:22999 [D loss: 0.430407, acc.: 81.25%] [G loss: 1.270953]\n",
      "epoch:29 step:23000 [D loss: 0.046297, acc.: 97.66%] [G loss: 0.344716]\n",
      "epoch:29 step:23001 [D loss: 0.057204, acc.: 97.66%] [G loss: 0.871762]\n",
      "epoch:29 step:23002 [D loss: 0.104598, acc.: 98.44%] [G loss: 0.843844]\n",
      "epoch:29 step:23003 [D loss: 0.021201, acc.: 100.00%] [G loss: 1.041168]\n",
      "epoch:29 step:23004 [D loss: 0.046010, acc.: 100.00%] [G loss: 1.053151]\n",
      "epoch:29 step:23005 [D loss: 0.080041, acc.: 97.66%] [G loss: 5.995114]\n",
      "epoch:29 step:23006 [D loss: 0.025184, acc.: 99.22%] [G loss: 3.153366]\n",
      "epoch:29 step:23007 [D loss: 0.087051, acc.: 96.88%] [G loss: 4.662229]\n",
      "epoch:29 step:23008 [D loss: 0.015399, acc.: 100.00%] [G loss: 0.557954]\n",
      "epoch:29 step:23009 [D loss: 0.106180, acc.: 96.09%] [G loss: 3.713131]\n",
      "epoch:29 step:23010 [D loss: 0.114515, acc.: 95.31%] [G loss: 2.918018]\n",
      "epoch:29 step:23011 [D loss: 0.034061, acc.: 99.22%] [G loss: 1.584432]\n",
      "epoch:29 step:23012 [D loss: 0.035209, acc.: 99.22%] [G loss: 1.870228]\n",
      "epoch:29 step:23013 [D loss: 0.012295, acc.: 100.00%] [G loss: 0.491429]\n",
      "epoch:29 step:23014 [D loss: 0.004838, acc.: 100.00%] [G loss: 0.836993]\n",
      "epoch:29 step:23015 [D loss: 0.037711, acc.: 100.00%] [G loss: 1.036503]\n",
      "epoch:29 step:23016 [D loss: 0.024859, acc.: 100.00%] [G loss: 0.565359]\n",
      "epoch:29 step:23017 [D loss: 0.005034, acc.: 100.00%] [G loss: 3.416603]\n",
      "epoch:29 step:23018 [D loss: 0.019131, acc.: 100.00%] [G loss: 0.670481]\n",
      "epoch:29 step:23019 [D loss: 0.147197, acc.: 95.31%] [G loss: 3.030302]\n",
      "epoch:29 step:23020 [D loss: 0.279369, acc.: 86.72%] [G loss: 3.817922]\n",
      "epoch:29 step:23021 [D loss: 0.345391, acc.: 85.94%] [G loss: 3.672748]\n",
      "epoch:29 step:23022 [D loss: 0.593231, acc.: 73.44%] [G loss: 1.411780]\n",
      "epoch:29 step:23023 [D loss: 0.029136, acc.: 98.44%] [G loss: 2.829477]\n",
      "epoch:29 step:23024 [D loss: 0.155533, acc.: 92.19%] [G loss: 2.479812]\n",
      "epoch:29 step:23025 [D loss: 0.014913, acc.: 100.00%] [G loss: 1.348711]\n",
      "epoch:29 step:23026 [D loss: 0.100045, acc.: 96.88%] [G loss: 0.825790]\n",
      "epoch:29 step:23027 [D loss: 0.055767, acc.: 98.44%] [G loss: 2.622314]\n",
      "epoch:29 step:23028 [D loss: 0.018723, acc.: 99.22%] [G loss: 1.408640]\n",
      "epoch:29 step:23029 [D loss: 0.052023, acc.: 99.22%] [G loss: 0.646089]\n",
      "epoch:29 step:23030 [D loss: 0.100891, acc.: 97.66%] [G loss: 2.059242]\n",
      "epoch:29 step:23031 [D loss: 0.040999, acc.: 98.44%] [G loss: 2.141547]\n",
      "epoch:29 step:23032 [D loss: 0.007029, acc.: 100.00%] [G loss: 2.458817]\n",
      "epoch:29 step:23033 [D loss: 0.005833, acc.: 100.00%] [G loss: 5.151955]\n",
      "epoch:29 step:23034 [D loss: 0.021739, acc.: 100.00%] [G loss: 1.636501]\n",
      "epoch:29 step:23035 [D loss: 0.165814, acc.: 94.53%] [G loss: 2.214896]\n",
      "epoch:29 step:23036 [D loss: 0.002722, acc.: 100.00%] [G loss: 2.992748]\n",
      "epoch:29 step:23037 [D loss: 0.628712, acc.: 67.19%] [G loss: 3.256416]\n",
      "epoch:29 step:23038 [D loss: 0.003538, acc.: 100.00%] [G loss: 4.149114]\n",
      "epoch:29 step:23039 [D loss: 0.013820, acc.: 100.00%] [G loss: 4.049397]\n",
      "epoch:29 step:23040 [D loss: 0.057494, acc.: 99.22%] [G loss: 2.378558]\n",
      "epoch:29 step:23041 [D loss: 0.016894, acc.: 100.00%] [G loss: 2.143040]\n",
      "epoch:29 step:23042 [D loss: 0.012893, acc.: 100.00%] [G loss: 2.553396]\n",
      "epoch:29 step:23043 [D loss: 0.002998, acc.: 100.00%] [G loss: 1.954209]\n",
      "epoch:29 step:23044 [D loss: 0.101284, acc.: 97.66%] [G loss: 1.368583]\n",
      "epoch:29 step:23045 [D loss: 0.253612, acc.: 92.19%] [G loss: 5.799990]\n",
      "epoch:29 step:23046 [D loss: 0.105416, acc.: 94.53%] [G loss: 6.237738]\n",
      "epoch:29 step:23047 [D loss: 0.077678, acc.: 96.88%] [G loss: 5.352090]\n",
      "epoch:29 step:23048 [D loss: 0.013853, acc.: 99.22%] [G loss: 4.334933]\n",
      "epoch:29 step:23049 [D loss: 0.025234, acc.: 99.22%] [G loss: 4.551428]\n",
      "epoch:29 step:23050 [D loss: 0.036229, acc.: 98.44%] [G loss: 4.773322]\n",
      "epoch:29 step:23051 [D loss: 0.106783, acc.: 99.22%] [G loss: 6.379037]\n",
      "epoch:29 step:23052 [D loss: 0.092426, acc.: 97.66%] [G loss: 5.752816]\n",
      "epoch:29 step:23053 [D loss: 0.023269, acc.: 99.22%] [G loss: 6.394434]\n",
      "epoch:29 step:23054 [D loss: 0.022461, acc.: 100.00%] [G loss: 6.288234]\n",
      "epoch:29 step:23055 [D loss: 0.008507, acc.: 100.00%] [G loss: 6.073138]\n",
      "epoch:29 step:23056 [D loss: 0.003025, acc.: 100.00%] [G loss: 0.355649]\n",
      "epoch:29 step:23057 [D loss: 0.018071, acc.: 100.00%] [G loss: 6.414699]\n",
      "epoch:29 step:23058 [D loss: 0.025926, acc.: 99.22%] [G loss: 5.221313]\n",
      "epoch:29 step:23059 [D loss: 0.004164, acc.: 100.00%] [G loss: 5.694576]\n",
      "epoch:29 step:23060 [D loss: 0.110894, acc.: 96.09%] [G loss: 4.450989]\n",
      "epoch:29 step:23061 [D loss: 0.120980, acc.: 94.53%] [G loss: 5.852792]\n",
      "epoch:29 step:23062 [D loss: 0.036412, acc.: 99.22%] [G loss: 5.331700]\n",
      "epoch:29 step:23063 [D loss: 0.048553, acc.: 98.44%] [G loss: 4.146804]\n",
      "epoch:29 step:23064 [D loss: 0.015999, acc.: 100.00%] [G loss: 3.436308]\n",
      "epoch:29 step:23065 [D loss: 0.076079, acc.: 99.22%] [G loss: 5.996701]\n",
      "epoch:29 step:23066 [D loss: 0.029690, acc.: 100.00%] [G loss: 5.922591]\n",
      "epoch:29 step:23067 [D loss: 0.036802, acc.: 98.44%] [G loss: 5.422882]\n",
      "epoch:29 step:23068 [D loss: 0.015173, acc.: 100.00%] [G loss: 5.825418]\n",
      "epoch:29 step:23069 [D loss: 0.031550, acc.: 100.00%] [G loss: 4.062588]\n",
      "epoch:29 step:23070 [D loss: 0.014834, acc.: 100.00%] [G loss: 4.532422]\n",
      "epoch:29 step:23071 [D loss: 0.015843, acc.: 100.00%] [G loss: 3.989264]\n",
      "epoch:29 step:23072 [D loss: 0.033799, acc.: 100.00%] [G loss: 5.997674]\n",
      "epoch:29 step:23073 [D loss: 0.101525, acc.: 96.88%] [G loss: 4.612371]\n",
      "epoch:29 step:23074 [D loss: 0.035265, acc.: 100.00%] [G loss: 3.003145]\n",
      "epoch:29 step:23075 [D loss: 0.010121, acc.: 100.00%] [G loss: 7.414375]\n",
      "epoch:29 step:23076 [D loss: 0.005334, acc.: 100.00%] [G loss: 7.129157]\n",
      "epoch:29 step:23077 [D loss: 0.081966, acc.: 96.88%] [G loss: 5.318244]\n",
      "epoch:29 step:23078 [D loss: 0.025319, acc.: 100.00%] [G loss: 0.035353]\n",
      "epoch:29 step:23079 [D loss: 0.326050, acc.: 83.59%] [G loss: 8.678523]\n",
      "epoch:29 step:23080 [D loss: 2.396795, acc.: 51.56%] [G loss: 3.434278]\n",
      "epoch:29 step:23081 [D loss: 0.040574, acc.: 99.22%] [G loss: 1.974648]\n",
      "epoch:29 step:23082 [D loss: 0.029722, acc.: 100.00%] [G loss: 2.165572]\n",
      "epoch:29 step:23083 [D loss: 0.006167, acc.: 100.00%] [G loss: 1.505650]\n",
      "epoch:29 step:23084 [D loss: 0.014327, acc.: 100.00%] [G loss: 1.822426]\n",
      "epoch:29 step:23085 [D loss: 0.096996, acc.: 98.44%] [G loss: 5.004535]\n",
      "epoch:29 step:23086 [D loss: 0.010047, acc.: 100.00%] [G loss: 3.999383]\n",
      "epoch:29 step:23087 [D loss: 0.030093, acc.: 99.22%] [G loss: 3.529453]\n",
      "epoch:29 step:23088 [D loss: 0.055940, acc.: 98.44%] [G loss: 0.337462]\n",
      "epoch:29 step:23089 [D loss: 0.004388, acc.: 100.00%] [G loss: 1.487757]\n",
      "epoch:29 step:23090 [D loss: 0.005098, acc.: 100.00%] [G loss: 1.180916]\n",
      "epoch:29 step:23091 [D loss: 0.108460, acc.: 95.31%] [G loss: 3.573555]\n",
      "epoch:29 step:23092 [D loss: 0.002837, acc.: 100.00%] [G loss: 3.905535]\n",
      "epoch:29 step:23093 [D loss: 0.037749, acc.: 98.44%] [G loss: 3.268218]\n",
      "epoch:29 step:23094 [D loss: 0.089003, acc.: 96.09%] [G loss: 1.887949]\n",
      "epoch:29 step:23095 [D loss: 0.042265, acc.: 99.22%] [G loss: 2.009207]\n",
      "epoch:29 step:23096 [D loss: 0.014843, acc.: 100.00%] [G loss: 2.420385]\n",
      "epoch:29 step:23097 [D loss: 0.074457, acc.: 99.22%] [G loss: 1.169992]\n",
      "epoch:29 step:23098 [D loss: 0.002498, acc.: 100.00%] [G loss: 7.614677]\n",
      "epoch:29 step:23099 [D loss: 0.035978, acc.: 99.22%] [G loss: 7.508740]\n",
      "epoch:29 step:23100 [D loss: 0.036525, acc.: 99.22%] [G loss: 5.071176]\n",
      "epoch:29 step:23101 [D loss: 0.010379, acc.: 100.00%] [G loss: 4.904766]\n",
      "epoch:29 step:23102 [D loss: 0.043921, acc.: 100.00%] [G loss: 5.469444]\n",
      "epoch:29 step:23103 [D loss: 0.011918, acc.: 100.00%] [G loss: 5.119847]\n",
      "epoch:29 step:23104 [D loss: 0.021714, acc.: 98.44%] [G loss: 4.716861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23105 [D loss: 0.058451, acc.: 100.00%] [G loss: 5.801821]\n",
      "epoch:29 step:23106 [D loss: 0.039645, acc.: 99.22%] [G loss: 4.799538]\n",
      "epoch:29 step:23107 [D loss: 0.034978, acc.: 99.22%] [G loss: 4.126245]\n",
      "epoch:29 step:23108 [D loss: 0.233210, acc.: 90.62%] [G loss: 7.673989]\n",
      "epoch:29 step:23109 [D loss: 0.584841, acc.: 75.78%] [G loss: 0.026649]\n",
      "epoch:29 step:23110 [D loss: 0.184866, acc.: 92.19%] [G loss: 8.560799]\n",
      "epoch:29 step:23111 [D loss: 0.004184, acc.: 100.00%] [G loss: 0.514270]\n",
      "epoch:29 step:23112 [D loss: 0.065244, acc.: 96.09%] [G loss: 8.284101]\n",
      "epoch:29 step:23113 [D loss: 0.026195, acc.: 98.44%] [G loss: 7.014193]\n",
      "epoch:29 step:23114 [D loss: 0.044723, acc.: 99.22%] [G loss: 7.762117]\n",
      "epoch:29 step:23115 [D loss: 0.000748, acc.: 100.00%] [G loss: 7.335103]\n",
      "epoch:29 step:23116 [D loss: 0.029231, acc.: 99.22%] [G loss: 6.042106]\n",
      "epoch:29 step:23117 [D loss: 0.004245, acc.: 100.00%] [G loss: 6.355092]\n",
      "epoch:29 step:23118 [D loss: 0.001433, acc.: 100.00%] [G loss: 5.472862]\n",
      "epoch:29 step:23119 [D loss: 0.005154, acc.: 100.00%] [G loss: 4.947785]\n",
      "epoch:29 step:23120 [D loss: 0.003846, acc.: 100.00%] [G loss: 4.851375]\n",
      "epoch:29 step:23121 [D loss: 0.089444, acc.: 96.88%] [G loss: 5.275509]\n",
      "epoch:29 step:23122 [D loss: 0.001539, acc.: 100.00%] [G loss: 6.449982]\n",
      "epoch:29 step:23123 [D loss: 0.009734, acc.: 100.00%] [G loss: 5.047500]\n",
      "epoch:29 step:23124 [D loss: 0.021418, acc.: 99.22%] [G loss: 5.801668]\n",
      "epoch:29 step:23125 [D loss: 0.042754, acc.: 99.22%] [G loss: 5.839945]\n",
      "epoch:29 step:23126 [D loss: 0.003521, acc.: 100.00%] [G loss: 0.163894]\n",
      "epoch:29 step:23127 [D loss: 0.031616, acc.: 99.22%] [G loss: 0.052550]\n",
      "epoch:29 step:23128 [D loss: 0.020618, acc.: 99.22%] [G loss: 0.793742]\n",
      "epoch:29 step:23129 [D loss: 0.004670, acc.: 100.00%] [G loss: 0.199430]\n",
      "epoch:29 step:23130 [D loss: 0.055975, acc.: 99.22%] [G loss: 6.418869]\n",
      "epoch:29 step:23131 [D loss: 0.011837, acc.: 100.00%] [G loss: 0.020980]\n",
      "epoch:29 step:23132 [D loss: 0.055877, acc.: 99.22%] [G loss: 5.914868]\n",
      "epoch:29 step:23133 [D loss: 0.001193, acc.: 100.00%] [G loss: 6.244431]\n",
      "epoch:29 step:23134 [D loss: 0.051348, acc.: 98.44%] [G loss: 5.042486]\n",
      "epoch:29 step:23135 [D loss: 0.012831, acc.: 100.00%] [G loss: 4.277589]\n",
      "epoch:29 step:23136 [D loss: 0.005844, acc.: 100.00%] [G loss: 4.058434]\n",
      "epoch:29 step:23137 [D loss: 0.002152, acc.: 100.00%] [G loss: 2.992533]\n",
      "epoch:29 step:23138 [D loss: 0.018170, acc.: 100.00%] [G loss: 2.481457]\n",
      "epoch:29 step:23139 [D loss: 0.003568, acc.: 100.00%] [G loss: 0.080811]\n",
      "epoch:29 step:23140 [D loss: 0.007962, acc.: 100.00%] [G loss: 1.165023]\n",
      "epoch:29 step:23141 [D loss: 0.060833, acc.: 97.66%] [G loss: 2.406915]\n",
      "epoch:29 step:23142 [D loss: 0.077063, acc.: 99.22%] [G loss: 1.942076]\n",
      "epoch:29 step:23143 [D loss: 0.000899, acc.: 100.00%] [G loss: 2.334208]\n",
      "epoch:29 step:23144 [D loss: 0.031686, acc.: 99.22%] [G loss: 1.436333]\n",
      "epoch:29 step:23145 [D loss: 0.002981, acc.: 100.00%] [G loss: 1.829740]\n",
      "epoch:29 step:23146 [D loss: 0.020496, acc.: 100.00%] [G loss: 1.045093]\n",
      "epoch:29 step:23147 [D loss: 0.243530, acc.: 90.62%] [G loss: 4.043080]\n",
      "epoch:29 step:23148 [D loss: 0.128684, acc.: 92.97%] [G loss: 4.703867]\n",
      "epoch:29 step:23149 [D loss: 0.009014, acc.: 100.00%] [G loss: 3.283859]\n",
      "epoch:29 step:23150 [D loss: 0.322924, acc.: 87.50%] [G loss: 0.848105]\n",
      "epoch:29 step:23151 [D loss: 0.088906, acc.: 96.09%] [G loss: 1.968137]\n",
      "epoch:29 step:23152 [D loss: 0.002001, acc.: 100.00%] [G loss: 1.648174]\n",
      "epoch:29 step:23153 [D loss: 0.006419, acc.: 100.00%] [G loss: 1.715276]\n",
      "epoch:29 step:23154 [D loss: 0.003938, acc.: 100.00%] [G loss: 1.762935]\n",
      "epoch:29 step:23155 [D loss: 0.010659, acc.: 100.00%] [G loss: 1.107263]\n",
      "epoch:29 step:23156 [D loss: 0.019205, acc.: 100.00%] [G loss: 1.913877]\n",
      "epoch:29 step:23157 [D loss: 0.009765, acc.: 100.00%] [G loss: 1.885675]\n",
      "epoch:29 step:23158 [D loss: 0.105055, acc.: 98.44%] [G loss: 3.538734]\n",
      "epoch:29 step:23159 [D loss: 0.015755, acc.: 99.22%] [G loss: 4.799644]\n",
      "epoch:29 step:23160 [D loss: 0.066340, acc.: 98.44%] [G loss: 2.088046]\n",
      "epoch:29 step:23161 [D loss: 0.028445, acc.: 99.22%] [G loss: 4.287531]\n",
      "epoch:29 step:23162 [D loss: 0.079227, acc.: 99.22%] [G loss: 5.359129]\n",
      "epoch:29 step:23163 [D loss: 0.006452, acc.: 100.00%] [G loss: 0.102003]\n",
      "epoch:29 step:23164 [D loss: 0.124989, acc.: 93.75%] [G loss: 3.993274]\n",
      "epoch:29 step:23165 [D loss: 0.024528, acc.: 100.00%] [G loss: 1.913688]\n",
      "epoch:29 step:23166 [D loss: 0.011108, acc.: 100.00%] [G loss: 0.013102]\n",
      "epoch:29 step:23167 [D loss: 0.004479, acc.: 100.00%] [G loss: 2.047236]\n",
      "epoch:29 step:23168 [D loss: 0.011709, acc.: 100.00%] [G loss: 1.018851]\n",
      "epoch:29 step:23169 [D loss: 0.039253, acc.: 100.00%] [G loss: 2.320590]\n",
      "epoch:29 step:23170 [D loss: 0.016802, acc.: 100.00%] [G loss: 4.368738]\n",
      "epoch:29 step:23171 [D loss: 0.539428, acc.: 75.00%] [G loss: 5.861527]\n",
      "epoch:29 step:23172 [D loss: 1.868983, acc.: 55.47%] [G loss: 6.349628]\n",
      "epoch:29 step:23173 [D loss: 0.025777, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:29 step:23174 [D loss: 0.002786, acc.: 100.00%] [G loss: 3.568940]\n",
      "epoch:29 step:23175 [D loss: 0.067201, acc.: 97.66%] [G loss: 0.692473]\n",
      "epoch:29 step:23176 [D loss: 0.008741, acc.: 100.00%] [G loss: 0.134007]\n",
      "epoch:29 step:23177 [D loss: 0.007229, acc.: 100.00%] [G loss: 0.301980]\n",
      "epoch:29 step:23178 [D loss: 0.049652, acc.: 99.22%] [G loss: 0.405811]\n",
      "epoch:29 step:23179 [D loss: 0.105504, acc.: 97.66%] [G loss: 2.075159]\n",
      "epoch:29 step:23180 [D loss: 0.013815, acc.: 100.00%] [G loss: 1.097903]\n",
      "epoch:29 step:23181 [D loss: 0.776286, acc.: 60.16%] [G loss: 4.399398]\n",
      "epoch:29 step:23182 [D loss: 0.230017, acc.: 89.06%] [G loss: 3.780554]\n",
      "epoch:29 step:23183 [D loss: 0.033894, acc.: 99.22%] [G loss: 1.952307]\n",
      "epoch:29 step:23184 [D loss: 0.078587, acc.: 98.44%] [G loss: 0.984842]\n",
      "epoch:29 step:23185 [D loss: 0.006625, acc.: 100.00%] [G loss: 0.906486]\n",
      "epoch:29 step:23186 [D loss: 0.527588, acc.: 71.09%] [G loss: 0.268090]\n",
      "epoch:29 step:23187 [D loss: 0.046501, acc.: 97.66%] [G loss: 5.708010]\n",
      "epoch:29 step:23188 [D loss: 0.026251, acc.: 100.00%] [G loss: 4.461894]\n",
      "epoch:29 step:23189 [D loss: 0.137217, acc.: 95.31%] [G loss: 2.325371]\n",
      "epoch:29 step:23190 [D loss: 0.014972, acc.: 100.00%] [G loss: 0.438023]\n",
      "epoch:29 step:23191 [D loss: 0.061442, acc.: 97.66%] [G loss: 0.708392]\n",
      "epoch:29 step:23192 [D loss: 0.006258, acc.: 100.00%] [G loss: 0.663047]\n",
      "epoch:29 step:23193 [D loss: 0.003541, acc.: 100.00%] [G loss: 0.009185]\n",
      "epoch:29 step:23194 [D loss: 0.008061, acc.: 100.00%] [G loss: 0.156977]\n",
      "epoch:29 step:23195 [D loss: 0.032975, acc.: 99.22%] [G loss: 0.391946]\n",
      "epoch:29 step:23196 [D loss: 0.071766, acc.: 99.22%] [G loss: 0.029475]\n",
      "epoch:29 step:23197 [D loss: 0.004935, acc.: 100.00%] [G loss: 0.726837]\n",
      "epoch:29 step:23198 [D loss: 0.005602, acc.: 100.00%] [G loss: 0.477670]\n",
      "epoch:29 step:23199 [D loss: 0.097272, acc.: 96.88%] [G loss: 0.396820]\n",
      "epoch:29 step:23200 [D loss: 0.019085, acc.: 100.00%] [G loss: 0.850605]\n",
      "epoch:29 step:23201 [D loss: 0.144036, acc.: 95.31%] [G loss: 2.529515]\n",
      "epoch:29 step:23202 [D loss: 0.058111, acc.: 97.66%] [G loss: 2.015237]\n",
      "epoch:29 step:23203 [D loss: 0.111261, acc.: 95.31%] [G loss: 0.590692]\n",
      "epoch:29 step:23204 [D loss: 0.142721, acc.: 95.31%] [G loss: 0.950644]\n",
      "epoch:29 step:23205 [D loss: 0.180671, acc.: 93.75%] [G loss: 0.629503]\n",
      "epoch:29 step:23206 [D loss: 0.037111, acc.: 99.22%] [G loss: 0.068231]\n",
      "epoch:29 step:23207 [D loss: 0.042291, acc.: 98.44%] [G loss: 0.406286]\n",
      "epoch:29 step:23208 [D loss: 0.000915, acc.: 100.00%] [G loss: 0.038698]\n",
      "epoch:29 step:23209 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.151467]\n",
      "epoch:29 step:23210 [D loss: 0.004760, acc.: 100.00%] [G loss: 0.051425]\n",
      "epoch:29 step:23211 [D loss: 0.004762, acc.: 100.00%] [G loss: 0.027597]\n",
      "epoch:29 step:23212 [D loss: 0.003695, acc.: 100.00%] [G loss: 0.022995]\n",
      "epoch:29 step:23213 [D loss: 0.000967, acc.: 100.00%] [G loss: 0.014417]\n",
      "epoch:29 step:23214 [D loss: 0.004708, acc.: 100.00%] [G loss: 0.022112]\n",
      "epoch:29 step:23215 [D loss: 0.010119, acc.: 100.00%] [G loss: 0.007795]\n",
      "epoch:29 step:23216 [D loss: 0.010323, acc.: 100.00%] [G loss: 0.014270]\n",
      "epoch:29 step:23217 [D loss: 0.002035, acc.: 100.00%] [G loss: 0.002350]\n",
      "epoch:29 step:23218 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.016299]\n",
      "epoch:29 step:23219 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.004939]\n",
      "epoch:29 step:23220 [D loss: 0.000907, acc.: 100.00%] [G loss: 0.004888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23221 [D loss: 0.016262, acc.: 100.00%] [G loss: 0.019542]\n",
      "epoch:29 step:23222 [D loss: 0.002669, acc.: 100.00%] [G loss: 0.014189]\n",
      "epoch:29 step:23223 [D loss: 0.005054, acc.: 100.00%] [G loss: 0.026099]\n",
      "epoch:29 step:23224 [D loss: 0.001788, acc.: 100.00%] [G loss: 0.045636]\n",
      "epoch:29 step:23225 [D loss: 0.008370, acc.: 100.00%] [G loss: 0.008325]\n",
      "epoch:29 step:23226 [D loss: 0.044627, acc.: 99.22%] [G loss: 0.004053]\n",
      "epoch:29 step:23227 [D loss: 0.033402, acc.: 98.44%] [G loss: 0.080913]\n",
      "epoch:29 step:23228 [D loss: 0.014664, acc.: 100.00%] [G loss: 0.226049]\n",
      "epoch:29 step:23229 [D loss: 0.049839, acc.: 98.44%] [G loss: 0.177595]\n",
      "epoch:29 step:23230 [D loss: 0.064538, acc.: 99.22%] [G loss: 0.702575]\n",
      "epoch:29 step:23231 [D loss: 0.010169, acc.: 100.00%] [G loss: 0.256892]\n",
      "epoch:29 step:23232 [D loss: 0.127414, acc.: 93.75%] [G loss: 0.089398]\n",
      "epoch:29 step:23233 [D loss: 0.008767, acc.: 100.00%] [G loss: 0.116319]\n",
      "epoch:29 step:23234 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.015149]\n",
      "epoch:29 step:23235 [D loss: 0.015694, acc.: 99.22%] [G loss: 0.012011]\n",
      "epoch:29 step:23236 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.002067]\n",
      "epoch:29 step:23237 [D loss: 0.000716, acc.: 100.00%] [G loss: 0.005869]\n",
      "epoch:29 step:23238 [D loss: 0.014717, acc.: 100.00%] [G loss: 0.215975]\n",
      "epoch:29 step:23239 [D loss: 0.068641, acc.: 98.44%] [G loss: 1.834965]\n",
      "epoch:29 step:23240 [D loss: 0.006668, acc.: 100.00%] [G loss: 2.415750]\n",
      "epoch:29 step:23241 [D loss: 0.033479, acc.: 99.22%] [G loss: 3.634768]\n",
      "epoch:29 step:23242 [D loss: 0.279825, acc.: 85.94%] [G loss: 8.743931]\n",
      "epoch:29 step:23243 [D loss: 0.094905, acc.: 97.66%] [G loss: 6.403413]\n",
      "epoch:29 step:23244 [D loss: 0.465449, acc.: 78.91%] [G loss: 9.259664]\n",
      "epoch:29 step:23245 [D loss: 0.242362, acc.: 88.28%] [G loss: 8.371227]\n",
      "epoch:29 step:23246 [D loss: 0.033696, acc.: 99.22%] [G loss: 1.484899]\n",
      "epoch:29 step:23247 [D loss: 0.004182, acc.: 100.00%] [G loss: 6.844562]\n",
      "epoch:29 step:23248 [D loss: 0.019316, acc.: 99.22%] [G loss: 5.076525]\n",
      "epoch:29 step:23249 [D loss: 0.056062, acc.: 98.44%] [G loss: 4.649958]\n",
      "epoch:29 step:23250 [D loss: 0.086953, acc.: 97.66%] [G loss: 0.642311]\n",
      "epoch:29 step:23251 [D loss: 0.079734, acc.: 96.09%] [G loss: 6.262339]\n",
      "epoch:29 step:23252 [D loss: 0.075109, acc.: 98.44%] [G loss: 3.687768]\n",
      "epoch:29 step:23253 [D loss: 0.165545, acc.: 93.75%] [G loss: 1.059048]\n",
      "epoch:29 step:23254 [D loss: 0.007912, acc.: 100.00%] [G loss: 0.000612]\n",
      "epoch:29 step:23255 [D loss: 0.014485, acc.: 100.00%] [G loss: 1.014206]\n",
      "epoch:29 step:23256 [D loss: 0.009444, acc.: 99.22%] [G loss: 0.001026]\n",
      "epoch:29 step:23257 [D loss: 0.010642, acc.: 100.00%] [G loss: 0.251727]\n",
      "epoch:29 step:23258 [D loss: 0.011489, acc.: 99.22%] [G loss: 0.000098]\n",
      "epoch:29 step:23259 [D loss: 0.016401, acc.: 100.00%] [G loss: 0.029554]\n",
      "epoch:29 step:23260 [D loss: 0.002395, acc.: 100.00%] [G loss: 0.112932]\n",
      "epoch:29 step:23261 [D loss: 0.001156, acc.: 100.00%] [G loss: 0.016974]\n",
      "epoch:29 step:23262 [D loss: 0.057578, acc.: 97.66%] [G loss: 0.005887]\n",
      "epoch:29 step:23263 [D loss: 0.007272, acc.: 100.00%] [G loss: 0.001121]\n",
      "epoch:29 step:23264 [D loss: 0.004045, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:29 step:23265 [D loss: 0.004714, acc.: 100.00%] [G loss: 0.001551]\n",
      "epoch:29 step:23266 [D loss: 0.000912, acc.: 100.00%] [G loss: 0.000391]\n",
      "epoch:29 step:23267 [D loss: 0.001745, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:29 step:23268 [D loss: 0.000899, acc.: 100.00%] [G loss: 0.001744]\n",
      "epoch:29 step:23269 [D loss: 0.002764, acc.: 100.00%] [G loss: 2.794395]\n",
      "epoch:29 step:23270 [D loss: 0.154950, acc.: 95.31%] [G loss: 2.697646]\n",
      "epoch:29 step:23271 [D loss: 0.024682, acc.: 99.22%] [G loss: 6.227352]\n",
      "epoch:29 step:23272 [D loss: 0.394777, acc.: 85.16%] [G loss: 0.589047]\n",
      "epoch:29 step:23273 [D loss: 0.037590, acc.: 99.22%] [G loss: 4.052208]\n",
      "epoch:29 step:23274 [D loss: 0.088011, acc.: 97.66%] [G loss: 4.442949]\n",
      "epoch:29 step:23275 [D loss: 0.044354, acc.: 99.22%] [G loss: 0.068027]\n",
      "epoch:29 step:23276 [D loss: 0.015581, acc.: 99.22%] [G loss: 0.057673]\n",
      "epoch:29 step:23277 [D loss: 0.002375, acc.: 100.00%] [G loss: 0.973621]\n",
      "epoch:29 step:23278 [D loss: 0.004175, acc.: 100.00%] [G loss: 0.006673]\n",
      "epoch:29 step:23279 [D loss: 0.000899, acc.: 100.00%] [G loss: 0.006582]\n",
      "epoch:29 step:23280 [D loss: 0.003525, acc.: 100.00%] [G loss: 0.026528]\n",
      "epoch:29 step:23281 [D loss: 0.003821, acc.: 100.00%] [G loss: 0.009619]\n",
      "epoch:29 step:23282 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.027331]\n",
      "epoch:29 step:23283 [D loss: 0.013031, acc.: 99.22%] [G loss: 0.005356]\n",
      "epoch:29 step:23284 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.239118]\n",
      "epoch:29 step:23285 [D loss: 0.001709, acc.: 100.00%] [G loss: 0.007496]\n",
      "epoch:29 step:23286 [D loss: 0.008730, acc.: 100.00%] [G loss: 0.011594]\n",
      "epoch:29 step:23287 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.030738]\n",
      "epoch:29 step:23288 [D loss: 0.004676, acc.: 100.00%] [G loss: 0.009419]\n",
      "epoch:29 step:23289 [D loss: 0.001302, acc.: 100.00%] [G loss: 1.222760]\n",
      "epoch:29 step:23290 [D loss: 0.021624, acc.: 99.22%] [G loss: 0.005064]\n",
      "epoch:29 step:23291 [D loss: 0.063397, acc.: 99.22%] [G loss: 0.193363]\n",
      "epoch:29 step:23292 [D loss: 0.002646, acc.: 100.00%] [G loss: 0.370969]\n",
      "epoch:29 step:23293 [D loss: 0.036816, acc.: 99.22%] [G loss: 0.010300]\n",
      "epoch:29 step:23294 [D loss: 0.002811, acc.: 100.00%] [G loss: 4.643848]\n",
      "epoch:29 step:23295 [D loss: 1.550200, acc.: 50.78%] [G loss: 10.345523]\n",
      "epoch:29 step:23296 [D loss: 2.641958, acc.: 50.00%] [G loss: 5.218699]\n",
      "epoch:29 step:23297 [D loss: 0.521479, acc.: 78.91%] [G loss: 0.015924]\n",
      "epoch:29 step:23298 [D loss: 0.011661, acc.: 100.00%] [G loss: 0.007311]\n",
      "epoch:29 step:23299 [D loss: 0.023741, acc.: 100.00%] [G loss: 2.816585]\n",
      "epoch:29 step:23300 [D loss: 0.062599, acc.: 99.22%] [G loss: 0.027909]\n",
      "epoch:29 step:23301 [D loss: 0.002493, acc.: 100.00%] [G loss: 0.731218]\n",
      "epoch:29 step:23302 [D loss: 0.008840, acc.: 100.00%] [G loss: 0.134248]\n",
      "epoch:29 step:23303 [D loss: 0.014413, acc.: 100.00%] [G loss: 0.088080]\n",
      "epoch:29 step:23304 [D loss: 0.038504, acc.: 99.22%] [G loss: 0.152825]\n",
      "epoch:29 step:23305 [D loss: 0.009782, acc.: 100.00%] [G loss: 1.152193]\n",
      "epoch:29 step:23306 [D loss: 0.002066, acc.: 100.00%] [G loss: 2.160289]\n",
      "epoch:29 step:23307 [D loss: 0.029517, acc.: 99.22%] [G loss: 0.109993]\n",
      "epoch:29 step:23308 [D loss: 0.048467, acc.: 99.22%] [G loss: 0.343796]\n",
      "epoch:29 step:23309 [D loss: 0.005062, acc.: 100.00%] [G loss: 0.695245]\n",
      "epoch:29 step:23310 [D loss: 0.097390, acc.: 96.88%] [G loss: 0.012592]\n",
      "epoch:29 step:23311 [D loss: 0.007637, acc.: 100.00%] [G loss: 0.088011]\n",
      "epoch:29 step:23312 [D loss: 0.040137, acc.: 99.22%] [G loss: 0.028167]\n",
      "epoch:29 step:23313 [D loss: 0.000687, acc.: 100.00%] [G loss: 0.030759]\n",
      "epoch:29 step:23314 [D loss: 0.198592, acc.: 89.84%] [G loss: 0.014582]\n",
      "epoch:29 step:23315 [D loss: 0.001836, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:29 step:23316 [D loss: 0.001659, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:29 step:23317 [D loss: 0.010866, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:29 step:23318 [D loss: 0.007024, acc.: 100.00%] [G loss: 0.000685]\n",
      "epoch:29 step:23319 [D loss: 0.010011, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:29 step:23320 [D loss: 0.001189, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:29 step:23321 [D loss: 0.091104, acc.: 97.66%] [G loss: 0.984211]\n",
      "epoch:29 step:23322 [D loss: 0.002665, acc.: 100.00%] [G loss: 0.058929]\n",
      "epoch:29 step:23323 [D loss: 0.002577, acc.: 100.00%] [G loss: 0.496503]\n",
      "epoch:29 step:23324 [D loss: 0.009513, acc.: 99.22%] [G loss: 0.027774]\n",
      "epoch:29 step:23325 [D loss: 0.009763, acc.: 100.00%] [G loss: 2.233370]\n",
      "epoch:29 step:23326 [D loss: 0.008389, acc.: 100.00%] [G loss: 0.001004]\n",
      "epoch:29 step:23327 [D loss: 0.000766, acc.: 100.00%] [G loss: 1.775197]\n",
      "epoch:29 step:23328 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.001125]\n",
      "epoch:29 step:23329 [D loss: 0.000976, acc.: 100.00%] [G loss: 0.002242]\n",
      "epoch:29 step:23330 [D loss: 0.009031, acc.: 100.00%] [G loss: 0.009420]\n",
      "epoch:29 step:23331 [D loss: 0.001095, acc.: 100.00%] [G loss: 0.002843]\n",
      "epoch:29 step:23332 [D loss: 0.001025, acc.: 100.00%] [G loss: 0.008923]\n",
      "epoch:29 step:23333 [D loss: 0.004185, acc.: 100.00%] [G loss: 0.006461]\n",
      "epoch:29 step:23334 [D loss: 0.005393, acc.: 100.00%] [G loss: 0.025035]\n",
      "epoch:29 step:23335 [D loss: 0.038764, acc.: 100.00%] [G loss: 0.019571]\n",
      "epoch:29 step:23336 [D loss: 0.039625, acc.: 99.22%] [G loss: 0.001493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29 step:23337 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.101730]\n",
      "epoch:29 step:23338 [D loss: 0.003149, acc.: 100.00%] [G loss: 0.001036]\n",
      "epoch:29 step:23339 [D loss: 0.001347, acc.: 100.00%] [G loss: 0.161887]\n",
      "epoch:29 step:23340 [D loss: 0.002516, acc.: 100.00%] [G loss: 0.000384]\n",
      "epoch:29 step:23341 [D loss: 0.015573, acc.: 100.00%] [G loss: 0.009167]\n",
      "epoch:29 step:23342 [D loss: 0.001474, acc.: 100.00%] [G loss: 0.002103]\n",
      "epoch:29 step:23343 [D loss: 0.002131, acc.: 100.00%] [G loss: 0.001409]\n",
      "epoch:29 step:23344 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.004507]\n",
      "epoch:29 step:23345 [D loss: 0.024068, acc.: 100.00%] [G loss: 1.235823]\n",
      "epoch:29 step:23346 [D loss: 0.005351, acc.: 100.00%] [G loss: 0.001890]\n",
      "epoch:29 step:23347 [D loss: 0.011340, acc.: 100.00%] [G loss: 0.038745]\n",
      "epoch:29 step:23348 [D loss: 0.007794, acc.: 100.00%] [G loss: 0.012082]\n",
      "epoch:29 step:23349 [D loss: 0.002000, acc.: 100.00%] [G loss: 0.006476]\n",
      "epoch:29 step:23350 [D loss: 0.001214, acc.: 100.00%] [G loss: 0.158674]\n",
      "epoch:29 step:23351 [D loss: 0.016316, acc.: 100.00%] [G loss: 0.005639]\n",
      "epoch:29 step:23352 [D loss: 0.004655, acc.: 100.00%] [G loss: 0.357682]\n",
      "epoch:29 step:23353 [D loss: 0.074646, acc.: 97.66%] [G loss: 0.006901]\n",
      "epoch:29 step:23354 [D loss: 0.002119, acc.: 100.00%] [G loss: 0.011703]\n",
      "epoch:29 step:23355 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.046996]\n",
      "epoch:29 step:23356 [D loss: 0.010300, acc.: 100.00%] [G loss: 0.106518]\n",
      "epoch:29 step:23357 [D loss: 0.026244, acc.: 99.22%] [G loss: 0.018288]\n",
      "epoch:29 step:23358 [D loss: 0.050198, acc.: 100.00%] [G loss: 0.691591]\n",
      "epoch:29 step:23359 [D loss: 0.024398, acc.: 100.00%] [G loss: 1.183899]\n",
      "epoch:29 step:23360 [D loss: 0.095527, acc.: 98.44%] [G loss: 0.513441]\n",
      "epoch:29 step:23361 [D loss: 0.113251, acc.: 94.53%] [G loss: 2.565889]\n",
      "epoch:29 step:23362 [D loss: 1.334188, acc.: 39.84%] [G loss: 1.393091]\n",
      "epoch:29 step:23363 [D loss: 0.063819, acc.: 96.88%] [G loss: 1.362830]\n",
      "epoch:29 step:23364 [D loss: 1.216915, acc.: 60.16%] [G loss: 0.001140]\n",
      "epoch:29 step:23365 [D loss: 0.329297, acc.: 88.28%] [G loss: 0.002090]\n",
      "epoch:29 step:23366 [D loss: 0.100931, acc.: 96.09%] [G loss: 0.001878]\n",
      "epoch:29 step:23367 [D loss: 0.033212, acc.: 98.44%] [G loss: 0.007745]\n",
      "epoch:29 step:23368 [D loss: 0.017933, acc.: 100.00%] [G loss: 0.006686]\n",
      "epoch:29 step:23369 [D loss: 0.041291, acc.: 98.44%] [G loss: 0.000064]\n",
      "epoch:29 step:23370 [D loss: 0.003567, acc.: 100.00%] [G loss: 0.002115]\n",
      "epoch:29 step:23371 [D loss: 0.002821, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:29 step:23372 [D loss: 0.010874, acc.: 100.00%] [G loss: 0.000618]\n",
      "epoch:29 step:23373 [D loss: 0.004350, acc.: 100.00%] [G loss: 5.187117]\n",
      "epoch:29 step:23374 [D loss: 0.015963, acc.: 99.22%] [G loss: 3.704842]\n",
      "epoch:29 step:23375 [D loss: 0.004324, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:29 step:23376 [D loss: 0.001524, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:29 step:23377 [D loss: 0.004456, acc.: 100.00%] [G loss: 0.000299]\n",
      "epoch:29 step:23378 [D loss: 0.009736, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:29 step:23379 [D loss: 0.020224, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:29 step:23380 [D loss: 0.003780, acc.: 100.00%] [G loss: 3.145487]\n",
      "epoch:29 step:23381 [D loss: 0.010501, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:29 step:23382 [D loss: 0.019704, acc.: 100.00%] [G loss: 0.000323]\n",
      "epoch:29 step:23383 [D loss: 0.019288, acc.: 99.22%] [G loss: 0.000205]\n",
      "epoch:29 step:23384 [D loss: 0.013393, acc.: 99.22%] [G loss: 0.000223]\n",
      "epoch:29 step:23385 [D loss: 0.006376, acc.: 100.00%] [G loss: 1.033505]\n",
      "epoch:29 step:23386 [D loss: 0.005774, acc.: 100.00%] [G loss: 0.000571]\n",
      "epoch:29 step:23387 [D loss: 0.017320, acc.: 100.00%] [G loss: 1.178957]\n",
      "epoch:29 step:23388 [D loss: 0.094298, acc.: 98.44%] [G loss: 1.856862]\n",
      "epoch:29 step:23389 [D loss: 0.064651, acc.: 98.44%] [G loss: 1.556462]\n",
      "epoch:29 step:23390 [D loss: 0.383559, acc.: 79.69%] [G loss: 2.670709]\n",
      "epoch:29 step:23391 [D loss: 0.858743, acc.: 65.62%] [G loss: 0.242648]\n",
      "epoch:29 step:23392 [D loss: 0.043507, acc.: 99.22%] [G loss: 0.114402]\n",
      "epoch:29 step:23393 [D loss: 0.010911, acc.: 100.00%] [G loss: 0.042511]\n",
      "epoch:29 step:23394 [D loss: 0.011554, acc.: 100.00%] [G loss: 0.046243]\n",
      "epoch:29 step:23395 [D loss: 0.006090, acc.: 100.00%] [G loss: 0.012846]\n",
      "epoch:29 step:23396 [D loss: 0.006221, acc.: 100.00%] [G loss: 0.133795]\n",
      "epoch:29 step:23397 [D loss: 0.015230, acc.: 100.00%] [G loss: 2.800226]\n",
      "epoch:29 step:23398 [D loss: 0.005954, acc.: 100.00%] [G loss: 0.053667]\n",
      "epoch:29 step:23399 [D loss: 0.004814, acc.: 100.00%] [G loss: 0.011966]\n",
      "epoch:29 step:23400 [D loss: 0.012305, acc.: 100.00%] [G loss: 0.014826]\n",
      "epoch:29 step:23401 [D loss: 0.002087, acc.: 100.00%] [G loss: 0.003517]\n",
      "epoch:29 step:23402 [D loss: 0.006265, acc.: 100.00%] [G loss: 0.022512]\n",
      "epoch:29 step:23403 [D loss: 0.014769, acc.: 100.00%] [G loss: 0.009558]\n",
      "epoch:29 step:23404 [D loss: 0.004214, acc.: 100.00%] [G loss: 0.032399]\n",
      "epoch:29 step:23405 [D loss: 0.015046, acc.: 100.00%] [G loss: 0.017948]\n",
      "epoch:29 step:23406 [D loss: 0.017096, acc.: 100.00%] [G loss: 0.005784]\n",
      "epoch:29 step:23407 [D loss: 0.070272, acc.: 99.22%] [G loss: 4.058949]\n",
      "epoch:29 step:23408 [D loss: 0.008144, acc.: 100.00%] [G loss: 1.256418]\n",
      "epoch:29 step:23409 [D loss: 0.012318, acc.: 99.22%] [G loss: 0.086488]\n",
      "epoch:29 step:23410 [D loss: 0.003898, acc.: 100.00%] [G loss: 0.044723]\n",
      "epoch:29 step:23411 [D loss: 0.034304, acc.: 99.22%] [G loss: 0.051585]\n",
      "epoch:29 step:23412 [D loss: 0.024911, acc.: 100.00%] [G loss: 0.278486]\n",
      "epoch:29 step:23413 [D loss: 0.003250, acc.: 100.00%] [G loss: 0.967292]\n",
      "epoch:29 step:23414 [D loss: 0.022615, acc.: 100.00%] [G loss: 0.022507]\n",
      "epoch:29 step:23415 [D loss: 0.057667, acc.: 100.00%] [G loss: 0.025178]\n",
      "epoch:29 step:23416 [D loss: 0.020376, acc.: 100.00%] [G loss: 0.073164]\n",
      "epoch:29 step:23417 [D loss: 0.012989, acc.: 100.00%] [G loss: 0.315781]\n",
      "epoch:29 step:23418 [D loss: 0.036835, acc.: 100.00%] [G loss: 0.349201]\n",
      "epoch:29 step:23419 [D loss: 0.123864, acc.: 98.44%] [G loss: 3.495145]\n",
      "epoch:29 step:23420 [D loss: 0.448342, acc.: 75.00%] [G loss: 0.058882]\n",
      "epoch:29 step:23421 [D loss: 0.299034, acc.: 82.03%] [G loss: 1.127158]\n",
      "epoch:29 step:23422 [D loss: 0.005582, acc.: 100.00%] [G loss: 5.350836]\n",
      "epoch:29 step:23423 [D loss: 0.079222, acc.: 97.66%] [G loss: 4.900188]\n",
      "epoch:29 step:23424 [D loss: 0.063739, acc.: 98.44%] [G loss: 1.958883]\n",
      "epoch:29 step:23425 [D loss: 0.013458, acc.: 100.00%] [G loss: 1.776750]\n",
      "epoch:29 step:23426 [D loss: 0.011862, acc.: 100.00%] [G loss: 0.185342]\n",
      "epoch:29 step:23427 [D loss: 0.074354, acc.: 97.66%] [G loss: 0.505799]\n",
      "epoch:29 step:23428 [D loss: 0.013997, acc.: 100.00%] [G loss: 4.708670]\n",
      "epoch:29 step:23429 [D loss: 0.023903, acc.: 100.00%] [G loss: 3.394955]\n",
      "epoch:29 step:23430 [D loss: 0.428998, acc.: 82.81%] [G loss: 3.389202]\n",
      "epoch:30 step:23431 [D loss: 0.153738, acc.: 93.75%] [G loss: 3.547622]\n",
      "epoch:30 step:23432 [D loss: 0.104185, acc.: 96.09%] [G loss: 1.305004]\n",
      "epoch:30 step:23433 [D loss: 0.123118, acc.: 94.53%] [G loss: 4.492730]\n",
      "epoch:30 step:23434 [D loss: 0.013238, acc.: 100.00%] [G loss: 2.922422]\n",
      "epoch:30 step:23435 [D loss: 0.016393, acc.: 99.22%] [G loss: 0.981957]\n",
      "epoch:30 step:23436 [D loss: 0.047747, acc.: 100.00%] [G loss: 0.785135]\n",
      "epoch:30 step:23437 [D loss: 0.022763, acc.: 100.00%] [G loss: 3.508052]\n",
      "epoch:30 step:23438 [D loss: 0.083950, acc.: 97.66%] [G loss: 2.528407]\n",
      "epoch:30 step:23439 [D loss: 0.070198, acc.: 99.22%] [G loss: 1.405756]\n",
      "epoch:30 step:23440 [D loss: 0.313628, acc.: 89.84%] [G loss: 0.371830]\n",
      "epoch:30 step:23441 [D loss: 0.135948, acc.: 94.53%] [G loss: 3.037636]\n",
      "epoch:30 step:23442 [D loss: 0.057449, acc.: 98.44%] [G loss: 3.492291]\n",
      "epoch:30 step:23443 [D loss: 0.073021, acc.: 98.44%] [G loss: 4.829082]\n",
      "epoch:30 step:23444 [D loss: 0.303394, acc.: 91.41%] [G loss: 0.237049]\n",
      "epoch:30 step:23445 [D loss: 0.007406, acc.: 100.00%] [G loss: 1.976556]\n",
      "epoch:30 step:23446 [D loss: 0.105217, acc.: 96.88%] [G loss: 0.211140]\n",
      "epoch:30 step:23447 [D loss: 0.003348, acc.: 100.00%] [G loss: 0.370515]\n",
      "epoch:30 step:23448 [D loss: 0.032790, acc.: 99.22%] [G loss: 0.578113]\n",
      "epoch:30 step:23449 [D loss: 0.060802, acc.: 98.44%] [G loss: 1.499589]\n",
      "epoch:30 step:23450 [D loss: 0.007593, acc.: 100.00%] [G loss: 0.590347]\n",
      "epoch:30 step:23451 [D loss: 0.033041, acc.: 98.44%] [G loss: 0.032048]\n",
      "epoch:30 step:23452 [D loss: 0.003622, acc.: 100.00%] [G loss: 0.035074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23453 [D loss: 0.007590, acc.: 100.00%] [G loss: 0.006831]\n",
      "epoch:30 step:23454 [D loss: 0.004231, acc.: 100.00%] [G loss: 2.780550]\n",
      "epoch:30 step:23455 [D loss: 0.024005, acc.: 100.00%] [G loss: 0.200721]\n",
      "epoch:30 step:23456 [D loss: 0.003875, acc.: 100.00%] [G loss: 0.054776]\n",
      "epoch:30 step:23457 [D loss: 0.011231, acc.: 100.00%] [G loss: 0.069234]\n",
      "epoch:30 step:23458 [D loss: 0.024597, acc.: 100.00%] [G loss: 0.020104]\n",
      "epoch:30 step:23459 [D loss: 0.466513, acc.: 72.66%] [G loss: 5.708292]\n",
      "epoch:30 step:23460 [D loss: 0.533016, acc.: 78.91%] [G loss: 3.992980]\n",
      "epoch:30 step:23461 [D loss: 0.271611, acc.: 89.06%] [G loss: 0.881138]\n",
      "epoch:30 step:23462 [D loss: 0.132265, acc.: 90.62%] [G loss: 2.456348]\n",
      "epoch:30 step:23463 [D loss: 0.016327, acc.: 100.00%] [G loss: 3.331290]\n",
      "epoch:30 step:23464 [D loss: 0.033681, acc.: 100.00%] [G loss: 3.991369]\n",
      "epoch:30 step:23465 [D loss: 0.253172, acc.: 89.06%] [G loss: 2.893112]\n",
      "epoch:30 step:23466 [D loss: 0.051491, acc.: 97.66%] [G loss: 3.786886]\n",
      "epoch:30 step:23467 [D loss: 0.008089, acc.: 100.00%] [G loss: 3.160074]\n",
      "epoch:30 step:23468 [D loss: 0.023811, acc.: 99.22%] [G loss: 1.618640]\n",
      "epoch:30 step:23469 [D loss: 0.015009, acc.: 100.00%] [G loss: 1.266426]\n",
      "epoch:30 step:23470 [D loss: 0.040631, acc.: 99.22%] [G loss: 1.036481]\n",
      "epoch:30 step:23471 [D loss: 0.055456, acc.: 98.44%] [G loss: 3.436873]\n",
      "epoch:30 step:23472 [D loss: 0.040504, acc.: 99.22%] [G loss: 6.227234]\n",
      "epoch:30 step:23473 [D loss: 0.249802, acc.: 87.50%] [G loss: 3.889347]\n",
      "epoch:30 step:23474 [D loss: 0.045216, acc.: 99.22%] [G loss: 2.869006]\n",
      "epoch:30 step:23475 [D loss: 0.082994, acc.: 98.44%] [G loss: 3.907531]\n",
      "epoch:30 step:23476 [D loss: 0.038153, acc.: 99.22%] [G loss: 4.610415]\n",
      "epoch:30 step:23477 [D loss: 0.026892, acc.: 100.00%] [G loss: 3.293538]\n",
      "epoch:30 step:23478 [D loss: 0.095713, acc.: 99.22%] [G loss: 2.343427]\n",
      "epoch:30 step:23479 [D loss: 0.073942, acc.: 96.88%] [G loss: 3.933583]\n",
      "epoch:30 step:23480 [D loss: 0.040577, acc.: 99.22%] [G loss: 2.998240]\n",
      "epoch:30 step:23481 [D loss: 0.012525, acc.: 100.00%] [G loss: 1.952373]\n",
      "epoch:30 step:23482 [D loss: 0.193732, acc.: 91.41%] [G loss: 5.249621]\n",
      "epoch:30 step:23483 [D loss: 0.130357, acc.: 93.75%] [G loss: 5.606742]\n",
      "epoch:30 step:23484 [D loss: 0.209716, acc.: 91.41%] [G loss: 5.122862]\n",
      "epoch:30 step:23485 [D loss: 0.094560, acc.: 96.88%] [G loss: 2.522115]\n",
      "epoch:30 step:23486 [D loss: 0.029584, acc.: 100.00%] [G loss: 5.507764]\n",
      "epoch:30 step:23487 [D loss: 0.021383, acc.: 100.00%] [G loss: 4.144322]\n",
      "epoch:30 step:23488 [D loss: 0.010425, acc.: 100.00%] [G loss: 4.108392]\n",
      "epoch:30 step:23489 [D loss: 0.031755, acc.: 99.22%] [G loss: 1.893520]\n",
      "epoch:30 step:23490 [D loss: 0.005185, acc.: 100.00%] [G loss: 1.073970]\n",
      "epoch:30 step:23491 [D loss: 0.061339, acc.: 97.66%] [G loss: 1.680266]\n",
      "epoch:30 step:23492 [D loss: 0.012521, acc.: 100.00%] [G loss: 0.257337]\n",
      "epoch:30 step:23493 [D loss: 0.008024, acc.: 100.00%] [G loss: 0.785595]\n",
      "epoch:30 step:23494 [D loss: 0.006337, acc.: 100.00%] [G loss: 0.645739]\n",
      "epoch:30 step:23495 [D loss: 0.117770, acc.: 95.31%] [G loss: 0.288140]\n",
      "epoch:30 step:23496 [D loss: 0.004075, acc.: 100.00%] [G loss: 0.539804]\n",
      "epoch:30 step:23497 [D loss: 0.002464, acc.: 100.00%] [G loss: 0.448076]\n",
      "epoch:30 step:23498 [D loss: 0.199142, acc.: 93.75%] [G loss: 1.840653]\n",
      "epoch:30 step:23499 [D loss: 0.026866, acc.: 100.00%] [G loss: 2.704346]\n",
      "epoch:30 step:23500 [D loss: 0.004425, acc.: 100.00%] [G loss: 2.843328]\n",
      "epoch:30 step:23501 [D loss: 0.087008, acc.: 97.66%] [G loss: 1.086616]\n",
      "epoch:30 step:23502 [D loss: 0.115917, acc.: 96.88%] [G loss: 5.443805]\n",
      "epoch:30 step:23503 [D loss: 0.088389, acc.: 96.88%] [G loss: 4.631508]\n",
      "epoch:30 step:23504 [D loss: 0.055322, acc.: 98.44%] [G loss: 5.206272]\n",
      "epoch:30 step:23505 [D loss: 0.024247, acc.: 99.22%] [G loss: 5.327404]\n",
      "epoch:30 step:23506 [D loss: 0.093290, acc.: 96.09%] [G loss: 5.152916]\n",
      "epoch:30 step:23507 [D loss: 1.767381, acc.: 46.09%] [G loss: 10.115817]\n",
      "epoch:30 step:23508 [D loss: 0.834252, acc.: 66.41%] [G loss: 8.936445]\n",
      "epoch:30 step:23509 [D loss: 0.277611, acc.: 86.72%] [G loss: 6.797056]\n",
      "epoch:30 step:23510 [D loss: 0.033153, acc.: 99.22%] [G loss: 0.007317]\n",
      "epoch:30 step:23511 [D loss: 0.022047, acc.: 99.22%] [G loss: 5.357338]\n",
      "epoch:30 step:23512 [D loss: 0.033475, acc.: 99.22%] [G loss: 4.882997]\n",
      "epoch:30 step:23513 [D loss: 0.053293, acc.: 100.00%] [G loss: 4.676656]\n",
      "epoch:30 step:23514 [D loss: 0.016404, acc.: 99.22%] [G loss: 4.578777]\n",
      "epoch:30 step:23515 [D loss: 0.025360, acc.: 100.00%] [G loss: 3.769016]\n",
      "epoch:30 step:23516 [D loss: 0.035284, acc.: 100.00%] [G loss: 0.350997]\n",
      "epoch:30 step:23517 [D loss: 0.010237, acc.: 100.00%] [G loss: 4.139990]\n",
      "epoch:30 step:23518 [D loss: 0.035860, acc.: 100.00%] [G loss: 2.815445]\n",
      "epoch:30 step:23519 [D loss: 0.015431, acc.: 100.00%] [G loss: 3.473126]\n",
      "epoch:30 step:23520 [D loss: 0.117699, acc.: 96.88%] [G loss: 0.505268]\n",
      "epoch:30 step:23521 [D loss: 0.026762, acc.: 100.00%] [G loss: 2.357773]\n",
      "epoch:30 step:23522 [D loss: 0.085009, acc.: 99.22%] [G loss: 2.899423]\n",
      "epoch:30 step:23523 [D loss: 0.045033, acc.: 99.22%] [G loss: 3.074522]\n",
      "epoch:30 step:23524 [D loss: 0.057829, acc.: 99.22%] [G loss: 3.334860]\n",
      "epoch:30 step:23525 [D loss: 0.038650, acc.: 99.22%] [G loss: 1.671254]\n",
      "epoch:30 step:23526 [D loss: 0.075302, acc.: 98.44%] [G loss: 1.553445]\n",
      "epoch:30 step:23527 [D loss: 0.192841, acc.: 95.31%] [G loss: 4.143234]\n",
      "epoch:30 step:23528 [D loss: 0.205155, acc.: 89.84%] [G loss: 4.489625]\n",
      "epoch:30 step:23529 [D loss: 0.513652, acc.: 80.47%] [G loss: 6.585621]\n",
      "epoch:30 step:23530 [D loss: 0.054629, acc.: 98.44%] [G loss: 7.273734]\n",
      "epoch:30 step:23531 [D loss: 0.081428, acc.: 96.88%] [G loss: 6.348247]\n",
      "epoch:30 step:23532 [D loss: 0.067394, acc.: 97.66%] [G loss: 6.259912]\n",
      "epoch:30 step:23533 [D loss: 0.016863, acc.: 100.00%] [G loss: 6.340762]\n",
      "epoch:30 step:23534 [D loss: 0.049146, acc.: 99.22%] [G loss: 6.241643]\n",
      "epoch:30 step:23535 [D loss: 0.011291, acc.: 100.00%] [G loss: 5.557576]\n",
      "epoch:30 step:23536 [D loss: 0.011476, acc.: 100.00%] [G loss: 6.173310]\n",
      "epoch:30 step:23537 [D loss: 0.037975, acc.: 100.00%] [G loss: 0.692291]\n",
      "epoch:30 step:23538 [D loss: 0.011096, acc.: 100.00%] [G loss: 6.145625]\n",
      "epoch:30 step:23539 [D loss: 0.040758, acc.: 99.22%] [G loss: 4.955270]\n",
      "epoch:30 step:23540 [D loss: 0.022050, acc.: 100.00%] [G loss: 5.039594]\n",
      "epoch:30 step:23541 [D loss: 0.319178, acc.: 83.59%] [G loss: 5.361189]\n",
      "epoch:30 step:23542 [D loss: 0.008740, acc.: 100.00%] [G loss: 7.014125]\n",
      "epoch:30 step:23543 [D loss: 0.073204, acc.: 95.31%] [G loss: 6.811620]\n",
      "epoch:30 step:23544 [D loss: 0.011409, acc.: 100.00%] [G loss: 5.183964]\n",
      "epoch:30 step:23545 [D loss: 0.011795, acc.: 100.00%] [G loss: 5.036105]\n",
      "epoch:30 step:23546 [D loss: 0.107848, acc.: 96.09%] [G loss: 5.549360]\n",
      "epoch:30 step:23547 [D loss: 0.010197, acc.: 100.00%] [G loss: 5.908608]\n",
      "epoch:30 step:23548 [D loss: 0.013684, acc.: 100.00%] [G loss: 5.385042]\n",
      "epoch:30 step:23549 [D loss: 0.028509, acc.: 100.00%] [G loss: 5.838182]\n",
      "epoch:30 step:23550 [D loss: 0.051797, acc.: 99.22%] [G loss: 6.180051]\n",
      "epoch:30 step:23551 [D loss: 0.093128, acc.: 96.09%] [G loss: 7.407434]\n",
      "epoch:30 step:23552 [D loss: 0.040302, acc.: 98.44%] [G loss: 6.484128]\n",
      "epoch:30 step:23553 [D loss: 0.065097, acc.: 97.66%] [G loss: 5.918727]\n",
      "epoch:30 step:23554 [D loss: 0.012804, acc.: 100.00%] [G loss: 5.170249]\n",
      "epoch:30 step:23555 [D loss: 0.039521, acc.: 100.00%] [G loss: 6.262113]\n",
      "epoch:30 step:23556 [D loss: 0.010786, acc.: 100.00%] [G loss: 5.738409]\n",
      "epoch:30 step:23557 [D loss: 0.016510, acc.: 100.00%] [G loss: 5.338974]\n",
      "epoch:30 step:23558 [D loss: 0.101350, acc.: 97.66%] [G loss: 3.648925]\n",
      "epoch:30 step:23559 [D loss: 0.022019, acc.: 98.44%] [G loss: 0.889925]\n",
      "epoch:30 step:23560 [D loss: 0.052460, acc.: 98.44%] [G loss: 3.579371]\n",
      "epoch:30 step:23561 [D loss: 0.005086, acc.: 100.00%] [G loss: 7.387696]\n",
      "epoch:30 step:23562 [D loss: 0.408918, acc.: 81.25%] [G loss: 6.412977]\n",
      "epoch:30 step:23563 [D loss: 0.008932, acc.: 99.22%] [G loss: 7.607387]\n",
      "epoch:30 step:23564 [D loss: 0.187240, acc.: 91.41%] [G loss: 3.359169]\n",
      "epoch:30 step:23565 [D loss: 0.109434, acc.: 96.88%] [G loss: 4.801333]\n",
      "epoch:30 step:23566 [D loss: 0.035641, acc.: 98.44%] [G loss: 5.152455]\n",
      "epoch:30 step:23567 [D loss: 0.010825, acc.: 100.00%] [G loss: 2.940508]\n",
      "epoch:30 step:23568 [D loss: 0.039402, acc.: 97.66%] [G loss: 0.978274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23569 [D loss: 0.059196, acc.: 100.00%] [G loss: 1.668224]\n",
      "epoch:30 step:23570 [D loss: 0.077041, acc.: 98.44%] [G loss: 1.390970]\n",
      "epoch:30 step:23571 [D loss: 0.219066, acc.: 92.19%] [G loss: 6.413346]\n",
      "epoch:30 step:23572 [D loss: 0.543715, acc.: 74.22%] [G loss: 1.204983]\n",
      "epoch:30 step:23573 [D loss: 0.051893, acc.: 98.44%] [G loss: 2.291935]\n",
      "epoch:30 step:23574 [D loss: 0.006039, acc.: 100.00%] [G loss: 1.906792]\n",
      "epoch:30 step:23575 [D loss: 0.007574, acc.: 100.00%] [G loss: 0.442252]\n",
      "epoch:30 step:23576 [D loss: 0.112140, acc.: 95.31%] [G loss: 2.707623]\n",
      "epoch:30 step:23577 [D loss: 0.010651, acc.: 100.00%] [G loss: 2.732484]\n",
      "epoch:30 step:23578 [D loss: 0.203525, acc.: 91.41%] [G loss: 0.410080]\n",
      "epoch:30 step:23579 [D loss: 0.294937, acc.: 88.28%] [G loss: 6.876050]\n",
      "epoch:30 step:23580 [D loss: 0.025385, acc.: 99.22%] [G loss: 5.485107]\n",
      "epoch:30 step:23581 [D loss: 0.513055, acc.: 80.47%] [G loss: 1.730024]\n",
      "epoch:30 step:23582 [D loss: 0.001791, acc.: 100.00%] [G loss: 0.086279]\n",
      "epoch:30 step:23583 [D loss: 0.003913, acc.: 100.00%] [G loss: 0.215956]\n",
      "epoch:30 step:23584 [D loss: 0.002681, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:30 step:23585 [D loss: 0.009740, acc.: 100.00%] [G loss: 0.030781]\n",
      "epoch:30 step:23586 [D loss: 0.022313, acc.: 99.22%] [G loss: 2.909793]\n",
      "epoch:30 step:23587 [D loss: 0.005695, acc.: 100.00%] [G loss: 0.484369]\n",
      "epoch:30 step:23588 [D loss: 0.188097, acc.: 90.62%] [G loss: 5.646792]\n",
      "epoch:30 step:23589 [D loss: 0.002251, acc.: 100.00%] [G loss: 4.940836]\n",
      "epoch:30 step:23590 [D loss: 0.608933, acc.: 71.88%] [G loss: 0.081414]\n",
      "epoch:30 step:23591 [D loss: 0.023559, acc.: 100.00%] [G loss: 0.478548]\n",
      "epoch:30 step:23592 [D loss: 0.062560, acc.: 99.22%] [G loss: 1.196269]\n",
      "epoch:30 step:23593 [D loss: 0.003604, acc.: 100.00%] [G loss: 1.200930]\n",
      "epoch:30 step:23594 [D loss: 0.008337, acc.: 100.00%] [G loss: 0.153603]\n",
      "epoch:30 step:23595 [D loss: 0.020057, acc.: 100.00%] [G loss: 0.550417]\n",
      "epoch:30 step:23596 [D loss: 0.059761, acc.: 99.22%] [G loss: 1.254013]\n",
      "epoch:30 step:23597 [D loss: 0.030936, acc.: 100.00%] [G loss: 1.615075]\n",
      "epoch:30 step:23598 [D loss: 0.014075, acc.: 100.00%] [G loss: 1.384313]\n",
      "epoch:30 step:23599 [D loss: 0.013655, acc.: 100.00%] [G loss: 1.423194]\n",
      "epoch:30 step:23600 [D loss: 1.167643, acc.: 50.00%] [G loss: 10.857363]\n",
      "epoch:30 step:23601 [D loss: 2.665294, acc.: 50.00%] [G loss: 7.750847]\n",
      "epoch:30 step:23602 [D loss: 0.455316, acc.: 77.34%] [G loss: 0.094664]\n",
      "epoch:30 step:23603 [D loss: 0.033588, acc.: 99.22%] [G loss: 2.526543]\n",
      "epoch:30 step:23604 [D loss: 0.020085, acc.: 100.00%] [G loss: 2.429775]\n",
      "epoch:30 step:23605 [D loss: 0.040202, acc.: 99.22%] [G loss: 1.143166]\n",
      "epoch:30 step:23606 [D loss: 0.082902, acc.: 98.44%] [G loss: 1.179928]\n",
      "epoch:30 step:23607 [D loss: 0.096493, acc.: 96.09%] [G loss: 0.051087]\n",
      "epoch:30 step:23608 [D loss: 0.121499, acc.: 94.53%] [G loss: 0.073692]\n",
      "epoch:30 step:23609 [D loss: 0.210138, acc.: 93.75%] [G loss: 0.664788]\n",
      "epoch:30 step:23610 [D loss: 0.100741, acc.: 96.88%] [G loss: 0.176896]\n",
      "epoch:30 step:23611 [D loss: 0.006628, acc.: 100.00%] [G loss: 0.394295]\n",
      "epoch:30 step:23612 [D loss: 0.018025, acc.: 99.22%] [G loss: 1.786737]\n",
      "epoch:30 step:23613 [D loss: 0.021867, acc.: 99.22%] [G loss: 0.149668]\n",
      "epoch:30 step:23614 [D loss: 0.044470, acc.: 100.00%] [G loss: 0.486087]\n",
      "epoch:30 step:23615 [D loss: 0.017922, acc.: 100.00%] [G loss: 0.095439]\n",
      "epoch:30 step:23616 [D loss: 0.021373, acc.: 100.00%] [G loss: 0.070248]\n",
      "epoch:30 step:23617 [D loss: 0.082488, acc.: 99.22%] [G loss: 0.062649]\n",
      "epoch:30 step:23618 [D loss: 0.045379, acc.: 99.22%] [G loss: 0.175675]\n",
      "epoch:30 step:23619 [D loss: 0.008242, acc.: 100.00%] [G loss: 0.091128]\n",
      "epoch:30 step:23620 [D loss: 0.021430, acc.: 100.00%] [G loss: 0.436501]\n",
      "epoch:30 step:23621 [D loss: 0.046095, acc.: 98.44%] [G loss: 0.006554]\n",
      "epoch:30 step:23622 [D loss: 0.016193, acc.: 100.00%] [G loss: 0.011130]\n",
      "epoch:30 step:23623 [D loss: 0.009459, acc.: 100.00%] [G loss: 0.056863]\n",
      "epoch:30 step:23624 [D loss: 0.022704, acc.: 100.00%] [G loss: 0.031717]\n",
      "epoch:30 step:23625 [D loss: 0.025447, acc.: 100.00%] [G loss: 0.030020]\n",
      "epoch:30 step:23626 [D loss: 0.021014, acc.: 100.00%] [G loss: 0.021811]\n",
      "epoch:30 step:23627 [D loss: 0.001427, acc.: 100.00%] [G loss: 0.074658]\n",
      "epoch:30 step:23628 [D loss: 0.016204, acc.: 100.00%] [G loss: 0.048292]\n",
      "epoch:30 step:23629 [D loss: 0.018117, acc.: 100.00%] [G loss: 0.027866]\n",
      "epoch:30 step:23630 [D loss: 0.010015, acc.: 100.00%] [G loss: 0.090765]\n",
      "epoch:30 step:23631 [D loss: 0.003546, acc.: 100.00%] [G loss: 0.012024]\n",
      "epoch:30 step:23632 [D loss: 0.003058, acc.: 100.00%] [G loss: 0.019668]\n",
      "epoch:30 step:23633 [D loss: 0.002806, acc.: 100.00%] [G loss: 1.185869]\n",
      "epoch:30 step:23634 [D loss: 0.005650, acc.: 100.00%] [G loss: 0.042169]\n",
      "epoch:30 step:23635 [D loss: 0.018339, acc.: 100.00%] [G loss: 0.489925]\n",
      "epoch:30 step:23636 [D loss: 0.021635, acc.: 100.00%] [G loss: 0.019799]\n",
      "epoch:30 step:23637 [D loss: 0.008125, acc.: 100.00%] [G loss: 0.058722]\n",
      "epoch:30 step:23638 [D loss: 0.007431, acc.: 100.00%] [G loss: 0.197825]\n",
      "epoch:30 step:23639 [D loss: 0.019712, acc.: 99.22%] [G loss: 0.438792]\n",
      "epoch:30 step:23640 [D loss: 0.043211, acc.: 99.22%] [G loss: 0.006231]\n",
      "epoch:30 step:23641 [D loss: 0.037506, acc.: 100.00%] [G loss: 0.051207]\n",
      "epoch:30 step:23642 [D loss: 0.004635, acc.: 100.00%] [G loss: 0.140781]\n",
      "epoch:30 step:23643 [D loss: 0.004378, acc.: 100.00%] [G loss: 0.103186]\n",
      "epoch:30 step:23644 [D loss: 0.132422, acc.: 94.53%] [G loss: 0.000621]\n",
      "epoch:30 step:23645 [D loss: 0.240754, acc.: 85.94%] [G loss: 2.106587]\n",
      "epoch:30 step:23646 [D loss: 0.098079, acc.: 95.31%] [G loss: 2.013850]\n",
      "epoch:30 step:23647 [D loss: 0.028182, acc.: 100.00%] [G loss: 2.682921]\n",
      "epoch:30 step:23648 [D loss: 0.048277, acc.: 98.44%] [G loss: 0.095023]\n",
      "epoch:30 step:23649 [D loss: 0.002882, acc.: 100.00%] [G loss: 0.406147]\n",
      "epoch:30 step:23650 [D loss: 0.038020, acc.: 99.22%] [G loss: 0.024940]\n",
      "epoch:30 step:23651 [D loss: 0.001652, acc.: 100.00%] [G loss: 0.258276]\n",
      "epoch:30 step:23652 [D loss: 0.007723, acc.: 100.00%] [G loss: 0.006093]\n",
      "epoch:30 step:23653 [D loss: 0.004524, acc.: 100.00%] [G loss: 0.015885]\n",
      "epoch:30 step:23654 [D loss: 0.005762, acc.: 100.00%] [G loss: 0.017930]\n",
      "epoch:30 step:23655 [D loss: 0.009338, acc.: 100.00%] [G loss: 0.033081]\n",
      "epoch:30 step:23656 [D loss: 0.010962, acc.: 100.00%] [G loss: 1.019344]\n",
      "epoch:30 step:23657 [D loss: 0.001382, acc.: 100.00%] [G loss: 0.008094]\n",
      "epoch:30 step:23658 [D loss: 0.021077, acc.: 100.00%] [G loss: 0.026379]\n",
      "epoch:30 step:23659 [D loss: 0.014174, acc.: 100.00%] [G loss: 0.050264]\n",
      "epoch:30 step:23660 [D loss: 0.010809, acc.: 99.22%] [G loss: 0.271240]\n",
      "epoch:30 step:23661 [D loss: 0.088352, acc.: 100.00%] [G loss: 0.011113]\n",
      "epoch:30 step:23662 [D loss: 0.008245, acc.: 100.00%] [G loss: 0.181888]\n",
      "epoch:30 step:23663 [D loss: 0.028451, acc.: 99.22%] [G loss: 0.102436]\n",
      "epoch:30 step:23664 [D loss: 0.031605, acc.: 99.22%] [G loss: 0.002279]\n",
      "epoch:30 step:23665 [D loss: 0.002684, acc.: 100.00%] [G loss: 0.149418]\n",
      "epoch:30 step:23666 [D loss: 0.009709, acc.: 100.00%] [G loss: 1.296785]\n",
      "epoch:30 step:23667 [D loss: 0.008347, acc.: 100.00%] [G loss: 0.000530]\n",
      "epoch:30 step:23668 [D loss: 0.001545, acc.: 100.00%] [G loss: 0.177779]\n",
      "epoch:30 step:23669 [D loss: 0.024322, acc.: 100.00%] [G loss: 0.006346]\n",
      "epoch:30 step:23670 [D loss: 0.022932, acc.: 100.00%] [G loss: 0.002401]\n",
      "epoch:30 step:23671 [D loss: 0.000950, acc.: 100.00%] [G loss: 0.065400]\n",
      "epoch:30 step:23672 [D loss: 0.002921, acc.: 100.00%] [G loss: 0.000567]\n",
      "epoch:30 step:23673 [D loss: 0.005964, acc.: 100.00%] [G loss: 0.003700]\n",
      "epoch:30 step:23674 [D loss: 0.002388, acc.: 100.00%] [G loss: 0.001215]\n",
      "epoch:30 step:23675 [D loss: 0.000812, acc.: 100.00%] [G loss: 0.076261]\n",
      "epoch:30 step:23676 [D loss: 0.005809, acc.: 100.00%] [G loss: 0.002132]\n",
      "epoch:30 step:23677 [D loss: 0.008547, acc.: 100.00%] [G loss: 0.001748]\n",
      "epoch:30 step:23678 [D loss: 0.007013, acc.: 100.00%] [G loss: 0.002398]\n",
      "epoch:30 step:23679 [D loss: 0.001844, acc.: 100.00%] [G loss: 0.033865]\n",
      "epoch:30 step:23680 [D loss: 0.035403, acc.: 99.22%] [G loss: 0.004170]\n",
      "epoch:30 step:23681 [D loss: 0.011440, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:30 step:23682 [D loss: 0.034280, acc.: 100.00%] [G loss: 0.001107]\n",
      "epoch:30 step:23683 [D loss: 0.002696, acc.: 100.00%] [G loss: 0.003397]\n",
      "epoch:30 step:23684 [D loss: 0.000792, acc.: 100.00%] [G loss: 0.042236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23685 [D loss: 0.001727, acc.: 100.00%] [G loss: 0.009514]\n",
      "epoch:30 step:23686 [D loss: 0.000657, acc.: 100.00%] [G loss: 0.025911]\n",
      "epoch:30 step:23687 [D loss: 0.004569, acc.: 100.00%] [G loss: 0.014478]\n",
      "epoch:30 step:23688 [D loss: 0.002038, acc.: 100.00%] [G loss: 0.001818]\n",
      "epoch:30 step:23689 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.001096]\n",
      "epoch:30 step:23690 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.005427]\n",
      "epoch:30 step:23691 [D loss: 0.024920, acc.: 99.22%] [G loss: 0.000294]\n",
      "epoch:30 step:23692 [D loss: 0.001020, acc.: 100.00%] [G loss: 0.000445]\n",
      "epoch:30 step:23693 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:30 step:23694 [D loss: 0.005797, acc.: 100.00%] [G loss: 0.045603]\n",
      "epoch:30 step:23695 [D loss: 0.003117, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:30 step:23696 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.001324]\n",
      "epoch:30 step:23697 [D loss: 0.001696, acc.: 100.00%] [G loss: 0.000375]\n",
      "epoch:30 step:23698 [D loss: 0.000421, acc.: 100.00%] [G loss: 3.552830]\n",
      "epoch:30 step:23699 [D loss: 0.005337, acc.: 100.00%] [G loss: 0.038190]\n",
      "epoch:30 step:23700 [D loss: 0.032150, acc.: 100.00%] [G loss: 0.010963]\n",
      "epoch:30 step:23701 [D loss: 0.011464, acc.: 100.00%] [G loss: 0.135724]\n",
      "epoch:30 step:23702 [D loss: 0.012398, acc.: 100.00%] [G loss: 1.164363]\n",
      "epoch:30 step:23703 [D loss: 0.016200, acc.: 100.00%] [G loss: 0.110978]\n",
      "epoch:30 step:23704 [D loss: 0.009509, acc.: 100.00%] [G loss: 0.488018]\n",
      "epoch:30 step:23705 [D loss: 0.057959, acc.: 97.66%] [G loss: 0.002162]\n",
      "epoch:30 step:23706 [D loss: 0.119637, acc.: 96.09%] [G loss: 1.735961]\n",
      "epoch:30 step:23707 [D loss: 0.553013, acc.: 70.31%] [G loss: 0.005237]\n",
      "epoch:30 step:23708 [D loss: 0.164179, acc.: 94.53%] [G loss: 0.103016]\n",
      "epoch:30 step:23709 [D loss: 0.000827, acc.: 100.00%] [G loss: 5.377402]\n",
      "epoch:30 step:23710 [D loss: 0.011462, acc.: 100.00%] [G loss: 4.074920]\n",
      "epoch:30 step:23711 [D loss: 0.003623, acc.: 100.00%] [G loss: 3.128175]\n",
      "epoch:30 step:23712 [D loss: 0.012782, acc.: 100.00%] [G loss: 1.028017]\n",
      "epoch:30 step:23713 [D loss: 0.001635, acc.: 100.00%] [G loss: 4.512855]\n",
      "epoch:30 step:23714 [D loss: 0.001394, acc.: 100.00%] [G loss: 0.327003]\n",
      "epoch:30 step:23715 [D loss: 0.001839, acc.: 100.00%] [G loss: 0.463073]\n",
      "epoch:30 step:23716 [D loss: 0.015727, acc.: 100.00%] [G loss: 0.080420]\n",
      "epoch:30 step:23717 [D loss: 0.007886, acc.: 100.00%] [G loss: 0.006096]\n",
      "epoch:30 step:23718 [D loss: 0.010520, acc.: 100.00%] [G loss: 0.992189]\n",
      "epoch:30 step:23719 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.011083]\n",
      "epoch:30 step:23720 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.007182]\n",
      "epoch:30 step:23721 [D loss: 0.001132, acc.: 100.00%] [G loss: 0.016352]\n",
      "epoch:30 step:23722 [D loss: 0.001152, acc.: 100.00%] [G loss: 0.059943]\n",
      "epoch:30 step:23723 [D loss: 0.072179, acc.: 96.88%] [G loss: 0.035252]\n",
      "epoch:30 step:23724 [D loss: 0.005458, acc.: 100.00%] [G loss: 0.021978]\n",
      "epoch:30 step:23725 [D loss: 0.147798, acc.: 92.97%] [G loss: 0.146131]\n",
      "epoch:30 step:23726 [D loss: 0.002731, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:30 step:23727 [D loss: 0.001879, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:30 step:23728 [D loss: 0.007859, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:30 step:23729 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.713404]\n",
      "epoch:30 step:23730 [D loss: 0.004812, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:30 step:23731 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:30 step:23732 [D loss: 0.001485, acc.: 100.00%] [G loss: 0.014735]\n",
      "epoch:30 step:23733 [D loss: 0.002808, acc.: 100.00%] [G loss: 0.225722]\n",
      "epoch:30 step:23734 [D loss: 0.000543, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:30 step:23735 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.000469]\n",
      "epoch:30 step:23736 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:30 step:23737 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:30 step:23738 [D loss: 0.003216, acc.: 100.00%] [G loss: 0.007598]\n",
      "epoch:30 step:23739 [D loss: 0.002246, acc.: 100.00%] [G loss: 0.000865]\n",
      "epoch:30 step:23740 [D loss: 0.000525, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:30 step:23741 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.020172]\n",
      "epoch:30 step:23742 [D loss: 0.002913, acc.: 100.00%] [G loss: 0.067415]\n",
      "epoch:30 step:23743 [D loss: 0.004296, acc.: 100.00%] [G loss: 0.008475]\n",
      "epoch:30 step:23744 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.000822]\n",
      "epoch:30 step:23745 [D loss: 0.012436, acc.: 99.22%] [G loss: 0.000588]\n",
      "epoch:30 step:23746 [D loss: 0.000511, acc.: 100.00%] [G loss: 0.000632]\n",
      "epoch:30 step:23747 [D loss: 0.002609, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:30 step:23748 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:30 step:23749 [D loss: 0.001260, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:30 step:23750 [D loss: 0.015622, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:30 step:23751 [D loss: 0.003701, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:30 step:23752 [D loss: 0.037944, acc.: 99.22%] [G loss: 0.151652]\n",
      "epoch:30 step:23753 [D loss: 0.010972, acc.: 100.00%] [G loss: 1.208701]\n",
      "epoch:30 step:23754 [D loss: 0.013321, acc.: 100.00%] [G loss: 0.025405]\n",
      "epoch:30 step:23755 [D loss: 0.001908, acc.: 100.00%] [G loss: 0.018170]\n",
      "epoch:30 step:23756 [D loss: 0.100165, acc.: 96.88%] [G loss: 1.182031]\n",
      "epoch:30 step:23757 [D loss: 0.053232, acc.: 98.44%] [G loss: 1.452044]\n",
      "epoch:30 step:23758 [D loss: 0.006970, acc.: 100.00%] [G loss: 2.848486]\n",
      "epoch:30 step:23759 [D loss: 0.007231, acc.: 100.00%] [G loss: 0.150703]\n",
      "epoch:30 step:23760 [D loss: 0.002210, acc.: 100.00%] [G loss: 0.011126]\n",
      "epoch:30 step:23761 [D loss: 0.013375, acc.: 100.00%] [G loss: 0.012694]\n",
      "epoch:30 step:23762 [D loss: 0.002048, acc.: 100.00%] [G loss: 0.003667]\n",
      "epoch:30 step:23763 [D loss: 0.002066, acc.: 100.00%] [G loss: 0.002742]\n",
      "epoch:30 step:23764 [D loss: 0.004409, acc.: 100.00%] [G loss: 0.001649]\n",
      "epoch:30 step:23765 [D loss: 0.144013, acc.: 96.09%] [G loss: 1.027149]\n",
      "epoch:30 step:23766 [D loss: 0.403366, acc.: 85.94%] [G loss: 0.106840]\n",
      "epoch:30 step:23767 [D loss: 0.030809, acc.: 99.22%] [G loss: 0.014881]\n",
      "epoch:30 step:23768 [D loss: 0.001912, acc.: 100.00%] [G loss: 0.094023]\n",
      "epoch:30 step:23769 [D loss: 0.013996, acc.: 100.00%] [G loss: 2.967668]\n",
      "epoch:30 step:23770 [D loss: 0.030121, acc.: 98.44%] [G loss: 0.104326]\n",
      "epoch:30 step:23771 [D loss: 0.001175, acc.: 100.00%] [G loss: 0.803217]\n",
      "epoch:30 step:23772 [D loss: 0.024092, acc.: 100.00%] [G loss: 0.044371]\n",
      "epoch:30 step:23773 [D loss: 0.010212, acc.: 100.00%] [G loss: 0.096606]\n",
      "epoch:30 step:23774 [D loss: 0.001524, acc.: 100.00%] [G loss: 0.025538]\n",
      "epoch:30 step:23775 [D loss: 0.003127, acc.: 100.00%] [G loss: 0.179900]\n",
      "epoch:30 step:23776 [D loss: 0.002309, acc.: 100.00%] [G loss: 0.002865]\n",
      "epoch:30 step:23777 [D loss: 0.003100, acc.: 100.00%] [G loss: 0.001322]\n",
      "epoch:30 step:23778 [D loss: 0.021868, acc.: 99.22%] [G loss: 0.604413]\n",
      "epoch:30 step:23779 [D loss: 0.011807, acc.: 100.00%] [G loss: 0.046101]\n",
      "epoch:30 step:23780 [D loss: 0.119326, acc.: 96.09%] [G loss: 0.013890]\n",
      "epoch:30 step:23781 [D loss: 0.316787, acc.: 84.38%] [G loss: 6.986653]\n",
      "epoch:30 step:23782 [D loss: 1.874439, acc.: 53.91%] [G loss: 0.942055]\n",
      "epoch:30 step:23783 [D loss: 0.391781, acc.: 85.94%] [G loss: 4.470086]\n",
      "epoch:30 step:23784 [D loss: 0.002227, acc.: 100.00%] [G loss: 1.550689]\n",
      "epoch:30 step:23785 [D loss: 0.240517, acc.: 92.97%] [G loss: 0.043890]\n",
      "epoch:30 step:23786 [D loss: 0.005640, acc.: 100.00%] [G loss: 3.619886]\n",
      "epoch:30 step:23787 [D loss: 0.000319, acc.: 100.00%] [G loss: 1.754616]\n",
      "epoch:30 step:23788 [D loss: 0.004968, acc.: 100.00%] [G loss: 0.021616]\n",
      "epoch:30 step:23789 [D loss: 0.002324, acc.: 100.00%] [G loss: 0.010431]\n",
      "epoch:30 step:23790 [D loss: 0.055419, acc.: 100.00%] [G loss: 1.784296]\n",
      "epoch:30 step:23791 [D loss: 0.001678, acc.: 100.00%] [G loss: 0.857783]\n",
      "epoch:30 step:23792 [D loss: 0.000694, acc.: 100.00%] [G loss: 0.693564]\n",
      "epoch:30 step:23793 [D loss: 0.009286, acc.: 100.00%] [G loss: 0.260547]\n",
      "epoch:30 step:23794 [D loss: 0.011951, acc.: 100.00%] [G loss: 0.077207]\n",
      "epoch:30 step:23795 [D loss: 0.002599, acc.: 100.00%] [G loss: 0.238074]\n",
      "epoch:30 step:23796 [D loss: 0.005685, acc.: 100.00%] [G loss: 0.546915]\n",
      "epoch:30 step:23797 [D loss: 0.001307, acc.: 100.00%] [G loss: 0.022162]\n",
      "epoch:30 step:23798 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.188889]\n",
      "epoch:30 step:23799 [D loss: 0.000605, acc.: 100.00%] [G loss: 0.110084]\n",
      "epoch:30 step:23800 [D loss: 0.001834, acc.: 100.00%] [G loss: 0.012785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23801 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.010454]\n",
      "epoch:30 step:23802 [D loss: 0.031172, acc.: 99.22%] [G loss: 0.008536]\n",
      "epoch:30 step:23803 [D loss: 0.083460, acc.: 96.09%] [G loss: 0.541835]\n",
      "epoch:30 step:23804 [D loss: 0.007008, acc.: 100.00%] [G loss: 0.298825]\n",
      "epoch:30 step:23805 [D loss: 0.036679, acc.: 98.44%] [G loss: 0.047840]\n",
      "epoch:30 step:23806 [D loss: 0.024803, acc.: 100.00%] [G loss: 0.005878]\n",
      "epoch:30 step:23807 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.004367]\n",
      "epoch:30 step:23808 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.010563]\n",
      "epoch:30 step:23809 [D loss: 0.000523, acc.: 100.00%] [G loss: 4.259701]\n",
      "epoch:30 step:23810 [D loss: 0.001566, acc.: 100.00%] [G loss: 0.021530]\n",
      "epoch:30 step:23811 [D loss: 0.001287, acc.: 100.00%] [G loss: 0.005991]\n",
      "epoch:30 step:23812 [D loss: 0.001114, acc.: 100.00%] [G loss: 0.011010]\n",
      "epoch:30 step:23813 [D loss: 0.000755, acc.: 100.00%] [G loss: 0.885652]\n",
      "epoch:30 step:23814 [D loss: 0.001867, acc.: 100.00%] [G loss: 0.057494]\n",
      "epoch:30 step:23815 [D loss: 0.004836, acc.: 100.00%] [G loss: 0.030176]\n",
      "epoch:30 step:23816 [D loss: 0.005025, acc.: 100.00%] [G loss: 0.003415]\n",
      "epoch:30 step:23817 [D loss: 0.002946, acc.: 100.00%] [G loss: 0.410430]\n",
      "epoch:30 step:23818 [D loss: 0.006538, acc.: 100.00%] [G loss: 1.260325]\n",
      "epoch:30 step:23819 [D loss: 0.004336, acc.: 100.00%] [G loss: 0.021657]\n",
      "epoch:30 step:23820 [D loss: 0.003072, acc.: 100.00%] [G loss: 0.003926]\n",
      "epoch:30 step:23821 [D loss: 0.011098, acc.: 100.00%] [G loss: 0.008663]\n",
      "epoch:30 step:23822 [D loss: 0.002669, acc.: 100.00%] [G loss: 0.004675]\n",
      "epoch:30 step:23823 [D loss: 0.013813, acc.: 100.00%] [G loss: 0.005958]\n",
      "epoch:30 step:23824 [D loss: 0.010128, acc.: 100.00%] [G loss: 0.021538]\n",
      "epoch:30 step:23825 [D loss: 0.037953, acc.: 99.22%] [G loss: 0.023236]\n",
      "epoch:30 step:23826 [D loss: 0.023683, acc.: 100.00%] [G loss: 0.225765]\n",
      "epoch:30 step:23827 [D loss: 0.023212, acc.: 100.00%] [G loss: 0.446868]\n",
      "epoch:30 step:23828 [D loss: 0.031565, acc.: 100.00%] [G loss: 3.757595]\n",
      "epoch:30 step:23829 [D loss: 0.065346, acc.: 98.44%] [G loss: 4.862730]\n",
      "epoch:30 step:23830 [D loss: 0.010863, acc.: 100.00%] [G loss: 5.091329]\n",
      "epoch:30 step:23831 [D loss: 0.266019, acc.: 90.62%] [G loss: 2.985595]\n",
      "epoch:30 step:23832 [D loss: 0.223717, acc.: 88.28%] [G loss: 7.616001]\n",
      "epoch:30 step:23833 [D loss: 0.199547, acc.: 90.62%] [G loss: 6.577568]\n",
      "epoch:30 step:23834 [D loss: 0.074710, acc.: 96.88%] [G loss: 7.602341]\n",
      "epoch:30 step:23835 [D loss: 0.110359, acc.: 94.53%] [G loss: 5.292887]\n",
      "epoch:30 step:23836 [D loss: 0.008553, acc.: 100.00%] [G loss: 5.352645]\n",
      "epoch:30 step:23837 [D loss: 0.089788, acc.: 96.09%] [G loss: 7.193707]\n",
      "epoch:30 step:23838 [D loss: 0.009428, acc.: 100.00%] [G loss: 7.484344]\n",
      "epoch:30 step:23839 [D loss: 0.027400, acc.: 100.00%] [G loss: 6.027819]\n",
      "epoch:30 step:23840 [D loss: 0.032497, acc.: 99.22%] [G loss: 1.343808]\n",
      "epoch:30 step:23841 [D loss: 0.405007, acc.: 78.12%] [G loss: 2.540312]\n",
      "epoch:30 step:23842 [D loss: 0.016675, acc.: 100.00%] [G loss: 8.339146]\n",
      "epoch:30 step:23843 [D loss: 0.475050, acc.: 78.91%] [G loss: 4.338695]\n",
      "epoch:30 step:23844 [D loss: 0.148645, acc.: 93.75%] [G loss: 6.048467]\n",
      "epoch:30 step:23845 [D loss: 0.002640, acc.: 100.00%] [G loss: 0.099971]\n",
      "epoch:30 step:23846 [D loss: 0.004139, acc.: 100.00%] [G loss: 7.285652]\n",
      "epoch:30 step:23847 [D loss: 0.040737, acc.: 99.22%] [G loss: 5.236389]\n",
      "epoch:30 step:23848 [D loss: 0.011620, acc.: 100.00%] [G loss: 5.508514]\n",
      "epoch:30 step:23849 [D loss: 0.005002, acc.: 100.00%] [G loss: 4.305146]\n",
      "epoch:30 step:23850 [D loss: 0.059691, acc.: 98.44%] [G loss: 4.002488]\n",
      "epoch:30 step:23851 [D loss: 0.011159, acc.: 100.00%] [G loss: 2.618393]\n",
      "epoch:30 step:23852 [D loss: 0.040047, acc.: 99.22%] [G loss: 3.782418]\n",
      "epoch:30 step:23853 [D loss: 0.022800, acc.: 100.00%] [G loss: 4.525621]\n",
      "epoch:30 step:23854 [D loss: 0.118557, acc.: 96.88%] [G loss: 0.152141]\n",
      "epoch:30 step:23855 [D loss: 0.007573, acc.: 100.00%] [G loss: 3.700933]\n",
      "epoch:30 step:23856 [D loss: 0.006695, acc.: 99.22%] [G loss: 2.202507]\n",
      "epoch:30 step:23857 [D loss: 0.000263, acc.: 100.00%] [G loss: 2.072505]\n",
      "epoch:30 step:23858 [D loss: 0.001811, acc.: 100.00%] [G loss: 2.541302]\n",
      "epoch:30 step:23859 [D loss: 0.004450, acc.: 100.00%] [G loss: 0.000787]\n",
      "epoch:30 step:23860 [D loss: 0.009157, acc.: 100.00%] [G loss: 0.532869]\n",
      "epoch:30 step:23861 [D loss: 0.016795, acc.: 100.00%] [G loss: 0.193375]\n",
      "epoch:30 step:23862 [D loss: 0.002937, acc.: 100.00%] [G loss: 0.180636]\n",
      "epoch:30 step:23863 [D loss: 0.007152, acc.: 100.00%] [G loss: 0.015024]\n",
      "epoch:30 step:23864 [D loss: 0.001452, acc.: 100.00%] [G loss: 0.151450]\n",
      "epoch:30 step:23865 [D loss: 0.002498, acc.: 100.00%] [G loss: 0.102743]\n",
      "epoch:30 step:23866 [D loss: 0.012393, acc.: 100.00%] [G loss: 0.173644]\n",
      "epoch:30 step:23867 [D loss: 0.043020, acc.: 100.00%] [G loss: 0.783462]\n",
      "epoch:30 step:23868 [D loss: 0.003459, acc.: 100.00%] [G loss: 0.601448]\n",
      "epoch:30 step:23869 [D loss: 0.014389, acc.: 99.22%] [G loss: 0.398145]\n",
      "epoch:30 step:23870 [D loss: 0.001334, acc.: 100.00%] [G loss: 0.751934]\n",
      "epoch:30 step:23871 [D loss: 0.533367, acc.: 72.66%] [G loss: 6.837175]\n",
      "epoch:30 step:23872 [D loss: 1.633187, acc.: 57.03%] [G loss: 5.933000]\n",
      "epoch:30 step:23873 [D loss: 0.028247, acc.: 99.22%] [G loss: 3.318171]\n",
      "epoch:30 step:23874 [D loss: 0.063644, acc.: 98.44%] [G loss: 1.199708]\n",
      "epoch:30 step:23875 [D loss: 0.393566, acc.: 85.16%] [G loss: 0.319765]\n",
      "epoch:30 step:23876 [D loss: 0.027252, acc.: 99.22%] [G loss: 4.795521]\n",
      "epoch:30 step:23877 [D loss: 0.152753, acc.: 92.97%] [G loss: 3.700090]\n",
      "epoch:30 step:23878 [D loss: 0.180765, acc.: 90.62%] [G loss: 0.000131]\n",
      "epoch:30 step:23879 [D loss: 0.006860, acc.: 100.00%] [G loss: 0.511919]\n",
      "epoch:30 step:23880 [D loss: 0.003203, acc.: 100.00%] [G loss: 0.361373]\n",
      "epoch:30 step:23881 [D loss: 0.003345, acc.: 100.00%] [G loss: 0.158325]\n",
      "epoch:30 step:23882 [D loss: 0.001904, acc.: 100.00%] [G loss: 0.153743]\n",
      "epoch:30 step:23883 [D loss: 0.008915, acc.: 100.00%] [G loss: 0.048239]\n",
      "epoch:30 step:23884 [D loss: 0.009480, acc.: 100.00%] [G loss: 0.029811]\n",
      "epoch:30 step:23885 [D loss: 0.066259, acc.: 97.66%] [G loss: 0.003169]\n",
      "epoch:30 step:23886 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.644927]\n",
      "epoch:30 step:23887 [D loss: 0.000202, acc.: 100.00%] [G loss: 1.083926]\n",
      "epoch:30 step:23888 [D loss: 0.000556, acc.: 100.00%] [G loss: 2.359669]\n",
      "epoch:30 step:23889 [D loss: 0.009196, acc.: 100.00%] [G loss: 0.267805]\n",
      "epoch:30 step:23890 [D loss: 0.017652, acc.: 100.00%] [G loss: 0.040596]\n",
      "epoch:30 step:23891 [D loss: 0.013794, acc.: 100.00%] [G loss: 0.108566]\n",
      "epoch:30 step:23892 [D loss: 0.007763, acc.: 100.00%] [G loss: 0.006745]\n",
      "epoch:30 step:23893 [D loss: 0.001367, acc.: 100.00%] [G loss: 0.003789]\n",
      "epoch:30 step:23894 [D loss: 0.003794, acc.: 100.00%] [G loss: 0.453732]\n",
      "epoch:30 step:23895 [D loss: 0.006077, acc.: 100.00%] [G loss: 0.023102]\n",
      "epoch:30 step:23896 [D loss: 0.003469, acc.: 100.00%] [G loss: 0.638986]\n",
      "epoch:30 step:23897 [D loss: 0.069801, acc.: 97.66%] [G loss: 0.142701]\n",
      "epoch:30 step:23898 [D loss: 0.007158, acc.: 100.00%] [G loss: 0.161047]\n",
      "epoch:30 step:23899 [D loss: 0.001371, acc.: 100.00%] [G loss: 1.941590]\n",
      "epoch:30 step:23900 [D loss: 0.005640, acc.: 100.00%] [G loss: 0.082472]\n",
      "epoch:30 step:23901 [D loss: 0.006515, acc.: 100.00%] [G loss: 0.041745]\n",
      "epoch:30 step:23902 [D loss: 0.033279, acc.: 99.22%] [G loss: 0.958012]\n",
      "epoch:30 step:23903 [D loss: 0.006877, acc.: 100.00%] [G loss: 0.014155]\n",
      "epoch:30 step:23904 [D loss: 0.027857, acc.: 100.00%] [G loss: 0.058996]\n",
      "epoch:30 step:23905 [D loss: 0.058567, acc.: 100.00%] [G loss: 0.419473]\n",
      "epoch:30 step:23906 [D loss: 0.007653, acc.: 100.00%] [G loss: 0.163070]\n",
      "epoch:30 step:23907 [D loss: 0.002193, acc.: 100.00%] [G loss: 0.566485]\n",
      "epoch:30 step:23908 [D loss: 0.072199, acc.: 96.88%] [G loss: 0.738774]\n",
      "epoch:30 step:23909 [D loss: 0.000853, acc.: 100.00%] [G loss: 0.087145]\n",
      "epoch:30 step:23910 [D loss: 0.003553, acc.: 100.00%] [G loss: 0.001960]\n",
      "epoch:30 step:23911 [D loss: 0.001616, acc.: 100.00%] [G loss: 0.001298]\n",
      "epoch:30 step:23912 [D loss: 0.013288, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:30 step:23913 [D loss: 0.004419, acc.: 100.00%] [G loss: 0.000898]\n",
      "epoch:30 step:23914 [D loss: 0.014467, acc.: 100.00%] [G loss: 0.019071]\n",
      "epoch:30 step:23915 [D loss: 0.007244, acc.: 100.00%] [G loss: 0.002295]\n",
      "epoch:30 step:23916 [D loss: 0.001003, acc.: 100.00%] [G loss: 0.006349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:23917 [D loss: 0.002789, acc.: 100.00%] [G loss: 0.012506]\n",
      "epoch:30 step:23918 [D loss: 0.057787, acc.: 98.44%] [G loss: 1.275042]\n",
      "epoch:30 step:23919 [D loss: 0.001833, acc.: 100.00%] [G loss: 0.557521]\n",
      "epoch:30 step:23920 [D loss: 0.005942, acc.: 100.00%] [G loss: 0.359331]\n",
      "epoch:30 step:23921 [D loss: 0.026750, acc.: 99.22%] [G loss: 0.043836]\n",
      "epoch:30 step:23922 [D loss: 0.005607, acc.: 100.00%] [G loss: 0.097698]\n",
      "epoch:30 step:23923 [D loss: 0.001776, acc.: 100.00%] [G loss: 0.004691]\n",
      "epoch:30 step:23924 [D loss: 0.021017, acc.: 100.00%] [G loss: 1.649399]\n",
      "epoch:30 step:23925 [D loss: 0.008079, acc.: 100.00%] [G loss: 0.036696]\n",
      "epoch:30 step:23926 [D loss: 0.001249, acc.: 100.00%] [G loss: 0.237902]\n",
      "epoch:30 step:23927 [D loss: 0.008108, acc.: 100.00%] [G loss: 0.027359]\n",
      "epoch:30 step:23928 [D loss: 0.022921, acc.: 99.22%] [G loss: 0.052662]\n",
      "epoch:30 step:23929 [D loss: 0.020035, acc.: 98.44%] [G loss: 0.029961]\n",
      "epoch:30 step:23930 [D loss: 0.002738, acc.: 100.00%] [G loss: 0.072704]\n",
      "epoch:30 step:23931 [D loss: 0.055790, acc.: 100.00%] [G loss: 0.125387]\n",
      "epoch:30 step:23932 [D loss: 0.003285, acc.: 100.00%] [G loss: 0.519013]\n",
      "epoch:30 step:23933 [D loss: 0.011244, acc.: 100.00%] [G loss: 1.015323]\n",
      "epoch:30 step:23934 [D loss: 0.004420, acc.: 100.00%] [G loss: 0.623997]\n",
      "epoch:30 step:23935 [D loss: 0.029228, acc.: 100.00%] [G loss: 2.993321]\n",
      "epoch:30 step:23936 [D loss: 0.011992, acc.: 100.00%] [G loss: 0.251239]\n",
      "epoch:30 step:23937 [D loss: 0.010644, acc.: 100.00%] [G loss: 1.080415]\n",
      "epoch:30 step:23938 [D loss: 0.007612, acc.: 100.00%] [G loss: 0.477092]\n",
      "epoch:30 step:23939 [D loss: 0.061362, acc.: 97.66%] [G loss: 0.032387]\n",
      "epoch:30 step:23940 [D loss: 0.050220, acc.: 98.44%] [G loss: 1.016385]\n",
      "epoch:30 step:23941 [D loss: 0.007478, acc.: 100.00%] [G loss: 4.521358]\n",
      "epoch:30 step:23942 [D loss: 0.001170, acc.: 100.00%] [G loss: 1.541875]\n",
      "epoch:30 step:23943 [D loss: 1.071498, acc.: 57.81%] [G loss: 10.266010]\n",
      "epoch:30 step:23944 [D loss: 0.543230, acc.: 76.56%] [G loss: 10.667873]\n",
      "epoch:30 step:23945 [D loss: 0.359816, acc.: 82.03%] [G loss: 6.016221]\n",
      "epoch:30 step:23946 [D loss: 0.403684, acc.: 87.50%] [G loss: 0.020917]\n",
      "epoch:30 step:23947 [D loss: 0.002331, acc.: 100.00%] [G loss: 1.006679]\n",
      "epoch:30 step:23948 [D loss: 0.067739, acc.: 98.44%] [G loss: 11.283590]\n",
      "epoch:30 step:23949 [D loss: 0.077225, acc.: 96.88%] [G loss: 9.452728]\n",
      "epoch:30 step:23950 [D loss: 0.012224, acc.: 99.22%] [G loss: 0.102814]\n",
      "epoch:30 step:23951 [D loss: 0.008970, acc.: 100.00%] [G loss: 0.059920]\n",
      "epoch:30 step:23952 [D loss: 0.003769, acc.: 100.00%] [G loss: 7.333903]\n",
      "epoch:30 step:23953 [D loss: 0.004127, acc.: 100.00%] [G loss: 4.250742]\n",
      "epoch:30 step:23954 [D loss: 0.007459, acc.: 100.00%] [G loss: 1.907315]\n",
      "epoch:30 step:23955 [D loss: 0.005132, acc.: 100.00%] [G loss: 0.995432]\n",
      "epoch:30 step:23956 [D loss: 0.033375, acc.: 100.00%] [G loss: 0.114258]\n",
      "epoch:30 step:23957 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.157894]\n",
      "epoch:30 step:23958 [D loss: 0.000870, acc.: 100.00%] [G loss: 0.038493]\n",
      "epoch:30 step:23959 [D loss: 0.000937, acc.: 100.00%] [G loss: 0.039198]\n",
      "epoch:30 step:23960 [D loss: 0.001635, acc.: 100.00%] [G loss: 0.075647]\n",
      "epoch:30 step:23961 [D loss: 0.014055, acc.: 100.00%] [G loss: 0.001434]\n",
      "epoch:30 step:23962 [D loss: 0.009103, acc.: 100.00%] [G loss: 0.006495]\n",
      "epoch:30 step:23963 [D loss: 0.004465, acc.: 100.00%] [G loss: 0.008578]\n",
      "epoch:30 step:23964 [D loss: 0.006934, acc.: 100.00%] [G loss: 0.049506]\n",
      "epoch:30 step:23965 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.045947]\n",
      "epoch:30 step:23966 [D loss: 0.002519, acc.: 100.00%] [G loss: 0.036913]\n",
      "epoch:30 step:23967 [D loss: 0.027404, acc.: 99.22%] [G loss: 0.001109]\n",
      "epoch:30 step:23968 [D loss: 0.000555, acc.: 100.00%] [G loss: 0.011691]\n",
      "epoch:30 step:23969 [D loss: 0.000823, acc.: 100.00%] [G loss: 0.006437]\n",
      "epoch:30 step:23970 [D loss: 0.003265, acc.: 100.00%] [G loss: 0.000969]\n",
      "epoch:30 step:23971 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.012974]\n",
      "epoch:30 step:23972 [D loss: 0.002574, acc.: 100.00%] [G loss: 0.003473]\n",
      "epoch:30 step:23973 [D loss: 0.000767, acc.: 100.00%] [G loss: 0.076136]\n",
      "epoch:30 step:23974 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.741999]\n",
      "epoch:30 step:23975 [D loss: 0.001807, acc.: 100.00%] [G loss: 0.001034]\n",
      "epoch:30 step:23976 [D loss: 0.017109, acc.: 100.00%] [G loss: 0.325813]\n",
      "epoch:30 step:23977 [D loss: 0.267713, acc.: 89.84%] [G loss: 2.089283]\n",
      "epoch:30 step:23978 [D loss: 0.118033, acc.: 92.97%] [G loss: 0.829851]\n",
      "epoch:30 step:23979 [D loss: 0.058061, acc.: 96.88%] [G loss: 0.090542]\n",
      "epoch:30 step:23980 [D loss: 0.143262, acc.: 92.19%] [G loss: 0.000045]\n",
      "epoch:30 step:23981 [D loss: 0.007327, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:30 step:23982 [D loss: 0.021710, acc.: 98.44%] [G loss: 0.172987]\n",
      "epoch:30 step:23983 [D loss: 0.041981, acc.: 99.22%] [G loss: 0.191448]\n",
      "epoch:30 step:23984 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.186314]\n",
      "epoch:30 step:23985 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.005147]\n",
      "epoch:30 step:23986 [D loss: 0.001680, acc.: 100.00%] [G loss: 0.027911]\n",
      "epoch:30 step:23987 [D loss: 0.001052, acc.: 100.00%] [G loss: 0.004216]\n",
      "epoch:30 step:23988 [D loss: 0.002311, acc.: 100.00%] [G loss: 0.004713]\n",
      "epoch:30 step:23989 [D loss: 0.003734, acc.: 100.00%] [G loss: 0.011369]\n",
      "epoch:30 step:23990 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.578155]\n",
      "epoch:30 step:23991 [D loss: 0.000538, acc.: 100.00%] [G loss: 0.009256]\n",
      "epoch:30 step:23992 [D loss: 0.000883, acc.: 100.00%] [G loss: 0.265457]\n",
      "epoch:30 step:23993 [D loss: 0.001070, acc.: 100.00%] [G loss: 0.003931]\n",
      "epoch:30 step:23994 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.942212]\n",
      "epoch:30 step:23995 [D loss: 0.005762, acc.: 100.00%] [G loss: 0.001704]\n",
      "epoch:30 step:23996 [D loss: 0.001328, acc.: 100.00%] [G loss: 0.010976]\n",
      "epoch:30 step:23997 [D loss: 0.001531, acc.: 100.00%] [G loss: 0.002861]\n",
      "epoch:30 step:23998 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.002785]\n",
      "epoch:30 step:23999 [D loss: 0.007511, acc.: 100.00%] [G loss: 0.231419]\n",
      "epoch:30 step:24000 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.009897]\n",
      "epoch:30 step:24001 [D loss: 0.002110, acc.: 100.00%] [G loss: 0.036752]\n",
      "epoch:30 step:24002 [D loss: 0.000989, acc.: 100.00%] [G loss: 0.009767]\n",
      "epoch:30 step:24003 [D loss: 0.004619, acc.: 100.00%] [G loss: 0.003089]\n",
      "epoch:30 step:24004 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.056842]\n",
      "epoch:30 step:24005 [D loss: 0.013926, acc.: 100.00%] [G loss: 0.005149]\n",
      "epoch:30 step:24006 [D loss: 0.000782, acc.: 100.00%] [G loss: 0.006359]\n",
      "epoch:30 step:24007 [D loss: 0.004945, acc.: 100.00%] [G loss: 0.176958]\n",
      "epoch:30 step:24008 [D loss: 0.011400, acc.: 99.22%] [G loss: 0.120394]\n",
      "epoch:30 step:24009 [D loss: 0.002450, acc.: 100.00%] [G loss: 0.125074]\n",
      "epoch:30 step:24010 [D loss: 0.095590, acc.: 95.31%] [G loss: 0.167047]\n",
      "epoch:30 step:24011 [D loss: 0.038878, acc.: 98.44%] [G loss: 2.394087]\n",
      "epoch:30 step:24012 [D loss: 0.002816, acc.: 100.00%] [G loss: 0.128536]\n",
      "epoch:30 step:24013 [D loss: 0.011534, acc.: 100.00%] [G loss: 0.105813]\n",
      "epoch:30 step:24014 [D loss: 0.004923, acc.: 100.00%] [G loss: 0.009827]\n",
      "epoch:30 step:24015 [D loss: 0.003309, acc.: 100.00%] [G loss: 0.002959]\n",
      "epoch:30 step:24016 [D loss: 0.008219, acc.: 99.22%] [G loss: 0.003199]\n",
      "epoch:30 step:24017 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.025433]\n",
      "epoch:30 step:24018 [D loss: 0.001237, acc.: 100.00%] [G loss: 0.005038]\n",
      "epoch:30 step:24019 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.286135]\n",
      "epoch:30 step:24020 [D loss: 0.001864, acc.: 100.00%] [G loss: 0.422665]\n",
      "epoch:30 step:24021 [D loss: 0.002213, acc.: 100.00%] [G loss: 0.332847]\n",
      "epoch:30 step:24022 [D loss: 0.006703, acc.: 100.00%] [G loss: 0.001586]\n",
      "epoch:30 step:24023 [D loss: 0.008288, acc.: 100.00%] [G loss: 0.004947]\n",
      "epoch:30 step:24024 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.112168]\n",
      "epoch:30 step:24025 [D loss: 0.073258, acc.: 98.44%] [G loss: 0.192714]\n",
      "epoch:30 step:24026 [D loss: 0.027839, acc.: 99.22%] [G loss: 0.205889]\n",
      "epoch:30 step:24027 [D loss: 0.017787, acc.: 100.00%] [G loss: 0.015661]\n",
      "epoch:30 step:24028 [D loss: 0.000762, acc.: 100.00%] [G loss: 0.018874]\n",
      "epoch:30 step:24029 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.018257]\n",
      "epoch:30 step:24030 [D loss: 0.005711, acc.: 100.00%] [G loss: 0.001818]\n",
      "epoch:30 step:24031 [D loss: 0.002216, acc.: 100.00%] [G loss: 0.002593]\n",
      "epoch:30 step:24032 [D loss: 0.003056, acc.: 100.00%] [G loss: 0.433407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:24033 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.006650]\n",
      "epoch:30 step:24034 [D loss: 0.110260, acc.: 94.53%] [G loss: 0.000035]\n",
      "epoch:30 step:24035 [D loss: 0.002252, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:30 step:24036 [D loss: 0.072890, acc.: 98.44%] [G loss: 0.117956]\n",
      "epoch:30 step:24037 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.180176]\n",
      "epoch:30 step:24038 [D loss: 0.004510, acc.: 100.00%] [G loss: 0.082211]\n",
      "epoch:30 step:24039 [D loss: 0.005060, acc.: 100.00%] [G loss: 0.112537]\n",
      "epoch:30 step:24040 [D loss: 0.009666, acc.: 100.00%] [G loss: 0.226322]\n",
      "epoch:30 step:24041 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.036561]\n",
      "epoch:30 step:24042 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.078972]\n",
      "epoch:30 step:24043 [D loss: 0.018775, acc.: 99.22%] [G loss: 0.062768]\n",
      "epoch:30 step:24044 [D loss: 0.001199, acc.: 100.00%] [G loss: 0.001648]\n",
      "epoch:30 step:24045 [D loss: 0.000930, acc.: 100.00%] [G loss: 0.002206]\n",
      "epoch:30 step:24046 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.002033]\n",
      "epoch:30 step:24047 [D loss: 0.000896, acc.: 100.00%] [G loss: 0.089818]\n",
      "epoch:30 step:24048 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.005773]\n",
      "epoch:30 step:24049 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.126929]\n",
      "epoch:30 step:24050 [D loss: 0.001281, acc.: 100.00%] [G loss: 0.006702]\n",
      "epoch:30 step:24051 [D loss: 0.003812, acc.: 100.00%] [G loss: 0.002167]\n",
      "epoch:30 step:24052 [D loss: 0.006524, acc.: 100.00%] [G loss: 0.002818]\n",
      "epoch:30 step:24053 [D loss: 0.003722, acc.: 100.00%] [G loss: 0.001700]\n",
      "epoch:30 step:24054 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.067250]\n",
      "epoch:30 step:24055 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.005670]\n",
      "epoch:30 step:24056 [D loss: 0.002659, acc.: 100.00%] [G loss: 0.002153]\n",
      "epoch:30 step:24057 [D loss: 0.001085, acc.: 100.00%] [G loss: 0.007310]\n",
      "epoch:30 step:24058 [D loss: 0.002058, acc.: 100.00%] [G loss: 0.009919]\n",
      "epoch:30 step:24059 [D loss: 0.002181, acc.: 100.00%] [G loss: 0.182903]\n",
      "epoch:30 step:24060 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.000385]\n",
      "epoch:30 step:24061 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.001331]\n",
      "epoch:30 step:24062 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.002104]\n",
      "epoch:30 step:24063 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000578]\n",
      "epoch:30 step:24064 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.000414]\n",
      "epoch:30 step:24065 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.001397]\n",
      "epoch:30 step:24066 [D loss: 0.000078, acc.: 100.00%] [G loss: 1.297808]\n",
      "epoch:30 step:24067 [D loss: 0.011818, acc.: 100.00%] [G loss: 0.000857]\n",
      "epoch:30 step:24068 [D loss: 0.005978, acc.: 100.00%] [G loss: 0.015271]\n",
      "epoch:30 step:24069 [D loss: 0.004543, acc.: 100.00%] [G loss: 0.001508]\n",
      "epoch:30 step:24070 [D loss: 0.006143, acc.: 100.00%] [G loss: 0.006707]\n",
      "epoch:30 step:24071 [D loss: 0.008412, acc.: 100.00%] [G loss: 0.010654]\n",
      "epoch:30 step:24072 [D loss: 0.007545, acc.: 100.00%] [G loss: 0.607516]\n",
      "epoch:30 step:24073 [D loss: 0.002956, acc.: 100.00%] [G loss: 0.011166]\n",
      "epoch:30 step:24074 [D loss: 0.002135, acc.: 100.00%] [G loss: 0.062155]\n",
      "epoch:30 step:24075 [D loss: 0.003967, acc.: 100.00%] [G loss: 0.059910]\n",
      "epoch:30 step:24076 [D loss: 0.834997, acc.: 53.91%] [G loss: 2.783311]\n",
      "epoch:30 step:24077 [D loss: 0.734984, acc.: 67.97%] [G loss: 3.417441]\n",
      "epoch:30 step:24078 [D loss: 0.021176, acc.: 100.00%] [G loss: 0.301336]\n",
      "epoch:30 step:24079 [D loss: 0.127126, acc.: 97.66%] [G loss: 1.045251]\n",
      "epoch:30 step:24080 [D loss: 0.008442, acc.: 100.00%] [G loss: 0.069953]\n",
      "epoch:30 step:24081 [D loss: 0.056645, acc.: 97.66%] [G loss: 0.055628]\n",
      "epoch:30 step:24082 [D loss: 0.012960, acc.: 100.00%] [G loss: 0.005901]\n",
      "epoch:30 step:24083 [D loss: 0.006947, acc.: 100.00%] [G loss: 0.004572]\n",
      "epoch:30 step:24084 [D loss: 0.026482, acc.: 99.22%] [G loss: 0.009063]\n",
      "epoch:30 step:24085 [D loss: 0.004417, acc.: 100.00%] [G loss: 0.017741]\n",
      "epoch:30 step:24086 [D loss: 0.026570, acc.: 100.00%] [G loss: 0.010725]\n",
      "epoch:30 step:24087 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.011613]\n",
      "epoch:30 step:24088 [D loss: 0.022205, acc.: 100.00%] [G loss: 0.619570]\n",
      "epoch:30 step:24089 [D loss: 0.002855, acc.: 100.00%] [G loss: 0.052943]\n",
      "epoch:30 step:24090 [D loss: 0.000737, acc.: 100.00%] [G loss: 3.977537]\n",
      "epoch:30 step:24091 [D loss: 0.009819, acc.: 100.00%] [G loss: 0.009101]\n",
      "epoch:30 step:24092 [D loss: 0.050727, acc.: 100.00%] [G loss: 0.008679]\n",
      "epoch:30 step:24093 [D loss: 0.016221, acc.: 100.00%] [G loss: 0.035659]\n",
      "epoch:30 step:24094 [D loss: 0.075496, acc.: 98.44%] [G loss: 0.866506]\n",
      "epoch:30 step:24095 [D loss: 0.105439, acc.: 96.09%] [G loss: 0.008434]\n",
      "epoch:30 step:24096 [D loss: 0.002911, acc.: 100.00%] [G loss: 0.006252]\n",
      "epoch:30 step:24097 [D loss: 0.011660, acc.: 100.00%] [G loss: 0.079281]\n",
      "epoch:30 step:24098 [D loss: 0.005927, acc.: 100.00%] [G loss: 0.472774]\n",
      "epoch:30 step:24099 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.201808]\n",
      "epoch:30 step:24100 [D loss: 0.001324, acc.: 100.00%] [G loss: 0.283004]\n",
      "epoch:30 step:24101 [D loss: 0.001647, acc.: 100.00%] [G loss: 0.057596]\n",
      "epoch:30 step:24102 [D loss: 0.003694, acc.: 100.00%] [G loss: 0.007873]\n",
      "epoch:30 step:24103 [D loss: 0.000973, acc.: 100.00%] [G loss: 0.159547]\n",
      "epoch:30 step:24104 [D loss: 0.001793, acc.: 100.00%] [G loss: 0.002134]\n",
      "epoch:30 step:24105 [D loss: 0.010880, acc.: 100.00%] [G loss: 0.005247]\n",
      "epoch:30 step:24106 [D loss: 0.072161, acc.: 96.09%] [G loss: 0.000130]\n",
      "epoch:30 step:24107 [D loss: 0.005203, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:30 step:24108 [D loss: 0.182798, acc.: 90.62%] [G loss: 4.390974]\n",
      "epoch:30 step:24109 [D loss: 0.002186, acc.: 100.00%] [G loss: 3.775464]\n",
      "epoch:30 step:24110 [D loss: 0.436909, acc.: 83.59%] [G loss: 0.014152]\n",
      "epoch:30 step:24111 [D loss: 0.027276, acc.: 100.00%] [G loss: 0.159710]\n",
      "epoch:30 step:24112 [D loss: 0.007651, acc.: 100.00%] [G loss: 0.075519]\n",
      "epoch:30 step:24113 [D loss: 0.004314, acc.: 100.00%] [G loss: 0.326781]\n",
      "epoch:30 step:24114 [D loss: 0.003485, acc.: 100.00%] [G loss: 0.022317]\n",
      "epoch:30 step:24115 [D loss: 0.007678, acc.: 100.00%] [G loss: 0.307377]\n",
      "epoch:30 step:24116 [D loss: 0.011857, acc.: 100.00%] [G loss: 0.100712]\n",
      "epoch:30 step:24117 [D loss: 0.002823, acc.: 100.00%] [G loss: 0.029031]\n",
      "epoch:30 step:24118 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.038791]\n",
      "epoch:30 step:24119 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.017650]\n",
      "epoch:30 step:24120 [D loss: 0.000615, acc.: 100.00%] [G loss: 0.012116]\n",
      "epoch:30 step:24121 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.005304]\n",
      "epoch:30 step:24122 [D loss: 0.006161, acc.: 100.00%] [G loss: 0.001153]\n",
      "epoch:30 step:24123 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.286318]\n",
      "epoch:30 step:24124 [D loss: 0.005892, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:30 step:24125 [D loss: 0.110025, acc.: 96.88%] [G loss: 0.346390]\n",
      "epoch:30 step:24126 [D loss: 0.001364, acc.: 100.00%] [G loss: 1.772207]\n",
      "epoch:30 step:24127 [D loss: 0.101926, acc.: 96.09%] [G loss: 0.003486]\n",
      "epoch:30 step:24128 [D loss: 0.001196, acc.: 100.00%] [G loss: 2.048803]\n",
      "epoch:30 step:24129 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.001058]\n",
      "epoch:30 step:24130 [D loss: 0.000084, acc.: 100.00%] [G loss: 4.053544]\n",
      "epoch:30 step:24131 [D loss: 0.009362, acc.: 100.00%] [G loss: 0.001124]\n",
      "epoch:30 step:24132 [D loss: 0.010446, acc.: 100.00%] [G loss: 0.000771]\n",
      "epoch:30 step:24133 [D loss: 0.007228, acc.: 100.00%] [G loss: 0.005703]\n",
      "epoch:30 step:24134 [D loss: 0.010021, acc.: 100.00%] [G loss: 0.018994]\n",
      "epoch:30 step:24135 [D loss: 0.130969, acc.: 95.31%] [G loss: 1.106404]\n",
      "epoch:30 step:24136 [D loss: 0.002673, acc.: 100.00%] [G loss: 6.796472]\n",
      "epoch:30 step:24137 [D loss: 0.337586, acc.: 85.16%] [G loss: 0.580881]\n",
      "epoch:30 step:24138 [D loss: 0.059060, acc.: 97.66%] [G loss: 0.125627]\n",
      "epoch:30 step:24139 [D loss: 0.000982, acc.: 100.00%] [G loss: 0.523918]\n",
      "epoch:30 step:24140 [D loss: 0.084938, acc.: 99.22%] [G loss: 0.311782]\n",
      "epoch:30 step:24141 [D loss: 0.003213, acc.: 100.00%] [G loss: 0.669406]\n",
      "epoch:30 step:24142 [D loss: 0.002084, acc.: 100.00%] [G loss: 0.511794]\n",
      "epoch:30 step:24143 [D loss: 0.010254, acc.: 99.22%] [G loss: 0.210097]\n",
      "epoch:30 step:24144 [D loss: 0.009878, acc.: 100.00%] [G loss: 0.040779]\n",
      "epoch:30 step:24145 [D loss: 0.032456, acc.: 99.22%] [G loss: 0.305724]\n",
      "epoch:30 step:24146 [D loss: 0.003584, acc.: 100.00%] [G loss: 0.095173]\n",
      "epoch:30 step:24147 [D loss: 0.006082, acc.: 100.00%] [G loss: 0.026591]\n",
      "epoch:30 step:24148 [D loss: 0.050219, acc.: 98.44%] [G loss: 1.200552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:30 step:24149 [D loss: 0.004144, acc.: 100.00%] [G loss: 0.780184]\n",
      "epoch:30 step:24150 [D loss: 0.009048, acc.: 100.00%] [G loss: 0.599449]\n",
      "epoch:30 step:24151 [D loss: 0.021061, acc.: 100.00%] [G loss: 1.476863]\n",
      "epoch:30 step:24152 [D loss: 0.019674, acc.: 100.00%] [G loss: 0.397939]\n",
      "epoch:30 step:24153 [D loss: 0.032418, acc.: 100.00%] [G loss: 1.780759]\n",
      "epoch:30 step:24154 [D loss: 0.333185, acc.: 84.38%] [G loss: 8.719856]\n",
      "epoch:30 step:24155 [D loss: 0.265451, acc.: 85.94%] [G loss: 2.904058]\n",
      "epoch:30 step:24156 [D loss: 0.037264, acc.: 100.00%] [G loss: 5.614619]\n",
      "epoch:30 step:24157 [D loss: 0.046350, acc.: 99.22%] [G loss: 4.610575]\n",
      "epoch:30 step:24158 [D loss: 0.016062, acc.: 100.00%] [G loss: 5.567153]\n",
      "epoch:30 step:24159 [D loss: 0.016197, acc.: 100.00%] [G loss: 5.216610]\n",
      "epoch:30 step:24160 [D loss: 0.056756, acc.: 96.88%] [G loss: 2.859786]\n",
      "epoch:30 step:24161 [D loss: 0.224843, acc.: 92.19%] [G loss: 7.751944]\n",
      "epoch:30 step:24162 [D loss: 0.640068, acc.: 75.78%] [G loss: 9.283897]\n",
      "epoch:30 step:24163 [D loss: 0.055314, acc.: 99.22%] [G loss: 10.260720]\n",
      "epoch:30 step:24164 [D loss: 0.009232, acc.: 100.00%] [G loss: 0.296150]\n",
      "epoch:30 step:24165 [D loss: 0.013128, acc.: 100.00%] [G loss: 0.046505]\n",
      "epoch:30 step:24166 [D loss: 0.013215, acc.: 99.22%] [G loss: 9.627005]\n",
      "epoch:30 step:24167 [D loss: 0.015012, acc.: 99.22%] [G loss: 9.238401]\n",
      "epoch:30 step:24168 [D loss: 0.005800, acc.: 100.00%] [G loss: 8.969372]\n",
      "epoch:30 step:24169 [D loss: 0.023318, acc.: 100.00%] [G loss: 0.136794]\n",
      "epoch:30 step:24170 [D loss: 0.018675, acc.: 100.00%] [G loss: 8.363085]\n",
      "epoch:30 step:24171 [D loss: 0.009112, acc.: 100.00%] [G loss: 7.635534]\n",
      "epoch:30 step:24172 [D loss: 0.016739, acc.: 99.22%] [G loss: 6.871001]\n",
      "epoch:30 step:24173 [D loss: 0.026955, acc.: 99.22%] [G loss: 5.605626]\n",
      "epoch:30 step:24174 [D loss: 0.002359, acc.: 100.00%] [G loss: 4.871951]\n",
      "epoch:30 step:24175 [D loss: 0.013953, acc.: 99.22%] [G loss: 3.858601]\n",
      "epoch:30 step:24176 [D loss: 0.044910, acc.: 99.22%] [G loss: 2.323358]\n",
      "epoch:30 step:24177 [D loss: 0.035728, acc.: 98.44%] [G loss: 3.165507]\n",
      "epoch:30 step:24178 [D loss: 0.039641, acc.: 99.22%] [G loss: 1.270634]\n",
      "epoch:30 step:24179 [D loss: 0.042909, acc.: 99.22%] [G loss: 3.170755]\n",
      "epoch:30 step:24180 [D loss: 0.016561, acc.: 100.00%] [G loss: 0.550853]\n",
      "epoch:30 step:24181 [D loss: 0.010627, acc.: 100.00%] [G loss: 2.227270]\n",
      "epoch:30 step:24182 [D loss: 0.013304, acc.: 100.00%] [G loss: 0.165033]\n",
      "epoch:30 step:24183 [D loss: 0.002485, acc.: 100.00%] [G loss: 0.167196]\n",
      "epoch:30 step:24184 [D loss: 0.012465, acc.: 99.22%] [G loss: 3.748106]\n",
      "epoch:30 step:24185 [D loss: 0.034410, acc.: 100.00%] [G loss: 0.277601]\n",
      "epoch:30 step:24186 [D loss: 0.019359, acc.: 99.22%] [G loss: 2.981382]\n",
      "epoch:30 step:24187 [D loss: 0.003657, acc.: 100.00%] [G loss: 1.065255]\n",
      "epoch:30 step:24188 [D loss: 0.002580, acc.: 100.00%] [G loss: 0.314079]\n",
      "epoch:30 step:24189 [D loss: 0.002987, acc.: 100.00%] [G loss: 0.488441]\n",
      "epoch:30 step:24190 [D loss: 0.032958, acc.: 99.22%] [G loss: 0.025985]\n",
      "epoch:30 step:24191 [D loss: 0.144125, acc.: 94.53%] [G loss: 0.243577]\n",
      "epoch:30 step:24192 [D loss: 1.216341, acc.: 62.50%] [G loss: 0.000030]\n",
      "epoch:30 step:24193 [D loss: 0.102230, acc.: 96.09%] [G loss: 0.000163]\n",
      "epoch:30 step:24194 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.392238]\n",
      "epoch:30 step:24195 [D loss: 0.001903, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:30 step:24196 [D loss: 0.009480, acc.: 100.00%] [G loss: 0.358363]\n",
      "epoch:30 step:24197 [D loss: 0.019102, acc.: 100.00%] [G loss: 2.152856]\n",
      "epoch:30 step:24198 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.703171]\n",
      "epoch:30 step:24199 [D loss: 0.011522, acc.: 100.00%] [G loss: 0.001486]\n",
      "epoch:30 step:24200 [D loss: 0.059332, acc.: 98.44%] [G loss: 0.768169]\n",
      "epoch:30 step:24201 [D loss: 0.001706, acc.: 100.00%] [G loss: 0.167074]\n",
      "epoch:30 step:24202 [D loss: 0.001830, acc.: 100.00%] [G loss: 0.861871]\n",
      "epoch:30 step:24203 [D loss: 0.009649, acc.: 99.22%] [G loss: 0.552279]\n",
      "epoch:30 step:24204 [D loss: 0.004036, acc.: 100.00%] [G loss: 0.206753]\n",
      "epoch:30 step:24205 [D loss: 0.009204, acc.: 99.22%] [G loss: 0.297375]\n",
      "epoch:30 step:24206 [D loss: 0.003938, acc.: 100.00%] [G loss: 0.054659]\n",
      "epoch:30 step:24207 [D loss: 0.001293, acc.: 100.00%] [G loss: 0.189934]\n",
      "epoch:30 step:24208 [D loss: 0.001350, acc.: 100.00%] [G loss: 0.122386]\n",
      "epoch:30 step:24209 [D loss: 0.003990, acc.: 100.00%] [G loss: 0.096718]\n",
      "epoch:30 step:24210 [D loss: 0.003178, acc.: 100.00%] [G loss: 0.037231]\n",
      "epoch:30 step:24211 [D loss: 0.000711, acc.: 100.00%] [G loss: 0.106042]\n",
      "epoch:31 step:24212 [D loss: 0.003135, acc.: 100.00%] [G loss: 0.033145]\n",
      "epoch:31 step:24213 [D loss: 0.006875, acc.: 100.00%] [G loss: 0.062906]\n",
      "epoch:31 step:24214 [D loss: 0.003299, acc.: 100.00%] [G loss: 0.003828]\n",
      "epoch:31 step:24215 [D loss: 0.005202, acc.: 100.00%] [G loss: 0.016383]\n",
      "epoch:31 step:24216 [D loss: 0.000854, acc.: 100.00%] [G loss: 0.007512]\n",
      "epoch:31 step:24217 [D loss: 0.001283, acc.: 100.00%] [G loss: 0.077378]\n",
      "epoch:31 step:24218 [D loss: 0.002732, acc.: 100.00%] [G loss: 0.065118]\n",
      "epoch:31 step:24219 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.263735]\n",
      "epoch:31 step:24220 [D loss: 0.009105, acc.: 100.00%] [G loss: 0.004940]\n",
      "epoch:31 step:24221 [D loss: 0.001335, acc.: 100.00%] [G loss: 0.022404]\n",
      "epoch:31 step:24222 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.015639]\n",
      "epoch:31 step:24223 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.003915]\n",
      "epoch:31 step:24224 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.017420]\n",
      "epoch:31 step:24225 [D loss: 0.001187, acc.: 100.00%] [G loss: 0.003625]\n",
      "epoch:31 step:24226 [D loss: 0.002552, acc.: 100.00%] [G loss: 1.707232]\n",
      "epoch:31 step:24227 [D loss: 0.005633, acc.: 100.00%] [G loss: 0.002278]\n",
      "epoch:31 step:24228 [D loss: 0.025214, acc.: 100.00%] [G loss: 0.001113]\n",
      "epoch:31 step:24229 [D loss: 0.001939, acc.: 100.00%] [G loss: 0.082532]\n",
      "epoch:31 step:24230 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.023768]\n",
      "epoch:31 step:24231 [D loss: 0.000754, acc.: 100.00%] [G loss: 0.002401]\n",
      "epoch:31 step:24232 [D loss: 0.002503, acc.: 100.00%] [G loss: 0.832656]\n",
      "epoch:31 step:24233 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.006036]\n",
      "epoch:31 step:24234 [D loss: 0.000617, acc.: 100.00%] [G loss: 0.000303]\n",
      "epoch:31 step:24235 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.064514]\n",
      "epoch:31 step:24236 [D loss: 0.004804, acc.: 100.00%] [G loss: 0.000547]\n",
      "epoch:31 step:24237 [D loss: 0.002181, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:31 step:24238 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:31 step:24239 [D loss: 0.001988, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:31 step:24240 [D loss: 0.005313, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:31 step:24241 [D loss: 0.000941, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:31 step:24242 [D loss: 0.025856, acc.: 100.00%] [G loss: 0.000377]\n",
      "epoch:31 step:24243 [D loss: 0.001571, acc.: 100.00%] [G loss: 0.005134]\n",
      "epoch:31 step:24244 [D loss: 0.003717, acc.: 100.00%] [G loss: 0.001326]\n",
      "epoch:31 step:24245 [D loss: 0.407359, acc.: 81.25%] [G loss: 6.443453]\n",
      "epoch:31 step:24246 [D loss: 1.696773, acc.: 56.25%] [G loss: 0.010192]\n",
      "epoch:31 step:24247 [D loss: 0.378741, acc.: 83.59%] [G loss: 7.609373]\n",
      "epoch:31 step:24248 [D loss: 0.006917, acc.: 100.00%] [G loss: 7.047109]\n",
      "epoch:31 step:24249 [D loss: 0.919394, acc.: 69.53%] [G loss: 3.080832]\n",
      "epoch:31 step:24250 [D loss: 0.014902, acc.: 100.00%] [G loss: 2.884631]\n",
      "epoch:31 step:24251 [D loss: 0.026707, acc.: 99.22%] [G loss: 0.905498]\n",
      "epoch:31 step:24252 [D loss: 0.000999, acc.: 100.00%] [G loss: 0.000833]\n",
      "epoch:31 step:24253 [D loss: 0.072789, acc.: 99.22%] [G loss: 0.097270]\n",
      "epoch:31 step:24254 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.036344]\n",
      "epoch:31 step:24255 [D loss: 0.014610, acc.: 100.00%] [G loss: 0.472256]\n",
      "epoch:31 step:24256 [D loss: 0.000785, acc.: 100.00%] [G loss: 0.279304]\n",
      "epoch:31 step:24257 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.125812]\n",
      "epoch:31 step:24258 [D loss: 0.000567, acc.: 100.00%] [G loss: 0.012202]\n",
      "epoch:31 step:24259 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.038404]\n",
      "epoch:31 step:24260 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.007901]\n",
      "epoch:31 step:24261 [D loss: 0.000294, acc.: 100.00%] [G loss: 2.876506]\n",
      "epoch:31 step:24262 [D loss: 0.000458, acc.: 100.00%] [G loss: 0.007827]\n",
      "epoch:31 step:24263 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.096775]\n",
      "epoch:31 step:24264 [D loss: 0.035953, acc.: 99.22%] [G loss: 0.032991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24265 [D loss: 0.006158, acc.: 100.00%] [G loss: 1.230790]\n",
      "epoch:31 step:24266 [D loss: 0.001849, acc.: 100.00%] [G loss: 0.045968]\n",
      "epoch:31 step:24267 [D loss: 0.060846, acc.: 98.44%] [G loss: 0.115012]\n",
      "epoch:31 step:24268 [D loss: 0.002299, acc.: 100.00%] [G loss: 0.170543]\n",
      "epoch:31 step:24269 [D loss: 0.027120, acc.: 99.22%] [G loss: 1.570799]\n",
      "epoch:31 step:24270 [D loss: 0.003050, acc.: 100.00%] [G loss: 0.055661]\n",
      "epoch:31 step:24271 [D loss: 0.040101, acc.: 99.22%] [G loss: 0.052445]\n",
      "epoch:31 step:24272 [D loss: 0.015770, acc.: 100.00%] [G loss: 0.095555]\n",
      "epoch:31 step:24273 [D loss: 0.010325, acc.: 100.00%] [G loss: 0.991521]\n",
      "epoch:31 step:24274 [D loss: 0.005917, acc.: 100.00%] [G loss: 0.119847]\n",
      "epoch:31 step:24275 [D loss: 0.051191, acc.: 99.22%] [G loss: 0.958366]\n",
      "epoch:31 step:24276 [D loss: 1.156480, acc.: 57.03%] [G loss: 4.989232]\n",
      "epoch:31 step:24277 [D loss: 0.262659, acc.: 88.28%] [G loss: 5.775931]\n",
      "epoch:31 step:24278 [D loss: 0.004870, acc.: 100.00%] [G loss: 3.687271]\n",
      "epoch:31 step:24279 [D loss: 0.049213, acc.: 98.44%] [G loss: 3.975585]\n",
      "epoch:31 step:24280 [D loss: 0.005615, acc.: 100.00%] [G loss: 0.811259]\n",
      "epoch:31 step:24281 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.476992]\n",
      "epoch:31 step:24282 [D loss: 0.001216, acc.: 100.00%] [G loss: 0.472551]\n",
      "epoch:31 step:24283 [D loss: 0.004659, acc.: 100.00%] [G loss: 0.032047]\n",
      "epoch:31 step:24284 [D loss: 0.001382, acc.: 100.00%] [G loss: 0.025011]\n",
      "epoch:31 step:24285 [D loss: 0.000710, acc.: 100.00%] [G loss: 0.174690]\n",
      "epoch:31 step:24286 [D loss: 0.002017, acc.: 100.00%] [G loss: 6.767555]\n",
      "epoch:31 step:24287 [D loss: 0.003659, acc.: 100.00%] [G loss: 0.019205]\n",
      "epoch:31 step:24288 [D loss: 0.000839, acc.: 100.00%] [G loss: 0.055207]\n",
      "epoch:31 step:24289 [D loss: 0.005389, acc.: 100.00%] [G loss: 0.029023]\n",
      "epoch:31 step:24290 [D loss: 0.053417, acc.: 98.44%] [G loss: 0.067761]\n",
      "epoch:31 step:24291 [D loss: 0.015711, acc.: 99.22%] [G loss: 0.078455]\n",
      "epoch:31 step:24292 [D loss: 0.007316, acc.: 100.00%] [G loss: 0.025178]\n",
      "epoch:31 step:24293 [D loss: 0.002525, acc.: 100.00%] [G loss: 0.021950]\n",
      "epoch:31 step:24294 [D loss: 0.002324, acc.: 100.00%] [G loss: 0.006426]\n",
      "epoch:31 step:24295 [D loss: 0.000455, acc.: 100.00%] [G loss: 0.026622]\n",
      "epoch:31 step:24296 [D loss: 0.003942, acc.: 100.00%] [G loss: 0.037452]\n",
      "epoch:31 step:24297 [D loss: 0.000957, acc.: 100.00%] [G loss: 2.587458]\n",
      "epoch:31 step:24298 [D loss: 0.003620, acc.: 100.00%] [G loss: 0.003899]\n",
      "epoch:31 step:24299 [D loss: 0.002735, acc.: 100.00%] [G loss: 0.001865]\n",
      "epoch:31 step:24300 [D loss: 0.005966, acc.: 100.00%] [G loss: 0.013883]\n",
      "epoch:31 step:24301 [D loss: 0.370099, acc.: 79.69%] [G loss: 3.274567]\n",
      "epoch:31 step:24302 [D loss: 0.142901, acc.: 92.97%] [G loss: 2.917726]\n",
      "epoch:31 step:24303 [D loss: 0.026817, acc.: 98.44%] [G loss: 1.976891]\n",
      "epoch:31 step:24304 [D loss: 0.001328, acc.: 100.00%] [G loss: 0.876766]\n",
      "epoch:31 step:24305 [D loss: 0.002133, acc.: 100.00%] [G loss: 0.727047]\n",
      "epoch:31 step:24306 [D loss: 0.004993, acc.: 100.00%] [G loss: 0.572094]\n",
      "epoch:31 step:24307 [D loss: 0.015280, acc.: 99.22%] [G loss: 0.167349]\n",
      "epoch:31 step:24308 [D loss: 0.016245, acc.: 99.22%] [G loss: 0.020699]\n",
      "epoch:31 step:24309 [D loss: 0.008053, acc.: 100.00%] [G loss: 0.005241]\n",
      "epoch:31 step:24310 [D loss: 0.006289, acc.: 100.00%] [G loss: 0.004133]\n",
      "epoch:31 step:24311 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.007384]\n",
      "epoch:31 step:24312 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.002266]\n",
      "epoch:31 step:24313 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.014562]\n",
      "epoch:31 step:24314 [D loss: 0.000939, acc.: 100.00%] [G loss: 0.001833]\n",
      "epoch:31 step:24315 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.001254]\n",
      "epoch:31 step:24316 [D loss: 0.001575, acc.: 100.00%] [G loss: 0.000971]\n",
      "epoch:31 step:24317 [D loss: 0.001058, acc.: 100.00%] [G loss: 0.002341]\n",
      "epoch:31 step:24318 [D loss: 0.093631, acc.: 96.09%] [G loss: 0.033066]\n",
      "epoch:31 step:24319 [D loss: 0.003371, acc.: 100.00%] [G loss: 0.623133]\n",
      "epoch:31 step:24320 [D loss: 0.004458, acc.: 100.00%] [G loss: 0.433954]\n",
      "epoch:31 step:24321 [D loss: 0.011465, acc.: 100.00%] [G loss: 0.289126]\n",
      "epoch:31 step:24322 [D loss: 0.736738, acc.: 67.19%] [G loss: 5.329190]\n",
      "epoch:31 step:24323 [D loss: 0.093490, acc.: 94.53%] [G loss: 6.377839]\n",
      "epoch:31 step:24324 [D loss: 0.195473, acc.: 90.62%] [G loss: 0.463235]\n",
      "epoch:31 step:24325 [D loss: 0.091646, acc.: 96.88%] [G loss: 3.239118]\n",
      "epoch:31 step:24326 [D loss: 0.001156, acc.: 100.00%] [G loss: 1.203408]\n",
      "epoch:31 step:24327 [D loss: 0.013110, acc.: 100.00%] [G loss: 1.465652]\n",
      "epoch:31 step:24328 [D loss: 0.040633, acc.: 98.44%] [G loss: 1.853527]\n",
      "epoch:31 step:24329 [D loss: 0.048400, acc.: 99.22%] [G loss: 2.617916]\n",
      "epoch:31 step:24330 [D loss: 0.008610, acc.: 100.00%] [G loss: 0.051738]\n",
      "epoch:31 step:24331 [D loss: 0.004336, acc.: 100.00%] [G loss: 0.049908]\n",
      "epoch:31 step:24332 [D loss: 0.001072, acc.: 100.00%] [G loss: 1.660478]\n",
      "epoch:31 step:24333 [D loss: 0.002344, acc.: 100.00%] [G loss: 3.288563]\n",
      "epoch:31 step:24334 [D loss: 0.005024, acc.: 100.00%] [G loss: 1.789682]\n",
      "epoch:31 step:24335 [D loss: 0.006560, acc.: 100.00%] [G loss: 0.021803]\n",
      "epoch:31 step:24336 [D loss: 0.010377, acc.: 100.00%] [G loss: 1.603881]\n",
      "epoch:31 step:24337 [D loss: 0.015596, acc.: 100.00%] [G loss: 0.723103]\n",
      "epoch:31 step:24338 [D loss: 0.005006, acc.: 100.00%] [G loss: 0.509943]\n",
      "epoch:31 step:24339 [D loss: 0.002187, acc.: 100.00%] [G loss: 0.259168]\n",
      "epoch:31 step:24340 [D loss: 0.001567, acc.: 100.00%] [G loss: 1.851385]\n",
      "epoch:31 step:24341 [D loss: 0.009235, acc.: 100.00%] [G loss: 0.130391]\n",
      "epoch:31 step:24342 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.151997]\n",
      "epoch:31 step:24343 [D loss: 0.071586, acc.: 97.66%] [G loss: 0.309072]\n",
      "epoch:31 step:24344 [D loss: 0.007511, acc.: 100.00%] [G loss: 1.535650]\n",
      "epoch:31 step:24345 [D loss: 0.006731, acc.: 100.00%] [G loss: 0.725512]\n",
      "epoch:31 step:24346 [D loss: 0.143377, acc.: 93.75%] [G loss: 0.011678]\n",
      "epoch:31 step:24347 [D loss: 0.000686, acc.: 100.00%] [G loss: 0.002517]\n",
      "epoch:31 step:24348 [D loss: 0.233903, acc.: 89.84%] [G loss: 2.732636]\n",
      "epoch:31 step:24349 [D loss: 0.012187, acc.: 100.00%] [G loss: 5.306641]\n",
      "epoch:31 step:24350 [D loss: 0.223021, acc.: 92.97%] [G loss: 0.572535]\n",
      "epoch:31 step:24351 [D loss: 0.092956, acc.: 98.44%] [G loss: 0.121736]\n",
      "epoch:31 step:24352 [D loss: 0.007715, acc.: 100.00%] [G loss: 0.018908]\n",
      "epoch:31 step:24353 [D loss: 0.005626, acc.: 100.00%] [G loss: 0.420893]\n",
      "epoch:31 step:24354 [D loss: 0.032323, acc.: 99.22%] [G loss: 0.055884]\n",
      "epoch:31 step:24355 [D loss: 0.002665, acc.: 100.00%] [G loss: 0.090881]\n",
      "epoch:31 step:24356 [D loss: 0.003462, acc.: 100.00%] [G loss: 0.007710]\n",
      "epoch:31 step:24357 [D loss: 0.000517, acc.: 100.00%] [G loss: 0.009561]\n",
      "epoch:31 step:24358 [D loss: 0.002897, acc.: 100.00%] [G loss: 0.002011]\n",
      "epoch:31 step:24359 [D loss: 0.000605, acc.: 100.00%] [G loss: 0.696401]\n",
      "epoch:31 step:24360 [D loss: 0.000718, acc.: 100.00%] [G loss: 0.004693]\n",
      "epoch:31 step:24361 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.004902]\n",
      "epoch:31 step:24362 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.000864]\n",
      "epoch:31 step:24363 [D loss: 0.000498, acc.: 100.00%] [G loss: 3.056792]\n",
      "epoch:31 step:24364 [D loss: 0.006642, acc.: 100.00%] [G loss: 3.502557]\n",
      "epoch:31 step:24365 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.002082]\n",
      "epoch:31 step:24366 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.030303]\n",
      "epoch:31 step:24367 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.000482]\n",
      "epoch:31 step:24368 [D loss: 0.000692, acc.: 100.00%] [G loss: 0.116426]\n",
      "epoch:31 step:24369 [D loss: 0.001669, acc.: 100.00%] [G loss: 0.003320]\n",
      "epoch:31 step:24370 [D loss: 0.003512, acc.: 100.00%] [G loss: 0.001201]\n",
      "epoch:31 step:24371 [D loss: 0.001707, acc.: 100.00%] [G loss: 0.004094]\n",
      "epoch:31 step:24372 [D loss: 0.000998, acc.: 100.00%] [G loss: 0.008981]\n",
      "epoch:31 step:24373 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.001405]\n",
      "epoch:31 step:24374 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.006986]\n",
      "epoch:31 step:24375 [D loss: 0.001473, acc.: 100.00%] [G loss: 0.063037]\n",
      "epoch:31 step:24376 [D loss: 0.001790, acc.: 100.00%] [G loss: 0.003739]\n",
      "epoch:31 step:24377 [D loss: 0.002443, acc.: 100.00%] [G loss: 0.039858]\n",
      "epoch:31 step:24378 [D loss: 0.001690, acc.: 100.00%] [G loss: 0.015944]\n",
      "epoch:31 step:24379 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.007625]\n",
      "epoch:31 step:24380 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.002680]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24381 [D loss: 0.000659, acc.: 100.00%] [G loss: 0.003769]\n",
      "epoch:31 step:24382 [D loss: 0.001638, acc.: 100.00%] [G loss: 0.001118]\n",
      "epoch:31 step:24383 [D loss: 0.000497, acc.: 100.00%] [G loss: 0.004512]\n",
      "epoch:31 step:24384 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.001496]\n",
      "epoch:31 step:24385 [D loss: 0.002937, acc.: 100.00%] [G loss: 0.000463]\n",
      "epoch:31 step:24386 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.001410]\n",
      "epoch:31 step:24387 [D loss: 0.001970, acc.: 100.00%] [G loss: 0.008046]\n",
      "epoch:31 step:24388 [D loss: 0.002523, acc.: 100.00%] [G loss: 0.002723]\n",
      "epoch:31 step:24389 [D loss: 0.001354, acc.: 100.00%] [G loss: 0.001158]\n",
      "epoch:31 step:24390 [D loss: 0.088757, acc.: 97.66%] [G loss: 0.002072]\n",
      "epoch:31 step:24391 [D loss: 0.011025, acc.: 100.00%] [G loss: 0.007444]\n",
      "epoch:31 step:24392 [D loss: 0.014302, acc.: 100.00%] [G loss: 0.639333]\n",
      "epoch:31 step:24393 [D loss: 0.003175, acc.: 100.00%] [G loss: 0.122566]\n",
      "epoch:31 step:24394 [D loss: 0.000754, acc.: 100.00%] [G loss: 0.104002]\n",
      "epoch:31 step:24395 [D loss: 0.013737, acc.: 99.22%] [G loss: 0.032527]\n",
      "epoch:31 step:24396 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.007581]\n",
      "epoch:31 step:24397 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.007032]\n",
      "epoch:31 step:24398 [D loss: 0.015885, acc.: 100.00%] [G loss: 0.008834]\n",
      "epoch:31 step:24399 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.004977]\n",
      "epoch:31 step:24400 [D loss: 0.004211, acc.: 100.00%] [G loss: 0.001742]\n",
      "epoch:31 step:24401 [D loss: 0.001021, acc.: 100.00%] [G loss: 0.012808]\n",
      "epoch:31 step:24402 [D loss: 0.002808, acc.: 100.00%] [G loss: 0.006982]\n",
      "epoch:31 step:24403 [D loss: 0.001504, acc.: 100.00%] [G loss: 0.001345]\n",
      "epoch:31 step:24404 [D loss: 0.000745, acc.: 100.00%] [G loss: 0.006826]\n",
      "epoch:31 step:24405 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.026802]\n",
      "epoch:31 step:24406 [D loss: 0.001282, acc.: 100.00%] [G loss: 0.000734]\n",
      "epoch:31 step:24407 [D loss: 0.016812, acc.: 100.00%] [G loss: 0.005059]\n",
      "epoch:31 step:24408 [D loss: 0.011162, acc.: 100.00%] [G loss: 0.005860]\n",
      "epoch:31 step:24409 [D loss: 0.002122, acc.: 100.00%] [G loss: 0.002763]\n",
      "epoch:31 step:24410 [D loss: 0.001826, acc.: 100.00%] [G loss: 0.100057]\n",
      "epoch:31 step:24411 [D loss: 0.003817, acc.: 100.00%] [G loss: 0.006970]\n",
      "epoch:31 step:24412 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.007779]\n",
      "epoch:31 step:24413 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.005933]\n",
      "epoch:31 step:24414 [D loss: 0.001275, acc.: 100.00%] [G loss: 0.003237]\n",
      "epoch:31 step:24415 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.004178]\n",
      "epoch:31 step:24416 [D loss: 0.001703, acc.: 100.00%] [G loss: 0.008205]\n",
      "epoch:31 step:24417 [D loss: 0.001844, acc.: 100.00%] [G loss: 0.024727]\n",
      "epoch:31 step:24418 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.008845]\n",
      "epoch:31 step:24419 [D loss: 0.000597, acc.: 100.00%] [G loss: 0.037759]\n",
      "epoch:31 step:24420 [D loss: 0.009393, acc.: 100.00%] [G loss: 0.001383]\n",
      "epoch:31 step:24421 [D loss: 0.002755, acc.: 100.00%] [G loss: 0.002506]\n",
      "epoch:31 step:24422 [D loss: 0.003035, acc.: 100.00%] [G loss: 0.001422]\n",
      "epoch:31 step:24423 [D loss: 0.001076, acc.: 100.00%] [G loss: 0.002246]\n",
      "epoch:31 step:24424 [D loss: 0.002782, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:31 step:24425 [D loss: 0.005232, acc.: 100.00%] [G loss: 0.002619]\n",
      "epoch:31 step:24426 [D loss: 0.004120, acc.: 100.00%] [G loss: 0.002796]\n",
      "epoch:31 step:24427 [D loss: 0.006800, acc.: 100.00%] [G loss: 0.000883]\n",
      "epoch:31 step:24428 [D loss: 0.004663, acc.: 100.00%] [G loss: 0.038541]\n",
      "epoch:31 step:24429 [D loss: 0.006796, acc.: 100.00%] [G loss: 0.001495]\n",
      "epoch:31 step:24430 [D loss: 0.003200, acc.: 100.00%] [G loss: 0.005993]\n",
      "epoch:31 step:24431 [D loss: 0.002737, acc.: 100.00%] [G loss: 0.001514]\n",
      "epoch:31 step:24432 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.001181]\n",
      "epoch:31 step:24433 [D loss: 0.001884, acc.: 100.00%] [G loss: 0.064953]\n",
      "epoch:31 step:24434 [D loss: 0.004037, acc.: 100.00%] [G loss: 0.000941]\n",
      "epoch:31 step:24435 [D loss: 0.002501, acc.: 100.00%] [G loss: 0.002449]\n",
      "epoch:31 step:24436 [D loss: 0.001195, acc.: 100.00%] [G loss: 0.002336]\n",
      "epoch:31 step:24437 [D loss: 0.013616, acc.: 100.00%] [G loss: 0.013305]\n",
      "epoch:31 step:24438 [D loss: 0.015744, acc.: 100.00%] [G loss: 0.077125]\n",
      "epoch:31 step:24439 [D loss: 0.001734, acc.: 100.00%] [G loss: 0.214312]\n",
      "epoch:31 step:24440 [D loss: 0.108435, acc.: 96.88%] [G loss: 0.081532]\n",
      "epoch:31 step:24441 [D loss: 0.011805, acc.: 100.00%] [G loss: 0.233779]\n",
      "epoch:31 step:24442 [D loss: 0.000027, acc.: 100.00%] [G loss: 2.837303]\n",
      "epoch:31 step:24443 [D loss: 0.023150, acc.: 98.44%] [G loss: 0.060019]\n",
      "epoch:31 step:24444 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.620139]\n",
      "epoch:31 step:24445 [D loss: 0.005631, acc.: 100.00%] [G loss: 0.000791]\n",
      "epoch:31 step:24446 [D loss: 0.001031, acc.: 100.00%] [G loss: 0.116030]\n",
      "epoch:31 step:24447 [D loss: 0.006932, acc.: 100.00%] [G loss: 0.075192]\n",
      "epoch:31 step:24448 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:31 step:24449 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.048440]\n",
      "epoch:31 step:24450 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.068055]\n",
      "epoch:31 step:24451 [D loss: 0.005807, acc.: 100.00%] [G loss: 0.000487]\n",
      "epoch:31 step:24452 [D loss: 0.002738, acc.: 100.00%] [G loss: 0.001792]\n",
      "epoch:31 step:24453 [D loss: 0.051484, acc.: 98.44%] [G loss: 0.056391]\n",
      "epoch:31 step:24454 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.044326]\n",
      "epoch:31 step:24455 [D loss: 0.068928, acc.: 98.44%] [G loss: 1.379964]\n",
      "epoch:31 step:24456 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000708]\n",
      "epoch:31 step:24457 [D loss: 0.000585, acc.: 100.00%] [G loss: 0.293265]\n",
      "epoch:31 step:24458 [D loss: 0.021820, acc.: 100.00%] [G loss: 0.000449]\n",
      "epoch:31 step:24459 [D loss: 0.001915, acc.: 100.00%] [G loss: 0.008912]\n",
      "epoch:31 step:24460 [D loss: 0.084395, acc.: 98.44%] [G loss: 2.810419]\n",
      "epoch:31 step:24461 [D loss: 0.096491, acc.: 95.31%] [G loss: 0.035738]\n",
      "epoch:31 step:24462 [D loss: 0.004852, acc.: 100.00%] [G loss: 0.002099]\n",
      "epoch:31 step:24463 [D loss: 0.001475, acc.: 100.00%] [G loss: 1.424709]\n",
      "epoch:31 step:24464 [D loss: 0.000895, acc.: 100.00%] [G loss: 0.781111]\n",
      "epoch:31 step:24465 [D loss: 0.000832, acc.: 100.00%] [G loss: 0.008030]\n",
      "epoch:31 step:24466 [D loss: 0.002063, acc.: 100.00%] [G loss: 0.305496]\n",
      "epoch:31 step:24467 [D loss: 0.008676, acc.: 100.00%] [G loss: 0.086116]\n",
      "epoch:31 step:24468 [D loss: 0.006631, acc.: 100.00%] [G loss: 0.592772]\n",
      "epoch:31 step:24469 [D loss: 0.027151, acc.: 100.00%] [G loss: 0.004722]\n",
      "epoch:31 step:24470 [D loss: 0.000286, acc.: 100.00%] [G loss: 2.314063]\n",
      "epoch:31 step:24471 [D loss: 0.083404, acc.: 96.09%] [G loss: 0.000553]\n",
      "epoch:31 step:24472 [D loss: 0.086999, acc.: 95.31%] [G loss: 0.297869]\n",
      "epoch:31 step:24473 [D loss: 0.002265, acc.: 100.00%] [G loss: 1.183489]\n",
      "epoch:31 step:24474 [D loss: 0.095919, acc.: 96.88%] [G loss: 0.115574]\n",
      "epoch:31 step:24475 [D loss: 0.041486, acc.: 99.22%] [G loss: 0.003478]\n",
      "epoch:31 step:24476 [D loss: 0.014663, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:31 step:24477 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:31 step:24478 [D loss: 0.000109, acc.: 100.00%] [G loss: 2.783975]\n",
      "epoch:31 step:24479 [D loss: 0.000932, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:31 step:24480 [D loss: 0.001953, acc.: 100.00%] [G loss: 0.004933]\n",
      "epoch:31 step:24481 [D loss: 0.041492, acc.: 98.44%] [G loss: 0.001136]\n",
      "epoch:31 step:24482 [D loss: 0.001964, acc.: 100.00%] [G loss: 0.040436]\n",
      "epoch:31 step:24483 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.006432]\n",
      "epoch:31 step:24484 [D loss: 0.011958, acc.: 100.00%] [G loss: 2.775468]\n",
      "epoch:31 step:24485 [D loss: 0.000914, acc.: 100.00%] [G loss: 2.065249]\n",
      "epoch:31 step:24486 [D loss: 0.006733, acc.: 100.00%] [G loss: 0.034633]\n",
      "epoch:31 step:24487 [D loss: 0.015966, acc.: 100.00%] [G loss: 1.184725]\n",
      "epoch:31 step:24488 [D loss: 0.080099, acc.: 97.66%] [G loss: 0.024965]\n",
      "epoch:31 step:24489 [D loss: 0.001178, acc.: 100.00%] [G loss: 0.011706]\n",
      "epoch:31 step:24490 [D loss: 0.106564, acc.: 97.66%] [G loss: 2.243143]\n",
      "epoch:31 step:24491 [D loss: 0.037067, acc.: 99.22%] [G loss: 4.288028]\n",
      "epoch:31 step:24492 [D loss: 0.208832, acc.: 89.84%] [G loss: 0.078341]\n",
      "epoch:31 step:24493 [D loss: 0.015222, acc.: 100.00%] [G loss: 0.104094]\n",
      "epoch:31 step:24494 [D loss: 0.016170, acc.: 100.00%] [G loss: 0.084941]\n",
      "epoch:31 step:24495 [D loss: 0.005654, acc.: 100.00%] [G loss: 0.065782]\n",
      "epoch:31 step:24496 [D loss: 0.001572, acc.: 100.00%] [G loss: 0.065795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24497 [D loss: 0.008355, acc.: 99.22%] [G loss: 0.037592]\n",
      "epoch:31 step:24498 [D loss: 0.000891, acc.: 100.00%] [G loss: 0.010546]\n",
      "epoch:31 step:24499 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.053790]\n",
      "epoch:31 step:24500 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.008683]\n",
      "epoch:31 step:24501 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.058526]\n",
      "epoch:31 step:24502 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.068460]\n",
      "epoch:31 step:24503 [D loss: 0.011074, acc.: 100.00%] [G loss: 0.026109]\n",
      "epoch:31 step:24504 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.019887]\n",
      "epoch:31 step:24505 [D loss: 0.000487, acc.: 100.00%] [G loss: 0.015177]\n",
      "epoch:31 step:24506 [D loss: 0.006925, acc.: 100.00%] [G loss: 0.024269]\n",
      "epoch:31 step:24507 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.014935]\n",
      "epoch:31 step:24508 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.079319]\n",
      "epoch:31 step:24509 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.083008]\n",
      "epoch:31 step:24510 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.005219]\n",
      "epoch:31 step:24511 [D loss: 0.001314, acc.: 100.00%] [G loss: 0.004566]\n",
      "epoch:31 step:24512 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000440]\n",
      "epoch:31 step:24513 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.222316]\n",
      "epoch:31 step:24514 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.005464]\n",
      "epoch:31 step:24515 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000695]\n",
      "epoch:31 step:24516 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.002675]\n",
      "epoch:31 step:24517 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.015253]\n",
      "epoch:31 step:24518 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:31 step:24519 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.018723]\n",
      "epoch:31 step:24520 [D loss: 0.000030, acc.: 100.00%] [G loss: 1.981046]\n",
      "epoch:31 step:24521 [D loss: 0.001950, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:31 step:24522 [D loss: 0.000862, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:31 step:24523 [D loss: 0.001921, acc.: 100.00%] [G loss: 0.000865]\n",
      "epoch:31 step:24524 [D loss: 0.001330, acc.: 100.00%] [G loss: 0.125491]\n",
      "epoch:31 step:24525 [D loss: 0.001349, acc.: 100.00%] [G loss: 0.070054]\n",
      "epoch:31 step:24526 [D loss: 0.344999, acc.: 85.94%] [G loss: 0.000159]\n",
      "epoch:31 step:24527 [D loss: 0.119168, acc.: 95.31%] [G loss: 2.524706]\n",
      "epoch:31 step:24528 [D loss: 0.092967, acc.: 95.31%] [G loss: 0.017871]\n",
      "epoch:31 step:24529 [D loss: 0.004426, acc.: 100.00%] [G loss: 0.002889]\n",
      "epoch:31 step:24530 [D loss: 0.005899, acc.: 100.00%] [G loss: 2.355783]\n",
      "epoch:31 step:24531 [D loss: 0.003223, acc.: 100.00%] [G loss: 0.000975]\n",
      "epoch:31 step:24532 [D loss: 0.002466, acc.: 100.00%] [G loss: 0.003965]\n",
      "epoch:31 step:24533 [D loss: 0.000886, acc.: 100.00%] [G loss: 0.004937]\n",
      "epoch:31 step:24534 [D loss: 0.001957, acc.: 100.00%] [G loss: 0.000656]\n",
      "epoch:31 step:24535 [D loss: 0.006603, acc.: 100.00%] [G loss: 1.081095]\n",
      "epoch:31 step:24536 [D loss: 0.003742, acc.: 100.00%] [G loss: 0.000716]\n",
      "epoch:31 step:24537 [D loss: 0.004319, acc.: 100.00%] [G loss: 0.005194]\n",
      "epoch:31 step:24538 [D loss: 0.003265, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:31 step:24539 [D loss: 0.028017, acc.: 100.00%] [G loss: 1.615100]\n",
      "epoch:31 step:24540 [D loss: 0.000990, acc.: 100.00%] [G loss: 0.019201]\n",
      "epoch:31 step:24541 [D loss: 0.008899, acc.: 100.00%] [G loss: 0.004093]\n",
      "epoch:31 step:24542 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.582317]\n",
      "epoch:31 step:24543 [D loss: 0.000945, acc.: 100.00%] [G loss: 0.024670]\n",
      "epoch:31 step:24544 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.233656]\n",
      "epoch:31 step:24545 [D loss: 0.017683, acc.: 100.00%] [G loss: 0.005450]\n",
      "epoch:31 step:24546 [D loss: 0.019634, acc.: 100.00%] [G loss: 0.022507]\n",
      "epoch:31 step:24547 [D loss: 0.010791, acc.: 99.22%] [G loss: 0.071583]\n",
      "epoch:31 step:24548 [D loss: 0.020756, acc.: 99.22%] [G loss: 0.074760]\n",
      "epoch:31 step:24549 [D loss: 0.016139, acc.: 100.00%] [G loss: 0.002250]\n",
      "epoch:31 step:24550 [D loss: 0.054264, acc.: 98.44%] [G loss: 0.320051]\n",
      "epoch:31 step:24551 [D loss: 0.042925, acc.: 99.22%] [G loss: 0.253043]\n",
      "epoch:31 step:24552 [D loss: 0.007929, acc.: 100.00%] [G loss: 0.356112]\n",
      "epoch:31 step:24553 [D loss: 0.027732, acc.: 99.22%] [G loss: 3.512737]\n",
      "epoch:31 step:24554 [D loss: 0.150275, acc.: 96.88%] [G loss: 8.057796]\n",
      "epoch:31 step:24555 [D loss: 0.107391, acc.: 96.09%] [G loss: 4.316141]\n",
      "epoch:31 step:24556 [D loss: 0.024948, acc.: 99.22%] [G loss: 5.192851]\n",
      "epoch:31 step:24557 [D loss: 0.051752, acc.: 99.22%] [G loss: 3.303607]\n",
      "epoch:31 step:24558 [D loss: 0.006143, acc.: 100.00%] [G loss: 3.878850]\n",
      "epoch:31 step:24559 [D loss: 0.016974, acc.: 100.00%] [G loss: 3.695696]\n",
      "epoch:31 step:24560 [D loss: 0.008947, acc.: 100.00%] [G loss: 3.002136]\n",
      "epoch:31 step:24561 [D loss: 1.769345, acc.: 49.22%] [G loss: 9.397155]\n",
      "epoch:31 step:24562 [D loss: 3.427447, acc.: 49.22%] [G loss: 4.673376]\n",
      "epoch:31 step:24563 [D loss: 0.696399, acc.: 73.44%] [G loss: 0.789310]\n",
      "epoch:31 step:24564 [D loss: 0.671188, acc.: 76.56%] [G loss: 4.442741]\n",
      "epoch:31 step:24565 [D loss: 0.126853, acc.: 95.31%] [G loss: 4.689219]\n",
      "epoch:31 step:24566 [D loss: 0.148790, acc.: 92.19%] [G loss: 3.255571]\n",
      "epoch:31 step:24567 [D loss: 0.266721, acc.: 85.94%] [G loss: 0.995578]\n",
      "epoch:31 step:24568 [D loss: 0.005888, acc.: 100.00%] [G loss: 0.370936]\n",
      "epoch:31 step:24569 [D loss: 0.159059, acc.: 92.97%] [G loss: 1.914649]\n",
      "epoch:31 step:24570 [D loss: 0.015626, acc.: 100.00%] [G loss: 2.473153]\n",
      "epoch:31 step:24571 [D loss: 0.008485, acc.: 100.00%] [G loss: 0.030682]\n",
      "epoch:31 step:24572 [D loss: 0.081219, acc.: 96.88%] [G loss: 0.678476]\n",
      "epoch:31 step:24573 [D loss: 0.002506, acc.: 100.00%] [G loss: 0.623310]\n",
      "epoch:31 step:24574 [D loss: 0.014790, acc.: 100.00%] [G loss: 0.429131]\n",
      "epoch:31 step:24575 [D loss: 0.009339, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:31 step:24576 [D loss: 0.007994, acc.: 100.00%] [G loss: 0.308565]\n",
      "epoch:31 step:24577 [D loss: 0.003427, acc.: 100.00%] [G loss: 0.157383]\n",
      "epoch:31 step:24578 [D loss: 0.002623, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:31 step:24579 [D loss: 0.010776, acc.: 100.00%] [G loss: 0.154193]\n",
      "epoch:31 step:24580 [D loss: 0.002415, acc.: 100.00%] [G loss: 0.160800]\n",
      "epoch:31 step:24581 [D loss: 0.018288, acc.: 99.22%] [G loss: 0.055313]\n",
      "epoch:31 step:24582 [D loss: 0.004501, acc.: 100.00%] [G loss: 0.014300]\n",
      "epoch:31 step:24583 [D loss: 0.004515, acc.: 100.00%] [G loss: 0.028262]\n",
      "epoch:31 step:24584 [D loss: 0.050038, acc.: 99.22%] [G loss: 0.126309]\n",
      "epoch:31 step:24585 [D loss: 0.063353, acc.: 99.22%] [G loss: 0.161522]\n",
      "epoch:31 step:24586 [D loss: 0.059014, acc.: 99.22%] [G loss: 0.093639]\n",
      "epoch:31 step:24587 [D loss: 0.003611, acc.: 100.00%] [G loss: 0.057312]\n",
      "epoch:31 step:24588 [D loss: 0.083811, acc.: 99.22%] [G loss: 1.562669]\n",
      "epoch:31 step:24589 [D loss: 0.004645, acc.: 100.00%] [G loss: 4.119073]\n",
      "epoch:31 step:24590 [D loss: 0.021883, acc.: 99.22%] [G loss: 2.267759]\n",
      "epoch:31 step:24591 [D loss: 0.065118, acc.: 97.66%] [G loss: 0.546778]\n",
      "epoch:31 step:24592 [D loss: 0.672855, acc.: 67.97%] [G loss: 7.968520]\n",
      "epoch:31 step:24593 [D loss: 0.573688, acc.: 75.00%] [G loss: 7.595075]\n",
      "epoch:31 step:24594 [D loss: 0.175320, acc.: 90.62%] [G loss: 5.750560]\n",
      "epoch:31 step:24595 [D loss: 0.025416, acc.: 98.44%] [G loss: 3.911785]\n",
      "epoch:31 step:24596 [D loss: 0.047234, acc.: 97.66%] [G loss: 0.203156]\n",
      "epoch:31 step:24597 [D loss: 0.038110, acc.: 99.22%] [G loss: 4.935555]\n",
      "epoch:31 step:24598 [D loss: 0.042976, acc.: 99.22%] [G loss: 4.608139]\n",
      "epoch:31 step:24599 [D loss: 0.182105, acc.: 92.19%] [G loss: 4.269456]\n",
      "epoch:31 step:24600 [D loss: 0.008671, acc.: 100.00%] [G loss: 4.699894]\n",
      "epoch:31 step:24601 [D loss: 0.017871, acc.: 100.00%] [G loss: 4.449923]\n",
      "epoch:31 step:24602 [D loss: 0.009893, acc.: 100.00%] [G loss: 0.031993]\n",
      "epoch:31 step:24603 [D loss: 0.034904, acc.: 100.00%] [G loss: 5.448362]\n",
      "epoch:31 step:24604 [D loss: 0.006127, acc.: 100.00%] [G loss: 4.716887]\n",
      "epoch:31 step:24605 [D loss: 0.018262, acc.: 100.00%] [G loss: 0.037913]\n",
      "epoch:31 step:24606 [D loss: 0.011385, acc.: 100.00%] [G loss: 4.300273]\n",
      "epoch:31 step:24607 [D loss: 0.023233, acc.: 100.00%] [G loss: 6.433207]\n",
      "epoch:31 step:24608 [D loss: 0.054228, acc.: 98.44%] [G loss: 0.137824]\n",
      "epoch:31 step:24609 [D loss: 0.227547, acc.: 88.28%] [G loss: 3.416888]\n",
      "epoch:31 step:24610 [D loss: 0.265291, acc.: 85.94%] [G loss: 6.031193]\n",
      "epoch:31 step:24611 [D loss: 0.034500, acc.: 98.44%] [G loss: 6.822228]\n",
      "epoch:31 step:24612 [D loss: 0.342495, acc.: 83.59%] [G loss: 4.028925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24613 [D loss: 0.044010, acc.: 100.00%] [G loss: 2.637225]\n",
      "epoch:31 step:24614 [D loss: 0.037944, acc.: 100.00%] [G loss: 2.712209]\n",
      "epoch:31 step:24615 [D loss: 0.051284, acc.: 98.44%] [G loss: 2.754885]\n",
      "epoch:31 step:24616 [D loss: 0.135522, acc.: 93.75%] [G loss: 4.497336]\n",
      "epoch:31 step:24617 [D loss: 0.007479, acc.: 100.00%] [G loss: 5.244209]\n",
      "epoch:31 step:24618 [D loss: 0.009610, acc.: 100.00%] [G loss: 5.046184]\n",
      "epoch:31 step:24619 [D loss: 0.037324, acc.: 100.00%] [G loss: 5.886423]\n",
      "epoch:31 step:24620 [D loss: 0.047819, acc.: 99.22%] [G loss: 0.746920]\n",
      "epoch:31 step:24621 [D loss: 0.013805, acc.: 100.00%] [G loss: 5.969037]\n",
      "epoch:31 step:24622 [D loss: 0.250744, acc.: 89.06%] [G loss: 1.922243]\n",
      "epoch:31 step:24623 [D loss: 0.291667, acc.: 85.94%] [G loss: 4.602941]\n",
      "epoch:31 step:24624 [D loss: 0.005656, acc.: 100.00%] [G loss: 1.424852]\n",
      "epoch:31 step:24625 [D loss: 0.126530, acc.: 95.31%] [G loss: 5.781001]\n",
      "epoch:31 step:24626 [D loss: 0.045900, acc.: 97.66%] [G loss: 0.247646]\n",
      "epoch:31 step:24627 [D loss: 0.008119, acc.: 100.00%] [G loss: 3.240397]\n",
      "epoch:31 step:24628 [D loss: 0.010950, acc.: 100.00%] [G loss: 0.114077]\n",
      "epoch:31 step:24629 [D loss: 0.001705, acc.: 100.00%] [G loss: 0.007334]\n",
      "epoch:31 step:24630 [D loss: 0.014850, acc.: 100.00%] [G loss: 2.575509]\n",
      "epoch:31 step:24631 [D loss: 0.025840, acc.: 99.22%] [G loss: 1.648315]\n",
      "epoch:31 step:24632 [D loss: 0.007413, acc.: 100.00%] [G loss: 0.007216]\n",
      "epoch:31 step:24633 [D loss: 0.121981, acc.: 94.53%] [G loss: 1.412607]\n",
      "epoch:31 step:24634 [D loss: 0.001278, acc.: 100.00%] [G loss: 1.895717]\n",
      "epoch:31 step:24635 [D loss: 0.056606, acc.: 98.44%] [G loss: 0.469395]\n",
      "epoch:31 step:24636 [D loss: 0.007598, acc.: 100.00%] [G loss: 0.127839]\n",
      "epoch:31 step:24637 [D loss: 0.024817, acc.: 99.22%] [G loss: 0.070289]\n",
      "epoch:31 step:24638 [D loss: 0.006984, acc.: 100.00%] [G loss: 0.038014]\n",
      "epoch:31 step:24639 [D loss: 0.071928, acc.: 96.88%] [G loss: 0.152950]\n",
      "epoch:31 step:24640 [D loss: 0.025154, acc.: 99.22%] [G loss: 0.621287]\n",
      "epoch:31 step:24641 [D loss: 0.697964, acc.: 67.19%] [G loss: 0.355084]\n",
      "epoch:31 step:24642 [D loss: 0.079665, acc.: 97.66%] [G loss: 6.267581]\n",
      "epoch:31 step:24643 [D loss: 0.066483, acc.: 98.44%] [G loss: 5.324954]\n",
      "epoch:31 step:24644 [D loss: 0.001949, acc.: 100.00%] [G loss: 4.239724]\n",
      "epoch:31 step:24645 [D loss: 0.000642, acc.: 100.00%] [G loss: 1.909550]\n",
      "epoch:31 step:24646 [D loss: 0.001112, acc.: 100.00%] [G loss: 1.489961]\n",
      "epoch:31 step:24647 [D loss: 0.005627, acc.: 100.00%] [G loss: 0.895149]\n",
      "epoch:31 step:24648 [D loss: 0.002123, acc.: 100.00%] [G loss: 0.608199]\n",
      "epoch:31 step:24649 [D loss: 0.017193, acc.: 99.22%] [G loss: 0.371477]\n",
      "epoch:31 step:24650 [D loss: 0.015300, acc.: 100.00%] [G loss: 0.049839]\n",
      "epoch:31 step:24651 [D loss: 0.029186, acc.: 100.00%] [G loss: 0.148751]\n",
      "epoch:31 step:24652 [D loss: 0.030151, acc.: 100.00%] [G loss: 0.504483]\n",
      "epoch:31 step:24653 [D loss: 0.122656, acc.: 95.31%] [G loss: 3.454453]\n",
      "epoch:31 step:24654 [D loss: 0.046693, acc.: 98.44%] [G loss: 3.335957]\n",
      "epoch:31 step:24655 [D loss: 0.078349, acc.: 96.09%] [G loss: 2.120365]\n",
      "epoch:31 step:24656 [D loss: 0.075785, acc.: 98.44%] [G loss: 0.431642]\n",
      "epoch:31 step:24657 [D loss: 0.011509, acc.: 100.00%] [G loss: 1.871152]\n",
      "epoch:31 step:24658 [D loss: 0.057744, acc.: 97.66%] [G loss: 1.931536]\n",
      "epoch:31 step:24659 [D loss: 0.105371, acc.: 96.09%] [G loss: 0.520859]\n",
      "epoch:31 step:24660 [D loss: 0.067872, acc.: 99.22%] [G loss: 1.652739]\n",
      "epoch:31 step:24661 [D loss: 0.020225, acc.: 100.00%] [G loss: 3.493959]\n",
      "epoch:31 step:24662 [D loss: 0.030006, acc.: 100.00%] [G loss: 1.047449]\n",
      "epoch:31 step:24663 [D loss: 0.061315, acc.: 98.44%] [G loss: 3.667766]\n",
      "epoch:31 step:24664 [D loss: 1.100693, acc.: 53.91%] [G loss: 5.711110]\n",
      "epoch:31 step:24665 [D loss: 1.491114, acc.: 60.16%] [G loss: 9.077953]\n",
      "epoch:31 step:24666 [D loss: 0.290860, acc.: 90.62%] [G loss: 7.111062]\n",
      "epoch:31 step:24667 [D loss: 0.005443, acc.: 100.00%] [G loss: 6.056415]\n",
      "epoch:31 step:24668 [D loss: 0.009137, acc.: 100.00%] [G loss: 5.457889]\n",
      "epoch:31 step:24669 [D loss: 0.014201, acc.: 100.00%] [G loss: 4.817548]\n",
      "epoch:31 step:24670 [D loss: 0.117699, acc.: 94.53%] [G loss: 5.721245]\n",
      "epoch:31 step:24671 [D loss: 0.055162, acc.: 97.66%] [G loss: 5.874053]\n",
      "epoch:31 step:24672 [D loss: 0.007332, acc.: 100.00%] [G loss: 0.074264]\n",
      "epoch:31 step:24673 [D loss: 0.009674, acc.: 100.00%] [G loss: 5.469971]\n",
      "epoch:31 step:24674 [D loss: 0.006614, acc.: 100.00%] [G loss: 3.924183]\n",
      "epoch:31 step:24675 [D loss: 0.020481, acc.: 100.00%] [G loss: 2.894669]\n",
      "epoch:31 step:24676 [D loss: 0.032676, acc.: 99.22%] [G loss: 2.882977]\n",
      "epoch:31 step:24677 [D loss: 0.051508, acc.: 100.00%] [G loss: 0.343334]\n",
      "epoch:31 step:24678 [D loss: 0.127571, acc.: 96.09%] [G loss: 2.248280]\n",
      "epoch:31 step:24679 [D loss: 0.019988, acc.: 100.00%] [G loss: 1.924215]\n",
      "epoch:31 step:24680 [D loss: 0.049583, acc.: 99.22%] [G loss: 2.279903]\n",
      "epoch:31 step:24681 [D loss: 0.091156, acc.: 96.88%] [G loss: 0.705215]\n",
      "epoch:31 step:24682 [D loss: 0.092719, acc.: 96.88%] [G loss: 1.235932]\n",
      "epoch:31 step:24683 [D loss: 0.073617, acc.: 97.66%] [G loss: 0.374128]\n",
      "epoch:31 step:24684 [D loss: 0.091972, acc.: 97.66%] [G loss: 0.414058]\n",
      "epoch:31 step:24685 [D loss: 0.383392, acc.: 78.91%] [G loss: 6.783181]\n",
      "epoch:31 step:24686 [D loss: 1.429113, acc.: 58.59%] [G loss: 4.401413]\n",
      "epoch:31 step:24687 [D loss: 0.154072, acc.: 94.53%] [G loss: 3.981275]\n",
      "epoch:31 step:24688 [D loss: 0.022705, acc.: 100.00%] [G loss: 0.808276]\n",
      "epoch:31 step:24689 [D loss: 0.027122, acc.: 100.00%] [G loss: 3.680821]\n",
      "epoch:31 step:24690 [D loss: 0.012645, acc.: 100.00%] [G loss: 2.869565]\n",
      "epoch:31 step:24691 [D loss: 0.054474, acc.: 98.44%] [G loss: 2.443679]\n",
      "epoch:31 step:24692 [D loss: 0.029866, acc.: 99.22%] [G loss: 2.867330]\n",
      "epoch:31 step:24693 [D loss: 0.527691, acc.: 70.31%] [G loss: 7.497569]\n",
      "epoch:31 step:24694 [D loss: 0.539796, acc.: 78.12%] [G loss: 1.934856]\n",
      "epoch:31 step:24695 [D loss: 0.035747, acc.: 99.22%] [G loss: 5.232607]\n",
      "epoch:31 step:24696 [D loss: 0.028275, acc.: 100.00%] [G loss: 4.648600]\n",
      "epoch:31 step:24697 [D loss: 0.045036, acc.: 99.22%] [G loss: 3.608095]\n",
      "epoch:31 step:24698 [D loss: 0.208263, acc.: 88.28%] [G loss: 5.432493]\n",
      "epoch:31 step:24699 [D loss: 0.030731, acc.: 99.22%] [G loss: 5.719286]\n",
      "epoch:31 step:24700 [D loss: 0.106685, acc.: 96.09%] [G loss: 3.486765]\n",
      "epoch:31 step:24701 [D loss: 0.076180, acc.: 98.44%] [G loss: 2.425777]\n",
      "epoch:31 step:24702 [D loss: 0.106237, acc.: 96.88%] [G loss: 2.932755]\n",
      "epoch:31 step:24703 [D loss: 0.068090, acc.: 99.22%] [G loss: 2.607442]\n",
      "epoch:31 step:24704 [D loss: 0.208447, acc.: 91.41%] [G loss: 4.138287]\n",
      "epoch:31 step:24705 [D loss: 0.086700, acc.: 97.66%] [G loss: 4.740516]\n",
      "epoch:31 step:24706 [D loss: 0.063654, acc.: 97.66%] [G loss: 2.195631]\n",
      "epoch:31 step:24707 [D loss: 0.043439, acc.: 98.44%] [G loss: 2.837662]\n",
      "epoch:31 step:24708 [D loss: 0.034047, acc.: 99.22%] [G loss: 0.036559]\n",
      "epoch:31 step:24709 [D loss: 0.038999, acc.: 99.22%] [G loss: 0.300212]\n",
      "epoch:31 step:24710 [D loss: 0.021213, acc.: 100.00%] [G loss: 0.103935]\n",
      "epoch:31 step:24711 [D loss: 0.002501, acc.: 100.00%] [G loss: 0.095134]\n",
      "epoch:31 step:24712 [D loss: 0.005465, acc.: 100.00%] [G loss: 0.796174]\n",
      "epoch:31 step:24713 [D loss: 0.003044, acc.: 100.00%] [G loss: 0.068017]\n",
      "epoch:31 step:24714 [D loss: 0.041049, acc.: 100.00%] [G loss: 0.107277]\n",
      "epoch:31 step:24715 [D loss: 0.002172, acc.: 100.00%] [G loss: 0.030419]\n",
      "epoch:31 step:24716 [D loss: 0.008164, acc.: 100.00%] [G loss: 0.168531]\n",
      "epoch:31 step:24717 [D loss: 0.001159, acc.: 100.00%] [G loss: 2.909738]\n",
      "epoch:31 step:24718 [D loss: 0.011138, acc.: 100.00%] [G loss: 0.050721]\n",
      "epoch:31 step:24719 [D loss: 0.004564, acc.: 100.00%] [G loss: 0.004432]\n",
      "epoch:31 step:24720 [D loss: 0.007618, acc.: 100.00%] [G loss: 0.076025]\n",
      "epoch:31 step:24721 [D loss: 0.052474, acc.: 97.66%] [G loss: 0.079838]\n",
      "epoch:31 step:24722 [D loss: 0.015364, acc.: 100.00%] [G loss: 0.288901]\n",
      "epoch:31 step:24723 [D loss: 0.001503, acc.: 100.00%] [G loss: 0.402293]\n",
      "epoch:31 step:24724 [D loss: 0.034591, acc.: 99.22%] [G loss: 0.189326]\n",
      "epoch:31 step:24725 [D loss: 0.001999, acc.: 100.00%] [G loss: 0.102632]\n",
      "epoch:31 step:24726 [D loss: 0.001812, acc.: 100.00%] [G loss: 0.047946]\n",
      "epoch:31 step:24727 [D loss: 0.001059, acc.: 100.00%] [G loss: 0.006563]\n",
      "epoch:31 step:24728 [D loss: 0.002569, acc.: 100.00%] [G loss: 0.001492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24729 [D loss: 0.003878, acc.: 100.00%] [G loss: 0.006416]\n",
      "epoch:31 step:24730 [D loss: 0.000670, acc.: 100.00%] [G loss: 0.001989]\n",
      "epoch:31 step:24731 [D loss: 0.000708, acc.: 100.00%] [G loss: 0.022942]\n",
      "epoch:31 step:24732 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.002868]\n",
      "epoch:31 step:24733 [D loss: 0.003075, acc.: 100.00%] [G loss: 0.009334]\n",
      "epoch:31 step:24734 [D loss: 0.000809, acc.: 100.00%] [G loss: 0.001531]\n",
      "epoch:31 step:24735 [D loss: 0.008423, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:31 step:24736 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.000449]\n",
      "epoch:31 step:24737 [D loss: 0.022170, acc.: 100.00%] [G loss: 0.000684]\n",
      "epoch:31 step:24738 [D loss: 0.000511, acc.: 100.00%] [G loss: 0.001690]\n",
      "epoch:31 step:24739 [D loss: 0.017624, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:31 step:24740 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.001600]\n",
      "epoch:31 step:24741 [D loss: 0.008581, acc.: 100.00%] [G loss: 0.001244]\n",
      "epoch:31 step:24742 [D loss: 0.000613, acc.: 100.00%] [G loss: 0.000725]\n",
      "epoch:31 step:24743 [D loss: 0.002334, acc.: 100.00%] [G loss: 0.001217]\n",
      "epoch:31 step:24744 [D loss: 0.001662, acc.: 100.00%] [G loss: 0.002745]\n",
      "epoch:31 step:24745 [D loss: 0.000777, acc.: 100.00%] [G loss: 0.000519]\n",
      "epoch:31 step:24746 [D loss: 0.003630, acc.: 100.00%] [G loss: 0.025183]\n",
      "epoch:31 step:24747 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.007060]\n",
      "epoch:31 step:24748 [D loss: 0.040729, acc.: 99.22%] [G loss: 0.000020]\n",
      "epoch:31 step:24749 [D loss: 0.004169, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:31 step:24750 [D loss: 0.098256, acc.: 96.88%] [G loss: 0.011945]\n",
      "epoch:31 step:24751 [D loss: 0.000337, acc.: 100.00%] [G loss: 2.665752]\n",
      "epoch:31 step:24752 [D loss: 0.006548, acc.: 100.00%] [G loss: 1.677080]\n",
      "epoch:31 step:24753 [D loss: 0.686759, acc.: 68.75%] [G loss: 1.248140]\n",
      "epoch:31 step:24754 [D loss: 0.011542, acc.: 100.00%] [G loss: 0.614196]\n",
      "epoch:31 step:24755 [D loss: 0.109546, acc.: 95.31%] [G loss: 1.993153]\n",
      "epoch:31 step:24756 [D loss: 0.019543, acc.: 99.22%] [G loss: 0.456194]\n",
      "epoch:31 step:24757 [D loss: 0.015338, acc.: 99.22%] [G loss: 0.357955]\n",
      "epoch:31 step:24758 [D loss: 0.085384, acc.: 96.88%] [G loss: 0.120086]\n",
      "epoch:31 step:24759 [D loss: 0.087204, acc.: 97.66%] [G loss: 0.352879]\n",
      "epoch:31 step:24760 [D loss: 0.011287, acc.: 100.00%] [G loss: 0.962054]\n",
      "epoch:31 step:24761 [D loss: 0.095176, acc.: 96.88%] [G loss: 0.068485]\n",
      "epoch:31 step:24762 [D loss: 0.011356, acc.: 100.00%] [G loss: 0.038843]\n",
      "epoch:31 step:24763 [D loss: 0.021559, acc.: 100.00%] [G loss: 0.216353]\n",
      "epoch:31 step:24764 [D loss: 0.005014, acc.: 100.00%] [G loss: 0.277556]\n",
      "epoch:31 step:24765 [D loss: 0.007403, acc.: 100.00%] [G loss: 0.213126]\n",
      "epoch:31 step:24766 [D loss: 0.092524, acc.: 98.44%] [G loss: 0.161115]\n",
      "epoch:31 step:24767 [D loss: 0.047183, acc.: 97.66%] [G loss: 2.980419]\n",
      "epoch:31 step:24768 [D loss: 0.102136, acc.: 95.31%] [G loss: 0.291346]\n",
      "epoch:31 step:24769 [D loss: 0.007194, acc.: 100.00%] [G loss: 0.185105]\n",
      "epoch:31 step:24770 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.007811]\n",
      "epoch:31 step:24771 [D loss: 0.001150, acc.: 100.00%] [G loss: 0.006535]\n",
      "epoch:31 step:24772 [D loss: 0.000617, acc.: 100.00%] [G loss: 0.012840]\n",
      "epoch:31 step:24773 [D loss: 0.023704, acc.: 99.22%] [G loss: 0.080272]\n",
      "epoch:31 step:24774 [D loss: 0.002670, acc.: 100.00%] [G loss: 0.044336]\n",
      "epoch:31 step:24775 [D loss: 0.031232, acc.: 99.22%] [G loss: 1.716127]\n",
      "epoch:31 step:24776 [D loss: 0.006435, acc.: 100.00%] [G loss: 0.421895]\n",
      "epoch:31 step:24777 [D loss: 0.032445, acc.: 98.44%] [G loss: 0.014754]\n",
      "epoch:31 step:24778 [D loss: 0.025718, acc.: 99.22%] [G loss: 0.218011]\n",
      "epoch:31 step:24779 [D loss: 0.001740, acc.: 100.00%] [G loss: 0.361719]\n",
      "epoch:31 step:24780 [D loss: 0.074434, acc.: 96.88%] [G loss: 1.747364]\n",
      "epoch:31 step:24781 [D loss: 0.003968, acc.: 100.00%] [G loss: 1.391104]\n",
      "epoch:31 step:24782 [D loss: 0.257574, acc.: 91.41%] [G loss: 0.012954]\n",
      "epoch:31 step:24783 [D loss: 0.022647, acc.: 99.22%] [G loss: 0.273029]\n",
      "epoch:31 step:24784 [D loss: 0.016123, acc.: 100.00%] [G loss: 0.266398]\n",
      "epoch:31 step:24785 [D loss: 0.006896, acc.: 100.00%] [G loss: 0.105102]\n",
      "epoch:31 step:24786 [D loss: 0.245508, acc.: 87.50%] [G loss: 1.412161]\n",
      "epoch:31 step:24787 [D loss: 0.789568, acc.: 67.19%] [G loss: 1.259250]\n",
      "epoch:31 step:24788 [D loss: 0.015508, acc.: 99.22%] [G loss: 0.688628]\n",
      "epoch:31 step:24789 [D loss: 0.007844, acc.: 100.00%] [G loss: 0.378564]\n",
      "epoch:31 step:24790 [D loss: 0.001249, acc.: 100.00%] [G loss: 0.328346]\n",
      "epoch:31 step:24791 [D loss: 0.041923, acc.: 98.44%] [G loss: 0.396104]\n",
      "epoch:31 step:24792 [D loss: 0.010627, acc.: 100.00%] [G loss: 0.376802]\n",
      "epoch:31 step:24793 [D loss: 0.015514, acc.: 100.00%] [G loss: 0.081288]\n",
      "epoch:31 step:24794 [D loss: 0.018134, acc.: 100.00%] [G loss: 0.146367]\n",
      "epoch:31 step:24795 [D loss: 0.012007, acc.: 100.00%] [G loss: 0.569351]\n",
      "epoch:31 step:24796 [D loss: 0.006873, acc.: 100.00%] [G loss: 0.779652]\n",
      "epoch:31 step:24797 [D loss: 0.022274, acc.: 100.00%] [G loss: 0.258099]\n",
      "epoch:31 step:24798 [D loss: 0.084790, acc.: 100.00%] [G loss: 0.145096]\n",
      "epoch:31 step:24799 [D loss: 0.131195, acc.: 94.53%] [G loss: 0.866834]\n",
      "epoch:31 step:24800 [D loss: 0.001987, acc.: 100.00%] [G loss: 0.529899]\n",
      "epoch:31 step:24801 [D loss: 0.004268, acc.: 100.00%] [G loss: 0.078093]\n",
      "epoch:31 step:24802 [D loss: 0.003389, acc.: 100.00%] [G loss: 0.481097]\n",
      "epoch:31 step:24803 [D loss: 0.016370, acc.: 100.00%] [G loss: 0.020150]\n",
      "epoch:31 step:24804 [D loss: 0.003268, acc.: 100.00%] [G loss: 0.056311]\n",
      "epoch:31 step:24805 [D loss: 0.004978, acc.: 100.00%] [G loss: 0.097475]\n",
      "epoch:31 step:24806 [D loss: 0.004721, acc.: 100.00%] [G loss: 0.029213]\n",
      "epoch:31 step:24807 [D loss: 0.005319, acc.: 100.00%] [G loss: 0.104787]\n",
      "epoch:31 step:24808 [D loss: 0.023833, acc.: 100.00%] [G loss: 0.005082]\n",
      "epoch:31 step:24809 [D loss: 0.002278, acc.: 100.00%] [G loss: 0.006980]\n",
      "epoch:31 step:24810 [D loss: 0.003165, acc.: 100.00%] [G loss: 0.109009]\n",
      "epoch:31 step:24811 [D loss: 0.008065, acc.: 100.00%] [G loss: 0.034102]\n",
      "epoch:31 step:24812 [D loss: 0.005023, acc.: 100.00%] [G loss: 0.005433]\n",
      "epoch:31 step:24813 [D loss: 0.001449, acc.: 100.00%] [G loss: 0.013347]\n",
      "epoch:31 step:24814 [D loss: 0.001147, acc.: 100.00%] [G loss: 0.004972]\n",
      "epoch:31 step:24815 [D loss: 0.076718, acc.: 96.88%] [G loss: 0.001971]\n",
      "epoch:31 step:24816 [D loss: 0.001618, acc.: 100.00%] [G loss: 0.001966]\n",
      "epoch:31 step:24817 [D loss: 0.011187, acc.: 100.00%] [G loss: 0.010740]\n",
      "epoch:31 step:24818 [D loss: 0.015997, acc.: 100.00%] [G loss: 0.008889]\n",
      "epoch:31 step:24819 [D loss: 0.000808, acc.: 100.00%] [G loss: 0.017031]\n",
      "epoch:31 step:24820 [D loss: 0.006670, acc.: 100.00%] [G loss: 0.004110]\n",
      "epoch:31 step:24821 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.009498]\n",
      "epoch:31 step:24822 [D loss: 0.001662, acc.: 100.00%] [G loss: 0.031125]\n",
      "epoch:31 step:24823 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.007602]\n",
      "epoch:31 step:24824 [D loss: 0.019927, acc.: 100.00%] [G loss: 0.000671]\n",
      "epoch:31 step:24825 [D loss: 0.002441, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:31 step:24826 [D loss: 0.000773, acc.: 100.00%] [G loss: 0.000983]\n",
      "epoch:31 step:24827 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.000420]\n",
      "epoch:31 step:24828 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000543]\n",
      "epoch:31 step:24829 [D loss: 0.004729, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:31 step:24830 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000559]\n",
      "epoch:31 step:24831 [D loss: 0.000548, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:31 step:24832 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000614]\n",
      "epoch:31 step:24833 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.012323]\n",
      "epoch:31 step:24834 [D loss: 0.000902, acc.: 100.00%] [G loss: 0.004026]\n",
      "epoch:31 step:24835 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.002418]\n",
      "epoch:31 step:24836 [D loss: 0.000615, acc.: 100.00%] [G loss: 0.000350]\n",
      "epoch:31 step:24837 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:31 step:24838 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.001883]\n",
      "epoch:31 step:24839 [D loss: 0.017361, acc.: 100.00%] [G loss: 0.248280]\n",
      "epoch:31 step:24840 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.029180]\n",
      "epoch:31 step:24841 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.009181]\n",
      "epoch:31 step:24842 [D loss: 0.005964, acc.: 100.00%] [G loss: 0.045799]\n",
      "epoch:31 step:24843 [D loss: 0.001080, acc.: 100.00%] [G loss: 0.031037]\n",
      "epoch:31 step:24844 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.000909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24845 [D loss: 0.002558, acc.: 100.00%] [G loss: 0.004709]\n",
      "epoch:31 step:24846 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.002582]\n",
      "epoch:31 step:24847 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.003669]\n",
      "epoch:31 step:24848 [D loss: 0.001116, acc.: 100.00%] [G loss: 0.010392]\n",
      "epoch:31 step:24849 [D loss: 0.001552, acc.: 100.00%] [G loss: 0.000952]\n",
      "epoch:31 step:24850 [D loss: 0.009040, acc.: 100.00%] [G loss: 0.041593]\n",
      "epoch:31 step:24851 [D loss: 0.014090, acc.: 100.00%] [G loss: 0.059718]\n",
      "epoch:31 step:24852 [D loss: 0.001016, acc.: 100.00%] [G loss: 0.234481]\n",
      "epoch:31 step:24853 [D loss: 0.020910, acc.: 99.22%] [G loss: 0.088985]\n",
      "epoch:31 step:24854 [D loss: 0.014889, acc.: 99.22%] [G loss: 0.023625]\n",
      "epoch:31 step:24855 [D loss: 0.002219, acc.: 100.00%] [G loss: 0.447106]\n",
      "epoch:31 step:24856 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.002915]\n",
      "epoch:31 step:24857 [D loss: 0.091815, acc.: 96.88%] [G loss: 0.000020]\n",
      "epoch:31 step:24858 [D loss: 0.001815, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:31 step:24859 [D loss: 0.033748, acc.: 99.22%] [G loss: 0.000016]\n",
      "epoch:31 step:24860 [D loss: 0.000760, acc.: 100.00%] [G loss: 0.000456]\n",
      "epoch:31 step:24861 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.001352]\n",
      "epoch:31 step:24862 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:31 step:24863 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:31 step:24864 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:31 step:24865 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:31 step:24866 [D loss: 0.000040, acc.: 100.00%] [G loss: 4.267533]\n",
      "epoch:31 step:24867 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.001643]\n",
      "epoch:31 step:24868 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.093651]\n",
      "epoch:31 step:24869 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.020889]\n",
      "epoch:31 step:24870 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.011812]\n",
      "epoch:31 step:24871 [D loss: 0.003434, acc.: 100.00%] [G loss: 0.001149]\n",
      "epoch:31 step:24872 [D loss: 0.004927, acc.: 100.00%] [G loss: 0.001826]\n",
      "epoch:31 step:24873 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.001203]\n",
      "epoch:31 step:24874 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.009625]\n",
      "epoch:31 step:24875 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.012142]\n",
      "epoch:31 step:24876 [D loss: 0.000713, acc.: 100.00%] [G loss: 0.003464]\n",
      "epoch:31 step:24877 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.001999]\n",
      "epoch:31 step:24878 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.676077]\n",
      "epoch:31 step:24879 [D loss: 0.007150, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:31 step:24880 [D loss: 0.050970, acc.: 99.22%] [G loss: 0.090872]\n",
      "epoch:31 step:24881 [D loss: 0.014773, acc.: 100.00%] [G loss: 0.309760]\n",
      "epoch:31 step:24882 [D loss: 0.061452, acc.: 96.88%] [G loss: 0.002285]\n",
      "epoch:31 step:24883 [D loss: 0.003071, acc.: 100.00%] [G loss: 0.002919]\n",
      "epoch:31 step:24884 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.000343]\n",
      "epoch:31 step:24885 [D loss: 0.009912, acc.: 100.00%] [G loss: 0.002209]\n",
      "epoch:31 step:24886 [D loss: 0.000807, acc.: 100.00%] [G loss: 0.001070]\n",
      "epoch:31 step:24887 [D loss: 0.003089, acc.: 100.00%] [G loss: 0.006295]\n",
      "epoch:31 step:24888 [D loss: 0.006255, acc.: 100.00%] [G loss: 0.013633]\n",
      "epoch:31 step:24889 [D loss: 0.009086, acc.: 100.00%] [G loss: 0.011378]\n",
      "epoch:31 step:24890 [D loss: 0.010722, acc.: 100.00%] [G loss: 0.099378]\n",
      "epoch:31 step:24891 [D loss: 0.007500, acc.: 100.00%] [G loss: 0.357992]\n",
      "epoch:31 step:24892 [D loss: 0.000690, acc.: 100.00%] [G loss: 0.671145]\n",
      "epoch:31 step:24893 [D loss: 0.010376, acc.: 100.00%] [G loss: 2.267632]\n",
      "epoch:31 step:24894 [D loss: 0.004465, acc.: 100.00%] [G loss: 0.075276]\n",
      "epoch:31 step:24895 [D loss: 0.053724, acc.: 99.22%] [G loss: 0.000161]\n",
      "epoch:31 step:24896 [D loss: 0.011400, acc.: 100.00%] [G loss: 0.001960]\n",
      "epoch:31 step:24897 [D loss: 0.013856, acc.: 100.00%] [G loss: 0.039581]\n",
      "epoch:31 step:24898 [D loss: 0.017832, acc.: 99.22%] [G loss: 0.054974]\n",
      "epoch:31 step:24899 [D loss: 0.003785, acc.: 100.00%] [G loss: 0.016730]\n",
      "epoch:31 step:24900 [D loss: 0.008624, acc.: 100.00%] [G loss: 0.099761]\n",
      "epoch:31 step:24901 [D loss: 0.011330, acc.: 100.00%] [G loss: 0.017237]\n",
      "epoch:31 step:24902 [D loss: 0.008142, acc.: 100.00%] [G loss: 2.720300]\n",
      "epoch:31 step:24903 [D loss: 0.052727, acc.: 98.44%] [G loss: 0.001231]\n",
      "epoch:31 step:24904 [D loss: 0.009598, acc.: 100.00%] [G loss: 0.461223]\n",
      "epoch:31 step:24905 [D loss: 0.001926, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:31 step:24906 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.014785]\n",
      "epoch:31 step:24907 [D loss: 0.001908, acc.: 100.00%] [G loss: 0.150254]\n",
      "epoch:31 step:24908 [D loss: 0.001104, acc.: 100.00%] [G loss: 0.124010]\n",
      "epoch:31 step:24909 [D loss: 0.004077, acc.: 100.00%] [G loss: 0.006996]\n",
      "epoch:31 step:24910 [D loss: 0.000944, acc.: 100.00%] [G loss: 0.014016]\n",
      "epoch:31 step:24911 [D loss: 0.001274, acc.: 100.00%] [G loss: 0.006761]\n",
      "epoch:31 step:24912 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000950]\n",
      "epoch:31 step:24913 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.001369]\n",
      "epoch:31 step:24914 [D loss: 0.003789, acc.: 100.00%] [G loss: 0.198589]\n",
      "epoch:31 step:24915 [D loss: 0.012680, acc.: 100.00%] [G loss: 0.026859]\n",
      "epoch:31 step:24916 [D loss: 0.003302, acc.: 100.00%] [G loss: 0.005365]\n",
      "epoch:31 step:24917 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:31 step:24918 [D loss: 0.000731, acc.: 100.00%] [G loss: 0.104722]\n",
      "epoch:31 step:24919 [D loss: 0.005697, acc.: 100.00%] [G loss: 0.000347]\n",
      "epoch:31 step:24920 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.001591]\n",
      "epoch:31 step:24921 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:31 step:24922 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.002263]\n",
      "epoch:31 step:24923 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:31 step:24924 [D loss: 0.016458, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:31 step:24925 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.000726]\n",
      "epoch:31 step:24926 [D loss: 0.008272, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:31 step:24927 [D loss: 0.003102, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:31 step:24928 [D loss: 0.011421, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:31 step:24929 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:31 step:24930 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.015916]\n",
      "epoch:31 step:24931 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:31 step:24932 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:31 step:24933 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:31 step:24934 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:31 step:24935 [D loss: 0.004344, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:31 step:24936 [D loss: 0.004164, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:31 step:24937 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.023353]\n",
      "epoch:31 step:24938 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:31 step:24939 [D loss: 0.012714, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:31 step:24940 [D loss: 0.015506, acc.: 100.00%] [G loss: 0.627597]\n",
      "epoch:31 step:24941 [D loss: 0.002503, acc.: 100.00%] [G loss: 0.026973]\n",
      "epoch:31 step:24942 [D loss: 0.010838, acc.: 100.00%] [G loss: 2.442178]\n",
      "epoch:31 step:24943 [D loss: 0.020270, acc.: 99.22%] [G loss: 0.097833]\n",
      "epoch:31 step:24944 [D loss: 0.004784, acc.: 100.00%] [G loss: 0.008296]\n",
      "epoch:31 step:24945 [D loss: 0.001379, acc.: 100.00%] [G loss: 0.047801]\n",
      "epoch:31 step:24946 [D loss: 0.009822, acc.: 100.00%] [G loss: 0.060419]\n",
      "epoch:31 step:24947 [D loss: 0.122267, acc.: 97.66%] [G loss: 1.994987]\n",
      "epoch:31 step:24948 [D loss: 0.542867, acc.: 75.78%] [G loss: 0.096810]\n",
      "epoch:31 step:24949 [D loss: 0.152644, acc.: 93.75%] [G loss: 2.090793]\n",
      "epoch:31 step:24950 [D loss: 0.002616, acc.: 100.00%] [G loss: 4.015187]\n",
      "epoch:31 step:24951 [D loss: 0.010682, acc.: 100.00%] [G loss: 0.520362]\n",
      "epoch:31 step:24952 [D loss: 0.006950, acc.: 100.00%] [G loss: 2.353277]\n",
      "epoch:31 step:24953 [D loss: 0.039751, acc.: 98.44%] [G loss: 0.302004]\n",
      "epoch:31 step:24954 [D loss: 0.004897, acc.: 100.00%] [G loss: 0.074491]\n",
      "epoch:31 step:24955 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.230871]\n",
      "epoch:31 step:24956 [D loss: 0.101943, acc.: 96.09%] [G loss: 0.335357]\n",
      "epoch:31 step:24957 [D loss: 0.039154, acc.: 98.44%] [G loss: 0.614884]\n",
      "epoch:31 step:24958 [D loss: 0.080121, acc.: 96.09%] [G loss: 2.436186]\n",
      "epoch:31 step:24959 [D loss: 0.049159, acc.: 98.44%] [G loss: 0.036455]\n",
      "epoch:31 step:24960 [D loss: 0.002472, acc.: 100.00%] [G loss: 0.207646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:31 step:24961 [D loss: 0.001515, acc.: 100.00%] [G loss: 2.289433]\n",
      "epoch:31 step:24962 [D loss: 0.004330, acc.: 100.00%] [G loss: 1.419361]\n",
      "epoch:31 step:24963 [D loss: 0.050560, acc.: 97.66%] [G loss: 0.099992]\n",
      "epoch:31 step:24964 [D loss: 0.095597, acc.: 96.88%] [G loss: 0.023805]\n",
      "epoch:31 step:24965 [D loss: 0.004468, acc.: 100.00%] [G loss: 0.012507]\n",
      "epoch:31 step:24966 [D loss: 0.025399, acc.: 100.00%] [G loss: 2.434336]\n",
      "epoch:31 step:24967 [D loss: 0.006282, acc.: 100.00%] [G loss: 0.004257]\n",
      "epoch:31 step:24968 [D loss: 0.003927, acc.: 100.00%] [G loss: 1.386178]\n",
      "epoch:31 step:24969 [D loss: 0.001730, acc.: 100.00%] [G loss: 0.471896]\n",
      "epoch:31 step:24970 [D loss: 0.021655, acc.: 100.00%] [G loss: 0.372888]\n",
      "epoch:31 step:24971 [D loss: 0.008576, acc.: 100.00%] [G loss: 0.043557]\n",
      "epoch:31 step:24972 [D loss: 0.039270, acc.: 100.00%] [G loss: 2.149251]\n",
      "epoch:31 step:24973 [D loss: 0.057796, acc.: 96.88%] [G loss: 0.126160]\n",
      "epoch:31 step:24974 [D loss: 0.038828, acc.: 97.66%] [G loss: 0.002829]\n",
      "epoch:31 step:24975 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.456730]\n",
      "epoch:31 step:24976 [D loss: 0.001402, acc.: 100.00%] [G loss: 0.000704]\n",
      "epoch:31 step:24977 [D loss: 0.005498, acc.: 100.00%] [G loss: 0.464771]\n",
      "epoch:31 step:24978 [D loss: 0.003757, acc.: 100.00%] [G loss: 0.055439]\n",
      "epoch:31 step:24979 [D loss: 0.008305, acc.: 100.00%] [G loss: 0.008479]\n",
      "epoch:31 step:24980 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.836722]\n",
      "epoch:31 step:24981 [D loss: 0.003816, acc.: 100.00%] [G loss: 0.828908]\n",
      "epoch:31 step:24982 [D loss: 0.107104, acc.: 95.31%] [G loss: 5.348909]\n",
      "epoch:31 step:24983 [D loss: 0.021991, acc.: 99.22%] [G loss: 6.189901]\n",
      "epoch:31 step:24984 [D loss: 0.167930, acc.: 93.75%] [G loss: 3.314790]\n",
      "epoch:31 step:24985 [D loss: 0.011302, acc.: 100.00%] [G loss: 1.409589]\n",
      "epoch:31 step:24986 [D loss: 0.006583, acc.: 100.00%] [G loss: 0.022458]\n",
      "epoch:31 step:24987 [D loss: 0.010645, acc.: 100.00%] [G loss: 0.007042]\n",
      "epoch:31 step:24988 [D loss: 0.000685, acc.: 100.00%] [G loss: 1.675450]\n",
      "epoch:31 step:24989 [D loss: 0.016695, acc.: 100.00%] [G loss: 0.254221]\n",
      "epoch:31 step:24990 [D loss: 0.016301, acc.: 99.22%] [G loss: 0.013108]\n",
      "epoch:31 step:24991 [D loss: 0.042079, acc.: 99.22%] [G loss: 0.032739]\n",
      "epoch:31 step:24992 [D loss: 0.001738, acc.: 100.00%] [G loss: 4.946444]\n",
      "epoch:32 step:24993 [D loss: 0.002464, acc.: 100.00%] [G loss: 0.235558]\n",
      "epoch:32 step:24994 [D loss: 0.028160, acc.: 99.22%] [G loss: 1.253752]\n",
      "epoch:32 step:24995 [D loss: 0.004126, acc.: 100.00%] [G loss: 0.011379]\n",
      "epoch:32 step:24996 [D loss: 0.008404, acc.: 100.00%] [G loss: 0.003957]\n",
      "epoch:32 step:24997 [D loss: 0.008624, acc.: 100.00%] [G loss: 0.038480]\n",
      "epoch:32 step:24998 [D loss: 0.004843, acc.: 100.00%] [G loss: 1.905361]\n",
      "epoch:32 step:24999 [D loss: 0.000917, acc.: 100.00%] [G loss: 0.016110]\n",
      "epoch:32 step:25000 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.003279]\n",
      "epoch:32 step:25001 [D loss: 0.001204, acc.: 100.00%] [G loss: 0.684906]\n",
      "epoch:32 step:25002 [D loss: 0.008804, acc.: 100.00%] [G loss: 0.489621]\n",
      "epoch:32 step:25003 [D loss: 0.066715, acc.: 96.88%] [G loss: 5.230400]\n",
      "epoch:32 step:25004 [D loss: 0.055012, acc.: 98.44%] [G loss: 0.932158]\n",
      "epoch:32 step:25005 [D loss: 0.007458, acc.: 100.00%] [G loss: 0.014461]\n",
      "epoch:32 step:25006 [D loss: 0.028190, acc.: 98.44%] [G loss: 0.004455]\n",
      "epoch:32 step:25007 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:32 step:25008 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.003956]\n",
      "epoch:32 step:25009 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.007306]\n",
      "epoch:32 step:25010 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.100979]\n",
      "epoch:32 step:25011 [D loss: 0.000010, acc.: 100.00%] [G loss: 1.612561]\n",
      "epoch:32 step:25012 [D loss: 0.005072, acc.: 100.00%] [G loss: 1.971627]\n",
      "epoch:32 step:25013 [D loss: 0.003025, acc.: 100.00%] [G loss: 0.242631]\n",
      "epoch:32 step:25014 [D loss: 0.017336, acc.: 100.00%] [G loss: 0.000740]\n",
      "epoch:32 step:25015 [D loss: 0.047594, acc.: 98.44%] [G loss: 0.141192]\n",
      "epoch:32 step:25016 [D loss: 0.187625, acc.: 92.19%] [G loss: 0.000000]\n",
      "epoch:32 step:25017 [D loss: 0.090508, acc.: 95.31%] [G loss: 0.002031]\n",
      "epoch:32 step:25018 [D loss: 0.001341, acc.: 100.00%] [G loss: 0.475445]\n",
      "epoch:32 step:25019 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.744774]\n",
      "epoch:32 step:25020 [D loss: 0.022170, acc.: 99.22%] [G loss: 5.204389]\n",
      "epoch:32 step:25021 [D loss: 0.002504, acc.: 100.00%] [G loss: 1.194388]\n",
      "epoch:32 step:25022 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000468]\n",
      "epoch:32 step:25023 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:32 step:25024 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.133415]\n",
      "epoch:32 step:25025 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:32 step:25026 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001962]\n",
      "epoch:32 step:25027 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:32 step:25028 [D loss: 0.010700, acc.: 100.00%] [G loss: 0.159213]\n",
      "epoch:32 step:25029 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001524]\n",
      "epoch:32 step:25030 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:32 step:25031 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.005425]\n",
      "epoch:32 step:25032 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.736131]\n",
      "epoch:32 step:25033 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.001646]\n",
      "epoch:32 step:25034 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.027304]\n",
      "epoch:32 step:25035 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:32 step:25036 [D loss: 0.002001, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:32 step:25037 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000416]\n",
      "epoch:32 step:25038 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.324450]\n",
      "epoch:32 step:25039 [D loss: 0.000615, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:32 step:25040 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.060495]\n",
      "epoch:32 step:25041 [D loss: 0.000598, acc.: 100.00%] [G loss: 0.035738]\n",
      "epoch:32 step:25042 [D loss: 0.000541, acc.: 100.00%] [G loss: 0.003111]\n",
      "epoch:32 step:25043 [D loss: 0.001472, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:32 step:25044 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:32 step:25045 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:32 step:25046 [D loss: 0.000601, acc.: 100.00%] [G loss: 0.001210]\n",
      "epoch:32 step:25047 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.002860]\n",
      "epoch:32 step:25048 [D loss: 0.001471, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:32 step:25049 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:32 step:25050 [D loss: 0.010349, acc.: 100.00%] [G loss: 0.056737]\n",
      "epoch:32 step:25051 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.396239]\n",
      "epoch:32 step:25052 [D loss: 0.022596, acc.: 100.00%] [G loss: 0.104545]\n",
      "epoch:32 step:25053 [D loss: 0.004632, acc.: 100.00%] [G loss: 0.021887]\n",
      "epoch:32 step:25054 [D loss: 0.037773, acc.: 97.66%] [G loss: 0.000110]\n",
      "epoch:32 step:25055 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:32 step:25056 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.009185]\n",
      "epoch:32 step:25057 [D loss: 0.004987, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:32 step:25058 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:32 step:25059 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:32 step:25060 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.199970]\n",
      "epoch:32 step:25061 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:32 step:25062 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:32 step:25063 [D loss: 0.002582, acc.: 100.00%] [G loss: 0.014795]\n",
      "epoch:32 step:25064 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:32 step:25065 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.016291]\n",
      "epoch:32 step:25066 [D loss: 0.000883, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:32 step:25067 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:32 step:25068 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:32 step:25069 [D loss: 0.000595, acc.: 100.00%] [G loss: 0.043742]\n",
      "epoch:32 step:25070 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:32 step:25071 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.299590]\n",
      "epoch:32 step:25072 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.021164]\n",
      "epoch:32 step:25073 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:32 step:25074 [D loss: 0.003739, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:32 step:25075 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.008195]\n",
      "epoch:32 step:25076 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.233476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25077 [D loss: 0.020659, acc.: 100.00%] [G loss: 0.243971]\n",
      "epoch:32 step:25078 [D loss: 0.000491, acc.: 100.00%] [G loss: 0.004095]\n",
      "epoch:32 step:25079 [D loss: 0.001012, acc.: 100.00%] [G loss: 1.786798]\n",
      "epoch:32 step:25080 [D loss: 0.025426, acc.: 99.22%] [G loss: 0.000041]\n",
      "epoch:32 step:25081 [D loss: 0.012628, acc.: 100.00%] [G loss: 0.018783]\n",
      "epoch:32 step:25082 [D loss: 0.084908, acc.: 97.66%] [G loss: 0.000107]\n",
      "epoch:32 step:25083 [D loss: 1.340926, acc.: 60.94%] [G loss: 15.926528]\n",
      "epoch:32 step:25084 [D loss: 6.433635, acc.: 50.00%] [G loss: 6.709145]\n",
      "epoch:32 step:25085 [D loss: 1.476980, acc.: 50.78%] [G loss: 0.914182]\n",
      "epoch:32 step:25086 [D loss: 0.625055, acc.: 77.34%] [G loss: 5.375152]\n",
      "epoch:32 step:25087 [D loss: 0.048724, acc.: 99.22%] [G loss: 3.404891]\n",
      "epoch:32 step:25088 [D loss: 0.395224, acc.: 80.47%] [G loss: 0.450944]\n",
      "epoch:32 step:25089 [D loss: 0.048877, acc.: 98.44%] [G loss: 0.128547]\n",
      "epoch:32 step:25090 [D loss: 0.030147, acc.: 100.00%] [G loss: 0.020712]\n",
      "epoch:32 step:25091 [D loss: 0.027603, acc.: 99.22%] [G loss: 0.032183]\n",
      "epoch:32 step:25092 [D loss: 0.035866, acc.: 99.22%] [G loss: 3.375602]\n",
      "epoch:32 step:25093 [D loss: 0.027049, acc.: 98.44%] [G loss: 0.031195]\n",
      "epoch:32 step:25094 [D loss: 0.018228, acc.: 100.00%] [G loss: 1.174375]\n",
      "epoch:32 step:25095 [D loss: 0.024433, acc.: 100.00%] [G loss: 0.011273]\n",
      "epoch:32 step:25096 [D loss: 0.052976, acc.: 99.22%] [G loss: 0.006248]\n",
      "epoch:32 step:25097 [D loss: 0.053291, acc.: 98.44%] [G loss: 0.003034]\n",
      "epoch:32 step:25098 [D loss: 0.027097, acc.: 100.00%] [G loss: 2.493376]\n",
      "epoch:32 step:25099 [D loss: 0.077264, acc.: 98.44%] [G loss: 2.795945]\n",
      "epoch:32 step:25100 [D loss: 0.081213, acc.: 98.44%] [G loss: 3.096506]\n",
      "epoch:32 step:25101 [D loss: 0.056583, acc.: 98.44%] [G loss: 1.977802]\n",
      "epoch:32 step:25102 [D loss: 0.426759, acc.: 81.25%] [G loss: 0.091088]\n",
      "epoch:32 step:25103 [D loss: 0.167971, acc.: 92.97%] [G loss: 0.089995]\n",
      "epoch:32 step:25104 [D loss: 0.069078, acc.: 98.44%] [G loss: 0.139937]\n",
      "epoch:32 step:25105 [D loss: 0.023497, acc.: 100.00%] [G loss: 0.106477]\n",
      "epoch:32 step:25106 [D loss: 0.011040, acc.: 100.00%] [G loss: 3.597989]\n",
      "epoch:32 step:25107 [D loss: 0.093164, acc.: 97.66%] [G loss: 0.128348]\n",
      "epoch:32 step:25108 [D loss: 0.103501, acc.: 98.44%] [G loss: 3.595750]\n",
      "epoch:32 step:25109 [D loss: 0.039568, acc.: 99.22%] [G loss: 0.010930]\n",
      "epoch:32 step:25110 [D loss: 0.008165, acc.: 100.00%] [G loss: 0.039088]\n",
      "epoch:32 step:25111 [D loss: 0.041248, acc.: 97.66%] [G loss: 2.845122]\n",
      "epoch:32 step:25112 [D loss: 0.009510, acc.: 100.00%] [G loss: 1.334043]\n",
      "epoch:32 step:25113 [D loss: 0.081445, acc.: 98.44%] [G loss: 0.014169]\n",
      "epoch:32 step:25114 [D loss: 0.026997, acc.: 100.00%] [G loss: 3.512926]\n",
      "epoch:32 step:25115 [D loss: 0.105562, acc.: 96.09%] [G loss: 0.009596]\n",
      "epoch:32 step:25116 [D loss: 0.071082, acc.: 98.44%] [G loss: 1.265471]\n",
      "epoch:32 step:25117 [D loss: 0.024339, acc.: 100.00%] [G loss: 0.005734]\n",
      "epoch:32 step:25118 [D loss: 0.010481, acc.: 100.00%] [G loss: 0.030943]\n",
      "epoch:32 step:25119 [D loss: 0.005969, acc.: 100.00%] [G loss: 0.001217]\n",
      "epoch:32 step:25120 [D loss: 0.056392, acc.: 96.88%] [G loss: 0.000414]\n",
      "epoch:32 step:25121 [D loss: 0.038284, acc.: 99.22%] [G loss: 0.000228]\n",
      "epoch:32 step:25122 [D loss: 0.032847, acc.: 99.22%] [G loss: 0.000064]\n",
      "epoch:32 step:25123 [D loss: 0.019096, acc.: 100.00%] [G loss: 2.622271]\n",
      "epoch:32 step:25124 [D loss: 0.003015, acc.: 100.00%] [G loss: 0.002399]\n",
      "epoch:32 step:25125 [D loss: 0.011258, acc.: 100.00%] [G loss: 1.618677]\n",
      "epoch:32 step:25126 [D loss: 0.184187, acc.: 89.06%] [G loss: 6.721627]\n",
      "epoch:32 step:25127 [D loss: 0.364067, acc.: 84.38%] [G loss: 0.234020]\n",
      "epoch:32 step:25128 [D loss: 0.003664, acc.: 100.00%] [G loss: 0.001729]\n",
      "epoch:32 step:25129 [D loss: 0.008325, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:32 step:25130 [D loss: 0.019995, acc.: 98.44%] [G loss: 0.001105]\n",
      "epoch:32 step:25131 [D loss: 0.005171, acc.: 100.00%] [G loss: 0.000895]\n",
      "epoch:32 step:25132 [D loss: 0.044270, acc.: 99.22%] [G loss: 0.000160]\n",
      "epoch:32 step:25133 [D loss: 0.000908, acc.: 100.00%] [G loss: 0.003148]\n",
      "epoch:32 step:25134 [D loss: 0.000831, acc.: 100.00%] [G loss: 0.018258]\n",
      "epoch:32 step:25135 [D loss: 0.049581, acc.: 97.66%] [G loss: 1.495515]\n",
      "epoch:32 step:25136 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.010313]\n",
      "epoch:32 step:25137 [D loss: 0.000834, acc.: 100.00%] [G loss: 0.596364]\n",
      "epoch:32 step:25138 [D loss: 0.001006, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:32 step:25139 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:32 step:25140 [D loss: 0.002579, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:32 step:25141 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.000625]\n",
      "epoch:32 step:25142 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.822158]\n",
      "epoch:32 step:25143 [D loss: 0.003120, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:32 step:25144 [D loss: 0.000938, acc.: 100.00%] [G loss: 0.001187]\n",
      "epoch:32 step:25145 [D loss: 0.003630, acc.: 100.00%] [G loss: 0.000566]\n",
      "epoch:32 step:25146 [D loss: 0.002989, acc.: 100.00%] [G loss: 0.002270]\n",
      "epoch:32 step:25147 [D loss: 0.018532, acc.: 100.00%] [G loss: 0.194921]\n",
      "epoch:32 step:25148 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.002986]\n",
      "epoch:32 step:25149 [D loss: 0.000908, acc.: 100.00%] [G loss: 0.000773]\n",
      "epoch:32 step:25150 [D loss: 0.001808, acc.: 100.00%] [G loss: 0.008238]\n",
      "epoch:32 step:25151 [D loss: 0.000420, acc.: 100.00%] [G loss: 0.049710]\n",
      "epoch:32 step:25152 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.044414]\n",
      "epoch:32 step:25153 [D loss: 0.002384, acc.: 100.00%] [G loss: 0.000459]\n",
      "epoch:32 step:25154 [D loss: 0.002093, acc.: 100.00%] [G loss: 0.061918]\n",
      "epoch:32 step:25155 [D loss: 0.003333, acc.: 100.00%] [G loss: 0.029650]\n",
      "epoch:32 step:25156 [D loss: 0.001388, acc.: 100.00%] [G loss: 0.003140]\n",
      "epoch:32 step:25157 [D loss: 0.005052, acc.: 100.00%] [G loss: 0.033920]\n",
      "epoch:32 step:25158 [D loss: 0.001448, acc.: 100.00%] [G loss: 0.000463]\n",
      "epoch:32 step:25159 [D loss: 0.002608, acc.: 100.00%] [G loss: 0.035270]\n",
      "epoch:32 step:25160 [D loss: 0.078733, acc.: 98.44%] [G loss: 0.018388]\n",
      "epoch:32 step:25161 [D loss: 0.001423, acc.: 100.00%] [G loss: 3.009694]\n",
      "epoch:32 step:25162 [D loss: 0.071156, acc.: 96.88%] [G loss: 0.007833]\n",
      "epoch:32 step:25163 [D loss: 0.000459, acc.: 100.00%] [G loss: 0.003470]\n",
      "epoch:32 step:25164 [D loss: 0.000374, acc.: 100.00%] [G loss: 0.046865]\n",
      "epoch:32 step:25165 [D loss: 0.000814, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:32 step:25166 [D loss: 0.000853, acc.: 100.00%] [G loss: 0.016707]\n",
      "epoch:32 step:25167 [D loss: 0.000984, acc.: 100.00%] [G loss: 0.000790]\n",
      "epoch:32 step:25168 [D loss: 0.001015, acc.: 100.00%] [G loss: 0.231922]\n",
      "epoch:32 step:25169 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.046535]\n",
      "epoch:32 step:25170 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.440323]\n",
      "epoch:32 step:25171 [D loss: 0.012660, acc.: 100.00%] [G loss: 0.000609]\n",
      "epoch:32 step:25172 [D loss: 0.012553, acc.: 100.00%] [G loss: 0.001592]\n",
      "epoch:32 step:25173 [D loss: 0.001661, acc.: 100.00%] [G loss: 0.041474]\n",
      "epoch:32 step:25174 [D loss: 0.005307, acc.: 100.00%] [G loss: 0.006657]\n",
      "epoch:32 step:25175 [D loss: 0.000762, acc.: 100.00%] [G loss: 0.000739]\n",
      "epoch:32 step:25176 [D loss: 0.028798, acc.: 100.00%] [G loss: 0.010370]\n",
      "epoch:32 step:25177 [D loss: 0.000391, acc.: 100.00%] [G loss: 0.001573]\n",
      "epoch:32 step:25178 [D loss: 0.006411, acc.: 100.00%] [G loss: 0.583600]\n",
      "epoch:32 step:25179 [D loss: 0.006289, acc.: 100.00%] [G loss: 0.000616]\n",
      "epoch:32 step:25180 [D loss: 0.007436, acc.: 100.00%] [G loss: 0.080284]\n",
      "epoch:32 step:25181 [D loss: 0.001184, acc.: 100.00%] [G loss: 0.021269]\n",
      "epoch:32 step:25182 [D loss: 0.144122, acc.: 95.31%] [G loss: 0.680019]\n",
      "epoch:32 step:25183 [D loss: 0.742901, acc.: 75.00%] [G loss: 0.000020]\n",
      "epoch:32 step:25184 [D loss: 0.653026, acc.: 73.44%] [G loss: 1.144220]\n",
      "epoch:32 step:25185 [D loss: 0.433776, acc.: 84.38%] [G loss: 1.140421]\n",
      "epoch:32 step:25186 [D loss: 0.300462, acc.: 88.28%] [G loss: 0.088554]\n",
      "epoch:32 step:25187 [D loss: 0.013772, acc.: 99.22%] [G loss: 1.321291]\n",
      "epoch:32 step:25188 [D loss: 0.028762, acc.: 99.22%] [G loss: 0.000818]\n",
      "epoch:32 step:25189 [D loss: 0.058852, acc.: 96.88%] [G loss: 0.000688]\n",
      "epoch:32 step:25190 [D loss: 0.163453, acc.: 92.19%] [G loss: 3.597823]\n",
      "epoch:32 step:25191 [D loss: 0.516884, acc.: 76.56%] [G loss: 0.199940]\n",
      "epoch:32 step:25192 [D loss: 0.009910, acc.: 100.00%] [G loss: 0.000732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25193 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.013597]\n",
      "epoch:32 step:25194 [D loss: 0.005235, acc.: 100.00%] [G loss: 0.000909]\n",
      "epoch:32 step:25195 [D loss: 0.005234, acc.: 100.00%] [G loss: 0.001186]\n",
      "epoch:32 step:25196 [D loss: 0.001647, acc.: 100.00%] [G loss: 0.004184]\n",
      "epoch:32 step:25197 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:32 step:25198 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.017200]\n",
      "epoch:32 step:25199 [D loss: 0.006458, acc.: 100.00%] [G loss: 0.000728]\n",
      "epoch:32 step:25200 [D loss: 0.002518, acc.: 100.00%] [G loss: 0.000565]\n",
      "epoch:32 step:25201 [D loss: 1.213091, acc.: 60.16%] [G loss: 0.994810]\n",
      "epoch:32 step:25202 [D loss: 1.080095, acc.: 57.81%] [G loss: 7.773118]\n",
      "epoch:32 step:25203 [D loss: 0.144811, acc.: 92.97%] [G loss: 0.112452]\n",
      "epoch:32 step:25204 [D loss: 0.014597, acc.: 100.00%] [G loss: 0.015571]\n",
      "epoch:32 step:25205 [D loss: 0.074652, acc.: 99.22%] [G loss: 0.000525]\n",
      "epoch:32 step:25206 [D loss: 0.008215, acc.: 100.00%] [G loss: 0.000449]\n",
      "epoch:32 step:25207 [D loss: 0.015081, acc.: 99.22%] [G loss: 0.000453]\n",
      "epoch:32 step:25208 [D loss: 0.001808, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:32 step:25209 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:32 step:25210 [D loss: 0.005998, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:32 step:25211 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.017842]\n",
      "epoch:32 step:25212 [D loss: 0.004757, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:32 step:25213 [D loss: 0.045727, acc.: 99.22%] [G loss: 0.000046]\n",
      "epoch:32 step:25214 [D loss: 0.002532, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:32 step:25215 [D loss: 0.010132, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:32 step:25216 [D loss: 0.043196, acc.: 99.22%] [G loss: 0.000191]\n",
      "epoch:32 step:25217 [D loss: 0.003889, acc.: 100.00%] [G loss: 0.003667]\n",
      "epoch:32 step:25218 [D loss: 0.022605, acc.: 100.00%] [G loss: 0.004850]\n",
      "epoch:32 step:25219 [D loss: 0.035049, acc.: 99.22%] [G loss: 0.009548]\n",
      "epoch:32 step:25220 [D loss: 0.040371, acc.: 100.00%] [G loss: 0.076378]\n",
      "epoch:32 step:25221 [D loss: 0.041158, acc.: 99.22%] [G loss: 5.317280]\n",
      "epoch:32 step:25222 [D loss: 0.105068, acc.: 96.09%] [G loss: 0.005818]\n",
      "epoch:32 step:25223 [D loss: 0.002845, acc.: 100.00%] [G loss: 1.863912]\n",
      "epoch:32 step:25224 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.011710]\n",
      "epoch:32 step:25225 [D loss: 0.003224, acc.: 100.00%] [G loss: 0.000525]\n",
      "epoch:32 step:25226 [D loss: 0.000408, acc.: 100.00%] [G loss: 1.060203]\n",
      "epoch:32 step:25227 [D loss: 0.005504, acc.: 100.00%] [G loss: 1.362279]\n",
      "epoch:32 step:25228 [D loss: 0.005262, acc.: 100.00%] [G loss: 0.284987]\n",
      "epoch:32 step:25229 [D loss: 0.066598, acc.: 99.22%] [G loss: 0.002379]\n",
      "epoch:32 step:25230 [D loss: 0.008243, acc.: 100.00%] [G loss: 0.047982]\n",
      "epoch:32 step:25231 [D loss: 0.003313, acc.: 100.00%] [G loss: 0.033371]\n",
      "epoch:32 step:25232 [D loss: 0.139560, acc.: 92.97%] [G loss: 0.000436]\n",
      "epoch:32 step:25233 [D loss: 0.003202, acc.: 100.00%] [G loss: 0.003983]\n",
      "epoch:32 step:25234 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:32 step:25235 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.073243]\n",
      "epoch:32 step:25236 [D loss: 0.002398, acc.: 100.00%] [G loss: 0.002269]\n",
      "epoch:32 step:25237 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.013126]\n",
      "epoch:32 step:25238 [D loss: 0.004294, acc.: 100.00%] [G loss: 0.580454]\n",
      "epoch:32 step:25239 [D loss: 0.012298, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:32 step:25240 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:32 step:25241 [D loss: 0.004417, acc.: 100.00%] [G loss: 0.135846]\n",
      "epoch:32 step:25242 [D loss: 0.010859, acc.: 99.22%] [G loss: 0.000215]\n",
      "epoch:32 step:25243 [D loss: 0.018969, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:32 step:25244 [D loss: 0.000840, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:32 step:25245 [D loss: 0.006879, acc.: 100.00%] [G loss: 0.476228]\n",
      "epoch:32 step:25246 [D loss: 0.006284, acc.: 100.00%] [G loss: 0.597433]\n",
      "epoch:32 step:25247 [D loss: 0.086156, acc.: 96.88%] [G loss: 0.000127]\n",
      "epoch:32 step:25248 [D loss: 0.002885, acc.: 100.00%] [G loss: 3.297832]\n",
      "epoch:32 step:25249 [D loss: 0.013005, acc.: 100.00%] [G loss: 0.000891]\n",
      "epoch:32 step:25250 [D loss: 0.023265, acc.: 99.22%] [G loss: 1.645703]\n",
      "epoch:32 step:25251 [D loss: 0.016681, acc.: 100.00%] [G loss: 0.001121]\n",
      "epoch:32 step:25252 [D loss: 0.016767, acc.: 100.00%] [G loss: 0.371305]\n",
      "epoch:32 step:25253 [D loss: 0.025277, acc.: 99.22%] [G loss: 0.002866]\n",
      "epoch:32 step:25254 [D loss: 0.002864, acc.: 100.00%] [G loss: 0.001602]\n",
      "epoch:32 step:25255 [D loss: 0.001901, acc.: 100.00%] [G loss: 0.000894]\n",
      "epoch:32 step:25256 [D loss: 0.002657, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:32 step:25257 [D loss: 0.064647, acc.: 96.88%] [G loss: 0.000053]\n",
      "epoch:32 step:25258 [D loss: 0.000835, acc.: 100.00%] [G loss: 0.000723]\n",
      "epoch:32 step:25259 [D loss: 0.002006, acc.: 100.00%] [G loss: 0.001166]\n",
      "epoch:32 step:25260 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:32 step:25261 [D loss: 0.001985, acc.: 100.00%] [G loss: 0.275412]\n",
      "epoch:32 step:25262 [D loss: 0.004846, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:32 step:25263 [D loss: 0.093375, acc.: 99.22%] [G loss: 0.077306]\n",
      "epoch:32 step:25264 [D loss: 0.001414, acc.: 100.00%] [G loss: 1.290922]\n",
      "epoch:32 step:25265 [D loss: 0.020699, acc.: 100.00%] [G loss: 0.118387]\n",
      "epoch:32 step:25266 [D loss: 0.007206, acc.: 100.00%] [G loss: 0.182316]\n",
      "epoch:32 step:25267 [D loss: 0.005616, acc.: 100.00%] [G loss: 1.125476]\n",
      "epoch:32 step:25268 [D loss: 0.005603, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:32 step:25269 [D loss: 0.061919, acc.: 98.44%] [G loss: 0.361817]\n",
      "epoch:32 step:25270 [D loss: 0.156386, acc.: 94.53%] [G loss: 0.015798]\n",
      "epoch:32 step:25271 [D loss: 0.002029, acc.: 100.00%] [G loss: 0.161362]\n",
      "epoch:32 step:25272 [D loss: 0.004367, acc.: 100.00%] [G loss: 3.349433]\n",
      "epoch:32 step:25273 [D loss: 0.015043, acc.: 100.00%] [G loss: 2.683412]\n",
      "epoch:32 step:25274 [D loss: 0.342795, acc.: 84.38%] [G loss: 2.498839]\n",
      "epoch:32 step:25275 [D loss: 0.087910, acc.: 96.88%] [G loss: 0.076423]\n",
      "epoch:32 step:25276 [D loss: 0.017576, acc.: 99.22%] [G loss: 3.697470]\n",
      "epoch:32 step:25277 [D loss: 0.072390, acc.: 97.66%] [G loss: 0.043901]\n",
      "epoch:32 step:25278 [D loss: 0.060342, acc.: 99.22%] [G loss: 0.002081]\n",
      "epoch:32 step:25279 [D loss: 0.009123, acc.: 100.00%] [G loss: 0.074487]\n",
      "epoch:32 step:25280 [D loss: 0.008667, acc.: 100.00%] [G loss: 0.008859]\n",
      "epoch:32 step:25281 [D loss: 0.004260, acc.: 100.00%] [G loss: 0.000822]\n",
      "epoch:32 step:25282 [D loss: 0.078319, acc.: 99.22%] [G loss: 0.006037]\n",
      "epoch:32 step:25283 [D loss: 0.034367, acc.: 98.44%] [G loss: 3.537248]\n",
      "epoch:32 step:25284 [D loss: 0.026894, acc.: 99.22%] [G loss: 0.029976]\n",
      "epoch:32 step:25285 [D loss: 0.110133, acc.: 96.88%] [G loss: 2.935978]\n",
      "epoch:32 step:25286 [D loss: 0.019272, acc.: 100.00%] [G loss: 0.295536]\n",
      "epoch:32 step:25287 [D loss: 0.157496, acc.: 93.75%] [G loss: 0.000555]\n",
      "epoch:32 step:25288 [D loss: 0.053652, acc.: 99.22%] [G loss: 0.001323]\n",
      "epoch:32 step:25289 [D loss: 0.001353, acc.: 100.00%] [G loss: 1.428141]\n",
      "epoch:32 step:25290 [D loss: 0.001154, acc.: 100.00%] [G loss: 0.009850]\n",
      "epoch:32 step:25291 [D loss: 0.005276, acc.: 100.00%] [G loss: 2.044353]\n",
      "epoch:32 step:25292 [D loss: 0.077153, acc.: 97.66%] [G loss: 0.054701]\n",
      "epoch:32 step:25293 [D loss: 0.015203, acc.: 100.00%] [G loss: 0.285510]\n",
      "epoch:32 step:25294 [D loss: 0.043291, acc.: 99.22%] [G loss: 0.354555]\n",
      "epoch:32 step:25295 [D loss: 0.074739, acc.: 98.44%] [G loss: 0.417034]\n",
      "epoch:32 step:25296 [D loss: 0.132784, acc.: 96.09%] [G loss: 0.372526]\n",
      "epoch:32 step:25297 [D loss: 0.076884, acc.: 96.88%] [G loss: 0.054913]\n",
      "epoch:32 step:25298 [D loss: 0.034365, acc.: 99.22%] [G loss: 0.054658]\n",
      "epoch:32 step:25299 [D loss: 0.017101, acc.: 100.00%] [G loss: 3.106346]\n",
      "epoch:32 step:25300 [D loss: 0.001960, acc.: 100.00%] [G loss: 0.568820]\n",
      "epoch:32 step:25301 [D loss: 0.023533, acc.: 99.22%] [G loss: 3.335153]\n",
      "epoch:32 step:25302 [D loss: 0.359439, acc.: 82.03%] [G loss: 6.955468]\n",
      "epoch:32 step:25303 [D loss: 1.231856, acc.: 58.59%] [G loss: 2.665291]\n",
      "epoch:32 step:25304 [D loss: 0.224424, acc.: 91.41%] [G loss: 5.121890]\n",
      "epoch:32 step:25305 [D loss: 0.100858, acc.: 96.88%] [G loss: 4.906714]\n",
      "epoch:32 step:25306 [D loss: 0.151096, acc.: 96.09%] [G loss: 4.542446]\n",
      "epoch:32 step:25307 [D loss: 0.238054, acc.: 89.84%] [G loss: 5.234993]\n",
      "epoch:32 step:25308 [D loss: 0.047719, acc.: 99.22%] [G loss: 3.996032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25309 [D loss: 0.182281, acc.: 90.62%] [G loss: 6.508699]\n",
      "epoch:32 step:25310 [D loss: 0.061651, acc.: 97.66%] [G loss: 6.956776]\n",
      "epoch:32 step:25311 [D loss: 0.100585, acc.: 95.31%] [G loss: 1.502875]\n",
      "epoch:32 step:25312 [D loss: 0.081883, acc.: 97.66%] [G loss: 2.764308]\n",
      "epoch:32 step:25313 [D loss: 0.095745, acc.: 96.09%] [G loss: 3.981276]\n",
      "epoch:32 step:25314 [D loss: 0.017718, acc.: 100.00%] [G loss: 3.820648]\n",
      "epoch:32 step:25315 [D loss: 0.051911, acc.: 99.22%] [G loss: 2.249527]\n",
      "epoch:32 step:25316 [D loss: 0.028991, acc.: 100.00%] [G loss: 2.116685]\n",
      "epoch:32 step:25317 [D loss: 0.203076, acc.: 93.75%] [G loss: 4.210165]\n",
      "epoch:32 step:25318 [D loss: 0.046347, acc.: 98.44%] [G loss: 4.081380]\n",
      "epoch:32 step:25319 [D loss: 0.192683, acc.: 91.41%] [G loss: 2.800798]\n",
      "epoch:32 step:25320 [D loss: 0.057218, acc.: 99.22%] [G loss: 2.439373]\n",
      "epoch:32 step:25321 [D loss: 0.008487, acc.: 100.00%] [G loss: 5.056735]\n",
      "epoch:32 step:25322 [D loss: 0.043749, acc.: 98.44%] [G loss: 2.971940]\n",
      "epoch:32 step:25323 [D loss: 0.028556, acc.: 98.44%] [G loss: 2.146497]\n",
      "epoch:32 step:25324 [D loss: 0.006786, acc.: 100.00%] [G loss: 1.291014]\n",
      "epoch:32 step:25325 [D loss: 0.006339, acc.: 100.00%] [G loss: 0.919124]\n",
      "epoch:32 step:25326 [D loss: 0.003528, acc.: 100.00%] [G loss: 0.349509]\n",
      "epoch:32 step:25327 [D loss: 0.004006, acc.: 100.00%] [G loss: 0.306519]\n",
      "epoch:32 step:25328 [D loss: 0.051802, acc.: 98.44%] [G loss: 0.123772]\n",
      "epoch:32 step:25329 [D loss: 0.009721, acc.: 100.00%] [G loss: 0.231452]\n",
      "epoch:32 step:25330 [D loss: 0.020593, acc.: 99.22%] [G loss: 0.166344]\n",
      "epoch:32 step:25331 [D loss: 0.075723, acc.: 99.22%] [G loss: 1.170166]\n",
      "epoch:32 step:25332 [D loss: 0.043436, acc.: 98.44%] [G loss: 0.573526]\n",
      "epoch:32 step:25333 [D loss: 0.004497, acc.: 100.00%] [G loss: 1.883803]\n",
      "epoch:32 step:25334 [D loss: 0.094461, acc.: 96.88%] [G loss: 0.694413]\n",
      "epoch:32 step:25335 [D loss: 0.044750, acc.: 99.22%] [G loss: 0.482404]\n",
      "epoch:32 step:25336 [D loss: 0.049091, acc.: 99.22%] [G loss: 0.979320]\n",
      "epoch:32 step:25337 [D loss: 0.005922, acc.: 100.00%] [G loss: 1.716051]\n",
      "epoch:32 step:25338 [D loss: 0.025343, acc.: 100.00%] [G loss: 1.673826]\n",
      "epoch:32 step:25339 [D loss: 0.046056, acc.: 98.44%] [G loss: 0.501688]\n",
      "epoch:32 step:25340 [D loss: 0.194715, acc.: 91.41%] [G loss: 6.285286]\n",
      "epoch:32 step:25341 [D loss: 0.188612, acc.: 91.41%] [G loss: 9.049959]\n",
      "epoch:32 step:25342 [D loss: 0.378698, acc.: 82.81%] [G loss: 5.908325]\n",
      "epoch:32 step:25343 [D loss: 0.174181, acc.: 96.88%] [G loss: 6.727582]\n",
      "epoch:32 step:25344 [D loss: 0.068545, acc.: 96.88%] [G loss: 7.324895]\n",
      "epoch:32 step:25345 [D loss: 0.081581, acc.: 97.66%] [G loss: 6.174422]\n",
      "epoch:32 step:25346 [D loss: 0.040124, acc.: 99.22%] [G loss: 0.261245]\n",
      "epoch:32 step:25347 [D loss: 0.058817, acc.: 97.66%] [G loss: 6.208388]\n",
      "epoch:32 step:25348 [D loss: 0.039001, acc.: 100.00%] [G loss: 6.475025]\n",
      "epoch:32 step:25349 [D loss: 0.010606, acc.: 100.00%] [G loss: 6.749004]\n",
      "epoch:32 step:25350 [D loss: 0.315448, acc.: 88.28%] [G loss: 1.767973]\n",
      "epoch:32 step:25351 [D loss: 0.061231, acc.: 97.66%] [G loss: 5.878555]\n",
      "epoch:32 step:25352 [D loss: 0.217357, acc.: 89.84%] [G loss: 9.852457]\n",
      "epoch:32 step:25353 [D loss: 0.231760, acc.: 87.50%] [G loss: 8.530108]\n",
      "epoch:32 step:25354 [D loss: 0.039686, acc.: 98.44%] [G loss: 7.290730]\n",
      "epoch:32 step:25355 [D loss: 0.018384, acc.: 100.00%] [G loss: 6.076909]\n",
      "epoch:32 step:25356 [D loss: 0.212153, acc.: 91.41%] [G loss: 3.515192]\n",
      "epoch:32 step:25357 [D loss: 0.165456, acc.: 90.62%] [G loss: 0.132324]\n",
      "epoch:32 step:25358 [D loss: 0.002694, acc.: 100.00%] [G loss: 6.140013]\n",
      "epoch:32 step:25359 [D loss: 0.003678, acc.: 100.00%] [G loss: 5.981013]\n",
      "epoch:32 step:25360 [D loss: 0.026657, acc.: 99.22%] [G loss: 4.634365]\n",
      "epoch:32 step:25361 [D loss: 0.007987, acc.: 100.00%] [G loss: 3.197734]\n",
      "epoch:32 step:25362 [D loss: 0.040841, acc.: 100.00%] [G loss: 0.777965]\n",
      "epoch:32 step:25363 [D loss: 0.023451, acc.: 99.22%] [G loss: 2.852390]\n",
      "epoch:32 step:25364 [D loss: 0.019087, acc.: 100.00%] [G loss: 0.086153]\n",
      "epoch:32 step:25365 [D loss: 0.048521, acc.: 98.44%] [G loss: 3.460722]\n",
      "epoch:32 step:25366 [D loss: 0.112245, acc.: 96.09%] [G loss: 1.238008]\n",
      "epoch:32 step:25367 [D loss: 0.378142, acc.: 80.47%] [G loss: 4.158016]\n",
      "epoch:32 step:25368 [D loss: 0.213223, acc.: 92.97%] [G loss: 3.425595]\n",
      "epoch:32 step:25369 [D loss: 0.036682, acc.: 98.44%] [G loss: 1.008134]\n",
      "epoch:32 step:25370 [D loss: 0.004654, acc.: 100.00%] [G loss: 1.098279]\n",
      "epoch:32 step:25371 [D loss: 0.001207, acc.: 100.00%] [G loss: 0.674435]\n",
      "epoch:32 step:25372 [D loss: 0.009344, acc.: 100.00%] [G loss: 0.518758]\n",
      "epoch:32 step:25373 [D loss: 0.002458, acc.: 100.00%] [G loss: 0.510610]\n",
      "epoch:32 step:25374 [D loss: 0.001565, acc.: 100.00%] [G loss: 0.467996]\n",
      "epoch:32 step:25375 [D loss: 0.004879, acc.: 100.00%] [G loss: 0.043675]\n",
      "epoch:32 step:25376 [D loss: 0.000971, acc.: 100.00%] [G loss: 0.037282]\n",
      "epoch:32 step:25377 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.095522]\n",
      "epoch:32 step:25378 [D loss: 0.008813, acc.: 100.00%] [G loss: 0.001760]\n",
      "epoch:32 step:25379 [D loss: 0.001094, acc.: 100.00%] [G loss: 0.002645]\n",
      "epoch:32 step:25380 [D loss: 0.020219, acc.: 99.22%] [G loss: 0.010109]\n",
      "epoch:32 step:25381 [D loss: 0.012563, acc.: 100.00%] [G loss: 0.004168]\n",
      "epoch:32 step:25382 [D loss: 0.187506, acc.: 89.84%] [G loss: 4.480482]\n",
      "epoch:32 step:25383 [D loss: 0.088968, acc.: 96.09%] [G loss: 2.779573]\n",
      "epoch:32 step:25384 [D loss: 0.019218, acc.: 100.00%] [G loss: 0.417754]\n",
      "epoch:32 step:25385 [D loss: 0.009245, acc.: 100.00%] [G loss: 0.095664]\n",
      "epoch:32 step:25386 [D loss: 0.003322, acc.: 100.00%] [G loss: 0.046524]\n",
      "epoch:32 step:25387 [D loss: 0.069533, acc.: 96.09%] [G loss: 0.000744]\n",
      "epoch:32 step:25388 [D loss: 0.004055, acc.: 100.00%] [G loss: 0.000397]\n",
      "epoch:32 step:25389 [D loss: 0.000794, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:32 step:25390 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.310164]\n",
      "epoch:32 step:25391 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.662318]\n",
      "epoch:32 step:25392 [D loss: 0.000549, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:32 step:25393 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:32 step:25394 [D loss: 0.006239, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:32 step:25395 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:32 step:25396 [D loss: 0.001558, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:32 step:25397 [D loss: 0.000601, acc.: 100.00%] [G loss: 0.001992]\n",
      "epoch:32 step:25398 [D loss: 0.000575, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:32 step:25399 [D loss: 0.002160, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:32 step:25400 [D loss: 0.003300, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:32 step:25401 [D loss: 0.001912, acc.: 100.00%] [G loss: 0.018609]\n",
      "epoch:32 step:25402 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.053523]\n",
      "epoch:32 step:25403 [D loss: 0.002193, acc.: 100.00%] [G loss: 0.002096]\n",
      "epoch:32 step:25404 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.000656]\n",
      "epoch:32 step:25405 [D loss: 0.001081, acc.: 100.00%] [G loss: 0.006373]\n",
      "epoch:32 step:25406 [D loss: 0.008332, acc.: 100.00%] [G loss: 0.006346]\n",
      "epoch:32 step:25407 [D loss: 0.014548, acc.: 100.00%] [G loss: 0.001524]\n",
      "epoch:32 step:25408 [D loss: 0.025752, acc.: 100.00%] [G loss: 0.005018]\n",
      "epoch:32 step:25409 [D loss: 0.002216, acc.: 100.00%] [G loss: 0.003921]\n",
      "epoch:32 step:25410 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.010018]\n",
      "epoch:32 step:25411 [D loss: 0.000845, acc.: 100.00%] [G loss: 0.003162]\n",
      "epoch:32 step:25412 [D loss: 0.039895, acc.: 99.22%] [G loss: 0.001188]\n",
      "epoch:32 step:25413 [D loss: 0.000740, acc.: 100.00%] [G loss: 0.001367]\n",
      "epoch:32 step:25414 [D loss: 0.000781, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:32 step:25415 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:32 step:25416 [D loss: 0.000922, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:32 step:25417 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000467]\n",
      "epoch:32 step:25418 [D loss: 0.042149, acc.: 97.66%] [G loss: 0.004325]\n",
      "epoch:32 step:25419 [D loss: 0.000464, acc.: 100.00%] [G loss: 0.100678]\n",
      "epoch:32 step:25420 [D loss: 0.161495, acc.: 93.75%] [G loss: 0.001196]\n",
      "epoch:32 step:25421 [D loss: 0.268953, acc.: 88.28%] [G loss: 2.509124]\n",
      "epoch:32 step:25422 [D loss: 0.265980, acc.: 86.72%] [G loss: 1.832059]\n",
      "epoch:32 step:25423 [D loss: 0.319807, acc.: 89.06%] [G loss: 4.711611]\n",
      "epoch:32 step:25424 [D loss: 0.091746, acc.: 96.88%] [G loss: 4.198423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25425 [D loss: 0.014759, acc.: 99.22%] [G loss: 2.206035]\n",
      "epoch:32 step:25426 [D loss: 0.055432, acc.: 97.66%] [G loss: 2.324189]\n",
      "epoch:32 step:25427 [D loss: 0.008630, acc.: 100.00%] [G loss: 0.869455]\n",
      "epoch:32 step:25428 [D loss: 0.009377, acc.: 100.00%] [G loss: 0.563725]\n",
      "epoch:32 step:25429 [D loss: 0.003004, acc.: 100.00%] [G loss: 1.032393]\n",
      "epoch:32 step:25430 [D loss: 0.023640, acc.: 99.22%] [G loss: 0.049688]\n",
      "epoch:32 step:25431 [D loss: 0.001294, acc.: 100.00%] [G loss: 0.008180]\n",
      "epoch:32 step:25432 [D loss: 0.001148, acc.: 100.00%] [G loss: 0.119752]\n",
      "epoch:32 step:25433 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.004275]\n",
      "epoch:32 step:25434 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.001770]\n",
      "epoch:32 step:25435 [D loss: 0.137501, acc.: 96.09%] [G loss: 0.126207]\n",
      "epoch:32 step:25436 [D loss: 0.048931, acc.: 98.44%] [G loss: 0.136049]\n",
      "epoch:32 step:25437 [D loss: 0.054003, acc.: 96.88%] [G loss: 0.480639]\n",
      "epoch:32 step:25438 [D loss: 0.002071, acc.: 100.00%] [G loss: 0.780359]\n",
      "epoch:32 step:25439 [D loss: 0.061907, acc.: 99.22%] [G loss: 0.001784]\n",
      "epoch:32 step:25440 [D loss: 0.001109, acc.: 100.00%] [G loss: 0.027942]\n",
      "epoch:32 step:25441 [D loss: 0.000696, acc.: 100.00%] [G loss: 0.021802]\n",
      "epoch:32 step:25442 [D loss: 0.003677, acc.: 100.00%] [G loss: 0.026760]\n",
      "epoch:32 step:25443 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.004104]\n",
      "epoch:32 step:25444 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.003754]\n",
      "epoch:32 step:25445 [D loss: 0.020054, acc.: 99.22%] [G loss: 0.001583]\n",
      "epoch:32 step:25446 [D loss: 0.001217, acc.: 100.00%] [G loss: 0.012558]\n",
      "epoch:32 step:25447 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.397042]\n",
      "epoch:32 step:25448 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000991]\n",
      "epoch:32 step:25449 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.031373]\n",
      "epoch:32 step:25450 [D loss: 0.000016, acc.: 100.00%] [G loss: 2.069863]\n",
      "epoch:32 step:25451 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.018234]\n",
      "epoch:32 step:25452 [D loss: 0.011545, acc.: 99.22%] [G loss: 0.003307]\n",
      "epoch:32 step:25453 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.077204]\n",
      "epoch:32 step:25454 [D loss: 0.001983, acc.: 100.00%] [G loss: 0.021142]\n",
      "epoch:32 step:25455 [D loss: 0.001696, acc.: 100.00%] [G loss: 0.008872]\n",
      "epoch:32 step:25456 [D loss: 0.077918, acc.: 98.44%] [G loss: 0.002175]\n",
      "epoch:32 step:25457 [D loss: 0.002423, acc.: 100.00%] [G loss: 0.313367]\n",
      "epoch:32 step:25458 [D loss: 0.003682, acc.: 100.00%] [G loss: 0.016902]\n",
      "epoch:32 step:25459 [D loss: 0.038024, acc.: 99.22%] [G loss: 0.001424]\n",
      "epoch:32 step:25460 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:32 step:25461 [D loss: 0.000017, acc.: 100.00%] [G loss: 1.628563]\n",
      "epoch:32 step:25462 [D loss: 0.002132, acc.: 100.00%] [G loss: 0.001068]\n",
      "epoch:32 step:25463 [D loss: 0.004999, acc.: 100.00%] [G loss: 0.000402]\n",
      "epoch:32 step:25464 [D loss: 0.264407, acc.: 91.41%] [G loss: 4.094045]\n",
      "epoch:32 step:25465 [D loss: 0.466666, acc.: 80.47%] [G loss: 0.953269]\n",
      "epoch:32 step:25466 [D loss: 0.038934, acc.: 99.22%] [G loss: 0.117701]\n",
      "epoch:32 step:25467 [D loss: 0.000989, acc.: 100.00%] [G loss: 0.148941]\n",
      "epoch:32 step:25468 [D loss: 0.010481, acc.: 100.00%] [G loss: 0.514327]\n",
      "epoch:32 step:25469 [D loss: 0.002569, acc.: 100.00%] [G loss: 0.168023]\n",
      "epoch:32 step:25470 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.039964]\n",
      "epoch:32 step:25471 [D loss: 0.000418, acc.: 100.00%] [G loss: 0.011541]\n",
      "epoch:32 step:25472 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.027605]\n",
      "epoch:32 step:25473 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.069685]\n",
      "epoch:32 step:25474 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.017324]\n",
      "epoch:32 step:25475 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.050724]\n",
      "epoch:32 step:25476 [D loss: 0.005382, acc.: 100.00%] [G loss: 0.030344]\n",
      "epoch:32 step:25477 [D loss: 0.060559, acc.: 98.44%] [G loss: 0.280663]\n",
      "epoch:32 step:25478 [D loss: 0.001774, acc.: 100.00%] [G loss: 0.107566]\n",
      "epoch:32 step:25479 [D loss: 0.019085, acc.: 99.22%] [G loss: 0.014811]\n",
      "epoch:32 step:25480 [D loss: 0.001171, acc.: 100.00%] [G loss: 0.012825]\n",
      "epoch:32 step:25481 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.006487]\n",
      "epoch:32 step:25482 [D loss: 0.000476, acc.: 100.00%] [G loss: 4.017230]\n",
      "epoch:32 step:25483 [D loss: 0.014114, acc.: 99.22%] [G loss: 1.003814]\n",
      "epoch:32 step:25484 [D loss: 0.006757, acc.: 100.00%] [G loss: 0.001072]\n",
      "epoch:32 step:25485 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.036355]\n",
      "epoch:32 step:25486 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.004984]\n",
      "epoch:32 step:25487 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.324030]\n",
      "epoch:32 step:25488 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.026318]\n",
      "epoch:32 step:25489 [D loss: 0.002083, acc.: 100.00%] [G loss: 0.001256]\n",
      "epoch:32 step:25490 [D loss: 0.008496, acc.: 99.22%] [G loss: 0.000455]\n",
      "epoch:32 step:25491 [D loss: 0.002457, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:32 step:25492 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:32 step:25493 [D loss: 0.001600, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:32 step:25494 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000329]\n",
      "epoch:32 step:25495 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.002924]\n",
      "epoch:32 step:25496 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.053784]\n",
      "epoch:32 step:25497 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:32 step:25498 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.009840]\n",
      "epoch:32 step:25499 [D loss: 0.018492, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:32 step:25500 [D loss: 0.000817, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:32 step:25501 [D loss: 0.017620, acc.: 99.22%] [G loss: 0.003424]\n",
      "epoch:32 step:25502 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:32 step:25503 [D loss: 0.003279, acc.: 100.00%] [G loss: 0.000672]\n",
      "epoch:32 step:25504 [D loss: 0.001288, acc.: 100.00%] [G loss: 0.085622]\n",
      "epoch:32 step:25505 [D loss: 0.001551, acc.: 100.00%] [G loss: 0.007634]\n",
      "epoch:32 step:25506 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.000698]\n",
      "epoch:32 step:25507 [D loss: 0.000545, acc.: 100.00%] [G loss: 0.069022]\n",
      "epoch:32 step:25508 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:32 step:25509 [D loss: 0.001268, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:32 step:25510 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.027041]\n",
      "epoch:32 step:25511 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.001035]\n",
      "epoch:32 step:25512 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.001218]\n",
      "epoch:32 step:25513 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.001902]\n",
      "epoch:32 step:25514 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.000299]\n",
      "epoch:32 step:25515 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:32 step:25516 [D loss: 0.011922, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:32 step:25517 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.001570]\n",
      "epoch:32 step:25518 [D loss: 0.005145, acc.: 100.00%] [G loss: 0.000168]\n",
      "epoch:32 step:25519 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:32 step:25520 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:32 step:25521 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:32 step:25522 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:32 step:25523 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:32 step:25524 [D loss: 0.000877, acc.: 100.00%] [G loss: 0.000757]\n",
      "epoch:32 step:25525 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.020083]\n",
      "epoch:32 step:25526 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:32 step:25527 [D loss: 0.003386, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:32 step:25528 [D loss: 0.004399, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:32 step:25529 [D loss: 0.522468, acc.: 72.66%] [G loss: 7.556380]\n",
      "epoch:32 step:25530 [D loss: 1.056005, acc.: 63.28%] [G loss: 2.109281]\n",
      "epoch:32 step:25531 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.261919]\n",
      "epoch:32 step:25532 [D loss: 0.001805, acc.: 100.00%] [G loss: 0.020114]\n",
      "epoch:32 step:25533 [D loss: 0.060182, acc.: 98.44%] [G loss: 0.123984]\n",
      "epoch:32 step:25534 [D loss: 0.011588, acc.: 99.22%] [G loss: 0.210013]\n",
      "epoch:32 step:25535 [D loss: 0.060634, acc.: 99.22%] [G loss: 0.983813]\n",
      "epoch:32 step:25536 [D loss: 0.001012, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:32 step:25537 [D loss: 0.023515, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:32 step:25538 [D loss: 0.003176, acc.: 100.00%] [G loss: 0.880356]\n",
      "epoch:32 step:25539 [D loss: 0.052217, acc.: 96.88%] [G loss: 0.020610]\n",
      "epoch:32 step:25540 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.006318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25541 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:32 step:25542 [D loss: 0.016196, acc.: 100.00%] [G loss: 0.002138]\n",
      "epoch:32 step:25543 [D loss: 0.001720, acc.: 100.00%] [G loss: 0.007883]\n",
      "epoch:32 step:25544 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.002735]\n",
      "epoch:32 step:25545 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.004102]\n",
      "epoch:32 step:25546 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:32 step:25547 [D loss: 0.012430, acc.: 100.00%] [G loss: 0.251177]\n",
      "epoch:32 step:25548 [D loss: 0.001033, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:32 step:25549 [D loss: 0.141843, acc.: 93.75%] [G loss: 0.802327]\n",
      "epoch:32 step:25550 [D loss: 0.005512, acc.: 100.00%] [G loss: 0.848573]\n",
      "epoch:32 step:25551 [D loss: 0.006272, acc.: 100.00%] [G loss: 4.634875]\n",
      "epoch:32 step:25552 [D loss: 0.060622, acc.: 97.66%] [G loss: 0.030283]\n",
      "epoch:32 step:25553 [D loss: 0.052989, acc.: 98.44%] [G loss: 0.002981]\n",
      "epoch:32 step:25554 [D loss: 0.023613, acc.: 99.22%] [G loss: 0.004001]\n",
      "epoch:32 step:25555 [D loss: 0.000660, acc.: 100.00%] [G loss: 0.001064]\n",
      "epoch:32 step:25556 [D loss: 0.008976, acc.: 100.00%] [G loss: 0.001618]\n",
      "epoch:32 step:25557 [D loss: 0.003205, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:32 step:25558 [D loss: 0.001357, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:32 step:25559 [D loss: 0.038629, acc.: 99.22%] [G loss: 0.000724]\n",
      "epoch:32 step:25560 [D loss: 0.002456, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:32 step:25561 [D loss: 0.034189, acc.: 100.00%] [G loss: 0.006050]\n",
      "epoch:32 step:25562 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.002824]\n",
      "epoch:32 step:25563 [D loss: 0.001900, acc.: 100.00%] [G loss: 0.024476]\n",
      "epoch:32 step:25564 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.004071]\n",
      "epoch:32 step:25565 [D loss: 0.026317, acc.: 99.22%] [G loss: 0.000730]\n",
      "epoch:32 step:25566 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.000803]\n",
      "epoch:32 step:25567 [D loss: 0.002058, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:32 step:25568 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.001479]\n",
      "epoch:32 step:25569 [D loss: 0.006151, acc.: 100.00%] [G loss: 0.011425]\n",
      "epoch:32 step:25570 [D loss: 0.000858, acc.: 100.00%] [G loss: 0.001096]\n",
      "epoch:32 step:25571 [D loss: 0.017575, acc.: 99.22%] [G loss: 0.000759]\n",
      "epoch:32 step:25572 [D loss: 0.011191, acc.: 100.00%] [G loss: 0.004170]\n",
      "epoch:32 step:25573 [D loss: 0.011602, acc.: 100.00%] [G loss: 0.009976]\n",
      "epoch:32 step:25574 [D loss: 0.002144, acc.: 100.00%] [G loss: 0.951836]\n",
      "epoch:32 step:25575 [D loss: 0.005424, acc.: 100.00%] [G loss: 0.039207]\n",
      "epoch:32 step:25576 [D loss: 0.269880, acc.: 91.41%] [G loss: 5.589834]\n",
      "epoch:32 step:25577 [D loss: 0.373243, acc.: 86.72%] [G loss: 2.602729]\n",
      "epoch:32 step:25578 [D loss: 0.315419, acc.: 89.84%] [G loss: 7.900943]\n",
      "epoch:32 step:25579 [D loss: 0.076410, acc.: 97.66%] [G loss: 2.567833]\n",
      "epoch:32 step:25580 [D loss: 0.162619, acc.: 92.19%] [G loss: 0.029145]\n",
      "epoch:32 step:25581 [D loss: 0.008710, acc.: 100.00%] [G loss: 2.298775]\n",
      "epoch:32 step:25582 [D loss: 0.005103, acc.: 100.00%] [G loss: 1.430963]\n",
      "epoch:32 step:25583 [D loss: 0.011394, acc.: 100.00%] [G loss: 0.004542]\n",
      "epoch:32 step:25584 [D loss: 0.006170, acc.: 100.00%] [G loss: 0.564344]\n",
      "epoch:32 step:25585 [D loss: 0.065975, acc.: 97.66%] [G loss: 0.361527]\n",
      "epoch:32 step:25586 [D loss: 0.034469, acc.: 99.22%] [G loss: 0.642925]\n",
      "epoch:32 step:25587 [D loss: 0.005442, acc.: 100.00%] [G loss: 0.061377]\n",
      "epoch:32 step:25588 [D loss: 0.002902, acc.: 100.00%] [G loss: 0.147285]\n",
      "epoch:32 step:25589 [D loss: 0.016392, acc.: 99.22%] [G loss: 0.141100]\n",
      "epoch:32 step:25590 [D loss: 0.001927, acc.: 100.00%] [G loss: 0.442761]\n",
      "epoch:32 step:25591 [D loss: 0.001058, acc.: 100.00%] [G loss: 0.249764]\n",
      "epoch:32 step:25592 [D loss: 0.005138, acc.: 100.00%] [G loss: 0.024841]\n",
      "epoch:32 step:25593 [D loss: 0.004256, acc.: 100.00%] [G loss: 0.005746]\n",
      "epoch:32 step:25594 [D loss: 0.000724, acc.: 100.00%] [G loss: 0.022861]\n",
      "epoch:32 step:25595 [D loss: 0.003319, acc.: 100.00%] [G loss: 0.004589]\n",
      "epoch:32 step:25596 [D loss: 0.061978, acc.: 98.44%] [G loss: 0.008670]\n",
      "epoch:32 step:25597 [D loss: 0.016025, acc.: 100.00%] [G loss: 0.000383]\n",
      "epoch:32 step:25598 [D loss: 0.017319, acc.: 100.00%] [G loss: 0.007880]\n",
      "epoch:32 step:25599 [D loss: 0.004788, acc.: 100.00%] [G loss: 1.515066]\n",
      "epoch:32 step:25600 [D loss: 0.002911, acc.: 100.00%] [G loss: 0.029558]\n",
      "epoch:32 step:25601 [D loss: 0.003986, acc.: 100.00%] [G loss: 0.006804]\n",
      "epoch:32 step:25602 [D loss: 0.000496, acc.: 100.00%] [G loss: 0.071014]\n",
      "epoch:32 step:25603 [D loss: 0.001308, acc.: 100.00%] [G loss: 0.000724]\n",
      "epoch:32 step:25604 [D loss: 0.003535, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:32 step:25605 [D loss: 0.106360, acc.: 95.31%] [G loss: 0.092479]\n",
      "epoch:32 step:25606 [D loss: 0.588949, acc.: 70.31%] [G loss: 5.699620]\n",
      "epoch:32 step:25607 [D loss: 0.529762, acc.: 78.91%] [G loss: 2.954474]\n",
      "epoch:32 step:25608 [D loss: 0.059430, acc.: 97.66%] [G loss: 0.182207]\n",
      "epoch:32 step:25609 [D loss: 0.006194, acc.: 100.00%] [G loss: 5.421052]\n",
      "epoch:32 step:25610 [D loss: 0.027719, acc.: 98.44%] [G loss: 0.036241]\n",
      "epoch:32 step:25611 [D loss: 0.003231, acc.: 100.00%] [G loss: 0.183232]\n",
      "epoch:32 step:25612 [D loss: 0.040030, acc.: 99.22%] [G loss: 2.915928]\n",
      "epoch:32 step:25613 [D loss: 0.001808, acc.: 100.00%] [G loss: 0.038006]\n",
      "epoch:32 step:25614 [D loss: 0.073038, acc.: 99.22%] [G loss: 0.992767]\n",
      "epoch:32 step:25615 [D loss: 0.421534, acc.: 84.38%] [G loss: 0.013124]\n",
      "epoch:32 step:25616 [D loss: 0.065201, acc.: 99.22%] [G loss: 0.131778]\n",
      "epoch:32 step:25617 [D loss: 0.005293, acc.: 100.00%] [G loss: 0.372589]\n",
      "epoch:32 step:25618 [D loss: 0.018061, acc.: 99.22%] [G loss: 0.402239]\n",
      "epoch:32 step:25619 [D loss: 0.029440, acc.: 99.22%] [G loss: 0.142000]\n",
      "epoch:32 step:25620 [D loss: 0.001196, acc.: 100.00%] [G loss: 0.067221]\n",
      "epoch:32 step:25621 [D loss: 0.004278, acc.: 100.00%] [G loss: 1.384612]\n",
      "epoch:32 step:25622 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.003211]\n",
      "epoch:32 step:25623 [D loss: 0.003025, acc.: 100.00%] [G loss: 0.003159]\n",
      "epoch:32 step:25624 [D loss: 0.001677, acc.: 100.00%] [G loss: 0.003046]\n",
      "epoch:32 step:25625 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.003014]\n",
      "epoch:32 step:25626 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.049608]\n",
      "epoch:32 step:25627 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.018344]\n",
      "epoch:32 step:25628 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.000707]\n",
      "epoch:32 step:25629 [D loss: 0.002149, acc.: 100.00%] [G loss: 0.000962]\n",
      "epoch:32 step:25630 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.001510]\n",
      "epoch:32 step:25631 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.003705]\n",
      "epoch:32 step:25632 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.001751]\n",
      "epoch:32 step:25633 [D loss: 0.012456, acc.: 100.00%] [G loss: 0.058083]\n",
      "epoch:32 step:25634 [D loss: 0.007281, acc.: 100.00%] [G loss: 0.003073]\n",
      "epoch:32 step:25635 [D loss: 0.000568, acc.: 100.00%] [G loss: 0.003198]\n",
      "epoch:32 step:25636 [D loss: 0.467017, acc.: 75.78%] [G loss: 5.237034]\n",
      "epoch:32 step:25637 [D loss: 1.000162, acc.: 68.75%] [G loss: 0.181434]\n",
      "epoch:32 step:25638 [D loss: 0.339952, acc.: 83.59%] [G loss: 0.001166]\n",
      "epoch:32 step:25639 [D loss: 0.036767, acc.: 99.22%] [G loss: 0.286873]\n",
      "epoch:32 step:25640 [D loss: 0.107207, acc.: 97.66%] [G loss: 0.001028]\n",
      "epoch:32 step:25641 [D loss: 0.000682, acc.: 100.00%] [G loss: 0.000386]\n",
      "epoch:32 step:25642 [D loss: 0.001038, acc.: 100.00%] [G loss: 4.706192]\n",
      "epoch:32 step:25643 [D loss: 0.001271, acc.: 100.00%] [G loss: 0.001742]\n",
      "epoch:32 step:25644 [D loss: 0.000961, acc.: 100.00%] [G loss: 0.000901]\n",
      "epoch:32 step:25645 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:32 step:25646 [D loss: 0.009412, acc.: 99.22%] [G loss: 2.935390]\n",
      "epoch:32 step:25647 [D loss: 0.003137, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:32 step:25648 [D loss: 0.003000, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:32 step:25649 [D loss: 0.006523, acc.: 100.00%] [G loss: 0.001105]\n",
      "epoch:32 step:25650 [D loss: 0.000879, acc.: 100.00%] [G loss: 0.702953]\n",
      "epoch:32 step:25651 [D loss: 0.003412, acc.: 100.00%] [G loss: 0.519959]\n",
      "epoch:32 step:25652 [D loss: 0.005115, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:32 step:25653 [D loss: 0.010189, acc.: 100.00%] [G loss: 0.002872]\n",
      "epoch:32 step:25654 [D loss: 0.016065, acc.: 100.00%] [G loss: 0.002943]\n",
      "epoch:32 step:25655 [D loss: 0.002168, acc.: 100.00%] [G loss: 0.001878]\n",
      "epoch:32 step:25656 [D loss: 0.002234, acc.: 100.00%] [G loss: 0.004146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25657 [D loss: 0.005826, acc.: 100.00%] [G loss: 0.001537]\n",
      "epoch:32 step:25658 [D loss: 0.001124, acc.: 100.00%] [G loss: 0.641534]\n",
      "epoch:32 step:25659 [D loss: 0.032027, acc.: 99.22%] [G loss: 0.190355]\n",
      "epoch:32 step:25660 [D loss: 0.067009, acc.: 98.44%] [G loss: 0.193973]\n",
      "epoch:32 step:25661 [D loss: 0.014699, acc.: 100.00%] [G loss: 0.408067]\n",
      "epoch:32 step:25662 [D loss: 0.069963, acc.: 99.22%] [G loss: 2.429373]\n",
      "epoch:32 step:25663 [D loss: 0.013099, acc.: 100.00%] [G loss: 0.022060]\n",
      "epoch:32 step:25664 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.164658]\n",
      "epoch:32 step:25665 [D loss: 0.001567, acc.: 100.00%] [G loss: 0.002708]\n",
      "epoch:32 step:25666 [D loss: 0.009404, acc.: 99.22%] [G loss: 0.000177]\n",
      "epoch:32 step:25667 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.095818]\n",
      "epoch:32 step:25668 [D loss: 0.008227, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:32 step:25669 [D loss: 0.000688, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:32 step:25670 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:32 step:25671 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.047486]\n",
      "epoch:32 step:25672 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.028774]\n",
      "epoch:32 step:25673 [D loss: 0.026567, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:32 step:25674 [D loss: 0.000587, acc.: 100.00%] [G loss: 0.546442]\n",
      "epoch:32 step:25675 [D loss: 0.001510, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:32 step:25676 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.075191]\n",
      "epoch:32 step:25677 [D loss: 0.017291, acc.: 99.22%] [G loss: 0.013134]\n",
      "epoch:32 step:25678 [D loss: 0.001770, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:32 step:25679 [D loss: 0.012891, acc.: 100.00%] [G loss: 0.001594]\n",
      "epoch:32 step:25680 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:32 step:25681 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.001552]\n",
      "epoch:32 step:25682 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:32 step:25683 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.086419]\n",
      "epoch:32 step:25684 [D loss: 0.023964, acc.: 98.44%] [G loss: 0.000142]\n",
      "epoch:32 step:25685 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.004270]\n",
      "epoch:32 step:25686 [D loss: 0.001292, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:32 step:25687 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:32 step:25688 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:32 step:25689 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.010240]\n",
      "epoch:32 step:25690 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.002843]\n",
      "epoch:32 step:25691 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.003699]\n",
      "epoch:32 step:25692 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:32 step:25693 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.002236]\n",
      "epoch:32 step:25694 [D loss: 0.081779, acc.: 97.66%] [G loss: 0.011387]\n",
      "epoch:32 step:25695 [D loss: 0.023298, acc.: 99.22%] [G loss: 0.019103]\n",
      "epoch:32 step:25696 [D loss: 0.649254, acc.: 70.31%] [G loss: 1.303459]\n",
      "epoch:32 step:25697 [D loss: 0.000230, acc.: 100.00%] [G loss: 2.239869]\n",
      "epoch:32 step:25698 [D loss: 0.017395, acc.: 99.22%] [G loss: 0.000064]\n",
      "epoch:32 step:25699 [D loss: 0.000293, acc.: 100.00%] [G loss: 1.573298]\n",
      "epoch:32 step:25700 [D loss: 0.028776, acc.: 99.22%] [G loss: 0.012542]\n",
      "epoch:32 step:25701 [D loss: 0.000840, acc.: 100.00%] [G loss: 0.003652]\n",
      "epoch:32 step:25702 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.011677]\n",
      "epoch:32 step:25703 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.004094]\n",
      "epoch:32 step:25704 [D loss: 0.005933, acc.: 99.22%] [G loss: 0.018891]\n",
      "epoch:32 step:25705 [D loss: 0.015952, acc.: 99.22%] [G loss: 0.001182]\n",
      "epoch:32 step:25706 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.001922]\n",
      "epoch:32 step:25707 [D loss: 0.018138, acc.: 99.22%] [G loss: 0.002344]\n",
      "epoch:32 step:25708 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000409]\n",
      "epoch:32 step:25709 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.001657]\n",
      "epoch:32 step:25710 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.574813]\n",
      "epoch:32 step:25711 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:32 step:25712 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:32 step:25713 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000459]\n",
      "epoch:32 step:25714 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000722]\n",
      "epoch:32 step:25715 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:32 step:25716 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:32 step:25717 [D loss: 0.000841, acc.: 100.00%] [G loss: 0.000867]\n",
      "epoch:32 step:25718 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.263711]\n",
      "epoch:32 step:25719 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:32 step:25720 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:32 step:25721 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.034721]\n",
      "epoch:32 step:25722 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.031458]\n",
      "epoch:32 step:25723 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000561]\n",
      "epoch:32 step:25724 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.050239]\n",
      "epoch:32 step:25725 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:32 step:25726 [D loss: 0.007888, acc.: 99.22%] [G loss: 0.000011]\n",
      "epoch:32 step:25727 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:32 step:25728 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.106092]\n",
      "epoch:32 step:25729 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:32 step:25730 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:32 step:25731 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:32 step:25732 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:32 step:25733 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.015437]\n",
      "epoch:32 step:25734 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:32 step:25735 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.002699]\n",
      "epoch:32 step:25736 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.015431]\n",
      "epoch:32 step:25737 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:32 step:25738 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.015762]\n",
      "epoch:32 step:25739 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:32 step:25740 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.004790]\n",
      "epoch:32 step:25741 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:32 step:25742 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:32 step:25743 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.008733]\n",
      "epoch:32 step:25744 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:32 step:25745 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.001954]\n",
      "epoch:32 step:25746 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.000598]\n",
      "epoch:32 step:25747 [D loss: 0.049850, acc.: 98.44%] [G loss: 0.010951]\n",
      "epoch:32 step:25748 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.001154]\n",
      "epoch:32 step:25749 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.001630]\n",
      "epoch:32 step:25750 [D loss: 0.000206, acc.: 100.00%] [G loss: 1.322268]\n",
      "epoch:32 step:25751 [D loss: 0.008705, acc.: 100.00%] [G loss: 0.607767]\n",
      "epoch:32 step:25752 [D loss: 0.000289, acc.: 100.00%] [G loss: 1.236899]\n",
      "epoch:32 step:25753 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.330942]\n",
      "epoch:32 step:25754 [D loss: 0.028753, acc.: 99.22%] [G loss: 0.139350]\n",
      "epoch:32 step:25755 [D loss: 0.018522, acc.: 100.00%] [G loss: 0.001258]\n",
      "epoch:32 step:25756 [D loss: 0.002507, acc.: 100.00%] [G loss: 0.184690]\n",
      "epoch:32 step:25757 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:32 step:25758 [D loss: 0.001600, acc.: 100.00%] [G loss: 0.004728]\n",
      "epoch:32 step:25759 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.001120]\n",
      "epoch:32 step:25760 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.043105]\n",
      "epoch:32 step:25761 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.001514]\n",
      "epoch:32 step:25762 [D loss: 0.000549, acc.: 100.00%] [G loss: 0.309320]\n",
      "epoch:32 step:25763 [D loss: 0.016691, acc.: 99.22%] [G loss: 0.000158]\n",
      "epoch:32 step:25764 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:32 step:25765 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:32 step:25766 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:32 step:25767 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.020117]\n",
      "epoch:32 step:25768 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:32 step:25769 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.003252]\n",
      "epoch:32 step:25770 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:32 step:25771 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.003226]\n",
      "epoch:32 step:25772 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.018378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:32 step:25773 [D loss: 0.000625, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:33 step:25774 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.001779]\n",
      "epoch:33 step:25775 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:33 step:25776 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000882]\n",
      "epoch:33 step:25777 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.002087]\n",
      "epoch:33 step:25778 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000753]\n",
      "epoch:33 step:25779 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:33 step:25780 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:33 step:25781 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:33 step:25782 [D loss: 0.002932, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:33 step:25783 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.106457]\n",
      "epoch:33 step:25784 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:33 step:25785 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:33 step:25786 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000845]\n",
      "epoch:33 step:25787 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:33 step:25788 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.001389]\n",
      "epoch:33 step:25789 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:33 step:25790 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:33 step:25791 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.101294]\n",
      "epoch:33 step:25792 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:33 step:25793 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:33 step:25794 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:33 step:25795 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:33 step:25796 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:33 step:25797 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.000259]\n",
      "epoch:33 step:25798 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:33 step:25799 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002231]\n",
      "epoch:33 step:25800 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:33 step:25801 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:33 step:25802 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:33 step:25803 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000736]\n",
      "epoch:33 step:25804 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001573]\n",
      "epoch:33 step:25805 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:33 step:25806 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000414]\n",
      "epoch:33 step:25807 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:33 step:25808 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000962]\n",
      "epoch:33 step:25809 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:33 step:25810 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:33 step:25811 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:33 step:25812 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:33 step:25813 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.001288]\n",
      "epoch:33 step:25814 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.004412]\n",
      "epoch:33 step:25815 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:33 step:25816 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:33 step:25817 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:33 step:25818 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:33 step:25819 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:33 step:25820 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:33 step:25821 [D loss: 0.001983, acc.: 100.00%] [G loss: 0.001459]\n",
      "epoch:33 step:25822 [D loss: 0.000374, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:33 step:25823 [D loss: 0.002850, acc.: 100.00%] [G loss: 0.077316]\n",
      "epoch:33 step:25824 [D loss: 0.000869, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:33 step:25825 [D loss: 0.000754, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:33 step:25826 [D loss: 0.058516, acc.: 99.22%] [G loss: 0.075818]\n",
      "epoch:33 step:25827 [D loss: 0.003733, acc.: 100.00%] [G loss: 0.014347]\n",
      "epoch:33 step:25828 [D loss: 0.000455, acc.: 100.00%] [G loss: 0.445466]\n",
      "epoch:33 step:25829 [D loss: 0.257089, acc.: 87.50%] [G loss: 0.000072]\n",
      "epoch:33 step:25830 [D loss: 0.001624, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:33 step:25831 [D loss: 0.001863, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:33 step:25832 [D loss: 0.002523, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:33 step:25833 [D loss: 0.007672, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:33 step:25834 [D loss: 0.028058, acc.: 98.44%] [G loss: 0.000001]\n",
      "epoch:33 step:25835 [D loss: 0.000569, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:33 step:25836 [D loss: 0.000418, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:33 step:25837 [D loss: 0.000864, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:33 step:25838 [D loss: 0.193570, acc.: 90.62%] [G loss: 0.084732]\n",
      "epoch:33 step:25839 [D loss: 0.000688, acc.: 100.00%] [G loss: 2.745319]\n",
      "epoch:33 step:25840 [D loss: 0.001461, acc.: 100.00%] [G loss: 1.607184]\n",
      "epoch:33 step:25841 [D loss: 0.157570, acc.: 91.41%] [G loss: 0.064579]\n",
      "epoch:33 step:25842 [D loss: 0.018157, acc.: 100.00%] [G loss: 0.059314]\n",
      "epoch:33 step:25843 [D loss: 0.121841, acc.: 96.09%] [G loss: 1.727541]\n",
      "epoch:33 step:25844 [D loss: 0.026546, acc.: 99.22%] [G loss: 3.170222]\n",
      "epoch:33 step:25845 [D loss: 0.001310, acc.: 100.00%] [G loss: 2.120818]\n",
      "epoch:33 step:25846 [D loss: 0.085256, acc.: 98.44%] [G loss: 0.469078]\n",
      "epoch:33 step:25847 [D loss: 0.016034, acc.: 100.00%] [G loss: 0.406341]\n",
      "epoch:33 step:25848 [D loss: 1.241776, acc.: 55.47%] [G loss: 10.386494]\n",
      "epoch:33 step:25849 [D loss: 4.326762, acc.: 50.00%] [G loss: 7.453537]\n",
      "epoch:33 step:25850 [D loss: 1.793080, acc.: 50.78%] [G loss: 4.256921]\n",
      "epoch:33 step:25851 [D loss: 0.241454, acc.: 89.84%] [G loss: 2.398337]\n",
      "epoch:33 step:25852 [D loss: 0.143530, acc.: 95.31%] [G loss: 2.148590]\n",
      "epoch:33 step:25853 [D loss: 0.126469, acc.: 97.66%] [G loss: 2.495238]\n",
      "epoch:33 step:25854 [D loss: 0.087212, acc.: 100.00%] [G loss: 2.650041]\n",
      "epoch:33 step:25855 [D loss: 0.083317, acc.: 97.66%] [G loss: 2.563576]\n",
      "epoch:33 step:25856 [D loss: 0.219312, acc.: 95.31%] [G loss: 2.405940]\n",
      "epoch:33 step:25857 [D loss: 0.082623, acc.: 99.22%] [G loss: 0.176520]\n",
      "epoch:33 step:25858 [D loss: 0.022231, acc.: 100.00%] [G loss: 3.452178]\n",
      "epoch:33 step:25859 [D loss: 0.022931, acc.: 100.00%] [G loss: 2.877327]\n",
      "epoch:33 step:25860 [D loss: 0.019382, acc.: 100.00%] [G loss: 2.598247]\n",
      "epoch:33 step:25861 [D loss: 0.056169, acc.: 99.22%] [G loss: 1.763986]\n",
      "epoch:33 step:25862 [D loss: 0.038507, acc.: 100.00%] [G loss: 2.274014]\n",
      "epoch:33 step:25863 [D loss: 0.061403, acc.: 100.00%] [G loss: 1.317579]\n",
      "epoch:33 step:25864 [D loss: 0.146539, acc.: 96.09%] [G loss: 0.674498]\n",
      "epoch:33 step:25865 [D loss: 0.018876, acc.: 100.00%] [G loss: 3.760175]\n",
      "epoch:33 step:25866 [D loss: 0.024003, acc.: 100.00%] [G loss: 3.529391]\n",
      "epoch:33 step:25867 [D loss: 0.037092, acc.: 97.66%] [G loss: 3.107846]\n",
      "epoch:33 step:25868 [D loss: 0.004285, acc.: 100.00%] [G loss: 2.487583]\n",
      "epoch:33 step:25869 [D loss: 0.027672, acc.: 100.00%] [G loss: 0.123929]\n",
      "epoch:33 step:25870 [D loss: 0.021033, acc.: 100.00%] [G loss: 1.581527]\n",
      "epoch:33 step:25871 [D loss: 0.008385, acc.: 100.00%] [G loss: 0.840442]\n",
      "epoch:33 step:25872 [D loss: 0.005518, acc.: 100.00%] [G loss: 1.321441]\n",
      "epoch:33 step:25873 [D loss: 0.129699, acc.: 94.53%] [G loss: 0.064936]\n",
      "epoch:33 step:25874 [D loss: 0.035476, acc.: 99.22%] [G loss: 1.723346]\n",
      "epoch:33 step:25875 [D loss: 0.057884, acc.: 97.66%] [G loss: 1.122061]\n",
      "epoch:33 step:25876 [D loss: 0.021914, acc.: 100.00%] [G loss: 0.716406]\n",
      "epoch:33 step:25877 [D loss: 0.016786, acc.: 100.00%] [G loss: 0.860001]\n",
      "epoch:33 step:25878 [D loss: 0.033095, acc.: 98.44%] [G loss: 0.140885]\n",
      "epoch:33 step:25879 [D loss: 0.003254, acc.: 100.00%] [G loss: 0.523117]\n",
      "epoch:33 step:25880 [D loss: 0.005306, acc.: 100.00%] [G loss: 0.160083]\n",
      "epoch:33 step:25881 [D loss: 0.008840, acc.: 100.00%] [G loss: 0.066508]\n",
      "epoch:33 step:25882 [D loss: 0.027662, acc.: 100.00%] [G loss: 0.062504]\n",
      "epoch:33 step:25883 [D loss: 0.006049, acc.: 100.00%] [G loss: 0.293091]\n",
      "epoch:33 step:25884 [D loss: 0.067301, acc.: 99.22%] [G loss: 0.130683]\n",
      "epoch:33 step:25885 [D loss: 0.007690, acc.: 100.00%] [G loss: 0.721205]\n",
      "epoch:33 step:25886 [D loss: 0.002196, acc.: 100.00%] [G loss: 0.603679]\n",
      "epoch:33 step:25887 [D loss: 0.007614, acc.: 100.00%] [G loss: 0.127644]\n",
      "epoch:33 step:25888 [D loss: 0.170680, acc.: 94.53%] [G loss: 0.581025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:25889 [D loss: 0.095366, acc.: 96.88%] [G loss: 0.289488]\n",
      "epoch:33 step:25890 [D loss: 0.007456, acc.: 100.00%] [G loss: 0.151388]\n",
      "epoch:33 step:25891 [D loss: 0.002243, acc.: 100.00%] [G loss: 0.165367]\n",
      "epoch:33 step:25892 [D loss: 0.439715, acc.: 76.56%] [G loss: 4.073246]\n",
      "epoch:33 step:25893 [D loss: 0.760071, acc.: 67.19%] [G loss: 3.324380]\n",
      "epoch:33 step:25894 [D loss: 0.048581, acc.: 98.44%] [G loss: 2.114129]\n",
      "epoch:33 step:25895 [D loss: 0.023282, acc.: 100.00%] [G loss: 1.001829]\n",
      "epoch:33 step:25896 [D loss: 0.023248, acc.: 100.00%] [G loss: 0.543452]\n",
      "epoch:33 step:25897 [D loss: 0.072072, acc.: 100.00%] [G loss: 0.370258]\n",
      "epoch:33 step:25898 [D loss: 0.150886, acc.: 92.97%] [G loss: 0.460397]\n",
      "epoch:33 step:25899 [D loss: 0.015582, acc.: 100.00%] [G loss: 2.096296]\n",
      "epoch:33 step:25900 [D loss: 0.029569, acc.: 98.44%] [G loss: 0.336961]\n",
      "epoch:33 step:25901 [D loss: 0.032658, acc.: 100.00%] [G loss: 2.248672]\n",
      "epoch:33 step:25902 [D loss: 0.039105, acc.: 100.00%] [G loss: 1.290047]\n",
      "epoch:33 step:25903 [D loss: 0.009392, acc.: 100.00%] [G loss: 0.117941]\n",
      "epoch:33 step:25904 [D loss: 0.009248, acc.: 100.00%] [G loss: 0.075474]\n",
      "epoch:33 step:25905 [D loss: 0.089213, acc.: 98.44%] [G loss: 0.031786]\n",
      "epoch:33 step:25906 [D loss: 0.020110, acc.: 100.00%] [G loss: 0.059439]\n",
      "epoch:33 step:25907 [D loss: 0.011990, acc.: 100.00%] [G loss: 0.158358]\n",
      "epoch:33 step:25908 [D loss: 0.015889, acc.: 100.00%] [G loss: 0.017219]\n",
      "epoch:33 step:25909 [D loss: 0.007822, acc.: 100.00%] [G loss: 0.061141]\n",
      "epoch:33 step:25910 [D loss: 0.016160, acc.: 100.00%] [G loss: 0.012851]\n",
      "epoch:33 step:25911 [D loss: 0.139825, acc.: 95.31%] [G loss: 2.711342]\n",
      "epoch:33 step:25912 [D loss: 0.013582, acc.: 100.00%] [G loss: 1.996660]\n",
      "epoch:33 step:25913 [D loss: 0.408507, acc.: 79.69%] [G loss: 0.020615]\n",
      "epoch:33 step:25914 [D loss: 0.091828, acc.: 97.66%] [G loss: 0.149136]\n",
      "epoch:33 step:25915 [D loss: 0.010437, acc.: 100.00%] [G loss: 0.517541]\n",
      "epoch:33 step:25916 [D loss: 0.003335, acc.: 100.00%] [G loss: 0.595207]\n",
      "epoch:33 step:25917 [D loss: 0.003986, acc.: 100.00%] [G loss: 0.491694]\n",
      "epoch:33 step:25918 [D loss: 0.005789, acc.: 100.00%] [G loss: 0.375744]\n",
      "epoch:33 step:25919 [D loss: 0.036748, acc.: 99.22%] [G loss: 0.507338]\n",
      "epoch:33 step:25920 [D loss: 0.003707, acc.: 100.00%] [G loss: 0.528801]\n",
      "epoch:33 step:25921 [D loss: 0.012907, acc.: 100.00%] [G loss: 1.075111]\n",
      "epoch:33 step:25922 [D loss: 0.010086, acc.: 100.00%] [G loss: 0.371220]\n",
      "epoch:33 step:25923 [D loss: 0.019327, acc.: 100.00%] [G loss: 0.585150]\n",
      "epoch:33 step:25924 [D loss: 0.063346, acc.: 99.22%] [G loss: 1.299823]\n",
      "epoch:33 step:25925 [D loss: 0.004558, acc.: 100.00%] [G loss: 3.544362]\n",
      "epoch:33 step:25926 [D loss: 0.263205, acc.: 87.50%] [G loss: 0.161230]\n",
      "epoch:33 step:25927 [D loss: 0.157555, acc.: 94.53%] [G loss: 0.184376]\n",
      "epoch:33 step:25928 [D loss: 0.000699, acc.: 100.00%] [G loss: 3.692858]\n",
      "epoch:33 step:25929 [D loss: 0.004295, acc.: 100.00%] [G loss: 2.368441]\n",
      "epoch:33 step:25930 [D loss: 0.002099, acc.: 100.00%] [G loss: 2.134309]\n",
      "epoch:33 step:25931 [D loss: 0.018061, acc.: 100.00%] [G loss: 1.369218]\n",
      "epoch:33 step:25932 [D loss: 0.011903, acc.: 100.00%] [G loss: 0.879629]\n",
      "epoch:33 step:25933 [D loss: 0.020882, acc.: 100.00%] [G loss: 0.551139]\n",
      "epoch:33 step:25934 [D loss: 0.142808, acc.: 94.53%] [G loss: 4.174787]\n",
      "epoch:33 step:25935 [D loss: 0.010013, acc.: 100.00%] [G loss: 5.797511]\n",
      "epoch:33 step:25936 [D loss: 0.020127, acc.: 100.00%] [G loss: 5.899623]\n",
      "epoch:33 step:25937 [D loss: 0.012097, acc.: 100.00%] [G loss: 4.336196]\n",
      "epoch:33 step:25938 [D loss: 0.030445, acc.: 100.00%] [G loss: 4.804426]\n",
      "epoch:33 step:25939 [D loss: 0.167761, acc.: 95.31%] [G loss: 1.329921]\n",
      "epoch:33 step:25940 [D loss: 0.010082, acc.: 100.00%] [G loss: 6.435145]\n",
      "epoch:33 step:25941 [D loss: 0.001457, acc.: 100.00%] [G loss: 6.226098]\n",
      "epoch:33 step:25942 [D loss: 0.002595, acc.: 100.00%] [G loss: 1.034366]\n",
      "epoch:33 step:25943 [D loss: 0.027355, acc.: 99.22%] [G loss: 5.919231]\n",
      "epoch:33 step:25944 [D loss: 0.003996, acc.: 100.00%] [G loss: 5.541348]\n",
      "epoch:33 step:25945 [D loss: 0.012059, acc.: 100.00%] [G loss: 4.356779]\n",
      "epoch:33 step:25946 [D loss: 0.025763, acc.: 100.00%] [G loss: 4.924234]\n",
      "epoch:33 step:25947 [D loss: 0.014009, acc.: 99.22%] [G loss: 4.530753]\n",
      "epoch:33 step:25948 [D loss: 0.007859, acc.: 100.00%] [G loss: 4.304414]\n",
      "epoch:33 step:25949 [D loss: 0.207641, acc.: 91.41%] [G loss: 0.459387]\n",
      "epoch:33 step:25950 [D loss: 0.003457, acc.: 100.00%] [G loss: 6.517647]\n",
      "epoch:33 step:25951 [D loss: 0.010655, acc.: 100.00%] [G loss: 6.126929]\n",
      "epoch:33 step:25952 [D loss: 0.122241, acc.: 93.75%] [G loss: 3.043911]\n",
      "epoch:33 step:25953 [D loss: 0.011853, acc.: 100.00%] [G loss: 2.599188]\n",
      "epoch:33 step:25954 [D loss: 0.008339, acc.: 100.00%] [G loss: 0.511250]\n",
      "epoch:33 step:25955 [D loss: 0.047521, acc.: 98.44%] [G loss: 0.761705]\n",
      "epoch:33 step:25956 [D loss: 0.006400, acc.: 100.00%] [G loss: 0.459321]\n",
      "epoch:33 step:25957 [D loss: 0.009808, acc.: 100.00%] [G loss: 0.090203]\n",
      "epoch:33 step:25958 [D loss: 0.005821, acc.: 100.00%] [G loss: 0.047234]\n",
      "epoch:33 step:25959 [D loss: 0.009361, acc.: 100.00%] [G loss: 0.515896]\n",
      "epoch:33 step:25960 [D loss: 0.032487, acc.: 99.22%] [G loss: 0.261721]\n",
      "epoch:33 step:25961 [D loss: 0.007375, acc.: 100.00%] [G loss: 0.018417]\n",
      "epoch:33 step:25962 [D loss: 0.006430, acc.: 100.00%] [G loss: 0.807873]\n",
      "epoch:33 step:25963 [D loss: 0.021392, acc.: 100.00%] [G loss: 0.062359]\n",
      "epoch:33 step:25964 [D loss: 0.041150, acc.: 99.22%] [G loss: 0.538353]\n",
      "epoch:33 step:25965 [D loss: 0.001360, acc.: 100.00%] [G loss: 0.096924]\n",
      "epoch:33 step:25966 [D loss: 0.016702, acc.: 99.22%] [G loss: 0.377374]\n",
      "epoch:33 step:25967 [D loss: 0.000858, acc.: 100.00%] [G loss: 0.144834]\n",
      "epoch:33 step:25968 [D loss: 0.003148, acc.: 100.00%] [G loss: 0.002688]\n",
      "epoch:33 step:25969 [D loss: 0.001171, acc.: 100.00%] [G loss: 0.008304]\n",
      "epoch:33 step:25970 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.057979]\n",
      "epoch:33 step:25971 [D loss: 0.003569, acc.: 100.00%] [G loss: 0.034087]\n",
      "epoch:33 step:25972 [D loss: 0.009639, acc.: 100.00%] [G loss: 0.191974]\n",
      "epoch:33 step:25973 [D loss: 0.009680, acc.: 100.00%] [G loss: 0.027893]\n",
      "epoch:33 step:25974 [D loss: 0.001134, acc.: 100.00%] [G loss: 0.083579]\n",
      "epoch:33 step:25975 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.176438]\n",
      "epoch:33 step:25976 [D loss: 0.007400, acc.: 100.00%] [G loss: 0.198227]\n",
      "epoch:33 step:25977 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.008522]\n",
      "epoch:33 step:25978 [D loss: 0.001153, acc.: 100.00%] [G loss: 0.117107]\n",
      "epoch:33 step:25979 [D loss: 0.001341, acc.: 100.00%] [G loss: 0.005651]\n",
      "epoch:33 step:25980 [D loss: 0.000947, acc.: 100.00%] [G loss: 0.003381]\n",
      "epoch:33 step:25981 [D loss: 0.001406, acc.: 100.00%] [G loss: 0.011908]\n",
      "epoch:33 step:25982 [D loss: 0.071720, acc.: 96.88%] [G loss: 0.005501]\n",
      "epoch:33 step:25983 [D loss: 0.011331, acc.: 100.00%] [G loss: 0.004631]\n",
      "epoch:33 step:25984 [D loss: 0.025966, acc.: 100.00%] [G loss: 0.799289]\n",
      "epoch:33 step:25985 [D loss: 0.002820, acc.: 100.00%] [G loss: 0.576963]\n",
      "epoch:33 step:25986 [D loss: 0.107146, acc.: 97.66%] [G loss: 2.895249]\n",
      "epoch:33 step:25987 [D loss: 2.348497, acc.: 37.50%] [G loss: 10.450001]\n",
      "epoch:33 step:25988 [D loss: 2.778221, acc.: 50.00%] [G loss: 6.988598]\n",
      "epoch:33 step:25989 [D loss: 0.809508, acc.: 63.28%] [G loss: 4.107140]\n",
      "epoch:33 step:25990 [D loss: 0.564043, acc.: 82.03%] [G loss: 5.067922]\n",
      "epoch:33 step:25991 [D loss: 0.123165, acc.: 94.53%] [G loss: 0.575023]\n",
      "epoch:33 step:25992 [D loss: 0.051326, acc.: 96.88%] [G loss: 4.337666]\n",
      "epoch:33 step:25993 [D loss: 0.010456, acc.: 100.00%] [G loss: 4.616728]\n",
      "epoch:33 step:25994 [D loss: 0.016113, acc.: 100.00%] [G loss: 0.180230]\n",
      "epoch:33 step:25995 [D loss: 0.013147, acc.: 100.00%] [G loss: 2.332129]\n",
      "epoch:33 step:25996 [D loss: 0.009168, acc.: 100.00%] [G loss: 1.711975]\n",
      "epoch:33 step:25997 [D loss: 0.004037, acc.: 100.00%] [G loss: 0.488180]\n",
      "epoch:33 step:25998 [D loss: 0.001473, acc.: 100.00%] [G loss: 0.032091]\n",
      "epoch:33 step:25999 [D loss: 0.016981, acc.: 100.00%] [G loss: 0.052527]\n",
      "epoch:33 step:26000 [D loss: 0.010375, acc.: 100.00%] [G loss: 0.157191]\n",
      "epoch:33 step:26001 [D loss: 0.009509, acc.: 100.00%] [G loss: 0.069120]\n",
      "epoch:33 step:26002 [D loss: 0.026684, acc.: 100.00%] [G loss: 0.192858]\n",
      "epoch:33 step:26003 [D loss: 0.019511, acc.: 100.00%] [G loss: 0.047548]\n",
      "epoch:33 step:26004 [D loss: 0.076589, acc.: 98.44%] [G loss: 0.062586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26005 [D loss: 0.007485, acc.: 100.00%] [G loss: 0.061104]\n",
      "epoch:33 step:26006 [D loss: 0.007168, acc.: 100.00%] [G loss: 0.171239]\n",
      "epoch:33 step:26007 [D loss: 0.015609, acc.: 100.00%] [G loss: 0.372645]\n",
      "epoch:33 step:26008 [D loss: 0.040515, acc.: 100.00%] [G loss: 0.021690]\n",
      "epoch:33 step:26009 [D loss: 2.142761, acc.: 40.62%] [G loss: 6.779926]\n",
      "epoch:33 step:26010 [D loss: 1.995531, acc.: 50.78%] [G loss: 4.069177]\n",
      "epoch:33 step:26011 [D loss: 0.858911, acc.: 57.81%] [G loss: 4.703471]\n",
      "epoch:33 step:26012 [D loss: 0.109023, acc.: 94.53%] [G loss: 3.867636]\n",
      "epoch:33 step:26013 [D loss: 0.191989, acc.: 92.19%] [G loss: 2.194676]\n",
      "epoch:33 step:26014 [D loss: 0.035992, acc.: 99.22%] [G loss: 2.265175]\n",
      "epoch:33 step:26015 [D loss: 0.068406, acc.: 99.22%] [G loss: 2.306776]\n",
      "epoch:33 step:26016 [D loss: 0.047406, acc.: 100.00%] [G loss: 2.375837]\n",
      "epoch:33 step:26017 [D loss: 0.214615, acc.: 89.84%] [G loss: 2.741310]\n",
      "epoch:33 step:26018 [D loss: 0.052959, acc.: 99.22%] [G loss: 0.551213]\n",
      "epoch:33 step:26019 [D loss: 0.114680, acc.: 96.09%] [G loss: 2.253599]\n",
      "epoch:33 step:26020 [D loss: 0.263517, acc.: 88.28%] [G loss: 0.399531]\n",
      "epoch:33 step:26021 [D loss: 0.010304, acc.: 100.00%] [G loss: 0.151579]\n",
      "epoch:33 step:26022 [D loss: 0.068988, acc.: 97.66%] [G loss: 0.056127]\n",
      "epoch:33 step:26023 [D loss: 0.027404, acc.: 99.22%] [G loss: 0.321332]\n",
      "epoch:33 step:26024 [D loss: 0.037904, acc.: 100.00%] [G loss: 0.037953]\n",
      "epoch:33 step:26025 [D loss: 0.086462, acc.: 96.88%] [G loss: 0.149939]\n",
      "epoch:33 step:26026 [D loss: 0.034372, acc.: 100.00%] [G loss: 2.946305]\n",
      "epoch:33 step:26027 [D loss: 0.018209, acc.: 100.00%] [G loss: 0.499361]\n",
      "epoch:33 step:26028 [D loss: 0.021388, acc.: 100.00%] [G loss: 0.797431]\n",
      "epoch:33 step:26029 [D loss: 0.045240, acc.: 100.00%] [G loss: 0.236673]\n",
      "epoch:33 step:26030 [D loss: 0.018876, acc.: 99.22%] [G loss: 0.106677]\n",
      "epoch:33 step:26031 [D loss: 0.028542, acc.: 99.22%] [G loss: 0.018466]\n",
      "epoch:33 step:26032 [D loss: 0.026681, acc.: 99.22%] [G loss: 0.010299]\n",
      "epoch:33 step:26033 [D loss: 0.018741, acc.: 100.00%] [G loss: 0.007433]\n",
      "epoch:33 step:26034 [D loss: 0.020422, acc.: 100.00%] [G loss: 0.043770]\n",
      "epoch:33 step:26035 [D loss: 0.200428, acc.: 90.62%] [G loss: 0.520073]\n",
      "epoch:33 step:26036 [D loss: 0.003905, acc.: 100.00%] [G loss: 1.281998]\n",
      "epoch:33 step:26037 [D loss: 0.245862, acc.: 87.50%] [G loss: 0.951490]\n",
      "epoch:33 step:26038 [D loss: 0.301777, acc.: 85.16%] [G loss: 0.002673]\n",
      "epoch:33 step:26039 [D loss: 0.125192, acc.: 94.53%] [G loss: 0.016538]\n",
      "epoch:33 step:26040 [D loss: 0.002203, acc.: 100.00%] [G loss: 1.123652]\n",
      "epoch:33 step:26041 [D loss: 0.014852, acc.: 100.00%] [G loss: 0.030331]\n",
      "epoch:33 step:26042 [D loss: 0.004303, acc.: 100.00%] [G loss: 0.222464]\n",
      "epoch:33 step:26043 [D loss: 0.058942, acc.: 100.00%] [G loss: 0.024249]\n",
      "epoch:33 step:26044 [D loss: 0.028138, acc.: 99.22%] [G loss: 0.891172]\n",
      "epoch:33 step:26045 [D loss: 0.082883, acc.: 99.22%] [G loss: 0.462914]\n",
      "epoch:33 step:26046 [D loss: 0.159840, acc.: 92.97%] [G loss: 0.423109]\n",
      "epoch:33 step:26047 [D loss: 0.021631, acc.: 100.00%] [G loss: 0.063184]\n",
      "epoch:33 step:26048 [D loss: 0.020607, acc.: 99.22%] [G loss: 0.213307]\n",
      "epoch:33 step:26049 [D loss: 0.190720, acc.: 92.19%] [G loss: 0.184251]\n",
      "epoch:33 step:26050 [D loss: 0.400797, acc.: 83.59%] [G loss: 0.735942]\n",
      "epoch:33 step:26051 [D loss: 0.088870, acc.: 97.66%] [G loss: 1.450431]\n",
      "epoch:33 step:26052 [D loss: 0.026587, acc.: 100.00%] [G loss: 0.164902]\n",
      "epoch:33 step:26053 [D loss: 0.003913, acc.: 100.00%] [G loss: 3.731993]\n",
      "epoch:33 step:26054 [D loss: 0.013252, acc.: 100.00%] [G loss: 1.673746]\n",
      "epoch:33 step:26055 [D loss: 0.111645, acc.: 97.66%] [G loss: 0.107129]\n",
      "epoch:33 step:26056 [D loss: 0.013274, acc.: 100.00%] [G loss: 1.641719]\n",
      "epoch:33 step:26057 [D loss: 0.028031, acc.: 100.00%] [G loss: 0.107063]\n",
      "epoch:33 step:26058 [D loss: 0.019042, acc.: 100.00%] [G loss: 0.854635]\n",
      "epoch:33 step:26059 [D loss: 0.113102, acc.: 96.09%] [G loss: 0.598362]\n",
      "epoch:33 step:26060 [D loss: 0.025070, acc.: 100.00%] [G loss: 0.003458]\n",
      "epoch:33 step:26061 [D loss: 0.033959, acc.: 99.22%] [G loss: 0.004264]\n",
      "epoch:33 step:26062 [D loss: 0.029100, acc.: 99.22%] [G loss: 0.588561]\n",
      "epoch:33 step:26063 [D loss: 0.010833, acc.: 100.00%] [G loss: 0.002686]\n",
      "epoch:33 step:26064 [D loss: 0.010156, acc.: 100.00%] [G loss: 0.002898]\n",
      "epoch:33 step:26065 [D loss: 0.039162, acc.: 100.00%] [G loss: 0.005199]\n",
      "epoch:33 step:26066 [D loss: 0.002239, acc.: 100.00%] [G loss: 0.012155]\n",
      "epoch:33 step:26067 [D loss: 0.025439, acc.: 100.00%] [G loss: 0.545716]\n",
      "epoch:33 step:26068 [D loss: 0.009536, acc.: 100.00%] [G loss: 0.291296]\n",
      "epoch:33 step:26069 [D loss: 0.008291, acc.: 100.00%] [G loss: 0.010791]\n",
      "epoch:33 step:26070 [D loss: 0.002380, acc.: 100.00%] [G loss: 0.001925]\n",
      "epoch:33 step:26071 [D loss: 0.000899, acc.: 100.00%] [G loss: 0.004343]\n",
      "epoch:33 step:26072 [D loss: 0.000814, acc.: 100.00%] [G loss: 0.020164]\n",
      "epoch:33 step:26073 [D loss: 0.005490, acc.: 100.00%] [G loss: 0.112417]\n",
      "epoch:33 step:26074 [D loss: 0.001418, acc.: 100.00%] [G loss: 0.001893]\n",
      "epoch:33 step:26075 [D loss: 0.000669, acc.: 100.00%] [G loss: 0.043519]\n",
      "epoch:33 step:26076 [D loss: 0.003231, acc.: 100.00%] [G loss: 0.664768]\n",
      "epoch:33 step:26077 [D loss: 0.001229, acc.: 100.00%] [G loss: 0.001404]\n",
      "epoch:33 step:26078 [D loss: 0.004025, acc.: 100.00%] [G loss: 0.020875]\n",
      "epoch:33 step:26079 [D loss: 0.074627, acc.: 98.44%] [G loss: 0.797884]\n",
      "epoch:33 step:26080 [D loss: 0.004265, acc.: 100.00%] [G loss: 0.064584]\n",
      "epoch:33 step:26081 [D loss: 0.033246, acc.: 99.22%] [G loss: 0.058395]\n",
      "epoch:33 step:26082 [D loss: 0.002928, acc.: 100.00%] [G loss: 0.015255]\n",
      "epoch:33 step:26083 [D loss: 0.002717, acc.: 100.00%] [G loss: 0.056349]\n",
      "epoch:33 step:26084 [D loss: 0.005798, acc.: 100.00%] [G loss: 0.005912]\n",
      "epoch:33 step:26085 [D loss: 0.002707, acc.: 100.00%] [G loss: 0.015787]\n",
      "epoch:33 step:26086 [D loss: 0.005849, acc.: 100.00%] [G loss: 0.129921]\n",
      "epoch:33 step:26087 [D loss: 0.022473, acc.: 100.00%] [G loss: 0.004692]\n",
      "epoch:33 step:26088 [D loss: 0.228303, acc.: 88.28%] [G loss: 0.000057]\n",
      "epoch:33 step:26089 [D loss: 0.073536, acc.: 98.44%] [G loss: 0.000203]\n",
      "epoch:33 step:26090 [D loss: 0.014212, acc.: 100.00%] [G loss: 0.081389]\n",
      "epoch:33 step:26091 [D loss: 0.000664, acc.: 100.00%] [G loss: 0.062507]\n",
      "epoch:33 step:26092 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.159150]\n",
      "epoch:33 step:26093 [D loss: 0.002012, acc.: 100.00%] [G loss: 0.169664]\n",
      "epoch:33 step:26094 [D loss: 0.002963, acc.: 100.00%] [G loss: 0.007418]\n",
      "epoch:33 step:26095 [D loss: 0.000827, acc.: 100.00%] [G loss: 0.020739]\n",
      "epoch:33 step:26096 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.055777]\n",
      "epoch:33 step:26097 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.002360]\n",
      "epoch:33 step:26098 [D loss: 0.000914, acc.: 100.00%] [G loss: 0.006752]\n",
      "epoch:33 step:26099 [D loss: 0.000521, acc.: 100.00%] [G loss: 0.027212]\n",
      "epoch:33 step:26100 [D loss: 0.001110, acc.: 100.00%] [G loss: 0.049778]\n",
      "epoch:33 step:26101 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.002604]\n",
      "epoch:33 step:26102 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.003902]\n",
      "epoch:33 step:26103 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.001567]\n",
      "epoch:33 step:26104 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.005157]\n",
      "epoch:33 step:26105 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.004321]\n",
      "epoch:33 step:26106 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.004330]\n",
      "epoch:33 step:26107 [D loss: 0.002206, acc.: 100.00%] [G loss: 0.147400]\n",
      "epoch:33 step:26108 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.007958]\n",
      "epoch:33 step:26109 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.019241]\n",
      "epoch:33 step:26110 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.011602]\n",
      "epoch:33 step:26111 [D loss: 0.000614, acc.: 100.00%] [G loss: 0.009178]\n",
      "epoch:33 step:26112 [D loss: 0.000380, acc.: 100.00%] [G loss: 0.001009]\n",
      "epoch:33 step:26113 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.001669]\n",
      "epoch:33 step:26114 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.010641]\n",
      "epoch:33 step:26115 [D loss: 0.000449, acc.: 100.00%] [G loss: 0.001548]\n",
      "epoch:33 step:26116 [D loss: 0.001696, acc.: 100.00%] [G loss: 0.018720]\n",
      "epoch:33 step:26117 [D loss: 0.002812, acc.: 100.00%] [G loss: 0.044174]\n",
      "epoch:33 step:26118 [D loss: 0.001709, acc.: 100.00%] [G loss: 0.001184]\n",
      "epoch:33 step:26119 [D loss: 0.002852, acc.: 100.00%] [G loss: 0.002800]\n",
      "epoch:33 step:26120 [D loss: 0.005057, acc.: 100.00%] [G loss: 0.001023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26121 [D loss: 0.003215, acc.: 100.00%] [G loss: 0.010883]\n",
      "epoch:33 step:26122 [D loss: 0.000670, acc.: 100.00%] [G loss: 0.011776]\n",
      "epoch:33 step:26123 [D loss: 0.003669, acc.: 100.00%] [G loss: 0.003069]\n",
      "epoch:33 step:26124 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.028807]\n",
      "epoch:33 step:26125 [D loss: 0.008107, acc.: 100.00%] [G loss: 0.016583]\n",
      "epoch:33 step:26126 [D loss: 0.011439, acc.: 99.22%] [G loss: 0.003559]\n",
      "epoch:33 step:26127 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.001801]\n",
      "epoch:33 step:26128 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.003907]\n",
      "epoch:33 step:26129 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.000443]\n",
      "epoch:33 step:26130 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.003363]\n",
      "epoch:33 step:26131 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000946]\n",
      "epoch:33 step:26132 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.048187]\n",
      "epoch:33 step:26133 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.003997]\n",
      "epoch:33 step:26134 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:33 step:26135 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.009049]\n",
      "epoch:33 step:26136 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.002358]\n",
      "epoch:33 step:26137 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.000428]\n",
      "epoch:33 step:26138 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001079]\n",
      "epoch:33 step:26139 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.001335]\n",
      "epoch:33 step:26140 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.001331]\n",
      "epoch:33 step:26141 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.001745]\n",
      "epoch:33 step:26142 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.001942]\n",
      "epoch:33 step:26143 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.001126]\n",
      "epoch:33 step:26144 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:33 step:26145 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.004446]\n",
      "epoch:33 step:26146 [D loss: 0.002212, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:33 step:26147 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.009905]\n",
      "epoch:33 step:26148 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.011413]\n",
      "epoch:33 step:26149 [D loss: 0.000957, acc.: 100.00%] [G loss: 0.002147]\n",
      "epoch:33 step:26150 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.000679]\n",
      "epoch:33 step:26151 [D loss: 0.002657, acc.: 100.00%] [G loss: 0.003681]\n",
      "epoch:33 step:26152 [D loss: 0.004471, acc.: 100.00%] [G loss: 0.004883]\n",
      "epoch:33 step:26153 [D loss: 0.010324, acc.: 100.00%] [G loss: 0.014334]\n",
      "epoch:33 step:26154 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.033035]\n",
      "epoch:33 step:26155 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.085204]\n",
      "epoch:33 step:26156 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.002921]\n",
      "epoch:33 step:26157 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.009783]\n",
      "epoch:33 step:26158 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.007188]\n",
      "epoch:33 step:26159 [D loss: 0.000800, acc.: 100.00%] [G loss: 0.033724]\n",
      "epoch:33 step:26160 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.006818]\n",
      "epoch:33 step:26161 [D loss: 0.000803, acc.: 100.00%] [G loss: 0.005386]\n",
      "epoch:33 step:26162 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.016805]\n",
      "epoch:33 step:26163 [D loss: 0.000701, acc.: 100.00%] [G loss: 0.156296]\n",
      "epoch:33 step:26164 [D loss: 0.004886, acc.: 100.00%] [G loss: 0.002761]\n",
      "epoch:33 step:26165 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.037203]\n",
      "epoch:33 step:26166 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.008673]\n",
      "epoch:33 step:26167 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.001447]\n",
      "epoch:33 step:26168 [D loss: 0.000853, acc.: 100.00%] [G loss: 0.000768]\n",
      "epoch:33 step:26169 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.002749]\n",
      "epoch:33 step:26170 [D loss: 0.004396, acc.: 100.00%] [G loss: 0.027209]\n",
      "epoch:33 step:26171 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.007164]\n",
      "epoch:33 step:26172 [D loss: 0.000755, acc.: 100.00%] [G loss: 0.002474]\n",
      "epoch:33 step:26173 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000654]\n",
      "epoch:33 step:26174 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.000742]\n",
      "epoch:33 step:26175 [D loss: 0.000877, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:33 step:26176 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.001065]\n",
      "epoch:33 step:26177 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.002938]\n",
      "epoch:33 step:26178 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.001024]\n",
      "epoch:33 step:26179 [D loss: 0.000581, acc.: 100.00%] [G loss: 0.001715]\n",
      "epoch:33 step:26180 [D loss: 0.011725, acc.: 100.00%] [G loss: 0.002368]\n",
      "epoch:33 step:26181 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.004105]\n",
      "epoch:33 step:26182 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.006466]\n",
      "epoch:33 step:26183 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.013742]\n",
      "epoch:33 step:26184 [D loss: 0.001641, acc.: 100.00%] [G loss: 0.011559]\n",
      "epoch:33 step:26185 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.035333]\n",
      "epoch:33 step:26186 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.001288]\n",
      "epoch:33 step:26187 [D loss: 0.001528, acc.: 100.00%] [G loss: 0.011320]\n",
      "epoch:33 step:26188 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.001198]\n",
      "epoch:33 step:26189 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000989]\n",
      "epoch:33 step:26190 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.007841]\n",
      "epoch:33 step:26191 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.003791]\n",
      "epoch:33 step:26192 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.001367]\n",
      "epoch:33 step:26193 [D loss: 0.000687, acc.: 100.00%] [G loss: 0.001300]\n",
      "epoch:33 step:26194 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.002795]\n",
      "epoch:33 step:26195 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.001044]\n",
      "epoch:33 step:26196 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:33 step:26197 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000568]\n",
      "epoch:33 step:26198 [D loss: 0.001076, acc.: 100.00%] [G loss: 0.001442]\n",
      "epoch:33 step:26199 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.002584]\n",
      "epoch:33 step:26200 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.002669]\n",
      "epoch:33 step:26201 [D loss: 0.002337, acc.: 100.00%] [G loss: 0.000893]\n",
      "epoch:33 step:26202 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.000463]\n",
      "epoch:33 step:26203 [D loss: 0.042861, acc.: 99.22%] [G loss: 0.000050]\n",
      "epoch:33 step:26204 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:33 step:26205 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:33 step:26206 [D loss: 0.002912, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:33 step:26207 [D loss: 0.000817, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:33 step:26208 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000397]\n",
      "epoch:33 step:26209 [D loss: 0.619084, acc.: 67.19%] [G loss: 6.668417]\n",
      "epoch:33 step:26210 [D loss: 0.854579, acc.: 67.97%] [G loss: 4.088092]\n",
      "epoch:33 step:26211 [D loss: 0.456986, acc.: 78.12%] [G loss: 1.353947]\n",
      "epoch:33 step:26212 [D loss: 0.004571, acc.: 100.00%] [G loss: 0.302336]\n",
      "epoch:33 step:26213 [D loss: 0.075808, acc.: 95.31%] [G loss: 0.352036]\n",
      "epoch:33 step:26214 [D loss: 0.007942, acc.: 100.00%] [G loss: 0.424099]\n",
      "epoch:33 step:26215 [D loss: 0.003891, acc.: 100.00%] [G loss: 0.052193]\n",
      "epoch:33 step:26216 [D loss: 0.001621, acc.: 100.00%] [G loss: 0.405698]\n",
      "epoch:33 step:26217 [D loss: 0.015781, acc.: 100.00%] [G loss: 0.026481]\n",
      "epoch:33 step:26218 [D loss: 0.021773, acc.: 99.22%] [G loss: 0.080212]\n",
      "epoch:33 step:26219 [D loss: 0.011687, acc.: 100.00%] [G loss: 0.087551]\n",
      "epoch:33 step:26220 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.101193]\n",
      "epoch:33 step:26221 [D loss: 0.004284, acc.: 100.00%] [G loss: 0.023475]\n",
      "epoch:33 step:26222 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.001525]\n",
      "epoch:33 step:26223 [D loss: 0.001372, acc.: 100.00%] [G loss: 0.020463]\n",
      "epoch:33 step:26224 [D loss: 0.002165, acc.: 100.00%] [G loss: 0.006781]\n",
      "epoch:33 step:26225 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.017308]\n",
      "epoch:33 step:26226 [D loss: 0.002068, acc.: 100.00%] [G loss: 0.012212]\n",
      "epoch:33 step:26227 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.015080]\n",
      "epoch:33 step:26228 [D loss: 0.001297, acc.: 100.00%] [G loss: 0.023624]\n",
      "epoch:33 step:26229 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.003910]\n",
      "epoch:33 step:26230 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.024690]\n",
      "epoch:33 step:26231 [D loss: 0.001034, acc.: 100.00%] [G loss: 0.019403]\n",
      "epoch:33 step:26232 [D loss: 0.000861, acc.: 100.00%] [G loss: 0.022379]\n",
      "epoch:33 step:26233 [D loss: 0.003751, acc.: 100.00%] [G loss: 0.019815]\n",
      "epoch:33 step:26234 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.026468]\n",
      "epoch:33 step:26235 [D loss: 0.044487, acc.: 99.22%] [G loss: 0.020440]\n",
      "epoch:33 step:26236 [D loss: 0.004297, acc.: 100.00%] [G loss: 0.281863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26237 [D loss: 0.022797, acc.: 99.22%] [G loss: 0.013176]\n",
      "epoch:33 step:26238 [D loss: 0.002486, acc.: 100.00%] [G loss: 0.046420]\n",
      "epoch:33 step:26239 [D loss: 0.023655, acc.: 100.00%] [G loss: 0.012705]\n",
      "epoch:33 step:26240 [D loss: 0.009875, acc.: 100.00%] [G loss: 0.039958]\n",
      "epoch:33 step:26241 [D loss: 0.058264, acc.: 98.44%] [G loss: 0.336484]\n",
      "epoch:33 step:26242 [D loss: 0.002095, acc.: 100.00%] [G loss: 0.589633]\n",
      "epoch:33 step:26243 [D loss: 0.023317, acc.: 100.00%] [G loss: 0.330439]\n",
      "epoch:33 step:26244 [D loss: 0.017698, acc.: 100.00%] [G loss: 0.095438]\n",
      "epoch:33 step:26245 [D loss: 0.040470, acc.: 99.22%] [G loss: 0.163045]\n",
      "epoch:33 step:26246 [D loss: 0.037654, acc.: 98.44%] [G loss: 0.032148]\n",
      "epoch:33 step:26247 [D loss: 0.000793, acc.: 100.00%] [G loss: 0.001387]\n",
      "epoch:33 step:26248 [D loss: 0.012109, acc.: 100.00%] [G loss: 0.015391]\n",
      "epoch:33 step:26249 [D loss: 0.002065, acc.: 100.00%] [G loss: 0.020805]\n",
      "epoch:33 step:26250 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.000564]\n",
      "epoch:33 step:26251 [D loss: 0.011644, acc.: 100.00%] [G loss: 0.010596]\n",
      "epoch:33 step:26252 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.019334]\n",
      "epoch:33 step:26253 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.015126]\n",
      "epoch:33 step:26254 [D loss: 0.004291, acc.: 100.00%] [G loss: 0.006278]\n",
      "epoch:33 step:26255 [D loss: 0.003639, acc.: 100.00%] [G loss: 0.004705]\n",
      "epoch:33 step:26256 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.037148]\n",
      "epoch:33 step:26257 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:33 step:26258 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.001638]\n",
      "epoch:33 step:26259 [D loss: 0.001169, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:33 step:26260 [D loss: 0.003762, acc.: 100.00%] [G loss: 0.002101]\n",
      "epoch:33 step:26261 [D loss: 0.001430, acc.: 100.00%] [G loss: 0.002809]\n",
      "epoch:33 step:26262 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.003499]\n",
      "epoch:33 step:26263 [D loss: 0.004026, acc.: 100.00%] [G loss: 0.002595]\n",
      "epoch:33 step:26264 [D loss: 0.001453, acc.: 100.00%] [G loss: 0.002441]\n",
      "epoch:33 step:26265 [D loss: 0.000711, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:33 step:26266 [D loss: 0.000935, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:33 step:26267 [D loss: 0.007498, acc.: 100.00%] [G loss: 0.000821]\n",
      "epoch:33 step:26268 [D loss: 0.001272, acc.: 100.00%] [G loss: 0.013332]\n",
      "epoch:33 step:26269 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.000732]\n",
      "epoch:33 step:26270 [D loss: 0.003905, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:33 step:26271 [D loss: 0.003180, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:33 step:26272 [D loss: 0.028500, acc.: 98.44%] [G loss: 0.000579]\n",
      "epoch:33 step:26273 [D loss: 0.178059, acc.: 94.53%] [G loss: 0.033105]\n",
      "epoch:33 step:26274 [D loss: 0.024385, acc.: 99.22%] [G loss: 0.277135]\n",
      "epoch:33 step:26275 [D loss: 0.085031, acc.: 96.88%] [G loss: 2.987188]\n",
      "epoch:33 step:26276 [D loss: 0.001599, acc.: 100.00%] [G loss: 0.227260]\n",
      "epoch:33 step:26277 [D loss: 0.032961, acc.: 99.22%] [G loss: 0.062757]\n",
      "epoch:33 step:26278 [D loss: 0.019823, acc.: 99.22%] [G loss: 0.022085]\n",
      "epoch:33 step:26279 [D loss: 0.005770, acc.: 100.00%] [G loss: 0.011731]\n",
      "epoch:33 step:26280 [D loss: 0.003482, acc.: 100.00%] [G loss: 0.015718]\n",
      "epoch:33 step:26281 [D loss: 0.070487, acc.: 98.44%] [G loss: 0.047906]\n",
      "epoch:33 step:26282 [D loss: 0.106822, acc.: 94.53%] [G loss: 0.028066]\n",
      "epoch:33 step:26283 [D loss: 0.021448, acc.: 100.00%] [G loss: 0.087115]\n",
      "epoch:33 step:26284 [D loss: 0.024878, acc.: 99.22%] [G loss: 0.044483]\n",
      "epoch:33 step:26285 [D loss: 0.005361, acc.: 100.00%] [G loss: 0.020693]\n",
      "epoch:33 step:26286 [D loss: 0.121494, acc.: 97.66%] [G loss: 0.484407]\n",
      "epoch:33 step:26287 [D loss: 0.010062, acc.: 100.00%] [G loss: 1.682662]\n",
      "epoch:33 step:26288 [D loss: 0.266765, acc.: 92.19%] [G loss: 0.087263]\n",
      "epoch:33 step:26289 [D loss: 0.286577, acc.: 83.59%] [G loss: 3.747047]\n",
      "epoch:33 step:26290 [D loss: 0.155060, acc.: 91.41%] [G loss: 3.672564]\n",
      "epoch:33 step:26291 [D loss: 0.019887, acc.: 100.00%] [G loss: 3.154660]\n",
      "epoch:33 step:26292 [D loss: 0.020232, acc.: 100.00%] [G loss: 1.723447]\n",
      "epoch:33 step:26293 [D loss: 0.090163, acc.: 96.09%] [G loss: 0.959879]\n",
      "epoch:33 step:26294 [D loss: 0.114668, acc.: 95.31%] [G loss: 0.165474]\n",
      "epoch:33 step:26295 [D loss: 0.008262, acc.: 100.00%] [G loss: 2.951606]\n",
      "epoch:33 step:26296 [D loss: 0.029560, acc.: 99.22%] [G loss: 1.853278]\n",
      "epoch:33 step:26297 [D loss: 0.416488, acc.: 79.69%] [G loss: 0.521313]\n",
      "epoch:33 step:26298 [D loss: 0.207834, acc.: 91.41%] [G loss: 2.964658]\n",
      "epoch:33 step:26299 [D loss: 0.222537, acc.: 89.84%] [G loss: 2.138408]\n",
      "epoch:33 step:26300 [D loss: 0.199387, acc.: 95.31%] [G loss: 4.054491]\n",
      "epoch:33 step:26301 [D loss: 0.020499, acc.: 99.22%] [G loss: 4.273946]\n",
      "epoch:33 step:26302 [D loss: 0.099059, acc.: 97.66%] [G loss: 3.231212]\n",
      "epoch:33 step:26303 [D loss: 0.050554, acc.: 99.22%] [G loss: 3.718323]\n",
      "epoch:33 step:26304 [D loss: 0.028691, acc.: 99.22%] [G loss: 0.099164]\n",
      "epoch:33 step:26305 [D loss: 0.073613, acc.: 96.88%] [G loss: 0.059151]\n",
      "epoch:33 step:26306 [D loss: 0.015457, acc.: 100.00%] [G loss: 2.333725]\n",
      "epoch:33 step:26307 [D loss: 0.003675, acc.: 100.00%] [G loss: 1.602786]\n",
      "epoch:33 step:26308 [D loss: 0.086976, acc.: 98.44%] [G loss: 0.853744]\n",
      "epoch:33 step:26309 [D loss: 0.012800, acc.: 100.00%] [G loss: 0.614233]\n",
      "epoch:33 step:26310 [D loss: 0.010406, acc.: 100.00%] [G loss: 0.250300]\n",
      "epoch:33 step:26311 [D loss: 0.006029, acc.: 100.00%] [G loss: 0.161935]\n",
      "epoch:33 step:26312 [D loss: 0.123932, acc.: 97.66%] [G loss: 0.908032]\n",
      "epoch:33 step:26313 [D loss: 0.002969, acc.: 100.00%] [G loss: 2.097554]\n",
      "epoch:33 step:26314 [D loss: 0.003827, acc.: 100.00%] [G loss: 1.976218]\n",
      "epoch:33 step:26315 [D loss: 0.053303, acc.: 99.22%] [G loss: 0.504919]\n",
      "epoch:33 step:26316 [D loss: 0.537298, acc.: 74.22%] [G loss: 6.116160]\n",
      "epoch:33 step:26317 [D loss: 0.694243, acc.: 69.53%] [G loss: 4.665251]\n",
      "epoch:33 step:26318 [D loss: 0.056263, acc.: 98.44%] [G loss: 3.631105]\n",
      "epoch:33 step:26319 [D loss: 0.046796, acc.: 99.22%] [G loss: 3.009108]\n",
      "epoch:33 step:26320 [D loss: 0.095786, acc.: 96.88%] [G loss: 2.892993]\n",
      "epoch:33 step:26321 [D loss: 0.040955, acc.: 99.22%] [G loss: 0.163940]\n",
      "epoch:33 step:26322 [D loss: 0.012567, acc.: 99.22%] [G loss: 0.185863]\n",
      "epoch:33 step:26323 [D loss: 0.007683, acc.: 100.00%] [G loss: 4.292698]\n",
      "epoch:33 step:26324 [D loss: 0.032519, acc.: 99.22%] [G loss: 3.767048]\n",
      "epoch:33 step:26325 [D loss: 0.007459, acc.: 100.00%] [G loss: 3.148814]\n",
      "epoch:33 step:26326 [D loss: 0.027033, acc.: 100.00%] [G loss: 2.928349]\n",
      "epoch:33 step:26327 [D loss: 0.007900, acc.: 100.00%] [G loss: 0.288311]\n",
      "epoch:33 step:26328 [D loss: 0.005555, acc.: 100.00%] [G loss: 0.010239]\n",
      "epoch:33 step:26329 [D loss: 0.013023, acc.: 99.22%] [G loss: 2.394743]\n",
      "epoch:33 step:26330 [D loss: 0.029610, acc.: 100.00%] [G loss: 1.823036]\n",
      "epoch:33 step:26331 [D loss: 0.025711, acc.: 100.00%] [G loss: 0.006467]\n",
      "epoch:33 step:26332 [D loss: 0.005361, acc.: 100.00%] [G loss: 1.875433]\n",
      "epoch:33 step:26333 [D loss: 0.002980, acc.: 100.00%] [G loss: 0.994123]\n",
      "epoch:33 step:26334 [D loss: 0.016891, acc.: 100.00%] [G loss: 0.901449]\n",
      "epoch:33 step:26335 [D loss: 0.005908, acc.: 100.00%] [G loss: 0.442899]\n",
      "epoch:33 step:26336 [D loss: 0.006087, acc.: 100.00%] [G loss: 0.529699]\n",
      "epoch:33 step:26337 [D loss: 0.001609, acc.: 100.00%] [G loss: 0.241128]\n",
      "epoch:33 step:26338 [D loss: 0.014781, acc.: 99.22%] [G loss: 0.484898]\n",
      "epoch:33 step:26339 [D loss: 0.036778, acc.: 100.00%] [G loss: 0.226726]\n",
      "epoch:33 step:26340 [D loss: 0.021123, acc.: 100.00%] [G loss: 0.076559]\n",
      "epoch:33 step:26341 [D loss: 0.014954, acc.: 99.22%] [G loss: 0.727139]\n",
      "epoch:33 step:26342 [D loss: 0.002160, acc.: 100.00%] [G loss: 0.365845]\n",
      "epoch:33 step:26343 [D loss: 0.001725, acc.: 100.00%] [G loss: 0.317877]\n",
      "epoch:33 step:26344 [D loss: 0.005048, acc.: 100.00%] [G loss: 0.302195]\n",
      "epoch:33 step:26345 [D loss: 0.006149, acc.: 100.00%] [G loss: 0.189573]\n",
      "epoch:33 step:26346 [D loss: 0.006989, acc.: 100.00%] [G loss: 0.100146]\n",
      "epoch:33 step:26347 [D loss: 0.244468, acc.: 89.06%] [G loss: 4.918302]\n",
      "epoch:33 step:26348 [D loss: 0.395675, acc.: 80.47%] [G loss: 4.379363]\n",
      "epoch:33 step:26349 [D loss: 0.006603, acc.: 100.00%] [G loss: 3.402661]\n",
      "epoch:33 step:26350 [D loss: 0.082752, acc.: 96.09%] [G loss: 2.092297]\n",
      "epoch:33 step:26351 [D loss: 0.074538, acc.: 99.22%] [G loss: 2.781400]\n",
      "epoch:33 step:26352 [D loss: 0.012985, acc.: 100.00%] [G loss: 4.008733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26353 [D loss: 0.095000, acc.: 96.09%] [G loss: 3.061481]\n",
      "epoch:33 step:26354 [D loss: 0.164682, acc.: 92.19%] [G loss: 6.357007]\n",
      "epoch:33 step:26355 [D loss: 0.013708, acc.: 99.22%] [G loss: 7.333183]\n",
      "epoch:33 step:26356 [D loss: 0.056086, acc.: 98.44%] [G loss: 6.842884]\n",
      "epoch:33 step:26357 [D loss: 0.028630, acc.: 98.44%] [G loss: 6.597258]\n",
      "epoch:33 step:26358 [D loss: 0.018120, acc.: 99.22%] [G loss: 6.027425]\n",
      "epoch:33 step:26359 [D loss: 0.015215, acc.: 100.00%] [G loss: 5.812639]\n",
      "epoch:33 step:26360 [D loss: 0.024450, acc.: 100.00%] [G loss: 5.653235]\n",
      "epoch:33 step:26361 [D loss: 0.024744, acc.: 100.00%] [G loss: 5.485732]\n",
      "epoch:33 step:26362 [D loss: 0.011549, acc.: 100.00%] [G loss: 1.757019]\n",
      "epoch:33 step:26363 [D loss: 0.011337, acc.: 100.00%] [G loss: 4.697506]\n",
      "epoch:33 step:26364 [D loss: 0.131301, acc.: 96.09%] [G loss: 2.885417]\n",
      "epoch:33 step:26365 [D loss: 0.010833, acc.: 100.00%] [G loss: 0.513851]\n",
      "epoch:33 step:26366 [D loss: 0.024343, acc.: 97.66%] [G loss: 6.140994]\n",
      "epoch:33 step:26367 [D loss: 0.028729, acc.: 100.00%] [G loss: 3.818698]\n",
      "epoch:33 step:26368 [D loss: 0.019449, acc.: 99.22%] [G loss: 2.926519]\n",
      "epoch:33 step:26369 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.014666]\n",
      "epoch:33 step:26370 [D loss: 0.004557, acc.: 100.00%] [G loss: 0.034839]\n",
      "epoch:33 step:26371 [D loss: 0.061111, acc.: 98.44%] [G loss: 0.098388]\n",
      "epoch:33 step:26372 [D loss: 0.077259, acc.: 98.44%] [G loss: 5.276943]\n",
      "epoch:33 step:26373 [D loss: 0.021137, acc.: 99.22%] [G loss: 4.623390]\n",
      "epoch:33 step:26374 [D loss: 0.058704, acc.: 97.66%] [G loss: 2.201073]\n",
      "epoch:33 step:26375 [D loss: 0.005729, acc.: 100.00%] [G loss: 0.185155]\n",
      "epoch:33 step:26376 [D loss: 0.004878, acc.: 100.00%] [G loss: 0.968568]\n",
      "epoch:33 step:26377 [D loss: 0.135150, acc.: 97.66%] [G loss: 2.106425]\n",
      "epoch:33 step:26378 [D loss: 0.021965, acc.: 99.22%] [G loss: 1.469300]\n",
      "epoch:33 step:26379 [D loss: 0.034316, acc.: 99.22%] [G loss: 1.499107]\n",
      "epoch:33 step:26380 [D loss: 0.086441, acc.: 96.88%] [G loss: 0.214747]\n",
      "epoch:33 step:26381 [D loss: 0.002957, acc.: 100.00%] [G loss: 0.054165]\n",
      "epoch:33 step:26382 [D loss: 0.020547, acc.: 99.22%] [G loss: 0.012525]\n",
      "epoch:33 step:26383 [D loss: 0.002934, acc.: 100.00%] [G loss: 0.003046]\n",
      "epoch:33 step:26384 [D loss: 0.005240, acc.: 100.00%] [G loss: 0.003892]\n",
      "epoch:33 step:26385 [D loss: 0.003050, acc.: 100.00%] [G loss: 0.021061]\n",
      "epoch:33 step:26386 [D loss: 0.028988, acc.: 99.22%] [G loss: 0.003062]\n",
      "epoch:33 step:26387 [D loss: 0.003025, acc.: 100.00%] [G loss: 0.014279]\n",
      "epoch:33 step:26388 [D loss: 0.015619, acc.: 100.00%] [G loss: 0.002039]\n",
      "epoch:33 step:26389 [D loss: 0.000998, acc.: 100.00%] [G loss: 0.006936]\n",
      "epoch:33 step:26390 [D loss: 0.008116, acc.: 100.00%] [G loss: 0.029681]\n",
      "epoch:33 step:26391 [D loss: 0.005039, acc.: 100.00%] [G loss: 0.038915]\n",
      "epoch:33 step:26392 [D loss: 0.002283, acc.: 100.00%] [G loss: 0.579309]\n",
      "epoch:33 step:26393 [D loss: 0.006212, acc.: 100.00%] [G loss: 0.019706]\n",
      "epoch:33 step:26394 [D loss: 0.018664, acc.: 100.00%] [G loss: 0.116209]\n",
      "epoch:33 step:26395 [D loss: 0.017700, acc.: 99.22%] [G loss: 0.275102]\n",
      "epoch:33 step:26396 [D loss: 0.044682, acc.: 99.22%] [G loss: 0.258201]\n",
      "epoch:33 step:26397 [D loss: 0.005687, acc.: 100.00%] [G loss: 0.472207]\n",
      "epoch:33 step:26398 [D loss: 0.013338, acc.: 100.00%] [G loss: 0.046352]\n",
      "epoch:33 step:26399 [D loss: 0.003536, acc.: 100.00%] [G loss: 0.004477]\n",
      "epoch:33 step:26400 [D loss: 0.023449, acc.: 99.22%] [G loss: 0.028233]\n",
      "epoch:33 step:26401 [D loss: 0.004445, acc.: 100.00%] [G loss: 0.086944]\n",
      "epoch:33 step:26402 [D loss: 0.016979, acc.: 99.22%] [G loss: 2.236949]\n",
      "epoch:33 step:26403 [D loss: 0.036713, acc.: 100.00%] [G loss: 0.093511]\n",
      "epoch:33 step:26404 [D loss: 0.106736, acc.: 94.53%] [G loss: 2.251602]\n",
      "epoch:33 step:26405 [D loss: 0.038260, acc.: 98.44%] [G loss: 5.835898]\n",
      "epoch:33 step:26406 [D loss: 0.075323, acc.: 96.09%] [G loss: 3.062751]\n",
      "epoch:33 step:26407 [D loss: 0.000677, acc.: 100.00%] [G loss: 0.006896]\n",
      "epoch:33 step:26408 [D loss: 0.000857, acc.: 100.00%] [G loss: 0.036564]\n",
      "epoch:33 step:26409 [D loss: 0.002275, acc.: 100.00%] [G loss: 0.827148]\n",
      "epoch:33 step:26410 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.480133]\n",
      "epoch:33 step:26411 [D loss: 0.003495, acc.: 100.00%] [G loss: 0.183191]\n",
      "epoch:33 step:26412 [D loss: 0.000427, acc.: 100.00%] [G loss: 0.006454]\n",
      "epoch:33 step:26413 [D loss: 0.001638, acc.: 100.00%] [G loss: 0.000439]\n",
      "epoch:33 step:26414 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.002493]\n",
      "epoch:33 step:26415 [D loss: 0.005087, acc.: 100.00%] [G loss: 0.002303]\n",
      "epoch:33 step:26416 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000506]\n",
      "epoch:33 step:26417 [D loss: 0.000658, acc.: 100.00%] [G loss: 0.000593]\n",
      "epoch:33 step:26418 [D loss: 0.001291, acc.: 100.00%] [G loss: 0.039217]\n",
      "epoch:33 step:26419 [D loss: 0.009110, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:33 step:26420 [D loss: 0.089452, acc.: 96.88%] [G loss: 0.024288]\n",
      "epoch:33 step:26421 [D loss: 0.015708, acc.: 100.00%] [G loss: 0.405311]\n",
      "epoch:33 step:26422 [D loss: 0.299474, acc.: 87.50%] [G loss: 0.536369]\n",
      "epoch:33 step:26423 [D loss: 0.770593, acc.: 79.69%] [G loss: 1.838509]\n",
      "epoch:33 step:26424 [D loss: 1.062907, acc.: 71.09%] [G loss: 0.034850]\n",
      "epoch:33 step:26425 [D loss: 0.002535, acc.: 100.00%] [G loss: 0.501588]\n",
      "epoch:33 step:26426 [D loss: 0.110292, acc.: 96.09%] [G loss: 0.011977]\n",
      "epoch:33 step:26427 [D loss: 0.015731, acc.: 99.22%] [G loss: 0.122858]\n",
      "epoch:33 step:26428 [D loss: 0.000902, acc.: 100.00%] [G loss: 0.027110]\n",
      "epoch:33 step:26429 [D loss: 0.011514, acc.: 99.22%] [G loss: 0.005310]\n",
      "epoch:33 step:26430 [D loss: 0.000710, acc.: 100.00%] [G loss: 0.001111]\n",
      "epoch:33 step:26431 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.003036]\n",
      "epoch:33 step:26432 [D loss: 0.000908, acc.: 100.00%] [G loss: 3.990636]\n",
      "epoch:33 step:26433 [D loss: 0.019979, acc.: 100.00%] [G loss: 0.001358]\n",
      "epoch:33 step:26434 [D loss: 0.044706, acc.: 97.66%] [G loss: 4.437784]\n",
      "epoch:33 step:26435 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.023669]\n",
      "epoch:33 step:26436 [D loss: 0.001961, acc.: 100.00%] [G loss: 0.014870]\n",
      "epoch:33 step:26437 [D loss: 0.001900, acc.: 100.00%] [G loss: 0.038569]\n",
      "epoch:33 step:26438 [D loss: 0.075330, acc.: 97.66%] [G loss: 0.000113]\n",
      "epoch:33 step:26439 [D loss: 0.131402, acc.: 96.09%] [G loss: 0.186946]\n",
      "epoch:33 step:26440 [D loss: 0.002923, acc.: 100.00%] [G loss: 3.254251]\n",
      "epoch:33 step:26441 [D loss: 0.169220, acc.: 92.97%] [G loss: 4.386840]\n",
      "epoch:33 step:26442 [D loss: 0.008397, acc.: 100.00%] [G loss: 0.002825]\n",
      "epoch:33 step:26443 [D loss: 0.012868, acc.: 100.00%] [G loss: 0.000620]\n",
      "epoch:33 step:26444 [D loss: 0.022971, acc.: 100.00%] [G loss: 0.245394]\n",
      "epoch:33 step:26445 [D loss: 0.031993, acc.: 100.00%] [G loss: 0.116345]\n",
      "epoch:33 step:26446 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.081085]\n",
      "epoch:33 step:26447 [D loss: 0.005672, acc.: 100.00%] [G loss: 0.194012]\n",
      "epoch:33 step:26448 [D loss: 0.106028, acc.: 97.66%] [G loss: 2.479727]\n",
      "epoch:33 step:26449 [D loss: 0.590119, acc.: 76.56%] [G loss: 1.333342]\n",
      "epoch:33 step:26450 [D loss: 0.006840, acc.: 100.00%] [G loss: 5.333941]\n",
      "epoch:33 step:26451 [D loss: 0.113987, acc.: 92.97%] [G loss: 1.953929]\n",
      "epoch:33 step:26452 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.076151]\n",
      "epoch:33 step:26453 [D loss: 0.021474, acc.: 100.00%] [G loss: 0.193488]\n",
      "epoch:33 step:26454 [D loss: 0.008592, acc.: 99.22%] [G loss: 0.049356]\n",
      "epoch:33 step:26455 [D loss: 0.002620, acc.: 100.00%] [G loss: 0.016177]\n",
      "epoch:33 step:26456 [D loss: 0.004282, acc.: 100.00%] [G loss: 0.439155]\n",
      "epoch:33 step:26457 [D loss: 0.001112, acc.: 100.00%] [G loss: 0.034561]\n",
      "epoch:33 step:26458 [D loss: 0.004921, acc.: 100.00%] [G loss: 0.062577]\n",
      "epoch:33 step:26459 [D loss: 0.001928, acc.: 100.00%] [G loss: 1.134650]\n",
      "epoch:33 step:26460 [D loss: 0.044536, acc.: 99.22%] [G loss: 0.023306]\n",
      "epoch:33 step:26461 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.010879]\n",
      "epoch:33 step:26462 [D loss: 0.013953, acc.: 99.22%] [G loss: 0.002931]\n",
      "epoch:33 step:26463 [D loss: 0.010438, acc.: 100.00%] [G loss: 0.004518]\n",
      "epoch:33 step:26464 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.117661]\n",
      "epoch:33 step:26465 [D loss: 0.005322, acc.: 100.00%] [G loss: 0.020484]\n",
      "epoch:33 step:26466 [D loss: 0.011954, acc.: 100.00%] [G loss: 0.021128]\n",
      "epoch:33 step:26467 [D loss: 0.120364, acc.: 96.88%] [G loss: 3.232909]\n",
      "epoch:33 step:26468 [D loss: 0.045554, acc.: 99.22%] [G loss: 3.820217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:33 step:26469 [D loss: 0.692475, acc.: 75.00%] [G loss: 0.785566]\n",
      "epoch:33 step:26470 [D loss: 0.063227, acc.: 98.44%] [G loss: 4.526220]\n",
      "epoch:33 step:26471 [D loss: 0.106699, acc.: 97.66%] [G loss: 6.343880]\n",
      "epoch:33 step:26472 [D loss: 0.183363, acc.: 92.19%] [G loss: 2.905154]\n",
      "epoch:33 step:26473 [D loss: 0.080476, acc.: 98.44%] [G loss: 0.204245]\n",
      "epoch:33 step:26474 [D loss: 0.001805, acc.: 100.00%] [G loss: 5.796881]\n",
      "epoch:33 step:26475 [D loss: 0.055612, acc.: 96.88%] [G loss: 3.427897]\n",
      "epoch:33 step:26476 [D loss: 0.033002, acc.: 98.44%] [G loss: 1.120157]\n",
      "epoch:33 step:26477 [D loss: 0.010129, acc.: 100.00%] [G loss: 0.004086]\n",
      "epoch:33 step:26478 [D loss: 0.121079, acc.: 96.88%] [G loss: 0.050211]\n",
      "epoch:33 step:26479 [D loss: 0.000287, acc.: 100.00%] [G loss: 2.544087]\n",
      "epoch:33 step:26480 [D loss: 0.111460, acc.: 92.97%] [G loss: 0.003980]\n",
      "epoch:33 step:26481 [D loss: 0.005097, acc.: 100.00%] [G loss: 0.144372]\n",
      "epoch:33 step:26482 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.045107]\n",
      "epoch:33 step:26483 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.365333]\n",
      "epoch:33 step:26484 [D loss: 0.004895, acc.: 100.00%] [G loss: 0.001584]\n",
      "epoch:33 step:26485 [D loss: 0.002010, acc.: 100.00%] [G loss: 0.003751]\n",
      "epoch:33 step:26486 [D loss: 0.008224, acc.: 100.00%] [G loss: 0.039556]\n",
      "epoch:33 step:26487 [D loss: 0.000853, acc.: 100.00%] [G loss: 0.000194]\n",
      "epoch:33 step:26488 [D loss: 0.002123, acc.: 100.00%] [G loss: 0.002700]\n",
      "epoch:33 step:26489 [D loss: 0.008814, acc.: 100.00%] [G loss: 0.001085]\n",
      "epoch:33 step:26490 [D loss: 0.000732, acc.: 100.00%] [G loss: 0.001468]\n",
      "epoch:33 step:26491 [D loss: 0.084196, acc.: 95.31%] [G loss: 0.518190]\n",
      "epoch:33 step:26492 [D loss: 0.002444, acc.: 100.00%] [G loss: 4.566636]\n",
      "epoch:33 step:26493 [D loss: 0.110582, acc.: 96.88%] [G loss: 0.052664]\n",
      "epoch:33 step:26494 [D loss: 0.071234, acc.: 99.22%] [G loss: 2.254017]\n",
      "epoch:33 step:26495 [D loss: 0.001528, acc.: 100.00%] [G loss: 1.409055]\n",
      "epoch:33 step:26496 [D loss: 0.258112, acc.: 88.28%] [G loss: 5.613482]\n",
      "epoch:33 step:26497 [D loss: 2.009411, acc.: 52.34%] [G loss: 1.454412]\n",
      "epoch:33 step:26498 [D loss: 0.314380, acc.: 83.59%] [G loss: 7.188795]\n",
      "epoch:33 step:26499 [D loss: 0.001175, acc.: 100.00%] [G loss: 8.377977]\n",
      "epoch:33 step:26500 [D loss: 0.237641, acc.: 87.50%] [G loss: 0.019044]\n",
      "epoch:33 step:26501 [D loss: 0.044957, acc.: 98.44%] [G loss: 4.228298]\n",
      "epoch:33 step:26502 [D loss: 0.005293, acc.: 100.00%] [G loss: 3.543710]\n",
      "epoch:33 step:26503 [D loss: 0.002193, acc.: 100.00%] [G loss: 1.757360]\n",
      "epoch:33 step:26504 [D loss: 0.030453, acc.: 100.00%] [G loss: 0.027315]\n",
      "epoch:33 step:26505 [D loss: 0.004466, acc.: 100.00%] [G loss: 1.263707]\n",
      "epoch:33 step:26506 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.946287]\n",
      "epoch:33 step:26507 [D loss: 0.001071, acc.: 100.00%] [G loss: 0.528360]\n",
      "epoch:33 step:26508 [D loss: 0.008642, acc.: 100.00%] [G loss: 0.603175]\n",
      "epoch:33 step:26509 [D loss: 0.003913, acc.: 100.00%] [G loss: 0.212761]\n",
      "epoch:33 step:26510 [D loss: 0.029267, acc.: 99.22%] [G loss: 0.037398]\n",
      "epoch:33 step:26511 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.358202]\n",
      "epoch:33 step:26512 [D loss: 0.011815, acc.: 100.00%] [G loss: 0.037684]\n",
      "epoch:33 step:26513 [D loss: 0.011720, acc.: 99.22%] [G loss: 0.111938]\n",
      "epoch:33 step:26514 [D loss: 0.004668, acc.: 100.00%] [G loss: 0.056538]\n",
      "epoch:33 step:26515 [D loss: 0.010425, acc.: 100.00%] [G loss: 0.194764]\n",
      "epoch:33 step:26516 [D loss: 0.001812, acc.: 100.00%] [G loss: 0.025233]\n",
      "epoch:33 step:26517 [D loss: 0.010101, acc.: 100.00%] [G loss: 0.015698]\n",
      "epoch:33 step:26518 [D loss: 0.001398, acc.: 100.00%] [G loss: 0.023998]\n",
      "epoch:33 step:26519 [D loss: 0.023964, acc.: 100.00%] [G loss: 0.118183]\n",
      "epoch:33 step:26520 [D loss: 0.017909, acc.: 100.00%] [G loss: 0.126712]\n",
      "epoch:33 step:26521 [D loss: 0.015052, acc.: 100.00%] [G loss: 0.058948]\n",
      "epoch:33 step:26522 [D loss: 0.013550, acc.: 100.00%] [G loss: 0.078008]\n",
      "epoch:33 step:26523 [D loss: 0.009101, acc.: 100.00%] [G loss: 0.059154]\n",
      "epoch:33 step:26524 [D loss: 0.001683, acc.: 100.00%] [G loss: 0.002497]\n",
      "epoch:33 step:26525 [D loss: 0.000829, acc.: 100.00%] [G loss: 0.015982]\n",
      "epoch:33 step:26526 [D loss: 0.001121, acc.: 100.00%] [G loss: 0.036252]\n",
      "epoch:33 step:26527 [D loss: 0.003938, acc.: 100.00%] [G loss: 0.209093]\n",
      "epoch:33 step:26528 [D loss: 0.001969, acc.: 100.00%] [G loss: 0.028125]\n",
      "epoch:33 step:26529 [D loss: 0.001379, acc.: 100.00%] [G loss: 0.026382]\n",
      "epoch:33 step:26530 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.002490]\n",
      "epoch:33 step:26531 [D loss: 0.073653, acc.: 98.44%] [G loss: 0.557673]\n",
      "epoch:33 step:26532 [D loss: 0.000676, acc.: 100.00%] [G loss: 1.026712]\n",
      "epoch:33 step:26533 [D loss: 0.008299, acc.: 100.00%] [G loss: 0.780012]\n",
      "epoch:33 step:26534 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.154035]\n",
      "epoch:33 step:26535 [D loss: 0.686088, acc.: 68.75%] [G loss: 8.310785]\n",
      "epoch:33 step:26536 [D loss: 0.877734, acc.: 66.41%] [G loss: 6.398050]\n",
      "epoch:33 step:26537 [D loss: 0.031784, acc.: 99.22%] [G loss: 4.387269]\n",
      "epoch:33 step:26538 [D loss: 0.018573, acc.: 100.00%] [G loss: 3.857773]\n",
      "epoch:33 step:26539 [D loss: 0.090318, acc.: 98.44%] [G loss: 4.987592]\n",
      "epoch:33 step:26540 [D loss: 0.003762, acc.: 100.00%] [G loss: 5.172704]\n",
      "epoch:33 step:26541 [D loss: 0.026119, acc.: 99.22%] [G loss: 3.176761]\n",
      "epoch:33 step:26542 [D loss: 0.042109, acc.: 100.00%] [G loss: 2.994421]\n",
      "epoch:33 step:26543 [D loss: 0.088780, acc.: 97.66%] [G loss: 2.488971]\n",
      "epoch:33 step:26544 [D loss: 1.385715, acc.: 50.78%] [G loss: 7.493032]\n",
      "epoch:33 step:26545 [D loss: 0.465598, acc.: 80.47%] [G loss: 3.440629]\n",
      "epoch:33 step:26546 [D loss: 0.399437, acc.: 86.72%] [G loss: 5.168766]\n",
      "epoch:33 step:26547 [D loss: 0.025765, acc.: 100.00%] [G loss: 4.431359]\n",
      "epoch:33 step:26548 [D loss: 0.039233, acc.: 100.00%] [G loss: 4.249095]\n",
      "epoch:33 step:26549 [D loss: 0.020528, acc.: 100.00%] [G loss: 3.954994]\n",
      "epoch:33 step:26550 [D loss: 0.048643, acc.: 98.44%] [G loss: 3.717465]\n",
      "epoch:33 step:26551 [D loss: 0.028651, acc.: 100.00%] [G loss: 2.747952]\n",
      "epoch:33 step:26552 [D loss: 0.076260, acc.: 99.22%] [G loss: 3.599337]\n",
      "epoch:33 step:26553 [D loss: 0.024134, acc.: 99.22%] [G loss: 5.443759]\n",
      "epoch:33 step:26554 [D loss: 0.035019, acc.: 99.22%] [G loss: 4.038009]\n",
      "epoch:34 step:26555 [D loss: 0.171130, acc.: 93.75%] [G loss: 3.753723]\n",
      "epoch:34 step:26556 [D loss: 0.032375, acc.: 100.00%] [G loss: 4.121489]\n",
      "epoch:34 step:26557 [D loss: 0.274363, acc.: 85.94%] [G loss: 4.918393]\n",
      "epoch:34 step:26558 [D loss: 0.005660, acc.: 100.00%] [G loss: 5.528495]\n",
      "epoch:34 step:26559 [D loss: 0.072685, acc.: 97.66%] [G loss: 2.125069]\n",
      "epoch:34 step:26560 [D loss: 0.411646, acc.: 82.81%] [G loss: 7.535410]\n",
      "epoch:34 step:26561 [D loss: 1.302077, acc.: 62.50%] [G loss: 3.571328]\n",
      "epoch:34 step:26562 [D loss: 0.178333, acc.: 90.62%] [G loss: 5.422274]\n",
      "epoch:34 step:26563 [D loss: 0.021652, acc.: 100.00%] [G loss: 3.617865]\n",
      "epoch:34 step:26564 [D loss: 0.151843, acc.: 92.19%] [G loss: 3.222237]\n",
      "epoch:34 step:26565 [D loss: 0.053150, acc.: 100.00%] [G loss: 0.330021]\n",
      "epoch:34 step:26566 [D loss: 0.015830, acc.: 100.00%] [G loss: 3.689116]\n",
      "epoch:34 step:26567 [D loss: 0.358896, acc.: 82.03%] [G loss: 5.619084]\n",
      "epoch:34 step:26568 [D loss: 0.348406, acc.: 84.38%] [G loss: 4.877515]\n",
      "epoch:34 step:26569 [D loss: 0.112229, acc.: 98.44%] [G loss: 3.442093]\n",
      "epoch:34 step:26570 [D loss: 0.179348, acc.: 90.62%] [G loss: 4.554843]\n",
      "epoch:34 step:26571 [D loss: 0.039327, acc.: 100.00%] [G loss: 5.233249]\n",
      "epoch:34 step:26572 [D loss: 0.141945, acc.: 93.75%] [G loss: 3.732238]\n",
      "epoch:34 step:26573 [D loss: 0.053197, acc.: 100.00%] [G loss: 4.062456]\n",
      "epoch:34 step:26574 [D loss: 0.027181, acc.: 100.00%] [G loss: 0.876345]\n",
      "epoch:34 step:26575 [D loss: 0.063729, acc.: 98.44%] [G loss: 3.845487]\n",
      "epoch:34 step:26576 [D loss: 0.078055, acc.: 99.22%] [G loss: 3.990168]\n",
      "epoch:34 step:26577 [D loss: 0.046309, acc.: 99.22%] [G loss: 4.849297]\n",
      "epoch:34 step:26578 [D loss: 0.034080, acc.: 99.22%] [G loss: 4.627904]\n",
      "epoch:34 step:26579 [D loss: 0.259240, acc.: 91.41%] [G loss: 2.332653]\n",
      "epoch:34 step:26580 [D loss: 0.127819, acc.: 96.88%] [G loss: 4.503554]\n",
      "epoch:34 step:26581 [D loss: 0.007198, acc.: 100.00%] [G loss: 6.121108]\n",
      "epoch:34 step:26582 [D loss: 0.349562, acc.: 83.59%] [G loss: 4.163746]\n",
      "epoch:34 step:26583 [D loss: 0.183350, acc.: 92.19%] [G loss: 5.541245]\n",
      "epoch:34 step:26584 [D loss: 0.059966, acc.: 97.66%] [G loss: 5.507506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26585 [D loss: 0.416272, acc.: 85.16%] [G loss: 1.131136]\n",
      "epoch:34 step:26586 [D loss: 0.155307, acc.: 96.09%] [G loss: 1.188036]\n",
      "epoch:34 step:26587 [D loss: 0.003594, acc.: 100.00%] [G loss: 3.765376]\n",
      "epoch:34 step:26588 [D loss: 0.002346, acc.: 100.00%] [G loss: 6.036245]\n",
      "epoch:34 step:26589 [D loss: 0.139798, acc.: 92.97%] [G loss: 5.093509]\n",
      "epoch:34 step:26590 [D loss: 0.027657, acc.: 100.00%] [G loss: 3.994636]\n",
      "epoch:34 step:26591 [D loss: 0.009616, acc.: 100.00%] [G loss: 2.408393]\n",
      "epoch:34 step:26592 [D loss: 0.022695, acc.: 100.00%] [G loss: 0.366547]\n",
      "epoch:34 step:26593 [D loss: 0.011380, acc.: 100.00%] [G loss: 1.663268]\n",
      "epoch:34 step:26594 [D loss: 0.017011, acc.: 100.00%] [G loss: 0.438476]\n",
      "epoch:34 step:26595 [D loss: 0.016237, acc.: 100.00%] [G loss: 0.878997]\n",
      "epoch:34 step:26596 [D loss: 0.022020, acc.: 99.22%] [G loss: 0.657060]\n",
      "epoch:34 step:26597 [D loss: 0.011756, acc.: 99.22%] [G loss: 0.921096]\n",
      "epoch:34 step:26598 [D loss: 0.013908, acc.: 100.00%] [G loss: 0.073895]\n",
      "epoch:34 step:26599 [D loss: 0.028604, acc.: 99.22%] [G loss: 0.064403]\n",
      "epoch:34 step:26600 [D loss: 0.021474, acc.: 100.00%] [G loss: 0.102790]\n",
      "epoch:34 step:26601 [D loss: 0.033333, acc.: 100.00%] [G loss: 0.167351]\n",
      "epoch:34 step:26602 [D loss: 0.024744, acc.: 99.22%] [G loss: 0.037478]\n",
      "epoch:34 step:26603 [D loss: 0.003888, acc.: 100.00%] [G loss: 0.142255]\n",
      "epoch:34 step:26604 [D loss: 0.058173, acc.: 99.22%] [G loss: 1.142163]\n",
      "epoch:34 step:26605 [D loss: 0.092141, acc.: 97.66%] [G loss: 1.005118]\n",
      "epoch:34 step:26606 [D loss: 0.036818, acc.: 99.22%] [G loss: 0.019651]\n",
      "epoch:34 step:26607 [D loss: 0.001218, acc.: 100.00%] [G loss: 0.038613]\n",
      "epoch:34 step:26608 [D loss: 0.013861, acc.: 100.00%] [G loss: 4.183045]\n",
      "epoch:34 step:26609 [D loss: 0.020185, acc.: 99.22%] [G loss: 0.282862]\n",
      "epoch:34 step:26610 [D loss: 0.017451, acc.: 99.22%] [G loss: 0.059990]\n",
      "epoch:34 step:26611 [D loss: 0.001611, acc.: 100.00%] [G loss: 0.007098]\n",
      "epoch:34 step:26612 [D loss: 0.605237, acc.: 72.66%] [G loss: 8.755465]\n",
      "epoch:34 step:26613 [D loss: 1.867375, acc.: 53.12%] [G loss: 2.554162]\n",
      "epoch:34 step:26614 [D loss: 0.183394, acc.: 94.53%] [G loss: 1.492067]\n",
      "epoch:34 step:26615 [D loss: 0.122440, acc.: 95.31%] [G loss: 0.787650]\n",
      "epoch:34 step:26616 [D loss: 0.004827, acc.: 100.00%] [G loss: 1.350444]\n",
      "epoch:34 step:26617 [D loss: 0.129307, acc.: 97.66%] [G loss: 0.862010]\n",
      "epoch:34 step:26618 [D loss: 0.027304, acc.: 99.22%] [G loss: 1.160864]\n",
      "epoch:34 step:26619 [D loss: 4.509987, acc.: 24.22%] [G loss: 7.219810]\n",
      "epoch:34 step:26620 [D loss: 0.913057, acc.: 60.94%] [G loss: 6.702275]\n",
      "epoch:34 step:26621 [D loss: 0.393933, acc.: 78.91%] [G loss: 0.617826]\n",
      "epoch:34 step:26622 [D loss: 0.054687, acc.: 98.44%] [G loss: 0.700084]\n",
      "epoch:34 step:26623 [D loss: 0.049161, acc.: 99.22%] [G loss: 3.869754]\n",
      "epoch:34 step:26624 [D loss: 0.029688, acc.: 99.22%] [G loss: 3.902291]\n",
      "epoch:34 step:26625 [D loss: 0.073097, acc.: 98.44%] [G loss: 3.549855]\n",
      "epoch:34 step:26626 [D loss: 0.019836, acc.: 100.00%] [G loss: 3.273977]\n",
      "epoch:34 step:26627 [D loss: 0.050526, acc.: 98.44%] [G loss: 2.640500]\n",
      "epoch:34 step:26628 [D loss: 0.017468, acc.: 99.22%] [G loss: 1.833493]\n",
      "epoch:34 step:26629 [D loss: 0.021301, acc.: 100.00%] [G loss: 1.225404]\n",
      "epoch:34 step:26630 [D loss: 0.313877, acc.: 86.72%] [G loss: 2.824053]\n",
      "epoch:34 step:26631 [D loss: 0.415145, acc.: 80.47%] [G loss: 1.662608]\n",
      "epoch:34 step:26632 [D loss: 0.123821, acc.: 95.31%] [G loss: 0.955306]\n",
      "epoch:34 step:26633 [D loss: 0.113483, acc.: 96.88%] [G loss: 1.958055]\n",
      "epoch:34 step:26634 [D loss: 0.133398, acc.: 94.53%] [G loss: 1.528828]\n",
      "epoch:34 step:26635 [D loss: 0.031673, acc.: 99.22%] [G loss: 0.737617]\n",
      "epoch:34 step:26636 [D loss: 0.211752, acc.: 92.19%] [G loss: 1.741471]\n",
      "epoch:34 step:26637 [D loss: 0.086202, acc.: 97.66%] [G loss: 2.323179]\n",
      "epoch:34 step:26638 [D loss: 0.093409, acc.: 99.22%] [G loss: 2.456022]\n",
      "epoch:34 step:26639 [D loss: 0.475161, acc.: 77.34%] [G loss: 1.683794]\n",
      "epoch:34 step:26640 [D loss: 0.045896, acc.: 100.00%] [G loss: 0.702768]\n",
      "epoch:34 step:26641 [D loss: 0.035018, acc.: 100.00%] [G loss: 2.494642]\n",
      "epoch:34 step:26642 [D loss: 0.073603, acc.: 97.66%] [G loss: 0.309644]\n",
      "epoch:34 step:26643 [D loss: 0.080453, acc.: 97.66%] [G loss: 1.875167]\n",
      "epoch:34 step:26644 [D loss: 0.293050, acc.: 86.72%] [G loss: 0.896786]\n",
      "epoch:34 step:26645 [D loss: 0.028134, acc.: 99.22%] [G loss: 0.056697]\n",
      "epoch:34 step:26646 [D loss: 0.027893, acc.: 100.00%] [G loss: 0.767094]\n",
      "epoch:34 step:26647 [D loss: 0.015974, acc.: 100.00%] [G loss: 0.399621]\n",
      "epoch:34 step:26648 [D loss: 0.007143, acc.: 100.00%] [G loss: 0.096005]\n",
      "epoch:34 step:26649 [D loss: 0.019381, acc.: 100.00%] [G loss: 1.232823]\n",
      "epoch:34 step:26650 [D loss: 0.024044, acc.: 100.00%] [G loss: 0.337219]\n",
      "epoch:34 step:26651 [D loss: 0.018586, acc.: 100.00%] [G loss: 0.277732]\n",
      "epoch:34 step:26652 [D loss: 0.009337, acc.: 100.00%] [G loss: 0.321460]\n",
      "epoch:34 step:26653 [D loss: 0.061541, acc.: 98.44%] [G loss: 0.059941]\n",
      "epoch:34 step:26654 [D loss: 0.005339, acc.: 100.00%] [G loss: 0.026302]\n",
      "epoch:34 step:26655 [D loss: 0.018402, acc.: 100.00%] [G loss: 0.011498]\n",
      "epoch:34 step:26656 [D loss: 0.007573, acc.: 100.00%] [G loss: 0.031266]\n",
      "epoch:34 step:26657 [D loss: 0.025081, acc.: 100.00%] [G loss: 0.010414]\n",
      "epoch:34 step:26658 [D loss: 0.009809, acc.: 100.00%] [G loss: 0.021300]\n",
      "epoch:34 step:26659 [D loss: 0.100372, acc.: 96.09%] [G loss: 0.254187]\n",
      "epoch:34 step:26660 [D loss: 0.002819, acc.: 100.00%] [G loss: 0.589830]\n",
      "epoch:34 step:26661 [D loss: 0.053990, acc.: 98.44%] [G loss: 0.610587]\n",
      "epoch:34 step:26662 [D loss: 0.018078, acc.: 100.00%] [G loss: 0.068495]\n",
      "epoch:34 step:26663 [D loss: 0.006404, acc.: 100.00%] [G loss: 0.011594]\n",
      "epoch:34 step:26664 [D loss: 0.011249, acc.: 100.00%] [G loss: 0.016723]\n",
      "epoch:34 step:26665 [D loss: 0.004600, acc.: 100.00%] [G loss: 0.072812]\n",
      "epoch:34 step:26666 [D loss: 0.003353, acc.: 100.00%] [G loss: 0.014547]\n",
      "epoch:34 step:26667 [D loss: 0.003506, acc.: 100.00%] [G loss: 0.076700]\n",
      "epoch:34 step:26668 [D loss: 0.017916, acc.: 99.22%] [G loss: 0.038864]\n",
      "epoch:34 step:26669 [D loss: 0.009463, acc.: 100.00%] [G loss: 0.009146]\n",
      "epoch:34 step:26670 [D loss: 0.003738, acc.: 100.00%] [G loss: 0.005306]\n",
      "epoch:34 step:26671 [D loss: 0.003066, acc.: 100.00%] [G loss: 0.006274]\n",
      "epoch:34 step:26672 [D loss: 0.004384, acc.: 100.00%] [G loss: 0.003512]\n",
      "epoch:34 step:26673 [D loss: 0.001860, acc.: 100.00%] [G loss: 0.004688]\n",
      "epoch:34 step:26674 [D loss: 0.001299, acc.: 100.00%] [G loss: 0.010366]\n",
      "epoch:34 step:26675 [D loss: 0.048346, acc.: 99.22%] [G loss: 0.025928]\n",
      "epoch:34 step:26676 [D loss: 0.006489, acc.: 100.00%] [G loss: 0.328553]\n",
      "epoch:34 step:26677 [D loss: 0.012663, acc.: 100.00%] [G loss: 5.568901]\n",
      "epoch:34 step:26678 [D loss: 0.011366, acc.: 100.00%] [G loss: 0.017686]\n",
      "epoch:34 step:26679 [D loss: 0.007834, acc.: 100.00%] [G loss: 2.473787]\n",
      "epoch:34 step:26680 [D loss: 0.024174, acc.: 100.00%] [G loss: 0.043700]\n",
      "epoch:34 step:26681 [D loss: 0.013551, acc.: 100.00%] [G loss: 0.010387]\n",
      "epoch:34 step:26682 [D loss: 0.060433, acc.: 99.22%] [G loss: 0.165903]\n",
      "epoch:34 step:26683 [D loss: 0.019365, acc.: 100.00%] [G loss: 0.859179]\n",
      "epoch:34 step:26684 [D loss: 0.054703, acc.: 99.22%] [G loss: 0.207745]\n",
      "epoch:34 step:26685 [D loss: 0.014095, acc.: 100.00%] [G loss: 0.627462]\n",
      "epoch:34 step:26686 [D loss: 0.045731, acc.: 100.00%] [G loss: 0.069258]\n",
      "epoch:34 step:26687 [D loss: 0.014018, acc.: 100.00%] [G loss: 0.083184]\n",
      "epoch:34 step:26688 [D loss: 0.021509, acc.: 100.00%] [G loss: 0.251865]\n",
      "epoch:34 step:26689 [D loss: 0.064803, acc.: 97.66%] [G loss: 0.030100]\n",
      "epoch:34 step:26690 [D loss: 0.012998, acc.: 100.00%] [G loss: 0.022656]\n",
      "epoch:34 step:26691 [D loss: 0.003149, acc.: 100.00%] [G loss: 0.004805]\n",
      "epoch:34 step:26692 [D loss: 0.007410, acc.: 100.00%] [G loss: 0.067241]\n",
      "epoch:34 step:26693 [D loss: 0.006458, acc.: 100.00%] [G loss: 0.013398]\n",
      "epoch:34 step:26694 [D loss: 0.013455, acc.: 99.22%] [G loss: 0.030693]\n",
      "epoch:34 step:26695 [D loss: 0.004784, acc.: 100.00%] [G loss: 0.123725]\n",
      "epoch:34 step:26696 [D loss: 0.009014, acc.: 100.00%] [G loss: 0.007539]\n",
      "epoch:34 step:26697 [D loss: 0.011313, acc.: 100.00%] [G loss: 0.005944]\n",
      "epoch:34 step:26698 [D loss: 0.002328, acc.: 100.00%] [G loss: 0.028314]\n",
      "epoch:34 step:26699 [D loss: 0.000723, acc.: 100.00%] [G loss: 0.004312]\n",
      "epoch:34 step:26700 [D loss: 0.004110, acc.: 100.00%] [G loss: 0.066322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26701 [D loss: 0.004632, acc.: 100.00%] [G loss: 0.004853]\n",
      "epoch:34 step:26702 [D loss: 0.002991, acc.: 100.00%] [G loss: 0.004993]\n",
      "epoch:34 step:26703 [D loss: 0.002418, acc.: 100.00%] [G loss: 0.016236]\n",
      "epoch:34 step:26704 [D loss: 0.002697, acc.: 100.00%] [G loss: 0.002818]\n",
      "epoch:34 step:26705 [D loss: 0.007462, acc.: 100.00%] [G loss: 0.003219]\n",
      "epoch:34 step:26706 [D loss: 0.002359, acc.: 100.00%] [G loss: 0.009599]\n",
      "epoch:34 step:26707 [D loss: 0.001894, acc.: 100.00%] [G loss: 0.001316]\n",
      "epoch:34 step:26708 [D loss: 0.001388, acc.: 100.00%] [G loss: 0.003695]\n",
      "epoch:34 step:26709 [D loss: 0.002327, acc.: 100.00%] [G loss: 0.002796]\n",
      "epoch:34 step:26710 [D loss: 0.000754, acc.: 100.00%] [G loss: 0.075728]\n",
      "epoch:34 step:26711 [D loss: 0.005733, acc.: 100.00%] [G loss: 0.021507]\n",
      "epoch:34 step:26712 [D loss: 0.006776, acc.: 100.00%] [G loss: 0.002154]\n",
      "epoch:34 step:26713 [D loss: 0.120806, acc.: 97.66%] [G loss: 0.665244]\n",
      "epoch:34 step:26714 [D loss: 0.031506, acc.: 99.22%] [G loss: 1.663268]\n",
      "epoch:34 step:26715 [D loss: 0.271623, acc.: 88.28%] [G loss: 2.177867]\n",
      "epoch:34 step:26716 [D loss: 0.117914, acc.: 94.53%] [G loss: 0.793433]\n",
      "epoch:34 step:26717 [D loss: 0.020585, acc.: 100.00%] [G loss: 1.170920]\n",
      "epoch:34 step:26718 [D loss: 0.130088, acc.: 96.09%] [G loss: 0.482312]\n",
      "epoch:34 step:26719 [D loss: 0.041236, acc.: 100.00%] [G loss: 0.829279]\n",
      "epoch:34 step:26720 [D loss: 0.117870, acc.: 95.31%] [G loss: 0.123655]\n",
      "epoch:34 step:26721 [D loss: 0.008732, acc.: 100.00%] [G loss: 0.003109]\n",
      "epoch:34 step:26722 [D loss: 0.001067, acc.: 100.00%] [G loss: 0.086163]\n",
      "epoch:34 step:26723 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.012875]\n",
      "epoch:34 step:26724 [D loss: 0.001657, acc.: 100.00%] [G loss: 0.004463]\n",
      "epoch:34 step:26725 [D loss: 0.003288, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:34 step:26726 [D loss: 0.000327, acc.: 100.00%] [G loss: 0.088481]\n",
      "epoch:34 step:26727 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.034136]\n",
      "epoch:34 step:26728 [D loss: 0.000394, acc.: 100.00%] [G loss: 0.003342]\n",
      "epoch:34 step:26729 [D loss: 0.002245, acc.: 100.00%] [G loss: 0.004731]\n",
      "epoch:34 step:26730 [D loss: 0.023204, acc.: 98.44%] [G loss: 0.325048]\n",
      "epoch:34 step:26731 [D loss: 0.001408, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:34 step:26732 [D loss: 0.008867, acc.: 100.00%] [G loss: 0.000605]\n",
      "epoch:34 step:26733 [D loss: 0.001250, acc.: 100.00%] [G loss: 0.003032]\n",
      "epoch:34 step:26734 [D loss: 0.016970, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:34 step:26735 [D loss: 0.003048, acc.: 100.00%] [G loss: 0.000626]\n",
      "epoch:34 step:26736 [D loss: 0.001632, acc.: 100.00%] [G loss: 0.007908]\n",
      "epoch:34 step:26737 [D loss: 0.000665, acc.: 100.00%] [G loss: 0.002339]\n",
      "epoch:34 step:26738 [D loss: 0.002110, acc.: 100.00%] [G loss: 0.031338]\n",
      "epoch:34 step:26739 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.006064]\n",
      "epoch:34 step:26740 [D loss: 0.002127, acc.: 100.00%] [G loss: 0.002489]\n",
      "epoch:34 step:26741 [D loss: 0.002381, acc.: 100.00%] [G loss: 0.020189]\n",
      "epoch:34 step:26742 [D loss: 0.000933, acc.: 100.00%] [G loss: 0.000613]\n",
      "epoch:34 step:26743 [D loss: 0.003033, acc.: 100.00%] [G loss: 0.002025]\n",
      "epoch:34 step:26744 [D loss: 0.006767, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:34 step:26745 [D loss: 0.003587, acc.: 100.00%] [G loss: 0.001183]\n",
      "epoch:34 step:26746 [D loss: 0.004104, acc.: 100.00%] [G loss: 0.001199]\n",
      "epoch:34 step:26747 [D loss: 0.003388, acc.: 100.00%] [G loss: 0.002433]\n",
      "epoch:34 step:26748 [D loss: 0.005531, acc.: 100.00%] [G loss: 0.000839]\n",
      "epoch:34 step:26749 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:34 step:26750 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:34 step:26751 [D loss: 0.006900, acc.: 100.00%] [G loss: 0.001690]\n",
      "epoch:34 step:26752 [D loss: 0.001961, acc.: 100.00%] [G loss: 0.001605]\n",
      "epoch:34 step:26753 [D loss: 0.000802, acc.: 100.00%] [G loss: 0.002908]\n",
      "epoch:34 step:26754 [D loss: 0.006256, acc.: 100.00%] [G loss: 0.002952]\n",
      "epoch:34 step:26755 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:34 step:26756 [D loss: 0.000737, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:34 step:26757 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.001005]\n",
      "epoch:34 step:26758 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.000973]\n",
      "epoch:34 step:26759 [D loss: 0.000916, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:34 step:26760 [D loss: 0.011342, acc.: 100.00%] [G loss: 0.049607]\n",
      "epoch:34 step:26761 [D loss: 0.000669, acc.: 100.00%] [G loss: 0.001674]\n",
      "epoch:34 step:26762 [D loss: 0.004941, acc.: 100.00%] [G loss: 0.018130]\n",
      "epoch:34 step:26763 [D loss: 0.014432, acc.: 100.00%] [G loss: 0.003919]\n",
      "epoch:34 step:26764 [D loss: 0.001878, acc.: 100.00%] [G loss: 0.001368]\n",
      "epoch:34 step:26765 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:34 step:26766 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.002405]\n",
      "epoch:34 step:26767 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.473491]\n",
      "epoch:34 step:26768 [D loss: 0.002801, acc.: 100.00%] [G loss: 0.067856]\n",
      "epoch:34 step:26769 [D loss: 0.001935, acc.: 100.00%] [G loss: 0.000784]\n",
      "epoch:34 step:26770 [D loss: 0.002920, acc.: 100.00%] [G loss: 0.000551]\n",
      "epoch:34 step:26771 [D loss: 0.001068, acc.: 100.00%] [G loss: 0.004526]\n",
      "epoch:34 step:26772 [D loss: 0.006952, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:34 step:26773 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:34 step:26774 [D loss: 0.001599, acc.: 100.00%] [G loss: 0.001289]\n",
      "epoch:34 step:26775 [D loss: 0.000687, acc.: 100.00%] [G loss: 0.027569]\n",
      "epoch:34 step:26776 [D loss: 0.000683, acc.: 100.00%] [G loss: 0.003492]\n",
      "epoch:34 step:26777 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.002987]\n",
      "epoch:34 step:26778 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:34 step:26779 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:34 step:26780 [D loss: 0.005272, acc.: 100.00%] [G loss: 0.001520]\n",
      "epoch:34 step:26781 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:34 step:26782 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:34 step:26783 [D loss: 0.002740, acc.: 100.00%] [G loss: 0.000487]\n",
      "epoch:34 step:26784 [D loss: 0.003192, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:34 step:26785 [D loss: 0.003036, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:34 step:26786 [D loss: 0.013797, acc.: 100.00%] [G loss: 0.000390]\n",
      "epoch:34 step:26787 [D loss: 0.005233, acc.: 100.00%] [G loss: 0.001113]\n",
      "epoch:34 step:26788 [D loss: 0.070185, acc.: 97.66%] [G loss: 0.226035]\n",
      "epoch:34 step:26789 [D loss: 0.008810, acc.: 100.00%] [G loss: 0.495662]\n",
      "epoch:34 step:26790 [D loss: 0.189658, acc.: 89.84%] [G loss: 0.010906]\n",
      "epoch:34 step:26791 [D loss: 0.011207, acc.: 100.00%] [G loss: 0.013262]\n",
      "epoch:34 step:26792 [D loss: 0.019531, acc.: 100.00%] [G loss: 0.075438]\n",
      "epoch:34 step:26793 [D loss: 0.057599, acc.: 100.00%] [G loss: 0.722261]\n",
      "epoch:34 step:26794 [D loss: 3.243287, acc.: 16.41%] [G loss: 7.786505]\n",
      "epoch:34 step:26795 [D loss: 1.697489, acc.: 53.12%] [G loss: 6.035237]\n",
      "epoch:34 step:26796 [D loss: 0.841118, acc.: 65.62%] [G loss: 3.238473]\n",
      "epoch:34 step:26797 [D loss: 0.110110, acc.: 95.31%] [G loss: 2.213227]\n",
      "epoch:34 step:26798 [D loss: 0.399984, acc.: 81.25%] [G loss: 3.158129]\n",
      "epoch:34 step:26799 [D loss: 0.117129, acc.: 95.31%] [G loss: 3.603751]\n",
      "epoch:34 step:26800 [D loss: 0.177143, acc.: 94.53%] [G loss: 3.145196]\n",
      "epoch:34 step:26801 [D loss: 0.150675, acc.: 96.88%] [G loss: 2.594468]\n",
      "epoch:34 step:26802 [D loss: 0.082265, acc.: 99.22%] [G loss: 3.139913]\n",
      "epoch:34 step:26803 [D loss: 0.164778, acc.: 96.09%] [G loss: 3.302262]\n",
      "epoch:34 step:26804 [D loss: 0.116860, acc.: 97.66%] [G loss: 3.126681]\n",
      "epoch:34 step:26805 [D loss: 0.056947, acc.: 99.22%] [G loss: 0.089943]\n",
      "epoch:34 step:26806 [D loss: 0.047625, acc.: 98.44%] [G loss: 2.942974]\n",
      "epoch:34 step:26807 [D loss: 0.047729, acc.: 99.22%] [G loss: 0.805634]\n",
      "epoch:34 step:26808 [D loss: 0.106866, acc.: 96.88%] [G loss: 3.432992]\n",
      "epoch:34 step:26809 [D loss: 0.193501, acc.: 94.53%] [G loss: 2.626825]\n",
      "epoch:34 step:26810 [D loss: 0.115770, acc.: 97.66%] [G loss: 0.206686]\n",
      "epoch:34 step:26811 [D loss: 0.074728, acc.: 97.66%] [G loss: 2.088924]\n",
      "epoch:34 step:26812 [D loss: 0.081945, acc.: 100.00%] [G loss: 2.277504]\n",
      "epoch:34 step:26813 [D loss: 0.083017, acc.: 99.22%] [G loss: 1.903301]\n",
      "epoch:34 step:26814 [D loss: 0.219690, acc.: 91.41%] [G loss: 1.890166]\n",
      "epoch:34 step:26815 [D loss: 0.164419, acc.: 96.09%] [G loss: 1.180270]\n",
      "epoch:34 step:26816 [D loss: 0.141196, acc.: 97.66%] [G loss: 0.199771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26817 [D loss: 0.053955, acc.: 99.22%] [G loss: 3.358155]\n",
      "epoch:34 step:26818 [D loss: 0.117214, acc.: 94.53%] [G loss: 2.257984]\n",
      "epoch:34 step:26819 [D loss: 0.126283, acc.: 96.88%] [G loss: 1.572768]\n",
      "epoch:34 step:26820 [D loss: 0.153526, acc.: 96.09%] [G loss: 1.955663]\n",
      "epoch:34 step:26821 [D loss: 0.359504, acc.: 80.47%] [G loss: 1.277106]\n",
      "epoch:34 step:26822 [D loss: 0.117814, acc.: 95.31%] [G loss: 1.086044]\n",
      "epoch:34 step:26823 [D loss: 0.201660, acc.: 93.75%] [G loss: 1.474725]\n",
      "epoch:34 step:26824 [D loss: 0.263327, acc.: 87.50%] [G loss: 1.018624]\n",
      "epoch:34 step:26825 [D loss: 0.116830, acc.: 96.09%] [G loss: 1.959051]\n",
      "epoch:34 step:26826 [D loss: 0.078843, acc.: 97.66%] [G loss: 1.433981]\n",
      "epoch:34 step:26827 [D loss: 0.551129, acc.: 69.53%] [G loss: 4.897674]\n",
      "epoch:34 step:26828 [D loss: 0.275104, acc.: 85.94%] [G loss: 4.254039]\n",
      "epoch:34 step:26829 [D loss: 0.460141, acc.: 82.03%] [G loss: 4.229566]\n",
      "epoch:34 step:26830 [D loss: 0.103428, acc.: 97.66%] [G loss: 4.237810]\n",
      "epoch:34 step:26831 [D loss: 0.249707, acc.: 89.84%] [G loss: 3.470305]\n",
      "epoch:34 step:26832 [D loss: 0.019700, acc.: 100.00%] [G loss: 4.219943]\n",
      "epoch:34 step:26833 [D loss: 0.114556, acc.: 95.31%] [G loss: 3.948995]\n",
      "epoch:34 step:26834 [D loss: 0.038371, acc.: 100.00%] [G loss: 0.839471]\n",
      "epoch:34 step:26835 [D loss: 0.012733, acc.: 100.00%] [G loss: 4.363131]\n",
      "epoch:34 step:26836 [D loss: 0.023490, acc.: 99.22%] [G loss: 3.426436]\n",
      "epoch:34 step:26837 [D loss: 0.053528, acc.: 99.22%] [G loss: 3.367778]\n",
      "epoch:34 step:26838 [D loss: 0.038817, acc.: 100.00%] [G loss: 3.130493]\n",
      "epoch:34 step:26839 [D loss: 0.065006, acc.: 98.44%] [G loss: 3.298330]\n",
      "epoch:34 step:26840 [D loss: 0.431448, acc.: 81.25%] [G loss: 1.046988]\n",
      "epoch:34 step:26841 [D loss: 0.299704, acc.: 87.50%] [G loss: 2.972463]\n",
      "epoch:34 step:26842 [D loss: 0.092270, acc.: 96.09%] [G loss: 3.729732]\n",
      "epoch:34 step:26843 [D loss: 0.282299, acc.: 89.06%] [G loss: 3.315747]\n",
      "epoch:34 step:26844 [D loss: 0.065061, acc.: 98.44%] [G loss: 2.363187]\n",
      "epoch:34 step:26845 [D loss: 0.222392, acc.: 92.19%] [G loss: 3.164579]\n",
      "epoch:34 step:26846 [D loss: 0.206534, acc.: 93.75%] [G loss: 2.256739]\n",
      "epoch:34 step:26847 [D loss: 0.122095, acc.: 96.88%] [G loss: 2.140385]\n",
      "epoch:34 step:26848 [D loss: 0.086025, acc.: 98.44%] [G loss: 0.140179]\n",
      "epoch:34 step:26849 [D loss: 0.039865, acc.: 100.00%] [G loss: 1.857973]\n",
      "epoch:34 step:26850 [D loss: 0.013236, acc.: 100.00%] [G loss: 0.445040]\n",
      "epoch:34 step:26851 [D loss: 0.024897, acc.: 100.00%] [G loss: 0.828430]\n",
      "epoch:34 step:26852 [D loss: 0.023949, acc.: 99.22%] [G loss: 0.455227]\n",
      "epoch:34 step:26853 [D loss: 0.069400, acc.: 100.00%] [G loss: 0.351072]\n",
      "epoch:34 step:26854 [D loss: 0.197101, acc.: 95.31%] [G loss: 0.474571]\n",
      "epoch:34 step:26855 [D loss: 0.107062, acc.: 99.22%] [G loss: 1.572147]\n",
      "epoch:34 step:26856 [D loss: 0.053542, acc.: 98.44%] [G loss: 2.609248]\n",
      "epoch:34 step:26857 [D loss: 0.140301, acc.: 96.88%] [G loss: 1.445215]\n",
      "epoch:34 step:26858 [D loss: 0.090951, acc.: 98.44%] [G loss: 0.272251]\n",
      "epoch:34 step:26859 [D loss: 0.084804, acc.: 96.88%] [G loss: 1.271610]\n",
      "epoch:34 step:26860 [D loss: 0.019598, acc.: 99.22%] [G loss: 0.383515]\n",
      "epoch:34 step:26861 [D loss: 0.002225, acc.: 100.00%] [G loss: 0.005518]\n",
      "epoch:34 step:26862 [D loss: 0.006544, acc.: 100.00%] [G loss: 0.162516]\n",
      "epoch:34 step:26863 [D loss: 0.002174, acc.: 100.00%] [G loss: 0.190322]\n",
      "epoch:34 step:26864 [D loss: 0.012029, acc.: 99.22%] [G loss: 0.764185]\n",
      "epoch:34 step:26865 [D loss: 0.010019, acc.: 99.22%] [G loss: 0.051982]\n",
      "epoch:34 step:26866 [D loss: 0.004604, acc.: 100.00%] [G loss: 0.033589]\n",
      "epoch:34 step:26867 [D loss: 0.025901, acc.: 99.22%] [G loss: 0.067750]\n",
      "epoch:34 step:26868 [D loss: 0.011999, acc.: 100.00%] [G loss: 0.103532]\n",
      "epoch:34 step:26869 [D loss: 0.117391, acc.: 93.75%] [G loss: 0.002397]\n",
      "epoch:34 step:26870 [D loss: 0.039361, acc.: 100.00%] [G loss: 0.005151]\n",
      "epoch:34 step:26871 [D loss: 0.040728, acc.: 100.00%] [G loss: 0.014511]\n",
      "epoch:34 step:26872 [D loss: 0.038825, acc.: 99.22%] [G loss: 0.083198]\n",
      "epoch:34 step:26873 [D loss: 0.041818, acc.: 99.22%] [G loss: 0.200027]\n",
      "epoch:34 step:26874 [D loss: 0.033514, acc.: 99.22%] [G loss: 0.672834]\n",
      "epoch:34 step:26875 [D loss: 0.097699, acc.: 96.09%] [G loss: 0.014526]\n",
      "epoch:34 step:26876 [D loss: 0.007738, acc.: 100.00%] [G loss: 0.615728]\n",
      "epoch:34 step:26877 [D loss: 0.004406, acc.: 100.00%] [G loss: 0.751898]\n",
      "epoch:34 step:26878 [D loss: 0.079453, acc.: 97.66%] [G loss: 0.222082]\n",
      "epoch:34 step:26879 [D loss: 0.051792, acc.: 99.22%] [G loss: 0.000425]\n",
      "epoch:34 step:26880 [D loss: 0.002382, acc.: 100.00%] [G loss: 0.391016]\n",
      "epoch:34 step:26881 [D loss: 0.046437, acc.: 97.66%] [G loss: 0.033528]\n",
      "epoch:34 step:26882 [D loss: 0.000925, acc.: 100.00%] [G loss: 1.456270]\n",
      "epoch:34 step:26883 [D loss: 0.001808, acc.: 100.00%] [G loss: 0.012100]\n",
      "epoch:34 step:26884 [D loss: 0.008734, acc.: 100.00%] [G loss: 0.019994]\n",
      "epoch:34 step:26885 [D loss: 0.026797, acc.: 98.44%] [G loss: 0.024233]\n",
      "epoch:34 step:26886 [D loss: 0.002183, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:34 step:26887 [D loss: 0.000894, acc.: 100.00%] [G loss: 0.010860]\n",
      "epoch:34 step:26888 [D loss: 0.004326, acc.: 100.00%] [G loss: 0.027692]\n",
      "epoch:34 step:26889 [D loss: 0.002126, acc.: 100.00%] [G loss: 0.008041]\n",
      "epoch:34 step:26890 [D loss: 0.006610, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:34 step:26891 [D loss: 0.009473, acc.: 100.00%] [G loss: 0.017477]\n",
      "epoch:34 step:26892 [D loss: 0.025175, acc.: 99.22%] [G loss: 0.003570]\n",
      "epoch:34 step:26893 [D loss: 0.001852, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:34 step:26894 [D loss: 0.010147, acc.: 99.22%] [G loss: 0.001228]\n",
      "epoch:34 step:26895 [D loss: 0.008646, acc.: 100.00%] [G loss: 0.001924]\n",
      "epoch:34 step:26896 [D loss: 0.003105, acc.: 100.00%] [G loss: 0.000815]\n",
      "epoch:34 step:26897 [D loss: 0.003656, acc.: 100.00%] [G loss: 0.000514]\n",
      "epoch:34 step:26898 [D loss: 0.008820, acc.: 100.00%] [G loss: 0.008056]\n",
      "epoch:34 step:26899 [D loss: 0.003390, acc.: 100.00%] [G loss: 0.000503]\n",
      "epoch:34 step:26900 [D loss: 0.005957, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:34 step:26901 [D loss: 0.086876, acc.: 96.88%] [G loss: 0.006327]\n",
      "epoch:34 step:26902 [D loss: 0.010437, acc.: 100.00%] [G loss: 0.972975]\n",
      "epoch:34 step:26903 [D loss: 0.020565, acc.: 99.22%] [G loss: 0.044115]\n",
      "epoch:34 step:26904 [D loss: 0.036352, acc.: 99.22%] [G loss: 0.003661]\n",
      "epoch:34 step:26905 [D loss: 0.003642, acc.: 100.00%] [G loss: 0.003179]\n",
      "epoch:34 step:26906 [D loss: 0.003129, acc.: 100.00%] [G loss: 0.000460]\n",
      "epoch:34 step:26907 [D loss: 0.018738, acc.: 99.22%] [G loss: 0.000958]\n",
      "epoch:34 step:26908 [D loss: 0.036679, acc.: 100.00%] [G loss: 0.004532]\n",
      "epoch:34 step:26909 [D loss: 0.002745, acc.: 100.00%] [G loss: 0.005734]\n",
      "epoch:34 step:26910 [D loss: 0.001202, acc.: 100.00%] [G loss: 0.092840]\n",
      "epoch:34 step:26911 [D loss: 0.002382, acc.: 100.00%] [G loss: 0.053675]\n",
      "epoch:34 step:26912 [D loss: 0.004654, acc.: 100.00%] [G loss: 0.002891]\n",
      "epoch:34 step:26913 [D loss: 0.004705, acc.: 100.00%] [G loss: 0.014566]\n",
      "epoch:34 step:26914 [D loss: 0.002331, acc.: 100.00%] [G loss: 0.017710]\n",
      "epoch:34 step:26915 [D loss: 0.011713, acc.: 100.00%] [G loss: 0.000974]\n",
      "epoch:34 step:26916 [D loss: 0.004105, acc.: 100.00%] [G loss: 0.006608]\n",
      "epoch:34 step:26917 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.003644]\n",
      "epoch:34 step:26918 [D loss: 0.018204, acc.: 99.22%] [G loss: 0.000580]\n",
      "epoch:34 step:26919 [D loss: 0.000486, acc.: 100.00%] [G loss: 0.006905]\n",
      "epoch:34 step:26920 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.000420]\n",
      "epoch:34 step:26921 [D loss: 0.001057, acc.: 100.00%] [G loss: 0.001415]\n",
      "epoch:34 step:26922 [D loss: 0.000414, acc.: 100.00%] [G loss: 0.001118]\n",
      "epoch:34 step:26923 [D loss: 0.000830, acc.: 100.00%] [G loss: 0.005602]\n",
      "epoch:34 step:26924 [D loss: 0.001075, acc.: 100.00%] [G loss: 0.002646]\n",
      "epoch:34 step:26925 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000595]\n",
      "epoch:34 step:26926 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.001521]\n",
      "epoch:34 step:26927 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.002037]\n",
      "epoch:34 step:26928 [D loss: 0.002854, acc.: 100.00%] [G loss: 0.000297]\n",
      "epoch:34 step:26929 [D loss: 0.000911, acc.: 100.00%] [G loss: 0.000852]\n",
      "epoch:34 step:26930 [D loss: 0.001130, acc.: 100.00%] [G loss: 0.000919]\n",
      "epoch:34 step:26931 [D loss: 0.000761, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:34 step:26932 [D loss: 0.000678, acc.: 100.00%] [G loss: 0.008409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:26933 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.001172]\n",
      "epoch:34 step:26934 [D loss: 0.000652, acc.: 100.00%] [G loss: 0.000709]\n",
      "epoch:34 step:26935 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.000753]\n",
      "epoch:34 step:26936 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.001280]\n",
      "epoch:34 step:26937 [D loss: 0.002713, acc.: 100.00%] [G loss: 0.000992]\n",
      "epoch:34 step:26938 [D loss: 0.021715, acc.: 100.00%] [G loss: 0.013949]\n",
      "epoch:34 step:26939 [D loss: 0.003367, acc.: 100.00%] [G loss: 0.003933]\n",
      "epoch:34 step:26940 [D loss: 0.002367, acc.: 100.00%] [G loss: 0.004762]\n",
      "epoch:34 step:26941 [D loss: 0.001702, acc.: 100.00%] [G loss: 0.023678]\n",
      "epoch:34 step:26942 [D loss: 0.284182, acc.: 84.38%] [G loss: 4.216162]\n",
      "epoch:34 step:26943 [D loss: 0.532089, acc.: 77.34%] [G loss: 1.091604]\n",
      "epoch:34 step:26944 [D loss: 0.070337, acc.: 96.09%] [G loss: 0.806538]\n",
      "epoch:34 step:26945 [D loss: 0.033431, acc.: 99.22%] [G loss: 1.292038]\n",
      "epoch:34 step:26946 [D loss: 0.164090, acc.: 92.97%] [G loss: 4.488842]\n",
      "epoch:34 step:26947 [D loss: 0.081501, acc.: 96.88%] [G loss: 2.754836]\n",
      "epoch:34 step:26948 [D loss: 0.071938, acc.: 98.44%] [G loss: 3.075659]\n",
      "epoch:34 step:26949 [D loss: 0.266271, acc.: 88.28%] [G loss: 6.134237]\n",
      "epoch:34 step:26950 [D loss: 0.207731, acc.: 91.41%] [G loss: 6.242754]\n",
      "epoch:34 step:26951 [D loss: 0.188741, acc.: 93.75%] [G loss: 3.668036]\n",
      "epoch:34 step:26952 [D loss: 0.036289, acc.: 99.22%] [G loss: 4.208811]\n",
      "epoch:34 step:26953 [D loss: 0.134238, acc.: 94.53%] [G loss: 0.405066]\n",
      "epoch:34 step:26954 [D loss: 0.009741, acc.: 100.00%] [G loss: 6.256506]\n",
      "epoch:34 step:26955 [D loss: 0.272670, acc.: 87.50%] [G loss: 3.731413]\n",
      "epoch:34 step:26956 [D loss: 0.043709, acc.: 99.22%] [G loss: 2.987630]\n",
      "epoch:34 step:26957 [D loss: 0.007565, acc.: 100.00%] [G loss: 1.716862]\n",
      "epoch:34 step:26958 [D loss: 0.059171, acc.: 97.66%] [G loss: 1.571767]\n",
      "epoch:34 step:26959 [D loss: 0.109195, acc.: 98.44%] [G loss: 2.476310]\n",
      "epoch:34 step:26960 [D loss: 0.091146, acc.: 96.88%] [G loss: 2.396416]\n",
      "epoch:34 step:26961 [D loss: 0.014903, acc.: 100.00%] [G loss: 1.686760]\n",
      "epoch:34 step:26962 [D loss: 0.013101, acc.: 100.00%] [G loss: 1.095943]\n",
      "epoch:34 step:26963 [D loss: 0.337255, acc.: 84.38%] [G loss: 4.924837]\n",
      "epoch:34 step:26964 [D loss: 0.310758, acc.: 89.84%] [G loss: 5.391949]\n",
      "epoch:34 step:26965 [D loss: 0.489921, acc.: 76.56%] [G loss: 3.908027]\n",
      "epoch:34 step:26966 [D loss: 0.055723, acc.: 98.44%] [G loss: 4.812123]\n",
      "epoch:34 step:26967 [D loss: 0.085719, acc.: 96.88%] [G loss: 5.059863]\n",
      "epoch:34 step:26968 [D loss: 0.046263, acc.: 98.44%] [G loss: 3.986137]\n",
      "epoch:34 step:26969 [D loss: 0.014892, acc.: 100.00%] [G loss: 5.410771]\n",
      "epoch:34 step:26970 [D loss: 0.061586, acc.: 97.66%] [G loss: 4.908921]\n",
      "epoch:34 step:26971 [D loss: 0.083209, acc.: 97.66%] [G loss: 4.435540]\n",
      "epoch:34 step:26972 [D loss: 0.215334, acc.: 92.97%] [G loss: 1.148727]\n",
      "epoch:34 step:26973 [D loss: 0.067169, acc.: 97.66%] [G loss: 7.091934]\n",
      "epoch:34 step:26974 [D loss: 0.282024, acc.: 89.06%] [G loss: 0.080698]\n",
      "epoch:34 step:26975 [D loss: 0.007875, acc.: 100.00%] [G loss: 0.003758]\n",
      "epoch:34 step:26976 [D loss: 0.009511, acc.: 100.00%] [G loss: 1.573708]\n",
      "epoch:34 step:26977 [D loss: 0.143510, acc.: 95.31%] [G loss: 0.322687]\n",
      "epoch:34 step:26978 [D loss: 0.037033, acc.: 97.66%] [G loss: 2.349783]\n",
      "epoch:34 step:26979 [D loss: 0.044851, acc.: 98.44%] [G loss: 0.962055]\n",
      "epoch:34 step:26980 [D loss: 0.003098, acc.: 100.00%] [G loss: 0.536098]\n",
      "epoch:34 step:26981 [D loss: 0.001625, acc.: 100.00%] [G loss: 0.622744]\n",
      "epoch:34 step:26982 [D loss: 0.118318, acc.: 96.09%] [G loss: 0.022145]\n",
      "epoch:34 step:26983 [D loss: 0.403896, acc.: 82.03%] [G loss: 0.997836]\n",
      "epoch:34 step:26984 [D loss: 0.680189, acc.: 72.66%] [G loss: 0.716823]\n",
      "epoch:34 step:26985 [D loss: 0.057912, acc.: 98.44%] [G loss: 0.025345]\n",
      "epoch:34 step:26986 [D loss: 0.021402, acc.: 99.22%] [G loss: 0.016760]\n",
      "epoch:34 step:26987 [D loss: 0.013830, acc.: 100.00%] [G loss: 0.022179]\n",
      "epoch:34 step:26988 [D loss: 0.037375, acc.: 99.22%] [G loss: 0.146066]\n",
      "epoch:34 step:26989 [D loss: 0.023351, acc.: 100.00%] [G loss: 5.568738]\n",
      "epoch:34 step:26990 [D loss: 0.040114, acc.: 99.22%] [G loss: 0.275541]\n",
      "epoch:34 step:26991 [D loss: 0.030846, acc.: 99.22%] [G loss: 0.372264]\n",
      "epoch:34 step:26992 [D loss: 0.071238, acc.: 96.88%] [G loss: 0.188932]\n",
      "epoch:34 step:26993 [D loss: 0.015741, acc.: 100.00%] [G loss: 0.491097]\n",
      "epoch:34 step:26994 [D loss: 0.000721, acc.: 100.00%] [G loss: 0.634277]\n",
      "epoch:34 step:26995 [D loss: 0.013826, acc.: 99.22%] [G loss: 0.103921]\n",
      "epoch:34 step:26996 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.235319]\n",
      "epoch:34 step:26997 [D loss: 0.012648, acc.: 99.22%] [G loss: 0.097936]\n",
      "epoch:34 step:26998 [D loss: 0.031669, acc.: 100.00%] [G loss: 0.021216]\n",
      "epoch:34 step:26999 [D loss: 0.006135, acc.: 100.00%] [G loss: 0.030143]\n",
      "epoch:34 step:27000 [D loss: 0.001153, acc.: 100.00%] [G loss: 0.069065]\n",
      "epoch:34 step:27001 [D loss: 0.002000, acc.: 100.00%] [G loss: 0.060425]\n",
      "epoch:34 step:27002 [D loss: 0.025201, acc.: 100.00%] [G loss: 0.008182]\n",
      "epoch:34 step:27003 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.200533]\n",
      "epoch:34 step:27004 [D loss: 0.022091, acc.: 100.00%] [G loss: 0.005504]\n",
      "epoch:34 step:27005 [D loss: 0.003779, acc.: 100.00%] [G loss: 0.008968]\n",
      "epoch:34 step:27006 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.044943]\n",
      "epoch:34 step:27007 [D loss: 0.012649, acc.: 100.00%] [G loss: 0.015946]\n",
      "epoch:34 step:27008 [D loss: 0.006016, acc.: 100.00%] [G loss: 0.002128]\n",
      "epoch:34 step:27009 [D loss: 0.001425, acc.: 100.00%] [G loss: 0.039412]\n",
      "epoch:34 step:27010 [D loss: 0.003221, acc.: 100.00%] [G loss: 0.005269]\n",
      "epoch:34 step:27011 [D loss: 0.036923, acc.: 100.00%] [G loss: 2.591947]\n",
      "epoch:34 step:27012 [D loss: 0.002192, acc.: 100.00%] [G loss: 0.037059]\n",
      "epoch:34 step:27013 [D loss: 0.002608, acc.: 100.00%] [G loss: 0.961878]\n",
      "epoch:34 step:27014 [D loss: 0.057270, acc.: 99.22%] [G loss: 0.457185]\n",
      "epoch:34 step:27015 [D loss: 0.030131, acc.: 99.22%] [G loss: 0.481232]\n",
      "epoch:34 step:27016 [D loss: 0.005206, acc.: 100.00%] [G loss: 0.267764]\n",
      "epoch:34 step:27017 [D loss: 0.028259, acc.: 99.22%] [G loss: 0.007787]\n",
      "epoch:34 step:27018 [D loss: 0.013416, acc.: 100.00%] [G loss: 0.013050]\n",
      "epoch:34 step:27019 [D loss: 0.002912, acc.: 100.00%] [G loss: 0.080324]\n",
      "epoch:34 step:27020 [D loss: 0.002995, acc.: 100.00%] [G loss: 0.001504]\n",
      "epoch:34 step:27021 [D loss: 0.003487, acc.: 100.00%] [G loss: 0.008573]\n",
      "epoch:34 step:27022 [D loss: 0.008157, acc.: 100.00%] [G loss: 0.006213]\n",
      "epoch:34 step:27023 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.109495]\n",
      "epoch:34 step:27024 [D loss: 0.003702, acc.: 100.00%] [G loss: 0.002249]\n",
      "epoch:34 step:27025 [D loss: 0.001761, acc.: 100.00%] [G loss: 0.212882]\n",
      "epoch:34 step:27026 [D loss: 0.004862, acc.: 100.00%] [G loss: 0.001767]\n",
      "epoch:34 step:27027 [D loss: 0.002079, acc.: 100.00%] [G loss: 0.006606]\n",
      "epoch:34 step:27028 [D loss: 0.004626, acc.: 100.00%] [G loss: 0.074284]\n",
      "epoch:34 step:27029 [D loss: 0.020943, acc.: 99.22%] [G loss: 0.000394]\n",
      "epoch:34 step:27030 [D loss: 0.004794, acc.: 100.00%] [G loss: 0.000258]\n",
      "epoch:34 step:27031 [D loss: 0.006197, acc.: 100.00%] [G loss: 0.168015]\n",
      "epoch:34 step:27032 [D loss: 0.002423, acc.: 100.00%] [G loss: 0.000856]\n",
      "epoch:34 step:27033 [D loss: 0.006047, acc.: 100.00%] [G loss: 0.002690]\n",
      "epoch:34 step:27034 [D loss: 0.026765, acc.: 100.00%] [G loss: 0.007615]\n",
      "epoch:34 step:27035 [D loss: 0.012161, acc.: 100.00%] [G loss: 0.005096]\n",
      "epoch:34 step:27036 [D loss: 0.008595, acc.: 100.00%] [G loss: 0.177162]\n",
      "epoch:34 step:27037 [D loss: 0.025828, acc.: 100.00%] [G loss: 0.227501]\n",
      "epoch:34 step:27038 [D loss: 0.000538, acc.: 100.00%] [G loss: 1.069119]\n",
      "epoch:34 step:27039 [D loss: 0.008855, acc.: 100.00%] [G loss: 0.039464]\n",
      "epoch:34 step:27040 [D loss: 0.041049, acc.: 100.00%] [G loss: 0.147526]\n",
      "epoch:34 step:27041 [D loss: 0.024049, acc.: 99.22%] [G loss: 2.159606]\n",
      "epoch:34 step:27042 [D loss: 0.005936, acc.: 100.00%] [G loss: 1.329835]\n",
      "epoch:34 step:27043 [D loss: 0.065934, acc.: 99.22%] [G loss: 0.196640]\n",
      "epoch:34 step:27044 [D loss: 0.053972, acc.: 97.66%] [G loss: 1.939557]\n",
      "epoch:34 step:27045 [D loss: 0.314924, acc.: 89.06%] [G loss: 0.514491]\n",
      "epoch:34 step:27046 [D loss: 0.056628, acc.: 98.44%] [G loss: 0.730388]\n",
      "epoch:34 step:27047 [D loss: 0.018208, acc.: 100.00%] [G loss: 4.100772]\n",
      "epoch:34 step:27048 [D loss: 0.023602, acc.: 99.22%] [G loss: 0.129409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27049 [D loss: 0.003087, acc.: 100.00%] [G loss: 0.191517]\n",
      "epoch:34 step:27050 [D loss: 0.001397, acc.: 100.00%] [G loss: 0.066917]\n",
      "epoch:34 step:27051 [D loss: 0.004395, acc.: 100.00%] [G loss: 0.001822]\n",
      "epoch:34 step:27052 [D loss: 0.006542, acc.: 100.00%] [G loss: 0.014724]\n",
      "epoch:34 step:27053 [D loss: 0.003456, acc.: 100.00%] [G loss: 0.016017]\n",
      "epoch:34 step:27054 [D loss: 0.008050, acc.: 99.22%] [G loss: 0.002271]\n",
      "epoch:34 step:27055 [D loss: 0.079167, acc.: 96.88%] [G loss: 0.000164]\n",
      "epoch:34 step:27056 [D loss: 0.006547, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:34 step:27057 [D loss: 0.000969, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:34 step:27058 [D loss: 0.001424, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:34 step:27059 [D loss: 0.033192, acc.: 99.22%] [G loss: 0.241044]\n",
      "epoch:34 step:27060 [D loss: 0.002138, acc.: 100.00%] [G loss: 0.112257]\n",
      "epoch:34 step:27061 [D loss: 0.001758, acc.: 100.00%] [G loss: 0.017866]\n",
      "epoch:34 step:27062 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.013492]\n",
      "epoch:34 step:27063 [D loss: 0.000949, acc.: 100.00%] [G loss: 0.132531]\n",
      "epoch:34 step:27064 [D loss: 0.001579, acc.: 100.00%] [G loss: 0.000883]\n",
      "epoch:34 step:27065 [D loss: 0.001642, acc.: 100.00%] [G loss: 0.001384]\n",
      "epoch:34 step:27066 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.011428]\n",
      "epoch:34 step:27067 [D loss: 0.002009, acc.: 100.00%] [G loss: 0.001081]\n",
      "epoch:34 step:27068 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.002835]\n",
      "epoch:34 step:27069 [D loss: 0.000318, acc.: 100.00%] [G loss: 0.004524]\n",
      "epoch:34 step:27070 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.002569]\n",
      "epoch:34 step:27071 [D loss: 0.001393, acc.: 100.00%] [G loss: 0.002675]\n",
      "epoch:34 step:27072 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:34 step:27073 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.000770]\n",
      "epoch:34 step:27074 [D loss: 0.003319, acc.: 100.00%] [G loss: 0.001056]\n",
      "epoch:34 step:27075 [D loss: 0.000837, acc.: 100.00%] [G loss: 0.002456]\n",
      "epoch:34 step:27076 [D loss: 0.007870, acc.: 100.00%] [G loss: 0.066842]\n",
      "epoch:34 step:27077 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.768854]\n",
      "epoch:34 step:27078 [D loss: 0.004373, acc.: 100.00%] [G loss: 0.001503]\n",
      "epoch:34 step:27079 [D loss: 0.003824, acc.: 100.00%] [G loss: 0.020518]\n",
      "epoch:34 step:27080 [D loss: 0.274240, acc.: 86.72%] [G loss: 1.000090]\n",
      "epoch:34 step:27081 [D loss: 0.057951, acc.: 97.66%] [G loss: 3.982661]\n",
      "epoch:34 step:27082 [D loss: 0.267886, acc.: 86.72%] [G loss: 0.000603]\n",
      "epoch:34 step:27083 [D loss: 0.021997, acc.: 99.22%] [G loss: 0.003333]\n",
      "epoch:34 step:27084 [D loss: 0.019967, acc.: 99.22%] [G loss: 0.004175]\n",
      "epoch:34 step:27085 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.000794]\n",
      "epoch:34 step:27086 [D loss: 0.002024, acc.: 100.00%] [G loss: 0.000812]\n",
      "epoch:34 step:27087 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.000806]\n",
      "epoch:34 step:27088 [D loss: 0.002307, acc.: 100.00%] [G loss: 0.010904]\n",
      "epoch:34 step:27089 [D loss: 0.010429, acc.: 100.00%] [G loss: 0.016085]\n",
      "epoch:34 step:27090 [D loss: 0.428509, acc.: 82.03%] [G loss: 6.422335]\n",
      "epoch:34 step:27091 [D loss: 2.343213, acc.: 53.12%] [G loss: 0.773930]\n",
      "epoch:34 step:27092 [D loss: 0.307653, acc.: 90.62%] [G loss: 4.544774]\n",
      "epoch:34 step:27093 [D loss: 0.026709, acc.: 100.00%] [G loss: 2.807979]\n",
      "epoch:34 step:27094 [D loss: 0.031560, acc.: 100.00%] [G loss: 2.576634]\n",
      "epoch:34 step:27095 [D loss: 0.117298, acc.: 96.88%] [G loss: 3.539894]\n",
      "epoch:34 step:27096 [D loss: 0.131704, acc.: 97.66%] [G loss: 4.054415]\n",
      "epoch:34 step:27097 [D loss: 0.021508, acc.: 100.00%] [G loss: 3.806495]\n",
      "epoch:34 step:27098 [D loss: 0.039735, acc.: 100.00%] [G loss: 3.949899]\n",
      "epoch:34 step:27099 [D loss: 0.125211, acc.: 96.88%] [G loss: 4.575696]\n",
      "epoch:34 step:27100 [D loss: 0.181132, acc.: 90.62%] [G loss: 1.346312]\n",
      "epoch:34 step:27101 [D loss: 0.062390, acc.: 100.00%] [G loss: 5.238981]\n",
      "epoch:34 step:27102 [D loss: 0.014640, acc.: 99.22%] [G loss: 0.030739]\n",
      "epoch:34 step:27103 [D loss: 0.002802, acc.: 100.00%] [G loss: 3.928224]\n",
      "epoch:34 step:27104 [D loss: 0.022009, acc.: 100.00%] [G loss: 0.441280]\n",
      "epoch:34 step:27105 [D loss: 0.005892, acc.: 100.00%] [G loss: 2.510279]\n",
      "epoch:34 step:27106 [D loss: 0.028032, acc.: 99.22%] [G loss: 2.123316]\n",
      "epoch:34 step:27107 [D loss: 0.002617, acc.: 100.00%] [G loss: 1.217957]\n",
      "epoch:34 step:27108 [D loss: 0.073633, acc.: 96.09%] [G loss: 1.104323]\n",
      "epoch:34 step:27109 [D loss: 0.014626, acc.: 100.00%] [G loss: 1.002985]\n",
      "epoch:34 step:27110 [D loss: 0.046931, acc.: 98.44%] [G loss: 0.817144]\n",
      "epoch:34 step:27111 [D loss: 0.136905, acc.: 96.88%] [G loss: 1.691737]\n",
      "epoch:34 step:27112 [D loss: 0.108080, acc.: 94.53%] [G loss: 5.294358]\n",
      "epoch:34 step:27113 [D loss: 0.053533, acc.: 98.44%] [G loss: 1.110967]\n",
      "epoch:34 step:27114 [D loss: 0.056005, acc.: 99.22%] [G loss: 1.179703]\n",
      "epoch:34 step:27115 [D loss: 0.156366, acc.: 93.75%] [G loss: 3.375399]\n",
      "epoch:34 step:27116 [D loss: 0.246408, acc.: 89.84%] [G loss: 1.453086]\n",
      "epoch:34 step:27117 [D loss: 0.156342, acc.: 92.97%] [G loss: 3.413011]\n",
      "epoch:34 step:27118 [D loss: 0.023790, acc.: 99.22%] [G loss: 4.833816]\n",
      "epoch:34 step:27119 [D loss: 0.207170, acc.: 92.19%] [G loss: 4.107090]\n",
      "epoch:34 step:27120 [D loss: 0.060874, acc.: 99.22%] [G loss: 4.348548]\n",
      "epoch:34 step:27121 [D loss: 0.022612, acc.: 100.00%] [G loss: 4.870055]\n",
      "epoch:34 step:27122 [D loss: 0.048488, acc.: 98.44%] [G loss: 6.254162]\n",
      "epoch:34 step:27123 [D loss: 0.006900, acc.: 100.00%] [G loss: 7.180868]\n",
      "epoch:34 step:27124 [D loss: 0.050563, acc.: 98.44%] [G loss: 7.127608]\n",
      "epoch:34 step:27125 [D loss: 0.021450, acc.: 100.00%] [G loss: 6.658955]\n",
      "epoch:34 step:27126 [D loss: 0.120429, acc.: 96.88%] [G loss: 6.093433]\n",
      "epoch:34 step:27127 [D loss: 0.047647, acc.: 99.22%] [G loss: 7.005372]\n",
      "epoch:34 step:27128 [D loss: 0.017377, acc.: 100.00%] [G loss: 6.791402]\n",
      "epoch:34 step:27129 [D loss: 0.076607, acc.: 96.09%] [G loss: 3.366218]\n",
      "epoch:34 step:27130 [D loss: 0.142412, acc.: 96.09%] [G loss: 0.985615]\n",
      "epoch:34 step:27131 [D loss: 0.180204, acc.: 93.75%] [G loss: 7.348907]\n",
      "epoch:34 step:27132 [D loss: 0.034957, acc.: 100.00%] [G loss: 7.648012]\n",
      "epoch:34 step:27133 [D loss: 0.007026, acc.: 100.00%] [G loss: 6.735710]\n",
      "epoch:34 step:27134 [D loss: 0.094802, acc.: 96.09%] [G loss: 3.916860]\n",
      "epoch:34 step:27135 [D loss: 0.058291, acc.: 99.22%] [G loss: 3.591309]\n",
      "epoch:34 step:27136 [D loss: 0.009415, acc.: 100.00%] [G loss: 4.718947]\n",
      "epoch:34 step:27137 [D loss: 0.019654, acc.: 99.22%] [G loss: 3.200259]\n",
      "epoch:34 step:27138 [D loss: 0.025707, acc.: 100.00%] [G loss: 2.848798]\n",
      "epoch:34 step:27139 [D loss: 0.015530, acc.: 100.00%] [G loss: 2.538702]\n",
      "epoch:34 step:27140 [D loss: 0.008374, acc.: 100.00%] [G loss: 2.814111]\n",
      "epoch:34 step:27141 [D loss: 0.041730, acc.: 99.22%] [G loss: 4.315514]\n",
      "epoch:34 step:27142 [D loss: 0.065575, acc.: 98.44%] [G loss: 6.321532]\n",
      "epoch:34 step:27143 [D loss: 0.192562, acc.: 90.62%] [G loss: 7.187395]\n",
      "epoch:34 step:27144 [D loss: 0.051690, acc.: 97.66%] [G loss: 7.346401]\n",
      "epoch:34 step:27145 [D loss: 0.098073, acc.: 96.09%] [G loss: 9.499681]\n",
      "epoch:34 step:27146 [D loss: 0.040644, acc.: 99.22%] [G loss: 9.546070]\n",
      "epoch:34 step:27147 [D loss: 0.083361, acc.: 96.88%] [G loss: 0.058301]\n",
      "epoch:34 step:27148 [D loss: 0.007425, acc.: 100.00%] [G loss: 7.104848]\n",
      "epoch:34 step:27149 [D loss: 0.002320, acc.: 100.00%] [G loss: 6.373345]\n",
      "epoch:34 step:27150 [D loss: 0.014737, acc.: 99.22%] [G loss: 5.599626]\n",
      "epoch:34 step:27151 [D loss: 0.123238, acc.: 98.44%] [G loss: 0.523485]\n",
      "epoch:34 step:27152 [D loss: 0.005306, acc.: 100.00%] [G loss: 0.442033]\n",
      "epoch:34 step:27153 [D loss: 0.652084, acc.: 82.03%] [G loss: 1.611228]\n",
      "epoch:34 step:27154 [D loss: 0.005090, acc.: 100.00%] [G loss: 0.823635]\n",
      "epoch:34 step:27155 [D loss: 0.001582, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:34 step:27156 [D loss: 0.033042, acc.: 100.00%] [G loss: 0.500826]\n",
      "epoch:34 step:27157 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.648731]\n",
      "epoch:34 step:27158 [D loss: 0.000724, acc.: 100.00%] [G loss: 0.387401]\n",
      "epoch:34 step:27159 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.286205]\n",
      "epoch:34 step:27160 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:34 step:27161 [D loss: 0.003176, acc.: 100.00%] [G loss: 0.158917]\n",
      "epoch:34 step:27162 [D loss: 0.001378, acc.: 100.00%] [G loss: 0.155581]\n",
      "epoch:34 step:27163 [D loss: 0.001103, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:34 step:27164 [D loss: 0.012697, acc.: 100.00%] [G loss: 0.097242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27165 [D loss: 0.017678, acc.: 100.00%] [G loss: 0.253196]\n",
      "epoch:34 step:27166 [D loss: 0.000861, acc.: 100.00%] [G loss: 0.080104]\n",
      "epoch:34 step:27167 [D loss: 0.001762, acc.: 100.00%] [G loss: 0.121339]\n",
      "epoch:34 step:27168 [D loss: 0.002115, acc.: 100.00%] [G loss: 0.054944]\n",
      "epoch:34 step:27169 [D loss: 0.002480, acc.: 100.00%] [G loss: 0.137592]\n",
      "epoch:34 step:27170 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.047231]\n",
      "epoch:34 step:27171 [D loss: 0.004130, acc.: 100.00%] [G loss: 0.017095]\n",
      "epoch:34 step:27172 [D loss: 0.005581, acc.: 100.00%] [G loss: 0.007493]\n",
      "epoch:34 step:27173 [D loss: 0.139882, acc.: 93.75%] [G loss: 2.566414]\n",
      "epoch:34 step:27174 [D loss: 0.012136, acc.: 100.00%] [G loss: 0.107149]\n",
      "epoch:34 step:27175 [D loss: 0.022897, acc.: 100.00%] [G loss: 4.387880]\n",
      "epoch:34 step:27176 [D loss: 0.062256, acc.: 97.66%] [G loss: 0.012315]\n",
      "epoch:34 step:27177 [D loss: 0.050815, acc.: 98.44%] [G loss: 0.554253]\n",
      "epoch:34 step:27178 [D loss: 0.000734, acc.: 100.00%] [G loss: 0.581034]\n",
      "epoch:34 step:27179 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.106758]\n",
      "epoch:34 step:27180 [D loss: 0.002766, acc.: 100.00%] [G loss: 0.526415]\n",
      "epoch:34 step:27181 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:34 step:27182 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.132526]\n",
      "epoch:34 step:27183 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.012624]\n",
      "epoch:34 step:27184 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:34 step:27185 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.009848]\n",
      "epoch:34 step:27186 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.008035]\n",
      "epoch:34 step:27187 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.009795]\n",
      "epoch:34 step:27188 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.013107]\n",
      "epoch:34 step:27189 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.006884]\n",
      "epoch:34 step:27190 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.013484]\n",
      "epoch:34 step:27191 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.006265]\n",
      "epoch:34 step:27192 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.019475]\n",
      "epoch:34 step:27193 [D loss: 0.000459, acc.: 100.00%] [G loss: 0.006740]\n",
      "epoch:34 step:27194 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.002026]\n",
      "epoch:34 step:27195 [D loss: 0.002124, acc.: 100.00%] [G loss: 0.000503]\n",
      "epoch:34 step:27196 [D loss: 0.001443, acc.: 100.00%] [G loss: 0.005562]\n",
      "epoch:34 step:27197 [D loss: 0.013919, acc.: 100.00%] [G loss: 0.004983]\n",
      "epoch:34 step:27198 [D loss: 0.022159, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:34 step:27199 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.196849]\n",
      "epoch:34 step:27200 [D loss: 0.093847, acc.: 96.09%] [G loss: 0.004027]\n",
      "epoch:34 step:27201 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.001805]\n",
      "epoch:34 step:27202 [D loss: 0.001773, acc.: 100.00%] [G loss: 0.003125]\n",
      "epoch:34 step:27203 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.001031]\n",
      "epoch:34 step:27204 [D loss: 0.000809, acc.: 100.00%] [G loss: 0.001497]\n",
      "epoch:34 step:27205 [D loss: 0.003973, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:34 step:27206 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000357]\n",
      "epoch:34 step:27207 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:34 step:27208 [D loss: 0.003041, acc.: 100.00%] [G loss: 0.001027]\n",
      "epoch:34 step:27209 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.001490]\n",
      "epoch:34 step:27210 [D loss: 0.000715, acc.: 100.00%] [G loss: 0.003346]\n",
      "epoch:34 step:27211 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.003920]\n",
      "epoch:34 step:27212 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:34 step:27213 [D loss: 0.014003, acc.: 100.00%] [G loss: 0.029497]\n",
      "epoch:34 step:27214 [D loss: 0.000416, acc.: 100.00%] [G loss: 0.009740]\n",
      "epoch:34 step:27215 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.003814]\n",
      "epoch:34 step:27216 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:34 step:27217 [D loss: 0.001429, acc.: 100.00%] [G loss: 0.016583]\n",
      "epoch:34 step:27218 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:34 step:27219 [D loss: 0.005281, acc.: 100.00%] [G loss: 0.003863]\n",
      "epoch:34 step:27220 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:34 step:27221 [D loss: 0.001781, acc.: 100.00%] [G loss: 0.134630]\n",
      "epoch:34 step:27222 [D loss: 0.001522, acc.: 100.00%] [G loss: 0.029747]\n",
      "epoch:34 step:27223 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.017476]\n",
      "epoch:34 step:27224 [D loss: 0.118461, acc.: 95.31%] [G loss: 0.002607]\n",
      "epoch:34 step:27225 [D loss: 0.016312, acc.: 100.00%] [G loss: 1.556701]\n",
      "epoch:34 step:27226 [D loss: 0.016716, acc.: 99.22%] [G loss: 0.948001]\n",
      "epoch:34 step:27227 [D loss: 0.006710, acc.: 100.00%] [G loss: 0.119515]\n",
      "epoch:34 step:27228 [D loss: 0.032047, acc.: 98.44%] [G loss: 0.034312]\n",
      "epoch:34 step:27229 [D loss: 0.000696, acc.: 100.00%] [G loss: 0.001422]\n",
      "epoch:34 step:27230 [D loss: 0.001975, acc.: 100.00%] [G loss: 0.139001]\n",
      "epoch:34 step:27231 [D loss: 0.002057, acc.: 100.00%] [G loss: 0.043260]\n",
      "epoch:34 step:27232 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.005320]\n",
      "epoch:34 step:27233 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.004460]\n",
      "epoch:34 step:27234 [D loss: 0.005795, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:34 step:27235 [D loss: 0.000676, acc.: 100.00%] [G loss: 0.011709]\n",
      "epoch:34 step:27236 [D loss: 0.000787, acc.: 100.00%] [G loss: 0.016389]\n",
      "epoch:34 step:27237 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.004047]\n",
      "epoch:34 step:27238 [D loss: 0.001464, acc.: 100.00%] [G loss: 0.002318]\n",
      "epoch:34 step:27239 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.020251]\n",
      "epoch:34 step:27240 [D loss: 0.006049, acc.: 100.00%] [G loss: 0.777475]\n",
      "epoch:34 step:27241 [D loss: 0.008999, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:34 step:27242 [D loss: 0.000368, acc.: 100.00%] [G loss: 0.004682]\n",
      "epoch:34 step:27243 [D loss: 0.001766, acc.: 100.00%] [G loss: 0.003597]\n",
      "epoch:34 step:27244 [D loss: 0.001989, acc.: 100.00%] [G loss: 0.004757]\n",
      "epoch:34 step:27245 [D loss: 0.036683, acc.: 99.22%] [G loss: 0.079716]\n",
      "epoch:34 step:27246 [D loss: 0.037660, acc.: 100.00%] [G loss: 0.019920]\n",
      "epoch:34 step:27247 [D loss: 0.000706, acc.: 100.00%] [G loss: 0.240736]\n",
      "epoch:34 step:27248 [D loss: 0.002267, acc.: 100.00%] [G loss: 0.044924]\n",
      "epoch:34 step:27249 [D loss: 0.027091, acc.: 100.00%] [G loss: 0.110227]\n",
      "epoch:34 step:27250 [D loss: 0.001046, acc.: 100.00%] [G loss: 0.102763]\n",
      "epoch:34 step:27251 [D loss: 0.004661, acc.: 100.00%] [G loss: 0.130376]\n",
      "epoch:34 step:27252 [D loss: 0.000788, acc.: 100.00%] [G loss: 0.159505]\n",
      "epoch:34 step:27253 [D loss: 0.005970, acc.: 100.00%] [G loss: 0.091734]\n",
      "epoch:34 step:27254 [D loss: 0.002624, acc.: 100.00%] [G loss: 0.041850]\n",
      "epoch:34 step:27255 [D loss: 0.019982, acc.: 100.00%] [G loss: 0.278039]\n",
      "epoch:34 step:27256 [D loss: 0.030083, acc.: 98.44%] [G loss: 0.162197]\n",
      "epoch:34 step:27257 [D loss: 0.024315, acc.: 100.00%] [G loss: 0.151059]\n",
      "epoch:34 step:27258 [D loss: 0.148961, acc.: 97.66%] [G loss: 2.604424]\n",
      "epoch:34 step:27259 [D loss: 0.045736, acc.: 98.44%] [G loss: 2.321832]\n",
      "epoch:34 step:27260 [D loss: 0.010803, acc.: 100.00%] [G loss: 4.189604]\n",
      "epoch:34 step:27261 [D loss: 0.225592, acc.: 91.41%] [G loss: 3.880993]\n",
      "epoch:34 step:27262 [D loss: 0.419004, acc.: 79.69%] [G loss: 11.063366]\n",
      "epoch:34 step:27263 [D loss: 0.521822, acc.: 79.69%] [G loss: 4.403967]\n",
      "epoch:34 step:27264 [D loss: 0.437071, acc.: 82.81%] [G loss: 10.681488]\n",
      "epoch:34 step:27265 [D loss: 0.665761, acc.: 75.00%] [G loss: 6.548061]\n",
      "epoch:34 step:27266 [D loss: 0.580390, acc.: 84.38%] [G loss: 8.744869]\n",
      "epoch:34 step:27267 [D loss: 0.717164, acc.: 70.31%] [G loss: 6.855235]\n",
      "epoch:34 step:27268 [D loss: 0.050645, acc.: 98.44%] [G loss: 5.991280]\n",
      "epoch:34 step:27269 [D loss: 0.093983, acc.: 97.66%] [G loss: 0.098592]\n",
      "epoch:34 step:27270 [D loss: 0.006428, acc.: 100.00%] [G loss: 5.969000]\n",
      "epoch:34 step:27271 [D loss: 0.014628, acc.: 100.00%] [G loss: 4.524762]\n",
      "epoch:34 step:27272 [D loss: 0.063261, acc.: 97.66%] [G loss: 3.762673]\n",
      "epoch:34 step:27273 [D loss: 0.007934, acc.: 100.00%] [G loss: 0.539142]\n",
      "epoch:34 step:27274 [D loss: 0.041005, acc.: 100.00%] [G loss: 3.870230]\n",
      "epoch:34 step:27275 [D loss: 0.116075, acc.: 97.66%] [G loss: 3.960360]\n",
      "epoch:34 step:27276 [D loss: 0.114390, acc.: 96.09%] [G loss: 0.769048]\n",
      "epoch:34 step:27277 [D loss: 0.024829, acc.: 100.00%] [G loss: 3.086657]\n",
      "epoch:34 step:27278 [D loss: 0.070867, acc.: 97.66%] [G loss: 3.310126]\n",
      "epoch:34 step:27279 [D loss: 0.233518, acc.: 89.06%] [G loss: 4.181922]\n",
      "epoch:34 step:27280 [D loss: 0.024559, acc.: 99.22%] [G loss: 4.913202]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:34 step:27281 [D loss: 0.474486, acc.: 83.59%] [G loss: 0.504444]\n",
      "epoch:34 step:27282 [D loss: 0.431899, acc.: 73.44%] [G loss: 6.611993]\n",
      "epoch:34 step:27283 [D loss: 0.068907, acc.: 95.31%] [G loss: 6.955601]\n",
      "epoch:34 step:27284 [D loss: 0.554732, acc.: 76.56%] [G loss: 0.975301]\n",
      "epoch:34 step:27285 [D loss: 1.554937, acc.: 55.47%] [G loss: 11.215094]\n",
      "epoch:34 step:27286 [D loss: 1.558447, acc.: 53.91%] [G loss: 6.788403]\n",
      "epoch:34 step:27287 [D loss: 0.180621, acc.: 92.97%] [G loss: 0.309060]\n",
      "epoch:34 step:27288 [D loss: 0.034201, acc.: 99.22%] [G loss: 0.030600]\n",
      "epoch:34 step:27289 [D loss: 0.013460, acc.: 100.00%] [G loss: 0.027028]\n",
      "epoch:34 step:27290 [D loss: 0.179646, acc.: 93.75%] [G loss: 4.054028]\n",
      "epoch:34 step:27291 [D loss: 0.022864, acc.: 99.22%] [G loss: 4.207694]\n",
      "epoch:34 step:27292 [D loss: 0.035375, acc.: 98.44%] [G loss: 0.046860]\n",
      "epoch:34 step:27293 [D loss: 0.026010, acc.: 99.22%] [G loss: 1.675709]\n",
      "epoch:34 step:27294 [D loss: 0.004418, acc.: 100.00%] [G loss: 1.392316]\n",
      "epoch:34 step:27295 [D loss: 0.007607, acc.: 100.00%] [G loss: 0.973714]\n",
      "epoch:34 step:27296 [D loss: 0.032671, acc.: 99.22%] [G loss: 0.068350]\n",
      "epoch:34 step:27297 [D loss: 0.033972, acc.: 100.00%] [G loss: 0.222057]\n",
      "epoch:34 step:27298 [D loss: 0.017446, acc.: 100.00%] [G loss: 0.227156]\n",
      "epoch:34 step:27299 [D loss: 0.010345, acc.: 100.00%] [G loss: 0.213210]\n",
      "epoch:34 step:27300 [D loss: 0.012102, acc.: 99.22%] [G loss: 0.030989]\n",
      "epoch:34 step:27301 [D loss: 0.013260, acc.: 100.00%] [G loss: 1.229266]\n",
      "epoch:34 step:27302 [D loss: 0.066305, acc.: 97.66%] [G loss: 0.025090]\n",
      "epoch:34 step:27303 [D loss: 0.022232, acc.: 100.00%] [G loss: 0.016037]\n",
      "epoch:34 step:27304 [D loss: 0.159766, acc.: 94.53%] [G loss: 0.973190]\n",
      "epoch:34 step:27305 [D loss: 0.063632, acc.: 97.66%] [G loss: 1.804218]\n",
      "epoch:34 step:27306 [D loss: 0.157599, acc.: 92.19%] [G loss: 0.096580]\n",
      "epoch:34 step:27307 [D loss: 0.056530, acc.: 97.66%] [G loss: 0.013584]\n",
      "epoch:34 step:27308 [D loss: 0.012796, acc.: 100.00%] [G loss: 0.001145]\n",
      "epoch:34 step:27309 [D loss: 0.007706, acc.: 100.00%] [G loss: 0.014038]\n",
      "epoch:34 step:27310 [D loss: 0.002744, acc.: 100.00%] [G loss: 0.016238]\n",
      "epoch:34 step:27311 [D loss: 0.001469, acc.: 100.00%] [G loss: 0.003032]\n",
      "epoch:34 step:27312 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.002538]\n",
      "epoch:34 step:27313 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.001258]\n",
      "epoch:34 step:27314 [D loss: 0.000921, acc.: 100.00%] [G loss: 0.009034]\n",
      "epoch:34 step:27315 [D loss: 0.001586, acc.: 100.00%] [G loss: 0.085366]\n",
      "epoch:34 step:27316 [D loss: 0.014901, acc.: 99.22%] [G loss: 0.002247]\n",
      "epoch:34 step:27317 [D loss: 0.009808, acc.: 100.00%] [G loss: 0.008999]\n",
      "epoch:34 step:27318 [D loss: 0.018669, acc.: 100.00%] [G loss: 0.000712]\n",
      "epoch:34 step:27319 [D loss: 0.002047, acc.: 100.00%] [G loss: 0.066732]\n",
      "epoch:34 step:27320 [D loss: 0.001443, acc.: 100.00%] [G loss: 0.003074]\n",
      "epoch:34 step:27321 [D loss: 0.000938, acc.: 100.00%] [G loss: 0.003054]\n",
      "epoch:34 step:27322 [D loss: 0.006755, acc.: 100.00%] [G loss: 0.004596]\n",
      "epoch:34 step:27323 [D loss: 0.001708, acc.: 100.00%] [G loss: 0.003686]\n",
      "epoch:34 step:27324 [D loss: 0.005941, acc.: 100.00%] [G loss: 0.012678]\n",
      "epoch:34 step:27325 [D loss: 0.015596, acc.: 100.00%] [G loss: 0.005436]\n",
      "epoch:34 step:27326 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.001205]\n",
      "epoch:34 step:27327 [D loss: 0.001467, acc.: 100.00%] [G loss: 0.005101]\n",
      "epoch:34 step:27328 [D loss: 0.001568, acc.: 100.00%] [G loss: 0.001314]\n",
      "epoch:34 step:27329 [D loss: 0.001502, acc.: 100.00%] [G loss: 0.001131]\n",
      "epoch:34 step:27330 [D loss: 0.001599, acc.: 100.00%] [G loss: 0.001207]\n",
      "epoch:34 step:27331 [D loss: 0.004050, acc.: 100.00%] [G loss: 0.012013]\n",
      "epoch:34 step:27332 [D loss: 0.021037, acc.: 99.22%] [G loss: 0.002242]\n",
      "epoch:34 step:27333 [D loss: 0.005639, acc.: 100.00%] [G loss: 0.049719]\n",
      "epoch:34 step:27334 [D loss: 0.002230, acc.: 100.00%] [G loss: 0.007850]\n",
      "epoch:34 step:27335 [D loss: 0.001786, acc.: 100.00%] [G loss: 0.716635]\n",
      "epoch:35 step:27336 [D loss: 0.002683, acc.: 100.00%] [G loss: 0.001367]\n",
      "epoch:35 step:27337 [D loss: 0.017123, acc.: 100.00%] [G loss: 0.002292]\n",
      "epoch:35 step:27338 [D loss: 0.006889, acc.: 100.00%] [G loss: 0.002722]\n",
      "epoch:35 step:27339 [D loss: 0.001790, acc.: 100.00%] [G loss: 0.002138]\n",
      "epoch:35 step:27340 [D loss: 0.013102, acc.: 100.00%] [G loss: 0.015139]\n",
      "epoch:35 step:27341 [D loss: 0.006600, acc.: 100.00%] [G loss: 0.041205]\n",
      "epoch:35 step:27342 [D loss: 0.051683, acc.: 97.66%] [G loss: 0.569861]\n",
      "epoch:35 step:27343 [D loss: 0.054624, acc.: 98.44%] [G loss: 0.243692]\n",
      "epoch:35 step:27344 [D loss: 0.045609, acc.: 99.22%] [G loss: 0.973369]\n",
      "epoch:35 step:27345 [D loss: 0.071694, acc.: 97.66%] [G loss: 0.293005]\n",
      "epoch:35 step:27346 [D loss: 0.021698, acc.: 100.00%] [G loss: 0.283482]\n",
      "epoch:35 step:27347 [D loss: 0.042992, acc.: 100.00%] [G loss: 1.531249]\n",
      "epoch:35 step:27348 [D loss: 0.064338, acc.: 99.22%] [G loss: 1.983498]\n",
      "epoch:35 step:27349 [D loss: 0.375561, acc.: 82.03%] [G loss: 8.000518]\n",
      "epoch:35 step:27350 [D loss: 1.436084, acc.: 58.59%] [G loss: 3.581285]\n",
      "epoch:35 step:27351 [D loss: 0.808119, acc.: 67.19%] [G loss: 9.054523]\n",
      "epoch:35 step:27352 [D loss: 0.268462, acc.: 86.72%] [G loss: 8.369772]\n",
      "epoch:35 step:27353 [D loss: 0.163659, acc.: 89.84%] [G loss: 1.018435]\n",
      "epoch:35 step:27354 [D loss: 0.007835, acc.: 100.00%] [G loss: 5.360106]\n",
      "epoch:35 step:27355 [D loss: 0.005961, acc.: 100.00%] [G loss: 4.489515]\n",
      "epoch:35 step:27356 [D loss: 0.017578, acc.: 100.00%] [G loss: 0.043409]\n",
      "epoch:35 step:27357 [D loss: 0.006442, acc.: 100.00%] [G loss: 3.886466]\n",
      "epoch:35 step:27358 [D loss: 0.012669, acc.: 99.22%] [G loss: 2.609996]\n",
      "epoch:35 step:27359 [D loss: 0.011578, acc.: 100.00%] [G loss: 2.032897]\n",
      "epoch:35 step:27360 [D loss: 0.014724, acc.: 99.22%] [G loss: 1.238341]\n",
      "epoch:35 step:27361 [D loss: 0.015083, acc.: 100.00%] [G loss: 1.109940]\n",
      "epoch:35 step:27362 [D loss: 0.241412, acc.: 86.72%] [G loss: 4.586853]\n",
      "epoch:35 step:27363 [D loss: 0.037333, acc.: 98.44%] [G loss: 5.060340]\n",
      "epoch:35 step:27364 [D loss: 0.102892, acc.: 94.53%] [G loss: 4.154840]\n",
      "epoch:35 step:27365 [D loss: 0.041891, acc.: 99.22%] [G loss: 3.698910]\n",
      "epoch:35 step:27366 [D loss: 0.042428, acc.: 98.44%] [G loss: 1.554286]\n",
      "epoch:35 step:27367 [D loss: 0.165880, acc.: 91.41%] [G loss: 3.022304]\n",
      "epoch:35 step:27368 [D loss: 0.068192, acc.: 97.66%] [G loss: 3.208605]\n",
      "epoch:35 step:27369 [D loss: 0.057811, acc.: 98.44%] [G loss: 3.326831]\n",
      "epoch:35 step:27370 [D loss: 0.191741, acc.: 92.19%] [G loss: 1.790868]\n",
      "epoch:35 step:27371 [D loss: 0.517552, acc.: 74.22%] [G loss: 2.210893]\n",
      "epoch:35 step:27372 [D loss: 0.080274, acc.: 96.88%] [G loss: 0.777874]\n",
      "epoch:35 step:27373 [D loss: 0.266091, acc.: 85.16%] [G loss: 5.616436]\n",
      "epoch:35 step:27374 [D loss: 0.005237, acc.: 100.00%] [G loss: 3.525176]\n",
      "epoch:35 step:27375 [D loss: 0.002822, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:35 step:27376 [D loss: 0.004503, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:35 step:27377 [D loss: 0.295974, acc.: 88.28%] [G loss: 4.547340]\n",
      "epoch:35 step:27378 [D loss: 0.237016, acc.: 87.50%] [G loss: 0.005806]\n",
      "epoch:35 step:27379 [D loss: 0.052839, acc.: 96.88%] [G loss: 0.651717]\n",
      "epoch:35 step:27380 [D loss: 0.024205, acc.: 100.00%] [G loss: 0.071332]\n",
      "epoch:35 step:27381 [D loss: 0.102124, acc.: 97.66%] [G loss: 2.649556]\n",
      "epoch:35 step:27382 [D loss: 0.019589, acc.: 99.22%] [G loss: 3.282509]\n",
      "epoch:35 step:27383 [D loss: 0.015849, acc.: 100.00%] [G loss: 0.246002]\n",
      "epoch:35 step:27384 [D loss: 0.004206, acc.: 100.00%] [G loss: 1.230678]\n",
      "epoch:35 step:27385 [D loss: 0.028427, acc.: 100.00%] [G loss: 0.685229]\n",
      "epoch:35 step:27386 [D loss: 0.000957, acc.: 100.00%] [G loss: 0.009189]\n",
      "epoch:35 step:27387 [D loss: 0.026670, acc.: 100.00%] [G loss: 0.478128]\n",
      "epoch:35 step:27388 [D loss: 0.000700, acc.: 100.00%] [G loss: 0.177770]\n",
      "epoch:35 step:27389 [D loss: 0.005662, acc.: 100.00%] [G loss: 0.297607]\n",
      "epoch:35 step:27390 [D loss: 0.000698, acc.: 100.00%] [G loss: 0.085146]\n",
      "epoch:35 step:27391 [D loss: 0.050286, acc.: 99.22%] [G loss: 0.006836]\n",
      "epoch:35 step:27392 [D loss: 0.002585, acc.: 100.00%] [G loss: 0.200303]\n",
      "epoch:35 step:27393 [D loss: 0.002164, acc.: 100.00%] [G loss: 0.010581]\n",
      "epoch:35 step:27394 [D loss: 0.003896, acc.: 100.00%] [G loss: 0.012774]\n",
      "epoch:35 step:27395 [D loss: 0.000460, acc.: 100.00%] [G loss: 0.002414]\n",
      "epoch:35 step:27396 [D loss: 0.002375, acc.: 100.00%] [G loss: 0.056923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27397 [D loss: 0.007257, acc.: 100.00%] [G loss: 0.026107]\n",
      "epoch:35 step:27398 [D loss: 0.002064, acc.: 100.00%] [G loss: 0.032043]\n",
      "epoch:35 step:27399 [D loss: 0.005832, acc.: 100.00%] [G loss: 0.018830]\n",
      "epoch:35 step:27400 [D loss: 0.017098, acc.: 99.22%] [G loss: 0.001726]\n",
      "epoch:35 step:27401 [D loss: 0.045165, acc.: 100.00%] [G loss: 0.085765]\n",
      "epoch:35 step:27402 [D loss: 0.004275, acc.: 100.00%] [G loss: 0.208186]\n",
      "epoch:35 step:27403 [D loss: 0.005585, acc.: 100.00%] [G loss: 0.195210]\n",
      "epoch:35 step:27404 [D loss: 0.000773, acc.: 100.00%] [G loss: 0.062391]\n",
      "epoch:35 step:27405 [D loss: 0.001670, acc.: 100.00%] [G loss: 0.124691]\n",
      "epoch:35 step:27406 [D loss: 0.023698, acc.: 100.00%] [G loss: 0.780308]\n",
      "epoch:35 step:27407 [D loss: 0.004679, acc.: 100.00%] [G loss: 0.369109]\n",
      "epoch:35 step:27408 [D loss: 0.014702, acc.: 99.22%] [G loss: 0.160780]\n",
      "epoch:35 step:27409 [D loss: 0.016616, acc.: 100.00%] [G loss: 0.081208]\n",
      "epoch:35 step:27410 [D loss: 0.003115, acc.: 100.00%] [G loss: 0.068047]\n",
      "epoch:35 step:27411 [D loss: 0.008772, acc.: 100.00%] [G loss: 0.277898]\n",
      "epoch:35 step:27412 [D loss: 0.083447, acc.: 98.44%] [G loss: 0.004579]\n",
      "epoch:35 step:27413 [D loss: 0.008151, acc.: 100.00%] [G loss: 0.005303]\n",
      "epoch:35 step:27414 [D loss: 0.003760, acc.: 100.00%] [G loss: 0.007044]\n",
      "epoch:35 step:27415 [D loss: 0.071325, acc.: 98.44%] [G loss: 0.166545]\n",
      "epoch:35 step:27416 [D loss: 0.006084, acc.: 100.00%] [G loss: 0.324964]\n",
      "epoch:35 step:27417 [D loss: 0.001822, acc.: 100.00%] [G loss: 0.952500]\n",
      "epoch:35 step:27418 [D loss: 0.031436, acc.: 98.44%] [G loss: 0.047156]\n",
      "epoch:35 step:27419 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.100539]\n",
      "epoch:35 step:27420 [D loss: 0.004385, acc.: 100.00%] [G loss: 0.070559]\n",
      "epoch:35 step:27421 [D loss: 0.002023, acc.: 100.00%] [G loss: 0.017847]\n",
      "epoch:35 step:27422 [D loss: 0.004259, acc.: 100.00%] [G loss: 0.060299]\n",
      "epoch:35 step:27423 [D loss: 0.004810, acc.: 100.00%] [G loss: 0.017388]\n",
      "epoch:35 step:27424 [D loss: 0.010682, acc.: 100.00%] [G loss: 0.023134]\n",
      "epoch:35 step:27425 [D loss: 0.194932, acc.: 89.84%] [G loss: 0.015712]\n",
      "epoch:35 step:27426 [D loss: 0.013061, acc.: 100.00%] [G loss: 0.003979]\n",
      "epoch:35 step:27427 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.337079]\n",
      "epoch:35 step:27428 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.073514]\n",
      "epoch:35 step:27429 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.098288]\n",
      "epoch:35 step:27430 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000698]\n",
      "epoch:35 step:27431 [D loss: 0.001406, acc.: 100.00%] [G loss: 0.086255]\n",
      "epoch:35 step:27432 [D loss: 0.000968, acc.: 100.00%] [G loss: 0.051208]\n",
      "epoch:35 step:27433 [D loss: 0.002889, acc.: 100.00%] [G loss: 0.038199]\n",
      "epoch:35 step:27434 [D loss: 0.010516, acc.: 100.00%] [G loss: 0.014846]\n",
      "epoch:35 step:27435 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.005598]\n",
      "epoch:35 step:27436 [D loss: 0.002381, acc.: 100.00%] [G loss: 0.553856]\n",
      "epoch:35 step:27437 [D loss: 0.001618, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:35 step:27438 [D loss: 0.002868, acc.: 100.00%] [G loss: 0.009837]\n",
      "epoch:35 step:27439 [D loss: 0.006625, acc.: 100.00%] [G loss: 0.007013]\n",
      "epoch:35 step:27440 [D loss: 0.008851, acc.: 100.00%] [G loss: 0.001421]\n",
      "epoch:35 step:27441 [D loss: 0.001079, acc.: 100.00%] [G loss: 0.028402]\n",
      "epoch:35 step:27442 [D loss: 0.014795, acc.: 100.00%] [G loss: 0.004416]\n",
      "epoch:35 step:27443 [D loss: 0.005698, acc.: 100.00%] [G loss: 0.131169]\n",
      "epoch:35 step:27444 [D loss: 0.002298, acc.: 100.00%] [G loss: 0.017558]\n",
      "epoch:35 step:27445 [D loss: 0.000980, acc.: 100.00%] [G loss: 0.067278]\n",
      "epoch:35 step:27446 [D loss: 0.005756, acc.: 100.00%] [G loss: 0.008732]\n",
      "epoch:35 step:27447 [D loss: 0.000980, acc.: 100.00%] [G loss: 0.004045]\n",
      "epoch:35 step:27448 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.004479]\n",
      "epoch:35 step:27449 [D loss: 0.002439, acc.: 100.00%] [G loss: 0.016879]\n",
      "epoch:35 step:27450 [D loss: 0.001516, acc.: 100.00%] [G loss: 0.018036]\n",
      "epoch:35 step:27451 [D loss: 0.008239, acc.: 100.00%] [G loss: 0.005199]\n",
      "epoch:35 step:27452 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.009242]\n",
      "epoch:35 step:27453 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.000669]\n",
      "epoch:35 step:27454 [D loss: 0.004517, acc.: 100.00%] [G loss: 0.003834]\n",
      "epoch:35 step:27455 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.002428]\n",
      "epoch:35 step:27456 [D loss: 0.000712, acc.: 100.00%] [G loss: 0.027794]\n",
      "epoch:35 step:27457 [D loss: 0.000850, acc.: 100.00%] [G loss: 0.001384]\n",
      "epoch:35 step:27458 [D loss: 0.004297, acc.: 100.00%] [G loss: 0.007716]\n",
      "epoch:35 step:27459 [D loss: 0.004967, acc.: 100.00%] [G loss: 0.023976]\n",
      "epoch:35 step:27460 [D loss: 0.021566, acc.: 100.00%] [G loss: 0.008376]\n",
      "epoch:35 step:27461 [D loss: 0.001094, acc.: 100.00%] [G loss: 0.096748]\n",
      "epoch:35 step:27462 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.032529]\n",
      "epoch:35 step:27463 [D loss: 0.005100, acc.: 100.00%] [G loss: 0.011868]\n",
      "epoch:35 step:27464 [D loss: 0.000736, acc.: 100.00%] [G loss: 0.846400]\n",
      "epoch:35 step:27465 [D loss: 0.006502, acc.: 100.00%] [G loss: 0.012325]\n",
      "epoch:35 step:27466 [D loss: 0.002788, acc.: 100.00%] [G loss: 0.047937]\n",
      "epoch:35 step:27467 [D loss: 0.037758, acc.: 99.22%] [G loss: 0.060039]\n",
      "epoch:35 step:27468 [D loss: 0.002728, acc.: 100.00%] [G loss: 0.115652]\n",
      "epoch:35 step:27469 [D loss: 0.002142, acc.: 100.00%] [G loss: 0.261717]\n",
      "epoch:35 step:27470 [D loss: 0.066355, acc.: 99.22%] [G loss: 0.085650]\n",
      "epoch:35 step:27471 [D loss: 0.004619, acc.: 100.00%] [G loss: 0.016430]\n",
      "epoch:35 step:27472 [D loss: 0.006860, acc.: 100.00%] [G loss: 0.069943]\n",
      "epoch:35 step:27473 [D loss: 0.003436, acc.: 100.00%] [G loss: 0.041062]\n",
      "epoch:35 step:27474 [D loss: 0.011061, acc.: 99.22%] [G loss: 0.030993]\n",
      "epoch:35 step:27475 [D loss: 0.020321, acc.: 100.00%] [G loss: 0.020474]\n",
      "epoch:35 step:27476 [D loss: 0.002606, acc.: 100.00%] [G loss: 0.014504]\n",
      "epoch:35 step:27477 [D loss: 0.001200, acc.: 100.00%] [G loss: 0.006927]\n",
      "epoch:35 step:27478 [D loss: 0.001586, acc.: 100.00%] [G loss: 0.007756]\n",
      "epoch:35 step:27479 [D loss: 0.001188, acc.: 100.00%] [G loss: 0.017758]\n",
      "epoch:35 step:27480 [D loss: 0.001423, acc.: 100.00%] [G loss: 0.014146]\n",
      "epoch:35 step:27481 [D loss: 0.006774, acc.: 100.00%] [G loss: 0.022925]\n",
      "epoch:35 step:27482 [D loss: 0.008085, acc.: 100.00%] [G loss: 0.005967]\n",
      "epoch:35 step:27483 [D loss: 0.001899, acc.: 100.00%] [G loss: 0.013332]\n",
      "epoch:35 step:27484 [D loss: 0.001782, acc.: 100.00%] [G loss: 0.202764]\n",
      "epoch:35 step:27485 [D loss: 0.004983, acc.: 100.00%] [G loss: 0.002685]\n",
      "epoch:35 step:27486 [D loss: 0.031953, acc.: 100.00%] [G loss: 0.019390]\n",
      "epoch:35 step:27487 [D loss: 0.004750, acc.: 100.00%] [G loss: 0.409107]\n",
      "epoch:35 step:27488 [D loss: 0.070532, acc.: 97.66%] [G loss: 0.095822]\n",
      "epoch:35 step:27489 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.022354]\n",
      "epoch:35 step:27490 [D loss: 0.000955, acc.: 100.00%] [G loss: 0.023352]\n",
      "epoch:35 step:27491 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.004558]\n",
      "epoch:35 step:27492 [D loss: 0.002621, acc.: 100.00%] [G loss: 0.054201]\n",
      "epoch:35 step:27493 [D loss: 0.004126, acc.: 100.00%] [G loss: 0.003326]\n",
      "epoch:35 step:27494 [D loss: 0.002099, acc.: 100.00%] [G loss: 0.003597]\n",
      "epoch:35 step:27495 [D loss: 0.008722, acc.: 100.00%] [G loss: 0.002207]\n",
      "epoch:35 step:27496 [D loss: 0.010161, acc.: 100.00%] [G loss: 0.003291]\n",
      "epoch:35 step:27497 [D loss: 0.013376, acc.: 100.00%] [G loss: 0.062369]\n",
      "epoch:35 step:27498 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.053134]\n",
      "epoch:35 step:27499 [D loss: 0.002496, acc.: 100.00%] [G loss: 3.199014]\n",
      "epoch:35 step:27500 [D loss: 0.111593, acc.: 95.31%] [G loss: 4.700653]\n",
      "epoch:35 step:27501 [D loss: 0.240614, acc.: 89.84%] [G loss: 0.306344]\n",
      "epoch:35 step:27502 [D loss: 0.341994, acc.: 84.38%] [G loss: 8.442062]\n",
      "epoch:35 step:27503 [D loss: 0.601225, acc.: 73.44%] [G loss: 0.407514]\n",
      "epoch:35 step:27504 [D loss: 0.032545, acc.: 99.22%] [G loss: 2.290214]\n",
      "epoch:35 step:27505 [D loss: 0.012324, acc.: 100.00%] [G loss: 1.761393]\n",
      "epoch:35 step:27506 [D loss: 0.001970, acc.: 100.00%] [G loss: 1.441402]\n",
      "epoch:35 step:27507 [D loss: 0.011334, acc.: 100.00%] [G loss: 0.938534]\n",
      "epoch:35 step:27508 [D loss: 0.002486, acc.: 100.00%] [G loss: 0.584810]\n",
      "epoch:35 step:27509 [D loss: 0.081714, acc.: 96.88%] [G loss: 1.460319]\n",
      "epoch:35 step:27510 [D loss: 0.019053, acc.: 99.22%] [G loss: 1.557779]\n",
      "epoch:35 step:27511 [D loss: 0.304926, acc.: 85.16%] [G loss: 0.059589]\n",
      "epoch:35 step:27512 [D loss: 0.216597, acc.: 88.28%] [G loss: 1.617405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27513 [D loss: 0.033346, acc.: 98.44%] [G loss: 2.724893]\n",
      "epoch:35 step:27514 [D loss: 0.045517, acc.: 97.66%] [G loss: 2.321836]\n",
      "epoch:35 step:27515 [D loss: 0.138915, acc.: 97.66%] [G loss: 0.210206]\n",
      "epoch:35 step:27516 [D loss: 0.025354, acc.: 100.00%] [G loss: 0.164918]\n",
      "epoch:35 step:27517 [D loss: 0.004767, acc.: 100.00%] [G loss: 0.000808]\n",
      "epoch:35 step:27518 [D loss: 0.001836, acc.: 100.00%] [G loss: 0.301251]\n",
      "epoch:35 step:27519 [D loss: 0.000498, acc.: 100.00%] [G loss: 0.119149]\n",
      "epoch:35 step:27520 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.080144]\n",
      "epoch:35 step:27521 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.110384]\n",
      "epoch:35 step:27522 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.026592]\n",
      "epoch:35 step:27523 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.030688]\n",
      "epoch:35 step:27524 [D loss: 0.000890, acc.: 100.00%] [G loss: 0.012242]\n",
      "epoch:35 step:27525 [D loss: 0.000730, acc.: 100.00%] [G loss: 0.001154]\n",
      "epoch:35 step:27526 [D loss: 0.001753, acc.: 100.00%] [G loss: 0.028019]\n",
      "epoch:35 step:27527 [D loss: 0.000043, acc.: 100.00%] [G loss: 1.764665]\n",
      "epoch:35 step:27528 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.013395]\n",
      "epoch:35 step:27529 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:35 step:27530 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:35 step:27531 [D loss: 0.094014, acc.: 95.31%] [G loss: 0.345687]\n",
      "epoch:35 step:27532 [D loss: 0.012464, acc.: 100.00%] [G loss: 0.995660]\n",
      "epoch:35 step:27533 [D loss: 0.104680, acc.: 97.66%] [G loss: 0.128671]\n",
      "epoch:35 step:27534 [D loss: 0.008142, acc.: 100.00%] [G loss: 0.022524]\n",
      "epoch:35 step:27535 [D loss: 0.171228, acc.: 93.75%] [G loss: 1.596548]\n",
      "epoch:35 step:27536 [D loss: 0.001547, acc.: 100.00%] [G loss: 1.217992]\n",
      "epoch:35 step:27537 [D loss: 0.251680, acc.: 89.84%] [G loss: 1.588503]\n",
      "epoch:35 step:27538 [D loss: 0.000995, acc.: 100.00%] [G loss: 0.024719]\n",
      "epoch:35 step:27539 [D loss: 0.002823, acc.: 100.00%] [G loss: 0.007324]\n",
      "epoch:35 step:27540 [D loss: 0.011692, acc.: 99.22%] [G loss: 0.044763]\n",
      "epoch:35 step:27541 [D loss: 0.018973, acc.: 100.00%] [G loss: 0.069712]\n",
      "epoch:35 step:27542 [D loss: 0.020326, acc.: 100.00%] [G loss: 0.014511]\n",
      "epoch:35 step:27543 [D loss: 0.036982, acc.: 99.22%] [G loss: 0.015273]\n",
      "epoch:35 step:27544 [D loss: 0.028322, acc.: 99.22%] [G loss: 0.001650]\n",
      "epoch:35 step:27545 [D loss: 0.000989, acc.: 100.00%] [G loss: 0.000741]\n",
      "epoch:35 step:27546 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.006226]\n",
      "epoch:35 step:27547 [D loss: 0.000795, acc.: 100.00%] [G loss: 0.045920]\n",
      "epoch:35 step:27548 [D loss: 0.001716, acc.: 100.00%] [G loss: 0.132872]\n",
      "epoch:35 step:27549 [D loss: 0.036325, acc.: 100.00%] [G loss: 0.025174]\n",
      "epoch:35 step:27550 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.006793]\n",
      "epoch:35 step:27551 [D loss: 0.012942, acc.: 99.22%] [G loss: 0.021536]\n",
      "epoch:35 step:27552 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.519745]\n",
      "epoch:35 step:27553 [D loss: 0.005696, acc.: 100.00%] [G loss: 0.000436]\n",
      "epoch:35 step:27554 [D loss: 0.001268, acc.: 100.00%] [G loss: 0.001369]\n",
      "epoch:35 step:27555 [D loss: 0.014354, acc.: 99.22%] [G loss: 0.000052]\n",
      "epoch:35 step:27556 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.004461]\n",
      "epoch:35 step:27557 [D loss: 0.000443, acc.: 100.00%] [G loss: 0.016082]\n",
      "epoch:35 step:27558 [D loss: 0.000727, acc.: 100.00%] [G loss: 0.001161]\n",
      "epoch:35 step:27559 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:35 step:27560 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:35 step:27561 [D loss: 0.000672, acc.: 100.00%] [G loss: 0.025148]\n",
      "epoch:35 step:27562 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:35 step:27563 [D loss: 0.000538, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:35 step:27564 [D loss: 0.022100, acc.: 98.44%] [G loss: 0.003211]\n",
      "epoch:35 step:27565 [D loss: 0.001936, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:35 step:27566 [D loss: 0.001179, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:35 step:27567 [D loss: 0.003321, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:35 step:27568 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000806]\n",
      "epoch:35 step:27569 [D loss: 0.000808, acc.: 100.00%] [G loss: 0.000410]\n",
      "epoch:35 step:27570 [D loss: 0.007683, acc.: 100.00%] [G loss: 0.966918]\n",
      "epoch:35 step:27571 [D loss: 0.002188, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:35 step:27572 [D loss: 0.000416, acc.: 100.00%] [G loss: 0.005189]\n",
      "epoch:35 step:27573 [D loss: 0.024329, acc.: 100.00%] [G loss: 0.102264]\n",
      "epoch:35 step:27574 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.251591]\n",
      "epoch:35 step:27575 [D loss: 0.008858, acc.: 100.00%] [G loss: 0.011107]\n",
      "epoch:35 step:27576 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.016771]\n",
      "epoch:35 step:27577 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.007857]\n",
      "epoch:35 step:27578 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.010307]\n",
      "epoch:35 step:27579 [D loss: 0.001428, acc.: 100.00%] [G loss: 0.003626]\n",
      "epoch:35 step:27580 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.003252]\n",
      "epoch:35 step:27581 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:35 step:27582 [D loss: 0.001042, acc.: 100.00%] [G loss: 0.001416]\n",
      "epoch:35 step:27583 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.003571]\n",
      "epoch:35 step:27584 [D loss: 0.000983, acc.: 100.00%] [G loss: 0.001661]\n",
      "epoch:35 step:27585 [D loss: 0.008359, acc.: 99.22%] [G loss: 0.000604]\n",
      "epoch:35 step:27586 [D loss: 0.003532, acc.: 100.00%] [G loss: 0.004727]\n",
      "epoch:35 step:27587 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000856]\n",
      "epoch:35 step:27588 [D loss: 0.000924, acc.: 100.00%] [G loss: 0.003400]\n",
      "epoch:35 step:27589 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001572]\n",
      "epoch:35 step:27590 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:35 step:27591 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000299]\n",
      "epoch:35 step:27592 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:35 step:27593 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:35 step:27594 [D loss: 0.000788, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:35 step:27595 [D loss: 0.000625, acc.: 100.00%] [G loss: 0.001521]\n",
      "epoch:35 step:27596 [D loss: 0.028122, acc.: 99.22%] [G loss: 0.029645]\n",
      "epoch:35 step:27597 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.015718]\n",
      "epoch:35 step:27598 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.050593]\n",
      "epoch:35 step:27599 [D loss: 0.002683, acc.: 100.00%] [G loss: 0.011981]\n",
      "epoch:35 step:27600 [D loss: 0.024256, acc.: 100.00%] [G loss: 0.000709]\n",
      "epoch:35 step:27601 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:35 step:27602 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:35 step:27603 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000839]\n",
      "epoch:35 step:27604 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:35 step:27605 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000993]\n",
      "epoch:35 step:27606 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:35 step:27607 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000412]\n",
      "epoch:35 step:27608 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.001399]\n",
      "epoch:35 step:27609 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.006242]\n",
      "epoch:35 step:27610 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.002286]\n",
      "epoch:35 step:27611 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:35 step:27612 [D loss: 0.005252, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:35 step:27613 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000924]\n",
      "epoch:35 step:27614 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:35 step:27615 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000376]\n",
      "epoch:35 step:27616 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:35 step:27617 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.041205]\n",
      "epoch:35 step:27618 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:35 step:27619 [D loss: 0.000774, acc.: 100.00%] [G loss: 0.122749]\n",
      "epoch:35 step:27620 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:35 step:27621 [D loss: 0.001877, acc.: 100.00%] [G loss: 0.001014]\n",
      "epoch:35 step:27622 [D loss: 0.018213, acc.: 100.00%] [G loss: 0.004123]\n",
      "epoch:35 step:27623 [D loss: 0.000399, acc.: 100.00%] [G loss: 0.002342]\n",
      "epoch:35 step:27624 [D loss: 0.011087, acc.: 100.00%] [G loss: 0.004022]\n",
      "epoch:35 step:27625 [D loss: 0.000026, acc.: 100.00%] [G loss: 1.680538]\n",
      "epoch:35 step:27626 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.003764]\n",
      "epoch:35 step:27627 [D loss: 0.002682, acc.: 100.00%] [G loss: 0.020155]\n",
      "epoch:35 step:27628 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.004290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27629 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.234487]\n",
      "epoch:35 step:27630 [D loss: 0.012903, acc.: 100.00%] [G loss: 0.000904]\n",
      "epoch:35 step:27631 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.000945]\n",
      "epoch:35 step:27632 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000985]\n",
      "epoch:35 step:27633 [D loss: 0.001458, acc.: 100.00%] [G loss: 0.001301]\n",
      "epoch:35 step:27634 [D loss: 0.005309, acc.: 100.00%] [G loss: 0.019938]\n",
      "epoch:35 step:27635 [D loss: 0.005425, acc.: 100.00%] [G loss: 0.000565]\n",
      "epoch:35 step:27636 [D loss: 0.003323, acc.: 100.00%] [G loss: 0.000772]\n",
      "epoch:35 step:27637 [D loss: 0.027053, acc.: 100.00%] [G loss: 0.075591]\n",
      "epoch:35 step:27638 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.106378]\n",
      "epoch:35 step:27639 [D loss: 0.001962, acc.: 100.00%] [G loss: 0.107378]\n",
      "epoch:35 step:27640 [D loss: 0.002326, acc.: 100.00%] [G loss: 0.026175]\n",
      "epoch:35 step:27641 [D loss: 0.001359, acc.: 100.00%] [G loss: 0.091775]\n",
      "epoch:35 step:27642 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.103714]\n",
      "epoch:35 step:27643 [D loss: 0.000807, acc.: 100.00%] [G loss: 0.018097]\n",
      "epoch:35 step:27644 [D loss: 0.001041, acc.: 100.00%] [G loss: 0.008687]\n",
      "epoch:35 step:27645 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.013092]\n",
      "epoch:35 step:27646 [D loss: 0.001357, acc.: 100.00%] [G loss: 0.193257]\n",
      "epoch:35 step:27647 [D loss: 0.022657, acc.: 100.00%] [G loss: 0.066881]\n",
      "epoch:35 step:27648 [D loss: 0.010159, acc.: 100.00%] [G loss: 0.009269]\n",
      "epoch:35 step:27649 [D loss: 0.000907, acc.: 100.00%] [G loss: 0.044937]\n",
      "epoch:35 step:27650 [D loss: 6.443021, acc.: 8.59%] [G loss: 6.089442]\n",
      "epoch:35 step:27651 [D loss: 0.363900, acc.: 83.59%] [G loss: 6.046744]\n",
      "epoch:35 step:27652 [D loss: 0.581626, acc.: 71.88%] [G loss: 1.090088]\n",
      "epoch:35 step:27653 [D loss: 0.002864, acc.: 100.00%] [G loss: 0.115680]\n",
      "epoch:35 step:27654 [D loss: 0.012670, acc.: 100.00%] [G loss: 0.136206]\n",
      "epoch:35 step:27655 [D loss: 0.096301, acc.: 97.66%] [G loss: 0.176498]\n",
      "epoch:35 step:27656 [D loss: 0.045282, acc.: 98.44%] [G loss: 0.247026]\n",
      "epoch:35 step:27657 [D loss: 0.077473, acc.: 100.00%] [G loss: 0.273664]\n",
      "epoch:35 step:27658 [D loss: 0.005047, acc.: 100.00%] [G loss: 0.573841]\n",
      "epoch:35 step:27659 [D loss: 0.036653, acc.: 100.00%] [G loss: 0.153650]\n",
      "epoch:35 step:27660 [D loss: 0.045985, acc.: 100.00%] [G loss: 0.237659]\n",
      "epoch:35 step:27661 [D loss: 0.009645, acc.: 99.22%] [G loss: 0.161996]\n",
      "epoch:35 step:27662 [D loss: 0.227618, acc.: 89.06%] [G loss: 3.570099]\n",
      "epoch:35 step:27663 [D loss: 0.043349, acc.: 97.66%] [G loss: 4.407204]\n",
      "epoch:35 step:27664 [D loss: 0.118773, acc.: 93.75%] [G loss: 4.149512]\n",
      "epoch:35 step:27665 [D loss: 0.172370, acc.: 95.31%] [G loss: 3.574401]\n",
      "epoch:35 step:27666 [D loss: 0.114648, acc.: 94.53%] [G loss: 3.948279]\n",
      "epoch:35 step:27667 [D loss: 0.028994, acc.: 100.00%] [G loss: 2.569247]\n",
      "epoch:35 step:27668 [D loss: 0.008891, acc.: 100.00%] [G loss: 1.269019]\n",
      "epoch:35 step:27669 [D loss: 0.035168, acc.: 99.22%] [G loss: 0.306634]\n",
      "epoch:35 step:27670 [D loss: 0.013729, acc.: 100.00%] [G loss: 0.038361]\n",
      "epoch:35 step:27671 [D loss: 0.019104, acc.: 100.00%] [G loss: 0.736984]\n",
      "epoch:35 step:27672 [D loss: 0.004473, acc.: 100.00%] [G loss: 0.197901]\n",
      "epoch:35 step:27673 [D loss: 0.041515, acc.: 99.22%] [G loss: 0.437594]\n",
      "epoch:35 step:27674 [D loss: 0.001401, acc.: 100.00%] [G loss: 0.304509]\n",
      "epoch:35 step:27675 [D loss: 0.005123, acc.: 100.00%] [G loss: 0.056041]\n",
      "epoch:35 step:27676 [D loss: 0.000884, acc.: 100.00%] [G loss: 0.019982]\n",
      "epoch:35 step:27677 [D loss: 0.002550, acc.: 100.00%] [G loss: 0.018932]\n",
      "epoch:35 step:27678 [D loss: 0.003826, acc.: 100.00%] [G loss: 0.007287]\n",
      "epoch:35 step:27679 [D loss: 0.001964, acc.: 100.00%] [G loss: 0.045149]\n",
      "epoch:35 step:27680 [D loss: 0.001353, acc.: 100.00%] [G loss: 0.012742]\n",
      "epoch:35 step:27681 [D loss: 0.006556, acc.: 100.00%] [G loss: 0.005920]\n",
      "epoch:35 step:27682 [D loss: 0.038172, acc.: 99.22%] [G loss: 0.036401]\n",
      "epoch:35 step:27683 [D loss: 0.006776, acc.: 100.00%] [G loss: 0.010314]\n",
      "epoch:35 step:27684 [D loss: 0.011543, acc.: 100.00%] [G loss: 0.079439]\n",
      "epoch:35 step:27685 [D loss: 0.051396, acc.: 99.22%] [G loss: 0.053540]\n",
      "epoch:35 step:27686 [D loss: 0.028079, acc.: 100.00%] [G loss: 0.114696]\n",
      "epoch:35 step:27687 [D loss: 0.076268, acc.: 97.66%] [G loss: 0.885529]\n",
      "epoch:35 step:27688 [D loss: 0.063579, acc.: 98.44%] [G loss: 1.928416]\n",
      "epoch:35 step:27689 [D loss: 0.065733, acc.: 97.66%] [G loss: 0.665119]\n",
      "epoch:35 step:27690 [D loss: 0.042237, acc.: 99.22%] [G loss: 0.011996]\n",
      "epoch:35 step:27691 [D loss: 0.047427, acc.: 98.44%] [G loss: 0.001703]\n",
      "epoch:35 step:27692 [D loss: 0.001284, acc.: 100.00%] [G loss: 1.706616]\n",
      "epoch:35 step:27693 [D loss: 0.005438, acc.: 100.00%] [G loss: 0.969004]\n",
      "epoch:35 step:27694 [D loss: 0.004225, acc.: 100.00%] [G loss: 0.476632]\n",
      "epoch:35 step:27695 [D loss: 0.003669, acc.: 100.00%] [G loss: 0.014304]\n",
      "epoch:35 step:27696 [D loss: 0.272541, acc.: 86.72%] [G loss: 3.566167]\n",
      "epoch:35 step:27697 [D loss: 0.062939, acc.: 97.66%] [G loss: 2.695074]\n",
      "epoch:35 step:27698 [D loss: 0.725465, acc.: 67.97%] [G loss: 0.076630]\n",
      "epoch:35 step:27699 [D loss: 0.198996, acc.: 91.41%] [G loss: 0.480824]\n",
      "epoch:35 step:27700 [D loss: 0.001017, acc.: 100.00%] [G loss: 1.006156]\n",
      "epoch:35 step:27701 [D loss: 0.010779, acc.: 100.00%] [G loss: 0.746428]\n",
      "epoch:35 step:27702 [D loss: 0.053853, acc.: 98.44%] [G loss: 0.205357]\n",
      "epoch:35 step:27703 [D loss: 0.038135, acc.: 100.00%] [G loss: 0.175299]\n",
      "epoch:35 step:27704 [D loss: 0.047115, acc.: 99.22%] [G loss: 0.325937]\n",
      "epoch:35 step:27705 [D loss: 0.041718, acc.: 99.22%] [G loss: 0.408971]\n",
      "epoch:35 step:27706 [D loss: 0.048059, acc.: 98.44%] [G loss: 0.524318]\n",
      "epoch:35 step:27707 [D loss: 0.056144, acc.: 98.44%] [G loss: 1.307448]\n",
      "epoch:35 step:27708 [D loss: 0.024333, acc.: 99.22%] [G loss: 0.484710]\n",
      "epoch:35 step:27709 [D loss: 0.053659, acc.: 98.44%] [G loss: 0.146410]\n",
      "epoch:35 step:27710 [D loss: 0.005078, acc.: 100.00%] [G loss: 0.070792]\n",
      "epoch:35 step:27711 [D loss: 0.009097, acc.: 100.00%] [G loss: 0.014857]\n",
      "epoch:35 step:27712 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.005724]\n",
      "epoch:35 step:27713 [D loss: 0.001258, acc.: 100.00%] [G loss: 0.004561]\n",
      "epoch:35 step:27714 [D loss: 0.001189, acc.: 100.00%] [G loss: 0.026255]\n",
      "epoch:35 step:27715 [D loss: 0.001594, acc.: 100.00%] [G loss: 5.185276]\n",
      "epoch:35 step:27716 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.002991]\n",
      "epoch:35 step:27717 [D loss: 0.011147, acc.: 100.00%] [G loss: 0.005662]\n",
      "epoch:35 step:27718 [D loss: 0.022144, acc.: 100.00%] [G loss: 0.019810]\n",
      "epoch:35 step:27719 [D loss: 0.005527, acc.: 100.00%] [G loss: 0.075486]\n",
      "epoch:35 step:27720 [D loss: 0.016040, acc.: 100.00%] [G loss: 0.300651]\n",
      "epoch:35 step:27721 [D loss: 0.012921, acc.: 100.00%] [G loss: 0.115770]\n",
      "epoch:35 step:27722 [D loss: 0.007364, acc.: 100.00%] [G loss: 0.135092]\n",
      "epoch:35 step:27723 [D loss: 0.473841, acc.: 77.34%] [G loss: 5.604142]\n",
      "epoch:35 step:27724 [D loss: 0.093750, acc.: 95.31%] [G loss: 5.566957]\n",
      "epoch:35 step:27725 [D loss: 0.132862, acc.: 95.31%] [G loss: 4.106672]\n",
      "epoch:35 step:27726 [D loss: 0.130739, acc.: 95.31%] [G loss: 0.004837]\n",
      "epoch:35 step:27727 [D loss: 0.005968, acc.: 100.00%] [G loss: 1.382324]\n",
      "epoch:35 step:27728 [D loss: 0.010760, acc.: 100.00%] [G loss: 1.005446]\n",
      "epoch:35 step:27729 [D loss: 0.007849, acc.: 100.00%] [G loss: 0.448866]\n",
      "epoch:35 step:27730 [D loss: 0.036559, acc.: 100.00%] [G loss: 0.678798]\n",
      "epoch:35 step:27731 [D loss: 0.006792, acc.: 100.00%] [G loss: 0.697652]\n",
      "epoch:35 step:27732 [D loss: 0.077607, acc.: 98.44%] [G loss: 2.172740]\n",
      "epoch:35 step:27733 [D loss: 0.016705, acc.: 100.00%] [G loss: 2.378811]\n",
      "epoch:35 step:27734 [D loss: 0.034047, acc.: 99.22%] [G loss: 1.319472]\n",
      "epoch:35 step:27735 [D loss: 0.081271, acc.: 99.22%] [G loss: 0.081521]\n",
      "epoch:35 step:27736 [D loss: 0.011574, acc.: 100.00%] [G loss: 5.883060]\n",
      "epoch:35 step:27737 [D loss: 0.187844, acc.: 90.62%] [G loss: 0.010364]\n",
      "epoch:35 step:27738 [D loss: 0.001287, acc.: 100.00%] [G loss: 2.112345]\n",
      "epoch:35 step:27739 [D loss: 0.001804, acc.: 100.00%] [G loss: 0.632677]\n",
      "epoch:35 step:27740 [D loss: 0.057631, acc.: 98.44%] [G loss: 0.008585]\n",
      "epoch:35 step:27741 [D loss: 0.004448, acc.: 100.00%] [G loss: 2.103196]\n",
      "epoch:35 step:27742 [D loss: 0.002756, acc.: 100.00%] [G loss: 0.951868]\n",
      "epoch:35 step:27743 [D loss: 0.000707, acc.: 100.00%] [G loss: 0.937526]\n",
      "epoch:35 step:27744 [D loss: 0.008866, acc.: 100.00%] [G loss: 0.925458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27745 [D loss: 0.007680, acc.: 100.00%] [G loss: 0.599888]\n",
      "epoch:35 step:27746 [D loss: 0.095292, acc.: 96.88%] [G loss: 0.003454]\n",
      "epoch:35 step:27747 [D loss: 0.002319, acc.: 100.00%] [G loss: 0.034218]\n",
      "epoch:35 step:27748 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.000873]\n",
      "epoch:35 step:27749 [D loss: 0.033132, acc.: 100.00%] [G loss: 0.017129]\n",
      "epoch:35 step:27750 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.035666]\n",
      "epoch:35 step:27751 [D loss: 0.003164, acc.: 100.00%] [G loss: 0.037359]\n",
      "epoch:35 step:27752 [D loss: 0.000828, acc.: 100.00%] [G loss: 0.021822]\n",
      "epoch:35 step:27753 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.058808]\n",
      "epoch:35 step:27754 [D loss: 0.000679, acc.: 100.00%] [G loss: 0.006916]\n",
      "epoch:35 step:27755 [D loss: 0.011786, acc.: 100.00%] [G loss: 0.025356]\n",
      "epoch:35 step:27756 [D loss: 0.001180, acc.: 100.00%] [G loss: 0.005321]\n",
      "epoch:35 step:27757 [D loss: 0.026148, acc.: 99.22%] [G loss: 0.004934]\n",
      "epoch:35 step:27758 [D loss: 0.010505, acc.: 100.00%] [G loss: 0.174065]\n",
      "epoch:35 step:27759 [D loss: 0.066907, acc.: 98.44%] [G loss: 0.002524]\n",
      "epoch:35 step:27760 [D loss: 0.002142, acc.: 100.00%] [G loss: 0.017268]\n",
      "epoch:35 step:27761 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.002256]\n",
      "epoch:35 step:27762 [D loss: 0.000538, acc.: 100.00%] [G loss: 0.002505]\n",
      "epoch:35 step:27763 [D loss: 0.006742, acc.: 100.00%] [G loss: 0.002964]\n",
      "epoch:35 step:27764 [D loss: 0.012886, acc.: 100.00%] [G loss: 0.001167]\n",
      "epoch:35 step:27765 [D loss: 0.011195, acc.: 100.00%] [G loss: 0.000813]\n",
      "epoch:35 step:27766 [D loss: 0.009445, acc.: 100.00%] [G loss: 0.002523]\n",
      "epoch:35 step:27767 [D loss: 0.001167, acc.: 100.00%] [G loss: 0.000822]\n",
      "epoch:35 step:27768 [D loss: 0.002274, acc.: 100.00%] [G loss: 0.002675]\n",
      "epoch:35 step:27769 [D loss: 0.007568, acc.: 100.00%] [G loss: 0.006736]\n",
      "epoch:35 step:27770 [D loss: 0.155114, acc.: 93.75%] [G loss: 0.020274]\n",
      "epoch:35 step:27771 [D loss: 0.044269, acc.: 98.44%] [G loss: 0.253893]\n",
      "epoch:35 step:27772 [D loss: 0.022027, acc.: 99.22%] [G loss: 0.058773]\n",
      "epoch:35 step:27773 [D loss: 0.038252, acc.: 99.22%] [G loss: 0.006441]\n",
      "epoch:35 step:27774 [D loss: 0.002964, acc.: 100.00%] [G loss: 2.855514]\n",
      "epoch:35 step:27775 [D loss: 0.026284, acc.: 99.22%] [G loss: 0.249151]\n",
      "epoch:35 step:27776 [D loss: 0.003129, acc.: 100.00%] [G loss: 0.000440]\n",
      "epoch:35 step:27777 [D loss: 0.006804, acc.: 100.00%] [G loss: 0.096485]\n",
      "epoch:35 step:27778 [D loss: 0.005632, acc.: 100.00%] [G loss: 0.000980]\n",
      "epoch:35 step:27779 [D loss: 0.006667, acc.: 100.00%] [G loss: 1.656014]\n",
      "epoch:35 step:27780 [D loss: 0.008073, acc.: 100.00%] [G loss: 0.000853]\n",
      "epoch:35 step:27781 [D loss: 0.004657, acc.: 100.00%] [G loss: 0.000476]\n",
      "epoch:35 step:27782 [D loss: 0.003680, acc.: 100.00%] [G loss: 0.056361]\n",
      "epoch:35 step:27783 [D loss: 0.007681, acc.: 100.00%] [G loss: 0.000780]\n",
      "epoch:35 step:27784 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:35 step:27785 [D loss: 0.001524, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:35 step:27786 [D loss: 0.000858, acc.: 100.00%] [G loss: 0.073237]\n",
      "epoch:35 step:27787 [D loss: 0.003148, acc.: 100.00%] [G loss: 0.001127]\n",
      "epoch:35 step:27788 [D loss: 0.004022, acc.: 100.00%] [G loss: 0.020747]\n",
      "epoch:35 step:27789 [D loss: 0.002489, acc.: 100.00%] [G loss: 0.001256]\n",
      "epoch:35 step:27790 [D loss: 0.001574, acc.: 100.00%] [G loss: 0.001950]\n",
      "epoch:35 step:27791 [D loss: 0.000957, acc.: 100.00%] [G loss: 0.001274]\n",
      "epoch:35 step:27792 [D loss: 0.002994, acc.: 100.00%] [G loss: 0.116981]\n",
      "epoch:35 step:27793 [D loss: 0.001212, acc.: 100.00%] [G loss: 0.104451]\n",
      "epoch:35 step:27794 [D loss: 0.002950, acc.: 100.00%] [G loss: 0.001670]\n",
      "epoch:35 step:27795 [D loss: 0.022444, acc.: 99.22%] [G loss: 0.012935]\n",
      "epoch:35 step:27796 [D loss: 0.008449, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:35 step:27797 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.000655]\n",
      "epoch:35 step:27798 [D loss: 0.001118, acc.: 100.00%] [G loss: 0.176518]\n",
      "epoch:35 step:27799 [D loss: 0.002092, acc.: 100.00%] [G loss: 0.113840]\n",
      "epoch:35 step:27800 [D loss: 0.006481, acc.: 100.00%] [G loss: 0.002797]\n",
      "epoch:35 step:27801 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.003669]\n",
      "epoch:35 step:27802 [D loss: 0.000799, acc.: 100.00%] [G loss: 0.021263]\n",
      "epoch:35 step:27803 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.003416]\n",
      "epoch:35 step:27804 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.001731]\n",
      "epoch:35 step:27805 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.002521]\n",
      "epoch:35 step:27806 [D loss: 0.000649, acc.: 100.00%] [G loss: 0.001001]\n",
      "epoch:35 step:27807 [D loss: 0.001632, acc.: 100.00%] [G loss: 0.054951]\n",
      "epoch:35 step:27808 [D loss: 0.001019, acc.: 100.00%] [G loss: 0.043285]\n",
      "epoch:35 step:27809 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.141517]\n",
      "epoch:35 step:27810 [D loss: 0.004828, acc.: 100.00%] [G loss: 0.006450]\n",
      "epoch:35 step:27811 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.006298]\n",
      "epoch:35 step:27812 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.037337]\n",
      "epoch:35 step:27813 [D loss: 0.002857, acc.: 100.00%] [G loss: 0.002616]\n",
      "epoch:35 step:27814 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.002964]\n",
      "epoch:35 step:27815 [D loss: 0.000633, acc.: 100.00%] [G loss: 0.000320]\n",
      "epoch:35 step:27816 [D loss: 0.000703, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:35 step:27817 [D loss: 0.001145, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:35 step:27818 [D loss: 0.000623, acc.: 100.00%] [G loss: 0.000296]\n",
      "epoch:35 step:27819 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000673]\n",
      "epoch:35 step:27820 [D loss: 0.000710, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:35 step:27821 [D loss: 0.000888, acc.: 100.00%] [G loss: 0.004361]\n",
      "epoch:35 step:27822 [D loss: 0.001106, acc.: 100.00%] [G loss: 0.004150]\n",
      "epoch:35 step:27823 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.001223]\n",
      "epoch:35 step:27824 [D loss: 0.002589, acc.: 100.00%] [G loss: 0.051886]\n",
      "epoch:35 step:27825 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:35 step:27826 [D loss: 0.002885, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:35 step:27827 [D loss: 0.001704, acc.: 100.00%] [G loss: 0.000422]\n",
      "epoch:35 step:27828 [D loss: 0.015701, acc.: 100.00%] [G loss: 0.009792]\n",
      "epoch:35 step:27829 [D loss: 0.000458, acc.: 100.00%] [G loss: 0.004212]\n",
      "epoch:35 step:27830 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.338880]\n",
      "epoch:35 step:27831 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.006925]\n",
      "epoch:35 step:27832 [D loss: 0.001482, acc.: 100.00%] [G loss: 0.058079]\n",
      "epoch:35 step:27833 [D loss: 0.001832, acc.: 100.00%] [G loss: 0.027428]\n",
      "epoch:35 step:27834 [D loss: 0.001572, acc.: 100.00%] [G loss: 0.005539]\n",
      "epoch:35 step:27835 [D loss: 0.000665, acc.: 100.00%] [G loss: 0.041376]\n",
      "epoch:35 step:27836 [D loss: 0.015354, acc.: 99.22%] [G loss: 0.001617]\n",
      "epoch:35 step:27837 [D loss: 0.000427, acc.: 100.00%] [G loss: 0.012191]\n",
      "epoch:35 step:27838 [D loss: 0.001077, acc.: 100.00%] [G loss: 0.000449]\n",
      "epoch:35 step:27839 [D loss: 0.000700, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:35 step:27840 [D loss: 0.004542, acc.: 100.00%] [G loss: 0.000823]\n",
      "epoch:35 step:27841 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.001493]\n",
      "epoch:35 step:27842 [D loss: 0.000953, acc.: 100.00%] [G loss: 0.001226]\n",
      "epoch:35 step:27843 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:35 step:27844 [D loss: 0.005693, acc.: 100.00%] [G loss: 0.000204]\n",
      "epoch:35 step:27845 [D loss: 0.001201, acc.: 100.00%] [G loss: 0.000865]\n",
      "epoch:35 step:27846 [D loss: 0.003240, acc.: 100.00%] [G loss: 0.002779]\n",
      "epoch:35 step:27847 [D loss: 0.060243, acc.: 100.00%] [G loss: 0.115745]\n",
      "epoch:35 step:27848 [D loss: 0.053138, acc.: 97.66%] [G loss: 0.291248]\n",
      "epoch:35 step:27849 [D loss: 0.002999, acc.: 100.00%] [G loss: 0.088701]\n",
      "epoch:35 step:27850 [D loss: 0.005022, acc.: 100.00%] [G loss: 0.013285]\n",
      "epoch:35 step:27851 [D loss: 0.053741, acc.: 98.44%] [G loss: 0.585749]\n",
      "epoch:35 step:27852 [D loss: 0.016937, acc.: 100.00%] [G loss: 0.448661]\n",
      "epoch:35 step:27853 [D loss: 0.000681, acc.: 100.00%] [G loss: 2.802821]\n",
      "epoch:35 step:27854 [D loss: 0.001612, acc.: 100.00%] [G loss: 1.289348]\n",
      "epoch:35 step:27855 [D loss: 0.026144, acc.: 99.22%] [G loss: 0.191783]\n",
      "epoch:35 step:27856 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.109881]\n",
      "epoch:35 step:27857 [D loss: 0.004538, acc.: 100.00%] [G loss: 0.022432]\n",
      "epoch:35 step:27858 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.039348]\n",
      "epoch:35 step:27859 [D loss: 0.009387, acc.: 100.00%] [G loss: 0.046218]\n",
      "epoch:35 step:27860 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.020705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27861 [D loss: 0.106975, acc.: 93.75%] [G loss: 0.000358]\n",
      "epoch:35 step:27862 [D loss: 0.001454, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:35 step:27863 [D loss: 0.002331, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:35 step:27864 [D loss: 0.145917, acc.: 96.09%] [G loss: 0.021600]\n",
      "epoch:35 step:27865 [D loss: 0.000148, acc.: 100.00%] [G loss: 1.260952]\n",
      "epoch:35 step:27866 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.067094]\n",
      "epoch:35 step:27867 [D loss: 0.015111, acc.: 100.00%] [G loss: 0.011030]\n",
      "epoch:35 step:27868 [D loss: 0.007103, acc.: 100.00%] [G loss: 0.027904]\n",
      "epoch:35 step:27869 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.475433]\n",
      "epoch:35 step:27870 [D loss: 0.011256, acc.: 100.00%] [G loss: 0.311666]\n",
      "epoch:35 step:27871 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.181952]\n",
      "epoch:35 step:27872 [D loss: 0.006920, acc.: 100.00%] [G loss: 0.045311]\n",
      "epoch:35 step:27873 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.008159]\n",
      "epoch:35 step:27874 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.025482]\n",
      "epoch:35 step:27875 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.023728]\n",
      "epoch:35 step:27876 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.003041]\n",
      "epoch:35 step:27877 [D loss: 0.003164, acc.: 100.00%] [G loss: 0.002614]\n",
      "epoch:35 step:27878 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.005142]\n",
      "epoch:35 step:27879 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:35 step:27880 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000829]\n",
      "epoch:35 step:27881 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.003113]\n",
      "epoch:35 step:27882 [D loss: 0.001030, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:35 step:27883 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.018876]\n",
      "epoch:35 step:27884 [D loss: 0.006579, acc.: 100.00%] [G loss: 0.002674]\n",
      "epoch:35 step:27885 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.005416]\n",
      "epoch:35 step:27886 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.018001]\n",
      "epoch:35 step:27887 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.007162]\n",
      "epoch:35 step:27888 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.004716]\n",
      "epoch:35 step:27889 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.257018]\n",
      "epoch:35 step:27890 [D loss: 0.001649, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:35 step:27891 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:35 step:27892 [D loss: 0.002692, acc.: 100.00%] [G loss: 0.002186]\n",
      "epoch:35 step:27893 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.000861]\n",
      "epoch:35 step:27894 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.003407]\n",
      "epoch:35 step:27895 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.007627]\n",
      "epoch:35 step:27896 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.004984]\n",
      "epoch:35 step:27897 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.005150]\n",
      "epoch:35 step:27898 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.004443]\n",
      "epoch:35 step:27899 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.003301]\n",
      "epoch:35 step:27900 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000987]\n",
      "epoch:35 step:27901 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.001474]\n",
      "epoch:35 step:27902 [D loss: 0.001064, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:35 step:27903 [D loss: 0.000510, acc.: 100.00%] [G loss: 0.004211]\n",
      "epoch:35 step:27904 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.003524]\n",
      "epoch:35 step:27905 [D loss: 0.000050, acc.: 100.00%] [G loss: 1.073504]\n",
      "epoch:35 step:27906 [D loss: 0.001896, acc.: 100.00%] [G loss: 0.000697]\n",
      "epoch:35 step:27907 [D loss: 0.016557, acc.: 100.00%] [G loss: 0.118779]\n",
      "epoch:35 step:27908 [D loss: 0.041893, acc.: 100.00%] [G loss: 0.333193]\n",
      "epoch:35 step:27909 [D loss: 0.001822, acc.: 100.00%] [G loss: 0.618010]\n",
      "epoch:35 step:27910 [D loss: 0.008084, acc.: 100.00%] [G loss: 0.074228]\n",
      "epoch:35 step:27911 [D loss: 0.022567, acc.: 99.22%] [G loss: 0.065537]\n",
      "epoch:35 step:27912 [D loss: 0.018706, acc.: 99.22%] [G loss: 0.002996]\n",
      "epoch:35 step:27913 [D loss: 0.000690, acc.: 100.00%] [G loss: 0.001393]\n",
      "epoch:35 step:27914 [D loss: 0.003248, acc.: 100.00%] [G loss: 0.001450]\n",
      "epoch:35 step:27915 [D loss: 0.051069, acc.: 99.22%] [G loss: 0.130808]\n",
      "epoch:35 step:27916 [D loss: 0.010535, acc.: 100.00%] [G loss: 0.184871]\n",
      "epoch:35 step:27917 [D loss: 0.032530, acc.: 98.44%] [G loss: 0.068737]\n",
      "epoch:35 step:27918 [D loss: 0.008679, acc.: 99.22%] [G loss: 0.019598]\n",
      "epoch:35 step:27919 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.006373]\n",
      "epoch:35 step:27920 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.061713]\n",
      "epoch:35 step:27921 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.017936]\n",
      "epoch:35 step:27922 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.002748]\n",
      "epoch:35 step:27923 [D loss: 0.000027, acc.: 100.00%] [G loss: 2.516761]\n",
      "epoch:35 step:27924 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.003609]\n",
      "epoch:35 step:27925 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.001987]\n",
      "epoch:35 step:27926 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.001344]\n",
      "epoch:35 step:27927 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.023173]\n",
      "epoch:35 step:27928 [D loss: 0.000327, acc.: 100.00%] [G loss: 0.003459]\n",
      "epoch:35 step:27929 [D loss: 0.000688, acc.: 100.00%] [G loss: 0.011389]\n",
      "epoch:35 step:27930 [D loss: 0.003846, acc.: 100.00%] [G loss: 0.042040]\n",
      "epoch:35 step:27931 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.008436]\n",
      "epoch:35 step:27932 [D loss: 0.001212, acc.: 100.00%] [G loss: 0.002771]\n",
      "epoch:35 step:27933 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.002557]\n",
      "epoch:35 step:27934 [D loss: 0.006563, acc.: 100.00%] [G loss: 0.016958]\n",
      "epoch:35 step:27935 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.002090]\n",
      "epoch:35 step:27936 [D loss: 0.000846, acc.: 100.00%] [G loss: 0.052278]\n",
      "epoch:35 step:27937 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.006240]\n",
      "epoch:35 step:27938 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.029894]\n",
      "epoch:35 step:27939 [D loss: 0.001385, acc.: 100.00%] [G loss: 0.129945]\n",
      "epoch:35 step:27940 [D loss: 0.001248, acc.: 100.00%] [G loss: 0.020014]\n",
      "epoch:35 step:27941 [D loss: 0.001662, acc.: 100.00%] [G loss: 0.005125]\n",
      "epoch:35 step:27942 [D loss: 0.001549, acc.: 100.00%] [G loss: 0.005183]\n",
      "epoch:35 step:27943 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.004989]\n",
      "epoch:35 step:27944 [D loss: 0.224198, acc.: 85.94%] [G loss: 5.268121]\n",
      "epoch:35 step:27945 [D loss: 0.287237, acc.: 85.94%] [G loss: 2.596874]\n",
      "epoch:35 step:27946 [D loss: 0.638616, acc.: 83.59%] [G loss: 9.280202]\n",
      "epoch:35 step:27947 [D loss: 0.351247, acc.: 83.59%] [G loss: 8.670609]\n",
      "epoch:35 step:27948 [D loss: 1.111946, acc.: 58.59%] [G loss: 7.658352]\n",
      "epoch:35 step:27949 [D loss: 0.026824, acc.: 99.22%] [G loss: 7.810292]\n",
      "epoch:35 step:27950 [D loss: 0.055691, acc.: 97.66%] [G loss: 8.709357]\n",
      "epoch:35 step:27951 [D loss: 0.066727, acc.: 96.88%] [G loss: 6.069030]\n",
      "epoch:35 step:27952 [D loss: 0.075355, acc.: 95.31%] [G loss: 6.563818]\n",
      "epoch:35 step:27953 [D loss: 0.017928, acc.: 99.22%] [G loss: 6.601127]\n",
      "epoch:35 step:27954 [D loss: 0.072558, acc.: 99.22%] [G loss: 6.376670]\n",
      "epoch:35 step:27955 [D loss: 0.077744, acc.: 96.88%] [G loss: 3.684206]\n",
      "epoch:35 step:27956 [D loss: 0.026943, acc.: 99.22%] [G loss: 5.517201]\n",
      "epoch:35 step:27957 [D loss: 0.019538, acc.: 99.22%] [G loss: 4.501768]\n",
      "epoch:35 step:27958 [D loss: 0.273244, acc.: 92.19%] [G loss: 7.742171]\n",
      "epoch:35 step:27959 [D loss: 0.040574, acc.: 99.22%] [G loss: 8.480183]\n",
      "epoch:35 step:27960 [D loss: 0.496976, acc.: 82.81%] [G loss: 3.995368]\n",
      "epoch:35 step:27961 [D loss: 0.130501, acc.: 95.31%] [G loss: 2.429142]\n",
      "epoch:35 step:27962 [D loss: 0.047319, acc.: 99.22%] [G loss: 1.028936]\n",
      "epoch:35 step:27963 [D loss: 0.012311, acc.: 100.00%] [G loss: 8.924307]\n",
      "epoch:35 step:27964 [D loss: 0.061921, acc.: 96.88%] [G loss: 7.112796]\n",
      "epoch:35 step:27965 [D loss: 0.014130, acc.: 100.00%] [G loss: 4.018100]\n",
      "epoch:35 step:27966 [D loss: 0.177368, acc.: 92.97%] [G loss: 7.358594]\n",
      "epoch:35 step:27967 [D loss: 0.012094, acc.: 100.00%] [G loss: 6.696575]\n",
      "epoch:35 step:27968 [D loss: 0.010696, acc.: 100.00%] [G loss: 2.658344]\n",
      "epoch:35 step:27969 [D loss: 0.155925, acc.: 96.09%] [G loss: 5.477934]\n",
      "epoch:35 step:27970 [D loss: 0.349892, acc.: 89.06%] [G loss: 9.141066]\n",
      "epoch:35 step:27971 [D loss: 0.402189, acc.: 85.94%] [G loss: 6.105520]\n",
      "epoch:35 step:27972 [D loss: 0.084161, acc.: 96.88%] [G loss: 6.470462]\n",
      "epoch:35 step:27973 [D loss: 0.004800, acc.: 100.00%] [G loss: 6.613721]\n",
      "epoch:35 step:27974 [D loss: 0.020015, acc.: 100.00%] [G loss: 4.310015]\n",
      "epoch:35 step:27975 [D loss: 0.070047, acc.: 99.22%] [G loss: 1.544578]\n",
      "epoch:35 step:27976 [D loss: 0.015921, acc.: 100.00%] [G loss: 6.603247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:27977 [D loss: 0.120747, acc.: 96.09%] [G loss: 0.003117]\n",
      "epoch:35 step:27978 [D loss: 0.004713, acc.: 100.00%] [G loss: 4.059977]\n",
      "epoch:35 step:27979 [D loss: 0.003372, acc.: 100.00%] [G loss: 1.865264]\n",
      "epoch:35 step:27980 [D loss: 0.081289, acc.: 96.88%] [G loss: 1.790898]\n",
      "epoch:35 step:27981 [D loss: 0.042153, acc.: 100.00%] [G loss: 1.126077]\n",
      "epoch:35 step:27982 [D loss: 0.006173, acc.: 100.00%] [G loss: 0.049398]\n",
      "epoch:35 step:27983 [D loss: 0.001465, acc.: 100.00%] [G loss: 1.136397]\n",
      "epoch:35 step:27984 [D loss: 0.181586, acc.: 96.09%] [G loss: 0.000427]\n",
      "epoch:35 step:27985 [D loss: 0.013368, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:35 step:27986 [D loss: 0.078307, acc.: 99.22%] [G loss: 0.013090]\n",
      "epoch:35 step:27987 [D loss: 0.000094, acc.: 100.00%] [G loss: 5.310716]\n",
      "epoch:35 step:27988 [D loss: 0.001538, acc.: 100.00%] [G loss: 0.549707]\n",
      "epoch:35 step:27989 [D loss: 0.053570, acc.: 96.88%] [G loss: 0.020880]\n",
      "epoch:35 step:27990 [D loss: 0.010315, acc.: 100.00%] [G loss: 0.017500]\n",
      "epoch:35 step:27991 [D loss: 0.013783, acc.: 100.00%] [G loss: 0.003015]\n",
      "epoch:35 step:27992 [D loss: 0.005435, acc.: 100.00%] [G loss: 0.017858]\n",
      "epoch:35 step:27993 [D loss: 0.001846, acc.: 100.00%] [G loss: 0.038321]\n",
      "epoch:35 step:27994 [D loss: 0.281441, acc.: 83.59%] [G loss: 10.062693]\n",
      "epoch:35 step:27995 [D loss: 1.136892, acc.: 61.72%] [G loss: 2.264153]\n",
      "epoch:35 step:27996 [D loss: 0.187372, acc.: 91.41%] [G loss: 4.421855]\n",
      "epoch:35 step:27997 [D loss: 0.009373, acc.: 99.22%] [G loss: 4.759300]\n",
      "epoch:35 step:27998 [D loss: 0.030925, acc.: 98.44%] [G loss: 1.645207]\n",
      "epoch:35 step:27999 [D loss: 0.023683, acc.: 100.00%] [G loss: 0.177113]\n",
      "epoch:35 step:28000 [D loss: 0.117719, acc.: 96.09%] [G loss: 0.088503]\n",
      "epoch:35 step:28001 [D loss: 0.001598, acc.: 100.00%] [G loss: 0.240934]\n",
      "epoch:35 step:28002 [D loss: 0.001895, acc.: 100.00%] [G loss: 0.013565]\n",
      "epoch:35 step:28003 [D loss: 0.003004, acc.: 100.00%] [G loss: 0.177592]\n",
      "epoch:35 step:28004 [D loss: 0.005839, acc.: 100.00%] [G loss: 0.009937]\n",
      "epoch:35 step:28005 [D loss: 0.005376, acc.: 100.00%] [G loss: 0.017595]\n",
      "epoch:35 step:28006 [D loss: 0.003161, acc.: 100.00%] [G loss: 0.029865]\n",
      "epoch:35 step:28007 [D loss: 0.003241, acc.: 100.00%] [G loss: 0.023546]\n",
      "epoch:35 step:28008 [D loss: 0.007891, acc.: 100.00%] [G loss: 0.002099]\n",
      "epoch:35 step:28009 [D loss: 0.038600, acc.: 99.22%] [G loss: 0.132038]\n",
      "epoch:35 step:28010 [D loss: 0.001256, acc.: 100.00%] [G loss: 0.062235]\n",
      "epoch:35 step:28011 [D loss: 0.035301, acc.: 99.22%] [G loss: 1.312044]\n",
      "epoch:35 step:28012 [D loss: 0.004142, acc.: 100.00%] [G loss: 0.052392]\n",
      "epoch:35 step:28013 [D loss: 0.036638, acc.: 99.22%] [G loss: 2.382626]\n",
      "epoch:35 step:28014 [D loss: 0.047469, acc.: 98.44%] [G loss: 1.618455]\n",
      "epoch:35 step:28015 [D loss: 0.056880, acc.: 99.22%] [G loss: 0.728447]\n",
      "epoch:35 step:28016 [D loss: 0.043268, acc.: 98.44%] [G loss: 1.110756]\n",
      "epoch:35 step:28017 [D loss: 0.092061, acc.: 97.66%] [G loss: 0.799984]\n",
      "epoch:35 step:28018 [D loss: 0.034254, acc.: 98.44%] [G loss: 0.303814]\n",
      "epoch:35 step:28019 [D loss: 0.003971, acc.: 100.00%] [G loss: 0.326174]\n",
      "epoch:35 step:28020 [D loss: 0.055361, acc.: 98.44%] [G loss: 0.158547]\n",
      "epoch:35 step:28021 [D loss: 0.038876, acc.: 98.44%] [G loss: 0.108665]\n",
      "epoch:35 step:28022 [D loss: 0.015848, acc.: 100.00%] [G loss: 0.094999]\n",
      "epoch:35 step:28023 [D loss: 0.003287, acc.: 100.00%] [G loss: 0.092702]\n",
      "epoch:35 step:28024 [D loss: 0.003489, acc.: 100.00%] [G loss: 0.018435]\n",
      "epoch:35 step:28025 [D loss: 0.002523, acc.: 100.00%] [G loss: 0.009473]\n",
      "epoch:35 step:28026 [D loss: 0.002141, acc.: 100.00%] [G loss: 0.006361]\n",
      "epoch:35 step:28027 [D loss: 0.101904, acc.: 95.31%] [G loss: 0.004179]\n",
      "epoch:35 step:28028 [D loss: 0.000909, acc.: 100.00%] [G loss: 0.052865]\n",
      "epoch:35 step:28029 [D loss: 0.001475, acc.: 100.00%] [G loss: 1.383703]\n",
      "epoch:35 step:28030 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.579060]\n",
      "epoch:35 step:28031 [D loss: 0.008275, acc.: 100.00%] [G loss: 0.101904]\n",
      "epoch:35 step:28032 [D loss: 0.030330, acc.: 99.22%] [G loss: 0.085562]\n",
      "epoch:35 step:28033 [D loss: 0.035661, acc.: 99.22%] [G loss: 0.143009]\n",
      "epoch:35 step:28034 [D loss: 0.062139, acc.: 97.66%] [G loss: 0.061655]\n",
      "epoch:35 step:28035 [D loss: 0.006015, acc.: 100.00%] [G loss: 0.005192]\n",
      "epoch:35 step:28036 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.001277]\n",
      "epoch:35 step:28037 [D loss: 0.000932, acc.: 100.00%] [G loss: 1.942041]\n",
      "epoch:35 step:28038 [D loss: 0.012456, acc.: 100.00%] [G loss: 0.000510]\n",
      "epoch:35 step:28039 [D loss: 0.140077, acc.: 92.97%] [G loss: 5.387363]\n",
      "epoch:35 step:28040 [D loss: 0.011078, acc.: 100.00%] [G loss: 5.223825]\n",
      "epoch:35 step:28041 [D loss: 0.076840, acc.: 96.88%] [G loss: 0.281172]\n",
      "epoch:35 step:28042 [D loss: 0.088766, acc.: 95.31%] [G loss: 3.366675]\n",
      "epoch:35 step:28043 [D loss: 0.095230, acc.: 96.09%] [G loss: 0.099624]\n",
      "epoch:35 step:28044 [D loss: 0.060222, acc.: 98.44%] [G loss: 3.823754]\n",
      "epoch:35 step:28045 [D loss: 0.018651, acc.: 99.22%] [G loss: 3.675457]\n",
      "epoch:35 step:28046 [D loss: 0.050201, acc.: 98.44%] [G loss: 0.497386]\n",
      "epoch:35 step:28047 [D loss: 0.135587, acc.: 94.53%] [G loss: 3.170462]\n",
      "epoch:35 step:28048 [D loss: 1.101569, acc.: 57.03%] [G loss: 0.459052]\n",
      "epoch:35 step:28049 [D loss: 0.010164, acc.: 99.22%] [G loss: 0.605140]\n",
      "epoch:35 step:28050 [D loss: 0.016208, acc.: 99.22%] [G loss: 0.111316]\n",
      "epoch:35 step:28051 [D loss: 0.004246, acc.: 100.00%] [G loss: 6.867659]\n",
      "epoch:35 step:28052 [D loss: 0.001246, acc.: 100.00%] [G loss: 2.327566]\n",
      "epoch:35 step:28053 [D loss: 0.010204, acc.: 100.00%] [G loss: 0.000600]\n",
      "epoch:35 step:28054 [D loss: 0.014112, acc.: 100.00%] [G loss: 0.000779]\n",
      "epoch:35 step:28055 [D loss: 0.033134, acc.: 99.22%] [G loss: 1.779925]\n",
      "epoch:35 step:28056 [D loss: 0.015484, acc.: 99.22%] [G loss: 0.824935]\n",
      "epoch:35 step:28057 [D loss: 0.004788, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:35 step:28058 [D loss: 0.012043, acc.: 100.00%] [G loss: 0.949763]\n",
      "epoch:35 step:28059 [D loss: 0.065658, acc.: 97.66%] [G loss: 0.879588]\n",
      "epoch:35 step:28060 [D loss: 0.024217, acc.: 100.00%] [G loss: 2.405118]\n",
      "epoch:35 step:28061 [D loss: 0.008228, acc.: 100.00%] [G loss: 0.556102]\n",
      "epoch:35 step:28062 [D loss: 0.254448, acc.: 85.16%] [G loss: 6.572256]\n",
      "epoch:35 step:28063 [D loss: 0.210728, acc.: 91.41%] [G loss: 0.106791]\n",
      "epoch:35 step:28064 [D loss: 0.056912, acc.: 97.66%] [G loss: 0.889085]\n",
      "epoch:35 step:28065 [D loss: 0.034395, acc.: 99.22%] [G loss: 0.253570]\n",
      "epoch:35 step:28066 [D loss: 0.002254, acc.: 100.00%] [G loss: 0.476791]\n",
      "epoch:35 step:28067 [D loss: 0.019409, acc.: 98.44%] [G loss: 0.122037]\n",
      "epoch:35 step:28068 [D loss: 0.012678, acc.: 100.00%] [G loss: 0.027390]\n",
      "epoch:35 step:28069 [D loss: 0.034408, acc.: 98.44%] [G loss: 0.003564]\n",
      "epoch:35 step:28070 [D loss: 0.020547, acc.: 99.22%] [G loss: 4.364570]\n",
      "epoch:35 step:28071 [D loss: 0.022513, acc.: 99.22%] [G loss: 0.000268]\n",
      "epoch:35 step:28072 [D loss: 0.001175, acc.: 100.00%] [G loss: 0.273026]\n",
      "epoch:35 step:28073 [D loss: 0.001450, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:35 step:28074 [D loss: 0.002212, acc.: 100.00%] [G loss: 0.104909]\n",
      "epoch:35 step:28075 [D loss: 0.001275, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:35 step:28076 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.410566]\n",
      "epoch:35 step:28077 [D loss: 0.001306, acc.: 100.00%] [G loss: 0.000905]\n",
      "epoch:35 step:28078 [D loss: 0.004787, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:35 step:28079 [D loss: 0.000526, acc.: 100.00%] [G loss: 0.003617]\n",
      "epoch:35 step:28080 [D loss: 0.003325, acc.: 100.00%] [G loss: 0.319879]\n",
      "epoch:35 step:28081 [D loss: 0.001150, acc.: 100.00%] [G loss: 0.102441]\n",
      "epoch:35 step:28082 [D loss: 0.017432, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:35 step:28083 [D loss: 0.005041, acc.: 100.00%] [G loss: 0.213377]\n",
      "epoch:35 step:28084 [D loss: 0.006221, acc.: 100.00%] [G loss: 0.001204]\n",
      "epoch:35 step:28085 [D loss: 0.001772, acc.: 100.00%] [G loss: 0.001847]\n",
      "epoch:35 step:28086 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.001199]\n",
      "epoch:35 step:28087 [D loss: 0.011084, acc.: 99.22%] [G loss: 0.407554]\n",
      "epoch:35 step:28088 [D loss: 0.003112, acc.: 100.00%] [G loss: 1.975113]\n",
      "epoch:35 step:28089 [D loss: 0.004345, acc.: 100.00%] [G loss: 0.029988]\n",
      "epoch:35 step:28090 [D loss: 0.001045, acc.: 100.00%] [G loss: 0.004849]\n",
      "epoch:35 step:28091 [D loss: 0.007008, acc.: 100.00%] [G loss: 0.000581]\n",
      "epoch:35 step:28092 [D loss: 0.001432, acc.: 100.00%] [G loss: 0.000316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:35 step:28093 [D loss: 0.000800, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:35 step:28094 [D loss: 0.004011, acc.: 100.00%] [G loss: 0.001682]\n",
      "epoch:35 step:28095 [D loss: 0.049466, acc.: 99.22%] [G loss: 0.507627]\n",
      "epoch:35 step:28096 [D loss: 0.001262, acc.: 100.00%] [G loss: 0.312040]\n",
      "epoch:35 step:28097 [D loss: 0.500148, acc.: 83.59%] [G loss: 4.446136]\n",
      "epoch:35 step:28098 [D loss: 0.018462, acc.: 99.22%] [G loss: 2.217839]\n",
      "epoch:35 step:28099 [D loss: 0.061666, acc.: 96.88%] [G loss: 0.217188]\n",
      "epoch:35 step:28100 [D loss: 0.005120, acc.: 100.00%] [G loss: 0.003685]\n",
      "epoch:35 step:28101 [D loss: 0.002403, acc.: 100.00%] [G loss: 0.145275]\n",
      "epoch:35 step:28102 [D loss: 0.000666, acc.: 100.00%] [G loss: 0.005124]\n",
      "epoch:35 step:28103 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:35 step:28104 [D loss: 0.003220, acc.: 100.00%] [G loss: 0.000452]\n",
      "epoch:35 step:28105 [D loss: 0.001833, acc.: 100.00%] [G loss: 0.001191]\n",
      "epoch:35 step:28106 [D loss: 0.001252, acc.: 100.00%] [G loss: 0.007158]\n",
      "epoch:35 step:28107 [D loss: 0.001597, acc.: 100.00%] [G loss: 0.000728]\n",
      "epoch:35 step:28108 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.005725]\n",
      "epoch:35 step:28109 [D loss: 0.001821, acc.: 100.00%] [G loss: 0.474274]\n",
      "epoch:35 step:28110 [D loss: 0.005824, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:35 step:28111 [D loss: 0.000492, acc.: 100.00%] [G loss: 0.009356]\n",
      "epoch:35 step:28112 [D loss: 0.002118, acc.: 100.00%] [G loss: 0.001397]\n",
      "epoch:35 step:28113 [D loss: 0.024307, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:35 step:28114 [D loss: 0.003098, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:35 step:28115 [D loss: 0.002159, acc.: 100.00%] [G loss: 0.007266]\n",
      "epoch:35 step:28116 [D loss: 0.003265, acc.: 100.00%] [G loss: 0.152687]\n",
      "epoch:36 step:28117 [D loss: 0.003961, acc.: 100.00%] [G loss: 0.001752]\n",
      "epoch:36 step:28118 [D loss: 0.004538, acc.: 100.00%] [G loss: 0.000732]\n",
      "epoch:36 step:28119 [D loss: 0.003638, acc.: 100.00%] [G loss: 0.001570]\n",
      "epoch:36 step:28120 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:36 step:28121 [D loss: 0.000489, acc.: 100.00%] [G loss: 0.021992]\n",
      "epoch:36 step:28122 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:36 step:28123 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:36 step:28124 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:36 step:28125 [D loss: 0.001863, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:36 step:28126 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:36 step:28127 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.007660]\n",
      "epoch:36 step:28128 [D loss: 0.001600, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:36 step:28129 [D loss: 0.000529, acc.: 100.00%] [G loss: 0.016873]\n",
      "epoch:36 step:28130 [D loss: 0.000397, acc.: 100.00%] [G loss: 0.007067]\n",
      "epoch:36 step:28131 [D loss: 0.004572, acc.: 100.00%] [G loss: 0.007341]\n",
      "epoch:36 step:28132 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.000890]\n",
      "epoch:36 step:28133 [D loss: 0.003728, acc.: 100.00%] [G loss: 0.067116]\n",
      "epoch:36 step:28134 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:36 step:28135 [D loss: 0.001299, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:36 step:28136 [D loss: 0.139701, acc.: 94.53%] [G loss: 4.775797]\n",
      "epoch:36 step:28137 [D loss: 0.437998, acc.: 82.03%] [G loss: 0.046707]\n",
      "epoch:36 step:28138 [D loss: 0.121019, acc.: 92.97%] [G loss: 0.324483]\n",
      "epoch:36 step:28139 [D loss: 0.000837, acc.: 100.00%] [G loss: 2.141300]\n",
      "epoch:36 step:28140 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.315078]\n",
      "epoch:36 step:28141 [D loss: 0.000931, acc.: 100.00%] [G loss: 6.542178]\n",
      "epoch:36 step:28142 [D loss: 0.002098, acc.: 100.00%] [G loss: 1.480186]\n",
      "epoch:36 step:28143 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.142909]\n",
      "epoch:36 step:28144 [D loss: 0.047566, acc.: 99.22%] [G loss: 0.005400]\n",
      "epoch:36 step:28145 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.021322]\n",
      "epoch:36 step:28146 [D loss: 0.021722, acc.: 100.00%] [G loss: 0.008494]\n",
      "epoch:36 step:28147 [D loss: 0.000376, acc.: 100.00%] [G loss: 1.643839]\n",
      "epoch:36 step:28148 [D loss: 0.029658, acc.: 100.00%] [G loss: 0.064512]\n",
      "epoch:36 step:28149 [D loss: 0.010069, acc.: 99.22%] [G loss: 0.050028]\n",
      "epoch:36 step:28150 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.120383]\n",
      "epoch:36 step:28151 [D loss: 0.004781, acc.: 100.00%] [G loss: 0.013596]\n",
      "epoch:36 step:28152 [D loss: 0.074702, acc.: 99.22%] [G loss: 0.000754]\n",
      "epoch:36 step:28153 [D loss: 0.001245, acc.: 100.00%] [G loss: 0.003465]\n",
      "epoch:36 step:28154 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.001955]\n",
      "epoch:36 step:28155 [D loss: 0.000650, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:36 step:28156 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.006141]\n",
      "epoch:36 step:28157 [D loss: 0.000693, acc.: 100.00%] [G loss: 0.000756]\n",
      "epoch:36 step:28158 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.000949]\n",
      "epoch:36 step:28159 [D loss: 0.002975, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:36 step:28160 [D loss: 0.000757, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:36 step:28161 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:36 step:28162 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.011352]\n",
      "epoch:36 step:28163 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.002231]\n",
      "epoch:36 step:28164 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:36 step:28165 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.209320]\n",
      "epoch:36 step:28166 [D loss: 0.001347, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:36 step:28167 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.003351]\n",
      "epoch:36 step:28168 [D loss: 0.016273, acc.: 100.00%] [G loss: 0.000225]\n",
      "epoch:36 step:28169 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000684]\n",
      "epoch:36 step:28170 [D loss: 0.013622, acc.: 99.22%] [G loss: 0.000143]\n",
      "epoch:36 step:28171 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.001524]\n",
      "epoch:36 step:28172 [D loss: 0.001627, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:36 step:28173 [D loss: 0.000655, acc.: 100.00%] [G loss: 0.004997]\n",
      "epoch:36 step:28174 [D loss: 0.000515, acc.: 100.00%] [G loss: 0.080475]\n",
      "epoch:36 step:28175 [D loss: 0.035798, acc.: 100.00%] [G loss: 0.007788]\n",
      "epoch:36 step:28176 [D loss: 0.000757, acc.: 100.00%] [G loss: 1.021124]\n",
      "epoch:36 step:28177 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.249033]\n",
      "epoch:36 step:28178 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.140727]\n",
      "epoch:36 step:28179 [D loss: 0.000823, acc.: 100.00%] [G loss: 0.055926]\n",
      "epoch:36 step:28180 [D loss: 0.003673, acc.: 100.00%] [G loss: 0.114623]\n",
      "epoch:36 step:28181 [D loss: 0.083134, acc.: 97.66%] [G loss: 0.476680]\n",
      "epoch:36 step:28182 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:36 step:28183 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.001854]\n",
      "epoch:36 step:28184 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:36 step:28185 [D loss: 0.012575, acc.: 99.22%] [G loss: 0.000029]\n",
      "epoch:36 step:28186 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000591]\n",
      "epoch:36 step:28187 [D loss: 0.004077, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:36 step:28188 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:36 step:28189 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:36 step:28190 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:36 step:28191 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:36 step:28192 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.001961]\n",
      "epoch:36 step:28193 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.177830]\n",
      "epoch:36 step:28194 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:36 step:28195 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:36 step:28196 [D loss: 0.002117, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:36 step:28197 [D loss: 0.000691, acc.: 100.00%] [G loss: 0.005852]\n",
      "epoch:36 step:28198 [D loss: 0.028828, acc.: 100.00%] [G loss: 0.000469]\n",
      "epoch:36 step:28199 [D loss: 0.007615, acc.: 100.00%] [G loss: 0.027004]\n",
      "epoch:36 step:28200 [D loss: 0.106185, acc.: 96.88%] [G loss: 8.440067]\n",
      "epoch:36 step:28201 [D loss: 0.044957, acc.: 97.66%] [G loss: 5.063232]\n",
      "epoch:36 step:28202 [D loss: 1.362299, acc.: 54.69%] [G loss: 12.489066]\n",
      "epoch:36 step:28203 [D loss: 1.756173, acc.: 53.12%] [G loss: 9.716574]\n",
      "epoch:36 step:28204 [D loss: 0.172251, acc.: 93.75%] [G loss: 7.098162]\n",
      "epoch:36 step:28205 [D loss: 0.130722, acc.: 96.09%] [G loss: 1.492220]\n",
      "epoch:36 step:28206 [D loss: 0.016460, acc.: 100.00%] [G loss: 7.079298]\n",
      "epoch:36 step:28207 [D loss: 0.192949, acc.: 90.62%] [G loss: 7.711332]\n",
      "epoch:36 step:28208 [D loss: 0.002876, acc.: 100.00%] [G loss: 8.338409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28209 [D loss: 0.027256, acc.: 99.22%] [G loss: 7.232883]\n",
      "epoch:36 step:28210 [D loss: 0.038730, acc.: 98.44%] [G loss: 5.125001]\n",
      "epoch:36 step:28211 [D loss: 0.031342, acc.: 100.00%] [G loss: 0.937487]\n",
      "epoch:36 step:28212 [D loss: 0.061825, acc.: 98.44%] [G loss: 0.469469]\n",
      "epoch:36 step:28213 [D loss: 0.051178, acc.: 99.22%] [G loss: 5.640978]\n",
      "epoch:36 step:28214 [D loss: 0.044491, acc.: 99.22%] [G loss: 0.057109]\n",
      "epoch:36 step:28215 [D loss: 0.182060, acc.: 92.97%] [G loss: 1.139051]\n",
      "epoch:36 step:28216 [D loss: 0.002968, acc.: 100.00%] [G loss: 0.560919]\n",
      "epoch:36 step:28217 [D loss: 0.002984, acc.: 100.00%] [G loss: 0.511031]\n",
      "epoch:36 step:28218 [D loss: 0.004638, acc.: 100.00%] [G loss: 0.189116]\n",
      "epoch:36 step:28219 [D loss: 0.005029, acc.: 100.00%] [G loss: 0.010238]\n",
      "epoch:36 step:28220 [D loss: 0.010984, acc.: 100.00%] [G loss: 0.017727]\n",
      "epoch:36 step:28221 [D loss: 0.005200, acc.: 100.00%] [G loss: 0.029524]\n",
      "epoch:36 step:28222 [D loss: 0.004400, acc.: 100.00%] [G loss: 0.034850]\n",
      "epoch:36 step:28223 [D loss: 0.004239, acc.: 100.00%] [G loss: 0.028876]\n",
      "epoch:36 step:28224 [D loss: 0.018098, acc.: 100.00%] [G loss: 0.156010]\n",
      "epoch:36 step:28225 [D loss: 0.010285, acc.: 100.00%] [G loss: 0.027753]\n",
      "epoch:36 step:28226 [D loss: 0.004471, acc.: 100.00%] [G loss: 0.043616]\n",
      "epoch:36 step:28227 [D loss: 0.018442, acc.: 98.44%] [G loss: 0.081546]\n",
      "epoch:36 step:28228 [D loss: 0.005242, acc.: 100.00%] [G loss: 0.034945]\n",
      "epoch:36 step:28229 [D loss: 0.020138, acc.: 100.00%] [G loss: 0.080679]\n",
      "epoch:36 step:28230 [D loss: 0.016916, acc.: 100.00%] [G loss: 0.567060]\n",
      "epoch:36 step:28231 [D loss: 0.052292, acc.: 97.66%] [G loss: 0.119758]\n",
      "epoch:36 step:28232 [D loss: 0.044083, acc.: 98.44%] [G loss: 0.249203]\n",
      "epoch:36 step:28233 [D loss: 0.003695, acc.: 100.00%] [G loss: 0.465908]\n",
      "epoch:36 step:28234 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.195580]\n",
      "epoch:36 step:28235 [D loss: 0.023174, acc.: 99.22%] [G loss: 0.087521]\n",
      "epoch:36 step:28236 [D loss: 0.012503, acc.: 99.22%] [G loss: 0.085096]\n",
      "epoch:36 step:28237 [D loss: 0.001132, acc.: 100.00%] [G loss: 0.147485]\n",
      "epoch:36 step:28238 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.002374]\n",
      "epoch:36 step:28239 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.006211]\n",
      "epoch:36 step:28240 [D loss: 0.001271, acc.: 100.00%] [G loss: 0.017198]\n",
      "epoch:36 step:28241 [D loss: 0.001137, acc.: 100.00%] [G loss: 0.001930]\n",
      "epoch:36 step:28242 [D loss: 0.001606, acc.: 100.00%] [G loss: 0.001381]\n",
      "epoch:36 step:28243 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.001384]\n",
      "epoch:36 step:28244 [D loss: 0.000643, acc.: 100.00%] [G loss: 0.000744]\n",
      "epoch:36 step:28245 [D loss: 0.005102, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:36 step:28246 [D loss: 0.021173, acc.: 100.00%] [G loss: 0.007188]\n",
      "epoch:36 step:28247 [D loss: 0.002742, acc.: 100.00%] [G loss: 0.036359]\n",
      "epoch:36 step:28248 [D loss: 0.101253, acc.: 96.88%] [G loss: 0.569441]\n",
      "epoch:36 step:28249 [D loss: 0.002421, acc.: 100.00%] [G loss: 1.176985]\n",
      "epoch:36 step:28250 [D loss: 0.009988, acc.: 99.22%] [G loss: 0.390972]\n",
      "epoch:36 step:28251 [D loss: 0.063658, acc.: 96.88%] [G loss: 0.191003]\n",
      "epoch:36 step:28252 [D loss: 0.003909, acc.: 100.00%] [G loss: 0.040120]\n",
      "epoch:36 step:28253 [D loss: 0.125007, acc.: 95.31%] [G loss: 0.318453]\n",
      "epoch:36 step:28254 [D loss: 0.003221, acc.: 100.00%] [G loss: 1.428090]\n",
      "epoch:36 step:28255 [D loss: 0.130751, acc.: 92.97%] [G loss: 0.017543]\n",
      "epoch:36 step:28256 [D loss: 0.009081, acc.: 99.22%] [G loss: 0.006375]\n",
      "epoch:36 step:28257 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.001123]\n",
      "epoch:36 step:28258 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.002601]\n",
      "epoch:36 step:28259 [D loss: 0.000598, acc.: 100.00%] [G loss: 0.001493]\n",
      "epoch:36 step:28260 [D loss: 0.000762, acc.: 100.00%] [G loss: 2.490701]\n",
      "epoch:36 step:28261 [D loss: 0.042456, acc.: 99.22%] [G loss: 0.006531]\n",
      "epoch:36 step:28262 [D loss: 0.001750, acc.: 100.00%] [G loss: 0.017459]\n",
      "epoch:36 step:28263 [D loss: 0.003944, acc.: 100.00%] [G loss: 0.027075]\n",
      "epoch:36 step:28264 [D loss: 0.022437, acc.: 100.00%] [G loss: 0.259465]\n",
      "epoch:36 step:28265 [D loss: 0.001844, acc.: 100.00%] [G loss: 0.107514]\n",
      "epoch:36 step:28266 [D loss: 0.005635, acc.: 100.00%] [G loss: 0.398420]\n",
      "epoch:36 step:28267 [D loss: 0.416793, acc.: 75.78%] [G loss: 8.663418]\n",
      "epoch:36 step:28268 [D loss: 2.398062, acc.: 50.78%] [G loss: 5.648874]\n",
      "epoch:36 step:28269 [D loss: 0.089758, acc.: 96.09%] [G loss: 0.273266]\n",
      "epoch:36 step:28270 [D loss: 0.030713, acc.: 100.00%] [G loss: 0.013607]\n",
      "epoch:36 step:28271 [D loss: 0.027246, acc.: 100.00%] [G loss: 3.939325]\n",
      "epoch:36 step:28272 [D loss: 0.022428, acc.: 99.22%] [G loss: 3.479002]\n",
      "epoch:36 step:28273 [D loss: 0.007043, acc.: 100.00%] [G loss: 0.032485]\n",
      "epoch:36 step:28274 [D loss: 0.043252, acc.: 97.66%] [G loss: 0.005720]\n",
      "epoch:36 step:28275 [D loss: 0.079957, acc.: 96.88%] [G loss: 0.102814]\n",
      "epoch:36 step:28276 [D loss: 0.047048, acc.: 99.22%] [G loss: 4.262131]\n",
      "epoch:36 step:28277 [D loss: 0.076124, acc.: 96.09%] [G loss: 2.297466]\n",
      "epoch:36 step:28278 [D loss: 0.001529, acc.: 100.00%] [G loss: 2.078865]\n",
      "epoch:36 step:28279 [D loss: 0.007635, acc.: 100.00%] [G loss: 0.775401]\n",
      "epoch:36 step:28280 [D loss: 0.021413, acc.: 100.00%] [G loss: 1.178770]\n",
      "epoch:36 step:28281 [D loss: 0.005329, acc.: 100.00%] [G loss: 2.591912]\n",
      "epoch:36 step:28282 [D loss: 0.087635, acc.: 96.88%] [G loss: 0.739582]\n",
      "epoch:36 step:28283 [D loss: 0.011384, acc.: 100.00%] [G loss: 0.209146]\n",
      "epoch:36 step:28284 [D loss: 0.008121, acc.: 100.00%] [G loss: 0.745680]\n",
      "epoch:36 step:28285 [D loss: 0.027097, acc.: 99.22%] [G loss: 0.087627]\n",
      "epoch:36 step:28286 [D loss: 0.022815, acc.: 100.00%] [G loss: 0.044931]\n",
      "epoch:36 step:28287 [D loss: 0.006651, acc.: 100.00%] [G loss: 0.034703]\n",
      "epoch:36 step:28288 [D loss: 0.034514, acc.: 100.00%] [G loss: 0.032853]\n",
      "epoch:36 step:28289 [D loss: 0.010478, acc.: 100.00%] [G loss: 0.154761]\n",
      "epoch:36 step:28290 [D loss: 0.048224, acc.: 99.22%] [G loss: 0.302162]\n",
      "epoch:36 step:28291 [D loss: 0.050162, acc.: 98.44%] [G loss: 0.154960]\n",
      "epoch:36 step:28292 [D loss: 0.080070, acc.: 97.66%] [G loss: 0.052032]\n",
      "epoch:36 step:28293 [D loss: 0.002011, acc.: 100.00%] [G loss: 0.706101]\n",
      "epoch:36 step:28294 [D loss: 0.071298, acc.: 97.66%] [G loss: 0.014770]\n",
      "epoch:36 step:28295 [D loss: 0.059462, acc.: 96.88%] [G loss: 0.000205]\n",
      "epoch:36 step:28296 [D loss: 0.001365, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:36 step:28297 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:36 step:28298 [D loss: 0.006252, acc.: 100.00%] [G loss: 0.001376]\n",
      "epoch:36 step:28299 [D loss: 0.002787, acc.: 100.00%] [G loss: 0.000599]\n",
      "epoch:36 step:28300 [D loss: 0.025499, acc.: 99.22%] [G loss: 0.001083]\n",
      "epoch:36 step:28301 [D loss: 0.045460, acc.: 100.00%] [G loss: 0.078048]\n",
      "epoch:36 step:28302 [D loss: 0.004110, acc.: 100.00%] [G loss: 0.608368]\n",
      "epoch:36 step:28303 [D loss: 0.011251, acc.: 100.00%] [G loss: 0.322864]\n",
      "epoch:36 step:28304 [D loss: 0.027094, acc.: 99.22%] [G loss: 0.463562]\n",
      "epoch:36 step:28305 [D loss: 0.053292, acc.: 99.22%] [G loss: 0.279780]\n",
      "epoch:36 step:28306 [D loss: 0.128859, acc.: 96.88%] [G loss: 0.245286]\n",
      "epoch:36 step:28307 [D loss: 0.057952, acc.: 100.00%] [G loss: 4.758574]\n",
      "epoch:36 step:28308 [D loss: 0.004632, acc.: 100.00%] [G loss: 3.789964]\n",
      "epoch:36 step:28309 [D loss: 0.481167, acc.: 81.25%] [G loss: 1.519705]\n",
      "epoch:36 step:28310 [D loss: 0.022021, acc.: 98.44%] [G loss: 2.896492]\n",
      "epoch:36 step:28311 [D loss: 0.264036, acc.: 85.16%] [G loss: 8.620766]\n",
      "epoch:36 step:28312 [D loss: 0.272992, acc.: 87.50%] [G loss: 1.512685]\n",
      "epoch:36 step:28313 [D loss: 0.885931, acc.: 67.19%] [G loss: 2.710809]\n",
      "epoch:36 step:28314 [D loss: 0.221099, acc.: 90.62%] [G loss: 0.000281]\n",
      "epoch:36 step:28315 [D loss: 0.006182, acc.: 100.00%] [G loss: 6.297690]\n",
      "epoch:36 step:28316 [D loss: 0.038577, acc.: 99.22%] [G loss: 0.148658]\n",
      "epoch:36 step:28317 [D loss: 0.007775, acc.: 100.00%] [G loss: 4.885849]\n",
      "epoch:36 step:28318 [D loss: 0.013518, acc.: 100.00%] [G loss: 4.017606]\n",
      "epoch:36 step:28319 [D loss: 0.013877, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:36 step:28320 [D loss: 0.096741, acc.: 96.88%] [G loss: 3.217360]\n",
      "epoch:36 step:28321 [D loss: 0.036703, acc.: 98.44%] [G loss: 1.600469]\n",
      "epoch:36 step:28322 [D loss: 0.158487, acc.: 92.97%] [G loss: 0.185829]\n",
      "epoch:36 step:28323 [D loss: 0.081623, acc.: 97.66%] [G loss: 0.073467]\n",
      "epoch:36 step:28324 [D loss: 0.005599, acc.: 100.00%] [G loss: 0.175213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28325 [D loss: 0.024693, acc.: 100.00%] [G loss: 0.213546]\n",
      "epoch:36 step:28326 [D loss: 0.025556, acc.: 100.00%] [G loss: 0.185374]\n",
      "epoch:36 step:28327 [D loss: 0.006507, acc.: 100.00%] [G loss: 0.145679]\n",
      "epoch:36 step:28328 [D loss: 0.040380, acc.: 99.22%] [G loss: 0.275725]\n",
      "epoch:36 step:28329 [D loss: 0.090093, acc.: 97.66%] [G loss: 3.396945]\n",
      "epoch:36 step:28330 [D loss: 0.182118, acc.: 92.97%] [G loss: 0.617415]\n",
      "epoch:36 step:28331 [D loss: 0.023765, acc.: 100.00%] [G loss: 0.038007]\n",
      "epoch:36 step:28332 [D loss: 0.015945, acc.: 100.00%] [G loss: 0.054215]\n",
      "epoch:36 step:28333 [D loss: 0.003507, acc.: 100.00%] [G loss: 0.044824]\n",
      "epoch:36 step:28334 [D loss: 0.022542, acc.: 99.22%] [G loss: 0.057533]\n",
      "epoch:36 step:28335 [D loss: 0.005044, acc.: 100.00%] [G loss: 0.072186]\n",
      "epoch:36 step:28336 [D loss: 0.137339, acc.: 96.09%] [G loss: 1.500715]\n",
      "epoch:36 step:28337 [D loss: 0.022932, acc.: 99.22%] [G loss: 2.688146]\n",
      "epoch:36 step:28338 [D loss: 0.145279, acc.: 98.44%] [G loss: 1.673459]\n",
      "epoch:36 step:28339 [D loss: 0.060242, acc.: 97.66%] [G loss: 0.380564]\n",
      "epoch:36 step:28340 [D loss: 0.006601, acc.: 100.00%] [G loss: 0.650346]\n",
      "epoch:36 step:28341 [D loss: 0.007660, acc.: 100.00%] [G loss: 0.378599]\n",
      "epoch:36 step:28342 [D loss: 0.008733, acc.: 100.00%] [G loss: 0.242254]\n",
      "epoch:36 step:28343 [D loss: 0.002551, acc.: 100.00%] [G loss: 0.045762]\n",
      "epoch:36 step:28344 [D loss: 0.007521, acc.: 100.00%] [G loss: 0.022362]\n",
      "epoch:36 step:28345 [D loss: 0.006326, acc.: 100.00%] [G loss: 0.048260]\n",
      "epoch:36 step:28346 [D loss: 0.014390, acc.: 100.00%] [G loss: 0.017316]\n",
      "epoch:36 step:28347 [D loss: 0.001089, acc.: 100.00%] [G loss: 0.002800]\n",
      "epoch:36 step:28348 [D loss: 0.015779, acc.: 100.00%] [G loss: 0.010297]\n",
      "epoch:36 step:28349 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.004169]\n",
      "epoch:36 step:28350 [D loss: 0.009476, acc.: 99.22%] [G loss: 0.005166]\n",
      "epoch:36 step:28351 [D loss: 0.002645, acc.: 100.00%] [G loss: 0.001328]\n",
      "epoch:36 step:28352 [D loss: 0.005389, acc.: 100.00%] [G loss: 0.004471]\n",
      "epoch:36 step:28353 [D loss: 0.003163, acc.: 100.00%] [G loss: 0.061208]\n",
      "epoch:36 step:28354 [D loss: 0.001010, acc.: 100.00%] [G loss: 0.000393]\n",
      "epoch:36 step:28355 [D loss: 0.001931, acc.: 100.00%] [G loss: 0.000538]\n",
      "epoch:36 step:28356 [D loss: 0.010498, acc.: 100.00%] [G loss: 0.011097]\n",
      "epoch:36 step:28357 [D loss: 0.007250, acc.: 100.00%] [G loss: 0.001354]\n",
      "epoch:36 step:28358 [D loss: 0.003214, acc.: 100.00%] [G loss: 0.003029]\n",
      "epoch:36 step:28359 [D loss: 0.031165, acc.: 100.00%] [G loss: 0.011829]\n",
      "epoch:36 step:28360 [D loss: 0.002862, acc.: 100.00%] [G loss: 0.192077]\n",
      "epoch:36 step:28361 [D loss: 0.011056, acc.: 100.00%] [G loss: 0.196131]\n",
      "epoch:36 step:28362 [D loss: 0.015157, acc.: 100.00%] [G loss: 0.313669]\n",
      "epoch:36 step:28363 [D loss: 0.088747, acc.: 96.88%] [G loss: 0.006975]\n",
      "epoch:36 step:28364 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.192754]\n",
      "epoch:36 step:28365 [D loss: 0.002906, acc.: 100.00%] [G loss: 0.008466]\n",
      "epoch:36 step:28366 [D loss: 0.009250, acc.: 99.22%] [G loss: 0.068003]\n",
      "epoch:36 step:28367 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.173995]\n",
      "epoch:36 step:28368 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.032575]\n",
      "epoch:36 step:28369 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.022767]\n",
      "epoch:36 step:28370 [D loss: 0.000636, acc.: 100.00%] [G loss: 0.116795]\n",
      "epoch:36 step:28371 [D loss: 0.000612, acc.: 100.00%] [G loss: 0.511905]\n",
      "epoch:36 step:28372 [D loss: 0.007070, acc.: 100.00%] [G loss: 0.170728]\n",
      "epoch:36 step:28373 [D loss: 0.000745, acc.: 100.00%] [G loss: 0.041816]\n",
      "epoch:36 step:28374 [D loss: 0.001356, acc.: 100.00%] [G loss: 0.089249]\n",
      "epoch:36 step:28375 [D loss: 0.016404, acc.: 100.00%] [G loss: 0.042055]\n",
      "epoch:36 step:28376 [D loss: 0.001271, acc.: 100.00%] [G loss: 0.181293]\n",
      "epoch:36 step:28377 [D loss: 0.010577, acc.: 99.22%] [G loss: 0.310744]\n",
      "epoch:36 step:28378 [D loss: 0.008688, acc.: 100.00%] [G loss: 0.075101]\n",
      "epoch:36 step:28379 [D loss: 0.000949, acc.: 100.00%] [G loss: 0.042053]\n",
      "epoch:36 step:28380 [D loss: 0.000723, acc.: 100.00%] [G loss: 0.023618]\n",
      "epoch:36 step:28381 [D loss: 0.021715, acc.: 99.22%] [G loss: 0.016740]\n",
      "epoch:36 step:28382 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.007317]\n",
      "epoch:36 step:28383 [D loss: 0.003528, acc.: 100.00%] [G loss: 0.000407]\n",
      "epoch:36 step:28384 [D loss: 0.000956, acc.: 100.00%] [G loss: 0.002321]\n",
      "epoch:36 step:28385 [D loss: 0.000486, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:36 step:28386 [D loss: 0.013840, acc.: 100.00%] [G loss: 0.026192]\n",
      "epoch:36 step:28387 [D loss: 0.016227, acc.: 100.00%] [G loss: 0.003479]\n",
      "epoch:36 step:28388 [D loss: 0.011944, acc.: 100.00%] [G loss: 0.026215]\n",
      "epoch:36 step:28389 [D loss: 0.002565, acc.: 100.00%] [G loss: 0.062499]\n",
      "epoch:36 step:28390 [D loss: 0.005629, acc.: 100.00%] [G loss: 0.019724]\n",
      "epoch:36 step:28391 [D loss: 0.001181, acc.: 100.00%] [G loss: 0.025987]\n",
      "epoch:36 step:28392 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.013022]\n",
      "epoch:36 step:28393 [D loss: 0.012638, acc.: 100.00%] [G loss: 0.006515]\n",
      "epoch:36 step:28394 [D loss: 0.001256, acc.: 100.00%] [G loss: 0.002926]\n",
      "epoch:36 step:28395 [D loss: 0.000663, acc.: 100.00%] [G loss: 0.002650]\n",
      "epoch:36 step:28396 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.006392]\n",
      "epoch:36 step:28397 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.015248]\n",
      "epoch:36 step:28398 [D loss: 0.000905, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:36 step:28399 [D loss: 0.000513, acc.: 100.00%] [G loss: 0.001492]\n",
      "epoch:36 step:28400 [D loss: 0.000326, acc.: 100.00%] [G loss: 4.788295]\n",
      "epoch:36 step:28401 [D loss: 0.002128, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:36 step:28402 [D loss: 0.008668, acc.: 100.00%] [G loss: 0.003234]\n",
      "epoch:36 step:28403 [D loss: 0.086286, acc.: 96.88%] [G loss: 0.221805]\n",
      "epoch:36 step:28404 [D loss: 0.004433, acc.: 100.00%] [G loss: 0.532458]\n",
      "epoch:36 step:28405 [D loss: 0.132056, acc.: 94.53%] [G loss: 0.101933]\n",
      "epoch:36 step:28406 [D loss: 0.000867, acc.: 100.00%] [G loss: 0.005447]\n",
      "epoch:36 step:28407 [D loss: 0.002673, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:36 step:28408 [D loss: 0.083892, acc.: 96.88%] [G loss: 0.449875]\n",
      "epoch:36 step:28409 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.742058]\n",
      "epoch:36 step:28410 [D loss: 0.069997, acc.: 96.88%] [G loss: 0.341694]\n",
      "epoch:36 step:28411 [D loss: 0.040322, acc.: 98.44%] [G loss: 0.019332]\n",
      "epoch:36 step:28412 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.035741]\n",
      "epoch:36 step:28413 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.017531]\n",
      "epoch:36 step:28414 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.001188]\n",
      "epoch:36 step:28415 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.006893]\n",
      "epoch:36 step:28416 [D loss: 0.002240, acc.: 100.00%] [G loss: 0.000323]\n",
      "epoch:36 step:28417 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.002285]\n",
      "epoch:36 step:28418 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.001502]\n",
      "epoch:36 step:28419 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.013849]\n",
      "epoch:36 step:28420 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.004071]\n",
      "epoch:36 step:28421 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.002203]\n",
      "epoch:36 step:28422 [D loss: 0.000920, acc.: 100.00%] [G loss: 0.003108]\n",
      "epoch:36 step:28423 [D loss: 0.001289, acc.: 100.00%] [G loss: 0.000457]\n",
      "epoch:36 step:28424 [D loss: 0.002447, acc.: 100.00%] [G loss: 0.003644]\n",
      "epoch:36 step:28425 [D loss: 0.004560, acc.: 100.00%] [G loss: 0.046760]\n",
      "epoch:36 step:28426 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.006626]\n",
      "epoch:36 step:28427 [D loss: 0.000914, acc.: 100.00%] [G loss: 3.924723]\n",
      "epoch:36 step:28428 [D loss: 0.006651, acc.: 100.00%] [G loss: 0.035874]\n",
      "epoch:36 step:28429 [D loss: 0.065346, acc.: 96.88%] [G loss: 1.407442]\n",
      "epoch:36 step:28430 [D loss: 0.003204, acc.: 100.00%] [G loss: 2.303597]\n",
      "epoch:36 step:28431 [D loss: 1.508223, acc.: 42.19%] [G loss: 9.709234]\n",
      "epoch:36 step:28432 [D loss: 1.135248, acc.: 60.16%] [G loss: 5.527776]\n",
      "epoch:36 step:28433 [D loss: 0.215685, acc.: 88.28%] [G loss: 6.029276]\n",
      "epoch:36 step:28434 [D loss: 0.068461, acc.: 99.22%] [G loss: 6.400561]\n",
      "epoch:36 step:28435 [D loss: 0.020767, acc.: 99.22%] [G loss: 5.825856]\n",
      "epoch:36 step:28436 [D loss: 0.324285, acc.: 83.59%] [G loss: 5.931594]\n",
      "epoch:36 step:28437 [D loss: 0.051196, acc.: 99.22%] [G loss: 7.075708]\n",
      "epoch:36 step:28438 [D loss: 0.017646, acc.: 100.00%] [G loss: 6.426411]\n",
      "epoch:36 step:28439 [D loss: 0.044677, acc.: 97.66%] [G loss: 1.344978]\n",
      "epoch:36 step:28440 [D loss: 0.010289, acc.: 100.00%] [G loss: 0.341265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28441 [D loss: 0.019145, acc.: 99.22%] [G loss: 4.420588]\n",
      "epoch:36 step:28442 [D loss: 0.163719, acc.: 95.31%] [G loss: 0.290040]\n",
      "epoch:36 step:28443 [D loss: 0.059823, acc.: 98.44%] [G loss: 0.100244]\n",
      "epoch:36 step:28444 [D loss: 0.030791, acc.: 100.00%] [G loss: 6.232807]\n",
      "epoch:36 step:28445 [D loss: 0.014319, acc.: 99.22%] [G loss: 5.642998]\n",
      "epoch:36 step:28446 [D loss: 0.011267, acc.: 100.00%] [G loss: 3.635622]\n",
      "epoch:36 step:28447 [D loss: 0.012935, acc.: 99.22%] [G loss: 3.305734]\n",
      "epoch:36 step:28448 [D loss: 0.002599, acc.: 100.00%] [G loss: 0.851050]\n",
      "epoch:36 step:28449 [D loss: 0.007838, acc.: 100.00%] [G loss: 0.651981]\n",
      "epoch:36 step:28450 [D loss: 0.018269, acc.: 100.00%] [G loss: 0.422433]\n",
      "epoch:36 step:28451 [D loss: 0.015864, acc.: 100.00%] [G loss: 0.847492]\n",
      "epoch:36 step:28452 [D loss: 0.027878, acc.: 100.00%] [G loss: 2.766623]\n",
      "epoch:36 step:28453 [D loss: 0.171224, acc.: 92.97%] [G loss: 4.834035]\n",
      "epoch:36 step:28454 [D loss: 0.501539, acc.: 79.69%] [G loss: 0.510016]\n",
      "epoch:36 step:28455 [D loss: 0.820082, acc.: 72.66%] [G loss: 8.632184]\n",
      "epoch:36 step:28456 [D loss: 1.100419, acc.: 61.72%] [G loss: 5.793993]\n",
      "epoch:36 step:28457 [D loss: 0.020144, acc.: 100.00%] [G loss: 4.524951]\n",
      "epoch:36 step:28458 [D loss: 0.024986, acc.: 100.00%] [G loss: 3.537522]\n",
      "epoch:36 step:28459 [D loss: 0.041215, acc.: 99.22%] [G loss: 2.309853]\n",
      "epoch:36 step:28460 [D loss: 0.209756, acc.: 90.62%] [G loss: 3.375561]\n",
      "epoch:36 step:28461 [D loss: 0.052085, acc.: 98.44%] [G loss: 4.146530]\n",
      "epoch:36 step:28462 [D loss: 0.091709, acc.: 96.09%] [G loss: 3.420079]\n",
      "epoch:36 step:28463 [D loss: 0.410195, acc.: 79.69%] [G loss: 4.234401]\n",
      "epoch:36 step:28464 [D loss: 0.043927, acc.: 99.22%] [G loss: 0.079943]\n",
      "epoch:36 step:28465 [D loss: 0.021811, acc.: 100.00%] [G loss: 5.387045]\n",
      "epoch:36 step:28466 [D loss: 0.217305, acc.: 88.28%] [G loss: 2.076250]\n",
      "epoch:36 step:28467 [D loss: 0.253331, acc.: 87.50%] [G loss: 4.769079]\n",
      "epoch:36 step:28468 [D loss: 0.009126, acc.: 100.00%] [G loss: 5.318131]\n",
      "epoch:36 step:28469 [D loss: 0.173729, acc.: 90.62%] [G loss: 3.785817]\n",
      "epoch:36 step:28470 [D loss: 0.039372, acc.: 98.44%] [G loss: 0.142548]\n",
      "epoch:36 step:28471 [D loss: 0.034321, acc.: 100.00%] [G loss: 3.094845]\n",
      "epoch:36 step:28472 [D loss: 0.029484, acc.: 100.00%] [G loss: 2.846659]\n",
      "epoch:36 step:28473 [D loss: 0.010030, acc.: 100.00%] [G loss: 1.880836]\n",
      "epoch:36 step:28474 [D loss: 0.054355, acc.: 99.22%] [G loss: 2.050801]\n",
      "epoch:36 step:28475 [D loss: 0.098034, acc.: 97.66%] [G loss: 2.329870]\n",
      "epoch:36 step:28476 [D loss: 0.038837, acc.: 99.22%] [G loss: 2.701849]\n",
      "epoch:36 step:28477 [D loss: 0.202942, acc.: 92.19%] [G loss: 4.660502]\n",
      "epoch:36 step:28478 [D loss: 0.017779, acc.: 100.00%] [G loss: 4.489980]\n",
      "epoch:36 step:28479 [D loss: 0.115681, acc.: 96.09%] [G loss: 4.086067]\n",
      "epoch:36 step:28480 [D loss: 0.126405, acc.: 93.75%] [G loss: 3.799554]\n",
      "epoch:36 step:28481 [D loss: 0.050139, acc.: 100.00%] [G loss: 4.213132]\n",
      "epoch:36 step:28482 [D loss: 0.100733, acc.: 96.88%] [G loss: 5.189517]\n",
      "epoch:36 step:28483 [D loss: 0.104958, acc.: 97.66%] [G loss: 5.361027]\n",
      "epoch:36 step:28484 [D loss: 0.046787, acc.: 98.44%] [G loss: 6.349136]\n",
      "epoch:36 step:28485 [D loss: 0.070397, acc.: 97.66%] [G loss: 5.526401]\n",
      "epoch:36 step:28486 [D loss: 0.212633, acc.: 92.97%] [G loss: 5.948782]\n",
      "epoch:36 step:28487 [D loss: 0.028685, acc.: 100.00%] [G loss: 5.507910]\n",
      "epoch:36 step:28488 [D loss: 0.064707, acc.: 97.66%] [G loss: 4.869023]\n",
      "epoch:36 step:28489 [D loss: 0.050449, acc.: 97.66%] [G loss: 3.304335]\n",
      "epoch:36 step:28490 [D loss: 0.098583, acc.: 99.22%] [G loss: 4.158278]\n",
      "epoch:36 step:28491 [D loss: 0.071344, acc.: 98.44%] [G loss: 4.938263]\n",
      "epoch:36 step:28492 [D loss: 0.124184, acc.: 95.31%] [G loss: 0.024702]\n",
      "epoch:36 step:28493 [D loss: 0.170638, acc.: 90.62%] [G loss: 1.135530]\n",
      "epoch:36 step:28494 [D loss: 0.056378, acc.: 96.88%] [G loss: 7.980995]\n",
      "epoch:36 step:28495 [D loss: 0.027398, acc.: 100.00%] [G loss: 5.614869]\n",
      "epoch:36 step:28496 [D loss: 0.187276, acc.: 93.75%] [G loss: 0.017102]\n",
      "epoch:36 step:28497 [D loss: 0.017059, acc.: 100.00%] [G loss: 3.792918]\n",
      "epoch:36 step:28498 [D loss: 0.004644, acc.: 100.00%] [G loss: 1.064811]\n",
      "epoch:36 step:28499 [D loss: 0.001414, acc.: 100.00%] [G loss: 1.536277]\n",
      "epoch:36 step:28500 [D loss: 0.001890, acc.: 100.00%] [G loss: 1.158564]\n",
      "epoch:36 step:28501 [D loss: 0.002799, acc.: 100.00%] [G loss: 0.363148]\n",
      "epoch:36 step:28502 [D loss: 0.004454, acc.: 100.00%] [G loss: 0.261112]\n",
      "epoch:36 step:28503 [D loss: 0.023064, acc.: 100.00%] [G loss: 0.170150]\n",
      "epoch:36 step:28504 [D loss: 0.006745, acc.: 100.00%] [G loss: 0.070238]\n",
      "epoch:36 step:28505 [D loss: 0.005957, acc.: 100.00%] [G loss: 0.070868]\n",
      "epoch:36 step:28506 [D loss: 0.001101, acc.: 100.00%] [G loss: 0.018117]\n",
      "epoch:36 step:28507 [D loss: 0.016826, acc.: 99.22%] [G loss: 0.022232]\n",
      "epoch:36 step:28508 [D loss: 0.027587, acc.: 99.22%] [G loss: 0.373804]\n",
      "epoch:36 step:28509 [D loss: 0.002185, acc.: 100.00%] [G loss: 0.072868]\n",
      "epoch:36 step:28510 [D loss: 0.010581, acc.: 100.00%] [G loss: 0.530010]\n",
      "epoch:36 step:28511 [D loss: 0.038052, acc.: 100.00%] [G loss: 0.103599]\n",
      "epoch:36 step:28512 [D loss: 0.035739, acc.: 100.00%] [G loss: 0.022436]\n",
      "epoch:36 step:28513 [D loss: 0.009070, acc.: 100.00%] [G loss: 0.161184]\n",
      "epoch:36 step:28514 [D loss: 0.010248, acc.: 100.00%] [G loss: 0.596990]\n",
      "epoch:36 step:28515 [D loss: 0.041287, acc.: 98.44%] [G loss: 0.512063]\n",
      "epoch:36 step:28516 [D loss: 0.001670, acc.: 100.00%] [G loss: 1.434666]\n",
      "epoch:36 step:28517 [D loss: 0.241788, acc.: 89.84%] [G loss: 1.366598]\n",
      "epoch:36 step:28518 [D loss: 0.148722, acc.: 94.53%] [G loss: 2.087531]\n",
      "epoch:36 step:28519 [D loss: 0.148156, acc.: 94.53%] [G loss: 4.311996]\n",
      "epoch:36 step:28520 [D loss: 0.087760, acc.: 96.09%] [G loss: 1.189427]\n",
      "epoch:36 step:28521 [D loss: 0.264329, acc.: 84.38%] [G loss: 0.068378]\n",
      "epoch:36 step:28522 [D loss: 0.002749, acc.: 100.00%] [G loss: 0.028288]\n",
      "epoch:36 step:28523 [D loss: 0.095584, acc.: 95.31%] [G loss: 0.343348]\n",
      "epoch:36 step:28524 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.117452]\n",
      "epoch:36 step:28525 [D loss: 0.001838, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:36 step:28526 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.351501]\n",
      "epoch:36 step:28527 [D loss: 0.010620, acc.: 100.00%] [G loss: 0.165762]\n",
      "epoch:36 step:28528 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.033066]\n",
      "epoch:36 step:28529 [D loss: 0.002962, acc.: 100.00%] [G loss: 0.016672]\n",
      "epoch:36 step:28530 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.034575]\n",
      "epoch:36 step:28531 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.008263]\n",
      "epoch:36 step:28532 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.005840]\n",
      "epoch:36 step:28533 [D loss: 0.002298, acc.: 100.00%] [G loss: 0.016886]\n",
      "epoch:36 step:28534 [D loss: 0.054251, acc.: 99.22%] [G loss: 0.168872]\n",
      "epoch:36 step:28535 [D loss: 0.002770, acc.: 100.00%] [G loss: 0.892167]\n",
      "epoch:36 step:28536 [D loss: 0.058903, acc.: 96.88%] [G loss: 0.000047]\n",
      "epoch:36 step:28537 [D loss: 0.000555, acc.: 100.00%] [G loss: 0.170511]\n",
      "epoch:36 step:28538 [D loss: 0.001855, acc.: 100.00%] [G loss: 0.055694]\n",
      "epoch:36 step:28539 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.134154]\n",
      "epoch:36 step:28540 [D loss: 0.001756, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:36 step:28541 [D loss: 0.000896, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:36 step:28542 [D loss: 0.015299, acc.: 100.00%] [G loss: 0.007498]\n",
      "epoch:36 step:28543 [D loss: 0.001870, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:36 step:28544 [D loss: 0.116587, acc.: 94.53%] [G loss: 0.652714]\n",
      "epoch:36 step:28545 [D loss: 0.022781, acc.: 99.22%] [G loss: 3.032537]\n",
      "epoch:36 step:28546 [D loss: 0.185383, acc.: 91.41%] [G loss: 0.001382]\n",
      "epoch:36 step:28547 [D loss: 0.002521, acc.: 100.00%] [G loss: 0.004011]\n",
      "epoch:36 step:28548 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.003735]\n",
      "epoch:36 step:28549 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.060581]\n",
      "epoch:36 step:28550 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.012761]\n",
      "epoch:36 step:28551 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.000558]\n",
      "epoch:36 step:28552 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:36 step:28553 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.001266]\n",
      "epoch:36 step:28554 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:36 step:28555 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.001343]\n",
      "epoch:36 step:28556 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28557 [D loss: 0.000428, acc.: 100.00%] [G loss: 0.258039]\n",
      "epoch:36 step:28558 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000522]\n",
      "epoch:36 step:28559 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.001886]\n",
      "epoch:36 step:28560 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000428]\n",
      "epoch:36 step:28561 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:36 step:28562 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:36 step:28563 [D loss: 0.000542, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:36 step:28564 [D loss: 0.001236, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:36 step:28565 [D loss: 0.001583, acc.: 100.00%] [G loss: 0.000323]\n",
      "epoch:36 step:28566 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:36 step:28567 [D loss: 0.044936, acc.: 99.22%] [G loss: 0.028737]\n",
      "epoch:36 step:28568 [D loss: 0.000589, acc.: 100.00%] [G loss: 0.164232]\n",
      "epoch:36 step:28569 [D loss: 0.017219, acc.: 100.00%] [G loss: 0.209710]\n",
      "epoch:36 step:28570 [D loss: 0.004344, acc.: 100.00%] [G loss: 0.162654]\n",
      "epoch:36 step:28571 [D loss: 0.002159, acc.: 100.00%] [G loss: 0.001076]\n",
      "epoch:36 step:28572 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.006549]\n",
      "epoch:36 step:28573 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.042902]\n",
      "epoch:36 step:28574 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.038480]\n",
      "epoch:36 step:28575 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.050342]\n",
      "epoch:36 step:28576 [D loss: 0.003537, acc.: 100.00%] [G loss: 0.000931]\n",
      "epoch:36 step:28577 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.005119]\n",
      "epoch:36 step:28578 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.004271]\n",
      "epoch:36 step:28579 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.006602]\n",
      "epoch:36 step:28580 [D loss: 0.000698, acc.: 100.00%] [G loss: 0.001937]\n",
      "epoch:36 step:28581 [D loss: 0.000512, acc.: 100.00%] [G loss: 0.012041]\n",
      "epoch:36 step:28582 [D loss: 0.003210, acc.: 100.00%] [G loss: 0.001919]\n",
      "epoch:36 step:28583 [D loss: 0.003955, acc.: 100.00%] [G loss: 0.004794]\n",
      "epoch:36 step:28584 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.001230]\n",
      "epoch:36 step:28585 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.003219]\n",
      "epoch:36 step:28586 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.001678]\n",
      "epoch:36 step:28587 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.003498]\n",
      "epoch:36 step:28588 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.004729]\n",
      "epoch:36 step:28589 [D loss: 0.001310, acc.: 100.00%] [G loss: 0.003127]\n",
      "epoch:36 step:28590 [D loss: 0.007160, acc.: 100.00%] [G loss: 2.183670]\n",
      "epoch:36 step:28591 [D loss: 0.384645, acc.: 77.34%] [G loss: 10.301527]\n",
      "epoch:36 step:28592 [D loss: 0.977619, acc.: 59.38%] [G loss: 4.103139]\n",
      "epoch:36 step:28593 [D loss: 0.138158, acc.: 92.97%] [G loss: 4.456246]\n",
      "epoch:36 step:28594 [D loss: 0.018593, acc.: 100.00%] [G loss: 4.062636]\n",
      "epoch:36 step:28595 [D loss: 0.004647, acc.: 100.00%] [G loss: 1.626678]\n",
      "epoch:36 step:28596 [D loss: 0.008305, acc.: 100.00%] [G loss: 1.838223]\n",
      "epoch:36 step:28597 [D loss: 0.217794, acc.: 89.84%] [G loss: 1.983099]\n",
      "epoch:36 step:28598 [D loss: 0.174328, acc.: 92.19%] [G loss: 2.925861]\n",
      "epoch:36 step:28599 [D loss: 0.044188, acc.: 99.22%] [G loss: 2.825474]\n",
      "epoch:36 step:28600 [D loss: 0.013107, acc.: 100.00%] [G loss: 2.383572]\n",
      "epoch:36 step:28601 [D loss: 0.038483, acc.: 100.00%] [G loss: 1.171751]\n",
      "epoch:36 step:28602 [D loss: 0.089761, acc.: 97.66%] [G loss: 6.680401]\n",
      "epoch:36 step:28603 [D loss: 0.252404, acc.: 91.41%] [G loss: 5.460186]\n",
      "epoch:36 step:28604 [D loss: 0.009133, acc.: 100.00%] [G loss: 6.797379]\n",
      "epoch:36 step:28605 [D loss: 0.169169, acc.: 92.19%] [G loss: 4.262202]\n",
      "epoch:36 step:28606 [D loss: 0.129195, acc.: 96.09%] [G loss: 4.681644]\n",
      "epoch:36 step:28607 [D loss: 0.028929, acc.: 100.00%] [G loss: 5.231675]\n",
      "epoch:36 step:28608 [D loss: 0.021106, acc.: 99.22%] [G loss: 0.033920]\n",
      "epoch:36 step:28609 [D loss: 0.008601, acc.: 100.00%] [G loss: 0.364963]\n",
      "epoch:36 step:28610 [D loss: 0.005422, acc.: 100.00%] [G loss: 2.159586]\n",
      "epoch:36 step:28611 [D loss: 0.001119, acc.: 100.00%] [G loss: 1.422845]\n",
      "epoch:36 step:28612 [D loss: 0.007372, acc.: 100.00%] [G loss: 0.889187]\n",
      "epoch:36 step:28613 [D loss: 0.014232, acc.: 99.22%] [G loss: 0.466038]\n",
      "epoch:36 step:28614 [D loss: 0.020799, acc.: 99.22%] [G loss: 0.096179]\n",
      "epoch:36 step:28615 [D loss: 0.353705, acc.: 86.72%] [G loss: 3.869480]\n",
      "epoch:36 step:28616 [D loss: 0.350922, acc.: 87.50%] [G loss: 2.342071]\n",
      "epoch:36 step:28617 [D loss: 0.334469, acc.: 85.16%] [G loss: 0.086662]\n",
      "epoch:36 step:28618 [D loss: 0.017129, acc.: 100.00%] [G loss: 4.438401]\n",
      "epoch:36 step:28619 [D loss: 0.004941, acc.: 100.00%] [G loss: 3.385045]\n",
      "epoch:36 step:28620 [D loss: 0.074073, acc.: 96.09%] [G loss: 0.490665]\n",
      "epoch:36 step:28621 [D loss: 0.001438, acc.: 100.00%] [G loss: 0.203276]\n",
      "epoch:36 step:28622 [D loss: 0.004171, acc.: 100.00%] [G loss: 0.154751]\n",
      "epoch:36 step:28623 [D loss: 0.001686, acc.: 100.00%] [G loss: 0.118731]\n",
      "epoch:36 step:28624 [D loss: 0.017116, acc.: 99.22%] [G loss: 2.424692]\n",
      "epoch:36 step:28625 [D loss: 0.097440, acc.: 96.09%] [G loss: 2.556725]\n",
      "epoch:36 step:28626 [D loss: 0.004231, acc.: 100.00%] [G loss: 1.517191]\n",
      "epoch:36 step:28627 [D loss: 0.158565, acc.: 94.53%] [G loss: 2.643008]\n",
      "epoch:36 step:28628 [D loss: 0.086380, acc.: 96.88%] [G loss: 1.805159]\n",
      "epoch:36 step:28629 [D loss: 0.311236, acc.: 85.94%] [G loss: 3.681628]\n",
      "epoch:36 step:28630 [D loss: 0.036154, acc.: 100.00%] [G loss: 4.334786]\n",
      "epoch:36 step:28631 [D loss: 0.239700, acc.: 93.75%] [G loss: 2.026017]\n",
      "epoch:36 step:28632 [D loss: 0.015317, acc.: 99.22%] [G loss: 3.547849]\n",
      "epoch:36 step:28633 [D loss: 0.401375, acc.: 83.59%] [G loss: 9.613403]\n",
      "epoch:36 step:28634 [D loss: 0.691739, acc.: 72.66%] [G loss: 5.913503]\n",
      "epoch:36 step:28635 [D loss: 0.083002, acc.: 96.09%] [G loss: 6.153796]\n",
      "epoch:36 step:28636 [D loss: 0.024971, acc.: 99.22%] [G loss: 0.141399]\n",
      "epoch:36 step:28637 [D loss: 0.006266, acc.: 100.00%] [G loss: 5.215619]\n",
      "epoch:36 step:28638 [D loss: 0.044815, acc.: 98.44%] [G loss: 0.000519]\n",
      "epoch:36 step:28639 [D loss: 0.001500, acc.: 100.00%] [G loss: 2.328561]\n",
      "epoch:36 step:28640 [D loss: 0.055632, acc.: 98.44%] [G loss: 0.000050]\n",
      "epoch:36 step:28641 [D loss: 0.089068, acc.: 97.66%] [G loss: 0.813453]\n",
      "epoch:36 step:28642 [D loss: 0.400087, acc.: 81.25%] [G loss: 0.102746]\n",
      "epoch:36 step:28643 [D loss: 0.001670, acc.: 100.00%] [G loss: 0.440728]\n",
      "epoch:36 step:28644 [D loss: 0.005715, acc.: 100.00%] [G loss: 0.122122]\n",
      "epoch:36 step:28645 [D loss: 0.000907, acc.: 100.00%] [G loss: 3.960952]\n",
      "epoch:36 step:28646 [D loss: 0.004479, acc.: 100.00%] [G loss: 1.657930]\n",
      "epoch:36 step:28647 [D loss: 0.003170, acc.: 100.00%] [G loss: 0.327728]\n",
      "epoch:36 step:28648 [D loss: 0.051483, acc.: 99.22%] [G loss: 1.779203]\n",
      "epoch:36 step:28649 [D loss: 0.032959, acc.: 100.00%] [G loss: 0.764414]\n",
      "epoch:36 step:28650 [D loss: 0.058875, acc.: 98.44%] [G loss: 0.275835]\n",
      "epoch:36 step:28651 [D loss: 0.032407, acc.: 100.00%] [G loss: 0.081169]\n",
      "epoch:36 step:28652 [D loss: 0.049429, acc.: 100.00%] [G loss: 2.272846]\n",
      "epoch:36 step:28653 [D loss: 0.443344, acc.: 83.59%] [G loss: 1.317644]\n",
      "epoch:36 step:28654 [D loss: 0.004851, acc.: 100.00%] [G loss: 0.110934]\n",
      "epoch:36 step:28655 [D loss: 0.004663, acc.: 100.00%] [G loss: 0.205253]\n",
      "epoch:36 step:28656 [D loss: 0.006923, acc.: 100.00%] [G loss: 0.089770]\n",
      "epoch:36 step:28657 [D loss: 0.001232, acc.: 100.00%] [G loss: 0.042873]\n",
      "epoch:36 step:28658 [D loss: 0.035202, acc.: 99.22%] [G loss: 0.002718]\n",
      "epoch:36 step:28659 [D loss: 0.008397, acc.: 100.00%] [G loss: 0.000621]\n",
      "epoch:36 step:28660 [D loss: 0.011745, acc.: 100.00%] [G loss: 0.779938]\n",
      "epoch:36 step:28661 [D loss: 0.000521, acc.: 100.00%] [G loss: 0.000917]\n",
      "epoch:36 step:28662 [D loss: 0.001619, acc.: 100.00%] [G loss: 0.001692]\n",
      "epoch:36 step:28663 [D loss: 0.003255, acc.: 100.00%] [G loss: 0.006571]\n",
      "epoch:36 step:28664 [D loss: 0.003226, acc.: 100.00%] [G loss: 0.001438]\n",
      "epoch:36 step:28665 [D loss: 0.005747, acc.: 100.00%] [G loss: 0.006928]\n",
      "epoch:36 step:28666 [D loss: 0.008025, acc.: 100.00%] [G loss: 0.004751]\n",
      "epoch:36 step:28667 [D loss: 0.002785, acc.: 100.00%] [G loss: 0.025653]\n",
      "epoch:36 step:28668 [D loss: 0.006467, acc.: 100.00%] [G loss: 0.026500]\n",
      "epoch:36 step:28669 [D loss: 0.003180, acc.: 100.00%] [G loss: 0.892516]\n",
      "epoch:36 step:28670 [D loss: 0.002469, acc.: 100.00%] [G loss: 0.003940]\n",
      "epoch:36 step:28671 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.002134]\n",
      "epoch:36 step:28672 [D loss: 0.001978, acc.: 100.00%] [G loss: 0.022498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28673 [D loss: 0.004413, acc.: 100.00%] [G loss: 0.001901]\n",
      "epoch:36 step:28674 [D loss: 0.003720, acc.: 100.00%] [G loss: 0.146611]\n",
      "epoch:36 step:28675 [D loss: 0.000696, acc.: 100.00%] [G loss: 0.004417]\n",
      "epoch:36 step:28676 [D loss: 0.001176, acc.: 100.00%] [G loss: 0.000549]\n",
      "epoch:36 step:28677 [D loss: 0.001427, acc.: 100.00%] [G loss: 0.001656]\n",
      "epoch:36 step:28678 [D loss: 0.004692, acc.: 100.00%] [G loss: 0.005229]\n",
      "epoch:36 step:28679 [D loss: 0.001697, acc.: 100.00%] [G loss: 0.001507]\n",
      "epoch:36 step:28680 [D loss: 0.006247, acc.: 100.00%] [G loss: 0.003028]\n",
      "epoch:36 step:28681 [D loss: 0.001791, acc.: 100.00%] [G loss: 0.287806]\n",
      "epoch:36 step:28682 [D loss: 0.002702, acc.: 100.00%] [G loss: 0.001554]\n",
      "epoch:36 step:28683 [D loss: 0.005401, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:36 step:28684 [D loss: 0.000709, acc.: 100.00%] [G loss: 0.012805]\n",
      "epoch:36 step:28685 [D loss: 0.000990, acc.: 100.00%] [G loss: 0.084615]\n",
      "epoch:36 step:28686 [D loss: 0.000649, acc.: 100.00%] [G loss: 0.001721]\n",
      "epoch:36 step:28687 [D loss: 0.003720, acc.: 100.00%] [G loss: 0.001531]\n",
      "epoch:36 step:28688 [D loss: 0.001905, acc.: 100.00%] [G loss: 0.047498]\n",
      "epoch:36 step:28689 [D loss: 0.012817, acc.: 99.22%] [G loss: 0.011060]\n",
      "epoch:36 step:28690 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.007357]\n",
      "epoch:36 step:28691 [D loss: 0.005214, acc.: 100.00%] [G loss: 0.000487]\n",
      "epoch:36 step:28692 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.003137]\n",
      "epoch:36 step:28693 [D loss: 0.001003, acc.: 100.00%] [G loss: 0.002053]\n",
      "epoch:36 step:28694 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000437]\n",
      "epoch:36 step:28695 [D loss: 0.000600, acc.: 100.00%] [G loss: 1.671057]\n",
      "epoch:36 step:28696 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.001538]\n",
      "epoch:36 step:28697 [D loss: 0.003894, acc.: 100.00%] [G loss: 0.001675]\n",
      "epoch:36 step:28698 [D loss: 0.207055, acc.: 88.28%] [G loss: 5.244874]\n",
      "epoch:36 step:28699 [D loss: 0.477682, acc.: 79.69%] [G loss: 0.003999]\n",
      "epoch:36 step:28700 [D loss: 0.131569, acc.: 93.75%] [G loss: 0.003112]\n",
      "epoch:36 step:28701 [D loss: 0.001917, acc.: 100.00%] [G loss: 2.085320]\n",
      "epoch:36 step:28702 [D loss: 0.019924, acc.: 99.22%] [G loss: 1.424554]\n",
      "epoch:36 step:28703 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.495015]\n",
      "epoch:36 step:28704 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.366153]\n",
      "epoch:36 step:28705 [D loss: 0.008951, acc.: 100.00%] [G loss: 0.006067]\n",
      "epoch:36 step:28706 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.012010]\n",
      "epoch:36 step:28707 [D loss: 0.000417, acc.: 100.00%] [G loss: 0.178236]\n",
      "epoch:36 step:28708 [D loss: 0.001354, acc.: 100.00%] [G loss: 0.003628]\n",
      "epoch:36 step:28709 [D loss: 0.003704, acc.: 100.00%] [G loss: 0.131031]\n",
      "epoch:36 step:28710 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.062742]\n",
      "epoch:36 step:28711 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.010244]\n",
      "epoch:36 step:28712 [D loss: 0.003250, acc.: 100.00%] [G loss: 0.006366]\n",
      "epoch:36 step:28713 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.002180]\n",
      "epoch:36 step:28714 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.005187]\n",
      "epoch:36 step:28715 [D loss: 0.000695, acc.: 100.00%] [G loss: 0.010152]\n",
      "epoch:36 step:28716 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.001221]\n",
      "epoch:36 step:28717 [D loss: 0.002503, acc.: 100.00%] [G loss: 0.001919]\n",
      "epoch:36 step:28718 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.002932]\n",
      "epoch:36 step:28719 [D loss: 0.001601, acc.: 100.00%] [G loss: 0.000778]\n",
      "epoch:36 step:28720 [D loss: 0.003343, acc.: 100.00%] [G loss: 0.004519]\n",
      "epoch:36 step:28721 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000349]\n",
      "epoch:36 step:28722 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.006078]\n",
      "epoch:36 step:28723 [D loss: 0.000390, acc.: 100.00%] [G loss: 0.008454]\n",
      "epoch:36 step:28724 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000972]\n",
      "epoch:36 step:28725 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000917]\n",
      "epoch:36 step:28726 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000844]\n",
      "epoch:36 step:28727 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.003073]\n",
      "epoch:36 step:28728 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.012117]\n",
      "epoch:36 step:28729 [D loss: 0.003525, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:36 step:28730 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.009976]\n",
      "epoch:36 step:28731 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.005147]\n",
      "epoch:36 step:28732 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:36 step:28733 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000237]\n",
      "epoch:36 step:28734 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:36 step:28735 [D loss: 0.002538, acc.: 100.00%] [G loss: 0.001278]\n",
      "epoch:36 step:28736 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:36 step:28737 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.010018]\n",
      "epoch:36 step:28738 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.016438]\n",
      "epoch:36 step:28739 [D loss: 0.005472, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:36 step:28740 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.000501]\n",
      "epoch:36 step:28741 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.002077]\n",
      "epoch:36 step:28742 [D loss: 0.000519, acc.: 100.00%] [G loss: 1.180696]\n",
      "epoch:36 step:28743 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:36 step:28744 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.004295]\n",
      "epoch:36 step:28745 [D loss: 0.000549, acc.: 100.00%] [G loss: 0.001913]\n",
      "epoch:36 step:28746 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000894]\n",
      "epoch:36 step:28747 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.000412]\n",
      "epoch:36 step:28748 [D loss: 0.001297, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:36 step:28749 [D loss: 0.003416, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:36 step:28750 [D loss: 0.000605, acc.: 100.00%] [G loss: 0.002717]\n",
      "epoch:36 step:28751 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.151579]\n",
      "epoch:36 step:28752 [D loss: 0.000440, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:36 step:28753 [D loss: 0.007556, acc.: 100.00%] [G loss: 0.001314]\n",
      "epoch:36 step:28754 [D loss: 0.014934, acc.: 100.00%] [G loss: 0.013220]\n",
      "epoch:36 step:28755 [D loss: 0.001145, acc.: 100.00%] [G loss: 0.060748]\n",
      "epoch:36 step:28756 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.003157]\n",
      "epoch:36 step:28757 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.045361]\n",
      "epoch:36 step:28758 [D loss: 0.010278, acc.: 99.22%] [G loss: 0.012129]\n",
      "epoch:36 step:28759 [D loss: 0.000589, acc.: 100.00%] [G loss: 0.000917]\n",
      "epoch:36 step:28760 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.002282]\n",
      "epoch:36 step:28761 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.006079]\n",
      "epoch:36 step:28762 [D loss: 0.003231, acc.: 100.00%] [G loss: 0.010799]\n",
      "epoch:36 step:28763 [D loss: 0.008596, acc.: 100.00%] [G loss: 0.001772]\n",
      "epoch:36 step:28764 [D loss: 0.001435, acc.: 100.00%] [G loss: 0.003218]\n",
      "epoch:36 step:28765 [D loss: 0.002723, acc.: 100.00%] [G loss: 0.012163]\n",
      "epoch:36 step:28766 [D loss: 0.004087, acc.: 100.00%] [G loss: 0.002756]\n",
      "epoch:36 step:28767 [D loss: 0.000911, acc.: 100.00%] [G loss: 0.000994]\n",
      "epoch:36 step:28768 [D loss: 0.001289, acc.: 100.00%] [G loss: 0.005306]\n",
      "epoch:36 step:28769 [D loss: 0.000391, acc.: 100.00%] [G loss: 0.000678]\n",
      "epoch:36 step:28770 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.010569]\n",
      "epoch:36 step:28771 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.000929]\n",
      "epoch:36 step:28772 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.000948]\n",
      "epoch:36 step:28773 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.000646]\n",
      "epoch:36 step:28774 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.003999]\n",
      "epoch:36 step:28775 [D loss: 0.003038, acc.: 100.00%] [G loss: 0.008661]\n",
      "epoch:36 step:28776 [D loss: 0.000499, acc.: 100.00%] [G loss: 0.003099]\n",
      "epoch:36 step:28777 [D loss: 0.000897, acc.: 100.00%] [G loss: 0.002142]\n",
      "epoch:36 step:28778 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.452017]\n",
      "epoch:36 step:28779 [D loss: 0.001179, acc.: 100.00%] [G loss: 0.008098]\n",
      "epoch:36 step:28780 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.001040]\n",
      "epoch:36 step:28781 [D loss: 0.002392, acc.: 100.00%] [G loss: 0.008107]\n",
      "epoch:36 step:28782 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.003205]\n",
      "epoch:36 step:28783 [D loss: 0.054161, acc.: 99.22%] [G loss: 0.492154]\n",
      "epoch:36 step:28784 [D loss: 0.007525, acc.: 100.00%] [G loss: 2.004107]\n",
      "epoch:36 step:28785 [D loss: 0.051187, acc.: 97.66%] [G loss: 0.141766]\n",
      "epoch:36 step:28786 [D loss: 0.005143, acc.: 100.00%] [G loss: 0.117423]\n",
      "epoch:36 step:28787 [D loss: 0.000721, acc.: 100.00%] [G loss: 0.006375]\n",
      "epoch:36 step:28788 [D loss: 0.003304, acc.: 100.00%] [G loss: 0.003787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:36 step:28789 [D loss: 0.000793, acc.: 100.00%] [G loss: 0.001643]\n",
      "epoch:36 step:28790 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000835]\n",
      "epoch:36 step:28791 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000845]\n",
      "epoch:36 step:28792 [D loss: 0.000496, acc.: 100.00%] [G loss: 0.003822]\n",
      "epoch:36 step:28793 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.012736]\n",
      "epoch:36 step:28794 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:36 step:28795 [D loss: 0.000349, acc.: 100.00%] [G loss: 0.142079]\n",
      "epoch:36 step:28796 [D loss: 0.000795, acc.: 100.00%] [G loss: 0.001365]\n",
      "epoch:36 step:28797 [D loss: 0.000794, acc.: 100.00%] [G loss: 0.056659]\n",
      "epoch:36 step:28798 [D loss: 0.007002, acc.: 100.00%] [G loss: 0.000374]\n",
      "epoch:36 step:28799 [D loss: 0.005204, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:36 step:28800 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.033451]\n",
      "epoch:36 step:28801 [D loss: 0.006732, acc.: 100.00%] [G loss: 0.003385]\n",
      "epoch:36 step:28802 [D loss: 0.018051, acc.: 100.00%] [G loss: 0.003197]\n",
      "epoch:36 step:28803 [D loss: 0.034841, acc.: 99.22%] [G loss: 0.158555]\n",
      "epoch:36 step:28804 [D loss: 0.005275, acc.: 100.00%] [G loss: 0.034312]\n",
      "epoch:36 step:28805 [D loss: 0.058399, acc.: 98.44%] [G loss: 1.880086]\n",
      "epoch:36 step:28806 [D loss: 0.061309, acc.: 96.88%] [G loss: 0.037563]\n",
      "epoch:36 step:28807 [D loss: 0.008818, acc.: 100.00%] [G loss: 0.042090]\n",
      "epoch:36 step:28808 [D loss: 0.047899, acc.: 97.66%] [G loss: 0.010445]\n",
      "epoch:36 step:28809 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.000494]\n",
      "epoch:36 step:28810 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.003805]\n",
      "epoch:36 step:28811 [D loss: 0.011188, acc.: 100.00%] [G loss: 0.004366]\n",
      "epoch:36 step:28812 [D loss: 0.007732, acc.: 100.00%] [G loss: 0.022360]\n",
      "epoch:36 step:28813 [D loss: 0.001681, acc.: 100.00%] [G loss: 0.017992]\n",
      "epoch:36 step:28814 [D loss: 0.009418, acc.: 100.00%] [G loss: 0.102533]\n",
      "epoch:36 step:28815 [D loss: 0.017376, acc.: 100.00%] [G loss: 0.017262]\n",
      "epoch:36 step:28816 [D loss: 0.000973, acc.: 100.00%] [G loss: 1.222700]\n",
      "epoch:36 step:28817 [D loss: 0.000151, acc.: 100.00%] [G loss: 5.449191]\n",
      "epoch:36 step:28818 [D loss: 0.002695, acc.: 100.00%] [G loss: 0.238819]\n",
      "epoch:36 step:28819 [D loss: 0.009492, acc.: 100.00%] [G loss: 0.092637]\n",
      "epoch:36 step:28820 [D loss: 0.001762, acc.: 100.00%] [G loss: 0.006859]\n",
      "epoch:36 step:28821 [D loss: 0.001639, acc.: 100.00%] [G loss: 0.075768]\n",
      "epoch:36 step:28822 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.006237]\n",
      "epoch:36 step:28823 [D loss: 0.001288, acc.: 100.00%] [G loss: 0.003380]\n",
      "epoch:36 step:28824 [D loss: 0.007204, acc.: 100.00%] [G loss: 0.007274]\n",
      "epoch:36 step:28825 [D loss: 0.004393, acc.: 100.00%] [G loss: 0.036601]\n",
      "epoch:36 step:28826 [D loss: 0.029925, acc.: 99.22%] [G loss: 1.485481]\n",
      "epoch:36 step:28827 [D loss: 0.129778, acc.: 95.31%] [G loss: 2.406770]\n",
      "epoch:36 step:28828 [D loss: 0.532837, acc.: 76.56%] [G loss: 1.668435]\n",
      "epoch:36 step:28829 [D loss: 0.098775, acc.: 95.31%] [G loss: 0.719387]\n",
      "epoch:36 step:28830 [D loss: 0.082425, acc.: 97.66%] [G loss: 2.709664]\n",
      "epoch:36 step:28831 [D loss: 0.071663, acc.: 96.09%] [G loss: 1.001358]\n",
      "epoch:36 step:28832 [D loss: 0.003840, acc.: 100.00%] [G loss: 0.260990]\n",
      "epoch:36 step:28833 [D loss: 0.001355, acc.: 100.00%] [G loss: 0.168091]\n",
      "epoch:36 step:28834 [D loss: 0.005923, acc.: 100.00%] [G loss: 0.067675]\n",
      "epoch:36 step:28835 [D loss: 0.059402, acc.: 97.66%] [G loss: 0.298960]\n",
      "epoch:36 step:28836 [D loss: 0.000879, acc.: 100.00%] [G loss: 1.658019]\n",
      "epoch:36 step:28837 [D loss: 0.013135, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:36 step:28838 [D loss: 0.003413, acc.: 100.00%] [G loss: 0.556269]\n",
      "epoch:36 step:28839 [D loss: 0.097761, acc.: 96.09%] [G loss: 0.001205]\n",
      "epoch:36 step:28840 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.002706]\n",
      "epoch:36 step:28841 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.009801]\n",
      "epoch:36 step:28842 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:36 step:28843 [D loss: 0.155464, acc.: 98.44%] [G loss: 0.022607]\n",
      "epoch:36 step:28844 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.005599]\n",
      "epoch:36 step:28845 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.113226]\n",
      "epoch:36 step:28846 [D loss: 0.005959, acc.: 100.00%] [G loss: 0.189240]\n",
      "epoch:36 step:28847 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.124664]\n",
      "epoch:36 step:28848 [D loss: 0.007358, acc.: 100.00%] [G loss: 0.038155]\n",
      "epoch:36 step:28849 [D loss: 0.000996, acc.: 100.00%] [G loss: 0.006423]\n",
      "epoch:36 step:28850 [D loss: 0.007490, acc.: 100.00%] [G loss: 0.016278]\n",
      "epoch:36 step:28851 [D loss: 0.048883, acc.: 99.22%] [G loss: 1.254230]\n",
      "epoch:36 step:28852 [D loss: 0.010802, acc.: 99.22%] [G loss: 1.088186]\n",
      "epoch:36 step:28853 [D loss: 0.688813, acc.: 73.44%] [G loss: 9.332613]\n",
      "epoch:36 step:28854 [D loss: 1.446803, acc.: 64.06%] [G loss: 6.015162]\n",
      "epoch:36 step:28855 [D loss: 0.149642, acc.: 92.19%] [G loss: 7.168876]\n",
      "epoch:36 step:28856 [D loss: 0.048655, acc.: 98.44%] [G loss: 7.042492]\n",
      "epoch:36 step:28857 [D loss: 0.027815, acc.: 100.00%] [G loss: 5.241301]\n",
      "epoch:36 step:28858 [D loss: 0.094353, acc.: 97.66%] [G loss: 5.280759]\n",
      "epoch:36 step:28859 [D loss: 0.049595, acc.: 99.22%] [G loss: 5.236258]\n",
      "epoch:36 step:28860 [D loss: 0.034697, acc.: 98.44%] [G loss: 6.056218]\n",
      "epoch:36 step:28861 [D loss: 0.085956, acc.: 97.66%] [G loss: 7.007005]\n",
      "epoch:36 step:28862 [D loss: 0.024439, acc.: 99.22%] [G loss: 6.930740]\n",
      "epoch:36 step:28863 [D loss: 0.363244, acc.: 82.81%] [G loss: 5.455337]\n",
      "epoch:36 step:28864 [D loss: 0.027019, acc.: 100.00%] [G loss: 5.735318]\n",
      "epoch:36 step:28865 [D loss: 0.349719, acc.: 83.59%] [G loss: 7.127794]\n",
      "epoch:36 step:28866 [D loss: 0.061501, acc.: 96.88%] [G loss: 7.044504]\n",
      "epoch:36 step:28867 [D loss: 0.307931, acc.: 88.28%] [G loss: 4.689012]\n",
      "epoch:36 step:28868 [D loss: 0.087799, acc.: 96.88%] [G loss: 6.072098]\n",
      "epoch:36 step:28869 [D loss: 0.034738, acc.: 98.44%] [G loss: 5.820971]\n",
      "epoch:36 step:28870 [D loss: 0.085751, acc.: 96.09%] [G loss: 4.916509]\n",
      "epoch:36 step:28871 [D loss: 0.073494, acc.: 97.66%] [G loss: 5.188481]\n",
      "epoch:36 step:28872 [D loss: 0.076755, acc.: 98.44%] [G loss: 5.350287]\n",
      "epoch:36 step:28873 [D loss: 0.060150, acc.: 97.66%] [G loss: 5.310070]\n",
      "epoch:36 step:28874 [D loss: 0.016598, acc.: 100.00%] [G loss: 3.877878]\n",
      "epoch:36 step:28875 [D loss: 0.050684, acc.: 99.22%] [G loss: 5.209859]\n",
      "epoch:36 step:28876 [D loss: 0.056400, acc.: 98.44%] [G loss: 4.364689]\n",
      "epoch:36 step:28877 [D loss: 0.098243, acc.: 98.44%] [G loss: 5.125343]\n",
      "epoch:36 step:28878 [D loss: 0.440164, acc.: 85.16%] [G loss: 7.495814]\n",
      "epoch:36 step:28879 [D loss: 0.202218, acc.: 92.97%] [G loss: 6.765810]\n",
      "epoch:36 step:28880 [D loss: 0.014164, acc.: 100.00%] [G loss: 5.473899]\n",
      "epoch:36 step:28881 [D loss: 0.077680, acc.: 97.66%] [G loss: 6.823354]\n",
      "epoch:36 step:28882 [D loss: 0.070407, acc.: 96.09%] [G loss: 0.456680]\n",
      "epoch:36 step:28883 [D loss: 0.032418, acc.: 98.44%] [G loss: 3.351321]\n",
      "epoch:36 step:28884 [D loss: 0.189818, acc.: 91.41%] [G loss: 6.984213]\n",
      "epoch:36 step:28885 [D loss: 0.013454, acc.: 100.00%] [G loss: 7.875367]\n",
      "epoch:36 step:28886 [D loss: 0.264642, acc.: 90.62%] [G loss: 5.151261]\n",
      "epoch:36 step:28887 [D loss: 0.200776, acc.: 93.75%] [G loss: 3.320179]\n",
      "epoch:36 step:28888 [D loss: 0.017052, acc.: 100.00%] [G loss: 4.231803]\n",
      "epoch:36 step:28889 [D loss: 0.006360, acc.: 100.00%] [G loss: 4.170536]\n",
      "epoch:36 step:28890 [D loss: 0.025202, acc.: 100.00%] [G loss: 0.140412]\n",
      "epoch:36 step:28891 [D loss: 0.040519, acc.: 99.22%] [G loss: 3.113318]\n",
      "epoch:36 step:28892 [D loss: 0.017985, acc.: 99.22%] [G loss: 1.686277]\n",
      "epoch:36 step:28893 [D loss: 0.725061, acc.: 68.75%] [G loss: 8.898203]\n",
      "epoch:36 step:28894 [D loss: 1.128588, acc.: 66.41%] [G loss: 6.455607]\n",
      "epoch:36 step:28895 [D loss: 0.042251, acc.: 98.44%] [G loss: 5.007742]\n",
      "epoch:36 step:28896 [D loss: 0.173507, acc.: 92.19%] [G loss: 4.679208]\n",
      "epoch:36 step:28897 [D loss: 0.004173, acc.: 100.00%] [G loss: 5.666187]\n",
      "epoch:37 step:28898 [D loss: 0.021306, acc.: 98.44%] [G loss: 4.328360]\n",
      "epoch:37 step:28899 [D loss: 0.023245, acc.: 100.00%] [G loss: 3.909635]\n",
      "epoch:37 step:28900 [D loss: 0.073452, acc.: 98.44%] [G loss: 1.973358]\n",
      "epoch:37 step:28901 [D loss: 0.090216, acc.: 98.44%] [G loss: 5.654295]\n",
      "epoch:37 step:28902 [D loss: 0.079132, acc.: 96.88%] [G loss: 2.075418]\n",
      "epoch:37 step:28903 [D loss: 0.107832, acc.: 95.31%] [G loss: 2.731605]\n",
      "epoch:37 step:28904 [D loss: 0.016944, acc.: 100.00%] [G loss: 0.063655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:28905 [D loss: 0.003199, acc.: 100.00%] [G loss: 1.805919]\n",
      "epoch:37 step:28906 [D loss: 0.002690, acc.: 100.00%] [G loss: 0.223755]\n",
      "epoch:37 step:28907 [D loss: 0.016128, acc.: 100.00%] [G loss: 2.056159]\n",
      "epoch:37 step:28908 [D loss: 0.000751, acc.: 100.00%] [G loss: 0.367029]\n",
      "epoch:37 step:28909 [D loss: 0.014620, acc.: 100.00%] [G loss: 0.085361]\n",
      "epoch:37 step:28910 [D loss: 0.001853, acc.: 100.00%] [G loss: 0.169199]\n",
      "epoch:37 step:28911 [D loss: 0.006277, acc.: 100.00%] [G loss: 0.011971]\n",
      "epoch:37 step:28912 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.149442]\n",
      "epoch:37 step:28913 [D loss: 0.002554, acc.: 100.00%] [G loss: 0.015632]\n",
      "epoch:37 step:28914 [D loss: 0.002100, acc.: 100.00%] [G loss: 0.010630]\n",
      "epoch:37 step:28915 [D loss: 0.000849, acc.: 100.00%] [G loss: 0.004193]\n",
      "epoch:37 step:28916 [D loss: 0.000544, acc.: 100.00%] [G loss: 0.098848]\n",
      "epoch:37 step:28917 [D loss: 0.000669, acc.: 100.00%] [G loss: 0.002752]\n",
      "epoch:37 step:28918 [D loss: 0.004402, acc.: 100.00%] [G loss: 0.023422]\n",
      "epoch:37 step:28919 [D loss: 0.011819, acc.: 100.00%] [G loss: 0.017914]\n",
      "epoch:37 step:28920 [D loss: 0.000967, acc.: 100.00%] [G loss: 0.012918]\n",
      "epoch:37 step:28921 [D loss: 0.015292, acc.: 99.22%] [G loss: 0.006355]\n",
      "epoch:37 step:28922 [D loss: 0.003066, acc.: 100.00%] [G loss: 0.024089]\n",
      "epoch:37 step:28923 [D loss: 0.003218, acc.: 100.00%] [G loss: 0.000944]\n",
      "epoch:37 step:28924 [D loss: 0.006370, acc.: 100.00%] [G loss: 0.001485]\n",
      "epoch:37 step:28925 [D loss: 0.001628, acc.: 100.00%] [G loss: 0.009288]\n",
      "epoch:37 step:28926 [D loss: 0.006490, acc.: 100.00%] [G loss: 0.001412]\n",
      "epoch:37 step:28927 [D loss: 0.002302, acc.: 100.00%] [G loss: 0.063813]\n",
      "epoch:37 step:28928 [D loss: 0.001931, acc.: 100.00%] [G loss: 0.001852]\n",
      "epoch:37 step:28929 [D loss: 0.001322, acc.: 100.00%] [G loss: 0.001521]\n",
      "epoch:37 step:28930 [D loss: 0.023163, acc.: 99.22%] [G loss: 0.006195]\n",
      "epoch:37 step:28931 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.145706]\n",
      "epoch:37 step:28932 [D loss: 0.002583, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:37 step:28933 [D loss: 0.008775, acc.: 100.00%] [G loss: 0.009416]\n",
      "epoch:37 step:28934 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.005766]\n",
      "epoch:37 step:28935 [D loss: 0.001207, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:37 step:28936 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.003671]\n",
      "epoch:37 step:28937 [D loss: 0.000877, acc.: 100.00%] [G loss: 0.002014]\n",
      "epoch:37 step:28938 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:37 step:28939 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.000777]\n",
      "epoch:37 step:28940 [D loss: 0.001590, acc.: 100.00%] [G loss: 0.000313]\n",
      "epoch:37 step:28941 [D loss: 0.007660, acc.: 100.00%] [G loss: 0.000735]\n",
      "epoch:37 step:28942 [D loss: 0.025424, acc.: 99.22%] [G loss: 0.005037]\n",
      "epoch:37 step:28943 [D loss: 0.002979, acc.: 100.00%] [G loss: 0.001438]\n",
      "epoch:37 step:28944 [D loss: 0.003959, acc.: 100.00%] [G loss: 0.007853]\n",
      "epoch:37 step:28945 [D loss: 0.001438, acc.: 100.00%] [G loss: 0.001942]\n",
      "epoch:37 step:28946 [D loss: 0.009160, acc.: 100.00%] [G loss: 0.007205]\n",
      "epoch:37 step:28947 [D loss: 0.004025, acc.: 100.00%] [G loss: 0.018212]\n",
      "epoch:37 step:28948 [D loss: 0.000528, acc.: 100.00%] [G loss: 0.001843]\n",
      "epoch:37 step:28949 [D loss: 0.013695, acc.: 100.00%] [G loss: 0.001908]\n",
      "epoch:37 step:28950 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.044300]\n",
      "epoch:37 step:28951 [D loss: 0.003917, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:37 step:28952 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.023100]\n",
      "epoch:37 step:28953 [D loss: 0.000945, acc.: 100.00%] [G loss: 0.001685]\n",
      "epoch:37 step:28954 [D loss: 0.000736, acc.: 100.00%] [G loss: 0.003162]\n",
      "epoch:37 step:28955 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000694]\n",
      "epoch:37 step:28956 [D loss: 0.094994, acc.: 96.88%] [G loss: 2.729353]\n",
      "epoch:37 step:28957 [D loss: 0.004506, acc.: 100.00%] [G loss: 4.494396]\n",
      "epoch:37 step:28958 [D loss: 0.380980, acc.: 85.16%] [G loss: 0.000581]\n",
      "epoch:37 step:28959 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.009344]\n",
      "epoch:37 step:28960 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.022170]\n",
      "epoch:37 step:28961 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:37 step:28962 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.000361]\n",
      "epoch:37 step:28963 [D loss: 0.004254, acc.: 100.00%] [G loss: 0.014256]\n",
      "epoch:37 step:28964 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.006082]\n",
      "epoch:37 step:28965 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000848]\n",
      "epoch:37 step:28966 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:37 step:28967 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:37 step:28968 [D loss: 0.004198, acc.: 100.00%] [G loss: 0.003437]\n",
      "epoch:37 step:28969 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.009040]\n",
      "epoch:37 step:28970 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.011392]\n",
      "epoch:37 step:28971 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.015082]\n",
      "epoch:37 step:28972 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.013228]\n",
      "epoch:37 step:28973 [D loss: 0.001347, acc.: 100.00%] [G loss: 0.010937]\n",
      "epoch:37 step:28974 [D loss: 0.000759, acc.: 100.00%] [G loss: 0.001896]\n",
      "epoch:37 step:28975 [D loss: 0.001474, acc.: 100.00%] [G loss: 0.002048]\n",
      "epoch:37 step:28976 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.009592]\n",
      "epoch:37 step:28977 [D loss: 0.001009, acc.: 100.00%] [G loss: 0.013057]\n",
      "epoch:37 step:28978 [D loss: 0.001074, acc.: 100.00%] [G loss: 0.000690]\n",
      "epoch:37 step:28979 [D loss: 0.000397, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:37 step:28980 [D loss: 0.002792, acc.: 100.00%] [G loss: 0.001841]\n",
      "epoch:37 step:28981 [D loss: 0.010714, acc.: 100.00%] [G loss: 0.003108]\n",
      "epoch:37 step:28982 [D loss: 0.001263, acc.: 100.00%] [G loss: 1.840052]\n",
      "epoch:37 step:28983 [D loss: 1.618892, acc.: 53.91%] [G loss: 10.971679]\n",
      "epoch:37 step:28984 [D loss: 2.019851, acc.: 52.34%] [G loss: 5.994377]\n",
      "epoch:37 step:28985 [D loss: 0.533913, acc.: 74.22%] [G loss: 2.570002]\n",
      "epoch:37 step:28986 [D loss: 0.215859, acc.: 94.53%] [G loss: 2.773401]\n",
      "epoch:37 step:28987 [D loss: 0.026263, acc.: 100.00%] [G loss: 3.719017]\n",
      "epoch:37 step:28988 [D loss: 0.074061, acc.: 98.44%] [G loss: 4.026610]\n",
      "epoch:37 step:28989 [D loss: 0.009393, acc.: 100.00%] [G loss: 4.176118]\n",
      "epoch:37 step:28990 [D loss: 0.037738, acc.: 100.00%] [G loss: 3.351380]\n",
      "epoch:37 step:28991 [D loss: 0.101304, acc.: 98.44%] [G loss: 3.500752]\n",
      "epoch:37 step:28992 [D loss: 0.063838, acc.: 99.22%] [G loss: 4.375939]\n",
      "epoch:37 step:28993 [D loss: 0.096187, acc.: 98.44%] [G loss: 5.287004]\n",
      "epoch:37 step:28994 [D loss: 0.130490, acc.: 92.19%] [G loss: 5.337648]\n",
      "epoch:37 step:28995 [D loss: 0.131230, acc.: 95.31%] [G loss: 2.995631]\n",
      "epoch:37 step:28996 [D loss: 0.282199, acc.: 85.16%] [G loss: 4.899327]\n",
      "epoch:37 step:28997 [D loss: 0.334684, acc.: 86.72%] [G loss: 0.321334]\n",
      "epoch:37 step:28998 [D loss: 0.102056, acc.: 96.09%] [G loss: 1.014966]\n",
      "epoch:37 step:28999 [D loss: 0.163345, acc.: 92.97%] [G loss: 5.677144]\n",
      "epoch:37 step:29000 [D loss: 0.066635, acc.: 97.66%] [G loss: 4.138382]\n",
      "epoch:37 step:29001 [D loss: 0.024436, acc.: 99.22%] [G loss: 3.809386]\n",
      "epoch:37 step:29002 [D loss: 0.003641, acc.: 100.00%] [G loss: 2.007180]\n",
      "epoch:37 step:29003 [D loss: 0.021632, acc.: 100.00%] [G loss: 0.004420]\n",
      "epoch:37 step:29004 [D loss: 0.022074, acc.: 100.00%] [G loss: 1.073838]\n",
      "epoch:37 step:29005 [D loss: 0.049791, acc.: 99.22%] [G loss: 0.907603]\n",
      "epoch:37 step:29006 [D loss: 0.006865, acc.: 100.00%] [G loss: 0.872472]\n",
      "epoch:37 step:29007 [D loss: 0.003687, acc.: 100.00%] [G loss: 0.310428]\n",
      "epoch:37 step:29008 [D loss: 0.105919, acc.: 96.88%] [G loss: 0.220160]\n",
      "epoch:37 step:29009 [D loss: 0.013856, acc.: 99.22%] [G loss: 1.468331]\n",
      "epoch:37 step:29010 [D loss: 0.013494, acc.: 99.22%] [G loss: 1.443255]\n",
      "epoch:37 step:29011 [D loss: 0.041304, acc.: 98.44%] [G loss: 0.531431]\n",
      "epoch:37 step:29012 [D loss: 0.026370, acc.: 100.00%] [G loss: 0.131780]\n",
      "epoch:37 step:29013 [D loss: 0.181151, acc.: 91.41%] [G loss: 4.773358]\n",
      "epoch:37 step:29014 [D loss: 0.103597, acc.: 96.88%] [G loss: 3.213352]\n",
      "epoch:37 step:29015 [D loss: 0.052203, acc.: 98.44%] [G loss: 1.771707]\n",
      "epoch:37 step:29016 [D loss: 0.029043, acc.: 99.22%] [G loss: 1.022207]\n",
      "epoch:37 step:29017 [D loss: 0.106015, acc.: 96.88%] [G loss: 0.767060]\n",
      "epoch:37 step:29018 [D loss: 0.056845, acc.: 98.44%] [G loss: 0.026265]\n",
      "epoch:37 step:29019 [D loss: 0.006367, acc.: 100.00%] [G loss: 1.205243]\n",
      "epoch:37 step:29020 [D loss: 0.120150, acc.: 95.31%] [G loss: 0.310032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29021 [D loss: 0.101720, acc.: 96.88%] [G loss: 1.873925]\n",
      "epoch:37 step:29022 [D loss: 0.024445, acc.: 99.22%] [G loss: 1.466263]\n",
      "epoch:37 step:29023 [D loss: 0.081530, acc.: 96.88%] [G loss: 1.836991]\n",
      "epoch:37 step:29024 [D loss: 0.009445, acc.: 100.00%] [G loss: 1.187951]\n",
      "epoch:37 step:29025 [D loss: 0.026609, acc.: 100.00%] [G loss: 0.900022]\n",
      "epoch:37 step:29026 [D loss: 0.029565, acc.: 99.22%] [G loss: 1.447045]\n",
      "epoch:37 step:29027 [D loss: 0.064711, acc.: 98.44%] [G loss: 0.649186]\n",
      "epoch:37 step:29028 [D loss: 0.099848, acc.: 97.66%] [G loss: 2.078420]\n",
      "epoch:37 step:29029 [D loss: 0.088731, acc.: 95.31%] [G loss: 0.283031]\n",
      "epoch:37 step:29030 [D loss: 0.200406, acc.: 92.19%] [G loss: 3.569901]\n",
      "epoch:37 step:29031 [D loss: 0.021862, acc.: 99.22%] [G loss: 4.689755]\n",
      "epoch:37 step:29032 [D loss: 1.085002, acc.: 59.38%] [G loss: 3.468029]\n",
      "epoch:37 step:29033 [D loss: 0.029957, acc.: 99.22%] [G loss: 5.919231]\n",
      "epoch:37 step:29034 [D loss: 0.152327, acc.: 93.75%] [G loss: 5.184623]\n",
      "epoch:37 step:29035 [D loss: 0.060979, acc.: 96.88%] [G loss: 4.677392]\n",
      "epoch:37 step:29036 [D loss: 0.011903, acc.: 100.00%] [G loss: 0.008388]\n",
      "epoch:37 step:29037 [D loss: 0.054538, acc.: 98.44%] [G loss: 4.598454]\n",
      "epoch:37 step:29038 [D loss: 0.013969, acc.: 100.00%] [G loss: 3.732627]\n",
      "epoch:37 step:29039 [D loss: 0.131098, acc.: 93.75%] [G loss: 0.200099]\n",
      "epoch:37 step:29040 [D loss: 0.035330, acc.: 98.44%] [G loss: 5.080171]\n",
      "epoch:37 step:29041 [D loss: 0.016334, acc.: 99.22%] [G loss: 3.995029]\n",
      "epoch:37 step:29042 [D loss: 0.156934, acc.: 94.53%] [G loss: 2.920462]\n",
      "epoch:37 step:29043 [D loss: 0.023993, acc.: 100.00%] [G loss: 0.920773]\n",
      "epoch:37 step:29044 [D loss: 0.017459, acc.: 100.00%] [G loss: 1.074790]\n",
      "epoch:37 step:29045 [D loss: 0.539069, acc.: 75.00%] [G loss: 6.216758]\n",
      "epoch:37 step:29046 [D loss: 0.508452, acc.: 78.91%] [G loss: 4.072202]\n",
      "epoch:37 step:29047 [D loss: 0.071929, acc.: 97.66%] [G loss: 2.562944]\n",
      "epoch:37 step:29048 [D loss: 0.055012, acc.: 99.22%] [G loss: 1.961443]\n",
      "epoch:37 step:29049 [D loss: 0.047083, acc.: 99.22%] [G loss: 1.411198]\n",
      "epoch:37 step:29050 [D loss: 0.131961, acc.: 95.31%] [G loss: 1.439125]\n",
      "epoch:37 step:29051 [D loss: 0.075355, acc.: 98.44%] [G loss: 2.041677]\n",
      "epoch:37 step:29052 [D loss: 0.047995, acc.: 100.00%] [G loss: 2.346868]\n",
      "epoch:37 step:29053 [D loss: 0.386513, acc.: 85.16%] [G loss: 5.238227]\n",
      "epoch:37 step:29054 [D loss: 0.343408, acc.: 82.03%] [G loss: 3.973667]\n",
      "epoch:37 step:29055 [D loss: 0.195640, acc.: 92.97%] [G loss: 3.331909]\n",
      "epoch:37 step:29056 [D loss: 0.145674, acc.: 92.97%] [G loss: 4.214072]\n",
      "epoch:37 step:29057 [D loss: 0.066714, acc.: 98.44%] [G loss: 4.705724]\n",
      "epoch:37 step:29058 [D loss: 0.044688, acc.: 100.00%] [G loss: 1.667990]\n",
      "epoch:37 step:29059 [D loss: 0.007462, acc.: 100.00%] [G loss: 3.561158]\n",
      "epoch:37 step:29060 [D loss: 0.027651, acc.: 99.22%] [G loss: 0.110809]\n",
      "epoch:37 step:29061 [D loss: 0.012568, acc.: 100.00%] [G loss: 2.143671]\n",
      "epoch:37 step:29062 [D loss: 0.025920, acc.: 100.00%] [G loss: 1.223641]\n",
      "epoch:37 step:29063 [D loss: 0.016228, acc.: 100.00%] [G loss: 1.195568]\n",
      "epoch:37 step:29064 [D loss: 0.027961, acc.: 100.00%] [G loss: 0.039685]\n",
      "epoch:37 step:29065 [D loss: 0.003837, acc.: 100.00%] [G loss: 0.018501]\n",
      "epoch:37 step:29066 [D loss: 0.001799, acc.: 100.00%] [G loss: 0.428636]\n",
      "epoch:37 step:29067 [D loss: 0.017702, acc.: 99.22%] [G loss: 0.342236]\n",
      "epoch:37 step:29068 [D loss: 0.010006, acc.: 99.22%] [G loss: 0.531551]\n",
      "epoch:37 step:29069 [D loss: 0.000932, acc.: 100.00%] [G loss: 0.052761]\n",
      "epoch:37 step:29070 [D loss: 0.002110, acc.: 100.00%] [G loss: 0.042084]\n",
      "epoch:37 step:29071 [D loss: 0.007712, acc.: 100.00%] [G loss: 0.088064]\n",
      "epoch:37 step:29072 [D loss: 0.006448, acc.: 100.00%] [G loss: 0.050902]\n",
      "epoch:37 step:29073 [D loss: 0.010603, acc.: 100.00%] [G loss: 0.011948]\n",
      "epoch:37 step:29074 [D loss: 0.042191, acc.: 100.00%] [G loss: 0.184265]\n",
      "epoch:37 step:29075 [D loss: 0.125818, acc.: 95.31%] [G loss: 0.968754]\n",
      "epoch:37 step:29076 [D loss: 0.069754, acc.: 98.44%] [G loss: 0.061140]\n",
      "epoch:37 step:29077 [D loss: 0.006022, acc.: 100.00%] [G loss: 0.046853]\n",
      "epoch:37 step:29078 [D loss: 0.009109, acc.: 100.00%] [G loss: 0.193308]\n",
      "epoch:37 step:29079 [D loss: 0.018624, acc.: 99.22%] [G loss: 0.220361]\n",
      "epoch:37 step:29080 [D loss: 0.005014, acc.: 100.00%] [G loss: 0.175417]\n",
      "epoch:37 step:29081 [D loss: 0.269572, acc.: 88.28%] [G loss: 6.269460]\n",
      "epoch:37 step:29082 [D loss: 0.094846, acc.: 96.09%] [G loss: 3.779407]\n",
      "epoch:37 step:29083 [D loss: 0.535515, acc.: 81.25%] [G loss: 1.063706]\n",
      "epoch:37 step:29084 [D loss: 0.166472, acc.: 95.31%] [G loss: 1.683160]\n",
      "epoch:37 step:29085 [D loss: 0.000954, acc.: 100.00%] [G loss: 3.560455]\n",
      "epoch:37 step:29086 [D loss: 0.013205, acc.: 99.22%] [G loss: 2.522223]\n",
      "epoch:37 step:29087 [D loss: 0.016793, acc.: 100.00%] [G loss: 1.129279]\n",
      "epoch:37 step:29088 [D loss: 0.179480, acc.: 91.41%] [G loss: 2.487996]\n",
      "epoch:37 step:29089 [D loss: 0.005191, acc.: 100.00%] [G loss: 0.277632]\n",
      "epoch:37 step:29090 [D loss: 0.431102, acc.: 82.03%] [G loss: 0.641413]\n",
      "epoch:37 step:29091 [D loss: 0.005677, acc.: 100.00%] [G loss: 0.242501]\n",
      "epoch:37 step:29092 [D loss: 0.111593, acc.: 95.31%] [G loss: 2.201088]\n",
      "epoch:37 step:29093 [D loss: 0.001949, acc.: 100.00%] [G loss: 1.500546]\n",
      "epoch:37 step:29094 [D loss: 0.023442, acc.: 100.00%] [G loss: 2.186776]\n",
      "epoch:37 step:29095 [D loss: 0.025978, acc.: 98.44%] [G loss: 0.007383]\n",
      "epoch:37 step:29096 [D loss: 0.062244, acc.: 98.44%] [G loss: 0.627895]\n",
      "epoch:37 step:29097 [D loss: 0.024061, acc.: 100.00%] [G loss: 0.568982]\n",
      "epoch:37 step:29098 [D loss: 0.015943, acc.: 100.00%] [G loss: 0.004200]\n",
      "epoch:37 step:29099 [D loss: 0.001486, acc.: 100.00%] [G loss: 0.453794]\n",
      "epoch:37 step:29100 [D loss: 0.001842, acc.: 100.00%] [G loss: 0.539888]\n",
      "epoch:37 step:29101 [D loss: 0.004642, acc.: 100.00%] [G loss: 0.330069]\n",
      "epoch:37 step:29102 [D loss: 0.003566, acc.: 100.00%] [G loss: 0.187967]\n",
      "epoch:37 step:29103 [D loss: 0.011277, acc.: 100.00%] [G loss: 0.427171]\n",
      "epoch:37 step:29104 [D loss: 0.005454, acc.: 100.00%] [G loss: 0.056675]\n",
      "epoch:37 step:29105 [D loss: 0.064142, acc.: 99.22%] [G loss: 0.105689]\n",
      "epoch:37 step:29106 [D loss: 0.113698, acc.: 96.88%] [G loss: 0.172540]\n",
      "epoch:37 step:29107 [D loss: 0.303766, acc.: 89.06%] [G loss: 1.820184]\n",
      "epoch:37 step:29108 [D loss: 0.002863, acc.: 100.00%] [G loss: 0.314883]\n",
      "epoch:37 step:29109 [D loss: 0.064402, acc.: 96.88%] [G loss: 2.572674]\n",
      "epoch:37 step:29110 [D loss: 0.048624, acc.: 97.66%] [G loss: 1.427768]\n",
      "epoch:37 step:29111 [D loss: 0.014296, acc.: 100.00%] [G loss: 0.793116]\n",
      "epoch:37 step:29112 [D loss: 0.000570, acc.: 100.00%] [G loss: 0.000366]\n",
      "epoch:37 step:29113 [D loss: 0.028791, acc.: 98.44%] [G loss: 0.302803]\n",
      "epoch:37 step:29114 [D loss: 0.000860, acc.: 100.00%] [G loss: 0.851881]\n",
      "epoch:37 step:29115 [D loss: 0.002483, acc.: 100.00%] [G loss: 0.230552]\n",
      "epoch:37 step:29116 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.567740]\n",
      "epoch:37 step:29117 [D loss: 0.000923, acc.: 100.00%] [G loss: 0.186912]\n",
      "epoch:37 step:29118 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.720780]\n",
      "epoch:37 step:29119 [D loss: 0.002042, acc.: 100.00%] [G loss: 0.010156]\n",
      "epoch:37 step:29120 [D loss: 0.001184, acc.: 100.00%] [G loss: 0.085187]\n",
      "epoch:37 step:29121 [D loss: 0.004524, acc.: 100.00%] [G loss: 0.002889]\n",
      "epoch:37 step:29122 [D loss: 0.014318, acc.: 100.00%] [G loss: 0.004711]\n",
      "epoch:37 step:29123 [D loss: 0.007222, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:37 step:29124 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.022903]\n",
      "epoch:37 step:29125 [D loss: 0.007374, acc.: 100.00%] [G loss: 0.042006]\n",
      "epoch:37 step:29126 [D loss: 0.050209, acc.: 97.66%] [G loss: 0.001107]\n",
      "epoch:37 step:29127 [D loss: 0.007101, acc.: 100.00%] [G loss: 0.000358]\n",
      "epoch:37 step:29128 [D loss: 0.002488, acc.: 100.00%] [G loss: 0.000781]\n",
      "epoch:37 step:29129 [D loss: 0.001438, acc.: 100.00%] [G loss: 0.003555]\n",
      "epoch:37 step:29130 [D loss: 0.003177, acc.: 100.00%] [G loss: 0.001798]\n",
      "epoch:37 step:29131 [D loss: 0.021268, acc.: 99.22%] [G loss: 0.007440]\n",
      "epoch:37 step:29132 [D loss: 0.001740, acc.: 100.00%] [G loss: 0.010678]\n",
      "epoch:37 step:29133 [D loss: 0.022658, acc.: 99.22%] [G loss: 0.001948]\n",
      "epoch:37 step:29134 [D loss: 0.021062, acc.: 100.00%] [G loss: 0.018075]\n",
      "epoch:37 step:29135 [D loss: 0.008150, acc.: 100.00%] [G loss: 0.015972]\n",
      "epoch:37 step:29136 [D loss: 0.004817, acc.: 100.00%] [G loss: 0.056808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29137 [D loss: 0.038599, acc.: 99.22%] [G loss: 0.000046]\n",
      "epoch:37 step:29138 [D loss: 0.001256, acc.: 100.00%] [G loss: 0.107794]\n",
      "epoch:37 step:29139 [D loss: 0.005456, acc.: 100.00%] [G loss: 0.086827]\n",
      "epoch:37 step:29140 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.094258]\n",
      "epoch:37 step:29141 [D loss: 0.001241, acc.: 100.00%] [G loss: 0.006620]\n",
      "epoch:37 step:29142 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.008406]\n",
      "epoch:37 step:29143 [D loss: 0.001764, acc.: 100.00%] [G loss: 0.005221]\n",
      "epoch:37 step:29144 [D loss: 0.007425, acc.: 100.00%] [G loss: 0.005973]\n",
      "epoch:37 step:29145 [D loss: 0.000548, acc.: 100.00%] [G loss: 0.004598]\n",
      "epoch:37 step:29146 [D loss: 0.017125, acc.: 99.22%] [G loss: 0.001472]\n",
      "epoch:37 step:29147 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.004508]\n",
      "epoch:37 step:29148 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:37 step:29149 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.003497]\n",
      "epoch:37 step:29150 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000602]\n",
      "epoch:37 step:29151 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000748]\n",
      "epoch:37 step:29152 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000978]\n",
      "epoch:37 step:29153 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.004641]\n",
      "epoch:37 step:29154 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.003023]\n",
      "epoch:37 step:29155 [D loss: 0.012619, acc.: 100.00%] [G loss: 0.006810]\n",
      "epoch:37 step:29156 [D loss: 0.031866, acc.: 100.00%] [G loss: 0.006759]\n",
      "epoch:37 step:29157 [D loss: 0.003566, acc.: 100.00%] [G loss: 0.030976]\n",
      "epoch:37 step:29158 [D loss: 0.024989, acc.: 99.22%] [G loss: 0.165148]\n",
      "epoch:37 step:29159 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.268057]\n",
      "epoch:37 step:29160 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.059177]\n",
      "epoch:37 step:29161 [D loss: 0.002388, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:37 step:29162 [D loss: 0.000633, acc.: 100.00%] [G loss: 0.003231]\n",
      "epoch:37 step:29163 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:37 step:29164 [D loss: 0.000921, acc.: 100.00%] [G loss: 0.109290]\n",
      "epoch:37 step:29165 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.029623]\n",
      "epoch:37 step:29166 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.000504]\n",
      "epoch:37 step:29167 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.006897]\n",
      "epoch:37 step:29168 [D loss: 0.000709, acc.: 100.00%] [G loss: 0.009567]\n",
      "epoch:37 step:29169 [D loss: 0.000848, acc.: 100.00%] [G loss: 0.002003]\n",
      "epoch:37 step:29170 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.023774]\n",
      "epoch:37 step:29171 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.002740]\n",
      "epoch:37 step:29172 [D loss: 0.000957, acc.: 100.00%] [G loss: 0.021245]\n",
      "epoch:37 step:29173 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.005776]\n",
      "epoch:37 step:29174 [D loss: 0.021112, acc.: 99.22%] [G loss: 0.000557]\n",
      "epoch:37 step:29175 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:37 step:29176 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000522]\n",
      "epoch:37 step:29177 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.001227]\n",
      "epoch:37 step:29178 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.005989]\n",
      "epoch:37 step:29179 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:37 step:29180 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:37 step:29181 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000688]\n",
      "epoch:37 step:29182 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000395]\n",
      "epoch:37 step:29183 [D loss: 0.001404, acc.: 100.00%] [G loss: 1.162448]\n",
      "epoch:37 step:29184 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.000528]\n",
      "epoch:37 step:29185 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.002465]\n",
      "epoch:37 step:29186 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.003983]\n",
      "epoch:37 step:29187 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.003772]\n",
      "epoch:37 step:29188 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.005289]\n",
      "epoch:37 step:29189 [D loss: 0.003820, acc.: 100.00%] [G loss: 0.005185]\n",
      "epoch:37 step:29190 [D loss: 0.006579, acc.: 100.00%] [G loss: 0.004861]\n",
      "epoch:37 step:29191 [D loss: 0.001924, acc.: 100.00%] [G loss: 0.005269]\n",
      "epoch:37 step:29192 [D loss: 0.011897, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:37 step:29193 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:37 step:29194 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.005513]\n",
      "epoch:37 step:29195 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.029499]\n",
      "epoch:37 step:29196 [D loss: 0.004528, acc.: 100.00%] [G loss: 0.047042]\n",
      "epoch:37 step:29197 [D loss: 0.011638, acc.: 100.00%] [G loss: 0.125898]\n",
      "epoch:37 step:29198 [D loss: 0.195202, acc.: 94.53%] [G loss: 6.489171]\n",
      "epoch:37 step:29199 [D loss: 0.085721, acc.: 96.09%] [G loss: 4.452223]\n",
      "epoch:37 step:29200 [D loss: 0.035898, acc.: 98.44%] [G loss: 0.045383]\n",
      "epoch:37 step:29201 [D loss: 0.121427, acc.: 96.09%] [G loss: 0.696894]\n",
      "epoch:37 step:29202 [D loss: 0.002667, acc.: 100.00%] [G loss: 0.990924]\n",
      "epoch:37 step:29203 [D loss: 0.004279, acc.: 100.00%] [G loss: 0.838296]\n",
      "epoch:37 step:29204 [D loss: 0.015720, acc.: 99.22%] [G loss: 0.098586]\n",
      "epoch:37 step:29205 [D loss: 0.027807, acc.: 99.22%] [G loss: 1.228749]\n",
      "epoch:37 step:29206 [D loss: 0.140320, acc.: 95.31%] [G loss: 4.749427]\n",
      "epoch:37 step:29207 [D loss: 0.226384, acc.: 89.84%] [G loss: 3.178100]\n",
      "epoch:37 step:29208 [D loss: 0.084251, acc.: 96.88%] [G loss: 0.586946]\n",
      "epoch:37 step:29209 [D loss: 0.444243, acc.: 79.69%] [G loss: 6.548443]\n",
      "epoch:37 step:29210 [D loss: 1.422713, acc.: 52.34%] [G loss: 3.102859]\n",
      "epoch:37 step:29211 [D loss: 0.075409, acc.: 96.88%] [G loss: 4.711002]\n",
      "epoch:37 step:29212 [D loss: 0.042213, acc.: 98.44%] [G loss: 5.057317]\n",
      "epoch:37 step:29213 [D loss: 0.010015, acc.: 100.00%] [G loss: 4.344930]\n",
      "epoch:37 step:29214 [D loss: 0.074568, acc.: 99.22%] [G loss: 1.367846]\n",
      "epoch:37 step:29215 [D loss: 0.016372, acc.: 100.00%] [G loss: 0.182205]\n",
      "epoch:37 step:29216 [D loss: 0.009635, acc.: 100.00%] [G loss: 6.082492]\n",
      "epoch:37 step:29217 [D loss: 0.063275, acc.: 97.66%] [G loss: 4.786951]\n",
      "epoch:37 step:29218 [D loss: 0.001312, acc.: 100.00%] [G loss: 0.006680]\n",
      "epoch:37 step:29219 [D loss: 0.015594, acc.: 100.00%] [G loss: 1.466050]\n",
      "epoch:37 step:29220 [D loss: 0.006864, acc.: 100.00%] [G loss: 0.002893]\n",
      "epoch:37 step:29221 [D loss: 0.033115, acc.: 99.22%] [G loss: 0.813634]\n",
      "epoch:37 step:29222 [D loss: 0.008076, acc.: 100.00%] [G loss: 0.034880]\n",
      "epoch:37 step:29223 [D loss: 0.006460, acc.: 100.00%] [G loss: 0.375511]\n",
      "epoch:37 step:29224 [D loss: 0.001830, acc.: 100.00%] [G loss: 0.255517]\n",
      "epoch:37 step:29225 [D loss: 0.005758, acc.: 100.00%] [G loss: 0.125115]\n",
      "epoch:37 step:29226 [D loss: 0.002915, acc.: 100.00%] [G loss: 0.068745]\n",
      "epoch:37 step:29227 [D loss: 0.007928, acc.: 100.00%] [G loss: 0.039950]\n",
      "epoch:37 step:29228 [D loss: 0.239513, acc.: 88.28%] [G loss: 3.091292]\n",
      "epoch:37 step:29229 [D loss: 0.077316, acc.: 97.66%] [G loss: 4.186491]\n",
      "epoch:37 step:29230 [D loss: 0.103012, acc.: 95.31%] [G loss: 1.187382]\n",
      "epoch:37 step:29231 [D loss: 0.009674, acc.: 100.00%] [G loss: 0.722997]\n",
      "epoch:37 step:29232 [D loss: 0.004763, acc.: 100.00%] [G loss: 0.258772]\n",
      "epoch:37 step:29233 [D loss: 0.023161, acc.: 100.00%] [G loss: 0.186339]\n",
      "epoch:37 step:29234 [D loss: 0.015883, acc.: 100.00%] [G loss: 0.320244]\n",
      "epoch:37 step:29235 [D loss: 0.051813, acc.: 100.00%] [G loss: 0.391225]\n",
      "epoch:37 step:29236 [D loss: 0.127644, acc.: 96.88%] [G loss: 1.726762]\n",
      "epoch:37 step:29237 [D loss: 0.243522, acc.: 89.84%] [G loss: 1.013812]\n",
      "epoch:37 step:29238 [D loss: 0.001792, acc.: 100.00%] [G loss: 0.391801]\n",
      "epoch:37 step:29239 [D loss: 0.005805, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:37 step:29240 [D loss: 0.000755, acc.: 100.00%] [G loss: 0.839865]\n",
      "epoch:37 step:29241 [D loss: 0.000539, acc.: 100.00%] [G loss: 0.001248]\n",
      "epoch:37 step:29242 [D loss: 0.001144, acc.: 100.00%] [G loss: 0.673784]\n",
      "epoch:37 step:29243 [D loss: 0.000599, acc.: 100.00%] [G loss: 0.050303]\n",
      "epoch:37 step:29244 [D loss: 0.004908, acc.: 100.00%] [G loss: 0.067878]\n",
      "epoch:37 step:29245 [D loss: 0.002506, acc.: 100.00%] [G loss: 0.015618]\n",
      "epoch:37 step:29246 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.032590]\n",
      "epoch:37 step:29247 [D loss: 0.009066, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:37 step:29248 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.013064]\n",
      "epoch:37 step:29249 [D loss: 0.000605, acc.: 100.00%] [G loss: 0.010736]\n",
      "epoch:37 step:29250 [D loss: 0.007043, acc.: 100.00%] [G loss: 0.042351]\n",
      "epoch:37 step:29251 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.041258]\n",
      "epoch:37 step:29252 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29253 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.005019]\n",
      "epoch:37 step:29254 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001243]\n",
      "epoch:37 step:29255 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.024108]\n",
      "epoch:37 step:29256 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.047101]\n",
      "epoch:37 step:29257 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:37 step:29258 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.093172]\n",
      "epoch:37 step:29259 [D loss: 0.000477, acc.: 100.00%] [G loss: 0.015478]\n",
      "epoch:37 step:29260 [D loss: 0.001012, acc.: 100.00%] [G loss: 0.002162]\n",
      "epoch:37 step:29261 [D loss: 0.001530, acc.: 100.00%] [G loss: 0.001490]\n",
      "epoch:37 step:29262 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.016381]\n",
      "epoch:37 step:29263 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.181700]\n",
      "epoch:37 step:29264 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.033931]\n",
      "epoch:37 step:29265 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.045379]\n",
      "epoch:37 step:29266 [D loss: 0.001018, acc.: 100.00%] [G loss: 0.018578]\n",
      "epoch:37 step:29267 [D loss: 0.006846, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:37 step:29268 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.007944]\n",
      "epoch:37 step:29269 [D loss: 0.001450, acc.: 100.00%] [G loss: 0.016328]\n",
      "epoch:37 step:29270 [D loss: 0.006239, acc.: 100.00%] [G loss: 0.017451]\n",
      "epoch:37 step:29271 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.012397]\n",
      "epoch:37 step:29272 [D loss: 0.091890, acc.: 97.66%] [G loss: 0.442704]\n",
      "epoch:37 step:29273 [D loss: 0.014181, acc.: 100.00%] [G loss: 2.291374]\n",
      "epoch:37 step:29274 [D loss: 0.041783, acc.: 97.66%] [G loss: 0.701688]\n",
      "epoch:37 step:29275 [D loss: 0.013825, acc.: 100.00%] [G loss: 0.724141]\n",
      "epoch:37 step:29276 [D loss: 0.221453, acc.: 90.62%] [G loss: 5.735463]\n",
      "epoch:37 step:29277 [D loss: 0.845409, acc.: 67.97%] [G loss: 0.000034]\n",
      "epoch:37 step:29278 [D loss: 0.000046, acc.: 100.00%] [G loss: 1.388276]\n",
      "epoch:37 step:29279 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:37 step:29280 [D loss: 0.000027, acc.: 100.00%] [G loss: 1.252280]\n",
      "epoch:37 step:29281 [D loss: 0.000036, acc.: 100.00%] [G loss: 2.140350]\n",
      "epoch:37 step:29282 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:37 step:29283 [D loss: 0.002389, acc.: 100.00%] [G loss: 0.162881]\n",
      "epoch:37 step:29284 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001227]\n",
      "epoch:37 step:29285 [D loss: 0.008451, acc.: 100.00%] [G loss: 0.153704]\n",
      "epoch:37 step:29286 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.097227]\n",
      "epoch:37 step:29287 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.031053]\n",
      "epoch:37 step:29288 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.552034]\n",
      "epoch:37 step:29289 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:37 step:29290 [D loss: 0.011028, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:37 step:29291 [D loss: 0.160015, acc.: 93.75%] [G loss: 0.711577]\n",
      "epoch:37 step:29292 [D loss: 0.005967, acc.: 100.00%] [G loss: 0.706957]\n",
      "epoch:37 step:29293 [D loss: 0.051112, acc.: 98.44%] [G loss: 0.316217]\n",
      "epoch:37 step:29294 [D loss: 0.020840, acc.: 99.22%] [G loss: 0.042606]\n",
      "epoch:37 step:29295 [D loss: 0.008756, acc.: 100.00%] [G loss: 0.009963]\n",
      "epoch:37 step:29296 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.003567]\n",
      "epoch:37 step:29297 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.017652]\n",
      "epoch:37 step:29298 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.005906]\n",
      "epoch:37 step:29299 [D loss: 0.001015, acc.: 100.00%] [G loss: 0.002482]\n",
      "epoch:37 step:29300 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.003557]\n",
      "epoch:37 step:29301 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.001521]\n",
      "epoch:37 step:29302 [D loss: 0.002520, acc.: 100.00%] [G loss: 0.099746]\n",
      "epoch:37 step:29303 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.002749]\n",
      "epoch:37 step:29304 [D loss: 0.003186, acc.: 100.00%] [G loss: 0.001003]\n",
      "epoch:37 step:29305 [D loss: 0.001934, acc.: 100.00%] [G loss: 0.002134]\n",
      "epoch:37 step:29306 [D loss: 0.060256, acc.: 98.44%] [G loss: 0.038423]\n",
      "epoch:37 step:29307 [D loss: 0.002463, acc.: 100.00%] [G loss: 0.181238]\n",
      "epoch:37 step:29308 [D loss: 0.105952, acc.: 96.09%] [G loss: 0.008004]\n",
      "epoch:37 step:29309 [D loss: 0.002442, acc.: 100.00%] [G loss: 0.132924]\n",
      "epoch:37 step:29310 [D loss: 0.012901, acc.: 100.00%] [G loss: 0.004240]\n",
      "epoch:37 step:29311 [D loss: 0.015966, acc.: 100.00%] [G loss: 0.004251]\n",
      "epoch:37 step:29312 [D loss: 0.000539, acc.: 100.00%] [G loss: 0.052324]\n",
      "epoch:37 step:29313 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.096633]\n",
      "epoch:37 step:29314 [D loss: 0.002487, acc.: 100.00%] [G loss: 0.042241]\n",
      "epoch:37 step:29315 [D loss: 0.008410, acc.: 100.00%] [G loss: 0.159256]\n",
      "epoch:37 step:29316 [D loss: 0.153195, acc.: 93.75%] [G loss: 0.976418]\n",
      "epoch:37 step:29317 [D loss: 0.210803, acc.: 90.62%] [G loss: 3.516571]\n",
      "epoch:37 step:29318 [D loss: 0.041709, acc.: 99.22%] [G loss: 1.515580]\n",
      "epoch:37 step:29319 [D loss: 0.022559, acc.: 99.22%] [G loss: 0.809009]\n",
      "epoch:37 step:29320 [D loss: 0.009500, acc.: 99.22%] [G loss: 0.133668]\n",
      "epoch:37 step:29321 [D loss: 0.000498, acc.: 100.00%] [G loss: 0.006055]\n",
      "epoch:37 step:29322 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.004512]\n",
      "epoch:37 step:29323 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.042824]\n",
      "epoch:37 step:29324 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.117841]\n",
      "epoch:37 step:29325 [D loss: 0.003047, acc.: 100.00%] [G loss: 0.046120]\n",
      "epoch:37 step:29326 [D loss: 0.002626, acc.: 100.00%] [G loss: 0.012623]\n",
      "epoch:37 step:29327 [D loss: 0.013535, acc.: 99.22%] [G loss: 0.039151]\n",
      "epoch:37 step:29328 [D loss: 0.004374, acc.: 100.00%] [G loss: 0.001605]\n",
      "epoch:37 step:29329 [D loss: 0.001579, acc.: 100.00%] [G loss: 0.026942]\n",
      "epoch:37 step:29330 [D loss: 0.003951, acc.: 100.00%] [G loss: 0.016006]\n",
      "epoch:37 step:29331 [D loss: 0.075422, acc.: 99.22%] [G loss: 0.319717]\n",
      "epoch:37 step:29332 [D loss: 0.000237, acc.: 100.00%] [G loss: 1.726241]\n",
      "epoch:37 step:29333 [D loss: 0.011670, acc.: 100.00%] [G loss: 0.015546]\n",
      "epoch:37 step:29334 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.013455]\n",
      "epoch:37 step:29335 [D loss: 0.004895, acc.: 100.00%] [G loss: 1.115496]\n",
      "epoch:37 step:29336 [D loss: 0.001316, acc.: 100.00%] [G loss: 0.586548]\n",
      "epoch:37 step:29337 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.057844]\n",
      "epoch:37 step:29338 [D loss: 0.003643, acc.: 100.00%] [G loss: 0.019728]\n",
      "epoch:37 step:29339 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.125978]\n",
      "epoch:37 step:29340 [D loss: 0.001597, acc.: 100.00%] [G loss: 0.212498]\n",
      "epoch:37 step:29341 [D loss: 0.000923, acc.: 100.00%] [G loss: 0.005776]\n",
      "epoch:37 step:29342 [D loss: 0.014559, acc.: 99.22%] [G loss: 0.004597]\n",
      "epoch:37 step:29343 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.004905]\n",
      "epoch:37 step:29344 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.009372]\n",
      "epoch:37 step:29345 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.001354]\n",
      "epoch:37 step:29346 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.019947]\n",
      "epoch:37 step:29347 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.007695]\n",
      "epoch:37 step:29348 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.001197]\n",
      "epoch:37 step:29349 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000792]\n",
      "epoch:37 step:29350 [D loss: 0.000633, acc.: 100.00%] [G loss: 0.008524]\n",
      "epoch:37 step:29351 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.026735]\n",
      "epoch:37 step:29352 [D loss: 0.000861, acc.: 100.00%] [G loss: 0.007283]\n",
      "epoch:37 step:29353 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.001331]\n",
      "epoch:37 step:29354 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001408]\n",
      "epoch:37 step:29355 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000678]\n",
      "epoch:37 step:29356 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.001278]\n",
      "epoch:37 step:29357 [D loss: 0.001844, acc.: 100.00%] [G loss: 0.001459]\n",
      "epoch:37 step:29358 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001864]\n",
      "epoch:37 step:29359 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.001138]\n",
      "epoch:37 step:29360 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000864]\n",
      "epoch:37 step:29361 [D loss: 0.000683, acc.: 100.00%] [G loss: 0.006010]\n",
      "epoch:37 step:29362 [D loss: 0.001500, acc.: 100.00%] [G loss: 0.001256]\n",
      "epoch:37 step:29363 [D loss: 0.001735, acc.: 100.00%] [G loss: 0.013086]\n",
      "epoch:37 step:29364 [D loss: 0.018367, acc.: 100.00%] [G loss: 0.004769]\n",
      "epoch:37 step:29365 [D loss: 0.009095, acc.: 100.00%] [G loss: 0.259496]\n",
      "epoch:37 step:29366 [D loss: 0.005571, acc.: 100.00%] [G loss: 0.079430]\n",
      "epoch:37 step:29367 [D loss: 0.004057, acc.: 100.00%] [G loss: 0.115144]\n",
      "epoch:37 step:29368 [D loss: 0.021460, acc.: 100.00%] [G loss: 0.130292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29369 [D loss: 0.162233, acc.: 97.66%] [G loss: 7.127735]\n",
      "epoch:37 step:29370 [D loss: 1.036891, acc.: 60.94%] [G loss: 4.424320]\n",
      "epoch:37 step:29371 [D loss: 0.083340, acc.: 96.88%] [G loss: 0.001026]\n",
      "epoch:37 step:29372 [D loss: 0.000328, acc.: 100.00%] [G loss: 8.237358]\n",
      "epoch:37 step:29373 [D loss: 0.000766, acc.: 100.00%] [G loss: 6.756749]\n",
      "epoch:37 step:29374 [D loss: 0.000855, acc.: 100.00%] [G loss: 6.374015]\n",
      "epoch:37 step:29375 [D loss: 0.002525, acc.: 100.00%] [G loss: 4.651693]\n",
      "epoch:37 step:29376 [D loss: 0.024307, acc.: 100.00%] [G loss: 3.747006]\n",
      "epoch:37 step:29377 [D loss: 0.013911, acc.: 100.00%] [G loss: 2.584846]\n",
      "epoch:37 step:29378 [D loss: 0.048224, acc.: 100.00%] [G loss: 2.746606]\n",
      "epoch:37 step:29379 [D loss: 0.026805, acc.: 100.00%] [G loss: 2.507991]\n",
      "epoch:37 step:29380 [D loss: 0.018895, acc.: 100.00%] [G loss: 0.101548]\n",
      "epoch:37 step:29381 [D loss: 0.000920, acc.: 100.00%] [G loss: 1.741040]\n",
      "epoch:37 step:29382 [D loss: 0.061134, acc.: 96.88%] [G loss: 0.690103]\n",
      "epoch:37 step:29383 [D loss: 0.002115, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:37 step:29384 [D loss: 0.007261, acc.: 100.00%] [G loss: 0.213902]\n",
      "epoch:37 step:29385 [D loss: 0.003659, acc.: 100.00%] [G loss: 0.036963]\n",
      "epoch:37 step:29386 [D loss: 0.000966, acc.: 100.00%] [G loss: 0.176233]\n",
      "epoch:37 step:29387 [D loss: 0.000805, acc.: 100.00%] [G loss: 0.061819]\n",
      "epoch:37 step:29388 [D loss: 0.013544, acc.: 99.22%] [G loss: 0.027152]\n",
      "epoch:37 step:29389 [D loss: 0.003199, acc.: 100.00%] [G loss: 0.013652]\n",
      "epoch:37 step:29390 [D loss: 0.130707, acc.: 97.66%] [G loss: 1.553185]\n",
      "epoch:37 step:29391 [D loss: 0.002078, acc.: 100.00%] [G loss: 2.680343]\n",
      "epoch:37 step:29392 [D loss: 0.115003, acc.: 95.31%] [G loss: 0.660662]\n",
      "epoch:37 step:29393 [D loss: 0.227363, acc.: 89.06%] [G loss: 5.110255]\n",
      "epoch:37 step:29394 [D loss: 0.199413, acc.: 92.97%] [G loss: 3.468366]\n",
      "epoch:37 step:29395 [D loss: 0.079832, acc.: 96.88%] [G loss: 5.762630]\n",
      "epoch:37 step:29396 [D loss: 0.207155, acc.: 88.28%] [G loss: 5.104370]\n",
      "epoch:37 step:29397 [D loss: 0.007649, acc.: 100.00%] [G loss: 6.426849]\n",
      "epoch:37 step:29398 [D loss: 0.436813, acc.: 81.25%] [G loss: 3.545199]\n",
      "epoch:37 step:29399 [D loss: 0.024524, acc.: 100.00%] [G loss: 3.459183]\n",
      "epoch:37 step:29400 [D loss: 0.042528, acc.: 100.00%] [G loss: 4.474433]\n",
      "epoch:37 step:29401 [D loss: 0.023052, acc.: 100.00%] [G loss: 4.057344]\n",
      "epoch:37 step:29402 [D loss: 0.084632, acc.: 99.22%] [G loss: 5.007301]\n",
      "epoch:37 step:29403 [D loss: 0.036952, acc.: 100.00%] [G loss: 4.552112]\n",
      "epoch:37 step:29404 [D loss: 0.274927, acc.: 92.19%] [G loss: 7.132642]\n",
      "epoch:37 step:29405 [D loss: 0.152328, acc.: 92.19%] [G loss: 6.660480]\n",
      "epoch:37 step:29406 [D loss: 0.190979, acc.: 93.75%] [G loss: 6.605235]\n",
      "epoch:37 step:29407 [D loss: 0.005470, acc.: 100.00%] [G loss: 7.905853]\n",
      "epoch:37 step:29408 [D loss: 0.060061, acc.: 98.44%] [G loss: 6.771894]\n",
      "epoch:37 step:29409 [D loss: 0.005684, acc.: 100.00%] [G loss: 5.119392]\n",
      "epoch:37 step:29410 [D loss: 0.113125, acc.: 96.09%] [G loss: 6.898588]\n",
      "epoch:37 step:29411 [D loss: 0.005464, acc.: 100.00%] [G loss: 8.055363]\n",
      "epoch:37 step:29412 [D loss: 0.230929, acc.: 89.06%] [G loss: 4.267271]\n",
      "epoch:37 step:29413 [D loss: 0.139782, acc.: 94.53%] [G loss: 6.541385]\n",
      "epoch:37 step:29414 [D loss: 0.026414, acc.: 97.66%] [G loss: 8.725933]\n",
      "epoch:37 step:29415 [D loss: 0.048021, acc.: 98.44%] [G loss: 5.996448]\n",
      "epoch:37 step:29416 [D loss: 0.015738, acc.: 100.00%] [G loss: 4.692042]\n",
      "epoch:37 step:29417 [D loss: 0.023274, acc.: 100.00%] [G loss: 4.363508]\n",
      "epoch:37 step:29418 [D loss: 0.024333, acc.: 100.00%] [G loss: 5.415577]\n",
      "epoch:37 step:29419 [D loss: 0.030026, acc.: 99.22%] [G loss: 2.968394]\n",
      "epoch:37 step:29420 [D loss: 0.175731, acc.: 90.62%] [G loss: 7.330572]\n",
      "epoch:37 step:29421 [D loss: 0.627928, acc.: 75.00%] [G loss: 6.547124]\n",
      "epoch:37 step:29422 [D loss: 0.008388, acc.: 100.00%] [G loss: 0.763095]\n",
      "epoch:37 step:29423 [D loss: 0.476549, acc.: 80.47%] [G loss: 0.936929]\n",
      "epoch:37 step:29424 [D loss: 0.121611, acc.: 95.31%] [G loss: 0.720507]\n",
      "epoch:37 step:29425 [D loss: 0.001125, acc.: 100.00%] [G loss: 1.653500]\n",
      "epoch:37 step:29426 [D loss: 0.009043, acc.: 100.00%] [G loss: 1.545904]\n",
      "epoch:37 step:29427 [D loss: 0.183731, acc.: 90.62%] [G loss: 4.625168]\n",
      "epoch:37 step:29428 [D loss: 0.002285, acc.: 100.00%] [G loss: 7.081988]\n",
      "epoch:37 step:29429 [D loss: 0.155354, acc.: 96.09%] [G loss: 2.920221]\n",
      "epoch:37 step:29430 [D loss: 0.032850, acc.: 98.44%] [G loss: 3.686595]\n",
      "epoch:37 step:29431 [D loss: 0.039605, acc.: 99.22%] [G loss: 3.037342]\n",
      "epoch:37 step:29432 [D loss: 0.060772, acc.: 97.66%] [G loss: 2.848086]\n",
      "epoch:37 step:29433 [D loss: 0.003408, acc.: 100.00%] [G loss: 0.018134]\n",
      "epoch:37 step:29434 [D loss: 0.067364, acc.: 97.66%] [G loss: 1.108414]\n",
      "epoch:37 step:29435 [D loss: 0.010768, acc.: 99.22%] [G loss: 0.002013]\n",
      "epoch:37 step:29436 [D loss: 0.001365, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:37 step:29437 [D loss: 0.007016, acc.: 100.00%] [G loss: 0.957580]\n",
      "epoch:37 step:29438 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.131511]\n",
      "epoch:37 step:29439 [D loss: 0.013367, acc.: 99.22%] [G loss: 0.235568]\n",
      "epoch:37 step:29440 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.023183]\n",
      "epoch:37 step:29441 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.085652]\n",
      "epoch:37 step:29442 [D loss: 0.000458, acc.: 100.00%] [G loss: 0.054533]\n",
      "epoch:37 step:29443 [D loss: 0.000431, acc.: 100.00%] [G loss: 0.077401]\n",
      "epoch:37 step:29444 [D loss: 0.002723, acc.: 100.00%] [G loss: 0.122733]\n",
      "epoch:37 step:29445 [D loss: 0.003065, acc.: 100.00%] [G loss: 0.003507]\n",
      "epoch:37 step:29446 [D loss: 0.006962, acc.: 100.00%] [G loss: 0.003856]\n",
      "epoch:37 step:29447 [D loss: 0.202740, acc.: 91.41%] [G loss: 5.870958]\n",
      "epoch:37 step:29448 [D loss: 0.037163, acc.: 99.22%] [G loss: 5.899447]\n",
      "epoch:37 step:29449 [D loss: 0.094664, acc.: 94.53%] [G loss: 3.839104]\n",
      "epoch:37 step:29450 [D loss: 0.001187, acc.: 100.00%] [G loss: 0.821928]\n",
      "epoch:37 step:29451 [D loss: 0.093446, acc.: 97.66%] [G loss: 3.836398]\n",
      "epoch:37 step:29452 [D loss: 0.005719, acc.: 100.00%] [G loss: 3.873414]\n",
      "epoch:37 step:29453 [D loss: 0.036918, acc.: 97.66%] [G loss: 1.582017]\n",
      "epoch:37 step:29454 [D loss: 0.065353, acc.: 97.66%] [G loss: 2.168617]\n",
      "epoch:37 step:29455 [D loss: 0.027767, acc.: 99.22%] [G loss: 2.229874]\n",
      "epoch:37 step:29456 [D loss: 0.050959, acc.: 99.22%] [G loss: 1.946943]\n",
      "epoch:37 step:29457 [D loss: 0.033075, acc.: 99.22%] [G loss: 7.215807]\n",
      "epoch:37 step:29458 [D loss: 0.184426, acc.: 91.41%] [G loss: 4.509702]\n",
      "epoch:37 step:29459 [D loss: 0.605974, acc.: 75.78%] [G loss: 0.241512]\n",
      "epoch:37 step:29460 [D loss: 1.666128, acc.: 53.91%] [G loss: 11.203009]\n",
      "epoch:37 step:29461 [D loss: 2.770419, acc.: 51.56%] [G loss: 6.894668]\n",
      "epoch:37 step:29462 [D loss: 0.130793, acc.: 94.53%] [G loss: 5.472704]\n",
      "epoch:37 step:29463 [D loss: 0.157079, acc.: 93.75%] [G loss: 2.996097]\n",
      "epoch:37 step:29464 [D loss: 0.086680, acc.: 98.44%] [G loss: 1.941231]\n",
      "epoch:37 step:29465 [D loss: 0.817982, acc.: 68.75%] [G loss: 6.742319]\n",
      "epoch:37 step:29466 [D loss: 0.184401, acc.: 88.28%] [G loss: 7.193833]\n",
      "epoch:37 step:29467 [D loss: 0.801290, acc.: 68.75%] [G loss: 4.709442]\n",
      "epoch:37 step:29468 [D loss: 0.039630, acc.: 100.00%] [G loss: 3.250294]\n",
      "epoch:37 step:29469 [D loss: 0.132481, acc.: 96.09%] [G loss: 3.927197]\n",
      "epoch:37 step:29470 [D loss: 0.032482, acc.: 100.00%] [G loss: 4.125723]\n",
      "epoch:37 step:29471 [D loss: 0.045605, acc.: 99.22%] [G loss: 3.709072]\n",
      "epoch:37 step:29472 [D loss: 0.059810, acc.: 99.22%] [G loss: 3.253117]\n",
      "epoch:37 step:29473 [D loss: 0.123232, acc.: 96.88%] [G loss: 0.100574]\n",
      "epoch:37 step:29474 [D loss: 0.295066, acc.: 85.16%] [G loss: 2.151510]\n",
      "epoch:37 step:29475 [D loss: 0.018704, acc.: 100.00%] [G loss: 1.286503]\n",
      "epoch:37 step:29476 [D loss: 0.108629, acc.: 96.09%] [G loss: 1.491808]\n",
      "epoch:37 step:29477 [D loss: 0.015830, acc.: 100.00%] [G loss: 1.268507]\n",
      "epoch:37 step:29478 [D loss: 0.137767, acc.: 93.75%] [G loss: 0.587033]\n",
      "epoch:37 step:29479 [D loss: 0.057657, acc.: 99.22%] [G loss: 0.510103]\n",
      "epoch:37 step:29480 [D loss: 0.041591, acc.: 98.44%] [G loss: 1.008954]\n",
      "epoch:37 step:29481 [D loss: 0.062034, acc.: 100.00%] [G loss: 1.564169]\n",
      "epoch:37 step:29482 [D loss: 0.037562, acc.: 100.00%] [G loss: 1.375474]\n",
      "epoch:37 step:29483 [D loss: 0.069159, acc.: 98.44%] [G loss: 0.482251]\n",
      "epoch:37 step:29484 [D loss: 0.444538, acc.: 78.12%] [G loss: 6.239305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29485 [D loss: 0.671626, acc.: 66.41%] [G loss: 5.073580]\n",
      "epoch:37 step:29486 [D loss: 0.052078, acc.: 99.22%] [G loss: 4.540732]\n",
      "epoch:37 step:29487 [D loss: 0.034526, acc.: 100.00%] [G loss: 0.157934]\n",
      "epoch:37 step:29488 [D loss: 0.021963, acc.: 99.22%] [G loss: 3.479998]\n",
      "epoch:37 step:29489 [D loss: 0.028254, acc.: 100.00%] [G loss: 2.912332]\n",
      "epoch:37 step:29490 [D loss: 0.096926, acc.: 98.44%] [G loss: 1.749774]\n",
      "epoch:37 step:29491 [D loss: 0.026036, acc.: 100.00%] [G loss: 0.046836]\n",
      "epoch:37 step:29492 [D loss: 0.023044, acc.: 99.22%] [G loss: 0.150481]\n",
      "epoch:37 step:29493 [D loss: 0.037723, acc.: 99.22%] [G loss: 0.813719]\n",
      "epoch:37 step:29494 [D loss: 0.004869, acc.: 100.00%] [G loss: 0.351573]\n",
      "epoch:37 step:29495 [D loss: 0.006443, acc.: 100.00%] [G loss: 0.006840]\n",
      "epoch:37 step:29496 [D loss: 0.013602, acc.: 100.00%] [G loss: 0.451660]\n",
      "epoch:37 step:29497 [D loss: 0.016361, acc.: 99.22%] [G loss: 0.034908]\n",
      "epoch:37 step:29498 [D loss: 0.021608, acc.: 99.22%] [G loss: 0.052236]\n",
      "epoch:37 step:29499 [D loss: 0.005748, acc.: 100.00%] [G loss: 0.064689]\n",
      "epoch:37 step:29500 [D loss: 0.003765, acc.: 100.00%] [G loss: 0.006373]\n",
      "epoch:37 step:29501 [D loss: 0.049993, acc.: 98.44%] [G loss: 0.025351]\n",
      "epoch:37 step:29502 [D loss: 0.008316, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:37 step:29503 [D loss: 0.009052, acc.: 100.00%] [G loss: 0.044751]\n",
      "epoch:37 step:29504 [D loss: 0.013139, acc.: 100.00%] [G loss: 0.011942]\n",
      "epoch:37 step:29505 [D loss: 0.002509, acc.: 100.00%] [G loss: 0.025079]\n",
      "epoch:37 step:29506 [D loss: 0.004884, acc.: 100.00%] [G loss: 0.006436]\n",
      "epoch:37 step:29507 [D loss: 0.003886, acc.: 100.00%] [G loss: 0.016534]\n",
      "epoch:37 step:29508 [D loss: 0.003750, acc.: 100.00%] [G loss: 0.029753]\n",
      "epoch:37 step:29509 [D loss: 0.009675, acc.: 100.00%] [G loss: 0.036822]\n",
      "epoch:37 step:29510 [D loss: 0.028614, acc.: 100.00%] [G loss: 0.007510]\n",
      "epoch:37 step:29511 [D loss: 0.020771, acc.: 100.00%] [G loss: 0.093069]\n",
      "epoch:37 step:29512 [D loss: 0.004685, acc.: 100.00%] [G loss: 0.140514]\n",
      "epoch:37 step:29513 [D loss: 0.008000, acc.: 100.00%] [G loss: 0.127931]\n",
      "epoch:37 step:29514 [D loss: 0.019674, acc.: 100.00%] [G loss: 0.177087]\n",
      "epoch:37 step:29515 [D loss: 0.031915, acc.: 99.22%] [G loss: 0.478370]\n",
      "epoch:37 step:29516 [D loss: 0.025235, acc.: 100.00%] [G loss: 0.252852]\n",
      "epoch:37 step:29517 [D loss: 0.017098, acc.: 100.00%] [G loss: 0.071572]\n",
      "epoch:37 step:29518 [D loss: 0.014523, acc.: 100.00%] [G loss: 0.579150]\n",
      "epoch:37 step:29519 [D loss: 0.120732, acc.: 98.44%] [G loss: 1.505094]\n",
      "epoch:37 step:29520 [D loss: 0.176318, acc.: 92.97%] [G loss: 0.639645]\n",
      "epoch:37 step:29521 [D loss: 0.206174, acc.: 92.19%] [G loss: 6.268765]\n",
      "epoch:37 step:29522 [D loss: 0.203389, acc.: 89.06%] [G loss: 6.286284]\n",
      "epoch:37 step:29523 [D loss: 0.063868, acc.: 98.44%] [G loss: 4.924598]\n",
      "epoch:37 step:29524 [D loss: 0.014553, acc.: 100.00%] [G loss: 0.032454]\n",
      "epoch:37 step:29525 [D loss: 0.009745, acc.: 100.00%] [G loss: 1.453781]\n",
      "epoch:37 step:29526 [D loss: 0.010439, acc.: 99.22%] [G loss: 0.253219]\n",
      "epoch:37 step:29527 [D loss: 0.005740, acc.: 100.00%] [G loss: 0.146929]\n",
      "epoch:37 step:29528 [D loss: 0.014101, acc.: 99.22%] [G loss: 0.085293]\n",
      "epoch:37 step:29529 [D loss: 0.006976, acc.: 100.00%] [G loss: 0.016937]\n",
      "epoch:37 step:29530 [D loss: 0.041626, acc.: 99.22%] [G loss: 0.067585]\n",
      "epoch:37 step:29531 [D loss: 0.017110, acc.: 100.00%] [G loss: 0.244297]\n",
      "epoch:37 step:29532 [D loss: 0.034070, acc.: 99.22%] [G loss: 0.077847]\n",
      "epoch:37 step:29533 [D loss: 0.023008, acc.: 99.22%] [G loss: 0.226294]\n",
      "epoch:37 step:29534 [D loss: 0.129194, acc.: 94.53%] [G loss: 2.807613]\n",
      "epoch:37 step:29535 [D loss: 0.070936, acc.: 97.66%] [G loss: 2.895963]\n",
      "epoch:37 step:29536 [D loss: 0.217878, acc.: 91.41%] [G loss: 0.656669]\n",
      "epoch:37 step:29537 [D loss: 0.001834, acc.: 100.00%] [G loss: 2.778121]\n",
      "epoch:37 step:29538 [D loss: 0.091662, acc.: 96.09%] [G loss: 0.787487]\n",
      "epoch:37 step:29539 [D loss: 0.043687, acc.: 99.22%] [G loss: 0.466437]\n",
      "epoch:37 step:29540 [D loss: 0.011533, acc.: 100.00%] [G loss: 0.462531]\n",
      "epoch:37 step:29541 [D loss: 0.081055, acc.: 99.22%] [G loss: 1.590199]\n",
      "epoch:37 step:29542 [D loss: 0.069767, acc.: 96.88%] [G loss: 0.287124]\n",
      "epoch:37 step:29543 [D loss: 0.192395, acc.: 89.84%] [G loss: 0.039347]\n",
      "epoch:37 step:29544 [D loss: 0.001409, acc.: 100.00%] [G loss: 0.251687]\n",
      "epoch:37 step:29545 [D loss: 0.005255, acc.: 100.00%] [G loss: 0.012327]\n",
      "epoch:37 step:29546 [D loss: 0.004736, acc.: 100.00%] [G loss: 0.006399]\n",
      "epoch:37 step:29547 [D loss: 0.011447, acc.: 100.00%] [G loss: 0.003826]\n",
      "epoch:37 step:29548 [D loss: 0.005444, acc.: 100.00%] [G loss: 0.003635]\n",
      "epoch:37 step:29549 [D loss: 0.001899, acc.: 100.00%] [G loss: 0.010424]\n",
      "epoch:37 step:29550 [D loss: 0.004708, acc.: 100.00%] [G loss: 0.015468]\n",
      "epoch:37 step:29551 [D loss: 0.018683, acc.: 100.00%] [G loss: 0.129917]\n",
      "epoch:37 step:29552 [D loss: 0.006233, acc.: 100.00%] [G loss: 0.027172]\n",
      "epoch:37 step:29553 [D loss: 0.017358, acc.: 100.00%] [G loss: 0.080195]\n",
      "epoch:37 step:29554 [D loss: 0.019330, acc.: 100.00%] [G loss: 0.794074]\n",
      "epoch:37 step:29555 [D loss: 0.004116, acc.: 100.00%] [G loss: 0.660822]\n",
      "epoch:37 step:29556 [D loss: 0.008041, acc.: 100.00%] [G loss: 0.330270]\n",
      "epoch:37 step:29557 [D loss: 0.070934, acc.: 99.22%] [G loss: 2.892107]\n",
      "epoch:37 step:29558 [D loss: 0.123374, acc.: 95.31%] [G loss: 2.593349]\n",
      "epoch:37 step:29559 [D loss: 0.668088, acc.: 63.28%] [G loss: 12.088783]\n",
      "epoch:37 step:29560 [D loss: 2.666676, acc.: 50.00%] [G loss: 1.071226]\n",
      "epoch:37 step:29561 [D loss: 0.175849, acc.: 92.97%] [G loss: 5.973863]\n",
      "epoch:37 step:29562 [D loss: 0.087267, acc.: 96.88%] [G loss: 3.908919]\n",
      "epoch:37 step:29563 [D loss: 0.039907, acc.: 99.22%] [G loss: 0.000069]\n",
      "epoch:37 step:29564 [D loss: 0.052946, acc.: 98.44%] [G loss: 5.648767]\n",
      "epoch:37 step:29565 [D loss: 0.020344, acc.: 100.00%] [G loss: 4.160976]\n",
      "epoch:37 step:29566 [D loss: 0.016927, acc.: 99.22%] [G loss: 2.041409]\n",
      "epoch:37 step:29567 [D loss: 0.028712, acc.: 99.22%] [G loss: 2.114357]\n",
      "epoch:37 step:29568 [D loss: 0.103292, acc.: 96.09%] [G loss: 2.973260]\n",
      "epoch:37 step:29569 [D loss: 0.017721, acc.: 100.00%] [G loss: 3.496084]\n",
      "epoch:37 step:29570 [D loss: 0.099165, acc.: 96.09%] [G loss: 0.953417]\n",
      "epoch:37 step:29571 [D loss: 0.270711, acc.: 86.72%] [G loss: 5.717411]\n",
      "epoch:37 step:29572 [D loss: 0.095396, acc.: 96.09%] [G loss: 6.316164]\n",
      "epoch:37 step:29573 [D loss: 0.323240, acc.: 83.59%] [G loss: 2.398418]\n",
      "epoch:37 step:29574 [D loss: 0.030223, acc.: 100.00%] [G loss: 1.118670]\n",
      "epoch:37 step:29575 [D loss: 0.220266, acc.: 89.84%] [G loss: 5.218935]\n",
      "epoch:37 step:29576 [D loss: 0.048409, acc.: 99.22%] [G loss: 6.294106]\n",
      "epoch:37 step:29577 [D loss: 0.113918, acc.: 95.31%] [G loss: 4.358428]\n",
      "epoch:37 step:29578 [D loss: 0.046802, acc.: 99.22%] [G loss: 0.408867]\n",
      "epoch:37 step:29579 [D loss: 0.041333, acc.: 98.44%] [G loss: 2.909049]\n",
      "epoch:37 step:29580 [D loss: 0.010058, acc.: 100.00%] [G loss: 1.264456]\n",
      "epoch:37 step:29581 [D loss: 0.011534, acc.: 100.00%] [G loss: 0.906525]\n",
      "epoch:37 step:29582 [D loss: 0.143367, acc.: 94.53%] [G loss: 0.691728]\n",
      "epoch:37 step:29583 [D loss: 0.051239, acc.: 97.66%] [G loss: 2.274418]\n",
      "epoch:37 step:29584 [D loss: 0.091675, acc.: 95.31%] [G loss: 0.649401]\n",
      "epoch:37 step:29585 [D loss: 0.070537, acc.: 98.44%] [G loss: 0.254565]\n",
      "epoch:37 step:29586 [D loss: 0.015026, acc.: 100.00%] [G loss: 1.873755]\n",
      "epoch:37 step:29587 [D loss: 0.008981, acc.: 100.00%] [G loss: 1.334521]\n",
      "epoch:37 step:29588 [D loss: 0.015225, acc.: 100.00%] [G loss: 1.376513]\n",
      "epoch:37 step:29589 [D loss: 0.097417, acc.: 97.66%] [G loss: 2.381600]\n",
      "epoch:37 step:29590 [D loss: 0.008032, acc.: 100.00%] [G loss: 1.262906]\n",
      "epoch:37 step:29591 [D loss: 0.135947, acc.: 95.31%] [G loss: 0.255373]\n",
      "epoch:37 step:29592 [D loss: 0.002472, acc.: 100.00%] [G loss: 0.394898]\n",
      "epoch:37 step:29593 [D loss: 0.056101, acc.: 97.66%] [G loss: 0.421445]\n",
      "epoch:37 step:29594 [D loss: 0.025533, acc.: 99.22%] [G loss: 0.109083]\n",
      "epoch:37 step:29595 [D loss: 0.013628, acc.: 100.00%] [G loss: 0.094315]\n",
      "epoch:37 step:29596 [D loss: 0.026654, acc.: 99.22%] [G loss: 0.082337]\n",
      "epoch:37 step:29597 [D loss: 0.004934, acc.: 100.00%] [G loss: 0.122709]\n",
      "epoch:37 step:29598 [D loss: 0.013554, acc.: 100.00%] [G loss: 0.037071]\n",
      "epoch:37 step:29599 [D loss: 0.020685, acc.: 100.00%] [G loss: 0.114995]\n",
      "epoch:37 step:29600 [D loss: 0.011750, acc.: 100.00%] [G loss: 0.231929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:37 step:29601 [D loss: 0.040609, acc.: 99.22%] [G loss: 0.108771]\n",
      "epoch:37 step:29602 [D loss: 0.007731, acc.: 100.00%] [G loss: 0.265766]\n",
      "epoch:37 step:29603 [D loss: 0.017537, acc.: 99.22%] [G loss: 0.149704]\n",
      "epoch:37 step:29604 [D loss: 0.061958, acc.: 100.00%] [G loss: 1.449269]\n",
      "epoch:37 step:29605 [D loss: 0.036662, acc.: 99.22%] [G loss: 1.572463]\n",
      "epoch:37 step:29606 [D loss: 0.017858, acc.: 100.00%] [G loss: 1.030514]\n",
      "epoch:37 step:29607 [D loss: 0.138558, acc.: 97.66%] [G loss: 8.397694]\n",
      "epoch:37 step:29608 [D loss: 0.276911, acc.: 87.50%] [G loss: 2.326115]\n",
      "epoch:37 step:29609 [D loss: 0.041293, acc.: 100.00%] [G loss: 2.305359]\n",
      "epoch:37 step:29610 [D loss: 0.486549, acc.: 75.78%] [G loss: 8.836832]\n",
      "epoch:37 step:29611 [D loss: 0.343234, acc.: 87.50%] [G loss: 8.048503]\n",
      "epoch:37 step:29612 [D loss: 0.748161, acc.: 66.41%] [G loss: 0.018120]\n",
      "epoch:37 step:29613 [D loss: 0.039824, acc.: 98.44%] [G loss: 0.012645]\n",
      "epoch:37 step:29614 [D loss: 0.002776, acc.: 100.00%] [G loss: 6.338371]\n",
      "epoch:37 step:29615 [D loss: 0.018660, acc.: 99.22%] [G loss: 5.526797]\n",
      "epoch:37 step:29616 [D loss: 0.020458, acc.: 100.00%] [G loss: 3.712013]\n",
      "epoch:37 step:29617 [D loss: 0.008784, acc.: 100.00%] [G loss: 2.655473]\n",
      "epoch:37 step:29618 [D loss: 0.021196, acc.: 100.00%] [G loss: 2.782923]\n",
      "epoch:37 step:29619 [D loss: 0.023675, acc.: 100.00%] [G loss: 3.289493]\n",
      "epoch:37 step:29620 [D loss: 0.029598, acc.: 100.00%] [G loss: 0.598002]\n",
      "epoch:37 step:29621 [D loss: 0.035544, acc.: 98.44%] [G loss: 0.081739]\n",
      "epoch:37 step:29622 [D loss: 0.011944, acc.: 99.22%] [G loss: 1.416986]\n",
      "epoch:37 step:29623 [D loss: 0.003928, acc.: 100.00%] [G loss: 1.026383]\n",
      "epoch:37 step:29624 [D loss: 0.024551, acc.: 100.00%] [G loss: 0.683868]\n",
      "epoch:37 step:29625 [D loss: 0.014470, acc.: 100.00%] [G loss: 0.509938]\n",
      "epoch:37 step:29626 [D loss: 0.019270, acc.: 99.22%] [G loss: 0.735474]\n",
      "epoch:37 step:29627 [D loss: 0.011739, acc.: 100.00%] [G loss: 1.166065]\n",
      "epoch:37 step:29628 [D loss: 0.013546, acc.: 100.00%] [G loss: 0.045287]\n",
      "epoch:37 step:29629 [D loss: 0.036160, acc.: 99.22%] [G loss: 0.106190]\n",
      "epoch:37 step:29630 [D loss: 0.010114, acc.: 100.00%] [G loss: 0.457791]\n",
      "epoch:37 step:29631 [D loss: 0.023159, acc.: 99.22%] [G loss: 0.267560]\n",
      "epoch:37 step:29632 [D loss: 0.029464, acc.: 99.22%] [G loss: 0.153495]\n",
      "epoch:37 step:29633 [D loss: 0.008489, acc.: 100.00%] [G loss: 0.106842]\n",
      "epoch:37 step:29634 [D loss: 0.054730, acc.: 100.00%] [G loss: 0.334384]\n",
      "epoch:37 step:29635 [D loss: 0.003565, acc.: 100.00%] [G loss: 1.681579]\n",
      "epoch:37 step:29636 [D loss: 0.012346, acc.: 100.00%] [G loss: 4.921187]\n",
      "epoch:37 step:29637 [D loss: 0.026343, acc.: 99.22%] [G loss: 0.571154]\n",
      "epoch:37 step:29638 [D loss: 0.032629, acc.: 100.00%] [G loss: 0.673403]\n",
      "epoch:37 step:29639 [D loss: 0.022663, acc.: 100.00%] [G loss: 0.617923]\n",
      "epoch:37 step:29640 [D loss: 0.202724, acc.: 91.41%] [G loss: 1.740796]\n",
      "epoch:37 step:29641 [D loss: 0.007815, acc.: 100.00%] [G loss: 2.463967]\n",
      "epoch:37 step:29642 [D loss: 0.053295, acc.: 99.22%] [G loss: 1.418856]\n",
      "epoch:37 step:29643 [D loss: 0.043093, acc.: 100.00%] [G loss: 1.222963]\n",
      "epoch:37 step:29644 [D loss: 0.039442, acc.: 99.22%] [G loss: 1.410303]\n",
      "epoch:37 step:29645 [D loss: 0.030885, acc.: 100.00%] [G loss: 2.180243]\n",
      "epoch:37 step:29646 [D loss: 0.271205, acc.: 89.06%] [G loss: 8.566357]\n",
      "epoch:37 step:29647 [D loss: 0.278117, acc.: 88.28%] [G loss: 5.679224]\n",
      "epoch:37 step:29648 [D loss: 0.018058, acc.: 99.22%] [G loss: 5.318908]\n",
      "epoch:37 step:29649 [D loss: 0.032735, acc.: 99.22%] [G loss: 5.320576]\n",
      "epoch:37 step:29650 [D loss: 0.035085, acc.: 100.00%] [G loss: 5.350613]\n",
      "epoch:37 step:29651 [D loss: 0.036662, acc.: 98.44%] [G loss: 4.487738]\n",
      "epoch:37 step:29652 [D loss: 0.023631, acc.: 100.00%] [G loss: 3.548845]\n",
      "epoch:37 step:29653 [D loss: 0.031664, acc.: 99.22%] [G loss: 0.190754]\n",
      "epoch:37 step:29654 [D loss: 0.010588, acc.: 100.00%] [G loss: 3.375057]\n",
      "epoch:37 step:29655 [D loss: 0.015636, acc.: 100.00%] [G loss: 3.443508]\n",
      "epoch:37 step:29656 [D loss: 0.030696, acc.: 100.00%] [G loss: 3.317611]\n",
      "epoch:37 step:29657 [D loss: 0.043539, acc.: 100.00%] [G loss: 0.348535]\n",
      "epoch:37 step:29658 [D loss: 0.019416, acc.: 100.00%] [G loss: 5.472677]\n",
      "epoch:37 step:29659 [D loss: 0.080337, acc.: 100.00%] [G loss: 3.195083]\n",
      "epoch:37 step:29660 [D loss: 0.081689, acc.: 97.66%] [G loss: 2.948325]\n",
      "epoch:37 step:29661 [D loss: 0.110555, acc.: 95.31%] [G loss: 3.577462]\n",
      "epoch:37 step:29662 [D loss: 0.035858, acc.: 98.44%] [G loss: 5.349846]\n",
      "epoch:37 step:29663 [D loss: 0.337779, acc.: 87.50%] [G loss: 1.633019]\n",
      "epoch:37 step:29664 [D loss: 0.692526, acc.: 68.75%] [G loss: 7.397752]\n",
      "epoch:37 step:29665 [D loss: 1.750116, acc.: 57.81%] [G loss: 7.253988]\n",
      "epoch:37 step:29666 [D loss: 0.014100, acc.: 99.22%] [G loss: 0.501006]\n",
      "epoch:37 step:29667 [D loss: 0.058543, acc.: 98.44%] [G loss: 5.079641]\n",
      "epoch:37 step:29668 [D loss: 0.047936, acc.: 99.22%] [G loss: 3.372917]\n",
      "epoch:37 step:29669 [D loss: 0.039370, acc.: 98.44%] [G loss: 2.796300]\n",
      "epoch:37 step:29670 [D loss: 0.002460, acc.: 100.00%] [G loss: 2.539683]\n",
      "epoch:37 step:29671 [D loss: 0.011274, acc.: 100.00%] [G loss: 1.978491]\n",
      "epoch:37 step:29672 [D loss: 0.012787, acc.: 100.00%] [G loss: 1.667192]\n",
      "epoch:37 step:29673 [D loss: 0.026030, acc.: 100.00%] [G loss: 1.021555]\n",
      "epoch:37 step:29674 [D loss: 0.064853, acc.: 99.22%] [G loss: 0.049206]\n",
      "epoch:37 step:29675 [D loss: 0.003930, acc.: 100.00%] [G loss: 1.797964]\n",
      "epoch:37 step:29676 [D loss: 0.053496, acc.: 98.44%] [G loss: 0.668224]\n",
      "epoch:37 step:29677 [D loss: 0.006280, acc.: 100.00%] [G loss: 0.346657]\n",
      "epoch:37 step:29678 [D loss: 0.036903, acc.: 99.22%] [G loss: 0.006542]\n",
      "epoch:38 step:29679 [D loss: 0.012414, acc.: 100.00%] [G loss: 0.292616]\n",
      "epoch:38 step:29680 [D loss: 0.009073, acc.: 100.00%] [G loss: 0.095293]\n",
      "epoch:38 step:29681 [D loss: 0.019658, acc.: 99.22%] [G loss: 0.215584]\n",
      "epoch:38 step:29682 [D loss: 0.004076, acc.: 100.00%] [G loss: 0.642524]\n",
      "epoch:38 step:29683 [D loss: 0.005831, acc.: 100.00%] [G loss: 0.043941]\n",
      "epoch:38 step:29684 [D loss: 0.050613, acc.: 100.00%] [G loss: 0.036205]\n",
      "epoch:38 step:29685 [D loss: 0.045873, acc.: 99.22%] [G loss: 0.746328]\n",
      "epoch:38 step:29686 [D loss: 0.010072, acc.: 100.00%] [G loss: 3.100407]\n",
      "epoch:38 step:29687 [D loss: 0.021888, acc.: 99.22%] [G loss: 1.950415]\n",
      "epoch:38 step:29688 [D loss: 0.123357, acc.: 95.31%] [G loss: 5.923685]\n",
      "epoch:38 step:29689 [D loss: 0.016600, acc.: 100.00%] [G loss: 2.547782]\n",
      "epoch:38 step:29690 [D loss: 0.033435, acc.: 99.22%] [G loss: 1.365220]\n",
      "epoch:38 step:29691 [D loss: 0.067913, acc.: 99.22%] [G loss: 5.127386]\n",
      "epoch:38 step:29692 [D loss: 0.184074, acc.: 92.97%] [G loss: 2.324405]\n",
      "epoch:38 step:29693 [D loss: 0.054403, acc.: 98.44%] [G loss: 3.017023]\n",
      "epoch:38 step:29694 [D loss: 0.016136, acc.: 100.00%] [G loss: 4.377510]\n",
      "epoch:38 step:29695 [D loss: 0.021762, acc.: 100.00%] [G loss: 4.696145]\n",
      "epoch:38 step:29696 [D loss: 0.012186, acc.: 100.00%] [G loss: 4.264142]\n",
      "epoch:38 step:29697 [D loss: 0.034511, acc.: 100.00%] [G loss: 1.773443]\n",
      "epoch:38 step:29698 [D loss: 0.017631, acc.: 99.22%] [G loss: 1.323052]\n",
      "epoch:38 step:29699 [D loss: 0.004537, acc.: 100.00%] [G loss: 0.101591]\n",
      "epoch:38 step:29700 [D loss: 0.005084, acc.: 100.00%] [G loss: 5.441960]\n",
      "epoch:38 step:29701 [D loss: 0.038774, acc.: 100.00%] [G loss: 3.390374]\n",
      "epoch:38 step:29702 [D loss: 0.000191, acc.: 100.00%] [G loss: 2.453866]\n",
      "epoch:38 step:29703 [D loss: 0.021264, acc.: 99.22%] [G loss: 0.918313]\n",
      "epoch:38 step:29704 [D loss: 0.009930, acc.: 100.00%] [G loss: 0.713189]\n",
      "epoch:38 step:29705 [D loss: 0.001112, acc.: 100.00%] [G loss: 0.629960]\n",
      "epoch:38 step:29706 [D loss: 0.015467, acc.: 100.00%] [G loss: 0.002235]\n",
      "epoch:38 step:29707 [D loss: 0.003528, acc.: 100.00%] [G loss: 3.495777]\n",
      "epoch:38 step:29708 [D loss: 0.001724, acc.: 100.00%] [G loss: 0.205353]\n",
      "epoch:38 step:29709 [D loss: 0.015012, acc.: 100.00%] [G loss: 0.110668]\n",
      "epoch:38 step:29710 [D loss: 0.045406, acc.: 100.00%] [G loss: 0.013388]\n",
      "epoch:38 step:29711 [D loss: 0.013391, acc.: 100.00%] [G loss: 2.374687]\n",
      "epoch:38 step:29712 [D loss: 0.000308, acc.: 100.00%] [G loss: 1.495957]\n",
      "epoch:38 step:29713 [D loss: 0.005568, acc.: 100.00%] [G loss: 1.051560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29714 [D loss: 0.047276, acc.: 99.22%] [G loss: 0.099900]\n",
      "epoch:38 step:29715 [D loss: 0.000880, acc.: 100.00%] [G loss: 0.045051]\n",
      "epoch:38 step:29716 [D loss: 0.005790, acc.: 100.00%] [G loss: 0.113559]\n",
      "epoch:38 step:29717 [D loss: 0.002180, acc.: 100.00%] [G loss: 0.032773]\n",
      "epoch:38 step:29718 [D loss: 0.005836, acc.: 100.00%] [G loss: 0.019645]\n",
      "epoch:38 step:29719 [D loss: 0.027117, acc.: 100.00%] [G loss: 0.000713]\n",
      "epoch:38 step:29720 [D loss: 0.000798, acc.: 100.00%] [G loss: 0.131791]\n",
      "epoch:38 step:29721 [D loss: 0.006652, acc.: 100.00%] [G loss: 0.152971]\n",
      "epoch:38 step:29722 [D loss: 0.004989, acc.: 100.00%] [G loss: 0.140142]\n",
      "epoch:38 step:29723 [D loss: 0.009099, acc.: 100.00%] [G loss: 0.008828]\n",
      "epoch:38 step:29724 [D loss: 0.030961, acc.: 100.00%] [G loss: 0.031409]\n",
      "epoch:38 step:29725 [D loss: 0.017119, acc.: 100.00%] [G loss: 0.061269]\n",
      "epoch:38 step:29726 [D loss: 0.014242, acc.: 100.00%] [G loss: 0.098711]\n",
      "epoch:38 step:29727 [D loss: 0.002285, acc.: 100.00%] [G loss: 0.518323]\n",
      "epoch:38 step:29728 [D loss: 0.015078, acc.: 100.00%] [G loss: 0.231521]\n",
      "epoch:38 step:29729 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.105728]\n",
      "epoch:38 step:29730 [D loss: 0.031290, acc.: 99.22%] [G loss: 0.014952]\n",
      "epoch:38 step:29731 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.018724]\n",
      "epoch:38 step:29732 [D loss: 0.008887, acc.: 100.00%] [G loss: 0.024441]\n",
      "epoch:38 step:29733 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:38 step:29734 [D loss: 0.003668, acc.: 100.00%] [G loss: 0.015352]\n",
      "epoch:38 step:29735 [D loss: 0.000327, acc.: 100.00%] [G loss: 0.009825]\n",
      "epoch:38 step:29736 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.058835]\n",
      "epoch:38 step:29737 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.043571]\n",
      "epoch:38 step:29738 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.007799]\n",
      "epoch:38 step:29739 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.006535]\n",
      "epoch:38 step:29740 [D loss: 0.000815, acc.: 100.00%] [G loss: 0.007062]\n",
      "epoch:38 step:29741 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.006905]\n",
      "epoch:38 step:29742 [D loss: 0.002798, acc.: 100.00%] [G loss: 0.120657]\n",
      "epoch:38 step:29743 [D loss: 0.004576, acc.: 100.00%] [G loss: 0.002527]\n",
      "epoch:38 step:29744 [D loss: 0.000826, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:38 step:29745 [D loss: 0.000466, acc.: 100.00%] [G loss: 0.005375]\n",
      "epoch:38 step:29746 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.002750]\n",
      "epoch:38 step:29747 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.002908]\n",
      "epoch:38 step:29748 [D loss: 0.003875, acc.: 100.00%] [G loss: 0.037743]\n",
      "epoch:38 step:29749 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.082712]\n",
      "epoch:38 step:29750 [D loss: 0.001827, acc.: 100.00%] [G loss: 0.021735]\n",
      "epoch:38 step:29751 [D loss: 0.002180, acc.: 100.00%] [G loss: 0.009615]\n",
      "epoch:38 step:29752 [D loss: 0.021680, acc.: 100.00%] [G loss: 0.000713]\n",
      "epoch:38 step:29753 [D loss: 0.000374, acc.: 100.00%] [G loss: 0.024641]\n",
      "epoch:38 step:29754 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.026869]\n",
      "epoch:38 step:29755 [D loss: 0.045563, acc.: 98.44%] [G loss: 0.000190]\n",
      "epoch:38 step:29756 [D loss: 0.000446, acc.: 100.00%] [G loss: 0.014729]\n",
      "epoch:38 step:29757 [D loss: 0.000380, acc.: 100.00%] [G loss: 0.024894]\n",
      "epoch:38 step:29758 [D loss: 0.000611, acc.: 100.00%] [G loss: 0.007337]\n",
      "epoch:38 step:29759 [D loss: 0.001035, acc.: 100.00%] [G loss: 0.012516]\n",
      "epoch:38 step:29760 [D loss: 0.054287, acc.: 96.88%] [G loss: 0.057978]\n",
      "epoch:38 step:29761 [D loss: 0.027145, acc.: 99.22%] [G loss: 0.357626]\n",
      "epoch:38 step:29762 [D loss: 0.001804, acc.: 100.00%] [G loss: 4.850319]\n",
      "epoch:38 step:29763 [D loss: 0.004483, acc.: 100.00%] [G loss: 0.967246]\n",
      "epoch:38 step:29764 [D loss: 0.108156, acc.: 97.66%] [G loss: 0.547790]\n",
      "epoch:38 step:29765 [D loss: 0.012643, acc.: 100.00%] [G loss: 5.468199]\n",
      "epoch:38 step:29766 [D loss: 0.266040, acc.: 88.28%] [G loss: 1.312226]\n",
      "epoch:38 step:29767 [D loss: 0.046246, acc.: 98.44%] [G loss: 0.434518]\n",
      "epoch:38 step:29768 [D loss: 0.018859, acc.: 100.00%] [G loss: 1.512529]\n",
      "epoch:38 step:29769 [D loss: 0.001531, acc.: 100.00%] [G loss: 1.581526]\n",
      "epoch:38 step:29770 [D loss: 0.020546, acc.: 100.00%] [G loss: 0.608197]\n",
      "epoch:38 step:29771 [D loss: 0.004678, acc.: 100.00%] [G loss: 4.237958]\n",
      "epoch:38 step:29772 [D loss: 0.278929, acc.: 85.16%] [G loss: 8.216793]\n",
      "epoch:38 step:29773 [D loss: 0.557248, acc.: 77.34%] [G loss: 6.513751]\n",
      "epoch:38 step:29774 [D loss: 0.373088, acc.: 82.81%] [G loss: 8.679654]\n",
      "epoch:38 step:29775 [D loss: 0.382386, acc.: 84.38%] [G loss: 7.443753]\n",
      "epoch:38 step:29776 [D loss: 0.290143, acc.: 90.62%] [G loss: 9.385901]\n",
      "epoch:38 step:29777 [D loss: 0.324967, acc.: 88.28%] [G loss: 6.910130]\n",
      "epoch:38 step:29778 [D loss: 0.016159, acc.: 100.00%] [G loss: 6.850359]\n",
      "epoch:38 step:29779 [D loss: 0.122105, acc.: 94.53%] [G loss: 0.180542]\n",
      "epoch:38 step:29780 [D loss: 0.066933, acc.: 96.88%] [G loss: 0.611168]\n",
      "epoch:38 step:29781 [D loss: 0.037525, acc.: 98.44%] [G loss: 0.008395]\n",
      "epoch:38 step:29782 [D loss: 0.006103, acc.: 100.00%] [G loss: 5.285601]\n",
      "epoch:38 step:29783 [D loss: 0.003693, acc.: 100.00%] [G loss: 6.871310]\n",
      "epoch:38 step:29784 [D loss: 0.001088, acc.: 100.00%] [G loss: 0.001583]\n",
      "epoch:38 step:29785 [D loss: 0.055395, acc.: 97.66%] [G loss: 5.650648]\n",
      "epoch:38 step:29786 [D loss: 0.004167, acc.: 100.00%] [G loss: 4.441789]\n",
      "epoch:38 step:29787 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.201816]\n",
      "epoch:38 step:29788 [D loss: 0.015319, acc.: 100.00%] [G loss: 3.679471]\n",
      "epoch:38 step:29789 [D loss: 0.000850, acc.: 100.00%] [G loss: 2.377447]\n",
      "epoch:38 step:29790 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.000818]\n",
      "epoch:38 step:29791 [D loss: 0.001997, acc.: 100.00%] [G loss: 0.557978]\n",
      "epoch:38 step:29792 [D loss: 0.000561, acc.: 100.00%] [G loss: 0.264760]\n",
      "epoch:38 step:29793 [D loss: 0.013566, acc.: 99.22%] [G loss: 0.354984]\n",
      "epoch:38 step:29794 [D loss: 0.016380, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:38 step:29795 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.049400]\n",
      "epoch:38 step:29796 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.063068]\n",
      "epoch:38 step:29797 [D loss: 0.004567, acc.: 100.00%] [G loss: 0.014219]\n",
      "epoch:38 step:29798 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.030659]\n",
      "epoch:38 step:29799 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.010658]\n",
      "epoch:38 step:29800 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:38 step:29801 [D loss: 0.002230, acc.: 100.00%] [G loss: 0.022017]\n",
      "epoch:38 step:29802 [D loss: 0.005683, acc.: 100.00%] [G loss: 0.070306]\n",
      "epoch:38 step:29803 [D loss: 0.008050, acc.: 100.00%] [G loss: 0.053490]\n",
      "epoch:38 step:29804 [D loss: 0.003913, acc.: 100.00%] [G loss: 0.139623]\n",
      "epoch:38 step:29805 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.026828]\n",
      "epoch:38 step:29806 [D loss: 0.003902, acc.: 100.00%] [G loss: 0.080965]\n",
      "epoch:38 step:29807 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.144191]\n",
      "epoch:38 step:29808 [D loss: 0.009323, acc.: 100.00%] [G loss: 0.010896]\n",
      "epoch:38 step:29809 [D loss: 0.003949, acc.: 100.00%] [G loss: 0.072251]\n",
      "epoch:38 step:29810 [D loss: 0.004501, acc.: 100.00%] [G loss: 0.020701]\n",
      "epoch:38 step:29811 [D loss: 0.004144, acc.: 100.00%] [G loss: 0.034820]\n",
      "epoch:38 step:29812 [D loss: 0.006330, acc.: 100.00%] [G loss: 0.069501]\n",
      "epoch:38 step:29813 [D loss: 0.020655, acc.: 100.00%] [G loss: 0.434663]\n",
      "epoch:38 step:29814 [D loss: 0.002259, acc.: 100.00%] [G loss: 0.602200]\n",
      "epoch:38 step:29815 [D loss: 0.052658, acc.: 100.00%] [G loss: 1.554008]\n",
      "epoch:38 step:29816 [D loss: 0.030128, acc.: 99.22%] [G loss: 1.368615]\n",
      "epoch:38 step:29817 [D loss: 0.027237, acc.: 100.00%] [G loss: 0.916749]\n",
      "epoch:38 step:29818 [D loss: 0.229662, acc.: 92.19%] [G loss: 0.000306]\n",
      "epoch:38 step:29819 [D loss: 0.392879, acc.: 82.81%] [G loss: 5.734239]\n",
      "epoch:38 step:29820 [D loss: 0.137041, acc.: 96.09%] [G loss: 5.846403]\n",
      "epoch:38 step:29821 [D loss: 0.572871, acc.: 79.69%] [G loss: 0.304929]\n",
      "epoch:38 step:29822 [D loss: 1.414386, acc.: 64.06%] [G loss: 9.508074]\n",
      "epoch:38 step:29823 [D loss: 0.411978, acc.: 82.03%] [G loss: 9.280148]\n",
      "epoch:38 step:29824 [D loss: 0.267021, acc.: 89.06%] [G loss: 7.238794]\n",
      "epoch:38 step:29825 [D loss: 0.038596, acc.: 99.22%] [G loss: 5.579488]\n",
      "epoch:38 step:29826 [D loss: 0.063812, acc.: 97.66%] [G loss: 0.015657]\n",
      "epoch:38 step:29827 [D loss: 0.001194, acc.: 100.00%] [G loss: 5.456955]\n",
      "epoch:38 step:29828 [D loss: 0.003137, acc.: 100.00%] [G loss: 5.215737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29829 [D loss: 0.005835, acc.: 100.00%] [G loss: 3.003109]\n",
      "epoch:38 step:29830 [D loss: 0.001908, acc.: 100.00%] [G loss: 0.059539]\n",
      "epoch:38 step:29831 [D loss: 0.070465, acc.: 96.09%] [G loss: 1.185960]\n",
      "epoch:38 step:29832 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.294015]\n",
      "epoch:38 step:29833 [D loss: 0.003112, acc.: 100.00%] [G loss: 0.943064]\n",
      "epoch:38 step:29834 [D loss: 0.009043, acc.: 100.00%] [G loss: 0.248495]\n",
      "epoch:38 step:29835 [D loss: 0.007200, acc.: 100.00%] [G loss: 0.202734]\n",
      "epoch:38 step:29836 [D loss: 0.248847, acc.: 86.72%] [G loss: 5.428199]\n",
      "epoch:38 step:29837 [D loss: 0.039666, acc.: 99.22%] [G loss: 7.047846]\n",
      "epoch:38 step:29838 [D loss: 1.831942, acc.: 46.09%] [G loss: 2.523134]\n",
      "epoch:38 step:29839 [D loss: 0.095364, acc.: 95.31%] [G loss: 3.536752]\n",
      "epoch:38 step:29840 [D loss: 0.015752, acc.: 100.00%] [G loss: 5.019331]\n",
      "epoch:38 step:29841 [D loss: 0.052428, acc.: 99.22%] [G loss: 3.164332]\n",
      "epoch:38 step:29842 [D loss: 0.016808, acc.: 100.00%] [G loss: 3.396146]\n",
      "epoch:38 step:29843 [D loss: 0.119459, acc.: 96.88%] [G loss: 5.339325]\n",
      "epoch:38 step:29844 [D loss: 0.035600, acc.: 98.44%] [G loss: 5.699403]\n",
      "epoch:38 step:29845 [D loss: 0.081027, acc.: 98.44%] [G loss: 4.491584]\n",
      "epoch:38 step:29846 [D loss: 0.056844, acc.: 99.22%] [G loss: 4.893208]\n",
      "epoch:38 step:29847 [D loss: 0.009429, acc.: 100.00%] [G loss: 4.949009]\n",
      "epoch:38 step:29848 [D loss: 0.037622, acc.: 99.22%] [G loss: 4.687037]\n",
      "epoch:38 step:29849 [D loss: 0.017048, acc.: 100.00%] [G loss: 0.142987]\n",
      "epoch:38 step:29850 [D loss: 0.008778, acc.: 100.00%] [G loss: 2.618925]\n",
      "epoch:38 step:29851 [D loss: 0.004545, acc.: 100.00%] [G loss: 2.741674]\n",
      "epoch:38 step:29852 [D loss: 0.124052, acc.: 96.88%] [G loss: 0.720774]\n",
      "epoch:38 step:29853 [D loss: 0.006010, acc.: 100.00%] [G loss: 0.293000]\n",
      "epoch:38 step:29854 [D loss: 0.393402, acc.: 84.38%] [G loss: 5.272859]\n",
      "epoch:38 step:29855 [D loss: 0.168164, acc.: 92.19%] [G loss: 4.320069]\n",
      "epoch:38 step:29856 [D loss: 0.289268, acc.: 85.94%] [G loss: 0.047815]\n",
      "epoch:38 step:29857 [D loss: 0.017255, acc.: 99.22%] [G loss: 0.368323]\n",
      "epoch:38 step:29858 [D loss: 0.009469, acc.: 100.00%] [G loss: 0.343878]\n",
      "epoch:38 step:29859 [D loss: 0.010027, acc.: 100.00%] [G loss: 0.506027]\n",
      "epoch:38 step:29860 [D loss: 0.045411, acc.: 100.00%] [G loss: 0.458616]\n",
      "epoch:38 step:29861 [D loss: 0.050030, acc.: 99.22%] [G loss: 2.890844]\n",
      "epoch:38 step:29862 [D loss: 0.004447, acc.: 100.00%] [G loss: 0.012649]\n",
      "epoch:38 step:29863 [D loss: 0.001714, acc.: 100.00%] [G loss: 2.698703]\n",
      "epoch:38 step:29864 [D loss: 0.001810, acc.: 100.00%] [G loss: 1.571815]\n",
      "epoch:38 step:29865 [D loss: 0.085454, acc.: 96.09%] [G loss: 0.335083]\n",
      "epoch:38 step:29866 [D loss: 0.091482, acc.: 96.88%] [G loss: 0.313051]\n",
      "epoch:38 step:29867 [D loss: 0.006209, acc.: 100.00%] [G loss: 1.279753]\n",
      "epoch:38 step:29868 [D loss: 0.028263, acc.: 100.00%] [G loss: 0.660253]\n",
      "epoch:38 step:29869 [D loss: 0.031610, acc.: 98.44%] [G loss: 0.035682]\n",
      "epoch:38 step:29870 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.713909]\n",
      "epoch:38 step:29871 [D loss: 0.005637, acc.: 100.00%] [G loss: 0.002214]\n",
      "epoch:38 step:29872 [D loss: 0.001358, acc.: 100.00%] [G loss: 0.530484]\n",
      "epoch:38 step:29873 [D loss: 0.008714, acc.: 100.00%] [G loss: 0.290855]\n",
      "epoch:38 step:29874 [D loss: 0.001419, acc.: 100.00%] [G loss: 0.031780]\n",
      "epoch:38 step:29875 [D loss: 0.019858, acc.: 99.22%] [G loss: 0.076610]\n",
      "epoch:38 step:29876 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.017380]\n",
      "epoch:38 step:29877 [D loss: 0.003372, acc.: 100.00%] [G loss: 0.017160]\n",
      "epoch:38 step:29878 [D loss: 0.009936, acc.: 100.00%] [G loss: 0.006335]\n",
      "epoch:38 step:29879 [D loss: 0.000839, acc.: 100.00%] [G loss: 0.001291]\n",
      "epoch:38 step:29880 [D loss: 0.006085, acc.: 100.00%] [G loss: 0.000432]\n",
      "epoch:38 step:29881 [D loss: 0.002398, acc.: 100.00%] [G loss: 0.000538]\n",
      "epoch:38 step:29882 [D loss: 0.003675, acc.: 100.00%] [G loss: 0.005698]\n",
      "epoch:38 step:29883 [D loss: 0.005871, acc.: 100.00%] [G loss: 0.003757]\n",
      "epoch:38 step:29884 [D loss: 0.005217, acc.: 100.00%] [G loss: 0.005756]\n",
      "epoch:38 step:29885 [D loss: 0.005796, acc.: 100.00%] [G loss: 0.003032]\n",
      "epoch:38 step:29886 [D loss: 0.006681, acc.: 100.00%] [G loss: 0.000652]\n",
      "epoch:38 step:29887 [D loss: 0.040756, acc.: 99.22%] [G loss: 0.001001]\n",
      "epoch:38 step:29888 [D loss: 0.084107, acc.: 96.09%] [G loss: 0.501391]\n",
      "epoch:38 step:29889 [D loss: 0.008401, acc.: 99.22%] [G loss: 2.231736]\n",
      "epoch:38 step:29890 [D loss: 0.007589, acc.: 100.00%] [G loss: 0.466892]\n",
      "epoch:38 step:29891 [D loss: 0.004556, acc.: 100.00%] [G loss: 0.253780]\n",
      "epoch:38 step:29892 [D loss: 0.160364, acc.: 93.75%] [G loss: 0.001805]\n",
      "epoch:38 step:29893 [D loss: 0.038808, acc.: 99.22%] [G loss: 0.000370]\n",
      "epoch:38 step:29894 [D loss: 0.008292, acc.: 100.00%] [G loss: 0.003407]\n",
      "epoch:38 step:29895 [D loss: 0.000963, acc.: 100.00%] [G loss: 0.016986]\n",
      "epoch:38 step:29896 [D loss: 0.009098, acc.: 100.00%] [G loss: 0.015938]\n",
      "epoch:38 step:29897 [D loss: 0.001119, acc.: 100.00%] [G loss: 0.001740]\n",
      "epoch:38 step:29898 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.000587]\n",
      "epoch:38 step:29899 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.010559]\n",
      "epoch:38 step:29900 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.001693]\n",
      "epoch:38 step:29901 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.001372]\n",
      "epoch:38 step:29902 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.000688]\n",
      "epoch:38 step:29903 [D loss: 0.000723, acc.: 100.00%] [G loss: 0.000582]\n",
      "epoch:38 step:29904 [D loss: 0.011303, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:38 step:29905 [D loss: 0.004860, acc.: 100.00%] [G loss: 0.001234]\n",
      "epoch:38 step:29906 [D loss: 0.001697, acc.: 100.00%] [G loss: 0.001813]\n",
      "epoch:38 step:29907 [D loss: 0.055977, acc.: 100.00%] [G loss: 0.005673]\n",
      "epoch:38 step:29908 [D loss: 0.007029, acc.: 100.00%] [G loss: 0.020816]\n",
      "epoch:38 step:29909 [D loss: 0.009167, acc.: 100.00%] [G loss: 0.182012]\n",
      "epoch:38 step:29910 [D loss: 0.027246, acc.: 100.00%] [G loss: 0.056880]\n",
      "epoch:38 step:29911 [D loss: 0.222534, acc.: 91.41%] [G loss: 4.139315]\n",
      "epoch:38 step:29912 [D loss: 0.360832, acc.: 84.38%] [G loss: 2.454374]\n",
      "epoch:38 step:29913 [D loss: 0.006581, acc.: 100.00%] [G loss: 2.987907]\n",
      "epoch:38 step:29914 [D loss: 0.490982, acc.: 81.25%] [G loss: 5.311210]\n",
      "epoch:38 step:29915 [D loss: 0.095082, acc.: 96.09%] [G loss: 6.262943]\n",
      "epoch:38 step:29916 [D loss: 0.138303, acc.: 93.75%] [G loss: 1.907239]\n",
      "epoch:38 step:29917 [D loss: 0.149743, acc.: 91.41%] [G loss: 2.978280]\n",
      "epoch:38 step:29918 [D loss: 0.875494, acc.: 63.28%] [G loss: 4.561174]\n",
      "epoch:38 step:29919 [D loss: 0.004041, acc.: 100.00%] [G loss: 0.852358]\n",
      "epoch:38 step:29920 [D loss: 0.038783, acc.: 99.22%] [G loss: 0.195751]\n",
      "epoch:38 step:29921 [D loss: 0.001013, acc.: 100.00%] [G loss: 5.913613]\n",
      "epoch:38 step:29922 [D loss: 0.005089, acc.: 100.00%] [G loss: 4.070890]\n",
      "epoch:38 step:29923 [D loss: 0.002151, acc.: 100.00%] [G loss: 3.757609]\n",
      "epoch:38 step:29924 [D loss: 0.031850, acc.: 99.22%] [G loss: 1.486540]\n",
      "epoch:38 step:29925 [D loss: 0.008008, acc.: 100.00%] [G loss: 0.618165]\n",
      "epoch:38 step:29926 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.079941]\n",
      "epoch:38 step:29927 [D loss: 0.000880, acc.: 100.00%] [G loss: 0.208248]\n",
      "epoch:38 step:29928 [D loss: 0.002444, acc.: 100.00%] [G loss: 0.048016]\n",
      "epoch:38 step:29929 [D loss: 0.029840, acc.: 99.22%] [G loss: 0.122888]\n",
      "epoch:38 step:29930 [D loss: 0.002571, acc.: 100.00%] [G loss: 0.833901]\n",
      "epoch:38 step:29931 [D loss: 0.015478, acc.: 100.00%] [G loss: 0.047792]\n",
      "epoch:38 step:29932 [D loss: 0.023750, acc.: 99.22%] [G loss: 0.222319]\n",
      "epoch:38 step:29933 [D loss: 0.012223, acc.: 100.00%] [G loss: 0.045181]\n",
      "epoch:38 step:29934 [D loss: 0.004455, acc.: 100.00%] [G loss: 0.415695]\n",
      "epoch:38 step:29935 [D loss: 0.011206, acc.: 100.00%] [G loss: 0.126325]\n",
      "epoch:38 step:29936 [D loss: 0.001664, acc.: 100.00%] [G loss: 0.051116]\n",
      "epoch:38 step:29937 [D loss: 0.059858, acc.: 98.44%] [G loss: 1.437143]\n",
      "epoch:38 step:29938 [D loss: 0.032617, acc.: 99.22%] [G loss: 0.541432]\n",
      "epoch:38 step:29939 [D loss: 0.080116, acc.: 96.09%] [G loss: 0.047925]\n",
      "epoch:38 step:29940 [D loss: 0.373194, acc.: 81.25%] [G loss: 5.308017]\n",
      "epoch:38 step:29941 [D loss: 0.116990, acc.: 93.75%] [G loss: 2.440737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:29942 [D loss: 0.651881, acc.: 75.78%] [G loss: 0.713251]\n",
      "epoch:38 step:29943 [D loss: 0.072282, acc.: 97.66%] [G loss: 0.000180]\n",
      "epoch:38 step:29944 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:38 step:29945 [D loss: 0.001540, acc.: 100.00%] [G loss: 0.025608]\n",
      "epoch:38 step:29946 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.030099]\n",
      "epoch:38 step:29947 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.020672]\n",
      "epoch:38 step:29948 [D loss: 0.002313, acc.: 100.00%] [G loss: 0.011628]\n",
      "epoch:38 step:29949 [D loss: 0.000399, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:38 step:29950 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.003721]\n",
      "epoch:38 step:29951 [D loss: 0.002549, acc.: 100.00%] [G loss: 0.017966]\n",
      "epoch:38 step:29952 [D loss: 0.002469, acc.: 100.00%] [G loss: 0.022005]\n",
      "epoch:38 step:29953 [D loss: 0.003110, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:38 step:29954 [D loss: 0.000487, acc.: 100.00%] [G loss: 0.048732]\n",
      "epoch:38 step:29955 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.004518]\n",
      "epoch:38 step:29956 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:38 step:29957 [D loss: 0.001525, acc.: 100.00%] [G loss: 0.000432]\n",
      "epoch:38 step:29958 [D loss: 0.010158, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:38 step:29959 [D loss: 0.002955, acc.: 100.00%] [G loss: 0.010322]\n",
      "epoch:38 step:29960 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:38 step:29961 [D loss: 0.007406, acc.: 100.00%] [G loss: 0.008477]\n",
      "epoch:38 step:29962 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.008551]\n",
      "epoch:38 step:29963 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.011417]\n",
      "epoch:38 step:29964 [D loss: 0.001266, acc.: 100.00%] [G loss: 0.016459]\n",
      "epoch:38 step:29965 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.005582]\n",
      "epoch:38 step:29966 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.008060]\n",
      "epoch:38 step:29967 [D loss: 0.001177, acc.: 100.00%] [G loss: 0.003947]\n",
      "epoch:38 step:29968 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.012182]\n",
      "epoch:38 step:29969 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:38 step:29970 [D loss: 0.000891, acc.: 100.00%] [G loss: 0.032813]\n",
      "epoch:38 step:29971 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.007947]\n",
      "epoch:38 step:29972 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.002092]\n",
      "epoch:38 step:29973 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.014702]\n",
      "epoch:38 step:29974 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.005320]\n",
      "epoch:38 step:29975 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.005043]\n",
      "epoch:38 step:29976 [D loss: 0.002240, acc.: 100.00%] [G loss: 0.035629]\n",
      "epoch:38 step:29977 [D loss: 0.040466, acc.: 99.22%] [G loss: 0.027253]\n",
      "epoch:38 step:29978 [D loss: 0.010832, acc.: 99.22%] [G loss: 0.028804]\n",
      "epoch:38 step:29979 [D loss: 0.007672, acc.: 100.00%] [G loss: 0.441905]\n",
      "epoch:38 step:29980 [D loss: 0.001779, acc.: 100.00%] [G loss: 0.047169]\n",
      "epoch:38 step:29981 [D loss: 0.140942, acc.: 93.75%] [G loss: 5.022491]\n",
      "epoch:38 step:29982 [D loss: 0.088628, acc.: 97.66%] [G loss: 1.377415]\n",
      "epoch:38 step:29983 [D loss: 0.017325, acc.: 99.22%] [G loss: 4.236464]\n",
      "epoch:38 step:29984 [D loss: 0.007228, acc.: 100.00%] [G loss: 2.387432]\n",
      "epoch:38 step:29985 [D loss: 0.006731, acc.: 100.00%] [G loss: 0.957036]\n",
      "epoch:38 step:29986 [D loss: 0.010552, acc.: 100.00%] [G loss: 0.415570]\n",
      "epoch:38 step:29987 [D loss: 0.021818, acc.: 100.00%] [G loss: 0.260400]\n",
      "epoch:38 step:29988 [D loss: 0.129154, acc.: 94.53%] [G loss: 4.725857]\n",
      "epoch:38 step:29989 [D loss: 0.080829, acc.: 96.09%] [G loss: 1.617211]\n",
      "epoch:38 step:29990 [D loss: 0.085539, acc.: 96.09%] [G loss: 1.043716]\n",
      "epoch:38 step:29991 [D loss: 0.281670, acc.: 89.06%] [G loss: 7.409649]\n",
      "epoch:38 step:29992 [D loss: 0.754177, acc.: 67.97%] [G loss: 0.802843]\n",
      "epoch:38 step:29993 [D loss: 1.323123, acc.: 44.53%] [G loss: 0.329458]\n",
      "epoch:38 step:29994 [D loss: 0.127100, acc.: 94.53%] [G loss: 7.472628]\n",
      "epoch:38 step:29995 [D loss: 0.449241, acc.: 79.69%] [G loss: 0.008388]\n",
      "epoch:38 step:29996 [D loss: 0.005850, acc.: 100.00%] [G loss: 4.179152]\n",
      "epoch:38 step:29997 [D loss: 0.028238, acc.: 98.44%] [G loss: 3.569672]\n",
      "epoch:38 step:29998 [D loss: 0.009648, acc.: 100.00%] [G loss: 2.788889]\n",
      "epoch:38 step:29999 [D loss: 0.014901, acc.: 100.00%] [G loss: 1.586312]\n",
      "epoch:38 step:30000 [D loss: 0.005044, acc.: 100.00%] [G loss: 1.046204]\n",
      "epoch:38 step:30001 [D loss: 0.005545, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:38 step:30002 [D loss: 0.003233, acc.: 100.00%] [G loss: 0.487517]\n",
      "epoch:38 step:30003 [D loss: 0.003815, acc.: 100.00%] [G loss: 0.001011]\n",
      "epoch:38 step:30004 [D loss: 0.006409, acc.: 100.00%] [G loss: 0.555713]\n",
      "epoch:38 step:30005 [D loss: 0.006903, acc.: 100.00%] [G loss: 0.147759]\n",
      "epoch:38 step:30006 [D loss: 0.001767, acc.: 100.00%] [G loss: 0.163991]\n",
      "epoch:38 step:30007 [D loss: 0.001676, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:38 step:30008 [D loss: 0.007519, acc.: 100.00%] [G loss: 0.037762]\n",
      "epoch:38 step:30009 [D loss: 0.004256, acc.: 100.00%] [G loss: 0.248225]\n",
      "epoch:38 step:30010 [D loss: 0.007576, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:38 step:30011 [D loss: 0.014828, acc.: 100.00%] [G loss: 0.088325]\n",
      "epoch:38 step:30012 [D loss: 0.006444, acc.: 100.00%] [G loss: 0.020761]\n",
      "epoch:38 step:30013 [D loss: 0.001036, acc.: 100.00%] [G loss: 0.060417]\n",
      "epoch:38 step:30014 [D loss: 0.005321, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:38 step:30015 [D loss: 0.020570, acc.: 100.00%] [G loss: 0.069247]\n",
      "epoch:38 step:30016 [D loss: 0.015057, acc.: 100.00%] [G loss: 0.030027]\n",
      "epoch:38 step:30017 [D loss: 0.006436, acc.: 100.00%] [G loss: 0.029086]\n",
      "epoch:38 step:30018 [D loss: 0.005844, acc.: 100.00%] [G loss: 0.000375]\n",
      "epoch:38 step:30019 [D loss: 0.002761, acc.: 100.00%] [G loss: 0.006437]\n",
      "epoch:38 step:30020 [D loss: 0.019005, acc.: 99.22%] [G loss: 0.016704]\n",
      "epoch:38 step:30021 [D loss: 0.007692, acc.: 100.00%] [G loss: 0.008758]\n",
      "epoch:38 step:30022 [D loss: 0.001122, acc.: 100.00%] [G loss: 0.004822]\n",
      "epoch:38 step:30023 [D loss: 0.009269, acc.: 100.00%] [G loss: 0.006544]\n",
      "epoch:38 step:30024 [D loss: 0.004870, acc.: 100.00%] [G loss: 0.003083]\n",
      "epoch:38 step:30025 [D loss: 0.022314, acc.: 100.00%] [G loss: 0.015153]\n",
      "epoch:38 step:30026 [D loss: 0.003345, acc.: 100.00%] [G loss: 0.034894]\n",
      "epoch:38 step:30027 [D loss: 0.001740, acc.: 100.00%] [G loss: 0.038384]\n",
      "epoch:38 step:30028 [D loss: 0.032915, acc.: 100.00%] [G loss: 0.036694]\n",
      "epoch:38 step:30029 [D loss: 0.205970, acc.: 91.41%] [G loss: 3.423001]\n",
      "epoch:38 step:30030 [D loss: 0.069151, acc.: 96.88%] [G loss: 0.272074]\n",
      "epoch:38 step:30031 [D loss: 0.831329, acc.: 67.19%] [G loss: 0.015099]\n",
      "epoch:38 step:30032 [D loss: 0.482829, acc.: 75.00%] [G loss: 1.459614]\n",
      "epoch:38 step:30033 [D loss: 0.000175, acc.: 100.00%] [G loss: 4.301506]\n",
      "epoch:38 step:30034 [D loss: 0.028997, acc.: 99.22%] [G loss: 3.417911]\n",
      "epoch:38 step:30035 [D loss: 0.020404, acc.: 99.22%] [G loss: 0.331930]\n",
      "epoch:38 step:30036 [D loss: 0.045566, acc.: 97.66%] [G loss: 1.209678]\n",
      "epoch:38 step:30037 [D loss: 0.000599, acc.: 100.00%] [G loss: 0.030608]\n",
      "epoch:38 step:30038 [D loss: 0.000599, acc.: 100.00%] [G loss: 0.643831]\n",
      "epoch:38 step:30039 [D loss: 0.020523, acc.: 99.22%] [G loss: 0.196492]\n",
      "epoch:38 step:30040 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.004981]\n",
      "epoch:38 step:30041 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.034877]\n",
      "epoch:38 step:30042 [D loss: 0.009877, acc.: 100.00%] [G loss: 0.019530]\n",
      "epoch:38 step:30043 [D loss: 0.000609, acc.: 100.00%] [G loss: 0.010791]\n",
      "epoch:38 step:30044 [D loss: 0.002745, acc.: 100.00%] [G loss: 0.004973]\n",
      "epoch:38 step:30045 [D loss: 0.000137, acc.: 100.00%] [G loss: 1.635470]\n",
      "epoch:38 step:30046 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:38 step:30047 [D loss: 0.001179, acc.: 100.00%] [G loss: 0.006941]\n",
      "epoch:38 step:30048 [D loss: 0.001090, acc.: 100.00%] [G loss: 0.310908]\n",
      "epoch:38 step:30049 [D loss: 0.002382, acc.: 100.00%] [G loss: 0.006913]\n",
      "epoch:38 step:30050 [D loss: 0.000776, acc.: 100.00%] [G loss: 0.009648]\n",
      "epoch:38 step:30051 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.001213]\n",
      "epoch:38 step:30052 [D loss: 0.003721, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:38 step:30053 [D loss: 0.004966, acc.: 100.00%] [G loss: 0.013348]\n",
      "epoch:38 step:30054 [D loss: 0.003731, acc.: 100.00%] [G loss: 0.008276]\n",
      "epoch:38 step:30055 [D loss: 0.000409, acc.: 100.00%] [G loss: 0.008247]\n",
      "epoch:38 step:30056 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.000239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30057 [D loss: 0.000687, acc.: 100.00%] [G loss: 0.001408]\n",
      "epoch:38 step:30058 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.001626]\n",
      "epoch:38 step:30059 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.051022]\n",
      "epoch:38 step:30060 [D loss: 0.001350, acc.: 100.00%] [G loss: 0.002031]\n",
      "epoch:38 step:30061 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.003172]\n",
      "epoch:38 step:30062 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.004202]\n",
      "epoch:38 step:30063 [D loss: 0.002787, acc.: 100.00%] [G loss: 0.001791]\n",
      "epoch:38 step:30064 [D loss: 0.001157, acc.: 100.00%] [G loss: 0.005524]\n",
      "epoch:38 step:30065 [D loss: 0.113706, acc.: 95.31%] [G loss: 0.523148]\n",
      "epoch:38 step:30066 [D loss: 0.168187, acc.: 92.97%] [G loss: 0.003512]\n",
      "epoch:38 step:30067 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.109897]\n",
      "epoch:38 step:30068 [D loss: 0.000599, acc.: 100.00%] [G loss: 0.067142]\n",
      "epoch:38 step:30069 [D loss: 0.001479, acc.: 100.00%] [G loss: 0.185643]\n",
      "epoch:38 step:30070 [D loss: 0.000475, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:38 step:30071 [D loss: 0.016174, acc.: 99.22%] [G loss: 0.018589]\n",
      "epoch:38 step:30072 [D loss: 0.008078, acc.: 100.00%] [G loss: 0.040647]\n",
      "epoch:38 step:30073 [D loss: 0.006100, acc.: 100.00%] [G loss: 0.000500]\n",
      "epoch:38 step:30074 [D loss: 0.001101, acc.: 100.00%] [G loss: 0.056061]\n",
      "epoch:38 step:30075 [D loss: 0.002135, acc.: 100.00%] [G loss: 0.006658]\n",
      "epoch:38 step:30076 [D loss: 0.004084, acc.: 100.00%] [G loss: 0.000383]\n",
      "epoch:38 step:30077 [D loss: 0.008057, acc.: 100.00%] [G loss: 0.054980]\n",
      "epoch:38 step:30078 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.014382]\n",
      "epoch:38 step:30079 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.006653]\n",
      "epoch:38 step:30080 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.004291]\n",
      "epoch:38 step:30081 [D loss: 0.000466, acc.: 100.00%] [G loss: 0.109069]\n",
      "epoch:38 step:30082 [D loss: 0.003066, acc.: 100.00%] [G loss: 0.003488]\n",
      "epoch:38 step:30083 [D loss: 0.001037, acc.: 100.00%] [G loss: 0.002289]\n",
      "epoch:38 step:30084 [D loss: 0.006779, acc.: 100.00%] [G loss: 0.012442]\n",
      "epoch:38 step:30085 [D loss: 0.003879, acc.: 100.00%] [G loss: 0.010364]\n",
      "epoch:38 step:30086 [D loss: 0.002803, acc.: 100.00%] [G loss: 0.021012]\n",
      "epoch:38 step:30087 [D loss: 0.001945, acc.: 100.00%] [G loss: 0.013532]\n",
      "epoch:38 step:30088 [D loss: 0.012460, acc.: 100.00%] [G loss: 0.014282]\n",
      "epoch:38 step:30089 [D loss: 0.012116, acc.: 99.22%] [G loss: 0.001623]\n",
      "epoch:38 step:30090 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.045787]\n",
      "epoch:38 step:30091 [D loss: 0.001750, acc.: 100.00%] [G loss: 0.025944]\n",
      "epoch:38 step:30092 [D loss: 0.001450, acc.: 100.00%] [G loss: 0.006424]\n",
      "epoch:38 step:30093 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.008073]\n",
      "epoch:38 step:30094 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.001021]\n",
      "epoch:38 step:30095 [D loss: 0.000589, acc.: 100.00%] [G loss: 0.008744]\n",
      "epoch:38 step:30096 [D loss: 0.004215, acc.: 100.00%] [G loss: 0.002574]\n",
      "epoch:38 step:30097 [D loss: 0.000489, acc.: 100.00%] [G loss: 0.009427]\n",
      "epoch:38 step:30098 [D loss: 0.002584, acc.: 100.00%] [G loss: 0.023997]\n",
      "epoch:38 step:30099 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000698]\n",
      "epoch:38 step:30100 [D loss: 0.005289, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:38 step:30101 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:38 step:30102 [D loss: 0.155039, acc.: 96.88%] [G loss: 0.020586]\n",
      "epoch:38 step:30103 [D loss: 0.005110, acc.: 100.00%] [G loss: 1.340400]\n",
      "epoch:38 step:30104 [D loss: 0.002761, acc.: 100.00%] [G loss: 0.447981]\n",
      "epoch:38 step:30105 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.292092]\n",
      "epoch:38 step:30106 [D loss: 0.125114, acc.: 93.75%] [G loss: 0.003074]\n",
      "epoch:38 step:30107 [D loss: 0.001207, acc.: 100.00%] [G loss: 0.033576]\n",
      "epoch:38 step:30108 [D loss: 0.026578, acc.: 99.22%] [G loss: 0.096588]\n",
      "epoch:38 step:30109 [D loss: 0.007186, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:38 step:30110 [D loss: 0.006812, acc.: 100.00%] [G loss: 0.000535]\n",
      "epoch:38 step:30111 [D loss: 0.000598, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:38 step:30112 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:38 step:30113 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.002314]\n",
      "epoch:38 step:30114 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.000905]\n",
      "epoch:38 step:30115 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.009951]\n",
      "epoch:38 step:30116 [D loss: 0.004615, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:38 step:30117 [D loss: 0.001825, acc.: 100.00%] [G loss: 0.000724]\n",
      "epoch:38 step:30118 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.003900]\n",
      "epoch:38 step:30119 [D loss: 0.003019, acc.: 100.00%] [G loss: 0.001503]\n",
      "epoch:38 step:30120 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.002412]\n",
      "epoch:38 step:30121 [D loss: 0.004430, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:38 step:30122 [D loss: 0.001290, acc.: 100.00%] [G loss: 0.000168]\n",
      "epoch:38 step:30123 [D loss: 0.004836, acc.: 100.00%] [G loss: 0.007811]\n",
      "epoch:38 step:30124 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.000583]\n",
      "epoch:38 step:30125 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:38 step:30126 [D loss: 0.003093, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:38 step:30127 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.000801]\n",
      "epoch:38 step:30128 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:38 step:30129 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.000503]\n",
      "epoch:38 step:30130 [D loss: 0.002718, acc.: 100.00%] [G loss: 0.000103]\n",
      "epoch:38 step:30131 [D loss: 0.000803, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:38 step:30132 [D loss: 0.053444, acc.: 100.00%] [G loss: 0.022805]\n",
      "epoch:38 step:30133 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.261983]\n",
      "epoch:38 step:30134 [D loss: 0.000451, acc.: 100.00%] [G loss: 1.497717]\n",
      "epoch:38 step:30135 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.322200]\n",
      "epoch:38 step:30136 [D loss: 0.001646, acc.: 100.00%] [G loss: 0.096767]\n",
      "epoch:38 step:30137 [D loss: 0.014936, acc.: 100.00%] [G loss: 0.065482]\n",
      "epoch:38 step:30138 [D loss: 0.064360, acc.: 97.66%] [G loss: 0.010063]\n",
      "epoch:38 step:30139 [D loss: 0.004919, acc.: 100.00%] [G loss: 0.051023]\n",
      "epoch:38 step:30140 [D loss: 0.061728, acc.: 98.44%] [G loss: 1.122752]\n",
      "epoch:38 step:30141 [D loss: 0.018607, acc.: 100.00%] [G loss: 2.033984]\n",
      "epoch:38 step:30142 [D loss: 0.618620, acc.: 63.28%] [G loss: 9.384015]\n",
      "epoch:38 step:30143 [D loss: 1.200342, acc.: 59.38%] [G loss: 0.360996]\n",
      "epoch:38 step:30144 [D loss: 0.005395, acc.: 100.00%] [G loss: 4.188016]\n",
      "epoch:38 step:30145 [D loss: 0.052901, acc.: 97.66%] [G loss: 3.524846]\n",
      "epoch:38 step:30146 [D loss: 0.031246, acc.: 99.22%] [G loss: 4.119687]\n",
      "epoch:38 step:30147 [D loss: 0.089534, acc.: 96.09%] [G loss: 5.042009]\n",
      "epoch:38 step:30148 [D loss: 0.012404, acc.: 100.00%] [G loss: 5.687541]\n",
      "epoch:38 step:30149 [D loss: 0.087925, acc.: 96.88%] [G loss: 3.394517]\n",
      "epoch:38 step:30150 [D loss: 0.106549, acc.: 97.66%] [G loss: 3.316211]\n",
      "epoch:38 step:30151 [D loss: 0.066434, acc.: 99.22%] [G loss: 5.327896]\n",
      "epoch:38 step:30152 [D loss: 0.374988, acc.: 87.50%] [G loss: 4.732704]\n",
      "epoch:38 step:30153 [D loss: 0.090203, acc.: 95.31%] [G loss: 5.019885]\n",
      "epoch:38 step:30154 [D loss: 0.007814, acc.: 100.00%] [G loss: 5.781206]\n",
      "epoch:38 step:30155 [D loss: 0.037081, acc.: 99.22%] [G loss: 4.138925]\n",
      "epoch:38 step:30156 [D loss: 0.077543, acc.: 97.66%] [G loss: 4.887847]\n",
      "epoch:38 step:30157 [D loss: 0.012025, acc.: 100.00%] [G loss: 4.254073]\n",
      "epoch:38 step:30158 [D loss: 0.036875, acc.: 100.00%] [G loss: 3.830604]\n",
      "epoch:38 step:30159 [D loss: 0.021132, acc.: 100.00%] [G loss: 3.655218]\n",
      "epoch:38 step:30160 [D loss: 0.280646, acc.: 86.72%] [G loss: 5.785192]\n",
      "epoch:38 step:30161 [D loss: 0.110830, acc.: 95.31%] [G loss: 3.506001]\n",
      "epoch:38 step:30162 [D loss: 0.041111, acc.: 98.44%] [G loss: 5.063708]\n",
      "epoch:38 step:30163 [D loss: 0.013153, acc.: 100.00%] [G loss: 4.019387]\n",
      "epoch:38 step:30164 [D loss: 0.026751, acc.: 97.66%] [G loss: 1.813081]\n",
      "epoch:38 step:30165 [D loss: 0.004366, acc.: 100.00%] [G loss: 1.111008]\n",
      "epoch:38 step:30166 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.000642]\n",
      "epoch:38 step:30167 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.101252]\n",
      "epoch:38 step:30168 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.043820]\n",
      "epoch:38 step:30169 [D loss: 0.001454, acc.: 100.00%] [G loss: 0.291129]\n",
      "epoch:38 step:30170 [D loss: 0.001505, acc.: 100.00%] [G loss: 0.034802]\n",
      "epoch:38 step:30171 [D loss: 0.000856, acc.: 100.00%] [G loss: 0.019381]\n",
      "epoch:38 step:30172 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.020109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30173 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.004068]\n",
      "epoch:38 step:30174 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.005242]\n",
      "epoch:38 step:30175 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.009113]\n",
      "epoch:38 step:30176 [D loss: 0.000786, acc.: 100.00%] [G loss: 0.025315]\n",
      "epoch:38 step:30177 [D loss: 0.001113, acc.: 100.00%] [G loss: 0.006090]\n",
      "epoch:38 step:30178 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.002193]\n",
      "epoch:38 step:30179 [D loss: 0.005431, acc.: 100.00%] [G loss: 0.002836]\n",
      "epoch:38 step:30180 [D loss: 0.001381, acc.: 100.00%] [G loss: 0.001362]\n",
      "epoch:38 step:30181 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.001166]\n",
      "epoch:38 step:30182 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.016133]\n",
      "epoch:38 step:30183 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.013297]\n",
      "epoch:38 step:30184 [D loss: 0.000935, acc.: 100.00%] [G loss: 0.002217]\n",
      "epoch:38 step:30185 [D loss: 0.002476, acc.: 100.00%] [G loss: 0.006685]\n",
      "epoch:38 step:30186 [D loss: 0.002553, acc.: 100.00%] [G loss: 0.006572]\n",
      "epoch:38 step:30187 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.004477]\n",
      "epoch:38 step:30188 [D loss: 0.001544, acc.: 100.00%] [G loss: 0.005555]\n",
      "epoch:38 step:30189 [D loss: 0.003277, acc.: 100.00%] [G loss: 0.003063]\n",
      "epoch:38 step:30190 [D loss: 0.001411, acc.: 100.00%] [G loss: 0.008172]\n",
      "epoch:38 step:30191 [D loss: 0.016387, acc.: 100.00%] [G loss: 0.006322]\n",
      "epoch:38 step:30192 [D loss: 0.025725, acc.: 100.00%] [G loss: 0.002709]\n",
      "epoch:38 step:30193 [D loss: 0.005775, acc.: 100.00%] [G loss: 0.101302]\n",
      "epoch:38 step:30194 [D loss: 0.002071, acc.: 100.00%] [G loss: 0.026091]\n",
      "epoch:38 step:30195 [D loss: 0.003287, acc.: 100.00%] [G loss: 0.038486]\n",
      "epoch:38 step:30196 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.014537]\n",
      "epoch:38 step:30197 [D loss: 0.000352, acc.: 100.00%] [G loss: 0.016118]\n",
      "epoch:38 step:30198 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.007457]\n",
      "epoch:38 step:30199 [D loss: 0.001403, acc.: 100.00%] [G loss: 0.012043]\n",
      "epoch:38 step:30200 [D loss: 0.003295, acc.: 100.00%] [G loss: 0.006760]\n",
      "epoch:38 step:30201 [D loss: 0.000852, acc.: 100.00%] [G loss: 0.005040]\n",
      "epoch:38 step:30202 [D loss: 0.003776, acc.: 100.00%] [G loss: 0.435025]\n",
      "epoch:38 step:30203 [D loss: 0.037875, acc.: 97.66%] [G loss: 0.014846]\n",
      "epoch:38 step:30204 [D loss: 0.158834, acc.: 92.19%] [G loss: 0.000025]\n",
      "epoch:38 step:30205 [D loss: 0.086949, acc.: 97.66%] [G loss: 0.000302]\n",
      "epoch:38 step:30206 [D loss: 0.002407, acc.: 100.00%] [G loss: 0.044989]\n",
      "epoch:38 step:30207 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.027808]\n",
      "epoch:38 step:30208 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.051869]\n",
      "epoch:38 step:30209 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.186799]\n",
      "epoch:38 step:30210 [D loss: 0.002234, acc.: 100.00%] [G loss: 0.766269]\n",
      "epoch:38 step:30211 [D loss: 0.001083, acc.: 100.00%] [G loss: 0.009826]\n",
      "epoch:38 step:30212 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.009113]\n",
      "epoch:38 step:30213 [D loss: 0.003149, acc.: 100.00%] [G loss: 0.016984]\n",
      "epoch:38 step:30214 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.009520]\n",
      "epoch:38 step:30215 [D loss: 0.019717, acc.: 100.00%] [G loss: 0.002871]\n",
      "epoch:38 step:30216 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:38 step:30217 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.003248]\n",
      "epoch:38 step:30218 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:38 step:30219 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000745]\n",
      "epoch:38 step:30220 [D loss: 0.000496, acc.: 100.00%] [G loss: 0.001080]\n",
      "epoch:38 step:30221 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000746]\n",
      "epoch:38 step:30222 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000430]\n",
      "epoch:38 step:30223 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000770]\n",
      "epoch:38 step:30224 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.002556]\n",
      "epoch:38 step:30225 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.001061]\n",
      "epoch:38 step:30226 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.002552]\n",
      "epoch:38 step:30227 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000347]\n",
      "epoch:38 step:30228 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.001607]\n",
      "epoch:38 step:30229 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:38 step:30230 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000594]\n",
      "epoch:38 step:30231 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001104]\n",
      "epoch:38 step:30232 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.003488]\n",
      "epoch:38 step:30233 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.001107]\n",
      "epoch:38 step:30234 [D loss: 0.001028, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:38 step:30235 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000559]\n",
      "epoch:38 step:30236 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.000591]\n",
      "epoch:38 step:30237 [D loss: 0.010076, acc.: 100.00%] [G loss: 0.001235]\n",
      "epoch:38 step:30238 [D loss: 0.007950, acc.: 100.00%] [G loss: 0.001023]\n",
      "epoch:38 step:30239 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.001000]\n",
      "epoch:38 step:30240 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.008487]\n",
      "epoch:38 step:30241 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.012609]\n",
      "epoch:38 step:30242 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.001514]\n",
      "epoch:38 step:30243 [D loss: 0.000582, acc.: 100.00%] [G loss: 0.001256]\n",
      "epoch:38 step:30244 [D loss: 0.000638, acc.: 100.00%] [G loss: 0.003859]\n",
      "epoch:38 step:30245 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.097183]\n",
      "epoch:38 step:30246 [D loss: 0.000827, acc.: 100.00%] [G loss: 0.006490]\n",
      "epoch:38 step:30247 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.001659]\n",
      "epoch:38 step:30248 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.003432]\n",
      "epoch:38 step:30249 [D loss: 0.002643, acc.: 100.00%] [G loss: 0.012157]\n",
      "epoch:38 step:30250 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:38 step:30251 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.004619]\n",
      "epoch:38 step:30252 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.002837]\n",
      "epoch:38 step:30253 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:38 step:30254 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.002190]\n",
      "epoch:38 step:30255 [D loss: 0.002236, acc.: 100.00%] [G loss: 0.008076]\n",
      "epoch:38 step:30256 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001780]\n",
      "epoch:38 step:30257 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000434]\n",
      "epoch:38 step:30258 [D loss: 0.001999, acc.: 100.00%] [G loss: 0.001792]\n",
      "epoch:38 step:30259 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.083003]\n",
      "epoch:38 step:30260 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000408]\n",
      "epoch:38 step:30261 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:38 step:30262 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.002758]\n",
      "epoch:38 step:30263 [D loss: 0.002265, acc.: 100.00%] [G loss: 0.000860]\n",
      "epoch:38 step:30264 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.023075]\n",
      "epoch:38 step:30265 [D loss: 0.015175, acc.: 99.22%] [G loss: 0.003343]\n",
      "epoch:38 step:30266 [D loss: 0.002140, acc.: 100.00%] [G loss: 0.529373]\n",
      "epoch:38 step:30267 [D loss: 0.005315, acc.: 100.00%] [G loss: 0.007755]\n",
      "epoch:38 step:30268 [D loss: 0.002017, acc.: 100.00%] [G loss: 0.002209]\n",
      "epoch:38 step:30269 [D loss: 0.001252, acc.: 100.00%] [G loss: 0.033424]\n",
      "epoch:38 step:30270 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.010295]\n",
      "epoch:38 step:30271 [D loss: 0.000875, acc.: 100.00%] [G loss: 0.004743]\n",
      "epoch:38 step:30272 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.024474]\n",
      "epoch:38 step:30273 [D loss: 0.000703, acc.: 100.00%] [G loss: 0.002520]\n",
      "epoch:38 step:30274 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.004826]\n",
      "epoch:38 step:30275 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.011594]\n",
      "epoch:38 step:30276 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.001298]\n",
      "epoch:38 step:30277 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.002808]\n",
      "epoch:38 step:30278 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.006866]\n",
      "epoch:38 step:30279 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.009242]\n",
      "epoch:38 step:30280 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.013095]\n",
      "epoch:38 step:30281 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.003069]\n",
      "epoch:38 step:30282 [D loss: 0.012390, acc.: 100.00%] [G loss: 0.029728]\n",
      "epoch:38 step:30283 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.003885]\n",
      "epoch:38 step:30284 [D loss: 0.003249, acc.: 100.00%] [G loss: 0.000801]\n",
      "epoch:38 step:30285 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.002264]\n",
      "epoch:38 step:30286 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.003703]\n",
      "epoch:38 step:30287 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.002600]\n",
      "epoch:38 step:30288 [D loss: 0.000758, acc.: 100.00%] [G loss: 0.002273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30289 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.001528]\n",
      "epoch:38 step:30290 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.002749]\n",
      "epoch:38 step:30291 [D loss: 0.001555, acc.: 100.00%] [G loss: 0.001302]\n",
      "epoch:38 step:30292 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002614]\n",
      "epoch:38 step:30293 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.012092]\n",
      "epoch:38 step:30294 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.004011]\n",
      "epoch:38 step:30295 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000444]\n",
      "epoch:38 step:30296 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.007305]\n",
      "epoch:38 step:30297 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.071118]\n",
      "epoch:38 step:30298 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000695]\n",
      "epoch:38 step:30299 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.003772]\n",
      "epoch:38 step:30300 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.001536]\n",
      "epoch:38 step:30301 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.000713]\n",
      "epoch:38 step:30302 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:38 step:30303 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.004002]\n",
      "epoch:38 step:30304 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:38 step:30305 [D loss: 0.000463, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:38 step:30306 [D loss: 0.004150, acc.: 100.00%] [G loss: 0.001072]\n",
      "epoch:38 step:30307 [D loss: 0.003205, acc.: 100.00%] [G loss: 0.003244]\n",
      "epoch:38 step:30308 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.001832]\n",
      "epoch:38 step:30309 [D loss: 0.003461, acc.: 100.00%] [G loss: 0.003480]\n",
      "epoch:38 step:30310 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.001878]\n",
      "epoch:38 step:30311 [D loss: 0.000733, acc.: 100.00%] [G loss: 0.010422]\n",
      "epoch:38 step:30312 [D loss: 0.016474, acc.: 100.00%] [G loss: 0.004668]\n",
      "epoch:38 step:30313 [D loss: 0.011388, acc.: 100.00%] [G loss: 0.259868]\n",
      "epoch:38 step:30314 [D loss: 0.000478, acc.: 100.00%] [G loss: 0.079375]\n",
      "epoch:38 step:30315 [D loss: 0.031266, acc.: 100.00%] [G loss: 0.137267]\n",
      "epoch:38 step:30316 [D loss: 0.003362, acc.: 100.00%] [G loss: 0.317044]\n",
      "epoch:38 step:30317 [D loss: 0.033700, acc.: 99.22%] [G loss: 0.384348]\n",
      "epoch:38 step:30318 [D loss: 0.015105, acc.: 100.00%] [G loss: 1.035311]\n",
      "epoch:38 step:30319 [D loss: 0.022964, acc.: 99.22%] [G loss: 0.515511]\n",
      "epoch:38 step:30320 [D loss: 0.032187, acc.: 98.44%] [G loss: 0.492998]\n",
      "epoch:38 step:30321 [D loss: 0.005767, acc.: 100.00%] [G loss: 0.021217]\n",
      "epoch:38 step:30322 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.014815]\n",
      "epoch:38 step:30323 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.027258]\n",
      "epoch:38 step:30324 [D loss: 0.048562, acc.: 98.44%] [G loss: 0.000840]\n",
      "epoch:38 step:30325 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:38 step:30326 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:38 step:30327 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.000609]\n",
      "epoch:38 step:30328 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:38 step:30329 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:38 step:30330 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:38 step:30331 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:38 step:30332 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:38 step:30333 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:38 step:30334 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:38 step:30335 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:38 step:30336 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:38 step:30337 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:38 step:30338 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.009491]\n",
      "epoch:38 step:30339 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:38 step:30340 [D loss: 0.001041, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:38 step:30341 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:38 step:30342 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.000289]\n",
      "epoch:38 step:30343 [D loss: 0.027555, acc.: 99.22%] [G loss: 0.000410]\n",
      "epoch:38 step:30344 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:38 step:30345 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.002342]\n",
      "epoch:38 step:30346 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.006957]\n",
      "epoch:38 step:30347 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.006147]\n",
      "epoch:38 step:30348 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.005199]\n",
      "epoch:38 step:30349 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:38 step:30350 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000924]\n",
      "epoch:38 step:30351 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.000408]\n",
      "epoch:38 step:30352 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:38 step:30353 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:38 step:30354 [D loss: 0.000463, acc.: 100.00%] [G loss: 0.017411]\n",
      "epoch:38 step:30355 [D loss: 0.017636, acc.: 100.00%] [G loss: 0.001867]\n",
      "epoch:38 step:30356 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.009236]\n",
      "epoch:38 step:30357 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.005333]\n",
      "epoch:38 step:30358 [D loss: 0.012396, acc.: 100.00%] [G loss: 0.010492]\n",
      "epoch:38 step:30359 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.061912]\n",
      "epoch:38 step:30360 [D loss: 0.000590, acc.: 100.00%] [G loss: 0.023238]\n",
      "epoch:38 step:30361 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.030872]\n",
      "epoch:38 step:30362 [D loss: 0.004712, acc.: 100.00%] [G loss: 0.002393]\n",
      "epoch:38 step:30363 [D loss: 0.019108, acc.: 99.22%] [G loss: 0.002274]\n",
      "epoch:38 step:30364 [D loss: 0.006107, acc.: 100.00%] [G loss: 2.096054]\n",
      "epoch:38 step:30365 [D loss: 0.024696, acc.: 100.00%] [G loss: 0.022661]\n",
      "epoch:38 step:30366 [D loss: 0.000850, acc.: 100.00%] [G loss: 0.162651]\n",
      "epoch:38 step:30367 [D loss: 0.000701, acc.: 100.00%] [G loss: 0.021105]\n",
      "epoch:38 step:30368 [D loss: 0.026496, acc.: 99.22%] [G loss: 0.001036]\n",
      "epoch:38 step:30369 [D loss: 0.000735, acc.: 100.00%] [G loss: 0.017825]\n",
      "epoch:38 step:30370 [D loss: 0.010729, acc.: 99.22%] [G loss: 0.002315]\n",
      "epoch:38 step:30371 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.043252]\n",
      "epoch:38 step:30372 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.002818]\n",
      "epoch:38 step:30373 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.006251]\n",
      "epoch:38 step:30374 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.011004]\n",
      "epoch:38 step:30375 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.001438]\n",
      "epoch:38 step:30376 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.008873]\n",
      "epoch:38 step:30377 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.001606]\n",
      "epoch:38 step:30378 [D loss: 0.001129, acc.: 100.00%] [G loss: 0.001893]\n",
      "epoch:38 step:30379 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.006758]\n",
      "epoch:38 step:30380 [D loss: 0.000891, acc.: 100.00%] [G loss: 0.004031]\n",
      "epoch:38 step:30381 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.001427]\n",
      "epoch:38 step:30382 [D loss: 0.002803, acc.: 100.00%] [G loss: 0.002720]\n",
      "epoch:38 step:30383 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.000822]\n",
      "epoch:38 step:30384 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001822]\n",
      "epoch:38 step:30385 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.001804]\n",
      "epoch:38 step:30386 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.000804]\n",
      "epoch:38 step:30387 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.001875]\n",
      "epoch:38 step:30388 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.001915]\n",
      "epoch:38 step:30389 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:38 step:30390 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.018070]\n",
      "epoch:38 step:30391 [D loss: 0.084639, acc.: 96.88%] [G loss: 0.000298]\n",
      "epoch:38 step:30392 [D loss: 0.016060, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:38 step:30393 [D loss: 0.006873, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:38 step:30394 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.011607]\n",
      "epoch:38 step:30395 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:38 step:30396 [D loss: 0.001140, acc.: 100.00%] [G loss: 0.048718]\n",
      "epoch:38 step:30397 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:38 step:30398 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.004220]\n",
      "epoch:38 step:30399 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.025917]\n",
      "epoch:38 step:30400 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:38 step:30401 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:38 step:30402 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:38 step:30403 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.000667]\n",
      "epoch:38 step:30404 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38 step:30405 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000240]\n",
      "epoch:38 step:30406 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:38 step:30407 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001763]\n",
      "epoch:38 step:30408 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:38 step:30409 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:38 step:30410 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:38 step:30411 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:38 step:30412 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000510]\n",
      "epoch:38 step:30413 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000390]\n",
      "epoch:38 step:30414 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:38 step:30415 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:38 step:30416 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002506]\n",
      "epoch:38 step:30417 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:38 step:30418 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:38 step:30419 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000615]\n",
      "epoch:38 step:30420 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.001194]\n",
      "epoch:38 step:30421 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:38 step:30422 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:38 step:30423 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000686]\n",
      "epoch:38 step:30424 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.000908]\n",
      "epoch:38 step:30425 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:38 step:30426 [D loss: 0.000491, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:38 step:30427 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.000395]\n",
      "epoch:38 step:30428 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:38 step:30429 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:38 step:30430 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001108]\n",
      "epoch:38 step:30431 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:38 step:30432 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:38 step:30433 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.004150]\n",
      "epoch:38 step:30434 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000745]\n",
      "epoch:38 step:30435 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.001864]\n",
      "epoch:38 step:30436 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000259]\n",
      "epoch:38 step:30437 [D loss: 0.000746, acc.: 100.00%] [G loss: 0.001559]\n",
      "epoch:38 step:30438 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:38 step:30439 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.000371]\n",
      "epoch:38 step:30440 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:38 step:30441 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.000549]\n",
      "epoch:38 step:30442 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.005657]\n",
      "epoch:38 step:30443 [D loss: 0.003038, acc.: 100.00%] [G loss: 0.004832]\n",
      "epoch:38 step:30444 [D loss: 0.005406, acc.: 100.00%] [G loss: 0.002903]\n",
      "epoch:38 step:30445 [D loss: 0.355458, acc.: 87.50%] [G loss: 8.194873]\n",
      "epoch:38 step:30446 [D loss: 1.199471, acc.: 65.62%] [G loss: 0.030162]\n",
      "epoch:38 step:30447 [D loss: 0.004852, acc.: 100.00%] [G loss: 0.002931]\n",
      "epoch:38 step:30448 [D loss: 0.024542, acc.: 98.44%] [G loss: 0.000492]\n",
      "epoch:38 step:30449 [D loss: 0.054230, acc.: 96.88%] [G loss: 2.006730]\n",
      "epoch:38 step:30450 [D loss: 0.003978, acc.: 100.00%] [G loss: 2.095734]\n",
      "epoch:38 step:30451 [D loss: 0.000681, acc.: 100.00%] [G loss: 0.412193]\n",
      "epoch:38 step:30452 [D loss: 0.011482, acc.: 99.22%] [G loss: 0.403703]\n",
      "epoch:38 step:30453 [D loss: 0.005046, acc.: 100.00%] [G loss: 0.120230]\n",
      "epoch:38 step:30454 [D loss: 0.005140, acc.: 100.00%] [G loss: 0.042445]\n",
      "epoch:38 step:30455 [D loss: 0.000651, acc.: 100.00%] [G loss: 0.076474]\n",
      "epoch:38 step:30456 [D loss: 0.000305, acc.: 100.00%] [G loss: 0.027341]\n",
      "epoch:38 step:30457 [D loss: 0.022808, acc.: 99.22%] [G loss: 0.004263]\n",
      "epoch:38 step:30458 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.185949]\n",
      "epoch:38 step:30459 [D loss: 0.000728, acc.: 100.00%] [G loss: 0.001319]\n",
      "epoch:39 step:30460 [D loss: 0.002214, acc.: 100.00%] [G loss: 0.001763]\n",
      "epoch:39 step:30461 [D loss: 0.004533, acc.: 100.00%] [G loss: 0.127668]\n",
      "epoch:39 step:30462 [D loss: 0.057679, acc.: 99.22%] [G loss: 0.053767]\n",
      "epoch:39 step:30463 [D loss: 0.006493, acc.: 100.00%] [G loss: 0.585931]\n",
      "epoch:39 step:30464 [D loss: 0.009104, acc.: 99.22%] [G loss: 0.110972]\n",
      "epoch:39 step:30465 [D loss: 0.009558, acc.: 100.00%] [G loss: 0.236963]\n",
      "epoch:39 step:30466 [D loss: 0.058652, acc.: 100.00%] [G loss: 0.616543]\n",
      "epoch:39 step:30467 [D loss: 0.027463, acc.: 99.22%] [G loss: 1.368091]\n",
      "epoch:39 step:30468 [D loss: 0.045387, acc.: 98.44%] [G loss: 0.729334]\n",
      "epoch:39 step:30469 [D loss: 0.334589, acc.: 82.03%] [G loss: 8.595983]\n",
      "epoch:39 step:30470 [D loss: 1.243969, acc.: 60.94%] [G loss: 0.121391]\n",
      "epoch:39 step:30471 [D loss: 0.001518, acc.: 100.00%] [G loss: 3.822725]\n",
      "epoch:39 step:30472 [D loss: 0.002900, acc.: 100.00%] [G loss: 0.070277]\n",
      "epoch:39 step:30473 [D loss: 0.004937, acc.: 100.00%] [G loss: 2.551938]\n",
      "epoch:39 step:30474 [D loss: 0.003522, acc.: 100.00%] [G loss: 1.125824]\n",
      "epoch:39 step:30475 [D loss: 0.000782, acc.: 100.00%] [G loss: 1.066635]\n",
      "epoch:39 step:30476 [D loss: 0.000871, acc.: 100.00%] [G loss: 0.001538]\n",
      "epoch:39 step:30477 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.301859]\n",
      "epoch:39 step:30478 [D loss: 0.000486, acc.: 100.00%] [G loss: 0.005235]\n",
      "epoch:39 step:30479 [D loss: 0.000643, acc.: 100.00%] [G loss: 0.169514]\n",
      "epoch:39 step:30480 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.246756]\n",
      "epoch:39 step:30481 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.001312]\n",
      "epoch:39 step:30482 [D loss: 0.002977, acc.: 100.00%] [G loss: 0.077402]\n",
      "epoch:39 step:30483 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:39 step:30484 [D loss: 0.045052, acc.: 98.44%] [G loss: 0.022276]\n",
      "epoch:39 step:30485 [D loss: 0.000549, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:39 step:30486 [D loss: 0.001190, acc.: 100.00%] [G loss: 0.001666]\n",
      "epoch:39 step:30487 [D loss: 0.002390, acc.: 100.00%] [G loss: 0.048696]\n",
      "epoch:39 step:30488 [D loss: 0.000550, acc.: 100.00%] [G loss: 0.014455]\n",
      "epoch:39 step:30489 [D loss: 0.000900, acc.: 100.00%] [G loss: 0.002561]\n",
      "epoch:39 step:30490 [D loss: 0.000638, acc.: 100.00%] [G loss: 0.001372]\n",
      "epoch:39 step:30491 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.001067]\n",
      "epoch:39 step:30492 [D loss: 0.002669, acc.: 100.00%] [G loss: 0.005032]\n",
      "epoch:39 step:30493 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.193019]\n",
      "epoch:39 step:30494 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.002097]\n",
      "epoch:39 step:30495 [D loss: 0.006517, acc.: 100.00%] [G loss: 0.002408]\n",
      "epoch:39 step:30496 [D loss: 0.002779, acc.: 100.00%] [G loss: 0.000436]\n",
      "epoch:39 step:30497 [D loss: 0.026176, acc.: 100.00%] [G loss: 0.000387]\n",
      "epoch:39 step:30498 [D loss: 0.010548, acc.: 100.00%] [G loss: 0.003243]\n",
      "epoch:39 step:30499 [D loss: 0.004020, acc.: 100.00%] [G loss: 0.007102]\n",
      "epoch:39 step:30500 [D loss: 0.000841, acc.: 100.00%] [G loss: 0.236691]\n",
      "epoch:39 step:30501 [D loss: 0.869870, acc.: 66.41%] [G loss: 10.125637]\n",
      "epoch:39 step:30502 [D loss: 1.496771, acc.: 56.25%] [G loss: 6.364882]\n",
      "epoch:39 step:30503 [D loss: 0.247973, acc.: 91.41%] [G loss: 1.683161]\n",
      "epoch:39 step:30504 [D loss: 0.430340, acc.: 80.47%] [G loss: 4.402381]\n",
      "epoch:39 step:30505 [D loss: 0.062679, acc.: 98.44%] [G loss: 4.973051]\n",
      "epoch:39 step:30506 [D loss: 0.206773, acc.: 92.97%] [G loss: 0.424217]\n",
      "epoch:39 step:30507 [D loss: 0.032131, acc.: 98.44%] [G loss: 2.033913]\n",
      "epoch:39 step:30508 [D loss: 0.001832, acc.: 100.00%] [G loss: 1.706673]\n",
      "epoch:39 step:30509 [D loss: 0.003181, acc.: 100.00%] [G loss: 0.416408]\n",
      "epoch:39 step:30510 [D loss: 0.002348, acc.: 100.00%] [G loss: 0.836275]\n",
      "epoch:39 step:30511 [D loss: 0.023563, acc.: 100.00%] [G loss: 0.073009]\n",
      "epoch:39 step:30512 [D loss: 0.119503, acc.: 96.88%] [G loss: 0.084076]\n",
      "epoch:39 step:30513 [D loss: 0.015493, acc.: 100.00%] [G loss: 2.749667]\n",
      "epoch:39 step:30514 [D loss: 0.131453, acc.: 94.53%] [G loss: 0.019941]\n",
      "epoch:39 step:30515 [D loss: 0.002441, acc.: 100.00%] [G loss: 0.615882]\n",
      "epoch:39 step:30516 [D loss: 0.004071, acc.: 100.00%] [G loss: 0.003957]\n",
      "epoch:39 step:30517 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.055066]\n",
      "epoch:39 step:30518 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.002374]\n",
      "epoch:39 step:30519 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.035433]\n",
      "epoch:39 step:30520 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.010316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30521 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.034312]\n",
      "epoch:39 step:30522 [D loss: 0.002519, acc.: 100.00%] [G loss: 0.029401]\n",
      "epoch:39 step:30523 [D loss: 0.001507, acc.: 100.00%] [G loss: 0.008292]\n",
      "epoch:39 step:30524 [D loss: 0.004788, acc.: 100.00%] [G loss: 0.011661]\n",
      "epoch:39 step:30525 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:39 step:30526 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.001163]\n",
      "epoch:39 step:30527 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.003271]\n",
      "epoch:39 step:30528 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.002713]\n",
      "epoch:39 step:30529 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.002636]\n",
      "epoch:39 step:30530 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.014955]\n",
      "epoch:39 step:30531 [D loss: 0.000649, acc.: 100.00%] [G loss: 0.007528]\n",
      "epoch:39 step:30532 [D loss: 0.000480, acc.: 100.00%] [G loss: 0.001836]\n",
      "epoch:39 step:30533 [D loss: 0.002770, acc.: 100.00%] [G loss: 0.028330]\n",
      "epoch:39 step:30534 [D loss: 0.029872, acc.: 100.00%] [G loss: 0.009736]\n",
      "epoch:39 step:30535 [D loss: 0.001279, acc.: 100.00%] [G loss: 0.078496]\n",
      "epoch:39 step:30536 [D loss: 0.003555, acc.: 100.00%] [G loss: 0.011338]\n",
      "epoch:39 step:30537 [D loss: 0.007691, acc.: 100.00%] [G loss: 0.089165]\n",
      "epoch:39 step:30538 [D loss: 0.005010, acc.: 100.00%] [G loss: 0.041814]\n",
      "epoch:39 step:30539 [D loss: 0.016087, acc.: 100.00%] [G loss: 0.504249]\n",
      "epoch:39 step:30540 [D loss: 0.009664, acc.: 100.00%] [G loss: 0.155052]\n",
      "epoch:39 step:30541 [D loss: 0.051436, acc.: 99.22%] [G loss: 4.288770]\n",
      "epoch:39 step:30542 [D loss: 0.035438, acc.: 100.00%] [G loss: 1.135833]\n",
      "epoch:39 step:30543 [D loss: 0.196761, acc.: 89.84%] [G loss: 0.334929]\n",
      "epoch:39 step:30544 [D loss: 0.215766, acc.: 92.97%] [G loss: 4.618363]\n",
      "epoch:39 step:30545 [D loss: 0.002691, acc.: 100.00%] [G loss: 3.315632]\n",
      "epoch:39 step:30546 [D loss: 0.000889, acc.: 100.00%] [G loss: 1.363899]\n",
      "epoch:39 step:30547 [D loss: 0.003350, acc.: 100.00%] [G loss: 0.967516]\n",
      "epoch:39 step:30548 [D loss: 0.015435, acc.: 99.22%] [G loss: 0.441230]\n",
      "epoch:39 step:30549 [D loss: 0.389576, acc.: 82.03%] [G loss: 4.050647]\n",
      "epoch:39 step:30550 [D loss: 0.061581, acc.: 97.66%] [G loss: 5.433773]\n",
      "epoch:39 step:30551 [D loss: 0.093180, acc.: 94.53%] [G loss: 1.743513]\n",
      "epoch:39 step:30552 [D loss: 0.018146, acc.: 100.00%] [G loss: 1.197206]\n",
      "epoch:39 step:30553 [D loss: 0.085661, acc.: 96.88%] [G loss: 0.124141]\n",
      "epoch:39 step:30554 [D loss: 0.005029, acc.: 100.00%] [G loss: 5.207170]\n",
      "epoch:39 step:30555 [D loss: 0.021533, acc.: 100.00%] [G loss: 2.942228]\n",
      "epoch:39 step:30556 [D loss: 0.105816, acc.: 96.09%] [G loss: 0.420457]\n",
      "epoch:39 step:30557 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:39 step:30558 [D loss: 0.006324, acc.: 100.00%] [G loss: 0.173409]\n",
      "epoch:39 step:30559 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.024664]\n",
      "epoch:39 step:30560 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:39 step:30561 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:39 step:30562 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.012234]\n",
      "epoch:39 step:30563 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:39 step:30564 [D loss: 0.002310, acc.: 100.00%] [G loss: 0.009986]\n",
      "epoch:39 step:30565 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.005889]\n",
      "epoch:39 step:30566 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.017181]\n",
      "epoch:39 step:30567 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.013014]\n",
      "epoch:39 step:30568 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.008089]\n",
      "epoch:39 step:30569 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.021124]\n",
      "epoch:39 step:30570 [D loss: 0.002817, acc.: 100.00%] [G loss: 0.026934]\n",
      "epoch:39 step:30571 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.084203]\n",
      "epoch:39 step:30572 [D loss: 0.002805, acc.: 100.00%] [G loss: 0.012369]\n",
      "epoch:39 step:30573 [D loss: 0.004363, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:39 step:30574 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.468336]\n",
      "epoch:39 step:30575 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.002285]\n",
      "epoch:39 step:30576 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.001837]\n",
      "epoch:39 step:30577 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:39 step:30578 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.010166]\n",
      "epoch:39 step:30579 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.003439]\n",
      "epoch:39 step:30580 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.004259]\n",
      "epoch:39 step:30581 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.014509]\n",
      "epoch:39 step:30582 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.007219]\n",
      "epoch:39 step:30583 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.000653]\n",
      "epoch:39 step:30584 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.002260]\n",
      "epoch:39 step:30585 [D loss: 0.001625, acc.: 100.00%] [G loss: 0.002369]\n",
      "epoch:39 step:30586 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.000946]\n",
      "epoch:39 step:30587 [D loss: 0.042572, acc.: 99.22%] [G loss: 0.014844]\n",
      "epoch:39 step:30588 [D loss: 0.000637, acc.: 100.00%] [G loss: 0.115260]\n",
      "epoch:39 step:30589 [D loss: 0.048825, acc.: 99.22%] [G loss: 1.406779]\n",
      "epoch:39 step:30590 [D loss: 0.001321, acc.: 100.00%] [G loss: 2.502986]\n",
      "epoch:39 step:30591 [D loss: 0.191426, acc.: 93.75%] [G loss: 0.109806]\n",
      "epoch:39 step:30592 [D loss: 0.130110, acc.: 96.88%] [G loss: 2.228544]\n",
      "epoch:39 step:30593 [D loss: 0.002651, acc.: 100.00%] [G loss: 4.779969]\n",
      "epoch:39 step:30594 [D loss: 0.431853, acc.: 79.69%] [G loss: 7.363236]\n",
      "epoch:39 step:30595 [D loss: 0.075115, acc.: 95.31%] [G loss: 8.354876]\n",
      "epoch:39 step:30596 [D loss: 0.775975, acc.: 72.66%] [G loss: 4.796848]\n",
      "epoch:39 step:30597 [D loss: 0.056913, acc.: 98.44%] [G loss: 5.432950]\n",
      "epoch:39 step:30598 [D loss: 0.007902, acc.: 100.00%] [G loss: 0.186518]\n",
      "epoch:39 step:30599 [D loss: 0.003873, acc.: 100.00%] [G loss: 4.161563]\n",
      "epoch:39 step:30600 [D loss: 0.006285, acc.: 100.00%] [G loss: 3.329906]\n",
      "epoch:39 step:30601 [D loss: 0.009669, acc.: 100.00%] [G loss: 3.159469]\n",
      "epoch:39 step:30602 [D loss: 0.006934, acc.: 100.00%] [G loss: 2.264828]\n",
      "epoch:39 step:30603 [D loss: 0.020394, acc.: 99.22%] [G loss: 1.019710]\n",
      "epoch:39 step:30604 [D loss: 0.031438, acc.: 100.00%] [G loss: 0.882761]\n",
      "epoch:39 step:30605 [D loss: 0.200386, acc.: 89.84%] [G loss: 5.801810]\n",
      "epoch:39 step:30606 [D loss: 0.050985, acc.: 97.66%] [G loss: 6.805475]\n",
      "epoch:39 step:30607 [D loss: 0.508711, acc.: 75.78%] [G loss: 7.819303]\n",
      "epoch:39 step:30608 [D loss: 0.187683, acc.: 91.41%] [G loss: 6.623543]\n",
      "epoch:39 step:30609 [D loss: 0.028458, acc.: 99.22%] [G loss: 5.550295]\n",
      "epoch:39 step:30610 [D loss: 0.022957, acc.: 100.00%] [G loss: 4.735688]\n",
      "epoch:39 step:30611 [D loss: 0.030664, acc.: 100.00%] [G loss: 5.343857]\n",
      "epoch:39 step:30612 [D loss: 0.086879, acc.: 98.44%] [G loss: 5.922794]\n",
      "epoch:39 step:30613 [D loss: 0.062454, acc.: 99.22%] [G loss: 4.042061]\n",
      "epoch:39 step:30614 [D loss: 0.024197, acc.: 100.00%] [G loss: 3.299067]\n",
      "epoch:39 step:30615 [D loss: 0.034841, acc.: 100.00%] [G loss: 5.228210]\n",
      "epoch:39 step:30616 [D loss: 0.069935, acc.: 96.88%] [G loss: 0.235650]\n",
      "epoch:39 step:30617 [D loss: 0.029952, acc.: 100.00%] [G loss: 4.386780]\n",
      "epoch:39 step:30618 [D loss: 0.009443, acc.: 100.00%] [G loss: 2.846292]\n",
      "epoch:39 step:30619 [D loss: 0.041208, acc.: 99.22%] [G loss: 0.220512]\n",
      "epoch:39 step:30620 [D loss: 0.033333, acc.: 99.22%] [G loss: 0.039692]\n",
      "epoch:39 step:30621 [D loss: 0.015774, acc.: 100.00%] [G loss: 0.019690]\n",
      "epoch:39 step:30622 [D loss: 0.010968, acc.: 100.00%] [G loss: 1.569561]\n",
      "epoch:39 step:30623 [D loss: 0.009218, acc.: 100.00%] [G loss: 0.009426]\n",
      "epoch:39 step:30624 [D loss: 0.033257, acc.: 100.00%] [G loss: 2.052505]\n",
      "epoch:39 step:30625 [D loss: 0.009495, acc.: 100.00%] [G loss: 1.286456]\n",
      "epoch:39 step:30626 [D loss: 0.025207, acc.: 99.22%] [G loss: 0.010057]\n",
      "epoch:39 step:30627 [D loss: 0.038667, acc.: 99.22%] [G loss: 0.471242]\n",
      "epoch:39 step:30628 [D loss: 0.001562, acc.: 100.00%] [G loss: 0.246722]\n",
      "epoch:39 step:30629 [D loss: 0.034613, acc.: 97.66%] [G loss: 0.024563]\n",
      "epoch:39 step:30630 [D loss: 0.003331, acc.: 100.00%] [G loss: 0.011444]\n",
      "epoch:39 step:30631 [D loss: 0.002136, acc.: 100.00%] [G loss: 0.005009]\n",
      "epoch:39 step:30632 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.001271]\n",
      "epoch:39 step:30633 [D loss: 0.002189, acc.: 100.00%] [G loss: 0.011936]\n",
      "epoch:39 step:30634 [D loss: 0.068689, acc.: 100.00%] [G loss: 0.079710]\n",
      "epoch:39 step:30635 [D loss: 0.138717, acc.: 95.31%] [G loss: 0.011839]\n",
      "epoch:39 step:30636 [D loss: 0.043635, acc.: 99.22%] [G loss: 1.969121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30637 [D loss: 0.029337, acc.: 99.22%] [G loss: 0.033856]\n",
      "epoch:39 step:30638 [D loss: 0.015498, acc.: 100.00%] [G loss: 0.150027]\n",
      "epoch:39 step:30639 [D loss: 0.000679, acc.: 100.00%] [G loss: 0.023041]\n",
      "epoch:39 step:30640 [D loss: 0.060874, acc.: 99.22%] [G loss: 0.274354]\n",
      "epoch:39 step:30641 [D loss: 0.001014, acc.: 100.00%] [G loss: 1.871710]\n",
      "epoch:39 step:30642 [D loss: 0.009482, acc.: 99.22%] [G loss: 0.405548]\n",
      "epoch:39 step:30643 [D loss: 0.036653, acc.: 98.44%] [G loss: 0.129729]\n",
      "epoch:39 step:30644 [D loss: 0.250437, acc.: 86.72%] [G loss: 6.274637]\n",
      "epoch:39 step:30645 [D loss: 0.068866, acc.: 97.66%] [G loss: 9.380939]\n",
      "epoch:39 step:30646 [D loss: 2.354461, acc.: 28.12%] [G loss: 5.932525]\n",
      "epoch:39 step:30647 [D loss: 0.004956, acc.: 100.00%] [G loss: 0.742400]\n",
      "epoch:39 step:30648 [D loss: 0.012011, acc.: 100.00%] [G loss: 5.982527]\n",
      "epoch:39 step:30649 [D loss: 0.113964, acc.: 97.66%] [G loss: 3.215514]\n",
      "epoch:39 step:30650 [D loss: 0.019058, acc.: 100.00%] [G loss: 4.172827]\n",
      "epoch:39 step:30651 [D loss: 0.006533, acc.: 100.00%] [G loss: 1.450104]\n",
      "epoch:39 step:30652 [D loss: 0.061082, acc.: 97.66%] [G loss: 2.287384]\n",
      "epoch:39 step:30653 [D loss: 0.054741, acc.: 99.22%] [G loss: 1.917027]\n",
      "epoch:39 step:30654 [D loss: 0.018543, acc.: 100.00%] [G loss: 0.970403]\n",
      "epoch:39 step:30655 [D loss: 0.012610, acc.: 100.00%] [G loss: 0.868753]\n",
      "epoch:39 step:30656 [D loss: 0.131224, acc.: 97.66%] [G loss: 0.364513]\n",
      "epoch:39 step:30657 [D loss: 0.003080, acc.: 100.00%] [G loss: 1.444382]\n",
      "epoch:39 step:30658 [D loss: 0.042924, acc.: 96.88%] [G loss: 0.150935]\n",
      "epoch:39 step:30659 [D loss: 0.006525, acc.: 100.00%] [G loss: 0.054892]\n",
      "epoch:39 step:30660 [D loss: 0.031166, acc.: 98.44%] [G loss: 0.133893]\n",
      "epoch:39 step:30661 [D loss: 0.001122, acc.: 100.00%] [G loss: 0.170805]\n",
      "epoch:39 step:30662 [D loss: 0.008423, acc.: 100.00%] [G loss: 0.096397]\n",
      "epoch:39 step:30663 [D loss: 0.018143, acc.: 99.22%] [G loss: 0.075256]\n",
      "epoch:39 step:30664 [D loss: 0.030355, acc.: 99.22%] [G loss: 0.140128]\n",
      "epoch:39 step:30665 [D loss: 0.066819, acc.: 96.88%] [G loss: 0.029548]\n",
      "epoch:39 step:30666 [D loss: 0.006535, acc.: 100.00%] [G loss: 0.025007]\n",
      "epoch:39 step:30667 [D loss: 0.195899, acc.: 90.62%] [G loss: 5.059507]\n",
      "epoch:39 step:30668 [D loss: 0.211237, acc.: 91.41%] [G loss: 6.037637]\n",
      "epoch:39 step:30669 [D loss: 0.351244, acc.: 84.38%] [G loss: 1.749009]\n",
      "epoch:39 step:30670 [D loss: 0.074446, acc.: 98.44%] [G loss: 3.214292]\n",
      "epoch:39 step:30671 [D loss: 0.010432, acc.: 100.00%] [G loss: 2.459775]\n",
      "epoch:39 step:30672 [D loss: 0.023793, acc.: 100.00%] [G loss: 2.263489]\n",
      "epoch:39 step:30673 [D loss: 0.263624, acc.: 85.94%] [G loss: 7.620387]\n",
      "epoch:39 step:30674 [D loss: 0.148113, acc.: 90.62%] [G loss: 6.658228]\n",
      "epoch:39 step:30675 [D loss: 0.155723, acc.: 92.97%] [G loss: 3.381780]\n",
      "epoch:39 step:30676 [D loss: 0.206701, acc.: 90.62%] [G loss: 7.815324]\n",
      "epoch:39 step:30677 [D loss: 0.512238, acc.: 80.47%] [G loss: 4.560557]\n",
      "epoch:39 step:30678 [D loss: 0.058775, acc.: 98.44%] [G loss: 4.277951]\n",
      "epoch:39 step:30679 [D loss: 0.136306, acc.: 94.53%] [G loss: 4.987900]\n",
      "epoch:39 step:30680 [D loss: 0.050730, acc.: 99.22%] [G loss: 4.965619]\n",
      "epoch:39 step:30681 [D loss: 0.106149, acc.: 95.31%] [G loss: 6.157675]\n",
      "epoch:39 step:30682 [D loss: 0.083878, acc.: 97.66%] [G loss: 4.284568]\n",
      "epoch:39 step:30683 [D loss: 0.058394, acc.: 97.66%] [G loss: 3.818381]\n",
      "epoch:39 step:30684 [D loss: 0.024660, acc.: 99.22%] [G loss: 5.090873]\n",
      "epoch:39 step:30685 [D loss: 0.126507, acc.: 95.31%] [G loss: 4.707471]\n",
      "epoch:39 step:30686 [D loss: 0.015881, acc.: 100.00%] [G loss: 3.393011]\n",
      "epoch:39 step:30687 [D loss: 0.210317, acc.: 91.41%] [G loss: 7.430113]\n",
      "epoch:39 step:30688 [D loss: 0.990116, acc.: 67.19%] [G loss: 4.393115]\n",
      "epoch:39 step:30689 [D loss: 0.113705, acc.: 93.75%] [G loss: 4.955272]\n",
      "epoch:39 step:30690 [D loss: 0.030600, acc.: 100.00%] [G loss: 5.561162]\n",
      "epoch:39 step:30691 [D loss: 0.120562, acc.: 96.09%] [G loss: 4.410096]\n",
      "epoch:39 step:30692 [D loss: 0.031698, acc.: 99.22%] [G loss: 4.541333]\n",
      "epoch:39 step:30693 [D loss: 0.091262, acc.: 96.88%] [G loss: 4.016778]\n",
      "epoch:39 step:30694 [D loss: 0.030628, acc.: 100.00%] [G loss: 5.015269]\n",
      "epoch:39 step:30695 [D loss: 0.398759, acc.: 89.84%] [G loss: 5.550294]\n",
      "epoch:39 step:30696 [D loss: 0.060221, acc.: 98.44%] [G loss: 0.681535]\n",
      "epoch:39 step:30697 [D loss: 0.049601, acc.: 99.22%] [G loss: 5.951551]\n",
      "epoch:39 step:30698 [D loss: 0.013724, acc.: 100.00%] [G loss: 4.658989]\n",
      "epoch:39 step:30699 [D loss: 0.115245, acc.: 96.09%] [G loss: 4.016400]\n",
      "epoch:39 step:30700 [D loss: 0.017195, acc.: 100.00%] [G loss: 3.327790]\n",
      "epoch:39 step:30701 [D loss: 0.179816, acc.: 89.06%] [G loss: 5.270597]\n",
      "epoch:39 step:30702 [D loss: 0.005026, acc.: 100.00%] [G loss: 6.381774]\n",
      "epoch:39 step:30703 [D loss: 0.228458, acc.: 88.28%] [G loss: 3.451000]\n",
      "epoch:39 step:30704 [D loss: 0.201396, acc.: 88.28%] [G loss: 0.279029]\n",
      "epoch:39 step:30705 [D loss: 0.029833, acc.: 99.22%] [G loss: 6.039732]\n",
      "epoch:39 step:30706 [D loss: 0.225885, acc.: 90.62%] [G loss: 2.887056]\n",
      "epoch:39 step:30707 [D loss: 0.007209, acc.: 100.00%] [G loss: 0.000461]\n",
      "epoch:39 step:30708 [D loss: 0.000865, acc.: 100.00%] [G loss: 1.207203]\n",
      "epoch:39 step:30709 [D loss: 0.003452, acc.: 100.00%] [G loss: 0.574642]\n",
      "epoch:39 step:30710 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.546299]\n",
      "epoch:39 step:30711 [D loss: 0.000561, acc.: 100.00%] [G loss: 0.238247]\n",
      "epoch:39 step:30712 [D loss: 0.004666, acc.: 100.00%] [G loss: 0.003597]\n",
      "epoch:39 step:30713 [D loss: 0.000859, acc.: 100.00%] [G loss: 0.567808]\n",
      "epoch:39 step:30714 [D loss: 0.033202, acc.: 100.00%] [G loss: 0.276541]\n",
      "epoch:39 step:30715 [D loss: 0.001069, acc.: 100.00%] [G loss: 0.258126]\n",
      "epoch:39 step:30716 [D loss: 0.002497, acc.: 100.00%] [G loss: 0.325394]\n",
      "epoch:39 step:30717 [D loss: 0.001180, acc.: 100.00%] [G loss: 0.195155]\n",
      "epoch:39 step:30718 [D loss: 0.010467, acc.: 100.00%] [G loss: 0.009089]\n",
      "epoch:39 step:30719 [D loss: 0.001596, acc.: 100.00%] [G loss: 0.073282]\n",
      "epoch:39 step:30720 [D loss: 0.019772, acc.: 99.22%] [G loss: 0.009397]\n",
      "epoch:39 step:30721 [D loss: 0.003070, acc.: 100.00%] [G loss: 0.021903]\n",
      "epoch:39 step:30722 [D loss: 0.001794, acc.: 100.00%] [G loss: 0.033960]\n",
      "epoch:39 step:30723 [D loss: 0.001745, acc.: 100.00%] [G loss: 0.042277]\n",
      "epoch:39 step:30724 [D loss: 0.004493, acc.: 100.00%] [G loss: 0.002652]\n",
      "epoch:39 step:30725 [D loss: 0.003645, acc.: 100.00%] [G loss: 0.022959]\n",
      "epoch:39 step:30726 [D loss: 0.003466, acc.: 100.00%] [G loss: 0.011200]\n",
      "epoch:39 step:30727 [D loss: 0.001461, acc.: 100.00%] [G loss: 0.288661]\n",
      "epoch:39 step:30728 [D loss: 0.026598, acc.: 100.00%] [G loss: 0.078607]\n",
      "epoch:39 step:30729 [D loss: 0.001488, acc.: 100.00%] [G loss: 0.037583]\n",
      "epoch:39 step:30730 [D loss: 0.005980, acc.: 100.00%] [G loss: 0.124286]\n",
      "epoch:39 step:30731 [D loss: 0.003750, acc.: 100.00%] [G loss: 0.061867]\n",
      "epoch:39 step:30732 [D loss: 0.109949, acc.: 100.00%] [G loss: 1.830806]\n",
      "epoch:39 step:30733 [D loss: 0.007800, acc.: 100.00%] [G loss: 0.554891]\n",
      "epoch:39 step:30734 [D loss: 0.213512, acc.: 90.62%] [G loss: 0.901910]\n",
      "epoch:39 step:30735 [D loss: 0.001207, acc.: 100.00%] [G loss: 0.017618]\n",
      "epoch:39 step:30736 [D loss: 0.011266, acc.: 99.22%] [G loss: 0.051433]\n",
      "epoch:39 step:30737 [D loss: 0.006397, acc.: 100.00%] [G loss: 0.064120]\n",
      "epoch:39 step:30738 [D loss: 0.006252, acc.: 100.00%] [G loss: 0.030227]\n",
      "epoch:39 step:30739 [D loss: 0.018341, acc.: 100.00%] [G loss: 0.419518]\n",
      "epoch:39 step:30740 [D loss: 0.030471, acc.: 100.00%] [G loss: 0.039657]\n",
      "epoch:39 step:30741 [D loss: 0.018231, acc.: 99.22%] [G loss: 0.015830]\n",
      "epoch:39 step:30742 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000436]\n",
      "epoch:39 step:30743 [D loss: 0.003372, acc.: 100.00%] [G loss: 0.111735]\n",
      "epoch:39 step:30744 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.040415]\n",
      "epoch:39 step:30745 [D loss: 0.010931, acc.: 99.22%] [G loss: 0.003157]\n",
      "epoch:39 step:30746 [D loss: 0.009504, acc.: 100.00%] [G loss: 0.000549]\n",
      "epoch:39 step:30747 [D loss: 0.001308, acc.: 100.00%] [G loss: 0.001084]\n",
      "epoch:39 step:30748 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.004493]\n",
      "epoch:39 step:30749 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.286824]\n",
      "epoch:39 step:30750 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.005339]\n",
      "epoch:39 step:30751 [D loss: 0.000756, acc.: 100.00%] [G loss: 0.000652]\n",
      "epoch:39 step:30752 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.069119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30753 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.004172]\n",
      "epoch:39 step:30754 [D loss: 0.001703, acc.: 100.00%] [G loss: 0.000853]\n",
      "epoch:39 step:30755 [D loss: 0.001390, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:39 step:30756 [D loss: 0.017327, acc.: 99.22%] [G loss: 0.011454]\n",
      "epoch:39 step:30757 [D loss: 0.001578, acc.: 100.00%] [G loss: 0.003948]\n",
      "epoch:39 step:30758 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.017867]\n",
      "epoch:39 step:30759 [D loss: 0.002602, acc.: 100.00%] [G loss: 0.001075]\n",
      "epoch:39 step:30760 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.001421]\n",
      "epoch:39 step:30761 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.003195]\n",
      "epoch:39 step:30762 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.002061]\n",
      "epoch:39 step:30763 [D loss: 0.000718, acc.: 100.00%] [G loss: 0.002433]\n",
      "epoch:39 step:30764 [D loss: 0.001708, acc.: 100.00%] [G loss: 0.003429]\n",
      "epoch:39 step:30765 [D loss: 0.000314, acc.: 100.00%] [G loss: 0.000490]\n",
      "epoch:39 step:30766 [D loss: 0.001552, acc.: 100.00%] [G loss: 0.015357]\n",
      "epoch:39 step:30767 [D loss: 0.007147, acc.: 100.00%] [G loss: 0.002307]\n",
      "epoch:39 step:30768 [D loss: 0.001515, acc.: 100.00%] [G loss: 0.002367]\n",
      "epoch:39 step:30769 [D loss: 0.091044, acc.: 99.22%] [G loss: 0.661136]\n",
      "epoch:39 step:30770 [D loss: 0.002265, acc.: 100.00%] [G loss: 2.632546]\n",
      "epoch:39 step:30771 [D loss: 0.133986, acc.: 92.19%] [G loss: 0.314296]\n",
      "epoch:39 step:30772 [D loss: 0.032070, acc.: 100.00%] [G loss: 0.049623]\n",
      "epoch:39 step:30773 [D loss: 0.025533, acc.: 99.22%] [G loss: 0.084192]\n",
      "epoch:39 step:30774 [D loss: 0.084179, acc.: 100.00%] [G loss: 0.015831]\n",
      "epoch:39 step:30775 [D loss: 0.001762, acc.: 100.00%] [G loss: 3.731715]\n",
      "epoch:39 step:30776 [D loss: 0.002524, acc.: 100.00%] [G loss: 2.588475]\n",
      "epoch:39 step:30777 [D loss: 0.155250, acc.: 91.41%] [G loss: 0.244719]\n",
      "epoch:39 step:30778 [D loss: 0.003444, acc.: 100.00%] [G loss: 0.000847]\n",
      "epoch:39 step:30779 [D loss: 0.181901, acc.: 93.75%] [G loss: 3.098704]\n",
      "epoch:39 step:30780 [D loss: 0.028708, acc.: 100.00%] [G loss: 6.529778]\n",
      "epoch:39 step:30781 [D loss: 0.046583, acc.: 96.88%] [G loss: 2.357906]\n",
      "epoch:39 step:30782 [D loss: 0.059296, acc.: 98.44%] [G loss: 0.579428]\n",
      "epoch:39 step:30783 [D loss: 0.006263, acc.: 100.00%] [G loss: 0.592944]\n",
      "epoch:39 step:30784 [D loss: 0.116325, acc.: 95.31%] [G loss: 4.332477]\n",
      "epoch:39 step:30785 [D loss: 0.029988, acc.: 99.22%] [G loss: 4.409955]\n",
      "epoch:39 step:30786 [D loss: 0.139682, acc.: 96.09%] [G loss: 0.407822]\n",
      "epoch:39 step:30787 [D loss: 0.061093, acc.: 97.66%] [G loss: 1.150811]\n",
      "epoch:39 step:30788 [D loss: 0.009801, acc.: 100.00%] [G loss: 0.809831]\n",
      "epoch:39 step:30789 [D loss: 0.003103, acc.: 100.00%] [G loss: 0.850644]\n",
      "epoch:39 step:30790 [D loss: 0.059761, acc.: 98.44%] [G loss: 1.801018]\n",
      "epoch:39 step:30791 [D loss: 0.004745, acc.: 100.00%] [G loss: 1.711719]\n",
      "epoch:39 step:30792 [D loss: 0.002718, acc.: 100.00%] [G loss: 2.884176]\n",
      "epoch:39 step:30793 [D loss: 1.620192, acc.: 51.56%] [G loss: 9.881152]\n",
      "epoch:39 step:30794 [D loss: 3.213010, acc.: 50.00%] [G loss: 6.114250]\n",
      "epoch:39 step:30795 [D loss: 1.182574, acc.: 53.91%] [G loss: 0.167934]\n",
      "epoch:39 step:30796 [D loss: 0.096113, acc.: 97.66%] [G loss: 1.896033]\n",
      "epoch:39 step:30797 [D loss: 0.430996, acc.: 85.16%] [G loss: 0.810419]\n",
      "epoch:39 step:30798 [D loss: 0.056579, acc.: 98.44%] [G loss: 4.457635]\n",
      "epoch:39 step:30799 [D loss: 0.297972, acc.: 86.72%] [G loss: 2.357256]\n",
      "epoch:39 step:30800 [D loss: 0.119223, acc.: 95.31%] [G loss: 2.120148]\n",
      "epoch:39 step:30801 [D loss: 0.170734, acc.: 93.75%] [G loss: 1.004977]\n",
      "epoch:39 step:30802 [D loss: 0.087602, acc.: 100.00%] [G loss: 1.742935]\n",
      "epoch:39 step:30803 [D loss: 0.138498, acc.: 96.09%] [G loss: 1.533545]\n",
      "epoch:39 step:30804 [D loss: 0.087974, acc.: 98.44%] [G loss: 0.998181]\n",
      "epoch:39 step:30805 [D loss: 0.302741, acc.: 84.38%] [G loss: 2.396595]\n",
      "epoch:39 step:30806 [D loss: 0.287012, acc.: 85.16%] [G loss: 0.104322]\n",
      "epoch:39 step:30807 [D loss: 0.020272, acc.: 100.00%] [G loss: 0.840762]\n",
      "epoch:39 step:30808 [D loss: 0.005968, acc.: 100.00%] [G loss: 0.823616]\n",
      "epoch:39 step:30809 [D loss: 0.132956, acc.: 96.88%] [G loss: 0.228520]\n",
      "epoch:39 step:30810 [D loss: 0.025508, acc.: 99.22%] [G loss: 0.190730]\n",
      "epoch:39 step:30811 [D loss: 0.014504, acc.: 100.00%] [G loss: 0.312015]\n",
      "epoch:39 step:30812 [D loss: 0.066800, acc.: 97.66%] [G loss: 0.037596]\n",
      "epoch:39 step:30813 [D loss: 0.032699, acc.: 100.00%] [G loss: 1.494203]\n",
      "epoch:39 step:30814 [D loss: 0.128568, acc.: 98.44%] [G loss: 1.946947]\n",
      "epoch:39 step:30815 [D loss: 0.038374, acc.: 98.44%] [G loss: 1.538182]\n",
      "epoch:39 step:30816 [D loss: 0.049523, acc.: 98.44%] [G loss: 1.698477]\n",
      "epoch:39 step:30817 [D loss: 0.054457, acc.: 98.44%] [G loss: 0.587956]\n",
      "epoch:39 step:30818 [D loss: 0.156904, acc.: 94.53%] [G loss: 2.224246]\n",
      "epoch:39 step:30819 [D loss: 0.134731, acc.: 94.53%] [G loss: 0.499548]\n",
      "epoch:39 step:30820 [D loss: 0.238943, acc.: 90.62%] [G loss: 2.076032]\n",
      "epoch:39 step:30821 [D loss: 0.017632, acc.: 100.00%] [G loss: 1.486186]\n",
      "epoch:39 step:30822 [D loss: 0.191718, acc.: 92.19%] [G loss: 0.594115]\n",
      "epoch:39 step:30823 [D loss: 0.536478, acc.: 71.88%] [G loss: 5.488601]\n",
      "epoch:39 step:30824 [D loss: 0.680998, acc.: 75.78%] [G loss: 4.706545]\n",
      "epoch:39 step:30825 [D loss: 0.321702, acc.: 85.94%] [G loss: 2.474424]\n",
      "epoch:39 step:30826 [D loss: 0.080427, acc.: 97.66%] [G loss: 2.224813]\n",
      "epoch:39 step:30827 [D loss: 0.066009, acc.: 98.44%] [G loss: 0.503168]\n",
      "epoch:39 step:30828 [D loss: 0.037989, acc.: 99.22%] [G loss: 0.066607]\n",
      "epoch:39 step:30829 [D loss: 0.009538, acc.: 100.00%] [G loss: 3.271752]\n",
      "epoch:39 step:30830 [D loss: 0.010234, acc.: 100.00%] [G loss: 3.808923]\n",
      "epoch:39 step:30831 [D loss: 0.022905, acc.: 100.00%] [G loss: 1.202714]\n",
      "epoch:39 step:30832 [D loss: 0.021676, acc.: 100.00%] [G loss: 1.152614]\n",
      "epoch:39 step:30833 [D loss: 0.024396, acc.: 99.22%] [G loss: 0.873544]\n",
      "epoch:39 step:30834 [D loss: 0.061786, acc.: 100.00%] [G loss: 0.572776]\n",
      "epoch:39 step:30835 [D loss: 0.037842, acc.: 100.00%] [G loss: 0.210108]\n",
      "epoch:39 step:30836 [D loss: 0.056490, acc.: 100.00%] [G loss: 0.034893]\n",
      "epoch:39 step:30837 [D loss: 0.045163, acc.: 99.22%] [G loss: 1.258023]\n",
      "epoch:39 step:30838 [D loss: 0.020442, acc.: 100.00%] [G loss: 0.420102]\n",
      "epoch:39 step:30839 [D loss: 0.171774, acc.: 93.75%] [G loss: 1.343322]\n",
      "epoch:39 step:30840 [D loss: 0.141702, acc.: 93.75%] [G loss: 1.312404]\n",
      "epoch:39 step:30841 [D loss: 0.144145, acc.: 95.31%] [G loss: 0.640013]\n",
      "epoch:39 step:30842 [D loss: 0.087998, acc.: 97.66%] [G loss: 0.468790]\n",
      "epoch:39 step:30843 [D loss: 0.005429, acc.: 100.00%] [G loss: 0.670661]\n",
      "epoch:39 step:30844 [D loss: 0.009983, acc.: 100.00%] [G loss: 1.187881]\n",
      "epoch:39 step:30845 [D loss: 0.050107, acc.: 97.66%] [G loss: 0.057512]\n",
      "epoch:39 step:30846 [D loss: 0.069001, acc.: 99.22%] [G loss: 0.342505]\n",
      "epoch:39 step:30847 [D loss: 0.078272, acc.: 96.09%] [G loss: 0.030705]\n",
      "epoch:39 step:30848 [D loss: 0.006632, acc.: 100.00%] [G loss: 0.022598]\n",
      "epoch:39 step:30849 [D loss: 0.005424, acc.: 100.00%] [G loss: 0.037504]\n",
      "epoch:39 step:30850 [D loss: 0.010113, acc.: 100.00%] [G loss: 0.032317]\n",
      "epoch:39 step:30851 [D loss: 0.012047, acc.: 99.22%] [G loss: 0.002316]\n",
      "epoch:39 step:30852 [D loss: 0.000959, acc.: 100.00%] [G loss: 0.017322]\n",
      "epoch:39 step:30853 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.005110]\n",
      "epoch:39 step:30854 [D loss: 0.002608, acc.: 100.00%] [G loss: 0.016678]\n",
      "epoch:39 step:30855 [D loss: 0.002296, acc.: 100.00%] [G loss: 0.000757]\n",
      "epoch:39 step:30856 [D loss: 0.001799, acc.: 100.00%] [G loss: 0.044054]\n",
      "epoch:39 step:30857 [D loss: 0.004972, acc.: 100.00%] [G loss: 0.008944]\n",
      "epoch:39 step:30858 [D loss: 0.004346, acc.: 100.00%] [G loss: 0.002864]\n",
      "epoch:39 step:30859 [D loss: 0.002718, acc.: 100.00%] [G loss: 0.181108]\n",
      "epoch:39 step:30860 [D loss: 0.022431, acc.: 100.00%] [G loss: 0.011745]\n",
      "epoch:39 step:30861 [D loss: 0.003234, acc.: 100.00%] [G loss: 0.021144]\n",
      "epoch:39 step:30862 [D loss: 0.003694, acc.: 100.00%] [G loss: 2.682019]\n",
      "epoch:39 step:30863 [D loss: 0.001953, acc.: 100.00%] [G loss: 0.010621]\n",
      "epoch:39 step:30864 [D loss: 0.012486, acc.: 100.00%] [G loss: 0.009565]\n",
      "epoch:39 step:30865 [D loss: 0.088328, acc.: 96.88%] [G loss: 0.215892]\n",
      "epoch:39 step:30866 [D loss: 0.001409, acc.: 100.00%] [G loss: 1.361334]\n",
      "epoch:39 step:30867 [D loss: 0.016925, acc.: 100.00%] [G loss: 0.127848]\n",
      "epoch:39 step:30868 [D loss: 0.035131, acc.: 100.00%] [G loss: 0.009336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30869 [D loss: 0.002974, acc.: 100.00%] [G loss: 0.009845]\n",
      "epoch:39 step:30870 [D loss: 0.084331, acc.: 97.66%] [G loss: 0.001608]\n",
      "epoch:39 step:30871 [D loss: 0.003064, acc.: 100.00%] [G loss: 0.000581]\n",
      "epoch:39 step:30872 [D loss: 0.009843, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:39 step:30873 [D loss: 0.002260, acc.: 100.00%] [G loss: 0.002554]\n",
      "epoch:39 step:30874 [D loss: 0.001706, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:39 step:30875 [D loss: 0.000642, acc.: 100.00%] [G loss: 0.001017]\n",
      "epoch:39 step:30876 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.000859]\n",
      "epoch:39 step:30877 [D loss: 0.000650, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:39 step:30878 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:39 step:30879 [D loss: 0.001620, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:39 step:30880 [D loss: 0.001405, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:39 step:30881 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:39 step:30882 [D loss: 0.002724, acc.: 100.00%] [G loss: 0.000730]\n",
      "epoch:39 step:30883 [D loss: 0.002649, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:39 step:30884 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:39 step:30885 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:39 step:30886 [D loss: 0.000659, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:39 step:30887 [D loss: 0.010773, acc.: 100.00%] [G loss: 0.001046]\n",
      "epoch:39 step:30888 [D loss: 0.002391, acc.: 100.00%] [G loss: 0.123947]\n",
      "epoch:39 step:30889 [D loss: 0.039279, acc.: 98.44%] [G loss: 0.000457]\n",
      "epoch:39 step:30890 [D loss: 0.002519, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:39 step:30891 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:39 step:30892 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:39 step:30893 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:39 step:30894 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000431]\n",
      "epoch:39 step:30895 [D loss: 0.001600, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:39 step:30896 [D loss: 0.002114, acc.: 100.00%] [G loss: 0.000287]\n",
      "epoch:39 step:30897 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:39 step:30898 [D loss: 0.001096, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:39 step:30899 [D loss: 0.005577, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:39 step:30900 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:39 step:30901 [D loss: 0.003953, acc.: 100.00%] [G loss: 0.002771]\n",
      "epoch:39 step:30902 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.001979]\n",
      "epoch:39 step:30903 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:39 step:30904 [D loss: 0.003532, acc.: 100.00%] [G loss: 0.000315]\n",
      "epoch:39 step:30905 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:39 step:30906 [D loss: 0.000588, acc.: 100.00%] [G loss: 0.046653]\n",
      "epoch:39 step:30907 [D loss: 0.132062, acc.: 95.31%] [G loss: 0.187712]\n",
      "epoch:39 step:30908 [D loss: 0.015073, acc.: 99.22%] [G loss: 1.206637]\n",
      "epoch:39 step:30909 [D loss: 0.051141, acc.: 98.44%] [G loss: 0.482993]\n",
      "epoch:39 step:30910 [D loss: 0.011960, acc.: 100.00%] [G loss: 0.415893]\n",
      "epoch:39 step:30911 [D loss: 0.007974, acc.: 100.00%] [G loss: 0.138690]\n",
      "epoch:39 step:30912 [D loss: 0.013809, acc.: 99.22%] [G loss: 0.013787]\n",
      "epoch:39 step:30913 [D loss: 0.009771, acc.: 99.22%] [G loss: 0.013959]\n",
      "epoch:39 step:30914 [D loss: 0.001847, acc.: 100.00%] [G loss: 0.002695]\n",
      "epoch:39 step:30915 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.001421]\n",
      "epoch:39 step:30916 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.002744]\n",
      "epoch:39 step:30917 [D loss: 0.001094, acc.: 100.00%] [G loss: 0.001909]\n",
      "epoch:39 step:30918 [D loss: 0.000664, acc.: 100.00%] [G loss: 0.005861]\n",
      "epoch:39 step:30919 [D loss: 0.001701, acc.: 100.00%] [G loss: 0.001280]\n",
      "epoch:39 step:30920 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.002347]\n",
      "epoch:39 step:30921 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.030758]\n",
      "epoch:39 step:30922 [D loss: 0.002942, acc.: 100.00%] [G loss: 0.002447]\n",
      "epoch:39 step:30923 [D loss: 0.000816, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:39 step:30924 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.001670]\n",
      "epoch:39 step:30925 [D loss: 0.001430, acc.: 100.00%] [G loss: 0.004451]\n",
      "epoch:39 step:30926 [D loss: 0.002939, acc.: 100.00%] [G loss: 0.002849]\n",
      "epoch:39 step:30927 [D loss: 0.000607, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:39 step:30928 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001096]\n",
      "epoch:39 step:30929 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.020763]\n",
      "epoch:39 step:30930 [D loss: 0.001374, acc.: 100.00%] [G loss: 0.000756]\n",
      "epoch:39 step:30931 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:39 step:30932 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000931]\n",
      "epoch:39 step:30933 [D loss: 0.000413, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:39 step:30934 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.000957]\n",
      "epoch:39 step:30935 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000403]\n",
      "epoch:39 step:30936 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:39 step:30937 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.023249]\n",
      "epoch:39 step:30938 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:39 step:30939 [D loss: 0.000434, acc.: 100.00%] [G loss: 1.620631]\n",
      "epoch:39 step:30940 [D loss: 0.001359, acc.: 100.00%] [G loss: 0.016214]\n",
      "epoch:39 step:30941 [D loss: 0.001258, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:39 step:30942 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.001007]\n",
      "epoch:39 step:30943 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.000573]\n",
      "epoch:39 step:30944 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.000399]\n",
      "epoch:39 step:30945 [D loss: 0.000570, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:39 step:30946 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.000782]\n",
      "epoch:39 step:30947 [D loss: 0.000494, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:39 step:30948 [D loss: 0.044085, acc.: 100.00%] [G loss: 0.000483]\n",
      "epoch:39 step:30949 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.001451]\n",
      "epoch:39 step:30950 [D loss: 0.012660, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:39 step:30951 [D loss: 0.001019, acc.: 100.00%] [G loss: 0.002272]\n",
      "epoch:39 step:30952 [D loss: 0.003141, acc.: 100.00%] [G loss: 0.000615]\n",
      "epoch:39 step:30953 [D loss: 0.000368, acc.: 100.00%] [G loss: 0.003328]\n",
      "epoch:39 step:30954 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.001992]\n",
      "epoch:39 step:30955 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:39 step:30956 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.016130]\n",
      "epoch:39 step:30957 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:39 step:30958 [D loss: 0.003329, acc.: 100.00%] [G loss: 0.002530]\n",
      "epoch:39 step:30959 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000474]\n",
      "epoch:39 step:30960 [D loss: 0.000800, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:39 step:30961 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:39 step:30962 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.004987]\n",
      "epoch:39 step:30963 [D loss: 0.000934, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:39 step:30964 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001051]\n",
      "epoch:39 step:30965 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:39 step:30966 [D loss: 0.003975, acc.: 100.00%] [G loss: 0.007858]\n",
      "epoch:39 step:30967 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000778]\n",
      "epoch:39 step:30968 [D loss: 0.020998, acc.: 99.22%] [G loss: 0.000054]\n",
      "epoch:39 step:30969 [D loss: 0.008805, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:39 step:30970 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.004311]\n",
      "epoch:39 step:30971 [D loss: 0.004625, acc.: 100.00%] [G loss: 0.001073]\n",
      "epoch:39 step:30972 [D loss: 0.003073, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:39 step:30973 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.084640]\n",
      "epoch:39 step:30974 [D loss: 0.039262, acc.: 100.00%] [G loss: 1.085757]\n",
      "epoch:39 step:30975 [D loss: 0.004149, acc.: 100.00%] [G loss: 0.008911]\n",
      "epoch:39 step:30976 [D loss: 0.005374, acc.: 100.00%] [G loss: 0.001351]\n",
      "epoch:39 step:30977 [D loss: 0.002853, acc.: 100.00%] [G loss: 0.051382]\n",
      "epoch:39 step:30978 [D loss: 0.005384, acc.: 100.00%] [G loss: 0.004483]\n",
      "epoch:39 step:30979 [D loss: 0.016999, acc.: 99.22%] [G loss: 0.000204]\n",
      "epoch:39 step:30980 [D loss: 0.012375, acc.: 100.00%] [G loss: 0.000719]\n",
      "epoch:39 step:30981 [D loss: 0.009396, acc.: 100.00%] [G loss: 0.001879]\n",
      "epoch:39 step:30982 [D loss: 0.001818, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:39 step:30983 [D loss: 0.005050, acc.: 100.00%] [G loss: 0.000537]\n",
      "epoch:39 step:30984 [D loss: 0.004479, acc.: 100.00%] [G loss: 0.000083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:30985 [D loss: 0.038134, acc.: 99.22%] [G loss: 0.000166]\n",
      "epoch:39 step:30986 [D loss: 0.004812, acc.: 100.00%] [G loss: 0.000699]\n",
      "epoch:39 step:30987 [D loss: 0.009520, acc.: 100.00%] [G loss: 0.011082]\n",
      "epoch:39 step:30988 [D loss: 0.008838, acc.: 100.00%] [G loss: 0.007512]\n",
      "epoch:39 step:30989 [D loss: 0.000571, acc.: 100.00%] [G loss: 0.236989]\n",
      "epoch:39 step:30990 [D loss: 0.000971, acc.: 100.00%] [G loss: 0.000345]\n",
      "epoch:39 step:30991 [D loss: 0.007669, acc.: 100.00%] [G loss: 0.001356]\n",
      "epoch:39 step:30992 [D loss: 0.001845, acc.: 100.00%] [G loss: 0.000799]\n",
      "epoch:39 step:30993 [D loss: 0.002482, acc.: 100.00%] [G loss: 0.002262]\n",
      "epoch:39 step:30994 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.002156]\n",
      "epoch:39 step:30995 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:39 step:30996 [D loss: 0.012885, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:39 step:30997 [D loss: 0.000548, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:39 step:30998 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:39 step:30999 [D loss: 0.003269, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:39 step:31000 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:39 step:31001 [D loss: 0.001179, acc.: 100.00%] [G loss: 0.292742]\n",
      "epoch:39 step:31002 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:39 step:31003 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:39 step:31004 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:39 step:31005 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:39 step:31006 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.001890]\n",
      "epoch:39 step:31007 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000583]\n",
      "epoch:39 step:31008 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:39 step:31009 [D loss: 0.002205, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:39 step:31010 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:39 step:31011 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:39 step:31012 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:39 step:31013 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:39 step:31014 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.020205]\n",
      "epoch:39 step:31015 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:39 step:31016 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.006169]\n",
      "epoch:39 step:31017 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:39 step:31018 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:39 step:31019 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.000559]\n",
      "epoch:39 step:31020 [D loss: 0.014711, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:39 step:31021 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.000428]\n",
      "epoch:39 step:31022 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.010294]\n",
      "epoch:39 step:31023 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.001098]\n",
      "epoch:39 step:31024 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.008803]\n",
      "epoch:39 step:31025 [D loss: 0.001337, acc.: 100.00%] [G loss: 0.001569]\n",
      "epoch:39 step:31026 [D loss: 0.003091, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:39 step:31027 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.012834]\n",
      "epoch:39 step:31028 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:39 step:31029 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:39 step:31030 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.003268]\n",
      "epoch:39 step:31031 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.000861]\n",
      "epoch:39 step:31032 [D loss: 0.002221, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:39 step:31033 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:39 step:31034 [D loss: 0.001559, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:39 step:31035 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:39 step:31036 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.001608]\n",
      "epoch:39 step:31037 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:39 step:31038 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.004674]\n",
      "epoch:39 step:31039 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:39 step:31040 [D loss: 0.006999, acc.: 99.22%] [G loss: 0.000215]\n",
      "epoch:39 step:31041 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:39 step:31042 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000479]\n",
      "epoch:39 step:31043 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.002826]\n",
      "epoch:39 step:31044 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:39 step:31045 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:39 step:31046 [D loss: 0.001527, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:39 step:31047 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:39 step:31048 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000614]\n",
      "epoch:39 step:31049 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:39 step:31050 [D loss: 0.000545, acc.: 100.00%] [G loss: 0.007883]\n",
      "epoch:39 step:31051 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:39 step:31052 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.303162]\n",
      "epoch:39 step:31053 [D loss: 0.006582, acc.: 100.00%] [G loss: 0.000415]\n",
      "epoch:39 step:31054 [D loss: 0.000829, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:39 step:31055 [D loss: 0.000561, acc.: 100.00%] [G loss: 0.000635]\n",
      "epoch:39 step:31056 [D loss: 0.001176, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:39 step:31057 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000733]\n",
      "epoch:39 step:31058 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:39 step:31059 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.000422]\n",
      "epoch:39 step:31060 [D loss: 0.003078, acc.: 100.00%] [G loss: 0.004881]\n",
      "epoch:39 step:31061 [D loss: 0.000691, acc.: 100.00%] [G loss: 0.006668]\n",
      "epoch:39 step:31062 [D loss: 0.001166, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:39 step:31063 [D loss: 0.047013, acc.: 99.22%] [G loss: 0.006752]\n",
      "epoch:39 step:31064 [D loss: 0.001836, acc.: 100.00%] [G loss: 0.893228]\n",
      "epoch:39 step:31065 [D loss: 0.092797, acc.: 94.53%] [G loss: 0.000647]\n",
      "epoch:39 step:31066 [D loss: 0.131128, acc.: 95.31%] [G loss: 0.023546]\n",
      "epoch:39 step:31067 [D loss: 0.000689, acc.: 100.00%] [G loss: 1.403919]\n",
      "epoch:39 step:31068 [D loss: 0.108832, acc.: 96.09%] [G loss: 0.139166]\n",
      "epoch:39 step:31069 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.002920]\n",
      "epoch:39 step:31070 [D loss: 0.000708, acc.: 100.00%] [G loss: 0.095308]\n",
      "epoch:39 step:31071 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.062493]\n",
      "epoch:39 step:31072 [D loss: 0.003916, acc.: 100.00%] [G loss: 0.003022]\n",
      "epoch:39 step:31073 [D loss: 0.001349, acc.: 100.00%] [G loss: 0.003050]\n",
      "epoch:39 step:31074 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.003258]\n",
      "epoch:39 step:31075 [D loss: 0.001628, acc.: 100.00%] [G loss: 0.000575]\n",
      "epoch:39 step:31076 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.002413]\n",
      "epoch:39 step:31077 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.007830]\n",
      "epoch:39 step:31078 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000793]\n",
      "epoch:39 step:31079 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000417]\n",
      "epoch:39 step:31080 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.034494]\n",
      "epoch:39 step:31081 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.017153]\n",
      "epoch:39 step:31082 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.001712]\n",
      "epoch:39 step:31083 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.185910]\n",
      "epoch:39 step:31084 [D loss: 0.003943, acc.: 100.00%] [G loss: 0.051081]\n",
      "epoch:39 step:31085 [D loss: 0.000399, acc.: 100.00%] [G loss: 0.000473]\n",
      "epoch:39 step:31086 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:39 step:31087 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:39 step:31088 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:39 step:31089 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:39 step:31090 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.590422]\n",
      "epoch:39 step:31091 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:39 step:31092 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000830]\n",
      "epoch:39 step:31093 [D loss: 0.001226, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:39 step:31094 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.004209]\n",
      "epoch:39 step:31095 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:39 step:31096 [D loss: 0.004346, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:39 step:31097 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001627]\n",
      "epoch:39 step:31098 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.002468]\n",
      "epoch:39 step:31099 [D loss: 0.089845, acc.: 96.88%] [G loss: 0.003464]\n",
      "epoch:39 step:31100 [D loss: 0.000293, acc.: 100.00%] [G loss: 4.707376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:31101 [D loss: 0.055277, acc.: 97.66%] [G loss: 0.006571]\n",
      "epoch:39 step:31102 [D loss: 0.001520, acc.: 100.00%] [G loss: 0.002182]\n",
      "epoch:39 step:31103 [D loss: 0.001160, acc.: 100.00%] [G loss: 0.005116]\n",
      "epoch:39 step:31104 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.054448]\n",
      "epoch:39 step:31105 [D loss: 0.067382, acc.: 98.44%] [G loss: 0.000542]\n",
      "epoch:39 step:31106 [D loss: 0.038355, acc.: 99.22%] [G loss: 1.260952]\n",
      "epoch:39 step:31107 [D loss: 0.000803, acc.: 100.00%] [G loss: 0.031228]\n",
      "epoch:39 step:31108 [D loss: 0.058931, acc.: 98.44%] [G loss: 0.000410]\n",
      "epoch:39 step:31109 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.000321]\n",
      "epoch:39 step:31110 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:39 step:31111 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:39 step:31112 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:39 step:31113 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:39 step:31114 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.043386]\n",
      "epoch:39 step:31115 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.026892]\n",
      "epoch:39 step:31116 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:39 step:31117 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:39 step:31118 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:39 step:31119 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:39 step:31120 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:39 step:31121 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:39 step:31122 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.012487]\n",
      "epoch:39 step:31123 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:39 step:31124 [D loss: 0.001181, acc.: 100.00%] [G loss: 0.057707]\n",
      "epoch:39 step:31125 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:39 step:31126 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:39 step:31127 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:39 step:31128 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:39 step:31129 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:39 step:31130 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:39 step:31131 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:39 step:31132 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:39 step:31133 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.002631]\n",
      "epoch:39 step:31134 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:39 step:31135 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:39 step:31136 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:39 step:31137 [D loss: 0.001054, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:39 step:31138 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:39 step:31139 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:39 step:31140 [D loss: 0.004440, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:39 step:31141 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:39 step:31142 [D loss: 0.002134, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:39 step:31143 [D loss: 0.098229, acc.: 97.66%] [G loss: 0.856815]\n",
      "epoch:39 step:31144 [D loss: 0.172186, acc.: 91.41%] [G loss: 0.452050]\n",
      "epoch:39 step:31145 [D loss: 0.001024, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:39 step:31146 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.003586]\n",
      "epoch:39 step:31147 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:39 step:31148 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.009207]\n",
      "epoch:39 step:31149 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001949]\n",
      "epoch:39 step:31150 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000963]\n",
      "epoch:39 step:31151 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:39 step:31152 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.069679]\n",
      "epoch:39 step:31153 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000407]\n",
      "epoch:39 step:31154 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:39 step:31155 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.004533]\n",
      "epoch:39 step:31156 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000383]\n",
      "epoch:39 step:31157 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000939]\n",
      "epoch:39 step:31158 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:39 step:31159 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:39 step:31160 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.010957]\n",
      "epoch:39 step:31161 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:39 step:31162 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.002179]\n",
      "epoch:39 step:31163 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000322]\n",
      "epoch:39 step:31164 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000958]\n",
      "epoch:39 step:31165 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.002304]\n",
      "epoch:39 step:31166 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:39 step:31167 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:39 step:31168 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000286]\n",
      "epoch:39 step:31169 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000444]\n",
      "epoch:39 step:31170 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.002179]\n",
      "epoch:39 step:31171 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:39 step:31172 [D loss: 0.002817, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:39 step:31173 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:39 step:31174 [D loss: 0.001140, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:39 step:31175 [D loss: 0.001638, acc.: 100.00%] [G loss: 0.218078]\n",
      "epoch:39 step:31176 [D loss: 0.073627, acc.: 97.66%] [G loss: 0.360930]\n",
      "epoch:39 step:31177 [D loss: 0.006600, acc.: 100.00%] [G loss: 0.501256]\n",
      "epoch:39 step:31178 [D loss: 0.098718, acc.: 96.09%] [G loss: 0.018901]\n",
      "epoch:39 step:31179 [D loss: 0.004174, acc.: 100.00%] [G loss: 0.193890]\n",
      "epoch:39 step:31180 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.104324]\n",
      "epoch:39 step:31181 [D loss: 0.008874, acc.: 100.00%] [G loss: 0.148675]\n",
      "epoch:39 step:31182 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:39 step:31183 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:39 step:31184 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.041464]\n",
      "epoch:39 step:31185 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.001035]\n",
      "epoch:39 step:31186 [D loss: 0.000365, acc.: 100.00%] [G loss: 0.001656]\n",
      "epoch:39 step:31187 [D loss: 0.001037, acc.: 100.00%] [G loss: 0.001653]\n",
      "epoch:39 step:31188 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.000461]\n",
      "epoch:39 step:31189 [D loss: 0.003310, acc.: 100.00%] [G loss: 0.017360]\n",
      "epoch:39 step:31190 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.003807]\n",
      "epoch:39 step:31191 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:39 step:31192 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.005851]\n",
      "epoch:39 step:31193 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.009983]\n",
      "epoch:39 step:31194 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.005831]\n",
      "epoch:39 step:31195 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:39 step:31196 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.023185]\n",
      "epoch:39 step:31197 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.001009]\n",
      "epoch:39 step:31198 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:39 step:31199 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.002046]\n",
      "epoch:39 step:31200 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.001395]\n",
      "epoch:39 step:31201 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.003303]\n",
      "epoch:39 step:31202 [D loss: 0.012348, acc.: 99.22%] [G loss: 0.000239]\n",
      "epoch:39 step:31203 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.003834]\n",
      "epoch:39 step:31204 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:39 step:31205 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:39 step:31206 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:39 step:31207 [D loss: 0.002077, acc.: 100.00%] [G loss: 0.001453]\n",
      "epoch:39 step:31208 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:39 step:31209 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:39 step:31210 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.002079]\n",
      "epoch:39 step:31211 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000710]\n",
      "epoch:39 step:31212 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000773]\n",
      "epoch:39 step:31213 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.003734]\n",
      "epoch:39 step:31214 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000554]\n",
      "epoch:39 step:31215 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000397]\n",
      "epoch:39 step:31216 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:39 step:31217 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:39 step:31218 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:39 step:31219 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.002660]\n",
      "epoch:39 step:31220 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:39 step:31221 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:39 step:31222 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:39 step:31223 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:39 step:31224 [D loss: 0.032562, acc.: 100.00%] [G loss: 0.004859]\n",
      "epoch:39 step:31225 [D loss: 0.009814, acc.: 99.22%] [G loss: 0.000039]\n",
      "epoch:39 step:31226 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.077990]\n",
      "epoch:39 step:31227 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:39 step:31228 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.015738]\n",
      "epoch:39 step:31229 [D loss: 0.000906, acc.: 100.00%] [G loss: 0.077895]\n",
      "epoch:39 step:31230 [D loss: 0.322415, acc.: 86.72%] [G loss: 0.000000]\n",
      "epoch:39 step:31231 [D loss: 2.720105, acc.: 54.69%] [G loss: 5.538534]\n",
      "epoch:39 step:31232 [D loss: 0.854978, acc.: 71.09%] [G loss: 0.160429]\n",
      "epoch:39 step:31233 [D loss: 0.045604, acc.: 98.44%] [G loss: 0.219761]\n",
      "epoch:39 step:31234 [D loss: 0.013671, acc.: 100.00%] [G loss: 0.833643]\n",
      "epoch:39 step:31235 [D loss: 0.020081, acc.: 100.00%] [G loss: 0.078575]\n",
      "epoch:39 step:31236 [D loss: 0.083622, acc.: 96.88%] [G loss: 0.187751]\n",
      "epoch:39 step:31237 [D loss: 0.005995, acc.: 100.00%] [G loss: 0.194682]\n",
      "epoch:39 step:31238 [D loss: 0.005170, acc.: 100.00%] [G loss: 0.029093]\n",
      "epoch:39 step:31239 [D loss: 0.000961, acc.: 100.00%] [G loss: 0.021970]\n",
      "epoch:39 step:31240 [D loss: 0.002078, acc.: 100.00%] [G loss: 0.005191]\n",
      "epoch:40 step:31241 [D loss: 0.022844, acc.: 98.44%] [G loss: 0.007437]\n",
      "epoch:40 step:31242 [D loss: 0.002476, acc.: 100.00%] [G loss: 0.133708]\n",
      "epoch:40 step:31243 [D loss: 0.004924, acc.: 100.00%] [G loss: 0.022001]\n",
      "epoch:40 step:31244 [D loss: 0.000813, acc.: 100.00%] [G loss: 0.042659]\n",
      "epoch:40 step:31245 [D loss: 0.006243, acc.: 100.00%] [G loss: 0.003232]\n",
      "epoch:40 step:31246 [D loss: 0.000774, acc.: 100.00%] [G loss: 0.005601]\n",
      "epoch:40 step:31247 [D loss: 0.001400, acc.: 100.00%] [G loss: 0.006934]\n",
      "epoch:40 step:31248 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.000512]\n",
      "epoch:40 step:31249 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.000430]\n",
      "epoch:40 step:31250 [D loss: 0.003676, acc.: 100.00%] [G loss: 0.013161]\n",
      "epoch:40 step:31251 [D loss: 0.003061, acc.: 100.00%] [G loss: 0.007380]\n",
      "epoch:40 step:31252 [D loss: 0.009232, acc.: 100.00%] [G loss: 0.034118]\n",
      "epoch:40 step:31253 [D loss: 0.006566, acc.: 100.00%] [G loss: 0.001539]\n",
      "epoch:40 step:31254 [D loss: 0.006955, acc.: 100.00%] [G loss: 0.001774]\n",
      "epoch:40 step:31255 [D loss: 0.006182, acc.: 100.00%] [G loss: 0.001225]\n",
      "epoch:40 step:31256 [D loss: 0.005158, acc.: 100.00%] [G loss: 0.002896]\n",
      "epoch:40 step:31257 [D loss: 0.003737, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:40 step:31258 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:40 step:31259 [D loss: 0.000289, acc.: 100.00%] [G loss: 0.010997]\n",
      "epoch:40 step:31260 [D loss: 0.005024, acc.: 100.00%] [G loss: 0.002159]\n",
      "epoch:40 step:31261 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.000744]\n",
      "epoch:40 step:31262 [D loss: 0.088549, acc.: 96.09%] [G loss: 0.418462]\n",
      "epoch:40 step:31263 [D loss: 0.027284, acc.: 99.22%] [G loss: 0.389159]\n",
      "epoch:40 step:31264 [D loss: 0.045883, acc.: 98.44%] [G loss: 0.129557]\n",
      "epoch:40 step:31265 [D loss: 0.102705, acc.: 95.31%] [G loss: 0.252910]\n",
      "epoch:40 step:31266 [D loss: 0.004263, acc.: 100.00%] [G loss: 0.052611]\n",
      "epoch:40 step:31267 [D loss: 0.140610, acc.: 95.31%] [G loss: 2.022422]\n",
      "epoch:40 step:31268 [D loss: 1.036310, acc.: 61.72%] [G loss: 0.001366]\n",
      "epoch:40 step:31269 [D loss: 1.540621, acc.: 63.28%] [G loss: 3.017021]\n",
      "epoch:40 step:31270 [D loss: 0.161029, acc.: 92.19%] [G loss: 4.445066]\n",
      "epoch:40 step:31271 [D loss: 1.464473, acc.: 57.03%] [G loss: 1.262178]\n",
      "epoch:40 step:31272 [D loss: 0.495650, acc.: 80.47%] [G loss: 3.401184]\n",
      "epoch:40 step:31273 [D loss: 0.182856, acc.: 94.53%] [G loss: 3.850320]\n",
      "epoch:40 step:31274 [D loss: 0.108016, acc.: 96.09%] [G loss: 3.352720]\n",
      "epoch:40 step:31275 [D loss: 0.151766, acc.: 94.53%] [G loss: 2.084214]\n",
      "epoch:40 step:31276 [D loss: 0.592605, acc.: 67.19%] [G loss: 5.933715]\n",
      "epoch:40 step:31277 [D loss: 0.377162, acc.: 78.12%] [G loss: 0.122675]\n",
      "epoch:40 step:31278 [D loss: 0.217616, acc.: 92.97%] [G loss: 4.134135]\n",
      "epoch:40 step:31279 [D loss: 0.043745, acc.: 98.44%] [G loss: 2.931285]\n",
      "epoch:40 step:31280 [D loss: 0.010413, acc.: 99.22%] [G loss: 2.133155]\n",
      "epoch:40 step:31281 [D loss: 0.073682, acc.: 96.88%] [G loss: 1.134226]\n",
      "epoch:40 step:31282 [D loss: 0.202126, acc.: 91.41%] [G loss: 1.649166]\n",
      "epoch:40 step:31283 [D loss: 0.074742, acc.: 96.09%] [G loss: 2.478268]\n",
      "epoch:40 step:31284 [D loss: 0.277133, acc.: 89.06%] [G loss: 0.148012]\n",
      "epoch:40 step:31285 [D loss: 1.142347, acc.: 65.62%] [G loss: 1.010851]\n",
      "epoch:40 step:31286 [D loss: 0.269642, acc.: 88.28%] [G loss: 6.108510]\n",
      "epoch:40 step:31287 [D loss: 1.048720, acc.: 60.16%] [G loss: 0.039428]\n",
      "epoch:40 step:31288 [D loss: 0.026012, acc.: 99.22%] [G loss: 2.898488]\n",
      "epoch:40 step:31289 [D loss: 0.003917, acc.: 100.00%] [G loss: 1.779541]\n",
      "epoch:40 step:31290 [D loss: 0.007608, acc.: 100.00%] [G loss: 1.153082]\n",
      "epoch:40 step:31291 [D loss: 0.006789, acc.: 100.00%] [G loss: 0.587732]\n",
      "epoch:40 step:31292 [D loss: 0.005571, acc.: 100.00%] [G loss: 0.293325]\n",
      "epoch:40 step:31293 [D loss: 0.008062, acc.: 100.00%] [G loss: 0.248383]\n",
      "epoch:40 step:31294 [D loss: 0.013697, acc.: 100.00%] [G loss: 0.056818]\n",
      "epoch:40 step:31295 [D loss: 0.036713, acc.: 100.00%] [G loss: 0.122517]\n",
      "epoch:40 step:31296 [D loss: 0.011522, acc.: 100.00%] [G loss: 0.002924]\n",
      "epoch:40 step:31297 [D loss: 0.007738, acc.: 100.00%] [G loss: 0.048670]\n",
      "epoch:40 step:31298 [D loss: 0.005331, acc.: 100.00%] [G loss: 0.049382]\n",
      "epoch:40 step:31299 [D loss: 0.026685, acc.: 100.00%] [G loss: 0.044826]\n",
      "epoch:40 step:31300 [D loss: 0.032036, acc.: 100.00%] [G loss: 0.001471]\n",
      "epoch:40 step:31301 [D loss: 0.004826, acc.: 100.00%] [G loss: 0.181199]\n",
      "epoch:40 step:31302 [D loss: 0.001523, acc.: 100.00%] [G loss: 0.134051]\n",
      "epoch:40 step:31303 [D loss: 0.006709, acc.: 100.00%] [G loss: 0.027084]\n",
      "epoch:40 step:31304 [D loss: 0.009477, acc.: 100.00%] [G loss: 0.000545]\n",
      "epoch:40 step:31305 [D loss: 0.037931, acc.: 99.22%] [G loss: 0.010524]\n",
      "epoch:40 step:31306 [D loss: 0.004510, acc.: 100.00%] [G loss: 0.004440]\n",
      "epoch:40 step:31307 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.019421]\n",
      "epoch:40 step:31308 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.005882]\n",
      "epoch:40 step:31309 [D loss: 0.001622, acc.: 100.00%] [G loss: 0.000607]\n",
      "epoch:40 step:31310 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.000566]\n",
      "epoch:40 step:31311 [D loss: 0.001773, acc.: 100.00%] [G loss: 0.005583]\n",
      "epoch:40 step:31312 [D loss: 0.001051, acc.: 100.00%] [G loss: 0.005850]\n",
      "epoch:40 step:31313 [D loss: 0.002400, acc.: 100.00%] [G loss: 0.006259]\n",
      "epoch:40 step:31314 [D loss: 0.001429, acc.: 100.00%] [G loss: 0.005803]\n",
      "epoch:40 step:31315 [D loss: 0.005758, acc.: 100.00%] [G loss: 0.026691]\n",
      "epoch:40 step:31316 [D loss: 0.012041, acc.: 100.00%] [G loss: 0.024284]\n",
      "epoch:40 step:31317 [D loss: 0.084405, acc.: 97.66%] [G loss: 0.009435]\n",
      "epoch:40 step:31318 [D loss: 0.001498, acc.: 100.00%] [G loss: 0.026874]\n",
      "epoch:40 step:31319 [D loss: 0.024670, acc.: 99.22%] [G loss: 0.026232]\n",
      "epoch:40 step:31320 [D loss: 0.003628, acc.: 100.00%] [G loss: 0.000649]\n",
      "epoch:40 step:31321 [D loss: 0.002156, acc.: 100.00%] [G loss: 0.002894]\n",
      "epoch:40 step:31322 [D loss: 0.001055, acc.: 100.00%] [G loss: 0.011367]\n",
      "epoch:40 step:31323 [D loss: 0.004742, acc.: 100.00%] [G loss: 0.019716]\n",
      "epoch:40 step:31324 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.020711]\n",
      "epoch:40 step:31325 [D loss: 0.001078, acc.: 100.00%] [G loss: 0.007392]\n",
      "epoch:40 step:31326 [D loss: 0.000719, acc.: 100.00%] [G loss: 0.000626]\n",
      "epoch:40 step:31327 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.009379]\n",
      "epoch:40 step:31328 [D loss: 0.009098, acc.: 99.22%] [G loss: 0.008070]\n",
      "epoch:40 step:31329 [D loss: 0.002641, acc.: 100.00%] [G loss: 0.012089]\n",
      "epoch:40 step:31330 [D loss: 0.016613, acc.: 100.00%] [G loss: 0.049321]\n",
      "epoch:40 step:31331 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.004208]\n",
      "epoch:40 step:31332 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.006314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31333 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.001988]\n",
      "epoch:40 step:31334 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.006041]\n",
      "epoch:40 step:31335 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.010437]\n",
      "epoch:40 step:31336 [D loss: 0.015285, acc.: 100.00%] [G loss: 0.001886]\n",
      "epoch:40 step:31337 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.000637]\n",
      "epoch:40 step:31338 [D loss: 0.004662, acc.: 100.00%] [G loss: 0.006492]\n",
      "epoch:40 step:31339 [D loss: 0.013596, acc.: 99.22%] [G loss: 0.003394]\n",
      "epoch:40 step:31340 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.001335]\n",
      "epoch:40 step:31341 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.007140]\n",
      "epoch:40 step:31342 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.002157]\n",
      "epoch:40 step:31343 [D loss: 0.003744, acc.: 100.00%] [G loss: 0.003375]\n",
      "epoch:40 step:31344 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.005440]\n",
      "epoch:40 step:31345 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:40 step:31346 [D loss: 0.000648, acc.: 100.00%] [G loss: 0.001934]\n",
      "epoch:40 step:31347 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.001789]\n",
      "epoch:40 step:31348 [D loss: 0.000515, acc.: 100.00%] [G loss: 0.003645]\n",
      "epoch:40 step:31349 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.005061]\n",
      "epoch:40 step:31350 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.002166]\n",
      "epoch:40 step:31351 [D loss: 0.001364, acc.: 100.00%] [G loss: 0.001020]\n",
      "epoch:40 step:31352 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.002641]\n",
      "epoch:40 step:31353 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.001218]\n",
      "epoch:40 step:31354 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.001084]\n",
      "epoch:40 step:31355 [D loss: 0.000650, acc.: 100.00%] [G loss: 0.003121]\n",
      "epoch:40 step:31356 [D loss: 0.001509, acc.: 100.00%] [G loss: 0.001156]\n",
      "epoch:40 step:31357 [D loss: 0.000636, acc.: 100.00%] [G loss: 0.001237]\n",
      "epoch:40 step:31358 [D loss: 0.001568, acc.: 100.00%] [G loss: 0.003085]\n",
      "epoch:40 step:31359 [D loss: 0.160076, acc.: 95.31%] [G loss: 0.017361]\n",
      "epoch:40 step:31360 [D loss: 0.002261, acc.: 100.00%] [G loss: 1.006127]\n",
      "epoch:40 step:31361 [D loss: 0.015487, acc.: 100.00%] [G loss: 0.096719]\n",
      "epoch:40 step:31362 [D loss: 0.008030, acc.: 100.00%] [G loss: 1.881449]\n",
      "epoch:40 step:31363 [D loss: 0.048215, acc.: 98.44%] [G loss: 0.175574]\n",
      "epoch:40 step:31364 [D loss: 0.197665, acc.: 91.41%] [G loss: 0.000905]\n",
      "epoch:40 step:31365 [D loss: 0.002486, acc.: 100.00%] [G loss: 0.011534]\n",
      "epoch:40 step:31366 [D loss: 0.010601, acc.: 99.22%] [G loss: 0.000003]\n",
      "epoch:40 step:31367 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.001496]\n",
      "epoch:40 step:31368 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:40 step:31369 [D loss: 0.035377, acc.: 99.22%] [G loss: 0.001920]\n",
      "epoch:40 step:31370 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:40 step:31371 [D loss: 0.057895, acc.: 98.44%] [G loss: 0.003709]\n",
      "epoch:40 step:31372 [D loss: 0.000670, acc.: 100.00%] [G loss: 0.007419]\n",
      "epoch:40 step:31373 [D loss: 0.002377, acc.: 100.00%] [G loss: 0.014536]\n",
      "epoch:40 step:31374 [D loss: 0.000683, acc.: 100.00%] [G loss: 0.007745]\n",
      "epoch:40 step:31375 [D loss: 0.001398, acc.: 100.00%] [G loss: 0.039967]\n",
      "epoch:40 step:31376 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.009981]\n",
      "epoch:40 step:31377 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.000827]\n",
      "epoch:40 step:31378 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.006964]\n",
      "epoch:40 step:31379 [D loss: 0.002359, acc.: 100.00%] [G loss: 0.000849]\n",
      "epoch:40 step:31380 [D loss: 0.000899, acc.: 100.00%] [G loss: 0.002168]\n",
      "epoch:40 step:31381 [D loss: 0.000975, acc.: 100.00%] [G loss: 0.004675]\n",
      "epoch:40 step:31382 [D loss: 0.001568, acc.: 100.00%] [G loss: 0.001989]\n",
      "epoch:40 step:31383 [D loss: 0.001566, acc.: 100.00%] [G loss: 0.007508]\n",
      "epoch:40 step:31384 [D loss: 0.001156, acc.: 100.00%] [G loss: 0.007052]\n",
      "epoch:40 step:31385 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.007314]\n",
      "epoch:40 step:31386 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.005232]\n",
      "epoch:40 step:31387 [D loss: 0.000701, acc.: 100.00%] [G loss: 0.013271]\n",
      "epoch:40 step:31388 [D loss: 0.000873, acc.: 100.00%] [G loss: 0.006514]\n",
      "epoch:40 step:31389 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.002111]\n",
      "epoch:40 step:31390 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.001886]\n",
      "epoch:40 step:31391 [D loss: 0.000811, acc.: 100.00%] [G loss: 0.003069]\n",
      "epoch:40 step:31392 [D loss: 0.017356, acc.: 100.00%] [G loss: 0.006799]\n",
      "epoch:40 step:31393 [D loss: 0.001652, acc.: 100.00%] [G loss: 0.000414]\n",
      "epoch:40 step:31394 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.006915]\n",
      "epoch:40 step:31395 [D loss: 0.001266, acc.: 100.00%] [G loss: 0.014126]\n",
      "epoch:40 step:31396 [D loss: 0.088091, acc.: 98.44%] [G loss: 0.046537]\n",
      "epoch:40 step:31397 [D loss: 0.001699, acc.: 100.00%] [G loss: 0.354765]\n",
      "epoch:40 step:31398 [D loss: 0.041578, acc.: 99.22%] [G loss: 0.097505]\n",
      "epoch:40 step:31399 [D loss: 0.004756, acc.: 100.00%] [G loss: 0.067680]\n",
      "epoch:40 step:31400 [D loss: 0.024632, acc.: 100.00%] [G loss: 0.027149]\n",
      "epoch:40 step:31401 [D loss: 0.015022, acc.: 99.22%] [G loss: 0.019291]\n",
      "epoch:40 step:31402 [D loss: 0.115690, acc.: 96.88%] [G loss: 0.678478]\n",
      "epoch:40 step:31403 [D loss: 0.006176, acc.: 100.00%] [G loss: 2.555725]\n",
      "epoch:40 step:31404 [D loss: 0.040626, acc.: 99.22%] [G loss: 1.573481]\n",
      "epoch:40 step:31405 [D loss: 0.085831, acc.: 100.00%] [G loss: 1.307090]\n",
      "epoch:40 step:31406 [D loss: 0.377704, acc.: 83.59%] [G loss: 2.119784]\n",
      "epoch:40 step:31407 [D loss: 0.337038, acc.: 84.38%] [G loss: 1.458815]\n",
      "epoch:40 step:31408 [D loss: 0.256416, acc.: 87.50%] [G loss: 4.328500]\n",
      "epoch:40 step:31409 [D loss: 0.116285, acc.: 94.53%] [G loss: 4.637053]\n",
      "epoch:40 step:31410 [D loss: 0.234043, acc.: 89.06%] [G loss: 2.836238]\n",
      "epoch:40 step:31411 [D loss: 0.508294, acc.: 82.03%] [G loss: 1.175575]\n",
      "epoch:40 step:31412 [D loss: 0.096484, acc.: 95.31%] [G loss: 6.422478]\n",
      "epoch:40 step:31413 [D loss: 0.276385, acc.: 89.84%] [G loss: 4.226940]\n",
      "epoch:40 step:31414 [D loss: 0.083384, acc.: 97.66%] [G loss: 3.302281]\n",
      "epoch:40 step:31415 [D loss: 0.014512, acc.: 100.00%] [G loss: 2.482605]\n",
      "epoch:40 step:31416 [D loss: 0.018902, acc.: 99.22%] [G loss: 1.764717]\n",
      "epoch:40 step:31417 [D loss: 0.043062, acc.: 98.44%] [G loss: 1.542150]\n",
      "epoch:40 step:31418 [D loss: 0.216169, acc.: 93.75%] [G loss: 2.126989]\n",
      "epoch:40 step:31419 [D loss: 0.199738, acc.: 89.84%] [G loss: 1.487287]\n",
      "epoch:40 step:31420 [D loss: 0.341147, acc.: 88.28%] [G loss: 0.313827]\n",
      "epoch:40 step:31421 [D loss: 0.116002, acc.: 96.88%] [G loss: 0.022043]\n",
      "epoch:40 step:31422 [D loss: 0.019188, acc.: 99.22%] [G loss: 3.515392]\n",
      "epoch:40 step:31423 [D loss: 0.008901, acc.: 100.00%] [G loss: 3.030630]\n",
      "epoch:40 step:31424 [D loss: 0.015724, acc.: 100.00%] [G loss: 2.448349]\n",
      "epoch:40 step:31425 [D loss: 0.012794, acc.: 99.22%] [G loss: 3.263416]\n",
      "epoch:40 step:31426 [D loss: 0.016837, acc.: 99.22%] [G loss: 0.968933]\n",
      "epoch:40 step:31427 [D loss: 0.012020, acc.: 100.00%] [G loss: 0.002669]\n",
      "epoch:40 step:31428 [D loss: 0.005504, acc.: 100.00%] [G loss: 0.484161]\n",
      "epoch:40 step:31429 [D loss: 0.002660, acc.: 100.00%] [G loss: 0.248527]\n",
      "epoch:40 step:31430 [D loss: 0.004639, acc.: 100.00%] [G loss: 0.093491]\n",
      "epoch:40 step:31431 [D loss: 0.014043, acc.: 100.00%] [G loss: 0.079360]\n",
      "epoch:40 step:31432 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.045447]\n",
      "epoch:40 step:31433 [D loss: 0.002969, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:40 step:31434 [D loss: 0.001228, acc.: 100.00%] [G loss: 0.130231]\n",
      "epoch:40 step:31435 [D loss: 0.018465, acc.: 99.22%] [G loss: 0.024416]\n",
      "epoch:40 step:31436 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.001239]\n",
      "epoch:40 step:31437 [D loss: 0.145524, acc.: 92.97%] [G loss: 0.114453]\n",
      "epoch:40 step:31438 [D loss: 0.016771, acc.: 100.00%] [G loss: 1.096602]\n",
      "epoch:40 step:31439 [D loss: 0.069249, acc.: 99.22%] [G loss: 0.160849]\n",
      "epoch:40 step:31440 [D loss: 0.014158, acc.: 100.00%] [G loss: 0.282538]\n",
      "epoch:40 step:31441 [D loss: 0.041410, acc.: 100.00%] [G loss: 0.232777]\n",
      "epoch:40 step:31442 [D loss: 0.015280, acc.: 100.00%] [G loss: 0.201722]\n",
      "epoch:40 step:31443 [D loss: 0.006688, acc.: 100.00%] [G loss: 0.089489]\n",
      "epoch:40 step:31444 [D loss: 0.017669, acc.: 100.00%] [G loss: 0.030989]\n",
      "epoch:40 step:31445 [D loss: 0.009508, acc.: 100.00%] [G loss: 0.049350]\n",
      "epoch:40 step:31446 [D loss: 0.010024, acc.: 100.00%] [G loss: 0.061593]\n",
      "epoch:40 step:31447 [D loss: 0.005985, acc.: 100.00%] [G loss: 0.012232]\n",
      "epoch:40 step:31448 [D loss: 0.016566, acc.: 99.22%] [G loss: 0.024668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31449 [D loss: 0.018281, acc.: 100.00%] [G loss: 0.021495]\n",
      "epoch:40 step:31450 [D loss: 0.035706, acc.: 99.22%] [G loss: 0.009350]\n",
      "epoch:40 step:31451 [D loss: 0.001016, acc.: 100.00%] [G loss: 0.009606]\n",
      "epoch:40 step:31452 [D loss: 0.003190, acc.: 100.00%] [G loss: 0.002673]\n",
      "epoch:40 step:31453 [D loss: 0.009804, acc.: 100.00%] [G loss: 0.007643]\n",
      "epoch:40 step:31454 [D loss: 0.006352, acc.: 100.00%] [G loss: 0.012456]\n",
      "epoch:40 step:31455 [D loss: 0.017477, acc.: 100.00%] [G loss: 0.015293]\n",
      "epoch:40 step:31456 [D loss: 0.009002, acc.: 100.00%] [G loss: 0.086819]\n",
      "epoch:40 step:31457 [D loss: 0.038530, acc.: 100.00%] [G loss: 0.019134]\n",
      "epoch:40 step:31458 [D loss: 0.060078, acc.: 96.88%] [G loss: 0.133900]\n",
      "epoch:40 step:31459 [D loss: 0.008170, acc.: 100.00%] [G loss: 0.042039]\n",
      "epoch:40 step:31460 [D loss: 0.003208, acc.: 100.00%] [G loss: 0.029958]\n",
      "epoch:40 step:31461 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.007226]\n",
      "epoch:40 step:31462 [D loss: 0.001543, acc.: 100.00%] [G loss: 0.011441]\n",
      "epoch:40 step:31463 [D loss: 0.001492, acc.: 100.00%] [G loss: 0.012828]\n",
      "epoch:40 step:31464 [D loss: 0.000545, acc.: 100.00%] [G loss: 0.018767]\n",
      "epoch:40 step:31465 [D loss: 0.000921, acc.: 100.00%] [G loss: 0.007675]\n",
      "epoch:40 step:31466 [D loss: 0.015434, acc.: 100.00%] [G loss: 0.006842]\n",
      "epoch:40 step:31467 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.008139]\n",
      "epoch:40 step:31468 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.011526]\n",
      "epoch:40 step:31469 [D loss: 0.003038, acc.: 100.00%] [G loss: 0.004200]\n",
      "epoch:40 step:31470 [D loss: 0.013799, acc.: 100.00%] [G loss: 0.003028]\n",
      "epoch:40 step:31471 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.002628]\n",
      "epoch:40 step:31472 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.002086]\n",
      "epoch:40 step:31473 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.002457]\n",
      "epoch:40 step:31474 [D loss: 0.001009, acc.: 100.00%] [G loss: 0.035055]\n",
      "epoch:40 step:31475 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.004649]\n",
      "epoch:40 step:31476 [D loss: 0.001966, acc.: 100.00%] [G loss: 0.002092]\n",
      "epoch:40 step:31477 [D loss: 0.006424, acc.: 100.00%] [G loss: 0.006749]\n",
      "epoch:40 step:31478 [D loss: 0.000622, acc.: 100.00%] [G loss: 0.001607]\n",
      "epoch:40 step:31479 [D loss: 0.006632, acc.: 100.00%] [G loss: 0.042017]\n",
      "epoch:40 step:31480 [D loss: 0.058109, acc.: 98.44%] [G loss: 0.003311]\n",
      "epoch:40 step:31481 [D loss: 0.044676, acc.: 100.00%] [G loss: 1.211929]\n",
      "epoch:40 step:31482 [D loss: 0.001627, acc.: 100.00%] [G loss: 0.049789]\n",
      "epoch:40 step:31483 [D loss: 0.007025, acc.: 100.00%] [G loss: 0.121897]\n",
      "epoch:40 step:31484 [D loss: 0.173131, acc.: 94.53%] [G loss: 1.003446]\n",
      "epoch:40 step:31485 [D loss: 0.036711, acc.: 98.44%] [G loss: 1.165528]\n",
      "epoch:40 step:31486 [D loss: 0.152768, acc.: 93.75%] [G loss: 0.468674]\n",
      "epoch:40 step:31487 [D loss: 0.162400, acc.: 96.09%] [G loss: 0.548864]\n",
      "epoch:40 step:31488 [D loss: 0.012687, acc.: 100.00%] [G loss: 0.871550]\n",
      "epoch:40 step:31489 [D loss: 0.036487, acc.: 100.00%] [G loss: 1.500528]\n",
      "epoch:40 step:31490 [D loss: 0.171035, acc.: 96.88%] [G loss: 2.082334]\n",
      "epoch:40 step:31491 [D loss: 0.019294, acc.: 100.00%] [G loss: 4.274529]\n",
      "epoch:40 step:31492 [D loss: 0.140858, acc.: 95.31%] [G loss: 1.174942]\n",
      "epoch:40 step:31493 [D loss: 0.018378, acc.: 100.00%] [G loss: 4.266490]\n",
      "epoch:40 step:31494 [D loss: 0.004320, acc.: 100.00%] [G loss: 3.465020]\n",
      "epoch:40 step:31495 [D loss: 0.041055, acc.: 98.44%] [G loss: 0.152472]\n",
      "epoch:40 step:31496 [D loss: 0.002190, acc.: 100.00%] [G loss: 2.466918]\n",
      "epoch:40 step:31497 [D loss: 0.000749, acc.: 100.00%] [G loss: 1.043962]\n",
      "epoch:40 step:31498 [D loss: 0.001444, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:40 step:31499 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.001135]\n",
      "epoch:40 step:31500 [D loss: 0.037314, acc.: 99.22%] [G loss: 0.516376]\n",
      "epoch:40 step:31501 [D loss: 0.031800, acc.: 98.44%] [G loss: 3.956420]\n",
      "epoch:40 step:31502 [D loss: 0.009573, acc.: 100.00%] [G loss: 0.205079]\n",
      "epoch:40 step:31503 [D loss: 0.001162, acc.: 100.00%] [G loss: 0.221140]\n",
      "epoch:40 step:31504 [D loss: 0.002879, acc.: 100.00%] [G loss: 0.000993]\n",
      "epoch:40 step:31505 [D loss: 0.054963, acc.: 97.66%] [G loss: 0.005297]\n",
      "epoch:40 step:31506 [D loss: 0.137836, acc.: 94.53%] [G loss: 0.186706]\n",
      "epoch:40 step:31507 [D loss: 0.100323, acc.: 94.53%] [G loss: 0.110122]\n",
      "epoch:40 step:31508 [D loss: 0.009698, acc.: 100.00%] [G loss: 0.159510]\n",
      "epoch:40 step:31509 [D loss: 0.006236, acc.: 100.00%] [G loss: 0.252977]\n",
      "epoch:40 step:31510 [D loss: 0.103420, acc.: 94.53%] [G loss: 0.120646]\n",
      "epoch:40 step:31511 [D loss: 0.000963, acc.: 100.00%] [G loss: 0.001115]\n",
      "epoch:40 step:31512 [D loss: 0.078000, acc.: 99.22%] [G loss: 0.000148]\n",
      "epoch:40 step:31513 [D loss: 0.001664, acc.: 100.00%] [G loss: 0.002105]\n",
      "epoch:40 step:31514 [D loss: 0.000420, acc.: 100.00%] [G loss: 0.000606]\n",
      "epoch:40 step:31515 [D loss: 0.000981, acc.: 100.00%] [G loss: 0.002067]\n",
      "epoch:40 step:31516 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.001693]\n",
      "epoch:40 step:31517 [D loss: 0.023288, acc.: 99.22%] [G loss: 0.002603]\n",
      "epoch:40 step:31518 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.001166]\n",
      "epoch:40 step:31519 [D loss: 0.002026, acc.: 100.00%] [G loss: 0.000853]\n",
      "epoch:40 step:31520 [D loss: 0.001056, acc.: 100.00%] [G loss: 0.000729]\n",
      "epoch:40 step:31521 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.012798]\n",
      "epoch:40 step:31522 [D loss: 0.010909, acc.: 100.00%] [G loss: 0.005013]\n",
      "epoch:40 step:31523 [D loss: 0.006849, acc.: 100.00%] [G loss: 0.000828]\n",
      "epoch:40 step:31524 [D loss: 0.363346, acc.: 82.03%] [G loss: 3.291004]\n",
      "epoch:40 step:31525 [D loss: 0.616425, acc.: 75.78%] [G loss: 0.001462]\n",
      "epoch:40 step:31526 [D loss: 0.072705, acc.: 96.09%] [G loss: 0.034893]\n",
      "epoch:40 step:31527 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.026722]\n",
      "epoch:40 step:31528 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.028298]\n",
      "epoch:40 step:31529 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.002578]\n",
      "epoch:40 step:31530 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.007376]\n",
      "epoch:40 step:31531 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:40 step:31532 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.042862]\n",
      "epoch:40 step:31533 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.003466]\n",
      "epoch:40 step:31534 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.003237]\n",
      "epoch:40 step:31535 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31536 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.002705]\n",
      "epoch:40 step:31537 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:40 step:31538 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31539 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.007670]\n",
      "epoch:40 step:31540 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:40 step:31541 [D loss: 0.019311, acc.: 99.22%] [G loss: 0.007825]\n",
      "epoch:40 step:31542 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.003437]\n",
      "epoch:40 step:31543 [D loss: 0.000880, acc.: 100.00%] [G loss: 0.005720]\n",
      "epoch:40 step:31544 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:40 step:31545 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:40 step:31546 [D loss: 0.006828, acc.: 100.00%] [G loss: 0.006254]\n",
      "epoch:40 step:31547 [D loss: 0.000544, acc.: 100.00%] [G loss: 0.005819]\n",
      "epoch:40 step:31548 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.004248]\n",
      "epoch:40 step:31549 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.010711]\n",
      "epoch:40 step:31550 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:40 step:31551 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.004213]\n",
      "epoch:40 step:31552 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.002385]\n",
      "epoch:40 step:31553 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.005828]\n",
      "epoch:40 step:31554 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.011842]\n",
      "epoch:40 step:31555 [D loss: 0.086502, acc.: 96.88%] [G loss: 0.000104]\n",
      "epoch:40 step:31556 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31557 [D loss: 0.000594, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:40 step:31558 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31559 [D loss: 0.000881, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:40 step:31560 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:40 step:31561 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.001604]\n",
      "epoch:40 step:31562 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:40 step:31563 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:40 step:31564 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31565 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:40 step:31566 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:40 step:31567 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31568 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.007960]\n",
      "epoch:40 step:31569 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.001297]\n",
      "epoch:40 step:31570 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.001183]\n",
      "epoch:40 step:31571 [D loss: 0.000873, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:40 step:31572 [D loss: 0.000878, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:40 step:31573 [D loss: 0.000926, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31574 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:40 step:31575 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31576 [D loss: 0.002630, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:40 step:31577 [D loss: 0.000682, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:40 step:31578 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31579 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:40 step:31580 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31581 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:40 step:31582 [D loss: 0.000936, acc.: 100.00%] [G loss: 0.003341]\n",
      "epoch:40 step:31583 [D loss: 0.007050, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:40 step:31584 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000919]\n",
      "epoch:40 step:31585 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:40 step:31586 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31587 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:40 step:31588 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:40 step:31589 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31590 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:40 step:31591 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:40 step:31592 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:40 step:31593 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:40 step:31594 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:40 step:31595 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:40 step:31596 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000431]\n",
      "epoch:40 step:31597 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:40 step:31598 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:40 step:31599 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:40 step:31600 [D loss: 0.009156, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:40 step:31601 [D loss: 0.013217, acc.: 100.00%] [G loss: 0.000499]\n",
      "epoch:40 step:31602 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.002343]\n",
      "epoch:40 step:31603 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001490]\n",
      "epoch:40 step:31604 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000635]\n",
      "epoch:40 step:31605 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:40 step:31606 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:40 step:31607 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:40 step:31608 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.023911]\n",
      "epoch:40 step:31609 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.000717]\n",
      "epoch:40 step:31610 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:40 step:31611 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.000677]\n",
      "epoch:40 step:31612 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:40 step:31613 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.004723]\n",
      "epoch:40 step:31614 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.032836]\n",
      "epoch:40 step:31615 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:40 step:31616 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.002957]\n",
      "epoch:40 step:31617 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000716]\n",
      "epoch:40 step:31618 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:40 step:31619 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000395]\n",
      "epoch:40 step:31620 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.022186]\n",
      "epoch:40 step:31621 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000548]\n",
      "epoch:40 step:31622 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:40 step:31623 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:40 step:31624 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:40 step:31625 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.000638]\n",
      "epoch:40 step:31626 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:40 step:31627 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000311]\n",
      "epoch:40 step:31628 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:40 step:31629 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001679]\n",
      "epoch:40 step:31630 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:40 step:31631 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:40 step:31632 [D loss: 0.000767, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:40 step:31633 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.001687]\n",
      "epoch:40 step:31634 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000638]\n",
      "epoch:40 step:31635 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000793]\n",
      "epoch:40 step:31636 [D loss: 0.001880, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:40 step:31637 [D loss: 0.012245, acc.: 100.00%] [G loss: 0.000773]\n",
      "epoch:40 step:31638 [D loss: 0.001039, acc.: 100.00%] [G loss: 0.000471]\n",
      "epoch:40 step:31639 [D loss: 0.567402, acc.: 74.22%] [G loss: 6.324724]\n",
      "epoch:40 step:31640 [D loss: 0.213327, acc.: 91.41%] [G loss: 3.147845]\n",
      "epoch:40 step:31641 [D loss: 0.344384, acc.: 83.59%] [G loss: 0.463755]\n",
      "epoch:40 step:31642 [D loss: 0.033835, acc.: 99.22%] [G loss: 0.140930]\n",
      "epoch:40 step:31643 [D loss: 0.164834, acc.: 92.97%] [G loss: 1.048302]\n",
      "epoch:40 step:31644 [D loss: 0.001119, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:40 step:31645 [D loss: 0.038066, acc.: 98.44%] [G loss: 2.122168]\n",
      "epoch:40 step:31646 [D loss: 0.011220, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:40 step:31647 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.166724]\n",
      "epoch:40 step:31648 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:40 step:31649 [D loss: 0.001424, acc.: 100.00%] [G loss: 0.262968]\n",
      "epoch:40 step:31650 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:40 step:31651 [D loss: 1.063647, acc.: 56.25%] [G loss: 4.952501]\n",
      "epoch:40 step:31652 [D loss: 0.635608, acc.: 75.78%] [G loss: 3.929590]\n",
      "epoch:40 step:31653 [D loss: 0.183084, acc.: 90.62%] [G loss: 1.834354]\n",
      "epoch:40 step:31654 [D loss: 0.008904, acc.: 99.22%] [G loss: 1.014050]\n",
      "epoch:40 step:31655 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.751369]\n",
      "epoch:40 step:31656 [D loss: 0.000717, acc.: 100.00%] [G loss: 0.086514]\n",
      "epoch:40 step:31657 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.318008]\n",
      "epoch:40 step:31658 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.198252]\n",
      "epoch:40 step:31659 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.073462]\n",
      "epoch:40 step:31660 [D loss: 0.001383, acc.: 100.00%] [G loss: 0.012293]\n",
      "epoch:40 step:31661 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.015435]\n",
      "epoch:40 step:31662 [D loss: 0.000314, acc.: 100.00%] [G loss: 0.026431]\n",
      "epoch:40 step:31663 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.020973]\n",
      "epoch:40 step:31664 [D loss: 0.000895, acc.: 100.00%] [G loss: 0.016669]\n",
      "epoch:40 step:31665 [D loss: 0.010870, acc.: 100.00%] [G loss: 0.022706]\n",
      "epoch:40 step:31666 [D loss: 0.011139, acc.: 100.00%] [G loss: 0.094119]\n",
      "epoch:40 step:31667 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.005125]\n",
      "epoch:40 step:31668 [D loss: 0.001261, acc.: 100.00%] [G loss: 0.013902]\n",
      "epoch:40 step:31669 [D loss: 0.000479, acc.: 100.00%] [G loss: 0.008958]\n",
      "epoch:40 step:31670 [D loss: 0.031256, acc.: 98.44%] [G loss: 0.048909]\n",
      "epoch:40 step:31671 [D loss: 0.077871, acc.: 99.22%] [G loss: 0.021689]\n",
      "epoch:40 step:31672 [D loss: 0.003516, acc.: 100.00%] [G loss: 0.031540]\n",
      "epoch:40 step:31673 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.044989]\n",
      "epoch:40 step:31674 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.043953]\n",
      "epoch:40 step:31675 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.010421]\n",
      "epoch:40 step:31676 [D loss: 0.002419, acc.: 100.00%] [G loss: 0.028031]\n",
      "epoch:40 step:31677 [D loss: 0.001321, acc.: 100.00%] [G loss: 0.022784]\n",
      "epoch:40 step:31678 [D loss: 0.116070, acc.: 96.09%] [G loss: 1.678898]\n",
      "epoch:40 step:31679 [D loss: 0.023944, acc.: 100.00%] [G loss: 0.406273]\n",
      "epoch:40 step:31680 [D loss: 0.013172, acc.: 100.00%] [G loss: 0.575562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31681 [D loss: 0.030128, acc.: 99.22%] [G loss: 0.669097]\n",
      "epoch:40 step:31682 [D loss: 0.392020, acc.: 83.59%] [G loss: 4.831197]\n",
      "epoch:40 step:31683 [D loss: 0.765224, acc.: 65.62%] [G loss: 1.280899]\n",
      "epoch:40 step:31684 [D loss: 0.200799, acc.: 89.84%] [G loss: 0.036451]\n",
      "epoch:40 step:31685 [D loss: 0.014362, acc.: 100.00%] [G loss: 3.034411]\n",
      "epoch:40 step:31686 [D loss: 0.027476, acc.: 98.44%] [G loss: 2.129073]\n",
      "epoch:40 step:31687 [D loss: 0.005201, acc.: 100.00%] [G loss: 1.695409]\n",
      "epoch:40 step:31688 [D loss: 0.022466, acc.: 100.00%] [G loss: 0.830456]\n",
      "epoch:40 step:31689 [D loss: 0.009459, acc.: 100.00%] [G loss: 0.018781]\n",
      "epoch:40 step:31690 [D loss: 0.001677, acc.: 100.00%] [G loss: 0.484160]\n",
      "epoch:40 step:31691 [D loss: 0.025964, acc.: 100.00%] [G loss: 0.221454]\n",
      "epoch:40 step:31692 [D loss: 0.003765, acc.: 100.00%] [G loss: 0.005162]\n",
      "epoch:40 step:31693 [D loss: 0.006012, acc.: 100.00%] [G loss: 1.888288]\n",
      "epoch:40 step:31694 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.187417]\n",
      "epoch:40 step:31695 [D loss: 0.002443, acc.: 100.00%] [G loss: 0.056279]\n",
      "epoch:40 step:31696 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.205908]\n",
      "epoch:40 step:31697 [D loss: 0.002599, acc.: 100.00%] [G loss: 0.078205]\n",
      "epoch:40 step:31698 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000510]\n",
      "epoch:40 step:31699 [D loss: 0.001637, acc.: 100.00%] [G loss: 0.049599]\n",
      "epoch:40 step:31700 [D loss: 0.016655, acc.: 99.22%] [G loss: 0.034694]\n",
      "epoch:40 step:31701 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.036785]\n",
      "epoch:40 step:31702 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.003480]\n",
      "epoch:40 step:31703 [D loss: 0.000792, acc.: 100.00%] [G loss: 0.113747]\n",
      "epoch:40 step:31704 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.090814]\n",
      "epoch:40 step:31705 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.024785]\n",
      "epoch:40 step:31706 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.005042]\n",
      "epoch:40 step:31707 [D loss: 0.003135, acc.: 100.00%] [G loss: 0.066393]\n",
      "epoch:40 step:31708 [D loss: 0.016911, acc.: 100.00%] [G loss: 0.002760]\n",
      "epoch:40 step:31709 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.033168]\n",
      "epoch:40 step:31710 [D loss: 0.536843, acc.: 75.78%] [G loss: 0.539551]\n",
      "epoch:40 step:31711 [D loss: 0.227155, acc.: 90.62%] [G loss: 1.545964]\n",
      "epoch:40 step:31712 [D loss: 0.579949, acc.: 75.78%] [G loss: 0.521862]\n",
      "epoch:40 step:31713 [D loss: 0.000914, acc.: 100.00%] [G loss: 0.230086]\n",
      "epoch:40 step:31714 [D loss: 0.000451, acc.: 100.00%] [G loss: 0.215097]\n",
      "epoch:40 step:31715 [D loss: 0.000517, acc.: 100.00%] [G loss: 0.125940]\n",
      "epoch:40 step:31716 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.016401]\n",
      "epoch:40 step:31717 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.011228]\n",
      "epoch:40 step:31718 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.004971]\n",
      "epoch:40 step:31719 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:40 step:31720 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.010700]\n",
      "epoch:40 step:31721 [D loss: 0.000513, acc.: 100.00%] [G loss: 0.002944]\n",
      "epoch:40 step:31722 [D loss: 0.004295, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:40 step:31723 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.019669]\n",
      "epoch:40 step:31724 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:40 step:31725 [D loss: 0.005729, acc.: 100.00%] [G loss: 0.005705]\n",
      "epoch:40 step:31726 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.006334]\n",
      "epoch:40 step:31727 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.009990]\n",
      "epoch:40 step:31728 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.025507]\n",
      "epoch:40 step:31729 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:40 step:31730 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.004817]\n",
      "epoch:40 step:31731 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.021948]\n",
      "epoch:40 step:31732 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.027893]\n",
      "epoch:40 step:31733 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.175859]\n",
      "epoch:40 step:31734 [D loss: 0.000711, acc.: 100.00%] [G loss: 0.006334]\n",
      "epoch:40 step:31735 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:40 step:31736 [D loss: 0.010268, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:40 step:31737 [D loss: 0.403847, acc.: 85.94%] [G loss: 0.351120]\n",
      "epoch:40 step:31738 [D loss: 0.051205, acc.: 97.66%] [G loss: 0.751527]\n",
      "epoch:40 step:31739 [D loss: 0.413601, acc.: 82.03%] [G loss: 0.098784]\n",
      "epoch:40 step:31740 [D loss: 0.038777, acc.: 99.22%] [G loss: 0.278505]\n",
      "epoch:40 step:31741 [D loss: 0.006563, acc.: 100.00%] [G loss: 0.326855]\n",
      "epoch:40 step:31742 [D loss: 0.001971, acc.: 100.00%] [G loss: 0.007940]\n",
      "epoch:40 step:31743 [D loss: 1.103087, acc.: 61.72%] [G loss: 1.244163]\n",
      "epoch:40 step:31744 [D loss: 0.351666, acc.: 85.16%] [G loss: 5.789178]\n",
      "epoch:40 step:31745 [D loss: 0.510110, acc.: 77.34%] [G loss: 0.850920]\n",
      "epoch:40 step:31746 [D loss: 0.170629, acc.: 86.72%] [G loss: 0.834629]\n",
      "epoch:40 step:31747 [D loss: 0.006348, acc.: 100.00%] [G loss: 0.051263]\n",
      "epoch:40 step:31748 [D loss: 0.003541, acc.: 100.00%] [G loss: 1.909129]\n",
      "epoch:40 step:31749 [D loss: 0.039363, acc.: 99.22%] [G loss: 0.017300]\n",
      "epoch:40 step:31750 [D loss: 0.003808, acc.: 100.00%] [G loss: 1.341620]\n",
      "epoch:40 step:31751 [D loss: 0.007980, acc.: 100.00%] [G loss: 0.679253]\n",
      "epoch:40 step:31752 [D loss: 0.086857, acc.: 97.66%] [G loss: 1.190489]\n",
      "epoch:40 step:31753 [D loss: 0.032405, acc.: 98.44%] [G loss: 0.805316]\n",
      "epoch:40 step:31754 [D loss: 0.011872, acc.: 100.00%] [G loss: 0.300994]\n",
      "epoch:40 step:31755 [D loss: 0.011748, acc.: 100.00%] [G loss: 1.014077]\n",
      "epoch:40 step:31756 [D loss: 0.059174, acc.: 99.22%] [G loss: 0.534023]\n",
      "epoch:40 step:31757 [D loss: 0.033334, acc.: 100.00%] [G loss: 0.058161]\n",
      "epoch:40 step:31758 [D loss: 0.010855, acc.: 100.00%] [G loss: 0.086964]\n",
      "epoch:40 step:31759 [D loss: 0.004029, acc.: 100.00%] [G loss: 2.213800]\n",
      "epoch:40 step:31760 [D loss: 0.022975, acc.: 100.00%] [G loss: 0.028810]\n",
      "epoch:40 step:31761 [D loss: 0.006996, acc.: 100.00%] [G loss: 0.019666]\n",
      "epoch:40 step:31762 [D loss: 0.027568, acc.: 99.22%] [G loss: 0.002494]\n",
      "epoch:40 step:31763 [D loss: 0.004303, acc.: 100.00%] [G loss: 0.032574]\n",
      "epoch:40 step:31764 [D loss: 0.027593, acc.: 99.22%] [G loss: 0.171692]\n",
      "epoch:40 step:31765 [D loss: 0.000814, acc.: 100.00%] [G loss: 0.026213]\n",
      "epoch:40 step:31766 [D loss: 0.050446, acc.: 99.22%] [G loss: 0.014363]\n",
      "epoch:40 step:31767 [D loss: 0.026847, acc.: 100.00%] [G loss: 0.001835]\n",
      "epoch:40 step:31768 [D loss: 0.006696, acc.: 100.00%] [G loss: 0.009801]\n",
      "epoch:40 step:31769 [D loss: 0.001047, acc.: 100.00%] [G loss: 0.000767]\n",
      "epoch:40 step:31770 [D loss: 0.001045, acc.: 100.00%] [G loss: 0.014248]\n",
      "epoch:40 step:31771 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.003684]\n",
      "epoch:40 step:31772 [D loss: 0.016839, acc.: 99.22%] [G loss: 0.001277]\n",
      "epoch:40 step:31773 [D loss: 0.000924, acc.: 100.00%] [G loss: 0.309012]\n",
      "epoch:40 step:31774 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.003228]\n",
      "epoch:40 step:31775 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.091781]\n",
      "epoch:40 step:31776 [D loss: 0.001782, acc.: 100.00%] [G loss: 0.077816]\n",
      "epoch:40 step:31777 [D loss: 0.018354, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:40 step:31778 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.000303]\n",
      "epoch:40 step:31779 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:40 step:31780 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.029066]\n",
      "epoch:40 step:31781 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.015440]\n",
      "epoch:40 step:31782 [D loss: 0.003111, acc.: 100.00%] [G loss: 0.022484]\n",
      "epoch:40 step:31783 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.022356]\n",
      "epoch:40 step:31784 [D loss: 0.002296, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:40 step:31785 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.005813]\n",
      "epoch:40 step:31786 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.015878]\n",
      "epoch:40 step:31787 [D loss: 0.032777, acc.: 100.00%] [G loss: 0.017425]\n",
      "epoch:40 step:31788 [D loss: 0.008157, acc.: 100.00%] [G loss: 0.036225]\n",
      "epoch:40 step:31789 [D loss: 0.000522, acc.: 100.00%] [G loss: 1.053049]\n",
      "epoch:40 step:31790 [D loss: 0.021972, acc.: 99.22%] [G loss: 0.029991]\n",
      "epoch:40 step:31791 [D loss: 0.012115, acc.: 100.00%] [G loss: 0.000419]\n",
      "epoch:40 step:31792 [D loss: 0.001095, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:40 step:31793 [D loss: 0.008054, acc.: 100.00%] [G loss: 0.000593]\n",
      "epoch:40 step:31794 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.212061]\n",
      "epoch:40 step:31795 [D loss: 0.003125, acc.: 100.00%] [G loss: 0.000402]\n",
      "epoch:40 step:31796 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31797 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.071648]\n",
      "epoch:40 step:31798 [D loss: 0.003119, acc.: 100.00%] [G loss: 0.000195]\n",
      "epoch:40 step:31799 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.124192]\n",
      "epoch:40 step:31800 [D loss: 0.001512, acc.: 100.00%] [G loss: 0.000576]\n",
      "epoch:40 step:31801 [D loss: 0.001985, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:40 step:31802 [D loss: 0.003305, acc.: 100.00%] [G loss: 0.002113]\n",
      "epoch:40 step:31803 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:40 step:31804 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000432]\n",
      "epoch:40 step:31805 [D loss: 0.014201, acc.: 99.22%] [G loss: 0.001332]\n",
      "epoch:40 step:31806 [D loss: 0.001059, acc.: 100.00%] [G loss: 0.032590]\n",
      "epoch:40 step:31807 [D loss: 0.000758, acc.: 100.00%] [G loss: 0.127555]\n",
      "epoch:40 step:31808 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.001101]\n",
      "epoch:40 step:31809 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.011054]\n",
      "epoch:40 step:31810 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.003743]\n",
      "epoch:40 step:31811 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.013123]\n",
      "epoch:40 step:31812 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:40 step:31813 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.000159]\n",
      "epoch:40 step:31814 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.005498]\n",
      "epoch:40 step:31815 [D loss: 0.004671, acc.: 100.00%] [G loss: 0.000825]\n",
      "epoch:40 step:31816 [D loss: 0.024386, acc.: 99.22%] [G loss: 0.001173]\n",
      "epoch:40 step:31817 [D loss: 0.175818, acc.: 92.19%] [G loss: 0.106344]\n",
      "epoch:40 step:31818 [D loss: 0.020413, acc.: 99.22%] [G loss: 0.825837]\n",
      "epoch:40 step:31819 [D loss: 0.198111, acc.: 89.84%] [G loss: 0.038836]\n",
      "epoch:40 step:31820 [D loss: 0.002083, acc.: 100.00%] [G loss: 0.026346]\n",
      "epoch:40 step:31821 [D loss: 0.610291, acc.: 71.88%] [G loss: 1.926022]\n",
      "epoch:40 step:31822 [D loss: 0.988195, acc.: 65.62%] [G loss: 1.221826]\n",
      "epoch:40 step:31823 [D loss: 0.008778, acc.: 100.00%] [G loss: 0.295650]\n",
      "epoch:40 step:31824 [D loss: 0.009424, acc.: 100.00%] [G loss: 0.133717]\n",
      "epoch:40 step:31825 [D loss: 0.050088, acc.: 100.00%] [G loss: 0.200016]\n",
      "epoch:40 step:31826 [D loss: 0.029467, acc.: 100.00%] [G loss: 0.304673]\n",
      "epoch:40 step:31827 [D loss: 0.027345, acc.: 100.00%] [G loss: 0.490291]\n",
      "epoch:40 step:31828 [D loss: 0.011300, acc.: 100.00%] [G loss: 0.452777]\n",
      "epoch:40 step:31829 [D loss: 0.011916, acc.: 100.00%] [G loss: 0.668144]\n",
      "epoch:40 step:31830 [D loss: 0.001663, acc.: 100.00%] [G loss: 0.169221]\n",
      "epoch:40 step:31831 [D loss: 0.019991, acc.: 100.00%] [G loss: 0.084924]\n",
      "epoch:40 step:31832 [D loss: 0.001496, acc.: 100.00%] [G loss: 0.075970]\n",
      "epoch:40 step:31833 [D loss: 0.007817, acc.: 99.22%] [G loss: 0.016505]\n",
      "epoch:40 step:31834 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.135375]\n",
      "epoch:40 step:31835 [D loss: 0.001255, acc.: 100.00%] [G loss: 0.146697]\n",
      "epoch:40 step:31836 [D loss: 0.002059, acc.: 100.00%] [G loss: 0.018889]\n",
      "epoch:40 step:31837 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.025158]\n",
      "epoch:40 step:31838 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.007829]\n",
      "epoch:40 step:31839 [D loss: 0.001403, acc.: 100.00%] [G loss: 0.027604]\n",
      "epoch:40 step:31840 [D loss: 0.003452, acc.: 100.00%] [G loss: 0.007464]\n",
      "epoch:40 step:31841 [D loss: 0.003885, acc.: 100.00%] [G loss: 0.001522]\n",
      "epoch:40 step:31842 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.001997]\n",
      "epoch:40 step:31843 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.006730]\n",
      "epoch:40 step:31844 [D loss: 0.007854, acc.: 100.00%] [G loss: 0.002933]\n",
      "epoch:40 step:31845 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.000726]\n",
      "epoch:40 step:31846 [D loss: 0.000106, acc.: 100.00%] [G loss: 2.478980]\n",
      "epoch:40 step:31847 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.001500]\n",
      "epoch:40 step:31848 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000303]\n",
      "epoch:40 step:31849 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000721]\n",
      "epoch:40 step:31850 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.001826]\n",
      "epoch:40 step:31851 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001077]\n",
      "epoch:40 step:31852 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000786]\n",
      "epoch:40 step:31853 [D loss: 0.002397, acc.: 100.00%] [G loss: 0.001172]\n",
      "epoch:40 step:31854 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001805]\n",
      "epoch:40 step:31855 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000907]\n",
      "epoch:40 step:31856 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.001658]\n",
      "epoch:40 step:31857 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000966]\n",
      "epoch:40 step:31858 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000216]\n",
      "epoch:40 step:31859 [D loss: 0.006662, acc.: 100.00%] [G loss: 0.000806]\n",
      "epoch:40 step:31860 [D loss: 0.032371, acc.: 99.22%] [G loss: 0.039185]\n",
      "epoch:40 step:31861 [D loss: 0.020822, acc.: 100.00%] [G loss: 0.156686]\n",
      "epoch:40 step:31862 [D loss: 0.009712, acc.: 100.00%] [G loss: 0.152314]\n",
      "epoch:40 step:31863 [D loss: 0.032263, acc.: 98.44%] [G loss: 0.091638]\n",
      "epoch:40 step:31864 [D loss: 0.038860, acc.: 100.00%] [G loss: 0.756270]\n",
      "epoch:40 step:31865 [D loss: 0.022556, acc.: 100.00%] [G loss: 0.084497]\n",
      "epoch:40 step:31866 [D loss: 0.086321, acc.: 98.44%] [G loss: 0.512645]\n",
      "epoch:40 step:31867 [D loss: 0.088311, acc.: 96.88%] [G loss: 4.209117]\n",
      "epoch:40 step:31868 [D loss: 0.091212, acc.: 95.31%] [G loss: 2.517834]\n",
      "epoch:40 step:31869 [D loss: 0.002301, acc.: 100.00%] [G loss: 0.441877]\n",
      "epoch:40 step:31870 [D loss: 0.001019, acc.: 100.00%] [G loss: 0.411960]\n",
      "epoch:40 step:31871 [D loss: 0.032298, acc.: 98.44%] [G loss: 0.351788]\n",
      "epoch:40 step:31872 [D loss: 0.012760, acc.: 100.00%] [G loss: 0.477187]\n",
      "epoch:40 step:31873 [D loss: 0.025766, acc.: 100.00%] [G loss: 1.295451]\n",
      "epoch:40 step:31874 [D loss: 0.111356, acc.: 95.31%] [G loss: 4.700175]\n",
      "epoch:40 step:31875 [D loss: 0.063080, acc.: 96.88%] [G loss: 3.828252]\n",
      "epoch:40 step:31876 [D loss: 0.011751, acc.: 100.00%] [G loss: 4.263559]\n",
      "epoch:40 step:31877 [D loss: 0.300240, acc.: 85.94%] [G loss: 0.128172]\n",
      "epoch:40 step:31878 [D loss: 0.005939, acc.: 100.00%] [G loss: 7.852628]\n",
      "epoch:40 step:31879 [D loss: 0.121940, acc.: 92.97%] [G loss: 5.952276]\n",
      "epoch:40 step:31880 [D loss: 0.000760, acc.: 100.00%] [G loss: 4.486464]\n",
      "epoch:40 step:31881 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.008504]\n",
      "epoch:40 step:31882 [D loss: 0.000034, acc.: 100.00%] [G loss: 2.924678]\n",
      "epoch:40 step:31883 [D loss: 0.000167, acc.: 100.00%] [G loss: 2.982499]\n",
      "epoch:40 step:31884 [D loss: 0.000522, acc.: 100.00%] [G loss: 1.337837]\n",
      "epoch:40 step:31885 [D loss: 0.004733, acc.: 100.00%] [G loss: 2.080182]\n",
      "epoch:40 step:31886 [D loss: 0.011470, acc.: 100.00%] [G loss: 1.780881]\n",
      "epoch:40 step:31887 [D loss: 0.016174, acc.: 100.00%] [G loss: 1.963098]\n",
      "epoch:40 step:31888 [D loss: 0.030321, acc.: 100.00%] [G loss: 1.001888]\n",
      "epoch:40 step:31889 [D loss: 0.008809, acc.: 100.00%] [G loss: 0.030583]\n",
      "epoch:40 step:31890 [D loss: 0.001514, acc.: 100.00%] [G loss: 0.866621]\n",
      "epoch:40 step:31891 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.603878]\n",
      "epoch:40 step:31892 [D loss: 0.003012, acc.: 100.00%] [G loss: 0.588229]\n",
      "epoch:40 step:31893 [D loss: 0.002419, acc.: 100.00%] [G loss: 0.542443]\n",
      "epoch:40 step:31894 [D loss: 0.000451, acc.: 100.00%] [G loss: 0.133482]\n",
      "epoch:40 step:31895 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.124838]\n",
      "epoch:40 step:31896 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.002619]\n",
      "epoch:40 step:31897 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.046678]\n",
      "epoch:40 step:31898 [D loss: 0.001146, acc.: 100.00%] [G loss: 0.010888]\n",
      "epoch:40 step:31899 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.010114]\n",
      "epoch:40 step:31900 [D loss: 0.001021, acc.: 100.00%] [G loss: 0.023061]\n",
      "epoch:40 step:31901 [D loss: 0.001101, acc.: 100.00%] [G loss: 0.055375]\n",
      "epoch:40 step:31902 [D loss: 0.001790, acc.: 100.00%] [G loss: 0.003057]\n",
      "epoch:40 step:31903 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.011752]\n",
      "epoch:40 step:31904 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.001394]\n",
      "epoch:40 step:31905 [D loss: 0.013427, acc.: 99.22%] [G loss: 0.002116]\n",
      "epoch:40 step:31906 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000931]\n",
      "epoch:40 step:31907 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.001753]\n",
      "epoch:40 step:31908 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.001486]\n",
      "epoch:40 step:31909 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.017064]\n",
      "epoch:40 step:31910 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.001393]\n",
      "epoch:40 step:31911 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.001307]\n",
      "epoch:40 step:31912 [D loss: 0.001363, acc.: 100.00%] [G loss: 0.000137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:40 step:31913 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.003183]\n",
      "epoch:40 step:31914 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.003507]\n",
      "epoch:40 step:31915 [D loss: 0.000491, acc.: 100.00%] [G loss: 0.002157]\n",
      "epoch:40 step:31916 [D loss: 0.000882, acc.: 100.00%] [G loss: 0.000674]\n",
      "epoch:40 step:31917 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.002812]\n",
      "epoch:40 step:31918 [D loss: 0.002547, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:40 step:31919 [D loss: 0.000760, acc.: 100.00%] [G loss: 0.002186]\n",
      "epoch:40 step:31920 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:40 step:31921 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:40 step:31922 [D loss: 0.000801, acc.: 100.00%] [G loss: 0.000897]\n",
      "epoch:40 step:31923 [D loss: 0.002449, acc.: 100.00%] [G loss: 0.001987]\n",
      "epoch:40 step:31924 [D loss: 0.002516, acc.: 100.00%] [G loss: 0.010426]\n",
      "epoch:40 step:31925 [D loss: 0.006823, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:40 step:31926 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.001811]\n",
      "epoch:40 step:31927 [D loss: 0.004129, acc.: 100.00%] [G loss: 0.000448]\n",
      "epoch:40 step:31928 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.005263]\n",
      "epoch:40 step:31929 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:40 step:31930 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.000587]\n",
      "epoch:40 step:31931 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000954]\n",
      "epoch:40 step:31932 [D loss: 0.000704, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:40 step:31933 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:40 step:31934 [D loss: 0.001564, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:40 step:31935 [D loss: 0.000391, acc.: 100.00%] [G loss: 0.001429]\n",
      "epoch:40 step:31936 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:40 step:31937 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:40 step:31938 [D loss: 0.000561, acc.: 100.00%] [G loss: 0.000286]\n",
      "epoch:40 step:31939 [D loss: 0.005066, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:40 step:31940 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:40 step:31941 [D loss: 0.001716, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:40 step:31942 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.000498]\n",
      "epoch:40 step:31943 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000888]\n",
      "epoch:40 step:31944 [D loss: 0.005476, acc.: 100.00%] [G loss: 0.000289]\n",
      "epoch:40 step:31945 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.001263]\n",
      "epoch:40 step:31946 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:40 step:31947 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:40 step:31948 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000592]\n",
      "epoch:40 step:31949 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:40 step:31950 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:40 step:31951 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.000899]\n",
      "epoch:40 step:31952 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000959]\n",
      "epoch:40 step:31953 [D loss: 0.004278, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:40 step:31954 [D loss: 0.053706, acc.: 99.22%] [G loss: 0.000247]\n",
      "epoch:40 step:31955 [D loss: 0.011610, acc.: 100.00%] [G loss: 0.012356]\n",
      "epoch:40 step:31956 [D loss: 0.001090, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:40 step:31957 [D loss: 0.001543, acc.: 100.00%] [G loss: 0.038692]\n",
      "epoch:40 step:31958 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.005053]\n",
      "epoch:40 step:31959 [D loss: 0.001463, acc.: 100.00%] [G loss: 0.002458]\n",
      "epoch:40 step:31960 [D loss: 0.000368, acc.: 100.00%] [G loss: 0.007731]\n",
      "epoch:40 step:31961 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.013388]\n",
      "epoch:40 step:31962 [D loss: 0.000987, acc.: 100.00%] [G loss: 0.004386]\n",
      "epoch:40 step:31963 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.006123]\n",
      "epoch:40 step:31964 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.003094]\n",
      "epoch:40 step:31965 [D loss: 0.013178, acc.: 99.22%] [G loss: 0.002541]\n",
      "epoch:40 step:31966 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:40 step:31967 [D loss: 0.000979, acc.: 100.00%] [G loss: 0.002484]\n",
      "epoch:40 step:31968 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.002819]\n",
      "epoch:40 step:31969 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:40 step:31970 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.003616]\n",
      "epoch:40 step:31971 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.001851]\n",
      "epoch:40 step:31972 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.002774]\n",
      "epoch:40 step:31973 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.007441]\n",
      "epoch:40 step:31974 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.010233]\n",
      "epoch:40 step:31975 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.005270]\n",
      "epoch:40 step:31976 [D loss: 0.000516, acc.: 100.00%] [G loss: 0.001655]\n",
      "epoch:40 step:31977 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:40 step:31978 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.038481]\n",
      "epoch:40 step:31979 [D loss: 0.002528, acc.: 100.00%] [G loss: 0.000300]\n",
      "epoch:40 step:31980 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:40 step:31981 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:40 step:31982 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:40 step:31983 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.001541]\n",
      "epoch:40 step:31984 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:40 step:31985 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:40 step:31986 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:40 step:31987 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000505]\n",
      "epoch:40 step:31988 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.000518]\n",
      "epoch:40 step:31989 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:40 step:31990 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:40 step:31991 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.000746]\n",
      "epoch:40 step:31992 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:40 step:31993 [D loss: 0.002176, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:40 step:31994 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.000893]\n",
      "epoch:40 step:31995 [D loss: 0.001060, acc.: 100.00%] [G loss: 0.004361]\n",
      "epoch:40 step:31996 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.003959]\n",
      "epoch:40 step:31997 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:40 step:31998 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:40 step:31999 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.001548]\n",
      "epoch:40 step:32000 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000522]\n",
      "epoch:40 step:32001 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001315]\n",
      "epoch:40 step:32002 [D loss: 0.001029, acc.: 100.00%] [G loss: 0.000641]\n",
      "epoch:40 step:32003 [D loss: 0.000780, acc.: 100.00%] [G loss: 0.002961]\n",
      "epoch:40 step:32004 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:40 step:32005 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:40 step:32006 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.001555]\n",
      "epoch:40 step:32007 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:40 step:32008 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.001610]\n",
      "epoch:40 step:32009 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.001058]\n",
      "epoch:40 step:32010 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:40 step:32011 [D loss: 0.002806, acc.: 100.00%] [G loss: 0.001309]\n",
      "epoch:40 step:32012 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:40 step:32013 [D loss: 0.000993, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:40 step:32014 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.002445]\n",
      "epoch:40 step:32015 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000947]\n",
      "epoch:40 step:32016 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.004365]\n",
      "epoch:40 step:32017 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:40 step:32018 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.002165]\n",
      "epoch:40 step:32019 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:40 step:32020 [D loss: 0.001243, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:40 step:32021 [D loss: 0.009118, acc.: 100.00%] [G loss: 0.000583]\n",
      "epoch:41 step:32022 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.001219]\n",
      "epoch:41 step:32023 [D loss: 0.136328, acc.: 94.53%] [G loss: 0.227942]\n",
      "epoch:41 step:32024 [D loss: 0.074095, acc.: 96.88%] [G loss: 2.667544]\n",
      "epoch:41 step:32025 [D loss: 0.010362, acc.: 100.00%] [G loss: 0.503804]\n",
      "epoch:41 step:32026 [D loss: 0.002386, acc.: 100.00%] [G loss: 0.003220]\n",
      "epoch:41 step:32027 [D loss: 0.006036, acc.: 100.00%] [G loss: 1.551405]\n",
      "epoch:41 step:32028 [D loss: 0.003343, acc.: 100.00%] [G loss: 0.729640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32029 [D loss: 0.002509, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:41 step:32030 [D loss: 0.001692, acc.: 100.00%] [G loss: 0.124386]\n",
      "epoch:41 step:32031 [D loss: 0.040292, acc.: 99.22%] [G loss: 0.016384]\n",
      "epoch:41 step:32032 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.002475]\n",
      "epoch:41 step:32033 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.031632]\n",
      "epoch:41 step:32034 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.006778]\n",
      "epoch:41 step:32035 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:41 step:32036 [D loss: 0.011055, acc.: 99.22%] [G loss: 0.000330]\n",
      "epoch:41 step:32037 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:41 step:32038 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.000505]\n",
      "epoch:41 step:32039 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:41 step:32040 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.002463]\n",
      "epoch:41 step:32041 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000811]\n",
      "epoch:41 step:32042 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:41 step:32043 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:41 step:32044 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000465]\n",
      "epoch:41 step:32045 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.001788]\n",
      "epoch:41 step:32046 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.001151]\n",
      "epoch:41 step:32047 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001999]\n",
      "epoch:41 step:32048 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.006230]\n",
      "epoch:41 step:32049 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.002084]\n",
      "epoch:41 step:32050 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:41 step:32051 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000621]\n",
      "epoch:41 step:32052 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000714]\n",
      "epoch:41 step:32053 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.001303]\n",
      "epoch:41 step:32054 [D loss: 0.000322, acc.: 100.00%] [G loss: 0.000397]\n",
      "epoch:41 step:32055 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.021550]\n",
      "epoch:41 step:32056 [D loss: 0.000514, acc.: 100.00%] [G loss: 0.000777]\n",
      "epoch:41 step:32057 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.000624]\n",
      "epoch:41 step:32058 [D loss: 0.002838, acc.: 100.00%] [G loss: 0.000550]\n",
      "epoch:41 step:32059 [D loss: 0.007373, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:41 step:32060 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001388]\n",
      "epoch:41 step:32061 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.002163]\n",
      "epoch:41 step:32062 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.001095]\n",
      "epoch:41 step:32063 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.003095]\n",
      "epoch:41 step:32064 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000654]\n",
      "epoch:41 step:32065 [D loss: 0.007731, acc.: 100.00%] [G loss: 0.001217]\n",
      "epoch:41 step:32066 [D loss: 0.000825, acc.: 100.00%] [G loss: 0.003876]\n",
      "epoch:41 step:32067 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000707]\n",
      "epoch:41 step:32068 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.001098]\n",
      "epoch:41 step:32069 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.000929]\n",
      "epoch:41 step:32070 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.001436]\n",
      "epoch:41 step:32071 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000710]\n",
      "epoch:41 step:32072 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:41 step:32073 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000531]\n",
      "epoch:41 step:32074 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.001081]\n",
      "epoch:41 step:32075 [D loss: 0.000947, acc.: 100.00%] [G loss: 0.000435]\n",
      "epoch:41 step:32076 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:41 step:32077 [D loss: 0.003552, acc.: 100.00%] [G loss: 0.000401]\n",
      "epoch:41 step:32078 [D loss: 0.001355, acc.: 100.00%] [G loss: 0.000760]\n",
      "epoch:41 step:32079 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.001739]\n",
      "epoch:41 step:32080 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:41 step:32081 [D loss: 0.004290, acc.: 100.00%] [G loss: 0.003041]\n",
      "epoch:41 step:32082 [D loss: 0.169730, acc.: 91.41%] [G loss: 0.029262]\n",
      "epoch:41 step:32083 [D loss: 0.003199, acc.: 100.00%] [G loss: 0.474678]\n",
      "epoch:41 step:32084 [D loss: 0.407402, acc.: 82.03%] [G loss: 0.267829]\n",
      "epoch:41 step:32085 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.284903]\n",
      "epoch:41 step:32086 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.022410]\n",
      "epoch:41 step:32087 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:41 step:32088 [D loss: 0.000688, acc.: 100.00%] [G loss: 0.020339]\n",
      "epoch:41 step:32089 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:41 step:32090 [D loss: 0.328203, acc.: 89.84%] [G loss: 0.414014]\n",
      "epoch:41 step:32091 [D loss: 0.003099, acc.: 100.00%] [G loss: 4.608890]\n",
      "epoch:41 step:32092 [D loss: 1.711022, acc.: 50.78%] [G loss: 0.000671]\n",
      "epoch:41 step:32093 [D loss: 2.841396, acc.: 50.78%] [G loss: 5.250791]\n",
      "epoch:41 step:32094 [D loss: 0.781248, acc.: 69.53%] [G loss: 2.885951]\n",
      "epoch:41 step:32095 [D loss: 0.144159, acc.: 93.75%] [G loss: 0.981422]\n",
      "epoch:41 step:32096 [D loss: 0.015190, acc.: 99.22%] [G loss: 0.650729]\n",
      "epoch:41 step:32097 [D loss: 0.013904, acc.: 100.00%] [G loss: 0.311376]\n",
      "epoch:41 step:32098 [D loss: 0.089453, acc.: 96.09%] [G loss: 0.027909]\n",
      "epoch:41 step:32099 [D loss: 0.001701, acc.: 100.00%] [G loss: 1.657276]\n",
      "epoch:41 step:32100 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.798734]\n",
      "epoch:41 step:32101 [D loss: 0.001267, acc.: 100.00%] [G loss: 0.294056]\n",
      "epoch:41 step:32102 [D loss: 0.009028, acc.: 100.00%] [G loss: 0.000986]\n",
      "epoch:41 step:32103 [D loss: 0.000832, acc.: 100.00%] [G loss: 0.001591]\n",
      "epoch:41 step:32104 [D loss: 0.000831, acc.: 100.00%] [G loss: 0.001717]\n",
      "epoch:41 step:32105 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.159473]\n",
      "epoch:41 step:32106 [D loss: 0.003782, acc.: 100.00%] [G loss: 0.433836]\n",
      "epoch:41 step:32107 [D loss: 0.001868, acc.: 100.00%] [G loss: 0.002488]\n",
      "epoch:41 step:32108 [D loss: 0.124627, acc.: 96.09%] [G loss: 0.004268]\n",
      "epoch:41 step:32109 [D loss: 0.006304, acc.: 100.00%] [G loss: 0.008952]\n",
      "epoch:41 step:32110 [D loss: 0.005353, acc.: 100.00%] [G loss: 0.008321]\n",
      "epoch:41 step:32111 [D loss: 0.142158, acc.: 92.97%] [G loss: 0.000330]\n",
      "epoch:41 step:32112 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.046000]\n",
      "epoch:41 step:32113 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.082060]\n",
      "epoch:41 step:32114 [D loss: 0.001957, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:41 step:32115 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.124994]\n",
      "epoch:41 step:32116 [D loss: 0.002638, acc.: 100.00%] [G loss: 0.000489]\n",
      "epoch:41 step:32117 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.000547]\n",
      "epoch:41 step:32118 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000318]\n",
      "epoch:41 step:32119 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:41 step:32120 [D loss: 0.001413, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:41 step:32121 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000368]\n",
      "epoch:41 step:32122 [D loss: 0.000758, acc.: 100.00%] [G loss: 0.000335]\n",
      "epoch:41 step:32123 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.009342]\n",
      "epoch:41 step:32124 [D loss: 0.000707, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:41 step:32125 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.015946]\n",
      "epoch:41 step:32126 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:41 step:32127 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:41 step:32128 [D loss: 0.000352, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:41 step:32129 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:41 step:32130 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.004671]\n",
      "epoch:41 step:32131 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:41 step:32132 [D loss: 0.006088, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:41 step:32133 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:41 step:32134 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:41 step:32135 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:41 step:32136 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:41 step:32137 [D loss: 0.001882, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:41 step:32138 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:41 step:32139 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.045228]\n",
      "epoch:41 step:32140 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.002237]\n",
      "epoch:41 step:32141 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:41 step:32142 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000635]\n",
      "epoch:41 step:32143 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.003395]\n",
      "epoch:41 step:32144 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32145 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.005676]\n",
      "epoch:41 step:32146 [D loss: 0.000598, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:41 step:32147 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.096033]\n",
      "epoch:41 step:32148 [D loss: 0.000541, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:41 step:32149 [D loss: 0.001422, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:41 step:32150 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:41 step:32151 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:41 step:32152 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.002152]\n",
      "epoch:41 step:32153 [D loss: 0.000707, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:41 step:32154 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:41 step:32155 [D loss: 0.000931, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:41 step:32156 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.002828]\n",
      "epoch:41 step:32157 [D loss: 0.005501, acc.: 100.00%] [G loss: 0.004565]\n",
      "epoch:41 step:32158 [D loss: 0.259713, acc.: 89.06%] [G loss: 0.007299]\n",
      "epoch:41 step:32159 [D loss: 0.002511, acc.: 100.00%] [G loss: 5.098840]\n",
      "epoch:41 step:32160 [D loss: 0.084733, acc.: 96.88%] [G loss: 0.083712]\n",
      "epoch:41 step:32161 [D loss: 0.041790, acc.: 99.22%] [G loss: 1.389363]\n",
      "epoch:41 step:32162 [D loss: 0.006217, acc.: 100.00%] [G loss: 1.198029]\n",
      "epoch:41 step:32163 [D loss: 0.029692, acc.: 99.22%] [G loss: 0.005217]\n",
      "epoch:41 step:32164 [D loss: 0.047097, acc.: 98.44%] [G loss: 0.000554]\n",
      "epoch:41 step:32165 [D loss: 0.004055, acc.: 100.00%] [G loss: 0.055664]\n",
      "epoch:41 step:32166 [D loss: 0.006365, acc.: 100.00%] [G loss: 0.077217]\n",
      "epoch:41 step:32167 [D loss: 0.032752, acc.: 99.22%] [G loss: 0.002793]\n",
      "epoch:41 step:32168 [D loss: 0.002165, acc.: 100.00%] [G loss: 0.004789]\n",
      "epoch:41 step:32169 [D loss: 0.002434, acc.: 100.00%] [G loss: 0.001995]\n",
      "epoch:41 step:32170 [D loss: 0.000701, acc.: 100.00%] [G loss: 0.004281]\n",
      "epoch:41 step:32171 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.427310]\n",
      "epoch:41 step:32172 [D loss: 0.006559, acc.: 100.00%] [G loss: 0.005377]\n",
      "epoch:41 step:32173 [D loss: 0.000615, acc.: 100.00%] [G loss: 0.005080]\n",
      "epoch:41 step:32174 [D loss: 0.003408, acc.: 100.00%] [G loss: 0.000890]\n",
      "epoch:41 step:32175 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:41 step:32176 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001347]\n",
      "epoch:41 step:32177 [D loss: 0.001187, acc.: 100.00%] [G loss: 0.001016]\n",
      "epoch:41 step:32178 [D loss: 0.000911, acc.: 100.00%] [G loss: 0.000781]\n",
      "epoch:41 step:32179 [D loss: 0.007920, acc.: 100.00%] [G loss: 0.034873]\n",
      "epoch:41 step:32180 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.159761]\n",
      "epoch:41 step:32181 [D loss: 0.030984, acc.: 98.44%] [G loss: 0.000183]\n",
      "epoch:41 step:32182 [D loss: 0.001185, acc.: 100.00%] [G loss: 0.019102]\n",
      "epoch:41 step:32183 [D loss: 0.002086, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:41 step:32184 [D loss: 0.001784, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:41 step:32185 [D loss: 0.000455, acc.: 100.00%] [G loss: 0.004671]\n",
      "epoch:41 step:32186 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.003448]\n",
      "epoch:41 step:32187 [D loss: 0.028076, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:41 step:32188 [D loss: 0.014181, acc.: 100.00%] [G loss: 0.003956]\n",
      "epoch:41 step:32189 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.002862]\n",
      "epoch:41 step:32190 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.214081]\n",
      "epoch:41 step:32191 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.004460]\n",
      "epoch:41 step:32192 [D loss: 0.002018, acc.: 100.00%] [G loss: 0.077289]\n",
      "epoch:41 step:32193 [D loss: 0.001220, acc.: 100.00%] [G loss: 0.019216]\n",
      "epoch:41 step:32194 [D loss: 0.021844, acc.: 100.00%] [G loss: 0.004424]\n",
      "epoch:41 step:32195 [D loss: 0.011802, acc.: 100.00%] [G loss: 0.303691]\n",
      "epoch:41 step:32196 [D loss: 0.007288, acc.: 100.00%] [G loss: 0.376870]\n",
      "epoch:41 step:32197 [D loss: 0.016590, acc.: 100.00%] [G loss: 0.003484]\n",
      "epoch:41 step:32198 [D loss: 0.018978, acc.: 100.00%] [G loss: 0.058460]\n",
      "epoch:41 step:32199 [D loss: 0.010286, acc.: 100.00%] [G loss: 0.023716]\n",
      "epoch:41 step:32200 [D loss: 0.042572, acc.: 99.22%] [G loss: 0.044442]\n",
      "epoch:41 step:32201 [D loss: 0.007789, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:41 step:32202 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:41 step:32203 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:41 step:32204 [D loss: 0.000904, acc.: 100.00%] [G loss: 0.000759]\n",
      "epoch:41 step:32205 [D loss: 0.003276, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:41 step:32206 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:41 step:32207 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000704]\n",
      "epoch:41 step:32208 [D loss: 0.002604, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:41 step:32209 [D loss: 0.002071, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:41 step:32210 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000478]\n",
      "epoch:41 step:32211 [D loss: 0.029326, acc.: 99.22%] [G loss: 0.019032]\n",
      "epoch:41 step:32212 [D loss: 0.011986, acc.: 99.22%] [G loss: 0.000078]\n",
      "epoch:41 step:32213 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.007619]\n",
      "epoch:41 step:32214 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.029080]\n",
      "epoch:41 step:32215 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:41 step:32216 [D loss: 0.000352, acc.: 100.00%] [G loss: 0.002117]\n",
      "epoch:41 step:32217 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:41 step:32218 [D loss: 0.000936, acc.: 100.00%] [G loss: 0.001664]\n",
      "epoch:41 step:32219 [D loss: 0.018412, acc.: 100.00%] [G loss: 0.001504]\n",
      "epoch:41 step:32220 [D loss: 0.109983, acc.: 97.66%] [G loss: 0.021858]\n",
      "epoch:41 step:32221 [D loss: 0.114909, acc.: 95.31%] [G loss: 2.016806]\n",
      "epoch:41 step:32222 [D loss: 0.000494, acc.: 100.00%] [G loss: 0.009348]\n",
      "epoch:41 step:32223 [D loss: 0.001092, acc.: 100.00%] [G loss: 0.010058]\n",
      "epoch:41 step:32224 [D loss: 0.003848, acc.: 100.00%] [G loss: 0.600415]\n",
      "epoch:41 step:32225 [D loss: 0.000957, acc.: 100.00%] [G loss: 0.324624]\n",
      "epoch:41 step:32226 [D loss: 0.006535, acc.: 100.00%] [G loss: 0.172394]\n",
      "epoch:41 step:32227 [D loss: 0.020171, acc.: 100.00%] [G loss: 0.013148]\n",
      "epoch:41 step:32228 [D loss: 0.004133, acc.: 100.00%] [G loss: 0.172316]\n",
      "epoch:41 step:32229 [D loss: 0.008767, acc.: 100.00%] [G loss: 0.136009]\n",
      "epoch:41 step:32230 [D loss: 0.007795, acc.: 100.00%] [G loss: 0.007892]\n",
      "epoch:41 step:32231 [D loss: 0.032055, acc.: 99.22%] [G loss: 0.003970]\n",
      "epoch:41 step:32232 [D loss: 0.002166, acc.: 100.00%] [G loss: 0.354455]\n",
      "epoch:41 step:32233 [D loss: 0.020204, acc.: 100.00%] [G loss: 0.006064]\n",
      "epoch:41 step:32234 [D loss: 0.003645, acc.: 100.00%] [G loss: 0.007767]\n",
      "epoch:41 step:32235 [D loss: 0.037982, acc.: 99.22%] [G loss: 0.001805]\n",
      "epoch:41 step:32236 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000376]\n",
      "epoch:41 step:32237 [D loss: 0.001378, acc.: 100.00%] [G loss: 0.002396]\n",
      "epoch:41 step:32238 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:41 step:32239 [D loss: 0.000629, acc.: 100.00%] [G loss: 0.000866]\n",
      "epoch:41 step:32240 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.046190]\n",
      "epoch:41 step:32241 [D loss: 0.000713, acc.: 100.00%] [G loss: 0.000401]\n",
      "epoch:41 step:32242 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000532]\n",
      "epoch:41 step:32243 [D loss: 0.000997, acc.: 100.00%] [G loss: 0.035033]\n",
      "epoch:41 step:32244 [D loss: 0.001076, acc.: 100.00%] [G loss: 0.035813]\n",
      "epoch:41 step:32245 [D loss: 0.000386, acc.: 100.00%] [G loss: 0.000358]\n",
      "epoch:41 step:32246 [D loss: 0.002831, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:41 step:32247 [D loss: 0.001424, acc.: 100.00%] [G loss: 0.013206]\n",
      "epoch:41 step:32248 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:41 step:32249 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.002635]\n",
      "epoch:41 step:32250 [D loss: 0.002386, acc.: 100.00%] [G loss: 0.019524]\n",
      "epoch:41 step:32251 [D loss: 0.001807, acc.: 100.00%] [G loss: 0.001262]\n",
      "epoch:41 step:32252 [D loss: 0.000622, acc.: 100.00%] [G loss: 0.036307]\n",
      "epoch:41 step:32253 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.000863]\n",
      "epoch:41 step:32254 [D loss: 0.002577, acc.: 100.00%] [G loss: 0.002093]\n",
      "epoch:41 step:32255 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.001047]\n",
      "epoch:41 step:32256 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.000491]\n",
      "epoch:41 step:32257 [D loss: 0.004225, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:41 step:32258 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:41 step:32259 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.002900]\n",
      "epoch:41 step:32260 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.004170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32261 [D loss: 0.003016, acc.: 100.00%] [G loss: 0.003102]\n",
      "epoch:41 step:32262 [D loss: 0.005819, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:41 step:32263 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:41 step:32264 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:41 step:32265 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.002850]\n",
      "epoch:41 step:32266 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000299]\n",
      "epoch:41 step:32267 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:41 step:32268 [D loss: 0.001818, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:41 step:32269 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.001756]\n",
      "epoch:41 step:32270 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.300232]\n",
      "epoch:41 step:32271 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.000605]\n",
      "epoch:41 step:32272 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.002265]\n",
      "epoch:41 step:32273 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:41 step:32274 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.001534]\n",
      "epoch:41 step:32275 [D loss: 0.000811, acc.: 100.00%] [G loss: 0.000935]\n",
      "epoch:41 step:32276 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.004110]\n",
      "epoch:41 step:32277 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.003853]\n",
      "epoch:41 step:32278 [D loss: 0.019376, acc.: 100.00%] [G loss: 0.018212]\n",
      "epoch:41 step:32279 [D loss: 0.003234, acc.: 100.00%] [G loss: 0.072966]\n",
      "epoch:41 step:32280 [D loss: 0.001064, acc.: 100.00%] [G loss: 0.266826]\n",
      "epoch:41 step:32281 [D loss: 0.024310, acc.: 100.00%] [G loss: 0.001198]\n",
      "epoch:41 step:32282 [D loss: 0.077550, acc.: 98.44%] [G loss: 0.000112]\n",
      "epoch:41 step:32283 [D loss: 0.010823, acc.: 99.22%] [G loss: 0.049674]\n",
      "epoch:41 step:32284 [D loss: 0.021194, acc.: 100.00%] [G loss: 0.003967]\n",
      "epoch:41 step:32285 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.002281]\n",
      "epoch:41 step:32286 [D loss: 0.030735, acc.: 99.22%] [G loss: 0.000277]\n",
      "epoch:41 step:32287 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:41 step:32288 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.035807]\n",
      "epoch:41 step:32289 [D loss: 0.000515, acc.: 100.00%] [G loss: 0.016956]\n",
      "epoch:41 step:32290 [D loss: 0.010592, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:41 step:32291 [D loss: 0.001608, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:41 step:32292 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.000299]\n",
      "epoch:41 step:32293 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.142144]\n",
      "epoch:41 step:32294 [D loss: 0.002679, acc.: 100.00%] [G loss: 0.000920]\n",
      "epoch:41 step:32295 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000558]\n",
      "epoch:41 step:32296 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.145343]\n",
      "epoch:41 step:32297 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000814]\n",
      "epoch:41 step:32298 [D loss: 0.003092, acc.: 100.00%] [G loss: 0.028891]\n",
      "epoch:41 step:32299 [D loss: 0.000976, acc.: 100.00%] [G loss: 0.000689]\n",
      "epoch:41 step:32300 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.001394]\n",
      "epoch:41 step:32301 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.070109]\n",
      "epoch:41 step:32302 [D loss: 0.001880, acc.: 100.00%] [G loss: 0.011085]\n",
      "epoch:41 step:32303 [D loss: 0.004752, acc.: 100.00%] [G loss: 0.058895]\n",
      "epoch:41 step:32304 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.063706]\n",
      "epoch:41 step:32305 [D loss: 0.005822, acc.: 100.00%] [G loss: 0.003906]\n",
      "epoch:41 step:32306 [D loss: 0.018531, acc.: 99.22%] [G loss: 0.254052]\n",
      "epoch:41 step:32307 [D loss: 0.002595, acc.: 100.00%] [G loss: 0.396124]\n",
      "epoch:41 step:32308 [D loss: 0.005349, acc.: 100.00%] [G loss: 0.559517]\n",
      "epoch:41 step:32309 [D loss: 0.014490, acc.: 100.00%] [G loss: 0.117799]\n",
      "epoch:41 step:32310 [D loss: 0.134742, acc.: 96.88%] [G loss: 1.821800]\n",
      "epoch:41 step:32311 [D loss: 0.269807, acc.: 87.50%] [G loss: 1.614327]\n",
      "epoch:41 step:32312 [D loss: 0.019717, acc.: 100.00%] [G loss: 0.002796]\n",
      "epoch:41 step:32313 [D loss: 0.004498, acc.: 100.00%] [G loss: 0.000787]\n",
      "epoch:41 step:32314 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.779124]\n",
      "epoch:41 step:32315 [D loss: 0.003476, acc.: 100.00%] [G loss: 0.336444]\n",
      "epoch:41 step:32316 [D loss: 0.014329, acc.: 100.00%] [G loss: 0.001536]\n",
      "epoch:41 step:32317 [D loss: 0.001872, acc.: 100.00%] [G loss: 0.002786]\n",
      "epoch:41 step:32318 [D loss: 0.000293, acc.: 100.00%] [G loss: 0.004882]\n",
      "epoch:41 step:32319 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.731805]\n",
      "epoch:41 step:32320 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.009169]\n",
      "epoch:41 step:32321 [D loss: 0.024439, acc.: 99.22%] [G loss: 0.001786]\n",
      "epoch:41 step:32322 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.013464]\n",
      "epoch:41 step:32323 [D loss: 0.067310, acc.: 97.66%] [G loss: 0.007879]\n",
      "epoch:41 step:32324 [D loss: 0.000685, acc.: 100.00%] [G loss: 0.076803]\n",
      "epoch:41 step:32325 [D loss: 0.007454, acc.: 100.00%] [G loss: 0.040160]\n",
      "epoch:41 step:32326 [D loss: 0.005768, acc.: 100.00%] [G loss: 3.631771]\n",
      "epoch:41 step:32327 [D loss: 0.001992, acc.: 100.00%] [G loss: 2.288449]\n",
      "epoch:41 step:32328 [D loss: 0.001201, acc.: 100.00%] [G loss: 0.946094]\n",
      "epoch:41 step:32329 [D loss: 0.065854, acc.: 96.88%] [G loss: 0.081496]\n",
      "epoch:41 step:32330 [D loss: 0.053468, acc.: 98.44%] [G loss: 0.002301]\n",
      "epoch:41 step:32331 [D loss: 0.001737, acc.: 100.00%] [G loss: 2.033823]\n",
      "epoch:41 step:32332 [D loss: 0.010924, acc.: 100.00%] [G loss: 0.006142]\n",
      "epoch:41 step:32333 [D loss: 0.023736, acc.: 99.22%] [G loss: 0.000756]\n",
      "epoch:41 step:32334 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:41 step:32335 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000541]\n",
      "epoch:41 step:32336 [D loss: 0.211375, acc.: 90.62%] [G loss: 0.000002]\n",
      "epoch:41 step:32337 [D loss: 0.604873, acc.: 78.91%] [G loss: 2.767509]\n",
      "epoch:41 step:32338 [D loss: 0.000156, acc.: 100.00%] [G loss: 1.734735]\n",
      "epoch:41 step:32339 [D loss: 0.653102, acc.: 70.31%] [G loss: 0.001001]\n",
      "epoch:41 step:32340 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:41 step:32341 [D loss: 0.034867, acc.: 98.44%] [G loss: 0.095447]\n",
      "epoch:41 step:32342 [D loss: 0.010145, acc.: 100.00%] [G loss: 0.010212]\n",
      "epoch:41 step:32343 [D loss: 0.154802, acc.: 93.75%] [G loss: 0.000082]\n",
      "epoch:41 step:32344 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.008988]\n",
      "epoch:41 step:32345 [D loss: 0.006613, acc.: 99.22%] [G loss: 0.002660]\n",
      "epoch:41 step:32346 [D loss: 0.011271, acc.: 99.22%] [G loss: 3.976754]\n",
      "epoch:41 step:32347 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.026010]\n",
      "epoch:41 step:32348 [D loss: 0.178457, acc.: 91.41%] [G loss: 0.245476]\n",
      "epoch:41 step:32349 [D loss: 0.000801, acc.: 100.00%] [G loss: 7.287041]\n",
      "epoch:41 step:32350 [D loss: 0.020820, acc.: 99.22%] [G loss: 0.736845]\n",
      "epoch:41 step:32351 [D loss: 0.064467, acc.: 98.44%] [G loss: 3.566380]\n",
      "epoch:41 step:32352 [D loss: 0.045883, acc.: 98.44%] [G loss: 0.004722]\n",
      "epoch:41 step:32353 [D loss: 0.001876, acc.: 100.00%] [G loss: 0.754759]\n",
      "epoch:41 step:32354 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.002599]\n",
      "epoch:41 step:32355 [D loss: 0.000960, acc.: 100.00%] [G loss: 0.003043]\n",
      "epoch:41 step:32356 [D loss: 0.000207, acc.: 100.00%] [G loss: 0.001078]\n",
      "epoch:41 step:32357 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.005012]\n",
      "epoch:41 step:32358 [D loss: 0.001125, acc.: 100.00%] [G loss: 0.001101]\n",
      "epoch:41 step:32359 [D loss: 0.012324, acc.: 100.00%] [G loss: 0.001264]\n",
      "epoch:41 step:32360 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.244104]\n",
      "epoch:41 step:32361 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.290247]\n",
      "epoch:41 step:32362 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.068396]\n",
      "epoch:41 step:32363 [D loss: 0.031527, acc.: 99.22%] [G loss: 0.000321]\n",
      "epoch:41 step:32364 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.008800]\n",
      "epoch:41 step:32365 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.248672]\n",
      "epoch:41 step:32366 [D loss: 0.001082, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:41 step:32367 [D loss: 0.019604, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:41 step:32368 [D loss: 0.002130, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:41 step:32369 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.055516]\n",
      "epoch:41 step:32370 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.000258]\n",
      "epoch:41 step:32371 [D loss: 0.011114, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:41 step:32372 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:41 step:32373 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:41 step:32374 [D loss: 0.014679, acc.: 99.22%] [G loss: 0.000017]\n",
      "epoch:41 step:32375 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:41 step:32376 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.006849]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32377 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.014782]\n",
      "epoch:41 step:32378 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:41 step:32379 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.031976]\n",
      "epoch:41 step:32380 [D loss: 0.006291, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:41 step:32381 [D loss: 0.001871, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:41 step:32382 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:41 step:32383 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.129552]\n",
      "epoch:41 step:32384 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:41 step:32385 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:41 step:32386 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:41 step:32387 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:41 step:32388 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:41 step:32389 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:41 step:32390 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:41 step:32391 [D loss: 0.000004, acc.: 100.00%] [G loss: 1.231907]\n",
      "epoch:41 step:32392 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:41 step:32393 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.208260]\n",
      "epoch:41 step:32394 [D loss: 0.004041, acc.: 100.00%] [G loss: 0.023966]\n",
      "epoch:41 step:32395 [D loss: 0.014297, acc.: 100.00%] [G loss: 0.016516]\n",
      "epoch:41 step:32396 [D loss: 0.005210, acc.: 100.00%] [G loss: 0.120269]\n",
      "epoch:41 step:32397 [D loss: 0.032135, acc.: 99.22%] [G loss: 0.572199]\n",
      "epoch:41 step:32398 [D loss: 0.002363, acc.: 100.00%] [G loss: 0.355309]\n",
      "epoch:41 step:32399 [D loss: 0.002951, acc.: 100.00%] [G loss: 1.076109]\n",
      "epoch:41 step:32400 [D loss: 0.030388, acc.: 100.00%] [G loss: 0.041999]\n",
      "epoch:41 step:32401 [D loss: 0.018764, acc.: 99.22%] [G loss: 1.658983]\n",
      "epoch:41 step:32402 [D loss: 0.017590, acc.: 100.00%] [G loss: 1.069699]\n",
      "epoch:41 step:32403 [D loss: 0.066918, acc.: 99.22%] [G loss: 0.046839]\n",
      "epoch:41 step:32404 [D loss: 0.005037, acc.: 100.00%] [G loss: 0.144034]\n",
      "epoch:41 step:32405 [D loss: 0.145200, acc.: 92.19%] [G loss: 0.000104]\n",
      "epoch:41 step:32406 [D loss: 0.004967, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:41 step:32407 [D loss: 0.000765, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:41 step:32408 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.174343]\n",
      "epoch:41 step:32409 [D loss: 0.002537, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:41 step:32410 [D loss: 0.000795, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:41 step:32411 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.001894]\n",
      "epoch:41 step:32412 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:41 step:32413 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.030103]\n",
      "epoch:41 step:32414 [D loss: 0.000601, acc.: 100.00%] [G loss: 0.042296]\n",
      "epoch:41 step:32415 [D loss: 0.001359, acc.: 100.00%] [G loss: 0.044322]\n",
      "epoch:41 step:32416 [D loss: 0.006914, acc.: 100.00%] [G loss: 0.021663]\n",
      "epoch:41 step:32417 [D loss: 0.050585, acc.: 98.44%] [G loss: 0.449626]\n",
      "epoch:41 step:32418 [D loss: 0.004061, acc.: 100.00%] [G loss: 0.001151]\n",
      "epoch:41 step:32419 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.004677]\n",
      "epoch:41 step:32420 [D loss: 0.000981, acc.: 100.00%] [G loss: 3.448162]\n",
      "epoch:41 step:32421 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.002402]\n",
      "epoch:41 step:32422 [D loss: 0.005511, acc.: 100.00%] [G loss: 0.001887]\n",
      "epoch:41 step:32423 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.001611]\n",
      "epoch:41 step:32424 [D loss: 0.006596, acc.: 100.00%] [G loss: 0.346435]\n",
      "epoch:41 step:32425 [D loss: 0.033501, acc.: 100.00%] [G loss: 0.010025]\n",
      "epoch:41 step:32426 [D loss: 0.018329, acc.: 99.22%] [G loss: 0.001849]\n",
      "epoch:41 step:32427 [D loss: 0.001379, acc.: 100.00%] [G loss: 1.996311]\n",
      "epoch:41 step:32428 [D loss: 0.000090, acc.: 100.00%] [G loss: 1.903861]\n",
      "epoch:41 step:32429 [D loss: 0.006102, acc.: 100.00%] [G loss: 0.003819]\n",
      "epoch:41 step:32430 [D loss: 0.011589, acc.: 100.00%] [G loss: 0.017068]\n",
      "epoch:41 step:32431 [D loss: 0.013353, acc.: 99.22%] [G loss: 0.005061]\n",
      "epoch:41 step:32432 [D loss: 0.176619, acc.: 92.97%] [G loss: 0.000001]\n",
      "epoch:41 step:32433 [D loss: 0.024554, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:41 step:32434 [D loss: 0.017524, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:41 step:32435 [D loss: 0.001244, acc.: 100.00%] [G loss: 0.210252]\n",
      "epoch:41 step:32436 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.073831]\n",
      "epoch:41 step:32437 [D loss: 0.018403, acc.: 100.00%] [G loss: 0.462518]\n",
      "epoch:41 step:32438 [D loss: 0.086464, acc.: 96.88%] [G loss: 0.002561]\n",
      "epoch:41 step:32439 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.072324]\n",
      "epoch:41 step:32440 [D loss: 0.008940, acc.: 100.00%] [G loss: 3.992068]\n",
      "epoch:41 step:32441 [D loss: 0.109027, acc.: 96.09%] [G loss: 0.002238]\n",
      "epoch:41 step:32442 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000643]\n",
      "epoch:41 step:32443 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.002702]\n",
      "epoch:41 step:32444 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.021458]\n",
      "epoch:41 step:32445 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.079809]\n",
      "epoch:41 step:32446 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.003661]\n",
      "epoch:41 step:32447 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:41 step:32448 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000869]\n",
      "epoch:41 step:32449 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000346]\n",
      "epoch:41 step:32450 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:41 step:32451 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:41 step:32452 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.048652]\n",
      "epoch:41 step:32453 [D loss: 0.000327, acc.: 100.00%] [G loss: 0.000804]\n",
      "epoch:41 step:32454 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.090394]\n",
      "epoch:41 step:32455 [D loss: 0.001409, acc.: 100.00%] [G loss: 0.053102]\n",
      "epoch:41 step:32456 [D loss: 0.181217, acc.: 92.97%] [G loss: 1.248266]\n",
      "epoch:41 step:32457 [D loss: 0.001300, acc.: 100.00%] [G loss: 0.264525]\n",
      "epoch:41 step:32458 [D loss: 0.013023, acc.: 99.22%] [G loss: 2.250054]\n",
      "epoch:41 step:32459 [D loss: 0.018194, acc.: 100.00%] [G loss: 0.074649]\n",
      "epoch:41 step:32460 [D loss: 0.005461, acc.: 100.00%] [G loss: 0.251164]\n",
      "epoch:41 step:32461 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.007490]\n",
      "epoch:41 step:32462 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.591413]\n",
      "epoch:41 step:32463 [D loss: 0.002405, acc.: 100.00%] [G loss: 0.258641]\n",
      "epoch:41 step:32464 [D loss: 0.009333, acc.: 100.00%] [G loss: 0.003235]\n",
      "epoch:41 step:32465 [D loss: 0.020726, acc.: 99.22%] [G loss: 0.000966]\n",
      "epoch:41 step:32466 [D loss: 0.022679, acc.: 99.22%] [G loss: 0.050192]\n",
      "epoch:41 step:32467 [D loss: 0.010587, acc.: 99.22%] [G loss: 0.001584]\n",
      "epoch:41 step:32468 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000884]\n",
      "epoch:41 step:32469 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.123892]\n",
      "epoch:41 step:32470 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.153486]\n",
      "epoch:41 step:32471 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.001548]\n",
      "epoch:41 step:32472 [D loss: 0.000374, acc.: 100.00%] [G loss: 0.493773]\n",
      "epoch:41 step:32473 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000784]\n",
      "epoch:41 step:32474 [D loss: 0.011883, acc.: 99.22%] [G loss: 0.001335]\n",
      "epoch:41 step:32475 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000552]\n",
      "epoch:41 step:32476 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:41 step:32477 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000782]\n",
      "epoch:41 step:32478 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:41 step:32479 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.018887]\n",
      "epoch:41 step:32480 [D loss: 0.000930, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:41 step:32481 [D loss: 0.000325, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:41 step:32482 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.144898]\n",
      "epoch:41 step:32483 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:41 step:32484 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.029869]\n",
      "epoch:41 step:32485 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.015381]\n",
      "epoch:41 step:32486 [D loss: 0.000015, acc.: 100.00%] [G loss: 1.215479]\n",
      "epoch:41 step:32487 [D loss: 0.005901, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:41 step:32488 [D loss: 0.002537, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:41 step:32489 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:41 step:32490 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.000767]\n",
      "epoch:41 step:32491 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.025682]\n",
      "epoch:41 step:32492 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.070120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32493 [D loss: 0.020077, acc.: 100.00%] [G loss: 0.008440]\n",
      "epoch:41 step:32494 [D loss: 0.002214, acc.: 100.00%] [G loss: 0.041846]\n",
      "epoch:41 step:32495 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.287331]\n",
      "epoch:41 step:32496 [D loss: 0.006945, acc.: 100.00%] [G loss: 1.027344]\n",
      "epoch:41 step:32497 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.003333]\n",
      "epoch:41 step:32498 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.003940]\n",
      "epoch:41 step:32499 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.033616]\n",
      "epoch:41 step:32500 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000794]\n",
      "epoch:41 step:32501 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.002678]\n",
      "epoch:41 step:32502 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.005619]\n",
      "epoch:41 step:32503 [D loss: 0.004696, acc.: 100.00%] [G loss: 0.014431]\n",
      "epoch:41 step:32504 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.002021]\n",
      "epoch:41 step:32505 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.001169]\n",
      "epoch:41 step:32506 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.037888]\n",
      "epoch:41 step:32507 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.009507]\n",
      "epoch:41 step:32508 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.001693]\n",
      "epoch:41 step:32509 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:41 step:32510 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.003146]\n",
      "epoch:41 step:32511 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:41 step:32512 [D loss: 0.000989, acc.: 100.00%] [G loss: 0.000190]\n",
      "epoch:41 step:32513 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:41 step:32514 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000420]\n",
      "epoch:41 step:32515 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:41 step:32516 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.042206]\n",
      "epoch:41 step:32517 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.011427]\n",
      "epoch:41 step:32518 [D loss: 0.002016, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:41 step:32519 [D loss: 0.001732, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:41 step:32520 [D loss: 0.008459, acc.: 99.22%] [G loss: 0.000140]\n",
      "epoch:41 step:32521 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:41 step:32522 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.036149]\n",
      "epoch:41 step:32523 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.022116]\n",
      "epoch:41 step:32524 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:41 step:32525 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:41 step:32526 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:41 step:32527 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:41 step:32528 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.013709]\n",
      "epoch:41 step:32529 [D loss: 0.007681, acc.: 100.00%] [G loss: 0.000934]\n",
      "epoch:41 step:32530 [D loss: 0.004466, acc.: 100.00%] [G loss: 0.000245]\n",
      "epoch:41 step:32531 [D loss: 0.001661, acc.: 100.00%] [G loss: 0.000349]\n",
      "epoch:41 step:32532 [D loss: 0.000777, acc.: 100.00%] [G loss: 0.000542]\n",
      "epoch:41 step:32533 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000893]\n",
      "epoch:41 step:32534 [D loss: 0.004225, acc.: 100.00%] [G loss: 0.014858]\n",
      "epoch:41 step:32535 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:41 step:32536 [D loss: 0.106121, acc.: 96.09%] [G loss: 0.299217]\n",
      "epoch:41 step:32537 [D loss: 0.030743, acc.: 99.22%] [G loss: 0.828444]\n",
      "epoch:41 step:32538 [D loss: 0.116442, acc.: 96.09%] [G loss: 0.036952]\n",
      "epoch:41 step:32539 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.008420]\n",
      "epoch:41 step:32540 [D loss: 0.001130, acc.: 100.00%] [G loss: 0.008315]\n",
      "epoch:41 step:32541 [D loss: 0.026229, acc.: 99.22%] [G loss: 0.011687]\n",
      "epoch:41 step:32542 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.009401]\n",
      "epoch:41 step:32543 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.096626]\n",
      "epoch:41 step:32544 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.012050]\n",
      "epoch:41 step:32545 [D loss: 0.002030, acc.: 100.00%] [G loss: 0.030149]\n",
      "epoch:41 step:32546 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.011558]\n",
      "epoch:41 step:32547 [D loss: 0.030230, acc.: 98.44%] [G loss: 0.001196]\n",
      "epoch:41 step:32548 [D loss: 0.152465, acc.: 96.09%] [G loss: 2.481371]\n",
      "epoch:41 step:32549 [D loss: 0.007222, acc.: 100.00%] [G loss: 0.285307]\n",
      "epoch:41 step:32550 [D loss: 0.420126, acc.: 78.91%] [G loss: 6.055024]\n",
      "epoch:41 step:32551 [D loss: 1.609028, acc.: 49.22%] [G loss: 4.149723]\n",
      "epoch:41 step:32552 [D loss: 0.001377, acc.: 100.00%] [G loss: 4.732137]\n",
      "epoch:41 step:32553 [D loss: 0.387480, acc.: 83.59%] [G loss: 1.133281]\n",
      "epoch:41 step:32554 [D loss: 0.753739, acc.: 74.22%] [G loss: 3.533933]\n",
      "epoch:41 step:32555 [D loss: 0.046748, acc.: 98.44%] [G loss: 5.190639]\n",
      "epoch:41 step:32556 [D loss: 0.462887, acc.: 82.81%] [G loss: 1.412384]\n",
      "epoch:41 step:32557 [D loss: 0.297162, acc.: 84.38%] [G loss: 5.108041]\n",
      "epoch:41 step:32558 [D loss: 0.053117, acc.: 99.22%] [G loss: 6.014672]\n",
      "epoch:41 step:32559 [D loss: 0.048502, acc.: 97.66%] [G loss: 5.681263]\n",
      "epoch:41 step:32560 [D loss: 0.011189, acc.: 100.00%] [G loss: 6.372318]\n",
      "epoch:41 step:32561 [D loss: 0.045175, acc.: 99.22%] [G loss: 5.499306]\n",
      "epoch:41 step:32562 [D loss: 0.099928, acc.: 96.88%] [G loss: 5.890288]\n",
      "epoch:41 step:32563 [D loss: 0.057852, acc.: 98.44%] [G loss: 6.012378]\n",
      "epoch:41 step:32564 [D loss: 0.010170, acc.: 100.00%] [G loss: 5.394134]\n",
      "epoch:41 step:32565 [D loss: 0.013327, acc.: 100.00%] [G loss: 3.888114]\n",
      "epoch:41 step:32566 [D loss: 0.078420, acc.: 98.44%] [G loss: 5.407860]\n",
      "epoch:41 step:32567 [D loss: 0.018424, acc.: 100.00%] [G loss: 5.182249]\n",
      "epoch:41 step:32568 [D loss: 0.073826, acc.: 99.22%] [G loss: 4.816813]\n",
      "epoch:41 step:32569 [D loss: 0.016544, acc.: 100.00%] [G loss: 5.064115]\n",
      "epoch:41 step:32570 [D loss: 0.135283, acc.: 92.97%] [G loss: 6.154187]\n",
      "epoch:41 step:32571 [D loss: 0.037160, acc.: 99.22%] [G loss: 6.700531]\n",
      "epoch:41 step:32572 [D loss: 0.040526, acc.: 98.44%] [G loss: 7.038960]\n",
      "epoch:41 step:32573 [D loss: 0.099063, acc.: 95.31%] [G loss: 3.770104]\n",
      "epoch:41 step:32574 [D loss: 0.618301, acc.: 75.00%] [G loss: 9.307045]\n",
      "epoch:41 step:32575 [D loss: 0.717002, acc.: 64.84%] [G loss: 6.729049]\n",
      "epoch:41 step:32576 [D loss: 0.066743, acc.: 98.44%] [G loss: 5.994654]\n",
      "epoch:41 step:32577 [D loss: 0.010187, acc.: 100.00%] [G loss: 4.563267]\n",
      "epoch:41 step:32578 [D loss: 0.140183, acc.: 96.88%] [G loss: 4.648136]\n",
      "epoch:41 step:32579 [D loss: 0.030376, acc.: 100.00%] [G loss: 5.406569]\n",
      "epoch:41 step:32580 [D loss: 0.010024, acc.: 100.00%] [G loss: 0.818614]\n",
      "epoch:41 step:32581 [D loss: 0.057135, acc.: 99.22%] [G loss: 5.301516]\n",
      "epoch:41 step:32582 [D loss: 0.047023, acc.: 99.22%] [G loss: 1.293205]\n",
      "epoch:41 step:32583 [D loss: 0.044780, acc.: 99.22%] [G loss: 4.667030]\n",
      "epoch:41 step:32584 [D loss: 0.046975, acc.: 100.00%] [G loss: 3.737111]\n",
      "epoch:41 step:32585 [D loss: 0.031519, acc.: 100.00%] [G loss: 3.374128]\n",
      "epoch:41 step:32586 [D loss: 0.259364, acc.: 87.50%] [G loss: 0.966694]\n",
      "epoch:41 step:32587 [D loss: 0.202168, acc.: 89.06%] [G loss: 4.088023]\n",
      "epoch:41 step:32588 [D loss: 0.053390, acc.: 98.44%] [G loss: 0.001845]\n",
      "epoch:41 step:32589 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.568274]\n",
      "epoch:41 step:32590 [D loss: 0.003343, acc.: 100.00%] [G loss: 0.247100]\n",
      "epoch:41 step:32591 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.306825]\n",
      "epoch:41 step:32592 [D loss: 0.010456, acc.: 100.00%] [G loss: 0.000732]\n",
      "epoch:41 step:32593 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.070300]\n",
      "epoch:41 step:32594 [D loss: 0.002572, acc.: 100.00%] [G loss: 0.018528]\n",
      "epoch:41 step:32595 [D loss: 0.000702, acc.: 100.00%] [G loss: 0.017772]\n",
      "epoch:41 step:32596 [D loss: 0.004854, acc.: 100.00%] [G loss: 0.000778]\n",
      "epoch:41 step:32597 [D loss: 0.002219, acc.: 100.00%] [G loss: 0.070432]\n",
      "epoch:41 step:32598 [D loss: 0.006034, acc.: 100.00%] [G loss: 0.014483]\n",
      "epoch:41 step:32599 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.016542]\n",
      "epoch:41 step:32600 [D loss: 0.008547, acc.: 100.00%] [G loss: 0.010279]\n",
      "epoch:41 step:32601 [D loss: 0.059572, acc.: 100.00%] [G loss: 0.033994]\n",
      "epoch:41 step:32602 [D loss: 0.011801, acc.: 100.00%] [G loss: 0.046169]\n",
      "epoch:41 step:32603 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.317619]\n",
      "epoch:41 step:32604 [D loss: 0.026571, acc.: 99.22%] [G loss: 0.030985]\n",
      "epoch:41 step:32605 [D loss: 0.017959, acc.: 100.00%] [G loss: 0.066184]\n",
      "epoch:41 step:32606 [D loss: 0.001160, acc.: 100.00%] [G loss: 0.025394]\n",
      "epoch:41 step:32607 [D loss: 0.001042, acc.: 100.00%] [G loss: 0.013496]\n",
      "epoch:41 step:32608 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.039259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32609 [D loss: 0.000728, acc.: 100.00%] [G loss: 0.001977]\n",
      "epoch:41 step:32610 [D loss: 0.001854, acc.: 100.00%] [G loss: 0.001217]\n",
      "epoch:41 step:32611 [D loss: 0.000624, acc.: 100.00%] [G loss: 0.016874]\n",
      "epoch:41 step:32612 [D loss: 0.006285, acc.: 100.00%] [G loss: 0.026313]\n",
      "epoch:41 step:32613 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.015775]\n",
      "epoch:41 step:32614 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.011841]\n",
      "epoch:41 step:32615 [D loss: 0.001326, acc.: 100.00%] [G loss: 0.006859]\n",
      "epoch:41 step:32616 [D loss: 0.000219, acc.: 100.00%] [G loss: 0.012626]\n",
      "epoch:41 step:32617 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.036678]\n",
      "epoch:41 step:32618 [D loss: 0.000927, acc.: 100.00%] [G loss: 0.112944]\n",
      "epoch:41 step:32619 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.001384]\n",
      "epoch:41 step:32620 [D loss: 0.025932, acc.: 100.00%] [G loss: 0.092568]\n",
      "epoch:41 step:32621 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.083480]\n",
      "epoch:41 step:32622 [D loss: 0.001277, acc.: 100.00%] [G loss: 0.086062]\n",
      "epoch:41 step:32623 [D loss: 0.002824, acc.: 100.00%] [G loss: 0.051792]\n",
      "epoch:41 step:32624 [D loss: 0.010276, acc.: 100.00%] [G loss: 0.009489]\n",
      "epoch:41 step:32625 [D loss: 0.019435, acc.: 100.00%] [G loss: 0.014663]\n",
      "epoch:41 step:32626 [D loss: 0.005835, acc.: 100.00%] [G loss: 0.005694]\n",
      "epoch:41 step:32627 [D loss: 0.082666, acc.: 99.22%] [G loss: 0.056709]\n",
      "epoch:41 step:32628 [D loss: 0.015283, acc.: 100.00%] [G loss: 1.032667]\n",
      "epoch:41 step:32629 [D loss: 0.001413, acc.: 100.00%] [G loss: 0.802242]\n",
      "epoch:41 step:32630 [D loss: 0.039464, acc.: 99.22%] [G loss: 0.031712]\n",
      "epoch:41 step:32631 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.024073]\n",
      "epoch:41 step:32632 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.004577]\n",
      "epoch:41 step:32633 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.003999]\n",
      "epoch:41 step:32634 [D loss: 0.131107, acc.: 92.97%] [G loss: 0.000042]\n",
      "epoch:41 step:32635 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:41 step:32636 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:41 step:32637 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:41 step:32638 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:41 step:32639 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:41 step:32640 [D loss: 0.001455, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:41 step:32641 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:41 step:32642 [D loss: 0.002835, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:41 step:32643 [D loss: 0.005843, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:41 step:32644 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:41 step:32645 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:41 step:32646 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:41 step:32647 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:41 step:32648 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:41 step:32649 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:41 step:32650 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:41 step:32651 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:41 step:32652 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000558]\n",
      "epoch:41 step:32653 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:41 step:32654 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:41 step:32655 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:41 step:32656 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:41 step:32657 [D loss: 0.001610, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:41 step:32658 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:41 step:32659 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:41 step:32660 [D loss: 0.002212, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:41 step:32661 [D loss: 0.028446, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:41 step:32662 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.001779]\n",
      "epoch:41 step:32663 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:41 step:32664 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:41 step:32665 [D loss: 0.000416, acc.: 100.00%] [G loss: 0.000537]\n",
      "epoch:41 step:32666 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000559]\n",
      "epoch:41 step:32667 [D loss: 0.027564, acc.: 99.22%] [G loss: 0.000178]\n",
      "epoch:41 step:32668 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:41 step:32669 [D loss: 0.172570, acc.: 90.62%] [G loss: 0.356828]\n",
      "epoch:41 step:32670 [D loss: 0.128498, acc.: 93.75%] [G loss: 0.258293]\n",
      "epoch:41 step:32671 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.084462]\n",
      "epoch:41 step:32672 [D loss: 0.001906, acc.: 100.00%] [G loss: 2.137232]\n",
      "epoch:41 step:32673 [D loss: 0.008716, acc.: 100.00%] [G loss: 0.072819]\n",
      "epoch:41 step:32674 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.072581]\n",
      "epoch:41 step:32675 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.273666]\n",
      "epoch:41 step:32676 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.376958]\n",
      "epoch:41 step:32677 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.060082]\n",
      "epoch:41 step:32678 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.016976]\n",
      "epoch:41 step:32679 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.001624]\n",
      "epoch:41 step:32680 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.001183]\n",
      "epoch:41 step:32681 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.000312]\n",
      "epoch:41 step:32682 [D loss: 0.031265, acc.: 100.00%] [G loss: 0.009068]\n",
      "epoch:41 step:32683 [D loss: 0.027276, acc.: 98.44%] [G loss: 0.002128]\n",
      "epoch:41 step:32684 [D loss: 0.002218, acc.: 100.00%] [G loss: 0.008164]\n",
      "epoch:41 step:32685 [D loss: 0.001047, acc.: 100.00%] [G loss: 0.079107]\n",
      "epoch:41 step:32686 [D loss: 0.198163, acc.: 92.19%] [G loss: 0.000000]\n",
      "epoch:41 step:32687 [D loss: 0.840751, acc.: 66.41%] [G loss: 3.375335]\n",
      "epoch:41 step:32688 [D loss: 0.048814, acc.: 99.22%] [G loss: 5.641472]\n",
      "epoch:41 step:32689 [D loss: 1.282896, acc.: 58.59%] [G loss: 0.429049]\n",
      "epoch:41 step:32690 [D loss: 0.959514, acc.: 73.44%] [G loss: 0.235858]\n",
      "epoch:41 step:32691 [D loss: 0.006492, acc.: 100.00%] [G loss: 8.180590]\n",
      "epoch:41 step:32692 [D loss: 0.166767, acc.: 92.97%] [G loss: 3.048970]\n",
      "epoch:41 step:32693 [D loss: 0.024996, acc.: 99.22%] [G loss: 0.028571]\n",
      "epoch:41 step:32694 [D loss: 0.006939, acc.: 100.00%] [G loss: 0.085198]\n",
      "epoch:41 step:32695 [D loss: 0.002364, acc.: 100.00%] [G loss: 0.022429]\n",
      "epoch:41 step:32696 [D loss: 0.001393, acc.: 100.00%] [G loss: 0.149722]\n",
      "epoch:41 step:32697 [D loss: 0.002125, acc.: 100.00%] [G loss: 0.000967]\n",
      "epoch:41 step:32698 [D loss: 0.002488, acc.: 100.00%] [G loss: 0.025990]\n",
      "epoch:41 step:32699 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.005197]\n",
      "epoch:41 step:32700 [D loss: 0.002535, acc.: 100.00%] [G loss: 1.061546]\n",
      "epoch:41 step:32701 [D loss: 0.001136, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:41 step:32702 [D loss: 0.024522, acc.: 99.22%] [G loss: 0.000079]\n",
      "epoch:41 step:32703 [D loss: 0.013944, acc.: 100.00%] [G loss: 0.001097]\n",
      "epoch:41 step:32704 [D loss: 0.000551, acc.: 100.00%] [G loss: 0.008674]\n",
      "epoch:41 step:32705 [D loss: 0.000918, acc.: 100.00%] [G loss: 0.000568]\n",
      "epoch:41 step:32706 [D loss: 0.001583, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:41 step:32707 [D loss: 0.007593, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:41 step:32708 [D loss: 0.016162, acc.: 100.00%] [G loss: 0.070503]\n",
      "epoch:41 step:32709 [D loss: 0.000193, acc.: 100.00%] [G loss: 0.226200]\n",
      "epoch:41 step:32710 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.001342]\n",
      "epoch:41 step:32711 [D loss: 0.000458, acc.: 100.00%] [G loss: 0.000926]\n",
      "epoch:41 step:32712 [D loss: 0.000721, acc.: 100.00%] [G loss: 0.002540]\n",
      "epoch:41 step:32713 [D loss: 0.011348, acc.: 99.22%] [G loss: 0.001360]\n",
      "epoch:41 step:32714 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.006504]\n",
      "epoch:41 step:32715 [D loss: 0.001411, acc.: 100.00%] [G loss: 0.021057]\n",
      "epoch:41 step:32716 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:41 step:32717 [D loss: 0.001470, acc.: 100.00%] [G loss: 0.000725]\n",
      "epoch:41 step:32718 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000750]\n",
      "epoch:41 step:32719 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:41 step:32720 [D loss: 0.000597, acc.: 100.00%] [G loss: 0.000690]\n",
      "epoch:41 step:32721 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.167071]\n",
      "epoch:41 step:32722 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.001460]\n",
      "epoch:41 step:32723 [D loss: 0.003122, acc.: 100.00%] [G loss: 0.007534]\n",
      "epoch:41 step:32724 [D loss: 0.001383, acc.: 100.00%] [G loss: 0.000256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:41 step:32725 [D loss: 0.007309, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:41 step:32726 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:41 step:32727 [D loss: 0.036508, acc.: 99.22%] [G loss: 0.002002]\n",
      "epoch:41 step:32728 [D loss: 0.000496, acc.: 100.00%] [G loss: 0.071755]\n",
      "epoch:41 step:32729 [D loss: 0.008113, acc.: 100.00%] [G loss: 0.003092]\n",
      "epoch:41 step:32730 [D loss: 0.000521, acc.: 100.00%] [G loss: 0.014674]\n",
      "epoch:41 step:32731 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.005384]\n",
      "epoch:41 step:32732 [D loss: 0.003264, acc.: 100.00%] [G loss: 0.029336]\n",
      "epoch:41 step:32733 [D loss: 0.001168, acc.: 100.00%] [G loss: 0.010407]\n",
      "epoch:41 step:32734 [D loss: 0.036599, acc.: 99.22%] [G loss: 0.000625]\n",
      "epoch:41 step:32735 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000590]\n",
      "epoch:41 step:32736 [D loss: 0.006611, acc.: 100.00%] [G loss: 0.000768]\n",
      "epoch:41 step:32737 [D loss: 0.000721, acc.: 100.00%] [G loss: 0.000573]\n",
      "epoch:41 step:32738 [D loss: 0.012146, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:41 step:32739 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.000847]\n",
      "epoch:41 step:32740 [D loss: 0.000449, acc.: 100.00%] [G loss: 0.000954]\n",
      "epoch:41 step:32741 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000665]\n",
      "epoch:41 step:32742 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.889933]\n",
      "epoch:41 step:32743 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:41 step:32744 [D loss: 0.000977, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:41 step:32745 [D loss: 0.385528, acc.: 77.34%] [G loss: 4.594592]\n",
      "epoch:41 step:32746 [D loss: 0.842385, acc.: 67.19%] [G loss: 0.293124]\n",
      "epoch:41 step:32747 [D loss: 0.012996, acc.: 100.00%] [G loss: 0.029429]\n",
      "epoch:41 step:32748 [D loss: 0.106795, acc.: 95.31%] [G loss: 0.216893]\n",
      "epoch:41 step:32749 [D loss: 0.002507, acc.: 100.00%] [G loss: 0.520096]\n",
      "epoch:41 step:32750 [D loss: 0.005107, acc.: 100.00%] [G loss: 0.461327]\n",
      "epoch:41 step:32751 [D loss: 0.039994, acc.: 98.44%] [G loss: 0.389749]\n",
      "epoch:41 step:32752 [D loss: 0.064384, acc.: 98.44%] [G loss: 4.767206]\n",
      "epoch:41 step:32753 [D loss: 0.078285, acc.: 96.88%] [G loss: 0.572433]\n",
      "epoch:41 step:32754 [D loss: 0.012807, acc.: 100.00%] [G loss: 3.687795]\n",
      "epoch:41 step:32755 [D loss: 0.039614, acc.: 100.00%] [G loss: 0.337527]\n",
      "epoch:41 step:32756 [D loss: 0.030324, acc.: 100.00%] [G loss: 0.233957]\n",
      "epoch:41 step:32757 [D loss: 0.026916, acc.: 99.22%] [G loss: 1.366831]\n",
      "epoch:41 step:32758 [D loss: 0.029292, acc.: 100.00%] [G loss: 0.062210]\n",
      "epoch:41 step:32759 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.031049]\n",
      "epoch:41 step:32760 [D loss: 0.003673, acc.: 100.00%] [G loss: 0.013798]\n",
      "epoch:41 step:32761 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.004978]\n",
      "epoch:41 step:32762 [D loss: 0.002291, acc.: 100.00%] [G loss: 0.002181]\n",
      "epoch:41 step:32763 [D loss: 0.001458, acc.: 100.00%] [G loss: 0.008795]\n",
      "epoch:41 step:32764 [D loss: 0.000389, acc.: 100.00%] [G loss: 0.001921]\n",
      "epoch:41 step:32765 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.004755]\n",
      "epoch:41 step:32766 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.022068]\n",
      "epoch:41 step:32767 [D loss: 0.005065, acc.: 100.00%] [G loss: 0.000444]\n",
      "epoch:41 step:32768 [D loss: 0.003587, acc.: 100.00%] [G loss: 0.012578]\n",
      "epoch:41 step:32769 [D loss: 0.015923, acc.: 100.00%] [G loss: 0.254254]\n",
      "epoch:41 step:32770 [D loss: 0.084113, acc.: 96.09%] [G loss: 0.081129]\n",
      "epoch:41 step:32771 [D loss: 0.020828, acc.: 100.00%] [G loss: 0.118273]\n",
      "epoch:41 step:32772 [D loss: 0.001789, acc.: 100.00%] [G loss: 0.015929]\n",
      "epoch:41 step:32773 [D loss: 0.005329, acc.: 100.00%] [G loss: 0.035979]\n",
      "epoch:41 step:32774 [D loss: 0.057155, acc.: 96.88%] [G loss: 0.000223]\n",
      "epoch:41 step:32775 [D loss: 0.006410, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:41 step:32776 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:41 step:32777 [D loss: 0.001298, acc.: 100.00%] [G loss: 0.013572]\n",
      "epoch:41 step:32778 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:41 step:32779 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:41 step:32780 [D loss: 0.000778, acc.: 100.00%] [G loss: 0.000507]\n",
      "epoch:41 step:32781 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:41 step:32782 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.006218]\n",
      "epoch:41 step:32783 [D loss: 0.000718, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:41 step:32784 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.004660]\n",
      "epoch:41 step:32785 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:41 step:32786 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.001709]\n",
      "epoch:41 step:32787 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:41 step:32788 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.001079]\n",
      "epoch:41 step:32789 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:41 step:32790 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:41 step:32791 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:41 step:32792 [D loss: 0.003781, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:41 step:32793 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.007803]\n",
      "epoch:41 step:32794 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.001383]\n",
      "epoch:41 step:32795 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:41 step:32796 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.015944]\n",
      "epoch:41 step:32797 [D loss: 0.000400, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:41 step:32798 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:41 step:32799 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:41 step:32800 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:41 step:32801 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:41 step:32802 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:32803 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.005946]\n",
      "epoch:42 step:32804 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.016104]\n",
      "epoch:42 step:32805 [D loss: 0.002315, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:42 step:32806 [D loss: 0.000949, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:42 step:32807 [D loss: 0.000594, acc.: 100.00%] [G loss: 0.211368]\n",
      "epoch:42 step:32808 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:42 step:32809 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:42 step:32810 [D loss: 0.005652, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:42 step:32811 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:42 step:32812 [D loss: 0.005839, acc.: 100.00%] [G loss: 0.011607]\n",
      "epoch:42 step:32813 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.005129]\n",
      "epoch:42 step:32814 [D loss: 0.001768, acc.: 100.00%] [G loss: 0.000331]\n",
      "epoch:42 step:32815 [D loss: 0.001289, acc.: 100.00%] [G loss: 0.348011]\n",
      "epoch:42 step:32816 [D loss: 0.002897, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:42 step:32817 [D loss: 0.127715, acc.: 96.88%] [G loss: 4.981740]\n",
      "epoch:42 step:32818 [D loss: 0.053180, acc.: 98.44%] [G loss: 1.281968]\n",
      "epoch:42 step:32819 [D loss: 0.040595, acc.: 98.44%] [G loss: 0.076010]\n",
      "epoch:42 step:32820 [D loss: 0.022644, acc.: 100.00%] [G loss: 0.506853]\n",
      "epoch:42 step:32821 [D loss: 0.001410, acc.: 100.00%] [G loss: 0.047628]\n",
      "epoch:42 step:32822 [D loss: 0.000492, acc.: 100.00%] [G loss: 0.162985]\n",
      "epoch:42 step:32823 [D loss: 0.015418, acc.: 99.22%] [G loss: 0.011060]\n",
      "epoch:42 step:32824 [D loss: 0.019254, acc.: 99.22%] [G loss: 0.000071]\n",
      "epoch:42 step:32825 [D loss: 0.004203, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:42 step:32826 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.067133]\n",
      "epoch:42 step:32827 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.015417]\n",
      "epoch:42 step:32828 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:32829 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:42 step:32830 [D loss: 0.001414, acc.: 100.00%] [G loss: 0.000511]\n",
      "epoch:42 step:32831 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:42 step:32832 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:42 step:32833 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.001425]\n",
      "epoch:42 step:32834 [D loss: 0.003545, acc.: 100.00%] [G loss: 0.000613]\n",
      "epoch:42 step:32835 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:32836 [D loss: 0.000377, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:42 step:32837 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:42 step:32838 [D loss: 0.004097, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:42 step:32839 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:42 step:32840 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.003356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:32841 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000417]\n",
      "epoch:42 step:32842 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:42 step:32843 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:42 step:32844 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:32845 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:42 step:32846 [D loss: 0.003055, acc.: 100.00%] [G loss: 0.000617]\n",
      "epoch:42 step:32847 [D loss: 0.003589, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:42 step:32848 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:32849 [D loss: 0.001081, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:32850 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000123]\n",
      "epoch:42 step:32851 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:42 step:32852 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:42 step:32853 [D loss: 0.022100, acc.: 99.22%] [G loss: 0.005731]\n",
      "epoch:42 step:32854 [D loss: 0.002541, acc.: 100.00%] [G loss: 0.000597]\n",
      "epoch:42 step:32855 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000551]\n",
      "epoch:42 step:32856 [D loss: 0.017622, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:42 step:32857 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:42 step:32858 [D loss: 0.006216, acc.: 100.00%] [G loss: 0.002039]\n",
      "epoch:42 step:32859 [D loss: 0.000643, acc.: 100.00%] [G loss: 0.000803]\n",
      "epoch:42 step:32860 [D loss: 0.003774, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:42 step:32861 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.027062]\n",
      "epoch:42 step:32862 [D loss: 0.015595, acc.: 100.00%] [G loss: 0.011929]\n",
      "epoch:42 step:32863 [D loss: 0.001139, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:42 step:32864 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.000197]\n",
      "epoch:42 step:32865 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.001989]\n",
      "epoch:42 step:32866 [D loss: 0.001439, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:42 step:32867 [D loss: 0.126617, acc.: 96.88%] [G loss: 0.000000]\n",
      "epoch:42 step:32868 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000000]\n",
      "epoch:42 step:32869 [D loss: 0.118015, acc.: 97.66%] [G loss: 0.000007]\n",
      "epoch:42 step:32870 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.050864]\n",
      "epoch:42 step:32871 [D loss: 0.004657, acc.: 100.00%] [G loss: 0.118260]\n",
      "epoch:42 step:32872 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.246727]\n",
      "epoch:42 step:32873 [D loss: 0.014181, acc.: 99.22%] [G loss: 0.000571]\n",
      "epoch:42 step:32874 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.000545]\n",
      "epoch:42 step:32875 [D loss: 0.000238, acc.: 100.00%] [G loss: 0.018377]\n",
      "epoch:42 step:32876 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.003109]\n",
      "epoch:42 step:32877 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:42 step:32878 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000514]\n",
      "epoch:42 step:32879 [D loss: 0.001336, acc.: 100.00%] [G loss: 0.000257]\n",
      "epoch:42 step:32880 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000870]\n",
      "epoch:42 step:32881 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.002161]\n",
      "epoch:42 step:32882 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.002734]\n",
      "epoch:42 step:32883 [D loss: 0.001195, acc.: 100.00%] [G loss: 0.000596]\n",
      "epoch:42 step:32884 [D loss: 0.005570, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:42 step:32885 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.001291]\n",
      "epoch:42 step:32886 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000522]\n",
      "epoch:42 step:32887 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.001518]\n",
      "epoch:42 step:32888 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.042023]\n",
      "epoch:42 step:32889 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.013699]\n",
      "epoch:42 step:32890 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.254481]\n",
      "epoch:42 step:32891 [D loss: 0.001355, acc.: 100.00%] [G loss: 0.001121]\n",
      "epoch:42 step:32892 [D loss: 0.034071, acc.: 98.44%] [G loss: 0.000001]\n",
      "epoch:42 step:32893 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:42 step:32894 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.004792]\n",
      "epoch:42 step:32895 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:32896 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:42 step:32897 [D loss: 0.007361, acc.: 100.00%] [G loss: 0.000689]\n",
      "epoch:42 step:32898 [D loss: 0.037925, acc.: 100.00%] [G loss: 1.864351]\n",
      "epoch:42 step:32899 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:42 step:32900 [D loss: 0.001829, acc.: 100.00%] [G loss: 0.004180]\n",
      "epoch:42 step:32901 [D loss: 0.047793, acc.: 99.22%] [G loss: 0.145822]\n",
      "epoch:42 step:32902 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.048989]\n",
      "epoch:42 step:32903 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:42 step:32904 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.000609]\n",
      "epoch:42 step:32905 [D loss: 0.001258, acc.: 100.00%] [G loss: 0.000700]\n",
      "epoch:42 step:32906 [D loss: 0.002055, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:32907 [D loss: 0.001272, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:32908 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000852]\n",
      "epoch:42 step:32909 [D loss: 0.000235, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:42 step:32910 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:42 step:32911 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:42 step:32912 [D loss: 0.001117, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:42 step:32913 [D loss: 0.003352, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:42 step:32914 [D loss: 0.001425, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:42 step:32915 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.002417]\n",
      "epoch:42 step:32916 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.024975]\n",
      "epoch:42 step:32917 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:42 step:32918 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.000393]\n",
      "epoch:42 step:32919 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:42 step:32920 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:42 step:32921 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:42 step:32922 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:42 step:32923 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:42 step:32924 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000705]\n",
      "epoch:42 step:32925 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.005854]\n",
      "epoch:42 step:32926 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.011973]\n",
      "epoch:42 step:32927 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:42 step:32928 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.021315]\n",
      "epoch:42 step:32929 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:32930 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.006293]\n",
      "epoch:42 step:32931 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:42 step:32932 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:32933 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.008376]\n",
      "epoch:42 step:32934 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:42 step:32935 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:42 step:32936 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.007529]\n",
      "epoch:42 step:32937 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.041028]\n",
      "epoch:42 step:32938 [D loss: 0.000967, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:42 step:32939 [D loss: 0.004798, acc.: 100.00%] [G loss: 0.012604]\n",
      "epoch:42 step:32940 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.014519]\n",
      "epoch:42 step:32941 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000398]\n",
      "epoch:42 step:32942 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:32943 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:42 step:32944 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:42 step:32945 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.010699]\n",
      "epoch:42 step:32946 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:42 step:32947 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:42 step:32948 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:42 step:32949 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:42 step:32950 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:42 step:32951 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:42 step:32952 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.007012]\n",
      "epoch:42 step:32953 [D loss: 0.000590, acc.: 100.00%] [G loss: 0.021477]\n",
      "epoch:42 step:32954 [D loss: 0.001946, acc.: 100.00%] [G loss: 0.001297]\n",
      "epoch:42 step:32955 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:42 step:32956 [D loss: 0.026567, acc.: 100.00%] [G loss: 0.050089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:32957 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.008928]\n",
      "epoch:42 step:32958 [D loss: 0.001665, acc.: 100.00%] [G loss: 0.011052]\n",
      "epoch:42 step:32959 [D loss: 0.003448, acc.: 100.00%] [G loss: 0.003269]\n",
      "epoch:42 step:32960 [D loss: 0.049135, acc.: 97.66%] [G loss: 0.000824]\n",
      "epoch:42 step:32961 [D loss: 0.042687, acc.: 99.22%] [G loss: 0.444234]\n",
      "epoch:42 step:32962 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.116565]\n",
      "epoch:42 step:32963 [D loss: 0.022888, acc.: 99.22%] [G loss: 0.083281]\n",
      "epoch:42 step:32964 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.064007]\n",
      "epoch:42 step:32965 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.019574]\n",
      "epoch:42 step:32966 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.006673]\n",
      "epoch:42 step:32967 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.038456]\n",
      "epoch:42 step:32968 [D loss: 0.008441, acc.: 99.22%] [G loss: 0.011196]\n",
      "epoch:42 step:32969 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001560]\n",
      "epoch:42 step:32970 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.004623]\n",
      "epoch:42 step:32971 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.001128]\n",
      "epoch:42 step:32972 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.004040]\n",
      "epoch:42 step:32973 [D loss: 0.000764, acc.: 100.00%] [G loss: 0.000838]\n",
      "epoch:42 step:32974 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.003045]\n",
      "epoch:42 step:32975 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000320]\n",
      "epoch:42 step:32976 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.003671]\n",
      "epoch:42 step:32977 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000851]\n",
      "epoch:42 step:32978 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:42 step:32979 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001924]\n",
      "epoch:42 step:32980 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.001255]\n",
      "epoch:42 step:32981 [D loss: 0.001443, acc.: 100.00%] [G loss: 0.001143]\n",
      "epoch:42 step:32982 [D loss: 0.001565, acc.: 100.00%] [G loss: 0.000819]\n",
      "epoch:42 step:32983 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:42 step:32984 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.001615]\n",
      "epoch:42 step:32985 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000872]\n",
      "epoch:42 step:32986 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000411]\n",
      "epoch:42 step:32987 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:42 step:32988 [D loss: 0.004610, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:42 step:32989 [D loss: 0.004517, acc.: 100.00%] [G loss: 0.010204]\n",
      "epoch:42 step:32990 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.001111]\n",
      "epoch:42 step:32991 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000402]\n",
      "epoch:42 step:32992 [D loss: 0.009318, acc.: 100.00%] [G loss: 0.001221]\n",
      "epoch:42 step:32993 [D loss: 0.006652, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:42 step:32994 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:42 step:32995 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:42 step:32996 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000093]\n",
      "epoch:42 step:32997 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.002742]\n",
      "epoch:42 step:32998 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:42 step:32999 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:42 step:33000 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.002128]\n",
      "epoch:42 step:33001 [D loss: 0.002346, acc.: 100.00%] [G loss: 0.000330]\n",
      "epoch:42 step:33002 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:42 step:33003 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:42 step:33004 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:42 step:33005 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:42 step:33006 [D loss: 0.001289, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:42 step:33007 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.000424]\n",
      "epoch:42 step:33008 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33009 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:33010 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:42 step:33011 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:42 step:33012 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:42 step:33013 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:42 step:33014 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:42 step:33015 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:42 step:33016 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000374]\n",
      "epoch:42 step:33017 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000373]\n",
      "epoch:42 step:33018 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:42 step:33019 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:42 step:33020 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.011118]\n",
      "epoch:42 step:33021 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:42 step:33022 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:42 step:33023 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:42 step:33024 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.056410]\n",
      "epoch:42 step:33025 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:42 step:33026 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:42 step:33027 [D loss: 0.004912, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:42 step:33028 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.016457]\n",
      "epoch:42 step:33029 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000676]\n",
      "epoch:42 step:33030 [D loss: 0.002302, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:42 step:33031 [D loss: 0.002211, acc.: 100.00%] [G loss: 0.000527]\n",
      "epoch:42 step:33032 [D loss: 0.001715, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:42 step:33033 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:42 step:33034 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:42 step:33035 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000578]\n",
      "epoch:42 step:33036 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.001265]\n",
      "epoch:42 step:33037 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33038 [D loss: 0.001231, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:42 step:33039 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:42 step:33040 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:42 step:33041 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:42 step:33042 [D loss: 0.006053, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:42 step:33043 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:42 step:33044 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:42 step:33045 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:33046 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:42 step:33047 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:42 step:33048 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:42 step:33049 [D loss: 0.000895, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:42 step:33050 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:42 step:33051 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:42 step:33052 [D loss: 0.000886, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:42 step:33053 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:42 step:33054 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:42 step:33055 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:42 step:33056 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:42 step:33057 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:42 step:33058 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:33059 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:42 step:33060 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:42 step:33061 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:42 step:33062 [D loss: 0.000519, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:42 step:33063 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:42 step:33064 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.023133]\n",
      "epoch:42 step:33065 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:33066 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:42 step:33067 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:42 step:33068 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:42 step:33069 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000426]\n",
      "epoch:42 step:33070 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:42 step:33071 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:42 step:33072 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33073 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.004546]\n",
      "epoch:42 step:33074 [D loss: 0.017288, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:42 step:33075 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.366629]\n",
      "epoch:42 step:33076 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.000620]\n",
      "epoch:42 step:33077 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.114297]\n",
      "epoch:42 step:33078 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.060238]\n",
      "epoch:42 step:33079 [D loss: 1.324608, acc.: 51.56%] [G loss: 5.808799]\n",
      "epoch:42 step:33080 [D loss: 3.098919, acc.: 46.09%] [G loss: 0.247834]\n",
      "epoch:42 step:33081 [D loss: 0.347998, acc.: 79.69%] [G loss: 1.219494]\n",
      "epoch:42 step:33082 [D loss: 0.118727, acc.: 92.19%] [G loss: 0.453876]\n",
      "epoch:42 step:33083 [D loss: 0.279880, acc.: 85.16%] [G loss: 0.004422]\n",
      "epoch:42 step:33084 [D loss: 0.100006, acc.: 96.88%] [G loss: 3.750013]\n",
      "epoch:42 step:33085 [D loss: 0.012831, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:42 step:33086 [D loss: 0.071786, acc.: 97.66%] [G loss: 2.694265]\n",
      "epoch:42 step:33087 [D loss: 0.035153, acc.: 99.22%] [G loss: 0.000056]\n",
      "epoch:42 step:33088 [D loss: 0.099730, acc.: 95.31%] [G loss: 0.000517]\n",
      "epoch:42 step:33089 [D loss: 0.023561, acc.: 99.22%] [G loss: 0.001141]\n",
      "epoch:42 step:33090 [D loss: 0.049092, acc.: 98.44%] [G loss: 0.000405]\n",
      "epoch:42 step:33091 [D loss: 0.041092, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:42 step:33092 [D loss: 0.042323, acc.: 99.22%] [G loss: 0.000857]\n",
      "epoch:42 step:33093 [D loss: 0.137589, acc.: 96.09%] [G loss: 0.003957]\n",
      "epoch:42 step:33094 [D loss: 0.062464, acc.: 98.44%] [G loss: 0.004206]\n",
      "epoch:42 step:33095 [D loss: 0.019868, acc.: 100.00%] [G loss: 0.028065]\n",
      "epoch:42 step:33096 [D loss: 0.135020, acc.: 96.88%] [G loss: 0.005422]\n",
      "epoch:42 step:33097 [D loss: 0.108468, acc.: 97.66%] [G loss: 0.002024]\n",
      "epoch:42 step:33098 [D loss: 0.028559, acc.: 100.00%] [G loss: 0.002536]\n",
      "epoch:42 step:33099 [D loss: 0.086169, acc.: 96.88%] [G loss: 4.119764]\n",
      "epoch:42 step:33100 [D loss: 0.065990, acc.: 98.44%] [G loss: 0.319525]\n",
      "epoch:42 step:33101 [D loss: 0.096318, acc.: 96.88%] [G loss: 0.428534]\n",
      "epoch:42 step:33102 [D loss: 1.824061, acc.: 40.62%] [G loss: 6.805043]\n",
      "epoch:42 step:33103 [D loss: 0.559323, acc.: 75.00%] [G loss: 5.918345]\n",
      "epoch:42 step:33104 [D loss: 0.381930, acc.: 83.59%] [G loss: 5.186284]\n",
      "epoch:42 step:33105 [D loss: 0.055080, acc.: 98.44%] [G loss: 3.434197]\n",
      "epoch:42 step:33106 [D loss: 0.086831, acc.: 96.88%] [G loss: 1.815819]\n",
      "epoch:42 step:33107 [D loss: 0.228442, acc.: 89.06%] [G loss: 1.023335]\n",
      "epoch:42 step:33108 [D loss: 0.089500, acc.: 96.88%] [G loss: 2.540801]\n",
      "epoch:42 step:33109 [D loss: 0.033851, acc.: 98.44%] [G loss: 1.710009]\n",
      "epoch:42 step:33110 [D loss: 0.112550, acc.: 96.88%] [G loss: 0.430111]\n",
      "epoch:42 step:33111 [D loss: 0.038119, acc.: 99.22%] [G loss: 0.214081]\n",
      "epoch:42 step:33112 [D loss: 0.240798, acc.: 91.41%] [G loss: 1.459503]\n",
      "epoch:42 step:33113 [D loss: 0.201264, acc.: 92.97%] [G loss: 0.991701]\n",
      "epoch:42 step:33114 [D loss: 0.113601, acc.: 96.88%] [G loss: 0.211623]\n",
      "epoch:42 step:33115 [D loss: 0.073036, acc.: 98.44%] [G loss: 0.112837]\n",
      "epoch:42 step:33116 [D loss: 0.015845, acc.: 99.22%] [G loss: 0.012291]\n",
      "epoch:42 step:33117 [D loss: 0.396116, acc.: 82.03%] [G loss: 0.000558]\n",
      "epoch:42 step:33118 [D loss: 0.384746, acc.: 85.16%] [G loss: 0.163583]\n",
      "epoch:42 step:33119 [D loss: 0.001818, acc.: 100.00%] [G loss: 0.537683]\n",
      "epoch:42 step:33120 [D loss: 0.039807, acc.: 99.22%] [G loss: 0.410429]\n",
      "epoch:42 step:33121 [D loss: 0.012202, acc.: 100.00%] [G loss: 0.147009]\n",
      "epoch:42 step:33122 [D loss: 0.059221, acc.: 98.44%] [G loss: 0.059866]\n",
      "epoch:42 step:33123 [D loss: 0.013895, acc.: 100.00%] [G loss: 0.031677]\n",
      "epoch:42 step:33124 [D loss: 0.030977, acc.: 100.00%] [G loss: 0.039818]\n",
      "epoch:42 step:33125 [D loss: 0.003740, acc.: 100.00%] [G loss: 0.020064]\n",
      "epoch:42 step:33126 [D loss: 0.086726, acc.: 100.00%] [G loss: 0.149643]\n",
      "epoch:42 step:33127 [D loss: 0.044735, acc.: 98.44%] [G loss: 0.639226]\n",
      "epoch:42 step:33128 [D loss: 0.014714, acc.: 100.00%] [G loss: 5.641942]\n",
      "epoch:42 step:33129 [D loss: 0.029691, acc.: 99.22%] [G loss: 1.626334]\n",
      "epoch:42 step:33130 [D loss: 0.050575, acc.: 98.44%] [G loss: 0.155025]\n",
      "epoch:42 step:33131 [D loss: 0.022559, acc.: 100.00%] [G loss: 0.064699]\n",
      "epoch:42 step:33132 [D loss: 0.032338, acc.: 99.22%] [G loss: 0.135345]\n",
      "epoch:42 step:33133 [D loss: 0.075542, acc.: 96.88%] [G loss: 0.026844]\n",
      "epoch:42 step:33134 [D loss: 0.077139, acc.: 97.66%] [G loss: 0.050938]\n",
      "epoch:42 step:33135 [D loss: 0.007902, acc.: 100.00%] [G loss: 0.061039]\n",
      "epoch:42 step:33136 [D loss: 0.087583, acc.: 99.22%] [G loss: 0.105416]\n",
      "epoch:42 step:33137 [D loss: 0.011854, acc.: 100.00%] [G loss: 0.041232]\n",
      "epoch:42 step:33138 [D loss: 0.013590, acc.: 99.22%] [G loss: 0.029024]\n",
      "epoch:42 step:33139 [D loss: 0.052220, acc.: 99.22%] [G loss: 0.039867]\n",
      "epoch:42 step:33140 [D loss: 0.242767, acc.: 94.53%] [G loss: 0.032145]\n",
      "epoch:42 step:33141 [D loss: 0.153332, acc.: 92.19%] [G loss: 1.221850]\n",
      "epoch:42 step:33142 [D loss: 0.031058, acc.: 99.22%] [G loss: 1.357263]\n",
      "epoch:42 step:33143 [D loss: 0.011756, acc.: 100.00%] [G loss: 0.196171]\n",
      "epoch:42 step:33144 [D loss: 0.102900, acc.: 96.09%] [G loss: 0.581296]\n",
      "epoch:42 step:33145 [D loss: 0.014619, acc.: 100.00%] [G loss: 0.000362]\n",
      "epoch:42 step:33146 [D loss: 0.014624, acc.: 100.00%] [G loss: 0.040860]\n",
      "epoch:42 step:33147 [D loss: 0.058670, acc.: 97.66%] [G loss: 0.025931]\n",
      "epoch:42 step:33148 [D loss: 0.009043, acc.: 100.00%] [G loss: 0.183244]\n",
      "epoch:42 step:33149 [D loss: 0.023950, acc.: 100.00%] [G loss: 0.194686]\n",
      "epoch:42 step:33150 [D loss: 0.007820, acc.: 100.00%] [G loss: 0.034309]\n",
      "epoch:42 step:33151 [D loss: 0.019835, acc.: 99.22%] [G loss: 0.011992]\n",
      "epoch:42 step:33152 [D loss: 0.185923, acc.: 94.53%] [G loss: 0.028625]\n",
      "epoch:42 step:33153 [D loss: 0.001058, acc.: 100.00%] [G loss: 0.015651]\n",
      "epoch:42 step:33154 [D loss: 0.001973, acc.: 100.00%] [G loss: 0.035186]\n",
      "epoch:42 step:33155 [D loss: 0.121635, acc.: 95.31%] [G loss: 0.002120]\n",
      "epoch:42 step:33156 [D loss: 0.003545, acc.: 100.00%] [G loss: 0.003654]\n",
      "epoch:42 step:33157 [D loss: 0.044973, acc.: 100.00%] [G loss: 0.001733]\n",
      "epoch:42 step:33158 [D loss: 0.001658, acc.: 100.00%] [G loss: 0.044745]\n",
      "epoch:42 step:33159 [D loss: 0.018702, acc.: 100.00%] [G loss: 0.011466]\n",
      "epoch:42 step:33160 [D loss: 0.002741, acc.: 100.00%] [G loss: 0.008639]\n",
      "epoch:42 step:33161 [D loss: 0.002125, acc.: 100.00%] [G loss: 0.008026]\n",
      "epoch:42 step:33162 [D loss: 0.004086, acc.: 100.00%] [G loss: 0.020694]\n",
      "epoch:42 step:33163 [D loss: 0.007642, acc.: 100.00%] [G loss: 0.019495]\n",
      "epoch:42 step:33164 [D loss: 0.000346, acc.: 100.00%] [G loss: 0.005582]\n",
      "epoch:42 step:33165 [D loss: 0.002339, acc.: 100.00%] [G loss: 0.003015]\n",
      "epoch:42 step:33166 [D loss: 0.016493, acc.: 99.22%] [G loss: 0.002510]\n",
      "epoch:42 step:33167 [D loss: 0.001651, acc.: 100.00%] [G loss: 0.000909]\n",
      "epoch:42 step:33168 [D loss: 0.001113, acc.: 100.00%] [G loss: 0.000601]\n",
      "epoch:42 step:33169 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.001457]\n",
      "epoch:42 step:33170 [D loss: 0.007560, acc.: 100.00%] [G loss: 0.001793]\n",
      "epoch:42 step:33171 [D loss: 0.005260, acc.: 100.00%] [G loss: 0.000917]\n",
      "epoch:42 step:33172 [D loss: 0.153442, acc.: 93.75%] [G loss: 0.513636]\n",
      "epoch:42 step:33173 [D loss: 0.012442, acc.: 99.22%] [G loss: 1.827296]\n",
      "epoch:42 step:33174 [D loss: 0.066376, acc.: 96.88%] [G loss: 1.514256]\n",
      "epoch:42 step:33175 [D loss: 0.033481, acc.: 100.00%] [G loss: 1.180966]\n",
      "epoch:42 step:33176 [D loss: 0.250858, acc.: 86.72%] [G loss: 0.006744]\n",
      "epoch:42 step:33177 [D loss: 0.006733, acc.: 99.22%] [G loss: 0.015381]\n",
      "epoch:42 step:33178 [D loss: 0.001031, acc.: 100.00%] [G loss: 0.002171]\n",
      "epoch:42 step:33179 [D loss: 0.002231, acc.: 100.00%] [G loss: 0.000499]\n",
      "epoch:42 step:33180 [D loss: 0.002172, acc.: 100.00%] [G loss: 0.000392]\n",
      "epoch:42 step:33181 [D loss: 0.032957, acc.: 99.22%] [G loss: 0.002747]\n",
      "epoch:42 step:33182 [D loss: 0.001697, acc.: 100.00%] [G loss: 0.044892]\n",
      "epoch:42 step:33183 [D loss: 0.001975, acc.: 100.00%] [G loss: 0.006709]\n",
      "epoch:42 step:33184 [D loss: 0.006926, acc.: 100.00%] [G loss: 0.031494]\n",
      "epoch:42 step:33185 [D loss: 0.098758, acc.: 96.09%] [G loss: 0.279177]\n",
      "epoch:42 step:33186 [D loss: 0.002812, acc.: 100.00%] [G loss: 0.366604]\n",
      "epoch:42 step:33187 [D loss: 0.005864, acc.: 100.00%] [G loss: 0.642725]\n",
      "epoch:42 step:33188 [D loss: 0.045965, acc.: 97.66%] [G loss: 0.260282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33189 [D loss: 0.001347, acc.: 100.00%] [G loss: 0.001044]\n",
      "epoch:42 step:33190 [D loss: 0.075123, acc.: 97.66%] [G loss: 0.042289]\n",
      "epoch:42 step:33191 [D loss: 0.003605, acc.: 100.00%] [G loss: 0.008635]\n",
      "epoch:42 step:33192 [D loss: 0.003157, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:42 step:33193 [D loss: 0.500350, acc.: 83.59%] [G loss: 4.044630]\n",
      "epoch:42 step:33194 [D loss: 0.082912, acc.: 97.66%] [G loss: 1.011969]\n",
      "epoch:42 step:33195 [D loss: 0.437490, acc.: 83.59%] [G loss: 0.001405]\n",
      "epoch:42 step:33196 [D loss: 0.005646, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:42 step:33197 [D loss: 0.027710, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:42 step:33198 [D loss: 0.018977, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:42 step:33199 [D loss: 0.022550, acc.: 99.22%] [G loss: 0.000833]\n",
      "epoch:42 step:33200 [D loss: 0.056422, acc.: 98.44%] [G loss: 0.001481]\n",
      "epoch:42 step:33201 [D loss: 0.023112, acc.: 100.00%] [G loss: 1.225585]\n",
      "epoch:42 step:33202 [D loss: 0.159375, acc.: 96.09%] [G loss: 1.488556]\n",
      "epoch:42 step:33203 [D loss: 0.080383, acc.: 96.09%] [G loss: 2.145779]\n",
      "epoch:42 step:33204 [D loss: 0.142886, acc.: 94.53%] [G loss: 0.992979]\n",
      "epoch:42 step:33205 [D loss: 0.010817, acc.: 100.00%] [G loss: 0.110133]\n",
      "epoch:42 step:33206 [D loss: 0.009132, acc.: 100.00%] [G loss: 0.039982]\n",
      "epoch:42 step:33207 [D loss: 0.008263, acc.: 100.00%] [G loss: 0.019499]\n",
      "epoch:42 step:33208 [D loss: 0.055574, acc.: 100.00%] [G loss: 0.041442]\n",
      "epoch:42 step:33209 [D loss: 0.054058, acc.: 98.44%] [G loss: 0.186256]\n",
      "epoch:42 step:33210 [D loss: 0.010432, acc.: 100.00%] [G loss: 0.196282]\n",
      "epoch:42 step:33211 [D loss: 0.017329, acc.: 99.22%] [G loss: 0.066940]\n",
      "epoch:42 step:33212 [D loss: 0.000860, acc.: 100.00%] [G loss: 0.094685]\n",
      "epoch:42 step:33213 [D loss: 0.060338, acc.: 97.66%] [G loss: 0.047880]\n",
      "epoch:42 step:33214 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.009752]\n",
      "epoch:42 step:33215 [D loss: 0.007772, acc.: 100.00%] [G loss: 0.004747]\n",
      "epoch:42 step:33216 [D loss: 0.002704, acc.: 100.00%] [G loss: 0.004076]\n",
      "epoch:42 step:33217 [D loss: 0.006561, acc.: 100.00%] [G loss: 0.011805]\n",
      "epoch:42 step:33218 [D loss: 0.000402, acc.: 100.00%] [G loss: 0.000718]\n",
      "epoch:42 step:33219 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.001685]\n",
      "epoch:42 step:33220 [D loss: 0.000491, acc.: 100.00%] [G loss: 0.002584]\n",
      "epoch:42 step:33221 [D loss: 0.001021, acc.: 100.00%] [G loss: 0.000967]\n",
      "epoch:42 step:33222 [D loss: 0.001371, acc.: 100.00%] [G loss: 0.003186]\n",
      "epoch:42 step:33223 [D loss: 0.001594, acc.: 100.00%] [G loss: 0.002434]\n",
      "epoch:42 step:33224 [D loss: 0.002748, acc.: 100.00%] [G loss: 0.003086]\n",
      "epoch:42 step:33225 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000617]\n",
      "epoch:42 step:33226 [D loss: 0.004370, acc.: 100.00%] [G loss: 1.730749]\n",
      "epoch:42 step:33227 [D loss: 0.000261, acc.: 100.00%] [G loss: 0.000786]\n",
      "epoch:42 step:33228 [D loss: 0.000405, acc.: 100.00%] [G loss: 0.000933]\n",
      "epoch:42 step:33229 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.003931]\n",
      "epoch:42 step:33230 [D loss: 0.004557, acc.: 100.00%] [G loss: 0.000685]\n",
      "epoch:42 step:33231 [D loss: 0.032268, acc.: 100.00%] [G loss: 0.005042]\n",
      "epoch:42 step:33232 [D loss: 0.063117, acc.: 97.66%] [G loss: 0.000503]\n",
      "epoch:42 step:33233 [D loss: 0.001985, acc.: 100.00%] [G loss: 0.001125]\n",
      "epoch:42 step:33234 [D loss: 0.000638, acc.: 100.00%] [G loss: 0.000915]\n",
      "epoch:42 step:33235 [D loss: 0.000365, acc.: 100.00%] [G loss: 0.002224]\n",
      "epoch:42 step:33236 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:42 step:33237 [D loss: 0.006332, acc.: 100.00%] [G loss: 0.002091]\n",
      "epoch:42 step:33238 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.008743]\n",
      "epoch:42 step:33239 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:42 step:33240 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.001438]\n",
      "epoch:42 step:33241 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.003247]\n",
      "epoch:42 step:33242 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.011407]\n",
      "epoch:42 step:33243 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:42 step:33244 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:42 step:33245 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000619]\n",
      "epoch:42 step:33246 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:42 step:33247 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.003542]\n",
      "epoch:42 step:33248 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.004654]\n",
      "epoch:42 step:33249 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.001343]\n",
      "epoch:42 step:33250 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.001165]\n",
      "epoch:42 step:33251 [D loss: 0.001025, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:42 step:33252 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.000349]\n",
      "epoch:42 step:33253 [D loss: 0.006458, acc.: 99.22%] [G loss: 0.009204]\n",
      "epoch:42 step:33254 [D loss: 0.006501, acc.: 100.00%] [G loss: 0.000473]\n",
      "epoch:42 step:33255 [D loss: 0.011222, acc.: 100.00%] [G loss: 0.002675]\n",
      "epoch:42 step:33256 [D loss: 0.000573, acc.: 100.00%] [G loss: 0.006168]\n",
      "epoch:42 step:33257 [D loss: 0.007356, acc.: 100.00%] [G loss: 0.006182]\n",
      "epoch:42 step:33258 [D loss: 0.000698, acc.: 100.00%] [G loss: 0.000894]\n",
      "epoch:42 step:33259 [D loss: 0.038185, acc.: 99.22%] [G loss: 0.059611]\n",
      "epoch:42 step:33260 [D loss: 0.001645, acc.: 100.00%] [G loss: 0.071305]\n",
      "epoch:42 step:33261 [D loss: 0.004107, acc.: 100.00%] [G loss: 0.717937]\n",
      "epoch:42 step:33262 [D loss: 0.008294, acc.: 100.00%] [G loss: 0.077080]\n",
      "epoch:42 step:33263 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.027970]\n",
      "epoch:42 step:33264 [D loss: 0.000901, acc.: 100.00%] [G loss: 0.033769]\n",
      "epoch:42 step:33265 [D loss: 0.002274, acc.: 100.00%] [G loss: 0.015327]\n",
      "epoch:42 step:33266 [D loss: 0.002842, acc.: 100.00%] [G loss: 0.001542]\n",
      "epoch:42 step:33267 [D loss: 0.000750, acc.: 100.00%] [G loss: 0.012375]\n",
      "epoch:42 step:33268 [D loss: 0.004677, acc.: 100.00%] [G loss: 0.017877]\n",
      "epoch:42 step:33269 [D loss: 0.011845, acc.: 100.00%] [G loss: 0.003234]\n",
      "epoch:42 step:33270 [D loss: 0.057762, acc.: 99.22%] [G loss: 0.176472]\n",
      "epoch:42 step:33271 [D loss: 0.001162, acc.: 100.00%] [G loss: 0.842194]\n",
      "epoch:42 step:33272 [D loss: 0.023501, acc.: 100.00%] [G loss: 0.123494]\n",
      "epoch:42 step:33273 [D loss: 0.028545, acc.: 98.44%] [G loss: 0.086554]\n",
      "epoch:42 step:33274 [D loss: 0.038028, acc.: 100.00%] [G loss: 0.047012]\n",
      "epoch:42 step:33275 [D loss: 0.016847, acc.: 100.00%] [G loss: 0.157287]\n",
      "epoch:42 step:33276 [D loss: 0.000979, acc.: 100.00%] [G loss: 0.112857]\n",
      "epoch:42 step:33277 [D loss: 0.040458, acc.: 99.22%] [G loss: 0.005647]\n",
      "epoch:42 step:33278 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.019317]\n",
      "epoch:42 step:33279 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:42 step:33280 [D loss: 0.003460, acc.: 100.00%] [G loss: 0.005201]\n",
      "epoch:42 step:33281 [D loss: 0.001182, acc.: 100.00%] [G loss: 0.005848]\n",
      "epoch:42 step:33282 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000490]\n",
      "epoch:42 step:33283 [D loss: 0.000537, acc.: 100.00%] [G loss: 0.001331]\n",
      "epoch:42 step:33284 [D loss: 0.001795, acc.: 100.00%] [G loss: 0.005832]\n",
      "epoch:42 step:33285 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.004489]\n",
      "epoch:42 step:33286 [D loss: 0.000985, acc.: 100.00%] [G loss: 0.002018]\n",
      "epoch:42 step:33287 [D loss: 0.001004, acc.: 100.00%] [G loss: 0.003018]\n",
      "epoch:42 step:33288 [D loss: 0.002933, acc.: 100.00%] [G loss: 0.020983]\n",
      "epoch:42 step:33289 [D loss: 0.001154, acc.: 100.00%] [G loss: 0.003091]\n",
      "epoch:42 step:33290 [D loss: 0.036043, acc.: 100.00%] [G loss: 0.006033]\n",
      "epoch:42 step:33291 [D loss: 0.000479, acc.: 100.00%] [G loss: 0.061236]\n",
      "epoch:42 step:33292 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.221085]\n",
      "epoch:42 step:33293 [D loss: 0.088299, acc.: 95.31%] [G loss: 0.080464]\n",
      "epoch:42 step:33294 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.008629]\n",
      "epoch:42 step:33295 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.007015]\n",
      "epoch:42 step:33296 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.002070]\n",
      "epoch:42 step:33297 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001476]\n",
      "epoch:42 step:33298 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:42 step:33299 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.001484]\n",
      "epoch:42 step:33300 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.015465]\n",
      "epoch:42 step:33301 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000761]\n",
      "epoch:42 step:33302 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000976]\n",
      "epoch:42 step:33303 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:33304 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33305 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001859]\n",
      "epoch:42 step:33306 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.003871]\n",
      "epoch:42 step:33307 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.003040]\n",
      "epoch:42 step:33308 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.000987]\n",
      "epoch:42 step:33309 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.001493]\n",
      "epoch:42 step:33310 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.007677]\n",
      "epoch:42 step:33311 [D loss: 0.002468, acc.: 100.00%] [G loss: 0.002862]\n",
      "epoch:42 step:33312 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.183661]\n",
      "epoch:42 step:33313 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.001484]\n",
      "epoch:42 step:33314 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:42 step:33315 [D loss: 0.000931, acc.: 100.00%] [G loss: 0.000239]\n",
      "epoch:42 step:33316 [D loss: 0.148708, acc.: 92.19%] [G loss: 0.319983]\n",
      "epoch:42 step:33317 [D loss: 0.005235, acc.: 100.00%] [G loss: 2.633991]\n",
      "epoch:42 step:33318 [D loss: 0.059841, acc.: 96.88%] [G loss: 0.575871]\n",
      "epoch:42 step:33319 [D loss: 0.010102, acc.: 100.00%] [G loss: 0.112674]\n",
      "epoch:42 step:33320 [D loss: 0.001376, acc.: 100.00%] [G loss: 0.046642]\n",
      "epoch:42 step:33321 [D loss: 0.139133, acc.: 93.75%] [G loss: 4.691276]\n",
      "epoch:42 step:33322 [D loss: 0.221898, acc.: 88.28%] [G loss: 2.019914]\n",
      "epoch:42 step:33323 [D loss: 0.064956, acc.: 98.44%] [G loss: 2.751247]\n",
      "epoch:42 step:33324 [D loss: 0.080381, acc.: 97.66%] [G loss: 3.063714]\n",
      "epoch:42 step:33325 [D loss: 0.034404, acc.: 100.00%] [G loss: 3.791981]\n",
      "epoch:42 step:33326 [D loss: 1.477209, acc.: 42.97%] [G loss: 10.700422]\n",
      "epoch:42 step:33327 [D loss: 1.082216, acc.: 67.19%] [G loss: 2.334615]\n",
      "epoch:42 step:33328 [D loss: 0.439056, acc.: 80.47%] [G loss: 5.633279]\n",
      "epoch:42 step:33329 [D loss: 0.012406, acc.: 100.00%] [G loss: 3.651443]\n",
      "epoch:42 step:33330 [D loss: 0.206721, acc.: 92.97%] [G loss: 5.123479]\n",
      "epoch:42 step:33331 [D loss: 0.015237, acc.: 100.00%] [G loss: 4.955560]\n",
      "epoch:42 step:33332 [D loss: 0.017936, acc.: 99.22%] [G loss: 0.157922]\n",
      "epoch:42 step:33333 [D loss: 0.010332, acc.: 100.00%] [G loss: 3.578035]\n",
      "epoch:42 step:33334 [D loss: 0.084721, acc.: 97.66%] [G loss: 2.310328]\n",
      "epoch:42 step:33335 [D loss: 0.021150, acc.: 100.00%] [G loss: 0.076945]\n",
      "epoch:42 step:33336 [D loss: 0.013962, acc.: 100.00%] [G loss: 2.029928]\n",
      "epoch:42 step:33337 [D loss: 0.017835, acc.: 99.22%] [G loss: 1.383891]\n",
      "epoch:42 step:33338 [D loss: 0.739477, acc.: 65.62%] [G loss: 5.703889]\n",
      "epoch:42 step:33339 [D loss: 1.385977, acc.: 56.25%] [G loss: 3.031206]\n",
      "epoch:42 step:33340 [D loss: 0.088953, acc.: 96.09%] [G loss: 2.249927]\n",
      "epoch:42 step:33341 [D loss: 0.056159, acc.: 98.44%] [G loss: 0.112738]\n",
      "epoch:42 step:33342 [D loss: 0.025510, acc.: 99.22%] [G loss: 2.166465]\n",
      "epoch:42 step:33343 [D loss: 0.014968, acc.: 100.00%] [G loss: 2.052496]\n",
      "epoch:42 step:33344 [D loss: 0.041012, acc.: 100.00%] [G loss: 2.383834]\n",
      "epoch:42 step:33345 [D loss: 0.031512, acc.: 100.00%] [G loss: 1.855168]\n",
      "epoch:42 step:33346 [D loss: 0.053518, acc.: 99.22%] [G loss: 1.441775]\n",
      "epoch:42 step:33347 [D loss: 0.044731, acc.: 100.00%] [G loss: 1.368589]\n",
      "epoch:42 step:33348 [D loss: 0.049237, acc.: 100.00%] [G loss: 2.778777]\n",
      "epoch:42 step:33349 [D loss: 0.119620, acc.: 96.88%] [G loss: 0.344552]\n",
      "epoch:42 step:33350 [D loss: 0.022570, acc.: 99.22%] [G loss: 2.029836]\n",
      "epoch:42 step:33351 [D loss: 0.029447, acc.: 100.00%] [G loss: 1.924521]\n",
      "epoch:42 step:33352 [D loss: 0.204061, acc.: 90.62%] [G loss: 0.893585]\n",
      "epoch:42 step:33353 [D loss: 0.043003, acc.: 99.22%] [G loss: 3.376425]\n",
      "epoch:42 step:33354 [D loss: 0.175140, acc.: 92.97%] [G loss: 0.983139]\n",
      "epoch:42 step:33355 [D loss: 0.017382, acc.: 100.00%] [G loss: 0.824879]\n",
      "epoch:42 step:33356 [D loss: 0.095800, acc.: 95.31%] [G loss: 1.079829]\n",
      "epoch:42 step:33357 [D loss: 0.097169, acc.: 98.44%] [G loss: 2.258911]\n",
      "epoch:42 step:33358 [D loss: 0.142271, acc.: 95.31%] [G loss: 1.132389]\n",
      "epoch:42 step:33359 [D loss: 0.067756, acc.: 97.66%] [G loss: 0.741841]\n",
      "epoch:42 step:33360 [D loss: 0.386131, acc.: 78.12%] [G loss: 4.228210]\n",
      "epoch:42 step:33361 [D loss: 0.094730, acc.: 96.09%] [G loss: 2.363060]\n",
      "epoch:42 step:33362 [D loss: 0.398707, acc.: 85.94%] [G loss: 0.197070]\n",
      "epoch:42 step:33363 [D loss: 0.006702, acc.: 100.00%] [G loss: 1.708480]\n",
      "epoch:42 step:33364 [D loss: 0.019139, acc.: 99.22%] [G loss: 0.707733]\n",
      "epoch:42 step:33365 [D loss: 0.014972, acc.: 99.22%] [G loss: 0.277899]\n",
      "epoch:42 step:33366 [D loss: 0.005161, acc.: 100.00%] [G loss: 0.268822]\n",
      "epoch:42 step:33367 [D loss: 0.016830, acc.: 100.00%] [G loss: 0.409321]\n",
      "epoch:42 step:33368 [D loss: 0.054689, acc.: 99.22%] [G loss: 0.242325]\n",
      "epoch:42 step:33369 [D loss: 0.028820, acc.: 100.00%] [G loss: 0.347724]\n",
      "epoch:42 step:33370 [D loss: 0.384897, acc.: 78.12%] [G loss: 5.701851]\n",
      "epoch:42 step:33371 [D loss: 0.056450, acc.: 97.66%] [G loss: 1.315622]\n",
      "epoch:42 step:33372 [D loss: 0.930733, acc.: 71.09%] [G loss: 2.324635]\n",
      "epoch:42 step:33373 [D loss: 0.021939, acc.: 99.22%] [G loss: 1.055055]\n",
      "epoch:42 step:33374 [D loss: 0.001568, acc.: 100.00%] [G loss: 0.322165]\n",
      "epoch:42 step:33375 [D loss: 0.026182, acc.: 100.00%] [G loss: 0.074781]\n",
      "epoch:42 step:33376 [D loss: 0.037764, acc.: 100.00%] [G loss: 0.056861]\n",
      "epoch:42 step:33377 [D loss: 0.038414, acc.: 100.00%] [G loss: 0.236182]\n",
      "epoch:42 step:33378 [D loss: 0.003347, acc.: 100.00%] [G loss: 0.018306]\n",
      "epoch:42 step:33379 [D loss: 0.009636, acc.: 100.00%] [G loss: 0.103584]\n",
      "epoch:42 step:33380 [D loss: 0.025648, acc.: 98.44%] [G loss: 0.018096]\n",
      "epoch:42 step:33381 [D loss: 0.001738, acc.: 100.00%] [G loss: 0.028443]\n",
      "epoch:42 step:33382 [D loss: 0.005115, acc.: 100.00%] [G loss: 0.025321]\n",
      "epoch:42 step:33383 [D loss: 0.008965, acc.: 100.00%] [G loss: 0.117472]\n",
      "epoch:42 step:33384 [D loss: 0.044901, acc.: 98.44%] [G loss: 0.209770]\n",
      "epoch:42 step:33385 [D loss: 0.002637, acc.: 100.00%] [G loss: 0.066184]\n",
      "epoch:42 step:33386 [D loss: 0.007660, acc.: 100.00%] [G loss: 2.392585]\n",
      "epoch:42 step:33387 [D loss: 0.051503, acc.: 100.00%] [G loss: 0.153487]\n",
      "epoch:42 step:33388 [D loss: 0.022127, acc.: 100.00%] [G loss: 0.825740]\n",
      "epoch:42 step:33389 [D loss: 0.280735, acc.: 85.16%] [G loss: 5.772719]\n",
      "epoch:42 step:33390 [D loss: 0.207778, acc.: 91.41%] [G loss: 4.859784]\n",
      "epoch:42 step:33391 [D loss: 0.100400, acc.: 94.53%] [G loss: 2.917933]\n",
      "epoch:42 step:33392 [D loss: 0.039586, acc.: 100.00%] [G loss: 1.495524]\n",
      "epoch:42 step:33393 [D loss: 0.967681, acc.: 57.03%] [G loss: 6.385295]\n",
      "epoch:42 step:33394 [D loss: 0.771870, acc.: 68.75%] [G loss: 6.065833]\n",
      "epoch:42 step:33395 [D loss: 0.191890, acc.: 90.62%] [G loss: 5.151299]\n",
      "epoch:42 step:33396 [D loss: 0.076977, acc.: 96.88%] [G loss: 4.352011]\n",
      "epoch:42 step:33397 [D loss: 0.031341, acc.: 100.00%] [G loss: 4.036216]\n",
      "epoch:42 step:33398 [D loss: 0.090241, acc.: 97.66%] [G loss: 4.995486]\n",
      "epoch:42 step:33399 [D loss: 0.029084, acc.: 100.00%] [G loss: 4.633856]\n",
      "epoch:42 step:33400 [D loss: 0.035523, acc.: 100.00%] [G loss: 3.870501]\n",
      "epoch:42 step:33401 [D loss: 0.023739, acc.: 100.00%] [G loss: 0.323621]\n",
      "epoch:42 step:33402 [D loss: 0.087384, acc.: 97.66%] [G loss: 3.622169]\n",
      "epoch:42 step:33403 [D loss: 0.027778, acc.: 99.22%] [G loss: 2.797232]\n",
      "epoch:42 step:33404 [D loss: 0.103108, acc.: 97.66%] [G loss: 2.519429]\n",
      "epoch:42 step:33405 [D loss: 0.056284, acc.: 99.22%] [G loss: 2.693839]\n",
      "epoch:42 step:33406 [D loss: 0.705817, acc.: 65.62%] [G loss: 1.541679]\n",
      "epoch:42 step:33407 [D loss: 0.177964, acc.: 95.31%] [G loss: 5.598706]\n",
      "epoch:42 step:33408 [D loss: 0.322896, acc.: 86.72%] [G loss: 1.342723]\n",
      "epoch:42 step:33409 [D loss: 0.057487, acc.: 98.44%] [G loss: 2.316395]\n",
      "epoch:42 step:33410 [D loss: 0.083075, acc.: 96.88%] [G loss: 3.001734]\n",
      "epoch:42 step:33411 [D loss: 0.070898, acc.: 99.22%] [G loss: 3.384970]\n",
      "epoch:42 step:33412 [D loss: 0.069039, acc.: 98.44%] [G loss: 2.083767]\n",
      "epoch:42 step:33413 [D loss: 0.041096, acc.: 99.22%] [G loss: 2.529696]\n",
      "epoch:42 step:33414 [D loss: 0.189548, acc.: 92.19%] [G loss: 4.517200]\n",
      "epoch:42 step:33415 [D loss: 0.923062, acc.: 60.94%] [G loss: 4.806227]\n",
      "epoch:42 step:33416 [D loss: 0.021452, acc.: 100.00%] [G loss: 5.283488]\n",
      "epoch:42 step:33417 [D loss: 0.083166, acc.: 97.66%] [G loss: 4.583789]\n",
      "epoch:42 step:33418 [D loss: 0.144071, acc.: 95.31%] [G loss: 5.670688]\n",
      "epoch:42 step:33419 [D loss: 0.128749, acc.: 96.09%] [G loss: 4.487563]\n",
      "epoch:42 step:33420 [D loss: 0.142096, acc.: 96.09%] [G loss: 4.704182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33421 [D loss: 0.159615, acc.: 96.09%] [G loss: 0.584691]\n",
      "epoch:42 step:33422 [D loss: 0.091783, acc.: 94.53%] [G loss: 0.074523]\n",
      "epoch:42 step:33423 [D loss: 0.011335, acc.: 100.00%] [G loss: 4.308746]\n",
      "epoch:42 step:33424 [D loss: 0.010680, acc.: 100.00%] [G loss: 0.034702]\n",
      "epoch:42 step:33425 [D loss: 0.014897, acc.: 100.00%] [G loss: 4.405413]\n",
      "epoch:42 step:33426 [D loss: 0.003956, acc.: 100.00%] [G loss: 2.160872]\n",
      "epoch:42 step:33427 [D loss: 0.005851, acc.: 100.00%] [G loss: 0.013230]\n",
      "epoch:42 step:33428 [D loss: 0.012701, acc.: 100.00%] [G loss: 0.846951]\n",
      "epoch:42 step:33429 [D loss: 0.017556, acc.: 99.22%] [G loss: 0.230885]\n",
      "epoch:42 step:33430 [D loss: 0.009764, acc.: 100.00%] [G loss: 0.155366]\n",
      "epoch:42 step:33431 [D loss: 0.006033, acc.: 100.00%] [G loss: 0.007140]\n",
      "epoch:42 step:33432 [D loss: 0.012399, acc.: 100.00%] [G loss: 0.030484]\n",
      "epoch:42 step:33433 [D loss: 0.020511, acc.: 99.22%] [G loss: 0.004431]\n",
      "epoch:42 step:33434 [D loss: 0.005975, acc.: 100.00%] [G loss: 0.003164]\n",
      "epoch:42 step:33435 [D loss: 0.002969, acc.: 100.00%] [G loss: 0.056375]\n",
      "epoch:42 step:33436 [D loss: 0.010418, acc.: 100.00%] [G loss: 0.005049]\n",
      "epoch:42 step:33437 [D loss: 0.002741, acc.: 100.00%] [G loss: 0.017456]\n",
      "epoch:42 step:33438 [D loss: 0.000884, acc.: 100.00%] [G loss: 0.005179]\n",
      "epoch:42 step:33439 [D loss: 0.014674, acc.: 100.00%] [G loss: 0.004120]\n",
      "epoch:42 step:33440 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.003903]\n",
      "epoch:42 step:33441 [D loss: 0.011054, acc.: 99.22%] [G loss: 0.001793]\n",
      "epoch:42 step:33442 [D loss: 0.009429, acc.: 100.00%] [G loss: 0.004640]\n",
      "epoch:42 step:33443 [D loss: 0.001733, acc.: 100.00%] [G loss: 0.016384]\n",
      "epoch:42 step:33444 [D loss: 0.002269, acc.: 100.00%] [G loss: 0.004772]\n",
      "epoch:42 step:33445 [D loss: 0.002165, acc.: 100.00%] [G loss: 0.010861]\n",
      "epoch:42 step:33446 [D loss: 0.009526, acc.: 100.00%] [G loss: 0.581137]\n",
      "epoch:42 step:33447 [D loss: 0.012342, acc.: 100.00%] [G loss: 0.005312]\n",
      "epoch:42 step:33448 [D loss: 0.042445, acc.: 98.44%] [G loss: 0.000448]\n",
      "epoch:42 step:33449 [D loss: 0.016788, acc.: 100.00%] [G loss: 0.013347]\n",
      "epoch:42 step:33450 [D loss: 0.001854, acc.: 100.00%] [G loss: 0.038487]\n",
      "epoch:42 step:33451 [D loss: 0.030708, acc.: 99.22%] [G loss: 0.004614]\n",
      "epoch:42 step:33452 [D loss: 0.026702, acc.: 100.00%] [G loss: 0.006496]\n",
      "epoch:42 step:33453 [D loss: 0.105090, acc.: 97.66%] [G loss: 1.202956]\n",
      "epoch:42 step:33454 [D loss: 0.041826, acc.: 98.44%] [G loss: 1.986220]\n",
      "epoch:42 step:33455 [D loss: 0.005207, acc.: 100.00%] [G loss: 0.419829]\n",
      "epoch:42 step:33456 [D loss: 0.268578, acc.: 91.41%] [G loss: 0.061675]\n",
      "epoch:42 step:33457 [D loss: 0.069269, acc.: 97.66%] [G loss: 0.365012]\n",
      "epoch:42 step:33458 [D loss: 0.002892, acc.: 100.00%] [G loss: 2.564318]\n",
      "epoch:42 step:33459 [D loss: 0.021543, acc.: 99.22%] [G loss: 0.428363]\n",
      "epoch:42 step:33460 [D loss: 0.012027, acc.: 99.22%] [G loss: 0.272698]\n",
      "epoch:42 step:33461 [D loss: 0.016044, acc.: 99.22%] [G loss: 0.049069]\n",
      "epoch:42 step:33462 [D loss: 0.002244, acc.: 100.00%] [G loss: 0.107698]\n",
      "epoch:42 step:33463 [D loss: 0.005875, acc.: 100.00%] [G loss: 0.466915]\n",
      "epoch:42 step:33464 [D loss: 0.025723, acc.: 100.00%] [G loss: 0.007762]\n",
      "epoch:42 step:33465 [D loss: 0.007225, acc.: 100.00%] [G loss: 0.057429]\n",
      "epoch:42 step:33466 [D loss: 0.002247, acc.: 100.00%] [G loss: 0.015159]\n",
      "epoch:42 step:33467 [D loss: 0.441731, acc.: 81.25%] [G loss: 0.143110]\n",
      "epoch:42 step:33468 [D loss: 0.001980, acc.: 100.00%] [G loss: 1.622810]\n",
      "epoch:42 step:33469 [D loss: 0.051857, acc.: 98.44%] [G loss: 0.978298]\n",
      "epoch:42 step:33470 [D loss: 0.195024, acc.: 92.19%] [G loss: 1.001150]\n",
      "epoch:42 step:33471 [D loss: 0.016961, acc.: 99.22%] [G loss: 1.034227]\n",
      "epoch:42 step:33472 [D loss: 1.251810, acc.: 52.34%] [G loss: 9.538992]\n",
      "epoch:42 step:33473 [D loss: 1.839524, acc.: 53.91%] [G loss: 1.077163]\n",
      "epoch:42 step:33474 [D loss: 0.105060, acc.: 95.31%] [G loss: 0.145632]\n",
      "epoch:42 step:33475 [D loss: 0.025085, acc.: 99.22%] [G loss: 0.029716]\n",
      "epoch:42 step:33476 [D loss: 0.031009, acc.: 99.22%] [G loss: 2.861246]\n",
      "epoch:42 step:33477 [D loss: 0.014822, acc.: 100.00%] [G loss: 1.979543]\n",
      "epoch:42 step:33478 [D loss: 0.014505, acc.: 99.22%] [G loss: 1.290927]\n",
      "epoch:42 step:33479 [D loss: 0.020039, acc.: 99.22%] [G loss: 0.643700]\n",
      "epoch:42 step:33480 [D loss: 0.006712, acc.: 100.00%] [G loss: 0.078967]\n",
      "epoch:42 step:33481 [D loss: 0.005895, acc.: 100.00%] [G loss: 0.057328]\n",
      "epoch:42 step:33482 [D loss: 0.004193, acc.: 100.00%] [G loss: 0.007554]\n",
      "epoch:42 step:33483 [D loss: 0.007487, acc.: 100.00%] [G loss: 0.039527]\n",
      "epoch:42 step:33484 [D loss: 0.006585, acc.: 100.00%] [G loss: 0.035526]\n",
      "epoch:42 step:33485 [D loss: 0.012912, acc.: 100.00%] [G loss: 0.037563]\n",
      "epoch:42 step:33486 [D loss: 0.019472, acc.: 99.22%] [G loss: 0.000861]\n",
      "epoch:42 step:33487 [D loss: 0.005687, acc.: 100.00%] [G loss: 0.094607]\n",
      "epoch:42 step:33488 [D loss: 0.005918, acc.: 100.00%] [G loss: 0.020295]\n",
      "epoch:42 step:33489 [D loss: 0.017819, acc.: 99.22%] [G loss: 0.004543]\n",
      "epoch:42 step:33490 [D loss: 0.004638, acc.: 100.00%] [G loss: 0.008685]\n",
      "epoch:42 step:33491 [D loss: 0.001192, acc.: 100.00%] [G loss: 0.008116]\n",
      "epoch:42 step:33492 [D loss: 0.001253, acc.: 100.00%] [G loss: 0.001491]\n",
      "epoch:42 step:33493 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.002802]\n",
      "epoch:42 step:33494 [D loss: 0.008463, acc.: 100.00%] [G loss: 0.003191]\n",
      "epoch:42 step:33495 [D loss: 0.000533, acc.: 100.00%] [G loss: 0.010629]\n",
      "epoch:42 step:33496 [D loss: 0.080470, acc.: 97.66%] [G loss: 0.025525]\n",
      "epoch:42 step:33497 [D loss: 0.000849, acc.: 100.00%] [G loss: 0.028713]\n",
      "epoch:42 step:33498 [D loss: 0.010021, acc.: 100.00%] [G loss: 0.842466]\n",
      "epoch:42 step:33499 [D loss: 0.001954, acc.: 100.00%] [G loss: 0.708698]\n",
      "epoch:42 step:33500 [D loss: 0.002121, acc.: 100.00%] [G loss: 0.211254]\n",
      "epoch:42 step:33501 [D loss: 0.009452, acc.: 100.00%] [G loss: 0.077287]\n",
      "epoch:42 step:33502 [D loss: 0.003920, acc.: 100.00%] [G loss: 0.087356]\n",
      "epoch:42 step:33503 [D loss: 0.005719, acc.: 100.00%] [G loss: 0.014916]\n",
      "epoch:42 step:33504 [D loss: 0.016318, acc.: 100.00%] [G loss: 0.005590]\n",
      "epoch:42 step:33505 [D loss: 0.063311, acc.: 99.22%] [G loss: 0.012217]\n",
      "epoch:42 step:33506 [D loss: 0.042699, acc.: 98.44%] [G loss: 0.008602]\n",
      "epoch:42 step:33507 [D loss: 0.001267, acc.: 100.00%] [G loss: 0.006817]\n",
      "epoch:42 step:33508 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.001459]\n",
      "epoch:42 step:33509 [D loss: 0.005747, acc.: 100.00%] [G loss: 0.001624]\n",
      "epoch:42 step:33510 [D loss: 0.003072, acc.: 100.00%] [G loss: 0.007221]\n",
      "epoch:42 step:33511 [D loss: 0.001718, acc.: 100.00%] [G loss: 0.013336]\n",
      "epoch:42 step:33512 [D loss: 0.002264, acc.: 100.00%] [G loss: 0.004650]\n",
      "epoch:42 step:33513 [D loss: 0.024333, acc.: 99.22%] [G loss: 0.010631]\n",
      "epoch:42 step:33514 [D loss: 0.001196, acc.: 100.00%] [G loss: 0.019013]\n",
      "epoch:42 step:33515 [D loss: 0.212954, acc.: 90.62%] [G loss: 0.000024]\n",
      "epoch:42 step:33516 [D loss: 0.004077, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:42 step:33517 [D loss: 0.012331, acc.: 100.00%] [G loss: 0.001385]\n",
      "epoch:42 step:33518 [D loss: 0.008770, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:42 step:33519 [D loss: 0.011733, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:42 step:33520 [D loss: 0.015440, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:42 step:33521 [D loss: 0.000990, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:42 step:33522 [D loss: 0.006118, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:42 step:33523 [D loss: 0.082957, acc.: 96.09%] [G loss: 0.010509]\n",
      "epoch:42 step:33524 [D loss: 0.001398, acc.: 100.00%] [G loss: 0.159637]\n",
      "epoch:42 step:33525 [D loss: 0.003331, acc.: 100.00%] [G loss: 0.020366]\n",
      "epoch:42 step:33526 [D loss: 0.001679, acc.: 100.00%] [G loss: 0.013640]\n",
      "epoch:42 step:33527 [D loss: 0.006428, acc.: 100.00%] [G loss: 0.146820]\n",
      "epoch:42 step:33528 [D loss: 0.004957, acc.: 100.00%] [G loss: 0.045767]\n",
      "epoch:42 step:33529 [D loss: 0.003547, acc.: 100.00%] [G loss: 0.024715]\n",
      "epoch:42 step:33530 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.016309]\n",
      "epoch:42 step:33531 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.046591]\n",
      "epoch:42 step:33532 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.009023]\n",
      "epoch:42 step:33533 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.011360]\n",
      "epoch:42 step:33534 [D loss: 0.000529, acc.: 100.00%] [G loss: 0.005907]\n",
      "epoch:42 step:33535 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.037473]\n",
      "epoch:42 step:33536 [D loss: 0.001235, acc.: 100.00%] [G loss: 0.022170]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:42 step:33537 [D loss: 0.003429, acc.: 100.00%] [G loss: 0.020437]\n",
      "epoch:42 step:33538 [D loss: 0.011586, acc.: 100.00%] [G loss: 0.010639]\n",
      "epoch:42 step:33539 [D loss: 0.006219, acc.: 100.00%] [G loss: 0.016392]\n",
      "epoch:42 step:33540 [D loss: 0.000495, acc.: 100.00%] [G loss: 0.074576]\n",
      "epoch:42 step:33541 [D loss: 0.001583, acc.: 100.00%] [G loss: 0.019832]\n",
      "epoch:42 step:33542 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.001858]\n",
      "epoch:42 step:33543 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.013543]\n",
      "epoch:42 step:33544 [D loss: 0.005978, acc.: 100.00%] [G loss: 0.001582]\n",
      "epoch:42 step:33545 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.001989]\n",
      "epoch:42 step:33546 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.006727]\n",
      "epoch:42 step:33547 [D loss: 0.002864, acc.: 100.00%] [G loss: 0.005759]\n",
      "epoch:42 step:33548 [D loss: 0.001180, acc.: 100.00%] [G loss: 0.006909]\n",
      "epoch:42 step:33549 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.002236]\n",
      "epoch:42 step:33550 [D loss: 0.002741, acc.: 100.00%] [G loss: 0.013567]\n",
      "epoch:42 step:33551 [D loss: 0.003505, acc.: 100.00%] [G loss: 0.001113]\n",
      "epoch:42 step:33552 [D loss: 0.000420, acc.: 100.00%] [G loss: 0.004437]\n",
      "epoch:42 step:33553 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.005060]\n",
      "epoch:42 step:33554 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.006736]\n",
      "epoch:42 step:33555 [D loss: 0.003902, acc.: 100.00%] [G loss: 0.002750]\n",
      "epoch:42 step:33556 [D loss: 0.003195, acc.: 100.00%] [G loss: 0.002738]\n",
      "epoch:42 step:33557 [D loss: 0.003263, acc.: 100.00%] [G loss: 0.002760]\n",
      "epoch:42 step:33558 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.998153]\n",
      "epoch:42 step:33559 [D loss: 0.000278, acc.: 100.00%] [G loss: 0.001552]\n",
      "epoch:42 step:33560 [D loss: 0.000370, acc.: 100.00%] [G loss: 0.004791]\n",
      "epoch:42 step:33561 [D loss: 0.008564, acc.: 100.00%] [G loss: 0.008000]\n",
      "epoch:42 step:33562 [D loss: 0.003111, acc.: 100.00%] [G loss: 0.108201]\n",
      "epoch:42 step:33563 [D loss: 0.080226, acc.: 98.44%] [G loss: 0.378798]\n",
      "epoch:42 step:33564 [D loss: 0.041293, acc.: 99.22%] [G loss: 0.310308]\n",
      "epoch:42 step:33565 [D loss: 0.012555, acc.: 100.00%] [G loss: 0.149205]\n",
      "epoch:42 step:33566 [D loss: 0.002703, acc.: 100.00%] [G loss: 0.053151]\n",
      "epoch:42 step:33567 [D loss: 0.001009, acc.: 100.00%] [G loss: 0.025233]\n",
      "epoch:42 step:33568 [D loss: 0.005109, acc.: 100.00%] [G loss: 0.001016]\n",
      "epoch:42 step:33569 [D loss: 0.001979, acc.: 100.00%] [G loss: 0.004136]\n",
      "epoch:42 step:33570 [D loss: 0.001687, acc.: 100.00%] [G loss: 0.001478]\n",
      "epoch:42 step:33571 [D loss: 0.001542, acc.: 100.00%] [G loss: 0.000790]\n",
      "epoch:42 step:33572 [D loss: 0.003240, acc.: 100.00%] [G loss: 0.018119]\n",
      "epoch:42 step:33573 [D loss: 0.023262, acc.: 100.00%] [G loss: 0.000660]\n",
      "epoch:42 step:33574 [D loss: 0.076807, acc.: 97.66%] [G loss: 0.027600]\n",
      "epoch:42 step:33575 [D loss: 0.001017, acc.: 100.00%] [G loss: 1.193838]\n",
      "epoch:42 step:33576 [D loss: 0.014804, acc.: 99.22%] [G loss: 0.899613]\n",
      "epoch:42 step:33577 [D loss: 0.005738, acc.: 100.00%] [G loss: 0.135669]\n",
      "epoch:42 step:33578 [D loss: 0.002016, acc.: 100.00%] [G loss: 0.356712]\n",
      "epoch:42 step:33579 [D loss: 0.009325, acc.: 100.00%] [G loss: 0.061001]\n",
      "epoch:42 step:33580 [D loss: 0.004181, acc.: 100.00%] [G loss: 0.000714]\n",
      "epoch:42 step:33581 [D loss: 0.002518, acc.: 100.00%] [G loss: 0.077239]\n",
      "epoch:42 step:33582 [D loss: 0.003779, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:42 step:33583 [D loss: 0.002610, acc.: 100.00%] [G loss: 0.001653]\n",
      "epoch:43 step:33584 [D loss: 0.014540, acc.: 99.22%] [G loss: 0.117616]\n",
      "epoch:43 step:33585 [D loss: 0.003332, acc.: 100.00%] [G loss: 0.001242]\n",
      "epoch:43 step:33586 [D loss: 0.001164, acc.: 100.00%] [G loss: 0.000346]\n",
      "epoch:43 step:33587 [D loss: 0.009245, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:43 step:33588 [D loss: 0.025231, acc.: 100.00%] [G loss: 0.161754]\n",
      "epoch:43 step:33589 [D loss: 0.004490, acc.: 100.00%] [G loss: 0.113163]\n",
      "epoch:43 step:33590 [D loss: 0.007711, acc.: 100.00%] [G loss: 0.001976]\n",
      "epoch:43 step:33591 [D loss: 0.002116, acc.: 100.00%] [G loss: 0.048415]\n",
      "epoch:43 step:33592 [D loss: 0.006303, acc.: 100.00%] [G loss: 0.001049]\n",
      "epoch:43 step:33593 [D loss: 0.005194, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:43 step:33594 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:43 step:33595 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:43 step:33596 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000326]\n",
      "epoch:43 step:33597 [D loss: 0.001634, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:43 step:33598 [D loss: 0.000869, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:43 step:33599 [D loss: 0.001411, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:43 step:33600 [D loss: 0.000938, acc.: 100.00%] [G loss: 0.000504]\n",
      "epoch:43 step:33601 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:43 step:33602 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:43 step:33603 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:43 step:33604 [D loss: 0.009220, acc.: 99.22%] [G loss: 0.001632]\n",
      "epoch:43 step:33605 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.008043]\n",
      "epoch:43 step:33606 [D loss: 0.001603, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:43 step:33607 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:43 step:33608 [D loss: 0.000998, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:43 step:33609 [D loss: 0.000660, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:43 step:33610 [D loss: 0.003520, acc.: 100.00%] [G loss: 0.908685]\n",
      "epoch:43 step:33611 [D loss: 0.179932, acc.: 92.19%] [G loss: 0.420152]\n",
      "epoch:43 step:33612 [D loss: 0.234050, acc.: 89.84%] [G loss: 0.031193]\n",
      "epoch:43 step:33613 [D loss: 0.019758, acc.: 100.00%] [G loss: 0.067129]\n",
      "epoch:43 step:33614 [D loss: 0.033104, acc.: 100.00%] [G loss: 0.404336]\n",
      "epoch:43 step:33615 [D loss: 0.095250, acc.: 96.09%] [G loss: 4.026731]\n",
      "epoch:43 step:33616 [D loss: 0.197774, acc.: 90.62%] [G loss: 0.031234]\n",
      "epoch:43 step:33617 [D loss: 0.006569, acc.: 99.22%] [G loss: 1.507320]\n",
      "epoch:43 step:33618 [D loss: 0.014379, acc.: 99.22%] [G loss: 3.586472]\n",
      "epoch:43 step:33619 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.281131]\n",
      "epoch:43 step:33620 [D loss: 0.000807, acc.: 100.00%] [G loss: 0.512522]\n",
      "epoch:43 step:33621 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.135532]\n",
      "epoch:43 step:33622 [D loss: 0.001576, acc.: 100.00%] [G loss: 0.113590]\n",
      "epoch:43 step:33623 [D loss: 0.003091, acc.: 100.00%] [G loss: 0.004989]\n",
      "epoch:43 step:33624 [D loss: 0.013348, acc.: 100.00%] [G loss: 0.156860]\n",
      "epoch:43 step:33625 [D loss: 0.091688, acc.: 98.44%] [G loss: 0.896208]\n",
      "epoch:43 step:33626 [D loss: 0.012517, acc.: 100.00%] [G loss: 1.553114]\n",
      "epoch:43 step:33627 [D loss: 0.420670, acc.: 82.03%] [G loss: 0.000307]\n",
      "epoch:43 step:33628 [D loss: 1.377372, acc.: 59.38%] [G loss: 7.989340]\n",
      "epoch:43 step:33629 [D loss: 1.223448, acc.: 58.59%] [G loss: 4.378247]\n",
      "epoch:43 step:33630 [D loss: 0.204462, acc.: 92.19%] [G loss: 4.652976]\n",
      "epoch:43 step:33631 [D loss: 0.162517, acc.: 93.75%] [G loss: 5.032461]\n",
      "epoch:43 step:33632 [D loss: 0.085634, acc.: 97.66%] [G loss: 1.168563]\n",
      "epoch:43 step:33633 [D loss: 0.137634, acc.: 92.97%] [G loss: 0.039757]\n",
      "epoch:43 step:33634 [D loss: 0.004900, acc.: 100.00%] [G loss: 1.381714]\n",
      "epoch:43 step:33635 [D loss: 0.012420, acc.: 100.00%] [G loss: 0.193702]\n",
      "epoch:43 step:33636 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.065249]\n",
      "epoch:43 step:33637 [D loss: 0.003734, acc.: 100.00%] [G loss: 0.149865]\n",
      "epoch:43 step:33638 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.062441]\n",
      "epoch:43 step:33639 [D loss: 0.001174, acc.: 100.00%] [G loss: 0.013345]\n",
      "epoch:43 step:33640 [D loss: 0.010112, acc.: 100.00%] [G loss: 0.027866]\n",
      "epoch:43 step:33641 [D loss: 0.007821, acc.: 100.00%] [G loss: 0.014455]\n",
      "epoch:43 step:33642 [D loss: 0.027610, acc.: 100.00%] [G loss: 0.038659]\n",
      "epoch:43 step:33643 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.053280]\n",
      "epoch:43 step:33644 [D loss: 0.255238, acc.: 89.06%] [G loss: 3.320375]\n",
      "epoch:43 step:33645 [D loss: 0.060858, acc.: 98.44%] [G loss: 4.111952]\n",
      "epoch:43 step:33646 [D loss: 0.304504, acc.: 84.38%] [G loss: 0.024637]\n",
      "epoch:43 step:33647 [D loss: 0.037976, acc.: 99.22%] [G loss: 0.013809]\n",
      "epoch:43 step:33648 [D loss: 0.010779, acc.: 100.00%] [G loss: 0.006492]\n",
      "epoch:43 step:33649 [D loss: 0.001474, acc.: 100.00%] [G loss: 0.002121]\n",
      "epoch:43 step:33650 [D loss: 0.000970, acc.: 100.00%] [G loss: 0.000597]\n",
      "epoch:43 step:33651 [D loss: 0.001162, acc.: 100.00%] [G loss: 0.000575]\n",
      "epoch:43 step:33652 [D loss: 0.007213, acc.: 100.00%] [G loss: 0.003615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33653 [D loss: 0.001087, acc.: 100.00%] [G loss: 0.008350]\n",
      "epoch:43 step:33654 [D loss: 0.020979, acc.: 100.00%] [G loss: 0.004887]\n",
      "epoch:43 step:33655 [D loss: 0.004075, acc.: 100.00%] [G loss: 0.015756]\n",
      "epoch:43 step:33656 [D loss: 0.004488, acc.: 100.00%] [G loss: 0.024819]\n",
      "epoch:43 step:33657 [D loss: 0.003395, acc.: 100.00%] [G loss: 0.015928]\n",
      "epoch:43 step:33658 [D loss: 0.001153, acc.: 100.00%] [G loss: 0.025853]\n",
      "epoch:43 step:33659 [D loss: 0.007302, acc.: 100.00%] [G loss: 0.081538]\n",
      "epoch:43 step:33660 [D loss: 0.038217, acc.: 100.00%] [G loss: 0.004728]\n",
      "epoch:43 step:33661 [D loss: 0.057873, acc.: 98.44%] [G loss: 0.040157]\n",
      "epoch:43 step:33662 [D loss: 0.010590, acc.: 100.00%] [G loss: 0.040070]\n",
      "epoch:43 step:33663 [D loss: 0.005829, acc.: 100.00%] [G loss: 0.052628]\n",
      "epoch:43 step:33664 [D loss: 0.012036, acc.: 100.00%] [G loss: 0.165620]\n",
      "epoch:43 step:33665 [D loss: 0.011478, acc.: 100.00%] [G loss: 0.004346]\n",
      "epoch:43 step:33666 [D loss: 0.093857, acc.: 98.44%] [G loss: 0.304224]\n",
      "epoch:43 step:33667 [D loss: 0.042917, acc.: 98.44%] [G loss: 1.375983]\n",
      "epoch:43 step:33668 [D loss: 0.066920, acc.: 96.88%] [G loss: 0.490037]\n",
      "epoch:43 step:33669 [D loss: 0.079874, acc.: 100.00%] [G loss: 0.300782]\n",
      "epoch:43 step:33670 [D loss: 0.110557, acc.: 96.88%] [G loss: 3.497185]\n",
      "epoch:43 step:33671 [D loss: 0.327495, acc.: 83.59%] [G loss: 0.231256]\n",
      "epoch:43 step:33672 [D loss: 0.577021, acc.: 71.88%] [G loss: 3.301443]\n",
      "epoch:43 step:33673 [D loss: 2.430313, acc.: 50.00%] [G loss: 3.780289]\n",
      "epoch:43 step:33674 [D loss: 0.029556, acc.: 99.22%] [G loss: 2.362482]\n",
      "epoch:43 step:33675 [D loss: 0.032552, acc.: 100.00%] [G loss: 0.036072]\n",
      "epoch:43 step:33676 [D loss: 0.024082, acc.: 100.00%] [G loss: 2.149436]\n",
      "epoch:43 step:33677 [D loss: 0.041682, acc.: 100.00%] [G loss: 0.011859]\n",
      "epoch:43 step:33678 [D loss: 0.091899, acc.: 97.66%] [G loss: 1.788210]\n",
      "epoch:43 step:33679 [D loss: 0.008959, acc.: 100.00%] [G loss: 1.809396]\n",
      "epoch:43 step:33680 [D loss: 0.031219, acc.: 99.22%] [G loss: 0.252762]\n",
      "epoch:43 step:33681 [D loss: 0.038274, acc.: 100.00%] [G loss: 0.636597]\n",
      "epoch:43 step:33682 [D loss: 0.038932, acc.: 98.44%] [G loss: 0.004017]\n",
      "epoch:43 step:33683 [D loss: 0.197043, acc.: 92.19%] [G loss: 1.052607]\n",
      "epoch:43 step:33684 [D loss: 0.048081, acc.: 99.22%] [G loss: 1.506421]\n",
      "epoch:43 step:33685 [D loss: 0.022963, acc.: 99.22%] [G loss: 0.944420]\n",
      "epoch:43 step:33686 [D loss: 0.121196, acc.: 94.53%] [G loss: 0.054145]\n",
      "epoch:43 step:33687 [D loss: 0.008897, acc.: 100.00%] [G loss: 0.040530]\n",
      "epoch:43 step:33688 [D loss: 0.033324, acc.: 99.22%] [G loss: 0.041788]\n",
      "epoch:43 step:33689 [D loss: 0.000966, acc.: 100.00%] [G loss: 0.055556]\n",
      "epoch:43 step:33690 [D loss: 0.024291, acc.: 100.00%] [G loss: 0.056175]\n",
      "epoch:43 step:33691 [D loss: 0.034798, acc.: 99.22%] [G loss: 0.178219]\n",
      "epoch:43 step:33692 [D loss: 0.034726, acc.: 100.00%] [G loss: 0.012062]\n",
      "epoch:43 step:33693 [D loss: 0.010255, acc.: 100.00%] [G loss: 0.041491]\n",
      "epoch:43 step:33694 [D loss: 0.027792, acc.: 99.22%] [G loss: 0.082914]\n",
      "epoch:43 step:33695 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.025616]\n",
      "epoch:43 step:33696 [D loss: 0.000867, acc.: 100.00%] [G loss: 0.004276]\n",
      "epoch:43 step:33697 [D loss: 0.008973, acc.: 100.00%] [G loss: 0.003153]\n",
      "epoch:43 step:33698 [D loss: 0.028294, acc.: 100.00%] [G loss: 0.013817]\n",
      "epoch:43 step:33699 [D loss: 0.022127, acc.: 99.22%] [G loss: 0.043581]\n",
      "epoch:43 step:33700 [D loss: 0.001672, acc.: 100.00%] [G loss: 0.014498]\n",
      "epoch:43 step:33701 [D loss: 0.000650, acc.: 100.00%] [G loss: 0.007581]\n",
      "epoch:43 step:33702 [D loss: 0.007115, acc.: 99.22%] [G loss: 0.001820]\n",
      "epoch:43 step:33703 [D loss: 0.003380, acc.: 100.00%] [G loss: 0.093934]\n",
      "epoch:43 step:33704 [D loss: 0.002882, acc.: 100.00%] [G loss: 0.070216]\n",
      "epoch:43 step:33705 [D loss: 0.000722, acc.: 100.00%] [G loss: 0.007118]\n",
      "epoch:43 step:33706 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.002109]\n",
      "epoch:43 step:33707 [D loss: 0.008096, acc.: 100.00%] [G loss: 0.001233]\n",
      "epoch:43 step:33708 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.000999]\n",
      "epoch:43 step:33709 [D loss: 0.001155, acc.: 100.00%] [G loss: 0.001684]\n",
      "epoch:43 step:33710 [D loss: 0.003893, acc.: 100.00%] [G loss: 0.005102]\n",
      "epoch:43 step:33711 [D loss: 0.001815, acc.: 100.00%] [G loss: 0.008075]\n",
      "epoch:43 step:33712 [D loss: 0.009518, acc.: 100.00%] [G loss: 0.003136]\n",
      "epoch:43 step:33713 [D loss: 0.003479, acc.: 100.00%] [G loss: 0.003062]\n",
      "epoch:43 step:33714 [D loss: 0.000957, acc.: 100.00%] [G loss: 0.004073]\n",
      "epoch:43 step:33715 [D loss: 0.007441, acc.: 100.00%] [G loss: 0.003672]\n",
      "epoch:43 step:33716 [D loss: 0.000807, acc.: 100.00%] [G loss: 0.001327]\n",
      "epoch:43 step:33717 [D loss: 0.001424, acc.: 100.00%] [G loss: 0.003076]\n",
      "epoch:43 step:33718 [D loss: 0.000687, acc.: 100.00%] [G loss: 0.010006]\n",
      "epoch:43 step:33719 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.004501]\n",
      "epoch:43 step:33720 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.004526]\n",
      "epoch:43 step:33721 [D loss: 0.000669, acc.: 100.00%] [G loss: 0.068585]\n",
      "epoch:43 step:33722 [D loss: 0.002223, acc.: 100.00%] [G loss: 0.002801]\n",
      "epoch:43 step:33723 [D loss: 0.004490, acc.: 100.00%] [G loss: 0.001998]\n",
      "epoch:43 step:33724 [D loss: 0.000977, acc.: 100.00%] [G loss: 0.003115]\n",
      "epoch:43 step:33725 [D loss: 0.001409, acc.: 100.00%] [G loss: 0.002310]\n",
      "epoch:43 step:33726 [D loss: 0.003522, acc.: 100.00%] [G loss: 0.000690]\n",
      "epoch:43 step:33727 [D loss: 0.000567, acc.: 100.00%] [G loss: 0.000547]\n",
      "epoch:43 step:33728 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.000441]\n",
      "epoch:43 step:33729 [D loss: 0.002030, acc.: 100.00%] [G loss: 0.003310]\n",
      "epoch:43 step:33730 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.004161]\n",
      "epoch:43 step:33731 [D loss: 0.001361, acc.: 100.00%] [G loss: 0.002063]\n",
      "epoch:43 step:33732 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.002160]\n",
      "epoch:43 step:33733 [D loss: 0.000368, acc.: 100.00%] [G loss: 0.001419]\n",
      "epoch:43 step:33734 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.434983]\n",
      "epoch:43 step:33735 [D loss: 0.000264, acc.: 100.00%] [G loss: 0.002983]\n",
      "epoch:43 step:33736 [D loss: 0.000786, acc.: 100.00%] [G loss: 0.117308]\n",
      "epoch:43 step:33737 [D loss: 0.004663, acc.: 100.00%] [G loss: 0.000966]\n",
      "epoch:43 step:33738 [D loss: 0.024604, acc.: 100.00%] [G loss: 0.001770]\n",
      "epoch:43 step:33739 [D loss: 0.047039, acc.: 98.44%] [G loss: 0.030407]\n",
      "epoch:43 step:33740 [D loss: 0.000761, acc.: 100.00%] [G loss: 0.018972]\n",
      "epoch:43 step:33741 [D loss: 0.038438, acc.: 99.22%] [G loss: 0.019720]\n",
      "epoch:43 step:33742 [D loss: 0.000686, acc.: 100.00%] [G loss: 0.006860]\n",
      "epoch:43 step:33743 [D loss: 0.005763, acc.: 100.00%] [G loss: 0.001028]\n",
      "epoch:43 step:33744 [D loss: 0.001626, acc.: 100.00%] [G loss: 0.004125]\n",
      "epoch:43 step:33745 [D loss: 0.000522, acc.: 100.00%] [G loss: 0.004309]\n",
      "epoch:43 step:33746 [D loss: 0.011971, acc.: 100.00%] [G loss: 0.002136]\n",
      "epoch:43 step:33747 [D loss: 0.001588, acc.: 100.00%] [G loss: 0.004818]\n",
      "epoch:43 step:33748 [D loss: 0.001016, acc.: 100.00%] [G loss: 0.003423]\n",
      "epoch:43 step:33749 [D loss: 0.002933, acc.: 100.00%] [G loss: 0.004854]\n",
      "epoch:43 step:33750 [D loss: 0.001959, acc.: 100.00%] [G loss: 0.007688]\n",
      "epoch:43 step:33751 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.004442]\n",
      "epoch:43 step:33752 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.003255]\n",
      "epoch:43 step:33753 [D loss: 0.003605, acc.: 100.00%] [G loss: 0.004080]\n",
      "epoch:43 step:33754 [D loss: 0.002711, acc.: 100.00%] [G loss: 0.005182]\n",
      "epoch:43 step:33755 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.004235]\n",
      "epoch:43 step:33756 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.003555]\n",
      "epoch:43 step:33757 [D loss: 0.001016, acc.: 100.00%] [G loss: 0.004702]\n",
      "epoch:43 step:33758 [D loss: 0.022987, acc.: 100.00%] [G loss: 0.009389]\n",
      "epoch:43 step:33759 [D loss: 0.006032, acc.: 100.00%] [G loss: 0.005162]\n",
      "epoch:43 step:33760 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.013216]\n",
      "epoch:43 step:33761 [D loss: 0.005991, acc.: 100.00%] [G loss: 0.002755]\n",
      "epoch:43 step:33762 [D loss: 0.032951, acc.: 98.44%] [G loss: 0.002315]\n",
      "epoch:43 step:33763 [D loss: 0.000812, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:43 step:33764 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:43 step:33765 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.002671]\n",
      "epoch:43 step:33766 [D loss: 0.012882, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:43 step:33767 [D loss: 0.002552, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:43 step:33768 [D loss: 0.000309, acc.: 100.00%] [G loss: 0.001953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33769 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.015677]\n",
      "epoch:43 step:33770 [D loss: 0.010645, acc.: 100.00%] [G loss: 0.000314]\n",
      "epoch:43 step:33771 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000258]\n",
      "epoch:43 step:33772 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.002939]\n",
      "epoch:43 step:33773 [D loss: 0.005755, acc.: 100.00%] [G loss: 0.000204]\n",
      "epoch:43 step:33774 [D loss: 0.001168, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:43 step:33775 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.037000]\n",
      "epoch:43 step:33776 [D loss: 0.000343, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:43 step:33777 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:43 step:33778 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.010714]\n",
      "epoch:43 step:33779 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:43 step:33780 [D loss: 0.004184, acc.: 100.00%] [G loss: 0.000194]\n",
      "epoch:43 step:33781 [D loss: 0.000907, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:43 step:33782 [D loss: 0.001730, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:43 step:33783 [D loss: 0.001342, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:43 step:33784 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:43 step:33785 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:43 step:33786 [D loss: 0.001301, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:43 step:33787 [D loss: 0.039980, acc.: 99.22%] [G loss: 0.011560]\n",
      "epoch:43 step:33788 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.005297]\n",
      "epoch:43 step:33789 [D loss: 0.002982, acc.: 100.00%] [G loss: 0.018125]\n",
      "epoch:43 step:33790 [D loss: 0.000540, acc.: 100.00%] [G loss: 0.005154]\n",
      "epoch:43 step:33791 [D loss: 0.003157, acc.: 100.00%] [G loss: 0.001895]\n",
      "epoch:43 step:33792 [D loss: 0.063652, acc.: 99.22%] [G loss: 0.000308]\n",
      "epoch:43 step:33793 [D loss: 0.002204, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:43 step:33794 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:43 step:33795 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:43 step:33796 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000415]\n",
      "epoch:43 step:33797 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:43 step:33798 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:43 step:33799 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:43 step:33800 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:43 step:33801 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:33802 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:43 step:33803 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:43 step:33804 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:43 step:33805 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:33806 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:43 step:33807 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.001058]\n",
      "epoch:43 step:33808 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:43 step:33809 [D loss: 0.000522, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:43 step:33810 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.006381]\n",
      "epoch:43 step:33811 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:43 step:33812 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:43 step:33813 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:43 step:33814 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:43 step:33815 [D loss: 0.004988, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:43 step:33816 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:43 step:33817 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:43 step:33818 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:43 step:33819 [D loss: 0.012775, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:43 step:33820 [D loss: 0.017027, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:43 step:33821 [D loss: 0.001708, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:43 step:33822 [D loss: 0.000677, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:43 step:33823 [D loss: 0.012804, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:43 step:33824 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:43 step:33825 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:43 step:33826 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:43 step:33827 [D loss: 0.003264, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:43 step:33828 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000605]\n",
      "epoch:43 step:33829 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:43 step:33830 [D loss: 0.001264, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:43 step:33831 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.001738]\n",
      "epoch:43 step:33832 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.001696]\n",
      "epoch:43 step:33833 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.000996]\n",
      "epoch:43 step:33834 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001900]\n",
      "epoch:43 step:33835 [D loss: 0.000398, acc.: 100.00%] [G loss: 0.004387]\n",
      "epoch:43 step:33836 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:43 step:33837 [D loss: 0.000641, acc.: 100.00%] [G loss: 0.000434]\n",
      "epoch:43 step:33838 [D loss: 0.001001, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:43 step:33839 [D loss: 0.001498, acc.: 100.00%] [G loss: 0.001168]\n",
      "epoch:43 step:33840 [D loss: 0.021897, acc.: 100.00%] [G loss: 0.003414]\n",
      "epoch:43 step:33841 [D loss: 0.002695, acc.: 100.00%] [G loss: 0.000767]\n",
      "epoch:43 step:33842 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.037633]\n",
      "epoch:43 step:33843 [D loss: 0.001102, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:43 step:33844 [D loss: 0.003136, acc.: 100.00%] [G loss: 0.001237]\n",
      "epoch:43 step:33845 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:43 step:33846 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:43 step:33847 [D loss: 0.000476, acc.: 100.00%] [G loss: 0.000583]\n",
      "epoch:43 step:33848 [D loss: 0.002135, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:43 step:33849 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.001155]\n",
      "epoch:43 step:33850 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.009314]\n",
      "epoch:43 step:33851 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:43 step:33852 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:43 step:33853 [D loss: 0.000507, acc.: 100.00%] [G loss: 0.002203]\n",
      "epoch:43 step:33854 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:43 step:33855 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.002070]\n",
      "epoch:43 step:33856 [D loss: 0.001482, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:43 step:33857 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000996]\n",
      "epoch:43 step:33858 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.017741]\n",
      "epoch:43 step:33859 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:43 step:33860 [D loss: 0.003831, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:43 step:33861 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:43 step:33862 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:43 step:33863 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000680]\n",
      "epoch:43 step:33864 [D loss: 0.002011, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:43 step:33865 [D loss: 0.000898, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:43 step:33866 [D loss: 0.002732, acc.: 100.00%] [G loss: 0.000263]\n",
      "epoch:43 step:33867 [D loss: 0.049779, acc.: 98.44%] [G loss: 0.030333]\n",
      "epoch:43 step:33868 [D loss: 0.000924, acc.: 100.00%] [G loss: 0.338984]\n",
      "epoch:43 step:33869 [D loss: 0.099379, acc.: 97.66%] [G loss: 0.022649]\n",
      "epoch:43 step:33870 [D loss: 0.005911, acc.: 100.00%] [G loss: 0.046116]\n",
      "epoch:43 step:33871 [D loss: 0.012120, acc.: 100.00%] [G loss: 0.034642]\n",
      "epoch:43 step:33872 [D loss: 0.010416, acc.: 100.00%] [G loss: 0.052709]\n",
      "epoch:43 step:33873 [D loss: 0.126048, acc.: 96.88%] [G loss: 4.070956]\n",
      "epoch:43 step:33874 [D loss: 0.086962, acc.: 97.66%] [G loss: 4.248171]\n",
      "epoch:43 step:33875 [D loss: 2.000592, acc.: 32.81%] [G loss: 7.149579]\n",
      "epoch:43 step:33876 [D loss: 0.608735, acc.: 72.66%] [G loss: 5.083456]\n",
      "epoch:43 step:33877 [D loss: 0.117461, acc.: 96.09%] [G loss: 3.505960]\n",
      "epoch:43 step:33878 [D loss: 0.088090, acc.: 98.44%] [G loss: 2.862077]\n",
      "epoch:43 step:33879 [D loss: 0.076673, acc.: 96.88%] [G loss: 3.867576]\n",
      "epoch:43 step:33880 [D loss: 0.204798, acc.: 90.62%] [G loss: 5.492085]\n",
      "epoch:43 step:33881 [D loss: 0.052385, acc.: 98.44%] [G loss: 5.510239]\n",
      "epoch:43 step:33882 [D loss: 0.214513, acc.: 90.62%] [G loss: 0.158357]\n",
      "epoch:43 step:33883 [D loss: 0.172215, acc.: 92.97%] [G loss: 3.461864]\n",
      "epoch:43 step:33884 [D loss: 0.014279, acc.: 100.00%] [G loss: 3.190002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:33885 [D loss: 0.057849, acc.: 99.22%] [G loss: 3.318412]\n",
      "epoch:43 step:33886 [D loss: 0.052973, acc.: 99.22%] [G loss: 3.858457]\n",
      "epoch:43 step:33887 [D loss: 0.043369, acc.: 100.00%] [G loss: 3.120695]\n",
      "epoch:43 step:33888 [D loss: 0.576907, acc.: 73.44%] [G loss: 6.498575]\n",
      "epoch:43 step:33889 [D loss: 0.572878, acc.: 78.12%] [G loss: 5.391234]\n",
      "epoch:43 step:33890 [D loss: 0.233330, acc.: 90.62%] [G loss: 3.761890]\n",
      "epoch:43 step:33891 [D loss: 0.086098, acc.: 98.44%] [G loss: 0.171279]\n",
      "epoch:43 step:33892 [D loss: 0.023341, acc.: 100.00%] [G loss: 4.013597]\n",
      "epoch:43 step:33893 [D loss: 0.006059, acc.: 100.00%] [G loss: 2.694552]\n",
      "epoch:43 step:33894 [D loss: 0.007373, acc.: 100.00%] [G loss: 1.306045]\n",
      "epoch:43 step:33895 [D loss: 0.012715, acc.: 100.00%] [G loss: 0.633846]\n",
      "epoch:43 step:33896 [D loss: 0.043257, acc.: 100.00%] [G loss: 0.162064]\n",
      "epoch:43 step:33897 [D loss: 0.009062, acc.: 100.00%] [G loss: 0.215091]\n",
      "epoch:43 step:33898 [D loss: 0.745127, acc.: 54.69%] [G loss: 2.182879]\n",
      "epoch:43 step:33899 [D loss: 0.043584, acc.: 96.88%] [G loss: 4.370173]\n",
      "epoch:43 step:33900 [D loss: 0.183873, acc.: 92.19%] [G loss: 0.201782]\n",
      "epoch:43 step:33901 [D loss: 0.025667, acc.: 99.22%] [G loss: 0.939742]\n",
      "epoch:43 step:33902 [D loss: 0.000939, acc.: 100.00%] [G loss: 0.901969]\n",
      "epoch:43 step:33903 [D loss: 0.097565, acc.: 95.31%] [G loss: 0.145272]\n",
      "epoch:43 step:33904 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.043574]\n",
      "epoch:43 step:33905 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.081363]\n",
      "epoch:43 step:33906 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.026156]\n",
      "epoch:43 step:33907 [D loss: 0.001423, acc.: 100.00%] [G loss: 0.005833]\n",
      "epoch:43 step:33908 [D loss: 0.002430, acc.: 100.00%] [G loss: 0.008771]\n",
      "epoch:43 step:33909 [D loss: 0.000843, acc.: 100.00%] [G loss: 0.000748]\n",
      "epoch:43 step:33910 [D loss: 0.036500, acc.: 100.00%] [G loss: 0.034295]\n",
      "epoch:43 step:33911 [D loss: 0.004808, acc.: 100.00%] [G loss: 0.001148]\n",
      "epoch:43 step:33912 [D loss: 0.000420, acc.: 100.00%] [G loss: 0.120301]\n",
      "epoch:43 step:33913 [D loss: 0.001359, acc.: 100.00%] [G loss: 0.045763]\n",
      "epoch:43 step:33914 [D loss: 0.010846, acc.: 100.00%] [G loss: 0.067075]\n",
      "epoch:43 step:33915 [D loss: 0.004029, acc.: 100.00%] [G loss: 0.121270]\n",
      "epoch:43 step:33916 [D loss: 0.002179, acc.: 100.00%] [G loss: 0.072387]\n",
      "epoch:43 step:33917 [D loss: 0.009832, acc.: 99.22%] [G loss: 0.031154]\n",
      "epoch:43 step:33918 [D loss: 0.063210, acc.: 99.22%] [G loss: 0.028405]\n",
      "epoch:43 step:33919 [D loss: 0.004738, acc.: 100.00%] [G loss: 0.665292]\n",
      "epoch:43 step:33920 [D loss: 0.014352, acc.: 100.00%] [G loss: 1.191370]\n",
      "epoch:43 step:33921 [D loss: 0.215720, acc.: 92.19%] [G loss: 0.531458]\n",
      "epoch:43 step:33922 [D loss: 0.028478, acc.: 100.00%] [G loss: 0.044234]\n",
      "epoch:43 step:33923 [D loss: 0.012876, acc.: 100.00%] [G loss: 0.554714]\n",
      "epoch:43 step:33924 [D loss: 0.003295, acc.: 100.00%] [G loss: 0.241952]\n",
      "epoch:43 step:33925 [D loss: 0.005739, acc.: 100.00%] [G loss: 0.048364]\n",
      "epoch:43 step:33926 [D loss: 0.002211, acc.: 100.00%] [G loss: 0.021184]\n",
      "epoch:43 step:33927 [D loss: 0.002487, acc.: 100.00%] [G loss: 0.007553]\n",
      "epoch:43 step:33928 [D loss: 0.000568, acc.: 100.00%] [G loss: 0.003037]\n",
      "epoch:43 step:33929 [D loss: 0.024482, acc.: 98.44%] [G loss: 0.028163]\n",
      "epoch:43 step:33930 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.003382]\n",
      "epoch:43 step:33931 [D loss: 0.002153, acc.: 100.00%] [G loss: 0.001676]\n",
      "epoch:43 step:33932 [D loss: 0.000696, acc.: 100.00%] [G loss: 0.342028]\n",
      "epoch:43 step:33933 [D loss: 0.014623, acc.: 100.00%] [G loss: 0.113335]\n",
      "epoch:43 step:33934 [D loss: 0.023317, acc.: 100.00%] [G loss: 0.002003]\n",
      "epoch:43 step:33935 [D loss: 0.003597, acc.: 100.00%] [G loss: 0.006735]\n",
      "epoch:43 step:33936 [D loss: 0.099541, acc.: 97.66%] [G loss: 0.000190]\n",
      "epoch:43 step:33937 [D loss: 0.012615, acc.: 99.22%] [G loss: 0.000545]\n",
      "epoch:43 step:33938 [D loss: 0.010618, acc.: 100.00%] [G loss: 0.001682]\n",
      "epoch:43 step:33939 [D loss: 0.006649, acc.: 100.00%] [G loss: 0.009839]\n",
      "epoch:43 step:33940 [D loss: 0.010286, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:43 step:33941 [D loss: 0.001356, acc.: 100.00%] [G loss: 0.006895]\n",
      "epoch:43 step:33942 [D loss: 0.001279, acc.: 100.00%] [G loss: 0.001236]\n",
      "epoch:43 step:33943 [D loss: 0.000304, acc.: 100.00%] [G loss: 1.472678]\n",
      "epoch:43 step:33944 [D loss: 0.011080, acc.: 100.00%] [G loss: 0.028151]\n",
      "epoch:43 step:33945 [D loss: 0.004374, acc.: 100.00%] [G loss: 0.094360]\n",
      "epoch:43 step:33946 [D loss: 0.023181, acc.: 100.00%] [G loss: 0.113930]\n",
      "epoch:43 step:33947 [D loss: 0.003216, acc.: 100.00%] [G loss: 0.084567]\n",
      "epoch:43 step:33948 [D loss: 0.001859, acc.: 100.00%] [G loss: 0.119946]\n",
      "epoch:43 step:33949 [D loss: 0.024833, acc.: 99.22%] [G loss: 0.000489]\n",
      "epoch:43 step:33950 [D loss: 0.002239, acc.: 100.00%] [G loss: 0.034305]\n",
      "epoch:43 step:33951 [D loss: 0.007766, acc.: 100.00%] [G loss: 0.000488]\n",
      "epoch:43 step:33952 [D loss: 0.003021, acc.: 100.00%] [G loss: 0.004151]\n",
      "epoch:43 step:33953 [D loss: 0.056487, acc.: 99.22%] [G loss: 0.074750]\n",
      "epoch:43 step:33954 [D loss: 0.000974, acc.: 100.00%] [G loss: 0.748897]\n",
      "epoch:43 step:33955 [D loss: 0.008830, acc.: 100.00%] [G loss: 0.516667]\n",
      "epoch:43 step:33956 [D loss: 0.001684, acc.: 100.00%] [G loss: 0.195535]\n",
      "epoch:43 step:33957 [D loss: 0.040769, acc.: 98.44%] [G loss: 0.098664]\n",
      "epoch:43 step:33958 [D loss: 0.035179, acc.: 100.00%] [G loss: 0.001152]\n",
      "epoch:43 step:33959 [D loss: 0.007911, acc.: 99.22%] [G loss: 0.001351]\n",
      "epoch:43 step:33960 [D loss: 0.004979, acc.: 100.00%] [G loss: 0.002057]\n",
      "epoch:43 step:33961 [D loss: 0.003244, acc.: 100.00%] [G loss: 0.003141]\n",
      "epoch:43 step:33962 [D loss: 0.001047, acc.: 100.00%] [G loss: 0.160726]\n",
      "epoch:43 step:33963 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.049029]\n",
      "epoch:43 step:33964 [D loss: 0.001534, acc.: 100.00%] [G loss: 0.031311]\n",
      "epoch:43 step:33965 [D loss: 0.013994, acc.: 100.00%] [G loss: 0.012059]\n",
      "epoch:43 step:33966 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.133748]\n",
      "epoch:43 step:33967 [D loss: 0.000664, acc.: 100.00%] [G loss: 0.011469]\n",
      "epoch:43 step:33968 [D loss: 0.001545, acc.: 100.00%] [G loss: 0.051321]\n",
      "epoch:43 step:33969 [D loss: 0.032641, acc.: 98.44%] [G loss: 0.000085]\n",
      "epoch:43 step:33970 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.205985]\n",
      "epoch:43 step:33971 [D loss: 0.003094, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:43 step:33972 [D loss: 0.002938, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:43 step:33973 [D loss: 0.000809, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:43 step:33974 [D loss: 0.007158, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:43 step:33975 [D loss: 0.024524, acc.: 100.00%] [G loss: 0.010922]\n",
      "epoch:43 step:33976 [D loss: 0.001100, acc.: 100.00%] [G loss: 0.017800]\n",
      "epoch:43 step:33977 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.004797]\n",
      "epoch:43 step:33978 [D loss: 0.162904, acc.: 94.53%] [G loss: 0.009665]\n",
      "epoch:43 step:33979 [D loss: 0.004044, acc.: 100.00%] [G loss: 0.340428]\n",
      "epoch:43 step:33980 [D loss: 0.019903, acc.: 100.00%] [G loss: 0.082366]\n",
      "epoch:43 step:33981 [D loss: 0.007577, acc.: 100.00%] [G loss: 0.181739]\n",
      "epoch:43 step:33982 [D loss: 0.043864, acc.: 100.00%] [G loss: 1.038733]\n",
      "epoch:43 step:33983 [D loss: 0.009819, acc.: 100.00%] [G loss: 3.176414]\n",
      "epoch:43 step:33984 [D loss: 0.216526, acc.: 92.97%] [G loss: 2.172342]\n",
      "epoch:43 step:33985 [D loss: 0.038213, acc.: 98.44%] [G loss: 7.307563]\n",
      "epoch:43 step:33986 [D loss: 0.053486, acc.: 97.66%] [G loss: 0.004645]\n",
      "epoch:43 step:33987 [D loss: 0.001269, acc.: 100.00%] [G loss: 0.956928]\n",
      "epoch:43 step:33988 [D loss: 0.004682, acc.: 100.00%] [G loss: 0.000724]\n",
      "epoch:43 step:33989 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.017907]\n",
      "epoch:43 step:33990 [D loss: 0.001616, acc.: 100.00%] [G loss: 0.027476]\n",
      "epoch:43 step:33991 [D loss: 0.001226, acc.: 100.00%] [G loss: 0.006019]\n",
      "epoch:43 step:33992 [D loss: 0.000613, acc.: 100.00%] [G loss: 0.001977]\n",
      "epoch:43 step:33993 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.010754]\n",
      "epoch:43 step:33994 [D loss: 0.003622, acc.: 100.00%] [G loss: 0.001571]\n",
      "epoch:43 step:33995 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.003297]\n",
      "epoch:43 step:33996 [D loss: 0.005847, acc.: 100.00%] [G loss: 0.000790]\n",
      "epoch:43 step:33997 [D loss: 0.007113, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:43 step:33998 [D loss: 0.005658, acc.: 100.00%] [G loss: 0.001827]\n",
      "epoch:43 step:33999 [D loss: 0.199437, acc.: 90.62%] [G loss: 5.334335]\n",
      "epoch:43 step:34000 [D loss: 0.646393, acc.: 72.66%] [G loss: 0.458747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34001 [D loss: 1.203110, acc.: 62.50%] [G loss: 7.281224]\n",
      "epoch:43 step:34002 [D loss: 1.003685, acc.: 64.06%] [G loss: 5.019451]\n",
      "epoch:43 step:34003 [D loss: 0.123409, acc.: 95.31%] [G loss: 3.248209]\n",
      "epoch:43 step:34004 [D loss: 0.028972, acc.: 100.00%] [G loss: 2.290407]\n",
      "epoch:43 step:34005 [D loss: 0.244327, acc.: 90.62%] [G loss: 4.099688]\n",
      "epoch:43 step:34006 [D loss: 0.020254, acc.: 99.22%] [G loss: 3.876826]\n",
      "epoch:43 step:34007 [D loss: 0.384813, acc.: 89.84%] [G loss: 1.132035]\n",
      "epoch:43 step:34008 [D loss: 0.301627, acc.: 84.38%] [G loss: 5.630597]\n",
      "epoch:43 step:34009 [D loss: 0.174633, acc.: 91.41%] [G loss: 5.400743]\n",
      "epoch:43 step:34010 [D loss: 0.030333, acc.: 97.66%] [G loss: 4.948853]\n",
      "epoch:43 step:34011 [D loss: 0.302678, acc.: 86.72%] [G loss: 5.749421]\n",
      "epoch:43 step:34012 [D loss: 0.014989, acc.: 100.00%] [G loss: 5.436401]\n",
      "epoch:43 step:34013 [D loss: 0.197682, acc.: 93.75%] [G loss: 4.723841]\n",
      "epoch:43 step:34014 [D loss: 0.075633, acc.: 99.22%] [G loss: 3.707186]\n",
      "epoch:43 step:34015 [D loss: 0.052086, acc.: 100.00%] [G loss: 3.003084]\n",
      "epoch:43 step:34016 [D loss: 0.031920, acc.: 100.00%] [G loss: 3.422955]\n",
      "epoch:43 step:34017 [D loss: 0.046493, acc.: 99.22%] [G loss: 4.161826]\n",
      "epoch:43 step:34018 [D loss: 0.129684, acc.: 95.31%] [G loss: 3.651586]\n",
      "epoch:43 step:34019 [D loss: 0.084872, acc.: 96.88%] [G loss: 4.078103]\n",
      "epoch:43 step:34020 [D loss: 0.054434, acc.: 99.22%] [G loss: 4.080873]\n",
      "epoch:43 step:34021 [D loss: 0.129553, acc.: 95.31%] [G loss: 5.794329]\n",
      "epoch:43 step:34022 [D loss: 0.135198, acc.: 94.53%] [G loss: 3.581794]\n",
      "epoch:43 step:34023 [D loss: 0.399255, acc.: 82.03%] [G loss: 7.444141]\n",
      "epoch:43 step:34024 [D loss: 0.474494, acc.: 83.59%] [G loss: 0.074209]\n",
      "epoch:43 step:34025 [D loss: 0.003381, acc.: 100.00%] [G loss: 6.171317]\n",
      "epoch:43 step:34026 [D loss: 0.006334, acc.: 100.00%] [G loss: 4.178442]\n",
      "epoch:43 step:34027 [D loss: 0.051459, acc.: 98.44%] [G loss: 2.330215]\n",
      "epoch:43 step:34028 [D loss: 0.011046, acc.: 100.00%] [G loss: 0.001920]\n",
      "epoch:43 step:34029 [D loss: 0.001919, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:43 step:34030 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:43 step:34031 [D loss: 0.000894, acc.: 100.00%] [G loss: 0.732783]\n",
      "epoch:43 step:34032 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.344697]\n",
      "epoch:43 step:34033 [D loss: 0.011500, acc.: 100.00%] [G loss: 0.167924]\n",
      "epoch:43 step:34034 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.224256]\n",
      "epoch:43 step:34035 [D loss: 0.000861, acc.: 100.00%] [G loss: 0.251317]\n",
      "epoch:43 step:34036 [D loss: 0.001830, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:43 step:34037 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.031418]\n",
      "epoch:43 step:34038 [D loss: 0.000752, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:43 step:34039 [D loss: 0.044895, acc.: 98.44%] [G loss: 0.306674]\n",
      "epoch:43 step:34040 [D loss: 0.003743, acc.: 100.00%] [G loss: 0.095753]\n",
      "epoch:43 step:34041 [D loss: 0.003416, acc.: 100.00%] [G loss: 0.038339]\n",
      "epoch:43 step:34042 [D loss: 0.011093, acc.: 100.00%] [G loss: 0.429597]\n",
      "epoch:43 step:34043 [D loss: 0.019939, acc.: 100.00%] [G loss: 0.102860]\n",
      "epoch:43 step:34044 [D loss: 0.000977, acc.: 100.00%] [G loss: 0.075831]\n",
      "epoch:43 step:34045 [D loss: 0.002468, acc.: 100.00%] [G loss: 0.011716]\n",
      "epoch:43 step:34046 [D loss: 0.001187, acc.: 100.00%] [G loss: 0.017700]\n",
      "epoch:43 step:34047 [D loss: 0.000613, acc.: 100.00%] [G loss: 0.011151]\n",
      "epoch:43 step:34048 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000748]\n",
      "epoch:43 step:34049 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.003852]\n",
      "epoch:43 step:34050 [D loss: 0.003346, acc.: 100.00%] [G loss: 0.029861]\n",
      "epoch:43 step:34051 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.008871]\n",
      "epoch:43 step:34052 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.004347]\n",
      "epoch:43 step:34053 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.003963]\n",
      "epoch:43 step:34054 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000750]\n",
      "epoch:43 step:34055 [D loss: 0.003780, acc.: 100.00%] [G loss: 0.004396]\n",
      "epoch:43 step:34056 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.005934]\n",
      "epoch:43 step:34057 [D loss: 0.000492, acc.: 100.00%] [G loss: 0.014742]\n",
      "epoch:43 step:34058 [D loss: 0.002292, acc.: 100.00%] [G loss: 0.005927]\n",
      "epoch:43 step:34059 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.001724]\n",
      "epoch:43 step:34060 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.000885]\n",
      "epoch:43 step:34061 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.004149]\n",
      "epoch:43 step:34062 [D loss: 0.000822, acc.: 100.00%] [G loss: 0.012018]\n",
      "epoch:43 step:34063 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.154866]\n",
      "epoch:43 step:34064 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.002089]\n",
      "epoch:43 step:34065 [D loss: 0.003702, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:43 step:34066 [D loss: 0.000690, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:43 step:34067 [D loss: 0.008450, acc.: 100.00%] [G loss: 0.001635]\n",
      "epoch:43 step:34068 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.001858]\n",
      "epoch:43 step:34069 [D loss: 0.006184, acc.: 100.00%] [G loss: 0.011916]\n",
      "epoch:43 step:34070 [D loss: 0.000951, acc.: 100.00%] [G loss: 0.000819]\n",
      "epoch:43 step:34071 [D loss: 0.001032, acc.: 100.00%] [G loss: 0.203234]\n",
      "epoch:43 step:34072 [D loss: 0.001795, acc.: 100.00%] [G loss: 0.000347]\n",
      "epoch:43 step:34073 [D loss: 0.000968, acc.: 100.00%] [G loss: 0.000941]\n",
      "epoch:43 step:34074 [D loss: 0.001664, acc.: 100.00%] [G loss: 0.002524]\n",
      "epoch:43 step:34075 [D loss: 0.001523, acc.: 100.00%] [G loss: 0.001478]\n",
      "epoch:43 step:34076 [D loss: 0.012156, acc.: 100.00%] [G loss: 0.002137]\n",
      "epoch:43 step:34077 [D loss: 0.010864, acc.: 100.00%] [G loss: 0.002897]\n",
      "epoch:43 step:34078 [D loss: 0.037023, acc.: 100.00%] [G loss: 0.113876]\n",
      "epoch:43 step:34079 [D loss: 0.005047, acc.: 100.00%] [G loss: 0.209876]\n",
      "epoch:43 step:34080 [D loss: 0.093896, acc.: 96.88%] [G loss: 0.002674]\n",
      "epoch:43 step:34081 [D loss: 0.001432, acc.: 100.00%] [G loss: 0.000249]\n",
      "epoch:43 step:34082 [D loss: 0.028456, acc.: 100.00%] [G loss: 0.000738]\n",
      "epoch:43 step:34083 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.468135]\n",
      "epoch:43 step:34084 [D loss: 0.012442, acc.: 100.00%] [G loss: 0.025571]\n",
      "epoch:43 step:34085 [D loss: 0.007056, acc.: 100.00%] [G loss: 0.071251]\n",
      "epoch:43 step:34086 [D loss: 0.012579, acc.: 100.00%] [G loss: 0.532671]\n",
      "epoch:43 step:34087 [D loss: 0.005515, acc.: 100.00%] [G loss: 0.119962]\n",
      "epoch:43 step:34088 [D loss: 0.037531, acc.: 98.44%] [G loss: 0.091914]\n",
      "epoch:43 step:34089 [D loss: 0.057991, acc.: 100.00%] [G loss: 1.196077]\n",
      "epoch:43 step:34090 [D loss: 0.100129, acc.: 93.75%] [G loss: 0.017058]\n",
      "epoch:43 step:34091 [D loss: 0.019593, acc.: 100.00%] [G loss: 0.127784]\n",
      "epoch:43 step:34092 [D loss: 0.013245, acc.: 99.22%] [G loss: 0.142483]\n",
      "epoch:43 step:34093 [D loss: 0.004044, acc.: 100.00%] [G loss: 0.596279]\n",
      "epoch:43 step:34094 [D loss: 0.007639, acc.: 100.00%] [G loss: 0.265346]\n",
      "epoch:43 step:34095 [D loss: 0.005677, acc.: 100.00%] [G loss: 0.012061]\n",
      "epoch:43 step:34096 [D loss: 0.013085, acc.: 100.00%] [G loss: 0.008258]\n",
      "epoch:43 step:34097 [D loss: 0.022497, acc.: 100.00%] [G loss: 0.015663]\n",
      "epoch:43 step:34098 [D loss: 0.002226, acc.: 100.00%] [G loss: 0.033669]\n",
      "epoch:43 step:34099 [D loss: 0.003576, acc.: 100.00%] [G loss: 0.504860]\n",
      "epoch:43 step:34100 [D loss: 0.003651, acc.: 100.00%] [G loss: 0.379589]\n",
      "epoch:43 step:34101 [D loss: 0.002554, acc.: 100.00%] [G loss: 0.049545]\n",
      "epoch:43 step:34102 [D loss: 0.008104, acc.: 100.00%] [G loss: 0.102454]\n",
      "epoch:43 step:34103 [D loss: 0.013701, acc.: 100.00%] [G loss: 0.020198]\n",
      "epoch:43 step:34104 [D loss: 0.006207, acc.: 100.00%] [G loss: 0.039625]\n",
      "epoch:43 step:34105 [D loss: 0.009331, acc.: 100.00%] [G loss: 0.032254]\n",
      "epoch:43 step:34106 [D loss: 0.009678, acc.: 100.00%] [G loss: 0.158399]\n",
      "epoch:43 step:34107 [D loss: 0.069771, acc.: 98.44%] [G loss: 0.777359]\n",
      "epoch:43 step:34108 [D loss: 0.006341, acc.: 100.00%] [G loss: 0.016999]\n",
      "epoch:43 step:34109 [D loss: 0.108276, acc.: 95.31%] [G loss: 0.017765]\n",
      "epoch:43 step:34110 [D loss: 0.051085, acc.: 100.00%] [G loss: 0.000311]\n",
      "epoch:43 step:34111 [D loss: 0.002327, acc.: 100.00%] [G loss: 0.004085]\n",
      "epoch:43 step:34112 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.014606]\n",
      "epoch:43 step:34113 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.004708]\n",
      "epoch:43 step:34114 [D loss: 0.002414, acc.: 100.00%] [G loss: 0.012915]\n",
      "epoch:43 step:34115 [D loss: 0.021642, acc.: 100.00%] [G loss: 0.778800]\n",
      "epoch:43 step:34116 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34117 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:43 step:34118 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.366948]\n",
      "epoch:43 step:34119 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:43 step:34120 [D loss: 0.004280, acc.: 100.00%] [G loss: 0.261261]\n",
      "epoch:43 step:34121 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34122 [D loss: 0.008879, acc.: 100.00%] [G loss: 0.120233]\n",
      "epoch:43 step:34123 [D loss: 0.000965, acc.: 100.00%] [G loss: 0.000135]\n",
      "epoch:43 step:34124 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.094146]\n",
      "epoch:43 step:34125 [D loss: 0.001355, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:43 step:34126 [D loss: 0.000635, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:43 step:34127 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.013994]\n",
      "epoch:43 step:34128 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:43 step:34129 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.023488]\n",
      "epoch:43 step:34130 [D loss: 0.000910, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:43 step:34131 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:43 step:34132 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:43 step:34133 [D loss: 0.004340, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34134 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:43 step:34135 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34136 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34137 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.013402]\n",
      "epoch:43 step:34138 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:43 step:34139 [D loss: 0.000865, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:43 step:34140 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:43 step:34141 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:43 step:34142 [D loss: 0.001111, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:43 step:34143 [D loss: 0.040561, acc.: 97.66%] [G loss: 0.000048]\n",
      "epoch:43 step:34144 [D loss: 0.005534, acc.: 100.00%] [G loss: 0.954762]\n",
      "epoch:43 step:34145 [D loss: 0.002546, acc.: 100.00%] [G loss: 0.000953]\n",
      "epoch:43 step:34146 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000904]\n",
      "epoch:43 step:34147 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.001071]\n",
      "epoch:43 step:34148 [D loss: 0.007439, acc.: 100.00%] [G loss: 0.002637]\n",
      "epoch:43 step:34149 [D loss: 0.006521, acc.: 100.00%] [G loss: 0.079179]\n",
      "epoch:43 step:34150 [D loss: 0.001666, acc.: 100.00%] [G loss: 0.054593]\n",
      "epoch:43 step:34151 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.039155]\n",
      "epoch:43 step:34152 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.001179]\n",
      "epoch:43 step:34153 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000868]\n",
      "epoch:43 step:34154 [D loss: 0.002215, acc.: 100.00%] [G loss: 0.000935]\n",
      "epoch:43 step:34155 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:43 step:34156 [D loss: 0.000624, acc.: 100.00%] [G loss: 0.013830]\n",
      "epoch:43 step:34157 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:43 step:34158 [D loss: 0.015834, acc.: 99.22%] [G loss: 0.000014]\n",
      "epoch:43 step:34159 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:43 step:34160 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:43 step:34161 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:43 step:34162 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:43 step:34163 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:43 step:34164 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:43 step:34165 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:43 step:34166 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:43 step:34167 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:43 step:34168 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.009445]\n",
      "epoch:43 step:34169 [D loss: 0.008627, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:43 step:34170 [D loss: 0.003349, acc.: 100.00%] [G loss: 0.000691]\n",
      "epoch:43 step:34171 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000570]\n",
      "epoch:43 step:34172 [D loss: 0.073171, acc.: 96.88%] [G loss: 1.192600]\n",
      "epoch:43 step:34173 [D loss: 0.005161, acc.: 100.00%] [G loss: 1.950289]\n",
      "epoch:43 step:34174 [D loss: 2.097817, acc.: 32.81%] [G loss: 8.070770]\n",
      "epoch:43 step:34175 [D loss: 2.962264, acc.: 50.00%] [G loss: 4.375645]\n",
      "epoch:43 step:34176 [D loss: 0.193085, acc.: 89.84%] [G loss: 1.230179]\n",
      "epoch:43 step:34177 [D loss: 0.018215, acc.: 100.00%] [G loss: 0.428928]\n",
      "epoch:43 step:34178 [D loss: 0.015228, acc.: 100.00%] [G loss: 0.259891]\n",
      "epoch:43 step:34179 [D loss: 0.032692, acc.: 100.00%] [G loss: 0.014955]\n",
      "epoch:43 step:34180 [D loss: 0.063296, acc.: 100.00%] [G loss: 0.516311]\n",
      "epoch:43 step:34181 [D loss: 0.004204, acc.: 100.00%] [G loss: 0.609905]\n",
      "epoch:43 step:34182 [D loss: 0.016470, acc.: 100.00%] [G loss: 0.404213]\n",
      "epoch:43 step:34183 [D loss: 0.011379, acc.: 100.00%] [G loss: 0.119586]\n",
      "epoch:43 step:34184 [D loss: 0.067598, acc.: 98.44%] [G loss: 0.184464]\n",
      "epoch:43 step:34185 [D loss: 0.011216, acc.: 100.00%] [G loss: 0.350252]\n",
      "epoch:43 step:34186 [D loss: 0.031114, acc.: 99.22%] [G loss: 0.198511]\n",
      "epoch:43 step:34187 [D loss: 0.081664, acc.: 98.44%] [G loss: 0.014129]\n",
      "epoch:43 step:34188 [D loss: 0.064890, acc.: 100.00%] [G loss: 0.026191]\n",
      "epoch:43 step:34189 [D loss: 0.003106, acc.: 100.00%] [G loss: 0.193334]\n",
      "epoch:43 step:34190 [D loss: 0.039600, acc.: 98.44%] [G loss: 0.043156]\n",
      "epoch:43 step:34191 [D loss: 0.020041, acc.: 100.00%] [G loss: 0.011208]\n",
      "epoch:43 step:34192 [D loss: 0.030057, acc.: 99.22%] [G loss: 0.006366]\n",
      "epoch:43 step:34193 [D loss: 0.025949, acc.: 100.00%] [G loss: 0.004530]\n",
      "epoch:43 step:34194 [D loss: 0.077316, acc.: 96.88%] [G loss: 0.090501]\n",
      "epoch:43 step:34195 [D loss: 0.018594, acc.: 99.22%] [G loss: 0.365101]\n",
      "epoch:43 step:34196 [D loss: 0.227091, acc.: 87.50%] [G loss: 0.009505]\n",
      "epoch:43 step:34197 [D loss: 0.615160, acc.: 71.09%] [G loss: 2.129670]\n",
      "epoch:43 step:34198 [D loss: 0.016198, acc.: 100.00%] [G loss: 3.834248]\n",
      "epoch:43 step:34199 [D loss: 0.704688, acc.: 72.66%] [G loss: 0.189972]\n",
      "epoch:43 step:34200 [D loss: 0.013766, acc.: 100.00%] [G loss: 0.012837]\n",
      "epoch:43 step:34201 [D loss: 0.002613, acc.: 100.00%] [G loss: 0.004776]\n",
      "epoch:43 step:34202 [D loss: 0.008076, acc.: 100.00%] [G loss: 0.021327]\n",
      "epoch:43 step:34203 [D loss: 0.001998, acc.: 100.00%] [G loss: 0.652619]\n",
      "epoch:43 step:34204 [D loss: 0.004849, acc.: 100.00%] [G loss: 0.392492]\n",
      "epoch:43 step:34205 [D loss: 0.042426, acc.: 98.44%] [G loss: 0.189164]\n",
      "epoch:43 step:34206 [D loss: 0.110112, acc.: 96.09%] [G loss: 0.025149]\n",
      "epoch:43 step:34207 [D loss: 0.048114, acc.: 98.44%] [G loss: 0.009817]\n",
      "epoch:43 step:34208 [D loss: 0.061046, acc.: 98.44%] [G loss: 0.002344]\n",
      "epoch:43 step:34209 [D loss: 0.015911, acc.: 100.00%] [G loss: 1.969982]\n",
      "epoch:43 step:34210 [D loss: 0.041345, acc.: 99.22%] [G loss: 0.002192]\n",
      "epoch:43 step:34211 [D loss: 0.103166, acc.: 99.22%] [G loss: 0.009201]\n",
      "epoch:43 step:34212 [D loss: 0.023870, acc.: 100.00%] [G loss: 0.126429]\n",
      "epoch:43 step:34213 [D loss: 0.045218, acc.: 97.66%] [G loss: 0.015558]\n",
      "epoch:43 step:34214 [D loss: 0.033832, acc.: 100.00%] [G loss: 0.578319]\n",
      "epoch:43 step:34215 [D loss: 0.016916, acc.: 100.00%] [G loss: 0.008287]\n",
      "epoch:43 step:34216 [D loss: 0.022772, acc.: 100.00%] [G loss: 0.608784]\n",
      "epoch:43 step:34217 [D loss: 0.085920, acc.: 96.09%] [G loss: 0.543135]\n",
      "epoch:43 step:34218 [D loss: 0.021162, acc.: 100.00%] [G loss: 1.532907]\n",
      "epoch:43 step:34219 [D loss: 0.049925, acc.: 97.66%] [G loss: 1.093545]\n",
      "epoch:43 step:34220 [D loss: 0.507520, acc.: 73.44%] [G loss: 0.218351]\n",
      "epoch:43 step:34221 [D loss: 0.087626, acc.: 96.88%] [G loss: 0.274504]\n",
      "epoch:43 step:34222 [D loss: 0.393284, acc.: 82.81%] [G loss: 0.000200]\n",
      "epoch:43 step:34223 [D loss: 0.180826, acc.: 93.75%] [G loss: 2.256520]\n",
      "epoch:43 step:34224 [D loss: 0.004874, acc.: 100.00%] [G loss: 0.017385]\n",
      "epoch:43 step:34225 [D loss: 0.018522, acc.: 100.00%] [G loss: 0.006718]\n",
      "epoch:43 step:34226 [D loss: 0.003885, acc.: 100.00%] [G loss: 0.003867]\n",
      "epoch:43 step:34227 [D loss: 0.024655, acc.: 98.44%] [G loss: 0.001485]\n",
      "epoch:43 step:34228 [D loss: 0.002069, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:43 step:34229 [D loss: 0.017297, acc.: 100.00%] [G loss: 1.389607]\n",
      "epoch:43 step:34230 [D loss: 0.009605, acc.: 100.00%] [G loss: 1.575020]\n",
      "epoch:43 step:34231 [D loss: 0.075554, acc.: 96.88%] [G loss: 0.002239]\n",
      "epoch:43 step:34232 [D loss: 0.038077, acc.: 98.44%] [G loss: 0.056830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34233 [D loss: 0.064562, acc.: 98.44%] [G loss: 1.351449]\n",
      "epoch:43 step:34234 [D loss: 0.034350, acc.: 99.22%] [G loss: 0.005925]\n",
      "epoch:43 step:34235 [D loss: 0.017102, acc.: 100.00%] [G loss: 0.014104]\n",
      "epoch:43 step:34236 [D loss: 0.002073, acc.: 100.00%] [G loss: 0.000530]\n",
      "epoch:43 step:34237 [D loss: 0.002826, acc.: 100.00%] [G loss: 0.727083]\n",
      "epoch:43 step:34238 [D loss: 0.001389, acc.: 100.00%] [G loss: 0.140778]\n",
      "epoch:43 step:34239 [D loss: 0.012935, acc.: 100.00%] [G loss: 0.023609]\n",
      "epoch:43 step:34240 [D loss: 0.042104, acc.: 99.22%] [G loss: 0.000376]\n",
      "epoch:43 step:34241 [D loss: 0.028661, acc.: 98.44%] [G loss: 0.040987]\n",
      "epoch:43 step:34242 [D loss: 0.074994, acc.: 97.66%] [G loss: 0.272430]\n",
      "epoch:43 step:34243 [D loss: 0.010840, acc.: 99.22%] [G loss: 0.151158]\n",
      "epoch:43 step:34244 [D loss: 0.339085, acc.: 84.38%] [G loss: 0.000003]\n",
      "epoch:43 step:34245 [D loss: 0.044869, acc.: 99.22%] [G loss: 0.000001]\n",
      "epoch:43 step:34246 [D loss: 0.000805, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:43 step:34247 [D loss: 0.155952, acc.: 92.19%] [G loss: 0.003798]\n",
      "epoch:43 step:34248 [D loss: 0.006670, acc.: 100.00%] [G loss: 0.245331]\n",
      "epoch:43 step:34249 [D loss: 0.004269, acc.: 100.00%] [G loss: 3.781803]\n",
      "epoch:43 step:34250 [D loss: 0.003844, acc.: 100.00%] [G loss: 1.179697]\n",
      "epoch:43 step:34251 [D loss: 0.015428, acc.: 100.00%] [G loss: 0.022156]\n",
      "epoch:43 step:34252 [D loss: 0.077618, acc.: 96.88%] [G loss: 0.032472]\n",
      "epoch:43 step:34253 [D loss: 0.017371, acc.: 99.22%] [G loss: 0.108397]\n",
      "epoch:43 step:34254 [D loss: 0.097454, acc.: 98.44%] [G loss: 0.120457]\n",
      "epoch:43 step:34255 [D loss: 0.009366, acc.: 100.00%] [G loss: 0.275102]\n",
      "epoch:43 step:34256 [D loss: 0.128808, acc.: 96.09%] [G loss: 1.956362]\n",
      "epoch:43 step:34257 [D loss: 0.016716, acc.: 100.00%] [G loss: 0.056774]\n",
      "epoch:43 step:34258 [D loss: 0.016713, acc.: 99.22%] [G loss: 0.895599]\n",
      "epoch:43 step:34259 [D loss: 0.396769, acc.: 84.38%] [G loss: 0.000000]\n",
      "epoch:43 step:34260 [D loss: 1.216336, acc.: 53.91%] [G loss: 3.301745]\n",
      "epoch:43 step:34261 [D loss: 0.563742, acc.: 79.69%] [G loss: 3.885984]\n",
      "epoch:43 step:34262 [D loss: 0.350795, acc.: 88.28%] [G loss: 0.442043]\n",
      "epoch:43 step:34263 [D loss: 0.050843, acc.: 98.44%] [G loss: 0.268877]\n",
      "epoch:43 step:34264 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.049923]\n",
      "epoch:43 step:34265 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.040756]\n",
      "epoch:43 step:34266 [D loss: 0.009240, acc.: 100.00%] [G loss: 0.022624]\n",
      "epoch:43 step:34267 [D loss: 0.001405, acc.: 100.00%] [G loss: 0.017623]\n",
      "epoch:43 step:34268 [D loss: 0.009253, acc.: 99.22%] [G loss: 0.016581]\n",
      "epoch:43 step:34269 [D loss: 0.002744, acc.: 100.00%] [G loss: 0.022107]\n",
      "epoch:43 step:34270 [D loss: 0.000507, acc.: 100.00%] [G loss: 0.043378]\n",
      "epoch:43 step:34271 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.039020]\n",
      "epoch:43 step:34272 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.428940]\n",
      "epoch:43 step:34273 [D loss: 0.000984, acc.: 100.00%] [G loss: 0.003656]\n",
      "epoch:43 step:34274 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.008495]\n",
      "epoch:43 step:34275 [D loss: 0.016604, acc.: 99.22%] [G loss: 0.000775]\n",
      "epoch:43 step:34276 [D loss: 0.000543, acc.: 100.00%] [G loss: 0.020240]\n",
      "epoch:43 step:34277 [D loss: 0.099932, acc.: 96.88%] [G loss: 0.041985]\n",
      "epoch:43 step:34278 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.045743]\n",
      "epoch:43 step:34279 [D loss: 0.000634, acc.: 100.00%] [G loss: 0.030210]\n",
      "epoch:43 step:34280 [D loss: 0.002212, acc.: 100.00%] [G loss: 0.072930]\n",
      "epoch:43 step:34281 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.929018]\n",
      "epoch:43 step:34282 [D loss: 0.002226, acc.: 100.00%] [G loss: 0.075760]\n",
      "epoch:43 step:34283 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.022422]\n",
      "epoch:43 step:34284 [D loss: 0.000808, acc.: 100.00%] [G loss: 0.018607]\n",
      "epoch:43 step:34285 [D loss: 0.008313, acc.: 100.00%] [G loss: 0.074924]\n",
      "epoch:43 step:34286 [D loss: 0.012841, acc.: 100.00%] [G loss: 0.012478]\n",
      "epoch:43 step:34287 [D loss: 0.053536, acc.: 97.66%] [G loss: 0.005123]\n",
      "epoch:43 step:34288 [D loss: 0.158554, acc.: 91.41%] [G loss: 0.014903]\n",
      "epoch:43 step:34289 [D loss: 0.001359, acc.: 100.00%] [G loss: 0.308995]\n",
      "epoch:43 step:34290 [D loss: 0.038320, acc.: 98.44%] [G loss: 0.042232]\n",
      "epoch:43 step:34291 [D loss: 0.040277, acc.: 99.22%] [G loss: 0.170140]\n",
      "epoch:43 step:34292 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.107200]\n",
      "epoch:43 step:34293 [D loss: 0.001226, acc.: 100.00%] [G loss: 0.213982]\n",
      "epoch:43 step:34294 [D loss: 0.002074, acc.: 100.00%] [G loss: 0.003648]\n",
      "epoch:43 step:34295 [D loss: 0.001583, acc.: 100.00%] [G loss: 0.006252]\n",
      "epoch:43 step:34296 [D loss: 0.100136, acc.: 97.66%] [G loss: 0.001357]\n",
      "epoch:43 step:34297 [D loss: 0.003839, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:43 step:34298 [D loss: 0.001983, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:43 step:34299 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:43 step:34300 [D loss: 0.077960, acc.: 96.88%] [G loss: 0.001713]\n",
      "epoch:43 step:34301 [D loss: 0.000461, acc.: 100.00%] [G loss: 0.005084]\n",
      "epoch:43 step:34302 [D loss: 0.001114, acc.: 100.00%] [G loss: 0.001266]\n",
      "epoch:43 step:34303 [D loss: 0.000873, acc.: 100.00%] [G loss: 0.017617]\n",
      "epoch:43 step:34304 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.038458]\n",
      "epoch:43 step:34305 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.002849]\n",
      "epoch:43 step:34306 [D loss: 0.004354, acc.: 100.00%] [G loss: 0.017516]\n",
      "epoch:43 step:34307 [D loss: 0.000489, acc.: 100.00%] [G loss: 0.002432]\n",
      "epoch:43 step:34308 [D loss: 0.001758, acc.: 100.00%] [G loss: 0.001163]\n",
      "epoch:43 step:34309 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.001347]\n",
      "epoch:43 step:34310 [D loss: 0.000752, acc.: 100.00%] [G loss: 0.002942]\n",
      "epoch:43 step:34311 [D loss: 0.000669, acc.: 100.00%] [G loss: 0.004733]\n",
      "epoch:43 step:34312 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.003851]\n",
      "epoch:43 step:34313 [D loss: 0.000630, acc.: 100.00%] [G loss: 0.016225]\n",
      "epoch:43 step:34314 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.001673]\n",
      "epoch:43 step:34315 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.002325]\n",
      "epoch:43 step:34316 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.046555]\n",
      "epoch:43 step:34317 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.004253]\n",
      "epoch:43 step:34318 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000763]\n",
      "epoch:43 step:34319 [D loss: 0.001312, acc.: 100.00%] [G loss: 0.028843]\n",
      "epoch:43 step:34320 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.024907]\n",
      "epoch:43 step:34321 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.008430]\n",
      "epoch:43 step:34322 [D loss: 0.000700, acc.: 100.00%] [G loss: 0.000275]\n",
      "epoch:43 step:34323 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.042331]\n",
      "epoch:43 step:34324 [D loss: 0.001131, acc.: 100.00%] [G loss: 0.001287]\n",
      "epoch:43 step:34325 [D loss: 0.000711, acc.: 100.00%] [G loss: 0.000755]\n",
      "epoch:43 step:34326 [D loss: 0.002758, acc.: 100.00%] [G loss: 0.000516]\n",
      "epoch:43 step:34327 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.556082]\n",
      "epoch:43 step:34328 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.005434]\n",
      "epoch:43 step:34329 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.012128]\n",
      "epoch:43 step:34330 [D loss: 0.001414, acc.: 100.00%] [G loss: 0.000499]\n",
      "epoch:43 step:34331 [D loss: 0.006692, acc.: 100.00%] [G loss: 0.000925]\n",
      "epoch:43 step:34332 [D loss: 0.001516, acc.: 100.00%] [G loss: 0.001043]\n",
      "epoch:43 step:34333 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000829]\n",
      "epoch:43 step:34334 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.007182]\n",
      "epoch:43 step:34335 [D loss: 0.003397, acc.: 100.00%] [G loss: 0.000343]\n",
      "epoch:43 step:34336 [D loss: 0.001493, acc.: 100.00%] [G loss: 0.007026]\n",
      "epoch:43 step:34337 [D loss: 0.001501, acc.: 100.00%] [G loss: 0.000305]\n",
      "epoch:43 step:34338 [D loss: 0.000845, acc.: 100.00%] [G loss: 0.001024]\n",
      "epoch:43 step:34339 [D loss: 0.003377, acc.: 100.00%] [G loss: 0.030573]\n",
      "epoch:43 step:34340 [D loss: 0.003792, acc.: 100.00%] [G loss: 0.012981]\n",
      "epoch:43 step:34341 [D loss: 0.009685, acc.: 100.00%] [G loss: 0.007563]\n",
      "epoch:43 step:34342 [D loss: 0.002604, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:43 step:34343 [D loss: 0.004492, acc.: 100.00%] [G loss: 0.000367]\n",
      "epoch:43 step:34344 [D loss: 0.000313, acc.: 100.00%] [G loss: 0.035139]\n",
      "epoch:43 step:34345 [D loss: 0.001588, acc.: 100.00%] [G loss: 0.227242]\n",
      "epoch:43 step:34346 [D loss: 0.011691, acc.: 100.00%] [G loss: 0.001516]\n",
      "epoch:43 step:34347 [D loss: 0.005243, acc.: 100.00%] [G loss: 0.000801]\n",
      "epoch:43 step:34348 [D loss: 0.000679, acc.: 100.00%] [G loss: 0.003902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:43 step:34349 [D loss: 0.002886, acc.: 100.00%] [G loss: 0.001406]\n",
      "epoch:43 step:34350 [D loss: 0.000576, acc.: 100.00%] [G loss: 0.001899]\n",
      "epoch:43 step:34351 [D loss: 0.011673, acc.: 100.00%] [G loss: 0.003576]\n",
      "epoch:43 step:34352 [D loss: 0.016028, acc.: 100.00%] [G loss: 0.007265]\n",
      "epoch:43 step:34353 [D loss: 0.001707, acc.: 100.00%] [G loss: 0.020171]\n",
      "epoch:43 step:34354 [D loss: 0.052236, acc.: 100.00%] [G loss: 0.002111]\n",
      "epoch:43 step:34355 [D loss: 0.352325, acc.: 82.81%] [G loss: 5.313212]\n",
      "epoch:43 step:34356 [D loss: 0.122050, acc.: 96.09%] [G loss: 3.418825]\n",
      "epoch:43 step:34357 [D loss: 0.269559, acc.: 87.50%] [G loss: 0.154870]\n",
      "epoch:43 step:34358 [D loss: 0.014702, acc.: 99.22%] [G loss: 1.384219]\n",
      "epoch:43 step:34359 [D loss: 0.049929, acc.: 98.44%] [G loss: 0.685308]\n",
      "epoch:43 step:34360 [D loss: 0.027441, acc.: 100.00%] [G loss: 0.429798]\n",
      "epoch:43 step:34361 [D loss: 0.006857, acc.: 100.00%] [G loss: 0.228940]\n",
      "epoch:43 step:34362 [D loss: 0.005454, acc.: 100.00%] [G loss: 0.232563]\n",
      "epoch:43 step:34363 [D loss: 0.002533, acc.: 100.00%] [G loss: 0.316484]\n",
      "epoch:43 step:34364 [D loss: 0.010885, acc.: 100.00%] [G loss: 0.020100]\n",
      "epoch:44 step:34365 [D loss: 0.006497, acc.: 100.00%] [G loss: 0.129353]\n",
      "epoch:44 step:34366 [D loss: 0.011416, acc.: 99.22%] [G loss: 0.177553]\n",
      "epoch:44 step:34367 [D loss: 0.007503, acc.: 100.00%] [G loss: 0.001796]\n",
      "epoch:44 step:34368 [D loss: 0.001335, acc.: 100.00%] [G loss: 0.002571]\n",
      "epoch:44 step:34369 [D loss: 0.000679, acc.: 100.00%] [G loss: 0.001338]\n",
      "epoch:44 step:34370 [D loss: 0.016470, acc.: 100.00%] [G loss: 0.004482]\n",
      "epoch:44 step:34371 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.004182]\n",
      "epoch:44 step:34372 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.001504]\n",
      "epoch:44 step:34373 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:44 step:34374 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000596]\n",
      "epoch:44 step:34375 [D loss: 0.000658, acc.: 100.00%] [G loss: 0.315749]\n",
      "epoch:44 step:34376 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000627]\n",
      "epoch:44 step:34377 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.001989]\n",
      "epoch:44 step:34378 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000494]\n",
      "epoch:44 step:34379 [D loss: 0.000920, acc.: 100.00%] [G loss: 0.000893]\n",
      "epoch:44 step:34380 [D loss: 0.000633, acc.: 100.00%] [G loss: 0.000909]\n",
      "epoch:44 step:34381 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000287]\n",
      "epoch:44 step:34382 [D loss: 0.004231, acc.: 100.00%] [G loss: 0.000345]\n",
      "epoch:44 step:34383 [D loss: 0.001635, acc.: 100.00%] [G loss: 0.001327]\n",
      "epoch:44 step:34384 [D loss: 0.008301, acc.: 100.00%] [G loss: 0.001005]\n",
      "epoch:44 step:34385 [D loss: 0.072911, acc.: 98.44%] [G loss: 0.014621]\n",
      "epoch:44 step:34386 [D loss: 0.024621, acc.: 99.22%] [G loss: 0.095378]\n",
      "epoch:44 step:34387 [D loss: 0.005258, acc.: 100.00%] [G loss: 0.200524]\n",
      "epoch:44 step:34388 [D loss: 0.017575, acc.: 100.00%] [G loss: 0.072387]\n",
      "epoch:44 step:34389 [D loss: 0.013161, acc.: 100.00%] [G loss: 0.034838]\n",
      "epoch:44 step:34390 [D loss: 0.011893, acc.: 100.00%] [G loss: 0.028205]\n",
      "epoch:44 step:34391 [D loss: 0.003075, acc.: 100.00%] [G loss: 0.014406]\n",
      "epoch:44 step:34392 [D loss: 0.024613, acc.: 99.22%] [G loss: 0.010572]\n",
      "epoch:44 step:34393 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.000861]\n",
      "epoch:44 step:34394 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.008598]\n",
      "epoch:44 step:34395 [D loss: 0.000609, acc.: 100.00%] [G loss: 0.008966]\n",
      "epoch:44 step:34396 [D loss: 0.003737, acc.: 100.00%] [G loss: 0.008500]\n",
      "epoch:44 step:34397 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.001803]\n",
      "epoch:44 step:34398 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.001194]\n",
      "epoch:44 step:34399 [D loss: 0.003834, acc.: 100.00%] [G loss: 0.002063]\n",
      "epoch:44 step:34400 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.007581]\n",
      "epoch:44 step:34401 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:44 step:34402 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.001513]\n",
      "epoch:44 step:34403 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:44 step:34404 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.001565]\n",
      "epoch:44 step:34405 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000288]\n",
      "epoch:44 step:34406 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.001983]\n",
      "epoch:44 step:34407 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:44 step:34408 [D loss: 0.000318, acc.: 100.00%] [G loss: 0.007306]\n",
      "epoch:44 step:34409 [D loss: 0.000911, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:44 step:34410 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.001396]\n",
      "epoch:44 step:34411 [D loss: 0.001253, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:44 step:34412 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000472]\n",
      "epoch:44 step:34413 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000536]\n",
      "epoch:44 step:34414 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.000920]\n",
      "epoch:44 step:34415 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:44 step:34416 [D loss: 0.000818, acc.: 100.00%] [G loss: 0.003366]\n",
      "epoch:44 step:34417 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000436]\n",
      "epoch:44 step:34418 [D loss: 0.003300, acc.: 100.00%] [G loss: 0.001569]\n",
      "epoch:44 step:34419 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:44 step:34420 [D loss: 0.002907, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:44 step:34421 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.001120]\n",
      "epoch:44 step:34422 [D loss: 0.001056, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:44 step:34423 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:44 step:34424 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:44 step:34425 [D loss: 0.004703, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:44 step:34426 [D loss: 0.151438, acc.: 93.75%] [G loss: 1.152004]\n",
      "epoch:44 step:34427 [D loss: 0.004970, acc.: 100.00%] [G loss: 1.542236]\n",
      "epoch:44 step:34428 [D loss: 0.408096, acc.: 83.59%] [G loss: 0.000005]\n",
      "epoch:44 step:34429 [D loss: 0.235832, acc.: 89.06%] [G loss: 0.005680]\n",
      "epoch:44 step:34430 [D loss: 0.000310, acc.: 100.00%] [G loss: 3.400420]\n",
      "epoch:44 step:34431 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.184960]\n",
      "epoch:44 step:34432 [D loss: 0.036042, acc.: 100.00%] [G loss: 1.244313]\n",
      "epoch:44 step:34433 [D loss: 0.000792, acc.: 100.00%] [G loss: 0.017245]\n",
      "epoch:44 step:34434 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.671365]\n",
      "epoch:44 step:34435 [D loss: 0.007070, acc.: 99.22%] [G loss: 0.006027]\n",
      "epoch:44 step:34436 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.180565]\n",
      "epoch:44 step:34437 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.085535]\n",
      "epoch:44 step:34438 [D loss: 0.001560, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:44 step:34439 [D loss: 0.001104, acc.: 100.00%] [G loss: 0.051706]\n",
      "epoch:44 step:34440 [D loss: 0.000891, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:44 step:34441 [D loss: 0.050602, acc.: 99.22%] [G loss: 0.000069]\n",
      "epoch:44 step:34442 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.001173]\n",
      "epoch:44 step:34443 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.009253]\n",
      "epoch:44 step:34444 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.009443]\n",
      "epoch:44 step:34445 [D loss: 0.002290, acc.: 100.00%] [G loss: 0.002213]\n",
      "epoch:44 step:34446 [D loss: 0.001409, acc.: 100.00%] [G loss: 0.001829]\n",
      "epoch:44 step:34447 [D loss: 0.002150, acc.: 100.00%] [G loss: 0.001797]\n",
      "epoch:44 step:34448 [D loss: 0.018997, acc.: 99.22%] [G loss: 0.009872]\n",
      "epoch:44 step:34449 [D loss: 0.000502, acc.: 100.00%] [G loss: 0.002841]\n",
      "epoch:44 step:34450 [D loss: 0.001805, acc.: 100.00%] [G loss: 0.000348]\n",
      "epoch:44 step:34451 [D loss: 0.051147, acc.: 99.22%] [G loss: 0.352555]\n",
      "epoch:44 step:34452 [D loss: 0.000537, acc.: 100.00%] [G loss: 0.049842]\n",
      "epoch:44 step:34453 [D loss: 0.007476, acc.: 100.00%] [G loss: 0.351869]\n",
      "epoch:44 step:34454 [D loss: 0.270966, acc.: 89.06%] [G loss: 0.000006]\n",
      "epoch:44 step:34455 [D loss: 0.025117, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:34456 [D loss: 1.704329, acc.: 51.56%] [G loss: 5.896908]\n",
      "epoch:44 step:34457 [D loss: 1.102884, acc.: 60.16%] [G loss: 2.414790]\n",
      "epoch:44 step:34458 [D loss: 0.081562, acc.: 95.31%] [G loss: 0.316598]\n",
      "epoch:44 step:34459 [D loss: 0.005102, acc.: 100.00%] [G loss: 3.243430]\n",
      "epoch:44 step:34460 [D loss: 0.026229, acc.: 98.44%] [G loss: 2.012378]\n",
      "epoch:44 step:34461 [D loss: 0.015437, acc.: 100.00%] [G loss: 0.752548]\n",
      "epoch:44 step:34462 [D loss: 0.004469, acc.: 100.00%] [G loss: 0.152119]\n",
      "epoch:44 step:34463 [D loss: 0.023782, acc.: 99.22%] [G loss: 0.027298]\n",
      "epoch:44 step:34464 [D loss: 0.006246, acc.: 100.00%] [G loss: 0.007035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34465 [D loss: 0.004537, acc.: 100.00%] [G loss: 0.003931]\n",
      "epoch:44 step:34466 [D loss: 0.002216, acc.: 100.00%] [G loss: 0.003780]\n",
      "epoch:44 step:34467 [D loss: 0.147802, acc.: 96.09%] [G loss: 0.111327]\n",
      "epoch:44 step:34468 [D loss: 0.004835, acc.: 100.00%] [G loss: 0.701129]\n",
      "epoch:44 step:34469 [D loss: 0.013416, acc.: 100.00%] [G loss: 0.113184]\n",
      "epoch:44 step:34470 [D loss: 0.006898, acc.: 100.00%] [G loss: 0.012098]\n",
      "epoch:44 step:34471 [D loss: 0.013700, acc.: 99.22%] [G loss: 0.007554]\n",
      "epoch:44 step:34472 [D loss: 0.036129, acc.: 99.22%] [G loss: 0.004100]\n",
      "epoch:44 step:34473 [D loss: 0.000897, acc.: 100.00%] [G loss: 0.006120]\n",
      "epoch:44 step:34474 [D loss: 0.001521, acc.: 100.00%] [G loss: 0.004662]\n",
      "epoch:44 step:34475 [D loss: 0.015950, acc.: 99.22%] [G loss: 0.006705]\n",
      "epoch:44 step:34476 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.002090]\n",
      "epoch:44 step:34477 [D loss: 0.001501, acc.: 100.00%] [G loss: 0.003713]\n",
      "epoch:44 step:34478 [D loss: 0.002371, acc.: 100.00%] [G loss: 0.003330]\n",
      "epoch:44 step:34479 [D loss: 0.003994, acc.: 100.00%] [G loss: 0.004142]\n",
      "epoch:44 step:34480 [D loss: 0.003454, acc.: 100.00%] [G loss: 0.002790]\n",
      "epoch:44 step:34481 [D loss: 0.000412, acc.: 100.00%] [G loss: 0.007635]\n",
      "epoch:44 step:34482 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000800]\n",
      "epoch:44 step:34483 [D loss: 0.001418, acc.: 100.00%] [G loss: 0.000861]\n",
      "epoch:44 step:34484 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.003411]\n",
      "epoch:44 step:34485 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.002156]\n",
      "epoch:44 step:34486 [D loss: 0.000732, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:44 step:34487 [D loss: 0.006345, acc.: 100.00%] [G loss: 0.006464]\n",
      "epoch:44 step:34488 [D loss: 0.003514, acc.: 100.00%] [G loss: 0.000668]\n",
      "epoch:44 step:34489 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.009455]\n",
      "epoch:44 step:34490 [D loss: 0.000989, acc.: 100.00%] [G loss: 0.001633]\n",
      "epoch:44 step:34491 [D loss: 0.001771, acc.: 100.00%] [G loss: 0.000826]\n",
      "epoch:44 step:34492 [D loss: 0.011169, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:44 step:34493 [D loss: 0.000887, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:44 step:34494 [D loss: 0.003227, acc.: 100.00%] [G loss: 0.428788]\n",
      "epoch:44 step:34495 [D loss: 0.036480, acc.: 99.22%] [G loss: 0.042963]\n",
      "epoch:44 step:34496 [D loss: 0.007633, acc.: 100.00%] [G loss: 0.010870]\n",
      "epoch:44 step:34497 [D loss: 0.004445, acc.: 100.00%] [G loss: 0.000847]\n",
      "epoch:44 step:34498 [D loss: 0.002635, acc.: 100.00%] [G loss: 0.004796]\n",
      "epoch:44 step:34499 [D loss: 0.021728, acc.: 99.22%] [G loss: 0.006480]\n",
      "epoch:44 step:34500 [D loss: 0.002148, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:44 step:34501 [D loss: 0.000728, acc.: 100.00%] [G loss: 0.000938]\n",
      "epoch:44 step:34502 [D loss: 0.002220, acc.: 100.00%] [G loss: 0.109086]\n",
      "epoch:44 step:34503 [D loss: 0.000836, acc.: 100.00%] [G loss: 0.005820]\n",
      "epoch:44 step:34504 [D loss: 0.013001, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:44 step:34505 [D loss: 0.002434, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:44 step:34506 [D loss: 0.019710, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:44 step:34507 [D loss: 0.002013, acc.: 100.00%] [G loss: 0.005067]\n",
      "epoch:44 step:34508 [D loss: 0.003453, acc.: 100.00%] [G loss: 0.038107]\n",
      "epoch:44 step:34509 [D loss: 0.002830, acc.: 100.00%] [G loss: 0.008844]\n",
      "epoch:44 step:34510 [D loss: 0.001493, acc.: 100.00%] [G loss: 0.002518]\n",
      "epoch:44 step:34511 [D loss: 0.002293, acc.: 100.00%] [G loss: 0.002556]\n",
      "epoch:44 step:34512 [D loss: 0.003741, acc.: 100.00%] [G loss: 0.014781]\n",
      "epoch:44 step:34513 [D loss: 0.002140, acc.: 100.00%] [G loss: 0.000912]\n",
      "epoch:44 step:34514 [D loss: 0.003061, acc.: 100.00%] [G loss: 0.003637]\n",
      "epoch:44 step:34515 [D loss: 0.158961, acc.: 92.97%] [G loss: 0.092819]\n",
      "epoch:44 step:34516 [D loss: 0.021184, acc.: 100.00%] [G loss: 1.078790]\n",
      "epoch:44 step:34517 [D loss: 0.329839, acc.: 85.16%] [G loss: 0.001037]\n",
      "epoch:44 step:34518 [D loss: 0.333529, acc.: 88.28%] [G loss: 2.153535]\n",
      "epoch:44 step:34519 [D loss: 0.004978, acc.: 100.00%] [G loss: 2.844712]\n",
      "epoch:44 step:34520 [D loss: 0.264193, acc.: 90.62%] [G loss: 0.763260]\n",
      "epoch:44 step:34521 [D loss: 0.005328, acc.: 100.00%] [G loss: 0.405559]\n",
      "epoch:44 step:34522 [D loss: 0.029039, acc.: 99.22%] [G loss: 0.027225]\n",
      "epoch:44 step:34523 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.716617]\n",
      "epoch:44 step:34524 [D loss: 0.078066, acc.: 96.09%] [G loss: 0.000446]\n",
      "epoch:44 step:34525 [D loss: 0.003127, acc.: 100.00%] [G loss: 0.001435]\n",
      "epoch:44 step:34526 [D loss: 0.010031, acc.: 100.00%] [G loss: 0.000347]\n",
      "epoch:44 step:34527 [D loss: 0.009002, acc.: 100.00%] [G loss: 0.001258]\n",
      "epoch:44 step:34528 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.001213]\n",
      "epoch:44 step:34529 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:44 step:34530 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.000827]\n",
      "epoch:44 step:34531 [D loss: 0.014553, acc.: 100.00%] [G loss: 0.002054]\n",
      "epoch:44 step:34532 [D loss: 0.004540, acc.: 100.00%] [G loss: 0.003852]\n",
      "epoch:44 step:34533 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.016531]\n",
      "epoch:44 step:34534 [D loss: 0.000950, acc.: 100.00%] [G loss: 0.001339]\n",
      "epoch:44 step:34535 [D loss: 0.000624, acc.: 100.00%] [G loss: 0.005169]\n",
      "epoch:44 step:34536 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000799]\n",
      "epoch:44 step:34537 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.003081]\n",
      "epoch:44 step:34538 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.050878]\n",
      "epoch:44 step:34539 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.002662]\n",
      "epoch:44 step:34540 [D loss: 0.000511, acc.: 100.00%] [G loss: 0.119139]\n",
      "epoch:44 step:34541 [D loss: 0.000142, acc.: 100.00%] [G loss: 0.001531]\n",
      "epoch:44 step:34542 [D loss: 0.008097, acc.: 100.00%] [G loss: 0.001637]\n",
      "epoch:44 step:34543 [D loss: 0.000998, acc.: 100.00%] [G loss: 0.002988]\n",
      "epoch:44 step:34544 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.000878]\n",
      "epoch:44 step:34545 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.001728]\n",
      "epoch:44 step:34546 [D loss: 0.002085, acc.: 100.00%] [G loss: 0.000653]\n",
      "epoch:44 step:34547 [D loss: 0.014289, acc.: 100.00%] [G loss: 0.000593]\n",
      "epoch:44 step:34548 [D loss: 0.005949, acc.: 100.00%] [G loss: 0.010049]\n",
      "epoch:44 step:34549 [D loss: 0.004225, acc.: 100.00%] [G loss: 0.002631]\n",
      "epoch:44 step:34550 [D loss: 0.000583, acc.: 100.00%] [G loss: 0.013631]\n",
      "epoch:44 step:34551 [D loss: 0.023792, acc.: 100.00%] [G loss: 0.001162]\n",
      "epoch:44 step:34552 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000593]\n",
      "epoch:44 step:34553 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:44 step:34554 [D loss: 0.000853, acc.: 100.00%] [G loss: 0.018271]\n",
      "epoch:44 step:34555 [D loss: 0.000467, acc.: 100.00%] [G loss: 0.000607]\n",
      "epoch:44 step:34556 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.006249]\n",
      "epoch:44 step:34557 [D loss: 0.000913, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:44 step:34558 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.024219]\n",
      "epoch:44 step:34559 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:44 step:34560 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000585]\n",
      "epoch:44 step:34561 [D loss: 0.000924, acc.: 100.00%] [G loss: 0.000703]\n",
      "epoch:44 step:34562 [D loss: 0.004511, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:44 step:34563 [D loss: 0.006238, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:44 step:34564 [D loss: 0.002780, acc.: 100.00%] [G loss: 0.071865]\n",
      "epoch:44 step:34565 [D loss: 0.016710, acc.: 100.00%] [G loss: 0.000395]\n",
      "epoch:44 step:34566 [D loss: 0.000497, acc.: 100.00%] [G loss: 0.003910]\n",
      "epoch:44 step:34567 [D loss: 0.000851, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:44 step:34568 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.001126]\n",
      "epoch:44 step:34569 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.000913]\n",
      "epoch:44 step:34570 [D loss: 0.011413, acc.: 100.00%] [G loss: 0.000248]\n",
      "epoch:44 step:34571 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.002361]\n",
      "epoch:44 step:34572 [D loss: 0.002371, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:44 step:34573 [D loss: 0.017541, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:44 step:34574 [D loss: 0.016919, acc.: 99.22%] [G loss: 0.006017]\n",
      "epoch:44 step:34575 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.000474]\n",
      "epoch:44 step:34576 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:44 step:34577 [D loss: 0.002739, acc.: 100.00%] [G loss: 0.002880]\n",
      "epoch:44 step:34578 [D loss: 0.016231, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:44 step:34579 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:44 step:34580 [D loss: 0.001239, acc.: 100.00%] [G loss: 0.000197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34581 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.000728]\n",
      "epoch:44 step:34582 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:44 step:34583 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.001552]\n",
      "epoch:44 step:34584 [D loss: 0.005972, acc.: 100.00%] [G loss: 0.001051]\n",
      "epoch:44 step:34585 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:44 step:34586 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:44 step:34587 [D loss: 0.000976, acc.: 100.00%] [G loss: 0.002796]\n",
      "epoch:44 step:34588 [D loss: 0.000460, acc.: 100.00%] [G loss: 0.001416]\n",
      "epoch:44 step:34589 [D loss: 0.004670, acc.: 100.00%] [G loss: 0.001772]\n",
      "epoch:44 step:34590 [D loss: 0.000727, acc.: 100.00%] [G loss: 0.002649]\n",
      "epoch:44 step:34591 [D loss: 0.001650, acc.: 100.00%] [G loss: 0.001289]\n",
      "epoch:44 step:34592 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.001652]\n",
      "epoch:44 step:34593 [D loss: 0.003724, acc.: 100.00%] [G loss: 0.001549]\n",
      "epoch:44 step:34594 [D loss: 0.042310, acc.: 99.22%] [G loss: 0.000050]\n",
      "epoch:44 step:34595 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:44 step:34596 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.000845]\n",
      "epoch:44 step:34597 [D loss: 0.002504, acc.: 100.00%] [G loss: 0.000227]\n",
      "epoch:44 step:34598 [D loss: 0.003271, acc.: 100.00%] [G loss: 0.000689]\n",
      "epoch:44 step:34599 [D loss: 0.000664, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:44 step:34600 [D loss: 0.005423, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:44 step:34601 [D loss: 0.004267, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:44 step:34602 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:44 step:34603 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000249]\n",
      "epoch:44 step:34604 [D loss: 0.006359, acc.: 100.00%] [G loss: 0.000403]\n",
      "epoch:44 step:34605 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:44 step:34606 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000422]\n",
      "epoch:44 step:34607 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:44 step:34608 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.002384]\n",
      "epoch:44 step:34609 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000569]\n",
      "epoch:44 step:34610 [D loss: 0.000728, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:44 step:34611 [D loss: 0.001003, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:44 step:34612 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000578]\n",
      "epoch:44 step:34613 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000380]\n",
      "epoch:44 step:34614 [D loss: 0.000496, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:44 step:34615 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.008227]\n",
      "epoch:44 step:34616 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:44 step:34617 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:44 step:34618 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:44 step:34619 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:44 step:34620 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:44 step:34621 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000778]\n",
      "epoch:44 step:34622 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:44 step:34623 [D loss: 0.000304, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:44 step:34624 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:44 step:34625 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:44 step:34626 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:44 step:34627 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.001129]\n",
      "epoch:44 step:34628 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:44 step:34629 [D loss: 0.000551, acc.: 100.00%] [G loss: 0.000277]\n",
      "epoch:44 step:34630 [D loss: 0.000448, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:44 step:34631 [D loss: 0.053038, acc.: 97.66%] [G loss: 0.006180]\n",
      "epoch:44 step:34632 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.011594]\n",
      "epoch:44 step:34633 [D loss: 0.011785, acc.: 100.00%] [G loss: 0.032798]\n",
      "epoch:44 step:34634 [D loss: 0.010188, acc.: 100.00%] [G loss: 0.072535]\n",
      "epoch:44 step:34635 [D loss: 0.006692, acc.: 100.00%] [G loss: 2.941294]\n",
      "epoch:44 step:34636 [D loss: 0.014168, acc.: 100.00%] [G loss: 0.133910]\n",
      "epoch:44 step:34637 [D loss: 0.003417, acc.: 100.00%] [G loss: 0.532540]\n",
      "epoch:44 step:34638 [D loss: 0.022135, acc.: 100.00%] [G loss: 0.846938]\n",
      "epoch:44 step:34639 [D loss: 0.125243, acc.: 95.31%] [G loss: 0.012349]\n",
      "epoch:44 step:34640 [D loss: 0.050220, acc.: 99.22%] [G loss: 0.005995]\n",
      "epoch:44 step:34641 [D loss: 0.024320, acc.: 100.00%] [G loss: 0.017247]\n",
      "epoch:44 step:34642 [D loss: 0.001855, acc.: 100.00%] [G loss: 0.528250]\n",
      "epoch:44 step:34643 [D loss: 0.008976, acc.: 100.00%] [G loss: 0.008608]\n",
      "epoch:44 step:34644 [D loss: 0.006687, acc.: 100.00%] [G loss: 0.127211]\n",
      "epoch:44 step:34645 [D loss: 0.144593, acc.: 96.09%] [G loss: 4.453890]\n",
      "epoch:44 step:34646 [D loss: 1.436025, acc.: 46.88%] [G loss: 0.659109]\n",
      "epoch:44 step:34647 [D loss: 0.001291, acc.: 100.00%] [G loss: 0.874477]\n",
      "epoch:44 step:34648 [D loss: 0.000855, acc.: 100.00%] [G loss: 1.508881]\n",
      "epoch:44 step:34649 [D loss: 0.001635, acc.: 100.00%] [G loss: 0.665551]\n",
      "epoch:44 step:34650 [D loss: 0.011630, acc.: 100.00%] [G loss: 0.576709]\n",
      "epoch:44 step:34651 [D loss: 0.022338, acc.: 100.00%] [G loss: 0.768646]\n",
      "epoch:44 step:34652 [D loss: 0.008609, acc.: 100.00%] [G loss: 0.253640]\n",
      "epoch:44 step:34653 [D loss: 0.002036, acc.: 100.00%] [G loss: 0.598068]\n",
      "epoch:44 step:34654 [D loss: 0.001516, acc.: 100.00%] [G loss: 0.107702]\n",
      "epoch:44 step:34655 [D loss: 0.007007, acc.: 100.00%] [G loss: 0.050594]\n",
      "epoch:44 step:34656 [D loss: 0.000647, acc.: 100.00%] [G loss: 0.007129]\n",
      "epoch:44 step:34657 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.005531]\n",
      "epoch:44 step:34658 [D loss: 0.001015, acc.: 100.00%] [G loss: 0.001587]\n",
      "epoch:44 step:34659 [D loss: 0.000801, acc.: 100.00%] [G loss: 0.004014]\n",
      "epoch:44 step:34660 [D loss: 0.000408, acc.: 100.00%] [G loss: 0.009660]\n",
      "epoch:44 step:34661 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.001309]\n",
      "epoch:44 step:34662 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.005760]\n",
      "epoch:44 step:34663 [D loss: 0.001334, acc.: 100.00%] [G loss: 0.001919]\n",
      "epoch:44 step:34664 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.000405]\n",
      "epoch:44 step:34665 [D loss: 0.001146, acc.: 100.00%] [G loss: 0.050332]\n",
      "epoch:44 step:34666 [D loss: 0.000193, acc.: 100.00%] [G loss: 3.034446]\n",
      "epoch:44 step:34667 [D loss: 0.000834, acc.: 100.00%] [G loss: 0.006564]\n",
      "epoch:44 step:34668 [D loss: 0.000778, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:44 step:34669 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.005251]\n",
      "epoch:44 step:34670 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.004539]\n",
      "epoch:44 step:34671 [D loss: 0.004448, acc.: 100.00%] [G loss: 0.012311]\n",
      "epoch:44 step:34672 [D loss: 0.005400, acc.: 100.00%] [G loss: 0.006605]\n",
      "epoch:44 step:34673 [D loss: 0.016676, acc.: 100.00%] [G loss: 0.019241]\n",
      "epoch:44 step:34674 [D loss: 0.006962, acc.: 100.00%] [G loss: 0.022164]\n",
      "epoch:44 step:34675 [D loss: 0.002703, acc.: 100.00%] [G loss: 0.006407]\n",
      "epoch:44 step:34676 [D loss: 0.001200, acc.: 100.00%] [G loss: 0.002122]\n",
      "epoch:44 step:34677 [D loss: 0.003925, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:44 step:34678 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.017793]\n",
      "epoch:44 step:34679 [D loss: 0.002018, acc.: 100.00%] [G loss: 0.015580]\n",
      "epoch:44 step:34680 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.000318]\n",
      "epoch:44 step:34681 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.000375]\n",
      "epoch:44 step:34682 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.354613]\n",
      "epoch:44 step:34683 [D loss: 0.000813, acc.: 100.00%] [G loss: 0.027019]\n",
      "epoch:44 step:34684 [D loss: 0.006962, acc.: 100.00%] [G loss: 0.001232]\n",
      "epoch:44 step:34685 [D loss: 0.000414, acc.: 100.00%] [G loss: 0.297664]\n",
      "epoch:44 step:34686 [D loss: 0.007609, acc.: 100.00%] [G loss: 0.000486]\n",
      "epoch:44 step:34687 [D loss: 0.000588, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:44 step:34688 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000557]\n",
      "epoch:44 step:34689 [D loss: 0.010881, acc.: 99.22%] [G loss: 0.004171]\n",
      "epoch:44 step:34690 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:44 step:34691 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.001305]\n",
      "epoch:44 step:34692 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000588]\n",
      "epoch:44 step:34693 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:44 step:34694 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:44 step:34695 [D loss: 0.000848, acc.: 100.00%] [G loss: 0.000382]\n",
      "epoch:44 step:34696 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34697 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.001145]\n",
      "epoch:44 step:34698 [D loss: 0.000363, acc.: 100.00%] [G loss: 0.000784]\n",
      "epoch:44 step:34699 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:44 step:34700 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.001835]\n",
      "epoch:44 step:34701 [D loss: 0.002559, acc.: 100.00%] [G loss: 0.004268]\n",
      "epoch:44 step:34702 [D loss: 0.003685, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:44 step:34703 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:44 step:34704 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.000199]\n",
      "epoch:44 step:34705 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.002989]\n",
      "epoch:44 step:34706 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.000321]\n",
      "epoch:44 step:34707 [D loss: 0.006923, acc.: 100.00%] [G loss: 0.002790]\n",
      "epoch:44 step:34708 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.000622]\n",
      "epoch:44 step:34709 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:44 step:34710 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.002976]\n",
      "epoch:44 step:34711 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.006491]\n",
      "epoch:44 step:34712 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:44 step:34713 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000568]\n",
      "epoch:44 step:34714 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.006109]\n",
      "epoch:44 step:34715 [D loss: 0.007456, acc.: 100.00%] [G loss: 0.000300]\n",
      "epoch:44 step:34716 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.008838]\n",
      "epoch:44 step:34717 [D loss: 0.001637, acc.: 100.00%] [G loss: 0.000830]\n",
      "epoch:44 step:34718 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:44 step:34719 [D loss: 0.001001, acc.: 100.00%] [G loss: 0.000344]\n",
      "epoch:44 step:34720 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:44 step:34721 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.000584]\n",
      "epoch:44 step:34722 [D loss: 0.000549, acc.: 100.00%] [G loss: 0.025405]\n",
      "epoch:44 step:34723 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:44 step:34724 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.111739]\n",
      "epoch:44 step:34725 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.013679]\n",
      "epoch:44 step:34726 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:44 step:34727 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000577]\n",
      "epoch:44 step:34728 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:44 step:34729 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:44 step:34730 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:44 step:34731 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.031630]\n",
      "epoch:44 step:34732 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.001178]\n",
      "epoch:44 step:34733 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.002775]\n",
      "epoch:44 step:34734 [D loss: 0.008286, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:44 step:34735 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.000530]\n",
      "epoch:44 step:34736 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000325]\n",
      "epoch:44 step:34737 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.000413]\n",
      "epoch:44 step:34738 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000388]\n",
      "epoch:44 step:34739 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.003834]\n",
      "epoch:44 step:34740 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.017079]\n",
      "epoch:44 step:34741 [D loss: 0.000721, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:44 step:34742 [D loss: 0.000732, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:44 step:34743 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.000939]\n",
      "epoch:44 step:34744 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000459]\n",
      "epoch:44 step:34745 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:44 step:34746 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000660]\n",
      "epoch:44 step:34747 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:44 step:34748 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:44 step:34749 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:44 step:34750 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:44 step:34751 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.001042]\n",
      "epoch:44 step:34752 [D loss: 0.004112, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:44 step:34753 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:44 step:34754 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.002131]\n",
      "epoch:44 step:34755 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:44 step:34756 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:44 step:34757 [D loss: 0.001063, acc.: 100.00%] [G loss: 0.000851]\n",
      "epoch:44 step:34758 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:44 step:34759 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:44 step:34760 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:34761 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000975]\n",
      "epoch:44 step:34762 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000962]\n",
      "epoch:44 step:34763 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:44 step:34764 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:44 step:34765 [D loss: 0.000528, acc.: 100.00%] [G loss: 0.035622]\n",
      "epoch:44 step:34766 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.000541]\n",
      "epoch:44 step:34767 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.002152]\n",
      "epoch:44 step:34768 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.000168]\n",
      "epoch:44 step:34769 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.000353]\n",
      "epoch:44 step:34770 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:44 step:34771 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:44 step:34772 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:44 step:34773 [D loss: 0.001531, acc.: 100.00%] [G loss: 0.000585]\n",
      "epoch:44 step:34774 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:44 step:34775 [D loss: 0.000561, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:44 step:34776 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:44 step:34777 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:44 step:34778 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:44 step:34779 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:44 step:34780 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.000693]\n",
      "epoch:44 step:34781 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:44 step:34782 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:44 step:34783 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:44 step:34784 [D loss: 0.000427, acc.: 100.00%] [G loss: 0.000607]\n",
      "epoch:44 step:34785 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:44 step:34786 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:44 step:34787 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.008516]\n",
      "epoch:44 step:34788 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:44 step:34789 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.000690]\n",
      "epoch:44 step:34790 [D loss: 0.000839, acc.: 100.00%] [G loss: 0.001480]\n",
      "epoch:44 step:34791 [D loss: 0.001565, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:44 step:34792 [D loss: 0.000455, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:44 step:34793 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:44 step:34794 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:44 step:34795 [D loss: 0.020613, acc.: 99.22%] [G loss: 0.000649]\n",
      "epoch:44 step:34796 [D loss: 0.001145, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:44 step:34797 [D loss: 0.003435, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:44 step:34798 [D loss: 0.176801, acc.: 92.19%] [G loss: 0.020155]\n",
      "epoch:44 step:34799 [D loss: 0.026348, acc.: 100.00%] [G loss: 6.035000]\n",
      "epoch:44 step:34800 [D loss: 0.295062, acc.: 85.16%] [G loss: 0.359661]\n",
      "epoch:44 step:34801 [D loss: 0.022790, acc.: 99.22%] [G loss: 0.000005]\n",
      "epoch:44 step:34802 [D loss: 0.002896, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:44 step:34803 [D loss: 0.003211, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:44 step:34804 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:44 step:34805 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.570583]\n",
      "epoch:44 step:34806 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000217]\n",
      "epoch:44 step:34807 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.001117]\n",
      "epoch:44 step:34808 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.295936]\n",
      "epoch:44 step:34809 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.000303]\n",
      "epoch:44 step:34810 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:44 step:34811 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.015813]\n",
      "epoch:44 step:34812 [D loss: 0.001564, acc.: 100.00%] [G loss: 0.011157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34813 [D loss: 0.009592, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:44 step:34814 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:44 step:34815 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:44 step:34816 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:44 step:34817 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.001212]\n",
      "epoch:44 step:34818 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:44 step:34819 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.010171]\n",
      "epoch:44 step:34820 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:44 step:34821 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:44 step:34822 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:44 step:34823 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.001865]\n",
      "epoch:44 step:34824 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000293]\n",
      "epoch:44 step:34825 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:44 step:34826 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.225228]\n",
      "epoch:44 step:34827 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.002270]\n",
      "epoch:44 step:34828 [D loss: 0.008680, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:44 step:34829 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000523]\n",
      "epoch:44 step:34830 [D loss: 0.001165, acc.: 100.00%] [G loss: 0.195712]\n",
      "epoch:44 step:34831 [D loss: 0.008741, acc.: 100.00%] [G loss: 0.000494]\n",
      "epoch:44 step:34832 [D loss: 0.001468, acc.: 100.00%] [G loss: 0.001053]\n",
      "epoch:44 step:34833 [D loss: 0.000472, acc.: 100.00%] [G loss: 0.000926]\n",
      "epoch:44 step:34834 [D loss: 0.026410, acc.: 100.00%] [G loss: 0.089848]\n",
      "epoch:44 step:34835 [D loss: 0.000445, acc.: 100.00%] [G loss: 0.030070]\n",
      "epoch:44 step:34836 [D loss: 0.001874, acc.: 100.00%] [G loss: 0.027890]\n",
      "epoch:44 step:34837 [D loss: 0.140106, acc.: 95.31%] [G loss: 2.146248]\n",
      "epoch:44 step:34838 [D loss: 0.009171, acc.: 100.00%] [G loss: 3.246528]\n",
      "epoch:44 step:34839 [D loss: 0.691590, acc.: 74.22%] [G loss: 4.432437]\n",
      "epoch:44 step:34840 [D loss: 0.011897, acc.: 100.00%] [G loss: 6.796251]\n",
      "epoch:44 step:34841 [D loss: 0.178308, acc.: 92.97%] [G loss: 3.200419]\n",
      "epoch:44 step:34842 [D loss: 0.078423, acc.: 98.44%] [G loss: 5.062838]\n",
      "epoch:44 step:34843 [D loss: 0.001271, acc.: 100.00%] [G loss: 3.149065]\n",
      "epoch:44 step:34844 [D loss: 0.000200, acc.: 100.00%] [G loss: 1.857216]\n",
      "epoch:44 step:34845 [D loss: 0.000920, acc.: 100.00%] [G loss: 1.085117]\n",
      "epoch:44 step:34846 [D loss: 0.065975, acc.: 96.09%] [G loss: 0.133182]\n",
      "epoch:44 step:34847 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.215490]\n",
      "epoch:44 step:34848 [D loss: 0.025217, acc.: 99.22%] [G loss: 0.124284]\n",
      "epoch:44 step:34849 [D loss: 0.004539, acc.: 100.00%] [G loss: 0.146843]\n",
      "epoch:44 step:34850 [D loss: 0.009605, acc.: 100.00%] [G loss: 0.219573]\n",
      "epoch:44 step:34851 [D loss: 0.058682, acc.: 99.22%] [G loss: 0.297559]\n",
      "epoch:44 step:34852 [D loss: 0.052237, acc.: 98.44%] [G loss: 1.608468]\n",
      "epoch:44 step:34853 [D loss: 0.145874, acc.: 95.31%] [G loss: 0.141102]\n",
      "epoch:44 step:34854 [D loss: 0.003693, acc.: 100.00%] [G loss: 0.028814]\n",
      "epoch:44 step:34855 [D loss: 0.001650, acc.: 100.00%] [G loss: 0.070079]\n",
      "epoch:44 step:34856 [D loss: 0.004144, acc.: 100.00%] [G loss: 0.011078]\n",
      "epoch:44 step:34857 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.009872]\n",
      "epoch:44 step:34858 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.005882]\n",
      "epoch:44 step:34859 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.004932]\n",
      "epoch:44 step:34860 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.010658]\n",
      "epoch:44 step:34861 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.007376]\n",
      "epoch:44 step:34862 [D loss: 0.007614, acc.: 100.00%] [G loss: 0.003630]\n",
      "epoch:44 step:34863 [D loss: 0.007213, acc.: 100.00%] [G loss: 0.003017]\n",
      "epoch:44 step:34864 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.004750]\n",
      "epoch:44 step:34865 [D loss: 0.001226, acc.: 100.00%] [G loss: 0.006795]\n",
      "epoch:44 step:34866 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.015939]\n",
      "epoch:44 step:34867 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.004098]\n",
      "epoch:44 step:34868 [D loss: 0.014433, acc.: 99.22%] [G loss: 0.012107]\n",
      "epoch:44 step:34869 [D loss: 0.003273, acc.: 100.00%] [G loss: 0.004368]\n",
      "epoch:44 step:34870 [D loss: 0.781577, acc.: 60.94%] [G loss: 6.684842]\n",
      "epoch:44 step:34871 [D loss: 2.778267, acc.: 50.00%] [G loss: 1.234752]\n",
      "epoch:44 step:34872 [D loss: 0.324379, acc.: 85.94%] [G loss: 0.346370]\n",
      "epoch:44 step:34873 [D loss: 0.037500, acc.: 99.22%] [G loss: 0.017944]\n",
      "epoch:44 step:34874 [D loss: 0.002714, acc.: 100.00%] [G loss: 0.025644]\n",
      "epoch:44 step:34875 [D loss: 0.008024, acc.: 100.00%] [G loss: 0.028625]\n",
      "epoch:44 step:34876 [D loss: 0.027117, acc.: 100.00%] [G loss: 0.010880]\n",
      "epoch:44 step:34877 [D loss: 0.012502, acc.: 100.00%] [G loss: 0.008430]\n",
      "epoch:44 step:34878 [D loss: 0.070749, acc.: 98.44%] [G loss: 0.000713]\n",
      "epoch:44 step:34879 [D loss: 0.004367, acc.: 100.00%] [G loss: 0.222861]\n",
      "epoch:44 step:34880 [D loss: 0.002473, acc.: 100.00%] [G loss: 0.173154]\n",
      "epoch:44 step:34881 [D loss: 0.008564, acc.: 100.00%] [G loss: 0.040079]\n",
      "epoch:44 step:34882 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.065777]\n",
      "epoch:44 step:34883 [D loss: 0.004569, acc.: 100.00%] [G loss: 0.023308]\n",
      "epoch:44 step:34884 [D loss: 0.003356, acc.: 100.00%] [G loss: 0.000761]\n",
      "epoch:44 step:34885 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.011024]\n",
      "epoch:44 step:34886 [D loss: 0.004780, acc.: 100.00%] [G loss: 0.023618]\n",
      "epoch:44 step:34887 [D loss: 0.002658, acc.: 100.00%] [G loss: 0.000686]\n",
      "epoch:44 step:34888 [D loss: 0.004333, acc.: 100.00%] [G loss: 0.017185]\n",
      "epoch:44 step:34889 [D loss: 0.001908, acc.: 100.00%] [G loss: 0.002749]\n",
      "epoch:44 step:34890 [D loss: 0.014069, acc.: 100.00%] [G loss: 0.030058]\n",
      "epoch:44 step:34891 [D loss: 0.004992, acc.: 100.00%] [G loss: 0.011619]\n",
      "epoch:44 step:34892 [D loss: 0.005318, acc.: 100.00%] [G loss: 0.251636]\n",
      "epoch:44 step:34893 [D loss: 0.002451, acc.: 100.00%] [G loss: 0.005773]\n",
      "epoch:44 step:34894 [D loss: 0.005849, acc.: 100.00%] [G loss: 0.008083]\n",
      "epoch:44 step:34895 [D loss: 0.019620, acc.: 100.00%] [G loss: 0.071377]\n",
      "epoch:44 step:34896 [D loss: 0.013804, acc.: 100.00%] [G loss: 0.006066]\n",
      "epoch:44 step:34897 [D loss: 0.004907, acc.: 100.00%] [G loss: 0.005519]\n",
      "epoch:44 step:34898 [D loss: 0.005458, acc.: 100.00%] [G loss: 0.017374]\n",
      "epoch:44 step:34899 [D loss: 0.016290, acc.: 100.00%] [G loss: 0.036613]\n",
      "epoch:44 step:34900 [D loss: 0.016511, acc.: 100.00%] [G loss: 0.018888]\n",
      "epoch:44 step:34901 [D loss: 0.026400, acc.: 100.00%] [G loss: 0.042950]\n",
      "epoch:44 step:34902 [D loss: 0.005563, acc.: 100.00%] [G loss: 0.044748]\n",
      "epoch:44 step:34903 [D loss: 0.124502, acc.: 97.66%] [G loss: 1.530042]\n",
      "epoch:44 step:34904 [D loss: 0.077535, acc.: 97.66%] [G loss: 1.142551]\n",
      "epoch:44 step:34905 [D loss: 0.115937, acc.: 98.44%] [G loss: 1.314385]\n",
      "epoch:44 step:34906 [D loss: 0.136649, acc.: 96.09%] [G loss: 0.076530]\n",
      "epoch:44 step:34907 [D loss: 0.019424, acc.: 100.00%] [G loss: 2.043909]\n",
      "epoch:44 step:34908 [D loss: 0.094172, acc.: 96.88%] [G loss: 2.957275]\n",
      "epoch:44 step:34909 [D loss: 0.034459, acc.: 98.44%] [G loss: 0.041902]\n",
      "epoch:44 step:34910 [D loss: 0.020910, acc.: 99.22%] [G loss: 0.039792]\n",
      "epoch:44 step:34911 [D loss: 0.097430, acc.: 95.31%] [G loss: 1.760876]\n",
      "epoch:44 step:34912 [D loss: 0.001161, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:44 step:34913 [D loss: 0.108529, acc.: 96.09%] [G loss: 1.483504]\n",
      "epoch:44 step:34914 [D loss: 0.065356, acc.: 96.88%] [G loss: 0.168608]\n",
      "epoch:44 step:34915 [D loss: 0.006476, acc.: 100.00%] [G loss: 0.216349]\n",
      "epoch:44 step:34916 [D loss: 0.002940, acc.: 100.00%] [G loss: 0.031614]\n",
      "epoch:44 step:34917 [D loss: 0.251732, acc.: 88.28%] [G loss: 1.450094]\n",
      "epoch:44 step:34918 [D loss: 0.120284, acc.: 95.31%] [G loss: 1.084462]\n",
      "epoch:44 step:34919 [D loss: 0.056076, acc.: 97.66%] [G loss: 0.252183]\n",
      "epoch:44 step:34920 [D loss: 0.002101, acc.: 100.00%] [G loss: 3.325686]\n",
      "epoch:44 step:34921 [D loss: 0.022814, acc.: 100.00%] [G loss: 0.009135]\n",
      "epoch:44 step:34922 [D loss: 0.020178, acc.: 99.22%] [G loss: 0.640726]\n",
      "epoch:44 step:34923 [D loss: 0.003045, acc.: 100.00%] [G loss: 0.000645]\n",
      "epoch:44 step:34924 [D loss: 0.002167, acc.: 100.00%] [G loss: 0.000791]\n",
      "epoch:44 step:34925 [D loss: 0.007687, acc.: 100.00%] [G loss: 0.039668]\n",
      "epoch:44 step:34926 [D loss: 0.013816, acc.: 100.00%] [G loss: 0.000346]\n",
      "epoch:44 step:34927 [D loss: 0.002143, acc.: 100.00%] [G loss: 0.000714]\n",
      "epoch:44 step:34928 [D loss: 0.001112, acc.: 100.00%] [G loss: 0.000392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:34929 [D loss: 0.003112, acc.: 100.00%] [G loss: 0.002649]\n",
      "epoch:44 step:34930 [D loss: 0.003231, acc.: 100.00%] [G loss: 0.112535]\n",
      "epoch:44 step:34931 [D loss: 0.012275, acc.: 99.22%] [G loss: 0.000153]\n",
      "epoch:44 step:34932 [D loss: 0.006371, acc.: 100.00%] [G loss: 0.096323]\n",
      "epoch:44 step:34933 [D loss: 0.000765, acc.: 100.00%] [G loss: 0.008081]\n",
      "epoch:44 step:34934 [D loss: 0.028609, acc.: 100.00%] [G loss: 0.014781]\n",
      "epoch:44 step:34935 [D loss: 0.008955, acc.: 100.00%] [G loss: 0.171752]\n",
      "epoch:44 step:34936 [D loss: 0.001914, acc.: 100.00%] [G loss: 0.163627]\n",
      "epoch:44 step:34937 [D loss: 0.019727, acc.: 100.00%] [G loss: 0.002977]\n",
      "epoch:44 step:34938 [D loss: 0.006524, acc.: 100.00%] [G loss: 0.116376]\n",
      "epoch:44 step:34939 [D loss: 0.020001, acc.: 100.00%] [G loss: 0.503414]\n",
      "epoch:44 step:34940 [D loss: 0.010511, acc.: 100.00%] [G loss: 0.001936]\n",
      "epoch:44 step:34941 [D loss: 0.017764, acc.: 100.00%] [G loss: 0.014132]\n",
      "epoch:44 step:34942 [D loss: 0.010805, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:44 step:34943 [D loss: 0.004831, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:44 step:34944 [D loss: 0.004304, acc.: 100.00%] [G loss: 0.024313]\n",
      "epoch:44 step:34945 [D loss: 0.003275, acc.: 100.00%] [G loss: 0.009234]\n",
      "epoch:44 step:34946 [D loss: 0.026411, acc.: 99.22%] [G loss: 0.128469]\n",
      "epoch:44 step:34947 [D loss: 0.005220, acc.: 100.00%] [G loss: 0.001575]\n",
      "epoch:44 step:34948 [D loss: 0.001607, acc.: 100.00%] [G loss: 0.004077]\n",
      "epoch:44 step:34949 [D loss: 0.001233, acc.: 100.00%] [G loss: 0.375548]\n",
      "epoch:44 step:34950 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.144195]\n",
      "epoch:44 step:34951 [D loss: 0.009629, acc.: 99.22%] [G loss: 0.000171]\n",
      "epoch:44 step:34952 [D loss: 0.000959, acc.: 100.00%] [G loss: 0.000100]\n",
      "epoch:44 step:34953 [D loss: 0.002289, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:44 step:34954 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.000610]\n",
      "epoch:44 step:34955 [D loss: 0.001028, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:44 step:34956 [D loss: 0.000826, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:44 step:34957 [D loss: 0.000830, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:44 step:34958 [D loss: 0.001539, acc.: 100.00%] [G loss: 0.008722]\n",
      "epoch:44 step:34959 [D loss: 0.000390, acc.: 100.00%] [G loss: 0.000118]\n",
      "epoch:44 step:34960 [D loss: 0.000952, acc.: 100.00%] [G loss: 0.000796]\n",
      "epoch:44 step:34961 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:44 step:34962 [D loss: 0.001037, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:44 step:34963 [D loss: 0.005365, acc.: 100.00%] [G loss: 0.000649]\n",
      "epoch:44 step:34964 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.006870]\n",
      "epoch:44 step:34965 [D loss: 0.000695, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:44 step:34966 [D loss: 0.000762, acc.: 100.00%] [G loss: 0.001542]\n",
      "epoch:44 step:34967 [D loss: 0.001696, acc.: 100.00%] [G loss: 0.000484]\n",
      "epoch:44 step:34968 [D loss: 0.013439, acc.: 99.22%] [G loss: 0.000112]\n",
      "epoch:44 step:34969 [D loss: 0.000284, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:44 step:34970 [D loss: 0.000668, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:44 step:34971 [D loss: 0.001360, acc.: 100.00%] [G loss: 0.002949]\n",
      "epoch:44 step:34972 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000622]\n",
      "epoch:44 step:34973 [D loss: 0.001022, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:44 step:34974 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:44 step:34975 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:44 step:34976 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:44 step:34977 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:44 step:34978 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.013293]\n",
      "epoch:44 step:34979 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000092]\n",
      "epoch:44 step:34980 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:44 step:34981 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:44 step:34982 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:44 step:34983 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.001028]\n",
      "epoch:44 step:34984 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.003439]\n",
      "epoch:44 step:34985 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:44 step:34986 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:44 step:34987 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:44 step:34988 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.002315]\n",
      "epoch:44 step:34989 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:44 step:34990 [D loss: 0.000463, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:44 step:34991 [D loss: 0.000858, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:44 step:34992 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.003447]\n",
      "epoch:44 step:34993 [D loss: 0.000512, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:44 step:34994 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:44 step:34995 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:44 step:34996 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000647]\n",
      "epoch:44 step:34997 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:44 step:34998 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.003064]\n",
      "epoch:44 step:34999 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:35000 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000668]\n",
      "epoch:44 step:35001 [D loss: 0.000592, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:35002 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:44 step:35003 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.000966]\n",
      "epoch:44 step:35004 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:44 step:35005 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:44 step:35006 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:44 step:35007 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:44 step:35008 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:44 step:35009 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:44 step:35010 [D loss: 0.007521, acc.: 100.00%] [G loss: 0.000003]\n",
      "epoch:44 step:35011 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000376]\n",
      "epoch:44 step:35012 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.001094]\n",
      "epoch:44 step:35013 [D loss: 0.003442, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:44 step:35014 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:44 step:35015 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:35016 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.002491]\n",
      "epoch:44 step:35017 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:44 step:35018 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:44 step:35019 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:35020 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:44 step:35021 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000516]\n",
      "epoch:44 step:35022 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:35023 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:44 step:35024 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:35025 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000454]\n",
      "epoch:44 step:35026 [D loss: 0.000801, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:44 step:35027 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:44 step:35028 [D loss: 0.007465, acc.: 100.00%] [G loss: 0.004104]\n",
      "epoch:44 step:35029 [D loss: 0.003088, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:44 step:35030 [D loss: 0.005068, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:44 step:35031 [D loss: 0.007006, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:44 step:35032 [D loss: 0.016502, acc.: 100.00%] [G loss: 0.100514]\n",
      "epoch:44 step:35033 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.001725]\n",
      "epoch:44 step:35034 [D loss: 0.000877, acc.: 100.00%] [G loss: 0.004044]\n",
      "epoch:44 step:35035 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000430]\n",
      "epoch:44 step:35036 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.006747]\n",
      "epoch:44 step:35037 [D loss: 0.002014, acc.: 100.00%] [G loss: 0.003038]\n",
      "epoch:44 step:35038 [D loss: 0.003593, acc.: 100.00%] [G loss: 0.012666]\n",
      "epoch:44 step:35039 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000473]\n",
      "epoch:44 step:35040 [D loss: 0.001692, acc.: 100.00%] [G loss: 0.000525]\n",
      "epoch:44 step:35041 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.018755]\n",
      "epoch:44 step:35042 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.000681]\n",
      "epoch:44 step:35043 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.000326]\n",
      "epoch:44 step:35044 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:44 step:35045 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.029159]\n",
      "epoch:44 step:35046 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.001023]\n",
      "epoch:44 step:35047 [D loss: 0.000791, acc.: 100.00%] [G loss: 0.000788]\n",
      "epoch:44 step:35048 [D loss: 0.000460, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:44 step:35049 [D loss: 0.003228, acc.: 100.00%] [G loss: 0.002241]\n",
      "epoch:44 step:35050 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:44 step:35051 [D loss: 0.001710, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:44 step:35052 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:44 step:35053 [D loss: 0.001410, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:44 step:35054 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:44 step:35055 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:44 step:35056 [D loss: 0.000839, acc.: 100.00%] [G loss: 0.001530]\n",
      "epoch:44 step:35057 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:44 step:35058 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:44 step:35059 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:44 step:35060 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000149]\n",
      "epoch:44 step:35061 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:44 step:35062 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:44 step:35063 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:44 step:35064 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:44 step:35065 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:44 step:35066 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.000407]\n",
      "epoch:44 step:35067 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.001401]\n",
      "epoch:44 step:35068 [D loss: 0.004269, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:44 step:35069 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:44 step:35070 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:44 step:35071 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:44 step:35072 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.015214]\n",
      "epoch:44 step:35073 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:44 step:35074 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:44 step:35075 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:44 step:35076 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.005793]\n",
      "epoch:44 step:35077 [D loss: 0.001943, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:44 step:35078 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:44 step:35079 [D loss: 0.008334, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:44 step:35080 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.002375]\n",
      "epoch:44 step:35081 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:44 step:35082 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:44 step:35083 [D loss: 0.001558, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:44 step:35084 [D loss: 0.003467, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:44 step:35085 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:44 step:35086 [D loss: 0.007314, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:44 step:35087 [D loss: 0.000983, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:44 step:35088 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:44 step:35089 [D loss: 0.001144, acc.: 100.00%] [G loss: 0.001293]\n",
      "epoch:44 step:35090 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:44 step:35091 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:44 step:35092 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000472]\n",
      "epoch:44 step:35093 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:44 step:35094 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:44 step:35095 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000084]\n",
      "epoch:44 step:35096 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000403]\n",
      "epoch:44 step:35097 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:44 step:35098 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000432]\n",
      "epoch:44 step:35099 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:44 step:35100 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:44 step:35101 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:44 step:35102 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:44 step:35103 [D loss: 0.000718, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:44 step:35104 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000240]\n",
      "epoch:44 step:35105 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:44 step:35106 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:44 step:35107 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:44 step:35108 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000349]\n",
      "epoch:44 step:35109 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.003697]\n",
      "epoch:44 step:35110 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:44 step:35111 [D loss: 0.001361, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:44 step:35112 [D loss: 0.000371, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:44 step:35113 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:44 step:35114 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.018118]\n",
      "epoch:44 step:35115 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000777]\n",
      "epoch:44 step:35116 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.002782]\n",
      "epoch:44 step:35117 [D loss: 0.008038, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:44 step:35118 [D loss: 0.000604, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:44 step:35119 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:44 step:35120 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:44 step:35121 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:44 step:35122 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:44 step:35123 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:44 step:35124 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:44 step:35125 [D loss: 0.001810, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:44 step:35126 [D loss: 0.002026, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:44 step:35127 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.000569]\n",
      "epoch:44 step:35128 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000196]\n",
      "epoch:44 step:35129 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:44 step:35130 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:44 step:35131 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:44 step:35132 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.001655]\n",
      "epoch:44 step:35133 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:44 step:35134 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:44 step:35135 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:44 step:35136 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:44 step:35137 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.000730]\n",
      "epoch:44 step:35138 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:44 step:35139 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:44 step:35140 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:44 step:35141 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:44 step:35142 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:44 step:35143 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:44 step:35144 [D loss: 0.029531, acc.: 100.00%] [G loss: 0.034246]\n",
      "epoch:44 step:35145 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.022622]\n",
      "epoch:45 step:35146 [D loss: 0.011660, acc.: 99.22%] [G loss: 0.016340]\n",
      "epoch:45 step:35147 [D loss: 0.000984, acc.: 100.00%] [G loss: 0.002687]\n",
      "epoch:45 step:35148 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.004581]\n",
      "epoch:45 step:35149 [D loss: 0.000648, acc.: 100.00%] [G loss: 0.029233]\n",
      "epoch:45 step:35150 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.004335]\n",
      "epoch:45 step:35151 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.008362]\n",
      "epoch:45 step:35152 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.000742]\n",
      "epoch:45 step:35153 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.019143]\n",
      "epoch:45 step:35154 [D loss: 0.000991, acc.: 100.00%] [G loss: 0.001259]\n",
      "epoch:45 step:35155 [D loss: 0.001702, acc.: 100.00%] [G loss: 0.003905]\n",
      "epoch:45 step:35156 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.001436]\n",
      "epoch:45 step:35157 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000653]\n",
      "epoch:45 step:35158 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000657]\n",
      "epoch:45 step:35159 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000430]\n",
      "epoch:45 step:35160 [D loss: 0.000477, acc.: 100.00%] [G loss: 0.000946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35161 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.000787]\n",
      "epoch:45 step:35162 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.011791]\n",
      "epoch:45 step:35163 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.001670]\n",
      "epoch:45 step:35164 [D loss: 0.001102, acc.: 100.00%] [G loss: 0.002773]\n",
      "epoch:45 step:35165 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:45 step:35166 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000256]\n",
      "epoch:45 step:35167 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.004680]\n",
      "epoch:45 step:35168 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000849]\n",
      "epoch:45 step:35169 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000915]\n",
      "epoch:45 step:35170 [D loss: 0.000525, acc.: 100.00%] [G loss: 0.001296]\n",
      "epoch:45 step:35171 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.001046]\n",
      "epoch:45 step:35172 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.003423]\n",
      "epoch:45 step:35173 [D loss: 0.000973, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:45 step:35174 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.004963]\n",
      "epoch:45 step:35175 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:45 step:35176 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000467]\n",
      "epoch:45 step:35177 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.000644]\n",
      "epoch:45 step:35178 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.001288]\n",
      "epoch:45 step:35179 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:45 step:35180 [D loss: 0.000424, acc.: 100.00%] [G loss: 0.002716]\n",
      "epoch:45 step:35181 [D loss: 0.000863, acc.: 100.00%] [G loss: 0.001134]\n",
      "epoch:45 step:35182 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:45 step:35183 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000434]\n",
      "epoch:45 step:35184 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:45 step:35185 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:45 step:35186 [D loss: 0.000934, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:45 step:35187 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.005922]\n",
      "epoch:45 step:35188 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:45 step:35189 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:45 step:35190 [D loss: 0.001832, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:45 step:35191 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000617]\n",
      "epoch:45 step:35192 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:45 step:35193 [D loss: 0.001628, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:45 step:35194 [D loss: 0.000653, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:45 step:35195 [D loss: 0.030664, acc.: 100.00%] [G loss: 0.016070]\n",
      "epoch:45 step:35196 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.172957]\n",
      "epoch:45 step:35197 [D loss: 0.032806, acc.: 99.22%] [G loss: 0.023485]\n",
      "epoch:45 step:35198 [D loss: 0.006331, acc.: 100.00%] [G loss: 0.487475]\n",
      "epoch:45 step:35199 [D loss: 0.079758, acc.: 98.44%] [G loss: 2.178664]\n",
      "epoch:45 step:35200 [D loss: 0.002304, acc.: 100.00%] [G loss: 2.551123]\n",
      "epoch:45 step:35201 [D loss: 4.142014, acc.: 24.22%] [G loss: 9.585150]\n",
      "epoch:45 step:35202 [D loss: 0.999513, acc.: 62.50%] [G loss: 1.391865]\n",
      "epoch:45 step:35203 [D loss: 0.133918, acc.: 95.31%] [G loss: 6.087663]\n",
      "epoch:45 step:35204 [D loss: 0.019799, acc.: 100.00%] [G loss: 4.999448]\n",
      "epoch:45 step:35205 [D loss: 0.045986, acc.: 98.44%] [G loss: 4.179343]\n",
      "epoch:45 step:35206 [D loss: 0.052701, acc.: 100.00%] [G loss: 3.179500]\n",
      "epoch:45 step:35207 [D loss: 0.037137, acc.: 100.00%] [G loss: 2.845417]\n",
      "epoch:45 step:35208 [D loss: 0.101597, acc.: 99.22%] [G loss: 0.065862]\n",
      "epoch:45 step:35209 [D loss: 0.048150, acc.: 99.22%] [G loss: 2.000708]\n",
      "epoch:45 step:35210 [D loss: 0.193723, acc.: 95.31%] [G loss: 0.583463]\n",
      "epoch:45 step:35211 [D loss: 0.002603, acc.: 100.00%] [G loss: 0.122758]\n",
      "epoch:45 step:35212 [D loss: 0.002702, acc.: 100.00%] [G loss: 0.093818]\n",
      "epoch:45 step:35213 [D loss: 0.013698, acc.: 100.00%] [G loss: 0.056001]\n",
      "epoch:45 step:35214 [D loss: 0.530577, acc.: 73.44%] [G loss: 2.470715]\n",
      "epoch:45 step:35215 [D loss: 0.057197, acc.: 98.44%] [G loss: 3.930737]\n",
      "epoch:45 step:35216 [D loss: 0.904960, acc.: 64.06%] [G loss: 0.669966]\n",
      "epoch:45 step:35217 [D loss: 0.139526, acc.: 92.97%] [G loss: 0.414678]\n",
      "epoch:45 step:35218 [D loss: 0.010038, acc.: 100.00%] [G loss: 1.332953]\n",
      "epoch:45 step:35219 [D loss: 0.250090, acc.: 86.72%] [G loss: 1.738828]\n",
      "epoch:45 step:35220 [D loss: 0.032624, acc.: 99.22%] [G loss: 2.917318]\n",
      "epoch:45 step:35221 [D loss: 0.134214, acc.: 96.09%] [G loss: 1.644327]\n",
      "epoch:45 step:35222 [D loss: 0.260942, acc.: 85.16%] [G loss: 0.386618]\n",
      "epoch:45 step:35223 [D loss: 0.079933, acc.: 96.09%] [G loss: 3.071394]\n",
      "epoch:45 step:35224 [D loss: 0.049233, acc.: 97.66%] [G loss: 2.080695]\n",
      "epoch:45 step:35225 [D loss: 0.021466, acc.: 99.22%] [G loss: 0.084371]\n",
      "epoch:45 step:35226 [D loss: 0.025986, acc.: 98.44%] [G loss: 0.503727]\n",
      "epoch:45 step:35227 [D loss: 0.007328, acc.: 100.00%] [G loss: 0.780422]\n",
      "epoch:45 step:35228 [D loss: 0.004330, acc.: 100.00%] [G loss: 0.128258]\n",
      "epoch:45 step:35229 [D loss: 0.011197, acc.: 100.00%] [G loss: 0.004400]\n",
      "epoch:45 step:35230 [D loss: 0.007741, acc.: 100.00%] [G loss: 0.118564]\n",
      "epoch:45 step:35231 [D loss: 0.005542, acc.: 100.00%] [G loss: 0.102864]\n",
      "epoch:45 step:35232 [D loss: 0.013491, acc.: 100.00%] [G loss: 0.072291]\n",
      "epoch:45 step:35233 [D loss: 0.012137, acc.: 100.00%] [G loss: 0.006789]\n",
      "epoch:45 step:35234 [D loss: 0.000997, acc.: 100.00%] [G loss: 0.003361]\n",
      "epoch:45 step:35235 [D loss: 0.078485, acc.: 96.88%] [G loss: 0.029043]\n",
      "epoch:45 step:35236 [D loss: 0.014215, acc.: 100.00%] [G loss: 0.857317]\n",
      "epoch:45 step:35237 [D loss: 0.008470, acc.: 100.00%] [G loss: 0.054272]\n",
      "epoch:45 step:35238 [D loss: 0.004478, acc.: 100.00%] [G loss: 0.187192]\n",
      "epoch:45 step:35239 [D loss: 0.003339, acc.: 100.00%] [G loss: 0.085572]\n",
      "epoch:45 step:35240 [D loss: 0.002220, acc.: 100.00%] [G loss: 0.009594]\n",
      "epoch:45 step:35241 [D loss: 0.020918, acc.: 100.00%] [G loss: 0.009210]\n",
      "epoch:45 step:35242 [D loss: 0.009513, acc.: 100.00%] [G loss: 0.005340]\n",
      "epoch:45 step:35243 [D loss: 0.003855, acc.: 100.00%] [G loss: 0.001088]\n",
      "epoch:45 step:35244 [D loss: 0.008046, acc.: 100.00%] [G loss: 0.007957]\n",
      "epoch:45 step:35245 [D loss: 0.007878, acc.: 100.00%] [G loss: 0.004055]\n",
      "epoch:45 step:35246 [D loss: 0.006236, acc.: 100.00%] [G loss: 0.000678]\n",
      "epoch:45 step:35247 [D loss: 0.009752, acc.: 100.00%] [G loss: 0.004204]\n",
      "epoch:45 step:35248 [D loss: 0.005679, acc.: 100.00%] [G loss: 0.075354]\n",
      "epoch:45 step:35249 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.002735]\n",
      "epoch:45 step:35250 [D loss: 0.001586, acc.: 100.00%] [G loss: 0.003166]\n",
      "epoch:45 step:35251 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.008694]\n",
      "epoch:45 step:35252 [D loss: 0.005385, acc.: 100.00%] [G loss: 0.031042]\n",
      "epoch:45 step:35253 [D loss: 0.006055, acc.: 100.00%] [G loss: 0.008352]\n",
      "epoch:45 step:35254 [D loss: 0.003182, acc.: 100.00%] [G loss: 0.008505]\n",
      "epoch:45 step:35255 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.002030]\n",
      "epoch:45 step:35256 [D loss: 0.002017, acc.: 100.00%] [G loss: 0.057382]\n",
      "epoch:45 step:35257 [D loss: 0.001051, acc.: 100.00%] [G loss: 0.000848]\n",
      "epoch:45 step:35258 [D loss: 0.001292, acc.: 100.00%] [G loss: 0.001343]\n",
      "epoch:45 step:35259 [D loss: 0.005202, acc.: 100.00%] [G loss: 0.001798]\n",
      "epoch:45 step:35260 [D loss: 0.002456, acc.: 100.00%] [G loss: 0.017356]\n",
      "epoch:45 step:35261 [D loss: 0.006643, acc.: 100.00%] [G loss: 0.001053]\n",
      "epoch:45 step:35262 [D loss: 0.017764, acc.: 100.00%] [G loss: 0.001090]\n",
      "epoch:45 step:35263 [D loss: 0.201081, acc.: 89.06%] [G loss: 1.257249]\n",
      "epoch:45 step:35264 [D loss: 0.040804, acc.: 99.22%] [G loss: 3.125918]\n",
      "epoch:45 step:35265 [D loss: 0.264929, acc.: 92.97%] [G loss: 0.007797]\n",
      "epoch:45 step:35266 [D loss: 0.001643, acc.: 100.00%] [G loss: 0.134114]\n",
      "epoch:45 step:35267 [D loss: 0.003741, acc.: 100.00%] [G loss: 0.076036]\n",
      "epoch:45 step:35268 [D loss: 0.000760, acc.: 100.00%] [G loss: 0.000739]\n",
      "epoch:45 step:35269 [D loss: 0.003866, acc.: 100.00%] [G loss: 0.092725]\n",
      "epoch:45 step:35270 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.029042]\n",
      "epoch:45 step:35271 [D loss: 0.000819, acc.: 100.00%] [G loss: 0.007355]\n",
      "epoch:45 step:35272 [D loss: 0.000364, acc.: 100.00%] [G loss: 0.012764]\n",
      "epoch:45 step:35273 [D loss: 0.000822, acc.: 100.00%] [G loss: 0.006700]\n",
      "epoch:45 step:35274 [D loss: 0.000488, acc.: 100.00%] [G loss: 0.002003]\n",
      "epoch:45 step:35275 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.018787]\n",
      "epoch:45 step:35276 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.033048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35277 [D loss: 0.002654, acc.: 100.00%] [G loss: 0.003089]\n",
      "epoch:45 step:35278 [D loss: 0.000454, acc.: 100.00%] [G loss: 0.050928]\n",
      "epoch:45 step:35279 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.004094]\n",
      "epoch:45 step:35280 [D loss: 0.001155, acc.: 100.00%] [G loss: 0.008936]\n",
      "epoch:45 step:35281 [D loss: 0.000982, acc.: 100.00%] [G loss: 0.008064]\n",
      "epoch:45 step:35282 [D loss: 0.001951, acc.: 100.00%] [G loss: 0.013555]\n",
      "epoch:45 step:35283 [D loss: 0.016815, acc.: 100.00%] [G loss: 0.006553]\n",
      "epoch:45 step:35284 [D loss: 0.023181, acc.: 100.00%] [G loss: 0.029221]\n",
      "epoch:45 step:35285 [D loss: 0.018526, acc.: 100.00%] [G loss: 0.080678]\n",
      "epoch:45 step:35286 [D loss: 0.016231, acc.: 100.00%] [G loss: 0.061535]\n",
      "epoch:45 step:35287 [D loss: 0.008146, acc.: 100.00%] [G loss: 0.056780]\n",
      "epoch:45 step:35288 [D loss: 0.162581, acc.: 94.53%] [G loss: 3.135229]\n",
      "epoch:45 step:35289 [D loss: 0.080562, acc.: 96.88%] [G loss: 0.385525]\n",
      "epoch:45 step:35290 [D loss: 0.134994, acc.: 94.53%] [G loss: 2.369806]\n",
      "epoch:45 step:35291 [D loss: 0.001016, acc.: 100.00%] [G loss: 0.863432]\n",
      "epoch:45 step:35292 [D loss: 0.015450, acc.: 100.00%] [G loss: 0.019536]\n",
      "epoch:45 step:35293 [D loss: 0.001390, acc.: 100.00%] [G loss: 0.681423]\n",
      "epoch:45 step:35294 [D loss: 0.003499, acc.: 100.00%] [G loss: 0.063181]\n",
      "epoch:45 step:35295 [D loss: 0.001313, acc.: 100.00%] [G loss: 0.016051]\n",
      "epoch:45 step:35296 [D loss: 0.002473, acc.: 100.00%] [G loss: 0.076254]\n",
      "epoch:45 step:35297 [D loss: 0.004377, acc.: 100.00%] [G loss: 0.058932]\n",
      "epoch:45 step:35298 [D loss: 0.002113, acc.: 100.00%] [G loss: 0.002694]\n",
      "epoch:45 step:35299 [D loss: 0.000553, acc.: 100.00%] [G loss: 0.091318]\n",
      "epoch:45 step:35300 [D loss: 0.000889, acc.: 100.00%] [G loss: 0.059223]\n",
      "epoch:45 step:35301 [D loss: 0.004882, acc.: 100.00%] [G loss: 0.008822]\n",
      "epoch:45 step:35302 [D loss: 0.002833, acc.: 100.00%] [G loss: 0.007596]\n",
      "epoch:45 step:35303 [D loss: 0.009509, acc.: 100.00%] [G loss: 0.010850]\n",
      "epoch:45 step:35304 [D loss: 0.028165, acc.: 99.22%] [G loss: 0.058053]\n",
      "epoch:45 step:35305 [D loss: 0.023418, acc.: 99.22%] [G loss: 0.026658]\n",
      "epoch:45 step:35306 [D loss: 0.006217, acc.: 100.00%] [G loss: 0.009956]\n",
      "epoch:45 step:35307 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.056307]\n",
      "epoch:45 step:35308 [D loss: 0.005565, acc.: 100.00%] [G loss: 0.034561]\n",
      "epoch:45 step:35309 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.023630]\n",
      "epoch:45 step:35310 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.018038]\n",
      "epoch:45 step:35311 [D loss: 0.002541, acc.: 100.00%] [G loss: 0.004548]\n",
      "epoch:45 step:35312 [D loss: 0.002891, acc.: 100.00%] [G loss: 0.046376]\n",
      "epoch:45 step:35313 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.048723]\n",
      "epoch:45 step:35314 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.007218]\n",
      "epoch:45 step:35315 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.013535]\n",
      "epoch:45 step:35316 [D loss: 0.005988, acc.: 100.00%] [G loss: 0.002529]\n",
      "epoch:45 step:35317 [D loss: 0.000969, acc.: 100.00%] [G loss: 0.002698]\n",
      "epoch:45 step:35318 [D loss: 0.957949, acc.: 58.59%] [G loss: 8.867251]\n",
      "epoch:45 step:35319 [D loss: 1.697786, acc.: 55.47%] [G loss: 0.804997]\n",
      "epoch:45 step:35320 [D loss: 0.117456, acc.: 96.09%] [G loss: 3.621320]\n",
      "epoch:45 step:35321 [D loss: 0.047228, acc.: 97.66%] [G loss: 3.108816]\n",
      "epoch:45 step:35322 [D loss: 0.023567, acc.: 99.22%] [G loss: 1.712508]\n",
      "epoch:45 step:35323 [D loss: 0.020185, acc.: 100.00%] [G loss: 0.273811]\n",
      "epoch:45 step:35324 [D loss: 0.024063, acc.: 99.22%] [G loss: 1.454723]\n",
      "epoch:45 step:35325 [D loss: 0.025620, acc.: 100.00%] [G loss: 0.481552]\n",
      "epoch:45 step:35326 [D loss: 0.049021, acc.: 100.00%] [G loss: 1.121587]\n",
      "epoch:45 step:35327 [D loss: 0.027533, acc.: 100.00%] [G loss: 0.008377]\n",
      "epoch:45 step:35328 [D loss: 0.001842, acc.: 100.00%] [G loss: 0.812990]\n",
      "epoch:45 step:35329 [D loss: 0.058707, acc.: 98.44%] [G loss: 0.467656]\n",
      "epoch:45 step:35330 [D loss: 0.002522, acc.: 100.00%] [G loss: 0.114026]\n",
      "epoch:45 step:35331 [D loss: 0.011659, acc.: 99.22%] [G loss: 0.057442]\n",
      "epoch:45 step:35332 [D loss: 0.004465, acc.: 100.00%] [G loss: 0.005366]\n",
      "epoch:45 step:35333 [D loss: 0.006048, acc.: 100.00%] [G loss: 0.109578]\n",
      "epoch:45 step:35334 [D loss: 0.005284, acc.: 100.00%] [G loss: 0.015056]\n",
      "epoch:45 step:35335 [D loss: 0.004734, acc.: 100.00%] [G loss: 0.002820]\n",
      "epoch:45 step:35336 [D loss: 0.014209, acc.: 100.00%] [G loss: 0.017123]\n",
      "epoch:45 step:35337 [D loss: 0.001883, acc.: 100.00%] [G loss: 0.011215]\n",
      "epoch:45 step:35338 [D loss: 0.003532, acc.: 100.00%] [G loss: 0.000968]\n",
      "epoch:45 step:35339 [D loss: 0.000855, acc.: 100.00%] [G loss: 0.012590]\n",
      "epoch:45 step:35340 [D loss: 0.001665, acc.: 100.00%] [G loss: 0.016537]\n",
      "epoch:45 step:35341 [D loss: 0.004496, acc.: 100.00%] [G loss: 0.000590]\n",
      "epoch:45 step:35342 [D loss: 0.028055, acc.: 99.22%] [G loss: 0.003214]\n",
      "epoch:45 step:35343 [D loss: 0.024602, acc.: 100.00%] [G loss: 0.012519]\n",
      "epoch:45 step:35344 [D loss: 0.002314, acc.: 100.00%] [G loss: 0.058130]\n",
      "epoch:45 step:35345 [D loss: 0.013808, acc.: 98.44%] [G loss: 0.036984]\n",
      "epoch:45 step:35346 [D loss: 0.014868, acc.: 99.22%] [G loss: 0.000770]\n",
      "epoch:45 step:35347 [D loss: 0.006944, acc.: 100.00%] [G loss: 0.079055]\n",
      "epoch:45 step:35348 [D loss: 0.000975, acc.: 100.00%] [G loss: 0.003042]\n",
      "epoch:45 step:35349 [D loss: 0.000526, acc.: 100.00%] [G loss: 0.013859]\n",
      "epoch:45 step:35350 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.115983]\n",
      "epoch:45 step:35351 [D loss: 0.002355, acc.: 100.00%] [G loss: 0.000856]\n",
      "epoch:45 step:35352 [D loss: 0.003306, acc.: 100.00%] [G loss: 0.006620]\n",
      "epoch:45 step:35353 [D loss: 0.001514, acc.: 100.00%] [G loss: 0.007063]\n",
      "epoch:45 step:35354 [D loss: 0.003388, acc.: 100.00%] [G loss: 0.004447]\n",
      "epoch:45 step:35355 [D loss: 0.001028, acc.: 100.00%] [G loss: 0.007466]\n",
      "epoch:45 step:35356 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.002693]\n",
      "epoch:45 step:35357 [D loss: 0.000925, acc.: 100.00%] [G loss: 0.004504]\n",
      "epoch:45 step:35358 [D loss: 0.002544, acc.: 100.00%] [G loss: 0.001707]\n",
      "epoch:45 step:35359 [D loss: 0.001673, acc.: 100.00%] [G loss: 0.007714]\n",
      "epoch:45 step:35360 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.001961]\n",
      "epoch:45 step:35361 [D loss: 0.003302, acc.: 100.00%] [G loss: 0.004378]\n",
      "epoch:45 step:35362 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.004095]\n",
      "epoch:45 step:35363 [D loss: 0.002518, acc.: 100.00%] [G loss: 0.007266]\n",
      "epoch:45 step:35364 [D loss: 0.000477, acc.: 100.00%] [G loss: 0.003351]\n",
      "epoch:45 step:35365 [D loss: 0.003850, acc.: 100.00%] [G loss: 0.001306]\n",
      "epoch:45 step:35366 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.001020]\n",
      "epoch:45 step:35367 [D loss: 0.001129, acc.: 100.00%] [G loss: 0.000754]\n",
      "epoch:45 step:35368 [D loss: 0.006107, acc.: 100.00%] [G loss: 0.010404]\n",
      "epoch:45 step:35369 [D loss: 0.000478, acc.: 100.00%] [G loss: 0.005311]\n",
      "epoch:45 step:35370 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.001450]\n",
      "epoch:45 step:35371 [D loss: 0.000807, acc.: 100.00%] [G loss: 0.003108]\n",
      "epoch:45 step:35372 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.001428]\n",
      "epoch:45 step:35373 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.001247]\n",
      "epoch:45 step:35374 [D loss: 0.000479, acc.: 100.00%] [G loss: 0.002581]\n",
      "epoch:45 step:35375 [D loss: 0.006086, acc.: 100.00%] [G loss: 0.001505]\n",
      "epoch:45 step:35376 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000972]\n",
      "epoch:45 step:35377 [D loss: 0.000538, acc.: 100.00%] [G loss: 0.000491]\n",
      "epoch:45 step:35378 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.003635]\n",
      "epoch:45 step:35379 [D loss: 0.000551, acc.: 100.00%] [G loss: 0.001340]\n",
      "epoch:45 step:35380 [D loss: 0.001171, acc.: 100.00%] [G loss: 0.002437]\n",
      "epoch:45 step:35381 [D loss: 0.001316, acc.: 100.00%] [G loss: 0.000953]\n",
      "epoch:45 step:35382 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.000915]\n",
      "epoch:45 step:35383 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.004129]\n",
      "epoch:45 step:35384 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.003929]\n",
      "epoch:45 step:35385 [D loss: 0.003916, acc.: 100.00%] [G loss: 0.000721]\n",
      "epoch:45 step:35386 [D loss: 0.011323, acc.: 100.00%] [G loss: 0.001802]\n",
      "epoch:45 step:35387 [D loss: 0.021053, acc.: 100.00%] [G loss: 0.002867]\n",
      "epoch:45 step:35388 [D loss: 0.002368, acc.: 100.00%] [G loss: 0.002515]\n",
      "epoch:45 step:35389 [D loss: 0.036151, acc.: 100.00%] [G loss: 0.001675]\n",
      "epoch:45 step:35390 [D loss: 0.001191, acc.: 100.00%] [G loss: 0.004992]\n",
      "epoch:45 step:35391 [D loss: 0.026214, acc.: 98.44%] [G loss: 0.053616]\n",
      "epoch:45 step:35392 [D loss: 0.024112, acc.: 100.00%] [G loss: 0.051723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35393 [D loss: 0.001720, acc.: 100.00%] [G loss: 0.013403]\n",
      "epoch:45 step:35394 [D loss: 0.000508, acc.: 100.00%] [G loss: 0.032794]\n",
      "epoch:45 step:35395 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.010409]\n",
      "epoch:45 step:35396 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.002985]\n",
      "epoch:45 step:35397 [D loss: 0.001141, acc.: 100.00%] [G loss: 0.000539]\n",
      "epoch:45 step:35398 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.007325]\n",
      "epoch:45 step:35399 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.003015]\n",
      "epoch:45 step:35400 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:45 step:35401 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000278]\n",
      "epoch:45 step:35402 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.003108]\n",
      "epoch:45 step:35403 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.009819]\n",
      "epoch:45 step:35404 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.008867]\n",
      "epoch:45 step:35405 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:45 step:35406 [D loss: 0.000878, acc.: 100.00%] [G loss: 0.004216]\n",
      "epoch:45 step:35407 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.002431]\n",
      "epoch:45 step:35408 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.014042]\n",
      "epoch:45 step:35409 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:45 step:35410 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.007922]\n",
      "epoch:45 step:35411 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.018757]\n",
      "epoch:45 step:35412 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.005187]\n",
      "epoch:45 step:35413 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.003154]\n",
      "epoch:45 step:35414 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:45 step:35415 [D loss: 0.000588, acc.: 100.00%] [G loss: 0.001296]\n",
      "epoch:45 step:35416 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.002245]\n",
      "epoch:45 step:35417 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.001933]\n",
      "epoch:45 step:35418 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.002567]\n",
      "epoch:45 step:35419 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.001956]\n",
      "epoch:45 step:35420 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.003392]\n",
      "epoch:45 step:35421 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.002332]\n",
      "epoch:45 step:35422 [D loss: 0.001836, acc.: 100.00%] [G loss: 0.000619]\n",
      "epoch:45 step:35423 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.001550]\n",
      "epoch:45 step:35424 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.001037]\n",
      "epoch:45 step:35425 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.002109]\n",
      "epoch:45 step:35426 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.003086]\n",
      "epoch:45 step:35427 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:45 step:35428 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.003064]\n",
      "epoch:45 step:35429 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.003515]\n",
      "epoch:45 step:35430 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.002106]\n",
      "epoch:45 step:35431 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:45 step:35432 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.002705]\n",
      "epoch:45 step:35433 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:45 step:35434 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.002585]\n",
      "epoch:45 step:35435 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.001604]\n",
      "epoch:45 step:35436 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.008612]\n",
      "epoch:45 step:35437 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.001454]\n",
      "epoch:45 step:35438 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.003003]\n",
      "epoch:45 step:35439 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.002365]\n",
      "epoch:45 step:35440 [D loss: 0.001025, acc.: 100.00%] [G loss: 0.000285]\n",
      "epoch:45 step:35441 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000192]\n",
      "epoch:45 step:35442 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.001623]\n",
      "epoch:45 step:35443 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.001576]\n",
      "epoch:45 step:35444 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000953]\n",
      "epoch:45 step:35445 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.002091]\n",
      "epoch:45 step:35446 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:45 step:35447 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:45 step:35448 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.003810]\n",
      "epoch:45 step:35449 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.000603]\n",
      "epoch:45 step:35450 [D loss: 0.001757, acc.: 100.00%] [G loss: 0.035849]\n",
      "epoch:45 step:35451 [D loss: 0.000606, acc.: 100.00%] [G loss: 0.264788]\n",
      "epoch:45 step:35452 [D loss: 0.007268, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:45 step:35453 [D loss: 0.077630, acc.: 96.88%] [G loss: 0.055755]\n",
      "epoch:45 step:35454 [D loss: 0.000790, acc.: 100.00%] [G loss: 0.199030]\n",
      "epoch:45 step:35455 [D loss: 0.004469, acc.: 100.00%] [G loss: 0.102357]\n",
      "epoch:45 step:35456 [D loss: 0.009139, acc.: 100.00%] [G loss: 0.096205]\n",
      "epoch:45 step:35457 [D loss: 0.008345, acc.: 100.00%] [G loss: 0.078869]\n",
      "epoch:45 step:35458 [D loss: 0.008146, acc.: 100.00%] [G loss: 0.056858]\n",
      "epoch:45 step:35459 [D loss: 0.002789, acc.: 100.00%] [G loss: 0.009388]\n",
      "epoch:45 step:35460 [D loss: 0.214311, acc.: 89.84%] [G loss: 0.000049]\n",
      "epoch:45 step:35461 [D loss: 0.009969, acc.: 99.22%] [G loss: 0.000087]\n",
      "epoch:45 step:35462 [D loss: 0.148957, acc.: 94.53%] [G loss: 0.000191]\n",
      "epoch:45 step:35463 [D loss: 0.000455, acc.: 100.00%] [G loss: 0.014752]\n",
      "epoch:45 step:35464 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.102391]\n",
      "epoch:45 step:35465 [D loss: 0.011586, acc.: 100.00%] [G loss: 0.086867]\n",
      "epoch:45 step:35466 [D loss: 0.005453, acc.: 100.00%] [G loss: 0.020470]\n",
      "epoch:45 step:35467 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.912086]\n",
      "epoch:45 step:35468 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.010853]\n",
      "epoch:45 step:35469 [D loss: 0.000740, acc.: 100.00%] [G loss: 0.384573]\n",
      "epoch:45 step:35470 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.003357]\n",
      "epoch:45 step:35471 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.021294]\n",
      "epoch:45 step:35472 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.061566]\n",
      "epoch:45 step:35473 [D loss: 0.006297, acc.: 100.00%] [G loss: 0.005280]\n",
      "epoch:45 step:35474 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.005312]\n",
      "epoch:45 step:35475 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.003871]\n",
      "epoch:45 step:35476 [D loss: 0.000896, acc.: 100.00%] [G loss: 0.914198]\n",
      "epoch:45 step:35477 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.004047]\n",
      "epoch:45 step:35478 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.032524]\n",
      "epoch:45 step:35479 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.002865]\n",
      "epoch:45 step:35480 [D loss: 0.001319, acc.: 100.00%] [G loss: 0.001745]\n",
      "epoch:45 step:35481 [D loss: 0.001955, acc.: 100.00%] [G loss: 0.005080]\n",
      "epoch:45 step:35482 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.001230]\n",
      "epoch:45 step:35483 [D loss: 0.004793, acc.: 100.00%] [G loss: 0.002678]\n",
      "epoch:45 step:35484 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:45 step:35485 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.000620]\n",
      "epoch:45 step:35486 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.018823]\n",
      "epoch:45 step:35487 [D loss: 0.002664, acc.: 100.00%] [G loss: 0.064424]\n",
      "epoch:45 step:35488 [D loss: 0.000917, acc.: 100.00%] [G loss: 0.000572]\n",
      "epoch:45 step:35489 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.001621]\n",
      "epoch:45 step:35490 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.001314]\n",
      "epoch:45 step:35491 [D loss: 0.000698, acc.: 100.00%] [G loss: 0.001132]\n",
      "epoch:45 step:35492 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000831]\n",
      "epoch:45 step:35493 [D loss: 0.000860, acc.: 100.00%] [G loss: 0.004048]\n",
      "epoch:45 step:35494 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.001544]\n",
      "epoch:45 step:35495 [D loss: 0.002097, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:45 step:35496 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:45 step:35497 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.001049]\n",
      "epoch:45 step:35498 [D loss: 0.001244, acc.: 100.00%] [G loss: 0.006898]\n",
      "epoch:45 step:35499 [D loss: 0.013707, acc.: 99.22%] [G loss: 0.003439]\n",
      "epoch:45 step:35500 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.003853]\n",
      "epoch:45 step:35501 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.005236]\n",
      "epoch:45 step:35502 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.000614]\n",
      "epoch:45 step:35503 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.005189]\n",
      "epoch:45 step:35504 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.008076]\n",
      "epoch:45 step:35505 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.002887]\n",
      "epoch:45 step:35506 [D loss: 0.002594, acc.: 100.00%] [G loss: 0.000694]\n",
      "epoch:45 step:35507 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.003675]\n",
      "epoch:45 step:35508 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.003556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35509 [D loss: 0.000608, acc.: 100.00%] [G loss: 0.000858]\n",
      "epoch:45 step:35510 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:45 step:35511 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000283]\n",
      "epoch:45 step:35512 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.000851]\n",
      "epoch:45 step:35513 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.001363]\n",
      "epoch:45 step:35514 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.005153]\n",
      "epoch:45 step:35515 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.000578]\n",
      "epoch:45 step:35516 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.002127]\n",
      "epoch:45 step:35517 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000564]\n",
      "epoch:45 step:35518 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001303]\n",
      "epoch:45 step:35519 [D loss: 0.001111, acc.: 100.00%] [G loss: 0.002848]\n",
      "epoch:45 step:35520 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.003457]\n",
      "epoch:45 step:35521 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.007711]\n",
      "epoch:45 step:35522 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.001570]\n",
      "epoch:45 step:35523 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.006116]\n",
      "epoch:45 step:35524 [D loss: 0.007207, acc.: 100.00%] [G loss: 0.001149]\n",
      "epoch:45 step:35525 [D loss: 0.000374, acc.: 100.00%] [G loss: 0.001409]\n",
      "epoch:45 step:35526 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.007233]\n",
      "epoch:45 step:35527 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:45 step:35528 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.003547]\n",
      "epoch:45 step:35529 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.004816]\n",
      "epoch:45 step:35530 [D loss: 0.000705, acc.: 100.00%] [G loss: 0.002158]\n",
      "epoch:45 step:35531 [D loss: 0.001753, acc.: 100.00%] [G loss: 0.000555]\n",
      "epoch:45 step:35532 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.001040]\n",
      "epoch:45 step:35533 [D loss: 0.000622, acc.: 100.00%] [G loss: 0.010165]\n",
      "epoch:45 step:35534 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.000612]\n",
      "epoch:45 step:35535 [D loss: 0.001152, acc.: 100.00%] [G loss: 0.004134]\n",
      "epoch:45 step:35536 [D loss: 0.001113, acc.: 100.00%] [G loss: 0.002554]\n",
      "epoch:45 step:35537 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.000765]\n",
      "epoch:45 step:35538 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.002723]\n",
      "epoch:45 step:35539 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.001290]\n",
      "epoch:45 step:35540 [D loss: 0.001146, acc.: 100.00%] [G loss: 0.003409]\n",
      "epoch:45 step:35541 [D loss: 0.000789, acc.: 100.00%] [G loss: 0.000548]\n",
      "epoch:45 step:35542 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.000264]\n",
      "epoch:45 step:35543 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000199]\n",
      "epoch:45 step:35544 [D loss: 0.000352, acc.: 100.00%] [G loss: 0.000558]\n",
      "epoch:45 step:35545 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002446]\n",
      "epoch:45 step:35546 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:45 step:35547 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:45 step:35548 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.001029]\n",
      "epoch:45 step:35549 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000344]\n",
      "epoch:45 step:35550 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000421]\n",
      "epoch:45 step:35551 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.010435]\n",
      "epoch:45 step:35552 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001417]\n",
      "epoch:45 step:35553 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.005264]\n",
      "epoch:45 step:35554 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000441]\n",
      "epoch:45 step:35555 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.002605]\n",
      "epoch:45 step:35556 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.001467]\n",
      "epoch:45 step:35557 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000366]\n",
      "epoch:45 step:35558 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.002722]\n",
      "epoch:45 step:35559 [D loss: 0.001083, acc.: 100.00%] [G loss: 0.062516]\n",
      "epoch:45 step:35560 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.034259]\n",
      "epoch:45 step:35561 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000304]\n",
      "epoch:45 step:35562 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:45 step:35563 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000767]\n",
      "epoch:45 step:35564 [D loss: 0.003801, acc.: 100.00%] [G loss: 0.000236]\n",
      "epoch:45 step:35565 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.000397]\n",
      "epoch:45 step:35566 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.002423]\n",
      "epoch:45 step:35567 [D loss: 0.001232, acc.: 100.00%] [G loss: 0.000560]\n",
      "epoch:45 step:35568 [D loss: 0.001137, acc.: 100.00%] [G loss: 0.002202]\n",
      "epoch:45 step:35569 [D loss: 0.005065, acc.: 100.00%] [G loss: 0.000351]\n",
      "epoch:45 step:35570 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.000463]\n",
      "epoch:45 step:35571 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000327]\n",
      "epoch:45 step:35572 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.038295]\n",
      "epoch:45 step:35573 [D loss: 0.002192, acc.: 100.00%] [G loss: 0.005926]\n",
      "epoch:45 step:35574 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.002866]\n",
      "epoch:45 step:35575 [D loss: 0.019623, acc.: 100.00%] [G loss: 0.015023]\n",
      "epoch:45 step:35576 [D loss: 0.056933, acc.: 99.22%] [G loss: 0.413855]\n",
      "epoch:45 step:35577 [D loss: 0.204006, acc.: 88.28%] [G loss: 0.833747]\n",
      "epoch:45 step:35578 [D loss: 0.001386, acc.: 100.00%] [G loss: 0.168845]\n",
      "epoch:45 step:35579 [D loss: 0.004472, acc.: 100.00%] [G loss: 1.894579]\n",
      "epoch:45 step:35580 [D loss: 0.007279, acc.: 100.00%] [G loss: 0.105984]\n",
      "epoch:45 step:35581 [D loss: 0.032097, acc.: 99.22%] [G loss: 0.012236]\n",
      "epoch:45 step:35582 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.020245]\n",
      "epoch:45 step:35583 [D loss: 0.002672, acc.: 100.00%] [G loss: 0.004516]\n",
      "epoch:45 step:35584 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.005396]\n",
      "epoch:45 step:35585 [D loss: 0.000510, acc.: 100.00%] [G loss: 0.156993]\n",
      "epoch:45 step:35586 [D loss: 0.000869, acc.: 100.00%] [G loss: 0.101355]\n",
      "epoch:45 step:35587 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.045343]\n",
      "epoch:45 step:35588 [D loss: 0.000877, acc.: 100.00%] [G loss: 0.000761]\n",
      "epoch:45 step:35589 [D loss: 0.000977, acc.: 100.00%] [G loss: 0.154410]\n",
      "epoch:45 step:35590 [D loss: 0.005121, acc.: 100.00%] [G loss: 0.000243]\n",
      "epoch:45 step:35591 [D loss: 0.000740, acc.: 100.00%] [G loss: 0.002592]\n",
      "epoch:45 step:35592 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.045614]\n",
      "epoch:45 step:35593 [D loss: 0.001088, acc.: 100.00%] [G loss: 0.000836]\n",
      "epoch:45 step:35594 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.002871]\n",
      "epoch:45 step:35595 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.025460]\n",
      "epoch:45 step:35596 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.002308]\n",
      "epoch:45 step:35597 [D loss: 0.001398, acc.: 100.00%] [G loss: 0.000723]\n",
      "epoch:45 step:35598 [D loss: 0.001233, acc.: 100.00%] [G loss: 0.003490]\n",
      "epoch:45 step:35599 [D loss: 0.000491, acc.: 100.00%] [G loss: 0.000962]\n",
      "epoch:45 step:35600 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.000880]\n",
      "epoch:45 step:35601 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.041615]\n",
      "epoch:45 step:35602 [D loss: 0.001025, acc.: 100.00%] [G loss: 0.000644]\n",
      "epoch:45 step:35603 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000766]\n",
      "epoch:45 step:35604 [D loss: 0.001904, acc.: 100.00%] [G loss: 0.001103]\n",
      "epoch:45 step:35605 [D loss: 0.004716, acc.: 100.00%] [G loss: 0.000753]\n",
      "epoch:45 step:35606 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.156001]\n",
      "epoch:45 step:35607 [D loss: 0.000360, acc.: 100.00%] [G loss: 0.003150]\n",
      "epoch:45 step:35608 [D loss: 0.000587, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:45 step:35609 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000581]\n",
      "epoch:45 step:35610 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.000630]\n",
      "epoch:45 step:35611 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.002212]\n",
      "epoch:45 step:35612 [D loss: 0.009277, acc.: 99.22%] [G loss: 0.000262]\n",
      "epoch:45 step:35613 [D loss: 0.010716, acc.: 100.00%] [G loss: 0.001858]\n",
      "epoch:45 step:35614 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.029103]\n",
      "epoch:45 step:35615 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.052547]\n",
      "epoch:45 step:35616 [D loss: 0.007742, acc.: 100.00%] [G loss: 0.002721]\n",
      "epoch:45 step:35617 [D loss: 0.006701, acc.: 100.00%] [G loss: 0.002280]\n",
      "epoch:45 step:35618 [D loss: 0.003160, acc.: 100.00%] [G loss: 0.002832]\n",
      "epoch:45 step:35619 [D loss: 0.000814, acc.: 100.00%] [G loss: 0.003061]\n",
      "epoch:45 step:35620 [D loss: 0.000793, acc.: 100.00%] [G loss: 0.021891]\n",
      "epoch:45 step:35621 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.005712]\n",
      "epoch:45 step:35622 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.002773]\n",
      "epoch:45 step:35623 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.002351]\n",
      "epoch:45 step:35624 [D loss: 0.000554, acc.: 100.00%] [G loss: 0.001113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35625 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.006520]\n",
      "epoch:45 step:35626 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000770]\n",
      "epoch:45 step:35627 [D loss: 0.010253, acc.: 100.00%] [G loss: 0.001905]\n",
      "epoch:45 step:35628 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.001915]\n",
      "epoch:45 step:35629 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000449]\n",
      "epoch:45 step:35630 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.081800]\n",
      "epoch:45 step:35631 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000817]\n",
      "epoch:45 step:35632 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000621]\n",
      "epoch:45 step:35633 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.003100]\n",
      "epoch:45 step:35634 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000623]\n",
      "epoch:45 step:35635 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.004085]\n",
      "epoch:45 step:35636 [D loss: 0.003477, acc.: 100.00%] [G loss: 0.000631]\n",
      "epoch:45 step:35637 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.001513]\n",
      "epoch:45 step:35638 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000628]\n",
      "epoch:45 step:35639 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000173]\n",
      "epoch:45 step:35640 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000199]\n",
      "epoch:45 step:35641 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000281]\n",
      "epoch:45 step:35642 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.000438]\n",
      "epoch:45 step:35643 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:45 step:35644 [D loss: 0.001506, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:45 step:35645 [D loss: 0.001307, acc.: 100.00%] [G loss: 0.009110]\n",
      "epoch:45 step:35646 [D loss: 0.543475, acc.: 70.31%] [G loss: 8.428435]\n",
      "epoch:45 step:35647 [D loss: 2.809119, acc.: 49.22%] [G loss: 1.206636]\n",
      "epoch:45 step:35648 [D loss: 0.851190, acc.: 71.09%] [G loss: 4.588241]\n",
      "epoch:45 step:35649 [D loss: 0.039627, acc.: 100.00%] [G loss: 1.680092]\n",
      "epoch:45 step:35650 [D loss: 0.736965, acc.: 75.00%] [G loss: 3.118659]\n",
      "epoch:45 step:35651 [D loss: 0.187474, acc.: 89.84%] [G loss: 2.594746]\n",
      "epoch:45 step:35652 [D loss: 0.025383, acc.: 99.22%] [G loss: 0.131094]\n",
      "epoch:45 step:35653 [D loss: 0.060883, acc.: 98.44%] [G loss: 2.029409]\n",
      "epoch:45 step:35654 [D loss: 0.049558, acc.: 100.00%] [G loss: 1.086325]\n",
      "epoch:45 step:35655 [D loss: 0.050808, acc.: 99.22%] [G loss: 1.016091]\n",
      "epoch:45 step:35656 [D loss: 0.021061, acc.: 100.00%] [G loss: 0.029379]\n",
      "epoch:45 step:35657 [D loss: 0.008091, acc.: 100.00%] [G loss: 1.222617]\n",
      "epoch:45 step:35658 [D loss: 0.041565, acc.: 99.22%] [G loss: 0.477326]\n",
      "epoch:45 step:35659 [D loss: 0.300303, acc.: 86.72%] [G loss: 1.923771]\n",
      "epoch:45 step:35660 [D loss: 0.053502, acc.: 96.88%] [G loss: 2.866106]\n",
      "epoch:45 step:35661 [D loss: 0.177100, acc.: 91.41%] [G loss: 1.304759]\n",
      "epoch:45 step:35662 [D loss: 0.041988, acc.: 99.22%] [G loss: 0.786477]\n",
      "epoch:45 step:35663 [D loss: 0.012213, acc.: 100.00%] [G loss: 0.734722]\n",
      "epoch:45 step:35664 [D loss: 0.056848, acc.: 100.00%] [G loss: 0.510453]\n",
      "epoch:45 step:35665 [D loss: 0.057008, acc.: 99.22%] [G loss: 1.300571]\n",
      "epoch:45 step:35666 [D loss: 0.016530, acc.: 100.00%] [G loss: 1.706802]\n",
      "epoch:45 step:35667 [D loss: 0.194734, acc.: 94.53%] [G loss: 3.018220]\n",
      "epoch:45 step:35668 [D loss: 0.070018, acc.: 98.44%] [G loss: 4.098750]\n",
      "epoch:45 step:35669 [D loss: 0.078833, acc.: 97.66%] [G loss: 4.242686]\n",
      "epoch:45 step:35670 [D loss: 0.108675, acc.: 96.88%] [G loss: 3.962181]\n",
      "epoch:45 step:35671 [D loss: 0.272333, acc.: 88.28%] [G loss: 2.110895]\n",
      "epoch:45 step:35672 [D loss: 0.239720, acc.: 88.28%] [G loss: 3.509707]\n",
      "epoch:45 step:35673 [D loss: 0.025336, acc.: 100.00%] [G loss: 4.380740]\n",
      "epoch:45 step:35674 [D loss: 0.186372, acc.: 92.97%] [G loss: 0.340837]\n",
      "epoch:45 step:35675 [D loss: 0.097266, acc.: 94.53%] [G loss: 5.302752]\n",
      "epoch:45 step:35676 [D loss: 0.035408, acc.: 99.22%] [G loss: 4.991179]\n",
      "epoch:45 step:35677 [D loss: 0.234466, acc.: 93.75%] [G loss: 3.052882]\n",
      "epoch:45 step:35678 [D loss: 0.077826, acc.: 96.88%] [G loss: 3.113755]\n",
      "epoch:45 step:35679 [D loss: 0.060771, acc.: 96.88%] [G loss: 3.047237]\n",
      "epoch:45 step:35680 [D loss: 0.110847, acc.: 98.44%] [G loss: 3.463405]\n",
      "epoch:45 step:35681 [D loss: 0.190731, acc.: 90.62%] [G loss: 4.242353]\n",
      "epoch:45 step:35682 [D loss: 0.540113, acc.: 76.56%] [G loss: 3.292482]\n",
      "epoch:45 step:35683 [D loss: 0.092162, acc.: 96.88%] [G loss: 0.082185]\n",
      "epoch:45 step:35684 [D loss: 0.005309, acc.: 100.00%] [G loss: 0.531591]\n",
      "epoch:45 step:35685 [D loss: 0.004850, acc.: 100.00%] [G loss: 4.377733]\n",
      "epoch:45 step:35686 [D loss: 0.003290, acc.: 100.00%] [G loss: 3.967883]\n",
      "epoch:45 step:35687 [D loss: 0.032655, acc.: 100.00%] [G loss: 3.582398]\n",
      "epoch:45 step:35688 [D loss: 0.005810, acc.: 100.00%] [G loss: 2.793712]\n",
      "epoch:45 step:35689 [D loss: 0.006017, acc.: 100.00%] [G loss: 2.029916]\n",
      "epoch:45 step:35690 [D loss: 0.012475, acc.: 100.00%] [G loss: 1.564995]\n",
      "epoch:45 step:35691 [D loss: 0.018545, acc.: 100.00%] [G loss: 1.112436]\n",
      "epoch:45 step:35692 [D loss: 0.577054, acc.: 68.75%] [G loss: 4.397161]\n",
      "epoch:45 step:35693 [D loss: 0.590745, acc.: 68.75%] [G loss: 4.014123]\n",
      "epoch:45 step:35694 [D loss: 0.023259, acc.: 100.00%] [G loss: 3.075236]\n",
      "epoch:45 step:35695 [D loss: 0.084342, acc.: 99.22%] [G loss: 2.065171]\n",
      "epoch:45 step:35696 [D loss: 0.215565, acc.: 91.41%] [G loss: 1.738763]\n",
      "epoch:45 step:35697 [D loss: 0.004362, acc.: 100.00%] [G loss: 4.684700]\n",
      "epoch:45 step:35698 [D loss: 0.116854, acc.: 94.53%] [G loss: 3.524256]\n",
      "epoch:45 step:35699 [D loss: 0.015322, acc.: 99.22%] [G loss: 2.523556]\n",
      "epoch:45 step:35700 [D loss: 0.011642, acc.: 100.00%] [G loss: 1.711713]\n",
      "epoch:45 step:35701 [D loss: 0.035634, acc.: 100.00%] [G loss: 1.479276]\n",
      "epoch:45 step:35702 [D loss: 0.124866, acc.: 97.66%] [G loss: 2.067690]\n",
      "epoch:45 step:35703 [D loss: 0.163645, acc.: 96.09%] [G loss: 2.072281]\n",
      "epoch:45 step:35704 [D loss: 0.084444, acc.: 99.22%] [G loss: 2.609795]\n",
      "epoch:45 step:35705 [D loss: 0.115458, acc.: 96.09%] [G loss: 2.131298]\n",
      "epoch:45 step:35706 [D loss: 0.254470, acc.: 89.84%] [G loss: 2.503531]\n",
      "epoch:45 step:35707 [D loss: 0.054594, acc.: 98.44%] [G loss: 2.636367]\n",
      "epoch:45 step:35708 [D loss: 0.282213, acc.: 89.06%] [G loss: 5.497390]\n",
      "epoch:45 step:35709 [D loss: 0.145739, acc.: 91.41%] [G loss: 5.205883]\n",
      "epoch:45 step:35710 [D loss: 0.129441, acc.: 95.31%] [G loss: 0.273377]\n",
      "epoch:45 step:35711 [D loss: 0.020544, acc.: 99.22%] [G loss: 3.851683]\n",
      "epoch:45 step:35712 [D loss: 0.005960, acc.: 100.00%] [G loss: 2.152685]\n",
      "epoch:45 step:35713 [D loss: 0.004691, acc.: 100.00%] [G loss: 0.959267]\n",
      "epoch:45 step:35714 [D loss: 0.007621, acc.: 100.00%] [G loss: 0.918590]\n",
      "epoch:45 step:35715 [D loss: 0.280574, acc.: 86.72%] [G loss: 2.382315]\n",
      "epoch:45 step:35716 [D loss: 0.296353, acc.: 85.16%] [G loss: 0.023876]\n",
      "epoch:45 step:35717 [D loss: 0.010479, acc.: 100.00%] [G loss: 0.004530]\n",
      "epoch:45 step:35718 [D loss: 0.042095, acc.: 97.66%] [G loss: 4.232181]\n",
      "epoch:45 step:35719 [D loss: 0.017600, acc.: 100.00%] [G loss: 2.360770]\n",
      "epoch:45 step:35720 [D loss: 0.004902, acc.: 100.00%] [G loss: 1.206164]\n",
      "epoch:45 step:35721 [D loss: 0.006639, acc.: 100.00%] [G loss: 1.488354]\n",
      "epoch:45 step:35722 [D loss: 0.006805, acc.: 100.00%] [G loss: 0.097949]\n",
      "epoch:45 step:35723 [D loss: 0.000783, acc.: 100.00%] [G loss: 0.009556]\n",
      "epoch:45 step:35724 [D loss: 0.001073, acc.: 100.00%] [G loss: 0.096326]\n",
      "epoch:45 step:35725 [D loss: 0.004288, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:45 step:35726 [D loss: 0.001197, acc.: 100.00%] [G loss: 0.005614]\n",
      "epoch:45 step:35727 [D loss: 0.002617, acc.: 100.00%] [G loss: 0.042929]\n",
      "epoch:45 step:35728 [D loss: 0.000766, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:45 step:35729 [D loss: 0.003514, acc.: 100.00%] [G loss: 0.000902]\n",
      "epoch:45 step:35730 [D loss: 0.123290, acc.: 94.53%] [G loss: 0.126974]\n",
      "epoch:45 step:35731 [D loss: 0.000969, acc.: 100.00%] [G loss: 1.101068]\n",
      "epoch:45 step:35732 [D loss: 0.016015, acc.: 100.00%] [G loss: 0.419428]\n",
      "epoch:45 step:35733 [D loss: 0.008885, acc.: 100.00%] [G loss: 0.187209]\n",
      "epoch:45 step:35734 [D loss: 0.012711, acc.: 100.00%] [G loss: 0.085430]\n",
      "epoch:45 step:35735 [D loss: 0.002090, acc.: 100.00%] [G loss: 0.017865]\n",
      "epoch:45 step:35736 [D loss: 0.006211, acc.: 100.00%] [G loss: 1.991607]\n",
      "epoch:45 step:35737 [D loss: 0.000649, acc.: 100.00%] [G loss: 0.025886]\n",
      "epoch:45 step:35738 [D loss: 0.015476, acc.: 99.22%] [G loss: 0.003475]\n",
      "epoch:45 step:35739 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.009936]\n",
      "epoch:45 step:35740 [D loss: 0.003142, acc.: 100.00%] [G loss: 0.005814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35741 [D loss: 0.006096, acc.: 100.00%] [G loss: 0.009741]\n",
      "epoch:45 step:35742 [D loss: 0.006851, acc.: 100.00%] [G loss: 0.006571]\n",
      "epoch:45 step:35743 [D loss: 0.207984, acc.: 89.84%] [G loss: 2.155715]\n",
      "epoch:45 step:35744 [D loss: 0.065910, acc.: 97.66%] [G loss: 3.018226]\n",
      "epoch:45 step:35745 [D loss: 0.234731, acc.: 89.84%] [G loss: 0.000670]\n",
      "epoch:45 step:35746 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.036399]\n",
      "epoch:45 step:35747 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.016370]\n",
      "epoch:45 step:35748 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.014093]\n",
      "epoch:45 step:35749 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.002397]\n",
      "epoch:45 step:35750 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.009377]\n",
      "epoch:45 step:35751 [D loss: 0.001698, acc.: 100.00%] [G loss: 0.001879]\n",
      "epoch:45 step:35752 [D loss: 0.032544, acc.: 99.22%] [G loss: 0.072158]\n",
      "epoch:45 step:35753 [D loss: 0.001523, acc.: 100.00%] [G loss: 0.022613]\n",
      "epoch:45 step:35754 [D loss: 0.024787, acc.: 100.00%] [G loss: 0.109978]\n",
      "epoch:45 step:35755 [D loss: 0.179702, acc.: 91.41%] [G loss: 2.881138]\n",
      "epoch:45 step:35756 [D loss: 0.003204, acc.: 100.00%] [G loss: 4.400105]\n",
      "epoch:45 step:35757 [D loss: 0.327820, acc.: 85.16%] [G loss: 2.454996]\n",
      "epoch:45 step:35758 [D loss: 0.313315, acc.: 85.94%] [G loss: 2.037284]\n",
      "epoch:45 step:35759 [D loss: 0.029556, acc.: 100.00%] [G loss: 3.795247]\n",
      "epoch:45 step:35760 [D loss: 0.007857, acc.: 100.00%] [G loss: 3.897368]\n",
      "epoch:45 step:35761 [D loss: 0.047511, acc.: 99.22%] [G loss: 3.824003]\n",
      "epoch:45 step:35762 [D loss: 0.253903, acc.: 88.28%] [G loss: 6.559682]\n",
      "epoch:45 step:35763 [D loss: 0.548285, acc.: 79.69%] [G loss: 5.643505]\n",
      "epoch:45 step:35764 [D loss: 0.053847, acc.: 99.22%] [G loss: 6.768068]\n",
      "epoch:45 step:35765 [D loss: 0.006271, acc.: 100.00%] [G loss: 6.301156]\n",
      "epoch:45 step:35766 [D loss: 0.007566, acc.: 100.00%] [G loss: 5.914314]\n",
      "epoch:45 step:35767 [D loss: 0.011698, acc.: 100.00%] [G loss: 5.339134]\n",
      "epoch:45 step:35768 [D loss: 0.127594, acc.: 96.09%] [G loss: 6.510329]\n",
      "epoch:45 step:35769 [D loss: 0.004541, acc.: 100.00%] [G loss: 6.862688]\n",
      "epoch:45 step:35770 [D loss: 0.021820, acc.: 99.22%] [G loss: 6.442226]\n",
      "epoch:45 step:35771 [D loss: 0.010702, acc.: 100.00%] [G loss: 5.299773]\n",
      "epoch:45 step:35772 [D loss: 0.131625, acc.: 94.53%] [G loss: 5.653207]\n",
      "epoch:45 step:35773 [D loss: 0.019538, acc.: 99.22%] [G loss: 6.815934]\n",
      "epoch:45 step:35774 [D loss: 0.020643, acc.: 99.22%] [G loss: 5.764759]\n",
      "epoch:45 step:35775 [D loss: 0.134773, acc.: 96.09%] [G loss: 5.160956]\n",
      "epoch:45 step:35776 [D loss: 0.107490, acc.: 92.97%] [G loss: 1.501985]\n",
      "epoch:45 step:35777 [D loss: 0.145399, acc.: 92.19%] [G loss: 6.801463]\n",
      "epoch:45 step:35778 [D loss: 0.023340, acc.: 98.44%] [G loss: 10.148824]\n",
      "epoch:45 step:35779 [D loss: 2.725307, acc.: 23.44%] [G loss: 8.560241]\n",
      "epoch:45 step:35780 [D loss: 1.767670, acc.: 56.25%] [G loss: 5.891811]\n",
      "epoch:45 step:35781 [D loss: 0.134666, acc.: 94.53%] [G loss: 3.653860]\n",
      "epoch:45 step:35782 [D loss: 0.065448, acc.: 99.22%] [G loss: 3.219290]\n",
      "epoch:45 step:35783 [D loss: 0.081065, acc.: 100.00%] [G loss: 2.677590]\n",
      "epoch:45 step:35784 [D loss: 0.126028, acc.: 96.09%] [G loss: 1.314156]\n",
      "epoch:45 step:35785 [D loss: 0.038945, acc.: 100.00%] [G loss: 1.234965]\n",
      "epoch:45 step:35786 [D loss: 0.085100, acc.: 96.09%] [G loss: 3.374763]\n",
      "epoch:45 step:35787 [D loss: 0.357147, acc.: 83.59%] [G loss: 1.114999]\n",
      "epoch:45 step:35788 [D loss: 0.145243, acc.: 96.09%] [G loss: 1.228982]\n",
      "epoch:45 step:35789 [D loss: 0.008442, acc.: 100.00%] [G loss: 0.941771]\n",
      "epoch:45 step:35790 [D loss: 0.154431, acc.: 95.31%] [G loss: 1.133533]\n",
      "epoch:45 step:35791 [D loss: 1.742884, acc.: 26.56%] [G loss: 4.677976]\n",
      "epoch:45 step:35792 [D loss: 0.163938, acc.: 93.75%] [G loss: 7.875051]\n",
      "epoch:45 step:35793 [D loss: 0.340963, acc.: 81.25%] [G loss: 4.435022]\n",
      "epoch:45 step:35794 [D loss: 0.213037, acc.: 90.62%] [G loss: 1.414949]\n",
      "epoch:45 step:35795 [D loss: 0.121687, acc.: 96.09%] [G loss: 0.722727]\n",
      "epoch:45 step:35796 [D loss: 0.523479, acc.: 73.44%] [G loss: 4.783877]\n",
      "epoch:45 step:35797 [D loss: 0.110418, acc.: 95.31%] [G loss: 5.466226]\n",
      "epoch:45 step:35798 [D loss: 0.074995, acc.: 96.88%] [G loss: 4.549932]\n",
      "epoch:45 step:35799 [D loss: 0.261314, acc.: 88.28%] [G loss: 2.422575]\n",
      "epoch:45 step:35800 [D loss: 0.519200, acc.: 71.88%] [G loss: 3.929601]\n",
      "epoch:45 step:35801 [D loss: 0.085071, acc.: 96.88%] [G loss: 4.801577]\n",
      "epoch:45 step:35802 [D loss: 0.206199, acc.: 91.41%] [G loss: 2.613404]\n",
      "epoch:45 step:35803 [D loss: 0.124407, acc.: 97.66%] [G loss: 2.104790]\n",
      "epoch:45 step:35804 [D loss: 0.097725, acc.: 99.22%] [G loss: 1.925833]\n",
      "epoch:45 step:35805 [D loss: 0.069767, acc.: 99.22%] [G loss: 3.351489]\n",
      "epoch:45 step:35806 [D loss: 0.218615, acc.: 92.19%] [G loss: 2.452864]\n",
      "epoch:45 step:35807 [D loss: 0.046939, acc.: 98.44%] [G loss: 0.479879]\n",
      "epoch:45 step:35808 [D loss: 0.073690, acc.: 99.22%] [G loss: 2.064301]\n",
      "epoch:45 step:35809 [D loss: 0.007636, acc.: 100.00%] [G loss: 0.623726]\n",
      "epoch:45 step:35810 [D loss: 0.311718, acc.: 81.25%] [G loss: 0.442140]\n",
      "epoch:45 step:35811 [D loss: 0.011073, acc.: 100.00%] [G loss: 1.976137]\n",
      "epoch:45 step:35812 [D loss: 0.042957, acc.: 99.22%] [G loss: 1.268742]\n",
      "epoch:45 step:35813 [D loss: 0.190150, acc.: 92.97%] [G loss: 0.470416]\n",
      "epoch:45 step:35814 [D loss: 0.137128, acc.: 94.53%] [G loss: 1.403134]\n",
      "epoch:45 step:35815 [D loss: 0.070597, acc.: 98.44%] [G loss: 0.252931]\n",
      "epoch:45 step:35816 [D loss: 0.019338, acc.: 100.00%] [G loss: 0.148043]\n",
      "epoch:45 step:35817 [D loss: 0.005668, acc.: 100.00%] [G loss: 0.285081]\n",
      "epoch:45 step:35818 [D loss: 0.317644, acc.: 84.38%] [G loss: 1.369270]\n",
      "epoch:45 step:35819 [D loss: 0.305075, acc.: 85.16%] [G loss: 1.319375]\n",
      "epoch:45 step:35820 [D loss: 0.094400, acc.: 96.09%] [G loss: 3.249127]\n",
      "epoch:45 step:35821 [D loss: 0.100545, acc.: 96.09%] [G loss: 0.739984]\n",
      "epoch:45 step:35822 [D loss: 0.002778, acc.: 100.00%] [G loss: 0.248507]\n",
      "epoch:45 step:35823 [D loss: 0.001153, acc.: 100.00%] [G loss: 0.100143]\n",
      "epoch:45 step:35824 [D loss: 0.005953, acc.: 100.00%] [G loss: 0.001848]\n",
      "epoch:45 step:35825 [D loss: 0.001924, acc.: 100.00%] [G loss: 0.106287]\n",
      "epoch:45 step:35826 [D loss: 0.004762, acc.: 100.00%] [G loss: 0.010065]\n",
      "epoch:45 step:35827 [D loss: 0.003153, acc.: 100.00%] [G loss: 0.040059]\n",
      "epoch:45 step:35828 [D loss: 0.007654, acc.: 100.00%] [G loss: 0.054227]\n",
      "epoch:45 step:35829 [D loss: 0.010077, acc.: 100.00%] [G loss: 0.000707]\n",
      "epoch:45 step:35830 [D loss: 0.004940, acc.: 100.00%] [G loss: 0.178106]\n",
      "epoch:45 step:35831 [D loss: 0.005687, acc.: 100.00%] [G loss: 0.036182]\n",
      "epoch:45 step:35832 [D loss: 0.048617, acc.: 99.22%] [G loss: 0.000244]\n",
      "epoch:45 step:35833 [D loss: 0.009059, acc.: 99.22%] [G loss: 0.166644]\n",
      "epoch:45 step:35834 [D loss: 0.011124, acc.: 100.00%] [G loss: 0.019405]\n",
      "epoch:45 step:35835 [D loss: 0.007119, acc.: 100.00%] [G loss: 0.057600]\n",
      "epoch:45 step:35836 [D loss: 0.005391, acc.: 100.00%] [G loss: 0.055201]\n",
      "epoch:45 step:35837 [D loss: 0.001334, acc.: 100.00%] [G loss: 0.006440]\n",
      "epoch:45 step:35838 [D loss: 0.001218, acc.: 100.00%] [G loss: 0.004658]\n",
      "epoch:45 step:35839 [D loss: 0.002630, acc.: 100.00%] [G loss: 0.015371]\n",
      "epoch:45 step:35840 [D loss: 0.006712, acc.: 100.00%] [G loss: 0.002900]\n",
      "epoch:45 step:35841 [D loss: 0.054084, acc.: 100.00%] [G loss: 0.021760]\n",
      "epoch:45 step:35842 [D loss: 0.030032, acc.: 100.00%] [G loss: 0.466680]\n",
      "epoch:45 step:35843 [D loss: 0.030403, acc.: 97.66%] [G loss: 0.233810]\n",
      "epoch:45 step:35844 [D loss: 0.017270, acc.: 100.00%] [G loss: 0.004298]\n",
      "epoch:45 step:35845 [D loss: 0.001797, acc.: 100.00%] [G loss: 0.114645]\n",
      "epoch:45 step:35846 [D loss: 0.004428, acc.: 100.00%] [G loss: 0.268961]\n",
      "epoch:45 step:35847 [D loss: 0.004860, acc.: 100.00%] [G loss: 0.067229]\n",
      "epoch:45 step:35848 [D loss: 0.028864, acc.: 99.22%] [G loss: 0.065758]\n",
      "epoch:45 step:35849 [D loss: 0.039242, acc.: 99.22%] [G loss: 0.000318]\n",
      "epoch:45 step:35850 [D loss: 0.007544, acc.: 99.22%] [G loss: 0.003698]\n",
      "epoch:45 step:35851 [D loss: 0.000632, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:45 step:35852 [D loss: 0.002412, acc.: 100.00%] [G loss: 0.078277]\n",
      "epoch:45 step:35853 [D loss: 0.001047, acc.: 100.00%] [G loss: 0.000710]\n",
      "epoch:45 step:35854 [D loss: 0.006391, acc.: 100.00%] [G loss: 0.000842]\n",
      "epoch:45 step:35855 [D loss: 0.003232, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:45 step:35856 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.001811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:45 step:35857 [D loss: 0.001021, acc.: 100.00%] [G loss: 0.004363]\n",
      "epoch:45 step:35858 [D loss: 0.071836, acc.: 97.66%] [G loss: 0.000063]\n",
      "epoch:45 step:35859 [D loss: 0.004286, acc.: 100.00%] [G loss: 0.000554]\n",
      "epoch:45 step:35860 [D loss: 0.014775, acc.: 100.00%] [G loss: 0.000176]\n",
      "epoch:45 step:35861 [D loss: 0.008504, acc.: 100.00%] [G loss: 0.002579]\n",
      "epoch:45 step:35862 [D loss: 0.034148, acc.: 100.00%] [G loss: 0.000605]\n",
      "epoch:45 step:35863 [D loss: 0.007677, acc.: 100.00%] [G loss: 0.104350]\n",
      "epoch:45 step:35864 [D loss: 0.001103, acc.: 100.00%] [G loss: 0.337986]\n",
      "epoch:45 step:35865 [D loss: 0.006241, acc.: 100.00%] [G loss: 0.389077]\n",
      "epoch:45 step:35866 [D loss: 0.001060, acc.: 100.00%] [G loss: 0.024521]\n",
      "epoch:45 step:35867 [D loss: 0.003368, acc.: 100.00%] [G loss: 0.043087]\n",
      "epoch:45 step:35868 [D loss: 0.008540, acc.: 100.00%] [G loss: 0.063838]\n",
      "epoch:45 step:35869 [D loss: 0.001978, acc.: 100.00%] [G loss: 0.011727]\n",
      "epoch:45 step:35870 [D loss: 0.005179, acc.: 100.00%] [G loss: 0.000601]\n",
      "epoch:45 step:35871 [D loss: 0.000291, acc.: 100.00%] [G loss: 0.018327]\n",
      "epoch:45 step:35872 [D loss: 0.002098, acc.: 100.00%] [G loss: 0.002353]\n",
      "epoch:45 step:35873 [D loss: 0.023773, acc.: 100.00%] [G loss: 0.025367]\n",
      "epoch:45 step:35874 [D loss: 0.001657, acc.: 100.00%] [G loss: 1.703655]\n",
      "epoch:45 step:35875 [D loss: 0.017650, acc.: 100.00%] [G loss: 0.041820]\n",
      "epoch:45 step:35876 [D loss: 0.036685, acc.: 100.00%] [G loss: 0.479044]\n",
      "epoch:45 step:35877 [D loss: 0.036289, acc.: 99.22%] [G loss: 0.575907]\n",
      "epoch:45 step:35878 [D loss: 0.092788, acc.: 98.44%] [G loss: 3.203800]\n",
      "epoch:45 step:35879 [D loss: 4.353503, acc.: 12.50%] [G loss: 7.775262]\n",
      "epoch:45 step:35880 [D loss: 1.349736, acc.: 51.56%] [G loss: 5.948983]\n",
      "epoch:45 step:35881 [D loss: 0.680148, acc.: 65.62%] [G loss: 3.320726]\n",
      "epoch:45 step:35882 [D loss: 0.212823, acc.: 92.19%] [G loss: 2.221969]\n",
      "epoch:45 step:35883 [D loss: 0.279654, acc.: 86.72%] [G loss: 3.191385]\n",
      "epoch:45 step:35884 [D loss: 0.115512, acc.: 97.66%] [G loss: 3.131115]\n",
      "epoch:45 step:35885 [D loss: 0.093584, acc.: 99.22%] [G loss: 3.424444]\n",
      "epoch:45 step:35886 [D loss: 0.134234, acc.: 96.88%] [G loss: 1.477671]\n",
      "epoch:45 step:35887 [D loss: 0.130999, acc.: 94.53%] [G loss: 2.835130]\n",
      "epoch:45 step:35888 [D loss: 0.042356, acc.: 99.22%] [G loss: 0.533121]\n",
      "epoch:45 step:35889 [D loss: 0.085108, acc.: 100.00%] [G loss: 1.923229]\n",
      "epoch:45 step:35890 [D loss: 0.027114, acc.: 99.22%] [G loss: 0.966909]\n",
      "epoch:45 step:35891 [D loss: 0.028430, acc.: 99.22%] [G loss: 0.899550]\n",
      "epoch:45 step:35892 [D loss: 0.052382, acc.: 98.44%] [G loss: 0.304837]\n",
      "epoch:45 step:35893 [D loss: 0.020229, acc.: 100.00%] [G loss: 0.317162]\n",
      "epoch:45 step:35894 [D loss: 0.024587, acc.: 100.00%] [G loss: 0.136948]\n",
      "epoch:45 step:35895 [D loss: 0.008104, acc.: 100.00%] [G loss: 0.064788]\n",
      "epoch:45 step:35896 [D loss: 0.012373, acc.: 100.00%] [G loss: 0.057922]\n",
      "epoch:45 step:35897 [D loss: 0.051380, acc.: 97.66%] [G loss: 0.069400]\n",
      "epoch:45 step:35898 [D loss: 0.136425, acc.: 94.53%] [G loss: 0.819647]\n",
      "epoch:45 step:35899 [D loss: 0.043760, acc.: 98.44%] [G loss: 1.601489]\n",
      "epoch:45 step:35900 [D loss: 0.069221, acc.: 98.44%] [G loss: 0.379681]\n",
      "epoch:45 step:35901 [D loss: 0.029393, acc.: 100.00%] [G loss: 0.148439]\n",
      "epoch:45 step:35902 [D loss: 0.041770, acc.: 99.22%] [G loss: 0.117009]\n",
      "epoch:45 step:35903 [D loss: 0.130956, acc.: 99.22%] [G loss: 0.902460]\n",
      "epoch:45 step:35904 [D loss: 0.064449, acc.: 99.22%] [G loss: 0.621750]\n",
      "epoch:45 step:35905 [D loss: 0.096158, acc.: 98.44%] [G loss: 0.569098]\n",
      "epoch:45 step:35906 [D loss: 0.057219, acc.: 98.44%] [G loss: 0.574348]\n",
      "epoch:45 step:35907 [D loss: 0.272055, acc.: 89.06%] [G loss: 0.037625]\n",
      "epoch:45 step:35908 [D loss: 0.013354, acc.: 100.00%] [G loss: 0.062566]\n",
      "epoch:45 step:35909 [D loss: 0.002775, acc.: 100.00%] [G loss: 0.021766]\n",
      "epoch:45 step:35910 [D loss: 0.006433, acc.: 100.00%] [G loss: 0.009256]\n",
      "epoch:45 step:35911 [D loss: 0.293328, acc.: 90.62%] [G loss: 0.274558]\n",
      "epoch:45 step:35912 [D loss: 0.114594, acc.: 95.31%] [G loss: 0.850889]\n",
      "epoch:45 step:35913 [D loss: 0.093341, acc.: 96.09%] [G loss: 0.227147]\n",
      "epoch:45 step:35914 [D loss: 0.002498, acc.: 100.00%] [G loss: 0.074724]\n",
      "epoch:45 step:35915 [D loss: 0.010815, acc.: 100.00%] [G loss: 0.139729]\n",
      "epoch:45 step:35916 [D loss: 0.063308, acc.: 99.22%] [G loss: 0.026426]\n",
      "epoch:45 step:35917 [D loss: 0.052378, acc.: 97.66%] [G loss: 0.034243]\n",
      "epoch:45 step:35918 [D loss: 0.151700, acc.: 94.53%] [G loss: 0.665122]\n",
      "epoch:45 step:35919 [D loss: 0.021442, acc.: 100.00%] [G loss: 0.312312]\n",
      "epoch:45 step:35920 [D loss: 0.067826, acc.: 98.44%] [G loss: 1.385652]\n",
      "epoch:45 step:35921 [D loss: 0.028346, acc.: 99.22%] [G loss: 0.328144]\n",
      "epoch:45 step:35922 [D loss: 0.037844, acc.: 97.66%] [G loss: 0.086910]\n",
      "epoch:45 step:35923 [D loss: 0.004521, acc.: 100.00%] [G loss: 0.032842]\n",
      "epoch:45 step:35924 [D loss: 0.020414, acc.: 100.00%] [G loss: 0.073797]\n",
      "epoch:45 step:35925 [D loss: 0.014586, acc.: 100.00%] [G loss: 0.037122]\n",
      "epoch:45 step:35926 [D loss: 0.025257, acc.: 99.22%] [G loss: 0.011686]\n",
      "epoch:46 step:35927 [D loss: 0.006021, acc.: 100.00%] [G loss: 0.028178]\n",
      "epoch:46 step:35928 [D loss: 0.040526, acc.: 99.22%] [G loss: 0.026038]\n",
      "epoch:46 step:35929 [D loss: 0.035961, acc.: 98.44%] [G loss: 0.086458]\n",
      "epoch:46 step:35930 [D loss: 0.018165, acc.: 100.00%] [G loss: 0.030205]\n",
      "epoch:46 step:35931 [D loss: 0.008223, acc.: 100.00%] [G loss: 0.056385]\n",
      "epoch:46 step:35932 [D loss: 0.025764, acc.: 100.00%] [G loss: 0.103045]\n",
      "epoch:46 step:35933 [D loss: 0.017951, acc.: 100.00%] [G loss: 0.313756]\n",
      "epoch:46 step:35934 [D loss: 0.034975, acc.: 100.00%] [G loss: 0.474666]\n",
      "epoch:46 step:35935 [D loss: 0.095647, acc.: 98.44%] [G loss: 0.236248]\n",
      "epoch:46 step:35936 [D loss: 0.042975, acc.: 99.22%] [G loss: 0.036869]\n",
      "epoch:46 step:35937 [D loss: 0.027516, acc.: 99.22%] [G loss: 0.306812]\n",
      "epoch:46 step:35938 [D loss: 0.003874, acc.: 100.00%] [G loss: 0.236559]\n",
      "epoch:46 step:35939 [D loss: 0.001367, acc.: 100.00%] [G loss: 0.202207]\n",
      "epoch:46 step:35940 [D loss: 0.027951, acc.: 99.22%] [G loss: 0.013316]\n",
      "epoch:46 step:35941 [D loss: 0.068993, acc.: 98.44%] [G loss: 0.007052]\n",
      "epoch:46 step:35942 [D loss: 0.009412, acc.: 100.00%] [G loss: 0.002825]\n",
      "epoch:46 step:35943 [D loss: 0.001720, acc.: 100.00%] [G loss: 0.001834]\n",
      "epoch:46 step:35944 [D loss: 0.002558, acc.: 100.00%] [G loss: 0.001336]\n",
      "epoch:46 step:35945 [D loss: 0.000723, acc.: 100.00%] [G loss: 0.000566]\n",
      "epoch:46 step:35946 [D loss: 0.001923, acc.: 100.00%] [G loss: 0.001411]\n",
      "epoch:46 step:35947 [D loss: 0.001369, acc.: 100.00%] [G loss: 0.001813]\n",
      "epoch:46 step:35948 [D loss: 0.006362, acc.: 100.00%] [G loss: 0.143901]\n",
      "epoch:46 step:35949 [D loss: 0.002424, acc.: 100.00%] [G loss: 0.000352]\n",
      "epoch:46 step:35950 [D loss: 0.000802, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:46 step:35951 [D loss: 0.002184, acc.: 100.00%] [G loss: 0.001759]\n",
      "epoch:46 step:35952 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.001366]\n",
      "epoch:46 step:35953 [D loss: 0.000465, acc.: 100.00%] [G loss: 0.003389]\n",
      "epoch:46 step:35954 [D loss: 0.002935, acc.: 100.00%] [G loss: 0.001232]\n",
      "epoch:46 step:35955 [D loss: 0.003989, acc.: 100.00%] [G loss: 0.000903]\n",
      "epoch:46 step:35956 [D loss: 0.003726, acc.: 100.00%] [G loss: 0.000623]\n",
      "epoch:46 step:35957 [D loss: 0.001118, acc.: 100.00%] [G loss: 0.001579]\n",
      "epoch:46 step:35958 [D loss: 0.005881, acc.: 100.00%] [G loss: 0.001808]\n",
      "epoch:46 step:35959 [D loss: 0.005179, acc.: 100.00%] [G loss: 0.001145]\n",
      "epoch:46 step:35960 [D loss: 0.011366, acc.: 99.22%] [G loss: 0.000213]\n",
      "epoch:46 step:35961 [D loss: 0.006622, acc.: 100.00%] [G loss: 0.002581]\n",
      "epoch:46 step:35962 [D loss: 0.008154, acc.: 100.00%] [G loss: 0.000907]\n",
      "epoch:46 step:35963 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.000693]\n",
      "epoch:46 step:35964 [D loss: 0.001297, acc.: 100.00%] [G loss: 0.000886]\n",
      "epoch:46 step:35965 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000656]\n",
      "epoch:46 step:35966 [D loss: 0.000737, acc.: 100.00%] [G loss: 0.000592]\n",
      "epoch:46 step:35967 [D loss: 0.002732, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:46 step:35968 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:46 step:35969 [D loss: 0.005406, acc.: 100.00%] [G loss: 0.000574]\n",
      "epoch:46 step:35970 [D loss: 0.004380, acc.: 100.00%] [G loss: 0.002224]\n",
      "epoch:46 step:35971 [D loss: 0.000995, acc.: 100.00%] [G loss: 0.000369]\n",
      "epoch:46 step:35972 [D loss: 0.004840, acc.: 100.00%] [G loss: 0.000575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:35973 [D loss: 0.002676, acc.: 100.00%] [G loss: 0.000458]\n",
      "epoch:46 step:35974 [D loss: 0.045706, acc.: 99.22%] [G loss: 0.002949]\n",
      "epoch:46 step:35975 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.002411]\n",
      "epoch:46 step:35976 [D loss: 0.004484, acc.: 100.00%] [G loss: 0.009344]\n",
      "epoch:46 step:35977 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.004428]\n",
      "epoch:46 step:35978 [D loss: 0.014152, acc.: 100.00%] [G loss: 0.928810]\n",
      "epoch:46 step:35979 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.001692]\n",
      "epoch:46 step:35980 [D loss: 0.006460, acc.: 100.00%] [G loss: 0.000988]\n",
      "epoch:46 step:35981 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.000685]\n",
      "epoch:46 step:35982 [D loss: 0.020165, acc.: 99.22%] [G loss: 0.001270]\n",
      "epoch:46 step:35983 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.000548]\n",
      "epoch:46 step:35984 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.007029]\n",
      "epoch:46 step:35985 [D loss: 0.000692, acc.: 100.00%] [G loss: 0.000733]\n",
      "epoch:46 step:35986 [D loss: 0.005221, acc.: 100.00%] [G loss: 0.000994]\n",
      "epoch:46 step:35987 [D loss: 0.005866, acc.: 100.00%] [G loss: 0.000800]\n",
      "epoch:46 step:35988 [D loss: 0.000960, acc.: 100.00%] [G loss: 0.001977]\n",
      "epoch:46 step:35989 [D loss: 0.001695, acc.: 100.00%] [G loss: 0.004274]\n",
      "epoch:46 step:35990 [D loss: 0.000748, acc.: 100.00%] [G loss: 0.006590]\n",
      "epoch:46 step:35991 [D loss: 0.048268, acc.: 98.44%] [G loss: 0.000520]\n",
      "epoch:46 step:35992 [D loss: 0.010220, acc.: 100.00%] [G loss: 0.000837]\n",
      "epoch:46 step:35993 [D loss: 0.010801, acc.: 100.00%] [G loss: 0.000343]\n",
      "epoch:46 step:35994 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.001092]\n",
      "epoch:46 step:35995 [D loss: 0.002310, acc.: 100.00%] [G loss: 0.002641]\n",
      "epoch:46 step:35996 [D loss: 0.142615, acc.: 94.53%] [G loss: 1.206023]\n",
      "epoch:46 step:35997 [D loss: 0.033815, acc.: 98.44%] [G loss: 1.746707]\n",
      "epoch:46 step:35998 [D loss: 0.027282, acc.: 99.22%] [G loss: 0.634275]\n",
      "epoch:46 step:35999 [D loss: 0.036637, acc.: 98.44%] [G loss: 0.312755]\n",
      "epoch:46 step:36000 [D loss: 0.431618, acc.: 76.56%] [G loss: 5.469277]\n",
      "epoch:46 step:36001 [D loss: 0.520509, acc.: 76.56%] [G loss: 3.346495]\n",
      "epoch:46 step:36002 [D loss: 0.790616, acc.: 67.19%] [G loss: 4.475729]\n",
      "epoch:46 step:36003 [D loss: 0.411317, acc.: 83.59%] [G loss: 3.623961]\n",
      "epoch:46 step:36004 [D loss: 0.049554, acc.: 98.44%] [G loss: 3.383819]\n",
      "epoch:46 step:36005 [D loss: 0.066418, acc.: 98.44%] [G loss: 2.877345]\n",
      "epoch:46 step:36006 [D loss: 0.040894, acc.: 99.22%] [G loss: 4.066348]\n",
      "epoch:46 step:36007 [D loss: 0.055220, acc.: 99.22%] [G loss: 2.365402]\n",
      "epoch:46 step:36008 [D loss: 0.151640, acc.: 95.31%] [G loss: 2.104343]\n",
      "epoch:46 step:36009 [D loss: 0.034870, acc.: 99.22%] [G loss: 2.451838]\n",
      "epoch:46 step:36010 [D loss: 0.095559, acc.: 99.22%] [G loss: 2.411967]\n",
      "epoch:46 step:36011 [D loss: 0.165873, acc.: 94.53%] [G loss: 2.247398]\n",
      "epoch:46 step:36012 [D loss: 0.097679, acc.: 98.44%] [G loss: 0.943788]\n",
      "epoch:46 step:36013 [D loss: 0.015122, acc.: 99.22%] [G loss: 4.338754]\n",
      "epoch:46 step:36014 [D loss: 0.095735, acc.: 96.88%] [G loss: 0.218463]\n",
      "epoch:46 step:36015 [D loss: 0.023168, acc.: 100.00%] [G loss: 0.025403]\n",
      "epoch:46 step:36016 [D loss: 0.259966, acc.: 86.72%] [G loss: 0.103579]\n",
      "epoch:46 step:36017 [D loss: 0.012656, acc.: 100.00%] [G loss: 3.116316]\n",
      "epoch:46 step:36018 [D loss: 0.005550, acc.: 100.00%] [G loss: 2.493002]\n",
      "epoch:46 step:36019 [D loss: 0.027211, acc.: 99.22%] [G loss: 1.497576]\n",
      "epoch:46 step:36020 [D loss: 0.015054, acc.: 99.22%] [G loss: 1.004433]\n",
      "epoch:46 step:36021 [D loss: 0.003483, acc.: 100.00%] [G loss: 2.512338]\n",
      "epoch:46 step:36022 [D loss: 0.015830, acc.: 100.00%] [G loss: 0.141338]\n",
      "epoch:46 step:36023 [D loss: 0.013643, acc.: 100.00%] [G loss: 0.284327]\n",
      "epoch:46 step:36024 [D loss: 0.067087, acc.: 96.88%] [G loss: 0.331795]\n",
      "epoch:46 step:36025 [D loss: 0.031485, acc.: 99.22%] [G loss: 0.322413]\n",
      "epoch:46 step:36026 [D loss: 0.009418, acc.: 100.00%] [G loss: 0.376558]\n",
      "epoch:46 step:36027 [D loss: 0.013371, acc.: 100.00%] [G loss: 0.255201]\n",
      "epoch:46 step:36028 [D loss: 0.003957, acc.: 100.00%] [G loss: 0.159428]\n",
      "epoch:46 step:36029 [D loss: 0.009698, acc.: 100.00%] [G loss: 0.088244]\n",
      "epoch:46 step:36030 [D loss: 0.003423, acc.: 100.00%] [G loss: 0.018091]\n",
      "epoch:46 step:36031 [D loss: 0.012585, acc.: 100.00%] [G loss: 0.460472]\n",
      "epoch:46 step:36032 [D loss: 0.003222, acc.: 100.00%] [G loss: 0.114050]\n",
      "epoch:46 step:36033 [D loss: 0.005079, acc.: 100.00%] [G loss: 0.023609]\n",
      "epoch:46 step:36034 [D loss: 0.008875, acc.: 100.00%] [G loss: 0.428302]\n",
      "epoch:46 step:36035 [D loss: 0.108384, acc.: 97.66%] [G loss: 0.216728]\n",
      "epoch:46 step:36036 [D loss: 0.040043, acc.: 99.22%] [G loss: 0.753716]\n",
      "epoch:46 step:36037 [D loss: 0.228803, acc.: 89.84%] [G loss: 0.029770]\n",
      "epoch:46 step:36038 [D loss: 0.001866, acc.: 100.00%] [G loss: 0.009638]\n",
      "epoch:46 step:36039 [D loss: 0.000781, acc.: 100.00%] [G loss: 0.033193]\n",
      "epoch:46 step:36040 [D loss: 0.006059, acc.: 100.00%] [G loss: 0.036582]\n",
      "epoch:46 step:36041 [D loss: 0.001930, acc.: 100.00%] [G loss: 0.008030]\n",
      "epoch:46 step:36042 [D loss: 0.002262, acc.: 100.00%] [G loss: 0.001823]\n",
      "epoch:46 step:36043 [D loss: 0.002750, acc.: 100.00%] [G loss: 0.017809]\n",
      "epoch:46 step:36044 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.006570]\n",
      "epoch:46 step:36045 [D loss: 0.002248, acc.: 100.00%] [G loss: 0.002528]\n",
      "epoch:46 step:36046 [D loss: 0.025786, acc.: 100.00%] [G loss: 0.007206]\n",
      "epoch:46 step:36047 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.012706]\n",
      "epoch:46 step:36048 [D loss: 0.003912, acc.: 100.00%] [G loss: 0.012090]\n",
      "epoch:46 step:36049 [D loss: 0.002230, acc.: 100.00%] [G loss: 0.059725]\n",
      "epoch:46 step:36050 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.035067]\n",
      "epoch:46 step:36051 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.042753]\n",
      "epoch:46 step:36052 [D loss: 0.000394, acc.: 100.00%] [G loss: 0.376439]\n",
      "epoch:46 step:36053 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.031506]\n",
      "epoch:46 step:36054 [D loss: 0.001866, acc.: 100.00%] [G loss: 0.004065]\n",
      "epoch:46 step:36055 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.008030]\n",
      "epoch:46 step:36056 [D loss: 0.000542, acc.: 100.00%] [G loss: 0.047067]\n",
      "epoch:46 step:36057 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.030488]\n",
      "epoch:46 step:36058 [D loss: 0.001712, acc.: 100.00%] [G loss: 0.004781]\n",
      "epoch:46 step:36059 [D loss: 0.004417, acc.: 100.00%] [G loss: 0.352977]\n",
      "epoch:46 step:36060 [D loss: 0.002937, acc.: 100.00%] [G loss: 0.039941]\n",
      "epoch:46 step:36061 [D loss: 0.004336, acc.: 100.00%] [G loss: 0.004313]\n",
      "epoch:46 step:36062 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.006763]\n",
      "epoch:46 step:36063 [D loss: 0.003461, acc.: 100.00%] [G loss: 0.014510]\n",
      "epoch:46 step:36064 [D loss: 0.000588, acc.: 100.00%] [G loss: 0.011147]\n",
      "epoch:46 step:36065 [D loss: 0.004524, acc.: 100.00%] [G loss: 0.002641]\n",
      "epoch:46 step:36066 [D loss: 0.004357, acc.: 100.00%] [G loss: 0.021012]\n",
      "epoch:46 step:36067 [D loss: 0.002060, acc.: 100.00%] [G loss: 0.007439]\n",
      "epoch:46 step:36068 [D loss: 0.002303, acc.: 100.00%] [G loss: 0.010953]\n",
      "epoch:46 step:36069 [D loss: 0.001735, acc.: 100.00%] [G loss: 0.006082]\n",
      "epoch:46 step:36070 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.013053]\n",
      "epoch:46 step:36071 [D loss: 0.001210, acc.: 100.00%] [G loss: 0.015588]\n",
      "epoch:46 step:36072 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.003021]\n",
      "epoch:46 step:36073 [D loss: 0.001205, acc.: 100.00%] [G loss: 0.006237]\n",
      "epoch:46 step:36074 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.019048]\n",
      "epoch:46 step:36075 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.009921]\n",
      "epoch:46 step:36076 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.004407]\n",
      "epoch:46 step:36077 [D loss: 0.000507, acc.: 100.00%] [G loss: 0.010817]\n",
      "epoch:46 step:36078 [D loss: 0.001028, acc.: 100.00%] [G loss: 0.011519]\n",
      "epoch:46 step:36079 [D loss: 0.002291, acc.: 100.00%] [G loss: 0.005232]\n",
      "epoch:46 step:36080 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.003961]\n",
      "epoch:46 step:36081 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.004590]\n",
      "epoch:46 step:36082 [D loss: 0.000648, acc.: 100.00%] [G loss: 0.418983]\n",
      "epoch:46 step:36083 [D loss: 0.002045, acc.: 100.00%] [G loss: 0.000835]\n",
      "epoch:46 step:36084 [D loss: 0.005529, acc.: 100.00%] [G loss: 0.002932]\n",
      "epoch:46 step:36085 [D loss: 0.000622, acc.: 100.00%] [G loss: 0.003912]\n",
      "epoch:46 step:36086 [D loss: 0.005922, acc.: 100.00%] [G loss: 0.009769]\n",
      "epoch:46 step:36087 [D loss: 0.014311, acc.: 100.00%] [G loss: 0.005331]\n",
      "epoch:46 step:36088 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.003591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36089 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.007490]\n",
      "epoch:46 step:36090 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.020822]\n",
      "epoch:46 step:36091 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.006882]\n",
      "epoch:46 step:36092 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.014993]\n",
      "epoch:46 step:36093 [D loss: 0.000157, acc.: 100.00%] [G loss: 0.006759]\n",
      "epoch:46 step:36094 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.006251]\n",
      "epoch:46 step:36095 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.014749]\n",
      "epoch:46 step:36096 [D loss: 0.000463, acc.: 100.00%] [G loss: 0.004066]\n",
      "epoch:46 step:36097 [D loss: 0.000765, acc.: 100.00%] [G loss: 0.035705]\n",
      "epoch:46 step:36098 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.010889]\n",
      "epoch:46 step:36099 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.009964]\n",
      "epoch:46 step:36100 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.023014]\n",
      "epoch:46 step:36101 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.002940]\n",
      "epoch:46 step:36102 [D loss: 0.004182, acc.: 100.00%] [G loss: 0.008338]\n",
      "epoch:46 step:36103 [D loss: 0.002930, acc.: 100.00%] [G loss: 0.010897]\n",
      "epoch:46 step:36104 [D loss: 0.000713, acc.: 100.00%] [G loss: 0.001534]\n",
      "epoch:46 step:36105 [D loss: 0.005225, acc.: 100.00%] [G loss: 0.005853]\n",
      "epoch:46 step:36106 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.003011]\n",
      "epoch:46 step:36107 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.002122]\n",
      "epoch:46 step:36108 [D loss: 0.031566, acc.: 100.00%] [G loss: 0.050806]\n",
      "epoch:46 step:36109 [D loss: 0.001324, acc.: 100.00%] [G loss: 0.037495]\n",
      "epoch:46 step:36110 [D loss: 0.020338, acc.: 99.22%] [G loss: 0.523261]\n",
      "epoch:46 step:36111 [D loss: 0.000468, acc.: 100.00%] [G loss: 0.082727]\n",
      "epoch:46 step:36112 [D loss: 0.000415, acc.: 100.00%] [G loss: 0.009153]\n",
      "epoch:46 step:36113 [D loss: 0.005964, acc.: 100.00%] [G loss: 0.012388]\n",
      "epoch:46 step:36114 [D loss: 0.002211, acc.: 100.00%] [G loss: 0.017852]\n",
      "epoch:46 step:36115 [D loss: 0.001950, acc.: 100.00%] [G loss: 0.009521]\n",
      "epoch:46 step:36116 [D loss: 0.018259, acc.: 99.22%] [G loss: 0.056363]\n",
      "epoch:46 step:36117 [D loss: 0.012542, acc.: 100.00%] [G loss: 0.013686]\n",
      "epoch:46 step:36118 [D loss: 0.002153, acc.: 100.00%] [G loss: 0.012177]\n",
      "epoch:46 step:36119 [D loss: 0.069302, acc.: 100.00%] [G loss: 1.078786]\n",
      "epoch:46 step:36120 [D loss: 0.024054, acc.: 99.22%] [G loss: 4.169536]\n",
      "epoch:46 step:36121 [D loss: 0.017485, acc.: 100.00%] [G loss: 0.487557]\n",
      "epoch:46 step:36122 [D loss: 0.007749, acc.: 100.00%] [G loss: 0.731020]\n",
      "epoch:46 step:36123 [D loss: 0.311661, acc.: 87.50%] [G loss: 5.324397]\n",
      "epoch:46 step:36124 [D loss: 0.743521, acc.: 72.66%] [G loss: 1.484098]\n",
      "epoch:46 step:36125 [D loss: 0.062769, acc.: 98.44%] [G loss: 0.069609]\n",
      "epoch:46 step:36126 [D loss: 0.006590, acc.: 100.00%] [G loss: 0.065623]\n",
      "epoch:46 step:36127 [D loss: 0.010335, acc.: 100.00%] [G loss: 3.243316]\n",
      "epoch:46 step:36128 [D loss: 0.010327, acc.: 100.00%] [G loss: 2.452192]\n",
      "epoch:46 step:36129 [D loss: 0.007159, acc.: 100.00%] [G loss: 2.750961]\n",
      "epoch:46 step:36130 [D loss: 0.016699, acc.: 99.22%] [G loss: 1.396482]\n",
      "epoch:46 step:36131 [D loss: 0.017243, acc.: 100.00%] [G loss: 0.184745]\n",
      "epoch:46 step:36132 [D loss: 0.010190, acc.: 100.00%] [G loss: 0.143140]\n",
      "epoch:46 step:36133 [D loss: 0.024030, acc.: 99.22%] [G loss: 2.891804]\n",
      "epoch:46 step:36134 [D loss: 0.123430, acc.: 95.31%] [G loss: 0.351894]\n",
      "epoch:46 step:36135 [D loss: 0.092275, acc.: 96.09%] [G loss: 0.251786]\n",
      "epoch:46 step:36136 [D loss: 0.032405, acc.: 98.44%] [G loss: 0.451632]\n",
      "epoch:46 step:36137 [D loss: 0.003599, acc.: 100.00%] [G loss: 1.754726]\n",
      "epoch:46 step:36138 [D loss: 0.023062, acc.: 100.00%] [G loss: 0.551274]\n",
      "epoch:46 step:36139 [D loss: 0.061576, acc.: 99.22%] [G loss: 0.986509]\n",
      "epoch:46 step:36140 [D loss: 0.262402, acc.: 89.84%] [G loss: 0.076126]\n",
      "epoch:46 step:36141 [D loss: 0.010112, acc.: 100.00%] [G loss: 0.262412]\n",
      "epoch:46 step:36142 [D loss: 0.048228, acc.: 98.44%] [G loss: 0.056482]\n",
      "epoch:46 step:36143 [D loss: 0.006380, acc.: 100.00%] [G loss: 0.352416]\n",
      "epoch:46 step:36144 [D loss: 0.142113, acc.: 97.66%] [G loss: 1.407960]\n",
      "epoch:46 step:36145 [D loss: 0.070092, acc.: 98.44%] [G loss: 2.366178]\n",
      "epoch:46 step:36146 [D loss: 0.320064, acc.: 87.50%] [G loss: 6.428690]\n",
      "epoch:46 step:36147 [D loss: 0.035163, acc.: 98.44%] [G loss: 7.232582]\n",
      "epoch:46 step:36148 [D loss: 0.631030, acc.: 78.12%] [G loss: 2.391402]\n",
      "epoch:46 step:36149 [D loss: 0.378295, acc.: 81.25%] [G loss: 6.194035]\n",
      "epoch:46 step:36150 [D loss: 0.064037, acc.: 97.66%] [G loss: 2.300867]\n",
      "epoch:46 step:36151 [D loss: 0.229101, acc.: 87.50%] [G loss: 3.942872]\n",
      "epoch:46 step:36152 [D loss: 0.143572, acc.: 94.53%] [G loss: 3.557272]\n",
      "epoch:46 step:36153 [D loss: 0.037413, acc.: 98.44%] [G loss: 4.389915]\n",
      "epoch:46 step:36154 [D loss: 0.026367, acc.: 99.22%] [G loss: 2.987695]\n",
      "epoch:46 step:36155 [D loss: 0.339530, acc.: 86.72%] [G loss: 5.405272]\n",
      "epoch:46 step:36156 [D loss: 0.089645, acc.: 96.88%] [G loss: 1.860555]\n",
      "epoch:46 step:36157 [D loss: 0.027114, acc.: 100.00%] [G loss: 0.587836]\n",
      "epoch:46 step:36158 [D loss: 0.112842, acc.: 95.31%] [G loss: 0.047976]\n",
      "epoch:46 step:36159 [D loss: 0.597029, acc.: 69.53%] [G loss: 5.639090]\n",
      "epoch:46 step:36160 [D loss: 0.743294, acc.: 70.31%] [G loss: 1.619337]\n",
      "epoch:46 step:36161 [D loss: 0.154324, acc.: 92.97%] [G loss: 3.285590]\n",
      "epoch:46 step:36162 [D loss: 0.118342, acc.: 96.88%] [G loss: 8.688090]\n",
      "epoch:46 step:36163 [D loss: 0.073764, acc.: 97.66%] [G loss: 5.139865]\n",
      "epoch:46 step:36164 [D loss: 0.061951, acc.: 97.66%] [G loss: 3.737709]\n",
      "epoch:46 step:36165 [D loss: 0.000924, acc.: 100.00%] [G loss: 2.713597]\n",
      "epoch:46 step:36166 [D loss: 0.041884, acc.: 98.44%] [G loss: 0.816495]\n",
      "epoch:46 step:36167 [D loss: 0.007784, acc.: 100.00%] [G loss: 0.810043]\n",
      "epoch:46 step:36168 [D loss: 0.001956, acc.: 100.00%] [G loss: 0.303733]\n",
      "epoch:46 step:36169 [D loss: 0.000514, acc.: 100.00%] [G loss: 0.842091]\n",
      "epoch:46 step:36170 [D loss: 0.000775, acc.: 100.00%] [G loss: 0.387777]\n",
      "epoch:46 step:36171 [D loss: 0.005149, acc.: 100.00%] [G loss: 0.322978]\n",
      "epoch:46 step:36172 [D loss: 0.002944, acc.: 100.00%] [G loss: 0.102241]\n",
      "epoch:46 step:36173 [D loss: 0.138788, acc.: 93.75%] [G loss: 2.167511]\n",
      "epoch:46 step:36174 [D loss: 0.026506, acc.: 97.66%] [G loss: 1.362120]\n",
      "epoch:46 step:36175 [D loss: 0.158805, acc.: 93.75%] [G loss: 0.033454]\n",
      "epoch:46 step:36176 [D loss: 0.018886, acc.: 99.22%] [G loss: 0.061943]\n",
      "epoch:46 step:36177 [D loss: 0.001155, acc.: 100.00%] [G loss: 0.086531]\n",
      "epoch:46 step:36178 [D loss: 0.032193, acc.: 99.22%] [G loss: 0.050982]\n",
      "epoch:46 step:36179 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.046201]\n",
      "epoch:46 step:36180 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.031771]\n",
      "epoch:46 step:36181 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.805437]\n",
      "epoch:46 step:36182 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.638282]\n",
      "epoch:46 step:36183 [D loss: 0.006409, acc.: 100.00%] [G loss: 0.200102]\n",
      "epoch:46 step:36184 [D loss: 0.048387, acc.: 100.00%] [G loss: 0.093858]\n",
      "epoch:46 step:36185 [D loss: 0.000937, acc.: 100.00%] [G loss: 0.330781]\n",
      "epoch:46 step:36186 [D loss: 0.048171, acc.: 97.66%] [G loss: 0.471102]\n",
      "epoch:46 step:36187 [D loss: 0.010225, acc.: 100.00%] [G loss: 3.794760]\n",
      "epoch:46 step:36188 [D loss: 0.001001, acc.: 100.00%] [G loss: 0.308572]\n",
      "epoch:46 step:36189 [D loss: 0.016453, acc.: 100.00%] [G loss: 0.018382]\n",
      "epoch:46 step:36190 [D loss: 0.002200, acc.: 100.00%] [G loss: 0.030550]\n",
      "epoch:46 step:36191 [D loss: 0.004612, acc.: 100.00%] [G loss: 0.177888]\n",
      "epoch:46 step:36192 [D loss: 0.023859, acc.: 100.00%] [G loss: 0.061379]\n",
      "epoch:46 step:36193 [D loss: 0.022026, acc.: 99.22%] [G loss: 0.877423]\n",
      "epoch:46 step:36194 [D loss: 0.001088, acc.: 100.00%] [G loss: 0.556069]\n",
      "epoch:46 step:36195 [D loss: 0.003765, acc.: 100.00%] [G loss: 0.012806]\n",
      "epoch:46 step:36196 [D loss: 0.007205, acc.: 100.00%] [G loss: 0.004667]\n",
      "epoch:46 step:36197 [D loss: 0.005235, acc.: 100.00%] [G loss: 0.006390]\n",
      "epoch:46 step:36198 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.002553]\n",
      "epoch:46 step:36199 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.004836]\n",
      "epoch:46 step:36200 [D loss: 0.004137, acc.: 100.00%] [G loss: 0.003253]\n",
      "epoch:46 step:36201 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.007113]\n",
      "epoch:46 step:36202 [D loss: 0.000756, acc.: 100.00%] [G loss: 0.003526]\n",
      "epoch:46 step:36203 [D loss: 0.012940, acc.: 100.00%] [G loss: 0.001320]\n",
      "epoch:46 step:36204 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.005912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36205 [D loss: 0.000657, acc.: 100.00%] [G loss: 0.002133]\n",
      "epoch:46 step:36206 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000484]\n",
      "epoch:46 step:36207 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.002872]\n",
      "epoch:46 step:36208 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.025875]\n",
      "epoch:46 step:36209 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.022370]\n",
      "epoch:46 step:36210 [D loss: 0.003861, acc.: 100.00%] [G loss: 0.000447]\n",
      "epoch:46 step:36211 [D loss: 0.000605, acc.: 100.00%] [G loss: 0.007811]\n",
      "epoch:46 step:36212 [D loss: 0.009346, acc.: 100.00%] [G loss: 0.004014]\n",
      "epoch:46 step:36213 [D loss: 0.021782, acc.: 100.00%] [G loss: 0.042805]\n",
      "epoch:46 step:36214 [D loss: 0.000769, acc.: 100.00%] [G loss: 0.006057]\n",
      "epoch:46 step:36215 [D loss: 0.007469, acc.: 100.00%] [G loss: 0.014166]\n",
      "epoch:46 step:36216 [D loss: 0.007540, acc.: 100.00%] [G loss: 0.044176]\n",
      "epoch:46 step:36217 [D loss: 0.002468, acc.: 100.00%] [G loss: 0.010549]\n",
      "epoch:46 step:36218 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.043259]\n",
      "epoch:46 step:36219 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.106659]\n",
      "epoch:46 step:36220 [D loss: 0.004640, acc.: 100.00%] [G loss: 0.018526]\n",
      "epoch:46 step:36221 [D loss: 0.008790, acc.: 99.22%] [G loss: 0.001978]\n",
      "epoch:46 step:36222 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.007247]\n",
      "epoch:46 step:36223 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.023514]\n",
      "epoch:46 step:36224 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.010305]\n",
      "epoch:46 step:36225 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.002766]\n",
      "epoch:46 step:36226 [D loss: 0.002078, acc.: 100.00%] [G loss: 0.058161]\n",
      "epoch:46 step:36227 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.002018]\n",
      "epoch:46 step:36228 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.010675]\n",
      "epoch:46 step:36229 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.002891]\n",
      "epoch:46 step:36230 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.007068]\n",
      "epoch:46 step:36231 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.012301]\n",
      "epoch:46 step:36232 [D loss: 0.003472, acc.: 100.00%] [G loss: 0.004194]\n",
      "epoch:46 step:36233 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.010617]\n",
      "epoch:46 step:36234 [D loss: 0.000995, acc.: 100.00%] [G loss: 0.000570]\n",
      "epoch:46 step:36235 [D loss: 0.002897, acc.: 100.00%] [G loss: 0.003361]\n",
      "epoch:46 step:36236 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.006155]\n",
      "epoch:46 step:36237 [D loss: 0.001813, acc.: 100.00%] [G loss: 0.005615]\n",
      "epoch:46 step:36238 [D loss: 0.000996, acc.: 100.00%] [G loss: 0.001535]\n",
      "epoch:46 step:36239 [D loss: 0.008310, acc.: 100.00%] [G loss: 0.003144]\n",
      "epoch:46 step:36240 [D loss: 0.000813, acc.: 100.00%] [G loss: 0.008353]\n",
      "epoch:46 step:36241 [D loss: 0.003585, acc.: 100.00%] [G loss: 0.005076]\n",
      "epoch:46 step:36242 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.002985]\n",
      "epoch:46 step:36243 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.019323]\n",
      "epoch:46 step:36244 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.058867]\n",
      "epoch:46 step:36245 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.015453]\n",
      "epoch:46 step:36246 [D loss: 0.004076, acc.: 100.00%] [G loss: 0.000858]\n",
      "epoch:46 step:36247 [D loss: 0.000762, acc.: 100.00%] [G loss: 0.002685]\n",
      "epoch:46 step:36248 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.003066]\n",
      "epoch:46 step:36249 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.005065]\n",
      "epoch:46 step:36250 [D loss: 0.000909, acc.: 100.00%] [G loss: 0.005517]\n",
      "epoch:46 step:36251 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.080194]\n",
      "epoch:46 step:36252 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.002106]\n",
      "epoch:46 step:36253 [D loss: 0.001353, acc.: 100.00%] [G loss: 0.002594]\n",
      "epoch:46 step:36254 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.004168]\n",
      "epoch:46 step:36255 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.001621]\n",
      "epoch:46 step:36256 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.002194]\n",
      "epoch:46 step:36257 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.004560]\n",
      "epoch:46 step:36258 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.002860]\n",
      "epoch:46 step:36259 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.006369]\n",
      "epoch:46 step:36260 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:46 step:36261 [D loss: 0.000137, acc.: 100.00%] [G loss: 0.014608]\n",
      "epoch:46 step:36262 [D loss: 0.000352, acc.: 100.00%] [G loss: 0.000540]\n",
      "epoch:46 step:36263 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.001950]\n",
      "epoch:46 step:36264 [D loss: 0.008040, acc.: 99.22%] [G loss: 0.001215]\n",
      "epoch:46 step:36265 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:46 step:36266 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.001642]\n",
      "epoch:46 step:36267 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000574]\n",
      "epoch:46 step:36268 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:46 step:36269 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.001484]\n",
      "epoch:46 step:36270 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.000415]\n",
      "epoch:46 step:36271 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000395]\n",
      "epoch:46 step:36272 [D loss: 0.002116, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:46 step:36273 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000168]\n",
      "epoch:46 step:36274 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.002559]\n",
      "epoch:46 step:36275 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.001974]\n",
      "epoch:46 step:36276 [D loss: 0.001286, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:46 step:36277 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000712]\n",
      "epoch:46 step:36278 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.001010]\n",
      "epoch:46 step:36279 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:46 step:36280 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:46 step:36281 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:46 step:36282 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000276]\n",
      "epoch:46 step:36283 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000630]\n",
      "epoch:46 step:36284 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000989]\n",
      "epoch:46 step:36285 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.028571]\n",
      "epoch:46 step:36286 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000881]\n",
      "epoch:46 step:36287 [D loss: 0.000373, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:46 step:36288 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:46 step:36289 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:46 step:36290 [D loss: 0.000687, acc.: 100.00%] [G loss: 0.000776]\n",
      "epoch:46 step:36291 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.008873]\n",
      "epoch:46 step:36292 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:46 step:36293 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:46 step:36294 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:46 step:36295 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:46 step:36296 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:46 step:36297 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.001156]\n",
      "epoch:46 step:36298 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:46 step:36299 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:46 step:36300 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000439]\n",
      "epoch:46 step:36301 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.000432]\n",
      "epoch:46 step:36302 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:46 step:36303 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001111]\n",
      "epoch:46 step:36304 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000194]\n",
      "epoch:46 step:36305 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000896]\n",
      "epoch:46 step:36306 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.015338]\n",
      "epoch:46 step:36307 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:46 step:36308 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000877]\n",
      "epoch:46 step:36309 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.001387]\n",
      "epoch:46 step:36310 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.007616]\n",
      "epoch:46 step:36311 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:46 step:36312 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.001303]\n",
      "epoch:46 step:36313 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:46 step:36314 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:46 step:36315 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.001444]\n",
      "epoch:46 step:36316 [D loss: 0.008726, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:46 step:36317 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.006050]\n",
      "epoch:46 step:36318 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.001955]\n",
      "epoch:46 step:36319 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.005629]\n",
      "epoch:46 step:36320 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36321 [D loss: 0.002465, acc.: 100.00%] [G loss: 0.001464]\n",
      "epoch:46 step:36322 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000241]\n",
      "epoch:46 step:36323 [D loss: 0.001987, acc.: 100.00%] [G loss: 0.000873]\n",
      "epoch:46 step:36324 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.004546]\n",
      "epoch:46 step:36325 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.002331]\n",
      "epoch:46 step:36326 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.001299]\n",
      "epoch:46 step:36327 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000421]\n",
      "epoch:46 step:36328 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000362]\n",
      "epoch:46 step:36329 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:46 step:36330 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.003638]\n",
      "epoch:46 step:36331 [D loss: 0.001104, acc.: 100.00%] [G loss: 0.000682]\n",
      "epoch:46 step:36332 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:46 step:36333 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:46 step:36334 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000756]\n",
      "epoch:46 step:36335 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:46 step:36336 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000927]\n",
      "epoch:46 step:36337 [D loss: 0.000354, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:46 step:36338 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:46 step:36339 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.000169]\n",
      "epoch:46 step:36340 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.001424]\n",
      "epoch:46 step:36341 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000892]\n",
      "epoch:46 step:36342 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:46 step:36343 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:46 step:36344 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:46 step:36345 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.002458]\n",
      "epoch:46 step:36346 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:46 step:36347 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.004936]\n",
      "epoch:46 step:36348 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000432]\n",
      "epoch:46 step:36349 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.000645]\n",
      "epoch:46 step:36350 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.001658]\n",
      "epoch:46 step:36351 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000238]\n",
      "epoch:46 step:36352 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.001731]\n",
      "epoch:46 step:36353 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:46 step:36354 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:46 step:36355 [D loss: 0.009103, acc.: 99.22%] [G loss: 0.000492]\n",
      "epoch:46 step:36356 [D loss: 0.000625, acc.: 100.00%] [G loss: 0.001184]\n",
      "epoch:46 step:36357 [D loss: 0.000752, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:46 step:36358 [D loss: 0.000696, acc.: 100.00%] [G loss: 0.000156]\n",
      "epoch:46 step:36359 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.001262]\n",
      "epoch:46 step:36360 [D loss: 0.001457, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:46 step:36361 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.000206]\n",
      "epoch:46 step:36362 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:46 step:36363 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:46 step:36364 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:46 step:36365 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:46 step:36366 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000884]\n",
      "epoch:46 step:36367 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.000142]\n",
      "epoch:46 step:36368 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:46 step:36369 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:46 step:36370 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:46 step:36371 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:46 step:36372 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.003714]\n",
      "epoch:46 step:36373 [D loss: 0.002967, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:46 step:36374 [D loss: 0.000961, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:46 step:36375 [D loss: 0.001377, acc.: 100.00%] [G loss: 0.001260]\n",
      "epoch:46 step:36376 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:46 step:36377 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.003382]\n",
      "epoch:46 step:36378 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:46 step:36379 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:46 step:36380 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.001502]\n",
      "epoch:46 step:36381 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.002433]\n",
      "epoch:46 step:36382 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:46 step:36383 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:46 step:36384 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:46 step:36385 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.002004]\n",
      "epoch:46 step:36386 [D loss: 0.000903, acc.: 100.00%] [G loss: 0.002170]\n",
      "epoch:46 step:36387 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000470]\n",
      "epoch:46 step:36388 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000914]\n",
      "epoch:46 step:36389 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000581]\n",
      "epoch:46 step:36390 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:46 step:36391 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:46 step:36392 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:46 step:36393 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000228]\n",
      "epoch:46 step:36394 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.011741]\n",
      "epoch:46 step:36395 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000317]\n",
      "epoch:46 step:36396 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000727]\n",
      "epoch:46 step:36397 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000780]\n",
      "epoch:46 step:36398 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.001046]\n",
      "epoch:46 step:36399 [D loss: 0.000356, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:46 step:36400 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:46 step:36401 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000365]\n",
      "epoch:46 step:36402 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001951]\n",
      "epoch:46 step:36403 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000301]\n",
      "epoch:46 step:36404 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.001568]\n",
      "epoch:46 step:36405 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:46 step:36406 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.002777]\n",
      "epoch:46 step:36407 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000882]\n",
      "epoch:46 step:36408 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000703]\n",
      "epoch:46 step:36409 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000259]\n",
      "epoch:46 step:36410 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000184]\n",
      "epoch:46 step:36411 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000679]\n",
      "epoch:46 step:36412 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000247]\n",
      "epoch:46 step:36413 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:46 step:36414 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:46 step:36415 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000609]\n",
      "epoch:46 step:36416 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:46 step:36417 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.002206]\n",
      "epoch:46 step:36418 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000768]\n",
      "epoch:46 step:36419 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.000634]\n",
      "epoch:46 step:36420 [D loss: 0.010080, acc.: 100.00%] [G loss: 0.006859]\n",
      "epoch:46 step:36421 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.002218]\n",
      "epoch:46 step:36422 [D loss: 0.000812, acc.: 100.00%] [G loss: 0.000592]\n",
      "epoch:46 step:36423 [D loss: 0.000544, acc.: 100.00%] [G loss: 0.001678]\n",
      "epoch:46 step:36424 [D loss: 0.044137, acc.: 99.22%] [G loss: 0.174086]\n",
      "epoch:46 step:36425 [D loss: 0.022144, acc.: 99.22%] [G loss: 0.824294]\n",
      "epoch:46 step:36426 [D loss: 0.000174, acc.: 100.00%] [G loss: 0.058085]\n",
      "epoch:46 step:36427 [D loss: 0.031038, acc.: 99.22%] [G loss: 0.036378]\n",
      "epoch:46 step:36428 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.003381]\n",
      "epoch:46 step:36429 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.002007]\n",
      "epoch:46 step:36430 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.161044]\n",
      "epoch:46 step:36431 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000234]\n",
      "epoch:46 step:36432 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.004196]\n",
      "epoch:46 step:36433 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.002680]\n",
      "epoch:46 step:36434 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.471508]\n",
      "epoch:46 step:36435 [D loss: 0.009868, acc.: 99.22%] [G loss: 0.030723]\n",
      "epoch:46 step:36436 [D loss: 0.003478, acc.: 100.00%] [G loss: 0.112382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36437 [D loss: 0.000787, acc.: 100.00%] [G loss: 0.018496]\n",
      "epoch:46 step:36438 [D loss: 0.002498, acc.: 100.00%] [G loss: 0.017898]\n",
      "epoch:46 step:36439 [D loss: 0.033155, acc.: 99.22%] [G loss: 0.038216]\n",
      "epoch:46 step:36440 [D loss: 0.011035, acc.: 100.00%] [G loss: 0.006872]\n",
      "epoch:46 step:36441 [D loss: 0.002774, acc.: 100.00%] [G loss: 0.031773]\n",
      "epoch:46 step:36442 [D loss: 0.000754, acc.: 100.00%] [G loss: 0.007113]\n",
      "epoch:46 step:36443 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.009496]\n",
      "epoch:46 step:36444 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.015357]\n",
      "epoch:46 step:36445 [D loss: 0.000326, acc.: 100.00%] [G loss: 0.014354]\n",
      "epoch:46 step:36446 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.001852]\n",
      "epoch:46 step:36447 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.005844]\n",
      "epoch:46 step:36448 [D loss: 0.002190, acc.: 100.00%] [G loss: 0.145636]\n",
      "epoch:46 step:36449 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.001144]\n",
      "epoch:46 step:36450 [D loss: 0.007651, acc.: 100.00%] [G loss: 0.004808]\n",
      "epoch:46 step:36451 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000812]\n",
      "epoch:46 step:36452 [D loss: 0.017389, acc.: 99.22%] [G loss: 0.016894]\n",
      "epoch:46 step:36453 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.063415]\n",
      "epoch:46 step:36454 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000182]\n",
      "epoch:46 step:36455 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.002439]\n",
      "epoch:46 step:36456 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.008272]\n",
      "epoch:46 step:36457 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:46 step:36458 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000969]\n",
      "epoch:46 step:36459 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.022943]\n",
      "epoch:46 step:36460 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.006652]\n",
      "epoch:46 step:36461 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.004154]\n",
      "epoch:46 step:36462 [D loss: 0.000438, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:46 step:36463 [D loss: 0.003206, acc.: 100.00%] [G loss: 0.004530]\n",
      "epoch:46 step:36464 [D loss: 0.003337, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:46 step:36465 [D loss: 0.002081, acc.: 100.00%] [G loss: 0.000225]\n",
      "epoch:46 step:36466 [D loss: 0.000202, acc.: 100.00%] [G loss: 0.022399]\n",
      "epoch:46 step:36467 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.000141]\n",
      "epoch:46 step:36468 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:46 step:36469 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000614]\n",
      "epoch:46 step:36470 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.000370]\n",
      "epoch:46 step:36471 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.000350]\n",
      "epoch:46 step:36472 [D loss: 0.003332, acc.: 100.00%] [G loss: 0.001697]\n",
      "epoch:46 step:36473 [D loss: 0.000958, acc.: 100.00%] [G loss: 0.002947]\n",
      "epoch:46 step:36474 [D loss: 0.005109, acc.: 100.00%] [G loss: 0.026061]\n",
      "epoch:46 step:36475 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:46 step:36476 [D loss: 0.000651, acc.: 100.00%] [G loss: 0.003681]\n",
      "epoch:46 step:36477 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.004468]\n",
      "epoch:46 step:36478 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.015522]\n",
      "epoch:46 step:36479 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.000446]\n",
      "epoch:46 step:36480 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:46 step:36481 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.002122]\n",
      "epoch:46 step:36482 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000572]\n",
      "epoch:46 step:36483 [D loss: 0.000734, acc.: 100.00%] [G loss: 0.001343]\n",
      "epoch:46 step:36484 [D loss: 0.131354, acc.: 95.31%] [G loss: 2.626058]\n",
      "epoch:46 step:36485 [D loss: 0.030517, acc.: 99.22%] [G loss: 3.212787]\n",
      "epoch:46 step:36486 [D loss: 0.191700, acc.: 92.19%] [G loss: 0.087654]\n",
      "epoch:46 step:36487 [D loss: 0.005650, acc.: 100.00%] [G loss: 0.038174]\n",
      "epoch:46 step:36488 [D loss: 0.032726, acc.: 98.44%] [G loss: 0.143749]\n",
      "epoch:46 step:36489 [D loss: 0.006856, acc.: 100.00%] [G loss: 0.005889]\n",
      "epoch:46 step:36490 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.133004]\n",
      "epoch:46 step:36491 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.160373]\n",
      "epoch:46 step:36492 [D loss: 0.001656, acc.: 100.00%] [G loss: 1.297378]\n",
      "epoch:46 step:36493 [D loss: 0.003905, acc.: 100.00%] [G loss: 0.009175]\n",
      "epoch:46 step:36494 [D loss: 0.002558, acc.: 100.00%] [G loss: 0.009426]\n",
      "epoch:46 step:36495 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.002790]\n",
      "epoch:46 step:36496 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.000642]\n",
      "epoch:46 step:36497 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.005725]\n",
      "epoch:46 step:36498 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.006071]\n",
      "epoch:46 step:36499 [D loss: 0.001238, acc.: 100.00%] [G loss: 0.002628]\n",
      "epoch:46 step:36500 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.006223]\n",
      "epoch:46 step:36501 [D loss: 0.014146, acc.: 99.22%] [G loss: 0.000332]\n",
      "epoch:46 step:36502 [D loss: 0.005142, acc.: 100.00%] [G loss: 0.001847]\n",
      "epoch:46 step:36503 [D loss: 0.022869, acc.: 100.00%] [G loss: 0.001636]\n",
      "epoch:46 step:36504 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.009452]\n",
      "epoch:46 step:36505 [D loss: 0.000686, acc.: 100.00%] [G loss: 0.005826]\n",
      "epoch:46 step:36506 [D loss: 0.006784, acc.: 100.00%] [G loss: 0.014333]\n",
      "epoch:46 step:36507 [D loss: 0.002438, acc.: 100.00%] [G loss: 0.010400]\n",
      "epoch:46 step:36508 [D loss: 0.002331, acc.: 100.00%] [G loss: 0.004250]\n",
      "epoch:46 step:36509 [D loss: 0.011156, acc.: 99.22%] [G loss: 0.001296]\n",
      "epoch:46 step:36510 [D loss: 0.028427, acc.: 99.22%] [G loss: 0.124291]\n",
      "epoch:46 step:36511 [D loss: 0.000948, acc.: 100.00%] [G loss: 0.061990]\n",
      "epoch:46 step:36512 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.070337]\n",
      "epoch:46 step:36513 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.129908]\n",
      "epoch:46 step:36514 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.033198]\n",
      "epoch:46 step:36515 [D loss: 0.001008, acc.: 100.00%] [G loss: 0.060690]\n",
      "epoch:46 step:36516 [D loss: 0.000685, acc.: 100.00%] [G loss: 0.038488]\n",
      "epoch:46 step:36517 [D loss: 0.012238, acc.: 99.22%] [G loss: 0.042176]\n",
      "epoch:46 step:36518 [D loss: 0.000250, acc.: 100.00%] [G loss: 0.000930]\n",
      "epoch:46 step:36519 [D loss: 0.001761, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:46 step:36520 [D loss: 0.007534, acc.: 100.00%] [G loss: 0.000824]\n",
      "epoch:46 step:36521 [D loss: 0.008310, acc.: 100.00%] [G loss: 0.008992]\n",
      "epoch:46 step:36522 [D loss: 0.002704, acc.: 100.00%] [G loss: 2.843620]\n",
      "epoch:46 step:36523 [D loss: 0.009824, acc.: 100.00%] [G loss: 0.008172]\n",
      "epoch:46 step:36524 [D loss: 0.032856, acc.: 98.44%] [G loss: 1.168583]\n",
      "epoch:46 step:36525 [D loss: 0.020371, acc.: 99.22%] [G loss: 0.328444]\n",
      "epoch:46 step:36526 [D loss: 0.015740, acc.: 99.22%] [G loss: 0.111358]\n",
      "epoch:46 step:36527 [D loss: 0.004168, acc.: 100.00%] [G loss: 0.006275]\n",
      "epoch:46 step:36528 [D loss: 0.001077, acc.: 100.00%] [G loss: 0.007962]\n",
      "epoch:46 step:36529 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.085288]\n",
      "epoch:46 step:36530 [D loss: 0.104998, acc.: 93.75%] [G loss: 0.000000]\n",
      "epoch:46 step:36531 [D loss: 0.023105, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36532 [D loss: 0.004211, acc.: 100.00%] [G loss: 0.000002]\n",
      "epoch:46 step:36533 [D loss: 0.000575, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:46 step:36534 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.010197]\n",
      "epoch:46 step:36535 [D loss: 0.007047, acc.: 100.00%] [G loss: 0.000001]\n",
      "epoch:46 step:36536 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:46 step:36537 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:46 step:36538 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.001360]\n",
      "epoch:46 step:36539 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:46 step:36540 [D loss: 0.000677, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:46 step:36541 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.067571]\n",
      "epoch:46 step:36542 [D loss: 0.001591, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:46 step:36543 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:46 step:36544 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000279]\n",
      "epoch:46 step:36545 [D loss: 0.000369, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:46 step:36546 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000014]\n",
      "epoch:46 step:36547 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.198156]\n",
      "epoch:46 step:36548 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.001465]\n",
      "epoch:46 step:36549 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.000073]\n",
      "epoch:46 step:36550 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:46 step:36551 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:46 step:36552 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.001387]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36553 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:46 step:36554 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:46 step:36555 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:46 step:36556 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:46 step:36557 [D loss: 0.006518, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:46 step:36558 [D loss: 0.001224, acc.: 100.00%] [G loss: 0.002039]\n",
      "epoch:46 step:36559 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.000633]\n",
      "epoch:46 step:36560 [D loss: 0.000851, acc.: 100.00%] [G loss: 0.000177]\n",
      "epoch:46 step:36561 [D loss: 0.000961, acc.: 100.00%] [G loss: 0.001261]\n",
      "epoch:46 step:36562 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.012891]\n",
      "epoch:46 step:36563 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.002729]\n",
      "epoch:46 step:36564 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000366]\n",
      "epoch:46 step:36565 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.003712]\n",
      "epoch:46 step:36566 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:46 step:36567 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.429982]\n",
      "epoch:46 step:36568 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:46 step:36569 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.003199]\n",
      "epoch:46 step:36570 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:46 step:36571 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.003928]\n",
      "epoch:46 step:36572 [D loss: 0.140127, acc.: 92.97%] [G loss: 3.469617]\n",
      "epoch:46 step:36573 [D loss: 0.025219, acc.: 100.00%] [G loss: 1.239155]\n",
      "epoch:46 step:36574 [D loss: 0.069244, acc.: 97.66%] [G loss: 0.000524]\n",
      "epoch:46 step:36575 [D loss: 0.003919, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:46 step:36576 [D loss: 0.000686, acc.: 100.00%] [G loss: 0.000131]\n",
      "epoch:46 step:36577 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:46 step:36578 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.120989]\n",
      "epoch:46 step:36579 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.002921]\n",
      "epoch:46 step:36580 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.002860]\n",
      "epoch:46 step:36581 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.183740]\n",
      "epoch:46 step:36582 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.001573]\n",
      "epoch:46 step:36583 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:46 step:36584 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.078957]\n",
      "epoch:46 step:36585 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:46 step:36586 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:46 step:36587 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000715]\n",
      "epoch:46 step:36588 [D loss: 0.000825, acc.: 100.00%] [G loss: 0.000560]\n",
      "epoch:46 step:36589 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.009043]\n",
      "epoch:46 step:36590 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.013224]\n",
      "epoch:46 step:36591 [D loss: 0.001787, acc.: 100.00%] [G loss: 0.004174]\n",
      "epoch:46 step:36592 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:46 step:36593 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:46 step:36594 [D loss: 0.002246, acc.: 100.00%] [G loss: 0.000442]\n",
      "epoch:46 step:36595 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:46 step:36596 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000657]\n",
      "epoch:46 step:36597 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.003835]\n",
      "epoch:46 step:36598 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:46 step:36599 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.001075]\n",
      "epoch:46 step:36600 [D loss: 0.006222, acc.: 100.00%] [G loss: 0.017365]\n",
      "epoch:46 step:36601 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000248]\n",
      "epoch:46 step:36602 [D loss: 0.006290, acc.: 100.00%] [G loss: 0.010745]\n",
      "epoch:46 step:36603 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:46 step:36604 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000412]\n",
      "epoch:46 step:36605 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000575]\n",
      "epoch:46 step:36606 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:46 step:36607 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:46 step:36608 [D loss: 0.003778, acc.: 100.00%] [G loss: 0.000955]\n",
      "epoch:46 step:36609 [D loss: 0.006884, acc.: 100.00%] [G loss: 0.000323]\n",
      "epoch:46 step:36610 [D loss: 0.001581, acc.: 100.00%] [G loss: 0.000902]\n",
      "epoch:46 step:36611 [D loss: 0.022599, acc.: 100.00%] [G loss: 0.073079]\n",
      "epoch:46 step:36612 [D loss: 0.010294, acc.: 100.00%] [G loss: 0.082459]\n",
      "epoch:46 step:36613 [D loss: 0.039753, acc.: 100.00%] [G loss: 6.227991]\n",
      "epoch:46 step:36614 [D loss: 0.028563, acc.: 100.00%] [G loss: 0.288567]\n",
      "epoch:46 step:36615 [D loss: 0.150842, acc.: 95.31%] [G loss: 8.353347]\n",
      "epoch:46 step:36616 [D loss: 5.375455, acc.: 10.94%] [G loss: 8.493052]\n",
      "epoch:46 step:36617 [D loss: 0.744472, acc.: 72.66%] [G loss: 5.614930]\n",
      "epoch:46 step:36618 [D loss: 0.796334, acc.: 67.19%] [G loss: 2.573375]\n",
      "epoch:46 step:36619 [D loss: 0.140721, acc.: 97.66%] [G loss: 3.358929]\n",
      "epoch:46 step:36620 [D loss: 0.062936, acc.: 97.66%] [G loss: 3.385993]\n",
      "epoch:46 step:36621 [D loss: 0.158342, acc.: 96.09%] [G loss: 0.622607]\n",
      "epoch:46 step:36622 [D loss: 0.116928, acc.: 96.88%] [G loss: 3.900926]\n",
      "epoch:46 step:36623 [D loss: 0.118023, acc.: 98.44%] [G loss: 3.406383]\n",
      "epoch:46 step:36624 [D loss: 0.131364, acc.: 96.88%] [G loss: 3.339129]\n",
      "epoch:46 step:36625 [D loss: 0.106519, acc.: 97.66%] [G loss: 0.237362]\n",
      "epoch:46 step:36626 [D loss: 0.173695, acc.: 93.75%] [G loss: 3.361801]\n",
      "epoch:46 step:36627 [D loss: 0.133370, acc.: 94.53%] [G loss: 0.414642]\n",
      "epoch:46 step:36628 [D loss: 0.172571, acc.: 95.31%] [G loss: 0.402317]\n",
      "epoch:46 step:36629 [D loss: 0.443064, acc.: 75.78%] [G loss: 2.520476]\n",
      "epoch:46 step:36630 [D loss: 0.055553, acc.: 98.44%] [G loss: 2.121850]\n",
      "epoch:46 step:36631 [D loss: 0.081929, acc.: 96.09%] [G loss: 0.937063]\n",
      "epoch:46 step:36632 [D loss: 0.014325, acc.: 100.00%] [G loss: 0.494574]\n",
      "epoch:46 step:36633 [D loss: 0.023496, acc.: 100.00%] [G loss: 0.337846]\n",
      "epoch:46 step:36634 [D loss: 0.024043, acc.: 100.00%] [G loss: 0.337188]\n",
      "epoch:46 step:36635 [D loss: 0.014296, acc.: 100.00%] [G loss: 0.360489]\n",
      "epoch:46 step:36636 [D loss: 0.057758, acc.: 98.44%] [G loss: 0.232449]\n",
      "epoch:46 step:36637 [D loss: 0.023289, acc.: 100.00%] [G loss: 0.037484]\n",
      "epoch:46 step:36638 [D loss: 0.009975, acc.: 100.00%] [G loss: 1.036252]\n",
      "epoch:46 step:36639 [D loss: 0.041159, acc.: 99.22%] [G loss: 0.331037]\n",
      "epoch:46 step:36640 [D loss: 0.043534, acc.: 97.66%] [G loss: 0.027413]\n",
      "epoch:46 step:36641 [D loss: 0.072566, acc.: 98.44%] [G loss: 0.022904]\n",
      "epoch:46 step:36642 [D loss: 0.036074, acc.: 98.44%] [G loss: 0.006476]\n",
      "epoch:46 step:36643 [D loss: 0.009335, acc.: 100.00%] [G loss: 0.021803]\n",
      "epoch:46 step:36644 [D loss: 0.002277, acc.: 100.00%] [G loss: 0.027329]\n",
      "epoch:46 step:36645 [D loss: 0.085540, acc.: 96.88%] [G loss: 0.145184]\n",
      "epoch:46 step:36646 [D loss: 0.064198, acc.: 98.44%] [G loss: 0.183054]\n",
      "epoch:46 step:36647 [D loss: 0.010124, acc.: 100.00%] [G loss: 0.089679]\n",
      "epoch:46 step:36648 [D loss: 0.002712, acc.: 100.00%] [G loss: 0.141267]\n",
      "epoch:46 step:36649 [D loss: 0.007764, acc.: 100.00%] [G loss: 0.220151]\n",
      "epoch:46 step:36650 [D loss: 0.015521, acc.: 100.00%] [G loss: 0.019213]\n",
      "epoch:46 step:36651 [D loss: 0.353083, acc.: 82.03%] [G loss: 1.592218]\n",
      "epoch:46 step:36652 [D loss: 0.223802, acc.: 90.62%] [G loss: 0.497410]\n",
      "epoch:46 step:36653 [D loss: 0.622508, acc.: 72.66%] [G loss: 0.001673]\n",
      "epoch:46 step:36654 [D loss: 0.013527, acc.: 100.00%] [G loss: 0.001114]\n",
      "epoch:46 step:36655 [D loss: 0.008736, acc.: 100.00%] [G loss: 0.437039]\n",
      "epoch:46 step:36656 [D loss: 0.005949, acc.: 100.00%] [G loss: 0.000775]\n",
      "epoch:46 step:36657 [D loss: 0.001528, acc.: 100.00%] [G loss: 0.005906]\n",
      "epoch:46 step:36658 [D loss: 0.126960, acc.: 95.31%] [G loss: 0.007600]\n",
      "epoch:46 step:36659 [D loss: 0.003832, acc.: 100.00%] [G loss: 0.106854]\n",
      "epoch:46 step:36660 [D loss: 0.020865, acc.: 99.22%] [G loss: 0.052683]\n",
      "epoch:46 step:36661 [D loss: 0.003282, acc.: 100.00%] [G loss: 2.066376]\n",
      "epoch:46 step:36662 [D loss: 0.004655, acc.: 100.00%] [G loss: 1.722872]\n",
      "epoch:46 step:36663 [D loss: 0.024595, acc.: 98.44%] [G loss: 0.439363]\n",
      "epoch:46 step:36664 [D loss: 0.006766, acc.: 100.00%] [G loss: 0.059385]\n",
      "epoch:46 step:36665 [D loss: 0.035625, acc.: 99.22%] [G loss: 0.094341]\n",
      "epoch:46 step:36666 [D loss: 0.033982, acc.: 100.00%] [G loss: 0.002165]\n",
      "epoch:46 step:36667 [D loss: 0.001999, acc.: 100.00%] [G loss: 0.002788]\n",
      "epoch:46 step:36668 [D loss: 0.006122, acc.: 100.00%] [G loss: 0.001886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:46 step:36669 [D loss: 0.017569, acc.: 100.00%] [G loss: 0.000743]\n",
      "epoch:46 step:36670 [D loss: 0.001989, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:46 step:36671 [D loss: 0.002857, acc.: 100.00%] [G loss: 0.009487]\n",
      "epoch:46 step:36672 [D loss: 0.011088, acc.: 100.00%] [G loss: 0.001616]\n",
      "epoch:46 step:36673 [D loss: 0.004393, acc.: 100.00%] [G loss: 0.417480]\n",
      "epoch:46 step:36674 [D loss: 0.002655, acc.: 100.00%] [G loss: 0.000267]\n",
      "epoch:46 step:36675 [D loss: 0.014915, acc.: 99.22%] [G loss: 0.580478]\n",
      "epoch:46 step:36676 [D loss: 0.153991, acc.: 91.41%] [G loss: 0.026196]\n",
      "epoch:46 step:36677 [D loss: 0.052301, acc.: 99.22%] [G loss: 1.545518]\n",
      "epoch:46 step:36678 [D loss: 0.067735, acc.: 98.44%] [G loss: 0.035509]\n",
      "epoch:46 step:36679 [D loss: 0.143583, acc.: 93.75%] [G loss: 0.001517]\n",
      "epoch:46 step:36680 [D loss: 0.105136, acc.: 95.31%] [G loss: 0.040944]\n",
      "epoch:46 step:36681 [D loss: 0.011345, acc.: 100.00%] [G loss: 0.105526]\n",
      "epoch:46 step:36682 [D loss: 0.022727, acc.: 99.22%] [G loss: 0.353366]\n",
      "epoch:46 step:36683 [D loss: 0.006919, acc.: 100.00%] [G loss: 0.089869]\n",
      "epoch:46 step:36684 [D loss: 0.040161, acc.: 98.44%] [G loss: 0.008755]\n",
      "epoch:46 step:36685 [D loss: 0.004639, acc.: 100.00%] [G loss: 0.163016]\n",
      "epoch:46 step:36686 [D loss: 0.263367, acc.: 89.84%] [G loss: 0.513474]\n",
      "epoch:46 step:36687 [D loss: 0.217702, acc.: 87.50%] [G loss: 3.414351]\n",
      "epoch:46 step:36688 [D loss: 0.124548, acc.: 94.53%] [G loss: 0.002426]\n",
      "epoch:46 step:36689 [D loss: 0.052727, acc.: 99.22%] [G loss: 0.000705]\n",
      "epoch:46 step:36690 [D loss: 0.010661, acc.: 99.22%] [G loss: 0.087177]\n",
      "epoch:46 step:36691 [D loss: 0.016542, acc.: 100.00%] [G loss: 0.002213]\n",
      "epoch:46 step:36692 [D loss: 0.027197, acc.: 99.22%] [G loss: 0.314133]\n",
      "epoch:46 step:36693 [D loss: 0.001805, acc.: 100.00%] [G loss: 0.000450]\n",
      "epoch:46 step:36694 [D loss: 0.008623, acc.: 99.22%] [G loss: 0.000208]\n",
      "epoch:46 step:36695 [D loss: 0.001705, acc.: 100.00%] [G loss: 0.063017]\n",
      "epoch:46 step:36696 [D loss: 0.000596, acc.: 100.00%] [G loss: 0.085407]\n",
      "epoch:46 step:36697 [D loss: 0.194470, acc.: 91.41%] [G loss: 0.802431]\n",
      "epoch:46 step:36698 [D loss: 0.010484, acc.: 100.00%] [G loss: 0.021109]\n",
      "epoch:46 step:36699 [D loss: 0.001523, acc.: 100.00%] [G loss: 1.430635]\n",
      "epoch:46 step:36700 [D loss: 0.028399, acc.: 100.00%] [G loss: 0.039521]\n",
      "epoch:46 step:36701 [D loss: 0.038193, acc.: 99.22%] [G loss: 0.175516]\n",
      "epoch:46 step:36702 [D loss: 0.023342, acc.: 100.00%] [G loss: 0.181513]\n",
      "epoch:46 step:36703 [D loss: 0.017292, acc.: 100.00%] [G loss: 0.001910]\n",
      "epoch:46 step:36704 [D loss: 0.060799, acc.: 98.44%] [G loss: 0.003102]\n",
      "epoch:46 step:36705 [D loss: 0.009528, acc.: 100.00%] [G loss: 0.035816]\n",
      "epoch:46 step:36706 [D loss: 0.000892, acc.: 100.00%] [G loss: 0.004435]\n",
      "epoch:46 step:36707 [D loss: 0.006669, acc.: 100.00%] [G loss: 0.015285]\n",
      "epoch:47 step:36708 [D loss: 0.005459, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:47 step:36709 [D loss: 0.004962, acc.: 100.00%] [G loss: 0.190452]\n",
      "epoch:47 step:36710 [D loss: 0.018299, acc.: 100.00%] [G loss: 0.000832]\n",
      "epoch:47 step:36711 [D loss: 0.001117, acc.: 100.00%] [G loss: 0.096530]\n",
      "epoch:47 step:36712 [D loss: 0.026183, acc.: 99.22%] [G loss: 0.000159]\n",
      "epoch:47 step:36713 [D loss: 0.003952, acc.: 100.00%] [G loss: 0.069775]\n",
      "epoch:47 step:36714 [D loss: 0.005402, acc.: 100.00%] [G loss: 0.000400]\n",
      "epoch:47 step:36715 [D loss: 0.001513, acc.: 100.00%] [G loss: 0.010133]\n",
      "epoch:47 step:36716 [D loss: 0.002063, acc.: 100.00%] [G loss: 0.117936]\n",
      "epoch:47 step:36717 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.084355]\n",
      "epoch:47 step:36718 [D loss: 0.009110, acc.: 100.00%] [G loss: 0.001179]\n",
      "epoch:47 step:36719 [D loss: 0.004634, acc.: 100.00%] [G loss: 0.147783]\n",
      "epoch:47 step:36720 [D loss: 0.002468, acc.: 100.00%] [G loss: 1.073525]\n",
      "epoch:47 step:36721 [D loss: 0.041898, acc.: 99.22%] [G loss: 0.311373]\n",
      "epoch:47 step:36722 [D loss: 0.050992, acc.: 98.44%] [G loss: 0.000761]\n",
      "epoch:47 step:36723 [D loss: 0.067293, acc.: 98.44%] [G loss: 0.001298]\n",
      "epoch:47 step:36724 [D loss: 0.006462, acc.: 100.00%] [G loss: 0.002337]\n",
      "epoch:47 step:36725 [D loss: 0.015473, acc.: 100.00%] [G loss: 0.000807]\n",
      "epoch:47 step:36726 [D loss: 0.006908, acc.: 100.00%] [G loss: 0.194327]\n",
      "epoch:47 step:36727 [D loss: 0.024158, acc.: 99.22%] [G loss: 0.000322]\n",
      "epoch:47 step:36728 [D loss: 0.027093, acc.: 98.44%] [G loss: 0.003317]\n",
      "epoch:47 step:36729 [D loss: 0.015590, acc.: 100.00%] [G loss: 0.075949]\n",
      "epoch:47 step:36730 [D loss: 0.028352, acc.: 100.00%] [G loss: 0.465240]\n",
      "epoch:47 step:36731 [D loss: 0.007408, acc.: 100.00%] [G loss: 0.006401]\n",
      "epoch:47 step:36732 [D loss: 0.030034, acc.: 98.44%] [G loss: 0.000125]\n",
      "epoch:47 step:36733 [D loss: 0.008729, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:47 step:36734 [D loss: 0.006273, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:47 step:36735 [D loss: 0.000737, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:47 step:36736 [D loss: 0.001661, acc.: 100.00%] [G loss: 0.001500]\n",
      "epoch:47 step:36737 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.162499]\n",
      "epoch:47 step:36738 [D loss: 0.001260, acc.: 100.00%] [G loss: 0.076335]\n",
      "epoch:47 step:36739 [D loss: 0.012034, acc.: 100.00%] [G loss: 0.110103]\n",
      "epoch:47 step:36740 [D loss: 0.002369, acc.: 100.00%] [G loss: 0.000202]\n",
      "epoch:47 step:36741 [D loss: 0.001528, acc.: 100.00%] [G loss: 0.000703]\n",
      "epoch:47 step:36742 [D loss: 0.005220, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:47 step:36743 [D loss: 0.003034, acc.: 100.00%] [G loss: 0.116226]\n",
      "epoch:47 step:36744 [D loss: 0.004251, acc.: 100.00%] [G loss: 0.303523]\n",
      "epoch:47 step:36745 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.111066]\n",
      "epoch:47 step:36746 [D loss: 0.019448, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:47 step:36747 [D loss: 0.017146, acc.: 99.22%] [G loss: 0.004292]\n",
      "epoch:47 step:36748 [D loss: 0.003201, acc.: 100.00%] [G loss: 0.119693]\n",
      "epoch:47 step:36749 [D loss: 0.011061, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:47 step:36750 [D loss: 0.008546, acc.: 100.00%] [G loss: 0.096950]\n",
      "epoch:47 step:36751 [D loss: 0.003933, acc.: 100.00%] [G loss: 0.061900]\n",
      "epoch:47 step:36752 [D loss: 0.007753, acc.: 100.00%] [G loss: 0.067876]\n",
      "epoch:47 step:36753 [D loss: 0.007205, acc.: 100.00%] [G loss: 0.015353]\n",
      "epoch:47 step:36754 [D loss: 0.008611, acc.: 100.00%] [G loss: 0.356384]\n",
      "epoch:47 step:36755 [D loss: 0.002664, acc.: 100.00%] [G loss: 0.009426]\n",
      "epoch:47 step:36756 [D loss: 0.060267, acc.: 98.44%] [G loss: 0.005432]\n",
      "epoch:47 step:36757 [D loss: 0.015089, acc.: 99.22%] [G loss: 0.000525]\n",
      "epoch:47 step:36758 [D loss: 0.000473, acc.: 100.00%] [G loss: 0.114474]\n",
      "epoch:47 step:36759 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.113833]\n",
      "epoch:47 step:36760 [D loss: 0.001574, acc.: 100.00%] [G loss: 0.000968]\n",
      "epoch:47 step:36761 [D loss: 0.003361, acc.: 100.00%] [G loss: 0.000634]\n",
      "epoch:47 step:36762 [D loss: 0.001803, acc.: 100.00%] [G loss: 0.001156]\n",
      "epoch:47 step:36763 [D loss: 0.028876, acc.: 99.22%] [G loss: 0.000030]\n",
      "epoch:47 step:36764 [D loss: 0.001081, acc.: 100.00%] [G loss: 0.010742]\n",
      "epoch:47 step:36765 [D loss: 0.018806, acc.: 99.22%] [G loss: 0.059014]\n",
      "epoch:47 step:36766 [D loss: 0.027123, acc.: 99.22%] [G loss: 0.010795]\n",
      "epoch:47 step:36767 [D loss: 0.003625, acc.: 100.00%] [G loss: 0.265884]\n",
      "epoch:47 step:36768 [D loss: 0.008572, acc.: 100.00%] [G loss: 0.008064]\n",
      "epoch:47 step:36769 [D loss: 0.002523, acc.: 100.00%] [G loss: 0.003868]\n",
      "epoch:47 step:36770 [D loss: 0.003046, acc.: 100.00%] [G loss: 0.147103]\n",
      "epoch:47 step:36771 [D loss: 0.003472, acc.: 100.00%] [G loss: 0.002604]\n",
      "epoch:47 step:36772 [D loss: 0.004276, acc.: 100.00%] [G loss: 0.007011]\n",
      "epoch:47 step:36773 [D loss: 0.001494, acc.: 100.00%] [G loss: 0.000111]\n",
      "epoch:47 step:36774 [D loss: 0.000752, acc.: 100.00%] [G loss: 0.000644]\n",
      "epoch:47 step:36775 [D loss: 0.021862, acc.: 100.00%] [G loss: 0.035487]\n",
      "epoch:47 step:36776 [D loss: 0.002160, acc.: 100.00%] [G loss: 0.000102]\n",
      "epoch:47 step:36777 [D loss: 0.000735, acc.: 100.00%] [G loss: 0.000162]\n",
      "epoch:47 step:36778 [D loss: 0.000763, acc.: 100.00%] [G loss: 0.017434]\n",
      "epoch:47 step:36779 [D loss: 0.000591, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:47 step:36780 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:47 step:36781 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.013528]\n",
      "epoch:47 step:36782 [D loss: 0.000986, acc.: 100.00%] [G loss: 0.007850]\n",
      "epoch:47 step:36783 [D loss: 0.028415, acc.: 100.00%] [G loss: 0.067026]\n",
      "epoch:47 step:36784 [D loss: 0.004188, acc.: 100.00%] [G loss: 0.027767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36785 [D loss: 0.000912, acc.: 100.00%] [G loss: 0.000747]\n",
      "epoch:47 step:36786 [D loss: 0.004436, acc.: 100.00%] [G loss: 0.000410]\n",
      "epoch:47 step:36787 [D loss: 0.001479, acc.: 100.00%] [G loss: 0.003091]\n",
      "epoch:47 step:36788 [D loss: 0.008903, acc.: 100.00%] [G loss: 0.000449]\n",
      "epoch:47 step:36789 [D loss: 0.001965, acc.: 100.00%] [G loss: 0.037755]\n",
      "epoch:47 step:36790 [D loss: 0.011991, acc.: 99.22%] [G loss: 0.089981]\n",
      "epoch:47 step:36791 [D loss: 0.008624, acc.: 99.22%] [G loss: 0.000101]\n",
      "epoch:47 step:36792 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:47 step:36793 [D loss: 0.001071, acc.: 100.00%] [G loss: 0.018718]\n",
      "epoch:47 step:36794 [D loss: 0.001811, acc.: 100.00%] [G loss: 0.000282]\n",
      "epoch:47 step:36795 [D loss: 0.000640, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:47 step:36796 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.008400]\n",
      "epoch:47 step:36797 [D loss: 0.001536, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:47 step:36798 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.022858]\n",
      "epoch:47 step:36799 [D loss: 0.001768, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:47 step:36800 [D loss: 0.015228, acc.: 99.22%] [G loss: 0.064757]\n",
      "epoch:47 step:36801 [D loss: 0.001275, acc.: 100.00%] [G loss: 0.102444]\n",
      "epoch:47 step:36802 [D loss: 0.000730, acc.: 100.00%] [G loss: 0.127359]\n",
      "epoch:47 step:36803 [D loss: 0.017122, acc.: 99.22%] [G loss: 0.129464]\n",
      "epoch:47 step:36804 [D loss: 0.010363, acc.: 99.22%] [G loss: 0.078547]\n",
      "epoch:47 step:36805 [D loss: 0.006608, acc.: 100.00%] [G loss: 0.000739]\n",
      "epoch:47 step:36806 [D loss: 0.031029, acc.: 97.66%] [G loss: 0.000046]\n",
      "epoch:47 step:36807 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.001001]\n",
      "epoch:47 step:36808 [D loss: 0.001574, acc.: 100.00%] [G loss: 0.021550]\n",
      "epoch:47 step:36809 [D loss: 0.004278, acc.: 100.00%] [G loss: 0.000194]\n",
      "epoch:47 step:36810 [D loss: 0.001354, acc.: 100.00%] [G loss: 0.000993]\n",
      "epoch:47 step:36811 [D loss: 0.000872, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:47 step:36812 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:47 step:36813 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.019496]\n",
      "epoch:47 step:36814 [D loss: 0.000947, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:47 step:36815 [D loss: 0.001386, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:47 step:36816 [D loss: 0.000498, acc.: 100.00%] [G loss: 0.049605]\n",
      "epoch:47 step:36817 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.075550]\n",
      "epoch:47 step:36818 [D loss: 0.001444, acc.: 100.00%] [G loss: 0.004424]\n",
      "epoch:47 step:36819 [D loss: 0.000474, acc.: 100.00%] [G loss: 0.007554]\n",
      "epoch:47 step:36820 [D loss: 0.013420, acc.: 100.00%] [G loss: 0.032938]\n",
      "epoch:47 step:36821 [D loss: 0.001875, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:47 step:36822 [D loss: 0.004486, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:47 step:36823 [D loss: 0.001496, acc.: 100.00%] [G loss: 0.001180]\n",
      "epoch:47 step:36824 [D loss: 0.000347, acc.: 100.00%] [G loss: 0.140418]\n",
      "epoch:47 step:36825 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.094292]\n",
      "epoch:47 step:36826 [D loss: 0.002241, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:47 step:36827 [D loss: 0.001794, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:47 step:36828 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.000376]\n",
      "epoch:47 step:36829 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:47 step:36830 [D loss: 0.000859, acc.: 100.00%] [G loss: 0.028717]\n",
      "epoch:47 step:36831 [D loss: 0.010829, acc.: 100.00%] [G loss: 0.001198]\n",
      "epoch:47 step:36832 [D loss: 0.007605, acc.: 100.00%] [G loss: 0.084208]\n",
      "epoch:47 step:36833 [D loss: 0.000675, acc.: 100.00%] [G loss: 0.030282]\n",
      "epoch:47 step:36834 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.000567]\n",
      "epoch:47 step:36835 [D loss: 0.002783, acc.: 100.00%] [G loss: 0.063292]\n",
      "epoch:47 step:36836 [D loss: 0.001940, acc.: 100.00%] [G loss: 0.003816]\n",
      "epoch:47 step:36837 [D loss: 0.001535, acc.: 100.00%] [G loss: 0.001194]\n",
      "epoch:47 step:36838 [D loss: 0.002042, acc.: 100.00%] [G loss: 0.003677]\n",
      "epoch:47 step:36839 [D loss: 0.000433, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:47 step:36840 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.051375]\n",
      "epoch:47 step:36841 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.133799]\n",
      "epoch:47 step:36842 [D loss: 0.005333, acc.: 100.00%] [G loss: 0.035719]\n",
      "epoch:47 step:36843 [D loss: 0.000918, acc.: 100.00%] [G loss: 0.009460]\n",
      "epoch:47 step:36844 [D loss: 0.001442, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:47 step:36845 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.000389]\n",
      "epoch:47 step:36846 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.005009]\n",
      "epoch:47 step:36847 [D loss: 0.001242, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:47 step:36848 [D loss: 0.001676, acc.: 100.00%] [G loss: 0.002408]\n",
      "epoch:47 step:36849 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:47 step:36850 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.044774]\n",
      "epoch:47 step:36851 [D loss: 0.000858, acc.: 100.00%] [G loss: 0.005761]\n",
      "epoch:47 step:36852 [D loss: 0.000969, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:47 step:36853 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.011639]\n",
      "epoch:47 step:36854 [D loss: 0.001705, acc.: 100.00%] [G loss: 0.000414]\n",
      "epoch:47 step:36855 [D loss: 0.000767, acc.: 100.00%] [G loss: 0.007147]\n",
      "epoch:47 step:36856 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.087675]\n",
      "epoch:47 step:36857 [D loss: 0.001183, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:47 step:36858 [D loss: 0.001499, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:47 step:36859 [D loss: 0.000688, acc.: 100.00%] [G loss: 0.001988]\n",
      "epoch:47 step:36860 [D loss: 0.001443, acc.: 100.00%] [G loss: 0.014768]\n",
      "epoch:47 step:36861 [D loss: 0.002430, acc.: 100.00%] [G loss: 0.023283]\n",
      "epoch:47 step:36862 [D loss: 0.000970, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:47 step:36863 [D loss: 0.001049, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:47 step:36864 [D loss: 0.000871, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:47 step:36865 [D loss: 0.000943, acc.: 100.00%] [G loss: 0.010497]\n",
      "epoch:47 step:36866 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.002489]\n",
      "epoch:47 step:36867 [D loss: 0.000899, acc.: 100.00%] [G loss: 0.036231]\n",
      "epoch:47 step:36868 [D loss: 0.004295, acc.: 100.00%] [G loss: 0.027395]\n",
      "epoch:47 step:36869 [D loss: 0.000805, acc.: 100.00%] [G loss: 0.076124]\n",
      "epoch:47 step:36870 [D loss: 0.002050, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:47 step:36871 [D loss: 0.003464, acc.: 100.00%] [G loss: 0.004223]\n",
      "epoch:47 step:36872 [D loss: 0.002394, acc.: 100.00%] [G loss: 0.122271]\n",
      "epoch:47 step:36873 [D loss: 0.001435, acc.: 100.00%] [G loss: 0.022403]\n",
      "epoch:47 step:36874 [D loss: 0.002450, acc.: 100.00%] [G loss: 0.000334]\n",
      "epoch:47 step:36875 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000038]\n",
      "epoch:47 step:36876 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000545]\n",
      "epoch:47 step:36877 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.000268]\n",
      "epoch:47 step:36878 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.014361]\n",
      "epoch:47 step:36879 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.020905]\n",
      "epoch:47 step:36880 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.039292]\n",
      "epoch:47 step:36881 [D loss: 0.005120, acc.: 100.00%] [G loss: 0.000631]\n",
      "epoch:47 step:36882 [D loss: 0.000428, acc.: 100.00%] [G loss: 0.001381]\n",
      "epoch:47 step:36883 [D loss: 0.001266, acc.: 100.00%] [G loss: 0.085002]\n",
      "epoch:47 step:36884 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000364]\n",
      "epoch:47 step:36885 [D loss: 0.004262, acc.: 100.00%] [G loss: 0.000518]\n",
      "epoch:47 step:36886 [D loss: 0.006156, acc.: 100.00%] [G loss: 0.001967]\n",
      "epoch:47 step:36887 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.018740]\n",
      "epoch:47 step:36888 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:47 step:36889 [D loss: 0.041725, acc.: 99.22%] [G loss: 0.000083]\n",
      "epoch:47 step:36890 [D loss: 0.000989, acc.: 100.00%] [G loss: 0.000058]\n",
      "epoch:47 step:36891 [D loss: 0.001032, acc.: 100.00%] [G loss: 0.005854]\n",
      "epoch:47 step:36892 [D loss: 0.013047, acc.: 99.22%] [G loss: 0.002943]\n",
      "epoch:47 step:36893 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:47 step:36894 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:47 step:36895 [D loss: 0.001666, acc.: 100.00%] [G loss: 0.000548]\n",
      "epoch:47 step:36896 [D loss: 0.001079, acc.: 100.00%] [G loss: 0.047981]\n",
      "epoch:47 step:36897 [D loss: 0.001838, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:47 step:36898 [D loss: 0.003053, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:47 step:36899 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.004757]\n",
      "epoch:47 step:36900 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:36901 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.018459]\n",
      "epoch:47 step:36902 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000881]\n",
      "epoch:47 step:36903 [D loss: 0.000420, acc.: 100.00%] [G loss: 0.001320]\n",
      "epoch:47 step:36904 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.004536]\n",
      "epoch:47 step:36905 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.002089]\n",
      "epoch:47 step:36906 [D loss: 0.004350, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:47 step:36907 [D loss: 0.002075, acc.: 100.00%] [G loss: 0.001994]\n",
      "epoch:47 step:36908 [D loss: 0.000944, acc.: 100.00%] [G loss: 0.013075]\n",
      "epoch:47 step:36909 [D loss: 0.002249, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:47 step:36910 [D loss: 0.000172, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:47 step:36911 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:47 step:36912 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000340]\n",
      "epoch:47 step:36913 [D loss: 0.001470, acc.: 100.00%] [G loss: 0.007945]\n",
      "epoch:47 step:36914 [D loss: 0.003099, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:47 step:36915 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:47 step:36916 [D loss: 0.003873, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:47 step:36917 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000140]\n",
      "epoch:47 step:36918 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000065]\n",
      "epoch:47 step:36919 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.006070]\n",
      "epoch:47 step:36920 [D loss: 0.000663, acc.: 100.00%] [G loss: 0.015087]\n",
      "epoch:47 step:36921 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:47 step:36922 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:47 step:36923 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:47 step:36924 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000015]\n",
      "epoch:47 step:36925 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:47 step:36926 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:47 step:36927 [D loss: 0.000984, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:47 step:36928 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000387]\n",
      "epoch:47 step:36929 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:47 step:36930 [D loss: 0.002490, acc.: 100.00%] [G loss: 0.008162]\n",
      "epoch:47 step:36931 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000024]\n",
      "epoch:47 step:36932 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000372]\n",
      "epoch:47 step:36933 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.002278]\n",
      "epoch:47 step:36934 [D loss: 0.001110, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:47 step:36935 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.000422]\n",
      "epoch:47 step:36936 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:47 step:36937 [D loss: 0.004214, acc.: 100.00%] [G loss: 0.004346]\n",
      "epoch:47 step:36938 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001023]\n",
      "epoch:47 step:36939 [D loss: 0.000687, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:47 step:36940 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.000496]\n",
      "epoch:47 step:36941 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.011383]\n",
      "epoch:47 step:36942 [D loss: 0.001923, acc.: 100.00%] [G loss: 0.001366]\n",
      "epoch:47 step:36943 [D loss: 0.000760, acc.: 100.00%] [G loss: 0.012970]\n",
      "epoch:47 step:36944 [D loss: 0.000942, acc.: 100.00%] [G loss: 0.000077]\n",
      "epoch:47 step:36945 [D loss: 0.002209, acc.: 100.00%] [G loss: 0.017207]\n",
      "epoch:47 step:36946 [D loss: 0.005302, acc.: 100.00%] [G loss: 0.000009]\n",
      "epoch:47 step:36947 [D loss: 0.003893, acc.: 100.00%] [G loss: 0.000480]\n",
      "epoch:47 step:36948 [D loss: 0.002619, acc.: 100.00%] [G loss: 0.052985]\n",
      "epoch:47 step:36949 [D loss: 0.004640, acc.: 100.00%] [G loss: 0.014732]\n",
      "epoch:47 step:36950 [D loss: 0.005278, acc.: 100.00%] [G loss: 0.000472]\n",
      "epoch:47 step:36951 [D loss: 0.000700, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:47 step:36952 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.037486]\n",
      "epoch:47 step:36953 [D loss: 0.003045, acc.: 100.00%] [G loss: 0.026262]\n",
      "epoch:47 step:36954 [D loss: 0.027460, acc.: 99.22%] [G loss: 0.000200]\n",
      "epoch:47 step:36955 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.044922]\n",
      "epoch:47 step:36956 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.168222]\n",
      "epoch:47 step:36957 [D loss: 0.005622, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:47 step:36958 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:47 step:36959 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:47 step:36960 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:47 step:36961 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.549508]\n",
      "epoch:47 step:36962 [D loss: 0.000811, acc.: 100.00%] [G loss: 0.000437]\n",
      "epoch:47 step:36963 [D loss: 0.000684, acc.: 100.00%] [G loss: 0.005040]\n",
      "epoch:47 step:36964 [D loss: 0.003248, acc.: 100.00%] [G loss: 0.000445]\n",
      "epoch:47 step:36965 [D loss: 0.000897, acc.: 100.00%] [G loss: 0.026815]\n",
      "epoch:47 step:36966 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.036760]\n",
      "epoch:47 step:36967 [D loss: 0.000321, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:47 step:36968 [D loss: 0.000146, acc.: 100.00%] [G loss: 0.000232]\n",
      "epoch:47 step:36969 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.000252]\n",
      "epoch:47 step:36970 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.057525]\n",
      "epoch:47 step:36971 [D loss: 0.006619, acc.: 100.00%] [G loss: 0.000758]\n",
      "epoch:47 step:36972 [D loss: 0.001400, acc.: 100.00%] [G loss: 0.038606]\n",
      "epoch:47 step:36973 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000080]\n",
      "epoch:47 step:36974 [D loss: 0.007724, acc.: 100.00%] [G loss: 0.000165]\n",
      "epoch:47 step:36975 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.000958]\n",
      "epoch:47 step:36976 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000889]\n",
      "epoch:47 step:36977 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.000381]\n",
      "epoch:47 step:36978 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:47 step:36979 [D loss: 0.000533, acc.: 100.00%] [G loss: 0.013480]\n",
      "epoch:47 step:36980 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.002611]\n",
      "epoch:47 step:36981 [D loss: 0.004394, acc.: 100.00%] [G loss: 0.025257]\n",
      "epoch:47 step:36982 [D loss: 0.000918, acc.: 100.00%] [G loss: 0.000781]\n",
      "epoch:47 step:36983 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.006973]\n",
      "epoch:47 step:36984 [D loss: 0.415358, acc.: 78.91%] [G loss: 4.550074]\n",
      "epoch:47 step:36985 [D loss: 2.101821, acc.: 56.25%] [G loss: 0.000272]\n",
      "epoch:47 step:36986 [D loss: 0.672136, acc.: 76.56%] [G loss: 5.107104]\n",
      "epoch:47 step:36987 [D loss: 0.005537, acc.: 100.00%] [G loss: 7.994877]\n",
      "epoch:47 step:36988 [D loss: 0.152107, acc.: 95.31%] [G loss: 0.109588]\n",
      "epoch:47 step:36989 [D loss: 0.032774, acc.: 99.22%] [G loss: 0.019680]\n",
      "epoch:47 step:36990 [D loss: 0.007542, acc.: 100.00%] [G loss: 0.068321]\n",
      "epoch:47 step:36991 [D loss: 0.004933, acc.: 100.00%] [G loss: 0.238080]\n",
      "epoch:47 step:36992 [D loss: 0.012027, acc.: 100.00%] [G loss: 2.803198]\n",
      "epoch:47 step:36993 [D loss: 0.009484, acc.: 99.22%] [G loss: 0.002432]\n",
      "epoch:47 step:36994 [D loss: 0.017486, acc.: 99.22%] [G loss: 0.007684]\n",
      "epoch:47 step:36995 [D loss: 0.004568, acc.: 100.00%] [G loss: 0.002634]\n",
      "epoch:47 step:36996 [D loss: 0.006572, acc.: 100.00%] [G loss: 1.641591]\n",
      "epoch:47 step:36997 [D loss: 0.010240, acc.: 100.00%] [G loss: 1.251781]\n",
      "epoch:47 step:36998 [D loss: 0.028782, acc.: 100.00%] [G loss: 0.571640]\n",
      "epoch:47 step:36999 [D loss: 0.015944, acc.: 100.00%] [G loss: 1.293134]\n",
      "epoch:47 step:37000 [D loss: 0.158836, acc.: 96.09%] [G loss: 3.715624]\n",
      "epoch:47 step:37001 [D loss: 0.173471, acc.: 91.41%] [G loss: 3.013350]\n",
      "epoch:47 step:37002 [D loss: 0.039923, acc.: 99.22%] [G loss: 0.040671]\n",
      "epoch:47 step:37003 [D loss: 0.007790, acc.: 100.00%] [G loss: 0.112883]\n",
      "epoch:47 step:37004 [D loss: 0.002022, acc.: 100.00%] [G loss: 2.508901]\n",
      "epoch:47 step:37005 [D loss: 0.010936, acc.: 100.00%] [G loss: 0.028466]\n",
      "epoch:47 step:37006 [D loss: 0.024946, acc.: 99.22%] [G loss: 0.091424]\n",
      "epoch:47 step:37007 [D loss: 0.002860, acc.: 100.00%] [G loss: 0.032874]\n",
      "epoch:47 step:37008 [D loss: 0.003682, acc.: 100.00%] [G loss: 0.033421]\n",
      "epoch:47 step:37009 [D loss: 0.002365, acc.: 100.00%] [G loss: 0.024138]\n",
      "epoch:47 step:37010 [D loss: 0.001713, acc.: 100.00%] [G loss: 0.017019]\n",
      "epoch:47 step:37011 [D loss: 0.000655, acc.: 100.00%] [G loss: 0.017715]\n",
      "epoch:47 step:37012 [D loss: 0.003844, acc.: 100.00%] [G loss: 0.006823]\n",
      "epoch:47 step:37013 [D loss: 0.005222, acc.: 100.00%] [G loss: 0.008574]\n",
      "epoch:47 step:37014 [D loss: 0.017460, acc.: 100.00%] [G loss: 0.004940]\n",
      "epoch:47 step:37015 [D loss: 0.028657, acc.: 99.22%] [G loss: 0.006770]\n",
      "epoch:47 step:37016 [D loss: 0.006415, acc.: 100.00%] [G loss: 0.006843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37017 [D loss: 0.011930, acc.: 100.00%] [G loss: 0.003029]\n",
      "epoch:47 step:37018 [D loss: 0.007150, acc.: 100.00%] [G loss: 0.006995]\n",
      "epoch:47 step:37019 [D loss: 0.031311, acc.: 100.00%] [G loss: 2.865152]\n",
      "epoch:47 step:37020 [D loss: 0.000733, acc.: 100.00%] [G loss: 0.038223]\n",
      "epoch:47 step:37021 [D loss: 0.000756, acc.: 100.00%] [G loss: 0.033625]\n",
      "epoch:47 step:37022 [D loss: 0.007744, acc.: 100.00%] [G loss: 0.009573]\n",
      "epoch:47 step:37023 [D loss: 0.002010, acc.: 100.00%] [G loss: 0.009330]\n",
      "epoch:47 step:37024 [D loss: 0.006420, acc.: 100.00%] [G loss: 1.802037]\n",
      "epoch:47 step:37025 [D loss: 0.000764, acc.: 100.00%] [G loss: 0.025829]\n",
      "epoch:47 step:37026 [D loss: 0.000359, acc.: 100.00%] [G loss: 1.091421]\n",
      "epoch:47 step:37027 [D loss: 0.004151, acc.: 100.00%] [G loss: 1.283677]\n",
      "epoch:47 step:37028 [D loss: 0.003146, acc.: 100.00%] [G loss: 0.002722]\n",
      "epoch:47 step:37029 [D loss: 0.014706, acc.: 100.00%] [G loss: 0.003102]\n",
      "epoch:47 step:37030 [D loss: 0.002040, acc.: 100.00%] [G loss: 0.865394]\n",
      "epoch:47 step:37031 [D loss: 0.007971, acc.: 100.00%] [G loss: 0.037545]\n",
      "epoch:47 step:37032 [D loss: 0.006014, acc.: 100.00%] [G loss: 0.010529]\n",
      "epoch:47 step:37033 [D loss: 0.001604, acc.: 100.00%] [G loss: 0.004112]\n",
      "epoch:47 step:37034 [D loss: 0.003549, acc.: 100.00%] [G loss: 0.006875]\n",
      "epoch:47 step:37035 [D loss: 0.003467, acc.: 100.00%] [G loss: 1.701751]\n",
      "epoch:47 step:37036 [D loss: 0.098492, acc.: 96.88%] [G loss: 1.476951]\n",
      "epoch:47 step:37037 [D loss: 0.045805, acc.: 97.66%] [G loss: 0.208520]\n",
      "epoch:47 step:37038 [D loss: 0.032837, acc.: 98.44%] [G loss: 0.918420]\n",
      "epoch:47 step:37039 [D loss: 0.065545, acc.: 97.66%] [G loss: 1.931279]\n",
      "epoch:47 step:37040 [D loss: 0.025525, acc.: 98.44%] [G loss: 0.304521]\n",
      "epoch:47 step:37041 [D loss: 0.026587, acc.: 99.22%] [G loss: 2.813252]\n",
      "epoch:47 step:37042 [D loss: 0.058239, acc.: 98.44%] [G loss: 0.125151]\n",
      "epoch:47 step:37043 [D loss: 0.086485, acc.: 97.66%] [G loss: 2.143773]\n",
      "epoch:47 step:37044 [D loss: 0.012491, acc.: 100.00%] [G loss: 0.003525]\n",
      "epoch:47 step:37045 [D loss: 0.019608, acc.: 100.00%] [G loss: 0.002214]\n",
      "epoch:47 step:37046 [D loss: 0.001588, acc.: 100.00%] [G loss: 1.163172]\n",
      "epoch:47 step:37047 [D loss: 0.004478, acc.: 100.00%] [G loss: 1.617770]\n",
      "epoch:47 step:37048 [D loss: 0.034430, acc.: 100.00%] [G loss: 0.011907]\n",
      "epoch:47 step:37049 [D loss: 0.009840, acc.: 99.22%] [G loss: 1.774959]\n",
      "epoch:47 step:37050 [D loss: 0.035643, acc.: 100.00%] [G loss: 0.033419]\n",
      "epoch:47 step:37051 [D loss: 0.064639, acc.: 98.44%] [G loss: 1.403913]\n",
      "epoch:47 step:37052 [D loss: 0.026336, acc.: 100.00%] [G loss: 3.788638]\n",
      "epoch:47 step:37053 [D loss: 0.071863, acc.: 97.66%] [G loss: 0.046804]\n",
      "epoch:47 step:37054 [D loss: 0.005450, acc.: 100.00%] [G loss: 0.050777]\n",
      "epoch:47 step:37055 [D loss: 0.005280, acc.: 100.00%] [G loss: 0.006424]\n",
      "epoch:47 step:37056 [D loss: 0.010399, acc.: 100.00%] [G loss: 3.936814]\n",
      "epoch:47 step:37057 [D loss: 0.013234, acc.: 100.00%] [G loss: 1.744672]\n",
      "epoch:47 step:37058 [D loss: 0.017261, acc.: 100.00%] [G loss: 0.061558]\n",
      "epoch:47 step:37059 [D loss: 0.015122, acc.: 100.00%] [G loss: 2.476359]\n",
      "epoch:47 step:37060 [D loss: 0.009162, acc.: 100.00%] [G loss: 3.571570]\n",
      "epoch:47 step:37061 [D loss: 0.039987, acc.: 99.22%] [G loss: 0.367802]\n",
      "epoch:47 step:37062 [D loss: 0.095061, acc.: 96.88%] [G loss: 0.018953]\n",
      "epoch:47 step:37063 [D loss: 0.002162, acc.: 100.00%] [G loss: 3.453447]\n",
      "epoch:47 step:37064 [D loss: 0.001276, acc.: 100.00%] [G loss: 0.043231]\n",
      "epoch:47 step:37065 [D loss: 0.013386, acc.: 100.00%] [G loss: 2.681082]\n",
      "epoch:47 step:37066 [D loss: 0.008284, acc.: 100.00%] [G loss: 3.655403]\n",
      "epoch:47 step:37067 [D loss: 0.006357, acc.: 100.00%] [G loss: 2.699967]\n",
      "epoch:47 step:37068 [D loss: 0.029756, acc.: 100.00%] [G loss: 3.354403]\n",
      "epoch:47 step:37069 [D loss: 0.021636, acc.: 100.00%] [G loss: 0.840535]\n",
      "epoch:47 step:37070 [D loss: 0.008029, acc.: 100.00%] [G loss: 0.014497]\n",
      "epoch:47 step:37071 [D loss: 0.041118, acc.: 99.22%] [G loss: 0.001523]\n",
      "epoch:47 step:37072 [D loss: 0.031481, acc.: 100.00%] [G loss: 0.000904]\n",
      "epoch:47 step:37073 [D loss: 0.011704, acc.: 99.22%] [G loss: 5.676366]\n",
      "epoch:47 step:37074 [D loss: 0.007474, acc.: 100.00%] [G loss: 0.003012]\n",
      "epoch:47 step:37075 [D loss: 0.009557, acc.: 100.00%] [G loss: 0.004269]\n",
      "epoch:47 step:37076 [D loss: 0.004247, acc.: 100.00%] [G loss: 0.004385]\n",
      "epoch:47 step:37077 [D loss: 0.002040, acc.: 100.00%] [G loss: 0.015120]\n",
      "epoch:47 step:37078 [D loss: 0.001219, acc.: 100.00%] [G loss: 0.004641]\n",
      "epoch:47 step:37079 [D loss: 0.004906, acc.: 100.00%] [G loss: 0.023549]\n",
      "epoch:47 step:37080 [D loss: 0.003529, acc.: 100.00%] [G loss: 0.020909]\n",
      "epoch:47 step:37081 [D loss: 0.006479, acc.: 100.00%] [G loss: 0.006102]\n",
      "epoch:47 step:37082 [D loss: 0.001945, acc.: 100.00%] [G loss: 0.009476]\n",
      "epoch:47 step:37083 [D loss: 0.013928, acc.: 99.22%] [G loss: 0.005364]\n",
      "epoch:47 step:37084 [D loss: 0.036431, acc.: 100.00%] [G loss: 0.046859]\n",
      "epoch:47 step:37085 [D loss: 0.041650, acc.: 97.66%] [G loss: 0.070668]\n",
      "epoch:47 step:37086 [D loss: 0.001845, acc.: 100.00%] [G loss: 6.141665]\n",
      "epoch:47 step:37087 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.318193]\n",
      "epoch:47 step:37088 [D loss: 0.002311, acc.: 100.00%] [G loss: 5.166090]\n",
      "epoch:47 step:37089 [D loss: 0.003657, acc.: 100.00%] [G loss: 0.017375]\n",
      "epoch:47 step:37090 [D loss: 0.003950, acc.: 100.00%] [G loss: 0.042447]\n",
      "epoch:47 step:37091 [D loss: 0.023362, acc.: 100.00%] [G loss: 5.831167]\n",
      "epoch:47 step:37092 [D loss: 0.006026, acc.: 100.00%] [G loss: 0.537097]\n",
      "epoch:47 step:37093 [D loss: 0.006167, acc.: 100.00%] [G loss: 0.114848]\n",
      "epoch:47 step:37094 [D loss: 0.000992, acc.: 100.00%] [G loss: 0.066597]\n",
      "epoch:47 step:37095 [D loss: 0.003677, acc.: 100.00%] [G loss: 0.053786]\n",
      "epoch:47 step:37096 [D loss: 0.001485, acc.: 100.00%] [G loss: 0.004663]\n",
      "epoch:47 step:37097 [D loss: 0.002716, acc.: 100.00%] [G loss: 0.001434]\n",
      "epoch:47 step:37098 [D loss: 0.004607, acc.: 100.00%] [G loss: 0.008620]\n",
      "epoch:47 step:37099 [D loss: 0.006368, acc.: 100.00%] [G loss: 0.042054]\n",
      "epoch:47 step:37100 [D loss: 0.002144, acc.: 100.00%] [G loss: 5.048828]\n",
      "epoch:47 step:37101 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.329761]\n",
      "epoch:47 step:37102 [D loss: 0.109808, acc.: 96.09%] [G loss: 0.000198]\n",
      "epoch:47 step:37103 [D loss: 0.008594, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:47 step:37104 [D loss: 0.003149, acc.: 100.00%] [G loss: 0.127378]\n",
      "epoch:47 step:37105 [D loss: 0.003253, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:47 step:37106 [D loss: 0.000359, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:47 step:37107 [D loss: 0.001042, acc.: 100.00%] [G loss: 0.000082]\n",
      "epoch:47 step:37108 [D loss: 0.000483, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:47 step:37109 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.674961]\n",
      "epoch:47 step:37110 [D loss: 0.116107, acc.: 95.31%] [G loss: 0.280704]\n",
      "epoch:47 step:37111 [D loss: 0.000000, acc.: 100.00%] [G loss: 3.915409]\n",
      "epoch:47 step:37112 [D loss: 2.467684, acc.: 42.19%] [G loss: 9.411289]\n",
      "epoch:47 step:37113 [D loss: 0.103937, acc.: 95.31%] [G loss: 10.779589]\n",
      "epoch:47 step:37114 [D loss: 0.491903, acc.: 82.81%] [G loss: 3.569542]\n",
      "epoch:47 step:37115 [D loss: 0.033344, acc.: 98.44%] [G loss: 4.047338]\n",
      "epoch:47 step:37116 [D loss: 0.003097, acc.: 100.00%] [G loss: 2.932475]\n",
      "epoch:47 step:37117 [D loss: 0.048824, acc.: 98.44%] [G loss: 2.090933]\n",
      "epoch:47 step:37118 [D loss: 0.106966, acc.: 96.09%] [G loss: 1.303175]\n",
      "epoch:47 step:37119 [D loss: 0.024986, acc.: 99.22%] [G loss: 1.797269]\n",
      "epoch:47 step:37120 [D loss: 0.051954, acc.: 97.66%] [G loss: 0.421658]\n",
      "epoch:47 step:37121 [D loss: 0.104295, acc.: 96.09%] [G loss: 1.338592]\n",
      "epoch:47 step:37122 [D loss: 0.051871, acc.: 97.66%] [G loss: 1.660599]\n",
      "epoch:47 step:37123 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.719558]\n",
      "epoch:47 step:37124 [D loss: 0.134674, acc.: 94.53%] [G loss: 2.075593]\n",
      "epoch:47 step:37125 [D loss: 0.059167, acc.: 97.66%] [G loss: 1.437840]\n",
      "epoch:47 step:37126 [D loss: 0.009200, acc.: 100.00%] [G loss: 0.057769]\n",
      "epoch:47 step:37127 [D loss: 0.083520, acc.: 96.09%] [G loss: 0.562378]\n",
      "epoch:47 step:37128 [D loss: 0.039550, acc.: 99.22%] [G loss: 0.001190]\n",
      "epoch:47 step:37129 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.002559]\n",
      "epoch:47 step:37130 [D loss: 0.002554, acc.: 100.00%] [G loss: 0.000620]\n",
      "epoch:47 step:37131 [D loss: 0.005307, acc.: 100.00%] [G loss: 0.428641]\n",
      "epoch:47 step:37132 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.000366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37133 [D loss: 0.000779, acc.: 100.00%] [G loss: 0.000209]\n",
      "epoch:47 step:37134 [D loss: 0.019494, acc.: 100.00%] [G loss: 0.622698]\n",
      "epoch:47 step:37135 [D loss: 0.015386, acc.: 99.22%] [G loss: 0.030719]\n",
      "epoch:47 step:37136 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.305020]\n",
      "epoch:47 step:37137 [D loss: 0.018731, acc.: 100.00%] [G loss: 2.754162]\n",
      "epoch:47 step:37138 [D loss: 0.975911, acc.: 64.84%] [G loss: 8.331279]\n",
      "epoch:47 step:37139 [D loss: 5.921395, acc.: 50.00%] [G loss: 2.005005]\n",
      "epoch:47 step:37140 [D loss: 0.166282, acc.: 94.53%] [G loss: 0.608841]\n",
      "epoch:47 step:37141 [D loss: 0.126335, acc.: 93.75%] [G loss: 3.149071]\n",
      "epoch:47 step:37142 [D loss: 0.027133, acc.: 100.00%] [G loss: 0.342698]\n",
      "epoch:47 step:37143 [D loss: 0.075508, acc.: 98.44%] [G loss: 0.362838]\n",
      "epoch:47 step:37144 [D loss: 0.060781, acc.: 97.66%] [G loss: 0.529629]\n",
      "epoch:47 step:37145 [D loss: 0.130036, acc.: 96.09%] [G loss: 0.928711]\n",
      "epoch:47 step:37146 [D loss: 0.043496, acc.: 100.00%] [G loss: 2.692308]\n",
      "epoch:47 step:37147 [D loss: 0.085285, acc.: 98.44%] [G loss: 0.164578]\n",
      "epoch:47 step:37148 [D loss: 0.071090, acc.: 99.22%] [G loss: 0.546700]\n",
      "epoch:47 step:37149 [D loss: 0.043781, acc.: 100.00%] [G loss: 0.031822]\n",
      "epoch:47 step:37150 [D loss: 0.033775, acc.: 99.22%] [G loss: 0.116699]\n",
      "epoch:47 step:37151 [D loss: 0.053477, acc.: 100.00%] [G loss: 2.185901]\n",
      "epoch:47 step:37152 [D loss: 0.281281, acc.: 87.50%] [G loss: 3.858919]\n",
      "epoch:47 step:37153 [D loss: 0.293801, acc.: 88.28%] [G loss: 4.212780]\n",
      "epoch:47 step:37154 [D loss: 0.142153, acc.: 96.88%] [G loss: 2.777177]\n",
      "epoch:47 step:37155 [D loss: 0.103227, acc.: 95.31%] [G loss: 1.104892]\n",
      "epoch:47 step:37156 [D loss: 0.017457, acc.: 100.00%] [G loss: 0.566286]\n",
      "epoch:47 step:37157 [D loss: 0.020799, acc.: 100.00%] [G loss: 0.408269]\n",
      "epoch:47 step:37158 [D loss: 0.045287, acc.: 98.44%] [G loss: 0.120474]\n",
      "epoch:47 step:37159 [D loss: 0.017725, acc.: 100.00%] [G loss: 0.104154]\n",
      "epoch:47 step:37160 [D loss: 0.039766, acc.: 100.00%] [G loss: 0.041315]\n",
      "epoch:47 step:37161 [D loss: 0.014671, acc.: 100.00%] [G loss: 2.460833]\n",
      "epoch:47 step:37162 [D loss: 0.035198, acc.: 98.44%] [G loss: 0.004456]\n",
      "epoch:47 step:37163 [D loss: 0.021790, acc.: 100.00%] [G loss: 0.037791]\n",
      "epoch:47 step:37164 [D loss: 0.020162, acc.: 100.00%] [G loss: 3.265415]\n",
      "epoch:47 step:37165 [D loss: 0.015424, acc.: 99.22%] [G loss: 0.095750]\n",
      "epoch:47 step:37166 [D loss: 0.035570, acc.: 99.22%] [G loss: 0.042359]\n",
      "epoch:47 step:37167 [D loss: 0.055715, acc.: 99.22%] [G loss: 1.081256]\n",
      "epoch:47 step:37168 [D loss: 0.009744, acc.: 100.00%] [G loss: 0.023602]\n",
      "epoch:47 step:37169 [D loss: 0.002934, acc.: 100.00%] [G loss: 0.052712]\n",
      "epoch:47 step:37170 [D loss: 0.002413, acc.: 100.00%] [G loss: 0.011288]\n",
      "epoch:47 step:37171 [D loss: 0.020423, acc.: 99.22%] [G loss: 0.054098]\n",
      "epoch:47 step:37172 [D loss: 0.001565, acc.: 100.00%] [G loss: 0.272234]\n",
      "epoch:47 step:37173 [D loss: 0.007898, acc.: 100.00%] [G loss: 0.009662]\n",
      "epoch:47 step:37174 [D loss: 0.014297, acc.: 100.00%] [G loss: 0.007812]\n",
      "epoch:47 step:37175 [D loss: 0.005210, acc.: 100.00%] [G loss: 0.028299]\n",
      "epoch:47 step:37176 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.019036]\n",
      "epoch:47 step:37177 [D loss: 0.010690, acc.: 100.00%] [G loss: 0.178795]\n",
      "epoch:47 step:37178 [D loss: 0.002107, acc.: 100.00%] [G loss: 0.009295]\n",
      "epoch:47 step:37179 [D loss: 0.001547, acc.: 100.00%] [G loss: 0.008038]\n",
      "epoch:47 step:37180 [D loss: 0.001792, acc.: 100.00%] [G loss: 0.037747]\n",
      "epoch:47 step:37181 [D loss: 0.002153, acc.: 100.00%] [G loss: 0.007163]\n",
      "epoch:47 step:37182 [D loss: 0.007721, acc.: 100.00%] [G loss: 0.004882]\n",
      "epoch:47 step:37183 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.003894]\n",
      "epoch:47 step:37184 [D loss: 0.000999, acc.: 100.00%] [G loss: 0.001554]\n",
      "epoch:47 step:37185 [D loss: 0.004953, acc.: 100.00%] [G loss: 0.007739]\n",
      "epoch:47 step:37186 [D loss: 0.003016, acc.: 100.00%] [G loss: 0.000874]\n",
      "epoch:47 step:37187 [D loss: 0.013351, acc.: 100.00%] [G loss: 0.042040]\n",
      "epoch:47 step:37188 [D loss: 0.002884, acc.: 100.00%] [G loss: 0.007144]\n",
      "epoch:47 step:37189 [D loss: 0.005767, acc.: 100.00%] [G loss: 0.018269]\n",
      "epoch:47 step:37190 [D loss: 0.000678, acc.: 100.00%] [G loss: 0.003776]\n",
      "epoch:47 step:37191 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.102602]\n",
      "epoch:47 step:37192 [D loss: 0.003464, acc.: 100.00%] [G loss: 0.006898]\n",
      "epoch:47 step:37193 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.011607]\n",
      "epoch:47 step:37194 [D loss: 0.000581, acc.: 100.00%] [G loss: 0.055446]\n",
      "epoch:47 step:37195 [D loss: 0.000237, acc.: 100.00%] [G loss: 0.001118]\n",
      "epoch:47 step:37196 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.000560]\n",
      "epoch:47 step:37197 [D loss: 0.000151, acc.: 100.00%] [G loss: 0.000657]\n",
      "epoch:47 step:37198 [D loss: 0.000888, acc.: 100.00%] [G loss: 0.002211]\n",
      "epoch:47 step:37199 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.000377]\n",
      "epoch:47 step:37200 [D loss: 0.002895, acc.: 100.00%] [G loss: 0.000736]\n",
      "epoch:47 step:37201 [D loss: 0.000453, acc.: 100.00%] [G loss: 0.002464]\n",
      "epoch:47 step:37202 [D loss: 0.000441, acc.: 100.00%] [G loss: 0.009825]\n",
      "epoch:47 step:37203 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.000845]\n",
      "epoch:47 step:37204 [D loss: 0.002699, acc.: 100.00%] [G loss: 0.003017]\n",
      "epoch:47 step:37205 [D loss: 0.004595, acc.: 100.00%] [G loss: 0.000552]\n",
      "epoch:47 step:37206 [D loss: 0.000577, acc.: 100.00%] [G loss: 0.001526]\n",
      "epoch:47 step:37207 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000074]\n",
      "epoch:47 step:37208 [D loss: 0.007283, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:47 step:37209 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000152]\n",
      "epoch:47 step:37210 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.001744]\n",
      "epoch:47 step:37211 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:47 step:37212 [D loss: 0.001134, acc.: 100.00%] [G loss: 0.013698]\n",
      "epoch:47 step:37213 [D loss: 0.002495, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:47 step:37214 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:47 step:37215 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:47 step:37216 [D loss: 0.002087, acc.: 100.00%] [G loss: 0.002797]\n",
      "epoch:47 step:37217 [D loss: 0.014382, acc.: 100.00%] [G loss: 0.146476]\n",
      "epoch:47 step:37218 [D loss: 0.001520, acc.: 100.00%] [G loss: 0.008735]\n",
      "epoch:47 step:37219 [D loss: 0.002353, acc.: 100.00%] [G loss: 0.002218]\n",
      "epoch:47 step:37220 [D loss: 0.002482, acc.: 100.00%] [G loss: 0.004577]\n",
      "epoch:47 step:37221 [D loss: 0.000549, acc.: 100.00%] [G loss: 0.001603]\n",
      "epoch:47 step:37222 [D loss: 0.001133, acc.: 100.00%] [G loss: 0.001375]\n",
      "epoch:47 step:37223 [D loss: 0.002438, acc.: 100.00%] [G loss: 0.005387]\n",
      "epoch:47 step:37224 [D loss: 0.029976, acc.: 100.00%] [G loss: 0.007028]\n",
      "epoch:47 step:37225 [D loss: 0.003432, acc.: 100.00%] [G loss: 0.013722]\n",
      "epoch:47 step:37226 [D loss: 0.006502, acc.: 100.00%] [G loss: 0.066579]\n",
      "epoch:47 step:37227 [D loss: 0.000565, acc.: 100.00%] [G loss: 0.031347]\n",
      "epoch:47 step:37228 [D loss: 0.001560, acc.: 100.00%] [G loss: 2.905560]\n",
      "epoch:47 step:37229 [D loss: 0.001728, acc.: 100.00%] [G loss: 0.019435]\n",
      "epoch:47 step:37230 [D loss: 0.015347, acc.: 100.00%] [G loss: 0.142558]\n",
      "epoch:47 step:37231 [D loss: 0.017485, acc.: 98.44%] [G loss: 0.005283]\n",
      "epoch:47 step:37232 [D loss: 0.013859, acc.: 100.00%] [G loss: 0.013044]\n",
      "epoch:47 step:37233 [D loss: 0.052715, acc.: 97.66%] [G loss: 0.001920]\n",
      "epoch:47 step:37234 [D loss: 0.016470, acc.: 100.00%] [G loss: 0.070048]\n",
      "epoch:47 step:37235 [D loss: 0.722074, acc.: 67.97%] [G loss: 6.841863]\n",
      "epoch:47 step:37236 [D loss: 2.337485, acc.: 53.91%] [G loss: 2.911801]\n",
      "epoch:47 step:37237 [D loss: 0.028674, acc.: 99.22%] [G loss: 1.129246]\n",
      "epoch:47 step:37238 [D loss: 0.065246, acc.: 98.44%] [G loss: 0.818456]\n",
      "epoch:47 step:37239 [D loss: 0.122403, acc.: 96.09%] [G loss: 0.826733]\n",
      "epoch:47 step:37240 [D loss: 0.037837, acc.: 99.22%] [G loss: 2.694216]\n",
      "epoch:47 step:37241 [D loss: 0.023814, acc.: 99.22%] [G loss: 0.335890]\n",
      "epoch:47 step:37242 [D loss: 0.010472, acc.: 100.00%] [G loss: 0.133156]\n",
      "epoch:47 step:37243 [D loss: 0.056673, acc.: 97.66%] [G loss: 0.647206]\n",
      "epoch:47 step:37244 [D loss: 0.081365, acc.: 97.66%] [G loss: 0.248387]\n",
      "epoch:47 step:37245 [D loss: 0.054523, acc.: 99.22%] [G loss: 0.192189]\n",
      "epoch:47 step:37246 [D loss: 0.056705, acc.: 98.44%] [G loss: 0.070948]\n",
      "epoch:47 step:37247 [D loss: 0.045607, acc.: 97.66%] [G loss: 0.231481]\n",
      "epoch:47 step:37248 [D loss: 0.014253, acc.: 100.00%] [G loss: 0.139101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37249 [D loss: 0.033350, acc.: 99.22%] [G loss: 0.040972]\n",
      "epoch:47 step:37250 [D loss: 0.005706, acc.: 100.00%] [G loss: 0.001782]\n",
      "epoch:47 step:37251 [D loss: 0.001555, acc.: 100.00%] [G loss: 0.003349]\n",
      "epoch:47 step:37252 [D loss: 0.003881, acc.: 100.00%] [G loss: 0.039789]\n",
      "epoch:47 step:37253 [D loss: 0.000504, acc.: 100.00%] [G loss: 0.002178]\n",
      "epoch:47 step:37254 [D loss: 0.003195, acc.: 100.00%] [G loss: 0.129013]\n",
      "epoch:47 step:37255 [D loss: 0.006346, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:47 step:37256 [D loss: 0.000486, acc.: 100.00%] [G loss: 0.001365]\n",
      "epoch:47 step:37257 [D loss: 0.003485, acc.: 100.00%] [G loss: 0.003246]\n",
      "epoch:47 step:37258 [D loss: 0.000529, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:47 step:37259 [D loss: 0.048959, acc.: 97.66%] [G loss: 0.003115]\n",
      "epoch:47 step:37260 [D loss: 0.002417, acc.: 100.00%] [G loss: 0.059628]\n",
      "epoch:47 step:37261 [D loss: 0.006910, acc.: 100.00%] [G loss: 0.167287]\n",
      "epoch:47 step:37262 [D loss: 0.004886, acc.: 100.00%] [G loss: 0.041965]\n",
      "epoch:47 step:37263 [D loss: 0.037964, acc.: 99.22%] [G loss: 0.036989]\n",
      "epoch:47 step:37264 [D loss: 0.047819, acc.: 99.22%] [G loss: 0.036270]\n",
      "epoch:47 step:37265 [D loss: 0.042414, acc.: 100.00%] [G loss: 0.254299]\n",
      "epoch:47 step:37266 [D loss: 0.019693, acc.: 100.00%] [G loss: 0.762439]\n",
      "epoch:47 step:37267 [D loss: 0.191112, acc.: 91.41%] [G loss: 5.638050]\n",
      "epoch:47 step:37268 [D loss: 0.376754, acc.: 82.81%] [G loss: 0.525440]\n",
      "epoch:47 step:37269 [D loss: 0.001538, acc.: 100.00%] [G loss: 0.129535]\n",
      "epoch:47 step:37270 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.058140]\n",
      "epoch:47 step:37271 [D loss: 0.063323, acc.: 97.66%] [G loss: 0.332772]\n",
      "epoch:47 step:37272 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.401782]\n",
      "epoch:47 step:37273 [D loss: 0.004712, acc.: 100.00%] [G loss: 0.477182]\n",
      "epoch:47 step:37274 [D loss: 0.009174, acc.: 100.00%] [G loss: 0.152501]\n",
      "epoch:47 step:37275 [D loss: 0.000671, acc.: 100.00%] [G loss: 0.141412]\n",
      "epoch:47 step:37276 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.061791]\n",
      "epoch:47 step:37277 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.025460]\n",
      "epoch:47 step:37278 [D loss: 0.018884, acc.: 99.22%] [G loss: 0.006814]\n",
      "epoch:47 step:37279 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.040618]\n",
      "epoch:47 step:37280 [D loss: 0.001593, acc.: 100.00%] [G loss: 0.001708]\n",
      "epoch:47 step:37281 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.002427]\n",
      "epoch:47 step:37282 [D loss: 0.009448, acc.: 99.22%] [G loss: 0.022712]\n",
      "epoch:47 step:37283 [D loss: 0.000442, acc.: 100.00%] [G loss: 0.042592]\n",
      "epoch:47 step:37284 [D loss: 0.068860, acc.: 97.66%] [G loss: 0.115456]\n",
      "epoch:47 step:37285 [D loss: 0.004307, acc.: 100.00%] [G loss: 0.012703]\n",
      "epoch:47 step:37286 [D loss: 0.005015, acc.: 100.00%] [G loss: 0.005195]\n",
      "epoch:47 step:37287 [D loss: 0.055431, acc.: 99.22%] [G loss: 0.001234]\n",
      "epoch:47 step:37288 [D loss: 0.001591, acc.: 100.00%] [G loss: 0.001166]\n",
      "epoch:47 step:37289 [D loss: 0.004231, acc.: 100.00%] [G loss: 0.001976]\n",
      "epoch:47 step:37290 [D loss: 0.004266, acc.: 100.00%] [G loss: 0.000453]\n",
      "epoch:47 step:37291 [D loss: 0.001932, acc.: 100.00%] [G loss: 0.000233]\n",
      "epoch:47 step:37292 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:47 step:37293 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:47 step:37294 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.012265]\n",
      "epoch:47 step:37295 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.005949]\n",
      "epoch:47 step:37296 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000796]\n",
      "epoch:47 step:37297 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.008032]\n",
      "epoch:47 step:37298 [D loss: 0.002083, acc.: 100.00%] [G loss: 0.000183]\n",
      "epoch:47 step:37299 [D loss: 0.001148, acc.: 100.00%] [G loss: 0.001360]\n",
      "epoch:47 step:37300 [D loss: 0.001857, acc.: 100.00%] [G loss: 0.000715]\n",
      "epoch:47 step:37301 [D loss: 0.001119, acc.: 100.00%] [G loss: 0.265334]\n",
      "epoch:47 step:37302 [D loss: 0.002747, acc.: 100.00%] [G loss: 0.001519]\n",
      "epoch:47 step:37303 [D loss: 0.039533, acc.: 99.22%] [G loss: 0.001433]\n",
      "epoch:47 step:37304 [D loss: 0.000388, acc.: 100.00%] [G loss: 0.004615]\n",
      "epoch:47 step:37305 [D loss: 0.001110, acc.: 100.00%] [G loss: 0.004528]\n",
      "epoch:47 step:37306 [D loss: 0.000533, acc.: 100.00%] [G loss: 0.082044]\n",
      "epoch:47 step:37307 [D loss: 0.005707, acc.: 100.00%] [G loss: 0.003730]\n",
      "epoch:47 step:37308 [D loss: 0.030652, acc.: 98.44%] [G loss: 0.015100]\n",
      "epoch:47 step:37309 [D loss: 0.001907, acc.: 100.00%] [G loss: 0.000593]\n",
      "epoch:47 step:37310 [D loss: 0.001039, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:47 step:37311 [D loss: 0.001356, acc.: 100.00%] [G loss: 0.029972]\n",
      "epoch:47 step:37312 [D loss: 0.003340, acc.: 100.00%] [G loss: 0.007444]\n",
      "epoch:47 step:37313 [D loss: 0.215195, acc.: 90.62%] [G loss: 0.360374]\n",
      "epoch:47 step:37314 [D loss: 0.128439, acc.: 95.31%] [G loss: 0.507112]\n",
      "epoch:47 step:37315 [D loss: 0.032308, acc.: 97.66%] [G loss: 0.060177]\n",
      "epoch:47 step:37316 [D loss: 0.031837, acc.: 99.22%] [G loss: 0.003613]\n",
      "epoch:47 step:37317 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.001714]\n",
      "epoch:47 step:37318 [D loss: 0.000335, acc.: 100.00%] [G loss: 0.000884]\n",
      "epoch:47 step:37319 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.002572]\n",
      "epoch:47 step:37320 [D loss: 0.012048, acc.: 99.22%] [G loss: 1.187488]\n",
      "epoch:47 step:37321 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.588802]\n",
      "epoch:47 step:37322 [D loss: 0.000494, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:47 step:37323 [D loss: 0.001024, acc.: 100.00%] [G loss: 0.000461]\n",
      "epoch:47 step:37324 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.262350]\n",
      "epoch:47 step:37325 [D loss: 0.004259, acc.: 100.00%] [G loss: 0.001421]\n",
      "epoch:47 step:37326 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:47 step:37327 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.032940]\n",
      "epoch:47 step:37328 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.001616]\n",
      "epoch:47 step:37329 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000207]\n",
      "epoch:47 step:37330 [D loss: 0.011845, acc.: 99.22%] [G loss: 0.000079]\n",
      "epoch:47 step:37331 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:47 step:37332 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:47 step:37333 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.099061]\n",
      "epoch:47 step:37334 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:47 step:37335 [D loss: 0.000844, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:47 step:37336 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:47 step:37337 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:47 step:37338 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:47 step:37339 [D loss: 0.000626, acc.: 100.00%] [G loss: 0.001235]\n",
      "epoch:47 step:37340 [D loss: 0.015141, acc.: 99.22%] [G loss: 0.000058]\n",
      "epoch:47 step:37341 [D loss: 0.001715, acc.: 100.00%] [G loss: 0.001283]\n",
      "epoch:47 step:37342 [D loss: 0.073151, acc.: 99.22%] [G loss: 0.923681]\n",
      "epoch:47 step:37343 [D loss: 0.001281, acc.: 100.00%] [G loss: 1.491594]\n",
      "epoch:47 step:37344 [D loss: 0.091482, acc.: 95.31%] [G loss: 2.769083]\n",
      "epoch:47 step:37345 [D loss: 0.004814, acc.: 100.00%] [G loss: 1.880444]\n",
      "epoch:47 step:37346 [D loss: 0.007238, acc.: 100.00%] [G loss: 0.051955]\n",
      "epoch:47 step:37347 [D loss: 0.611030, acc.: 72.66%] [G loss: 8.228545]\n",
      "epoch:47 step:37348 [D loss: 0.926489, acc.: 62.50%] [G loss: 2.173965]\n",
      "epoch:47 step:37349 [D loss: 0.104589, acc.: 96.09%] [G loss: 0.991977]\n",
      "epoch:47 step:37350 [D loss: 0.005907, acc.: 100.00%] [G loss: 0.990527]\n",
      "epoch:47 step:37351 [D loss: 0.000943, acc.: 100.00%] [G loss: 0.573261]\n",
      "epoch:47 step:37352 [D loss: 0.000609, acc.: 100.00%] [G loss: 0.309254]\n",
      "epoch:47 step:37353 [D loss: 0.080830, acc.: 97.66%] [G loss: 0.018871]\n",
      "epoch:47 step:37354 [D loss: 0.001121, acc.: 100.00%] [G loss: 0.011051]\n",
      "epoch:47 step:37355 [D loss: 0.000842, acc.: 100.00%] [G loss: 0.003844]\n",
      "epoch:47 step:37356 [D loss: 0.012020, acc.: 99.22%] [G loss: 0.068271]\n",
      "epoch:47 step:37357 [D loss: 0.006916, acc.: 100.00%] [G loss: 0.006355]\n",
      "epoch:47 step:37358 [D loss: 0.005971, acc.: 100.00%] [G loss: 0.002173]\n",
      "epoch:47 step:37359 [D loss: 0.000944, acc.: 100.00%] [G loss: 0.004412]\n",
      "epoch:47 step:37360 [D loss: 0.001128, acc.: 100.00%] [G loss: 0.006229]\n",
      "epoch:47 step:37361 [D loss: 0.001184, acc.: 100.00%] [G loss: 0.003629]\n",
      "epoch:47 step:37362 [D loss: 0.000642, acc.: 100.00%] [G loss: 0.002035]\n",
      "epoch:47 step:37363 [D loss: 0.002989, acc.: 100.00%] [G loss: 0.003204]\n",
      "epoch:47 step:37364 [D loss: 0.001920, acc.: 100.00%] [G loss: 0.016139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37365 [D loss: 0.000875, acc.: 100.00%] [G loss: 0.017028]\n",
      "epoch:47 step:37366 [D loss: 0.034213, acc.: 100.00%] [G loss: 0.048492]\n",
      "epoch:47 step:37367 [D loss: 0.014915, acc.: 100.00%] [G loss: 0.072241]\n",
      "epoch:47 step:37368 [D loss: 0.029508, acc.: 99.22%] [G loss: 0.188777]\n",
      "epoch:47 step:37369 [D loss: 0.014392, acc.: 100.00%] [G loss: 0.453626]\n",
      "epoch:47 step:37370 [D loss: 0.010312, acc.: 100.00%] [G loss: 0.079307]\n",
      "epoch:47 step:37371 [D loss: 0.006243, acc.: 100.00%] [G loss: 0.081557]\n",
      "epoch:47 step:37372 [D loss: 0.099494, acc.: 97.66%] [G loss: 0.302583]\n",
      "epoch:47 step:37373 [D loss: 0.013152, acc.: 100.00%] [G loss: 1.477353]\n",
      "epoch:47 step:37374 [D loss: 0.002377, acc.: 100.00%] [G loss: 1.130289]\n",
      "epoch:47 step:37375 [D loss: 0.063270, acc.: 98.44%] [G loss: 0.653180]\n",
      "epoch:47 step:37376 [D loss: 0.008978, acc.: 100.00%] [G loss: 0.039640]\n",
      "epoch:47 step:37377 [D loss: 0.004227, acc.: 100.00%] [G loss: 0.133755]\n",
      "epoch:47 step:37378 [D loss: 0.008252, acc.: 100.00%] [G loss: 0.023617]\n",
      "epoch:47 step:37379 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.010200]\n",
      "epoch:47 step:37380 [D loss: 0.001219, acc.: 100.00%] [G loss: 0.006306]\n",
      "epoch:47 step:37381 [D loss: 0.003597, acc.: 100.00%] [G loss: 0.002780]\n",
      "epoch:47 step:37382 [D loss: 0.000600, acc.: 100.00%] [G loss: 0.121869]\n",
      "epoch:47 step:37383 [D loss: 0.001287, acc.: 100.00%] [G loss: 0.035783]\n",
      "epoch:47 step:37384 [D loss: 0.000270, acc.: 100.00%] [G loss: 0.005024]\n",
      "epoch:47 step:37385 [D loss: 0.001006, acc.: 100.00%] [G loss: 0.038214]\n",
      "epoch:47 step:37386 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.006180]\n",
      "epoch:47 step:37387 [D loss: 0.001797, acc.: 100.00%] [G loss: 1.706590]\n",
      "epoch:47 step:37388 [D loss: 0.000752, acc.: 100.00%] [G loss: 0.007803]\n",
      "epoch:47 step:37389 [D loss: 0.004087, acc.: 100.00%] [G loss: 0.005771]\n",
      "epoch:47 step:37390 [D loss: 0.007993, acc.: 100.00%] [G loss: 0.003747]\n",
      "epoch:47 step:37391 [D loss: 0.005665, acc.: 100.00%] [G loss: 0.005194]\n",
      "epoch:47 step:37392 [D loss: 0.008414, acc.: 100.00%] [G loss: 0.008765]\n",
      "epoch:47 step:37393 [D loss: 0.062614, acc.: 98.44%] [G loss: 0.065993]\n",
      "epoch:47 step:37394 [D loss: 0.023928, acc.: 99.22%] [G loss: 0.023917]\n",
      "epoch:47 step:37395 [D loss: 0.026282, acc.: 100.00%] [G loss: 0.130707]\n",
      "epoch:47 step:37396 [D loss: 0.076821, acc.: 98.44%] [G loss: 0.624764]\n",
      "epoch:47 step:37397 [D loss: 0.014887, acc.: 99.22%] [G loss: 0.433989]\n",
      "epoch:47 step:37398 [D loss: 0.014748, acc.: 99.22%] [G loss: 0.696657]\n",
      "epoch:47 step:37399 [D loss: 0.137024, acc.: 94.53%] [G loss: 0.059066]\n",
      "epoch:47 step:37400 [D loss: 0.007365, acc.: 100.00%] [G loss: 0.006668]\n",
      "epoch:47 step:37401 [D loss: 0.000425, acc.: 100.00%] [G loss: 0.022825]\n",
      "epoch:47 step:37402 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.023019]\n",
      "epoch:47 step:37403 [D loss: 0.000623, acc.: 100.00%] [G loss: 0.002191]\n",
      "epoch:47 step:37404 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.006191]\n",
      "epoch:47 step:37405 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.002102]\n",
      "epoch:47 step:37406 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.268443]\n",
      "epoch:47 step:37407 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.001545]\n",
      "epoch:47 step:37408 [D loss: 0.000772, acc.: 100.00%] [G loss: 0.000310]\n",
      "epoch:47 step:37409 [D loss: 0.005279, acc.: 100.00%] [G loss: 0.001586]\n",
      "epoch:47 step:37410 [D loss: 0.001870, acc.: 100.00%] [G loss: 0.042223]\n",
      "epoch:47 step:37411 [D loss: 0.178777, acc.: 93.75%] [G loss: 0.051259]\n",
      "epoch:47 step:37412 [D loss: 0.012122, acc.: 99.22%] [G loss: 0.094341]\n",
      "epoch:47 step:37413 [D loss: 0.004125, acc.: 100.00%] [G loss: 1.380188]\n",
      "epoch:47 step:37414 [D loss: 0.141763, acc.: 97.66%] [G loss: 0.001608]\n",
      "epoch:47 step:37415 [D loss: 0.013768, acc.: 99.22%] [G loss: 1.205214]\n",
      "epoch:47 step:37416 [D loss: 0.004807, acc.: 100.00%] [G loss: 0.008325]\n",
      "epoch:47 step:37417 [D loss: 0.015296, acc.: 100.00%] [G loss: 0.003574]\n",
      "epoch:47 step:37418 [D loss: 0.101206, acc.: 96.88%] [G loss: 0.936704]\n",
      "epoch:47 step:37419 [D loss: 0.001626, acc.: 100.00%] [G loss: 2.670794]\n",
      "epoch:47 step:37420 [D loss: 2.773032, acc.: 21.88%] [G loss: 6.610412]\n",
      "epoch:47 step:37421 [D loss: 0.329813, acc.: 85.16%] [G loss: 2.198350]\n",
      "epoch:47 step:37422 [D loss: 0.676569, acc.: 68.75%] [G loss: 0.629616]\n",
      "epoch:47 step:37423 [D loss: 0.032425, acc.: 98.44%] [G loss: 0.053493]\n",
      "epoch:47 step:37424 [D loss: 0.084856, acc.: 96.09%] [G loss: 4.344520]\n",
      "epoch:47 step:37425 [D loss: 0.027854, acc.: 100.00%] [G loss: 0.001091]\n",
      "epoch:47 step:37426 [D loss: 0.028423, acc.: 100.00%] [G loss: 3.249128]\n",
      "epoch:47 step:37427 [D loss: 0.058793, acc.: 97.66%] [G loss: 3.119462]\n",
      "epoch:47 step:37428 [D loss: 0.051528, acc.: 99.22%] [G loss: 0.247365]\n",
      "epoch:47 step:37429 [D loss: 0.064300, acc.: 99.22%] [G loss: 0.007828]\n",
      "epoch:47 step:37430 [D loss: 0.008839, acc.: 100.00%] [G loss: 0.004900]\n",
      "epoch:47 step:37431 [D loss: 0.171182, acc.: 94.53%] [G loss: 0.000178]\n",
      "epoch:47 step:37432 [D loss: 0.040590, acc.: 98.44%] [G loss: 0.000038]\n",
      "epoch:47 step:37433 [D loss: 0.009370, acc.: 100.00%] [G loss: 0.000072]\n",
      "epoch:47 step:37434 [D loss: 0.016856, acc.: 100.00%] [G loss: 0.000465]\n",
      "epoch:47 step:37435 [D loss: 0.013451, acc.: 100.00%] [G loss: 2.922570]\n",
      "epoch:47 step:37436 [D loss: 0.008898, acc.: 100.00%] [G loss: 0.000067]\n",
      "epoch:47 step:37437 [D loss: 0.005867, acc.: 100.00%] [G loss: 0.000114]\n",
      "epoch:47 step:37438 [D loss: 0.005708, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:47 step:37439 [D loss: 0.015821, acc.: 99.22%] [G loss: 0.000763]\n",
      "epoch:47 step:37440 [D loss: 0.008077, acc.: 100.00%] [G loss: 0.000545]\n",
      "epoch:47 step:37441 [D loss: 0.009007, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:47 step:37442 [D loss: 0.003892, acc.: 100.00%] [G loss: 0.000311]\n",
      "epoch:47 step:37443 [D loss: 0.005298, acc.: 100.00%] [G loss: 2.158842]\n",
      "epoch:47 step:37444 [D loss: 0.002754, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:47 step:37445 [D loss: 0.006221, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:47 step:37446 [D loss: 0.003498, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:47 step:37447 [D loss: 0.004823, acc.: 100.00%] [G loss: 0.000095]\n",
      "epoch:47 step:37448 [D loss: 0.009759, acc.: 100.00%] [G loss: 0.942037]\n",
      "epoch:47 step:37449 [D loss: 0.001366, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:47 step:37450 [D loss: 0.009473, acc.: 100.00%] [G loss: 1.677465]\n",
      "epoch:47 step:37451 [D loss: 0.002524, acc.: 100.00%] [G loss: 0.635566]\n",
      "epoch:47 step:37452 [D loss: 0.037129, acc.: 98.44%] [G loss: 0.000335]\n",
      "epoch:47 step:37453 [D loss: 0.015209, acc.: 100.00%] [G loss: 0.003895]\n",
      "epoch:47 step:37454 [D loss: 0.042204, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:47 step:37455 [D loss: 0.006762, acc.: 100.00%] [G loss: 0.426810]\n",
      "epoch:47 step:37456 [D loss: 0.014808, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:47 step:37457 [D loss: 0.002998, acc.: 100.00%] [G loss: 0.000129]\n",
      "epoch:47 step:37458 [D loss: 0.005993, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:47 step:37459 [D loss: 0.004919, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:47 step:37460 [D loss: 0.003251, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:47 step:37461 [D loss: 0.005923, acc.: 100.00%] [G loss: 0.550706]\n",
      "epoch:47 step:37462 [D loss: 0.002376, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:47 step:37463 [D loss: 0.000997, acc.: 100.00%] [G loss: 0.000063]\n",
      "epoch:47 step:37464 [D loss: 0.001318, acc.: 100.00%] [G loss: 0.000025]\n",
      "epoch:47 step:37465 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:47 step:37466 [D loss: 0.001454, acc.: 100.00%] [G loss: 0.284934]\n",
      "epoch:47 step:37467 [D loss: 0.003331, acc.: 100.00%] [G loss: 0.000619]\n",
      "epoch:47 step:37468 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:47 step:37469 [D loss: 0.003705, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:47 step:37470 [D loss: 0.002358, acc.: 100.00%] [G loss: 0.199854]\n",
      "epoch:47 step:37471 [D loss: 0.000727, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:47 step:37472 [D loss: 0.014253, acc.: 98.44%] [G loss: 0.000054]\n",
      "epoch:47 step:37473 [D loss: 0.014817, acc.: 100.00%] [G loss: 0.074674]\n",
      "epoch:47 step:37474 [D loss: 0.024823, acc.: 99.22%] [G loss: 0.146957]\n",
      "epoch:47 step:37475 [D loss: 0.000761, acc.: 100.00%] [G loss: 0.297411]\n",
      "epoch:47 step:37476 [D loss: 0.002270, acc.: 100.00%] [G loss: 0.001750]\n",
      "epoch:47 step:37477 [D loss: 0.004025, acc.: 100.00%] [G loss: 0.073143]\n",
      "epoch:47 step:37478 [D loss: 0.016546, acc.: 99.22%] [G loss: 0.002639]\n",
      "epoch:47 step:37479 [D loss: 0.014132, acc.: 99.22%] [G loss: 0.001539]\n",
      "epoch:47 step:37480 [D loss: 0.005739, acc.: 100.00%] [G loss: 0.001773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:47 step:37481 [D loss: 0.000772, acc.: 100.00%] [G loss: 0.004749]\n",
      "epoch:47 step:37482 [D loss: 0.001195, acc.: 100.00%] [G loss: 0.130722]\n",
      "epoch:47 step:37483 [D loss: 0.009720, acc.: 99.22%] [G loss: 0.000102]\n",
      "epoch:47 step:37484 [D loss: 0.004093, acc.: 100.00%] [G loss: 0.000718]\n",
      "epoch:47 step:37485 [D loss: 0.001311, acc.: 100.00%] [G loss: 0.001014]\n",
      "epoch:47 step:37486 [D loss: 0.003496, acc.: 100.00%] [G loss: 0.023068]\n",
      "epoch:47 step:37487 [D loss: 0.001465, acc.: 100.00%] [G loss: 0.000320]\n",
      "epoch:47 step:37488 [D loss: 0.009264, acc.: 99.22%] [G loss: 0.000033]\n",
      "epoch:48 step:37489 [D loss: 0.002820, acc.: 100.00%] [G loss: 0.011237]\n",
      "epoch:48 step:37490 [D loss: 0.001701, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:48 step:37491 [D loss: 0.001098, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:48 step:37492 [D loss: 0.001133, acc.: 100.00%] [G loss: 0.023801]\n",
      "epoch:48 step:37493 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.008500]\n",
      "epoch:48 step:37494 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.000879]\n",
      "epoch:48 step:37495 [D loss: 0.000942, acc.: 100.00%] [G loss: 0.037620]\n",
      "epoch:48 step:37496 [D loss: 0.005097, acc.: 100.00%] [G loss: 0.024520]\n",
      "epoch:48 step:37497 [D loss: 0.006652, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:48 step:37498 [D loss: 0.000526, acc.: 100.00%] [G loss: 0.001566]\n",
      "epoch:48 step:37499 [D loss: 0.002148, acc.: 100.00%] [G loss: 0.000278]\n",
      "epoch:48 step:37500 [D loss: 0.000977, acc.: 100.00%] [G loss: 0.000481]\n",
      "epoch:48 step:37501 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000287]\n",
      "epoch:48 step:37502 [D loss: 0.000434, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:48 step:37503 [D loss: 0.000397, acc.: 100.00%] [G loss: 0.000679]\n",
      "epoch:48 step:37504 [D loss: 0.006973, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:48 step:37505 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000309]\n",
      "epoch:48 step:37506 [D loss: 0.000365, acc.: 100.00%] [G loss: 0.012514]\n",
      "epoch:48 step:37507 [D loss: 0.001439, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:48 step:37508 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.172801]\n",
      "epoch:48 step:37509 [D loss: 0.002085, acc.: 100.00%] [G loss: 0.000244]\n",
      "epoch:48 step:37510 [D loss: 0.001174, acc.: 100.00%] [G loss: 0.000265]\n",
      "epoch:48 step:37511 [D loss: 0.006517, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:48 step:37512 [D loss: 0.002112, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:48 step:37513 [D loss: 0.004929, acc.: 100.00%] [G loss: 0.002714]\n",
      "epoch:48 step:37514 [D loss: 0.004530, acc.: 100.00%] [G loss: 0.001366]\n",
      "epoch:48 step:37515 [D loss: 0.001287, acc.: 100.00%] [G loss: 0.000924]\n",
      "epoch:48 step:37516 [D loss: 0.001017, acc.: 100.00%] [G loss: 0.670799]\n",
      "epoch:48 step:37517 [D loss: 0.036423, acc.: 100.00%] [G loss: 0.430041]\n",
      "epoch:48 step:37518 [D loss: 0.002059, acc.: 100.00%] [G loss: 0.035382]\n",
      "epoch:48 step:37519 [D loss: 0.023316, acc.: 100.00%] [G loss: 0.010902]\n",
      "epoch:48 step:37520 [D loss: 0.009783, acc.: 100.00%] [G loss: 0.002287]\n",
      "epoch:48 step:37521 [D loss: 0.006924, acc.: 100.00%] [G loss: 0.001376]\n",
      "epoch:48 step:37522 [D loss: 0.007284, acc.: 100.00%] [G loss: 0.001094]\n",
      "epoch:48 step:37523 [D loss: 0.004205, acc.: 100.00%] [G loss: 0.001497]\n",
      "epoch:48 step:37524 [D loss: 0.031799, acc.: 100.00%] [G loss: 0.232465]\n",
      "epoch:48 step:37525 [D loss: 0.005676, acc.: 100.00%] [G loss: 0.805356]\n",
      "epoch:48 step:37526 [D loss: 0.012740, acc.: 100.00%] [G loss: 0.051862]\n",
      "epoch:48 step:37527 [D loss: 0.001198, acc.: 100.00%] [G loss: 0.018629]\n",
      "epoch:48 step:37528 [D loss: 0.003354, acc.: 100.00%] [G loss: 0.017622]\n",
      "epoch:48 step:37529 [D loss: 0.007431, acc.: 100.00%] [G loss: 0.002607]\n",
      "epoch:48 step:37530 [D loss: 0.003215, acc.: 100.00%] [G loss: 0.002703]\n",
      "epoch:48 step:37531 [D loss: 0.002132, acc.: 100.00%] [G loss: 0.005404]\n",
      "epoch:48 step:37532 [D loss: 0.008068, acc.: 100.00%] [G loss: 0.008507]\n",
      "epoch:48 step:37533 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.011427]\n",
      "epoch:48 step:37534 [D loss: 0.000374, acc.: 100.00%] [G loss: 0.000178]\n",
      "epoch:48 step:37535 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.000379]\n",
      "epoch:48 step:37536 [D loss: 0.000375, acc.: 100.00%] [G loss: 0.000831]\n",
      "epoch:48 step:37537 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.004409]\n",
      "epoch:48 step:37538 [D loss: 0.000986, acc.: 100.00%] [G loss: 0.000813]\n",
      "epoch:48 step:37539 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.044886]\n",
      "epoch:48 step:37540 [D loss: 0.001287, acc.: 100.00%] [G loss: 0.004459]\n",
      "epoch:48 step:37541 [D loss: 0.006987, acc.: 100.00%] [G loss: 0.000406]\n",
      "epoch:48 step:37542 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.000563]\n",
      "epoch:48 step:37543 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.001736]\n",
      "epoch:48 step:37544 [D loss: 0.001494, acc.: 100.00%] [G loss: 0.056892]\n",
      "epoch:48 step:37545 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.010935]\n",
      "epoch:48 step:37546 [D loss: 0.001704, acc.: 100.00%] [G loss: 0.017109]\n",
      "epoch:48 step:37547 [D loss: 0.005151, acc.: 100.00%] [G loss: 0.001556]\n",
      "epoch:48 step:37548 [D loss: 0.002806, acc.: 100.00%] [G loss: 0.002897]\n",
      "epoch:48 step:37549 [D loss: 0.002523, acc.: 100.00%] [G loss: 0.008178]\n",
      "epoch:48 step:37550 [D loss: 0.000216, acc.: 100.00%] [G loss: 0.000486]\n",
      "epoch:48 step:37551 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.000772]\n",
      "epoch:48 step:37552 [D loss: 0.003686, acc.: 100.00%] [G loss: 0.025887]\n",
      "epoch:48 step:37553 [D loss: 0.005487, acc.: 100.00%] [G loss: 0.011730]\n",
      "epoch:48 step:37554 [D loss: 0.001159, acc.: 100.00%] [G loss: 0.001037]\n",
      "epoch:48 step:37555 [D loss: 0.000720, acc.: 100.00%] [G loss: 0.000106]\n",
      "epoch:48 step:37556 [D loss: 0.001638, acc.: 100.00%] [G loss: 0.000445]\n",
      "epoch:48 step:37557 [D loss: 0.000394, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:48 step:37558 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.017008]\n",
      "epoch:48 step:37559 [D loss: 0.000798, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:48 step:37560 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.001014]\n",
      "epoch:48 step:37561 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.092914]\n",
      "epoch:48 step:37562 [D loss: 0.000225, acc.: 100.00%] [G loss: 0.058151]\n",
      "epoch:48 step:37563 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:48 step:37564 [D loss: 0.006019, acc.: 99.22%] [G loss: 0.000400]\n",
      "epoch:48 step:37565 [D loss: 0.001487, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:48 step:37566 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.001665]\n",
      "epoch:48 step:37567 [D loss: 0.000494, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:48 step:37568 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.000637]\n",
      "epoch:48 step:37569 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:48 step:37570 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:48 step:37571 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:48 step:37572 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.001131]\n",
      "epoch:48 step:37573 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:48 step:37574 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.000786]\n",
      "epoch:48 step:37575 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:48 step:37576 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.001511]\n",
      "epoch:48 step:37577 [D loss: 0.002701, acc.: 100.00%] [G loss: 0.000298]\n",
      "epoch:48 step:37578 [D loss: 0.003270, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:48 step:37579 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.000639]\n",
      "epoch:48 step:37580 [D loss: 0.000726, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:48 step:37581 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000394]\n",
      "epoch:48 step:37582 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000205]\n",
      "epoch:48 step:37583 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.001960]\n",
      "epoch:48 step:37584 [D loss: 0.000357, acc.: 100.00%] [G loss: 0.001024]\n",
      "epoch:48 step:37585 [D loss: 0.000646, acc.: 100.00%] [G loss: 0.002249]\n",
      "epoch:48 step:37586 [D loss: 0.003228, acc.: 100.00%] [G loss: 0.185650]\n",
      "epoch:48 step:37587 [D loss: 0.011982, acc.: 100.00%] [G loss: 0.069306]\n",
      "epoch:48 step:37588 [D loss: 0.006432, acc.: 100.00%] [G loss: 0.005275]\n",
      "epoch:48 step:37589 [D loss: 0.011807, acc.: 100.00%] [G loss: 0.005564]\n",
      "epoch:48 step:37590 [D loss: 0.001302, acc.: 100.00%] [G loss: 0.013416]\n",
      "epoch:48 step:37591 [D loss: 0.005083, acc.: 100.00%] [G loss: 0.011738]\n",
      "epoch:48 step:37592 [D loss: 0.000612, acc.: 100.00%] [G loss: 0.295081]\n",
      "epoch:48 step:37593 [D loss: 0.001929, acc.: 100.00%] [G loss: 0.002778]\n",
      "epoch:48 step:37594 [D loss: 0.000794, acc.: 100.00%] [G loss: 0.081870]\n",
      "epoch:48 step:37595 [D loss: 0.002038, acc.: 100.00%] [G loss: 0.082885]\n",
      "epoch:48 step:37596 [D loss: 0.004544, acc.: 100.00%] [G loss: 0.103303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37597 [D loss: 0.004547, acc.: 100.00%] [G loss: 0.003559]\n",
      "epoch:48 step:37598 [D loss: 0.003904, acc.: 100.00%] [G loss: 0.005356]\n",
      "epoch:48 step:37599 [D loss: 0.001731, acc.: 100.00%] [G loss: 0.001032]\n",
      "epoch:48 step:37600 [D loss: 0.000312, acc.: 100.00%] [G loss: 0.001251]\n",
      "epoch:48 step:37601 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.002472]\n",
      "epoch:48 step:37602 [D loss: 0.000799, acc.: 100.00%] [G loss: 0.001159]\n",
      "epoch:48 step:37603 [D loss: 0.003113, acc.: 100.00%] [G loss: 0.026804]\n",
      "epoch:48 step:37604 [D loss: 0.007765, acc.: 99.22%] [G loss: 0.000512]\n",
      "epoch:48 step:37605 [D loss: 0.001101, acc.: 100.00%] [G loss: 0.000316]\n",
      "epoch:48 step:37606 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.028191]\n",
      "epoch:48 step:37607 [D loss: 0.001119, acc.: 100.00%] [G loss: 0.000494]\n",
      "epoch:48 step:37608 [D loss: 0.000783, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:48 step:37609 [D loss: 0.035763, acc.: 98.44%] [G loss: 0.049965]\n",
      "epoch:48 step:37610 [D loss: 0.005255, acc.: 100.00%] [G loss: 0.250654]\n",
      "epoch:48 step:37611 [D loss: 0.037850, acc.: 98.44%] [G loss: 0.004523]\n",
      "epoch:48 step:37612 [D loss: 0.062092, acc.: 98.44%] [G loss: 0.000010]\n",
      "epoch:48 step:37613 [D loss: 0.018990, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:48 step:37614 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.001134]\n",
      "epoch:48 step:37615 [D loss: 0.107278, acc.: 96.88%] [G loss: 5.608777]\n",
      "epoch:48 step:37616 [D loss: 0.004532, acc.: 100.00%] [G loss: 3.139828]\n",
      "epoch:48 step:37617 [D loss: 1.126017, acc.: 60.16%] [G loss: 7.169515]\n",
      "epoch:48 step:37618 [D loss: 1.648088, acc.: 53.12%] [G loss: 2.161038]\n",
      "epoch:48 step:37619 [D loss: 0.638305, acc.: 78.12%] [G loss: 4.334161]\n",
      "epoch:48 step:37620 [D loss: 0.085156, acc.: 98.44%] [G loss: 4.603388]\n",
      "epoch:48 step:37621 [D loss: 0.039178, acc.: 99.22%] [G loss: 4.105716]\n",
      "epoch:48 step:37622 [D loss: 0.109870, acc.: 95.31%] [G loss: 2.798240]\n",
      "epoch:48 step:37623 [D loss: 0.162022, acc.: 93.75%] [G loss: 3.317947]\n",
      "epoch:48 step:37624 [D loss: 0.046496, acc.: 98.44%] [G loss: 6.045667]\n",
      "epoch:48 step:37625 [D loss: 0.086888, acc.: 98.44%] [G loss: 4.271469]\n",
      "epoch:48 step:37626 [D loss: 0.106123, acc.: 99.22%] [G loss: 3.975687]\n",
      "epoch:48 step:37627 [D loss: 0.142653, acc.: 94.53%] [G loss: 5.670365]\n",
      "epoch:48 step:37628 [D loss: 0.498067, acc.: 75.00%] [G loss: 4.686628]\n",
      "epoch:48 step:37629 [D loss: 0.355394, acc.: 84.38%] [G loss: 7.382829]\n",
      "epoch:48 step:37630 [D loss: 0.028126, acc.: 99.22%] [G loss: 6.522235]\n",
      "epoch:48 step:37631 [D loss: 0.179806, acc.: 92.19%] [G loss: 7.471069]\n",
      "epoch:48 step:37632 [D loss: 0.026675, acc.: 99.22%] [G loss: 6.899098]\n",
      "epoch:48 step:37633 [D loss: 0.010825, acc.: 100.00%] [G loss: 5.839511]\n",
      "epoch:48 step:37634 [D loss: 0.005532, acc.: 100.00%] [G loss: 2.660717]\n",
      "epoch:48 step:37635 [D loss: 0.015656, acc.: 100.00%] [G loss: 1.930833]\n",
      "epoch:48 step:37636 [D loss: 0.000367, acc.: 100.00%] [G loss: 5.403369]\n",
      "epoch:48 step:37637 [D loss: 0.002045, acc.: 100.00%] [G loss: 5.331278]\n",
      "epoch:48 step:37638 [D loss: 0.004755, acc.: 100.00%] [G loss: 3.839038]\n",
      "epoch:48 step:37639 [D loss: 0.013405, acc.: 100.00%] [G loss: 1.004298]\n",
      "epoch:48 step:37640 [D loss: 0.003641, acc.: 100.00%] [G loss: 2.696950]\n",
      "epoch:48 step:37641 [D loss: 0.029685, acc.: 99.22%] [G loss: 1.448225]\n",
      "epoch:48 step:37642 [D loss: 0.001440, acc.: 100.00%] [G loss: 0.870229]\n",
      "epoch:48 step:37643 [D loss: 0.007731, acc.: 100.00%] [G loss: 0.370454]\n",
      "epoch:48 step:37644 [D loss: 0.002813, acc.: 100.00%] [G loss: 0.123680]\n",
      "epoch:48 step:37645 [D loss: 0.013306, acc.: 100.00%] [G loss: 0.107396]\n",
      "epoch:48 step:37646 [D loss: 0.001687, acc.: 100.00%] [G loss: 0.401901]\n",
      "epoch:48 step:37647 [D loss: 0.000618, acc.: 100.00%] [G loss: 0.071655]\n",
      "epoch:48 step:37648 [D loss: 0.002861, acc.: 100.00%] [G loss: 0.032406]\n",
      "epoch:48 step:37649 [D loss: 0.006224, acc.: 100.00%] [G loss: 0.010411]\n",
      "epoch:48 step:37650 [D loss: 0.016525, acc.: 100.00%] [G loss: 0.098690]\n",
      "epoch:48 step:37651 [D loss: 0.001611, acc.: 100.00%] [G loss: 0.193558]\n",
      "epoch:48 step:37652 [D loss: 0.000932, acc.: 100.00%] [G loss: 0.015431]\n",
      "epoch:48 step:37653 [D loss: 0.000830, acc.: 100.00%] [G loss: 0.010206]\n",
      "epoch:48 step:37654 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.014841]\n",
      "epoch:48 step:37655 [D loss: 0.001676, acc.: 100.00%] [G loss: 0.036931]\n",
      "epoch:48 step:37656 [D loss: 0.001517, acc.: 100.00%] [G loss: 0.024530]\n",
      "epoch:48 step:37657 [D loss: 0.000404, acc.: 100.00%] [G loss: 0.012035]\n",
      "epoch:48 step:37658 [D loss: 0.008255, acc.: 100.00%] [G loss: 0.039200]\n",
      "epoch:48 step:37659 [D loss: 0.074389, acc.: 98.44%] [G loss: 0.009938]\n",
      "epoch:48 step:37660 [D loss: 0.001025, acc.: 100.00%] [G loss: 1.125256]\n",
      "epoch:48 step:37661 [D loss: 0.007730, acc.: 100.00%] [G loss: 0.074416]\n",
      "epoch:48 step:37662 [D loss: 0.001619, acc.: 100.00%] [G loss: 0.035273]\n",
      "epoch:48 step:37663 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.139843]\n",
      "epoch:48 step:37664 [D loss: 0.028588, acc.: 99.22%] [G loss: 0.005011]\n",
      "epoch:48 step:37665 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.009872]\n",
      "epoch:48 step:37666 [D loss: 0.006131, acc.: 100.00%] [G loss: 0.004278]\n",
      "epoch:48 step:37667 [D loss: 0.024722, acc.: 99.22%] [G loss: 0.030371]\n",
      "epoch:48 step:37668 [D loss: 0.000836, acc.: 100.00%] [G loss: 0.004345]\n",
      "epoch:48 step:37669 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.045644]\n",
      "epoch:48 step:37670 [D loss: 0.001422, acc.: 100.00%] [G loss: 0.017192]\n",
      "epoch:48 step:37671 [D loss: 0.002354, acc.: 100.00%] [G loss: 0.017185]\n",
      "epoch:48 step:37672 [D loss: 0.049815, acc.: 99.22%] [G loss: 0.003326]\n",
      "epoch:48 step:37673 [D loss: 0.000542, acc.: 100.00%] [G loss: 0.023827]\n",
      "epoch:48 step:37674 [D loss: 0.014309, acc.: 99.22%] [G loss: 0.136409]\n",
      "epoch:48 step:37675 [D loss: 0.010572, acc.: 100.00%] [G loss: 0.240215]\n",
      "epoch:48 step:37676 [D loss: 0.002262, acc.: 100.00%] [G loss: 0.213519]\n",
      "epoch:48 step:37677 [D loss: 0.023154, acc.: 100.00%] [G loss: 0.005223]\n",
      "epoch:48 step:37678 [D loss: 0.003787, acc.: 100.00%] [G loss: 0.016850]\n",
      "epoch:48 step:37679 [D loss: 0.004810, acc.: 100.00%] [G loss: 0.002394]\n",
      "epoch:48 step:37680 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.008894]\n",
      "epoch:48 step:37681 [D loss: 0.000212, acc.: 100.00%] [G loss: 0.005515]\n",
      "epoch:48 step:37682 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.015980]\n",
      "epoch:48 step:37683 [D loss: 0.001972, acc.: 100.00%] [G loss: 0.063613]\n",
      "epoch:48 step:37684 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.037149]\n",
      "epoch:48 step:37685 [D loss: 0.003795, acc.: 100.00%] [G loss: 0.001134]\n",
      "epoch:48 step:37686 [D loss: 0.001473, acc.: 100.00%] [G loss: 0.004969]\n",
      "epoch:48 step:37687 [D loss: 0.002382, acc.: 100.00%] [G loss: 0.102449]\n",
      "epoch:48 step:37688 [D loss: 0.006711, acc.: 100.00%] [G loss: 0.001387]\n",
      "epoch:48 step:37689 [D loss: 0.004126, acc.: 100.00%] [G loss: 0.054590]\n",
      "epoch:48 step:37690 [D loss: 0.002886, acc.: 100.00%] [G loss: 0.007094]\n",
      "epoch:48 step:37691 [D loss: 0.001096, acc.: 100.00%] [G loss: 0.002758]\n",
      "epoch:48 step:37692 [D loss: 0.001513, acc.: 100.00%] [G loss: 0.053035]\n",
      "epoch:48 step:37693 [D loss: 0.002952, acc.: 100.00%] [G loss: 0.033106]\n",
      "epoch:48 step:37694 [D loss: 0.002975, acc.: 100.00%] [G loss: 0.183373]\n",
      "epoch:48 step:37695 [D loss: 0.005314, acc.: 100.00%] [G loss: 0.001225]\n",
      "epoch:48 step:37696 [D loss: 0.002899, acc.: 100.00%] [G loss: 0.055622]\n",
      "epoch:48 step:37697 [D loss: 0.005052, acc.: 100.00%] [G loss: 0.031541]\n",
      "epoch:48 step:37698 [D loss: 0.002162, acc.: 100.00%] [G loss: 0.004516]\n",
      "epoch:48 step:37699 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.005178]\n",
      "epoch:48 step:37700 [D loss: 0.000820, acc.: 100.00%] [G loss: 0.006711]\n",
      "epoch:48 step:37701 [D loss: 0.000205, acc.: 100.00%] [G loss: 0.001465]\n",
      "epoch:48 step:37702 [D loss: 0.003970, acc.: 100.00%] [G loss: 0.020157]\n",
      "epoch:48 step:37703 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.003291]\n",
      "epoch:48 step:37704 [D loss: 0.004036, acc.: 100.00%] [G loss: 0.010404]\n",
      "epoch:48 step:37705 [D loss: 0.011077, acc.: 100.00%] [G loss: 0.007285]\n",
      "epoch:48 step:37706 [D loss: 0.017363, acc.: 99.22%] [G loss: 0.001465]\n",
      "epoch:48 step:37707 [D loss: 0.003380, acc.: 100.00%] [G loss: 0.073139]\n",
      "epoch:48 step:37708 [D loss: 0.015263, acc.: 99.22%] [G loss: 0.013573]\n",
      "epoch:48 step:37709 [D loss: 0.000979, acc.: 100.00%] [G loss: 0.013856]\n",
      "epoch:48 step:37710 [D loss: 0.000628, acc.: 100.00%] [G loss: 0.028438]\n",
      "epoch:48 step:37711 [D loss: 0.012826, acc.: 99.22%] [G loss: 0.002189]\n",
      "epoch:48 step:37712 [D loss: 0.001222, acc.: 100.00%] [G loss: 0.000874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37713 [D loss: 0.000992, acc.: 100.00%] [G loss: 0.010674]\n",
      "epoch:48 step:37714 [D loss: 0.000619, acc.: 100.00%] [G loss: 0.000853]\n",
      "epoch:48 step:37715 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.003619]\n",
      "epoch:48 step:37716 [D loss: 0.001041, acc.: 100.00%] [G loss: 0.004832]\n",
      "epoch:48 step:37717 [D loss: 0.001139, acc.: 100.00%] [G loss: 0.006782]\n",
      "epoch:48 step:37718 [D loss: 0.001531, acc.: 100.00%] [G loss: 0.093960]\n",
      "epoch:48 step:37719 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.072360]\n",
      "epoch:48 step:37720 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.006553]\n",
      "epoch:48 step:37721 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.048049]\n",
      "epoch:48 step:37722 [D loss: 0.007988, acc.: 99.22%] [G loss: 0.005593]\n",
      "epoch:48 step:37723 [D loss: 0.000909, acc.: 100.00%] [G loss: 0.000532]\n",
      "epoch:48 step:37724 [D loss: 0.005912, acc.: 100.00%] [G loss: 0.001665]\n",
      "epoch:48 step:37725 [D loss: 0.001530, acc.: 100.00%] [G loss: 0.000683]\n",
      "epoch:48 step:37726 [D loss: 0.000336, acc.: 100.00%] [G loss: 0.000743]\n",
      "epoch:48 step:37727 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.001877]\n",
      "epoch:48 step:37728 [D loss: 0.001932, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:48 step:37729 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.000777]\n",
      "epoch:48 step:37730 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:48 step:37731 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.001000]\n",
      "epoch:48 step:37732 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:48 step:37733 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.000222]\n",
      "epoch:48 step:37734 [D loss: 0.001417, acc.: 100.00%] [G loss: 0.001009]\n",
      "epoch:48 step:37735 [D loss: 0.001876, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:48 step:37736 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.000568]\n",
      "epoch:48 step:37737 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.016641]\n",
      "epoch:48 step:37738 [D loss: 0.006626, acc.: 100.00%] [G loss: 0.000541]\n",
      "epoch:48 step:37739 [D loss: 0.002222, acc.: 100.00%] [G loss: 0.000374]\n",
      "epoch:48 step:37740 [D loss: 0.000590, acc.: 100.00%] [G loss: 0.000647]\n",
      "epoch:48 step:37741 [D loss: 0.000254, acc.: 100.00%] [G loss: 0.000675]\n",
      "epoch:48 step:37742 [D loss: 0.002683, acc.: 100.00%] [G loss: 0.000461]\n",
      "epoch:48 step:37743 [D loss: 0.000523, acc.: 100.00%] [G loss: 0.006267]\n",
      "epoch:48 step:37744 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.022729]\n",
      "epoch:48 step:37745 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.005570]\n",
      "epoch:48 step:37746 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.003520]\n",
      "epoch:48 step:37747 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:48 step:37748 [D loss: 0.000366, acc.: 100.00%] [G loss: 0.000462]\n",
      "epoch:48 step:37749 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.001798]\n",
      "epoch:48 step:37750 [D loss: 0.000917, acc.: 100.00%] [G loss: 0.016843]\n",
      "epoch:48 step:37751 [D loss: 0.001370, acc.: 100.00%] [G loss: 0.008828]\n",
      "epoch:48 step:37752 [D loss: 0.001913, acc.: 100.00%] [G loss: 0.002318]\n",
      "epoch:48 step:37753 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.002304]\n",
      "epoch:48 step:37754 [D loss: 0.000317, acc.: 100.00%] [G loss: 0.000838]\n",
      "epoch:48 step:37755 [D loss: 0.000808, acc.: 100.00%] [G loss: 0.004738]\n",
      "epoch:48 step:37756 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:48 step:37757 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.000947]\n",
      "epoch:48 step:37758 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.001918]\n",
      "epoch:48 step:37759 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.000444]\n",
      "epoch:48 step:37760 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.001345]\n",
      "epoch:48 step:37761 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000565]\n",
      "epoch:48 step:37762 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.000951]\n",
      "epoch:48 step:37763 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.006916]\n",
      "epoch:48 step:37764 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.000322]\n",
      "epoch:48 step:37765 [D loss: 0.003744, acc.: 100.00%] [G loss: 0.000294]\n",
      "epoch:48 step:37766 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000884]\n",
      "epoch:48 step:37767 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000627]\n",
      "epoch:48 step:37768 [D loss: 0.000493, acc.: 100.00%] [G loss: 0.000172]\n",
      "epoch:48 step:37769 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000460]\n",
      "epoch:48 step:37770 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:48 step:37771 [D loss: 0.002932, acc.: 100.00%] [G loss: 0.000554]\n",
      "epoch:48 step:37772 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.030986]\n",
      "epoch:48 step:37773 [D loss: 0.000149, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:48 step:37774 [D loss: 0.000329, acc.: 100.00%] [G loss: 0.000553]\n",
      "epoch:48 step:37775 [D loss: 0.002742, acc.: 100.00%] [G loss: 0.000803]\n",
      "epoch:48 step:37776 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.190594]\n",
      "epoch:48 step:37777 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.003061]\n",
      "epoch:48 step:37778 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.000556]\n",
      "epoch:48 step:37779 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000638]\n",
      "epoch:48 step:37780 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.041468]\n",
      "epoch:48 step:37781 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.007458]\n",
      "epoch:48 step:37782 [D loss: 0.004220, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:48 step:37783 [D loss: 0.001159, acc.: 100.00%] [G loss: 0.000221]\n",
      "epoch:48 step:37784 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.001143]\n",
      "epoch:48 step:37785 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.000574]\n",
      "epoch:48 step:37786 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000769]\n",
      "epoch:48 step:37787 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.077149]\n",
      "epoch:48 step:37788 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.001895]\n",
      "epoch:48 step:37789 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.001240]\n",
      "epoch:48 step:37790 [D loss: 0.004357, acc.: 100.00%] [G loss: 0.000554]\n",
      "epoch:48 step:37791 [D loss: 0.002128, acc.: 100.00%] [G loss: 0.001361]\n",
      "epoch:48 step:37792 [D loss: 0.003545, acc.: 100.00%] [G loss: 0.003768]\n",
      "epoch:48 step:37793 [D loss: 0.001987, acc.: 100.00%] [G loss: 0.003157]\n",
      "epoch:48 step:37794 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.003204]\n",
      "epoch:48 step:37795 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.000843]\n",
      "epoch:48 step:37796 [D loss: 0.000372, acc.: 100.00%] [G loss: 0.017854]\n",
      "epoch:48 step:37797 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.004328]\n",
      "epoch:48 step:37798 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.003154]\n",
      "epoch:48 step:37799 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.002509]\n",
      "epoch:48 step:37800 [D loss: 0.000728, acc.: 100.00%] [G loss: 0.001364]\n",
      "epoch:48 step:37801 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.004896]\n",
      "epoch:48 step:37802 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.010469]\n",
      "epoch:48 step:37803 [D loss: 0.011072, acc.: 99.22%] [G loss: 0.000586]\n",
      "epoch:48 step:37804 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.013872]\n",
      "epoch:48 step:37805 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.000595]\n",
      "epoch:48 step:37806 [D loss: 0.002199, acc.: 100.00%] [G loss: 0.001028]\n",
      "epoch:48 step:37807 [D loss: 0.002550, acc.: 100.00%] [G loss: 0.010745]\n",
      "epoch:48 step:37808 [D loss: 0.020610, acc.: 100.00%] [G loss: 0.002098]\n",
      "epoch:48 step:37809 [D loss: 0.001136, acc.: 100.00%] [G loss: 0.001312]\n",
      "epoch:48 step:37810 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.216643]\n",
      "epoch:48 step:37811 [D loss: 0.000901, acc.: 100.00%] [G loss: 0.010883]\n",
      "epoch:48 step:37812 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.002133]\n",
      "epoch:48 step:37813 [D loss: 0.001499, acc.: 100.00%] [G loss: 0.004856]\n",
      "epoch:48 step:37814 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.012177]\n",
      "epoch:48 step:37815 [D loss: 0.001354, acc.: 100.00%] [G loss: 0.053469]\n",
      "epoch:48 step:37816 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.363362]\n",
      "epoch:48 step:37817 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.010325]\n",
      "epoch:48 step:37818 [D loss: 0.000638, acc.: 100.00%] [G loss: 0.002559]\n",
      "epoch:48 step:37819 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.001218]\n",
      "epoch:48 step:37820 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.004980]\n",
      "epoch:48 step:37821 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.008966]\n",
      "epoch:48 step:37822 [D loss: 0.001825, acc.: 100.00%] [G loss: 0.024733]\n",
      "epoch:48 step:37823 [D loss: 0.000793, acc.: 100.00%] [G loss: 0.015475]\n",
      "epoch:48 step:37824 [D loss: 0.002972, acc.: 100.00%] [G loss: 0.003803]\n",
      "epoch:48 step:37825 [D loss: 0.001187, acc.: 100.00%] [G loss: 0.063733]\n",
      "epoch:48 step:37826 [D loss: 0.007607, acc.: 100.00%] [G loss: 0.008441]\n",
      "epoch:48 step:37827 [D loss: 0.000541, acc.: 100.00%] [G loss: 0.001812]\n",
      "epoch:48 step:37828 [D loss: 0.001150, acc.: 100.00%] [G loss: 0.001805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37829 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.004803]\n",
      "epoch:48 step:37830 [D loss: 0.000260, acc.: 100.00%] [G loss: 0.009432]\n",
      "epoch:48 step:37831 [D loss: 0.001289, acc.: 100.00%] [G loss: 0.017124]\n",
      "epoch:48 step:37832 [D loss: 0.000609, acc.: 100.00%] [G loss: 0.001120]\n",
      "epoch:48 step:37833 [D loss: 0.000597, acc.: 100.00%] [G loss: 0.084497]\n",
      "epoch:48 step:37834 [D loss: 0.000908, acc.: 100.00%] [G loss: 0.002172]\n",
      "epoch:48 step:37835 [D loss: 0.004347, acc.: 100.00%] [G loss: 0.001570]\n",
      "epoch:48 step:37836 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.019306]\n",
      "epoch:48 step:37837 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.002992]\n",
      "epoch:48 step:37838 [D loss: 0.007042, acc.: 100.00%] [G loss: 0.000879]\n",
      "epoch:48 step:37839 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.011247]\n",
      "epoch:48 step:37840 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.000915]\n",
      "epoch:48 step:37841 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:48 step:37842 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.002550]\n",
      "epoch:48 step:37843 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.002624]\n",
      "epoch:48 step:37844 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.002573]\n",
      "epoch:48 step:37845 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000396]\n",
      "epoch:48 step:37846 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.001911]\n",
      "epoch:48 step:37847 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:48 step:37848 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000943]\n",
      "epoch:48 step:37849 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.001109]\n",
      "epoch:48 step:37850 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:48 step:37851 [D loss: 0.000168, acc.: 100.00%] [G loss: 0.000729]\n",
      "epoch:48 step:37852 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.001039]\n",
      "epoch:48 step:37853 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.000139]\n",
      "epoch:48 step:37854 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.001931]\n",
      "epoch:48 step:37855 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.000324]\n",
      "epoch:48 step:37856 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.009285]\n",
      "epoch:48 step:37857 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.003498]\n",
      "epoch:48 step:37858 [D loss: 0.001124, acc.: 100.00%] [G loss: 0.001188]\n",
      "epoch:48 step:37859 [D loss: 0.000283, acc.: 100.00%] [G loss: 0.002492]\n",
      "epoch:48 step:37860 [D loss: 0.007191, acc.: 100.00%] [G loss: 0.000179]\n",
      "epoch:48 step:37861 [D loss: 0.000512, acc.: 100.00%] [G loss: 0.000721]\n",
      "epoch:48 step:37862 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.011228]\n",
      "epoch:48 step:37863 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.003745]\n",
      "epoch:48 step:37864 [D loss: 0.000884, acc.: 100.00%] [G loss: 0.002334]\n",
      "epoch:48 step:37865 [D loss: 0.001092, acc.: 100.00%] [G loss: 0.000501]\n",
      "epoch:48 step:37866 [D loss: 0.000277, acc.: 100.00%] [G loss: 0.001468]\n",
      "epoch:48 step:37867 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.002027]\n",
      "epoch:48 step:37868 [D loss: 0.000981, acc.: 100.00%] [G loss: 0.000620]\n",
      "epoch:48 step:37869 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.000930]\n",
      "epoch:48 step:37870 [D loss: 0.002821, acc.: 100.00%] [G loss: 0.000584]\n",
      "epoch:48 step:37871 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.159073]\n",
      "epoch:48 step:37872 [D loss: 0.003102, acc.: 100.00%] [G loss: 0.011949]\n",
      "epoch:48 step:37873 [D loss: 0.005705, acc.: 100.00%] [G loss: 0.013143]\n",
      "epoch:48 step:37874 [D loss: 0.001527, acc.: 100.00%] [G loss: 0.005802]\n",
      "epoch:48 step:37875 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.002666]\n",
      "epoch:48 step:37876 [D loss: 0.061953, acc.: 96.88%] [G loss: 0.000550]\n",
      "epoch:48 step:37877 [D loss: 0.055906, acc.: 98.44%] [G loss: 0.301046]\n",
      "epoch:48 step:37878 [D loss: 0.000243, acc.: 100.00%] [G loss: 1.309840]\n",
      "epoch:48 step:37879 [D loss: 0.003819, acc.: 100.00%] [G loss: 0.392115]\n",
      "epoch:48 step:37880 [D loss: 0.001463, acc.: 100.00%] [G loss: 0.088918]\n",
      "epoch:48 step:37881 [D loss: 0.000564, acc.: 100.00%] [G loss: 0.351243]\n",
      "epoch:48 step:37882 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.019535]\n",
      "epoch:48 step:37883 [D loss: 0.045371, acc.: 97.66%] [G loss: 0.004803]\n",
      "epoch:48 step:37884 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.003317]\n",
      "epoch:48 step:37885 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.003231]\n",
      "epoch:48 step:37886 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.001409]\n",
      "epoch:48 step:37887 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000284]\n",
      "epoch:48 step:37888 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000711]\n",
      "epoch:48 step:37889 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000626]\n",
      "epoch:48 step:37890 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.002346]\n",
      "epoch:48 step:37891 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.002622]\n",
      "epoch:48 step:37892 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.004654]\n",
      "epoch:48 step:37893 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.014769]\n",
      "epoch:48 step:37894 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.606216]\n",
      "epoch:48 step:37895 [D loss: 0.002637, acc.: 100.00%] [G loss: 0.000390]\n",
      "epoch:48 step:37896 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.007961]\n",
      "epoch:48 step:37897 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.150994]\n",
      "epoch:48 step:37898 [D loss: 0.000427, acc.: 100.00%] [G loss: 0.015940]\n",
      "epoch:48 step:37899 [D loss: 0.001201, acc.: 100.00%] [G loss: 0.007319]\n",
      "epoch:48 step:37900 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.006012]\n",
      "epoch:48 step:37901 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.007147]\n",
      "epoch:48 step:37902 [D loss: 0.001939, acc.: 100.00%] [G loss: 0.029397]\n",
      "epoch:48 step:37903 [D loss: 0.007875, acc.: 100.00%] [G loss: 0.002234]\n",
      "epoch:48 step:37904 [D loss: 0.000211, acc.: 100.00%] [G loss: 0.003620]\n",
      "epoch:48 step:37905 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.009435]\n",
      "epoch:48 step:37906 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.022962]\n",
      "epoch:48 step:37907 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.004872]\n",
      "epoch:48 step:37908 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.001597]\n",
      "epoch:48 step:37909 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.001769]\n",
      "epoch:48 step:37910 [D loss: 0.000953, acc.: 100.00%] [G loss: 0.000374]\n",
      "epoch:48 step:37911 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.083092]\n",
      "epoch:48 step:37912 [D loss: 0.012771, acc.: 99.22%] [G loss: 0.000079]\n",
      "epoch:48 step:37913 [D loss: 0.001120, acc.: 100.00%] [G loss: 0.000261]\n",
      "epoch:48 step:37914 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.000147]\n",
      "epoch:48 step:37915 [D loss: 0.001156, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:48 step:37916 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.000929]\n",
      "epoch:48 step:37917 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.001390]\n",
      "epoch:48 step:37918 [D loss: 0.001153, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:48 step:37919 [D loss: 0.001437, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:48 step:37920 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.004347]\n",
      "epoch:48 step:37921 [D loss: 0.000874, acc.: 100.00%] [G loss: 0.054819]\n",
      "epoch:48 step:37922 [D loss: 0.000893, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:48 step:37923 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000094]\n",
      "epoch:48 step:37924 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.006550]\n",
      "epoch:48 step:37925 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:48 step:37926 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000290]\n",
      "epoch:48 step:37927 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000248]\n",
      "epoch:48 step:37928 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:48 step:37929 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000771]\n",
      "epoch:48 step:37930 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.003764]\n",
      "epoch:48 step:37931 [D loss: 0.000660, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:48 step:37932 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.003907]\n",
      "epoch:48 step:37933 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:48 step:37934 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000148]\n",
      "epoch:48 step:37935 [D loss: 0.000429, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:48 step:37936 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.000244]\n",
      "epoch:48 step:37937 [D loss: 0.017275, acc.: 99.22%] [G loss: 0.010732]\n",
      "epoch:48 step:37938 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.004387]\n",
      "epoch:48 step:37939 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.012036]\n",
      "epoch:48 step:37940 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.025566]\n",
      "epoch:48 step:37941 [D loss: 0.005349, acc.: 100.00%] [G loss: 0.000545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:37942 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.002600]\n",
      "epoch:48 step:37943 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.032680]\n",
      "epoch:48 step:37944 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.098464]\n",
      "epoch:48 step:37945 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001645]\n",
      "epoch:48 step:37946 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.001829]\n",
      "epoch:48 step:37947 [D loss: 0.000570, acc.: 100.00%] [G loss: 0.000616]\n",
      "epoch:48 step:37948 [D loss: 0.001278, acc.: 100.00%] [G loss: 0.000986]\n",
      "epoch:48 step:37949 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000772]\n",
      "epoch:48 step:37950 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:48 step:37951 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.004153]\n",
      "epoch:48 step:37952 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.244443]\n",
      "epoch:48 step:37953 [D loss: 0.002248, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:48 step:37954 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.001188]\n",
      "epoch:48 step:37955 [D loss: 0.005769, acc.: 100.00%] [G loss: 0.000417]\n",
      "epoch:48 step:37956 [D loss: 0.029303, acc.: 99.22%] [G loss: 0.016270]\n",
      "epoch:48 step:37957 [D loss: 0.000029, acc.: 100.00%] [G loss: 1.162917]\n",
      "epoch:48 step:37958 [D loss: 0.003417, acc.: 100.00%] [G loss: 0.400700]\n",
      "epoch:48 step:37959 [D loss: 0.011279, acc.: 100.00%] [G loss: 0.110417]\n",
      "epoch:48 step:37960 [D loss: 0.057210, acc.: 99.22%] [G loss: 0.234134]\n",
      "epoch:48 step:37961 [D loss: 0.016674, acc.: 100.00%] [G loss: 0.432841]\n",
      "epoch:48 step:37962 [D loss: 0.005666, acc.: 100.00%] [G loss: 0.109659]\n",
      "epoch:48 step:37963 [D loss: 0.206202, acc.: 91.41%] [G loss: 8.186068]\n",
      "epoch:48 step:37964 [D loss: 1.248129, acc.: 57.03%] [G loss: 7.891400]\n",
      "epoch:48 step:37965 [D loss: 0.000751, acc.: 100.00%] [G loss: 6.210242]\n",
      "epoch:48 step:37966 [D loss: 0.562176, acc.: 82.81%] [G loss: 3.947101]\n",
      "epoch:48 step:37967 [D loss: 0.156564, acc.: 96.88%] [G loss: 1.226011]\n",
      "epoch:48 step:37968 [D loss: 0.002406, acc.: 100.00%] [G loss: 0.901867]\n",
      "epoch:48 step:37969 [D loss: 0.725916, acc.: 71.88%] [G loss: 10.190287]\n",
      "epoch:48 step:37970 [D loss: 2.383935, acc.: 50.78%] [G loss: 2.198827]\n",
      "epoch:48 step:37971 [D loss: 0.860608, acc.: 66.41%] [G loss: 2.752165]\n",
      "epoch:48 step:37972 [D loss: 0.045719, acc.: 98.44%] [G loss: 4.334407]\n",
      "epoch:48 step:37973 [D loss: 0.221306, acc.: 88.28%] [G loss: 0.528910]\n",
      "epoch:48 step:37974 [D loss: 0.024048, acc.: 99.22%] [G loss: 4.846111]\n",
      "epoch:48 step:37975 [D loss: 0.006525, acc.: 100.00%] [G loss: 4.082042]\n",
      "epoch:48 step:37976 [D loss: 0.002489, acc.: 100.00%] [G loss: 0.011635]\n",
      "epoch:48 step:37977 [D loss: 0.000503, acc.: 100.00%] [G loss: 2.462507]\n",
      "epoch:48 step:37978 [D loss: 0.000944, acc.: 100.00%] [G loss: 1.541963]\n",
      "epoch:48 step:37979 [D loss: 0.003951, acc.: 100.00%] [G loss: 0.023368]\n",
      "epoch:48 step:37980 [D loss: 0.002032, acc.: 100.00%] [G loss: 0.681933]\n",
      "epoch:48 step:37981 [D loss: 0.000830, acc.: 100.00%] [G loss: 0.226235]\n",
      "epoch:48 step:37982 [D loss: 0.001107, acc.: 100.00%] [G loss: 0.156834]\n",
      "epoch:48 step:37983 [D loss: 0.002237, acc.: 100.00%] [G loss: 0.120694]\n",
      "epoch:48 step:37984 [D loss: 0.000557, acc.: 100.00%] [G loss: 0.000518]\n",
      "epoch:48 step:37985 [D loss: 0.002206, acc.: 100.00%] [G loss: 0.001326]\n",
      "epoch:48 step:37986 [D loss: 0.002606, acc.: 100.00%] [G loss: 0.040764]\n",
      "epoch:48 step:37987 [D loss: 0.004100, acc.: 100.00%] [G loss: 0.042647]\n",
      "epoch:48 step:37988 [D loss: 0.000673, acc.: 100.00%] [G loss: 0.063741]\n",
      "epoch:48 step:37989 [D loss: 0.001399, acc.: 100.00%] [G loss: 0.033579]\n",
      "epoch:48 step:37990 [D loss: 0.005605, acc.: 100.00%] [G loss: 0.010777]\n",
      "epoch:48 step:37991 [D loss: 0.001197, acc.: 100.00%] [G loss: 0.011782]\n",
      "epoch:48 step:37992 [D loss: 0.002329, acc.: 100.00%] [G loss: 0.015737]\n",
      "epoch:48 step:37993 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.000421]\n",
      "epoch:48 step:37994 [D loss: 0.001746, acc.: 100.00%] [G loss: 0.007388]\n",
      "epoch:48 step:37995 [D loss: 0.000610, acc.: 100.00%] [G loss: 0.012506]\n",
      "epoch:48 step:37996 [D loss: 0.000552, acc.: 100.00%] [G loss: 0.000521]\n",
      "epoch:48 step:37997 [D loss: 0.002936, acc.: 100.00%] [G loss: 0.003981]\n",
      "epoch:48 step:37998 [D loss: 0.000651, acc.: 100.00%] [G loss: 0.003709]\n",
      "epoch:48 step:37999 [D loss: 0.000947, acc.: 100.00%] [G loss: 0.008160]\n",
      "epoch:48 step:38000 [D loss: 0.005265, acc.: 100.00%] [G loss: 0.004746]\n",
      "epoch:48 step:38001 [D loss: 0.002030, acc.: 100.00%] [G loss: 0.397193]\n",
      "epoch:48 step:38002 [D loss: 0.001382, acc.: 100.00%] [G loss: 0.005619]\n",
      "epoch:48 step:38003 [D loss: 0.001211, acc.: 100.00%] [G loss: 0.006111]\n",
      "epoch:48 step:38004 [D loss: 0.028141, acc.: 99.22%] [G loss: 0.001463]\n",
      "epoch:48 step:38005 [D loss: 0.000580, acc.: 100.00%] [G loss: 0.005012]\n",
      "epoch:48 step:38006 [D loss: 0.002028, acc.: 100.00%] [G loss: 0.022736]\n",
      "epoch:48 step:38007 [D loss: 0.006227, acc.: 100.00%] [G loss: 0.008262]\n",
      "epoch:48 step:38008 [D loss: 0.056867, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:48 step:38009 [D loss: 0.001559, acc.: 100.00%] [G loss: 0.139470]\n",
      "epoch:48 step:38010 [D loss: 0.010115, acc.: 99.22%] [G loss: 0.236022]\n",
      "epoch:48 step:38011 [D loss: 0.000703, acc.: 100.00%] [G loss: 0.045578]\n",
      "epoch:48 step:38012 [D loss: 0.002755, acc.: 100.00%] [G loss: 0.034222]\n",
      "epoch:48 step:38013 [D loss: 0.001273, acc.: 100.00%] [G loss: 0.100467]\n",
      "epoch:48 step:38014 [D loss: 0.011317, acc.: 100.00%] [G loss: 0.004554]\n",
      "epoch:48 step:38015 [D loss: 0.003967, acc.: 100.00%] [G loss: 0.031365]\n",
      "epoch:48 step:38016 [D loss: 0.000858, acc.: 100.00%] [G loss: 0.018135]\n",
      "epoch:48 step:38017 [D loss: 0.000615, acc.: 100.00%] [G loss: 0.032034]\n",
      "epoch:48 step:38018 [D loss: 0.005487, acc.: 99.22%] [G loss: 0.013883]\n",
      "epoch:48 step:38019 [D loss: 0.000799, acc.: 100.00%] [G loss: 0.052245]\n",
      "epoch:48 step:38020 [D loss: 0.004002, acc.: 100.00%] [G loss: 0.000849]\n",
      "epoch:48 step:38021 [D loss: 0.000427, acc.: 100.00%] [G loss: 0.008457]\n",
      "epoch:48 step:38022 [D loss: 0.000675, acc.: 100.00%] [G loss: 0.016043]\n",
      "epoch:48 step:38023 [D loss: 0.000948, acc.: 100.00%] [G loss: 0.799546]\n",
      "epoch:48 step:38024 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.003744]\n",
      "epoch:48 step:38025 [D loss: 0.054400, acc.: 100.00%] [G loss: 0.013262]\n",
      "epoch:48 step:38026 [D loss: 0.008794, acc.: 100.00%] [G loss: 0.205267]\n",
      "epoch:48 step:38027 [D loss: 0.004750, acc.: 100.00%] [G loss: 0.100740]\n",
      "epoch:48 step:38028 [D loss: 0.011397, acc.: 100.00%] [G loss: 0.059396]\n",
      "epoch:48 step:38029 [D loss: 0.035435, acc.: 100.00%] [G loss: 0.041675]\n",
      "epoch:48 step:38030 [D loss: 0.020749, acc.: 99.22%] [G loss: 0.029187]\n",
      "epoch:48 step:38031 [D loss: 0.021019, acc.: 99.22%] [G loss: 0.000270]\n",
      "epoch:48 step:38032 [D loss: 0.004532, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:48 step:38033 [D loss: 0.003773, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:48 step:38034 [D loss: 0.045820, acc.: 97.66%] [G loss: 0.057442]\n",
      "epoch:48 step:38035 [D loss: 0.022315, acc.: 99.22%] [G loss: 0.183382]\n",
      "epoch:48 step:38036 [D loss: 0.003452, acc.: 100.00%] [G loss: 0.028649]\n",
      "epoch:48 step:38037 [D loss: 0.002007, acc.: 100.00%] [G loss: 0.024357]\n",
      "epoch:48 step:38038 [D loss: 0.014470, acc.: 99.22%] [G loss: 0.007358]\n",
      "epoch:48 step:38039 [D loss: 0.001620, acc.: 100.00%] [G loss: 0.009951]\n",
      "epoch:48 step:38040 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.005097]\n",
      "epoch:48 step:38041 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.008460]\n",
      "epoch:48 step:38042 [D loss: 0.000383, acc.: 100.00%] [G loss: 0.003530]\n",
      "epoch:48 step:38043 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.016444]\n",
      "epoch:48 step:38044 [D loss: 0.002602, acc.: 100.00%] [G loss: 0.009140]\n",
      "epoch:48 step:38045 [D loss: 0.002930, acc.: 100.00%] [G loss: 0.051453]\n",
      "epoch:48 step:38046 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.001129]\n",
      "epoch:48 step:38047 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.003204]\n",
      "epoch:48 step:38048 [D loss: 0.000970, acc.: 100.00%] [G loss: 0.011373]\n",
      "epoch:48 step:38049 [D loss: 0.002403, acc.: 100.00%] [G loss: 0.020615]\n",
      "epoch:48 step:38050 [D loss: 0.016324, acc.: 99.22%] [G loss: 0.002868]\n",
      "epoch:48 step:38051 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.011658]\n",
      "epoch:48 step:38052 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.001325]\n",
      "epoch:48 step:38053 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.000280]\n",
      "epoch:48 step:38054 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000254]\n",
      "epoch:48 step:38055 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.002294]\n",
      "epoch:48 step:38056 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.000127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38057 [D loss: 0.003192, acc.: 100.00%] [G loss: 0.001545]\n",
      "epoch:48 step:38058 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.000525]\n",
      "epoch:48 step:38059 [D loss: 0.756353, acc.: 64.06%] [G loss: 7.370701]\n",
      "epoch:48 step:38060 [D loss: 0.976020, acc.: 67.19%] [G loss: 4.552059]\n",
      "epoch:48 step:38061 [D loss: 0.032672, acc.: 99.22%] [G loss: 3.231777]\n",
      "epoch:48 step:38062 [D loss: 0.006910, acc.: 100.00%] [G loss: 1.987896]\n",
      "epoch:48 step:38063 [D loss: 0.246551, acc.: 92.97%] [G loss: 2.930985]\n",
      "epoch:48 step:38064 [D loss: 0.040288, acc.: 98.44%] [G loss: 2.032054]\n",
      "epoch:48 step:38065 [D loss: 0.157724, acc.: 94.53%] [G loss: 0.033490]\n",
      "epoch:48 step:38066 [D loss: 0.026237, acc.: 98.44%] [G loss: 0.007556]\n",
      "epoch:48 step:38067 [D loss: 0.003536, acc.: 100.00%] [G loss: 2.683912]\n",
      "epoch:48 step:38068 [D loss: 0.011263, acc.: 100.00%] [G loss: 1.619282]\n",
      "epoch:48 step:38069 [D loss: 0.001765, acc.: 100.00%] [G loss: 1.091635]\n",
      "epoch:48 step:38070 [D loss: 0.001130, acc.: 100.00%] [G loss: 0.000825]\n",
      "epoch:48 step:38071 [D loss: 0.016530, acc.: 99.22%] [G loss: 0.225260]\n",
      "epoch:48 step:38072 [D loss: 0.000779, acc.: 100.00%] [G loss: 0.000710]\n",
      "epoch:48 step:38073 [D loss: 0.091976, acc.: 96.09%] [G loss: 0.644577]\n",
      "epoch:48 step:38074 [D loss: 0.007623, acc.: 100.00%] [G loss: 0.558057]\n",
      "epoch:48 step:38075 [D loss: 0.010055, acc.: 100.00%] [G loss: 0.609872]\n",
      "epoch:48 step:38076 [D loss: 0.013474, acc.: 100.00%] [G loss: 0.172458]\n",
      "epoch:48 step:38077 [D loss: 0.009899, acc.: 100.00%] [G loss: 0.203227]\n",
      "epoch:48 step:38078 [D loss: 0.001499, acc.: 100.00%] [G loss: 0.034547]\n",
      "epoch:48 step:38079 [D loss: 0.012866, acc.: 100.00%] [G loss: 0.095295]\n",
      "epoch:48 step:38080 [D loss: 0.006398, acc.: 100.00%] [G loss: 0.050826]\n",
      "epoch:48 step:38081 [D loss: 0.006182, acc.: 100.00%] [G loss: 0.027842]\n",
      "epoch:48 step:38082 [D loss: 0.000896, acc.: 100.00%] [G loss: 0.002368]\n",
      "epoch:48 step:38083 [D loss: 0.000536, acc.: 100.00%] [G loss: 0.011901]\n",
      "epoch:48 step:38084 [D loss: 0.004500, acc.: 100.00%] [G loss: 0.019222]\n",
      "epoch:48 step:38085 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.004880]\n",
      "epoch:48 step:38086 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.035513]\n",
      "epoch:48 step:38087 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.006874]\n",
      "epoch:48 step:38088 [D loss: 0.001205, acc.: 100.00%] [G loss: 0.023673]\n",
      "epoch:48 step:38089 [D loss: 0.002715, acc.: 100.00%] [G loss: 0.035581]\n",
      "epoch:48 step:38090 [D loss: 0.050735, acc.: 98.44%] [G loss: 0.021798]\n",
      "epoch:48 step:38091 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.105838]\n",
      "epoch:48 step:38092 [D loss: 0.183336, acc.: 93.75%] [G loss: 0.000435]\n",
      "epoch:48 step:38093 [D loss: 0.306576, acc.: 82.81%] [G loss: 0.353045]\n",
      "epoch:48 step:38094 [D loss: 0.200837, acc.: 87.50%] [G loss: 0.212397]\n",
      "epoch:48 step:38095 [D loss: 0.121864, acc.: 95.31%] [G loss: 1.441422]\n",
      "epoch:48 step:38096 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.005881]\n",
      "epoch:48 step:38097 [D loss: 0.003097, acc.: 100.00%] [G loss: 0.006169]\n",
      "epoch:48 step:38098 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.008069]\n",
      "epoch:48 step:38099 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.053518]\n",
      "epoch:48 step:38100 [D loss: 0.000117, acc.: 100.00%] [G loss: 0.019805]\n",
      "epoch:48 step:38101 [D loss: 0.001664, acc.: 100.00%] [G loss: 0.003246]\n",
      "epoch:48 step:38102 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000480]\n",
      "epoch:48 step:38103 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.208026]\n",
      "epoch:48 step:38104 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.001964]\n",
      "epoch:48 step:38105 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.071188]\n",
      "epoch:48 step:38106 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.001358]\n",
      "epoch:48 step:38107 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.048891]\n",
      "epoch:48 step:38108 [D loss: 0.003357, acc.: 100.00%] [G loss: 0.000431]\n",
      "epoch:48 step:38109 [D loss: 0.000444, acc.: 100.00%] [G loss: 0.084743]\n",
      "epoch:48 step:38110 [D loss: 0.002013, acc.: 100.00%] [G loss: 0.141199]\n",
      "epoch:48 step:38111 [D loss: 0.002529, acc.: 100.00%] [G loss: 0.001789]\n",
      "epoch:48 step:38112 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.023887]\n",
      "epoch:48 step:38113 [D loss: 0.003731, acc.: 100.00%] [G loss: 0.001912]\n",
      "epoch:48 step:38114 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.039522]\n",
      "epoch:48 step:38115 [D loss: 0.007854, acc.: 100.00%] [G loss: 0.001656]\n",
      "epoch:48 step:38116 [D loss: 0.008884, acc.: 100.00%] [G loss: 0.000639]\n",
      "epoch:48 step:38117 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.002468]\n",
      "epoch:48 step:38118 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.099391]\n",
      "epoch:48 step:38119 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.023665]\n",
      "epoch:48 step:38120 [D loss: 0.000569, acc.: 100.00%] [G loss: 0.023793]\n",
      "epoch:48 step:38121 [D loss: 0.012349, acc.: 100.00%] [G loss: 0.000278]\n",
      "epoch:48 step:38122 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.001234]\n",
      "epoch:48 step:38123 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.032509]\n",
      "epoch:48 step:38124 [D loss: 0.000132, acc.: 100.00%] [G loss: 2.028235]\n",
      "epoch:48 step:38125 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.005767]\n",
      "epoch:48 step:38126 [D loss: 0.010714, acc.: 100.00%] [G loss: 0.070731]\n",
      "epoch:48 step:38127 [D loss: 0.027436, acc.: 100.00%] [G loss: 0.001184]\n",
      "epoch:48 step:38128 [D loss: 0.000668, acc.: 100.00%] [G loss: 0.010320]\n",
      "epoch:48 step:38129 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.001838]\n",
      "epoch:48 step:38130 [D loss: 0.000797, acc.: 100.00%] [G loss: 0.002707]\n",
      "epoch:48 step:38131 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.037841]\n",
      "epoch:48 step:38132 [D loss: 0.001906, acc.: 100.00%] [G loss: 0.001256]\n",
      "epoch:48 step:38133 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.033645]\n",
      "epoch:48 step:38134 [D loss: 0.017910, acc.: 100.00%] [G loss: 0.024823]\n",
      "epoch:48 step:38135 [D loss: 0.005765, acc.: 100.00%] [G loss: 0.009115]\n",
      "epoch:48 step:38136 [D loss: 0.002039, acc.: 100.00%] [G loss: 0.007167]\n",
      "epoch:48 step:38137 [D loss: 0.004735, acc.: 100.00%] [G loss: 0.000575]\n",
      "epoch:48 step:38138 [D loss: 0.001128, acc.: 100.00%] [G loss: 0.001570]\n",
      "epoch:48 step:38139 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.000354]\n",
      "epoch:48 step:38140 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.192565]\n",
      "epoch:48 step:38141 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000387]\n",
      "epoch:48 step:38142 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.005383]\n",
      "epoch:48 step:38143 [D loss: 0.002234, acc.: 100.00%] [G loss: 0.075883]\n",
      "epoch:48 step:38144 [D loss: 0.000280, acc.: 100.00%] [G loss: 0.006438]\n",
      "epoch:48 step:38145 [D loss: 0.001978, acc.: 100.00%] [G loss: 0.016927]\n",
      "epoch:48 step:38146 [D loss: 0.014033, acc.: 100.00%] [G loss: 0.004210]\n",
      "epoch:48 step:38147 [D loss: 0.000982, acc.: 100.00%] [G loss: 0.055399]\n",
      "epoch:48 step:38148 [D loss: 0.001149, acc.: 100.00%] [G loss: 0.252069]\n",
      "epoch:48 step:38149 [D loss: 0.000478, acc.: 100.00%] [G loss: 0.005356]\n",
      "epoch:48 step:38150 [D loss: 0.005189, acc.: 100.00%] [G loss: 0.000545]\n",
      "epoch:48 step:38151 [D loss: 0.000725, acc.: 100.00%] [G loss: 0.020777]\n",
      "epoch:48 step:38152 [D loss: 0.004836, acc.: 100.00%] [G loss: 0.002186]\n",
      "epoch:48 step:38153 [D loss: 0.047153, acc.: 98.44%] [G loss: 0.022303]\n",
      "epoch:48 step:38154 [D loss: 0.000579, acc.: 100.00%] [G loss: 0.000459]\n",
      "epoch:48 step:38155 [D loss: 0.010887, acc.: 100.00%] [G loss: 0.000582]\n",
      "epoch:48 step:38156 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000544]\n",
      "epoch:48 step:38157 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.010408]\n",
      "epoch:48 step:38158 [D loss: 0.007153, acc.: 99.22%] [G loss: 0.002510]\n",
      "epoch:48 step:38159 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.002252]\n",
      "epoch:48 step:38160 [D loss: 0.004439, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:48 step:38161 [D loss: 0.000686, acc.: 100.00%] [G loss: 0.025686]\n",
      "epoch:48 step:38162 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.000250]\n",
      "epoch:48 step:38163 [D loss: 0.000817, acc.: 100.00%] [G loss: 0.011649]\n",
      "epoch:48 step:38164 [D loss: 0.001476, acc.: 100.00%] [G loss: 0.012498]\n",
      "epoch:48 step:38165 [D loss: 0.003945, acc.: 100.00%] [G loss: 0.000289]\n",
      "epoch:48 step:38166 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:48 step:38167 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.011780]\n",
      "epoch:48 step:38168 [D loss: 0.000361, acc.: 100.00%] [G loss: 0.001220]\n",
      "epoch:48 step:38169 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.002533]\n",
      "epoch:48 step:38170 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.002142]\n",
      "epoch:48 step:38171 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.004170]\n",
      "epoch:48 step:38172 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:48 step:38173 [D loss: 0.000707, acc.: 100.00%] [G loss: 0.041332]\n",
      "epoch:48 step:38174 [D loss: 0.001304, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:48 step:38175 [D loss: 0.001813, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:48 step:38176 [D loss: 0.000310, acc.: 100.00%] [G loss: 0.000157]\n",
      "epoch:48 step:38177 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:48 step:38178 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:48 step:38179 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000188]\n",
      "epoch:48 step:38180 [D loss: 0.001061, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:48 step:38181 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.006925]\n",
      "epoch:48 step:38182 [D loss: 0.000916, acc.: 100.00%] [G loss: 0.002201]\n",
      "epoch:48 step:38183 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001487]\n",
      "epoch:48 step:38184 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000166]\n",
      "epoch:48 step:38185 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:48 step:38186 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:48 step:38187 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.001204]\n",
      "epoch:48 step:38188 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.020446]\n",
      "epoch:48 step:38189 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000117]\n",
      "epoch:48 step:38190 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.000125]\n",
      "epoch:48 step:38191 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:48 step:38192 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.000044]\n",
      "epoch:48 step:38193 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.003390]\n",
      "epoch:48 step:38194 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.002773]\n",
      "epoch:48 step:38195 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000136]\n",
      "epoch:48 step:38196 [D loss: 0.000485, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:48 step:38197 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000053]\n",
      "epoch:48 step:38198 [D loss: 0.000163, acc.: 100.00%] [G loss: 0.002042]\n",
      "epoch:48 step:38199 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:48 step:38200 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.063188]\n",
      "epoch:48 step:38201 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.004092]\n",
      "epoch:48 step:38202 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.001253]\n",
      "epoch:48 step:38203 [D loss: 0.016467, acc.: 99.22%] [G loss: 0.001401]\n",
      "epoch:48 step:38204 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000589]\n",
      "epoch:48 step:38205 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.001391]\n",
      "epoch:48 step:38206 [D loss: 0.001820, acc.: 100.00%] [G loss: 0.001971]\n",
      "epoch:48 step:38207 [D loss: 0.006198, acc.: 100.00%] [G loss: 0.002623]\n",
      "epoch:48 step:38208 [D loss: 0.004952, acc.: 100.00%] [G loss: 0.001612]\n",
      "epoch:48 step:38209 [D loss: 0.020958, acc.: 100.00%] [G loss: 0.068637]\n",
      "epoch:48 step:38210 [D loss: 0.005225, acc.: 100.00%] [G loss: 0.002739]\n",
      "epoch:48 step:38211 [D loss: 0.001110, acc.: 100.00%] [G loss: 0.119870]\n",
      "epoch:48 step:38212 [D loss: 0.001319, acc.: 100.00%] [G loss: 0.001327]\n",
      "epoch:48 step:38213 [D loss: 0.002848, acc.: 100.00%] [G loss: 0.001583]\n",
      "epoch:48 step:38214 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.023564]\n",
      "epoch:48 step:38215 [D loss: 0.001093, acc.: 100.00%] [G loss: 0.000459]\n",
      "epoch:48 step:38216 [D loss: 0.000395, acc.: 100.00%] [G loss: 0.070926]\n",
      "epoch:48 step:38217 [D loss: 0.000527, acc.: 100.00%] [G loss: 0.000502]\n",
      "epoch:48 step:38218 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.001387]\n",
      "epoch:48 step:38219 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000124]\n",
      "epoch:48 step:38220 [D loss: 0.000328, acc.: 100.00%] [G loss: 0.000328]\n",
      "epoch:48 step:38221 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.000737]\n",
      "epoch:48 step:38222 [D loss: 0.000769, acc.: 100.00%] [G loss: 0.001228]\n",
      "epoch:48 step:38223 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.020568]\n",
      "epoch:48 step:38224 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.000862]\n",
      "epoch:48 step:38225 [D loss: 0.000779, acc.: 100.00%] [G loss: 0.002896]\n",
      "epoch:48 step:38226 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.001493]\n",
      "epoch:48 step:38227 [D loss: 0.001450, acc.: 100.00%] [G loss: 0.000116]\n",
      "epoch:48 step:38228 [D loss: 0.000337, acc.: 100.00%] [G loss: 0.000843]\n",
      "epoch:48 step:38229 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.001787]\n",
      "epoch:48 step:38230 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.009939]\n",
      "epoch:48 step:38231 [D loss: 0.000365, acc.: 100.00%] [G loss: 0.000134]\n",
      "epoch:48 step:38232 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.000443]\n",
      "epoch:48 step:38233 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000291]\n",
      "epoch:48 step:38234 [D loss: 0.008528, acc.: 100.00%] [G loss: 0.000775]\n",
      "epoch:48 step:38235 [D loss: 0.001949, acc.: 100.00%] [G loss: 0.001825]\n",
      "epoch:48 step:38236 [D loss: 0.006471, acc.: 100.00%] [G loss: 0.003295]\n",
      "epoch:48 step:38237 [D loss: 0.010190, acc.: 100.00%] [G loss: 0.313890]\n",
      "epoch:48 step:38238 [D loss: 0.013061, acc.: 100.00%] [G loss: 0.006738]\n",
      "epoch:48 step:38239 [D loss: 0.101615, acc.: 96.88%] [G loss: 0.729912]\n",
      "epoch:48 step:38240 [D loss: 0.060348, acc.: 96.09%] [G loss: 0.249712]\n",
      "epoch:48 step:38241 [D loss: 0.181010, acc.: 92.97%] [G loss: 0.001192]\n",
      "epoch:48 step:38242 [D loss: 0.496568, acc.: 78.91%] [G loss: 6.509332]\n",
      "epoch:48 step:38243 [D loss: 1.072405, acc.: 64.84%] [G loss: 0.082863]\n",
      "epoch:48 step:38244 [D loss: 0.000111, acc.: 100.00%] [G loss: 3.190803]\n",
      "epoch:48 step:38245 [D loss: 0.000082, acc.: 100.00%] [G loss: 1.235350]\n",
      "epoch:48 step:38246 [D loss: 0.000978, acc.: 100.00%] [G loss: 0.003641]\n",
      "epoch:48 step:38247 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.232995]\n",
      "epoch:48 step:38248 [D loss: 0.000511, acc.: 100.00%] [G loss: 0.084298]\n",
      "epoch:48 step:38249 [D loss: 0.000229, acc.: 100.00%] [G loss: 0.025760]\n",
      "epoch:48 step:38250 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.155352]\n",
      "epoch:48 step:38251 [D loss: 0.004020, acc.: 100.00%] [G loss: 0.001247]\n",
      "epoch:48 step:38252 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.004377]\n",
      "epoch:48 step:38253 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001949]\n",
      "epoch:48 step:38254 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.036067]\n",
      "epoch:48 step:38255 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.001173]\n",
      "epoch:48 step:38256 [D loss: 0.001009, acc.: 100.00%] [G loss: 0.048706]\n",
      "epoch:48 step:38257 [D loss: 0.000725, acc.: 100.00%] [G loss: 0.001173]\n",
      "epoch:48 step:38258 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.003425]\n",
      "epoch:48 step:38259 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.000966]\n",
      "epoch:48 step:38260 [D loss: 0.001036, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:48 step:38261 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.005036]\n",
      "epoch:48 step:38262 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000501]\n",
      "epoch:48 step:38263 [D loss: 0.001936, acc.: 100.00%] [G loss: 0.001223]\n",
      "epoch:48 step:38264 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.008759]\n",
      "epoch:48 step:38265 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.002836]\n",
      "epoch:48 step:38266 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.001705]\n",
      "epoch:48 step:38267 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.006597]\n",
      "epoch:48 step:38268 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000674]\n",
      "epoch:48 step:38269 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.001308]\n",
      "epoch:49 step:38270 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.000403]\n",
      "epoch:49 step:38271 [D loss: 0.001763, acc.: 100.00%] [G loss: 0.000272]\n",
      "epoch:49 step:38272 [D loss: 0.078418, acc.: 98.44%] [G loss: 0.032100]\n",
      "epoch:49 step:38273 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.182320]\n",
      "epoch:49 step:38274 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.136067]\n",
      "epoch:49 step:38275 [D loss: 0.014141, acc.: 99.22%] [G loss: 0.102813]\n",
      "epoch:49 step:38276 [D loss: 0.005201, acc.: 100.00%] [G loss: 0.328592]\n",
      "epoch:49 step:38277 [D loss: 0.000825, acc.: 100.00%] [G loss: 0.004467]\n",
      "epoch:49 step:38278 [D loss: 0.000135, acc.: 100.00%] [G loss: 0.017391]\n",
      "epoch:49 step:38279 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.597265]\n",
      "epoch:49 step:38280 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.002502]\n",
      "epoch:49 step:38281 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.029614]\n",
      "epoch:49 step:38282 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000517]\n",
      "epoch:49 step:38283 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.012053]\n",
      "epoch:49 step:38284 [D loss: 0.000774, acc.: 100.00%] [G loss: 0.008595]\n",
      "epoch:49 step:38285 [D loss: 0.000620, acc.: 100.00%] [G loss: 0.066850]\n",
      "epoch:49 step:38286 [D loss: 0.000834, acc.: 100.00%] [G loss: 0.018724]\n",
      "epoch:49 step:38287 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.008881]\n",
      "epoch:49 step:38288 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.003125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38289 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.005272]\n",
      "epoch:49 step:38290 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.001347]\n",
      "epoch:49 step:38291 [D loss: 0.014361, acc.: 99.22%] [G loss: 0.001047]\n",
      "epoch:49 step:38292 [D loss: 0.000978, acc.: 100.00%] [G loss: 0.020758]\n",
      "epoch:49 step:38293 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001012]\n",
      "epoch:49 step:38294 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.000332]\n",
      "epoch:49 step:38295 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.003164]\n",
      "epoch:49 step:38296 [D loss: 0.000210, acc.: 100.00%] [G loss: 0.000289]\n",
      "epoch:49 step:38297 [D loss: 0.000173, acc.: 100.00%] [G loss: 0.000673]\n",
      "epoch:49 step:38298 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000380]\n",
      "epoch:49 step:38299 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:49 step:38300 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.001654]\n",
      "epoch:49 step:38301 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:49 step:38302 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.002111]\n",
      "epoch:49 step:38303 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:49 step:38304 [D loss: 0.000866, acc.: 100.00%] [G loss: 0.000915]\n",
      "epoch:49 step:38305 [D loss: 0.002260, acc.: 100.00%] [G loss: 0.001318]\n",
      "epoch:49 step:38306 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.000066]\n",
      "epoch:49 step:38307 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.001100]\n",
      "epoch:49 step:38308 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001033]\n",
      "epoch:49 step:38309 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:49 step:38310 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.000792]\n",
      "epoch:49 step:38311 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.006344]\n",
      "epoch:49 step:38312 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000781]\n",
      "epoch:49 step:38313 [D loss: 0.000292, acc.: 100.00%] [G loss: 0.003871]\n",
      "epoch:49 step:38314 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:49 step:38315 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.008747]\n",
      "epoch:49 step:38316 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:49 step:38317 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.001542]\n",
      "epoch:49 step:38318 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:49 step:38319 [D loss: 0.000192, acc.: 100.00%] [G loss: 0.004131]\n",
      "epoch:49 step:38320 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:49 step:38321 [D loss: 0.002148, acc.: 100.00%] [G loss: 0.000851]\n",
      "epoch:49 step:38322 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000242]\n",
      "epoch:49 step:38323 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000343]\n",
      "epoch:49 step:38324 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000538]\n",
      "epoch:49 step:38325 [D loss: 0.009178, acc.: 100.00%] [G loss: 0.008135]\n",
      "epoch:49 step:38326 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:49 step:38327 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.002056]\n",
      "epoch:49 step:38328 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:49 step:38329 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000753]\n",
      "epoch:49 step:38330 [D loss: 0.000460, acc.: 100.00%] [G loss: 0.000887]\n",
      "epoch:49 step:38331 [D loss: 0.000882, acc.: 100.00%] [G loss: 0.000720]\n",
      "epoch:49 step:38332 [D loss: 0.000996, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:49 step:38333 [D loss: 0.000282, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:49 step:38334 [D loss: 0.001161, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:49 step:38335 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:49 step:38336 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000497]\n",
      "epoch:49 step:38337 [D loss: 0.000350, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38338 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:49 step:38339 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.000494]\n",
      "epoch:49 step:38340 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000087]\n",
      "epoch:49 step:38341 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.012018]\n",
      "epoch:49 step:38342 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:49 step:38343 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:49 step:38344 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:49 step:38345 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:49 step:38346 [D loss: 0.000307, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:49 step:38347 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000978]\n",
      "epoch:49 step:38348 [D loss: 0.000602, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:49 step:38349 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:49 step:38350 [D loss: 0.003513, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:49 step:38351 [D loss: 0.005978, acc.: 100.00%] [G loss: 0.000610]\n",
      "epoch:49 step:38352 [D loss: 0.000303, acc.: 100.00%] [G loss: 0.000441]\n",
      "epoch:49 step:38353 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.001026]\n",
      "epoch:49 step:38354 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:49 step:38355 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.001522]\n",
      "epoch:49 step:38356 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:49 step:38357 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:49 step:38358 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:49 step:38359 [D loss: 0.001107, acc.: 100.00%] [G loss: 0.000017]\n",
      "epoch:49 step:38360 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000695]\n",
      "epoch:49 step:38361 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000108]\n",
      "epoch:49 step:38362 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000235]\n",
      "epoch:49 step:38363 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000215]\n",
      "epoch:49 step:38364 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000201]\n",
      "epoch:49 step:38365 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:49 step:38366 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000473]\n",
      "epoch:49 step:38367 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.003223]\n",
      "epoch:49 step:38368 [D loss: 0.000660, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:49 step:38369 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:49 step:38370 [D loss: 0.000122, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:49 step:38371 [D loss: 0.025377, acc.: 99.22%] [G loss: 0.000151]\n",
      "epoch:49 step:38372 [D loss: 0.003229, acc.: 100.00%] [G loss: 0.007170]\n",
      "epoch:49 step:38373 [D loss: 0.066475, acc.: 98.44%] [G loss: 0.502355]\n",
      "epoch:49 step:38374 [D loss: 0.001709, acc.: 100.00%] [G loss: 0.628924]\n",
      "epoch:49 step:38375 [D loss: 0.007686, acc.: 100.00%] [G loss: 0.292029]\n",
      "epoch:49 step:38376 [D loss: 0.083140, acc.: 96.88%] [G loss: 0.003185]\n",
      "epoch:49 step:38377 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.001085]\n",
      "epoch:49 step:38378 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:49 step:38379 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.001728]\n",
      "epoch:49 step:38380 [D loss: 0.001353, acc.: 100.00%] [G loss: 0.000171]\n",
      "epoch:49 step:38381 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000359]\n",
      "epoch:49 step:38382 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.006813]\n",
      "epoch:49 step:38383 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000506]\n",
      "epoch:49 step:38384 [D loss: 0.002072, acc.: 100.00%] [G loss: 0.002854]\n",
      "epoch:49 step:38385 [D loss: 0.040257, acc.: 98.44%] [G loss: 0.007855]\n",
      "epoch:49 step:38386 [D loss: 0.000431, acc.: 100.00%] [G loss: 0.183485]\n",
      "epoch:49 step:38387 [D loss: 0.841097, acc.: 68.75%] [G loss: 7.055026]\n",
      "epoch:49 step:38388 [D loss: 2.906878, acc.: 50.78%] [G loss: 5.904210]\n",
      "epoch:49 step:38389 [D loss: 0.194332, acc.: 89.06%] [G loss: 2.580581]\n",
      "epoch:49 step:38390 [D loss: 0.010427, acc.: 99.22%] [G loss: 0.832540]\n",
      "epoch:49 step:38391 [D loss: 0.004304, acc.: 100.00%] [G loss: 0.114191]\n",
      "epoch:49 step:38392 [D loss: 0.003880, acc.: 100.00%] [G loss: 0.201850]\n",
      "epoch:49 step:38393 [D loss: 0.005306, acc.: 100.00%] [G loss: 0.218499]\n",
      "epoch:49 step:38394 [D loss: 0.058720, acc.: 99.22%] [G loss: 0.266857]\n",
      "epoch:49 step:38395 [D loss: 0.001695, acc.: 100.00%] [G loss: 0.143225]\n",
      "epoch:49 step:38396 [D loss: 0.005449, acc.: 100.00%] [G loss: 0.105962]\n",
      "epoch:49 step:38397 [D loss: 0.022499, acc.: 100.00%] [G loss: 0.038702]\n",
      "epoch:49 step:38398 [D loss: 0.046663, acc.: 100.00%] [G loss: 0.146323]\n",
      "epoch:49 step:38399 [D loss: 0.326554, acc.: 86.72%] [G loss: 2.698017]\n",
      "epoch:49 step:38400 [D loss: 0.100871, acc.: 96.09%] [G loss: 3.396003]\n",
      "epoch:49 step:38401 [D loss: 0.392086, acc.: 77.34%] [G loss: 1.530674]\n",
      "epoch:49 step:38402 [D loss: 0.022740, acc.: 99.22%] [G loss: 0.021434]\n",
      "epoch:49 step:38403 [D loss: 0.001573, acc.: 100.00%] [G loss: 0.075362]\n",
      "epoch:49 step:38404 [D loss: 0.000670, acc.: 100.00%] [G loss: 0.021992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38405 [D loss: 0.001269, acc.: 100.00%] [G loss: 0.056732]\n",
      "epoch:49 step:38406 [D loss: 0.014073, acc.: 100.00%] [G loss: 0.010037]\n",
      "epoch:49 step:38407 [D loss: 0.001299, acc.: 100.00%] [G loss: 0.044406]\n",
      "epoch:49 step:38408 [D loss: 0.011684, acc.: 99.22%] [G loss: 0.037794]\n",
      "epoch:49 step:38409 [D loss: 0.001089, acc.: 100.00%] [G loss: 0.006813]\n",
      "epoch:49 step:38410 [D loss: 0.000503, acc.: 100.00%] [G loss: 0.003026]\n",
      "epoch:49 step:38411 [D loss: 0.000789, acc.: 100.00%] [G loss: 0.007396]\n",
      "epoch:49 step:38412 [D loss: 0.003102, acc.: 100.00%] [G loss: 0.033736]\n",
      "epoch:49 step:38413 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.013884]\n",
      "epoch:49 step:38414 [D loss: 0.004098, acc.: 100.00%] [G loss: 0.003748]\n",
      "epoch:49 step:38415 [D loss: 0.000384, acc.: 100.00%] [G loss: 0.002234]\n",
      "epoch:49 step:38416 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.001752]\n",
      "epoch:49 step:38417 [D loss: 0.000598, acc.: 100.00%] [G loss: 0.008271]\n",
      "epoch:49 step:38418 [D loss: 0.000419, acc.: 100.00%] [G loss: 0.004797]\n",
      "epoch:49 step:38419 [D loss: 0.002332, acc.: 100.00%] [G loss: 0.003307]\n",
      "epoch:49 step:38420 [D loss: 0.003969, acc.: 100.00%] [G loss: 0.021477]\n",
      "epoch:49 step:38421 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.001172]\n",
      "epoch:49 step:38422 [D loss: 0.001841, acc.: 100.00%] [G loss: 0.003689]\n",
      "epoch:49 step:38423 [D loss: 0.000298, acc.: 100.00%] [G loss: 0.007039]\n",
      "epoch:49 step:38424 [D loss: 0.000615, acc.: 100.00%] [G loss: 0.005633]\n",
      "epoch:49 step:38425 [D loss: 0.000164, acc.: 100.00%] [G loss: 0.006300]\n",
      "epoch:49 step:38426 [D loss: 0.000355, acc.: 100.00%] [G loss: 0.005942]\n",
      "epoch:49 step:38427 [D loss: 0.002113, acc.: 100.00%] [G loss: 0.114040]\n",
      "epoch:49 step:38428 [D loss: 0.001650, acc.: 100.00%] [G loss: 0.001195]\n",
      "epoch:49 step:38429 [D loss: 0.009015, acc.: 100.00%] [G loss: 0.002406]\n",
      "epoch:49 step:38430 [D loss: 0.000919, acc.: 100.00%] [G loss: 0.004049]\n",
      "epoch:49 step:38431 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.001449]\n",
      "epoch:49 step:38432 [D loss: 0.008460, acc.: 100.00%] [G loss: 0.005872]\n",
      "epoch:49 step:38433 [D loss: 0.017976, acc.: 100.00%] [G loss: 0.001537]\n",
      "epoch:49 step:38434 [D loss: 0.001684, acc.: 100.00%] [G loss: 0.006259]\n",
      "epoch:49 step:38435 [D loss: 0.002415, acc.: 100.00%] [G loss: 0.024759]\n",
      "epoch:49 step:38436 [D loss: 0.001608, acc.: 100.00%] [G loss: 0.006843]\n",
      "epoch:49 step:38437 [D loss: 0.006118, acc.: 100.00%] [G loss: 0.024223]\n",
      "epoch:49 step:38438 [D loss: 0.001459, acc.: 100.00%] [G loss: 0.007829]\n",
      "epoch:49 step:38439 [D loss: 0.001282, acc.: 100.00%] [G loss: 0.011279]\n",
      "epoch:49 step:38440 [D loss: 0.001882, acc.: 100.00%] [G loss: 0.001048]\n",
      "epoch:49 step:38441 [D loss: 0.000481, acc.: 100.00%] [G loss: 0.047375]\n",
      "epoch:49 step:38442 [D loss: 0.001092, acc.: 100.00%] [G loss: 0.003262]\n",
      "epoch:49 step:38443 [D loss: 0.002063, acc.: 100.00%] [G loss: 0.000933]\n",
      "epoch:49 step:38444 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.004139]\n",
      "epoch:49 step:38445 [D loss: 0.018919, acc.: 99.22%] [G loss: 0.008905]\n",
      "epoch:49 step:38446 [D loss: 0.000265, acc.: 100.00%] [G loss: 0.001063]\n",
      "epoch:49 step:38447 [D loss: 0.001282, acc.: 100.00%] [G loss: 0.014936]\n",
      "epoch:49 step:38448 [D loss: 0.008669, acc.: 100.00%] [G loss: 0.000399]\n",
      "epoch:49 step:38449 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.001716]\n",
      "epoch:49 step:38450 [D loss: 0.001169, acc.: 100.00%] [G loss: 0.000273]\n",
      "epoch:49 step:38451 [D loss: 0.000170, acc.: 100.00%] [G loss: 0.002152]\n",
      "epoch:49 step:38452 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.000229]\n",
      "epoch:49 step:38453 [D loss: 0.000975, acc.: 100.00%] [G loss: 0.000329]\n",
      "epoch:49 step:38454 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.001845]\n",
      "epoch:49 step:38455 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.001520]\n",
      "epoch:49 step:38456 [D loss: 0.011153, acc.: 100.00%] [G loss: 0.000269]\n",
      "epoch:49 step:38457 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.001153]\n",
      "epoch:49 step:38458 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000375]\n",
      "epoch:49 step:38459 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000820]\n",
      "epoch:49 step:38460 [D loss: 0.001441, acc.: 100.00%] [G loss: 0.000203]\n",
      "epoch:49 step:38461 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.000998]\n",
      "epoch:49 step:38462 [D loss: 0.000534, acc.: 100.00%] [G loss: 0.005637]\n",
      "epoch:49 step:38463 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.000616]\n",
      "epoch:49 step:38464 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.004152]\n",
      "epoch:49 step:38465 [D loss: 0.001674, acc.: 100.00%] [G loss: 0.009031]\n",
      "epoch:49 step:38466 [D loss: 0.005776, acc.: 100.00%] [G loss: 0.001420]\n",
      "epoch:49 step:38467 [D loss: 0.096806, acc.: 96.88%] [G loss: 0.048438]\n",
      "epoch:49 step:38468 [D loss: 0.006206, acc.: 100.00%] [G loss: 1.077052]\n",
      "epoch:49 step:38469 [D loss: 0.043199, acc.: 99.22%] [G loss: 0.004928]\n",
      "epoch:49 step:38470 [D loss: 0.000884, acc.: 100.00%] [G loss: 0.073217]\n",
      "epoch:49 step:38471 [D loss: 0.000759, acc.: 100.00%] [G loss: 0.008092]\n",
      "epoch:49 step:38472 [D loss: 0.002497, acc.: 100.00%] [G loss: 0.002312]\n",
      "epoch:49 step:38473 [D loss: 0.001239, acc.: 100.00%] [G loss: 0.004511]\n",
      "epoch:49 step:38474 [D loss: 0.000838, acc.: 100.00%] [G loss: 0.006496]\n",
      "epoch:49 step:38475 [D loss: 0.004832, acc.: 100.00%] [G loss: 0.003829]\n",
      "epoch:49 step:38476 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.018088]\n",
      "epoch:49 step:38477 [D loss: 0.003195, acc.: 100.00%] [G loss: 0.001208]\n",
      "epoch:49 step:38478 [D loss: 0.038650, acc.: 99.22%] [G loss: 0.004603]\n",
      "epoch:49 step:38479 [D loss: 0.000516, acc.: 100.00%] [G loss: 0.000345]\n",
      "epoch:49 step:38480 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000295]\n",
      "epoch:49 step:38481 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.001375]\n",
      "epoch:49 step:38482 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000429]\n",
      "epoch:49 step:38483 [D loss: 0.001573, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:49 step:38484 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:49 step:38485 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.013010]\n",
      "epoch:49 step:38486 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000362]\n",
      "epoch:49 step:38487 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.000187]\n",
      "epoch:49 step:38488 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:49 step:38489 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.039667]\n",
      "epoch:49 step:38490 [D loss: 0.000487, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:49 step:38491 [D loss: 0.002317, acc.: 100.00%] [G loss: 0.010667]\n",
      "epoch:49 step:38492 [D loss: 0.000637, acc.: 100.00%] [G loss: 0.035142]\n",
      "epoch:49 step:38493 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.001837]\n",
      "epoch:49 step:38494 [D loss: 0.007233, acc.: 100.00%] [G loss: 0.000581]\n",
      "epoch:49 step:38495 [D loss: 0.000783, acc.: 100.00%] [G loss: 0.002281]\n",
      "epoch:49 step:38496 [D loss: 0.005409, acc.: 100.00%] [G loss: 0.000788]\n",
      "epoch:49 step:38497 [D loss: 0.001327, acc.: 100.00%] [G loss: 0.001164]\n",
      "epoch:49 step:38498 [D loss: 0.008290, acc.: 100.00%] [G loss: 0.000425]\n",
      "epoch:49 step:38499 [D loss: 0.003575, acc.: 100.00%] [G loss: 0.000529]\n",
      "epoch:49 step:38500 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.001049]\n",
      "epoch:49 step:38501 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000451]\n",
      "epoch:49 step:38502 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.001993]\n",
      "epoch:49 step:38503 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.006288]\n",
      "epoch:49 step:38504 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:49 step:38505 [D loss: 0.001990, acc.: 100.00%] [G loss: 0.001459]\n",
      "epoch:49 step:38506 [D loss: 0.000136, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:49 step:38507 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000983]\n",
      "epoch:49 step:38508 [D loss: 0.004292, acc.: 100.00%] [G loss: 0.000754]\n",
      "epoch:49 step:38509 [D loss: 0.005254, acc.: 100.00%] [G loss: 0.002895]\n",
      "epoch:49 step:38510 [D loss: 0.000555, acc.: 100.00%] [G loss: 0.000223]\n",
      "epoch:49 step:38511 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.007271]\n",
      "epoch:49 step:38512 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000185]\n",
      "epoch:49 step:38513 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.004174]\n",
      "epoch:49 step:38514 [D loss: 0.000505, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:49 step:38515 [D loss: 0.000522, acc.: 100.00%] [G loss: 0.000251]\n",
      "epoch:49 step:38516 [D loss: 0.000506, acc.: 100.00%] [G loss: 0.000248]\n",
      "epoch:49 step:38517 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.001925]\n",
      "epoch:49 step:38518 [D loss: 0.000197, acc.: 100.00%] [G loss: 0.000339]\n",
      "epoch:49 step:38519 [D loss: 0.000452, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:49 step:38520 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.000123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38521 [D loss: 0.000189, acc.: 100.00%] [G loss: 0.000637]\n",
      "epoch:49 step:38522 [D loss: 0.000471, acc.: 100.00%] [G loss: 0.000393]\n",
      "epoch:49 step:38523 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:49 step:38524 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:49 step:38525 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000636]\n",
      "epoch:49 step:38526 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:49 step:38527 [D loss: 0.000581, acc.: 100.00%] [G loss: 0.004753]\n",
      "epoch:49 step:38528 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.000128]\n",
      "epoch:49 step:38529 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.000703]\n",
      "epoch:49 step:38530 [D loss: 0.001949, acc.: 100.00%] [G loss: 0.000078]\n",
      "epoch:49 step:38531 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:49 step:38532 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.003344]\n",
      "epoch:49 step:38533 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:49 step:38534 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.001340]\n",
      "epoch:49 step:38535 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:49 step:38536 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:49 step:38537 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:49 step:38538 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.000121]\n",
      "epoch:49 step:38539 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000030]\n",
      "epoch:49 step:38540 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:49 step:38541 [D loss: 0.000198, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:49 step:38542 [D loss: 0.000353, acc.: 100.00%] [G loss: 0.000089]\n",
      "epoch:49 step:38543 [D loss: 0.001852, acc.: 100.00%] [G loss: 0.000769]\n",
      "epoch:49 step:38544 [D loss: 0.000311, acc.: 100.00%] [G loss: 0.000191]\n",
      "epoch:49 step:38545 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000193]\n",
      "epoch:49 step:38546 [D loss: 0.001778, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:49 step:38547 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:49 step:38548 [D loss: 0.006669, acc.: 100.00%] [G loss: 0.002783]\n",
      "epoch:49 step:38549 [D loss: 0.011182, acc.: 100.00%] [G loss: 0.046484]\n",
      "epoch:49 step:38550 [D loss: 0.021404, acc.: 100.00%] [G loss: 0.001337]\n",
      "epoch:49 step:38551 [D loss: 0.006196, acc.: 100.00%] [G loss: 0.000726]\n",
      "epoch:49 step:38552 [D loss: 0.064808, acc.: 100.00%] [G loss: 0.017223]\n",
      "epoch:49 step:38553 [D loss: 0.003903, acc.: 100.00%] [G loss: 0.509913]\n",
      "epoch:49 step:38554 [D loss: 0.002475, acc.: 100.00%] [G loss: 1.959446]\n",
      "epoch:49 step:38555 [D loss: 0.121819, acc.: 95.31%] [G loss: 0.001936]\n",
      "epoch:49 step:38556 [D loss: 0.177770, acc.: 92.97%] [G loss: 0.918808]\n",
      "epoch:49 step:38557 [D loss: 0.025662, acc.: 100.00%] [G loss: 0.330445]\n",
      "epoch:49 step:38558 [D loss: 0.118533, acc.: 95.31%] [G loss: 0.336900]\n",
      "epoch:49 step:38559 [D loss: 0.003323, acc.: 100.00%] [G loss: 0.000373]\n",
      "epoch:49 step:38560 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.007657]\n",
      "epoch:49 step:38561 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.001592]\n",
      "epoch:49 step:38562 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001066]\n",
      "epoch:49 step:38563 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.001204]\n",
      "epoch:49 step:38564 [D loss: 0.000735, acc.: 100.00%] [G loss: 0.001054]\n",
      "epoch:49 step:38565 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.035374]\n",
      "epoch:49 step:38566 [D loss: 0.000223, acc.: 100.00%] [G loss: 0.001351]\n",
      "epoch:49 step:38567 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000952]\n",
      "epoch:49 step:38568 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.002030]\n",
      "epoch:49 step:38569 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.001574]\n",
      "epoch:49 step:38570 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.006062]\n",
      "epoch:49 step:38571 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000271]\n",
      "epoch:49 step:38572 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000585]\n",
      "epoch:49 step:38573 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.001978]\n",
      "epoch:49 step:38574 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.000593]\n",
      "epoch:49 step:38575 [D loss: 0.000783, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:49 step:38576 [D loss: 0.027794, acc.: 100.00%] [G loss: 0.003486]\n",
      "epoch:49 step:38577 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.007379]\n",
      "epoch:49 step:38578 [D loss: 0.000367, acc.: 100.00%] [G loss: 0.017513]\n",
      "epoch:49 step:38579 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.016961]\n",
      "epoch:49 step:38580 [D loss: 0.002185, acc.: 100.00%] [G loss: 0.020420]\n",
      "epoch:49 step:38581 [D loss: 0.002497, acc.: 100.00%] [G loss: 0.021871]\n",
      "epoch:49 step:38582 [D loss: 0.005955, acc.: 100.00%] [G loss: 0.008591]\n",
      "epoch:49 step:38583 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.002420]\n",
      "epoch:49 step:38584 [D loss: 0.066990, acc.: 97.66%] [G loss: 0.000262]\n",
      "epoch:49 step:38585 [D loss: 0.000253, acc.: 100.00%] [G loss: 0.001152]\n",
      "epoch:49 step:38586 [D loss: 0.000817, acc.: 100.00%] [G loss: 0.000098]\n",
      "epoch:49 step:38587 [D loss: 0.001161, acc.: 100.00%] [G loss: 0.000432]\n",
      "epoch:49 step:38588 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:49 step:38589 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.000046]\n",
      "epoch:49 step:38590 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000099]\n",
      "epoch:49 step:38591 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000198]\n",
      "epoch:49 step:38592 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000153]\n",
      "epoch:49 step:38593 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000113]\n",
      "epoch:49 step:38594 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:49 step:38595 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:49 step:38596 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000138]\n",
      "epoch:49 step:38597 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000253]\n",
      "epoch:49 step:38598 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000974]\n",
      "epoch:49 step:38599 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000026]\n",
      "epoch:49 step:38600 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000482]\n",
      "epoch:49 step:38601 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:49 step:38602 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:49 step:38603 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:49 step:38604 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000010]\n",
      "epoch:49 step:38605 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:49 step:38606 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000091]\n",
      "epoch:49 step:38607 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:49 step:38608 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000528]\n",
      "epoch:49 step:38609 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:49 step:38610 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:49 step:38611 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:49 step:38612 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:49 step:38613 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000049]\n",
      "epoch:49 step:38614 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:49 step:38615 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:49 step:38616 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:49 step:38617 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:49 step:38618 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:49 step:38619 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:49 step:38620 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000061]\n",
      "epoch:49 step:38621 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:49 step:38622 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.000068]\n",
      "epoch:49 step:38623 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:49 step:38624 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000647]\n",
      "epoch:49 step:38625 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:49 step:38626 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000130]\n",
      "epoch:49 step:38627 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000133]\n",
      "epoch:49 step:38628 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000122]\n",
      "epoch:49 step:38629 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:49 step:38630 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.000076]\n",
      "epoch:49 step:38631 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000126]\n",
      "epoch:49 step:38632 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000075]\n",
      "epoch:49 step:38633 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000097]\n",
      "epoch:49 step:38634 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:49 step:38635 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:49 step:38636 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38637 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:49 step:38638 [D loss: 0.000156, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:49 step:38639 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:49 step:38640 [D loss: 0.000435, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:49 step:38641 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000220]\n",
      "epoch:49 step:38642 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000031]\n",
      "epoch:49 step:38643 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001048]\n",
      "epoch:49 step:38644 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:49 step:38645 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:49 step:38646 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000144]\n",
      "epoch:49 step:38647 [D loss: 0.004612, acc.: 100.00%] [G loss: 0.007971]\n",
      "epoch:49 step:38648 [D loss: 0.001252, acc.: 100.00%] [G loss: 0.000083]\n",
      "epoch:49 step:38649 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:49 step:38650 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.000127]\n",
      "epoch:49 step:38651 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000161]\n",
      "epoch:49 step:38652 [D loss: 0.000664, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:49 step:38653 [D loss: 0.002843, acc.: 100.00%] [G loss: 0.000143]\n",
      "epoch:49 step:38654 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000109]\n",
      "epoch:49 step:38655 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:49 step:38656 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000170]\n",
      "epoch:49 step:38657 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000255]\n",
      "epoch:49 step:38658 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000146]\n",
      "epoch:49 step:38659 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000342]\n",
      "epoch:49 step:38660 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000211]\n",
      "epoch:49 step:38661 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000657]\n",
      "epoch:49 step:38662 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000338]\n",
      "epoch:49 step:38663 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000090]\n",
      "epoch:49 step:38664 [D loss: 0.000236, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:49 step:38665 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.001083]\n",
      "epoch:49 step:38666 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000259]\n",
      "epoch:49 step:38667 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000132]\n",
      "epoch:49 step:38668 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000081]\n",
      "epoch:49 step:38669 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000150]\n",
      "epoch:49 step:38670 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.000045]\n",
      "epoch:49 step:38671 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:49 step:38672 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000281]\n",
      "epoch:49 step:38673 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:49 step:38674 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000016]\n",
      "epoch:49 step:38675 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000329]\n",
      "epoch:49 step:38676 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:49 step:38677 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:49 step:38678 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.005999]\n",
      "epoch:49 step:38679 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:49 step:38680 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.000060]\n",
      "epoch:49 step:38681 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:49 step:38682 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.000033]\n",
      "epoch:49 step:38683 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.000200]\n",
      "epoch:49 step:38684 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000266]\n",
      "epoch:49 step:38685 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:49 step:38686 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:49 step:38687 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000064]\n",
      "epoch:49 step:38688 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:49 step:38689 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.002324]\n",
      "epoch:49 step:38690 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000274]\n",
      "epoch:49 step:38691 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:49 step:38692 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:49 step:38693 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000043]\n",
      "epoch:49 step:38694 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:49 step:38695 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.000155]\n",
      "epoch:49 step:38696 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000051]\n",
      "epoch:49 step:38697 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.000842]\n",
      "epoch:49 step:38698 [D loss: 0.000940, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:49 step:38699 [D loss: 0.001028, acc.: 100.00%] [G loss: 0.000160]\n",
      "epoch:49 step:38700 [D loss: 0.025885, acc.: 100.00%] [G loss: 0.001394]\n",
      "epoch:49 step:38701 [D loss: 0.001407, acc.: 100.00%] [G loss: 0.001749]\n",
      "epoch:49 step:38702 [D loss: 0.001507, acc.: 100.00%] [G loss: 0.004472]\n",
      "epoch:49 step:38703 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.004015]\n",
      "epoch:49 step:38704 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.003329]\n",
      "epoch:49 step:38705 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.005256]\n",
      "epoch:49 step:38706 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.004206]\n",
      "epoch:49 step:38707 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.094999]\n",
      "epoch:49 step:38708 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.001587]\n",
      "epoch:49 step:38709 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.006934]\n",
      "epoch:49 step:38710 [D loss: 0.004884, acc.: 100.00%] [G loss: 0.002210]\n",
      "epoch:49 step:38711 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.002332]\n",
      "epoch:49 step:38712 [D loss: 0.000244, acc.: 100.00%] [G loss: 0.057985]\n",
      "epoch:49 step:38713 [D loss: 0.004695, acc.: 100.00%] [G loss: 0.001737]\n",
      "epoch:49 step:38714 [D loss: 0.004257, acc.: 100.00%] [G loss: 0.005010]\n",
      "epoch:49 step:38715 [D loss: 0.000686, acc.: 100.00%] [G loss: 0.009978]\n",
      "epoch:49 step:38716 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.030468]\n",
      "epoch:49 step:38717 [D loss: 0.001287, acc.: 100.00%] [G loss: 0.000581]\n",
      "epoch:49 step:38718 [D loss: 0.001176, acc.: 100.00%] [G loss: 0.006976]\n",
      "epoch:49 step:38719 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.001756]\n",
      "epoch:49 step:38720 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.006205]\n",
      "epoch:49 step:38721 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.009015]\n",
      "epoch:49 step:38722 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.001387]\n",
      "epoch:49 step:38723 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.016673]\n",
      "epoch:49 step:38724 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.002453]\n",
      "epoch:49 step:38725 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.001719]\n",
      "epoch:49 step:38726 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000487]\n",
      "epoch:49 step:38727 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.004629]\n",
      "epoch:49 step:38728 [D loss: 0.000177, acc.: 100.00%] [G loss: 0.001901]\n",
      "epoch:49 step:38729 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.001731]\n",
      "epoch:49 step:38730 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000752]\n",
      "epoch:49 step:38731 [D loss: 0.000154, acc.: 100.00%] [G loss: 0.000862]\n",
      "epoch:49 step:38732 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.006568]\n",
      "epoch:49 step:38733 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.007233]\n",
      "epoch:49 step:38734 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000816]\n",
      "epoch:49 step:38735 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.005307]\n",
      "epoch:49 step:38736 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.001236]\n",
      "epoch:49 step:38737 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000772]\n",
      "epoch:49 step:38738 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000664]\n",
      "epoch:49 step:38739 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000533]\n",
      "epoch:49 step:38740 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.004028]\n",
      "epoch:49 step:38741 [D loss: 0.002703, acc.: 100.00%] [G loss: 0.000698]\n",
      "epoch:49 step:38742 [D loss: 0.000239, acc.: 100.00%] [G loss: 0.002152]\n",
      "epoch:49 step:38743 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.004117]\n",
      "epoch:49 step:38744 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000619]\n",
      "epoch:49 step:38745 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.000483]\n",
      "epoch:49 step:38746 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000925]\n",
      "epoch:49 step:38747 [D loss: 0.000187, acc.: 100.00%] [G loss: 0.000319]\n",
      "epoch:49 step:38748 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.000547]\n",
      "epoch:49 step:38749 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000574]\n",
      "epoch:49 step:38750 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.002176]\n",
      "epoch:49 step:38751 [D loss: 0.000190, acc.: 100.00%] [G loss: 0.000437]\n",
      "epoch:49 step:38752 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38753 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.001275]\n",
      "epoch:49 step:38754 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000522]\n",
      "epoch:49 step:38755 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.000648]\n",
      "epoch:49 step:38756 [D loss: 0.000201, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:49 step:38757 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.001605]\n",
      "epoch:49 step:38758 [D loss: 0.000556, acc.: 100.00%] [G loss: 0.000772]\n",
      "epoch:49 step:38759 [D loss: 0.000245, acc.: 100.00%] [G loss: 0.000548]\n",
      "epoch:49 step:38760 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000189]\n",
      "epoch:49 step:38761 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000292]\n",
      "epoch:49 step:38762 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:49 step:38763 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000754]\n",
      "epoch:49 step:38764 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.000443]\n",
      "epoch:49 step:38765 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000355]\n",
      "epoch:49 step:38766 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.000527]\n",
      "epoch:49 step:38767 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.000495]\n",
      "epoch:49 step:38768 [D loss: 0.000986, acc.: 100.00%] [G loss: 0.000480]\n",
      "epoch:49 step:38769 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000816]\n",
      "epoch:49 step:38770 [D loss: 0.000504, acc.: 100.00%] [G loss: 0.000827]\n",
      "epoch:49 step:38771 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.021198]\n",
      "epoch:49 step:38772 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:49 step:38773 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.000735]\n",
      "epoch:49 step:38774 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:49 step:38775 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.000333]\n",
      "epoch:49 step:38776 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.000433]\n",
      "epoch:49 step:38777 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000115]\n",
      "epoch:49 step:38778 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:49 step:38779 [D loss: 0.000338, acc.: 100.00%] [G loss: 0.000079]\n",
      "epoch:49 step:38780 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.000362]\n",
      "epoch:49 step:38781 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000210]\n",
      "epoch:49 step:38782 [D loss: 0.000559, acc.: 100.00%] [G loss: 0.000107]\n",
      "epoch:49 step:38783 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.001716]\n",
      "epoch:49 step:38784 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000716]\n",
      "epoch:49 step:38785 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000335]\n",
      "epoch:49 step:38786 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000411]\n",
      "epoch:49 step:38787 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000763]\n",
      "epoch:49 step:38788 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:49 step:38789 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000218]\n",
      "epoch:49 step:38790 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000888]\n",
      "epoch:49 step:38791 [D loss: 0.000131, acc.: 100.00%] [G loss: 0.000554]\n",
      "epoch:49 step:38792 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000164]\n",
      "epoch:49 step:38793 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.001288]\n",
      "epoch:49 step:38794 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000208]\n",
      "epoch:49 step:38795 [D loss: 0.011568, acc.: 100.00%] [G loss: 0.000186]\n",
      "epoch:49 step:38796 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:49 step:38797 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000041]\n",
      "epoch:49 step:38798 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000212]\n",
      "epoch:49 step:38799 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:49 step:38800 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:49 step:38801 [D loss: 0.000339, acc.: 100.00%] [G loss: 0.000020]\n",
      "epoch:49 step:38802 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:49 step:38803 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000022]\n",
      "epoch:49 step:38804 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:49 step:38805 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000032]\n",
      "epoch:49 step:38806 [D loss: 0.000246, acc.: 100.00%] [G loss: 0.000021]\n",
      "epoch:49 step:38807 [D loss: 0.000272, acc.: 100.00%] [G loss: 0.000005]\n",
      "epoch:49 step:38808 [D loss: 0.000524, acc.: 100.00%] [G loss: 0.000040]\n",
      "epoch:49 step:38809 [D loss: 0.000285, acc.: 100.00%] [G loss: 0.000047]\n",
      "epoch:49 step:38810 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.000013]\n",
      "epoch:49 step:38811 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.000062]\n",
      "epoch:49 step:38812 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000054]\n",
      "epoch:49 step:38813 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000096]\n",
      "epoch:49 step:38814 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:49 step:38815 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000042]\n",
      "epoch:49 step:38816 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:49 step:38817 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:49 step:38818 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000004]\n",
      "epoch:49 step:38819 [D loss: 0.000387, acc.: 100.00%] [G loss: 0.000048]\n",
      "epoch:49 step:38820 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000069]\n",
      "epoch:49 step:38821 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:49 step:38822 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38823 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:49 step:38824 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000104]\n",
      "epoch:49 step:38825 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000337]\n",
      "epoch:49 step:38826 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:49 step:38827 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000057]\n",
      "epoch:49 step:38828 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000180]\n",
      "epoch:49 step:38829 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000028]\n",
      "epoch:49 step:38830 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000226]\n",
      "epoch:49 step:38831 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000019]\n",
      "epoch:49 step:38832 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000765]\n",
      "epoch:49 step:38833 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000011]\n",
      "epoch:49 step:38834 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000012]\n",
      "epoch:49 step:38835 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.000052]\n",
      "epoch:49 step:38836 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.000070]\n",
      "epoch:49 step:38837 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000027]\n",
      "epoch:49 step:38838 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000050]\n",
      "epoch:49 step:38839 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:49 step:38840 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.000034]\n",
      "epoch:49 step:38841 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000037]\n",
      "epoch:49 step:38842 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.000006]\n",
      "epoch:49 step:38843 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000029]\n",
      "epoch:49 step:38844 [D loss: 0.009327, acc.: 100.00%] [G loss: 0.000018]\n",
      "epoch:49 step:38845 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.000262]\n",
      "epoch:49 step:38846 [D loss: 0.002425, acc.: 100.00%] [G loss: 0.001977]\n",
      "epoch:49 step:38847 [D loss: 0.043281, acc.: 100.00%] [G loss: 0.020983]\n",
      "epoch:49 step:38848 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.137508]\n",
      "epoch:49 step:38849 [D loss: 0.040793, acc.: 99.22%] [G loss: 0.020018]\n",
      "epoch:49 step:38850 [D loss: 0.013165, acc.: 99.22%] [G loss: 0.001220]\n",
      "epoch:49 step:38851 [D loss: 0.001113, acc.: 100.00%] [G loss: 0.170820]\n",
      "epoch:49 step:38852 [D loss: 0.000593, acc.: 100.00%] [G loss: 0.035430]\n",
      "epoch:49 step:38853 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.026115]\n",
      "epoch:49 step:38854 [D loss: 0.000783, acc.: 100.00%] [G loss: 0.000307]\n",
      "epoch:49 step:38855 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.000445]\n",
      "epoch:49 step:38856 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.005839]\n",
      "epoch:49 step:38857 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.000437]\n",
      "epoch:49 step:38858 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000388]\n",
      "epoch:49 step:38859 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.000085]\n",
      "epoch:49 step:38860 [D loss: 0.000160, acc.: 100.00%] [G loss: 0.000154]\n",
      "epoch:49 step:38861 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.000112]\n",
      "epoch:49 step:38862 [D loss: 0.003724, acc.: 100.00%] [G loss: 0.000405]\n",
      "epoch:49 step:38863 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.000059]\n",
      "epoch:49 step:38864 [D loss: 0.002335, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:49 step:38865 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.000036]\n",
      "epoch:49 step:38866 [D loss: 0.000450, acc.: 100.00%] [G loss: 0.000214]\n",
      "epoch:49 step:38867 [D loss: 0.008713, acc.: 99.22%] [G loss: 0.000086]\n",
      "epoch:49 step:38868 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.000226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38869 [D loss: 0.001509, acc.: 100.00%] [G loss: 0.001300]\n",
      "epoch:49 step:38870 [D loss: 0.004536, acc.: 100.00%] [G loss: 0.000820]\n",
      "epoch:49 step:38871 [D loss: 0.013978, acc.: 100.00%] [G loss: 0.066206]\n",
      "epoch:49 step:38872 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.245754]\n",
      "epoch:49 step:38873 [D loss: 0.014051, acc.: 100.00%] [G loss: 0.032070]\n",
      "epoch:49 step:38874 [D loss: 0.001442, acc.: 100.00%] [G loss: 0.000476]\n",
      "epoch:49 step:38875 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.000560]\n",
      "epoch:49 step:38876 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.057442]\n",
      "epoch:49 step:38877 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.003446]\n",
      "epoch:49 step:38878 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.030849]\n",
      "epoch:49 step:38879 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.000936]\n",
      "epoch:49 step:38880 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.001335]\n",
      "epoch:49 step:38881 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.000374]\n",
      "epoch:49 step:38882 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.000231]\n",
      "epoch:49 step:38883 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:49 step:38884 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.004227]\n",
      "epoch:49 step:38885 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.004927]\n",
      "epoch:49 step:38886 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.000467]\n",
      "epoch:49 step:38887 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.000230]\n",
      "epoch:49 step:38888 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.001634]\n",
      "epoch:49 step:38889 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.002582]\n",
      "epoch:49 step:38890 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000728]\n",
      "epoch:49 step:38891 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.011548]\n",
      "epoch:49 step:38892 [D loss: 0.004047, acc.: 100.00%] [G loss: 0.000945]\n",
      "epoch:49 step:38893 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000545]\n",
      "epoch:49 step:38894 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.000167]\n",
      "epoch:49 step:38895 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.000623]\n",
      "epoch:49 step:38896 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.000163]\n",
      "epoch:49 step:38897 [D loss: 0.000668, acc.: 100.00%] [G loss: 0.002077]\n",
      "epoch:49 step:38898 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.000427]\n",
      "epoch:49 step:38899 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.001673]\n",
      "epoch:49 step:38900 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000260]\n",
      "epoch:49 step:38901 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.000356]\n",
      "epoch:49 step:38902 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.002365]\n",
      "epoch:49 step:38903 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.004037]\n",
      "epoch:49 step:38904 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.000039]\n",
      "epoch:49 step:38905 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000174]\n",
      "epoch:49 step:38906 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.000110]\n",
      "epoch:49 step:38907 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000056]\n",
      "epoch:49 step:38908 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.000158]\n",
      "epoch:49 step:38909 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.000419]\n",
      "epoch:49 step:38910 [D loss: 0.000129, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:49 step:38911 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.000119]\n",
      "epoch:49 step:38912 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000071]\n",
      "epoch:49 step:38913 [D loss: 0.000199, acc.: 100.00%] [G loss: 0.000120]\n",
      "epoch:49 step:38914 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.000270]\n",
      "epoch:49 step:38915 [D loss: 0.000376, acc.: 100.00%] [G loss: 0.000151]\n",
      "epoch:49 step:38916 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.000055]\n",
      "epoch:49 step:38917 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000145]\n",
      "epoch:49 step:38918 [D loss: 0.000169, acc.: 100.00%] [G loss: 0.000086]\n",
      "epoch:49 step:38919 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.000035]\n",
      "epoch:49 step:38920 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.000518]\n",
      "epoch:49 step:38921 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.000306]\n",
      "epoch:49 step:38922 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:49 step:38923 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.000088]\n",
      "epoch:49 step:38924 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.000175]\n",
      "epoch:49 step:38925 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.000101]\n",
      "epoch:49 step:38926 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.004240]\n",
      "epoch:49 step:38927 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:49 step:38928 [D loss: 0.000358, acc.: 100.00%] [G loss: 0.001463]\n",
      "epoch:49 step:38929 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.000213]\n",
      "epoch:49 step:38930 [D loss: 0.004842, acc.: 100.00%] [G loss: 0.000181]\n",
      "epoch:49 step:38931 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.000378]\n",
      "epoch:49 step:38932 [D loss: 0.000603, acc.: 100.00%] [G loss: 0.000341]\n",
      "epoch:49 step:38933 [D loss: 0.001756, acc.: 100.00%] [G loss: 0.000219]\n",
      "epoch:49 step:38934 [D loss: 0.003053, acc.: 100.00%] [G loss: 0.000105]\n",
      "epoch:49 step:38935 [D loss: 0.305026, acc.: 82.81%] [G loss: 7.907489]\n",
      "epoch:49 step:38936 [D loss: 0.942789, acc.: 65.62%] [G loss: 1.487880]\n",
      "epoch:49 step:38937 [D loss: 0.247094, acc.: 86.72%] [G loss: 2.067900]\n",
      "epoch:49 step:38938 [D loss: 0.015980, acc.: 100.00%] [G loss: 1.777736]\n",
      "epoch:49 step:38939 [D loss: 0.151867, acc.: 95.31%] [G loss: 0.439145]\n",
      "epoch:49 step:38940 [D loss: 0.003168, acc.: 100.00%] [G loss: 0.241300]\n",
      "epoch:49 step:38941 [D loss: 0.001518, acc.: 100.00%] [G loss: 0.059268]\n",
      "epoch:49 step:38942 [D loss: 0.005765, acc.: 100.00%] [G loss: 0.068732]\n",
      "epoch:49 step:38943 [D loss: 0.114835, acc.: 92.97%] [G loss: 1.135072]\n",
      "epoch:49 step:38944 [D loss: 0.005131, acc.: 100.00%] [G loss: 2.171148]\n",
      "epoch:49 step:38945 [D loss: 0.178578, acc.: 93.75%] [G loss: 2.863002]\n",
      "epoch:49 step:38946 [D loss: 0.302002, acc.: 88.28%] [G loss: 4.497988]\n",
      "epoch:49 step:38947 [D loss: 0.079708, acc.: 96.09%] [G loss: 5.029376]\n",
      "epoch:49 step:38948 [D loss: 0.047255, acc.: 99.22%] [G loss: 0.638783]\n",
      "epoch:49 step:38949 [D loss: 0.066217, acc.: 96.88%] [G loss: 2.091712]\n",
      "epoch:49 step:38950 [D loss: 0.004909, acc.: 100.00%] [G loss: 1.553025]\n",
      "epoch:49 step:38951 [D loss: 0.016886, acc.: 99.22%] [G loss: 0.716862]\n",
      "epoch:49 step:38952 [D loss: 0.053390, acc.: 99.22%] [G loss: 1.180694]\n",
      "epoch:49 step:38953 [D loss: 0.020479, acc.: 100.00%] [G loss: 1.353312]\n",
      "epoch:49 step:38954 [D loss: 0.078719, acc.: 96.88%] [G loss: 1.340777]\n",
      "epoch:49 step:38955 [D loss: 0.067281, acc.: 99.22%] [G loss: 2.550180]\n",
      "epoch:49 step:38956 [D loss: 0.023246, acc.: 100.00%] [G loss: 3.116564]\n",
      "epoch:49 step:38957 [D loss: 0.011335, acc.: 100.00%] [G loss: 4.667818]\n",
      "epoch:49 step:38958 [D loss: 0.021924, acc.: 100.00%] [G loss: 4.178659]\n",
      "epoch:49 step:38959 [D loss: 0.027690, acc.: 100.00%] [G loss: 4.392089]\n",
      "epoch:49 step:38960 [D loss: 0.010424, acc.: 100.00%] [G loss: 4.630923]\n",
      "epoch:49 step:38961 [D loss: 0.088015, acc.: 96.88%] [G loss: 0.040123]\n",
      "epoch:49 step:38962 [D loss: 0.001998, acc.: 100.00%] [G loss: 5.408667]\n",
      "epoch:49 step:38963 [D loss: 0.002165, acc.: 100.00%] [G loss: 4.590332]\n",
      "epoch:49 step:38964 [D loss: 0.014979, acc.: 99.22%] [G loss: 4.530819]\n",
      "epoch:49 step:38965 [D loss: 0.024505, acc.: 100.00%] [G loss: 0.100406]\n",
      "epoch:49 step:38966 [D loss: 0.044747, acc.: 99.22%] [G loss: 10.438015]\n",
      "epoch:49 step:38967 [D loss: 0.006655, acc.: 100.00%] [G loss: 8.937438]\n",
      "epoch:49 step:38968 [D loss: 0.149740, acc.: 93.75%] [G loss: 0.477947]\n",
      "epoch:49 step:38969 [D loss: 0.002381, acc.: 100.00%] [G loss: 8.397774]\n",
      "epoch:49 step:38970 [D loss: 0.002288, acc.: 100.00%] [G loss: 6.796459]\n",
      "epoch:49 step:38971 [D loss: 0.003627, acc.: 100.00%] [G loss: 4.821642]\n",
      "epoch:49 step:38972 [D loss: 0.009125, acc.: 99.22%] [G loss: 4.498459]\n",
      "epoch:49 step:38973 [D loss: 0.118671, acc.: 96.88%] [G loss: 0.004375]\n",
      "epoch:49 step:38974 [D loss: 0.001071, acc.: 100.00%] [G loss: 0.002265]\n",
      "epoch:49 step:38975 [D loss: 0.002423, acc.: 100.00%] [G loss: 2.194248]\n",
      "epoch:49 step:38976 [D loss: 0.006959, acc.: 100.00%] [G loss: 0.387775]\n",
      "epoch:49 step:38977 [D loss: 0.002643, acc.: 100.00%] [G loss: 0.007457]\n",
      "epoch:49 step:38978 [D loss: 0.000585, acc.: 100.00%] [G loss: 0.026628]\n",
      "epoch:49 step:38979 [D loss: 0.016567, acc.: 99.22%] [G loss: 0.655974]\n",
      "epoch:49 step:38980 [D loss: 0.081115, acc.: 97.66%] [G loss: 1.510393]\n",
      "epoch:49 step:38981 [D loss: 0.001298, acc.: 100.00%] [G loss: 2.214638]\n",
      "epoch:49 step:38982 [D loss: 0.685687, acc.: 71.88%] [G loss: 0.000000]\n",
      "epoch:49 step:38983 [D loss: 0.082979, acc.: 97.66%] [G loss: 0.000000]\n",
      "epoch:49 step:38984 [D loss: 0.002386, acc.: 100.00%] [G loss: 0.013860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:49 step:38985 [D loss: 0.020560, acc.: 99.22%] [G loss: 0.001927]\n",
      "epoch:49 step:38986 [D loss: 0.025735, acc.: 100.00%] [G loss: 0.000137]\n",
      "epoch:49 step:38987 [D loss: 0.001449, acc.: 100.00%] [G loss: 0.002807]\n",
      "epoch:49 step:38988 [D loss: 0.000594, acc.: 100.00%] [G loss: 0.000302]\n",
      "epoch:49 step:38989 [D loss: 0.043789, acc.: 99.22%] [G loss: 0.000067]\n",
      "epoch:49 step:38990 [D loss: 0.000654, acc.: 100.00%] [G loss: 0.000023]\n",
      "epoch:49 step:38991 [D loss: 0.001069, acc.: 100.00%] [G loss: 0.000007]\n",
      "epoch:49 step:38992 [D loss: 0.000537, acc.: 100.00%] [G loss: 0.000008]\n",
      "epoch:49 step:38993 [D loss: 0.105021, acc.: 96.09%] [G loss: 0.143792]\n",
      "epoch:49 step:38994 [D loss: 0.010640, acc.: 100.00%] [G loss: 0.833050]\n",
      "epoch:49 step:38995 [D loss: 0.010526, acc.: 99.22%] [G loss: 0.120712]\n",
      "epoch:49 step:38996 [D loss: 0.025748, acc.: 98.44%] [G loss: 0.044934]\n",
      "epoch:49 step:38997 [D loss: 0.007924, acc.: 100.00%] [G loss: 1.031363]\n",
      "epoch:49 step:38998 [D loss: 0.000184, acc.: 100.00%] [G loss: 0.000335]\n",
      "epoch:49 step:38999 [D loss: 0.000179, acc.: 100.00%] [G loss: 0.002818]\n",
      "epoch:49 step:39000 [D loss: 0.003848, acc.: 100.00%] [G loss: 0.008904]\n",
      "epoch:49 step:39001 [D loss: 0.001191, acc.: 100.00%] [G loss: 0.076705]\n",
      "epoch:49 step:39002 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.008301]\n",
      "epoch:49 step:39003 [D loss: 0.000480, acc.: 100.00%] [G loss: 0.460391]\n",
      "epoch:49 step:39004 [D loss: 0.000664, acc.: 100.00%] [G loss: 0.000404]\n",
      "epoch:49 step:39005 [D loss: 0.001653, acc.: 100.00%] [G loss: 0.001000]\n",
      "epoch:49 step:39006 [D loss: 0.002817, acc.: 100.00%] [G loss: 0.005725]\n",
      "epoch:49 step:39007 [D loss: 0.001796, acc.: 100.00%] [G loss: 0.009410]\n",
      "epoch:49 step:39008 [D loss: 0.095219, acc.: 98.44%] [G loss: 1.103246]\n",
      "epoch:49 step:39009 [D loss: 0.023842, acc.: 99.22%] [G loss: 2.828116]\n",
      "epoch:49 step:39010 [D loss: 6.055257, acc.: 18.75%] [G loss: 8.901972]\n",
      "epoch:49 step:39011 [D loss: 0.310055, acc.: 89.06%] [G loss: 7.288294]\n",
      "epoch:49 step:39012 [D loss: 0.875165, acc.: 64.06%] [G loss: 2.805696]\n",
      "epoch:49 step:39013 [D loss: 0.145269, acc.: 94.53%] [G loss: 0.214982]\n",
      "epoch:49 step:39014 [D loss: 0.022533, acc.: 100.00%] [G loss: 0.197438]\n",
      "epoch:49 step:39015 [D loss: 0.047268, acc.: 98.44%] [G loss: 2.200916]\n",
      "epoch:49 step:39016 [D loss: 0.002019, acc.: 100.00%] [G loss: 1.813787]\n",
      "epoch:49 step:39017 [D loss: 0.002282, acc.: 100.00%] [G loss: 0.029224]\n",
      "epoch:49 step:39018 [D loss: 0.028402, acc.: 98.44%] [G loss: 0.688516]\n",
      "epoch:49 step:39019 [D loss: 0.022315, acc.: 99.22%] [G loss: 0.238864]\n",
      "epoch:49 step:39020 [D loss: 0.015731, acc.: 99.22%] [G loss: 0.076428]\n",
      "epoch:49 step:39021 [D loss: 0.009111, acc.: 100.00%] [G loss: 0.035853]\n",
      "epoch:49 step:39022 [D loss: 0.014453, acc.: 99.22%] [G loss: 0.010685]\n",
      "epoch:49 step:39023 [D loss: 0.136705, acc.: 94.53%] [G loss: 0.414546]\n",
      "epoch:49 step:39024 [D loss: 0.001968, acc.: 100.00%] [G loss: 0.284804]\n",
      "epoch:49 step:39025 [D loss: 0.036646, acc.: 98.44%] [G loss: 0.735067]\n",
      "epoch:49 step:39026 [D loss: 0.018719, acc.: 99.22%] [G loss: 0.200293]\n",
      "epoch:49 step:39027 [D loss: 0.068736, acc.: 97.66%] [G loss: 0.046121]\n",
      "epoch:49 step:39028 [D loss: 0.002330, acc.: 100.00%] [G loss: 0.043132]\n",
      "epoch:49 step:39029 [D loss: 0.008639, acc.: 100.00%] [G loss: 0.010116]\n",
      "epoch:49 step:39030 [D loss: 0.053332, acc.: 99.22%] [G loss: 0.028430]\n",
      "epoch:49 step:39031 [D loss: 0.004095, acc.: 100.00%] [G loss: 0.016895]\n",
      "epoch:49 step:39032 [D loss: 0.020801, acc.: 99.22%] [G loss: 0.008842]\n",
      "epoch:49 step:39033 [D loss: 0.018393, acc.: 100.00%] [G loss: 0.030919]\n",
      "epoch:49 step:39034 [D loss: 0.006984, acc.: 100.00%] [G loss: 0.055521]\n",
      "epoch:49 step:39035 [D loss: 0.008945, acc.: 100.00%] [G loss: 0.007715]\n",
      "epoch:49 step:39036 [D loss: 0.000973, acc.: 100.00%] [G loss: 0.007322]\n",
      "epoch:49 step:39037 [D loss: 0.004107, acc.: 100.00%] [G loss: 0.045687]\n",
      "epoch:49 step:39038 [D loss: 0.038094, acc.: 100.00%] [G loss: 0.046056]\n",
      "epoch:49 step:39039 [D loss: 0.001347, acc.: 100.00%] [G loss: 0.050713]\n",
      "epoch:49 step:39040 [D loss: 0.137237, acc.: 94.53%] [G loss: 0.003247]\n",
      "epoch:49 step:39041 [D loss: 0.870111, acc.: 59.38%] [G loss: 3.186743]\n",
      "epoch:49 step:39042 [D loss: 0.768285, acc.: 67.97%] [G loss: 2.223273]\n",
      "epoch:49 step:39043 [D loss: 0.010369, acc.: 100.00%] [G loss: 1.619098]\n",
      "epoch:49 step:39044 [D loss: 0.024932, acc.: 99.22%] [G loss: 0.076602]\n",
      "epoch:49 step:39045 [D loss: 0.105102, acc.: 96.09%] [G loss: 1.488203]\n",
      "epoch:49 step:39046 [D loss: 0.015500, acc.: 100.00%] [G loss: 1.109282]\n",
      "epoch:49 step:39047 [D loss: 0.053619, acc.: 97.66%] [G loss: 0.391994]\n",
      "epoch:49 step:39048 [D loss: 0.013077, acc.: 100.00%] [G loss: 0.279273]\n",
      "epoch:49 step:39049 [D loss: 0.021273, acc.: 99.22%] [G loss: 0.192464]\n",
      "epoch:49 step:39050 [D loss: 0.029169, acc.: 99.22%] [G loss: 0.376603]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import fashion_mnist,cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Conv2DTranspose\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers import Concatenate, GaussianNoise,Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "class CGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 32\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 10\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,))\n",
    "        img = self.generator([noise, label])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        valid = self.discriminator([img, label])\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(2 * 2 * 512, activation='relu',input_dim=self.latent_dim))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Reshape((2, 2, 512)))\n",
    "\n",
    "        model.add(Conv2DTranspose(256, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "        model.add(Conv2DTranspose(128, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "        model.add(Conv2DTranspose(64, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add( BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "        model.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same', activation='tanh'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)\n",
    "    def build_discriminator(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Conv 1: 16x16x64\n",
    "        model.add(Conv2D(64, kernel_size=5, strides=2, padding='same' ,input_shape=self.img_shape))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "        # Conv 2:\n",
    "        model.add(Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "        # Conv 3:\n",
    "        model.add(Conv2D(256, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add( BatchNormalization(momentum=0.9))\n",
    "        model.add( LeakyReLU(alpha=0.1))\n",
    "\n",
    "        # Conv 4:\n",
    "        model.add(Conv2D(512, kernel_size=5, strides=2, padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.9))\n",
    "        model.add( LeakyReLU(alpha=0.1))\n",
    "        model.summary()\n",
    "\n",
    "        # FC\n",
    "        model.add(Flatten())\n",
    "        img = Input(shape=self.img_shape)\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        labels = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "        discriminator =model(img)\n",
    "\n",
    "        # Concatenate\n",
    "        merged_layer = Concatenate()([discriminator, labels])\n",
    "        discriminator = Dense(512, activation='relu')(merged_layer)\n",
    "\n",
    "        # Output\n",
    "        discriminator = Dense(1, activation='sigmoid')(discriminator)\n",
    "\n",
    "        return Model([img, label], discriminator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (_, _) = cifar10.load_data()\n",
    "\n",
    "        # Configure input\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        # X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                labels = y_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "                # Generate a half batch of new images\n",
    "                gen_imgs = self.generator.predict([noise, labels])\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on labels\n",
    "                sampled_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
    "\n",
    "                # Train the generator\n",
    "                g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch,global_step, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if  global_step % sample_interval == 0:\n",
    "                    self.sample_images(epoch,global_step)\n",
    "\n",
    "\n",
    "    def sample_images(self, epoch,global_step):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        generated_images = self.generator.predict([noise, sampled_labels])\n",
    "\n",
    "\n",
    "        generated_images = np.asarray((generated_images * 127.5 + 127.5).astype(np.uint8))\n",
    "\n",
    "        def vis_square(data, padsize=1, padval=0):\n",
    "            # force the number of filters to be square\n",
    "            n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "            padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "            data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
    "\n",
    "            # tile the filters into an image\n",
    "            data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "            data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "            return data\n",
    "\n",
    "        img = vis_square(generated_images)\n",
    "        if not os.path.isdir('images_cgan_cifar10'):\n",
    "            os.mkdir('images_cgan_cifar10')\n",
    "        Image.fromarray(img).save(\n",
    "            \"images_cgan_cifar10/epoch_%d_step_%d.png\" % (epoch, global_step))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cgan = CGAN()\n",
    "    cgan.train(epochs=50, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
