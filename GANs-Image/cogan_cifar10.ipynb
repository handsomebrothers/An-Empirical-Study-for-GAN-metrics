{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "=================================================================\n",
      "Total params: 160,512\n",
      "Trainable params: 158,976\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "epoch:0 step:1 [D1 loss: 0.797631, acc.: 15.62%] [D2 loss: 0.743604, acc.: 28.91%] [G loss: 1.154930]\n",
      "epoch:0 step:2 [D1 loss: 0.707435, acc.: 49.22%] [D2 loss: 0.844839, acc.: 42.19%] [G loss: 1.180272]\n",
      "epoch:0 step:3 [D1 loss: 0.682236, acc.: 50.00%] [D2 loss: 0.774728, acc.: 47.66%] [G loss: 1.390672]\n",
      "epoch:0 step:4 [D1 loss: 0.614179, acc.: 52.34%] [D2 loss: 0.618658, acc.: 53.91%] [G loss: 1.635339]\n",
      "epoch:0 step:5 [D1 loss: 0.507455, acc.: 68.75%] [D2 loss: 0.558418, acc.: 65.62%] [G loss: 1.741699]\n",
      "epoch:0 step:6 [D1 loss: 0.509766, acc.: 70.31%] [D2 loss: 0.544215, acc.: 65.62%] [G loss: 1.964562]\n",
      "epoch:0 step:7 [D1 loss: 0.602284, acc.: 57.03%] [D2 loss: 0.536873, acc.: 64.84%] [G loss: 2.072633]\n",
      "epoch:0 step:8 [D1 loss: 0.539166, acc.: 67.97%] [D2 loss: 0.441998, acc.: 75.78%] [G loss: 2.295834]\n",
      "epoch:0 step:9 [D1 loss: 0.523450, acc.: 75.00%] [D2 loss: 0.524866, acc.: 71.09%] [G loss: 2.450255]\n",
      "epoch:0 step:10 [D1 loss: 0.464497, acc.: 72.66%] [D2 loss: 0.533414, acc.: 67.19%] [G loss: 2.447492]\n",
      "epoch:0 step:11 [D1 loss: 0.446328, acc.: 78.12%] [D2 loss: 0.425527, acc.: 78.91%] [G loss: 2.595342]\n",
      "epoch:0 step:12 [D1 loss: 0.501712, acc.: 74.22%] [D2 loss: 0.494850, acc.: 74.22%] [G loss: 2.664411]\n",
      "epoch:0 step:13 [D1 loss: 0.465902, acc.: 80.47%] [D2 loss: 0.414840, acc.: 81.25%] [G loss: 2.806785]\n",
      "epoch:0 step:14 [D1 loss: 0.462878, acc.: 82.81%] [D2 loss: 0.468399, acc.: 78.12%] [G loss: 2.759529]\n",
      "epoch:0 step:15 [D1 loss: 0.370954, acc.: 85.16%] [D2 loss: 0.463934, acc.: 79.69%] [G loss: 3.099647]\n",
      "epoch:0 step:16 [D1 loss: 0.380788, acc.: 84.38%] [D2 loss: 0.453265, acc.: 76.56%] [G loss: 3.053623]\n",
      "epoch:0 step:17 [D1 loss: 0.442839, acc.: 78.12%] [D2 loss: 0.509736, acc.: 75.78%] [G loss: 3.088822]\n",
      "epoch:0 step:18 [D1 loss: 0.390887, acc.: 84.38%] [D2 loss: 0.419891, acc.: 85.16%] [G loss: 3.420851]\n",
      "epoch:0 step:19 [D1 loss: 0.435647, acc.: 83.59%] [D2 loss: 0.448147, acc.: 80.47%] [G loss: 3.284331]\n",
      "epoch:0 step:20 [D1 loss: 0.514791, acc.: 79.69%] [D2 loss: 0.393203, acc.: 85.16%] [G loss: 3.494325]\n",
      "epoch:0 step:21 [D1 loss: 0.662975, acc.: 64.84%] [D2 loss: 0.391228, acc.: 85.94%] [G loss: 3.657592]\n",
      "epoch:0 step:22 [D1 loss: 0.524371, acc.: 74.22%] [D2 loss: 0.492429, acc.: 81.25%] [G loss: 3.863214]\n",
      "epoch:0 step:23 [D1 loss: 0.532081, acc.: 75.78%] [D2 loss: 0.383458, acc.: 86.72%] [G loss: 4.257764]\n",
      "epoch:0 step:24 [D1 loss: 0.708867, acc.: 72.66%] [D2 loss: 0.412897, acc.: 85.94%] [G loss: 3.923439]\n",
      "epoch:0 step:25 [D1 loss: 0.759498, acc.: 63.28%] [D2 loss: 0.405780, acc.: 82.03%] [G loss: 3.953991]\n",
      "epoch:0 step:26 [D1 loss: 0.532830, acc.: 70.31%] [D2 loss: 0.674693, acc.: 66.41%] [G loss: 4.674666]\n",
      "epoch:0 step:27 [D1 loss: 0.608121, acc.: 74.22%] [D2 loss: 0.525504, acc.: 77.34%] [G loss: 3.924856]\n",
      "epoch:0 step:28 [D1 loss: 0.740576, acc.: 62.50%] [D2 loss: 0.534940, acc.: 76.56%] [G loss: 4.022490]\n",
      "epoch:0 step:29 [D1 loss: 0.512015, acc.: 73.44%] [D2 loss: 0.537266, acc.: 73.44%] [G loss: 4.689020]\n",
      "epoch:0 step:30 [D1 loss: 0.715050, acc.: 67.97%] [D2 loss: 0.528442, acc.: 73.44%] [G loss: 5.286940]\n",
      "epoch:0 step:31 [D1 loss: 1.293851, acc.: 52.34%] [D2 loss: 0.865599, acc.: 63.28%] [G loss: 3.868503]\n",
      "epoch:0 step:32 [D1 loss: 0.750142, acc.: 67.97%] [D2 loss: 0.496296, acc.: 77.34%] [G loss: 3.922072]\n",
      "epoch:0 step:33 [D1 loss: 0.586092, acc.: 66.41%] [D2 loss: 0.503252, acc.: 75.78%] [G loss: 3.690863]\n",
      "epoch:0 step:34 [D1 loss: 0.464624, acc.: 77.34%] [D2 loss: 0.537944, acc.: 71.09%] [G loss: 4.007651]\n",
      "epoch:0 step:35 [D1 loss: 0.618997, acc.: 73.44%] [D2 loss: 0.513246, acc.: 72.66%] [G loss: 4.135315]\n",
      "epoch:0 step:36 [D1 loss: 0.654790, acc.: 71.88%] [D2 loss: 0.474123, acc.: 80.47%] [G loss: 3.839257]\n",
      "epoch:0 step:37 [D1 loss: 0.572467, acc.: 72.66%] [D2 loss: 0.396732, acc.: 85.94%] [G loss: 4.144024]\n",
      "epoch:0 step:38 [D1 loss: 0.514610, acc.: 75.00%] [D2 loss: 0.396183, acc.: 82.81%] [G loss: 4.138200]\n",
      "epoch:0 step:39 [D1 loss: 0.549299, acc.: 71.09%] [D2 loss: 0.364525, acc.: 87.50%] [G loss: 4.227520]\n",
      "epoch:0 step:40 [D1 loss: 0.504786, acc.: 76.56%] [D2 loss: 0.418800, acc.: 85.16%] [G loss: 4.178510]\n",
      "epoch:0 step:41 [D1 loss: 0.480069, acc.: 74.22%] [D2 loss: 0.544853, acc.: 75.00%] [G loss: 4.073905]\n",
      "epoch:0 step:42 [D1 loss: 0.530851, acc.: 69.53%] [D2 loss: 0.462635, acc.: 77.34%] [G loss: 4.163458]\n",
      "epoch:0 step:43 [D1 loss: 0.530241, acc.: 75.00%] [D2 loss: 0.496550, acc.: 75.78%] [G loss: 4.521806]\n",
      "epoch:0 step:44 [D1 loss: 0.874240, acc.: 58.59%] [D2 loss: 0.721661, acc.: 70.31%] [G loss: 5.055659]\n",
      "epoch:0 step:45 [D1 loss: 1.210787, acc.: 59.38%] [D2 loss: 0.668311, acc.: 68.75%] [G loss: 4.527479]\n",
      "epoch:0 step:46 [D1 loss: 1.148785, acc.: 60.16%] [D2 loss: 0.776757, acc.: 64.84%] [G loss: 4.001520]\n",
      "epoch:0 step:47 [D1 loss: 0.832812, acc.: 60.16%] [D2 loss: 0.600272, acc.: 71.09%] [G loss: 3.842905]\n",
      "epoch:0 step:48 [D1 loss: 0.666973, acc.: 61.72%] [D2 loss: 0.574160, acc.: 72.66%] [G loss: 4.263927]\n",
      "epoch:0 step:49 [D1 loss: 0.765526, acc.: 60.16%] [D2 loss: 0.592171, acc.: 71.09%] [G loss: 3.769740]\n",
      "epoch:0 step:50 [D1 loss: 0.723812, acc.: 67.19%] [D2 loss: 0.485909, acc.: 74.22%] [G loss: 4.015285]\n",
      "epoch:0 step:51 [D1 loss: 0.714491, acc.: 69.53%] [D2 loss: 0.477843, acc.: 78.12%] [G loss: 4.038107]\n",
      "epoch:0 step:52 [D1 loss: 0.599168, acc.: 67.19%] [D2 loss: 0.733321, acc.: 61.72%] [G loss: 4.427517]\n",
      "epoch:0 step:53 [D1 loss: 0.694648, acc.: 67.19%] [D2 loss: 0.536930, acc.: 71.88%] [G loss: 4.275908]\n",
      "epoch:0 step:54 [D1 loss: 0.717539, acc.: 61.72%] [D2 loss: 0.539574, acc.: 74.22%] [G loss: 4.785406]\n",
      "epoch:0 step:55 [D1 loss: 0.797187, acc.: 60.16%] [D2 loss: 0.533288, acc.: 74.22%] [G loss: 4.171269]\n",
      "epoch:0 step:56 [D1 loss: 0.830630, acc.: 60.94%] [D2 loss: 0.668940, acc.: 58.59%] [G loss: 4.217845]\n",
      "epoch:0 step:57 [D1 loss: 0.727127, acc.: 60.94%] [D2 loss: 0.598951, acc.: 71.88%] [G loss: 4.161530]\n",
      "epoch:0 step:58 [D1 loss: 0.690149, acc.: 57.81%] [D2 loss: 0.570204, acc.: 67.19%] [G loss: 4.345244]\n",
      "epoch:0 step:59 [D1 loss: 0.713626, acc.: 66.41%] [D2 loss: 0.663571, acc.: 67.19%] [G loss: 4.299236]\n",
      "epoch:0 step:60 [D1 loss: 0.774396, acc.: 67.19%] [D2 loss: 0.598225, acc.: 71.88%] [G loss: 4.387265]\n",
      "epoch:0 step:61 [D1 loss: 0.963127, acc.: 43.75%] [D2 loss: 0.752159, acc.: 60.16%] [G loss: 3.999049]\n",
      "epoch:0 step:62 [D1 loss: 0.915112, acc.: 51.56%] [D2 loss: 0.627721, acc.: 65.62%] [G loss: 5.104714]\n",
      "epoch:0 step:63 [D1 loss: 0.916847, acc.: 53.91%] [D2 loss: 0.705037, acc.: 66.41%] [G loss: 4.543494]\n",
      "epoch:0 step:64 [D1 loss: 0.913383, acc.: 49.22%] [D2 loss: 0.746754, acc.: 51.56%] [G loss: 3.712761]\n",
      "epoch:0 step:65 [D1 loss: 0.657698, acc.: 60.16%] [D2 loss: 0.664670, acc.: 63.28%] [G loss: 4.614732]\n",
      "epoch:0 step:66 [D1 loss: 0.803148, acc.: 52.34%] [D2 loss: 0.585650, acc.: 71.09%] [G loss: 4.291244]\n",
      "epoch:0 step:67 [D1 loss: 0.929610, acc.: 41.41%] [D2 loss: 0.766245, acc.: 58.59%] [G loss: 3.752467]\n",
      "epoch:0 step:68 [D1 loss: 0.767286, acc.: 50.78%] [D2 loss: 0.637426, acc.: 67.97%] [G loss: 3.878586]\n",
      "epoch:0 step:69 [D1 loss: 0.874847, acc.: 48.44%] [D2 loss: 0.662390, acc.: 64.06%] [G loss: 4.032928]\n",
      "epoch:0 step:70 [D1 loss: 0.730099, acc.: 52.34%] [D2 loss: 0.646408, acc.: 66.41%] [G loss: 3.851243]\n",
      "epoch:0 step:71 [D1 loss: 0.801807, acc.: 46.09%] [D2 loss: 0.676649, acc.: 75.78%] [G loss: 4.388641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:72 [D1 loss: 0.738930, acc.: 57.81%] [D2 loss: 0.791218, acc.: 51.56%] [G loss: 3.971403]\n",
      "epoch:0 step:73 [D1 loss: 0.767198, acc.: 51.56%] [D2 loss: 0.691995, acc.: 65.62%] [G loss: 3.625148]\n",
      "epoch:0 step:74 [D1 loss: 0.730717, acc.: 61.72%] [D2 loss: 0.581367, acc.: 68.75%] [G loss: 4.029885]\n",
      "epoch:0 step:75 [D1 loss: 0.731491, acc.: 60.94%] [D2 loss: 0.780753, acc.: 60.94%] [G loss: 4.015647]\n",
      "epoch:0 step:76 [D1 loss: 0.740478, acc.: 63.28%] [D2 loss: 0.641445, acc.: 71.09%] [G loss: 3.761047]\n",
      "epoch:0 step:77 [D1 loss: 0.759484, acc.: 53.91%] [D2 loss: 0.603537, acc.: 67.97%] [G loss: 4.138339]\n",
      "epoch:0 step:78 [D1 loss: 0.697549, acc.: 65.62%] [D2 loss: 0.734927, acc.: 67.19%] [G loss: 4.393838]\n",
      "epoch:0 step:79 [D1 loss: 1.108809, acc.: 42.97%] [D2 loss: 0.730742, acc.: 63.28%] [G loss: 3.633979]\n",
      "epoch:0 step:80 [D1 loss: 0.753716, acc.: 60.16%] [D2 loss: 0.657848, acc.: 63.28%] [G loss: 3.617743]\n",
      "epoch:0 step:81 [D1 loss: 0.791153, acc.: 57.03%] [D2 loss: 0.763648, acc.: 53.91%] [G loss: 3.992576]\n",
      "epoch:0 step:82 [D1 loss: 0.815238, acc.: 52.34%] [D2 loss: 0.565961, acc.: 67.97%] [G loss: 4.194998]\n",
      "epoch:0 step:83 [D1 loss: 0.701883, acc.: 62.50%] [D2 loss: 0.747587, acc.: 60.94%] [G loss: 3.829529]\n",
      "epoch:0 step:84 [D1 loss: 0.777297, acc.: 64.84%] [D2 loss: 0.725338, acc.: 59.38%] [G loss: 4.016097]\n",
      "epoch:0 step:85 [D1 loss: 0.762035, acc.: 60.16%] [D2 loss: 0.689755, acc.: 60.16%] [G loss: 3.813135]\n",
      "epoch:0 step:86 [D1 loss: 0.691170, acc.: 59.38%] [D2 loss: 0.564430, acc.: 70.31%] [G loss: 3.777230]\n",
      "epoch:0 step:87 [D1 loss: 0.726279, acc.: 58.59%] [D2 loss: 0.547317, acc.: 68.75%] [G loss: 3.962512]\n",
      "epoch:0 step:88 [D1 loss: 0.697091, acc.: 53.91%] [D2 loss: 0.606662, acc.: 74.22%] [G loss: 3.751822]\n",
      "epoch:0 step:89 [D1 loss: 0.793013, acc.: 58.59%] [D2 loss: 0.646723, acc.: 69.53%] [G loss: 3.827880]\n",
      "epoch:0 step:90 [D1 loss: 0.848868, acc.: 50.00%] [D2 loss: 0.753319, acc.: 58.59%] [G loss: 4.071448]\n",
      "epoch:0 step:91 [D1 loss: 0.754813, acc.: 55.47%] [D2 loss: 0.642835, acc.: 72.66%] [G loss: 3.789143]\n",
      "epoch:0 step:92 [D1 loss: 0.768203, acc.: 56.25%] [D2 loss: 0.787062, acc.: 48.44%] [G loss: 3.247781]\n",
      "epoch:0 step:93 [D1 loss: 0.664188, acc.: 65.62%] [D2 loss: 0.631376, acc.: 61.72%] [G loss: 4.068026]\n",
      "epoch:0 step:94 [D1 loss: 0.709385, acc.: 62.50%] [D2 loss: 0.612150, acc.: 67.97%] [G loss: 3.779861]\n",
      "epoch:0 step:95 [D1 loss: 0.764770, acc.: 52.34%] [D2 loss: 0.652312, acc.: 64.06%] [G loss: 3.661567]\n",
      "epoch:0 step:96 [D1 loss: 0.615250, acc.: 65.62%] [D2 loss: 0.549167, acc.: 69.53%] [G loss: 3.828244]\n",
      "epoch:0 step:97 [D1 loss: 0.641214, acc.: 56.25%] [D2 loss: 0.627655, acc.: 64.06%] [G loss: 3.553910]\n",
      "epoch:0 step:98 [D1 loss: 0.736372, acc.: 60.16%] [D2 loss: 0.663018, acc.: 65.62%] [G loss: 3.690938]\n",
      "epoch:0 step:99 [D1 loss: 0.760917, acc.: 55.47%] [D2 loss: 0.600315, acc.: 64.06%] [G loss: 3.755070]\n",
      "epoch:0 step:100 [D1 loss: 0.833646, acc.: 50.00%] [D2 loss: 0.633852, acc.: 60.94%] [G loss: 3.512506]\n",
      "epoch:0 step:101 [D1 loss: 0.619743, acc.: 63.28%] [D2 loss: 0.594121, acc.: 68.75%] [G loss: 3.794489]\n",
      "epoch:0 step:102 [D1 loss: 0.679260, acc.: 66.41%] [D2 loss: 0.545676, acc.: 67.19%] [G loss: 3.889179]\n",
      "epoch:0 step:103 [D1 loss: 0.641366, acc.: 67.97%] [D2 loss: 0.580988, acc.: 70.31%] [G loss: 3.723006]\n",
      "epoch:0 step:104 [D1 loss: 0.680375, acc.: 60.16%] [D2 loss: 0.514215, acc.: 74.22%] [G loss: 4.202551]\n",
      "epoch:0 step:105 [D1 loss: 0.662643, acc.: 64.06%] [D2 loss: 0.540132, acc.: 72.66%] [G loss: 4.201499]\n",
      "epoch:0 step:106 [D1 loss: 0.759959, acc.: 55.47%] [D2 loss: 0.499787, acc.: 76.56%] [G loss: 3.820756]\n",
      "epoch:0 step:107 [D1 loss: 0.761935, acc.: 60.94%] [D2 loss: 0.633752, acc.: 61.72%] [G loss: 3.701575]\n",
      "epoch:0 step:108 [D1 loss: 0.673390, acc.: 60.94%] [D2 loss: 0.681439, acc.: 58.59%] [G loss: 3.605013]\n",
      "epoch:0 step:109 [D1 loss: 0.662353, acc.: 61.72%] [D2 loss: 0.644085, acc.: 60.94%] [G loss: 4.102519]\n",
      "epoch:0 step:110 [D1 loss: 0.687586, acc.: 65.62%] [D2 loss: 0.684112, acc.: 60.16%] [G loss: 4.050691]\n",
      "epoch:0 step:111 [D1 loss: 0.583271, acc.: 73.44%] [D2 loss: 0.540903, acc.: 66.41%] [G loss: 3.868382]\n",
      "epoch:0 step:112 [D1 loss: 0.688779, acc.: 60.16%] [D2 loss: 0.674915, acc.: 58.59%] [G loss: 3.612663]\n",
      "epoch:0 step:113 [D1 loss: 0.831233, acc.: 59.38%] [D2 loss: 0.593534, acc.: 67.97%] [G loss: 3.827274]\n",
      "epoch:0 step:114 [D1 loss: 0.797124, acc.: 49.22%] [D2 loss: 0.584207, acc.: 68.75%] [G loss: 3.826358]\n",
      "epoch:0 step:115 [D1 loss: 0.648529, acc.: 60.94%] [D2 loss: 0.525994, acc.: 73.44%] [G loss: 3.670425]\n",
      "epoch:0 step:116 [D1 loss: 0.664720, acc.: 61.72%] [D2 loss: 0.631203, acc.: 67.97%] [G loss: 3.706383]\n",
      "epoch:0 step:117 [D1 loss: 0.698602, acc.: 56.25%] [D2 loss: 0.768926, acc.: 58.59%] [G loss: 3.629122]\n",
      "epoch:0 step:118 [D1 loss: 0.726143, acc.: 53.91%] [D2 loss: 0.716025, acc.: 62.50%] [G loss: 3.921787]\n",
      "epoch:0 step:119 [D1 loss: 0.761011, acc.: 53.12%] [D2 loss: 0.625030, acc.: 64.84%] [G loss: 3.992386]\n",
      "epoch:0 step:120 [D1 loss: 0.690531, acc.: 60.94%] [D2 loss: 0.727165, acc.: 56.25%] [G loss: 3.769937]\n",
      "epoch:0 step:121 [D1 loss: 0.586692, acc.: 67.19%] [D2 loss: 0.543599, acc.: 72.66%] [G loss: 3.812986]\n",
      "epoch:0 step:122 [D1 loss: 0.778933, acc.: 54.69%] [D2 loss: 0.722105, acc.: 58.59%] [G loss: 3.689626]\n",
      "epoch:0 step:123 [D1 loss: 0.741650, acc.: 50.00%] [D2 loss: 0.731557, acc.: 55.47%] [G loss: 3.976954]\n",
      "epoch:0 step:124 [D1 loss: 0.731567, acc.: 56.25%] [D2 loss: 0.602439, acc.: 69.53%] [G loss: 3.807415]\n",
      "epoch:0 step:125 [D1 loss: 0.687328, acc.: 64.84%] [D2 loss: 0.650125, acc.: 58.59%] [G loss: 3.356431]\n",
      "epoch:0 step:126 [D1 loss: 0.787682, acc.: 53.91%] [D2 loss: 0.660394, acc.: 65.62%] [G loss: 3.375465]\n",
      "epoch:0 step:127 [D1 loss: 0.789625, acc.: 51.56%] [D2 loss: 0.591666, acc.: 71.09%] [G loss: 3.818186]\n",
      "epoch:0 step:128 [D1 loss: 0.887607, acc.: 42.19%] [D2 loss: 0.697535, acc.: 60.16%] [G loss: 3.452557]\n",
      "epoch:0 step:129 [D1 loss: 0.793826, acc.: 43.75%] [D2 loss: 0.695851, acc.: 61.72%] [G loss: 3.472193]\n",
      "epoch:0 step:130 [D1 loss: 0.819073, acc.: 48.44%] [D2 loss: 0.727272, acc.: 60.16%] [G loss: 3.324548]\n",
      "epoch:0 step:131 [D1 loss: 0.730629, acc.: 53.91%] [D2 loss: 0.538755, acc.: 67.19%] [G loss: 3.549623]\n",
      "epoch:0 step:132 [D1 loss: 0.640165, acc.: 60.94%] [D2 loss: 0.607215, acc.: 69.53%] [G loss: 3.762085]\n",
      "epoch:0 step:133 [D1 loss: 0.823092, acc.: 48.44%] [D2 loss: 0.582118, acc.: 67.97%] [G loss: 3.571761]\n",
      "epoch:0 step:134 [D1 loss: 0.637957, acc.: 61.72%] [D2 loss: 0.631824, acc.: 64.84%] [G loss: 3.276484]\n",
      "epoch:0 step:135 [D1 loss: 0.685637, acc.: 62.50%] [D2 loss: 0.605091, acc.: 69.53%] [G loss: 3.340263]\n",
      "epoch:0 step:136 [D1 loss: 0.800306, acc.: 52.34%] [D2 loss: 0.618577, acc.: 65.62%] [G loss: 3.410776]\n",
      "epoch:0 step:137 [D1 loss: 0.645697, acc.: 64.84%] [D2 loss: 0.693839, acc.: 65.62%] [G loss: 3.800467]\n",
      "epoch:0 step:138 [D1 loss: 0.694123, acc.: 53.91%] [D2 loss: 0.582980, acc.: 71.88%] [G loss: 3.418314]\n",
      "epoch:0 step:139 [D1 loss: 0.856130, acc.: 50.00%] [D2 loss: 0.576286, acc.: 65.62%] [G loss: 3.565722]\n",
      "epoch:0 step:140 [D1 loss: 0.694230, acc.: 56.25%] [D2 loss: 0.649904, acc.: 69.53%] [G loss: 3.593004]\n",
      "epoch:0 step:141 [D1 loss: 0.806431, acc.: 45.31%] [D2 loss: 0.662436, acc.: 61.72%] [G loss: 3.549571]\n",
      "epoch:0 step:142 [D1 loss: 0.640171, acc.: 64.06%] [D2 loss: 0.606054, acc.: 68.75%] [G loss: 3.677036]\n",
      "epoch:0 step:143 [D1 loss: 0.771882, acc.: 43.75%] [D2 loss: 0.607777, acc.: 64.06%] [G loss: 3.289584]\n",
      "epoch:0 step:144 [D1 loss: 0.743380, acc.: 57.03%] [D2 loss: 0.671506, acc.: 65.62%] [G loss: 3.200616]\n",
      "epoch:0 step:145 [D1 loss: 0.671986, acc.: 56.25%] [D2 loss: 0.575615, acc.: 67.97%] [G loss: 3.276945]\n",
      "epoch:0 step:146 [D1 loss: 0.622894, acc.: 66.41%] [D2 loss: 0.624884, acc.: 65.62%] [G loss: 3.680463]\n",
      "epoch:0 step:147 [D1 loss: 0.610740, acc.: 71.09%] [D2 loss: 0.567854, acc.: 72.66%] [G loss: 3.583467]\n",
      "epoch:0 step:148 [D1 loss: 0.674105, acc.: 57.81%] [D2 loss: 0.694363, acc.: 60.94%] [G loss: 3.446170]\n",
      "epoch:0 step:149 [D1 loss: 0.728310, acc.: 53.12%] [D2 loss: 0.735993, acc.: 57.03%] [G loss: 3.368553]\n",
      "epoch:0 step:150 [D1 loss: 0.650898, acc.: 66.41%] [D2 loss: 0.592021, acc.: 67.97%] [G loss: 3.394232]\n",
      "epoch:0 step:151 [D1 loss: 0.720839, acc.: 53.12%] [D2 loss: 0.581531, acc.: 64.84%] [G loss: 3.641618]\n",
      "epoch:0 step:152 [D1 loss: 0.681967, acc.: 58.59%] [D2 loss: 0.536056, acc.: 77.34%] [G loss: 3.736688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:153 [D1 loss: 0.715175, acc.: 54.69%] [D2 loss: 0.621574, acc.: 71.09%] [G loss: 3.485997]\n",
      "epoch:0 step:154 [D1 loss: 0.752877, acc.: 52.34%] [D2 loss: 0.625656, acc.: 67.19%] [G loss: 3.250177]\n",
      "epoch:0 step:155 [D1 loss: 0.658243, acc.: 60.94%] [D2 loss: 0.611460, acc.: 66.41%] [G loss: 3.381281]\n",
      "epoch:0 step:156 [D1 loss: 0.706044, acc.: 60.94%] [D2 loss: 0.542375, acc.: 68.75%] [G loss: 3.337784]\n",
      "epoch:0 step:157 [D1 loss: 0.741575, acc.: 55.47%] [D2 loss: 0.706682, acc.: 57.03%] [G loss: 3.591253]\n",
      "epoch:0 step:158 [D1 loss: 0.693994, acc.: 54.69%] [D2 loss: 0.645383, acc.: 64.06%] [G loss: 3.389635]\n",
      "epoch:0 step:159 [D1 loss: 0.733816, acc.: 56.25%] [D2 loss: 0.583495, acc.: 69.53%] [G loss: 3.631968]\n",
      "epoch:0 step:160 [D1 loss: 0.676784, acc.: 60.94%] [D2 loss: 0.580536, acc.: 67.97%] [G loss: 3.574291]\n",
      "epoch:0 step:161 [D1 loss: 0.754097, acc.: 56.25%] [D2 loss: 0.668106, acc.: 65.62%] [G loss: 3.702013]\n",
      "epoch:0 step:162 [D1 loss: 0.676740, acc.: 64.06%] [D2 loss: 0.703644, acc.: 60.16%] [G loss: 3.178435]\n",
      "epoch:0 step:163 [D1 loss: 0.827034, acc.: 51.56%] [D2 loss: 0.609596, acc.: 65.62%] [G loss: 3.551368]\n",
      "epoch:0 step:164 [D1 loss: 0.759706, acc.: 52.34%] [D2 loss: 0.714404, acc.: 55.47%] [G loss: 3.290003]\n",
      "epoch:0 step:165 [D1 loss: 0.742897, acc.: 53.12%] [D2 loss: 0.640625, acc.: 61.72%] [G loss: 3.135575]\n",
      "epoch:0 step:166 [D1 loss: 0.675802, acc.: 58.59%] [D2 loss: 0.599676, acc.: 62.50%] [G loss: 3.256452]\n",
      "epoch:0 step:167 [D1 loss: 0.649867, acc.: 58.59%] [D2 loss: 0.613775, acc.: 63.28%] [G loss: 3.271503]\n",
      "epoch:0 step:168 [D1 loss: 0.676801, acc.: 59.38%] [D2 loss: 0.641373, acc.: 62.50%] [G loss: 3.258404]\n",
      "epoch:0 step:169 [D1 loss: 0.746170, acc.: 54.69%] [D2 loss: 0.665262, acc.: 54.69%] [G loss: 3.238817]\n",
      "epoch:0 step:170 [D1 loss: 0.739422, acc.: 50.78%] [D2 loss: 0.654575, acc.: 60.16%] [G loss: 3.105235]\n",
      "epoch:0 step:171 [D1 loss: 0.578209, acc.: 73.44%] [D2 loss: 0.639023, acc.: 56.25%] [G loss: 3.378172]\n",
      "epoch:0 step:172 [D1 loss: 0.645321, acc.: 63.28%] [D2 loss: 0.651215, acc.: 60.94%] [G loss: 3.225132]\n",
      "epoch:0 step:173 [D1 loss: 0.594638, acc.: 71.09%] [D2 loss: 0.580008, acc.: 72.66%] [G loss: 3.091516]\n",
      "epoch:0 step:174 [D1 loss: 0.653875, acc.: 55.47%] [D2 loss: 0.652959, acc.: 67.97%] [G loss: 3.419963]\n",
      "epoch:0 step:175 [D1 loss: 0.799947, acc.: 53.91%] [D2 loss: 0.592036, acc.: 66.41%] [G loss: 3.313267]\n",
      "epoch:0 step:176 [D1 loss: 0.755216, acc.: 50.00%] [D2 loss: 0.637074, acc.: 62.50%] [G loss: 3.375315]\n",
      "epoch:0 step:177 [D1 loss: 0.789699, acc.: 46.09%] [D2 loss: 0.601400, acc.: 65.62%] [G loss: 3.054158]\n",
      "epoch:0 step:178 [D1 loss: 0.631914, acc.: 65.62%] [D2 loss: 0.598612, acc.: 67.19%] [G loss: 3.239752]\n",
      "epoch:0 step:179 [D1 loss: 0.665720, acc.: 65.62%] [D2 loss: 0.684722, acc.: 62.50%] [G loss: 3.173778]\n",
      "epoch:0 step:180 [D1 loss: 0.649484, acc.: 63.28%] [D2 loss: 0.564004, acc.: 70.31%] [G loss: 3.362236]\n",
      "epoch:0 step:181 [D1 loss: 0.668706, acc.: 64.06%] [D2 loss: 0.628619, acc.: 60.94%] [G loss: 3.324533]\n",
      "epoch:0 step:182 [D1 loss: 0.744783, acc.: 59.38%] [D2 loss: 0.640017, acc.: 69.53%] [G loss: 3.143043]\n",
      "epoch:0 step:183 [D1 loss: 0.675049, acc.: 55.47%] [D2 loss: 0.688910, acc.: 60.94%] [G loss: 3.402592]\n",
      "epoch:0 step:184 [D1 loss: 0.590828, acc.: 69.53%] [D2 loss: 0.689957, acc.: 64.84%] [G loss: 3.492762]\n",
      "epoch:0 step:185 [D1 loss: 0.682955, acc.: 60.16%] [D2 loss: 0.587795, acc.: 64.06%] [G loss: 3.427917]\n",
      "epoch:0 step:186 [D1 loss: 0.720165, acc.: 57.03%] [D2 loss: 0.591278, acc.: 66.41%] [G loss: 3.267024]\n",
      "epoch:0 step:187 [D1 loss: 0.671489, acc.: 60.94%] [D2 loss: 0.630191, acc.: 60.16%] [G loss: 3.523975]\n",
      "epoch:0 step:188 [D1 loss: 0.733914, acc.: 52.34%] [D2 loss: 0.536490, acc.: 73.44%] [G loss: 3.297777]\n",
      "epoch:0 step:189 [D1 loss: 0.682487, acc.: 60.94%] [D2 loss: 0.591251, acc.: 71.88%] [G loss: 3.251286]\n",
      "epoch:0 step:190 [D1 loss: 0.708492, acc.: 55.47%] [D2 loss: 0.651470, acc.: 58.59%] [G loss: 3.256529]\n",
      "epoch:0 step:191 [D1 loss: 0.640277, acc.: 64.84%] [D2 loss: 0.578628, acc.: 69.53%] [G loss: 3.263195]\n",
      "epoch:0 step:192 [D1 loss: 0.769602, acc.: 47.66%] [D2 loss: 0.633300, acc.: 64.84%] [G loss: 3.255991]\n",
      "epoch:0 step:193 [D1 loss: 0.658593, acc.: 60.16%] [D2 loss: 0.616531, acc.: 65.62%] [G loss: 3.186703]\n",
      "epoch:0 step:194 [D1 loss: 0.780876, acc.: 50.78%] [D2 loss: 0.619186, acc.: 64.84%] [G loss: 3.571980]\n",
      "epoch:0 step:195 [D1 loss: 0.791418, acc.: 61.72%] [D2 loss: 0.599270, acc.: 68.75%] [G loss: 3.443908]\n",
      "epoch:0 step:196 [D1 loss: 0.678129, acc.: 52.34%] [D2 loss: 0.662814, acc.: 61.72%] [G loss: 3.241789]\n",
      "epoch:0 step:197 [D1 loss: 0.651061, acc.: 63.28%] [D2 loss: 0.600570, acc.: 62.50%] [G loss: 3.127544]\n",
      "epoch:0 step:198 [D1 loss: 0.658416, acc.: 61.72%] [D2 loss: 0.664850, acc.: 62.50%] [G loss: 3.993781]\n",
      "epoch:0 step:199 [D1 loss: 0.702651, acc.: 61.72%] [D2 loss: 0.615692, acc.: 65.62%] [G loss: 3.579998]\n",
      "epoch:0 step:200 [D1 loss: 0.820004, acc.: 45.31%] [D2 loss: 0.567588, acc.: 69.53%] [G loss: 3.239384]\n",
      "epoch:0 step:201 [D1 loss: 0.711839, acc.: 57.03%] [D2 loss: 0.679840, acc.: 55.47%] [G loss: 3.284377]\n",
      "epoch:0 step:202 [D1 loss: 0.727553, acc.: 57.81%] [D2 loss: 0.675920, acc.: 62.50%] [G loss: 3.516451]\n",
      "epoch:0 step:203 [D1 loss: 0.651969, acc.: 63.28%] [D2 loss: 0.633947, acc.: 61.72%] [G loss: 3.109552]\n",
      "epoch:0 step:204 [D1 loss: 0.731225, acc.: 55.47%] [D2 loss: 0.607909, acc.: 63.28%] [G loss: 3.093510]\n",
      "epoch:0 step:205 [D1 loss: 0.730118, acc.: 57.81%] [D2 loss: 0.616329, acc.: 71.09%] [G loss: 3.124849]\n",
      "epoch:0 step:206 [D1 loss: 0.712025, acc.: 55.47%] [D2 loss: 0.610108, acc.: 62.50%] [G loss: 3.293613]\n",
      "epoch:0 step:207 [D1 loss: 0.674272, acc.: 57.81%] [D2 loss: 0.614209, acc.: 66.41%] [G loss: 3.381956]\n",
      "epoch:0 step:208 [D1 loss: 0.702974, acc.: 53.12%] [D2 loss: 0.527640, acc.: 76.56%] [G loss: 3.699042]\n",
      "epoch:0 step:209 [D1 loss: 0.897409, acc.: 48.44%] [D2 loss: 0.622835, acc.: 60.94%] [G loss: 3.433795]\n",
      "epoch:0 step:210 [D1 loss: 0.800692, acc.: 50.78%] [D2 loss: 0.571759, acc.: 72.66%] [G loss: 3.085148]\n",
      "epoch:0 step:211 [D1 loss: 0.830876, acc.: 44.53%] [D2 loss: 0.566662, acc.: 71.88%] [G loss: 3.243946]\n",
      "epoch:0 step:212 [D1 loss: 0.697443, acc.: 53.12%] [D2 loss: 0.570186, acc.: 71.88%] [G loss: 3.275303]\n",
      "epoch:0 step:213 [D1 loss: 0.726907, acc.: 50.00%] [D2 loss: 0.583164, acc.: 66.41%] [G loss: 3.227060]\n",
      "epoch:0 step:214 [D1 loss: 0.602457, acc.: 67.19%] [D2 loss: 0.525646, acc.: 73.44%] [G loss: 3.409572]\n",
      "epoch:0 step:215 [D1 loss: 0.681250, acc.: 54.69%] [D2 loss: 0.632077, acc.: 69.53%] [G loss: 3.591418]\n",
      "epoch:0 step:216 [D1 loss: 0.712732, acc.: 54.69%] [D2 loss: 0.561846, acc.: 72.66%] [G loss: 3.479138]\n",
      "epoch:0 step:217 [D1 loss: 0.693338, acc.: 57.03%] [D2 loss: 0.559310, acc.: 71.88%] [G loss: 3.277067]\n",
      "epoch:0 step:218 [D1 loss: 0.600787, acc.: 65.62%] [D2 loss: 0.562923, acc.: 72.66%] [G loss: 3.400583]\n",
      "epoch:0 step:219 [D1 loss: 0.674654, acc.: 60.94%] [D2 loss: 0.607842, acc.: 67.19%] [G loss: 3.493504]\n",
      "epoch:0 step:220 [D1 loss: 0.637494, acc.: 63.28%] [D2 loss: 0.622656, acc.: 57.81%] [G loss: 3.325743]\n",
      "epoch:0 step:221 [D1 loss: 0.654753, acc.: 60.16%] [D2 loss: 0.606067, acc.: 65.62%] [G loss: 3.281922]\n",
      "epoch:0 step:222 [D1 loss: 0.670630, acc.: 53.12%] [D2 loss: 0.547546, acc.: 71.88%] [G loss: 3.534388]\n",
      "epoch:0 step:223 [D1 loss: 0.720366, acc.: 50.00%] [D2 loss: 0.511575, acc.: 76.56%] [G loss: 3.463185]\n",
      "epoch:0 step:224 [D1 loss: 0.713200, acc.: 59.38%] [D2 loss: 0.548612, acc.: 71.09%] [G loss: 3.378089]\n",
      "epoch:0 step:225 [D1 loss: 0.654452, acc.: 61.72%] [D2 loss: 0.613951, acc.: 64.06%] [G loss: 3.224273]\n",
      "epoch:0 step:226 [D1 loss: 0.658320, acc.: 60.94%] [D2 loss: 0.632659, acc.: 67.19%] [G loss: 3.353871]\n",
      "epoch:0 step:227 [D1 loss: 0.760041, acc.: 50.00%] [D2 loss: 0.574750, acc.: 64.06%] [G loss: 3.268335]\n",
      "epoch:0 step:228 [D1 loss: 0.655023, acc.: 57.81%] [D2 loss: 0.650467, acc.: 57.03%] [G loss: 3.100332]\n",
      "epoch:0 step:229 [D1 loss: 0.605184, acc.: 69.53%] [D2 loss: 0.513048, acc.: 78.91%] [G loss: 3.280408]\n",
      "epoch:0 step:230 [D1 loss: 0.668743, acc.: 59.38%] [D2 loss: 0.661600, acc.: 57.03%] [G loss: 3.596065]\n",
      "epoch:0 step:231 [D1 loss: 0.657970, acc.: 59.38%] [D2 loss: 0.578815, acc.: 67.19%] [G loss: 3.368942]\n",
      "epoch:0 step:232 [D1 loss: 0.735958, acc.: 58.59%] [D2 loss: 0.594079, acc.: 66.41%] [G loss: 3.266799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:233 [D1 loss: 0.706380, acc.: 53.12%] [D2 loss: 0.561797, acc.: 74.22%] [G loss: 3.654655]\n",
      "epoch:0 step:234 [D1 loss: 0.685467, acc.: 62.50%] [D2 loss: 0.548635, acc.: 74.22%] [G loss: 3.222407]\n",
      "epoch:0 step:235 [D1 loss: 0.614432, acc.: 68.75%] [D2 loss: 0.569089, acc.: 70.31%] [G loss: 3.198163]\n",
      "epoch:0 step:236 [D1 loss: 0.565462, acc.: 69.53%] [D2 loss: 0.551756, acc.: 72.66%] [G loss: 3.425713]\n",
      "epoch:0 step:237 [D1 loss: 0.615429, acc.: 67.97%] [D2 loss: 0.601173, acc.: 72.66%] [G loss: 3.139544]\n",
      "epoch:0 step:238 [D1 loss: 0.670134, acc.: 65.62%] [D2 loss: 0.531264, acc.: 71.09%] [G loss: 3.058449]\n",
      "epoch:0 step:239 [D1 loss: 0.686562, acc.: 62.50%] [D2 loss: 0.534818, acc.: 73.44%] [G loss: 3.313481]\n",
      "epoch:0 step:240 [D1 loss: 0.808764, acc.: 48.44%] [D2 loss: 0.558301, acc.: 71.09%] [G loss: 3.651941]\n",
      "epoch:0 step:241 [D1 loss: 0.746781, acc.: 54.69%] [D2 loss: 0.519529, acc.: 71.88%] [G loss: 3.525084]\n",
      "epoch:0 step:242 [D1 loss: 0.669733, acc.: 60.16%] [D2 loss: 0.564252, acc.: 65.62%] [G loss: 3.340119]\n",
      "epoch:0 step:243 [D1 loss: 0.669276, acc.: 58.59%] [D2 loss: 0.667215, acc.: 58.59%] [G loss: 3.169898]\n",
      "epoch:0 step:244 [D1 loss: 0.688416, acc.: 58.59%] [D2 loss: 0.588804, acc.: 67.97%] [G loss: 3.338968]\n",
      "epoch:0 step:245 [D1 loss: 0.642747, acc.: 70.31%] [D2 loss: 0.605027, acc.: 61.72%] [G loss: 3.286988]\n",
      "epoch:0 step:246 [D1 loss: 0.640479, acc.: 64.84%] [D2 loss: 0.616635, acc.: 65.62%] [G loss: 3.211037]\n",
      "epoch:0 step:247 [D1 loss: 0.589334, acc.: 68.75%] [D2 loss: 0.679903, acc.: 67.97%] [G loss: 3.578946]\n",
      "epoch:0 step:248 [D1 loss: 0.752462, acc.: 52.34%] [D2 loss: 0.608882, acc.: 64.84%] [G loss: 3.511197]\n",
      "epoch:0 step:249 [D1 loss: 0.777336, acc.: 53.12%] [D2 loss: 0.533840, acc.: 76.56%] [G loss: 3.533922]\n",
      "epoch:0 step:250 [D1 loss: 0.699300, acc.: 62.50%] [D2 loss: 0.624563, acc.: 63.28%] [G loss: 3.418136]\n",
      "epoch:0 step:251 [D1 loss: 0.681769, acc.: 57.81%] [D2 loss: 0.555364, acc.: 67.97%] [G loss: 3.345946]\n",
      "epoch:0 step:252 [D1 loss: 0.706787, acc.: 64.84%] [D2 loss: 0.596498, acc.: 71.09%] [G loss: 2.948377]\n",
      "epoch:0 step:253 [D1 loss: 0.693547, acc.: 57.03%] [D2 loss: 0.543064, acc.: 71.09%] [G loss: 3.335901]\n",
      "epoch:0 step:254 [D1 loss: 0.675905, acc.: 62.50%] [D2 loss: 0.623859, acc.: 64.84%] [G loss: 3.380547]\n",
      "epoch:0 step:255 [D1 loss: 0.788986, acc.: 57.81%] [D2 loss: 0.606456, acc.: 66.41%] [G loss: 3.214942]\n",
      "epoch:0 step:256 [D1 loss: 0.689982, acc.: 60.94%] [D2 loss: 0.487802, acc.: 79.69%] [G loss: 3.332122]\n",
      "epoch:0 step:257 [D1 loss: 0.764691, acc.: 50.78%] [D2 loss: 0.601603, acc.: 70.31%] [G loss: 3.198559]\n",
      "epoch:0 step:258 [D1 loss: 0.652479, acc.: 57.81%] [D2 loss: 0.564377, acc.: 71.88%] [G loss: 3.422015]\n",
      "epoch:0 step:259 [D1 loss: 0.691970, acc.: 58.59%] [D2 loss: 0.565515, acc.: 71.88%] [G loss: 3.317608]\n",
      "epoch:0 step:260 [D1 loss: 0.769425, acc.: 54.69%] [D2 loss: 0.615617, acc.: 67.19%] [G loss: 3.296032]\n",
      "epoch:0 step:261 [D1 loss: 0.680132, acc.: 57.81%] [D2 loss: 0.587108, acc.: 70.31%] [G loss: 3.348297]\n",
      "epoch:0 step:262 [D1 loss: 0.778222, acc.: 50.00%] [D2 loss: 0.632652, acc.: 63.28%] [G loss: 3.318297]\n",
      "epoch:0 step:263 [D1 loss: 0.839007, acc.: 45.31%] [D2 loss: 0.701083, acc.: 53.91%] [G loss: 3.402381]\n",
      "epoch:0 step:264 [D1 loss: 0.669827, acc.: 56.25%] [D2 loss: 0.655247, acc.: 58.59%] [G loss: 3.360544]\n",
      "epoch:0 step:265 [D1 loss: 0.652966, acc.: 59.38%] [D2 loss: 0.625578, acc.: 66.41%] [G loss: 3.150281]\n",
      "epoch:0 step:266 [D1 loss: 0.695720, acc.: 48.44%] [D2 loss: 0.591247, acc.: 66.41%] [G loss: 3.162334]\n",
      "epoch:0 step:267 [D1 loss: 0.826277, acc.: 46.88%] [D2 loss: 0.733951, acc.: 55.47%] [G loss: 3.202532]\n",
      "epoch:0 step:268 [D1 loss: 0.789665, acc.: 57.03%] [D2 loss: 0.546068, acc.: 70.31%] [G loss: 3.223177]\n",
      "epoch:0 step:269 [D1 loss: 0.669212, acc.: 63.28%] [D2 loss: 0.618721, acc.: 63.28%] [G loss: 3.301534]\n",
      "epoch:0 step:270 [D1 loss: 0.641778, acc.: 62.50%] [D2 loss: 0.581232, acc.: 71.88%] [G loss: 3.122044]\n",
      "epoch:0 step:271 [D1 loss: 0.700143, acc.: 59.38%] [D2 loss: 0.566425, acc.: 70.31%] [G loss: 3.289066]\n",
      "epoch:0 step:272 [D1 loss: 0.641910, acc.: 57.81%] [D2 loss: 0.569551, acc.: 70.31%] [G loss: 3.322772]\n",
      "epoch:0 step:273 [D1 loss: 0.644731, acc.: 64.06%] [D2 loss: 0.584191, acc.: 71.09%] [G loss: 3.118881]\n",
      "epoch:0 step:274 [D1 loss: 0.656068, acc.: 62.50%] [D2 loss: 0.611153, acc.: 69.53%] [G loss: 3.301897]\n",
      "epoch:0 step:275 [D1 loss: 0.688884, acc.: 61.72%] [D2 loss: 0.510450, acc.: 79.69%] [G loss: 3.330291]\n",
      "epoch:0 step:276 [D1 loss: 0.665385, acc.: 60.94%] [D2 loss: 0.534039, acc.: 72.66%] [G loss: 3.438395]\n",
      "epoch:0 step:277 [D1 loss: 0.722247, acc.: 53.12%] [D2 loss: 0.561941, acc.: 71.09%] [G loss: 3.388411]\n",
      "epoch:0 step:278 [D1 loss: 0.728140, acc.: 51.56%] [D2 loss: 0.620970, acc.: 61.72%] [G loss: 3.292909]\n",
      "epoch:0 step:279 [D1 loss: 0.674865, acc.: 57.03%] [D2 loss: 0.596808, acc.: 67.97%] [G loss: 3.408340]\n",
      "epoch:0 step:280 [D1 loss: 0.689530, acc.: 57.03%] [D2 loss: 0.600946, acc.: 64.84%] [G loss: 3.256729]\n",
      "epoch:0 step:281 [D1 loss: 0.670698, acc.: 53.91%] [D2 loss: 0.547613, acc.: 67.97%] [G loss: 3.426749]\n",
      "epoch:0 step:282 [D1 loss: 0.699595, acc.: 53.91%] [D2 loss: 0.808558, acc.: 53.91%] [G loss: 4.125875]\n",
      "epoch:0 step:283 [D1 loss: 0.945150, acc.: 52.34%] [D2 loss: 0.698199, acc.: 56.25%] [G loss: 2.980856]\n",
      "epoch:0 step:284 [D1 loss: 0.681498, acc.: 65.62%] [D2 loss: 0.575185, acc.: 67.97%] [G loss: 3.069182]\n",
      "epoch:0 step:285 [D1 loss: 0.678863, acc.: 59.38%] [D2 loss: 0.565672, acc.: 66.41%] [G loss: 3.238348]\n",
      "epoch:0 step:286 [D1 loss: 0.624112, acc.: 62.50%] [D2 loss: 0.571554, acc.: 67.19%] [G loss: 3.165508]\n",
      "epoch:0 step:287 [D1 loss: 0.736094, acc.: 48.44%] [D2 loss: 0.664190, acc.: 60.16%] [G loss: 3.437650]\n",
      "epoch:0 step:288 [D1 loss: 0.762696, acc.: 46.88%] [D2 loss: 0.575550, acc.: 65.62%] [G loss: 3.158721]\n",
      "epoch:0 step:289 [D1 loss: 0.634681, acc.: 64.84%] [D2 loss: 0.506905, acc.: 82.03%] [G loss: 3.342375]\n",
      "epoch:0 step:290 [D1 loss: 0.746942, acc.: 52.34%] [D2 loss: 0.578819, acc.: 72.66%] [G loss: 3.058544]\n",
      "epoch:0 step:291 [D1 loss: 0.662549, acc.: 60.16%] [D2 loss: 0.821331, acc.: 58.59%] [G loss: 3.295902]\n",
      "epoch:0 step:292 [D1 loss: 0.687132, acc.: 60.16%] [D2 loss: 0.595918, acc.: 68.75%] [G loss: 3.105305]\n",
      "epoch:0 step:293 [D1 loss: 0.686968, acc.: 61.72%] [D2 loss: 0.617325, acc.: 64.84%] [G loss: 2.946885]\n",
      "epoch:0 step:294 [D1 loss: 0.667527, acc.: 58.59%] [D2 loss: 0.487599, acc.: 75.78%] [G loss: 3.060337]\n",
      "epoch:0 step:295 [D1 loss: 0.665110, acc.: 57.03%] [D2 loss: 0.553849, acc.: 76.56%] [G loss: 3.233430]\n",
      "epoch:0 step:296 [D1 loss: 0.692457, acc.: 57.81%] [D2 loss: 0.481047, acc.: 79.69%] [G loss: 3.142772]\n",
      "epoch:0 step:297 [D1 loss: 0.660810, acc.: 56.25%] [D2 loss: 0.533859, acc.: 74.22%] [G loss: 3.170254]\n",
      "epoch:0 step:298 [D1 loss: 0.670181, acc.: 60.94%] [D2 loss: 0.644721, acc.: 63.28%] [G loss: 3.016791]\n",
      "epoch:0 step:299 [D1 loss: 0.737060, acc.: 50.78%] [D2 loss: 0.552131, acc.: 74.22%] [G loss: 3.420542]\n",
      "epoch:0 step:300 [D1 loss: 0.680043, acc.: 57.03%] [D2 loss: 0.678292, acc.: 57.03%] [G loss: 3.915142]\n",
      "epoch:0 step:301 [D1 loss: 0.886085, acc.: 51.56%] [D2 loss: 0.583057, acc.: 69.53%] [G loss: 3.216278]\n",
      "epoch:0 step:302 [D1 loss: 0.717486, acc.: 58.59%] [D2 loss: 0.549089, acc.: 71.09%] [G loss: 2.906421]\n",
      "epoch:0 step:303 [D1 loss: 0.673672, acc.: 62.50%] [D2 loss: 0.607804, acc.: 73.44%] [G loss: 3.366551]\n",
      "epoch:0 step:304 [D1 loss: 0.673254, acc.: 57.81%] [D2 loss: 0.573159, acc.: 64.06%] [G loss: 3.033000]\n",
      "epoch:0 step:305 [D1 loss: 0.663519, acc.: 67.97%] [D2 loss: 0.569933, acc.: 65.62%] [G loss: 2.964274]\n",
      "epoch:0 step:306 [D1 loss: 0.720685, acc.: 61.72%] [D2 loss: 0.580922, acc.: 72.66%] [G loss: 3.188902]\n",
      "epoch:0 step:307 [D1 loss: 0.786162, acc.: 52.34%] [D2 loss: 0.569829, acc.: 68.75%] [G loss: 3.057215]\n",
      "epoch:0 step:308 [D1 loss: 0.611212, acc.: 68.75%] [D2 loss: 0.620703, acc.: 65.62%] [G loss: 2.952829]\n",
      "epoch:0 step:309 [D1 loss: 0.632531, acc.: 63.28%] [D2 loss: 0.531095, acc.: 72.66%] [G loss: 2.962765]\n",
      "epoch:0 step:310 [D1 loss: 0.665307, acc.: 56.25%] [D2 loss: 0.544741, acc.: 74.22%] [G loss: 2.957557]\n",
      "epoch:0 step:311 [D1 loss: 0.634454, acc.: 59.38%] [D2 loss: 0.478052, acc.: 78.12%] [G loss: 3.132234]\n",
      "epoch:0 step:312 [D1 loss: 0.666273, acc.: 60.16%] [D2 loss: 0.544806, acc.: 75.00%] [G loss: 3.983518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 step:313 [D1 loss: 0.870607, acc.: 57.03%] [D2 loss: 0.532602, acc.: 81.25%] [G loss: 3.441453]\n",
      "epoch:0 step:314 [D1 loss: 0.778739, acc.: 49.22%] [D2 loss: 0.540202, acc.: 78.12%] [G loss: 3.280724]\n",
      "epoch:0 step:315 [D1 loss: 0.709105, acc.: 47.66%] [D2 loss: 0.592575, acc.: 69.53%] [G loss: 3.027941]\n",
      "epoch:0 step:316 [D1 loss: 0.723248, acc.: 51.56%] [D2 loss: 0.626860, acc.: 62.50%] [G loss: 2.921394]\n",
      "epoch:0 step:317 [D1 loss: 0.650515, acc.: 67.19%] [D2 loss: 0.679467, acc.: 56.25%] [G loss: 2.886636]\n",
      "epoch:0 step:318 [D1 loss: 0.680366, acc.: 58.59%] [D2 loss: 0.602189, acc.: 64.06%] [G loss: 3.326323]\n",
      "epoch:0 step:319 [D1 loss: 0.728918, acc.: 53.12%] [D2 loss: 0.598242, acc.: 64.84%] [G loss: 3.176640]\n",
      "epoch:0 step:320 [D1 loss: 0.697174, acc.: 57.03%] [D2 loss: 0.609131, acc.: 66.41%] [G loss: 3.126348]\n",
      "epoch:0 step:321 [D1 loss: 0.624062, acc.: 64.84%] [D2 loss: 0.570539, acc.: 71.88%] [G loss: 3.298130]\n",
      "epoch:0 step:322 [D1 loss: 0.665827, acc.: 65.62%] [D2 loss: 0.566722, acc.: 68.75%] [G loss: 2.917744]\n",
      "epoch:0 step:323 [D1 loss: 0.695556, acc.: 55.47%] [D2 loss: 0.545118, acc.: 76.56%] [G loss: 3.269426]\n",
      "epoch:0 step:324 [D1 loss: 0.643049, acc.: 58.59%] [D2 loss: 0.690143, acc.: 58.59%] [G loss: 2.888918]\n",
      "epoch:0 step:325 [D1 loss: 0.625290, acc.: 70.31%] [D2 loss: 0.513034, acc.: 75.00%] [G loss: 3.013721]\n",
      "epoch:0 step:326 [D1 loss: 0.635876, acc.: 67.19%] [D2 loss: 0.578955, acc.: 74.22%] [G loss: 3.040323]\n",
      "epoch:0 step:327 [D1 loss: 0.662385, acc.: 57.81%] [D2 loss: 0.562631, acc.: 69.53%] [G loss: 3.376114]\n",
      "epoch:0 step:328 [D1 loss: 0.693396, acc.: 52.34%] [D2 loss: 0.489241, acc.: 81.25%] [G loss: 3.154343]\n",
      "epoch:0 step:329 [D1 loss: 0.642683, acc.: 60.94%] [D2 loss: 0.566529, acc.: 69.53%] [G loss: 3.134606]\n",
      "epoch:0 step:330 [D1 loss: 0.680656, acc.: 57.03%] [D2 loss: 0.544265, acc.: 75.00%] [G loss: 3.270526]\n",
      "epoch:0 step:331 [D1 loss: 0.704687, acc.: 57.81%] [D2 loss: 0.708344, acc.: 63.28%] [G loss: 3.256836]\n",
      "epoch:0 step:332 [D1 loss: 0.774780, acc.: 59.38%] [D2 loss: 0.579638, acc.: 66.41%] [G loss: 3.384460]\n",
      "epoch:0 step:333 [D1 loss: 0.721382, acc.: 57.03%] [D2 loss: 0.571402, acc.: 71.09%] [G loss: 3.179883]\n",
      "epoch:0 step:334 [D1 loss: 0.620533, acc.: 67.97%] [D2 loss: 0.637600, acc.: 62.50%] [G loss: 2.946056]\n",
      "epoch:0 step:335 [D1 loss: 0.573668, acc.: 74.22%] [D2 loss: 0.672143, acc.: 64.06%] [G loss: 3.225326]\n",
      "epoch:0 step:336 [D1 loss: 0.679699, acc.: 58.59%] [D2 loss: 0.543747, acc.: 75.00%] [G loss: 3.025061]\n",
      "epoch:0 step:337 [D1 loss: 0.715372, acc.: 53.12%] [D2 loss: 0.536786, acc.: 77.34%] [G loss: 2.886493]\n",
      "epoch:0 step:338 [D1 loss: 0.546987, acc.: 73.44%] [D2 loss: 0.534591, acc.: 68.75%] [G loss: 2.988431]\n",
      "epoch:0 step:339 [D1 loss: 0.687132, acc.: 58.59%] [D2 loss: 0.601602, acc.: 68.75%] [G loss: 3.159965]\n",
      "epoch:0 step:340 [D1 loss: 0.633030, acc.: 68.75%] [D2 loss: 0.572591, acc.: 70.31%] [G loss: 3.081998]\n",
      "epoch:0 step:341 [D1 loss: 0.635424, acc.: 66.41%] [D2 loss: 0.512814, acc.: 72.66%] [G loss: 3.177456]\n",
      "epoch:0 step:342 [D1 loss: 0.635757, acc.: 63.28%] [D2 loss: 0.684424, acc.: 57.03%] [G loss: 2.938909]\n",
      "epoch:0 step:343 [D1 loss: 0.634658, acc.: 67.97%] [D2 loss: 0.595949, acc.: 63.28%] [G loss: 3.157420]\n",
      "epoch:0 step:344 [D1 loss: 0.663435, acc.: 64.06%] [D2 loss: 0.587196, acc.: 64.84%] [G loss: 3.212163]\n",
      "epoch:0 step:345 [D1 loss: 0.765297, acc.: 52.34%] [D2 loss: 0.557493, acc.: 69.53%] [G loss: 3.090099]\n",
      "epoch:0 step:346 [D1 loss: 0.653765, acc.: 65.62%] [D2 loss: 0.510303, acc.: 77.34%] [G loss: 3.504529]\n",
      "epoch:0 step:347 [D1 loss: 0.799521, acc.: 43.75%] [D2 loss: 0.572089, acc.: 71.09%] [G loss: 3.075846]\n",
      "epoch:0 step:348 [D1 loss: 0.728665, acc.: 59.38%] [D2 loss: 0.652130, acc.: 62.50%] [G loss: 2.926028]\n",
      "epoch:0 step:349 [D1 loss: 0.573595, acc.: 71.88%] [D2 loss: 0.515982, acc.: 73.44%] [G loss: 3.125599]\n",
      "epoch:0 step:350 [D1 loss: 0.628285, acc.: 62.50%] [D2 loss: 0.633197, acc.: 70.31%] [G loss: 3.243840]\n",
      "epoch:0 step:351 [D1 loss: 0.755701, acc.: 55.47%] [D2 loss: 0.571966, acc.: 72.66%] [G loss: 3.396072]\n",
      "epoch:0 step:352 [D1 loss: 0.708794, acc.: 64.84%] [D2 loss: 0.527711, acc.: 73.44%] [G loss: 3.154853]\n",
      "epoch:0 step:353 [D1 loss: 0.590493, acc.: 64.84%] [D2 loss: 0.565480, acc.: 72.66%] [G loss: 3.230416]\n",
      "epoch:0 step:354 [D1 loss: 0.688594, acc.: 57.81%] [D2 loss: 0.558022, acc.: 70.31%] [G loss: 3.188473]\n",
      "epoch:0 step:355 [D1 loss: 0.622020, acc.: 68.75%] [D2 loss: 0.514042, acc.: 74.22%] [G loss: 3.220055]\n",
      "epoch:0 step:356 [D1 loss: 0.636411, acc.: 67.97%] [D2 loss: 0.502768, acc.: 77.34%] [G loss: 3.008093]\n",
      "epoch:0 step:357 [D1 loss: 0.617905, acc.: 67.19%] [D2 loss: 0.610261, acc.: 65.62%] [G loss: 3.120668]\n",
      "epoch:0 step:358 [D1 loss: 0.620108, acc.: 67.19%] [D2 loss: 0.534431, acc.: 77.34%] [G loss: 3.547004]\n",
      "epoch:0 step:359 [D1 loss: 0.706659, acc.: 48.44%] [D2 loss: 0.514518, acc.: 76.56%] [G loss: 3.397083]\n",
      "epoch:0 step:360 [D1 loss: 0.651143, acc.: 64.84%] [D2 loss: 0.571754, acc.: 68.75%] [G loss: 3.093955]\n",
      "epoch:0 step:361 [D1 loss: 0.638716, acc.: 62.50%] [D2 loss: 0.569679, acc.: 71.88%] [G loss: 3.625974]\n",
      "epoch:0 step:362 [D1 loss: 0.719957, acc.: 57.81%] [D2 loss: 0.641252, acc.: 63.28%] [G loss: 3.224786]\n",
      "epoch:0 step:363 [D1 loss: 0.778092, acc.: 53.91%] [D2 loss: 0.577891, acc.: 70.31%] [G loss: 3.246844]\n",
      "epoch:0 step:364 [D1 loss: 0.754318, acc.: 58.59%] [D2 loss: 0.604860, acc.: 64.84%] [G loss: 3.252769]\n",
      "epoch:0 step:365 [D1 loss: 0.778710, acc.: 52.34%] [D2 loss: 0.687582, acc.: 57.81%] [G loss: 2.964843]\n",
      "epoch:0 step:366 [D1 loss: 0.622575, acc.: 64.84%] [D2 loss: 0.608579, acc.: 67.19%] [G loss: 3.018278]\n",
      "epoch:0 step:367 [D1 loss: 0.700159, acc.: 62.50%] [D2 loss: 0.629459, acc.: 63.28%] [G loss: 3.079492]\n",
      "epoch:0 step:368 [D1 loss: 0.615544, acc.: 65.62%] [D2 loss: 0.592283, acc.: 71.09%] [G loss: 3.253513]\n",
      "epoch:0 step:369 [D1 loss: 0.794338, acc.: 60.16%] [D2 loss: 0.626180, acc.: 69.53%] [G loss: 3.141698]\n",
      "epoch:0 step:370 [D1 loss: 0.678073, acc.: 60.94%] [D2 loss: 0.580293, acc.: 70.31%] [G loss: 3.133609]\n",
      "epoch:0 step:371 [D1 loss: 0.600454, acc.: 67.97%] [D2 loss: 0.572129, acc.: 64.06%] [G loss: 3.020780]\n",
      "epoch:0 step:372 [D1 loss: 0.618782, acc.: 59.38%] [D2 loss: 0.585415, acc.: 67.97%] [G loss: 3.441868]\n",
      "epoch:0 step:373 [D1 loss: 0.644546, acc.: 57.81%] [D2 loss: 0.585398, acc.: 68.75%] [G loss: 3.089412]\n",
      "epoch:0 step:374 [D1 loss: 0.612065, acc.: 69.53%] [D2 loss: 0.502899, acc.: 74.22%] [G loss: 3.154077]\n",
      "epoch:0 step:375 [D1 loss: 0.656315, acc.: 65.62%] [D2 loss: 0.529124, acc.: 75.00%] [G loss: 3.007984]\n",
      "epoch:0 step:376 [D1 loss: 0.607045, acc.: 69.53%] [D2 loss: 0.495377, acc.: 77.34%] [G loss: 3.382019]\n",
      "epoch:0 step:377 [D1 loss: 0.651635, acc.: 60.16%] [D2 loss: 0.559689, acc.: 70.31%] [G loss: 3.216333]\n",
      "epoch:0 step:378 [D1 loss: 0.634717, acc.: 60.16%] [D2 loss: 0.624473, acc.: 65.62%] [G loss: 3.272017]\n",
      "epoch:0 step:379 [D1 loss: 0.685737, acc.: 60.94%] [D2 loss: 0.552875, acc.: 67.19%] [G loss: 3.376197]\n",
      "epoch:0 step:380 [D1 loss: 0.692451, acc.: 57.03%] [D2 loss: 0.571618, acc.: 71.09%] [G loss: 3.372574]\n",
      "epoch:0 step:381 [D1 loss: 0.673012, acc.: 60.16%] [D2 loss: 0.647824, acc.: 62.50%] [G loss: 3.192636]\n",
      "epoch:0 step:382 [D1 loss: 0.764525, acc.: 52.34%] [D2 loss: 0.477783, acc.: 80.47%] [G loss: 3.567704]\n",
      "epoch:0 step:383 [D1 loss: 0.768218, acc.: 55.47%] [D2 loss: 0.686974, acc.: 58.59%] [G loss: 3.407147]\n",
      "epoch:0 step:384 [D1 loss: 0.936277, acc.: 56.25%] [D2 loss: 0.596314, acc.: 63.28%] [G loss: 2.943800]\n",
      "epoch:0 step:385 [D1 loss: 0.616251, acc.: 65.62%] [D2 loss: 0.497936, acc.: 81.25%] [G loss: 3.232574]\n",
      "epoch:0 step:386 [D1 loss: 0.650082, acc.: 61.72%] [D2 loss: 0.637457, acc.: 64.84%] [G loss: 3.120920]\n",
      "epoch:0 step:387 [D1 loss: 0.669330, acc.: 59.38%] [D2 loss: 0.556077, acc.: 74.22%] [G loss: 2.994697]\n",
      "epoch:0 step:388 [D1 loss: 0.604545, acc.: 70.31%] [D2 loss: 0.585221, acc.: 63.28%] [G loss: 2.974223]\n",
      "epoch:0 step:389 [D1 loss: 0.618045, acc.: 68.75%] [D2 loss: 0.571340, acc.: 68.75%] [G loss: 3.290688]\n",
      "epoch:0 step:390 [D1 loss: 0.646248, acc.: 61.72%] [D2 loss: 0.591298, acc.: 69.53%] [G loss: 3.162475]\n",
      "epoch:0 step:391 [D1 loss: 0.568073, acc.: 68.75%] [D2 loss: 0.505058, acc.: 75.00%] [G loss: 3.046484]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9f0e8d99ed9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-9f0e8d99ed9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0md1_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0md2_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0md1_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0md2_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0md1_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md1_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     78\u001b[0m                              'for each key in: ' + str(names))\n\u001b[1;32m     79\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class COGAN():\n",
    "    \"\"\"Reference: https://wiseodd.github.io/techblog/2017/02/18/coupled_gan/\"\"\"\n",
    "    def __init__(self):\n",
    "        self.img_rows = 32\n",
    "        self.img_cols = 32\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.d1, self.d2 = self.build_discriminators()\n",
    "        self.d1.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d2.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.g1, self.g2 = self.build_generators()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img1 = self.g1(z)\n",
    "        img2 = self.g2(z)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d1.trainable = False\n",
    "        self.d2.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid1 = self.d1(img1)\n",
    "        valid2 = self.d2(img2)\n",
    "\n",
    "        # The combined model  (stacked generators and discriminators)\n",
    "        # Trains generators to fool discriminators\n",
    "        self.combined = Model(z, [valid1, valid2])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
    "                                    optimizer=optimizer)\n",
    "\n",
    "    def build_generators(self):\n",
    "\n",
    "        # Shared weights between generators\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        feature_repr = model(noise)\n",
    "\n",
    "        # Generator 1\n",
    "        g1 = Dense(1024)(feature_repr)\n",
    "        g1 = LeakyReLU(alpha=0.2)(g1)\n",
    "        g1 = BatchNormalization(momentum=0.8)(g1)\n",
    "        g1 = Dense(np.prod(self.img_shape), activation='tanh')(g1)\n",
    "        img1 = Reshape(self.img_shape)(g1)\n",
    "\n",
    "        # Generator 2\n",
    "        g2 = Dense(1024)(feature_repr)\n",
    "        g2 = LeakyReLU(alpha=0.2)(g2)\n",
    "        g2 = BatchNormalization(momentum=0.8)(g2)\n",
    "        g2 = Dense(np.prod(self.img_shape), activation='tanh')(g2)\n",
    "        img2 = Reshape(self.img_shape)(g2)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return Model(noise, img1), Model(noise, img2)\n",
    "\n",
    "    def build_discriminators(self):\n",
    "\n",
    "        img1 = Input(shape=self.img_shape)\n",
    "        img2 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Shared discriminator layers\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        img1_embedding = model(img1)\n",
    "        img2_embedding = model(img2)\n",
    "\n",
    "        # Discriminator 1\n",
    "        validity1 = Dense(1, activation='sigmoid')(img1_embedding)\n",
    "        # Discriminator 2\n",
    "        validity2 = Dense(1, activation='sigmoid')(img2_embedding)\n",
    "\n",
    "        return Model(img1, validity1), Model(img2, validity2)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = cifar10.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        # X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Images in domain A and B (rotated)\n",
    "        X1 = X_train[:int(X_train.shape[0]//2)]\n",
    "        X2 = X_train[int(X_train.shape[0]//2):]\n",
    "        X2 = scipy.ndimage.interpolation.rotate(X2, 90, axes=(1, 2))\n",
    "\n",
    "        \n",
    "\n",
    "        nb_batches = int(X_train.shape[0] // batch_size)\n",
    "        global_step = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                # imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "                imgs1 = X1[index * batch_size:(index + 1) * batch_size]\n",
    "                imgs2 = X2[index * batch_size:(index + 1) * batch_size]\n",
    "                # Adversarial ground truths\n",
    "                valid = np.ones((len(imgs1), 1))\n",
    "                fake = np.zeros((len(imgs2), 1))\n",
    "\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (len(imgs1), 100))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs1 = self.g1.predict(noise)\n",
    "                gen_imgs2 = self.g2.predict(noise)\n",
    "\n",
    "                # Train the discriminators\n",
    "                d1_loss_real = self.d1.train_on_batch(imgs1, valid)\n",
    "                d2_loss_real = self.d2.train_on_batch(imgs2, valid)\n",
    "                d1_loss_fake = self.d1.train_on_batch(gen_imgs1, fake)\n",
    "                d2_loss_fake = self.d2.train_on_batch(gen_imgs2, fake)\n",
    "                d1_loss = 0.5 * np.add(d1_loss_real, d1_loss_fake)\n",
    "                d2_loss = 0.5 * np.add(d2_loss_real, d2_loss_fake)\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                g_loss = self.combined.train_on_batch(noise, [valid, valid])\n",
    "\n",
    "                # Plot the progress\n",
    "                print(\"epoch:%d step:%d [D1 loss: %f, acc.: %.2f%%] [D2 loss: %f, acc.: %.2f%%] [G loss: %f]\" \\\n",
    "                      % (epoch, global_step, d1_loss[0], 100 * d1_loss[1], d2_loss[0], 100 * d2_loss[1], g_loss[0]))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if  global_step % sample_interval == 0:\n",
    "                    self.sample_images(epoch, global_step)\n",
    "\n",
    "    def sample_images(self, epoch,global_step):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * int(c//2), 100))\n",
    "        gen_imgs1 = self.g1.predict(noise)\n",
    "        gen_imgs2 = self.g2.predict(noise)\n",
    "\n",
    "        generated_images = np.concatenate([gen_imgs1, gen_imgs2])\n",
    "\n",
    "        generated_images = np.asarray((generated_images * 127.5 + 127.5).astype(np.uint8))\n",
    "\n",
    "        def vis_square(data, padsize=1, padval=0):\n",
    "            # force the number of filters to be square\n",
    "            n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "            padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "            data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
    "\n",
    "            # tile the filters into an image\n",
    "            data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "            data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "            return data\n",
    "\n",
    "        img = vis_square(generated_images)\n",
    "        if not os.path.isdir('images_cogan_cifar10'):\n",
    "            os.mkdir('images_cogan_cifar10')\n",
    "        Image.fromarray(img).save(\n",
    "            \"images_cogan_cifar10/epoch_%d_step_%d.png\" % (epoch, global_step))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = COGAN()\n",
    "    gan.train(epochs=50, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
