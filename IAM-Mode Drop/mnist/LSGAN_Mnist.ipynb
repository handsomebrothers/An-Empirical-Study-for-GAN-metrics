{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import util\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import util\n",
    "import torch\n",
    "from keras.utils.np_utils import *\n",
    "from keras.datasets import mnist\n",
    "MNIST_CLASSIFIER_FROZEN_GRAPH ='/home/imi432_006/huangdengrong/GAN-Research/metrics_gan/metrics/classify_mnist_graph_def.pb'\n",
    "INPUT_TENSOR = 'inputs:0'\n",
    "OUTPUT_TENSOR = 'logits:0'\n",
    "def calculate_labels(images,epoch,global_step):\n",
    "    label_dict={}\n",
    "    for i in range(10):\n",
    "        label_dict[i]=0\n",
    "    eval_images = tf.convert_to_tensor(images)\n",
    "    y_logit = util.mnist_logits(eval_images, MNIST_CLASSIFIER_FROZEN_GRAPH, INPUT_TENSOR, OUTPUT_TENSOR)\n",
    "    labels=tf.argmax(y_logit, 1)\n",
    "    labels=tf.Session().run(labels)\n",
    "    for data in labels:\n",
    "        label_dict[data]+=1\n",
    "    for i in range(10):\n",
    "        label_dict[i]=label_dict[i]/len(images)\n",
    "    max_value=max(label_dict.values())\n",
    "    min_value=min(label_dict.values())\n",
    "    print('epoch:%d   global_step:%d'%(epoch,global_step))\n",
    "    print(label_dict)\n",
    "    print('chazhi:%.8f'%(max_value-min_value))\n",
    "    return label_dict,max_value-min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/imi432_006/anaconda3/envs/tf/lib/python3.5/site-packages/tensorflow/contrib/gan/python/eval/python/classifier_metrics_impl.py:185: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "epoch:0   global_step:200\n",
      "{0: 0.0005, 1: 0.0, 2: 0.0, 3: 0.0097, 4: 0.0115, 5: 0.0, 6: 0.0, 7: 0.0001, 8: 0.9782, 9: 0.0}\n",
      "chazhi:0.97820000\n",
      "epoch:0   global_step:400\n",
      "{0: 0.0001, 1: 0.0, 2: 0.0002, 3: 0.0001, 4: 0.0449, 5: 0.0, 6: 0.0, 7: 0.0008, 8: 0.9539, 9: 0.0}\n",
      "chazhi:0.95390000\n",
      "epoch:0   global_step:600\n",
      "{0: 0.0059, 1: 0.0, 2: 0.0002, 3: 0.0536, 4: 0.0425, 5: 0.0002, 6: 0.0, 7: 0.0443, 8: 0.8533, 9: 0.0}\n",
      "chazhi:0.85330000\n",
      "epoch:0   global_step:800\n",
      "{0: 0.2466, 1: 0.0332, 2: 0.0344, 3: 0.0519, 4: 0.0002, 5: 0.0152, 6: 0.0, 7: 0.1555, 8: 0.4029, 9: 0.0601}\n",
      "chazhi:0.40290000\n",
      "epoch:1   global_step:1000\n",
      "{0: 0.1636, 1: 0.0171, 2: 0.037, 3: 0.2102, 4: 0.1546, 5: 0.0602, 6: 0.0092, 7: 0.069, 8: 0.2533, 9: 0.0258}\n",
      "chazhi:0.24410000\n",
      "epoch:1   global_step:1200\n",
      "{0: 0.0816, 1: 0.0024, 2: 0.0478, 3: 0.1784, 4: 0.1342, 5: 0.0244, 6: 0.0322, 7: 0.1345, 8: 0.261, 9: 0.1035}\n",
      "chazhi:0.25860000\n",
      "epoch:1   global_step:1400\n",
      "{0: 0.0356, 1: 0.0186, 2: 0.0681, 3: 0.1598, 4: 0.153, 5: 0.0261, 6: 0.0206, 7: 0.1854, 8: 0.276, 9: 0.0568}\n",
      "chazhi:0.25740000\n",
      "epoch:1   global_step:1600\n",
      "{0: 0.0152, 1: 0.0247, 2: 0.0847, 3: 0.1247, 4: 0.2347, 5: 0.0096, 6: 0.0004, 7: 0.2172, 8: 0.2415, 9: 0.0473}\n",
      "chazhi:0.24110000\n",
      "epoch:1   global_step:1800\n",
      "{0: 0.0223, 1: 0.037, 2: 0.066, 3: 0.2239, 4: 0.1204, 5: 0.0229, 6: 0.0282, 7: 0.2091, 8: 0.216, 9: 0.0542}\n",
      "chazhi:0.20160000\n",
      "epoch:2   global_step:2000\n",
      "{0: 0.0222, 1: 0.0273, 2: 0.0902, 3: 0.1241, 4: 0.2708, 5: 0.03, 6: 0.0074, 7: 0.1807, 8: 0.1805, 9: 0.0668}\n",
      "chazhi:0.26340000\n",
      "epoch:2   global_step:2200\n",
      "{0: 0.0502, 1: 0.0348, 2: 0.1004, 3: 0.1659, 4: 0.1209, 5: 0.0136, 6: 0.0198, 7: 0.2257, 8: 0.19, 9: 0.0787}\n",
      "chazhi:0.21210000\n",
      "epoch:2   global_step:2400\n",
      "{0: 0.0531, 1: 0.044, 2: 0.0961, 3: 0.1266, 4: 0.1023, 5: 0.028, 6: 0.0566, 7: 0.1915, 8: 0.217, 9: 0.0848}\n",
      "chazhi:0.18900000\n",
      "epoch:2   global_step:2600\n",
      "{0: 0.0679, 1: 0.0389, 2: 0.0742, 3: 0.1329, 4: 0.1149, 5: 0.0356, 6: 0.0575, 7: 0.1716, 8: 0.2051, 9: 0.1014}\n",
      "chazhi:0.16950000\n",
      "epoch:2   global_step:2800\n",
      "{0: 0.0619, 1: 0.0396, 2: 0.0744, 3: 0.182, 4: 0.1353, 5: 0.0303, 6: 0.0455, 7: 0.1435, 8: 0.1802, 9: 0.1073}\n",
      "chazhi:0.15170000\n",
      "epoch:3   global_step:3000\n",
      "{0: 0.0569, 1: 0.0861, 2: 0.0633, 3: 0.1854, 4: 0.1184, 5: 0.0294, 6: 0.0347, 7: 0.1848, 8: 0.1548, 9: 0.0862}\n",
      "chazhi:0.15600000\n",
      "epoch:3   global_step:3200\n",
      "{0: 0.0737, 1: 0.0625, 2: 0.0672, 3: 0.165, 4: 0.1301, 5: 0.0386, 6: 0.0472, 7: 0.1737, 8: 0.1514, 9: 0.0906}\n",
      "chazhi:0.13510000\n",
      "epoch:3   global_step:3400\n",
      "{0: 0.0791, 1: 0.0614, 2: 0.0662, 3: 0.1535, 4: 0.1247, 5: 0.0291, 6: 0.0584, 7: 0.1924, 8: 0.167, 9: 0.0682}\n",
      "chazhi:0.16330000\n",
      "epoch:3   global_step:3600\n",
      "{0: 0.0885, 1: 0.0557, 2: 0.0761, 3: 0.1258, 4: 0.117, 5: 0.0359, 6: 0.0834, 7: 0.1591, 8: 0.1565, 9: 0.102}\n",
      "chazhi:0.12320000\n",
      "epoch:4   global_step:3800\n",
      "{0: 0.0694, 1: 0.0614, 2: 0.0793, 3: 0.1438, 4: 0.1315, 5: 0.0411, 6: 0.0519, 7: 0.1734, 8: 0.1739, 9: 0.0743}\n",
      "chazhi:0.13280000\n",
      "epoch:4   global_step:4000\n",
      "{0: 0.0612, 1: 0.0745, 2: 0.0698, 3: 0.131, 4: 0.1394, 5: 0.0371, 6: 0.0628, 7: 0.1638, 8: 0.1745, 9: 0.0859}\n",
      "chazhi:0.13740000\n",
      "epoch:4   global_step:4200\n",
      "{0: 0.0569, 1: 0.0799, 2: 0.0685, 3: 0.1248, 4: 0.1292, 5: 0.0368, 6: 0.0754, 7: 0.1471, 8: 0.1697, 9: 0.1117}\n",
      "chazhi:0.13290000\n",
      "epoch:4   global_step:4400\n",
      "{0: 0.0578, 1: 0.0816, 2: 0.0692, 3: 0.1389, 4: 0.1262, 5: 0.0446, 6: 0.0813, 7: 0.1864, 8: 0.1269, 9: 0.0871}\n",
      "chazhi:0.14180000\n",
      "epoch:4   global_step:4600\n",
      "{0: 0.0548, 1: 0.0717, 2: 0.0817, 3: 0.1307, 4: 0.1306, 5: 0.0505, 6: 0.0778, 7: 0.1664, 8: 0.1414, 9: 0.0944}\n",
      "chazhi:0.11590000\n",
      "epoch:5   global_step:4800\n",
      "{0: 0.0587, 1: 0.0813, 2: 0.064, 3: 0.1348, 4: 0.1567, 5: 0.0384, 6: 0.0736, 7: 0.1523, 8: 0.1375, 9: 0.1027}\n",
      "chazhi:0.11830000\n",
      "epoch:5   global_step:5000\n",
      "{0: 0.0573, 1: 0.1001, 2: 0.0718, 3: 0.1287, 4: 0.1465, 5: 0.0414, 6: 0.0504, 7: 0.1522, 8: 0.1369, 9: 0.1147}\n",
      "chazhi:0.11080000\n",
      "epoch:5   global_step:5200\n",
      "{0: 0.0672, 1: 0.088, 2: 0.0601, 3: 0.1199, 4: 0.1279, 5: 0.0523, 6: 0.0684, 7: 0.1484, 8: 0.1419, 9: 0.1259}\n",
      "chazhi:0.09610000\n",
      "epoch:5   global_step:5400\n",
      "{0: 0.0824, 1: 0.0894, 2: 0.0693, 3: 0.154, 4: 0.1192, 5: 0.0563, 6: 0.0504, 7: 0.1516, 8: 0.1348, 9: 0.0926}\n",
      "chazhi:0.10360000\n",
      "epoch:5   global_step:5600\n",
      "{0: 0.0699, 1: 0.0845, 2: 0.0752, 3: 0.1556, 4: 0.1552, 5: 0.0435, 6: 0.0618, 7: 0.1224, 8: 0.1499, 9: 0.082}\n",
      "chazhi:0.11210000\n",
      "epoch:6   global_step:5800\n",
      "{0: 0.0705, 1: 0.0909, 2: 0.0683, 3: 0.1443, 4: 0.1524, 5: 0.0401, 6: 0.0616, 7: 0.1446, 8: 0.1282, 9: 0.0991}\n",
      "chazhi:0.11230000\n",
      "epoch:6   global_step:6000\n",
      "{0: 0.0573, 1: 0.1012, 2: 0.0641, 3: 0.1249, 4: 0.1462, 5: 0.0544, 6: 0.0729, 7: 0.1288, 8: 0.1476, 9: 0.1026}\n",
      "chazhi:0.09320000\n",
      "epoch:6   global_step:6200\n",
      "{0: 0.0815, 1: 0.0994, 2: 0.0565, 3: 0.0992, 4: 0.1409, 5: 0.0485, 6: 0.0722, 7: 0.1342, 8: 0.1536, 9: 0.114}\n",
      "chazhi:0.10510000\n",
      "epoch:6   global_step:6400\n",
      "{0: 0.0952, 1: 0.098, 2: 0.0786, 3: 0.1291, 4: 0.1484, 5: 0.0444, 6: 0.0608, 7: 0.1244, 8: 0.1374, 9: 0.0837}\n",
      "chazhi:0.10400000\n",
      "epoch:7   global_step:6600\n",
      "{0: 0.0738, 1: 0.0921, 2: 0.0653, 3: 0.1457, 4: 0.1285, 5: 0.0428, 6: 0.0567, 7: 0.1579, 8: 0.148, 9: 0.0892}\n",
      "chazhi:0.11510000\n",
      "epoch:7   global_step:6800\n",
      "{0: 0.0864, 1: 0.0981, 2: 0.0547, 3: 0.1279, 4: 0.1264, 5: 0.0558, 6: 0.0547, 7: 0.1489, 8: 0.1445, 9: 0.1026}\n",
      "chazhi:0.09420000\n",
      "epoch:7   global_step:7000\n",
      "{0: 0.087, 1: 0.0826, 2: 0.0755, 3: 0.1218, 4: 0.1343, 5: 0.0573, 6: 0.0653, 7: 0.1489, 8: 0.1325, 9: 0.0948}\n",
      "chazhi:0.09160000\n",
      "epoch:7   global_step:7200\n",
      "{0: 0.0869, 1: 0.0924, 2: 0.0584, 3: 0.1118, 4: 0.1531, 5: 0.0527, 6: 0.0767, 7: 0.1475, 8: 0.1381, 9: 0.0824}\n",
      "chazhi:0.10040000\n",
      "epoch:7   global_step:7400\n",
      "{0: 0.0771, 1: 0.1093, 2: 0.0607, 3: 0.1234, 4: 0.1207, 5: 0.055, 6: 0.0812, 7: 0.1658, 8: 0.1142, 9: 0.0926}\n",
      "chazhi:0.11080000\n",
      "epoch:8   global_step:7600\n",
      "{0: 0.0858, 1: 0.103, 2: 0.0569, 3: 0.1202, 4: 0.143, 5: 0.0465, 6: 0.0822, 7: 0.1569, 8: 0.106, 9: 0.0995}\n",
      "chazhi:0.11040000\n",
      "epoch:8   global_step:7800\n",
      "{0: 0.0937, 1: 0.1031, 2: 0.0698, 3: 0.0949, 4: 0.1286, 5: 0.04, 6: 0.0716, 7: 0.1721, 8: 0.1155, 9: 0.1107}\n",
      "chazhi:0.13210000\n",
      "epoch:8   global_step:8000\n",
      "{0: 0.0701, 1: 0.118, 2: 0.075, 3: 0.1013, 4: 0.1197, 5: 0.0548, 6: 0.0735, 7: 0.1466, 8: 0.1453, 9: 0.0957}\n",
      "chazhi:0.09180000\n",
      "epoch:8   global_step:8200\n",
      "{0: 0.0848, 1: 0.0985, 2: 0.0757, 3: 0.1023, 4: 0.1441, 5: 0.0483, 6: 0.0717, 7: 0.1356, 8: 0.1341, 9: 0.1049}\n",
      "chazhi:0.09580000\n",
      "epoch:8   global_step:8400\n",
      "{0: 0.0754, 1: 0.0978, 2: 0.0664, 3: 0.1288, 4: 0.1237, 5: 0.0513, 6: 0.0697, 7: 0.1521, 8: 0.133, 9: 0.1018}\n",
      "chazhi:0.10080000\n",
      "epoch:9   global_step:8600\n",
      "{0: 0.0959, 1: 0.1306, 2: 0.0536, 3: 0.1056, 4: 0.1202, 5: 0.0536, 6: 0.0759, 7: 0.1398, 8: 0.118, 9: 0.1068}\n",
      "chazhi:0.08620000\n",
      "epoch:9   global_step:8800\n",
      "{0: 0.0768, 1: 0.1099, 2: 0.0689, 3: 0.0992, 4: 0.1463, 5: 0.0378, 6: 0.0836, 7: 0.1412, 8: 0.1262, 9: 0.1101}\n",
      "chazhi:0.10850000\n",
      "epoch:9   global_step:9000\n",
      "{0: 0.0768, 1: 0.0904, 2: 0.0553, 3: 0.1166, 4: 0.1224, 5: 0.0622, 6: 0.0956, 7: 0.1306, 8: 0.1512, 9: 0.0989}\n",
      "chazhi:0.09590000\n",
      "epoch:9   global_step:9200\n",
      "{0: 0.0948, 1: 0.1169, 2: 0.06, 3: 0.0972, 4: 0.15, 5: 0.0555, 6: 0.091, 7: 0.135, 8: 0.1165, 9: 0.0831}\n",
      "chazhi:0.09450000\n",
      "epoch:10   global_step:9400\n",
      "{0: 0.0726, 1: 0.0954, 2: 0.0518, 3: 0.127, 4: 0.1524, 5: 0.0459, 6: 0.0717, 7: 0.1632, 8: 0.1197, 9: 0.1003}\n",
      "chazhi:0.11730000\n",
      "epoch:10   global_step:9600\n",
      "{0: 0.0888, 1: 0.0975, 2: 0.0467, 3: 0.129, 4: 0.1513, 5: 0.0489, 6: 0.0696, 7: 0.1525, 8: 0.1134, 9: 0.1023}\n",
      "chazhi:0.10580000\n",
      "epoch:10   global_step:9800\n",
      "{0: 0.094, 1: 0.1045, 2: 0.0643, 3: 0.1032, 4: 0.1332, 5: 0.0591, 6: 0.086, 7: 0.1267, 8: 0.126, 9: 0.103}\n",
      "chazhi:0.07410000\n",
      "epoch:10   global_step:10000\n",
      "{0: 0.0995, 1: 0.1091, 2: 0.0665, 3: 0.1089, 4: 0.144, 5: 0.055, 6: 0.0777, 7: 0.1293, 8: 0.1286, 9: 0.0814}\n",
      "chazhi:0.08900000\n",
      "epoch:10   global_step:10200\n",
      "{0: 0.0845, 1: 0.1006, 2: 0.0641, 3: 0.1128, 4: 0.1332, 5: 0.0551, 6: 0.0873, 7: 0.1335, 8: 0.1471, 9: 0.0818}\n",
      "chazhi:0.09200000\n",
      "epoch:11   global_step:10400\n",
      "{0: 0.0824, 1: 0.11, 2: 0.0478, 3: 0.1276, 4: 0.1382, 5: 0.0642, 6: 0.0687, 7: 0.1351, 8: 0.1239, 9: 0.1021}\n",
      "chazhi:0.09040000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11   global_step:10600\n",
      "{0: 0.097, 1: 0.1132, 2: 0.0663, 3: 0.12, 4: 0.1043, 5: 0.047, 6: 0.0712, 7: 0.1535, 8: 0.128, 9: 0.0995}\n",
      "chazhi:0.10650000\n",
      "epoch:11   global_step:10800\n",
      "{0: 0.0991, 1: 0.0945, 2: 0.0701, 3: 0.112, 4: 0.1312, 5: 0.0482, 6: 0.0882, 7: 0.1378, 8: 0.1362, 9: 0.0827}\n",
      "chazhi:0.08960000\n",
      "epoch:11   global_step:11000\n",
      "{0: 0.0912, 1: 0.0968, 2: 0.0769, 3: 0.1074, 4: 0.1385, 5: 0.0521, 6: 0.0612, 7: 0.1459, 8: 0.1306, 9: 0.0994}\n",
      "chazhi:0.09380000\n",
      "epoch:11   global_step:11200\n",
      "{0: 0.083, 1: 0.1236, 2: 0.0659, 3: 0.119, 4: 0.109, 5: 0.0643, 6: 0.0811, 7: 0.1427, 8: 0.1245, 9: 0.0869}\n",
      "chazhi:0.07840000\n",
      "epoch:12   global_step:11400\n",
      "{0: 0.1186, 1: 0.1009, 2: 0.0641, 3: 0.1057, 4: 0.1297, 5: 0.0432, 6: 0.0759, 7: 0.1415, 8: 0.1095, 9: 0.1109}\n",
      "chazhi:0.09830000\n",
      "epoch:12   global_step:11600\n",
      "{0: 0.1007, 1: 0.1077, 2: 0.057, 3: 0.1043, 4: 0.1523, 5: 0.0388, 6: 0.0829, 7: 0.1425, 8: 0.1165, 9: 0.0973}\n",
      "chazhi:0.11350000\n",
      "epoch:12   global_step:11800\n",
      "{0: 0.1015, 1: 0.101, 2: 0.0603, 3: 0.0994, 4: 0.1587, 5: 0.0532, 6: 0.0795, 7: 0.1213, 8: 0.1214, 9: 0.1037}\n",
      "chazhi:0.10550000\n",
      "epoch:12   global_step:12000\n",
      "{0: 0.086, 1: 0.1052, 2: 0.079, 3: 0.0944, 4: 0.1384, 5: 0.0465, 6: 0.0823, 7: 0.1489, 8: 0.1257, 9: 0.0936}\n",
      "chazhi:0.10240000\n",
      "epoch:13   global_step:12200\n",
      "{0: 0.0973, 1: 0.106, 2: 0.0612, 3: 0.1146, 4: 0.1313, 5: 0.0662, 6: 0.0843, 7: 0.1189, 8: 0.122, 9: 0.0982}\n",
      "chazhi:0.07010000\n",
      "epoch:13   global_step:12400\n",
      "{0: 0.1012, 1: 0.1041, 2: 0.0625, 3: 0.1294, 4: 0.12, 5: 0.0525, 6: 0.0755, 7: 0.1523, 8: 0.1086, 9: 0.0939}\n",
      "chazhi:0.09980000\n",
      "epoch:13   global_step:12600\n",
      "{0: 0.1117, 1: 0.0935, 2: 0.0735, 3: 0.1036, 4: 0.11, 5: 0.0551, 6: 0.0718, 7: 0.1562, 8: 0.1321, 9: 0.0925}\n",
      "chazhi:0.10110000\n",
      "epoch:13   global_step:12800\n",
      "{0: 0.1092, 1: 0.1223, 2: 0.0583, 3: 0.1079, 4: 0.126, 5: 0.0587, 6: 0.086, 7: 0.1506, 8: 0.1078, 9: 0.0732}\n",
      "chazhi:0.09230000\n",
      "epoch:13   global_step:13000\n",
      "{0: 0.1012, 1: 0.1062, 2: 0.0727, 3: 0.1151, 4: 0.1132, 5: 0.0584, 6: 0.0891, 7: 0.1138, 8: 0.1241, 9: 0.1062}\n",
      "chazhi:0.06570000\n",
      "epoch:14   global_step:13200\n",
      "{0: 0.1, 1: 0.1142, 2: 0.0772, 3: 0.1011, 4: 0.1322, 5: 0.0497, 6: 0.0749, 7: 0.1431, 8: 0.1144, 9: 0.0932}\n",
      "chazhi:0.09340000\n",
      "epoch:14   global_step:13400\n",
      "{0: 0.1081, 1: 0.1104, 2: 0.0752, 3: 0.1168, 4: 0.1159, 5: 0.0449, 6: 0.0882, 7: 0.1298, 8: 0.1148, 9: 0.0959}\n",
      "chazhi:0.08490000\n",
      "epoch:14   global_step:13600\n",
      "{0: 0.1016, 1: 0.1001, 2: 0.0694, 3: 0.1085, 4: 0.1281, 5: 0.0517, 6: 0.0756, 7: 0.1336, 8: 0.1307, 9: 0.1007}\n",
      "chazhi:0.08190000\n",
      "epoch:14   global_step:13800\n",
      "{0: 0.0977, 1: 0.1168, 2: 0.0634, 3: 0.1035, 4: 0.123, 5: 0.0738, 6: 0.0872, 7: 0.1321, 8: 0.1128, 9: 0.0897}\n",
      "chazhi:0.06870000\n",
      "epoch:14   global_step:14000\n",
      "{0: 0.0961, 1: 0.1158, 2: 0.0759, 3: 0.1228, 4: 0.1136, 5: 0.0714, 6: 0.0722, 7: 0.1351, 8: 0.109, 9: 0.0881}\n",
      "chazhi:0.06370000\n",
      "epoch:15   global_step:14200\n",
      "{0: 0.1027, 1: 0.1095, 2: 0.0539, 3: 0.1181, 4: 0.1134, 5: 0.0524, 6: 0.0855, 7: 0.1469, 8: 0.1108, 9: 0.1068}\n",
      "chazhi:0.09450000\n",
      "epoch:15   global_step:14400\n",
      "{0: 0.087, 1: 0.1009, 2: 0.065, 3: 0.1071, 4: 0.1125, 5: 0.0603, 6: 0.0784, 7: 0.1604, 8: 0.1291, 9: 0.0993}\n",
      "chazhi:0.10010000\n",
      "epoch:15   global_step:14600\n",
      "{0: 0.1054, 1: 0.1291, 2: 0.063, 3: 0.1141, 4: 0.1218, 5: 0.0549, 6: 0.0924, 7: 0.1139, 8: 0.1233, 9: 0.0821}\n",
      "chazhi:0.07420000\n",
      "epoch:15   global_step:14800\n",
      "{0: 0.1105, 1: 0.1064, 2: 0.0635, 3: 0.1165, 4: 0.1219, 5: 0.0592, 6: 0.0785, 7: 0.1361, 8: 0.1158, 9: 0.0916}\n",
      "chazhi:0.07690000\n",
      "epoch:16   global_step:15000\n",
      "{0: 0.107, 1: 0.1168, 2: 0.0635, 3: 0.1147, 4: 0.1279, 5: 0.0507, 6: 0.0789, 7: 0.1345, 8: 0.1121, 9: 0.0939}\n",
      "chazhi:0.08380000\n",
      "epoch:16   global_step:15200\n",
      "{0: 0.1007, 1: 0.118, 2: 0.0696, 3: 0.0935, 4: 0.1455, 5: 0.0427, 6: 0.1026, 7: 0.1309, 8: 0.1166, 9: 0.0799}\n",
      "chazhi:0.10280000\n",
      "epoch:16   global_step:15400\n",
      "{0: 0.1036, 1: 0.1184, 2: 0.0583, 3: 0.1079, 4: 0.1236, 5: 0.0496, 6: 0.0981, 7: 0.1257, 8: 0.1065, 9: 0.1083}\n",
      "chazhi:0.07610000\n",
      "epoch:16   global_step:15600\n",
      "{0: 0.0886, 1: 0.118, 2: 0.0719, 3: 0.0948, 4: 0.1261, 5: 0.0523, 6: 0.1013, 7: 0.1466, 8: 0.1172, 9: 0.0832}\n",
      "chazhi:0.09430000\n",
      "epoch:16   global_step:15800\n",
      "{0: 0.1034, 1: 0.1177, 2: 0.0672, 3: 0.1068, 4: 0.1172, 5: 0.0645, 6: 0.0714, 7: 0.1421, 8: 0.1068, 9: 0.1029}\n",
      "chazhi:0.07760000\n",
      "epoch:17   global_step:16000\n",
      "{0: 0.0987, 1: 0.0985, 2: 0.0824, 3: 0.1078, 4: 0.1153, 5: 0.06, 6: 0.0928, 7: 0.1453, 8: 0.1132, 9: 0.086}\n",
      "chazhi:0.08530000\n",
      "epoch:17   global_step:16200\n",
      "{0: 0.1124, 1: 0.0997, 2: 0.0598, 3: 0.109, 4: 0.1138, 5: 0.0619, 6: 0.0999, 7: 0.1316, 8: 0.1214, 9: 0.0905}\n",
      "chazhi:0.07180000\n",
      "epoch:17   global_step:16400\n",
      "{0: 0.1034, 1: 0.1285, 2: 0.0602, 3: 0.0905, 4: 0.1382, 5: 0.0536, 6: 0.0756, 7: 0.1335, 8: 0.1369, 9: 0.0796}\n",
      "chazhi:0.08460000\n",
      "epoch:17   global_step:16600\n",
      "{0: 0.1004, 1: 0.1233, 2: 0.0759, 3: 0.1023, 4: 0.114, 5: 0.0557, 6: 0.0806, 7: 0.1365, 8: 0.1048, 9: 0.1065}\n",
      "chazhi:0.08080000\n",
      "epoch:17   global_step:16800\n",
      "{0: 0.1131, 1: 0.1019, 2: 0.0707, 3: 0.1128, 4: 0.1115, 5: 0.057, 6: 0.0895, 7: 0.1324, 8: 0.1154, 9: 0.0957}\n",
      "chazhi:0.07540000\n",
      "epoch:18   global_step:17000\n",
      "{0: 0.1278, 1: 0.1124, 2: 0.0615, 3: 0.1162, 4: 0.1065, 5: 0.0416, 6: 0.0846, 7: 0.1409, 8: 0.105, 9: 0.1035}\n",
      "chazhi:0.09930000\n",
      "epoch:18   global_step:17200\n",
      "{0: 0.1057, 1: 0.1188, 2: 0.0616, 3: 0.106, 4: 0.1068, 5: 0.0612, 6: 0.0847, 7: 0.1254, 8: 0.1073, 9: 0.1225}\n",
      "chazhi:0.06420000\n",
      "epoch:18   global_step:17400\n",
      "{0: 0.1009, 1: 0.0935, 2: 0.0595, 3: 0.1117, 4: 0.133, 5: 0.0574, 6: 0.1038, 7: 0.1289, 8: 0.1143, 9: 0.097}\n",
      "chazhi:0.07560000\n",
      "epoch:18   global_step:17600\n",
      "{0: 0.1156, 1: 0.0918, 2: 0.0676, 3: 0.1245, 4: 0.1296, 5: 0.0646, 6: 0.0818, 7: 0.1184, 8: 0.1056, 9: 0.1005}\n",
      "chazhi:0.06500000\n",
      "epoch:18   global_step:17800\n",
      "{0: 0.0991, 1: 0.1038, 2: 0.0728, 3: 0.1052, 4: 0.1286, 5: 0.0576, 6: 0.0878, 7: 0.1305, 8: 0.1223, 9: 0.0923}\n",
      "chazhi:0.07290000\n",
      "epoch:19   global_step:18000\n",
      "{0: 0.0983, 1: 0.1109, 2: 0.0675, 3: 0.1159, 4: 0.1488, 5: 0.0467, 6: 0.0912, 7: 0.1256, 8: 0.1051, 9: 0.09}\n",
      "chazhi:0.10210000\n",
      "epoch:19   global_step:18200\n",
      "{0: 0.107, 1: 0.1154, 2: 0.0744, 3: 0.1103, 4: 0.11, 5: 0.0512, 6: 0.0955, 7: 0.1245, 8: 0.1104, 9: 0.1013}\n",
      "chazhi:0.07330000\n",
      "epoch:19   global_step:18400\n",
      "{0: 0.1162, 1: 0.1002, 2: 0.065, 3: 0.1162, 4: 0.1284, 5: 0.0555, 6: 0.0804, 7: 0.1287, 8: 0.1115, 9: 0.0979}\n",
      "chazhi:0.07320000\n",
      "epoch:19   global_step:18600\n",
      "{0: 0.0992, 1: 0.1056, 2: 0.0654, 3: 0.1024, 4: 0.1199, 5: 0.0631, 6: 0.0805, 7: 0.1298, 8: 0.1131, 9: 0.121}\n",
      "chazhi:0.06670000\n",
      "epoch:20   global_step:18800\n",
      "{0: 0.1091, 1: 0.1073, 2: 0.0733, 3: 0.1002, 4: 0.1246, 5: 0.0498, 6: 0.0889, 7: 0.1441, 8: 0.1049, 9: 0.0978}\n",
      "chazhi:0.09430000\n",
      "epoch:20   global_step:19000\n",
      "{0: 0.114, 1: 0.1114, 2: 0.0667, 3: 0.1104, 4: 0.1124, 5: 0.0536, 6: 0.0865, 7: 0.1229, 8: 0.1271, 9: 0.095}\n",
      "chazhi:0.07350000\n",
      "epoch:20   global_step:19200\n",
      "{0: 0.1105, 1: 0.1093, 2: 0.0634, 3: 0.1061, 4: 0.1202, 5: 0.0627, 6: 0.0922, 7: 0.1211, 8: 0.1214, 9: 0.0931}\n",
      "chazhi:0.05870000\n",
      "epoch:20   global_step:19400\n",
      "{0: 0.1117, 1: 0.1198, 2: 0.0675, 3: 0.0994, 4: 0.1202, 5: 0.056, 6: 0.0871, 7: 0.1385, 8: 0.1056, 9: 0.0942}\n",
      "chazhi:0.08250000\n",
      "epoch:20   global_step:19600\n",
      "{0: 0.1158, 1: 0.1238, 2: 0.0585, 3: 0.0957, 4: 0.1298, 5: 0.0613, 6: 0.0889, 7: 0.1109, 8: 0.1115, 9: 0.1038}\n",
      "chazhi:0.07130000\n",
      "epoch:21   global_step:19800\n",
      "{0: 0.0936, 1: 0.1102, 2: 0.0684, 3: 0.1017, 4: 0.0989, 5: 0.0542, 6: 0.1011, 7: 0.1333, 8: 0.1388, 9: 0.0998}\n",
      "chazhi:0.08460000\n",
      "epoch:21   global_step:20000\n",
      "{0: 0.101, 1: 0.1206, 2: 0.0637, 3: 0.0945, 4: 0.1209, 5: 0.0578, 6: 0.0923, 7: 0.111, 8: 0.1238, 9: 0.1144}\n",
      "chazhi:0.06600000\n",
      "epoch:21   global_step:20200\n",
      "{0: 0.0939, 1: 0.1274, 2: 0.0781, 3: 0.1083, 4: 0.1079, 5: 0.0522, 6: 0.0856, 7: 0.1263, 8: 0.1188, 9: 0.1015}\n",
      "chazhi:0.07520000\n",
      "epoch:21   global_step:20400\n",
      "{0: 0.0997, 1: 0.1188, 2: 0.0718, 3: 0.11, 4: 0.1155, 5: 0.0568, 6: 0.0711, 7: 0.1299, 8: 0.1251, 9: 0.1013}\n",
      "chazhi:0.07310000\n",
      "epoch:21   global_step:20600\n",
      "{0: 0.1062, 1: 0.1084, 2: 0.0625, 3: 0.0949, 4: 0.1236, 5: 0.0601, 6: 0.0835, 7: 0.1525, 8: 0.1237, 9: 0.0846}\n",
      "chazhi:0.09240000\n",
      "epoch:22   global_step:20800\n",
      "{0: 0.103, 1: 0.1116, 2: 0.0738, 3: 0.1134, 4: 0.1265, 5: 0.0599, 6: 0.0841, 7: 0.1191, 8: 0.1143, 9: 0.0943}\n",
      "chazhi:0.06660000\n",
      "epoch:22   global_step:21000\n",
      "{0: 0.1049, 1: 0.0987, 2: 0.0609, 3: 0.1082, 4: 0.1263, 5: 0.0684, 6: 0.0843, 7: 0.1248, 8: 0.1099, 9: 0.1136}\n",
      "chazhi:0.06540000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:22   global_step:21200\n",
      "{0: 0.0947, 1: 0.1061, 2: 0.0632, 3: 0.1036, 4: 0.1104, 5: 0.0639, 6: 0.0975, 7: 0.163, 8: 0.1018, 9: 0.0958}\n",
      "chazhi:0.09980000\n",
      "epoch:22   global_step:21400\n",
      "{0: 0.0941, 1: 0.1041, 2: 0.0883, 3: 0.1041, 4: 0.1152, 5: 0.0484, 6: 0.081, 7: 0.1471, 8: 0.1083, 9: 0.1094}\n",
      "chazhi:0.09870000\n",
      "epoch:23   global_step:21600\n",
      "{0: 0.0984, 1: 0.1331, 2: 0.0689, 3: 0.1021, 4: 0.114, 5: 0.0486, 6: 0.0968, 7: 0.132, 8: 0.1157, 9: 0.0904}\n",
      "chazhi:0.08450000\n",
      "epoch:23   global_step:21800\n",
      "{0: 0.1151, 1: 0.1194, 2: 0.0658, 3: 0.1068, 4: 0.1129, 5: 0.0575, 6: 0.0914, 7: 0.1059, 8: 0.1273, 9: 0.0979}\n",
      "chazhi:0.06980000\n",
      "epoch:23   global_step:22000\n",
      "{0: 0.1035, 1: 0.1171, 2: 0.0761, 3: 0.1088, 4: 0.1105, 5: 0.0554, 6: 0.0872, 7: 0.1483, 8: 0.1029, 9: 0.0902}\n",
      "chazhi:0.09290000\n",
      "epoch:23   global_step:22200\n",
      "{0: 0.0878, 1: 0.1183, 2: 0.0513, 3: 0.0965, 4: 0.1236, 5: 0.0611, 6: 0.1061, 7: 0.1281, 8: 0.1139, 9: 0.1133}\n",
      "chazhi:0.07680000\n",
      "epoch:23   global_step:22400\n",
      "{0: 0.0903, 1: 0.1356, 2: 0.0564, 3: 0.1234, 4: 0.1183, 5: 0.0526, 6: 0.0831, 7: 0.125, 8: 0.1113, 9: 0.104}\n",
      "chazhi:0.08300000\n",
      "epoch:24   global_step:22600\n",
      "{0: 0.1141, 1: 0.1015, 2: 0.0667, 3: 0.1009, 4: 0.1181, 5: 0.06, 6: 0.1033, 7: 0.1268, 8: 0.1102, 9: 0.0984}\n",
      "chazhi:0.06680000\n",
      "epoch:24   global_step:22800\n",
      "{0: 0.1022, 1: 0.1276, 2: 0.0584, 3: 0.1023, 4: 0.1311, 5: 0.0586, 6: 0.0885, 7: 0.1297, 8: 0.1152, 9: 0.0864}\n",
      "chazhi:0.07270000\n",
      "epoch:24   global_step:23000\n",
      "{0: 0.1053, 1: 0.106, 2: 0.0589, 3: 0.0916, 4: 0.1481, 5: 0.0475, 6: 0.0958, 7: 0.1444, 8: 0.1152, 9: 0.0872}\n",
      "chazhi:0.10060000\n",
      "epoch:24   global_step:23200\n",
      "{0: 0.1143, 1: 0.1113, 2: 0.0684, 3: 0.1016, 4: 0.1125, 5: 0.0638, 6: 0.0898, 7: 0.119, 8: 0.1129, 9: 0.1064}\n",
      "chazhi:0.05520000\n",
      "epoch:24   global_step:23400\n",
      "{0: 0.0941, 1: 0.1027, 2: 0.0645, 3: 0.0914, 4: 0.1327, 5: 0.0597, 6: 0.0994, 7: 0.1394, 8: 0.1177, 9: 0.0984}\n",
      "chazhi:0.07970000\n",
      "epoch:25   global_step:23600\n",
      "{0: 0.1215, 1: 0.1226, 2: 0.059, 3: 0.1068, 4: 0.1156, 5: 0.0519, 6: 0.0925, 7: 0.1148, 8: 0.1217, 9: 0.0936}\n",
      "chazhi:0.07070000\n",
      "epoch:25   global_step:23800\n",
      "{0: 0.1146, 1: 0.1306, 2: 0.0638, 3: 0.108, 4: 0.108, 5: 0.0566, 6: 0.0921, 7: 0.1244, 8: 0.102, 9: 0.0999}\n",
      "chazhi:0.07400000\n",
      "epoch:25   global_step:24000\n",
      "{0: 0.1052, 1: 0.1128, 2: 0.072, 3: 0.0906, 4: 0.1225, 5: 0.0499, 6: 0.0848, 7: 0.1481, 8: 0.1108, 9: 0.1033}\n",
      "chazhi:0.09820000\n",
      "epoch:25   global_step:24200\n",
      "{0: 0.1048, 1: 0.1183, 2: 0.0721, 3: 0.0938, 4: 0.1098, 5: 0.0654, 6: 0.1016, 7: 0.1305, 8: 0.1082, 9: 0.0955}\n",
      "chazhi:0.06510000\n",
      "epoch:26   global_step:24400\n",
      "{0: 0.104, 1: 0.1088, 2: 0.0654, 3: 0.091, 4: 0.1236, 5: 0.064, 6: 0.1026, 7: 0.1188, 8: 0.1102, 9: 0.1116}\n",
      "chazhi:0.05960000\n",
      "epoch:26   global_step:24600\n",
      "{0: 0.113, 1: 0.1203, 2: 0.0673, 3: 0.1062, 4: 0.1128, 5: 0.0566, 6: 0.0901, 7: 0.1375, 8: 0.1082, 9: 0.088}\n",
      "chazhi:0.08090000\n",
      "epoch:26   global_step:24800\n",
      "{0: 0.116, 1: 0.1178, 2: 0.0794, 3: 0.1073, 4: 0.1065, 5: 0.054, 6: 0.1041, 7: 0.1038, 8: 0.1122, 9: 0.0989}\n",
      "chazhi:0.06380000\n",
      "epoch:26   global_step:25000\n",
      "{0: 0.0997, 1: 0.1141, 2: 0.0665, 3: 0.0832, 4: 0.1437, 5: 0.059, 6: 0.0977, 7: 0.1101, 8: 0.1123, 9: 0.1137}\n",
      "chazhi:0.08470000\n",
      "epoch:26   global_step:25200\n",
      "{0: 0.1011, 1: 0.1206, 2: 0.0628, 3: 0.1073, 4: 0.1156, 5: 0.0556, 6: 0.1063, 7: 0.1371, 8: 0.1, 9: 0.0936}\n",
      "chazhi:0.08150000\n",
      "epoch:27   global_step:25400\n",
      "{0: 0.1107, 1: 0.1209, 2: 0.0652, 3: 0.1128, 4: 0.1176, 5: 0.0557, 6: 0.0894, 7: 0.1192, 8: 0.1102, 9: 0.0983}\n",
      "chazhi:0.06520000\n",
      "epoch:27   global_step:25600\n",
      "{0: 0.0989, 1: 0.14, 2: 0.0587, 3: 0.0992, 4: 0.119, 5: 0.0611, 6: 0.0998, 7: 0.1081, 8: 0.1227, 9: 0.0925}\n",
      "chazhi:0.08130000\n",
      "epoch:27   global_step:25800\n",
      "{0: 0.1182, 1: 0.1207, 2: 0.0695, 3: 0.0991, 4: 0.1089, 5: 0.0579, 6: 0.0715, 7: 0.1279, 8: 0.1171, 9: 0.1092}\n",
      "chazhi:0.07000000\n",
      "epoch:27   global_step:26000\n",
      "{0: 0.1109, 1: 0.1119, 2: 0.0773, 3: 0.1106, 4: 0.1142, 5: 0.0551, 6: 0.0742, 7: 0.1218, 8: 0.1158, 9: 0.1082}\n",
      "chazhi:0.06670000\n",
      "epoch:27   global_step:26200\n",
      "{0: 0.1094, 1: 0.0913, 2: 0.0705, 3: 0.1026, 4: 0.1214, 5: 0.0595, 6: 0.0955, 7: 0.1322, 8: 0.1269, 9: 0.0907}\n",
      "chazhi:0.07270000\n",
      "epoch:28   global_step:26400\n",
      "{0: 0.1062, 1: 0.1093, 2: 0.063, 3: 0.1036, 4: 0.129, 5: 0.0549, 6: 0.1018, 7: 0.1225, 8: 0.1134, 9: 0.0963}\n",
      "chazhi:0.07410000\n",
      "epoch:28   global_step:26600\n",
      "{0: 0.1051, 1: 0.1288, 2: 0.0604, 3: 0.0971, 4: 0.1256, 5: 0.0564, 6: 0.1089, 7: 0.123, 8: 0.0962, 9: 0.0985}\n",
      "chazhi:0.07240000\n",
      "epoch:28   global_step:26800\n",
      "{0: 0.1148, 1: 0.1248, 2: 0.0639, 3: 0.1048, 4: 0.1087, 5: 0.0711, 6: 0.1045, 7: 0.1133, 8: 0.0985, 9: 0.0956}\n",
      "chazhi:0.06090000\n",
      "epoch:28   global_step:27000\n",
      "{0: 0.1104, 1: 0.0987, 2: 0.06, 3: 0.1158, 4: 0.109, 5: 0.0815, 6: 0.0958, 7: 0.1304, 8: 0.0977, 9: 0.1007}\n",
      "chazhi:0.07040000\n",
      "epoch:29   global_step:27200\n",
      "{0: 0.1154, 1: 0.1071, 2: 0.0661, 3: 0.1008, 4: 0.1386, 5: 0.0534, 6: 0.098, 7: 0.1248, 8: 0.1001, 9: 0.0957}\n",
      "chazhi:0.08520000\n",
      "epoch:29   global_step:27400\n",
      "{0: 0.0968, 1: 0.1211, 2: 0.0653, 3: 0.1135, 4: 0.1082, 5: 0.0537, 6: 0.0852, 7: 0.1268, 8: 0.1135, 9: 0.1159}\n",
      "chazhi:0.07310000\n",
      "epoch:29   global_step:27600\n",
      "{0: 0.0998, 1: 0.1016, 2: 0.0682, 3: 0.1158, 4: 0.1057, 5: 0.0674, 6: 0.108, 7: 0.115, 8: 0.1107, 9: 0.1078}\n",
      "chazhi:0.04840000\n",
      "epoch:29   global_step:27800\n",
      "{0: 0.0993, 1: 0.1192, 2: 0.0649, 3: 0.0939, 4: 0.1372, 5: 0.05, 6: 0.103, 7: 0.1145, 8: 0.106, 9: 0.112}\n",
      "chazhi:0.08720000\n",
      "epoch:29   global_step:28000\n",
      "{0: 0.1088, 1: 0.111, 2: 0.0673, 3: 0.119, 4: 0.1051, 5: 0.0641, 6: 0.0882, 7: 0.111, 8: 0.1092, 9: 0.1163}\n",
      "chazhi:0.05490000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class LSGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        # (!!!) Optimize w.r.t. MSE loss instead of crossentropy\n",
    "        self.combined.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        # (!!!) No softmax\n",
    "        model.add(Dense(1))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        nb_batches = int(X_train.shape[0] / batch_size)\n",
    "        global_step = 0\n",
    "        steps=[]\n",
    "        values=[]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for index in range(nb_batches):\n",
    "                global_step += 1\n",
    "                imgs = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the discriminator\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Generator\n",
    "                # ---------------------\n",
    "\n",
    "                g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "                # Plot the progress\n",
    "#                 print(\"epoch:%d step:%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, global_step,d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if global_step % sample_interval == 0:\n",
    "                    label_dict,value=self.mode_drop(epoch,global_step)\n",
    "                    steps.append(global_step)\n",
    "                    values.append(value)\n",
    "#             plt.subplots(1, 1)\n",
    "        plt.plot(steps,values)\n",
    "        plt.xlim([0,40000])\n",
    "        plt.ylim([0.0,1.0])\n",
    "        plt.xlabel('steps')\n",
    "        plt.ylabel('epochs')\n",
    "        plt.tick_params(axis='both',which='major',labelsize=13)\n",
    "        plt.legend(loc='lower right')\n",
    "        if not os.path.isdir('images_lsgan'):\n",
    "            os.mkdir('images_lsgan')\n",
    "        plt.savefig(\"images_lsgan/mode_drop.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def sample_images(self, epoch,global_step):\n",
    "        r, c = 10, 10\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        if not os.path.isdir('images_lsgan'):\n",
    "            os.mkdir('images_lsgan')\n",
    "        fig.savefig(\"images_lsgan/epoch_%d_step_%d.png\" % (epoch,global_step))\n",
    "        plt.close()\n",
    "    def mode_drop(self,epoch,global_step):\n",
    "        r, c = 10, 1000\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        label_dict,value=calculate_labels(gen_imgs,epoch,global_step)\n",
    "        return label_dict,value\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = LSGAN()\n",
    "    gan.train(epochs=30, batch_size=64, sample_interval=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pppppppp [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
